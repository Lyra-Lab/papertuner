{
  "id": "http://arxiv.org/abs/2209.15224v4",
  "title": "Robust Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models",
  "authors": [
    "Ye Tian",
    "Haolei Weng",
    "Lucy Xia",
    "Yang Feng"
  ],
  "abstract": "Unsupervised learning has been widely used in many real-world applications.\nOne of the simplest and most important unsupervised learning models is the\nGaussian mixture model (GMM). In this work, we study the multi-task learning\nproblem on GMMs, which aims to leverage potentially similar GMM parameter\nstructures among tasks to obtain improved learning performance compared to\nsingle-task learning. We propose a multi-task GMM learning procedure based on\nthe EM algorithm that effectively utilizes unknown similarities between related\ntasks and is robust against a fraction of outlier tasks from arbitrary\ndistributions. The proposed procedure is shown to achieve the minimax optimal\nrate of convergence for both parameter estimation error and the excess\nmis-clustering error, in a wide range of regimes. Moreover, we generalize our\napproach to tackle the problem of transfer learning for GMMs, where similar\ntheoretical results are derived. Additionally, iterative unsupervised\nmulti-task and transfer learning methods may suffer from an initialization\nalignment problem, and two alignment algorithms are proposed to resolve the\nissue. Finally, we demonstrate the effectiveness of our methods through\nsimulations and real data examples. To the best of our knowledge, this is the\nfirst work studying multi-task and transfer learning on GMMs with theoretical\nguarantees.",
  "text": "Robust Unsupervised Multi-task and\nTransfer Learning on Gaussian Mixture\nModels\nYe Tian1, Haolei Weng2, Lucy Xia3, and Yang Feng4\n1Department of Statistics, Columbia University\n2Department of Statistics and Probability, Michigan State University\n3Department of ISOM, School of Business and Management, Hong Kong\nUniversity of Science and Technology\n4Department of Biostatistics, School of Global Public Health, New York\nUniversity\nAbstract\nUnsupervised learning has been widely used in many real-world applications. One\nof the simplest and most important unsupervised learning models is the Gaussian mix-\nture model (GMM). In this work, we study the multi-task learning problem on GMMs,\nwhich aims to leverage potentially similar GMM parameter structures among tasks to\nobtain improved learning performance compared to single-task learning. We propose\na multi-task GMM learning procedure based on the EM algorithm that effectively\nutilizes unknown similarities between related tasks and is robust against a fraction\nof outlier tasks from arbitrary distributions. The proposed procedure is shown to\nachieve the minimax optimal rate of convergence for both parameter estimation error\nand the excess mis-clustering error, in a wide range of regimes. Moreover, we general-\nize our approach to tackle the problem of transfer learning for GMMs, where similar\ntheoretical results are derived. Additionally, iterative unsupervised multi-task and\ntransfer learning methods may suffer from an initialization alignment problem, and\ntwo alignment algorithms are proposed to resolve the issue. Finally, we demonstrate\nthe effectiveness of our methods through simulations and real data examples. To the\nbest of our knowledge, this is the first work studying multi-task and transfer learning\non GMMs with theoretical guarantees.\nKeywords: Multi-task learning, transfer learning, unsupervised learning, Gaussian mixture\nmodels, robustness, minimax rate, EM algorithm\n1\narXiv:2209.15224v4  [stat.ML]  2 Aug 2024\n1\nIntroduction\n1.1\nGaussian mixture models (GMMs)\nUnsupervised learning that learns patterns from unlabeled data is a prevalent problem\nin statistics and machine learning. Clustering is one of the most important problems in\nunsupervised learning, where the goal is to group the observations based on some metrics\nof similarity. Researchers have developed numerous clustering methods including k-means\n(Forgy, 1965), k-medians (Jain and Dubes, 1988), spectral clustering (Ng et al., 2001),\nand hierarchical clustering (Murtagh and Contreras, 2012), among others. On the other\nhand, clustering problems have been analyzed from the perspective of the mixture of several\nprobability distributions (Scott and Symons, 1971). The mixture of Gaussian distributions\nis one of the simplest models in this category and has been widely applied in many real\napplications (Yang and Ahuja, 1998; Lee et al., 2012).\nIn the binary Gaussian mixture models (GMMs) with common covariances, each obser-\nvation Z ∈Rp comes from the following mixture of two Gaussian distributions:\nY =\n\n\n\n\n\n\n\n1,\nwith probability 1 −w,\n2,\nwith probability w,\nZ|Y = r ∼N(µr, Σ),\nr = 1, 2,\nwhere w ∈(0, 1), µ1 ∈Rp, µ2 ∈Rp and Σ ≻0 are parameters. This is the same setting\nas the linear discriminant analysis (LDA) problem in classification (Hastie et al., 2009),\nexcept that the label Y is unknown in the clustering problem, while it is observed in the\nclassification case. It has been shown that the Bayes classifier for the LDA problem is\nC(z) =\n\n\n\n\n\n\n\n1,\nif β⊤z −δ ≤log\n\u0000 1−w\nw\n\u0001\n;\n2,\notherwise,\n(1)\n2\nwhere β = Σ−1(µ2 −µ1) ∈Rp and δ = β⊤(µ1 + µ2)/2. Note that β is usually referred\nto as the discriminant coefficient (Anderson, 1958; Efron, 1975). Naturally, this classifier\nis useful in clustering too. In clustering, after learning w, µ1, µ2 and β, we can plug\ntheir estimators into (1) to group any new observation Znew.\nGenerally, we define the\nmis-clustering error rate of any given clustering method C as\nR(C) =\nmin\nπ:{1,2}→{1,2} P(C(Znew) ̸= π(Y new)),\nwhere π is a permutation function, Y new is the label of a future observation Znew, and the\nprobability is taken w.r.t. the joint distribution of (Znew, Y new) based on parameters w,\nµ1, µ2 and Σ. Here the error is calculated up to a permutation due to the lack of label\ninformation. It is clear that in the ideal case where the parameters are known, C(·) in\n(1) achieves the optimal mis-clustering error. Multi-cluster Gaussian mixture models with\nR ≥3 components can be described in a similar way. We provide the details in Section S.1\nof the supplementary materials.\nThere is a large volume of published studies on learning a GMM. The vast majority of\napproaches can be roughly divided into three categories. The first category is the method\nof moments, where the parameters are estimated through several moment equations (Pear-\nson, 1894; Kalai et al., 2010; Hsu and Kakade, 2013; Ge et al., 2015). The second category\nis the spectral method, where the estimation is based on the spectral decomposition (Vem-\npala and Wang, 2004; Hsu and Kakade, 2013; Jin et al., 2017). The last category is the\nlikelihood-based method including the popular expectation-maximization (EM) algorithm\nas a canonical example. The general form of the EM algorithm was formalized by Dempster\net al. (1977) in the context of incomplete data, though earlier works (Hartley, 1958; Hassel-\nblad, 1966; Baum et al., 1970; Sundberg, 1974) have studied EM-style algorithms in various\nconcrete settings. Classical convergence results on the EM algorithm (Wu, 1983; Redner\nand Walker, 1984; Meng and Rubin, 1994; McLachlan and Krishnan, 2007) guarantee local\nconvergence of the algorithm to fixed points of the sample likelihood. Recent advances in\n3\nthe analysis of EM algorithm and its variants provide stronger guarantees by establishing\ngeometric convergence rates of the algorithm to the underlying true parameters under mild\ninitialization conditions. See, for example, Dasgupta and Schulman (2013); Wang et al.\n(2014); Xu et al. (2016); Balakrishnan et al. (2017); Yan et al. (2017); Cai et al. (2019);\nKwon and Caramanis (2020); Zhao et al. (2020) for GMM related works. In this paper, we\npropose modified versions of the EM algorithm with similarly strong guarantees to learn\nGMMs, under the new context of multi-task and transfer learning.\n1.2\nMulti-task learning and transfer learning\nMulti-tasking is an ability that helps people pay attention to more than one task simultane-\nously. Moreover, we often find that the knowledge learned from one task can also be useful\nin other tasks. Multi-task learning (MTL) is a learning paradigm inspired by the human\nlearning ability, which aims to learn multiple tasks and improve performance by utilizing\nthe similarity between these tasks (Zhang and Yang, 2021). There has been numerous re-\nsearch on MTL, which can be classified into five categories (Zhang and Yang, 2021): feature\nlearning approach (Argyriou et al., 2008; Obozinski et al., 2006), low-rank approach (Ando\net al., 2005), task clustering approach (Thrun and O’Sullivan, 1996), task relation learning\napproach (Evgeniou and Pontil, 2004) and decomposition approach (Jalali et al., 2010).\nThe majority of existing works focus on the use of MTL in supervised learning problems,\nwhile the application of MTL in unsupervised learning, such as clustering, has received\nless attention. Zhang and Zhang (2011) developed an MTL clustering method based on a\npenalization framework, where the objective function consists of a local loss function and\na pairwise task regularization term, both of which are related to the Bregman divergence.\nIn Gu et al. (2011), a reproducing kernel Hilbert space (RKHS) was first established, and\nthen a multi-task kernel k-means clustering was applied based on that RKHS. Yang et al.\n(2014) proposed a spectral MTL clustering method with a novel ℓ2,p-norm, which can also\n4\nproduce a linear regression function to predict labels for out-of-sample data. Zhang et al.\n(2018) suggested a new method based on the similarity matrix of samples in each task,\nwhich can learn the within-task clustering structure as well as the task relatedness simul-\ntaneously. Marfoq et al. (2021) established a new federated multi-task EM algorithm to\nlearn the mixture of distributions and provided some theory on the convergence guaran-\ntee, but the statistical properties of the estimators were not fully understood. Zhang and\nChen (2022) proposed a distributed learning algorithm for GMMs based on transportation\ndivergence when all GMMs are identical. In general, there are very few theoretical results\nabout unsupervised MTL.\nTransfer learning (TL) is another learning paradigm similar to multi-task learning but\nhas different objectives. While MTL aims to learn all the tasks well with no priority for\nany specific task, the goal of TL is to improve the performance on the target task using\nthe information from the source tasks (Zhang and Yang, 2021). According to Pan and\nYang (2009), most of TL approaches can be classified into four categories: instance-based\ntransfer (Dai et al., 2007), feature representation transfer (Dai et al., 2008), parameter\ntransfer (Lawrence and Platt, 2004) and relational-knowledge transfer (Mihalkova et al.,\n2007).\nSimilar to MTL, most of the TL methods focus on supervised learning.\nSome\nTL approaches are also developed for the semi-supervised learning setting (Chattopadhyay\net al., 2012; Li et al., 2013), where only part of target or source data is labeled. There\nare much fewer discussions on the unsupervised TL approaches 1, which focuses on the\ncases where both target and source data are unlabeled. Dai et al. (2008) developed a co-\nclustering approach to transfer information from a single source to the target, which relies\non the loss in mutual information and requires the features to be discrete. Wang et al.\n(2008) proposed a TL discriminant analysis method, where the target data is allowed to be\n1There are different definitions for unsupervised TL. Sometimes people call the semi-supervised TL an\nunsupervised TL as well. We follow the definition in Pan and Yang (2009) here.\n5\nunlabeled, but some labeled source data is necessary. In Wang et al. (2021), a TL approach\nwas developed to learn Gaussian mixture models with only one source by weighting the\ntarget and source likelihood functions. Zuo et al. (2018) proposed a TL method based on\ninfinite Gaussian mixture models and active learning, but their approach needs sufficient\nlabeled source data and a few labeled target samples.\nThere are some recent studies on TL and MTL under various statistical settings, in-\ncluding high-dimensional linear regression (Xu and Bastani, 2021; Li et al., 2022b; Zhang\net al., 2022; Li et al., 2022a), high-dimensional generalized linear models (Bastani, 2021; Li\net al., 2023; Tian and Feng, 2023), functional linear regression (Lin and Reimherr, 2022),\nhigh-dimensional graphical models (Li et al., 2022b), reinforcement learning (Chen et al.,\n2022), among others. The recent work Duan and Wang (2023) developed an adaptive and\nrobust MTL framework with sharp statistical guarantees for a broad class of models. We\ndiscuss its connection to our work in Section 2.\n1.3\nOur contributions and paper structure\nOur main contributions in this work can be summarized in the following:\n(i) We develop efficient polynomial-time iterative procedures to learn GMMs in both\nMTL and TL settings. These procedures can be viewed as adaptations of the standard\nEM algorithm for MTL and TL problems.\n(ii) The developed procedures come with provable statistical guarantees.\nSpecifically,\nwe derive the upper bounds of their estimation and excess mis-clustering error rates\nunder mild conditions. For MTL, it is shown that when the tasks are close to each\nother, our method can achieve better upper bounds than those from the single-task\nlearning; when the tasks are substantially different from each other, our method can\nstill obtain competitive convergence rates compared to single-task learning. Similarly\n6\nfor TL, our method can achieve better upper bounds than those from fitting GMM\nonly to target data when the target and sources are similar, and remains competitive\notherwise. In addition, the derived upper bounds reveal the robustness of our methods\nagainst a fraction of outlier tasks (for MTL) or outlier sources (for TL) from arbitrary\ndistributions. These guarantees certify our procedures as adaptive (to the unknown\ntask relatedness) and robust (to contaminated data) learning approaches.\n(iii) We derive the minimax lower bounds for parameter estimation and excess mis-clustering\nerrors.\nIn various regimes, the upper bounds from our methods match the lower\nbounds (up to small order terms), showing that the proposed methods are (nearly)\nminimax rate optimal.\n(iv) Our MTL and TL approaches require the initial estimates for different tasks to be\n“well-aligned”, due to the non-identifiability of GMM. We propose two pre-processing\nalignment algorithms to provably resolve the alignment problem. Similar problems\narise in many unsupervised MTL settings. However, to our knowledge, there is no\nformal discussion of the alignment issue in the existing literature on unsupervised\nMTL (Gu et al., 2011; Zhang and Zhang, 2011; Yang et al., 2014; Zhang et al., 2018;\nDieuleveut et al., 2021; Marfoq et al., 2021). Therefore, our rigorous treatment of the\nalignment problem is an important step forward in this field.\nThe rest of the paper is organized as follows. In Section 2, we first discuss the multi-\ntask learning problem for binary GMMs, by introducing the problem setting, our method,\nand the associated theory. The above-mentioned alignment problem is discussed in Section\n2.4. We present a simulation study in Section 3 to validate our theory. Finally, in Section\n4, we point out some interesting future research directions. Due to the space limit, the\nextension to multi-cluster GMMs, additional numerical results, a full treatment of the\ntransfer learning problem, and all the proofs are delegated to the supplementary materials.\n7\nWe summarize the notations used throughout the paper here for convenience. We use\nbold capital letters (e.g., Σ) to denote matrices and use bold small letters (e.g., x, y) to\ndenote vectors. For a matrix A = [aij]p×q ∈Rp×q, its 2-norm or spectral norm is defined as\n∥A∥2 = maxx:∥x∥2=1 ∥Ax∥2. If q = 1, A becomes a p-dimensional vector and ∥A∥2 equals\nits Euclidean norm. For symmetric A, we define λmax(A) and λmin(A) as the maximum\nand minimum eigenvalues of A, respectively. For two non-zero real sequences {an}∞\nn=1 and\n{bn}∞\nn=1, we use an ≪bn, bn ≫an or an = O(bn) to represent |an/bn| →0 as n →∞.\nAnd an ≲bn, bn ≳an or an = O(bn) means supn |an/bn| < ∞. For two random variable\nsequences {xn}∞\nn=1 and {yn}∞\nn=1, the notation xn = OP(yn) means that for any ϵ > 0, there\nexists a positive constant M such that supn P(|xn/yn| > M) ≤ϵ. For two real numbers\na and b, a ∨b and a ∧b represent max(a, b) and min(a, b), respectively. For any positive\ninteger K, both 1 : K and [K] stand for the set {1, 2, . . . , K}. For any set S ⊆[K], |S|\ndenotes its cardinality, and Sc denotes its complement. Without further notice, c, C, C1,\nC2, . . . represent some positive constants and can change from line to line.\n2\nMulti-task Learning\n2.1\nProblem setting\nSuppose there are K tasks, for which we have nk observations {z(k)\ni }nk\ni=1 from the k-th task.\nSuppose there exists an unknown subset S ⊆1 : K, such that observations from each task\nin S independently follow a GMM, while samples from tasks outside S can be arbitrarily\ndistributed. This means,\ny(k)\ni\n=\n\n\n\n\n\n\n\n1,\nwith probability 1 −w(k)∗;\n2,\nwith probability w(k)∗;\nz(k)\ni |y(k)\ni\n= r ∼N(µ(k)∗\nr\n, Σ(k)∗),\nr = 1, 2,\n8\nfor all k ∈S, i = 1 : nk, and\n{z(k)\ni }i,k∈Sc ∼QS,\nwhere QS is some probability measure on (Rp)⊗nSc and nSc = P\nk∈Sc nk. In unsupervised\nlearning, we have no access to the true labels {y(k)\ni }i,k. To formalize the multi-task learning\nproblem, we first introduce the parameter space for a single GMM:\nΘ = {θ = (w, µ1, µ2, Σ) :∥µ1∥2 ∨∥µ2∥2 ≤M, w ∈(cw, 1 −cw),\nc−1\nΣ ≤λmin(Σ) ≤λmax(Σ) ≤cΣ},\n(2)\nwhere M, cw ∈(0, 1/2] and cΣ are some fixed positive constants. For simplicity, through-\nout the main text, we assume these constants are fixed. Hence, we have suppressed the\ndependency on them in the notation Θ. The parameter space Θ is a standard formulation.\nSimilar parameter spaces have been considered, for example, in Cai et al. (2019).\nOur goal of multi-task learning is to leverage the potential similarity shared by different\ntasks in S to collectively learn them all. The tasks outside S can be arbitrarily distributed\nand they can be potentially outlier tasks. This motivates us to define a joint parameter\nspace for GMMs in S:\nΘS(h) =\nn\n{θ\n(k)}k∈S = {(w(k), µ(k)\n1 , µ(k)\n2 , Σ(k))}k∈S : θ\n(k) ∈Θ, inf\nβ max\nk∈S ∥β(k) −β∥2 ≤h\no\n,\n(3)\nwhere β(k) = (Σ(k))−1(µ(k)\n2\n−µ(k)\n1 ) is called the discriminant coefficient in the k-th task\n(recall Section 1.1). For convenience, we define δ(k) = (β(k))⊤(µ(k)\n1 +µ(k)\n2 )/2, which together\nwith log((1−w(k))/w(k)) is part of the decision boundary. Note that this parameter space is\ndefined only for GMMs of tasks in S. To model potentially corrupted or contaminated data,\nwe do not impose any distributional constraints for tasks in Sc. Such a modeling framework\nis reminiscent of Huber’s ϵ-contamination model (Huber, 1964). Similar formulations have\nbeen adopted in recent multi-task learning research such as Konstantinov et al. (2020) and\nDuan and Wang (2023).\n9\nFor GMMs in ΘS(h), we assume that they share similar discriminant coefficients. The\nsimilarity is formalized by assuming that all the discriminant coefficients in S are within\nEuclidean distance h from a “center”. Given that the discriminant coefficient has a major\nimpact on the clustering performance (see the discriminant rule in (1)), the parameter\nspace ΘS(h) is tailored to characterize the task relatedness from the clustering perspective.\nA similar viewpoint that focuses on modeling the discriminant coefficient has appeared\nin the study of high-dimensional GMM clustering (Cai et al., 2019) and sparse linear\ndiscriminant analysis (Cai and Liu, 2011; Mai et al., 2012). With both S and h being\nunknown in practice, we aim to develop a multi-task learning procedure that is robust\nto outlier tasks in Sc, and achieves improved performance for tasks in S (compared to\nthe single-task learning), in terms of discriminant coefficient estimation and clustering,\nwhenever h is small.\nThe parameter space does not require the mean vectors {µ(k)\n1 , µ(k)\n2 }k∈S or the covari-\nance matrices {Σ(k)}k∈S to be similar, although they are not free parameters due to the\nconstraint on {β(k)}k∈S. And the mixture proportions {w(k)}k∈S do not need to be similar\neither. We thus avoid imposing restrictive conditions on those parameters. On the other\nhand, it implies that estimation of the mixture proportions, mean vectors, and covariance\nmatrices in multi-task learning may not be generally improvable over that in the single-task\nlearning. This is verified by the theoretical results in Section 2.3. While the current treat-\nment in the paper does not consider similarity structure among {µ(k)\n1 , µ(k)\n2 }k∈S, {Σ(k)}k∈S\nor {w(k)}k∈S, our methods and theory can be readily adapted to handle such scenarios, if\ndesired.\nThere are two main reasons why this MTL problem can be challenging. First, commonly\nused strategies like data pooling are fragile with respect to outlier tasks and can lead to\narbitrarily inaccurate outcomes in the presence of even a small number of outliers. Also,\nsince the distribution of data from outlier tasks can be adversarial to the learner, the idea\n10\nof outlier task detection in the recent literature (Li et al., 2021; Tian and Feng, 2023) may\nnot be applicable. Second, to address the nonconvexity of the likelihood, we propose to\nexplore the similarity among tasks via a generalization of the EM algorithm. However, a\nclear theoretical understanding of such an iterative procedure requires a delicate analysis of\nthe whole iterative process. In particular, as in the analysis of EM algorithms (Cai et al.,\n2019; Kwon and Caramanis, 2020), the estimates of similar discriminant vectors {β(k)∗}k∈S\nand other potentially dissimilar parameters are entangled in the iterations. It is highly non-\ntrivial to separate the impact of estimating {β(k)∗}k∈S and other parameters to derive the\ndesired statistical error rates. We manage to address this challenge through a localization\ntechnique by carefully shrinking the analysis radius of estimators as the iteration proceeds.\n2.2\nMethod\nWe aim to tackle the problem of GMM estimation under the context of multi-task learning.\nThe EM algorithm is commonly used to address the non-convexity of the log-likelihood\nfunction arising from the latent labels. In the standard EM algorithm, we “classify” the\nobservations (update the posterior) in E-step and update the parameter estimations in\nM-step (Redner and Walker, 1984). For multi-task and transfer learning problems, the\npenalization framework is very popular, where we solve an optimization problem based on\na new objective function. This objective function consists of a local loss function and a\npenalty term, forcing the estimators of similar tasks to be close to each other. For examples,\nsee Zhang and Zhang (2011); Zhang et al. (2015); Bastani (2021); Xu and Bastani (2021);\nLi et al. (2021); Duan and Wang (2023); Lin and Reimherr (2022); Li et al. (2023); Tian\nand Feng (2023). Thus motivated, our method seeks a combination of the EM algorithm\nand the penalization framework.\nIn particular, we adapt the penalization framework of Duan and Wang (2023) and mod-\nify the updating formulas in the M-step accordingly. The proposed procedure is summarized\n11\nin Algorithm 1. For simplicity, in Algorithm 1 we have used the notation\nγθ(z) =\nw exp(β⊤z −δ)\n1 −w + w exp(β⊤z −δ),\nfor θ = (w, β, δ).\nNote that γθ(z) is the posterior probability P(Y = 2|Z = z) given the observation z. The\nestimated posterior probability is calculated in every E-step given the updated parameter\nestimates.\nRecall that the parameter space ΘS(h) introduced in (3) does not encode similarity for\nthe mixture proportions {w(k)∗}k∈S, mean vectors {µ(k)∗\n1\n, µ(k)∗\n2\n}k∈S, or covariance matrices\n{Σ(k)∗}k∈S. Hence, the updates of them in Steps 5-7 are kept the same as in the standard\nEM algorithm. Regarding the update for discriminant coefficients in Step 9, the quadratic\nloss function is motivated by the direct estimation of the discriminant coefficient in high-\ndimensional GMM (Cai et al., 2019) and high-dimensional LDA literature (Cai and Liu,\n2011; Witten and Tibshirani, 2011; Fan et al., 2012; Mai et al., 2012, 2019). The penalty\nterm in Step 9 penalizes the contrasts of β(k)’s to exploit the similarity structure among\ntasks. Having the “center” parameter β in the penalization induces robustness against\noutlier tasks. We refer to Duan and Wang (2023) for a systematic treatment of this penal-\nization framework. It is straightforward to verify that when the tuning parameters {λ[t]}T\nt=1\nare set to zero, Algorithm 1 reduces to the standard EM algorithm performed separately on\nthe K tasks. That is, for each k = 1 : K, given the parameter estimate from the previous\nstep bθ(k)[t−1] = ( bw(k)[t−1], bβ(k)[t−1], bδ(k)[t−1]), we update bw(k)[t], bµ(k)[t]\n1\n, bµ(k)[t]\n2\n, bΣ(k)[t], and bδ(k)[t]\nas in Algorithm 1, and update bβ(k)[t] via\nbβ(k)[t] = (bΣ(k)[t])−1(bµ(k)[t]\n2\n−bµ(k)[t]\n1\n).\nFor the maximum number of iteration rounds, T, our theory will show that T ≳log(maxk=1:K nk)\nis sufficient to reach the desired statistical error rates. In practice, we can terminate the it-\neration when the change of estimates within two successive rounds falls below some pre-set\nsmall tolerance level. We discuss the initialization in detail in Sections 2.3 and 2.4.\n12\nAlgorithm 1: MTL-GMM\nInput: Initialization {( bw(k)[0], bβ(k)[0], bµ(k)[0]\n1\n, bµ(k)[0]\n2\n)}K\nk=1, maximum number of\niteration rounds T, initial penalty parameter λ[0], tuning parameters\nCλ > 0 and κ ∈(0, 1)\n1 bθ(k)[0] = ( bw(k)[0], bβ(k)[0], bδ(k)[0]) and bδ(k)[0] = 1\n2( bβ(k)[0])⊤(bµ(k)[0]\n1\n+ bµ(k)[0]\n2\n) for k = 1 : K\n2 for t = 1 to T do\n3\nλ[t] = κλ[t−1] + Cλ\n√p + log K\n// Penalty parameter update\n4\nfor k = 1 to K do\n// Local update for each task\n5\nbw(k)[t] =\n1\nnk\nPnk\ni=1 γbθ(k)[t−1](z(k)\ni )\n6\nbµ(k)[t]\n1\n=\nPnk\ni=1[1−γbθ(k)[t−1](z(k)\ni\n)]z(k)\ni\nnk(1−bw(k)[t])\n, bµ(k)[t]\n2\n=\nPnk\ni=1 γbθ(k)[t−1](z(k)\ni\n)z(k)\ni\nnk bw(k)[t]\n7\nbΣ(k)[t] =\n1\nnk\nPnk\ni=1\nn\n[1 −γbθ(k)[t−1](z(k)\ni )] · (z(k)\ni\n−bµ(k)[t]\n1\n)(z(k)\ni\n−bµ(k)[t]\n1\n)⊤\n+γbθ(k)[t−1](z(k)\ni ) · (z(k)\ni\n−bµ(k)[t]\n2\n)(z(k)\ni\n−bµ(k)[t]\n2\n)⊤o\n8\nend\n9\n{ bβ(k)[t]}K\nk=1, β\n[t] =\narg min\nβ(1),...,β(K),β\n\u001a PK\nk=1 nk\nh\n1\n2(β(k))⊤bΣ(k)[t]β(k) −(β(k))⊤(bµ(k)[t]\n2\n−\nbµ(k)[t]\n1\n)\ni\n+ PK\nk=1\n√nkλ[t] · ∥β(k) −β∥2\n\u001b\n// Aggregation to learn { bβ(k)[t]}K\nk=1\n10\nfor k = 1 to K do\n// Local update for each task\n11\nbδ(k)[t] = 1\n2( bβ(k)[t])⊤(bµ(k)[t]\n1\n+ bµ(k)[t]\n2\n)\n12\nLet bθ(k)[t] = ( bw(k)[t], bβ(k)[t], bδ(k)[t])\n13\nend\n14 end\nOutput: {(bθ(k)[T], bµ(k)[T]\n1\n, bµ(k)[T]\n2\n, bΣ(k)[T])}K\nk=1 with bθ(k)[T] = ( bw(k)[T], bβ(k)[T], bδ(k)[T]),\nand β\n[T]\n2.3\nTheory\nIn this section, we develop statistical theories for our proposed procedure MTL-GMM (see\nAlgorithm 1). As mentioned in Section 2.1, we are interested in the performance of both\n13\nparameter estimation and clustering, although the latter is the main focus and motivation.\nFirst, we impose conditions in the following assumption set.\nAssumption 1. Denote ∆(k) =\nq\n(µ(k)∗\n1\n−µ(k)∗\n2\n)⊤(Σ(k)∗)−1(µ(k)∗\n1\n−µ(k)∗\n2\n) for k ∈S. The\nquantity ∆(k) is the Mahalanobis distance between µ(k)∗\n1\nand µ(k)∗\n2\nwith covariance matrix\nΣ(k)∗, and can be viewed as the signal-to-noise ratio (SNR) in the k-th task (Anderson,\n1958). Suppose the following conditions hold:\n(i) nS = P\nk∈S nk ≥C1|S| maxk=1:K nk with a constant C1 ∈(0, 1];\n(ii) mink∈S nk ≥C2(p + log K) with some constant C2 > 0;\n(iii) Either of the following two conditions holds with some constant C3 > 0:\n(a) maxk∈S\n\u0000∥bβ(k)[0]−β(k)∗∥2∨∥bµ(k)[0]\n1\n−µ(k)∗\n1\n∥2∨∥bµ(k)[0]\n2\n−µ(k)∗\n2\n∥2\n\u0001\n≤C3 mink∈S ∆(k),\nmaxk∈S | bw(k)[0] −w(k)∗| ≤cw/2;\n(b) maxk∈S\n\u0000∥bβ(k)[0]+β(k)∗∥2∨∥bµ(k)[0]\n1\n−µ(k)∗\n2\n∥2∨∥bµ(k)[0]\n2\n−µ(k)∗\n1\n∥2\n\u0001\n≤C3 mink∈S ∆(k),\nmaxk∈S |1 −bw(k)[0] −w(k)∗| ≤cw/2.\n(iv) mink∈S ∆(k) ≥C4 > 0 with some constant C4 > 0;\nRemark 1. These are common and mild conditions related to the sample size, initializa-\ntion, and signal-to-noise ratio of GMMs. Condition (i) requires the maximum sample size\nof all tasks not to be much larger than the average sample size of tasks in S. Similar con-\nditions can be found in Duan and Wang (2023). Condition (ii) is the requirement of the\nsample size of tasks in S. The usual condition for low-dimensional single-task learning is\nnk ≳p (Cai et al., 2019). The additional log K term arises from the simultaneous control\nof performance on all tasks in S, where S can be as large as 1 : K. Condition (iii) re-\nquires that the initialization should not be too far away from the truth, which is commonly\nassumed in either the analysis of EM algorithm (Redner and Walker, 1984; Balakrishnan\net al., 2017; Cai et al., 2019) or other iterative algorithms like the local estimation used\n14\nin semi-parametric models (Carroll et al., 1997; Li and Liang, 2008) and adaptive Lasso\n(Zou, 2006). The two possible forms considered in this condition are due to the fact that\nbinary GMM is only identifiable up to label permutation. Condition (iv) requires that the\nsignal strength of GMM (in terms of Mahalanobis distance) is strong enough, which is usu-\nally assumed in the literature about the likelihood-based methods of GMMs (Dasgupta and\nSchulman, 2013; Azizyan et al., 2013; Balakrishnan et al., 2017; Cai et al., 2019).\nWe first establish the rate of convergence for the estimation. Recalling the parameter\nspace ΘS(h) in (3), let us denote the true parameter by\n{θ\n(k)∗}k∈S = {(w(k)∗, µ(k)∗\n1\n, µ(k)∗\n2\n, Σ(k)∗)}k∈S ∈ΘS(h).\nTo better present the results for parameters related to the optimal discriminant rule (1),\nwe further denote\nθ(k)∗= (w(k)∗, β(k)∗, δ(k)∗),\n∀k ∈S,\nwhere β(k)∗= (Σ(k)∗)−1(µ(k)∗\n2\n−µ(k)∗\n1\n), δ(k)∗= 1\n2(β(k)∗)⊤(µ(k)∗\n1\n+ µ(k)∗\n2\n). Note that θ(k)∗is\na function of θ\n(k)∗. For the estimators returned by MTL-GMM (see Algorithm 1), we are\nparticularly interested in the following two error metrics:\nd(bθ(k)[T], θ(k)∗) := min{| bw(k)[T] −w(k)∗| ∨∥bβ(k)[T] −β(k)∗∥2 ∨|bδ(k)[T] −δ(k)∗|,\n|1 −bw(k)[T] −w(k)∗| ∨∥bβ(k)[T] + β(k)∗∥2 ∨|bδ(k)[T] + δ(k)∗|},\n(4)\n\u0010\nmin\nπ:[2]→[2] max\nr=1:2 ∥bµ(k)[T]\nr\n−µ(k)∗\nπ(r)∥2\n\u0011\n∨∥bΣ(k)[T] −Σ(k)∗∥2,\n(5)\nwhere π : [2] →[2] is a permutation on {1, 2}. Again, we take the minimum above because\nbinary GMM is identifiable up to label permutation. The first error metric d(bθ(k)[T], θ(k)∗)\ninvolves the error for discriminant coefficients and is closely related to the clustering perfor-\nmance. It reveals how well our method utilizes similarity structure in multi-task learning.\nThe second error metric is about the mean vectors and covariance matrix. As discussed in\nSection 2.1, we shall not expect it to be improved compared to that in single-task learning,\nas these parameters are not necessarily similar.\n15\nWe are ready to present upper bounds for the estimation error of MTL-GMM. We recall\nthat ΘS(h) and QS are the parameter space and probability measure that we use in Section\n2.1 to describe the data distributions for tasks in S and Sc, respectively.\nTheorem 1. (Upper bounds of the estimation error of GMM parameters for MTL-GMM)\nSuppose Assumption 1 holds for some S with |S| ≥s and ϵ :=\nK−s\nK\n< 1/3. Let λ[0] ≥\nC1 maxk=1:K √nk, Cλ ≥C1 and κ > C2 with some constants C1 > 0, C2 ∈(0, 1) 2. Then\nthere exist a constant C3 > 0, such that for any {θ\n(k)∗}k∈S = {(w(k)∗, µ(k)∗\n1\n, µ(k)∗\n2\n, Σ(k)∗)}k∈S ∈\nΘS(h) and any probability measure QS on (Rp)⊗nSc, with probability 1 −C3K−1, the fol-\nlowing hold for all k ∈S:\nd(bθ(k)[T], θ(k)∗) ≲\nr p\nnS\n+\nr\nlog K\nnk\n+ h ∧\nr\np + log K\nnk\n+ ϵ\nr\np + log K\nmaxk=1:K nk\n+ T 2(κ′)⊤,\n\u0010\nmin\nπ:[2]→[2] max\nr=1:2 ∥bµ(k)[T]\nr\n−µ(k)∗\nπ(r)∥2\n\u0011\n∨∥bΣ(k)[T] −Σ(k)∗∥2 ≲\nr\np + log K\nnk\n+ T 2(κ′)⊤,\nwhere κ′ ∈(0, 1) is some constant and nS = P\nk∈S nk. When T ≥C log(maxk=1:K nk) with\na large constant C > 0, the last term on the right-hand side will be dominated by other\nterms in both inequalities.\nThe upper bound of\n\u0000minπ:[2]→[2] maxr=1:2 ∥bµ(k)[T]\nr\n−µ(k)∗\nπ(r)∥2\n\u0001\n∨∥bΣ(k)[T]−Σ(k)∗∥2 contains\ntwo parts. The first part is comparable to the single-task learning rate (Cai et al., 2019)\n(up to a √log K term due to the simultaneous control over all tasks in S), and the second\npart characterizes the geometric convergence of iterates. As expected, since µ(k)∗\n1\n, µ(k)∗\n2\n,\nΣ(k)∗in S are not necessarily similar, an improved error rate over single-task learning\nis generally impossible.\nThe upper bound for d(bθ(k)[T], θ(k)∗) is directly related to the\nclustering performance of our method. Thus we will provide a detailed discussion about it\nafter presenting the clustering result in the next theorem.\nAs introduced in Section 1.1, using the estimate bθ(k)[T] = ( bw(k)[T], bβ(k)[T], bδ(k)[T]) from\n2C1 and C2 depend on the constants M, cw, and cΣ etc.\n16\nAlgorithm 1, we can construct a classifier for task k as\nbC(k)[T](z) =\n\n\n\n\n\n\n\n1,\nif ( bβ(k)[T])⊤z −bδ(k)[T] ≤log\n\u0010\n1−bw(k)[T ]\nbw(k)[T ]\n\u0011\n;\n2,\notherwise.\n(6)\nRecall that for a clustering method C : Rp →{1, 2}, its mis-clustering error rate under\nGMM with parameter θ = (w, µ1, µ2, Σ) is\nRθ(C) =\nmin\nπ:[2]→[2] Pθ(C(Znew) ̸= π(Y new)),\n(7)\nwhere Znew ∼(1 −w)N(µ1, Σ) + wN(µ2, Σ) is a future observation associated with the\nlabel Y new, independent from C; the probability Pθ is w.r.t. (Znew, Y new), and the minimum\nis taken over two permutation functions on {1, 2}. Denote Cθ as the Bayes classifier that\nminimizes Rθ(C).\nIn the following theorem, we obtain the upper bound of the excess\nmis-clustering error of bC(k)[T] for k ∈S.\nTheorem 2. (Upper bound of the excess mis-clustering error for MTL-GMM) Suppose\nthe same conditions in Theorem 1 hold. Then there exist a constant C1 > 0 such that\nfor any {θ\n(k)∗}k∈S ∈ΘS(h) and any probability measure QS on (Rp)⊗nSc, with probability\n1 −C1K−1, the following holds for all k ∈S:\nRθ\n(k)∗( bC(k)[T]) −Rθ\n(k)∗(Cθ\n(k)∗) ≲d2(bθ(k)[T], θ(k)∗)\n≲\np\nnS\n|{z}\n(I)\n+ log K\nnk\n| {z }\n(II)\n+ h2 ∧p + log K\nnk\n|\n{z\n}\n(III)\n+ ϵ2 p + log K\nmaxk=1:K nk\n|\n{z\n}\n(IV)\n+ T 4(κ′)2T\n|\n{z\n}\n(V)\n,\nwith some κ′ ∈(0, 1). When T ≥C log(maxk=1:K nk) with a large constant C > 0, the last\nterm on the right-hand side will be dominated by the second term.\nThe upper bounds of d(bθ(k)[T], θ(k)∗) in Theorem 1 and Rθ\n(k)∗( bC(k)[T]) −Rθ\n(k)∗(Cθ\n(k)∗) in\nTheorem 2 consist of five parts with one-to-one correspondence. It is sufficient to discuss\nthe bound of Rθ\n(k)∗( bC(k)[T]) −Rθ\n(k)∗(Cθ\n(k)∗). Part (I) represents the “oracle rate”, which can\n17\nbe achieved when all tasks in S are the same. This is the best rate to possibly achieve.\nPart (II) is a dimension-free error caused by estimating scalar parameters δ(k)∗and w(k)∗\nthat appears in the optimal discriminant rule. Part (III) includes h that measures the\ndegree of similarity among the tasks in S. When these tasks are very similar, h will be\nsmall, contributing a small term to the upper bound. Nicely, even when h is large, the term\nbecomes p+log K\nnk\nand it is still comparable to the minimax error rate of single-task learning\nOP(p/nk) (e.g., Theorems 4.1 and 4.2 in Cai et al. (2019)). We have the extra log K term\nhere due to the simultaneous control over all tasks in S. Part (IV) quantifies the influence\nfrom the outlier tasks in Sc.\nWhen there are more outlier tasks, ϵ increases, and the\nbound becomes worse. On the other hand, as long as ϵ is small enough to make this term\ndominated by any other part, the error rate induced by outlier tasks becomes negligible.\nGiven that data from outlier tasks can be arbitrarily contaminated, we can conclude that\nour method is robust against a fraction of outlier tasks from arbitrary sources. The term\nin Part (V) decreases geometrically in the iteration number T, implying that the iterates\nin Algorithm 1 converge geometrically to a ball of radius determined by the errors from\nParts (I)-(IV).\nAfter explaining each part of the upper bound, we now compare it with the convergence\nrate OP(p+log K\nnk\n) (including log K here since we consider all the tasks simultaneously) in\nthe single-task learning and reveal how our method performs. With a quick inspection, we\ncan conclude the following:\n• The rate of the upper bound is never larger than p+log K\nnk\n. So, in terms of rate of\nconvergence, our method MTL-GMM performs at least as well as single-task learning,\nregardless of the similarity level h and outlier task fraction ϵ.\n• When nS ≫nk (large total sample size for tasks in S), p increases with nk (di-\nverging dimension), h ≪\nq\np+log K\nnk\n(sufficient similarity between tasks in S), and\nϵ ≪\np\n(maxk=1:K nk)/nk (small fraction of outlier tasks), MTL-GMM attains a faster\n18\nexcess mis-clustering error rate and improves over single-task learning.\nThe preceding discussions on the upper bounds have demonstrated the superiority of\nour method. But can we do better? To further evaluate the upper bounds of our method,\nwe next derive complementary minimax lower bounds for both estimation error and excess\nmis-clustering error. We will show that our method is (nearly) minimax rate optimal in a\nbroad range of regimes.\nTheorem 3. (Lower bounds of the estimation error of GMM parameters in multi-task\nlearning) Suppose ϵ = K−s\nK\n< 1/3. Suppose there exists a subset S with |S| ≥s such that\nmink∈S nk ≥C1(p + log K) and mink∈S ∆(k) ≥C2, where C1, C2 > 0 are some constants.\nThen\ninf\n{bθ(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS(h)\nQS\nP\n [\nk∈S\n(\nd(bθ(k), θ(k)∗) ≳\nr p\nnS\n+\nr\nlog K\nnk\n+ h ∧\nr\np + log K\nnk\n+\nϵ\n√maxk=1:K nk\n)!\n≥1\n10,\ninf\n{bµ(k)\n1\n,bµ(k)\n2\n}K\nk=1\n{bΣ(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS(h)\nQS\nP\n [\nk∈S\n(\u0010\nmin\nπ:[2]→[2] max\nr=1:2 ∥bµ(k)\nr\n−µ(k)∗\nπ(r)∥2\n\u0011\n∨∥bΣ(k) −Σ(k)∗∥2\n≳\nr\np + log K\nnk\n)!\n≥1\n10.\nTheorem 4. (Lower bound of the excess mis-clustering error in multi-task learning) Sup-\npose the same conditions in Theorem 3 hold. Then\ninf\n{ bC(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS(h)\nQS\nP\n [\nk∈S\n(\nRθ\n(k)∗( bC(k)) −Rθ\n(k)∗(Cθ\n(k)∗) ≳p\nnS\n+ log K\nnk\n+ h2 ∧p + log K\nnk\n+\nϵ2\nmaxk=1:K nk\n)!\n≥1\n10.\n19\nComparing the upper and lower bounds in Theorems 1-4, we make several remarks:\n• Regarding the estimation of mean vectors {µ(k)∗\n1\n, µ(k)∗\n2\n}k∈S and covariance matrices\n{Σ(k)∗}k∈S, the upper and lower bounds match, hence our method is minimax rate\noptimal.\n• For the estimation error d(bθ(k), θ(k)∗) and excess mis-clustering error Rθ\n(k)∗( bC(k)[T]) −\nRθ\n(k)∗(Cθ\n(k)∗) with k ∈S, the first three terms in the upper and lower bounds match.\nOnly the term involving ϵ in the lower bound differs from that in the upper bound by\na factor √p + log K or p + log K. As a result, in the classical low-dimensional regime\nwhere p is bounded, the upper and lower bounds match (up to a logarithmic factor).\nTherefore, our method is (nearly) minimax rate optimal for estimating {θ(k)∗}k∈S\nand clustering in such a classical regime.\n• When the dimension p diverges, there might exist a non-negligible gap between the\nupper and lower bounds for d(bθ(k), θ(k)∗) and Rθ\n(k)∗( bC(k)[T])−Rθ\n(k)∗(Cθ\n(k)∗) with k ∈S.\nNevertheless, this only occurs when the fraction of outlier task ϵ is above the threshold\nq\nmaxk=1:K nk\np+log K\n\u0000h2 ∨log K\nnk\n∨\np\nnS\n\u0001\n.\nBelow the threshold, our method remains minimax\nrate optimal even though p is unbounded.\n• Does the gap, when it exists, arise from the upper bound or the lower bound? We\nbelieve that it is the upper bound that sometimes becomes not sharp. As can be seen\nfrom the proof of Theorem 1, the term ϵ\nq\np+log K\nmaxk=1:K nk is due to the estimation of those\n“center” parameters in Algorithm 1. Recent advances in robust statistics (Chen et al.,\n2018) have shown that estimators based on statistical depth functions such as Tukey’s\ndepth function (Tukey, 1975) can achieve optimal minimax rate under Huber’s ϵ-\ncontamination model for location and covariance estimation. It might be possible to\nutilize depth functions to estimate “center” parameters in our problem and kill the\nfactor √p in the upper bound. We leave a rigorous development of optimal robustness\n20\nas an interesting future research. On the other hand, such statistical improvement\nmay come with expensive computation, as depth function-based estimation typically\nrequires solving a challenging non-convex optimization problem.\n2.4\nInitialization and cluster alignment\nAs specified by Condition (iii) in Assumption 1, our proposed learning procedure requires\nthat for each task in S, initial values of the GMM parameter estimates lie within a distance\nof SNR-order from the ground truth. This can be satisfied by the method of moments pro-\nposed in Ge et al. (2015). In practice, a natural initialization method is to run the standard\nEM algorithm or other common clustering methods like k-means on each task and use the\ncorresponding estimate as the initial values. We adopted the standard EM algorithm in our\nnumerical experiments, and the numerical results in Section 3 and supplements showed that\nthis practical initialization works quite well. However, in the context of multi-task learning,\nCondition (iii) further requires a correct alignment of those good initializations from each\ntask, owing to the non-identifiability of GMMs. We discuss in detail the alignment issue\nin Section 2.4.1 and propose two algorithms to resolve this issue in Section 2.4.2.\n2.4.1\nThe alignment issue\nRecall that Section 2.1 introduces the binary GMM with parameters (w(k)∗, µ(k)∗\n1\n, µ(k)∗\n2\n, Σ(k)∗)\nfor each task k ∈S. Because the two sets of parameter values {(w, u, v, Σ), (1−w, v, u, Σ)}\nfor (w(k)∗, µ(k)∗\n1\n, µ(k)∗\n2\n, Σ(k)∗) index the same distribution, a good initialization close to the\ntruth is up to a permutation of the two cluster labels. The permutations in the initialization\nof different tasks could be different. Therefore, in light of the joint parameter space ΘS(h)\ndefined in (3) and Condition (iii) in Assumption 1, for given initializations from different\ntasks, we may need to permute their cluster labels to feed the well-aligned initialization\ninto Algorithm 1.\n21\nWe further elaborate on the alignment issue using Algorithm 1. The penalization in\nStep 9 aims to push the estimators bβ(k)[t]’s with different k towards each other, which\nis expected to improve the performance thanks to the similarity among underlying true\nparameters {β(k)∗}k∈S. However, due to the potential permutation of two cluster labels, the\nvanilla single-task initializations (without alignment) cannot guarantee that the estimators\n{ bβ(k)[t]}k∈S at each iteration are all estimating the corresponding β(k)∗’s (some may estimate\n−β(k)∗’s).\nFigure 1: Examples of well-aligned (left) and badly-aligned (right) initializations.\nFigure 1 illustrates the alignment issue in the case of two tasks. The left-hand-side\nsituation is ideal where bβ(1)[0], bβ(2)[0] are estimates of β(1)∗, β(2)∗(which are similar). The\nright-hand-side situation is problematic because bβ(1)[0], bβ(2)[0] are estimates of β(1)∗, −β(2)∗\n(which are not similar).\nTherefore, in practice, after obtaining the initializations from\neach task, it is necessary to align their cluster labels to ensure that estimators of similar\nparameters are correctly put together in the penalization framework in Algorithm 1. We\nformalize the problem and provide two solutions in the next subsection.\n22\n2.4.2\nTwo alignment algorithms\nSuppose { bβ(k)[0]}K\nk=1 are the initial estimates of discriminant coefficients with potentially\nbad alignment for k ∈S. Note that a good initialization and alignment is not required\n(in fact, it is not even well defined) for the outlier tasks in Sc, because they can be from\narbitrary distributions. However, since S is unknown, we will have to address the alignment\nissue for tasks in S based on initial estimates from all the tasks.\nFor binary GMMs,\neach alignment of { bβ(k)[0]}K\nk=1 can be represented by a K-dimensional Rademacher vector\nr ∈{±1}K. Define the ideal alignment as r∗\nk = arg minrk=±1 ∥rk bβ(k)[0] −β(k)∗∥2, k ∈S.\nThe goal is to recover the well-aligned initializers {r∗\nk bβ(k)[0]}k∈S from the initial estimates\n{ bβ(k)[0]}K\nk=1 (equivalently, to recover {r∗\nk}k∈S), which can then be fed into Algorithm 1. Once\n{ bβ(k)[0]}k∈S are well aligned, other initial estimates in Algorithm 1 will be automatically\nwell aligned.\nIn the following, we will introduce two alignment algorithms.\nThe first one is the\n“exhaustive search” method (Algorithm 2), where we search among all possible alignments\nto find the best one. The second one is the “greedy search” method (Algorithm 3), where we\nflip the sign of bβ(k)[0] in a greedy way to recover {r∗\nk bβ(k)[0]}k∈S. Both methods are proved\nto recover {r∗\nk}k∈S under mild conditions.\nThe conditions required by the “exhaustive\nsearch” method are slightly weaker than those required by the “greedy search” method.\nAs for computational complexity, the latter enjoys a linear time complexity O(K), while\nthe former suffers from an exponential time complexity O(2K) due to optimization over all\npossible 2K alignments.\nTo this end, for a given alignment r = {rk}K\nk=1 ∈{±1}K with the correspondingly\naligned estimates {rk bβ(k)[0]}K\nk=1, define its alignment score as\nscore(r) =\nX\n1≤k1̸=k2≤K\n∥rk1 bβ(k1)[0] −rk2 bβ(k2)[0]∥2.\nThe intuition is that as long as the initializations { bβ(k)[0]}k∈S are close to the ground\n23\ntruth, a smaller score indicates less difference among {rk bβ(k)[0]}k∈S, which implies a better\nalignment. The score can be thus used to evaluate the quality of an alignment. Note that\nthe score is defined in a symmetric way, that is, score(r) = score(−r). The exhaustive\nsearch algorithm is presented in Algorithm 2, where scores of all alignments are calculated,\nand the alignment that minimizes the score is output. Since the score is symmetric, there\nare at least two alignments with the minimum score. The algorithm can arbitrarily choose\nand output one of them.\nAlgorithm 2: Exhaustive search for the alignment\nInput: Initialization { bβ(k)[0]}K\nk=1\n1 br ←arg minr∈{±1}K score(r)\nOutput: br\nThe following theorem reveals that the exhaustive search algorithm can successfully\nfind the ideal alignment under mild conditions.\nTheorem 5 (Alignment correctness for Algorithm 2). Assume that\n(i) ϵ < 1\n3;\n(ii) mink∈S ∥β(k)∗∥2 ≥4(1−ϵ)\n1−3ϵ h + 2(2−ϵ)\n1−3ϵ maxk∈S\n\u0000∥bβ(k)[0] −β(k)∗∥2 ∧∥bβ(k)[0] + β(k)∗∥2\n\u0001\n,\nwhere ϵ =\nK−|S|\nK\nis the outlier task proportion introduced in Theorem 1, and h is the\nsimilarity level of discriminant coefficient defined in (3). Then the output of Algorithm 2\nsatisfies\nbrk = r∗\nk for all k ∈S\nor\nbrk = −r∗\nk for all k ∈S\nRemark 2. The conditions imposed in Theorem 5 are no stronger than conditions re-\nquired by Theorem 1. First of all, Condition (i) is also required in Theorem 1. Moreover,\nfrom the definition of h in (3), it is bounded by a constant. This together with Conditions\n(iii) and (iv) in Assumption 1 implies Condition (ii) in Theorem 5.\n24\nRemark 3. With Theorem 5, we can relax the original Condition (iii) in Assumption 1 to\nthe following condition:\nFor all k ∈S, either of the following two conditions holds with a sufficiently small\nconstant C3:\n(a) ∥bβ(k)[0]−β(k)∗∥2∨∥bµ(k)[0]\n1\n−µ(k)∗\n1\n∥2∨∥bµ(k)[0]\n2\n−µ(k)∗\n2\n∥2 ≤C3 mink∈S ∆(k), | bw(k)[0]−w(k)∗| ≤\ncw/2;\n(b) ∥bβ(k)[0] + β(k)∗∥2 ∨∥bµ(k)[0]\n1\n−µ(k)∗\n2\n∥2 ∨∥bµ(k)[0]\n2\n−µ(k)∗\n1\n∥2 ≤C3 mink∈S ∆(k), |1 −bw(k)[0] −\nw(k)∗| ≤cw/2.\nIn the relaxed version, the initialization for each task only needs to be good up to an ar-\nbitrary permutation, while in the original version, the initialization for each task needs to\nbe good under the same permutation.\nNext, we would like to introduce the second alignment algorithm, the “greedy search”\nmethod, summarized in Algorithm 3. The main idea is to flip the sign of the discriminant\ncoefficient estimates (equivalently, swap the two cluster labels) from K tasks in a sequential\nfashion to check whether the alignment score decreases or not. If yes, we keep the alignment\nafter the flip and proceed with the next task. Otherwise, we keep the alignment before the\nflip and proceed with the next task. A surprising fact of Algorithm 3 is that it is sufficient\nto iterate this procedure for all tasks just once to recover the ideal alignment, making the\nalgorithm computationally efficient.\nTo help state the theory of the greedy search algorithm, we define the “mismatch\nproportion” of { bβ(k)[0]}k∈S as\npa = min{#{k ∈S : r∗\nk = 1}, #{k ∈S : r∗\nk = −1}}/|S|\nIntuitively, pa represents the level of mismatch between the initial alignment and the ideal\none. It’s straightforward to verify that pa ∈[0, 1/2]; pa = 0 means the initial alignment\n25\nAlgorithm 3: Greedy search for the alignment\nInput: Initialization { bβ(k)[0]}K\nk=1\n1 br = (1, . . . , 1) ∈{±1}K\n2 for k = 1 to K do\n3\ner ←flip the sign of brk in br\n4\nif score(br) > score(er) then\n5\nbr ←er\n6\nend\n7 end\nOutput: br\nequals the ideal one, while pa = 1/2 (or\n|S|−1\n2|S|\nwhen |S| is odd) represents the “worst”\nalignment, where almost half of the tasks are badly-aligned. The smaller pa is, the better\nalignment { bβ(k)[0]}k∈S is. Note that we only care about the alignment of tasks in S.\nThe following theorem shows that the greedy search algorithm can succeed in finding\nthe ideal alignment under mild conditions.\nTheorem 6 (Alignment correctness for Algorithm 3). Assume that\n(i) ϵ < 1\n2;\n(ii) mink∈S ∥β(k)∗∥2 ≥\n2(1−ϵ)\n2(1−ϵ)(1−pa)−1h +\n2−ϵ\n2(1−ϵ)(1−pa)−1 maxk∈S\n\u0000∥bβ(k)[0] −β(k)∗∥2 ∧∥bβ(k)[0] +\nβ(k)∗∥2\n\u0001\n;\n(iii) pa <\n1−2ϵ\n2(1−ϵ),\nwhere ϵ and h are the same as in Theorem 5. Then the output of Algorithm 3 satisfies\nrk = r∗\nk for all k ∈S\nor\nrk = −r∗\nk for all k ∈S\nRemark 4. Conditions (i) and (ii) required by Theorem 6 are similar to the requirements\nin Theorem 5, which have been shown to be no stronger than conditions in Assumption 1\n26\nand Theorem 1 (See Remark 2). However, Condition (iii) is an additional requirement for\nthe success of the greedy label-swapping algorithm. The intuition is that in the exhaustive\nsearch algorithm, we compare the scores of all alignments and only need to ensure the ideal\nalignment can defeat the badly-aligned ones in terms of the alignment score. In contrast, the\nsuccess of the greedy search algorithm relies on the correct move at each step. We need to\nguarantee that the “better” alignment after the swap (which may still be badly aligned) can\noutperform the “worse” one before the swap. This is more difficult to satisfy. Hence, more\nconditions are needed for the success of Algorithm 3. Condition (iii) is one such condition\nto provide a reasonably good initial alignment to start the greedy search process.\nMore\ndetails of the analysis can be found in the proofs of Theorems 5 and 6 in the supplements.\nRemark 5. In practice, Condition (iii) can fail to hold with a non-zero probability. One\nsolution is to start with random alignments, run the greedy search algorithm multiple times,\nand use the alignment that appears most frequently in the output. Nevertheless, this will\nincrease the computational burden. In our numerical studies, Algorithm 3 without multiple\nrandom alignments works well.\nOne appealing feature of the two alignment algorithms is that they are robust against\na fraction of outlier tasks from arbitrary distributions. According to the definition of the\nalignment score, this may appear impossible at first glance because the score depends on\nthe estimators from all tasks.\nHowever, it turns out that the impact of outliers when\ncomparing the scores in Algorithm 2 and 3 can be bounded by parameters and constants\nthat are unrelated to outlier tasks via the triangle inequality of Euclidean norms. The key\nidea is that the alignment of outlier tasks in Sc does not matter in Theorems 5 and 6. More\ndetails can be found in the proof of Theorems 5 and 6 in the supplementary materials.\nIn contrast with supervised MTL, the alignment issue commonly exists in unsupervised\nMTL. It generally occurs when aggregating information (up to latent label permutation)\nacross different tasks. Alignment pre-processing is thus necessary and important. However,\n27\nto our knowledge, there is no formal discussion regarding alignment in the existing literature\nof unsupervised MTL (Gu et al., 2011; Zhang and Zhang, 2011; Yang et al., 2014; Zhang\net al., 2018; Dieuleveut et al., 2021; Marfoq et al., 2021). Our treatment of alignment in\nSection 2.4 is an important step forward in this field. Our algorithms can be potentially\nextended to other unsupervised MTL scenarios and we leave it for future studies.\n3\nSimulations\nIn this section, we present a simulation study of our multi-task learning procedure MTL-\nGMM, i.e., Algorithms 1. The tuning parameter κ ∈(0, 1) is set as 1/3, and the value\nof Cλ is determined by a 10-fold cross-validation based on the log-likelihood of the final\nfitted model. The candidates of Cλ are chosen in a data-driven way, which is described\nin detail in Section S.3.1.5 of the supplements. All the experiments in this section are\nimplemented in R. Function Mcluster in R package mclust is called to fit a single GMM.\nWe also conducted two additional simulation studies and two real-data studies. Due to the\npage limit, we included these in Section S.3 of the supplementary materials.\nWe consider a binary GMM setting. There are K = 10 tasks of which each has sample\nsize nk = 100 and dimension p = 15. When k ∈S, we generate each w(k)∗from Unif(0.1, 0.9)\nand µ(k)∗\n1\nfrom (2, 2, 0p−2)⊤+ h/2 · (Σ(k)∗)−1u, where u ∼Unif({u ∈Rp : ∥u∥2 = 1}),\nΣ(k)∗= (0.2|i−j|)p×p, and let µ(k)∗\n2\n= −µ(k)∗\n1\n. When k /∈S, the distributions still follow\nGMM, but we generate each w(k)∗from Unif(0.2, 0.4) and µ(k)∗\n1\nfrom Unif({u ∈Rp : ∥u∥2 =\n0.5}), and let µ(k)∗\n2\n= −µ(k)∗\n1\n, Σ(k)∗= (0.5|i−j|))p×p. In this setup, it is clear that h quantifies\nthe similarity among tasks in S, and tasks in Sc have very distinct distributions and can\nbe viewed as outlier tasks. For a given ϵ ∈[0, 1), the outlier task index set Sc in each\nreplication is uniformly sampled from all subsets of 1 : K with cardinality Kϵ. We consider\ntwo cases:\n28\n(i) No outlier tasks (ϵ = 0), and h changes from 0 to 10 with increment 1;\n(ii) 2 outlier tasks (ϵ = 0.2), and h changes from 0 to 10 with increment 1;\nWe fit Single-task-GMM on each separate task, Pooled-GMM on the merged data of\nall tasks, and our MTL-GMM in Algorithm 1 coupled with the exhaustive search for the\nalignment in Algorithm 2. The performances of all three methods are evaluated by the\nestimation error of w(k)∗, µ(k)∗\n1\n, µ(k)∗\n2\n, β(k)∗, δ(k)∗, and Σ(k)∗, as well as the empirical mis-\nclustering error calculated on a test data set of size 500, for tasks in S. Due to page limit,\nwe only present the estimation error of β(k)∗and the mis-clustering error here, and leave\nthe others to Section S.3.1.1 of the supplements. These two errors are the maximum errors\nover tasks in S. For each setting, the simulation is replicated 200 times, and the average\nof the maximum errors together with the standard deviation are reported in Figure 2.\nWhen there are no outlier tasks, it can be seen that MTL-GMM and Pooled-GMM\nare competitive when h is small (i.e. the tasks are similar), and they outperform Single-\ntask-GMM. As h increases (i.e. tasks become more heterogenous), MTL-GMM starts to\noutperform Pooled-GMM by a large margin. Moreover, MTL-GMM is significantly better\nthan Single-task-GMM in terms of both estimation and mis-clustering errors over a wide\nrange of h. These comparisons demonstrate that MTL-GMM not only effectively utilizes\nthe unknown similarity structure among tasks, but also adapts to it. When the outlier\ntasks exist, even when h is very small, MTL-GMM still performs better than Pooled-GMM,\nshowing the robustness of MTL-GMM against a fraction of outlier tasks.\n4\nDiscussions\nWe would like to highlight several interesting open problems for potential future work:\n• What if only some clusters are similar among different tasks? This may be a more\nrealistic situation in particular when there are more than 2 clusters in each task. Our\n29\n5\n10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {β(k) ∗ }k∈S (without outlier tasks)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nMaximum mis−clustering error (without outlier tasks)\n2.5\n5.0\n7.5\n10.0\n12.5\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {β(k) ∗ }k∈S (with 2 outlier tasks)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nMaximum mis−clustering error (with 2 outlier tasks)\nmethod\nMTL−GMM\nPooled−GMM\nSingle−task−GMM\nFigure 2: The performance of different methods in Simulation 1 under different outlier\nproportions. The upper panel shows the performance without outlier tasks (ϵ = 0), and\nthe lower panel shows the performance with 2 outlier tasks (ϵ = 0.2). h changes from 0 to\n10 with increment 1. Estimation error of {β(k)∗}k∈S stands for maxk∈S(∥bβ(k)[T] −β(k)∗∥2 ∧\n∥bβ(k)[T] + β(k)∗∥2) and maximum mis-clustering error represents the maximum empirical\nmis-clustering error rate calculated on the test set of tasks in S.\ncurrent proposed algorithms may not work well because they do not take into account\nthis extra layer of heterogeneity. Furthermore, in this situation, different tasks may\nhave a different number of Gaussian clusters. Such a setting with various numbers\nof clusters has been considered in some literature on general unsupervised multi-task\nlearning (Zhang and Zhang, 2011; Yang et al., 2014; Zhang et al., 2018). It would be\nof great interest to develop multi-task and transfer learning methods with provable\nguarantees for GMMs under these more complicated settings.\n• How to accommodate heterogeneous covariance matrices for different Gaussian clus-\nters within each task? This is related to the quadratic discriminant analysis (QDA)\nin supervised learning where the Bayes classifier has a leading quadratic term. It\n30\nmay require more delicate analysis for methodological and theoretical development.\nSome recent QDA literature might be helpful (Li and Shao, 2015; Fan et al., 2015;\nHao et al., 2018; Jiang et al., 2018).\n• In this paper, we have focused on the pure unsupervised learning problem, where all\nthe samples are unlabeled. It would be interesting to consider the semi-supervised\nlearning setting, where labels in some tasks (or sources) are known. Li et al. (2022a)\ndiscusses a similar problem under the linear regression setting, but how the labeled\ndata can help the estimation and clustering in the context of GMMs remains unknown.\nReferences\nAnderson, T. W. (1958). An introduction to multivariate statistical analysis: Wiley series in\nprobability and mathematical statistics: Probability and mathematical statistics.\nAndo, R. K., Zhang, T., and Bartlett, P. (2005). A framework for learning predictive structures\nfrom multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(11).\nArgyriou, A., Evgeniou, T., and Pontil, M. (2008). Convex multi-task feature learning. Machine\nlearning, 73(3):243–272.\nAzizyan, M., Singh, A., and Wasserman, L. (2013). Minimax theory for high-dimensional gaussian\nmixtures with sparse mean separation. Advances in Neural Information Processing Systems,\n26.\nBalakrishnan, S., Wainwright, M. J., and Yu, B. (2017). Statistical guarantees for the em algo-\nrithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77–120.\nBastani, H. (2021). Predicting with proxies: Transfer learning in high dimension. Management\nScience, 67(5):2964–2984.\nBaum, L. E., Petrie, T., Soules, G., and Weiss, N. (1970). A maximization technique occurring in\nthe statistical analysis of probabilistic functions of markov chains. The Annals of Mathematical\nStatistics, 41(1):164–171.\nCai, T. and Liu, W. (2011). A direct estimation approach to sparse linear discriminant analysis.\nJournal of the American statistical association, 106(496):1566–1577.\nCai, T. T., Ma, J., and Zhang, L. (2019). Chime: Clustering of high-dimensional gaussian mixtures\nwith em algorithm and its optimality. The Annals of Statistics, 47(3):1234–1267.\nCarroll, R. J., Fan, J., Gijbels, I., and Wand, M. P. (1997). Generalized partially linear single-\nindex models. Journal of the American Statistical Association, 92(438):477–489.\nChattopadhyay, R., Sun, Q., Fan, W., Davidson, I., Panchanathan, S., and Ye, J. (2012). Multi-\nsource domain adaptation and its application to early detection of fatigue. ACM Transactions\non Knowledge Discovery from Data (TKDD), 6(4):1–26.\nChen, E. Y., Jordan, M. I., and Li, S. (2022).\nTransferred q-learning.\narXiv preprint\narXiv:2202.04709.\n31\nChen, M., Gao, C., and Ren, Z. (2018). Robust covariance and scatter matrix estimation under\nhuber’s contamination model. The Annals of Statistics, 46(5):1932–1960.\nDai, W., Yang, Q., Xue, G., and Yu, Y. (2007). Boosting for transfer learning. In ACM Interna-\ntional Conference Proceeding Series, volume 227, page 193.\nDai, W., Yang, Q., Xue, G.-R., and Yu, Y. (2008). Self-taught clustering. In Proceedings of the\n25th international conference on Machine learning, pages 200–207.\nDasgupta, S. and Schulman, L. (2013). A two-round variant of em for gaussian mixtures. arXiv\npreprint arXiv:1301.3850.\nDempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incomplete\ndata via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological),\n39(1):1–22.\nDieuleveut, A., Fort, G., Moulines, E., and Robin, G. (2021). Federated-em with heterogene-\nity mitigation and variance reduction. Advances in Neural Information Processing Systems,\n34:29553–29566.\nDuan, Y. and Wang, K. (2023). Adaptive and robust multi-task learning. The Annals of Statistics,\n51(5):2015–2039.\nEfron, B. (1975). The efficiency of logistic regression compared to normal discriminant analysis.\nJournal of the American Statistical Association, 70(352):892–898.\nEvgeniou, T. and Pontil, M. (2004).\nRegularized multi–task learning.\nIn Proceedings of the\ntenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages\n109–117.\nFan, J., Feng, Y., and Tong, X. (2012).\nA road to classification in high dimensional space:\nthe regularized optimal affine discriminant. Journal of the Royal Statistical Society: Series B\n(Statistical Methodology), 74(4):745–771.\nFan, Y., Kong, Y., Li, D., and Zheng, Z. (2015).\nInnovated interaction screening for high-\ndimensional nonlinear classification. The Annals of Statistics, 43(3):1243–1272.\nForgy, E. W. (1965). Cluster analysis of multivariate data: efficiency versus interpretability of\nclassifications. Biometrics, 21:768–769.\nGe, R., Huang, Q., and Kakade, S. M. (2015). Learning mixtures of gaussians in high dimensions.\nIn Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages\n761–770.\nGu, Q., Li, Z., and Han, J. (2011). Learning a kernel for multi-task clustering. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 25, pages 368–373.\nHao, N., Feng, Y., and Zhang, H. H. (2018). Model selection for high-dimensional quadratic\nregression via regularization. Journal of the American Statistical Association, 113(522):615–\n625.\nHartley, H. O. (1958).\nMaximum likelihood estimation from incomplete data.\nBiometrics,\n14(2):174–194.\nHasselblad, V. (1966). Estimation of parameters for a mixture of normal distributions. Techno-\nmetrics, 8(3):431–444.\nHastie, T., Tibshirani, R., Friedman, J. H., and Friedman, J. H. (2009). The elements of statistical\nlearning: data mining, inference, and prediction, volume 2. Springer.\n32\nHsu, D. and Kakade, S. M. (2013). Learning mixtures of spherical gaussians: moment methods\nand spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical\nComputer Science, pages 11–20.\nHuber, P. J. (1964). Robust estimation of a location parameter. The Annals of Mathematical\nStatistics, pages 73–101.\nJain, A. K. and Dubes, R. C. (1988). Algorithms for clustering data. Prentice-Hall, Inc.\nJalali, A., Sanghavi, S., Ruan, C., and Ravikumar, P. (2010).\nA dirty model for multi-task\nlearning. Advances in neural information processing systems, 23.\nJiang, B., Wang, X., and Leng, C. (2018). A direct approach for sparse quadratic discriminant\nanalysis. Journal of Machine Learning Research, 19(1):1098–1134.\nJin, J., Ke, Z. T., and Wang, W. (2017). Phase transitions for high dimensional clustering and\nrelated problems. The Annals of Statistics, 45(5):2151–2189.\nKalai, A. T., Moitra, A., and Valiant, G. (2010). Efficiently learning mixtures of two gaussians.\nIn Proceedings of the forty-second ACM symposium on Theory of computing, pages 553–562.\nKonstantinov, N., Frantar, E., Alistarh, D., and Lampert, C. (2020). On the sample complexity\nof adversarial multi-source pac learning. In International Conference on Machine Learning,\npages 5416–5425. PMLR.\nKwon, J. and Caramanis, C. (2020).\nThe em algorithm gives sample-optimality for learning\nmixtures of well-separated gaussians. In Conference on Learning Theory, pages 2425–2487.\nPMLR.\nLawrence, N. D. and Platt, J. C. (2004). Learning to learn with the informative vector machine.\nIn Proceedings of the twenty-first international conference on Machine learning, page 65.\nLee, K., Guillemot, L., Yue, Y., Kramer, M., and Champion, D. (2012).\nApplication of the\ngaussian mixture model in pulsar astronomy-pulsar classification and candidates ranking for\nthe fermi 2fgl catalogue. Monthly Notices of the Royal Astronomical Society, 424(4):2832–2840.\nLi, Q. and Shao, J. (2015). Sparse quadratic discriminant analysis for high dimensional data.\nStatistica Sinica, pages 457–473.\nLi, R. and Liang, H. (2008). Variable selection in semiparametric regression modeling. The Annals\nof Statistics, 36(1):261–286.\nLi, S., Cai, T., and Duan, R. (2023).\nTargeting underrepresented populations in precision\nmedicine: A federated transfer learning approach. The Annals of Applied Statistics, 17(4):2970–\n2992.\nLi, S., Cai, T. T., and Li, H. (2021). Transfer learning for high-dimensional linear regression:\nPrediction, estimation and minimax optimality. Journal of the Royal Statistical Society: Series\nB (Statistical Methodology), pages 1–25.\nLi, S., Cai, T. T., and Li, H. (2022a). Estimation and inference with proxy data and its genetic\napplications. arXiv preprint arXiv:2201.03727.\nLi, S., Cai, T. T., and Li, H. (2022b). Transfer learning in large-scale gaussian graphical models\nwith false discovery rate control. Journal of the American Statistical Association, pages 1–13.\nLi, W., Duan, L., Xu, D., and Tsang, I. W. (2013). Learning with augmented features for su-\npervised and semi-supervised heterogeneous domain adaptation. IEEE transactions on pattern\nanalysis and machine intelligence, 36(6):1134–1148.\n33\nLin, H. and Reimherr, M. (2022). On transfer learning in functional linear regression. arXiv\npreprint arXiv:2206.04277.\nMai, Q., Yang, Y., and Zou, H. (2019). Multiclass sparse discriminant analysis. Statistica Sinica,\n29(1):97–111.\nMai, Q., Zou, H., and Yuan, M. (2012). A direct approach to sparse discriminant analysis in\nultra-high dimensions. Biometrika, 99(1):29–42.\nMarfoq, O., Neglia, G., Bellet, A., Kameni, L., and Vidal, R. (2021).\nFederated multi-task\nlearning under a mixture of distributions. Advances in Neural Information Processing Systems,\n34:15434–15447.\nMcLachlan, G. J. and Krishnan, T. (2007). The EM algorithm and extensions. John Wiley &\nSons.\nMeng, X.-L. and Rubin, D. B. (1994). On the global and componentwise rates of convergence of\nthe em algorithm. Linear Algebra and its Applications, 199:413–425.\nMihalkova, L., Huynh, T., and Mooney, R. J. (2007). Mapping and revising markov logic networks\nfor transfer learning. In Proceedings of the 22nd national conference on Artificial intelligence-\nVolume 1, pages 608–614.\nMurtagh, F. and Contreras, P. (2012). Algorithms for hierarchical clustering: an overview. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge Discovery, 2(1):86–97.\nNg, A., Jordan, M., and Weiss, Y. (2001). On spectral clustering: Analysis and an algorithm.\nAdvances in neural information processing systems, 14.\nObozinski, G., Taskar, B., and Jordan, M. (2006). Multi-task feature selection. Statistics Depart-\nment, UC Berkeley, Tech. Rep, 2(2.2):2.\nPan, S. J. and Yang, Q. (2009). A survey on transfer learning. IEEE Transactions on knowledge\nand data engineering, 22(10):1345–1359.\nPearson, K. (1894). Contributions to the mathematical theory of evolution. Philosophical Trans-\nactions of the Royal Society of London. A, 185:71–110.\nRedner, R. A. and Walker, H. F. (1984). Mixture densities, maximum likelihood and the em\nalgorithm. SIAM review, 26(2):195–239.\nScott, A. J. and Symons, M. J. (1971). Clustering methods based on likelihood ratio criteria.\nBiometrics, pages 387–397.\nSundberg, R. (1974). Maximum likelihood theory for incomplete data from an exponential family.\nScandinavian Journal of Statistics, pages 49–58.\nThrun, S. and O’Sullivan, J. (1996). Discovering structure in multiple learning tasks: The tc\nalgorithm. In ICML, volume 96, pages 489–497.\nTian, Y. and Feng, Y. (2023). Transfer learning under high-dimensional generalized linear models.\nJournal of the American Statistical Association, 118(544):2684–2697.\nTukey, J. W. (1975). Mathematics and the picturing of data. In Proceedings of the International\nCongress of Mathematicians, Vancouver, 1975, volume 2, pages 523–531.\nVempala, S. and Wang, G. (2004). A spectral algorithm for learning mixture models. Journal of\nComputer and System Sciences, 68(4):841–860.\nWang, R., Zhou, J., Jiang, H., Han, S., Wang, L., Wang, D., and Chen, Y. (2021). A general\ntransfer learning-based gaussian mixture model for clustering. International Journal of Fuzzy\nSystems, 23(3):776–793.\n34\nWang, Z., Gu, Q., Ning, Y., and Liu, H. (2014). High dimensional expectation-maximization\nalgorithm: Statistical optimization and asymptotic normality. arXiv preprint arXiv:1412.8729.\nWang, Z., Song, Y., and Zhang, C. (2008).\nTransferred dimensionality reduction.\nIn Joint\nEuropean conference on machine learning and knowledge discovery in databases, pages 550–\n565. Springer.\nWitten, D. M. and Tibshirani, R. (2011). Penalized classification using fisher’s linear discriminant.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 73(5):753–772.\nWu, C. J. (1983). On the convergence properties of the em algorithm. The Annals of statistics,\npages 95–103.\nXu, J., Hsu, D. J., and Maleki, A. (2016).\nGlobal analysis of expectation maximization for\nmixtures of two gaussians. Advances in Neural Information Processing Systems, 29.\nXu, K. and Bastani, H. (2021). Learning across bandits in high dimension via robust statistics.\narXiv preprint arXiv:2112.14233.\nYan, B., Yin, M., and Sarkar, P. (2017). Convergence of gradient em on multi-component mixture\nof gaussians. Advances in Neural Information Processing Systems, 30.\nYang, M.-H. and Ahuja, N. (1998). Gaussian mixture model for human skin color and its appli-\ncations in image and video databases. In Storage and retrieval for image and video databases\nVII, volume 3656, pages 458–466. SPIE.\nYang, Y., Ma, Z., Yang, Y., Nie, F., and Shen, H. T. (2014). Multitask spectral clustering by\nexploring intertask correlation. IEEE transactions on cybernetics, 45(5):1083–1094.\nZhang, J. and Zhang, C. (2011). Multitask bregman clustering. Neurocomputing, 74(10):1720–\n1734.\nZhang, Q. and Chen, J. (2022).\nDistributed learning of finite gaussian mixtures.\nJournal of\nMachine Learning Research, 23(99):1–40.\nZhang, X., Blanchet, J., Ghosh, S., and Squillante, M. S. (2022). A class of geometric structures\nin transfer learning: Minimax bounds and optimality. In International Conference on Artificial\nIntelligence and Statistics, pages 3794–3820. PMLR.\nZhang, X., Zhang, X., and Liu, H. (2015). Smart multitask bregman clustering and multitask\nkernel clustering. ACM Transactions on Knowledge Discovery from Data (TKDD), 10(1):1–29.\nZhang, X., Zhang, X., Liu, H., and Luo, J. (2018). Multi-task clustering with model relation\nlearning. In Proceedings of the 27th International Joint Conference on Artificial Intelligence,\npages 3132–3140.\nZhang, Y. and Yang, Q. (2021). A survey on multi-task learning. IEEE Transactions on Knowledge\nand Data Engineering.\nZhao, R., Li, Y., and Sun, Y. (2020). Statistical convergence of the em algorithm on gaussian\nmixture models. Electronic Journal of Statistics, 14:632–660.\nZou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical\nAssociation, 101(476):1418–1429.\nZuo, H., Lu, J., Zhang, G., and Liu, F. (2018). Fuzzy transfer learning using an infinite gaussian\nmixture model and active learning. IEEE Transactions on Fuzzy Systems, 27(2):291–303.\n35\nSupplementary Materials of “Robust Unsupervised Multi-task\nand Transfer Learning on Gaussian Mixture Models”\nContents\nS.1\nExtension to Multi-cluster GMMs\n3\nS.1.1\nTheory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\nS.1.2\nAlignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\nS.2\nTransfer Learning\n11\nS.2.1\nProblem setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\nS.2.2\nMethod . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nS.2.3\nTheory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nS.2.4\nLabel alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nS.3\nAdditional Numerical Studies\n20\nS.3.1\nSimulations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nS.3.1.1\nSimulation 1 of MTL . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nS.3.1.2\nSimulation 2\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nS.3.1.3\nSimulation 3 of MTL . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nS.3.1.4\nSimulation of TL . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nS.3.1.5\nTuning parameters Cλ and Cλ0 in Algorithms 1, 4, and 7 . . . . .\n32\nS.3.1.6\nTuning parameter κ and κ0 in Algorithms 1, 4, and 7 . . . . . . .\n32\nS.3.2\nReal-data analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nS.3.2.1\nHuman activity recognition . . . . . . . . . . . . . . . . . . . . .\n34\nS.3.2.2\nPen-based recognition of handwritten digits (PRHD) . . . . . . .\n36\nS.4\nTechnical Lemmas\n38\nS.4.1\nGeneral lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nS.5\nProofs\n40\nS.5.1\nProofs of general lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nS.5.1.1\nProof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n1\nS.5.1.2\nProof of Lemma 4 . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\nS.5.1.3\nProof of Lemma 5 . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\nS.5.1.4\nProof of Lemma 6 . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\nS.5.2\nProof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\nS.5.2.1\nLemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nS.5.2.2\nMain proof of Theorem 1\n. . . . . . . . . . . . . . . . . . . . . .\n44\nS.5.2.3\nProofs of lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\nS.5.3\nProof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\nS.5.3.1\nLemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\nS.5.3.2\nMain proof of Theorem 3\n. . . . . . . . . . . . . . . . . . . . . .\n67\nS.5.3.3\nProofs of lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\nS.5.4\nProof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\nS.5.5\nProof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\nS.5.5.1\nLemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\nS.5.5.2\nMain proof of Theorem 4\n. . . . . . . . . . . . . . . . . . . . . .\n79\nS.5.5.3\nProof of lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\nS.5.6\nProof of Theorem 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\nS.5.7\nProof of Theorem 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\nS.5.8\nProof of Theorem 13\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\nS.5.8.1\nLemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\nS.5.8.2\nMain proof of Theorem 13 . . . . . . . . . . . . . . . . . . . . . .\n89\nS.5.8.3\nProof of lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\nS.5.9\nProof of Theorem 15\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\nS.5.9.1\nLemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\nS.5.9.2\nMain proof of Theorem 15 . . . . . . . . . . . . . . . . . . . . . .\n93\nS.5.9.3\nProof of lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\nS.5.10 Proof of Theorem 14\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n96\nS.5.11 Proof of Theorem 16\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\nS.5.11.1\nLemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\nS.5.11.2\nMain proof of Theorem 16 . . . . . . . . . . . . . . . . . . . . . .\n97\nS.5.11.3\nProof of lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n2\nS.5.12 Proof of Theorem 17\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n100\nS.5.13 Proof of Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n102\nS.5.13.1\nLemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n102\nS.5.13.2\nMain proof of Theorem 7\n. . . . . . . . . . . . . . . . . . . . . .\n103\nS.5.13.3\nProof of lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . .\n106\nS.5.14 Proof of Theorem 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n111\nS.5.15 Proof of Theorem 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n113\nS.5.16 Proof of Theorem 10\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n114\nS.5.16.1\nLemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n114\nS.5.16.2\nMain proof of Theorem 10 . . . . . . . . . . . . . . . . . . . . . .\n114\nS.5.16.3\nProof of lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . .\n114\nS.5.17 Proof of Theorem 11\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n116\nS.5.18 Proof of Theorem 12\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n122\nS.1\nExtension to Multi-cluster GMMs\nIn the main text, we have discussed the MTL problem for binary GMMs. In this section,\nwe extend our methods and theory to Gaussian mixtures with R clusters (R ≥3).\nWe first generalize the problem setting introduced in Sections 1.1 and 2.1. There are\nK tasks where we have nk observations {z(k)\ni }nk\ni=1 from the k-th task. An unknown subset\nS ⊆1 : K denotes tasks whose samples follow multi-cluster GMMs, and Sc refers to outlier\ntasks that can have arbitrary distributions. Specifically, for all k ∈S, i = 1 : nk,\ny(k)\ni\n= r with probability w(k)∗\nr\n,\nz(k)\ni |y(k)\ni\n= r ∼N(µ(k)∗\nr\n, Σ(k)∗),\nr = 1 : R,\nwith PR\nr=1 w(k)∗\nr\n= 1, and\n{z(k)\ni }i,k∈Sc ∼QS,\nwhere QS is some probability measure on (Rp)⊗nSc and nSc = P\nk∈Sc nk. We focus on the\nfollowing joint parameter space\nΘS(h) =\nn\n{θ\n(k)}k∈S = {({w(k)\nr }R\nr=1, {µ(k)\nr }R\nr=1, Σ(k))}k∈S : θ\n(k) ∈Θ, max\nr=2:R inf\nβ max\nk∈S ∥β(k)\nr −β∥2 ≤h\no\n,\n(S.1.8)\n3\nwhere β(k)\nr\n= (Σ(k))−1(µ(k)\nr\n−µ(k)\n1 ) is the r-th discriminant coefficient in the k-th task, and\nΘ is the parameter space for a single multi-cluster GMM,\nΘ =\n\u001a\nθ = ({wr}R\nr=1, {µr}R\nr=1, Σ) : max\nr=1:R ∥µr∥2 ≤M, cw ≤min\nr=1:R wr ≤max\nr=1:R wr ≤1 −cw,\nR\nX\nr=1\nwr = 1, c−1\nΣ ≤λmin(Σ) ≤λmax(Σ) ≤cΣ\n\u001b\n. (S.1.9)\nNote that (S.1.8) and (S.1.9) are natural generalizations of (3) and (2), respectively.\nUnder a multi-cluster GMM with parameter θ = ({wr}R\nr=1, {µr}R\nr=1, Σ), compared with\n(1), the optimal discriminant rule now becomes\nCθ(z) = arg max\nr=1:R\n\u001a\u0012\nz −µ1 + µr\n2\n\u0013⊤\nβr + log\n\u0012wr\nw1\n\u0013\u001b\n,\n(S.1.10)\nwhere βr = Σ−1(µr −µ1). Once we have the parameter estimators, we plug them into the\nabove rule to obtain the plug-in clustering method. Recall that for a clustering method\nC : Rp →[R], its mis-clustering error is\nRθ(C) =\nmin\nπ:[R]→[R] Pθ(C(Znew) ̸= π(Y new)).\nHere, Znew ∼PR\nr=1 wrN(µr, Σ) is an independent future observation associated with the\nlabel Y new, the probability Pθ is w.r.t.\n(Znew, Y new), and the minimum is taken over\nR! permutation functions on [R]. Since (S.1.10) is the optimal clustering method that\nminimizes Rθ(C), the excess mis-clustering error for a given clustering C is Rθ(C)−Rθ(Cθ).\nThe rest of the section aims to extend the EM-stylized multi-task learning procedure and\nthe two alignment algorithms in Section 2 to the general multi-cluster GMM setting, and\nprovide similar statistical guarantees in terms of estimation and excess mis-clustering errors.\nFor simplicity, throughout this section, we assume the number of clusters R to be bounded\nand known. We leave the case of diverging R as a future work.\nSince both the EM algorithm and the penalization framework work beyond binary\nGMM, our methodological idea described in Section 2.2 can be directly adapted to extend\nAlgorithm 1 to the multi-cluster case. We summarize the general procedure in Algorithm\n4. Like in Algorithm 1, we have adopted the following notation for posterior probability in\nAlgorithm 4,\nγ(r)\nθ (z) =\nwr exp(β⊤\nr z −δr)\nPR\nr=1 wr exp(β⊤\nr z −δr)\n,\nfor θ = ({wr}R\nr=2, {βr}R\nr=2, {δr}R\nr=2),\n4\nwhere w1 = 1 −PR\nr=2 wr, β1 := 0, and δ1 := 0. Specifically, γ(r)\nθ (z) is the posterior proba-\nbility P(Y = r|Z = z) given the observation z, when the true parameter of a multi-cluster\nGMM ({w∗\nr}R\nr=1, {µ∗\nr}R\nr=1, Σ∗) satisfies wr = w∗\nr, βr = (Σ∗)−1(µ∗\nr −µ∗\n1), δr = 1\n2β⊤\nr (µ∗\n1 + µ∗\nr),\nfor r = 1 : R.\nHaving the estimates ({ bw(k)[T]\nr\n}R\nr=1, { bβ(k)[T]\nr\n}R\nr=2, {bµ(k)[T]\nr\n}R\nr=1) from Algorithm 4, we can\nplug them into (S.1.10) to construct the clustering method, denoted by bC(k)[T](z). Equiva-\nlently,\nbC(k)[T](z) = arg max\nr=1:R\nγ(r)\nbθ(k)[T ](z).\n(S.1.11)\nS.1.1\nTheory\nWe need the following assumption before stating the theory.\nAssumption 2. Denote ∆(k)\nrj =\nq\n(µ(k)∗\nr\n−µ(k)∗\nj\n)⊤(Σ(k)∗)−1(µ(k)∗\nr\n−µ(k)∗\nj\n) for k ∈S. Sup-\npose the following conditions hold:\n(i) nS = P\nk∈S nk ≥C1|S| maxk=1:K nk with a constant C1 > 0;\n(ii) mink∈S nk ≥C2(p + log K) with some constant C2;\n(iii) There exists a permutation π : [R] →[R] such that\n(a) maxk∈S\n\b\u0002\nmaxr=2:R ∥bβ(k)[0]\nr\n−(Σ(k)∗)−1(µ(k)∗\nπ(r) −µ(k)∗\nπ(1))∥2\n\u0003\n∨\n\u0000maxr=1:R ∥bµ(k)[0]\nr\n−\nµ(k)∗\nπ(r)∥2\n\u0001\t\n≤C3 mink∈S minr̸=j ∆(k)\nrj , with some constant C3;\n(b) maxk∈S maxr=2:R | bw(k)[0]\nr\n−w(k)∗\nπ(r)| ≤cw/2.\n(iv) mink∈S minr̸=j ∆(k)\nrj ≥C4 > 0 with some constant C4;\nRemark 6. The above set of conditions are analogues of those in Assumption 1. We refer\nto Remark 1 for a detailed explanation of each condition.\nWe first present the result for parameter estimation.\nWe adopt similar error met-\nrics as the ones in (4) and (5). Specifically, denote the true parameter by {θ\n(k)∗}k∈S =\n{({w(k)∗\nr\n}R\nr=2, {µ(k)∗\nr\n}R\nr=1, Σ(k)∗)}k∈S which belongs to the parameter space ΘS(h) in (S.1.8).\nFor each k ∈S, define the functional θ(k)∗= ({w(k)∗\nr\n}R\nr=2, {β(k)∗\nr\n}R\nr=2, {δ(k)∗\nr\n}R\nr=2), where\n5\n6\nAlgorithm 4: MTL-GMM (Multi-cluster)\nInput: Initialization {({ bw(k)[0]\nr\n}R\nr=1, { bβ(k)[0]\nr\n}R\nr=2, {bµ(k)[0]\nr\n}R\nr=1)}K\nk=1, maximum\nnumber of iteration rounds T, initial penalty parameter λ[0], tuning\nparameters Cλ > 0, κ ∈(0, 1)\n1 bθ(k)[0] = ({ bw(k)[0]\nr\n}R\nr=1, { bβ(k)[0]\nr\n}R\nr=2, {bδ(k)[0]\nr\n}R\nr=2), where\nbδ(k)[0]\nr\n= 1\n2( bβ(k)[0]\nr\n)⊤(bµ(k)[0]\n1\n+ bµ(k)[0]\nr\n), for k = 1 : K\n2 for t = 1 to T do\n3\nλ[t] = κλ[t−1] + Cλ\n√p + log K\n// Update the penalty parameter\n4\nfor k = 1 to K do\n// Local update for each task\n5\nfor r = 1 to R do\n6\nbw(k)[t]\nr\n=\n1\nnk\nPnk\ni=1 γ(r)\nbθ(k)[t−1](z(k)\ni )\n7\nbµ(k)[t]\nr\n=\nPnk\ni=1 γ(r)\nbθ(k)[t−1](z(k)\ni\n)z(k)\ni\nnk bw(k)[t]\nr\n8\nend\n9\nbΣ(k)[t] =\n1\nnk\nPnk\ni=1\nPR\nr=1 γ(r)\nbθ(k)[t−1](z(k)\ni ) · (z(k)\ni\n−bµ(k)[t]\nr\n)(z(k)\ni\n−bµ(k)[t]\nr\n)⊤\n10\nend\n11\nfor r = 2 to R do\n12\n{ bβ(k)[t]\nr\n}K\nk=1, β\n[t]\nr =\narg min\nβ(1),...,β(K),β\n\u001a PK\nk=1 nk\nh\n1\n2(β(k))⊤bΣ(k)[t]β(k) −\n(β(k))⊤(bµ(k)[t]\nr\n−bµ(k)[t]\n1\n)\ni\n+ PK\nk=1\n√nkλ[t] · ∥β(k) −β∥2\n\u001b\n// Aggregation\n13\nend\n14\nfor k = 1 to K do\n// Local update for each task\n15\nfor r = 2 to R do\n16\nbδ(k)[t]\nr\n= 1\n2( bβ(k)[t]\nr\n)⊤(bµ(k)[t]\n1\n+ bµ(k)[t]\nr\n)\n17\nend\n18\nLet bθ(k)[t] = ({ bw(k)[t]\nr\n}R\nr=1, { bβ(k)[t]\nr\n}R\nr=2, {bδ(k)[t]}R\nr=2)\n19\nend\n20 end\nOutput: {(bθ(k)[T], {bµ(k)[T]\nr\n}R\nr=1, bΣ(k)[T])}K\nk=1 with bθ(k)[T] = ({ bw(k)[T]\nr\n}R\nr=1, { bβ(k)[T]\nr\n}R\nr=2,\n{bδ(k)[T]}R\nr=2)\nβ(k)∗\nr\n= (Σ(k)∗)−1(µ(k)∗\nr\n−µ(k)∗\n1\n), δ(k)∗\nr\n=\n1\n2(β(k)∗)⊤(µ(k)∗\n1\n+ µ(k)∗\nr\n). For the estimators re-\nturned by Algorithm 4, we are interested in the error metrics 3:\nd(bθ(k)[T], θ(k)∗) =\nmin\nπ:[R]→[R] max\nr=2:R\nn\n| bw(k)[T]\nr\n−w(k)∗\nπ(r)| ∨∥bβ(k)[T]\nr\n−(Σ(k)∗)−1(µ(k)∗\nπ(r) −µ(k)∗\nπ(1))∥2\n∨|bδ(k)[T]\nr\n−(µ(k)∗\nπ(r) + µ(k)∗\nπ(1))⊤(Σ(k)∗)−1(µ(k)∗\nπ(r) −µ(k)∗\nπ(1))/2|\no\n,\n\u0010\nmin\nπ:[R]→[R] max\nr=1:R ∥bµ(k)[T]\nr\n−µ(k)∗\nπ(r)∥2\n\u0011\n∨∥bΣ(k)[T] −Σ(k)∗∥2.\nTheorem 7. (Upper bounds of the estimation error of GMM parameters for multi-cluster\nMTL-GMM) Suppose Assumption 2 holds, |S| ≥s, and ϵ =\nK−s\nK\n< 1/3.\nLet λ[0] ≥\nC1 maxk=1:K √nk, Cλ ≥C1 and κ > C2 with some constants C1 > 0, C2 ∈(0, 1) 4 Then\nthere exists a constant C3 > 0, such that for any {θ\n(k)∗}k∈S ∈ΘS(h) and any probability\nmeasure QS on (Rp)⊗nSc, with probability 1 −C3K−1, the following hold for all k ∈S:\nd(bθ(k)[T], θ(k)∗) ≲\nr p\nnS\n+\nr\nlog K\nnk\n+ h ∧\nr\np + log K\nnk\n+ ϵ\nr\np + log K\nmaxk=1:K nk\n+ T 2(κ′)T,\n\u0012\nmin\nπ:[R]→[R] max\nr=1:R ∥bµ(k)[T]\nr\n−µ(k)∗\nπ(r)∥2\n\u0013\n∨∥bΣ(k)[T] −Σ(k)∗∥2 ≲\nr\np + log K\nnk\n+ T 2(κ′)T,\nwhere κ′ ∈(0, 1) is some constant and nS = P\nk∈S nk. When T ≥C log(maxk=1:K nk) with\nsome large constant C > 0, the last term on the right-hand side will be dominated by other\nterms in both inequalities.\nRecall the clustering method bC(k)[T] defined in (S.1.11). The next theorem obtains the\nupper bound of the excess mis-clustering error of bC(k)[T] for k ∈S.\nTheorem 8. (Upper bound of the excess mis-clustering error for multi-cluster MTL-GMM)\nSuppose the same conditions in Theorem 7 hold. Then there exists a constant C1 > 0\nsuch that for any {θ\n(k)∗}k∈S ∈ΘS(h) and any probability measure QS on (Rp)⊗nSc, with\nprobability at least 1 −C1K−1, the following holds for all k ∈S:\nRθ\n(k)∗( bC(k)[T]) −Rθ\n(k)∗(Cθ\n(k)∗) ≲d2(bθ(k)[T], θ(k)∗) · log d−1(bθ(k)[T], θ(k)∗)\n3Similar to the binary case, the minimum is taken due to the non-identifiability in multi-cluster GMMs.\n4C1 and C2 depend on the constants M, cw, and cΣ etc.\n7\n≲\n\u0014 p\nnS\n+ log K\nnk\n+ h2 ∧p + log K\nnk\n+ ϵ2 p + log K\nmaxk=1:K nk\n+ T 4(κ′)2T\n\u0015\n· log\n\u0012nS\np ∧\nnk\nlog K\n\u0013\n,\nwhere κ′ ∈(0, 1) is some constant. When T ≥C log(maxk=1:K nk) with a large constant\nC > 0, the term involving T on the right-hand side will be dominated by other terms.\nComparing the upper bounds in Theorems 7 and 8 with those in Theorems 1 and 2,\nthe only difference is an extra logarithmic term log\n\u0000 nS\np ∧\nnk\nlog K\n\u0001\nin Theorem 8, which we\nbelieve is a proof artifact. Similar logarithmic terms appear in other multi-cluster GMM\nliteratures as well, see for example, Yan et al. (2017) and Zhao et al. (2020). To understand\nthe upper bounds in Theorems 7 and 8, we can follow the discussion after Theorems 1 and\n2. We do not repeat it here.\nThe following lower bounds together with the derived upper bounds will show that our\nmethod is (nearly) minimax optimal in a wide range of regimes.\nTheorem 9. (Lower bounds of the estimation error of GMM parameters in multi-task\nlearning) Suppose ϵ = K−s\nK\n< 1/3. When there exists a subset S with |S| ≥s such that\nmink∈S nk ≥C1(p + log K) and mink∈S minr,j ∆(k)\nrj\n≥C2, where C1, C2 > 0 are some\nconstants, we have\ninf\n{bθ(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS(h)\nQS\nP\n [\nk∈S\n(\nd(bθ(k), θ(k)∗) ≳\nr p\nnS\n+\nr\nlog K\nnk\n+ h ∧\nr\np + log K\nnk\n+\nϵ\n√maxk=1:K nk\n)!\n≥1\n10,\ninf\n{bµ(k)\nr\n}k=1:K,r=1:R\n{bΣ(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS(h)\nQS\nP\n [\nk∈S\n( \u0012\nmin\nπ:[R]→[R] max\nr=1:R ∥bµ(k)\nr\n−µ(k)∗\nπ(r)∥2\n\u0013\n∨\n∥bΣ(k) −Σ(k)∗∥2 ≳\nr\np + log K\nnk\n)!\n≥1\n10.\nTheorem 10. (Lower bound of the excess mis-clustering error in multi-task learning) Sup-\npose the same conditions in Theorem 9 hold. Then\ninf\n{ bC(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS(h)\nQS\nP\n [\nk∈S\n(\nRθ\n(k)∗( bC(k)) −Rθ\n(k)∗(Cθ\n(k)∗) ≳p\nnS\n+ log K\nnk\n+ h2 ∧p + log K\nnk\n+\nϵ2\nmaxk=1:K nk\n)!\n≥1\n10.\n8\nThe lower bounds in Theorems 9 and 10 are the same as those in Theorems 3 and\n4. Therefore, the remarks on the comparison of upper and lower bounds presented after\nTheorem 4 carry over to the multi-cluster setting (up to the logarithmic term from Theorem\n8). We do not repeat the details here.\nS.1.2\nAlignment\nSimilar to the binary case, we have the alignment issues in multi-cluster case as well. In\nthis section, we propose two alignment algorithms as the extension to the Algorithms 2\nand 3.\nIn the multi-cluster case, the alignment of each task can be represented as a permutation\nof [R]. Consider a series of permutations π = {πk}K\nk=1, where each πk is a permutation\nfunction on [R]. Define a score of π as\nscore(π) =\nR\nX\nr=2\nX\nk̸=k′\n\r\r\r(bΣ(k)[0])−1\u0000bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ(k′)[0])−1(bµ(k′)[0]\nπk′(r) −bµ(k′)[0]\nπk′(1)\n\u0001\r\r\r\n2 .\nWe want to recover the correct alignment π∗\nk = arg min\nπk:[R]→[R]\nPR\nr=2\n\r\r\r(bΣ(k)[0])−1\u0000bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −β(k)∗\nr\n\r\r\r\n2.\nWe proposed an exhaustive search algorithm, which is summarized in Algorithm 5.\nAlgorithm 5: Exhaustive search for the alignment in multi-cluster GMMs\nInput: Initialization {{bµ(k)[0]\nr\n}R\nr=1, bΣ(k)[0]}K\nk=1\n1 bπ = {bπk}K\nk=1 ←arg minπ score(π)\nOutput: bπ = {bπk}K\nk=1\nThe following theorem shows that under certain conditions, the output from Algorithm\n5 recovers the correct alignment up to a permutation.\nTheorem 11 (Alignment correctness for Algorithm 5). Assume that\n(i) maxk∈S maxr̸=j ∆(k)\nrj / mink∈S minr̸=j ∆(k)\nrj ≤D with D ≥1.\n(ii) ϵ <\n1\n24DcΣ+1;\n(iii) mink∈S minr̸=j ∆(k)\nrj ≥\n\u0014\n4(1−ϵ)c1/2\nΣ\n1−(24DcΣ+1)ϵh +\n(4+20ϵ)c1/2\nΣ\n1−(24DcΣ+1)ϵξ\n\u0015\n∨\n\u0014\n13(1−ϵ)c1/2\nΣ\n1−(9DcΣ+1)ϵh +\n(13−4ϵ)c1/2\nΣ\n1−(9DcΣ+1)ϵξ\n\u0015\n,\n9\nwhere ϵ =\nK−s\nK\nis the outlier task proportion introduced in Theorem 7, h is degree of\ndiscriminant coefficient similarity defined in (S.1.8), and\nξ = max\nk∈S\nmin\nπ:[R]→[R] max\nr=1:R\n\r\r\r(bΣ(k)[0])−1\u0000bµ(k)[0]\nπ(r) −bµ(k)[0]\nπ(1)\n\u0001\n−β(k)∗\nr\n\r\r\r\n2\n= max\nk∈S\nmin\nπ:[R]→[R] max\nr=1:R\n\r\r\r(bΣ(k)[0])−1\u0000bµ(k)[0]\nπ(r) −bµ(k)[0]\nπ(1)\n\u0001\n−(Σ(k)∗)−1\u0000µ(k)∗\nr\n−µ(k)∗\n1\n\u0001\r\r\r\n2 .\nThen there exists a permutation ι : [R] →[R], such that the output of Algorithm 5 satisfies\nbπk = ι ◦π∗\nk,\nfor all k ∈S.\nThe biggest issue of Algorithm 5 is the computational time. It is easy to see that the time\ncomplexity of it is O((R!)K ·RK2), because it needs to search over all permutations. This is\nnot practically feasible when R and K are large. Therefore, we propose the following greedy\nsearch algorithm to reduce the computational cost, which is summarized in Algorithm 6.\nNote that its main idea is similar to Algorithm 3 for the binary GMM, but the procedure\nis different. We define the score of alignments {πk′}k\nk′=1 of tasks 1-k′ as\nscore({πk′}k\nk′=1|{{bµ(k′)[0]\nr\n}R\nr=1}k\nk′=1, {bΣ(k′)[0]}k\nk′=1)\n=\nR\nX\nr=2\nX\n˜k,k′≤k\n\r\r\r(bΣ(˜k)[0])−1\u0000bµ(˜k)[0]\nπ˜k(r) −bµ(˜k)[0]\nπ˜k(1)) −(bΣ(k′)[0])−1(bµ(k′)[0]\nπk′(r) −bµ(k′)[0]\nπk′(1)\n\u0001\r\r\r\n2 .\nAlgorithm 6: Greedy search for the alignment in multi-cluster GMMs\nInput: Initialization {{bµ(k)[0]\nr\n}R\nr=1, bΣ(k)[0]}K\nk=1\n1 for k = 1 to K do\n2\nWith {bπk′}k−1\nk′=1 fixed (∅when k = 1), set\nbπk = arg min\nπ:[R]→[R]\nscore({bπk′}k−1\nk′=1 ∪π|{{bµ(k′)[0]\nr\n}R\nr=1}k\nk′=1, {bΣ(k′)[0]}k\nk′=1)\n3 end\nOutput: bπ = {bπk}K\nk=1\nThe subsequent theorem demonstrates that, with slightly stronger assumptions than\nthose required by Algorithm 5, the greedy search algorithm can recover the correct align-\nment up to a permutation with high probability. Importantly, this approach significantly\nalleviates the computational cost from O((R!)K · RK2) to O(R!K · RK2).\n10\nTheorem 12. Assume there are no outlier tasks in the first K0 tasks, and\n(i) maxk∈S maxr̸=j ∆(k)\nrj / mink∈S minr̸=j ∆(k)\nrj ≤D with D ≥1.\n(ii) ϵ <\n1\n2DcΣ+1;\n(iii) K0 > 2DcΣKϵ;\n(iv) mink∈S minr̸=j ∆(k)\nrj ≥\n\u0014\n4K0c1/2\nΣ\nK0−DcΣKϵh +\n(4K0+Kϵ)c1/2\nΣ\nK0−DcΣKϵ ξ\n\u0015\n∨\n\u0014\n2K0c1/2\nΣ\nK0−2DcΣKϵh +\n(2K0+2Kϵ)c1/2\nΣ\nK0−2DcΣKϵ ξ\n\u0015\n,\nwhere ϵ = K−s\nK\nis the outlier task proportion and cΣ appears in the condition that c−1\nΣ ≤\nmink∈S λmin(Σ(k)∗) ≤maxk∈S λmax(Σ(k)∗) ≤cΣ. Then there exists a permutation ι : [R] →\n[R], such that the output of Algorithm 6 satisfies\nbπk = ι ◦π∗\nk,\nfor all k ∈S.\nRemark 7. Conditions (ii)-(iv) are similar to the conditions in Theorem 6. The inclusion\nof Condition (i) aims to facilitate the analysis in the proof, and we conjecture that the\nobtained results persist even if this condition is omitted.\nWhen R is very large, the computational burden becomes prohibitive, rendering even\nthe O(R!K · RK2) time complexity impractical. Addressing this computational challenge\nrequires the development of more efficient alignment algorithms, a pursuit that we defer\nto future investigations. In addition, one caveat of the greedy search algorithm is that we\nneed to know K0 non-outlier tasks a priori, which may not be unrealistic in practice. In our\nempirical examinations, we enhance the algorithm’s performance by introducing a random\nshuffle of the K tasks in each iteration. Specifically, we execute Algorithm 6 for 200 times,\nyielding 200 alignment candidates. The final alignment is then determined by selecting the\nconfiguration that attains the minimum score among the candidates.\nS.2\nTransfer Learning\nS.2.1\nProblem setting\nIn the main text and Section S.1, we discussed GMMs under the context of multi-task\nlearning, where the goal is to learn all tasks jointly by utilizing the potential similarities\n11\nshared by different tasks.\nIn this section, we will study binary GMMs in the transfer\nlearning context where the focus is on the improvement of learning in one target task\nthrough the transfer of knowledge from related source tasks. Multi-cluster results can be\nobtained similarly as in the MTL case, and we omit the details given the extensive length\nof the paper.\nSuppose that there are (K + 1) tasks in total, where the first task is called the target\nand the K remaining ones are called K sources. As in multi-task learning, we assume that\nthere exists an unknown subset S ⊆1 : K, such that samples from sources in S follow an\nindependent GMM, while samples from sources outside S can be arbitrarily distributed.\nThis means,\ny(k)\ni\n=\n\n\n\n\n\n1,\nwith probability 1 −w(k)∗;\n2,\nwith probability w(k)∗;\nz(k)\ni |y(k)\ni\n= j ∼N(µ(k)∗\nj\n, Σ(k)∗), j = 1, 2,\nfor all k ∈S, i = 1 : nk, and\n{z(k)\ni }i,k∈Sc ∼QS,\nwhere QS is some probability measure on (Rp)⊗nSc and nSc = P\nk∈Sc nk.\nFor the target task, we observe sample {z(0)\ni }n0\ni=1 independently sampled from the fol-\nlowing GMM:\ny(0)\ni\n=\n\n\n\n\n\n1,\nwith probability 1 −w(0)∗;\n2,\nwith probability w(0)∗;\nz(0)\ni |y(0)\ni\n= j ∼N(µ(0)∗\nj\n, Σ(0)∗), j = 1, 2.\nThe objective of transfer learning is to use source data to help improve GMM learning\nin the target task. As for multi-task learning, we measure the learning performance by\nboth parameter estimation error and the excess mis-clustering error, but only on the target\nGMM. Toward this end, we define the joint parameter space for GMM parameters of the\ntarget and sources in S:\nΘ\n′\nS(h) =\nn\n{θ\n(k)}k∈{0}∪S = {(w(k), µ(k)\n1 , µ(k)\n2 , Σ(k))}k∈{0}∪S : θ\n(k) ∈Θ, max\nk∈S ∥β(k)−β(0)∥2 ≤h\no\n,\n(S.2.12)\n12\nwhere Θ is the single GMM parameter space introduced in (2), and β(k) = (Σ(k))−1(µ(k)\n2 −\nµ(k)\n1 ), k ∈{0} ∪S. Comparing Θ\n′\nS(h) with the parameter space ΘS(h) from multi-task\nlearning in (3), here the target discriminant coefficient β(0) serves as the “center” of dis-\ncriminant coefficients of sources in S. The quantity h characterizes the closeness between\nsources in S and the target.\nS.2.2\nMethod\nLike the MTL-GMM procedure developed in Section 2.2, we combine the EM algorithm\nand the penalization framework to develop a variant of the EM algorithm for transfer\nlearning. The key idea is to first apply MTL-GMM to all the sources to obtain estimates\nof discriminant coefficient “center” as good summary statistics of the K source data sets,\nand then shrink the target discriminant coefficient towards those center estimates in the\nEM iterations to explore the relatedness between sources and the target. See Section 3.3\nof Duan and Wang (2023) for more general discussions on this idea. Our proposed transfer\nlearning procedure TL-GMM is summarized in Algorithm 7.\nWhile the steps of TL-GMM look very similar to those of MTL-GMM, there exist two\nmajor differences between them. First, for each optimization problem in TL-GMM, the first\npart of the objective function only involves the target data {z(0)\ni }n0\ni=1, while in MTL-GMM,\nit is a weighted average of all tasks. Second, in TL-GMM, the penalty is imposed on the\ndistance between a discriminant coefficient estimator and a given center estimator produced\nby MTL-GMM from the source data. In contrast, the center is estimated simultaneously\nwith other parameters through the penalization in MTL-GMM. In light of existing transfer\nlearning approaches in the literature, TL-GMM can be considered as the “debiasing” step\ndescribed in Li et al. (2021) and Tian and Feng (2023), which corrects potential bias of the\ncenter estimate using the target data.\nThe tuning parameters {λ[t]\n0 }T0\nt=1 in Algorithm 7 control the amount of knowledge to\nbe transferred from sources. Setting tuning parameters large enough pushes parameter\nestimates for the target task to be exactly equal to the center learned from sources while\nletting them be zero makes TL-GMM reduce to the standard EM algorithm on the target\ndata.\n13\nAlgorithm 7: TL-GMM\nInput: Initialization bθ(0)[0] = ( bw(0)[0], bβ(0)[0], bδ(0)[0]), output β\n[T] from Algorithm 1,\nmaximum number of iteration rounds T0, initial penalty parameter λ[0]\n0 ,\ntuning parameters Cλ0 > 0 and κ0 ∈(0, 1)\n1 for t = 1 to T0 do\n2\nλ[t]\n0 = κ0λ[t−1]\n0\n+ Cλ0\n√p + log K\n// Update the penalty parameter\n3\nbw(0)[t] =\n1\nn0\nPn0\ni=1 γbθ(0)[t−1](z(0)\ni )\n4\nbµ(0)[t]\n1\n=\nPn0\ni=1[1−γbθ(0)[t−1](z(0)\ni\n)]z(0)\ni\nn0(1−bw(0)[t])\n, bµ(0)[t]\n2\n=\nPn0\ni=1 γbθ(0)[t−1](z(0)\ni\n)z(0)\ni\nn0 bw(0)[t]\n5\nbΣ(0)[t] =\n1\nn0\nPn0\ni=1\nn\n[1 −γbθ(0)[t−1](z(0)\ni )] · (z(0)\ni\n−bµ(0)[t]\n1\n)(z(0)\ni\n−bµ(0)[t]\n1\n)⊤\n+γbθ(0)[t−1](z(0)\ni ) · (z(0)\ni\n−bµ(0)[t]\n2\n)(z(0)\ni\n−bµ(0)[t]\n2\n)⊤o\n6\nbβ(0)[t] =\narg min\nβ(0)\n\u001ah\n1\n2(β(0))⊤bΣ(0)[t]β(0) −(β(0))⊤(bµ(0)[t]\n2\n−bµ(0)[t]\n1\n)\ni\n+ λ[t]\n0\n√n0∥β(0) −β\n[T]∥2\n\u001b\n7\nbδ(0)[t] = 1\n2( bβ(0)[t])⊤(bµ(0)[t]\n1\n+ bµ(0)[t]\n2\n)\n8\nLet bθ(0)[t] = ( bw(0)[t], bβ(0)[t], bδ(0)[t])\n9 end\nOutput: (bθ(0)[T0], bµ(0)[T0]\n1\n, bµ(0)[T0]\n2\n, bΣ(0)[T0]) with bθ(0)[T0] = ( bw(0)[T0], bβ(0)[T0], bδ(0)[T0])\nS.2.3\nTheory\nIn this section, we will establish the upper and lower bounds for the GMM parameter\nestimation error and the excess mis-clustering error on the target task. First, we impose\nthe following assumption set.\nAssumption 3. Denote ∆(0) =\nq\n(µ(0)∗\n1\n−µ(0)∗\n2\n)⊤(Σ(0)∗)−1(µ(0)∗\n1\n−µ(0)∗\n2\n).\nAssume the\nfollowing conditions hold:\n(i) C1\nh\nlog K\np\n+ ϵ2\u00001 + log K\np\n\u0001i\n≤maxk∈S nk\nn0\n≤C2\n\u00001+ log K\np\n\u0001\nwith constants C1 and C2, where\nϵ = K−s\nK .\n(ii) n0 ≥C3p with some constant C3;\n(iii) Either of the following two conditions holds with some constant C4:\n(a) ∥bβ(0)[0] −β(0)∗∥2 ∨|bδ(0)[0] −δ(0)∗| ≤C4∆(0), | bw(0)[0] −w(0)∗| ≤cw/2;\n14\n(b) ∥bβ(0)[0] + β(0)∗∥2 ∨|bδ(0)[0] + δ(0)∗| ≤C4∆(0), |1 −bw(0)[0] −w(0)∗| ≤cw/2.\n(iv) ∆(0) ≥C5 > 0 with some constant C5;\nRemark 8. Condition (i) requires the target sample size not to be much smaller than\nthe maximum source sample size, which appears due to technical reasons in the proof.\nConditions (ii)-(iv) can be seen as the counterpart of Conditions (ii)-(iv) in Assumption 1\nfor the target GMM.\nWe are in the position to present the upper bounds of the estimation error of GMM\nparameters for TL-GMM.\nTheorem 13. (Upper bounds of the estimation error of GMM parameters for TL-GMM)\nSuppose the conditions in Theorem 1 and Assumption 3 hold. Let λ[0]\n0 ≥C1 maxk=1:K √nk,\nCλ0 ≥C1, κ0 > C2 with some specific constants C1 > 0, C2 ∈(0, 1). Then there exists a\nconstant C3 > 0, such that for any {θ\n(k)∗}k∈{0}∪S ∈Θ\n′\nS(h) and any probability measure QS\non (Rp)⊗nSc, we have\nd(bθ(0)[T0], θ(0)∗) ≲\nr\np\nnS + n0\n+\nr\n1\nn0\n+ h ∧\nr p\nn0\n+\n\u0012\nϵ\nr\np + log K\nmaxk=1:K nk\n\u0013\n∧\nr p\nn0\n+\nr\nlog K\nmaxk=1:K nk\n+ T0(κ′\n0)T0,\nmin\nπ:[2]→[2] max\nr=1:2 ∥bµ(0)[T0]\nπ(r)\n−µ(0)∗\nr\n∥2 ∨∥bΣ(0)[T0] −Σ(0)∗∥2 ≲\nr p\nn0\n+ T0(κ′\n0)T0,\nwith probability at least 1 −C3K−1, where κ′\n0 ∈(0, 1) and nS = P\nk∈S nk. When T0 ≥\nC log n0 with a large constant C > 0, in both inequalities, the last term on the right-hand\nside will be dominated by other terms.\nNext, we present the upper bound of the excess mis-clustering error on the target task\nfor TL-GMM. Having the estimator bθ(0)[T0] and the truth θ(0)∗, the clustering method bC(0)[T0]\nand its mis-clustering error Rθ\n(0)∗( bC(0)[T0]) are defined in the same way as in (6) and (7).\nTheorem 14. (Upper bound of the target excess mis-clustering error for TL-GMM) Sup-\npose the same conditions in Theorem 13 hold. Then there exists a constant C1 > 0 such\nthat for any {θ\n(k)∗}k∈{0}∪S ∈Θ\n′\nS(h) and any probability measure QS on (Rp)⊗nSc, with\n15\nprobability at least 1 −C1K−1 the following holds:\nRθ\n(0)∗( bC(0)[T0]) −Rθ\n(0)∗(Cθ\n(0)∗) ≲\np\nnS + n0\n|\n{z\n}\n(I)\n+\n1\nn0\n|{z}\n(II)\n+ h2 ∧p\nn0\n| {z }\n(III)\n+ ϵ2 p + log K\nmaxk=1:K nk\n∧p\nn0\n|\n{z\n}\n(IV)\n+\nlog K\nmaxk=1:K nk\n|\n{z\n}\n(V)\n+ T 2\n0 (κ′\n0)2T0\n|\n{z\n}\n(VI)\n,\nwith some constant κ′\n0 ∈(0, 1). When T0 ≥C log n0 with some large constant C > 0, the\nlast term in the upper bound will be dominated by the second term.\nSimilar to the upper bounds of d(bθ(k)[T], θ(k)∗) and Rθ\n(k)∗( bC(k)[T])−Rθ\n(k)∗(Cθ\n(k)∗) in Theo-\nrems 1 and 2, the upper bounds for d(bθ(0)[T0], θ(0)∗) and Rθ\n(0)∗( bC(0)[T0])−Rθ\n(0)∗(Cθ\n(0)∗) consist\nof multiple parts with one-to-one correspondence. We take the bound of Rθ\n(0)∗( bC(0)[T0]) −\nRθ\n(0)∗(Cθ\n(0)∗) in Theorem 14 as an example. Part (I) is the oracle rate OP\n\u0000p\nnS+n0\n\u0001\n. Part (II)\nis the error caused by estimating scalar parameters δ(0)∗and w(0)∗in the decision bound-\nary, which thus do not depend on dimension p. Part (III) quantifies the contribution of\nrelated sources to the target task learning. The more related the sources in S to the target\n(i.e. the smaller h is), the smaller Part (III) becomes. Part (IV) captures the impact of\noutlier sources on the estimation error. As ϵ increases (i.e. the proportion of outlier sources\nincreases), Part (IV) first increases and then flats out. It never exceeds the minimax rate\nOP(p/n0) of the single task learning on target task (Balakrishnan et al., 2017; Cai et al.,\n2019). Therefore, our method is robust against a fraction of outlier sources with arbitrary\ncontaminated data. Part (V) is an extra term caused by estimating the center in MTL-\nGMM, which by Assumption 3.(i) is smaller than the single-task learning rate OP(p/n0).\nPart (VI) decreases geometrically in the iteration number T0 of Algorithm 7, which becomes\nnegligible by setting the iteration numbers T0 large enough.\nConsider the general scenario T0 ≳log n0.\nThen the upper bound of excess mis-\nclustering error rate Rθ\n(0)∗( bC(0)[T0]) −Rθ\n(0)∗(Cθ\n(0)∗) is guaranteed to be no worse than the\noptimal single-task learning rate OP(p/n0). More importantly, in the general regime where\nϵ ≪\nq\np maxk=1:K nk\n(p+log K)n0\n(small number of outlier sources), h ≪\np\np/n0 (enough similarity be-\ntween sources and target), nS ≫n0 (large total source sample size), and maxk∈S nk/n0 ≫\nlog K/p (large maximum source sample size), TL-GMM improves the GMM learning on\nthe target task by achieving a better estimation error rate. As for the upper bound of\n16\nminπ:[2]→[2] maxr=1:2 ∥bµ(0)[T0]\nπ(r)\n−µ(0)∗\nr\n∥2 and ∥bΣ(0)[T0] −Σ(0)∗∥2, when T0 ≳log n0, it has the\nsingle-task learning rate OP(\np\np/n0).\nThis is expected since the mean vectors and co-\nvariance matrices from sources are not necessarily similar to the one from target in the\nparameter space Θ\n′\nS(h).\nThe following result of minimax lower bounds shows that the upper bounds in Theorems\n13 and 14 are optimal in a broad range of regimes.\nTheorem 15. (Lower bounds of the estimation error of GMM parameters in transfer\nlearning) Suppose ϵ =\nK−s\nK\n< 1/3. Suppose there exists a subset S with |S| ≥s such\nthat mink∈S nk ≥C1(p + log K), n0 ≥C1p and mink∈{0}∪S ∆(k) ≥C2 with some constants\nC1, C2 > 0. Then we have\ninf\nbθ(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS(h)\nQS\nP\n \nd(bθ(0), θ(0)∗) ≳\nr\np\nnS + n0\n+\nr\n1\nn0\n+ h ∧\nr p\nn0\n+\nϵ\n√maxk=1:K nk\n∧\nr p\nn0\n!\n≥1\n10,\ninf\nbµ(0)\n1 ,bµ(0)\n2\nbΣ(0)\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS(h)\nQS\nP\n \nmin\nπ:[2]→[2] max\nr=1:2 ∥bµ(0)\nπ(r)−µ(0)∗\nr\n∥2∨∥bΣ(0)−Σ(0)∗∥2 ≳\nr p\nn0\n!\n≥1\n10.\nTheorem 16. (Lower bound of the target excess mis-clustering error in transfer learning)\nSuppose the same conditions in Theorem 15 hold. Then we have\ninf\nbC(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS(h)\nQS\nP\n \nRθ\n(0)∗( bC(0)) −Rθ\n(0)∗(Cθ\n(0)∗) ≳\np\nnS + n0\n+ 1\nn0\n+ h2 ∧p\nn0\n+\nϵ2\nmaxk=1:K nk\n∧1\nn0\n!\n≥1\n10.\nComparing the upper and lower bounds in Theorems 13- 16, several remarks are in\norder:\n• With T0 ≳log n0, our estimators bµ(0)[T0]\n1\n, bµ(0)[T0]\n2\n, bΣ(0)[T0] achieve the minimax optimal\nrate for estimating the mean vectors µ(0)∗\n1\n, µ(0)∗\n2\nand the covariance matrix Σ(0)∗.\n• Regarding the target excess mis-clustering error, with the choices T0 ≳log n0, Part\n(VI) in the upper bound becomes negligible. We thus compare the other five terms\nin the upper bound with the corresponding terms in the lower bound.\n17\n1. Part (IV) in the upper bound differs from the one in the lower bound by a factor\np (up to log K). Hence the gap can arise when the dimension p diverges. The\nreason is similar to the one in a multi-task learning setting and using statistical\ndepth function based “center” estimates might be able to close the gap. We\nrefer to the paragraph after Theorem 4 for more details.\n2. Part (V) in the upper bound does not appear in the lower bound. This term\nis due to the center estimate from the upper bound in MTL-GMM. When\nmaxk∈S nk/n0 ≳log K, this term is dominated by Part (II).\n3. The other three terms from the upper bound match with the ones in the lower\nbound.\n• Based on the above comparisons, we can conclude that under the mild condition\nmaxk∈S nk/n0 ≳log K, our method is minimax rate optimal for the estimation of\nθ(0)∗in the classical low-dimensional regime p = O(1). Even when p is unbounded,\nthe gap between the upper and lower bounds appears only when the fourth or fifth\nterm is the dominating term in the upper bound. Like the discussions after Theorem\n4, similar restricted regimes where our method might become sub-optimal can be\nderived.\nS.2.4\nLabel alignment\nAs in multi-task learning, the alignment issue exists in transfer learning as well. Referring\nto the parameter space Θ\n′(h) and the conditions of initialization in Assumptions 1 and\n3, the success of Algorithm 7 requires correct alignments in two places. First, the center\nestimate β\n[T] used as input of Algorithm 7 are obtained from Algorithm 1 which involves\nthe alignment of initial estimates for sources. This alignment problem can be readily solved\nby Algorithm 2 or 3. Second, the initialization of the target problem bβ(0)[0] needs to be\ncorrectly aligned with the aforementioned center estimates. This is easy to address using\nthe alignment score described in Section 2.4.2 as there are only two different alignment\noptions. We summarize the steps in Algorithm 8.\nLike Algorithms 2 and 3, Algorithm 8 is able to find the correct alignments under mild\nconditions. Suppose { bβ(k)[0]}K\nk=0 are the initialization values with potentially wrong align-\n18\nment. Define the correct alignment as r∗= (r∗\n0, r∗\n1, . . . , r∗\nK) with r∗\nk = arg minrk=±1 ∥rk bβ(k)[0]−\nβ(k)∗∥2. For any r = {rk}K\nk=0 ∈{±1}K+1 which is a permutation order of { bβ(k)[0]}K\nk=0 and\nits corresponding alignment {rk bβ(k)[0]}K\nk=0, define its alignment score as\nscore(r) =\nX\n0≤k1̸=k2≤K\n∥rk1 bβ(k1)[0] −rk2 bβ(k2)[0]∥2.\nAlgorithm 8: Alignment for transfer learning\nInput: Initialization {( bβ(k)[0])}K\nk=0, and br from Algorithm 2 or 3\n1 if score((−1, br)) > score((1, br)) then\n2\nbr′ = (1, br)\n3 else\n4\nbr′ = (−1, br)\n5 end\nOutput: br′\nAs expected, under the conditions from Algorithms 2 or 3 for sources together with\nsome similar conditions on the target, Algorithm 8 will output the ideal alignment br′\n(equivalently, the good initialization br′\n0 bβ(0)[0] for Algorithm 7).\nTheorem 17 (Alignment correctness for Algorithm 8). Assume that\n(i) ϵ < 1\n2;\n(ii) ∥β(0)∗∥2 > 2(1−ϵ)\n1−2ϵ h + 2−ϵ\n1−2ϵ maxk∈{0}∪S\n\u0000∥bβ(k)[0] −β(k)∗∥2 ∧∥bβ(k)[0] + β(k)∗∥2\n\u0001\n,\nwhere ϵ = K−s\nK\nis the outlier source task proportion, and h is the degree of discriminant\ncoefficient relatedness defined in (S.2.12).\nFor br in Algorithm 8: if it is from Algorithm 2, assume the conditions of Theorem 5\nhold; if it is from Algorithm 3, assume the conditions of Theorem 6 hold. Then the output\nof Algorithm 8 satisfies\nbr′\nk = r∗\nk for all k ∈{0} ∪S\nor\nbr′\nk = −r∗\nk for all k ∈{0} ∪S.\n19\nS.3\nAdditional Numerical Studies\nIn this section, we present results from additional numerical studies, including supple-\nmentary results from the simulation study in Section 3 of the main text. Additionally, we\nprovide results from two new MTL simulations, one TL simulation, explorations of different\npenalty parameters, and two real-data studies.\nS.3.1\nSimulations\nS.3.1.1\nSimulation 1 of MTL\nIn this subsection, we provide additional performance evaluations for the three methods\n(MTL-GMM, Pooled-GMM, and Single-task-GMM) in the simulation presented in the main\ntext (referred to as Simulation 1). The results are displayed in Figures S.3 and S.4.\nReferring to Figure S.3 for the case without outlier tasks, MTL-GMM outperforms\nPooled-GMM in estimating w(k)∗all the time. This makes sense because Pooled-GMM\ndoes not take the heterogeneity of w(k)∗’s into account. For the estimation of other pa-\nrameters (except δ(k)∗5) and clustering, MTL-GMM and Pooled-GMM are competitive\nwhen h is small (i.e. the tasks are similar). As h increases (i.e. tasks become more het-\nerogenous), MTL-GMM starts to outperform Pooled-GMM by a large margin. Moreover,\nMTL-GMM is significantly better than Single-task-GMM in terms of both estimation and\nmis-clustering errors over a wide range of h. They only become comparable when h is very\nlarge. These comparisons demonstrate that MTL-GMM not only effectively utilizes the\nunknown similarity structure among tasks, but also adapts to it.\nThe results for the case with two outlier tasks are shown in Figure S.4. It is clear\nthat the comparison between MTL-GMM and Single-task-GMM is similar to the one in\nFigure S.3. What is new here is that even when h is very small, MTL-GMM still performs\nmuch better than Pooled-GMM, showing the robustness of MTL-GMM against a fraction\nof outlier tasks. Note that in this simulation, δ(k)∗= 0 for all k ∈[K], which might explain\nthe phenomenon where Pooled-GMM outperforms MTL-GMM in estimating δ(k)∗’s.\n5Actually it is not surprising to see Pooled-GMM estimates µ(k)∗\n1\n, µ(k)∗\n2\n, δ(k)∗, and Σ(k)∗better than\nMTL-GMM when h is small in this example. The reason is that these parameters are similar to each other\n(although MTL-GMM does not rely on this similarity) which makes pooling the data a good approach.\n20\n0.1\n0.2\n0.3\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {w(k) ∗ }k∈S\n0\n2\n4\n6\n8\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {µ1\n(k) ∗ }k∈S and {µ2\n(k) ∗ }k∈S\n0\n3\n6\n9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {Σ(k) ∗ }k∈S\n0\n2\n4\n6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {δ(k) ∗ }k∈S\n0.00\n0.05\n0.10\n0.15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nAverage mis−clustering error\nmethod\nMTL−GMM\nPooled−GMM\nSingle−task−GMM\nFigure S.3: The performance of different methods in Simulation 1.(i) of multi-task learning,\nwith no outlier tasks (ϵ = 0), and h changing from 0 to 10 with increment 1. Estimation\nerror of {w(k)∗}k∈S stands for maxk∈S(| bw(k)[T]−w(k)∗|∧|1−bw(k)[T]−w(k)∗|). Estimation error\nof {µ(k)∗\n1\n}k∈S and {µ(k)∗\n2\n}k∈S stands for maxk∈S minπ:[2]→[2](∥bµ(k)[T]\n1\n−µ(k)∗\nπ(1)∥2 ∨∥bµ(k)[T]\n2\n−\nµ(k)∗\nπ(2)∥2). Estimation error of {Σ(k)∗}k∈S stands for maxk∈S ∥bΣ(k)[T] −Σ(k)∗∥2. Estimation\nerror of {δ(k)∗}k∈S stands for maxk∈S(|bδ(k)[T]−δ(k)∗|∧|bδ(k)[T]+δ(k)∗|). Average mis-clustering\nerror represents the average empirical mis-clustering error rate calculated on the test set\nof tasks in S.\nS.3.1.2\nSimulation 2\nThe second simulation is a multi-cluster example, which is built based on Simulation 1.\nConsider a multi-task learning problem with K = 10 tasks, where each task has sample\n21\n0.1\n0.2\n0.3\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {w(k) ∗ }k∈S\n2\n4\n6\n8\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {µ1\n(k) ∗ }k∈S and {µ2\n(k) ∗ }k∈S\n0\n3\n6\n9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {Σ(k) ∗ }k∈S\n0\n2\n4\n6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {δ(k) ∗ }k∈S\n0.00\n0.05\n0.10\n0.15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nAverage mis−clustering error\nmethod\nMTL−GMM\nPooled−GMM\nSingle−task−GMM\nFigure S.4: The performance of different methods in Simulation 1.(ii) of multi-task learning,\nwith 2 outlier tasks (ϵ = 0.2), and h changing from 0 to 10 with increment 1. The meaning\nof each subfigure’s title is the same as in Figure S.3.\nsize nk = 100 and dimension p = 15, and follows a GMM with R = 4 clusters. For all\nk ∈[K], we generate (w(k)∗\n1\n, . . . , w(k)∗\nR\n) independently from Dirichlet(α) with α = 5 · 1R.\nWhen k ∈S, we generate µ(k)∗\nr\nfrom (2 · 02r−2, 2, 2, 0p−2r)⊤+ h/2 · (Σ(k)∗)−1u, where\nu ∼Unif({u ∈Rp : ∥u∥2 = 1}), Σ(k)∗= (0.2|i−j|)p×p. When k /∈S, we generate each\nw(k)∗from the same Dirichlet distribution and set Σ(k)∗= (0.5|i−j|))p×p and µ(k)∗\nr\nfrom\nUnif({u ∈Rp : ∥u∥2 = 0.5}) for r = 1 : R. For a given ϵ ∈[0, 1), in each replication the\noutlier task index set Sc is uniformly sampled from all subsets of 1 : K with cardinality\nKϵ. We consider two cases:\n22\n2\n4\n6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {βr\n(k) ∗ }r∈[R], k∈S (without outlier tasks)\n0.0\n0.2\n0.4\n0.6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nMaximum mis−clustering error (without outlier tasks)\n2\n4\n6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {βr\n(k) ∗ }r∈[R], k∈S (with 2 outlier tasks)\n0.0\n0.2\n0.4\n0.6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nMaximum mis−clustering error (with 2 outlier tasks)\nmethod\nMTL−GMM\nPooled−GMM\nSingle−task−GMM\nFigure S.5: The performance of different methods in Simulation 2 under different out-\nlier proportions.\nThe upper panel shows the performance without outlier tasks (ϵ =\n0), and the lower panel shows the performance with 2 outlier tasks (ϵ = 0.2).\nh\nchanges from 0 to 10 with increment 1.\nEstimation error of {β(k)∗\nr\n}r∈[R],k∈S stands\nfor maxk∈S minπ:[R]→[R] maxr=1:R ∥bβ(k)[T]\nr\n−(Σ(k)∗)−1(µ(k)∗\nπ(r) −µ(k)∗\nπ(1))∥2 and maximum mis-\nclustering error represents the maximum empirical mis-clustering error rate calculated on\nthe test set of tasks in S.\n(i) No outlier tasks (ϵ = 0), and h changes from 0 to 10 with increment 1;\n(ii) 2 outlier tasks (ϵ = 0.2), and h changes from 0 to 10 with increment 1.\nAlgorithm 4 is run with the alignment Algorithm 6. The results of it and other bench-\nmarks are reported in Figures S.5. The main message is the same as in Simulation 1:\nPooled-GMM is sensitive to outlier tasks and suffers from negative transfer when h is\nlarge, while MTL-GMM is robust to outliers and can adapt to the unknown similarity level\nh. Note that in this example, {µ(k)∗\nr\n}k∈S are similar, and {Σ(k)∗}k∈S are the same, therefore\nrunning the EM algorithm by pooling all the data when h is small without outliers may\nbe more effective than our MTL algorithm. This could explain why MTL-GMM performs\nslightly worse than Pooled-GMM in terms of maximum mis-clustering error when h is small\nand ϵ = 0.\n23\nWe also provide additional performance evaluations for the three methods in Simulation\n2. The results are presented in Figures S.6 and S.7. The main takeaway is the same as\nin the previous simulation example: Pooled-GMM is sensitive to outlier tasks and suffers\nfrom negative transfer when h is large, while MTL-GMM is robust to outliers and can\nadapt to the unknown similarity level h. The results verify the theoretical findings in the\nmulti-cluster case.\nS.3.1.3\nSimulation 3 of MTL\nIn the third simulation of MTL, we consider a different similarity structure among tasks\nin S and a different type of outlier tasks. For a multi-task learning problem with K = 10\ntasks, set the sample size of each task equal to 100. Let β(1)∗= (2.5, 0, 0, 0, 0), Σ(1)∗=\n(0.5|i−j|)5×5, and 1 ∈S, i.e., the first task is not an outlier task. We generate each w(k)∗\nfrom Unif(0.1, 0.9) for all k ∈S. For k ∈S\\{1}, we generate Σ(k)∗as\nΣ(k)∗=\n\n\n\n\n\n(0.5|i−j|)5×5,\nwith probability 1/2,\n(a|i−j|)5×5,\nwith probability 1/2,\n(S.3.13)\nand set β(k)∗= (Σ(k)∗)−1Σ(1)∗β(1)∗. Here, the value of a is determined by max{a ∈[0.5, 1) :\n∥β(k)∗−β(1)∗∥2 ≤h} for a given h. Let µ(k)∗\n2\n= Σ(k)∗β(k)∗and µ(k)∗\n1\n= 0, ∀k ∈S. In this\ngeneration process, µ(k)∗\n2\n= µ(1)∗\n2\n= Σ(1)∗β(1)∗= (5/2, 5/4, 5/8, 5/16, 5/32)⊤for all k ∈S.\nThe covariance matrix of tasks in S can differ. When k /∈S, we generate the data of\ntask k from two clusters with probability 1 −w(k)∗and w(k)∗, where w(k)∗∼Unif(0.1, 0.9).\nSamples from the second cluster follow N(µ(k)∗\n2\n, Σ(k)∗), with Σ(k)∗coming from (S.3.13),\nβ(k)∗= (−2.5, −2.5, −2.5, −2.5, −2.5)⊤, and µ(k)∗\n2\n= Σ(k)∗β(k)∗. For each sample from the\nfirst cluster, each component is independently generated from a t-distribution with degrees\nof freedom 4. In each replication, for given ϵ, the outlier task index set Sc is uniformly\nsampled from all subsets of 2 : K with cardinality Kϵ (since task 1 has been fixed in S).\nWe consider two cases:\n(i) No outlier tasks (ϵ = 0), and h changes from 0 to 10 with increment 1;\n(ii) 2 outlier tasks (ϵ = 0.2), and h changes from 0 to 10 with increment 1.\n24\n0.1\n0.2\n0.3\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {wr\n(k) ∗ }r∈[R], k∈S\n0\n2\n4\n6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {µr\n(k) ∗ }r∈[R], k∈S\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {Σ(k) ∗ }k∈S\n5\n10\n15\n20\n25\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {δr\n(k) ∗ }r∈[R], k∈S\n0.0\n0.1\n0.2\n0.3\n0.4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nAverage mis−clustering error\nmethod\nMTL−GMM\nPooled−GMM\nSingle−task−GMM\nFigure S.6: The performance of different methods in Simulation 2.(i) of multi-task learning,\nwith no outlier tasks (ϵ = 0), and h changing from 0 to 10 with increment 1. Estimation er-\nror of {w(k)∗\nr\n}r∈[R],k∈S stands for maxk∈S minπ:[R]→[R] maxr∈[R] | bw(k)[T]\nr\n−w(k)∗\nπ(r)|. Estimation er-\nror of {µ(k)∗\nr\n}r∈[R],k∈S stands for maxk∈S minπ:[R]→[R] maxr∈[R] ∥bµ(k)[T]\nr\n−µ(k)∗\nπ(r)∥2. Estimation\nerror of {Σ(k)∗}k∈S stands for maxk∈S ∥bΣ(k)[T] −Σ(k)∗∥2. Estimation error of {δ(k)∗\nr\n}r∈[R],k∈S\nstands for maxk∈S minπ:[R]→[R] maxr∈[R] |bδ(k)[T]\nr\n−(µ(k)∗\nπ(r) + µ(k)∗\nπ(1))⊤(Σ(k)∗)−1(µ(k)∗\nπ(r) −µ(k)∗\nπ(1))/2|.\nAverage mis-clustering error represents the average empirical mis-clustering error rate cal-\nculated on the test set of tasks in S.\n25\n0.1\n0.2\n0.3\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {wr\n(k) ∗ }r∈[R], k∈S\n0\n2\n4\n6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {µr\n(k) ∗ }r∈[R], k∈S\n1\n2\n3\n4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {Σ(k) ∗ }k∈S\n5\n10\n15\n20\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {δr\n(k) ∗ }r∈[R], k∈S\n0.0\n0.1\n0.2\n0.3\n0.4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nAverage mis−clustering error\nmethod\nMTL−GMM\nPooled−GMM\nSingle−task−GMM\nFigure S.7: The performance of different methods in Simulation 2.(ii) of multi-task learning,\nwith 2 outlier tasks (ϵ = 0.2), and h changing from 0 to 10 with increment 1. The meaning\nof each subfigure’s title is the same as in Figure S.6.\nWe implement the same three methods as in Simulation 1 and the results are reported in\nFigures S.8 and S.9. When there are no outlier tasks, both MTL-GMM and Pooled-GMM\nsignificantly outperform Single-task-GMM. Note that in this simulation, µ(k)∗\n1\n= µ(k′)∗\n1\nand\nµ(k)∗\n2\n= µ(k′)∗\n2\nfor all k ̸= k′ ∈[K], which might explain the phenomenon where Pooled-\nGMM outperforms MTL-GMM in estimating µ(k)∗\n1\nand µ(k)∗\n2\n’s. When there are two outlier\ntasks, Figure S.9 shows that Pooled-GMM performs much worse than Single-task-GMM on\n26\n0.1\n0.2\n0.3\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {w(k) ∗ }k∈S\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {µ1\n(k) ∗ }k∈S and {µ2\n(k) ∗ }k∈S\n0\n5\n10\n15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {β(k) ∗ }k∈S\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {Σ(k) ∗ }k∈S\n0\n5\n10\n15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {δ(k) ∗ }k∈S\n0.0\n0.2\n0.4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nMaximum mis−clustering error\n−0.05\n0.00\n0.05\n0.10\n0.15\n0.20\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nAverage mis−clustering error\nmethod\nMTL−GMM\nPooled−GMM\nSingle−task−GMM\nFigure S.8: The performance of different methods in Simulation 3.(i) of multi-task learning,\nwith no outlier tasks (ϵ = 0), and h changing from 0 to 10 with increment 1. Estimation\nerror of {β(k)∗}k∈S stands for maxk∈S(∥bβ(k)[T] −β(k)∗∥2 ∧∥bβ(k)[T] + β(k)∗∥2). Maximum\nmis-clustering error represents maximum empirical mis-clustering error rate calculated on\ntest set of tasks in S. The meaning of other subfigures’ titles is the same as in Figure S.3.\n27\n0.1\n0.2\n0.3\n0.4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {w(k) ∗ }k∈S\n0.0\n2.5\n5.0\n7.5\n10.0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {µ1\n(k) ∗ }k∈S and {µ2\n(k) ∗ }k∈S\n0\n5\n10\n15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {β(k) ∗ }k∈S\n2\n4\n6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {Σ(k) ∗ }k∈S\n0\n5\n10\n15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {δ(k) ∗ }k∈S\n0.0\n0.2\n0.4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nMaximum mis−clustering error\n0.0\n0.1\n0.2\n0.3\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nAverage mis−clustering error\nmethod\nMTL−GMM\nPooled−GMM\nSingle−task−GMM\nFigure S.9: The performance of different methods in Simulation 3.(ii) of multi-task learning,\nwith 2 outlier tasks (ϵ = 0.2), and h changing from 0 to 10 with increment 1. The meaning\nof each subfigure’s title is the same as in Figure S.8.\nmost of the estimation errors of GMM parameters as well as the mis-clustering error rate.\nIn contrast, MTL-GMM greatly improves the performance of Single-task-GMM, showing\nthe advantage of MTL-GMM when dealing with outlier tasks and heterogeneous covariance\n28\nmatrices.\nS.3.1.4\nSimulation of TL\nConsider a transfer learning problem with K = 10 source data sets, where all sources\nare from the same GMM. The setting is modified based on Simulation 1 of MTL. The\nsource and target sample sizes are equal to 100. For each of the source and target task,\nw(k)∗∼Unif(0.1, 0.9) and µ(k)∗\n1\n= −µ(k)∗\n2\n= (2, 2, 0p−2)⊤+ h/2 · (Σ(k)∗)−1u, where p = 15,\nΣ(k)∗= (0.2|i−j|)5×5, and u ∼Unif({u ∈Rp : ∥u∥2 = 1}). We consider the case that h\nchanges from 0 to 10 with increment 1.\nWe compare five different methods, including Target-GMM fitted on target data only,\nMTL-GMM fitted on all the data, MTL-GMM-center which fits MTL-GMM on source data\nand outputs the estimated “center” β\n[T] as the target estimate 6, Pooled-GMM which fits\na merged GMM on all the data, and our TL-GMM. The performance is evaluated by the\nestimation errors of w(0)∗, µ(0)∗\n1\n, µ(0)∗\n2\n, β(0)∗, δ(0)∗, and Σ(0)∗as well as the mis-clustering\nerror rate calculated on an independent test target data of size 500. Results are presented\nin Figure S.10.\nFigure S.10 shows that when h is small, the performances of MTL-GMM, MTL-GMM-\ncenter, Pooled-GMM, and TL-GMM are comparable, and all of them are much better\nthan Target-GMM. This is expected, because the sources are very similar to the target\nand can be easily used to improve the target task learning. As h keeps increasing, the\ntarget and sources become increasingly different. This is the phase where the knowledge of\nsources needs to be carefully transferred for the possible learning improvement on the target\ntask. As is clear from Figure S.10, MTL-GMM, MTL-GMM-center, and Pooled-GMM do\nnot handle heterogeneous resources well, thus outperformed by Target-GMM. By contrast,\nTL-GMM remains effective in transferring source knowledge to improve over Target-GMM;\nwhen h is very large so that sources are not useful anymore, TL-GMM is robust enough to\nstill have competitive performance compared to Target-GMM.\nFigure S.11 shows the results when there are two outlier tasks (ϵ = 0.2). It can be seen\nthat MTL-GMM is robust to outliers.\n6MTL-GMM-center only appears in the comparison of estimation error of β(k)∗’s.\n29\n30\n0.0\n0.1\n0.2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of w(0) ∗ \n0\n2\n4\n6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of µ1\n(0) ∗  and µ2\n(0) ∗ \n0\n4\n8\n12\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of β(0) ∗ \n0\n4\n8\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of Σ(0) ∗ \n−1\n0\n1\n2\n3\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of δ(0) ∗ \n0.0\n0.1\n0.2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nMis−clustering error\nmethod\nMTL−GMM\nMTL−GMM−center\nPooled−GMM\nTarget−GMM\nTL−GMM\nFigure S.10: The performance of different methods in the simulation of transfer learning,\nwith no outlier tasks (ϵ = 0) and h changing from 0 to 10 with increment 1. Estimation\nerror of w(0)∗stands for | bw(0)[T0] −w(0)∗|∧|1−bw(0)[T0] −w(0)∗|. Estimation error of µ(0)∗\n1\nand\nµ(0)∗\n2\nstands for minπ:[2]→[2] maxr∈[2] ∥bµ(0)[T0]\nr\n−µ(0)∗\nπ(r)∥2. Estimation error of β(0)∗stands for\n∥bβ(0)[T0]−β(0)∗∥2∧∥bβ(0)[T0]+β(0)∗∥2. Estimation error of Σ(k)∗stands for ∥bΣ(0)[T0]−Σ(0)∗∥2.\nEstimation error of δ(0)∗\nr\nstands for |bδ(0)[T0] −δ(0)∗| ∧|bδ(0)[T0] + δ(0)∗|. Mis-clustering error\nrepresents the empirical mis-clustering error rate calculated on the test set of the target\ndata.\n0.0\n0.1\n0.2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of w(0) ∗ \n0\n2\n4\n6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of µ1\n(0) ∗  and µ2\n(0) ∗ \n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of β(0) ∗ \n−4\n0\n4\n8\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of Σ(0) ∗ \n−1\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of δ(0) ∗ \n0.0\n0.1\n0.2\n0.3\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nMis−clustering error\nmethod\nMTL−GMM\nMTL−GMM−center\nPooled−GMM\nTarget−GMM\nTL−GMM\nFigure S.11: The performance of different methods in the simulation of transfer learning,\nwith 2 outlier tasks (ϵ = 0.2) and h changing from 0 to 10 with increment 1. The meaning\nof each subfigure’s title is the same as in Figure S.10.\n31\nS.3.1.5\nTuning parameters Cλ and Cλ0 in Algorithms 1, 4, and 7\nThe candidates of Cλ and Cλ0 values used in the 10-fold cross-validation are chosen through\na data-driven way. For Cλ in Algorithm 1, we first determine the smallest Cλ value which\nmakes all β(k) estimators identical, which is denoted as Cmax. Then the Cλ candidates are\nset to be the sequence from Cmax/50 and 2Cmax with equal logarithm distance. For Cλ0\nin Algorithm 7, we first determine the smallest Cλ0 value which makes the β(0) estimator\nequal to β\n[T], which is denoted as C′\nmax. Then the Cλ0 candidates are set to be the sequence\nfrom C′\nmax/50 and 2C′\nmax with equal logarithm distance.\nWe also run MTL-GMM with different Cλ values in Simulation 1 to test the impact of\nthe penalty parameter. The results are presented in Figure S.12. The values 1.29, 2.15,\n3.59, 5.99, and 10 are the last 5 elements in sequence from 0.1 to 10 with equal logarithm\ndistance. It can be seen that with small Cλ values like 1.29 and 2.15, the performance\nof MTL-GMM is similar to that of Single-task-GMM, although MTL-GMM-2.15 improves\nSingle-task-GMM a lot when h is small. With large Cλ values like 5.99 and 10, MTL-GMM\nperforms similarly to Pooled-GMM when h is small while suffering from negative transfer\nwhen h is large.\nHowever, as h continues to increase, the performance of MTL-GMM\nwith large Cλ values starts to improve and finally becomes similar to Single-task-GMM.\nThis phenomenon is in accordance with the theory, as the theory predicts that MTL-\nGMM achieves the same rate as Single-task-GMM for large h. The negative transfer effect\nof MTL-GMM with large Cλ could be caused by large unknown constants in the upper\nbound. Comparing Figure S.12 with figures in Sections 3 and S.3.1.1, we can see that\ncross-validation enhances the performance of MTL-GMM.\nS.3.1.6\nTuning parameter κ and κ0 in Algorithms 1, 4, and 7\nWe set κ = κ0 = 1/3 in Algorithms 1, 4, and 7. We run MTL-GMM with different κ values\nin Simulation 1 to test the impact of κ on the performance. The results are presented in\nFigure S.13. We tried κ = 0.1, 0.3, 0.5, 0.7, 0.9 in Algorithms 1. It can be seen that the\nlines representing MTL-GMM with different κ values highly overlap with each other, which\nshows that the performance of MTL-GMM is very robust to the choice of κ. In practice,\nwe take κ = 1/3 for convenience.\n32\n0.1\n0.2\n0.3\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {w(k) ∗ }k∈S\n0\n2\n4\n6\n8\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {µ1\n(k) ∗ }k∈S and {µ2\n(k) ∗ }k∈S\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {β(k) ∗ }k∈S\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {Σ(k) ∗ }k∈S\n0\n2\n4\n6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {δ(k) ∗ }k∈S\n−0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nMaximum mis−clustering error\n0.00\n0.05\n0.10\n0.15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nAverage mis−clustering error\nmethod\nMTL−GMM−1.29\nMTL−GMM−2.15\nMTL−GMM−3.59\nMTL−GMM−5.99\nMTL−GMM−10\nSingle−task−GMM\nPooled−GMM\nFigure S.12: The performance of different methods in Simulation 1 of multi-task learning,\nwith no outlier tasks (ϵ = 0) and h changing from 0 to 10 with increment 1. The meaning\nof each subfigure’s title is the same as in Figures 2 and S.3.\n33\n0.1\n0.2\n0.3\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {w(k) ∗ }k∈S\n0\n2\n4\n6\n8\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {µ1\n(k) ∗ }k∈S and {µ2\n(k) ∗ }k∈S\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {β(k) ∗ }k∈S\n0\n3\n6\n9\n12\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {Σ(k) ∗ }k∈S\n0\n2\n4\n6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nEstimation error of {δ(k) ∗ }k∈S\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nMaximum mis−clustering error\n0.00\n0.05\n0.10\n0.15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nh\nError\nAverage mis−clustering error\nmethod\nMTL−GMM−0.1\nMTL−GMM−0.3\nMTL−GMM−0.5\nMTL−GMM−0.7\nMTL−GMM−0.9\nSingle−task−GMM\nPooled−GMM\nFigure S.13: The performance of different methods in Simulation 1 of multi-task learning,\nwith no outlier tasks (ϵ = 0) and h changing from 0 to 10 with increment 1. The meaning\nof each subfigure’s title is the same as in Figures 2 and S.3.\nS.3.2\nReal-data analysis\nS.3.2.1\nHuman activity recognition\nHuman Activity Recognition (HAR) Using Smartphones Data Set contains the data col-\nlected from 30 volunteers when they performed six activities (walking, walking upstairs,\n34\nwalking downstairs, sitting, standing, and laying) wearing a smartphone (Anguita et al.,\n2013). Each observation has 561 time and frequency domain variables. Each volunteer can\nbe viewed as a task, and the sample size of each task varies from 281 to 409. The original\ndata set is available at UCI Machine Learning Repository: https://archive.ics.uci.\nedu/ml/datasets/human+activity+recognition+using+smartphones.\nHere, we first focus on two activities, standing and laying, and perform clustering\nwithout the label information, to test our method in the binary case. This is a binary\nMTL clustering problem with 30 tasks. The sample size of each task varies from 95 to 179.\nFor each task, in each replication, we use 90% of the samples as training data and hold\n10% of the samples as test data.\nWe first run a principal component analysis (PCA) on the training data of each task and\nproject both the training and test data onto the first 15 principal components. PCA has\noften been used for dimension reduction in pre-processing the HAR data set (Zeng et al.,\n2014; Walse et al., 2016; Aljarrah and Ali, 2019; Duan and Wang, 2023). We fit Single-\ntask-GMM on each task separately, Pooled-GMM on merged data from 30 tasks, and our\nMTL-GMM with the greedy label swapping alignment algorithm. The performance of the\nthree methods is evaluated by the mis-clustering error rate on the test data of all 30 tasks.\nThe maximum and average mis-clustering errors among the 30 tasks are calculated in each\nreplication. The mean and standard deviation of these two errors over 200 replications are\nreported on the left side of Table S.1. To better display the clustering performance on each\ntask, we further generate the box plot of mis-clustering errors of 30 tasks (averaged over\n200 replications) for each method in the left plot of Figure S.14. It is clear that MTL-GMM\noutperforms both Pooled-GMM and Single-task-GMM.\nBinary\nMulti-cluster\nMethod\nSingle-task\nPooled\nMTL\nSingle-task\nPooled\nMTL\nMax. error\n0.49 (0.02)\n0.40 (0.12)\n0.37 (0.09)\n0.51 (0.04)\n0.50 (0.04)\n0.51 (0.04)\nAvg. error\n0.28 (0.02)\n0.18 (0.18)\n0.04 (0.04)\n0.25 (0.01)\n0.35 (0.03)\n0.25 (0.01)\nTable S.1: Maximum and average mis-clustering errors and standard deviations (numbers\nin the parentheses) in binary and multi-cluster HAR data sets.\n35\n0.0\n0.1\n0.2\n0.3\n0.4\nMTL-GMM\nPooled-GMM\nSingle-task-GMM\nMethod\nMis-clustering error\nBinary\n0.1\n0.2\n0.3\n0.4\nMTL-GMM\nPooled-GMM\nSingle-task-GMM\nMethod\nMis-clustering error\nMulti-cluster\nmethod\nMTL-GMM\nPooled-GMM\nSingle-task-GMM\nFigure S.14: Box plots of mis-clustering errors of 30 tasks for each method for HAR data\nset. (Left: binary case; Right: multi-cluster case)\nNext, we consider all six activities and compare the performance of the three approaches\nusing the same sample-splitting strategy, to test our method in a multi-cluster scenario.\nNow the sample size of each task varies from 281 to 409. The maximum and average mis-\nclustering error rates and standard deviations over 200 replications are reported on the right\nside of Table S.1. We can see that Pooled-GMM might suffer from negative transfer with a\nworse performance than the other two methods, while MTL-GMM and Single-task-GMM\nhave similar performances.\nThe right plot in Figure S.14 reveals the same comparison\nresults.\nIn summary, the HAR data set exhibits different levels of similarity in binary and multi-\ncluster cases: tasks in the binary data are sufficiently similar so that Pooled-GMM achieves\na large margin of improvement over Single-task GMM, while tasks in the multi-cluster\ndata become much more heterogeneous, resulting in the degraded performance of Pooled-\nGMM compared to Single-task GMM. Nevertheless, our method MTL-GMM performs\neither competitively or better than the best of the two, regardless of the similarity level.\nThese results lend further support to the effectiveness of our method.\nS.3.2.2\nPen-based recognition of handwritten digits (PRHD)\nThe Pen-based Recognition of Handwritten Digits (PRHD) data set contains 250 samples\nfrom each of the 44 writers.\nEach of these writers was asked to write digits 0-9 on a\npressure-sensitive tablet with an integrated LCD display and a cordless stylus.\nThe x\n36\nand y tablet coordinates and pressure level values of the pen were recorded. After some\ntransformations, each observation has 16 features. The data set and more information\nabout it are available at UCI Machine Learning Repository: https://archive.ics.uci.\nedu/dataset/81/pen+based+recognition+of+handwritten+digits.\nSimilar to the previous real-data example, we first focus on a binary clustering problem\nby clustering observations of digits 8 and 9. The number of observations varies between\n47 and 48 among the 44 tasks, showing that this is a more balanced data set with a\nsmaller sample size (per dimension) than the HAR data. For each task, in each replication,\nwe use 90% of the samples as training data and hold 10% of the samples as test data.\nThe maximum and average mis-clustering error rates and standard deviations over 200\nreplications are reported on the left side of Table S.2, and the box plots of mis-clustering\nerrors of 44 tasks are shown in Figure S.15. We can see that Pooled-GMM and MTL-GMM\nperform similarly and are much better than Single-task-GMM.\nBinary\nMulti-cluster\nMethod\nSingle-task\nPooled\nMTL\nSingle-task\nPooled\nMTL\nMax. error\n0.32 (0.10)\n0.03 (0.07)\n0.03 (0.09)\n0.26 (0.07)\n0.37 (0.06)\n0.27 (0.07)\nAvg. error\n0.02 (0.01)\n0.00 (0.00)\n0.00 (0.02)\n0.03 (0.01)\n0.12 (0.01)\n0.03 (0.01)\nTable S.2: Maximum and average mis-clustering errors and standard deviations (numbers\nin the parentheses) in binary and multi-cluster PRHD data sets.\nNext, we consider the observations of digits 5-9, i.e.\na 5-class clustering problem.\nThe maximum and average mis-clustering error rates and standard deviations over 200\nreplications are reported on the right side of Table S.2, and the box plots of mis-clustering\nerrors of 44 tasks are shown in Figure S.15. In this multi-cluster case, MTL-GMM and\nSingle-task-GMM have similar performance which is better than that of Pooled-GMM. Like\nin the first real-data example, our method MTL-GMM adapts to the unknown similarity\nand is competitive with the best of the other two methods.\n37\n0.00\n0.05\n0.10\nMTL-GMM\nPooled-GMM\nSingle-task-GMM\nMethod\nMis-clustering error\nBinary\n0.0\n0.1\n0.2\n0.3\nMTL-GMM\nPooled-GMM\nSingle-task-GMM\nMethod\nMis-clustering error\nMulti-cluster\nmethod\nMTL-GMM\nPooled-GMM\nSingle-task-GMM\nFigure S.15: Box plots of mis-clustering errors of 44 tasks for each method for PRHD data\nset. (Left: binary case; Right: multi-cluster case)\nS.4\nTechnical Lemmas\nS.4.1\nGeneral lemmas\nDenote the unit ball Bp = {u ∈Rp : ∥u∥2 ≤1} and the unit sphere Sp−1 = {u ∈Rp :\n∥u∥2 = 1}.\nLemma 1 (Covering number of the unit ball under Euclidean norm, Example 5.8 in\nWainwright, 2019). Denote the ϵ-covering number of a unit ball Bp in Rp under Euclidean\nnorm as N(ϵ, Bp, ∥·∥2), where the centers of covering balls are required to be on the sphere.\nWe have (1/ϵ)p ≤N(ϵ, Bp, ∥· ∥2) ≤(1 + 2/ϵ)p.\nLemma 2 (Packing number of the unit sphere under Euclidean norm). Denote the ϵ-\npacking number of the unit sphere Sp−1 in Rp under Euclidean norm as M(ϵ, Sp−1, ∥· ∥2).\nWhen p ≥2, we have M(ϵ, Sp−1, ∥· ∥2) ≥N(ϵ, Bp−1, ∥· ∥2) ≥(1/ϵ)p−1.\nLemma 3 (Fano’s lemma, see Chapter 2 of Tsybakov, 2009, Chapter 15 of Wainwright,\n2019). Suppose (Θ, d) is a metric space and each θ in this space is associated with a prob-\nability measure Pθ. If {θj}N\nj=1 is an s-separated set (i.e. d(θj, θk) ≥s for any j ̸= k), and\nKL(Pθj, Pθk) ≤α log N, then\ninf\nbθ\nsup\nθ∈Θ\nPθ(d(bθ, θ) ≥s/2) ≥inf\nψ\nsup\nj=1:N\nPθj(ψ ̸= j) ≥1 −α −log 2\nlog N ,\nwhere ψ : X 7→ψ(X) ∈{1, . . . , N}.\n38\nLemma 4 (Packing number of the unit sphere in a quadrant under Euclidean norm). In\nRp, we can use a vector v ∈{±1}⊗p to indicate each quadrant Qv = {[0, +∞) · 1(vj =\n+1) + (−∞, 0) · 1(vj = −1)}⊗p. Then when p ≥2, there exists a quadrant Qv0 such that\nM(ϵ, Sp−1 ∩Qv0, ∥· ∥2) ≥(1\n2)p(1\nϵ)p−1.\nLemma 5. For one-dimensional Gaussian mixture variable Z ∼(1 −w)N(µ1, σ2) +\nwN(µ2, σ2) with (1 −w)µ1 + wµ2 = 0, it is a\nq\nσ2 + 1\n4|µ1 −µ2|2-subGaussian variable.\nThat means,\nEeλZ ≤exp\n\u001a1\n2λ2\n\u0012\nσ2 + 1\n4|µ1 −µ2|2\n\u0013\u001b\n.\nLemma 6 (Duan and Wang, 2023). Let\n({bθj}K\nk=1, bβ) = arg min\nθk,β∈Rp\n( K\nX\nk=1\nωkf (k)(θk) + √ωkλ∥β −θk∥2)\n)\n.\nSuppose there exists S ⊆1 : K such that the following conditions are satisfied:\n(i) For any k ∈S, fk is (θ∗\nk, M, ρ, L)-regular, that is\n• fk is convex and twice differentiable;\n• ρI ⪯∇2fk(θ) ⪯LI for all θ ∈B(θ∗\nk, M);\n• ∥∇fk(θ∗\nk)∥2 ≤ρM/2.\n(ii) minθ∈Rd maxk∈S{∥θ∗\nk −θ∥2} ≤h, P\nk∈Sc √ωk ≤ϵ′ P\nk∈S ωk/ maxk∈S √ωk, with ϵ′ =\n|Sc|\n|S| .\nThen we have the following conclusions:\n(i) ∥bθk −θ∗\nk∥2 ≤1\nρ(∥∇f (k)(θ∗\nk)∥2 + λ/√wk) for all k ∈S.\n(ii) If\n5ϱκw maxk∈S{√wk∥∇f (k)(θ∗\nk)∥2}\n1 −ϱϵ′\n< λ < ρM\n2 min\nk∈S\n√ωk,\nwhere ϱ = L/ρ, ϱϵ′ < 1, and maxk∈S √nk ·\nP\nk∈S\n√nk\nnS\n≤\nq\n|S| maxk∈S nk\nnS\n:= κw, then\n∥bθk−θ∗\nk∥2 ≤∥P\nk∈S ωk∇f (k)(θ∗\nk)∥2\nρ P\nk∈S ωk\n+\n6\n1 −ϱϵ′ min\n\u001a\n3ϱ2κwh,\n2λ\n5ρ√wk\n\u001b\n+\nλϵ′\nρ maxk∈S √ωk\n.\n39\nFurthermore, if we also have\nλ ≥15ϱκwL maxk∈S √ωkh\n1 −ϱϵ′\n,\nthen bθk = bβ for all k ∈S, and\nsup\nk∈S\n∥bθk −θ∗\nk∥2 ≤∥P\nk∈S ωk∇f (k)(θ∗\nk)∥2\nρ P\nk∈S ωk\n+ 2ϱκwh +\nλϵ′\nρ maxk∈S √ωk\n.\nLemma 7. Suppose\nbθ = arg min\nθ\n\u001a\nf (0)(θ) +\nλ\n√n0\n∥bθ −θ∥2\n\u001b\nwith some θ ∈Rp. Assume f (0) is convex and twice differentiable, and ρIp ≤∇2f (0)(θ) ≤\nLIp for any θ ∈Rp. Then\n(i) ∥bθ −θ∗∥2 ≤∇f(0)(θ)\nρ\n+\nλ\nρ√n0, for any θ∗∈Rp and λ ≥0;\n(ii) bθ = θ, if λ ≥2∥∇f (0)(θ∗)∥2√n0 and ∥θ −θ∗∥2 ≤(λ/√n0 −∥∇f (0)(θ∗)∥2)/L.\nS.5\nProofs\nS.5.1\nProofs of general lemmas\nS.5.1.1\nProof of Lemma 2\nThe second half inequality is due to Lemma 1. It suffices to show the first half inequality.\nFor any x = (x1, . . . , xp−1)⊤∈Bp−1, define xp =\nq\n1 −Pp−1\nj=1 x2\nj. Then we can define a\nmapping\nx ∈Bp−1 7→ex = (ex1, . . . , exp−1, exp) ∈Sp,\nwith exj = xj for j ≤p −1 and exp = ±xp. And it’s easy to see that for any x, y ∈Bp−1,\nwe have ∥x −y∥2 ≤∥ex −ey∥2. Therefore, if {exj}N\nj=1 is an ϵ-cover of Sp−1 under Euclidean\nnorm, then it {xj}N\nj=1 must be an ϵ-cover of Bp−1 under Euclidean norm. Then\nN(ϵ, Bp−1, ∥· ∥2) ≤N(ϵ, Sp−1, ∥· ∥2) ≤M(ϵ, Sp−1, ∥· ∥2).\n40\nS.5.1.2\nProof of Lemma 4\nIf {uj}N\nj=1 is an ϵ-packing of Sp−1 under Euclidean norm, then {uj}N\nj=1 ∩Qv must be an\nϵ-packing of Sp ∩Qv under Euclidean norm. Then by Lemma 2,\n2p\nmax\nv∈{±1}⊗p M(ϵ, Sp−1 ∩Qv, ∥· ∥2) ≥\nX\nv∈{±1}⊗p\nM(ϵ, Sp−1 ∩Qv, ∥· ∥2)\n≥M(ϵ, Sp−1, ∥· ∥2)\n≥\n\u00121\nϵ\n\u0013p−1\n,\nimplying\nmax\nv∈{±1}⊗p M(ϵ, Sp ∩Qv, ∥· ∥2) ≥\n\u00121\n2\n\u0013p \u00121\nϵ\n\u0013p−1\n.\nS.5.1.3\nProof of Lemma 5\nSuppose Z1 ∼N(µ1, σ2) ⊥⊥Z2 ∼N(µ2, σ2), then we can write Z = (1 −I)Z1 + IZ2 =\n(1−I)(Z1−µ1)+I(Z2−µ2)+[µ1(1−I)+µ2I], where I ∼Bernoulli(w) that is independent\nwith Z1 and Z2. Then\nEeλZ ≤Eeλ(1−I)(Z1−µ1)+λI(Z2−µ2) · Eeλ[µ1(1−I)+µ2I]\n≤EI\n\u0002\n(1 −I)EeλZ1 + IEeλZ2\u0003\n· Eeλ[µ1(1−I)+µ2I]\n≤exp\n\u001a1\n2σ2λ2 + 1\n8(µ2 −µ1)2λ2\n\u001b\n,\nwhere the last second inequality comes from Jensen’s inequality and the independence\nbetween Z1, Z2, and I. This completes the proof.\nS.5.1.4\nProof of Lemma 6\nThe result follows from Theorem A.2, Lemma B.1, and Claim B.1 in Duan and Wang\n(2023).\nS.5.2\nProof of Theorem 1\nDefine the contraction basin of one GMM as\nBcon(θ(k)∗) =\n\u001a\nθ = {w, β, δ} : wr ∈[cw/2, 1 −cw/2], ∥β −β(k)∗∥2 ≤Cb∆, δ = 1\n2β⊤(µ1 + µ2)\n41\nmax\nr=1:2 ∥µr −µ∗\nr∥2 ≤Cb∆\n\u001b\n,\nfor which we may shorthand as Bcon in the following. And given the index set S, two joint\ncontraction basins are defined as\nBJ,1\ncon({θ(k)∗}k∈S) =\n\b\n{θ(k)}k∈S = {(w(k), β(k), δ(k))}k∈S : θ(k) ∈Bcon(θ(k)∗)\n\t\n,\nBJ,2\ncon({θ(k)∗}k∈S) =\n\b\n{θ(k)}k∈S = {(w(k), β(k), δ(k))}k∈S : θ(k) ∈Bcon(θ(k)∗), β(k) ≡β for all k\n\t\n.\nFor simplicity, at some places, we will write them as BJ,1\ncon and BJ,2\ncon, respectively.\nFor θ = (w, β, δ) and θ′ = (w′, β′, δ′), define\nd(θ, θ′) = |w −w′| ∨∥β −β′∥2 ∨|δ −δ′|.\nAnd denote the minimum SNR ∆= mink∈S ∆(k).\nS.5.2.1\nLemmas\nFor GMM z ∼(1 −w∗)N(µ∗\n1, Σ∗) + w∗N(µ∗\n2, Σ∗) and any θ = (w, β, δ), define\nγθ(z) =\nw exp{β⊤z −δ}\n1 −w + w exp{β⊤z −δr},\nw(θ) = E[γθ(z)],\nµ1(θ) = E[(1 −γθ(z))z]\nE[1 −γθ(z)] ,\nµ2(θ) = E[γθ(z)z]\nE[γθ(z)] .\nLemma 8 (Contraction of binary GMMs, a special case of Lemma 37 when R = 2). When\nCb ≤cc−1/2\nΣ\nwith a small constant c > 0 and ∆≥C log(cΣMc−1\nw ) with a large constant\nC > 0, there exist positive constants C′ > 0 and C′′ > 0, for any θ ∈Bcon(θ(k)∗),\n|wr(θ) −w∗\nr| ≤C′exp{−C′′∆2} · d(θ, θ∗),\n∥µr(θ) −µ∗\nr∥2 ≤C′ exp{−C′′∆2} · d(θ, θ∗),\nwhere C′∆exp{−C′′∆2} ≤κ0 < 1 with a constant κ0.\nLemma 9. When h ≤Cb∆, BJ,2\ncon({θ(k)∗}k∈S) ̸= ∅.\nLemma 10 (Theorem 3 in Maurer and Pontil, 2021). Let f : X n →R and X =\n(X1, . . . , Xn) be a vector of independent random variables with values in a space X. Then\nfor any t > 0 we have\nP(f(X) −Ef(X) > t) ≤exp\n(\n−\nt2\n32e\n\r\rPn\ni=1 ∥fi(X)∥2\nψ2\n\r\r\n∞\n)\n,\n42\nwhere fi(X) as a random function of x is defined to be (fi(X))(x) := f(x1, . . . , xi−1, Xi, xi+1, . . . , Xn)−\nEXi[f(x1, . . . , xi−1, Xi, xi+1, . . . , Xn)], the sub-Gaussian norm ∥Z∥ψ2 := supd≥1{∥Z∥d/\n√\nd},\nand ∥Z∥d = (E|Z|d)1/d.\nLemma 11. Suppose Assumption 1 holds.\n(i) With probability at least 1 −C′K−2,\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\f\n1\nnk\nnk\nX\ni=1\nγθ(k)(z(k)\ni ) −E[γθ(k)(z(k))]\n\f\f\f\f\f ≲ξ(k)\nr p\nnk\n+\nr\nlog K\nnk\n,\nfor all k ∈S.\n(ii) With probability at least 1 −C′K−2e−C′′p,\nsup\n{θ(k)}k∈S∈BJ,2\ncon\nsup\n| ewk|≤1\n1\nnS\n\f\f\f\f\f\nX\nk∈S\newk\nnk\nX\ni=1\nh\nγθ(k)(z(k)\ni ) −E[γθ(k)(z(k))]\ni\f\f\f\f\f ≲\nr\np + K\nnS\n.\nLemma 12. Suppose Assumption 1 holds.\n(i) With probability at least 1 −C′(K−2 + K−2e−C′′p),\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\f\n1\nnk\nnk\nX\ni=1\n\u0002\n1 −γθ(k)(z(k)\ni )\n\u0003\n(z(k)\ni )⊤β(k)∗−E\n\u0002\n[1 −γθ(k)(z(k))](z(k))⊤β(k)∗\u0003\n\f\f\f\f\f\n≲ξ(k)\nr p\nnk\n+\nr\nlog K\nnk\n,\nfor all k ∈S.\n(ii) With probability at least 1 −C′K−2e−C′′p,\nsup\n{θ(k)}k∈S∈BJ,2\ncon\nsup\n| ewk|≤1\n1\nnS\n\f\f\f\f\f\nX\nk∈S\newk\nnk\nX\ni=1\nh\u0002\n1 −γθ(k)(z(k)\ni )\n\u0003\n(z(k)\ni )⊤β(k)∗−E\n\u0002\n[1 −γθ(k)(z(k))](z(k))⊤β(k)∗\u0003i\f\f\f\f\f\n≲\nr\np + K\nnS\n.\nLemma 13. Suppose Assumption 1 holds.\n43\n(i) With probability at least 1 −C′(K−2 + K−2e−C′′p),\nsup\nθ(k)∈Bcon\n\r\r\r\r\r\n1\nnk\nnk\nX\ni=1\nγθ(k)(z(k)\ni )z(k)\ni\n−E[γθ(k)(z(k))z(k)]\n\r\r\r\r\r\n2\n≲\nr\np + log K\nnk\n,\nfor all k ∈S.\n(ii) With probability at least 1 −C′K−2e−C′′p,\nsup\n{θ(k)}k∈S∈BJ,2\ncon\nsup\n| ewk|≤1\n1\nnS\n\r\r\r\r\r\nX\nk∈S\newk\nnk\nX\ni=1\nh\nγθ(k)(z(k)\ni )z(k)\ni\n−E[γθ(k)(z(k))z(k)]\ni\r\r\r\r\r\n2\n≲\nr\np + K\nnS\n.\n(iii) With probability at least 1 −C′K−2e−C′′p,\nsup\n{θ(k)}k∈S∈BJ,2\ncon\nsup\n| ewk|≤1\n1\nnS\n\r\r\r\r\r\nX\nk∈S\newk\nnk\nX\ni=1\nh\nγθ(k)(z(k)\ni ) −E[γθ(k)(z(k))]\ni\nµ(k)∗\n1\n\r\r\r\r\r\n2\n≲\nr\np + K\nnS\n.\nLemma 14. Suppose Assumption 1 holds.\n(i) With probability at least 1 −C′(K−2 + K−1e−C′′p),\n\r\r\r\r\r\n1\nnk\nnk\nX\ni=1\n\u0002\nz(k)\ni (z(k)\ni )⊤−E[z(k)\ni (z(k)\ni )⊤]\n\u0003\nβ(k)∗\n\r\r\r\r\r\n2\n≲\nr\np + log K\nnk\n,\nfor all k ∈S.\n(ii) With probability at least 1 −C′K−2e−C′′p,\n\r\r\r\r\r\n1\nnS\nX\nk∈S\nnk\nX\ni=1\n\u0002\nz(k)\ni (z(k)\ni )⊤−E[z(k)\ni (z(k)\ni )⊤]\n\u0003\nβ(k)∗\n\r\r\r\r\r\n2\n≲\nr p\nnS\n.\nS.5.2.2\nMain proof of Theorem 1\nThe proof of Theorem 1 consists of two cases.\nIn Case 1, we study the scenario h ≳\nq\np+log K\nmaxk∈S nk , where we take a fixed contraction radius. In this case, proving a single-task\nestimation error rate\nq\nK(p+log K)\nnS\nis sufficient, which is relatively straightforward. In Case 2,\nwe explore the scenario h ≲\nq\np+log K\nmaxk∈S nk , in which regime multi-task learning can outperform\nthe classical single-task learning. In this case, the classical finite-sample analysis of EM\nin Balakrishnan et al. (2017) and Cai et al. (2019), which uses a fixed contraction radius\nas we did in Case 1, does not work. This is because the heterogenous µ(k)∗\n1\nand µ(k)∗\n2\n44\nlead to an error of\nq\nK(p+log K)\nnS\nwhen estimating w(k)∗. This term\nq\nK(p+log K)\nnS\nultimately\naffects the estimation error of β(k)∗and δ(k)∗, preventing us from proving the improvement\nof multi-task learning over single-task learning. To resolve this issue, we creatively use\na “localization” strategy to adaptively shrink the contraction radius in each iteration.\nThis method effectively eliminates the term\nq\nK(p+log K)\nnS\n. By combining the two cases, we\ncomplete the proof.\nWLOG, in Assumptions 1.(iii) and 1.(iv), we assume\n• maxk∈S\n\u0000∥bβ(k)[0] −β(k)∗∥2 ∨∥bµ(k)[0]\n1\n−µ(k)∗\n1\n∥2 ∨∥bµ(k)[0]\n2\n−µ(k)∗\n2\n∥2\n\u0001\n≤C′ mink∈S ∆(k),\nwith a small constant C′ > 0;\n• maxk∈S | bw(k)[0] −w(k)∗| ≤cw/2.\n(I) Case 1: Let us consider the case that h ≥C\nq\np+log K\nmaxk∈S nk . Consider an event E defined to\nbe the intersection of the events in Lemmas 11.(i), 12.(i), 13.(i), and 14.(i), with ξ(k) = a\nlarge constant C, which satisfies P(E) ≥1−C′(K−2+K−2e−C′′p). Throughout the analysis\nin Case 1, we condition on E, therefore all the arguments hold with probability at least\n1 −C′(K−2 + K−2e−C′′p).\nConsider the case t = 1. Lemma 6 tells us that when λ[t] ≥C maxk∈S{√nk∥bΣ(k)[t]β(k)∗−\n(bµ(k)[t]\n2\n−bµ(k)[t]\n1\n)∥2}, we have\n∥bβ(k)[t] −β(k)∗∥2 ≲\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n[bΣ(k)[t]β(k)∗−(bµ(k)[t]\n2\n−bµ(k)[t]\n1\n)]\n\r\r\r\r\r\n2\n+ h ∧λ[t]\n√nk\n+ ϵ\nλ[t]\n√maxk=1:K nk\n.\n(S.5.14)\nAnd if further λ[t] ≥C maxk∈S √nkh, we have (S.5.14) holds with bβ(k)[t] = β\n[t] for all k ∈S.\nNote that\n∥bΣ(k)[t]β(k)∗−(bµ(k)[t]\n2\n−bµ(k)[t]\n1\n)∥2 ≤∥(bΣ(k)[t]−Σ(k)∗)β(k)∗∥2+∥bµ(k)[t]\n2\n−bµ(k)[t]\n1\n−µ(k)∗\n2\n+µ(k)∗\n1\n∥2.\n(S.5.15)\nAnd the first term on the RHS can be controlled as\n∥(bΣ(k)[t] −Σ(k)∗)β(k)∗∥2\n≤\n\r\r\r\r\r\n1\nnk\nnk\nX\ni=1\n\u0002\nz(k)\ni (z(k)\ni )⊤−E[z(k)\ni (z(k)\ni )⊤]\n\u0003\nβ(k)∗\n\r\r\r\r\r\n2\n|\n{z\n}\n1\n45\n+\n\r\r\r\n\u0002\n(1 −bw(k)[t])bµ(k)[t]\n1\n(bµ(k)[t]\n1\n)⊤−(1 −w(k)∗)µ(k)∗\n1\n(µ(k)∗\n1\n)⊤\u0003\nβ(k)∗\r\r\r\n2\n|\n{z\n}\n2\n+\n\r\r\r\n\u0002\nbw(k)[t]bµ(k)[t]\n2\n(bµ(k)[t]\n2\n)⊤−w(k)∗µ(k)∗\n2\n(µ(k)∗\n2\n)⊤\u0003\nβ(k)∗\r\r\r\n2\n|\n{z\n}\n3\n.\n(S.5.16)\nConditioned on E, we have\n1 ≲\nr\np + log K\nnk\n,\nAnd\n2 ≤\n\r\r\r(1 −bw(k)[t])(bµ(k)[t]\n1\n−µ(k)∗\n1\n) · (bµ(k)[t]\n1\n)⊤β(k)∗\r\r\r\n2\n|\n{z\n}\n2 .1\n+\n\r\r\r\n\u0002\n(1 −bw(k)[t])µ(k)∗\n1\n(bµ(k)[t]\n1\n)⊤−(1 −w(k)∗)µ(k)∗\n1\n(µ(k)∗\n1\n)⊤\u0003\nβ(k)∗\r\r\r\n2\n|\n{z\n}\n2 .2\n,\nwhere\n2 .2 ≤\n\r\r\r( bw(k)[t] −w(k)∗)µ(k)∗\n1\n(bµ(k)[t]\n1\n)⊤β(k)∗\r\r\r\n2 +\n\r\r\r(1 −bw(k)[t])µ(k)∗\n1\n(bµ(k)[t]\n1\n−µ(k)∗\n1\n)⊤β(k)∗\r\r\r\n2 .\n(S.5.17)\nBefore we discuss how to control the terms on the RHS, let us first try to control | bw(k)[t] −\nw(k)∗| as it will be used to bound the existing terms. Note that by Lemma 8,\n| bw(k)[t] −w(k)∗| ≤|w(k)(bθ(k)[t−1]) −w(k)∗| + | bw(k)[t] −w(k)(bθ(k)[t−1])|\n≤κ0d(bθ(k)[t−1], θ(k)∗) +\n\f\f\f\f\f\n1\nnk\nnk\nX\ni=1\nγbθ(k)[t−1](z(k)\ni ) −Ez(k)[γbθ(k)[t−1](z(k))]\n\f\f\f\f\f\n≲κ0d(bθ(k)[t−1], θ(k)∗) +\nr\np + log K\nnk\n(S.5.18)\n≤c,\nwhere c is a small constant. By Lemma 8 again,\n∥bµ(k)[t]\n1\n−µ(k)∗\n1\n∥2 =\n\r\r\r\r\r\n1\nnk\nPnk\ni=1(1 −γbθ(k)[t−1](z(k)\ni ))z(k)\ni\n1 −bw(k)[t]\n−Ez(k)[(1 −γbθ(k)[t−1](z(k)))z(k)]\n1 −w(k)(bθ(k)[t−1])\n\r\r\r\r\r\n2\n≤\n\r\r\r\r\r\n1\nnk\nPnk\ni=1(1 −γbθ(k)[t−1](z(k)\ni ))z(k)\ni\n−Ez(k)[(1 −γbθ(k)[t−1](z(k)))z(k)]\n1 −bw(k)[t]\n\r\r\r\r\r\n2\n+\n\r\r\r\r\r\nEz(k)[(1 −γbθ(k)[t−1](z(k)))z(k)]\n(1 −bw(k)[t])(1 −w(k)(bθ(k)[t−1]))\n( bw(k)[t] −w(k)(bθ(k)[t−1]))\n\r\r\r\r\r\n2\n46\n≲\n\r\r\r\r\r\n1\nnk\nnk\nX\ni=1\n(1 −γbθ(k)[t−1](z(k)\ni ))z(k)\ni\n−Ez(k)[(1 −γbθ(k)[t−1](z(k)))z(k)]\n\r\r\r\r\r\n2\n+ | bw(k)[t] −w(k)∗| + κ0d(bθ(k)[t−1], θ(k)∗)\n≲κ0d(bθ(k)[t−1], θ(k)∗) +\nr\np + log K\nnk\n≤Cb∆.\nTherefore, we can bound the RHS of (S.5.17) as\n2 .2 ≲| bw(k)[t] −w(k)∗| + ∥bµ(k)[t]\n1\n−µ(k)∗\n1\n∥2 ≲κ0d(bθ(k)[t−1], θ(k)∗) +\nr\np + log K\nnk\n.\n(S.5.19)\nSimilarly, we have\n2 .1 ≲∥bµ(k)[t]\n1\n−µ(k)∗\n1\n∥2 ≲κ0d(bθ(k)[t−1], θ(k)∗) +\nr\np + log K\nnk\n,\n(S.5.20)\nCombining (S.5.19) and (S.5.20), we have\n2 ≲κ0d(bθ(k)[t−1], θ(k)∗) +\nr\np + log K\nnk\n,\nSimilarly, we can bound 3 in the same way, and get\n3 ≲κ0d(bθ(k)[t−1], θ(k)∗) +\nr\np + log K\nnk\n,\nHence\n∥(bΣ(k)[t] −Σ(k)∗)β(k)∗∥2 ≲κ0d(bθ(k)[t−1], θ(k)∗) +\nr\np + log K\nnk\n,\nAnd the second term on the RHS of (S.5.15) satisfies\n∥bµ(k)[t]\n2\n−bµ(k)[t]\n1\n−µ(k)∗\n2\n+µ(k)∗\n1\n∥2 ≲∥bµ(k)[t]\n2\n−µ(k)∗\n2\n∥2∨∥bµ(k)[t]\n1\n−µ(k)∗\n1\n∥2 ≲κ0d(bθ(k)[t−1], θ(k)∗)+\nr\np + log K\nnk\n.\n(S.5.21)\nAll together, we have\n∥bΣ(k)[t]β(k)∗−(bµ(k)[t]\n2\n−bµ(k)[t]\n1\n)∥2 ≲κ0d(bθ(k)[t−1], θ(k)∗) +\nr\np + log K\nnk\n.\n(S.5.22)\nThis implies that λ[t] = Cλ\n√p + log K + κλ[0] ≥C maxk∈S{√nk∥bΣ(k)[t]β(k)∗−(bµ(k)[t]\n2\n−\nbµ(k)[t]\n1\n)∥2}, therefore by (S.5.14),\n∥bβ(k)[t]−β(k)∗∥2 ≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗)+\ns\nK(p + log K)\nnS\n+ϵ\nλ[t]\n√maxk=1:K nk\n, (S.5.23)\n47\nAnd by (S.5.18),\nX\nk∈S\nnk\nnS\n| bw(k)[t] −w(k)∗| ≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\ns\nK(p + log K)\nnS\n.\n(S.5.24)\nAlso,\n|bδ(k)[t] −δ(k)∗| = 1\n2\n\r\r\r( bβ(k)[t])⊤(bµ(k)[t]\n1\n+ bµ(k)[t]\n2\n) −(β(k)∗)⊤(µ(k)∗\n1\n+ µ(k)∗\n2\n)\n\r\r\r\n2\n≲∥bβ(k)[t] −β(k)∗∥2 + ∥bµ(k)[t]\n1\n−µ(k)∗\n1\n∥2 + ∥bµ(k)[t]\n2\n−µ(k)∗\n2\n∥2\n≲κ0d(bθ(k)[t−1], θ(k)∗) +\nr\np + log K\nnk\n,\n(S.5.25)\nwhich entails that\nX\nk∈S\nnk\nnS\n|bδ(k)[t] −δ(k)∗| ≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\ns\nK(p + log K)\nnS\n.\n(S.5.26)\nCombining (S.5.23), (S.5.24), and (S.5.26), we have\nX\nk∈S\nnk\nnS\nd(bθ(k)[t], θ(k)∗) ≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\ns\nK(p + log K)\nnS\n+ ϵ\nλ[t]\n√maxk=1:K nk\n.\n(S.5.27)\nAlso,\nmax\nk∈S\n\b√nkd(bθ(k)[t], θ(k)∗)\n\t\n≲κ0 max\nk∈S\n\b√nkd(bθ(k)[t−1], θ(k)∗)\n\t\n+ λ[t].\n(S.5.28)\nWhen we assume (S.5.23), (S.5.27), (S.5.28) hold for all t = 1 : t′, via the same analysis\nwe will have (S.5.22) hold again for t = t′ + 1. Hence\nmax\nk∈S\n\b√nk∥bΣ(k)[t′+1]β(k)∗−(bµ(k)[t′+1]\n2\n−bµ(k)[t′+1]\n1\n)∥2\n\t\n≲κ0 max\nk∈S\n\b√nkd(bθ(k)[t′], θ(k)∗)\n\t\n+\np\np + log K.\nThen by (S.5.28) when t = t′,\nκ0 max\nk∈S\n\b√nkd(bθ(k)[t′], θ(k)∗)\n\t\n+\np\np + log K ≲κ2\n0 max\nk∈S\n\b√nkd(bθ(k)[t′−1], θ(k)∗)\n\t\n+\np\np + log K + κ0λ[t′]\n≤κ0λ[t′] +\np\np + log K\n≤λ[t′+1],\nwhere we need κ ≥Cκ0 with a large constant C > 0. Recall that κ ∈(0, 1) is one of\nthe tuning parameters in the update formula of λ[t]. Therefore we can follow the same\n48\narguments as above to obtain (S.5.18), (S.5.21), (S.5.23), (S.5.25), (S.5.27), (S.5.28) for\nt = t′ + 1.\nSo far, we have shown that (S.5.18), (S.5.21), (S.5.23), (S.5.25), (S.5.27), (S.5.28) hold\nfor any t. By the update formula of λ[t], when t ≥1, we have\nλ[t] = 1 −κt\n1 −κ Cλ\np\np + log K + κt−1λ[0].\nTherefore by (S.5.27),\nX\nk∈S\nnk\nnS\nd(bθ(k)[t], θ(k)∗) ≤Cκ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) + C′\ns\nK(p + log K)\nnS\n+ C′\nr\nK\nnS\nλ[t]\n≤(Cκ0)t X\nk∈S\nnk\nnS\nd(bθ(k)[0], θ(k)∗) + C′\ns\nK(p + log K)\nnS\n+ C′\nr\nK\nnS\nt\nX\nt′=1\nλ[t′] · (Cκ0)t−t′\n≤(Cκ0)t X\nk∈S\nnk\nnS\nd(bθ(k)[0], θ(k)∗) + C′\ns\nK(p + log K)\nnS\n+ C′\nr\nK\nnS\nt\nX\nt′=1\nλ[t′] · κt−t′\n≤C′′tκt + C′′\ns\nK(p + log K)\nnS\n.\n(S.5.29)\nConsider a new event E′ defined to be the intersection of the events in Lemmas 11.(i),\n12.(i), 13.(i), and 14.(i), with ξ(k) = C\nq\nnk\nmaxk∈S nk , which satisfies P(E′) ≥1 −C′(K−2 +\nK−2e−C′′p). Throughout the following analysis in Case 1, we condition on E ∩E′, therefore\nall the arguments hold with probability at least 1 −C′(K−2 + K−2e−C′′p). When h ≥\nC\nq\np+log K\nmaxk∈S nk , since nS ≳K maxk∈S nk, we have\nq\nK(p+log K)\nnS\n≲\nq\np+log K\nmaxk∈S nk ≲h ∧\nq\np+log K\nnk\n.\nFurthermore, when t ≥C′ log\n\u0000 maxk∈S nk\nmink∈S nk\n\u0001\nwith a large C′ > 0, we have ξ(k)q\np\nnk ≲h ∧\nq\np+log K\nnk\n+ϵ\nq\np+log K\nmaxk=1:K nk +\nq\nlog K\nnk\nand tκt+C\nq\nK(p+log K)\nnS\n+Cϵ\nq\np+log K\nmaxk=1:K nk +C\nq\nlog K\nnk\n≤ξ(k),\nwhere we used the fact nS ≳K maxk∈S nk again to get the second inequality.\nPlugging (S.5.29) back into (S.5.23), we have\n∥bβ(k)[t] −β(k)∗∥2 ≤C′′tκt + C′′\ns\nK(p + log K)\nnS\n+ C′′ξ(k)\nr p\nnk\n+ C′′\nr\nlog K\nnk\n≤Ctκt + C\ns\nK(p + log K)\nnS\n+ C · h ∧\nr\np + log K\nnk\n+ Cϵ\nr\np + log K\nnk\n+ C\nr\nlog K\nnk\n49\n≤Ctκt + C · h ∧\nr\np + log K\nnk\n+ Cϵ\nr\np + log K\nmaxk=1:K nk\n+ C\nr\nlog K\nnk\n(S.5.30)\nThen by (S.5.18),\n| bw(k)[t] −w(k)∗| ≲κ0d(bθ(k)[t−1], θ(k)∗) + ξ(k)\nr p\nnk\n+\nr\nlog K\nnk\n≲κ0∥bβ(k)[t−1] −β(k)∗∥2 + κ0| bw(k)[t−1] −w(k)∗| ∨|bδ(k)[t−1] −δ(k)∗| + ξ(k)\nr p\nnk\n+\nr\nlog K\nnk\n≲Ctκt + κ0| bw(k)[t−1] −w(k)∗| ∨|bδ(k)[t−1] −δ(k)∗| + h ∧\nr\np + log K\nnk\n+ ϵ\nr\np + log K\nmaxk=1:K nk\n+\nr\nlog K\nnk\n.\nSimilarly, by (S.5.25),\n|bδ(k)[t−1] −δ(k)∗| ≲Ctκt + κ0| bw(k)[t−1] −w(k)∗| ∨|bδ(k)[t−1] −δ(k)∗|\n+ h ∧\nr\np + log K\nnk\n+ ϵ\nr\np + log K\nmaxk=1:K nk\n+\nr\nlog K\nnk\n.\nTherefore,\n| bw(k)[t] −w(k)∗| ∨|bδ(k)[t] −δ(k)∗| ≤Ctκt + Cκ0| bw(k)[t−1] −w(k)∗| ∨|bδ(k)[t−1] −δ(k)∗|\n+ h ∧\nr\np + log K\nnk\n+ ϵ\nr\np + log K\nnk\n+\nr\nlog K\nnk\n≤C′′t2κt + C′′ · h ∧\nr\np + log K\nnk\n+ C′′ϵ\nr\np + log K\nmaxk=1:K nk\n+ C′′\nr\nlog K\nnk\n.\nCombine it with (S.5.30), we obtain that\nd(bθ(k)[t], θ(k)∗) ≤C′′t2κt + C′′ · h ∧\nr\np + log K\nnk\n+ C′′ϵ\nr\np + log K\nmaxk=1:K nk\n+ C′′\nr\nlog K\nnk\n.\nPlugging this back into (S.5.20), we get\n∥bµ(k)[t]\n1\n−µ(k)∗\n1\n∥2 ≤C′′t2κt + C′′\nr\np + log K\nnk\n.\nAnd the same bound holds for ∥bµ(k)[t]\n2\n−µ(k)∗\n2\n∥2 as well. The same bound for ∥bΣ(k)[t]−Σ(k)∗∥2\ncan be obtained in the same spirit as in (S.5.16).\n50\n(II) Case 2: We now focus on the case that h ≤C\nq\np+log K\nmaxk∈S nk . As mentioned at the begin-\nning of this proof, we need to adaptively shrink the radius of the contraction basin to prove\nthe desired convergence rate. The analysis of Case 2 can be divided into two stages. In\nthe first stage, we use the same fixed contraction radius as in Case 1 and follow the same\nanalysis until the iterative error t2κt has reduced to the single-task error\nq\nK(p+log K)\nnS\n. In\nthe second stage, we apply the localization argument to shrink the contraction basin until\nwe achieve the desired rate of convergence.\nSimilar to Case 1, we consider an event E defined to be the intersection of the events\nin Lemmas 11, 12, 13, and 14, with ξ(k) = a large constant C, which satisfies P(E) ≥\n1 −C′(K−2 + K−2e−C′′p). Throughout the analysis in Case 2, we condition on E, therefore\nall the arguments hold with probability at least 1 −C′(K−2 + K−2e−C′′p).\nConsider t0 as the number of iterations in the first stage which satisfies t2\n0κt0 ≍\nq\nK(p+log K)\nnS\n.\nWhen t = 1 : t0, we can go through the same analysis as in Case 1, and show that condi-\ntioned on E,\nX\nk∈S\nnk\nnS\nd(bθ(k)[t], θ(k)∗) ≲C′′tκt + C′′\ns\nK(p + log K)\nnS\n,\nand\nd(bθ(k)[t], θ(k)∗) ≤C′′t2κt + C′′\ns\nK(p + log K)\nnS\n+ C′′ · h ∧\nr\np + log K\nnk\n+ C′′ϵ\nr\np + log K\nmaxk=1:K nk\n+ C′′\nr\nlog K\nnk\n,\n∥bµ(k)[t]\n1\n−µ(k)∗\n1\n∥2 ∨∥bµ(k)[t]\n2\n−µ(k)∗\n2\n∥2 ∨∥bΣ(k)[t] −Σ(k)∗∥2 ≤C′′t2κt + C′′\nr\np + log K\nnk\n.\nSince t2\n0κt0 ≍\nq\nK(p+log K)\nnS\n, the rates above are the desired rates. In the following, we will\nderive the results for the case t ≥t0 + 1.\nDefine\nξ(k)\nt0 = C′′\ns\nK(p + log K)\nnS\n+ C′′ · h ∧\nr\np + log K\nnk\n+ C′′ϵ\nr\np + log K\nmaxk=1:K nk\n+ C′′\nr\nlog K\nnk\n,\n51\nξt0 =\nX\nk∈S\nnk\nnS\nξ(k)\nt0 ≤C′′\ns\nK(p + log K)\nnS\n+ C′′ · h ∧\ns\nK(p + log K)\nnS\n+ C′′ϵ\ns\nK(p + log K)\nnS\n+ C′′\nr\nK log K\nnS\n.\nConsider an event Et0 defined to be the intersection of the events in Lemmas 11, 12, 13, and\n14, with ξ(k) = ξ(k)\nt0 , which satisfies P(Et0) ≥1 −C′(K−2 + K−2e−C′′p). In the following, we\ncondition on E ∩Et0, therefore all the arguments hold with probability at least 1 −C′K−1.\nLet t = t0 + 1. Since λ[t−1] ≥C√p + log K ≥C maxk∈S √nkh, by Lemma 6, we also\nhave bβ(k)[t−1] = β\n[t−1] for all k ∈S. Similar to (S.5.18),\n| bw(k)[t] −w(k)∗| ≤|w(k)(bθ(k)[t−1]) −w(k)∗| + | bw(k)[t] −w(k)(bθ(k)[t−1])|\n≤κ0d(bθ(k)[t−1], θ(k)∗) +\n\f\f\f\f\f\n1\nnk\nnk\nX\ni=1\nγbθ(k)[t−1](z(k)\ni ) −Ez(k)[γbθ(k)[t−1](z(k))]\n\f\f\f\f\f\n≤κ0d(bθ(k)[t−1], θ(k)∗) + C′′ξ(k)\nt−1\nr p\nnk\n+ C′′\nr\nlog K\nnk\n≤κ0d(bθ(k)[t−1], θ(k)∗) + κ0ξ(k)\nt−1 + C′′\nr\nlog K\nnk\n.\nThis implies that\nX\nk∈S\nnk\nnS\n| bw(k)[t] −w(k)∗| ≤κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) + κ0\nX\nk∈S\nnk\nnS\nξ(k)\nt−1 + C′′\nr\nK log K\nnS\n.\nAnd by Lemma 6,\n∥bβ(k)[t] −β(k)∗∥2 ≲\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n[bΣ(k)[t]β(k)∗−(bµ(k)[t]\n2\n−bµ(k)[t]\n1\n)]\n\r\r\r\r\r\n2\n+ h ∧λ[t]\n√nk\n+ ϵ λ[t]\n√nk\n≲\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n[bΣ(k)[t]β(k)∗−(bµ(k)[t]\n2\n−bµ(k)[t]\n1\n)]\n\r\r\r\r\r\n2\n+ h ∧\nr\np + log K\nnk\n+ ϵ\nr\np + log K\nmaxk=1:K nk\n,\n(S.5.31)\nwhere\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n[bΣ(k)[t]β(k)∗−(bµ(k)[t]\n2\n−bµ(k)[t]\n1\n)]\n\r\r\r\r\r\n2\n≤\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n(bΣ(k)[t] −Σ(k)∗)β(k)∗\n\r\r\r\r\r\n2\n+\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n\u0000bµ(k)[t]\n2\n−bµ(k)[t]\n1\n−µ(k)∗\n2\n+ µ(k)∗\n1\n\u0001\n\r\r\r\r\r\n2\n.\n52\nAnd the first term on the RHS can be controlled as\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n(bΣ(k)[t] −Σ(k)∗)β(k)∗\n\r\r\r\r\r\n2\n≤\n\r\r\r\r\r\n1\nnS\nX\nk∈S\nnk\nX\ni=1\n\u0002\nz(k)\ni (z(k)\ni )⊤−E[z(k)\ni (z(k)\ni )⊤]\n\u0003\nβ(k)∗\n\r\r\r\r\r\n2\n|\n{z\n}\n4\n+\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n\u0002\n(1 −bw(k)[t])bµ(k)[t]\n1\n(bµ(k)[t]\n1\n)⊤−(1 −w(k)∗)µ(k)∗\n1\n(µ(k)∗\n1\n)⊤\u0003\nβ(k)∗\n\r\r\r\r\r\n2\n|\n{z\n}\n5\n+\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n\u0002\nbw(k)[t]bµ(k)[t]\n2\n(bµ(k)[t]\n2\n)⊤−w(k)∗µ(k)∗\n2\n(µ(k)∗\n2\n)⊤\u0003\nβ(k)∗\n\r\r\r\r\r\n2\n|\n{z\n}\n6\n.\nConditioned on E, we have\n4 ≲\nr p\nnS\n.\nAnd\n5 ≤\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n(1 −bw(k)[t])(bµ(k)[t]\n1\n−µ(k)∗\n1\n) · (bµ(k)[t]\n1\n)⊤β(k)∗\n\r\r\r\r\r\n2\n|\n{z\n}\n5 .1\n+\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n\u0002\n(1 −bw(k)[t])µ(k)∗\n1\n(bµ(k)[t]\n1\n)⊤−(1 −w(k)∗)µ(k)∗\n1\n(µ(k)∗\n1\n)⊤\u0003\nβ(k)∗\n\r\r\r\r\r\n2\n|\n{z\n}\n5 .2\n,\nwhere\n5 .2 ≤\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n( bw(k)[t] −w(k)∗)µ(k)∗\n1\n(bµ(k)[t]\n1\n)⊤β(k)∗\n\r\r\r\r\r\n2\n+\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n(1 −bw(k)[t])µ(k)∗\n1\n(bµ(k)[t]\n1\n−µ(k)∗\n1\n)⊤β(k)∗\n\r\r\r\r\r\n2\n.\nFor the first term, we have\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n( bw(k)[t] −w(k)∗)µ(k)∗\n1\n(bµ(k)[t]\n1\n)⊤β(k)∗\n\r\r\r\r\r\n2\n≲\nX\nk∈S\nnk\nnS\n|w(k)(bθ(k)[t−1]) −w(k)∗|\n+ 1\nnS\n\r\r\r\r\r\nX\nk∈S\nnk\nX\ni=1\nn\nγbθ(k)[t−1](z(k)\ni )µ(k)∗\n1\n(bµ(k)[t]\n1\n)⊤β(k)∗−Ez(k)\n\u0002\nγbθ(k)[t−1](z(k))µ(k)∗\n1\n(bµ(k)[t]\n1\n)⊤β(k)∗\u0003o\r\r\r\r\r\n2\n53\n≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗)\n+ 1\nnS\nsup\n| ewk|≤U\n\r\r\r\r\r\nX\nk∈S\nnk\nX\ni=1\newk\nn\nγbθ(k)[t−1](z(k)\ni )µ(k)∗\n1\n−Ez(k)\n\u0002\nγbθ(k)[t−1](z(k))µ(k)∗\n1\n\u0003o\r\r\r\r\r\n2\n,\nwhere U > 0 is some constant such that U ≥∥bµ(k)[t]\n1\n−µ(k)∗\n1\n∥2∥β(k)∗∥2 +∥µ(k)∗\n1\n∥2∥β(k)∗∥2 ≥\n|(bµ(k)[t]\n1\n)⊤β(k)∗| under event Et0. Note that the last inequality holds because the expectation\nEz(k) is w.r.t. z(k) which is independent of bµ(k)[t]. By Lemma 13.(iii) and the definition of\nEt0, the second term can be bounded as\n1\nnS\nsup\n| ewk|≤U\n\r\r\r\r\r\nX\nk∈S\nnk\nX\ni=1\newk\nn\nγbθ(k)[t−1](z(k)\ni )µ(k)∗\n1\n−Ez(k)\n\u0002\nγbθ(k)[t−1](z(k))µ(k)∗\n1\n\u0003o\r\r\r\r\r\n2\n≲\nr\np + K\nnS\n.\nTherefore,\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n( bw(k)[t] −w(k)∗)µ(k)∗\n1\n(bµ(k)[t]\n1\n)⊤β(k)∗\n\r\r\r\r\r\n2\n≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\nr\np + K\nnS\n.\nOn the other hand, by simple calculations, we have\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n(1 −bw(k)[t])µ(k)∗\n1\n(bµ(k)[t]\n1\n−µ(k)∗\n1\n)⊤β(k)∗\n\r\r\r\r\r\n2\n≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) + sup\n| ewk|≤U\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\newk[w(k)(bθ(k)[t−1]) −bw(k)[t]]µ(k)∗\n1\n\r\r\r\r\r\n2\n+ 1\nnS\nsup\n| ewk|≤U′\n\r\r\r\r\r\nX\nk∈S\newk\nnk\nX\ni=1\n\b\nγbθ(k)[t−1](z(k)\ni )(z(k)\ni )⊤β(k)∗−Ez(k)[γbθ(k)[t−1](z(k))(z(k))⊤β(k)∗]\n\t\nµ(k)∗\n1\n\r\r\r\r\r\n2\n≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) + 1\nnS\nsup\n| ewk|≤U\n\r\r\r\r\r\nX\nk∈S\newk\nnk\nX\ni=1\n\u0002\nγbθ(k)[t−1](z(k)\ni ) −Ez(k)[γbθ(k)[t−1](z(k))]\n\u0003\nµ(k)∗\n1\n\r\r\r\r\r\n2\n+ 1\nnS\nsup\n| ewk|≤U′ sup\n∥u∥2≤1\n\f\f\f\f\f\nX\nk∈S\newk\nnk\nX\ni=1\n\b\nγbθ(k)[t−1](z(k)\ni )(z(k)\ni )⊤β(k)∗−Ez(k)[γbθ(k)[t−1](z(k))(z(k))⊤β(k)∗]\n\t\n(µ(k)∗\n1\n)⊤u\n\f\f\f\f\f\n≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\nr\np + K\nnS\n+ 1\nnS\nsup\n| ewk|≤U′\n\f\f\f\f\f\nX\nk∈S\newk\nnk\nX\ni=1\n\b\nγbθ(k)[t−1](z(k)\ni )(z(k)\ni )⊤β(k)∗−Ez(k)[γbθ(k)[t−1](z(k))(z(k))⊤β(k)∗]\n\t\n\f\f\f\f\f\n54\n≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\nr\np + K\nnS\n.\nHence\n5 .2 ≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\nr\np + K\nnS\n.\nA similar discussion leads to the same bound for 5 .1. Therefore,\n5 ≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\nr\np + K\nnS\n.\nAnd the same bound holds for 6 , which can be shown in the same spirit. Putting all the\npieces together,\n\r\r\r\r\r\nX\nk∈S\nnk\nnS\n(bΣ(k)[t] −Σ(k)∗)β(k)∗\n\r\r\r\r\r\n2\n≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\nr\np + K\nnS\n.\nTherefore by Lemma 6 and (S.5.31), we have bβ(k)[t−1] = β\n[t−1] for all k ∈S, and\n∥bβ(k)[t] −β(k)∗∥2 ≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\nr\np + K\nnS\n+ h ∧\nr\np + log K\nnk\n+ ϵ\nr\np + log K\nmaxk=1:K nk\n≤Cκ0 ·\nX\nk∈S\nnk\nnS\nξ(k)\nt−1 +\nr\np + K\nnS\n+ h ∧\nr\np + log K\nnk\n+ ϵ\nr\np + log K\nmaxk=1:K nk\n≤κ′\n0ξt−1 + C′\nr p\nnS\n+ C′ · h ∧\nr\np + log K\nnk\n+ C′ϵ\nr\np + log K\nmaxk=1:K nk\n+ C′\nr\nK log K\nnS\n:= ξ(k)\nt .\n(S.5.32)\nThis entails that\nξt =\nX\nk∈S\nnk\nnS\nξ(k)\nt\n= κ′\n0ξt−1+C′\nr p\nnS\n+C′·h∧\ns\nK(p + log K)\nnS\n+C′ϵ\ns\nK(p + log K)\nnS\n+C′\nr\nK log K\nnS\n(S.5.33)\nThis implies that\nX\nk∈S\nnk\nnS\n∥bβ(k)[t]−β(k)∗∥2 ≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗)+\nr\np + K\nnS\n+h∧\ns\nK(p + log K)\nnS\n+ϵ\ns\nK(p + log K)\nnS\n.\nAlso,\n|bδ(k)[t] −δ(k)∗| = 1\n2\n\r\r\r( bβ(k)[t])⊤(bµ(k)[t]\n1\n+ bµ(k)[t]\n2\n) −(β(k)∗)⊤(µ(k)∗\n1\n+ µ(k)∗\n2\n)\n\r\r\r\n2\n55\n≲∥bβ(k)[t] −β(k)∗∥2 + ∥(β(k)∗)⊤(bµ(k)[t]\n1\n−µ(k)∗\n1\n)∥2 + ∥(β(k)∗)⊤(bµ(k)[t]\n2\n−µ(k)∗\n2\n)∥2\n≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\nr\np + K\nnS\n+ h ∧\nr\np + log K\nnk\n+ ϵ\nr\np + log K\nnk\n+\n\r\r\r\r\r\n1\nnk\nnk\nX\ni=1\nγbθ(k)[t−1](z(k)\ni )(β(k)∗)⊤z(k)\ni\n−Ez(k)\n\u0002\nγbθ(k)[t−1](z(k))(β(k)∗)⊤z(k)\u0003\n\r\r\r\r\r\n2\n≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\nr\np + K\nnS\n+ h ∧\nr\np + log K\nnk\n+ ϵ\nr\np + log K\nnk\n+ ξ(k)\nt−1\nr p\nnk\n+\nr\nlog K\nnk\n≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\nr p\nnS\n+ h ∧\nr\np + log K\nnk\n+ ϵ\nr\np + log K\nmaxk=1:K nk\n+ κ0ξ(k)\nt−1 +\nr\nlog K\nnk\nTherefore,\nX\nk∈S\nnk\nnS\n|bδ(k)[t] −δ(k)∗| ≲κ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) +\nr p\nnS\n+ h ∧\ns\nK(p + log K)\nnS\n+ ϵ\ns\nK(p + log K)\nnS\n+ κ0\nX\nk∈S\nnk\nnS\nξ(k)\nt−1 +\nr\nK log K\nnS\n.\nHence\nX\nk∈S\nnk\nnS\nd(bθ(k)[t], θ(k)∗) ≤Cκ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) + C\nr p\nnS\n+ C · h ∧\ns\nK(p + log K)\nnS\n+ Cϵ\ns\nK(p + log K)\nnS\n+ Cκ0ξt−1 + C\nr\nK log K\nnS\n≤κ′\n0ξt−1 + C\nr p\nnS\n+ C · h ∧\ns\nK(p + log K)\nnS\n+ Cϵ\ns\nK(p + log K)\nnS\n+ C′\nr\nK log K\nnS\n,\n56\nand\nd(bθ(k)[t], θ(k)∗) ≤Cκ0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) + C\nr p\nnS\n+ Ch ∧\nr\np + log K\nnk\n+ Cϵ\nr\np + log K\nmaxk=1:K nk\n+ Cκ0ξ(k)\nt−1 + C\nr\nlog K\nnk\n≤1\n2κ′\n0\nX\nk∈S\nnk\nnS\nd(bθ(k)[t−1], θ(k)∗) + C\nr p\nnS\n+ Ch ∧\nr\np + log K\nnk\n+ Cϵ\nr\np + log K\nmaxk=1:K nk\n+ 1\n2κ′\n0ξ(k)\nt−1 + C\nr\nlog K\nnk\n≤κ′\n0ξ(k)\nt−1 + C\nr p\nnS\n+ Ch ∧\nr\np + log K\nnk\n+ Cϵ\nr\np + log K\nmaxk=1:K nk\n+ C\nr\nlog K\nnk\n,\n(S.5.34)\nwhere κ′\n0 = 2Cκ0 ∈(0, 1).\nTherefore, for t = (t0 + 1) : (t0 + t′\n0), where (κ′\n0)t′\n0 ≍K−1/2 hence t′\n0 ≍log K, we have\nupdating formulas (S.5.32) and (S.5.33) for ξ(k)\nt . We can get\nξt0+t′ = (κ′\n0)t′ξt0 + C′\nr p\nnS\n+ C′ · h ∧\ns\nK(p + log K)\nnS\n+ C′ϵ\ns\nK(p + log K)\nnS\n+ C′\nr\nK log K\nnS\n≤C(κ′\n0)t′\ns\nK(p + log K)\nnS\n+ C′\nr p\nnS\n+ C′ · h ∧\ns\nK(p + log K)\nnS\n+ C′ϵ\ns\nK(p + log K)\nnS\n+ C′\nr\nK log K\nnS\n,\nand\nξ(k)\nt0+t′ = κ′\n0ξt0+t′−1 + C′\nr p\nnS\n+ C′ · h ∧\nr\np + log K\nnk\n+ C′ϵ\nr\np + log K\nnk\n+ C′\nr\nK log K\nnS\n≤C(κ′\n0)t′\ns\nK(p + log K)\nnS\n+ C′\nr p\nnS\n+ C′ · h ∧\nr\np + log K\nnk\n+ C′ϵ\nr\np + log K\nmaxk=1:K nk\n+ C′\nr\nK log K\nnS\n,\n57\nwith\nξ(k)\nt0+t′\n0 ≤C′\nr p\nnS\n+ C′ · h ∧\nr\np + log K\nnk\n+ C′ϵ\nr\np + log K\nmaxk=1:K nk\n+ C′\nr\nK log K\nnS\n,\n(S.5.35)\nwhere the last inequality is due to (κ′\n0)t′\n0\nq\nK(p+log K)\nnS\n≍\nq\np+log K\nnS\n.\nConsider an event series {Et}\nt0+t′\n0\nt=t0\neach of which is defined to be the intersection of\nthe events in Lemmas 11, 12, 13, and 14, with ξ(k) = ξ(k)\nt , which satisfies P(Et) ≥1 −\nC′(K−2 + K−2e−C′′p) hence P(Tt0+t′\n0\nt=t0 Et) ≥1 −C′t′\n0(K−2 + K−2e−C′′p) ≥1 −C′′(K−2 +\nK−2e−C′′p) log K ≥1−C′K−1. In the following, we condition on E ∩(∩\nt0+t′\n0\nt=t0 Et), therefore all\nthe arguments hold with probability at least 1−C′K−1. Therefore, for t = (t0+1) : (t0+t′\n0),\nwe have (S.5.34) hold, which leads to\nd(bθ(k)[t0+t′], θ(k)∗) ≤κ′\n0ξ(k)\nt0+t′−1 + C\nr p\nnS\n+ Ch ∧\nr\np + log K\nnk\n+ Cϵ\nr\np + log K\nmaxk=1:K nk\n+ C\nr\nlog K\nnk\n≤C(κ′\n0)t′\ns\nK(p + log K)\nnS\n+ C\nr p\nnS\n+ Ch ∧\nr\np + log K\nnk\n+ Cϵ\nr\np + log K\nmaxk=1:K nk\n+ C\nr\nlog K\nnk\n≤C′(κ′\n0)t′ · t2\n0κt0 + C\nr p\nnS\n+ Ch ∧\nr\np + log K\nnk\n+ Cϵ\nr\np + log K\nmaxk=1:K nk\n+ C\nr\nlog K\nnk\n≤(t0 + t′)2(κ ∨κ′\n0)t0+t′ + C\nr p\nnS\n+ Ch ∧\nr\np + log K\nnk\n+ Cϵ\nr\np + log K\nmaxk=1:K nk\n+ C\nr\nlog K\nnk\n,\nwhere t′ = 1, . . . , t′\n0, which provides the desired rate for t = (t0 + 1) : (t0 + t′\n0). When\nt′ ≥t′\n0, by (S.5.35), we have\nd(bθ(k)[t0+t′], θ(k)∗) ≤(κ′\n0)t′−t′\n0ξ(k)\nt0+t′\n0 + C\nr p\nnS\n+ Ch ∧\nr\np + log K\nnk\n+ Cϵ\nr\np + log K\nmaxk=1:K nk\n+ C\nr\nlog K\nnk\n≤C\nr p\nnS\n+ Ch ∧\nr\np + log K\nnk\n+ Cϵ\nr\np + log K\nmaxk=1:K nk\n+ C\nr\nlog K\nnk\n,\n58\nwhich is the desired rate. We complete the proof for Theorem 1.\nS.5.2.3\nProofs of lemmas\nProof of Lemma 11. We prove part (i) first.\nDenote W = sup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f 1\nnk\nPnk\ni=1 γθ(k)(z(k)\ni ) −E[γθ(k)(z(k))]\n\f\f\f. By bounded dif-\nference inequality,\nW ≤EW + C\nr\nlog K\nnk\n,\n(S.5.36)\nwith probability at least 1−C′K−2. By the generalized symmetrization inequality (Propo-\nsition 4.11 in Wainwright (2019)), with i.i.d. Rademacher variables {ϵ(k)\ni }nk\ni=1,\nEW ≤2\nnk\nEzEϵ\n\n\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\nγθ(k)(z(k)\ni )ϵ(k)\ni\n\f\f\f\f\f\n\n\n≤2\nnk\nEzEϵ\n\n\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\nw(k) exp{Cθ(k)(z(k)\ni )}\n1 −w(k) + w(k) exp{Cθ(k)(z(k)\ni )}\nϵ(k)\ni\n\f\f\f\f\f\n\n\n≤2\nnk\nEzEϵ\n\n\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\n1\n1 + exp{Cθ(k)(z(k)\ni ) −log((w(k))−1 −1)}\nϵ(k)\ni\n\f\f\f\f\f\n\n,\n(S.5.37)\nwhere Cθ(k)(z(k)\ni ) = (β(k))⊤z(k)\ni\n−δ(k). Denote µ(k)∗= (1−w(k)∗)µ(k)∗\n1\n+w(k)∗µ(k)\n2\n= E[z(k)\ni ].\nBy the contraction inequality for Rademecher variables (Theorem 11.6 in Boucheron et al.,\n2013),\nRHS of (S.5.37) ≤C\nnk\nEzEϵ\n\n\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\n\u0002\nCθ(k)(z(k)\ni ) −log((w(k))−1 −1)\n\u0003\nϵ(k)\ni\n\f\f\f\f\f\n\n\n≤C\nnk\nEzEϵ\n\n\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\n(β(k))⊤(z(k)\ni\n−µ(k)∗) · ϵ(k)\ni\n\f\f\f\f\f\n\n\n+ C\nnk\nEzEϵ\n\n\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\n(β(k))⊤µ(k)∗· ϵ(k)\ni\n\f\f\f\f\f\n\n\n59\n+ C\nnk\nEϵ\n\n\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\flog((w(k))−1 −1)\n\f\f ·\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\ni\n\f\f\f\f\f\n\n\n≤C\nnk\nEzEϵ\n\"\nsup\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\n(β(k) −β(k)∗)⊤(z(k)\ni\n−µ(k)∗) · ϵ(k)\ni\n\f\f\f\f\f\n#\n+ C\nnk\nEzEϵ\n\f\f\f\f\f\nnk\nX\ni=1\n(β(k)∗)⊤(z(k)\ni\n−µ(k)∗) · ϵ(k)\ni\n\f\f\f\f\f\n+ C\nnk\nEϵ\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\ni\n\f\f\f\f\f + C\nnk\nEzEϵ\n\n\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\n(β(k))⊤µ(k)∗· ϵ(k)\ni\n\f\f\f\f\f\n\n.\nSince {ϵ(k)\ni }nk\ni=1 and {(β(k) −β(k)∗)⊤(z(k)\ni\n−µ(k)∗)ϵ(k)\ni }nk\ni=1 are i.i.d. sub-Gaussian variables,\nwe know that\nC\nnk\nEzEϵ\n\f\f\f\f\f\nnk\nX\ni=1\n(β(k)∗)⊤(z(k)\ni\n−µ(k)∗) · ϵ(k)\ni\n\f\f\f\f\f + C\nnk\nEϵ\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\ni\n\f\f\f\f\f ≲\nr\n1\nnk\n.\nSuppose {uj}N\nj=1 is a 1/2-cover of Bp := {u ∈Rp : ∥u∥2 ≤1} with N = 5p (see Example\n5.8 in Wainwright, 2019). Hence by standard arguments,\nC\nnk\nEzEϵ\n\"\nsup\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\n(β(k) −β(k)∗)⊤(z(k)\ni\n−µ(k)∗) · ϵ(k)\ni\n\f\f\f\f\f\n#\n≲ξ(k)\nnk\nEzEϵ\n\"\nsup\nj=1:N\n\f\f\f\f\f\nnk\nX\ni=1\nu⊤\nj (z(k)\ni\n−µ(k)∗) · ϵ(k)\ni\n\f\f\f\f\f\n#\n.\nAgain, since {u⊤\nj (z(k)\ni\n−µ(k)∗)ϵ(k)\ni }nk\ni=1 are i.i.d. sub-Gaussian variables,\n1\nnk\nEzEϵ\n\"\nsup\nj=1:N\n\f\f\f\f\f\nnk\nX\ni=1\nu⊤\nj (z(k)\ni\n−µ(k)∗) · ϵ(k)\ni\n\f\f\f\f\f\n#\n≲\nr\nlog N\nnk\n=\nr p\nnk\n.\nPutting all the pieces together,\nEW ≲ξ(k)\nr p\nnk\n+\nr\n1\nnk\n.\n(S.5.38)\nCombining (S.5.37) and (S.5.38), we get the result in (i).\nNext, we derive part (ii) using a similar analysis.\nDenote W ′ = sup{θ(k)}k∈S∈BJ,2\ncon sup| ewk|≤1\n1\nnS\n\f\f\fP\nk∈S ewk\nPnk\ni=1\nh\nγθ(k)(z(k)\ni ) −E[γθ(k)(z(k))]\ni\f\f\f.\nBy a similar standard symmetrization and contraction arguments we used in part (i),\nwith i.i.d. Rademacher variables {ϵ(k)\ni }nk\ni=1, for any λ ∈R, we have\nE exp{λW ′}\n60\n≤CEzEϵ exp\n(\n2λ\nnS\nsup\n{θ(k)}k∈S∈BJ,2\ncon\nsup\n| ewk|≤1\n\f\f\f\f\f\nX\nk∈S\newk\nnk\nX\ni=1\nγθ(k)(z(k)\ni )ϵ(k)\ni\n\f\f\f\f\f\n)\n≤CEzEϵ exp\n(\n4λ\nnS\nsup\n{θ(k)}k∈S∈BJ,2\ncon\nsup\newk=±1/2\n\f\f\f\f\f\nX\nk∈S\newk\nnk\nX\ni=1\nγθ(k)(z(k)\ni )ϵ(k)\ni\n\f\f\f\f\f\n)\n≤C\nX\newk=±1/2\nEzEϵ exp\n(\n4λ\nnS\nsup\n{θ(k)}k∈S∈BJ,2\ncon\n\f\f\f\f\f\nX\nk∈S\newk\nnk\nX\ni=1\nγθ(k)(z(k)\ni )ϵ(k)\ni\n\f\f\f\f\f\n)\n≤C\nX\newk=±1/2\nEzEϵ exp\n(\n4λ\nnS\nsup\n{θ(k)}k∈S∈BJ,2\ncon\n\f\f\f\f\f\nX\nk∈S\newk\nnk\nX\ni=1\n1\n1 + exp{Cθ(k)(z(k)\ni ) −log((w(k))−1 −1)}\nϵ(k)\ni\n\f\f\f\f\f\n)\n≤C\nX\newk=±1/2\nEzEϵ exp\n(\n4λ\nnS\nsup\n{θ(k)}k∈S∈BJ,2\ncon\n\f\f\f\f\f\nX\nk∈S\newk\nnk\nX\ni=1\n\u0002\nCθ(k)(z(k)\ni ) −log((w(k))−1 −1)\n\u0003\nϵ(k)\ni\n\f\f\f\f\f\n)\n,\nwhere Cθ(k)(z(k)\ni ) = (β(k))⊤z(k)\ni\n−δ(k). Denote µ(k)∗= (1−w(k)∗)µ(k)∗\n1\n+w(k)∗µ(k)\n2\n= E[z(k)\ni ].\nSuppose {uj}N\nj=1 is a 1/2-cover of Bp := {u ∈Rp : ∥u∥2 ≤1} with N = 5p. Then by\nCauchy-Schwarz inequality and standard arguments,\nEz,ϵ exp\n(\n4λ\nnS\nsup\n{θ(k)}k∈S∈BJ,2\ncon\n\f\f\f\f\f\nX\nk∈S\newk\nnk\nX\ni=1\n\u0002\nCθ(k)(z(k)\ni ) −log((w(k))−1 −1)\n\u0003\nϵ(k)\ni\n\f\f\f\f\f\n)\n≲\n\"\nEz,ϵ exp\n(\nCλ\nnS\nsup\n∥β∥2≤U\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\newkβ⊤(z(k)\ni\n−µ(k)∗)ϵ(k)\ni\n\f\f\f\f\f\n)#1/3\n+\n\"\nEϵ exp\n(\nCλ\nnS\nsup\n∥β∥2≤U\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\newkβ⊤µ(k)∗ϵ(k)\ni\n\f\f\f\f\f\n)#1/3\n+\n\"\nEϵ exp\n(\nCλ\nnS\nsup\ncw/2≤w(k)≤1−cw/2\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\newk log((w(k))−1 −1)ϵ(k)\ni\n\f\f\f\f\f\n)#1/3\n≲\n\"\nEz,ϵ exp\n(\nCλ\nnS\nsup\nj=1:N\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\newku⊤\nj (z(k)\ni\n−µ(k)∗)ϵ(k)\ni\n\f\f\f\f\f\n)#1/3\n+\n\"\nEϵ exp\n(\nCλ\nnS\nsup\nj=1:N\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\newku⊤\nj µ(k)∗ϵ(k)\ni\n\f\f\f\f\f\n)#1/3\n+\n\"\nEϵ exp\n(\nCλ\nnS\nsup\ncw/2≤w(k)≤1−cw/2\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\newk log((w(k))−1 −1)ϵ(k)\ni\n\f\f\f\f\f\n)#1/3\n61\n≲\nN\nX\nj=1\n\"\nEz,ϵ exp\n(\nCλ\nnS\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\newku⊤\nj (z(k)\ni\n−µ(k)∗)ϵ(k)\ni\n\f\f\f\f\f\n)#1/3\n|\n{z\n}\n[1]\n+\nN\nX\nj=1\n\"\nEϵ exp\n(\nCλ\nnS\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\newku⊤\nj µ(k)∗ϵ(k)\ni\n\f\f\f\f\f\n)#1/3\n|\n{z\n}\n[2]\n+\n\"\nEϵ exp\n(\nCλ\nnS\nX\nk∈S\n\f\f\f\f\f\nnk\nX\ni=1\newkϵ(k)\ni\n\f\f\f\f\f\n)#1/3\n|\n{z\n}\n[3]\nSince { ewku⊤\nj (z(k)\ni\n−µ(k)∗)ϵ(k)\ni }i,j, { ewk(β(k))⊤µ(k)∗ϵ(k)\ni }i,k, and { ewkϵ(k)\ni }nk\ni=1 are independent\nsub-Gaussian variables, we can bound the three terms on the RHS as\n[1] ≲5p · exp\n\u001aCλ2\nnS\n\u001b\n,\n[2] ≲5p · exp\n\u001aCλ2\nnS\n\u001b\n,\n[3] ≤\n\"Y\nk∈S\nEϵ exp\n(\nCλ\nnS\n\f\f\f\f\f\nnk\nX\ni=1\newkϵ(k)\ni\n\f\f\f\f\f\n)#1/3\n≲\n\"Y\nk∈S\nexp\n\u001a\nC λ2\nn2\nS\nnk\n\u001b#1/3\n≲exp\n\u001aCλ2\nnS\n\u001b\n.\nPutting all pieces together,\nE exp{λW ′} ≤C′2K5p · exp\n\u001aCλ2\nnS\n\u001b\n= exp\n\u001a\nC λ2\nnS\n+ C′′(K + p)\n\u001b\n.\nTherefore, for any δ > 0\nP(W ′ ≥δ) ≤e−λδE exp{λW ′} ≤exp\n\u001a\nC λ2\nnS\n+ C′′(K + p) −λδ\n\u001b\n.\nLet λ = nS\n2Cδ and δ = 4\nq\nCC′′(K+p)\nnS\n, we have\nP(W ′ ≥δ) ≤exp\nn\n−nS\n4C δ2 + C′′(K + p)\no\n= exp{−3C′′(K + p)} ≤C′K−2 exp{−3C′′p},\nwhich completes the proof.\nProof of Lemma 12. The proof of part (ii) is the same as the proof of part (ii) for Lemma\n11, so we omit it. The only difference between the proofs of part (i) for two lemmas is that\nhere the bounded difference inequality is not available. Denote\nW =\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\f\n1\nnk\nnk\nX\ni=1\n\u0002\n1 −γθ(k)(z(k)\ni )\n\u0003\n(z(k)\ni )⊤β(k)∗−E\n\u0002\n[1 −γθ(k)(z(k))](z(k))⊤β(k)∗\u0003\n\f\f\f\f\f .\n62\nWe need to use Lemma 10 to upper bound W −EW. Prior to that, we first verify the\nconditions required by the lemma. Fix z(k)\n1 , . . . , z(k)\ni−1, z(k)\ni+1, . . . , z(k)\nnk , and define\ng(k)\ni (z(k)\ni ) = W −E[W|z(k)\n1 , . . . , z(k)\ni−1, z(k)\ni+1, . . . , z(k)\nnk ].\nBy triangle inequality,\n\f\f\fg(k)\ni (z(k)\ni )\n\f\f\f\n=\n\f\f\f\f\f\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\n1\nnk\nnk\nX\ni=1\nγθ(k)(z(k)\ni )(z(k)\ni )⊤β(k)∗−E\n\u0002\nγθ(k)(z(k)\ni )(z(k)\ni )⊤β(k)∗\u0003\f\f\f\f\n−E\n\"\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\n1\nnk\nnk\nX\ni=1\nγθ(k)(z(k)\ni )(z(k)\ni )⊤β(k)∗−E\n\u0002\nγθ(k)(z(k)\ni )(z(k)\ni )⊤β(k)∗\u0003\f\f\f\f\n\f\f\f{z(k)\ni′ }i′̸=i\n#\f\f\f\f\f\n≤1\nnk\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\fγθ(k)(z(k)\ni )(z(k)\ni )⊤β(k)∗\n\f\f\f\f\n|\n{z\n}\nW1\n+ 2\nnk\nE\n\f\f\f\f\f\f\f\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\nγθ(k)(z(k)\ni )(z(k)\ni )⊤β(k)∗\n\f\f\f\f\f\f\f\n|\n{z\n}\nW2\n.\nNote that [E(W1 + W2)d]1/d ≤(EW d\n1 )1/d + (EW d\n2 )1/d, where\n(EW d\n1 )1/d ≤1\nnk\n\"\nE\nsup\nθ(k)∈Bcon\n[γθ(k)(z(k)\ni )]2d\n#1/2d h\nE\n\f\f(z(k)\ni )⊤β(k)∗\f\f2di1/2d\n≤1\nnk\n· C ·\n√\nd\n(EW d\n2 )1/d = EW1 ≤(EW d\n1 )1/d ≤1\nnk\n· C ·\n√\nd.\nTherefore [E(W1 + W2)d]1/d ≤C\nnk\n√\nd. Hence by applying Lemma 10, we have\nW ≤EW + C\nr\nlog K\nnk\n,\nwith probability at least 1 −C′K−2.\nProof of Lemma 13. For part (i), denote\nW =\nsup\nθ(k)∈Bcon\n\r\r\r\r\r\n1\nnk\nnk\nX\ni=1\nγθ(k)(z(k)\ni )z(k)\ni\n−E[γθ(k)(z(k))z(k)]\n\r\r\r\r\r\n2\n.\n63\nSuppose {uj}N\nj=1 is a 1/2-cover of Bp := {u ∈Rp : ∥u∥2 ≤1} with N = 5p. Define\nµ(k)∗= (1 −w(k)∗)µ(k)∗\n1\n+ w(k)∗µ(k)∗\n2\n. Then by the generalized symmetrization inequality\n(Proposition 4.11 in Wainwright, 2019), with i.i.d. Rademacher variables {ϵ(k)\ni }nk\ni=1, for any\nλ ∈R,\nE exp{λW}\n≲Ez,ϵ exp\n(\nCλ\nnk\nsup\nθ(k)∈Bcon\n\r\r\r\r\r\nnk\nX\ni=1\nγθ(k)(z(k)\ni )z(k)\ni ϵ(k)\ni\n\r\r\r\r\r\n2\n)\n≲Ez,ϵ exp\n(\nCλ\nnk\nsup\nj=1:N\nsup\nθ(k)∈Bcon\n\f\f\f\f\f\nnk\nX\ni=1\nγθ(k)(z(k)\ni )(z(k)\ni )⊤uj · ϵ(k)\ni\n\f\f\f\f\f\n)\n≲\nN\nX\nj=1\nEz,ϵ exp\n(\nCλ\nnk\nsup\nθ(k)∈Bcon\n\f\f\f\f\f\nnk\nX\ni=1\nγθ(k)(z(k)\ni )(z(k)\ni )⊤uj · ϵ(k)\ni\n\f\f\f\f\f\n)\n≲\nN\nX\nj=1\nEz,ϵ exp\n(\nCλ\nnk\nsup\nθ(k)∈Bcon\n\f\f\f\f\f\nnk\nX\ni=1\n\u0002\nCθ(k)(z(k)\ni ) −log((w(k))−1 −1)\n\u0003\n(z(k)\ni )⊤uj · ϵ(k)\ni\n\f\f\f\f\f\n)\n≲\nN\nX\nj=1\nEz,ϵ exp\n(\nCλ\nnk\nsup\n∥β(k)∥2≤U\n\f\f\f\f\f\nnk\nX\ni=1\n(β(k))⊤(z(k)\ni\n−µ(k)∗)(z(k)\ni )⊤uj · ϵ(k)\ni\n\f\f\f\f\f\n)\n+\nN\nX\nj=1\nEz,ϵ exp\n(\nCλ\nnk\nsup\n∥β(k)∥2≤U\n\f\f\f\f\f\nnk\nX\ni=1\n\u0002\n(β(k))⊤µ(k)∗−δ(k)\u0003\n(z(k)\ni )⊤uj · ϵ(k)\ni\n\f\f\f\f\f\n)\n+\nN\nX\nj=1\nEz,ϵ exp\n(\nCλ\nnk\nsup\ncw/2≤w(k)≤1−cw/2\n\f\f\f\f\f\nnk\nX\ni=1\nlog((w(k))−1 −1)(z(k)\ni )⊤uj · ϵ(k)\ni\n\f\f\f\f\f\n)\n≲\nN\nX\nj=1\nN\nX\nj′=1\nEz,ϵ exp\n(\nCλ\nnk\n\f\f\f\f\f\nnk\nX\ni=1\nu⊤\nj′(z(k)\ni\n−µ(k)∗)(z(k)\ni )⊤uj · ϵ(k)\ni\n\f\f\f\f\f\n)\n+\nN\nX\nj=1\nEz,ϵ exp\n(\nCλ\nnk\n\f\f\f\f\f\nnk\nX\ni=1\n(z(k)\ni )⊤uj · ϵ(k)\ni\n\f\f\f\f\f\n)\n.\nNote that since {u⊤\nj′(z(k)\ni\n−µ(k)∗)(z(k)\ni )⊤uj · ϵ(k)\ni }nk\ni=1 are i.i.d. sub-exponential variables and\n{(z(k)\ni )⊤uj · ϵ(k)\ni }nk\ni=1 are i.i.d. sub-Gaussian variables, we have\nEz,ϵ exp\n(\nCλ\nnk\n\f\f\f\f\f\nnk\nX\ni=1\nu⊤\nj′(z(k)\ni\n−µ(k)∗)(z(k)\ni )⊤uj · ϵ(k)\ni\n\f\f\f\f\f\n)\n≲exp\n\u001a\nC λ2\nnk\n\u001b\n,\nEz,ϵ exp\n(\nCλ\nnk\n\f\f\f\f\f\nnk\nX\ni=1\n(z(k)\ni )⊤uj · ϵ(k)\ni\n\f\f\f\f\f\n)\n≲exp\n\u001a\nC λ2\nnk\n\u001b\n,\nwhere the first inequality holds when λ ≤C′′nk where C′′ is small. Therefore,\nE exp{λW} ≲exp\n\u001a\nC λ2\nnk\n+ C′p\n\u001b\n,\n64\nwhen λ ≤C′′nk. The desired result follows from Chernoff’s bound.\nThe proofs of parts (ii) and (iii) are almost the same as the proofs of part (ii) of Lemma\n11, so we do not repeat them here.\nProof of Lemma 14. Note that\n\r\r\r\r\r\n1\nnk\nnk\nX\ni=1\n\u0002\nz(k)\ni (z(k)\ni )⊤−E[z(k)\ni (z(k)\ni )⊤]\n\u0003\nβ(k)∗\n\r\r\r\r\r\n2\n≲\n\r\r\r\r\r\n1\nnk\nnk\nX\ni=1\n\u0002\nz(k)\ni (z(k)\ni )⊤−E[z(k)\ni (z(k)\ni )⊤]\n\u0003\n\r\r\r\r\r\n2\n.\nThe bound of the RHS comes from Theorem 6.5 in Wainwright (2019). And the bound in\npart (ii) can be proved in the same way.\nS.5.3\nProof of Theorem 3\nS.5.3.1\nLemmas\nRecall the parameter space\nΘS(h) =\nn\n{θ\n(k)}k∈S = {(w(k), µ(k)\n1 , µ(k)\n2 , Σ(k))}k∈S : θ\n(k) ∈Θ, inf\nβ max\nk∈S ∥β(k) −β∥2 ≤h\no\n,\nwhere β(k) = (Σ(k))−1(µ(k)\n2\n−µ(k)\n1 ) and δ(k) = 1\n2(β(k))⊤(µ(k)\n1\n+ µ(k)\n2 ).\nLemma 15 (Lemma 8.4 in Cai et al., 2019). For any µ, eµ ∈Rp and w ∈(0, 1), denote\nPµ = (1 −w)N(µ, Ip) + N(−µ, Ip) and Peµ = (1 −w)N(eµ, Ip) + N(−eµ, Ip). Then\nKL(Pµ∥Peµ) ≤\n\u0012\n4∥µ∥2\n2 + 1\n2 log\n\u0012\nw\n1 −w\n\u0013\u0013\n· 2∥µ −eµ∥2\n2.\nLemma 16. For any µ, µ′, eµ, eµ′ ∈Rp and w ∈(0, 1), denote Pµ,eµ = (1 −w)N(µ, Ip) +\nwN(eµ, Ip) and Pµ′,eµ′ = (1 −w)N(µ′, Ip) + wN(eµ′, Ip). Then\nKL(Pµ,eµ∥Pµ′,eµ′) ≤(1 −w)∥µ −µ′∥2\n2 + w∥eµ −eµ′∥2\n2.\nLemma 17. Denote distribution (1−w)N(µ, Ip)+wN(−µ, Ip) as Pw for any w ∈(cw, 1−\ncw), where µ ∈Rp. Then\nKL(Pw∥Pw′) ≤\n1\n2c2\nw\n(w −w′)2.\nLemma 18. Denote distribution 1\n2N((−1/2, 0⊤\np−1)⊤, Ip) + 1\n2N((1/2 + eu, 0⊤\np−1)⊤, Ip) as Peu\nfor any eu ∈[−1, 1]. Then\nKL(Peu∥Peu′) ≤1\n2(eu −eu′)2.\n65\nLemma 19. When there exists an subset S such that mink∈S nk ≥C(p + log K) with some\nconstant C > 0, we have\ninf\n{bθ(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS\nQS\nP\n [\nk∈S\n\u001a\nd(bθ(k), θ(k)∗) ≥C1\nr p\nnS\n+ C2\nr\nlog K\nnk\n+ C3h ∧\nr\np + log K\nnk\n\u001b!\n≥1\n4.\nLemma 20. Denote eϵ = K−s\ns . Then\ninf\n{bθ(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS\nQS\nP\n \nmax\nk∈S d(bθ(k), θ(k)∗) ≥C1eϵ\nr\n1\nmaxk=1:K nk\n!\n≥1\n10.\nLemma 21 (The first variant of Theorem 5.1 in Chen et al., 2018). Given a series of\ndistributions {{P(k)\nθ }K\nk=1 : θ ∈Θ}, each of which is indexed by the same parameter θ ∈Θ.\nConsider x(k) ∼(1−eϵ)P(k)\nθ +eϵQ(k) independently for k = 1 : K. Denote the joint distribution\nof {x(k)}K\nk=1 as P(eϵ,θ,{Q(k)}K\nk=1). Then\ninf\nbθ\nsup\nθ∈Θ\n{Q(k)}K\nk=1\nP(eϵ,θ,{Q(k)}K\nk=1)\n\u0010\n∥bθ −θ∥≥Cϖ(eϵ, Θ)\n\u0011\n≥1\n2,\nwhere ϖ(eϵ, Θ) := sup\n\b\n∥θ1 −θ2∥: maxk=1:K dTV\n\u0000P(k)\nθ1 , P(k)\nθ2\n\u0001\n≤eϵ/(1 −eϵ)\n\t\n.\nLemma 22. Suppose K −s ≥1. Consider two data generating mechanisms:\n(i) x(k) ∼(1 −eϵ′)P(k)\nθ\n+ eϵ′Q(k) independently for k = 1 : K, where eϵ′ = K−s\nK ;\n(ii) With a preserved set S ⊆1 : K, generate {x(k)}k∈Sc ∼QS and x(k) ∼P(k)\nθ\nindepen-\ndently for k ∈S.\nDenote the joint distributions of {x(k)}K\nk=1 in (i) and (ii) as P(eϵ,θ,{Q(k)}K\nk=1) and P(S,θ,Q),\nrespectively. We claim that if\ninf\nbθ\nsup\nθ∈Θ\n{Q(k)}K\nk=1\nP( K−s\n50K ,θ,{Q(k)}K\nk=1)\n\u0012\n∥bθ −θ∥≥Cϖ\n\u0012K −s\n50K , Θ\n\u0013\u0013\n≥1\n2,\nthen\ninf\nbθ\nsup\nS:|S|≥s\nsup\nθ∈Θ\nQS\nP(S,θ,QS)\n\u0012\n∥bθ −θ∥≥Cϖ\n\u0012K −s\n50K , Θ\n\u0013\u0013\n≥1\n10,\n(S.5.39)\nwhere ϖ(eϵ, Θ) := sup\n\b\n∥θ1 −θ2∥: maxk=1:K KL\n\u0000P(k)\nθ1 ∥P(k)\nθ2\n\u0001\n≤[eϵ/(1−eϵ)]2\t\nfor any eϵ ∈(0, 1).\n66\nLemma 23. When there exists an subset S such that mink∈S nk ≥C(p ∨log K) with some\nconstant C > 0, we have\ninf\n{bΣ(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS\nQS\nP\n [\nk∈S\n\u001a\nmin\n\b\n∥bµ(k)\n1\n−µ(k)∗\n1\n∥2 ∨∥bµ(k)\n2\n−µ(k)∗\n2\n∥2,\n∥bµ(k)\n1\n−µ(k)∗\n2\n∥2 ∨∥bµ(k)\n2\n−µ(k)∗\n1\n∥2\n\t\n∨∥bΣ(k) −Σ(k)∗∥2 ≥C\nr\np + log K\nnk\n\u001b!\n≥1\n10.\nS.5.3.2\nMain proof of Theorem 3\nCombine conclusions of Lemmas 19 and 20 to get the first lower bound. Lemma 23 implies\nthe second one.\nS.5.3.3\nProofs of lemmas\nProof of Lemma 17. Denote g(w; ez) = log\n\u0002\n(1 −w)ez + w\n\u0003\n, g′(w; ez) =\n1−ez\n(1−w)ez+w, g′′(w; ez) =\n−\n(1−ez)2\n[(1−w)ez+w]2 and f(w; z, µ) =\n1−w\n(2π)p/2 exp{−1\n2∥z −µ∥2\n2} +\nw\n(2π)p/2 exp{−1\n2∥z + µ∥2\n2}.\nBy Taylor expansion,\nlog\n\u0014f(w′; z, µ)\nf(w; z, µ)\n\u0015\n= ∂log f(w; z, µ)\n∂w\n\f\f\f\f\nw\n· (w′ −w) + 1\n2\n∂2 log f(w; z, µ)\n∂w2\n\f\f\f\f\nw0\n· (w′ −w)2,\nwhere w0 = w0(z, µ) is between w and w′. By the property of score function,\nZ ∂log f(w; z, µ)\n∂w\ndPw = 0.\nBesides,\n∂2 log f(w; z, µ)\n∂w2\n= ∂2 log\n\u0002\nf(w; z, µ)/\n\u0000(2π)−p/2 exp{−1\n2∥z + µ∥2\n2}\n\u0001\u0003\n∂w2\n= g′′(w; ez),\nwhere ez = e−µ⊤z. Note that\n−g′′(w; ez) =\n1\n(1 −w)2 ·\n(ez −1)2\n(ez + w/(1 −w))2 ≤1\nc2\nw\n,\nfor any ez > 0. Therefore,\nKL(Pw∥Pw′) = −\nZ\nlog\n\u0014f(w′; z, µ)\nf(w; z, µ)\n\u0015\ndPw\n= −1\n2(w′ −w)2 ·\nZ\ng′′(w0(z, µ); ez)dPw\n≤\n1\n2c2\nw\n(w′ −w)2,\nwhich completes the proof.\n67\nProof of Lemma 18. Recall that we denote distribution 1\n2N((−1/2, 0⊤\np−1)⊤, Ip)+ 1\n2N((1/2+\neu, 0⊤\np−1)⊤, Ip) as Peu for any eu ∈[−1, 1]. By the bi-convexity of KL divergence, we have\nKL(Peu∥Peu′) ≤1\n2KL(N((1/2 + eu, 0⊤\np−1)⊤, Ip)∥N((1/2 + eu′, 0⊤\np−1)⊤, Ip))\n= 1\n2KL(N(1/2 + eu, 1)∥N(1/2 + eu′, 1))\n= 1\n2(eu −eu′)2,\nwhich completes the proof.\nProof of Lemma 19. WLOG, suppose ∆≥1. It’s easy to see that given any S, ΘS ⊇\nΘS,w ∪ΘS,β ∪ΘS,δ, where\nΘS,w =\nn\n{θ\n(k)}k∈S : µ(k)\n1\n= 1p/√p, µ(k)\n2\n= −µ(k)\n1\n= eµ, Σ(k) = Ip, w(k) ∈(cw, 1 −cw)\no\n,\nΘS,β =\nn\n{θ\n(k)}k∈S : Σ(k) = Ip, w(k) = 1\n2, ∥µ(k)\n1 ∥2 ∨∥µ(k)\n2 ∥2 ≤M, min\nβ max\nk∈S ∥β(k) −β∥2 ≤h\no\n,\nΘS,δ =\nn\n{θ\n(k)}k∈S : Σ(k) = Ip, w(k) = 1\n2, ∥µ(k)\n1 ∥2 ∨∥µ(k)\n2 ∥2 ≤M, µ(k)\n1\n= −1\n2µ0, µ(k)\n2\n= 1\n2µ0 + u,\n∥u∥2 ≤1\no\n.\n(i) By fixing an S and a QS, we want to show\ninf\n{ bβ(k)}K\nk=1\nsup\n{θ\n(k)∗}k∈S∈ΘS,β\nP\n [\nk∈S\n\u001a\n∥bβ(k) −β(k)∗∥2 ∧∥bβ(k) + β(k)∗∥2 ≥C\nr p\nnS\n\u001b!\n≥1\n4\nBy Lemma 4, ∃a quadrant Qv of Rp and a r/8-packing of (rSp) ∩Qv under Euclidean\nnorm: {eµj}N\nj=1, where r = (c\np\np/nS) ∧M ≤1 with a small constant c > 0 and N ≥\n(1\n2)p8p−1 = 1\n2 × 4p−1 ≥2p−1 when p ≥2. For any µ ∈Rp, denote distribution 1\n2N(µ0 +\nµ, Ip) + 1\n2N(−µ0 + µ, Ip) as Pµ, where µ0 can be any vector in Rp with ∥µ0∥2 ≥1. Then\nLHS ≥inf\nbµ\nsup\nµ∈(rSp)∩Qv\nP\n \n∥bµ −µ∥2 ∧∥bµ + µ∥2 ≥C\nr p\nnS\n!\n≥inf\nbµ\nsup\nµ∈(rSp)∩Qv\nP\n \n∥bµ −µ∥2 ≥C\nr p\nnS\n!\n,\n(S.5.40)\nwhere the last inequality holds because it suffices to consider estimator bµ satisfying bµ(X) ∈\n(rSp) ∩Qv almost surely. In addition, for any x, y ∈Qv, ∥x −y∥2 ≤∥x + y∥2.\n68\nBy Lemma 15,\nKL\n Y\nk∈S\nP⊗nk\neµj\n· QS\n\r\r\r\r\nY\nk∈S\nP⊗nk\neµj′ · QS\n!\n=\nX\nk∈S\nnkKL(Peµj∥Peµj′)\n≤\nX\nk∈S\nnk · 8∥eµj∥2\n2∥eµj −eµj′∥2\n2\n≤32nSr2\n≤32nSc2 · 2(p −1)\nnS\n≤64c2\nlog 2 log N.\nBy Lemma 3,\nLHS of (S.5.40) ≥1 −log 2\nlog N −64c2\nlog 2 ≥1 −\n1\np −1 −1\n4 ≥1\n4,\nwhen C = c/2, p ≥3 and c = √log 2/16.\n(ii) By fixing an S and a QS, we want to show\ninf\n{ bβ(k)}K\nk=1\nsup\n{θ\n(k)∗}k∈S∈ΘS\nP\n [\nk∈S\n\u001a\n∥bβ(k)−β(k)∗∥2∧∥bβ(k)+β(k)∗∥2 ≥C\n\u0014\nh∧\n\u0012\nc\nr p\nnk\n\u0013\u0015\u001b!\n≥1\n4.\nWLOG, suppose 1 ∈S. We have\ninf\nbβ(1)\nsup\n{θ\n(k)∗}k∈S∈ΘS\nP\n \n∥bβ(1) −β(1)∗∥2 ∧∥bβ(1) + β(1)∗∥2 ≥C\n\u0014\nh ∧\n\u0012\nc\nr p\nn1\n\u0013\u0015!\n≥1\n4,\n(S.5.41)\nBy Lemma 4, ∃a quadrant Qv of Rp and a r/8-packing of (rSp−1)∩Qv under Euclidean\nnorm: { eϑj}N\nj=1, where r = hβ ∧(c\np\np/n1) ∧M ≤1 with a small constant c > 0 and\nN ≥(1\n2)p−18p−2 = 1\n2 × 4p−2 ≥2p−2 when p ≥3. WLOG, assume M ≥2. Denote eµj =\n(1, eϑ⊤\nj )⊤∈Rp. Let µ(k)∗\n1\n= eµ = (1, 0p−1)⊤for all k ∈S\\{1}. And let µ(0)∗\n1\n= µ = (1, ϑ)\nwith ϑ ∈(rSp−1) ∩Qv. For any µ ∈Rp, denote distribution 1\n2N(µ, Ip) + 1\n2N(−µ, Ip) as\nPµ. Then similar to the arguments in (i),\nLHS ≥inf\nbµ\nsup\nϑ∈(rSp−1)∩Qv\nµ=(1,ϑ)⊤\nP\n \n∥bµ −µ∥2 ∧∥bµ + µ∥2 ≥C\n\u0014\nh ∧\n\u0012\nc\nr p\nn1\n\u0013\u0015!\n≥inf\nbµ\nsup\nϑ∈(rSp−1)∩Qv\nµ=(1,ϑ)⊤\nP\n \n∥bµ −µ∥2 ≥C\n\u0014\nh ∧\n\u0012\nc\nr p\nn1\n\u0013\u0015!\n.\n69\nThen by Lemma 15,\nKL\n\nY\nk∈S\\{1}\nP⊗nk\neµ\n· P⊗n1\neµj\n· QS\n\r\r\r\r\nY\nk∈S\\{1}\nP⊗nk\neµ\n· P⊗n1\neµj′ · QS\n\n= n1KL(Peµj∥Peµj′)\n≤n1 · 8∥eµj∥2\n2∥eµj −eµj′∥2\n2\n≤32n1r2\n≤32n1c2 · 3(p −2)\nn1\n≤96c2\nlog 2 log N,\nwhen n1 ≥(c2 ∨M −2)p and p ≥3. By Fano’s lemma (See Corollary 2.6 in Tsybakov,\n2009),\nLHS of (S.5.41) ≥1 −log 2\nlog N −96c2\nlog 2 ≥1 −\n1\np −2 −1\n4 ≥1\n4,\nwhen C = 1/2, p ≥4 and c =\np\n(log 2)/384.\n(iii) By fixing an S and a QS, we want to show\ninf\n{bθ(k)}K\nk=1\nsup\n{θ\n(k)∗}k∈S∈ΘS\nP\n [\nk∈S\n\u001a\n∥bβ(k)−β(k)∗∥2∧∥bβ(k)+β(k)∗∥2 ≥C\n\u0014\nh∧\n\u0012\nc\nr\nlog K\nnk\n\u0013\u0015\u001b!\n≥1\n4.\nSuppose v = 1p and denote the associated quadrant Qv = Rp\n+, ΥS = {{µ(k)}k∈S : µ(k) ∈\nRp\n+, minµ maxk∈S ∥µ(k) −µ∥2 ≤h, ∥µ(k)∥2 ≤M}. Let rk = h ∧(c\np\nlog K/nk) ∧M with\na small constant c > 0 for k ∈S. For any M = {µ(k)}k∈S, where µ(k) ∈Rp, denote\ndistribution Q\nk∈S\n\u00021\n2N(µ(k), Ip) + 1\n2N(−µ(k), Ip)\n\u0003⊗nk as PM, and the joint distribution of\nPM and QS as PM · QS. And denote distribution (1 −w)N(µ, Ip) + wN(−µ, Ip) as Pµ\nfor any µ ∈Rp. Similar to the arguments in (i), since it suffices to consider the estimators\n{bµ(k)}k∈S satisfying {bµ(k)}k∈S ∈ΥS almost surely and ∥x −y∥2 ≤∥x + y∥2 for any x,\ny ∈Rp\n+, we have\nLHS ≥\ninf\n{bµ(k)}k∈S\nsup\n{µ(k)}k∈S∈ΥS\nP{µ(k)}k∈S · QS\n [\nk∈S\n\u001a\n∥bµ(k) −µ(k)∥2 ∧∥bµ(k) + µ(k)∥2\n≥C\n\u0014\nh ∧\n\u0012\nc\nr\nlog K\nnk\n\u0013\u0015\u001b!\n≥\ninf\n{bµ(k)}k∈S\nsup\n{µ(k)}k∈S∈ΥS\nP{µ(k)}k∈S · QS\n [\nk∈S\n\u001a\n∥bµ(k) −µ(k)∥2 ≥C\n\u0014\nh ∧\n\u0012\nc\nr\nlog K\nnk\n\u0013\u0015\u001b!\n,\n70\nConsider M (k) = {µ(j)}j∈S where µ(j) =\nrj\n√\np−3/4 ·1p+µ0 for j ̸= k and µ(k) =\nrk\n2√\np−3/4 ·1p+\nµ0, where µ0 = (1, 0⊤\np−1)⊤. Define two new “distances” (which are not rigorously distances\nbecause triangle inequalities and the definiteness do not hold) between M = {µ(k)}k∈S and\nas M ′ = {µ′(k)}k∈S\ned(M, M ′) :=\nX\nk∈S\n1\n \n∥µ(k) −µ′(k)∥2 ≥\nrk\n2\np\np −3/4\n!\n,\ned′(M, M ′) :=\nX\nk∈S\n1\n \n∥µ(k) −µ′(k)∥2 ≥\nrk\n4\np\np −3/4\n!\n.\nTherefore ed(M (k), M (k′)) = 2 when k ̸= k′. For {bµ(k)}k∈S, define ψ∗= arg mink∈S ed′({bµ(k)}k∈S,\nM (k)). Because ed(M1, M2) ≤ed′(M1, M2) + ed′(M2, M3) for any M1, M2 and M3, it’s\neasy to see that\ninf\n{bµ(k)}k∈S\nsup\n{µ(k)}k∈S∈ΥS\nP{µ(k)}k∈S · QS\n [\nk∈S\n\u001a\n∥bµ(k) −µ(k)∥2 ≥\nrk\n4\np\np −3/4\n\u001b!\n≥\ninf\n{bµ(k)}k∈S\nsup\nk∈S\nPM(k)\n\u0010\ned′({bµ(k)\n1 }k∈S, M (k)) ≥1\n\u0011\n≥\ninf\n{bµ(k)}k∈S\nsup\nk∈S\nPM(k) (ψ∗̸= k)\n≥inf\nψ sup\nk∈S\nPM(k) (ψ ̸= k) .\n(S.5.42)\nBy Lemma 15,\nKL\n\u0000PM(k) · QS\n\r\rPM(k′) · QS\n\u0001\n= nkKL\n\u0012\nP⊗nkrk\n√\np−3/4 1p+µ0∥P⊗nkrk\n2√\np−3/4 1p+µ0\n\u0013\n+ nk′KL\n\u0012\nP\n⊗nk′\nrk′\n√\np−3/4 1p+µ0∥P\n⊗nk′\nrk′\n2√\np−3/4 1p+µ0\n\u0013\n≤nk · 8\n\r\r\r\r\r\nrk\np\np −3/4\n1p + µ0\n\r\r\r\r\r\n2\n2\n\r\r\r\r\r\nrk\n2\np\np −3/4\n1p\n\r\r\r\r\r\n2\n2\n+ nk′ · 8\n\r\r\r\r\r\nrk′\np\np −3/4\n1p + µ0\n\r\r\r\r\r\n2\n2\n\r\r\r\r\r\nrk′\n2\np\np −3/4\n1p\n\r\r\r\r\r\n2\n2\n≤nk · 8 · 2 · (2r2\nk + 1) · 1\n4 · 2r2\nk + nk′ · 8 · 2 · (2r2\nk′ + 1) · 1\n4 · 2r2\nk′\n≤16c2 log K,\nwhen p ≥3. By Fano’s lemma (See Corollary 2.6 in Tsybakov, 2009),\nRHS of (S.5.42) ≥1 −log 2\nlog K −16c2 ≥1\n4,\n71\nwhen K ≥3, c =\np\n1/160, and mink∈S nk ≥(c2 ∨M −2) log K.\n(iv) We want to show\ninf\n{bθ(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS,w\nQS\nP\n [\nk∈S\n\u001a\n| bw(k) −w(k)∗| ∧|1 −bw(k) −w(k)∗|\n≥C\nr\nlog K\nnk\n\u001b!\n≥1\n4.\nThe argument is similar to part (iii). The only two differences here are that the dimension\nof interested parameter w equals 1, and Lemma 15 is replaced by Lemma 17.\n(v) We want to show\ninf\n{bδ(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS,δ\nQS\nP\n [\nk∈S\n\u001a\n|bδ(k) −δ(k)∗| ∧|bδ(k) + δ(k)∗|\n≥C\nr\nlog K\nnk\n\u001b!\n≥1\n4,\nThe argument is similar to (iii). The only two differences here are that the dimension of\ninterested parameter δ equals 1, and Lemma 15 is replaced by Lemma 18.\nFinally, we get the desired conclusion by combining (i)-(v).\nProof of Lemma 20. Let eϵ =\nK−s\ns\nand eϵ′ =\nK−s\nK . Since s/K ≥c > 0, eϵ ≲eϵ′. Denote\nΥS = {{µ(k)}k∈S : µ(k) ∈Rp\n+, minµ maxk∈S ∥µ(k) −µ∥2 ≤hβ/2, ∥µ(k)∥2 ≤M}. For any\nµ ∈R, denote distribution\n1\n2N(µ, Ip) + 1\n2N(−µ, Ip) as Pµ, and denote Q\nk∈S P⊗nk\nµ(k) as\nP{µ(k)}k∈S. Note that β(k) = 2µ(k) for Pµ(k) with {µ(k)}k∈S ∈ΥS. Then it suffices to show\ninf\n{bµ(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{µ(k)}k∈S∈ΥS\nQS\nP\n \nmax\nk∈S ∥bµ(k) −µ(k)∥2 ≥C1eϵ′\nr\n1\nmaxk=1:K nk\n!\n≥1\n10,\n(S.5.43)\nwhere P = P{µ(k)}k∈S · QS. WLOG, assume M ≥1. For any eµ1, eµ2 ∈Rp with ∥eµ1∥2 =\n∥eµ2∥2 = 1, by Lemma 15,\nmax\nk=1:K KL\n\u0000P⊗nk\neµ1 ∥P⊗nk\neµ2\n\u0001\n≤max\nk=1:K nk · 8∥eµ1 −eµ2∥2\n2.\nLet 8 maxk=1:K nk · ∥eµ1 −eµ2∥2\n2 ≤(\neϵ′\n1−eϵ′)2, then ∥eµ1 −eµ2∥2 ≤C\nq\n1\nmaxk=1:K nkeϵ′ for some\nconstant C > 0. Then (S.5.43) follows by Lemma 22.\n72\nProof of Lemma 21. The proof is similar to the proof of Theorem 5.1 in Chen et al. (2018),\nso we omit it here.\nProof of Lemma 22. It’s easy to see that\nLHS of (S.5.39) ≥inf\nbθ\nsup\nθ∈Θ\nQS\nES∼Ps\n\u0014\nP(S,θ,QS)\n\u0012\n∥bθ −θ∥≥Cϖ\n\u0012K −s\n50K , Θ\n\u0013\u0013\u0015\n,\nwhere Ps can be any probability measure on all subsets of 1 : K with Ps(|S| ≥s) = 1.\nConsider a special distribution ePs as Ps:\nePs(S = S′) =\nP|Sc|∼Bin(K, K−s\n50K )(|S| = |S′|)\nP|Sc|∼Bin(K, K−s\n50K )(|Sc| ≤41(K−s)\n50\n)\n·\n1\n\u0000 K\n|S′|\n\u0001,\nfor any S′ with |(S′)c| ≤41(K−s)\n50\n. Given S, consider the distribution of {x(k)}K\nk=1 as\nPS =\nY\nk∈S\nP(k)\nθ\n·\nY\nk/∈S\nQ(k)\nThen consider the distribution of {x(k)}K\nk=1 as\nP′ =\nX\nS:|Sc|≤41(K−s)\n50\nePs(S) · PS.\nIt’s easy to see that P′ is the same as P( K−s\n50K ,θ,{Q(k)}K\nk=1) conditioning on the event\n\b\nS : |Sc| ≤\n41(K−s)\n50\n\t\n. Therefore,\ninf\nbθ\nsup\nθ∈Θ\nQS\nES∼ePs\n\u0014\nP(S,θ,QS)\n\u0012\n∥bθ −θ∥≥Cϖ\n\u0012K −s\n50K , Θ\n\u0013\u0013\u0015\n≥inf\nbθ\nsup\nθ∈Θ\n{Q(k)}K\nk=1\nES∼ePs\n\u0014\nPS\n\u0012\n∥bθ −θ∥≥Cϖ\n\u0012K −s\n50K , Θ\n\u0013\u0013\u0015\n= inf\nbθ\nsup\nθ∈Θ\n{Q(k)}K\nk=1\nP′\n\u0012\n∥bθ −θ∥≥Cϖ\n\u0012K −s\n50K , Θ\n\u0013\u0013\n≥inf\nbθ\nsup\nθ∈Θ\n{Q(k)}K\nk=1\nP( K−s\n50K ,θ,{Q(k)}K\nk=1)\n\u0012\n∥bθ −θ∥≥Cϖ\n\u0012K −s\n50K , Θ\n\u0013 \f\f\f\f|Sc| ≤41(K −s)\n50\n\u0013\n≥inf\nbθ\nsup\nθ∈Θ\n{Q(k)}K\nk=1\nP( K−s\n50K ,θ,{Q(k)}K\nk=1)\n\u0012\n∥bθ −θ∥≥Cϖ\n\u0012K −s\n50K , Θ\n\u0013\u0013\n−P|Sc|∼Bin(K, K−s\n50K )\n\u0012\n|Sc| > 41(K −s)\n50\n\u0013\n73\n≥1\n2 −exp\n(\n−\n1\n2[4\n5(K −s)]2\nK · K−s\n50K\n\u00001 −K−s\n50K\n\u0001\n+ 1\n3 · 4\n5(K −s)\n)\n≥1\n2 −exp\n\u001a\n−\n1\n2 · (4\n5)2\n1\n50 + 4\n15\n\u001b\n≥1\n10,\nwhere the last third inequality comes from Bernstein’s inequality, application of Lemma 17\nand the fact that d2\nTV(Pθ1, Pθ2) ≤KL(Pθ1∥Pθ2).\nProof of Lemma 23. (i) We want to show\ninf\n{bΣ(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS\nQS\nP\n [\nk∈S\n\u001a\n∥bΣ(k) −Σ(k)∗∥2 ≥C\nr p\nnk\n\u001b!\n≥1\n10.\nFix S and some QS. WLOG, assume 1 ∈S. Then it suffices to show\ninf\nbΣ(k)\nsup\n{θ\n(k)∗}k∈S∈ΘS\nP\n \n∥bΣ(k) −Σ(k)∗∥2 ≥C\nr p\nnk\n!\n≥1\n10.\nConsider a special subset of ΘS as\nΘS,Σ = {θ : w = 1/2, µ1 = µ2 = 0, Σ = Σ(γ), γ ∈{0, 1}p},\nwhere\nΣ(γ) =\n\n\n\n\n\nγ1e⊤\n1\n...\nγpe⊤\n1\n\n\n\n\n· τ + Ip,\nand τ > 0 is a small constant which we will specify later. For any γ ∈{0, 1}p, denote\nN(0, Σ(γ)) as Pγ. Therefore it suffices to show\ninf\nbΣ(1)\nsup\nγ∈{0,1}p P\n \n∥bΣ(1) −Σ(1)∗∥2 ≥C\nr p\nn1\n!\n≥1\n10.\n(S.5.44)\nNote that for any bΣ(1), we can define bγ = arg minγ∈{0,1}p ∥bΣ(1) −Σ(γ)∥2. Then by triangle\ninequality and definition of bγ, ∥bΣ(1) −Σ(γ)∥2 ≥∥Σ(bγ) −Σ(γ)∥2/2. Therefore\nLHS of (S.5.44) ≥\ninf\nbγ∈{0,1}p\nsup\nγ∈{0,1}p P\n \n∥Σ(bγ) −Σ(γ)∥2 ≥2C\nr p\nn1\n!\n.\n(S.5.45)\n74\nLet τ = c\np\n1/n1 where c > 0 is a small constant. Since ∥Σ(bγ) −Σ(γ)∥2 ≤τ for any bγ\nand γ ∈{0, 1}p, by Lemma D.2 in Duan and Wang (2023),\nLHS of (S.5.45) ≥\ninf bγ∈{0,1}p supγ∈{0,1}p E∥Σ(bγ) −Σ(γ)∥2\n2 −4C2 · p\nn1\n(c2 −4C2) p\nn1\n.\n(S.5.46)\nApplying Assouad’s lemma (Theorem 2.12 in Tsybakov, 2009 or Lemma 2 in Cai and Zhou,\n2012), we get\ninf\nbγ∈{0,1}p\nsup\nγ∈{0,1}p E∥Σ(bγ) −Σ(γ)∥2\n2 ≥p\n8\nmin\nρH(γ,γ′)≥1\n\u0014∥Σ(γ) −Σ(γ′)∥2\n2\nρH(γ, γ′)\n\u0015\n·\n\u0014\n1 −\nmax\nρH(γ,γ′)=1\n\u0000KL(P⊗n1\nγ\n∥P⊗n1\nγ′ )\n\u00011/2\n\u0015\n, (S.5.47)\nwhere ρH is the Hamming distance. For the first term on the RHS, it’s easy to see that\n∥Σ(γ) −Σ(γ′)∥2\n2 = τ 2ρH(γ, γ′),\nfor any γ and γ′ ∈{0, 1}p. For the second term, by the density form of Gaussian distribu-\ntion, we can show that if ρH(γ, γ′) = 1, then\nKL(P⊗n1\nγ\n∥P⊗n1\nγ′ ) = n1KL(Pγ∥Pγ′)\n≤n1 · 1\n2\n\b\nlog(|Σ(γ′)|/|Σ(γ)|) −Tr\n\u0002\n(Σ(γ)−1 −Σ(γ′)−1)Σ(γ)\n\u0003\t\n≤n1 · 1\n4τ 2\n≤c2\n4 .\nPlugging this back into (S.5.47), combining with (S.5.46), we have\nLHS of (S.5.45) ≥\nc2 ·\np\n8n1(1 −c\n2) −4C2 · p\nn1\n(c2 −4C2) p\nn1\n≥1\n10,\nwhen c = 2/9 and C ≤c/\n√\n324.\n(ii) We want to show\ninf\n{bΣ(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS\nQS\nP\n [\nk∈S\n\u001a\n∥bΣ(k) −Σ(k)∗∥2 ≥C\nr\nlog K\nnk\n\u001b!\n≥1\n10.\nThe proof idea is similar to part (iii) of the proof of Lemma 19, so we omit the details\nhere. It suffices to consider M (k) = {Σ(j)}K\nj=1 where Σ(j) = Ip when j ̸= k and Σ(k) =\nIp +\np\nlog K/nk · e1e⊤\n1 .\n75\nS.5.4\nProof of Theorem 2\nWe claim that with probability at least 1 −CK−1,\nRθ\n(k)∗( bC(k)[t]) −Rθ\n(k)∗(Cθ\n(k)∗) ≲d2(bθ(k)[t], θ(k)∗).\n(S.5.48)\nThen the conclusion immediately follows from Theorem 1. Hence it suffices to verify the\nclaim. For convenience, we write bC(k)[t] = Cbθ(k)[t] simply as Cbθ(k) and bθ(k)[t] as bθ(k).\nBy simple calculations, we have\nRθ\n(k)∗(Cbθ(k)) = (1 −w(k)∗)Φ\n\n−log(1−bw(k)\nbw(k) ) −bδ(k) + ( bβ(k))⊤µ(k)∗\n1\nq\n( bβ(k))⊤Σ(k)∗bβ(k)\n\n\n+ w(k)∗Φ\n\nlog(1−bw(k)\nbw(k) ) + bδ(k) −( bβ(k))⊤µ(k)∗\n2\nq\n( bβ(k))⊤Σ(k)∗bβ(k)\n\n,\nRθ\n(k)∗(Cθ(k)∗) = (1 −w(k)∗)Φ\n \n−log(1−w(k)∗\nw(k)∗) −δ(k)∗+ (β(k)∗)⊤µ(k)∗\n1\np\n(β(k)∗)⊤Σ(k)∗β(k)∗\n!\n+ w(k)∗Φ\n \nlog(1−w(k)∗\nw(k)∗) + δ(k)∗−(β(k)∗)⊤µ(k)∗\n2\np\n(β(k)∗)⊤Σ(k)∗β(k)∗\n!\n.\nThen by Taylor expansion,\nRθ\n(k)∗(Cbθ(k)) −Rθ\n(k)∗(Cθ(k)∗) ≤(1 −w(k)∗)Φ′\n \n−log(1−w(k)∗\nw(k)∗) −δ(k)∗+ (β(k)∗)⊤µ(k)∗\n1\np\n(β(k)∗)⊤Σ(k)∗β(k)∗\n!\n·\n\"\n−log(1−bw(k)\nbw(k) ) −bδ(k) + ( bβ(k))⊤µ(k)∗\n1\nq\n( bβ(k))⊤Σ(k)∗bβ(k)\n−−log(1−w(k)∗\nw(k)∗) −δ(k)∗+ (β(k)∗)⊤µ(k)∗\n1\np\n(β(k)∗)⊤Σ(k)∗β(k)∗\n#\n+ w(k)∗Φ′\n \nlog(1−w(k)∗\nw(k)∗) + δ(k)∗−(β(k)∗)⊤µ(k)∗\n2\np\n(β(k)∗)⊤Σ(k)∗β(k)∗\n!\n·\n\"\nlog(1−bw(k)\nbw(k) ) + bδ(k) −( bβ(k))⊤µ(k)∗\n2\nq\n( bβ(k))⊤Σ(k)∗bβ(k)\n−log(1−w(k)∗\nw(k)∗) + δ(k)∗−(β(k)∗)⊤µ(k)∗\n2\np\n(β(k)∗)⊤Σ(k)∗β(k)∗\n#\n+ C\n\"\n−log(1−bw(k)\nbw(k) ) −bδ(k) + ( bβ(k))⊤µ(k)∗\n1\nq\n( bβ(k))⊤Σ(k)∗bβ(k)\n−−log(1−w(k)∗\nw(k)∗) −δ(k)∗+ (β(k)∗)⊤µ(k)∗\n1\np\n(β(k)∗)⊤Σ(k)∗β(k)∗\n#2\n+ C\n\"\nlog(1−bw(k)\nbw(k) ) + bδ(k) −( bβ(k))⊤µ(k)∗\n2\nq\n( bβ(k))⊤Σ(k)∗bβ(k)\n−log(1−w(k)∗\nw(k)∗) + δ(k)∗−(β(k)∗)⊤µ(k)∗\n2\np\n(β(k)∗)⊤Σ(k)∗β(k)∗\n#2\n.\nDenote A = (1 −w(k)∗)Φ′\n \n−log( 1−w(k)∗\nw(k)∗\n)−δ(k)∗+(β(k)∗)⊤µ(k)∗\n1\n√\n(β(k)∗)⊤Σ(k)∗β(k)∗\n!\n·\n\"\n−log( 1−b\nw(k)\nb\nw(k)\n)−bδ(k)+( bβ(k))⊤µ(k)∗\n1\n√\n( bβ(k))⊤Σ(k)∗bβ(k)\n−\n76\n−log( 1−w(k)∗\nw(k)∗\n)−δ(k)∗+(β(k)∗)⊤µ(k)∗\n1\n√\n(β(k)∗)⊤Σ(k)∗β(k)∗\n#\n+w(k)∗Φ′\n \nlog( 1−w(k)∗\nw(k)∗\n)+δ(k)∗−(β(k)∗)⊤µ(k)∗\n2\n√\n(β(k)∗)⊤Σ(k)∗β(k)∗\n!\n·\n\"\nlog( 1−b\nw(k)\nb\nw(k)\n)+bδ(k)−( bβ(k))⊤µ(k)∗\n2\n√\n( bβ(k))⊤Σ(k)∗bβ(k)\n−\nlog( 1−w(k)∗\nw(k)∗\n)+δ(k)∗−(β(k)∗)⊤µ(k)∗\n2\n√\n(β(k)∗)⊤Σ(k)∗β(k)∗\n#\nand B = C\n\"\n−log( 1−b\nw(k)\nb\nw(k)\n)−bδ(k)+( bβ(k))⊤µ(k)∗\n1\n√\n( bβ(k))⊤Σ(k)∗bβ(k)\n−\n−log( 1−w(k)∗\nw(k)∗\n)−δ(k)∗+(β(k)∗)⊤µ(k)∗\n1\n√\n(β(k)∗)⊤Σ(k)∗β(k)∗\n#2\n+\nC\n\"\nlog( 1−b\nw(k)\nb\nw(k)\n)+bδ(k)−( bβ(k))⊤µ(k)∗\n2\n√\n( bβ(k))⊤Σ(k)∗bβ(k)\n−\nlog( 1−w(k)∗\nw(k)∗\n)+δ(k)∗−(β(k)∗)⊤µ(k)∗\n2\n√\n(β(k)∗)⊤Σ(k)∗β(k)∗\n#2\n. By plugging in the density for-\nmula of standard Gaussian distribution, it is easy to see that\nA ≲\nq\n(1 −w(k)∗)w(k)∗· exp\n(\n−\n\u0002\nlog(1−w(k)∗\nw(k)∗) + 1\n2(β(k)∗)⊤Σ(k)∗β(k)∗\u00032\n2(β(k)∗)⊤Σ(k)∗β(k)∗\n+ 1\n2 log\n\u00121 −w(k)∗\nw(k)∗\n\u0013)\n·\n\"\n−log(1−bw(k)\nbw(k) ) −bδ(k) + ( bβ(k))⊤µ(k)∗\n1\nq\n( bβ(k))⊤Σ(k)∗bβ(k)\n−−log(1−w(k)∗\nw(k)∗) −δ(k)∗+ (β(k)∗)⊤µ(k)∗\n1\np\n(β(k)∗)⊤Σ(k)∗β(k)∗\n#\n+\nq\n(1 −w(k)∗)w(k)∗· exp\n(\n−\n\u0002\nlog(1−w(k)∗\nw(k)∗) −1\n2(β(k)∗)⊤Σ(k)∗β(k)∗\u00032\n2(β(k)∗)⊤Σ(k)∗β(k)∗\n+ 1\n2 log\n\u0012\nw(k)∗\n1 −w(k)∗\n\u0013)\n·\n\"\nlog(1−bw(k)\nbw(k) ) + bδ(k) −( bβ(k))⊤µ(k)∗\n2\nq\n( bβ(k))⊤Σ(k)∗bβ(k)\n−log(1−w(k)∗\nw(k)∗) + δ(k)∗−(β(k)∗)⊤µ(k)∗\n2\np\n(β(k)∗)⊤Σ(k)∗β(k)∗\n#\n=\nq\n(1 −w(k)∗)w(k)∗· exp\n(\n−1\n8[(β(k)∗)⊤Σ(k)∗β(k)∗]2 −1\n2 ·\nlog2(1−w(k)∗\nw(k)∗)\n(β(k)∗)⊤Σ(k)∗β(k)∗\n)\n·\n\f\f\f\f\f\f\n( bβ(k))⊤(µ(k)∗\n1\n−µ(k)∗\n2\n)\nq\n( bβ(k))⊤Σ(k)∗bβ(k)\n−(β(k)∗)⊤(µ(k)∗\n1\n−µ(k)∗\n2\n)\np\n(β(k)∗)⊤Σ(k)∗β(k)∗\n\f\f\f\f\f\f\n≲\n\f\f\f\f\f\n(bξ(k))⊤ξ(k)∗\n∥bξ(k)∥2\n−∥ξ(k)∗∥2\n\f\f\f\f\f\n≲∥bξ(k) −ξ(k)∗∥2\n2,\n(S.5.49)\nwith probability at least 1−C′K−1, where bξ(k) = (Σ(k)∗)1/2 bβ(k) and ξ(k)∗= (Σ(k)∗)1/2β(k)∗,\nso ∥bξ(k) −ξ(k)∗∥2\n2 ≲∥bβ(k) −β(k)∗∥2\n2. Only the last inequality in (S.5.49) holds with high\nprobability and the others are deterministic. It comes from the fact that ∥bξ(k) −ξ(k)∗∥2 ≤\nc ≤∥ξ(k)∗∥2 for some c > 0 with probability at least 1 −C′K−1 and a direct application of\nLemma 8.1 in Cai et al. (2019). On the other hand, it is easy to see that B ≲d2(bθ(k), θ(k)∗).\nCombining these two facts leads to (S.5.48).\n77\nS.5.5\nProof of Theorem 4\nS.5.5.1\nLemmas\nRecall that for GMM associated with parameter set θ = (w, µ1, µ2, Σ), we define the mis-\nclustering error rate of any classifier C as Rθ(C) = minπ:{1,2}→{1,2} Pθ(C(Znew) ̸= π(Y new)),\nwhere Pθ represents the distribution of (Znew, Y new), i.e. (1 −w)N(µ1, Σ) + wN(µ2, Σ).\nDenote Cθ as the Bayes classifier corresponding to θ. Define a surrogate loss Lθ(C) =\nminπ:{1,2}→{1,2} Pθ(C(Znew) ̸= π(Cθ(Y new))).\nLemma 24. Assume there exists an subset S such that mink∈S nk ≥C(p ∨log K) and\nmink∈S ∆(k) ≥σ2 > 0 with some constants C > 0. We have\ninf\n{ bC(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS\nQS\nP\n [\nk∈S\n\u001a\nRθ\n(k)∗( bC(k)) −Rθ\n(k)∗(Cθ\n(k)∗) ≥C1\np\nnS\n+ C2\nlog K\nnk\n+ C3h2 ∧p + log K\nnk\n+ C4\nϵ2\nmaxk∈S nk\n\u001b!\n≥1\n10.\nLemma 25. Suppose θ = (w, µ1, µ2, β, Σ) satisfies ∆2 := (µ1−µ2)⊤Σ−1(µ1−µ2) ≥σ2 >\n0 with some constant σ2 > 0 and w, w′ ∈(cw, 1 −cw). Then ∃c > 0 such that\ncL2\nθ(C) ≤Rθ(C) −Rθ(Cθ),\nfor any classifier C, where Rθ(C) := minπ:1:2→1:2 Pθ(C(z) ̸= π(y)), Lθ(C) := minπ:{0,1}→{0,1}\nPθ(C(z) ̸= π(Cθ(z))), and Cθ is the corresponding Bayes classifier.\nLemma 26. Consider θ = (w, µ1, µ2, β, Σ) and θ\n′ = (w′, µ1, µ2, β, Σ) satisfies ∆2 :=\n(µ1 −µ2)⊤Σ−1(µ1 −µ2) ≥σ2 > 0 with some constant σ2 > 0.. We have\nc|w −w′| ≤Lθ(Cθ\n′) ≤c′|w −w′|,\nfor some constants c, c′ > 0.\nLemma 27. Consider θ = (w, µ1, µ2, Σ) and θ\n′ = (w, µ′\n1, µ′\n2, Σ) satisfies w = 1/2, µ1 =\n−µ0/2 + u, µ2 = µ0/2 + u, µ′\n1 = −µ0/2 + u′, µ′\n2 = µ0/2 + u′, Σ = Ip, µ0 = (1, 0⊤\np−1)⊤,\nu = (eu, 0⊤\np−1)⊤, and u′ = (eu′, 0⊤\np−1)⊤. We have\nc|eu −eu′| ≤Lθ(Cθ\n′) ≤c′|eu −eu′|,\nfor some constants c, c′ > 0.\n78\nLemma 28. Denote eϵ = K−s\ns . We have\ninf\n{ bC(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS\nQS\nP\n \nmax\nk∈S\nh\nRθ\n(k)∗( bC(k)) −Rθ\n(k)∗(Cθ\n(k)∗)\ni\n≥C1\neϵ′2\nmaxk=1:K nk\n!\n≥1\n10.\nS.5.5.2\nMain proof of Theorem 4\nCombine conclusions of Lemmas 24 and 28 to get the lower bound.\nS.5.5.3\nProof of lemmas\nProof of Lemma 24. Recall the definitions and proof idea of Lemma 19. We have ΘS ⊇\nΘ|S|,w ∪Θ|S|,δ ∪Θ|S|,β, where\nΘS,w =\nn\n{θ\n(k)}k∈S : µ(k)\n1\n= 1p/√p, µ(k)\n2\n= −µ(k)\n1\n= eµ, Σ(k) = Ip, w(k) ∈(cw, 1 −cw)\no\n,\nΘS,β =\nn\n{θ\n(k)}k∈S : Σ(k) = Ip, w(k) = 1\n2, ∥µ(k)\n1 ∥2 ∨∥µ(k)\n2 ∥2 ≤M, µ(k)\n2\n= −µ(k)\n1 ,\nmin\nβ max\nk∈S ∥β(k) −β∥2 ≤h\no\n,\nΘS,δ =\nn\n{θ\n(k)}k∈S : Σ(k) = Ip, w(k) = 1\n2, ∥µ(k)\n1 ∥2 ∨∥µ(k)\n2 ∥2 ≤M, µ(k)\n1\n= −1\n2µ0, µ(k)\n2\n= 1\n2µ0 + u,\n∥u∥2 ≤1\no\n.\nRecall the mis-clustering error for GMM associated with parameter set θ of any classifier C\nis Rθ(C) = minπ:{1,2}→{1,2} Pθ(C(Z) ̸= π(Y )). To help the analysis, following Azizyan et al.\n(2013) and Cai et al. (2019), we define a surrogate loss Lθ(C) = minπ:{1,2}→{1,2} Pθ(C(Z) ̸=\nπ(Cθ(Z))), where Cθ is the Bayes classifier. Suppose σ =\n√\n0.005.\n(i) We want to show\ninf\n{ bC(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS\nQS\nP\n [\nk∈S\n\u001a\nRθ\n(k)∗( bC(k)) −Rθ\n(k)∗(Cθ\n(k)∗) ≥C\nr p\nnS\n\u001b!\n≥1\n4. (S.5.50)\nConsider S = 1 : K and space Θ0 = {{θ\n(k)}K\nk=1 : Σ(k) = Ip, w(k) = 1/2, µ(k)\n1\n= µ1, µ(k)\n2\n=\nµ2, ∥µ1∥2 ∨∥µ2∥2 ≤M}. And\nLHS of (S.5.50) ≥inf\nbC(1)\nsup\n{θ\n(k)∗}K\nk=1∈Θ0\nP\n \nRθ\n(1)∗( bC(1)) −Rθ\n(1)∗(Cθ\n(1)∗) ≥C\nr p\nnS\n!\n.\n79\nLet r = c\np\np/nS ≤0.001 with some small constant c > 0.\nFor any µ ∈Rp, denote\ndistribution 1\n2N(µ, Ip)+ 1\n2N(−µ, Ip) as Pµ. Consider a r/4-packing of rSp−1: {evj}N\nj=1. By\nLemma 2, N ≥4p−1. Denote eµj = (σ, ev⊤\nj )⊤∈Rp, where σ =\n√\n0.005. Then by definition\nof KL divergence and Lemma 8.4 in Cai et al. (2019),\nKL\n Y\nk∈S\nP⊗nk\neµj\n· QS\n\r\r\r\r\nY\nk∈S\nP⊗nk\neµj′ · QS\n!\n=\nX\nk∈S\nnkKL(Peµj∥Peµj′)\n≤nS · 8(1 + σ2)∥eµj −eµj′∥2\n2\n≤32(1 + σ2)nSr2\n≤32(1 + σ2)nS · c22(p −1)\nnS\n≤32(1 + σ2)c2\nlog 2\nlog N.\nFor simplicity, we write Lθ with θ ∈Θ0 and µ1 = −µ2 = µ as Lµ. By Lemma 8.5 in Cai\net al. (2019),\nLeµi(Ceµj) ≥1\n√\n2g\n √\nσ2 + r2\n2\n!\n∥eµi −eµj∥2\n∥eµi∥2\n≥1\n√\n2 · 0.15 ·\nr/4\n√\nσ2 + r2 ≥2r,\nwhere g(x) = ϕ(x)[ϕ(x) −xΦ(x)]. The last inequality holds because\n√\nσ2 + r2 ≥\n√\n2σ and\ng(\n√\nσ2 + r2/2) ≥0.15 when r2 ≤σ2 = 0.001. Then by Lemma 3.5 in Cai et al. (2019)\n(Proposition 2 in Azizyan et al., 2013), for any classifier C, and i ̸= j,\nLeµi(C) + Leµj(C) ≥Leµi(Ceµj) −\nq\nKL(Peµi∥Peµj)/2 ≥2r −r = c\nr p\nnS\n.\n(S.5.51)\nFor any bC(1), consider a test ψ∗= arg minj=1:N Leµj( bC(1)). Therefore if there exists j0 such\nthat Leµj0( bC(1)) < c\n2\nq\np\nnS , then by (S.5.51), we must have ψ∗= j0. Let C1 ≤c/2, then by\nFano’s lemma (Corollary 6 in Tsybakov, 2009)\ninf\nbC(1)\nsup\n{θ\n(k)∗}K\nk=1∈Θ0\nP\n \nLθ\n(1)∗( bC(1)) ≥C1\nr p\nnS\n!\n≥inf\nbC(1) sup\nj=1:N\nP\n \nLeµ(j)( bC(1)) ≥C1\nr p\nnS\n!\n≥inf\nbC(1) sup\nj=1:N\nP\n \nψ∗̸= j\n!\n≥inf\nψ\nsup\nj=1:N\nP\n \nψ ̸= j\n!\n≥1 −log 2\nlog N −32(1 + σ2)c2\nlog 2\n80\n≥1\n4,\nwhen p ≥2 and c =\nq\nlog 2\n128(1+σ2). Then apply Lemma 25 to get the (S.5.50).\n(ii) We want to show\ninf\n{ bC(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS,β\nQS\nP\n [\nk∈S\n\u001a\nRθ\n(k)∗( bC(k)) −Rθ\n(k)∗(Cθ\n(k)∗) ≥Ch ∧\nr p\nnk\n\u001b!\n≥1\n4.\n(S.5.52)\nFixing an S and a QS. Suppose 1 ∈S. We have\nLHS of (S.5.52) ≥inf\nbC(1)\nsup\n{θ\n(k)∗}k∈S∈ΘS,β\nQS\nP\n\u0012\nRθ\n(1)∗( bC(1))−Rθ\n(1)∗(Cθ\n(1)∗) ≥Ch∧\nr p\nnk\n!\n. (S.5.53)\nLet r = h∧(c\np\np/n1)∧M with a small constant c > 0. For any µ ∈Rp, denote distribution\n1\n2N(µ, Ip) + 1\n2N(−µ, Ip) as Pµ. Consider a r/4-packing of rSp−1. By Lemma 2, N ≥4p−1.\nDenote eµj = (σ, ev⊤\nj )⊤∈Rp. WLOG, assume M ≥2. Let µ(k)∗\n1\n= eµ = (σ, 0p−1)⊤for\nall k ∈S\\{1}. Then by following the same arguments in (i) and part (ii) of the proof of\nLemma 19, we can show that the RHS of (S.5.53) is larger than or equal to 1/4 when p ≥3.\n(iii) We want to show\ninf\n{ bC(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS,β\nQS\nP\n [\nk∈S\n\u001a\nRθ\n(k)∗( bC(k))−Rθ\n(k)∗(Cθ\n(k)∗) ≥Ch∧\nr\nlog K\nnk\n\u001b!\n≥1\n4.\nThis can be proved by following similar ideas used in step (iii) of the proof of Lemma 19,\nso we omit the proof here.\n(iv) We want to show\ninf\n{ bC(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS,w\nQS\nP\n [\nk∈S\n\u001a\nRθ\n(k)∗( bC(k)) −Rθ\n(k)∗(Cθ\n(k)∗) ≥C\nr\nlog K\nnk\n\u001b!\n≥1\n4.\nThis can be similarly proved by following the arguments in part (i) with Lemmas 25 and\n26.\n(v) We want to show\ninf\n{ bC(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS,δ\nQS\nP\n [\nk∈S\n\u001a\nRθ\n(k)∗( bC(k)) −Rθ\n(k)∗(Cθ\n(k)∗) ≥C\nr\nlog K\nnk\n\u001b!\n≥1\n4.\n81\nThis can be similarly proved by following the arguments in part (i) with Lemmas 25 and\n27.\nFinally, we get the desired conclusion by combining (i)-(v).\nProof of Lemma 25. We follow a similar proof idea used in the proof of Lemma 3.4 in Cai\net al. (2019). Let ϕ1 and ϕ2 be the density of N(µ, Σ) and N(−µ, Σ), respectively. Denote\nηθ(z) =\n(1−w)ϕ1(z)\n(1−w)ϕ1(z)+wϕ2(z) and SC = {z ∈Rp : C(z) = 1} for any classifier C. Note that\nSCθ = {z ∈Rp : (1 −w)ϕ1(z) ≥wϕ2(z)}. The permutation actually doesn’t matter in\nthe proof. WLOG, we drop the permutations in the definition of misclassification error\nand surrogate loss by assuming π to be the identity function. If π is not identity in the\ndefinition of Rθ(C), for example, we can define SC = {z ∈Rp : C(z) = 2} instead and all\nthe following steps still follow.\nBy definition,\nPθ(C(z) ̸= y) = (1 −w)\nZ\nSc\nC\nϕ1dz + w\nZ\nSC\nϕ2dz,\nPθ(Cθ(z) ̸= y) = (1 −w)\nZ\nSc\nCθ\nϕ1dz + w\nZ\nSCθ\nϕ2dz,\nwhich leads to\nPθ(C(z) ̸= y) −Pθ(Cθ(z) ̸= y) =\nZ\nSCθ \\SC\n[(1 −w)ϕ1 −wϕ2]dz +\nZ\nSc\nCθ \\Sc\nC\n[wϕ2 −(1 −w)ϕ1]dz\n=\nZ\nSCθ △SC\n|(1 −w)ϕ1 −wϕ2|dz\n= Ez∼(1−w)ϕ1+wϕ2\n\u0002\n|2ηθ(z) −1| 1(SCθ△SC)\n\u0003\n≥2t · Pθ\n\u0000SCθ△SC, |2ηθ(z) −1| > 2t\n\u0001\n= 2t\n\u0002\nPθ(SCθ△SC) −Pθ(|2ηθ(z) −1| ≤2t)\n\u0003\n≥2t\n\u0002\nPθ(SCθ△SC) −ct\n\u0003\n≥1\n2cP2\nθ(SCθ△SC),\nwhere we let t =\n1\n2cPθ(SCθ△SC) with c = 1 +\n8\n√\n2πσ. This completes the proof. The last\nsecond inequality depends on the fact that\nPθ(|ηθ(z) −1/2| ≤t) ≤ct,\n82\nholds for all t ≤1/(2c). This is because\nPθ(|ηθ(z) −1/2| ≤t)\n= Pθ\n\u0012\nlog\n\u0012\nw\n1 −w\n\u0013\n+ log\n\u00121 −2t\n1 + 2t\n\u0013\n≤log\n\u0012ϕ1\nϕ2\n(z)\n\u0013\n≤log\n\u0012\nw\n1 −w\n\u0013\n+ log\n\u00121 + 2t\n1 −2t\n\u0013\u0013\n= Pθ\n\u0012\nlog\n\u0012\nw\n1 −w\n\u0013\n+ log\n\u00121 −2t\n1 + 2t\n\u0013\n≤(µ1 −µ2)⊤Σ−1\n\u0012\nz −µ1 + µ2\n2\n\u0013\n≤log\n\u0012\nw\n1 −w\n\u0013\n+ log\n\u00121 + 2t\n1 −2t\n\u0013 \u0013\n= 1\n2Pθ\n\u0012\nlog\n\u0012\nw\n1 −w\n\u0013\n+ log\n\u00121 −2t\n1 + 2t\n\u0013\n≤N(∆2/2, ∆2) ≤log\n\u0012\nw\n1 −w\n\u0013\n+ log\n\u00121 + 2t\n1 −2t\n\u0013 \u0013\n+ 1\n2Pθ\n\u0012\nlog\n\u0012\nw\n1 −w\n\u0013\n+ log\n\u00121 −2t\n1 + 2t\n\u0013\n≤N(−∆2/2, ∆2) ≤log\n\u0012\nw\n1 −w\n\u0013\n+ log\n\u00121 + 2t\n1 −2t\n\u0013 \u0013\n(S.5.54)\n≤\n1\n√\n2πσ ·\n\u0014\nlog\n\u00121 + 2t\n1 −2t\n\u0013\n−log\n\u00121 −2t\n1 + 2t\n\u0013\u0015\n≤\n1\n√\n2πσ ·\n8t\n1 −2t\n≤ct,\n(S.5.55)\nwhen t ≤1/(2c). Note that (S.5.55) implies that a binary GMM under the separation\nassumption ∆≳1 has Tsybakov’s margin with margin parameter 1. For the notion of\nTsybakov’s margin, see Audibert and Tsybakov (2007). We will prove a more general result\nshowing that a multi-cluster GMM under the separation assumption also has Tsybakov’s\nmargin with margin parameter 1. This turns out to be useful in proving the upper and\nlower bounds of misclassification error.\nProof of Lemma 26. WLOG, suppose w ≥w′. Similar to (S.5.54), it’s easy to see that\nLθ(Cθ\n′) = (1 −w)Pθ\n\u0000Cθ(z) ̸= Cθ\n′(z)|z = 1\n\u0001\n+ wPθ\n\u0000Cθ(z) ̸= Cθ\n′(z)|z = 2\n\u0001\n= (1 −w)P\n\u0012\nlog\n\u0012\nw′\n1 −w′\n\u0013\n≤N(∆2/2, ∆2) ≤log\n\u0012\nw\n1 −w\n\u0013\u0013\n+ wP\n\u0012\nlog\n\u0012\nw′\n1 −w′\n\u0013\n≤N(−∆2/2, ∆2) ≤log\n\u0012\nw\n1 −w\n\u0013\u0013\n(S.5.56)\n≤\n1\n√\n2πσ\n\u0014\nlog\n\u0012\nw\n1 −w\n\u0013\n−log\n\u0012\nw′\n1 −w′\n\u0013\u0015\n=\n1\n√\n2πcw(1 −cw)σ · |w −w′|.\n83\nOn the other hand,\n(S.5.56) ≥\n1\n√\n2πMcΣ\n· exp\n(\n−1\n2σ2\n\u0014\nlog\n\u00121 −cw\ncw\n\u0013\n+ 1\n2M 2cΣ\n\u00152) \u0014\nlog\n\u0012\nw\n1 −w\n\u0013\n−log\n\u0012\nw′\n1 −w′\n\u0013\u0015\n≥\n1\n√\n2πMcΣcw(1 −cw) · exp\n(\n−1\n2σ2\n\u0014\nlog\n\u00121 −cw\ncw\n\u0013\n+ 1\n2M 2cΣ\n\u00152)\n|w −w′|,\nwhich completes the proof.\nProof of Lemma 28. By Lemma 25, it suffices to prove\ninf\n{ bC(k)}K\nk=1\nsup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈S∈ΘS\nQS\nP\n \nmax\nk∈S Lθ\n(k)∗( bC(k)) ≥C1\neϵ′2\nmaxk=1:K nk\n!\n≥1\n10.\nFor any µ ∈Rp, denote distribution 1\n2N(µ, Ip) + 1\n2N(−µ, Ip) as Pµ. For simplicity, we\nwrite Lθ with θ satisfying µ1 = −µ2 = µ, w = 1/2 and Σ = Ip as Lµ. Consider Lµ(Cµ′)\nas a loss function between µ and µ′ in Lemmas 21 and 22. Considering ∥µ∥2 = ∥µ′∥2 = 1,\nby Lemma 15, note that\nmax\nk=1:K KL(P⊗nk\nµ\n∥P⊗nk\nµ′ ) ≤8 max\nk=1:K nk · ∥µ −µ′∥2\n2.\nBy Lemma 8.5 in Cai et al. (2019), this implies for some constants c, C > 0\nsup\nn\nLµ(Cµ′) : max\nk=1:K KL(P⊗nk\nµ\n∥P⊗nk\nµ′ ) ≤(eϵ′/(1 −eϵ))2o\n≥sup\nn\nc∥µ −µ′∥2 : max\nk=1:K KL(P⊗nk\nµ\n∥P⊗nk\nµ′ ) ≤(eϵ′/(1 −eϵ))2o\n≥sup\nn\nc∥µ −µ′∥2 : 8 max\nk=1:K nk · ∥µ −µ′∥2\n2 ≤(eϵ′/(1 −eϵ))2o\n= C ·\neϵ′\n√maxk=1:K nk\n.\nThen apply Lemmas 21 and 22 to get the desired bound.\nS.5.6\nProof of Theorem 5\nDenote ξ = maxk∈S minrk=±1 ∥rk bβ(k)[0] −β(k)∗∥2 = maxk∈S(∥bβ(k)[0] −β(k)∗∥2 ∧∥bβ(k)[0] +\nβ(k)∗∥2). WLOG, assume S = {1, . . . , s} and r∗\nk = 1 for all k ∈S. Hence ξ = maxk∈S ∥bβ(k)[0]−\nβ(k)∗∥2. For any k′ = 1, . . . , s, define\nr = (r1, . . . , rk′\n|\n{z\n}\n=−1\n, rk′+1, . . . , rs\n|\n{z\n}\n=1\n, rs+1, . . . , rK\n|\n{z\n}\noutlier tasks\n),\n84\nr′ = (1, 1, . . . , 1, 1, 1, . . . , 1, 1, rs+1, . . . , rK\n|\n{z\n}\noutlier tasks\n),\nr′′ = (−1, . . . , −1, −1, . . . , −1, rs+1, . . . , rK\n|\n{z\n}\noutlier tasks\n).\nWLOG, it suffices to prove that\nscore(r) −score(r′) > 0\nwhen k′ ≤⌊s/2⌋,\n(S.5.57)\nscore(r) −score(r′′) > 0\nwhen k′ > ⌊s/2⌋.\n(S.5.58)\nIn fact, if this holds, then we must have\nbrk = 1 for all k ∈S\nor\nbrk = −1 for all k ∈S.\nOtherwise, according to (S.5.57), if #{k ∈S : brk = −1} ≤⌊s/2⌋, by replacing the first s\nentries of br with 1, we get a different alignment whose score is smaller than the score of br,\nwhich is contradicted with the definition of br. If #{k ∈S : brk = −1} > ⌊s/2⌋, based on\n(S.5.58), by replacing the first s entries of br with −1, we get a different alignment whose\nscore is smaller than the score of br, which is again contradicted with the definition of br.\nIn the following, we prove (S.5.57). The proof of (S.5.58) is almost the same, so we do\nnot repeat it. Under the conditions we assume, it can be shown that\nscore(r) −score(r′) =\nk′\nX\nk1=1\nk′\nX\nk2=1\n∥bβ(k1)[0] −bβ(k2)[0]∥2 + 2\nk′\nX\nk1=1\ns\nX\nk2=k′+1\n∥bβ(k1)[0] + bβ(k2)[0]∥2\n+ 2\nk′\nX\nk1=1\nK\nX\nk2=s+1\n∥bβ(k1)[0] + rk2 bβ(k2)[0]∥2\n−\nk′\nX\nk1=1\nk′\nX\nk2=1\n∥bβ(k1)[0] −bβ(k2)[0]∥2 −2\nk′\nX\nk1=1\ns\nX\nk2=k′+1\n∥bβ(k1)[0] −bβ(k2)[0]∥2\n−2\nk′\nX\nk1=1\nK\nX\nk2=s+1\n∥−bβ(k1)[0] + rk2 bβ(k2)[0]∥2\n= 2\nk′\nX\nk1=1\ns\nX\nk2=k′+1\n∥bβ(k1)[0] + bβ(k2)[0]∥2\n|\n{z\n}\n(1)\n+2\nk′\nX\nk1=1\nK\nX\nk2=s+1\n∥bβ(k1)[0] + rk2 bβ(k2)[0]∥2\n|\n{z\n}\n(2)\n85\n−2\nk′\nX\nk1=1\ns\nX\nk2=k′+1\n∥bβ(k1)[0] −bβ(k2)[0]∥2\n|\n{z\n}\n(1)′\n−2\nk′\nX\nk1=1\nK\nX\nk2=s+1\n∥−bβ(k1)[0] + rk2 bβ(k2)[0]∥2\n|\n{z\n}\n(2)′\n.\n(S.5.59)\nAnd\n(1) −(1)′ =\nk′\nX\nk1=1\ns\nX\nk2=k′+1\n(∥bβ(k1)[0] + bβ(k2)[0]∥2 −∥bβ(k1)[0] −bβ(k2)[0]∥2)\n≥\nk′\nX\nk1=1\ns\nX\nk2=k′+1\n(∥β(k1)∗+ β(k2)∗∥2 −∥β(k1)∗−β(k2)∗∥2 −4ξ)\n≥\nk′\nX\nk1=1\ns\nX\nk2=k′+1\n(2∥β(k1)∗∥2 −2∥β(k1)∗−β(k2)∗∥2 −4ξ)\n≥2(s −k′)\nk′\nX\nk1=1\n∥β(k1)∗∥2 −4k′(s −k′)hβ −4k′(s −k′)ξ,\n(2) −(2)′ ≥−\nk′\nX\nk1=1\nK\nX\nk2=s+1\n2∥bβ(k1)[0]∥2 ≥−2(K −s)\nk′\nX\nk1=1\n∥β(k1)∗∥2 −2k′(K −s)ξ.\nCombining all these pieces,\nscore(r) −score(r′)\n≥2(2s −k′ −K)\nk′\nX\nk1=1\n∥β(k1)∗∥2 −4k′(s −k′)hµ −2k′(K −s)ξ −4k′(s −k′)ξ\n≥2k′\n\u0014\n(2s −k′ −K) min\nk∈S ∥β(k)∗∥2 −2(s −k′)hµ −(K −s)ξ −2(s −k′)ξ\n\u0015\n> 2k′\n\u0014\u00123\n2s −K\n\u0013\nmin\nk∈S ∥β(k)∗∥2 −2shµ −(K −s)ξ −2sξ\n\u0015\n(S.5.60)\n≥0,\n(S.5.61)\nwhere (S.5.60) holds because 1 ≤k′ ≤⌊s/2⌋and (S.5.61) is due to the condition (ii).\nS.5.7\nProof of Theorem 6\nDenote ξ = maxk∈S minrk=±1 ∥rk bβ(k)[0] −β(k)∗∥2 = maxk∈S(∥bβ(k)[0] −β(k)∗∥2 ∧∥bβ(k)[0] +\nβ(k)∗∥2). WLOG, assume S = {1, . . . , s} and r∗\nk = 1 for all k ∈S. Hence ξ = maxk∈S ∥bβ(k)[0]−\nβ(k)∗∥2. For any k′ = 1, . . . , spa, define\nr = (r1, . . . , rk′\n|\n{z\n}\n=−1\n, rk′+1, . . . , rs\n|\n{z\n}\n=1\n, rs+1, . . . , rK\n|\n{z\n}\noutlier tasks\n),\n86\nr′ = (r1, . . . , rk′−1\n|\n{z\n}\n=−1\n, r′\nk′, rk′+1, . . . , rs\n|\n{z\n}\n=1\n, rs+1, . . . , rK\n|\n{z\n}\noutlier tasks\n).\nBy the definition of pa, we must have #{k ∈S : brk = −1} = spa or #{k ∈S : brk = 1} =\nspa. If #{k ∈S : brk = −1} = spa and we have\nscore(r) −score(r′) > 0,\n(S.5.62)\nthen for each k ∈S in the for loop of Algorithm 3, the algorithm will flip the sign of brk′\nto decrease the mis-alignment proportion pa. Then after the for loop, the mis-alignment\nproportion pa will become zero, which means the correct alignment is achieved. The case\nthat #{k ∈S : brk = 1} = spa can be similarly discussed.\nNow we derive (S.5.62). Similar to the decomposition in (S.5.59), we have\nscore(r) −score(r′) = 2\ns\nX\nk=k′+1\n∥bβ(k′)[0] + bβ(k)[0]∥2\n|\n{z\n}\n(1)\n+2\nk′−1\nX\nk=1\n∥bβ(k′)[0] −bβ(k)[0]∥2\n|\n{z\n}\n(2)\n+ 2\nK\nX\nk=s+1\n∥−bβ(k′)[0] −rk bβ(k)[0]∥2\n|\n{z\n}\n(3)\n−2\ns\nX\nk=k′+1\n∥bβ(k′)[0] −bβ(k)[0]∥2\n|\n{z\n}\n(1)′\n−2\nk′−1\nX\nk=1\n∥bβ(k′)[0] + bβ(k)[0]∥2\n|\n{z\n}\n(2)′\n−2\nK\nX\nk=s+1\n∥bβ(k′)[0] −rk bβ(k)[0]∥2\n|\n{z\n}\n(3)′\n.\nNote that\n(1) −(1)′ =\ns\nX\nk=k′+1\n(∥bβ(k′)[0] + bβ(k)[0]∥2 −∥bβ(k′)[0] −bβ(k)[0]∥2)\n≥\ns\nX\nk=k′+1\n(∥β(k′)∗+ β(k)∗∥2 −∥β(k′)∗−β(k)∗∥2 −4ξ)\n≥\ns\nX\nk=k′+1\n(2∥β(k′)∗∥2 −2∥β(k′)∗−β(k)∗∥2 −4ξ)\n≥(s −k′)(2∥β(k′)∗∥2 −4hβ −4ξ),\n87\n(2) −(2)′ ≥−\nk′−1\nX\nk=1\n2∥bβ(k′)[0]∥2 ≥−2(k′ −1)∥β(k′)∗∥2 −2(k′ −1)ξ,\n(3) −(3)′ ≥−\nK\nX\nk=s+1\n2∥bβ(k′)[0]∥2 ≥−2(K −s)∥β(k′)∗∥2 −2(K −s)ξ.\nPutting all pieces together,\nscore(r) −score(r′)\n≥2\nh\n(2s −2k′ −K + 1)∥β(k′)∗∥2 −2(s −k′)hβ −(s −k′ + K −1)ξ\ni\n> 2\nh\n(2s −2spa −K)∥β(k′)∗∥2 −2shβ −2(s + K)ξ\ni\n(S.5.63)\n≥0.\n(S.5.64)\nwhere (S.5.63) holds because 1 ≤k′ ≤spa and (S.5.64) is due to the condition (iii).\nS.5.8\nProof of Theorem 13\nS.5.8.1\nLemmas\nDefine the contraction basin of one GMM as\nBcon(θ(k)∗) = {θ = {w, β, δ} : wr ∈[cw/2, 1−cw/2], ∥β−β(k)∗∥2 ≤Cb∆, |δ−δ(k)∗| ≤Cb∆},\nfor which we may shorthand as Bcon in the following.\nFor GMM z ∼(1 −w∗)N(µ∗\n1, Σ∗) + w∗N(µ∗\n2, Σ∗) and any θ = (w, β, δ), define\nγθ(z) =\nw exp{β⊤z −δ}\n1 −w + w exp{β⊤z −δr},\nw(θ) = E[γθ(z)],\nµ1(θ) = E[(1 −γθ(z))z]\nE[1 −γθ(z)] ,\nµ2(θ) = E[γθ(z)z]\nE[γθ(z)] .\nLemma 29. Suppose Assumption 3 holds.\n(i) With probability at least 1 −τ,\nsup\nθ(0)∈Bcon\n∥β(0)−β(0)∗∥2≤ξ(0)\n\f\f\f\f\f\n1\nn0\nn0\nX\ni=1\nγθ(0)(z(0)\ni ) −E[γθ(0)(z(0))]\n\f\f\f\f\f ≲ξ(0)\nr p\nn0\n+\ns\nlog(1/τ)\nn0\n.\n88\n(ii) With probability at least 1 −τ,\nsup\nθ(0)∈Bcon\n∥β(0)−β(0)∗∥2≤ξ(0)\n\f\f\f\f\f\n1\nn0\nn0\nX\ni=1\nγθ(k)(z(0)\ni )(z(0)\ni )⊤β(0)∗−E[γθ(0)(z(0))(z(0)\ni )⊤β(0)∗]\n\f\f\f\f\f ≲ξ(0)\nr p\nn0\n+\ns\nlog(1/τ)\nn0\n.\n(iii) With probability at least 1 −τ,\nsup\nθ(0)∈Bcon\n∥β(0)−β(0)∗∥2≤ξ(0)\n\r\r\r\r\r\n1\nn0\nn0\nX\ni=1\nγθ(k)(z(0)\ni )z(0)\ni\n−E[γθ(0)(z(0))z(0)\ni ]\n\r\r\r\r\r\n2\n≲ξ(0)\nr p\nn0\n+\ns\nlog(1/τ)\nn0\n.\nS.5.8.2\nMain proof of Theorem 13\nWLOG, in Assumptions 3.(iii) and 3.(iv), we assume\n• ∥bβ(0)[0] −β(0)∗∥2 ∨|bδ(0)[0] −δ(0)∗| ≤C4∆(0), with a sufficiently small constant C4;\n• | bw(0)[0] −w(0)∗| ≤cw/2.\n(I) Case 1: We first consider the case that h ≥C\nq\np\nn0. Consider an event E defined to be\nthe intersection of the events in Lemma 29, with ξ(k) = a large constant C, which satisfies\nP(E) ≥1 −τ. Throughout the analysis in Case 1, we condition on E, therefore all the\narguments hold with probability at least 1 −τ.\nSimilar to our analysis in the proof of Theorem 1, conditioned on E, we have\n| bw(0)[t] −w(0)∗| ≲κ′′\n0d(bθ(0)[t−1], θ(0)∗) +\nr p\nn0\n,\nmax\nr=1:2 ∥bµ(0)[t]\nr\n−µ(0)∗\nr\n∥2 ≲κ′′\n0d(bθ(0)[t−1], θ(0)∗) +\nr p\nn0\n,\n(S.5.65)\n∥(bΣ(0)[t] −Σ(0)∗)β(0)∗∥2 ≲κ′′\n0d(bθ(0)[t−1], θ(0)∗) +\nr p\nn0\n.\n(S.5.66)\nHence\n∥(bΣ(0)[t])β(0)∗−(bµ(0)[t−1]\n2\n−bµ(0)[t−1]\n1\n)∥2 ≲∥(bΣ(0)[t] −Σ(0)∗)β(0)∗∥2 + max\nr=1:2 ∥bµ(0)[t]\nr\n−µ(0)∗\nr\n∥2\n≲κ′′\n0d(bθ(0)[t−1], θ(0)∗) +\nr p\nn0\n.\n89\nBy Lemma 7, we have\n∥bβ(0)[t] −β(0)∗∥2 ≲∥(bΣ(0)[t])β(0)∗−(bµ(0)[t−1]\n2\n−bµ(0)[t−1]\n1\n)∥2 + λ[t]\n0\n√n0\n≲κ′′\n0d(bθ(0)[t−1], θ(0)∗) +\nr p\nn0\n+ λ[t]\n0\n√n0\n.\nCombining these results, we have\nd(bθ(0)[t], θ(0)∗) ≤Cκ′′\n0d(bθ(0)[t−1], θ(0)∗) + C′\nr p\nn0\n+ C′ λ[t]\n0\n√n0\n.\nBy the construction of λ[t]\n0 , we know that\nλ[t]\n0 = 1 −κt\n0\n1 −κ0\nCλ0\n√p + κt\n0λ[0]\n0 ,\nimplies that\nd(bθ(0)[t], θ(0)∗) ≤(Cκ′′\n0)td(bθ(0)[0], θ(0)∗) + C′′\nr p\nn0\n+ C′\nt\nX\nt′=1\nλ[t′]\n0\n√n0\n(Cκ′′\n0)t−t′\n≤(κ′\n0)td(bθ(0)[0], θ(0)∗) + C′′′\nr p\nn0\n+ C′′′t(κ′\n0)t\n≤Ct(κ′\n0)t + C′′′\nr p\nn0\n,\nwhich is the desired rate. The bound of maxr=1:2 ∥bµ(0)[t]\nr\n−µ(0)∗\nr\n∥2 and ∥bΣ(0)[t] −Σ(0)∗∥2 can\nbe derived similar to (S.5.65) and (S.5.66).\n(II) Case 2: Next, we consider the case that h ≤C\nq\np\nn0. According to Assumption 3, we\nhave\nq\np\nn0 ≲\nq\np+log K\nmaxk∈S nk . It is easy to see that the analysis in part (I) does not depend on the\ncondition h ≥C\nq\np\nn0. Hence we have proved the desired bounds of maxr=1:2 ∥bµ(0)[t]\nr\n−µ(0)∗\nr\n∥2\nand ∥bΣ(0)[t] −Σ(0)∗∥2. Denote t0 as an integer such that t0κt0\n0 ≍\nq\np\nn0. When 1 ≤t ≤t0,\nthe bound in part (I) is the desired bound since the term tκt\n0 dominates the other terms.\nLet us consider the case t = t0 + 1.\nConsider an event E′ defined to be the event of\n∥β\n[T] −β(k′)∗∥2 ≲h +\nr\nlog K\nnk′\n+\nr p\nnS\n+ ϵ\nr\np + log K\nmaxk=1:K nk\n,\nk′ ∈arg min\nk∈S\nnk.\nNote that since h ≤C\nq\np+log K\nmaxk∈S nk , by part (II) of the proof of Theorem 1, we know that\nP(E′) ≥1 −C(K−1 + exp{−C′p}). And E′ implies that\n∥β\n[T] −β(0)∗∥2 ≲h +\ns\nlog K\nmaxk∈S nk\n+\nr p\nnS\n+ ϵ\nr\np + log K\nmaxk=1:K nk\n≲\nr p\nn0\n,\n90\nwhere the second inequality comes from Assumption 3.\nAlso consider another event E′′ defined to be the intersection of the events in Lemma\n29, with ξ = C\nh\nh +\nq\nlog K\nmaxk∈S nk +\nq\np\nnS + ϵ\nq\np+log K\nmaxk=1:K nk\ni\n, which satisfies P(E′′) ≥1 −τ.\nThroughout the analysis in Case 1, we condition on E ∩E′ ∩E′′, therefore all the arguments\nhold with probability at least 1 −τ −C(K−1 + exp{−C′p}).\nNote that λ[t]\n0 ≥C√p ≥C′∥β\n[T] −β(0)∗∥2 and λ[t]\n0 ≥C√p ≥C′√n0∥(bΣ(0)[t])β(0)∗−\n(bµ(0)[t−1]\n2\n−bµ(0)[t−1]\n1\n)∥2. Hence by Lemma 7, we have bβ(0)[t] = β\n[T] thus\n∥bβ(0)[t] −β(0)∗∥2 ≲h +\ns\nlog K\nmaxk∈S nk\n+\nr p\nnS\n+ ϵ\nr\np + log K\nmaxk=1:K nk\n.\nSimilar to the analysis in part (II) in the proof of Theorem 1, we have\n| bw(0)[t] −w(0)∗| ≲κ′′\n0d(bθ(0)[t−1], θ(0)∗) + ξ\nr p\nn0\n+\nr\n1\nn0\n≲κ′′\n0d(bθ(0)[t−1], θ(0)∗) + ξ +\nr\n1\nn0\n,\n|bδ(0)[t] −δ(0)∗| ≲κ′′\n0d(bθ(0)[t−1], θ(0)∗) + ξ + ξ\nr p\nn0\n+\nr\n1\nn0\n,\n≲κ′′\n0d(bθ(0)[t−1], θ(0)∗) + ξ +\nr\n1\nn0\n.\nPutting all pieces together,\nd(bθ(0)[t], θ(0)∗) ≤Cκ′′\n0d(bθ(0)[t−1], θ(0)∗)+h+\ns\nlog K\nmaxk∈S nk\n+\nr p\nnS\n+ϵ\nr\np + log K\nmaxk=1:K nk\n+\nr\n1\nn0\n.\nWe can continue this analysis from t = t0 + 1 to t0 + 2, and so on. Hence for any t′ ≥1,\nwe have\nd(bθ(0)[t0+t′], θ(0)∗) ≤(Cκ′′\n0)t′d(bθ(0)[t0], θ(0)∗) + C′h + C′\ns\nlog K\nmaxk∈S nk\n+ C′\nr p\nnS\n+ C′ϵ\nr\np + log K\nmaxk=1:K nk\n+ C′\nr\n1\nn0\n≤(κ′\n0)t′d(bθ(0)[t0], θ(0)∗) + C′h + C′\ns\nlog K\nmaxk∈S nk\n+ C′\nr p\nnS\n+ C′ϵ\nr\np + log K\nmaxk=1:K nk\n+ C′\nr\n1\nn0\n≤(t′ + t0)(κ′\n0)t′+t0 + C′h + C′\ns\nlog K\nmaxk∈S nk\n+ C′\nr p\nnS\n91\n+ C′ϵ\nr\np + log K\nmaxk=1:K nk\n+ C′\nr\n1\nn0\n,\nwhere the last inequality holds because t0 is chosen to be the integer satisfying t0(κ0)t0 ≍\nq\np\nn0 ≳d(bθ(0)[t0], θ(0)∗).\nS.5.8.3\nProof of lemmas\nProof of Lemma 29. The proof is almost the same as the proofs of lemmas in Theorem 1,\nso we do not repeat it here.\nS.5.9\nProof of Theorem 15\nS.5.9.1\nLemmas\nRecall\nΘ\n′\nS(h) =\nn\n{θ\n(k)}k∈{0}∪S = {(w(k), µ(k)\n1 , µ(k)\n2 , Σ(k))}k∈{0}∪S : θ(k) ∈Θ, max\nk∈S ∥β(k) −β(0)∥2 ≤h\no\n.\nLemma 30. When n0 ≥Cp with some constant C > 0, we have\ninf\nbθ(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS\nQS\nP\n \nd(bθ(0), θ(0)∗) ≥C1\nr\np\nnS + n0\n+ C1h ∧\nr p\nn0\n+ C1\nr\n1\nn0\n!\n≥1\n10.\nLemma 31. Denote eϵ = K−s\ns . Then\ninf\nbθ(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS\nQS\nP\n \nd(bθ(0), θ(0)∗) ≥\n\u0012\nC1\neϵ\n√maxk=1:K nk\n\u0013\n∧\n\u0012\nC2\nr\n1\nn0\n\u0013!\n≥1\n10.\nLemma 32 (The second variant of Theorem 5.1 in Chen et al., 2018). Given a series of\ndistributions {{P(k)\nθ }K\nk=0 : θ ∈Θ}, each of which is indexed by the same parameter θ ∈Θ.\nConsider x(k) ∼(1 −eϵ)P(k)\nθ\n+ eϵQ(k) independently for k = 1 : K and x(0) ∼P(0)\nθ . Denote\nthe joint distribution of {x(k)}K\nk=0 as P(eϵ,θ,{Q(k)}K\nk=1). Then\ninf\nbθ\nsup\nθ∈Θ\n{Q(k)}K\nk=1\nP(eϵ,θ,{Q(k)}K\nk=1)\n\u0010\n∥bθ −θ∥≥Cϖ′(eϵ, Θ)\n\u0011\n≥9\n20,\nwhere ϖ′(eϵ, Θ) := sup\n\b\n∥θ1 −θ2∥: maxk=1:K dTV\n\u0000P(k)\nθ1 , P(k)\nθ2\n\u0001\n≤eϵ/(1 −eϵ), dTV\n\u0000P(0)\nθ1 , P(0)\nθ2\n\u0001\n≤\n1/20\n\t\n.\n92\nLemma 33. Consider two data generating mechanisms:\n(i) x(k) ∼(1 −eϵ′)P(k)\nθ\n+ eϵ′Q(k) independently for k = 1 : K and x(0) ∼P(0)\nθ , where\neϵ′ = K−s\nK ;\n(ii) With a preserved set S ⊆1 : K, generate {x(k)}k∈Sc ∼QS and x(k) ∼P(k)\nθ\nindepen-\ndently for k ∈S. And x(0) ∼P(0)\nθ .\nDenote the joint distributions of {x(k)}K\nk=0 in (i) and (ii) as P(eϵ,θ,{Q(k)}K\nk=1) and P(S,θ,Q),\nrespectively. We claim that if\ninf\nbθ\nsup\nθ∈Θ\n{Q(k)}K\nk=1\nP( K−s\n50K ,θ,{Q(k)}K\nk=1)\n\u0012\n∥bθ −θ∥≥Cϖ′\n\u0012K −s\n50K , Θ\n\u0013\u0013\n≥9\n20,\nthen\ninf\nbθ\nsup\nS:|S|≥s\nsup\nθ∈Θ\nQS\nP(S,θ,QS)\n\u0012\n∥bθ −θ∥≥Cϖ′\n\u0012K −s\n50K , Θ\n\u0013\u0013\n≥1\n10,\nwhere ϖ′(eϵ, Θ) := sup\n\b\n∥θ1 −θ2∥: maxk=1:K KL\n\u0000P(k)\nθ1 ∥P(k)\nθ2\n\u0001\n≤[eϵ/(1 −eϵ)]2, KL\n\u0000P(0)\nθ1 ∥P(0)\nθ2\n\u0001\n≤\n1/400\n\t\nfor any eϵ ∈(0, 1).\nLemma 34. When n0 ≥Cp with some constant C > 0, we have\ninf\nbΣ(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS\nQS\nP\n \n∥bΣ(0) −Σ(0)∗∥2 ≥C\nr p\nn0\n!\n≥1\n10.\nS.5.9.2\nMain proof of Theorem 15\nS.5.9.3\nProof of lemmas\nProof of Lemma 30. It’s easy to see that Θ\n′\nS ⊇Θ\n′\nS,w ∪Θ\n′\nS,β ∪Θ\n′\nS,δ, where\nΘ\n′\nS,w =\nn\n{θ\n(k)}k∈{0}∪S : µ(k)\n1\n= 1p/√p, µ(k)\n2\n= −µ(k)\n1 , Σ(k) = Ip, w(k) ∈(cw, 1 −cw)\no\n,\nΘ\n′\nS,β =\nn\n{θ\n(k)}k∈{0}∪S : Σ(k) = Ip, w(k) = 1\n2, ∥µ(k)\n1 ∥2 ∨∥µ(k)\n2 ∥2 ≤M, max\nk∈S ∥β(k) −β(0)∥2 ≤h\no\n,\nΘ\n′\nS,δ =\nn\n{θ\n(k)}k∈{0}∪S : Σ(k) = Ip, w(k) = 1\n2, ∥µ(k)\n1 ∥2 ∨∥µ(k)\n2 ∥2 ≤M, µ(k)\n1\n= −1\n2µ0,\nµ(k)\n2\n= 1\n2µ0 + u, ∥u∥2 ≤1\no\n.\n93\n(i) By fixing an S and a QS, we want to show\ninf\nbθ(0)\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS,β\nP\n \n∥bβ(0) −β(0)∗∥2 ∨∥bβ(0) + β(0)∗∥2 ≥C\nr\np\nnS + n0\n!\n≥1\n4.\nBy Lemma 4, ∃a quadrant Qv of Rp and a r/8-packing of (rSp) ∩Qv under Euclidean\nnorm: {eµj}N\nj=1, where r = (c\np\np/(nS + n0)) ∧M ≤1 with a small constant c > 0 and\nN ≥(1\n2)p8p−1 =\n1\n2 × 4p−1 ≥2p−1 when p ≥2.\nFor any µ ∈Rp, denote distribution\n1\n2N(µ, Ip) + 1\n2N(−µ, Ip) as Pµ. Then\nLHS ≥inf\nbµ\nsup\nµ∈(rSp)∩Qv\nP\n \n∥bµ −µ∥2 ∧∥bµ + µ∥2 ≥C\nr p\nnS\n!\n≥inf\nbµ\nsup\nµ∈(rSp)∩Qv\nP\n \n∥bµ −µ∥2 ≥C\nr p\nnS\n!\n,\n(S.5.67)\nwhere the last inequality holds because it suffices to consider estimator bµ satisfying bµ(X) ∈\n(rSp) ∩Qv almost surely. In addition, for any x, y ∈Qv, ∥x −y∥2 ≤∥x + y∥2.\nBy Lemma 15,\nKL\n\nY\nk∈{0}∪S\nP⊗nk\neµj\n· QS\n\r\r\r\r\nY\nk∈{0}∪S\nP⊗nk\neµj′ · QS\n\n=\nX\nk∈{0}∪S\nnkKL(Peµj∥Peµj′)\n≤\nX\nk∈{0}∪S\nnk · 8∥eµj∥2\n2∥eµj −eµj′∥2\n2\n≤32(nS + n0)r2\n≤32nSc2 · 2(p −1)\nnS + n0\n≤64c2\nlog 2 log N.\nBy Lemma 3,\nLHS of (S.5.67) ≥1 −log 2\nlog N −64c2\nlog 2 ≥1 −\n1\np −1 −1\n4 ≥1\n4,\nwhen C = c/2, p ≥3 and c = √log 2/16.\n(ii) By fixing an S and a QS, we want to show\ninf\nbθ(0)\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS\nP\n \n∥bβ(0) −β(0)∗∥2 ∨∥bβ(0) + β(0)∗∥2 ≥C\n\u0014\nh ∧\n\u0012\nc\nr p\nn0\n\u0013\u0015\u001b!\n≥1\n4.\n(S.5.68)\n94\nBy Lemma 4, ∃a quadrant Qv of Rp and a r/8-packing of (rSp−1) ∩Qv under Euclidean\nnorm: { eϑj}N\nj=1, where r = h ∧(c\np\np/n0) ∧M ≤1 with a small constant c > 0 and\nN ≥(1\n2)p−18p−2 =\n1\n2 × 4p−2 ≥2p−2 when p ≥3.\nWLOG, assume M ≥2.\nDenote\neµj = (1, eϑ⊤\nj )⊤∈Rp. Let µ(k)∗\n1\n= eµ = (1, 0p−1)⊤for all k ∈S and µ(0)∗\n1\n= µ = (1, ϑ) with\nϑ ∈(rSp−1) ∩Qv. For any µ ∈Rp, denote distribution 1\n2N(µ, Ip) + 1\n2N(−µ, Ip) as Pµ.\nThen similar to the arguments in (i),\nLHS ≥inf\nbµ\nsup\nϑ∈(rSp−1)∩Qv\nµ=(1,ϑ)⊤\nP\n \n∥bµ −µ∥2 ∧∥bµ + µ∥2 ≥C\n\u0014\nh ∧\n\u0012\nc\nr p\nn0\n\u0013\u0015!\n≥inf\nbµ\nsup\nϑ∈(rSp−1)∩Qv\nµ=(1,ϑ)⊤\nP\n \n∥bµ −µ∥2 ≥C\n\u0014\nh ∧\n\u0012\nc\nr p\nn0\n\u0013\u0015!\n.\nThen by Lemma 15,\nKL\n \nP⊗n0\neµj\n·\nY\nk∈S\nP⊗nk\neµ\n· QS\n\r\r\r\rP⊗n0\neµj′ ·\nY\nk∈S\nP⊗nk\neµ\n· QS\n!\n= n0KL(Peµj∥Peµj′)\n≤n0 · 8∥eµj∥2\n2∥eµj −eµj′∥2\n2\n≤32n0r2\n≤32n0c2 · 3(p −2)\nn0\n≤96c2\nlog 2 log N,\nwhen n0 ≥(c2 ∨M −2)p and p ≥3. By Fano’s lemma (See Corollary 2.6 in Tsybakov,\n2009),\nLHS of (S.5.68) ≥1 −log 2\nlog N −96c2\nlog 2 ≥1 −\n1\np −2 −1\n4 ≥1\n4,\nwhen C = 1/2, p ≥4 and c =\np\n(log 2)/384.\n(iii) We want to show\ninf\nbθ(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS,w\nQS\nP\n\u0012\n| bw(0) −w(0)∗| ≥C\nr\n1\nn0\n!\n≥1\n4.\nThe argument is similar to (ii). The only two differences here are that the dimension of\ninterested parameter w equals 1, and Lemma 15 is replaced by Lemma 17.\n95\n(iv) We want to show\ninf\nbθ(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS,δ\nQS\nP\n\u0012\n|bδ(0) −δ(0)∗| ≥C\nr\n1\nn0\n!\n≥1\n4\nThe argument is similar to (ii).\nFinally, we get the desired conclusion by combining (i)-(iv).\nProof of Lemma 31. Let eϵ =\nK−s\ns\nand eϵ′ =\nK−s\nK . Since s/K ≥c > 0, eϵ ≲eϵ′. Denote\nΥS = {{µ(k)}k∈{0}∪S : µ(k) ∈Rp\n+, maxk∈S ∥µ(k) −µ(0)∥2 ≤h/2, ∥µ(k)∥2 ≤M}. For any\nµ ∈R, denote distribution\n1\n2N(µ, Ip) + 1\n2N(−µ, Ip) as Pµ, and denote Q\nk∈S P⊗nk\nµ(k) as\nP{µ(k)}k∈S. It suffices to show\ninf\nbθ(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈ΘS\nQS\nP\n \n∥bµ(0) −µ(0)∗∥2 ≥\n\u0012\nC1eϵ′\nr\n1\nmaxk=1:K nk\n\u0013\n∧\n\u0012\nC2\nr\n1\nn0\n\u0013!\n≥1\n10.\n(S.5.69)\nwhere P = P⊗n0\nµ(0) · P{µ(k)}k∈S · QS.\nFor any µ ∈R, denote distribution 1\n2N(µ, Ip) + 1\n2N(−µ, Ip) as Pµ. WLOG, assume\nM ≥1. For any eµ1, eµ2 ∈Rp with ∥eµ1∥2 = ∥eµ2∥2 = 1, by Lemma 15,\nmax\nk=1:K KL\n\u0000P⊗nk\neµ1 ∥P⊗nk\neµ2\n\u0001\n≤max\nk=1:K nk · 8∥eµ1 −eµ2∥2\n2.\nfor any k = 1 : K.\nLet 8 maxk=1:K nk · ∥eµ1 −eµ2∥2\n2 ≤(\neϵ′\n1−eϵ′)2, then ∥eµ1 −eµ2∥2 ≤\nC\nq\n1\nmaxk=1:K nkeϵ′ for some constant C > 0.\nOn the other hand, let KL\n\u0000P⊗n0\neµ1 ∥P⊗n0\neµ2\n\u0001\n=\n8n0 · ∥eµ1 −eµ2∥2\n2 ≤1/100, then ∥eµ1 −eµ2∥2 ≤\nq\n1\n800\nq\n1\nn0 for some constant C > 0. Then\n(S.5.69) follows by Lemma 33.\nProof of Lemma 32. The proof is similar to the proof of Theorem 5.1 in Chen et al. (2018),\nso we omit it here.\nProof of Lemma 34. This can be similarly shown by Assouad’s Lemma as in the proof of\nLemma 23. We omit the proof here.\nS.5.10\nProof of Theorem 14\nThe result follows from (S.5.48) and Theorem 13.\n96\nS.5.11\nProof of Theorem 16\nS.5.11.1\nLemmas\nLemma 35. Assume n0 ≥Cp and ∆(0) ≥C′ > 0 with some constants C, C′ > 0. We\nhave\ninf\nbC(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS\nQS\nP\n \nRθ\n(0)∗( bC(0)) −Rθ\n(0)∗(Cθ\n(0)∗) ≥C1\np\nnS + n0\n+ C2h2 ∧p\nn0\n+ 1\nn0\n!\n≥1\n10.\nLemma 36. Denote eϵ = K−s\ns . We have\ninf\nbC(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ(T )\nS\nQS\nP\n \nRθ\n(0)∗( bC(0))−Rθ\n(0)∗(Cθ\n(0)∗) ≥\n\u0012\nC1\neϵ′2\nmaxk=1:K nk\n\u0013\n∧\n\u0012\nC2\nr\n1\nn0\n\u0013!\n≥1\n10.\nS.5.11.2\nMain proof of Theorem 16\nCombine Lemmas 35 and 36 to finish the proof.\nS.5.11.3\nProof of lemmas\nProof of Lemma 35. We proceed with similar proof ideas used in the proof of Lemma 35.\nRecall the definitions and proof idea of Lemma 30. We have Θ\n′\nS ⊇Θ\n′\nS,w ∪Θ\n′\nS,β ∪Θ\n′\nS,δ,\nwhere\nΘ\n′\nS,w =\nn\n{θ\n(k)}k∈{0}∪S : µ(k)\n1\n= 1p/√p, µ(k)\n2\n= −µ(k)\n1 , Σ(k) = Ip, w(k) ∈(cw, 1 −cw)\no\n,\nΘ\n′\nS,β =\nn\n{θ\n(k)}k∈{0}∪S : Σ(k) = Ip, w(k) = 1\n2, ∥µ(k)\n1 ∥2 ∨∥µ(k)\n2 ∥2 ≤M, max\nk∈S ∥β(k) −β(0)∥2 ≤h\no\n,\nΘ\n′\nS,δ =\nn\n{θ\n(k)}k∈{0}∪S : Σ(k) = Ip, w(k) = 1\n2, ∥µ(k)\n1 ∥2 ∨∥µ(k)\n2 ∥2 ≤M, µ(k)\n1\n= −1\n2µ0,\nµ(k)\n2\n= 1\n2µ0 + u, ∥u∥2 ≤1\no\n.\nRecall the mis-clustering error for GMM associated with parameter set θ of any classifier\nC is Rθ(C) = minπ:{1,2}→{1,2} Pθ(C(Z) ̸= π(Y )). To help the analysis, following Azizyan et al.\n(2013) and Cai et al. (2019), we define a surrogate loss Lθ(C) = minπ:{1,2}→{1,2} Pθ(C(Z) ̸=\nπ(Cθ(Z))), where Cθ is the Bayes classifier. Suppose σ =\n√\n0.005.\n97\n(i) We want to show\ninf\nbC(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS\nQS\nP\n \nRθ\n(0)∗( bC(0)) −Rθ\n(0)∗(Cθ\n(0)∗) ≥C\nr\np\nnS + n0\n!\n≥1\n4.\n(S.5.70)\nConsider S = 1 : K and space Θ\n′\n0 = {{θ\n(k)}K\nk=0 : Σ(k) = Ip, w(k) = 1/2, µ(k)\n1\n= µ1, µ(k)\n2\n=\nµ2, ∥µ1∥2 ∨∥µ2∥2 ≤M}. And\nLHS of (S.5.50) ≥inf\nbC(0)\nsup\n{θ\n(k)∗}K\nk=0∈Θ′\n0\nP\n \nRθ\n(0)∗( bC(0)) −Rθ\n(0)∗(Cθ\n(0)∗) ≥C\nr\np\nnS + n0\n!\n.\nLet r = c\np\np/(nS + n0) ≤0.001 with some small constant c > 0. For any µ ∈Rp, denote\ndistribution 1\n2N(µ, Ip)+ 1\n2N(−µ, Ip) as Pµ. Consider a r/4-packing of rSp−1: {evj}N\nj=1. By\nLemma 2, N ≥4p−1. Denote eµj = (σ, ev⊤\nj )⊤∈Rp, where σ =\n√\n0.005. Then by definition\nof KL divergence and Lemma 8.4 in Cai et al. (2019),\nKL\n\nY\nk∈{0}∪S\nP⊗nk\neµj\n· QS\n\r\r\r\r\nY\nk∈{0}∪S\nP⊗nk\neµj′ · QS\n\n=\nX\nk∈{0}∪S\nnkKL(Peµj∥Peµj′)\n≤(nS + n0) · 8(1 + σ2)∥eµj −eµj′∥2\n2\n≤32(1 + σ2)(nS + n0)r2\n≤32(1 + σ2)(nS + n0) · c22(p −1)\nnS + n0\n≤32(1 + σ2)c2\nlog 2\nlog N.\nFor simplicity, we write Lθ with θ ∈Θ0 and µ1 = −µ2 = µ as Lµ. By Lemma 8.5 in Cai\net al. (2019),\nLeµi(Ceµj) ≥1\n√\n2g\n √\nσ2 + r2\n2\n!\n∥eµi −eµj∥2\n∥eµi∥2\n≥1\n√\n2 · 0.15 ·\nr/4\n√\nσ2 + r2 ≥2r,\nwhere g(x) = ϕ(x)[ϕ(x) −xΦ(x)]. The last inequality holds because\n√\nσ2 + r2 ≥\n√\n2σ and\ng(\n√\nσ2 + r2/2) ≥0.15 when r2 ≤σ2 = 0.001. Then by Lemma 3.5 in Cai et al., 2019\n(Proposition 2 in Azizyan et al. (2013)), for any classifier C, and i ̸= j,\nLeµi(C) + Leµj(C) ≥Leµi(Ceµj) −\nq\nKL(Peµi∥Peµj)/2 ≥2r −r = c\nr\np\nnS + n0\n.\n(S.5.71)\nFor any bC(0), consider a test ψ∗= arg minj=1:N Leµj( bC(0)). Therefore if there exists j0 such\nthat Leµj0( bC(0)) < c\n2\nq\np\nnS+n0, then by (S.5.71), we must have ψ∗= j0. Let C1 ≤c/2, then\n98\nby Fano’s lemma (Corollary 6 in Tsybakov, 2009)\ninf\nbC(0)\nsup\n{θ\n(k)∗}K\nk=0∈Θ′\n0\nP\n \nLθ\n(0)∗( bC(0)) ≥C1\nr\np\nnS + n0\n!\n≥inf\nbC(0) sup\nj=1:N\nP\n \nLeµ(j)( bC(0)) ≥C1\nr\np\nnS + n0\n!\n≥inf\nbC(0) sup\nj=1:N\nP\n \nψ∗̸= j\n!\n≥inf\nψ\nsup\nj=1:N\nP\n \nψ ̸= j\n!\n≥1 −log 2\nlog N −32(1 + σ2)c2\nlog 2\n≥1\n4,\nwhen p ≥2 and c =\nq\nlog 2\n128(1+σ2). Then apply Lemma 25 to get the (S.5.70).\n(ii) We want to show\ninf\nbC(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS,β\nQS\nP\n \nRθ\n(0)∗( bC(0))−Rθ\n(0)∗(Cθ\n(0)∗) ≥C\n\u0012\nh ∧\nr p\nn0\n\u0013 !\n≥1\n4. (S.5.72)\nFixing an S and a QS. Suppose 1 ∈S. We have\nLHS of (S.5.72) ≥inf\nbC(0)\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS,β\nQS\nP\n\u0012\nRθ\n(0)∗( bC(0)) −Rθ\n(0)∗(Cθ\n(0)∗) ≥C\n\u0012\nh ∧\nr p\nn0\n\u0013 !\n.\n(S.5.73)\nLet r = h∧(c\np\np/n0)∧M with a small constant c > 0. For any µ ∈Rp, denote distribution\n1\n2N(µ, Ip) + 1\n2N(−µ, Ip) as Pµ. Consider a r/4-packing of rSp−1. By Lemma 2, N ≥4p−1.\nDenote eµj = (σ, ev⊤\nj )⊤∈Rp. WLOG, assume M ≥2. Let µ(k)∗\n1\n= eµ = (σ, 0p−1)⊤for\nall k ∈S and µ(0)∗\n1\n= µ = (1, ϑ) with ϑ ∈(rSp−1) ∩Qv. Then by following the same\narguments in part (ii) of the proof of Lemma 30, we can show that the RHS of (S.5.73) is\nlarger than or equal to 1/4 when p ≥3.\n(iii) We want to show\ninf\nbC(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS\nQS\nP\n \nRθ\n(0)∗( bC(0)) −Rθ\n(0)∗(Cθ\n(0)∗) ≥Ch2\nw ∧1\nn0\n!\n≥1\n4.\nThis can be similarly proved by following the arguments in part (i) with Lemmas 25 and\n26.\n99\n(iv) We want to show\ninf\nbC(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS\nQS\nP\n \nRθ\n(0)∗( bC(0)) −Rθ\n(k)∗(Cθ\n(0)∗) ≥Ch2\nβ ∧p\nn0\n!\n≥1\n4.\nThe conclusion can be obtained immediately from (ii), by noticing that Θ\n′\nS,β ⊇Θ\n′\nS,µ.\nFinally, we get the desired conclusion by combining (i)-(iv).\nProof of Lemma 36. By Lemma 25, it suffices to prove\ninf\nbC(0) sup\nS:|S|≥s\nsup\n{θ\n(k)∗}k∈{0}∪S∈Θ′\nS\nQS\nP\n \nLθ\n(0)∗( bC(0)) ≥C1\neϵ′2\nmaxk=1:K nk\n∧1\nn0\n!\n≥1\n10.\nFor any µ ∈Rp, denote distribution 1\n2N(µ, Ip) + 1\n2N(−µ, Ip) as Pµ. For simplicity, we\nwrite Lθ with θ satisfying µ1 = −µ2 = µ, w = 1/2 and Σ = Ip as Lµ. Consider Lµ(Cµ′)\nas a loss function between µ and µ′ in Lemmas 32 and 33. Considering ∥µ∥2 = ∥µ′∥2 = 1,\nby Lemma 15, note that\nmax\nk=1:K KL(P⊗nk\nµ\n∥P⊗nk\nµ′ ) ≤8 max\nk=1:K nk · ∥µ −µ′∥2\n2,\nKL(P⊗n0\nµ\n∥P⊗n0\nµ′ ) ≤8n0 · ∥µ −µ′∥2\n2.\nBy Lemma 8.5 in Cai et al. (2019), this implies for some constants c, C > 0\nsup\nn\nLµ(Cµ′) : max\nk=1:K KL(P⊗nk\nµ\n∥P⊗nk\nµ′ ) ≤(eϵ′/(1 −eϵ))2, KL(P⊗n0\nµ\n∥P⊗n0\nµ′ ) ≤1/100\no\n≥sup\nn\nc∥µ −µ′∥2 : max\nk=1:K KL(P⊗nk\nµ\n∥P⊗nk\nµ′ ) ≤(eϵ′/(1 −eϵ))2, KL(P⊗n0\nµ\n∥P⊗n0\nµ′ ) ≤1/100\no\n≥sup\nn\nc∥µ −µ′∥2 : 8 max\nk=1:K nk · ∥µ −µ′∥2\n2 ≤(eϵ′/(1 −eϵ))2, 8n0 · ∥µ −µ′∥2\n2 ≤1/800\no\n= C ·\neϵ′\n√maxk=1:K nk\n∧\nr\n1\nn0\n.\nThen apply Lemmas 32 and 33 to get the desired bound.\nS.5.12\nProof of Theorem 17\nDenote ξ = maxk∈{0}∪S minrk=±1 ∥rk bβ(k)[0] −β(k)∗∥2 = maxk∈{0}∪S(∥bβ(k)[0] −β(k)∗∥2 ∧\n∥bβ(k)[0] + β(k)∗∥2). WLOG, assume S = {1, . . . , s} and r∗\nk = 1 for all k ∈{0} ∪S. Hence\n100\nξ = maxk∈{0}∪S ∥bβ(k)[0] −β(k)∗∥2. WLOG, consider brk = 1 for all k ∈S (i.e., the tasks in S\nare already well-aligned). Consider\n(1, br) = ( 1\n|{z}\ntarget\n, 1, . . . , 1, 1\n|\n{z\n}\nS\n, rs+1, . . . , rK\n|\n{z\n}\noutlier tasks\n),\n(−1, br) = ( −1\n|{z}\ntarget\n, 1, . . . , 1, 1\n|\n{z\n}\nS\n, rs+1, . . . , rK\n|\n{z\n}\noutlier tasks\n).\nIt suffices to prove that\nscore((−1, br)) −score((1, br)) > 0.\nIn fact,\nscore((−1, br)) −score((1, br)) = 2\ns\nX\nk=1\n∥bβ(0)[0] + bβ(k)[0]∥2\n|\n{z\n}\n[1]\n+2\nK\nX\nk=s+1\n∥bβ(0)[0] + rk bβ(k)[0]∥2\n|\n{z\n}\n[2]\n−2\ns\nX\nk=1\n∥bβ(0)[0] −bβ(k)[0]∥2\n|\n{z\n}\n[1]′\n−2\nK\nX\nk=s+1\n∥−bβ(0)[0] + rk bβ(k)[0]∥2\n|\n{z\n}\n[2]′\n,\nwhere\n[1] −[1]′ ≥\ns\nX\nk=1\n(∥β(0)∗+ β(k)∗∥2 −∥β(0)∗−β(k)∗∥2 −4ξ)\n≥\ns\nX\nk=1\n(2∥β(0)∗∥2 −2∥β(0)∗−β(k)∗∥2 −4ξ)\n≥s(2∥β(0)∗∥2 −4h −4ξ),\nand\n[2] −[2]′ ≥−4\nK\nX\nk=s+1\n∥bβ(0)[0]∥2 ≥−4(K −s)(∥β(0)∗∥2 + ξ).\nHence\nscore((−1, br)) −score((1, br)) = 2([1] −[1]′) + 2([2] −[2]′)\n≥4[(2s −K)∥β(0)∗∥2 −2sh −(K + s)ξ]\n> 0,\nwhen ∥β(0)∗∥2 > 2(1−ϵ)\n1−2ϵ h + 2−ϵ\n1−2ϵξ, which completes our proof.\n101\nS.5.13\nProof of Theorem 7\nDefine the contraction basin of one GMM as\nBcon(θ(k)∗) = {θ = {{wr}R\nr=2, {βr}R\nr=2, {δr}R\nr=2} : w∗\nr ∈(cw/2, 1 −cw/2),\n∥βr −β∗\nr∥2 ≤Cb∆, |δr −δ∗\nr| ≤Cb∆}.\nAnd the joint contraction basin is defined as Bcon({θ(k)∗}k∈S) = TR\nr=1 Bcon(θ(k)∗).\nFor θ = ({wr}R\nr=2, {βr}R\nr=2, {δr}R\nr=2) and θ′ = ({w′\nr}R\nr=2, {β′\nr}R\nr=2, {δ′\nr}R\nr=2), define\nd(θ, θ′) = max\nr=2:R{|wr −w′\nr| ∨∥βr −β′\nr∥2 ∨|δr −δ′\nr|}.\nS.5.13.1\nLemmas\nFor GMM z ∼PR\nr=1 w∗\nrN(µ∗\nr, Σ∗) and any θ, define\nγ(r)\nθ (z) =\nwr exp{β⊤\nr z −δr}\nw1 + PR\nr=2 wr exp{β⊤\nr z −δr}\n, r = 2 : R,\nγ(1)\nθ (z) =\nw1\nw1 + PR\nr=2 wr exp{β⊤\nr z −δr}\n.\nDenote wr(θ) = E[γ(r)\nθ (z)] and µr(θ) =\nE[γ(r)\nθ (z)z]\nE[γ(r)\nθ (z)] .\nLemma 37 (Contraction of multi-cluster GMM). When Cb ≤cc−1/2\nΣ\nwith a small constant\nc > 0 and ∆≥C log(cΣMc−1\nw ) with a large constant C > 0, there exist positive constants\nC′ > 0 and C′′ > 0, for any θ ∈Bcon(θ(k)∗),\n|wr(θ) −w∗\nr| ≤C′ exp{−C′′∆2} · d(θ, θ∗),\n∥µr(θ) −µ∗\nr∥2 ≤C′ exp{−C′′∆2} · d(θ, θ∗),\nwhere C′ exp{−C′′∆2} ≤κ0 < 1 with a constant κ0.\nLemma 38 (Vectorized contraction of Rademacher complexity, Corollary 1 in Maurer,\n2016). Suppose {ϵir}i∈[n],r∈[R] and {ϵi}n\ni=1 are independent Rademacher variables. Let F be\na class of functions f : Rd →S ⊆RR and h : S →R is L-Lipschitz under ℓ2-norm, i.e.,\n|h(y) −h(y′)| ≤L∥y −y′∥2, where y = (y1, . . . , yR)⊤, y′ = (y′\n1, . . . , y′\nR)⊤∈S. Then\nE sup\nf∈F\nn\nX\ni=1\nϵih(f(xi)) ≤\n√\n2LE sup\nf∈F\nn\nX\ni=1\nR\nX\nr=1\nϵirfr(xi),\nwhere fr(xi) is the r-th component of f(xi) ∈S ⊆RR.\n102\nS.5.13.2\nMain proof of Theorem 7\nThe proof idea is almost the same as the idea used in the proof of Theorem 1. We still\nneed to show similar results presented in the lemmas associated with Theorem 1, then go\nthrough the same arguments in the proof of Theorem 7. We only sketch the key steps and\nthe differences here.\nThe biggest difference appears in the proofs of the lemmas associated with Theorem 1\nunder the context of multi-cluster GMM. The original arguments in the proofs of Lemmas\n11-14 involve the contraction inequality for Rademacher variables and univariate Lipschitz\nfunctions, which is not available anymore. We replace this part with an argument through\na vectorized Rademacher contraction inequality (Maurer, 2016).\nFirst, we will show that\nsup\nθ(k)∈Bcon\n∥β(k)\nr\n−β(k)∗\nr\n∥2≤ξ(k)\n\f\f\f\f\f\n1\nnk\nnk\nX\ni=1\nγ(r)\nθ(k)(z(k)\ni ) −E[γ(r)\nθ(k)(z(k))]\n\f\f\f\f\f ≲ξ(k)\nr p\nnk\n+\nr\nlog K\nnk\n,\n(S.5.74)\nfor all k ∈S and r ∈1 : R, with probability at least 1 −CK−2. Denote the LHS as W. By\nchanging one observation z(k)\ni , denote the new W as W ′. Since γ(r)\nθ (z) is bounded for all\nz ∈Rp, we know that |W −W ′| ≤1/nk. Then by bounded difference inequality, we have\nW ≤EW + C\nr\nlog K\nnk\n,\nwith probability at least 1 −C′K−2. On the other hand, by symmetrization,\nEW ≤2\nnk\nEzEϵ\nsup\nθ(k)∈Bcon\n∥β(k)\nr\n−β(k)∗\nr\n∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\ni γ(r)\nθ(k)(z(k)\ni )\n\f\f\f\f\f .\nNote that γ(r)\nθ(k)(z) =\nw(k)\nr\n·exp{(β(k)\nr\n)⊤z−δ(k)\nr\n}\nw(k)\n1\n+PR\nr=2 w(k)\nr\nexp{(β(k)\nr\n)⊤z−δ(k)\nr\n} =\nexp{(β(k)\nr\n)⊤z−δ(k)\nr\n+log w(k)\nr\n−log w(k)\n1\n}\n1+PR\nr=2 exp{(β(k)\nr\n)⊤z−δ(k)\nr\n+log w(k)\nr\n−log w(k)\n1\n} =\nφ({(β(k)\nr )⊤z −δ(k)\nr\n+ log w(k)\nr\n−log w(k)\n1 }R\nr=2), where φ(x) =\nexp{xr}\n1+PR\nr=2 exp{xr} is a 1-Lipschitz\nfunction (w.r.t. ℓ2-norm). By Lemma 38,\n2\nnk\nEzEϵ\nsup\nθ(k)∈Bcon\n∥β(k)\nr\n−β(k)∗\nr\n∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\ni γ(r)\nθ(k)(z(k)\ni )\n\f\f\f\f\f ≲1\nnk\nEzEϵ\nsup\nθ(k)∈Bcon\n∥β(k)\nr\n−β(k)∗\nr\n∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\nR\nX\nr=2\nϵ(k)\nir g(k)\nir\n\f\f\f\f\f\n≲1\nnk\nR\nX\nr=2\nEzEϵ\nsup\nθ(k)∈Bcon\n∥β(k)\nr\n−β(k)∗\nr\n∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\nir g(k)\nir\n\f\f\f\f\f ,\n103\nwhere g(k)\nir := (β(k)\nr )⊤z(k)\ni\n−δ(k)\nr\n+ log w(k)\nr\n−log w(k)\n1 . It follows that\n1\nnk\nEzEϵ\nsup\nθ(k)∈Bcon\n∥β(k)\nr\n−β(k)∗\nr\n∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\nir g(k)\nir\n\f\f\f\f\f\n≤1\nnk\nEz,ϵ\nsup\nθ(k)∈Bcon\n∥β(k)\nr\n−β(k)∗\nr\n∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\nir (β(k)\nr )⊤z(k)\ni\n\f\f\f\f\f\n+ 1\nnk\nEϵ\nsup\nθ(k)∈Bcon\n∥β(k)\nr\n−β(k)∗\nr\n∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\nir (δ(k)\nr\n−log w(k)\nr\n+ log w(k)\n1 )\n\f\f\f\f\f\n≤1\nnk\nEz,ϵ\nsup\n∥β(k)\nr\n−β(k)∗\nr\n∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\nir (β(k)\nr )⊤(z(k)\ni\n−µ(k)∗)\n\f\f\f\f\f\n+ 1\nnk\nEz,ϵ\nsup\n∥β(k)\nr\n−β(k)∗\nr\n∥2≤ξ(k)\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\nir (β(k)\nr )⊤µ(k)∗\n\f\f\f\f\f\n+ 1\nnk\nEϵ\nsup\n|δ(k)\nr\n|≤U\ncw/2≤w(k)\nr\n≤1−cw/2\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\nir (δ(k)\nr\n−log w(k)\nr\n+ log w(k)\n1 )\n\f\f\f\f\f\n≤ξ(k)\nnk\nEz,ϵ sup\nj=1:N\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\nir u⊤\nj (z(k)\ni\n−µ(k)∗)\n\f\f\f\f\f + 1\nnk\nEz,ϵ\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\nir (β(k)∗\nr\n)⊤(z(k)\ni\n−µ(k)∗)\n\f\f\f\f\f\n+ C\nnk\nEϵ\n\f\f\f\f\f\nnk\nX\ni=1\nϵ(k)\nir\n\f\f\f\f\f\n(S.5.75)\nwhere µ(k)∗:= PR\nr=1 w(k)∗\nr\nµ(k)∗\nr\n, {uj}N\nj=1 is a 1/2-cover of Sd−1 with N = 5p, and {ϵ(k)\nir u⊤\nj (z(k)\ni −\nµ(k)∗)}nk\ni=1, {ϵ(k)\nir (β(k)∗\nr\n)⊤(z(k)\ni\n−µ(k)∗)}nk\ni=1, and {ϵ(k)\nir }nk\ni=1 are all sub-Gaussian processes. Then\nby the property of sub-Gaussian variables,\nRHS of (S.5.75) ≲ξ(k)\nr p\nnk\n+\nr\nlog K\nnk\n.\nPutting all the pieces together, we obtain W ≲ξ(k)q\np\nnk +\nq\nlog K\nnk\nwith probability at least\n1 −CK−2.\nThe second bound we want to show is\nsup\n{θ(k)}k∈S∈BJ,2\ncon\nsup\n| ewk|≤1\n1\nnS\n\f\f\f\f\f\nX\nk∈S\newk\nnk\nX\ni=1\nh\nγ(r)\nθ(k)(z(k)\ni ) −E[γ(r)\nθ(k)(z(k))]\ni\f\f\f\f\f ≲\nr\np + K\nnS\n.\nDenote the LHS as W ′. Again by bounded difference inequality,\nW ′ ≤EW ′ + C\nr p\nnS\n,\n104\nwith probability at least 1−C′ exp{−C′′p}. It remains to control EW ′. By symmetrization,\nEW ′ ≤2\nnS\nE\nsup\n{θ(k)}k∈S∈BJ,2\ncon\nsup\n| ewk|≤1\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\newkγ(r)\nθ(k)(z(k)\ni )\n\f\f\f\f\f .\nDenote\nφ( ew, {(β(k)\nr )⊤z −δ(k)\nr\n+ log w(k)\nr\n−log w(k)\n1 }R\nr=2)\n= ewkγ(r)\nθ(k)(z(k)\ni )\n= ewk ·\nexp{(β(k)\nr )⊤z −δ(k)\nr\n+ log w(k)\nr\n−log w(k)\n1 }\n1 + PR\nr=2 exp{(β(k)\nr )⊤z −δ(k)\nr\n+ log w(k)\nr\n−log w(k)\n1 }\n,\nwhich is C-Lipschitz w.r.t. ( ew, {(β(k)\nr )⊤z −δ(k)\nr\n+log w(k)\nr\n−log w(k)\n1 }R\nr=2) as a R-dimensional\nvector with a constant C. Denote g(k)\nir = (β(k)\nr )⊤z(k)\ni\n−δ(k)\nr\n+ log w(k)\nr\n−log w(k)\n1 . A direct\napplication of Lemma 38 implies that\n1\nnS\nE\nsup\n{θ(k)}k∈S∈BJ,2\ncon\nsup\n| ewk|≤1\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\newkγ(r)\nθ(k)(z(k)\ni )\n\f\f\f\f\f\n≲1\nnS\nE sup\n| ewk|≤1\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\newkϵ(k)\ni1\n\f\f\f\f\f + 1\nnS\nR\nX\nr=2\nE\nsup\n{θ(k)}k∈S∈BJ,2\ncon\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\ng(k)\nir ϵ(k)\nir\n\f\f\f\f\f .\nBy a similar argument involving covering number as before, we can show that\n1\nnS\nE sup\n| ewk|≤1\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\newkϵ(k)\ni1\n\f\f\f\f\f + 1\nnS\nR\nX\nr=2\nE\nsup\n{θ(k)}k∈S∈BJ,2\ncon\n\f\f\f\f\f\nX\nk∈S\nnk\nX\ni=1\ng(k)\nir ϵ(k)\nir\n\f\f\f\f\f ≲\nr\np + K\nnS\n.\nTherefore W ≲\nq\np+K\nnS\nwith probability at least 1 −C′ exp{−C′′p}..\nThe third bound we want to show is\nsup\nθ(k)∈Bcon\n∥β(k)−β(k)∗∥2≤ξ(k)\n\f\f\f\f\f\n1\nnk\nnk\nX\ni=1\n\u0002\n1 −γ(r)\nθ(k)(z(k)\ni )\n\u0003\n(z(k)\ni )⊤β(k)∗−E\n\u0002\n[1 −γ(r)\nθ(k)(z(k))](z(k))⊤β(k)∗\u0003\n\f\f\f\f\f\n≲ξ(k)\nr p\nnk\n+\nr\nlog K\nnk\n,\nfor all k ∈S and r = 1 : R, with probability at least 1 −C′(K−2 + K−1e−C′′p). Denote\nthe LHS as W ′′. Similar to the previous two proofs, we derive an upper bound for W ′′ by\ncontrolling W ′′ −EW ′′ and EW ′′, seperately. The first part involving W ′′ −EW ′′ is similar\nto the proof of part (i) in Lemma 12 and the second part involving EW ′′ is similar to the\nproof of (S.5.74), so we omit the details.\n105\nThe arguments to derive these three bounds can be used to derive other results similar\nto the lemmas used in the proof of Theorem 1. With these lemmas in hand, the remaining\nproof is almost the same as the proof of Theorem 1.\nS.5.13.3\nProof of lemmas\nProof of Lemma 37. We will prove the contraction of wr first, and only sketch the different\npart for the proof of contraction of µr because the proofs are quite similar.\nPart 1: Contraction of |wr(θ) −w∗\nr|:\nFirst, note that wr(θ∗) = w∗\nr and µr(θ∗) = µ∗\nr. Therefore,\n|wr(θ) −w∗\nr| =\n\f\f\fE[γ(r)\nθ (z) −γ(r)\nθ∗(z)]\n\f\f\f\n≤\nR\nX\ner=1\nw(k)∗\nr\n\f\f\fE[γ(r)\nθ (z) −γ(r)\nθ∗(z)|y = er]\n\f\f\f\n≤\nR\nX\ner=1\nw(k)∗\nr\nR\nX\nr′=2\n\f\f\f\f\fE\n\"\n∂γ(r)\nθ (z)\n∂wr′\n\f\f\f\f\nθ=eθt\n\f\f\f\fy = er\n#\f\f\f\f\f · |wr′ −w∗\nr|\n+\nR\nX\ner=1\nw(k)∗\nr\nR\nX\nr′=2\n\f\f\f\f\fE\n\"\n∂γ(r)\nθ (z)\n∂δr′\n\f\f\f\f\nθ=eθt\n\f\f\f\fy = er\n#\f\f\f\f\f · |δr′ −δ∗\nr|\n+\nR\nX\ner=1\nw(k)∗\nr\nR\nX\nr′=2\n\f\f\f\f\f\f\nE\n\"\n∂γ(r)\nθ (z)\n∂βr′\n\f\f\f\f\nθ=eθt\n\f\f\f\fy = er\n#⊤\n(βr′ −β∗\nr)\n\f\f\f\f\f\f\n,\nWe only show how to bound\n\f\f\fE[γ(r)\nθ (z) −γ(r)\nθ∗(z)|y = 1]\n\f\f\f, i.e. the case when er = 1. For the\nother er = 2 : R, the proof is the same by changingt the reference level from y = 1 to y = er.\nNote that\n\f\f\fE[γ(r)\nθ (z) −γ(r)\nθ∗(z)|y = 1]\n\f\f\f ≤\nR\nX\nr′=2\n\f\f\f\f\fE\n\"\n∂γ(r)\nθ (z)\n∂wr′\n\f\f\f\f\nθ=eθt\n\f\f\f\fy = 1\n#\f\f\f\f\f · |wr′ −w∗\nr|\n+\nR\nX\nr′=2\n\f\f\f\f\fE\n\"\n∂γ(r)\nθ (z)\n∂δr′\n\f\f\f\f\nθ=eθt\n\f\f\f\fy = 1\n#\f\f\f\f\f · |δr′ −δ∗\nr|\n+\nR\nX\nr′=2\n\f\f\f\f\f\f\nE\n\"\n∂γ(r)\nθ (z)\n∂βr′\n\f\f\f\f\nθ=eθt\n\f\f\f\fy = 1\n#⊤\n(βr′ −β∗\nr)\n\f\f\f\f\f\f\n.\nwhere eθt = ({ ewr}R\nr=2, { eβr}R\nr=2, {eδr}R\nr=2) with ewr = twr + (1 −t)w∗\nr, eβr = tβr + (1 −t)β∗\nr,\neδr = tδr + (1 −t)δ∗\nr, and δr = 1\n2β⊤\nr (µr + µ1). We will bound the three terms on the RHS\n106\nseparately. Note that when θ ∈Bcon(θ∗), we have wr ∈[cw/2, 1 −cw], ∥βr −β∗\nr∥2 ≤Cb∆,\nand maxr=1:R ∥µr −µ∗\nr∥2 ≤Cb∆, hence ewr ∈[cw/2, 1 −cw], ∥eβr −β∗\nr∥2 ≤tCb∆..\n(i) Bounding |E[\n∂γ(r)\nθ (z)\n∂wr′ |θ=eθt|y = 1]|: Note that\n∂γ(r)\nθ (z)\n∂wr′\n=\nexp{ eβ⊤\nr z −δr}\new1 + PR\nr=2 ewr exp{ eβ⊤\nr z −δr}\n−ewr exp{ eβ⊤\nr z −δr}(exp{ eβ⊤\nr z −δr} −1)\n( ew1 + PR\nr=2 ewr exp{ eβ⊤\nr z −δr})2\n=\n\n\n\n\n\nexp{ eβ⊤\nr z−δr}( ew1+ ewr+P\nr′̸=r ewr′ exp{ eβ⊤\nr′z−δr′})\n( ew1+PR\nr=2 ewr exp{ eβ⊤\nr z−δr})2\n,\nr′ = r,\n−\newr·exp{ eβ⊤\nr z−δr}(exp{ eβ⊤\nr′z−δr′}−1)\n( ew1+PR\nr=2 ewr exp{ eβ⊤\nr z−δr})2\n,\nr′ ̸= r.\nHence\nE\n\"\n∂γ(r)\nθ (z)\n∂wr\n\f\f\f\fy = 1\n#\n= E\n\"\nexp{ eβ⊤\nr z(1) −δr}( ew1 + ewr + P\nr′̸=r ewr′ exp{ eβ⊤\nr′z(1) −δr′})\n( ew1 + PR\nr=2 ewr exp{ eβ⊤\nr z(1) −δr})2\n#\n|\n{z\n}\n(∗)\n.\nLet ezr′ = eβ⊤\nr′(z(1) −µ∗\n1) ∼N(0, eβ⊤\nr′Σ∗eβr′). And notice that\neβ⊤\nr′µ∗\n1 −eδr′ = t(β⊤\nr′µ∗\n1 −δr′) + (1 −t)[(β∗\nr′)⊤µ∗\n1 −δ∗\nr′],\nwhere\nβ⊤\nr′µ∗\n1 −δr′ = [β∗\nr′ + (βr′ −β∗\nr′)]⊤h1\n2(µ∗\n1 −µ∗\nr′) + 1\n2(µ∗\nr′ −µr′) + 1\n2(µ∗\n1 −µ1)\ni\n= −1\n2 (µ∗\nr′ −µ∗\n1)⊤(Σ∗)−1(µ∗\nr′ −µ∗\n1)\n|\n{z\n}\nA2\nr′\n+1\n2(β∗\nr′)⊤(Σ∗)1/2(Σ∗)−1/2[(µ∗\nr′ −µr′) + (µ∗\n1 −µ1)]\n+ 1\n2(βr′ −β∗\nr′)⊤(Σ∗)1/2(Σ∗)−1/2(µ∗\n1 −µ∗\nr′)\n+ 1\n2(βr′ −β∗\nr′)⊤[(µ∗\nr′ −µr′) + (µ∗\n1 −µ1)],\n(β∗\nr′)⊤µ∗\n1 −δ∗\nr′ = −1\n2Ar′,\nand Ar′ =\np\n(µ∗\nr′ −µ∗\n1)⊤(Σ∗)−1(µ∗\nr′ −µ∗\n1) =\np\n(β∗\nr)⊤Σ∗β∗\nr. By the fact that maxr=1:R ∥µr−\nµ∗\nr∥2 ≤Cb∆and maxr=1:R ∥βr −β∗\nr∥2 ≤Cb∆, we have\n−1\n2A2\nr′ −2c1/2\nΣ Cb∆Ar′ −C2\nb ∆2 ≤β⊤\nr′µ∗\n1 −δr′ ≤−1\n2A2\nr′ + 2c1/2\nΣ Cb∆Ar′ + C2\nb ∆2,\nimplying that\n−1\n2A2\nr′ −2c1/2\nΣ Cb∆Ar′ −C2\nb ∆2 ≤eβ⊤\nr′µ∗\n1 −eδr′ ≤−1\n2A2\nr′ + 2c1/2\nΣ Cb∆Ar′ + C2\nb ∆2.\n107\nBy Gaussian tail, we have\nP\n R\n\\\nr′=2\nn\n|ezr′| ≤1\n4\neβ⊤\nr′Σ∗eβr′\no!\n≥1 −CR exp\nn\n−1\n32\neβ⊤\nr′Σ∗eβr′\no\n.\nDenote event E = TR\nr′=1\n\b\n|ezr′| ≤1\n4cΣC2\nb ∆2 + 1\n2Cbc1/2\nΣ ∆Ar′ + 1\n4A2\nr′\n\t\n. Since\n1\n4\neβ⊤\nr′Σ∗eβr′ = 1\n4( eβr′ −β∗\nr′)⊤Σ∗( eβr′ −β∗\nr′) + 1\n2( eβr′ −β∗\nr′)⊤(Σ∗)1/2(Σ∗)1/2β∗\nr′ + 1\n4(β∗\nr′)⊤Σ∗β∗\nr′\n≤1\n4cΣC2\nb ∆2 + 1\n2Cbc1/2\nΣ ∆Ar′ + 1\n4A2\nr′,\nand\neβ⊤\nr′Σ∗eβr′ ≥A2\nr′ −cΣC2\nb ∆2 −2Cbc1/2\nΣ ∆Ar′ ≥(1 −cΣC2\nb −2Cbc1/2\nΣ )∆2 ≥1\n2∆2,\nwe have\nP(E) ≥1 −CR exp\nn\n−1\n64∆2o\n.\nThen since minr′=1:R Ar′ ≥∆≥5c1/2\nΣ Cb∆and Cb ≤\nc−1/2\nΣ\n40\n∧(2cΣ + 8)−1/2, we have\n(∗)\n≤E\n\"\nexp{−1\n4A2\nr + 5\n2c1/2\nΣ Cb∆Ar + (1\n4cΣ + 1)C2\nb ∆2}\new2\n1\n·\n\u0012\new1 + ewr +\nX\nr′̸=r\newr′ exp\nn\n−1\n4A2\nr + 5\n2c1/2\nΣ Cb∆Ar +\n\u00101\n4cΣ + 1\n\u0011\nC2\nb ∆2o\u0013\f\f\f\fE\n#\n+ P(Ec)\n≲c−2\nw exp{−C∆2}.\nHence,\n\f\f\f\f\fE\n\"\n∂γ(r)\nθ (z)\n∂wr\n\f\f\f\nθ=eθt\n\f\f\f\fy = 1\n#\f\f\f\f\f ≲c−2\nw exp{−C∆2}.\nSimilarly, it can be shown that\n\f\f\f\f\fE\n\"\n∂γ(r)\nθ (z)\n∂wr′\n\f\f\f\nθ=eθt\n\f\f\f\fy = 1\n#\f\f\f\f\f ≲c−2\nw exp{−C∆2}.\nfor any r′ = 2 : R.\n108\n(ii) Bounding |E[\n∂γ(r)\nθ (z)\n∂δr′\n|θ=eθt|y = 1]|: Note that\n∂γ(r)\nθ (z)\n∂δr′\n=\n\n\n\n\n\n−wr·exp{β⊤\nr z−δr}·P\nr′̸=r wr′ exp{β⊤\nr′z−δr′}\n(w1+PR\nr=2 wr exp{β⊤\nr z−δr})2\n,\nr′ = r,\n−\nwr·exp{β⊤\nr z−δr}·wr′·exp{β⊤\nr′z−δr′}\n(w1+PR\nr=2 wr exp{β⊤\nr z−δr})2\n,\nr′ ̸= r.\nThe analysis is almost the same as in (i), which leads to\n\f\f\f\f\fE\n\"\n∂γ(r)\nθ (z)\n∂δr′\n\f\f\f\nθ=θ\n#\f\f\f\f\f ≲c−2\nw exp{−C∆2},\nfor any r′ = 2 : R. We omit the proof here.\n(iii) Bounding |E[\n∂γ(r)\nθ (z)\n∂βr′ |θ=eθt|y = 1]⊤(βr′ −β∗\nr)|: Note that\n∂γ(r)\nθ (z)\n∂βr′\n=\n\n\n\n\n\n−wr·exp{β⊤\nr z−δr}·P\nr′̸=r wr′ exp{β⊤\nr′z−δr′}z\n(w1+PR\nr=2 wr exp{β⊤\nr z−δr})2\n,\nr′ = r,\n−\nwr·exp{β⊤\nr z−δr}·wr′·exp{β⊤\nr′z−δr′}z\n(w1+PR\nr=2 wr exp{β⊤\nr z−δr})2\n,\nr′ ̸= r.\n• When r′ = r:\nE\n\n\n \n∂γ(r)\nθ (z)\n∂βr′\n\f\f\f\f\nθ=eθt\n!⊤\n(βr −β∗\nr)\n\f\f\f\fy = 1\n\n\n= E\n\"\newr exp{ eβ⊤\nr z(1) −eδr} P\nr′̸=r ewr′ · exp{ eβ⊤\nr′z(1) −eδr′} · (z(1))⊤(βr −β∗\nr)\n( ew1 + PR\nr=2 ewr exp{ eβ⊤\nr z(1) −eδr})2\n#\n≤\nv\nu\nu\ntE\n\"\newr exp{ eβ⊤\nr z(1) −eδr} P\nr′̸=r ewr′ · exp{ eβ⊤\nr′z(1) −eδr′}\n( ew1 + PR\nr=2 ewr exp{ eβ⊤\nr z(1) −eδr})2\n#2\n|\n{z\n}\n(1)\n·\nq\nE[(z(1))⊤(βr −β∗\nr)]2\n|\n{z\n}\n(2)\n.\nSimilar to the previous argument in (i), let ezr′ = eβ⊤\nr′(z(er) −µ∗\n1) ∼N(0, eβ⊤\nr′Σ∗eβr′) and event\nE = TR\nr′=1\n\b\n|ezr′| ≤1\n4cΣC2\nb ∆2 + 1\n2Cbc1/2\nΣ ∆Ar′ + 1\n4A2\nr′\n\t\n, then\nP(E) ≥1 −CR exp\nn\n−1\n64∆2o\n.\nSimilar to (i), we have\n(1) ≲\nv\nu\nu\nu\ntE\n\n\n \newr exp{ eβ⊤\nr z(1) −eδr} P\nr′̸=r ewr′ · exp{ eβ⊤\nr′z(1) −eδr′}\n( ew1 + PR\nr=2 ewr exp{ eβ⊤\nr z(1) −eδr})2\n!2 \f\f\f\fE\n\n+ P(Ec) ≲exp{−C∆2}.\nMoreover, (z(1))⊤(βr−β∗\nr) = (z(1)−µ∗\n1)⊤(βr−β∗\nr)+(µ∗\n1)⊤(βr−β∗\nr), where (z(1)−µ∗\n1)⊤(βr−\nβ∗\nr) ∼N(0, (βr −β∗\nr)⊤Σ∗(βr −β∗\nr)) and |(µ∗\n1)⊤(βr −β∗\nr)| ≤M∥βr −β∗\nr∥2, hence\n(2) ≲\np\n(βr −β∗\nr)⊤Σ∗(βr −β∗\nr)+M∥βr−β∗\nr∥2 ≲(c1/2\nΣ +M)∥βr−β∗\nr∥2 ≲(c1/2\nΣ +M)Cb∆.\n109\nTherefore, since ∆≤2Mc1/2\nΣ\nand ∆≳log1/2(cΣMc−1\nw ),\nE\n\n\n \n∂γ(r)\nθ (z)\n∂βr′\n!⊤\n(βr −β∗\nr)\n\f\f\f\fy = 1\n\n≲c−2\nw exp{−C′∆2}·(c1/2\nΣ +M)Mc1/2\nΣ ≲exp{−C′∆2}.\n• When r′ ̸= r: we can obtain\nE\n\n\n \n∂γ(r)\nθ (z)\n∂βr′\n!⊤\n(βr −β∗\nr)\n\f\f\f\fy = 1\n\n≲exp{−C′∆2}.\nsimilarly.\nCombining (i)-(iii), we have\n|wr(θ) −w∗\nr| ≲exp{−C′′∆2} ·\nR\nX\nr=2\n(|wr −w∗\nr| + |δr −δ∗\nr| + ∥βr −β∗\nr∥2).\nPart 2: Contraction of ∥µr(θ) −µ∗\nr∥2:\nBy definition,\n∥µr(θ) −µ∗\nr∥2 ≤∥E[γ(r)\nθ (z)z]∥2\nwr(θ)w∗\nr\n· |w∗\nr −wr(θ)| + ∥E[(γ(r)\nθ (z) −γ(r)\nθ∗(z))z]∥2\nw∗\nr\n,\n(S.5.76)\nimplying that\n∥E[(γ(r)\nθ (z) −γ(r)\nθ∗(z))z]∥2 ≤\nR\nX\nr′=2\n\r\r\r\r\rE\n\u0014∂γ(r)\nθ (z)\n∂wr′\n\f\f\f\f\nθ=eθt\nz\n\u0015\r\r\r\r\r\n2\n· |wr′ −w∗\nr′|\n+\nR\nX\nr=2\n\r\r\r\r\rE\n\u0014∂γ(r)\nθ (z)\n∂δr′\n\f\f\f\f\nθ=eθt\nz\n\u0015\r\r\r\r\r\n2\n· |δr′ −δ∗\nr|\n+\nR\nX\nr=2\n\r\r\r\r\rE\n\u0014∂γ(r)\nθ (z)\n∂βr′\n\f\f\f\f\nθ=eθt\nz\n\u0015\r\r\r\r\r\n2\n· ∥βr′ −β∗\nr∥2,\nwhere eθt = ({ ewr}R\nr=2, { eβr}R\nr=2, {eδr}R\nr=2) with ewr = twr + (1 −t)w∗\nr, eβr = tβr + (1 −t)β∗\nr,\nand eδr = tδr + (1 −t)δ∗\nr. We will bound the three terms on the RHS separately. Note that\nwhen θ ∈Bcon(θ∗), we have ewr ∈(cw/2, 1 −cw), ∥eβr −β∗\nr∥2 ≤Cb∆, and |eδr −δr| ≤Cb∆.\nFor any u ∈Rp with ∥u∥2 ≤1 and any er ∈1 : R, similar to our previous arguments,\nwe have\n\f\f\f\f\fE\n\u0014∂γ(r)\nθ (z)\n∂wr′\n\f\f\f\f\nθ=eθt\nz⊤u\n\f\f\f\fy = er\n\u0015\f\f\f\f\f ≤\nv\nu\nu\ntE\n\"\n∂γ(r)\nθ (z)\n∂wr′\n#2\n·\nq\nE[(z(er))⊤u]2 ≲exp{−C′′∆2},\n110\nwhich leads to\n\r\r\r\r\rE\n\u0014∂γ(r)\nθ (z)\n∂wr′\nz\n\u0015\r\r\r\r\r\n2\n≲exp{−C′′∆2},\nfor any r′ ∈2 : R. Similarly, we have\n\r\r\r\r\rE\n\u0014∂γ(r)\nθ (z)\n∂δr′\nz\n\u0015\r\r\r\r\r\n2\n,\n\r\r\r\r\rE\n\u0014∂γ(r)\nθ (z)\n∂βr′\nz\n\u0015\r\r\r\r\r\n2\n≲exp{−C′′∆2}.\nfor any r′ ∈2 : R. Therefore, ∥E[(γ(r)\nθ (z) −γ(r)\nθ∗(z))z]∥2 ≲exp{−C′′∆2}. By part 1,\nwe have\n∥E[γ(r)\nθ (z)z]∥2\nwr(θ)w∗r\n· |w∗\nr −wr(θ)| ≲exp{−C′′∆2} · d(θ, θ∗). Hence by (S.5.76), we have\n∥µr(θ) −µ∗\nr∥2 ≲exp{−C′′∆2} · d(θ, θ∗).\nCombining part 1 and part 2, we complete the proof.\nS.5.14\nProof of Theorem 8\nNote that the excess risk\nRθ\n(k)∗(Cbθ(k)) −Rθ\n(k)∗(Cθ(k)∗)\n= P(y(k) ̸= Cbθ(k)(z(k))) −P(y(k) ̸= Cθ(k)∗(z(k)))\n=\nZ\nCbθ(k)̸=Cθ(k)∗\n\u0002\nP(y(k) = Cθ(k)∗(z)|z(k) = z) −P(y(k) = Cbθ(k)(z)|z(k) = z)\n\u0003\ndPθ(k)∗(z)\n=\nZ\nCbθ(k)̸=Cθ(k)∗\nh\nmax\nr=1:R P(y(k) = r|z(k) = z) −P(y(k) = Cbθ(k)(z)|z(k) = z)\ni\ndPθ(k)∗(z).\n(S.5.77)\nLet event E =\n\b\nz : maxr P(y(k) = r|z(k) = z) −maxj{P(y(k) = j|z(k) = z) : P(y(k) =\nj|z(k) = z) < maxr P(y(k) = r|z(k) = z)} ≤t\n\t\n. We claim that the margin condition\nP(E) ≲t holds for any t ≤a small constant c (to be verified). If this is the case, then\ndenote eE =\n\b\nmaxr |η(r)\nbθ(k)(z(k)) −η(r)\nθ(k)∗(z(k))| ≤t/2\n\t\n.\nr∗= arg max\nr\nη(r)\nθ(k)∗(z),\nbr = arg max\nr\nη(r)\nbθ(k)(z).\nWe have\nRHS of (S.5.77)\n111\n≤\nZ\nr∗̸=br\nE,eE\n\u0002\nη(r∗)\nθ(k)∗(z) −η(br)\nθ(k)∗(z)\n\u0003\ndPθ(k)∗(z) +\nZ\nr∗̸=br\nEc,eE\n\u0002\nη(r∗)\nθ(k)∗(z) −η(br)\nbθ(k)(z)\n\u0003\ndPθ(k)∗(z) + P(eEc)\n≤tP(E) + P(eEc),\n(S.5.78)\nwhere the last inequality comes from the fact that when r∗̸= br, η(r∗)\nθ(k)∗(z) −η(br)\nθ(k)∗(z) ≤t on\nE. And notice that on Ec ∩eE, we must have br = r∗because if br ̸= r∗, then\nη(br)\nbθ(k)(z) −η(r∗)\nbθ(k)(z) ≤η(br)\nθ(k)∗(z) −η(r∗)\nθ(k)∗(z) + t\n2 + t\n2 ≤−t + t < 0,\nwhich is a contradiction with the definition of br. Hence {r∗̸= br}∩E ∩eE is empty. Therefore\nR\nr∗̸=br\nEc,eE\n\u0002\nη(r∗)\nθ(k)∗(z) −η(br)\nbθ(k)(z)\n\u0003\ndPθ(k)∗(z) = 0. Finally, by Lipschitzness,\nP(eEc) = P\n\u0010\nmax\nr\n|η(r)\nbθ(k)(z(k)) −η(r)\nθ(k)∗(z(k))| > t/2\n\u0011\n≤\nR\nX\nr=1\nP\n\u0010\n|η(r)\nbθ(k)(z(k)) −η(r)\nθ(k)∗(z(k))| > t/2\n\u0011\n≲P(|( bβ(k) −β(k)∗)⊤z(k) −bδ(k) + δ(k)∗−log bw(k)\nr\n+ log bw(1)\nr\n+ log w(k)∗\nr\n−log w(k)∗\n1\n| > Ct)\n≲P(|( bβ(k) −β(k)∗)⊤(z(k) −µ(k))| > C′t)\n≲exp\n(\n−\nCt2\n∥bβ(k) −β(k)∗∥2\n2\n)\n.\nPlugging back into (S.5.78), we have\nRθ\n(k)∗(Cbθ(k)) −Rθ\n(k)∗(Cθ(k)∗) ≲t2 + exp\n(\n−\nCt2\n∥bβ(k) −β(k)∗∥2\n2\n)\n.\nLet t ≍d(bθ(k), θ(k)∗)\nq\nlog d−1(bθ(k), θ(k)∗):\nRθ\n(k)∗(Cbθ(k))−Rθ\n(k)∗(Cθ(k)∗) ≲d2(bθ(k), θ(k)∗) log d−1(bθ(k), θ(k)∗) ≲d2(bθ(k), θ(k)∗) log\n\u0012\nnS\np + log nS\n\u0013\n.\nThen plugging in the upper bound of d(bθ(k), θ(k)∗) in Theorem 7 completes the proof.\nIt remains to verify the margin condition P(E) ≲t for any t ≤a small constant c. In\nfact,\nP(E) =\nR\nX\nr=1\nX\nj̸=r\nP\n\u0012\narg max\nr′\nP(y = r′|z(k)) = r, arg max\nr′̸=r\nP(y = r′|z(k)) = j,\n112\nP(y = r|z(k)) −P(y = j|z(k)) ≤t\n\u0013\n≤\nR\nX\nr=1\nX\nj̸=r\nP\n\u0012\narg max\nr′\nP(y = r′|z(k)) = r, arg max\nr′̸=r\nP(y = r′|z(k)) = j,\n1 −P(y = j|z(k))\nP(y = r|z(k)) ≤\nt\nP(y = r|z(k)) ≤Rt\n\u0013\n≤\nR\nX\nr=1\nX\nj̸=r\nP\n\u0012\n1 −Rt ≤P(y = j|z(k))\nP(y = r|z(k))\n= exp{(β(k)∗\nj\n−β(k)∗\nr\n)⊤z(k) −δ(k)∗\nj\n+ δ(k)∗\nr\n+ log w(k)∗\nj\n−log w(k)∗\nr\n}\n\u0013\n≲\nR\nX\nr=1\nX\nj̸=r\nR\nX\nr′=1\nP\n\u0012\nlog(1 −Rt) ≤N\n\u0000(β(k)∗\nj\n−β(k)∗\nr\n)⊤µ(k)∗\nr\n−δ(k)∗\nj\n+ δ(k)∗\nr\n+ log w(k)∗\nj\n−log w(k)∗\nr\n,\n(β(k)∗\nj\n−β(k)∗\nr\n)⊤Σ(k)∗(β(k)∗\nj\n−β(k)∗\nr\n)\n\u0001\n≤0\n\u0013\n≲−log(1 −Rt)\n≲t,\nwhen t > 0 is less than some constant c > 0. Note that we used the fact that (β(k)∗\nj\n−\nβ(k)∗\nr\n)⊤Σ(k)∗(β(k)∗\nj\n−β(k)∗\nr\n) ≥∆2 ≥some constant C, which implies that the Gaussian\ndensity is upper bounded by a constant. Hence the marginal condition is true.\nWe want to point out that this multi-class extension of margin condition in binary case\nhas been widely used in literature of multi-class classification. For example, see Chen and\nSun (2006) and Vigogna et al. (2022).\nS.5.15\nProof of Theorem 9\nThe proof is almost the same as the proof of Theorem 3, by noticing that we can make the\nGMM parameters the same across r-th task with r ≥3 to reduce the problem to the case\nR = 2, so we do not repeat it here.\n113\nS.5.16\nProof of Theorem 10\nS.5.16.1\nLemmas\nLemma 39. Consider θ = {{wr}R\nr=2, {βr}R\nr=2, {δr}R\nr=2, {µr}R\nr=1, Σ} and θ\n′ = {{w′\nr}R\nr=2, {β′\nr}R\nr=2,\n{δ′\nr}R\nr=2, {µ′\nr}R\nr=1, Σ′} with wr = w′\nr for r ≥3, µr = µ′\nr = er for r ≥2, µ1 = µ′\n1 = 0, and\nΣ = Σ′ = Ip. Then βr = β′\nr = er, δr = δ′\nr = β⊤\nr (µ1+µ2\n2\n) = 1\n2 for r ≥2, and\nPθ(Cθ ̸= Cθ\n′) ≳|w2 −w′\n2|.\nLemma 40. Consider θ = {{wr}R\nr=2, {βr}R\nr=2, {δr}R\nr=2, {µr}R\nr=1, Σ} and θ\n′ = {{w′\nr}R\nr=2, {β′\nr}R\nr=2,\n{δ′\nr}R\nr=2, {µ′\nr}R\nr=1, Σ′} with wr = w′\nr = 1\nR for r = 1 : R, µr = (u + 1)er, µ′\nr = er for r ≥2,\nµ1 = uer, µ′\n1 = 0, and Σ = Σ′ = Ip, where 0 < u ≤C.\nThen βr = β′\nr = er,\nδr = e⊤\nr (µ1+µ2\n2\n) = u + 1\n2, δ′\nr = e⊤\nr (µ′\n1+µ′\n2\n2\n) = 1\n2 for r ≥2, and\nPθ(Cθ ̸= Cθ\n′) ≳u.\nLemma 41. Consider θ = {{wr}R\nr=2, {βr}R\nr=2, {δr}R\nr=2, {µr}R\nr=1, Σ} and θ\n′ = {{w′\nr}R\nr=2, {β′\nr}R\nr=2,\n{δ′\nr}R\nr=2, {µ′\nr}R\nr=1, Σ′} with wr = w′\nr =\n1\nR for r = 1 : R, µr = µ′\nr = er for r ≥3,\nµ1 = µ′\n1 = 0, and Σ = Σ′ = Ip. Suppose µ2 and µ′\n2 satisfy (µ2)3:R = (µ2)3:R = 0. Then\nβr = β′\nr = er for r ≥3, δr = δ′\nr for r ≥3, β2 = µ2, β′\n2 = µ′\n2, δ2 = 1\n2∥µ2∥2\n2, δ′\n2 = 1\n2∥µ′\n2∥2\n2\nwhere ∥µ2∥2 = ∥µ′\n2∥2 = 1 with µ⊤\n2 µ′\n2 >\n√\n2\n2 , and\nPθ(Cθ ̸= Cθ\n′) ≳∥µ2 −µ′\n2∥2.\nS.5.16.2\nMain proof of Theorem 10\nGiven the three lemmas we presented, the proof is almost the same as the proof of Theorem\n4. We do not repeat it here.\nS.5.16.3\nProof of lemmas\nProof of Lemma 39. Note that zj’s are independent given y = 3. We have\nPθ(Cθ ̸= Cθ\n′) ≥Pθ(Cθ(z) = 2, Cθ\n′(z) ̸= 2)\n≥Pθ\n\u0010\nz2 −1\n2 −log w1 + log w2 ≥0, z2 −1\n2 −log w′\n1 + log w′\n2 ≤0\nzr −1\n2 −log w1 + log wr ≤0 for all r ≥3\n\u0011\n114\n≥w3Pθ\n\u0010\nz2 −1\n2 −log w1 + log w2 ≥0, z2 −1\n2 −log w′\n1 + log w′\n2 ≤0\nzr −1\n2 −log w1 + log wr ≤0 for all r ≥3\n\f\f\fy = 3\n\u0011\n≳Pθ\n\u00121\n2 + log w1 −log w2 ≤z2 ≤1\n2 + log w′\n1 −log w′\n2\n\f\f\fy = 3\n\u0013\n·\nR\nY\nr=3\nP\n\u0010\nzr −1\n2 −log w1 + log wr ≤0\n\u0011\n≳|log w1 −log w2 −log w′\n1 + log w′\n2|\n= |log(1 −w2) −log w2 −log(1 −w′\n2) + log w′\n2|\n≳|w2 −w′\n2|.\nProof of Lemma 40. Note that zj’s are independent given y = 3. We have\nPθ(Cθ ̸= Cθ\n′) ≥Pθ(Cθ(z) = 2, Cθ\n′(z) ̸= 2)\n≥Pθ\n\u0010\nz2 −1\n2 −u ≥0, z2 −1\n2 > 0, zr −1\n2 −u ≤0, zr −1\n2 ≤0 for all r ≥3\n\u0011\n≥Pθ\n\u00101\n2 ≤z2 ≤1\n2 + u, zr ≤1\n2 for all r ≥3\n\u0011\n≳w3Pθ\n\u00121\n2 ≤z2 ≤1\n2 + u, zr ≤1\n2 for all r ≥3\n\f\f\fy = 3\n\u0013\n≳w3Pθ\n\u00121\n2 ≤z2 ≤1\n2 + u\n\f\f\fy = 3\n\u0013\n·\nR\nY\nr=3\nP\n\u0010\nzr ≤1\n2\n\u0011\n≳u,\nwhere we used the fact that P\n\u0000zr ≤1\n2|y = 3\n\u0001\n≥some constant C.\nProof of Lemma 40. Note that zj’s are independent given y = 3. We have\nPθ(Cθ ̸= Cθ\n′) ≥Pθ(Cθ(z) = 2, Cθ\n′(z) ̸= 2)\n≥Pθ\n\u0010\nµ⊤\n2 z −1\n2∥µ2∥2\n2 ≥0, (µ′\n2)⊤z −1\n2∥µ′\n2∥2\n2 ≤0, zr ≤1\n2 for all r ≥3\n\u0011\n≥w3Pθ\n\u0010\nµ⊤\n2 z −1\n2 ≥0, (µ′\n2)⊤z −1\n2 ≤0, zr ≤1\n2 for all r ≥3\n\f\f\fy = 3\n\u0011\n≳w3Pθ\n\u0010\nµ⊤\n2 z ≥1\n2, (µ′\n2)⊤z ≤1\n2\n\f\f\fy = 3\n\u0011\n·\nR\nY\nr=3\nP\n\u0010\nzr ≤1\n2\n\f\f\fy = 3\n\u0011\n115\n≳\ns\n1 −|µ⊤\n2 µ′\n2|2\n∥µ2∥4\n2\n· |µ⊤\n2 µ′\n2|\n∥µ2∥2\n2\n≳∥µ2 −µ′\n2∥2\nThe second last inequality is due to the fact that P\n\u0000zr ≤1\n2|y = 3\n\u0001\n≥some constant C and\nProposition 23 in Azizyan et al. (2013). The last inequality comes from Lemma 8.1 in Cai\net al. (2019).\nS.5.17\nProof of Theorem 11\nWLOG, suppose that π∗\nk satisfies π∗\nk(r) = the “majority class” er, if #{k ∈S : πk(r) =\ner} ≥1\n2|S|. Note that we can make this assumption because it suffices to recover {ι(π∗\nk)}k∈S\nwith a permutation ι. And WLOG, suppose π∗\nk(r) = r for all k ∈S. Let us consider\nany π = {πk}K\nk=1 with πk(r) = π∗\nk(r) for all k ∈Sc and π ̸= π∗. It suffices to prove that\nscore(π) > score(π∗).\nRecall that ξ = maxk∈S minπ maxr∈[R] ∥(bΣ(k))−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −β(k)∗\nr\n∥2. Note that\nscore(π) −score(π∗) =\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nR\nX\nr=2\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ(k′)[0])−1(bµ(k′)[0]\nπk′(r) −bµ(k′)[0]\nπk′(1))∥2\n|\n{z\n}\n[1]\n+\nX\nk̸=k′∈S\nπk(1)̸=πk′(1)\nR\nX\nr=2\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ(k′)[0])−1(bµ(k′)[0]\nπk′(r) −bµ(k′)[0]\nπk′(1))∥2\n|\n{z\n}\n[2]\n+ 2\nX\nk∈S\nk′∈Sc\nR\nX\nr=2\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ(k′)[0])−1(bµ(k′)[0]\nπk′(r) −bµ(k′)[0]\nπk′(1))∥2\n|\n{z\n}\n[3]\n116\n−\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nR\nX\nr=2\n∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ(k′)[0])−1(bµ(k′)[0]\nr\n−bµ(k′)[0]\n1\n)∥2\n|\n{z\n}\n[1]′\n−\nX\nk̸=k′∈S\nπk(1)̸=πk′(1)\nR\nX\nr=2\n∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ(k′)[0])−1(bµ(k′)[0]\nr\n−bµ(k′)[0]\n1\n)∥2\n|\n{z\n}\n[2]′\n−2\nX\nk∈S\nk′∈Sc\nR\nX\nr=2\n∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ(k′)[0])−1(bµ(k′)[0]\nπk′(r) −bµ(k′)[0]\nπk′(1))∥2\n|\n{z\n}\n[3]′\n.\nWe have\n[2] ≥\nX\nk̸=k′∈S\nπk(1)̸=πk′(1)\nR\nX\nr=2\n\u0010\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ(k)[0])−1(bµ(k)[0]\nπk′(r) −bµ(k)[0]\nπk′(1))∥2 −2h −ξ\n\u0011\n≥\nX\nk̸=k′∈S\nπk(1)̸=πk′(1)\nR\n\u0010\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(1) −bµ(k)[0]\nπk′(1))∥2 −2h −ξ\n\u0011\n≥\nX\nk̸=k′∈S\nπk(1)̸=πk′(1)\nR(c−1/2\nΣ\n∆−2h −2ξ).\nHence\n[2]−[2]′ ≥\nX\nk̸=k′∈S\nπk(1)̸=πk′(1)\n[R(c−1/2\nΣ\n∆−2h−2ξ)−R(2ξ +2h)] =\nX\nk̸=k′∈S\nπk(1)̸=πk′(1)\nR(c−1/2\nΣ\n∆−4h−4ξ).\nTherefore,\n[1] =\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nR\nX\nr=2\nX\nπk(r)=πk′(r)\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ(k′)[0])−1(bµ(k′)[0]\nπk′(r) −bµ(k′)[0]\nπk′(1))∥2\n|\n{z\n}\n[1]1\n117\n+\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nR\nX\nr=2\nX\nπk(r)̸=πk′(r)\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ(k′)[0])−1(bµ(k′)[0]\nπk′(r) −bµ(k′)[0]\nπk′(1))∥2\n|\n{z\n}\n[1]2\n.\nCorrespondingly, we can decompose [1]′ in the same way as [1]′ = [1]′\n1 + [1]′\n2 with\n[1]′\n1 =\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nR\nX\nr=2\nX\nπk(r)=πk′(r)\n∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ(k′)[0])−1(bµ(k′)[0]\nr\n−bµ(k′)[0]\n1\n)∥2,\n[1]′\n2 =\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nR\nX\nr=2\nX\nπk(r)=πk′(r)\n∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ(k′)[0])−1(bµ(k′)[0]\nr\n−bµ(k′)[0]\n1\n)∥2.\nNote that\n[1]2 =\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nR\nX\nr=2\nX\nπk(r)̸=πk′(r)\n\u0010\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ(k)[0])−1(bµ(k)[0]\nπk′(r) −bµ(k)[0]\nπk′(1))∥2 −2h −ξ\n\u0011\n=\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nR\nX\nr=2\nX\nπk(r)̸=πk′(r)\n\u0010\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk′(r))∥2 −2h −ξ\n\u0011\n≥\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nR\nX\nr=2\nX\nπk(r)̸=πk′(r)\n\u0010\nc−1/2\nΣ\n∆−2h −2ξ\n\u0011\n,\nhence\n[1]2 −[1]′\n2 ≥\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nR\nX\nr=2\nX\nπk(r)̸=πk′(r)\n\u0010\nc−1/2\nΣ\n∆−4h −4ξ\n\u0011\n.\nAnd\n[1]1 =\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nR\nX\nr=2\nX\nπk(r)=πk′(r)\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ(k′)[0])−1(bµ(k′)[0]\nπk′(r) −bµ(k′)[0]\nπk′(1))∥2\n118\n=\nX\nk̸=k′∈S\nπk(1)=πk′(1)=1\nR\nX\nr=2\nX\nπk(r)=πk′(r)\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ(k′)[0])−1(bµ(k′)[0]\nπk′(r) −bµ(k′)[0]\nπk′(1))∥2\n|\n{z\n}\n[1]11\n+\nX\nk̸=k′∈S\nπk(1)=πk′(1)̸=1\nR\nX\nr=2\nX\nπk(r)=πk′(r)\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ(k′)[0])−1(bµ(k′)[0]\nπk′(r) −bµ(k′)[0]\nπk′(1))∥2\n|\n{z\n}\n[1]12\n.\n[1]12 =\nX\nk̸=k′∈S\nπk(1)=πk′(1)̸=1\nR\nX\nr=2\nX\nπk(r)=πk′(r)\n(∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ(k)[0])−1(bµ(k)[0]\nπk′(r) −bµ(k)[0]\nπk′(1))∥2 −2h −ξ)\n≥−\nX\nk̸=k′∈S\nπk(1)=πk′(1)̸=1\nR\nX\nr=2\nX\nπk(r)=πk′(r)\n(2h + ξ).\nAn important observation is that [1]11 = [1]′\n11. Therefore,\n[1]1 −[1]′\n1 = [1]12 −[1]′\n12 ≥−\nX\nk̸=k′∈S\nπk(1)=πk′(1)̸=1\nR\nX\nr=2\nX\nπk(r)=πk′(r)\n(4h + 3ξ).\nAnd\n[1] −[1]′ = [1]2 −[1]′\n2 + [1]1 −[1]′\n1\n≥\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nR\nX\nr=2\nX\nπk(r)̸=πk′(r)\n(c−1/2\nΣ\n∆−4h −4ξ) −\nX\nk̸=k′∈S\nπk(1)=πk′(1)̸=1\nR\nX\nr=2\nX\nπk(r)=πk′(r)\n(4h + 3ξ).\nFurthermore, by triangle inequality,\n[3] −[3]′ = 2\nX\nk∈S\nk′∈Sc\nR\nX\nr=2\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ(k′)[0])−1(bµ(k′)[0]\nπk′(r) −bµ(k′)[0]\nπk′(1))∥2\n−2\nX\nk∈S\nk′∈Sc\nR\nX\nr=2\n∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ(k′)[0])−1(bµ(k′)[0]\nπk′(r) −bµ(k′)[0]\nπk′(1))∥2\n119\n≥−2\nX\nk∈S\nk′∈Sc\nR\nX\nr=2\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1) −bµ(k′)[0]\nr\n+ bµ(k′)[0]\n1\n)∥2.\nPutting all pieces together,\nscore(π) −score(π∗) ≥\nX\nk̸=k′∈S\nπk(1)̸=πk′(1)\nR(c−1/2\nΣ\n∆−4h −4ξ) +\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nR\nX\nr=2\nX\nπk(r)̸=πk′(r)\n(c−1/2\nΣ\n∆−4h −4ξ)\n−\nX\nk̸=k′∈S\nπk(1)=πk′(1)̸=1\nR\nX\nr=2\nX\nπk(r)=πk′(r)\n(4h + 3ξ)\n−2\nX\nk∈S\nR\nX\nr=2\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1) −bµ(k′)[0]\nr\n+ bµ(k′)[0]\n1\n)∥2 · |Sc|.\nDenote mr = the majority class among {πk(r)}k∈S and S(r)\ner\n= {k ∈S : πk(r) = er}.\n(i)Case 1: |S(1)\n1 | ≤2\n3|S|.\nSince π∗\nk(1) = 1, we have |S(r)\n1 | ≤2\n3|S| for all r ∈[R], otherwise by our assumption,\nm1 = r0 since r0 satisfies |S(r0)\n1\n| > 2\n3|S| ≥|S(1)\n1 |, which is a contradition. Therefore,\nX\nk̸=k′∈S\nπk(1)̸=πk′(1)\n1 = 2\nX\nr̸=r′\n|S(r)\n1 | · |S(r′)\n1\n|\n=\n\u0010\nR\nX\nr=1\n|S(r)\n1 |\n\u00112\n−\nR\nX\nr=1\n|S(r)\n1 |2\n= |S|2 −\nR\nX\nr=1\n|S(r)\n1 |2\n≥|S|2 −\n\u00104\n9|S|2 + 1\n9|S|2\u0011\n= 4\n9|S|2,\nand\nX\nk̸=k′∈S\nπk(1)=πk′(1)̸=1\nR\nX\nr=2\nX\nπk(r)=πk′(r)\n1 ≤R · 2\n\u0012|S| −|S(1)\n1 |\n2\n\u0013\n≤R(|S| −|S(1)\n1 |)2\n≤R|S|2.\n120\nAlso,\nX\nk∈S\nR\nX\nr=2\n1 ≤2R|S|.\nHence, since 4\n9|S| −4D|Sc| = 4\n9(1 −ϵ)K −4DϵK > 0,\nscore(π) −score(π∗) ≥4\n9|S|2R(c−1/2\nΣ\n∆−4h −4ξ) −|S|2R(4h + 4ξ) −2R|S||Sc|(2Dc1/2\nΣ ∆+ 2ξ)\n= |S|R\nh\u00104\n9c−1/2\nΣ\n|S| −4Dc1/2\nΣ |Sc|\n\u0011\n∆−52\n9 |S|h −\n\u001052\n9 |S| + 4|Sc|\n\u0011\nξ\ni\n> 0.\n(ii)Case 2: |S(1)\n1 | > 2\n3|S|.\nIn this case, by our assumption, we must have m1 = 1. And\nX\nk̸=k′∈S\nπk(1)̸=πk′(1)\n1 ≥2|S(1)\n1 |(|S| −|S(1)\n1 |),\nX\nk̸=k′∈S\nπk(1)=πk′(1)̸=1\nR\nX\nr=2\nX\nπk(r)=πk′(r)\n1 ≤R\nX\nk,k′\n1(k ̸= k′ ∈S, πk(1) = πk′(1) ̸= 1)\n≤R · 2\n\u0012|S| −|S(1)\n1 |\n2\n\u0013\n≤R(|S| −|S(1)\n1 |)2\n≤R1\n2|S(1)\n1 |(|S| −|S(1)\n1 |).\nMoreover,\nX\nk∈S\nR\nX\nr=2\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1) −bµ(k)[0]\nr\n+ bµ(k)[0]\n1\n)∥2 · |Sc|\n≤\nX\nk∈S\nπk(1)̸=1\nR\nX\nr=2\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1) −bµ(k)[0]\nr\n+ bµ(k)[0]\n1\n)∥2 · |Sc|\n+\nX\nk∈S\nX\nπk(1)=1\nπk(r)̸=r\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nr\n)∥2 · |Sc|\n≤(|S| −|S(1)\n1 |)R · (2c1/2\nΣ ∆+ 2ξ)|Sc| +\nX\nk∈S\nX\nπk(1)=1\nπk(r)̸=r\n(Dc1/2\nΣ ∆+ ξ)|Sc|.\n121\nFor r satisfying |S(r)\nr | ≤1\n2|S|, we have\nX\nk∈S\nX\nπk(1)=1\nπk(r)̸=r\n≤2|S(1)\n1 |,\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nX\nπk(r)̸=πk′(r)\n1 ≥2 · 1\n2|S|\n\u00102\n3|S| −1\n2|S|\n\u0011\n= 1\n6|S|2.\nFor r satisfying |S(r)\nr | > 1\n2|S|, we have mr = r. Define eS(r)\nr\n= {k ∈S : πk(1) = 1, πk(r) = r}.\nNote that |eS(r)\nr | ≥2\n3|S| −1\n2|S| = 1\n6|S|. Furthermore,\nX\nk∈S\nX\nπk(1)=1\nπk(r)̸=r\n≤|S(1)\n1 | −|eS(r)\nr |,\nX\nk̸=k′∈S\nπk(1)=πk′(1)\nX\nπk(r)̸=πk′(r)\n1 ≥2|eS(r)\nr |(|S(1)\n1 | −|eS(r)\nr |) ≥1\n3|S|(|S(1)\n1 | −|eS(r)\nr |).\nThis implies that\nscore(π) −score(π∗)\n≥2|S(1)\n1 |(|S| −|S(1)\n1 |)R(c−1/2\nΣ\n∆−4h −4ξ) −R1\n2|S(1)\n1 |(|S| −|S(1)\n1 |) · (4h + 3ξ)\n+\nX\nr:|S(r)\nr\n|≤|S|/2\nh1\n6|S|2(c−1/2\nΣ\n∆−4h −4ξ) −2|S(1)\n1 | · 2|Sc|(c1/2\nΣ D∆+ ξ)\ni\n+\nX\nr:|S(r)\nr\n|>|S|/2\nh1\n3|S|(|S(1)\n1 | −eS(r)\nr )(c−1/2\nΣ\n∆−4h −4ξ) −(|S(1)\n1 | −|eS(r)\nr |) · |Sc| · 2(c1/2\nΣ D∆+ ξ)\ni\n≥|S(1)\n1 |(|S| −|S(1)\n1 |)R\n\u0010\n2c−1/2\nΣ\n∆−10h −19\n2 ξ\n\u0011\n+\nX\nr:|S(r)\nr\n|≤|S|/2\n|S|\n\u0014\u00101\n6c−1/2\nΣ\n|S| −4Dc1/2\nΣ |Sc|\n\u0011\n∆−2\n3|S|h −\n\u00102\n3|S| + 4|Sc|\n\u0011\nξ\n\u0015\n+\nX\nr:|S(r)\nr\n|>|S|/2\n\u0014\u00101\n3c−1/2\nΣ\n|S| −2Dc1/2\nΣ |Sc|\n\u0011\n∆−4\n3|S|h −\n\u00104\n3|S| + 2|Sc|\n\u0011\nξ\n\u0015\n(|S(1)\n1 | −|eS(r)\nr |)\n> 0.\nS.5.18\nProof of Theorem 12\nWLOG, consider the step eK ∈[K] in the for loop and the case that ι = indentify mapping\nfrom [K] to [K], and eK ∈S. Denote eS = S ∩[ eK] and eSc = Sc ∩[ eK], hence [ eK] = eS ∪eSc.\n122\nWLOG, consider π1 = π2 = · · · π e\nK−1 = identify from [R] to [R]. Denote π = {πk} e\nK\nk=1 and\neπ = {πk}\ne\nK−1\nk=1 ∪{eπ e\nK} with eπ e\nK = identify from [R] to [R]. It suffices to show that\nscore(π) > score(eπ),\n(S.5.79)\nfor any π with π e\nK ̸= eπ e\nK = identify from [R] to [R]. If this is the case, then bπ = {bπk}K\nk=1\nsatisfies bπk = identity for all k ∈S, which completes the proof.\nWe focus on the derivation of (S.5.79) in the remaining part of the proof.\n(i) Case 1: π e\nK(1) = 1.\nscore(π) −score(π∗) =\nR\nX\nr=2\nX\nk∈eS\n∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nπ e\nK(r) −bµ( e\nK)[0]\n1\n)∥2\n|\n{z\n}\n[1]\n+\nR\nX\nr=2\nX\nk∈eSc\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nπ e\nK(r) −bµ( e\nK)[0]\n1\n)∥2\n|\n{z\n}\n[2]\n−\nR\nX\nr=2\nX\nk∈eS\n∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nr\n−bµ( e\nK)[0]\n1\n)∥2\n|\n{z\n}\n[1]′\n−\nR\nX\nr=2\nX\nk∈eSc\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nr\n−bµ( e\nK)[0]\n1\n)∥2\n|\n{z\n}\n[2]′\n.\nNote that\n[1] −[1]′ = #{r ∈2 : R : π e\nK(r) ̸= r}·\nX\nk∈eS\nh\n∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nπ e\nK(r) −bµ( e\nK)[0]\n1\n)∥2\n−∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nr\n−bµ( e\nK)[0]\n1\n)∥2\ni\n≥#{r ∈2 : R : π e\nK(r) ̸= r} ·\nh\n∥(Σ(k)∗)−1(µ(k)∗\nr\n−µ(k)∗\nπ e\nK(r))∥2 −2ξ −2h −2ξ −2h\ni\n123\n≥#{r ∈2 : R : π e\nK(r) ̸= r} · |eS| ·\n\u0000∆c−1/2\nΣ\n−4ξ −4h\n\u0001\n.\n[2] −[2]′ = #{r ∈2 : R : π e\nK(r) ̸= r}·\nX\nk∈eSc\nh\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nπ e\nK(r) −bµ( e\nK)[0]\n1\n)∥2\n−∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nr\n−bµ( e\nK)[0]\n1\n)∥2\ni\n≥−#{r ∈2 : R : π e\nK(r) ̸= r} ·\nX\nk∈eSc\n∥(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nπ e\nK(r) −bµ( e\nK)[0]\nr\n)∥2\n≥−#{r ∈2 : R : π e\nK(r) ̸= r} · |eSc| ·\n\u0002\n∥(Σ( e\nK)∗)−1(µ( e\nK)∗\nπ e\nK(r) −µ( e\nK)∗\nr\n)∥2 + ξ\n\u0003\n≥−#{r ∈2 : R : π e\nK(r) ̸= r} · |eSc| · (Dc1/2\nΣ ∆+ ξ).\nThese imply that\nscore(π) −score(eπ) = [1] −[1]′ + [2] −[2]′\n≥#{r ∈2 : R : π e\nK(r) ̸= r} ·\n\u0002\n(|eS|c−1/2\nΣ\n−|eSc|Dc1/2\nΣ )∆−(4|eS| + |eSc|)ξ −4|eS|h\n\u0003\n> 0,\nwhere we used the fact that |eS|/|eSc| ≤K0\nKϵ.\n(ii) Case 2: π e\nK(1) ̸= 1.\nscore(π) −score(π∗) =\nR\nX\nr=2\nX\nk∈eS\n∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nπ e\nK(r) −bµ( e\nK)[0]\nπ e\nK(1))∥2\n|\n{z\n}\n[1]\n+\nR\nX\nr=2\nX\nk∈eSc\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nπ e\nK(r) −bµ( e\nK)[0]\nπ e\nK(1))∥2\n|\n{z\n}\n[2]\n−\nR\nX\nr=2\nX\nk∈eS\n∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nr\n−bµ( e\nK)[0]\n1\n)∥2\n|\n{z\n}\n[1]′\n124\n−\nR\nX\nr=2\nX\nk∈eSc\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπk(r) −bµ(k)[0]\nπk(1)) −(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nr\n−bµ( e\nK)[0]\n1\n)∥2\n|\n{z\n}\n[2]′\n.\nBy previous results,\n[1] =\nX\nk∈eS\nR\nX\nr=2\n∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ( e\nK)[0])−1(bµ( e\nK)[0]\nπ e\nK(r) −bµ( e\nK)[0]\nπ e\nK(1))∥2\n≥\nX\nk∈eS\nR\nX\nr=2\n\u0010\n∥(bΣ(k)[0])−1(bµ(k)[0]\nr\n−bµ(k)[0]\n1\n) −(bΣ( e\nK)[0])−1(bµ(k)[0]\nπ e\nK(r) −bµ(k)[0]\nπ e\nK(1))∥2 −2h −ξ\n\u0011\n≥|eS|R ·\n\u0010\n∥(bΣ(k)[0])−1(bµ(k)[0]\nπ e\nK(1) −bµ(k)[0]\n1\n)∥2 −2h −ξ\n\u0011\n≥|eS|R ·\n\u0010\n∥(Σ(k)∗)−1(µ(k)∗\nπ e\nK(1) −µ(k)∗\n1\n)∥2 −ξ −2h −ξ\n\u0011\n≥|eS|R · (c−1/2\nΣ\n∆−2ξ −2h),\nand\n−[1]′ ≥−|eS|R · (2ξ + 2h).\nSimilar to case 1,\n[2] −[2]′ ≥−\nR\nX\nr=2\nX\nk∈eS\n∥(bΣ( e\nK))−1(bµ( e\nK)[0]\nπ e\nK(r) −bµ( e\nK)[0]\nπ e\nK(1) ) −(bΣ( e\nK))−1(bµ( e\nK)[0]\nr\n−bµ( e\nK)[0]\n1\n)∥2\n≥−R|eSc| · (2Dc1/2\nΣ ∆+ 2ξ).\nTherefore,\nscore(π) −score(eπ) = [1] −[1]′ + [2] −[2]′\n≥R\n\u0002\n(|eS|c−1/2\nΣ\n−|eSc| · 2Dc1/2\nΣ )∆−(2|eS| + 2|eSc|)ξ −2|eS|h\n\u0003\n> 0,\nwhere we used the fact that |eS|/|eSc| ≤K0\nKϵ.\nReferences\nAljarrah, A. A. and Ali, A. H. (2019). Human activity recognition using pca and bilstm recurrent\nneural networks.\nIn 2019 2nd International Conference on Engineering Technology and its\nApplications (IICETA), pages 156–160. IEEE.\n125\nAnguita, D., Ghio, A., Oneto, L., Parra Perez, X., and Reyes Ortiz, J. L. (2013).\nA public\ndomain dataset for human activity recognition using smartphones. In Proceedings of the 21th\ninternational European symposium on artificial neural networks, computational intelligence and\nmachine learning, pages 437–442.\nAudibert, J.-Y. and Tsybakov, A. B. (2007). Fast learning rates for plug-in classifiers. The Annals\nof Statistics, 35(2):608–633.\nAzizyan, M., Singh, A., and Wasserman, L. (2013). Minimax theory for high-dimensional gaussian\nmixtures with sparse mean separation. Advances in Neural Information Processing Systems,\n26.\nBalakrishnan, S., Wainwright, M. J., and Yu, B. (2017). Statistical guarantees for the em algo-\nrithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77–120.\nBoucheron, S., Lugosi, G., and Massart, P. (2013). Concentration inequalities: A nonasymptotic\ntheory of independence. Oxford university press.\nCai, T. T., Ma, J., and Zhang, L. (2019). Chime: Clustering of high-dimensional gaussian mixtures\nwith em algorithm and its optimality. The Annals of Statistics, 47(3):1234–1267.\nCai, T. T. and Zhou, H. H. (2012). Optimal rates of convergence for sparse covariance matrix\nestimation. The Annals of Statistics, 40(5):2389–2420.\nChen, D.-R. and Sun, T. (2006). Consistency of multiclass empirical risk minimization methods\nbased on convex loss. The Journal of Machine Learning Research, 7:2435–2447.\nChen, M., Gao, C., and Ren, Z. (2018). Robust covariance and scatter matrix estimation under\nhuber’s contamination model. The Annals of Statistics, 46(5):1932–1960.\nDuan, Y. and Wang, K. (2023). Adaptive and robust multi-task learning. The Annals of Statistics,\n51(5):2015–2039.\nLi, S., Cai, T. T., and Li, H. (2021). Transfer learning for high-dimensional linear regression:\nPrediction, estimation and minimax optimality. Journal of the Royal Statistical Society: Series\nB (Statistical Methodology), pages 1–25.\nMaurer, A. (2016). A vector-contraction inequality for rademacher complexities. In Algorithmic\nLearning Theory: 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016,\nProceedings 27, pages 3–17. Springer.\nMaurer, A. and Pontil, M. (2021).\nConcentration inequalities under sub-gaussian and sub-\nexponential conditions. Advances in Neural Information Processing Systems, 34:7588–7597.\nTian, Y. and Feng, Y. (2023). Transfer learning under high-dimensional generalized linear models.\nJournal of the American Statistical Association, 118(544):2684–2697.\nTsybakov, A. B. (2009). Introduction to nonparametric estimation. Springer, New York.\nVigogna, S., Meanti, G., De Vito, E., and Rosasco, L. (2022). Multiclass learning with margin:\nexponential rates with no bias-variance trade-off.\nIn International Conference on Machine\nLearning, pages 22260–22269. PMLR.\nWainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint, volume 48.\nCambridge University Press.\nWalse, K. H., Dharaskar, R. V., and Thakare, V. M. (2016). Pca based optimal ann classifiers\nfor human activity recognition using mobile sensors data. In Proceedings of First International\nConference on Information and Communication Technology for Intelligent Systems: Volume 1,\npages 429–436. Springer.\n126\nYan, B., Yin, M., and Sarkar, P. (2017). Convergence of gradient em on multi-component mixture\nof gaussians. Advances in Neural Information Processing Systems, 30.\nZeng, M., Nguyen, L. T., Yu, B., Mengshoel, O. J., Zhu, J., Wu, P., and Zhang, J. (2014).\nConvolutional neural networks for human activity recognition using mobile sensors.\nIn 6th\ninternational conference on mobile computing, applications and services, pages 197–205. IEEE.\nZhao, R., Li, Y., and Sun, Y. (2020). Statistical convergence of the em algorithm on gaussian\nmixture models. Electronic Journal of Statistics, 14:632–660.\n127\n",
  "categories": [
    "stat.ML",
    "cs.LG",
    "math.ST",
    "stat.ME",
    "stat.TH"
  ],
  "published": "2022-09-30",
  "updated": "2024-08-02"
}