{
  "id": "http://arxiv.org/abs/2209.15236v3",
  "title": "Language-Family Adapters for Low-Resource Multilingual Neural Machine Translation",
  "authors": [
    "Alexandra Chronopoulou",
    "Dario Stojanovski",
    "Alexander Fraser"
  ],
  "abstract": "Large multilingual models trained with self-supervision achieve\nstate-of-the-art results in a wide range of natural language processing tasks.\nSelf-supervised pretrained models are often fine-tuned on parallel data from\none or multiple language pairs for machine translation. Multilingual\nfine-tuning improves performance on low-resource languages but requires\nmodifying the entire model and can be prohibitively expensive. Training a new\nadapter on each language pair or training a single adapter on all language\npairs without updating the pretrained model has been proposed as a\nparameter-efficient alternative. However, the former does not permit any\nsharing between languages, while the latter shares parameters for all languages\nand is susceptible to negative interference. In this paper, we propose training\nlanguage-family adapters on top of mBART-50 to facilitate cross-lingual\ntransfer. Our approach outperforms related baselines, yielding higher\ntranslation scores on average when translating from English to 17 different\nlow-resource languages. We also show that language-family adapters provide an\neffective method to translate to languages unseen during pretraining.",
  "text": "Language-Family Adapters for Low-Resource\nMultilingual Neural Machine Translation\nAlexandra Chronopoulou△\nDario Stojanovski†▽\nAlexander Fraser△\n△Center for Information and Language Processing, LMU Munich, Germany\n△Munich Center for Machine Learning, Germany\n▽Microsoft, Belgrade, Serbia\nachron@cis.lmu.de\ndstojanovski@microsoft.com, fraser@cis.lmu.de\nAbstract\nLarge multilingual models trained with self-\nsupervision achieve state-of-the-art results in\na wide range of natural language processing\ntasks. Self-supervised pretrained models are\noften ﬁne-tuned on parallel data from one or\nmultiple language pairs for machine transla-\ntion. Multilingual ﬁne-tuning improves perfor-\nmance on low-resource languages but requires\nmodifying the entire model and can be pro-\nhibitively expensive. Training a new adapter\non each language pair or training a single\nadapter on all language pairs without updating\nthe pretrained model has been proposed as a\nparameter-efﬁcient alternative. However, the\nformer does not permit any sharing between\nlanguages, while the latter shares parameters\nfor all languages and is susceptible to nega-\ntive interference.\nIn this paper, we propose\ntraining language-family adapters on top of\nmBART-50 to facilitate cross-lingual transfer.\nOur approach outperforms related baselines,\nyielding higher translation scores on average\nwhen translating from English to 17 different\nlow-resource languages.\nWe also show that\nlanguage-family adapters provide an effective\nmethod to translate to languages unseen during\npretraining.\n1\nIntroduction\nRecent work in multilingual natural language pro-\ncessing (NLP) has created models that reach com-\npetitive performance, while incorporating many\nlanguages into a single architecture (Devlin et al.,\n2019; Conneau et al., 2020). Because of its abil-\nity to share cross-lingual representations, which\nlargely beneﬁts lower-resource languages, multi-\nlingual neural machine translation (NMT) is an\nattractive research ﬁeld (Firat et al., 2016; Zoph\net al., 2016; Johnson et al., 2017; Ha et al., 2016;\nZhang et al., 2020; Fan et al., 2021). Multilingual\nmodels are also appealing because they are more ef-\nﬁcient in terms of the number of model parameters,\n†Work done prior to joining Microsoft\nenabling simple deployment (Arivazhagan et al.,\n2019; Aharoni et al., 2019). Massively multilin-\ngual pretrained models can be used for multilingual\nNMT, if they are ﬁne-tuned in a many-to-one (to\nmap any of the source languages into a target lan-\nguage, which is usually English) or one-to-many (to\ntranslate a single source language into multiple tar-\nget languages) fashion (Aharoni et al., 2019; Tang\net al., 2020). Training a many-to-many (multiple\nsource to multiple target languages) NMT model\n(Fan et al., 2021) has also been proposed.\nMultilingual pretrained models generally permit\nimproving translation on low-resource language\npairs. Specializing the model to a speciﬁc lan-\nguage pair further boosts performance, but is com-\nputationally expensive. For example, mBART-50\n(Tang et al., 2020), a model pretrained on mono-\nlingual data of 50 languages using denoising auto-\nencoding with the BART objective (Lewis et al.,\n2020) still has to be fully ﬁne-tuned for NMT.\nTo avoid ﬁne-tuning large models, previous\nwork has focused on efﬁciently building multi-\nlingual NMT models. Adapters (Rebufﬁet al.,\n2017; Houlsby et al., 2019), which are lightweight\nfeedforward layers added in each Transformer\n(Vaswani et al., 2017) layer, have been proposed\nas a parameter-efﬁcient ﬁne-tuning method. In ma-\nchine translation, training a different adapter on\neach language pair on top of a frozen pretrained\nmultilingual NMT model, has shown to improve\nresults for high-resource languages (Bapna and Fi-\nrat, 2019). Low-resource languages do not beneﬁt\nfrom this approach though, as adapters are trained\nwith limited data. In a similar vein, Cooper Stick-\nland et al. (2021) ﬁne-tune a pretrained model for\nmultilingual NMT using a single set of adapters,\ntrained on all languages. Their approach manages\nto narrow the gap but still does not perform on par\nwith multilingual ﬁne-tuning.\nMany-to-one and one-to-many NMT force lan-\nguages into a joint space (in the encoder or decoder\narXiv:2209.15236v3  [cs.CL]  29 Mar 2023\nside) and neglect diversity. One-to-many NMT\nfaces the difﬁculty of learning a conditional lan-\nguage model and decoding into multiple languages\n(Arivazhagan et al., 2019; Tang et al., 2020). To bet-\nter model target languages, recent approaches pro-\npose exploiting both the unique and the shared fea-\ntures (Wang et al., 2018), reorganizing parameter-\nsharing (Sachan and Neubig, 2018), decoupling\nmultilingual word encodings (Wang et al., 2019a),\ntraining NMT models from scratch after creating\ngroups of languages (Tan et al., 2019), or inserting\nlanguage-speciﬁc layers (Fan et al., 2021).\nIn this work, we propose using language-family\nadapters that enable efﬁcient low-resource mul-\ntilingual NMT. We train adapters for NMT on\ntop of mBART-50 (Tang et al., 2020).\nThe\nadapters are trained using bi-text from each lan-\nguage family, while the pretrained model is not\nupdated. Groups of languages are formed based\non linguistic knowledge bases. Our approach im-\nproves positive cross-lingual transfer, compared to\nlanguage-pair adapters (Bapna and Firat, 2019),\nwhich do not leverage cross-lingual information be-\ntween languages, and language-agnostic adapters\n(Cooper Stickland et al., 2021), which are trained\non all languages and can suffer from negative in-\nterference (Wang et al., 2020). Our approach not\nonly yields better translation scores in the majority\nof languages examined, but also requires less than\n20% of trainable parameters compared to language-\npair adapters, i.e., the most competitive baseline.\nOur main contributions are:\n1. A novel, effective approach for low-resource\nmultilingual translation which trains adapters\non top of mBART-50 for each language fam-\nily. In the English-to-many setting which we\nexamine, language-family adapters achieve\na +1 BLEU improvement over language-\npair adapters and +2.7 BLEU improvement\nover language-agnostic adapters on 16 low-\nresource language pairs from OPUS-100.\n2. We\npropose\ninserting\nembedding-layer\nadapters into the Transformer to encode\nlexical information and conduct an ablation\nstudy to assess their utility.\n3. We contrast grouping languages based on lin-\nguistic knowledge to grouping them based\non the representations of a multilingual pre-\ntrained language model (PLM) with a Gaus-\nsian Mixture Model (GMM).\n4. We analyze the effect of our approach when\nevaluating on languages that are new to\nmBART-50.\n2\nBackground\nMassively Multilingual Models.\nMultilingual\nmasked language models have pushed the state-\nof-the-art on cross-lingual language understanding\nby training a single model for many languages (De-\nvlin et al., 2019; Conneau and Lample, 2019; Con-\nneau et al., 2020). Encoder-decoder Transformer\n(Vaswani et al., 2017) models that are pretrained us-\ning monolingual corpora from multiple languages,\nsuch as mBART (Liu et al., 2020), outperform\nstrong baselines in medium- and low-resource\nNMT. mBART-50 (Tang et al., 2020) is an exten-\nsion of mBART, pretrained in 50 languages and\nmultilingually ﬁne-tuned for NMT. However, while\nmultilingual NMT models are known to outperform\nstrong baselines and simplify model deployment,\nthey are susceptible to negative interference/trans-\nfer (McCann et al., 2018; Arivazhagan et al., 2019;\nWang et al., 2019b; Conneau et al., 2020) and catas-\ntrophic forgetting (Goodfellow et al., 2014) when\nthe parameters are shared across a large number of\nlanguages. Negative transfer affects the translation\nquality of high-resource (Conneau et al., 2020), but\nalso low-resource languages (Wang et al., 2020).\nAs a remedy, providing extra capacity to a multi-\nlingual model using language-speciﬁc modules has\nbeen proposed (Sachan and Neubig, 2018; Wang\net al., 2019a; Fan et al., 2021; Pfeiffer et al., 2022).\nWe take a step forward in this direction and train\nlanguage-family adapters on top of a pretrained\nmodel. Our approach introduces modular compo-\nnents which leverage the similarities of languages\nand can better decode into multiple directions, im-\nproving results compared to baselines.\nAdapters for NMT. Swietojanski and Renals\n(2014) and Vilar (2018) initially suggested learning\nadditional weights that rescale the hidden units for\ndomain adaptation. Adapter layers (Rebufﬁet al.,\n2017; Houlsby et al., 2019) are small modules that\nare typically added to a pretrained Transformer and\nare ﬁne-tuned on a downstream task, while the pre-\ntrained model is frozen. Bapna and Firat (2019)\nadd language-pair adapters to a pretrained mul-\ntilingual NMT model (one set for each language\npair), to recover performance for high-resource lan-\nguage pairs. Cooper Stickland et al. (2021) start\nfrom an unsupervised pretrained model and train\nlanguage-agnostic adapters (one set for all lan-\nguage pairs) for multilingual NMT. Philip et al.\n(2020) train monolingual adapters for zero-shot\ntranslation, while Üstün et al. (2021) propose de-\nnoising adapters, i.e., adapters trained using mono-\nlingual data, for unsupervised multilingual NMT.\nBaziotis et al. (2022) inject language-speciﬁc pa-\nrameters in MNMT using adapters, by generating\nthem from a hyper-network, while Lai et al. (2022)\nadapt a model for both a new domain and a new lan-\nguage pair at the same time by combining domain\nand language representations using meta-learning\nwith adapters.\nWe identify some challenges in previous works\n(Bapna and Firat, 2019; Cooper Stickland et al.,\n2021). Scaling language-agnostic adapters to a\nlarge number of languages is problematic, as when\nthey are updated with data from multiple languages,\nnegative transfer occurs. In contrast, language-pair\nadapters do not face this problem, but at the same\ntime do not allow any sharing between languages,\ntherefore provide poor translation to low-resource\nlanguage pairs. Language-family adapters arguably\nstrike a balance, providing a trade-off between the\ntwo approaches, and our experiments show that\nthey lead to higher translation quality.\nLanguage Families.\nExtensive work on cross-\nlingual transfer has demonstrated that jointly train-\ning a model using similar languages can improve\nlow-resource results in several NLP tasks, such\nas part-of-speech or morphological tagging (Täck-\nström et al., 2013; Straka et al., 2019), entity link-\ning (Tsai and Roth, 2016; Rijhwani et al., 2019),\nand machine translation (Zoph et al., 2016; John-\nson et al., 2017; Neubig and Hu, 2018; Oncevay\net al., 2020). Linguistic knowledge bases (Littell\net al., 2017; Dryer and Haspelmath, 2013) study\nlanguage variation and can provide insights to phe-\nnomena such as negative interference. Languages\ncan be organized together using linguistic informa-\ntion, forming language families. Tan et al. (2019)\nand Kong et al. (2021) leverage families for mul-\ntilingual NMT, the former by training language-\nfamily NMT models from scratch, the latter by\ntraining a separate shallow decoder for each fam-\nily. Instead, our approach keeps a pretrained model\nfrozen and only trains language-family adapters,\nwhich is parameter-efﬁcient. Compared to ﬁne-\ntuning the entire model (ML-FT), our approach re-\nquires less than 12.5% of the trainable parameters,\nas is shown in Table 3.\nSelf-Attention\nAdapter Layer\nFeed Forward\nSelf-Attention\nEncoder-decoder Attention\nFeed Forward\nAdapter Layer\nEmbed Adapter Layer\nEmbed Adapter Layer\nInput Embedding\nOutput Embedding\nOutput Sentence\nSource Sentence\nPositional \nEncoding\nPositional \nEncoding\nNx\nNx\nFigure 1: Proposed adapter architecture inside a Trans-\nformer model.\nAdapter layers, shown in green, are\ntrained for NMT. Figure best viewed in color.\n3\nLanguage-Family Adapters for\nLow-Resource NMT\nFine-tuning a pretrained model for multilingual\nNMT provides a competitive performance, yet is\ncomputationally expensive, as all layers of the\nmodel need to be updated. A parameter-efﬁcient\nalternative is to ﬁne-tune a pretrained multilingual\nmodel for NMT with data from all languages of\ninterest using adapters while keeping the pretrained\nmodel unchanged. However, as multiple language\nrepresentations are encoded in the same parameters,\ncapacity issues arise. Languages are also grouped\ntogether, even though they might be different in\nterms of geographic location, script, syntax, typol-\nogy, etc. As a result, linguistic diversity is not mod-\neled adequately and translation quality degrades.\nWe address the limitations of previous methods\nby proposing language-family adapters for low-\nresource multilingual NMT. An illustration of our\napproach is depicted in Figure 1. We exploit lin-\nguistic knowledge to selectively share parameters\nbetween related languages and avoid negative in-\nterference. Our approach is to train adapters using\nlanguage pairs of a linguistic family on top of a\npretrained model, which is not updated.\n3.1\nAdapter Architecture\nAdapters are usually added to each Transformer\nlayer. An adapter uses as input the output of the\nprevious layer. Formally: Let zi be the output\nof the i-th layer, of dimension h.\nWe apply a\nlayer-normalization (Ba et al., 2016), followed by\na down-projection D ∈Rh×d, a ReLU activation\nand an up-projection U ∈Rd×h, where d is the\nbottleneck dimension and the only tunable hyper-\nparameter. The up-projection is combined with\na residual connection (He et al., 2016) with zi ac-\ncording to the following equation: Adapteri(zi) =\nU ReLU(D LN(zi)) + zi. This follows Bapna and\nFirat (2019). Adapters are randomly initialized.\n3.2\nEmbedding-layer Adapter\nBecause we keep the token embeddings of mBART-\n50 frozen, adding ﬂexibility to the model to encode\nlexical information of the languages of interest is\ncrucial, especially for unseen languages (not part\nof its pretraining corpus). Lexical cross-lingual\ninformation could be encoded by learning new em-\nbeddings for the unseen languages (Artetxe et al.,\n2020) but this would be computationally expen-\nsive. We instead add an adapter after the embed-\nding layer, in both the encoder and the decoder,\nwhich receives as input the lexical representation\nof each sequence and aims to capture token-level\ncross-lingual transformations.\nOur approach draws inspiration from Pfeiffer\net al. (2020) and simpliﬁes the invertible adapters\nstructure. We use the large vocabulary of mBART-\n50 to extend the model to unseen languages. We\nnote that adding scripts that do not exist in the\nvocabulary of mBART-50 is not possible with\nour approach. We point out that Chronopoulou\net al. (2020); Pfeiffer et al. (2021); Vernikos and\nPopescu-Belis (2021) have proposed approaches\nto permit ﬁne-tuning to unseen languages/scripts\nwhen using PLMs and we leave further exploration\nto future work.\n3.3\nModel Architecture\nTo train a model for multilingual NMT, we lever-\nage mBART-50, a sequence-to-sequence generative\nmodel pretrained on monolingual data from 50 lan-\nguages using a denoising auto-encoding objective.\nThe model has essentially been trained by trying to\npredict the original text X, given g(X), where g is\na noising function that corrupts text.\nWe want to ﬁne-tune this model on a variety of\nlanguage pairs, by leveraging similarities between\nlanguages. Our model aims to provide a parameter-\nefﬁcient alternative to traditional ﬁne-tuning of the\nentire pretrained model. We note that the pretrained\nmBART-50 model cannot be used as is for MT, as\nit has never been trained on the task.\nTo this end, we insert adapters after each feed-\nforward layer both in the encoder and in the de-\ncoder and we also add embedding-layer adapters.\nLanguage (code)\nFamily\nTrain Set\nTED\nOPUS-100\n⋆Bulgarian (bg)\nBS\n174k\n1M\nPersian (fa)\nI\n151k\n1M\n⋆Serbian (sr)\nBS\n137k\n1M\nCroatian (hr)\nBS\n122k\n1M\nUkrainian (uk)\nBS\n108k\n1M\nIndonesian (id)\nA\n87k\n1M\n⋆Slovak (sk)\nBS\n61k\n1M\nMacedonian (mk)\nBS\n25k\n1M\nSlovenian (sl)\nBS\n20k\n1M\nHindi (hi)\nI\n19k\n534k\nMarathi (mr)\nI\n10k\n27k\n⋆Kurdish (ku)\nI\n10k\n45k\n⋆Bosnian (bs)\nBS\n6k\n1M\n⋆Malay (ms)\nA\n5k\n1M\nBengali (bn)\nI\n5k\n1M\n⋆Belarusian (be)\nBS\n5k\n67k\n⋆Filipino (ﬁl)\nA\n3k\n-\nTable 1: Languages used in the experiments. ⋆indi-\ncates languages that are unseen from mBART-50, i.e.,\nthey do not belong to the pretraining corpus. BS stands\nfor Balto-Slavic, I for Indo-Iranian, A for Austronesian.\nWe freeze the pretrained encoder-decoder Trans-\nformer and ﬁne-tune only the adapters on NMT. We\nleverage the knowledge of the pretrained model,\nbut encode additional cross-lingual information on\neach language family using adapters. We ﬁne-tune\na new set of adapters multilingually on each lan-\nguage family and evaluate the performance on and\nlow-resource language pairs.\n4\nExperimental Setup\nData. We initially ﬁne-tune the model on TED\ntalks (Qi et al., 2018), using data from 17 languages\npaired to English. We then scale to a larger paral-\nlel dataset, using OPUS-100 (Zhang et al., 2020)\nfor the same languages paired to English (with\nthe only exception being English-Filipino, which\ndoes not appear in OPUS-100). For the TED ex-\nperiments, we choose 17 languages, 9 of which\nwere present during pretraining, while 8 are new\nto mBART-50. For OPUS-100, we use the same\n16 languages (without Filipino), 9 of which were\npresent during pretraining and 7 are new. In both\nsets of experiments, the languages belong to 3 lan-\nguage families, namely Balto-Slavic, Austronesian\nand Indo-Iranian. Balto-Slavic and Indo-Iranian\nare actually distinct branches of the same language\nfamily (Indo-European). The parallel data details\nare reported in Table 1.\nBaselines. We compare the proposed language-\nfamily\nadapters\nwith\n1)\nlanguage-agnostic\n(LANG-AGNOSTIC)\nand\n2)\nlanguage-pair\nadapters (LANG-PAIR). While the adapters are\ntrained using parallel data, mBART-50 (pretrained\non monolingual data) is not updated. Moreover, we\ncompare our approach to multilingual ﬁne-tuning\n(ML-FT), although it requires ﬁne-tuning the entire\nmodel and is thus not directly comparable to the\nparameter-efﬁcient approaches we study. We show\nthis result in the Appendix.\nThe ﬁrst baseline, LANG-AGNOSTIC adapters,\nﬁne-tunes a set of adapters using data from all lan-\nguages (similar to Cooper Stickland et al., 2021).\nThe second baseline, LANG-PAIR adapters, fol-\nlows Bapna and Firat (2019): a new set of adapters\nis trained for each language pair, so no parameters\nare shared between different language pairs.\nTraining details. We start from the mBART-50\ncheckpoint.* We extend its embedding layer with\nrandomly initialized vectors to account for the new\nlanguages. We reuse the 250k sentencepiece (Kudo\nand Richardson, 2018) model of mBART-50. We\nuse the fairseq (Ott et al., 2019) library for all ex-\nperiments. We select the ﬁnal models using valida-\ntion perplexity. If the model is trained on multiple\nlanguages (using mixed mini-batches), we use the\noverall perplexity. We use beam search with size\n5 for decoding and evaluate BLEU scores using\nSacreBLEU† for OPUS-100 and SacreBLEU with-\nout tokenization for TED (Post, 2018). We also\ncompute COMET (Rei et al., 2020) scores using\nthe wmt-large-da-estimator-1719 pretrained model.\nResults are reported in the Appendix.\nTo train the models, we freeze mBART-50. We\nﬁne-tune the LANG-FAMILY, LANG-AGNOSTIC\nadapters in a multilingual, one-to-many setup, us-\ning English as the source language. LANG-PAIR\nadapters are ﬁne-tuned for each language pair. All\nmodels have a bottleneck dimension of 512. We\notherwise use the same hyperparameters as Tang\net al. (2020) and report them in the Appendix.\n5\nResults and Discussion\n5.1\nMain results\nTable 2 shows translation results for a subset of lan-\nguages of OPUS-100 and TED in terms of BLEU\nusing parallel data to ﬁne-tune mBART-50 in the\nen →xx direction. We also report COMET scores\n*https://dl.fbaipublicfiles.com/\nfairseq/models/mbart50/mbart50.\npretrained.tar.gz\n†Signature “BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.5.1”\nin the Appendix.\nOur approach (LANG-FAMILY) consistently im-\nproves results on the OPUS-100 dataset, with an\naverage +1 BLEU performance boost across all lan-\nguages compared to ﬁne-tuning with LANG-PAIR\nadapters and +2.7 improvement compared to\nLANG-AGNOSTIC adapters. We believe that this\nshows that representations from similar languages\nare beneﬁcial to a multilingual model in a low-\nresource setup. However, training a single adapter\nover all languages (LANG-AGNOSTIC) is detri-\nmental in terms of translation quality. Moreover,\nLANG-PAIR trains a different adapter on each lan-\nguage pair and does not permit sharing cross-\nlingual information. As a result, it obtains worse\nresults compared to our approach; it is also signiﬁ-\ncantly more computationally expensive, requiring\n5× parameters of LANG-FAMILY adapters.\nOur approach similarly outperforms both base-\nlines on TED. It yields a +1.5 improvement com-\npared to LANG-AGNOSTIC and +0.4 BLEU com-\npared to LANG-PAIR. These results conﬁrm our\nmain ﬁnding, which is that selectively sharing pa-\nrameters of related languages with adapters is use-\nful for low-resource NMT.\n5.2\nComputational cost\nWe show in Table 3 the number of trainable param-\neters used for each approach. We note that our ex-\nperiments were conducted using 8 NVIDIA-V100\nGPUs. The mBART-50 model has 680M parame-\nters. Our approach trains parameters that add up\nto just 11.9% of the full model. LANG-AGNOSTIC\nis the most efﬁcient approach, requiring just 8.4%\ntrainable parameters. However, there is a cost in\nterms of performance compared to our model. Fi-\nnally, training LANG-PAIR adapters is relatively\nexpensive (52.2% of the trainable parameters of\nmBART-50). All in all, our LANG-FAMILY ap-\nproach provides a trade-off between performance\nand efﬁciency in terms of model parameters and is\nan effective method of adapting pretrained multi-\nlingual models to low-resource languages.\n5.3\nEmbedding-layer adapter\nOur approach keeps the encoder and decoder em-\nbeddings frozen during ﬁne-tuning. Because of\nthat, the lexical representations of the model are\nnot updated to model the languages of interest. To\novercome this issue, we introduce an adapter af-\nter the encoder embedding layer, as well as after\nthe decoder embedding layer. We do not tie these\nModel\nBALTO-\nAUSTRO-\nINDO-\nSLAVIC\nNESIAN\nIRANIAN\nbg⋆\nsr⋆\nhr\nuk\nsk⋆\nmk\nsl\nbs⋆\nbe⋆\nid\nms⋆\nﬁl⋆\nfa\nhi\nmr\nku⋆\nbn\nAVG\nOPUS-100\nLang-pair\n27.8\n17.5\n23.7\n17.7\n25.0\n35.0\n24.1\n21.0\n10.1\n28.0\n24.5\n-\n10.5\n15.6\n17.0\n14.1\n13.0\n20.3\nLang-agnostic\n21.6\n19.7\n21.4\n13.8\n24.1\n28.9\n19.6\n19.5\n11.3\n28.6\n21.8\n-\n8.1\n16.9\n17.8\n12.8\n11.2\n18.6\nLang-family\n25.4\n20.9\n23.7\n15.1\n27.7\n31.9\n22.6\n20.3\n15.2\n31.3\n25.4\n-\n9.8\n18.7\n25.0\n15.3\n12.9\n21.3\nTED\nLang-pair\n35.7\n21.1\n30.5\n21.1\n24.2\n27.0\n21.4\n28.6\n12.5\n35.4\n23.4\n12.2\n14.0\n14.1\n10.0\n4.9\n9.0\n20.3\nLang-agnostic\n31.7\n24.0\n29.7\n21.9\n20.6\n26.5\n20.2\n27.8\n7.7\n33.8\n22.1\n11.6\n17.0\n15.5\n7.0\n3.3\n6.0\n19.2\nLang-family\n33.8\n25.1\n30.5\n22.2\n22.8\n28.0\n21.5\n27.8\n9.5\n34.7\n22.0\n11.5\n17.5\n19.8\n10.3\n4.1\n11.6\n20.7\nTable 2: Test set BLEU scores when translating out of English (en →xx) on OPUS-100 and TED. LANG-PAIR\nstands for language-pair, LANG-AGNOSTIC for language-agnostic, and LANG-FAMILY for language-family\nadapters. Languages denoted with ⋆are new to mBART-50. Results in bold are signiﬁcantly different (p < 0.01)\nfrom the best adapter baseline.\nParameters\nRuntime\nGPUs\nLANG-AGNOSTIC\n27M\n35h\n8\nLANG-FAMILY\n81M\n78h\n8\nLANG-PAIR\n432M\n192h\n8\nML-FT\n680M\n310h\n8\nTable 3: Parameters used by our approach and the base-\nlines to train on OPUS-100. We note that the GPUs\nused are NVIDIA-V100. For completeness, we also in-\nclude the parameters used for multilingual ﬁne-tuning\n(ML-FT) of the pre-trained model.\nadapter layers, since they only add up a small num-\nber of parameters (1M each, i.e., 0.1% of mBART-\n50 parameters).\nAs we can see in Table 4, we get consis-\ntent gains across almost all language pairs by\nadding these adapters, for both our model and the\nLANG-AGNOSTIC baseline. The former yields a\n+0.5 performance boost, while the latter a +0.7\nimprovement in terms of BLEU. While the gains\nare modest, they are consistent and come at a\nvery small computational overhead. For some lan-\nguages, such as Kurdish (which is an unseen lan-\nguage for mBART-50), results improve by +1.6\nwhen using embedding-layer adapters. Since Kur-\ndish is not part of mBART-50 pretraining corpus,\nencoding token-level representations is in this case\nmore challenging and embedding-layer adapters\nallows the model to specialize in this language.\n5.4\nAutomatic clustering of languages\nGaussian Mixture Model. For our main set of ex-\nperiments, we used language families from WALS.\nHowever, it might be that not all languages within\na language family share the same linguistic prop-\nerties (Ahmad et al., 2019). Therefore, we wanted\nto explore a data-driven approach to induce simi-\nlarities between languages. To this end, we group\nlanguages together using Gaussian Mixture Model\n(GMM) clustering of text representations obtained\nfrom a PLM (Aharoni and Goldberg, 2020). We\nused released code by the authors of the paper.‡\nWe use XLM-R (Conneau et al., 2020), a multi-\nlingual PLM and speciﬁcally the xlmr-roberta-base\nHuggingFace (Wolf et al., 2020) checkpoint. We\nencode 500 sequences of 512 tokens from each\nlanguage (using OPUS-100) to create sentence rep-\nresentations, by performing average pooling of the\nlast hidden state. We then use PCA projection of\ndimension 100 and ﬁt the sentence representations\nto a GMM with 3 components (3 Gaussian distri-\nbutions, i.e., clusters). As this is a soft assignment,\nevery language belongs with some probability to\none or more clusters. For simplicity, we map each\nlanguage to just one cluster based on where the\nmajority of its samples are assigned to.\nResults. Table 5 shows an evaluation of our ap-\nproach, where we select the language family based\non linguistic similarities (ling. family, ﬁrst row),\nGMM clustering (second row), and random sam-\npling (third row).\nThe main observation is that training adapters\nusing language groups computed by GMM clus-\ntering yields worse translation scores compared to\nlanguage groups based on linguistic similarities\n(ling. family). We believe that this is the case be-\ncause some languages were clustered together with\nlinguistically distant languages (e.g., Belarusian\nis assigned to the same group as Persian, Hindi,\nMarathi, and Bengali according to GMM cluster-\ning). This might be because of a domain mismatch\nbetween the English-Belarusian parallel dataset and\nthe datasets of the rest of the languages in the group.\nBased on our experiments, training adapters on lin-\n‡https://github.com/roeeaharoni/\nunsupervised-domain-clusters\nBALTO-\nAUSTRO-\nINDO-\nSLAVIC\nNESIAN\nIRANIAN\nbg\nhr\nmk\nbe\nid\nms\nfa\nku\nbn\nAVG-16\nLANG-AGNOSTIC w/o emb adapter\n21.3\n21.5\n28.3\n10.5\n28.7\n21.5\n7.6\n12.4\n10.9\n18.1\nLANG-AGNOSTIC with emb adapter (BASELINE)\n21.6\n21.4\n28.9\n11.3\n28.6\n21.8\n8.1\n12.8\n11.2\n18.6\nLANG-FAMILY w/o emb adapter\n24.3\n22.6\n31.2\n13.4\n31.4\n25.2\n9.0\n13.7\n12.2\n20.6\nLANG-FAMILY with emb adapter (OURS)\n25.4\n23.7\n31.9\n15.2\n31.3\n25.4\n9.8\n15.3\n12.9\n21.3\nTable 4: Ablation of the proposed architecture for en →xx (BLEU scores) on OPUS-100. We present results only\nfor a subset of languages per language family. Full results can be found in the Appendix.\nLanguage Groups\nid\nfa\nku\nAVG\nling. family (ours)\n<be, bg, sr, hr, uk, sk, mk, sl, bs>\n<id, ms>\n<ku, fa, hi, mr, bn>\n31.3\n9.8\n15.3\n21.3\nGMM\n<bg, sr, hr, uk, sk, mk, sl, bs>\n<ku, id, ms>\n<be, fa, hi, mr, bn>\n29.7\n9.2\n14.3\n19.4\nrandom\n<bg, hr, mk, bs, be, ms, hi, mr, ku>\n<sl, id>\n<sr, uk, sk, fa, bn>\n27.8\n7.0\n15.0\n18.4\nTable 5: Evaluation of different methods to form language families for en →xx on OPUS-100. We present results\nonly for a subset of languages and the overall average BLEU scores. Full results are shown in the Appendix.\nguistic families provides better translation scores\nand should therefore be preferred, if these exist. As\nexpected, randomly clustering languages together\nperforms worse than all approaches, showing that\ntaking into account similarities between languages\nis beneﬁcial when training a multilingual model for\nlow-resource NMT.\n6\nAnalysis\n6.1\nPerformance according to language\nfamily\nTo evaluate the contribution of grouping languages\nbased on linguistic information, we present the\nBLEU scores of the LANG-FAMILY adapters com-\npared to the baselines per language family. We\nshow the results in Figure 2.\nCompared to the LANG-AGNOSTIC baseline,\nLANG-FAMILY adapters perform better in all lan-\nguage families. On Balto-Slavic, our approach is\non par with LANG-PAIR adapters (<0.5 BLEU dif-\nference). On both Austronesian and Indo-Iranian,\nour approach largely outperforms (more than +2\nBLEU) both baselines. This is arguably the case\nbecause LANG-AGNOSTIC adapters, trained using\nparallel data from all languages, group dissimilar\nlanguages together and do not take into account\nlanguage variation. We instead train adapters on\nlanguages with common linguistic properties and\nobtain consistently improved translations.\nLANG-AGNOSTIC adapters perform worse than\nLANG-PAIR adapters on all language families.\nThis is mostly evident for Balto-Slavic. We believe\nthat this happens because Balto-Slavic languages\nare more similar to English compared to Austrone-\nBLEU\n0.0\n10.0\n20.0\n30.0\navg balto-\nslavic\navg \naustronesian\navg indo-\niranian\nlang-pair\nlang-family\nlang-agnostic\nFigure 2: Grouping based on language family using\nOPUS-100. Translation scores (measured with BLEU)\nare shown for the our method (LANG-FAMILY), as well\nas the LANG-PAIR and LANG-AGNOSTIC baselines.\nsian or Indo-Iranian. This means that translating be-\ntween Balto-Slavic and English is relatively easier,\nespecially since mBART-50 has been trained with\na large Indo-European bias and it already encodes\ncross-lingual information for most of the languages\nin this group. As a result, LANG-PAIR adapters\ncreate in this case a very competitive baseline.\n6.2\nPerformance on seen vs unseen languages\nWe also evaluate the performance of language-\nfamily adapters and the baselines on languages that\nare not included in the mBART-50 pretraining data\n(unseen), compared to languages that belong to its\npretraining corpus (seen). We present the results in\nFigure 3.\nOn unseen languages, LANG-FAMILY adapters\nimprove the translation quality compared to the\nLANG-PAIR adapter baseline. As the pretrained\nmodel has no knowledge of these languages,\nBLEU\n0.00\n5.00\n10.00\n15.00\n20.00\n25.00\navg unseen\navg seen\nlang-pair\nlang-family\nlang-agnostic\nFigure 3:\nGrouping based on “seen” (existing in\nthe pretraining corpus), or “unseen” language using\nOPUS-100. BLEU scores are shown for our method\n(LANG-FAMILY) and the baselines.\nLANG-FAMILY adapters provide useful cross-\nlingual signal. This makes our approach suitable\nfor extending an already trained multilingual model\nto new languages in a scalable way. The improve-\nment is, as expected, smaller for the seen lan-\nguages.\nLANG-AGNOSTIC adapters perform signiﬁ-\ncantly worse than both our approach and the\nLANG-PAIR baseline. This might be the case be-\ncause of negative transfer between unrelated lan-\nguages, that are clustered and trained together using\nthe LANG-AGNOSTIC model. This issue is preva-\nlent for both seen and unseen languages.\n7\nConclusion\nWe presented a novel approach for ﬁne-tuning\na pretrained multilingual model for NMT using\nlanguage-family adapters. Our approach can be\nused for low-resource multilingual NMT, com-\nbining the modularity of adapters with effective\ncross-lingual transfer between related languages.\nWe showed that language-family adapters perform\nbetter than both language-agnostic and language-\npair adapters, while being computationally efﬁ-\ncient. Finally, for languages new to mBART-50, we\nshowed that our approach provides an effective way\nof leveraging shared cross-lingual information be-\ntween similar languages, considerably improving\ntranslations compared to the baselines.\nIn the future, a more elaborate approach to en-\ncode lexical-level representations could further\nboost the performance of language-family adapters.\nWe also hypothesize that the effectiveness of our\nmodel could be leveraged for other cross-lingual\ntasks, such as natural language inference, document\nclassiﬁcation and question-answering.\nLimitations\nOur work uses a large seq2seq multilingual pre-\ntrained model, mBART-50. This model has been\npretrained on large chunks of monolingual data\nfrom Common Crawl (Wenzek et al., 2020), but we\ndo not have evaluations of generated text (e.g., on\nﬂuency, factuality, or other common metrics used to\nevaluate generated language). Therefore, this pre-\ntrained model can encode biases that could harm\nmarginalized populations (Bender et al., 2021) and\ncould also be used to translate harmful text.\nAcknowledgements\nThis project has received funding from the Euro-\npean Research Council under the European Union’s\nHorizon 2020 research and innovation program\n(grant agreement #640550) and from DFG (grant\nFR 2829/4-1).\nReferences\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7747–\n7763, Online. Association for Computational Lin-\nguistics.\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages\n3874–3884, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nWasi Ahmad, Zhisong Zhang, Xuezhe Ma, Eduard\nHovy, Kai-Wei Chang, and Nanyun Peng. 2019. On\ndifﬁculties of cross-lingual transfer with order differ-\nences: A case study on dependency parsing. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 2440–2452,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nNaveen Arivazhagan,\nAnkur Bapna,\nOrhan Firat,\nDmitry Lepikhin, Melvin Johnson, Maxim Krikun,\nMia Xu Chen, Yuan Cao, George Foster, Colin\nCherry, Wolfgang Macherey, Zhifeng Chen, and\nYonghui Wu. 2019. Massively multilingual neural\nmachine translation in the wild: Findings and chal-\nlenges.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization.\nAnkur Bapna and Orhan Firat. 2019.\nSimple, scal-\nable adaptation for neural machine translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1538–\n1548, Hong Kong, China. Association for Computa-\ntional Linguistics.\nChristos Baziotis, Mikel Artetxe, James Cross, and\nShruti Bhosale. 2022. Multilingual machine transla-\ntion with hyper-adapters. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021.\nOn the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\nFAccT ’21, page 610–623, New York, NY, USA. As-\nsociation for Computing Machinery.\nAlexandra Chronopoulou,\nDario Stojanovski,\nand\nAlexander Fraser. 2020. Reusing a Pretrained Lan-\nguage Model on Languages with Limited Corpora\nfor Unsupervised NMT. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 2703–2711, On-\nline. Association for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nIn\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems.\nAsa\nCooper\nStickland,\nXian\nLi,\nand\nMarjan\nGhazvininejad.\n2021.\nRecipes\nfor\nadapting\npre-trained monolingual and multilingual models\nto machine translation.\nIn Proceedings of the\n16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main\nVolume, pages 3440–3453, Online. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMatthew S. Dryer and Martin Haspelmath, editors.\n2013. WALS Online. Max Planck Institute for Evo-\nlutionary Anthropology.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Man-\ndeep Baines, Onur Celebi, Guillaume Wenzek,\nVishrav Chaudhary, Naman Goyal, Tom Birch, Vi-\ntaliy Liptchinsky, Sergey Edunov, Edouard Grave,\nMichael Auli, and Armand Joulin. 2021. Beyond\nenglish-centric multilingual machine translation. J.\nMach. Learn. Res., 22(1).\nOrhan Firat, Kyunghyun Cho, and Yoshua Bengio.\n2016. Multi-way, multilingual neural machine trans-\nlation with a shared attention mechanism. In Pro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n866–875, San Diego, California. Association for\nComputational Linguistics.\nIan J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron\nCourville, and Yoshua Bengio. 2014.\nAn empiri-\ncal investigation of catastrophic forgeting in gradi-\nent based neural networks. In Proceedings of Inter-\nnational Conference on Learning Representations\n(ICLR).\nThanh-Le Ha, Jan Niehues, and Alexander H. Waibel.\n2016. Toward multilingual neural machine transla-\ntion with universal encoder and decoder. CoRR.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 770–\n778.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the International Conference on\nMachine Learning, Proceedings of Machine Learn-\ning Research, pages 2790–2799.\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2017. Google’s\nmultilingual neural machine translation system: En-\nabling zero-shot translation. Transactions of the As-\nsociation for Computational Linguistics, 5:339–351.\nXiang Kong, Adithya Renduchintala, James Cross,\nYuqing Tang, Jiatao Gu, and Xian Li. 2021. Mul-\ntilingual neural machine translation with deep en-\ncoder and multiple shallow decoders. In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 1613–1624, Online. Associa-\ntion for Computational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nWen Lai, Alexandra Chronopoulou, and Alexander\nFraser. 2022.\nm^4 adapter:\nMultilingual multi-\ndomain adaptation for machine translation with a\nmeta-adapter.\nIn Findings of the Association for\nComputational Linguistics: EMNLP 2022, pages\n4282–4296, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nMike\nLewis,\nYinhan\nLiu,\nNaman\nGoyal,\nMar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nPatrick Littell, David R. Mortensen, Ke Lin, Kather-\nine Kairis, Carlisle Turner, and Lori Levin. 2017.\nURIEL and lang2vec: Representing languages as\ntypological, geographical, and phylogenetic vectors.\nIn Proceedings of the 15th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Volume 2, Short Papers, pages 8–14,\nValencia, Spain. Association for Computational Lin-\nguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020.\nMultilingual Denoising\nPre-training for Neural Machine Translation. Trans-\nactions of the Association for Computational Lin-\nguistics, 8:726–742.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language de-\ncathlon: Multitask learning as question answering.\nCoRR.\nGraham Neubig and Junjie Hu. 2018. Rapid adapta-\ntion of neural machine translation to new languages.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n875–880, Brussels, Belgium. Association for Com-\nputational Linguistics.\nArturo Oncevay, Barry Haddow, and Alexandra Birch.\n2020. Bridging linguistic typology and multilingual\nmachine translation with multi-view language repre-\nsentations. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2391–2406, Online. Associa-\ntion for Computational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019.\nfairseq: A fast, extensible\ntoolkit for sequence modeling.\nIn Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nJonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James\nCross, Sebastian Riedel, and Mikel Artetxe. 2022.\nLifting the curse of multilinguality by pre-training\nmodular transformers. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3479–3495, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2021. UNKs everywhere: Adapting mul-\ntilingual language models to new scripts.\nIn Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 10186–\n10203, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nJerin Philip, Alexandre Berard, Matthias Gallé, and\nLaurent Besacier. 2020. Monolingual adapters for\nzero-shot neural machine translation.\nIn Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n4465–4470, Online. Association for Computational\nLinguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nYe Qi, Devendra Sachan, Matthieu Felix, Sarguna Pad-\nmanabhan, and Graham Neubig. 2018. When and\nwhy are pre-trained word embeddings useful for neu-\nral machine translation? In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers),\npages 529–535, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea\nVedaldi. 2017.\nLearning multiple visual domains\nwith residual adapters. In Advances in Neural In-\nformation Processing Systems.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2685–2702, Online. Associa-\ntion for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2020.\nMaking\nmonolingual sentence embeddings multilingual us-\ning knowledge distillation.\nIn Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4512–4525.\nShruti Rijhwani, Jiateng Xie, Graham Neubig, and\nJaime Carbonell. 2019.\nZero-shot neural transfer\nfor cross-lingual entity linking. In The AAAI Con-\nference on Artiﬁcial Intelligence.\nDevendra Sachan and Graham Neubig. 2018. Parame-\nter sharing methods for multilingual self-attentional\ntranslation models. In Proceedings of the Third Con-\nference on Machine Translation: Research Papers,\npages 261–271, Brussels, Belgium. Association for\nComputational Linguistics.\nMilan Straka, Jana Straková, and Jan Hajic. 2019. UD-\nPipe at SIGMORPHON 2019: Contextualized em-\nbeddings, regularization with morphological cate-\ngories, corpora merging. In Proceedings of the 16th\nWorkshop on Computational Research in Phonetics,\nPhonology, and Morphology, pages 95–103, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nPawel Swietojanski and Steve Renals. 2014. Learning\nhidden unit contributions for unsupervised speaker\nadaptation of neural network acoustic models.\nIn\n2014 IEEE Spoken Language Technology Workshop\n(SLT), pages 171–176.\nOscar Täckström, Dipanjan Das, Slav Petrov, Ryan Mc-\nDonald, and Joakim Nivre. 2013. Token and type\nconstraints for cross-lingual part-of-speech tagging.\nTransactions of the Association for Computational\nLinguistics, 1:1–12.\nXu Tan, Jiale Chen, Di He, Yingce Xia, Tao Qin, and\nTie-Yan Liu. 2019.\nMultilingual neural machine\ntranslation with language clustering. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 963–973, Hong\nKong, China. Association for Computational Lin-\nguistics.\nY. Tang, C. Tran, Xian Li, Peng-Jen Chen, Naman\nGoyal, Vishrav Chaudhary, Jiatao Gu, and Angela\nFan. 2020.\nMultilingual translation with extensi-\nble multilingual pretraining and ﬁnetuning. ArXiv,\nabs/2008.00401.\nChen-Tse Tsai and Dan Roth. 2016.\nCross-lingual\nwikiﬁcation using multilingual embeddings. In Pro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n589–598, San Diego, California. Association for\nComputational Linguistics.\nAhmet Üstün, Alexandre Berard, Laurent Besacier, and\nMatthias Gallé. 2021.\nMultilingual unsupervised\nneural machine translation with denoising adapters.\nIn Proceedings of the Conference on Empirical\nMethods in Natural Language Processing, pages\n6650–6662.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems.\nGiorgos Vernikos and Andrei Popescu-Belis. 2021.\nSubword mapping and anchoring across languages.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 2633–2647, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nDavid Vilar. 2018.\nLearning hidden unit contribu-\ntion for adapting neural machine translation mod-\nels. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 2 (Short Papers), pages 500–505, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nXinyi Wang, Hieu Pham, Philip Arthur, and Graham\nNeubig. 2019a. Multilingual neural machine transla-\ntion with soft decoupled encoding. In International\nConference on Learning Representations.\nYining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu,\nand Chengqing Zong. 2018. Three strategies to im-\nprove one-to-many multilingual translation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2955–\n2960, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nZirui Wang, Zihang Dai, Barnabas Poczos, and Jaime\nCarbonell. 2019b. Characterizing and avoiding neg-\native transfer. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR).\nZirui Wang, Zachary C. Lipton, and Yulia Tsvetkov.\n2020. On negative interference in multilingual mod-\nels: Findings and a meta-learning treatment.\nIn\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4438–4450, Online. Association for Computa-\ntional Linguistics.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020.\nCCNet:\nExtracting high quality monolingual datasets from\nweb crawl data.\nIn Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nBiao Zhang, Philip Williams, Ivan Titov, and Rico Sen-\nnrich. 2020. Improving massively multilingual neu-\nral machine translation and zero-shot translation. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1628–\n1639, Online. Association for Computational Lin-\nguistics.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016.\nTransfer learning for low-resource\nneural machine translation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1568–1575, Austin,\nTexas. Association for Computational Linguistics.\nA\nAppendix\nA.1\nDataset statistics\nFirst, we show the script and language family (ac-\ncording to linguistic information) of each language\nused in our set of experiments in Table 6. We also\npresent in detail the statistics of all parallel data\nused in our set of experiments in Table 8. We note\nthat the number of train, validation and test set\npresented refers to sentences.\nThe TED dataset can be downloaded from phon-\ntron.com/data/ted_talks.tar.gz while OPUS-100\ncan be downloaded from object.pouta.csc.ﬁ/OPUS-\n100/v1.0/opus-100-corpus-v1.0.tar.gz.\nA.2\nTraining details\nWe train each model for 130k updates with a batch\nsize of 900 tokens per GPU for OPUS-100 and\n1024 tokens per GPU for TED. We use 8 NVIDIA-\nV100 GPUs for OPUS-100 and 2 GPUs for TED\n(much smaller dataset). We evaluate models after\n5k training steps. We use early stopping with a\nLanguage (code)\nFamily\nScript\n⋆Bulgarian (bg)\nBalto-Slavic\nCyrillic\nPersian (fa)\nIndo-Iranian\nArabic\n⋆Serbian (sr)\nBalto-Slavic\nCyrillic\nCroatian (hr)\nBalto-Slavic\nLatin\nUkrainian (uk)\nBalto-Slavic\nCyrillic\nIndonesian (id)\nAustronesian\nLatin\n⋆Slovak (sk)\nBalto-Slavic\nLatin\nMacedonian (mk)\nBalto-Slavic\nCyrillic\nSlovenian (sl)\nBalto-Slavic\nLatin\nHindi (hi)\nIndo-Iranian\nDevanagari\nMarathi (mr)\nIndo-Iranian\nDevanagari\n⋆Kurdish (ku)\nIndo-Iranian\nArabic\n⋆Bosnian (bs)\nBalto-Slavic\nCyrillic\n⋆Malay (ms)\nAustronesian\nLatin\nBengali (bn)\nIndo-Iranian\nBengali\n⋆Belarusian (be)\nBalto-Slavic\nCyrillic\n⋆Filipino (ﬁl)\nAustronesian\nLatin\nTable 6: Languages that are used in the experiments. ⋆\nindicates languages that are unseen from mBART-50,\ni.e., they do not belong to the pretraining corpus. Fil-\nipino is only used in the TED experiments.\nAdapter size\nDropout\nLang-Family\nLang-Agnostic\n128\n0.1\n16.8\n10.1\n128\n0.3\n16.4\n9.5\n256\n0.1\n19.0\n14.9\n256\n0.3\n18.6\n14.0\n512\n0.1\n20.7\n19.2\n512\n0.3\n19.9\n18.5\nTable 7: Hyperparameter tuning for dropout, adapter\nbottleneck size on TED. Average performance (on all\nlanguage pairs using TED) per model.\nWe chose\nthe best-performing combination of dropout and bottle-\nneck size for our experiments.\npatience of 5. To balance high and low-resource\nlanguage pairs, we use temperature-based sampling\n(Arivazhagan et al., 2019) with T = 1.5.\nA.3\nEvaluation of main results using 2\nmetrics\nWe evaluate the translations of our model\n(LANG-FAMILY adapters) and all the baselines\ntrained on OPUS-100 using COMET (Rei et al.,\n2020). COMET leverages progress in cross-lingual\nlanguage modeling, creating a multilingual ma-\nchine translation evaluation model that takes into\naccount both the source input and a reference\ntranslation in the target language.\nWe rely on\nwmt-large-da-estimator-1719. COMET\nscores are not bounded between 0 and 1; higher\nscores signify better translations. Our results are\nsummarized in Table 10. We see that COMET cor-\nLanguage\nSource\nTrain\nValid\nTest\nSource\nTrain\nValid\nTest\nBulgarian (bg)\nTED\n174k\n4082\n5060\nOPUS-100\n1M\n2k\n2k\nPersian (fa)\nTED\n151k\n3930\n4490\nOPUS-100\n1M\n2k\n2k\nSerbian (sr)\nTED\n137k\n3798\n4634\nOPUS-100\n1M\n2k\n2k\nCroatian (hr)\nTED\n122k\n3333\n4881\nOPUS-100\n1M\n2k\n2k\nUkrainian (uk)\nTED\n108k\n3060\n3751\nOPUS-100\n1M\n2k\n2k\nIndonesian (id)\nTED\n87k\n2677\n3179\nOPUS-100\n1M\n2k\n2k\nSlovak (sk)\nTED\n61k\n2271\n2445\nOPUS-100\n1M\n2k\n2k\nMacedonian (mk)\nTED\n25k\n640\n438\nOPUS-100\n1M\n2k\n2k\nSlovenian (sl)\nTED\n20k\n1068\n1251\nOPUS-100\n1M\n2k\n2k\nHindi (hi)\nTED\n19k\n854\n1243\nOPUS-100\n534k\n2k\n2k\nMarathi (mr)\nTED\n10k\n767\n1090\nOPUS-100\n27k\n2k\n2k\nKurdish (ku)\nTED\n10k\n265\n766\nOPUS-100\n45k\n2k\n2k\nBosnian (bs)\nTED\n6k\n474\n463\nOPUS-100\n1M\n2k\n2k\nMalay (ms)\nTED\n5k\n539\n260\nOPUS-100\n1M\n2k\n2k\nBengali (bn)\nTED\n5k\n896\n216\nOPUS-100\n1M\n2k\n2k\nBelarusian (be)\nTED\n5k\n248\n664\nOPUS-100\n67k\n2k\n2k\nFilipino (ﬁl)\nTED2020\n3k\n338\n338\nOPUS-100\n-\n-\n-\nTable 8: Dataset details for TED (Qi et al., 2018; Reimers and Gurevych, 2020) and OPUS-100 (Zhang et al.,\n2020).\nHyperparameter\nValue\nCheckpoint\nmbart50.pretrained\nArchitecture\nmbart_large\nOptimizer\nAdam\nβ1, β2\n0.9, 0.98\nWeight decay\n0.0\nLabel smoothing\n0.2\nDropout\n0.1\nAttention dropout\n0.1\nBatch size\n1024 tokens\nUpdate frequency\n2\nWarmup updates\n4k\nTotal number of updates\n130k\nMax learning rate\n1e-04\nTemperature sampling\n5\nAdapter dim.\n512\nTable 9: Fairseq hyperparameters used for our set of\nexperiments.\nrelates with BLEU in our experiments.\nA.4\nHyperparameters\nWe tune the dropout and the adapter bottleneck size\non TED. We use values 0.1, 0.3 for the dropout and\n128, 256, 512 for the bottleneck size. We list the\nhyperparameters we used to train both our proposed\nmodel and the baselines in Table 9.\nA.5\nEmbedding-layer results\nWe report in Table 11 the results of the abla-\ntion study concerning the use of embedding-layer\nadapters on all languages.\nA.6\nResults using GMM, random clustering\nand language families\nFull results of Table 5 can be seen in Table 12.\nLANG-FAMILY\nLANG-PAIR\nLANG-AGNOSTIC\nML-FT\nLang\nBLEU\nCOMET\nBLEU\nCOMET\nBLEU\nCOMET\nBLEU\nCOMET\nbg\n25.4\n67.2\n27.8\n72.1\n21.6\n44.6\n28.0\n76.5\nsr\n20.9\n44.3\n17.5\n38.2\n19.7\n41.1\n21.1\n48.4\nhr\n23.7\n55.0\n23.7\n53.1\n21.4\n43.4\n24.5\n55.1\nuk\n15.1\n-17.0\n17.7\n14.4\n13.8\n-18.5\n17.1\n35.9\nsk\n27.7\n54.3\n25.0\n50.1\n24.1\n57.0\n30.5\n64.9\nmk\n31.9\n62.9\n35.0\n64.1\n28.9\n65.2\n35.6\n62.1\nsl\n22.6\n48.9\n24.1\n65.8\n19.6\n42.3\n24.5\n64.3\nbs\n20.3\n44.1\n21.0\n37.1\n19.5\n43.9\n22.1\n50.8\nbe\n15.2\n-10.2\n10.1\n-21.6\n11.3\n-13.9\n17.9\n36.6\nid\n31.3\n60.1\n28.0\n64.0\n28.6\n77.0\n31.5\n60.1\nms\n25.4\n53.5\n24.5\n66.1\n21.8\n49.8\n25.5\n68.0\nfa\n9.8\n-23.5\n10.5\n-22.1\n8.1\n-24.4\n9.5\n-15.0\nhi\n18.7\n39.1\n15.6\n-19.1\n16.9\n10.1\n18.4\n36.4\nmr\n25.0\n67.0\n17.0\n9.0\n17.8\n19.5\n24.7\n58.1\nku\n15.3\n-18.5\n14.1\n-12.9\n12.8\n-11.5\n15.6\n-9.1\nbn\n12.9\n-16.0\n13.0\n-24.1\n11.2\n-18.1\n14.1\n-8.5\navg\n21.3\n32.0\n20.3\n27.1\n18.6\n25.5\n22.5\n42.8\nTable 10: Test set BLEU and COMET scores when translating out of English using OPUS-100. Languages are pre-\nsented by decreasing amount of parallel data per language family. LANG-PAIR stands for language-pair adapters,\nLANG-AGNOSTIC for language-agnostic, while LANG-FAMILY for language-family adapters. ML-FT stands for\nmultilingual ﬁne-tuning of the entire mBART-50 model.\nbg⋆\nsr⋆\nhr\nuk\nsk⋆\nmk\nsl\nbs⋆\nbe⋆\nid\nms⋆\nfa\nhi\nmr\nku⋆\nbn\nAVG\nLang-agnostic w/o emb\n21.3\n19.0\n21.5\n13.9\n23.6\n28.3\n19.1\n18.9\n10.5\n28.7\n21.5\n7.6\n16.1\n16.9\n12.4\n10.9\n18.1\nLang-agnostic with emb\n21.6\n19.7\n21.4\n13.8\n24.1\n28.9\n19.6\n19.5\n11.3\n28.6\n21.8\n8.1\n16.9\n17.8\n12.8\n11.2\n18.6\nLang-family w/o emb\n24.3\n20.4\n22.6\n14.8\n26.3\n31.2\n21.9\n20.6\n13.4\n31.4\n25.2\n9.0\n18.3\n23.7\n13.7\n12.2\n20.6\nLang-family with emb\n25.4\n20.9\n23.7\n15.1\n27.7\n31.9\n22.6\n20.3\n15.2\n31.3\n25.4\n9.8\n18.7\n25.0\n15.3\n12.9\n21.3\nTable 11: Full results of the ablation of the proposed architecture for en →xx (BLEU scores) on OPUS-100. Bold\nresults indicate best performance on average.\nbg\nsr\nhr\nuk\nsk\nmk\nsl\nbs\nbe\nid\nms\nﬁl\nfa\nhi\nmr\nku\nbn\nAVG\nGMM\n23.9\n17.7\n24.4\n11.0\n19.3\n22.9\n19.0\n23.6\n14.9\n29.7\n23.4\n-\n9.2\n18.8\n25.5\n14.3\n13.2\n19.4\nrandom\n22.9\n18.8\n23.5\n10.0\n22.5\n31.9\n21.1\n20.1\n12.1\n25.8\n24.9\n-\n5.0\n18.6\n22.9\n15.0\n8.1\n18.4\nTable 12: Evaluation of different methods to form language families for en →xx (BLEU) on OPUS-100.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-09-30",
  "updated": "2023-03-29"
}