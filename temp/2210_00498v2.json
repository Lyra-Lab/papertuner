{
  "id": "http://arxiv.org/abs/2210.00498v2",
  "title": "EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model",
  "authors": [
    "Yifu Yuan",
    "Jianye Hao",
    "Fei Ni",
    "Yao Mu",
    "Yan Zheng",
    "Yujing Hu",
    "Jinyi Liu",
    "Yingfeng Chen",
    "Changjie Fan"
  ],
  "abstract": "Unsupervised reinforcement learning (URL) poses a promising paradigm to learn\nuseful behaviors in a task-agnostic environment without the guidance of\nextrinsic rewards to facilitate the fast adaptation of various downstream\ntasks. Previous works focused on the pre-training in a model-free manner while\nlacking the study of transition dynamics modeling that leaves a large space for\nthe improvement of sample efficiency in downstream tasks. To this end, we\npropose an Efficient Unsupervised Reinforcement Learning Framework with\nMulti-choice Dynamics model (EUCLID), which introduces a novel model-fused\nparadigm to jointly pre-train the dynamics model and unsupervised exploration\npolicy in the pre-training phase, thus better leveraging the environmental\nsamples and improving the downstream task sampling efficiency. However,\nconstructing a generalizable model which captures the local dynamics under\ndifferent behaviors remains a challenging problem. We introduce the\nmulti-choice dynamics model that covers different local dynamics under\ndifferent behaviors concurrently, which uses different heads to learn the state\ntransition under different behaviors during unsupervised pre-training and\nselects the most appropriate head for prediction in the downstream task.\nExperimental results in the manipulation and locomotion domains demonstrate\nthat EUCLID achieves state-of-the-art performance with high sample efficiency,\nbasically solving the state-based URLB benchmark and reaching a mean normalized\nscore of 104.0$\\pm$1.2$\\%$ in downstream tasks with 100k fine-tuning steps,\nwhich is equivalent to DDPG's performance at 2M interactive steps with 20x more\ndata.",
  "text": "Published as a conference paper at ICLR 2023\nEUCLID: TOWARDS EFFICIENT UNSUPERVISED RE-\nINFORCEMENT LEARNING WITH MULTI-CHOICE DY-\nNAMICS MODEL\nYifu Yuan1, Jianye Hao∗,1, Fei Ni1, Yao Mu3, Yan Zheng1, Yujing Hu2, Jinyi Liu1,\nYingfeng Chen2, Changjie Fan2\n1College of Intelligence and Computing, Tianjin University,\n2Fuxi AI Lab, Netease, Inc., Hangzhou, China, 3The University of Hong Kong\nABSTRACT\nUnsupervised reinforcement learning (URL) poses a promising paradigm to learn\nuseful behaviors in a task-agnostic environment without the guidance of extrin-\nsic rewards to facilitate the fast adaptation of various downstream tasks. Previous\nworks focused on the pre-training in a model-free manner while lacking the study\nof transition dynamics modeling that leaves a large space for the improvement\nof sample efﬁciency in downstream tasks. To this end, we propose an Efﬁcient\nUnsupervised reinforCement Learning framework with multi-choIce Dynamics\nmodel (EUCLID), which introduces a novel model-fused paradigm to jointly pre-\ntrain the dynamics model and unsupervised exploration policy in the pre-training\nphase, thus better leveraging the environmental samples and improving the down-\nstream task sampling efﬁciency. However, constructing a generalizable model\nwhich captures the local dynamics under different behaviors remains a challeng-\ning problem. We introduce the multi-choice dynamics model that covers different\nlocal dynamics under different behaviors concurrently, which uses different heads\nto learn the state transition under different behaviors during unsupervised pre-\ntraining and selects the most appropriate head for prediction in the downstream\ntask. Experimental results in the manipulation and locomotion domains demon-\nstrate that EUCLID achieves state-of-the-art performance with high sample efﬁ-\nciency, basically solving the state-based URLB benchmark and reaching a mean\nnormalized score of 104.0±1.2% in downstream tasks with 100k ﬁne-tuning steps,\nwhich is equivalent to DDPG’s performance at 2M interactive steps with 20× more\ndata. More visualization videos are released on our homepage.\n1\nINTRODUCTION\nReinforcement learning (RL) has shown promising capabilities in many practical scenarios (Li et al.,\n2022b; Ni et al., 2021; Shen et al., 2020; Zheng et al., 2019). However, RL typically requires\nsubstantial interaction data and task-speciﬁc rewards for the policy learning without using any prior\nknowledge, resulting in low sample efﬁciency (Yarats et al., 2021c) and making it hard to generalize\nquickly to new downstream tasks (Zhang et al., 2018; Mu et al., 2022). For this, unsupervised\nreinforcement learning (URL) emerges and suggests a new paradigm: pre-training policies in an\nunsupervised way, and reusing them as prior for fast adapting to the speciﬁc downstream task (Li\net al., 2020; Peng et al., 2022; Seo et al., 2022), shedding a promising way to further promote RL to\nsolve complex real-world problems (ﬁlled with various unseen tasks).\nMost URL approaches focus on pre-train a policy with diverse skills via exploring the environment\nguided by the designed unsupervised signal instead of the task-speciﬁc reward signal (Hansen et al.,\n2020; Liu & Abbeel, 2021a). However, such a pre-training procedure may not always beneﬁt down-\nstream policy learning. As shown in Fig. 1, we pre-train a policy for 100k, 500k, 2M steps in the\nrobotic arm control benchmark Jaco, respectively, and use them as the prior for the downstream pol-\nicy learning to see how pre-training promotes the learning. Surprisingly, long-hour pre-training does\nnot always bring beneﬁts and sometimes deteriorates the downstream learning (500k vs 2M in the\n∗Corresponding authors: Jianye Hao (jianye.hao@tju.edu.cn)\n1\narXiv:2210.00498v2  [cs.LG]  22 Feb 2023\nPublished as a conference paper at ICLR 2023\n100k\n2M\nDownstream Task\n500k\nEUCLID\nPre-training (Disagreement)\nMismatch\nMismatch\nMatch\nPre-training Frame\nFigure 1: A motivation example.\norange line). We visualize three pre-trained policies on the left of Fig. 1, and ﬁnd they learn different\nskills (i.e., each covering a different state space). Evidently, only one policy (pre-trained with 500k)\nis beneﬁcial for downstream learning as it happens to focus on the area where the red brick exists.\nThis ﬁnding reveals that the downstream policy learning could heavily rely on the pre-trained policy,\nand poses a potential limitation in existing URL approaches: Only pre-training policy via diverse\nexploration is not enough for guaranteeing to facilitate downstream learning. Speciﬁcally, most\nmainstream URL approaches pre-train the policy in a model-free manner (Pathak et al., 2019; 2017;\nCampos et al., 2020), meaning that the skill discovered later in the pre-training, will more or less\nsuppress the earlier ones (like the catastrophic forgetting). This could result in an unpredictable skill\nthat is most likely not the one required for solving the downstream task (2M vs. 500k). We refer\nto this as the mismatch issue which could make pre-training even less effective than randomized\npolicy in the downstream learning. Similarly, Laskin et al. (2021) also found that simply increasing\npre-training steps sometimes brings no monotonic improvement but oscillation in performance.\nTo alleviate above issue, we propose the Efﬁcient Unsupervised reinforCement Learning frame-\nwork with multi-choIce Dynamic model (EUCLID), introducing the model-based RL paradigm\nto achieve rapid downstream task adaption and higher sample efﬁciency. First, in the pre-training\nphase, EUCLID proposes to pre-train the environment dynamics model, which barely suffers from\nthe mismatch issue as the upstream and downstream tasks in most time share the same environment\ndynamics. Notably, the pre-training dynamics model is also orthogonal to the pre-training policy,\nthus EUCLID pre-trains them together and achieves the best performance (see Fig. 1). In practice,\nEUCLID requires merely no additional sampling burden as the transition collected during the policy\npre-training can also be used for the dynamics model pre-training. On the other hand, in the ﬁne-\ntuning phase, EUCLID leverages the pre-trained dynamics model for planning, which is guided by\nthe pre-trained policy. Such a combination could eliminate the negative impact caused by the mis-\nmatch issue and gain fast adaptation performance. More importantly, EUCLID can monotonically\nbeneﬁt from an accurate dynamics model through a longer pre-training.\nAnother practical challenge is that, due to the model capacity, pre-training one single dynamics\nmodel is hard to accurately model all the environment dynamics. The inaccuracy can be further\nexacerbated in complex environments with huge state space, and thus deteriorates the downstream\nlearning performance. Inspired by multi-choice learning, EUCLID proposes a multi-headed dynam-\nics model with each head pre-trained with separate transition data. Each head focuses on a different\nregion of the environment, and is combined to predict the entire environment dynamics accurately.\nAs such, in the ﬁne-tuning phase, EUCLID could select the most appropriate head (sharing a similar\ndynamics to the downstream task) to achieve a fast adaptation.\nOur contributions are four-fold: (1) we extend the mainstream URL paradigm by innovative intro-\nducing the dynamics model in the pre-training phase, so that model-based planning can be leveraged\nin the ﬁne-tuning phase to alleviate the mismatch issue and further boost the downstream policy\nlearning performance; (2) We propose a multi-headed dynamics model to achieve a ﬁne-grained and\nmore accurate prediction, which promotes effective model planning in solving downstream tasks;\n(3) We empirically study the performance of EUCLID by comparing different mainstream URL\nmechanisms or designs, and comprehensively analyze how each part of EUCLID affect the ultimate\nperformance; (4) Extensive comparisons on diverse continuous control tasks are conducted and the\nresults demonstrate signiﬁcant superiority of EUCLID in performance and sample efﬁciency, espe-\ncially in challenging environments. Our approach basically solves the state-based URLB, achieving\nstate-of-the-art performance with a normalized score of 104.0±1.2% and outperforming the prior\nleading method by 1.35×, which is equivalent to DDPG with 20× more data.\n2\nPublished as a conference paper at ICLR 2023\n2\nBACKGROUND\nMarkov Decision Process (MDP) is widely used for formulating a continuous control task, deﬁned\nas a tuple (S, A, R, P, ρ0, γ) of the state space S, action space A, reward function R(s, a), transition\nprobability P ( s′ | s, a), initial state distribution ρ0 and discounting factor γ (Sutton & Barto, 1998).\nThe objective is to learn the optimal policy at ∼πφ (· | st) that maximizes the expected discounted\nreturn Es0∼ρ0,(s0,a0,...,sT )∼π\nhPT −1\nt=0 γtR (st, at)\ni\n, where T is the variable episode length.\nUnsupervised reinforcement learning (URL) poses a promising approach to learning useful pri-\nors from past experience and accelerating the learning of downstream tasks (Schwarzer et al., 2021).\nURLB (Laskin et al., 2021) split the whole learning phase into two parts, pre-training (PT) and ﬁne-\ntuning (FT).At every timestep in the PT phase, the agents can only interact with the task-agnostic\nreward-free environment to obtain intrinsic rewards learned through a self-supervised manner. In\ncontrast, in the FT phase, agents need to adapt quickly to downstream tasks with task-speciﬁc ex-\ntrinsic rewards provided by the environment.\nSpeciﬁcally, URL algorithms can be generalized into three categories, including knowledge-based,\ndata-based and competence-based methods (Oudeyer et al., 2007; Srinivas & Abbeel, 2021). The\ngoal of the knowledge-based methods is to increase knowledge of the world by maximizing predic-\ntion errors (Pathak et al., 2017; 2019; Burda et al., 2019) while data-based methods aim to maximize\nthe entropy of the state of the agents (Liu & Abbeel, 2021b; Yarats et al., 2021a; Hazan et al., 2019).\nCompetence-based methods learn an explicit skill vector by maximizing the mutual information be-\ntween the observation and skills (Campos et al., 2020; Eysenbach et al., 2019; Gregor et al., 2017).\nModel-based reinforcement learning (MBRL) leverages a learned dynamic model of the envi-\nronment to plan a sequence of actions in advance which augment the data (Sutton, 1991; Janner\net al., 2019; Pan et al., 2020; Mu et al., 2020; Peng et al., 2021) or obtain the desired behavior\nthrough planning (Chua et al., 2018; Hafner et al., 2019; Lowrey et al., 2019; Mu et al., 2021; Chen\net al., 2022). However, training the world model requires a large number of samples (Polydoros &\nNalpantidis, 2017; Plaat et al., 2021), and an imprecise model can lead to low-quality decisions for\nimaginary planning (Freeman et al., 2019). Thus constructing a reliable world model by pre-training\ncan greatly accelerate the learning process of downstream tasks (Chebotar et al., 2021; Seo et al.,\n2022). A concurrent work of ours is Rajeswar et al. (2022), which also focuses on promoting URL\nperformance through a world model, but only considers the incorporation of a simple single dynam-\nics model without realizing the low accuracy caused by the mismatch of the pre-training policies and\nthe optimal policies for the downstream tasks. Our work designs multi-choice learning and policy\nconstraints to explore how to construct a generic pre-trained dynamics model to cover different local\ndynamics under different behaviors.\nMulti-choice learning (MCL) learns ensemble models that produce multiple independent but di-\nverse predictions and selects the most accurate predictions to optimize the model, better reduce\ntraining loss and improve stability (Guzm´an-Rivera et al., 2012; 2014; Dey et al., 2015; Lee et al.,\n2017). Multi-choice learning has been widely used, such as in vision computing (Cuevas et al., 2011;\nTian et al., 2019; Lee et al., 2017) and visual question answer (Lei et al., 2020; Lu et al., 2022), but\nmost works focus on supervised learning. T-MCL (Seo et al., 2020) and DOMINO (Mu et al., 2022)\nbring MCL into the meta RL, which approximates multi-modal distribution with context and models\nthe dynamics of changing environments. Inspired by this, EUCLID builds a multi-headed dynamics\nmodel to cover specialized prediction regions corresponding to different downstream tasks.\n3\nMETHODOLOGY\nIn this work, we propose an Efﬁcient Unsupervised reinforCement Learning framework with multi-\nchoIce Dynamic model (EUCLID) to further improve the ability of mainstream URL paradigm\nin fast adapting to various downstream tasks. As a starter, EUCLID adopts the task-oriented la-\ntent dynamic model as the backbone for environment modeling, and contains two key parts: 1 a\nmodel-fused URL paradigm that innovatively integrates the world model into the pre-training and\nﬁne-tuning for facilitating downstream tasks learning, and 2 a multi-headed dynamics model that\ncaptures different environment dynamics separately for an accurate prediction of the entire envi-\nronment. In this way, EUCLID can achieve a fast downstream tasks adaptation by leveraging the\n3\nPublished as a conference paper at ICLR 2023\nGuide\nInit\nWeight\nPre-training with Intrinsic Rewards\nFine-tuning to Downstream Tasks\nTask-agnostic \nEnvironment\nState\nWorld Model\nAction & Intrinsic Reward\nExploration Policy\nPre-trained Policy\nWorld Model\nPlanner\nDownstream Task\nExtrinsic Reward\nIntrinsic Reward\nSelf-supervised Task\nFigure 2: Overview of the model-fused URL paradigm. In the pre-training phase (left), we jointly\nupdate the world models and policy using exploring samples collected from the environment inter-\naction. In the ﬁne-tuning phase (right), we reuse the pre-trained weights to initialize the downstream\nworld models and the policy, perform policy guided planning via dynamics model for fast adaption.\naccurate pre-trained environment model for an effective model planning in the downstream ﬁne-\ntuning. The detail pseudo code is given in Algorithm 1.\n3.1\nBACKBONE FOR ENVIRONMENT MODELING\nWe build world models for representation learning and planning based on the Task-Oriented Latent\nDynamic Model (TOLD) of TDMPC (Hansen et al., 2022). EUCLID learns world models that\ncompresses the history of observations into a compact feature space variable z and enables policy\nlearning and planning in this space (Zhang et al., 2019; Hafner et al., 2019). EUCLID’s world model\ncomponents include three major model components: (i) the representation model that encodes state\nst to a model state zt and characterizes spatial constraints by latent consistency loss, (ii) the latent\ndynamics model that predicts future model states zt+1 without reconstruction and (iii) the reward\npredictor allows the model to encoder task-relevant information in a compact potential space. The\nmodel can be summarized as follow:\nRepresentation:\nzt = Eθ (st)\nLatent dynamics: zt+1 = Dθ (zt, at)\nReward predictor:\nˆrt ∼Rθ (zt, at)\n(1)\nwhere s, a, z denote a state, action and latent state representation. Then, for effective value-based\nlearning guidance planning, we built actor and critic based on DDPG (Lillicrap et al., 2016):\nValue: ˆqt = Qθ (zt, at)\nPolicy: ˆat ∼πφ (zt) ,\n(2)\nAll model parameters θ except actor are jointly optimized by minimizing the temporally objective:\nL (θ, D) = E(s,a,zt,rt)t:t+H∼D\n\n\nt+H\nX\ni=t\n\n\nc1 ∥Rθ (zi, ai) −ri∥2\n2\n|\n{z\n}\nReward prediction\n+c2 ∥Dθ (zi, ai) −Eθ−(si+1)∥2\n2\n|\n{z\n}\nLatent state consistency\n+c3 ∥Qθ (zi, ai) −(ri + γQθ−(zi+1, πθ (zi+1)))∥2\n2\n|\n{z\n}\nValue prediction\n\n\n\n\n,\n(3)\nwhile actor πφ maximizes Qθ approximated cumulative discounted returns. The reward term pre-\ndicts a single-step task reward, the state transition term aims to predict future latent states accurately\nand the value term is optimized through TD-learning (Haarnoja et al., 2018; Lillicrap et al., 2016).\nHyper-parameters ci are adjusted to balance the multi-source loss function and a trajectory of length\nH is sampled from the replay buffer D.\n3.2\nMODEL-FUSED UNSUPERVISED REINFORCEMENT LEARNING PARADIGM\nOverall, EUCLID introduces the environment modeling into the pre-training (PT) and ﬁne-tuning\n(FT) phases, formulating a new model-fused URL paradigm. As shown in Fig. 2 (left), EUCLID\nadopts a reward-free exploration policy to collect task-agnostic transitions for pre-training the dy-\nnamics model and then uses them to boost downstream FT. In the following, we describe three\nimportant detail designs in EUCLID throughout the PT and FT processes.\nFirstly, in the PT phase, we expect as diverse data as possible to pre-train the dynamics model\nfor an accurate environment prediction. As EUCLID can be easily combined with almost any\n4\nPublished as a conference paper at ICLR 2023\n②Learn dynamics from experience\n③Act in the downstream tasks\nDownstream Task\n①Explore diverse specialized regions\nHead 1\nHead 2\nHead 3\nHead 4\nHead 1\nHead 2\nHead 3\nHead 4\nBackbone Net\nPlanning\nMaximum Zero-shot\nReward\nFigure 3: Illustrations of multi-choice learning in EUCLID. We design a multi-headed dynam-\nics model to predict different environment dynamics, training each predictor head separately using\ndiverse data, and select the most beneﬁcial predictor head for the downstream task.\nmainstream exploration methods (Hao et al., 2023), we empirically evaluate the knowledge-based\nmethod (Disagreement, Pathak et al. (2019)), data-based method (APT, Liu & Abbeel (2021b)) and\ncompetence-based method (DIAYN, Eysenbach et al. (2019)) (see more in Appendix B). We ﬁnd\nthe knowledge-based method (i.e., Disagreement) synthetically performs the best, and hypothesize\nthis happens because it explores via maximizing the prediction uncertainty, allowing discovery more\nunpredictable dynamics.\nSecondly, in the PT phase, how to select the loss function for the dynamics model pre-training is\nchallenging. We use latent consistency loss (Schwarzer et al., 2020; 2021; Hansen et al., 2022)\nto learn the dynamics model in latent space to directly predict the feature of future states without\nreconstruction (Hafner et al., 2020b;a; Ha & Schmidhuber, 2018). This improves the generalization\nof the dynamics model and avoids capturing task-irrelevant details between PT and FT phase while\nthe reconstruction loss forces to model everything in the environment in such a huge state space.\nLastly, shown in Fig. 2 (right), we utilize the pre-trained dynamics model to rollout trajectories for\nboth planning and policy gradient in the FT for downstream tasks. An important reason is that\nmodel-based planning can yield a stable and efﬁcient performance due to the similarity of the envi-\nronment dynamics between upstream and downstream tasks, thus an accurate pre-trained dynamics\nmodel could beneﬁt downstream learning. This can avoid the above-mentioned mismatch issue,\ncaused by utilizing a less effective pre-trained policy for the downstream tasks. For this, we adopt\na representative model predictive control method (Williams et al., 2015) to select the best action\nbased on the imaged rollouts (via dynamics model). Meanwhile, we also notice that the pre-trained\npolicy can master some simple skills like rolling, swinging, and crawling, which is useful and can\nbe utilized in the model planning to further boost downstream learning. Therefore, we additionally\nmix the trajectories generated by the pre-trained policy interacting with the dynamics model for\nplanning. This would further speed up the planning performance and effectiveness, especially in the\nearly stage of ﬁne-tuning (Wang & Ba, 2020). Another side beneﬁt is that the Q-value function (in\nthe critic) in the pre-trained policy could speed up the long-term planning via bootstrapping (Sikchi\net al., 2021) (see details in Appendix H).\n3.3\nMULTI-CHOICE LEARNING VIA MULTI-HEADED DYNAMICS MODEL\nPrior works (Wu et al., 2019; Seo et al., 2020) reveal that training one single dynamics model to\naccurately predict the whole environment dynamics is difﬁcult in complex environments. Besides,\npre-training one single dynamics model to predict the entire state space in the environment is ineffec-\ntive, as downstream tasks may involve only limited local state distributions (left in Fig. 3). Therefore,\nEUCLID proposes a multi-choice learning mechanism via multi-headed dynamics model in the PT\nstage for achieving a more accurate environment prediction and to select one of the heads for the FT\nstage to further boost downstream learning.\nAs shown in Fig. 3 (middle), we design a multi-headed dynamics model to predict different environ-\nment dynamics (i.e., state distributions), encouraging each predictor head to enjoy better prediction\naccuracy in its own specialized prediction region. The model contains a backbone parameterized by\nθ, sharing the high-level representations of the ﬁrst few layers of the neural network, while the latter\nlayers build H prediction heads parameterized by {θhead\nh\n}H\nh=1. The output is:\nzt+1 =\n\b\nDθ\n\u0000zt, at; θ, θhead\nh\n\u0001\tH\nh=1.\n(4)\nWe do not use separate ensemble models with completely different parameters because we empir-\nically observe that there may also be a large number of shared dynamics between different down-\n5\nPublished as a conference paper at ICLR 2023\nstream tasks which can be captured by the shared backbone to leverage environmental samples\nefﬁciently and also avoid actively reducing the information of acquired environment samples.\nTo encourage each head to focus on a separate region as possible, we assign different exploration\npolicies for each head, denoted by [π1\nφ(z), · · · , πh\nφ(z)].\nTo avoid region overlap, we design a\ndiversity-encouraging term as an additional regularization term as follows:\nLπ(φ, D) = Ezt∼D [Qθ (zt, πφ (zt)) −αDKL (eπφ (zt) ∥πφ (zt))],\n(5)\nwhere eπφ (zt) = Ph\ni=1 πi (z) /h. In this way, each policy is optimized by its own intrinsic for\nexploration and policies are encouraged to move away from each other (see more in Appendix G).\nIn the beginning of FT phase, we select one head in the pre-trained multi-headed dynamics model to\nconstruct a single-head dynamic model that can beneﬁt the downstream learning the most (right in\nFig. 3). To ﬁgure out the optimal head h∗(that covers the most appropriate state region) for solving\nthe downstream tasks, inspired by Seo et al. (2020), we evaluate the zero-shot performance of each\nhead and pick one with the highest zero-shot performance in downstream tasks. After that, only\noptimal head is used in the subsequent ﬁne-tuning for adapting to downstream tasks.\nOverall, we empirically show that different prediction heads can indeed cover different state regions\nwith respective closely related tasks and the multi-choice learning improves the performance by\nselecting the appropriate specialized prediction head (see Fig. 7).\n4\nEXPERIMENTS\nWe conduct experiments on various tasks to study the following research questions (RQs):\nCombination (RQ1): Which category of URL method works best in combination with EUCLID?\nPerformance (RQ2): How does EUCLID compare to existing URL methods?\nMonotonism (RQ3): Can EUCLID monotonically beneﬁt from longer pre-training steps?\nSpecialization (RQ4): Does multi-choice learning delineate multiple specialized prediction region?\nAblation (RQ5): How each module in EUCLID facilitates the downstream policy learning?\n4.1\nEXPERIMENTAL SETUP\nBenchmarks: We evaluate our approach on tasks from URLB (Laskin et al., 2021), which con-\nsists of three domains (walker, quadruped, jaco) and twelve challenging continuous control down-\nstream tasks. Besides, we extend the URLB benchmark (URLB-Extension) by adding a more com-\nplex humanoid domain and three corresponding downstream tasks based on the DeepMind Control\nSuite (DMC) (Tunyasuvunakool et al., 2020) to further demonstrate the efﬁciency improvement of\nEUCLID on more challenging environments. Environment details can be found in Appendix A.\nBaselines: The proposed EUCLID is a general framework that can be easily combined with any\nunsupervised RL algorithms. Therefore we make a comprehensive investigation of EUCLID com-\nbined with popular unsupervised RL algorithms of three exploration paradigms. Speciﬁcally, we\nchoose the most representative algorithms from each of the three categories including Disagree-\nment (Knowledge-based) (Pathak et al., 2019), APT (Data-based) (Liu & Abbeel, 2021b) and DI-\nAYN (Competence-based) (Eysenbach et al., 2019). In addition, we compare our method to the pre-\nvious state-of-the-art method CIC (Laskin et al., 2022) in the URLB, which is a hybrid data-based\nand competence-based method. More detailed information about the baselines is in Appendix B.\nEvaluation: In the PT phase, all model components are pre-trained under an unsupervised setting\nbased on the sample collected by interacting 2M steps in the reward-free environment. Then in\nthe FT phase, we ﬁne-tune each agent in downstream tasks with the extrinsic reward for only 100k\nsteps, which is moderated to 150k steps for the humanoid domain due to special difﬁculties. All\nstatistics are obtained based on 5 independent runs per downstream task, and we report the average\nwith 95% conﬁdence regions. For fairness of comparison, all model-free baselines and the policy\noptimization part of model-based methods opt DDPG (Lillicrap et al., 2016) agent as backbone,\nmaintaining the same setting as Laskin et al. (2021). To better compare the average performance\nof multiple downstream tasks, we choose the expert score of DDPG learning from scratch with 2M\nsteps in the URLB as a normalized score. Note that expert agents run for 2M steps with extrinsic\nreward, which is 20× more than the budget of the agent used for evaluation steps in the FT phase.\n6\nPublished as a conference paper at ICLR 2023\n40\n50\n60\n70\n80\n90\n100\n110\nNormalized Return (%)\nWalker\nDisagreement\nEUCLID(Disagreement)\nAPT\nEUCLID(APT)\nDIAYN\nEUCLID(DIAYN)\nCIC\nTDMPC@100k\n40\n50\n60\n70\n80\n90\n100\nQuadruped\n0\n20\n40\n60\n80\n100\n120\nJaco\n40\n50\n60\n70\n80\n90\n100\n110\nOverall\nFigure 4: We combine EUCLID with three types of exploration algorithms to show the synergistic\nbeneﬁt of URL and MBRL methods for 12 downstream tasks on URLB. The normalized scores are\nthe average performance after pre-training 2M steps and then ﬁne-tuning 100k steps with 5 indepen-\ndent seeds. EUCLID achieves signiﬁcant improvement over the corresponding exploration backbone\nand empirically shows the combination with the knowledge-based method is most effective.\n0\n30\n60\n90\n120\n150\nEnvironment Steps (×10³)\n0\n100\n200\n300\n400\nEpisode Return\nEUCLID\nEUCLID w/ naive PT\nTDMPC@150k\nDisagreement\n(a) Humanoid-Stand\n0\n30\n60\n90\n120\n150\nEnvironment Steps (×10³)\n0\n100\n200\n300\nEpisode Return\n(b) Humanoid-Walk\n0\n30\n60\n90\n120\n150\nEnvironment Steps (×10³)\n0\n20\n40\n60\n80\n100\nEpisode Return\n(c) Humanoid-Run\nFigure 5: Learning curves of EUCLID and three baselines on downstream tasks of the humanoid\ndomain. Curves show the mean and 95% conﬁdence intervals of performance across 5 independent\nseeds. The dashed reference lines are the asymptotic performance of the Disagreement algorithm\nwith 2M PT steps and 2M FT steps. These results show that EUCLID has better sample efﬁciency\nand large performance gains on tasks with complex dynamics.\nImplementation details: EUCLID employs both model planning and policy optimization in the FT\nphase but uses only policy optimization in the PT phase.\nDuring planning, we leverage Model Predictive Path Integral (MPPI) (Williams et al., 2015) control\nand choose a planning horizon of L = 5. Notably, we still retain seed steps for warm-start in the\nFT phase despite sufﬁcient pre-training steps to alleviate the extreme case of a strong mismatch\nbetween the pre-training policies and the optimal policies for the downstream tasks. For multi-\nchoice learning, we build a multi-headed dynamics model maintaining heads of H = 4 by default,\neach optimized by independent samples. To avoid unfair comparison, we keep the same training\nsettings for all methods.\n4.2\nCOMBINATION (RQ1)\nTo answer RQ1, we show the synergistic beneﬁt of EUCLID in unsupervised RL methods and\nmodel-based architecture. As shown in Fig. 4, We build EUCLID on a different URL backbone\nnamed EUCLID (Disagreement/APT/DIAYN), where the dark line denotes its normalized score and\nthe light line denotes the corresponding vanilla URL backbone results in Laskin et al. (2021). For\nthe purpose of simplicity, we do not use the multi-choice learning mechanism here. We show that\nwe can combine all three types of exploration baselines and obtain signiﬁcant performance gains in\nall environments. In particular, the previous competence-based approaches under-perform in URLB\nbecause of the weak discriminator, while the EUCLID (DIAYN) greatly improves fast adaptation\nto downstream tasks through pre-trained world models. Besides, we empirically ﬁnd that EUCLID\nis better combined with the knowledge-based approaches with the best performance and stability.\nUnless speciﬁed, we choose Disagreement as the exploration backbone in the PT stage by default.\n4.3\nPERFORMANCE (RQ2)\nComparative evaluation on URLB.\nTo answer RQ2, we evaluate the performance of EUCLID\nand other baseline methods in the URLB and URLB-Extension environment. The results in Table 1\nshow that EUCLID outperforms all other mainstream baselines in all domains and basically solves\n7\nPublished as a conference paper at ICLR 2023\nTable 1: Performance of EUCLID and EUCLID w/o multi headed structure on URLB after 2M\nreward-free pre-training steps and ﬁnetuning for 100k steps with extrinsic rewards. All baseline\nruns with 5 seeds. We refer to Appendix C for full results of all baselines.\nDomain\nTask\nDisagreement\nCIC\nTDMPC@100k\nEUCLID w/o MCL\nEUCLID\nWalker\nFlip\n491±21\n631±34\n930±28\n971±1\n969±2\nRun\n444±21\n486±25\n750±4\n765±10\n770±9\nStand\n907±15\n959±2\n940±22\n985±1\n985±1\nWalk\n782±33\n885±28\n967±2\n967±4\n972±1\nNormalized score\n72.5±2.6\n82.2±3.4\n101.4±1.6\n104.3±0.5\n104.6±0.4\nQuadruped\nJump\n668±24\n595±42\n723±98\n840±13\n858±14\nRun\n461±12\n505±47\n465±83\n651±35\n735±16\nStand\n840±33\n761±54\n765±92\n953±6\n958±5\nWalk\n721±56\n723±43\n710±77\n874±42\n925±6\nNormalized score\n73.5±3.5\n72.5±5.2\n74.7±9.8\n93.1±2.7\n97.6±1.1\nJaco\nReach bottom left\n134±8\n138±9\n168±7\n214±5\n220±3\nReach bottom right\n122±4\n145±7\n183±11\n205±9\n212±2\nReach top left\n117±14\n153±7\n178±11\n197±23\n225±5\nReach top right\n140±47\n163±4\n172±24\n219±7\n229±6\nNormalized score\n63.4±8.6\n74.0±3.4\n86.9±6.4\n103.3±5.6\n109.7±2.0\nOverall\nNormalized score\n69.8±4.9\n76.2±4.0\n87.7±6.0\n100.2±2.9\n104.0±1.2\nthe twelve downstream tasks of the state-based URLB in 100k steps, obtaining a performance com-\nparable to the expert scores while the expert agent trained 2M steps with extrinsic reward. It is\nworth noting that EUCLID signiﬁcantly improves performance in reward-sparse robotic arm control\ntasks, such as jaco, which is challenging for previous URL methods. Moreover, the results of EU-\nCLID also show signiﬁcant improvement over TDMPC@100k learned from scratch, especially with\ngreater gains in the Quadruped domain where is challenging to predict accurately. This suggests that\nunsupervised pre-training can signiﬁcantly reduce model error.\nEffects of multi-headed dynamics model.\nTo verify the importance of multi-choice learning,\nwe compared the performance of EUCLID and EUCLID w/o multi-headed dynamics model. Ta-\nble 1 demonstrates that EUCLID achieved almost all of the 12 downstream tasks’ average perfor-\nmance improvement and signiﬁcantly reduced the variance, reaching the highest normalized score\nof 104.0±1.2%. This indicates that multi-choice learning reduces the prediction error of the down-\nstream tasks corresponding to local dynamics and obtains better results.\nURLB-Extension experiments.\nAs shown in Fig. 5, we evaluate the agents in the humanoid do-\nmain which is the most difﬁcult to learn locomotion skills due to high-dimensional action spaces.\nWe found that the previous model-free URL method is difﬁcult to improve the learning efﬁ-\nciency of downstream tasks in such a complex environment, while the previous model-based meth-\nods (TDMPC) still make some initial learning progress. In contrast, EUCLID is able to consistently\nimprove performance in all three downstream tasks with small variance. Also, we compare EUCLID\nwith the naive pre-training scheme, which uses a random exploration policy to collect data in the PT\nphase to train the world models. As the task difﬁculty increases, we observe that EUCLID w/ naive\nPT struggles to achieve competitive performance because of the lack of intrinsic unsupervised goal.\n4.4\nMONOTONISM (RQ3)\nIntuitively, a longer pre-training phase is expected to allow for a more efﬁcient ﬁne-tuning phase,\nbut the empirical evidence of Laskin et al. (2021) demonstrated that longer pre-training is not always\nbeneﬁcial or even worse than random initialization when applying the previous URL algorithm, be-\ncause it is difﬁcult to capture such rich environmental information with only pre-trained policies.\nThis is a major drawback that makes the application URL pre-training paradigm fraught with un-\ncertainty. However, EUCLID better leverage environmental information to build additional world\nmodels. Fig. 6 shows that our methods achieve monotonically improving performance by varying\npre-training steps on most of the tasks. All results are better than random initialization, except for\nthe quadruped domain with 100k pre-training steps.\n4.5\nSPECIALIZATION (RQ4)\nPrediction heads corresponding to the specialized regions.\nTo investigate the ability of EU-\nCLID to learn specialized prediction heads, we visualized how to assign heads to downstream tasks\nin Fig. 7(a), we can observe that different prediction heads are better adapted to different downstream\ntasks, with their corresponding specialized region. i.e., head 2 works best for Run tasks while head\n8\nPublished as a conference paper at ICLR 2023\n100k\n500k\n2M\nPre-training Frames\n100\n101\n102\n103\n104\n105\nNormalized Return (%)\nWalker\nRandom init\nw/ Disagreement\nw/ APT\nw/ DIAYN\n100k\n500k\n2M\nPre-training Frames\n40\n50\n60\n70\n80\n90\n100\nNormalized Return (%)\nQuadruped\n100k\n500k\n2M\nPre-training Frames\n85\n88\n91\n94\n97\n100\n103\n106\nNormalized Return (%)\nJaco\n100k\n500k\n2M\nPre-training Frames\n75\n80\n85\n90\n95\n100\n105\nNormalized Return (%)\nOverall\nFigure 6: We show the ﬁne-tuning performance of EUCLID with different pre-training steps, in-\ncluding 100k, 500k, and 2M steps. EUCLID can beneﬁt from longer pre-training steps to achieve\nmonotonic performance gains, while previous URL algorithms fail.\n267\n266\n183\n66\n105\n109\n419\n408\n388\n178\n239\n218\nWalker\nHead 1\nHead 2\nHead 3\nHead 4\nStand\nFlip\nWalk\nRun\n268\n100\n422\n186\n296\n324\n242\n369\n215\n233\n138\n228\n431\n323\n359\n353\n217\n166\n173\n161\nQuadruped\nHead 1\nHead 2\nHead 3\nHead 4\nStand\nJump\nWalk\nRun\n(a) Specialized region of each predict head\nWalker\nQuadruped\nJaco\nOverall\n75\n80\n85\n90\n95\n100\n105\n110\n115\nNormalized Return (%)\nMultiple models\nMulti choice model(Ours)\n(b) Importance of multi-headed model\nFigure 7: (a) Zero-shot performance of the pre-trained multi-headed dynamics model for different\ntasks on Walker and Quadruped domain. We highlight the top-2 prediction heads which are suitable\nfor Corresponding tasks. (b) The average performance of the multi-headed dynamics model used\nby EUCLID and multiple dynamics models across 12 downstream tasks for 2M pre-training steps.\n4 for Jump tasks in the Quadruped environment. No one prediction head can perform optimally for\nall downstream tasks. See our homepage for more visualization analysis.\n70\n80\n90\n100\n110\nNormalized Return (%)\nDefault\nw/o PT Reward\nw/o PT Critic\nw/o PT Dynamic\nw/o PT Actor\nFigure 8: Ablation on different\npre-trained components for FT.\nComparison of multiple models and multi-headed models.\nFig. 7(b) shows the comparison of the multi-headed dynamics\nmodel used by EUCLID and multiple dynamics models without\na shared backbone. We ﬁnd that multiple models are less effec-\ntive than multi-headed models in all domains, especially in the\nquadruped domain which requires a large amount of sample to\nmodel complex dynamics. This also demonstrates that sharing\nthe underlying dynamics of agents is beneﬁcial to improve the\nsample efﬁciency for downstream tasks.\n4.6\nABLATION (RQ5)\nRoles for each module.\nAs shown in Fig. 8, We conducted ablation experiments with 2M steps\npretraining on the EUCLID, reusing different subsets of pre-trained components in the FT phase. To\nensure the generality of the conclusions, we conduct experiments based on different URL methods\nand take the average results. From the overall results, our default settings, reusing all components of\nthe world models (encoder, dynamics model, reward model) and policy (actor and critic) work best.\nWe show the detailed results for each domain in Appendix E with in-depth analysis.\n5\nCONCLUSION\nIn this work, we propose EUCLID which introduces the MBRL methods into the URL domain to\nboost the fast adaption and learning performance. We formulate a novel model-fused URL paradigm\nto alleviate the mismatch issue between the upstream and downstream tasks and propose the multi-\nchoice learning mechanism for the dynamics model to achieve more accurate predictions and further\nenhance downstream learning. The results demonstrate that EUCLID achieves state-of-the-art per-\nformance with high sample efﬁciency. Our framework points to a novel and promising paradigm\nfor URL to improve the sample efﬁciency and may be further improved by the more ﬁne-grained\ndynamics divided by skill prior (Pertsch et al., 2020), or the combination of ofﬂine RL (Yu et al.,\n9\nPublished as a conference paper at ICLR 2023\n2022). In addition, extending EUCLID in the multiagent RL (Li et al., 2022a; Hao et al., 2022;\nZheng et al., 2018; 2021) is also a promising direction, which we leave as future work.\nACKNOWLEDGMENTS\nThis work is supported by the National Natural Science Foundation of China (Grant No.62106172),\nthe “New Generation of Artiﬁcial Intelligence” Major Project of Science & Technology 2030 (Grant\nNo.2022ZD0116402), and the Science and Technology on Information Systems Engineering Labo-\nratory (Grant No.WDZC20235250409, No.WDZC20205250407).\nREFERENCES\nAlberto Bemporad and Manfred Morari. Control of systems integrating logic, dynamics, and con-\nstraints. Automatica, 35(3):407–427, 1999.\nYuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. Exploration by random network\ndistillation. In 7th International Conference on Learning Representations, 2019.\nVictor Campos, Alexander Trott, Caiming Xiong, Richard Socher, Xavier Gir´o-i-Nieto, and Jordi\nTorres. Explore, discover and learn: Unsupervised discovery of state-covering skills. In Proceed-\nings of the 37th International Conference on Machine Learning, 2020.\nYevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jacob Varley, Alex Ir-\npan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, and Sergey Levine. Actionable models:\nUnsupervised ofﬂine reinforcement learning of robotic skills. In Proceedings of the 38th Interna-\ntional Conference on Machine Learning, 2021.\nXiaoyu Chen, Yao Mark Mu, Ping Luo, Shengbo Li, and Jianyu Chen. Flow-based recurrent belief\nstate learning for pomdps. In International Conference on Machine Learning, pp. 3444–3468.\nPMLR, 2022.\nKurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learn-\ning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information\nProcessing Systems 31, 2018.\nErik Cuevas, Daniel Zaldivar, and Marco P´erez-Cisneros. Seeking multi-thresholds for image seg-\nmentation with learning automata. Machine Vision and Applications, 22(5):805–818, 2011.\nDebadeepta Dey, Varun Ramakrishna, Martial Hebert, and J. Andrew Bagnell. Predicting multiple\nstructured visual interpretations. In 2015 IEEE International Conference on Computer Vision,\n2015.\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:\nLearning skills without a reward function. In 7th International Conference on Learning Repre-\nsentations, 2019.\nC. Daniel Freeman, David Ha, and Luke Metz. Learning to predict without looking ahead: World\nmodels without forward prediction. In Advances in Neural Information Processing Systems 32,\n2019.\nKarol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. In 5th\nInternational Conference on Learning Representations, 2017.\nAbner Guzm´an-Rivera, Dhruv Batra, and Pushmeet Kohli. Multiple choice learning: Learning to\nproduce multiple structured outputs. In Advances in Neural Information Processing Systems 25,\n2012.\nAbner Guzm´an-Rivera, Pushmeet Kohli, Dhruv Batra, and Rob A. Rutenbar. Efﬁciently enforcing\ndiversity in multi-output structured prediction. In Proceedings of 17th International Conference\non Artiﬁcial Intelligence and Statistics, 2014.\nDavid Ha and J¨urgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.\n10\nPublished as a conference paper at ICLR 2023\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash\nKumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algo-\nrithms and applications. CoRR, abs/1812.05905, 2018.\nDanijar Hafner, Timothy P. Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and\nJames Davidson. Learning latent dynamics for planning from pixels. In Proceedings of the 36th\nInternational Conference on Machine Learning, 2019.\nDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with dis-\ncrete world models. arXiv preprint arXiv:2010.02193, 2020a.\nDanijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learn-\ning behaviors by latent imagination. In 8th International Conference on Learning Representa-\ntions, 2020b.\nNicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive\ncontrol. arXiv preprint arXiv:2203.04955, 2022.\nSteven Hansen, Will Dabney, Andr´e Barreto, David Warde-Farley, Tom Van de Wiele, and\nVolodymyr Mnih. Fast task inference with variational intrinsic successor features. In 8th In-\nternational Conference on Learning Representations, 2020.\nJianye Hao, Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Zhaopeng Meng, Peng Liu, and\nZhen Wang. Exploration in deep reinforcement learning: From single-agent to multiagent domain.\nIEEE Transactions on Neural Networks and Learning Systems, 2023.\nXiaotian Hao, Weixun Wang, Hangyu Mao, Yaodong Yang, Dong Li, Yan Zheng, Zhen Wang, and\nJianye Hao. Api: Boosting multi-agent reinforcement learning via agent-permutation-invariant\nnetworks. arXiv preprint arXiv:2203.05285, 2022.\nElad Hazan, Sham M. Kakade, Karan Singh, and Abby Van Soest. Provably efﬁcient maximum\nentropy exploration. In Proceedings of the 36th International Conference on Machine Learning,\n2019.\nMichael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-\nbased policy optimization. In Advances in Neural Information Processing Systems 32, 2019.\nHang Lai, Jian Shen, Weinan Zhang, and Yong Yu. Bidirectional model-based policy optimization.\nIn Proceedings of the 37th International Conference on Machine Learning, 2020.\nMichael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Ler-\nrel Pinto, and Pieter Abbeel. URLB: Unsupervised reinforcement learning benchmark. arXiv\npreprint arXiv:2110.15191, 2021.\nMichael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, and Pieter Abbeel.\nCIC: contrastive intrinsic control for unsupervised skill discovery. CoRR, abs/2202.00161, 2022.\nKimin Lee, Changho Hwang, KyoungSoo Park, and Jinwoo Shin. Conﬁdent multiple choice learn-\ning. In Proceedings of the 34th International Conference on Machine Learning, 2017.\nChenyi Lei, Lei Wu, Dong Liu, Zhao Li, Guoxin Wang, Haihong Tang, and Houqiang Li. Multi-\nquestion learning for visual question answering. In 34th AAAI Conference on Artiﬁcial Intelli-\ngence, 2020.\nJuncheng Li, Xin Wang, Siliang Tang, Haizhou Shi, Fei Wu, Yueting Zhuang, and William Yang\nWang. Unsupervised reinforcement learning of transferable meta-skills for embodied navigation.\nIn 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.\nP. Li, H. Tang, T. Yang, X. Hao, T. Sang, Y. Zheng, J. Hao, M. E. Taylor, W. Tao, and Z. Wang.\nPMIC: improving multi-agent reinforcement learning with progressive mutual information col-\nlaboration. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, Proceedings of Machine Learning Research, pp. 12979–12997, 2022a.\nPengyi Li, Hongyao Tang, Jianye Hao, Yan Zheng, Xian Fu, and Zhaopeng Meng. Erl-re2: Efﬁ-\ncient evolutionary reinforcement learning with shared state representation and individual policy\nrepresentation. arXiv preprint arXiv:2210.17375, 2022b.\n11\nPublished as a conference paper at ICLR 2023\nTimothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In 4th\nInternational Conference on Learning Representations, 2016.\nHao Liu and Pieter Abbeel. APS: active pretraining with successor features. In Proceedings of the\n38th International Conference on Machine Learning, 2021a.\nHao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. In Advances\nin Neural Information Processing Systems 34, 2021b.\nKendall Lowrey, Aravind Rajeswaran, Sham M. Kakade, Emanuel Todorov, and Igor Mordatch.\nPlan online, learn ofﬂine: Efﬁcient learning and exploration via model-based control. In 7th\nInternational Conference on Learning Representations, 2019.\nJiaying Lu, Xin Ye, Yi Ren, and Yezhou Yang. Good, better, best: Textual distractors generation for\nmultiple-choice visual question answering via reinforcement learning. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition Workshops, 2022.\nPietro Mazzaglia, Ozan Catal, Tim Verbelen, and Bart Dhoedt. Curiosity-driven exploration via\nlatent bayesian surprise. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2022.\nYao Mu, Baiyu Peng, Ziqing Gu, Shengbo Eben Li, Chang Liu, Bingbing Nie, Jianfeng Zheng, and\nBo Zhang. Mixed reinforcement learning for efﬁcient policy optimization in stochastic environ-\nments. In 2020 20th International Conference on Control, Automation and Systems (ICCAS), pp.\n1212–1219. IEEE, 2020.\nYao Mu, Yuzheng Zhuang, Bin Wang, Guangxiang Zhu, Wulong Liu, Jianyu Chen, Ping Luo,\nShengbo Li, Chongjie Zhang, and Jianye Hao. Model-based reinforcement learning via imagina-\ntion with derived memory. Advances in Neural Information Processing Systems, 34:9493–9505,\n2021.\nYao Mu, Yuzheng Zhuang, Fei Ni, Bin Wang, Jianyu Chen, HAO Jianye, and Ping Luo. Domino:\nDecomposed mutual information optimization for generalized context in meta-reinforcement\nlearning. In Advances in Neural Information Processing Systems, 2022.\nF. Ni, J. Hao, J. Lu, X. Tong, M. Yuan, J. Duan, Y. Ma, and K. He. A multi-graph attributed\nreinforcement learning based optimization algorithm for large-scale hybrid ﬂow shop scheduling\nproblem. In KDD, pp. 3441–3451, 2021.\nPierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for au-\ntonomous mental development. IEEE transactions on evolutionary computation, 11(2):265–286,\n2007.\nFeiyang Pan, Jia He, Dandan Tu, and Qing He. Trust the model when it is conﬁdent: Masked\nmodel-based actor-critic. In Advances in Neural Information Processing Systems 33, 2020.\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In Proceedings of the 34th International Conference on Machine\nLearning, 2017.\nDeepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.\nIn Proceedings of the 36th International Conference on Machine Learning, 2019.\nBaiyu Peng, Yao Mu, Yang Guan, Shengbo Eben Li, Yuming Yin, and Jianyu Chen. Model-based\nactor-critic with chance constraint for stochastic system. In 2021 60th IEEE Conference on Deci-\nsion and Control (CDC), pp. 4694–4700. IEEE, 2021.\nXue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler.\nASE: large-scale\nreusable adversarial skill embeddings for physically simulated characters. ACM Trans. Graph.,\n41(4):94:1–94:17, 2022.\nKarl Pertsch, Youngwoon Lee, and Joseph J. Lim. Accelerating reinforcement learning with learned\nskill priors. In 4th Conference on Robot Learning, 2020.\n12\nPublished as a conference paper at ICLR 2023\nAske Plaat, Walter A. Kosters, and Mike Preuss. High-accuracy model-based reinforcement learn-\ning, a survey. CoRR, abs/2107.08241, 2021.\nAthanasios S. Polydoros and Lazaros Nalpantidis. Survey of model-based reinforcement learning:\nApplications on robotics. J. Intell. Robotic Syst., 86(2):153–173, 2017.\nSai Rajeswar, Pietro Mazzaglia, Tim Verbelen, Alexandre Pich´e, Bart Dhoedt, Aaron Courville,\nand Alexandre Lacoste. Unsupervised model-based pre-training for data-efﬁcient reinforcement\nlearning from pixels. In Decision Awareness in Reinforcement Learning Workshop at ICML 2022,\n2022.\nReuven Y Rubinstein. Optimization of computer simulation models with rare events. European\nJournal of Operational Research, 99(1):89–112, 1997.\nMax Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bach-\nman. Data-efﬁcient reinforcement learning with self-predictive representations. arXiv preprint\narXiv:2007.05929, 2020.\nMax Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin,\nR. Devon Hjelm, Philip Bachman, and Aaron C. Courville. Pretraining representations for data-\nefﬁcient reinforcement learning. In Advances in Neural Information Processing Systems 34, 2021.\nYounggyo Seo, Kimin Lee, Ignasi Clavera Gilaberte, Thanard Kurutach, Jinwoo Shin, and Pieter\nAbbeel. Trajectory-wise multiple choice learning for dynamics generalization in reinforcement\nlearning. In Advances in Neural Information Processing Systems 33, 2020.\nYounggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. State en-\ntropy maximization with random encoders for efﬁcient exploration. In Proceedings of the 38th\nInternational Conference on Machine Learning, 2021.\nYounggyo Seo, Kimin Lee, Stephen L. James, and Pieter Abbeel. Reinforcement learning with\naction-free pre-training from videos. In Proceedings of the 39th International Conference on\nMachine Learning, 2022.\nRuimin Shen, Yan Zheng, Jianye Hao, Zhaopeng Meng, Yingfeng Chen, Changjie Fan, and Yang\nLiu. Generating behavior-diverse game ais with evolutionary multi-objective deep reinforcement\nlearning. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Con-\nference on Artiﬁcial Intelligence, IJCAI 2020, pp. 3371–3377. ijcai.org, 2020.\nHarshit Sikchi, Wenxuan Zhou, and David Held. Learning off-policy with online planning. In\nConference on Robot Learning, 2021.\nHarshinder Singh, Neeraj Misra, Vladimir Hnizdo, Adam Fedorowicz, and Eugene Demchuk. Near-\nest neighbor estimates of entropy. American journal of mathematical and management sciences,\n23(3-4):301–321, 2003.\nAravind Srinivas and Pieter Abbeel. Unsupervised learning for reinforcement learning. 2021. URL\nhttps://icml.cc/media/icml-2021/Slides/10843_QHaHBNU.pdf.\nRichard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. SIGART\nBull., 2(4):160–163, 1991.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press, 1998.\nKai Tian, Yi Xu, Shuigeng Zhou, and Jihong Guan. Versatile multiple choice learning and its ap-\nplication to vision computing. In IEEE Conference on Computer Vision and Pattern Recognition,\n2019.\nSaran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom\nErez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm control: Software and tasks for\ncontinuous control. Software Impacts, 2020.\nTingwu Wang and Jimmy Ba. Exploring model-based planning with policy networks. In 8th Inter-\nnational Conference on Learning Representations, 2020.\n13\nPublished as a conference paper at ICLR 2023\nGrady Williams, Andrew Aldrich, and Evangelos A. Theodorou. Model predictive path integral\ncontrol using covariance variable importance sampling. CoRR, abs/1509.01149, 2015.\nBohan Wu, Jayesh K. Gupta, and Mykel J. Kochenderfer. Model primitive hierarchical lifelong\nreinforcement learning. In Proceedings of the 18th International Conference on Autonomous\nAgents and MultiAgent Systems, 2019.\nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with proto-\ntypical representations. In Proceedings of the 38th International Conference on Machine Learn-\ning, 2021a.\nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous con-\ntrol: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645, 2021b.\nDenis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improv-\ning sample efﬁciency in model-free reinforcement learning from images. In 35th AAAI Confer-\nence on Artiﬁcial Intelligence, 2021c.\nTianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and Sergey Levine.\nHow to leverage unlabeled data in ofﬂine reinforcement learning. In International Conference on\nMachine Learning, 2022.\nChiyuan Zhang, Oriol Vinyals, R´emi Munos, and Samy Bengio. A study on overﬁtting in deep\nreinforcement learning. CoRR, abs/1804.06893, 2018.\nMarvin Zhang, Sharad Vikram, Laura M. Smith, Pieter Abbeel, Matthew J. Johnson, and Sergey\nLevine. SOLAR: deep structured representations for model-based reinforcement learning. In\nProceedings of the 36th International Conference on Machine Learning, 2019.\nYan Zheng, Zhaopeng Meng, Jianye Hao, Zongzhang Zhang, Tianpei Yang, and Changjie Fan. A\ndeep bayesian policy reuse approach against non-stationary agents. In Samy Bengio, Hanna M.\nWallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.),\nAdvances in Neural Information Processing Systems 31: Annual Conference on Neural Infor-\nmation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pp.\n962–972, 2018.\nYan Zheng, Changjie Fan, Xiaofei Xie, Ting Su, Lei Ma, Jianye Hao, Zhaopeng Meng, Yang Liu,\nRuimin Shen, and Yingfeng Chen. Wuji: Automatic online combat game testing using evolu-\ntionary deep reinforcement learning. In 34th IEEE/ACM International Conference on Automated\nSoftware Engineering, ASE 2019, San Diego, CA, USA, November 11-15, 2019, pp. 772–784.\nIEEE, 2019.\nYan Zheng, Jianye Hao, Zongzhang Zhang, Zhaopeng Meng, Tianpei Yang, Yanran Li, and Changjie\nFan. Efﬁcient policy detecting and reusing for non-stationarity in markov games. Auton. Agents\nMulti Agent Syst., 35(1):2, 2021.\n14\nPublished as a conference paper at ICLR 2023\nA\nENVIRONMENT DETAILS\n(a) Walker\n(b) Quadruped\n(c) Jaco\n(d) Humanoid\nFigure 9: Environment Details. We evaluate our method on four manipulation and locomotion\ndomains. Each domain contains 4 downstream tasks (3 tasks for humanoid).\nThe details of all environments and corresponding tasks are as follows, as illustrated in Fig. 9:\n• Walker1 (Flip, Run, Stand, Walk): Walker is a biped bound to a 2D vertical plane, which\nlearns balancing and locomotion skills.\n• Quadruped (Jump, Run, Stand, Walk): Quadruped also learns various types of locomotion\nskills but is harder because of high-dimensional state and action spaces and 3D environ-\nment.\n• Jaco (Reach bottom left, Reach bottom right, Reach top left, Reach top right): Jaco Arm\nis a 6-DOF robotic arm with a three-ﬁnger gripper, which requires to control the robot\narm and perform simple manipulation tasks. Prior works (Laskin et al., 2021; Yarats et al.,\n2021a) have shown that such reward-sparse manipulation tasks are particularly challenging\nfor unsupervised RL.\n• Humanoid2 (Run, Stand, Walk): Humanoid is a simpliﬁed human-like entity with 21 joints,\nwhich is extremely challenging to learn locomotion skills due to high-dimensional action\nspaces and exploration dilemma. We test ﬁne-tuning performance on relatively few inter-\naction steps (150k) whereas prior work typically executes for 30M steps (200x) learning\nfrom scratch.\nB\nDETAILS OF UNSUPERVISED RL BASELINES\nEUCLID can simply combine with various categories of URL exploration approaches as backbone.\nWe select several typical exploration algorithms as baselines.\nDisagreement (Pathak et al., 2019): Disagreement is a knowledge-based baseline which trains an\nensemble of forward models {gi (zt+1 | zt, at)} to predict the feature. Intrinsic rewards are deﬁned\nas the variance among the ensemble models:\nrDisagreement\nt\n∝Var {gi (zt+1 | zt, at)}\ni = 1, . . . , N.\n(6)\nAPT (Liu & Abbeel, 2021b): Active Pre-training (APT) is a data-based baseline which utilizes a\nparticle-based estimator (Singh et al., 2003) that uses K nearest-neighbors to estimate entropy for\na given state. Since APT needs the auxiliary representation learning to provide the latent variables\nfor entropy estimation, we implement of APT on the top of EUCLID encoder optimized by latent\nconsistency loss. Intrinsic reward are computed as:\nrAPT\nt\n∝\nk\nX\ni\nlog ∥zt −zk∥2 .\n(7)\n1https://github.com/rll-research/url_benchmark\n2https://github.com/deepmind/dm_control/blob/main/dm_control/suite/\nhumanoid.py\n15\nPublished as a conference paper at ICLR 2023\nDIAYN (Eysenbach et al., 2019): Diversity is All you need (DIAYN) is a competence-based base-\nline which learn explicit skill w and maximizes the mutual information between states and skills\nI(wt, zt). DIAYN decompose the mutual information via I(zt; wt) = H(wt) −H(wt | zt). The\nﬁrst term is sampled by a random prior distribution H(wt) and maximizes the entropy while the\nlatter term is estimated by the discriminator log q(wt | zt). Intrinsic reward are computed as:\nrDIAYN\nt\n∝log q(wt | zt) + const.\n(8)\nCIC (Laskin et al., 2022): CIC is a hybrid data-based and competence-based as the previous state-\nof-the-art method. CIC that uses an intrinsic reward structure based on particle entropy similar to\nAPT and distill behaviors into skills using contrastive learning.\nC\nFULL RESULTS ON THE URLB\nThe full results of ﬁne-tuning for 100k frames for each task and each method are presented in Table 2\non the state-based URLB. For Disagreement, APT and DIAYN, we utilize the results presented in\nthis Laskin et al. (2021).\nTable 2: Full results of pre-training for 2M and ﬁne-tuning for 100k steps on the state-based URLB.\nDomain\nTask\nDisagreement\nAPT\nDIAYN\nCIC\nTDMPC@100k\nEUCLID\n(Disagreement)\nw/o MCL\nEUCLID\n(APT)\nw/o MCL\nEUCLID\n(DIAYN)\nw/o MCL\nEUCLID\nwalker\nFlip\n491±21\n477±16\n381±17\n631±34\n930±28\n971±1\n967±3\n923±34\n969±2\nRun\n444±21\n344±28\n242±11\n486±25\n750±4\n765±10\n744±9\n793±7\n770±9\nStand\n907±15\n914±8\n860±26\n959±2\n940±22\n985±1\n987±1\n970±5\n985±1\nWalk\n782±33\n759±35\n661±26\n885±28\n967±2\n967±4\n974±2\n967±2\n972±1\nQuadruped\nJump\n668±24\n462±48\n578±46\n595±42\n723±98\n840±13\n792±5\n807±34\n858±14\nRun\n461±12\n339±40\n415±28\n505±47\n465±83\n651±35\n589±13\n571±19\n735±16\nStand\n840±33\n622±57\n706±48\n761±54\n765±92\n953±6\n911±31\n918±30\n958±5\nWalk\n721±56\n434±64\n406±64\n723±43\n710±77\n874±42\n864±21\n859±33\n925±6\nJaco\nReach bottom left\n134±8\n88±12\n17±5\n138±9\n168±7\n214±5\n207±6\n190±10\n220±3\nReach bottom right\n122±4\n115±12\n31±4\n145±7\n183±11\n205±9\n213±6\n179±11\n212±2\nReach top left\n117±14\n112±11\n11±3\n153±7\n178±11\n197±23\n204±7\n192±10\n225±5\nReach top right\n140±47\n136±5\n19±4\n163±4\n172±24\n219±7\n204±7\n197±4\n229±6\nIn Table 2, we ﬁnd that the best approach is to use EUCLID with multi-choice learning and with\ndisagreement as the backbone (EUCLID (Disagreement) with MCL) and we refer to EUCLID for\nthis default setting. EUCLID versions constructed based on each of the explored approaches signif-\nicantly improve the performance of the corresponding baseline.\nD\nIMPLEMENTATION DETAILS\nWe provide richer implementation details of EUCLID in this section.\nD.1\nDEEP DETERMINISTIC POLICY GRADIENT (DDPG)\nFor continuous action space, we opt for DDPG (Lillicrap et al., 2016) as our base optimization\nalgorithm. DDPG is an actor-critic off-policy algorithm, which update critics Qθ by minimizing the\nBellman error:\nLQ(θ, D) = E(st,at,rt,st+1)∼D\n\u0002\u0000Qθ (st, at) −y(s))2\u0003\n,\n(9)\nwhere the Q-target y(s) = R(s, a) + γQ¯θ (st+1, πφ (st+1)), D is a replay buffer and ¯θ is an slow-\nmoving average of the online critic parameters. At the same time, we learn a policy πφ that maxi-\nmizes Qθ by maximizing the objective:\nLπ(φ, D) = Est∼D [Qθ (st, πφ (st))] .\n(10)\nD.2\nHYPER-PARAMETERS\nEUCLID studies various categories of exploration algorithms as a backbone in the experiments, and\nwe list the individual hyper-parameters of each method in Table 3. As for the world models and\npolicy, most of the parameters remain the same as in the original TOLD of TDMPC (Hansen et al.,\n16\nPublished as a conference paper at ICLR 2023\nTable 3: Hyper-parameters of the exploration algorithms used in our experiments.\nDisagreement hyper-parameter\nValue\nEnsemble size\n5\nForward net\n(|O| + |A|) →1024\n→1024→|O| ReLU MLP\nAPT hyper-parameter\nValue\nRepresentation dim\n1024\nReward transition\nlog(r + 1.0)\nk in NN\n12\nAvg top k in NN\nTrue\nDIAYN hyper-parameter\nValue\nSkill dim\n16\nSkill sampling frequency\n50\nDiscriminator net\n512→1024\n→1024→16 ReLU MLP\n2022). We list the hyper-parameters of EUCLID in Table 4, with particular reference to the fact that\nsome of them change during the pre-training phase (PT) and the ﬁne-tuning phase (FT). Following\nprior work (Hafner et al., 2020b; Hansen et al., 2022), we use a task-speciﬁc action repeat hyper-\nparameter for URLB based on DMControl, which is set to 2 by default while 4 for the Quadruped\ndomain. During the pre-training process, we increase the number of policies and prediction heads of\nthe dynamics model one after another based on speciﬁc snapshot time steps, the detailed parameters\nare shown in the Table 5.\nD.3\nCOMPUTE RESOURCES\nWe conducted our experiments on an Intel(R) Xeon(R) Platinum 8171M CPU @ 2.60GHz processor\nbased system. The system consists of 2 processors, each with 26 cores running at 2.60GHz (52 cores\nin total) with 32KB of L1, 1024 KB of L2, 40MB of uniﬁed L3 cache, and 250 GB of memory.\nBesides, we use a single Nvidia RTX3090 GPU to facilitate the training procedure. The operating\nsystem is Ubuntu 16.04. Totally, we conduct experiments on 2500 seeds in 15 downstream tasks.\nE\nADDITIONAL ANALYSIS OF ABLATION\nWalker\nQuadruped\nJaco\nOverall\n60\n70\n80\n90\n100\n110\nNormalized Return (%)\nDefault\nw/o PT Reward\nw/o PT Critic\nw/o PT Model\nw/o PT Actor\nFigure 10: Ablation. Comparison with\nagents based on different pre-trained\ncomponents for ﬁne-tuning. The results\nare based on the average performance\nof three types of exploration backbone\nwith 2M steps of pre-training.\nThe results of the complete ablation experiment for each\nenvironment are shown in Fig. 10. For the generaliza-\ntion ability of experiments, the results are based on the\naverage performance of three types of exploration back-\nbone with 2M steps of pre-training and 100k steps of ﬁne-\ntuning (A total of 720 runs = 3 algorithms × 12 tasks × 5\nseeds × 4 settings).\nOverall, our default settings, i.e. reusing all components\nof the world models (encoder, dynamics model, reward\nmodel) and policies (actor and critic) works best. The\nmost important modules in EUCLID are dynamics mod-\nels and actor as these two always get a performance gain\nin all tasks, whereas reusing critic and reward predictor\ncan improve performance only in some domains.\nUsing pre-trained policies to initialize downstream policies shows signiﬁcant performance in\nQuadruped and Jaco domains, which illustrates the synergistic effect of model based planning and\npre-trained policy. Exploration policies trained by intrinsic rewards during the pre-training phase\nusually exhibit behaviours that are beneﬁcial for downstream tasks, e.g. agents in the Quadruped\n17\nPublished as a conference paper at ICLR 2023\nTable 4: Hyper-parameters of the world model, policy, and planner.\nWorld model\nValue\nBatch size\n1024\nMax buffer size\n1e6\nLatent dim\n50 (default)\n100 (Humanoid)\nMLP hidden dim\n256 (Encoder)\n1024 (otherwise)\nMLP activation\nELU\nOptimizer (θ)\nAdam\nLearning rate\n1e-4 (PT)\n1e-3 (FT)\nReward loss coefﬁcient (c1)\n0.5\nConsistency loss coefﬁcient (c2)\n2\nValue loss coefﬁcient (c3)\n0.1\nθ−update frequency\n2\nPolicy\nValue\nSeed steps\n0 (PT)\n4000 (FT)\nDiscount factor (γ)\n0.99\nAction repeat\n2 (default)\n4 (Quadruped)\nPlanning (Only for FT phase)\nValue\nIteration\n6\nPlanning horizon (L)\n5\nCEM population size\n512\nCEM elite fraction\n12\nCEM policy fraction (Policy/CEM)\n0.05\nCEM Temperature\n0.5\nTable 5: hyper-parameters of multi-choice learning mechanism\nMCL hyper-parameter\nValue\nNum of prediction heads (H)\n4\nRegularization strength (α)\n0.1\nspeciﬁc interval time steps T\n500k\ndomain trained by the Disagreement can exhibit behaviours such as tumbling and wobbly stand-\ning, while randomly initialised agents only fall and shake. Therefore, using such policies to guide\nthe planning process during the ﬁne-tuning phase can help the agents reduce exploration cost and\nincrease sampling efﬁciency which also demonstrates the beneﬁts of a hybrid strategy.\nSince the state transition of the environment are shared during the pre-training and ﬁne-tuning\nphases, it makes sense that pre-trained dynamics model would be more accurate than a model initial-\nized randomly. Moreover, we empirically ﬁnd that reusing both the critic and reward predictor at the\nbeginning of FT phase can improve overall performance, although the intrinsic reward which reward\npredictor predicts in pre-training phase is quite different from the expected extrinsic return given by\ndownstream tasks. Thus, we still maintain the reuse of critic and reward predictor by default and\nleave whether the pre-training of critic and reward predictor can beneﬁt for downstream tasks with\na theoretical guarantee as an open question.\n18\nPublished as a conference paper at ICLR 2023\nF\nPSEUDO CODES OF EUCLID\nWe show the full process of EUCLID, including both pre-training and ﬁne-tuning phases, in Algo-\nrithm 1.\nAlgorithm 1: Efﬁcient Unsupervised Reinforcement Learning Framework with Multi-choice\nDynamics Model (EUCLID)\n1 Input: speciﬁc interval time steps T , prediction head size H, regularization strength α, current\npolicy size h = 0, ensemble policy set S = ∅.\n2 Require: Initialize all networks: Encoder Eθ, Multi-headed dynamics Dθ, Reward predictor\nRθ, Critic Qθ, Actor πφ, Replay buffer D.\n3 Require: Environment (env), M downstream tasks Tk, k ∈[1, . . . , M].\n4 Require: Intrinsic reward rint, extrinsic reward rext.\n5 Require: pre-train NPT = 2M and ﬁne-tune NFT = 100K steps.\n6 # Part 1 : Unsupervised Pre-training\n7 for t = 0, 1, ..NPT do\n8\nif t == T ∗h then\n9\nInitialize πh\nφ with weights of πφ, h ←h + 1\n10\nExtend ensemble policy set S ←S ∪πh\nφ\n11\nUpdate average policy distribution eπφ (zt) = Ph\ni=1 πi\nφ (zt) /h\n12\nEncoder state zt = Eθ (st) and sample action at ∼πθ (zt)\n13\nApply action to the environment st+1 ∼P (· | st, at)\n14\nAdd transition to replay buffer D ←D ∪(st, at, st+1)\n15\nSample a minibatch from replay buffer D, compute intrinsic reward rint with exploration\nbackbone\n16\nUpdate encoder, reward predictor, critic and dynamics model parameters θ with prediction\nhead h\n▷see Eq. 3\n17\nUpdate actor φ by RL loss with additional diversity encouraging term\n▷see Eq. 5\n18 # Part 2 : Supervised Fine-tuning\n19 for Tk ∈[T1, . . . , TM] do\n20\nInitialize all networks with weights from the pre-training phase and an empty replay buffer\nD\n21\nSelect the most appropriate prediction head h∗with the highest zero-shot reward\n22\nFix head h∗and corresponding policy πh∗\nφ\n23\nInitialize ﬁne-tuning actor πFT\nφ with weights of πh∗\nφ\n24\nfor t = 1..NFT do\n25\nEncoder state zt = Eθ (st) and select action through planning at = Plan(zt) guided by\nπFT\nφ\n26\nApply action to the environment st+1, rext\nt\n∼P (· | st, at)\n27\nAdd transition to replay buffer D ←D ∪(st, at, rext\nt , st+1)\n28\nUpdate encoder, reward predictor, critic and dynamics model parameters θ with ﬁxed\nprediction head h∗\n▷see Eq. 3\n29\nUpdate actor φ only by RL loss\n▷see Eq. 10\n30\nEvaluate performance of RL agent on task Tk\nG\nDETAILS OF MULTI-CHOICE LEARNING\nTo stabilize the training process, we ﬁxed the equal number of ensemble policies and prediction\nheads of the multi-headed dynamics model as hyper-parameters in advance, and corresponded the\npolicies to the prediction heads one by one. Our core intuition is that diverse policies yield diverse\ndata distributions, and we use the different data distributions to train the model primitives separately\nto obtain a set of sub-optimal dynamics models with their speciﬁed region.\nIn the beginning of the PT stage, we initialize an empty set of policies S = ∅and a dynamics model\nDθ with H prediction head. We extend ensemble policy set S ←S ∪πh\nφ, h ∈1..H at every speciﬁc\n19\nPublished as a conference paper at ICLR 2023\ninterval time steps T for total 2M pre-training steps where network parameters of πh\nφ is initialized\nby the weights of current πφ.\nTo allow ensemble policies to be as diverse as possible, we calculate the current average policy\ndistribution eπφ each time we add a member to the ensemble and design policy diversity encour-\naging terms DKL (eπφ (zt) ∥πφ (zt)) to encourage diversity of new policy. We balance each policy\nof ensemble between the exploration and the diversity objectives that each model primitive models\ndifferent skill space. After a complete PT phase, we are able to obtain the multi-headed dynamics\nmodel and the corresponding set of policies [π1\nφ(z), · · · , πh\nφ(z)].\nIn the beginning of FT phase, we should select one head that can beneﬁt the downstream learning the\nmost. We follow the same procedure for competence-based method adaptation as in URLB (Laskin\net al., 2021). During the ﬁrst 4k steps, the zero-shot evaluation that we use is to interact with the\nenvironment for a whole episode by each prediction head and the corresponding pre-trained policy,\nget the episode extrinsic reward and select the prediction head with the largest reward as the expert.\nAfter this, we ﬁx the prediction head h∗and ﬁnetune world model parameters for the remaining\nsteps.\nH\nDETAILS OF EUCLID MIXTURE PLANNING\nFollowing TDMPC (Hansen et al., 2022), we leverage imaginary trajectories both for planning and\npolicy gradient. As for planning, we perform Model Predictive Control (Bemporad & Morari, 1999)\nmethod MPPI (Williams et al., 2015) to update parameters for a family of distributions using an\nimportance weighted average of the estimated top-k sampled trajectories of expected return. Specif-\nically, we use the cross entropy method (CEM) (Rubinstein, 1997) to optimize action sequences by\niteratively re-sampling action sequences near the best performing sequences from the last iteration.\nIt is worth noting that we plan at each decision step t and execute only the ﬁrst action. In addition,\nlong-term model planning is computational costly and inaccurate, and model errors accumulate with\nhorizon length (Lai et al., 2020). Therefore, we use short-term reward estimates generated by the\nlearned model and use the Q value function for long-term return estimates via bootstrapping. At the\nsame time, we hope that the pre-trained policy can guide the planning process and improve control\nperformance. For this, we additionally mix the trajectory samples generated by policy with the plan-\nning trajectories to guarantee the optimization when model is not that accurate. The detail pseudo\ncode is given in Algorithm 2.\nAlgorithm 2: Policy Guided Planning Algorithm\n1 Require: Encoder Eθ, Latent dynamics Dθ, Reward predictor Rθ, Critic Qθ, Actor πφ.\n2 Require: Initial parameters for N(µ0, (σ0)2).\n3 Require: Planning trajectories num N and policy trajectories num Nπ.\n4 Require: Current state st, rollout horizon L, iteration num J, elite trajectories num k.\n5 Encoder state zt = Eθ (st)\n▷trajectory starting state\n6 # Iterate J rounds starting from initial distribution N(µ0, (σ0)2)\n7 for each iteration j = 1, ..J do\n8\nSample N trajectories of length L from N\n\u0010\nµj−1,\n\u0000σj−1\u00012\u0011\n9\nSample Nπ trajectories of length L using Dθ and πφ\n10\nEstimating the cumulative discounted rewards for all trajectories, the ﬁrst L steps are\nestimated using the reward predictor Rθ and thereafter using Qθ\n11\nSelect top-k elite trajectories based on cumulative rewards\n12\nUpdate µ, σ by top-k sampled trajectories using MPPI (Williams et al., 2015)\n13 return a ∼N\n\u0010\nµJ,\n\u0000σJ\u00012\u0011\nI\nANALYSIS OF MODEL-BASED RL PERSPECTIVE\nIn the model-based RL community, how to collect data for model ﬁtting is the general challenge.\nMost of the traditional model-based RL (Zhang et al., 2019; Hafner et al., 2019) approaches use task\nbehavior policies to obtain data and train the world models. Some approaches enhance the breadth of\n20\nPublished as a conference paper at ICLR 2023\nthe model by designing additional intrinsic reward-driven goals as auxiliary tasks for learning to ob-\ntain more diverse samples (Seo et al., 2021; Mazzaglia et al., 2022). However, these approaches still\nface the problem of poor transferability, which requires complete retraining when faced with similar\ndownstream tasks. And we focus on how to improve the sample efﬁciency of model-based RL in the\nperspective of unsupervised RL and ﬁt general world models for multiple downstream tasks through\nreasonable reward-free pre-training and ﬁne-tuning paradigms. EUCLID stands for a PT and FT\nperspective, with multiple choice learning and a corresponding policy diversity-encouraging term,\nhoping to train a set of submodels with separate expert regions and select the most suitable model for\nthe downstream task. In this way, we can reuse the pre-trained model many times, adapting quickly\nusing fewer samples, rather than learning from scratch.\nJ\nADDITIONAL EXPERIMENTS\nJ.1\nABLATION OF MULTI-CHOICE LEARNING IN HUMANOID DOMAIN\n0\n30\n60\n90\n120\n150\nEnvironment Steps (×10³)\n0\n100\n200\n300\n400\nEpisode Return\nEUCLID\nEUCLID w/o MCL\n(a) Humanoid-Stand\n0\n30\n60\n90\n120\n150\nEnvironment Steps (×10³)\n0\n100\n200\n300\nEpisode Return\n(b) Humanoid-Walk\n0\n30\n60\n90\n120\n150\nEnvironment Steps (×10³)\n40\n60\n80\n100\nEpisode Return\n(c) Humanoid-Run\nFigure 11: Learning curves of EUCLID and EUCLID w/o MCL on downstream tasks of the hu-\nmanoid domain. Curves show the mean and 95% conﬁdence intervals of performance across 5\nindependent seeds. The dashed reference lines are the asymptotic performance of the Disagreement\nalgorithm with 2M PT steps and 2M FT steps.\nAs shown in Fig. 11, we ablate and evaluate the role of the MCL mechanism in the humanoid\ndomain. Experiments show that the MCL mechanism can achieve a stable improvement in humanoid\ndomain, which certiﬁes the positive effect of the MCL mechanism in complex reward functions and\ncomplex environments.\nJ.2\nSTATE PREDICTION ERROR REGRESSION ANALYSIS\n0\n2\n4\n6\n8\n10\nTraining Iterations (×10³)\n2\n4\n6\n8\n10\n12\nPrediction Error\nQuadruped-run\nPre-trained w/ MCL\nPre-trained w/o MCL\nRandomly Initialized\nFigure 12: We collect expert data for\nquadruped-run tasks to train a dynam-\nics model and report the prediction error\ncurves on the held-out dataset. Curves\nshow the mean and 95% conﬁdence in-\ntervals of performance across 5 inde-\npendent seeds.\nTo further verify the effectiveness of pre-training and\nMCL mechanism for dynamics model, we collected ex-\npert data for 30 episodes (30000 steps) of quadruped-\nrun tasks.\nThen, we train a dynamics model on\nthe pre-collected quadruped dataset.\nIn order to ex-\nclude other factors, we removed the encoder module in\nthis experiment, and the prediction loss is deﬁned by\n∥Dθ(st, at) −st+1∥2\n2. Fig. 12 show the prediction er-\nror curves for up to 10000 training iteration, including\nrandomly initialized, pre-trained with and without multi-\nchoice learning using EUCLID.\nFirstly, we ﬁnd that pre-trained dynamics models have\nsmaller prediction errors than randomly initialization, es-\npecially better when using multi-choice learning. Sec-\nondly, the pre-trained dynamics models converge more\nquickly. Thirdly, When the number of training iterations\nis large enough, a single general pre-trained dynamics\nmodel (without MCL) and a randomly initialized dynam-\nics model eventually converge to similar prediction er-\nror magnitude, while a multi-headed dynamics model us-\ning the MCL mechanism (selecting speciﬁc head) signiﬁ-\n21\nPublished as a conference paper at ICLR 2023\ncantly reduces the ﬁnal prediction error. This indicates that EUCLID is able to predict the dynamics\ncorresponding to the downstream task more accurately.\nJ.3\nEXTENSION EXPERIMENTS IN THE PIXEL-BASED URLB\nTo further demonstrate that EUCLID is a general and effective framework that can signiﬁcantly\naccelerate the learning efﬁciency of downstream tasks through pre-training, we conducted additional\npixel-based URLB experiments on four tasks. As for baseline in model-free manner, we opt for\nDrQ-v2 (Yarats et al., 2021b) (which can be simply viewed as DDPG + Image Augmentation for\nimage inputs) as our base optimization algorithm to learn from images. In the pre-training phase,\nwe consistently use disagreement as the backbone of exploration. The results shown in Table 6\nare means over 3 seeds. In the table, TDMPC@100k and DrQ-v2@100k represent agents with\n100k training steps and without pre-training. DrQ-v2 (Disagreement) and EUCLID (Disagreement)\nrepresent agents after 500k reward-free pre-training and 100k ﬁne-tuning. The results show that\nEUCLID can also lead signiﬁcant performance improvement in visual control tasks.\nTable 6: Comparisons on the URLB with image inputs. Results for DrQ-v2@100k and DrQ-v2 (Dis-\nagreement) are obtained from (Laskin et al., 2021).\nTask\nDrQ-v2@100k\nDrQ-v2\n(Disagreement)\nTDMPC@100k\nEUCLID\n(Disagreement)\nWalker-Flip\n81±23\n360±16\n403±30\n622±37\nWalker-Run\n41±11\n131±19\n201±15\n323±41\nWalker-Stand\n212±28\n398±65\n793±40\n950±15\nWalker-Walk\n141±53\n348±46\n573±43\n743±38\n22\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2022-10-02",
  "updated": "2023-02-22"
}