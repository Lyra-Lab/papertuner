{
  "id": "http://arxiv.org/abs/2210.00770v1",
  "title": "Accelerate Reinforcement Learning with PID Controllers in the Pendulum Simulations",
  "authors": [
    "Liping Bai"
  ],
  "abstract": "We propose a Proportional Integral Derivative (PID) controller-based coaching\nscheme to expedite reinforcement learning (RL).",
  "text": "1\nCoaching with PID Controllers: A Novel Approach\nfor Merging Control with Reinforcement Learning\nLiping Bai\nAbstract—We propose a Proportional Integral Derivative (PID)\ncontroller-based coaching scheme to expedite reinforcement learn-\ning (RL). Previous attempts to fuse classical control and RL are\nvariations on imitation learning or human-in-the-loop learning.\nAlso, in their approaches, the training acceleration comes with\nan implicit cap on what is attainable by the RL agent, therefore it\nis vital to have high-quality controllers. We ask if it is possible to\naccelerate RL with even a primitive hand-tuned PID controller,\nand we draw inspiration from the relationship between athletes\nand their coaches. At the top level of the athletic world, a coach’s\njob is not to function as a template to be imitated after, but rather\nto provide conditions for the athletes to collect critical experiences.\nWe seek to construct a coaching relationship between the PID\ncontroller and the RL agent, where the controller helps the agent\nexperience the most pertinent states for the task. We conduct\nexperiments in Mujoco locomotion simulation, but the setup can\nbe easily converted into real-world circumstances. We conclude\nfrom the data that when the coaching structure between the PID\ncontroller and its respective RL agent is set at its goldilocks spot,\nthe agent’s training can be accelerated by up to 37%, yielding\nuncompromised training results in the meantime. This is an\nimportant proof of concept that controller-based coaching can\nbe a novel and effective paradigm for merging classical control\nwith learning and warrants further investigations in this direction.\nAll the code and data can be found at github/BaiLiping/Coaching\nIndex Terms—Reinforcement Learning, Control, Learning for\nDynamic Control, L4DC\nI. INTRODUCTION\nL\nEARNING for Dynamic Control is an emerging ﬁeld of\nresearch located at the interaction between classic control\nand reinforcement learning (RL). Although RL community\nroutinely generate jaw-dropping results that seem out of reach\nto the control community[1][2][3], the theories that undergird\nRL are as bleak as it was ﬁrst introduced[4]. Today, those\ndeﬁciencies can be easily papered over by the advent of\nDeep Neural Networks (DNN) and ever faster computational\ncapacities. For RL to reach its full potential, existing control\ntheories and strategies have to be part of that new combined\nformulation.\nThere are three ways that classic control ﬁnds its way\ninto RL. First, theorists who are well versed in optimiza-\ntion techniques and mathematical formalism can provide\nsystematic perspectives to RL and invent the much needed\nanalytical tools[5][6][7][8][9]. Second, system identiﬁcation\nresearchers are exploring all possible conﬁgurations to com-\nbine existing system models with DNN and its varia-\ntions[10][11][12][13][14]. Third, proven controllers can provide\nNanjing Unversity of Posts and Telecommunications, College of Automa-\ntion & College of Artiﬁcial Intelligence, Nanjing, Jiangsu,210000 China\nemail:zqpang@njupt.edu.cn\ndata on successful control trajectories to be used in imitation\nlearning, reverse reinforcement learning, and \"human\"-in-the-\nloop learning[15][16][17][18][19].\nOur approach is an extension of the third way of combining\nclassing control with RL. Previous researches[20][21][22]\nare about making a functioning controller works better. To\nbegin with, they require high-quality controllers, and the\nimprovements brought about by the RL agents are merely\nicing on the cake. In addition, the controllers inadvertently\nimpose limits on what can be achieved by the RL agents. If,\nunfortunately, a bad controller is chosen, then the RL training\nprocess would be hindered rather than expedited. We ask the\nquestion, can we speed up RL training with hand-tuned PID\ncontrollers, which are primitive but still captures some of our\nunderstanding of the system? This inquiry leads us to the\nrelationship between athletes and their coaches.\nProfessional athletes don’t get where they are via trial-and-\nerror. Their skillsets are forged through painstakingly designed\ncoaching techniques. At the top level, a coach’s objective is\nnot to be a template for the athletes to imitate after, but rather\nis to facilitate data collection on critical states. Top athletes\nare not necessarily good coaches and vice versa.\nIn our approach, the ’coaches’ are PID controllers which\nwe deliberately tuned to be barely functioning, as shown by\nTable I. Yet, even with such bad controllers, when appropriately\nstructured, training acceleration is still observed in our experi-\nments, as shown by Table II. The critical idea of coaching is for\nthe PID controllers to take over when the RL agents deviated\nfrom the essential states. Our approach differs from previous\nresearches in one signiﬁcant way: controllers’ interventions\nand the rewards associated with such interventions are hidden\nfrom the RL agents. They are not part of the training data. We\nalso restrain from reward engineering and leave everything as\nit is, other than the coaching intervention. This way, we can\nbe conﬁdent that the observed acceleration does not stem from\nother alterations. The implementations would be detailed in\nsubsequent sections.\nEnvironment\nPID Controller\nRL Agent\nPID/RL\nInverted Pendulum\n240\n1000\n24.0%\nDouble Pendulum\n1107\n9319\n11.9%\nHopper\n581\n989\n58.7%\nWalker\n528\n1005\n52.5%\nTABLE I: Performance Comparison between PID controller and its respective RL agent.\nWe interfaced with Mujoco simulation through OpenAI GYM, and every simulated\nenvironment comes with predetermined maximum episode steps. The scores achieved\nby the RL agents would probably be high if not for this reason.\narXiv:2210.00770v1  [eess.SY]  3 Oct 2022\n2\nEnvironment\nTarget\nMeasure\nWith PID\nWithout\nPercentage\nName\nScore\nCoaching\nCoaching\nIncrease\nInverted\n800\nWin Streak\n100\n160\n37.5%\nPendulum\nAverage\n104\n159\n34.6%\nDouble\n5500\n5 Wins\n908\n1335\n31.9%\nPendulum\nAverage\n935\n1370\n29.9%\nHopper\n800\n5 Wins\n2073\n2851\n27.3%\nAverage\n2155\n2911\n25.9%\nWalker\n800\n5 Wins\n4784\n5170\n7.5%\nAverage\n5659\n7135\n20.7%\nTABLE II: Comparison Between Agents Trained With and Without PID Controller\nCoaching. Even though the PID controllers are less capable than the eventual RL agent,\nthey are still useful and can accelerate the RL agent training. There two measures we\nused to gauge training acceleration. The ﬁrst is ﬁve consecutive wins, and the second is\nthe scoring average. The \"win\" is a predetermined benchmark.\nIn section II, we present the idea of controller-based coaching.\nIn section III, we present the results of our experiments and\ntheir detailed implementations. We conclude what we have\nlearned and layout directions for further research in section IV.\nII. CONTROLLER BASED COACHING\nReinforcement Learning is the process of cycling between\ninteraction with an environment and reﬁnement of the under-\nstanding of that environment. RL agents methodically extract\ninformation from experiences, gradually bounding system\nmodels, policy distributions, or cost-to-go approximations\nto maximize the expected rewards along a trajectory, as\nshown by Figure1 which is an adaption of Benjamin Recht’s\npresentation[23].\nFig. 1: From Optimization to Learning. Model-Based or Model-Free learning refers\nto whether or not learning is used to approximate the system dynamics function. If\nthere is an explicit action policy, it is called on-policy learning. Otherwise, the optimal\naction would be implicitly captured by the Q value function, and that would be called\noff-policy learning instead. Importance sampling allows \"limited off-policy learning\"\ncapacity, which enables data reuse in a trusted region. Online learning means interleaving\ndata collection and iterative network parameters update. Ofﬂine learning means the\ndata is collected in bulk ﬁrst, and then the network parameters are set with regression\ncomputation. Batch learning, as the name suggested, is in between online and ofﬂine\nlearning. An agent would ﬁrst generate data that ﬁll its batch memory and then sample\nfrom the batch memories for iterative parameter update. New data would be generated\nwith the updated parameters to replace older data in the memory. This taxonomy is\nsomewhat outdated now. When Richard Sutton wrote his book, the algorisms he had in\nmind fall nicely into various categories. Today, however, the popular algorisms would\ncombine more than one route to derive superior performance and can’t be pigeonholed.\nA fundamental concept for RL is convergence through\nbootstrap. Instead of asymptotically approaching a known target\nfunction2a, bootstrap methods approach an assumed target ﬁrst\nand then update the target assumption based on collected data2b.\nWhen evaluating estimation functions with belief rather than\nof the real value, things could just run around in circles and\nnever converge. Without any guidance, the RL agent would\n(a) With Known Evaluation Function\n(b) Bootstrap\nhave just explored all the possible states, potentially resulting\nin this unstable behavior.\nOne method to produce more efﬁcient exploration and avoid\ninstability is to give more weight to critical states. Not all\nobservational states are created equal. Some are vital, while\nothers have nothing to do with the eventual objective. For\ninstance, in the inverted pendulum task, any states outside of\nthe Lyapunov stability bounds should be ignored since they\ncan’t be properly controlled anyway.\nThere are statistical techniques to distinguish critical states\nfrom the non-essential ones, and imitation learning works\nby marking crucial states with demonstrations. However, the\nformer approach is hard to implement, and the latter one\nrequires high-quality controllers. Our proposed controller-based\ncoaching method is easy to implement and does not have\nstringent requirements on the controllers it uses.\nController-based coaching works by adjusting the trajectory\nof the RL agent and avoid wasting valuable data collection\ncycle on the states that merely lead to eventual termination.\nWhen the agent is about to deviate from essential states, the\ncontroller will apply a force to nudge the agent back to where\nit should be, much like a human coach course-corrects on\nbehalf of the athlete. Crucially, the agent is oblivious to this\nintervention step, and it would not be part of the agent’s\ndata. Even if the controller didn’t adjust the agent to where it\nshould be, it would not have mattered since it is unaware of it\nbecause it is a high-quality controller. On the other hand, if the\ncontroller successfully adjusts the trajectory, the RL agent’s\nnext data collection cycle will be spent in a critical state. We\ntest our approach on four mujoco locomotion environments as a\nproof of concept, and in all four experiments, the hypothesized\nacceleration on RL training is observed.\nIII. EXPERIMENT SETUP\nMujoco physics engine[24], is one of many such simulation\ntools. We interface with it through a python wrapper provided\nby the OpenAI Gym[25] team. We choose four environments\nfor our experiments: inverted pendulum, double inverted\npendulum, hopper, and walker. Every environment comes with\na set of predetermined rewards and maximum episode steps.\nWe did not tinker with those parameters. The only change we\nmade to each environment is a controller-based coach ready\nto intervene when the agent steps out of the predetermined\ncritical states.\nWe use tensorforce’s[26] implementation of RL agents,\nspeciﬁcally the Proximal Policy Optimization (PPO) agent be-\ncause the learning curves generated by PPO agent are smoother,\n3\nFig. 3: Inverted Pendulum system controlled by RL agent and its PID coach. The left\nplot is RL agent and the right plot is the PID controller. The maximum episode steps are\n1000 and each step without termination is scored 1 point. The average score achieved\nby the RL agent is 1000 and the average score achieved by the PID controller is 240.\nas shown by the spinning up[27] team. Our paper aims to\nindicate the controller-based coaching method’s feasibility, and\na smoother curve makes things easier.\nIn this paper, human judgment is the basis for the determina-\ntion of critical states. In future works, we would like to provide\na more systematic process for critical states’ demarkation. We\nwill provide our reasonings when we discuss our experiments\nin each environment.\nOur experiments’ code and data can be accessed via this\ndeposite. The folders are named after each environment, and in\neach folder, you will ﬁnd a data record, an RL agent record, a\nagent.json ﬁle that indicates all the hyperparameters of the RL\nagent, a Normal ﬁle that trains an RL agent with the normal\nmethod, a Coached ﬁle which trains an agent based on the\ncontroller based coaching method, a PID ﬁle which is the PID\ncoach. In the PIDvsRL folder, you will ﬁnd ﬁles that generate\nall the plots shown in the following section.\nA. Inveretd Pendulum\nThe observation space of the inverted pendulum environment\nis: [Cart Position on X-axis, Cart Velocity, Pole Angle, Pole\nAngular Velocity]. The continuous action space is an action\nranging from -10 to 10, where -10 means the actuator moves the\ncart to the left with maximum power and 10 means the actuator\nmoves the cart to the right with full force. The maximum\nepisode step number is 1000. The terminal state for the inverted\npendulum is an angle of absolute value greater than 0.2 radius.\nThe reward is 1 for each non-terminal step and 0 for the\nterminal step.\nFigure3 shows how the RL agent and the PID controller\nmanage the pole angle and the angular velocity. The left plot\nis the RL agent, and the right plot is the PID controller\ncoach. While the PID controller tries hard to converge to\nzero, eventually, there would be too much accumulation on the\nintegral term, and the equilibrium breaks down. The average\nscore achieved by the RL agent is 1000, and the average score\nachieved by the PID controller is 240.\nBased on our observation of the system, we decided to put\nthe boundary between critical and noncritical states on the\nangular velocity of the absolute value of 0.4. The RL agent is\nfree to explore with the angular velocity with an absolute value\nsmaller than 0.4, but once it goes above this bound, the PID\ncontroller will kick in, trying to decrease the velocity back to\nthe 0.4 bound.\nFig. 4: Inverted Pendulum Coaching Result\nThe experiment result is presented by Figure4. The black line\nindicates the agent trained normally, and the blue line indicates\nthe agent trained with a PID coach. A win is set to be with a\nscore greater than 800. It takes the normal method 160 episodes\nto get ﬁve consecutive wins, and it takes the controller-based\ncoaching method 100 episodes to do the same. As measured\nby averaging over 10 episodes, it takes the normal method\n159 episodes to go beyond 800, and it takes controller-based\ncoaching method 104 episodes to do the same. The acceleration\nis roughly 35%. The agents trained with both methods pass the\nevaluation, and their respective average scores are presented\nin the upper left corner.\nB. Inverted Double Pendulum\nThe inverted double pendulum has observation space\nof\nthe\nfollowing:\n[x\nposition\nof\nthe\ncart,\nsin(θ1),\nsin(θ2),cos(θ1),cos(θ2),velocity of x, angular velocity of θ1,\nangular velocity of θ2, constraint force on x, constraint force\non θ1, constraint force on θ2]. θ1 and θ2 are the angles of\nthe upper and lower pole perspectively. The action space for\nInverted Double Pendulum is an action ranging from -10 to\n10, where -10 means the actuator moves the cart to the left\nwith maximum power and 10 means the actuator moves the\ncart to the right with maximum power. A score of roughly 10\npoints is assigned to non-terminal states, based on the velocity\non the x-axis. The detailed formular for score computation can\nbe found at the OpenAI site.\nFigure5 shows how the RL agent and the PID controller\nmanage the lower pole angle and its angular velocity. The left\nplot is the RL agent and the right plot is the PID controller\ncoach. The RL agent seems to settle on continuously oscillating\nfrom the left limit to the right limit until the maximum episode\nsteps are reached. The PID controller functions well until the\nequilibrium breaks down with too much accumulation on the\nintegral term. The average score achieved by the RL agent is\n9319, and the average score achieved by the PID controller is\n1107.\nBased on our observation of the system, we decided to put\nthe boundary between critical and noncritical states on the\nlower angle of the absolute value of 0.2. The RL agent is free\nto explore with the lower angle with an absolute value smaller\nthan 0.2, but once it goes above this bound, the PID controller\n4\nFig. 5: Inverted Double Pendulum system controlled by RL agent and its PID coach.\nThe left plot is RL agent and the right plot is the PID controller. The maximum episode\nsteps are 1000 and each step without termination is scored at around 10 points, based\non velocity on the x axis. The average score achieved by the RL agent is 9319 and the\naverage score achieved by the PID controller is 1107.\nFig. 6: Double Inverted Pendulum Coaching Result.\nwill kick in, trying to nudge the lower angle back to the 0.2\nbound.\nThe experiment result is presented by Figure6. The black\nline indicates the agent trained normally, and the blue line\nindicates the agent trained with a PID coach. A win is set\nto be with a score greater than 5500. It takes the normal\nmethod 1335 episodes to get ﬁve consecutive wins, and it\ntakes controller-based coaching method 908 episodes to do\nthe same. As measured by averaging over 100 episodes, it\ntakes normal method 1370 episodes to go beyond 5500, and\nit takes controller-based coaching method 935 episodes to do\nthe same. The acceleration is roughly 30%. The agents trained\nwith both methods pass the evaluation, and their respective\naverage scores are presented in the upper left corner.\nC. Hopper\nThe observation space of hopper is the following vector:\n[z position, y position, thigh joint angle, leg joint angle, foot\njoint angle, velocity at x-axis, velocity at z-axis, velocity at\ny-axis, angular velocity of thigh joint, angular velocity of leg\njoint, angular velocity of foot joint]. The hopper’s action space\nis three continuous action choices for three actuators [thigh\nactuator, leg actuator, foot actuator]. The range of actuators is\n-10, which means applying force towards the negative direction\nwith maximum power, and 10, which means applying force\ntowards the positive direction with maximum power. The\nterminal states for hopper are when the absolute y position is\ngreater than 0.2.\nBased on our observation of the system, we decided to put\nthe boundary between critical and noncritical states on the y\naxis velocity of the absolute value of 0.3. The RL agent is\nFig. 7: Hopper system controlled by RL agent and its PID coach. The left plot is RL agent\nand the right plot is the PID controller. The maximum episode steps are 1000 and each\nstep without termination is scored at around 1 point, depending on the x-axis velocity.\nThe average score achieved by the RL agent is 989 and the average score achieved by\nthe PID controller is 581.\nFig. 8: Hopper Coaching Result. Benchmarked against training without coaching,\nindicated by the black dotted line.\nfree to explore with the y axis velocity with an absolute value\nsmaller than 0.3, but once it goes above this bound, the PID\ncontroller will kick in, trying to decrease the y axis velocity\nback to the 0.3 bound.\nFigure7 shows how the RL agent and the PID controller\nmanage they position and its velocity. The left plot is the RL\nagent and the right plot is the PID controller coach. The RL\nagent seems to settle on doing nothing until the maximum\nepisode steps are reached. The PID controller can only manage\nthey position into an oscillation with ever-increasing magnitude.\nThe average score achieved by the RL agent is 989, and the\naverage score achieved by the PID controller is 581.\nThe experiment result is presented by Figure8. The agent\ntrained normally is indicated by the black line, and the agent\ntrained with PID coach is indicated by the blue line. A win\nis set to be with a score greater than 800. It takes the normal\nmethod 2851 episodes to get ﬁve consecutive wins, and it\ntakes controller-based coaching method 2073 episode to do the\nsame. As measured by averaging over 100 episodes, it takes the\nnormal method 2911 episodes to go beyond 800, and it takes\nthe controller-based coaching method 2155 episodes to do the\nsame. The acceleration is roughly 25%. The agents trained\nwith both methods pass the evaluation, and their respective\naverage scores are presented in the upper left corner.\nD. Walker\nThe walker system is just a two-legged hopper. The obser-\nvation space is the same as listed in the hopper environment\nbut for both legs. The terminal state is when the z position\nfalls below 0.8.\n5\nFig. 9: Walker system controlled by RL agent and its PID coach. The left plot is RL\nagent and the right plot is the PID coach. The maximum episode steps are 1000 and each\nstep without termination is scored at around 1 point, depending on the x-axis velocity.\nThe average score achieved by the RL agent is 1005 and the average score achieved by\nthe PID controller is 528.\nFig. 10: Hopper Coaching Result. Benchmarked against training without coaching,\nindicated by the black dotted line.\nBased on our observation of the system, we decided to put\nthe boundary between critical and noncritical states on the y\naxis velocity of the absolute value of 1. The RL agent is free to\nexplore with the y axis velocity with an absolute value smaller\nthan 1, but once it goes above this bound, the PID controller\nwill kick in, trying to decrease the y axis velocity back to the\n1 bound.\nFigure9 shows how the RL agent and the PID controller\nmanage the y position and its velocity. The left plot shows RL\nagent can handle the oscillation of y velocity in between the\n0.5 bound, but the PID controller is incapable of it, as indicated\nby the right plot. The PID controller can only manage the y\nposition into an oscillation with ever-increasing magnitude.\nThe average score achieved by the RL agent is 1005, and the\naverage score achieved by the PID controller is 528.\nThe experiment result is presented by Figure10. The black\nline indicates the agent trained normally, and the blue line\nindicates the agent trained with a PID coach. A win is set to be\nwith a score greater than 800. It takes the normal method 5170\nepisodes to get ﬁve consecutive wins, and it takes controller-\nbased coaching method 4784 episode to do the same. As\nmeasured by averaging over 100 episodes, it takes normal\nmethod 7135 episodes to go beyond 800, and it takes controller-\nbased coaching method 5659 episodes to do the same. The\nacceleration is roughly 10%. The agents trained with both\nmethods pass the evaluation, and their respective average scores\nare presented in the upper left corner.\nIV. CONCLUTION\nIn this paper, we present the controller based coaching\napproach for accelerated RL training. Unlike previous attempts\nfor using the controller as a guide to RL agent, our method\ncan work with the most primitive PID controllers. In all our\nexperiments, the PID controllers are barely functioning, yet\nacceleration is still observed. We ascribe this to the fact that\nthe coach intervention step is not part of the RL agent record.\nTherefore, even if the coach failed to do its job, it would not\nhave worsened the RL training process. Yet when the coach\ndoes its job, accelerated data collection on the critical states is\nachieved.\nNext step, we plan to implement the controller-based\ncoaching idea to the deep drone acrobat project [28]. The\ndrones are trained in simulation, and we think our approach\ncan signiﬁcantly accelerate their training. In future works, we\nwant to provide a theoretical basis for distinguishing between\ncritical and noncritical states, as opposed to base solely on\nhuman judgment, as we did here.\nWe believe our research opens the door to a rich reservoir of\npotential research on coaching RL agents with controllers.\nHuman achieves superhuman feats not because of talent,\nbut because of the meticulously engineered coaching tactics.\nCurrent research in RL focuses solely on the \"athlete\" side\nof the equation, i.e., building an efﬁcient RL agent, but we\nfeel \"coach\" is as important, if not more so. For instance, a\ntennis coach will challenge athletes, pushing them to experience\nsituations that are hard to encounter. A controller can function\nas a coach and pushes the RL agent into states that are hard\nto access. Controller-based coaching can be an effective way\nto merge controllers with RL agents.\nREFERENCES\n[1] O. M. Andrychowicz, B. Baker, M. Chociej, R. Józefowicz, B. McGrew,\nJ. W. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider,\nS. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba, “Learning\ndexterous in-hand manipulation,” The International Journal of Robotics\nResearch, vol. 39, pp. 20 – 3, 2020.\n[2] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang,\nD. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine,\n“Qt-opt: Scalable deep reinforcement learning for vision-based robotic\nmanipulation,” ArXiv, vol. abs/1806.10293, 2018.\n[3] J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter, “Learning\nquadrupedal locomotion over challenging terrain,” Science Robotics,\nvol. 5, 2020.\n[4] D. Bertsekas and J. Tsitsiklis, “Neuro-dynamic programming,” in\nEncyclopedia of Machine Learning, 1996.\n[5] M. Han, L. Zhang, J. Wang, and W. Pan, “Actor-critic reinforcement\nlearning for control with stability guarantee,” IEEE Robotics and\nAutomation Letters, vol. 5, pp. 6217–6224, 2020.\n[6] E. Weinan, “A proposal on machine learning via dynamical systems,”\n2017.\n[7] E. Dupont, A. Doucet, and Y. Teh, “Augmented neural odes,” in NeurIPS,\n2019.\n[8] M. Betancourt, M. I. Jordan, and A. Wilson, “On symplectic optimization,”\narXiv: Computation, 2018.\n[9] O. Nachum and B. Dai, “Reinforcement learning via fenchel-rockafellar\nduality,” ArXiv, vol. abs/2001.01866, 2020.\n[10] L. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, “Learning-\nbased model predictive control: Toward safe learning in control,” 2020.\n[11] A. Mohan, N. Lubbers, D. Livescu, and M. Chertkov, “Embedding hard\nphysical constraints in neural network coarse-graining of 3d turbulence.”\narXiv: Computational Physics, 2020.\n6\n[12] B. Lusch, J. N. Kutz, and S. Brunton, “Deep learning for universal linear\nembeddings of nonlinear dynamics,” Nature Communications, vol. 9,\n2018.\n[13] S. Bai, J. Z. Kolter, and V. Koltun, “Deep equilibrium models,” ArXiv,\nvol. abs/1909.01377, 2019.\n[14] F. de Avila Belbute-Peres, T. D. Economon, and J. Z. Kolter, “Combining\ndifferentiable pde solvers and graph neural networks for ﬂuid ﬂow\nprediction,” ArXiv, vol. abs/2007.04439, 2020.\n[15] W. B. Knox and P. Stone, “Interactively shaping agents via human\nreinforcement: the tamer framework,” in K-CAP ’09, 2009.\n[16] ——, “Combining manual feedback with subsequent mdp reward signals\nfor reinforcement learning,” in AAMAS, 2010.\n[17] X. Peng, P. Abbeel, S. Levine, and M. V. D. Panne, “Deepmimic:\nExample-guided deep reinforcement learning of physics-based character\nskills,” ACM Trans. Graph., vol. 37, pp. 143:1–143:14, 2018.\n[18] X. Peng, E. Coumans, T. Zhang, T. Lee, J. Tan, and S. Levine,\n“Learning agile robotic locomotion skills by imitating animals,” ArXiv,\nvol. abs/2004.00784, 2020.\n[19] T. Paine, S. G. Colmenarejo, Z. Wang, S. Reed, Y. Aytar, T. Pfaff, M. W.\nHoffman, G. Barth-Maron, S. Cabi, D. Budden, and N. D. Freitas, “One-\nshot high-ﬁdelity imitation: Training large-scale deep nets with rl,” ArXiv,\nvol. abs/1810.05017, 2018.\n[20] L. Xie, S. Wang, S. Rosa, A. Markham, and A. Trigoni, “Learning\nwith training wheels: Speeding up training with a simple controller for\ndeep reinforcement learning,” 2018 IEEE International Conference on\nRobotics and Automation (ICRA), pp. 6276–6283, 2018.\n[21] I. Carlucho, M. D. Paula, S. A. Villar, and G. G. Acosta, “Incremental\nq-learning strategy for adaptive pid control of mobile robots,” Expert\nSyst. Appl., vol. 80, pp. 183–199, 2017.\n[22] B. S. Pavse, F. Torabi, J. P. Hanna, G. Warnell, and P. Stone, “Ridm:\nReinforced inverse dynamics modeling for learning from a single\nobserved demonstration,” IEEE Robotics and Automation Letters, vol. 5,\npp. 6262–6269, 2020.\n[23] B. Recht, “A tour of reinforcement learning: The view from continuous\ncontrol,” ArXiv, vol. abs/1806.09460, 2018.\n[24] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for model-\nbased control,” in 2012 IEEE/RSJ International Conference on Intelligent\nRobots and Systems, 2012, pp. 5026–5033.\n[25] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,\nJ. Tang, and W. Zaremba, “Openai gym,” ArXiv, vol. abs/1606.01540,\n2016.\n[26] A. Kuhnle, M. Schaarschmidt, and K. Fricke, “Tensorforce: a tensorﬂow\nlibrary for applied reinforcement learning,” Web page, 2017. [Online].\nAvailable: https://github.com/tensorforce/tensorforce\n[27] J. Achiam, “Spinning Up in Deep Reinforcement Learning,” 2018.\n[28] E. Kaufmann, A. Loquercio, R. Ranftl, M. Müller, V. Koltun, and\nD. Scaramuzza, “Deep drone acrobatics,” ArXiv, vol. abs/2006.05768,\n2020.\n",
  "categories": [
    "eess.SY",
    "cs.LG",
    "cs.SY"
  ],
  "published": "2022-10-03",
  "updated": "2022-10-03"
}