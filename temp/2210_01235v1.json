{
  "id": "http://arxiv.org/abs/2210.01235v1",
  "title": "CaiRL: A High-Performance Reinforcement Learning Environment Toolkit",
  "authors": [
    "Per-Arne Andersen",
    "Morten Goodwin",
    "Ole-Christoffer Granmo"
  ],
  "abstract": "This paper addresses the dire need for a platform that efficiently provides a\nframework for running reinforcement learning (RL) experiments. We propose the\nCaiRL Environment Toolkit as an efficient, compatible, and more sustainable\nalternative for training learning agents and propose methods to develop more\nefficient environment simulations.\n  There is an increasing focus on developing sustainable artificial\nintelligence. However, little effort has been made to improve the efficiency of\nrunning environment simulations. The most popular development toolkit for\nreinforcement learning, OpenAI Gym, is built using Python, a powerful but slow\nprogramming language. We propose a toolkit written in C++ with the same\nflexibility level but works orders of magnitude faster to make up for Python's\ninefficiency. This would drastically cut climate emissions.\n  CaiRL also presents the first reinforcement learning toolkit with a built-in\nJVM and Flash support for running legacy flash games for reinforcement learning\nresearch. We demonstrate the effectiveness of CaiRL in the classic control\nbenchmark, comparing the execution speed to OpenAI Gym. Furthermore, we\nillustrate that CaiRL can act as a drop-in replacement for OpenAI Gym to\nleverage significantly faster training speeds because of the reduced\nenvironment computation time.",
  "text": "CaiRL: A High-Performance Reinforcement\nLearning Environment Toolkit\nPer-Arne Andersen\nDepartment of ICT\nUniversity of Agder\nGrimstad, Norway\nper.andersen@uia.no\nMorten Goodwin\nDepartment of ICT\nUniversity of Agder\nGrimstad, Norway\nmorten.goodwin@uia.no\nOle-Christoffer Granmo\nDepartment of ICT\nUniversity of Agder\nGrimstad, Norway\nole.granmo@uia.no\nAbstract—This paper addresses the dire need for a platform\nthat efﬁciently provides a framework for running reinforcement\nlearning (RL) experiments. We propose the CaiRL Environment\nToolkit as an efﬁcient, compatible, and more sustainable alterna-\ntive for training learning agents and propose methods to develop\nmore efﬁcient environment simulations.\nThere is an increasing focus on developing sustainable artiﬁcial\nintelligence. However, little effort has been made to improve the\nefﬁciency of running environment simulations. The most popular\ndevelopment toolkit for reinforcement learning, OpenAI Gym, is\nbuilt using Python, a powerful but slow programming language.\nWe propose a toolkit written in C++ with the same ﬂexibility level\nbut works orders of magnitude faster to make up for Python’s\ninefﬁciency. This would drastically cut climate emissions.\nCaiRL also presents the ﬁrst reinforcement learning toolkit\nwith a built-in JVM and Flash support for running legacy\nﬂash games for reinforcement learning research. We demonstrate\nthe effectiveness of CaiRL in the classic control benchmark,\ncomparing the execution speed to OpenAI Gym. Furthermore,\nwe illustrate that CaiRL can act as a drop-in replacement for\nOpenAI Gym to leverage signiﬁcantly faster training speeds\nbecause of the reduced environment computation time.\nIndex Terms—Reinforcement Learning, Environments, Sus-\ntainable AI\nI. INTRODUCTION\nReinforcement Learning (RL) is a machine learning area\nconcerned with sequential decision-making in real or simu-\nlated environments. RL has a solid theoretical background and\nshows outstanding capabilities to learn control in unknown\nnon-stationary state-spaces [1]–[3]. Recent literature demon-\nstrates that Deep RL can master complex games such as Go\n[4], StarCraft II [5], and progressively move towards mas-\ntering safe autonomous control [1]. Furthermore, RL has the\npotential to contribute to health care for tumor classiﬁcation\n[6], ﬁnances [7], and industry-4.0 [8] applications. RL solves\nproblems iteratively by making decisions while learning from\nreceived feedback signals.\nA. Research Gap\nHowever, fundamental challenges limit RL from working\nreliably in real-world systems. The ﬁrst issue is that the\nexploration-exploitation trade-off is difﬁcult to balance in real-\nworld systems because it is also a trade-off between suc-\ncessful learning and safe learning [1]. The reward-is-enough\nhypothesis suggests that RL algorithms can develop general\nand multi-attribute intelligence in complex environments by\nfollowing a well-deﬁned reward signal. However, the second\nchallenge is that reward functions that lead to efﬁcient and\nsafe training in complex environments are difﬁcult to deﬁne\n[9]. Given that it is feasible to craft an optimal reward function,\nagents could quickly learn to reach the desired behavior but\nstill require exploration to ﬁnd a good policy. RL also requires\nmany samples to learn an optimal behavior, making it difﬁcult\nfor a policy to converge without simulated environments.\nWhile there are efforts to address RL’s safety and sample\nefﬁciency concerns, it remains an open question [10]. These\nconcerns represent challenges for training RL algorithms\nclimate-efﬁcient in an environmentally responsible manner.\nMost RL algorithms require substantial calculations to train\nand simulate environments before achieving satisfactory data\nto conclude performance measurements. Therefore, current\nstate-of-the-art methods have a signiﬁcant negative impact on\nthe climate footprint of machine learning [11].\nBecause of the difﬁculties mentioned above, a large percent-\nage of RL research is conducted in environment simulations.\nLearning from a simulation is convenient because it simpliﬁes\nquantitative research by allowing agents to freely make deci-\nsions that learn from catastrophic occurrences without causing\nharm to humans or real systems. Furthermore, simulations can\noperate quicker than real-world systems, addressing some of\nthe issues caused by low sample efﬁciency.\nThere are substantial efforts in the RL ﬁeld that focus on\nimproving sample efﬁciency for algorithms but little work on\nimproving simulation performance through implementation or\nawareness. Currently, most environments and simulations in\nRL research are integrated, implemented, or used through the\nOpen AI Gym toolkit. The beneﬁt of using AI Gym is that it\nprovides a common interface that uniﬁes the API for running\nexperiments in different environments. There are other such\nefforts like Atari 2600 [12], Malmo Project [13], Vizdoom\n[14], and DeepMind Lab [15], but there is, to the best of\nour knowledge, no toolkit that competes with the environment\ndiversity seen in AI Gym. AI Gym is written in Python,\nan interpreted high-level programming language, leading to\na signiﬁcant performance penalty. At the same time, AI Gym\nhas substantial traction in the RL research community. Our\nconcern is that this gradually leads to more RL environments\narXiv:2210.01235v1  [cs.LG]  3 Oct 2022\nand problems being implemented in Python. Consequently,\nRL experiments may cause unnecessary computing costs and\ncomputation time, which results in a higher carbon emission\nfootprint [16]. Our concern is further increased by comparing\nthe number of RL environment implementations in Python\nversus other low-level programming languages.\nOur contribution addresses this gap by developing an al-\nternative to AI Gym without these adverse side effects by\noffering a comparable interface and increasing computational\nefﬁciency. As a result, we hope to reduce the carbon emissions\nof RL experiments for a more sustainable AI.\nB. Contribution Scope\nWe propose the CaiRL Environment toolkit to ﬁll the gap\nof a ﬂexible and high-performance toolkit for running rein-\nforcement learning experiments. CaiRL is a C++ interface to\nimprove setup, development, and execution times. Our toolkit\nmoves a considerable amount of computation to compile\ntime, which substantially reduces load times and the run-time\ncomputation requirements for environments implemented in\nthe toolkit. CaiRL aims to have a near-identical interface to\nAI Gym, ensuring that migrating existing codebases requires\nminimal effort. As part of the CaiRL toolkit, we present, to\nthe best of our knowledge, the ﬁrst Adobe Flash compatible\nRL interface with support for Actionscript 2 and 3.\nAdditionally, CaiRL supports environments running in the\nJava Virtual Machine (JVM), enabling the toolkit to run Java\nseamlessly if porting code to C++ is impractical. Finally,\nCaiRL supports the widely used AI Gym toolkit, enabling\nexisting Python environments to run seamlessly. Our contri-\nbutions summarize as follows:\n1) Implement a more climate-sustainable and efﬁcient ex-\nperiment execution toolkit for RL research.\n2) Contribute novel problems for reinforcement learning\nresearch as part of the CaiRL ecosystem.\n3) Empirically demonstrate the performance effectiveness\nof CaiRL.\n4) Show that our solution effectively reduces the carbon\nemission footprint when measuring following the met-\nrics in [17].\n5) Evaluate the training speed of CaiRL and AI Gym and\nempirically verify that improving environment execution\ntimes can substantially reduce the wall-clock time used\nto learn RL agents.\nC. Paper Organization\nIn Section 2, we dive into the existing literature on re-\ninforcement learning game design and compare the existing\nsolution to ﬁnd the gap for our research question. Section 3\ndetails reinforcement learning from the perspective of CaiRL\nand the problem we aim to solve. Section 4 details the design\nchoices of CaiRL and provides a thorough justiﬁcation for\ndesign choices. Section 5 presents our empirical ﬁndings of\nperformance, adoption challenges, and how they are solved,\nand ﬁnally compares the interface of the CaiRL framework\nto OpenAI Gym (AI Gym). Section 6 presents a brief design\nrecommendation for developers of new environments aimed\nat reinforcement learning research. Finally, we conclude our\nwork and outline a path forwards for adopting CaiRL.\nII. BACKGROUND\nA. Reinforcement Learning\nReinforcement Learning is modeled according to a Markov\nDecision Process (MDP) described formally by a tuple\n(S, A, T, R, γ, s0). S is the state-space, A is the action-space,\nT : S × A →S is the transition function, R: S × A →R\nis the reward function [18], γ is the discount factor, and s0\nis starting state. In the context of RL, the agent operates\niteratively until reaching a terminal state, at which time the\nprogram terminates. Q-Learning is an off-policy RL algorithm\nand seeks to ﬁnd the best action to take given the current\nstate. The algorithm operates off a Q-table, an n-dimensional\nmatrix that follows the shape of state dimensions where the\nﬁnal dimension is the Q-values. Q-Values quantify how good\nit is to act a at time t. This work uses Deep Learning\nfunction approximators in place of Q-tables to allow training\nin high-dimension domains [19]. This forms the algorithm\nDeep Q-Network (DQN), one of the ﬁrst deep learning-\nbased approaches to RL, and is commonly known for solving\nAtari 2600 with superhuman performance [19]. Section V-C\ndemonstrates that our toolkit signiﬁcantly reduces the run-\ntime and carbon emission footprint when training DQN in\ntraditional control environments.\nB. Graphics Acceleration\nA graphics accelerator or a graphical processing unit (GPU)\nintends to execute machine code to produce images stored in\na frame buffer. The machine code instructions are generated\nusing a rendering unit that communicates with the central\nprocessing unit (CPU) or the GPU. These methods are called\nsoftware rendering or hardware rendering, respectively. GPUs\nare specialized electronics for calculating graphics with vastly\nsuperior parallelization capabilities to their software counter-\npart, the CPU. Therefore, hardware rendering is typically pre-\nferred for computationally heavy rendering workloads. Con-\nsequently, it is reasonable to infer that hardware-accelerated\ngraphics provide the best performance due to their improved\ncapacity to generate frames quickly. On the other hand, we\nnote that when the rendering process is relatively basic (e.g.,\n2D graphics) and access to the frame buffer is desired, the\nexpense of moving the frame buffer from GPU memory to\nCPU memory dramatically outweighs the beneﬁts. [20]\nAccording to [20], software rendering in modern CPU chips\nperforms 2-10x faster due to specialized bytecode instructions.\nThis study concludes that the GPU can render frames faster,\nprovided that the frame permanently resides in GPU memory.\nHaving frames in the GPU memory is impractical for machine\nlearning applications because of the copy between the CPU\nand GPU. The authors in [21] propose using Single Instruction\nMultiple Data (SIMD) optimizations to improve game perfor-\nmance. SIMD extends the CPU instruction set for vectorized\narithmetic to increase instruction throughput. The authors ﬁnd\nthat using SIMD instructions increases performance by over\n80% compared to traditional CPU rendering techniques.\nThe ﬁndings in these studies suggest that software acceler-\nation is beneﬁcial in some graphic applications, and similarly,\nwe ﬁnd it useful in a reinforcement learning context. Empir-\nically, software rendering performs better for simple 2D and\n3D graphic applications due to the high-latency copy operation\nneeded between the GPU and CPU. Much of the success of\nCaiRL lies in the fact that software rendering, while being\nslower for advanced games such as StarCraft, signiﬁcantly\noutperforms hardware rendering for simple graphics. One\nalternative to improve performance in hardware rendering is\nto use pixel buffer objects or an equivalent implementation.\nA pixel buffer object (PBO) is a buffer storage that allows\nthe user to retrieve frame buffer pixels asynchronously while\na new frame buffer is drawn to the screen frame buffer.\nIn particular, copying pixels without PBO is slow because\nrendering must halt while the buffer is read [22].\nC. Programming Languages\nMachine learning research and application development\nhave been carried out in various programming languages\nthroughout history. In more recent history, the Python language\nhas been used more frequently in the scientiﬁc community and,\nmore speciﬁcally, in machine learning, and deep learning [23].\nUnfortunately, Python’s most used implementation is CPython,\na single-threaded implementation with little regard for efﬁ-\nciency compared to compiled languages. However, Python’s\nmost popular toolkits for machine learning are implemented\nin compiled languages and use wrapper code to interact to\nincrease performance. A study by Zehra et al. suggests that\nC++ has approximately a 50 times performance advantage over\nPython, and Python has advantages in code readability for\nbeginners in programming [24]. It is clear from these studies\nthat Python is great for prototyping and learning programming\nbut is not suitable for performance-sensitive tasks. It is natural\nto seek an approach that can preserve the simplicity of Python\nwhile also maintaining acceptable task execution performance.\nPybind11 is one such framework that provides a method\nto create an efﬁcient bridge between C++ and Python code.\nPybind11 is a lightweight library that exposes C++ types\nin Python and vice versa but focuses mainly on exposing\nC++ code paths to Python applications. There is a minor\nperformance penalty during the conversion between Python\nand C++ objects. Hence, implementations in C++ will run at\nnear-native performance in Python. For this reason, we follow\nthe path of implementing an efﬁcient experiment toolkit for\nreinforcement learning in C++ with binding code to allow\nPython to interface with CaiRL.\nD. Summary\nThe goal of CaiRL is to create an expanding set of high-\nperformance environments for RL research. It is essential to\nencourage good practices by adding novel environments to\nthe toolkit. Our extensive practical testing ﬁnds that rendering\ngraphics in software provides substantially higher throughput\nfor applications where access to the frame buffer is desirable.\nThis observation is especially prominent for simple 2D and\n3D-based applications. However, the beneﬁts diminish as the\ngraphical complexity increases. For example, it is clear from\nour ﬁndings that games such as StarCraft II render better using\nhardware acceleration.\nWe study the implications of implementation language for\nCaiRL and ﬁnd that the choice of programming language is\nessential to CaiRL because it aims to be efﬁcient and reduce\nthe carbon emission footprint as much as possible. C++ seems\nlike a natural choice as it is mature, has a stable standard\nlibrary, and supersets the C language.\nIII. DESIGN SPECIFICATIONS\nThe design goal of CaiRL is to have interoperability with\nAI Gym, but with orders of magnitude better performance\nand ﬂexibility to support environments in a multitude of\nprogramming languages. Keeping full compatibility with AI\nGym is central to trivializing the two frameworks without\nsigniﬁcant amendments to existing code.\nCaiRL is a novel reinforcement learning environment\ntoolkit for high-performance experiments. By designing such\na toolkit, reinforcement learning becomes more affordable\ndue to reduced execution costs and strives toward more\nsustainable AI. A bi-effect of these goals is that experiments\nrun signiﬁcantly faster, and most CPU cycles are spent on\ntraining AI instead of evaluating game states. The CaiRL\nenvironment toolkit supports classical RL problems such as (1)\nCart-Pole, Acro-Bot, Mountain-Car, and Pendulum, (2) Novel,\nhigh-complexity games such as Deep RTS, [25], Deep Line\nWars, X1337 Space Shooter, and (3) over 1 000 ﬂash games\navailable for experimentation. 1\nThe engine of CaiRL relies upon C++ with highly per-\nformant fast-paths such as Single Instruction Multiple Data\n(SIMD) for vectorized calculation that ﬁts into the processor\nregistry in a single instruction. The design of CaiRL mimics\nAI Gym but relies on templating and const expressions\nto evaluate calculations at compile-time instead of run-time.\nCaiRL is split into modules, and we dedicate this section to\ndescribing the design decisions and the resulting interaction\nlayer and beneﬁts compared to similar solutions.\nA. Building Blocks\nCaiRL follows the module design pattern to have mini-\nmal cross-dependencies between toolkit components. This has\nseveral beneﬁts, namely (1) being easier to maintain and (2)\nreducing compile times signiﬁcantly. CaiRL is composed of\nsix essential modules:\n1) Runners is a bridge for accessing non-native run-times,\nenabling a uniﬁed API for all environments. Flash envi-\nronments use the Lightspark runner to run Flash games\nseamlessly. Similarly, Java games have a specialized\nJava Virtual Machine (JVM) runner.\n1We invite the reader to http://github.com/cair/rl for detailed toolkit docu-\nmentation.\n2) Renderers is a module for drawing graphical frame\nbuffer output to the screen. Currently, Blend2D and\nOpenCV are part of this module. This module is es-\nsential for training agents in graphical environments.\n3) Environments are the module for integrating games and\napplications with a uniﬁed interface. This interface is\nnear-identical to AI Gym but has less overhead because\nof the more efﬁcient precompilation of machine code.\n4) Wrappers are also similar to what is found in AI\nGym. This module features code to wrap environment\ninstances to change the execution behavior, such as\nlimiting the number of timesteps before reaching the\nterminal state. The initial version of CaiRL features\nwrappers to ﬂatten the state observation and add max\ntimestamp restrictions.\n5) Spaces are a module for deﬁning the shape of state\nobservation and action spaces, similar to AI Gym. All of\nthe spaces use highly optimized code, which efﬁciently\nincreases populating data matrices. The Box type fea-\ntures n-dimensional matrices, and ﬁnally, the Discrete\ntype deﬁnes a one-dimensional vector of integers.\n6) Tooling is the module for contributions that reach a\nstable state and enrich the features of CaiRL. One\nsuch example is the tournament framework that trivi-\nalizes running single-elimination and Swiss-based tour-\nnaments.\nThe CaiRL toolkit has exposed interfaces through its native\nC++ API and the Python API. The binding code is auto-\nmatically generated for environments following the standard\ndeﬁnition found in the Env class, but for highly customized\nimplementations, such bindings must be added manually. Sim-\nilarly, the CaiRL toolkit compiles Python-compatible machine\ncode with signiﬁcantly lower overhead when loaded and\ninterpreted by CPython. See the discussion in Section II-C.\nB. Implementation Layer\nThere are two ways of building reinforcement learning envi-\nronments with CaiRL (1) using C++ directly or (2) through the\nPython to C++ bindings. CaiRL performs well in Python and\nC++ because most of the computation runs in optimized code.\nHowever, the Python bindings have additional computational\ncosts because each line is interpreted and translated from\nPython and C++. The interpreter overhead can be reduced by\ndiverging from the normal AI Gym API and implementing\na run function, notably eliminating the need for interpreted\nloop code in Python. The primary goal of the CaiRL API is\nto match the AI Gym API to enable a seamless experience\nwhen migrating existing codebases to CaiRL.\n1\ne =\nFlatten<TimeLimit<200,CartPoleEnv>>()\n2\nfor(int ep = 0; ep < 100; ep++){\n3\ne.reset();\n4\nint term, steps = 0;\n5\nwhile(!term){\n6\nsteps++;\n7\nconst auto [s1, r, term, info] =\n8\ne.step(e.action_space.sample());\n9\nauto obs = e.render();\n10\n}\n11\n}\nListing 1\nMINIMAL EXAMPLE OF CAIRL-CARTPOLE-V1 IN C++\nListing 1 shows the C++ interface of the CaiRL toolkit. In\nC++, we deviate from the AI Gym API to allow modules\nas static template classes, as seen in line 1. A template\ndeﬁnes a class that evaluates much of the program logic\nduring compile-time. This has considerable run-time beneﬁts\nbecause code initialization is done during compile-time. The\ndownsides are that compile times increase substantially, and\npolymorphism is impossible between Python classes and C++\ntemplates. However, it is possible to alleviate these challenges\nby predeﬁning classes from the template implementations.\nThis allows contributors to add Python-based environments to\nthe repository of available experiments, however, at the cost\nof providing diminishing performance beneﬁts.\nA very central component of CaiRL is the ability to run\nexperiments natively in Python. This becomes possible by\ncreating code that interfaces C++ and Python using Pybind11.\nPybind11 is a library that provides the ability to call code from\nthe CaiRL shared library (C++ machine code) and the Python\ninterpreter efﬁciently. There is no need for C++ experience\nusing the Python binding code, and it is possible to use and\ncustomize CaiRL with Python for specialized experiments.\nThe Python interface is similar to the C++ interface but focuses\nmore on compatibility with the AI Gym interface.\n1\n#e =\ngym.make(\"CartPole-v1\")\n2\ne =\ncairl.make(\"CartPole-v1\") # Use CaiRL\n3\nfor ep in range(100):\n4\ne.reset()\n5\nterm, steps = 0\n6\nwhile not term:\n7\nsteps++\n8\na = e.action_space.sample()\n9\ns1, r, term, info = e.step(a)\n10\nobs = e.render()\nListing 2\nMINIMAL EXAMPLE OF AI GYM AND CAIRL CARTPOLE-V1 IN PYTHON\nListing 2 illustrates the use of CaiRL in Python compared to\nAI Gym. In particular, to change between AI Gym and CaiRL,\nthe only change required is to use the cairl package (Line 2)\ninstead of the gym package (Line 1).\nC. Affordable and Sustainable AI\nAI is a constantly growing ﬁeld of research, and with the\nshifted focus on Deep Learning, it is well understood that\nthe need for computing power has increased sharply. Deep\nLearning models have a range of a few thousand parameters,\nup to several billion parameters that require carefully tuning\nwith algorithms such as stochastic gradient descent. Hence,\ncompute power plays an essential role in the performance of\nthe trained model. The same applies in Deep RL but requires\nextensive data sampling from an environment. It is reasonable\nto conclude that the cost of conducting trials increases rapidly\nand contributes against the emergence of more sustainable AI.\nCaiRL aims to minimize the cost of reinforcement learning\nby reducing environment execution time. In essence, this has\nthe bi-effect of reducing the carbon emission footprint in RL\nsigniﬁcantly compared to existing solutions, as observed in\nsection V-A.\nIV. GAME RUN-TIMES AND PLATFORMS\nThis section presents the primary run-times that CaiRL\nsupports to integrate environments from run-times other than\nPython and C++ seamlessly.\nA. JVM Applications\nJava is a popular programming language that runs in the\nJava Virtual Machine (JVM). Although Java is not the domi-\nnant language for environments in the RL research community,\nthere are a few notable examples, such as MicroRTS [26] and\nthe Showdown AI competition [27]. These environments have\nshown signiﬁcant value to several research communities in\nreinforcement learning, evolutionary algorithms, and planning-\nbased AI. To integrate JVM-based games in CaiRL, the pro-\ngrammer deﬁnes conﬁguration in a CMake ﬁle that describes\nhow the source code is built to a Java archive (JAR) ﬁle. Then\nthe programmer deﬁnes a C++ class that extends the Env class\ninterface. The JVM and the C++ machine code communication\nis through the Java Native Interface (JNI). Using the JNI\nbridge, it is trivial to create a mapping from C++ to JVM,\nand it is conveniently also performant as the JVM has good\noptimization options. There are similar efforts to bridge Java\ngames through JNI for games such as MicroRTS [28], but\nCaiRL aims toward a generic approach that encapsulates many\nexisting games.\nB. Python Environments\nPython is arguably the most used programming lan-\nguage for RL research in recent literature, as suggested\nby Github tag statistics. We perform the following the\nsearch queries: topic:reinforcement-learning+topic:game\n+language:python for ﬁnding relevant Python environments,\nand topic:reinforcement-learning+topic:game+language:\nc++ for C++ environments. We observe a ratio of 114:1 in\nfavor of Python. Consequently, many of the popular reinforce-\nment learning environments have native Python implementa-\ntions. We approach the task of improving such environments\nwith two possible solutions. The ﬁrst approach automatically\nconverts Python code into C++ using the Nuitka library\nfound at https://github.com/Nuitka/Nuitka. It is also possible\nto add environments directly as a CaiRL Python package,\nalthough this method does not improve the performance and\ndoes not address climate emission concerns. All third-party\nenvironments reside in the cairl.contrib package and are\nfreely available through the C++ and Python interface. For an\nenvironment to be fully compatible with the CaiRL interface,\nthe environment must inherit the abstract Env class and im-\nplement the step(action), reset() , and render() function.\nHowever, there are several open questions on how to efﬁciently\nimprove the performance of most environments implemented\nin Python, see Section VII.\nC. Flash Run-time\nThe most notable feature of CaiRL is the ability to run\nﬂash games without external applications. CaiRL extends the\nLightSpark ﬂash emulator for Actionscript 3 and falls back to\nGNU Gnash for ActionScript 2. CaiRL features a repository\nof over 1300 ﬂash games for conducting AI research and\nreinforcement learning research. In this paper, we focus on\nthe Multitask environment. Multitask is an environment that\nprovides minigames that the agent must control concurrently.\nIf the agent fails one of the tasks, the game terminates.\nThe reward function is deﬁned as positive rewards while the\ngame is running and negative rewards when the game engine\nterminates (e.g., end of the game), indicating that the game is\nlost. The game observations are either raw pixels or the virtual\nFlash memory, and the actions-space is discrete. Our obser-\nvation is that most existing Flash games have short-horizon\nepisodes with few objectives to reach a positive terminal state.\nIn addition, many of the games have simple game rules that\nare especially suited for benchmarking non-hierarchical RL\nalgorithms. The CaiRL ﬂash runner substantially expands the\nnumber of available game environments for experiments. To\nthe best of our knowledge, CaiRL is the only tool that can\ncontrol the game execution speed and guarantee broad support\nfor Actionscript 2 and 3.\nD. Puzzle Run-time\nCaiRL supports the comprehensive collection of puzzles\nfrom the Simon Tatham collection [29]. This collection aims to\nprovide logical puzzles that are solvable either by humans or\nalgorithms. While reinforcement learning is not mainly known\nfor solving logical puzzles, some literature suggests that RL\ncan solve puzzles [30], potentially with the options framework\nfrom [31]. We ﬁnd it beneﬁcial to add puzzles for future\nresearch and demonstrate ﬂexibility in adding new problems\nand environments. All puzzles include a heuristic-based solver,\nenabling transfer and curriculum learning research.\nV. EVALUATIONS\nA. Performance Evaluation\nTo evaluate the performance of CaiRL, we compare the\nclassic control environments from AI Gym with an identical\nimplementation using the CaiRL toolkit. Experiments run for\n100 000 timesteps, and the measurements are averaged over\n100 consecutive trials. The environments are evaluated with\nand without graphical rendering to demonstrate the effective-\nness of raw computation speed and software rendering.\nFigure V-A demonstrates the average console and rendering\nperformance and clearly shows that CaiRL performs 5x faster\nin simulations and over 80x faster on rendering than the AI\nGym equivalent. The console experiment indicates the raw per-\nformance boost when using high-performance programming\nlanguages. As discussed in Section II-B, the graphical experi-\nment validates the effectiveness of rendering the frame buffer\nin software instead of using hardware methods. Speciﬁcally,\nthe rendering backend in AI Gym utilizes OpenGL and has\n0.0K\n25.0K\n50.0K\n75.0K 100.0K 125.0K 150.0K 175.0K 200.0K\nExecution Time (msec) for 100K runs\nCaiRL-Console\nOpenAI-Console\nCaiRL-Render\nOpenAI-Render\nEnvironment Suite\nEnvironment = CartPole-v1\n0.0K\n20.0K\n40.0K\n60.0K\n80.0K\nExecution Time (msec) for 100K runs\nCaiRL-Console\nOpenAI-Console\nCaiRL-Render\nOpenAI-Render\nEnvironment Suite\nEnvironment = Acrobot-v1\n0.0K\n10.0K\n20.0K\n30.0K\n40.0K\n50.0K\n60.0K\nExecution Time (msec) for 100K runs\nCaiRL-Console\nOpenAI-Console\nCaiRL-Render\nOpenAI-Render\nEnvironment Suite\nEnvironment = MountainCar-v1\n0.0K\n20.0K\n40.0K\n60.0K\n80.0K\n100.0K\nExecution Time (msec) for 100K runs\nCaiRL-Console\nOpenAI-Console\nCaiRL-Render\nOpenAI-Render\nEnvironment Suite\nEnvironment = Pendulum-v1\nFig. 1.\nExecution run-time evaluation between CaiRL and AI Gym in the\nclassical control tasks. The x-axis illustrates the execution time for 100 000\nruns (episodes), averaged over 100 trials. The rows show the console and\nrender versions in CaiRL and AI Gym.\nfar greater computational costs when accessing frames, often\ndesirable in RL research.\nB. Algorithm Evaluation\nThe scope of the algorithm evaluation is two-fold. First, we\naim to ﬁnd if CaiRL implementations can improve training\ntime in that they are measurable, hence having positive effects\non economics and climate emission rates. Finally, we evaluate\nif DQN can improve its behavior using the Flash Run-\ntime in the Multitask game environment. We use the default\nhyperparameters proposed by [19] and use raw images as input\nto the algorithm for both experiments. The experiments run\nusing an Intel 8700K CPU and an Nvidia GeForce 2080TI.\nFigure V-B clearly shows that the DQN algorithm is trained\nmagnitudes faster in the CaiRL environment, indicating that\na large part of the training time is the result computation\ntime during sampling the environment. The algorithm trains\nuntil mastering the task (stopping criteria) for 100 trials,\nafter which we average the results. Our ﬁndings conclusively\nshow substantial wall-clock time reductions for training in the\nCaiRL environments compared to the AI Gym environments.\nThe average reduction in training time across all trials is\nroughly 30 percent, illustrating and conﬁrming that efﬁcient\nenvironments are essential for developing AI that trains more\nclimate-friendly.\n0.0K\n10.0K\n20.0K\n30.0K\n40.0K\n50.0K\nExecution Time (msec) for converging 100 times\nCaiRL\nOpenAI\nEnvironment Suite\nEnvironment = CartPole-v1\n0.0K\n20.0K\n40.0K\n60.0K\n80.0K\nExecution Time (msec) for converging 100 times\nCaiRL\nOpenAI\nEnvironment Suite\nEnvironment = Acrobot-v1\n0.0K\n20.0K\n40.0K\n60.0K\n80.0K\nExecution Time (msec) for converging 100 times\nCaiRL\nOpenAI\nEnvironment Suite\nEnvironment = MountainCar-v1\n0.0K\n20.0K\n40.0K\n60.0K\n80.0K\nExecution Time (msec) for converging 100 times\nCaiRL\nOpenAI\nEnvironment Suite\nEnvironment = Pendulum-v1\nFig. 2. The average DQN training time for 100 runs in the classical control\nenvironments. The x-axis is the total execution time in milliseconds for\ntraining the agent 100 times until reaching the optimal strategy. Each time\nthe agent converges, the policy is reset with a ﬁxed randomization seed.\nFigure V-B shows that the DQN algorithm successfully\nsolves the multitask environment after approximately 1 500\n000 frames averaged over ten trials. Note that we here only\nwish to verify that algorithms can learn from the ﬂash game\nengine. By unlocking the frame rate of the simulation, it\nis possible to achieve approximately 140 frames per second\nusing Intel 8700K in the Multitask environment. Compared\nto running the simulation in the integrated ﬂash run-time in\nbrowsers, our approach increases the game execution speed to\na factor of 4.6x in a majority of ﬂash games. This is because\nFlash games have the game loop inside the rendering loop.\nEach training trial took approximately 6 hours to ﬁnish, and\nin total, the experiment lasted for 60 hours.\nC. Carbon Emission Evaluation\nThis section aims to answer the following question: Is\nCaiRL a better alternative for lowering carbon emissions\nin RL. To begin answering this question, we rerun experi-\nments with the novel experiment-impact-tracker from [17].\nThe experiment-impact-tracker is a drop-in method to track\nenergy usage, carbon emissions, and compute utilization of the\nsystem and is recently proposed to encourage the researcher to\ncreate more sustainable AI. Our experiments run a DQN agent\non the classical control environment CartPole-v1 in CaiRL\nand AI Gym. We compare the toolkits using the console-\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTimesteps\n1e6\n500\n1000\n1500\n2000\n2500\n3000\nTotal Return\nCaiRL-Multitask-v0\nAlgorithm\nDQN\nConvergence\nFig. 3. DQN performance in the Multitask environment. The algorithm solves\nthe environment after approximately 3 000 000 timesteps where the training\nprocedure is averaged over 10 trials.\nonly version and the graphical variant. We use the following\nenvironment conﬁgurations and DQN parameters:\nTABLE I\nTHE DQN HYPERPARAMETERS FOR THE CARBON EMISSION EXPERIMENT\nHyperparameter\nValue\nDiscount\n0.99\nUnits\n32, 32\nActivation\nelu\nOptimizer\nAdam\nLoss Function\nHuber\nBatch Size\n32\nLearning Rate\n3e-4\nTarget Update Freq\n150\nMemory Size\n50 000\nExploration Start\n1.0\nExploration Final\n0.01\nThe experiment runs for 1 000 000 timesteps in the console\nversion and 10 000 timesteps for the graphical version2\nTABLE II\nTHE TABLE DESCRIBES THE TOTAL CARBON EMISSION VALUES AND\nPOWER CONSUMPTION USED DURING THE EXPERIMENTS. THE CARBON\nEMISSION IS MEASURED IN CO2/KG, AND THE POWER DRAW IS\nMEASURED IN MILLIWATT-HOUR (MWH).\nMeasurement\nEnvironment\nCaiRL\nGym\nRatio\nCO2/kg\nConsole\n0.000014\n0.000067\n20.8955\nCO2/kg\nGraphical\n0.000051\n0.075265\n147578.431373\nPower (mWh)\nConsole\n0.000319\n0.001483\n21.5104\nPower (mWh)\nGraphical\n0.001131\n1.673959\n148006.9849\nTable II shows that CaiRL has a considerably lower carbon\nemission than AI Gym. CaiRL has 20.89x less carbon emission\nin the console variant than Gym. The graphical experiment\nshows a more signiﬁcant difference with a 147578x reduction\nin carbon emissions. The reason AI Gym has high emission\n2The experiments code be accessed at https://github.com/cair/rl.\nrates is that it is locked to capturing images from the game\nwindow. We measure the emissions by subtracting the DQN\ntime usage with the total time to only account for the envi-\nronment run-time costs.\nVI.\nCONCLUSION\nCaiRL is a novel platform for RL and AI research and aims\nto reduce program execution time for experiments to reduce\nbudget costs and the carbon emission footprint of AI. CaiRL\noutperforms AI Gym implementations signiﬁcantly while also\nbeing compatible with existing AI Gym experiments. However,\nfor CaiRL to be effective, code needs to be ported to the\nCaiRL toolkit. While this may seem tedious, it reduces execu-\ntion times, reducing RL experiments’ economic and climate-\nrelated footprint. However, there are preliminary options for\nautomatically translating code from Python to C++, such as\nusing the Nuitka compiler.\nThis contribution clearly outlines new recommendations\nand considerations for developing new environments for RL\nresearch. First, we recommend using low-level languages such\nas C++ to implement the logic and, optionally, using code\nbinding libraries for interoperability between run-times. The\neffectiveness of this approach is further demonstrated by [32],\n[33]. Second, for 2D graphics, it is clear from our literature\nreview that using software rendering with SIMD capabilities\nmay provide signiﬁcant beneﬁts when accessing the frame\nbuffer. Following these recommendations, we show that CaiRL\nhas 30% less overhead than AI Gym.\nLastly, we have illustrated that CaiRL supports many\nprogramming languages, including C++, Java, Python, and\nActionScript 2 and 3. CaiRL supports over 1300 games in\nActionScript, Several C++ games, MicroRTS, and Showdown\nin Java and supports building python games out of the box.\nIn the evaluations of CaiRL, we demonstrate superiority in\nperformance and positively impact the carbon footprint of AI.\nVII. FUTURE WORK\nThis paper has presented CaIRL, a reinforcement learning\ntoolkit for running a wide range of environments from different\nrun-times in a uniﬁed framework.\nCaiRL is an ambitious project to improve the tools re-\nquired to conduct efﬁcient reinforcement learning research.\nIn fulﬁlling its role, the complexity of the toolkit demands\nextensive testing and veriﬁcation to ensure that all experiments\nare performed following the original version to provide reliable\nexperiment results. While CaiRL is now released, several\ninteresting problems potentially can improve the environment\nperformance further. For the continuation of this project, we\nbelieve that the following concerns may prove valuable to\naddress:\n• Find a suitable method of automatic conversion of Python\ncode. Alternatively, be able to run Python code in more\nefﬁcient run-times, such as the JVM.\n• Improve the JVM and Flash support so that researchers\ncan more easily add new environments.\n• Expand the number of run-times that CaiRL supports\nwhile maintaining portability and efﬁciency\n• Perform static code analysis and recommend code quality\nimprovements and efﬁciency to further reduce the climate\nfootprint of environments.\nREFERENCES\n[1] F. Berkenkamp, M. Turchetta, A. P. Schoellig, and A. Krause, “Safe\nModel-based Reinforcement Learning with Stability Guarantees,” in\nAdvances in Neural Information Processing Systems 30, I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and\nR. Garnett, Eds.\nLong Beach, CA, USA: Curran Associates, Inc., may\n2017, pp. 908–918. [Online]. Available: https://papers.nips.cc/paper/\n6692-safe-model-based-reinforcement-learning-with-stability-guarantees\n[2] W. C. Cheung, D. Simchi-Levi, and R. Zhu, “Reinforcement Learning\nfor Non-Stationary Markov Decision Processes: The Blessing of (More)\nOptimism,” in Proceedings of the 37th International Conference on\nMachine Learning, ser. Proceedings of Machine Learning Research,\nH. D. III and A. Singh, Eds., vol. 119.\nPMLR, 2020, pp. 1843–1854.\n[Online]. Available: https://proceedings.mlr.press/v119/cheung20a.html\n[3] S. Padakandla, P. K. J., and S. Bhatnagar, “Reinforcement learning\nalgorithm for non-stationary environments,” Applied Intelligence 2020\n50:11, vol. 50, no. 11, pp. 3590–3606, jun 2020. [Online]. Available:\nhttps://link.springer.com/article/10.1007/s10489-020-01758-5\n[4] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre,\nS.\nSchmitt,\nA.\nGuez,\nE.\nLockhart,\nD.\nHassabis,\nT.\nGraepel,\nT.\nLillicrap,\nand\nD.\nSilver,\n“Mastering\nAtari,\nGo,\nchess\nand\nshogi by planning with a learned model,” Nature 2020 588:7839,\nvol. 588, no. 7839, pp. 604–609, dec 2020. [Online]. Available:\nhttps://www.nature.com/articles/s41586-020-03051-4\n[5] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,\nJ. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, J. Oh,\nD. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai, J. P.\nAgapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen,\nV. Dalibard, D. Budden, Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre,\nZ. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yogatama, D. W¨unsch,\nK. McKinney, O. Smith, T. Schaul, T. Lillicrap, K. Kavukcuoglu,\nD. Hassabis, C. Apps, and D. Silver, “Grandmaster level in StarCraft\nII using multi-agent reinforcement learning,” Nature 2019 575:7782,\nvol. 575, no. 7782, pp. 350–354, oct 2019. [Online]. Available:\nhttps://www.nature.com/articles/s41586-019-1724-z\n[6] C. Yu, J. Liu, and S. Nemati, “Reinforcement Learning in Healthcare:\nA Survey,” arxiv preprint arXiv:1908.08796, aug 2019. [Online].\nAvailable: https://arxiv.org/abs/1908.08796\n[7] Y. Li, “Deep Reinforcement Learning: An Overview,” arxiv preprint\narXiv:1701.07274, jan 2017. [Online]. Available: http://arxiv.org/abs/\n1701.07274\n[8] Y. P. Pane, S. P. Nageshrao, J. Kober, and R. Babuˇska, “Reinforcement\nlearning based compensation methods for robot manipulators,” Engi-\nneering Applications of Artiﬁcial Intelligence, vol. 78, pp. 236–247, feb\n2019.\n[9] D. Silver, S. Singh, D. Precup, and R. S. Sutton, “Reward is enough,”\nArtiﬁcial Intelligence, vol. 299, p. 103535, oct 2021.\n[10] T. M. Moerland, J. Broekens, and C. M. Jonker, “Model-based\nReinforcement Learning: A Survey,” arxiv preprint arXiv:2006.16712,\njun 2020. [Online]. Available: https://arxiv.org/abs/2006.16712\n[11] P. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and\nJ. Pineau, “Towards the Systematic Reporting of the Energy and\nCarbon\nFootprints\nof\nMachine\nLearning,”\nJournal\nof\nMachine\nLearning Research, vol. 21, pp. 1–43, 2020. [Online]. Available:\nhttp://jmlr.org/papers/v21/20-312.html.\n[12] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, “The Arcade\nLearning Environment: An Evaluation Platform for General Agents,”\nJournal of Artiﬁcial Intelligence Research, vol. 47, pp. 253–279, jun\n2013.\n[13] M. Johnson, K. Hofmann, T. Hutton, and D. Bignell, “The malmo plat-\nform for artiﬁcial intelligence experimentation,” in IJCAI International\nJoint Conference on Artiﬁcial Intelligence, vol. 2016-Janua, 2016.\n[14] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Jaskowski,\n“ViZDoom: A Doom-based AI research platform for visual reinforce-\nment learning,” in IEEE Conference on Computatonal Intelligence and\nGames, CIG, vol. 0, 2016.\n[15] C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright,\nH. K¨uttler, A. Lefrancq, S. Green, V. Vald´es, A. Sadik, J. Schrittwieser,\nK. Anderson, S. York, M. Cant, A. Cain, A. Bolton, S. Gaffney,\nH.\nKing,\nD.\nHassabis,\nS.\nLegg,\nand\nS.\nPetersen,\n“DeepMind\nLab,” arxiv preprint arXiv:1612.03801, 2016. [Online]. Available:\nhttp://arxiv.org/abs/1612.03801\n[16] Q. Zhang, L. Xu, X. Zhang, and B. Xu, “Quantifying the interpretation\noverhead of Python,” Science of Computer Programming, vol. 215, p.\n102759, mar 2022.\n[17] P. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau,\n“Towards the systematic reporting of the energy and carbon footprints\nof machine learning,” Journal of Machine Learning Research, vol. 21,\nno. 248, pp. 1–43, 2020.\n[18] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction,\n2nd ed.\nCambridge, MA, USA: A Bradford Book, 2018. [Online].\nAvailable: https://dl.acm.org/doi/book/10.5555/3312046\n[19] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\nS. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\nD. Wierstra, S. Legg, and D. Hassabis, “Human-level control through\ndeep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533,\nfeb 2015.\n[20] P. Mileff and J. Dudra, “Efﬁcient 2D software rendering,” Production\nSystems and Information Engineering, vol. 6, no. 2, pp. 55–66, 2012.\n[21] O. Mendel and J. Bergstr¨om, “SIMD Optimizations of Software Ren-\ndering in 2D Video Games,” p. 27, 2019.\n[22] O. S. Lawlor, “Message passing for GPGPU clusters: CudaMPI,”\nProceedings - IEEE International Conference on Cluster Computing,\nICCC, 2009.\n[23] S. Raschka, J. Patterson, and C. Nolet, “Machine Learning in\nPython: Main Developments and Technology Trends in Data Science,\nMachine Learning, and Artiﬁcial Intelligence,” Information 2020,\nVol. 11, Page 193, vol. 11, no. 4, p. 193, apr 2020. [Online].\nAvailable: https://www.mdpi.com/2078-2489/11/4/193/htmhttps://www.\nmdpi.com/2078-2489/11/4/193\n[24] F. Zehra, M. Javed, D. Khan, and M. Pasha, “Comparative Analysis of\nC++ and Python in Terms of Memory and Time,” dec 2020. [Online].\nAvailable: https://www.preprints.org/manuscript/202012.0516/v1\n[25] P.-A. Andersen, M. Goodwin, and O.-C. Granmo, “Deep RTS: A Game\nEnvironment for Deep Reinforcement Learning in Real-Time Strategy\nGames,” in 2018 IEEE Conference on Computational Intelligence and\nGames (CIG), aug 2018, pp. 1–8.\n[26] S. Ontanon, “The combinatorial multi-armed bandit problem and\nits application to real-time strategy games,” in Proceedings, The\nNinth AAAI Conference on Artiﬁcial Intelligence and Interactive\nDigital Entertainment, 2013, pp. 58–64. [Online]. Available: http:\n//www.aaai.org/ocs/index.php/AIIDE/AIIDE13/paper/viewPaper/7377\n[27] S. Lee and J. Togelius, “Showdown AI competition,” 2017 IEEE\nConference on Computational Intelligence and Games, CIG 2017, pp.\n191–198, oct 2017.\n[28] S. Huang, S. Ontanon, C. Bamford, and L. Grela, “Gym-MicroRTS:\nToward Affordable Full Game Real-time Strategy Games Research\nwith Deep Reinforcement Learning,” in Proc. 3rd IEEE Conference\non Games, may 2021, p. 19. [Online]. Available: https://arxiv.org/abs/\n2105.13807v3\n[29] S.\nBauer,\n“Simon\nTatham’s\nPortable\nPuzzle\nCollection,”\nLinux\nUser Group, Frankfurt, Tech. Rep., feb 2021. [Online]. Available:\nhttps://www.lugfrankfurt.de/talks/SGTPuzzles FraLug.pdf\n[30] F. Dandurand, D. Cousineau, and T. R. Shultz, “Solving nonogram\npuzzles by reinforcement learning,” Proceedings of the Annual Meeting\nof the Cognitive Science Society, vol. 34, no. 34, p. 6, 2012.\n[31] R. S. Sutton, D. Precup, and S. Singh, “Between MDPs and semi-\nMDPs: A framework for temporal abstraction in reinforcement learning,”\nArtiﬁcial Intelligence, vol. 112, no. 1-2, pp. 181–211, 1999.\n[32] E. Bargiacchi, D. M. Roijers, and A. Now´e, “AI-Toolbox: A C++ library\nfor Reinforcement Learning and Planning (with Python Bindings),”\nJournal of Machine Learning Research, vol. 21, no. 102, pp. 1–12,\n2020. [Online]. Available: http://jmlr.org/papers/v21/18-402.html\n[33] P. Fua and K. Lis, “Comparing Python, Go, and C++ on the N-Queens\nProblem,” 2020. [Online]. Available: https://arxiv.org/abs/2001.02491\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-10-03",
  "updated": "2022-10-03"
}