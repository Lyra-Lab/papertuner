{
  "id": "http://arxiv.org/abs/2210.02324v1",
  "title": "Promising or Elusive? Unsupervised Object Segmentation from Real-world Single Images",
  "authors": [
    "Yafei Yang",
    "Bo Yang"
  ],
  "abstract": "In this paper, we study the problem of unsupervised object segmentation from\nsingle images. We do not introduce a new algorithm, but systematically\ninvestigate the effectiveness of existing unsupervised models on challenging\nreal-world images. We firstly introduce four complexity factors to\nquantitatively measure the distributions of object- and scene-level biases in\nappearance and geometry for datasets with human annotations. With the aid of\nthese factors, we empirically find that, not surprisingly, existing\nunsupervised models catastrophically fail to segment generic objects in\nreal-world images, although they can easily achieve excellent performance on\nnumerous simple synthetic datasets, due to the vast gap in objectness biases\nbetween synthetic and real images. By conducting extensive experiments on\nmultiple groups of ablated real-world datasets, we ultimately find that the key\nfactors underlying the colossal failure of existing unsupervised models on\nreal-world images are the challenging distributions of object- and scene-level\nbiases in appearance and geometry. Because of this, the inductive biases\nintroduced in existing unsupervised models can hardly capture the diverse\nobject distributions. Our research results suggest that future work should\nexploit more explicit objectness biases in the network design.",
  "text": "Promising or Elusive? Unsupervised Object\nSegmentation from Real-world Single Images\nYafei Yang\nBo Yang\nvLAR Group, The Hong Kong Polytechnic University\nya-fei.yang@connect.polyu.hk\nbo.yang@polyu.edu.hk\nAbstract\nIn this paper, we study the problem of unsupervised object segmentation from\nsingle images. We do not introduce a new algorithm, but systematically investi-\ngate the effectiveness of existing unsupervised models on challenging real-world\nimages. We ﬁrstly introduce four complexity factors to quantitatively measure\nthe distributions of object- and scene-level biases in appearance and geometry for\ndatasets with human annotations. With the aid of these factors, we empirically\nﬁnd that, not surprisingly, existing unsupervised models catastrophically fail to\nsegment generic objects in real-world images, although they can easily achieve\nexcellent performance on numerous simple synthetic datasets, due to the vast gap\nin objectness biases between synthetic and real images. By conducting extensive\nexperiments on multiple groups of ablated real-world datasets, we ultimately ﬁnd\nthat the key factors underlying the colossal failure of existing unsupervised models\non real-world images are the challenging distributions of object- and scene-level\nbiases in appearance and geometry. Because of this, the inductive biases introduced\nin existing unsupervised models can hardly capture the diverse object distribu-\ntions. Our research results suggest that future work should exploit more explicit\nobjectness biases in the network design.\n1\nIntroduction\nThe capability of automatically identifying individual objects from complex visual observations\nis a central aspect of human intelligence [53]. It serves as the key building block for higher-level\ncognition tasks such as planning and reasoning [28]. In last years, a plethora of models have been\nproposed to segment objects from single static images in an unsupervised fashion: from the early\nAIR [22] and MONet [8] to the recent SPACE [39], SlotAtt [40], GENESIS-V2 [20], etc. They\njointly learn to represent and segment multiple objects from a single image, without needing any\nhuman annotations in training. This process is often called perceptual grouping/binding or object-\ncentric learning. These methods and their variants have achieved impressive segmentation results on\nnumerous synthetic scene datasets such as dSprites [42] and CLEVR [33]. Such advances come with\ngreat expectations that the unsupervised techniques would likely close the gap with fully-supervised\nmethods for real-world visual understanding. However, few work has systematically investigated\nthe true potential of the emerging unsupervised object segmentation models on complex real-world\nimages such as COCO dataset [38]. This naturally raises an essential question:\nIs it promising or even possible to segment generic objects from real-world single images using\n(existing) unsupervised methods?\nWhat is an object? To answer the above question involves another fundamental question: what\nis an object? Exactly 100 years ago in Gestalt psychology, Wertheimer [61] ﬁrst introduced a set\nof Gestalt principles such as proximity, similarity and continuation to heuristically deﬁne visual\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2210.02324v1  [cs.CV]  5 Oct 2022\nSingle \nImages\nPredicted \nObject \nMasks\nFigure 1: The failure of SlotAtt [40] on three real-world images (right-hand side), although it can\nperfectly segment simple objects on three synthetic images (left-hand side).\ndata as objects. However, these factors are highly subjective, whilst the real-world generic objects\nare far more complex with extremely diverse appearances and shapes. Therefore, it is practically\nimpossible to quantitatively deﬁne what is an object, i.e., the objectness, from visual inputs (e.g., a\nset of image pixels). Nevertheless, to thoroughly understand whether unsupervised methods can truly\nlearn objectness akin to the psychological process of humans, it is vital to investigate the underlying\nfactors that potentially facilitate or otherwise hinder the ability of unsupervised models. In this regard,\nby drawing on Gestalt principles, we instead deﬁne a series of new factors to quantitatively measure\nthe complexity of objects and scenes in Section 2. By taking into account both the appearance and\ngeometry of objects and scenes, our complexity factors explicitly assess the difﬁculty of segmenting\na speciﬁc object. For example, a chair with colorful textures tend to have higher complexity than\na single-color ball for unsupervised methods. With the aid of these factors, we extensively study\nwhether and how existing unsupervised models can discover objects in Section 4.\nWhat is the problem of unsupervised object segmentation from single images? A large number\nof models [63] aim to tackle the problem of unsupervised object segmentation from single images.\nThey share several key problem settings: 1) all training images do not have any human annotations;\n2) every single image has multiple objects; 3) each image is treated as a static data point without\nany dynamic or temporal information; 4) all models are trained from scratch without requiring any\npretrained networks on additional datasets. Ultimately, the goal of these models is to segment all\nindividual objects as accurate as the ground truth human annotations. In this paper, we regard these\nsettings as the basic and necessary part of unsupervised object segmentation from single images, and\nempirically evaluate how successfully the existing models can exhibit on real-world images.\nContributions and ﬁndings. This paper addresses the essential question regarding the potential of\nunsupervised segmentation of generic objects from real-world single images. Our contributions are:\n• We ﬁrstly introduce 4 complexity factors to quantitatively measure the difﬁculty of objects and\nscenes. These factors are key to investigate the true potential of existing unsupervised models.\n• We extensively evaluate current unsupervised approaches in a large-scale experimental study.\nWe implement 4 representative methods and train more than 130 models on 6 curated datasets\nfrom scratch. The datasets, code and pretrained models are available at https://github.com/\nvLAR-group/UnsupObjSeg\n• We analyze our experimental results and ﬁnd that: 1) existing unsupervised object segmentation\nmodels cannot discover generic objects from single real-world images, although they can achieve\noutstanding performance on synthetic datasets, as qualitatively illustrated in Figure 1; 2) the\nchallenging distributions of both object- and scene-level biases in appearance and geometry from\nreal-world images are the key factors incurring the failure of existing models; 3) the inductive biases\nintroduced in existing unsupervised models are fundamentally not matched with the objectness\nbiases exhibited in real-world images, and therefore fail to discover the real objectness.\nRelated Work. Recently, ClevrTex [35] and the concurrent work [43] also study unsupervised object\nsegmentation on single images. Through evaluation on (complex) synthetic datasets only, both works\nfocus on benchmarking the effectiveness of particular network designs of baselines. By comparison,\nour paper aims to explore what and how the objectness distribution gaps between synthetic and\nreal-world images incur the failure of existing models. The recent work [60] which investigates video\nobject discovery is orthogonal to our work as the motion signals do not exist in single images.\nScope of this research. This paper does not investigate unsupervised object discovery on saliency\nmaps [59], static multi-views or dynamic videos [63]. Recent methods [10; 30] requiring pretrained\nmodels on monolithic object images such as ImageNet [47] are not evaluated as well.\n2\n2\nComplexity Factors\nObject #1\nObject #2\nObject #3\nScene #1\nScene #2\nScene #3\nFigure 2: Complexity in appearance\nand geometry for objects and scenes.\nAs illustrated in the top row of Figure 2, an individual object,\nrepresented by a set of color pixels painted within a mask,\ncan vary signiﬁcantly given different types of appearance and\ngeometric shape. A speciﬁc scene, represented by a set of\nobjects placed within an image, can also differ vastly given\ndifferent types of relative appearance and geometric layout\nbetween objects, as illustrated in the bottom row. Unarguably,\nsuch variation and complexity of appearance and geometry in\nboth object level and scene level directly affects human’s ability\nto precisely separate all objects. Naturally, the performance\nof unsupervised segmentation models are also expected to be\ninﬂuenced by the variation. In this regard, we carefully deﬁne\nthe following two groups of factors to quantitatively describe\nthe complexity of different datasets.\n2.1\nObject-level Complexity Factors\nAs to a speciﬁc object, all its information can be described by appearance and geometry. Therefore\nwe deﬁne the below two factors to measure the complexity of appearance and geometry respectively.\nNotably, both factors are nicely invariant to the object scale.\n• Object Color Gradient: This factor aims to calculate how frequently the appearance changes\nwithin the object mask. In particular, given the RGB image and mask of an object, we ﬁrstly\nconvert RGB into grayscale and then apply Sobel ﬁlter [51] to compute the gradients horizontally\nand vertically for each pixel within the mask. The ﬁnal gradient value is obtained by averaging out\nall object pixels. Note that, the object boundary pixels are removed to avoid the interference of\nbackground. Numerically, the higher this factor is, the more complex texture and/or lighting effect\nthe object has, and therefore it is likely harder to segment.\n• Object Shape Concavity: This factor is designed to evaluate how irregular the object boundary\nis. Particularly, given an object (binary) mask, denoted as M obj ∈RH×W , we ﬁrstly ﬁnd the\nsmallest convex polygon mask (M cvx ∈RH×W ) that surrounds the object mask using an existing\nalgorithm [19], and then the object shape concavity value is computed as: 1 −P M obj/ P M cvx.\nClearly, the higher this factor is, the more irregular object shape is, and segmentation is more tricky.\n2.2\nScene-level Complexity Factors\nAs to a speciﬁc image, in addition to the object-level complexity, the spatial and appearance relation-\nships between all objects can also incur extra difﬁculty for segmentation. We deﬁne the following two\nfactors to quantify the complexity of relative appearance and geometry between objects in an image.\n• Inter-object Color Similarity: This factor intends to assess the appearance similarity between all\nobjects in the same image. Speciﬁcally, we ﬁrstly calculate the average color for each object, and\nthen compute the pair-wise Euclidean distances of object colors, obtaining a K × K matrix where\nK represents the object number. The average color distance is calculated by averaging the matrix\nexcluding diagonal entries, and the ﬁnal inter-object color similarity is computed as: 1−average\ncolor distance/(255 ×\n√\n3). Intuitively, the higher this factor is, the more similar all objects appear\nto be, the less distinctive each object is, and it is harder to separate each object.\n• Inter-object Shape Variation: This factor aims to measure the relative geometry diversity between\nall objects in the image. We ﬁrstly calculate the diagonal length of bounding box for each object,\nand then compute the pair-wise absolute differences for all object diagonal lengths, obtaining a\nK ×K matrix. The ﬁnal inter-object shape variation is the average of the matrix excluding diagonal\nentries. The higher this factor, the objects within an image have more diverse and imbalanced sizes,\nand therefore segmenting both gigantic and tiny objects is likely more challenging.\nBy capturing the appearance and geometry in both object and scene levels, the four factors are\ndesigned to quantify the complexity of objects and images. For illustration, Figure 3 shows sample\nimages for the four factors at different values. The higher the values, the more complex the objects\n3\nobject -\nlevel\nObject Color Gradient\n0.000\n0.165\n0.518\n0.802\nObject Shape Concavity\n0.020\n0.121\n0.526\n0.750\nscene -\nlevel\n0.265\n0.359\n0.787\n0.936\n0.005\n0.105\n0.257\n0.565\nInter-object Color Similarity\nInter-object Shape Variation\nFigure 3: Sample objects and scenes for the four factors at different complexity values. All complexity\nvalues are normalized to the range of [0, 1].\nand scenes. In fact, these factors are carefully selected from more than 10 candidates because they are\nempirically more suitable to differentiate the gaps between synthetic and real-world images, and they\neventually serve as key indicators to diagnose existing unsupervised models in Section 4. Calculation\ndetails of the four factors and other candidates are in appendix.\n3\nExperimental Design\n3.1\nConsidered Methods\nA range of works have explored unsupervised object segmentation in recent years. They are typically\nformulated as (variational) autoencoders (AE/VAE) [36] or generative adversarial networks (GAN)\n[24]. GAN based models [13; 3; 7; 56; 5; 58; 1] are usually limited to identifying a single foreground\nobject and can hardly discover multiple objects due to the training instabilities, therefore not consid-\nered in this paper. As shown in Table 1, the majority of existing models are based on AE/VAE and\ncan be generally divided into two groups according to the object representation:\n• Factor-based models: Each object is represented by explicit factors such as size, position, ap-\npearance, etc., and the whole image is a spatial organization of multiple objects. Basically, such\nrepresentation explicitly enforces objects to be bounded within particular regions.\n• Layer-based models: Each object is represented by an image layer, i.e., a binary mask, and the\nwhole image is a spatial mixture of multiple object layers. Intuitively, this representation does not\nhave strict spatial constrains, and instead is more ﬂexible to cluster similar pixels as objects.\nIn order to decompose the input images into objects, these approaches introduce different types\nof network architecture, loss functions, and regularization terms as inductive biases. These biases\nbroadly include: 1) variational encoding which encourages the disentanglement of latent variables; 2)\niterative inference which likely ends up with better scene representations over occlusions; 3) object\nrelationship regularization such as depth estimation and autoregressive prior which aims at capturing\nthe dependency of multiple objects; and many other biases. With different combinations of these\nbiases, many methods have shown outstanding performance in synthetic datasets. Among them, we\nselect 4 representative models for our investigation: 1) AIR [22], 2) MONet [8], 3) IODINE [25],\nand 4) SlotAtt [40]. We also add the fully-supervised Mask R-CNN [29] as an additional baseline for\ncomprehensive comparison. Implementation details are provided in appendix.\n3.2\nConsidered Datasets\nWe consider two groups of datasets for extensive benchmarking and analysis: 1) three commonly-used\nsynthetic datasets: dSprites [42], Tetris [34] and CLEVR [33], 2) three real-world datasets: YCB [9],\nScanNet [17], and COCO [38], representing the small-scale, indoor- and outdoor-level real scenes\nrespectively. Naturally, objects and scenes in different datasets tend to have very different types of\nbiases. For example, the objects in dSprites tend to have the single-color bias, while COCO does not.\nGenerally, the object-level biases can be divided as: 1) appearance biases including different textures\nand lighting effects, and 2) geometry biases including the object shape and occlusions. Similarly,\nthe scene-level biases include: 1) appearance biases such as the color similarity between all objects,\nand 2) geometry biases such as the diversity of all object shapes. In fact, our complexity factors\nintroduced in Section 2 are designed to well capture these biases. Table 2 qualitatively summarizes\nthe biases of selected datasets. We may hypothesize that the large gaps of biases between synthetic\nand real-world datasets would have a huge impact on the effectiveness of existing models.\n4\nTable 1: Existing unsupervised models for object segmentation on single images. Each model\nincludes different inductive biases, such as variational autoencoding (VAE), iterative inference (Iter),\nobject relationship regularization (Rel), etc.\nFactor-based Models\nInductive Biases\nLayer-based Models\nInductive Biases\nVAE\nIter\nRel\nVAE\nIter\nRel\nCST-VAE [31]\nICLRW’16\n✓\nTagger [26]\nNIPS’16\n✓\nAIR [22]\nNIPS’16\n✓\nRC [26]\nICLRW’16\n✓\nSPAIR [16]\nAAAI’19\n✓\nNEM [27]\nNIPS’17\n✓\nSuPAIR [54]\nICML’19\n✓\nMONet [8]\narXiv’19\n✓\nGMIO [64]\nICML’19\n✓\n✓\nIODINE [25]\nICML’19\n✓\n✓\nASR [62]\nNeurIPS’19\n✓\nECON [57]\nICLRW’20\n✓\n✓\nSPACE [39]\nICLR’20\n✓\nGENESIS [21]\nICLR’20\n✓\n✓\nGNM [32]\nNeurIPS’20\n✓\n✓\nSlotAtt [40]\nNeurIPS’20\n✓\nSPLIT [11]\narXiv’20\n✓\nGENESIS-V2 [20]\nNeurIPS’21\n✓\n✓\nOCIC [2]\narXiv’20\n✓\n✓\nR-MONet [50]\narXiv’21\n✓\n✓\nGSGN [18]\nICLR’21\n✓\n✓\nCAE [41]\narXiv’22\n✓\nTable 2: The object- and scene-level biases in appearance and geometry of the considered datasets.\nSynthetic Datasets\nReal-world Datasets\ndSprites [42]\nTetris [34]\nCLEVR [33]\nYCB [9]\nScanNet [17]\nCOCO [38]\nObject-level Biases\nAppearance\nTexture:\nsimple\nsimple\nsimple\ndiverse\nsimple\ndiverse\nLighting:\nno\nno\nsynthetic\nreal\nreal\nreal\nGeometry\nShape:\nsimple\nsimple\nsimple\nsimple\ndiverse\ndiverse\nOcclusion:\nminor\nno\nminor\nsevere\nsevere\nsevere\nScene-level Biases\nAppearance\nSimilarity:\nlow\nlow\nhigh\nhigh\nhigh\nhigh\nGeometry\nDiversity:\nlow\nlow\nlow\nhigh\nhigh\nhigh\nTo guarantee the fairness and consistency of all experiments, we carefully prepare all six datasets\nusing the following same protocols. Preparation details for each dataset are provided in appendix.\n• All images are rerendered or cropped with the same resolution of 128 × 128.\n• Each image has about 2 to 6 solid objects with a blank background.\n• Each dataset has about 10000 images for training, 2000 images for testing.\n3.3\nConsidered Metrics\nHaving the six representative datasets and four existing unsupervised methods at hand, we choose\nthe following metrics to evaluate the object segmentation performance: 1) AP score which is widely\nused for object detection and segmentation [23], 2) PQ score which is used to measure non-overlap\npanoptic segmentation [37], and 3) Precision and Recall scores. A predicted mask is considered\ncorrect if its IoU against a ground truth mask is above 0.5. All objects are treated as a single class. The\nblank background is not taken into account for fair comparison. To compute AP, we simply treat the\nmean value of the soft object mask as the object conﬁdence score. Note that, the alternative metrics\nARI [45] and segmentation covering (SC) [4] are not considered as they can be easily saturated.\n4\nKey Experimental Results\n4.1\nCan current unsupervised models succeed on real-world datasets?\nFirst of all, we evaluate all baselines on our six datasets separately. In particular, we train each model\nfrom scratch on each dataset separately. For fair evaluation, we carefully tune the hyperparameters of\neach model on every dataset and fully optimize the networks until convergence. Figure 4 compares\nthe quantitative results. It can be seen that all methods demonstrate satisfactory segmentation results\non synthetic datasets, especially the recent strong baselines IODINE and SlotAtt. However, not\nsurprisingly, all unsupervised methods fail catastrophically on the three real-world datasets.\n5\n(a) AIR\n(b) MONet\n(c) IODINE\n(d) SlotAtt\n(e) MaskRCNN\n0.0%\n25.0%\n50.0%\n75.0%\n100.0%\ndSprites\nTetris\nCLEVR\nYCB\nScanNet\nCOCO\ndSprites\nTetris\nCLEVR\nYCB\nScanNet\nCOCO\ndSprites\nTetris\nCLEVR\nYCB\nScanNet\nCOCO\ndSprites\nTetris\nCLEVR\nYCB\nScanNet\nCOCO\ndSprites\nTetris\nCLEVR\nYCB\nScanNet\nCOCO\n%\nAP\nPQ\nPre\nRec\nFigure 4: Quantitative results of object segmentation from the ﬁve methods on six datasets.\nPreliminary Diagnosis: In order to diagnose the colossal failure, we hypothesize that it is because\nof the huge gaps in objectness biases between two types of datasets. In this regard, we quantitatively\ncompute the distributions of our four complexity factors on the six datasets. In particular, the two\nobject-level factors, i.e., Object Color Gradient and Object Shape Concavity, are computed for each\nobject of the six training splits. The two scene-level factors, i.e., Inter-object Color Similarity and\nInter-object Shape Variation, are computed for each image of the six training splits.\nFrom the distributions shown in the top row of Figure 5, we can see that, 1) for the two object-level\nfactors (Subﬁgs 1-a and 1-c), the three synthetic datasets tend to have extremely lower scores than\nthe real-world datasets, which means that the synthetic objects are more likely have uniform colors\nand convex shapes; 2) for the two scene-level factors (Subﬁgs 1-b and 1-d), the images in synthetic\ndatasets tend to include less similar objects in terms of color, which means that multiple objects in\nreal-world scenes are less distinctive in appearance. In addition, the multiple objects in synthetic\nscenes tend to have similar sizes, whereas real-world scenes usually have diverse object scales in\nsingle images. To validate whether these distribution biases are the true reasons incurring the failure,\nwe conduct extensive ablative experiments in Sections 4.2, 4.3 and 4.4.\nObject Color Gradient\nObject Shape Concavity\nInter-object Color Similarity\nInter-object Shape Variation\noriginal\nobject-\nlevel \nablation\nscene-\nlevel \nablation\ncolor ablations\nshape ablations\n(1-a)\n(1-b)\n(2-a)\n(2-b)\n(3-a)\n(3-b)\n(1-c)\n(1-d)\n(2-c)\n(2-d)\n(3-c)\n(3-d)\nFigure 5: Distributions of the four complexity factors. The top row shows the distributions of original\nsynthetic/real-world datasets in Sec 4.1. The 2nd row shows the distributions of object-level ablated\ndatasets in Sec 4.2. The 3rd row shows distributions of scene-level ablated datasets in Sec 4.3.\n4.2\nHow do object-level factors affect current models?\nIn this section, we aim to verify to what extent the distributions of object-level factors affect the\nsegmentation performance. In particular, we conduct the following three ablative experiments.\n• Ablation of Object Color Gradient: For each object of the three real-world datasets, we only replace\nall pixel colors by its average color rgb value within each object mask, without touching the object\nshapes. In this way, the color gradients of each object are totally erased, thus removing the potential\nimpact of Object Color Gradient. The three ablated datasets are: YCB-C / ScanNet-C / COCO-C.\n• Ablation of Object Shape Concavity: For each object in the real-world datasets, we ﬁnd the smallest\nconvex hull [19] for its object mask and then ﬁll the empty pixels by shifting the original object\n6\n100%\n75%\n50%\n25%\n0%\nT+U\nC+S+T\nC+S+U\nC+S+T+U\nScanNet\nC\nS\nC+S\nT\nU\nT+U\nC+S+T\nC+S+U\nC+S+T+U\nYCB\nC\nS\nC+S\nT\nU\nT+U\nC+S+T\nC+S+U\nC+S+T+U\nCOCO\nC\nS\nC+S\nT\nU\nSlotAtt\nT+U\nC+S+T\nC+S+U\nC+S+T+U\nScanNet\nC\nS\nC+S\nT\nU\nT+U\nC+S+T\nC+S+U\nC+S+T+U\nYCB\nC\nS\nC+S\nT\nU\nIODINE\nAP\nPQ\nPre\nRec\n100%\n75%\n50%\n25%\n0%\n100%\n75%\n50%\n25%\n0%\nMONet\nAIR\n100%\n75%\n50%\n25%\n0%\nT+U\nC+S+T\nC+S+U\nC+S+T+U\nYCB\nC\nS\nC+S\nT\nU\nT+U\nC+S+T\nC+S+U\nC+S+T+U\nYCB\nC\nS\nC+S\nT\nU\nT+U\nC+S+T\nC+S+U\nC+S+T+U\nScanNet\nC\nS\nC+S\nT\nU\nT+U\nC+S+T\nC+S+U\nC+S+T+U\nScanNet\nC\nS\nC+S\nT\nU\nT+U\nC+S+T\nC+S+U\nC+S+T+U\nCOCO\nC\nS\nC+S\nT\nU\nT+U\nC+S+T\nC+S+U\nC+S+T+U\nCOCO\nC\nS\nC+S\nT\nU\nT+U\nC+S+T\nC+S+U\nC+S+T+U\nCOCO\nC\nS\nC+S\nT\nU\nFigure 7: Quantitative results of baselines on the three real-world datasets and their variants. The\nletters C/S/C+S represent the three ablated datasets in Sec 4.2; T/U/T+U represent the three ablated\ndatasets in Sec 4.3; C+S+T/C+S+U/C+S+T+U represent the three ablated datasets in Sec 4.4.\npixels. Basically, this ablation aims to only reduce the irregularity of object shapes, yet retaining\nthe distributions of color gradients. The ablated datasets are: YCB-S / ScanNet-S / COCO-S.\n• Ablation of both Object Color Gradient and Shape Concavity: We simply combine the above two\nablation for each real-world object, getting datasets: YCB-C+S / ScanNet-C+S / COCO-C+S.\nYCB-C\nYCB-S\nYCB-C+S\nYCB-T\nYCB-U\nYCB-T+U\nYCB-C+S+T\nYCB-C+S+U YCB-C+S+T+U\nYCB\nFigure 6: Sample images with\ndifferent ablations. The top row\nshows an original image from\nYCB dataset.\nThe 2nd row\nshows examples from three ab-\nlated datasets in Sec 4.2. The 3rd\nrow presents examples from three\nablated datasets in Sec 4.3. The\nlast row shows examples from\nthree ablated datasets in Sec 4.4\nFor illustration, the 2nd row of Figure 6 shows example images\nof three ablated datasets: YCB-C / YCB-S / YCB-C+S.\nIn the 2nd row of Figure 5 (Subﬁgs 2-a/2-b), we calculate new\ndistributions of both Object Color Gradient and Inter-object Color\nSimilarity on the datasets YCB-C / ScanNet-C / COCO-C. We can\nsee that the object-level gradients become all zeros in Subﬁg 2-a,\neven simpler than three synthetic datasets. Yet, the distributions\nof Inter-object Color Similarity are almost the same as original\nYCB / ScanNet / COCO, i.e., Subﬁg 2-b is similar to 1-b.\nSimilarly, the 2nd row of of Figure 5 (Subﬁgs 2-c/2-d) shows that\nthe distributions of Object Shape Concavity of ablated datasets\nnow become similar to the synthetic datasets, while the distribu-\ntions of Inter-object Shape Variation keep the same, i.e., Subﬁg\n2-d is similar to Subﬁg 1-d. Note that, for the ablation of both\nObject Color Gradient and Shape Concavity, the distributions will\nbe the same as shown in the 2nd row of Figure 5. Having the\nthree groups of object-level ablated real-world datasets, we then\nevaluate segmentation performance of the baselines separately.\nBrief Analysis: As shown in Figures 7 & 9, we can see that: 1)\nOnce the pixels of real-world objects are replaced by its mean\ncolor, i.e., without any color gradients, the object segmentation\nperformance has been signiﬁcantly improved for almost all meth-\nods. 2) Reducing the irregularity of real-world objects can also\nimprove the object segmentation, although not signiﬁcantly. 3)\nOverall, these results show that existing methods are more likely\nto learn the objectness represented by uniform colors and/or reg-\nular objects. However, comparing with Figure 4, the results of\n7\ncurrent ablated datasets in Figure 7 still lag behind the synthetic datasets. This means that there must\nbe some other factors that also potentially affect the object segmentation of existing models. More\nresults are in appendix.\n4.3\nHow do scene-level factors affect current models?\nIn this section, we turn to investigate to what extent the distributions of scene-level factors affect the\nsegmentation performance. Particularly, we conduct the following three ablative experiments.\n• Ablation of Inter-object Color Similarity: In each image of the three real-world datasets, we replace\nall object textures by a set of new distinctive textures from the existing DTD database [15], as\nshown in Figure 8. In this way, the multiple objects look more distinctive in appearance, while\nthe per-object texture gradients are roughly preserved. The ablated datasets are denoted: YCB-T /\nScanNet-T / COCO-T.\n• Ablation of Inter-object Shape Variation: In each image of the real-world datasets, we normalize\nthe scales of multiple objects by shrinking or expanding the diagonal length of their bounding\nboxes, such that the new object sizes tend to be uniform. For each object, its shape and texture are\nlinearly scaled up or down. Basically, this aims to remove the diversity of object sizes within single\nimages. The ablated datasets are denoted as: YCB-U / ScanNet-U / COCO-U.\n• Ablation of both Inter-object Color Similarity and Shape Variation: We simply combine the above\ntwo ablation strategies for each real-world image. Ablation details are in appendix. The ablated\ndatasets are denoted as: YCB-T+U / ScanNet-T+U / COCO-T+U.\nFigure 8: Six selected texture images\nfrom DTD [15].\nFor illustration, the 3rd row of Figure 6 shows example\nimages of three ablated datasets: YCB-T / YCB-U / YCB-\nT+U.\nIn the 3rd row of Figure 5 (Subﬁgs 3-a/3-b), we calculate\nnew distributions of both Object Color Gradient and Inter-\nobject Color Similarity on the ablated datasets YCB-T /\nScanNet-T / COCO-T. We can see that the distributions of\nscene-level color similarity become more similar to synthetic\ndatasets in Subﬁg 3-b, while the distributions of object color\ngradients are still similar as original real-world datasets YCB\n/ ScanNet / COCO, i.e., Subﬁg 3-a is similar to 1-a.\nSimilarly, the 3rd row of Figure 5 shows that the distributions of Inter-object Shape Variation of\nablated datasets now become similar to the synthetic datasets in Subﬁg 3-d, whereas the distributions\nof Object Shape Concavity are still the same as original real-world datasets YCB / ScanNet / COCO,\ni.e., Subﬁg 3-c is similar to Subﬁg 1-c. Note that, for the ablation of both Inter-object Color Similarity\nand Shape Variation, the distributions will be the same as the 3rd row of Figure 5. Having these three\ngroups of scene-level ablated real-world datasets, we then separately evaluate each baseline.\nBrief Analysis: As shown in Figures 7 & 9, we can see that: 1) Once the textures of real-world\nimages are replaced by more distinctive textures, i.e., with lower similarity between object appearance,\nthe segmentation performance has been surprisingly boosted remarkably for almost all methods. 2)\nNormalizing object sizes over images can also reasonably improve the segmentation performance. 3)\nOverall, these results clearly show that existing unsupervised models signiﬁcantly favor objectness\nwith distinctive appearance in single images. However, compared with Figure 4, the results on current\nscene-level ablated datasets are still inferior to synthetic datasets, meaning that the scene-level factors\nalone are not enough to explain the performance gap. More results are in appendix.\n4.4\nHow do object- and scene-level factors jointly affect current models?\nIn this section, we aim to study how the object- and scene-level factors jointly affect the segmentation\nperformance. In particular, we conduct the following three ablative experiments.\n• Ablation of Object Color Gradient, Object Shape Concavity and Inter-object Color Similarity: In\neach image of the three real-world datasets, we replace the object color by averaging all pixels of\nthe distinctive texture, and also replace the object shape with a simple convex hull. The ablated\ndatasets are denoted: YCB-C+S+T / ScanNet-C+S+T / COCO-C+S+T.\n8\n• Ablation of Object Color Gradient, Object Shape Concavity and Inter-object Shape Variation:\nIn each image of the three real-world datasets, we replace the object color by averaging its own\ntexture, and modify the object shape as convex hull following by size normalization. The ablated\ndatasets are denoted as: YCB-C+S+U / ScanNet-C+S+U / COCO-C+S+U.\n• Ablation of all four factors: We aggressively combine all four ablation strategies and we get\ndatasets: YCB-C+S+T+U / ScanNet-C+S+T+U / COCO-C+S+T+U.\nFor illustration, the 4th row of Figure 6 shows example images of three ablated datasets: YCB-C+S+T\n/ YCB-C+S+U / YCB-C+S+T+U.\nSince these ablations are conducted independently, the new distributions of four complexity factors\non current jointly ablated datasets are exactly the same as the second and third rows of Figure 5.\nBrief Analysis: As shown in Figure 7, we can see that: 1) Combining the two object-level factors\nand either of scene-level factors to ablate the real-world datasets, the segmentation performance\ncan be improved as we expect, especially for IODINE and SlotAtt. 2) If the challenging real-world\nobjects and images are ablated in both object- and scene-level, the segmentation performance of\nall unsupervised models achieves the same level with three synthetic datasets as shown in Figure 4.\n3) Overall, these three groups of experiments demonstrate that the colossal failure of unsupervised\nmodels on real-world images involves both object- and scene-level dataset biases. More experiments\nand results are in appendix.\nMONet\nAIR\nGT \nMasks\nOriginal\nImages\nAblated\nImages\nIODINE\nSlotAtt\nInter-object Color \nSimilarity Ablation\nInter-object Shape \nVariation Ablation\nYCB-T\nScanNet-T COCO-T\nYCB-U\nScanNet-U COCO-U\nObject Shape Concavity Ablation\nYCB-S\nScanNet-S COCO-S\nYCB-C\nScanNet-C COCO-C\nObject Color Gradient Ablation\nFigure 9: Qualitative results of four representative methods on multiple ablation datasets in Sec 4.2\nand Sec 4.3.\n4.5\nWhy do current unsupervised models fail on real-world datasets?\nAs demonstrated in Sections 4.2/4.3/4.4, once the complexity factors are removed from the challenging\nreal-world datasets, existing unsupervised models can perform as excellent as on the synthetic datasets,\nas qualitatively illustrated in Figure 10. From this, we can safely conclude that the inductive biases\ndesigned in existing unsupervised models are far from able to match with and fully capture the true\nand complex objectness biases exhibited in real-world images. Nevertheless, from Figure 7, each\nbaseline tends to favor different objectness biases. In particular,\n• AIR [22]: As a factor based model, AIR has a strong spatial-locality bias. Despite its poor seg-\nmentation performance across all datasets, there is a notable improvement when inter-object shape\nvariation is ablated from real-world datasets (U / T+U / C+S+U / C+S+T+U). More convinc-\ningly, even when all other three factors are ablated (C+S+T), it can be hardly improved. These\nobservations show that object shape variation is a signiﬁcant factor for AIR.\n9\nYCB-\nC+S+T+U\nScanNet-\nC+S+T+U\nCOCO-\nC+S+T+U\nMONet\nAIR\nGT \nMasks\nOriginal\nImages\nAblated\nImages\nIODINE\nSlotAtt\nFigure 10: Qualitative results on fully ablated real-world datasets (YCB-C+S+T+U / ScanNet-\nC+S+T+U / COCO-C+S+T+U).\n• MONet [8]: MONet is more sensitive to color-related factors than shape-related factors. The abla-\ntions of object color gradient and inter-object color similarity signiﬁcantly improve its performance,\nwhile ablations of object shape concavity and inter-object shape variation make little differences.\nFor the two color-related factors, the scene-level one is more important than the object-level factor.\nFrom this, we can see that MONet has a strong dependency on color. Similar colors tend to be\ngrouped together while different colors are separated apart. Furthermore, the ablation on object\ncolor gradient alleviates over-segmentation whereas the ablation on inter-object color similarity\nalleviates under-segmentation. We conjecture that under-segmentation can be a more severe issue\nfor MONet on real-world datasets, leading to a larger sensitivity on the scene-level color factor.\n• IODINE [25]: IODINE also has a heavy dependency on both object- and scene-level color-related\nfactors. However, different from MONet, the ablation on object color gradient brings better\nperformance than inter-object color similarity. We speculate it is because the regularization on\nshape latent alleviates under-segmentation by biasing towards more regular shapes. In this way,\nover-segmentation is the key issue, making the object color gradient a dominant factor.\n• SlotAtt [40]: The ablations on all four factors increase the performance of SlotAtt at different levels,\namong which object- and scene-level color-related factors are more signiﬁcant. We conjecture\nthat it is because the feature embeddings used by Slot Attention module are learnt from both pixel\ncolors and coordinates, which contributes to its sensitivity to both shape and color factors.\n5\nConclusions\nWe systematically show that existing unsupervised methods are practically impossible to segment\ngeneric objects from single real-world images, and investigate the underlying factors that incur the\ncatastrophic failure. With the aid of our carefully designed four object- and scene-level complexity\nfactors, we conduct extensive experiments on multiple groups of ablated real-world objects and\nimages, and safely conclude that the distributions of both object- and scene-level biases in appearance\nand geometry of real-world datasets are particularly diverse and indiscriminative, such that current\nunsupervised models cannot segment real objects. Based on this ﬁnding, we suggest two main\ndirections for future study: 1) To exploit more discriminative objectness biases such as object motions\nwhich expressively describe the ownership of visual pixels as recently explored in [55; 12; 6] for\n2D images and in [52] for 3D point clouds. 2) To leverage pretrained features from single-object-\ndominant datasets which explicitly regard each image as an object as recently studied in [10; 30],\nalthough such settings are no longer purely unsupervised.\nBroader Impact: One limitation of this work is the lack of study about foreground-background\nsegmentation, but our work can still have a positive impact for researchers to design more practically\nuseful models. A potential negative impact could be using our ﬁndings to attack existing models.\nAcknowledgements: This work was partially supported by Shenzhen Science and Technology\nInnovation Commission (JCYJ20210324120603011).\n10\nReferences\n[1] R. Abdal, P. Zhu, N. Mitra, and P. Wonka. Labels4Free: Unsupervised Segmentation using StyleGAN.\nICCV, 2021. 4\n[2] T. Anciukevicius, C. H. Lampert, and P. Henderson. Object-Centric Image Generation with Factored\nDepths, Locations, and Appearances. arXiv:2004.00642, 2020. 5\n[3] R. Arandjelovic and A. Zisserman. Object Discovery with a Copy-Pasting GAN. arXiv:1905.11369, 2019.\n4\n[4] P. Arbeláez, M. Maire, C. Fowlkes, and J. Malik. Contour Detection and Hierarchical Image Segmentation.\nTPAMI, 33(5):898–916, 2011. 5\n[5] S. Azadi, D. Pathak, S. Ebrahimi, and T. Darrell. Compositional GAN: Learning Image-Conditional Binary\nComposition. IJCV, 2020. 4\n[6] D. M. Bear, C. Fan, D. Mrowca, Y. Li, S. Alter, A. Nayebi, J. Schwartz, L. Fei-Fei, J. Wu, J. B. Tenenbaum,\nand D. L. Yamins. Learning Physical Graph Representations from Visual Scenes. NeurIPS, 2020. 10\n[7] A. Bielski and P. Favaro. Emergence of object segmentation in perturbed generative models. NeurIPS,\n2019. 4\n[8] C. P. Burgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. Botvinick, and A. Lerchner. MONet:\nUnsupervised Scene Decomposition and Representation. arXiv:1901.11390, 2019. 1, 4, 5, 10, 18, 20, 21,\n22, 23, 24, 26, 27\n[9] B. Calli, A. Singh, J. Bruce, A. Walsman, K. Konolige, S. Srinivasa, P. Abbeel, and A. M. Dollar.\nYale-CMU-Berkeley dataset for robotic manipulation research. IJRR, 36(3):261 – 268, 2017. 4, 5, 19\n[10] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging Properties in\nSelf-Supervised Vision Transformers. ICCV, 2021. 2, 10\n[11] R. Charakorn, Y. Thawornwattana, S. Itthipuripat, N. Pawlowski, and N. Dilokthanakul. An Explicit\nLocal and Global Representation Disentanglement Framework with Applications in Deep Clustering and\nUnsupervised Object Detection. arXiv:2001.08957, 2020. 5\n[12] H. Chen, R. Venkatesh, Y. Friedman, J. Wu, J. B. Tenenbaum, D. L. K. Yamins, and D. M. Bear. Un-\nsupervised Segmentation in Real-World Images via Spelke Object Inference. arXiv:2205.08515, 2022.\n10\n[13] M. Chen, T. Artières, and L. Denoyer. Unsupervised object segmentation by redrawing. NeurIPS, 2019. 4\n[14] B. Cheng, R. Girshick, P. Dollár, A. C. Berg, and A. Kirillov. Boundary IoU: Improving Object-Centric\nImage Segmentation Evaluation. CVPR, 2021. 15, 17\n[15] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing Textures in the Wild. CVPR,\n2014. 8, 23\n[16] E. Crawford and J. Pineau. Spatially Invariant Unsupervised Object Detection with Convolutional Neural\nNetworks. AAAI, 2019. 5\n[17] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner. ScanNet: Richly-annotated\n3D Reconstructions of Indoor Scenes. CVPR, 2017. 4, 5, 19\n[18] F. Deng, Z. Zhi, D. Lee, and S. Ahn. Generative Scene Graph Networks. ICLR, 2021. 5\n[19] S. Eddins. Binary image convex hull. https://blogs.mathworks.com/steve/2011/10/04/binary-image-convex-\nhull-algorithm-notes/, 2011. 3, 6, 15, 24\n[20] M. Engelcke, O. P. Jones, and I. Posner. GENESIS-V2: Inferring Unordered Object Representations\nwithout Iterative Reﬁnement. NeurIPS, 2021. 1, 5\n[21] M. Engelcke, A. R. Kosiorek, O. P. Jones, and I. Posner. GENESIS: Generative Scene Inference and\nSampling with Object-Centric Latent Representations. ICLR, 2020. 5, 18\n[22] S. M. A. Eslami, N. Heess, T. Weber, Y. Tassa, K. Kavukcuoglu, and G. E. Hinton. Attend, Infer, Repeat:\nFast Scene Understanding with Generative Models. NIPS, 2016. 1, 4, 5, 9, 18, 20, 22, 23, 24, 26, 27\n[23] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL\nVisual Object Classes Challenge: A Retrospective. IJCV, 111:98–136, 2015. 5\n11\n[24] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and\nY. Bengio. Generative Adversarial Nets. NIPS, 2014. 4\n[25] K. Greff, R. L. Kaufman, R. Kabra, N. Watters, C. Burgess, D. Zoran, L. Matthey, M. Botvinick, and\nA. Lerchner. Multi-object representation learning with iterative variational inference. ICML, 2019. 4, 5,\n10, 18, 20, 21, 22, 23, 24, 26, 27\n[26] K. Greff, A. Rasmus, M. Berglund, T. H. Hao, J. Schmidhuber, and H. Valpola. Tagger: Deep unsupervised\nperceptual grouping. NIPS, 2016. 5\n[27] K. Greff, S. Van Steenkiste, and J. Schmidhuber. Neural Expectation Maximization. NIPS, 2017. 5\n[28] K. Greff, S. van Steenkiste, and J. Schmidhuber. On the binding problem in artiﬁcial neural networks.\narXiv:2012.05208, 2020. 1\n[29] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask R-CNN. ICCV, 2017. 4, 18, 21\n[30] O. J. Hénaff, S. Koppula, E. Shelhamer, D. Zoran, A. Jaegle, A. Zisserman, J. Carreira, and R. Arandjelovi´c.\nObject discovery and representation networks. ECCV, 2022. 2, 10\n[31] J. Huang and K. Murphy. Efﬁcient inference in occlusion-aware generative models of images. ICLR\nWorkshops, 2016. 5\n[32] J. Jiang and S. Ahn. Generative neurosymbolic machines. NeurIPS, 2020. 5\n[33] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and R. Girshick. CLEVR: A\nDiagnostic Dataset for Compositional Language and Elementary Visual Reasoning. CVPR, 2017. 1, 4, 5,\n18, 19\n[34] R. Kabra, C. Burgess, L. Matthey, R. L. Kaufman, K. Greff, M. Reynolds, and A. Lerchner. Multi-Object\nDatasets. https://github.com/deepmind/multi-object-datasets/, 2019. 4, 5, 19\n[35] L. Karazija, I. Laina, and C. Rupprecht. ClevrTex: A Texture-Rich Benchmark for Unsupervised Multi-\nObject Segmentation. NeurIPS, 2021. 2\n[36] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. ICLR, 2014. 4\n[37] A. Kirillov, K. He, R. Girshick, C. Rother, and P. Dollár. Panoptic Segmentation. CVPR, 2019. 5\n[38] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft\nCOCO: Common Objects in Context. ECCV, 2014. 1, 4, 5, 19\n[39] Z. Lin, Y.-F. Wu, S. V. Peri, W. Sun, G. Singh, F. Deng, J. Jiang, and S. Ahn. SPACE: Unsupervised\nObject-Oriented Scene Representation via Spatial Attention and Decomposition. ICLR, 2020. 1, 5\n[40] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Dosovitskiy, and\nT. Kipf. Object-Centric Learning with Slot Attention. NeurIPS, 2020. 1, 2, 4, 5, 10, 18, 20, 22, 23, 24, 26,\n27\n[41] S. Löwe, P. Lippe, M. Rudolph, and M. Welling. Complex-Valued Autoencoders for Object Discovery.\narXiv:2204.02075, 2022. 5\n[42] L. Matthey, I. Higgins, D. Hassabis, and A. Lerchner. dSprites - Disentanglement testing Sprites dataset.\nhttps://github.com/deepmind/dsprites-dataset/, 2017. 1, 4, 5, 19\n[43] S. Papa, O. Winther, and A. Dittadi. Inductive Biases for Object-Centric Representations in the Presence\nof Complex Textures. arXiv:2204.08479, 2022. 2\n[44] D. D. Polsby and R. D. Popper. The third criterion: Compactness as a procedural safeguard against partisan\ngerrymandering. Yale L. & Pol’y Rev., 9, 1991. 16\n[45] W. M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical\nAssociation, 66(336):846–850, 1971. 5\n[46] D. J. Rezende and F. Viola. Taming vaes. arXiv:1810.00597, 2018. 18\n[47] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-\nstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International\nJournal of Computer Vision, 2015. 2\n12\n[48] J. E. Schwartzberg. Reapportionment, gerrymanders, and the notion of compactness. Minn. L. Rev., 50:443,\n1965. 16\n[49] C. E. Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379–\n423, 1948. 16\n[50] Shengxin Qian. R-MONet: Region-Based Unsupervised Scene Decomposition and Representation via\nConsistency of Object Representations. OpenReview.Net, 2021. 5\n[51] I. Sobel and G. Feldman. A 3x3 Isotropic Gradient Operator for Image Processing. Pattern Classiﬁcation\nand Scene Analysis, pages 271–272, 1973. 3, 15\n[52] Z. Song and B. Yang. OGC: Unsupervised 3D Object Segmentation from Rigid Dynamics of Point Clouds.\nNeurIPS, 2022. 10\n[53] E. S. Spelke, K. Breinlinger, J. Macomber, and K. Jacobson. Origins of knowledge. Psychological Review,\n99(4):605–632, 1992. 1\n[54] K. Stelzner, R. Peharz, and K. Kersting. Faster attend-infer-repeat with tractable probabilistic models.\nICML, 2019. 5\n[55] M. Tangemann, S. Schneider, J. von Kügelgen, F. Locatello, P. Gehler, T. Brox, M. Kümmerer, M. Bethge,\nand B. Schölkopf. Unsupervised Object Learning via Common Fate. arXiv:2110.06562, 2021. 10\n[56] S. van Steenkiste, K. Kurach, J. Schmidhuber, and S. Gelly. Investigating object compositionality in\nGenerative Adversarial Networks. Neural Networks, 2020. 4\n[57] J. von Kügelgen, I. Ustyuzhaninov, P. Gehler, M. Bethge, and B. Schölkopf. Towards causal generative\nscene models via competition of experts. ICLR Workshops, 2020. 5\n[58] A. Voynov, S. Morozov, and A. Babenko. Object Segmentation Without Labels with Large-Scale Generative\nModels. ICML, 2021. 4\n[59] W. Wang, Q. Lai, H. Fu, J. Shen, H. Ling, and R. Yang. Salient Object Detection in the Deep Learning Era:\nAn In-Depth Survey. TPAMI, 2021. 2\n[60] M. A. Weis, K. Chitta, Y. Sharma, W. Brendel, M. Bethge, A. Geiger, and A. S. Ecker. Benchmarking\nUnsupervised Object Representations for Video Sequences. JMLR, 2021. 2\n[61] M. Wertheimer. Untersuchungen zur Lehre yon der Gestalt. Psychologische Forschung, 1923. 1\n[62] K. Xu, C. Li, J. Zhu, and B. Zhang. Multi-object generation with amortized structural regularization.\nNeurIPS, 2019. 5\n[63] J. Yuan, T. Chen, B. Li, and X. Xue. Compositional Scene Representation Learning via Reconstruction: A\nSurvey. arXiv:2202.07135, 2022. 2\n[64] J. Yuan, B. Li, and X. Xue. Generative modeling of inﬁnite occluded objects for compositional scene\nrepresentation. ICML, 2019. 5\n13\n1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes] See Section 5.\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] See\nSection 5.\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes]\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\n(b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments...\n(a) Did you include the code, data, and instructions needed to reproduce the main experi-\nmental results (either in the supplemental material or as a URL)? [Yes] See Contribu-\ntions and ﬁnding in Section 1.\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [Yes] See Implementation Details in Appendix.\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-\nments multiple times)? [Yes] In the tables for qunatitative results, we report standard\ndeviation of performances over three runs.\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\nof GPUs, internal cluster, or cloud provider)? [Yes] See Implementation Details in\nAppendix.\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes]\n(b) Did you mention the license of the assets? [N/A]\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\nSee Contributions and ﬁnding in Section 1.\n(d) Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? [N/A]\n(e) Did you discuss whether the data you are using/curating contains personally identiﬁable\ninformation or offensive content? [N/A]\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if\napplicable? [N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [N/A]\n14\nA\nAppendix\nA.1\nDetails of the Four Complexity Factors\nWe have introduced four complexity factors in both object- and scene-level appearance and geometry.\nDetails of these factors are as follows.\n• Object Color Gradient: As shown in Figure 11, given an RGB image of an object, we ﬁrst convert\nit to grayscale by applying Y = 0.299R + 0.587G + 0.114B. Then Sobel ﬁlter [51] with kernel\nsize 3 × 3 is applied horizontally and vertically to compute the image gradient for each pixel. Since\nthis factor should only be related to the appearance inside an object regardless of the background,\nwe remove the gradients at the object boundary. The object boundary is computed from its mask,\nfollowing the practice of [14]. The factor score is calculated as the average gradient of all object\npixels that are not on the boundary. This value ranges between 0 and 255, which is then divided by\n255 so as to be normalized to the range of [0, 1].\nObject Image\nObject Boundary\nGradient\nInner Gradient\nGrayscale Image\nFigure 11: The calculation of Object Color Gradient.\n• Object Shape Concavity: As shown in Figure 12, given the binary mask of an object (M obj ∈\nRH×W ), we ﬁrst calculate its smallest surrounding convex polygon mask (M cvx ∈RH×W ) using\nthe existing algorithm [19]. Basically, this polygon mask is the smallest region that can cover\nany lines between two points on the original object mask. The object shape concavity factor is\ncalculated as 1 −P M obj/ P M cvx. This factor naturally takes a value between 0 and 1. No\nfurther normalization is needed.\n(a) Object Mask\n(b) Smallest Convex Polygon Mask\nFigure 12: An illustration of the Object Shape Concavity.\n• Inter-object Color Similarity: As shown in Figure 13, we ﬁrst calculate the average RGB color of\neach object. Each color corresponds to a point in the RGB space. We then calculate the Euclidean\ndistance between each pair of object colors. The average value of all pairwise distances is divided\nby 255 ×\n√\n3, which is the largest distance between two colors in the RGB space, so as to be\nnormalized to the range of [0, 1]. The ﬁnal score for inter-object color similarity is calculated as\n1 −normalized RGB distance.\nFigure 13: An illustration of the Inter-object Color Similarity.\n• Inter-object Shape Variation: We ﬁrst compute an axis-aligned bounding box for each object,\nand calculate its diagonal length. Then the differences of diagonal lengths between each pair\nof objects are calculated. The average value of such pairwise differences is the raw value of\ninter-object shape variation. This raw value is divided by 255 ×\n√\n2, which is the largest possible\ndifference of diagonal length, so as to be normalized to the range of [0, 1].\n15\nA.2\nOther Candidates of Complexity Factors\nIn addition to the primary four complexity factors, we also explore other potential complexity\nfactors to quantitatively measure the distributions of object- and scene-level biases in appearance\nand geometry. Basically, we aim to consider as many aspects as possible to investigate key factors\nunderlying the distribution gaps between synthetic and real-world datasets. However, we empirically\nﬁnd that these candidate factors do not show signiﬁcant discrepancy between synthetic and real-world\ndatasets. Details are shown below.\nA.2.1\nCandidates of Object-level Complexity Factors\n• Object Color Count: This factor is deﬁned as the total number of unique colors within an object\nmask. Basically, this is to simply measure the diversity of object color.\n• Object Color Entropy: Inspired by Shannon entropy [49], we calculate the entropy value at each\npixel by applying a 3 × 3 ﬁlter on the grayscale image concerted from RGB. In particular, for\neach pixel, its color value becomes a discrete value in [0, 255]. We compute its entropy score:\nH(x) = −Pn\ni=1 p(xi) log2 p(xi), where p(xi) denotes the probability of a speciﬁc color value xi\nwithin the 3 × 3 neighbourhood. Basically, this factor aims to measure the color diversity within\n3 × 3 image patches. The higher this factor, the more frequently the object color changes in small\nlocal areas.\n• Object Shape Non-rectangularity: Given the binary mask of an object (M obj ∈RH×W ), we\nﬁrst calculate its axis-aligned bounding box (M bbox ∈RH×W ). Object shape non-rectangularity\nis calculated as 1 −P M obj/ P M bbox. Similar to object shape concavity, this factor is also\ndesigned to measure the complexity of object shapes. However, this factor is more likely to be\naffected by the object orientation since it takes axis-aligned bounding boxes as reference.\n• Object Shape Incompactness: There are two similar methods to quantify the compactness of\nobject shapes. The ﬁrst one is Polsby–Popper test [44]: PP(M obj) = 4πA(M obj)/P(M obj)2.\nThe other is Schwartzberg [48] compactness score: S(M obj) = (2π\np\nA(M obj)/π)/P(M obj).\nIn both formula, P(M obj) is the object perimeter and A(M obj) is the object area. For simplicity,\nwe choose PP(M obj) to calculate the object shape incompactness score: 1 −PP(M obj).\n• Object Shape Discontinuity: Given an object mask (M obj ∈RH×W ), we ﬁrst ﬁnd the largest\nconnected component (M lcc ∈RH×W ) in its binary mask. The discontinuity of shape is calculated\nas: 1 −P M lcc/ P M obj. This factor is to evaluate how continuous an object shape is.\n• Object Shape Decentralization: Given an object mask, we ﬁrst calculate its centroid (¯x, ¯y) by\naveraging all pixel coordinates in the object. Then, the second moment of this object is calculated\nas: P\nx\nP\ny(x −¯x)2(y −¯y)2, where (x, y) is the coordinates of pixels within the object. The\nhigher this factor, the object shape is less likely to be centralized.\nAs shown in Figure 14, we compare the distributions of the object-level factor candidates on both\nsynthetic and real-world datasets. It can be seen that the majority of these factors do not show\nsigniﬁcant gaps between the simple synthetic and the challenging real-world datasets. Therefore, we\ndo not conduct relevant ablation experiments.\nFigure 14: Distributions of Object-level Complexity Factors candidates.\n16\nA.2.2\nCandidates of Scene-level Complexity Factors\n• Inter-object Color Similarity with Chamfer Distance: In the calculation of this factor, we ﬁrst\nconvert each pixel into a point in RGB space. In this way, each object can be represented by a\npoint set in the RGB space. This factor is calculated between each pair of objects by measuring the\nChamfer distance of two point sets in the RGB space. Since Chamfer distance is an asymmetric\nmeasurement, we calculate and average out the bidirectional Chamfer distances. Compared with\nEuclidean distance, this measurement favors the most similar colors between two objects.\n• Inter-object Color Similarity with Hausdorff Distance: This factor is similar to the previous\none. The only difference is that we replace Chamfer distance with Hausdorff distance. Hausdorff\ndistance is also a directed and asymmetric measurement, so the ﬁnal score is the average of distance\nvalues in both direction.\n• Inter-object Shape Similarity over Boundaries: For each object mask, we ﬁrst ﬁnd its boundary\nusing the method in [14], and then crop it with its axis-aligned bounding box. Each bounding box\nis scaled and ﬁt into a unit box with its original aspect ratio. Lastly, we calculate the IoU between\nthe boundaries of two objects to measure their shape similarity.\n• Inter-object Shape Entropy between Boundaries: We ﬁrst combine all object masks into a\nsingle image by assigning different indices to different objects. Then we compute the entropy of\neach pixel with a 3 × 3 ﬁlter. The ﬁnal factor score is calculated by averaging all non-zero entropy\nvalues. Note that, the interior part of objects and background will not be considered because their\nentropy values will always be zeros. Basically, this factor is designed to evaluate how crowded an\nimage is. The higher this factor, the more objects are spatially adjacent.\n• Inter-object Proximity between Centroids: We ﬁrst calculate the centroid (¯x, ¯y) of each object\nby averaging all pixel coordinates in the object mask. Euclidean distances between object centroids\nare then computed pair-wisely before they are averaged to be the ﬁnal factor score. This factor is\ndesigned to measure the spatial proximity of multiple objects in a single image.\n• Inter-object Proximity with Chamfer Distance: In order to measure the spatial proximity be-\ntween objects, we also calculate the spatial Chamfer distance between objects. Speciﬁcally, each\nobject is represented by a set of x −y coordinates, and then the average of pair-wise bidirectional\nChamfer distances is calculated as the proximity score for each image.\nAs shown in Figure 15, we compare the distributions of the scene-level factor candidates on both\nsynthetic and real-world datasets. We can see that both the inter-object color similarity with Chamfer\nand Hausdorff distances share similar distributions gaps with our primary inter-object color similarity\nfactor deﬁned in Section 2. The remaining four candidate factors relating to inter-object shape\ncomplexity do not show signiﬁcant distribution gaps between the synthetic and real-world datasets.\nIn this regard, we choose not to conduct ablation experiments on these six candidate factors.\nFigure 15: Distributions of Scene-level Complexity Factors candidates.\n17\nA.3\nImplementation Details\nIn this section, we present the implementation details of four representative models.\nAIR [22]\n• Source Code: We refer to https://pyro.ai/examples/air.html and https://github.\ncom/addtt/attend-infer-repeat-pytorch for the implementation.\n• Important Adaptations: We use an additional parameter to weight the KL divergence loss and\nreconstruction loss. For each experiment, we choose the weight for KL divergence from 1, 10, 25\nand 50. The highest AP score is kept.\n• Training Details: All experiments of AIR [22] are conducted with a batch size of 64. The learning\nrate is set to 1e−4 for training inference networks and decoders, 1e−3 for baselines which is the\nsame as the original paper. Since the number of objects in our datasets ranges between 2 and 6, we\nset the maximum number of steps at inference to be 6 for all experiments. All models are trained\non a single GPU for 1000 epochs. We perform evaluation every 50 epochs and select the one with\nthe highest AP score.\nMONet [8]\n• Source\nCode:\nWe\nrefer\nto\n[21]’s\nre-implementation\nat:\nhttps://github.com/\napplied-ai-lab/genesis.\n• Important Adaptations: We train MONet [8] with the GECO objective [46] following the protocol\nmentioned in [21].\n• Training Details: All experiments on MONet [8] are conducted with a batch size of 32 and\nlearning rate of 1e−4. Since the maximum number of components is 7, including 1 background\nand 6 objects, we set the number of steps to be 7 for all experiments. All models are trained on\na single GPU for 200 epochs with the training loss converged. We perform evaluation every 10\nepochs and select the one with the highest AP score.\nIODINE [25]\n• Source Code: We use the ofﬁcial implementation at:\nhttps://github.com/deepmind/\ndeepmind-research/tree/master/iodine .\n• Important Adaptations: The architecture is set the same as what is used for CLEVR dataset [33]\nin the original paper [25].\n• Training Details: Since we use a single GPU for the training of all models, the batch size is\nadjusted to be 4 and learning rate 0.0001 ×\np\n1/8. The number of slots K is set as 7 and the\ninference iteration T as 5. We train each model for 500K iterations until the loss is fully converged.\nSlotAtt [40]\n• Source\nCode:\nWe\nuse\nthe\nofﬁcial\nimplementation\nat:\nhttps://github.com/\ngoogle-research/google-research/tree/master/slot_attention.\n• Important Adaptations: The architecture is set the same as what is used for CLEVR dataset [33]\nin the original paper [40].\n• Training Details: All experiments of SlotAtt [40] are conducted with a batch size of 32 and\nlearning rate selected from [4e−4, 4e−5]. The number of slots K is set as 7 and the number of\niterations T is set as 3. All models are trained on a single GPU for 500K iterations until the loss is\nfully converged.\nMask R-CNN [29]\n• Source Code: We use the implementation at: https://github.com/matterport/Mask_RCNN.\n• Important Adaptations: We use the same settings as training for COCO in above repository.\n• Training Details:\nTraining for all datasets starts from the pre-trained COCO weights\n(mask_rcnn_coco.h5) from https://github.com/matterport/Mask_RCNN/releases. All\nmodels are trained on a single GPU for 30 epochs until the loss is fully converged.\n18\nA.4\nDetails of six Benchmark Datasets\nIn this section, we present the details of three synthetic and three real-world datasets.\ndSprites [42] To generate a speciﬁc image for this dataset, we ﬁrst sample a random integer K from\na uniform distribution with interval [2, 6] as the number of objects in that image. Then, K object\nshapes are selected from the binary dsprites dataset [42] also in a uniformly random manner. Each\nobject is assigned with a random RGB color by sampling three random integers from a uniform\ndistribution with interval [0, 255]. In total, we generate 10000 images for training, 2000 for testing.\nTetris [34] For each image in this dataset, we ﬁrst sample a random integer K from a uniform\ndistribution with interval [2, 6] as the number of objects in this image. To render one tetris-like object\nonto the canvas, we randomly pick up a tetris object from a randomly selected image from [34].\nEach object is resized to be 88 × 88 and then placed onto the canvas. The position of each object is\nalso sampled from a uniform distribution with 2 criteria: 1) all objects shall be on the canvas with\ncomplete shapes; 2) all objects shall not overlap with each other.\nCLEVR\n[33]\nWe\nﬁrst\ngenerate\nCLEVR\nimages\nfollowing\nhttps://github.com/\nfacebookresearch/clevr-dataset-gen, where the number of objects per image is re-\nstricted between 3 to 6.\nGiven generated images with a resolution 640 × 480, we perform\ncenter-cropping and then resize them to be 128 × 128. Then, we remove tiny objects which have less\nthan 35 pixels from each image. Subsequently, the images with less than 2 objects are removed.\nBeing consistent with previous 2 synthetic datasets, all images have a black background.\nYCB [9] We sample single frames from the YCB video dataset [9] every 20 images. Given the\nsampled frames with a resolution of 640 × 480, we ﬁrst center crop and then resize them to be\n128 × 128. Then, the images consisting of less than 2 or more than 6 objects are removed. Similarly,\nall background pixels are replaced by a black color.\nScanNet [17] We sample single frames from the ScanNet dataset [17] every 20 images. Given the\nselected frames with a resolution of 1296 × 968, we ﬁrst center crop the images with a size of\n800 × 800 and then resize them to be 128 × 128. For each resized image, we remove objects that\ncontain more than 128 × 128 × 0.2 pixels or less than 128 × 128 × 0.007 pixels. The images with\nless than 2 or more than 6 are also dropped. All background pixels are replaced by the black color.\nCOCO [38] Given images in COCO-2017 [38] with various resolutions, we ﬁrst center crop and then\nresize images to be 128 × 128. For each resized image, we use the same criteria applied to ScanNet\nto remove too large and too small objects. The images with 2 ∼6 objects are kept. All background\npixels are replaced by the black color. Since the number of images that could meet all requirements is\nless than 2,000 (only 1,597) in the ofﬁcial validation split, we additionally select 403 different images\nfrom its ofﬁcial training split for testing. There is no overlap between our training and testing splits.\nFigure 16 shows three example images for each dataset. The distribution of images with different\nnumber of objects is also presented.\n(a) dSprites\n(b) Tetris\n(c) CLEVR\n(d) YCB\n(e) ScanNet\n(f) COCO\n0\n1000\n2000\n3000\n4000\n5000\n2\n3\n4\n5\n6\nnumber of images\nnumber of objects\n0\n1000\n2000\n3000\n4000\n5000\n2\n3\n4\n5\n6\nnumber of images\nnumber of objects\n0\n1000\n2000\n3000\n4000\n5000\n2\n3\n4\n5\n6\nnumber of images\nnumber of objects\n0\n1000\n2000\n3000\n4000\n5000\n2\n3\n4\n5\n6\nnumber of images\nnumber of objects\n0\n1000\n2000\n3000\n4000\n5000\n2\n3\n4\n5\n6\nnumber of images\nnumber of objects\n0\n1000\n2000\n3000\n4000\n5000\n2\n3\n4\n5\n6\nnumber of images\nnumber of objects\nFigure 16: Example images and the statistics of the six benchmark datasets.\n19\nA.5\nDetails and Results of Main Experiments\nIn this section, we present the result of four unsupervised methods and one supervised method on six\ndatasets. Both quantitative evaluation and qualitative results are provided.\nA.5.1\nFour Unsupervised Methods\nFigure 17 shows the qualitative results of 4 representative methods on 6 datasets. The results of\nAIR [22] are presented with predicted bounding boxes while others are represented with predicted\nsegmentation masks. Different colors in a segmentation mask indicate different objects. For the same\nobject, the assigned color may not be the same. We can see that all methods give reasonable results\non the three synthetic datasets in spite of some performance gaps between them due to the different\ncapability of these methods. However, they all fail on the three real-world datasets. Speciﬁcally, AIR\n[22] cannot reconstruct reasonable images. MONet [8] always performs segmentation based on color.\nIODINE [25] cannot recover meaningful object shapes and locations. SlotAtt [40] can roughly locate\nobjects but cannot identify the real object shapes.\nInput\nImages\nAIR\nMONet\nIODINE\nGT \nMasks\ndSprites\nTetris\nCLEVR\nYCB\nScanNet\nCOCO\nSlotAtt\nFigure 17: Qualitative results of object segmentation from the four methods on six datasets.\nTable 3: Quantitative results of object segmentation from the four methods on six datasets. Standard\ndeviations of performance are calculated over 3 runs (marked with blue).\ndSprites\nTetris\nCLEVR\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n45.4 1.8 / 38.2 3.0 / 57.6 7.4 / 58.1 7.5\n25.2 13.9 / 23.4 12.4 / 36.8 20.9 / 39.9 12.9\n46.4 14.0 / 44.3 12.4 / 67.4 9.9 / 52.5 15.9\nMONet [8]\n69.7 4.1 / 61.6 6.0 / 70.4 8.1 / 73.9 1.9\n85.9 13.0 / 75.8 13.6 / 85.1 16.4/ 89.7 8.2\n39.0 8.5 / 37.3 6.3 / 65.6 11.8 / 42.8 10.8\nIODINE [25]\n92.9 4.3/ 71.3 6.1/ 82.6 2.3/ 96.0 5.2\n52.2 2.3 / 37.9 4.6 / 48.0 2.3 / 61.8 1.7\n82.8 2.8 / 73.0 5.7 / 77.5 3.1 / 87.4 2.0\nSlotAtt [40]\n92.8 1.4 / 82.8 1.6 / 88.8 3.4 / 92.9 1.6\n94.3 1.2 / 79.9 6.4 / 90.5 3.3 / 94.4 1.3\n91.7 6.4 / 82.9 10.9 / 90.8 9.7 / 92.7 5.3\nYCB\nScanNet\nCOCO\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n0.0 0.1 / 0.6 0.3 / 1.1 0.4 / 0.8 0.2\n2.7 1.4 / 6.3 1.7 / 15.6 2.8 / 7.3 1.6\n2.7 0.1 / 6.7 0.5 / 14.3 2.6 / 8.6 0.8\nMONet [8]\n3.1 1.6 / 7.0 2.6 / 9.8 3.6 / 1.2 0.8\n24.8 1.6 / 24.6 1.6 / 31.0 1.6 / 40.7 1.8\n11.8 2.0 / 12.5 1.1/ 16.1 0.9/ 21.9 1.7\nIODINE [25]\n1.8 0.2 / 3.9 1.3 / 6.2 2.0 / 7.3 1.9\n10.1 2.9 / 13.7 2.7 / 18.6 4.2 / 24.4 3.8\n4.0 1.2 / 6.3 1.2 / 9.9 1.8 / 10.8 2.0\nSlotAtt [40]\n9.2 0.4 / 13.5 0.9 / 20.0 1.3 / 26.2 6.8\n5.7 0.3 / 9.0 1.5 / 12.4 2.5 / 18.3 2.7\n0.8 0.3 / 3.5 1.2 / 5.3 1.7 / 7.3 2.2\n20\nA.5.2\nOne Supervised Method\nApart from above four unsupervised approaches, we also include Mask R-CNN [29] as a supervised\nobject segmentation baseline. As shown in Figure 18, Mask R-CNN exhibits quite successful\nsegmentation on three synthetic datasets. Real-world datasets are more challenging for Mask R-CNN,\nbut the results are much better than unsupervised baselines.\ndSprites\nScanNet\nTetris\nCLEVR\nYCB\nCOCO\nInput \nimage\nGT \nmask\nPredicted \nmask\nMaskRCNN\nAP / PQ / Pre / Rec\ndSprites 98.4 / 90.2 / 99.6 / 98.4\nTetris 99.8 / 90.3 / 99.8 / 99.8\nCLEVR 98.2 / 90.0 / 97.8 / 99.5\nYCB 62.9 / 58.4 / 83.3 / 66.9\nScanNet 41.4 / 43.3 / 65.2 / 50.5\nCOCO 46.0 / 47.9 / 71.7 / 53.2\nFigure 18: Qualitative and quantitative results of Mask-RCNN on six datasets.\nA.6\nDetails and Results of Ablation Experiments\nIn this section, we shows details of ablated datasets including example images. Both qualitative and\nquantitative evaluation are also presented. Table 4 is a look-up table for individual ablated factor.\nTable 4: A look-up table for ablations.\nWhich level\nWhich aspect\nTarget factor\nAblation\nObject\nScene\nColor\nShape\nObject\nColor\nGradient\nObject\nShape\nConcavity\nInter-\nobject\nColor\nSimilarity\nInter-\nobject\nShape\nVariation\nC\n✓\n✓\n✓\nS\n✓\n✓\n✓\nT\n✓\n✓\n✓\nU\n✓\n✓\n✓\nA.6.1\nAblations on Object-level Factors\nAblated Datasets\n• Ablation of Object Color Gradient: In the three ablated datasets: YCB-C / ScanNet-C / COCO-C,\neach object is represented by a single average color.\n• Ablation of Object Shape Concavity: In the three ablated datasets: YCB-S / ScanNet-S / COCO-S,\neach object is represented by a convex shape.\n• Ablation of both Object Color Gradient and Shape Concavity: In the three ablated datasets: YCB-\nC+S / ScanNet-C+S / COCO-C+S, each object is represented by a convex shape with a single\naverage color.\nFigure 19 shows example images from the three real-world datasets with different types of object-level\nfactor ablations.\nQualitative and Quantitative Results\nAs shown in Figure 20 and Table 5, all four methods have\na signiﬁcant improvement in segmentation performance on the ablated datasets with object color\ngradients being removed. By comparison, the ablation of object shape concavity is less effective.\nThis shows that both object-level factors are relevant to the success of object segmentation, although\nobject color gradient is more important for the four methods. Speciﬁcally, MONet [8] and IODINE\n[25] are more sensitive to color gradient compared with the other two methods.\n21\nYCB\nYCB-C\nYCB-S\nYCB-C+S\nScanNet\nScanNet-C\nScanNet-S\nScanNet-C+S\nCOCO\nCOCO-C\nCOCO-S\nCOCO-C+S\nFigure 19: Example images of real-world datasets ablated with object-level factors.\nMONet\nAIR\nGT \nMasks\nOriginal\nImages\nAblated\nImages\nIODINE\nSlotAtt\nObject Color Gradient Ablation\nObject Shape Concavity Ablation\nBoth Ablation\nYCB-C\nScanNet-C COCO-C\nYCB-S\nScanNet-S COCO-S\nYCB-S+C ScanNet-S+C COCO-S+C\nFigure 20: Qualitative results on the datasets ablated with object-level factors.\nTable 5: Quantitative results on the datasets ablated with object-level factors. Standard deviations of\nperformance are calculated over 3 runs (marked with blue).\nYCB-C\nScanNet-C\nCOCO-C\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n4.4 0.2 / 1.0 7.3 / 20.7 7.1 / 12.5 1.2\n2.7 1.0 / 7.6 2.1 / 29.1 20.2 / 7.4 1.5\n2.9 1.8 / 7.9 2.5 / 32.0 29.3 / 7.7 4.5\nMONet [8]\n55.1 6.0 / 49.5 5.2 / 64.8 1.7 / 58.8 7.8\n31.1 15.9 / 33.1 9.6 / 41.6 10.3 / 45.6 11.1\n34.5 7.2 / 34.6 3.3 / 45.8 2.0 / 42.6 11.0\nIODINE [25]\n76.8 0.3 / 64.4 0.1 / 76.5 0.1 / 79.5 0.0\n64.4 6.4 / 54.3 5.9 / 66.3 4.6 / 71.0 5.9\n42.8 14.7 / 34.0 11.8 / 42.5 14.4 / 55.8 11.9\nSlotAtt [40]\n39.2 1.9 / 30.2 0.0 / 40.2 0.8 / 50.4 0.7\n5.7 8.2 / 9.0 6.1 / 12.4 7.6 / 18.3 8.9\n7.2 1.9 / 9.8 1.7/ 13.9 2.2 / 18.7 3.4\nYCB-S\nScanNet-S\nCOCO-S\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n3.6 3.0 / 6.8 3.2 / 12.5 6.7 / 9.7 2.7\n3.3 2.5 / 8.5 1.1 / 36.6 23.0 / 8.1 8.5\n3.3 0.0 / 7.1 1.1 / 16.8 5.6 / 8.3 1.0\nMONet [8]\n2.5 1.1 / 6.4 1.8 / 8.9 2.6 / 11.3 3.2\n18.3 10.4 / 22.2 3.9 / 28.4 4.4 / 37.5 5.3\n8.7 3.2 / 11.2 1.7 / 14.4 3.0 / 21.0 0.7\nIODINE [25]\n2.5 2.5 / 5.0 5.0 / 7.9 7.8 / 1.1 1.0\n8.3 1.1 / 13.7 0.4 / 18.4 0.4 / 24.8 0.7\n4.2 0.4 / 8.3 0.4 / 12.1 0.3 / 14.8 0.6\nSlotAtt [40]\n19.1 0.7 / 21.0 1.7 / 30.4 2.8 / 37.7 0.8\n1.9 8.5 / 6.6 7.5 / 8.7 9.8 / 13.7 11.9\n2.6 1.0 / 5.9 0.9 / 8.3 1.3 / 11.9 10.9\nYCB-C+S\nScanNet-C+S\nCOCO-C+S\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n2.0 0.3 / 6.4 0.2 / 15.1 5.2 / 7.3 4.1\n3.5 1.0 / 9.1 0.5 / 38.3 25.8 / 8.5 5.9\n4.8 0.1 / 10.8 0.1 / 41.5 10.2 / 10.1 1.7\nMONet [8]\n9.1 13.6 / 12.7 10.5 / 34.1 4.6 / 12.6 24.0\n48.1 1.0 / 47.9 6.8 / 70.4 23.3 / 51.5 6.5\n10.5 24.3 / 16.5 18.1 / 56.5 6.9 / 14.9 28.2\nIODINE [25]\n69.9 30.9 / 55.7 29.6 / 60.3 28.3 / 71.5 26.4\n73.8 11.4 / 63.2 10.1 / 73.5 6.1 / 76.8 10.8\n73.5 2.5 / 61.2 2.4 / 70.0 2.9 / 77.6 1.2\nSlotAtt [40]\n39.1 22.3 / 30.2 13.3 / 42.9 12.2 / 54.5 12.7\n22.5 7.6 / 21.5 6.2 / 28.8 6.0 / 39.6 5.4\n20.3 3.0 / 19.9 0.4 / 26.1 0.4 / 34.8 0.7\n22\nA.6.2\nAblations on Scene-level Factors\nAblated Datasets\n• Ablation of Inter-object Color Similarity: In the three ablated datasets: YCB-T / ScanNet-T /\nCOCO-T, each object’s original texture is replaced by one of the selected 6 distinctive textures\nfrom the DTD database [15], as shown in Figure 21. In particular, the 6 textures are chosen from\nthe ‘blotchy’ category, and we deliberately select those with distinctive colors. The texture images\nare center-cropped and resized to a resolution of 128 × 128. Each object appearance is replaced by\none of the 6 texture images, and each object in a single image is assigned with a different texture.\n• Ablation of Inter-object Shape Variation: In the three ablated datasets: YCB-U / ScanNet-U /\nCOCO-U, the size of each object is uniformly scaled. In particular, before generating the ablated\ndatasets, we ﬁrst calculate the average scale of objects in each dataset by calculating the mean of\nobject bounding box diagonals (YCB: 60 / ScanNet: 70 / COCO: 57, in pixels). Each object in a\nspeciﬁc dataset is scaled up or down to approach the average scale in this dataset. The shape and\nappearance are also linearly scaled, so that the shape variation is the only factor changed in the new\ndataset.\n• Ablation of both Inter-object Color Similarity and Shape Variation: In the three ablated datasets:\nYCB-T+U / ScanNet-T+U / COCO-T+U, each object’s texture is replaced and shape is uniformly\nscaled.\nFigure 22 shows example images from the three real-world datasets with different types of scene-level\nfactor ablations.\nQualitative and Quantitative Results\nAs shown in Figure 23 and Table 6, all methods have a\nsigniﬁcant improvement in object segmentation on the ablated datasets with reduced inter-object\ncolor similarity. The effect of uniform scale ablation is limited standalone. However, when both\nablations are combined, there is a larger performance gain compared with purely texture replaced\nablation. Speciﬁcally, AIR [22] is sensitive to both scene-level factors, since only the combination\nof two factors can result in a signiﬁcant improvement. MONet [8] and IODINE [25], however, are\nalmost only sensitive to the inter-object color similarity. Both factors have a noticeable effect on the\nsegmentation performance of SlotAtt [40].\nFigure 21: Six selected texture\nimages from DTD [15].\nYCB\nYCB-T\nYCB-U\nYCB-T+U\nScanNet\nScanNet-T\nScanNet-U\nScanNet-T+U\nCOCO\nCOCO-T\nCOCO-U\nCOCO-T+U\nFigure 22: Example images of datasets ablated with\nscene-level factors.\n23\nMONet\nAIR\nGT \nMasks\nOriginal\nImages\nAblated\nImages\nIODINE\nSlotAtt\nInter-object Color Similarity Ablation Inter-object Shape Variation Ablation\nBoth Ablation\nYCB-T\nScanNet-T COCO-T\nYCB-U\nScanNet-U COCO-U\nYCB-T+U ScanNet-T+U COCO-T+U\nFigure 23: Qualitative results on the datasets ablated with scene-level factors.\nTable 6: Quantitative results on the datasets ablated with scene-level factors. Standard deviations of\nperformance are calculated over 3 runs (marked with blue).\nYCB-T\nScanNet-T\nCOCO-T\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n5.9 3.7 / 9.6 3.3 / 19.9 10.3 / 12.7 1.3\n2.9 1.2 / 6.3 0.1 / 13.4 3.7 / 8.6 3.2\n7.4 3.1 / 13.0 4.0 / 29.1 9.8 / 16.4 4.3\nMONet [8] 86.5 12.4 / 78.3 13.3 / 81.1 11.6 / 86.8 12.2\n82.9 12.3 / 77.8 7.6 / 86.3 1.6 / 83.7 11.9\n80.0 9.4 / 68.8 13.5 / 74.5 15.0 / 82.0 5.6\nIODINE [25]\n32.4 9.0 / 27.3 6.8 / 35.3 8.3 / 43.6 10.6\n33.2 11.9 / 27.3 6.3 / 34.5 7.0 / 44.7 9.3\n40.8 12.7 / 33.7 10.2 / 41.8 10.4 / 55.9 15.8\nSlotAtt [40]\n64.6 5.3 / 48.5 3.2 / 58.3 3.5 / 74.94.6\n20.5 22.3 / 18.2 20.1 / 22.9 23.1 / 33.2 31.4 31.8 17.8 / 28.0 11.2 / 35.2 12.4 / 50.1 19.8\nYCB-U\nScanNet-U\nCOCO-U\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n9.8 3.5 / 12.2 2.6 / 18.6 4.1 / 19.9 2.8\n7.1 1.4 / 10.4 0.2 / 19.9 2.1 / 14.0 1.0\n12.9 4.8 / 16.3 4.6 / 28.6 1.8 / 24.3 9.3\nMONet [8]\n3.7 0.5 / 8.3 1.2 / 12.2 1.4 / 13.2 1.4\n25.4 8.3 / 23.6 7.4 / 29.2 8.9 / 37.2 12.5\n14.5 0.6 / 16.5 0.8 / 27.9 1.4 / 20.9 0.8\nIODINE [25]\n2.5 1.4 / 4.9 2.0 / 7.6 2.9 / 9.3 3.6\n16.9 0.8 / 17.2 0.0 / 23.8 0.8 / 30.2 0.1\n5.5 1.0 / 7.7 0.0 / 11.7 0.6 / 13.5 0.3\nSlotAtt [40]\n28.9 2.0 / 21.9 1.2 / 30.7 3.1 / 37.3 2.4\n9.4 4.8 / 10.4 3.8 / 15.4 4.6 / 18.1 7.2\n4.4 1.1 / 7.9 0.3 / 11.9 0.9 / 14.7 0.7\nYCB-T+U\nScanNet-T+U\nCOCO-T+U\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n18.9 13.7 / 21.0 9.9 / 32.5 15.7 / 32.6 13.2\n12.3 5.4 / 16.8 5.6 / 37.4 20.8 / 20.4 1.1\n25.3 13.0 / 28.3 9.0 / 49.4 13.2 / 38.2 12.4\nMONet [8]\n89.1 0.8 / 84.7 1.5 / 91.7 4.1 / 89.3 0.8\n87.0 8.9 / 77.5 10.3 / 80.5 11.1 / 87.4 8.7\n88.2 2.9 / 76.2 0.1 / 79.9 3.7 / 89.0 2.9\nIODINE [25]\n35.3 1.2 / 26.4 0.3 / 34.9 0.6 / 42.8 0.2\n27.4 0.8 / 25.0 0.8 / 32.1 0.5 / 41.8 1.8\n53.3 16.7 / 35.6 8.3 / 45.0 9.7 / 59.8 13.0\nSlotAtt [40]\n76.8 1.2 / 57.1 3.8 / 72.4 3.4 / 77.7 1.8\n47.3 11.2 / 33.8 9.2 / 42.6 9.1 / 57.3 8.4\n34.5 17.8 / 25.3 7.6 / 33.7 8.3 / 43.0 14.1\nA.6.3\nAblations on Object- and Scene-Level Factor Joint Ablations - Part 1\nAblated Datasets\n• Ablation of Object Color Gradient, Object Shape Concavity and Inter-object Color Similarity:\nIn the three ablated datasets: YCB-C+S+T / ScanNet-C+S+T / COCO-C+S+T, for each image,\nwe ﬁrst ﬁnd the smallest convex hull [19] for each object. Then each convex hull is ﬁlled with a\ndistinctive texture as shown in Figure 21. Lastly, we calculate the average color of the distinctive\ntexture in each convex hull and change each pixel inside to be that color.\n• Ablation of Object Color Gradient, Object Shape Concavity and Inter-object Shape Variation: In\nthe three ablated datasets: YCB-C+S+U / ScanNet-C+S+U / COCO-C+S+U, we use the (C+S)\ndatasets created before and ﬁnd the uniform scale of the object convex hull.\n• Ablation of all four factors: In the three ablated datasets: YCB-C+S+T+U / ScanNet-C+S+T+U /\nCOCO-C+S+T+U, each image is ablated with the four factors together.\n24\nYCB\nYCB-\nC+S+T\nYCB-\nC+S+U\nYCB-\nC+S+T+U\nScanNet\nScanNet-\nC+S+T\nScanNet-\nC+S+U\nScanNet-\nC+S+T+U\nCOCO\nCOCO-\nC+S+T\nCOCO-\nC+S+U\nCOCO-\nC+S+T+U\nFigure 24: Example images of datasets ablated with both object- and scene-level factors.\nFigure 24 shows example images from the three real-world datasets with different types of joint\nobject- and scene-level factor ablations.\nQualitative and Quantitative Results\nAs shown in Figure 25 and Table 7, all methods have\nachieved a signiﬁcant improvement in object segmentation on the datasets ablated with joint object-\nand scene-level factors. The datasets with all four factors ablated can lead to impressive performance\nsimilar to the synthetic datasets. This shows that the distribution gaps of the objectness biases\nrepresented by the four factors between the synthetic and real-world datasets lead to the failure of\nexisting unsupervised models.\nYCB-\nC+S+T\nScanNet-\nC+S+T\nCOCO-\nC+S+T\nYCB-\nC+S+U\nScanNet-\nC+S+U\nCOCO-\nC+S+U\nYCB-\nC+S+T+U\nScanNet-\nC+S+T+U\nCOCO-\nC+S+T+U\nInput\nImages\nAIR\nMONet\nIODINE\nGT \nMasks\nSlotAtt\nAblated\nImages\nFigure 25: Qualitative results on the datasets ablated with both object- and scene-level factors.\n25\nTable 7: Quantitative results on the datasets ablated with both object- and scene-level factors. Standard\ndeviations of performance are calculated over 3 runs (marked with blue).\nYCB-C+S+T\nScanNet-C+S+T\nCOCO-C+S+T\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n6.3 3.7 / 10.7 3.3 / 24.0 13.2 / 13.1 0.5\n1.9 3.9 / 6.4 5.2 / 27.6 10.7 / 6.3 13.9\n10.8 5.6 / 18.6 7.0 / 45.1 14.4 / 20.3 6.9\nMONet [8]\n67.7 4.2 / 55.0 8.5 / 62.7 19.8 / 75.4 10.2\n76.1 7.4 / 61.7 2.6 / 64.4 2.1 / 77.1 2.8\n67.1 15.3 / 56.4 10.1 / 62.2 6.2 / 73.9 17.3\nIODINE [25]\n82.6 2.6 / 63.8 0.9 / 75.5 5.1 / 86.0 1.6\n69.0 14.9 / 53.5 14.1 / 60.7 14.5 / 77.3 9.4\n77.6 7.8 / 59.8 6.8 / 72.5 1.8 / 83.0 7.5\nSlotAtt [40] 65.9 23.2 / 51.6 18.5 / 62.2 19.8 / 72.9 17.6\n60.2 8.7 / 45.5 8.1 / 52.9 9.2 / 68.3 7.3\n51.8 11.0 / 41.5 7.7 / 48.6 7.5 / 62.9 8.5\nYCB-C+S+U\nScanNet-C+S+U\nCOCO-C+S+U\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n12.5 6.4 / 17.7 7.0 / 27.5 11.2 / 29.6 10.5\n8.2 1.5 / 11.1 0.5 / 20.4 0.8 / 15.0 1.3\n23.7 9.8 / 27.5 8.0 / 51.3 9.6 / 32.8 8.3\nMONet [8]\n3.5 2.5 / 4.8 3.2 / 23.5 2.6 / 5.3 6.5\n60.1 9,5 / 56.3 13.3 / 69.7 20.7 / 64.0 6.2\n35.6 1.4 / 33.0 3.8 / 43.4 16.6 / 38.7 0.1\nIODINE [25]\n65.3 9.2 / 55.9 10.4 / 73.9 4.6 / 67.7 8.4\n64.6 3.2 / 52.1 5.1 / 61.3 7.6 / 68.2 2.2\n64.8 7.3 / 53.4 6.5 / 65.8 8.5 / 68.2 2.5\nSlotAtt [40]\n58.1 3.7 / 39.1 2.8 / 52.4 2.3 / 62.0 3.6\n50.5 2.9 / 38.2 3.6 / 51.8 6.9 / 55.0 1.3\n20.2 5.2 / 18.1 0.3 / 26.9 0.1 / 33.4 0.6\nYCB-C+S+T+U\nScanNet-C+S+T+U\nCOCO-C+S+T+U\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22] 24.8 20.1 / 23.2 11.9 / 33.7 16.8 / 39.1 20.1\n10.0 1.8 / 13.5 1.8/ 24.0 1.4 / 18.2 7.4\n36.4 19.3 / 39.4 14.5 / 63.1 16.9 / 49.2 17.9\nMONet [8]\n70.8 9.5 / 79.2 13.0 / 96.8 9.4 / 70.8 8.5\n79.2 3.4 / 65.0 3.6 / 69.1 4.2 / 82.4 2.2\n76.2 3.0 / 75.9 3.8 / 90.9 6.4 / 76.5 3.3\nIODINE [25]\n72.4 4.8 / 54.2 6.7 / 65.1 5.8 / 74.3 5.8\n76.1 8.7 / 56.5 9.2 / 66.9 7.0 / 80.3 6.2\n81.4 3.4 / 59.3 5.1 / 70.5 7.7 / 84.3 6.7\nSlotAtt [40]\n92.0 2.0 / 65.5 6.1 / 84.4 6.5 / 92.5 1.7\n62.7 25.4 / 42.6 28.4 / 50.5 34.3 / 69.4 19.0\n83.7 4.7 / 60.7 7.2 / 76.4 4.6 / 84.0 5.0\nA.6.4\nAblations on Object- and Scene-Level Factor Joint Ablations - Part 2\nIn addition to the experiments in Section A.6.3, we generate additional 6 groups of datasets ablated\nwith different combinations of both object- and scene-level factors as detailed in Section A.6.4, and\nconduct extra experiments shown in Section A.6.4. Figure 26 shows example images of the additional\nablated datasets.\nAblated Datasets\n• Ablation of Object Color Gradient and Inter-object Color Similarity: In each image of the three\nreal-world datasets, we replace the object color by averaging all pixels of a distinctive texture, and\nkeep the original shape unchanged, getting three ablated datasets: YCB-C+T / ScanNet-C+T /\nCOCO-C+T.\n• Ablation of Object Color Gradient and Inter-object Shape Variation: In each image of the three\nreal-world datasets, we replace the object color by averaging its own texture, and then apply\nsize normalization on the original shape of objects, getting three ablated datasets: YCB-C+U /\nScanNet-C+U / COCO-C+U.\n• Ablation of Object Color Gradient, Inter-object Color Similarity and Inter-object Shape Variation:\nIn each image of the three real-world datasets, we replace the object color by averaging all pixels\nof a distinctive texture, and then apply size normalization on the original shape of objects. The\nablated datasets are denoted as: YCB-C+T+U / ScanNet-C+T+U / COCO-C+T+U.\n• Ablation of Object Shape Concavity and Inter-object Color Similarity: In each image of the three\nreal-world datasets, we replace the object color by a distinctive texture, and modify the object shape\nas a convex hull, getting three ablated datasets: YCB-S+T / ScanNet-S+T / COCO-S+T.\n• Ablation of Object Shape Concavity and Inter-object Shape Variation: In each image of the\nthree real-world datasets, we keep the texture of object unchanged, and modify the object shape\nas a convex hull followed by size normalization, getting three ablated datasetss: YCB-S+U /\nScanNet-S+U / COCO-S+U.\n• Ablation of Object Shape Concavity, Inter-object Color Similarity and Inter-object Shape Variation:\nIn each image of the three real-world datasets, we replace the object color by a distinctive texture,\nand modify the object shape as a convex hull followed by size normalization. The ablated datasets\nare denoted as: YCB-S+T+U / ScanNet-S+T+U / COCO-S+T+U.\nQualitative and Quantitative Results\nAs shown in Table 8 and Figures 27/28/29/30, all 6 addi-\ntional combinations of object- and scene-level factors are explored, demonstrating consistent ﬁndings\nas our experiments in Section 4.4. Overall, all four methods show a high sensitivity to both object-\nand scene-level factors relating to appearance. This can be seen from the fact that for datasets without\nablations in appearance, i.e., the (S+U) ablated datasets, the object segmentation performance is\ninferior. By contrast, the object segmentation accuracy can be greatly improved on the datasets only\n26\nYCB\nScanNet\nCOCO\nYCB-C+T\nYCB-C+U\nYCB-S+T\nYCB-S+U\nYCB-C+T+U\nYCB-S+T+U\nScanNet-S+T+U\nScanNet-C+T+U\nScanNet-C+T\nScanNet-C+U\nScanNet-S+T\nScanNet-S+U\nCOCO-C+T\nCOCO-C+U\nCOCO-S+T\nCOCO-S+U\nCOCO-C+T+U\nCOCO-S+T+U\nFigure 26: Example images of additional datasets ablated with both object- and scene-level factors.\nTable 8: Quantitative results on additional datasets ablated with both object- and scene-level factors.\nStandard deviations of performance are calculated over 3 runs (marked with blue).\nYCB-C+T\nScanNet-C+T\nCOCO-C+T\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n8.0 6.1 / 13.2 7.3 / 26.6 17.6 / 17.9 7.3\n2.8 4.7 / 6.7 6.5 / 14.8 4.0 / 8.1 14.8\n12.4 9.4 / 30.2 23.0 / 40.1 25.6 / 24.2 13.8\nMONet [8]\n82.9 9.6 / 66.7 0.5 / 73.0 1.2 / 87.8 12.2\n36.5 28.4 / 33.5 21.8 / 40.4 24.1 / 50.3 19.7\n66.1 5.6 / 53.2 3.7 / 56.2 2.3 / 74.4 6.4\nIODINE [25]\n78.9 2.6 / 59.7 2.1 / 71.5 0.6 / 83.6 2.7\n65.1 2.9 / 51.1 1.0 / 62.4 2.9 / 74.2 0.5\n55.1 10.4 / 42.2 7.5 / 56.3 6.4 / 66.2 9.2\nSlotAtt [40]\n58.7 12.1 / 43.2 5.0 / 57.8 9.4 / 71.0 9.4\n29.2 2.9 / 27.6 1.3 / 34.4 1.4 / 49.2 1.2\n22.1 8.6 / 19.6 4.1 / 25.2 4.1 / 35.8 8.1\nYCB-C+U\nScanNet-C+U\nCOCO-C+U\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n11.4 4.9 / 16.4 5.4 / 25.8 8.7 / 25.1 6.5\n5.4 0.5 / 12.3 2.6 / 35.1 15.3 / 12.6 0.4\n20.5 10.8 / 24.9 7.1 / 47.7 10.2 / 30.5 8.8\nMONet [8]\n49.1 3.5 / 44.6 4.1 / 60.6 3.2 / 52.5 3.3\n31.9 5.7 / 33.7 5.7 / 46.8 5.4 / 35.7 8.7\n32.6 11.1 / 28.5 12.1 / 34.9 14.2 / 39.0 18.5\nIODINE [25]\n66.3 0.1 / 54.6 0.0 / 71.3 4.1 / 70.0 0.2\n34.8 20.9 / 30.0 14.7 / 36.4 20.9 / 47.2 14.0\n44.4 8.9 / 32.6 7.9 / 41.7 12.3 / 51.1 7.6\nSlotAtt [40]\n45.6 7.9 / 31.2 4.2 / 41.9 7.7 / 51.3 6.4\n30.4 8.9 / 24.2 6.2 / 33.8 9.5 / 40.6 11.0\n12.7 7.7 / 11.3 3.7 / 15.8 5.0 / 23.5 6.8\nYCB-S+T\nScanNet-S+T\nCOCO-S+T\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n9.3 7.8 / 13.4 8.4 / 21.9 14.3 / 21.2 12.3\n3.2 0.5 / 7.7 0.6 / 18.5 6.0 / 8.9 6.0\n10.5 5.4 / 18.2 7.2 / 4.3 23.2 / 20.3 7.3\nMONet [8]\n86.7 2.2 / 78.9 1.6 / 81.9 1.8 / 87.0 2.2\n83.2 3.7 / 77.8 13.0 / 86.0 20.0 / 83.8 2.9\n80.2 0.0 / 68.5 7.2 / 73.2 11.7 / 82.3 1.0\nIODINE [25] 41.9 17.9 / 33.2 11.4 / 41.5 12.0 / 51.3 14.7 54.9 23.1 / 44.4 16.2 / 52.0 16.7 / 68.0 21.6\n44.1 1.1 / 34.3 0.7 / 41.0 0.3 / 56.8 0.6\nSlotAtt [40]\n77.3 7.8 / 60.6 1.5 / 75.0 2.3 / 69.2 19.7\n24.9 46.1 / 21.2 35.0 / 25.5 38.3 / 37.9 43.2\n67.4 3.1 / 50.2 1.6 / 59.2 1.6 / 76.4 0.3\nYCB-S+U\nScanNet-S+U\nCOCO-S+U\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n0.8 5.4 / 2.9 7.1 / 5.0 10.0 / 5.2 12.7\n6.3 0.2 / 13.1 3.3 / 32.4 16.3 / 14.3 1.2\n20.0 9.5 / 25.8 10.0 / 48.3 15.6 / 31.6 10.9\nMONet [8]\n5.5 1.2 / 9.8 0.4 / 14.1 0.8 / 17.0 0.2\n36.9 2.9 / 31.4 2.6 / 38.2 3.6 / 50.1 0.1\n26.1 3.5 / 23.8 1.8 / 29.6 1.6 / 41.2 10.0\nIODINE [25]\n2.8 2.4 / 4.6 2.0 / 7.2 2.9 / 8.9 3.6\n17.3 2.4 / 17.8 1.4 / 24.1 1.8 / 31.6 2.1\n5.7 0.5 / 8.7 1.7 / 1.3 13.7 / 16.3 2.4\nSlotAtt [40]\n36.2 3.3 / 23.8 4.1 / 33.6 5.5 / 45.6 1.7\n21.1 0.9 / 18.9 0.2 / 26.2 0.1 / 32.5 2.0\n12.7 7.1 / 12.1 2.3 / 16.4 2.1 / 24.7 5.2\nYCB-C+T+U\nScanNet-C+T+U\nCOCO-C+T+U\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22]\n18.2 12.0 / 20.5 7.9 / 32.5 13.5 / 30.9 9.0\n13.5 6.6 / 20.2 9.1 / 42.4 25.5 / 24.1 5.6\n24.2 11.1 / 28.3 8.5 / 50.5 13.1 / 36.4 9.7\nMONet [8]\n63.2 6.9 / 66.5 11.8 / 86.3 13.0 / 64.4 5.7\n74.6 5.6 / 62.1 6.6 / 67.8 7.2 / 79.2 3.2\n73.0 3.3 / 75.7 10.3 / 94.2 23.1 / 73.1 7.5\nIODINE [25] 54.9 23.1 / 38.7 18.2 / 58.0 12.0 / 59.4 20.8\n65.5 3.8 / 48.0 3.2 / 59.8 5.9 / 70.7 2.9\n47.9 11.7 / 35.5 7.4 / 53.8 2.1 / 53.4 14.2\nSlotAtt [40]\n73.8 4.6 / 53.0 2.9 / 67.6 7.8 / 75.2 4.5\n58.5 8.1 / 45.5 10.2 / 52.9 8.9 / 68.3 11.9\n30.5 27.6 / 20.3 18.0 / 27.8 23.0 / 38.9 23.3\nYCB-S+T+U\nScanNet-S+T+U\nCOCO-S+T+U\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAP / PQ / Pre / Rec\nAIR [22] 24.6 20.0 / 24.8 13.7 / 37.4 20.7 / 37.6 18.6\n16.7 10.1 / 23.9 12.0 / 47.8 30.2 / 28.5 7.8\n29.4 16.5 / 34.6 18.4 / 58.3 35.7 / 42.2 13.4\nMONet [8]\n88.4 0.6 / 84.6 1.5 / 92.0 1.5 / 88.6 0.7\n97.8 0.3 / 87.3 0.8 / 85.4 0.1 / 98.1 0.4\n86.9 0.1 / 83.8 11.6 / 89.9 13.1 / 87.0 1.5\nIODINE [25]\n42.3 15.0 / 32.1 9.7 / 41.0 11.5 / 52.0 15.7\n39.1 4.7 / 29.9 1.8 / 41.8 7.2 / 47.9 2.4\n64.3 9.7 / 45.1 6.9 / 51.4 5.6 / 72.0 11.7\nSlotAtt [40]\n88.6 6.5 / 67.0 9.6 / 80.9 13.0 / 89.6 5.6\n82.6 8.5 / 61.5 10.1 / 73.2 10.2 / 83.9 7.7\n84.2 0.9 / 59.6 6.8 / 74.7 3.8 / 85.3 0.6\nwith appearance factors ablated, i.e., the (C+T) datasets. Meanwhile, more regular shapes and uniform\nscales of objects still have a signiﬁcant positive inﬂuence on the success of object segmentation\nespecially when the appearance factors are combined in ablated datasets. To be speciﬁc, AIR [22] is\nquite sensitive to the scale of objects apart from object color gradient and inter-object color similarity.\nMONet [8] can obtain comparable performance to the simple synthetic datasets once object color\ngradient and inter-object color similarity are ablated. All four factors are closely relevant the results\nof IODINE [25] and SlotAtt [40].\n27\nYCB and its 6 ablations\nScanNet and its 6 ablations\nCOCO and its 6 ablations\nAP\nPQ\nPre\nRec\nS+T+U\nYCB\nC+T\nC+U\nS+T\nC+T+U\nS+U\nS+T+U\nScanNet\nC+T\nC+U\nS+T\nC+T+U\nS+U\nS+T+U\nCOCO\nC+T\nC+U\nS+T\nC+T+U\nS+U\nS+T+U\nYCB\nC+T\nC+U\nS+T\nC+T+U\nS+U\nS+T+U\nScanNet\nC+T\nC+U\nS+T\nC+T+U\nS+U\nS+T+U\nCOCO\nC+T\nC+U\nS+T\nC+T+U\nS+U\nS+T+U\nYCB\nC+T\nC+U\nS+T\nC+T+U\nS+U\nS+T+U\nScanNet\nC+T\nC+U\nS+T\nC+T+U\nS+U\nS+T+U\nCOCO\nC+T\nC+U\nS+T\nC+T+U\nS+U\nS+T+U\nYCB\nC+T\nC+U\nS+T\nC+T+U\nS+U\nS+T+U\nScanNet\nC+T\nC+U\nS+T\nC+T+U\nS+U\nS+T+U\nCOCO\nC+T\nC+U\nS+T\nC+T+U\nS+U\n100%\n75%\n50%\n25%\n0%\n100%\n75%\n50%\n25%\n0%\n100%\n75%\n50%\n25%\n0%\n100%\n75%\n50%\n25%\n0%\nSlotAtt\nIODINE\nMONet\nAIR\nFigure 27: Quantitative results of baselines on three real-world datasets and their variants in Sec\nA.6.4\n28\nYCB-\nC+T\nScanNet-\nC+T\nCOCO-\nC+T\nYCB-\nC+U\nScanNet-\nC+U\nCOCO-\nC+U\nInput\nImages\nAIR\nMONet\nGT \nMasks\nSlotAtt\nAblated\nImages\nIODINE\nFigure 28: Qualitative results on the additional datasets ablated with object- and scene-level factors.\nYCB-\nS+T\nScanNet-\nS+T\nCOCO-\nS+T\nInput\nImages\nAIR\nMONet\nGT \nMasks\nSlotAtt\nAblated\nImages\nIODINE\nYCB-\nS+U\nScanNet-\nS+U\nCOCO-\nS+U\nFigure 29: Qualitative results on the additional datasets ablated with object- and scene-level factors.\n29\nYCB-\nC+T+U\nScanNet-\nC+T+U\nCOCO-\nC+T+U\nYCB-\nS+T+U\nScanNet-\nS+T+U\nCOCO-\nS+T+U\nInput\nImages\nAIR\nMONet\nIODINE\nGT \nMasks\nSlotAtt\nAblated\nImages\nFigure 30: Qualitative results on the additional datasets ablated with object- and scene-level factors.\n30\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "cs.MM",
    "cs.RO"
  ],
  "published": "2022-10-05",
  "updated": "2022-10-05"
}