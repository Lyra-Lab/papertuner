{
  "id": "http://arxiv.org/abs/2210.03292v2",
  "title": "Unsupervised Semantic Representation Learning of Scientific Literature Based on Graph Attention Mechanism and Maximum Mutual Information",
  "authors": [
    "Hongrui Gao",
    "Yawen Li",
    "Meiyu Liang",
    "Zeli Guan"
  ],
  "abstract": "Since most scientific literature data are unlabeled, this makes unsupervised\ngraph-based semantic representation learning crucial. Therefore, an\nunsupervised semantic representation learning method of scientific literature\nbased on graph attention mechanism and maximum mutual information (GAMMI) is\nproposed. By introducing a graph attention mechanism, the weighted summation of\nnearby node features make the weights of adjacent node features entirely depend\non the node features. Depending on the features of the nearby nodes, different\nweights can be applied to each node in the graph. Therefore, the correlations\nbetween vertex features can be better integrated into the model. In addition,\nan unsupervised graph contrastive learning strategy is proposed to solve the\nproblem of being unlabeled and scalable on large-scale graphs. By comparing the\nmutual information between the positive and negative local node representations\non the latent space and the global graph representation, the graph neural\nnetwork can capture both local and global information. Experimental results\ndemonstrate competitive performance on various node classification benchmarks,\nachieving good results and sometimes even surpassing the performance of\nsupervised learning.",
  "text": "*Corresponding author: Yawen Li (warmly0716@126.com). \nUnsupervised Semantic Representation Learning of Scientific \nLiterature Based on Graph Attention Mechanism and \nMaximum Mutual Information \nHongrui Gao1, Yawen Li2*, Meiyu Liang1, Zeli Guan1  \n1Beijing Key Laboratory of Intelligent Communication Software and Multimedia, School of Computer Science \n(National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing 100876 \n2School of Economics and Management, Beijing University of Posts and Telecommunications, Beijing 100876\n \nAbstract: Since most scientific literature data are \nunlabeled, this makes unsupervised graph-based semantic \nrepresentation \nlearning \ncrucial. \nTherefore, \nan \nunsupervised semantic representation learning method of \nscientific literature based on graph attention mechanism \nand maximum mutual information (GAMMI) is proposed. \nBy introducing a graph attention mechanism, the \nweighted summation of nearby node features makes the \nweights of adjacent node features entirely depend on the \nnode features. Depending on the features of the nearby \nnodes, different weights can be applied to each node in \nthe graph. In addition, an unsupervised graph contrastive \nlearning strategy is proposed to solve the problem of \nbeing unlabeled and scalable on large-scale graphs. By \ncomparing the mutual information between the positive \nand negative local node representations on the latent \nspace and the global graph representation, the graph \nneural network can capture both local and global \ninformation. \nExperimental \nresults \ndemonstrate \ncompetitive performance on various node classification \nbenchmarks, achieving good results and sometimes even \nsurpassing the performance of supervised learning. \nKeywords: Semantic representation; Graph neural \nnetwork; Graph attention; Maximum mutual information \n1 Introduction \nCurrently, scientific literature resources are flooding the \nInternet [1][2]. How to extract important information \nfrom scientific literature and effectively represent them \nsemantically is the core issue to realize the classification, \nretrieval, and recommendation of scientific literature.  \nTraditional methods mainly rely on expert experience to \nconstruct artificial features to represent scientific \nliterature. For example, in academic information retrieval, \nliterature titles and abstract texts are used to construct an \ninverted index [3]. But every time new text data is added, \nit is tedious to rebuild the entire index. In literature \nclassification and recommendation systems, bag of words \nmodel [4], vector space model, and topic model [5] are \nused to build scientific literature vectors. However, the \nshortcomings of the bag of words model are obvious. \nWhen the vocabulary increases, there are at most ten \nwords used for each sentence, which leads to the sparse \nmatrix of each sentence, seriously affecting the memory \nand computing resources. Mainstream methods can map \ndata into vector space and operate on vectors to complete \nspecific tasks[6-9][13]. However, the existing vector \nspace model-based document processing methods are \nbased on word frequency information[10][11]. The \nsimilarity of two documents depends on the number of \ncommon words, and the semantic ambiguity of natural \nlanguage cannot be distinguished. \nDeep learning-based representation learning has received \nextensive attention recently[12][14-16]. The fundamental \ndrawback is that the neural language model only focuses \non the text semantics information of academic \ndocuments[17] while ignoring the relationship between \nacademic documents. Some researchers solve the \nproblem of imperfect data characteristics by means of \nmulti-agent [18-19]. More and more researchers are \nfusing different features to better complete deep learning \ntasks[20-24]. \nIn view of this, graph neural network [25] is proposed to \nextract the relationship structure information between \ndocuments from the document citation network and fuse \nthem with the semantic information of the document text, \nso as to construct the representation vector of academic \ndocuments. However, most of the existing studies use \nsupervised graph neural networks to learn feature \nrepresentations of documents[26][27], which have \nfollowing two drawbacks: For specific tasks, supervised \ngraph neural networks need to develop a huge amount of \nexcellently labeled data; The feature representation of \ntext obtained by supervised graph neural network[28]is \nhighly coupled with the task of labeling datasets, and it is \ndifficult to directly transfer to other tasks, resulting in \npoor universality of feature representation. \nCompared to supervised learning method, unsupervised \ngraph neural networks perform better. Because they can \ndirectly learn general document feature representations \nfrom unlabeled document network data[29].  \nBased on this, this paper proposes an unsupervised \nsemantic representation learning method for scientific \nliterature based on graph attention mechanism and \nmaximum mutual information (GAMMI). The following \nare this paper’s main contributions: \n1) A semantic representation learning method for \nscientific literature based on graph attention mechanism \nand maximum mutual information is proposed. By \nProceedings of CCIS2022 \n \nintroducing graph attention mechanism, the relationships \nbetween nodes features are better included into the model, \nand the node representation is only related to adjacent \nnodes, which can be directly applied to inductive learning \nwithout receiving all graph information. \n2) An unsupervised graph contrastive learning strategy is \nproposed to address unlabeled and scalable problems on \nlarge-scale graphs. Contrastive learning-based methods \nwhich can capture both local and global information, \ncompare the mutual information between positive and \nnegative local node representations and global graph \nrepresentations on the latent space. \n3) Experimental results show that GAMMI performs \ncompetitively on node classification benchmarks, \nachieving good results and sometimes even surpassing \nthe performance of supervised learning.  \n2 Related work \nIn studies on natural language processing, the vector \nrepresentation of literature is obtained by training \nlargescale pre-trained language models, including \nWord2Vec [30]based on word context prediction, ELMo \n[31]based on contextual Word Embedding bidirectional \ndynamic \nadjustment, \nand \nTransformer[32] \nbased \nbidirectional \nlanguage \nmodel \nBERT[33][34]. \nAn \nimportant issue is how to build a suitable neural network \nstructure for certain specialized tasks in the research of \nrepresentation learning based on deep learning method. \nConvolutional \nneural \nnetworks, \nrecurrent \nneural \nnetworks[35] and attention mechanism are primary \nmethods. \nPetar[36] proposed a graph attention network, which uses \na masked self-attention layer to solve the shortcomings of \nprevious methods based on graph convolution or its \napproximation. \nEmpirical research shows that, through deep neural \nnetwork learning features, representation learning can \nhave strong data representation capabilities[37][38], and \ncan learn more general prior knowledge independent of a \nspecific task. \nAt present, there are three main categories of methods in \nthe field of graph embedding based on factorization, \nrandom walks, and deep learning. Yu proposed a \nfactorization-based text representation algorithm[39], \nwhere they created matrices that each measure a pair of \nexamples' similarities in two different ways. Kawin \nproposes a random walk model[40], in which the \nprobability of a word being generated is inversely \ncorrelated with the angular distance between the word and \nsentence embeddings. In order to match and rank text for \nrelevant information, a semantic representation method \nbased on CNN is proposed by Zhou[41]. \nSince most scientific literature data are unlabeled, this \nmakes unsupervised graph-based semantic representation \nlearning crucial. Unsupervised graph learning[42] mainly \nrelies on random walk objectives, which is highly \ndependent on the choice of parameters. Contrastive \nmethods are at the core of many popular word embedding \nmethods, and Yuning[43] proposed a graph contrast \nlearning framework to study the impact of various \ncombinations of graph enhancement on multiple data sets, \nwhich can generate graphical representations with similar \nor better versatility, portability and robustness. \n3 The proposed GAMMI method  \nIn this section, this paper proposes an unsupervised \nsemantic representation learning method for scientific \nliterature based on graph attention mechanism and \nmaximum mutual information (GAMMI). First, the graph \nattention encoder is utilized to learn representation for \nnodes, and then an unsupervised graph contrastive \nlearning strategy is utilized to solve the problem of being \nunlabeled and scalable on large-scale graphs. The \nframework of GAMMI method is shown in Figure 1. \n \nFigure 1: The framework of the proposed GAMMI method.  \n3.1 Encoder based on graph attention mechanism \nThis paper utilized linear transformation to convert the \ninput characteristics into higher-level features. In the \ngraph attention layer, first we use a weight matrix \n\n\nF\nF\nW\nR\n to impact on each literature, and then utilize \nself-attention to calculate an attention coefficient. The \nshared self-attention mechanism is expressed as attention \ncoefficients 𝑎: \n\n\n\n→\nF\nF\nR\nR\nR  \n(\n,\n)\n=\nij\ni\nj\ne\na W x W x\n          (1) \nwhere \nij\ne   is the significance of node j  s  features to \nProceedings of CCIS2022 \n \nnode i  . We introduce softmax regularizes for all i  ss \nneighbors j : \n(\n)\n(\n)\n(\n)\n\n\n=\n= \ni\nij\nij\nj\nij\nim\nm N\nexp e\nsoftmax\ne\nexp e\n    (2) \nAfter obtaining the weight matrix between the connection \nlayers of neural network, we use LeakyReLu function to \nthe output layer of the feedforward neural network. \nCombining the above formulas (1) and (2), the complete \nattention mechanism can be obtained as follows: \n(\n(\n[\n||\n]))\n(\n(\n[\n||\n]))\n\n\n= \ni\nT\ni\nj\nij\nT\ni\nm\nm N\nexp LeakyReLu a\nW x\nW x\nexp LeakyReLu a\nW x\nW x\n  (3) \nwhereij  and \nij\ne  are both called attention coefficients. \nij  is obtained after softmax normalization based on \nij\ne . \nThe regularized attention coefficients between different \nnodes are obtained through the above operations. We use \nit to predict the output features of each literature: \n(\n)\n\n\n\n=\n\ni\ni\nij\nj\nj N\nx\nPReLu\nW x\n        (4) \nwhere W  is the weight matrix multiplied by the features, \n𝛼 is the attention cross-correlation coefficient calculated \nearlier, and \n\ni\nj\nN  represents all nodes adjacent to i .  \nTo steady the process of self-attention learning, it is \nbeneficial to use multi-head attention. Specifically, Eq.4 \nis transformed by introducing K independent attention \nmechanisms, then K-average operations are performed on \ntheir features, and the final PReLU function is applied. It \ncan be expressed mathematically as: \n1\n1\n(\n)\n\n\n=\n\n=\n\ni\nK\nk\nk\ni\nij\nj\nk\nj N\nx\nPReLu\nW x\nK\n      (5) \nwhere K  represents the number of attention heads, k  \nrepresents the k-th attention mechanism, and \nk\nW  \nreflects the input featuress linear transformation weight \nmatrix under the k-th attention mechanism. \n3.2 Contrastive Learning Based on Maximum \nMutual Information \nGraph attention layer produces node embedding, \n\nix  , \nwhich summarizes graph patch centered around node 𝑖, \nnot just the node itself. To get graph-level summary vector \nt , this paper uses the readout function, \n:\n\n→\nN F\nF\nF R\nR\nwhich obtains the feature representation of the entire \ngraph by aggregating node features. The process is \nrepresented as: \n( (\n,\n))\n=\nt\nF E X A\n            (6) \nThe readout function F   can be a straightforward \npermutation-invariant function. This paper uses following \nreadout function to get graph-level representation: \n1\n1\n(\n)\n\n=\n= \nN\ni\ni\nF X\nx\nN\n           (7) \nAs a metric to maximize local mutual information, this \npaper uses a discriminator, \n:\n\n→\nF\nF\nD R\nR\nR . \n(\n, )\ni\nD x t  \nis the probability scores given to this patch-summary \ngroup. \n(\n, )\n(\n)\n\n=\nT\ni\ni\nD x t\nx Wt          (8) \nIn this paper, we use the random shuffle function to \ngenerate graphs with negative samples. This process is \nkeeping the adjacency matrix unchanged, and randomly \nscrambling the characteristic matrix by row.  \nIn this paper, a discriminator is used to measure noise by \ncontrast, and we use the binary cross-entropy to calculate \nloss. The discriminator can accurately differentiate \nbetween negative and positive samples to enhance the JS \ndivergence which increases the mutual information \nbetween local feature representation and global graph \nrepresentation. The following formula expresses the \nabove process: \n(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n,\n,?\n1\n1\n1\nlog\n,\nlog 1\n,\n=\n=\n\n\n\n\n\n\n=\n+\n−\n\n\n\n\n\n\n\n\n+\n\n\n\n\nN\nM\nj\ni\ni\nj\nx t\nx\nt\nN\nM\nX A\nX A\n (9) \n4 Experimental results and analysis \n4.1 Datasets \nThis paper utilizes Cora and Citeseer datasets[29]to \nevaluate the proposed GAMMI algorithm. In datasets, the \nnodes represent literature and the edges represent the \nconnection between two adjacent nodes. Table I shows an \noverview of the datasets. \nTable I Summary of the datasets used in experiments. \n \nCora \nCiteseer \nNode \n2708 \n3327 \nEdges \n5429 \n4732 \nFeatures/Node \n1433 \n3703 \nClasses \n7 \n6 \nTraining Nodes \n140 \n120 \nValidation Nodes \n500 \n500 \nTest Nodes \n1000 \n1000 \n4.2 Experiment 1: Comparative experimental \nanalysis \nIn the experiment, accuracy, Macro-F1 and recall are \nadopted to evaluate the performance of GAMMI method \nfor scientific literature classification tasks.  \nThe comparative experimental results are shown in Table \nII, which indicates that GAMMI is very effective. On all \ndatasets, the performance of GAMMI is the best of all \nmethods. On Cora and Cites, the performance of GAMMI \nis 1.6% and 1.6% better than that of GCN respectively, \nProceedings of CCIS2022 \n \nwhich shows that it is effective to differentiate weights to \nnodes in the same neighborhood. The main reason for this \nresult is that the weighted summation of nearby node \nfeatures makes the weights of adjacent node features \nentirely depend on the node features and independent of \nthe graph structure., while GCN is limited to two levels \nof neighborhood. \nTable II: Comparative experimental results. \nModel \nCore \nCiteseer \nAccu\nracy \nMacr\no-F1 \nRecall \nAccur\nacy \nMacro\n-F1 \nRecall \nDeep\nWalk \n0.673 \n0.652 \n0.667 \n0.443 \n0.416 \n0.423 \nMLP \n0.561 \n0.515 \n0.503 \n0.465 \n0.455 \n0.451 \nGAE \n0.783 \n0.756 \n0.758 \n0.578 \n0.569 \n0.562 \nVGA\nE \n0.775 \n0.767 \n0.764 \n0.564 \n0.557 \n0.533 \nGCN \n0.815 \n0.781 \n0.788 \n0.703 \n0.686 \n0.632 \nMoNe\nt \n0.811 \n0.779 \n0.787 \n0.691 \n0.672 \n0.611 \nGraph\nSAGE \n0.819 \n0.780 \n0.786 \n0.713 \n0.695 \n0.655 \nGAM\nMI  \n0.831 \n0.801 \n0.799 \n0.719 \n0.708 \n0.687 \n4.3 Experiment 2: Ablation experimental analysis \nThe GAMMI model consists of two components, one is a \ngraph attention encoder, and the other is based on a \ncontrastive learning model based on maximize mutual \ninformation.  \nTable Ⅲ Ablation experimental analysis. \nModel \nCore \nCiteseer \nAccur\nacy \nMacro\n-F1 \nRecall \nAccur\nacy \nMacr\no-F1 \nRecall \nGAM\nMI-\nattenti\non \n0.825 \n0.789 \n0.786 \n0.671 \n0.653 \n0.621 \nGAM\nMI- \ncontra\nstive \n0.731 \n0.687 \n0.653 \n0.462 \n0.398 \n0.387 \nGAM\nMI \n0.831 \n0.801 \n0.799 \n0.719 \n0.708 \n0.687 \nIn order to verify the effectiveness of each component, \nthese two components are removed separately, GAMMI-\nattention only uses the graph attention encoder to learn \nthe semantic representation of scientific literature data, \nand \nGAMMI-contrastive \nmaximizes \nthe \nmutual \ninformation of positive and negative samples based on \ncontrastive learning, so as to carry out the semantic \nrepresentation of scientific literature data. Then we \nevaluate the performance of different variants. Table Ⅲ \nshows the main results. According to the experimental \nresults, it is finally found that GAMMI performs better \nthan \nGAMMI-attention \nand \nGAMMI-contrastive, \nproving that these components are effective. \n5 Conclusions \nThis \npaper \nproposes \nunsupervised \nsemantic \nrepresentation learning model of scientific literature \nbased on graph attention mechanism and maximum \nmutual information (GAMMI). Through the introduction \nof graph attention mechanism, different weights are given \nto the nodes, and the characteristics of adjacent nodes are \nweighted and summed. In addition, an unsupervised \ngraph contrastive learning strategy is utilized. By \ncomparing the mutual information between the positive \nand negative local node representations on the latent \nspace and the global graph representation, the graph \nneural network can capture both local and global \ninformation. Experimental results indicates that GAMMI \nachieves competitive performance in scientific literature \nclassification tasks, sometimes even better than some \nsupervised architectures.  \nAcknowledgements \nThis work was supported by the National Natural Science \nFoundation \nof \nChina \n(No.62192784, \nNo.62172056, \nNo.61877006). \nReferences \n[1] \nR. M. Aidi Ahmi, “Bibliometric analysis of global \nscientific literature on web accessibility,” Nternational \nJournal of Recent Technology and Engineering (IJRTE), \nvol. 7, no. 6, pp. 250–258, 2019. \n[2] \nAng Li, Junping Du, Feifei Kou, Zhe Xue, Xin Xu, \nMingying Xu, Yang Jiang. “Scientific and Technological \nInformation Oriented Semantics-adversarial and Media-\nadversarial Cross-media Retrieval,” arXiv preprint \narXiv:2203.08615, 2022. \n[3] \nM. Ionescu, A. Sterca, and I. Badarinza, “Syntactic \nindexes for text retrieval,” IT in Industry, International \nJournal of Computer Applications, vol. 5, 2017. \n[4] \nD. Yan, K. Li, S. Gu, and L. Yang, “Network-based bag-\nof-words model for text classification,” IEEE Access, vol. \n8, pp. 82641–82652, 2020. \n[5] \nS. Sulova, L. Todoranova, B. Penchev, and R. Nacheva, \n“Using text mining to classify research papers,” in 17th \nInternational Multidisciplinary Scientific GeoConference \nSGEM 2017, 2017. \n[6] \nMingxing Li, Yinmin Jia, and Junping Du. “LPV control \nwith decoupling performance of 4WS vehicles under \nvelocity-varying motion,” IEEE Transactions on Control \nSystems Technology 2014, 22(5): 1708-1724. \n[7] \nZeli Guan, Yawen Li, Zhe Xue, Yuxin Liu, Hongrui Gao, \nYingxia Shao. “Federated Graph Neural Network for \nCross-graph Node Classification,” In 2021 IEEE 7th \nInternational Conference on Cloud Computing and \nIntelligent Systems, 418-422, 2021 \n[8] \nWenling Li, Yingmin Jia, and Junping Du. “Distributed \nextended Kalman filter with nonlinear consensus estimate,” \nJournal of the Franklin Institute, 2017, 354(17): 7983-\n7995. \n[9] \nWenling Li, Yingmin Jia, and Junping Du. “Tobit Kalman \nfilter with time-correlated multiplicative measurement \nnoise,” IET Control Theory & Applications, 2016, 11(1): \n122-128. \n[10] O. Shahmirzadi, A. Lugowski, and K. Younge, “Text \nsimilarity in vector space models: a comparative study,” \nin 2019 18th IEEE international conference on machine \nlearning and applications (ICMLA). IEEE, 2019, pp. 659–\n666. \nProceedings of CCIS2022 \n \n[11] Zeyu Liang, Junping Du, and Chaoyang Li. “Abstractive \nsocial media text summarization using selective \nreinforced Seq2Seq attention model,” Neurocomputing, \n410 (2020): 432-440. \n[12] C. Sun, X. Qiu, Y. Xu, and X. Huang, “How to fine-tune \nbert for text classification?” in China national conference \non Chinese computational linguistics. \nSpringer, 2019, \npp. 194–206. \n[13] Wenling Li, Jian Sun, Yingmin Jia, Junping Du, and \nXiaoyan Fu. “Variance-constrained state estimation for \nnonlinear complex networks with uncertain coupling \nstrength,” Digital Signal Processing, 2017, 67: 107-115. \n[14] Liang Xu, Junping Du, Qingping Li. “Image fusion based \non nonsubsampled contourlet transform and saliency-\nmotivated pulse coupled neural networks,” Mathematical \nProblems in Engineering, 2013.  \n[15] Wenling Li, Yingmin Jia, Junping Du. “Distributed \nconsensus extended Kalman filter: a variance-constrained \napproach,” IET Control Theory & Applications, 11(3): \n382-389, 2017.   \n[16] Deyuan Meng, Yingmin Jia, and Junping Du. “Consensus \nseeking via iterative learning for multi-agent systems with \nswitching topologies and communication time-delays,” \nInternational Journal of Robust and Nonlinear Control, \n2016, 26(17): 3772-3790.   \n[17] W. Wang, Z. Gan, W. Wang, D. Shen, J. Huang, W. Ping, \nS. Satheesh, and L. Carin, “Topic compositional neural \nlanguage model,” in International Conference on Artificial \nIntelligence and Statistics. PMLR, 2018, pp. 356–365. \n[18] Peng Lin, Yingmin Jia, Junping Du, Fashan Yu. “Average \nconsensus for networks of continuous-time agents with \ndelayed information and jointly-connected topologies,” \n2009 American Control Conference, 2009: 3884-3889.   \n[19] Deyuan Meng, Yingmin Jia, Junping Du, and Fashan Yu, \n“Tracking Algorithms for Multiagent Systems,” In IEEE \nTransactions on Neural Networks and Learning Systems, \n2013, 24(10): 1660-1676. \n[20] Yawen Li, Ye Yuan, Yishu Wang, Xiang Lian, Yuliang Ma, \nGuoren Wang. Distributed Multimodal Path Queries. \nIEEE Transactions on Knowledge and Data Engineering, \n34(7):3196-321, 2022. \n[21] Yawen Li, Guangcan Tang, Jiameng Du, Nan Zhou, Yue \nZhao, Tian Wu. Multilayer perceptron method to estimate \nreal-world fuel consumption rate of light duty vehicles. \nIEEE Access, 7, 63395-63402, 2019. \n[22] Feifei Kou, Junping Du, Congxian Yang, Yansong Shi, \nWanqiu Cui, Meiyu Liang, and Yue Geng. “Hashtag \nrecommendation based on multi-features of microblogs,” \nJournal of Computer Science and Technology, 2018, 33(4): \n711-726. \n[23] Yawen Li, Isabella Yunfei Zeng, Ziheng Niu, Jiahao Shi, \nZiyang Wang and Zeli Guan, “Predicting vehicle fuel \nconsumption based on multi-view deep neural network,” \nNeurocomputing, 502:140-147, 2022. \n[24] Yawen Li, Fang Liu, Tiannan Zhang, Fang Xu, Yuchen \nGao, Tian Wu.. Artificial intelligence in pediatrics. \nChinese Medical Journal, 133(03), 358-360, 2020. \n[25] S. Wu, F. Sun, W. Zhang, X. Xie, and B. Cui, “Graph \nneural networks in recommender systems: a survey,” \nACM Computing Surveys (CSUR), 2020. \n[26] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. \nPhilip, “A comprehensive survey on graph neural \nnetworks,” IEEE transactions on Neural Networks and \nLearning Systems, vol. 32, no. 1, pp. 4–24, 2020. \n[27] Yawen Li, Di Jiang, Rongzhong Lian, Xueyang Wu, \nConghui Tan, Yi Xu, Zhiyang Su. “Heterogeneous Latent \nTopic Discovery for Semantic Text Mining,” IEEE \nTransactions on Knowledge and Data Engineering, 2021. \n[28] R. Yin, K. Li, G. Zhang, and J. Lu, “A deeper graph neural \nnetwork for recommender systems,” Knowledge-Based \nSystems, vol. 185, p. 105020, 2019. \n[29] M. Khosla, V. Setty, and A. Anand, “A comparative study \nfor unsupervised network representation learning,” IEEE \nTransactions on Knowledge and Data Engineering, vol. 33, \nno. 5, pp. 1807–1818, 2019. \n[30] S. Thavareesan and S. Mahesan, “Sentiment lexicon \nexpansion using word2vec and fasttext for sentiment \nprediction in tamil texts,” in 2020 Moratuwa Engineering \nResearch Conference (MERCon). IEEE, 2020, pp. 272–\n276. \n[31] D. N. Popa, J. Perez, J. Henderson, and E. Gaussier, \n“Implicit discourse relation classification with syntax-\naware contextualized word representations,” in The \nThirty-Second International Flairs Conference, 2019. \n[32] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: \nPre-training of deep bidirectional transformers for \nlanguage \nunderstanding,” \narXiv \npreprint \narXiv:1810.04805, 2018. \n[33] I. Beltagy, K. Lo, and A. Cohan, “Scibert: A pretrained \nlanguage model for scientific text,” arXiv preprint \narXiv:1903.10676, 2019. \n[34] Xunpu Yuan, Yawen Li, Zhe Xue, Feifei Kou. “Financial \nsentiment analysis based on pre-training and textcnn,” \nChinese Intelligent Systems Conference, 48-56, 2020. \n[35] R. Wang, Z. Li, J. Cao, T. Chen, and L. Wang, \n“Convolutional recurrent neural networks for text \nclassification,” in 2019 International Joint Conference on \nNeural Networks (IJCNN). IEEE, 2019, pp. 1–6. \n[36] Petar Veličković, Guillem Cucurull, Arantxa Casanova et \nal. Graph Attention Networks. In ICLR. 2018. \n[37] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. \nChenaghlu, and J. Gao, “Deep learning– based text \nclassification: \na \ncomprehensive \nreview,” \nACM \nComputing Surveys (CSUR), vol. 54, no. 3, pp. 1–40, \n2021. \n[38] Yingxia Shao, Shiyue Huang, Yawen Li, Xupeng Miao, \nBin Cui, Lei Chen. Memory-aware framework for fast and \nscalable second-order random walk over billion-edge \nnatural graphs. \n[39] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. \nPhilip, “A comprehensive survey on graph neural \nnetworks,” IEEE transactions on Neural Networks and \nLearning Systems, vol. 32, no. 1, pp. 4–24, 2020. \n[40] K. Ethayarajh, “Unsupervised random walk sentence \nembeddings: A strong but simple baseline,” in \nProceedings of the Third Workshop on Representation \nLearning for NLP, 2018, pp. 91–100. \n[41] Nan Z, Junping Du, Xu Yao, et al. “Microblog Topic \nContent Search Method Based on Convolutional Neural \nNetworks”. Journal of Frontiers of Computer Science & \nTechnology, 2019, 13(5): 753. \n[42] R. Sato, “A survey on the expressive power of graph \nneural networks,” arXiv preprint arXiv:2003.04078, 2020. \n[43] Yuning You, Tianlong Chen, Yongduo Sui et al. Graph \nContrastive Learning with Augmentations. In NeurIPS. \n2020.  \n",
  "categories": [
    "cs.IR"
  ],
  "published": "2022-10-07",
  "updated": "2023-01-30"
}