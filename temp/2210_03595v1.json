{
  "id": "http://arxiv.org/abs/2210.03595v1",
  "title": "Unsupervised Few-shot Learning via Deep Laplacian Eigenmaps",
  "authors": [
    "Kuilin Chen",
    "Chi-Guhn Lee"
  ],
  "abstract": "Learning a new task from a handful of examples remains an open challenge in\nmachine learning. Despite the recent progress in few-shot learning, most\nmethods rely on supervised pretraining or meta-learning on labeled\nmeta-training data and cannot be applied to the case where the pretraining data\nis unlabeled. In this study, we present an unsupervised few-shot learning\nmethod via deep Laplacian eigenmaps. Our method learns representation from\nunlabeled data by grouping similar samples together and can be intuitively\ninterpreted by random walks on augmented training data. We analytically show\nhow deep Laplacian eigenmaps avoid collapsed representation in unsupervised\nlearning without explicit comparison between positive and negative samples. The\nproposed method significantly closes the performance gap between supervised and\nunsupervised few-shot learning. Our method also achieves comparable performance\nto current state-of-the-art self-supervised learning methods under linear\nevaluation protocol.",
  "text": "Unsupervised Few-shot Learning via Deep Laplacian\nEigenmaps\nKuilin Chen\nUniversity of Toronto\nkuilin.chen@mail.utoronto.ca\nChi-Guhn Lee\nUniversity of Toronto\ncglee@mie.utoronto.ca\nAbstract\nLearning a new task from a handful of examples remains an open challenge in machine learning. Despite\nthe recent progress in few-shot learning, most methods rely on supervised pretraining or meta-learning\non labeled meta-training data and cannot be applied to the case where the pretraining data is unlabeled.\nIn this study, we present an unsupervised few-shot learning method via deep Laplacian eigenmaps. Our\nmethod learns representation from unlabeled data by grouping similar samples together and can be intuitively\ninterpreted by random walks on augmented training data. We analytically show how deep Laplacian\neigenmaps avoid collapsed representation in unsupervised learning without explicit comparison between\npositive and negative samples. The proposed method signiﬁcantly closes the performance gap between\nsupervised and unsupervised few-shot learning. Our method also achieves comparable performance to\ncurrent state-of-the-art self-supervised learning methods under linear evaluation protocol.\n1\nIntroduction\nFew-shot learning (Fei-Fei et al., 2006) aims to learn a new classiﬁcation or regression model on\na novel task that is not seen during training, given only a few examples in the novel task. Existing\nfew-shot learning methods either rely on episodic meta-learning (Finn et al., 2017, Snell et al., 2017)\nor standard pretraining (Chen et al., 2019, Tian et al., 2020b) in a supervised manner to extract trans-\nferrable knowledge to a new few-shot task. Unfortunately, these methods require many labeled\nmeta-training samples. Acquiring a lot of labeled data is costly or even impossible in practice. Re-\ncently, several unsupervised meta-learning approaches have attempted to address this problem by\nconstructing synthetic tasks on unlabeled meta-training data (Hsu et al., 2019, Khodadadeh et al.,\n2019, 2021) or meta-training on self-supervised pretrained features (Lee et al., 2021a). However, the\nperformance of unsupervised meta-learning approaches is still far from their supervised counter-\nparts. Empirical studies in supervised pretraining show that representation learning via grouping\nsimilar samples together (Chen et al., 2019, Dhillon et al., 2020, Laenen and Bertinetto, 2021, Tian\net al., 2020b) outperforms a wide range of episodic meta-learning methods, where the deﬁnition\nof similar samples is given by class labels. The motivation of this study is to develop an unsuper-\nvised representation learning method by grouping unlabeled meta-training data without episodic\ntraining and close the performance gap between supervised and unsupervised few-shot learning.\nContrastive self-supervised learning has shown remarkable success in learning representation\nfrom unlabeled data, which is competitive with supervised learning on multiple visual tasks (Hé-\nnaff et al., 2020, Tian et al., 2020a). The common underlying theme behind contrastive learning\nis to pull together representation of augmented views of the same training sample (positive sam-\n1\narXiv:2210.03595v1  [cs.LG]  7 Oct 2022\nFigure 1: A graph on augmented views of unlabeled training data. The thickness of the edge\nindicates the transition probability between vertices, which is proportional to their similarity. We\ngroup similar vertices together by minimizing the total transition probability between different\ngroups.\nple) and disperse representation of augmented views from different training samples (negative\nsample) (Wang and Isola, 2020, Wu et al., 2018). Typically, contrastive learning methods require a\nlarge size of negative samples to learn high-quality representation from unlabeled data (Chen et al.,\n2020, He et al., 2020). This inevitably requires a large batch size of samples, demanding signiﬁ-\ncant computing resources. Non-contrastive methods try to overcome the issue by accomplishing\nself-supervised learning with only positive pairs. However, non-contrastive methods suffer from\ntrivial solutions where the model maps all inputs to the same constant vector, known as the col-\nlapsed representation. Various methods have been proposed to avoid collapsed representation on\nan ad hoc basis, such as asymmetric network architecture (Grill et al., 2020), stop gradient (Chen\n2\nand He, 2021), and feature decorrelation (Ermolov et al., 2021, Hua et al., 2021, Zbontar et al., 2021).\nHowever, theoretical understanding about how non-contrastive methods avoid collapsed represen-\ntation is limited, though some preliminary attempts are made to analyze the training dynamics of\nnon-contrastive methods (Tian et al., 2021). Besides, most self-supervised learning methods focus\non the linear evaluation task where the training and test data come from the same classes. They\ndo not account for the domain gap between training and test classes, which is the case in few-shot\nlearning and cross-domain few-shot learning.\nWe develop a novel unsupervised representation learning method in which a weighted graph\nis used to capture unlabeled samples as nodes and similarity among samples as the weights of\nedges. Two samples are deemed similar if they are augmentations of a single sample and cluster-\ning of samples is accomplished by partitioning the graph. We provide an intuitive understanding\nof the graph partition problem from the perspective of random walks on the graph, where the\ntransition probability between two vertices is proportional to their similarity. The optimal parti-\ntion can be found by minimizing the total transition probability between clusters. It is linked to\nthe well-known Laplacian eigenmaps in spectral analysis (Belkin and Niyogi, 2003, Meila and Shi,\n2000, Shi and Malik, 2000). We replace the locality-preserving projection in Laplacian eigenmaps\nwith deep neural networks for better scalability and ﬂexibility in learning high-dimensional data\nsuch as images.\nAn additional technique is integrated into deep Laplacian eigenmaps to handle the domain gap\nbetween the meta-training and meta-testing sets. Previous studies on word embeddings (e.g. king\n- man + woman ≈queen) (Mikolov et al., 2013) and disentangled generative models (Karras et al.,\n2019) show that interpolation between latent embeddings may correspond to the representation\nof a realistic sample, which may not be seen in the training data. In contrast, interpolation in the\ninput space does not result in realistic samples. In parallel, interpolation between the distributions\nof the nearest two meta-training classes in the embedding space can approximate the distribution\nof one meta-testing class after the feature extractor is trained (Yang et al., 2021). To enhance the\nperformance on downstream few-shot learning tasks, we make interpolation of unlabeled meta-\ntraining samples on data manifold to mimic unseen meta-test samples and integrate them into\nunsupervised training of the feature extractor.\nOur contributions are summarized as follows:\n• A new unsupervised few-shot learning method is developed based on deep Laplacian eigen-\nmaps with an intuitive explanation based on random walks.\n• Our loss function is analyzed to show how collapsed representation is avoided without ex-\nplicit comparison to negative samples, shedding light on existing feature decorrelation based\nself-supervised learning methods.\n• The proposed method signiﬁcantly closes the performance gap between unsupervised and\nsupervised few-shot learning methods.\n• Our method achieves comparable performance to current state-of-the-art (SOTA) self-\nsupervised learning methods under the linear evaluation protocol.\n2\nMethodology\n2.1\nGraph from augmented data\nFirst, we construct a graph using augmented views of unlabeled data. Let ¯x ∈Rd be a raw\nsample without augmentation. For image data, augmented views are created by the commonly\n3\nused augmentations deﬁned in SimCLR (Chen et al., 2020), including horizontal ﬂip, Gaussian\nblur, color jittering, and random cropping. X denotes the set of all augmented data and N = |X|.\nWe represent the augmented data in the form of a weighted graph G = (X, S), where each x ∈X is\na vertex of the graph and S denotes the edge weights. The edge between two vertices xi, xj ∈X is\nweighted by the non-negative similarity sij between them. For unrelated xi and xj, the similarity\nsij should be small. On the other hand, the similarity sij should be large when xi and xj are\naugmentations from the same image or augmentations from two images within the same latent\nclasses. An illustrative diagram is shown in Fig. 1.\nLet di = P\nxj∈X sij denote the total weights associated with xi, which is the degree of a vertex\nxi in a weighted graph. The degree matrix D is deﬁned as the diagonal matrix with the degrees\nd1, ..., dN on the diagonal. The volume of X is Vol(X) = P\nxi∈X di. Similarly, the volume of a\nsubset C ⊂X is deﬁned as Vol(C) = P\nxi∈C di. Let P = D−1S be the random walk Laplacian of G,\nwhere pij = (P)ij represents the transition probability from xi to xj, and each row of P sums to 1.\nL = D −S is the unnormalized Laplacian of G.\n2.2\nRandom walks and graph partition\nX can be grouped into K clusters, where similar vertices should be grouped into the same clus-\nter and dissimilar vertices should be grouped into different clusters. It resembles the supervised\npretraining by embedding samples from the same class together. We will show later that K is also\nthe dimension of the embedding. Since we do not know the number of classes in unlabeled training\ndata, we set the embedding dimension K = 2048, which works well on a wide range of datasets.\nGraph partition into clusters can be done by minimizing the total similarity P\nxi∈C,xj∈C′ sij between\ntwo clusters C, C′ ∈X, C ∩C′ = ∅. However, minimizing the total inter-cluster similarity is unde-\nsirable because it can simply separate one individual vertex from the rest of the graph. Instead, we\nrun random walks on vertices X.\nThe random walk Laplacian P deﬁnes a Markov chain on the vertices X. The stationary dis-\ntribution π of this chain has an explicit form πi = di/ Vol(X) for xi ∈X (Meila and Shi, 2000). The\ntransition probability P(C′ | C) = P(X1 ∈C′ | X0 ∈C) is given by\nP\n\u0000X1 ∈C′ | X0 ∈C\n\u0001\n=\n\n\n1\nVol(X)\nX\nxi∈C,xj∈C′\nsij\n\n\n\u0012Vol(C)\nVol(X)\n\u0013−1\n=\nP\nxi∈C,xj∈C′ sij\nVol(C)\n(1)\nThe inter-cluster transition probability is a proper criterion because it has a small value only if\nP\nxi∈C,xj∈C′ sij is small (low similarity for vertices in different clusters) and all clusters have sufﬁ-\nciently large volumes. As a result, minimization of the inter-cluster transition probability prevents\ntrivial solutions that simply separate one individual vertex from the rest of the graph.\nThe inter-cluster transition probability minimizing problem is a constrained optimization prob-\nlem. For the case of ﬁnding K clusters, we deﬁne a matrix Z ∈RN×K, where zik represents the\ncluster assignment of the vertex xi.\nzik =\n\u001a 1/\np\nvol (Ck)\nif xi ∈Ck\n0\notherwise\n(2)\n4\nwhere i = 1, . . . , N and k = 1, . . . , K. Let z(k) be the k-th column in the matrix Z. The connection\nbetween unnormalized graph Laplacian and inter-cluster transition probability is given by\nz⊤\n(k)Lz(k) = z⊤\n(k)(D −S)z(k)\n=\nN\nX\ni=1\ndiz2\nik −\nN\nX\ni=1\nN\nX\nj=1\nsijzikzjk\n=1\n2\n\n\nN\nX\ni=1\ndiz2\nik −2\nN\nX\ni=1\nN\nX\nj=1\nsijzikzjk +\nN\nX\nj=1\ndjz2\njk\n\n\n=1\n2\n\n\nN\nX\ni=1\nN\nX\nj=1\nsijz2\nik −2\nN\nX\ni=1\nN\nX\nj=1\nsijzikzjk +\nN\nX\nj=1\nN\nX\ni=1\nsjiz2\njk\n\n\n=1\n2\nN\nX\ni,j=1\nsij (zik −zjk)2\n=1\n2\nX\ni∈Ck,j∈¯Ck\nsij\n s\n1\nVol(Ck)\n!2\n= 1\n2P( ¯Ck | Ck)\n(3)\nIt is easy to verify that Z⊤DZ = I, and z⊤\n(k)Dz(k) = 1. We can write the problem of minimizing\ninter-cluster transition as\nmin\nC1,...,CK\nTr\n\u0010\nZ⊤LZ\n\u0011\nsubject to\nZ⊤DZ = I\nzik = 1/\np\nvol (Ck) if xi ∈Ck else 0\n(4)\nThis problem is NP-hard due to discreteness. Relaxing the discreteness condition, we obtain the\nrelaxed problem\nmin\nZ∈RN×K Tr\n\u0010\nZ⊤LZ\n\u0011\nsubject to Z⊤DZ = I\n(5)\nThis is the standard trace minimization problem which is solved when the column space of Z is the\nsubspace of the K generalized eigenvectors of Lz = λDz. The relaxed problem in Eq. (5) leads to\nthe lower bound on the optimal normalized cut of the graph (??) and retains the interpretation of\nminimizing the transition probability between clusters. In addition, such relaxation has asymptotic\nbehavior when the number of data points tends to inﬁnity (?). As such, rounding Z leads to cluster\nindicator because the relaxed problem is a good proxy of the original problem in Eq. (4) (Bach and\nJordan, 2006). We actually would not round the continuous Z as our goal is not clustering. We will\nlearn a linear classiﬁer on Z in the downstream tasks.\n2.3\nDeep representation learning\nLet zi ∈RK be the i-th row of the matrix Z. zi can serve as desirable representation of xi\nas it exhibits the clustering structure of the graph G. However, it is not sensible to obtain zi by\ngeneralized eigenvalue decomposition for two reasons. First, computation of eigenvectors may\nbe prohibitively expensive due to the large size of augmented data X. Second, it is non-trivial to\ncompute the embeddings for unseen data points in meta-test classes because eigenvalue decompo-\nsition is non-parametric (one K-dimensional vector z is computed for each x in the training data).\n5\nWe assume that z is parametrized by fθ(x), where fθ can be deep neural networks with trainable\nparameters θ. The relaxed problem in Eq. (5) is converted to\nmin\nθ\nTr\n\u0010\nZ⊤LZ\n\u0011\nsubject to Z⊤DZ = I\n(6)\nwhere Z = fθ(X). We design a proper loss function to learn θ that solves the constrained optimiza-\ntion problem in Eq. (6). The trace minimizing can be written as\nTr\n\u0010\nZ⊤LZ\n\u0011\n=\nX\nxi,xj∈X\n\u0000sij∥f(xi) −f(xj)∥2\u0001\n(7)\nwhere the summation is taken w.r.t. pairs (xi, xj) drawn i.i.d. from X. Note that the similarity sij\nfor an unrelated pair (xi, xj) should be negligibly small, compared to the similarity of a related pair\n(xi, xj). Therefore, the weighted summation in Eq. (7) can be approximated by summation of the\nEuclidean distance between the representation of positive pairs within a mini-batch Vbatch\nLtrace = Ez,z+∈Vbatch∥z −z+∥2\n(8)\nwhere z and z+ are representation of augmented views from the same image.\nThe constraint\nZ⊤DZ = I requires the covariance matrix of the representation to be a diagonal matrix. It is equiv-\nalent to minimizing the mean squared error on off-diagonal entries of the covariance matrix\nLconst =\nK\nX\nk=1\nK\nX\nl̸=k\n(ckl)2\n(9)\nwhere ckl = P\nz∈Vbatch(z)k(z)l/B is the covariance between the k-th and l-th dimensions of the\nfeature, B is the size of the mini-batch, and (·)k denotes the k-th element of a vector.\nThe total loss is\nL = Ltrace + γLconst\n(10)\nwhere Ltrace comes from trace minimization, Lconst is translated from the constraint on eigenvectors,\nand γ can be treated as a Lagrange multiplier.\nOur method does not require asymmetric twin neural networks, large batch size, large mem-\nory bank, or momentum update. It naturally avoids the trivial solution via the orthogonality con-\nstraint, which is realized by decorrelating each dimension of the representation. In Section 3, we\nwill provide a more detailed analysis of how collapsed representation is avoided without explicit\ncomparison between positive and negative samples in the loss function.\n2.4\nInterpolation between unlabeled training samples\nThe representation of the augmented views can be encoded as z = fθ(x) = fL(gL(x)), where\ngL is part of the feature extractor from the input layer to the hidden layer L and fL is the remaining\npart of the feature extractor to the output layer.The hidden layer L is randomly selected from a set of\neligible layers in the neural network fθ so that we can interpolate two samples on their intermediate\nrepresentation. Let xi and xi+ be a positive pair. We interpolate intermediate representation gL(xi+)\nand gL(xj+) to get the manifold mixup (Verma et al., 2019) as follows\nzij+ = fL(λgL(xi+) + (1 −λ)gL(xj+))\n(11)\nwhere λ ∈[0, 1] is a mixing coefﬁcient drawn from a Beta distribution and zij+ is the mixed rep-\nresentation of xi+ and xj+ after interpolation on the data manifold. zij+ should match the inter-\npolated representation of zi and zj in the embedding space with the same mixing coefﬁcient. The\ntrace minimization loss in Eq. (8) can be replaced by\nLtrace = Ez,z+∈Vbatch∥(λzi + (1 −λ)zj) −zij+∥2\n(12)\n6\nFigure 2: Interpolation of unlabeled training data on data manifold.\nAn illustrative diagram can be found in Fig. 2. The pseudo-code is presented in Algorithm 1.\nAlgorithm 1 Pseudo-code of deep Laplacian eigenmaps in a PyTorch-like style.\n# B\n: batch size\n# K\n: representation dim\n# d_x\n: input dim\n# x\n: Tensor, shape=[B, d_x]\n#\naugmented views\n# x_p\n: Tensor, shape=[B, d_x]\n#\npositive pairs of x\n# z\n: Tensor, shape=[B, K]\n#\nrepresentation of augmented views\n# z_p\n: Tensor, shape=[B, K]\n#\npositive pairs of z\n# gamma\n: hyperparameter balancing the two losses\n# lambda\n: interpolation coefficient\n# f_theta : feature extractor\n# L\n: an randomly selected layer\n# g_L\n: first L layers of f_theta\n# f_L\n: remaining layers of f_theta\n# off_diag:\noff-diagonal elements of a matrix\ndef similarity(z, z_p):\nreturn (z - z_p).pow(2).mean()\ndef decorrelation(z):\nc = z.T @ z / B\ndecorr_loss = off_diag(c).pow(2).sum()\nz = f_theta(x)\npermuted_index = randperm(B)\nmanifold_mix = lambda * g_L(x_p)\n+ (1 - lambda) * g_L(x_p[permuted_index])\nz_p_mix = f_L(manifold_mix)\nz_mix = lambda * z + (1 - lambda) * z[permuted_index]\nloss = similarity(z_mix, z_p_mix) + gamma * decorrelation(z)\n3\nUnravel feature decorrelation\nWe analyze the feature decorrelation loss in Eq. (9) and show that feature decorrelation is\nindeed mathematically equivalent to contrasting between positive and negative samples. At ﬁrst\nglance, the off-diagonal entries have nothing to do with the inner product between the represen-\ntation of positive and negative samples. Previous works (Hua et al., 2021, Zbontar et al., 2021)\nprovide qualitative and empirical analysis to show that feature decorrelation avoids collapsed rep-\nresentation. To the best of our knowledge, we are the ﬁrst to shed light on the equivalence and\nanalyze the gradient of the feature decorrelation loss.\n7\nWhen each dimension of Z is standardized to zero mean and unit variance (due to the ﬁnal\nbatch normalization layer), the diagonal entries of the covariance matrix C become constant cii = 1.\nMinimizing the square of off-diagonal entries in Eq. (9) is equivalent to minimizing the Frobenius\nnorm of the covariance matrix,\n∥C∥2\nF =\n1\nN2 ∥Z⊤Z∥2\nF =\n1\nN2 tr\n\u0012\nZ⊤Z\n\u0010\nZ⊤Z\n\u0011⊤\u0013\n=\n1\nN2 tr\n\u0010\nZZ⊤ZZ⊤\u0011\n=\n1\nN2 tr\n\u0012\u0010\nZZ⊤\u0011⊤\nZZ⊤\n\u0013\n=\n1\nN2\nN\nX\ni=1\nN\nX\nj=1\n\u0010\u0010\nZZ⊤\u0011\n◦\n\u0010\nZZ⊤\u0011\u0011\nij\n=\n1\nN2\nN\nX\ni=1\nN\nX\nj=1\n\u0010\nfθ(xi)⊤fθ(xj)\n\u00112\n(13)\nwhere ◦denotes the Hadamard product. The second line of Eq. (13) is based on the cyclic property\nof the trace operation and the fact that ZZ⊤is symmetric. After rewriting the trace operation via\nthe Hadamard product, we have the ﬁnal expression, showing that the squared Frobenius norm of\nthe covariance matrix can be expressed as a summation over the squared inner product between\npairs of representation. Note that zi = fθ(xi) is usually projected to a sphere ball with a radius of 1\nin self-supervised learning. The inner product between the representation of the same augmented\nview is constant z⊤\ni zi = 1. Minimizing the square of off-diagonal entries in Eq. (9) is equivalent to\nminimizing the total squared cosine similarity between random pairs.\nAfter demystifying the feature decorrelation loss, we can analyze the gradient of our loss func-\ntion to show how it pulls similar samples together and pushes dissimilar samples apart. Let zi be\nthe anchor sample, zi+ be a positive sample, and zj be an unspeciﬁed sample in the current batch\nVbatch. The gradient with respect to the anchor sample is given by\n∂L\n∂zi\n= 2\nB\n\n(1 −γ) zi −zi+ + γ\nX\nzj∈Vbatch\nz⊤\ni zj\nB zj\n\n\n(14)\nwhere B is the size of the mini-batch. Since γ is a small positive number, the gradient can be further\nsimpliﬁed as ∂L\n∂zi =\n2\nB\n\u0010\n(zi −zi+) + γ P\nzj∈Vbatch ωjzj\n\u0011\n, where ωj is a weighting factor for negative\nsamples which is proportional to the similarity between the positive and negative samples. The\nﬁrst term of the gradient pulls positive pairs together while the second term disperses the negative\nsamples.\n4\nRelated Work\nFew-shot learning is cast to optimization-based (Antoniou et al., 2018, Bertinetto et al., 2019,\nFinn et al., 2017, Flennerhag et al., 2020, Lee and Choi, 2018, Nichol et al., 2018, Park and Oliva,\n2019, Ravi and Larochelle, 2017, Rusu et al., 2019) or metric-based (Koch et al., 2015, Oreshkin et al.,\n2018, Qi et al., 2018, Snell et al., 2017, Sung et al., 2018, Vinyals et al., 2016, Yoon et al., 2019, 2020)\nmeta-learning problems through supervised episodic training because it mimics the circumstances\nencountered in few-shot learning. The model is trained by a series of learning episodes, each of\nwhich consists of a limited number of support (training) samples and query (validation) samples.\n8\nNevertheless, simple baselines can outperform SOTA episodic meta-learning methods by using em-\nbeddings pre-trained with standard supervised learning (Chen et al., 2019, Dhillon et al., 2020, Lae-\nnen and Bertinetto, 2021, Mangla et al., 2020, Tian et al., 2020b). Although episodic meta-learning\nmethods can be applied to unlabeled meta-training data by constructing synthetic tasks (Hsu et al.,\n2019, Khodadadeh et al., 2019, 2021) or modeling the multi-modality within each randomly sam-\npled episode (Lee et al., 2021a), the performance is much worse than the supervised counterparts.\nDifferent from established few-shot learning methods, our method learns useful representation for\ndownstream few-shot learning tasks using unlabeled meta-training data without episodic training.\nContrastive learning with variants of InfoNCE loss (Gutmann and Hyvärinen, 2010, Oord\net al., 2018) has been widely used in self-supervised/unsupervised representation learning (Chen\net al., 2020, He et al., 2020, Hénaff et al., 2020, Wu et al., 2018). It is derived from the maximiza-\ntion of the mutual information (MI) between related views (Poole et al., 2019). However, this\ninterpretation could be inconsistent with some empirical observations in self-supervised learn-\ning, such as tighter lower bounds of MI leading to worse performance (McAllester and Stratos,\n2020, Tschannen et al., 2020, Wang and Isola, 2020).\nThe InfoNCE loss can be expressed as\nLInfoNCE = −P\ni log\nexp(z⊤\ni zi+/τ)\nP\nzj ∈V exp(z⊤\ni zj/τ), where V can be a mini-batch or a memory bank. The core\nidea of contrastive learning is pulling positive pairs together while pushing negative samples apart\n(Wang and Isola, 2020). It can be easily veriﬁed from the gradient ∂LInfoNCE\n∂zi\n= (−zi++P\nzj∈V ωjzj)/τ,\nwhere ωj =\nexp(z⊤\ni zj/τ)\nP\nzj ∈V exp(z⊤\ni zj/τ) is a weighting factor that is proportional to the similarity between the\nanchor sample zi and the negative sample zj. Our method shares a similar form of the gradient\nwith a different weighting scheme on negative samples, though our loss function is derived from a\ndifferent perspective. Recent studies show that weighting schemes on negative samples affect the\nlearning efﬁciency with respect to the negative sample size (Wang and Liu, 2021, Yeh et al., 2021).\nCompared with contrastive learning with InfoNCE loss, our method does not require a large size\nof negative samples to work well.\nClustering methods have been employed in self-supervised learning (Asano et al., 2019, Caron\net al., 2018, 2020) by simultaneously clustering the unlabeled data while enforcing consistent cluster\nassignments for different augmented views of the same image. These methods do not compare pos-\nitive and negative samples directly as in contrastive learning, but careful implementation details\nand large batches are necessary because clustering methods are prone to collapse. The derivation\nof our method resembles spectral clustering but our method does not perform clustering. Unsu-\npervised representation learning via deep Laplacian eigenmaps can be intuitively interpreted by\nrandom walks on augmented views of unlabeled data and use deep neural networks to handle\nhigh-dimensional image data. Random walks on image pixels have been developed to solve a su-\npervised image segmentation problem by minimizing the Kullback-Leibler divergence between the\nlearned transition probability and the target transition probability (Meila and Shi, 2000). If fθ is a\nlinear function, the linear projection is locality preserving (He and Niyogi, 2004).\nFeature decorrelation methods avoid collapsed representation in self-supervised learning\nwithout using a large number of negative samples. Feature decorrelation can be achieved by dif-\nferentiable Cholesky decomposition on each batch of embeddings (Ermolov et al., 2021), forcing\nthe cross-correlation matrix of representations close to the identity matrix (Zbontar et al., 2021), or\nutilization of decorrelated batch normalization with a shufﬂing operation (Hua et al., 2021). Fea-\nture decorrelation methods show comparable performance to contrastive learning methods, but the\nfundamental principle behind it is unclear. Although Barlow Twins (Zbontar et al., 2021) are de-\nrived from the information bottleneck principle, it is only valid for Gaussian distributed data. The\n9\nloss function in our method is similar to those in decorrelation methods. However, our method\nis derived from the well-known spectral analysis of the Laplacian matrix and requires minimal\nassumptions about the training data. Existing feature decorrelation methods in self-supervised\nlearning can be uniﬁed in our framework - a trace minimization problem with orthogonality con-\nstraints, with minor differences in handling orthogonality constraints in practice. We also show the\nexact reason why feature decorrelation avoids collapsed representation.\nMixup (Zhang et al., 2018) and its variants (Verma et al., 2019, Yun et al., 2019) provide ef-\nfective data augmentation strategies to improve performance in supervised learning. Mixing up\nthe image pixels has been explored in self-supervised learning (Lee et al., 2021b, Shen et al., 2022).\nMoChi (Kalantidis et al., 2020) mixes the ﬁnal representation of negative samples to create hard neg-\native samples. Our method mixes up the intermediate representation to achieve better empirical\nperformance.\n5\nExperiments\nWe evaluate the performance of our model trained on unlabeled meta-training data on few-\nshot learning tasks, including in-domain and more challenging cross-domain settings. In addition,\nour method is also compared with SOTA self-supervised learning method under linear evaluation\nprotocol to show that the proposed method can be applied to a wide range of downstream tasks\nbeyond few-shot learning.\n5.1\nFew-shot classiﬁcation\nWe conduct few-shot classiﬁcation experiments on three widely used few-shot image recogni-\ntion benchmarks.\nminiImageNet is a 100-class subset of the original ImageNet dataset (Deng et al., 2009) for\nfew-shot learning (Vinyals et al., 2016). miniImageNet is split into 64 training classes, 16 valida-\ntion classes, and 20 testing classes, following the widely used data splitting protocol (Ravi and\nLarochelle, 2017).\nFC100 is a derivative of CIFAR-100 with minimized overlapped information between train\nclasses and test classes by grouping the 100 classes into 20 superclasses (Oreshkin et al., 2018). They\nare further split into 60 training classes (12 superclasses), 20 validation classes (4 superclasses), and\n20 test classes (4 superclasses).\nminiImageNet to CUB is a cross-domain few-shot classiﬁcation task, where the models are\ntrained on miniImageNet and tested on CUB (Welinder et al., 2010). Cross-domain few-shot clas-\nsiﬁcation is more challenging due to the big domain gap between two datasets. We can better\nevaluate the generalization capability in different algorithms. We follow the experiment setup in\n(Yue et al., 2020).\nThe feature extractor fθ contains two components: a backbone network and a projection net-\nwork. The backbone network can be a variant of ResNet architecture (He et al., 2016). The projection\nnetwork is a 3-layer MLP with batch normalization and ReLU activation. The dimension of each\nlayer in the projection MLP is 2048. We use the same augmentations deﬁned in SimCLR (Chen\net al., 2020), including horizontal ﬂip, Gaussian blur, color jittering, and random cropping.\nWe use ResNet12 (Lee et al., 2019, Ravichandran et al., 2019) and WRN-28-10 (Yue et al., 2020)\nas the backbone networks for few-shot learning and cross-domain few-shot learning, respectively.\nThose two backbone networks are selected because they are widely used in SOTA few-shot learning\nmethods. The feature extractor is trained on unlabeled meta-training data by the SGD optimizer\n10\n(momentum of 0.9 and weight decay of 5e-4) with a mini-batch size of 128. The learning rate starts\nat 0.05 and decreases to 0 with a cosine schedule. The projection network in the feature extractor is\ndiscarded after training on unlabeled data.\nDuring meta-testing, we train a regularized logistic regression model using 1 × 5 or 5 × 5\nsupport samples on frozen representations after the global average pooling layer in the backbone\nnetwork. Each few-shot task contains 5 classes and 75 query samples. The classiﬁcation accuracy\nis evaluated on the query samples.\nTable 1: Few-shot classiﬁcation results on miniImageNet and FC100.\nMethod\nBackbone\nminiImageNet 5-way\nFC100 5-way\n1-shot\n5-shot\n1-shot\n5-shot\nSupervised\nProto Net (Snell et al., 2017)\nResNet-12\n60.37 ± 0.83\n78.02 ± 0.57\n41.5 ± 0.7\n57.0 ± 0.7\nMAML (Finn et al., 2017)\nResNet-12\n56.58 ± 1.84\n70.85 ± 0.91\n36.9 ± 0.6\n51.2 ± 0.7\nTADAM (Oreshkin et al., 2018)\nResNet-12\n58.50 ± 0.30\n76.70 ± 0.30\n40.1 ± 0.4\n56.1 ± 0.4\nBaseline++ (Chen et al., 2019)\nResNet-12\n60.83 ± 0.81\n77.81 ± 0.76\n41.3 ± 0.7\n58.7 ± 0.7\nMetaOptNet (Lee et al., 2019)\nResNet-12\n62.64 ± 0.61\n78.63 ± 0.46\n41.1 ± 0.6\n55.5 ± 0.6\nUnsupervised\nCACTUs-MAML (Hsu et al., 2019)\nResNet-12\n49.41 ± 0.92\n63.72 ± 0.83\n31.3 ± 0.8\n45.7 ± 0.8\nUMTRA (Khodadadeh et al., 2019)\nResNet-12\n49.62 ± 0.91\n62.43 ± 0.84\n31.5 ± 0.8\n45.3 ± 0.8\nMeta-GMVAE (Lee et al., 2021a)\nResNet-12\n55.93 ± 0.85\n74.28 ± 0.72\n36.3 ± 0.7\n49.7 ± 0.7\nSimCLR (Chen et al., 2020)\nResNet-12\n55.76 ± 0.88\n75.59 ± 0.69\n36.2 ± 0.7\n49.9 ± 0.7\nMoCo v2 (He et al., 2020)\nResNet-12\n57.73 ± 0.84\n77.51 ± 0.63\n37.7 ± 0.7\n53.2 ± 0.7\nBYOL (Grill et al., 2020)\nResNet-12\n56.17 ± 0.89\n76.17 ± 0.66\n37.2 ± 0.7\n52.8 ± 0.6\nBarlow Twins (Zbontar et al., 2021)\nResNet-12\n57.79 ± 0.89\n77.42 ± 0.66\n37.9 ± 0.7\n54.1 ± 0.6\nOurs\nResNet-12\n59.47 ± 0.87\n78.79 ± 0.58\n39.7 ± 0.7\n57.9 ± 0.7\nThe results of the proposed method and previous few-shot learning methods using similar\nbackbones are reported in Table 1. The proposed method outperforms existing unsupervised few-\nshot learning methods such as CACTUS (Hsu et al., 2019) and UMTRA (Khodadadeh et al., 2019)\nby a large margin. It demonstrates that high-quality representation for downstream few-shot tasks\ncan be learned from unlabeled meta-training data without episodic training. Our method is in the\ncategory of self-supervised representation learning like SimCLR (Chen et al., 2020), MoCo v2 (He\net al., 2020), BYOL (Grill et al., 2020), and Barlow Twins (Zbontar et al., 2021) as it does not perform\nepisodic learning. SimCLR achieves weaker performance than other self-supervised learning meth-\nods because it typically requires very large batch sizes to perform well. Although Meta-GMVAE\n(Lee et al., 2021a) performs unsupervised meta-learning on top of the pretrained features from Sim-\nCLR, the performance gain versus vanilla representation from SimCLR is at most marginal when\ndeep backbones are used in our reproduction. This observation aligns with the empirical results\nin supervised few-shot learning where the advantage of episodic meta-learning diminishes as the\nbackbone becomes deep (Chen et al., 2019, Tian et al., 2020b). Our method is also compared with\nsome strong baselines in supervised few-shot learning. The performance gap between supervised\nand unsupervised few-shot learning is signiﬁcantly reduced by our method, compared with previ-\n11\nTable 2: Cross-domain few-shot classiﬁcation results on miniImageNet to CUB.\nMethod\nBackbone\nminiImageNet to CUB 5-way\n1-shot\n5-shot\nSupervised\nMAML (Finn et al., 2017)\nWRN-28-10\n39.06 ± 0.47\n55.04 ± 0.42\nLEO (Rusu et al., 2019)\nWRN-28-10\n41.45 ± 0.54\n56.66 ± 0.48\nMTL (Sun et al., 2019)\nWRN-28-10\n43.15 ± 0.44\n56.89 ± 0.41\nMatching Net (Vinyals et al., 2016)\nWRN-28-10\n42.04 ± 0.57\n53.08 ± 0.45\nSIB (Hu et al., 2020)\nWRN-28-10\n43.27 ± 0.44\n59.94 ± 0.42\nBaseline (Chen et al., 2019)\nWRN-28-10\n42.89 ± 0.41\n62.12 ± 0.40\nUnsupervised\nCACTUs-MAML (Hsu et al., 2019)\nWRN-28-10\n33.48 ± 0.49\n49.97 ± 0.41\nUMTRA (Khodadadeh et al., 2019)\nWRN-28-10\n33.59 ± 0.48\n50.21 ± 0.45\nMeta-GMVAE (Lee et al., 2021a)\nWRN-28-10\n38.09 ± 0.47\n55.65 ± 0.42\nSimCLR (Chen et al., 2020)\nWRN-28-10\n38.25 ± 0.49\n55.89 ± 0.46\nMoCo v2 (He et al., 2020)\nWRN-28-10\n39.29 ± 0.47\n56.49 ± 0.44\nBYOL (Grill et al., 2020)\nWRN-28-10\n40.63 ± 0.46\n56.92 ± 0.43\nBarlow Twins (Zbontar et al., 2021)\nWRN-28-10\n40.46 ± 0.47\n57.16 ± 0.42\nOurs\nWRN-28-10\n41.08 ± 0.48\n58.86 ± 0.45\nous results in unsupervised few-shot learning (Hsu et al., 2019, Khodadadeh et al., 2019, Lee et al.,\n2021a).\nOur method is also applied to the cross-domain few-shot classiﬁcation task as summarized in\nTable 2. The proposed method outperforms other unsupervised methods in this challenging task,\nindicating that the learned representation has strong generalization capability. We use the same\nhyperparameters (training epochs, learning rate, etc.) from in-domain few-shot learning to train\nthe model. The strong results indicate that our method is robust to hyperparameter choice. Al-\nthough meta-learning methods with adaptive embeddings are expected to perform better than a\nﬁxed embedding when the domain gap between base classes and novel classes is large, empirical\nresults show that a ﬁxed embedding from supervised or unsupervised pretraining achieves better\nperformance in both cases. (Tian et al., 2020b) also reports similar results that a ﬁxed embedding\nfrom supervised pretraining shows superior performance on a large-scale cross-domain few-shot\nclassiﬁcation dataset. We still believe that adaptive embeddings should be helpful when the do-\nmain gap between base and novel classes is large. Nevertheless, how to properly train a model on\nunlabeled meta-training training to obtain useful adaptive embeddings in novel tasks is an open\nquestion.\nAblation studies are conducted to analyze how individual components affect the performance\nof few-shot learning. We study four variants of our methods: (a) the model is trained by only min-\nimizing the similarity between positive pairs Ltrace; (b) the projector network is a 2-layer MLP; (c)\nmanifold mixup is not used in the model; (d) manifold mixup is replaced by input mixup. Table\n3 shows the results of our ablation studies on FC100. When the model is trained without feature\ndecorrelation, the accuracy on few-shot learning is close to random guess. It indicates that feature\ndecorrelation is the key to avoiding trivial representation in learning from unlabeled data. After\n12\nreplacing the projector network with a 2-layer MLP, we can see obvious performance loss in the\nproposed method. Sufﬁcient depth in the projector network is required to achieve optimal per-\nformance. The performance deteriorates without manifold mixup, indicating that manifold mixup\nhelps the model to learn task-relevant information for downstream meta-test tasks. Compared with\nmanifold mixup, input mixup is less effective in improving the few-shot learning performance.\nTable 3: Ablation studies on FC100.\nFC100 5-way\n1-shot\n5-shot\nOnly Ltrace\nCollapsed\nCollapsed\n2-layer MLP\n36.1 ± 0.7\n50.2 ± 0.7\nRemove manifold mixup\n38.2 ± 0.7\n54.4 ± 0.7\nUse input mixup\n38.7 ± 0.7\n55.6 ± 0.7\nOurs\n39.7 ± 0.7\n57.9 ± 0.7\n5.2\nLinear evaluation\nTo examine the quality of the learned representation, we follow the linear evaluation protocol\nin self-supervised learning. After the feature extractor is pretrained by unlabeled training data, a\nlinear classiﬁer is trained on top of the frozen backbone network using the labeled training data.\nThe linear evaluation performance is widely used as the proxy for representation quality because\nit is highly correlated to the performance in downstream tasks, such as transfer learning, objection\ndetection, and image segmentation (Chen and He, 2021, He et al., 2020). Different from few-shot\nlearning, unlabeled training data, labeled training data, and test data are from the same classes\nunder the linear evaluation protocol. We conduct experiments on CIFAR-10/100 and STL-10.\nCIFAR-10/100 are two datasets of tiny natural images with a size 32 × 32 (Krizhevsky, 2009).\nCIFAR-10 and CIFAR-100 have 10 and 100 classes, respectively. Both datasets contain 50,000 train-\ning images and 10,000 test images.\nSTL-10 is a 10-class image recognition dataset for unsupervised learning (Coates et al., 2011).\nEach class contains 500 labeled training images and 800 test images. In addition, it also contains\n100,000 unlabeled training images. Both labeled and unlabeled training images are used for feature\nextractor pretraining without using labels. The linear classiﬁer is learned using the labeled training\nimages.\nResNet18 is adopted as the backbone network in the feature extractor. We train the feature\nextractor using SGD with momentum of 0.9 and weight decay of 5e-4. The learning rate starts at\n0.05 and decreases to 0 with a cosine schedule. The feature extractor is trained for 800 epochs with\na batch size of 256.\nAfter the feature extractor is pretrained by unlabeled data, a linear classiﬁer is trained using\nSGD with a batch size of 256 and no weight decay for 100 epochs. The learning rate starts at 30.0\nand is decayed by 0.1 at the 60th and 80th epochs. The test accuracy is reported in Table 4.\nOur approach achieves comparable performance to SOTA self-supervised learning methods in\nthe linear evaluation under the same training recipe. Considering the good performance in linear\nevaluation, our method can be used in a wide range of downstream tasks beyond few-shot learning.\n13\nTable 4: Accuracy under linear evaluation protocol\nCIFAR-10\nCIFAR-100\nSTL-10\nSimCLR\n90.57\n63.84\n87.52\nMoCo v2\n90.67\n64.13\n87.71\nBYOL\n91.74\n65.92\n88.46\nBarlow Twins\n91.58\n65.83\n88.65\nOurs\n92.24\n66.16\n88.97\n6\nConclusions\nIn this article, we propose a new unsupervised few-shot learning method via deep Laplacian\neigenmaps. Our method learns representation from unlabeled data by grouping similar samples\ntogether and can be intuitively interpreted by random walks on augmented training data. We pro-\nvide a detailed analysis of our loss function derived from constrained trace minimization to show\nhow it avoids collapsed representation analytically and the connection to existing self-supervised\nlearning methods. The few-shot learning performance beneﬁts from the interpolation of unlabeled\ntraining samples on the data manifold. Compared with existing unsupervised few-shot learning\nmethods, the performance gap to supervised few-shot learning methods is signiﬁcantly narrowed.\nAdditional results on linear evaluation suggest that our method can be applied to a wide range of\ndownstream tasks beyond few-shot classiﬁcation.\nReferences\nAntreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your maml. In International\nConference on Learning Representations, 2018.\nYM Asano, C Rupprecht, and A Vedaldi. Self-labelling via simultaneous clustering and representa-\ntion learning. In International Conference on Learning Representations, 2019.\nFrancis R Bach and Michael I Jordan.\nLearning spectral clustering, with application to speech\nseparation. The Journal of Machine Learning Research, 7:1963–2001, 2006.\nMikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data\nrepresentation. Neural computation, 15(6):1373–1396, 2003.\nLuca Bertinetto, Joao F Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differen-\ntiable closed-form solvers. In International Conference on Learning Representations, 2019.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-\npervised learning of visual features. In Proceedings of the European Conference on Computer Vision\n(ECCV), pages 132–149, 2018.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. In Advances in Neu-\nral Information Processing Systems 33: Annual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning, pages\n1597–1607. PMLR, 2020.\n14\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look\nat few-shot classiﬁcation. In International Conference on Learning Representations, 2019.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\nAdam Coates, Andrew Ng, and Honglak Lee. An Analysis of Single Layer Networks in Unsuper-\nvised Feature Learning. In AISTATS, 2011.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.\nImagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npages 248–255. Ieee, 2009.\nGuneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline\nfor few-shot image classiﬁcation. In International Conference on Learning Representations, 2020.\nAleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe.\nWhitening for self-\nsupervised representation learning. In International Conference on Machine Learning, pages 3015–\n3024. PMLR, 2021.\nLi Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions\non pattern analysis and machine intelligence, 28(4):594–611, 2006.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,\npages 1126–1135. JMLR. org, 2017.\nSebastian Flennerhag, Andrei A Rusu, Razvan Pascanu, Francesco Visin, Hujun Yin, and Raia Had-\nsell. Meta-learning with warped gradient descent. In International Conference on Learning Repre-\nsentations, 2020.\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi\nAzar, et al. Bootstrap your own latent: A new approach to self-supervised learning. In Proceedings\nof the 33nd International Conference on Neural Information Processing Systems, 2020.\nMichael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation princi-\nple for unnormalized statistical models. In Proceedings of the thirteenth international conference on\nartiﬁcial intelligence and statistics, pages 297–304, 2010.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778,\n2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for un-\nsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9729–9738, 2020.\nXiaofei He and Partha Niyogi. Locality preserving projections. In Advances in neural information\nprocessing systems, volume 16, pages 153–160, 2004.\nOlivier J Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, SM Eslami, and\nAaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. In\nInternational Conference on Machine Learning, pages 4182–4192. PMLR, 2020.\n15\nKyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised learning via meta-learning. In Interna-\ntional Conference on Learning Representations, 2019.\nShell Xu Hu, Pablo Garcia Moreno, Yang Xiao, Xi Shen, Guillaume Obozinski, Neil Lawrence, and\nAndreas Damianou. Empirical bayes transductive meta-learning with synthetic gradients. In\nInternational Conference on Learning Representations, 2020.\nTianyu Hua, Wenxiao Wang, Zihui Xue, Sucheng Ren, Yue Wang, and Hang Zhao. On feature\ndecorrelation in self-supervised learning. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 9598–9608, 2021.\nYannis Kalantidis, Mert Bülent Sariyildiz, Noé Pion, Philippe Weinzaepfel, and Diane Larlus. Hard\nnegative mixing for contrastive learning. In Advances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual, 2020.\nTero Karras, Samuli Laine, and Timo Aila.\nA style-based generator architecture for generative\nadversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4401–4410, 2019.\nSiavash Khodadadeh, Ladislau Boloni, and Mubarak Shah. Unsupervised meta-learning for few-\nshot image classiﬁcation. In Advances in neural information processing systems, 2019.\nSiavash Khodadadeh, Sharare Zehtabian, Saeed Vahidian, Weijia Wang, Bill Lin, and Ladislau\nBoloni. Unsupervised meta-learning through latent-space interpolation in generative models.\nIn International Conference on Learning Representations, 2021.\nGregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot\nimage recognition. In ICML deep learning workshop, volume 2. Lille, 2015.\nA Krizhevsky.\nLearning Multiple Layers of Features from Tiny Images.\nPhD thesis, University of\nToronto, 2009.\nSteinar Laenen and Luca Bertinetto. On episodes, prototypical networks, and few-shot learning. In\nAdvances in Neural Information Processing Systems, 2021.\nDong Bok Lee, Dongchan Min, Seanie Lee, and Sung Ju Hwang. Meta-gmvae: Mixture of gaussian\nvae for unsupervised meta-learning. In International Conference on Learning Representations, 2021a.\nKibok Lee, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin, and Honglak Lee. $i$-mix: A\ndomain-agnostic strategy for contrastive representation learning. In International Conference on\nLearning Representations, 2021b.\nKwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with\ndifferentiable convex optimization. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 10657–10665, 2019.\nYoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and\nsubspace. In International Conference on Machine Learning, pages 2927–2936, 2018.\nPuneet Mangla, Mayank Singh, Abhishek Sinha, Nupur Kumari, Vineeth N Balasubramanian, and\nBalaji Krishnamurthy. Charting the right manifold: Manifold mixup for few-shot learning. In\n2020 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 2207–2216. IEEE,\n2020.\n16\nDavid McAllester and Karl Stratos. Formal limitations on the measurement of mutual information.\nIn International Conference on Artiﬁcial Intelligence and Statistics, pages 875–884. PMLR, 2020.\nMarina Meila and Jianbo Shi. Learning segmentation by random walks. In Advances in neural\ninformation processing systems, volume 13, pages 873–879, 2000.\nTomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word represen-\ntations in vector space. In Yoshua Bengio and Yann LeCun, editors, 1st International Conference on\nLearning Representations, ICLR 2013, 2013.\nAlex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv\npreprint arXiv:1803.02999, 2018.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\nBoris Oreshkin, Pau Rodríguez López, and Alexandre Lacoste. Tadam: Task dependent adaptive\nmetric for improved few-shot learning. In Advances in Neural Information Processing Systems, pages\n721–731, 2018.\nEunbyung Park and Junier B Oliva.\nMeta-curvature.\nAdvances in Neural Information Processing\nSystems, 32:3314–3324, 2019.\nBen Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational\nbounds of mutual information. In International Conference on Machine Learning, pages 5171–5180.\nPMLR, 2019.\nHang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 5822–5830, 2018.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In 5th Interna-\ntional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Confer-\nence Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=\nrJY0-Kcll.\nAvinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Few-shot learning with embedded class\nmodels and shot-free meta training. In Proceedings of the IEEE International Conference on Computer\nVision, pages 331–339, 2019.\nAndrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,\nand Raia Hadsell. Meta-learning with latent embedding optimization. In International Conference\non Learning Representations, 2019.\nZhiqiang Shen, Zechun Liu, Zhuang Liu, Marios Savvides, Trevor Darrell, and Eric Xing. Un-mix:\nRethinking image mixtures for unsupervised visual representation learning. In AAAI, 2022.\nJianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on\npattern analysis and machine intelligence, 22(8):888–905, 2000.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In\nAdvances in Neural Information Processing Systems, pages 4077–4087, 2017.\nQianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot\nlearning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n403–412, 2019.\n17\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learn-\ning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 1199–1208, 2018.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV 2020,\npages 776–794. Springer International Publishing, 2020a.\nYonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking few-\nshot image classiﬁcation: a good embedding is all you need? In European conference on computer\nvision. Springer, 2020b.\nYuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynam-\nics without contrastive pairs. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139\nof Proceedings of Machine Learning Research, pages 10268–10278. PMLR, 2021.\nMichael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mu-\ntual information maximization for representation learning. In International Conference on Learning\nRepresentations, 2020.\nVikas Verma, Alex Lamb, Christopher Beckham, Amir Najaﬁ, Ioannis Mitliagkas, David Lopez-Paz,\nand Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In\nInternational Conference on Machine Learning, pages 6438–6447. PMLR, 2019.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one\nshot learning. In Advances in neural information processing systems, pages 3630–3638, 2016.\nFeng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2495–2504, 2021.\nTongzhou Wang and Phillip Isola.\nUnderstanding contrastive representation learning through\nalignment and uniformity on the hypersphere. In International Conference on Machine Learning,\npages 9929–9939. PMLR, 2020.\nP. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds\n200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-\nparametric instance discrimination. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3733–3742, 2018.\nShuo Yang, Lu Liu, and Min Xu. Free lunch for few-shot learning: Distribution calibration. In\nInternational Conference on Learning Representations, 2021.\nChun-Hsiao Yeh, Cheng-Yao Hong, Yen-Chi Hsu, Tyng-Luh Liu, Yubei Chen, and Yann LeCun.\nDecoupled contrastive learning. arXiv preprint arXiv:2110.06848, 2021.\nSung Whan Yoon, Jun Seo, and Jaekyun Moon. Tapnet: Neural network augmented with task-\nadaptive projection for few-shot learning. In ICML 2019 (International Conference on Machine Learn-\ning). ICML, 2019.\nSung Whan Yoon, Do-Yeon Kim, Jun Seo, and Jaekyun Moon. Xtarnet: Learning to extract task-\nadaptive representation for incremental few-shot learning. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceed-\nings of Machine Learning Research, pages 10852–10860. PMLR, 2020.\n18\nZhongqi Yue, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua. Interventional few-shot learning.\nAdvances in Neural Information Processing Systems, 33, 2020.\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages 6023–6032, 2019.\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised\nlearning via redundancy reduction. In Proceedings of the 38th International Conference on Machine\nLearning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning\nResearch, pages 12310–12320. PMLR, 2021.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empir-\nical risk minimization. In International Conference on Learning Representations, 2018.\n19\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-10-07",
  "updated": "2022-10-07"
}