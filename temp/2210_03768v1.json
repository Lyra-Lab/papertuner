{
  "id": "http://arxiv.org/abs/2210.03768v1",
  "title": "xDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph",
  "authors": [
    "Arif Usta",
    "Akifhan Karakayali",
    "Özgür Ulusoy"
  ],
  "abstract": "Translating natural language queries (NLQ) into structured query language\n(SQL) in interfaces to relational databases is a challenging task that has been\nwidely studied by researchers from both the database and natural language\nprocessing communities. Numerous works have been proposed to attack the natural\nlanguage interfaces to databases (NLIDB) problem either as a conventional\npipeline-based or an end-to-end deep-learning-based solution. Nevertheless,\nregardless of the approach preferred, such solutions exhibit black-box nature,\nwhich makes it difficult for potential users targeted by these systems to\ncomprehend the decisions made to produce the translated SQL. To this end, we\npropose xDBTagger, an explainable hybrid translation pipeline that explains the\ndecisions made along the way to the user both textually and visually. We also\nevaluate xDBTagger quantitatively in three real-world relational databases. The\nevaluation results indicate that in addition to being fully interpretable,\nxDBTagger is effective in terms of accuracy and translates the queries more\nefficiently compared to other state-of-the-art pipeline-based systems up to\n10000 times.",
  "text": "Noname manuscript No.\n(will be inserted by the editor)\nxDBTagger: Explainable Natural Language Interface to\nDatabases Using Keyword Mappings and Schema Graph\nArif Usta · Akifhan Karakayali · ¨Ozg¨ur Ulusoy\nReceived: date / Accepted: date\nAbstract Translating natural language queries (NLQ)\ninto structured query language (SQL) in interfaces to\nrelational databases is a challenging task that has been\nwidely studied by researchers from both the database\nand natural language processing communities. Numer-\nous works have been proposed to attack the natural\nlanguage interfaces to databases (NLIDB) problem ei-\nther as a conventional pipeline-based or an end-to-end\ndeep-learning-based solution. Nevertheless, regardless\nof the approach preferred, such solutions exhibit black-\nbox nature, which makes it diﬃcult for potential users\ntargeted by these systems to comprehend the decisions\nmade to produce the translated SQL. To this end, we\npropose xDBTagger, an explainable hybrid translation\npipeline that explains the decisions made along the way\nto the user both textually and visually. We also evaluate\nxDBTagger quantitatively in three real-world relational\ndatabases. The evaluation results indicate that in addi-\ntion to being fully interpretable, xDBTagger is eﬀective\nin terms of accuracy and translates the queries more\neﬃciently compared to other state-of-the-art pipeline-\nbased systems up to 10000 times.\nArif Usta\nUniversity of Waterloo\nWaterloo, ON, Canada\nE-mail: arif.usta@uwaterloo.ca\nAkifhan Karakayali\nThe Central Bank of the Republic of T¨urkiye\nAnkara, Turkey\nE-mail: akifhan.karakayali@tcmb.gov.tr\n¨Ozg¨ur Ulusoy\nBilkent University\nAnkara, Turkey\nE-mail: oulusoy@cs.bilkent.edu.tr\nKeywords natural language interface for databases ·\nNLIDB · text-to-SQL · multi-task learning · explainable\nartiﬁcial intelligence\n1 Introduction\nSQL is used as a standard tool to extract data out of\na relational database. Although SQL is a powerfully\nexpressive language, even technically skilled users have\ndiﬃculties using SQL. Along with the syntax of SQL,\none has to know the schema underlying the database\nupon which the query is issued, which further causes\nhurdles in using SQL. Consequently, casual users ﬁnd\nit even more challenging to express their information\nneeds, which makes SQL less desirable. To remove this\nbarrier, an ideal solution is to provide a search engine-\nlike interface in databases. The goal of NLIDB is to\nbreak through these barriers to make it possible for\ncasual users to employ their natural language to extract\ninformation.\nRecently, many works have been developed attack-\ning the NLIDB problem; such as conventional pipeline-\nbased approaches [3, 27, 34, 43, 36] or end-to-end deep-\nlearning-based approaches [22, 50, 42, 47, 16, 40, 28,\n35]. Neural network-based solutions seem promising in\nterms of translation accuracy and robustness, cover-\ning semantic variations of queries. However, they strug-\ngle with queries requiring translation of complex SQL\nqueries, such as aggregation and nested queries, espe-\ncially if they include multiple tables. They also have a\nhuge drawback in that they need many SQL-NL pairs\nfor training to perform well, which makes pipeline-based\nor hybrid solutions still an attractive alternative. [31].\nWhether it is a pipeline-based or an end-to-end deep\nlearning approach, existing solutions have black-box na-\narXiv:2210.03768v1  [cs.DB]  7 Oct 2022\n2\nArif Usta et al.\nture when it comes to outputting translated SQL. Being\na black-box solution makes it diﬃcult for users to un-\nderstand how the result SQL is produced along the way,\nwhich is a vital defectiveness for any modern intelligent\nsystem that should aim to gain the trust of the users\n[17]. This undesirable property of NLIDB solutions be-\ncomes much more consequential in an online scenario,\nespecially for casual users (i.e., users with little to no\ntechnical expertise in SQL), who are the primary po-\ntential audience targeted by the problem of NLIDB.\nAlthough NLIDB is a well-studied problem in the\nliterature, the transparency and explainability of the\nproposed solutions have been overlooked. An intelligent\nsystem such as an NLIDB system has to be transpar-\nent and self-explanatory to the user so that they can\ncomprehend the decisions made by the system. As high-\nlighted by many previous studies [17, 14, 32, 15], having\nan explainable intelligent system exhibits many bene-\nﬁts including but not limited to improving users’ trust\nin the system, helping users understand the decisions\nmade by the system, and showing the limitations of the\nsystems for certain use-cases, all of which can be in-\nstrumental towards developing more user friendly and\npreferable NLIDB systems.\nConsider the below pair of NL query and SQL trans-\nlation from a movie database domain to understand\nbetter how an explainable NLIDB solution would be\nhandy and, in fact, essential for users to reason with\nthe results:\nNL Query 1 Who is the director of the series House\nof Cards produced by Netﬂix?\nTranslated SQL 1\nSelect * From\ntv_series, copyright, company,\ndirected_by, director\nWhere (tv_series.msid = copyright.msid)\nand (copyright.cid = company.cid)\nand (tv_series.msid = directed_by.msid)\nand (directed_by.did = director.did)\nand (tv_series.title = \"House of Cards\")\nand (company.name = \"Netflix\")\nAlthough such a SQL query is easy to understand\nfor an expert user at a glance, it is diﬃcult to inter-\npret for casual users, which are the ones targeted by\nthe NLIDB problem. In the above query, there are 2\nkeywords, House of Cards and Netﬂix, found under\nthe attributes title and name of their respective en-\ntity tables tv series and company. However, to ﬁnd the\ncorresponding tuple(s) matching these two values from\ndiﬀerent tables, SQL requires a join operation; in this\ncase, a 5-way join. The subsequent four lines after the\nWhere clause in the above example represent conditions\nto ensure the right join. More importantly, although one\nneeds access to 3 entity tables; 2 (tv series and com-\npany) for utterances found in the query and 1 (direc-\ntor) for the desired information asked by the user, SQL\nrequires two more intermediate tables, copyright and\ndirected by, to complete the join. In an ideal NLIDB,\nthe story behind the translated SQL, such as above,\nshould be provided to the user to some extent. In addi-\ntion to the explanations needed for understanding SQL\nstructure, the NLIDB system should also ideally give\nexplanations for how it matches schema elements (e.g.,\nthree tables and two attributes for the above example)\nto the corresponding utterances.\nTo address above-mentioned concerns, we propose\nan explainable, end-to-end NLIDB solution, Explain-\nable DBTagger (xDBTagger), by extending our pre-\nvious work in [37]. xDBTagger is a hybrid solution uti-\nlizing both deep learning and rule-based approaches.\nTo the best of our knowledge, xDBTagger is the ﬁrst\nstudy exercising explainable artiﬁcial intelligence (XAI)\nparadigm in the NLIDB problem. In what follows, we\nlist the main contributions of our work:\n– We use our previous work [37], which is a deep learn-\ning model speciﬁcally tailored for sequence tagging\nin NLIDB, to extract keyword mappings given the\nNLQ.\n– We propose a novel wrapper tailored for sequence\ntagging problems around a state-of-the-art XAI work,\nLIME [33], to explain decisions made for keyword\nmappings output for each token in NLQ. We provide\nexplanations for each keyword mapping correspond-\ning to tokens in NLQ by highlighting both the pos-\nitive and negative contributions of each surrounding\ntoken.\n– We propose an eﬀective and eﬃcient SQL transla-\ntion algorithm suitable for interpretability by utiliz-\ning keyword mappings and schema graphs. We pro-\nvide textual and visual explanations for the user to\ncomprehend how the translation algorithm works.\n– We quantitatively evaluate the entire pipeline in three\npublicly available datasets where xDBTagger per-\nforms competitively as the most eﬃcient and scalable\nsolution.\n– We deploy xDBTagger in a user-friendly and inter-\npretable interface in which the user is presented the\ntranslated SQL along with the explanations for the\ndecisions made throughout the pipeline.\nThe remainder of the paper is organized as follows.\nIn the next section, we give an overview of the system\narchitecture of xDBTagger. Section 3 presents the neu-\nral network structure we design for the keyword map-\nping step. We explain how we modify LIME to produce\nxDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph\n3\nFig. 1: System Architecture of xDBTagger\nexplanations for keyword mappings output by DBTag-\nger in Section 4. We thoroughly review the main com-\nponents of the SQL extraction algorithm in Section 5.\nIn Section 6, we provide quantitative experimental re-\nsults for both the keyword mapper and the entire SQL\ntranslation pipeline (Section 6.2). In addition to quan-\ntitative results, we illustrate the user interface and pro-\nvide examples of textual and visual explanations in the\n4\nArif Usta et al.\ninterface (Section 6.3). We summarize the related work\nand conclude the paper in Sections 7 and 8, respectively.\n2 System Architecture\nFigure 1 depicts an overview of the translation pipeline\nalong with explanation components to make the decision-\nmaking throughout the pipeline interpretable. The work-\nﬂow starts with an input NLQ from the user. The query\nﬁrst goes through pre-processing, which removes spe-\ncial characters and punctuations such as commas and\nquotes. After removing those characters, the query is to-\nkenized into words using spaces. These tokens are then\nconverted to 300-dimensional vector representations us-\ning a pre-trained word embedding model. In our imple-\nmentation, we used a pre-trained fast-text [4] model.\nThe output of the embedding model X = [x1, x2, ..., xn]\nis an array of 300-dimensional vectors where n is the\nlength of the query (i.e., the number of tokens in the\nNLQ). Each 300-dimensional vector is a representation\nof each word in the original query. Following that, X\nis fed as the input to DBTagger model, which outputs\ncorresponding keyword mappings for each token in the\nquery. DBTagger outputs 2 series of outputs; 1 for type\ntags (i.e., schema element such as table, attribute if the\ntoken is relevant for SQL translation or ”O” if irrele-\nvant) and 1 for schema tags (i.e., deeper level tags such\nas name of a table or an attribute) of the tokens in the\nNLQ.\nWe use LIME [33] to explain keyword mappings out-\nput by DBTagger. LIME requires a black-box model\nwhich can output prediction probabilities and raw in-\nput text for explanation. To satisfy these conditions,\nwe construct a DBTagger Model Blackbox, which takes\nthe raw text as input and gives predictions with proba-\nbilities as output. Vanilla LIME tries to explain a single\nclassiﬁed output given a sequential input text. However,\nit is not the case in our problem (i.e., sequence tagging\nproblem requiring a classiﬁcation and, therefore, an ex-\nplanation for each token in the query). To alleviate this\nissue, we implement a wrapper around LIME which is\nexplained in Section 4.1. This wrapper takes DBTagger\nModel Blackbox, raw text input, and predicted output\ntags to produce an explanation for each token.\n3 Keyword Mapper - DBTagger\nIn this section, we ﬁrst provide background information\nabout the neural network structure utilized for sequence\ntagging problems such as POS tagging and NER in the\nNLP community. Next, we explain the network struc-\nture of DBTagger, our keyword mapper solution in the\npipeline, by pointing out modiﬁcations we introduce on\ntop of the state-of-the-art sequence tagging architec-\nture. Lastly, we discuss how we annotate three diﬀerent\nclass labels of tokens to employ multi-task training.\n3.1 Deep Sequence Tagger Architecture\nPOS tagging and NER refer to sequence tagging prob-\nlems in NLP for a particular sentence to identify parts-\nof-speech such as noun, verb, and adjective and locate\nany entity names such as person and organization, re-\nspectively. We argue that these problems are formally\nsimilar to the keyword mapping problem in NLIDB.\nRecurrent Neural Networks (RNN) are at the core of\narchitectures to handle such problems since they are\na family of networks that perform well on sequential\ndata input such as a sentence. In this particular prob-\nlem, sequence tagging (keyword mapping), RNNs are\nemployed to output a sequence of labels for the original\nsentence (the query), input as a sequence of words.\nIn RNN networks, the basic goal is to carry past in-\nformation (previous words) to future time steps (future\nwords) to determine values of inner states and, conse-\nquently, the ﬁnal output, which makes them preferable\narchitecture for sequential data. Given xt as input at\ntime step t, calculation of hidden state ht at time step\nt is as follows:\nht = f(Uxt + Wht−1)\n(1)\nIn practice, however, RNN networks suﬀer from van-\nishing gradient problem; therefore, the limitation was\novercome by modifying the gated units of RNNs; such\nas LSTM [20] and GRU[6]. Compared to vanilla RNN,\nLSTM has forget gates and GRU comprises of reset and\nupdate gates additionally. We experimented with both\nstructures and chose GRU for its better performance in\nour experiments. In GRU, Update Gates decide what\ninformation to throw away and what new information\nto add, whereas Reset Gate is utilized to decide how\nmuch past information to forget. The calculation of\nGRU is as follows:\nz = σ(Uz.xt + Wz.ht−1)\n(2)\nr = σ(Ur.xt + Wr.ht−1)\n(3)\nzt = tanh(Uz.xt + Ws.(ht−1 • r))\n(4)\nzt = σ(Uzxt + Wzht−1)\n(5)\nIn the sequence tagging problem, in addition to past\ninformation, we also have future information at a given\nspeciﬁc time, t. For a particular word wi, we know\nxDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph\n5\nFig. 2: Deep Sequence Tagger Network\nFig. 3: DBTagger Network\nthe preceding words (past information) and succeed-\ning words (future information), which can be further\nexploited in the particular network architecture called,\nbi-directional RNN introduced in [13]. Bi-directional\nRNN has two sets of networks with diﬀerent param-\neters called forward and backward. The concatenation\nof the two networks is then fed into the last layer, where\nthe output is determined. This process is demonstrated\nin Figure 2.\nSequence tagging is a supervised classiﬁcation prob-\nlem where the model tries to predict the most probable\nlabel from the output space. For that purpose, although\n6\nArif Usta et al.\nconventional softmax classiﬁcation can be used, condi-\ntional random ﬁeld (CRF) [25] is preferred. Unlike in-\ndependent classiﬁcation by softmax, CRF tries to pre-\ndict labels sentence-wise by considering labels of the\nneighboring words as well. This feature of CRF is what\nmakes it an attractive choice, especially in a problem\nlike keyword mapping. This ﬁnding was also reported in\n[26], where authors claim that CRF as the output layer\ngives 1.79 more accuracy compared to the softmax layer\nin NER task. The ﬁnal outlook of the architecture of\ndeep sequence tagger is depicted in Figure 2.\n3.2 DBTagger Architecture\nFormally, for a given NL query, input X becomes a\nseries of vectors [x1, x2, ...xn] where xi represents the\nith word in the query. Similarly, output vector Y be-\ncomes [y1, y2, ...yn] where yi represents the label (ac-\ntual tag) of the yth word in the query. Input must be in\nnumerical format, which implies that a numerical rep-\nresentation of words is needed. For that purpose, the\nword embedding approach is state-of-the-art in various\nsequence tagging tasks in NLP [7] before feeding into\nthe network. So, the embedding matrix is extracted for\nthe given query, W ∈Rnxd, where n is the number of\nwords in the query and d is the dimension of the em-\nbedding vector for each word. For the pre-calculated\nembeddings, we used fastText[4] due to it being one\nof the representation techniques considering sub-word\n(character n-grams) as well to deal with the out-of-\nvocabulary token problem better.\nWe consider G to be 2-dimensional scores of output\nby the uni-directional GRU with size n × k where k\nrepresents the total number of tags. Gi,j refers to score\nof the jth tag for the ith word. For a sequence Y and\ngiven input X, we deﬁne tag scores as;\ns(X, Y ) =\nn\nX\ni=1\nAyi,yi+1 +\nn\nX\ni=1\nGi,yi\n(6)\nwhere A is a transition matrix in which Ai,j repre-\nsents the score of a transition from the ith tag to the\njth tag. After ﬁnding scores, we deﬁne the probability\nof the sequence Y :\np(Y |X) =\nes(X,Y )\nP\n¯Y ∈Yx es(X, ¯Y )\n(7)\nwhere ¯Y refers to any possible tag sequence. During\ntraining, we maximize the log-probability of the correct\ntag sequence, and for the inference, we simply select the\ntag sequence with the maximum score.\nIn our architecture, we utilize Multi-task learning by\nintroducing two other related tasks; POS and type lev-\nels (shown in Figure 3). The reason we apply multi-task\nlearning is to try to exploit the observation that actual\ndatabase tags of the tokens in the query are related to\nPOS tags. Besides, multi-task learning helps to increase\nmodel accuracy and eﬃciency by making more gener-\nalized models with the help of shared representations\nbetween tasks [5]. POS and Type tasks are trained with\nschema task to improve the accuracy of schema (ﬁnal)\ntags. For each task, we deﬁne the same loss function\ndescribed above. During back-propagation, we simply\ncombine the losses as follows;\nLtotal =\n3\nX\ni=1\nwi × Li subject to\n3\nX\ni=1\nwi = 1\n(8)\nwhere wi represents the weight of ith task and Li\nrepresents the loss calculated for the ith task similarly.\n3.3 Annotation Scheme\nWe tackle keyword mapping as a sequence tagging prob-\nlem, which is a supervised classiﬁcation problem. In\nour problem formulation, every token (i.e., words in the\nnatural language query) associates three diﬀerent tags:\npart-of-speech (POS) tag, type tag, and schema tag. In\nthe following subsections, we explain how we extract or\nannotate each of them in detail.\n3.3.1 POS Tags\nTo obtain the POS tags of our natural language queries,\nwe used the toolkit of Stanford Natural Language Pro-\ncessing Group named Stanford CoreNLP[29]. We use\nthem as they are output from the toolkit without do-\ning any further processing since the reported accuracy\nfor POS Tagger (97%) is suﬃcient enough.\n3.3.2 Type Tags\nIn each natural language query, there are keywords (words\nor consecutive words) which can be mapped to database\nschema elements such as table, attribute, or value. We\ndivide this mapping into two levels; type tagging and\nschema tagging. Type tags represent the type of the\nmapped schema element to be used in the SQL query.\nIn total, we have seven diﬀerent type tags;\nxDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph\n7\nNLQ\nType\nTag\nSchema\nTag\nWho\nO\nO\nis\nO\nO\nthe\nO\nO\ndirector\nTABLE\ndirector\nof\nO\nO\nthe\nO\nO\nseries\nTABLE\ntv series\nHouse\nVALUE\ntv series.title\nof\nVALUE\ntv series.title\nCards\nVALUE\ntv series.title\nproduced\nTABLEREF\ncopyright\nby\nO\nO\nNetﬂix\nVALUE\ncompany.name\nTable 1: An example NL query with its tags correspond-\ning to each word in two target levels\n– TABLE: NLQs contain nouns that may inhibit di-\nrect references to the tables in the schema, and we\ntag such nouns with TABLE tag. In the example NL\nquery given in Figure 1, noun director has a type tag\nas TABLE, which also supports the intuition that\nschema labels and pos tags are related.\n– TABLEREF: Although the primary sources for ta-\nble references are nouns, some verbs contain refer-\nences to the tables, most of which are relation ta-\nbles. TABLEREF tag is used to identify such verbs.\nRevisiting the example given in Figure 1, the verb\nproduced refers to the table copyright, and therefore\nit is tagged with TABLEREF to diﬀerentiate better\nthe roles of POS tags in the query.\n– ATTR: In SQL queries, attributes are mostly used in\nSELECT, WHERE, and GROUP BY clauses. Natu-\nral language queries may contain nouns that can be\nmapped to those attributes. We use the ATTR tag\nfor tagging such nouns in natural language queries.\n– ATTRREF: Like the TABLEREF tag, the ATTR-\nREF tag is used to tag the verbs in the natural lan-\nguage query that can be mapped to the attributes in\nthe SQL query.\n– VALUE: In NLQs, there are many entity-like key-\nwords that need to be mapped to their correspond-\ning database values. These words are mostly tagged\nas Proper noun-NNP such as the keyword House of\nCards in the example query. In addition to these tags,\nit is also likely for a word to have a noun-NN POS\ntag with a Value tag corresponding to schema level.\nIn order to handle these cases having diﬀerent POS\ntags, we have Value type tags (e.g., House keyword\nin the example query is part of a keyword that needs\nto be mapped as value to tv series.title). Keywords\nwith Value tags can later be used in the translation\nto determine ”where” clauses in SQL.\n– COND: After determining which keywords in the\nquery are to be mapped as values, it is also impor-\ntant to identify the words that imply which type of\nconditions to be met for the SQL query. For that\npurpose, we have the COND type tag.\n– O (OTHER): This type of tag represents words in\nthe query that are not needed to be mapped to any\nschema instrument related to the translation step.\nMost stop words in the query (e.g., the) fall into this\ncategory.\n3.3.3 Schema Tags\nSchema tags of keywords represent the database map-\nping that the keyword is referring to, the name of a\ntable, or the attribute. Tagging a keyword with a type\ntag is important yet incomplete. To ﬁnd the exact map-\nping the keyword refers to, we deﬁne a second-level\ntagging where the output is the name of the tables or\nattributes. For each entity table (e.g., movie table in\nthe shortest path component of Figure 6) and for each\nnon-PK or non-FK attribute (attributes which have se-\nmantics) we deﬁne a schema tag (e.g., movie, people,\nmovie.title, etc., referring to Figure 6). We complete\npossible schema tags by carrying OTHER and COND\nfrom type tags. We use the same schema tag for at-\ntributes and values (e.g., movie.title), but diﬀerentiate\nthem at the inference step by combining tags from both\ntype tags and schema tags. If a word is mapped into\nValue type tag as a result of the model, its schema tag\nrefers to the attribute in which the value resides.\nIn order to annotate queries, we annotate each word\nin the query for three diﬀerent levels mentioned above.\nWhile POS tags are extracted automatically, we man-\nually annotate the other two levels. Annotations were\ndone by three graduate and three undergraduate com-\nputer science students who are familiar with database\nsubject. Although annotation time varies depending on\nthe person, on the average, it took a week to anno-\ntate tokens by a single person for two levels (type and\nschema) for a query log with 150 NL questions, which\nwe believe is practical to apply in many domains.\n4 Explanations for Keyword Mapper\nIn this section, we provide the details about the tech-\nniques we employ to explain the decisions made by DB-\nTagger, our keyword mapper in the pipeline. First, we\ngive a short overview of the LIME [33] work and high-\nlight its applicability and limitation in the context of\nthe sequence tagging problem. Next, we explain how\nwe tailor LIME to the sequence tagging problem (i.e.,\n8\nArif Usta et al.\nclassiﬁcation problem for each item in the sequence) to\ndeploy in our pipeline.\n4.1 LIME\nLIME [33] is short for ”Local Interpretable Model-Agnostic\nExplanations”, where each part in the name exhibits a\ndesirable property a black-box explanation model must\nhave. ”Local” implies that LIME is an outcome expla-\nnation model, explaining the decision made on a par-\nticular instance, which is in line with our goal. ”Model-\nagnostic” refers that LIME works with any type of in-\nput data (e.g., image, text) or a black-box model (e.g,\na linear classiﬁer such as logistics regression or a neural\nnetwork based model), which is one of the reasons why\nwe use LIME in xDBTAgger that includes a neural net-\nwork based keyword mapper that we want to explain\nto the user.\nInterpretation and explanation are important terms\noften used interchangeably in the context of XAI; how-\never, they have distinct meanings. The former is more\ninvolved in providing abstracts in a way humans can\nmake sense of, whereas the latter revolves around high-\nlighting important features that play a role in decision-\nmaking for a given instance [12]. Analogously, the ex-\nplanations that are not interpretable are useless, which\nis addressed by LIME. LIME argues that interpretable\ndata representations diﬀer from actual feature represen-\ntations by asserting that interpretable data representa-\ntions, such as binary vectors stating the existence of\na word, are easily understood by humans. In contrast,\nactual feature representations, such as word embedding\nvectors, are not that straightforward and comprehensi-\nble. This distinctness is crucial since explanations pro-\nduced by LIME are based on interpretable data repre-\nsentations.\nIn particular, LIME provides the importance of the\nfeatures for a given instance as explanations, and to\nmake them interpretable it follows a binary approach\nthat highlights how important certain parts of the input\nare when they are present or absent. For a given input,\nLIME perturbs the input by randomly removing parts\nof the input and tries to understand how the model\nbehavior changes. For instance, LIME creates a series\nof artiﬁcial sentences for a particular text input as a\nsentence in which random tokens are removed. LIME\nthen tries to assign an importance score to each token\nfor the decision (e.g., a target label by a classiﬁer) by\nweighing the changes in model behavior. If the score is\npositive, the token is helpful when deciding the outcome\nfor a particular input, whereas it is disadvantageous to\nthe outcome when the score is negative. The absolute\nvalue of the score implies the contribution the token\nmakes to the outcome, either positively or negatively.\n4.2 LIME Wrapper\nDue to its properties, LIME is applicable in classiﬁca-\ntion problems where the input is a sentence, and it is\nimportant to explain the importance of each token in\nthe sentence in deciding a particular class. Also, note\nthat the architecture of DBTagger, our keyword map-\nper, also utilizes signals from neighboring tokens when\ndeciding the type and schema classes of a particular\ntoken by using CRF (see Figure 3) at the last layer.\nThis property of DBTagger aligns perfectly with the\napplicability of LIME in a text classiﬁcation problem,\nas explained above. However, vanilla LIME is not di-\nrectly applicable where the model generates multiple\noutcomes for a given sentence. In other words, vanilla\nLIME produces explanations for models that classify\nthe whole text sequence into one class (e.g., sequence\nclassiﬁcation such as sentiment analysis), whereas in\nour case, there is a classiﬁcation for every token in a\nsentence, referred to as sequence tagging problem in\nNLP. Hence, we make modiﬁcations and add a wrap-\nper around LIME to output explanations for each token\nsuitable for DBTagger.\nIn particular, the wrapper around LIME uses four\ngroups of information; the NLQ (i.e., the text input as\na list of tokens), DBTagger Model Black-box (i.e., the\nprobabilities of target classes for each token), predicted\ntype and schema classes of DBTagger Model, and out-\nput mask to perturb the sentence suitable for keyword\nmapping problem. The main purpose of this wrapper\nis to coordinate the communication between LIME and\nthe output mask. With the help of the output mask,\nit becomes possible for LIME to produce an explana-\ntion for a speciﬁc token; however, we still need to select\nthe token that will be explained. To achieve this, we\nonly explain the tokens with a predicted tag other than\n”O” for time eﬃciency since LIME uses many resources,\nand producing explanations is a time-consuming pro-\ncess, even for a single token. Once the token selection is\nmade, the wrapper adjusts the output mask so that the\nmodel gives the output for the selected token. Therefore\nLIME can analyze the output and produce an explana-\ntion for that token. This process is repeated for every\ntoken that is selected for the explanation.\n5 SQL Translation Algorithm\nWe use simple yet eﬀective algorithms to construct the\ntranslated query given a set of type and schema tags\nxDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph\n9\nExtractGraph (D)\nInputs\n: Relational database D\nOutputs: Graph model of database dbGraph\ndbGraph ←Graph();\nforeach table ti ∈D do\ndbGraph.addNode(ti)\nforeach column ci ∈ti do\nif si /∈dbGraph then\ndbGraph.addNode(ci)\ndbGraph.addEdge(ti, ci)\nend\nend\nreturn dbGraph;\nAlgorithm 1: Generating the schema graph out of\nthe database\noutput by the keyword mapper, DBTagger. In particu-\nlar, for the translation algorithm, we have three chan-\nnels of input (see Figure 1); (i) type tags (i.e., lists of\ntags indicating whether each word in NLQ is a table,\ncolumn or value) and (ii) schema tags (i.e., names of the\nschema elements found such as table or column names\nfor each word having a valid type tag) output by DB-\nTagger, and (iii) the input query.\nUtilizing the above-mentioned inputs, the SQL trans-\nlation algorithm has 4 main components, which are (i)\nSchema Graph Extraction, (ii) Join-Path Inference, (iii)\nWhere Clause Completion and (iv) Heuristics for Ag-\ngregate Queries. Each component is explained in detail\nin the following subsections.\n5.1 Schema Graph Extraction\nAs the ﬁrst step of the translation algorithm, we con-\nstruct a schema graph out of the database upon which\nthe NLQ is issued. In the graph, each node represents\neither a table or an attribute in the database. Each con-\nnection between a pair of nodes in the graph is an undi-\nrected edge, connecting table and attribute nodes to\nrepresent their has-A relations. An edge in the schema\ngraph can only be between two diﬀerent types of nodes\n(i.e., between a table and an attribute).\nAlgorithm 1 shows how to extract a simple schema\ngraph from a relational database assuming that foreign\nkeys and primary keys have the same name. This as-\nsumption can be relaxed by adding edges between for-\neign keys and primary keys if the given database schema\ncontains tables having diﬀerent name primary-foreign\npairs, or multiple foreign keys referencing the same pri-\nmary key, or self references. An example schema graph\nextracted from the type and schema tags output by\nDBTagger for the example NLQ given in Section 1 is\ndepicted in Figure 4.\nExtractJoinRelation (G, T)\nInput\n: Database Graph G and list of tables T\nOutput: Graph paths that contains SQL join\ninformation\njoinPath ←∅;\ncandidate ←null;\nforeach table ti ∈T do\nforeach table tj ∈T do\nif ti ̸= tj then\npaths ←\nfindShortestPaths(G, ti, tj)\nforeach path p ∈paths do\nif T ⊆p then\nreturnPaths.append(p);\nreturn returnPaths;\nelse\nmissingTables ←T \\ p;\nif candidate == null then\ncandidate ←\n(p, missingTables);\nelse\nif\nlength(missingTables) <\nlength(candidate1)\nthen\ncandidate ←\n(p, missingTables);\nend\nend\nend\nreturnPaths.append(candidate0);\nforeach table t ∈candidate1 do\npaths ←\nfindShortestPaths(G, candidate00, t);\nforeach path ∈paths do\nlistReduced ←False;\nforeach table t2 ∈candidate1 do\nif t2 ̸= t ∧t2 ∈path then\ncandidate1.remove(t2);\nlistReduced ←True;\nend\nif listReduced then\nreturnPaths.append(path);\nend\nend\nreturn returnPaths;\nAlgorithm 2: Inferring Shortest Join-Path\n5.2 Join-Path Inference\nAfter constructing the schema graph, we try to ﬁnd\nthe shortest way of combining all the nodes represent-\ning schema elements (i.e., tables and attributes). Recall\nthat we have type and schema tags output by DBTag-\nger to work with. We use Dijkstra’s shortest path al-\ngorithm to ﬁnd the in-between nodes required to infer\nthe join-path.\nThe entire algorithm is given in Algorithm 2. Using\nthe type and schema tags output by DBTagger, a set\nof tables T is created containing all the related tables\nfor the given NLQ. T is composed of found tables (i.e.,\n10\nArif Usta et al.\nFig. 4: An example schema graph consisting of tables,\ncolumns and their relations\nname of the tables for those words that have TABLE\nor TABLEREF as type tag), and tables of found at-\ntributes (i.e., name of the columns for those words that\nhave ATTR or ATTRREF or VALUE as type tag). Af-\nter that, T is given as an input to Algorithm 2 with the\nschema graph, and the shortest paths that contain and\njoin the tables in T are found. An example join-path\nfound for the given NLQ is depicted in Figure 4. In\nthe path, each consecutive nodes connected through an\nattribute node exhibits a join condition (e.g., director\nand directed by tables require a join through attribute\ndid).\n5.3 Where Clause Completion\nThe next step is to construct WHERE conditions of\nSQL. The process starts with gathering the outputs of\ntype and schema tags from DBTagger. A two dimen-\nsional array M is created where Mi,1 contains the ith\nquery token, Mi,2 contains type tag of the ith token, and\nMi,3 contains the schema tag of the ith token. Following\nthat, a pre-processing step is applied on the array M to\nsmooth out consecutive tokens that have the same map-\nping information by merging them together. In partic-\nular, the tokens that have VALUE as a type tag inhibit\nWHERE conditions to focus. For each list of consecu-\ntive tokens with V ALUE mapping, a pair of the query\ntoken (e.g., House of Cards) and column name of the\nrespective table (e.g., title column of the tv series table)\nis created and added to where conditions of SQL. Al-\ngorithm 3 shows how WHERE conditions are extracted\nfrom the given list M.\nExtractWhereConditions (M)\nInput\n: Two dimensional array M, containing\nthe NLQ and keyword mapping\ninformation\nOutput: SQL WHERE conditions\nwhereConditions\nwhereConditions ←∅;\nM ←mergeConsecutiveMappings(M) ;\n// Merges multi-word entities to a single\nmapping e.g: Brad Pitt\nforeach token k1i, k2i, k3i ∈M1, M2, M3 do\nif k2i is VALUE then\nwhereConditions.add(k1i, k3i) ;\n// k1i\nand k3i contains the keyword and\ncolumn information of that keyword\nrespectively\nend\nreturn whereConditions;\nAlgorithm 3: Extraction of SQL WHERE Condi-\ntions\nExtractAggregateClause (M, prevWindow)\nInput\n: Two dimensional list M containing\nnatural language query and keyword\nmapping information, prevWindow\nparameter for aggragete keyword\nsearch in sentence\nOutput: SQL AGGREGATE Clause\nSUMKeywords ←getSUMKeywords();\nCOUNTKeywords ←getCOUNTKeywords();\nAV GKeywords ←getAV GKeywords();\nforeach token k1i, k2i, k3i ∈M1, M2, M3 do\nif k2i ∈\n[TABLE, TABLEREF, ATTR, ATTRREF]\nthen\nforeach token\nk1j ∈[M1i−prevW indow, ..., M1i] do\nif k1j ∈SUMKeywords then\nreturn (SUM, k1i, k2i, k3i);\nif k1j ∈COUNTKeywords then\nreturn (COUNT, k1i, k2i, k3i);\nif k1j ∈AV GKeywords then\nreturn (AV G, k1i, k2i, k3i);\nend\nend\nreturn None\nAlgorithm 4: Extraction of SQL AGGREGATE\nClause\n5.4 Heuristics for Aggregate Queries\nFinally, we used a simple and eﬀective technique to de-\ntect potential aggregate operations for the constructed\nSQL. There are some speciﬁc keywords -such as total,\nmany, count etc.- that imply certain aggregate oper-\nations. Using these keywords, we deﬁne keyword sets\nfor each aggregate operation and perform a search in\nthe NLQ for potential aggregate keywords. Algorithm\n4 shows the outline of the performed search. After re-\ntrieving the mapping output from DBTagger, we select\nthe words that have TABLE, TABLEREF, ATTR, or\nxDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph\n11\nTable 2: Statistics of the databases used\nDatabase\nProperties (#)\nimdb\nmas\nyelp\nentity tables\n6\n7\n2\nrelation tables\n11\n5\n5\ntotal tables\n17\n12\n7\ntotal attributes\n55\n28\n38\nnonPK-FK attributes\n14\n7\n16\ntotal tags\n31\n19\n20\nqueries\n131\n599\n128\ntokens in queries\n1250\n4483\n1234\nATTRREF as the type tag as our candidates for aggre-\ngation. For each keyword set, we search the words that\nappear before our candidates. If we ﬁnd a matching\nkeyword, we return the candidate keyword, its map-\nping information, and the matching aggregate opera-\ntion. If no matching is found, the algorithm returns\nNone, implying that no aggregation should be applied.\nFor searching the previous words of the token ki, we de-\nﬁne a window size prevWindow and perform the key-\nword search for the words that are inside this window,\nnamely [ki−prevW indow, ..., ki−2, ki−1].\n6 Experimental Setup\n6.1 Datasets\nIn our experiments we used yelp, imdb [43], and mas\n[27] datasets which are heavily used in many NLIDB\nrelated works by the database community [27, 34, 43, 2].\nThe statistics about each dataset for which anno-\ntation is done are shown in Table 2. In Table 2 (re-\nferring to Figure 4), entity tables refer to main tables\n(e.g., Movie), relation tables refer to hub tables that\nstore connections between entity tables (e.g., cast, writ-\nten by), nonPK-FK attributes refer to attributes in any\ntable that is neither PK nor FK (e.g., gender in People\ntable), and ﬁnally total tags refer to a unique number of\ntaggings extracted from that particular schema depend-\ning on the above-mentioned values. Final schema tags\nof a particular database are determined by composing\ntable names and names of the nonPK-FK attributes in\naddition to COND and OTHER. In the last two rows\nof Table 2, we show the number of annotated NL ques-\ntions, referred to as queries, and the number of total\nwords inside these queries, referred to as tokens.\n6.2 Quantitative Evaluation of xDBTagger\n6.2.1 Keyword Mapping Evaluation\nIn order to train DBTagger, the keyword mapper for the\npipeline, we ﬁrst split the datasets into train-validation\nsets with a 5−1 ratio, respectively, to be used for tuning\ntask weights. For models trained on multiple tasks, we\nused 0.1 −0.2 −0.7 as tuned weights for POS, Type,\nand Schema tasks, respectively.\nWe train our deep neural models using the back-\npropagation algorithm with two diﬀerent optimizers;\nnamely Adadelta [49] and Nadam [11]. We start the\ntraining with Adadelta and continue with Nadam. We\nfound that using two diﬀerent optimizers resulted bet-\nter in our problem. For both shared and unshared bi-\ndirectional GRUs, we use 100 units and apply dropout\n[19] with the value of 0.5, including recurrent inner\nstates as well. For training, the batch size is set to\n32 for all datasets. Parameter values chosen are sim-\nilar to that reported in the study [26] (the state-of-the-\nart NER solution utilizing deep neural networks), such\nas the dropout and batch size values. We measure the\nperformance of each neural model by applying cross-\nvalidation with 6-folds. All the results reported are the\naverage test scores of 6-folds. During inference, we dis-\ncard POS and Type task results and use only Schema\n(ﬁnal) tasks to measure scores.\nWe implemented three diﬀerent unsupervised ap-\nproaches utilized in the state-of-the-art NLIDB works\nfor the keyword mapping task as baselines to compare\nwith DBTagger. We implemented sql querying over database\ncolumn approaches (regex and full-text search), which\nis preferred in NALIR [27]. We implemented a well-\nknown tf-idf baseline for exact matching by construct-\ning an inverted index over unique database values present,\nas in the work ATHENA [34]. We also implemented a\nsemantic similarity matching approach in which pre-\ndeﬁned word embeddings are used. This approach is ex-\nercised by Sqlizer [43]. In addition to these conventional\nunsupervised solutions, we also implemented TaBERT\n[45], a pre-trained language model utilizing transformer\narchitecture to compare with our proposed solution. We\ncategorize the keyword mapping task as relation match-\ning and non-relation matching. The former mapping\nrefers to matching for table or column names, and the\nlatter refers to matching for database values.\n- tf-idf: Similar to ATHENA [34], for each unique value\npresent in the database, we ﬁrst create an exact match-\ning index and then perform tf-idf for tokens in the\nNLQ. In case of matches to multiple columns, the\ncolumn with the biggest tf value is chosen as match-\ning. To handle multi-word keywords, we use n-grams\n12\nArif Usta et al.\nof tokens up to n = 3. For relation matching, we\nused lexical similarity based on the Edit Distance al-\ngorithm.\n- NALIR: NALIR [27] uses WordNet, a lexical database\nin which synonyms are stored for relation match-\ning. They calculate similarity for tokens present in\nthe NLQ over WordNet, and determine a matching\nif the similarity is bigger than a manually deﬁned\nthreshold. For non-relation matching, for each to-\nken present in the NLQ, it utilizes regex or full-text\nsearch queries over each database column whose type\nis text. In case of matches to multiple columns, the\ncolumn which returns more rows, as a result, is cho-\nsen as matching. For fast retrieval, we limit the num-\nber of rows returned from the query to 2000, as in\nthe implementation of NALIR.\n- word2vec: For each unique value present in the data-\nbase, cosine similarity over tokens in the NLQ is ap-\nplied to ﬁnd mappings using pre-deﬁned word2vec\nembeddings. The matching with the highest similar-\nity over a certain threshold is chosen.\n- TaBERT: TaBert [45] is a transformer-based en-\ncoder which generates dynamic word representations\n(unlike word2vec) using database content. The ap-\nproach also generates column encodings for a given\ntable, which makes it an applicable keyword map-\nper for non-relation matching by performing cosine\nsimilarity over both encodings. For a particular to-\nken, matching with the maximum similarity over a\ncertain threshold is chosen.\nEﬀectiveness Comparison\nFor a fair comparison, we do not apply any pre or\npost-processing over the NL queries or use an exter-\nnal source of knowledge, such as a keyword parser or\nmetadata extractor. Results are shown in Table 3. Each\npair of scores represents token-wise accuracy for rela-\ntion and non-relation matching. For TaBERT, we only\nreport for non-relation matching, because the approach\nis not applicable to relation matching.\nDBTagger outperforms unsupervised baselines in each\ndataset signiﬁcantly, by up to 31% and 65% compared\nto best counterpart for relation and non-relation match-\ning, respectively. For relation matching, the results of\nall approaches are similar to each other except the word2vec\nmethod for the mas dataset. The main reason for such\npoor performance is that the mas dataset has column\nnames such as venueName for which word2vec cannot\nproduce word representations, which radically reduces\nthe chances of semantic matching.\ntf-idf gives promising results on the yelp dataset,\nwhereas it fails on the imdb and mas datasets for non-\nrelation matching. This behavior is due to the pres-\nence of ambiguous values (the same database value in\nTable 3: Accuracy scores of keyword mappers for rela-\ntion and non-relation matching\nDatabase\nBaseline\nimdb\nmas\nyelp\ntf-idf\n0.594-0.051\n0.734-0.084\n0.659-0.557\nNALIR\n0.574-0.103\n0.742-0.476\n0.661-0.188\nword2vec\n0.625-0.093\n0.275-0.379\n0.677-0.269\nTaBERT\nNA-0.251\nNA-0.094\nNA-0.114\nDBTagger\n0.908-0.861\n0.964-0.950\n0.947-0.923\nmultiple columns) and not being able to ﬁnd a match\nfor values having more than three words. For the imdb\ndataset, none of the baselines performs well for non-\nrelation matching. The imdb dataset has entity-like val-\nues that are comprised of multiple words, such as movie\nnames, which makes it impossible for semantic match-\ning approaches to generate meaningful representations\nto perform similarity. NALIR’s approach of querying\nover the database has diﬃculties for the imdb and yelp\ndatasets since the approach does not solve ambiguities\nwithout user interaction.\nTaBERT performs poorly for all datasets for the\nnon-relation matching task, which we believe is due to\ntwo reasons. Firstly, TaBERT has its own tokenizer,\nwhich relies on BERT base. The tokenizer tries to deal\nwith out-of-vocabulary tokens by breaking the token\ninto sub-words that have representations. This approach\nmight be useful for a language model; however, it is\nproblematic in the keyword mapping setup since the\nvalues present in the databases are domain-speciﬁc, which\nare likely to not occur in the general corpus data used to\ntrain such transformers. Also, databases such as imdb,\nhave many entity-like values such as Eternal Sunshine\nof the Spotless Mind which is comprised of several words.\nSuch keywords appearing in the natural language query\nare therefore divided by the tokenizer into pieces, which\neventually leads to unrelated word representations and,\nthus, non-predictive similarity calculation. The other\nlimitation of TaBERT is its requirement of using co-\nsine similarity. Such an approach requires a manually\ndeﬁned threshold which is not easy to come up with.\nWhen a smaller similarity threshold is picked, chances\nof ﬁndind a true positive increases; however, the model\nbecomes prone to generate false positives as well for\nkeywords that are not related to database elements such\nas stop words and sql speciﬁc words (e.g., the, return,\nﬁnd, minimum).\nWe argue that unsupervised baselines may perform\nreasonably for relation-matching, whereas they fail to\nanswer the challenges raised by non-relation matching.\nThis is due to the ambiguity present in the databases,\nxDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph\n13\n(a) Run Time\n(b) Memory Usage\nFig. 5: Run Time and Memory Usage of state-of-the-art keyword mapping approaches\nsuch as having values that occur in multiple tables (e.g.,\n”Matt Damon” may appear in both actor and direc-\ntor tables) and domain-speciﬁc values that are not cov-\nered in word embeddings (e.g., word2vec and TaBERT)\ntrained on general corpus data.\nEﬃciency Comparison\nEﬃciency is one of the most important properties\nof a good keyword mapper to have to be deployable in\nonline interfaces. Therefore, the run-time performance\nof keyword mapping approaches mentioned in Section\n7.3.1 is also evaluated.\n- NALIR: We analyze both querying over database\ncolumn approaches used in NALIR [27], named as\nq regex and q ftext, which use like and match against\noperators respectively. NALIR [27] uses q regex ap-\nproach for tables having less than 2000 rows and\nq ftext for tables having more rows.\n- tf-idf: Similar to the indexing strategy exercised in\nAT-HENA [34], we implemented an exact matching\nstrategy, using an inverted index named as inv index,\nbeforehand to avoid querying over the database. The\ninverted index stores each unique value present in the\ndatabase along with its frequency in each candidate\ncollection (i.e., database columns).\n- word2vec: Many works such as Sqlizer [43] make use\nof pre-trained word embeddings to ﬁnd mappings,\nwhich requires keeping the model in the memory to\nperform the task using cosine similarity.\n- tabert on: TaBert [45] requires database content\n(referred to as content snapshot in the paper) to gen-\nerate encodings for both NL tokens and columns. We\ncall this setup tabert online, where the model gener-\nates the content snapshot on the ﬂy, hence online, to\nperform mapping when the query comes.\nTable 4: Overall SQL Query Translation Results\nAccuracy (%)\nxDBTagger\nNALIR+\nNALIR\nimdb\n61.83\n50.00\n38.30\nscholar\n58.96\n40.20\n33.00\nyelp\n69.53\n52.80\n47.20\n- tabert oﬀ: We also use TaBert in oﬄine setup. For\neach table, database content is generated beforehand\nto perform encodings. In this setup, we keep the con-\ntent in the memory to serve the query faster.\nWe measured the time elapsed for a single query\nto extract tags and the memory consumption needed\nto perform mapping for each approach. We also run\neach experiment with a diﬀerent number of row val-\nues to capture the impact of the database size. Fig-\nure 5 presents run time and memory usage analysis of\nkeyword mappers. DBTagger outputs the tags faster\nthan any other baseline, and it is scalable to much big-\nger databases. However, q regex, q ftext, tabert on, and\nword2vec do not seem applicable for bigger tables hav-\ning more than 10000 rows. The tf-idf technique has a\nnice balance between run-time and memory usage, but\nit is limited in terms of eﬀectiveness (Table 3). tabert-\noﬀperforms the tagging in a reasonable time, yet it\nrequires huge memory consumption, especially for big-\nger tables, and its eﬀectiveness as a candidate keyword\nmapper is not suﬃcient.\n6.2.2 Query Translation Results\nThe numbers of the queries for the three datasets we\nused in our experiments are provided in Table 2. Also,\nrecall that, we applied 6-fold cross-validation (i.e., leav-\n14\nArif Usta et al.\nTable 5: Translation Accuracy Results of xDBTagger According to Categorization of the Queries\nNon-nested\nNested\nSelect-Join (No-Aggregation)\nHaving Aggregation\nOverall\nSingle Table\nMultiple Table\nOverall\nimdb\n88.89(9)\n68.60(86)\n70.53(95)\n63.64(22)\n69.23(117)\n14\nscholar\n100.00(2)\n86.96(69)\n87.32(71)\n43.59(39)\n71.81(110)\n24\nyelp\n60.00(5)\n83.61(61)\n81.81(66)\n63.64(55)\n73.55(121)\n7\nTable 6: Accuracy of WHERE Conditions of the\nQueries with Aggregate Operations\nAggregate Queries\nwith Group By\nAggregate Queries\nwithout Group By\nOverall\nimdb\n100.00 (6)\n87.50 (16)\n91.27(22)\nscholar\n88.89 (18)\n90.48 (21)\n89.53(39)\nyelp\n63.64 (11)\n79.55 (44)\n76.80(55)\ning 1 fold out for test and using the other 5 folds for\ntraining the model) to train our keyword mapper. In or-\nder to evaluate query translation results, we performed\nthe translation pipeline for each test fold left out from\nthe training model for yelp and imdb. We used only\n1 test fold for mas dataset to make the ﬁnal number\nof test queries to be similar to each other. We man-\nually evaluated the translated SQL queries, counting\nas CORRECT if and only if the translated query is\nthe same as the ground truth in terms of SQL seman-\ntics and correct in SQL syntax; and INCORRECT\notherwise. Hence, we report binary accuracy results of\nxDBTagger.\nIn order to evaluate the comparative performance\nof xDBTagger, we used two diﬀerent pipeline-based so-\nlutions; namely NALIR [27] and TEMPLAR [2] (an\nenhanced version of the NALIR, referred as NALIR+,\nutilizing query logs to detect keyword mappings of the\ntokens in NLQ). The reason why we choose these two\nbaselines is 3-fold. Firstly, both studies reported accu-\nracy results of their translation pipeline for the same set\nof three datasets we used in our work. Secondly, both\nare pipeline-based solutions; that is, they are comprised\nof sub-solutions for each step in the translation pipeline\nsimilar to xDBTagger. Lastly, TEMPLAR [2] tries to\nenhance the translation pipeline of an existing NLIDB\nsolution (e.g., NALIR) by solely focusing on keyword\nmappings. Similarly, xDBTagger utilizes the keyword\nmappings output by DBTagger [37] in the translation\npipeline.\nThe overall translation accuracy results for xDB-\nTagger, along with the two baselines explained above,\nare provided in Table 4. Accuracy results of the base-\nlines are taken from the TEMPLAR study [2]. xDB-\nTagger outperforms both baselines in all three datasets,\nup to 78% and 46% compared to NALIR and NALIR+,\nrespectively. Considering eﬃciency (see Figure 5 for ref-\nerence) of the keyword mapper utilized in xDBTagger,\nsimplicity of the translation algorithm explained in Sec-\ntion 5 and having fully explainable end-to-end transla-\ntion pipeline, the accuracy of xDBTagger stands out\neven more compared to their counterparts.\nOne of the limitations of the translation pipeline of\nxDBTagger is that it fails to translate certain types of\nqueries correctly. The translation algorithm is not able\nto translate the queries requiring nested SQL queries.\nThe results provided in Table 4 include those queries\nas well; hence less accuracy is observed overall. An-\nother limitation is that queries requiring aggregate op-\nerations are diﬃcult to translate (i.e., prone to mis-\ntranslation) for xDBTagger Although we implemented\nheuristics (see Section 5.4 for reference) to address these\nqueries, most of the incorrectly translated queries fall\nunder this category.\nIn order to further show the eﬃcacy of xDBTagger\nfor other types of queries, we categorized the queries\nreﬂecting their diﬃculty in terms of translation and re-\nport accuracy for each category. The results are pre-\nsented in Table 5. The numbers in parenthesis repre-\nsent the number of queries falling under that particular\ncategory. The category Nested represents the number\nof queries xDBTagger could not translate due to the\ntranslation requiring a nested SQL query. Most of the\nqueries fall under the category Select-Join with Multiple\nTables, where xDBTagger performs most competitively\nacross all categories. Accuracy results in Table 5 also\nindicate that heuristics for aggregation queries are eﬀec-\ntive at translating more than half of the queries under\nthat category on the average.\nWe further manually evaluated WHERE conditions\nof the queries having aggregate operations and reported\naccuracies in Table 6. As it can be seen, xDBTagger is\nable to extract WHERE conditions fairly well with\n90% average accuracy for imdb and scholar, and 76%\naccuracy for yelp. Although xDBTagger performs the\nworst for queries having aggregate operations in terms\nof full translations (Table 5), it still extracts correct\nWHERE conditions for both utterances found in the\nxDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph\n15\nNLQ and the join-path, which is reported to be the\nmost challenging part of the translation in [42]. Results\nshow that extracting correct WHERE conditions for\nthe translation is one of the main strengths of xDBTag-\nger.\n6.3 Explainable User Interface of xDBTagger\nWe constructed a simple, single-page web application\nwhere users can input a natural language query into\nour NLIDB pipeline and retrieve the translated SQL.\nThis web application is developed using ﬂask micro-\nframework and javascript. Figure 6 depicts interface\ncomponents through the diﬀerent stages of the transla-\ntion pipeline. The pipeline has the following three main\nsteps that we explain to the user separately;\n1. Finding type and schema mappings of the tokens in\nthe NLQ using DBTagger. As discussed in Section\n3, DBTagger is a sequence-tagging deep learning\nmodel, which behaves as a black-box mechanism,\ni.e., it only classiﬁes the tokens in the NLQ to the\nmost probable class among the possible candidates\nbased on probabilities it derives, yet it does not pro-\nvide why it makes a particular decision. Therefore,\nwe need to explain how DBTagger maps tokens in\nthe NLQ to the schema elements in the database.\nTo this end, we provide two diﬀerent interface com-\nponents. First, we list all the type and schema map-\npings output by DBTagger for each token in a table\n(6c) to make the decisions made by DBTagger trans-\nparent. Second, we visually explain why DBTagger\ncame up with a particular pair of type and schema\nmapping for a token in a pop-up (6e,f) using LIME\nwrapper (explained in Section 4) to make it easier\nfor the user to comprehend the decisions made by\nDBTagger.\n2. Extracting the join path necessary to access tables\nthat include utterances found in the previous step.\nWe ﬁrst visually draw the schema graph on the\npanel (depicted in Figure 6b) so that the user can\nbetter understand the schema underlying the database\nand the relational connections it inherits. Moreover,\nwe also highlight the nodes and the edges visited\nalong the path on the graph to construct the JOIN\nclause to explain how certain tables that are not\npresent in predictions panel (6c) (i.e., intermediate\ntables completing the join) appear in the ﬁnal SQL\ntranslation.\n3. constructing the SQL by forming the WHERE clauses\nand applying post-processing heuristics to handle\ncertain group of queries. After ﬁnalizing the SQL\ntranslation, we part-by-part explain how the trans-\nlated SQL is composed in the result panel, shown in\n6d. In particular, we explain why we include each ta-\nble in the FROM clause and each logical expression\nin the WHERE clause.\nNLQ input panel (Figure 6a) allows the user to se-\nlect a database schema from a dropdown list and input\na query to the NLIDB pipeline for the selected schema.\nAfter selecting the schema over which the query is to be\nissued, the schema graph panel displays the extracted\ngraph (i.e., the graph without the highlighted nodes\nand edges, depicted in Figure 4), as explained in Sec-\ntion 5.1. When a query is processed in the pipeline and\na SQL is generated for the translation, we highlight the\nnodes and edges of the graph in blue color, as shown in\nFigure 6b, that are used in any part of the generated\nSQL; (i) tables and their attributes required for the\nWHERE clauses and (ii) tables required for the correct\njoin operation along with their attributes making the\nprimary-foreign key connections.\nIn the predictions panel shown in Figure 6c, we\npresent the type and schema mapping outputs of our\nkeyword mapper inside a table so that the user can\nvisually see how his/her query is predicted by the key-\nword mapper. Furthermore, we used pop-ups to display\nLIME explanations of each word of the query that is not\ntagged as ’O’ to make it easier for the user to compre-\nhend the decision-making behind the keyword mapper\nmodel. When the user clicks on a particular row of the\ntable in Figure 6c, a pop-up (e.g., Figure 6e) is dis-\nplayed containing the explanation for the word visually\nby highlighting the neighboring words that contribute\nthe most, either positively or negatively, when predict-\ning the output.\nFor example, for the query shown in the input panel\nFigure 6a, we provided the explanation pop-ups for to-\nkens ”House” and ”Netﬂix” in Figure 6e and Figure 6f,\nrespectively. In Figure 6e, there are two labels named\n’NOT TV SERIES.TITLE’ and ’TV SERIES.TITLE’,\nwhich correspond to categories of tokens contributing to\nthe label of ’tv series.title’ negatively and positively, re-\nspectively. Below both labels, on each side, there are to-\nkens from the NLQ with a contribution score associated\nwith them. A positive contribution means that the to-\nken increases the prediction probability of the explained\ntoken for the given class (i.e., TV SERIES.TITLE), and\na negative contribution decreases that probability.\nFor the token in Figure 6e, we can see that the word\n’series’ in the NLQ has the highest positive contribu-\ntion marginally compared to other tokens. This means\nthat the word ’series’ is the most inﬂuential neighbor-\ning word when determining the mapping classiﬁcation\nof the token ’House’. Similarly, Figure 6f gives the ex-\nplanation for the word ’Netﬂix’ in the given NLQ. The\n16\nArif Usta et al.\nFig. 6: Interface components of xDBTagger; (a) NLQ input panel, (b) schema graph panel on which required join\npath is drawn, (c) predictions panel for keyword mapper, (d) result panel containing the translated SQL query\nalong with its explanations, (e,f) explanation pop-ups for each token in NLQ. Components b,c,d,e and f contain\nvisual and/or textual explanations for the algorithms utilized through the translation pipeline.\nexplanation shows that the word itself has the highest\npositive contribution, which is expected since the entity\nis self-expressive and should infer the name attribute of\na particular entry in the company table.\nThe result panel, shown in Figure 6d, presents the\ngenerated SQL query and the explanation of how it is\ncomposed for the given NLQ. We divide the generated\nSQL statement into parts and explain why we include\neach part in the ﬁnal statement so that users with a\nless technical background in SQL can better compre-\nhend how the ﬁnal SQL is composed. In particular, we\nexplain why we include tables and logical expressions in\nthe FROM and WHERE clauses, respectively. For in-\nstance, for the example NLQ given in Figure 6a, there\nxDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph\n17\nare three diﬀerent explanations for why a certain ta-\nble is included in the FROM clause. ’tv series’ table is\nincluded because there are tokens whose schema map-\npings are title attribute of the ’tv series’ table (8-10th\nrows in the prediction table shown in Figure 6c). Di-\nrector and copyright tables appear thanks to the pre-\ndictions of DBTagger, whereas ’directed by’ is present\nbecause it is a required table to connect ’tv series’ and\n’director’ through join.\nFor each logical expression we put in the ﬁnalized\nSQL statement, we provide an explanation as well. Broadly,\nwe divide the explanations into two categories. If a logi-\ncal expression is to provide a connection between tables\nin the schema graph, we say it is required for the join\ncondition. If we detect a type mapping of VALUE (e.g.,\n8-10th and 13rd rows in the prediction table shown in\nFigure 6c), we state that a value is detected by DBTag-\nger as shown in the last two rows in the explanation\ntable in Figure 6d.\n7 Related Work\nAlthough the very ﬁrst eﬀort [18] of providing natural\nlanguage interface in databases dates back to multi-\nple decades ago, the popularity of the problem has in-\ncreased due to some recent pipeline-based systems pro-\nposed by the database community, such as SODA [3],\nNALIR [27], ATHENA[34] and SQLizer[43].\nHowever, with the recent advancements in deep neu-\nral networks, the problem of NLIDB has also attracted\nresearchers from the NLP community. [50] provided a\ndataset called WikiSql to the research community work-\ning on NLIDB problem for evaluation. WikiSql is com-\nprised of 26, 531 tables and 80, 654 pairs which can\nbe used for input for the translation problem. Conse-\nquently, many works [50, 42, 44, 21, 46] utilizing encoder-\ndecoder abstraction have been proposed to evaluate\ntheir translation solutions on the WikiSql dataset. How-\never, the dataset only includes schemas with a single\ntable, limiting detailed evaluation of the solutions due\nto simplicity.\nTo remediate this limitation, Spider dataset is pro-\nvided in the work [48] to the community. Many stud-\nies utilizing the pioneer work BERT [10], a pre-trained\nlanguage model based on the transformer [38] archi-\ntecture, have evaluated their solutions on Spider [48]\ndataset. Some works [16, 28] focus on the schema link-\ning process to enrich the input NLQ for better leverag-\ning the schema information. In IRNet [16], the authors\nﬁrst query n-grams of the NLQ over the database el-\nements to ﬁnd candidates and then feed these found\ncandidates to the additional schema encoder, whereas\nLin et al. [28] integrate these found candidates into\nthe input as a serialization technique before encoding.\nRat-SQL[39] proposes a modiﬁed transformer layer to\nleverage schema information better by introducing bias\ntowards the schema for the attention mechanism. In\naddition to these studies, language representation tech-\nniques utilizing BERT such as TaBERT [45] and Grappa\n[8] have been introduced to leverage tabular data spe-\nciﬁc representations in related downstream tasks such\nas NLIDB problem. For a comprehensive survey cover-\ning existing solutions in NLIDB, the reader can refer to\n[1, 23].\nTo the best of our knowledge, there is no hybrid\nsolution utilizing both neural network and rule-based\ntechniques, proposed similar to xDBTagger. Nonethe-\nless, some of the earlier works [44, 46, 16, 28] embracing\nend-to-end neural network approaches focused more on\nenriching the input by trying to map tokens in the NLQ\nto database values similar to our keyword mapper, DB-\nTagger. However, as shown in the work [37], such so-\nlutions proposing ad-hoc querying over database tables\nto ﬁnd candidate mappings are not eﬃcient and not\nscalable to bigger databases unlike xDBTagger.\nThere have been previous studies [24, 30, 41, 9] pro-\nposed in line with interpretable interfaces to databases.\nHowever, such solutions rather focus on providing ad-\nditional information for the SQL query results in the\nform of summarized texts or snippets exploiting signals\nof the tuples returned by the result SQL. In this work,\nour goal is not to explain query results but to explain\nthe decisions that lead to the result SQL for each step\nin the translation pipeline. To our knowledge, xDBTag-\nger is the ﬁrst NLIDB system exercising XAI principles\nto explain how the translation is performed.\n8 Conclusion\nIn this work, we presented xDBTagger, the ﬁrst end-\nto-end explainable NLIDB solution to translate NLQs\ninto their counterpart SQLs. xDBTagger is a hybrid so-\nlution taking advantage of both deep learning and rule-\nbased approaches. First, we detect keyword mappings\nof the tokens in the input NLQ using a novel deep learn-\ning model trained in a multi-task learning setup. Next,\nwe explain the decisions for the keyword mappings us-\ning a modiﬁed version of a state-of-the-art XAI solu-\ntion LIME [33]. We visually illustrate the importance\nof each surrounding word for each mapping by high-\nlighting their contributions which can be either positive\nor negative. In addition, we draw the schema graph to\nvisualize better the database schema over which the\nquery is issued. We also color the nodes representing\ntables and attributes in the graph to explain how the\nrequired join conditions in the result SQL are extracted.\n18\nArif Usta et al.\nFinally, we explain each part of the result SQL to the\nuser by providing the reason why we need that particu-\nlar part given the input NLQ. Our quantitative exper-\nimental results indicate that in addition to being fully\nexplainable, xDBTagger is eﬀective in terms of transla-\ntion accuracy and more preferable compared to other\npipeline-based solutions in terms of eﬃciency.\nAcknowledgements This research is supported by The Sci-\nentiﬁc and Technological Research Council of T¨urkiye\n(T¨UB˙ITAK) under the grant no 118E724.\nReferences\n1. Aﬀolter K, Stockinger K, Bernstein A (2019)\nA\ncomparative\nsurvey\nof\nrecent\nnatural\nlan-\nguage interfaces for databases. The VLDB Journal\n28(5):793–819\n2. Baik C, Jagadish HV, Li Y (2019) Bridging the se-\nmantic gap with sql query logs in natural language\ninterfaces to databases. In: 2019 IEEE 35th Inter-\nnational Conference on Data Engineering (ICDE),\npp 374–385\n3. Blunschi L, Jossen C, Kossmann D, Mori M,\nStockinger K (2012) Soda: Generating sql for busi-\nness users. Proc VLDB Endow 5(10):932–943\n4. Bojanowski P, Grave E, Joulin A, Mikolov T (2017)\nEnriching word vectors with subword information.\nTransactions of the Association for Computational\nLinguistics 5:135–146\n5. Caruana R (1997) Multitask learning. Machine\nlearning 28(1):41–75\n6. Chung J, Gulcehre C, Cho K, Bengio Y (2015)\nGated feedback recurrent neural networks. In: Pro-\nceedings of the 32nd International Conference on\nInternational Conference on Machine Learning -\nVolume 37, JMLR.org, ICML’15, p 2067–2075\n7. Collobert R, Weston J, Bottou L, Karlen M,\nKavukcuoglu K, Kuksa P (2011) Natural language\nprocessing (almost) from scratch. J Mach Learn Res\n12(null):2493–2537\n8. Deng X, Awadallah AH, Meek C, Polozov O, Sun\nH, Richardson M (2021) Structure-grounded pre-\ntraining for text-to-SQL. In: Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Association for Com-\nputational Linguistics, Online, pp 1337–1350\n9. Deutch D, Frost N, Gilad A (2020) Explaining nat-\nural language query results. The VLDB Journal\n29(1):485–508\n10. Devlin J, Chang MW, Lee K, Toutanova K (2019)\nBERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In: Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), Association for Com-\nputational Linguistics, Minneapolis, Minnesota, pp\n4171–4186\n11. Dozat T (2016) Incorporating nesterov momentum\ninto adam. In: ICLR Workshop, JMLR.org\n12. Doˇsilovi´c FK, Brˇci´c M, Hlupi´c N (2018) Explain-\nable artiﬁcial intelligence: A survey. In: 2018 41st\nInternational Convention on Information and Com-\nmunication Technology, Electronics and Microelec-\ntronics (MIPRO), pp 0210–0215\n13. Graves A, Mohamed A, Hinton G (2013) Speech\nrecognition with deep recurrent neural networks.\nIn: 2013 IEEE International Conference on Acous-\ntics, Speech and Signal Processing, pp 6645–6649\n14. Gregor S, Benbasat I (1999) Explanations from in-\ntelligent systems: Theoretical foundations and im-\nplications for practice. MIS quarterly pp 497–530\n15. Gunning D, Aha D (2019) Darpa’s explainable\nartiﬁcial intelligence (xai) program. AI magazine\n40(2):44–58\n16. Guo J, Zhan Z, Gao Y, Xiao Y, Lou JG, Liu T,\nZhang D (2019) Towards complex text-to-SQL in\ncross-domain database with intermediate represen-\ntation. Association for Computational Linguistics,\nFlorence, Italy, pp 4524–4535\n17. Hayes-Roth F, Jacobstein N (1994) The state\nof\nknowledge-based\nsystems.\nCommun\nACM\n37(3):26–39\n18. Hendrix GG, Sacerdoti ED, Sagalowicz D, Slocum\nJ (1978) Developing a natural language inter-\nface to complex data. ACM Trans Database Syst\n3(2):105–147\n19. Hinton GE, Srivastava N, Krizhevsky A, Sutskever\nI, Salakhutdinov R (2012) Improving neural net-\nworks by preventing co-adaptation of feature de-\ntectors. ArXiv abs/1207.0580\n20. Hochreiter S, Schmidhuber J (1997) Long short-\nterm memory. Neural computation 9:1735–80\n21. Huang PS, Wang C, Singh R, Yih Wt, He X (2018)\nNatural language to structured query generation\nvia meta-learning. Association for Computational\nLinguistics, New Orleans, Louisiana, pp 732–738\n22. Iyer S, Konstas I, Cheung A, Krishnamurthy J,\nZettlemoyer L (2017) Learning a neural semantic\nparser from user feedback. In: Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), Asso-\nciation for Computational Linguistics, Vancouver,\nCanada, pp 963–973\nxDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph\n19\n23. Kim H, So BH, Han WS, Lee H (2020) Natural\nlanguage to sql: Where are we today? Proc VLDB\nEndow 13(10):1737–1750\n24. Koutrika G, Simitsis A, Ioannidis YE (2010) Ex-\nplaining structured queries in natural language.\nIn: 2010 IEEE 26th International Conference on\nData Engineering (ICDE 2010), pp 333–344, DOI\n10.1109/ICDE.2010.5447824\n25. Laﬀerty JD, McCallum A, Pereira FCN (2001)\nConditional random ﬁelds: Probabilistic models\nfor segmenting and labeling sequence data. In:\nProceedings of the Eighteenth International Con-\nference on Machine Learning, Morgan Kaufmann\nPublishers Inc., San Francisco, CA, USA, ICML\n’01, p 282–289\n26. Lample\nG,\nBallesteros\nM,\nSubramanian\nS,\nKawakami K, Dyer C (2016) Neural architectures\nfor named entity recognition. In: Proceedings of the\n2016 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, Association for\nComputational Linguistics, San Diego, California,\npp 260–270\n27. Li F, Jagadish HV (2014) Constructing an in-\nteractive natural language interface for relational\ndatabases. Proc VLDB Endow 8(1):73–84\n28. Lin XV, Socher R, Xiong C (2020) Bridging tex-\ntual and tabular data for cross-domain text-to-SQL\nsemantic parsing. In: Findings of the Association\nfor Computational Linguistics: EMNLP 2020, As-\nsociation for Computational Linguistics, Online, pp\n4870–4888\n29. Manning CD, Surdeanu M, Bauer J, Finkel J,\nBethard SJ, McClosky D (2014) The Stanford\nCoreNLP natural language processing toolkit. In:\nAssociation for Computational Linguistics (ACL)\nSystem Demonstrations, pp 55–60\n30. M¨uller T, Grust T (2015) Provenance for sql\nthrough abstract interpretation: Value-less, but\nworthwhile. Proc VLDB Endow 8(12):1872–1875\n31. ¨Ozcan F, Quamar A, Sen J, Lei C, Efthymiou V\n(2020) State of the art and open challenges in nat-\nural language interfaces to data. In: Proceedings of\nthe 2020 ACM SIGMOD International Conference\non Management of Data, Association for Comput-\ning Machinery, New York, NY, USA, SIGMOD ’20,\np 2629–2636\n32. Poulin B, Eisner R, Szafron D, Lu P, Greiner R,\nWishart DS, Fyshe A, Pearcy B, MacDonell C, An-\nvik J (2006) Visual explanation of evidence with\nadditive classiﬁers. In: Proceedings of the National\nConference on Artiﬁcial Intelligence, Menlo Park,\nCA; Cambridge, MA; London; AAAI Press; MIT\nPress; 1999, vol 21, p 1822\n33. Ribeiro MT, Singh S, Guestrin C (2016) ”why\nshould I trust you?”: Explaining the predictions\nof any classiﬁer. In: Proceedings of the 22nd ACM\nSIGKDD International Conference on Knowledge\nDiscovery and Data Mining, San Francisco, CA,\nUSA, August 13-17, 2016, pp 1135–1144\n34. Saha D, Floratou A, Sankaranarayanan K, Min-\nhas UF, Mittal AR, ¨Ozcan F (2016) Athena: An\nontology-driven system for natural language query-\ning over relational data stores. Proc VLDB Endow\n9(12):1209–1220\n35. Scholak T, Schucher N, Bahdanau D (2021) PI-\nCARD: Parsing incrementally for constrained auto-\nregressive decoding from language models. In: Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, Association\nfor Computational Linguistics, Online and Punta\nCana, Dominican Republic, pp 9895–9901\n36. Sen J, Lei C, Quamar A, ¨Ozcan F, Efthymiou V,\nDalmia A, Stager G, Mittal A, Saha D, Sankara-\nnarayanan K (2020) Athena++: Natural language\nquerying for complex nested sql queries. Proc\nVLDB Endow 13(12):2747–2759\n37. Usta A, Karakayali A, Ulusoy O (2021) Dbtagger:\nMulti-task learning for keyword mapping in nlidbs\nusing bi-directional recurrent neural networks. Proc\nVLDB Endow 14(5):813–821\n38. Vaswani A, Shazeer N, Parmar N, Uszkoreit J,\nJones L, Gomez AN, Kaiser Lu, Polosukhin I (2017)\nAttention is all you need. In: Guyon I, Luxburg UV,\nBengio S, Wallach H, Fergus R, Vishwanathan S,\nGarnett R (eds) Advances in Neural Information\nProcessing Systems, Curran Associates, Inc., vol 30\n39. Wang B, Shin R, Liu X, Polozov O, Richardson M\n(2020) RAT-SQL: Relation-aware schema encoding\nand linking for text-to-SQL parsers. In: Proceed-\nings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, Association for\nComputational Linguistics, Online, pp 7567–7578\n40. Weir N, Utama P, Galakatos A, Crotty A, Ilkhechi\nA, Ramaswamy S, Bhushan R, Geisler N, H¨attasch\nB, Eger S, Cetintemel U, Binnig C (2020) Dbpal:\nA fully pluggable nl2sql training pipeline. In: Pro-\nceedings of the 2020 ACM SIGMOD International\nConference on Management of Data, Association\nfor Computing Machinery, New York, NY, USA,\nSIGMOD ’20, p 2347–2361\n41. Wen Y, Zhu X, Roy S, Yang J (2018) In-\nteractive summarization and exploration of top\naggregate query answers. Proc VLDB Endow\n11(13):2196–2208\n20\nArif Usta et al.\n42. Xu X, Liu C, Song D (2017) Sqlnet: Gener-\nating structured queries from natural language\nwithout reinforcement learning. arXiv preprint\narXiv:171104436 1711.04436\n43. Yaghmazadeh N, Wang Y, Dillig I, Dillig T (2017)\nSqlizer: Query synthesis from natural language.\nProc ACM Program Lang 1(OOPSLA):63:1–63:26\n44. Yavuz S, Gur I, Su Y, Yan X (2018) What it takes\nto achieve 100% condition accuracy on WikiSQL.\nIn: Proceedings of the 2018 Conference on Empir-\nical Methods in Natural Language Processing, As-\nsociation for Computational Linguistics, Brussels,\nBelgium, pp 1702–1711\n45. Yin P, Neubig G, Yih Wt, Riedel S (2020)\nTaBERT: Pretraining for joint understanding of\ntextual and tabular data. In: Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, Association for Computational\nLinguistics, Online, pp 8413–8426\n46. Yu T, Li Z, Zhang Z, Zhang R, Radev D (2018)\nTypeSQL:\nKnowledge-based\ntype-aware\nneural\ntext-to-SQL generation. Association for Computa-\ntional Linguistics, New Orleans, Louisiana, pp 588–\n594\n47. Yu T, Yasunaga M, Yang K, Zhang R, Wang D,\nLi Z, Radev D (2018) SyntaxSQLNet: Syntax tree\nnetworks for complex and cross-domain text-to-\nSQL task. In: Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Pro-\ncessing, Association for Computational Linguistics,\nBrussels, Belgium, pp 1653–1663\n48. Yu T, Zhang R, Yang K, Yasunaga M, Wang D, Li\nZ, Ma J, Li I, Yao Q, Roman S, Zhang Z, Radev D\n(2018) Spider: A large-scale human-labeled dataset\nfor complex and cross-domain semantic parsing and\ntext-to-SQL task. Association for Computational\nLinguistics, Brussels, Belgium, pp 3911–3921\n49. Zeiler MD (2012) Adadelta: An adaptive learning\nrate method. ArXiv abs/1212.5701\n50. Zhong V, Xiong C, Socher R (2017) Seq2sql:\nGenerating structured queries from natural lan-\nguage using reinforcement learning. arXiv preprint\narXiv:170900103 1709.00103\n",
  "categories": [
    "cs.DB",
    "cs.AI",
    "cs.CL",
    "cs.HC",
    "H.2.6; H.5.2; I.2.7"
  ],
  "published": "2022-10-07",
  "updated": "2022-10-07"
}