{
  "id": "http://arxiv.org/abs/2210.04076v1",
  "title": "Robustness of Unsupervised Representation Learning without Labels",
  "authors": [
    "Aleksandar Petrov",
    "Marta Kwiatkowska"
  ],
  "abstract": "Unsupervised representation learning leverages large unlabeled datasets and\nis competitive with supervised learning. But non-robust encoders may affect\ndownstream task robustness. Recently, robust representation encoders have\nbecome of interest. Still, all prior work evaluates robustness using a\ndownstream classification task. Instead, we propose a family of unsupervised\nrobustness measures, which are model- and task-agnostic and label-free. We\nbenchmark state-of-the-art representation encoders and show that none dominates\nthe rest. We offer unsupervised extensions to the FGSM and PGD attacks. When\nused in adversarial training, they improve most unsupervised robustness\nmeasures, including certified robustness. We validate our results against a\nlinear probe and show that, for MOCOv2, adversarial training results in 3 times\nhigher certified accuracy, a 2-fold decrease in impersonation attack success\nrate and considerable improvements in certified robustness.",
  "text": "ROBUSTNESS\nOF UNSUPERVISED REPRESENTATION\nLEARNING WITHOUT LABELS\nAleksandar Petrov\nDepartment of Engineering Science\nUniversity of Oxford\naleks@robots.ox.ac.uk\nMarta Kwiatkowska\nDepartment of Computer Science\nUniversity of Oxford\nmarta.kwiatkowska@cs.ox.ac.uk\nABSTRACT\nUnsupervised representation learning leverages large unlabeled datasets and is com-\npetitive with supervised learning. But non-robust encoders may affect downstream\ntask robustness. Recently, robust representation encoders have become of interest.\nStill, all prior work evaluates robustness using a downstream classiﬁcation task. In-\nstead, we propose a family of unsupervised robustness measures, which are model-\nand task-agnostic and label-free. We benchmark state-of-the-art representation\nencoders and show that none dominates the rest. We offer unsupervised extensions\nto the FGSM and PGD attacks. When used in adversarial training, they improve\nmost unsupervised robustness measures, including certiﬁed robustness. We validate\nour results against a linear probe and show that, for MOCOv2, adversarial training\nresults in 3 times higher certiﬁed accuracy, a 2-fold decrease in impersonation\nattack success rate and considerable improvements in certiﬁed robustness.\n1\nINTRODUCTION\nUnsupervised and self-supervised models extract useful representations without requiring labels.\nThey can learn patterns in the data and are competitive with supervised models for image classiﬁcation\nby leveraging large unlabeled datasets (Zbontar et al., 2021; Chen & He, 2021; He et al., 2020; Chen\net al., 2020b;d;c). Representation encoders do not use task-speciﬁc labels and can be employed for\nvarious downstream tasks. Such reuse is attractive as large datasets can make them expensive to train.\nTherefore, applications are often built on top of public domain representation encoders. However, lack\nof robustness of the encoder can be propagated to the downstream task. Consider the impersonation\nattack threat model in Fig. 1. An attacker tries to fool a classiﬁer that uses a representation encoder.\nThe attacker has white-box access to the representation extractor (e.g. an open-source model) but\nthey do not have access to the classiﬁcation model that uses the representations. By optimizing the\ninput to be similar to a benign input, but to have the representation of a different target input, the\nattacker can fool the classiﬁer. Even if the classiﬁer is private, one can attack the combined system if\nthe public encoder conﬂates two different concepts onto similar representations. Hence, robustness\nagainst such conﬂation is necessary to perform downstream inference on robust features.\nWe currently lack ways to evaluate robustness of representation encoders without specializing for\na particular task. While prior work has proposed improving the robustness of self-supervised\nrepresentation learning (Kim et al., 2020; Jiang et al., 2020; Ho & Vasconcelos, 2020; Chen et al.,\n2020a; Cemgil et al., 2020; Fan et al., 2021; Alayrac et al., 2019; Carmon et al., 2020; Nguyen et al.,\n2022), they all require labeled datasets to evaluate the robustness of the resulting models.\nInstead, we offer encoder robustness evaluation without labels. This is task-agnostic, in contrast to\nsupervised assessment, as labels are (implicitly) associated with a speciﬁc task. Labels can also be\nincomplete, misleading or stereotyping (Stock & Cisse, 2018; Steed & Caliskan, 2021; Birhane &\nPrabhu, 2021), and can inadvertently impose biases in the robustness assessment. In this work, we\npropose measures that do not require labels and methods for unsupervised adversarial training that\nresult in more robust models. To the best of our knowledge, this is the ﬁrst work on unsupervised\nrobustness evaluation and we make the following contributions to address this problem:\n1\narXiv:2210.04076v1  [cs.LG]  8 Oct 2022\nPublic\nencoder\nDog\nPublic\nencoder\nMinimize\nrepresentation\ndivergence\nPublic\nencoder\nPrivate\nclassifer\ntargets\nimpersonators\noriginal\nFigure 1: Impersonation attack threat model. The attacker has access only to the encoder on which\nthe classiﬁer is built. By attacking the input to have a similar representation to a sample from the target\nclass, the attacker can fool the classiﬁer without requiring any access to it. Cats who successfully\nimpersonate dogs under the MOCOv2 representation encoder and a linear probe classiﬁer are shown.\n1. Novel representational robustness measures based between clean and adversarial representa-\ntion divergences, requiring no labels or assumptions about underlying decision boundaries.\n2. A unifying framework for unsupervised adversarial attacks and training, which generalizes\nthe prior unsupervised adversarial training methods.\n3. Evidence that even the most basic unsupervised adversarial attacks in the framework result\nin more robust models relative to both supervised and unsupervised measures.\n4. Probabilistic guarantees on the unsupervised robustness measures based on center smoothing.\n2\nRELATED WORK\nAdversarial robustness of supervised learning\nDeep neural networks can have high accuracy on\nclean samples while performing poorly under imperceptible perturbations (adversarial examples)\n(Szegedy et al., 2014; Biggio et al., 2013). Adversarial examples can be viewed as spurious cor-\nrelations between labels and style (Zhang et al., 2022; Singla & Feizi, 2022) or shortcut solutions\n(Robinson et al., 2021). Adversarial training, i.e. incorporating adversarial examples in the training\nprocess, is a simple and widely used strategy against adversarial attacks (Goodfellow et al., 2015;\nMadry et al., 2018; Shafahi et al., 2019; Bai et al., 2021).\nUnsupervised representation learning\nRepresentation learning aims to extract useful features\nfrom data. Unsupervised approaches are frequently used to leverage large unlabeled datasets. Siamese\nnetworks map similar samples to similar representations (Bromley et al., 1993; Koch et al., 2015),\nbut may collapse to a constant representation. However, Chen & He (2021) showed that a simple\nstop-grad can prevent such collapse. Contrastive learning was proposed to address the representational\ncollapse by introducing negative samples (Hadsell et al., 2006; Le-Khac et al., 2020). It can beneﬁt\nfrom pretext tasks (Xie et al., 2021; Bachman et al., 2019; Tian et al., 2020; Oord et al., 2018; Ozair\net al., 2020; McAllester & Stratos, 2020). Some methods that do not need negative samples are\nVAEs (Kingma & Welling, 2014), generative models (Kingma et al., 2014; Goodfellow et al., 2014;\nDonahue & Simonyan, 2020), or bootstrapping methods such as BYOL by Grill et al. (2020).\nRobustness of unsupervised representation learning\nMost robustness work has focused on su-\npervised tasks, but there has been recent interest in unsupervised training for representation encoders.\nKim et al. (2020) and Jiang et al. (2020) propose generating instance-wise attacks by maximizing a\ncontrastive loss and using them for adversarial training. Fan et al. (2021) complement this by a high-\nfrequency view. Ho & Vasconcelos (2020) suggest attacking batches instead of individual samples.\nKL-divergence can also be used as a loss (Alayrac et al., 2019) or as a regularizer (Nguyen et al.,\n2022). Alternatively, a classiﬁer can be trained on a small labeled dataset with adversarial training\napplied to it (Carmon et al., 2020; Alayrac et al., 2019). For VAEs, Cemgil et al. (2020) generate\nattacks by maximizing the Wasserstein distance to the clean representations in representation space.\nPeychev et al. (2021) address robustness from the perspective of individual fairness: they certify\nthat samples close in a feature directions are close in representation space. However, their approach\nis limited to invertible encoders. While these methods obtain robust unsupervised representation\n2\nencoders, they all evaluate robustness on a single supervised classiﬁcation task. To the best of our\nknowledge, no prior work has proposed measures for robustness evaluation without labels.1\n3\nPROBLEM SETTING\nBreakaway risk\nOverlap risk\n✔\n✔\n✔\n✔\n✘\n✘\nFigure 2: Breakaway and overlap risks. Diver-\ngences that increase the corresponding risks are in\nred and those reducing them are in green.\nLet f : X →R be a differentiable encoder\nfrom X = [0, 1]n to a representation space R.\nWe require f to be white-box: we can query both\nf(x) and df(x)\ndx . R is endowed with a divergence\nd(r, r′), a function that has the non-negativity\n(d(r, r′) ≥0, ∀r, r′ ∈R) and identity of indis-\ncernibles (d(r, r′) = 0 ⇔r = r′) properties.\nThis includes metrics on R and statistical dis-\ntances, e.g. the KL-divergence. D is a dataset of\niid samples from a distribution D over X.\nPerturbations of the input x are denoted as ˆx =\nx + δ, ∥δ∥∞≤ϵ, with ϵ small enough so that\nany ˆx is semantically indistinguishable from x.\nWe assume that it is desirable that f maps ˆx\nclose to x, i.e. d(f(x), f(ˆx)) should be “small”. We call this property unsupervised robustness. It\ncan also be referred to as smoothness (Bengio et al., 2013; Alayrac et al., 2019), although it is more\nclosely related with the concept of (Lipschitz) continuity.\nIt is not immediately obvious what value for d(f(x), f(ˆx)) would be “small”. This is encoder- and\ninput-dependent, as some parts of the representation manifold can be more densely packed than others.\nTo circumvent this issue, we consider the breakaway risk, i.e. the probability that the worst-case\nperturbation of x with maximum size ϵ is closer to a different sample x′ than it is to x:\nP\nx,x′∼D [d(f(ˆx), f(x′)) < d(f(ˆx), f(x))] , ˆx ∈arg\nsup\n˜x∈B(x,ϵ)\nd(f(x), f(˜x)).\n(1)\nAnother indication of the lack of unsupervised robustness is if f(B(x, ϵ)) and f(B(x′, ϵ)) overlap,\nas then there exist perturbations δ, δ′ under which f does not distinguish between the two samples,\ni.e. f(x + δ) = f(x′ + δ′). We call this the overlap risk and deﬁne it as:\nP\nx,x′∼D [d(f(x), f(a(x′, x))) < d(f(x), f(a(x, x′)))] , a(o, t) ∈arg\ninf\n˜x∈B(o,ϵ) d(f(t), f(˜x)).\n(2)\nThe breakaway risk is based on the perturbation causing the largest divergence in R, while the overlap\nrisk measures if perturbations are sufﬁciently concentrated to be separated from other instances (see\nFig. 2). Labels are not required: the two risks are deﬁned with respect to the local properties of the\nrepresentation manifold of f under D. In fact, we neither explicitly nor implicitly consider decision\nboundaries or underlying classes, as we make no assumptions about the downstream task.\nWhat if x and x′ are very similar? Perhaps we shouldn’t consider breakaway and overlap in such cases?\nWe argue against this. First, the probability of very similar pairs would be low in a sufﬁciently diverse\ndistribution D. Second, there is no clear notion of similarity on X without making assumptions about\nthe downstream tasks. Finally, even if x is very similar to x′, it should still be more similar to any ˆx\nas x and ˆx are deﬁned to be visually indistinguishable. We call this the self-similarity assumption.\n4\nUNSUPERVISED ADVERSARIAL ATTACKS ON REPRESENTATION ENCODERS\nIt is not tractable to compute the supremum and inﬁmum in Eqs. (1) and (2) exactly for a general f.\nInstead, we can approximate them via constrained optimization in the form of adversarial attacks.\nThis section shows how to modify the FGSM and PGD supervised attacks for these objectives\n(Secs. 4.1 and 4.2), as well as how to generalize these methods to arbitrary loss functions (Sec. 4.3).\nThe adversarial examples can also be used for adversarial training (Sec. 4.4).\n1Concurrent work by Wang & Liu (2022) proposed RVCL: a method to evaluate robustness without labels.\nHowever, it focuses on contrastive learning models while the methods here work with arbitrary encoders.\n3\n4.1\nUNSUPERVISED FAST GRADIENT SIGN METHOD (U-FGSM) ATTACK\nThe Fast Gradient Sign Method (FGSM) is a popular one-step method to generate adversarial\nexamples in the supervised setting (Goodfellow et al., 2015). Its untargeted mode perturbs the input\nx by taking a step of size α ∈R>0 in the direction of maximizing the classiﬁcation loss Lcl relative\nto the true label y. In targeted mode, it minimizes the loss of classifying x as a target class t ̸= y:\nˆx = clip(x + α sign(∇xLcl(f(x), y)))\nuntargeted FGSM,\nˆx→t = clip(x −α sign(∇xLcl(f(x), t)))\ntargeted FGSM,\nwhere clip(x) clips all values of x to be between 0 and 1.\nUntargeted U-FGSM\nWe can approximate the supremum in Eq. (1) by replacing Lcl with the\nrepresentation divergence d, using a small perturbation η ∈Rn to ensure non-zero gradient:\nˆx = clip(x + α sign(∇xd(f(x), f(x + η)))).\nHo & Vasconcelos (2020) also propose an untargeted FGSM attack for the unsupervised setting,\nwhich requires batches rather than single images and uses a speciﬁc loss function, and hence is an\ninstance of the ¯\nL-FGSM attack in Sec. 4.3. The untargeted U-FGSM proposed here is independent\nof the loss used for training, making it more versatile.\nTargeted U-FGSM\nWe can also perturb xi ∈D so that its representation becomes close to f(xj)\nfor some xj ∈D, xj ̸= xi. Then a downstream model would struggle to distinguish between the\nattacked input and the target xj. This approximates the inﬁmum in Eq. (2):\nˆx→j\ni\n= clip(xi −α sign(∇xid(f(xi), f(xj)))).\n4.2\nUNSUPERVISED PROJECTED GRADIENT DESCENT (U-PGD) ATTACK\nPGD attack is the gold standard of supervised adversarial attacks (Madry et al., 2018). It comprises\niterating FGSM and projections onto B(x, ϵ), the ℓ∞ball of radius ϵ centered at x:\nˆx0 = ˆx→t\n0\n= clip (x + U n[−ϵ, ϵ])\nrandomized initialization,\nˆxu+1 = clip(Πx,ϵ[ˆxu + α sign(∇ˆxuLcl(f(ˆxu), y))])\nuntargeted PGD,\nˆx→t\nu+1 = clip(Πx,ϵ[ˆx→t\nu\n−α sign(∇ˆx→t\nu Lcl(f(ˆx→t\nu ), t))])\ntargeted PGD.\nWe can construct the unsupervised PGD (U-PGD) attacks similarly to the U-FGSM attack:\nˆx0 = ˆx→t\n0\n= clip (x + U n[−ϵ, ϵ])\nrandomized initialization,\nˆxu+1 = clip(Πx,ϵ[ˆxu + α sign(∇ˆxud(f(ˆxu), f(x)))])\nuntargeted U-PGD,\nˆx→t\nu+1 = clip(Πx,ϵ[ˆx→t\nu\n−α sign(∇ˆx→t\nu d(f(ˆx→t\nu ), f(xj)))])\ntargeted U-PGD.\nBy replacing the randomized initialization with the η perturbation in the ﬁrst iteration of the untargeted\ncase, one obtains an unsupervised version of the BIM attack (Kurakin et al., 2017). The adversarial\ntraining methods proposed by Alayrac et al. (2019); Nguyen et al. (2022); Cemgil et al. (2020) can\nbe considered as using U-PGD attacks with speciﬁc divergence choices (see App. A.1).\n4.3\nLOSS-BASED ATTACKS\nIn both their supervised and unsupervised variants, FGSM and PGD attacks work by perturbing the\ninput in order to maximize or minimize the divergence d. By considering arbitrary differentiable loss\nfunctions instead, we can deﬁne a more general class of loss-based attacks.\nInstance-wise loss-based attacks (L-FGSM, L-PGD)\nGiven an instance x ∈X and a loss\nfunction L : (X →R) × X →R, the L-FGSM attack takes a step in the direction maximizing L:\nˆx = clip (x + α sign (∇xL(f, x))) .\n4\nSimilarly, for a loss function L : (X →R) × X × X →R taking a representation encoder, a\nsample, and the previous iteration of the attack, the loss-based PGD attack is deﬁned as:\nˆx0 = clip (x + U n[−ϵ, ϵ]) ,\nˆxu+1 = clip (Πx,ϵ [ˆxu + α sign(∇ˆxuL(f, x, ˆxu)]) .\nIf we do not use random initialization for the attack, we get the L-BIM attack.\nThe supervised and unsupervised FGSM and PGD attacks are special cases of the L-FGSM and\nL-PGD attacks. Furthermore, prior unsupervised adversarial training methods can also be represented\nas L-PGD attacks (Kim et al., 2020; Jiang et al., 2020). A full description is provided in App. A.1.\nBatch-wise loss-based attacks ( ¯\nL-FGSM, ¯\nL-PGD)\nAttacking whole batches instead of single\ninputs can account for interactions between the individual inputs in a batch. The above attacks can be\nnaturally extended to work over batches by independently attacking all inputs from X = [x1, . . . , xN].\nThis can be done with a more general loss function ¯\nL : (X →R) × XN →R. The batch-wise\nloss-based FGSM attack ¯\nL-FGSM is provided in Eq. (3) with ¯\nL-PGD and ¯\nL-BIM deﬁned similarly.\nˆX = clip\n\u0000X + α sign\n\u0000∇X ¯\nL(f, X)\n\u0001\u0001\n.\n(3)\nAny instance-wise loss-based attack can be trivially represented as a batch-wise attack. Additionally,\nprior unsupervised adversarial training methods can also be represented as ¯\nL-FGSM and ¯\nL-PGD\nattacks (Ho & Vasconcelos, 2020; Fan et al., 2021; Jiang et al., 2020) (see App. A.2).\n4.4\nADVERSARIAL TRAINING FOR UNSUPERVISED LEARNING\nAdversarial training is a min-max problem minimizing a loss relative to a worst-case perturbation\nthat maximizes it (Goodfellow et al., 2015). As the worst-case perturbation cannot be computed\nexactly (similarly to Eqs. (1) and (2)), adversarial attacks are usually used to approximate it. Any\nof the aforementioned attacks can be used for the inner optimization for adversarial training. Prior\nworks use divergence-based (Alayrac et al., 2019; Cemgil et al., 2020; Nguyen et al., 2022) and\nloss-based attacks (Kim et al., 2020; Jiang et al., 2020; Ho & Vasconcelos, 2020; Fan et al., 2021).\nThese methods tend to depend on complex loss functions and might work only for certain models.\nTherefore, we propose using targeted or untargeted U-PGD, as well as ¯\nL-PGD with the loss used for\ntraining. They are simple to implement and can be applied to any representation learning model.\n5\nROBUSTNESS ASSESSMENT WITH NO LABELS\nThe success of a supervised attack is clear-cut: whether the predicted class is different from the one of\nthe clean sample. In the unsupervised case, however, it is not clear when an adversarial attack results\nin a representation that is “too far” from the clean one. In this section, we propose using quantiles to\nquantify distances and discuss estimating the breakaway and overlap risks (Eqs. (1) and (2)).\nUniversal quantiles for untargeted attacks\nFor untargeted attacks, we propose measuring\nd(f(ˆx), f(x)) relative to the distribution of divergences between representations of samples from D.\nIn particular, we suggest reporting the quantile q = Px′,x′′∼D [d(f(x′), f(x′′)) ≤d(f(ˆx), f(x))] .\nThis measure is independent of downstream tasks and depends only on the properties of the encoder\nand D. We can use it to compare different models, as it is agnostic to the different representation\nmagnitudes models may have. In practice, the quantile values can be estimated from the dataset D.\nRelative quantiles for targeted attacks\nNothing prevents universal quantiles to be applied to\ntargeted attacks. However, considering that targeted attacks try to “impersonate” a particular target\nsample, we propose using relative quantiles to assess their success. We assess the attack as the\ndistance d(f(ˆx→j\ni\n), f(xj)) induced by the attack relative to d(f(xi), f(xj)), the original distance\nbetween the clean sample and the target. The relative quantile for a targeted attack ˆx→j\ni\nis then the\nratio d(f(ˆx→j\ni\n), f(xj))/d(f(xi), f(xj)).\nQuantiles are a good way to assess the success of individual attacks or to compare different models.\nHowever, they do not take into account the local properties of the representation manifold, i.e. that\n5\nsome regions of R might be more densely populated than others. The breakaway and overlap risk\nmetrics were deﬁned with this exact purpose. Hence, we propose estimating them.\nEstimating the breakaway risk\nWhile the supremum in Eq. (1) cannot be computed explicitly, it\ncan be approximated using the untargeted U-FGSM and U-PGD attacks. Therefore, we can compute\na Monte Carlo estimate of Eq. (1) by sampling pairs (x, x′) from the dataset D and performing an\nuntargeted attack on x, for example with U-PGD.\nNearest neighbour accuracy\nAs the breakaway risk can be very small for robust encoders we\npropose also reporting the fraction of samples in D′ ⊆D whose untargeted attacks ˆx would have\ntheir nearest clean neighbour in D being their corresponding clean samples x. That is:\n1\n|D′|\nX\nx∈D′\n1 [∄x′ ∈D, x′ ̸= x, s.t. d(f(x′), f(ˆx)) < d(f(x), f(ˆx))] .\n(4)\nEstimating the overlap risk\nThe inﬁmums in Eq. (2) can be estimated with an unsupervised\ntargeted attack. Hence, an estimate of Eq. (2) can be computed by sampling pairs (xi, xj) from the\ndataset D and computing the targeted attacks ˆx→j\ni\nand ˆx→i\nj . The overlap risk estimate is then the\nfraction of pairs for which d(f(xi), f(ˆx→i\nj )) < d(f(xi), f(ˆx→j\ni\n)).\nAdversarial margin\nIn Eq. (2) one takes into account only whether overlap occurs but not the\nmagnitude of the violation. Therefore, we also propose looking at the margin between the two\nattacked representations, normalized by the divergence between the clean samples:\nd(f(xi), f(ˆx→i\nj )) −d(f(xi), f(ˆx→j\ni\n))\nd(f(xi), f(xj))\n,\nfor randomly selected pairs (xi, xj) from D. If overlap occurs, this ratio would be negative, with\nmore negative values pointing to stronger violations. The overlap risk is therefore equivalent to the\nprobability of occurrence of a negative adversarial margin.\nCertiﬁed unsupervised robustness\nThe present work depends on gradient-based attacks, which\ncan be fooled by gradient masking (Athalye et al., 2018; Uesato et al., 2018). Hence, we also assess\nthe certiﬁed robustness of the encoder. By using center smoothing (Kumar & Goldstein, 2021) we\ncan compute a probabilistic guarantee on the radius of the ℓ2-ball in R that contains at least half of\nthe probability mass of f(x + N(0, σ2)). The smaller this radius is, the closer f maps similar inputs.\nHence, this is a probabilistically certiﬁed alternative to assessing robustness via untargeted attacks. In\norder to compare certiﬁed radius values in R across models we report them as universal quantiles.\n6\nEXPERIMENTS\nWe assess the robustness of state-of-the-art representation encoders against the unsupervised attacks\nand robustness measures outlined in Secs. 4 and 5. We consider the ResNet50-based self-supervised\nlearning models MOCOv2 (200 epochs) (He et al., 2020; Chen et al., 2020d), MOCO with non-\nsemantic negatives (+Patch, k=16384, α=3) (Ge et al., 2021), PixPro (400 epochs) (Xie et al., 2021),\nAMDIM (Medium) (Bachman et al., 2019), SimCLRv2 (depth 50, width 1x, without selective kernels)\n(Chen et al., 2020c), and SimSiam (100 epochs, batch size 256) (Chen & He, 2021). To compare the\nself-supervised and the supervised methods, we also evaluate the penultimate layer of ResNet50 (He\net al., 2016). We assess the effect of using different unsupervised attacks by ﬁne-tuning MOCOv2\nwith the untargeted U-PGD, targeted U-PGD, as well as with ¯\nL-PGD using MOCOv2’s contrastive\nloss, as proposed in Sec. 4.4. See App. C.1 for details and pseudocode. Additionally, in App. B we\nevaluate the transformer-based models MAE (He et al., 2022) and MOCOv3 (Chen et al., 2021) as\nwell as adversarially ﬁne-tuned versions of MOCOv3 using the same three attacks as for MOCOv2.\nThe unsupervised evaluation uses the PASS dataset as it does not contain people and identiﬁable\ninformation and has proper licensing (Asano et al., 2021). ImageNet (Russakovsky et al., 2015) is\nused for accuracy benchmarking and the adversarial ﬁne-tuning of MOCO, as to be consistent with\nhow the model was trained. Assira (Elson et al., 2007) is used for the impersonation attacks.\n6\nTable 1: Standard and lowpass accuracy of linear probes of ResNet50-based encoders.\nPixPro AMDIM SimCLR SimSiam\nNonsemMOCOv2\nTAR\nUNTAR\nStandard\n74%\n58%\n62%\n67%\n63%\n50%\n67%\n60%\n60%\n57%\nLowpass\n68%\n50%\n53%\n60%\n56%\n47%\n62%\n59%\n58%\n55%\nAccuracy\nAdversarially fine-tuned MOCOv2\nStandard ResNet-based unsupervised models\nResNet50\nL!−PGD\nTable 2: Robustness of ResNet50 and ResNet50-based unsupervised encoders without unsupervised\nadversarial training. Arrows show if larger or smaller values are better.\nResNet50\nPixPro\nAMDIM\nSimCLR\nSimSiam\nNonsem MOCOv2\n5 iter. ↑\n66.38%\n80.51%\n80.82%\n75.04%\n73.25%\n61.54%\n65.64%\n10 iter. ↑\n52.68%\n67.91%\n70.26%\n65.77%\n65.94%\n46.52%\n52.08%\n50 iter. ↑\n27.42%\n37.57%\n44.28%\n36.38%\n28.13%\n22.23%\n20.51%\n5 iter. ↑\n69.27%\n83.06%\n85.59%\n78.02%\n78.11%\n64.28%\n68.68%\n10 iter. ↑\n55.63%\n71.18%\n75.93%\n69.01%\n70.45%\n48.57%\n55.02%\n50 iter. ↑\n27.92%\n38.07%\n47.28%\n38.60%\n30.33%\n22.32%\n21.88%\n5 iter. ↓\n98.70%\n14.35%\n81.30%\n43.65%\n72.55%\n46.80%\n65.10%\n10 iter. ↓\n99.90%\n85.60%\n98.50%\n99.40%\n98.60%\n98.10%\n99.00%\n50 iter. ↓\n99.90%\n99.90%\n99.90%\n99.90%\n99.90%\n99.90%\n99.90%\n5 iter. ↓\n99.40%\n11.25%\n92.10%\n48.30%\n65.65%\n43.20%\n69.25%\n10 iter. ↓\n99.90%\n77.50%\n98.90%\n98.70%\n98.00%\n96.65%\n98.80%\n50 iter. ↓\n99.90%\n99.90%\n99.90%\n99.90%\n99.90%\n99.90%\n99.90%\n0.120%\n0.190%\n0.414%\n0.039%\n0.146%\n0.211%\n0.254%\n0.00%\n0.00%\n0.10%\n0.10%\n0.00%\n0.00%\n0.00%\n92.48%\n39.65%\n34.30%\n45.61%\n40.72%\n93.46%\n91.80%\n-18.57%\n4.08%\n2.72%\n1.23%\n1.60%\n-27.18%\n-16.50%\n38.89%\n54.06%\n68.32%\n53.96%\n61.70%\n54.70%\n56.31%\nBreakaway risk ↓\nNearest neighbor acc. ↑\nOverlap risk ↓\nMed. adversarial margin ↑\nAvg. Certified Radius ↑\nTargeted U-PGD\nɛ = 0.05\nɛ = 0.10\nUntar. U-PGD\nɛ = 0.05\nɛ = 0.10\nWe report median universal and relative quantiles for the ℓ2 divergence for respectively untargeted\nand targeted U-PGD attacks with ϵ = 0.05 and ϵ = 0.10. In App. B we also report the median ℓ∞\ndivergence and cosine similarity. We also estimate the breakaway risk, nearest neighbour accuracy,\noverlap risk and adversarial margin, and certiﬁed unsupervised robustness, as described in Sec. 5.\nAs customary, we measure the quality of the representations with the top-1 and top-5 accuracy of a\nlinear probe. We also report the accuracy on samples without high-frequency components, as models\nmight be overly reliant on the high-frequency features in the data (Wang et al., 2020). Additionally,\nwe assess the certiﬁed robustness via randomized smoothing (Cohen et al., 2019) and report the\nresulting Average Certiﬁed Radius (Zhai et al., 2020).\nIn line with the impersonation threat model, we also evaluate to what extent attacking a representation\nencoder can fool a private downstream classiﬁer. Pairs of cats and dogs from the Assira dataset (Elson\net al., 2007) are attacked with targeted U-PGD so that the representation of one is close to the other.\nWe report the percentage of impersonations that successfully fool the linear probe.\nAll implementation details for these experiments can be found in App. C.\n7\nRESULTS\nIn this section, we present the results of the experiments on ResNet50 and the ResNet50-based\nunsupervised encoders. We defer the results for transformer architectures to App. B.\nThere is no “most robust” standard model.\nAmongst the standard unsupervised models, none\ndominates on all unsupervised robustness measures (see Tab. 2). AMDIM is least susceptible to\ntargeted U-PGD attacks and has the highest average certiﬁed radius but has the worst untargeted\nU-PGD, breakaway risk and nearest neighbor accuracy. PixPro signiﬁcantly outperforms the other\nmodels on untargeted attacks. AMDIM and PixPro also have the lowest overlap risk and largest\nmedian adversarial margin. At the same time, the model with the lowest breakaway risk is SimCLR.\nWhile either AMDIM or PixPro scores the best at most measures, they both have signiﬁcantly higher\nbreakaway risk than SimCLR. Therefore, no model is a clear choice for the “most robust model”.\n7\nUnsupervised robustness measures reveal signiﬁcant differences among standard models.\nThe gap between the best and worst performing unsupervised models for the six measures based on\ntargeted U-PGD attacks is between 19% and 27%. The gap reaches almost 81% for the untargeted\ncase (PixPro vs AMDIM, 5 it.), demonstrating that standard models on both extremes do exist.\nAMDIM has 10.5 times higher breakaway risk than SimCLR while at the same time 2.7 times lower\noverlap risk than MOCOv2. Observing values on both extremes of all unsupervised robustness metrics\ntestiﬁes to them being useful for differentiating between the different models. Additionally, AMDIM\nhaving the highest breakaway risk and lowest overlap risk indicates that unsupervised robustness is a\nmultifaceted problem and that models should be evaluated against an array of measures.\nTable 3: Robustness of MOCOv2 and its adversarially\nﬁne-tuned versions. Arrows show if larger or smaller\nvalues are better.\nMOCOv2\nTAR\nUNTAR\n5 iter. ↑\n65.64%\n92.97%\n94.00%\n95.41%\n10 iter. ↑\n52.08%\n87.38%\n88.86%\n91.57%\n50 iter. ↑\n20.51%\n62.79%\n64.83%\n71.65%\n5 iter. ↑\n68.68%\n93.01%\n93.92%\n95.24%\n10 iter. ↑\n55.02%\n87.23%\n88.59%\n91.11%\n50 iter. ↑\n21.88%\n59.67%\n61.21%\n68.19%\n5 iter. ↓\n65.10%\n0.00%\n0.00%\n0.00%\n10 iter. ↓\n99.00%\n0.00%\n0.00%\n0.00%\n50 iter. ↓\n99.90%\n75.35%\n30.35%\n3.40%\n5 iter. ↓\n69.25%\n0.00%\n0.00%\n0.00%\n10 iter. ↓\n98.80%\n0.01%\n0.00%\n0.00%\n50 iter. ↓\n99.90%\n91.65%\n65.30%\n18.40%\n0.254%\n0.049%\n0.001%\n0.000%\n0.00%\n17.00%\n35.60%\n64.40%\n91.80%\n0.00%\n0.00%\n0.00%\n-16.50%\n61.54%\n67.50%\n76.00%\n56.31%\n73.00%\n75.20%\n77.55%\nTargeted U-PGD\nɛ = 0.05\nɛ = 0.10\nMed. adversarial margin ↑\nAvg. Certified Radius ↑\nUntar. U-PGD\nɛ = 0.05\nɛ = 0.10\nBreakaway risk ↓\nNearest neighbor acc. ↑\nOverlap risk ↓\nL!−PGD\nUnsupervised\nadversarial\ntraining\nboosts robustness across all measures.\nAcross every single unsupervised measure,\nthe worst adversarially trained model\nperforms better than the best standard\nmodel (Tabs. 2 and 3).\nComparing the\nadversarially trained models with MOCOv2,\nwe observe a signiﬁcant improvement\nacross the board (Tab. 3). They are also\nmore certiﬁably robust (Fig. 4). However,\nthe added robustness comes at the price of\nreduced accuracy (7% to 10%, Tab. 1), as\nis typical for adversarial training (Zhang\net al., 2019; Tsipras et al., 2019).\nThis\ngap can likely be reduced by ﬁne-tuning\nthe trade-off between the adversarial and\nstandard objectives and by having separate\nbatch normalization parameters for standard\nand adversarial samples (Kim et al., 2020;\nHo & Vasconcelos, 2020).\nAdversarial\ntraining also reduces the impersonation rate\nof a downstream classiﬁer at 5 iterations by a half relative to MOCOv2 (Tab. 4). For 50 iterations,\nthe rate is similar to MOCOv2 but the attacked images of the adversarially trained models have\nstronger semantically meaningful distortions, which can be detected by a human auditor (see App. D\nfor examples). These results are for only 10 iterations of ﬁne-tuning of a standard-trained encoder.\nFurther impersonation rate reduction can likely be achieved with adversarial training applied to the\nwhole 200 epochs of training.\nTable 4: Impersonation attack success rate on\nMOCOv2 and its label-free adversarially ﬁne-\ntuned versions for different attack iterations.\nIterations\nMOCOv2\nTAR\nUNTAR\n3 iter.\n34.20%\n18.91%\n15.67%\n15.20%\n10 iter.\n62.78%\n60.15%\n56.27%\n54.59%\n50 iter.\n76.43%\n76.22%\n71.79%\n71.89%\nL!−PGD\nUnsupervised adversarial training results in cer-\ntiﬁably more robust classiﬁers.\nFig. 3 shows how\nthe randomized smoothened linear probes of the ad-\nversarially trained models uniformly outperform MO-\nCOv2. The difference is especially evident for large\nradii: 3 times higher certiﬁed accuracy when consid-\nering perturbations with radius of 0.935. These re-\nsults demonstrate that unsupervised adversarial train-\ning boosts the downstream certiﬁed accuracy.\nAdversarially trained models have better consistency between standard and low-pass accuracy.\nThe difference between standard and low-pass accuracy for the adversarially trained models is between\n1.7% and 2.1%, compared to 2.2% to 9.7% for the standard models (Tab. 1). This could be in part due\nto the lower accuracy of the adversarially trained models. However, compared with PixPro, AMDIM\nand SimSiam, which have similar accuracy but larger gaps, indicate that the lower accuracy cannot\nfully explain the lower gap. Therefore, this suggests that unsupervised adversarial training can help\nwith learning the robust low-frequency features and rejecting high-frequency non-semantic ones.\n¯\nL-PGD is the overall most robust model, albeit with lower accuracy.\n¯\nL-PGD dominates across\nall unsupervised robustness measures. These results support the ﬁndings of prior work on unsuper-\n8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nInput radius\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nCertified accuracy\nMOCOv2\nMOCOv2 TAR\nMOCOv2 UNTAR\nMOCOv2 -PGD\nFigure 3:\nCertiﬁed accuracy of randomized\nsmoothed MOCOv2 and its adversarially trained\nvariants on ImageNet. The adversarially trained\nmodels are uniformly more robust.\n10\n8\n10\n6\n10\n4\n10\n2\nCertified representation radius\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFraction of dataset\nMOCOv2\nMOCOv2 TAR\nMOCOv2 UNTAR\nMOCOv2 -PGD\nFigure 4: Certiﬁed robustness of MOCOv2 on\nPASS using center smoothing. The certiﬁed rep-\nresentation radius is represented as percentile of\nthe distribution of clean representation distances.\nvised adversarial training using batch loss optimization (Ho & Vasconcelos, 2020; Fan et al., 2021;\nJiang et al., 2020). However, ¯\nL-PGD also has lower certiﬁed accuracy for small radius values and the\nlowest supervised accuracy of the three models, as expected due to the accuracy-robustness trade-off.\nStill, the differences between the three models are small, and hence all three adversarial training\nmethods can improve the robustness of unsupervised representation learning models.\n8\nDISCUSSION, LIMITATIONS AND CONCLUSION\nUnsupervised task-independent adversarial training with simple extensions to classic adversarial\nattacks can improve the robustness of encoders used for multiple downstream tasks, especially when\nreleased publicly. That is why we will release the adversarially ﬁne-tuned versions of MOCOv2\nand MOCOv3, which can be used as more robust drop-in replacements for applications built on top\nof these two models. We showed how to assess the robustness of such encoders without resorting\nto labeled datasets or proxy tasks. However, there is no single “unsupervised robustness measure”:\nmodels can have drastically different performance across the different metrics. Still, unsupervised\nrobustness is a stronger requirement than classiﬁcation robustness as it requires not only the output\nbut also an intermediate state of the model to not be sensitive to small perturbations. Hence, we\nrecommend unsupervised assessment and adversarial training to also be applied to supervised tasks.\nWe do not compare with the prior methods in Sec. 2 as different base models, datasets and objective\nfunctions hinder a fair comparison. Moreover, the methods we propose generalize the previous works,\nhence this paper strengthens their conclusions rather than claiming improvement over them.\nThis work is not an exhaustive exploration of unsupervised attacks, robustness measures and defences.\nWe adversarially ﬁne-tuned only two models, one based on ResNet50 and one transformer-based\n(in App. B); assessing how these techniques work on other architectures is further required. There\nare many other areas warranting further investigation, such as non-gradient based attacks, measures\nwhich better predict the robustness of downstream tasks, certiﬁed defences, as well as studying the\naccuracy-robustness trade-off for representation learning. Still, we believe that robustness evaluation\nof representation learning models is necessary for a comprehensive assessment of their performance\nand robustness. This is especially important for encoders used for applications susceptible to\nimpersonation attacks. Therefore, we recommend reporting unsupervised robustness measures\ntogether with standard and low-pass linear probe accuracy when proposing new unsupervised and\nsupervised learning models. We hope that this paper illustrates the breadth of opportunities for\nrobustness evaluation in representation space and inspires further work on it.\nACKNOWLEDGEMENTS\nAP is supported by a grant from Toyota Motor Europe. MK received funding from the ERC under\nthe EU’s Horizon 2020 research and innovation programme (FUN2MODEL, grant No. 834115).\n9\nETHICS STATEMENT\nThis work discusses adversarial vulnerabilities in unsupervised models and therefore exposes potential\nattack vectors for malicious actors. However, it also proposes defence strategies in the form of\nadversarial training which can alleviate the problem, as well as measures to assess how vulnerable\nrepresentation learning models are. Therefore, we believe that it would empower the developers of\nsafety-, reliability- and fairness-critical systems to develop safer models. Moreover, we hope that this\nwork inspires further research into unsupervised robustness, which can contribute to more robust and\nsecure machine learning systems.\nREPRODUCIBILITY STATEMENT\nThe experiments in this paper were implemented using open-source software packages (Harris et al.,\n2020; Virtanen et al., 2020; McKinney, 2010; Paszke et al., 2019), as well as the publicly available\nMOCOv2 and MOCOv3 models (He et al., 2020; Chen et al., 2020d; 2021). We provide the code\nused for the adversarial training, as well as all the robustness evaluation implementations, together\nwith documentation on their use. We also release the weights of the models and linear probes. The\ncode reproducing all the experiments in this paper is provided as well. The details are available here.\nREFERENCES\nJean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert Stanforth, and\nPushmeet Kohli. Are labels required for improving adversarial robustness? In 32nd Conference on\nNeural Information Processing Systems, pp. 12214–12223, 2019.\nYuki M. Asano, Christian Rupprecht, Andrew Zisserman, and Andrea Vedaldi. PASS: An ImageNet\nreplacement for self-supervised pretraining without humans. In NeurIPS Track on Datasets and\nBenchmarks, 2021.\nAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of\nsecurity: Circumventing defenses to adversarial examples. In 35th International Conference on\nMachine Learning, pp. 274–283, 2018.\nPhilip Bachman, R. Devon Hjelm, and William Buchwalter. Learning representations by maximizing\nmutual information across views. In 32nd Conference on Neural Information Processing Systems,\npp. 15535–15545, 2019.\nTao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. Recent advances in adversarial training\nfor adversarial robustness. In 30th International Joint Conference on Artiﬁcial Intelligence, pp.\n4312–4321, 2021.\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new\nperspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828,\n2013.\nBattista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndi´c, Pavel Laskov, Giorgio\nGiacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In European\nConference on Machine Learning and Knowledge Discovery in Databases, pp. 387–402, 2013.\nAbeba Birhane and Vinay Uday Prabhu. Large image datasets: A pyrrhic win for computer vision?\nIn IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1536–1546, 2021.\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah. Signature\nveriﬁcation using a \"Siamese\" time delay neural network. In Advances in Neural Information\nProcessing Systems, pp. 737–744, 1993.\nYair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C Duchi. Unlabeled data\nimproves adversarial robustness. In 33rd Conference on Neural Information Processing Systems,\npp. 11192–11203, 2020.\n10\nTaylan Cemgil, Sumedh Ghaisas, Krishnamurthy Dvijotham, and Pushmeet Kohli. Adversarially\nrobust representations with smooth encoders. In International Conference on Learning Represen-\ntations, 2020.\nTianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversar-\nial robustness: From self-supervised pre-training to ﬁne-tuning. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 696–705, 2020a.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In Proceedings of the 37th International Conference\non Machine Learning, pp. 1597–1607, 2020b.\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-\nsupervised models are strong semi-supervised learners. In 34th Conference on Neural Information\nProcessing Systems, pp. 22243–22255, 2020c.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750–15758, 2021.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. Preprint arXiv:2003.04297, 2020d.\nXinlei Chen, Saining Xie, and Kaiming He. An Empirical Study of Training Self-Supervised Vision\nTransformers. In International Conference on Computer Vision, pp. 9640–9649, 2021.\nJeremy Cohen, Elan Rosenfeld, and J Zico Kolter. Certiﬁed adversarial robustness via randomized\nsmoothing. In 36th International Conference on Machine Learning, pp. 1310–1320, 2019.\nJeff Donahue and Karen Simonyan.\nLarge scale adversarial representation learning.\nIn 33rd\nConference on Neural Information Processing Systems, pp. 10542–10552, 2020.\nJeremy Elson, John Douceur, Jon Howell, and Jared Saul. Asirra: A CAPTCHA that exploits interest-\naligned manual image categorization. In ACM Conference on Computer and Communications\nSecurity, 2007.\nLijie Fan, Sijia Liu, Pin-Yu Chen, Gaoyuan Zhang, and Chuang Gan. When does contrastive learning\npreserve adversarial robustness from pretraining to ﬁnetuning? In 35th Conference on Neural\nInformation Processing Systems, pp. 21480–21492, 2021.\nSongwei Ge, Shlok Mishra, Haohan Wang, Chun-Liang Li, and David Jacobs. Robust contrastive\nlearning using negative samples with diminished semantics.\nIn 34th Conference on Neural\nInformation Processing Systems, pp. 27356–27368, 2021.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In 27th Conference on Neural\nInformation Processing Systems, pp. 2672–2680, 2014.\nIan J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\nexamples. In International Conference on Learning Representations, 2015.\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi\nAzar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent\na new approach to self-supervised learning. In 33rd Conference on Neural Information Processing\nSystems, pp. 21271–21284, 2020.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant\nmapping. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 1735–1742, 2006.\nCharles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David\nCournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti\nPicus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del\nRío, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren\nWeckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with\nNumPy. Nature, 585(7825):357–362, 2020.\n11\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 9729–9738, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked\nautoencoders are scalable vision learners. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 16000–16009, 2022.\nChih-Hui Ho and Nuno Vasconcelos. Contrastive learning with adversarial examples. In 34th\nConference on Neural Information Processing Systems, pp. 17081–17093, 2020.\nZiyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang. Robust pre-training by adversarial\ncontrastive learning. In 34th Conference on Neural Information Processing Systems, pp. 16199–\n16210, 2020.\nMinseon Kim, Jihoon Tack, and Sung Ju Hwang. Adversarial self-supervised contrastive learning. In\n34th Conference on Neural Information Processing Systems, pp. 2983–2994, 2020.\nDiederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference\non Learning Representations, 2014.\nDiederik P. Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised\nlearning with deep generative models. In 27th Conference on Neural Information Processing\nSystems, pp. 3581–3589, 2014.\nGregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot\nimage recognition. In Proceedings of the 32nd International Conference on Machine Learning,\n2015.\nAounon Kumar and Tom Goldstein. Center smoothing: Certiﬁed robustness for networks with\nstructured outputs. In 34th Conference on Neural Information Processing Systems, pp. 5560–5575,\n2021.\nAlexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In\nInternational Conference on Learning Representations, 2017.\nPhuc H. Le-Khac, Graham Healy, and Alan F. Smeaton. Contrastive representation learning: A\nframework and review. IEEE Access, 8:193907–193934, 2020.\nMathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certiﬁed\nrobustness to adversarial examples with differential privacy. In IEEE Symposium on Security and\nPrivacy (SP), pp. 656–672, 2019.\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. In International Conference on\nLearning Representations, 2018.\nDavid McAllester and Karl Stratos. Formal limitations on the measurement of mutual information.\nIn 23rd International Conference on Artiﬁcial Intelligence and Statistics, pp. 875–884, 2020.\nWes McKinney. Data structures for statistical computing in Python. In 9th Python in Science\nConference, pp. 56 – 61, 2010.\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: A\nregularization method for supervised and semi-supervised learning. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 41(8):1979–1993, 2019.\nA. Tuan Nguyen, Ser Nam Lim, and Philip Torr. Task-agnostic robust representation learning.\nPreprint arXiv:2203.07596, 2022.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. Preprint arXiv:1807.03748, 2018.\n12\nSherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron van den Oord, Sergey Levine, and Pierre Sermanet.\nWasserstein dependency measure for representation learning. In 33rd Conference on Neural\nInformation Processing Systems, pp. 15578–15588, 2020.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep\nlearning library. In 32nd Conference on Neural Information Processing Systems, pp. 8024–8035,\n2019.\nMomchil Peychev, Anian Ruoss, Mislav Balunovi´c, Maximilian Baader, and Martin Vechev. Latent\nspace smoothing for individually fair representations. Preprint arXiv:2111.13650, 2021.\nJoshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can\ncontrastive learning avoid shortcut solutions? In 35th Conference on Neural Information Processing\nSystems, pp. 4974–4986, 2021.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet\nlarge scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252,\n2015.\nAli Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer,\nLarry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In 33rd Conference\non Neural Information Processing Systems, pp. 3358–3369, 2019.\nSahil Singla and Soheil Feizi. Salient ImageNet: How to discover spurious features in deep learning?\nIn International Conference on Learning Representations, 2022.\nRyan Steed and Aylin Caliskan. Image representations learned with unsupervised pre-training\ncontain human-like biases. In ACT Conference on Fairness, Accountability, and Transparency, pp.\n701–713, 2021.\nPierre Stock and Moustapha Cisse. ConvNets and ImageNet beyond accuracy: Understanding\nmistakes and uncovering biases. In European Conference on Computer Vision, pp. 504–519, 2018.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow,\nand Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning\nRepresentations, pp. 23296–23308, 2014.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In European\nConference on Computer Vision, pp. 776–794, 2020.\nDimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Ro-\nbustness may be at odds with accuracy. In International Conference on Learning Representations,\n2019.\nJonathan Uesato, Brendan O’Donoghue, Pushmeet Kohli, and Aaron Oord. Adversarial risk and\nthe dangers of evaluating against weak attacks. In 35th International Conference on Machine\nLearning, pp. 5025–5034, 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is All you Need. In 30th Conference on Neural Information\nProcessing Systems, 2017.\nPauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau,\nEvgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt,\nMatthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric\nJones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas,\nDenis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris,\nAnne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0\nContributors. SciPy 1.0: Fundamental algorithms for scientiﬁc computing in Python. Nature\nMethods, 17:261–272, 2020.\n13\nHaohan Wang, Xindi Wu, Zeyi Huang, and Eric P. Xing. High-frequency component helps explain\nthe generalization of convolutional neural networks. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 8681–8691, 2020.\nZekai Wang and Weiwei Liu. Robustness veriﬁcation for contrastive learning. In Proceedings of the\n39th International Conference on Machine Learning, pp. 22865–22883, 2022.\nZhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself:\nExploring pixel-level consistency for unsupervised visual representation learning. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 16679–16688, 2021.\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow Twins: Self-supervised\nlearning via redundancy reduction. In Proceedings of the 38th International Conference on\nMachine Learning, pp. 12310–12320, 2021.\nRuntian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh,\nand Liwei Wang. MACER: attack-free and scalable robust training via maximizing certiﬁed radius.\nIn International Conference on Learning Representations, 2020.\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.\nTheoretically principled trade-off between robustness and accuracy. In Proceedings of the 36th\nInternational Conference on Machine Learning, pp. 7472–7482, 2019.\nYonggang Zhang, Mingming Gong, Tongliang Liu, Gang Niu, Xinmei Tian, Bo Han, Bernhard\nSchölkopf, and Kun Zhang. Adversarial robustness through the lens of causality. In International\nConference on Learning Representations, 2022.\n14\nA\nEXAMPLES OF LOSS-BASED ATTACKS\nIn this appendix we demonstrate how various supervised and unsupervised attacks can be represented\nas loss-based attacks. This generalizes the unifying view presented by Madry et al. (2018) by\nincorporating also unsupervised attacks. We also illustrate the resulting structure of which attacks\nare generalizations of other attacks in Fig. 5. In the following, π(x) is the true class of the instance\nx, ¯π(x) is any other class, σ(x) is a function that returns a sample from D such that σ(x) ̸= x, and\nκ(x) provides a different view of x, e.g. a different augmentation.\nA.1\nEXAMPLES OF INSTANCE-WISE LOSS-BASED ATTACKS (L-FGSM, L-PGD, L-BIM)\n• The supervised FGSM and PGD attacks can be considered as special cases of the unsuper-\nvised L-FGSM and L-PGD, where f is a classiﬁer rather than an encoder and we use a\nclassiﬁcation loss:\n– Untargeted FGSM attack: L-FGSM with L(f, x) = Lcl(f(x), π(x)),\n– Targeted FGSM attack: L-FGSM with L(f, x) = −Lcl(f(x), ¯π(x)),\n– Untargeted PGD attack: L-PGD with L(f, x, ˆxu) = Lcl(f(ˆxu), π(x)),\n– Targeted PGD attack: L-PGD with L(f, x, ˆxu) = −Lcl(f(ˆxu), ¯π(x)).\n• The U-LGSM and U-PGD attacks can be represented as L-FGSM and L-PGD attacks\nwhere the loss is the divergence d:\n– Untargeted U-FGSM attack: L-FGSM with L(f, x) = d(f(x), f(x + η)),\n– Targeted U-FGSM attack: L-FGSM with L(f, x) = −d(f(x), f(σ(x))),\n– Untargeted U-PGD attack: L-PGD with L(f, x, ˆxu) = d(f(ˆxu), f(x)),\n– Targeted U-PGD attack: L-PGD with L(f, x, ˆxu) = −d(f(ˆxu), f(σ(x))).\n• Untargeted U-PGD with the Kullback-Leibler divergence corresponds to the adversarial\nexample generation process of UAT-OT (Alayrac et al., 2019) which is based on the Virtual\nAdversarial Training method for semi-supervised adversarial learning (Miyato et al., 2019).\nUntargeted U-PGD with the Kullback-Leibler also corresponds to the robustness regularizer\nproposed by Nguyen et al. (2022).\n• Cemgil et al. (2020) propose an unsupervised attack for Variational Auto-Encoders (VAEs)\n(Kingma & Welling, 2014) based on the Wasserstein distance. It can be represented as the\nuntargeted U-PGD attack with the Wasserstein distance, or equivalently, as the L-PGD\nattack with the loss\nL(f, x, ˆxu) = W (N([f(x)]µ, I[f(x)]σ), N[(f(ˆxu)]µ, I[f(ˆxu)]σ)) ,\nwhere W is the Wasserstein distance, N is the normal distribution, I is the identity matrix\nand the subscripts µ and σ designate the respective outputs of the VAE encoder f.\n• The instance-wise unsupervised adversarial attack proposed by Kim et al. (2020) is equiva-\nlent to L-PGD with the contrastive loss\nL(f, x, ˆxu) = −log\nexp\n\u0000f(ˆxu)⊤f(κ(x))/T\n\u0001\nexp (f(ˆxu)⊤f(κ(x))/T) + exp (f(ˆxu)⊤f(σ(x))/T),\nwhere T is a temperature parameter. This loss encourages that the cosine similarity between\nthe adversarial example and another view of the same sample is small relative to the cosine\nsimilarity between the adversarial example and another sample from D.\n• Using the NT-Xent loss with L-PGD results in the attack used for the Adversarial-to-\nStandard adversarial contrastive learning proposed by Jiang et al. (2020).\nA.2\nEXAMPLES OF BATCH-WISE LOSS-BASED ATTACKS ( ¯\nL-FGSM, ¯\nL-PGD, ¯\nL-BIM)\n• Any L-FGSM attack with loss L is trivially an ¯\nL-FGSM attack by taking N = 1 and\nconsidering the loss function ¯\nL = L.\n• Similarly, any L-PGD attack is trivially an ¯\nL-PGD attack.\n15\n¯\nL-FGSM\nL-FGSM\n(Ho & Vascon-\ncelos, 2020)\nUntargeted\nU-FGSM\nTargeted\nU-FGSM\nUntargeted\nFGSM\nTargeted\nFGSM\n¯\nL-PGD\nAtA, DS\n(Jiang et al., 2020)\nL-PGD\n(Fan et al.,\n2021)\n(Kim et al.,\n2020)\nUntargeted\nU-PGD\nTargeted\nU-PGD\nAtS\n(Jiang et al., 2020)\nTargeted\nPGD\nUntargeted\nPGD\n(Cemgil\net al., 2020)\n(Alayrac\net al., 2019)\n(Nguyen\net al., 2022)\nFigure 5: The hierarchy of supervised and unsupervised attacks.\n• The adversarial attack proposed by Ho & Vasconcelos (2020) is equivalent to ¯\nL-FGSM\nwith the contrastive loss\n¯\nL(f, X) =\nN\nX\ni=1\n−log\nexp(f(xi)⊤f(κ(xi))/T)\nPN\nj=1 exp(f(xj)⊤f(κ(xi))/T)\n.\nHere adversarial examples are selected to jointly maximize the contrastive loss.\n• The adversarial training of AdvCL generates adversarial attacks by maximizing a multi-view\ncontrastive loss computed over the adversarial example, two views of x and its high-\nfrequency component HighPass(x) (Fan et al., 2021). It corresponds to ¯\nL-PGD with the\nloss\nL(f, X, ˆXu) = 1\nN\nN\nX\ni=1\nL′ (κ1(xi), κ2(xi), ˆxi,u, HighPass(xi); f, X) ,\nL′(z1, . . . , zm; f, X) = −\nm\nX\ni=1\nm\nX\nj=1\nj̸=i\nlog\nexp(sim(f(zi), f(zj))/T)\nP\nzk∈X\nP\nκ∈{κ1,κ2} exp(sim(f(zi), f(κ(zk)))/T),\nwith sim(·, ·) being the cosine similarity.\n• Using the NT-Xent loss and the ¯\nL-PGD attack on a pair of views of x is identical to the\nAdversarial-to-Adversarial and Dual Stream adversarial contrastive learning methods by\nJiang et al. (2020).\nB\nEXTENDED RESULTS\nThis appendix presents more comprehensive experimental results in Tabs. 5 and 6 and Figs. 6 to 9.\nIn addition to the ResNet50-based models discussed in Secs. 6 and 7, we also present results from two\nmodels with transformer architectures (Vaswani et al., 2017). MAE uses masked autoencoders (He\net al., 2022) and we use its ViT-Large variant. MOCOv3 is a modiﬁcation of MOCOv2 to work with\na transformer backbone (ViT-Small) (Chen et al., 2021). We apply the three unsupervised adversarial\nﬁne-tuning techniques from the main text to MOCOv3 to compare how they transfer to transformer\narchitectures. We use the exact same attack types and parameter values as for MOCOv2.\nFor all models, we present top-1 and top-5 accuracy for both the standard and the lowpass settings in\nTabs. 5 and 6. We report the results for the ℓ2- and ℓ∞-induced divergence in representation space,\nat iterations 5, 10, 30 and 50 for U-PGD attacks with both ϵ = 0.05 and ϵ = 0.10. The attacks are\nperformed with the ℓ2-induced divergence and α = 0.001. These are reported as universal quantiles\nfor the untargeted attacks and relative quantiles for the targeted attacks. We also report the breakaway\nand overlap risks, as well as the nearest neighbor accuracy, median adversarial margin, average\n16\nTable 5: Extended results for ResNet50-based models and adversarially ﬁne-tuned MOCOv2. Arrows\nshow if larger or smaller values are better.\nPixPro\nAMDIM\nSimCLR\nSimSiam\nNonsem MOCOv2\nTAR\nUNTAR\n74.1%\n58.0%\n62.3%\n67.2%\n62.7%\n49.7%\n67.4%\n60.2%\n59.9%\n57.2%\n91.0%\n80.5%\n82.2%\n86.6%\n85.2%\n73.5%\n87.7%\n82.4%\n82.2%\n80.0%\n67.7%\n50.0%\n52.7%\n59.7%\n56.3%\n47.3%\n62.2%\n58.6%\n58.2%\n55.4%\n86.4%\n73.3%\n73.9%\n81.2%\n80.2%\n71.2%\n84.0%\n81.2%\n80.7%\n78.4%\n5 iter. ↑\n66.4%\n80.5%\n80.8%\n75.0%\n73.3%\n61.5%\n65.6%\n93.0%\n94.0%\n95.4%\n10 iter. ↑\n52.7%\n67.9%\n70.3%\n65.8%\n65.9%\n46.5%\n52.1%\n87.4%\n88.9%\n91.6%\n30 iter. ↑\n33.5%\n45.6%\n52.5%\n45.3%\n44.1%\n28.3%\n29.3%\n72.4%\n74.3%\n79.9%\n50 iter. ↑\n27.4%\n37.6%\n44.3%\n36.4%\n28.1%\n22.2%\n20.5%\n62.8%\n64.8%\n71.6%\n5 iter. ↑\n70.3%\n85.5%\n62.0%\n89.4%\n76.0%\n62.2%\n65.2%\n93.3%\n94.3%\n95.6%\n10 iter. ↑\n52.3%\n69.4%\n46.8%\n76.7%\n66.9%\n46.0%\n52.3%\n87.8%\n89.3%\n91.9%\n30 iter. ↑\n31.3%\n39.9%\n29.4%\n43.4%\n45.7%\n27.3%\n29.2%\n72.5%\n74.9%\n80.2%\n50 iter. ↑\n24.5%\n30.7%\n24.2%\n32.7%\n28.4%\n21.1%\n20.5%\n62.9%\n65.2%\n72.0%\n5 iter. ↓\n0.77\n0.89\n0.49\n0.40\n0.09\n0.98\n0.57\n0.29\n0.29\n0.32\n10 iter. ↓\n0.86\n0.92\n0.59\n0.57\n0.18\n0.99\n0.75\n0.34\n0.34\n0.36\n30 iter. ↓\n0.95\n0.97\n0.78\n0.83\n0.74\n1.00\n0.93\n0.51\n0.50\n0.47\n50 iter. ↓\n0.97\n0.98\n0.84\n0.90\n0.91\n1.00\n0.97\n0.64\n0.62\n0.56\n5 iter. ↑\n69.3%\n83.1%\n85.6%\n78.0%\n78.1%\n64.3%\n68.7%\n93.0%\n93.9%\n95.2%\n10 iter. ↑\n55.6%\n71.2%\n75.9%\n69.0%\n70.4%\n48.6%\n55.0%\n87.2%\n88.6%\n91.1%\n30 iter. ↑\n34.7%\n47.2%\n56.4%\n48.6%\n48.0%\n29.0%\n31.4%\n70.9%\n72.5%\n77.9%\n50 iter. ↑\n27.9%\n38.1%\n47.3%\n38.6%\n30.3%\n22.3%\n21.9%\n59.7%\n61.2%\n68.2%\n5 iter. ↑\n74.3%\n89.0%\n68.0%\n90.9%\n78.6%\n65.8%\n68.8%\n93.1%\n94.1%\n95.3%\n10 iter. ↑\n58.0%\n74.2%\n54.0%\n80.5%\n69.4%\n48.3%\n55.4%\n87.3%\n88.9%\n91.2%\n30 iter. ↑\n34.5%\n42.1%\n30.7%\n48.6%\n48.0%\n28.0%\n31.4%\n70.9%\n72.8%\n78.2%\n50 iter. ↑\n27.0%\n31.2%\n24.0%\n35.3%\n29.8%\n21.2%\n22.1%\n60.3%\n62.1%\n68.2%\n5 iter. ↓\n0.74\n0.88\n0.41\n0.36\n0.09\n0.98\n0.53\n0.29\n0.30\n0.32\n10 iter. ↓\n0.84\n0.92\n0.50\n0.52\n0.17\n0.99\n0.72\n0.34\n0.35\n0.36\n30 iter. ↓\n0.94\n0.97\n0.71\n0.80\n0.68\n1.00\n0.92\n0.54\n0.53\n0.49\n50 iter. ↓\n0.96\n0.98\n0.80\n0.88\n0.90\n1.00\n0.96\n0.69\n0.67\n0.61\n5 iter. ↓\n98.7%\n14.4%\n81.3%\n43.7%\n72.6%\n46.8%\n65.1%\n0.0%\n0.0%\n0.0%\n10 iter. ↓\n99.9%\n85.6%\n98.5%\n99.4%\n98.6%\n98.1%\n99.0%\n0.0%\n0.0%\n0.0%\n30 iter. ↓\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n9.8%\n0.7%\n0.0%\n50 iter. ↓\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n75.4%\n30.4%\n3.4%\n5 iter. ↓\n74.1%\n25.0%\n76.4%\n64.2%\n43.8%\n51.7%\n62.3%\n0.0%\n0.0%\n0.0%\n10 iter. ↓\n99.3%\n72.7%\n95.7%\n89.9%\n96.8%\n97.7%\n98.7%\n0.0%\n0.0%\n0.0%\n30 iter. ↓\n99.9%\n97.4%\n99.9%\n99.8%\n99.9%\n99.9%\n99.9%\n15.1%\n2.0%\n0.1%\n50 iter. ↓\n99.9%\n98.7%\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n73.0%\n39.0%\n5.7%\n5 iter. ↑\n0.72\n0.88\n0.44\n0.63\n0.30\n0.97\n0.49\n0.98\n0.98\n0.99\n10 iter. ↑\n0.64\n0.78\n0.32\n0.52\n0.16\n0.91\n0.31\n0.93\n0.95\n0.97\n30 iter. ↑\n0.56\n0.62\n0.11\n0.40\n0.08\n0.78\n0.18\n0.69\n0.75\n0.85\n50 iter. ↑\n0.52\n0.56\n0.04\n0.36\n0.06\n0.73\n0.15\n0.52\n0.59\n0.72\n5 iter. ↓\n99.4%\n11.3%\n92.1%\n48.3%\n65.7%\n43.2%\n69.3%\n0.0%\n0.0%\n0.0%\n10 iter. ↓\n99.9%\n77.5%\n98.9%\n98.7%\n98.0%\n96.6%\n98.8%\n0.0%\n0.0%\n0.0%\n30 iter. ↓\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n30.6%\n6.1%\n0.4%\n50 iter. ↓\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n91.7%\n65.3%\n18.4%\n5 iter. ↓\n78.6%\n24.6%\n87.1%\n67.1%\n43.8%\n49.1%\n63.6%\n0.0%\n0.0%\n0.0%\n10 iter. ↓\n99.0%\n69.6%\n97.3%\n89.5%\n95.6%\n96.0%\n98.0%\n0.0%\n0.0%\n0.0%\n30 iter. ↓\n99.9%\n97.4%\n99.9%\n99.8%\n99.9%\n99.9%\n99.9%\n37.7%\n11.2%\n1.0%\n50 iter. ↓\n99.9%\n98.9%\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n90.2%\n66.6%\n23.7%\n5 iter. ↑\n0.68\n0.89\n0.37\n0.54\n0.21\n0.96\n0.41\n0.94\n0.95\n0.98\n10 iter. ↑\n0.62\n0.79\n0.27\n0.45\n0.08\n0.91\n0.26\n0.88\n0.90\n0.95\n30 iter. ↑\n0.54\n0.62\n0.08\n0.35\n0.03\n0.77\n0.15\n0.57\n0.64\n0.78\n50 iter. ↑\n0.52\n0.55\n0.00\n0.32\n0.02\n0.71\n0.13\n0.39\n0.45\n0.60\n0.120%\n0.190%\n0.414%\n0.039%\n0.146%\n0.211%\n0.254%\n0.049%\n0.001%\n0.000%\n0.0%\n0.0%\n0.1%\n0.1%\n0.0%\n0.0%\n0.0%\n17.0%\n35.6%\n64.4%\n92.5%\n39.6%\n34.3%\n45.6%\n40.7%\n93.5%\n91.8%\n0.0%\n0.0%\n0.0%\n-18.6%\n4.1%\n2.7%\n1.2%\n1.6%\n-27.2%\n-16.5%\n61.5%\n67.5%\n76.0%\n38.9%\n54.1%\n68.3%\n54.0%\n61.7%\n54.7%\n56.3%\n73.0%\n75.2%\n77.5%\n3 iter. ↓\n41.5%\n43.0%\n23.2%\n40.0%\n21.0%\n48.7%\n34.2%\n18.9%\n15.7%\n15.2%\n10 iter. ↓\n76.5%\n71.3%\n71.9%\n74.5%\n52.4%\n75.5%\n62.8%\n60.1%\n56.3%\n54.6%\n30 iter. ↓\n87.1%\n83.3%\n88.9%\n85.6%\n76.3%\n83.9%\n74.4%\n75.1%\n69.5%\n70.0%\n50 iter. ↓\n89.9%\n86.7%\n91.0%\n88.6%\n80.6%\n85.6%\n76.4%\n76.2%\n71.8%\n71.9%\nAdversarially fine-tuned MOCOv2\nLowpass Top-1 accuracy ↑\nLowpass Top-5 accuracy ↑\nℓ₂ distance\nℓ∞ distance\nCosine\nsimilarity\nTargeted U-PGD attack\nℓ∞ distance\nCosine\nsimilarity\nUnargeted U-PGD attack\nImpersonation \nrate\nBreakaway risk ↓\nNearest neighbor accuracy ↑\nOverlap risk ↓\nMedian adversarial margin ↑\nAverage Certified Radius ↑\nℓ₂ distance\nℓ∞ distance\nCosine\nsimilarity\nℓ₂ distance\nTop-1 accuracy ↑\nTop-5 accuracy ↑\nɛ = 0.10\nɛ = 0.05\nɛ = 0.10\nStandard ResNet-based unsupervised models\nResNet50\nɛ = 0.05\nCosine\nsimilarity\nℓ₂ distance\nℓ∞ distance\nL!−PGD\n17\nTable 6: Extended results for transformer-based models and adversarially ﬁne-tuned MOCOv3.\nArrows show if larger or smaller values are better.\nMAE MOCOv3\nTAR\nUNTAR\n71.8%\n68.6%\n67.9%\n66.5%\n65.3%\n88.9%\n87.7%\n87.5%\n86.6%\n85.8%\n66.2%\n63.4%\n64.2%\n63.3%\n62.6%\n85.1%\n83.8%\n84.7%\n84.0%\n83.4%\n5 iter. ↑\n80.6%\n72.0%\n86.2%\n87.1%\n89.8%\n10 iter. ↑\n66.3%\n63.2%\n77.3%\n79.3%\n82.3%\n30 iter. ↑\n42.2%\n31.0%\n50.6%\n49.9%\n47.6%\n50 iter. ↑\n33.7%\n20.3%\n31.3%\n30.6%\n26.2%\n5 iter. ↑\n72.4%\n72.6%\n87.9%\n89.0%\n90.5%\n10 iter. ↑\n54.4%\n62.4%\n78.5%\n81.2%\n82.8%\n30 iter. ↑\n27.6%\n30.6%\n52.3%\n51.3%\n48.3%\n50 iter. ↑\n20.0%\n19.7%\n32.1%\n30.4%\n26.2%\n5 iter. ↓\n0.99\n0.26\n0.06\n0.06\n0.06\n10 iter. ↓\n0.99\n0.44\n0.14\n0.14\n0.15\n30 iter. ↓\n1.00\n0.91\n0.65\n0.70\n0.76\n50 iter. ↓\n1.00\n0.96\n0.89\n0.91\n0.94\n5 iter. ↑\n83.6%\n76.1%\n87.2%\n89.2%\n91.2%\n10 iter. ↑\n71.7%\n66.9%\n78.8%\n81.0%\n83.4%\n30 iter. ↑\n44.7%\n33.7%\n51.5%\n47.9%\n46.1%\n50 iter. ↑\n33.7%\n21.4%\n30.5%\n27.9%\n24.2%\n5 iter. ↑\n76.4%\n76.1%\n88.7%\n90.2%\n91.8%\n10 iter. ↑\n60.6%\n64.9%\n80.2%\n82.2%\n84.0%\n30 iter. ↑\n29.5%\n33.0%\n53.1%\n48.0%\n47.0%\n50 iter. ↑\n20.3%\n20.7%\n31.2%\n27.8%\n24.7%\n5 iter. ↓\n0.99\n0.26\n0.06\n0.06\n0.07\n10 iter. ↓\n0.99\n0.42\n0.14\n0.16\n0.17\n30 iter. ↓\n1.00\n0.90\n0.63\n0.77\n0.80\n50 iter. ↓\n1.00\n0.96\n0.90\n0.94\n0.95\n5 iter. ↓\n3.2%\n91.1%\n1.9%\n0.4%\n0.1%\n10 iter. ↓\n68.7%\n99.0%\n36.0%\n4.9%\n0.5%\n30 iter. ↓\n99.9%\n99.8%\n99.3%\n98.7%\n96.5%\n50 iter. ↓\n99.9%\n99.9%\n99.8%\n99.9%\n99.9%\n5 iter. ↓\n8.7%\n88.2%\n1.7%\n0.4%\n0.1%\n10 iter. ↓\n40.3%\n99.0%\n35.9%\n4.1%\n0.7%\n30 iter. ↓\n99.9%\n99.9%\n99.1%\n98.3%\n95.5%\n50 iter. ↓\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n5 iter. ↑\n0.99\n0.22\n0.72\n0.74\n0.80\n10 iter. ↑\n0.98\n0.12\n0.47\n0.56\n0.68\n30 iter. ↑\n0.26\n0.07\n0.16\n0.10\n0.16\n50 iter. ↑\n-0.12\n0.06\n0.12\n0.05\n0.08\n5 iter. ↓\n58.9%\n91.6%\n3.4%\n0.8%\n0.1%\n10 iter. ↓\n99.9%\n99.0%\n43.7%\n9.5%\n1.0%\n30 iter. ↓\n99.9%\n99.8%\n99.5%\n99.3%\n98.9%\n50 iter. ↓\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n5 iter. ↓\n50.5%\n92.3%\n3.4%\n0.8%\n0.2%\n10 iter. ↓\n99.5%\n99.2%\n42.8%\n8.7%\n1.4%\n30 iter. ↓\n99.9%\n99.9%\n99.4%\n99.2%\n98.4%\n50 iter. ↓\n99.9%\n99.9%\n99.9%\n99.9%\n99.9%\n5 iter. ↑\n0.99\n0.16\n0.66\n0.71\n0.78\n10 iter. ↑\n0.94\n0.08\n0.40\n0.49\n0.62\n30 iter. ↑\n-0.06\n0.03\n0.13\n0.08\n0.12\n50 iter. ↑\n-0.23\n0.02\n0.10\n0.04\n0.06\n0.238%\n0.230%\n0.406%\n0.401%\n0.285%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n19.8%\n62.8%\n9.6%\n5.7%\n4.4%\n11.1%\n-3.4%\n18.0%\n19.6%\n26.6%\n42.8%\n13.2%\n31.4%\n30.6%\n38.5%\n3 iter. ↓\n12.5%\n21.0%\n38.3%\n29.7%\n22.7%\n10 iter. ↓\n43.2%\n52.4%\n77.5%\n75.1%\n76.0%\n30 iter. ↓\n74.6%\n82.7%\n93.4%\n93.1%\n92.9%\n50 iter. ↓\n83.9%\n90.2%\n95.6%\n95.4%\n95.4%\nTransformer models\nAdversarially fine-tuned MOCOv3\nTop-1 accuracy ↑\nTop-5 accuracy ↑\nLowpass Top-1 accuracy ↑\nLowpass Top-5 accuracy ↑\nℓ₂ distance\nℓ∞ distance\nCosine\nsimilarity\nTargeted U-PGD attack\nℓ∞ distance\nCosine\nsimilarity\nɛ = 0.05\nɛ = 0.10\nℓ₂ distance\nImpersonation \nrate\nℓ₂ distance\nℓ∞ distance\nCosine\nsimilarity\nℓ₂ distance\nBreakaway risk ↓\nNearest neighbor accuracy ↑\nOverlap risk ↓\nMedian adversarial margin ↑\nAverage Certified Radius ↑\nUnargeted U-PGD attack\nɛ = 0.05\nɛ = 0.10\nCosine\nsimilarity\nℓ∞ distance\nL!−PGD\n18\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nInput radius\n0.0\n0.1\n0.2\n0.3\n0.4\nCertified accuracy\nResNet50\nMOCOv2\nMOCOv3\nPixPro\nAMDIM\nMAE\nSimCLR\nSimSiam\nMOCO-Nonsem\nFigure 6: Certiﬁed accuracy of the standard mod-\nels on ImageNet.\n10\n8\n10\n6\n10\n4\n10\n2\n100\nCertified representation radius\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFraction of dataset\nResNet50\nMOCOv2\nMOCOv3\nPixPro\nAMDIM\nMAE\nSimCLR\nSimSiam\nMOCO-Nonsem\nFigure 7: Certiﬁed robustness of the standard\nmodels on PASS using center smoothing. The\ndistribution of certiﬁed radii in R reported as\npercentile of the distribution of clean representa-\ntion distances is shown. Smaller values indicate\nhigher unsupervised robustness.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nInput radius\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nCertified accuracy\nMOCOv3\nMOCOv3 TAR\nMOCOv3 UNTAR\nMOCOv3 -PGD\nFigure 8: Certiﬁed accuracy of MOCOv3 and\nits adversarially trained variants on ImageNet\ncomputed via randomized smoothing. The adver-\nsarially trained models are uniformly certiﬁably\nmore robust for almost all radius values.\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nCertified representation radius\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFraction of dataset\nMOCOv3\nMOCOv3 TAR\nMOCOv3 UNTAR\nMOCOv3 -PGD\nFigure 9: Certiﬁed robustness of MOCOv3 on\nPASS using center smoothing. The distribution\nof certiﬁed radii in R reported as a percentile\nof the distribution of clean representation dis-\ntances is shown. Smaller values indicate higher\nunsupervised robustness.\ncertiﬁed radius and impersonation rates as in the main text. Figs. 6 to 8 show the certiﬁed robustness\nand accuracy of the models which were omitted from the main text.\nSome of the models, including MOCOv2, are trained with a contrastive objective based on the cosine\nsimilarity. Therefore, using the Euclidean distances as the divergence in representation space could\nbe considered an unfair comparison as the adversarially trained models are optimized for it while\nthe standard models are optimized for the cosine similarity. Therefore, for targeted attacks, we also\nreport the median cosine similarity between the representations of the adversarial examples and the\nrepresentations of the target samples. Higher values mean that the attack is more successful and that\nthe model is less robust. For the untargeted attacks, we report the median cosine similarity between\nthe representations of the adversarial examples and the representations of the original samples. Hence,\nhigher values mean that the attack is less successful and the model is more robust. The results in\nTabs. 5 and 6 show that adversarial training with the ℓ2-induced divergence leads also to improvements\nwhen measuring the cosine similarity: the divergence that the standard models are trained for but\nthe adversarial ones are not. This evidence supports our claim that the improvements we see from\nunsupervised adversarial ﬁne-tuning are not due to our choice of divergence.\n19\nAmongst the standard models, MAE outperforms the ResNet50-based models on most measures.\nIt has the highest accuracy of all models trained in the unsupervised regime, i.e. excluding the\nsupervised ResNet50. MAE also has some of the best robustness against targeted U-PGD attacks and\nis competitive to PixPro for the untargeted case. It attains the lowest breakaway and overlap risks\namong all standard models as well as the largest median adversarial margin and is most robust to\nimpersonation attacks.\nMOCOv3 scores rather poorly in comparison across most measures though. Hence, the robustness\nof MAE cannot be solely attributed to the transformer backbone. This complements the observed\nvariation in robustness performance among the ResNet50 models and provides further evidence that\nit is the objective function, rather than the backbone architecture, that determines robustness.\nThe three adversarially trained MOCOv3 models witness a lower accuracy penalty than the corre-\nsponding MOCOv2 models: between 0.2% and 3.3% for the clean and between -0.9% and 0.8% for\nthe lowpass accuracy for MOCOv3 compared with correspondingly 5.3%-10.2% and 2.8%-6.8% for\nMOCOv2. In fact, the adversarially trained MOCOv3 TAR model has a higher lowpass accuracy\nthan the standard MOCOv3. Unsupervised adversarial training also leads to a uniformly better ro-\nbustness against targeted and untargeted U-PGD attacks, albeit with a lower improvement compared\nto MOCOv2. We similarly witness large improvements in the overlap risk and median adversarial\nmargin measures. The average certiﬁed radius and the certiﬁed accuracy (Fig. 8) are also signiﬁcantly\nimproved by adversarial training. Fig. 9 shows that the adversarially trained models are also more\ncertiﬁably robust than the baseline MOCOv3.\nHowever, the three adversarially trained models actually fare worse than the baseline MOCOv3 for\nbreakaway risk and are less robust to impersonation attacks. The impersonation attacks are also less\nsemantic in nature than the ones for the MOCOv2 adversarially trained models (Figs. 16 to 18 vs\nFigs. 21 to 23). This could also be due to the learning rate being too low, rather than due to transformer\nmodels being inherently more difﬁcult to adversarially ﬁne-tune in an unsupervised setting. The\nlower accuracy gap and the lower robustness further indicate that the adversarial training might not\nhave been as “aggressive” for MOCOv3 as it was for MOCOv2. Still, while the improvements for\nMOCOv3 are not as drastic as for MOCOv2, unsupervised adversarial training does improve most\nrobustness measures. The lower effectiveness for MOCOv3 of unsupervised adversarial training,\nespecially in its role as a defence against impersonation attacks, is an avenue for future work that\nshould examine whether there are fundamental differences between unsupervised adversarial training\nof CNN and Transformer models.\nC\nTRAINING AND EVALUATION DETAILS\nThis section provides further details on the unsupervised adversarial training and the evaluation\nmetrics implementations.\nC.1\nUNSUPERVISED ADVERSARIAL TRAINING FOR MOCOV2\nThe three variants for the adversarially trained MOCOv2 are obtained by using a modiﬁcation of\nthe ofﬁcial MOCO source code. We perform ﬁne-tuning by resuming the training procedure for\nadditional 10 epochs but with the modiﬁed training loop. The unsupervised adversarial examples\nare concatenated to the model’s q-inputs and the k-inputs are correspondingly duplicated as shown\nin List. 1. For ¯\nL-PGD we use InfoNCE (Oord et al., 2018), the loss that MOCOv2 is trained with.\nAll parameters, including the learning rate and its decay are as used for the original training and as\nreported by He et al. (2020). We only reduced the batch size from 256 to 192 in order to be able to\ntrain on four GeForce RTX 2080 Ti GPUs.\nListing 1: Pseudocode of adversarial ﬁne-tuning for MoCo (modiﬁed from (He et al., 2020)).\n# f_q, f_k: encoder networks for query and key\n# queue: dictionary as a queue of K keys (CxK)\n# m: momentum\n# t: temperature\nf_k.params = f_q.params # initialize\n20\nfor x in loader: # load a minibatch x with N samples\nx_q = aug(x) # a randomly augmented version\nx_k = aug(x) # another randomly augmented version\n# perform the attack\nswitch attack_type:\ncase targeted:\ntarget_representation = roll(f_q(x_k), shifts=1)\nx_adv = targeted_upgd(x_q, target_representation)\ncase untargeted:\nx_adv = untargeted_upgd(x_q)\ncase loss:\nx_adv = batch_loss_upgd(f_q, x_q, f_k, x_k, queue, m, t)\n# get the representations\nq_clean = f_q.forward(x_q) # queries: NxC\nq_adv = f_q.forward(x_adv) # adversarial: NxC\nq = cat([q_clean, q_adv], dim=0)\nk = f_k.forward(x_k) # keys: NxC\nk = k.detach() # no gradient to keys\n# positive logits: 2Nx1\nl_pos = bmm(q.view(2*N,1,C), cat([k, k], dim=0).view(2*N,C,1))\n# negative logits: 2NxK\nl_neg = mm(q.view(2*N,C), queue.view(C,K))\n# logits: 2Nx(1+K)\nlogits = cat([l_pos, l_neg], dim=1)\n# contrastive loss\nlabels = zeros(2N) # positives are the 0-th\nloss = CrossEntropyLoss(logits/t, labels)\n# SGD update: query network\nloss.backward()\nupdate(f_q.params)\n# momentum update: key network\nf_k.params = m*f_k.params+(1-m)*f_q.params\n# update dictionary\nenqueue(queue, k) # enqueue the current minibatch\ndequeue(queue) # dequeue the earliest minibatch\nC.2\nUNSUPERVISED ADVERSARIAL TRAINING FOR MOCOV3\nSimilarly to MOCOv2, the three variants for the adversarially trained MOCOv3 are obtained by using\na modiﬁcation of the ofﬁcial MOCOv3 source code. We perform ﬁne-tuning by resuming the training\nprocedure for additional 10 epochs but with the modiﬁed training loop. The unsupervised adversarial\nexamples are added to the contrastive loss as shown in List. 2. All parameters, are as used for the\noriginal training and as reported by Chen et al. (2021). We only increased the learning rate from\n1.5 × 10−4 to 1.5 × 10−3 and reduced the batch size from 256 to 192 in order to be able to train on\nfour GeForce RTX 2080 Ti GPUs.\nListing 2: Pseudocode of adversarial fune-tuning for MOCOv3.\n# f_base: base encoder network\n# f_predictor: predictor network\n# m: momentum\nf_momentum.params = f_q.params # initialize momentum encoder\nfor x in loader: # load a minibatch x with N samples\nx_0 = aug(x) # a randomly augmented version\nx_1 = aug(x) # another randomly augmented version\n21\n# perform the attack\nswitch attack_type:\ncase targeted:\ntarget_representation = roll(f_predictor(f_base(x_1)), shifts=1)\nx_adv = targeted_upgd(x_0, target_representation)\ncase untargeted:\nx_adv = untargeted_upgd(x_0)\ncase loss:\nx_adv = batch_loss_upgd(f_base, f_predictor, x_0, x_1)\n# update the momentum encoder\nf_momentum.params = f_momentum.params * m + f_base.params * (1-m)\n# get the base representations\nq_0 = f_predictor.forward(f_base.forward(x_0)) # x_0 reps: NxC\nq_1 = f_predictor.forward(f_base.forward(x_1)) # x_1 reps: NxC\nq_adv = f_predictor.forward(f_base.forward(x_adv)) # attacked reps: NxC\n# get the momentum representations\nk_0 = f_momentum.forward(x_0) # x_0 reps: NxC\nk_1 = f_momentum.forward(x_1) # x_1 reps: NxC\nk_adv = f_momentum.forward(x_adv) # attacked reps: NxC\n# compute the loss\nloss = contrastive_loss(q_0, k_1) + contrastive_loss(q_1, k_0) + \\\ncontrastive_loss(q_adv, k_1) + contrastive_loss(q_1, k_adv)\n# parameter update: query network\nloss.backward()\nupdate(f_q.params)\nC.3\nLINEAR PROBES\nAs part of the evaluation we train three linear probes for each model.\n• Standard linear probe: for computing the top-1 and top-5 accuracy on clean samples, as\nwell as for the impersonation attack evaluation.\n• Lowpass linear probe: for computing the top-1 and top-5 accuracy on samples with\nremoved high-frequency components. We use the implementation of Wang et al. (2020)\nand keep only the Fourier components that are within a radius of 50 from the center of the\nFourier-transformed image.\n• Gaussian noise linear probe: trained on samples with added Gaussian noise for computing\nthe certiﬁed accuracy as randomized smoothing results in a more robust model when the\nbase model is trained with aggressive Gaussian noise (Lecuyer et al., 2019). Therefore, we\nadd Gaussian noise with σ = 0.25 to all inputs.\nAll linear probes are trained with the train set of ImageNet Large Scale Visual Recognition Challenge\n2012 (Russakovsky et al., 2015) and are evaluated on its test set. For training we use modiﬁcation of\nthe MOCO linear probe evaluation code for 25 epochs. The starting learning rate is 30.0 with 10-fold\nreductions applied at epochs 15 and 20.\nFor fairness of the comparison, we use the same implementation to evaluate all models. Therefore,\nthere might be differences between the accuracy values reported by us and the ones reported in the\noriginal publications of the respective models.\n22\nC.4\nCOMPUTING THE DISTRIBUTION OF INTER-REPRESENTATIONAL DIVERGENCES\nThe distribution of ℓ2 and ℓ∞-induced divergences between the representations of clean samples\nof PASS (Asano et al., 2021) is needed for computing the universal and relative quantiles. Due to\ncomputational restrictions, we compute the representations of 10,000 samples and the divergences\nbetween all pairs of them in order to construct the empirical estimate of the distribution of inter-\nrepresentational divergences. We observe that 10,000 samples are more than sufﬁcient for the\nempirical estimate of the distribution to converge.\nC.5\nADVERSARIAL ATTACKS\nIn Tabs. 2, 3, 5 and 6 we report U-PGD attacks performed with d(x, x′) = ∥x −x′∥2 and α = 0.001.\nWe report median values over the same 1000 samples from PASS (Asano et al., 2021). The median\nuniversal quantile is reported for targeted attacks and the median relative quantile is reported for\ntargeted attacks as explained in Sec. 5. In Tabs. 2 and 3 we report only the resulting ℓ2 quantiles,\nwhile in the extended results (Tabs. 5 and 6) we also show the ℓ∞quantiles and cosine similarities.\nC.6\nBREAKAWAY RISK AND NEAREST NEIGHBOR ACCURACY\nThe breakaway risk and nearest neighbor accuracy are also computed by attacking the same 1000\nsamples from PASS (D′) and computing their divergences with all other samples from PASS (D).\nOur empirical estimate is then :\nˆpbreakaway =\n1\n|D′|(|D| −1)\nX\nxi∈D′\nX\nxj∈D/{xi}\n1 [d(f(ˆxi), f(xj)) < d(f(ˆxi), f(xi))] ,\nwhere ˆxi is the untargeted U-PGD attack with d(x, x′) = ∥x −x′∥2, ϵ = 0.05 and α = 0.001 for 25\niterations.\nC.7\nOVERLAP RISK AND MEDIAN ADVERSARIAL MARGIN\nThe overlap risk and median adversarial margin are computed over 1000 pairs of samples from PASS\n(D′). Each element of the pair is attacked to have a representation similar to the other element.\nˆpoverlap =\n1\n|D′|\nX\n(x,x′)∈D′\n1\nh\nd(f(xi), f(ˆx→i\nj )) < d(f(xi), f(ˆx→j\ni\n))\ni\n,\nwhere ˆx→j\ni\nis the targeted U-PGD attack on xi towards xj with d(x, x′) = ∥x −x′∥2, ϵ = 0.05 and\nα = 0.001 for 10 iterations.\nC.8\nCERTIFIED ACCURACY\nWe use the randomized smoothing implementation by Cohen et al. (2019). We evaluate the Gaussian\nnoise linear probe (see App. C.3) over 200 samples from the ImageNet test set (Russakovsky et al.,\n2015). We use σ = 0.25, N0 = 100, N = 100, 000 and an error probability α = 0.001, as originally\nused by Cohen et al. (2019). Figs. 3 and 8 show the resulting certiﬁed accuracy for MOCOv2,\nMOCOv3, and their unsupervised adversarially trained versions. Fig. 6 shows the certiﬁed accuracy\nfor the other models. These plots show the fraction of samples which are correctly classiﬁed and\nwhich certiﬁably have the same classiﬁcation within a given ℓ2 radius of the input space.\nTabs. 5 and 6 also show the Average Certiﬁed Radius for all models. The Average Certiﬁed Radius\nwas proposed by Zhai et al. (2020) as a way to summarize the certiﬁed accuracy vs radius plots with\na single number representing the average certiﬁed radius for the correctly classiﬁed samples.\nC.9\nCERTIFIED ROBUSTNESS\nThe certiﬁed robustness evaluation in Figs. 4 and 8 was done with the center smoothing implemen-\ntation by Kumar & Goldstein (2021). We evaluate the models over the same 200 samples from the\nImageNet test set (Russakovsky et al., 2015). We use σ = 0.25, N0 = 10, 000, N = 100, 000 and\nerror probabilities α1 = 0.005, α2 = 0.005, as originally proposed by Kumar & Goldstein (2021).\n23\nC.10\nIMPERSONATION ATTACKS\nThe impersonation attack evaluation is performed using targeted U-PGD attacks. We use the Assira\ndataset that contains 25,000 images, equally split between cats and dogs (Elson et al., 2007). When\nevaluating a model, we consider only the subset of images that the standard linear probe (see App. C.3)\nfor the given model classiﬁes correctly. Then, we construct pairs of an image of a cat and an image of\na dog. We perform two attacks: attacking the cat to have a representation as close as possible to that\nof the dog and vice-versa. The attacked images are then classiﬁed with the clean linear probe. The\nsuccess rate of cats impersonating dogs and dogs impersonating cats are computed separately and\nthen averaged to account for possible class-based differences. Note that the linear probe is not used\nfor constructing the attack, i.e. we indeed fool it without accessing it. App. D shows examples of the\nimpersonation attacks for all models.\nD\nIMPERSONATION ATTACKS\nThis appendix showcases samples of the impersonation attacks on the models discussed in the\npaper. The ﬁrst and third row in each sample are the original images of cats and dogs respectively.\nThe second row is the result when each cat image is attacked to have a representation close to the\nrepresentation of the corresponding dog image. The fourth row is the opposite: the dog image attacked\nto have a representation close to the representation of the cat image. The attack used was targeted\nU-PGD with d(r, r′) = ∥r −r′∥2 for 50 iterations with ϵ = 0.10 and α = 0.01. The samples shown\ndiffer from model to model as we restrict the evaluation to the samples that are correctly predicted by\nthe given model, see App. C.10 for details.\nA key observation is that the perturbations necessary to fool the standard models visually appear as\nnoise (Figs. 10 to 15). However, the perturbations applied to the three adversarially trained MOCOv2\nmodels (Figs. 16 to 18) are more “semantic” in nature, and in some cases even resemble features of\nthe target class. Still, this is not the case when comparing the impersonation attacks on MOCOv3\n(Fig. 20) with the attacks on the adversarially trained versions (Figs. 21 to 23).\nFigure 10: Impersonation attack samples for ResNet50 (He et al., 2016).\n24\nFigure 11: Impersonation attack samples for MOCO with non-semantic negatives (Ge et al., 2021).\nFigure 12: Impersonation attack samples for PixPro (Xie et al., 2021).\nFigure 13: Impersonation attack samples for SimCLRv2 (Chen et al., 2020c).\n25\nFigure 14: Impersonation attack samples for SimSiam (Chen & He, 2021).\nFigure 15: Impersonation attack samples for MOCOv2 (He et al., 2020; Chen et al., 2020d).\nFigure 16: Impersonation attack samples for MOCOv2 TAR.\n26\nFigure 17: Impersonation attack samples for MOCOv2 UNTAR.\nFigure 18: Impersonation attack samples for MOCOv2 LOSS.\nFigure 19: Impersonation attack samples for MAE (He et al., 2022).\n27\nFigure 20: Impersonation attack samples for MOCOv3 (Chen et al., 2021).\nFigure 21: Impersonation attack samples for MOCOv3 TAR.\n28\nFigure 22: Impersonation attack samples for MOCOv3 UNTAR.\nFigure 23: Impersonation attack samples for MOCOv3 LOSS.\n29\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-10-08",
  "updated": "2022-10-08"
}