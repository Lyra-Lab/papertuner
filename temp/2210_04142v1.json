{
  "id": "http://arxiv.org/abs/2210.04142v1",
  "title": "Deep Clustering: A Comprehensive Survey",
  "authors": [
    "Yazhou Ren",
    "Jingyu Pu",
    "Zhimeng Yang",
    "Jie Xu",
    "Guofeng Li",
    "Xiaorong Pu",
    "Philip S. Yu",
    "Lifang He"
  ],
  "abstract": "Cluster analysis plays an indispensable role in machine learning and data\nmining. Learning a good data representation is crucial for clustering\nalgorithms. Recently, deep clustering, which can learn clustering-friendly\nrepresentations using deep neural networks, has been broadly applied in a wide\nrange of clustering tasks. Existing surveys for deep clustering mainly focus on\nthe single-view fields and the network architectures, ignoring the complex\napplication scenarios of clustering. To address this issue, in this paper we\nprovide a comprehensive survey for deep clustering in views of data sources.\nWith different data sources and initial conditions, we systematically\ndistinguish the clustering methods in terms of methodology, prior knowledge,\nand architecture. Concretely, deep clustering methods are introduced according\nto four categories, i.e., traditional single-view deep clustering,\nsemi-supervised deep clustering, deep multi-view clustering, and deep transfer\nclustering. Finally, we discuss the open challenges and potential future\nopportunities in different fields of deep clustering.",
  "text": "1\nDeep Clustering: A Comprehensive Survey\nYazhou Ren, Member, IEEE, Jingyu Pu, Zhimeng Yang, Jie Xu, Guofeng Li, Xiaorong Pu,\nPhilip S. Yu, Fellow, IEEE, Lifang He, Member, IEEE\nAbstract—Cluster analysis plays an indispensable role in machine learning and data mining. Learning a good data representation is\ncrucial for clustering algorithms. Recently, deep clustering, which can learn clustering-friendly representations using deep neural\nnetworks, has been broadly applied in a wide range of clustering tasks. Existing surveys for deep clustering mainly focus on the\nsingle-view ﬁelds and the network architectures, ignoring the complex application scenarios of clustering. To address this issue, in this\npaper we provide a comprehensive survey for deep clustering in views of data sources. With different data sources and initial\nconditions, we systematically distinguish the clustering methods in terms of methodology, prior knowledge, and architecture.\nConcretely, deep clustering methods are introduced according to four categories, i.e., traditional single-view deep clustering,\nsemi-supervised deep clustering, deep multi-view clustering, and deep transfer clustering. Finally, we discuss the open challenges and\npotential future opportunities in different ﬁelds of deep clustering.\nIndex Terms—Deep clustering; semi-supervised clustering; multi-view clustering; transfer learning\n!\n1\nINTRODUCTION\nW\nITH the development of online media, abundant data with\nhigh complexity can be gathered easily. Through pinpoint\nanalysis of these data, we can dig the value out and use these\nconclusions in many ﬁelds, such as face recognition [1], [2],\nsentiment analysis [3], [4], intelligent manufacturing [5], [6], etc.\nA model which can be used to classify the data with different\nlabels is the base of many applications. For labeled data, it is\ntaken granted to use the labels as the most important information\nas a guide. For unlabeled data, ﬁnding a quantiﬁable objective as\nthe guide of the model-building process is the key question of\nclustering. Over the past decades, a large number of clustering\nmethods with shallow models have been proposed, including\ncentroid-based clustering [7], [8], density-based clustering [9],\n[10], [11], [12], [13], distribution-based clustering [14], hierar-\nchical clustering [15], ensemble clustering [16], [17], multi-view\nclustering [18], [19], [20], [21], [22], [23], etc. These shallow\nmodels are effective only when the features are representative,\nwhile their performance on the complex data is usually limited\ndue to the poor power of feature learning.\nIn order to map the original complex data to a feature space\nthat is easy to cluster, many clustering methods focus on feature\nextraction or feature transformation, such as PCA [24], kernel\nmethod [25], spectral method [26], deep neural network [27], etc.\nAmong these methods, the deep neural network is a promising ap-\nproach because of its excellent nonlinear mapping capability and\nits ﬂexibility in different scenarios. A well-designed deep learning\nbased clustering approach (referred to deep clustering) aims at\neffectively extracting more clustering-friendly features from data\nand performing clustering with learned features simultaneously.\nMuch research has been done in the ﬁeld of deep clustering\nand there are also some surveys about deep clustering methods\n•\nYazhou Ren, Jingyu Pu, Zhimeng Yang, Jie Xu, Guofeng Li and Xiaorong\nPu are with University of Electronic Science and Technology of China,\nChengdu 611731, China. Yazhou Ren is the corresponding author. E-mail:\nyazhou.ren@uestc.edu.cn.\n•\nPhilip S. Yu is with University of Illinois at Chicago, IL 60607, USA.\n•\nLifang He is with Lehigh University, PA 18015, USA.\nManuscript received Oct. 2022.\n[28], [29], [30], [31]. Speciﬁcally, existing systematic reviews for\ndeep clustering mainly focus on the single-view clustering tasks\nand the architectures of neural networks. For example, Aljalbout\net al. [28] focus only on deep single-view clustering methods\nwhich are based on deep autoencoder (AE or DAE). Min et\nal. [29] classify deep clustering methods from the perspective\nof different deep networks. Nutakki et al. [30] divide deep\nsingle-view clustering methods into three categories according\nto their training strategies: multi-step sequential deep clustering,\njoint deep clustering, and closed-loop multi-step deep clustering.\nZhou et al. [31] categorize deep single-view clustering methods\nby the interaction way between feature learning and clustering\nmodules. But in the real world, the datasets for clustering are\nalways associated, e.g., the taste for reading is correlated with\nthe taste for a movie, and the side face and full-face from the\nsame person should be labeled the same. For these data, deep\nclustering methods based on semi-supervised learning, multi-view\nlearning, and transfer learning have also made signiﬁcant progress.\nUnfortunately, existing reviews do not discuss them too much.\nTherefore, it is important to classify deep clustering from\nthe perspective of data sources and initial conditions. In this\nsurvey, we summarize the deep clustering from the perspective of\ninitial settings of data combined with deep learning methodology.\nWe introduce the newest progress of deep clustering from the\nperspective of network and data structure as shown in Fig. 1.\nSpeciﬁcally, we organize the deep clustering methods into the\nfollowing four categories:\n•\nDeep single-view clustering\nFor conventional clustering tasks, it is often assumed that\nthe data are of the same form and structure, as known as single-\nview or single-modal data. The extraction of representations for\nthese data by deep neural networks (DNNs) is a signiﬁcant\ncharacteristic of deep clustering. However, what is more note-\nworthy is the different applied deep learning techniques, which\nare highly correlated with the structure of DNNs. To compare the\ntechnical route of speciﬁc DNNs, we divide those algorithms into\nﬁve categories: deep autoencoder (DAE) based deep clustering,\narXiv:2210.04142v1  [cs.LG]  9 Oct 2022\n2\n6LQJOH\u0010YLHZ\n'HHS\nFOXVWHULQJ\n6HPL\u0010\nVXSHUYLVHG\n0XOWL\u0010YLHZ\n7UDQVIHU\nOHDUQLQJ\n'(&\u0010EDVHG\n6XEVSDFH\nFOXVWHULQJ\u0010EDVHG\n*11\u0010EDVHG\n'11\u0010EDVHG\n*$1\u0010EDVHG\n'DWD VWUXFWXUH\n9$(\u0010EDVHG\n'11\u0010EDVHG\n'$(\u0010EDVHG\n*$1\u0010EDVHG\n*11\u0010EDVHG\n1HWZRUN\nFig. 1: The directory tree of this survey.\ndeep neural network (DNN) based deep clustering, variational\nautoencoder (VAE) based deep clustering, generative adversarial\nnetwork (GAN) based deep clustering and graph nerual network\n(GNN) based deep clustering.\n•\nDeep clustering based on semi-supervised learning\nWhen the data to be processed contain a small part of\nprior constraints, traditional clustering methods cannot effectively\nutilize this prior information and semi-supervised clustering is an\neffective way to solve this question. In presence, the research\nof deep semi-supervised clustering has not been well explored.\nHowever, semi-supervised clustering is inevitable because it is\nfeasible to let a clustering method become a semi-supervised one\nby adding the additional information as a constraint loss to the\nmodel.\n•\nDeep clustering based on multi-view learning\nIn the real world, data are often obtained from different\nfeature collectors or have different structures. We call those data\n”multi-view data” or ”multi-modal data”, where each sample has\nmultiple representations. The purpose of deep clustering based on\nmulti-view learning is to utilize the consistent and complementary\ninformation contained in multi-view data to improve clustering\nperformance. In addition, the idea of multi-view learning may have\nguiding signiﬁcance for deep single-view clustering. In this survey,\nwe summarize deep multi-view clustering into three categories:\ndeep embedded clustering based, subspace clustering based, and\ngraph neural network based.\n•\nDeep clustering based on transfer learning\nFor a task that has a limited amount of instances and high\ndimensions, sometimes we can ﬁnd an assistant to offer additional\ninformation. For example, if task A is similar to another task B and\nB has more information for clustering than A (B is labeled or B is\neasier to clustering than A), it is useful to transfer the information\nfrom B to A. Transfer learning for unsupervised domain adaption\n(UDA) is boosted in recent years, which contains two domains:\nSource domain with labels and target domain which is unlabeled.\nThe goal of transfer learning is to apply the knowledge or patterns\nlearned from the source task to a different but related target\ntask. Deep clustering methods based on transfer learning aim to\nimprove the performance of current clustering tasks by utilizing\ninformation from relevant tasks.\nTABLE 1: Notations and their descriptions in this paper.\nNotations\nDescriptions\ni\na counter variable\nj\na counter variable\n|.|\nthe length of a set\n∥.∥\nthe 2-norm of a vector\nX\nthe data for clustering\nXs\nthe data in source domain (UDA methods)\nY s\nthe labels of source domain instances (UDA methods)\nXt\nthe data in target domain (UDA methods)\nDs\nthe source domain of UDA methods\nDt\nthe target domain of UDA methods\nxi\nthe vector of an oringinal data sample\nXi\nthe i-th view of X in multi-view learning\nˆY\nthe predicted labels of X\nS\nthe soft data assignments of X\nR\nthe adjusted assignments of S\nA\nthe pairwise constraint matrix\naij\nthe constraint of sample i and sample j\nzi\nthe vector of the embedded representation of xi\nε\nthe noise used in generative model\nE\nthe expectation\nLn\nthe network loss\nLc\nthe clustering loss\nLext\nthe extra task loss\nLrec\nthe reconstruction loss of autoencoder network\nLgan\nthe loss of GAN\nLELBO\nthe loss of evidence lower bound\nk\nthe number of clusters\nn\nthe number of data samples\nµ\nthe mean of the Gaussian distribution\nθ\nthe variance of the Gaussian distribution\nKL(.∥.)\nthe Kullback-Leibler divergence\np(.)\nthe probability distribution\np(.|.)\nthe conditional probability distribution\np(., .)\nthe joint probability distribution\nq(.)\nthe approximate probability distribution of p(.)\nq(.|.)\nthe approximate probability distribution of p(.|.)\nq(., .)\nthe approximate probability distribution of p(., .)\nf(.)\nthe feature extractor\nφe(.)\nthe encoder network of AE or VAE\nφr(.)\nthe decoder network of AE or VAE\nφg(.)\nthe generative network of GAN\nφd(.)\nthe discriminative network of GAN\nQ\nthe graph adjacency matrix\nD\nthe degree matrix of Q\nC\nthe feature matrix of a graph\nH\nthe node hidden feature matrix\nW\nthe learnable model parameters\nIt is necessary to pay attention to the different characteristics\nand conditions of the clustering data before studying the corre-\nsponding clustering methods. In this survey, existing deep cluster-\ning methods are systematically classiﬁed from data sources and\ninitial conditions. The advantages, disadvantages, and applicable\nconditions of different clustering methods are analyzed. Finally,\nwe present some interesting research directions in the ﬁeld of deep\nclustering.\n2\nDEFINITIONS AND PRELIMINARIES\nWe introduce the notations in this section. Throughout this\npaper, we use uppercase letters to denote matrices and lowercase\nletters to denote vectors. Unless otherwise stated, the notations\nused in this paper are summarized in Table 1.\nThis survey will introduce four kinds of deep clustering\nproblems based on different background conditions. Here, we\ndeﬁne these problems formally. Given a set of data samples X,\nwe aim at ﬁnding a map function F which can map X into k\n3\nclusters. The map result is represented with ˆY . So the tasks we\ncope with are:\n(1) Deep single-view clustering:\nF (X) →ˆY .\n(1)\n(2) Semi-supervised deep clustering:\nF (X, A) →ˆY ,\n(2)\nwhere A is a constrained matrix.\n(3) Deep multi-view clustering:\nF\n\u0000X1, ..., Xn\u0001 →ˆY ,\n(3)\nwhere Xi is the i-th view of X.\n(4) Deep clustering with domain adaptation:\nF\n\u0000Xs, Y s, Xt\u0001 →ˆY ,\n(4)\nwhere (Xs, Y s) is the labeled source domain and Xt is the\nunlabeled target domain.\n3\nDEEP SINGLE-VIEW CLUSTERING\nThe theory of representation learning [32] shows the impor-\ntance of feature learning (or representation learning) in machine\nlearning tasks. However, deep representation learning is mostly\nsupervised learning that requires many labeled data. As we men-\ntioned before, the obstacle of the deep clustering problem is what\ncan be used to guide the training process like labels in supervised\nproblem. The most “supervised” information in deep clustering is\nthe data itself. So how can we train an effective feature extractor to\nget good representation? According to the way the feature extrac-\ntor is trained, we divide deep single-view clustering algorithms\ninto ﬁve categories: DAE-based, DNN-based, VAE-based, GAN-\nbased, and GNN-based. The difference of these methods is mainly\nabout the loss components, where the loss terms are deﬁned in\nTable 1 and explained below:\n•\nDAE-based/GNN-based: L = Lrec + Lc,\n•\nDNN-based: L = Lext + Lc,\n•\nVAE-based: L = LELBO + Lc,\n•\nGAN-based: L = Lgan + Lc.\nIn unsupervised learning, the issue we cope with is to train\na reliable feature extractor without labels. There are mainly two\nways in existing works: 1) A loss function that optimizes the\npseudo labels according to the principle: narrowing the inner-\ncluster distance and widening the inter-cluster distance. 2) An ex-\ntra task that can help train the feature extractor. For the clustering\nmethods with specialized feature extractors, such as autoencoder,\nthe reconstruction loss Lrec can be interpreted as the extra task.\nIn this paper, the clustering-oriented loss Lc indicates the loss\nof the clustering objective. DAE-based/GNN-based methods use\nan autoencoder/graph autoencoder as the feature extractor, so the\nloss functions are always composed of a reconstruction loss Lrec\nand another clustering-oriented loss Lc. By contrast, DNN-based\nmethods optimize the feature extractor with extra tasks or other\nstrategies Lext. VAE-based methods optimize the loss of evidence\nlower bound LELBO. GAN-based methods are based on the\ngenerative adversarial loss Lgan. Based on these ﬁve dimensions,\nexisting deep single-view clustering methods are summarized in\nTable 2 and Table 3.\n3.1\nDAE-based\nThe autoencoder network [90] is originally designed for\nunsupervised representation learning of data and can learn a highly\nnon-linear mapping function. Using deep autoencoder (DAE) [91]\nis a common way to develop deep clustering methods. DAE aims\nto learn a low-dimensional embedding feature space by minimiz-\ning the reconstruction loss of the network, which is deﬁned as:\nLrec = min 1\nn\nn\nX\ni=1\n∥xi −φr (φe (xi))∥2\n(5)\nwhere φe(.) and φr(.) represent the encoder network and decoder\nnetwork of autoencoder respectively. Using the encoder as a\nfeature extractor, various clustering objective functions have been\nproposed. We summarize these deep autoencoder based cluster-\ning methods as DAE-based deep clustering. In DAE-based deep\nclustering methods, there are two main ways to get the labels.\nThe ﬁrst way embeds the data into low-dimensional features and\nthen clusters the embedded features with traditional clustering\nmethods such as the k-means algorithm [92]. The second way\njointly optimizes the feature extractor and the clustering results.\nWe refer to these two approaches as “separate analysis” and “joint\nanalysis” respectively, and elaborate on them below.\n“Separate analysis” means that learning features and clus-\ntering data are performed separately. In order to solve the prob-\nlem that representations learned by “separately analysis” are not\ncluster-oriented due to its innate characteristics, Huang et al.\npropose a deep embedding network for clustering (DEN) [34],\nwhich imposes two constraints based on DAE objective: locality-\npreserving constraint and group sparsity constraint. Locality-\npreserving constraint urges the embedded features in the same\ncluster to be similar. Group sparsity constraint aims to diagonalize\nthe afﬁnity of representations. These two constraints improve the\nclustering performance while reduce the inner-cluster distance and\nexpand inter-cluster distance. The objective of most clustering\nmethods based on DAE are working on these two kinds of\ndistance. So, in Table 2, we summarize these methods from the\nperspective of “characteristics”, which shows the way to optimize\nthe inner-cluster distance and inter-cluster distance.\nPeng et al. [35] propose a novel deep learning based frame-\nwork in the ﬁeld of Subspace clustering, namely, deep subspace\nclustering with sparsity prior (PARTY). PARTY enhances autoen-\ncoder by considering the relationship between different samples\n(i.e., structure prior) and solves the limitation of traditional sub-\nspace clustering methods. As far as we know, PARTY is the ﬁrst\ndeep learning based subspace clustering method, and it is the ﬁrst\nwork to introduce the global structure prior to the neural network\nfor unsupervised learning. Different from PARTY, Ji et al. [38]\npropose another deep subspace clustering networks (DSC-Nets)\narchitecture to learn non-linear mapping and introduce a self-\nexpressive layer to directly learn the afﬁnity matrix.\nDensity-based clustering [9], [93] is another kind of popular\nclustering methods. Ren et al. [50] propose deep density-based im-\nage clustering (DDIC) that uses DAE to learn the low-dimensional\nfeature representations and then performs density-based clustering\non the learned features. In particular, DDIC does not need to know\nthe number of clusters in advance.\n“Joint analysis” aims at learning a representation that is more\nsuitable for clustering which is different from separate analysis\napproaches that deep learning and clustering are carried out\nseparately, the neural network does not have a clustering-oriented\n4\nTABLE 2: The summaries of DAE-based and DNN-based methods in deep single-view clustering. We summarize the DAE-based methods\nbased on “Jointly or Separately” and “Characteristics”.\nNet\nMethods\nJointly or Sepa-\nrately\nCharacteristics\nDAE\nAEC (2013) [33]\nSeparately\nOptimize the distance between zi and its closest cluster centroid.\nDEN (2014) [34]\nSeparately\nLocality-preserving constraint, group sparsity constraint.\nPARTY (2016) [35]\nSeparately\nSubspace clustering.\nDEC (2016) [36]\nJointly\nOptimize the distribution of assignments.\nIDEC (2017) [37]\nJointly\nImprove DEC [36] with local structure preservation.\nDSC-Nets (2017) [38]\nSeparately\nSubspace clustering.\nDEPICT (2017) [39]\nJointly\nConvolutional autoencoder and relative entropy minimization.\nDCN (2017) [40]\nJointly\nTake the objective of k-means as the clustering loss.\nDMC (2017) [41]\nJointly\nMulti-manifold clustering.\nDEC-DA (2018) [42]\nJointly\nImprove DEC [36] with data augmentation.\nDBC (2018) [43]\nJointly\nSelf-paced learning.\nDCC (2018) [44]\nSeparately\nExtend robust continuous clustering [45] with autoencoder. Not given k.\nDDLSC (2018) [46]\nJointly\nPairwise loss function.\nDDC (2019) [47]\nSeparately\nGlobal and local constraints of relationships.\nDSCDAE (2019) [48]\nJointly\nSubspace Clustering.\nNCSC (2019) [49]\nJointly\nDual autoencoder network.\nDDIC (2020) [50]\nSeparately\nDensity-based clustering. Not given k.\nSC-EDAE (2020) [51]\nJointly\nSpectral clustering.\nASPC-DA (2020) [52]\nJointly\nSelf-paced learning and data augmentation.\nALRDC (2020) [53]\nJointly\nAdversarial learning.\nN2D (2021) [54]\nSeparately\nManifold learning.\nAGMDC (2021) [55]\nJointly\nGaussian Mixture Model. Improve the inter-cluster distance.\nNet\nMethods\nClustering-\noriented loss\nCharacteristics\nDNN\nJULE (2016) [56]\nYes\nAgglomerative clustering.\nDDBC (2017) [57]\nYes\nInformation theoretic measures.\nDAC (2017) [58]\nNo\nSelf-adaptation learning. Binary pairwise-classiﬁcation.\nDeepCluster (2018) [59]\nNo\nUse traditional clustering methods to assign labels.\nCCNN (2018) [60]\nNo\nMini-batch k-means. Feature drift compensation for large-scale image data\nADC (2018) [61]\nYes\nCentroid embeddings.\nST-DAC (2019) [62]\nNo\nSpatial transformer layers. Binary pairwise-classiﬁcation.\nRTM (2019) [63]\nNo\nRandom triplet mining.\nIIC (2019) [64]\nNo\nMutual information. Generated image pairs.\nDCCM (2019) [65]\nNo\nTriplet mutual information. Generated image pairs.\nMMDC (2019) [66]\nNo\nMulti-modal. Generated image pairs.\nSCAN (2020) [67]\nYes\nDecouple feature learning and clustering. Nearest neighbors mining.\nDRC (2020) [68]\nYes\nContrastive learning.\nPICA (2020) [69]\nYes\nMaximize the “global” partition conﬁdence.\nTABLE 3: The summaries of VAE-, GAN-, and GNN-based methods in deep single-view clustering.\nNet\nMethods\nCharacteristics\nVAE\nVaDE (2016) [70]\nGaussian mixture variational autoencoder.\nGMVAE (2016) [71]\nGaussian mixture variational autoencoder. Unbalanced clustering.\nMFVDC (2017) [72]\nContinuous Gumbel-Softmax distribution.\nLTVAE (2018) [73]\nLatent tree model.\nVLAC (2019) [74]\nVariational ladder autoencoders.\nVAEIC (2020) [75]\nNo pre-training process.\nS3VDC (2020) [76]\nImprovement on four generic algorithmic.\nDSVAE (2021) [77]\nSpherical latent embeddings.\nDVAE (2022) [78]\nAdditional classiﬁer to distinguish clusters.\nNet\nMethods\nWith DAE\nCharacteristics\nGAN\nCatGAN (2015) [79]\nNo\nCan be applied to both unsupervised and semi-supervised tasks.\nDAGC (2017) [80]\nYes\nBuild an encoder to make the data representations easier to cluster.\nDASC (2018) [81]\nYes\nSubspace clustering.\nClusterGAN-SPL (2019) [82]\nNo\nNo discrete latent variables and applies self-paced learning based on [83].\nClusterGAN (2019) [83]\nNo\nTrain a GAN with a clustering-speciﬁc loss.\nADEC (2020) [84]\nYes\nReconstruction loss and adversarial loss are optimized in turn.\nIMDGC (2022) [85]\nNo\nIntegrates a hierarchical generative adversarial network and mutual information maximization.\nNet\nMethods\nCharacteristics\nGNN\nDAEGC (2019) [71]\nPerform graph clustering and learn graph embedding in a uniﬁed framework.\nAGC (2019) [86]\nAttributed graph clustering.\nAGAE (2019) [87]\nEnsemble clustering.\nAGCHK (2020) [88]\nUtilize heat kernel in attributed graphs.\nSDCN (2020) [89]\nIntegrate the structural information into deep clustering.\nobjective when learning the features of data. Most subsequent deep\nclustering researches combine clustering objectives with feature\nlearning, which enables the neural network to learn features\nconducive to clustering from the potential distribution of data. In\nthis survey, those methods are summarized as “joint analysis”.\nInspired by the idea of non-parametric algorithm t-SNE [94],\nXie et al. [36] propose a joint framework to optimize feature\nlearning and clustering objective, which is named deep embedded\n5\nclustering (DEC). DEC ﬁrstly learns a mapping from the data\nspace to a lower-dimensional feature space via Lrec and then\niteratively optimizes the clustering loss KL(S∥R) (i.e., KL diver-\ngence). Here, S denotes the soft assignments of data that describes\nthe similarity between the embedded data and each cluster centroid\n(centroids are initialized with k-means), and R is the adjusted\ntarget distribution which has purer cluster assignments compared\nto S.\nDEC is a representative method in deep clustering due to its\njoint learning framework and low computing complexity. Based on\nDEC, a number of variants have been proposed. For example, to\nguarantee local structure in the ﬁne-tuning phase, improved deep\nembedded clustering with local structure preservation (IDEC)\n[37] is proposed to optimize the weighted clustering loss and\nthe reconstruction loss of autoencoder jointly. Deep embedded\nclustering with data augmentation (DEC-DA) [42] applies the\ndata augmentation strategy in DEC. Li et al. [43] propose dis-\ncriminatively boosted image clustering (DBC) to deal with image\nrepresentation learning and image clustering. DBC has a similar\npipeline as DEC but the learning procedure is self-paced [95],\nwhere easiest instances are ﬁrst selected and more complex sam-\nples are expanded progressively.\nIn DEC, the predicted clustering assignments are calculated\nby the Student’s t-distribution. Differently, Dizaji et al. [39]\npropose a deep embedded regularized clustering (DEPICT) with a\nnovel clustering loss by stacking a softmax layer on the embedded\nlayer of the convolutional autoencoder. What’s more, the cluster-\ning loss of DEPICT is regularized by a prior for the frequency\nof cluster assignments and layer-wise features reconstruction loss\nfunction. Yang et al. [40] directly take the objective of k-means\nas the clustering loss. The proposed model, named deep cluster-\ning network (DCN), is a joint dimensionality reduction and k-\nmeans clustering approach, in which dimensionality reduction is\naccomplished via learning a deep autoencoder. Shah et al. [44]\npropose deep continuous clustering (DCC), an extension of robust\ncontinuous clustering [45] by integrating autoencoder into the\nparadigm. DCC performs clustering learning by jointly optimizing\nthe deﬁned data loss, pairwise loss, and reconstruction loss. In\nparticular, it does not need prior knowledge of the number of\nclusters. Tzoreff et al [46] propose DDLSC (deep discriminative\nlatent space for clustering) to optimize the deep autoencoder w.r.t.\na discriminative pairwise loss function.\nDeep manifold clustering (DMC) [41] is the ﬁrst method to\napply deep learning in multi-manifold clustering [96], [97]. In\nDMC, an autoencoder consists of stacked RBMs [98] is trained to\nobtain the transformed representations. Both the reconstruction\nloss and clustering loss of DMC are different from previous\nmethods. That is, the reconstruction of one sample and its local\nneighborhood are used to deﬁne the locality preserving objective.\nThe penalty coefﬁcient and the distance, measured by the Gaussian\nkernel between samples and cluster centers, are used to deﬁne the\nclustering-oriented objective.\nThe recently proposed DAE-based clustering algorithms\nalso use the variants of deep autoencoder to learn better low-\ndimensional features and focus on improving the clustering per-\nformance by combining the ideas of traditional machine learn-\ning methods. For example, deep spectral clustering using dual\nautoencoder network (DSCDAE) [48] and spectral clustering via\nensemble deep autoencoder learning (SC-EDAE) [51] aim to inte-\ngrate spectral clustering into the carefully designed autoencoders\nfor deep clustering. Zhang et al. [49] propose neural collabo-\n•\n•\n•\n•\n•\n•\n&ODVVLILFDWLRQ\u0003/RVV\n••\u0003/RVV\n([WUD\u00037DVN\n•\nრ\nპ\nჟ\n•\nFig. 2: The framework of DNN-based learning (single-view cluster-\ning). X is the data for clustering, f is the feature extractor for X.\nPart I describes the framework of supervised learning. Y means the\nreal labels and S denotes the predicted results. With Y and S, we\ncan compute the classiﬁcation loss for backpropagation. Part II is the\nframework of methods with extra tasks. The extra tasks are used to\ntrain the nets for good embedding Z. Part III describes the process\nof the methods which need to ﬁne-tune the cluster assignments. S\ndenotes the predicted results, R is an adjustment of S.\nrative subspace clustering (NCSC) using two conﬁdence maps,\nwhich are established on the features learned by autoencoder,\nas supervision information for subspace clustering. In ASPC-DA\n(adaptive self-paced deep clustering with data augmentation [52]),\nself-paced learning idea [95] and data augmentation technique are\nsimultaneously incorporated. Its learning process is the same as\nDEC and consists of two stages, i.e., pre-training the autoencoder\nand ﬁne-tuning the encoder.\nIn general, we notice that the network structure adopted is\nrelated to the type of data to be processed. For example, fully\nconnected networks are generally used to extract one-dimensional\ndata features, while convolutional neural networks are used to\nextract image features. Most of the above DAE-based deep\nclustering methods can be implemented by both fully connected\nautoencoder and convolutional autoencoder, and thus they apply\nto various types of data to some extent. However, in the ﬁeld\nof computer vision, there is a class of deep clustering methods\nthat focus on image clustering. Those methods can date back to\n[99] and are summarized as DNN-based deep clustering because\nthey generally use convolutional neural networks to perform image\nfeature learning and semantic clustering.\n3.2\nDNN-based\nThis section introduces the DNN-based clustering methods.\nUnlike DAE-based clustering methods, DNN-based methods have\nto design extra tasks to train the feature extractor. In this survey, we\nsummarize DNN-based deep clustering methods in Table 2 from\ntwo perspectives: “clustering-oriented loss” and “characteristics”.\n“clustering-oriented loss” shows whether there is a loss function\nwhich explicitly narrows the inner-cluster distance or widens\nthe inter-cluster distance. Fig. 2 shows the framework of deep\nunsupervised learning based on a convolutional neural network.\n6\nWhen the DNN training process begins, the randomly initial-\nized feature extractor is unreliable. So, deep clustering methods\nbased on randomly initialized neural networks generally employ\ntraditional clustering tricks such as hierarchical clustering [100]\nor focus on extra tasks such as instances generation. For instance,\nYang et al. [56] propose a joint unsupervised learning method\nnamed JULE, which applies agglomerative clustering magic to\ntrain the feature extractor. Speciﬁcally, JULE formulates the joint\nlearning in a recurrent framework, where merging operations of\nagglomerative clustering are considered as a forward pass, and\nrepresentation learning of DNN as a backward pass. Based on this\nassumption, JULE also applies a loss that shrinks the inner-cluster\ndistance and expands the intra-cluster distance at the same time.\nIn each epoch, JULE merges two clusters into one and computes\nthe loss for the backward pass.\nChang et al. [58] propose deep adaptive image clustering\n(DAC) to tackle the combination of feature learning and clustering.\nIn DAC, the clustering problem is reconstructed into binary\npairwise classiﬁcation problems that judge whether the pairwise\nimages with estimated cosine similarities belong to the same\ncluster. Then it adaptively selects similar samples to train DNN in\na supervised manner. DAC provides a novel perspective for deep\nclustering, but it only focuses on relationships between pairwise\npatterns. DDC (deep discriminative clustering analysis [47]) is\na more robust and generalized version of DAC by introducing\nglobal and local constraints of relationships. ST-DAC ( spatial\ntransformer - deep adaptive clustering [62]) applies a visual atten-\ntion mechanism [101] to modify the structure of DAC. Haeusser\net al. [61] propose associative deep clustering (ADC), which\ncontains a group of centroid variables with the same shape as\nimage embeddings. With the intuition that centroid variables can\ncarry over high-level information about the data structure in the\niteration process, the authors introduce an objective function with\nmultiple loss terms to simultaneously train those centroid variables\nand the DNN’s parameters along with a clustering mapping layer.\nThe above mentioned clustering methods estimate the cluster\nof an instance by passing it through the entire deep network, which\ntends to extract the global features of the instance [102]. Some\nclustering methods use mature classiﬁcation network to initialize\nthe feature extractor. For instance, DeepCluster [59] applies k-\nmeans on the output features of the deep model (like AlexNet\n[103] and VGG-16 [104]) and uses the cluster assignments as\n“pseudo-labels” to optimize the parameters of the convolutional\nneural networks. Hsu et al. [60] propose clustering CNN (CCNN)\nwhich integrates mini-batch k-means with the model pretrained\nfrom the ImageNet dataset [105].\nTo improve the robustness of the model, more and more\napproaches make use of data augmentation for deep clustering\n[42], [52], [69]. For example, Huang et al. [69] extend the idea\nof classical maximal margin clustering [106], [107] to establish\na novel deep semantic clustering method (named PartItion Conﬁ-\ndence mAximisation - PICA). In PICA, three operations including\ncolor jitters, random rescale, and horizontal ﬂip are adopted for\ndata augmentation and perturbations.\nMutual information is also taken as a criterion to learn repre-\nsentations [108], [109] and becomes popular in recent clustering\nmethods especially for image data. Various data augmentation\ntechniques have been applied to generate transformed images that\nare used to mine their mutual information. For example, Ji et al.\n[64] propose invariant information clustering (IIC) for semantic\nclustering and image segmentation. In IIC, every image and its\nrandom transformation are treated as a sample pair. By maximiz-\ning mutual information between the clustering assignments of each\npair, the proposed model can ﬁnd semantically meaningful clusters\nand avoid degenerate solutions naturally. Instead of only using\npairwise information, deep comprehensive correlation mining\n(DCCM) [65] is a novel image clustering framework, which uses\npseudo-label loss as supervision information. Besides, the authors\nextend the instance level mutual information and present triplet\nmutual information loss to learn more discriminative features.\nBased on the currently fashionable contrastive learning [110],\nZhong et al. [68] propose deep robust clustering (DRC), where\ntwo contrastive loss terms are introduced to decrease intra-class\nvariance and increase inter-class variance. Mutual information and\ncontrastive learning are related. In DRC, the authors summarize\na framework that can turn maximize mutual information into\nminimizing contrastive loss.\nIn the ﬁeld of image clustering on the semantic level, people\nthink that the prediction of the original image should be consistent\nwith that of the transformed image by data augmentation. So in the\nunsupervised learning context, data augmentation techniques not\nonly are used to expand the training data but also can easily obtain\nsupervised information. This is why data augmentation can be\nwidely applied in many recent proposed image clustering methods.\nFor example, Nina et al. [63] propose a decoder-free approach\nwith data augmentation (called random triplet mining - RTM) for\nclustering and manifold learning. To learn a more robust encoder,\nthe model consists of three encoders with shared weights and is a\ntriplet network architecture conceptually. The ﬁrst and the second\nencoders take similar images generated by data augmentation as\npositive pair, the second and the third encoders take a negative pair\nselected by RTM. Usually, the objective of triplet networks [111]\nis deﬁned to make the features of the positive pair more similar\nand that of the negative pair more dissimilar.\nAlthough many existing deep clustering methods jointly learn\nthe representations and clusters, such as JULE and DAC, there are\nspecially designed representation learning methods [112], [113],\n[114], [115], [116] to learn the visual representations of images\nin a self-supervised manner. Those methods learn semantical\nrepresentations by training deep networks to solve extra tasks.\nSuch tasks can be predicting the patch context [112], inpainting\npatches [113], colorizing images [114], solving jigsaw puzzles\n[115], and predicting rotations [116], etc. Recently, these self-\nsupervised representation learning methods are adopted in image\nclustering. For example, MMDC (multi-modal deep clustering\n[66]) leverages an auxiliary task of predicting rotations to enhance\nclustering performance. SCAN (semantic clustering by adopting\nnearest neighbors [67]) ﬁrst employs a self-supervised represen-\ntation learning method to obtain semantically meaningful and\nhigh-level features. Then, it integrates the semantically meaningful\nnearest neighbors as prior information into a learnable clustering\napproach.\nSince DEC [36] and JULE [56] are proposed to jointly\nlearn feature representations and cluster assignments by deep neu-\nral networks, many DAE-based and DNN-based deep clustering\nmethods have been proposed and have made great progress in\nclustering tasks. However, the feature representations extracted\nin clustering methods are difﬁcult to extend to other tasks, such\nas generating samples. The deep generative models have recently\nattracted a lot of attention because they can use neural networks\nto obtain data distributions so that samples can be generated (VAE\n[117], GAN [118], Pixel-RNN [119], InfoGAN [120] and PPGN\n7\n[121]). Speciﬁcally, GAN and VAE are the two most typical\ndeep generative models. In recent years, researchers have applied\nthem to various tasks, such as semi-supervised classiﬁcation\n[122], [123], [124], [125], clustering [126], and image generation\n[127], [128]. In the next two subsections, we introduce the deep\nclustering algorithms based on the generated models: VAE-based\ndeep clustering and GAN-based deep clustering.\n3.3\nVAE-based\nDeep learning with nonparametric clustering (DNC) [129] is\na pioneer work in applying deep belief network to deep cluster-\ning. But in deep clustering based on the probabilistic graphical\nmodel, more research comes from the application of variational\nautoencoder (VAE), which combines variational inference and\ndeep autoencoder together.\nMost VAE-based deep clustering algorithms aim at solving\nan optimization problem about evidence lower bound (ELBO, see\nthe deduction details in [117], [130]), p is the joint probability dis-\ntribution, q is the approximate probability distribution of p(z|x), x\nis the input data for clustering and z the latent variable generated\ncorresponding to x:\nLELBO = Eq(z|x)\n\u0014\nlog p (x, z)\nq (z|x)\n\u0015\n(6)\nThe difference is that different algorithms have different generative\nmodels of latent variables or different regularizers. We list several\nVAE-based deep clustering methods that have attracted much\nattention in recent years as below. For convenience, we omit the\nparameterized form of the probability distribution.\nTraditional VAE generates a continuous latent vector z, x is\nthe vector of an original data sample. For the clustering task, the\nVAE-based methods generate latent vector (z, y), where z is the\nlatent vector representing the embedding and y is the label. So the\nELBO for optimization becomes:\nLELBO = Eq(z,y|x)\n\u0014\nlog p (x, z, y)\nq (z, y|x)\n\u0015\n(7)\nThe ﬁrst proposed unsupervised deep generative clustering frame-\nwork is VaDE (variational deep embedding [70]). VaDE models\nthe data generative procedure with a GMM (Gaussian mixture\nmodel [131]) combining a VAE. In this method, the cluster\nassignments and the latent variables are jointly considered in a\nGaussian mixture prior rather than a single Gaussian prior.\nSimilar to VaDE, GMVAE (Gaussian mixture variational\nautoencoder [71]) is another deep clustering method that com-\nbines VAE with GMM. Speciﬁcally, GMVAE considers the gen-\nerative model p(x, z, n, c) = p(x|z)p(z|n, c)p(n)p(c), where\nc is uniformly distributed of k categories and n is normally\ndistributed. z is a continuous latent variable, whose distribution\nis a Gaussian mixture with means and variances of c and n. Based\non the mean-ﬁeld theory [132], GMVAE factors q(z, n, c|x) =\nq(z|x)q(n|x)p(c|z, n) as posterior proxy. In the same way, those\nvariational factors are parameterized with neural networks and the\nELBO loss is optimized.\nOn the basis of GMM and VAE, LTVAE (latent tree varia-\ntional autoencoder [73]) applies latent tree model [133] to perform\nrepresentation learning and structure learning for clustering. Dif-\nferently, LTVAE has a variant of VAE with a superstructure of\nlatent variables. The superstructure is a tree structure of discrete\nlatent variables on top of the latent features. The connectivity\nstructure among all variables is deﬁned as a latent structure of\nthe latent tree model that is optimized via message passing [134].\nThe success of some deep generative clustering methods\ndepends on good initial pre-training. For example, in VaDE\n[70], pre-training is needed to initialize cluster centroids. In\nDGG [135], pre-training is needed to initialize the graph em-\nbeddings. Although GMVAE [71] learns the prior and posterior\nparameters jointly, the prior for each class is dependent on a\nrandom variable rather than the class itself, which seems counter-\nintuitive. Based on the ideas of GMVAE and VaDE, to solve\ntheir fallacies, Prasad et al. [75] propose a new model leveraging\nvariational autoencoders for image clustering (VAEIC). Different\nfrom the methods mentioned above, the prior of VAEIC is de-\nterministic, and the prior and posterior parameters are learned\njointly without the need for a pre-training process. Instead of\nperforming Bayesian classiﬁcation as done in GMVAE and VaDE,\nVAEIC adopts more straight-forward inference and more prin-\ncipled latent space priors, leading to a simpler inference model\np(x, z, c) = p(x|z)p(z|c)p(c) and a simpler approximate pos-\nterior q(z, c|x) = q(c|x)q(z|x, c). The cluster assignment is\ndirectly predicted by q(c|z). What is more, the authors adopt data\naugmentation and design an image augmentation loss to make the\nmodel robust.\nIn addition to the VAE-based deep clustering methods men-\ntioned above, Figueroa et al. [72] use the continuous Gumbel-\nSoftmax distribution [136], [137] to approximate the categorical\ndistribution for clustering. Willetts et al. [74] extend variational\nladder autoencoders [138] and propose a disentangled clustering\nalgorithm. Cao et al. [76] propose a simple, scalable, and stable\nvariational deep clustering algorithm, which introduces generic\nimprovements for variational deep clustering.\n3.4\nGAN-based\nIn adversarial learning, standard generative adversarial net-\nworks (GANs) [118] are deﬁned as an adversarial game between\ntwo networks: generator φg and discriminator φd. Speciﬁcally,\nthe generator is optimized to generate fake data that “fools” the\ndiscriminator, and the discriminator is optimized to tell apart real\nfrom fake input data as shown in Fig. 3.\nGAN has already been widely applied in various ﬁelds of\ndeep learning. Many deep clustering methods also adopt the idea\nof adversarial learning due to their strength in learning the latent\ndistribution of data. We summarize the important GAN-based deep\nclustering methods as follows. Probabilistic clustering algorithms\naddress many unlabeled data problems, such as regularized infor-\nmation maximization (RIM) [139], or the related entropy mini-\nmization [140]. The main idea of RIM is to train a discriminative\nclassiﬁer with unlabeled data. Unfortunately, these methods are\nprone to overﬁtting spurious correlations. Springenberg et al. [79]\npropose categorical generative adversarial networks (CatGAN) to\naddress this weakness. To make the model more general, GAN is\nintroduced to enhance the robustness of the classiﬁer. In CatGAN,\nall real samples are assigned to one of the k categories using the\ndiscriminator, while staying uncertain of clustering assignments\nfor samples from the generative model rather than simply judging\nthe false and true samples. In this way, the GAN framework\nis improved so that the discriminator can be used for multi-\nclass classiﬁcation. In particular, CatGAN can be applied to both\nunsupervised and semi-supervised tasks.\nInterpretable representation learning in the latent space has\nbeen investigated in the seminal work of InfoGAN [120]. Al-\n8\n••\n••\n•••••, ••; •••\n•••; •••\n••••, •; •••\n•\nX\n••\n••\nFig. 3: The framework of GAN-based learning. φg is the generator\nand φd is the discriminator, both εn and εc are inputs to the generator,\nεn is the noise and εn is the class information. X is the data for\nclustering, ˆ\nX is the fake data which “fools” the discriminator, the\nfunction f() operates on ˆ\nX to generate ˆεn and ˆεc.\nthough InfoGAN does use discrete latent variables, it is not\nspeciﬁcally designed for clustering. VAE [117] can jointly train\nthe inference network and autoencoder, which enables mapping\nfrom initial sample X to latent space Z that could potentially\npreserve cluster structure. Unfortunately, there is no such inference\nmechanism in GAN. To make use of their advantages, Mukherjee\net al. [83] propose ClusterGAN as a new mechanism for cluster-\ning. ClusterGAN samples latent variables from a mixture of one-\nhot variables and continuous variables and establishes a reverse-\nmapping network to project data into a latent space. It jointly trains\na GAN along with the inverse-mapping network with a clustering-\nspeciﬁc loss to achieve clustering.\nThere is another GAN-based deep clustering method [82] (we\ndenote it as ClusterGAN-SPL) that has a similar network module\nwith ClusterGAN. The main difference is that ClusterGAN-SPL\ndoes not set discrete latent variables but applies self-paced learning\n[95] to improve the robustness of the algorithm.\nIn some GAN-based deep clustering methods (e.g., DAGC\n[80], DASC [81], AGAE [87] and ADEC [84]), generative ad-\nversarial network and deep autoencoder are both applied. For\nexample, inspired by the adversarial autoencoders [126] and GAN\n[118], Harchaoui et al. [80] propose deep adversarial gaussian\nmixture autoencoder for clustering (DAGC). To make the data\nrepresentations easier to cluster than in the initial space, it builds\nan autoencoder [141] consisting of an encoder and a decoder.\nIn addition, an adversarial discriminator is added to continuously\nforce the latent space to follow the Gaussian mixture prior [131].\nThis framework improves the performance of clustering due to the\nintroduction of adversarial learning.\nMost existing subspace clustering approaches ignore the\ninherent errors of clustering and rely on the self-expression of\nhandcrafted representations. Therefore, their performance on real\ndata with complex underlying subspaces is not satisfactory. Zhou\net al. [81] propose deep adversarial subspace clustering (DASC)\nto alleviate this problem and apply adversarial learning into\ndeep subspace clustering. DASC consists of a generator and a\ndiscriminator that learn from each other. The generator outputs\nsubspace clustering results and consists of an autoencoder, a self-\nexpression layer, and a sampling layer. The deep autoencoder and\nself-expression layer are used to convert the original input samples\ninto better representations. In the pipeline, a new “fake” sample is\ngenerated by sampling from the estimated clusters and sent to the\ndiscriminator to evaluate the quality of the subspace cluster.\nMany autoencoder based clustering methods use reconstruc-\ntion for pretraining and let reconstruction loss be a regularizer\nin the clustering phase. Mrabah et al. [84] point out that such\na trade-off between clustering and reconstruction would lead to\nfeature drift phenomena. Hence, the authors adopt adversarial\ntraining to address the problem and propose adversarial deep\nembedded clustering (ADEC). It ﬁrst pretrains the autoencoder,\nwhere reconstruction loss is regularized by an adversarially con-\nstrained interpolation [142]. Then, the cluster loss (similar to DEC\n[36]), reconstruction loss, and adversarial loss are optimized in\nturn. ADEC can be viewed as a combination of deep embedded\nclustering and adversarial learning.\nBesides the above-mentioned methods, there are a small num-\nber of deep clustering methods whose used networks are difﬁcult\nto categorize. For example, IMSAT (information maximizing self-\naugmented training [108]) uses very simple networks to perform\nunsupervised discrete representation learning. SpectralNet [143] is\na deep learning method to approximate spectral clustering, where\nunsupervised siamese networks [144], [145] are used to compute\ndistances. In clustering tasks, it is a common phenomenon to\nadopt the appropriate neural network for different data formats.\nIn this survey, we focus more on deep learning techniques that are\nreﬂected in the used systematic neural network structures.\n3.5\nGNN-based\nGraph neural networks (GNNs) [146], [147] allow end-to-\nend differentiable losses over data with arbitrary graph structure\nand have been applied to a wide range of applications. Many\ntasks in the real world can be described as a graph, such as\nsocial networks, protein structures, trafﬁc networks, etc. With the\nsuggestion of Banach’s ﬁxed point theorem [148], GNN uses the\nfollowing classic iterative scheme to compute the state. F is a\nglobal transition function, the value of H is the ﬁxed point of\nH = F(H, X) and is uniquely deﬁned with the assumption that\nF is a contraction map [149].\nHt+1 = F(Ht, X)\n(8)\nIn the training process of GNN, many methods try to introduce\nattention and gating mechanism into a graph structure. Among\nthese methods, graph convolutional network (GCN) [150] which\nutilizes the convolution for information aggregation has gained\nremarkable achievement. H is the node hidden feature matrix, W\nis the learnable model parameters and C is the feature matrix of a\ngraph, the compact form of GCN is deﬁned as:\nH = eD−1\n2 eQ eD−1\n2 CW\n(9)\nIn the domain of unsupervised learning, there are also a variety of\nmethods trying to use the powerful structure capturing capabilities\nof GNNs to improve the performance of clustering algorithms. We\nsummarize the GNN-based deep clustering methods as follows.\nTian et al. propose DRGC (learning deep representations for\ngraph clustering) [151] to replace traditional spectral clustering\nwith sparse autoencoder and k-means algorithm. In DRGC, sparse\nautoencoder is adopted to learn non-linear graph representations\nthat can approximate the input matrix through reconstruction and\nachieve the desired sparse properties. The last layer of the deep\nmodel outputs a sparse encoding and k-means serves as the ﬁnal\nstep on it to obtain the clustering results. To accelerate graph\nclustering, Shao et al. propose deep linear coding for fast graph\nclustering (DLC) [152]. Unlike DRGC, DLC does not require\neigen-decomposition and greatly saves running time on large-scale\n9\ndatasets, while still maintaining a low-rank approximation of the\nafﬁnity graph.\nThe research on GNNs is closely related to graph embedding\nor network embedding [153], [154], [155], as GNNs can address\nthe network embedding problem through a graph autoencoder\nframework [156]. The purpose of graph embedding [157] is to\nﬁnd low-dimensional features that maintain similarity between\nthe vertex pairs in a sample similarity graph. If two samples\nare connected in the graph, their latent features will be close.\nThus, they should also have similar cluster assignments. Based\non this motivation, Yang et al. [135] propose deep clustering via\na Gaussian mixture variational autoencoder with graph embed-\nding (DGG). Like VaDE [70], the generative model of DGG is\np(x, z, c) = p(x|z)p(z|c)p(c). The prior distributions of z and\nc are set as a Gaussian mixture distribution and a categorical\ndistribution, respectively. The learning problem of GMM-based\nVAE is usually solved by maximizing the evidence lower bound\n(ELBO) of the log-likelihood function with reparameterization\ntrick. To achieve graph embedding, the authors add a graph embed-\nding constraint to the original optimization problem, which exists\nnot only on the features but also on the clustering assignments.\nSpeciﬁcally, the similarity between data points is measured with a\ntrained Siamese network [144].\nAutoencoder also works on graphs as an effective embedding\nmethod. In AGAE (adversarial graph autoEncoders) [87], the\nauthors apply ensemble clustering [16], [158] in the deep graph\nembedding process and develop an adversarial regularizer to\nguide the training of the autoencoder and discriminator. Recent\nstudies have mostly focused on the methods which are two-\nstep approaches. The drawback is that the learned embedding\nmay not be the best ﬁt for the clustering task. To address this,\nWang et al. propose a uniﬁed approach named deep attentional\nembedded graph clustering (DAEGC) [159]. DAEGC develops\na graph attention-based autoencoder to effectively integrate both\nstructure and content information, thereby achieving better cluster-\ning performance. The data stream framework of graph autoencoder\napplicated in clustering in Fig. 4.\nAs one of the most successful feature extractors for deep\nlearning, CNNs are mainly limited by Euclidean data. GCNs have\nproved that graph convolution is effective in deep clustering, e.g.,\nZhang et al. propose an adaptive graph convolution (AGC) [86]\nmethod for attributed graph clustering. AGC exploits high-order\ngraph convolution to capture global cluster structure and adap-\ntively selects the appropriate order for different graphs. Neverthe-\nless, AGC might not determine the appropriate neighborhood that\nreﬂects the relevant information of connected nodes represented\nin graph structures. Based on AGC, Zhu et al. exploit heat kernel\nto enhance the performance of graph convolution and propose\nAGCHK (AGC using heat kernel) [88], which could make the\nlow-pass performance of the graph ﬁlter better.\nIn summary, we can realize the importance of the structure\nof data. Motivated by the great success of GNNs in encoding\nthe graph structure, Bo et al. propose a structural deep clustering\nnetwork (SDCN) [89]. By stacking multiple layers of GNN,\nSDCN is able to capture the high-order structural information.\nAt the same time, beneﬁting from the self-supervision of AE and\nGNN, the multi-layer GNN does not exhibit the so-called over-\nsmooth phenomenon. SDCN is the ﬁrst work to apply structural\ninformation into deep clustering explicitly.\nTABLE 4: Semi-supervised deep clustering methods.\nMethods\nCharacteristics\nSDEC (2019) [160]\nBased on DEC [36].\nSSLDEC (2019) [161]\nBased on DEC [36].\nDECC (2019) [162]\nBased on DEC [36].\nSSCNN (2020) [163]\nCombine k-means loss and pairwise divergence.\n4\nSEMI-SUPERVISED DEEP CLUSTERING\nTraditional semi-supervised learning can be divided into\nthree categories, i.e., semi-supervised classiﬁcation [164], [165],\nsemi-supervised dimension reduction [166], [167], and semi-\nsupervised clustering [13], [168], [169]. Commonly, the constraint\nof unsupervised data is marked as “must-link” and “cannot-\nlink”. Samples with the “must-link” constraint belong to the same\ncluster, while samples with the “cannot-link” constraint belong\nto different clusters. Most semi-supervised clustering objectives\nare the combination of unsupervised clustering loss and constraint\nloss.\nSemi-supervised deep clustering has not been explored well.\nHere we introduce several representative works. These works\nuse different ways to combine the relationship constraints and\nthe neural networks to obtain better clustering performance. We\nsummarize these methods in Table 4.\nSemi-supervised deep embedded clustering (SDEC) [160] is\nbased on DEC [36] and incorporates pairwise constraints in the\nfeature learning process. Its loss function is deﬁned as:\nLoss = KL(S ∥R) + λ 1\nn\nn\nX\ni=1\nn\nX\nk=1\naij ∥zi −zj∥2,\n(10)\nwhere λ is a trade-off parameter. aij = 1 if xi and xj are assigned\nto the same cluster, aij = -1 if xi and xj satisfy cannot-link\nconstraints, aij = 0 otherwise. As the loss function shows, it is\nformed by two parts. The ﬁrst part is KL divergence loss which\nhas been explained in Section 3.1. The second part is semi-\nsupervised loss denotes the consistency between the embedded\nfeature {zi}n\ni=1 and parameter aij. Intuitively, if aij = 1, to\nminimize the loss function, ∥zi−zj∥2 should be small. In contrast,\nif aij = −1, to minimize the loss, ∥zi −zj∥2 should be large,\nwhich means zi is apart from zj in the latent space Z.\nLike SDEC, most semi-supervised deep clustering (DC)\nmethods are based on unsupervised DC methods. It is straightfor-\nward to expand an unsupervised DC method to a semi-supervised\nDC one through adding the semi-supervised loss. Compared with\nunsupervised deep clustering methods, the extra semi-supervised\ninformation of data can help the neural network to extract features\nmore suitable for clustering. There are also some works focusing\non extending the existing semi-supervised clustering method to a\ndeep learning version. For example, the feature extraction process\nof both SSLDEC (semi-supervised learning with deep embedded\nclustering for image classiﬁcation and segmentation) [161] and\nDECC (deep constrained clustering) [162] are based on DEC.\nTheir training process is similar to semi-supervised k-means [168]\nwhich learns feature representations by alternatively using labeled\nand unlabeled data samples. During the training process, the\nalgorithms use labeled samples to keep the model consistent and\nchoose a high degree of conﬁdence unlabeled samples as newly\nlabeled samples to tune the network. Semi-supervised clustering\nwith neural networks [163] combines a k-means loss and pairwise\ndivergence to simultaneously learn the cluster centers as well as\nsemantically meaningful feature representations.\n10\n1\nĮ\n•\n••\n•\n•\n0\n•••••, ••\nΎ\nFig. 4: The data stream framework of graph autoencoder applicated in clustering. GCN(N, M) is a graph autoencoder, GCN() is used to\nrepresent a graph convolutional neural network, graph autoencoder consists of two layers of graph convolutional neural networks. Both node\nattributes N and graph structure M are utilized as inputs to this encoder. Z is a matrix of node embedding vectors. α is an activation function,\nf\nM is the prediction of graph adjacency matrix M.\n5\nDEEP MULTI-VIEW CLUSTERING\nThe above-mentioned deep clustering methods can only deal\nwith single-view data. In practical clustering tasks, the input data\nusually have multiple views. For example, the report of the same\ntopic can be expressed with different languages; the same dog can\nbe captured from different angles by the cameras; the same word\ncan be written by people with different writing styles. Multi-view\nclustering (MVC) methods [18], [170], [171], [172], [173], [174],\n[175], [176], [177], [178], [179] are proposed to make use of\nthe complementary information among multiple views to improve\nclustering performance.\nIn recent years, the application of deep learning in multi-view\nclustering is a hot topic [180], [181], [182], [183], [184]. Those\ndeep multi-view clustering algorithms focus on solving the cluster-\ning problems with different forms of input data. Since the network\nstructures used in most of these methods are autoencoders, we\ndivided them into three categories based on the adopted clustering\ntheoretical basis:\nDEC-based, subspace clustering-based, and\nGNN-based. They are summarized in Table 5.\n5.1\nDEC-based\nAs mentioned previously, DEC (deep embedded clustering)\n[36] uses autoencoder to learn the low-dimensional embedded\nfeature representation and then minimizes the KL divergence of\nstudent’s t-distribution and auxiliary target distribution of feature\nrepresentations to achieve clustering. Improved DEC (IDEC) [37]\nemphasizes data structure preservation and adds the term of the\nreconstruction loss for the lower-dimensional feature represen-\ntation when processing ﬁne-tuning tasks. Some deep multi-view\nclustering methods also adopt this deep learning pipeline.\nTraditional MVC methods mostly use linear and shallow\nembedding to learn the latent structure of multi-view data. These\nmethods cannot fully utilize the non-linear property of data,\nwhich is vital to reveal a complex clustering structure. Based\non adversarial learning and deep autoencoder, Li et al. [185]\npropose deep adversarial multi-view clustering (DAMC) to learn\nthe intrinsic structure embedded in multi-view data. Speciﬁcally,\nDAMC consists of a multi-view encoder E, a multi-view generator\n(decoder) φg, V discriminators D1, ..., DV (V denotes the num-\nber of views), and a deep embedded clustering layer. The multi-\nview encoder outputs low-dimensional embedded features for each\nview. For each embedded feature, the multi-view generator gener-\nates a corresponding reconstruction sample. The discriminator is\nused to identify the generated sample from the real sample and\noutput feedback. The total loss function of DAMC is deﬁned as:\nLoss = min\nE,G\nmax\nD1,...,DV Lr + αLc + βLGAN,\n(11)\nwhere Lc comes from DEC [36] and represents the clustering loss,\nLr and LGAN represent the reconstruction loss and GAN loss\nrespectively, α and β are hyperparameters. Compared with tradi-\ntional MVC algorithms, DAMC can reveal the non-linear property\nof multi-view data and achieve better clustering performance.\nXu et al. [180] propose a novel collaborative training\nframework for deep embedded multi-view clustering (DEMVC).\nSpeciﬁcally, DEMVC deﬁnes a switched shared auxiliary target\ndistribution and fuses it into the overall clustering loss. Its main\nidea is that by sharing optimization objectives, each view, in turn,\nguides all views to learn the low-dimensional embedded features\nthat are conducive to clustering. At the same time, optimizing\nreconstruction loss makes the model retain discrepancies among\nmultiple views. Experiments show that DEMVC can mine the\ncorrect information contained in multiple views to correct other\nviews, which is helpful to improve the clustering accuracy. Exist-\ning methods tend to fuse multiple views’ representations, Xu et al.\n[187] present a novel VAE-based multi-view clustering framework\n(Multi-VAE) by learning disentangled visual representations.\nLin et al. [188] propose a contrastive multi-view hyperbolic\nhierarchical clustering (CMHHC). It consists of three components,\nmulti-view alignment learning, aligned feature similarity learn-\ning, and continuous hyperbolic hierarchical clustering. Through\ncapturing the invariance information across views and learn the\nmeaningful metric property for similarity-based continuous hier-\narchical clustering. CMHHC is capable of clustering multiview\ndata at diverse levels of granularity.\nXu et al. [189] propose a framework of multi-level feature\nlearning for contrastive multi-view clustering (MFLVC), which\ncombines multi-view clustering with contrastive learning to im-\nprove clustering effectiveness. MFLVC can learn different levels\nof features and reduce the adverse inﬂuence of view-private\ninformation. Xu et al. [190] also explore incomplete multi-view\nclustering, through mining the complementary information in\nthe high-dimensional feature space via a nonlinear mapping of\nmultiple views, the proposed method DIMVC can handle the\nincomplete data primely.\n5.2\nSubspace clustering-based\nSubspace clustering [195] is another popular clustering\nmethod, which holds the assumption that data points of different\n11\nTABLE 5: The summaries of deep multi-view clustering methods.\nNetworks\nMethods\nCharacteristics\nDAE + GAN\nDAMC (2019) [185]\nCapture the data distribution ulteriorly by adversarial training.\nVAE\nDMVCVAE (2020) [186]\nLearn a shared latent representation under the VAE framework.\nDAE\nDEMVC (2021) [180]\nThrough collaborative training, each view can guide all views.\nDAE\nDMVSSC (2018) [182]\nExtract multi-view deep features by CCA-guided convolutional auto-encoders.\nDAE\nRMSL (2019) [183]\nRecover the underlying low-dimensional subspaces in which the high dimensional data lie.\nDAE\nMVDSCN (2019) [184]\nCombine convolutional auto-encoder and self-representation together.\nVAE\nMulti-VAE (2021) [187]\nLearn disentangle and explainable representations.\nDAE\nCMHHC (2022) [188]\nEmploy multiple autoencoders and hyperbolic hierarchical clustering.\nDAE\nMFLVC (2022) [189]\nUtilize contrastive clustering to learn the common semantics across all views.\nDAE\nDIMVC (2022) [190]\nImputation-free and fusion-free incomplete multi-view clustering.\nGCN\nMulti-GCN (2019) [191]\nIncorporates nonredundant information from multiple views.\nGCN\nMAGCN (2020) [192]\nDual encoders for reconstructing and integrating.\nGAE\nO2MAC (2020) [181]\nPartition the graph into several nonoverlapping clusters.\nGAE\nCMGEC (2021) [193]\nMultiple graph autoencoder.\nGAE\nDMVCJ (2022) [194]\nWeighting strategy to alleviate the noisy issue.\nclusters are drawn from multiple subspaces. Subspace clustering\ntypically ﬁrstly estimates the afﬁnity of each pair of data points\nto form an afﬁnity matrix, and then applies spectral clustering\n[196] or a normalized cut [197] on the afﬁnity matrix to obtain\nclustering results. Some subspace clustering methods based on\nself-expression [198] have been proposed. The main idea of self-\nexpression is that each point can be expressed with a linear\ncombination C of the data points X themselves. The general\nobjective is:\nLoss = Lr + αR(C) = ∥X −XC∥+ αR(C),\n(12)\nwhere ∥X −XC∥is the reconstruction loss and R(C) is the\nregularization term for subspace representation C. In recent years,\na lot of works [199], [200], [201], [202], [203], [204], [205]\ngenerate a good afﬁnity matrix and achieve better results by using\nthe self-expression methodology.\nThere are also multi-view clustering methods [172], [174],\n[177] which are based on subspace learning. They construct the\nafﬁnity matrix with shallow features and lack of interaction across\ndifferent views, thus resulting in insufﬁcient use of complementary\ninformation included in multi-view datasets. To address this,\nresearchers focus more on multi-view subspace clustering methods\nbased on deep learning recently.\nExploring the consistency and complementarity of multiple\nviews is a long-standing important research topic of multi-view\nclustering [206]. Tang et al. [182] propose the deep multi-view\nsparse subspace clustering (DMVSSC), which consists of a canon-\nical correlation analysis (CCA) [207], [208], [209] based self-\nexpressive module and convolutional autoencoders (CAEs). The\nCCA-based self-expressive module is designed to extract and in-\ntegrate deep common latent features to explore the complementary\ninformation of multi-view data. A two-stage optimization strategy\nis used in DMVSSC. Firstly, it only trains CAEs of each view to\nobtain suitable initial values for parameters. Secondly, it ﬁne-tunes\nall the CAEs and CCA-based self-expressive modules to perform\nmulti-view clustering.\nUnlike CCA-based deep MVC methods (e.g., DMVSSC\n[182]) which project multiple views into a common low-\ndimensional space, Li et al. [183] present a novel algorithm named\nreciprocal multi-layer subspace learning (RMSL). RMSL contains\ntwo main parts: HSRL (hierarchical self-representative layers)\nand BEN (backward encoding networks). The self-representative\nlayers (SRL) contains the view-speciﬁc SRL which maps view-\nspeciﬁc features into view-speciﬁc subspace representations, and\nthe common SRL which further reveals the subspace structure\nbetween the common latent representation and view-speciﬁc rep-\nresentations. BEN implicitly optimizes the subspaces of all views\nto explore consistent and complementary structural information to\nget a common latent representation.\nMany multi-view subspace clustering methods ﬁrst extract\nhand-crafted features from multiple views and then learn the\nafﬁnity matrix jointly for clustering. This independent feature\nextraction stage may lead to the multi-view relations in data being\nignored. To alleviate this problem, Zhu et al. [184] propose a novel\nmulti-view deep subspace clustering network (MVDSCN) which\nconsists of diversity net (Dnet) and universality net (Unet). Dnet is\nused to learn view-speciﬁc self-representation matrices and Unet\nis used to learn a common self-representation matrix for multiple\nviews. The loss function is made up of the reconstruction loss of\nautoencoders, the self-representation loss of subspace clustering,\nand multiple well-designed regularization items.\n5.3\nGNN-based\nIn the real world, graph data are far more complex. For ex-\nample, we can use text, images and links to describe the same web\npage, or we can ask people with different styles to write the same\nnumber. Obviously, traditional single-view clustering methods are\nunable to meet the needs of such application scenarios. That is,\none usually needs to employ a multi-view graph [210], rather\nthan a single-view graph, to better represent the real graph data.\nSince GCN has made considerable achievements in processing\ngraph-structured data, Muhammad et al. develop a graph-based\nconvolutional network (Multi-GCN) [191] for multi-view data.\nMulti-GCN focuses attention on integrating subspace learning ap-\nproaches with recent innovations in graph convolutional networks,\nand proposes an efﬁcient method for adapting graph-based semi-\nsupervised learning (GSSL) to multiview contexts.\nMost GNNs can effectively process single-view graph data,\nbut they can not be directly applied to multi-view graph data.\nCheng et al. propose multi-view attribute graph convolution net-\nworks for clustering (MAGCN) [192] to handle graph-structured\ndata with multi-view attributes. The main innovative method\nof MAGCN is designed with two-pathway encoders. The ﬁrst\npathway develops multiview attribute graph attention networks to\ncapture the graph embedding features of multi-view graph data.\nAnother pathway develops consistent embedding encoders to cap-\nture the geometric relationship and the consistency of probability\ndistribution among different views.\nFan et al. [181] attempt to employ deep embedded learning\nfor multi-view graph clustering. The proposed model is named\n12\nOne2Multi graph autoencoder for multi-view graph clustering\n(O2MAC), which utilizes graph convolutional encoder of one\nview and decoders of multiple views to encode the multi-view\nattributed graphs to a low-dimensional feature space. Both the\nclustering loss and reconstruction loss of O2MAC are similar to\nother deep embedded clustering methods in form. What’s special\nis that graph convolutional network [150] is designed to deal with\ngraph clustering tasks [211]. Huang et al. [194] propose DMVCJ\n(deep embedded multi-view clustering via jointly learning latent\nrepresentations and graphs). By introducing a self-supervised\nGCN module, DMVCJ jointly learns both latent graph structures\nand feature representations.\nThe graph in most existing GCN-based multi-view clustering\nmethods is ﬁxed, which makes the clustering performance heavily\ndependent on the predeﬁned graph. A noisy graph with unreliable\nconnections can result in ineffective convolution with wrong\nneighbors on the graph [212], which may worsen the performance.\nTo alleviate this issue, Wang et al. propose a consistent multiple\ngraph embedding clustering framework (CMGEC) [193], which\nis mainly composed of multiple graph autoencoder (M-GAE),\nmulti-view mutual information maximization module (MMIM),\nand graph fusion network (GFN). CMGEC develops a multigraph\nattention fusion encoder to adaptively learn a common represen-\ntation from multiple views, and thereby CMGEC can deal with\nthree types of multi-view data, including multi-view data without\na graph, multi-view data with a common graph, and single-view\ndata with multiple graphs.\nAccording to our research, deep multi-view clustering al-\ngorithms have not been explored well. Other than the above-\nmentioned three categories, Yin et al. [186] propose a VAE-based\ndeep MVC method (deep multi-view clustering via variational au-\ntoencoders, DMVCVAE). DMVCVAE learns a shared generative\nlatent representation that obeys a mixture of Gaussian distributions\nand thus can be regarded as the promotion of VaDE [70] in multi-\nview clustering. There are also some application researches based\non deep multi-view clustering. For example, Perkins et al. [213]\nintroduce the dialog intent induction task and present a novel deep\nmulti-view clustering approach to tackle the problem. Abavisani\net al. [214] and Hu et al. [215] study multi-modal clustering,\nwhich is also related to multi-view clustering. Taking advantage of\nboth deep clustering and multi-view learning will be an interesting\nfuture research direction of deep multi-view clustering.\n6\nDEEP CLUSTERING WITH TRANSFER LEARNING\nTransfer learning has emerged as a new learning framework\nto address the problem that the training and testing data are\ndrawn from different feature spaces or distributions [216]. For\ncomplex data such as high-resolution real pictures of noisy videos,\ntraditional clustering methods even deep clustering methods can\nnot work very well because of the high dimensionality of the\nfeature space and no uniform criterion to guarantee the clustering\nprocess. Transfer learning provides new solutions to these prob-\nlems through transferring the information from source domain\nthat has additional information to guide the clustering process\nof the target domain. In the early phase, the ideas of deep\ndomain adaption are simple and clear, such as DRCN (deep\nreconstruction-classiﬁcation networks) [217] uses classiﬁcation\nloss for the source domain and reconstruction loss for target\ndomain. The two domains share the same feature extractor. With\nࣞ௦\nࣞ௧\nClassification Loss\nܻ\nܵ௧\nܵୱ\nMMD Loss\nFrozen\nFinetune\n݂\nFig. 5: The data stream framework of deep adaption network (DAN).\nDs is the source domain. Dt is the target domain. f is the shared\nencoder of both domains, which can be initialized with existing\nnetwork. The ﬁrst layers of f are frozen, the last layers of f can\nbe ﬁnetuned in the training process. fs is the encoder of Ds. ft is the\nencoder of Dt. Ss are the predicted label vector of Ds. Y are the real\nlabels of Ds. St are the predicted results of Dt.\nthe development of DNN, we now have more advanced ways to\ntransfer the knowledge.\nIn this section, we introduce some transfer learning work\nabout clustering which are separated into two parts. The ﬁrst part\nis “DNN-based”, and the second part is “GAN-based”.\n6.1\nDNN-based\nDNN-based UDA methods generally aim at projecting the\nsource and target domains into the same feature space, in which\nthe classiﬁer trained with source embedding and labels can be\napplied to the target domain.\nIn 2014, through a summary of the network training pro-\ncesses, Yosinski et al. [218] ﬁnd that many deep neural networks\ntrained on natural images exhibit a phenomenon in common:\nthe features learned in the ﬁrst several layers appear not to be\nspeciﬁc to a particular dataset or task and applicable to many other\ndatasets or tasks. Features must eventually transition from general\nto speciﬁc by the last layers of the network. Thus, we can use a\nmature network (e.g., AlexNet [103], GoogleNet [219]) which can\nprovide credible parameters as the initialization for a speciﬁc task.\nThis trick has been frequently used in feature extracted networks.\nDomain adaptive neural network (DaNN) [220] ﬁrst used\nmaximum mean discrepancy (MMD) [221] with DNN.\nMany domain-discrepancy-based methods adopt similar tech-\nniques with DaNN. Deep adaption networks (DAN) [222] use\nmultiple kernel variants of MMD (MK-MMD) as its domain\nadaption function. As shown in Fig. 5, the net of DAN minimizes\nthe distance at the last feature-speciﬁc layers and then the features\nfrom source-net and target-net would be projected into the same\nspace. After DAN, more and more methods based on MMD\nare proposed. The main optimization way is to choose different\nversions of MMD, such as joint adaption network (JAN) [223]\nand weighted deep adaption network (WDAN) [224]. JAN max-\nimizes joint MMD to make the distributions of both source and\ntarget domains more distinguishable. WDAN is proposed to solve\nthe question about imbalanced data distribution by introducing\nan auxiliary weight for each class in the source domain. RTN\n13\n(unsupervised domain adaptation with residual transfer networks)\n[225] uses residual networks and MMD for UDA task.\nSome discrepancy-based methods do not use MMD. Domain\nadaptive hash (DAH) [226] uses supervised hash loss and un-\nsupervised entropy loss to align the target hash values to their\ncorresponding source categories. Sliced wasserstein discrepancy\n(SWD) [227] adopts the novel SWD to capture the dissimilarity\nof probability. Correlation alignment (CORAL) [228] minimizes\ndomain shift by aligning the second-order statistics of source\nand target distributions. Higher-order moment matching (HoMM)\n[229] shows that the ﬁrst-order HoMM is equivalent to MMD and\nthe second-order HoMM is equivalent to CORAL. Contrastive\nadaptation network (CAN) [230] proposes contrastive domain\ndiscrepancy (CDD) to minimize the intra-class discrepancy and\nmaximize the inter-class margin. Besides, several new measure-\nments are proposed for the source and target domain [231], [232],\n[233]. Analysis of representations for domain adaptation [234]\ncontributes a lot in the domain adaption distance ﬁeld. Some\nworks try to improve the performance of UDA in other directions,\nsuch as unsupervised domain adaptation via structured prediction\nbased selective pseudo-labeling tries to learn a domain invariant\nsubspace by supervised locality preserving projection (SLPP)\nusing both labeled source data and pseudo-labeled target data.\nThe tricks used in deep clustering have also been used in\nUDA methods. For example, structurally regularized deep clus-\ntering (SRDC) [235] implements the structural source regular-\nization via a simple strategy of joint network training. It ﬁrst\nminimizes the KL divergence between the auxiliary distribution\n(that is the same with the auxiliary distribution of DEC [36]) and\nthe predictive label distribution. Then, it replaces the auxiliary\ndistribution with that of ground-truth labels of source data. Wang\net al. [236] propose a UDA method that uses novel selective\npseudo-labeling strategy and learns domain invariant subspace by\nsupervised locality preserving projection (SLPP) [237] using both\nlabeled source data and pseudo-labeled target data. Zhou et al.\n[238] apply ensemble learning in the training process. Prabhu et\nal. [239] apply entropy optimization in target domain.\n6.2\nGAN-based\nDNN-based UDA methods mainly focus on an appropriate\nmeasurement for the source and target domains. By contrast, GAN-\nbased UDA methods use the discriminator to ﬁt this measurement\nfunction. Usually, in GAN-based UDA methods, the generator\nφg is used to produce data followed by one distribution from\nanother distribution, and the discriminator φd is used to judge\nwhether the data generated follow the distribution of the target\ndomain. Traditional GAN can not satisfy the demands to project\ntwo domains into the same space, so different frameworks based\non GAN are proposed to cope with this challenge.\nIn 2016, domain-adversarial neural network (DANN) [246]\nand coupled generative adversarial networks (Co-GAN) [245] are\nproposed to introduce adversarial learning into transfer learning.\nDANN uses a discriminator to ensure the feature distributions over\nthe two domains are made similar. CO-GAN applies generator and\ndiscriminator all in UDA methods. It consists of a group of GANs,\neach corresponding to a domain. In UDA, there are two domains.\nThe framework of CO-GAN is shown in Fig. 6.\nIn deep transfer learning, we need to ﬁnd the proper layers\nfor MMD or weight sharing. In general, we could see that the\nnetworks which want to transfer the knowledge through domain\nࣞ௦\nࣞ௧\n(1,0)\n෢\nࣞ௦\n෢\nࣞ௧\n(1,0)\nShared weights\nShared weights\n߶௚ଵ\n߶௚ଶ\n߶ௗଵ\n߶ௗଶ\nGAN1\nGAN2\nߝଵ\nߝଶ\n݂௦\nClassification Loss\nܻ\nܵୱ\nFig. 6: The data stream framework of Co-GAN applicated in UDA.\nIt consists of a pair of GANs: GAN1 and GAN2. GAN1 and GAN2\nshare the weight in the ﬁrst layers of φg and last layers of φd. Ds\nis the source domain. Dt is the target domain. φd. c\nDs and φd. c\nDt\nare generated by the noise. The ﬁrst layers of φg is responsible for\ndecoding high-level semantics and the last layers of φd is responsible\nfor encoding high-level semantics. Add weight sharing constraint in\nthese layers can guarantee similar high-level semantic representations\nof both domains with different low-level feature representations.\nadaption must pay more attention to the layers which are respon-\nsible for high-level semantic layers. In DAN, the ﬁrst layers are\nfor basic features and the high layers for semantic information\nare zoomed in where the last layers are chosen to be projected\nwith MMD. In Co-GAN, also the semantic layers are chosen as\nthe transferring layers (take notice, the ﬁrst layers of DAN are\nnot transferring layers between two domains, as it is transferring\nthe feature extracting power of a mutual network to our domains’\nfeature extracting part). The weight-sharing constraint in the ﬁrst\nlayers of the generator urges two instances from a different domain\nto extract the same semantics and are destructed into different low-\nlevel details in the last layers of φg. In opposite, the discriminator\nlearns the features from low-level to high-level, so if we add\nweight-sharing constraint in the last layers, this can stimulate it\nto learn a joint distribution of multi-domain images from different\nlow-level representations.\nCo-GAN contributed signiﬁcant thought to UDA. Adversarial\nmethods in domain adaptation have sprung up. For the job that\nrelies on the synthesized instances to assist the domain adaptation\nprocess, they always perform not very well on real images such as\nthe OFFICE dataset. GenToAdapt-GAN [250] is proposed in\ncases where data generation is hard, even though the generator\nnetwork they use performs a mere style transfer, yet this is\nsufﬁcient for providing good gradient information for successfully\naligning the domains. Unlike Co-GAN, there is just one generator\nand one discriminator. Additionally, there are two classiﬁers and\none encoder to embed the instances into vectors.\nCo-GAN and GenToAdapt adopt different strategies to train a\nclassiﬁer for an unlabeled domain. The biggest difference between\n14\nTABLE 6: The summaries of DNN- and GAN-based methods in deep clustering with transfer learning.\nNet\nMethods\nCharacteristics\nDNN\nDaNN (2014) [220]\nMMD and the same feature extracter.\nDAN (2015) [222]\nMulti-kernel MMD. Different feature extracters.\nDRCN (2016) [217]\nClassiﬁcation of source and reconstruction of target.\nRTN (2016) [225]\nResidual networks and MMD.\nDAH (2017) [226]\nSupervised hash loss and unsupervised entropy loss.\nWDAN (2017) [224]\nImbalanced data distribution.\nJAN (2017) [223]\nJoint MMD.\nCORAL (2017) [228]\nMinimize domain shift by aligning the second-order statistics of source and target distributions.\nSWD (2019) [227]\nSliced Wasserstein discrepancy.\nCAN (2019) [230]\nContrastive Domain Discrepancy.\nSRDC (2020) [235]\nKL divergence and auxiliary distribution (the same with DEC [36].).\nSPL (2020) [236]\nSupervised locality preserving projection and selective pseudo-labeling strategy\nMDD (2020) [240]\nWithin-domain class imbalance and between-domain class distribution shift.\nHoMM (2020) [229]\nHigher-order moment matching for UDA.\nGSDA (2020) [231]\nModel the relationship among the local distribution pieces and global distribution synchronously.\nETD(2020) [232]\nAttention mecanism for samples similarity andattention scores for the transport distances.\nBAIT (2020) [241]\nSource-free unsupervised domain adaptation.\nDAEL (2021) [238]\nEnsemble Learning.\nSHOT (2021) [242]\nSource-free unsupervised domain adaptation.\nSHOT-plus (2021) [243]\nSource-free unsupervised domain adaptation.\nSENTRY (2021) [239]\nEntropy Optimization.\nRWOT (2021) [233]\nShrinking Subspace Reliability and weighted optimal transport strategy.\nN2DC-EX (2021) [244]\nSource-free unsupervised domain adaptation.\nGAN\nCo-GAN (2016) [245]\nA group of GANs with partly weight sharing, discriminator and label predictor are uniﬁed.\nDANN (2016) [246]\nDomain classiﬁer and label predictor.\nUNIT (2017) [247]\nUse variational autoencoder as feature extractor\nADDA(2017) [248]\nGeneralization of Co-GAN [245].\nPixelDA (2017) [249]\nGenerate instances follow target distribution with source samples.\nGenToAdapt (2018) [250]\nTwo classiﬁers and one encoder to embed the instances into vectors.\nSimNet (2018) [251]\nSimilarity-based classiﬁer .\nMADA (2018) [252]\nMulti-domains.\nDIFA (2018) [253]\nExtended ADDA [248] uses a pair of feature extractors.\nCyCADA (2018) [254]\nSemantic consistency at both the pixel-level and feature-level.\nSymNet (2019) [255]\nCategory-level and domain-level confusion losses.\nM-ADDA (2020) [256]\nTriplet loss function and ADDA [248].\nIIMT (2020) [257]\nMixup formulation and a feature-level consistency regularizer.\nMA-UDASD (2020) [258]\nSource-free unsupervised domain adaptation.\nDM-ADA (2020) [259]\nDomain mixup is jointly conducted on pixel and feature level.\nCo-GAN and GenToAdapt-GAN is whether the feature extractor\nis the same. The feature extractor of Co-GAN is the GAN itself,\nbut the feature extractor of GenToAdapt-GAN is a specialized\nencoder. In Co-GAN, GAN must do the jobs of adversarial process\nand encoding at the same time, but in GenToAdapt-GAN, these\ntwo jobs are separated which means GenToAdapt-GAN will be\nstabler and perform better when the data is complex. Most of\nthe methods proposed in recent years are based on these two\nways. [247] adopted different GAN for different domains and\nweight-sharing. The main change is that the generator is replaced\nby VAE. ADDA (adversarial discriminative domain adaptation)\n[248] adopted the discriminative model as the feature extractor\nis based on Co-GAN. ADDA can be viewed as generalization\nof CO-GAN framework. [253] extended ADDA using a pair of\nfeature extractors. [256] uses a metric learning approach to train\nthe source model on the source dataset by optimizing the triplet\nloss function as an optimized method and then using ADDA to\ncomplete its transferring process. SymNet [255] proposed a two-\nlevel domain confusion scheme that includes category-level and\ndomain-level confusion losses. With the same feature extractor\nof the source and target domain, MADA (multi-adversarial do-\nmain adaptation) [252] sets the generator as its feature extractor\nexpanding the UDA problem to multi-domains. Similarity-based\ndomain adaption network (SimNet) [251] uses discriminator as a\nfeature extractor and a similarity-based classiﬁer which compares\nthe embedding of an unlabeled image with a set of labeled proto-\ntypes to classify an image. [257] using mixup formulation and a\nfeature-level consistency regularizer to address the generalization\nperformance for target data. [259] uses domain mixup on both\npixel and feature level to improve the robustness of models.\nThere is also a very straightforward way to transfer the\nknowledge between domains: Generate new instances for the\ntarget domain. If we transfer the instance from the source domain\ninto a new instance that followed a joint distribution of both\ndomain and are labeled the same as its mother source instance,\nthen we get a batch of “labeled fake instances in target domain”.\nTraining a classiﬁer with these fake instances should be applicative\nto the real target data. In this way, we can easily use all the\nunsupervised adversarial domain adaptation methods in UDA as\nan effective data augmentation method. This accessible method\nalso performs well in the deep clustering problem and is called\npixel-level transfer learning.\nUnsupervised pixel–level domain adaptation with generative\nadversarial networks (Pixel-GAN) [249] aims at changing the\nimages from the source domain to appear as if they were sampled\nfrom the target domain while maintaining their original content\n(label). The authors proposed a novel GAN-based architecture\nthat can learn such a transformation in an unsupervised manner.\nThe training process of Pixel-GAN is shown in Fig. 7. It uses a\ngenerator φg to propose a fake image with the input composed of\na labeled source image and a noise vector. The fake images will\nbe discriminated against with target data by a discriminator φd. At\nthe same time, fake images c\nDs and source images are put into a\nclassiﬁer fs, when the model is convergent, the classiﬁer can be\n15\nࣞ௦\nࣞ௧\n෢\nࣞ௦\n(1,0)\n߶௚\n߶ௗ\n݂௦\nߝ\nClassification Loss\nܻ\nܵୱ\nFig. 7: An overview of the model architecture. The generator φg\ngenerates an image conditioned on a synthetic image which fed into\ndiscriminator as fake data and a noise vector ε. The discriminator\nφd discriminates between real and fake images. Ds is the source\ndomain. Dt is the target domain. c\nDs is the fake image, fs is trained\nwith generated data and source data. Y means the real labels and Ss\ndenotes the predicted results.\nused on the target domain.\nOn the whole, Pixel-GAN is a very explicit model, but this net\nrelies on the quality of the generated images too much. Although\nthe classiﬁer can guarantee the invariant information of classes,\nit is also hard to perform on complex images. Pixel-level trans-\nferring and feature-level transferring are not going against each\nother, as pixel-level can transfer visual features and feature-level\ntransferring can transfer the nature information of the instances.\nCycle-Consistent adversarial domain adaptation (CyCADA) [254]\nadapts representations at both the pixel-level and feature-level\nwhile enforcing semantic consistency. The authors enforce both\nstructural and semantic consistency during adaptation using a\ncycle-consistency loss and semantics losses based on a particular\nvisual recognition task. The semantics losses both guide the\noverall representation to be discriminative and enforce semantic\nconsistency before and after mapping between domains. Except\nfor GAN, adopting data augmentation to transfer learning can also\nuse traditional ways. [260] provides the efﬁciency to make data\naugmentation in the target domain even it is unlabeled. It adds self-\nsupervised tasks to target data and shows good performance. More\nimportant is that this skill can be combined with other domain\nadaptation methods such as CyCADA and DAN.\n7\nFUTURE DIRECTIONS OF DEEP CLUSTERING\nBased on the aforementioned literature review and analysis,\ndeep clustering has been applied to several domains, and we attach\nimportance to several aspects worth studying further:\n•\nTheoretical exploration\nAlthough\nremarkable\nclustering\nperformance\nhas\nbeen\nachieved by designing even more sophisticated deep clustering\npipelines for speciﬁc problem-solving needs, there is still no\nreliable theoretical analysis on how to qualitatively analyze the in-\nﬂuence of feature extraction and clustering loss on ﬁnal clustering.\nSo, exploring the theoretical basis of deep clustering optimization\nis of great signiﬁcance for guiding further research in this ﬁeld.\n•\nMassive complex data processing\nDue to the complexity brought by massive data, most of\nthe existing deep clustering models are designed for speciﬁc\ndata sets. Complex data from different sources and forms bring\nmore uncertainties and challenges to clustering. At present, deep\nlearning and graph learning are needed to solve complex data\nprocessing problems.\n•\nModel efﬁciency\nDeep clustering algorithm requires a large number of samples\nfor training. Therefore, in small sample data sets, deep clustering is\nprone to overﬁtting, which leads to the decrease of clustering effect\nand the reduction of the generalization performance of the model.\nOn the other hand, the deep clustering algorithm with large-scale\ndata has high computational complexity, so the model structure\noptimization and model compression technology can be adopted\nto reduce the computational load of the model and improve the\nefﬁciency in practical application conditions.\n•\nFusion of multi-view data\nIn practical application scenarios, clustering is often not\njust with a single image information, but also available text and\nvoice information. However, most of the current deep clustering\nalgorithms can only use one kind of information and can not make\ngood use of the existing information. The subsequent research can\nconsider to fully integrate the information of two or more views\nand make full use of the consistency and complementarity of data\nof different views to improve the clustering effect. Furthermore,\nhow to combine features of different views while ﬁltering noise to\nensure better view quality needs to be solved.\n•\nDeep clustering based on graph learning\nIn reality, a large number of data sets are stored in the form\nof graph structures. Graph structure can represent the structural\nassociation information between sample points. How to effectively\nuse the structural information is particularly important to improve\nthe clustering performance. Whether it is a single-view deep\nclustering or a relatively wide application of multi-view deep\nclustering, existing clustering methods based on graph learning\nstill have some problems, such as the graph structure information\nis not fully utilized, the differences and importance of different\nviews are not fully considered. Therefore, the effective analysis of\ncomplex graph structure information, especially the rational use of\ngraph structure information to complete the clustering task, needs\nfurther exploration.\n8\nSUMMARY OF DEEP CLUSTERING METHODS\nIn this paper, we introduce recent advances in the ﬁeld of\ndeep clustering. This is mainly kind of data structures: single-\nview, semi-supervised, multi-view, and transfer learning. Single-\nview methods are the most important part of our survey, which\ninherits the problem settings of traditional clustering methods.\nWe introduce them from the network they are based on. Among\nthese networks, DAE-based methods and DNN-based methods are\nproposed earlier but limited with their poor performance in a\nreal dataset. Compared to DAE-based and CNN-based methods,\nVAE-based and GAN-based methods attract attention in recent\nyears for their strong feature extraction and sample generation\npower. Graph neural network is one of the most popular networks\nrecently, especially in community discovery problems. So we\n16\nalso summarize the GNN-based clustering methods. With the\ndevelopment of the internet, the data for clustering have different\napplication scenarios, so we summarize some clustering methods\nwhich have different problem settings. Semi-supervised clustering\nmethods cluster the data with constraints that can be developed\nfrom single-view clustering methods by adding a constraints loss.\nMulti-view clustering methods use the information of different\nviews as a supplement. It has been used widely in both traditional\nneural networks and graph neural networks. Transfer learning can\ntransfer the knowledge of a labeled domain to an unlabeled do-\nmain. We introduce clustering methods based on transfer learning\nwith two types of networks: DNN and GAN. DNN-based methods\nfocus on the measurement strategy of two domains. GAN-based\nmethods use discriminators to ﬁt the measurement strategy.\nIn general, single-view clustering has a long history and it is\nstill a challenge especially on complex data. But the information\noutside should also be considered in application scenarios. For\ninstance, the news reported by multiple news organizations; sensor\nsignals decompose in the time and frequency domains; a mature\ndog classiﬁcation network is useful to class the cats’ images. Semi-\nsupervised models, multi-view models, and unsupervised domain\nadaptation models consider multi-source information would attract\nmore attention in practical application.\nREFERENCES\n[1]\nZhongyuan Wang, Guangcheng Wang, Baojin Huang, Zhangyang\nXiong, Qi Hong, Hao Wu, Peng Yi, Kui Jiang, Nanxi Wang, Yingjiao\nPei, et al.\nMasked face recognition dataset and application.\narXiv\npreprint arXiv:2003.09093, 2020.\n[2]\nJianzhu Guo, Xiangyu Zhu, Chenxu Zhao, Dong Cao, Zhen Lei, and\nStan Z Li.\nLearning meta face recognition in unseen domains.\nIn\nCVPR, pages 6163–6172, 2020.\n[3]\nAshima Yadav and Dinesh Kumar Vishwakarma. Sentiment analysis\nusing deep learning architectures: a review.\nArtiﬁcial Intelligence\nReview, 53(6):4335–4385, 2020.\n[4]\nGuixian Xu, Yueting Meng, Xiaoyu Qiu, Ziheng Yu, and Xu Wu.\nSentiment analysis of comment texts based on bilstm. IEEE Access,\n7:51522–51532, 2019.\n[5]\nJi Zhou, Peigen Li, Yanhong Zhou, Baicun Wang, Jiyuan Zang, and Liu\nMeng. Toward new-generation intelligent manufacturing. Engineering,\n4(1):11–20, 2018.\n[6]\nJi Zhou, Yanhong Zhou, Baicun Wang, and Jiyuan Zang.\nHuman–\ncyber–physical systems (hcpss) in the context of new-generation intel-\nligent manufacturing. Engineering, 5(4):624–636, 2019.\n[7]\nJ. MacQueen. Some methods for classiﬁcation and analysis of multi-\nvariate observations. In Proceedings of the 5th Berkeley Symposium on\nMathematical Statistics and Probability, pages 281–297, 1967.\n[8]\nYazhou Ren, Uday Kamath, Carlotta Domeniconi, and Zenglin Xu.\nParallel boosted clustering. Neurocomputing, 351:87–100, 2019.\n[9]\nMartin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al.\nA density-based algorithm for discovering clusters in large spatial\ndatabases with noise. In KDD, volume 96, pages 226–231, 1996.\n[10]\nDorin Comaniciu and Peter Meer. Mean shift: A robust approach toward\nfeature space analysis. TPAMI, 24(5):603–619, 2002.\n[11]\nYazhou Ren, Uday Kamath, Carlotta Domeniconi, and Guoji Zhang.\nBoosted mean shift clustering. In ECML-PKDD, pages 646–661, 2014.\n[12]\nYazhou Ren, Carlotta Domeniconi, Guoji Zhang, and Guoxian Yu. A\nweighted adaptive mean shift clustering algorithm. In SDM, pages 794–\n802, 2014.\n[13]\nYazhou Ren, Xiaohui Hu, Ke Shi, Guoxian Yu, Dezhong Yao, and\nZenglin Xu. Semi-supervised denpeak clustering with pairwise con-\nstraints. In PRICAI, pages 837–850, 2018.\n[14]\nChristopher M. Bishop. Pattern Recognition and Machine Learning,\nchapter 9, pages 430–439. Springer, 2006.\n[15]\nA. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: A review.\nACM Computing Surveys, 31(3):264–323, 1999.\n[16]\nAlexander Strehl and Joydeep Ghosh. Cluster ensembles - a knowledge\nreuse framework for combining multiple partitions. JMLR, 3:583–617,\n2002.\n[17]\nYazhou Ren, Carlotta Domeniconi, Guoji Zhang, and Guoxian Yu.\nWeighted-object ensemble clustering: methods and analysis.\nKAIS,\n51(2):661–689, 2017.\n[18]\nAbhishek Kumar and Hal Daumé. A co-training approach for multi-\nview spectral clustering. In ICML, pages 393–400, 2011.\n[19]\nAbhishek Kumar, Piyush Rai, and Hal Daume. Co-regularized multi-\nview spectral clustering. In NeurIPS, pages 1413—-1421, 2011.\n[20]\nXiao Cai, Feiping Nie, and Heng Huang. Multi-view k-means clustering\non big data. In IJCAI, pages 2598–2604, 2013.\n[21]\nZongmo Huang, Yazhou Ren, Xiaorong Pu, and Lifang He. Non-linear\nfusion for self-paced multi-view clustering. In ACM MM, pages 3211–\n3219, 2021.\n[22]\nZongmo Huang, Yazhou Ren, Xiaorong Pu, Lili Pan, Dezhong Yao, and\nGuoxian Yu. Dual self-paced multi-view clustering. Neural Networks,\n140:184–192, 2021.\n[23]\nShudong Huang, Yazhou Ren, and Zenglin Xu. Robust multi-view data\nclustering with multi-view capped-norm k-means.\nNeurocomputing,\n311:197–208, 2018.\n[24]\nSvante Wold, Kim Esbensen, and Paul Geladi. Principal component\nanalysis. Chemometr Intell Lab Syst, 2(1-3):37–52, 1987.\n[25]\nMarti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and\nBernhard Scholkopf. Support vector machines. IEEE Intelligent Systems\nand their applications, 13(4):18–28, 1998.\n[26]\nMD Feit, JA Fleck Jr, and A Steiger.\nSolution of the schrödinger\nequation by a spectral method.\nJournal of Computational Physics,\n47(3):412–433, 1982.\n[27]\nWeibo Liu, Zidong Wang, Xiaohui Liu, Nianyin Zeng, Yurong Liu, and\nFuad E Alsaadi. A survey of deep neural network architectures and\ntheir applications. Neurocomputing, 234:11–26, 2017.\n[28]\nElie Aljalbout, Vladimir Golkov, Yawar Siddiqui, Maximilian Strobel,\nand Daniel Cremers. Clustering with deep learning: Taxonomy and new\nmethods. arXiv preprint arXiv:1801.07648, 2018.\n[29]\nErxue Min, Xifeng Guo, Qiang Liu, Gen Zhang, Jianjing Cui, and Jun\nLong. A survey of clustering with deep learning: From the perspective\nof network architecture. IEEE Access, 6:39501–39514, 2018.\n[30]\nGopi Chand Nutakki, Behnoush Abdollahi, Wenlong Sun, and Olfa\nNasraoui. An introduction to deep clustering. In Clustering Methods\nfor Big Data Analytics, pages 73–89. Springer, 2019.\n[31]\nSheng Zhou, Hongjia Xu, Zhuonan Zheng, Jiawei Chen, Jiajun Bu, Jia\nWu, Xin Wang, Wenwu Zhu, Martin Ester, et al.\nA comprehensive\nsurvey on deep clustering: Taxonomy, challenges, and future directions.\narXiv preprint arXiv:2206.07579, 2022.\n[32]\nBengio Yoshua, Courville Aaron, and Vincent Pascal. Representation\nlearning: a review and new perspectives.\nTPAMI, 35(8):1798–1828,\n2013.\n[33]\nChunfeng Song, Feng Liu, Yongzhen Huang, Liang Wang, and Tieniu\nTan. Auto-encoder based data clustering. In CIARP, pages 117–124,\n2013.\n[34]\nPeihao Huang, Yan Huang, Wei Wang, and Liang Wang.\nDeep\nembedding network for clustering. In CVPR, pages 1532–1537, 2014.\n[35]\nXi Peng, Shijie Xiao, Jiashi Feng, Wei-Yun Yau, and Zhang Yi. Deep\nsubspace clustering with sparsity prior. In IJCAI, pages 1925–1931,\n2016.\n[36]\nJunyuan Xie, Ross Girshick, and Ali Farhadi.\nUnsupervised deep\nembedding for clustering analysis. In ICML, pages 478–487, 2016.\n[37]\nXifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. Improved deep\nembedded clustering with local structure preservation. In IJCAI, pages\n1753–1759, 2017.\n[38]\nPan Ji, Tong Zhang, Hongdong Li, Mathieu Salzmann, and Ian Reid.\nDeep subspace clustering networks. In NeurIPS, pages 24–33, 2017.\n[39]\nKamran Ghasedi Dizaji, Amirhossein Herandi, Cheng Deng, Weidong\nCai, and Heng Huang. Deep clustering via joint convolutional autoen-\ncoder embedding and relative entropy minimization. In ICCV, pages\n5736–5745, 2017.\n[40]\nBo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong.\nTowards k-means-friendly spaces: Simultaneous deep learning and\nclustering. In ICML, pages 3861–3870, 2017.\n[41]\nDongdong Chen, Jiancheng Lv, and Yi Zhang. Unsupervised multi-\nmanifold clustering by learning deep representation. In AAAI, 2017.\n[42]\nXifeng Guo, En Zhu, Xinwang Liu, and Jianping Yin. Aaaiwith data\naugmentation. In ACML, pages 550–565, 2018.\n[43]\nFengfu Li, Hong Qiao, and Bo Zhang. Discriminatively boosted image\nclustering with fully convolutional auto-encoders. Pattern Recognition,\n83:161–173, 2018.\n[44]\nSohil Atul Shah and Vladlen Koltun. Deep continuous clustering. arXiv\npreprint arXiv:1803.01449, 2018.\n17\n[45]\nSohil Atul Shah and Vladlen Koltun.\nRobust continuous clustering.\nPNAS, 114(37):9814–9819, 2017.\n[46]\nElad Tzoreff, Olga Kogan, and Yoni Choukroun. Deep discriminative\nlatent space for clustering. arXiv preprint arXiv:1805.10795, 2018.\n[47]\nJianlong Chang, Yiwen Guo, Lingfeng Wang, Gaofeng Meng, Shiming\nXiang, and Chunhong Pan.\nDeep discriminative clustering analysis.\narXiv preprint arXiv:1905.01681, 2019.\n[48]\nXu Yang, Cheng Deng, Feng Zheng, Junchi Yan, and Wei Liu. Deep\nspectral clustering using dual autoencoder network. In CVPR, pages\n4066–4075, 2019.\n[49]\nTong Zhang, Pan Ji, Mehrtash Harandi, Wenbing Huang, and Hong-\ndong Li.\nNeural collaborative subspace clustering.\narXiv preprint\narXiv:1904.10596, 2019.\n[50]\nYazhou Ren, Ni Wang, Mingxia Li, and Zenglin Xu. Deep density-\nbased image clustering. Knowledge-Based Systems, 197:105841, 2020.\n[51]\nSéverine Affeldt, Lazhar Labiod, and Mohamed Nadif.\nSpectral\nclustering via ensemble deep autoencoder learning (sc-edae). Pattern\nRecognition, 108:107522, 2020.\n[52]\nXifeng Guo, Xinwang Liu, En Zhu, Xinzhong Zhu, Miaomiao Li, Xin\nXu, and Jianping Yin. Adaptive self-paced deep clustering with data\naugmentation. TKDE, 32(9):1680–1693, 2020.\n[53]\nXu Yang, Cheng Deng, Kun Wei, Junchi Yan, and Wei Liu. Adversarial\nlearning for robust deep clustering. In NeurIPS, 2020.\n[54]\nRyan McConville, Raul Santos-Rodriguez, Robert J Piechocki, and\nIan Craddock. N2d:(not too) deep clustering via clustering the local\nmanifold of an autoencoded embedding. In ICPR, pages 5145–5152,\n2021.\n[55]\nJinghua Wang and Jianmin Jiang. Unsupervised deep clustering via\nadaptive gmm modeling and optimization. Neurocomputing, 433:199–\n211, 2021.\n[56]\nJianwei Yang, Devi Parikh, and Dhruv Batra.\nJoint unsupervised\nlearning of deep representations and image clusters. In CVPR, pages\n5147–5156, 2016.\n[57]\nMichael Kampffmeyer, Sigurd Løkse, Filippo M Bianchi, Robert\nJenssen, and Lorenzo Livi. Deep kernelized autoencoders. In SCIA,\npages 419–430, 2017.\n[58]\nJianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and\nChunhong Pan. Deep adaptive image clustering. In ICCV, pages 5879–\n5887, 2017.\n[59]\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze.\nDeep clustering for unsupervised learning of visual features. In ECCV,\npages 132–149, 2018.\n[60]\nChih-Chung Hsu and Chia-Wen Lin. Cnn-based joint clustering and\nrepresentation learning with feature drift compensation for large-scale\nimage data. IEEE Trans Multimedia, 20(2):421–429, 2018.\n[61]\nPhilip Haeusser, Johannes Plapp, Vladimir Golkov, Elie Aljalbout, and\nDaniel Cremers. Associative deep clustering: Training a classiﬁcation\nnetwork with no labels. In GCPR, pages 18–32, 2018.\n[62]\nThiago VM Souza and Cleber Zanchettin. Improving deep image clus-\ntering with spatial transformer layers. arXiv preprint arXiv:1902.05401,\n2019.\n[63]\nOliver Nina, Jamison Moody, and Clarissa Milligan.\nA decoder-\nfree approach for unsupervised clustering and manifold learning with\nrandom triplet mining. In ICCV, pages 0–0, 2019.\n[64]\nXu Ji, João F Henriques, and Andrea Vedaldi. Invariant information\nclustering for unsupervised image classiﬁcation and segmentation. In\nICCV, pages 9865–9874, 2019.\n[65]\nJianlong Wu, Keyu Long, Fei Wang, Chen Qian, Cheng Li, Zhouchen\nLin, and Hongbin Zha.\nDeep comprehensive correlation mining for\nimage clustering. In ICCV, pages 8150–8159, 2019.\n[66]\nGuy Shiran and Daphna Weinshall.\nMulti-modal deep clustering:\nUnsupervised partitioning of images. arXiv preprint arXiv:1912.02678,\n2019.\n[67]\nWouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis,\nMarc Proesmans, and Luc Van Gool.\nSCAN: Learning to classify\nimages without labels. In ECCV, 2020.\n[68]\nHuasong Zhong, Chong Chen, Zhongming Jin, and Xian-Sheng\nHua. Deep robust clustering by contrastive learning. arXiv preprint\narXiv:2008.03030, 2020.\n[69]\nJiabo Huang, Shaogang Gong, and Xiatian Zhu.\nDeep semantic\nclustering by partition conﬁdence maximisation. In CVPR, pages 8849–\n8858, 2020.\n[70]\nZhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning\nZhou. Variational deep embedding: An unsupervised and generative\napproach to clustering. arXiv preprint arXiv:1611.05148, 2016.\n[71]\nNat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH\nLee, Hugh Salimbeni, Kai Arulkumaran, and Murray Shanahan. Deep\nunsupervised clustering with gaussian mixture variational autoencoders.\narXiv preprint arXiv:1611.02648, 2016.\n[72]\nJhosimar Arias Figueroa and Adín Ramírez Rivera. Is simple better?:\nRevisiting simple generative models for unsupervised clustering.\nIn\nNeurIPS, 2017.\n[73]\nXiaopeng Li, Zhourong Chen, Leonard KM Poon, and Nevin L Zhang.\nLearning latent superstructures in variational autoencoders for deep\nmultidimensional clustering. arXiv preprint arXiv:1803.05206, 2018.\n[74]\nMatthew Willetts, Stephen Roberts, and Chris Holmes. Disentangling\nto cluster: Gaussian mixture variational ladder autoencoders.\narXiv\npreprint arXiv:1909.11501, 2019.\n[75]\nVignesh Prasad, Dipanjan Das, and Brojeshwar Bhowmick. Variational\nclustering: Leveraging variational autoencoders for image clustering.\narXiv preprint arXiv:2005.04613, 2020.\n[76]\nLele Cao, Sahar Asadi, Wenfei Zhu, Christian Schmidli, and Michael\nSjöberg. Simple, scalable, and stable variational deep clustering. arXiv\npreprint arXiv:2005.08047, 2020.\n[77]\nLin Yang, Wentao Fan, and Nizar Bouguila.\nDeep ieee t neur net\nlearclustering analysis via dual variational autoencoder with spherical\nlatent embeddings. IEEE T NEUR NET LEAR, pages 1–10, 2021.\n[78]\nHe Ma.\nAchieving deep clustering through the use of variational\nautoencoders and similarity-based loss. Mathematical Biosciences and\nEngineering, 19(10):10344–10360, 2022.\n[79]\nJost Tobias Springenberg.\nUnsupervised and semi-supervised learn-\ning with categorical generative adversarial networks.\narXiv preprint\narXiv:1511.06390, 2015.\n[80]\nWarith Harchaoui, Pierre-Alexandre Mattei, and Charles Bouveyron.\nDeep adversarial gaussian mixture auto-encoder for clustering. ICLR,\n2017.\n[81]\nPan Zhou, Yunqing Hou, and Jiashi Feng. Deep adversarial subspace\nclustering. In CVPR, pages 1596–1604, 2018.\n[82]\nKamran Ghasedi, Xiaoqian Wang, Cheng Deng, and Heng Huang. Bal-\nanced self-paced learning for generative adversarial clustering network.\nIn CVPR, pages 4391–4400, 2019.\n[83]\nSudipto Mukherjee, Himanshu Asnani, Eugene Lin, and Sreeram Kan-\nnan.\nClustergan: Latent space clustering in generative adversarial\nnetworks. In AAAI, volume 33, pages 4610–4617, 2019.\n[84]\nNairouz Mrabah, Mohamed Bouguessa, and Riadh Ksantini. Adver-\nsarial deep embedded clustering: on a better trade-off between feature\nrandomness and feature drift. KDE, 2020.\n[85]\nXiaojiang Yang, Junchi Yan, Yu Cheng, and Yizhe Zhang. Learning\ndeep generative clustering via mutual information maximization. IEEE\nT NEUR NET LEAR, pages 1–13, 2022.\n[86]\nXiaotong Zhang, Han Liu, Qimai Li, and Xiao-Ming Wu. Attributed\ngraph clustering via adaptive graph convolution.\narXiv preprint\narXiv:1906.01210, 2019.\n[87]\nZhiqiang Tao, Hongfu Liu, Jun Li, Zhaowen Wang, and Yun Fu.\nAdversarial graph embedding for ensemble clustering. In IJCAI, pages\n3562–3568, 2019.\n[88]\nDanyang Zhu, Shudong Chen, Xiuhui Ma, and Rong Du.\nAdaptive\ngraph convolution using heat kernel for attributed graph clustering.\nApplied Sciences, 10(4):1473, 2020.\n[89]\nDeyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, and Peng Cui.\nStructural deep clustering network. In WWW, pages 1400–1410, 2020.\n[90]\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation\nlearning: A review and new perspectives. TPAMI, 35(8):1798–1828,\n2013.\n[91]\nGeoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimen-\nsionality of data with neural networks. Science, 313(5786):504–507,\n2006.\n[92]\nJ. Macqueen. Some methods for classiﬁcation and analysis of multivari-\nate observations. In 5th Berkeley Symposium on Mathematical Statistics\nand Probability, pages 281–297, 1967.\n[93]\nAlex Rodriguez and Alessandro Laio. Clustering by fast search and ﬁnd\nof density peaks. Science, 344(6191):1492–1496, 2014.\n[94]\nLaurens Van Der Maaten.\nLearning a parametric embedding by\npreserving local structure. JMLR, 5:384–391, 2009.\n[95]\nM Pawan Kumar, Benjamin Packer, and Daphne Koller.\nSelf-paced\nlearning for latent variable models.\nIn NeurIPS, pages 1189–1197,\n2010.\n[96]\nRichard Souvenir and Robert Pless.\nManifold clustering.\nIn ICCV,\nvolume 1, pages 648–653, 2005.\n[97]\nEhsan Elhamifar and René Vidal.\nSparse manifold clustering and\nembedding. In NeurIPS, pages 55–63, 2011.\n[98]\nVinod Nair and Geoffrey E Hinton.\nRectiﬁed linear units improve\nrestricted boltzmann machines. In ICML, 2010.\n18\n[99]\nAysegul Dundar, Jonghoon Jin, and Eugenio Culurciello. Convolutional\nclustering for unsupervised learning. arXiv preprint arXiv:1511.06241,\n2015.\n[100] Stephen C Johnson. Hierarchical clustering schemes. Psychometrika,\n32(3):241–254, 1967.\n[101] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial\ntransformer networks. In NeurIPS, pages 2017–2025, 2015.\n[102] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks.\nIn NeurIPS,\nvolume 25, pages 1097–1105, 2012.\n[103] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. Commun. ACM,\n60(6):84–90, 2017.\n[104] Karen Simonyan and Andrew Zisserman.\nVery deep convolu-\ntional networks for large-scale image recognition.\narXiv preprint\narXiv:1409.1556, 2014.\n[105] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.\nImagenet: A large-scale hierarchical image database. In CVPR, pages\n248–255, 2009.\n[106] Linli Xu, James Neufeld, Bryce Larson, and Dale Schuurmans. Maxi-\nmum margin clustering. NeurIPS, 17:1537–1544, 2004.\n[107] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Mach\nLearn, 20(3):273–297, 1995.\n[108] Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and\nMasashi Sugiyama. Learning discrete representations via information\nmaximizing self-augmented training. In ICML, pages 1558–1567, 2017.\n[109] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Gre-\nwal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep\nrepresentations by mutual information estimation and maximization.\narXiv preprint arXiv:1808.06670, 2018.\n[110] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity\nmetric discriminatively, with application to face veriﬁcation. In CVPR,\nvolume 1, pages 539–546, 2005.\n[111] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A\nuniﬁed embedding for face recognition and clustering. In CVPR, pages\n815–823, 2015.\n[112] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual\nrepresentation learning by context prediction. In ICCV, pages 1422–\n1430, 2015.\n[113] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and\nAlexei A Efros. Context encoders: Feature learning by inpainting. In\nCVPR, pages 2536–2544, 2016.\n[114] Richard Zhang, Phillip Isola, and Alexei A Efros.\nColorful image\ncolorization. In ECCV, pages 649–666, 2016.\n[115] Mehdi Noroozi and Paolo Favaro.\nUnsupervised learning of visual\nrepresentations by solving jigsaw puzzles. In ECCV, pages 69–84, 2016.\n[116] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised\nrepresentation learning by predicting image rotations. arXiv preprint\narXiv:1803.07728, 2018.\n[117] Diederik P Kingma and Max Welling. Auto-encoding variational bayes.\narXiv preprint arXiv:1312.6114, 2013.\n[118] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\nWarde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.\nGenerative adversarial nets. In NeurIPS, pages 2672–2680, 2014.\n[119] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel\nrecurrent neural networks. arXiv preprint arXiv:1601.06759, 2016.\n[120] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever,\nand Pieter Abbeel.\nInfogan: Interpretable representation learning by\ninformation maximizing generative adversarial nets. In NeurIPS, pages\n2172–2180, 2016.\n[121] Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and\nJason Yosinski. Plug & play generative networks: Conditional iterative\ngeneration of images in latent space. In CVPR, pages 4467–4477, 2017.\n[122] M Ehsan Abbasnejad, Anthony Dick, and Anton van den Hengel.\nInﬁnite variational autoencoder for semi-supervised learning. In CVPR,\npages 5888–5897, 2017.\n[123] Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max\nWelling. Semi-supervised learning with deep generative models. In\nNeurIPS, pages 3581–3589, 2014.\n[124] Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, and\nOle Winther.\nAuxiliary deep generative models.\narXiv preprint\narXiv:1602.05473, 2016.\n[125] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec\nRadford, and Xi Chen.\nImproved techniques for training gans.\nIn\nNeurIPS, pages 2234–2242, 2016.\n[126] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfel-\nlow, and Brendan Frey.\nAdversarial autoencoders.\narXiv preprint\narXiv:1511.05644, 2015.\n[127] Alexey Dosovitskiy and Thomas Brox. Generating images with per-\nceptual similarity metrics based on deep networks. In NeurIPS, pages\n658–666, 2016.\n[128] Alec Radford, Luke Metz, and Soumith Chintala.\nUnsupervised\nrepresentation learning with deep convolutional generative adversarial\nnetworks. arXiv preprint arXiv:1511.06434, 2015.\n[129] Gang Chen.\nDeep learning with nonparametric clustering.\narXiv\npreprint arXiv:1501.03084, 2015.\n[130] Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another\nway to carve up the variational evidence lower bound. In NeurIPS,\n2016.\n[131] Geoffrey J McLachlan, Sharon X Lee, and Suren I Rathnayake. Finite\nmixture models. ANNU REV STAT APPL, 6:355–378, 2000.\n[132] Matthew J Beal.\nVariational algorithms for approximate Bayesian\ninference. PhD thesis, UCL (University College London), 2003.\n[133] Nevin L Zhang. Hierarchical latent class models for cluster analysis.\nJMLR, 5(6):697–723, 2004.\n[134] Daphne Koller and Nir Friedman.\nProbabilistic graphical models:\nprinciples and techniques. MIT press, 2009.\n[135] Linxiao Yang, Ngai-Man Cheung, Jiaying Li, and Jun Fang.\nDeep\nclustering by gaussian mixture variational autoencoders with graph\nembedding. In ICCV, pages 6440–6449, 2019.\n[136] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization\nwith gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.\n[137] Chris J Maddison, Andriy Mnih, and Yee Whye Teh.\nThe concrete\ndistribution: A continuous relaxation of discrete random variables.\narXiv preprint arXiv:1611.00712, 2016.\n[138] Shengjia Zhao, Jiaming Song, and Stefano Ermon. Learning hierarchi-\ncal features from generative models. arXiv preprint arXiv:1702.08396,\n2017.\n[139] Andreas Krause, Pietro Perona, and Ryan G Gomes. Discriminative\nclustering by regularized information maximization. In NeurIPS, pages\n775–783, 2010.\n[140] Yves Grandvalet and Yoshua Bengio.\nSemi-supervised learning by\nentropy minimization. In NeurIPS, pages 529–536, 2005.\n[141] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and\nPierre Antoine Manzagol. Stacked denoising autoencoders: Learning\nuseful representations in a deep network with a local denoising criterion.\nJMLR, 11(12):3371–3408, 2010.\n[142] David Berthelot, Colin Raffel, Aurko Roy, and Ian Goodfellow. Under-\nstanding and improving interpolation in autoencoders via an adversarial\nregularizer. arXiv preprint arXiv:1807.07543, 2018.\n[143] Uri Shaham, Kelly Stanton, Henry Li, Boaz Nadler, Ronen Basri,\nand Yuval Kluger. Spectralnet: Spectral clustering using deep neural\nnetworks. arXiv preprint arXiv:1801.01587, 2018.\n[144] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduc-\ntion by learning an invariant mapping.\nIn CVPR, volume 2, pages\n1735–1742, 2006.\n[145] Uri Shaham and Roy R Lederman. Learning by coincidence: Siamese\nnetworks and common variable learning. Pattern Recognition, 74:52–\n63, 2018.\n[146] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner,\nand Gabriele Monfardini. The graph neural network model. IEEE T\nNEURAL NETWOR, 20(1):61–80, 2008.\n[147] David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre,\nRafael Gómez-Bombarelli, Timothy Hirzel, Alán Aspuru-Guzik, and\nRyan P Adams. Convolutional networks on graphs for learning molec-\nular ﬁngerprints. arXiv preprint arXiv:1509.09292, 2015.\n[148] Mohamed A Khamsi and William A Kirk. An introduction to metric\nspaces and ﬁxed point theory, volume 53. John Wiley & Sons, 2011.\n[149] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang,\nZhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph\nneural networks: A review of methods and applications. AI Open, 1:57–\n81, 2020.\n[150] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with\ngraph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.\n[151] Fei Tian, Bin Gao, Qing Cui, Enhong Chen, and Tie-Yan Liu. Learning\ndeep representations for graph clustering. In AAAI, 2014.\n[152] Ming Shao, Sheng Li, Zhengming Ding, and Yun Fu.\nDeep linear\ncoding for fast graph clustering. In IJCAI, 2015.\n[153] Peng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. A survey on network\nembedding. TKDE, 31(5):833–852, 2018.\n19\n[154] Daokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang. Network\nrepresentation learning: A survey. IEEE Trans. Big Data, 6(1):3–28,\n2018.\n[155] Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. A\ncomprehensive survey of graph embedding: Problems, techniques, and\napplications. TKDE, 30(9):1616–1637, 2018.\n[156] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi\nZhang, and S Yu Philip.\nA comprehensive survey on graph neural\nnetworks. IEEE Trans. Neural Netw. Learn. Syst., 32(1):4–24, 2020.\n[157] Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-Jiang Zhang, Qiang\nYang, and Stephen Lin. Graph embedding and extensions: A general\nframework for dimensionality reduction. TPAMI, 29(1):40–51, 2006.\n[158] Ana LN Fred and Anil K Jain. Combining multiple clusterings using\nevidence accumulation. TPAMI, 27(6):835–850, 2005.\n[159] Chun Wang, Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, and\nChengqi Zhang. Attributed graph clustering: A deep attentional embed-\nding approach. arXiv preprint arXiv:1906.06532, 2019.\n[160] Yazhou Ren, Kangrong Hu, Xinyi Dai, Lili Pan, Steven CH Hoi, and\nZenglin Xu. Semi-supervised deep embedded clustering. Neurocom-\nputing, 325:121–130, 2019.\n[161] Joseph Enguehard, Peter O’Halloran, and Ali Gholipour.\nSemi-\nsupervised learning with deep embedded clustering for image classi-\nﬁcation and segmentation. IEEE Access, 7:11093–11104, 2019.\n[162] Hongjing Zhang, Sugato Basu, and Ian Davidson. A framework for\ndeep constrained clustering-algorithms and advances. In ECML-PKDD,\npages 57–72, 2019.\n[163] Ankita Shukla, Gullal S Cheema, and Saket Anand. Semi-supervised\nclustering with neural networks.\nIn BigMM, pages 152–161. IEEE,\n2020.\n[164] Olivier Chapelle and Alexander Zien. Semi-supervised classiﬁcation\nby low density separation. In AISTATS, volume 2005, pages 57–64.\nCiteseer, 2005.\n[165] Kaizhu Huang, Zenglin Xu, Irwin King, and Michael R Lyu. Semi-\nsupervised learning from general unlabeled data. In ICDM, pages 273–\n282. IEEE, 2008.\n[166] Zenglin Xu, Irwin King, Michael Rung-Tsong Lyu, and Rong Jin.\nDiscriminative semi-supervised feature selection via manifold regular-\nization. IEEE T NEURAL NETWOR, 21(7):1033–1047, 2010.\n[167] Yi Huang, Dong Xu, and Feiping Nie.\nSemi-supervised dimension\nreduction using trace ratio criterion.\nIEEE T NEUR NET LEAR,\n23(3):519–526, 2012.\n[168] Sugato Basu, Arindam Banerjee, and Raymond Mooney.\nSemi-\nsupervised clustering by seeding. In ICML, 2002.\n[169] Nizar Grira, Michel Crucianu, and Nozha Boujemaa. Unsupervised and\nsemi-supervised clustering: a brief survey. A review of machine learning\ntechniques for processing multimedia content, 1:9–16, 2004.\n[170] Kamalika Chaudhuri, Sham M Kakade, Karen Livescu, and Karthik\nSridharan. Multi-view clusieee t neur net lear. In ICML, pages 129–\n136, 2009.\n[171] Yeqing Li, Feiping Nie, Heng Huang, and Junzhou Huang. Large-scale\nmulti-view spectral clustering via bipartite graph. In AAAI, 2015.\n[172] Xiaochun Cao, Changqing Zhang, Huazhu Fu, Si Liu, and Hua Zhang.\nDiversity-induced multi-view subspace clustering. In CVPR, pages 586–\n594, 2015.\n[173] Feiping Nie, Jing Li, and Xuelong Li.\nSelf-weighted multi-view\nclustering with multiple graphs. In IJCAI, pages 2564–2570, 2017.\n[174] Changqing Zhang, Qinghua Hu, Huazhu Fu, Pengfei Zhu, and Xiaochun\nCao. Latent multi-view subspace clustering. In CVPR, pages 4279–\n4287, 2017.\n[175] Zheng Zhang, Li Liu, Fumin Shen, Heng Tao Shen, and Ling Shao.\nBinary multi-view clustering. TPAMI, 41(7):1774–1782, 2018.\n[176] Handong Zhao, Zhengming Ding, and Yun Fu. Multi-view clustering\nvia deep matrix factorization. In AAAI, 2017.\n[177] Maria Brbi´c and Ivica Kopriva. Multi-view low-rank sparse subspace\nclustering. Pattern Recognition, 73:247–258, 2018.\n[178] Yazhou Ren, Shudong Huang, Peng Zhao, Minghao Han, and Zenglin\nXu. Self-paced and auto-weighted multi-view clustering. Neurocom-\nputing, 383:248–256, 2019.\n[179] Chang Xu, Dacheng Tao, and Chao Xu. Multi-view self-paced learning\nfor clustering. In IJCAI, 2015.\n[180] Jie Xu, Yazhou Ren, Guofeng Li, Lili Pan, Ce Zhu, and Zenglin\nXu. Deep embedded multi-view clustering with collaborative training.\nInformation Sciences, 573:279–290, 2021.\n[181] Shaohua Fan, Xiao Wang, Chuan Shi, Emiao Lu, Ken Lin, and Bai\nWang. One2multi graph autoencoder for multi-view graph clustering.\nIn WWW, pages 3070–3076, 2020.\n[182] Xiaoliang Tang, Xuan Tang, Wanli Wang, Li Fang, and Xian Wei.\nDeep multi-view sparse subspace clustering. In ICNCC, pages 115–\n119, 2018.\n[183] Ruihuang Li, Changqing Zhang, Huazhu Fu, Xi Peng, Tianyi Zhou, and\nQinghua Hu. Reciprocal multi-layer subspace learning for multi-view\nclustering. In ICCV, pages 8172–8180, 2019.\n[184] Pengfei Zhu, Binyuan Hui, Changqing Zhang, Dawei Du, Longyin Wen,\nand Qinghua Hu. Multi-view deep subspace clustering networks. arXiv\npreprint arXiv:1908.01978, 2019.\n[185] Zhaoyang Li, Qianqian Wang, Zhiqiang Tao, Quanxue Gao, and Zhao-\nhua Yang. Deep adversarial multi-view clustering network. In IJCAI,\npages 2952–2958, 2019.\n[186] Ming Yin, Weitian Huang, and Junbin Gao. Shared generative latent\nrepresentation learning for multi-view clustering. In AAAI, pages 6688–\n6695, 2020.\n[187] Jie Xu, Yazhou Ren, Huayi Tang, Xiaorong Pu, Xiaofeng Zhu, Ming\nZeng, and Lifang He. Multi-VAE: Learning disentangled view-common\nand view-peculiar visual representations for multi-view clustering. In\nICCV, pages 9234–9243, 2021.\n[188] Fangfei Lin, Bing Bai, Kun Bai, Yazhou Ren, Peng Zhao, and Zenglin\nXu. Contrastive multi-view hyperbolic hierarchical clustering. In IJCAI,\npages 3250–3256, 2022.\n[189] Jie Xu, Huayi Tang, Yazhou Ren, Liang Peng, Xiaofeng Zhu, and Lifang\nHe. Multi-level feature learning for contrastive multi-view clustering.\nIn CVPR, pages 16051–16060, 2022.\n[190] Jie Xu, Chao Li, Yazhou Ren, Liang Peng, Yujie Mo, Xiaoshuang Shi,\nand Xiaofeng Zhu. Deep incomplete multi-view clustering via mining\ncluster complementarity. In AAAI, pages 8761–8769, 2022.\n[191] Muhammad Raza Khan and Joshua E Blumenstock. Multi-GCN: Graph\nconvolutional networks for multi-view networks, with applications to\nglobal poverty. In AAAI, volume 33, pages 606–613, 2019.\n[192] Jiafeng Cheng, Qianqian Wang, Zhiqiang Tao, De-Yan Xie, and\nQuanxue Gao.\nMulti-view attribute graph convolution networks for\nclustering. In IJCAI, pages 2973–2979, 2020.\n[193] Yiming Wang, Dongxia Chang, Zhiqiang Fu, and Yao Zhao. Consistent\nmultiple graph embedding for multi-view clustering.\narXiv preprint\narXiv:2105.04880, 2021.\n[194] Zongmo Huang, Yazhou Ren, Xiaorong Pu, and Lifang He. Deep em-\nbedded multi-view clustering via jointly learning latent representations\nand graphs. arXiv preprint arXiv:2205.03803, 2022.\n[195] René Vidal. Subspace clustering. IEEE Signal Processing Magazine,\n28(2):52–68, 2011.\n[196] Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering:\nAnalysis and an algorithm. In NeurIPS, pages 849–856, 2002.\n[197] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmenta-\ntion. Departmental Papers (CIS), page 107, 2000.\n[198] Ehsan Elhamifar and René Vidal. Sparse subspace clustering. In CVPR,\npages 2790–2797. IEEE, 2009.\n[199] Feiping Nie, Hua Wang, Heng Huang, and Chris Ding. Unsupervised\nand semi-supervised learning via l1-norm graph. In ICCV, pages 2268–\n2273, 2011.\n[200] Can-Yi Lu, Hai Min, Zhong-Qiu Zhao, Lin Zhu, De-Shuang Huang, and\nShuicheng Yan. Robust and efﬁcient subspace segmentation via least\nsquares regression. In ECCV, pages 347–360, 2012.\n[201] Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering: Algo-\nrithm, theory, and applications. TPAMI, 35(11):2765–2781, 2013.\n[202] Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, and\nYi Ma. Robust recovery of subspace structures by low-rank representa-\ntion. TPAMI, 35(1):171–184, 2012.\n[203] Jiashi Feng, Zhouchen Lin, Huan Xu, and Shuicheng Yan.\nRobust\nsubspace segmentation with block-diagonal prior.\nIn CVPR, pages\n3818–3825, 2014.\n[204] Xi Peng, Zhang Yi, and Huajin Tang. Robust subspace clustering via\nthresholding ridge regression. In AAAI, 2015.\n[205] Changqing Zhang, Huazhu Fu, Si Liu, Guangcan Liu, and Xiaochun\nCao. Low-rank tensor constrained multiview subspace clustering. In\nICCV, pages 1582–1590, 2015.\n[206] Chang Xu, Dacheng Tao, and Chao Xu.\nA survey on multi-view\nlearning. arXiv preprint arXiv:1304.5634, 2013.\n[207] Theodore Wilbur Anderson. An introduction to multivariate statistical\nanalysis. Technical report, Wiley New York, 1962.\n[208] Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep\ncanonical correlation analysis. In ICML, pages 1247–1255, 2013.\n[209] Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. On deep\nmulti-view representation learning. In ICML, pages 1083–1092, 2015.\n20\n[210] Meng Qu, Jian Tang, Jingbo Shang, Xiang Ren, Ming Zhang, and\nJiawei Han. An attention-based collaboration framework for multi-view\nnetwork representation learning. In CIKM, pages 1767–1776, 2017.\n[211] Satu Elisa Schaeffer.\nGraph clustering.\nComputer science review,\n1(1):27–64, 2007.\n[212] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and\nHyunwoo J Kim.\nGraph transformer networks.\nIn NeurIPS, pages\n11983–11993, 2019.\n[213] Hugh Perkins and Yi Yang. Dialog intent induction with deep multi-\nview clustering. arXiv preprint arXiv:1908.11487, 2019.\n[214] Mahdi Abavisani and Vishal M Patel.\nDeep multimodal subspace\nclustering networks. IEEE J-STSP, 12(6):1601–1614, 2018.\n[215] Di Hu, Feiping Nie, and Xuelong Li. Deep multimodal clustering for\nunsupervised audiovisual learning. In CVPR, pages 9248–9257, 2019.\n[216] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. KDE,\n22(10):1345–1359, 2010.\n[217] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Bal-\nduzzi, and Wen Li.\nDeep reconstruction-classiﬁcation networks for\nunsupervised domain adaptation. In ECCV, pages 597–613, 2016.\n[218] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.\nHow\ntransferable are features in deep neural networks? In NeurIPS, pages\n3320–3328, 2014.\n[219] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\nDragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew\nRabinovich. Going deeper with convolutions. In CVPR, pages 1–9,\n2015.\n[220] Muhammad Ghifary, W Bastiaan Kleijn, and Mengjie Zhang. Domain\nadaptive neural networks for object recognition. In PRICAI, pages 898–\n904, 2014.\n[221] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard\nSchölkopf, and Alexander Smola. A iciarst. JMLR, 13(1):723–773,\n2012.\n[222] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learn-\ning transferable features with deep adaptation networks. In ICML, pages\n97–105, 2015.\n[223] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep\ntransfer learning with joint adaptation networks. In ICML, pages 2208–\n2217, 2017.\n[224] Hongliang Yan, Yukang Ding, Peihua Li, Qilong Wang, Yong Xu, and\nWangmeng Zuo. Mind the class weight bias: Weighted maximum mean\ndiscrepancy for unsupervised domain adaptation. In CVPR, pages 2272–\n2281, 2017.\n[225] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan.\nUnsupervised domain adaptation with residual transfer networks. In\nNeurIPS, pages 136–144, 2016.\n[226] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethu-\nraman Panchanathan. Deep hashing network for unsupervised domain\nadaptation. In CVPR, pages 5018–5027, 2017.\n[227] Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel\nUlbricht.\nSliced wasserstein discrepancy for unsupervised domain\nadaptation. In CVPR, pages 10285–10295, 2019.\n[228] Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for\nunsupervised domain adaptation. In Domain Adaptation in Computer\nVision Applications, pages 153–171. Springer, 2017.\n[229] Chao Chen, Zhihang Fu, Zhihong Chen, Sheng Jin, Zhaowei Cheng,\nXinyu Jin, and Xian-Sheng Hua. Homm: Higher-order moment match-\ning for unsupervised domain adaptation. In AAAI, volume 34, pages\n3422–3429, 2020.\n[230] Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann.\nContrastive adaptation network for unsupervised domain adaptation. In\nCVPR, pages 4893–4902, 2019.\n[231] Lanqing Hu, Meina Kan, Shiguang Shan, and Xilin Chen. Unsupervised\ndomain adaptation with hierarchical gradient synchronization. In CVPR,\npages 4043–4052, 2020.\n[232] Mengxue Li, Yi-Ming Zhai, You-Wei Luo, Peng-Fei Ge, and Chuan-\nXian Ren.\nEnhanced transport distance for unsupervised domain\nadaptation. In CVPR, pages 13936–13944, 2020.\n[233] Renjun Xu, Pelen Liu, Liyan Wang, Chao Chen, and Jindong Wang. Re-\nliable weighted optimal transport for unsupervised domain adaptation.\nIn CVPR, pages 4394–4403, 2020.\n[234] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira.\nAnalysis of representations for domain adaptation. NeurIPS, pages 137–\n144, 2006.\n[235] Hui Tang, Ke Chen, and Kui Jia. Unsupervised domain adaptation via\nstructurally regularized deep clustering. In CVPR, pages 8725–8735,\n2020.\n[236] Qian Wang and Toby Breckon.\nUnsupervised domain adaptation\nvia structured prediction based selective pseudo-labeling.\nIn AAAI,\nvolume 34, pages 6243–6250, 2020.\n[237] Xiaofei He and Partha Niyogi.\nLocality preserving projections.\nIn\nNeurIPS, 2003.\n[238] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang.\nDomain\nadaptive ensemble learning. IEEE T IMAGE PROCESS, 30:8008–8018,\n2021.\n[239] Viraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoffman.\nSentry: Selective entropy optimization via committee consistency for\nunsupervised domain adaptation. In ICCV, pages 8558–8567, 2021.\n[240] Xiang Jiang, Qicheng Lao, Stan Matwin, and Mohammad Havaei.\nImplicit class-conditioned domain alignment for unsupervised domain\nadaptation. In ICML, pages 4816–4827, 2020.\n[241] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and\nShangling Jui. Unsupervised domain adaptation without source data\nby casting a bait. arXiv preprint arXiv:2010.12427, 1(2):3, 2020.\n[242] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access\nthe source data? source hypothesis transfer for unsupervised domain\nadaptation. In ICML, pages 6028–6039, 2020.\n[243] Jian Liang, Dapeng Hu, Yunbo Wang, Ran He, and Jiashi Feng. Source\ndata-absent unsupervised domain adaptation through hypothesis transfer\nand labeling transfer. IEEE T PATTERN ANAL, 2021.\n[244] Song Tang, Yan Yang, Zhiyuan Ma, Norman Hendrich, Fanyu Zeng,\nShuzhi Sam Ge, Changshui Zhang, and Jianwei Zhang.\nNearest\nneighborhood-based deep clustering for source data-absent unsuper-\nvised domain adaptation. arXiv preprint arXiv:2107.12585, 2021.\n[245] Ming-Yu Liu and Oncel Tuzel.\nCoupled generative adversarial net-\nworks. In NeurIPS, pages 469–477, 2016.\n[246] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain,\nHugo Larochelle, François Laviolette, Mario Marchand, and Victor\nLempitsky.\nDomain-adversarial training of neural networks.\nJMLR,\n17(1):2096–2030, 2016.\n[247] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-\nimage translation networks. In NeurIPS, pages 700–708, 2017.\n[248] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adver-\nsarial discriminative domain adaptation. In CVPR, pages 7167–7176,\n2017.\n[249] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru\nErhan, and Dilip Krishnan. Unsupervised pixel-level domain adaptation\nwith generative adversarial networks.\nIn CVPR, pages 3722–3731,\n2017.\n[250] Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama\nChellappa.\nGenerate to adapt: Aligning domains using generative\nadversarial networks. In CVPR, pages 8503–8512, 2018.\n[251] Pedro O Pinheiro.\nUnsupervised domain adaptation with similarity\nlearning. In CVPR, pages 8004–8013, 2018.\n[252] Zhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang.\nMulti-adversarial domain adaptation. arXiv preprint arXiv:1809.02176,\n2018.\n[253] Riccardo Volpi, Pietro Morerio, Silvio Savarese, and Vittorio Murino.\nAdversarial feature augmentation for unsupervised domain adaptation.\nIn CVPR, pages 5495–5504, 2018.\n[254] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola,\nKate Saenko, Alexei Efros, and Trevor Darrell.\nCycada: Cycle-\nconsistent adversarial domain adaptation. In ICML, pages 1989–1998,\n2018.\n[255] Yabin Zhang, Hui Tang, Kui Jia, and Mingkui Tan. Domain-symmetric\nnetworks for adversarial domain adaptation. In CVPR, pages 5031–\n5040, 2019.\n[256] Issam H Laradji and Reza Babanezhad. M-adda: Unsupervised domain\nadaptation with deep metric learning. In Domain Adaptation for Visual\nUnderstanding, pages 17–31. Springer, 2020.\n[257] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve\nunsupervised domain adaptation with mixup training. arXiv preprint\narXiv:2001.00677, 2020.\n[258] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model\nadaptation: Unsupervised domain adaptation without source data. In\nCVPR, pages 9641–9650, 2020.\n[259] Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang,\nQi Tian, and Wenjun Zhang.\nAdversarial domain adaptation with\ndomain mixup. In AAAI, volume 34, pages 6502–6509, 2020.\n[260] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros.\nUnsu-\npervised domain adaptation through self-supervision.\narXiv preprint\narXiv:1909.11825, 2019.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-10-09",
  "updated": "2022-10-09"
}