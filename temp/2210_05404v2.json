{
  "id": "http://arxiv.org/abs/2210.05404v2",
  "title": "Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting",
  "authors": [
    "Zifan Jiang",
    "Amit Moryossef",
    "Mathias Müller",
    "Sarah Ebling"
  ],
  "abstract": "This paper presents work on novel machine translation (MT) systems between\nspoken and signed languages, where signed languages are represented in\nSignWriting, a sign language writing system. Our work seeks to address the lack\nof out-of-the-box support for signed languages in current MT systems and is\nbased on the SignBank dataset, which contains pairs of spoken language text and\nSignWriting content. We introduce novel methods to parse, factorize, decode,\nand evaluate SignWriting, leveraging ideas from neural factored MT. In a\nbilingual setup--translating from American Sign Language to (American)\nEnglish--our method achieves over 30 BLEU, while in two multilingual\nsetups--translating in both directions between spoken languages and signed\nlanguages--we achieve over 20 BLEU. We find that common MT techniques used to\nimprove spoken language translation similarly affect the performance of sign\nlanguage translation. These findings validate our use of an intermediate text\nrepresentation for signed languages to include them in natural language\nprocessing research.",
  "text": "Machine Translation between Spoken Languages and Signed Languages\nRepresented in SignWriting\nZifan Jiang\nUniversity of Zurich\njiang@cl.uzh.ch\nAmit Moryossef\nBar-Ilan University\nUniversity of Zurich\namitmoryossef@gmail.com\nMathias Müller\nUniversity of Zurich\nmmueller@cl.uzh.ch\nSarah Ebling\nUniversity of Zurich\nebling@cl.uzh.ch\nAbstract\nThis paper presents work on novel machine\ntranslation (MT) systems between spoken and\nsigned languages, where signed languages are\nrepresented in SignWriting, a sign language\nwriting system. Our work1 seeks to address\nthe lack of out-of-the-box support for signed\nlanguages in current MT systems and is based\non the SignBank dataset, which contains pairs\nof spoken language text and SignWriting con-\ntent.\nWe introduce novel methods to parse,\nfactorize, decode, and evaluate SignWriting,\nleveraging ideas from neural factored MT. In\na bilingual setup—translating from American\nSign Language to (American) English—our\nmethod achieves over 30 BLEU, while in two\nmultilingual setups— translating in both direc-\ntions between spoken languages and signed\nlanguages—we achieve over 20 BLEU. We\nﬁnd that common MT techniques used to im-\nprove spoken language translation similarly af-\nfect the performance of sign language transla-\ntion. These ﬁndings validate our use of an in-\ntermediate text representation for signed lan-\nguages to include them in NLP research.\n1\nIntroduction\nMost current machine translation (MT) systems\nonly support spoken language input and output\n(text or speech), which excludes around 200 dif-\nferent signed languages used by up to 70 million\ndeaf people2 worldwide from modern language\ntechnology. Since signed languages are also natu-\nral languages, Yin et al. (2021) calls for including\nsign language processing (SLP) in natural language\nprocessing (NLP) research.\nFrom a technical point of view, SLP brings novel\nchallenges to NLP due to the visual-gestural modal-\nity of sign language and special linguistic features\n1Code and documentation available at https://github.\ncom/J22Melody/signwriting-translation\n2According to the World Federation of the Deaf: https:\n//wfdeaf.org/our-work/\nVerse 41. He gave\nher his hand and helped her up. \nThen he called in the\nwidows and all the believers, and \nhe presented her to\nthem alive.\nexpand_more\nswap_horiz\nexpand_more\n󲇈󵨑󸗨\n󱦡󰀡\n󿌁\n󲇡󸗨\n󲇸󲇐󶿈\n󰁈\n󳈠󳈘\n󺒲󺒡\n󲇡\n󸗢\n UNITED STATES\n \n UNITED KINGDOM\n \n FRANCE\n \nDETECT LANGUAGE\nENGLISH\nFRENCH\nSPANISH\nFigure 1: Demo application based on our models, trans-\nlating from spoken languages to signed languages rep-\nresented in SignWriting, then to human poses.\n(e.g., the use of space, simultaneity, referencing),\nwhich requires both computer vision (CV) and NLP\ntechnologies. Crucially, the lack of a standardized\nor widely used written form for signed languages\nhas hindered their inclusion in NLP research.\nHowever, sign language writing systems do exist\nand are sporadically used (e.g., SignWriting (Sut-\nton, 1990) and HamNoSys (Prillwitz and Zienert,\n1990)). Therefore, we adopt the proposal of Yin\net al. (2021) to formulate the sign language trans-\nlation (SLT) task using a sign language writing\nsystem as an intermediate step (illustrated by Fig-\nure 1): given spoken language text, we propose to\ntranslate to sign language in a written form, then\ntransform this intermediate result into a ﬁnal video\nor pose output3—and vice versa. According to this\nmulti-step view of SLT, in this work we study trans-\nlation between signed languages in written form\nand spoken languages. We use SignWriting as the\nintermediate writing system.\nSignWriting has many advantages, like being\nuniversal (multilingual), comparatively easy to un-\nderstand, extensively documented, and computer-\nsupported.\nIn addition, despite looking picto-\ngraphic, it is a well-deﬁned writing system. Every\n3Note that the second step, animation of SignWriting into\nhuman poses or video, is not included in this research. In the\ndemo application, spoken language text is translated directly\ninto sign language poses, resulting in low-quality output.\narXiv:2210.05404v2  [cs.CL]  23 Feb 2023\nsign can be written as a sequence of symbols (box\nmarkers, graphemes, and punctuation marks) and\ntheir location on a 2-dimensional plane.\nTo our knowledge, this work is the ﬁrst to create\nautomatic SLT systems that use SignWriting. Our\nmain contributions are as follows: (a) we propose\nmethods to parse (§3.3), factorize (§3.4), decode\n(§4.3), and evaluate (§4.3) SignWriting sequences;\n(b) we report experiments on multilingual machine\ntranslation systems between SignWriting and spo-\nken language text (§4); (c) we demonstrate that\ncommon techniques for low-resource MT are bene-\nﬁcial for SignWriting translation systems (§5).\n2\nBackground\n2.1\nSign language processing (SLP)\nSLP (Bragg et al., 2019; Yin et al., 2021; Moryossef\nand Goldberg, 2021) is an emerging subﬁeld of\nboth NLP and CV, which focuses on automatic\nprocessing and analysis of sign language content.\nProminent tasks include pose estimation from sign\nlanguage videos (Cao et al., 2017, 2021; Güler\net al., 2018), gloss transcription (Mesch and Wallin,\n2012; Johnston and Beuzeville, 2016; Konrad et al.,\n2018), sign language detection (Borg and Camilleri,\n2019; Moryossef et al., 2020), sign language identi-\nﬁcation (Gebre et al., 2013; Monteiro et al., 2016),\nand sign language segmentation (Bull et al., 2020;\nFarag and Brock, 2019; Santemiz et al., 2009).\nBesides, tasks including sign language recogni-\ntion (Adaloglou et al., 2021), translation, and pro-\nduction involve transforming one sign language rep-\nresentation to another or from/to spoken language\ntext, as shown in Figure 24. We ﬁnd that exist-\ning works cover gloss-to-text (Camgöz et al., 2018;\nYin and Read, 2020) (where “text” denotes spo-\nken language text), text-to-gloss (Zhao et al., 2000;\nOthman and Jemni, 2012), video-to-text (Camgöz\net al., 2020b,a), pose-to-text (Ko et al., 2019), and\ntext-to-pose (Saunders et al., 2020a,b,c; Zelinka\nand Kanis, 2020; Xiao et al., 2020).\n2.2\nMotivation\nOur work is the ﬁrst to explore translation between\nspoken language text and sign language content\nrepresented in SignWriting5. We focus on a sign\nlanguage writing system for the following reasons:\n4In the paper, we distinguish between a phonetic “writing\nsystem” (e.g., SignWriting) and “glosses” (lexical notation,\nmarking the semantics of each sign with a distinct category).\n5Related work based on HamNoSys: Morrissey (2011);\nSanaullah et al. (2021); Walsh et al. (2022)\nVideo\nText\nPose\nGlosses\nWriting System\nFigure 2: SLP tasks. Every edge on the left side rep-\nresents a task in CV (language-agnostic). Every edge\non the right side represents a task in NLP (language-\nspeciﬁc). Every edge crossing both sides represents a\ntask requiring a combination of CV and NLP. Figure\ntaken from Moryossef and Goldberg (2021).\n(a) currently an end-to-end (video-to-text/text-to-\nvideo) approach is not feasible. State-of-the-art\nsystems either have a BLEU score lower than 1\n(Müller et al., 2022a) or work only on a very nar-\nrow linguistic domain, e.g., Camgöz et al. (2020b,a)\nwork on the RWTH-PHOENIX-Weather T data set\nwhich covers only 1,231 unique signs from weather\nreports (less than what we use in Table 2); (b) a\nwriting system is lower-dimensional than videos\n(not all parts of a video are relevant in a linguistic\nsense), while adequate to encode information of\nsigns; (c) written sign language is a closer ﬁt to\ncurrent MT pipelines than videos or poses; (d) a\nphonetic writing system is a more universal solu-\ntion than glossing since glosses are semantic and\ntherefore language-speciﬁc, and are an inadequate\nrepresentation of meaning (Müller et al., 2022b).\n2.3\nSignWriting, FSW, and SWU\nSignWriting (Sutton, 1990) is a featural and vi-\nsually iconic sign language writing system (intro-\nduced extensively in Appendix A). Previous work\nexplored recognition (Stiehl et al., 2015) and ani-\nmation (Bouzid and Jemni, 2013) of SignWriting.\nSignWriting has two computerized speciﬁca-\ntions, Formal SignWriting in ASCII (FSW) and\nSignWriting in Unicode (SWU). SignWriting is\ntwo-dimensional, but FSW and SWU are written\nlinearly, similar to spoken languages. Figure 3\ngives an example of the relationship between Sign-\nWriting, FSW, and SWU6. We use FSW in our\nresearch instead of SWU to explore the potential\nof factorizing SignWriting symbols and utilizing\nnumerical values of their position (§3.3, §3.4).\n6Online demonstration:\nhttps://slevinski.github.\nio/SuttonSignWriting/characters/index.html.\nFigure 3: “Hello world.” in FSW, SWU and SignWriting graphics. In FSW/SWU, A/SWA and M/SWM are the box\nmarkers (acting as sign boundaries); S14c20 and S27106 (graphemes in SWU) are the symbols; 518 and 529 are\nthe x, y positional numbers on a 2-dimensional plane that denote symbols’ position within a sign box, S38800\n(horizontal bold line in SWU) is the punctuation full stop symbol.\n3\nData and method\nThe data source we use for this research is Sign-\nBank, the largest repository of SignPuddles7.\nA SignPuddle is a community-driven dictionary\nwhere users add parallel examples of SignWrit-\ning and spoken language text (not necessarily with\ncorresponding videos and glosses). The puddles\ncontain material from various signed languages and\nlinguistic domains (e.g., general literature or Bible)\nwithout a strict writing standard. We use the Sign\nLanguage Datasets (Moryossef and Müller, 2021)\nlibrary to load SignBank as a Tensorﬂow Dataset.\n3.1\nData statistics\nIn SignBank, there are roughly 220k parallel sam-\nples from 141 puddles covering 76 language pairs,\nyet the distribution is unbalanced (full details in\nAppendix C). Relatively high-resource language\npairs (over 10k samples) are listed in Table 1.\nNotably, most of the puddles are dictionaries,\nwhich we consider less valuable than sentence pairs\n(instances of continuous signing) for a general MT\nsystem. If dictionaries are used as training data,\nwe expect models to memorize word mappings and\nnot learn to generate sentences.\nTherefore, we treat the four sentence-pair pud-\ndles (Table 2) of the relatively high-resource lan-\nguage pairs as primary data and the other dictionary\npuddles as auxiliary data. Note that even the lan-\nguage pairs constituting the high-resource pairs of\nSignBank are low-resource compared to datasets\nused in mature MT systems for spoken languages,\nwhere millions of parallel sentences are common-\nplace (Akhbardeh et al., 2021).\n7https://www.signbank.org/signpuddle/\n3.2\nData preprocessing\nWe ﬁrst perform general data cleaning to extract\nthe main body of spoken language text and remove\nirrelevant parts such as HTML tags or samples that\nare empty or too long (100 words for a dictionary\nentry). We then learn a byte pair encoding (BPE)\nsegmentation (Sennrich et al., 2016) on the cleaned\nspoken language text, using a vocabulary size of\n2,000.\nMultilingual models\nIn our multilingual experi-\nments (§4.2, §4.3), we learn a shared BPE model\nacross all spoken languages.\nFollowing Johnson et al. (2017), we add special\ntags at the beginning of source sequences to indi-\ncate the desired target language and nature of the\ntraining data (sentence pair or dictionary). Three\ntypes of tags are designed to encode all necessary\ninformation: (a) spoken language code; (b) coun-\ntry code8; (c) dictionary vs. sentence pair. For\nexample, an English sentence to be translated into\nAmerican Sign Language is represented as the fol-\nlowing:\n<2en> <4us> <sent> Hello world.\nData split\nWe shufﬂe the data and split it into\n95%, 3%, and 2% for training, validation, and test\nsets, respectively.\n3.3\nFSW parsing\nOn the sign language side, an appropriate segmen-\ntation and tokenization strategy is needed for the\nFSW data. We parse an original FSW sequence\n(e.g. Figure 3) into several pieces:\n• box markers: A, M, L, R, B;\n8spoken language code plus country code speciﬁes a one-\nto-one mapping to a related signed language in our data.\nlanguage pair\n#samples\n#puddles\nen-us (American English & American Sign Language)\n43,698\n7\npt-br (Brazilian Portuguese & Brazilian Sign Language)\n42,454\n3\nde-de (Standard German & German Sign Language)\n24,704\n3\nfr-ca (Canadian French & Quebec Sign Language)\n11,189\n3\nTable 1: Relatively high-resource language pair statistics.\npuddle name\nlanguage pair\n#samples\n#signs\nmean sequence len\nLiterature US\nen-us\n700\n9,922\n24\nASL Bible Books NLT\nen-us\n11,667\n51,485\n24\nASL Bible Books Shores Deaf Church\nen-us\n4,321\n44,612\n31\nLiteratura Brasil\npt-br\n1,884\n19,221\n13\nTable 2: Primary sentence-pair puddles. Mean sequence length is measured by the mean number of words in the\nspoken language sentences.\n• symbols: S1f010, S18720, etc.;\n• positional numbers x and y: 515, 483, etc.;\n• punctuation marks (special symbols without\nbox markers): S38800, etc.\nWe further factorize each symbol into several\nparts regarding its orientation (see Figure 7 in Ap-\npendix A for an explicit motivation of this step).\nFor example, the symbol S1f010 is split into:\n• symbol core: S1f0;\n• column number (from 0 to 5): 1;\n• row number (from 0 to hex F): 0.\nFor positional numbers, which have a large range\n(from 250 to 750) and are encoded discretely, we\nhypothesise that models might have difﬁculty un-\nderstanding their relative order. Therefore, we fur-\nther calculate two additional factors that denote a\nsymbol’s relative position (based on the absolute\nnumbers) within a sign: relative x and relative y,\nboth ranging from 0 to #symbols - 1.\nWe provide a full example of the result of FSW\nparsing in Listing 1 in Appendix C.\n3.4\nFactored machine translation\nWe use a factored machine translation system\n(Koehn and Hoang, 2007; Garcia-Martinez et al.,\n2016) to encode or decode parsed FSW sequences.\nWe argue that this architecture is suitable because\nSource\nTarget\nsymbol\nS1f010\n“Hi”\n X\n515\n Y\n483\nrelative X\n0\nrelative Y\n1\nsymbol core\nS1f0\ncolumn\n1\nrow\n0\nFigure 4: Representation of translating a FSW symbol\ntogether with its factors to English.\nconcatenating all parsed FSW tokens results in se-\nquences much longer than the maximum length of\nmany Transformer models (e.g., 512).\nFrom another perspective, the essential infor-\nmation units are the symbols. Nevertheless, the\npositional numbers are necessary to determine how\nsymbols are assembled. The same symbols can be\narranged differently in space to convey different\nmeanings.\nIn our setup, we treat the symbols (including\npunctuation marks and box markers) as the primary\nsource/target tokens and the rest as source/target\nfactors that are strictly aligned with each source/-\ntarget token (illustrated by Figure 4).\nDepending on the translation direction, factored\nFSW representations need to be encoded or de-\ncoded. For encoding (when FSW is the source), we\nembed each factor separately and then concatenate\nthem to the aligned symbol’s embedding. For de-\ncoding (when FSW is the target), we use only a sub-\nset of factors (absolute x and y) because others are\nirrelevant for prediction, and additional weighted\ncross-entropy losses are calculated.\n4\nExperiments and results\nThis section introduces three lines of experiments\non both bilingual and multilingual SignWriting\ntranslation. We use Transformer models (Vaswani\net al., 2017) that support source and target factors.\nSee Appendix B for more details on our training\nconﬁguration.\n4.1\nInitial exploration with a bilingual model\nFor a ﬁrst exploration, we train a bilingual model\nthat translates from American Sign Language\n(ASL) to English (en-us). The purpose of this\nexperiment is (a) to demonstrate that automatic\nSignWriting translation is feasible and (b) to ex-\nplore different strategies for data processing and\nhyperparameters.\nWe use roughly 40k parallel training samples\ncomprising roughly 15k sentence pairs and 25k\ndictionary pairs. The quality of spoken language\ntranslation is measured by BLEU (Papineni et al.,\n2002) and chrF2 (Popovi´c, 2015). Table 3 shows\nthe evaluation results on the test set.\n4.2\nMultilingual sign-to-spoken translation\nHere we extend our initial bilingual model to a mul-\ntilingual setting, translating from multiple signed\nlanguages to multiple spoken languages. We deﬁne\ntwo data conditions:\n• high-resource: using roughly 100k parallel\ntraining samples (roughly 17k sentence pairs\nand roughly 83k dictionary pairs covering four\nlanguage pairs),\n• adding low-resource: in addition to the high-\nresource data, use all additional language pairs\nin SignBank that have at least 1k parallel sam-\nples (most of which are dictionaries). The\ntotal number of training examples grows to\nroughly 170k, covering 21 language pairs (Ta-\nble 7).\nThe exact factorization strategy and model hyperpa-\nrameters are informed by our bilingual experiments\nreported in Table 3.\nEvaluating dictionary entries\nFor these multi-\nlingual models, many of the training samples are\ndictionary entries, and so are some test samples.\nTo evaluate the translation quality for dictionary\nentries, we use top-n accuracy, which tests whether\none of the top-n translation candidates from beam\nsearch matches the entry from the reference.\nTable 4 shows the evaluation results on the test\nset.\n4.3\nMultilingual spoken-to-sign translation\nFinally, we train multilingual models that translate\nin the reverse direction, from spoken languages to\nsigned languages. The data and model conﬁgura-\ntion are the same as for the multilingual sign-to-\nspoken model under high-resource data condition.\nFSW decoding strategies\nSignWriting utter-\nances are parsed into a factored FSW representation\n(§3.3, §3.4) and are used for encoding successfully,\nyet it is not obvious how to best decode to FSW.\nWe try the following strategies: (a) predicting ev-\nerything (including positional numbers) as target\ntokens all in one long target sequence, inspired by\nChen et al. (2022); (b) predicting symbols only (as\na comparative experiment); (c) predicting symbols\nwith positional numbers as target factors.\nDuring decoding in the test phase, we apply\nbeam search only for the main target token pre-\ndiction. Target factors do not participate in beam\nsearch, i.e., each target factor prediction is the\nargmax of the corresponding output layer distri-\nbution. We shift target factors to the right by 1 to\ncondition their prediction on the previously gener-\nated target symbol.\nEvaluation of FSW output\nDue to variations of\nSignWriting symbols based on different orienta-\ntions that do not change meaning (Figure 7), evalu-\nating FSW output only at the token (symbol) level\nis not sufﬁcient. Therefore, we evaluate the out-\nput symbols (e.g., line 4 in Listing 1) not only\nwith BLEU, but also chrF2++, which captures both\nword-level and character-level statistics. Addition-\nally, we evaluate the output positional numbers by\nmean absolute error (MAE) to measure the dis-\ntance between predicted positional numbers and\nthe ones from the FSW reference (e.g., lines 5 and\n6 in Listing 1). Let x be the predicted sequence of\npositional numbers and y be the gold sequence:\nMAE(x, y) = 1\n|x|\n|x|\nX\ni=1\n|xi −yi|\nmodel\nBLEU\nchrF2\nE1\nbaseline (lowercase training and test data)\n22.5\n-\nE2\nE1 + dictionary data\n25.2\n-\nE3\nE2 + BPE\n27.0\n46.2\nE4\nE3 + x,y as factors\n27.5\n46.5\nE5\nE4 + symbol core as source + row, col as factors\n23.1\n41.2\nE6\nE4 + relative x, y as factors\n28.1\n47.5\nE7\nE6 + aggressive dropout + tied softmax\n31.4\n52.0\nE8\nE7 + symbol core, row, column as factors\n32.0\n52.7\nE9\nE8 + remove lowercasing\n30.8\n51.2\nE10\nE9 + smaller BPE vocab 2000 to 1000\n29.5\n50.8\nTable 3: Translation quality of ASL→en-us bilingual models. Note that E1 to E8 are trained and evaluated with all\nspoken language data lowercased, while from E9 to all later experiments we remove the lowercasing, so we expect\na little performance drop for the later experiments. We introduce chrF2 as an evaluation metric starting from E3.\nlanguage\nmetrics\n4 language pairs (100k)\n21 language pairs (170k)\nen-us (40k)\nBLEU\n29.5\n25.0\nchrF2\n49.8\n47.0\ntop-1\n0.37\n0.33\ntop-5\n0.52\n0.45\nen-sg (1k)\ntop-1\n-\n0.20\ntop-5\n-\n0.27\npt-br (40k)\nBLEU\n23.8\n6.4\nchrF2\n44.3\n17.5\ntop-1\n0.12\n0.09\ntop-5\n0.17\n0.15\nmt-mt (4k)\nBLEU\n-\n10.1\nchrF2\n-\n29.8\ntop-1\n-\n0.05\ntop-5\n-\n0.05\nde-de (20k)\ntop-1\n0.22\n0.15\ntop-5\n0.31\n0.27\nde-ch (4k)\ntop-1\n-\n0.04\ntop-5\n-\n0.06\nfr-ca (10k)\ntop-1\n0.04\n0.07\ntop-5\n0.08\n0.10\nfr-fr (1k)\ntop-1\n-\n0.16\ntop-5\n-\n0.24\nfr-ch (8k)\ntop-1\n-\n0.07\ntop-5\n-\n0.09\nTable 4: Translation quality of multilingual sign-to-spoken models (partial results on the most frequent languages).\nLanguages without sentence pairs are only evaluated by top-n accuracy. Empty cells mean that a language pair is\nnot supported by the model. In the parentheses are the rough numbers of samples.\nmodel\nBLEU\nchrF2++\nMAE x\nMAE y\nE1\n2symbol+numbers\n6.6\n23.1\n-\n-\nE2\n2symbol\n25.6\n44.2\n-\n-\nE3\n2symbol+factors (w=1)\n19.9\n39.1\n46.5\n52.6\nE4\n2symbol+factors (w=0.5)\n21.9\n40.8\n46.8\n52.7\nE5\n2symbol+factors (w=0.2)\n22.9\n42.0\n47.4\n53.0\nE6\n2symbol+factors (w=0.1)\n22.0\n41.7\n46.4\n52.2\nE7\n2symbol+factors (w=0.01)\n21.0\n40.9\n48.4\n58.3\nTable 5: Translation quality of multilingual spoken-to-sign models. Evaluated in BLEU (on symbol, higher is\nbetter), chrF2++ (on symbol, higher is better), and MAE (on positional numbers, lower is better). w denotes the\nweight between each factor’s loss and the main target loss.\nlanguage\nBLEU (on symbols)\nchrF2++ (on symbols)\nen-us (40k)\n35.7\n58.4\npt-br (40k)\n1.9\n14.9\nde-de (20k)\n17.3\n43.2\nfr-ca (10k)\n5.3\n19.1\nTable 6: Translation quality of multilingual spoken-to-sign model (w=0.1) per language. In the parentheses are the\nrough numbers of samples per language.\nwhere if the predicted and gold sequences do not\nhave the same length, they are padded with zeros.\nTable 5 shows the results of evaluation on the\ntest set. Table 6 shows the results of multilingual\nevaluation on E6 (w=0.1) of Table 5.\n5\nDiscussion\n5.1\nEffect of adding dictionaries, BPE, and\nlow-resource optimizations\nAs shown in Table 3, enlarging the sentence-level\ntraining data (15k sentence pairs) with 25k dic-\ntionary pairs improves the translation quality by\n2.7 BLEU (E1 vs. E2). Likewise, applying BPE\nsegmentation to the spoken language side also im-\nproves translations by 1.8 BLEU (E2 vs. E3).\nWe evaluate several low-resource “tricks” (Sen-\nnrich and Zhang, 2019) including aggressive\ndropout and weight tying9. These low-resource\noptimizations borrowed from spoken language MT\nprove to be effective for sign language translation\nas well, as they result in an improvement of 3.3\nBLEU and 4.5 chrF2 (E6 vs. E7 in Table 3).\n9The tying is only between the target embedding and the\nsoftmax output matrix since the source and target languages\nare of a very different nature and therefore cannot be tied.\n5.2\nUtilizing positional numbers\nIn earlier sections we introduce novel methods to\nparse and factorize FSW (§3.3, §3.4). However,\nfrom a model training perspective it is unclear how\nto best utilize additional factors such as positional\nnumbers. In E4, E5, E6, and E8 of Table 3, we\nexplore different ways of including factors.\nWe ﬁnd that the best strategy is explicitly adding\nall additional information (x, y, relative x, rela-\ntive y, symbol core, column number, row num-\nber) as source factors while keeping symbols as\nthe primary source tokens. This strategy achieves\nthe state-of-the-art performance of 32.0 BLEU and\n52.7 chrF2 in E8.\n5.3\nGenerating positional numbers\nWe explore different ways of generating positional\nnumbers in Table 5. As a ﬁrst attempt, we treat\nthem as normal target tokens in E1, which results in\npoor performance and overly long target sequences,\nand long beam search decoding time.\nIn all subsequent experiments, we treat posi-\ntional numbers as target factors and generally\nachieve over 20 BLEU (evaluating on symbols).\nIn E2, we translate only the symbols as a baseline.\nThen we also try translating with target factors and\nvarying the weights between factors and the pri-\nmary target, i.e., symbols. Finally, we observe that\nE6 (w=0.1) leads to the best trade-off between gen-\nerating symbols and positional numbers.\n5.4\nMultilingual performance\nWe discuss multilingual performance mainly based\non Table 4. Generally speaking, the more resources\na language has in the multilingual model, the bet-\nter its performance (Zhou et al., 2021). The two\ntarget languages most frequent in the training data–\nAmerican English (en-us, 40k) and Brazilian Por-\ntuguese (pt-br, 40k)–have the highest translation\nquality.\nMultilingual transfer effects\nWe observe exam-\nples of both positive and negative multilingual\ntransfer. Evidence shows that a relatively high-\nresource language can help a related low-resource\nlanguage. For instance, the performance of Singa-\nporean English (en-sg, 1k) is likely improved by\nAmerican English (en-us, 40k), which is almost as\ngood as Standard German (de-de, 20k).\nThe comparable bilingual en-us model (E9 in\nTable 3) outperforms en-us in our multilingual sign-\nto-spoken model in Table 4 by 1.3 BLEU and 1.4\nchrF2. However, when extending the training data\nfrom 4 to 21 language pairs, we observe severe\ndegradations: en-us drops by 4.5 BLEU and 2.8\nchrF2; Brazilian Portuguese (pt-br) drops by 17.4\nBLEU and 26.8 chrF2.\nSuch ﬁndings are in line with previous work\non highly multilingual translation systems. For\nexample, Aharoni et al. (2019) ﬁnds that average\nper-language performance drops when the number\nof languages increases. We conclude that Sign-\nWriting translation suffers from a similar curse of\nmultilinguality.\n5.5\nSide-by-side SignWriting example\nFinally, to gain intuition for how well the transla-\ntion model signs, we give a side-by-side example\nof SignWriting graphics. We compare the refer-\nence and model prediction of an ASL utterance\ncorresponding to an American English utterance\nfrom the Bible corpus, shown in Figure 5.\nWe ask an ASL user proﬁcient in SignWriting\nfor a translation of the predicted SignWriting back\nto English to assess the quality of the prediction.\nSimilar patterns appear in both: in the beginning,\nthe model signs “Verse 41” in the same way as in\nthe reference; the graphics in the top parts of all\nthe columns are consistent; and we see correct sym-\nFigure 5: Side-by-side SignWriting example.\nASL\ntranslation of the English sentence “Verse 41. He gave\nher his hand and helped her up. Then he called in the\nwidows and all the believers, and he presented her to\nthem alive.” Separated by the vertical bold (light blue)\nline, the left is the gold sentence, and the predicted sen-\ntence is on the right. The predicted sentence translated\nback to English is “Verse 41. His hand he gives her\nhand. Then he helped up, all believers he warned: he\nput there.”\nbols sometimes predicted with slightly different\npositions.\nMore translation examples can be seen in Ap-\npendix D.\n6\nConclusion\nThis work explores building bilingual and multi-\nlingual translation systems between spoken and\nsigned languages. Instead of representing sign lan-\nguage as videos (or as continuous features derived\nfrom videos) common in previous research, we pro-\npose to represent sign language in SignWriting, a\nsign language writing system. We argue that using\na written form is more amenable to well-established\nNLP techniques.\nHowever, encoding or decoding SignWriting in\nan MT system requires specialized tools. There-\nfore, we propose novel methods to parse, factor-\nize, decode, and evaluate SignWriting sequences.\nOur factorization technique divides SignWriting se-\nquences into meaningful units such as sign symbols\nand positional numbers. The factors are then en-\ncoded or decoded by a factored Transformer model.\nAs a result, we achieve over 30 BLEU in the\nbilingual setting and over 20 BLEU for some high-\nresource language pairs in both directions in the\nmultilingual setting.\nUsing SignWriting as an intermediate representa-\ntion enables us to reuse tools (e.g., evaluation met-\nrics) from spoken language translation. We also\nobserve striking similarities to spoken language\nMT in the experiments themselves. For example,\nlow-resource optimizations have a similar impact,\nand multilingual models exhibit similar transfer\neffects. These ﬁndings validate our use of an inter-\nmediate text representation for signed languages to\ninclude them in NLP research.\n7\nLimitations\n7.1\nA word on top-n accuracy\nIn the translation of dictionary data, if a dictionary\nentry has been seen during training, assuming the\nmodel has enough capacity, it should memorize and\npredict it. However, evaluating the translation is\ntricky, so in §4.2 we resort to using top-n accuracy.\nParadoxically, high top-n accuracy on the test set\ndoes not guarantee good generalization and might\nbe associated with overlap between the training and\ntest sets. Conversely, claiming a model is terrible\nwhen it performs poorly on the test set is unjustiﬁed,\nas there might be no overlap between the training\nand test sets. If a model has seen all the words from\na language, it should perform well on whatever\ndictionary test set. However, this is not the case in\nour low-resource setting.\n7.2\nFingerspelling tokenization\nFingerspelling (Battison, 1978; Wilcox, 1992;\nBrentari and Padden, 2001) is an interesting linguis-\ntic phenomenon where a signed language geograph-\nically coexists with a spoken language. For words\nwith no associated signs (e.g., names of people,\nlocations, organizations, etc.), sign language users\nborrow a word of a spoken language by spelling it\nletter-by-letter with predeﬁned signs for the letters\nof the alphabet of that language. The ﬁngerspelling\n(manual) alphabet of a sign language draws on a\nclosed set of hand shapes, which are supported by\nSignWriting.\nAs ﬁngerspelling is usually applied on a charac-\nter level (rarely extending to the level of multiple\ncharacters, such as “CH” or “SCH” for the ﬁnger\nalphabet of Swiss German Sign Language), the way\nBPE segmentation works (on subword level) does\nnot apply perfectly. However, if we could detect ﬁn-\ngerspelling during the segmentation/tokenization\nprocess, then force ﬁngerspelled words to be split\nletter-by-letter, our models should be able to learn\nbetter the mappings between ﬁngerspelling signs\nand spoken language letters.\n7.3\nTowards better multilingual models\nAs shown by Table 2, the data we use to train\nour models only contains many sentence pairs for\nAmerican English and American Sign Language.\nFor other language pairs, we train mainly on dictio-\nnary data.\nAt the time of writing, we ﬁnd a multilingual par-\nallel corpus created from translations of the Bible10\n(Christodoulopoulos and Steedman, 2014), which,\nif aligned correctly, can be used to translate the\n∼15k American Sign Language biblical text to an-\nother 100+ spoken languages. We believe we could\ntrain better multilingual translation models (at least\non the spoken language side) based on them.\n7.4\nRegression objective for positional\nnumbers\nIn our experiments, positional numbers are treated\nas target factors (§5.3), contributing cross-entropy\nloss to the training process. However, we are aware\nthat the positional numbers are, by nature, numeric\nvalues, so a regression objective/loss would pos-\nsibly work better than the current cross-entropy\nloss, as it better reﬂects the numeric relationship\nbetween positional numeric values.\nAs for now, the target factor function we use\nis only implemented with a classiﬁcation objec-\ntive (cross-entropy loss). We envision that custom\nimplementation of the regression objective might\nimprove translation quality in this scenario.\n7.5\nPossibly ﬂawed positional number\nevaluation\nWe note that using MAE for evaluating positional\nnumbers (§4.3) is possibly ﬂawed because the pre-\ndicted symbol sequences can deviate from the gold\nsymbol sequences. If this is the case, making a\ntoken-by-token comparison on the positional num-\nbers is meaningless, as even the sequence length\ncan mismatch.\n7.6\nAdvanced SignWriting evaluation\nFinally, we call for advanced and novel methods of\nSignWriting evaluation, considering its differences\nfrom spoken languages.\nIn our experiments, we separate the evaluation\nof FSW symbols and positional numbers. For sym-\nbols, we borrow BLEU and chrF2/chrF2++ from\nspoken language evaluation since FSW symbols\nare the basic graphemes in SignWriting that show\n10https://github.com/christos-c/bible-corpus\nmany similar linguistic features as spoken language\nwords. For positional numbers, MAE is used, and\nits limitation is discussed in §7.5.\nFrom a broader perspective, FSW is merely a lin-\nearized speciﬁcation of SignWriting, which means\nwe can also evaluate on the original graphical form,\nas we do manually in §5.5. Moreover, we can\nexploit CV techniques to do an automatic com-\nparative evaluation between predicted SignWriting\ngraphics and gold SignWriting graphics.\nIdeally, a cascading evaluation method is applied\nto SignWriting: we ﬁrst evaluate the overall graph-\nics of the signs, then the symbols within the signs,\nthen the position of the symbols, then the factorized\nrepresentation of the symbols. Finally, a thorough\nhuman evaluation is needed to gain better insight.\nNote on reproducibility\nWe will release the source code and documentation\nto train our models, an API server with the trained\nmodels, and a demo Web application. This will\nallow others to see and consistently reproduce our\nresults with minimal changes. We encourage the\ncommunity to attempt to reproduce our results and\npublish the results.\nAcknowledgements\nThis work is funded by the following projects:\nEASIER (Grant agreement number 101016982)\nand IICT (Grant agreement number PFFS-21-47).\nWe are grateful for their support. We also thank\nRico Sennrich for his suggestions.\nReferences\nNikolaos M. Adaloglou, Theocharis Chatzis, Ilias Pa-\npastratis, Andreas Stergioulas, Georgios Th Pa-\npadopoulos, Vassia Zacharopoulou, George Xy-\ndopoulos, Klimis Antzakas, Dimitris Papazachar-\niou, and Petros Daras. 2021.\nA comprehensive\nstudy on deep learning-based methods for sign lan-\nguage recognition. IEEE Transactions on Multime-\ndia, page 1–1.\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 3874–\n3884.\nFarhad Akhbardeh, Arkady Arkhangorodsky, Mag-\ndalena Biesialska, Ondˇrej Bojar, Rajen Chatter-\njee,\nVishrav Chaudhary,\nMarta R. Costa-jussa,\nCristina España-Bonet, Angela Fan, Christian Fe-\ndermann, Markus Freitag, Yvette Graham, Ro-\nman Grundkiewicz, Barry Haddow, Leonie Harter,\nKenneth Heaﬁeld, Christopher Homan, Matthias\nHuck, Kwabena Amponsah-Kaakyire, Jungo Ka-\nsai, Daniel Khashabi, Kevin Knight, Tom Kocmi,\nPhilipp Koehn, Nicholas Lourie, Christof Monz,\nMakoto Morishita, Masaaki Nagata, Ajay Nagesh,\nToshiaki Nakazawa, Matteo Negri, Santanu Pal, Al-\nlahsera Auguste Tapo, Marco Turchi, Valentin Vy-\ndrin, and Marcos Zampieri. 2021. Findings of the\n2021 conference on machine translation (WMT21).\nIn Proceedings of the Sixth Conference on Machine\nTranslation, pages 1–88, Online. Association for\nComputational Linguistics.\nRobbin Battison. 1978. Lexical borrowing in Ameri-\ncan sign language. ERIC, Linstok Press, Inc., Silver\nSpring, Maryland 20901.\nMark Borg and Kenneth P Camilleri. 2019. Sign lan-\nguage detection \"in the wild\" with recurrent neu-\nral networks. In 2019 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 1637–1641. IEEE.\nYosra Bouzid and Mohamed Jemni. 2013. An avatar\nbased approach for automatically interpreting a sign\nlanguage notation.\nIn 2013 IEEE 13th Interna-\ntional Conference on Advanced Learning Technolo-\ngies, pages 92–94.\nDanielle Bragg, Oscar Koller, Mary Bellard, Lar-\nwan Berke, Patrick Boudreault, Annelies Braffort,\nNaomi Caselli, Matt Huenerfauth, Hernisa Ka-\ncorri, Tessa Verhoef, Christian Vogler, and Mered-\nith Ringel Morris. 2019. Sign language recognition,\ngeneration, and translation: An interdisciplinary per-\nspective.\nIn The 21st International ACM SIGAC-\nCESS Conference on Computers and Accessibility,\nASSETS ’19, page 16–31. Association for Comput-\ning Machinery.\nDiane Brentari and Carol Padden. 2001. A language\nwith multiple origins: Native and foreign vocabu-\nlary in american sign language. Foreign vocabulary\nin sign language: A cross-linguistic investigation of\nword formation, pages 87–119.\nHannah Bull, Michèle Gouiffès, and Annelies Braffort.\n2020.\nAutomatic segmentation of sign language\ninto subtitle-units. In European Conference on Com-\nputer Vision, pages 186–198. Springer.\nNecati Cihan Camgöz, Simon Hadﬁeld, Oscar Koller,\nHermann Ney, and Richard Bowden. 2018.\nNeu-\nral sign language translation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 7784–7793.\nNecati Cihan Camgöz, Oscar Koller, Simon Hadﬁeld,\nand Richard Bowden. 2020a. Multi-channel trans-\nformers for multi-articulatory sign language transla-\ntion. In European Conference on Computer Vision,\npages 301–319.\nNecati Cihan Camgöz, Oscar Koller, Simon Hadﬁeld,\nand Richard Bowden. 2020b. Sign language trans-\nformers: Joint end-to-end sign language recognition\nand translation.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 10023–10033.\nZhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei,\nand Yaser Sheikh. 2021.\nOpenpose:\nRealtime\nmulti-person 2d pose estimation using part afﬁn-\nity ﬁelds. IEEE Trans. Pattern Anal. Mach. Intell.,\n43(1):172–186.\nZhe Cao, Tomas Simon, Shih-En Wei, and Yaser\nSheikh. 2017. Realtime multi-person 2d pose esti-\nmation using part afﬁnity ﬁelds. In CVPR.\nStanley F. Chen and Joshua Goodman. 1996. An em-\npirical study of smoothing techniques for language\nmodeling. In 34th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 310–318.\nTing Chen, Saurabh Saxena, Lala Li, David J. Fleet,\nand Geoffrey Hinton. 2022. Pix2seq: A language\nmodeling framework for object detection. In Inter-\nnational Conference on Learning Representations.\nChristos Christodoulopoulos and Mark Steedman.\n2014.\nA massively parallel corpus: the bible in\n100 languages.\nLanguage Resources and Evalua-\ntion, 49:1–21.\nIva Farag and Heike Brock. 2019. Learning motion dis-\nﬂuencies for automatic sign language segmentation.\nIn 2019 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n7360–7364. IEEE.\nMercedes Garcia-Martinez, Loïc Barrault, and Fethi\nBougares. 2016.\nFactored Neural Machine Trans-\nlation Architectures. In International Workshop on\nSpoken Language Translation (IWSLT’16).\nBinyam Gebrekidan Gebre, Peter Wittenburg, and Tom\nHeskes. 2013. Automatic sign language identiﬁca-\ntion. In 2013 IEEE International Conference on Im-\nage Processing, pages 2626–2630. IEEE.\nRıza Alp Güler, Natalia Neverova, and Iasonas Kokki-\nnos. 2018. Densepose: Dense human pose estima-\ntion in the wild. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition,\npages 7297–7306.\nFelix Hieber, Tobias Domhan, Michael Denkowski,\nand David Vilar. 2020. Sockeye 2: A toolkit for neu-\nral machine translation. In Proceedings of the 22nd\nAnnual Conference of the European Association for\nMachine Translation, pages 457–458. European As-\nsociation for Machine Translation.\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2017. Google’s\nmultilingual neural machine translation system: En-\nabling zero-shot translation. Transactions of the As-\nsociation for Computational Linguistics, 5:339–351.\nTrevor Johnston and Louise De Beuzeville. 2016. Aus-\nlan corpus annotation guidelines. Auslan Corpus.\nSang-Ki Ko, Chang Jo Kim, Hyedong Jung, and\nChoongsang Cho. 2019. Neural sign language trans-\nlation based on human keypoint estimation. Applied\nSciences, 9(13):2683.\nPhilipp Koehn and Hieu Hoang. 2007. Factored trans-\nlation models.\nIn Proceedings of the 2007 Joint\nConference on Empirical Methods in Natural Lan-\nguage Processing and Computational Natural Lan-\nguage Learning (EMNLP-CoNLL), pages 868–876.\nAssociation for Computational Linguistics.\nReiner Konrad, Thomas Hanke, Gabriele Langer, Su-\nsanne König, Lutz König, Rie Nishio, and Anja Re-\ngen. 2018. Public dgs corpus: Annotation conven-\ntions. Technical report, Hamburg University.\nJulia Kreutzer, Jasmijn Bastings, and Stefan Riezler.\n2019.\nJoey NMT: A minimalist NMT toolkit for\nnovices. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing (EMNLP-IJCNLP): Sys-\ntem Demonstrations, pages 109–114. Association\nfor Computational Linguistics.\nJohanna Mesch and Lars Wallin. 2012. From meaning\nto signs and back: Lexicography and the swedish\nsign language corpus.\nIn Proceedings of the 5th\nWorkshop on the Representation and Processing of\nSign Languages: Interactions between Corpus and\nLexicon [Language Resources and Evaluation Con-\nference (LREC)], pages 123–126.\nCaio DD Monteiro, Christy Maria Mathew, Ricardo\nGutierrez-Osuna, and Frank Shipman. 2016. Detect-\ning and identifying sign languages through visual\nfeatures. In 2016 IEEE International Symposium on\nMultimedia (ISM), pages 287–290. IEEE.\nSara Morrissey. 2011. Assessing three representation\nmethods for sign language machine translation and\nevaluation. In Proceedings of the 15th annual meet-\ning of the European Association for Machine Trans-\nlation (EAMT 2011), Leuven, Belgium, pages 137–\n144.\nAmit\nMoryossef\nand\nYoav\nGoldberg.\n2021.\nSign\nLanguage\nProcessing.\nhttps:\n//sign-language-processing.github.io/.\nAmit Moryossef and Mathias Müller. 2021.\nSign\nlanguage\ndatasets.\nhttps://github.com/\nsign-language-processing/datasets.\nAmit Moryossef, Ioannis Tsochantaridis, Roee Yosef\nAharoni, Sarah Ebling, and Srini Narayanan. 2020.\nReal-time sign-language detection using human\npose estimation.\nIn SLRTP 2020: The Sign Lan-\nguage Recognition, Translation & Production Work-\nshop.\nMathias Müller, Sarah Ebling, Eleftherios Avramidis,\nAlessia Battisti, Michèle Berger, Richard Bowden,\nAnnelies Braffort, Necati Cihan Camgöz, Cristina\nEspaña-Bonet, Roman Grundkiewicz, Zifan Jiang,\nOscar Koller, Amit Moryossef, Regula Perrollaz,\nSabine Reinhard, Annette Rios, Dimitar Shteri-\nonov, Sandra Sidler-Miserez, Katja Tissi, and Davy\nVan Landuyt. 2022a. Findings of the WMT 2022\nshared task on sign language translation. In Proceed-\nings of the Seventh Conference on Machine Transla-\ntion, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nMathias Müller, Zifan Jiang, Amit Moryossef, Annette\nRios, and Sarah Ebling. 2022b. Considerations for\nmeaningful sign language machine translation based\non glosses. arXiv preprint arXiv:2211.15464.\nAchraf Othman and Mohamed Jemni. 2012. English-\nasl gloss parallel corpus 2012: Aslg-pc12. In 5th\nWorkshop on the Representation and Processing of\nSign Languages: Interactions between Corpus and\nLexicon LREC.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting on Association for Computa-\ntional Linguistics, ACL ’02, page 311–318.\nMaja Popovi´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395. Association for Computational Lin-\nguistics.\nSiegmund Prillwitz and Heiko Zienert. 1990. Hamburg\nnotation system for sign language: Development of\na sign writing with computer application. In Current\ntrends in European Sign Language Research. Pro-\nceedings of the 3rd European Congress on Sign Lan-\nguage Research, pages 355–379.\nMuhammad Sanaullah, Babar Ahmad, Muhammad\nKashif, Tauqeer Safdar, Mehdi Hassan, Mohd Hilmi\nHasan, and NorShakirah Aziz. 2021.\nA real-time\nautomatic translation of text to sign language. Com-\nputers, Materials and Continua, 70:2471–2488.\nPinar Santemiz, Oya Aran, Murat Saraclar, and Lale\nAkarun. 2009. Automatic sign segmentation from\ncontinuous signing via multiple sequence align-\nment. In 2009 IEEE 12th International Conference\non Computer Vision Workshops, ICCV Workshops,\npages 2001–2008. IEEE.\nBen Saunders, Necati Cihan Camgöz, and Richard\nBowden. 2020a.\nAdversarial training for multi-\nchannel sign language production.\nIn The 31st\nBritish Machine Vision Virtual Conference. British\nMachine Vision Association.\nBen Saunders, Necati Cihan Camgöz, and Richard\nBowden. 2020b.\nEverybody sign now: Translat-\ning spoken language to photo realistic sign language\nvideo. arXiv preprint arXiv:2011.09846.\nBen Saunders, Necati Cihan Camgöz, and Richard\nBowden. 2020c. Progressive transformers for end-\nto-end sign language production. In European Con-\nference on Computer Vision, pages 687–705.\nRico Sennrich. 2012.\nPerplexity minimization for\ntranslation model domain adaptation in statistical\nmachine translation. In Proceedings of the 13th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics, pages 539–549.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016.\nNeural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725.\nRico Sennrich and Biao Zhang. 2019. Revisiting low-\nresource neural machine translation: A case study.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 211–\n221.\nSteve Slevinski. 2021. Formal SignWriting. Internet-\nDraft draft-slevinski-formal-signwriting-08, Inter-\nnet Engineering Task Force.\nD. Stiehl, L. Addams, L. S. Oliveira, C. Guimarães, and\nA. S. Britto. 2015. Towards a signwriting recogni-\ntion system. In 2015 13th International Conference\non Document Analysis and Recognition (ICDAR),\npages 26–30.\nValerie Sutton. 1990. Lessons in sign writing. Sign-\nWriting.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nHarry Walsh, Ben Saunders, and Richard Bowden.\n2022. Changing the representation: Examining lan-\nguage representation for neural sign language pro-\nduction. In LREC 2022.\nSherman Wilcox. 1992.\nThe phonetics of ﬁnger-\nspelling, volume 4. John Benjamins Publishing.\nQinkun Xiao, Minying Qin, and Yuting Yin. 2020.\nSkeleton-based chinese sign language recognition\nand generation for bidirectional communication be-\ntween deaf and hearing people. Neural Networks,\n125:41–55.\nKayo Yin, Amit Moryossef, Julie Hochgesang, Yoav\nGoldberg, and Malihe Alikhani. 2021.\nIncluding\nsigned languages in natural language processing.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n7347–7360, Online. Association for Computational\nLinguistics.\nKayo Yin and Jesse Read. 2020. Better sign language\ntranslation with stmc-transformer. In Proceedings of\nthe 28th International Conference on Computational\nLinguistics, pages 5975–5989.\nJan Zelinka and Jakub Kanis. 2020. Neural sign lan-\nguage synthesis: Words are our glosses.\nIn The\nIEEE Winter Conference on Applications of Com-\nputer Vision, pages 3395–3403.\nLiwei Zhao, Karin Kipper, William Schuler, Christian\nVogler, Norman Badler, and Martha Palmer. 2000. A\nmachine translation system from english to american\nsign language. In Conference of the Association for\nMachine Translation in the Americas, pages 54–67.\nSpringer.\nChunting Zhou,\nDaniel Levy,\nXian Li,\nMarjan\nGhazvininejad, and Graham Neubig. 2021. Distri-\nbutionally robust multilingual machine translation.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5664–5674. Association for Computational Linguis-\ntics.\nA\nExtended introduction to SignWriting\nSignWriting (Sutton, 1990) is a sign language writ-\ning system developed by Valerie Sutton11 and cur-\nrently managed by Steve Slevinski12. SignWriting\nis very featural and visually iconic, both in:\n• the shapes of the symbols, which are abstract\npictures of hand shapes (Figure 6), orientation\n(Figure 7), body locations, facial expressions,\ncontacts, and movement;\n• the symbols’ two-dimensional spatial arrange-\nment in an invisible “sign box” (Figure 8).\nOutside each sign, the script is written linearly to\nreﬂect the temporal order of signs. Signs are mostly\nwritten vertically, arranged from top to bottom\nwithin each column, interspersed with special punc-\ntuation symbols (horizontal lines), and the columns\nprogress left to right across the page. Within each\ncolumn, signs may be vertically aligned to the cen-\nter or shifted left or right to indicate body shifts.\nA.1\nFormal SignWriting in ASCII (FSW)\nIn 2012, Formal SignWriting in ASCII (FSW) spec-\niﬁcation (Slevinski, 2021) was released and docu-\nmented in an Internet Draft submitted to the IETF.\nThe design of FSW is computerized so that it can\nbe recognized and processed by programs. While\nsigned languages are natural languages, FSW is a\nformal language handy in mathematics, computer\nscience, and linguistics.\nAlthough SignWriting is two-dimensional, FSW\nis written linearly like spoken languages. Each sign\nis written as ﬁrst a box marker, then a sequence of\nsymbols, and their relative position, as illustrated\nby Figure 3.\nA.2\nSignWriting in Unicode (SWU)\nIn 2017, SignWriting in Unicode (SWU) speciﬁca-\ntion (Slevinski, 2021) was released, making Sign-\nWriting included in the Unicode Standard. The\nUnicode block for SWU is U+1D800 - U+1DAAF.\nAs illustrated in Figure 3, SWU is also written\nlinearly. FSW and SWU are isomorphic and inter-\nchangeable, and both faithfully encode the com-\nplete information of SignWriting.\n11Valerie Sutton:\nhttps://en.wikipedia.org/wiki/\nValerie_Sutton\n12Steve Slevinski: https://steveslevinski.me/\nFigure 6: Hand shapes and their equivalents in Sign-\nWriting.\nS100\n00\n10\n20\n30\n40\n50\n00\n􀀁󰀁\n􀀑󰀑\n󰀡\n􀀱󰀱\n􀁁󰁁\n󰁑\n01\n􀀂󰀂\n􀀒󰀒\n󰀢\n􀀲󰀲\n􀁂󰁂\n󰁒\n02\n􀀃\n󰀃\n􀀓\n󰀓\n󰀣\n􀀳\n󰀳\n􀁃\n󰁃\n󰁓\n03\n􀀄\n󰀄\n􀀔\n󰀔\n󰀤\n􀀴\n󰀴\n􀁄\n󰁄\n󰁔\n04\n􀀅󰀅\n􀀕󰀕\n󰀥\n􀀵󰀵\n􀁅󰁅\n󰁕\n05\n􀀆󰀆\n􀀖󰀖\n󰀦\n􀀶󰀶\n􀁆󰁆\n󰁖\n06\n􀀇󰀇\n􀀗󰀗\n󰀧\n􀀷󰀷\n􀁇󰁇\n󰁗\n07\n􀀈󰀈\n􀀘󰀘\n󰀨\n􀀸󰀸\n􀁈󰁈\n󰁘\n08\n􀀉󰀉\n􀀙󰀙\n󰀩\n􀀹󰀹\n􀁉󰁉\n󰁙\n09\n􀀊󰀊\n􀀚󰀚\n󰀪\n􀀺󰀺\n􀁊󰁊\n󰁚\n0a\n􀀋󰀋\n􀀛󰀛\n󰀫\n􀀻󰀻\n􀁋󰁋\n󰁛\n0b\n􀀌󰀌\n􀀜󰀜\n󰀬\n􀀼󰀼\n􀁌󰁌\n󰁜\nFigure 7: Orientation of a symbol in SignWriting in\n3D space. Each row applies a rotation of the palm in\na 2D space vertical to the ground. Each column ap-\nplies a rotation of the palm in a 2D space parallel to\nthe ground. This can be seen as a factorization of the\nsymbol S100xx to its core S100 plus row and column\nnumbers.\n󾷇\n􋛩󻛩\n􀀒󰀒\n󻚥\n􀀚󰀚\n􂇈󲇈󲇢\n􋎥󻎥\n􋎵󻎵\n􆙡󶙡\n􀀙󰀙􅨑󵨑\n󶉁\n󿌁\n􀕁󰕁\n􂌢󲌢􂇷󲇷􆙡󶙡󸗦\n􏊡\n󿊡\n󰀡\n󼏁􀀒󰀒\n󹁩\n􀕉󰕉\n􀕁󰕁\n󼀇󶇡\n󶇡󸩡\n􈩽\n󸩽󼀃\n󻺁\n􏊡\n󿊡\n󰕡\n󼏁􀀒󰀒\n󹁩\n􂇻󲇻\n􆕁󶕁󰂁\n󸟃\n􀀁󰀁\n󰀡􋎥󻎥\n􀀓\n󰀓\n􆕁󶕁\n􃛆󳛆\n􆿅󶿅\n󿌁\n󲇢\n􂇈󲇈􂇂󲇂􉳍󹳍\n􋡩󻡩\n􀀒󰀒\n󻠥\n􀀚󰀚\n􆿕󶿕􃧁󳧁\n􃧉󳧉\n􆿅󶿅\n󻸥\n󿌁\n􂇸󲇸\n􂇑󲇑\n􂇙󲇙\n󵡁\n􇀵󷀵\n󸗨\n􇀥󷀥\n󸗨\n􄹸󴹸\n􄾘󴾘\n󸗦\n􀀋󰀋\n􀁂󰁂\n􏊡\n󿊡\n󶅁􇅅󷅅\n󿌁\n󴵡󶅁󳉡\n􏊡\n󿊡\n􅩱󵩱\n󽳡\n􃛋󳛋􃛃󳛃󸙇\n􈙓󸙓\n󶇡\n􏊡\n󿊡\n􇆡󷆡􂋣\n󲋣\n􂋫󲋫􆕁󶕁\n􆄩󶄩\n􀟡󰟡\n􆕁󶕁\n󸟃\n󽩁\n􏊡\n󿊡\n󵊰􅊂󵊂\n󸙆\n􈙖󸙖\n􅑢󵑢\n󵒐\n􃁁󳁁\n􃁉󳁉\n􋡭󻡭󻠩\n󻸡\n󿌁\n󴵡󶅁󳉡\n􆿕󶿕\n􃧁󳧁\n􃧉󳧉\n􆿅󶿅\n󻸥\n􏊡\n󿊡\n􋡩󻡩\n􀀒󰀒\n󻠥\n􀀚󰀚\n􂇇󲇇󵡁\n􉨬󹨬\n􆅱󶅱\n􆅹󶅹􇆥󷆥\n􇆵󷆵\n󼁵\n󿌁\nFigure 8:\nAn example of SignWriting written in\ncolumns, ASL translation of an introduction to Formal\nSignWriting in ASCII. The relative positions of the\nsymbols within the box iconically represent the loca-\ntions of the hands and other parts of the body involved\nin the sign being represented.\nB\nExperimental setup\nWe performed all our experiments with Python\n3.8.11 on an Nvidia Tesla V100 GPU (32GB GPU\nram).\nB.1\n40k sign-to-en-us\nData includes:\n• ∼15k sentence pairs from 3 en-us puddles:\nLiterature US, ASL Bible Books NLT, ASL\nBible Books Shores Deaf Church,\n• ∼25k dictionary pairs from 3 en-us puddles:\nDictionary US, LLCN & SignTyp, ASL Bible\nDictionary,\nwhich leads to:\n• ∼6k source vocabulary size (number of non-\nfactorized symbols),\n• ∼2k target vocabulary size (determined by\nBPE).\nFinal model conﬁguration:\n• 6 layers + 8 heads + 512 embedding size (16\nfor each factor) (0.5 dropout) + 512 hidden\nsize (0.5 dropout) + 2,048 feed forward size\n(0.5 dropout),\n• initial learning rate 0.0001, decrease learning\nrate by a factor of 0.7 every 5 times validation\nscore (BLEU) not improved,\n• batch size 32 sentences, label smoothing 0.2,\nepochs 300,\n• for testing, decoding with a checkpoint with\nthe best validation score, beam size 5, alpha\nfor length penalty 1.\nExperiments were conducted with a custom ver-\nsion of Joey NMT (Kreutzer et al., 2019) to support\nsource factors. Each model (∼47 million parame-\nters) ﬁnished training within 1 day.\nB.2\n100k sign-to-spoken\nData includes:\n• ∼17k sentence pairs from 3 en-us puddles\n(Literature US, ASL Bible Books NLT, ASL\nBible Books Shores Deaf Church) and 1 pt-br\npuddle (Literatura Brasil),\n• ∼83k dictionary pairs from 3 en-us pud-\ndles (Dictionary US, LLCN & SignTyp, ASL\nBible Dictionary), 2 pt-br puddles (Dicionário\nBrasil, Enciclopédia Brasil), 1 de-de puddle\n(Wörterbuch DE) and 1 fr-ca puddle (Diction-\nnaire Quebec),\nwhich leads to:\n• ∼11k source vocabulary size (number of non-\nfactorized symbols),\n• ∼2k target vocabulary size (determined by\nBPE).\nA little change to the previous conﬁguration to\nmake training more efﬁcient:\n• batch size 4,096 tokens.\nExperiments were conducted with a custom ver-\nsion of Joey NMT to support source factors. Each\nmodel (∼50 million parameters) ﬁnished training\nwithin ∼1.5 days and ∼3 days, respectively.\nB.3\n100k spoken-to-sign\nData and model conﬁgurations are the same as\n100k sign-to-spoken, except that we use perplexity\n(Chen and Goodman, 1996; Sennrich, 2012) as\nvalidation score instead of BLEU.\nExperiments were conducted with Sockeye\n(Hieber et al., 2020) for the convenience of ready-\nto-use target factor support. Each model (∼60 mil-\nlion parameters) ﬁnished training within ∼0.5 day.\nC\nData\nFigure 9 visualizes the language pair distribution in SignBank. Table 7 contains an exhaustive list of all 21\nlanguage pairs used in this research. Listing 1 shows an example of FSW parsing and factorization.\nFigure 9: Data distribution (the ﬁrst 30 language pairs).\nlanguage\n#samples\n#puddles\nsentence pairs (>1k)\nen-us (American English)\n43,698\n7\nen-sg (Singaporean English)\n1,136\n2\npt-br (Brazilian Portuguese)\n42,454\n3\nmt-mt (Maltese Maltese)\n4,118\n4\nde-de (German German)\n24,704\n3\nde-ch (Swiss German)\n4,700\n2\nfr-ca (Canadian French)\n11,189\n3\nfr-ch (Swiss French)\n8,806\n3\nfr-be (Belgian French)\n3,439\n1\nfr-fr (French French)\n1,299\n2\nes-es (Spanish Spanish)\n8,806\n2\nes-hn (Honduran Spanish)\n3,399\n1\nes-ni (Nicaraguan Spanish)\n2,150\n2\nes-ar (Argentinian Spanish)\n1,774\n2\nar-tn (Tunisien Arabic)\n4,965\n2\nca-es (Spanish Catalan)\n3,419\n2\nko-kr (Korean Korean)\n1,525\n1\nnl-be (Belgian Flemish)\n8,301\n2\npl-pl (Polish Polish)\n2,130\n2\nsk-sk (Czech Czech)\n5,780\n2\nsl-sl (Slovenian Slovenian)\n3,808\n2\nTable 7: All 21 language pairs (spoken languages with corresponding signed languages).\n1 {\n2\n'fsw': 'M550x535S32a00482x483S15d09455x499S15d01522x497S22114516x484\n3\nS22114456x484S20f00524x522S20f00451x523 ',\n4\n'symbol ': 'M S32a00 S15d09 S15d01 S22114 S22114 S20f00 S20f00 ',\n5\n'feat_x ': '550 482 455 522 516 456 524 451',\n6\n'feat_y ': '535 483 499 497 484 484 522 523',\n7\n'feat_x_rel ': '-1 3 1 5 4 2 6 0',\n8\n'feat_y_rel ': '-1 0 4 3 1 2 5 6',\n9\n'feat_core ': 'M S32a S15d S15d S221 S221 S20f S20f',\n10\n'feat_col ': '-1 0 0 0 1 1 0 0',\n11\n'feat_row ': '-1 0 9 1 4 4 0 0',\n12 }\nListing 1: An example of FSW parsing and factorization.\nD\nMore side-by-side SignWriting examples\nSeparated by the vertical bold (light blue) line, the left is the gold sentence, and the predicted sentence is\non the right.\nFigure 10: ASL translation of the English sentence “Verse 22. Then the apostles and elders together with the whole\nchurch in Jerusalem chose delegates, and they sent them to Antioch of Syria”\nFigure 11: ASL translation of the English sentence “These are what deﬁle you. Eating with unwashed hands will\nnever deﬁle you.”\nFigure 12: German Sign Language translation of the German words “signen, senken, zehn”\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-10-11",
  "updated": "2023-02-23"
}