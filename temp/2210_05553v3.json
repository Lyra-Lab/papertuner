{
  "id": "http://arxiv.org/abs/2210.05553v3",
  "title": "Evaluating Unsupervised Denoising Requires Unsupervised Metrics",
  "authors": [
    "Adria Marcos-Morales",
    "Matan Leibovich",
    "Sreyas Mohan",
    "Joshua Lawrence Vincent",
    "Piyush Haluai",
    "Mai Tan",
    "Peter Crozier",
    "Carlos Fernandez-Granda"
  ],
  "abstract": "Unsupervised denoising is a crucial challenge in real-world imaging\napplications. Unsupervised deep-learning methods have demonstrated impressive\nperformance on benchmarks based on synthetic noise. However, no metrics are\navailable to evaluate these methods in an unsupervised fashion. This is highly\nproblematic for the many practical applications where ground-truth clean images\nare not available. In this work, we propose two novel metrics: the unsupervised\nmean squared error (MSE) and the unsupervised peak signal-to-noise ratio\n(PSNR), which are computed using only noisy data. We provide a theoretical\nanalysis of these metrics, showing that they are asymptotically consistent\nestimators of the supervised MSE and PSNR. Controlled numerical experiments\nwith synthetic noise confirm that they provide accurate approximations in\npractice. We validate our approach on real-world data from two imaging\nmodalities: videos in raw format and transmission electron microscopy. Our\nresults demonstrate that the proposed metrics enable unsupervised evaluation of\ndenoising methods based exclusively on noisy data.",
  "text": "Evaluating Unsupervised Denoising\nRequires Unsupervised Metrics\nAdri`a Marcos Morales 1 2 3 Matan Leibovich 4 Sreyas Mohan 1 Joshua Lawrence Vincent 5 Piyush Haluai 5\nMai Tan 5 Peter Crozier 5 Carlos Fernandez-Granda 1 4\nAbstract\nUnsupervised denoising is a crucial challenge in\nreal-world imaging applications. Unsupervised\ndeep-learning methods have demonstrated impres-\nsive performance on benchmarks based on syn-\nthetic noise. However, no metrics exist to eval-\nuate these methods in an unsupervised fashion.\nThis is highly problematic for the many practi-\ncal applications where ground-truth clean images\nare not available. In this work, we propose two\nnovel metrics: the unsupervised mean squared\nerror (MSE) and the unsupervised peak signal-\nto-noise ratio (PSNR), which are computed us-\ning only noisy data. We provide a theoretical\nanalysis of these metrics, showing that they are\nasymptotically consistent estimators of the super-\nvised MSE and PSNR. Controlled numerical ex-\nperiments with synthetic noise confirm that they\nprovide accurate approximations in practice. We\nvalidate our approach on real-world data from\ntwo imaging modalities: videos in raw format and\ntransmission electron microscopy. Our results\ndemonstrate that the proposed metrics enable un-\nsupervised evaluation of denoising methods based\nexclusively on noisy data.\n1. Introduction\nImage denoising is a fundamental challenge in image and\nsignal processing, as well as a key preprocessing step for\n1Center for Data Science, New York University, New York,\nNY 2Centre de Formaci´o Interdisciplin`aria Superior, Universitat\nPolit`ecnica de Catalunya, Barcelona, Spain 3Radiomics Group,\nVall d’Hebron Institute of Oncology, Vall d’Hebron Barcelona\nHospital Campus, Barcelona, Spain 4Courant Institute of Mathe-\nmatical Sciences, New York University, New York, NY 5School\nfor Engineering of Matter, Transport & Energy, Arizona State Uni-\nversity, Tempe, AZ. Correspondence to: Adri`a Marcos Morales\n<adriamm98@gmail.com>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\ncomputer vision tasks.\nConvolutional neural networks\nachieve state-of-the-art performance for this problem, when\ntrained using databases of clean images corrupted with sim-\nulated noise (Zhang et al., 2017a). However, in real-world\nimaging applications such as microscopy, noiseless ground\ntruth videos are often not available. This has motivated the\ndevelopment of unsupervised denoising approaches that can\nbe trained using only noisy measurements (Lehtinen et al.,\n2018; Xie et al., 2020; Laine et al., 2019; Sheth et al., 2021;\nHuang et al., 2021). These methods have demonstrated\nimpressive performance on natural-image benchmarks, es-\nsentially on par with the supervised state of the art. However,\nto the best of our knowledge, no unsupervised metrics are\ncurrently available to evaluate them using only noisy data.\nReliance on supervised metrics makes it very challenging to\ncreate benchmark datasets using real-world measurements,\nbecause obtaining the ground-truth clean images required by\nthese metrics is often either impossible or very constraining.\nIn practice, clean images are typically estimated through\ntemporal averaging, which suppresses dynamic information\nthat is often crucial in scientific applications. Consequently,\nquantitative evaluation of unsupervised denoising methods\nis currently almost completely dominated by natural image\nbenchmark datasets with simulated noise (Lehtinen et al.,\n2018; Xie et al., 2020; Laine et al., 2019; Sheth et al., 2021;\nHuang et al., 2021), which are not always representative of\nthe signal and noise characteristics that arise in real-world\nimaging applications.\nThe lack of unsupervised metrics also limits the applica-\ntion of unsupervised denoising techniques in practice. In\nthe absence of quantitative metrics, domain scientists must\noften rely on visual inspection to evaluate performance on\nreal measurements. This is particularly restrictive for deep-\nlearning approaches, because it makes it impossible to per-\nform systematic hyperparameter optimization and model\nselection on the data of interest.\nIn this work, we propose two novel unsupervised metrics\nto address these issues: the unsupervised mean-squared er-\nror (uMSE) and the unsupervised peak signal-to-noise ratio\n(uPSNR), which are computed exclusively from noisy data.\nThese metrics build upon existing unsupervised denoising\n1\narXiv:2210.05553v3  [cs.CV]  30 May 2023\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nmethods, which minimize an unsupervised cost function\nequal to the difference between the denoised estimate and\nadditional noisy copies of the signal of interest (Lehtinen\net al., 2018). The uMSE is equal to this cost function mod-\nified with a correction term, which renders it an unbiased\nestimator of the supervised MSE.\nWe provide a theoretical analysis of the uMSE and uPSNR,\nproving that they are asymptotically consistent estimators of\nthe supervised MSE and PSNR respectively. Controlled ex-\nperiments on supervised benchmarks, where the true MSE\nand PSNR can be computed exactly, confirm that the uMSE\nand uPSNR provide accurate approximations. In addition,\nwe validate the metrics on video data in RAW format, con-\ntaminated with real noise that does not follow a known\npredefined model.\nIn order to illustrate the potential impact of the pro-\nposed metrics on imaging applications where no ground-\ntruth is available, we apply them to transmission-electron-\nmicroscopy (TEM) data. Recent advances in direct electron\ndetection systems make it possible for experimentalists to\nacquire highly time-resolved movies of dynamic events at\nframe rates in the kilohertz range (Faruqi & McMullan,\n2018; Ercius et al., 2020), which is critical to advance our\nunderstanding of functional materials. Acquisition at such\nhigh temporal resolution results in severe degradation by\nshot noise. We show that unsupervised methods based on\ndeep learning can be effective in removing this noise, and\nthat our proposed metrics can be used to evaluate their per-\nformance quantitatively using only noisy data.\nTo summarize, our contributions are (1) two novel unsu-\npervised metrics presented in Section 3, (2) a theoretical\nanalysis providing an asymptotic characterization of their\nstatistical properties (Section 4), (3) experiments showing\nthe accuracy of the metrics in a controlled situation where\nground-truth clean images are available (Section 5), (4) val-\nidation on real-world videos in RAW format (Section 6),\nand (5) an application to a real-world electron-microscopy\ndataset, which illustrates the challenges of unsupervised\ndenoising in scientific imaging (Section 7).\nCode to reproduce all computational experiments is avail-\nable at https://github.com/adriamm98/umse\n2. Background and Related work\nUnsupervised denoising The past few years have seen\nground-breaking progress in unsupervised denoising, pio-\nneered by Noise2Noise, a technique where a neural network\nis trained on pairs of noisy images (Lehtinen et al., 2018).\nOur unsupervised metrics are inspired by Noise2Noise,\nwhich optimizes a cost function equal to our proposed\nunsupervised MSE, but without a correction term (which\nis not needed for training models). Subsequent work fo-\ncused on performing unsupervised denoising from single\nimages using variations of the blind-spot method, where\na model is trained to estimate each noisy pixel value us-\ning its neighborhood but not the noisy pixel itself (to avoid\nthe trivial identity solution) (Krull et al., 2019; Laine et al.,\n2019; Batson & Royer, 2019a; Sheth et al., 2021; Xie et al.,\n2020). More recently, Neighbor2Neighbor revisited the\nNoise2Noise method, generating noisy image pairs from a\nsingle noisy image via spatial subsampling (Huang et al.,\n2021), an insight that can also be leveraged in combination\nwith our proposed metrics, as explained in Section B. Our\ncontribution with respect to these methods is a novel un-\nsupervised metric that can be used for evaluation, as it is\ndesigned to be an unbiased and consistent estimator of the\nMSE.\nStein’s unbiased risk estimator (SURE) provides an\nasymptotically unbiased estimator of the MSE for i.i.d.\nGaussian noise (Donoho & Johnstone, 1995). This cost\nfunction has been used for training unsupervised denois-\ners (Metzler et al., 2018; Soltanayev & Chun, 2018; Zhussip\net al., 2019; Mohan et al., 2021). In principle, SURE could\nbe used to compute the MSE for evaluation, but it has certain\nlimitations: (1) a closed form expression of the noise likeli-\nhood is required, including the value of the noise parameters\n(for example, this is not known for the real-world datasets in\nSections 6 and 7), (2) computing SURE requires approximat-\ning the divergence of a denoiser (usually via Monte Carlo\nmethods (Ramani et al., 2008)), which is computationally\nvery expensive. Developing practical unsupervised metrics\nbased on SURE and studying their theoretical properties is\nan interesting direction for future research.\nExisting evaluation approaches In the literature, quanti-\ntative evaluation of unsupervised denoising techniques has\nmostly relied on images and videos corrupted with synthetic\nnoise (Lehtinen et al., 2018; Krull et al., 2019; Laine et al.,\n2019; Batson & Royer, 2019a; Sheth et al., 2021; Xie et al.,\n2020). Recently, a few datasets containing real noisy data\nhave been created (Abdelhamed et al., 2018; Plotz & Roth,\n2017; Xu et al., 2018; Zhang et al., 2019). Evaluation on\nthese datasets is based on supervised MSE and PSNR com-\nputed from estimated clean images obtained by averaging\nmultiple noisy frames. Unfortunately, as a result, the metrics\ncannot capture dynamically-changing features, which are\nof interest in many applied domains. In addition, unless the\nsignal-to-noise ratio is quite high, it is necessary to average\nover a large number of frames to approximate the MSE. For\nexample, as explained in Section D, for an image corrupted\nby additive Gaussian noise with standard deviation σ = 15\nwe need to average > 1500 noisy images to achieve the\nsame approximation accuracy as our proposed approach\n(see Figure 10), which only requires 3 noisy images, and\ncan also be computed from a single noisy image.\n2\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nFigure 1. MSE vs uMSE. The traditional supervised mean squared error (MSE) is computed by comparing the denoised estimate to the\nclean ground truth (left). The proposed unsupervised MSE is computed only from noisy data, via comparison with a noisy reference\ncorresponding to the same ground-truth but corrupted with independent noise (right). A correction term based on two additional noisy\nreferences debiases the estimator.\nNoise-Level Estimation. The correction term in uMSE can\nbe interpreted as an estimate of the noise level, obtained by\ncancelling out the clean signal. In this sense, it is related\nto noise-level estimation methods (Liu et al., 2013; Lebrun\net al., 2015; Arias & Morel, 2018). However, unlike uMSE,\nthese methods typically assume a parametric model for the\nnoise, and are not used for evaluation.\nNo-reference image quality assessment methods evaluate\nthe perceptual quality of an image (Li, 2002; Mittal et al.,\n2012), but not whether it is consistent with an underlying\nground-truth corresponding to the observed noisy measure-\nments, which is the goal of our proposed metrics.\n3. Unsupervised Metrics For Unsupervised\nDenoising\n3.1. The Unsupervised Mean Squared Error\nThe goal of denoising is to estimate a clean signal from noisy\nmeasurements. Let x ∈Rn be a signal or a set of signals\nwith n total entries. We denote the corresponding noisy data\nby y ∈Rn. A denoiser f : Rn →Rn is a function that\nmaps the input y to an estimate of x. A common metric to\nevaluate the quality of a denoiser is the mean squared error\nbetween the clean signal and the estimate,\nMSE := 1\nn\nn\nX\ni=1\n(xi −f(y)i)2 .\n(1)\nUnfortunately, in most real-world scenarios clean ground-\ntruth signals are not available and evaluation can only be\ncarried out in an unsupervised fashion, i.e. exclusively from\nthe noisy measurements. In this section we propose an\nunsupervised estimator of MSE inspired by recent advances\nin unsupervised denoising (Lehtinen et al., 2018). The key\nidea is to compare the denoised signal to a noisy reference,\nwhich corresponds to the same clean signal corrupted by\nindependent noise.\nIn order to motivate our approach, let us assume that the\nnoise is additive, so that y := x + z for a zero-mean noise\nvector z ∈Rn. Imagine that we have access to a noisy\nreference a := x + w corresponding to the same underlying\nsignal x, but corrupted with a different noise realization\nw ∈Rn independent from z (Section 3.3 explains how to\nobtain such references in practice). The mean squared dif-\nference between the denoised estimate and the reference is\napproximately equal to the sum of the MSE and the variance\nσ2 of the noise,\n1\nn\nn\nX\ni=1\n(ai −f(y)i)2 = 1\nn\nn\nX\ni=1\n(xi + wi −f(y)i)2\n≈1\nn\nn\nX\ni=1\n(xi −f(y)i)2 + 1\nn\nn\nX\ni=1\nw2\ni ≈MSE + σ2,\n(2)\nbecause the cross-term 1\nn\nPn\ni=1 wi (xi −f(y)i)2 cancels\nout if wi and yi (and hence f(yi)) are independent (and the\nmean of the noise is zero).\nApproximations to equation 2 are used by different unsuper-\nvised methods to train neural networks for denoising (Lehti-\nnen et al., 2018; Xie et al., 2020; Laine et al., 2019; Huang\net al., 2021). The noise term 1\nn\nPn\ni=1 w2\ni in equation 2\nis not problematic for training denoisers as long as it is\nindependent from the input y. However, it is definitely\nproblematic for evaluating denoisers, as the additional term\nwould change for different images and datasets, making it\nimpossible to perform quantitative comparisons. In order to\naddress this limitation we propose to modify the cost func-\ntion to neutralize the noise term. This can be achieved by\nusing two other noisy references b := x + v and c := x + u,\nwhich are noisy measurements corresponding to the clean\nsignal x, but corrupted with different, independent noise re-\nalizations v and u (just like a). Subtracting these references\nand dividing by two yields an estimate of the noise variance,\n1\nn\nn\nX\ni=1\n(bi −ci)2\n2\n= 1\nn\nn\nX\ni=1\n(vi −ui)2\n2\n≈1\n2n\nn\nX\ni=1\nv2\ni + 1\n2n\nn\nX\ni=1\nu2\ni ≈σ2,\n(3)\nwhich can then be subtracted from equation 2 to estimate the\nMSE. This yields our proposed unsupervised metric, which\n3\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nSpatial subsampling\nDifference\nS\nConsecutive frames\nDifference\n0\n50\n100\n150\n200\n250\nFigure 2. Noisy references. The proposed metrics require noisy references corresponding to the same clean image corrupted by\nindependent noise. These references can be obtained from a single image via spatial subsampling (above) or from consecutive frames\n(below). In both cases, there may be small differences in the signal content of the references, shown by the corresponding heatmaps.\nwe call unsupervised mean squared error (uMSE), depicted\nin Figure 1.\nDefinition 3.1 (Unsupervised mean squared error). Given a\nnoisy input signal y ∈Rn and three noisy references a, b,\nc ∈Rn the unsupervised mean squared error of a denoiser\nf : Rn →Rn is\nuMSE := 1\nn\nn\nX\ni=1\n(ai −f(y)i)2 −(bi −ci)2\n2\n.\n(4)\nTheorem 4.2 in Section 4 establishes that the uMSE is a con-\nsistent estimator of the MSE as long as (1) the noisy input\nand the noisy references are independent, (2) their means\nequal the corresponding entries of the ground-truth clean\nsignal, and (3) their higher-order moments are bounded.\nThese conditions are satisfied by most noise models of in-\nterest in signal and image processing, such as Poisson shot\nnoise or additive Gaussian noise. In Section 3.3 we address\nthe question of how to obtain the noisy references required\nto estimate the uMSE. Section A explains how to compute\nconfidence intervals for the uMSE via bootstrapping.\n3.2. The Unsupervised Peak Signal-To-Noise Ratio\nPeak signal-to-noise ratio (PSNR) is currently the most pop-\nular metric to evaluate denoising quality. It is a logarithmic\nfunction of MSE defined on a decibel scale,\nPSNR := 10 log\n\u0012 M2\nMSE\n\u0013\n,\n(5)\nwhere M is a fixed constant representing the maximum\npossible value of the signal of interest, which is usually set\nequal to 255 for images. Our definition of uMSE can be\nnaturally extended to yield an unsupervised PSNR (uPSNR).\nDefinition 3.2 (Unsupervised peak signal-to-noise ratio).\nGiven a noisy input signal y ∈Rn and three noisy ref-\nerences a, b, c ∈Rn the peak signal-to-noise ratio of a\ndenoiser f : Rn →Rn is\nuPSNR := 10 log\n\u0012 M2\nuMSE\n\u0013\n,\n(6)\nwhere M is the maximum possible value of the signal of\ninterest.\nCorollary 4.3 establishes that the uPSNR is a consistent\nestimator of the PSNR, under the same conditions that guar-\nantee consistency of the uMSE. Section A explains how to\ncompute confidence intervals for the uPSNR via bootstrap-\nping.\n3.3. Computing Noisy References In Practice\nOur proposed metrics rely on the availability of three noisy\nreferences, which ideally should correspond to the same\nclean image contaminated with independent noise. Devi-\nations between the clean signal in each reference violate\nCondition 2 in Section 4, and introduce a bias in the metrics.\nWe propose two approaches to compute the references in\npractice, illustrated in Figure 2.\nMultiple images: The references can be computed from\nconsecutive frames acquired within a short time interval.\n4\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\n20 pixels\n100 pixels\n1,000 pixels\nFigure 3. The uMSE is a consistent estimator of the MSE. The histograms at the top show the distribution of the uMSE computed from\nn pixels (n ∈{20, 100, 1000}) of a natural image corrupted with additive Gaussian noise (σ = 55) and denoised via a deep-learning\ndenoiser (DnCNN). Each point in the histogram corresponds to a different sample of the three noisy references used to compute the uMSE\n(˜ai, ˜bi and ˜ci in Eq. 8 for 1 ≤i ≤n), with the same underlying clean pixels. The distributions are centered at the MSE, showing that the\nestimator is unbiased (Theorem 4.1), and are well approximated by a Gaussian fit (Theorem 4.4). As the number of pixels n grows, the\nstandard deviation of the uMSE decreases proportionally to n−1/2, and the uMSE converges asymptotically to the MSE (Theorem 4.2), as\ndepicted in the scatterplot below (α is a constant).\nThis approach is preferable for datasets where the image\ncontent does not experience rapid dynamic changes from\nframe to frame. We apply this approach to the RAW videos\nin Section 6, where the content is static.\nSingle image: The references can be computed from a sin-\ngle image via spatial subsampling, as described in Section B.\nSection B shows that this approach is effective as long as\nthe image content is sufficiently smooth with respect to the\npixel resolution. We apply this approach to the electron-\nmicroscopy data in Section 7, where preserving dynamic\ncontent is important.\n4. Statistical Properties of the Proposed\nMetrics\nIn this section, we establish that the proposed unsupervised\nmetrics provide a consistent estimate of the MSE and PSNR.\nIn our analysis, the ground truth signal or set of signals is\nrepresented as a deterministic vector x ∈Rn. The corre-\nsponding noisy data are also modeled as a deterministic\nvector y ∈Rn that is fed into a denoiser f : Rn →Rn\nto produce the denoised estimate f(y). The MSE of the\nestimate is a deterministic quantity equal to\nMSE := 1\nn\nn\nX\ni=1\nSEi,\nSEi := (xi −f(y)i)2 .\n(7)\nNoise Model. The uMSE estimator in Definition 3.1 de-\npends on three noisy references ˜a, ˜b, ˜c, which we model as\nrandom variables.1 Our analysis assumes that these random\nvariables satisfy two conditions:\nCondition 1 (independence): The entries of ˜a, ˜b, ˜c are all\nmutually independent.\nCondition 2 (centered noise): The mean of the ith entry\nof ˜a, ˜b, ˜c equals the corresponding entry of the clean signal,\nE [˜ai] = E[˜bi] = E [˜ci] = xi, 1 ≤i ≤n.\nTwo popular noise models that satisfy these conditions are:\n• Additive Gaussian, where ˜ai := xi + ˜wi, ˜bi := xi + ˜vi,\n˜ci := xi + ˜ui, for i.i.d. Gaussian ˜wi, ˜vi, ˜ui.\n• Poisson, where ˜ai, ˜bi, ˜ci are i.i.d. Poisson random vari-\nables with parameter xi.\n1In our analysis, all random quantities are marked with a tilde\nfor clarity.\n5\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nNatural images (Gaussian noise)\nElectron microscopy (Poisson noise)\nSpatial subsampling\nSpatial subsampling\nFigure 4. Bias introduced by spatial subsampling. The histograms show the distribution of the uMSE (computed as in Figure 3)\ncorresponding to a natural image and a simulated electron-microscopy image corrupted by Gaussian (σ = 55) and Poisson noise\nrespectively, and denoised with a standard deep-learning denoiser (DnCNN). For each image, the uMSE is computed using noisy\nreferences with the same underlying clean image (left), and from noisy references obtained via spatial subsampling (right). For the natural\nimage, spatial subsampling introduces a substantial bias (compare the 1st and 2nd histogram), whereas for the electron-microscopy image\nthe bias is much smaller (compare the 3rd and 4th histogram).\nTheoretical Guarantees. Our goal is to study the statistical\nproperties of the uMSE\n^\nuMSE := 1\nn\nn\nX\ni=1\ng\nuSEi,\ng\nuSEi := (˜ai −f(y)i)2 −(˜bi −˜ci)2\n2\n.\n(8)\nAs indicated by the tilde, under our modeling assumptions,\nthe uMSE is a random variable. We first show that the\ncorrection factor in the definition of uMSE succeeds in\ndebiasing the estimator, so that its mean is equal to the\nMSE.\nTheorem 4.1 (The uMSE is unbiased, proof in Section E.1).\nIf Conditions 1 and 2 hold, the uMSE is an unbiased estima-\ntor of the MSE, i.e. E[ ^\nuMSE] = MSE.\nTheorem 4.1 establishes that the distribution of the uMSE\nis centered at the MSE. We now show that its variance\nshrinks at a rate inversely proportional to the number of\nsignal entries n, and therefore converges to the MSE in\nmean square and probability as n →∞(see Figure 3 for a\nnumerical demonstration). This occurs as long as the higher\ncentral moments of noise and the entrywise denoising error\nare bounded by a constant, which is to be expected in most\nrealistic scenarios.\nTheorem 4.2 (The uMSE is consistent, proof in Section E.2).\nLet µ[k]\ni\ndenote the kth central moment of ˜ai, ˜bi, ˜ci, and γ :=\nmax1≤i≤n |xi −f(y)i| the maximum entrywise denoising\nerror. If Conditions 1 and 2 hold, and there exists a constant\nα such that max1≤i≤n max\nn\nµ[4]\ni , µ[3]\ni γ, γ4o\n≤α, then\nthe mean squared error between the MSE and the uMSE\nsatisfies the bound\nE\n\u0014\u0010\n^\nuMSE −MSE\n\u00112\u0015\n= Var\nh\n^\nuMSE\ni\n≤α\nn.\n(9)\nConsequently, limn→∞E[( ^\nuMSE −MSE)2] = 0, so the\nuMSE converges to the MSE in mean square and therefore\nalso in probability.\nConsistency of the uMSE implies consistency of the uPSNR.\nCorollary 4.3 (The uPSNR is consistent, proof in Sec-\ntion E.3). Under the assumptions of Theorem 4.2, the uP-\nSNR defined as\n^\nuPSNR := 10 log\n\u0012 M2\n^\nuMSE\n\u0013\n,\n(10)\nwhere M is a fixed constant, converges in probability to the\nPSNR, as n →∞.\nThe uMSE converges to a Gaussian random variable asymp-\ntotically as n →∞.\nTheorem 4.4 (The uMSE is asymptotically normally dis-\ntributed, proof in Section E.4). If the first six central mo-\nments of ˜ai, ˜bi, ˜ci and the maximum entrywise denoising er-\nror max1≤i≤n |xi −f(y)i| are bounded, and Conditions 1\nand 2 hold, the uMSE is asymptotically normally distributed\nas n →∞.\nOur numerical experiments show that the distribution of the\nuMSE is well approximated as Gaussian even for relatively\nsmall values of n (see Figure 3). This can be exploited to\nbuild confidence intervals for the uMSE and uPSNR, as\nexplained in Section A.\n6\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nTable 1. Controlled comparison of PSNR and uPSNR. The table shows the PSNR computed from clean ground-truth images, compared\nto two versions of the proposed estimator: one using noisy references corresponding to the same clean image (uPSNR), and another using\na single noisy image combined with spatial subsampling (uPSNRs). The metrics are compared on the datasets and denoising methods\ndescribed in Section G.\nNatural images (Gaussian noise)\nσ = 25\nσ = 50\nσ = 75\nσ = 100\nMethod\nPSNR\nuPSNR\nuPSNRS\nPSNR\nuPSNR\nuPSNRS\nPSNR\nuPSNR\nuPSNRS\nPSNR\nuPSNR\nuPSNRS\nBilateral\n24.20\n24.18\n26.20\n21.84\n21.86\n22.90\n19.14\n19.17\n19.58\n16.30\n16.37\n16.47\nDenseNet\n26.54\n26.51\n27.61\n23.98\n24.06\n26.28\n22.75\n23.00\n24.69\n21.92\n21.97\n23.78\nDnCNN\n26.19\n26.21\n28.14\n23.95\n24.02\n26.08\n22.72\n22.75\n24.59\n21.84\n21.84\n23.71\nUNet\n27.22\n27.26\n25.40\n24.95\n24.96\n23.52\n23.33\n23.40\n22.33\n22.21\n22.28\n21.11\nBlindSpot\n25.55\n25.53\n24.10\n24.08\n24.07\n22.77\n22.79\n22.69\n21.82\n21.75\n21.82\n21.24\nNeighbor2N.\n25.91\n25.89\n24.91\n24.49\n24.58\n23.37\n22.77\n22.80\n21.67\n21.52\n21.44\n20.23\nNoise2Noise\n27.18\n27.22\n25.32\n24.94\n24.88\n23.31\n23.26\n23.19\n21.50\n22.10\n22.13\n20.11\nNoise2Self\n24.57\n24.56\n22.88\n23.38\n23.40\n21.94\n22.24\n22.33\n20.94\n21.34\n21.18\n20.15\nElectron microscopy (Poisson noise)\nBilateral\nBlindSpot\nDnCNN\nUNet\nPSNR\nuPSNR\nuPSNRS\nPSNR\nuPSNR\nuPSNRS\nPSNR\nuPSNR\nuPSNRS\nPSNR\nuPSNR\nuPSNRS\n20.18\n20.20\n20.21\n24.86\n24.87\n24.74\n25.74\n25.68\n25.86\n24.65\n24.69\n24.79\nTable 2. Comparison of averaging-based PSNR and uPSNR on RAW videos with real noise. The proposed uPSNR metric, computed\nusing three noisy references, is very similar to an averaging-based PSNR estimate computed from 10 noisy references. The metrics are\ncompared on the datasets and denoising methods described in Section H.\nImage (Wavelet)\nImage (CNN)\nVideo (Temp. Avg)\nVideo (CNN)\nISO\nPSNRavg\nuPSNR\nPSNRavg\nuPSNR\nPSNRavg\nuPSNR\nPSNRavg\nuPSNR\n1600\n37.56\n37.76\n46.88\n48.05\n34.32\n34.36\n48.06\n49.51\n3200\n35.52\n35.55\n44.91\n45.51\n32.48\n32.47\n46.45\n47.33\n6400\n32.60\n32.68\n42.74\n43.05\n30.77\n30.75\n44.75\n45.16\n12800\n28.43\n28.46\n40.22\n39.75\n27.71\n27.76\n42.22\n41.69\n25600\n26.79\n26.9\n40.19\n38.78\n27.08\n27.12\n42.13\n40.32\nMean\n32.18\n32.27\n42.99\n43.03\n30.47\n30.49\n44.72\n44.80\n5. Controlled Evaluation of the Proposed\nMetrics\nIn this section, we study the properties of the uMSE and\nuPSNR through numerical experiments in a controlled sce-\nnario where the ground-truth clean images are known. We\nuse a dataset of natural images (Martin et al., 2001; Zhang\net al., 2017b; Franzen, 1993) corrupted with additive Gaus-\nsian noise with σ ∈[25, 50, 75, 100], and a dataset of sim-\nulated electron-microscopy images (Vincent et al., 2021)\ncorrupted with Poisson noise. For the two datasets, we com-\npute the supervised MSE and PSNR using the ground-truth\nclean image. To compute the uMSE and uPSNR we use\nnoisy references corresponding to the same clean image\ncorrupted with independent noise. We also compute the\nuMSE and uPSNR using noisy references obtained from\na single noisy image via spatial subsampling, as described\nin Section B, which we denote by uMSES and uPSNRS\nrespectively.2 All metrics are applied to multiple denoising\napproches, as described in more detail in Section G.\nThe results are reported in Tables 1 and 3. When the noisy\nreferences correspond exactly to the same clean image (and\ntherefore satisfy the conditions in Section 4), the unsuper-\nvised metrics are extremely accurate across different noise\nlevels for all denoising methods.\nSingle-image results: When the noisy references are com-\nputed via spatial subsampling, the metrics are still very\naccurate for the electron-microscopy dataset, but less so for\nthe natural-image dataset if the PSNR is high (above 20\ndB). The culprit is the difference between the clean images\nunderlying each noisy reference (see Figure 2), which intro-\nduces a bias in the unsupervised metric, depicted in Figure 4.\n2The other metrics are applied to subsampled images in order\nto make them directly comparable to the uMSES and uPSNRS.\n7\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nFigure 8 shows that the difference is more pronounced in\nnatural images than in electron-microscopy images, which\nare smoother with respect to the pixel resolution. We further\nanalyze the influence of the image smoothness on the effect\nof spatial subsampling in the proposed metrics in Section B.\n6. Application to Videos in RAW Format\nWe evaluate our proposed metrics on a dataset of videos\nin raw format, consisting of direct readings from the sen-\nsor of a surveillance camera contaminated with real noise\nat five different ISO levels (Yue et al., 2020). The dataset\ncontains 11 unique videos divided into 7 segments, each\nconsisting of 10 noisy frames that capture the same static ob-\nject. We consider four different denoisers: a wavelet-based\nmethod, temporal averaging and two versions of a state-of-\nthe-art unsupervised deep-learning method using images\nand videos respectively (Sheth et al., 2021). A detailed\ndescription of the experiments is provided in Section H.\nTables 4 and 2 compare our proposed unsupervised met-\nrics (computed using three noisy frames in each segment)\nwith MSE and PSNR estimates obtained via averaging from\nten noisy frames. The two types of metric yield similar\nresults: the deep-learning methods clearly outperform the\nother baselines, and the video-based methods outperform\nthe image-based methods. As explained in Section D, the\naveraging-based MSE and PSNR are not consistent estima-\ntors of the true MSE and PSNR, and can be substantially\nless accurate than the uMSE and uPSNR (see Figure 10), so\nthey should not be considered ground-truth metrics.\n7. Application To Electron Microscopy\nOur proposed metrics enable quantitative evaluation of de-\nnoisers in the absence of ground-truth clean images. We\nshowcase this for transmission electron microscopy (TEM),\na key imaging modality in material sciences. Recent devel-\nopments enable the acquisition of high frame-rate images,\ncapturing high temporal resolution dynamics, thought to\nbe crucial in catalytic processes (Crozier et al., 2019). Im-\nages acquired under these conditions are severely limited by\nnoise. Recent work suggest that deep learning methods pro-\nvide an effective solution (Sheth et al., 2021; Mohan et al.,\n2022; 2021), but, instead of quantitative metrics, evaluation\non real data has been limited to visual inspection.\nThe TEM dataset consists of 18,597 noisy frames depicting\nplatinum nanoparticles on a cerium oxide support. A major\nchallenge for the application of unsupervised metrics is the\npresence of local correlations in the noise (see Figure 13).\nWe address this by performing spatial subsampling to reduce\nthe correlation and selecting two contiguous test sets with\nlow correlation: 155 images with moderate signal-to-noise\nratio (SNR), and 383 images with low SNR, which are\nModerate SNR test set\nLow SNR test set\nData\nGaussian smoothing\n20.4 dB\n16.0 dB\nNoise2Self\n25.8 dB\n17.2 dB\nBlindSpot\n25.3 dB\n18.1 dB\nNeighbor2Neighbor\n26.9 dB\n18.6 dB\nFigure 5. Denoising real-world electron-microscopy data. Ex-\nample noisy images (top) from the moderate-SNR (left 2 columns)\nand low-SNR (right 2 columns) test sets described in Section 7.\nThe data are denoised using a Gaussian-smoothing baseline and\nseveral unsupervised CNNs: Noise2Self, BlindSpot, and Neigh-\nbor2Neighbor. The uPSNR of each method on each test set is\nshown below the images. The uPSNR values and visual inspection\nindicate that the CNNs clearly outperform the baseline method,\nthat the best unsupervised approach is Neighbor2Neighbor, and\nthat all methods achieve worse results on the low-SNR test set.\n8\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nmore challenging. We train a UNet architecture following\nthe Neighbor2Neighbor and Noise2Self methods (Huang\net al., 2021; Batson & Royer, 2019b) and a BlindSpot UNet\n(using a single-frame version of (Sheth et al., 2021)) on a\ntraining set containing 70% of the data, and compare their\nperformance to a Gaussian-smoothing baseline on the two\ntest sets. Section I provides a more detailed description of\nthe dataset and the models.\nFigure 5 shows examples from the data and the corre-\nsponding denoised images, as well as the uPSNR of each\nmethod for the two test sets. Figure 11 shows a histogram\ncomparing the uMSE values of Gaussian smoothing and\nNeighbor2Neighbor for each individual test image. The\nunsupervised metrics indicate that the deep-learning meth-\nods achieve effective denoising on the moderate SNR set\n(clearly outperforming the Gaussian-smoothing baseline),\nthat Neighbor2Neighbor yields the best results, and that all\nmethods produce significantly worse results on the low-SNR\ntest set. Figure 12 shows that uMSE produces consistent\nimage-level evaluations between Neighbor2Neighbor and\nGaussian smoothing. These conclusions are supported by\nthe visual appearance of the images.\n8. Conclusion And Open Questions\nIn this work we introduce two novel unsupervised metrics\ncomputed exclusively from noisy data, which are asymp-\ntotically consistent estimators of the corresponding super-\nvised metrics, and yield accurate approximations in practice.\nThese results show that unsupervised evaluation is feasible\nand can be very effective, but several important challenges\nremain. Key open questions for future research include:\n• How to address the bias introduced by spatial subsam-\npling in the case of single images that are not suffi-\nciently smooth (see Section B), ideally achieving an\nunbiased approximation to the MSE from a single noisy\nimage.\n• How to design unsupervised metrics for noise distribu-\ntions and artifacts which are not pixel-wise indepen-\ndent (see for example (Prakash et al., 2021)).\n• How to obtain unsupervised approximations of percep-\ntual metrics such as SSIM (Wang et al., 2004).\n• How to perform unsupervised evaluation for inverse\nproblems beyond denoising, and related applications\nsuch as realistic image synthesis (Zwicker et al., 2015).\nAcknowledgements\nAMM was partially supported by the mobility grants pro-\ngram of Centre de Formaci´o Interdisciplin`aria Superior\n(CFIS) - Universitat Polit`ecnica de Catalunya (UPC). We\ngratefully acknowledge financial support from the National\nScience Foundation (NSF). NSF NRT HDR Award 1922658\npartially supported SM. NSF OAC-1940263 and 2104105\nsupported PAC aand PH, NSF CBET 1604971 supported JV,\nand NSF DMR 184084 supported MT. NSF OAC-1940097\nsupported ML. NSF OAC-2103936 supported CFG. The\nauthors acknowledge ASU Research Computing and NYU\nHPC for providing high performance computing resources,\nand the John M. Cowley Center for High Resolution Elec-\ntron Microscopy at Arizona State University. The direct\nelectron detector was support from NSF MRI 1920335.\nReferences\nAbdelhamed, A., Lin, S., and Brown, M. S. A high-quality\ndenoising dataset for smartphone cameras. In Proceed-\nings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 1692–1700, 2018.\nArias, P. and Morel, J.-M. Video denoising via empirical\nbayesian estimation of space-time patches. Journal of\nMathematical Imaging and Vision, 60(1):70–93, 2018.\nBatson, J. and Royer, L. Noise2self: Blind denoising by\nself-supervision, 2019a. URL https://arxiv.org/\nabs/1901.11365.\nBatson, J. and Royer, L. Noise2Self: Blind denoising by\nself-supervision. In Chaudhuri, K. and Salakhutdinov,\nR. (eds.), Proceedings of the 36th International Confer-\nence on Machine Learning, volume 97 of Proceedings\nof Machine Learning Research, pp. 524–533. PMLR, 09–\n15 Jun 2019b. URL https://proceedings.mlr.\npress/v97/batson19a.html.\nBreiman, L. Probability. SIAM, 1992.\nCrozier, P. A., Lawrence, E. L., Vincent, J. L., and Levin,\nB. D.\nDynamic restructuring during processing: ap-\nproaches to higher temporal resolution. Microscopy and\nMicroanalysis, 25(S2):1464–1465, 2019.\nDonoho, D. and Johnstone, I. Adapting to unknown smooth-\nness via wavelet shrinkage. J American Stat Assoc, 90\n(432), December 1995.\nEfron, B. and Tibshirani, R. J. An introduction to the boot-\nstrap. CRC press, 1994.\nErcius, P., Johnson, I., Brown, H., Pelz, P., Hsu, S.-\nL., Draney, B., Fong, E., Goldschmidt, A., Joseph,\nJ., Lee, J., and et al.\nThe 4d camera – a 87 khz\nframe-rate detector for counted 4d-stem experiments.\nMicroscopy and Microanalysis, pp. 1–3, 2020.\ndoi:\n10.1017/S1431927620019753.\n9\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nFaruqi, A. and McMullan, G.\nDirect imaging detec-\ntors for electron microscopy.\nNuclear Instruments\nand Methods in Physics Research Section A: Accel-\nerators,\nSpectrometers,\nDetectors and Associated\nEquipment, 878:180 – 190, 2018.\nISSN 0168-9002.\ndoi: https://doi.org/10.1016/j.nima.2017.07.037. URL\nhttp://www.sciencedirect.com/science/\narticle/pii/S0168900217307787.\nRadiation\nImaging Techniques and Applications.\nFranzen,\nR. W.,\n1993.\nURL http://r0k.us/\ngraphics/kodak/.\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger,\nK. Q. Densely connected convolutional networks. In Proc.\nIEEE Conf. Computer Vision and Pattern Recognition,\npp. 4700–4708, 2017.\nHuang, T., Li, S., Jia, X., Lu, H., and Liu, J.\nNeigh-\nbor2neighbor: Self-supervised denoising from single\nnoisy images. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp.\n14781–14790, 2021.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\narXiv preprint arXiv:1502.03167, 2015.\nKrull, A., Buchholz, T.-O., and Jug, F. Noise2void - learning\ndenoising from single noisy images. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 2124–2132, 2019.\nLaine, S., Karras, T., Lehtinen, J., and Aila, T. High-quality\nself-supervised deep image denoising. In Advances in\nNeural Information Processing Systems 32, pp. 6970–\n6980, 2019.\nLebrun, M., Colom, M., and Morel, J.-M. Multiscale image\nblind denoising. IEEE Transactions on Image Processing,\n24(10):3149–3161, 2015.\nLehtinen, J., Munkberg, J., Hasselgren, J., Laine, S., Karras,\nT., Aittala, M., and Aila, T. Noise2noise: Learning image\nrestoration without clean data, 2018. URL https://\narxiv.org/abs/1803.04189.\nLi, X. Blind image quality assessment. In Proceedings.\nInternational Conference on Image Processing, volume 1,\npp. I–I. IEEE, 2002.\nLiu, X., Tanaka, M., and Okutomi, M. Single-image noise\nlevel estimation for blind denoising. IEEE transactions\non image processing, 22(12):5226–5237, 2013.\nMartin, D., Fowlkes, C., Tal, D., and Malik, J. A database\nof human segmented natural images and its application\nto evaluating segmentation algorithms and measuring\necological statistics. In Proc. 8th Int’l Conf. Computer\nVision, volume 2, pp. 416–423, July 2001.\nMetzler, C. A., Mousavi, A., Heckel, R., and Baraniuk,\nR. G. Unsupervised learning with stein’s unbiased risk\nestimator. arXiv preprint arXiv:1805.10531, 2018.\nMittal, A., Moorthy, A. K., and Bovik, A. C. No-reference\nimage quality assessment in the spatial domain. IEEE\nTransactions on image processing, 21(12):4695–4708,\n2012.\nMohan, S., Kadkhodaie, Z., Simoncelli, E. P., and\nFernandez-Granda, C. Robust and interpretable blind im-\nage denoising via bias-free convolutional neural networks.\nIn International Conference on Learning Representations,\n2020. URL https://openreview.net/forum?\nid=HJlSmC4FPS.\nMohan, S., Vincent, J., Manzorro, R., Crozier, P., Fernandez-\nGranda, C., and Simoncelli, E. Adaptive denoising via\ngaintuning. Advances in Neural Information Processing\nSystems, 34, 2021.\nMohan, S., Manzorro, R., Vincent, J. L., Tang, B., Sheth,\nD. Y., Simoncelli, E., Matteson, D. S., Crozier, P. A.,\nand Fernandez-Granda, C. Deep denoising for scientific\ndiscovery: A case study in electron microscopy. IEEE\nTransactions on Computational Imaging, 2022.\nPlotz, T. and Roth, S. Benchmarking denoising algorithms\nwith real photographs. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pp.\n1586–1595, 2017.\nPrakash, M., Delbracio, M., Milanfar, P., and Jug, F. In-\nterpretable unsupervised diversity denoising and artefact\nremoval. In International Conference on Learning Repre-\nsentations, 2021.\nRamani, S., Blu, T., and Unser, M. Monte-carlo sure: A\nblack-box optimization of regularization parameters for\ngeneral denoising algorithms.\nIEEE Transactions on\nimage processing, 17(9):1540–1554, 2008.\nRonneberger, O., Fischer, P., and Brox, T. U-net: Convolu-\ntional networks for biomedical image segmentation. In In-\nternational Conference on Medical image computing and\ncomputer-assisted intervention, pp. 234–241. Springer,\n2015.\nSheth, D. Y., Mohan, S., Vincent, J. L., Manzorro, R.,\nCrozier, P. A., Khapra, M. M., Simoncelli, E. P., and\nFernandez-Granda, C. Unsupervised deep video denois-\ning. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pp. 1759–1768, 2021.\n10\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nSoltanayev, S. and Chun, S. Y.\nTraining deep learn-\ning based denoisers without ground truth data.\nIn\nAdvances in Neural Information Processing Systems, vol-\nume 31, 2018.\nURL https://proceedings.\nneurips.cc/paper/2018/file/\nc0560792e4a3c79e62f76cbf9fb277dd-Paper.\npdf.\nVincent, J. L., Manzorro, R., Mohan, S., Tang, B., Sheth,\nD. Y., Simoncelli, E. P., Matteson, D. S., Fernandez-\nGranda, C., and Crozier, P. A. Developing and evaluating\ndeep neural network-based denoising for nanoparticle\ntem images with ultra-low signal-to-noise. Microscopy\nand Microanalysis, 27(6):1431–1447, 2021.\nWang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P.\nImage quality assessment: from error visibility to struc-\ntural similarity. IEEE transactions on image processing,\n13(4):600–612, 2004.\nXie, Y., Wang, Z., and Ji, S. Noise2same: Optimizing a\nself-supervised bound for image denoising. Advances\nin Neural Information Processing Systems, 33:20320–\n20330, 2020.\nXu, J., Li, H., Liang, Z., Zhang, D., and Zhang, L. Real-\nworld noisy image denoising: A new benchmark. arXiv\npreprint arXiv:1804.02603, 2018.\nYue, H., Cao, C., Liao, L., Chu, R., and Yang, J. Super-\nvised raw video denoising with a benchmark dataset on\ndynamic scenes. In 2020 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pp. 2298–\n2307, 2020. doi: 10.1109/CVPR42600.2020.00237.\nZhang, K., Zuo, W., Chen, Y., Meng, D., and Zhang, L.\nBeyond a gaussian denoiser: Residual learning of deep\ncnn for image denoising. IEEE Transactions on Image\nProcessing, pp. 3142–3155, 2017a.\nZhang, K., Zuo, W., Chen, Y., Meng, D., and Zhang, L.\nBeyond a gaussian denoiser: Residual learning of deep\ncnn for image denoising. IEEE transactions on image\nprocessing, 26(7):3142–3155, 2017b.\nZhang, Y., Zhu, Y., Nichols, E., Wang, Q., Zhang, S., Smith,\nC., and Howard, S. A poisson-gaussian denoising dataset\nwith real fluorescence microscopy images. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 11710–11718, 2019.\nZhussip, M., Soltanayev, S., and Chun, S. Y. Extending\nstein’s unbiased risk estimator to train deep denoisers\nwith correlated pairs of noisy images. Advances in neural\ninformation processing systems, 32, 2019.\nZwicker, M., Jarosz, W., Lehtinen, J., Moon, B., Ramamoor-\nthi, R., Rousselle, F., Sen, P., Soler, C., and Yoon, S.-E.\nRecent advances in adaptive sampling and reconstruction\nfor monte carlo rendering. In Computer graphics forum,\nvolume 34, pp. 667–681. Wiley Online Library, 2015.\n11\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nSpatial subsampling\n0.01\n0.02\n0.03\n0.04\nMSE\n0.01\n0.02\n0.03\n0.04\n0.05\nuMSE-based confidence intervals\n0.01\n0.02\n0.03\n0.04\nMSE\n0.01\n0.02\n0.03\n0.04\nFigure 6. Unsupervised confidence intervals for the MSE. 0.95-\nConfidence intervals computed following Algorithm 1 of natural\nimages from the dataset in Section 5 corrupted with additive Gaus-\nsian noise (σ = 55) and denoised via a standard deep-learning\ndenoiser (DnCNN). The horizontal coordinate of each interval cor-\nresponds to the true MSE, so ideally 95% of the intervals should\noverlap with the diagonal dashed identity line. The left plot shows\nthat this is the case when the noisy references with the same under-\nlying clean image, demonstrating that Algorithm 1 produces valid\nconfidence intervals. The right plot shows confidence intervals\nbased on noisy references obtained via spatial subsampling (right).\nSpatial subsampling produces a systematic bias in the uMSE, ana-\nlyzed in Section B which shifts the intervals away from the identity\nline when the underlying image content is not sufficiently smooth\nwith respect to the pixel resolution.\nA. Confidence Intervals for Uncertainty\nQuantification\nThe uMSE and uPSNR are estimates of the MSE and PSNR\ncomputed from noisy data, so they are inherently uncertain.\nWe propose to quantify this uncertainty using confidence\nintervals obtained via bootstrapping.\nTheorem 4.4 establishes that the uMSE is asymptotically\nnormal. In addition, our numerical experiments show that\nthe distribution of the uMSE is well approximated as Gaus-\nsian even for relatively small values of n (see Figure 3). As\na result, the bootstrap confidence intervals for the uMSE\nproduced by Algorithm 1 contain the MSE with probability\napproximately 1 −α (see Section 13.3 in (Efron & Tib-\nshirani, 1994)). This also implies that the PSNR belongs\nto the bootstrap confidence intervals for the uPSNR with\nprobability approximately 1 −α because the function that\nmaps the uMSE to the uPSNR and the MSE to the PSNR is\nmonotone (see Section 13.6 in (Efron & Tibshirani, 1994)).\nFigure 6 shows a numerical verification that the proposed\napproach yields valid confidence intervals for MSE in the\ncontrolled experiments of Section 5, where the ground-truth\nclean images are known. It also shows that the bias in-\ntroduced by spatial subsampling for natural images (see\nSection B), shifts the confidence intervals away from the\ntrue MSE.\nB. Spatial Subsampling\nIn this section, we propose a method to obtain the noisy\nreferences required to estimate uMSE and uPSNR. We focus\nour discussion on images, but similar ideas can be applied to\nvideos and time-series data. In order to simplify the notation,\nwe consider N × N images. The n-dimensional signals in\nother sections can be interpreted as vectorized versions of\nthese images with n = N 2.\nWe assume that we have available a noisy image I of di-\nmensions 2N × 2N. We extract four noisy references from\nI by spatial subsampling. The method is inspired by the\nNeighbor2Neighbor unsupervised denoising method, which\nuses random subsampling to generate noisy image pairs\nduring training (Huang et al., 2021). Figure 7 illustrates the\napproach.\nC. Effect Of Spatial Subsampling on the\nProposed Metrics\nSpatial subsampling generates four noisy sub-images that\ncorrespond to the noisy input y and the three noisy refer-\nences a, b and c in Definition 3.1. In our derivation of the\nuMSE, we assume that these four noisy signals are gener-\nated by corrupting the same ground-truth clean signal with\nindependent noise. This holds for the sub-images in Defini-\ntion 2 if (1) the underlying clean image is smooth, so that\nadjacent pixels are approximately equal, and (2) the noise is\npixel-wise independent. Tables 1 and 3, and Figures 4 and\n6 show that these assumptions don’t hold completely for\nnatural images, which introduces a bias in the uMSE. This\nbias also exists for the electron-microscopy images but it is\nmuch smaller, because the images are smoother with respect\nto the pixel resolution. Figure 8 shows the relative root\nmean square error (RMSE) between clean copies of images\nobtained via spatial subsampling following Algorithm 2 for\nthe natural images (left) and electron-microscopy images\n(right) used for the experiments in Section 5. The difference\nis substantially larger in natural images, because they are\nless smooth with respect to the pixel resolution than the\nelectron-microscopy images.\nIn order to further analyze the effect of spatial subsampling\non the proposed metrics, we performed a controlled experi-\nment where we applied different degrees of smoothing (via\na Gaussian filter) to a natural image. We evaluated the rel-\native RMSE of the corresponding subsampled references.\nIn addition, we fed the smoothed images contaminated by\nnoise into a denoiser and compared the uMSE of the de-\nnoised image with its true MSE. The results are shown in\nFigure 9. We observe that smoothing results in a stark de-\ncrease of both the relative RMSE and the uMSE, suggesting\nthat spatial subsampling is effective as long as the under-\n12\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nAlgorithm 1 Bootstrap confidence intervals\nWe assume access to a noisy input signal y ∈Rn and three noisy references a, b, c ∈Rn. For 1 ≤k ≤K, build an index\nset Bk by sampling n entries from {1, 2, . . . , n} uniformly and independently at random with replacement. Then set\nuMSEk := 1\nn\nX\ni∈Bk\n(ai −f(y)i)2 −(bi −ci)2\n2\n,\nuPSNRk := 10 log\n\u0012\nM2\nuMSEk\n\u0013\n.\n(11)\nTo build 1 −α confidence intervals, 0 < α < 1 for the uMSE and uPSNR set\nIuMSE :=\nh\nquMSE\nα/2\n, quMSE\n1−α/2\ni\n,\nIuPSNR :=\nh\nquPSNR\nα/2\n, quPSNR\n1−α/2\ni\n,\n(12)\nwhere quMSE\nα/2\nand quMSE\n1−α/2 are the α/2 and 1 −α/2 quantiles of the set {uMSE1, . . . , uMSEK}, and quPSNR\nα/2\nand quPSNR\n1−α/2\nare the α/2 and 1 −α/2 quantiles of the set {uPSNR1, . . . , uPSNRK}.\nAlgorithm 2 Decomposition via spatial subsampling\nGiven an image I ∈R2N×2N, let\nS1(i, j) := I (2i −1, 2j −1) ,\nS2(i, j) := I (2i, 2j −1) ,\nS3(i, j) := I (2i −1, 2j) ,\nS4(i, j) := I (2i, 2j) ,\n1 ≤i, j ≤n.\n(13)\nThe spatial decomposition of I is equal to four sub-images Y , A, B, C ∈RN×N where Y (i, j), A(i, j), B(i, j), C(i, j)\nare set equal to S1(i, j), S2(i, j), S3(i, j), S4(i, j), or to a random permutation of the four values.\nNoisy image\nSubsampled\nreferences\nSubsampling\nscheme\nFigure 7. Spatial subsampling uses a single noisy image (left) to\nextract four noisy references (center) corresponding approximately\nto the same underlying clean image, but with independent noise.\nThe pixels of each 2×2 block are assigned to each of the references\neither deterministically, or at random (right).\nlying image content is sufficiently smooth with respect to\nthe pixel resolution (as supported also by the results on the\nelectron-microscopy data).\nD. Comparison With Averaging-Based MSE\nEstimation\nExisting denoising benchmarks containing images corrupted\nwith real noise perform evaluation by computing the MSE\nor PSNR using an estimate of the clean image obtained by\naveraging multiple noisy frames (Abdelhamed et al., 2018;\nPlotz & Roth, 2017; Xu et al., 2018; Zhang et al., 2019).\nNatural images\nElectron microscopy\n5\n10\n15\n20\n25\n30\nRelative RMSE\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nNormalized counts\n10\n2\n5\n10\n15\n20\n25\nRelative RMSE\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n10\n3\nFigure 8. Effect of spatial subsampling. The histograms show the\nrelative root mean square error (RMSE) between clean copies of\nimages obtained via spatial subsampling following Algorithm 2 for\nthe natural images (left) and electron-microscopy images (right)\nused for the experiments in Section 5. The difference is substan-\ntially larger in natural images, because they are less smooth with\nrespect to the pixel resolution than the electron-microscopy im-\nages.\nIn this section, we show both theoretically and numerically\nthat this approach produces a poor estimate of the MSE and\nPSNR, unless the signal-to-noise ratio of the data is very\nlow, or we use a large number of noisy frames.\nThe following lemma shows that in contrast to our proposed\nmetric uMSE, the approximation to the MSE obtained via\naveraging is biased and not consistent, in the sense that\nit does not converge to the true MSE when the number\n13\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\n0\n1\n2\n3\n4\n5\n0.5\n1\n1.5\n|uMSE-MSE|\n10\n3\n0\n1\n2\n3\n4\n5\nStandard deviation of Gaussian smoothing filter\n0\n5\n10\nRelative RMSE\nbetween subsamplings\n10\n2\nFigure 9. The bias produced by spatial subsampling is related\nto image smoothness. The top graph shows the relative RMSE\nof the subsampled references corresponding to a natural image\n(the same as in Figure 4), after smoothing with a Gaussian filter\nwith different standard deviations. In order to evaluate the effect of\nimage smoothness on the uMSE, we fed the smoothed images con-\ntaminated by Gaussian i.i.d. noise with standard deviation equal\nto 55 into a DnCNN denoiser (as in Figure 4). The bottom graph\nshows the absolute difference between the MSE and the uMSE\nas a function of the smoothness of the underlying clean image.\nSmoothing results in a clear decrease of both the relative RMSE\nand the uMSE, suggesting that spatial subsampling is effective as\nlong as the underlying image content is sufficiently smooth with\nrespect to the pixel resolution.\nof pixels tends to infinity. The metric does converge to the\nMSE as the number of noisy images tends to infinity, but this\nis of little practical significance, since this number cannot\nbe increased arbitrarily in actual applications.\nLemma D.1 (MSE via averaging). Consider a clean signal\nx ∈Rn, an estimate f(y) ∈Rn (obtained by applying a\ndenoiser f to the data y ∈Rn), and m noisy references\n˜r[m]\ni\n:= xi + ˜z[m]\ni\n,\n1 ≤i ≤n, 1 ≤j ≤m,\n(14)\nwhere ˜z[m]\ni\n, 1 ≤i ≤n, 1 ≤j ≤m, are i.i.d. zero-mean\nGaussian random variables with variance σ2. We define the\naveraging-based MSE as\nMSEavg := 1\nn\nn\nX\ni=1\n\n1\nm\nm\nX\nj=1\n˜r[m]\ni\n−f(y)i\n\n\n2\n.\n(15)\nThe MSEavg is a biased estimator of the true MSE\nMSE := 1\nn\nn\nX\ni=1\n(xi −f(y)i)2 ,\n(16)\nsince its mean equals\nE [MSEavg] = MSE + σ2\nm .\n(17)\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nNumber of averaged images\n79\n80\n81\n82\nMSE\nuMSE\nMSE estimation\n10\n4\nMSE via averaging\nFigure 10. Comparison of averaging-based MSE and uMSE.\nThe plot shows the MSE, uMSE and averaging-based MSEavg\ncorresponding to a natural image corrupted by Gaussian (σ = 15,\nto simulate a noise level similar to that of the RAW videos in\nSection 6) and denoised with a standard deep-learning denoiser\n(DnCNN). The uMSE is computed with 3 noisy references. The\naveraging-based MSEavg is computed with different number of\nnoisy references indicated by the horizontal axis. The blue shaded\nregion corresponds to an error that is smaller or equal to the error\nincurred by the uMSE. Averaging-based MSE requires 1,510 noisy\nimages to achieve this accuracy.\nProof. By the assumptions, and linearity of expectation,\nE [MSEavg] = E\n\n1\nn\nn\nX\ni=1\n\n1\nm\nm\nX\nj=1\n˜z[m]\ni\n+ xi −f(y)i\n\n\n2\n\n= 1\nn\nn\nX\ni=1\n(xi −f(y)i)2 + 1\nn\nn\nX\ni=1\nE\n\n\n\n1\nm\nm\nX\nj=1\n˜z[m]\ni\n\n\n2\n\n= MSE + σ2\nm .\n(18)\nAs established in Section 4, the proposed uMSE metric is an\nunbiased estimator of the MSE that is consistent as n →∞\nand only requires m := 3 noisy references. Figure 10 shows\na numerical comparison between uMSE and the averaging-\nbased MSE for one of the natural images used in the ex-\nperiments of Section 5. We observe that averaging-based\nMSE requires m := 1510 in order to match the accuracy\nachieved by the uMSE with only three noisy references.\nE. Proofs\nE.1. Proof of Theorem 4.1\nThe following lemma shows that each individual term in the\nuMSE is unbiased.\nLemma E.1 (Proof in Section E.5.2). If Conditions 1 and 2\n14\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nin Section 4 hold,\nE\nh\ng\nuSEi\ni\n= SEi,\n1 ≤i ≤n.\n(19)\nThe proof then follows immediately from linearity of expec-\ntation,\nE\nh\n^\nuMSE\ni\n= E\n\"\n1\nn\nn\nX\ni=1\ng\nuSEi\n#\n= 1\nn\nn\nX\ni=1\nE\nh\ng\nuSEi\ni\n= 1\nn\nn\nX\ni=1\nSEi = MSE.\n(20)\nE.2. Proof of Theorem 4.2\nThe following lemma bounds the variance of each individual\nterm in the uMSE.\nLemma E.2 (Proof in Section E.5.3). Under the assump-\ntions of the theorem,\nVar\nh\ng\nuSEi\ni\n≤14α,\n1 ≤i ≤n.\n(21)\nThe proof then follows from the fact that the variance of a\nsum of independent random variables is equal to the sum of\ntheir variances,\nE\n\u0014\u0010\n^\nuMSE −MSE\n\u00112\u0015\n= Var\nh\n^\nuMSE\ni\n=Var\n\"\n1\nn\nn\nX\ni=1\ng\nuSEi\n#\n= 1\nn2\nn\nX\ni=1\nVar\nh\ng\nuSEi\ni\n≤14α\nn .\n(22)\nThe bound immediately implies convergence in mean square\nas n →∞, which in turn implies convergence in probabil-\nity.\nE.3. Proof of Corollary 4.3\nThe uPSNR is a continuous function of the uMSE, which\nis the same function mapping the MSE to the PSNR. The\nresult then follows from Theorem 4.2 and the continuous\nmapping theorem.\nE.4. Proof of Theorem 4.4\nTo prove Theorem 4.4, we express the uMSE as a sum of\nzero-mean random variables,\n^\nuMSE =\nn\nX\ni=1\n˜ti,\n˜ti :=\ng\nuSEi −SEi\nn\n,\n(23)\nand apply the following version of the Lyapunov central\nlimit theorem.\nTheorem E.3 (Theorem 9.2 (Breiman, 1992)). Let ˜ti, 1 ≤\ni ≤n, be independent zero-mean random variables with\nbounded second and third moments, and let\ns2\nn :=\nn\nX\ni=1\nE\n\u0002˜t2\ni\n\u0003\n.\n(24)\nIf the Lyapunov condition\nlim\nn→∞\nPn\ni=1 E\nh\f\f˜ti\n\f\f3i\ns3n\n= 0\n(25)\nholds, then the random variable\n1\nsn\nn\nX\ni=1\n˜ti\n(26)\nconverges in distribution to a standard Gaussian as n →∞.\nTo complete the proof we show that the random variable\n˜ti :=\ng\nuSEi −SEi\nn\n(27)\nsatisfies the conditions of Theorem E.3. By Lemma E.1\nits mean is zero. By Lemma E.2 its second moment is\nbounded. To control sn, we apply the following auxiliary\nlemma, which provides a lower bound on the variance of\neach term in the uMSE.\nLemma E.4. Under the assumptions of Theorem 4.4,\nVar\nh\ng\nuSEi\ni\n≥µ[4]\ni\n+ σ4\ni\n2\n,\n(28)\nwhere µ[4]\ni\nand σ2\ni denote the fourth central moment and the\nvariance of ˜ai, ˜bi and ˜ci.\nThe lemma yields a lower bound for s2\nn,\ns2\nn :=\nn\nX\ni=1\nE\n\u0002˜t2\ni\n\u0003\n= 1\nn2\nn\nX\ni=1\nE\n\u0014\u0010\ng\nuSEi −SEi\n\u00112\u0015\n= 1\nn2\nn\nX\ni=1\nVar\nh\ng\nuSEi\ni\n≥2µ[4]\ni\n+ 2σ4\nn\n.\n(29)\nThe following lemma controls the numerator in the Lya-\npunov condition, and also shows that the third moment of ˜ti\nis bounded.\nLemma E.5 (Proof in Section E.5.5). Under the assump-\ntions of Theorem 4.4, there exists a numerical positive con-\nstant D such that\nn\nX\ni=1\nE\nh\f\f˜ti\n\f\f3i\n≤Dη\nn2 .\n(30)\n15\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nCombining equation 29 and Lemma E.5, we obtain\nPn\ni=1 E\nh\f\f˜ti\n\f\f3i\ns3n\n≤\nDη\n(2µ[4]\ni\n+ 2σ4)1.5√n\n,\n(31)\nwhich converges to zero as n →∞. The Lyapunov condi-\ntion therefore holds and the proof is complete.\nE.5. Proof of auxiliary results\nE.5.1. NOTATION\nTo alleviate notation in our proofs, we define the denoising\nerror erri := f(y)i −xi and the centered random vari-\nables C(˜ai) := ˜ai −xi, C(˜bi) := ˜bi −xi and C(˜ci) :=\n˜ci −xi, which are independent, have zero mean and satisfy\nVar [˜ai] = E\n\u0002\n˜a2\ni\n\u0003\n= Var[˜bi] = E[˜b2\ni ] = Var [˜ci] = E\n\u0002\n˜c2\ni\n\u0003\n.\nE.5.2. PROOF OF LEMMA E.1\nBy linearity of expectation and the fact that the variance\nof independent random variables equals the sum of their\nvariances,\nE\nh\ng\nuSEi\ni\n= E\n\"\n(˜ai −f(y)i)2 −(˜bi −˜ci)2\n2\n#\n= E\nh\n(C(˜ai) −erri))2i\n−\nE\nh\n(C(˜bi) −C(˜ci))2i\n2\n= E\n\u0002\nC(˜ai)2\u0003\n−2erriE [C(˜ai)] + SEi\n−\nVar\nh\nC(˜bi) −C(˜ci)\ni\n2\n= Var [˜ai] + SEi\n−Var[C(˜bi)] + Var [C(˜ci)]\n2\n= Var [˜ai] + SEi −Var[˜bi] + Var [˜ci]\n2\n= SEi.\n(32)\nE.5.3. PROOF OF LEMMA E.2\nBy linearity of expectation, the fact that the variance of inde-\npendent random variables equals the sum of their variances\nand the fact that the mean square is an upper bound on the\nvariance,\nVar\nh\ng\nuSEi\ni\n= Var\nh\n(˜ai −f(y)i)2i\n+\nVar\nh\n(˜bi −˜ci)2i\n4\n≤E\nh\n(˜ai −f(y)i)4i\n+\nE\nh\n(˜bi −˜ci)4i\n4\n= E\nh\n(C(˜ai) −erri)4i\n+\nE\n\u0014\u0010\nC(˜bi) −C(˜ci)\n\u00114\u0015\n4\n≤E\n\u0002\nC(˜ai)4\u0003\n+ 4E\n\u0002\nC(˜ai)3\u0003\n|erri|\n+ 6E\n\u0002\nC(˜ai)2\u0003\nerr2\ni + err4\ni\n+\nE\nh\nC(˜bi)4i\n+ 6E\nh\nC(˜bi)2i\nE\n\u0002\nC(˜ci)2\u0003\n+\n\u0002\nC(˜ci)4\u0003\n4\n≤14α,\n(33)\nwhere we have also used the fact that µ2\n2 ≤µ[4]\ni\nby Jensen’s\ninequality, which implies\nE\n\u0002\nC(˜ai)2\u0003\nSEi ≤\nq\nµ[4]\ni γ4 ≤α,\n(34)\nE\nh\nC(˜bi)2i\nE\n\u0002\nC(˜ci)2\u0003\n≤µ[4]\ni\n≤α.\n(35)\nE.5.4. PROOF OF LEMMA E.4\nThe variance of independent random variables equals the\nsum of their variances, so\nVar\nh\ng\nuSEi\ni\n= Var\nh\n(˜ai −f(y)i)2i\n+\nVar\nh\n(˜bi −˜ci)2i\n4\n≥\nVar\nh\n(˜bi −˜ci)2i\n4\n.\n(36)\nand since the mean of ˜bi −˜ci is zero,\nE\nh\n(˜bi −˜ci)2i\n= Var[˜bi −˜ci]\n= Var[˜bi] + Var[˜ci]\n= 2σ2\ni .\n(37)\nBy the definition of variance and linearity of expectation,\nVar\nh\n(˜bi −˜ci)2i\n= E\nh\n(˜bi −˜ci)4i\n−E\nh\n(˜bi −˜ci)2i2\n= E\n\u0014\u0010\nC(˜bi) −C(˜ci)\n\u00114\u0015\n−4σ4\ni\n= E\nh\nC(˜bi)4i\n+ E\n\u0002\nC(˜ci)4\u0003\n+ 6E\nh\nC(˜bi)2C(˜ci)2i\n−4σ4\ni\n= 2µ[4]\ni\n+ 2σ4\ni .\n(38)\n16\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nE.5.5. PROOF OF LEMMA E.5\nBy linearity of expectation,\nE\n\u0014\f\f\f g\nuSEi −SEi\n\f\f\f\n3\u0015\n= E\n\n\n\f\f\f\f\f(˜ai −f(y)i)2 −(˜bi −˜ci)2\n2\n−err2\ni\n\f\f\f\f\f\n3\n\n= E\n\n\n\f\f\f\f\f(C(˜ai) −erri)2 −(C(˜bi) −C(˜ci))2\n2\n−err2\ni\n\f\f\f\f\f\n3\n\n≤E\n\u0002\n(C(˜ai) −erri)6\u0003\n+ 1\n8E\nh\n(C(˜bi) −C(˜ci))6i\n+ err6\ni\n+ 3\n2E\n\u0002\n(C(˜ai) −erri)4\u0003\nE\nh\n(C(˜bi) −C(˜ci))2i\n+ 3\n4E\n\u0002\n(C(˜ai) −erri)2\u0003\nE\nh\n(C(˜bi) −C(˜ci))4i\n+ 3E\n\u0002\n(C(˜ai) −erri)4\u0003\nerr2\ni\n+ 3E\n\u0002\n(C(˜ai) −erri)2\u0003\nerr4\ni\n+ 3\n4E\nh\n(C(˜bi) −C(˜ci))4i\nerr2\ni\n+ 3\n2E\nh\n(C(˜bi) −C(˜ci))2i\nerr4\ni\n+ 3E\n\u0002\n(C(˜ai) −erri)2\u0003\nE\nh\n(C(˜bi) −C(˜ci))2i\nerr2\ni\n≤Dη.\n(39)\nThe final bound in equation 39 is obtained by bounding each\nterm in the sum using the assumption that the maximum\nentrywise denoising error and the central moments of ˜ai, ˜bi\nand ˜ci are bounded. For example,\nE\n\u0002\n(C(˜ai) −erri)6\u0003\n=\nE\n\u0002\nC(˜ai)6\u0003\n+ err6\ni + 6E\n\u0002\nC(˜ai)5\u0003\nerri + 15E\n\u0002\nC(˜ai)4\u0003\nerr2\ni\n+ 15E\n\u0002\nC(˜ai)2\u0003\nerr4\ni + 20E\n\u0002\nC(˜ai)3\u0003\nerr3\ni .\n(40)\nFinally, by linearity of expectation we have\nn\nX\ni=1\nE\nh\f\f˜ti\n\f\f3i\n= 1\nn3\nn\nX\ni=1\nE\n\u0014\f\f\f g\nuSEi −SEi\n\f\f\f\n3\u0015\n≤Dη\nn2 .\n(41)\nF. Additional Results\nThis section contains additional results, which are not in-\ncluded in the main paper due to space constraints. They\ninclude:\n• Table 3 shows a controlled comparison of MSE and\nuMSE on clean ground-truth images, and noisy frames\nfor both natural images with additive Gaussian noise\nand TEM images with Poisson noise.\n• Table 4 shows a comparison between averaging-based\nMSE and uMSE on RAW videos with real noise de-\nscribed in Sections 6 and H.\n• Figure 11 Shows the estimates of uMSE evaluated on\nTEM data for two denoisers, described in Section 7.\n• Figure 12 Shows that uMSE provides a consistent\nordering of images that are easier/harder to denoise,\nacross different denoisers.\n• Figure 13 shows the empirical correlation of neighbor-\ning pixels in the TEM data, necessitating subsampling\nin order to evaluate the uMSE and uPSNR.\n0\n1\n2\n3\nuMSE\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\nCNN (moderate SNR)\nGaussian Smoothing (moderate SNR)\nCNN (low SNR)\nGaussian Smoothing (low SNR)\nFigure 11. uMSE for real-world electron-microscopy data. The\nfigure shows the histograms of the uMSE (computed from a single\nnoisy image via spatial subsampling) of two denoisers (Neigh-\nbor2Neighbor CNN and Gaussian smoothing) on the two test sets\ndescribed in Section 7. The uMSE discriminates between the dif-\nferent methods and test sets, in a way that is consistent with the\nvisual appearance of the denoised images.\nG. Description of Controlled Experiments\nIn this section, we describe the architectures and training\nprocedure for models used in Section 5. For our experi-\nments with natural images, we use the pre-trained weights\nreleased in (Zhang et al., 2017a) and (Mohan et al., 2020).\nAll models are trained on 180 × 180 natural images from\nthe Berkeley Segmentation Dataset (Martin et al., 2001)\nsynthetically corrupted with Gaussian noise with standard\ndeviation uniformly sampled between 0 and 100. The train-\ning set contains 400 images and is augmented via down-\nsampling, random flips, and random rotations of patches in\nthese images (Zhang et al., 2017a; Mohan et al., 2020). We\nuse the standard test set containing 68 images for evaluation.\nWe describe each of the models we use in detail below.\n1. Bilateral filter OpenCV implementation for the Bi-\nlateral filter with a filter diameter of 15 pixels and\nσvalue = σspace = 1.\n2. DnCNN. DnCNN (Zhang et al., 2017a) consists of 20\nconvolutional layers, each consisting of 3 × 3 filters\n17\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\nTable 3. Controlled comparison of MSE and uMSE. The table shows the MSE computed from clean ground-truth images, compared to\ntwo versions of the proposed estimator: one using noisy references corresponding to the same clean image (uMSE), and another using\na single noisy image combined with spatial subsampling (uMSEs). The metrics are compared on the datasets and denoising methods\ndescribed in Section G\n.\nNatural images (Gaussian noise) ·10−3\nσ = 25\nσ = 50\nσ = 75\nσ = 100\nMethod\nMSE\nuMSE\nuMSES\nMSE\nuMSE\nuMSES\nMSE\nuMSE\nuMSES\nMSE\nuMSE\nuMSES\nBilateral\n4.38\n4.4\n2.64\n6.88\n6.87\n5.26\n12.3\n12.3\n11.1\n23.5\n23.2\n22.5\nDenseNet\n2.58\n2.59\n2.11\n4.70\n4.65\n2.81\n6.23\n6.16\n4.02\n7.44\n7.44\n5.00\nDnCNN\n2.85\n2.84\n1.81\n4.72\n4.71\n2.85\n6.21\n6.24\n3.96\n7.48\n7.60\n5.05\nUNet\n2.76\n2.77\n1.91\n4.78\n4.76\n2.84\n6.32\n6.22\n3.89\n7.47\n7.68\n5.05\nElectron microscopy (Poisson noise) ·10−3\nBilateral\nBlindSpot\nDnCNN\nUNet\nMSE\nuMSE\nuMSES\nMSE\nuMSE\nuMSES\nMSE\nuMSE\nuMSES\nMSE\nuMSE\nuMSES\n9.57\n9.55\n9.54\n3.97\n4.00\n3.96\n3.00\n3.03\n2.94\n4.18\n4.10\n4.12\nTable 4. Comparison of averaging-based MSE and uMSE on RAW videos with real noise. The proposed uMSE metric, computed\nusing three noisy references, is very similar to an averaging-based PSNR estimate computed from 10 noisy references. The metrics are\ncompared on the datasets and denoising methods described in Section H. All numbers in the table are scaled by ·10−4\nImage (Wavelet)\nImage (CNN)\nVideo (Temp. Avg)\nVideo (CNN)\nISO\nMSEavg\nuMSE\nMSEavg\nuMSE\nMSEavg\nuMSE\nMSEavg\nuMSE\n1600\n1.795\n1.69\n0.284\n0.183\n5.317\n5.282\n0.234\n0.131\n3200\n2.887\n2.866\n0.379\n0.336\n8.151\n8.134\n0.267\n0.217\n6400\n5.792\n5.702\n0.686\n0.624\n10.503\n10.573\n0.433\n0.361\n12800\n15.205\n15.11\n1.22\n1.31\n19.967\n19.871\n0.741\n0.78\n25600\n23.194\n22.549\n1.63\n1.737\n21.277\n21.1\n1.052\n1.079\nMean\n9.775\n9.583\n0.84\n0.838\n13.043\n12.992\n0.545\n0.514\n18\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\n1.5\n2.0\n2.5\n3.0\n3.5\nuMSE (Gaussian Smoothing)\n0.5\n1.0\n1.5\n2.0\n2.5\nuMSE (CNN)\nFigure 12. uMSE produces consistent image-level evaluations\nacross different denoisers. We compare the uMSE estimate per\nimage for both the Neighbor2Neighbor CNN and the Gaussian\nsmoothing denoisers on the low SNR data (green and red his-\ntograms in Figure 11). While the ranges are different for each\ndenoiser (the CNN denoises more effectively), the uMSE values\nare highly correlated, indicating that uMSE provides a consistent\nevaluation of the individual images.\n0\n1\n2\n3\n4\nj\n10\n3\n10\n2\n10\n1\n100\nCorr(pi, pj)\nFigure 13. Empirical correlation of adjacent pixels in electron-\nmicroscopy data. The graph shows the correlation coefficient\nbetween a pixel and a pixel that is j pixels away for different values\nof j. The correlation coefficient is computed after subtracting\na mean computed via averaging across frames. The correlation\nbetween adjacent pixels is particularly high, so spatial subsampling\nby a factor of two substantially reduces the pixel-wise correlation.\nand 64 channels, batch normalization (Ioffe & Szegedy,\n2015), and a ReLU nonlinearity. It has a skip connec-\ntion from the initial layer to the final layer, which has\nno nonlinear units. We use the pre-trained weights\nreleased by the authors.\n3. UNet. Our UNet model (Ronneberger et al., 2015) has\nthe following layers:\n(a) conv1 - Takes in input image and maps to 32 chan-\nnels with 5 × 5 convolutional kernels.\n(b) conv2 - Input: 32 channels. Output: 32 channels.\n3 × 3 convolutional kernels.\n(c) conv3 - Input: 32 channels. Output: 64 channels.\n3 × 3 convolutional kernels with stride 2.\n(d) conv4- Input: 64 channels. Output: 64 channels.\n3 × 3 convolutional kernels.\n(e) conv5- Input: 64 channels. Output: 64 channels.\n3 × 3 convolutional kernels with dilation factor of\n2.\n(f) conv6- Input: 64 channels. Output: 64 channels.\n3 × 3 convolutional kernels with dilation factor of\n4.\n(g) conv7- Transpose Convolution layer. Input: 64\nchannels. Output: 64 channels. 4 × 4 filters with\nstride 2.\n(h) conv8- Input: 96 channels. Output: 64 channels.\n3×3 convolutional kernels. The input to this layer\nis the concatenation of the outputs of layer conv7\nand conv2.\n(i) conv9- Input: 32 channels. Output: 1 channels.\n5 × 5 convolutional kernels.\nWe use pre-trained weights released by the authors of\n(Mohan et al., 2020).\n4. DenseNet The simplified version of the DenseNet ar-\nchitecture (Huang et al., 2017) has 4 blocks in total.\nEach block is a fully convolutional 5-layer CNN with\n3 × 3 filters and 64 channels in the intermediate lay-\ners with ReLU nonlinearity. The first three blocks\nhave an output layer with 64 channels, while the last\nblock has an output layer with only one channel. The\noutput of the ith block is concatenated with the input\nnoisy image and then fed to the (i + 1)th block, so\nthe last three blocks have 65 input channels. We use\npre-trained weights released by the authors of (Mohan\net al., 2020).\n5. Noise2Noise Introduced in (Lehtinen et al., 2018), this\nmethod proposes training a denoiser by using pairs of\nindependent noisy realizations of the same image as\ninput and target of a CNN. We apply this method using\nthe UNet architecture described above (3).\n19\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\n6. Noise2Self This method introduced in (Batson &\nRoyer, 2019b) proposes unsupervised denoisers by\nmasking a grid of pixels in the original image and\nreplacing their value by the average if its four neigh-\nbouring pixels. The loss function is the MSE between\nthe denoised output and the original noisy image, only\ntaking into account the masked positions. We apply this\nmethod using the UNet architecture described above\n(3).\n7. Neighbor2Neighbor Based on Noise2Noise, (Huang\net al., 2021) introduces an approach that obtains image\npairs by randomly down-sampling single images. This\nis done ensuring that pixels located in the same posi-\ntion in the sub-samplings are direct neighbors in the\noriginal image. We apply this method using the UNet\narchitecture described above (3).\n8. BlindSpot We use a BlindSpot UNet architecture from\n(Sheth et al., 2021) with the following layers:\n(a) conv1 - Takes in input image and maps to 48 chan-\nnels with 9 × 9 convolutional kernel with 90◦\nrotation symmetry and the central pixel blinded.\n(b) conv2-6 - Input: 48 channels. Output: 48 chan-\nnels. 9 × 9 convolutional kernels with 90◦rota-\ntion symmetry and the central pixel blinded, with\nstride 2.\n(c) conv7-11 - Input: 96 channels. Output: 48 chan-\nnels. 9×9 convolutional kernels with 90◦rotation\nsymmetry and the central pixel blinded, with dila-\ntion factor of 2.\n(d) conv12- Input: 96 channels. Output: 1 channel.\n9 × 9 convolutional kernel with 90◦rotation sym-\nmetry and the central pixel blinded.\nFor our experiments with electron microscopy data, we\nuse the simulated dataset of Pt nanoparticles introduced\nin (Mohan et al., 2022). Specifically, we used a subset of\n5,583 images corresponding to white contrast (the simu-\nlated dataset is divided into white, black and intermediate\ncontrast by a domain expert, see (Mohan et al., 2022) for\nmore details). 90% of the data were used for training. The\nremaining 559 images were evenly split into validation and\ntest sets. The UNet architecture used in these experiments\nis the one introduced in (Mohan et al., 2022) with 4 scales\nand 32 base channels. In addition to bilateral filter, UNet,\nand DnCNN models described for natural images, we used\na blindspot based network. BlindSpot (Laine et al., 2019)\nis a CNN which is constrained to predict the intensity of a\npixel as a function of the noisy pixels in its neighbourhood,\nwithout using the pixel itself. Following (Laine et al., 2019;\nSheth et al., 2021), we use a UNet architecture as the model\nbackbone.\nH. Description of Experiments with Videos in\nRAW Format\nAs explained in Section 6, the dataset contains 11 unique\nvideos, each containing 7 frames, captured at five different\nISO levels using a surveillance camera. Each video has 10\ndifferent noise realizations per frame, which are averaged\nto obtain an estimated clean version of the video. Following\n(Sheth et al., 2021), we perform evaluation on five videos\nfrom the test test.\nThe methods we use:\n1. Image Denoiser (Wavelet).\nWe use Daubechies\nwavelet to perform denoising, which is the default\nchoice in skimage.restoration, a widely used image\nrestoration package. We implement denoising using\nthe function denoise wavelet() from the package using\nthe default options. We set sigma=0.01.\n2. Image Denoiser (CNN). We perform image denoising\nby re-purposing the video denoiser (UDVD) trained for\nRAW videos in Ref. (Sheth et al., 2021). UDVD takes\nin five consecutive frames, and output the denoised\nimage corresponding to the frame in the middle. To\nsimulate image denoising using UDVD, we repeat the\nsame frame 5 times (i.e, all frames are the same image),\nand provide it as input to the trained network.\n3. Video Denoiser (Temp. Avg.). We use 5 consecutive\nframes to compute the denosied image corresponding\nto the middle frame. We assign a weight of 0.75 to the\nmiddle noisy frame, 0.1 to each of the previous and\nnext frame, and 0.025 to the rest of the two frames.\n4. Video Denoiser (CNN). We use the unsupervised\nvideo denoiser (UDVD) trained for RAW videos in\nRef. (Sheth et al., 2021). As explained, UDVD takes in\nfive consecutive frames, and output the denoised image\ncorresponding to the frame in the middle. We use the\npre-trained weights, and follow the experimental setup\ndescribed in Ref. (Sheth et al., 2021).\nWe use the pre-trained weights released by the authors of\n(Sheth et al., 2021) as our image and video denoiser. These\nweights are obtained by training UDVD on the first 9 real-\nizations of the 5 videos from the test set of the raw video\ndataset, holding out the last realization for early stopping\n(see (Sheth et al., 2021) for more details).\nI. Description of Experiments on Electron\nMicroscopy Data\nData acquisition: The dataset contains TEM images of\nPt nanoparticles on a CeO2 substrate. An electron beam\ninteracts with the sample, and then its intensity is recorded\n20\nEvaluating Unsupervised Denoising Requires Unsupervised Metrics\non an imaging plane by the detector. The pixel intensity\napproximately follows a Poisson distribution with parameter\nequal to the intensity of the electron beam.\nThe data were recorded at room temperature at a pressure of\n∼10−6 Torr. The electron beam intensity was 600e/ ˚A\n2/s.\nThe instances are part of 25 videos, taken at a frame rate\nof 75 frames per second. The instances show Pt particles\nin the size range 1 - 5 nm. In a subset of frame series, the\nparticles become unstable and undergo structural dynamic\nre-arrangements. The periods of instability are punctuated\nby periods of relative stability. Consequently, the nanopar-\nticles show a variety of different sizes and shapes and are\nalso viewed along many different crystallographic direc-\ntions. Data were collected using a FEI Titan ETEM in\nEFTEM mode, Gatan Tantalum hot stage, K3 camera in\nCDS counting mode.\nPixel-wise correlation: Our proposed unsupervised metrics\nrely on the assumption that the noise is pixel-wise indepen-\ndence. This is not the case for this dataset, as shown in Fig-\nure 13.We address this by performing spatial subsampling\nby a factor of two, which reduces the pixel-wise correla-\ntion by an order of magnitude. After this, some frames still\npresent relatively high pixel-wise correlations. We therefore\nselect the test sets from two sets of contiguous frames with\nlow correlation.\nTraining and test sets: The data were divided into three\nsets: training & validation set, consisting of 70% of the data,\nand two contiguous test sets with pixel-wise correlation:\none containing 155 images with moderate signal-to-noise\nratio (SNR), and one containing 383 images with low SNR,\nwhich are more challenging. The moderate SNR test set\nis interspersed with the training and validation sets, and\ncontains frames similar to those used to train the network.\nThe low SNR test set contains frames which are temporally\nseparated from the training and validation sets, and contains\nnanoparticle with different structures.\nDenoisers We compare the performance of four denois-\ners: (1) a convolutional neural network based on Neigh-\nbor2Neighbor with a UNet architecture as in (Huang\net al., 2021); (2) a convolutional neural network based\non Noise2Self with a UNet architecture as in (Batson &\nRoyer, 2019b); (3) a convolutional neural network based on\nBlindSpot with a single-frame same architecture based on\n(Sheth et al., 2021); (2) Gaussian smoothing with standard\ndeviation σ = 25.\nCNN training parameters The Neighbor2Neighbor and\nNoise2Self CNNs were based on a basic UNet archi-\ntecture with 5 double convolution hidden layers of size\n[64, 128, 256, 512, 1024] and were trained using the sub-\nsampling and masking functions provided in (Huang et al.,\n2021; Batson & Royer, 2019b) respectively. They were\ntrained for 1000 epochs using an Adam optimizer with an\ninitial learning rate of 0.001, and scheduled reduction of the\nlearning rate every 100 epochs. The networks have a total\nof 17,261,824 parameters.\nThe BlindSpot model uses a BlindUNet (Sheth et al., 2021)\narchitecture with 5 convolutional hidden layers and 48 chan-\nnels. It was trained for 1000 epochs using an Adam opti-\nmizer with an initial learning rate of 0.001, and scheduled\nreduction of the learning rate every 100 epochs. The net-\nwork has a total of 1,263,984 parameters.\n21\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2022-10-11",
  "updated": "2023-05-30"
}