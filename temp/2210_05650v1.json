{
  "id": "http://arxiv.org/abs/2210.05650v1",
  "title": "Regret Bounds for Risk-Sensitive Reinforcement Learning",
  "authors": [
    "O. Bastani",
    "Y. J. Ma",
    "E. Shen",
    "W. Xu"
  ],
  "abstract": "In safety-critical applications of reinforcement learning such as healthcare\nand robotics, it is often desirable to optimize risk-sensitive objectives that\naccount for tail outcomes rather than expected reward. We prove the first\nregret bounds for reinforcement learning under a general class of\nrisk-sensitive objectives including the popular CVaR objective. Our theory is\nbased on a novel characterization of the CVaR objective as well as a novel\noptimistic MDP construction.",
  "text": "Regret Bounds for Risk-Sensitive\nReinforcement Learning\nOsbert Bastani\nUniversity of Pennsylvania\nobastani@seas.upenn.edu\nYecheng Jason Ma\nUniversity of Pennsylvania\njasonyma@seas.upenn.edu\nEstelle Shen\nUniversity of Pennsylvania\npixna@sas.upenn.edu\nWanqiao Xu\nStanford University\nwanqiaox@stanford.edu\nAbstract\nIn safety-critical applications of reinforcement learning such as healthcare and\nrobotics, it is often desirable to optimize risk-sensitive objectives that account for\ntail outcomes rather than expected reward. We prove the ﬁrst regret bounds for\nreinforcement learning under a general class of risk-sensitive objectives including\nthe popular CVaR objective. Our theory is based on a novel characterization of the\nCVaR objective as well as a novel optimistic MDP construction.\n1\nIntroduction\nThere has been recent interest in risk-sensitive reinforcement learning, which replaces the usual\nexpected reward objective with one that accounts for variation in possible outcomes. One of the\nmost popular risk-sensitive objectives is the conditional value-at-risk (CVaR) objective [1, 2, 3, 4],\nwhich is the average risk at some tail of the distribution of returns (i.e., cumulative rewards) under a\ngiven policy [5, 6]. More generally, we consider a broad class of objectives in the form of a weighted\nintegral of quantiles of the return distribution, of which CVaR is a special case.\nA key question is providing regret bounds for risk-sensitive reinforcement learning. While there has\nbeen some work studying this question, it has focused on a speciﬁc objective called the entropic risk\nmeasure [7, 8], leaving open the question of bounds for more general risk-sensitive objectives. There\nhas also been work on optimistic exploration for CVaR [9], but without any regret bounds.\nWe provide the ﬁrst regret bounds for risk-sensitive reinforcement learning with objectives of form\nΦ(π) =\nZ 1\n0\nF †\nZ(π)(τ) · dG(τ),\n(1)\nwhere Z(π) is the random variable encoding the return of policy π, FZ(π) is its quantile function\n(roughly speaking, the inverse CDF), and G is a weighting function over the quantiles. This class\ncaptures a broad range of useful objectives, and has been studied in prior work [10, 4].\nWe focus on the episodic setting, where the agent interacts with the environment, modeled by a\nMarkov decision process (MDP), over a ﬁxed sequence of episodes. Its goal is to minimize the\nregret—i.e., the gap between the objective value it achieves compared to the optimal policy. Our\napproach is based on the upper conﬁdence bound strategy [11, 12], which makes decisions according\nto an optimistic estimate of the MDP. We prove that this algorithm (denoted A) has regret\nregret(A) = ˜O\n\u0010\nT 2 · LG · |S|3/2 · |A| ·\n√\nK\n\u0011\n,\nPreprint. Under review.\narXiv:2210.05650v1  [cs.LG]  11 Oct 2022\nwhere T is the length of a single episode, LG is the Lipschitz constant for the weighting function G,\n|S| is the number of states in the MDP, |A| is the number of actions, and K is the number of episodes\n(Theorem 4.1). Importantly, it achieves the optimal rate\n√\nK achievable for typical expected return\nobjectives (which is a lower bound in our setting since expected return is an objective in the class we\nconsider, taking G(τ) = τ). For CVaR objectives, we have LG = 1/α, where α is the size of the tail\nconsidered—e.g., when α is small, it averages over outliers with particularly small return.\nThe main challenge behind proving our result is bounding the gap between the objective value for the\nestimated MDP and the true MDP. In particular, even if we have a uniform bound ∥F ˆ\nZ(π) −FZ(π)∥∞\non the CDFs of the estimated return ˆZ(π) and the true return Z(π), we need to translate this to a\nbound on the corresponding objective values. To do so, we prove that equivalently, we have\nΦ(π) = 2T −\nZ\nR\nG(FZ(π)(x)) · dx.\nThis equivalent expression for Φ follows by variable substitution and integration by parts when FZ(π)\nis invertible (so F †\nZ(π)(τ) = F −1\nZ(π)(τ)), but the general case requires signiﬁcantly more care. We\nshow that it holds for an arbitrary CDF FZ(π).\nIn addition to our regret bound, we provide several other useful results for MDPs with risk-sensitive\nobjectives. In particular, optimal policies for risk-sensitive objectives may be non-Markov. For\nCVaR objectives, it is known that the optimal policy only needs to depend on the cumulative return\naccrued so far [13]. We prove that this holds in general for objectives of the form (1) (Theorem 3.1).\nFurthermore, the cumulative return so far is a continuous component; we prove that discretizing this\ncomponent yields an arbitrarily close approximation of the true MDP (Theorem 3.2).\nRelated work. To the best of our knowledge, the only prior work on regret bounds for risk-sensitive\nreinforcement learning is speciﬁc to the entropic risk objective [7, 8]:\nJ(π) = 1\nβ log EZ(π)\nh\neβZ(π)i\n,\nwhere β ∈R>0 is a hyperparameter. As β →0, this objective recovers the expected return objective;\nfor β < 0, it encourages risk aversion by upweighting negative returns; and for β > 0, it encourages\nrisk seeking behaviors by upweighting positive returns. This objective is amenable to theoretical\nanalysis since the value function satisﬁes a variant of the Bellman equation called the exponential\nBellman equation; however, it is a narrow family of risk measures and is not widely used in practice.\nIn contrast, we focus on a much broader class of risk measures including the popular CVaR objec-\ntive [1], which is used to minimize tail losses. To the best of our knowledge, we provide the ﬁrst\nregret bounds for the CVaR objective and for the wide range of objectives given by (1).\n2\nProblem Formulation\nMarkov decision process. We consider a Markov decision process (MDP) M = (S, A, D, P, P, T),\nwith ﬁnite state space S, ﬁnite action space A, initial state distribution D(s), ﬁnite time horizon\nT, transition probabilities P(s′ | s, a), and reward measure PR(s,a); without loss of generality, we\nassume r ∈[0, 1] with probability one. A history is a sequence\nξ ∈Z =\nT[\nt=1\nZt\nwhere\nZt = (S × A × R)t−1 × S\nIntuitively, a history captures the interaction between an agent and M up to step t. We consider\nstochastic, time-varying, history-dependent policies πt(at | ξt), where t is the time step. Given π, the\nhistory Ξ(π)\nt\ngenerated by π up to step t is a random variable with probability measure\nPΞ(π)\nt (ξt) =\n(\nD(s1)\nif t = 1\nPΞ(π)\nt−1(ξt−1) · πt(at | ξt−1) · PR(st,at)(rt) · P(st+1 | st, at)\notherwise,\nwhere for all τ ∈[T] we use the notation\nξτ = ((s1, a1, r1), ..., (sτ−1, aτ−1, rτ−1), sτ).\n2\nFinally, an episode (or rollout) is a history ξ ∈ZT of length T generated by a given policy π.\nBellman equation. The return of π on step t is the random variable (Z(π)\nt\n(ξt))(ξT ) = PT\nτ=t rt,\nwhere ξT ∼PΞ(π)\nT (· | Ξ(π)\nt\n= ξt)—i.e., it is the reward from step t given that the current history is ξt.\nDeﬁning Z(π)\nT +1(ξ, s) = 0, the distributional Bellman equation [14, 9] is\nFZ(π)\nt\n(ξ)(x) =\nX\na∈A\nπt(a | ξ)\nX\ns′∈S\nP(s′ | S(ξ), a)\nZ\nFZ(π)\nt+1(ξ◦(a,r,s′))(x −r) · dPR(s,a)(r),\nwhere S(ξ) = s for ξ = (..., s) is the current state in history ξ, and FX is the cumulative distribution\nfunction (CDF) of random variable X. Finally, the cumulative return of π is Z(π) = Z(π)\n1\n(ξ), where\nξ = (s) ∈Z1 for s ∼D is the initial history; in particular, we have\nFZ(π)(x) =\nZ\nFZ(π)\n1\n(ξ)(x) · dD(s).\nRisk-sensitive objective. The quantile function of a random variable X is\nF †\nX(τ) = inf {x ∈R | FX(x) ≥τ} .\nNote that if FX is strictly monotone, then it is invertible and we have F †\nX(τ) = F −1\nX (τ). Now, our\nobjective is given by the Riemann-Stieljes integral\nΦM(π) =\nZ 1\n0\nF †\nZ(π)(τ) · dG(τ),\nwhere G(τ) is a given CDF over quantiles τ ∈[0, 1]. This objective was originally studied in [15]\nfor the reinforcement learning setting. For example, choosing G(τ) = min{τ/α, 1} (i.e., the CDF\nof the distribution Uniform([0, α])) for α ∈[0, 1] yields the α-conditional value at risk (CVaR)\nobjective; furthermore, taking α = 1 yields the usual expected cumulative reward objective. In\naddition, choosing G(τ) = 1(τ ≤α) for α ∈[0, 1] yields the α value at risk (VaR) objective. Other\nrisk sensitive-objectives can also be captured in this form, for example the Wang measure [16], and\nthe cumulative probability weighting (CPW) metric [17]. We call any policy\nπ∗\nM ∈arg max\nπ\nΦM(π).\nan optimal policy—i.e., it maximizes the given objective for M.\nAssumptions. First, we have the following assumption on the quantile function for Z(π):\nAssumption 2.1. F †\nZ(π)(1) = T.\nSince T is the maximum reward attainable in an episode, this assumption says that the maximum\nreward is attained with some nontrivial probability. This assumption is very minor; for any given\nMDP M, we can modify M to include a path achieving reward T with arbitrarily low probability.\nAssumption 2.2. G is LG-Lipschitz continuous for some LG ∈R>0, and G(0) = 0.\nFor example, for the α-CVaR objective, we have LG = 1/α.\nAssumption 2.3. We are given an algorithm for computing π∗\nM for a given MDP M.\nFor CVaR objectives, existing algorithms [13] can compute π∗\nM with any desired approximation error.\nFor completeness, we give a formal description of the procedure in Appendix D. When unambiguous,\nwe drop the dependence on M and simply write π∗.\nFinally, our goal is to learn while interacting with the MDP M across a ﬁxed number of episodes K.\nIn particular, at the beginning of each episode k ∈[K], our algorithm chooses a policy π(k) = A(Hk),\nwhere Hk = {ξT,κ}k−1\nκ=1 is the random set of episodes observed so far, to use for the duration of\nepisode k. Then, our goal is to design an algorithm A that aims to minimize regret, which measures\nthe expected sub-optimality with respect to π∗:\nregret(A) = E\n\nX\nk∈[K]\nΦ(π∗) −Φ(π(k))\n\n.\nFinally, for simplicity, we assume that the initial state distribution D is known; in practice, we can\nremove this assumption using a standard strategy.\n3\n3\nOptimal Risk-Sensitive Policies\nIn this section, we characterize properties of the optimal risk-sensitive policy π∗\nM. First, we show\nthat it sufﬁces to consider policies dependent on the current state and the cumulative rewards\nobtained so far, rather than the entire history. Second, the cumulative reward is a continuous quantity,\nmaking it difﬁcult to compute the optimal policy; we prove that discretizing this component does\nnot signiﬁcantly reduce the objective value. For CVaR objectives, these results imply that existing\nalgorithms can be used to compute the optimal risk-sensitive policy [13].\nAugmented state space. We show there exists an optimal policy π∗\nt (at | yt, st) that only depends\non the current state st and cumulative reward yt = J(ξt) = Pt−1\nτ=1 rτ obtained so far. To this end, let\nZt(y, s) = {ξ ∈Zt | J(ξt) ≤y ∧st = s}\nbe the set of length t histories ξ with cumulative reward at most y so far, and current state s. For any\nhistory-dependent policy π, deﬁne the alternative policy ˜π by\n˜πt(at | ξt) = EΞ(π)\nt\nh\nπt(at | Ξ(π)\nt\n)\n\f\f\f Ξ(π)\nt\n∈Zt(J(ξt), st)\ni\n.\nNote that ˜π only depends on ξt through yt = J(ξt) and st, we can deﬁne ˜πt(at | ξt) = ˜πt(at | yt, st).\nTheorem 3.1. For any policy π, we have Φ(˜π) = Φ(π).\nWe give a proof in Appendix A. In particular, given any optimal policy π∗, we have Φ(˜π∗) = Φ(π∗);\nthus, we have ˜π∗∈arg maxπ Φ(π). Finally, we note that this result has already been shown for\nCVaR objectives [13]; our theorem generalizes the existing result to any risk-sensitive objective that\ncan be expressed as a weighted integral of the quantile function.\nAugmented MDP. As a consequence of Theorem 3.1, it sufﬁces to consider the augmented MDP\n˜\nM = ( ˜S, A, ˜D, ˜P, ˜P, T). First, ˜S = S × R is the augmented state space; for a state (s, y) ∈˜S, the\nﬁrst component encodes the current state and the second encodes the cumulative rewards so far. The\ninitial state distribution is a probability measure\n˜D((s, y)) = D(s) · δ0(y),\nwhere δ0 is the Dirac delta measure placing all probability mass on y = 0 (i.e., the cumulative reward\nso far is initially zero). The transitions are given by the product measure\n˜P((s′, y′) | (s, y), a) = P(s′ | s, a) · PR(s,a)(y′ −y),\ni.e., the second component of the state space is incremented as y′ = y + r, where r is the reward\nachieved in the original MDP. Finally, the rewards are now only provided on the ﬁnal step:\nPRt((s,y),a)(r) =\n\u001aδy(r)\nif t = T\n0\notherwise,\ni.e., the reward at the end of a rollout is simply the cumulative reward so far, as encoded by the\nsecond component of the state. By Theorem 3.1, it sufﬁces to compute the optimal policy for ˜\nM over\nhistory-independent policies πt(at | ˜st):\nmax\nπ∈Πind Φ ˜\nM(π) = max\nπ\nΦM(π),\nwhere Πind is the set of history-independent policies. Once we have π∗\n˜\nM, we can use it in M by\ndeﬁning πM(a | ξ, s) = π∗\n˜\nM(a | J(ξ), s).\nDiscretized augmented MDP. Planning over ˜\nM is complicated by the fact that the second compo-\nnent of its state space is continuous. Thus, we consider an η-discretization of ˜\nM, for some η ∈R>0.\nTo this end, we modify the reward function so that it only produces rewards in η ·N = {η ·n | n ∈N},\nby always rounding the reward up. Then, sums of these rewards are contained in η · N, so we can\nreplace the second component of ˜S with η · N. In particular, we consider the discretized MDP\nˆ\nM = ( ˆS, A, ˜D, ˆP, ˜P, T), where ˆS = S × (η · N), and transition probability measure\nˆP((s′, y′) | (s, y), a) = P(s′ | s, a) · (PR(s,a) ◦φ−1)(y′ −y)\n4\nwhere φ(r) = η · ⌈r/η⌉. That is, PR(s,a) is replaced with the pushforward measure PR(s,a) ◦φ−1,\nwhich gives reward η · i with probability PR(s,a)[η · (i −1) < r ≤η · i].\nNow, we prove that the optimal policy π∗\nˆ\nM for the discretized augmented MDP ˆ\nM achieves objective\nvalue close to the optimal policy π∗\nM for the original MDP M. Importantly, we want to consider\nmeasure performance of both policies based on the objective ΦM of the original MDP M. To do so,\nwe need a way to use π∗\nˆ\nM in M. Note that π∗\nˆ\nM depends only on the state ˆs = (s, y), where s ∈S\nis a state of the original MDP M, and y ∈η · N is a discretized version of the cumulative reward\nobtained so far. Thus, we can run π∗\nˆ\nM in M by simply rounding the reward rt at each step t up to\nthe nearest value ˆrt ∈η · N at each step—i.e., ˆrt = φ(rt); then, we increment the internal state as\nyt = yt−1 + ˆrt. We call the resulting policy πM the version of π∗\nˆ\nM adapted to M. Then, our next\nresult says that the performance of πM is not too much worse than the performance of π∗\nM.\nTheorem 3.2. Let π∗\nˆ\nM be the optimal policy for the discretized augmented MDP ˆ\nM, let πM be the\npolicy π∗\nˆ\nM adapted to the original MDP M, and let π∗\nM be the optimal (history-dependent) policy\nfor the original MDP M. Then, we have\nΦM(πM) ≥ΦM(π∗\nM) −η.\nWe give a proof in Appendix B. Note that we can set η to be sufﬁciently small to achieve any desired\nerror level (i.e., choose ϵ/T, where ϵ is the desired error). The only cost is in computation time.\nNote that the number of states in ˆ\nM is still inﬁnite; however, since the cumulative return satisﬁes\ny ∈[0, H], it sufﬁces to take ˆS = S × (ϵ · [⌈H/η⌉]); then, ˆ\nM has | ˆS| = |S| · ⌈H/η⌉states.\n4\nUpper Conﬁdence Bound Algorithm\nHere, we present our upper conﬁdence bound (UCB) algorithm (summarized in Algorithm 1). At a\nhigh level, for each episode, our algorithm constructs an estimate M(k) of the underlying MDP M\nbased on the prior episodes i ∈[k −1]; to ensure exploration, it optimistically inﬂates the estimate of\nthe reward probability measure P. Then, it plans in M(k) to obtain an optimistic policy π(k) = π∗\nM(k),\nand uses this policy to act in the MDP for episode k.\nOptimistic MDP. We deﬁne M(k). Without loss of generality, we assume S includes a distinguished\nstate s∞with rewards FR(s∞,a)(r) = 1(r ≥1) (i.e., achieve the maximum reward r = 1 with\nprobability one), and transitions P(s∞| s, a) = 1(s = s∞) and P(s′ | s∞, a) = 1(s′ = s∞) (i.e.,\ninaccessible from other states and only transitions to itself). Our construction of ˆ\nM(k) uses s∞for\noptimism. Now, let ˜\nM(k) be the MDP using the empirical estimates of the transitions and rewards:\n˜P (k)(s′ | s, a) = Nk,t(s, a, s′)\nNk,t(s, a)\nF ˜\nR(k)(s,a)(r) =\n1\nNk,t(s, a)\nk−1\nX\ni=1\nT\nX\nt=1\n1(r ≤ri,t) · 1 (si,t = s ∧ai,t = a) .\nThen, let ˆ\nM(k) be the optimistic MDP; in particular, its transitions\nˆP (k)(s′ | s, a) =\n\n\n\n\n\n1(s′ = s∞)\nif s = s∞\n1 −P\ns′∈S\\{s∞} ˜P (k)(s′ | s, a)\nif s′ = s∞\nmax\nn\n˜P (k)(s′ | s, a) −ϵ(k)\nR (s, a), 0\no\notherwise\ntransition to the optimistic state s∞when uncertain, and its rewards\nF ˆ\nR(k)(s,a)(r) =\n\n\n\n\n\n1(r ≥1)\nif s = s∞\n1\nif r ≥1\nmax\nn\nF ˜\nR(k)(s,a)(r) −ϵ(k)\nR (s, a), 0\no\notherwise\noptimistically shift the reward CDF downwards. Here, ϵ(k)\nP (s, a) and ϵ(k)\nR (s, a) are deﬁned in\nSection 5; intuitively, they are high-probability upper bounds on the errors of the empirical estimates\n˜P (k)(· | s, a) and F ˜\nR(k)(s,a) of the transitions and rewards, respectively.\n5\nAlgorithm 1 Upper Conﬁdence Bound Algorithm\n1: for k ∈[K] do\n2:\nCompute M(k) and π(k) = π∗\nM(k) using prior episodes {ξ(i) | i ∈[k −1]}\n3:\nExecute π(k) in the true MDP M and observe episode ξ(k) = [(sk,t, ak,t, rk,t)]T\nt=1 ∪[sk,T +1]\n4: end for\nTheoretical guarantees. We have the following upper bound on the regret of Algorithm 1.\nTheorem 4.1. Denote Algorithm 1 by A. For any δ ∈(0, 1], with probability at least 1 −δ, we have\nregret(A) ≤4T 3/2 · LG · |S| ·\ns\n5|S| · |A| · K · log\n\u00124|S| · |A| · K\nδ\n\u0013\n= ˜O(\n√\nK).\nWe brieﬂy compare our bound to existing ones in the setting of expected return objectives. The\ndependence on the number of episodes K matches existing bounds [11, 12]; since this is optimal in\nthe setting of expected return, and expected return is a special case of our setting (with G(τ) = τ),\nour bound is also optimal in K. In terms of the dependence on the number of states |S|, our bound\nhas an extra\np\n|S| factor compared to the UCRL2 algorithm [11], and an extra |S| factor compared\nto the improved bound of the UCBVI algorithm [12]. One extra\np\n|S| comes from down-shifting\ntransitions uniformly in the construction of the optimistic MDP ˆ\nM(k). This\np\n|S| may be removed\nby a more careful construction of the optimistic MDP. Another extra\np\n|S| compared to UCBVI\ncomes from bounding the estimation error of the reward distribution. We believe it may be possible\nto remove this\np\n|S| through a more careful treatment of the estimation error, similar to the one in\nUCBVI. We leave both of these potential reﬁnements to future work.\nIn terms of the dependence on the number of actions |A|, our bound matches the order of\np\n|A|\nin both UCRL2 and UCBVI. Our dependence on the horizon length T is T 3/2, compared to the\nsame order of T 3/2 in UCBVI and T in a variant of UCBVI [12] utilizing a carefully designed\nvariance-based bonus.\n5\nProof of Theorem 4.1\nWe prove Theorem 4.1; we defer proofs of several lemmas to Appendix C.\nAt a high level, the proof proceeds in three steps. First, we prove our key Lemma 5.1, which expresses\nthe objective Φ in terms of an integral of the weighted CDF of the return. This lemma allows us to\ntranslate bounds on the difference between CDFs of the estimated return ˆZ(π) and the true return\nZ(π) into bounds on the difference between corresponding objective values. The proof of this lemma\nis divided into three parts that deal with different sets of points in the domain of the quantile function\nF †\nZ(π): (i) discontinuous; (ii) continuous and strictly monotone; (iii) continuous and non-strictly\nmonotone. This result is used throughout the remainder of the proof.\nSecond, we deﬁne E to be the event where the optimistic estimated MDP ˆ\nM(k) falls into a certain\nconﬁdence set around the true MDP M for each k ∈[K]; in Lemma 5.2, we prove that E holds with\nhigh probability. Then, in Lemma 5.6, we prove that under event E, the objective values of ˆ\nM(k) and\nM are close. To prove this lemma, we separately show that (i) the objective values of the estimated\nMDP ˜\nM(k) (estimated without optimism) and M are close (Lemma 5.4), and (ii) the objective values\nof ˆ\nM(k) and ˜\nM(k) are close (Lemma 5.5).\nThird, in Lemma 5.7, we prove that under event E, the MDP ˆ\nM(k) is indeed optimistic. Together,\nthese results imply the regret bound using the standard UCB proof strategy.\nWe proceed with the proof. First, we have our key result providing an equivalent expression for Φ:\nLemma 5.1. We have\nΦ(π) = T −\nZ\nR\nG(FZ(π)(x)) · dx.\n6\nProof. First, note that by integration by parts, we have\nΦ(π) =\nZ 1\n0\nF †\nZ(π)(τ) · dG(τ) =\nh\nF †\nZ(π)(τ) · G(τ)\ni1\n0 −\nZ 1\n0\nG(τ) · dF †\nZ(π)(τ)\n= T −\nZ 1\n0\nG(τ) · dF †\nZ(π)(τ),\nwhere the last line follows by Assumptions 2.1 & 2.2. Thus, it sufﬁces to show that\nZ 1\n0\nG(τ) · dF †\nZ(π)(τ) =\nZ\nR\nG(FZ(π)(x)) · dx.\nThe quantile function F †\nZ(π) is monotonically increasing and left-continuous [18], so this integral is\nequivalently a Lebesgue-Stieltjes integral [19]. Dividing the unit interval I = [0, 1] into disjoint sets\nI(1) = {τ ∈I | F †\nZ(π)(τ) is discontinuous}\nI(2) = {τ ∈I | F †\nZ(π)(τ) is continuous and strictly monotone}\nI(3) = {τ ∈I | F †\nZ(π)(τ) is continuous and non-strictly monotone},\nthen we have\nZ 1\n0\nG(τ) · dF †\nZ(π)(τ) =\nZ\nI(1) G(τ) · dF †\nZ(π)(τ) +\nZ\nI(2) G(τ) · dF †\nZ(π)(τ) +\nZ\nI(3) G(τ) · dF †\nZ(π)(τ).\nWe consider each of the three terms separately and then combine them to ﬁnish the proof.\nFirst term. Note that I(1) = {τ (1)\ni\n}∞\ni=1 is countable since monotone functions can have countably\nmany discontinuities. Also, for each i ∈N, the measure assigned to τ (1)\ni\nby dF †\nZ(π) is\ndF †\nZ(π)({τ (1)\ni\n}) =\nlim\nτ→τ (1)\ni\n+\nF †\nZ(π)(τ) −F †\nZ(π)(τ (1)\ni\n) =: x(1)+\ni\n−x(1)\ni .\nThus, we have\nZ\nI(1) G(τ) · dF †\nZ(π)(τ) =\n∞\nX\ni=1\nG(τ (1)\ni\n) · (x(1)+\ni\n−x(1)\ni ) =\n∞\nX\ni=1\nG(τ (1)\ni\n) ·\nZ x(1)+\ni\nx(1)\ni\ndx\n=\n∞\nX\ni=1\nZ x(1)+\ni\nx(1)\ni\nG(FZ(π)(x)) · dx\n=\n∞\nX\ni=1\nZ\nF −1\nZ(π)({τ (1)\ni\n})\nG(FZ(π)(x)) · dx\n=\nZ\nF −1\nZ(π)(I(1))\nG(FZ(π)(x)) · dx.\nOn the second line, we have used the fact that FZ(π)(x) = τ (1)\ni\nfor all x ∈[x(1)\ni , x(1)+\ni\n). To see this\nfact, note that since x ≥x(1)\ni , by monotonicity of FZ(π), we have FZ(π)(x) ≥FZ(π)(x(1)\ni ) = τ (1)\ni\n.\nFurthermore, if FZ(π)(x) > τ (1)\ni\n, then we would have\nx(1)+\ni\n=\nlim\nτ→τ (1)\ni\n+\nF †\nZ(π)(τ) =\nlim\nτ→τ (1)+\ni\ninf{x′ ∈R | FZ(π)(x′) ≥τ}\n≤inf{x′ ∈R | FZ(π)(x′) ≥FZ(π)(x)}\n≤x,\nwhere the ﬁrst inequality follows since FZ(π)(x) ≥τ for τ sufﬁciently close to τ (1)\ni\n, and the second\nsince x ∈{x′ ∈R | FZ(π)(x′) ≥FZ(π)(x)}. Since we have assumed x < x(1)+\ni\n, we have a\n7\ncontradiction, so FZ(π)(x) ≤τ (1)\ni\n. Thus, it follows that FZ(π)(x) = τ (1)\ni\n, as claimed. The third line\nfollows since\nF −1\nZ(π)({τ (1)\ni\n}) = [x(1)\ni , x(1)+\ni\n)\nor\nF −1\nZ(π)({τ (1)\ni\n}) = [x(1)\ni , x(1)+\ni\n].\nIn particular, for any x ∈F −1\nZ(π)({τ (1)\ni\n}), we have FZ(π)(x) = τ (1)\ni\n, so\nx ≥inf{x ∈R | FZ(π)(x) ≥τ (1)\ni\n} = x(1)\ni .\nConversely, we have\nx(1)+\ni\n=\nlim\nτ→τ (1)\ni\n+\nF †\nZ(π)(τ) =\nlim\nτ→τ (1)+\ni\ninf{x′ ∈R | FZ(π)(x′) ≥τ} ≥x\nsince FZ(π)(x′) ≤τ (1)\ni\n< τ for all x′ ≤x so the inﬁmum must be ≥x. These two arguments show\nthat F −1\nZ(π)({τ (1)\ni\n}) ⊆[x(1)\ni , x(1)+\ni\n]. The fact that [x(1)\ni , x(1)+\ni\n) ⊆F −1\nZ(π)({τ (1)\ni\n}) follows by the same\nargument as for the second line. The claim follows. Finally, the fourth line follows since the sets\nF −1\nZ(π)({τ (1)\ni\n}) are disjoint.\nSecond term. For any τ ∈I(2), then F −1\nZ(π) exists at τ, and we have F †\nZ(π)(τ) = F −1\nZ(π)(τ). Thus, by\na substitution τ = FZ(π)(x), we have\nZ\nI(2) G(τ) · dF †\nZ(π)(τ) =\nZ\nF −1\nZ(π)(I(2))\nG(FZ(π)(x)) · dx.\nThird term. We can divide I(3) into a union of disjoint intervals I(3) = S∞\ni=1 I(3)\ni\n, where\nI(3)\ni\n= {τ ∈[0, 1] | F †\nZ(π)(τ) = x(3)\ni }\nfor some x(3)\ni\n∈R; there are only be countably many such intervals (since each one contains a distinct\nrational number). Then, we have\nZ\nI(3) G(τ) · dF †\nZ(π)(τ) = 0 =\nZ\nF −1\nZ(π)(I(3))\nG(FZ(π)(x)) · dx,\nsince F −1\nZ(π)(I(3)) = {x(3)\ni }∞\ni=1 has measure zero according to the Lebesgue measure dx.\nFinal proof. Finally, note that F −1\nZ(π)(I(1)), F −1\nZ(π)(I(2)), and F −1\nZ(π)(I(3)) cover R and are disjoint\nexcept possibly on a set of measure zero, so\nZ\nF −1\nZ(π)(I(1))\nG(FZ(π)(x)) · dx +\nZ\nF −1\nZ(π)(I(2))\nG(FZ(π)(x)) · dx +\nZ\nF −1\nZ(π)(I(3))\nG(FZ(π)(x)) · dx\n=\nZ\nR\nG(FZ(π)(x)) · dx.\nThe claim follows.\nNext, given δ ∈R>0, deﬁne E to be the event where the following hold:\n∥˜P (k)(· | s, a) −P(· | s, a)∥1 ≤\ns\n2|S|\nN (k)(s, a) log\n\u00126|S| · |A| · K\nδ\n\u0013\n=: ϵ(k)\nP (s, a)\n(∀s ∈S, a ∈A)\n∥F ˜\nR(k)(s,a) −FR(s,a)∥∞≤\ns\n1\n2N (k)(s, a) log\n\u00126|S| · |A| · K\nδ\n\u0013\n=: ϵ(k)\nR (s, a)\n(∀s ∈S, a ∈A)\n∥˜P (k)(· | s, a) −P(· | s, a)∥∞≤\ns\n1\n2N (k)(s, a) log\n\u00126|S| · |A| · K\nδ\n\u0013\n= ϵ(k)\nR (s, a)\n(∀s ∈S, a ∈A).\nLemma 5.2. We have P[E | {N (k)(s, a)}k∈[K],s∈S,a∈A] ≥1 −δ.\n8\nNext, let ˜Z(k,π) and ˆZ(k,π) be the returns for policy π for ˜\nM(k) and ˆ\nM(k), respectively, let Φ = ΦM,\n˜Φ(k) = Φ ˜\nM(k), and ˆΦ(k) = Φ ˆ\nM(k), and let π∗= π∗\nM, ˜π(k) = π∗\n˜\nM(k), and ˆπ(k) = π∗\nˆ\nM(k). Now, we\nprove two key results: (i) ˆΦ(k) is close to Φ, and (ii) ˆΦ(k) is optimistic compared to Φ. To this end,\nwe have the following key lemma; its proof depends critically on Lemma 5.1.\nLemma 5.3. Consider MDPs M = (S, A, D, P, P, T) and M′ = (S, A, D, P ′, P′, T), such that\n∥P ′(· | s, a) −P(· | s, a)∥1 ≤ϵP (s, a) and ∥FR′(s,a) −FR(s,a)∥∞≤ϵR(s, a). Then, we have\n|Φ′(π) −Φ(π)| ≤T · LG · B(π)\n(∀k ∈[K], π),\nwhere\nB(π) = EΞ(π)\nT\n\" T\nX\nt=1\nϵP (st, at) + ϵR(st, at)\n#\n.\nOur next lemma characterizes the connection between ˜Φ(k) and Φ.\nLemma 5.4. On event E and conditioned on {N (k)(s, a)}k∈[K],s∈S,a∈A, we have\n|˜Φ(k)(π) −Φ(π)| ≤T · LG · B(k)(π)\n(∀k ∈[K], π),\nwhere\nB(k)(π) = EΞ(π)\nT\n\" T\nX\nt=1\nϵ(k)\nP (st, at) + ϵ(k)\nR (st, at)\n\f\f\f\f {N (k)(s, a)}s∈S,a∈A\n#\n.\nProof. The result follows since on event E and conditioned on {N (k)(s, a)}k∈[K],s∈S,a∈A, ˜\nM(k)\nand M satisfy the conditions of Lemma 5.3 for all k ∈[K].\nOur next lemma characterizes the connection between ˆΦ(k) and ˜Φ(k).\nLemma 5.5. For each k ∈[K] and any policy π, we have\n|ˆΦ(k)(π) −˜Φ(π)| ≤T · LG ·\np\n|S| · B(k)(π).\nProof. The result follows since by deﬁnition of\nˆ\nM(k),\nˆ\nM(k) and\n˜\nM(k) satisfy the condition of\nLemma 5.3 with ϵP (s, a) = 2|S| · ϵ(k)\nR (s, a) ≤\np\n|S| · ϵ(k)\nP (s, a) and ϵR(s, a) = ϵ(k)\nR (s, a) for all\nk ∈[K].\nNow, we prove the ﬁrst key claim—i.e., ˆΦ(k) is close to Φ.\nLemma 5.6. On event E, for all k ∈[K] and any policy π, we have\n|ˆΦ(k)(π) −Φ(π)| ≤2T · LG ·\np\n|S| · B(k)(π).\nProof. Note that\n|ˆΦ(k)(π) −Φ(π)| ≤|ˆΦ(k)(π) −˜Φ(k)(π)| + |˜Φ(k)(π) −Φ(π)| ≤2T · LG ·\np\n|S| · B(k)(π),\nwhere the second inequality follows by Lemmas 5.4 & 5.5.\nNow, we prove the second key claim—i.e., ˆΦ(k) is optimistic compared to Φ.\nLemma 5.7. On event E, we have ˆΦ(k)(π) ≥Φ(π) for all k ∈[K] and all policies π.\nWith these two key claims, the proof of Theorem 4.1 follows by a standard upper conﬁdence bound\nargument; we give the proof in Appendix C.4.\n9\nFigure 1: Results on the frozen lake environment. Left: Regret of our algorithm vs. UCBVI (with\nexpected return) and a greedy exploration strategy. Right: Regret of our algorithm across different α\nvalues. We show mean and standard deviation across ﬁve random seeds.\n6\nExperiments\nWe consider a classic frozen lake problem with a ﬁnite horizon. The agent moves to a block next to its\ncurrent state at each timestep t and has a slipping probability of 0.1 in its moving direction if the next\nstate is an ice block. The objective is to maximize the cumulative reward without falling into holes.\nThe agent needs to choose among paths which correspond to different levels of risk and rewards. In\nother words, the agent should account for the tradeoff between the cumulative reward and risk of\nslipping into holes. We use a map with four paths of the same lengths that have different rewards at\nthe end and different levels of risk of falling into holes. We consider α ∈{0.40, 0.33, 0.25, 0.01},\nwhich correspond to optimal policies of choosing paths with best possible returns of {6, 4, 2, 1} and\nsuccess probabilities of {0.729, 0.81, 0.9, 1}, respectively (failure corresponds to zero return).\nFigure 1 (left) shows the comparison in cumulative regret between our algorithm, UCBVI (which\nmaximizes expected returns, not our risk-sensitive objective), and the an algorithm that optimizes our\nrisk-sensitive objective but explores in a greedy way (i.e., use the best policy for the current estimated\nMDP without any optimism), for α = 0.33. The regret is measured in terms of the CVaR objective\nwith respect to the optimal policy for the same CVaR objective. While UCBVI outperforms greedy,\nneither of them converge; in contrast, our algorithm converges within 40 episodes.\nFigure 1 (right) compares the regret between our algorithm under different values of α using the\nCVaR objective. Note that smaller values of α tend to lead our algorithm to converge more slowly;\nthis result matches our theory since smaller α corresponds to larger LG. Intuitively, more samples\nare needed to get a good estimate of the objective as α becomes small since the CVaR objective is the\naverage return over a tiny fraction of samples, causing high variance in our estimate of the objective.\n7\nConclusion\nWe have proposed a novel regret bound for risk sensitive reinforcement learning that applies to a broad\nclass of objective functions, including the popular conditional value-at-risk (CVaR) objective. Our\nresults recover the usual\n√\nK dependence on the number of episodes, and also highlights dependence\non the Lipschitz constant LG of the integral of the weighting function G used to deﬁne the objective.\nFuture work includes extending these ideas to the setting of function approximation and understanding\nwhether alternative exploration strategies such as Thompson sampling are applicable.\nAcknowledgments and Disclosure of Funding\nThis work is funded in part by NSF Award CCF-1910769, NSF Award CCF-1917852, and ARO\nAward W911NF-20-1-0080. The U.S. Government is authorized to reproduce and distribute reprints\nfor Government purposes notwithstanding any copyright notation herein.\n10\nReferences\n[1] R Tyrrell Rockafellar and Stanislav Uryasev. Conditional value-at-risk for general loss distribu-\ntions. Journal of banking & ﬁnance, 26(7):1443–1471, 2002.\n[2] Yichuan Charlie Tang, Jian Zhang, and Ruslan Salakhutdinov. Worst cases policy gradients.\narXiv preprint arXiv:1911.03618, 2019.\n[3] Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained\nreinforcement learning with percentile risk criteria. The Journal of Machine Learning Research,\n18(1):6070–6120, 2017.\n[4] Yecheng Ma, Dinesh Jayaraman, and Osbert Bastani.\nConservative ofﬂine distributional\nreinforcement learning. Advances in Neural Information Processing Systems, 34, 2021.\n[5] Aviv Tamar, Yonatan Glassner, and Shie Mannor. Optimizing the cvar via sampling. In\nTwenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015.\n[6] Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for cvar optimization in mdps.\nAdvances in neural information processing systems, 27, 2014.\n[7] Yingjie Fei, Zhuoran Yang, Yudong Chen, and Zhaoran Wang. Exponential bellman equation\nand improved regret bounds for risk-sensitive reinforcement learning. Advances in Neural\nInformation Processing Systems, 34, 2021.\n[8] Yingjie Fei, Zhuoran Yang, and Zhaoran Wang. Risk-sensitive reinforcement learning with\nfunction approximation: A debiasing approach. In International Conference on Machine\nLearning, pages 3198–3207. PMLR, 2021.\n[9] Ramtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill. Being optimistic to\nbe conservative: Quickly learning a cvar policy. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 34, pages 4436–4443, 2020.\n[10] Will Dabney, Mark Rowland, Marc Bellemare, and Rémi Munos. Distributional reinforce-\nment learning with quantile regression. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 32, 2018.\n[11] Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement\nlearning. Advances in neural information processing systems, 21, 2008.\n[12] Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for\nreinforcement learning. In International Conference on Machine Learning, pages 263–272.\nPMLR, 2017.\n[13] Nicole Bäuerle and Jonathan Ott. Markov decision processes with average-value-at-risk criteria.\nMathematical Methods of Operations Research, 74(3):361–379, 2011.\n[14] Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforce-\nment learning. In International Conference on Machine Learning, pages 449–458. PMLR,\n2017.\n[15] Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for\ndistributional reinforcement learning. In International conference on machine learning, pages\n1096–1105. PMLR, 2018.\n[16] Shaun S Wang. A class of distortion operators for pricing ﬁnancial and insurance risks. Journal\nof risk and insurance, pages 15–36, 2000.\n[17] Amos Tversky and Daniel Kahneman. Advances in prospect theory: Cumulative representation\nof uncertainty. Journal of Risk and uncertainty, 5(4):297–323, 1992.\n[18] Paul Embrechts and Marius Hofert. A note on generalized inverses. Mathematical Methods of\nOperations Research, 77(3):423–432, 2013.\n[19] Elias M Stein and Rami Shakarchi. Real analysis. In Real Analysis. Princeton University Press,\n2009.\n11\nA\nProof of Theorem 3.1\nIn this section, we prove Theorem 3.1, which says that it sufﬁces to the augmented state space (y, s)\nrather than the whole history ξ. First, we have the following lemma.\nLemma A.1. For any y ∈R and s ∈S, we have\nEΞ(π)\nt\nh\nπt(a | Ξ(π)\nt\n) · 1\n\u0010\nΞ(π)\nt\n∈Zt(y, s)\n\u0011i\n= EΞ(π)\nt\nh\n˜πt(a | Ξ(π)\nt\n) · 1\n\u0010\nΞ(π)\nt\n∈Zt(y, s)\n\u0011i\n.\nProof. Note that\nEΞ(π)\nt\nh\n˜πt(a | Ξ(π)\nt\n)\n\f\f\f Ξ(π)\nt\n∈Zt(y, s)\ni\n= EΞ(π)\nt\nh\nE˜Ξ(π)\nt\nh\nπt(at | ˜Ξ(π)\nt\n)\n\f\f\f ˜Ξ(π)\nt\n∈Zt(J(Ξ(π)\nt\n), st)\ni \f\f\f Ξ(π)\nt\n∈Zt(y, s)\ni\n= EΞ(π)\nt\nh\nE˜Ξ(π)\nt\nh\nπt(at | ˜Ξ(π)\nt\n)\n\f\f\f ˜Ξ(π)\nt\n∈Zt(y, s)\ni \f\f\f Ξ(π)\nt\n∈Zt(y, s)\ni\n= E˜Ξ(π)\nt\nh\nπt(at | ˜Ξ(π)\nt\n)\n\f\f\f Ξ(π)\nt\n∈Zt(y, s)\ni\n.\nThe claim follows by replacing ˜Ξ(π)\nt\nwith Ξ(π)\nt\nand multiplying by PΞ(π)\nt [Ξ(π)\nt\n∈Zt(y, s)].\nNext, let\nD(π)\nt\n(y, s) = PΞ(π)\nt\nh\nJ(Ξ(π)\nt\n) ≤y ∧S(Ξ(π)\nt\n) = s\ni\nbe the probability of a history achieving current cumulative return at most y and ending in state s.\nLemma A.2. We have D(π)\nt\n= D(˜π)\nt\n.\nProof. We prove by induction. The base case t = 1 follows trivially. For the inductive case, note that\nD(π)\nt+1(y, s′′) =\nZ\n1(ξ′ ∈Zt+1(y, s′′)) · dPΞ(π)\nt+1(ξ′)\n=\nZ X\na∈A\nX\ns′∈S\n1(ξ ◦(a, r, s′) ∈Zt+1(y, s′′)) · P(s′ | S(ξ), a) · dPR(s,a)(r) · π(a | ξ) · dPΞ(π)\nt (ξ)\n=\nZ X\na∈A\nX\ns′∈S\n1(J(ξ) + r ≤y) · 1(s′ = s′′) · P(s′ | S(ξ), a) · dPR(s,a)(r) · π(a | ξ) · dPΞ(π)\nt (ξ)\n=\nZ X\na∈A\n1(J(ξ) + r ≤y) · P(s′′ | S(ξ), a) · dPR(s,a)(r) · π(a | ξ) · dPΞ(π)\nt (ξ),\nwhere the ﬁrst line follows by deﬁnition of D(π)\nt+1, the second by the inductive formula for PΞ(π)\nt+1, the\nthird since J(ξ ◦(a, r, s′)) = J(ξ) + r, and the fourth by summing over s′. Continuing, we have\nD(π)\nt+1(y, s′′) =\nX\ns∈S\nX\na∈A\nZ\nπ(a | ξ) · 1(J(ξ) + r ≤y) · 1(S(ξ) = s) · dPΞ(π)\nt (ξ) · P(s′′ | s, a) · dPR(s,a)(r)\n=\nX\ns∈S\nX\na∈A\nZ\nπ(a | ξ) · 1(ξ ∈Zt(y −r, s)) · dPΞ(π)\nt (ξ) · P(s′′ | s, a) · dPR(s,a)(r)\n=\nX\ns∈S\nX\na∈A\nZ\n˜π(a | ξ) · 1(ξ ∈Zt(y −r, s)) · dPΞ(π)\nt (ξ) · P(s′′ | s, a) · dPR(s,a)(r),\n12\nwhere the ﬁrst line follows by introducing 1(S(ξ) = s) and rearranging, the second by deﬁnition of\nZt, and the third by Lemma A.1. Continuing, we have\nD(π)\nt+1(y, s′′) =\nX\ns∈S\nX\na∈A\nZ\n1(ξ ∈Zt(y −r, s)) · dPΞ(π)\nt (ξ) · ˜π(a | y −r, s) · P(s′′ | s, a) · dPR(s,a)(r)\n=\nX\ns∈S\nX\na∈A\nZ\nD(π)\nt\n(y −r, s) · ˜π(a | y −r, s) · P(s′′ | s, a) · dPR(s,a)(r)\n=\nX\ns∈S\nX\na∈A\nZ\nD(˜π)\nt\n(y −r, s) · ˜π(a | y −r, s) · P(s′′ | s, a) · dPR(s,a)(r)\n=\nX\ns∈S\nX\na∈A\nZ\n1(ξ ∈Zt(y −r, s)) · dPΞ(˜π)\nt (ξ) · ˜π(a | y −r, s) · P(s′′ | s, a) · dPR(s,a)(r),\nwhere the ﬁrst line follows since ˜π is independent of ξ and by rearranging, the second by deﬁnition\nof D(π)\nt\n, the third by induction, and the fourth by deﬁnition of D(˜π)\nt\n. Continuing, we have\nD(π)\nt+1(y, s′′) =\nX\ns∈S\nX\na∈A\nZ\n˜π(a | ξ) · 1(ξ ∈Zt(y −r, s)) · dPΞ(˜π)\nt (ξ) · P(s′′ | s, a) · dPR(s,a)(r)\n=\nX\ns∈S\nX\na∈A\nZ\n˜π(a | ξ) · 1(J(ξ) + r ≤y) · 1(S(ξ) = s) · dPΞ(˜π)\nt (ξ) · P(s′′ | s, a) · dPR(s,a)(r)\n=\nX\na∈A\nZ\n1(J(ξ) + r ≤y) · P(s′′ | S(ξ), a) · dPR(s,a)(r) · ˜π(a | ξ) · dPΞ(˜π)\nt (ξ)\n=\nX\na∈A\nX\ns′∈S\nZ\n1(J(ξ) + r ≤y) · 1(s′ = s′′) · P(s′ | S(ξ), a) · dPR(s,a)(r) · ˜π(a | ξ) · dPΞ(˜π)\nt (ξ)\n=\nX\na∈A\nX\ns′∈S\nZ\n1(ξ ◦(a, r, s′) ∈Zt+1(y, s′′)) · P(s′ | S(ξ), a) · dPR(s,a)(r) · ˜π(a | ξ) · dPΞ(˜π)\nt (ξ)\n=\nZ\n1(ξ′ ∈Zt+1(y, s′′)) · dPΞ(π)\nt+1(ξ′)\n= D(˜π)\nt+1(y, s′′),\nwhere the ﬁrst line follows by deﬁnition of ˜π, the second by deﬁnition of Zt, the third by summing\nover s and rearranging, the fourth by introducing 1(s′ = s′′), the ﬁfth by deﬁnition of Zt+1, the sixth\nby the inductive formula for PΞ(π)\nt+1, and the seventh by the deﬁnition of D(˜π)\nt+1. The claim follows.\nNow, we prove Theorem 3.1. By Lemma A.2, we have\nFZ(π)(x) =\nZ\n1(J(ξ) ≤x) · dPΞ(π)\nT (ξ)\n=\nX\ns∈S\nZ\n1(J(ξ) ≤x) · 1(S(ξ) = s) · dPΞ(π)\nT (ξ)\n=\nX\ns∈S\nZ\n1(ξ ∈ZT (x, s)) · dPΞ(π)\nT (ξ)\n=\nX\ns∈S\nZ\n1(ξ ∈ZT (x, s)) · dPΞ(˜π)\nT (ξ)\n=\nZ\n1(J(ξ) ≤x) · dPΞ(˜π)\nT (ξ)\n= FZ(˜π)(x).\nTheorem 3.1 follows straightforwardly from this result.\n13\nB\nProof of Theorem 3.2\nWe construct a sequence of MDPs M0, M1, ..., MT , such that M0 =\n˜\nM and MT =\nˆ\nM, and\nwhere we can bound the incremental errors\nΦ ˜\nM(π∗\nMτ ) −Φ ˆ\nM(π∗\nMτ−1),\nnoting a policy for one of the MDPs can be used in all the other MDPs. For each τ ∈[T], the\nMDP Mτ discretizes the reward assigned on the tth step of Mτ−1—more precisely, it discretizes\nthe transitions since the rewards are only assigned on the last step based on the cumulative reward\nrecorded in the second component of the state space. Formally, Mτ is identical to ˜\nM, except it uses\nthe (time-varying) transition probability measure ˆP (τ) deﬁned by\nˆP (τ)\nt\n((s′, y′) | (s, y), a) =\n\u001aP(s′ | s, a) · (PR(s,a) ◦φ−1)(y′ −y)\nif t ≤τ\nP(s′ | s, a) · PR(s,a)(y′ −y)\notherwise.\nThen, Mτ is identical to Mτ−1 except PR(s,a) is replaced with PR(s,a) ◦φ−1 on step τ. We prove\nthree lemmas showing a lower bound on the value of a policy π for Mτ when adapted to Mτ−1.\nLemma B.1. Given τ ∈[T], let M = Mτ−1 and ˆ\nM = Mτ (so compared to M,\nˆ\nM replaces\nPR(s,a) with PR(s,a) ◦φ−1 on step τ in its transitions). Given any policy ˆπ for ˆ\nM, deﬁne the policy\nπt(a | s, y, α) = ˆπt(a | s, y + α)\nfor M, where we initialize the (extra) policy internal state α1 = 0, and we update ατ+1 = φ(rτ)−rτ\non step τ and αt+1 = αt otherwise. Then, for all x, y, α ∈R, for t > τ, we have\nFZ(π)\nt\n(s,y,α)(x) = F ˆ\nZ(ˆπ)\nt\n(s,y+α)(x),\nand for t ≤τ, we have\nFZ(π)\nt\n(s,y,0)(x) ≤F ˆ\nZ(ˆπ)\nt\n(s,y)(x + η),\nwhere Z(π)\nt\n(resp., ˆZ(ˆπ)\nt\n) is the return of M (resp., ˆ\nM) from step t for policy π (resp., ˆπ).\nProof. We prove by backwards induction on t. The base case t = T follows by deﬁnition (and since\nthe reward measure does not change from M to ˆ\nM). For t > τ, we have\nFZ(π)\nt\n(s,y,α)(x) =\nX\na∈A\nX\ns′∈S\nZ\nπt(a | s, y, α) · P(s′ | s, a) · FZ(π)\nt+1(s′,y+r,α)(x −r) · dPR(s,a)(r)\n=\nX\na∈A\nX\ns′∈S\nZ\nˆπt(a | s, y + α) · P(s′ | s, a) · F ˆ\nZ(ˆπ)\nt+1(s′,y+α+r)(x −r) · dPR(s,a)(r)\n= F ˆ\nZ(ˆπ)\nt\n(s,y+α)(x),\nwhere the second line follows by induction and by the deﬁnition of π. Next, for t = τ, we have\nFZ(π)\nt\n(s,y,0)(x) =\nX\na∈A\nX\ns′∈S\nZ\nπt(a | s, y, 0) · P(s′ | s, a) · FZ(π)\nt+1(s′,y+r,φ(r)−r)(x −r) · dPR(s,a)(r)\n=\nX\na∈A\nX\ns′∈S\nZ\nˆπt(a | s, y) · P(s′ | s, a) · F ˆ\nZ(ˆπ)\nt+1(s′,y+φ(r))(x −r) · dPR(s,a)(r)\n=\nX\na∈A\nX\ns′∈S\nZ\nˆπt(a | s, y) · P(s′ | s, a) · F ˆ\nZ(ˆπ)\nt+1(s′,y+φ(r))(x −φ(r) + φ(r) −r) · dPR(s,a)(r)\n≤\nX\na∈A\nX\ns′∈S\nZ\nˆπt(a | s, y) · P(s′ | s, a) · F ˆ\nZ(ˆπ)\nt+1(s′,y+φ(r))(x −φ(r) + η) · dPR(s,a)(r)\n=\nX\na∈A\nX\ns′∈S\nZ\nˆπt(a | s, y) · P(s′ | s, a) · F ˆ\nZ(ˆπ)\nt+1(s′,y+ρ)(x −ρ + η) · dPR(s,a) ◦φ−1(ρ)\n= F ˆ\nZ(ˆπ)\nt\n(s,y)(x + η),\n14\nwhere the ﬁrst line uses the update from αt = 0 to αt+1 = r −φ(r) on this step, the second\nline follows by induction and by the deﬁnition of π, the fourth line follows by monotonicity of\nF ˆ\nZ(ˆπ)\nt+1(s′,y+r), and the ﬁfth line follows by a change of variables ρ = φ(r). For t < τ, we have\nFZ(π)\nt\n(s,y,0)(x) =\nX\na∈A\nX\ns′∈S\nZ\nπt(a | s, y, 0) · P(s′ | s, a) · FZ(π)\nt+1(s′,y+r,0)(x −r) · dPR(s,a)(r)\n=\nX\na∈A\nX\ns′∈S\nZ\nˆπt(a | s, y) · P(s′ | s, a) · FZ(π)\nt+1(s′,y+r,0)(x −r) · dPR(s,a)(r)\n≤\nX\na∈A\nX\ns′∈S\nZ\nˆπt(a | s, y) · P(s′ | s, a) · F ˆ\nZ(ˆπ)\nt+1(s′,y+r,0)(x −r + η) · dPR(s,a)(r)\n= F ˆ\nZ(ˆπ)\nt\n(s′,0)(x + η),\nwhere the second line follows by the deﬁnition of π, and the third line follows by induction. The\nclaim follows.\nLemma B.2. For any monotonically increasing F, we have F †(F(x)) ≤x, and F(F †(τ)) ≥τ.\nProof. See Proposition 1 in [18].\nLemma B.3. Let F, G : R →R be monotonically increasing. If F(x) ≤G(x + η) for all x ∈R,\nthen we have F †(τ) ≥G†(τ) −η for all τ ∈R.\nProof. By assumption, G(x) ≥F(x −η). Substituting x = F †(τ) + η into this formula, we obtain\nG(F †(τ) + η) ≥F(F †(τ)) ≥τ,\nwhere the second inequality follows by Lemma B.2. Also by Lemma B.2, since G is monotonically\nincreasing, so is G†, so we can apply G† to each side of the inequality to obtain\nG†(τ) ≤G†(G(F †(τ) + η)) ≤F †(τ) + η,\nwhere the second inequality follows by Lemma B.2. The claim follows.\nLemma B.4. Consider the same setup as in Lemma B.1. Let ˆπ be a policy for ˆ\nM, and let π be the\npolicy deﬁned in Lemma B.1 that adapts ˆπ to M. Then, we have\nΦ(π) ≥ˆΦ(ˆπ) −η,\nwhere Φ is the objective for M and ˆΦ is the objective for ˆ\nM.\nProof. Let ˆZ = ˆZ(ˆπ)\n1\n(s1, 0) and Z = Z(π)\n1\n(s1, 0, 0). Applying Lemma B.3 to the inequality in\nLemma B.1, we have\nF †\nZ(τ) ≥F †\nˆ\nZ(τ) −η.\nIntegrating this inequality, we have\nΦ(π) =\nZ\nF †\nZ(τ) · dG(τ) ≥\nZ \u0010\nF †\nˆ\nZ(τ) −η\n\u0011\n· dG(τ) = ˆΦ(ˆπ) −η,\nas claimed.\nLemma B.5. Consider the same setup as in Lemma B.1. Given any policy π for M, deﬁne the policy\nˆπt(a | s, y, α) = πt(a | s, y + α)\nfor ˆ\nM, where we initialize α1 = 0, and we update ατ+1 = r on step τ, where r is a random variable\nwith probability measure\nPR(s,a)(r | φ(r) = ρ),\nand αt+1 = αt otherwise. Then, for all x, y, α ∈R, for t > τ, we have\nF ˆ\nZ(ˆπ)\nt\n(s,y,α)(x) = FZ(π)\nt\n(s,y+α)(x),\nand for t ≤τ, we have\nF ˆ\nZ(ˆπ)\nt\n(s,y,0)(x) ≤FZ(π)\nt\n(s,y)(x).\n15\nProof. We prove by backwards induction on T. The base case t = T follows by deﬁnition. For\nt > τ, we have\nF ˆ\nZ(ˆπ)\nt\n(s,y,α)(x) =\nX\na∈A\nX\ns′∈S\nZ\nˆπt(a | s, y, α) · P(s′ | s, a) · F ˆ\nZ(π)\nt+1(s′,y+ρ,α)(x −ρ) · dPR(s,a)(ρ)\n=\nX\na∈A\nX\ns′∈S\nZ\nπt(a | s, y + α) · P(s′ | s, a) · FZ(π)\nt+1(s′,y+α+ρ)(x −ρ) · dPR(s,a)(ρ)\n= FZ(π)\nt\n(s,y+α)(x),\nwhere the second line follows by induction and by the deﬁnition of π. Next, for t = τ, we have\nF ˆ\nZ(ˆπ)\nt\n(s,c,0)(x)\n=\nX\na∈A\nX\ns′∈S\nZ\nˆπt(a | s, y, 0) · P(s′ | s, a) · F ˆ\nZ(ˆπ)\nt+1(s′,y+ρ,r−ρ)(x −ρ) · dPR(s,a)(r | φ(r) = ρ) · dPR(s,a) ◦φ−1(ρ)\n=\nX\na∈A\nX\ns′∈S\nZ\nπt(a | s, y) · P(s′ | s, a) · FZ(π)\nt+1(s′,y+r)(x −ρ) · dPR(s,a)(r | φ(r) = ρ) · dPR(s,a) ◦φ−1(ρ)\n≤\nX\na∈A\nX\ns′∈S\nZ\nπt(a | s, y) · P(s′ | s, a) · FZ(π)\nt+1(s′,y+r)(x −r) · dPR(s,a)(r | φ(r) = ρ) · dPR(s,a) ◦φ−1(ρ)\n=\nX\na∈A\nX\ns′∈S\nZ\nπt(a | s, y) · P(s′ | s, a) · FZ(π)\nt+1(s′,y+r)(x −r) · dPR(s,a)(r)\n= FZ(π)\nt\n(s,y)(x),\nwhere the second line uses the update from αt = 0 to αt+1 = r −ρ on this step, the third line follows\nby induction and by the deﬁnition of ˆπ, the fourth line follows by monotonicity of FZ(π)\nt+1(s′,y+r), and\nthe ﬁfth line follows by the deﬁnition of conditional probability—in particular,\nZ\nFZ(π)\nt+1(s′,y+r)(x −r) · dPR(s,a)(r | φ(r) = ρ) · dPR(s,a) ◦φ−1(ρ)\n=\nZ R\nFZ(π)\nt+1(s′,y+r)(x −r) · 1(r ∈φ−1(ρ)) · dPR(s,a)(r)\nR\n1(r′ ∈φ−1(ρ)) · dPR(s,a)(r′)\n· dPR(s,a) ◦φ−1(ρ)\n=\nZ\nFZ(π)\nt+1(s′,y+r)(x −r)\nZ\n·1(r ∈φ−1(ρ))\nR\n1(r′ ∈φ−1(ρ)) · dPR(s,a)(r′) · dPR(s,a) ◦φ−1(ρ) · dPR(s,a)(r)\n=\nZ\nFZ(π)\nt+1(s′,y+r)(x −r)\n∞\nX\ni=1\n·1(r ∈Bi)\nP(R(s, a) ∈Bi) · P(R(s, a) ∈Bi) · dPR(s,a)(r)\n=\nZ\nFZ(π)\nt+1(s′,y+r)(x −r) · dRR(s,a)(r),\nwhere in the third line, Bi = (η · (i −1), η · i]. For t < τ, we have\nF ˆ\nZ(ˆπ)\nt\n(s,y,0)(x) =\nX\na∈A\nX\ns′∈S\nZ\nˆπt(a | s, y, 0) · P(s′ | s, a) · F ˆ\nZ(ˆπ)\nt+1(s′,y+ρ,0)(x −ρ) · dPR(s,a)(ρ)\n=\nX\na∈A\nX\ns′∈S\nZ\nπt(a | s, y) · P(s′ | s, a) · F ˆ\nZ(ˆπ)\nt+1(s′,y+ρ,0)(x −ρ) · dPR(s,a)(ρ)\n≤\nX\na∈A\nX\ns′∈S\nZ\nπt(a | s, y) · P(s′ | s, a) · FZ(π)\nt+1(s′,y+ρ,0)(x −ρ) · dPR(s,a)(ρ)\n= FZ(π)\nt\n(s′,y,0)(x),\nwhere the second line follows by the deﬁnition of π, and the third line follows by induction. The\nclaim follows.\n16\nNext, we prove two lemmas showing a converse—namely, a lower bound on the value of a policy π\nfor Mτ−1 when adapted to Mτ.\nLemma B.6. Consider the same setup as in Lemma B.1. Letting π be a policy for M, and ˆπ be the\npolicy deﬁned in Lemma B.5 that adapts π to ˆ\nM. Then, we have\nˆΦ(ˆπ) ≥Φ(π),\nwhere ˆΦ is the objective for ˆ\nM and Φ is the objective for M.\nProof. Let Z = Z(π)\n1\n(s1, 0) and ˆZ = Z(ˆπ)\n1\n(s1, 0, 0). Applying Lemma B.3 to the inequality in\nLemma B.5, we have\nF †\nˆ\nZ(τ) ≥F †\nZ(τ).\nIntegrating this inequality, we have\nˆΦ(ˆπ) =\nZ\nF †\nˆ\nZ(τ) · dG(τ) ≥\nZ\nF †\nZ(τ) · dG(τ) = Φ(π),\nas claimed.\nFinally, we prove Theorem 3.2. Let πT\nT be the optimal policy for MT , and let πT\nτ be the policy\ndeﬁned in Lemma B.4 adapting πT\nτ from Mτ to Mτ−1 for each τ ∈[T].\nΦ0(πT\n0 ) ≥Φ1(πT\n1 ) −η ≥Φ2(πT\n2 ) ≥... ≥ΦT (πT\nT ) −T · η,\nwhere each inequality follows by Lemma B.4. Similarly, let π0\n0 be the optimal policy for M0, and let\nπ0\nτ be the policy deﬁned in Lemma B.6 adapting π0\nτ−1 from Mτ−1 to Mτ. Then, we have\nΦT (π0\nT ) ≥ΦT −1(π0\nT −1) ≥... ≥Φ0(π0\n0),\nwhere each inequality follows by Lemma B.6. Furthermore, by optimality of πT\nT for ΦT , we also\nhave ΦT (πT\nT ) ≥ΦT (π0\nT ); together, these three inequalities imply\nΦ0(πT\n0 ) ≥Φ0(π0\n0) −T · η.\nFinally, note that π0\n0 = π∗\n˜\nM is the optimal policy for ˜\nM = M0, and π0\nT = π ˆ\nM is π0\n0 adapted to ˆ\nM;\nalso, Φ0 = Φ ˜\nM is the objective for ˜\nM. Thus, we have\nΦ ˜\nM(π ˆ\nM) ≥Φ ˜\nM(π∗\n˜\nM) −T · η.\nBy Theorem 3.1, the optimal policy for\n˜\nM equals the optimal history-dependent policy for the\noriginal MDP M, so the claim follows.\nC\nProof of Lemmas for Section 5\nC.1\nProof of Lemma 5.2\nProof. First, by the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality and a union bound, for each\nk ∈[K], conditioned on {N (k)(s, a)}s∈S,a∈A, with probability at least 1 −δ/(3K), we have\n∥F ˜\nR(k)(s,a) −FR(s,a)∥∞≤ϵ(k)\nR (s, a)\n(∀s ∈S, a ∈A).\nSimilarly, by Hoeffding’s inequality, an ℓ1 concentration bound for multinomial distribution, and a\nunion bound, for each k ∈[K], conditioned on {N (k)(s, a)}s∈S,a∈A, we have\n∥˜P (k)(· | s, a) −P(· | s, a)∥1 ≤ϵ(k)\nP (s, a)\n(∀s ∈S, a ∈A)\n∥˜P (k)(· | s, a) −P(· | s, a)∥∞≤ϵ(k)\nR (s, a)\n(∀s ∈S, a ∈A).\neach holding with probability at least 1 −δ/(3K), respectively. Thus, both of these bounds hold for\nall k ∈[K] with probability at least 1 −δ. The claim follows.\n17\nC.2\nProof of Lemma 5.3\nProof. First, we prove that for all policies π, we have\n∥FZ′(π) −FZ(π)∥∞≤B(π).\nTo this end, let G(r) = FZ(π)\nt+1(s′,y+r)(x −r); note that G(−∞) = 1 and G(∞) = 0. Then, by\nintegration by parts, we have\nFZ(π)\nt\n(s,y)(x) =\nZ X\na∈A\nX\ns′∈S\nπ(a | s, y) · P(s′ | s, a) · G(r) · dPR(s,a)(r)\n= −\nZ X\na∈A\nX\ns′∈S\nπ(a | s, y) · P(s′ | s, a) · FR(s,a)(r) · dG(r),\nand similarly for FZ\n′(π)\nt\n(s,y)(x). Next, note that\nsup\nx∈R\n|FZ\n′(π)\nt\n(s,y)(x) −FZ(π)\nt\n(s,y)(x)|\n= sup\nx∈R\n\f\f\f\f\nX\na∈A\nX\ns′∈S\nπ(a | s, y) (P ′(s′ | s, a) −P(s′ | s, a))\nZ\nFZ\n′(π)\nt+1 (s′,y+r)(x −r)dP′\nR′(s,a)(r)\n+\nX\na∈A\nX\ns′∈S\nπ(a | s, y)P(s′ | s, a)\nZ \u0012\nFZ\n′(π)\nt+1 (s′,y+r)(x −r) −FZ(π)\nt+1(s′,y+r)(x −r)\n\u0013\ndP′\nR′(s,a)(r)\n\f\f\f\f\n−\nX\na∈A\nX\ns′∈S\nπ(a | s, y)P(s′ | s, a)\nZ \u0000FR′(s,a)(r) −FR(s,a)(r)\n\u0001\ndFZ(π)\nt+1(s′,y+r)(x −r)\n≤sup\nx∈R\nX\na∈A\nX\ns′∈S\nπ(a | s, y) · |P ′(s′ | s, a) −P(s′ | s, a)|\n+\nX\na∈A\nX\ns′∈S\nπ(a | s, y) ·\nZ\nsup\nx′∈R\n|FZ\n′(π)\nt+1 (s′,y+r)(x′) −FZ(π)\nt+1(s′,y+r)(x′)| · dP′\nR′(s,a)(r)\n+\nX\na∈A\nπ(a | s, y) · sup\nr′∈R\n|FR′(s,a)(r′) −FR(s,a)(r′)|\n≤E\n\u0014\nϵP (s, a) + ϵR(s, a) + sup\nx′∈R\n|FZ\n′(π)\nt+1 (s′,y+r)(x′) −FZ(π)\nt+1(s′,y+r)(x′)|\n\u0015\n.\nThus, we have\nϵ(π)\nt\n:= E\n\u0014\nsup\nx∈R\n|FZ\n′(π)\nt\n(s,y)(x) −FZ(π)\nt\n(s,y)(x)|\n\u0015\n= E\n\u0014\nϵ(k,P )\ns,a\n+ ϵ(k,R)\ns,a\n+ sup\nx′∈R\n|FZ\n′(π)\nt+1 (s′,y+r)(x′) −FZ(π)\nt+1(s′,y+r)(x′)|\n\u0015\n≤E [ϵP (s, a) + ϵR(s, a)] + ϵ(k,π)\nt+1\n= E\n\" T\nX\nτ=t\nϵP (sτ, aτ) + ϵR(sτ, aτ)\n#\n,\nwhere the last step follows by induction. Finally, we have\n|Φ′(π) −Φ(π)| =\n\f\f\f\f\f\nZ T\n0\n(G(FZ′(π)(x)) −G(FZ(π)(x))) · dx\n\f\f\f\f\f\n≤LG\nZ T\n0\n|FZ′(π)(x) −FZ(π)(x)| · dx\n≤T · LG · ϵ(π)\n1 ,\nwhere the ﬁrst line follows by Lemma 5.1. The claim follows since ϵ(π)\n1\nequals the desired bound.\n18\nC.3\nProof of Lemma 5.7\nProof. First, we prove that F ˆ\nZ(k,π)\nt\n(s,y)(x) ≤FZ(π)\nt\n(s,y)(x). The case s = s∞is straightforward,\nsince its transitions and rewards are equal in M and ˆ\nM, and it only transitions to itself. For s ̸= s∞,\nwe prove by induction on t. The base case t = T follows by deﬁnition. Then, we have\nF ˆ\nZ(k,π)\nt\n(s,y)(x) =\nZ X\na∈A\nX\ns′∈S\nπ(a | s, y) · ˆP (k)(s′ | s, a) · F ˆ\nZ(k,π)\nt+1 (s′,y+r)(x −r) · dˆP ˆ\nR(k)(s,a)(r)\n≤\nZ X\na∈A\nX\ns′∈S\nπ(a | s, y) · ˆP (k)(s′ | s, a) · FZ(π)\nt+1(s′,y+r)(x −r) · dˆP ˆ\nR(k)(s,a)(r)\n=\nZ X\na∈A\nX\ns′∈S\nπ(a | s, y) · ˆP (k)(s′ | s, a) · F ˆ\nR(k)(s,a)(x′ −x) · dFZ(π)\nt+1(s′,y+r)(x′)\n≤\nZ X\na∈A\nX\ns′∈S\nπ(a | s, y) · ˆP (k)(s′ | s, a) · FR(s,a)(x′ −x) · dFZ(π)\nt+1(s′,y+r)(x′)\n=\nZ X\na∈A\nX\ns′∈S\nπ(a | s, y) · ˆP (k)(s′ | s, a) · FZ(π)\nt+1(s′,y+r)(x −r) · dPR(s,a)(r),\n(2)\nwhere the second line follows by induction, the third by integration by parts and substituting x′ = x−r,\nthe fourth since FR(s,a)(r) = 1 = F ˆ\nR(k)(s,a)(r) for r ≥1, and for r < 1, on event E, we have\nFR(s,a)(r) ≥max\nn\nF ˜\nR(k)(s,a)(r) −ϵ(k)\nR (s, a), 0\no\n= F ˆ\nR(k)(s,a)(r),\nand the ﬁfth by integration by parts and substituting r = x′ −x. Next, since s ̸= s∞, we have\nˆP (k)(s∞| s, a) = 1 −\nX\ns′∈S\\{s∞}\nˆP (k)(s′ | s, a) =\nX\ns′∈S\\{s∞}\nP(s′ | s, a) −ˆP (k)(s′ | s, a),\nso we can decompose the summand ˆP (k)(s∞| s, a) · FZ(π)\nt+1(s∞,y+r)(x −r) (i.e., s′ = s∞) in (2)\nand distribute it across the other summands; in particular, the summands s′ ̸= s∞become\nˆP (k)(s′ | s, a) · FZ(π)\nt+1(s′,y+r)(x −r) +\n\u0010\nP(s′ | s, a) −ˆP (k)(s′ | s, a)\n\u0011\n· FZ(π)\nt+1(s∞,y+r)(x −r)\n≤ˆP (k)(s′ | s, a) · FZ(π)\nt+1(s′,y+r)(x −r) +\n\u0010\nP(s′ | s, a) −ˆP (k)(s′ | s, a)\n\u0011\n· FZ(π)\nt+1(s′,y+r)(x −r)\n= P(s′ | s, a) · FZ(π)\nt+1(s′,y+r)(x −r),\n(3)\nwhere the second line follows since FZ(π)\nt+1(s∞,y+r)(x −r) ≤FZ(π)\nt+1(s′,y+r)(x −r) for all s′ ̸= s∞,\nand since P(s′ | s, a) −ˆP (k)(s′ | s, a) ≥0 on event E. Continuing from (2), we have\nF ˆ\nZ(k,π)\nt\n(s,y)(x) ≤\nZ X\na∈A\nX\ns′∈S\nπ(a | s, y) · ˆP (k)(s′ | s, a) · FZ(π)\nt+1(s′,y+r)(x −r) · dPR(s,a)(r)\n≤\nZ X\na∈A\nX\ns′∈S\\{s∞}\nπ(a | s, y) · P(s′ | s, a) · FZ(π)\nt+1(s′,y+r)(x −r) · dPR(s,a)(r)\n= FZ(π)\nt\n(s,y)(x),\n(4)\nwhere the second line follows by distributing the summand s′ = s∞and applying (3). Since M and\nˆ\nM have the same initial state distribution, we have F ˆ\nZ(k,π)(x) ≤FZ(π)(x). By Lemma 5.1, we have\nˆΦ(k)(π) = T −\nZ T\n0\nG(F ˆ\nZ(k,π)(x)) · dx ≥T −\nZ T\n0\nG(FZ(π)(x)) · dx = Φ(π),\nwhere the inequality follows from (4) and since G is monotone. The claim follows.\n19\nC.4\nProof of Theorem 4.1\nProof. We prove Theorem 4.1. Note that on event E, we have\nregret(A)\n=\nK\nX\nk=1\nΦ(π∗) −Φ(ˆπ(k))\n≤\nK\nX\nk=1\nˆΦ(k)(π∗) −Φ(ˆπ(k))\n≤\nK\nX\nk=1\nˆΦ(k)(ˆπ(k)) −Φ(ˆπ(k))\n≤\nK\nX\nk=1\n2T · LG ·\np\n|S| · B(k)(ˆπ(k))\n= 2TLG\ns\n5|S|2 log\n\u00124|S| · |A| · K\nδ\n\u0013\n· E\nΞ(ˆπ(1:K))\nT\n\" K\nX\nk=1\nT\nX\nt=1\n1\np\nN (k)(st, at)\n\f\f\f\f {N (k)(s, a)}k∈[K],s∈S,a∈A\n#\n,\nwhere the ﬁrst inequality follows by Lemma 5.7, the second follows by optimality of ˆπ(k) for ˆΦ(k),\nand the third by Lemma 5.6. Furthermore, note that\nK\nX\nk=1\nT\nX\nt=1\n1\np\nN (k)(st, at)\n≤\nK\nX\nk=1\nT\nX\nt=1\n1(N (k)(st, at) ≤T) +\nK\nX\nk=1\nT\nX\nt=1\n1(N (k)(st, at) > T)\n1\np\nN (k)(st, at)\n.\nThe event (st, at) = (s, a) and (N (k)(s, a) ≤T) can happen fewer than 2T times per state action\npair. Therefore, PK\nk=1\nPT\nt=1 1(N (k)(st, at) ≤T) ≤2TSA. Now suppose N (k)(s, a) > T. Then\nfor any t ∈Wk, we have N (k)\nt\n(s, a) ≤N (k)(s, a) + T ≤2N (k)(s, a). Thus, we have\nK\nX\nk=1\nT\nX\nt=1\n1(N (k)(st, at) > T)\np\nN (k)(st, at)\n≤\nK\nX\nk=1\nT\nX\nt=1\ns\n2\nN (k)\nt\n(st, at)\n=\n√\n2\nK\nX\nk=1\nT\nX\nt=1\nX\ns∈S\nX\na∈A\n1((st, at) = (s, a))\nq\nN (k)\nt\n(s, a)\n≤\n√\n2\nX\ns∈S\nX\na∈A\nN(K+1)(s,a)\nX\nj=1\nj−1/2\n≤\n√\n2\nX\ns∈S\nX\na∈A\nZ N(K+1)(s,a)\nx=0\nx−1/2dx\n≤\ns\n2|S| · |A| ·\nX\ns∈S\nX\na∈A\nN (K+1)(s, a)\n=\np\n2|S| · |A| · KT.\nThe claim follows.\nD\nThe Optimal Policy for CVaR Objectives\nIn this section, we describe how to compute the optimal policy for the CVaR objective when the MDP\nis known; this approach is described in detail in [13]. Following this work, we consider the setting\nwhere we are trying to minimize cost rather than maximize reward. In particular, consider an MDP\nM = (S, A, D, P, P, T), and our goal is to compute a policy π that maximizes its CVaR objective.\n20\nStep 1: CVaR objective. We begin by rewriting the CVaR objective in a form that is more amenable\nto optimization. First, we have the following key result (see [13] for a proof):\nLemma D.1. For any random variable Z, we have\nCVaRα(Z) = inf\nρ∈R\n\u001a\nρ +\n1\n1 −α · EZ\n\u0002\n(Z −ρ)+\u0003\u001b\n,\nwhere the minimum is achieved by ρ∗= VaR(Z).\nAs a consequence of this lemma, we have\nmin\nπ∈Π CVaR(Z(π)) = min\nπ∈Π inf\nρ∈R\n\u001a\nρ +\n1\n1 −α · EZ(π)\nh\n(Z(π) −ρ)+i\u001b\n= inf\nρ∈R\n\u001a\nρ +\n1\n1 −α · min\nπ∈Π EZ(π)\nh\n(Z(π) −ρ)+i\u001b\n.\nThus, we have\nπ∗= arg min\nπ∈Π\nEZ(π)\nh\n(Z(π) −ρ∗)+i\n,\nwhere\nρ∗= arg inf\nρ∈R\nJ(ρ)\nwhere\nJ(ρ) = ρ +\n1\n1 −α · max\nπ∈Π EZ(π)\nh\n(Z(π) −ρ)+i\n.\nThe main challenge is evaluating the minimum over π ∈Π in J(ρ). To do so, we construct another\nMDP whose objective is EZ(π)\n\u0002\n(Z(π) −ρ)+\u0003\nfor the appropriate choice of initial state distribution.\nStep 2: Construct alternative MDP. The MDP we construct is ˜\nM = ( ˜S, A, ˜D, ˜P, ˜R, T), where\nthe states are ˜S = S × R, the (time-varying, deterministic) rewards ˜R : ˜S × [T] →R are\n˜R((s, r), t) =\n\u001amax{r, 0}\nif t = T\n0\notherwise,\nand the transitions are\n˜P((s′, r′) | (s, r), a) = P(s′ | s, a) × PR(s,a)(r′ −r),\nnoting that ˜P is a (conditional) probability measure since the state space ˜S includes a continuous\ncomponent; in practice, we discretize the continuous component of the state space.\nStep 3: Value iteration. Letting S1 be the random initial state of the original MDP M (with\ndistribution D), we have\nEZ(π)\nh\n(Z(π) −ρ)+i\n= ES1\nh\n˜V (π)\n1\n((S1, −ρ))\ni\n,\nwhere ˜V (π)\n1\nis the value function of policy π for MDP ˜\nM on step t = 1. Thus, we have\nmin\nπ∈Π EZ(π)\nh\n(Z(π) −ρ)+i\n= ES1\nh\n˜V ∗\n1 ((S1, −ρ))\ni\n,\nwhere ˜V ∗\n1 is the value function of the optimal policy for ˜\nM. Intuitively, this strategy works because\nthe augmented component of the state space r captures the cumulative reward so far plus its initial\nvalue −ρ; then, by the deﬁnition of ˜R, the reward is r+, which implies that ˜V (π)\n1\n((s, −ρ)) is the\nexpectation of the random variable (Z(π) −ρ)+. Thus, we can compute minπ∈Π EZ(π)[(Z(π) −ρ)+]\nby performing value iteration on ˜\nM to compute ˜V (π)\n1\n. In particular, we have\n˜V ∗\nT ((s, r)) = max{r, 0},\nand\n˜V ∗\nt ((s, r)) = min\na∈A\nZ\n˜V ∗\nt+1((s′, r′)) · d ˜P((s′, r′) | (s, r), a)\nfor all t ∈{1, ..., T −1}. Then, given an initial state s1, we construct state ˜s1 = (s1, −ρ∗), where\nρ∗= arg inf\nρ∈R\n\u001a\nρ +\n1\n1 −α · ˜V (π)\n1\n((s, −ρ))\n\u001b\n,\nand then acting optimally in ˜\nM according to ˜V ∗\nt .\n21\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-10-11",
  "updated": "2022-10-11"
}