{
  "id": "http://arxiv.org/abs/2210.05866v1",
  "title": "Deep Learning for Iris Recognition: A Survey",
  "authors": [
    "Kien Nguyen",
    "Hugo Proença",
    "Fernando Alonso-Fernandez"
  ],
  "abstract": "In this survey, we provide a comprehensive review of more than 200 papers,\ntechnical reports, and GitHub repositories published over the last 10 years on\nthe recent developments of deep learning techniques for iris recognition,\ncovering broad topics on algorithm designs, open-source tools, open challenges,\nand emerging research. First, we conduct a comprehensive analysis of deep\nlearning techniques developed for two main sub-tasks in iris biometrics:\nsegmentation and recognition. Second, we focus on deep learning techniques for\nthe robustness of iris recognition systems against presentation attacks and via\nhuman-machine pairing. Third, we delve deep into deep learning techniques for\nforensic application, especially in post-mortem iris recognition. Fourth, we\nreview open-source resources and tools in deep learning techniques for iris\nrecognition. Finally, we highlight the technical challenges, emerging research\ntrends, and outlook for the future of deep learning in iris recognition.",
  "text": "Deep Learning for Iris Recognition: A Survey\nKIEN NGUYEN, Queensland University of Technology, Australia\nHUGO PROENÇA, University of Beira Interior, IT: Instituto de Telecomunicações, Portugal\nFERNANDO ALONSO-FERNANDEZ, Halmstad University, Sweden\nABSTRACT\nIn this survey, we provide a comprehensive review of more than 200 papers, technical reports, and GitHub\nrepositories published over the last 10 years on the recent developments of deep learning techniques for iris\nrecognition, covering broad topics on algorithm designs, open-source tools, open challenges, and emerging\nresearch. First, we conduct a comprehensive analysis of deep learning techniques developed for two main\nsub-tasks in iris biometrics: segmentation and recognition. Second, we focus on deep learning techniques for\nthe robustness of iris recognition systems against presentation attacks and via human-machine pairing. Third,\nwe delve deep into deep learning techniques for forensic application, especially in post-mortem iris recognition.\nFourth, we review open-source resources and tools in deep learning techniques for iris recognition. Finally,\nwe highlight the technical challenges, emerging research trends, and outlook for the future of deep learning\nin iris recognition.\nCCS Concepts: • Security and privacy →Biometrics.\nAdditional Key Words and Phrases: Iris Recognition, Deep Learning, Neural Networks\nACM Reference Format:\nKien Nguyen, Hugo Proença, and Fernando Alonso-Fernandez. 2022. Deep Learning for Iris Recognition: A\nSurvey. 1, 1 (October 2022), 35 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1\nINTRODUCTION\nThe human iris is a sight organ that controls the amount of light reaching the retina, by changing\nthe size of the pupil. The texture of the iris is fully developed before birth, its minutiae do not depend\non genotype, it stays relatively stable across lifetime (except for disease- and normal aging-related\nbiological changes), and it may even be used for forensic identification shortly after subject’s\ndeath [36, 110, 170].\nIn terms of its information theory-related properties, the iris texture has an extremely high\nrandotypic randomness, and is stable (permanent) over time, providing an exceptionally high\nentropy per mm.2 that justifies its higher discriminating power, when compared to other biometric\nmodalities (e.g., face or fingerprint). The iris’ collectability is another feature of interest and has been\nthe subject of discussion over the last years: while it can be acquired using commercial off-the-shelf\n(COTS) hardware, either handheld or stationary, data can be even collected from at-a-distance, up to\ntens of meters away from the subjects [111]. Even though commercial visible-light (RGB) cameras\nare able to image the iris, the near infrared-based (NIR) sensing dominates in most applications, due\nAuthors’ addresses: Kien Nguyen, Queensland University of Technology, Australia, nguyentk@qut.edu.au; Hugo Proença,\nUniversity of Beira Interior, IT: Instituto de Telecomunicações, Portugal, hugomcp@di.ubi.pt; Fernando Alonso-Fernandez,\nHalmstad University, Sweden, feralo@hh.se.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2022 Association for Computing Machinery.\nXXXX-XXXX/2022/10-ART $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n, Vol. 1, No. 1, Article . Publication date: October 2022.\narXiv:2210.05866v1  [cs.CV]  12 Oct 2022\n2\nK. Nguyen, H. Proença, F. Alonso-Fernandez\nto a better visibility of iris texture for darker eyes, rich in melanin pigment, which is characterized\nby lower light absorption in NIR spectrum compared to shorter wavelengths. In addition, NIR\nwavelengths are barely perceivable by the human eye, which augment users’ comfort, and avoids\npupil contraction/dilation that would appear under visible light.\nA seminal work by John Daugman brought to the community the Gabor filtering-based approach\nthat became the dominant approach for iris recognition [34, 35, 37]. Even though subsequent\nsolutions to iris image encoding and matching appeared, the IrisCodes approach is still dominant due\nto its ability to effectively search in massive databases with a minimal probability of false matches,\nat extreme time performance. By considering binary words, pairs of signatures are matched using\nXOR parallel-bit logic at lightening speed, enabling millions of comparisons/second per processing\ncore. Also, most of the methods that outperformed the original techniques in terms of effectiveness\ndo not work under the one-shot learning paradigm, assume multiple observations of each class to\nobtain appropriate decision boundaries, and - most importantly - have encoding/matching steps\nwith time complexity that forbid their use in large environments (in particular, for all-against-all\nsettings).\nIn short, Daugman’s algorithm encodes the iris image into a binary sequence of 2,048 bits by\nfiltering the iris image with a family of Gabor kernels. The varying pupil size is rectified by the\nCartesian-to-polar coordinate system transformation, to end up with an image representation of\ncanonical size, guarantying identical structure of the iris code independently of the iris and pupil\nsize. This makes possible to use the Hamming Distance (HD) to measure the similarity between two\niris codes [37]. Its low false match rate at acceptable false non-match rates is the key factor behind\nthe success of global-scale iris recognition installments, such as the national person identification\nand border security program Aadhaar program in India (with over 1.2 billion pairs of irises enrolled)\n[174], the Homeland Advanced Recognition Technology (HART) in the US (up to 500 million\nidentities) [128], or the NEXUS system, designed to speed up border crossings for low-risk and\npre-approved travelers moving between Canada and the US.\nDeep learning-based methods, in particular using various Convolutional Neural Network archi-\ntectures, have been driving remarkable improvements in many computer vision applications over\nthe last decade. In terms of biometrics technologies, it’s not surprising that iris recognition has\nalso seen an increasing adoption of purely data-driven approaches at all stages of the recognition\npipeline: from preprocessing (such as off-axis gaze correction), segmentation, encoding to matching.\nInterestingly, however, the impact of deep learning on the various stages of iris recognition pipeline\nis uneven. One of the primary goals of this survey paper is to assess where deep learning helped\nin achieving highly performance and more secure systems, and which procedures did not benefit\nfrom more complex modeling.\nThe remainder of the paper is structured as follows. Section 2 and 3 review the application of deep\nlearning in two main stages of the recognition pipeline: segmentation and recognition (encoding\nand comparison). Section 4 and 5 analyze the state of the art of deep learning-based approaches in\ntwo applications: Presentation Attack Detection (PAD) and Forensic. Section 6 investigates how\nhuman and machine can pair to improve deep learning based iris recognition. Section 7 focuses on\napproaches in less controlled environments of iris and periocular analysis. Section 8 reviews public\nresources and tools available in the deep learning based iris recognition domain. Section 9 focuses\non the future of deep learning for iris recognition with discussion on emerging research directions\nin different aspects of iris analysis. The paper in concluded in Section 10.\n2\nDEEP LEARNING-BASED IRIS SEGMENTATION\nThe segmentation of the iris is seen as an extremely challenging problem. As illustrated in Fig. 1,\nsegmenting the iris involves essentially three tasks: detect and parameterize the inner (pupillary)\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n3\nand outer (scleric) biological boundaries of the iris and also to locally discriminate between the\nnoise-free/noisy regions inside the iris ring, which should be subsequently used in the feature\nencoding and matching processes.\nThis problem has motivated numerous research works for decades. From the pioneering integro-\ndifferential operator [34] up to subsequent handcrafted techniques based in active contours and\nneural networks (e.g., [68], [133], [147] and [175]) a long road has been traveled in this problem.\nRegardless an obvious evolution in the effectiveness of such techniques, they all face particular\ndifficulties in case of heavily degraded data. Images are frequently motion-blurred, poor focused,\npartially occluded and off-angle. Additionally, in case of visible light data, severe reflections from\nthe environments surrounding the subjects are visible, and even augment the difficulties of the\nsegmentation task.\nRecently, as in many other computer vision tasks, DL-based frameworks have been advocated as\nproviding consistent advances over the state-of-the-art for the iris segmentation problem, with\nnumerous models being proposed. A cohesive perspective of the most relevant recent DL-based\nmethods is given in Table 1, with the techniques appearing in chronographic (and then alphabetical)\norder. The type of data each model aims to handle is given in the ”Data” column, along with\nthe datasets where the corresponding experiments were carried out and a summary of the main\ncharacteristics of each proposal (”Features” column). Here, considering that models were empirically\nvalidated in completely heterogeneous ways and using very different metrics, we decided not to\ninclude the summary performance of each model/solution.\nScleric Boundary Parameterization\n2\nNoise-free Texture Detection\n3\nPupillary Boundary Parameterization\n1\nIris Segmentation Main Tasks\nDimensionless Noise-Free\nRepresentation\n2\n1 +\n+ 3\nFig. 1. Three main tasks typically associated to iris segmentation: 1) parameterization of the pupillary (inner)\nboundary; 2) parameterization of the scleric (outer) boundary; and 3) discrimination between the unoccluded\n(noise-free) and occluded (noisy) regions inside the iris ring. Such pieces of information are further used to\nobtain dimensionless polar representations of the iris texture, where feature extraction methods typically\noperate.\nSchlett et al. [144] provided a multi-spectral analysis to improve iris segmentation accuracy\nin visible wavelengths by preprocessing data before the actual segmentation phase, extracting\nmultiple spectral components in form of RGB color channels. Even though this approach does\npropose a DL-based framework, the different versions of the input could be easily used to feed\nDL-based models, and augment the robustness to non-ideal data. Chen et al. [22] used CNNs\nthat include dense blocks, referred to as a dense-fully convolutional network (DFCN), where the\nencoder part consists of dense blocks, and the decoder counterpart obtains the segmentation\nmasks via transpose convolutions. Hofbauer et al. [72] parameterize the iris boundaries based on\nsegmentation maps yielding from a CNN, using a a cascaded architecture with four RefineNet\nunits, each directly connecting to one Residual net. Huynh et al. [76] discriminate between three\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n4\nK. Nguyen, H. Proença, F. Alonso-Fernandez\ndistinct eye regions with a DL model, and removes incorrect areas with heuristic filters. The\nproposed architecture is based on the encoder-decoder model, with depth-wise convolutions used\nto reduce the computational cost. Roughly at the same time, Li et al. [94] described the Interleaved\nResidual U-Net model for semantic segmentation and iris mask synthesis. In this work, unsupervised\ntechniques (K-means clustering) were used to create intermediary pictorial representations of the\nocular region, from where saliency points deemed to belong to the iris boundaries were found.\nKerrigan et al. [85] assessed the performance of four different convolutional architectures designed\nfor semantic segmentation. Two of these models were based in dilated convolutions, as proposed\nby Yu and Koltun [188]. Wu and Zhao [186] described the Dense U-Net model, that combines dense\nlayers to the U-Net network. The idea is to take advantage of the reduced set of parameters of\nthe dense U-Net, while keeping the semantic segmentation capabilities of U-Net. The proposed\nmodel integrates dense connectivity into U-Net contraction and expansion paths. Compared with\ntraditional CNNs, this model is claimed to reduce learning redundancy and enhance information\nflow, while keeping controlled the number of parameters of the model. Wei et al. [205] suggested\nto perform dilated convolutions, which is claimed to obtain more consistent global features. In\nthis setting, convolutional kernels are not continuous, with zero-values being artificially inserted\nbetween each non-zero position, increasing the receptive field without augmenting the number of\nparameters of the model.\nMore recently, Ganeva and Myasnikov [55] compared the effectiveness of three convolutional\nneural network architectures (U-Net, LinkNet, and FC- DenseNet), determining the optimal pa-\nrameterization for each one. Jalilian et al. [79] introduced a scheme to compensate for texture\ndeformations caused by the off-angle distortions, re-projecting the off-angle images back to frontal\nview. The used architecture is a variant of RefineNet [96], which provides high-resolution prediction,\nwhile preserving the boundary information (required for parameterization purposes).\nThe idea of interactive learning for iris segmentation was suggested by Sardar et al. [142],\ndescribing an interactive variant of U-Net that includes Squeeze Expand modules. Trokielewicz\net al. [172] used DL-based iris segmentation models to extract highly irregular iris texture areas\nin post-mortem iris images. They used a pre-trained SegNet model, fine-tuned with a database\nof cadaver iris images. Wang et al. [178] (further extended in [179]) described a lightweight deep\nconvolutional neural network specifically designed for iris segmentation of degraded images\nacquired by handheld devices. The proposed approach jointly obtains the segmentation mask and\nparameterized pupillary/limbic boundaries of the iris.\nObserving that edge-based information is extremely sensitive to be obtained in degraded data,\nLi et al. [7] presented an hybrid method that combines edge-based information to deep learning\nframeworks. A compacted Faster R-CNN-like architecture was used to roughly detect the eye and\ndefine the initial region of interest, from where the pupil is further located using a Gaussian mixture\nmodel. Wang et al. [184] trained a deep convolutional neural network(DCNN) that automatically\nextracts the iris and pupil pixels of each eye from input images. This work combines the power of\nU-Net and SqueezeNet to obtain a compact CNN suitable for real time mobile applications. Finally,\nWang et al. [176] parameterize both the iris mask and the inner/outer iris boundaries jointly, by\nactively modeling such information into a unified multi-task network.\nA final word is given to segmentation-less techniques. Assuming that the accurate segmentation of\nthe iris boundaries is one of the hardest phases of the whole recognition chain and the main source\nfor recognition errors, some recent works have been proposing to perform biometrics recognition\nin non-segmented or roughly segmented data [132][135]. Here, the idea is to use the remarkable\ndiscriminating power of DL-frameworks to perceive the agreeing patterns between pairs of images,\neven on such segmentation-less representations.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n5\nTable 1. Cohesive comparison of the most relevant DL-based iris segmentation methods (NIR: near-infrared;\nVW: visible wavelength). Methods are listed in chronological (and then alphabetical) order.\nMethod\nYear\nData\nDatasets\nFeatures\nNIR\nVW\nSchlett et al. [144]\n2018\n✗\n✓\nMobBIO\nPreprocessing (combines different possibili-\nties of the input RGB channels)\nTrokielewicz\nand\nCzajka [166]\n2018\n✓\n✓\nWarsaw-Post-Mortem v1.0\nFine-tuned CNN (SegNet)\nChen et al. [22]\n2019\n✓\n✓\nCASIA-Irisv4-Interval, IITD, UBIRIS.v2\nDense CNN\nHofbauer\net\nal. [72]\n2019\n✓\n✗\nIITD, CASIA-Irisv4-Interval, ND-Iris-0405\nCascaded architecture of four RefineNet,\neach connecting to one Residual net\nHuynh et al. [76]\n2019\n✓\n✗\nOpenEDS\nMobileNetV2 + heuristic filtering postproc.\nLi et al. [7]\n2019\n✓\n✗\nCASIA-Iris-Thousand\nFaster-R-CNN (ROI detection)\nKerrigan et al. [85]\n2019\n✓\n✓\nCASIA-Irisv4-Interval,\nBioSec,\nND-Iris-\n0405,\nUBIRIS.v2,\nWarsaw-Post-Mortem\nv2.0, ND-TWINS-2009-2010\nResent + Segnet (with dilated convolutions)\nWu and Zhao [186]\n2019\n✓\n✓\nCASIA-Irisv4-Interval, UBIRIS.v2\nDense-U-Net (dense layers + U-Net)\nWei et al. [205]\n2019\n✓\n✓\nCASIA-Iris4-Interval,\nND-IRIS-0405,\nUBIRIS.v2\nU-Net with dilated convolutions\nFang\nand\nCzajka [50]\n2020\n✓\n✓\nND-Iris-0405,\nCASIA,\nBATH,\nBioSec,\nUBIRIS, Warsaw-Post-Mortem v1.0 & v2.0\nFine-tuned CC-Net [106]\nGaneva and Myas-\nnikov [55]\n2020\n✓\n✗\nMMU\nU-Net, LinkNet, and FC-DenseNet (perfor-\nmance comparison)\nJalilian et al. [79]\n2020\n✓\n✗\nRefineNet + morphological postprocessing\nSardar et al. [142]\n2020\n✓\n✓\nCASIA-Irisv4-Interval, IITD, UBIRIS.v2\nSqueeze-Expand module + active learning\n(interactive segmentation)\nTrokielewicz\net\nal. [172]\n2020\n✓\n✓\nND-Iris-0405,\nCASIA,\nBATH,\nBioSec,\nUBIRIS, Warsaw-Post-Mortem v1.0 & v2.0\nFined-tuned SegNet [9]\nWang et al [178]\n2020\n✓\n✓\nCASIA-Iris-M1-S1/S2/S3, MICHE-I\nHourglass network\nWang et al. [176]\n2020\n✓\n✓\nCASIA-v4-Distance, UBIRIS.v2, MICHE-I\nU-Net + multi-task attention net + postproc.\n(probabilistic masks priors & thresholding)\nLi et al. [94]\n2021\n✓\n✗\nCASIA-Iris-Thousand\nIRU-Net network\nWang et al. [184]\n2021\n✗\n✓\nOnline Video Streams and Internet Videos\nU-Net and Squeezenet to iris segmentation\nand detect eye closure\nKuehlkamp\net al. [91]\n2022\n✓\n✓\nND-Iris-0405,\nCASIA,\nBATH,\nBioSec,\nUBIRIS, Warsaw-Post-Mortem v2.0\nFined-tuning of Mask-RCNN architecture\n3\nDEEP LEARNING-BASED IRIS RECOGNITION\n3.1\nDeep Learning Models as a Feature Extractor\nAs illustrated in Fig. 2, the idea here is to analyze a dimensionless representation of the iris data and\nproduce a feature vector that lies in a hyperspace (embedding) where recognition is carried out.\nIn this context, Boyd et el. [15] explored five different sets of weights for the popular ResNet50\narchitecture to test if iris-specific feature extractors perform better than models trained for general\ntasks. Minaee et al. [105] studied the application of deep features extracted from VGG-Net for\niris recognition, having authors observed that the resulting features can be well transferred to\nbiometric recognition. Luo et al. [102] described a DL model with spatial attention and channel\nattention mechanisms, that are directly inserted into the feature extraction module. Also, a co-\nattention mechanism adaptively fuses features to obtain representative iris-periocular features.\nHafner et al. [65] adapted the classical Daugman’s pipeline, using convolutional neural networks\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n6\nK. Nguyen, H. Proença, F. Alonso-Fernandez\nDL-based Feature Representation\nDimensionless Noise-Free\nRepresentation\nFeature Set\n\n𝑓1\n𝑓2\n...\n𝑓𝑡\n\nFig. 2. The main task of DL-based iris feature extraction: given a dimensionless representation of the iris\ndata, obtain its compact and representative representation - the feature set - that is further used in the\nclassification phase.\nto function as feature extractors. The DenseNet-201 architecture outperformed its competitors\nachieving state-of- the-art results both in the open and close world settings. Menotti et al. [104]\nassessed how DL-based feature representations can be used in spoofing detection, observing that\nspoofing detection systems based on CNNs can be robust to attacks already known and adapted,\nwith little effort, to image-based attacks that are yet to come.\nYang et al. [196] generated multi-level spatially corresponding feature representations by an\nencoder-decoder structure. Also, a spatial attention feature fusion module was used to ensemble\nthe resulting features more effectively. Chen et al. [23] addressed the large-scale recognition\nproblem and described an optimized center loss function (tight center) to attenuate the insufficient\ndiscriminating power of the cross-entropy function. Nguyen et al. [112] explored the performance\nof state-of-the-art pre-trained CNNs on iris recognition, concluding that off-the-shelf CNN generic\nfeatures are also extremely good at representing iris images, effectively extracting discriminative\nvisual features and achieving promising results. Zhao et al. [207] proposed a method based on the\ncapsule network architecture, where a modified routing algorithm based on the dynamic routing\nbetween two capsule layers was described, with three pre-trained models (VGG16, InceptionV3,\nand ResNet50) extracting the primary iris features. Next, a convolution capsule replaces the full\nconnection capsule to reduce the number of parameters. Wang and Kumar [180] introduced the\nconcept of residual feature for iris recognition. They described a residual network learning procedure\nwith offline triplets selection and dilated convolutional kernels.\nOther works have addressed the extraction of appropriate feature representations in multi-\nbiometrics settings: Damer et al. [32] propose to jointly extract multi-biometric representations\nwithin a single DNN. Unlike previous solutions that create independent representations from\neach biometric modality, they create these representations from multi-modality (face and iris),\nmulti-instance (iris left and right), and multi- presentation (two face samples), which can be seen\nas a fusion at the data level policy. Finally, concerned about the difficulty of performing reliable\nrecognition in hand-held devices, Odinokikh et al. [121] combined the advantages of handcrafted\nfeature extractors and advanced deep learning techniques. The model utilizes shallow and deep\nfeature representations in combination with characteristics describing the environment, to reduce\nthe intra-subject variations expected in this kind of environments.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n7\n3.2\nDeep Learning-based Iris Matching Strategies\nThe existing matching strategies can be categorized into three categories: (1) using conventional\nclassifiers, such as SVM, RF, and Sparse Representation; (2) softmax-based losses; and (3) pairwise-\nbased losses. A cohesive perspective of the most relevant recent DL-based methods is given in\nTable 2, with the techniques appearing in chronographic (and then alphabetical) order\n3.2.1\nConventional classifiers. Various researchers have been using deep learning networks de-\nsigned and pre-trained on the ImageNet dataset to extract iris feature representations, followed by a\nconventional classifier such as SVM, RF, Sparse Representation, etc. [15, 18, 112]. The key benefit of\nthese approaches is the simplicity of “plug and play”, where proven and pre-trained deep learning\nnetworks inherited from large-scale computer vision challenges are widely available and ready to\nbe used [112]. Another benefit is that there is no need for large scale iris image datasets to train\nthese networks because they have already been trained on such large-scale datasets as ImageNet.\nConsidering these networks usually contain hundreds of layers and millions of parameters, and\nrequire millions of images to train, using pre-trained networks is extremely beneficial.\n3.2.2\nIris Classification Networks. Iris classification networks couple deep learning architectures\nwith a family of softmax-based losses to classify an iris image into a list of known identities.\nCoupling a softmax loss with a backbone network enables training the backbone network in an\nend-to-end manner via popular optimization strategies such as back-propagation and steepest\ngradient decent. Compared to the conventional classifier approaches, the DL-based backbones in\nthis category are learnable directly from the iris data, allowing them to better represent the iris.\nThe key benefit is that it is similar to a generic image classification task, hence all designs and\nalgorithms in the generic image classification task can be trivially applied with the iris image data.\nTypical examples of these iris classification networks are [15, 56]. However, these softmax-based\nnetworks require the iris in the test image be known in the identity classes in the training set,\nwhich means the networks must be re-trained whenever a new class (i.e. a new identity) is added.\nGangwar et al. proposed two backbone networks (i.e. DeepIrisNet-A and DeepIrisNet-B) followed\nby a softmax loss for the iris recognition task [56]. Later, they proposed another backbone network,\nbut still followed by a softmax loss to classify one normalized iris image into a pre-defined list of\nidentity [57].\nBackbone Network Architectures: A wide range of backbone network architectures have been\nborrowed from generic image classification for the iris recognition task due to their similarity.\n• AlexNet: AlexNet is the most primitive and been shown as least accurate for iris recognition\ncompared to others [16, 112].\n• VGG: Boyd et al. [16], Nguyen et al. [112] and Minaee et al. [105] all experimented VGG16 .\n• ResNet: ResNet with its variants are the most popular backbone network architecture. Nguyen\net al. experimented ResNet152 [112]. Boyd et al. experimented three variants ResNet18,\nResNet50 and ResNet152 in their post-mortem iris classification task [16].\n• Inception: Zhao et al. employed capsule network based on the InceptionV3 architecture [207].\n• EfficientNet: Hsiao et al. [74] employed EfficientNet to extract iris features.\n3.2.3\nIris Similarity Networks. Iris similarity networks couple deep learning architectures with a\nfamily of pairwise-based losses to learn a metric representing how similar or dissimilar two iris\nimages are without knowing their identities. The pairwise loss aims to pull images of the same iris\ncloser and push images of different irises away in the similarity distance space. Different to the iris\nclassification networks which only operate in an identification mode on a pre-defined identity list,\niris similarity networks operate across both verification and identification modes with an open\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n8\nK. Nguyen, H. Proença, F. Alonso-Fernandez\nset of identities [209]. Typical examples of these iris similarity networks are [80, 97, 113, 180, 209].\nThere are three key benefits of these networks: (i) verification and identification: iris similarity\nnetworks operate across both verification and identification modes; (ii) open set of identities: iris\nsimilarity networks operate on an open set of identities; and (iii) explicit reflection: iris similarity\nnetworks directly and explicitly reflect what we want to achieve, i.e., small distances between irises\nof the same subject and larger distances between irises of different subjects.\nPairwise loss: Nianfeng et al. [97] proposed a pairwise network, which accepts two input images and\ndirectly outputs a similarity score. They designed a pairwise layer which accepts two input images\nand encodes their features via a backbone network. The backbone network is trained iteratively to\nminimize the dissimilarity distance between genuine pairs (pairs of the same identity) and maximize\nthe dissimilarity distance between impostor pairs (pairs of the different identities).\nTriplet loss: Since the pairwise network is trained with separate genuine and impostor pairs, it may\nnot converge well, which has been proven in the face recognition [145]. Rather than using one\npair of two images to update the training as in the pairwise loss for each training iteration, the\ntriplet loss employs a triplet of three images: an anchor image, a positive image with the same\nidentity and a negative image with a different identity [145]. The backbone network is trained to\nsimultaneously minimize the similarity distance between the positive and the anchor images and\nmaximize the distance between the negative and the anchor images. Tailored for iris images, Zhao\net al. [180, 209, 211] proposed Extended Triplet Loss (EPL) to incorporate a bit-shifting operation to\ndeal with rotation in the normalized iris images. Nguyen et al. also employed the ETL for their iris\nrecognition network [113, 115]. Kuehlkamp et al. [91] proposed to improve the generic triplet loss\nfunction for iris recognition by forcing the distance to be positive (through the use of a sigmoid\noutput layer), and adding a logarithmic penalty to the error. This modification allows the network\nto learn even when the difference between samples is negative and converge faster. Yan et al. [195]\nextended the generic triplet loss to batch triplet loss, in which the triplet loss is calculated over a\nbatch of 𝑆subjects and 𝐾images for each subject. Performing batch triplet loss is usually expected\nto have smooth loss function. Yang et al. [196] improved triplet selection method for training by\nBatch Hard [197].\nBackbone Network Architectures: Different to the classification iris networks, similarity iris networks\nare usually designed with their own network architectures and are usually much “shallower” than\nthe classification counterparts.\n• FCN: All similarity iris networks employ Fully Convolutional Networks (FCNs) instead of\nCNNs. Compared to CNNs, FCNs [100] do not have fully connected layer, allowing the output\nmap to preserve the original spatial information. This is important to iris recognition since\nthe output map can preserve spatial correspondence with the original input image [113, 209],\nthus enabling pixel-to-pixel matching. Zhao et al. [209] proposed a FCN architecture with 3\nconvolutional layers, followed by activation and pooling layers. Outputs of convolutional\nlayers are up-sampled to the original input image size. The up-samples features are stacked\nand convolved by another convolutional layer to generate a 2-dimension features with\nthe same size as the input image. Later, they extended the backbone network with dilated\nconvolutions [180]. Yan et al. [195] employed a ResNet architecture and fine-tuned it with\nthe triplet loss. Kuehlkamp et al. only used a part of the ResNet architecture.\n• NAS: Nguyen et al. [113] proposed to learn the network architecture directly from data rather\nthan hand-designing it or using generic-image-classification architectures. They proposed a\ndifferential Neural Architecture Search (NAS) approach that models the architecture design\nprocess as a bi-level constrained optimization approach. This approach is not only able to\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n9\nsearch for the optimal network which achieves the best possible performance, but it can also\nimpose constraints on resources such as model size or number of computational operations.\n• Complex-valued: Observing that there is an intrinsic difference between the iris texture and\ngeneric object-based images where the iris texture is stochastic without consistent shapes,\nedges, or semantic structure, Nguyen et al. [115] argued the network architecture has to be\nbetter tailored to incorporate domain-specific knowledge in order to reach the full potential\nin the iris recognition setting. Another observation that they made is a majority of well-\nknown handcrafted features such as IrisCode [35] transformed iris texture image into a\ncomplex-valued representation first, then further encoded the complex-valued representation\nto get a final representation. They proposed to use fully complex-valued networks rather\nthan popular real-valued networks. Complex-valued backbone networks better retain the\nphase, are more invariant to multi-scale, multi-resolution and multi-orientation, have solid\ncorrespondence with the classic Gabor wavelets [173], hence are much better suited to iris\nrecognition than their real-valued counterparts.\n3.3\nEnd-to-end Joint Iris Segmentation+Recognition Networks\nAlmost all existing approaches perform segmentation and normalization to transform an input\nimage to a normalized rectangular 2D representation before recognition as this simplifies the repre-\nsentation learning. As segmentation and recognition may require a separate network themselves,\nthis would cause redundancy in both computation and training, further slowing down an DL-based\niris recognition approach. Several researchers have looked at approaches to perform end-to-end\nnetworks. One category is to perform segmentation-less recognition. Another category is to jointly\nlearn segmentation and recognition using an unified network via multi-task learning.\nSegmentation-less: These approaches feed the cropped iris images directly into a deep learning\nnetwork to extract features. For example, Kuehlkamp et al. [91] used Mask R-CNN for semantic\nsegmentation and fed the cropped iris region directly into a ResNet50 to extract features. Similarly,\nChen et al. [24] also fed the cropped iris images directly into a DenseNet. Rather than feeding\nthe cropped iris images directly, Proenca et al. transformed the cropped region (which is detected\nby SSD) into a polar representation first, then fed the polar representation into the VGG19 for\nextracting features [135].\nMulti-task: Segmentation and recognition can be jointly learned with one unified network. This\npaves a way for multi-task learning. However, segmentation and recognition may require different\nnumber of layers, hence research is required to perform using different intermediate layers for\neach task. To the best knowledge, there does not exist any approach to explore this direction.\n4\nDEEP LEARNING-BASED IRIS PRESENTATION ATTACK DETECTION\nIn parallel to the popularity of biometrics, the security of these systems against attacks has become\nof paramount importance. The most common attack is a Presentation Attack (PA), which refers\nto presenting a fake sample to the sensor. The goal can be either to impersonate somebody else\nidentity (also known as Impostor Attack Presentation), or to conceal the own identity (also known\nas Concealer Attack Presentation). Via impostor attacks, a person could also enroll fraudulently,\nallowing a continuous manipulation of the system. The previous acronyms and terms in italics\ncorrespond to the vocabulary recommended in the series of ISO/IEC 30107 standards of the ISO/IEC\nSubcommittee 37 (SC37) on Biometrics [163], which we will follow in the rest of this section.\nPresentation Attack Instruments (PAI) used to carry out impostor attacks are typically generated\nfrom bona fide images of an iris from an individual who has legitimate access to the system. The\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n10\nK. Nguyen, H. Proença, F. Alonso-Fernandez\nTable 2. Cohesive comparison of the most relevant DL-based iris recognition methods (NIR: near-infrared;\nVW: visible wavelength). Methods are listed in chronological (and then alphabetical) order.\nCategory\nMethod\nYear\nData\nDatasets\nFeatures\nNIR\nVW\nConventional\nclassifiers\nMenotti\net\nat.\n[104]\n2015\n✓\n✓\nBiosec, LivDet-2013-Warsaw, Mob-\nBIOfake\nShallow CNNs + SVM for Spoofing De-\ntection\nMinaee et al. [105] 2016\n✓\n✗\nCASIA-Iris-Thousand, IITD\nVGG + SVM\nNguyen\net\nal. [112]\n2017\n✓\n✗\nND-CrossSensor-2013,\nCASIA-Iris-\nThousand\nAlexNet,\nVGG,\nGoogle\nInception,\nResNet, DenseNet + SVM\nBoyd et al. [15]\n2019\n✓\n✓\nCASIA-Irisv4-Interval,\nIITD,\nUBIRIS.v2\nResNet50 + SVM\nBoyd et al. [18]\n2020\n✓\n✓\nDCMEO1, Warsaw\nAlexNet, ResNet, VGG, DenseNet + Co-\nsine, Euclidean, MSE\nHafner\net\nal. [65]\n2021\n✓\n✗\nCASIA-Iris-Thousand\nResNet101 + DenseNet-201 + Cosine\nSimilarity\nClassification\nNetworks\nGangwar\net\nal. [56]\n2016\n✓\n✗\nND-IRIS-0405,\nND-CrossSensor-\n2013\nDeepIrisNet\nGangwar\net\nal. [57]\n2019\n✓\n✓\nND-IRIS-0405, UBIRIS.v2, MICHE-I,\nCASIA-Irisv4-Interval\nDeepIrisNetV2\nOdinokikh\net\nal. [121]\n2019\n✓\n✗\nCASIA-Iris-M1-S2,\nCASIA-Iris-M1-\nS3, Iris-Mobile\nFeature Fusion + Softmax\nZhao et al.[207]\n2019\n✓\n✗\nJluIrisV3.1, JluIrisV4, CASIA-Irisv4-\nLamp\nCapsule network + Softmax\nChen et al. [23]\n2020\n✓\n✗\nND-IRIS-0405,\nCASIA-Iris-\nThousand, IITD cross sensor\nT-Center loss\nLuo et al.[102]\n2021\n✓\n✗\nND-IRIS-0405, CASIA-Iris-Thousand\nAttention + Softmax Loss + Center Loss\nSimilarity\nNetworks\nNianfeng\net\nal. [97]\n2016\n✓\n✗\nQ-FIRE, CASIA-Cross-Sensor\nDeepIris\nZhao et al. [209]\n2017\n✓\n✓\nCASIA-Irisv4-Interval,\nIITD,\nUBIRIS.v2\nUniNet (FeatNet+MaskNet) + Extended\nTriplet Loss\nDamer\net\nal. [32]\n2019\n✓\n✗\nBiosecure,\nCASIA-Iris-\nThousand/Lamp/Interval\nInception + Triplet Loss\nWang\net\nal. [180]\n2019\n✓\n✓\nCASIA-Irisv4-Interval,\nIITD,\nUBIRIS.v2\nFeatNet + Dilated Convolution + Ex-\ntended Triplet Loss\nZhao et al. [211]\n2019\n✓\n✗\nND-Iris-0405, Casia-Irisv4-Distance,\nIITD\nFeatNet + Mask RCNN + Extended\nTriplet Loss\nNguyen\net\nal. [113]\n2020\n✓\n✓\nCASIA-v4-Distance, UBIRIS.v2, ND-\nCrossSensor-2013\nConstrained Design Backbone + Ex-\ntended Triplet Loss\nYan et al. [195]\n2021\n✓\n✗\nCASIA-Iris-Thousand\nSpatial Feature Reconstruction + Triplet\nLoss\nYang et al. [196]\n2021\n✓\n✗\nCASIA-Irisv4-Thousand,\nCASIA-\nIrisv4-Distance, IITD\nDual Spatial Attention Network + Batch\nHard\nNguyen\net\nal. [115]\n2022\n✓\n✓\nND-CrossSensor-2013,\nCASIA-Iris-\nThousand, UBIRIS.v2\nComplex-valued Backbone + Extended\nTriplet Loss\nKuehlkamp\net\nal. [91]\n2022\n✓\n✓\nDCMEO1, DCMEO2, Warsaw-Post-\nMortem v2.0\nResNet + Triplet Loss\niris is printed on a piece of paper (printout attack) or displayed on a screen (replay attack) and\nthen presented to the sensor. The iris of deceased individuals can also be used as PAI, since the\ntexture remains intact for some hours [169]. Theoretically, it would be possible to print a genuine\niris texture into a contact lens as well, although this has not been successfully demonstrated yet\n[16]. Concealer attacks, on the other hand, are commonly done via textured contact lenses that\nobscure or alter properties of the eye (such as color) to prevent the system from identifying the\nuser. Synthetic iris images [191] not belonging to any specific identity could be used for similar\npurposes. Concealers can also present their legitimate iris, but in a way not expected by the system,\ne.g. closing eyelids as much as possible, looking to the sides (off-axis gaze), rotating the head, etc.\nTwo challenges of PAs is that they happen outside the physical limits of the system, and they do\nnot require specific knowledge of its inner workings, or any technical knowledge at all. Thus, if no\nproperly tackled, they can derail public perception of even the most reliable biometric modality. It is\neven more critical if authentication is done without any supervision. Presentation Attack Detection\n(PAD) methods to counteract such attacks can be done [54]: 𝑖) at the hardware (or sensor) level,\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n11\nusing additional illuminators or sensors that detect intrinsic properties of a living eye or responses\nto external stimuli (like pupil contraction or reflection), or 𝑖𝑖) at the software level, using only the\nfootprint of the PA (if any) left in the same images captured with the standard sensor that will\nbe employed for authentication. Software-based techniques are in principle less expensive and\nintrusive, since they do not demand extra hardware, and they will be the focus of this section.\nTwo comprehensive surveys on PAD are [30] (2018) and [16] (2020). While DL techniques were\nresidual in the 2018 survey, they rose in popularity thereafter. We build this section upon the\nlatest survey and summarize the most important developments in DL-PAD since it was published\n(Table 3). A descriptive summary of the datasets employed is given later in Section 8. The aim of\nPAD is to classify an image either as a bona fide or an attack presentation, so it is usually modeled\nas a two-class classification task. Typical strategies mimic the trend of the previous section when\napplying DL to iris recognition: either a CNN backbone is used to extract features that will feed a\nconventional classifier, or the network is trained end-to-end to do the classification itself. Some\nhybrid methods also combine traditional hand-crafted with deep-learned features. In the same\nmanner, the network may be initialized e.g. on the ImageNet dataset to take advantage of such\nlarge generic corpus, since available iris PAD data is more scarce. Another strategy also employed\nwidely in the PAD literature is to use adversarial networks, where a GAN [60] is trained to generate\nsynthetic iris images that the discriminator must use to detect attack samples.\n4.1\nCNNs for Feature Extraction\nSince each layer of a CNN represents a different level of abstraction, Fang et al. [44] fused the\nfeatures from the last four convolutional layers of two models (VGG16, MobileNetv3-small). The\nfeatures are projected to a lower dimensional space by PCA and either concatenated for classification\nwith SVM (feature fusion) or the classification scores of each level combined (score fusion). Using\ntwo databases of printouts and textured contact lenses, the method showed superiority over the use\nof the different layers individually, or the feature vector from the next-to-last layer of the networks.\n4.2\nEnd-to-end Classification Networks\nArora and Bathia [8] trained a CNN with 10 convolutional layers to detect contact lenses and\nprintouts. Rather than using the entire image, the network is trained on patches from all parts of\nthe iris image. The system showed superior performance compared to state-of-the-art methods\nwhich at that time, according to the paper, were mostly based on hand-crafted features.\nFocusing on embedded low-power devices, Peng et al. [126] adopted a Lite Anti-attack Iris Loca-\ntion Network (LAILNet) based on three dense blocks featuring depthwise separable convolutions\nto reduce the number of parameters. The algorithm demonstrated very good performance on three\ndatabases with printouts, synthetic irises, contact lenses and artificial plastic eyes.\nAlso focusing on mobiles, Fang et al. [45, 48] used MobileNetv3-small. The contribution lies in\nthe division of the normalized iris image into overlapped micro-stripes which are fed individually,\nand a decision reached by majority voting. The claimed advantages are that the classifier is forced\nto focus on the iris/sclera boundaries (given by their exact micro-stripes), the input dimensionality\nis lower and the amount of samples is higher (reducing overfitting), and the impact of imprecise\nsegmentation is alleviated. Using three databases with contact lenses and printouts, the paper\nfeatured an extensive experimentation with cross-database, cross-sensor, and cross-attack setting.\nSharma and Ross [150] proposed D-NetPAD, based on DenseNet121, chosen due to benefits such\nas maximum flow of information given by dense connections to all subsequent layers, or fewer\nparameters compared to counterparts like ResNet or VGG. The PAI included printouts, artificial eye,\ncosmetic contacts, kindle replay, and transparent dome on print, with experiments substantiating\nthe effectiveness of the method on cross-PAI, cross-sensor and cross-database scenarios.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n12\nK. Nguyen, H. Proença, F. Alonso-Fernandez\nChen and Ross [19] proposed an explainable attention-guided detector (AG-PAD). To do so,\nthe feature maps of a DenseNet121 were fed into two modules that independently capture inter-\nchannel and inter-spatial feature dependencies. The outputs were then fused via element-wise sum\nto capture complementary attention features from both channel and spatial dimensions. With three\ndatasets containing colored contact lenses, artificial eyes (Van Dyke/Doll fake eyes), printouts, and\ntextured contact lenses, the attention modules are shown to improve accuracy over the baseline\nnetwork. Using heatmap visualization, it is also shown that the attention modules force the network\nto attend to the annular iris textural region which, intuitively, plays a vital role for PAD.\nSpatial attention was also explored by Fang et al. [46]. To find local regions that contribute the\nmost to make accurate decisions and capture pixel/patch-level cues, they proposed an attention-\nbased pixel-wise binary supervision (A-PBS) method. To capture different levels of abstraction, they\nperform multi-scale fusion by adding spatial attention modules to feature maps from three levels of\na DenseNet backbone. Using six datasets with textured lenses and printouts, they outperformed\nprevious state-of-the-art including scenarios with unknown attacks, sensors, and databases.\nGiven the difficulty of collecting iris PAD data, most databases contain, at most, a few hundred\nsubjects. To address this, Fang et al. [47] studied data augmentation techniques that modify position,\nscale or illumination. Using three architectures (ResNet50, VGG16, MobileNetv3-small) and three\ndatabases with printouts and textured contact lenses, they found that data augmentation improves\nPAD performance significantly, but each technique has a positive role on a particular dataset or\nCNN. They also explored the selection of augmentation techniques, finding, again, no consensus\nregarding the best combination, which was attributed to differences in capture environment, subject\npopulation, scale of the different datasets or imbalance between bona fide and attack samples.\nGupta et al. [63] proposed MVANet, with 5 convolutional layers and 3 branches of fully connected\nlayers. They addressed the challenge of unseen databases, sensors, and imaging environment on\ntextured contact lenses detection. The size of each layer of MVANet is different, thus capturing\ndifferent features. They used three databases, each one captured in different settings (indoor/outdoor,\ndifferent times of the day, varying weather, fixed/mobile sensors, etc.), with MVANET trained in\none database at a time and tested on the other two. As baseline, they fine-tuned three popular\nCNNs (VGG16, ResNet18, DenseNet) initialized on ImageNet. The proposed network is shown to\nperform consistently better and more uniformly on the test databases than the baseline approaches.\nSharma and Ross [151] studied the viability of Optical Coherence Tomography (OCT). OCT\nprovides a cross-sectional view of the eye, whereas traditional NIR or VW imaging provides 2D\ntextural data. The PAIs considered are artificial eyes (Van Dyke eyes) and cosmetic lenses, evaluated\non three different CNNs (VGG19, ResNet50, DenseNet121). By both intra- (known PAs) and cross-\nattack (unknown PAs) scenarios, OCT is determined as a viable solution, although hardware cost is\nstill a limiting factor. Indeed, OCT outperforms NIR and VW in the intra-attack scenario, while\nNIR generalizes better to unseen PAs. Cosmetic lenses also appear to be more difficult to detect\nthan artificial eyes with any modality. Via heatmaps, it is seen as well that the fixation regions are\ndifferent for each imaging modality and for each PAI, which could be a source of complementarity.\nZhang et al. [199] proposed a Weighted Region Network (WRN) to detect cosmetic lenses\nthat includes a local attention Weight Network (for evaluating the discriminating information of\ndifferent regions) and a global classification Region Network (for characterizing global features).\nSuch strategy considers both the entire image and the attention effect by assigning different weights\nto regions. The mentioned networks are applied to a VGG16 backbone. The reported results showed\nimproved performance compared to the state-of-the-art over three different databases.\nThe works by Agarwal et al. [1, 2] evaluated the detection of contact lenses. In [2], they trained a\nsiamese CNN of 5 convolutional layers on two different inputs (the original image and its CLAHE\nversion), which are then combined by weighted score fusion of the softmax layer. Adding a processed\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n13\nversion of the raw image attempts to enhance the feature extraction capabilities of the CNN. A\nsimilar strategy is followed in [1], but here they used a siamese contraction-expansion CNN, and the\nprocessed image is a edge-enhanced image obtained via Histogram of Oriented Gradients (HOG).\nAnother difference was the use of feature-level fusion of the next-to-last CNN feature vectors,\ntesting different strategies (vector addition, multiplication, concatenation and distance). The papers\nemployed several databases, with an extensive protocol including unseen subjects, environments\n(indoor vs outdoor) and databases (sensors) that showcases the strength of the solutions against\ncross-domain changes. The methods also showed superiority against popular CNN models (VGG16,\nResNet18, DenseNet) and the popular LBP and HOG hand-crafted features.\nGautam et al. [59] proposed a Deep Supervised Class Encoding (DSCE) approach consisting of an\nAutoencoder that exploits class information, and minimizes simultaneously the reconstruction and\nclassification errors during training. Three datasets were used, containing textured lenses, printouts\nand synthetic images, showing superiority over a variety of hand-crafted and deep-learned features.\nTapia et al.[162] used a two-stages serial architecture based on a modified MobiletNetv2. A first\nnetwork was trained to only distinguish two classes (bona fide vs attack). If it votes bona fide, the\nimage is sent to a second network trained to classify it among three or four classes (bona fide or\na different type of PAI: contact lenses, printout, or cadaver). Four databases were combined to\nobtain a super-set with the different PAIs, and class-weights were also incorporated into the loss\nto compensate imbalance. The paper applied contrast enhancement (CLAHE), and an aggressive\ndata augmentation (rotation, blurring, contrast change, Gaussian noise, edge enhancement, image\nregion dropout, etc.). They tested two image sizes, 224×224 and 448×448, observing that the extra\ndetail of a higher resolution image results in more effective features. The paper also carried out\nleave-one-out PAI tests for open-set evaluation, showing robustness in detecting unknown attacks.\n4.3\nHybrid Methods\nChoudhary et al. [26, 27] applied a Friedman test-based selection method to identify the best features\nof a set of hand-crafted and deep-learned ones. Each feature method feeds a SVM classifier, and the\nscores of the individual SVMs are fused via weighted sum. A preliminary version of [26] without\nfeature selection appeared in [25]. The databases of [27] include a medley of different PA (printouts,\nsynthetic irises, artificial eyeballs, etc.), although the feature selection and classification methods\nare trained and evaluated separately on each database. The authors observed a saturation after a\ncertain number of features are combined, and a superiority of the score-level fusion over other\nmethods such as majority voting, feature-level fusion, and rank-level fusion. The work [26], on the\nother hand, concentrated on textured contact lenses attack, with an extensive set of evaluations\nincluding single sensor, cross-sensor and combined sensor experiments. Apart from the generic\nlive vs attack scenario, it also reports binary and ternary classification across the different types of\nreal (normal iris, soft lens) and fake (textured) classes. Naturally, the cross-sensor error is larger\ncompared to single-sensor, and the combined sensor error is also observed to be slightly larger. The\nlatter is attributed to the larger intraclass variation created when images from different sensors\nare combined. In any case, an improvement of performance over previous works with the three\ndatasets employed is observed after the proposed feature selection and score-level fusion method.\n4.4\nAdversarial Networks\nGenerative methods have been used by some approaches, either to use the trained discriminator\nfor iris PAD, or to generate synthetic samples and augment under-represented classes. In this\ndirection, Yadav and Ross [193] proposed CIT-GAN (Cyclic Image Translation Generative Ad-\nversarial Network) for multi-domain style transfer to generate synthetic samples of several PAIs\n(cosmetic contact lenses, printed eyes, artificial eyes and kindle-display attack). To do so, image\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n14\nK. Nguyen, H. Proença, F. Alonso-Fernandez\ntranslation is driven by a Styling Network that learns style characteristics of each given domain. It\nalso employs a Convolutional Autoencoder in the generator for image-to-image style translation,\nwhich takes a domain label as input along with an image. This is different than previous works\nof the same authors [191, 192] which employed the traditional generator/discriminator approach\ndriven by a noise vector. Different PAD methods using hand-crafted (BSIF, DESIST) and deep\nfeatures (VGG16, D-NetPAD, AlexNet) were evaluated, demonstrating that they can be improved by\nadding synthetically generated data. The quality of synthetic images is also superior to a competing\ngenerative method (Star-GAN v2), measured via FID score distributions.\n4.5\nOpen Research Questions in Iris PAD\nOne of the open research issues is to design robust iris PAD methods with cross-sensor and cross-\ndatabase capabilities, so they generalize to unseen imaging conditions. Attackers are constantly\ndeveloping new attack methodologies to circumvent PAD systems, so an even more important issue\nis unseen PAIs (i.e. cross-PAI capabilities) [149]. Great results have been achieved on detecting\nknown attack types (known as closed-set recognition), although cross-database evaluation (training\nin one database an testing in others) still appears as a difficult challenge due to changes in sensors,\nacquisition environments, or subjects. Moreover, generalizing to attacks that are unknown at the\ntime of training (open-set recognition) is even a greater challenge for state-of-the-art methods\n[45]. Part of the problem lies into the limited size of existing databases, which is an issue for\ndata-hungry DL approaches. Some solutions, as studied by some of the methods above, are data\naugmentation by geometric or illumination modifications [47], or creating additional synthetic\ndata via generative methods [193]. Human-aided DL training is another promising avenue. Indeed,\nhumans and machines cooperating in vision tasks is not new, and this strategy is finding its way\ninto DL as well [14, 17]. For example, Boyd et al. [14] analyzed the utility of human judgement about\nsalient regions of images to improve generalization of DL models. Asked about regions that humans\ndeem important for their decision about an image, the work proposed to transform the training\ndata to incorporate such opinions, demonstrating an improvement in accuracy and generalization\nin leave-one-attack-type-out scenarios. In a similar work, Boyd et al. [17] incorporated annotated\nsaliency maps into the loss function to penalize large differences with human judgement.\nRecently, concerns have emerged about the observed bias of DL methods that leads to discrim-\ninatory performance differences based on the user´s demographics, with face biometrics being\nthe most talked-about and many companies and authorities banning its use [78]. Obviously, this\nissue appears in iris PAD as well, as addressed by Fang et al. [49]. Using three baselines based on\nhand-crafted and DL approaches and a database of contact lenses, the authors showed a significant\ndifference in the performance between male and female samples. In dealing with this phenomenon,\nexamination of biases towards eye color or race are another directions worthwhile to consider.\nSome elements considered as PAIs in this section, such as cosmetic lenses, may be worn normally\nby users without the purpose of fooling the biometric system, as it is the case of facial retouching\nvia make-up, digital beautification or augmented reality [69]. This poses the question of whether it\nis possible to use such images for authentication, while diminishing the effect in the recognition\nperformance. Suggested alternatives have been to detect and match portions of live iris tissue still\nvisible [125] or incorporate ocular information of the surrounding area [4]. Unfortunately, in iris\nbiometrics, recognition with textured contact lenses remains a hard problem to solve.\nAnother under-researched task is iris PAD in the visible spectrum. The majority of studies and\ndatasets (Section 8) employ near-infrared illumination and specific iris close-up sensors. However,\nin some environments such as mobile or distant capture, such sensing is not guaranteed [119].\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n15\nTable 3. Cohesive comparison of the most relevant DL-based iris Presentation Attack Detection methods\nafter the surveys [16, 30] (NIR: near-infrared; VW: visible wavelength). Methods are listed in chronological\n(and then alphabetical) order.\nCategory\nMethod\nYear\nData\nDatasets\nFeatures\nNIR\nVW\nFeature\nExtraction\nFang et al. [44]\n2020\n✓\n✗\nLivDet-2017 (IIITD-WVU, ND- CLD)\nVGG16, MobileNetv3-small (multi-layer\nfeatures) + PCA + SVM\nEnd-to-end\nTraining\nArora\nand\nBathia [8]\n2020\n✓\n✗\nLivDet-2017 (IIITD-WVU)\nCNN with patch input\nPeng et al. [126]\n2020\n✓\n✗\nIPITRT, CASIA-Iris-v4, CASIA-Iris-\nFake\nLAILNet lightweight CNN\nSharma\nand\nRoss [150]\n2020\n✓\n✗\nProprietary,\nLivDet-2017\n(IIITD-\nWVU, ND-CLD, Warsaw, Clarkson)\nDenseNet121 pre-trained on ImageNet\nChen and Ross\n[19]\n2021\n✓\n✗\nJHU-APL, LivDet-2017 (Warsaw, ND-\nCLD)\nDenseNet121 pre-trained on ImageNet +\nAG-PAD channel and spatial attention\nFang et al. [45]\n2021\n✓\n✗\nLivDet-2017 (IIITD-WVU, ND-CLD),\nND-CLD-15,\nMobileNetv3-small with micro-stripes\nFang et al. [46]\n2021\n✓\n✗\nLivDet-2017 (IIITD-WVU, ND-CLD,\nClarkson), ND-CLD-13, ND-CLD-15,\nIIITD-CLI\nDenseNet + A-PBS spatial attention\nFang et al. [47]\n2021\n✓\n✗\nLivDet-2017 (IIITD-WVU, ND-CLD,\nClarkson)\nResNet50, VGG16, MobileNetv3-small\nGupta et al. [63]\n2021\n✓\n✗\nMUIPA, UnMIPA, IIITD-CLI\nCNN with multi-branch classification\nSharma\nand\nRoss [151]\n2021\n✓\n✓\nOCT, NIR and VW images\nVGG19, ResNet50, DenseNet121\nZhang\net\nal.\n[199]\n2021\n✓\n✗\nND-CLD-13, CASIA-Iris-Fake, IF-VE\nVGG16 + WRN local attention and\nglobal classification\nAgarwal et al.\n[1]\n2022\n✓\n✗\nMUIPA, UnMIPA, IIITD-CLI, LivDet-\n2017 (IIITD-WVU), ND-PSID\nSiamese\ncontraction-expansion\nCNN,\nfeature fusion\nAgarwal et al.\n[2]\n2022\n✓\n✗\nMUIPA, UnMIPA, IIITD-CLI, LivDet-\n2017\n(IIITD-WVU),\nND-PSID,\nNDIris3D\nSiamese CNN, score fusion\nGautam\net\nal.\n[59]\n2022\n✓\n✗\nSYN, IIITD-CLI, IIITD-IS\nAutoencoder with reconstruction and\nclassification loss\nTapia\net\nal.\n[162]\n2022\n✓\n✓\nLivDet-2020, Iris-CL1, Warsaw-Post-\nMortem v3.0\nMobileNetv2, data augmentation, class-\nweights\nHybrid\nMethods\nChoudhary et al.\n[27]\n2022\n✓\n✗\nIIITD-CLI,\nND-CLD-13,\nCASIA,\nLivDet-2017 (IIITD-WVU, ND-CLD,\nClarkson)\nMBISF (domain-specific filters), SIFT,\nHaralick, DenseNet, VGG8 + SVM clas-\nsification\nChoudhary et al.\n[26]\n2022\n✓\n✗\nIIITD-CLI, ND-CLD-13, LivDet-2017\n(Clarkson)\nMBSIF (generic filters), MBSIF (domain-\nspecific filters), SIFT, LBPV, DAISY,\nDenseNet121 + SVM classification\nAdversarial\nNetworks\nYadav and Ross\n[193]\n2021\n✓\n✗\nCasia-Iris-Fake, Berc-iris-fake, ND-\nCLD-15, LivDet-2017, MSU-IrisPA-\n01\nBSIF,\nDESIST,\nVGG16,\nD-NetPAD,\nAlexNet\n5\nDEEP LEARNING-BASED FORENSIC IRIS RECOGNITION\nIris recognition has become the next biometric mode (in addition to face, fingerprints and palmprints)\nconsidered for large-scale forensic applications [52], and coincides in time with discoveries made\nin recent years about possibility to employ iris in recognition of deceased subjects. This includes\nboth matching of iris patterns acquired a few hours after death with those with longer PMIs (Post-\nMortem Intervals), ranging from days [12, 143, 167, 168] to several weeks after demise [18, 170], as\nwell as matching patterns acquired before death with those collected post-mortem [141].\nDue to decomposition changes to the eye tissues, post-mortem iris images differ significantly\nfrom live iris images and rarely meet ISO/IEC 29794-6 quality requirements, as shown in Fig. 3(a).\nThe challenges are related to appropriate detection of places when cornea dries and generates\nirregular and large specular highlights, as well regions where iris muscle furrows show up when\nthe eyeball dehydrates. This is where DL-based methods may win over hand crafted approaches, as\nthe latter usually make strong assumptions about anatomy of the iris appearance, not possible to\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n16\nK. Nguyen, H. Proença, F. Alonso-Fernandez\n(a)\n(b)\n(c)\nFig. 3. Post-mortem iris recognition and visualization: (a) a good-quality post-mortem iris image; (b) top\nto bottom: deep learning-based detection of iris annulus, specular highlights and decomposition-induced\nwrinkles; (c) segmentation results presented to a human examiner along with an overlaid heatmap visualizing\nregions judged as salient by the matching algorithm. Source: [91]\nbe predicted for eyes undergoing random decomposition processes. Trokielewicz et al. proposed\nthe first known to us iris recognition method designed specifically to cadaver irises [171, 172]. It\nincorporates SegNet-based segmenter and Siamese networks-based feature extractor, both trained\nin a domain-specific way solely on post-mortem iris samples. An interesting element of this\napproach is that segmetation incorporates two models: one trained with “fine” ground truth masks,\nmarking all details associated with eye decomposition, and “coarse” model, aiming at detecting iris\nannulus and eyelids, as in classical iris recognition approaches. This allowed to apply a standard\n“rubber sheet” iris images normalization based on “coarse” masks, and at the same time exclude\ndecomposition-driven artifacts from encoding, marked by the “fine” mask. Kuehlkamp et al. [91]\nin addition to detecting post-mortem deformations, as shown in Fig. 3(c), they also proposed a\nhuman-interpretable visualization of a classification process. The visualization is based on Class\nActivation Mapping mechanism [212] and highlights salient features used by the classifier in its\njudgment. This novelty in iris recognition algorithms may help human examiners to locate iris\nregions that should be carefully inspected, or to verify the algorithm’s decision.\n6\nHUMAN-MACHINE PAIRING TO IMPROVE DEEP LEARNING-BASED IRIS\nRECOGNITION\nIris recognition is usually associated with automatic, solely machine-based and rapid biometric\nmeans. It has been changing in the recent decade due to constantly increasing ubiquitousness of\niris recognition, especially owing to large governmental applications such as [174] or FBI’s Next\nGeneration Identification System (NGI) gradually replacing the Integrated Automated Fingerprint\nIdentification System (IAFIS) [52]. This combined with unique identification power of iris whetted\nthe appetite to apply this technique to identification problems normally reserved for fingerprints\nand face: forensics, lost subjects search or post-mortem identification. To have the legal power,\nhowever, the judgment about samples originating or not from the same eye conclusion must be\nconfirmed by a trained human expert. And here is the place where DL-based iris image processing\nmay play a useful role.\nTrokielewicz et al. compared iris images in post-mortem iris recognition between humans and\nmachines. They investigated which iris image regions humans and machines mainly attend to\ncompare a pair of images. The machine-based attention maps are generated by Grad-CAM to\nhighlight the regions that contribute the most to the deep learning model’s prediction. The human-\nbased attention maps are learned by tracking the gaze as the human is looking around the screen\nthat display iris image pairs and recording the regions where the human spend most time on.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n17\nInterestingly while humans and machines tend to focus on a limited number of iris areas, however,\nthe region, appearance and density of these areas between humans and machines are different. As\nsalient regions proposed by the deep learning model and identified from human eye gaze do not\noverlap in general, the computer-added visual cues may potentially constitute a valuable addition\nto the forensic examiner’s expertise, as it can highlight important discriminatory regions that the\nhuman expert might miss in their proceedings. This human-machine pairing is important as human\nsubjects can provide an incorrect decision even despite spending quite sometime observing many\niris regions [120]. In addition, there has been a body of research showing that humans and machines\ndo not perform similarly well under different conditions [20, 108, 154]. For example, Moreira et\nal. also showed that machines can outperform humans in healthy easy iris image pairs; however,\nhumans outperform machines in disease-affected iris image pairs [108]. Human-machine pairing\nwill improve deep learning based iris recognition.\n7\nRECOGNITION IN LESS CONTROLLED ENVIRONMENTS: IRIS/PERIOCULAR\nANALYSIS\nRooted in the seminal work due to Park et al. [124], efforts have been paid to the development\nof human recognition methods that - apart the iris - also consider information in the vicinity of\nthe eye to infer the identity. This is a relatively recent topic, termed as periocular recognition. The\nrationale is that the periocular region represents a trade-off between the face and the iris. Periocular\nbiometrics has been claimed to be particularly useful in environments that produce poor quality\ndata (e.g., visual surveillance). Recently, as in the case of iris, several DL-based solutions have been\nproposed.\nHernandez-Diaz et al. [70] tested the suitability of off-the-shelf CNN architectures to the perioc-\nular recognition task, observing that albeit such networks are optimized to classify generic objects,\ntheir features still can be effectively transferred to the periocular domain.\nIn the visual surveillance context, Kim et al. [88] infer subjects identities based either in loose/tight\nregions-of-interest, depending of the perceived image quality. Hwang and Lee [77]prevents the loss\nof mid-level features and dynamically selects the most important features for classification. Luo et\nal. [102] used self-attention channel and spatial mechanisms into the feature encoding module of a\nCNN, in order to obtain the most discriminative features of the iris and periocular regions.\nJung et al. [82]’s work is based in the concept of label smoothing regularization (LSR). Having as\nmain goal to reduce the intra-class variability, they described a so-called Generalized LSR (GLSR) by\nlearning a pre-task network prediction that is claimed to improve the permanence of the obtained\nperiocular features. Having similar purposes, Zanlorensi et al. [198] described a preprocessing\nstep based in generative networks able to compensate for the typical data variations in visual\nsurveillance environments. Nie et al. [118] applied convolutional restricted Boltzmann machines to\nthe periocular recognition problem. Starting from a set of genuine pairs that are used as a constraint,\na Mahalanobis distance-metric is learned.\nObtaining auxiliary (e.g., soft biometrics) has been seen as an interesting direction for com-\npensating the lack of image quality. Zhao and Kumar [210] incorporate an attention model into\na DL-architecture to emphasize the most important regions in the periocular data. The same au-\nthors [208] described a semantics-assisted CNN framework to infer comprehensive periocular\nfeatures. The whole model is composed of different networks, trained upon ID and semantic (e.g.,\ngender, ethnicity) data, that are fused at the score and prediction levels. Similarly, Talreja et al. [158]\ndescribed a multi-branch CNN framework that predicts simultaneously soft biometrics and ID\nlabels, which are finally fused into the final response.\nWith regard to cross-spectral settings, Hernandez-Diaz et al. [71] used conditional GANs (CGANs)\nto convert periocular images between domains, that are further fed to intra-domain off-the-self\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n18\nK. Nguyen, H. Proença, F. Alonso-Fernandez\nframeworks. Sharma et el. [148] described a shallow neural architecture where each model learns\nthe data features in each spectrum. Then, at a subsequent phase, all models are jointly fine tuned,\nto learn the cross-spectral variability and correspondence features.\nFinally, several works have attempted to faithfully fuse the scores/responses from iris and peri-\nocular data. Wang and Kumar [181] used periocular features to adaptively match iris data acquired\nin less constrained conditions. Their framework incorporates such discriminative information\nusing a multilayer perceptron network. Zhang et al. [203] described a DL-model that exploits\ncomplementary information from the iris and the periocular regions, that applies maxout units to\nobtain compact representations for each modality and then fuses the discriminative features of\nthe modalities through weighted concatenation. In an opposite direction, Proença and Neves [134]\nargued that the periocular recognition performance is optimized when the components inside the\nocular globe (the iris and the sclera) are simply discarded.\nTable 4. Summary of datasets used in the DL-based iris segmentation and recognition methods of Tables 1\nand 2 (NIR: near-infrared; VW: visible wavelength).\nName\nData\nSize\n# IDs\n# Samples\n# Sessions\nFeatures\nBATH [107]\nNIR\n1280×960\n1600\n16000\n1\nHigh quality images\nBioSec [53]\nNIR\n640×480\n400\n3200\n2\nOffice environment\nBiosecure [123]\nNIR\n640×480\n1334\n2668\n2\nOffice environment\nCASIA-Cross-Sensor [187]\nNIR\nn/a\n700\n21000\n1\nMulti-sensor, multi-distance (12-30cm,\n3-5m)\nCASIA-Iris-Distance [40]\nNIR\n2352×1728\n284\n2567\n1\nDistant acquisition\nCASIA-Iris-Interval [103]\nNIR\n320×280\n395\n2639\n2\nHigh quality images\nCASIA-Iris-Lamp [185]\nNIR\n640×480\n819\n16212\n1\nNon-linear deformation\nCASIA-Iris-M1-S1 [204]\nNIR\n1920×1080\n140\n1400\n1\nMobile device\nCASIA-Iris-M1-S2 [202]\nNIR\n1968×1024\n400\n6000\n1\nMobile\ndevice,\nmulti-distance\n(20,25,30cm)\nCASIA-Iris-M1-S3 [203]\nNIR\n1920×1920\n720\n3600\n1\nMobile device\nCASIA-Iris-Thousand [200]\nNIR\n640×480\n2000\n20000\n1\nHigh quality images\nDCME01 [18]\nNIR,\nVW\nn/a\n254\n621\n1-9\n-\nDCME02 [91]\nNIR\nn/a\n259\n5770\n1-53\n-\nIITD [92]\nNIR\n320×240\n224\n1120\n1\nVarying quality\nIris-Mobile [121]\nNIR\nn/a\n750\n22966\nn/a\nMobile device, indoor & outdoor\nJluIrisV3.1 [207]\nNIR\n640×480\n120\n1780\nn/a\n-\nJluIrisV4 [207]\nNIR\n640×480\n172\n114904\nn/a\n-\nLivDet-2013-Warsaw [28]\nNIR\n640×480\n284\n1667\n1\nHigh quality images\nMICHE-I [38]\nVW\nvar.\n184\n3732\n2\nThree mobile devices\nMMU\nNIR\n320×240\n92\n460\n1\nHigh quality images\nMobBIOfake [146]\nVW\n300×200\n200\n1600\n1\nWith a handheld device\nND-CrossSensor-2013 [187]\nNIR\n640×480\n1352\n146550\n27\nMulti-sensor\nND-Iris-0405 [127]\nNIR\n640×480\n712\n64980\n1\nVarying quality\nND-TWINS-2009-2010\nVW\nn/a\n435\n24050\nn/a\nFacial pictures frontal, 3/4 and side\nviews. Indoor & outdoor\nOpenEDS [58]\nNIR\n640×400\n304\n356649\n1\nFrom head-mounted VR glasses\nQ-FIRE [81]\nNIR\nvar.\n390\n586560\n2\nIris/face Videos, various distances and\nquality\nUBIRIS.v1 [130]\nVW\n800×600\n241\n1877\n2\nSeveral noise factors\nUBIRIS.v2 [131]\nVW\n400×300\n522\n11102\n2\nDistant acquisition, on the move\nWarsaw [18]\nNIR,\nVW\nn/a\n157\n4866\n1-13\n-\nWarsaw-Post-Mortem v1.0 [167]\nNIR,\nVW\nvar.\n34\n1330\n2-3\nDeceased persons, 5-7h to 17 days post-\nmortem\nWarsaw-Post-Mortem v2.0 [170]\nNIR,\nVW\nvar.\n73\n2987\n1-13\nDeceased persons\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n19\n8\nOPEN-SOURCE DEEP LEARNING-BASED IRIS RECOGNITION TOOLS\nHere we summarize the main properties of the datasets employed by the methods of the previous\nsections for DL-based iris segmentation, recognition and PAD. We also describe available open-\nsource software code for these tasks, and other relevant tools.\n8.1\nData Sources\nTable 4 gives the technical details of the datasets used in the segmentation and recognition methods\nof Tables 1 and 2. Table 5 does the same for the iris PAD methods of Table 3. We show the main\nproperties (spectrum, image size, identities, images, sessions) and relevant features. Only the\ndatasets of the methods reported in previous section are presented. Since we focus on the most\nrecent developments, we consider that such approach provides the most relevant datasets for each\ntask. Of course, the list of available datasets after decades of iris research is much longer [122].\nA first observation is the dominance of near infrared (NIR) over the visible (VW) spectrum,\nwhich should not be surprising, since NIR is regarded as most suitable for iris analysis. However,\nresearch-wise, many segmentation and recognition studies (Tables 1, 2) use VW images, pushed by\nthe success of challenging databases such as MICHE and UBIRIS. On the contrary, the VW modality\nin iris PAD research is residual (Table 3), a tendency also observed in pre-DL research [16, 30].\nWhen it comes to the types of Presentation Attack Instruments (PAIs) employed in iris PAD\ndatabases, they can be categorized into:\n• PP: paper printout of a real iris image, i.e. from a live person\n• PPD: paper printout of a real iris image with a transparent 3D plastic eye dome on top\n• CLL: textured contact lenses worn by a live person\n• CLP: textured contact lenses on printout (either a printout of a CLL image, or a printout of a\nreal iris image with a textured contact lens placed on top)\n• RA: replay attack, i.e. a real iris image shown on a display\n• AE: artificial eyeball (plastic eyes of two different types: Van Dyke Eyes, with higher iris\nquality details, and Scary eyes, plastic fake eyes with a simple pattern on the iris region)\n• AEC: artificial eyeball with a textured contact lens on top\n• SY: synthetic iris, i.e. an image created via generative methods\n• PM: postmortem iris, i.e. an image acquired from cadaver eyes\nThese PAIs mostly entail presenting the mentioned instrument to the iris sensor, which then\ncaptures an image of the artifact. An exception is “SY”, which directly produces a synthetic digital\nimage, although such image could be used as base to, for example, PP, PPD, RA, or AE attacks. In\nTable 5, it can be seen that CLL (textured lenses live) and PP (paper printouts) largely dominates\nas the most popular PAIs on the existing databases, and consequently, on the related research\n(Table 3). CLP (textured lenses on printout) also appears in many studies, driven by the wide use of\nthe LivDet-2017-IIITD-WVU set, which includes such PAI. CASIA-Iris-Fake, which contains AE\n(artificial eyes) and SY (synthetic irises) also appears in a few studies. Other attacks that one may\nexpect on the digital era, such as RA (replay), however, are residual in datasets and recent studies.\n8.2\nSoftware Tools\nThe availability of DL-based tools for iris biometrics has been scarce for years, specially for PAD [51].\nIn the following, we provide a short description of peer-reviewed references with associated source\ncode (link included in the paper, or easily found on the websites of the authors or dedicated sites\nsuch as www.paperswithcode.com). We describe (in this order) tools for segmentation, recognition\nand PAD. For each type, the references are then presented in cronological order.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n20\nK. Nguyen, H. Proença, F. Alonso-Fernandez\nTable 5. Summary of datasets used in the DL-based iris Presentation Attack Detection methods of Table 3\n(NIR: near-infrared; VW: visible wavelength). The type of PAIs (second column) are PP: paper printout, PPD:\npaper printout with plastic dome, CLL: textured contact lenses (live), CLP: textured contact lenses (printout),\nRA: replay attack (display), AE: artificial eyeball, AEC: artificial eyeball with textured contact lens, SY: synthetic\niris, PM: postmortem iris. TTP (next to last column) indicates the existence of a training/test split. The features\n(last column) are MS: multi-sensor, ME: multi-environment (e.g. indoor/outdoor, light variability, mobile\nenvironment, etc.), UPAI: unseen PAIs in the test set.\nName\nPAIs\nData\nSize\n# IDs\n# Samples\nTTP\nFeatures\nlive\nfake\nlive\nfake\ntotal\nCASIA-Iris-Fake\n[155]\nPP, CLL, AE,\nSY\nNIR\n640×480\n1000\n815\n6000\n4120\n10240\nIF-VE [199]\nCLL\nNIR\nn/a\n200\n200\n25000\n25000\n50000\n✓\nMS, ME\nIPITRT [126]\nPP\nNIR\nvar.\n58\nn/a\n1800\n551\n2351\nME\nIIITD-CLI [89]\nCLL\nNIR\n640×480\n202\nn/a\nn/a\nn/a\n6570\n✓\nMS\nIIITD-IS3 [64]\nPP, CLP\nNIR\n640×480\n202\nn/a\n0\n4848\n4848\nMS\nLivDet-2017 [194]\n-Clarkson\nPP, CLL\nNIR\n640×480\n50\nn/a\n3954\n4141\n8095\n✓\nUPAI\n(additional\npatterned\nlenses)\n-IIITD-WVU1\nPP, CLL, CLP\nNIR\n640×480\nn/a\nn/a\n2952\n4507\n7459\n✓\nMS, ME, UPAI (additional pat-\nterned lenses)\n-ND-CLD2\nCLL\nNIR\n640×480\nn/a\nn/a\n2400\n2400\n4800\n✓\nUPAI\n(additional\npatterned\nlenses)\n-Warsaw\nPP\nNIR\n640×480\n457\n446\n5168\n6845\n12013\n✓\nMS\nLivDet-2020 [33]\nPP, PPD, CLL,\nCLP, RA, AE,\nAEC, PM\nNIR\n640×480\nn/a\nn/a\n5331\n7101\n12432\nMS\nIris-CL1 [162]\nPP\nNIR\nvar.\nn/a\nn/a\nn/a\n1800\nn/a\nMS\nJHU-APL [19]\nCLL, AE\nNIR\nn/a\nn/a\nn/a\n7191\n7214\n14405\nME\nMSU-IrisPA-01\n[191]\nPP, CLL, RA,\nAE\nNIR\n640×480\nn/a\nn/a\n1343\n2523\nMUIPA [190]\nPP, CLL\nNIR\n640×480\n70\n70\nn/a\nn/a\n10296\nME\nND-CLD-13 [42]\nCLL\nNIR\n640×480\n330\nn/a\n3400\n1700\n5100\n✓\nMS\nND-CLD-152 [41]\nCLL\nNIR\n640×480\nn/a\nn/a\n4800\n2500\n7300\n✓\nMS\nNDIris3D [51]\nCLL\nNIR\n640×480\n176\n176\n3458\n3392\n6850\nMS\nND-PSID4 [31]\nCLL\nNIR\n640×480\n238\n238\n3132\n2664\n5796\nUnMIPA [189]\nCLL\nNIR\n640×480\n162\n162\n9319\n9387\n18706\nMS, ME\nWarsaw-Post-\nMortem v3.0 [172]\nPM\nNIR,\nVW\nvar.\n0\n79\n0\n1879\n1879\nMS\n1 Contains IIITD-CLI and IIITD-IS\n2 Iris-LivDet-2017-ND-CLD is a subset of ND-CLD-15\n3 IIITD-IS images are printouts of IIITD-CLI captured with a iris scanner and a flatbed scanner\n4 ND-PSID is a subset of ND-CLD-15\n8.2.1\nSegmentation.\nLozej et al. [101] released their end-to-end DL model based on the U-Net architecture [139]. The\nmodel was trained and evaluated with a small set of 200 annotated iris images from CASIA database.\nThe authors also explored the impact of the model depth and the use of batch normalization layers.\nKerrigan et al. [85] released the code and models of Iris-recognition-OTS-DNN, a set of four\narchitectures based on off-the-shelf CNNs trained for iris segmentation (two VGG-16 with dilated\nconvolutions, one ResNet with dilated kernels, and one SegNet encoder/decoder). Training databases\nincluded CASIA-Irisv4-Interval, ND-Iris-0405, Warsaw-Post-Mortem v2.0 and ND-TWINS-2009-\n2010, whereas testing data came from ND-Iris-0405 (disjoint subject), BioSec and UBIRIS.v2. Results\nshowed that the DL solutions evaluated outperform traditional segmentation techniques, e.g. Hough\ntransform or integro-differential operators. It was also seen that each test dataset had a method that\nperforms best, with UBIRIS obtaining the worst performance. This should not come as a surprise,\nsince it contains VW images with high variability taking distantly with a digital camera, whereas\nthe other two are from close-up NIR iris sensors in controlled environments.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n21\nWang et al. [176] released the code and models of their high-efficiency segmentation approach,\nIrisParseNet. A multi-task attention network was first applied to simultaneously predict the iris\nmask, pupil mask and iris outer boundary. Then, from the predicted masks and outer boundary, a\nparameterization of the iris boundaries was calculated. The solution is complete, in the sense that\nthe mask (including light reflections and occlusions) and the parameterized inner and outer iris\nboundaries are jointly achieved.\nMore recently, authors from the same group presented IrisSegBenchmark [177], an open iris\nsegmentation evaluation benchmark where they implemented six different CNN architectures,\nincluding Fully Convolutional Networks (FCN) [100], Deeplab V1,V2,V3 [21], ParseNet [98], PSPNet\n[206], SegNet [9], and U-Net [139]. The methods were evaluated on CASIA-Irisv4-Distance, MICHE-\nI and UBIRIS.v2. As in [85], results showed that the best method depends on the database, being:\nParseNet for CASIA (NIR data), DeeplabV3 for MICHE (VW images from mobile devices), and\nU-Net for UBIRIS (VW images from a digital camera). In this case, however, the three tests databases\nbehaved approximately equal, since they all contain difficult distant data. CASIA showed a slightly\nbetter accuracy, suggesting that NIR data may be easier to segment. Traditional, non-DL methods\nwere also evaluated, concluding that DL-based segmentation achieves superior accuracy.\nBanerjee et al. [10] released the code of their V-Net architecture, designed to overcome some\ndrawbacks of U-Net, such as instability to tackle iris segmentation or tendency to overfit. A pre-\nprocessing stage on the YCrCb and HSV spaces was also added to detect salient regions and aid\ndetection of iris boundaries. The method was evaluated on the difficult UBIRIS.v2 VW dataset.\n8.2.2\nRecognition.\nThe code of the DL method ThirdEye was released by Ahmad and Fuller [3], based on a ResNet-\n50 trained with triplet loss. Authors directly used segmented images without normalization to a\nrectangular 2D representation, arguing that such step may be counterproductive in unconstrained\nimages. The model was evaluated on the ND-0405, IITD and UBIRIS.v2 datasets.\nThe models of Boyd et al. [15] for recognition have been also released, based on a ResNet-50\nwith different weight initialization techniques, comprising: from scratch (random), off-the-shelf\nImageNet (general-purpose vision weights), off-the shelf VGGFace2 (face recognition weights),\nfine-tuned ImageNet weights, and fine-tuned VGGFace2 weights. Both ImageNet and VGGFace2\nare very large datasets with millions of images, and face images contain the iris region. Thus, using\nthese datasets as initialization may be beneficial for iris recognition, where available training data\nis in the order of hundreds of thousand images only. This strategy has been followed e.g. in ocular\nsoft-biometrics as well [6]. The observed optimal strategy is indeed to fine-tune an off-the-shelf set\nof weights to the iris recognition domain, be general-purpose or face recognition weights.\n8.2.3\nSegmentation and Recognition Packages.\nA complete package comprising segmentation and feature encoding was provided by Tann et\nal.[161]. The segmentator is based on a Fully Convolutional Network (FCN), but encoding is based\non hand-crafted Gabor filters [35]. Evaluation was done on CASIA-Irisv4-Interval and IITD.\nIn forensic investigation for diseased eyes and post-mortem samples, Czajka [29] also released\na complete package combining segmentation and feature encoding. The models are based on\nprevious efforts of the author and co-workers, comprising a SegNet [172] and a CCNet [106] DL\nsegmentators, but the feature encoder is based on hand-crafted BSIF filters.\nAnother complete segmentation and recognition package was released by Kuehlkamp et al. [91].\nThe segmentator is based on a fine-tuned Mask-RCNN architecture, with the cropped iris region\nfed directly into a ResNet50 pre-trained for face recognition on the very large VGGFace2 dataset,\nand fine-tuned for iris recognition using triplet loss. The paper is oriented towards postmortem iris\nanalysis, so the methods use a mixture of live and postmortem images for training and evaluation.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n22\nK. Nguyen, H. Proença, F. Alonso-Fernandez\nParzianello and Czajka [125] also released the models and annotated data for their textured\ncontact lens aware iris recognition method. The foundation is that such lenses may be used normally\nfor cosmetic purposes, without intention to fool the biometric system. Therefore, they proposed to\ndetect and match portions of live iris tissue still visible in order to enable recognition even when\na person wears textured contact lenses. To do so, they applied a Mask R-CNN as a segmentation\nbackbone, trained to detect authentically-looking parts of the iris using manually segmented\nsamples from NDIris3D dataset. Non-iris information is then removed from the training images by\nblurring it or replacing it with random noise to guide the subsequent recognition network (based\non ResNet-18) to salient, non-occluded regions that should be used for matching.\n8.2.4\nIris PAD.\nIn the iris PAD arena, Gragnaniello et al. [61] proposed a CNN that incorporates domain-specific\nknowledge. Based on the assumption that PAD relies on residual artifacts left mostly in high-\nfrequencies, a regularization term was added to the loss function which forces the first layer to\nbehave as a high-pass filter. The method, which is available in the website of the first author, could\nbe applied to PAD in multiple modalities, including iris and face.\nThe code and model of the method of Sharma and Ross [150] (D-NetPAD) is also available. It is\nbased on DenseNet121 and trained for a variety of PAIs (printouts, artificial eye, cosmetic contacts,\nkindle replay, and transparent dome on print), with an script to retrain the method also available.\n8.3\nOther Tools: Iris Image Quality Assessment\nSeveral image properties considered to potentially influence the accuracy of iris biometrics have\nbeen defined in support of the standard ISO/IEC 29794-6 [164]. They include: grayscale spread\n(dynamic range), iris size (pixels across the iris radius when the boundaries are modeled by a circle),\ndilation (ratio of the pupil to iris radius), usable iris area (percentage of non-occluded iris, either\nby eyelashes, eyelids or reflections), contrast of pupil and sclera boundaries, shape (irregularity)\nof pupil and sclera boundaries, margin (distance between the iris boundary and the closest image\nedge), sharpness (absence of defocus blur), motion blur, signal to noise ratio, gaze (deviation of the\noptical axis of the eye from the optical axis of the camera), and interlace of the acquisition device.\nLow quality iris images, which can potentially appear in uncontrolled or non-cooperative envi-\nronments, are known to reduce the performance of iris location, segmentation and recognition.\nThus, an accurate quality assessment can be a valuable tool in support of the overall pipeline,\neither by dropping low quality images, or invoking specialized processing [5]. One possibility\nmight be to quantify the properties mentioned above, and placing thresholds on each. A more\nelaborated alternative is to combine them according to some rule and produce an overall quality\nscore. However, it is difficult to provide metrics that cover all types of quality distortions [157] and\ndoing so for some indeed entails to segment the iris.\nBroadly, a biometric sample is of good quality if it is suitable for recognition, so quality should\ncorrelate with recognition performance [62]. As such, quality assessment can be viewed as a\nregression problem. Wang et al. [182] considered that a non-ideal eye image will pivot in the\nfeature space around the embedding of an ideal image. They defined quality as the distance to the\nembedding of such “ideal” image which, is regarded as a registration sample collected under a highly\ncontrolled environment. They used a model to learn the mapping between images and Distance\nin Feature Space (DFS) directly from a given dataset. Quality is computed via attention-based\npooling that combines a heatmap that comes from a coarse segmentation based on U-Net and the\nfeature map of an extraction network based on MobileNetv2 pre-trained on CASIA-Iris-V4 and\nNDIRIS-0405.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n23\n9\nEMERGING RESEARCH DIRECTIONS\nIn this section, we discuss the most relevant open challenges and hypothesize about emerging\nresearch directions that could become hot-topics in biometrics literature in a close future.\n9.1\nResource-aware designs of iris recognition networks\nApplication-wise, iris recognition can be performed on a wide range of hardware, ranging from\nhigh-end computers to low-end embedded devices, or from large computer clusters to personal\ndevices such as mobile phones. Performing recognition on resource-limited hardware could pose\nnew challenges for deep learning based iris networks, which usually contain hundreds of layers\nand millions of parameters. Therefore designing these deep learning networks necessarily need to\nbe aware of the hardware platforms on which they will be run.\nLightweight models: Lightweight CNNs employ advanced techniques to efficiently trade-off between\nresource and accuracy, minimising their model size and computations in term of the number of\nfloating point operations (FLOPs), while retaining high accuracies. Specialized lightweight CNN\narchitectures include MobileNets [73] and U-Net [139]. There are a few lightweight deep learning\nbased models for both segmentation and feature extraction. Fang et al. [50] adapted the lightweight\nCC-Net [106] for iris segmentation. CC-Net has a U-Net structure [139], able to retain up to 95%\naccuracy using only 0.1% of the trainable parameters. Boutros et al. [13] benchmarked MobileNet-\nV3 against deeper networks for iris recognition and showed that the MobileNet based model can\nachieve similar EER with 85% less number of parameters and 80% less inference time.\nModel compression: Studies have found that most of the large deep learning models tend to be\noverparameterized, leading to lots of redundant parameters and operations in the network. This\nbecomes more severe considering iris texture images are different from generic object-based images.\nThis has motivated a hot trend looking to remove these redundancies from the models, including\npruning, quantization and low-rank factorization [95]. In our iris recognition literature, there a few\nlightweight deep learning based models for both segmentation and feature extraction. Tann et al.\n[161] quantized 64-bit floating points numbers of weights and activations of the full FCN-based iris\nsegmentation model using an 8-bit dynamic fixed-point (DFP) format, which provide a 8× memory\nsaving as well as speed enhancement due to reduced complexity of lower precision operations.\nNeural Architecture Search: Neural Architecture Search (NAS) automates the process of architecture\ndesign of neural networks by iteratively sampling a population of child networks, evaluating\nthe child models’ performance metrics as rewards and learning to generate high-performance\narchitecture candidates [43]. In our iris recognition literature, Nguyen et al. [113] showed that\ncomputation and memory can be incorporated into the NAS formulation to enable resource-\nconstrained design of deep iris networks.\n9.2\nHuman-interpretable methods and XAI\nWith hundreds of layers and millions of parameters, deep learning networks are usually opaque or\n“blackbox” where humans struggle to understand why a deep network predict what it predicts. This\nnecessitates approaches to make deep learning methods more interpretable and understandable\nto humans. Interestingly, the need for human-interpretable methods has been raised even from\nthe handcrafted era. For example, Shen et al. published a series of work [20, 152] on using iris\ncrypts for iris matching. Iris crypts are clearly visible to humans in a similar way as finger minutiae.\nAnother example is the macro-features [156] which use SIFT to detect keypoints and perform iris\nmatching based on these keypoints [136]. Another notable work is by Proença et al. [132] where\nthey proposed a deformation field to represent the correspondence between two iris images.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n24\nK. Nguyen, H. Proença, F. Alonso-Fernandez\nFrom a deep learning perspective, researchers have also attempted to visualize the matching.\nKuehlkamp et al. [91] argued that existing iris recognition methods offer limited and non-standard\nmethods of visualization to let human examiners interpret the model output. They applied Class\nActivation Maps (CAM) [212] to visualize the level of contribution of each iris region to the overall\nmatching score. Similarly, Nguyen et al. [115] also decomposed the final matching score into\npixel-level to visualize the level of contribution of each pixel to the overall matching score.\n9.3\nDeep learning-based synthetic iris generation\nData synthesis provides an alternative to time- and resource-consuming database collection. One\ncould create as many images as desired, with new textures that even do not match any existing\nidentity, which would avoid privacy problems too. On the other hand, fake irises that are indistin-\nguishable from real ones can be used for identity concealment attacks (if the image does not match\nany identity) or impersonation attacks (if the image resembles an existing identity) [30]. Indeed,\nsynthetic irises are present in databases employed for iris PAD, such as CASIA-Iris-Fake (Table 5).\nRegardless of the purpose or ability to detect if an image is synthetic, Generative Adversarial\nNetworks (GANs) [60] have shown impressive photo-realistic generating capabilities in many\ndomains. GANs learn to model image distributions by an adversarial process, where a discriminator\nassesses the realism of images synthesized by a generator. At the end, the generator have learned the\ndistribution of the training data, being able to synthesize new images with the same characteristics.\nFor iris generation, some methods by Yadav et al. [191–193] were mentioned in iris PAD con-\ntexts (Section 4.4). RaSGAN [191, 192] followed the traditional approach of driving the genera-\ntion/discrimination training by randomly sampling so-called latent vectors from a probabilistic\ndistribution. As training progresses, the generator learns to associate features of the latent vectors\nwith semantically meaningful attributes that naturally vary in the images. However, this does not\nimpose any restriction in the relationship between features in latent space and factors of variation\nin the image domain, making difficult to decode what the latent vectors represent. As a result, the\nimage characteristics (eye color, eyelids shape, eyelashes, gender, age...) are generated randomly.\nKohli et al. [90] presented iDCGAN for iris PAD, which also followed the latent vector sampling\nconcept. To counteract such issue, researchers have tried to incorporate constrains or mechanisms\nthat guide the generation process to a desired characteristic. For example, CIT-GAN [193] employed\na Styling Network that learns style characteristics of each given domain, while taking as input a\ndomain label that drives the network to embed a desired style into the generated data.\nIn a similar direction, Kaur and Manduchi [83, 84] proposed to synthesize eye images with a\ndesired style (skin color, texture, iris color, identity) using an encoder-decoder ResNet. The method\nis aimed at manipulating gaze, so the generator receives a segmentation mask with the desired gaze,\nand an image with the style that will see its gaze modified. To achieve cross-spectral recognition,\nHernandez-Diaz et al. [71] used CGANs to convert ocular images between VW and NIR spectra\nwhile keeping identity, so comparisons are done in the same spectrum. This allows the use of\nexisting feature methods, which are typically optimized to operate in a single spectrum.\nDespite great advances in DL-based synthetic image generation, one open problem is the possible\nidentity leakage from the training set when creating data of non-existing identities, resulting in\nprivacy issues. This has just been revealed recently in face generation [165]. Another issue in the\nopposite direction is the difficulty in preserving identity in the generation process when the target\nis precisely creating images of an existing identity with different properties. This is an issue being\naddressed in face generation methods [reference under review], but is lacking in iris synthesis\nresearch.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n25\n9.4\nDeep learning-based iris super-resolution\nOne of the main constraints for existing iris recognition systems is the short distance of image\nacquisition, which usually requires a subject to stay still less than 60 cm from iris cameras. This is\ndue to the requirement of high-resolution iris region, e.g. 120 pixels across the iris diameter due to\nthe European standard and NIST standard, despite the small physical size of an eye, i.e. 15 × 15\nmm. The lack of resolution of imaging systems has critically adverse impacts on the recognition\nand performance of biometric systems, especially in less constrained conditions and long range\nsurveillance applications [116].\nSuper-resolution, as one of the core innovations in computer vision, has been an attractive but\nchallenging solution to address the low resolution problem in both general imaging systems and\nbiometric systems. Deep learning based super-resolution approaches have been across multiple\nworks in iris recognition. Ribeiro et al. [137, 138] experimented two deep learning single-image\nsuper-resolution approaches: Stacked Auto-Encoders (SAE) and Convolutional Neural Networks\n(CNN). Both approaches learn one encoder to map the high resolution iris images to the low\nresolution domain, and one decoder to learn to reconstruct the original high resolution images\nfrom the low resolution ones. Zhang et al. [201] learned a single CNN to learn non-linear mapping\nfunction between LR images to HR images for mobile iris recognition. Wang et al. [183] extended\nthe single CNN to two CNNs: one generator CNN and one discriminator CNN as in the GAN\narchitecture. The generator functions similar to the single LR - HR mapping CNN. Adding the\ndiscriminator CNN allows them to control the generator to generate HR images not just visually\nhigher resolution but also preserve the identity of the iris. Mostofa et al. [109] incorporated a\nGAN-based photo-realistic super-resolution approach [93] to improve the resolution of LR iris\nimages from the NIR domain before cross-matching the HR outputs with the HR images from\nthe RGB domain. While these approaches showed improved performance, dealing with noisy\ndata in such cases as iris at a distance and on the move could require the quality of an input iris\nimage to be included in the super-resolution process [114]. In addition, Nguyen et al. argued that\na fundamental difference exists between conventional super-resolution motivations and those\nrequired for biometrics, hence proposing to perform super-resolution at the feature level targeting\nexplicitly the representation used by recognition [117].\n9.5\nPrivacy in deep learning-based iris recognition\nPrivacy is becoming a key issue in computer vision and machine learning domains. In particular, it\nis accepted that the accuracy attained by deep learning models depends on the availability of large\namounts of visual data, which stresses the need for privacy-preserving recognition solutions.\nIn short, the goal in privacy preserving deep-learning is to appropriately train models while\npreserving the privacy of the training datasets. While the utility of this kind of solutions is obvious,\nthere are certain concerns about the training data that supported the model creation, as the collection\nof images from a large number of individuals comes with significant privacy risks. In particular, it\nshould be considered that the subjects from whom the data were collected can neither delete nor\ncontrol what actually will be learned from their data.\nAs most of the existing biometric technologies, DL-based iris recognition pose challenges to\nprivacy, which are even more concerning, considering the data-driven feature of such kind of\nsystems. Particular attention should be paid to avoid function creep, guaranteeing that the system\nyielding from a set of data is not used for a different purpose than the originally communicated\nto the individual at the time of providing their information. Covert collection is another major\nconcern, which is also particular important for the iris trait, according to the possibility of being\nimaged from large distances and in surreptitious way.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n26\nK. Nguyen, H. Proença, F. Alonso-Fernandez\nParticular attention has been paid to the development of fair recognition systems, in the sense\nthat this kind of systems should attain similar effectiveness in different subgroups of the population,\nregarding different features such as gender, age, race or ethnicity. For data-driven systems, this\nmight be a relevant challenge, considering that most of the existing datasets that support the\nlearned systems have evident biases with regared tio the subjects’ characteristics above.\nLastly, in a more general machine learning perspective, potential attacks to the learned models\nhave been concerning the research community and have been the scope of various recent works,\nattempting to provide defense mechanisms against: i) model inversion attacks, that aim to recon-\nstruct the training data from the model parameters (e.g., [87] and [67]); ii) membership inference,\nthat attempt to infer whether one individual was part of a training set (e.g., [75] and [153]); and iii)\ntraining data extraction attacks, that aim to recover individual training samples by querying the\nmodels (e.g, [86] and [39]).\n9.6\nDeep learning-based iris segmentation\nBeing one of the earliest phases of the recognition process, segmentation is known as one of\nthe most challenging, as it is at the front line for facing the dynamics of the data acquisition\nenvironments. This is particularly true, in case of less constrained data acquisition protocols, where\nthe resulting data have highly varying features and the particular conditions of each environment\nstrongly determine the most likely data covariates.\nIn the segmentation context, the main challenge remains as the development of methods robust\nto cross-domain settings, i.e., able to segment the iris region for a broad range of image features, e.g.,\nin terms of: 1) illumination, 2) scale, 3) gaze, 4) occlusions, 5) rotation and 6) pose, corresponding to\nthe acquisition in very different environments. Over the past decades, many research groups have\nbeen devoting their attentions in improving the robustness of iris segmentation, which is known\nto be a primary factor for the final effectiveness of the recognition process. In this timeline, the\nproposed segmentation methods can be roughly grouped into three categories: 1) boundary-based\nmethods (using the integro-differential operator or Hough transform); 2) based in handcrafted\nfeatures (particularly suited for non-cooperative recognition, e.g., [160] and [159]) ; and 3) DL-based\nsolutions.\nFor the latter family of methods, the emerging trends are closely related to the general challenges\nof DL-based segmentation frameworks, namely to obtain interpretable models that allow us to\nperceive what exactly are these systems learning, or the minimal neural architecture that guarantees\na predefined level of accuracy. Also, the development of weakly supervised or even unsupervised\nframeworks is another grand-challenge, as it is accepted that such systems will likely adapt better\nto previously unseen data acquisition conditions. Finally, the computational cost of segmentation\n(both in terms of space and time) is another concern, with special impact in the deployment of this\nkind of frameworks in mobile settings, and in the IoT setting [140].\n9.7\nDeep learning-based iris recognition in visible wavelengths\nBeing a topic of study for over a decade (e.g. [99] and [129]), iris recognition in visible wavelengths\nremains essentially as an interesting possibility for delivering biometric recognition from large\ndistances (in conditions that are typically associated to visual surveillance settings) and in handheld\ncommercial devices, such as smartphones.\nThe emerging trends in this scope regard the development of alternate ways to analyze the\nmulti-spectral information available in visible light data (typically RGB), i.e., by developing deep\nlearning architectures optimized for fusion, either at the data, feature, score or decision levels [11].\nIn the visual surveillance setting, the main challenge regards the development of optimized data\nacquisition settings, profiting from the advances in remote sensing technologies, that should be\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n27\nable to augment the quality (e.g., resolution and sharpness) of the obtained irises. In this scope, the\nresearch on active data acquisition technologies (based in PTZ devices, or similar) might also be an\ninteresting emerging possibility [66].\n10\nCONCLUSIONS\nMotivated by the tremendous success of DL-based solutions for many different solutions to everyday\nproblems, machine learning is entering one of its golden era, attracting growing interests from the\nresearch, commercial and governmental communities. In short, deep learning uses multiple layers\nto represent the abstractions of data to build computational models that - even in a bit surprising\nway - typically surpass the previous generation of handcrafted-based automata. However, being\nextremely data-driven, the effectiveness of DL-based solutions is typically constrained by the\nexistence of massive amounts of data, annotated in a consistent way.\nAs in the generality of the computer-vision topics, a myriad of DL-based techniques has been\nproposed over the last years to perform biometric recognition, and - in particular - iris recognition.\nNowadays, the existing methods cover the whole phases of the typical processing chain, from the\npreprocessing, segmentation, feature extraction up to the matching and recognition steps.\nAccordingly, this article provides the first comprehensive review of the historical and state-of-\nthe-art approaches in DL-based techniques for iris recognition, followed by an in-depth analysis on\npivoting and groundbreaking advances in each phase of the processing chain. We summarize and\ncritically compare the most relevant methods for iris acquisition, segmentation, quality assessment,\nfeature encoding, matching and recognition problems, also presenting the most relevant open-\nproblems for each phase.\nFinally, we review the typical issues faced in DL-based methods in this domain of expertize,\nsuch as unsupervised learning, black-box models, and online learning and to illustrate how these\nchallenges can be important to open prolific future research paths and solutions.\nACKNOWLEDGMENTS\nWe would like to thank Adam Czajka from the University of Notre Dame, USA for the contribution\nin the early version of this survey paper in Sections 1, 5 and 6. The work due to Hugo Proença\nwas funded by FCT/MEC through national funds and co-funded by FEDER - PT2020 partnership\nagreement under the projects UIDB/50008/2020, POCI-01-0247-FEDER- 033395. Author Alonso-\nFernandez thanks the Swedish Innovation Agency VINNOVA (project MIDAS and DIFFUSE) and\nthe Swedish Research Council (project 2021-05110) for funding his research.\nREFERENCES\n[1] A. Agarwal, A. Noore, M. Vatsa, and R. Singh. 2022. Enhanced iris presentation attack detection via contraction-\nexpansion CNN. Pattern Recognition Letters 159 (2022), 61–69.\n[2] A. Agarwal, A. Noore, M. Vatsa, and R. Singh. 2022. Generalized Contact Lens Iris Presentation Attack Detection.\nIEEE Transactions on Biometrics, Behavior, and Identity Science (2022), 1–1.\n[3] S. Ahmad and B. Fuller. 2019. ThirdEye: Triplet Based Iris Recognition without Normalization. In IEEE Int. Conf. on\nBiometrics: Theory Applications and Systems (BTAS). 1–9.\n[4] F. Alonso-Fernandez and J. Bigun. 2016. A survey on periocular biometrics research. Pattern Recognition Letters 82\n(2016), 92–105.\n[5] F. Alonso-Fernandez, J. Fierrez, and J. Ortega-Garcia. 2012. Quality Measures in Biometric Systems. IEEE Security and\nPrivacy 10, 6 (2012), 52–62.\n[6] F. Alonso-Fernandez, K. Hernandez-Diaz, S. Ramis, F. J. Perales, and J. Bigun. 2021. Facial masks and soft-biometrics:\nLeveraging face recognition CNNs for age and gender prediction on mobile ocular images. IET Biometrics 10, 5 (2021).\n[7] M. Anisetti, Y.-H. Li, P.-J. Huang, and Y. Juan. 2019. An Efficient and Robust Iris Segmentation Algorithm Using Deep\nLearning. Mobile Information Systems (2019), 4568929.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n28\nK. Nguyen, H. Proença, F. Alonso-Fernandez\n[8] S. Arora and M.P.S. Bhatia. 2020. Presentation attack detection for iris recognition using deep learning. Int J Syst\nAssur Eng Manag 11 (2020), 232–238.\n[9] V. Badrinarayanan, A. Kendall, and R. Cipolla. 2017. SegNet: A Deep Convolutional Encoder-Decoder Architecture\nfor Image Segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 39, 12 (2017), 2481–2495.\n[10] A. Banerjee, C. Ghosh, and S. N. Mandal. 2022. Analysis of V-Net Architecture for Iris Segmentation in Unconstrained\nScenarios. SN Computer Science 3 (2022).\n[11] B. Bigdeli, P. Pahlavani, and H. A. Amirkolaee. 2021. An ensemble deep learning method as data fusion system for\nremote sensing multisensor classification. Applied Soft Computing 110 (2021), 107563.\n[12] D. S. Bolme, R. A. Tokola, C. B. Boehnen, T. B. Saul, K. A. Sauerwein, and D. W. Steadman. 2016. Impact of environmental\nfactors on biometric matching during human decomposition. In IEEE Int. Conf. on Biometrics: Theory Applications and\nSystems (BTAS). IEEE, USA, 1–8.\n[13] F. Boutros, N. Damer, K. Raja, R. Ramachandra, F. Kirchbuchner, and A. Kuijper. 2020. On Benchmarking Iris\nRecognition within a Head-mounted Display for AR/VR Applications. In IEEE Int. Joint Conf. on Biometrics (IJCB).\nUSA.\n[14] A. Boyd, K. Bowyer, and A. Czajka. 2022. Human-Aided Saliency Maps Improve Generalization of Deep Learning. In\nIEEE Winter Conference on Applications of Computer Vision (WACV).\n[15] A. Boyd, A. Czajka, and K. Bowyer. 2019. DL-Based Feature Extraction in Iris Recognition: Use Existing Models,\nFine-tune or Train From Scratch?. In IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS). 1–9.\n[16] A. Boyd, Z. Fang, A. Czajka, and K. Bowyer. 2020. Iris presentation attack detection: Where are we now? Pattern\nRecognition Letters 138 (2020), 483–489.\n[17] A. Boyd, P. Tinsley, K. Bowyer, and A. Czajka. 2021. CYBORG: Blending Human Saliency Into the Loss Improves\nDeep Learning. arXiv:2112.00686 [cs.CV]\n[18] A. Boyd, S. Yadav, T. Swearingen, A. Kuehlkamp, M. Trokielewicz, E. Benjamin, P. Maciejewicz, D. Chute, A. Ross, P.\nFlynn, K. Bowyer, and A. Czajka. 2020. Post-Mortem Iris Recognition—A Survey and Assessment of the State of the\nArt. IEEE Access 8 (2020), 136570–136593.\n[19] C. Chen and A. Ross. 2021. An Explainable Attention-Guided Iris Presentation Attack Detector. In IEEE Workshop on\nApplications of Computer Vision (WACV). 97–106.\n[20] J. Chen, F. Shen, D. Z. Chen, and P. Flynn. 2016. Iris Recognition Based on Human-Interpretable Features. IEEE\nTransactions on Information Forensics and Security 11, 7 (2016), 1476–1485.\n[21] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. Yuille. 2017. DeepLab: Semantic Image Segmentation with\nDeep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. IEEE Trans. Pattern Anal. Mach. Intell. 4\n(2017).\n[22] Y. Chen, W. Wang, Z. Zeng, and Y. Wang. 2019. An Adaptive CNNs Technology for Robust Iris Segmentation. IEEE\nAccess 7 (2019), 64517–64532.\n[23] Y. Chen, C. Wu, and Y. Wang. 2020. T-Center: A Novel Feature Extraction Approach Towards Large-Scale Iris\nRecognition. IEEE Access 8 (2020), 32365–32375.\n[24] Y. Chen, Z. Zeng, and F. Hu. 2019. End to End Robust Recognition Method for Iris Using a Dense Deep Convolutional\nNeural Network. In Biometric Recognition. Springer, Cham, 364–375.\n[25] M. Choudhary, V. Tiwari, and Venkanna U. 2021. Ensuring Secured Iris Authentication for Mobile Devices. In IEEE\nInternational Conference on Consumer Electronics (ICCE). 1–5.\n[26] M. Choudhary, V. Tiwari, and U. Venkanna. 2022. Iris Liveness Detection Using Fusion of Domain-Specific Multiple\nBSIF and DenseNet Features. IEEE Transactions on Cybernetics 52, 4 (2022), 2370–2381.\n[27] M. Choudhary, Tiwari V., and Venkanna U. 2022. Identifying discriminatory feature-vectors for fusion-based iris\nliveness detection. J Ambient Intell Human Comput (2022).\n[28] A. Czajka. 2013. Database of iris printouts and its application: Development of liveness detection method for iris\nrecognition. In International Conference on Methods and Models in Automation and Robotics (MMAR). 28–33.\n[29] A. Czajka. 2021. Iris recognition designed for post-mortem and diseased eyes. (2021). https://github.com/aczajka/\niris-recognition---pm-diseased-human-driven-bsif\n[30] A. Czajka and K. Bowyer. 2018. Presentation Attack Detection for Iris Recognition: An Assessment of the State-of-\nthe-Art. ACM Comput. Surv. 51, 4, Article 86 (Jul 2018), 35 pages.\n[31] A. Czajka, Z. Fang, and K. Bowyer. 2019. Iris Presentation Attack Detection Based on Photometric Stereo Features. In\nIEEE Winter Conference on Applications of Computer Vision (WACV). 877–885.\n[32] N. Damer, K. Dimitrov, A. Braun, and A. Kuijper. 2019. On Learning Joint Multi-biometric Representations by Deep\nFusion. In IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS). 1–8.\n[33] P. Das, J. Mcfiratht, Z. Fang, A. Boyd, G. Jang, A. Mohammadi, S. Purnapatra, D. Yambay, S. Marcel, M. Trokielewicz,\nP. Maciejewicz, K. Bowyer, A. Czajka, S. Schuckers, J. Tapia, S. Gonzalez, M. Fang, N. Damer, F. Boutros, A. Kuijper, R.\nSharma, C. Chen, and A. Ross. 2020. Iris Liveness Detection Competition (LivDet-Iris) - The 2020 Edition. In IEEE Int.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n29\nJoint Conf. on Biometrics (IJCB). 1–9.\n[34] J. Daugman. 1993. High confidence visual recognition of persons by a test of statistical independence. IEEE Trans.\nPattern Anal. Mach. Intell. 15, 11 (1993), 1148–1161.\n[35] J. Daugman. 2007. New methods in iris recognition. IEEE Transactions on Systems, Man and Cybernetics 37 (2007),\n1167–1175.\n[36] J. Daugman. 2016. Information Theory and the IrisCode. IEEE Transactions on Information Forensics and Security 11\n(2016), 400–409.\n[37] J. Daugman. 2021. Collision Avoidance on National and Global Scales: Understanding and Using Big Biometric\nEntropy. TechRxiv (2021).\n[38] M. De Marsico, M. Nappi, D. Riccio, and H. Wechsler. 2015. Mobile Iris Challenge Evaluation (MICHE)-I, biometric iris\ndataset and protocols. Pattern Recognition Letters 57 (2015), 17–23. Mobile Iris CHallenge Evaluation part I (MICHE I).\n[39] X. Ding, H. Fang, Z. Zhang, K.-K. R. Choo, and H. Jin. 2022. Privacy-Preserving Feature Extraction via Adversarial\nTraining. IEEE Transactions on Knowledge and Data Engineering 34, 4 (2022), 1967–1979.\n[40] W. Dong, Z. Sun, and T. Tan. 2009. A Design of Iris Recognition System at a Distance. In Chinese Conference on Pattern\nRecognition. 1–5.\n[41] J. S. Doyle and K. Bowyer. 2015. Robust Detection of Textured Contact Lenses in Iris Recognition Using BSIF. IEEE\nAccess 3 (2015), 1672–1683.\n[42] J. S. Doyle, K. Bowyer, and P. Flynn. 2013. Variation in accuracy of textured contact lens detection based on sensor\nand lens pattern. In IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS). 1–7.\n[43] T. Elsken, J. H. Metzen, and F. Hutter. 2019. Neural architecture search: A survey. J. Mach. Learn. Res. 20, 55 (2019).\n[44] M. Fang, N. Damer, Fadi Boutros, F. Kirchbuchner, and A. Kuijper. 2020. Deep Learning Multi-layer Fusion for an\nAccurate Iris Presentation Attack Detection. In IEEE International Conference on Information Fusion (FUSION). 1–8.\n[45] M. Fang, N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper. 2021. Cross-database and cross-attack Iris presentation\nattack detection using micro stripes analyses. Image and Vision Computing 105 (2021), 104057.\n[46] M. Fang, N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper. 2021. Iris Presentation Attack Detection by Attention-\nbased and Deep Pixel-wise Binary Supervision Network. In IEEE Int. Joint Conf. on Biometrics (IJCB). 1–8.\n[47] M. Fang, N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper. 2021. The overlapping effect and fusion protocols of\ndata augmentation techniques in iris PAD. Machine Vision and Applications 33 (2021).\n[48] M. Fang, N. Damer, F. Kirchbuchner, and A. Kuijper. 2020. Micro Stripes Analyses for Iris Presentation Attack\nDetection. In IEEE Int. Joint Conf. on Biometrics (IJCB). 1–10.\n[49] M. Fang, N. Damer, F. Kirchbuchner, and A. Kuijper. 2021. Demographic Bias in Presentation Attack Detection of Iris\nRecognition Systems. In 28th European Signal Processing Conference (EUSIPCO). 835–839.\n[50] Z. Fang and A. Czajka. 2020. Open Source Iris Recognition Hardware and Software with Presentation Attack Detection.\nIn IEEE Int. Joint Conf. on Biometrics (IJCB). 1–8.\n[51] Z. Fang, A. Czajka, and K. Bowyer. 2021. Robust Iris Presentation Attack Detection Fusing 2D and 3D Information.\nIEEE Transactions on Information Forensics and Security 16 (2021), 510–520.\n[52] FBI Criminal Justice Information Services (CJIS) Division. 2021. Next Generation Identification (NGI). Retrieved 2021\nfrom https://www.fbi.gov/services/cjis/fingerprints-and-other-biometrics/ngi\n[53] J. Fierrez, J. Ortega-Garcia, D. Torre Toledano, and J. Gonzalez-Rodriguez. 2007. Biosec baseline corpus: A multimodal\nbiometric database. Pattern Recognition 40, 4 (2007), 1389–1392.\n[54] J. Galbally and M. Gomez-Barrero. 2016. A review of iris anti-spoofing. In 4th International Conference on Biometrics\nand Forensics (IWBF). 1–6.\n[55] Y. Ganeeva and E. Myasnikov. 2020. Using Convolutional Neural Networks for Segmentation of Iris Images. In\nInternational Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon). 1–4.\n[56] A. Gangwar and A. Joshi. 2016. DeepIrisNet: Deep iris representation with applications in iris recognition and\ncross-sensor iris recognition. In IEEE International Conference on Image Processing. 2301–2305.\n[57] A. Gangwar, A. Joshi, P. Joshi, and R. Raghavendra. 2019. DeepIrisNet2: Learning Deep-IrisCodes from Scratch for\nSegmentation-Robust Visible Wavelength and Near Infrared Iris Recognition. CoRR abs/1902.05390 (2019).\n[58] S. J. Garbin, Y. Shen, I. Schuetz, R. Cavin, G. Hughes, and S. S. Talathi. 2019. OpenEDS: Open Eye Dataset. CoRR\nabs/1905.03702 (2019). arXiv:1905.03702\n[59] G. Gautam, A. Raj, and S. Mukhopadhyay. 2022. Deep supervised class encoding for iris presentation attack detection.\nDigital Signal Processing 121 (2022), 103329.\n[60] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. 2014. Generative\nAdversarial Nets. In International Conference on Neural Information Processing Systems (NIPS) (Canada). USA, 9.\n[61] D. Gragnaniello, C. Sansone, G. Poggi, and L. Verdoliva. 2016. Biometric Spoofing Detection by a Domain-Aware\nConvolutional Neural Network. In International Conference on Signal-Image Technology and Internet-Based Systems\n(SITIS).\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n30\nK. Nguyen, H. Proença, F. Alonso-Fernandez\n[62] P. Grother and E. Tabassi. 2007. Performance of Biometric Quality Measures. IEEE Trans. Pattern Anal. Mach. Intell.\n29 (2007), 531–543.\n[63] M. Gupta, S. Singh, A. Agarwal, M. Vatsa, and R. Singh. 2021. Generalized Iris Presentation Attack Detection Algorithm\nunder Cross-Database Settings. In Int. Conf. on Pattern Recognition (ICPR). 5318–5325.\n[64] P. Gupta, S. Behera, M. Vatsa, and R. Singh. 2014. On Iris Spoofing Using Print Attack. In Int. Conf. on Pattern\nRecognition (ICPR). 1681–1686.\n[65] A. Hafner, P. Peer, Ž. Emeršič, and M. Vitek. 2021. Deep Iris Feature Extraction. In International Conference on Artificial\nIntelligence in Information and Communication (ICAIIC). 258–262.\n[66] Y. Han. 2021. Design of An Active Infrared Iris Recognition Device. In IEEE Asia-Pacific Conference on Image Processing,\nElectronics and Computers (IPEC). 435–437.\n[67] Y. He, G. Meng, K. Chen, X. Hu, and J. He. 2022. Towards Security Threats of Deep Learning Systems: A Survey. IEEE\nTransactions on Software Engineering 48, 5 (2022), 1743–1770.\n[68] Z. He, T. Tan, Z. Sun, and X. Qiu. 2009. Toward Accurate and Fast Iris Segmentation for Iris Biometrics. IEEE Trans.\nPattern Anal. Mach. Intell. 31, 9 (2009), 1670–1684.\n[69] P. Hedman, V. Skepetzis, K. Hernandez-Diaz, J. Bigün, and F. Alonso-Fernandez. 2021. On the Effect of Selfie\nBeautification Filters on Face Detection and Recognition. CoRR abs/2110.08934 (2021). arXiv:2110.08934\n[70] K. Hernandez-Diaz, F. Alonso-Fernandez, and J. Bigun. 2018. Periocular Recognition Using CNN Features Off-the-Shelf.\nIn International Conference of the Biometrics Special Interest Group (BIOSIG). 1–5.\n[71] K. Hernandez-Diaz, F. Alonso-Fernandez, and J. Bigun. 2020. Cross-Spectral Periocular Recognition with Conditional\nAdversarial Networks. In IEEE Int. Joint Conf. on Biometrics (IJCB). 1–9.\n[72] H. Hofbauer, E. Jalilian, and A. Uhl. 2019. Exploiting superior CNN-based iris segmentation for better recognition\naccuracy. Pattern Recognition Letters 120 (2019), 17–23.\n[73] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, Q. V. Le, and H.\nAdam. 2019. Searching for MobileNetV3. In IEEE Int. Conf. on Computer Vision (ICCV).\n[74] C.-S. Hsiao and C.-P. Fan. 2021. EfficientNet Based Iris Biometric Recognition Methods with Pupil Positioning by\nU-Net. In 3rd International Conference on Computer Communication and the Internet (ICCCI). 1–5.\n[75] L. Hu, J. Li, G. Lin, S. Peng, Z. Zhang, Y. Zhang, and C. Dong. 2022. Defending against Membership Inference Attacks\nwith High Utility by GAN. IEEE Transactions on Dependable and Secure Computing (2022), 1–1.\n[76] V. T. Huynh, S.-H. Kim, G.-S. Lee, and H.-J. Yang. 2019. Eye Semantic Segmentation with A Lightweight Model. In\nIEEE/CVF International Conference on Computer Vision Workshop (ICCVW). 3694–3697.\n[77] H. Hwang and E. C. Lee. 2020. Near-Infrared Image-Based Periocular Biometric Method Using Convolutional Neural\nNetwork. IEEE Access 8 (2020), 158612–158621.\n[78] A. Jain, D. Deb, and J. Engelsma. 2021. Biometrics: Trust, but Verify. IEEE Transactions on Biometrics, Behavior, and\nIdentity Science (2021), 1–1.\n[79] E. Jalilian, M. Karakaya, and A. Uhl. 2020. End-to-end Off-angle Iris Recognition Using CNN Based Iris Segmentation.\nIn International Conference of the Biometrics Special Interest Group (BIOSIG). 1–7.\n[80] E. Jalilian, G. Wimmer, A. Uhl, and M. Karakaya. 2022. Deep Learning Based Off-Angle Iris Recognition. In IEEE Int.\nConf. on Acoustics, Speech, and Signal Processing (ICASSP). 4048–4052.\n[81] P. A. Johnson, P. Lopez-Meyer, N. Sazonova, F. Hua, and S. Schuckers. 2010. Quality in face and iris research ensemble\n(Q-FIRE). In IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS). 1–6.\n[82] Y. G. Jung, C. Y. Low, J. Park, and A. B. J. Teoh. 2020. Periocular Recognition in the Wild With Generalized Label\nSmoothing Regularization. IEEE Signal Processing Letters 27 (2020), 1455–1459.\n[83] H. Kaur and R. Manduchi. 2020. EyeGAN: Gaze–Preserving, Mask–Mediated Eye Image Synthesis. In IEEE Winter\nConference on Applications of Computer Vision (WACV). 299–308.\n[84] H. Kaur and R. Manduchi. 2021. Subject Guided Eye Image Synthesis with Application to Gaze Redirection. In IEEE\nWinter Conference on Applications of Computer Vision (WACV). 11–20.\n[85] D. Kerrigan, M. Trokielewicz, A. Czajka, and K. Bowyer. 2019. Iris Recognition with Image Segmentation Employing\nRetrained Off-the-Shelf Deep Neural Networks. In IEEE Int. Conf. on Biometrics (ICB). 1–7.\n[86] F. Khalid, M. A. Hanif, S. Rehman, R. Ahmed, and M. Shafique. 2019. TrISec: Training Data-Unaware Imperceptible\nSecurity Attacks on Deep Neural Networks. In IEEE 25th International Symposium on On-Line Testing and Robust\nSystem Design (IOLTS). 188–193.\n[87] M. Khosravy, K. Nakamura, Y. Hirose, N. Nitta, and N. Babaguchi. 2022. Model Inversion Attack by Integration of\nDeep Generative Models: Privacy-Sensitive Face Generation From a Face Recognition System. IEEE Transactions on\nInformation Forensics and Security 17 (2022), 357–372.\n[88] M. C. Kim, J. H. Koo, S. W. Cho, N. R. Baek, and K. R. Park. 2018. Convolutional Neural Network-Based Periocular\nRecognition in Surveillance Environments. IEEE Access 6 (2018), 57291–57310.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n31\n[89] N. Kohli, D. Yadav, M. Vatsa, and R. Singh. 2013. Revisiting iris recognition with color cosmetic contact lenses. In\nIEEE Int. Conf. on Biometrics (ICB). 1–7.\n[90] N. Kohli, D. Yadav, M. Vatsa, R. Singh, and A. Noore. 2017. Synthetic iris presentation attack using iDCGAN. In IEEE\nInt. Joint Conf. on Biometrics (IJCB). 674–680.\n[91] A. Kuehlkamp, A. Boyd, A. Czajka, K. Bowyer, P. Flynn, D. Chute, and E. Benjamin. 2022. Interpretable Deep\nLearning-Based Forensic Iris Segmentation and Recognition. In 2nd WACV Workshop on Explainable and Interpretable\nArtificial Intelligence for Biometrics (XAI4B). IEEE, USA, 1–8.\n[92] A. Kumar and A. Passi. 2010. Comparison and combination of iris matchers for reliable personal authentication.\nPattern Recognition 43, 3 (2010), 1016–1026.\n[93] C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W.\nShi. 2017. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. In IEEE Int. Conf.\non Computer Vision and Pattern Recognition (CVPR). 105–114.\n[94] Y.-H. Li, W. R. Putri, M. S. Aslam, and C.-C. Chang. 2021. Robust Iris Segmentation Algorithm in Non-Cooperative\nEnvironments Using Interleaved Residual U-Net. Sensors 21, 4 (2021).\n[95] T. Liang, J. Glossner, L. Wang, S. Shi, and X. Zhang. 2021. Pruning and quantization for deep neural network\nacceleration: A survey. Neurocomputing 461 (2021), 370–403.\n[96] G. Lin, A. Milan, C. Shen, and I. Reid. 2016. RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic\nSegmentation. arXiv:1611.06612 [cs.CV]\n[97] N. Liu, M. Zhang, H. Li, Z. Sun, and T. Tan. 2016. DeepIris: Learning pairwise filter bank for heterogeneous iris\nverification. Pattern Recognition Letters 82 (2016), 154–161.\n[98] W. Liu, A. Rabinovich, and A. Berg. 2016. ParseNet: Looking Wider to See Better. In International Conference on\nLearning Representations (ICLR).\n[99] X. Liu, Y. Bai, Y. Luo, Z. Yang, and Y. Liu. 2019. Iris recognition in visible spectrum based on multi-layer analogous\nconvolution and collaborative representation. Pattern Recognition Letters 117 (2019), 66–73.\n[100] J. Long, E. Shelhamer, and T. Darrell. 2015. Fully convolutional networks for semantic segmentation. In IEEE Int. Conf.\non Computer Vision and Pattern Recognition (CVPR). 3431–3440.\n[101] J. Lozej, B. Meden, V. Struc, and P. Peer. 2018. End-to-End Iris Segmentation Using U-Net. In IEEE International Work\nConference on Bioinspired Intelligence (IWOBI). 1–6.\n[102] Z. Luo, J. Li, and Y. Zhu. 2021. A Deep Feature Fusion Network Based on Multiple Attention Mechanisms for Joint\nIris-Periocular Biometric Recognition. IEEE Signal Processing Letters 28 (2021), 1060–1064.\n[103] L. Ma, T. Tan, Y. Wang, and D. Zhang. 2003. Personal identification based on iris texture analysis. IEEE Trans. Pattern\nAnal. Mach. Intell. 25, 12 (2003), 1519–1533.\n[104] D. Menotti, G. Chiachia, A. Pinto, W. R. Schwartz, H. Pedrini, A. X. Falcão, and A. Rocha. 2015. Deep Representations\nfor Iris, Face, and Fingerprint Spoofing Detection. IEEE Transactions on Information Forensics and Security 10, 4 (2015).\n[105] S. Minaee, A. Abdolrashidiy, and Y. Wang. 2016. An experimental study of deep convolutional features for iris\nrecognition. IEEE Signal Processing in Medicine and Biology Symposium (SPMB), 1–6.\n[106] S. Mishra, P. Liang, A. Czajka, Danny Z. Chen, and X. Hu. 2019. CC-NET: Image Complexity Guided Network\nCompression for Biomedical Image Segmentation. In IEEE 16th International Symposium on Biomedical Imaging (ISBI\n2019). 57–60.\n[107] D. Monro, S. Rakshit, and D. Zhang. 2007. DCT-Based Iris Recognition. IEEE Trans. Pattern Anal. Mach. Intell. 29, 4\n(2007), 586–595.\n[108] D. Moreira, M. Trokielewicz, A. Czajka, K. Bowyer, and P. Flynn. 2019. Performance of Humans in Iris Recognition:\nThe Impact of Iris Condition and Annotation-Driven Verification. In IEEE Winter Conference on Applications of\nComputer Vision (WACV). 941–949.\n[109] M. Mostofa, S. Mohamadi, J. Dawson, and N. Nasrabadi. 2021. Deep GAN-Based Cross-Spectral Cross-Resolution Iris\nRecognition. IEEE Transactions on Biometrics, Behavior, and Identity Science 3, 4 (2021), 443–463.\n[110] A. Muron and J. Pospisil. 2000. The human iris structure and its usages. In Acta Univ Plalcki Physica. Vol. 39. Acta\nUniversitatis, 87–95.\n[111] K. Nguyen, C. Fookes, R. Jillela, S. Sridharan, and A. Ross. 2017. Long range iris recognition: A survey. Pattern\nRecognition 72 (2017), 123–143.\n[112] K. Nguyen, C. Fookes, A. Ross, and S. Sridharan. 2017. Iris recognition with Off-the-Shelf CNN Features: A deep\nlearning perspective. IEEE Access 6 (2017), 18848–18855. Invited Paper.\n[113] K. Nguyen, C. Fookes, and S. Sridharan. 2020. Constrained Design of Deep Iris Networks. IEEE Transactions on Image\nProcessing 29 (2020), 7166–7175.\n[114] K. Nguyen, C. Fookes, S. Sridharan, and S. Denman. 2011. Quality-Driven Super-Resolution for Less Constrained Iris\nRecognition at a Distance and on the Move. IEEE Transactions on Information Forensics and Security 6, 4 (2011).\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n32\nK. Nguyen, H. Proença, F. Alonso-Fernandez\n[115] K. Nguyen, C. Fookes, S. Sridharan, and A. Ross. 2022. Complex-valued Iris Recognition Network. IEEE Trans. Pattern\nAnal. Mach. Intell. (2022).\n[116] K. Nguyen, C. Fookes, S. Sridharan, M. Tistarelli, and M. Nixon. 2018. Super-resolution for biometrics: A comprehensive\nsurvey. Pattern Recognition 78 (2018), 23–42.\n[117] K. Nguyen, S. Sridharan, S. Denman, and C. Fookes. 2012. Feature-domain super-resolution framework for Gabor-based\nface and iris recognition. In IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR). 2642–2649.\n[118] L. Nie, A. Kumar, and S. Zhan. 2014. Periocular Recognition Using Unsupervised Convolutional RBM Feature Learning.\nIn Int. Conf. on Pattern Recognition (ICPR). 399–404.\n[119] I. Nigam, M. Vatsa, and R. Singh. 2015. Ocular biometrics: A survey of modalities and fusion approaches. Information\nFusion 26 (2015), 1–35.\n[120] NIST. 2021. IEG: Iris Examiner Training Discussion: https://www.nist.gov/itl/iad/image-group/ieg-iris-examiner-\ntraining-discussion.\n[121] G. Odinokikh, M. Korobkin, I. Solomatin, I. Efimov, and A. Fartukov. 2019. Iris Feature Extraction and Matching\nMethod for Mobile Biometric Applications. In IEEE Int. Conf. on Biometrics (ICB). 1–6.\n[122] L. Omelina, J. Goga, J. Pavlovicova, M. Oravec, and B. Jansen. 2021. A survey of iris datasets. Image and Vision\nComputing 108 (2021), 104109.\n[123] J. Ortega, J. Fierrez, F. Alonso, J. Galbally, M. R. Freire, J. Gonzalez, C. Garcia, J. Alba, E. Gonzalez-Agulla, E. Otero, S.\nGarcia-Salicetti, L. Allano, B. Ly-Van, B. Dorizzi, J. Kittler, T. Bourlai, N. Poh, F. Deravi, Ming N. R. Ng, M. Fairhurst, J.\nHennebert, A. Humm, M. Tistarelli, L. Brodo, J. Richiardi, A. Drygajlo, H. Ganster, F. M. Sukno, S. Pavani, A. Frangi, L.\nAkarun, and A. Savran. 2010. The Multiscenario Multienvironment BioSecure Multimodal Database (BMDB). IEEE\nTrans. Pattern Anal. Mach. Intell. 32, 6 (2010), 1097–1111.\n[124] U. Park, R. Jillela, A. Ross, and A. Jain. 2011. Periocular Biometrics in the Visible Spectrum. IEEE Transactions on\nInformation Forensics and Security 6, 1 (2011), 96–106.\n[125] L. Parzianello and A. Czajka. 2022. Saliency-Guided Textured Contact Lens-Aware Iris Recognition. In IEEE Workshop\non Applications of Computer Vision (WACV). 330–337.\n[126] H. Peng, B. Li, D. He, and J. Wang. 2020. End-to-End Anti-Attack Iris Location Based on Lightweight Network. In\nIEEE International Conference on Advances in Electrical Engineering and Computer Applications (AEECA). 821–827.\n[127] P. J. Phillips, W. T. Scruggs, A. J. O’Toole, P. Flynn, K. Bowyer, C. L. Schott, and M. Sharpe. 2010. FRVT 2006 and ICE\n2006 Large-Scale Experimental Results. IEEE Trans. Pattern Anal. Mach. Intell. 32, 5 (2010), 831–846.\n[128] Planet Biometrics. 2017. Homeland Advanced Recognition Technology (HART). http://www.planetbiometrics.com/\narticle-details/i/5598/desc/dhs-launches-rfp-for-hart/\n[129] H. Proença. 2013. Iris Recognition in the Visible Wavelength. Springer, UK, 151–169.\n[130] H. Proença and L. Alexandre. 2005. UBIRIS: A Noisy Iris Image Database. In Image Analysis and Processing – ICIAP\n2005. Springer, Germany, 970–977.\n[131] H. Proenca, S. Filipe, R. Santos, J. Oliveira, and L. Alexandre. 2010. The UBIRIS.v2: A Database of Visible Wavelength\nIris Images Captured On-the-Move and At-a-Distance. IEEE Trans. Pattern Anal. Mach. Intell. 32, 8 (2010), 1529–1535.\n[132] H. Proença and J. C. Neves. 2017. IRINA: Iris Recognition (Even) in Inaccurately Segmented Data. In IEEE Int. Conf.\non Computer Vision and Pattern Recognition (CVPR). 6747–6756.\n[133] H. Proença. 2010. Iris Recognition: On the Segmentation of Degraded Images Acquired in the Visible Wavelength.\nIEEE Trans. Pattern Anal. Mach. Intell. 32, 8 (2010), 1502–1516.\n[134] H. Proença and J. Neves. 2018. Deep-PRWIS: Periocular Recognition Without the Iris and Sclera Using Deep Learning\nFrameworks. IEEE Transactions on Information Forensics and Security 13, 4 (2018), 888–896.\n[135] H. Proença and J. C. Neves. 2019. Segmentation-Less and Non-Holistic Deep-Learning Frameworks for Iris Recognition.\nIn IEEE Int. Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW). 2296–2305.\n[136] Quinn, G. and Matey, J. and Grother, P. and Watters, E. 2022. Statistics of Visual Features in the Human Iris.\n[137] E. Ribeiro, A. Uhl, and F. Alonso-Fernandez. 2019. Iris super-resolution using CNNs: is photo-realism important to\niris recognition? IET Biometrics 8 (2019), 69–78.\n[138] E. Ribeiro, A. Uhl, F. Alonso-Fernandez, and R. A. Farrugia. 2017. Exploring deep learning image super-resolution for\niris recognition. In 25th European Signal Processing Conference (EUSIPCO). 2176–2180.\n[139] O. Ronneberger, P.Fischer, and T. Brox. 2015. U-Net: Convolutional Networks for Biomedical Image Segmentation. In\nMedical Image Computing and Computer-Assisted Intervention (MICCAI) (LNCS, Vol. 9351). Springer, 234–241.\n[140] I. Saleh. 2018. Internet of Things (IoT): Concepts, Issues, Challenges and Perspectives. 1–26.\n[141] A. Sansola. 2015. Postmortem iris recognition and its application in human identification. Master’s thesis. Boston\nUniversity, USA.\n[142] M. Sardar, S. Banerjee, and S. Mitra. 2020. Iris Segmentation Using Interactive Deep Learning. IEEE Access 8 (2020),\n219322–219330.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n33\n[143] K. Sauerwein, T. B. Saul, D. W. Steadman, and C. B. Boehnen. 2017. The Effect of Decomposition on the Efficacy of\nBiometrics for Positive Identification. Journal of Forensic Sciences 62, 6 (2017), 1599–1602.\n[144] T. Schlett, C. Rathgeb, and C. Busch. 2018. Multi-spectral Iris Segmentation in Visible Wavelengths. In IEEE Int. Conf.\non Biometrics (ICB). 190–194.\n[145] F. Schroff, D. Kalenichenko, and J. Philbin. 2015. FaceNet: A unified embedding for face recognition and clustering. In\nIEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR). 815–823.\n[146] A. Sequeira, H. Oliveira, J. Monteiro, J. Monteiro, and J. Cardoso. 2014. MobILive 2014 - Mobile Iris Liveness Detection\nCompetition. In IEEE Int. Joint Conf. on Biometrics (IJCB). 1–6.\n[147] S. Shah and A. Ross. 2009. Iris Segmentation Using Geodesic Active Contours. IEEE Transactions on Information\nForensics and Security 4, 4 (2009), 824–836.\n[148] A. Sharma, S. Verma, M. Vatsa, and R. Singh. 2014. On cross spectral periocular recognition. In Int. IEEE Int. Conf. on\nImage Processing. 5007–5011.\n[149] D. Sharma and A. Selwal. 2021. On Data-Driven Approaches for Presentation Attack Detection in Iris Recognition\nSystems. In Recent Innovations in Computing. Springer, Singapore, 463–473.\n[150] R. Sharma and A. Ross. 2020. D-NetPAD: An Explainable and Interpretable Iris Presentation Attack Detector. In IEEE\nInt. Joint Conf. on Biometrics (IJCB). 1–10.\n[151] R. Sharma and A. Ross. 2021. Viability of Optical Coherence Tomography for Iris Presentation Attack Detection. In\nInt. Conf. on Pattern Recognition (ICPR). 6165–6172.\n[152] F. Shen and P. Flynn. 2013. Using crypts as iris minutiae. In Biometric and Surveillance Technology for Human and\nActivity Identification X, Vol. 8712. 87120B.\n[153] L. Song, R. Shokri, and P. Mittal. 2019. Membership Inference Attacks Against Adversarially Robust Deep Learning\nModels. In IEEE Security and Privacy Workshops (SPW). 50–56.\n[154] L. Stark, K. Bowyer, and S. Siena. 2010. Human perceptual categorization of iris texture patterns. In IEEE Int. Conf. on\nBiometrics: Theory Applications and Systems (BTAS). 1–7.\n[155] Z. Sun, H. Zhang, T. Tan, and J. Wang. 2014. Iris Image Classification Based on Hierarchical Visual Codebook. IEEE\nTrans. Pattern Anal. Mach. Intell. 36, 6 (2014), 1120–1133.\n[156] M. S. Sunder and A. Ross. 2010. Iris Image Retrieval Based on Macro-features. In Int. Conf. on Pattern Recognition\n(ICPR). 1318–1321.\n[157] E. Tabassi, P. Grother, and W. Salamon. 2011. IREX II - IQCE - Iris Quality Calibration and Evaluation. Performance\nof Iris Image Quality Assessment Algorithms. NISTIR 7296 - http://iris.nist.gov/irex/ (2011).\n[158] V. Talreja, N. M. Nasrabadi, and M. C. Valenti. 2022. Attribute-Based Deep Periocular Recognition: Leveraging Soft\nBiometrics to Improve Periocular Recognition. In IEEE Winter Conference on Applications of Computer Vision (WACV).\n[159] C.-W. Tan and A. Kumar. 2012. Unified Framework for Automated Iris Segmentation Using Distantly Acquired Face\nImages. IEEE Transactions on Image Processing 21, 9 (2012), 4068–4079.\n[160] T. Tan, Zh. He, and Z. Sun. 2010. Efficient and robust segmentation of noisy iris images for non-cooperative iris\nrecognition. Image and Vision Computing 28, 2 (2010), 223–230.\n[161] H. Tann, H. Zhao, and S. Reda. 2019. A Resource-Efficient Embedded Iris Recognition System Using Fully Convolutional\nNetworks. ACM Journal on Emerging Technologies in Computing Systems 16, 1 (2019), 1–23.\n[162] J. Tapia, S. Gonzalez, and C. Busch. 2022. Iris Liveness Detection Using a Cascade of Dedicated Deep Learning\nNetworks. IEEE Transactions on Information Forensics and Security 17 (2022), 42–52.\n[163] ISO/IEC 30107-1. Information technology — Biometric presentation attack detection — Part 1: Framework. 2016.\n[164] ISO/IEC 29794-6. Information technology — Biometric sample quality — Part 6: Iris image data. 2015.\n[165] P. Tinsley, A. Czajka, and P. Flynn. 2021. This Face Does Not Exist... But It Might Be Yours! Identity Leakage in\nGenerative Models. In IEEE Winter Conference on Applications of Computer Vision (WACV). 1319–1327.\n[166] M. Trokielewicz and A. Czajka. 2018. Data-driven segmentation of post-mortem iris images. In International Workshop\non Biometrics and Forensics (IWBF). IEEE, Italy, 1–7.\n[167] M. Trokielewicz, A. Czajka, and P. Maciejewicz. 2016. Human iris recognition in post-mortem subjects: Study and\ndatabase. In IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS). IEEE, USA, 1–6.\n[168] M. Trokielewicz, A. Czajka, and P. Maciejewicz. 2016. Post-mortem human iris recognition. In IEEE Int. Conf. on\nBiometrics (ICB). IEEE, Sweden, 1–6.\n[169] M. Trokielewicz, A. Czajka, and P. Maciejewicz. 2018. Presentation Attack Detection for Cadaver Iris. In IEEE Int.\nConf. on Biometrics: Theory Applications and Systems (BTAS). 1–10.\n[170] M. Trokielewicz, A. Czajka, and P. Maciejewicz. 2019. Iris Recognition After Death. IEEE Transactions on Information\nForensics and Security 14, 6 (June 2019), 1501–1514.\n[171] M. Trokielewicz, A. Czajka, and P. Maciejewicz. 2020. Post-Mortem Iris Recognition Resistant to Biological Eye Decay\nProcesses. In IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, USA, 1–8.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n34\nK. Nguyen, H. Proença, F. Alonso-Fernandez\n[172] M. Trokielewicz, A. Czajka, and P. Maciejewicz. 2020. Post-mortem iris recognition with deep-learning-based image\nsegmentation. Image and Vision Computing 94 (2020), 103866.\n[173] M. Tygert, J. Bruna, S. Chintala, Y. LeCun, S. Piantino, and A. Szlam. 2016. A Mathematical Motivation for Complex-\nvalued Convolutional Networks. Neural Computation 28 (2016), 815–825.\n[174] Unique Identification Authority of India. 2021. AADHAAR: http://uidai.gov.in.\n[175] M. Vatsa, R. Singh, and A. Noore. 2008. Improving Iris Recognition Performance Using Segmentation, Quality\nEnhancement, Match Score Fusion, and Indexing. IEEE Transactions on Systems, Man, and Cybernetics, Part B\n(Cybernetics) 38, 4 (2008), 1021–1035.\n[176] C. Wang, J. Muhammad, Y. Wang, Z. He, and Z. Sun. 2020. Towards Complete and Accurate Iris Segmentation Using\nDeep Multi-Task Attention Network for Non-Cooperative Iris Recognition. IEEE Transactions on Information Forensics\nand Security 15 (2020), 2944–2959.\n[177] C. Wang and Z. Sun. 2020. A Benchmark for Iris Segmentation. Journal of Computer Research and Development 57, 2\n(2020), 395–412.\n[178] C. Wang, Y. Wang, B. Xu, Y. He, Z. Dong, and Z. Sun. 2020. A Lightweight Multi-Label Segmentation Network for\nMobile Iris Biometrics. In IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP). 1006–1010.\n[179] C. Wang, Y. Zhu, Y. Liu, R. He, and Z. Sun. 2019. Joint Iris Segmentation and Localization Using Deep Multi-task\nLearning Framework. arXiv:1901.11195 [cs.CV]\n[180] K. Wang and A. Kumar. 2019. Toward More Accurate Iris Recognition Using Dilated Residual Features. IEEE\nTransactions on Information Forensics and Security 14, 12 (2019), 3233–3245.\n[181] K. Wang and A. Kumar. 2021. Periocular-Assisted Multi-Feature Collaboration for Dynamic Iris Recognition. IEEE\nTransactions on Information Forensics and Security 16 (2021), 866–879.\n[182] L. Wang, K. Zhang, M. Ren, Y. Wang, and Z. Sun. 2020. Recognition Oriented Iris Image Quality Assessment in the\nFeature Space. In IEEE Int. Joint Conf. on Biometrics (IJCB). 1–9.\n[183] X. Wang, H. Zhang, J. Liu, L. Xiao, Z. He, L. Liu, and P. Duan. 2019. Iris Image Super Resolution Based on GANs with\nAdversarial Triplets. In Chinese Conference on Biometric Recognition (LNCS). Switzerland, 346 – 53.\n[184] Z. Wang, J. Chai, and S. Xia. 2021. Realtime and Accurate 3D Eye Gaze Capture with DCNN-Based Iris and Pupil\nSegmentation. IEEE Transactions on Visualization and Computer Graphics 27, 1 (2021), 190–203.\n[185] Z. Wei, T. Tan, and Z. Sun. 2007. Nonlinear Iris Deformation Correction Based on Gaussian Model. In Advances in\nBiometrics. Springer, Germany, 780–789.\n[186] X. Wu and L. Zhao. 2019. Study on Iris Segmentation Algorithm Based on Dense U-Net. IEEE Access 7 (2019),\n123959–123968.\n[187] L. Xiao, Z. Sun, R. He, and T. Tan. 2013. Coupled feature selection for cross-sensor iris recognition. In IEEE Int. Conf.\non Biometrics: Theory Applications and Systems (BTAS). 1–6.\n[188] Fisher Y. and Vladlen K. 2016. Multi-Scale Context Aggregation by Dilated Convolutions. arXiv:1511.07122 [cs.CV]\n[189] D. Yadav, N. Kohli, M. Vatsa, R. Singh, and A. Noore. 2019. Detecting Textured Contact Lens in Uncontrolled\nEnvironment Using DensePAD. In IEEE Int. Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW).\n[190] D. Yadav, N. Kohli, S. Yadav, M. Vatsa, R. Singh, and A. Noore. 2018. Iris Presentation Attack via Textured Contact\nLens in Unconstrained Environment. In IEEE Winter Conference on Applications of Computer Vision (WACV). 503–511.\n[191] S. Yadav, C. Chen, and A. Ross. 2019. Synthesizing Iris Images Using RaSGAN With Application in Presentation\nAttack Detection. In IEEE Int. Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW). 2422–2430.\n[192] S. Yadav, C. Chen, and A. Ross. 2020. Relativistic Discriminator: A One-Class Classifier for Generalized Iris Presentation\nAttack Detection. In IEEE Winter Conference on Applications of Computer Vision (WACV). 2624–2633.\n[193] S. Yadav and A. Ross. 2021. CIT-GAN: Cyclic Image Translation Generative Adversarial Network With Application in\nIris Presentation Attack Detection. In IEEE Winter Conference on Applications of Computer Vision (WACV). 2411–2420.\n[194] D. Yambay, B. Becker, N. Kohli, D. Yadav, A. Czajka, K. Bowyer, S. Schuckers, R. Singh, M. Vatsa, A. Noore, D.\nGragnaniello, C. Sansone, L. Verdoliva, L. He, Y. Ru, H. Li, N. Liu, Z. Sun, and T. Tan. 2017. LivDet iris 2017 — Iris\nliveness detection competition 2017. In IEEE Int. Joint Conf. on Biometrics (IJCB). 733–741.\n[195] Z. Yan, L. He, Y. Wang, Z. Sun, and T. Tan. 2021. Flexible Iris Matching Based on Spatial Feature Reconstruction. IEEE\nTransactions on Biometrics, Behavior, and Identity Science (2021), 1–1.\n[196] K. Yang, Z. Xu, and J. Fei. 2021. DualSANet: Dual Spatial Attention Network for Iris Recognition. In IEEE Winter\nConference on Applications of Computer Vision (WACV). 888–896.\n[197] Y. Yuan, W. Chen, Y. Yang, and Z. Wang. 2020. In Defense of the Triplet Loss Again: Learning Robust Person\nRe-Identification with Fast Approximated Triplet Loss and Label Distillation. In IEEE Int. Conf. on Computer Vision\nand Pattern Recognition Workshops (CVPRW). 1454–1463.\n[198] L. A. Zanlorensi, H. Proença, and D. Menotti. 2020. Unconstrained Periocular Recognition: Using Generative Deep\nLearning Frameworks for Attribute Normalization. In Int. IEEE Int. Conf. on Image Processing. 1361–1365.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nDeep Learning for Iris Recognition: A Survey\n35\n[199] H. Zhang, Y. Bai, H. Zhang, J. Liu, X. Li, and Z. He. 2021. Local Attention and Global Representation Collaborating\nfor Fine-grained Classification. In Int. Conf. on Pattern Recognition (ICPR). 10658–10665.\n[200] H. Zhang, Z. Sun, and T. Tan. 2010. Contact Lens Detection Based on Weighted LBP. In Int. Conf. on Pattern Recognition\n(ICPR). 4279–4282.\n[201] Q. Zhang, H. Li, Z. He, and Z. Sun. 2016. Image Super-Resolution for Mobile Iris Recognition. In Biometric Recognition.\nSpringer, Cham, 399–406.\n[202] Q. Zhang, H. Li, Z. Sun, Z. He, and T. Tan. 2016. Exploring complementary features for iris recognition on mobile\ndevices. In IEEE Int. Conf. on Biometrics (ICB). 1–8.\n[203] Q. Zhang, H. Li, Z. Sun, and T. Tan. 2018. Deep Feature Fusion for Iris and Periocular Biometrics on Mobile Devices.\nIEEE Transactions on Information Forensics and Security 13, 11 (2018), 2897–2912.\n[204] Q. Zhang, H. Li, M. Zhang, Z. He, Z. Sun, and T. Tan. 2015. Fusion of Face and Iris Biometrics on Mobile Devices\nUsing Near-infrared Images. In Biometric Recognition. Springer, Cham, 569–578.\n[205] W. Zhang, X. Lu, Y. Gu, Y. Liu, X. Meng, and J. Li. 2019. A Robust Iris Segmentation Scheme Based on Improved\nU-Net. IEEE Access 7 (2019), 85082–85089.\n[206] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. 2017. Pyramid Scene Parsing Network. In IEEE Int. Conf. on Computer Vision\nand Pattern Recognition (CVPR). 6230–6239.\n[207] T. Zhao, Y. Liu, G. Huo, and X. Zhu. 2019. A Deep Learning Iris Recognition Method Based on Capsule Network\nArchitecture. IEEE Access 7 (2019), 49691–49701.\n[208] Z. Zhao and A. Kumar. 2017. Accurate Periocular Recognition Under Less Constrained Environment Using Semantics-\nAssisted CNN. IEEE Transactions on Information Forensics and Security 12, 5 (2017), 1017–1030.\n[209] Z. Zhao and A. Kumar. 2017. Towards More Accurate Iris Recognition Using Deeply Learned Spatially Corresponding\nFeatures. In IEEE Int. Conf. on Computer Vision (ICCV). 3829–3838.\n[210] Z. Zhao and A. Kumar. 2018. Improving Periocular Recognition by Explicit Attention to Critical Regions in Deep\nNeural Network. IEEE Transactions on Information Forensics and Security 13, 12 (2018), 2937–2952.\n[211] Z. Zhao and A. Kumar. 2019. A deep learning based unified framework to detect, segment and recognize irises using\nspatially corresponding features. Pattern Recognition 93 (2019), 546 – 557.\n[212] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. 2016. Learning Deep Features for Discriminative\nLocalization. In IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR). 2921–2929.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "published": "2022-10-12",
  "updated": "2022-10-12"
}