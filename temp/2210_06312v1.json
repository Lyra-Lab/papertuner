{
  "id": "http://arxiv.org/abs/2210.06312v1",
  "title": "Changing the Representation: Examining Language Representation for Neural Sign Language Production",
  "authors": [
    "Harry Walsh",
    "Ben Saunders",
    "Richard Bowden"
  ],
  "abstract": "Neural Sign Language Production (SLP) aims to automatically translate from\nspoken language sentences to sign language videos. Historically the SLP task\nhas been broken into two steps; Firstly, translating from a spoken language\nsentence to a gloss sequence and secondly, producing a sign language video\ngiven a sequence of glosses. In this paper we apply Natural Language Processing\ntechniques to the first step of the SLP pipeline. We use language models such\nas BERT and Word2Vec to create better sentence level embeddings, and apply\nseveral tokenization techniques, demonstrating how these improve performance on\nthe low resource translation task of Text to Gloss. We introduce Text to\nHamNoSys (T2H) translation, and show the advantages of using a phonetic\nrepresentation for sign language translation rather than a sign level gloss\nrepresentation. Furthermore, we use HamNoSys to extract the hand shape of a\nsign and use this as additional supervision during training, further increasing\nthe performance on T2H. Assembling best practise, we achieve a BLEU-4 score of\n26.99 on the MineDGS dataset and 25.09 on PHOENIX14T, two new state-of-the-art\nbaselines.",
  "text": "Changing the Representation: Examining Language Representation for\nNeural Sign Language Production\nHarry Walsh, Ben Saunders, Richard Bowden\nUniversity of Surrey\n{harry.walsh, b.saunders, r.bowden}@surrey.ac.uk\nAbstract\nNeural Sign Language Production (SLP) aims to automatically translate from spoken language sentences to sign language\nvideos. Historically the SLP task has been broken into two steps; Firstly, translating from a spoken language sentence to a gloss\nsequence and secondly, producing a sign language video given a sequence of glosses. In this paper we apply Natural Language\nProcessing techniques to the ﬁrst step of the SLP pipeline. We use language models such as BERT and Word2Vec to create\nbetter sentence level embeddings, and apply several tokenization techniques, demonstrating how these improve performance on\nthe low resource translation task of Text to Gloss. We introduce Text to HamNoSys (T2H) translation, and show the advantages\nof using a phonetic representation for sign language translation rather than a sign level gloss representation. Furthermore, we use\nHamNoSys to extract the hand shape of a sign and use this as additional supervision during training, further increasing the\nperformance on T2H. Assembling best practise, we achieve a BLEU-4 score of 26.99 on the MineDGS dataset and 25.09 on\nPHOENIX14T, two new state-of-the-art baselines.\nKeywords: Sign Language Translation (SLT), Natural Language Processing (NLP), Sign Language, Phonetic Repre-\nsentation\n1.\nIntroduction\nSign languages are the dominant form of communi-\ncation for Deaf communities, with 430 million users\nworldwide (WHO, 2021). Sign languages are complex\nmultichannel languages with their own grammatical\nstructure and vocabulary (Stokoe, 1980). For many\npeople, sign language is their primary language, and\nwritten forms of spoken language are their secondary\nlanguages.\nSign Language Production (SLP) aims to bridge the\ngap between hearing and Deaf communities, by trans-\nlating from spoken language sentences to sign language\nsequences. This problem has historically been broken\ninto two steps; 1) translation from spoken language to\ngloss1 and 2) subsequent production of sign language\nsequences from a sequence of glosses, commonly using\na graphical avatar (Elliott et al., 2008; Efthimiou et al.,\n2010; Efthimiou et al., 2009) or more recently, a photo-\nrealistic signer (Saunders et al., 2021a; Saunders et al.,\n2021b). In this paper, we improve the SLP pipeline by\nfocusing on the Text to Gloss (T2G) translation task of\nstep 1.\nModern deep learning is heavily dependent upon data.\nHowever, the creation of sign language datasets is both\ntime consuming and costly, restricting their size to or-\nders of magnitude smaller than their spoken language\ncounterparts. State-of-the-art datasets such as RWTH-\nPHOENIX-Weather-2014T (PHOENIX14T), and the\nnewer MineDGS (mDGS), contain only 8,257 and\n63,912 examples respectively (Koller et al., 2015;\nHanke et al., 2020), compared to over 15 million exam-\n1Gloss is the written word associated with a sign\nples for common spoken language datasets (Vrandeˇci´c\nand Kr¨otzsch, 2014). Hence, sign languages can be\nconsidered as low resource languages.\nIn this work, we take inspiration from NLP tech-\nniques to boost translation performance. We explore\nhow language can be modeled using different tokeniz-\ners, more speciﬁcally Byte Pair Encoding (BPE), Word-\nPiece, word and character level tokenizers. We show\nthat ﬁnding the correct tokenizer for the task helps sim-\nplify the translation problem.\nFurthermore, to help tackle our low resource language\ntask, we explore using pre-trained language models such\nas BERT (Devlin et al., 2018) and Word2Vec (Mikolov\net al., 2013b) to create improved sentence level em-\nbeddings. We also fuse contextual information from\nthe embedding to increase the amount of information\navailable to the network. We show that using models\ntrained on large corpuses of data improves translation\nperformance.\nPreviously the ﬁrst step of the SLP pipeline used T2G\ntranslation. We explore using a phonetic representation\nbased on the Hamburg Notation System (HamNoSys)\nwhich we deﬁne as Text to HamNoSys (T2H). Ham-\nNoSys encodes signs using a set of symbols and can be\nviewed as a phonetic representation of sign language\n(Hanke, 2004). There are three main components when\nrepresenting a sign in HamNoSys; a) its initial conﬁgu-\nration b) it’s hand shape and c) it’s action. An example\nof HamNoSys can be seen in Fig. 1 along with its gloss\nand text counterparts.\nWe evaluate our SLP models on both the mDGS and\nPHOENIX14T datasets, showing state-of-the-art per-\nformance on T2G (mDGS & PHX) and T2H (mDGS)\narXiv:2210.06312v1  [cs.CL]  16 Sep 2022\nFigure 1: A graph to show the word “running” which\nwould be ‘glossed’ as RUN and the associated sequence\nof HamNoSys, Top: Text, Middle: Gloss, Bottom: Ham-\nNoSys. HamNoSys is split into: a) it’s initial conﬁgura-\ntion b) it’s hand shape 3) it’s action\ntasks. We achieve a BLEU-4 score of 26.99 on mDGS,\na signiﬁcant increase compared to the state-of-the-art\nscore of 3.17 (Saunders et al., 2022).\nThe rest of this paper is structured as follows; In sec-\ntion 2 we review the related work in the ﬁeld. Section 3\npresents our methodology. Section 4 shows quantitative\nand qualitative results. Finally, we draw conclusions in\nsection 5 and suggest future work.\n2.\nRelated Work\nSign Language Recognition & Translation: Compu-\ntational sign language research has been studied for\nover 30 years (Tamura and Kawasaki, 1988). Research\nstarted with isolated Sign Language Recognition (SLR)\nwhere individual signs were classiﬁed using CNNs (Le-\ncun et al., 1998). Recently, the ﬁeld has moved to\nthe more challenging problem of Continuous Sign Lan-\nguage Recognition (CSLR), where a continuous sign\nlanguage video needs to be segmented and then clas-\nsiﬁed (Koller et al., 2015). Most modern approaches\nto SLR and CSLR rely on deep learning, but such ap-\nproaches are data hungry and therefore are limited by\nthe size of publicly available datasets.\nThe distinction between CSLR and Sign Language\nTranslation (SLT) was stressed by Camgoz et al. (2018).\nSLT aims to translate a continuous sequence of signs to\nspoken language sentences (Sign to Text (S2T)) or vice\nversa (Text to Sign (T2S)), a challenging problem due\nto the changes in grammar and sequence ordering.\nSign Language Production (SLP): focusses on T2S,\nthe production of a continuous sign language sequence\ngiven a spoken language input sentence. Current state-\nof-the-art approaches to SLP use transformer based ar-\nchitectures with attention (Stoll et al., 2018; Saunders\net al., 2020). In this paper, we tackle the SLP task of\nneural sign language translation, deﬁned as T2G or T2H\ntranslation.\nHamNoSys has been used before for statistical SLP,\nwith some success (Kaur and Kumar, 2014; Kaur and\nKumar, 2016). However, the produced motion becomes\nrobotic and is not practical for real world applications.\nNote that these approaches ﬁrst convert the HamNoSys\nto SiGML, an XML format of HamNoSys (Kaur and\nKumar, 2016).\nNeural Machine Translation (NMT): NMT aims to\ngenerate a target sequence given a source sequence us-\ning neural networks (Bahdanau et al., 2014) and is com-\nmonly used for spoken language translations. Initial\napproaches used recurrence to map a hidden state to\nan output sequence (Kalchbrenner and Blunsom, 2013),\nwith limited performance. Encoder-decoder structures\nwere later introduced, that map an input sequence to an\nembedding space (Wu et al., 2016). To address the bot-\ntleneck problem, attention was introduced to measure\nthe afﬁnity between sections of the input and embed-\nding space and allow the model to focus on speciﬁc\ncontext (Bahdanau et al., 2014). This was improved fur-\nther with the introduction of the transformer (Vaswani\net al., 2017) that used Multi-Headed Attention (MHA)\nto allow multiple projections of the learned attention.\nMore recently, model sizes have grown with architec-\ntures introduced such as GPT-2 (Radford et al., 2019)\nand BERT (Devlin et al., 2018).\nDifferent encoding/decoding schemes have been ex-\nplored.\nBPE was ﬁrst introduced in Sennrich et al.\n(2015), to create a set of tokens given a set vocabulary\nsize. This is achieved by merging the most commonly\noccurring sequential characters. WordPiece, a similar\ntokenizer to BPE, was ﬁrst introduced in Schuster and\nNakajima (2012) and is commonly used when training\nlanguage models such as BERT, DistilBERT and Elec-\ntra. Finally, word and character level tokenizers break\nup a sentence based on white space and unique symbols\nrespectively.\nNatural Language Processing: NLP has many appli-\ncations, for example Text Simpliﬁcation, Text Classiﬁca-\ntion, and Speech Recognition. Recently, deep learning\napproaches have outperformed older statistical meth-\nods (Vaswani et al., 2017). A successful NLP model\nmust understand the structure and context of language,\nlearned via supervised or unsupervised methods. Pre-\ntrained language models have been used to boost perfor-\nmance in other NLP tasks (Clinchant et al., 2019; Zhu et\nal., 2020), such as BERT (Devlin et al., 2018) achieving\nstate-of-the-art performance. Zhu et al., 2020 tried to\nfuse the embedding of BERT into a traditional trans-\nformer architecture using attention, increasing the trans-\nlation performance by approximately 2 BLEU score.\nOther methods have used Word2Vec to model lan-\nguage, this has been applied to many NLP tasks\n(Mikolov et al., 2013b). Word2Vec is designed to give\nmeaning to a numerical representation of words. The\ncentral idea being that words with similar meaning\nshould have a small euclidean distance between the\nvector representation.\nIn this paper, we take inspiration from these tech-\nniques to boost performance of the low resource task of\nT2G and T2H sign language production.\n3.\nMethodology\nThe task of neural sign language production aims to\nmap a source sequence of spoken language sentences,\nx = (x1, x2, ..., xW ) with W words, to a sequence\nof glosses, y = (y1, y2, ..., yG) with G glosses (Text\nto Gloss (T2G)), or a sequence of HamNoSys, z =\n(z1, z2, ..., zH) with H symbols (Text to HamNoSys\n(T2H)). T2G and T2H tasks thus learn the conditional\nprobabilities p(y|x) and p(z|x) respectively. Sign lan-\nguage translation is not a one to one mapping as sev-\neral words can be mapped to a single gloss (W > G),\n(W > H). This increases the complexity of the prob-\nlem as the model must learn to attend to multiple words\nin the input sequence.\nFig. 2 shows the general architecture of our\nmodel used to translate from spoken language to\ngloss/HamNoSys. For means of comparison, our base-\nline model is an encoder-decoder transformer with\nMHA. The input and output sequence are tokenized\nusing a word level tokenizer and the embedding for a\ngiven sequence is created using a single linear layer. We\nlater build on this base model using different tokenizers,\nembedding and supervision techniques. We train our\nmodel using a cross-entropy loss between the predicted\ntarget sequence, ˆx and the ground truth sequence, x∗,\ndeﬁned as LT .\nFigure 2: An overview of the different conﬁguration of\nour architecture for SLT\nIn this section, we follow the structure of Fig. 2 from top\nto bottom. We start by describing the different tokeniz-\ners used to split the source text and produce tokens (Sec.\n3.1). Next, we explain the different embedding tech-\nniques used to create a vector from the input tokens (Sec.\n3.2). Finally, we talk about the advantages of using ex-\ntra supervision and explain how this is implemented in\nconjunction with the translation loss.\n3.1.\nTokenizers\nSeveral tokenizer schemes can be used on both the input\nand output such as BPE, Word, character and Word-\nPiece. BPE (Sennrich et al., 2015), character and Word-\nPiece (Schuster and Nakajima, 2012) all change the\nvocabulary size of the model by breaking sentences\ninto sub-units. This reduces the number of singletons\nand reduces lexical inﬂections in the input and output\nsequences (Wolf et al., 2019).\nWord\nA word level tokenizer segments the input sen-\ntence based on white space. Therefore, a normal sen-\ntence is split into whole words.\nCharacter\nA character level tokenizer segments the\ntext based on the individual symbols, reducing the vo-\ncabulary to simply the alphabet plus punctuation.\nBPE\nBPE creates a base vocabulary containing all\nthe unique symbols in the data, from which it learns a\nnumber of merge rules based on the most commonly oc-\ncurring sequential symbols. An example of the BPE al-\ngorithm being applied to HamNoSys is shown in Fig. 3,\nwith the coloured boxes indicating what merges are\nmade at each step. Merging continues until a speciﬁc\nvocabulary size is reached. This helps reduce word in-\nﬂections e.g. the words low, lowest and lower can be\nsegmented to low, est and er. Over the whole corpus\nthe sufﬁx’s (est and er) can be reused, collapsing the\nvocabulary in this example from 3 to 1.\nFigure 3: An example of how BPE can be applied to\nHamNoSys.\nWordPiece\nWe only apply a WordPiece tokenizer\nwhen embedding with BERT, as this is what the BERT\nmodel was trained with. WordPiece is another sub-unit\ntokenization algorithm similar to BPE that evaluates the\nlost beneﬁt before merging two symbols, ensuring that\nall mergers are beneﬁcial.\n3.2.\nEmbedding\nAfter tokenization, the input sequence x is then em-\nbedded by projecting the sequence into a continuous\nspace (Mikolov et al., 2013a). The goal of embedding\nis to minimise the Euclidean distance between words\nwith similar meanings. The most common embedding\nis a single linear layer, which takes an input sequence\nx = (x1, x2, ..., xW ) with W words and turns it into\na matrix of [W × E] where E is the models embed-\nding width. In models such as BERT and Word2Vec,\nembeddings are learnt via training on a large corpus of\nspoken language data. To maximise the beneﬁt from\nusing BERT we ﬁne tune the pre-trained model on the\nmineDGS dataset using masked-language modeling.\nWhen using a BERT model, we deﬁne the transfor-\nmation as follows. Given an input sequence x we ﬁrst\napply WordPiece tokenization.\nXW P = WordPiece(x)\n(1)\nThen apply the BERT embeddings as:\nXBERT = BERT(XW P )\n(2)\nNote that we take the embedding from the last layer of\nBERT. We deﬁne the Word2Vec transformation as:\nXW 2V = Word2V ec(x)\n(3)\nAdditionally, we experiment with concatenating or\nfusing contextual information into the input x. We de-\nﬁne the contextual information as xave and the scaling\nfactor as S, used to place additional emphasis on the con-\ntextual information. In the case of Word2Vec we take a\nmean average of each word’s embedding in the sentence\nand treat this as a vector that contains information about\nthe whole sentence. For BERT we use the embedding\nof the classiﬁcation token ([CLS]), which contains con-\ntextual information about the sentence (Devlin et al.,\n2019). We either concatenate the information to the\nbeginning of a sequence x = (xave ∗S, x1, x2, ..., xW )\n(CON), or we fuse it into each step of the sequence x =\n((xave ∗S) + x1, (xave ∗S) + x2, ..., (xave ∗S) + xW )\n(ADD).\n3.3.\nSupervision\nIn sign language, there exists a strong correlation be-\ntween hand shape and meaning (Stokoe, 1980). There-\nfore, we investigate forcing the transformer to predict\nthe hand shape alongside the gloss or HamNoSys se-\nquences, to enrich the learnt representation. We scale\nthe loss from the hand shape prediction LH by factor F.\nWe combine both losses from the translation LT and\nhand shape prediction LH to create Ltotal as:\nLtotal = LT + (LH ∗F)\n(4)\nIn this setup, the model learns the joint conditional\nprobability of\np(y|x) ∗p(H|x)\n(5)\nwhere H is the sequence of hand shape symbols:\nH = (h1, h2, ..., hG)\n(6)\nand G is the number of glosses in the sequence. Overall\nthis forces that model to focus on hand shape during\ntraining. We show that by forcing the model to predict\nhand shape we improve the performance on T2H.\n4.\nExperiments\nIn this section we test the translation performance of\nour models in both the T2G and T2H setups. We ﬁrst\nexplain the experimental setup of our models. Next,\nwe compare quantitative results against previous state-\nof-the-art and our own baselines. Finally we provide\nqualitative results.\n4.1.\nExperimental Setup\nWhen training our T2G model, we experiment with dif-\nferent embedding sizes, number of layers and heads. We\nobserve a large change in performance based on these\nthree parameters, and search for the best conﬁgurations\nfor further tests. Our transformer uses a xavier initial-\nizer (Glorot and Bengio, 2010) with zero bias and Adam\noptimization (Kingma and Ba, 2014) with a learning\nrate of 10−4. We also employ dropout connections with\na probability of 0.2 (Srivastava et al., 2014). When\ndecoding, we use a beam search with a search size of 5.\nOur code base comes from Kreutzer et al. (2019)\nNMT toolkit, JoeyNMT (Kreutzer et al., 2019) and is\nimplemented using Pytorch. While our BPE and word\npiece tokenizers come from Huggingface’s python li-\nbrary transformers (Wolf et al., 2019). When embed-\nding with BERT, we use an open source pre-trained\nmodel from Deepset (Chan et al., 2020). Finally we\nused fasttext’s implementation of Word2Vec for word\nlevel embedding (Mikolov et al., 2013b).\nThe publicly available mDGS dataset contains\naligned spoken German sentences and their gloss\ncounter parts, from unconstrained dialogue between\ntwo native deaf signers (Kaur and Kumar, 2014). The\nproviders of this dataset also have a dictionary for all\nglosses in the corpus, of which some contain HamNoSys\ndescriptions. Following the translation protocols set in\nSaunders et al. (2022), we created a subset of the mDGS\ndataset with aligned sentences, glosses and HamNoSys.\nmDGS is a larger dataset compared to PHOENIX14T\n(7.5 times more parallel examples, with a source vocab-\nulary of 18,457) with 330 deaf participants performing\nfree form signing. The size of mDGS overcomes some\nof the limitation of PHOENIX2014T. Note we remove\nthe gloss variant numbers to reduce singletons.\nWe use the PHOENIX14T (Camgoz et al., 2018)\ndataset to compare our best model to previous NMT\nbaseline results (Saunders et al., 2020; Stoll et al., 2018;\nMoryossef et al., 2021; Li et al., 2021). PHOENIX14T\ncontains parallel monolingual German data, with ap-\nproximately 7000 examples of aligned gloss and text.\n4.2.\nQuantitative Evaluation\nIn this section, we evaluate our models on both mDGS\nand PHOENIX14T using BLEU (BLEU-1,2,3 and 4)\nand Rouge (F1-score) scores for both dev and test sets.\nWe group our experiments in ﬁve sections:\n1. Baseline T2G, T2H and Text to Gloss to Ham-\nNoSys (T2G2H) with a standard transformer.\n2. T2G and T2H with different embedding layers and\nsentence averaging.\n3. T2G and T2H with different tokenizers (BPE,\nWord, and Character).\n4. T2G and T2H with additional supervision.\n5. Comparison of our approach on PHOENIX14T\nand mDGS.\nDEV SET\nTEST SET\nApproach:\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nLinear Layer\n16.26\n24.14\n32.83\n43.05\n42.02\n16.47\n24.51\n33.27\n43.58\n41.53\nBERT\n14.69\n21.51\n29.39\n38.66\n30.87\n14.2\n21.19\n29.09\n38.33\n30.31\nBERT SA ADD\n13.23\n19.41\n26.43\n34.75\n32.38\n13.43\n19.47\n26.31\n34.3\n32.34\nBERT SA CON\n14.89\n21.45\n28.73\n36.85\n34.73\n15.14\n21.57\n28.79\n36.91\n34.44\nWord2Vec\n11.47\n17.59\n24.68\n34.21\n29.45\n11.73\n17.83\n25.14\n34.90\n30.22\nWord2Vec SA ADD\n13.8\n21.07\n29.72\n42.29\n30.65\n13.31\n20.56\n29.31\n42.13\n30.67\nWord2Vec SA CON\n0.03\n0.05\n0.06\n0.04\n9.44\n0.03\n0.06\n0.06\n0.04\n9.32\n(a) MineDGS (mDGS) on Text to Gloss to HamNoSys (T2G2H)\nDEV SET\nTEST SET\nApproach:\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nLinear Layer\n14.46\n23.27\n32.62\n47.44\n50.85\n14.80\n23.54\n32.89\n47.36\n50.87\nBERT\n20.26\n29.14\n38.01\n48.92\n53.67\n21.03\n29.87\n38.79\n49.77\n53.93\nBERT SA ADD\n14.64\n22.33\n30.91\n43.99\n50.30\n15.16\n22.92\n31.41\n44.21\n50.33\nBERT SA CON\n11.82\n19.2\n27.39\n40.58\n53.36\n12.21\n19.39\n27.44\n40.48\n53.67\nWord2Vec\n16.43\n24.77\n33.71\n46.62\n51.14\n17.09\n25.23\n34.22\n47.31\n51.52\nWord2Vec SA ADD\n16.72\n25.14\n34.39\n48.00\n51.28\n16.98\n25.31\n34.59\n48.08\n51.12\nWord2Vec SA CON\n14.98\n22.49\n30.65\n42.42\n51.11\n15.18\n22.65\n30.80\n42.75\n50.10\n(b) MineDGS (mDGS) on Text to HamNoSys (T2H)\nTable 1: Embedding transformer results for Text to Gloss (T2G) and Text to HamNoSys (T2H) translation.\nTokenizer\nDEV SET\nTEST SET\nInput\nOutput\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nWord\nWord\n16.47\n24.35\n33.06\n43.41\n36.32\n16.55\n24.45\n33.14\n43.54\n36.34\nWord\nBPE\n22.06\n28.53\n36.32\n47.55\n36.20\n21.87\n28.31\n36.02\n47.08\n35.74\nWord\nChar\n16.47\n24.35\n33.06\n43.41\n36.32\n16.55\n24.45\n33.14\n43.54\n36.34\nBPE\nWord\n20.84\n26.77\n34.02\n44.77\n35.31\n20.84\n26.80\n34.12\n44.97\n35.35\nBPE\nBPE\n21.39\n27.28\n34.31\n43.86\n36.61\n21.28\n27.25\n34.34\n43.86\n36.86\nBPE\nChar\n1.99\n5.5\n10.35\n30.01\n2.61\n1.46\n5.18\n10.0\n29.77\n2.61\n(a) MineDGS (mDGS) on Text to Gloss to HamNoSys (T2G2H)\nTokenizer\nDEV SET\nTEST SET\nInput\nOutput\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nWord\nWord\n21.81\n31.86\n42.05\n54.88\n55.39\n21.89\n31.92\n42.16\n55.04\n55.23\nWord\nBPE\n25.41\n29.28\n34.11\n41.25\n48.03\n25.54\n29.39\n34.25\n41.35\n48.09\nWord\nChar\n21.63\n31.76\n41.99\n54.94\n55.3\n21.59\n31.79\n42.09\n55.01\n55.14\nBPE\nWord\n20.98\n30.29\n39.38\n50.04\n55.24\n21.18\n30.37\n39.4\n49.84\n55.04\nBPE\nBPE\n26.14\n30.83\n36.47\n44.35\n49.95\n26.21\n30.84\n36.43\n44.14\n50.05\nBPE\nChar\n1.91\n6.01\n11.72\n37.56\n37.22\n1.92\n5.88\n11.59\n37.63\n37.31\n(b) MineDGS (mDGS) on Text to HamNoSys (T2H)\nTable 2: Tokenizer transformer results for Text to Gloss (T2G) and Text to HamNoSys (T2H) translation.\nNote we expect the performance to be lower than 100\nBLEU. As this is a translation problem there are several\nvalid answers for a given input, thus human evaluation\nis still necessary. We are also unable to provide T2H re-\nsults on PHOENIX14T, as HamNoSys is not available\nfor some words in its vocabulary.\n4.2.1.\nBaseline Results\nOur baseline models achieved a BLEU-4 score of 2.86\n(T2G), 16.26 (T2G2H) and 14.46 (T2H) on the mDGS\ndev set. Our baseline setup uses a word level tokenizer\non both the input and output, providing a baseline to\nablate our proposed techniques in the next three sec-\ntions. We perform a hyper-parameter search and make\nmodiﬁcation to the model architecture (number of heads,\nlayers and embedding size) to ﬁnd the best performance.\nIn general, a sequence of HamNoSys is signiﬁcantly\nlonger than it’s gloss counter part, (H >> G). As a\nresult our T2H performance is artiﬁcially higher than\nour T2G. Therefore, in order to make our T2G and T2H\nresults comparable, we perform a dictionary lookup to\nconvert the gloss to HamNoSys (T2G2H) before calcu-\nlating the BLEU score.\nGiven these results, we conclude a transformer architec-\nture is the best baseline approach and continue with this\nsetup for all future experiments.\n4.2.2.\nEmbedding\nNext we experiment with using different embedding\ntechniques for the T2G and T2H tasks. As discussed in\nSection 3.1 we use a linear layer, BERT and Word2Vec\nin combination with sentence averaging. From the re-\nsults in Table 1 we make several observations. Firstly,\nusing a language model improves the translation perfor-\nmance on the T2H task (Tab. 1a). While on the T2G\ntask, using language models was detrimental to the trans-\nlation performance (Tab. 1b). We assume this is due to\nthe reduced information within the gloss and smaller se-\nquence length. Secondly, we observe that applying sen-\ntence averaging to the BERT embedding has a negative\neffect on the scores, independent of what type of average\nwas used (adding or concatenating). On the other hand,\nadding the sentence averaging to the Word2Vec embed-\nding marginally improved performance compared to the\nDEV SET\nTEST SET\nApproach:\nSupervision\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nT2G2H\n\u0017\n22.06\n28.53\n36.32\n47.55\n36.20\n21.87\n28.31\n36.02\n47.08\n35.74\nT2G2H\n\u0013\n21.79\n27.98\n35.45\n46.21\n35.79\n21.49\n27.76\n35.27\n46.11\n35.99\nT2H\n\u0017\n26.14\n30.83\n36.47\n44.35\n49.95\n26.21\n30.84\n36.43\n44.14\n50.05\nT2H\n\u0013\n26.99\n31.07\n35.99\n42.73\n48.89\n27.37\n31.42\n36.3\n42.92\n48.85\nTable 3: HamNoSys hand shape supervision results for Text to Gloss to HamNoSys (T2G2H) and Text to HamNoSys\n(T2H) translation.\nstand alone Word2Vec embeddings on T2H. But note\nthat Word2Vec plus sentence averaging still has lower\nperformance than just using a linear layer. Overall, we\nﬁnd the best performing embedding to come from using\nBERT, which scored 5.8 BLEU-4 higher than using a\nlinear layer. This demonstrates that using a pre-trained\nlanguage model can enhance translation.\n4.2.3.\nTokenizer\nWe next experiment with using different tokenizers, as\ndescribed in Section 3.2. We performed a parameter\nsearch to ﬁnd the best vocabulary size for the BPE algo-\nrithm, which we ﬁnd to be 2250 and 7000 on the input\nand output respectively. The result of our experiments\nare shown in Table 2.\nWhen using a character level tokenizer each input\ncontains a minimal amount of information (one letter).\nAs expected this increases the difﬁculty of the problem,\nand reduces performance. When applied to the input\nit was extremely detrimental for the performance on\nboth T2G2H and T2H, independent of which output\ntokenizer was used. Therefore to save space, we do\nnot present the input character level results. Using a\nword level tokenizer achieved very reasonable results,\nsupporting our theory that using larger units of language\nthat contains more information is beneﬁcial for transla-\ntion. But as BPE outperformed the word level tokenizer\non the BLEU-4 score, we assume that by using whole\nwords we create a harder problem, as the dataset con-\ntains several word inﬂections. We conclude that BPE\nis the best algorithm to use when translating from T2H.\nThis is due to the algorithms ability to reduce inﬂec-\ntions and reduce the vocabulary size which simpliﬁes\nthe networks task. Our results also show that the biggest\nimpact comes from having BPE on the output, suggest-\ning that most of the challenge comes from the decoding\nsection of the network. Similarly, the best T2G result\ncame from using a word level and BPE tokenizers on\nthe input and output respectively.\n4.2.4.\nSupervision\nOur ﬁnal ablation study investigates an additional loss\nexplained in Section 3.3. This had a positive effect\non the translation performance for T2H. As can be\nseen from Table 3, the use of supervision increased the\nBLEU-4 scores by 0.85. We conclude supervision en-\nriches the learnt sign language representation due to\nthe correlation between hand shape and context. Super-\nvision forces the model to focus more on hand shape,\nallowing the model to group signs and ﬁnd better trends\nin the data. Although the use of supervision marginally\ndecreased the T2G2H BLEU score, we suggest this is\ndue to reduced information in the target gloss.\n4.3.\nState-of-the-art Comparisons\nFinally, in Table 4 (PHOENIX14T) and 5 (mDGS) we\ncompare our best performing models to state-of-the-\nart work. Note in Table 4 our baseline is marginally\nhigher than (Saunders et al., 2020), we assume this\nis due to a larger hyper-parameter search. On both\ndatasets, our best model for T2G and T2G2H uses a\nword level and BPE tokenizer on the input and output\nrespectively. While our best T2H result comes from\nadding additional supervision on to this setup. As can\nbe seen from Table 4 and 5 our models outperformed all\nother methods (Moryossef et al., 2021; Li et al., 2021;\nSaunders et al., 2020; Saunders et al., 2022; Stoll et al.,\n2018), setting a new state-of-the-art on PHOENIX14T\nand mDGS. Note we can only compare scores that\nare publicly available, therefore ’-’ denotes where the\nauthors did not provide results.\n4.4.\nQualitative Evaluation\nFor qualitative evaluation, we share translation exam-\nples from our best models and our baseline model in\nFig. 4, to allow the reader to better interpret the results.\nNote, we add a vertical black line after each word of\nHamNoSys to mark the end of a given sign. These\nresults show how our BPE model has learnt richer trans-\nlations than our baseline model.\nFigure 4: Translation examples from our baseline and\nbest model.\nDEV SET\nTEST SET\nApproach:\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nT2G (Stoll et al., 2018)\n16.34\n22.30\n32.47\n50.15\n48.42\n15.26\n21.54\n32.25\n50.67\n48.10\nT2G (Saunders et al., 2020)\n20.23\n27.36\n38.21\n55.65\n55.41\n19.10\n26.24\n37.10\n55.18\n54.55\nT2G (Li et al., 2021)\n18.89\n25.51\n-\n-\n49.91\n-\n-\n-\n-\n-\nT2G (Moryossef et al., 2021)\n23.17\n-\n-\n-\n-\n-\n-\n-\n-\n-\nT2G Baseline (ours)\n22.47\n30.03\n41.54\n58.98\n57.96\n20.95\n28.50\n39.99\n58.32\n57.28\nT2G Best Model (ours)\n25.09\n32.18\n42.85\n60.04\n58.82\n23.19\n30.24\n40.86\n58.74\n56.55\nTable 4: Baseline comparison results for Text to Gloss (T2G) translation on PHOENIX14T.\nDEV SET\nTEST SET\nApproach:\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nBLEU-4\nBLEU-3\nBLEU-2\nBLEU-1\nROUGE\nT2G (Saunders et al., 2022)\n3.17\n-\n-\n-\n32.93\n3.08\n-\n-\n-\n32.52\nT2G Our best\n10.5\n14.35\n20.43\n33.56\n35.79\n10.4\n14.21\n20.2\n33.59\n35.99\nT2G2H Our best\n22.06\n28.53\n36.32\n47.55\n36.20\n21.87\n28.31\n36.02\n47.08\n35.74\nT2H Our best\n26.99\n31.07\n35.99\n42.73\n48.89\n27.37\n31.42\n36.3\n42.92\n48.85\nTable 5: Baseline comparison results for Text to Gloss (T2G), Text to Gloss to HamNoSys (T2G2H) and Text to\nHamNoSys (T2H) translation on mDGS.\n5.\nConclusion\nIn this paper, we employed a transformer to translate\nfrom spoken language sentences to a sequence of gloss\nor HamNoSys. We introduced T2H translation, showing\nthe advantages of translating to HamNoSys instead of\njust gloss, and set baseline results for future work on\nmDGS. We showed that language models can be used\nto improve translation performance, but using more ad-\nvanced tokenization algorithms like BPE gives a larger\nperformance gain. Additionally, we have shown that\ntranslation can be improved by training the model to\njointly predict hand shape and HamNoSys. We achieved\na BLEU-4 score of 26.99 and 25.09, a new state-of-the-\narts for SLT on the mDGS and PHOENIX14T datasets.\nAs future work, it would be interesting to create a rep-\nresentation, gloss++. This could combine the beneﬁts of\ngloss and HamNoSys, including non-manual features as\nwell as hand shape information, as this has been shown\nto be useful for translation. Furthermore, this could be\nbeneﬁcial for down stream tasks in the SLP pipeline.\n6.\nAcknowledgements\nWe thank Adam Munder, Mariam Rahmani and Marina\nLovell from OmniBridge, an Intel Venture, for support-\ning this project. We also thank Thomas Hanke and Uni-\nversity of Hamburg for use of the mDGS data. We also\nthank the SNSF Sinergia project ‘SMILE II’ (CRSII5\n193686), the European Union’s Horizon2020 research\nproject EASIER (101016982) and the EPSRC project\n‘ExTOL’ (EP/R03298X/1). This work reﬂects only the\nauthors view and the Commission is not responsible for\nany use that may be made of the information it contains.\n7.\nBibliographical References\nBahdanau, D., Cho, K., and Bengio, Y. (2014). Neural\nmachine translation by jointly learning to align and\ntranslate. arXiv preprint arXiv:1409.0473.\nCamgoz, N. C., Hadﬁeld, S., Koller, O., Ney, H., and\nBowden, R. (2018). Neural sign language translation.\nIn Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition.\nChan, B., Schweter, S., and M¨oller, T. (2020). Ger-\nman’s next language model. In Proceedings of the\n28th International Conference on Computational Lin-\nguistics.\nClinchant, S., Jung, K. W., and Nikoulina, V. (2019).\nOn the use of bert for neural machine translation.\narXiv preprint arXiv:1909.12744.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova,\nK.\n(2018).\nBert:\nPre-training of deep bidirec-\ntional transformers for language understanding.\narXiv:1810.04805.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n(2019). BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceed-\nings of the 2019 Conference of the North.\nEfthimiou, E., Fotinea, S.-E., Vogler, C., Hanke, T.,\nGlauert, J., Bowden, R., Braffort, A., Collet, C., Mara-\ngos, P., and Segouat, J. (2009). Sign language recog-\nnition, generation, and modelling: A research effort\nwith applications in deaf communication. In Inter-\nnational Conference on Universal Access in Human-\nComputer Interaction.\nEfthimiou, E., Fontinea, S.-E., Hanke, T., Glauert, J.,\nBowden, R., Braffort, A., Collet, C., Maragos, P.,\nand Goudenove, F. (2010). Dicta-sign–sign language\nrecognition, generation and modelling: A research\neffort with applications in deaf communication. In\nProceedings of the 4th Workshop on the Representa-\ntion and Processing of Sign Languages: Corpora and\nSign Language Technologies.\nElliott, R., Glauert, J. R., Kennaway, J., Marshall, I., and\nSafar, E. (2008). Linguistic modelling and language-\nprocessing technologies for avatar-based sign lan-\nguage presentation. Universal Access in the Informa-\ntion Society.\nGlorot, X. and Bengio, Y. (2010). Understanding the\ndifﬁculty of training deep feedforward neural net-\nworks. In Proceedings of the 13th International Con-\nference on Artiﬁcial Intelligence and Statistics.\nHanke, T. (2004). Hamnosys-representing sign lan-\nguage data in language resources and language pro-\ncessing contexts. In LREC.\nKalchbrenner, N. and Blunsom, P. (2013). Recurrent\ncontinuous translation models. In Proceedings of the\n2013 Conference on Empirical Methods in Natural\nLanguage Processing.\nKaur, R. and Kumar, P. (2014). Hamnosys generation\nsystem for sign language. In 2014 International Con-\nference on Advances in Computing, Communications\nand Informatics (ICACCI).\nKaur, K. and Kumar, P. (2016). Hamnosys to sigml con-\nversion system for sign language automation. Proce-\ndia Computer Science.\nKingma, D. P. and Ba, J. (2014). Adam: A method for\nstochastic optimization. arXiv:1412.6980.\nKreutzer, J., Bastings, J., and Riezler, S.\n(2019).\nJoey nmt: A minimalist nmt toolkit for novices.\narXiv:1907.12484.\nLecun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998).\nGradient-based learning applied to document recog-\nnition. Proceedings of the IEEE.\nLi, D., Xu, C., Liu, L., Zhong, Y., Wang, R., Peters-\nson, L., and Li, H. (2021). Transcribing natural\nlanguages for the deaf via neural editing programs.\narXiv preprint arXiv:2112.09600.\nMikolov, T., Chen, K., Corrado, G., and Dean, J.\n(2013a). Efﬁcient estimation of word representations\nin vector space. arXiv preprint arXiv:1301.3781.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S.,\nand Dean, J. (2013b). Distributed representations of\nwords and phrases and their compositionality. Ad-\nvances in Neural Information Processing Systems.\nMoryossef, A., Yin, K., Neubig, G., and Goldberg, Y.\n(2021). Data augmentation for sign language gloss\ntranslation. arXiv:2105.07476.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. (2019). Language models are\nunsupervised multitask learners. OpenAI blog.\nSaunders, B., Camgoz, N. C., and Bowden, R. (2020).\nProgressive transformers for end-to-end sign lan-\nguage production. In European Conference on Com-\nputer Vision.\nSaunders, B., Camgoz, N. C., and Bowden, R. (2021a).\nAnonysign: Novel human appearance synthesis for\nsign language video anonymisation. In 2021 16th\nIEEE International Conference on Automatic Face\nand Gesture Recognition (FG 2021).\nSaunders, B., Camgoz, N. C., and Bowden, R. (2021b).\nMixed signals: Sign language production via a mix-\nture of motion primitives. In Proceedings of the\nIEEE/CVF International Conference on Computer\nVision.\nSaunders, B., Camgoz, N. C., and Bowden, R. (2022).\nSigning at Scale: Learning to Co-Articulate Signs\nfor Large-Scale Photo-Realistic Sign Language Pro-\nduction. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR).\nSchuster, M. and Nakajima, K. (2012). Japanese and\nkorean voice search. In 2012 IEEE International Con-\nference on Acoustics, Speech and Signal processing\n(ICASSP).\nSennrich, R., Haddow, B., and Birch, A. (2015). Neural\nmachine translation of rare words with subword units.\nCoRR.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever,\nI., and Salakhutdinov, R. (2014). Dropout: A simple\nway to prevent neural networks from overﬁtting. The\nJournal of Machine Learning Research.\nStokoe, W. C. (1980). Sign language structure. Annual\nReview of Anthropology.\nStoll, S., Camg¨oz, N. C., Hadﬁeld, S., and Bowden, R.\n(2018). Sign language production using neural ma-\nchine translation and generative adversarial networks.\nIn Proceedings of the 29th British Machine Vision\nConference.\nTamura, S. and Kawasaki, S. (1988). Recognition of\nsign language motion images. Pattern Recognition.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\nJones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin,\nI. (2017). Attention is all you need. Advances in\nNeural Information Processing Systems.\nVrandeˇci´c, D. and Kr¨otzsch, M. (2014). Wikidata: a\nfree collaborative knowledgebase. Communications\nof the ACM.\nWHO. (2021). Deafness and hearing loss.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue,\nC., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz,\nM., et al. (2019). Huggingface’s transformers: State-\nof-the-art natural language processing. arXiv preprint\narXiv:1910.03771.\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi,\nM., Macherey, W., Krikun, M., Cao, Y., Gao, Q.,\nMacherey, K., et al. (2016). Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation.\narXiv preprint\narXiv:1609.08144.\nZhu, J., Xia, Y., Wu, L., He, D., Qin, T., Zhou,\nW., Li, H., and Liu, T.-Y. (2020). Incorporating\nbert into neural machine translation. arXiv preprint\narXiv:2002.06823.\n8.\nLanguage Resource References\nHanke, T., Schulder, M., Konrad, R., and Jahn, E.\n(2020). Extending the Public DGS Corpus in size and\ndepth. In Eleni Efthimiou, et al., editors, Proceedings\nof the LREC2020 9th Workshop on the Representa-\ntion and Processing of Sign Languages, pages 75–82,\nMarseille, France, May. ELRA.\nKoller, O., Forster, J., and Ney, H. (2015). Continuous\nsign language recognition: Towards large vocabu-\nlary statistical recognition systems handling multiple\nsigners. Computer Vision and Image Understanding,\n141:108–125. Pose & Gesture.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "68T50 (Primary)"
  ],
  "published": "2022-09-16",
  "updated": "2022-09-16"
}