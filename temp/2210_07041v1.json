{
  "id": "http://arxiv.org/abs/2210.07041v1",
  "title": "Spontaneous Emerging Preference in Two-tower Language Model",
  "authors": [
    "Zhengqi He",
    "Taro Toyoizumi"
  ],
  "abstract": "The ever-growing size of the foundation language model has brought\nsignificant performance gains in various types of downstream tasks. With the\nexistence of side-effects brought about by the large size of the foundation\nlanguage model such as deployment cost, availability issues, and environmental\ncost, there is some interest in exploring other possible directions, such as a\ndivide-and-conquer scheme. In this paper, we are asking a basic question: are\nlanguage processes naturally dividable? We study this problem with a simple\ntwo-tower language model setting, where two language models with identical\nconfigurations are trained side-by-side cooperatively. With this setting, we\ndiscover the spontaneous emerging preference phenomenon, where some of the\ntokens are consistently better predicted by one tower while others by another\ntower. This phenomenon is qualitatively stable, regardless of model\nconfiguration and type, suggesting this as an intrinsic property of natural\nlanguage. This study suggests that interesting properties of natural language\nare still waiting to be discovered, which may aid the future development of\nnatural language processing techniques.",
  "text": "Spontaneous Emerging Preference in Two-tower\nLanguage Model\nZhengqi He\nLab for Neural Computation and Adaptation, RIKEN Center for Brain Science, Japan\nzhengqi.he@riken.jp\nTaro Toyoizumi\nLab for Neural Computation and Adaptation, RIKEN Center for Brain Science, Japan\nDepartment of Mathematical Informatics, Graduate School of Information Science\nand Technology, The University of Tokyo\ntaro.toyoizumi@riken.jp\nAbstract\nThe ever-growing size of the foundation language model has brought signiﬁcant\nperformance gains in various types of downstream tasks. With the existence of\nside-effects brought about by the large size of the foundation language model\nsuch as deployment cost, availability issues, and environmental cost, there is some\ninterest in exploring other possible directions, such as a divide-and-conquer scheme.\nIn this paper, we are asking a basic question: are language processes naturally\ndividable? We study this problem with a simple two-tower language model setting,\nwhere two language models with identical conﬁgurations are trained side-by-side\ncooperatively. With this setting, we discover the spontaneous emerging preference\nphenomenon, where some of the tokens are consistently better predicted by one\ntower while others by another tower. This phenomenon is qualitatively stable,\nregardless of model conﬁguration and type, suggesting this as an intrinsic property\nof natural language. This study suggests that interesting properties of natural\nlanguage are still waiting to be discovered, which may aid the future development\nof natural language processing techniques.\n1\nIntroduction\nRecently, AI is witnessing a rising trend of foundation models [Bommasani et al., 2021], which are\ngenerally deﬁned as models trained on broad data, usually unsupervised. The trained model can then\nbe used to solve other downstream tasks. In the ﬁeld of natural language processing, the famous\nfoundation model examples include ELMo [Peters et al., 2018], GPT [Radford et al., 2018] and Bert\n[Devlin et al., 2018]. As has been noticed, continuously increasing the scale of model size and data\nfor training boosts model performance. This phenomenon is described by a scaling law [Kaplan et al.,\n2020]. As a result, we see a trend of language models with larger sizes and better performance, such\nas T5 [Raffel et al., 2020], GPT3 [Brown et al., 2020] Megatron-Turing NLG [Smith et al., 2022].\nAlthough a larger language model brings signiﬁcant performance gains in downstream tasks, their\nenormous sizes also bring problems, such as deployment cost [Sanh et al., 2019], availability issue\n[Izsak et al., 2021], and environmental cost [Schwartz et al., 2020, Patterson et al., 2021].\nThere are lots of attempts aimed at solving these issues and making language models lighter and\nsmaller. Available techniques include knowledge distilling [Sanh et al., 2019], parameter sharing\n[Lan et al., 2019], weight pruning [Michel et al., 2019], reduced precision training [Micikevicius\net al., 2017], quantization [Zafrir et al., 2019] and adaptive methods [Hou et al., 2020], to name\nPreprint. Under review.\narXiv:2210.07041v1  [cs.CL]  13 Oct 2022\na few. One relative under-explored strategy is the divide-and-conquer way of building a language\nmodel. One effective method in this direction is mixture-of-experts language modeling. Famous\nworks include Switch Transformers [Fedus et al., 2021] and Glam [Du et al., 2022]. Both rely on\na mixture of experts who are usually sparsely activated. Compared to dense models, this design\nis capable of scaling up the parameters with reduced computational and memory resources. Some\nmethods also explicitly try to split syntax from semantics [Russin et al., 2019, Havrylov et al., 2019]\nand demonstrate performance gain on some downstream tasks.\nOne key question we can ask about the divide-and-conquer method of language modeling is, are\nlanguage processes naturally dividable? As a ﬁrst step towards studying this problem, we look at a\nsimple scenario where two identical language models are trained side-by-side. We call this setting\nthe \"two-tower language model.\" A preliminary study of this setting suggests a positive answer to our\nquestion. We discover a stable pattern of self-emergence of modeling preference in the two-tower\nlanguage model. In other words, one subgroup of language features is better modeled by one tower,\nwhereas the other subgroup is better modeled by the other tower. The emerging pattern of preference\nis qualitatively stable across different language model types. Thus, this phenomenon appears to be\nan inherent property of language rather than a result of the modeling process. Therefore, our main\ncontribution is the discovery of the self-emergence preference effect in a two-tower language model,\nindicating the existence of dividable structures to process natural language.\n2\nRelated Works\nOne line of related work is language modeling with deep neural networks (DNN). DNN-based\nlanguage models are usually trained to do self-prediction on a large corpus of unlabeled language\ndata. Common strategies to do self-prediction include cloze style, where each word is predicted\nwhen given a bi-lateral context; auto-regressive style, where each word is predicted when given the\nprevious context; and masked language modeling (MLM) style, where a certain percentage of words\nare masked and predicted by remaining un-masked words. Examples of language models using cloze,\nauto-regressive and MLM style are ELMo [Peters et al., 2018], GPT [Radford et al., 2018] and Bert\n[Devlin et al., 2018] respectively. Language models using a hybrid approach are also available, such\nas XLNet [Yang et al., 2019], a model that combines auto-regressive and masked language modeling.\nOther related works include language models with a two-tower modeling strategy. We can roughly\ndivide the models into two groups, symmetrical two-tower models and asymmetrical two-tower\nmodels. Symmetrical two-tower models have two symmetrical sub-units which work together to\nperform certain tasks. One example is the two-tower GPT [Baevski et al., 2019] language model,\nwhich performs cloze-style language modeling. It has one GPT handling forward context and another\nGPT handling backward context. Another example is mirror-Bert [Liu et al., 2021], where two\nparameter-shared Berts are ﬁne-tuned together to reﬁne the learned sentence representation via\ncontrastive learning. The asymmetrical two-tower model consists of two sub-units with different\nconﬁgurations. For example, Electra [Clark et al., 2020] has two MLM language models. A smaller\ngenerator language model generates word replacement and a bigger discriminator language model\npredicts if the word has been replaced. /cite[russin2019compositional] is another example, in which\nlanguage information is explicitly divided into syntax and semantics streams, with syntax information\nserving as an attention over semantics information.\n3\nMethod\n3.1\nTwo-tower Language Model\nThe network topology of our two-tower language model is shown in Fig. 1. Assume a sequence of\ntokens T = [t1, t2, ..., tn] going through a word embedding layer and form a sequence of word vectors\nX = [x1, x2, ..., xn]. (Note that Positional embedding would be added for Bert and GPT cases.)\nThen, the word vectors would go through two encoders with the same conﬁguration f1, f2 separately,\nand form two sequences of hidden representations h1i = f1i(x), h2i = f2(x)i, where i represents\nword position index. We choose three types of neural networks for language encoders f1, f2. For\ncase 1 we choose the ELMo style, where multiple layers of bidirectional LSTMs [Hochreiter and\nSchmidhuber, 1997, Graves et al., 2013] are utilized as the neural network backbone. For case 2 we\n2\nFigure 1: Network topology of the two-tower language model. Word vectors of a sequence of tokens\nare encoded separately by two language models. The language model can be either ELMo, GPT, or\nBert style. The encoded representations are then concatenated and do cloze/auto-regressive/MLM\nstyle self-prediction tasks.\nchoose the GPT style and for case 3 we choose the Bert style. For both the GPT style and the Bert\nstyle, transformers [Vaswani et al., 2017] are selected as the neural network backbone.\nDepending on the network type, token visibility at each position differs for each hidden representation.\nFor ELMo, where cloze style self-prediction is used, the i-th hidden representation would be able\nto access both the previous and following context. The hidden representation can be represented as\nh(1,2)i = ELMo(1,2)(x1, ..., xi−1, xi+1, ..., xn). For GPT, where auto-regressive style self-prediction\nis employed, the i-th hidden representation would only be able to see the previous context. The\nhidden representation can then be written as h(1,2)i = GPT(1,2)(x1, ..., xi−1). For Bert, where\nMLM style self-prediction is used, the i-th hidden representation would be able to see the whole\ncontext except for masked tokens. The hidden representation can then be written as h(1,2)i =\nBert(1,2)(x1, ..., ˜xi, ..., xn), where i represents the index for masked tokens only. ˜xi represents the\nword vector of a special token [MASK] at position i.\nThen, hidden representations coming from both towers are concatenated hi = cat(h1i, h2i). After\nconcatenating the representations, the probability at each word position can be calculated as follows:\npi(t|t(visible,i)) = exp(x(t)T hi)/\nX\na\nexp(xT\na hi)\n(1)\nwhere t(visible,i) means visible tokens for position i, namely t1, ..., ti−1, ti+1, ..., tn for ELMo case,\nt1, ..., ti−1 for GPT case and t1, ..., tn for Bert case. And for Bert, i is for masked tokens only. xa\nrepresents embedding vectors of all the words in our vocabulary, and a represents the token index in\nour vocabulary.\n3.2\nRecognizing Preference Pattern\nAs the two-tower language models start with symmetrical conﬁguration and random initialization,\nthey should start with no preference over any speciﬁc language function. In order to test whether\nany preference pattern emerges following the training of the two-tower language model, we ﬁx the\nparameters of the embedding layers and the language model part and retrain the output prediction\nlayers separately for each tower. The network conﬁguration for this step is shown in Fig. 2.\n3\nFigure 2: Network conﬁguration to study functional preference of the two-tower language model\nafter the previous language modeling step. The parameters of word embedding layers and language\nmodels are ﬁxed, and only output layers are retrained to do language modeling.\nThen we’ll have the probability of each word calculated separately from each tower:\npi1(t|t(visible,i)) = exp(x(t)T h1i)/\nX\na\nexp(xT\na h1i)\npi2(t|t(visible,i)) = exp(x(t)T h2i)/\nX\na\nexp(xT\na h2i)\n(2)\nOne thing we don’t yet deﬁne is which tower should be tower 1 and which tower should be tower 2.\nHere, we deﬁne tower 1 and tower 2 so that:\nE\nt∈C pi1(t|t(visible,i)) ≥E\nt∈C pi2(t|t(visible,i))\n(3)\nC is the collection of all the words in our corpus. Thus, the order of the tower is deﬁned so that the\noverall average word prediction probability for tower 1 is always greater than or equal to tower 2.\nHere, we name tower 1 \"the primary tower\", and tower 2 \"the secondary tower\". Hence, we can\ndeﬁne the measure of the token preference score as the difference of average log probability between\ntwo separately trained model towers, which is described as:\nsa = log ¯p1a −log ¯p2a\n¯p(1,2)a = E\nt∈C(1/n)\nn\nX\ni=1\npi(1,2)(t = ta|t(visible,i))\n(4)\nwhere sa is the preference score of the primary tower over the secondary tower for word a and ta\nrepresents the a-th token in our vocabulary.\n4\nExperiment Setup\n4.1\nData\nSince our goal is to study self-emerging preferences in two-tower language models, we perform only\nthe language modeling step without considering downstream tasks. Thus, only unlabeled language\n4\ncorpus data is needed. We use the same dataset as Bert, which is the BookCorpus [Zhu et al., 2015]\nand English Wikipedia, around 3 billion words in total. We use word piece tokenization, also the\nsame as Bert. The total number of unique tokens in our vocabulary is around 30K. The same dataset,\ntokenizer, and vocabulary are employed for ELMo/GPT/Bert cases.\n4.2\nModel\nWe study three types of language models, namely ELMo, GPT, and Bert. For Bert model, we study\ntwo different network conﬁgurations, namely Bert-base and Bert-large. The network conﬁguration of\neach type that we’ve experimented with is shown in the Table 1 below.\nTable 1: Network conﬁgurations.\nModel Type\nELMo\nGPT\nBert-base\nBert-large\nNumber of layers\n2\n12\n12\n24\nHidden size\n4096\n768\n384\n512\nIntermediate size\n-\n3072\n1024\n2048\nAttention heads\n-\n12\n12\n16\nFor ELMo, we combine two LSTMs, one for the forward direction and one for the backward direction.\nWe directly use the Pytorch [Paszke et al., 2019] implementation of the LSTM. The input token is\nshifted left or right by one position for the forward and backward LSTM. This is to make sure that\nthe output encoding of a speciﬁc token would not see itself. For GPT, we use Hugging Face [Wolf\net al., 2019] implementation of GPT2. Since the token has been shifted inside the model, we won’t\nneed to handle it explicitly. For Bert, we use Deepspeed [Rasley et al., 2020] implementation of Bing\nBert. We make use of the highly optimized Deepspeed transformer kernel for better performance.\n4.3\nTraining Details\nFor training software, we use the Deepspeed software package. Deepspeed offers some features\nuseful to our project. For example, Deepspeed provides an easy interface for distributed training,\nso that we can easily spread out the training load onto 6 Nvidia Tesla V100 GPUs. It also provides\na micro-batch management option to simulate large batch-size training to reduce computation and\ncommunication costs. It also supports a half-precision option with a fused Adam [Kingma and Ba,\n2014] optimizer compatible with the half-precision optimization. For the Adam optimizer parameter,\nwe choose β1 = 0.9, β2 = 0.98, weight decay of 0.01, learning rate 1e-3. Instead of training the\nmodels until they converge, we follow the idea proposed in 24hBert [Izsak et al., 2021] and control\nthe training time of each case within approximately one day.\n5\nResult\n5.1\nCorrelation of Token Preference Score\nAfter training a two-tower language model, we can calculate the preference score sj for each token\nusing Eq. 4. Since the two towers share network conﬁguration with random parameter initialization,\nwe expect sj to be somehow random. However, after training several instances of two-tower language\nmodels, we notice that the preference score sj follows some self-emerged pattern which is qualitatively\nthe same for all the three language model types we’ve tested.\nWe calculate sj for each token and each instance of the two-tower language model and form a\nvector s = [s1, s2, ..., sN], where N is the size of our vocabulary. The elements are usually sorted\nin descending order according to token frequency. By comparing vector s among different trained\ntwo-tower language model instances, we can have an idea of how consistent s can be. The consistency\ncan be simply measured by the correlation between two s vectors of two model instances.\nSame network conﬁguration: pattern of preference is very stable if we use the same network\nconﬁguration and vary little according to the random seed for initialization and training. Fig. 3 red\nand purple lines show the results of two runs of the same Bert-large network conﬁguration with a\ndifferent random seed. For simplicity, we only show the result for the most frequent thousand tokens.\n5\nFigure 3: Preference score of ﬁve cases of two-tower language models. The index of the word is\nsorted so that the preference score is in descending order concerning Bert-large-run2 case. The score\nof the most frequent 1000 words is plotted in this ﬁgure.\nThe x-axis represents the token index and the y-axis represents the corresponding preference score.\nNote that the token index is re-arranged with a descending order of preference score according to the\nBert-large-run2 case. We can see that the preference scores of two instances of Bert-large (run1 and\nrun2) follow almost the same trend. We can calculate the correlation between two s vectors for the\ntop 5000 words (to suppress noises introduced by rare words). The correlation can be as high as 0.94.\nDifferent network conﬁguration within same model type: then, we compare two instances within\nthe same model type but with different network conﬁgurations, namely Bert-base and Bert-large.\nThese results are shown in Fig. 3 green and purple lines. It can be seen that the preference score of\nBert-base and Bert-large strongly correlate with each other. The correlation between the top 5000\nwords of two s vectors is 0.72.\nDifferent model types: in this test, we compare two instances among different model types, namely\nBert-large, ELMo, and GPT. The comparison result is shown in Fig. 3. The purple line represents\nBert-large; the blue line represents the preference score of ELMo and the orange line represents GPT.\nIt can be seen that the preference score of Bert-large, ELMo, and GPT still correlate to some extent.\nThe correlation between Bert-large and ELMo is 0.57, and between Bert-large and GPT is also 0.57.\nThe correlation is also calculated for the top 5000 words.\nPair-wise Correlation: we conclude the pair-wise correlation among the ﬁve cases we’ve experi-\nmented with in Table 2. In light of the stable positive correlation, it can be concluded that there exists\na stable pattern of preference scores among two-tower language models.\n5.2\nPart-of-speech Preference Pattern\nSince the two-tower language model exhibits a stable preference pattern, some tokens are more\npredictable by one tower than the other. Then, it is natural to ask what kind of tokens are likely to be\nbetter predicted by the primary tower and what by the secondary tower. We study this question from\na part-of-speech (POS) point of view.\nAs a ﬁrst step, we need to assign a POS tag to each token. Even though, in theory, each token may\nhave multiple possible POS tags according to the context. For simpliﬁcation, we assign the most\n6\nTable 2: Pair-wise correlation among the ﬁve cases we’ve experimented on, calculated on the most\nfrequent 5000 words.\nModel Type\nELMo\nGPT\nBert-base\nBert-large-run1\nBert-large-run2\nELMo\n1.0\n0.91\n0.27\n0.57\n0.61\nGPT\n0.91\n1.0\n0.26\n0.57\n0.63\nBert-base\n0.27\n0.26\n1.0\n0.72\n0.62\nBert-large-run1\n0.57\n0.57\n0.72\n1.0\n0.94\nBert-large-run2\n0.61\n0.63\n0.62\n0.94\n1.0\nFigure 4: Preference score distribution with different POS tags, from left to right, shows Bert-\nlarge, GPT, and ELMo cases. The X axis is the preference score, and the Y axis is the normalized\ndistribution.\nfrequent POS tag to each token in the vocabulary. We pick a subset of our text corpus and use Stanza\n[Qi et al., 2020] to generate a POS tag for each token. Then, we go through the vocabulary and\nassign the most frequent POS tag to each token. As a result, we can now link each preference score\ncalculated for each token to a POS tag.\nFig. 4 shows the preference score distribution for 7 of the picked POS tags, namely: proper noun\n(PROPN), noun (NOUN), adjective (ADJ), auxiliary (AUX), adposition (ADP), adverb (ADV), verb\n(VERB). Note that we use the universal dependency [Nivre et al., 2016] convention for the POS tags.\nFrom left to right, three ﬁgures represent the Bert-large, GPT, and ELMo cases. It can be seen that\nVERB, ADP, ADV, and AUX are constantly preferred by the primary tower, PROPN is preferred\nby the secondary tower, while NOUN and ADJ are more spread out across the two towers. This\nphenomenon is consistent within all three model types.\n5.3\nICA Component Preference Pattern\nWe tested the distribution of preference scores for each POS category in the last section. In this\nsection, we want to study tower preference in a more semantically meaningful manner.\nTo look in this direction, we follow the idea proposed in Isomura and Toyoizumi [2021]. We ﬁrst\nuse a principal component analysis (PCA) plus independent component analysis (ICA) method to\nextract independent language components. Then, we can study the language preference score of each\nindependent language component.\nFor a subset of word vectors in our corpus X = [x1, x2, ..., xn], we can use PCA to reduce the\ndimension of word vectors:\nU, S, V = SVD(XT X/n)\nP = X · Vd ·\np\nSd\n(5)\nwhere P is the corpus with the word vectors reduced to dimension d. SVD is the operator for singular\nvalue decomposition. Sd represents the largest d eigenvalues of the covariance matrix XT X/n, n\nrepresents the number of tokens, and Vd represents the d eigenvectors corresponding to the largest d\neigenvalues. We choose d to be 128 in our study.\n7\nAfter that, we perform ICA analysis on the corpus of word vectors P with reduced dimension d. We\nuse Amari’s learning rule [Amari et al., 1995] to do ICA:\nWi+1 = Wm + lr · (I −g(Yi)Y T\ni /n) · Wi\nYi = Wi · P\n(6)\nwhere lr represents the learning rate (0.01 in our case). n represents the number of tokens. i represents\nthe iteration step. P is the corpus with the word vectors reduced to dimension d. g(y) is the nonlinear\nactivation function:\ng(y) = sgn(y) · (1 −exp(−\n√\n2|y|)/2\n(7)\nwhere sgn is the sign function.\nIn Eq. 6, we iteratively update the source separation matrix W (initialized as a unit matrix I) until\nconverge. The resulting Y then becomes the corpus of word vectors with independent language\ncomponents.\nWe notice that, after the ICA step, the representations of the words become sparse. We plot the\nrepresentation after ICA in Fig. 5. We can see from the ﬁgure that most of the values (around 80%)\nare within the range of -1 and 1, with a small fraction of the values being quite high. We can deﬁne a\nword cluster as a group of words whose value at a certain dimension is either larger than 2.5 or smaller\nthan -2.5. With this deﬁnition, we can calculate how many clusters each word may correspond to.\nThe result is shown in Fig. 6. It can be seen that nearly 45% of the words correspond to only one\ncluster. Around 13% of the words don’t belong to any cluster. Each word belongs to an average of\n1.45 clusters. We noticed that this method is able to get semantic meaningful word groups. Three\nexamples of the word groups are shown in Fig. 7 in the form of a word cloud. The font size of the\nword is proportional to its frequency. As can be seen, word group one is associated with the concept\nof numbers. Word group two is related to the concept of names. Word group three is about the past\ntense.\nAs a next step, we can check the average preference score for each word group. The result is shown\nin Table 3. The ﬁrst column lists selected word groups, which are represented by 5 typical words\nwhich belong to this cluster. We calculate the average preference score of all the words in the word\ngroup for three of our studied models, namely Bert-large, ELMo, and GPT. We show the result of\nboth the preference score (in the column of the pref-score) and the rank number for reference. As\ncan be read from the table, the self-emerged preference pattern is also stable at the level of the word\ngroup. One word group which has a higher preference score for one kind of model also tends to have\na higher preference score for another kind of model.\nAnother interesting trend that can be seen from the table is that syntactic words which have less\nsemantic meaning tend to have larger preference scores. In contrast, words with more solid semantic\nmeaning tend to have smaller preference scores. This result implies that the separation of syntax and\nsemantics is not only intuition but may also be a self-emerging phenomenon in a two-tower language\nmodel.\n6\nDiscussion\nLanguage is the product of the human brain. A self-emerged pattern of preference in the two-tower\nlanguage model may be related to various discoveries of the self-emerged pattern of preference in the\nhuman brain. One famous example is the cerebral dominance hypothesis [Brown and Jaffe, 1975]. It\nsuggests that one cerebral hemisphere is primarily responsible for certain types of functions, while the\nother functions would be controlled by the other cerebral hemisphere. A second example is the two\ncortical pathways hypothesis in human vision [Mishkin et al., 1983]. This hypothesis suggests there\nexist two separate pathways: the ventral pathway, which is in charge of the visual identiﬁcation of\nobjects, and the dorsal pathway, which is in charge of the localization of objects. Another hypothesis\nexists for speech perception. For example, Hickok et al. [Hickok and Poeppel, 2004] hypothesized\nthat there exists a ventral stream for mapping sound onto meaning, and a dorsal stream for mapping\nsound onto articulatory-based representations.\n8\nFigure 5: Matrix showing the pattern of sparse coding after ICA. The most frequent 500 words\nare shown in the ﬁgure. The X-axis represents the word index, and the Y-axis represents the 128\ndimensions of the word vector after ICA. The value of the word vector is represented in color.\nFigure 6: Distribution of the number of words that belongs to a certain number of word clusters.\n7\nConclusion\nIn this paper, we present our study of a two-tower language model setting, where two symmetrical\nlanguage models with the same network conﬁguration are trained side-by-side to do language\nmodeling tasks. We ﬁnd that, even though the language models start symmetrical, preference for\ncertain language functions emerges after training. We deﬁne the preference score of tokens and ﬁnd\na signiﬁcant correlation between the scores across model instances with different network settings.\nWe also discover that verbs, adverbs, adpositions, and auxiliary tokens are usually better modeled\nby the primary language model tower, proper nouns by the secondary language model tower. While\nadjectives and nouns are more spread out. This preference is rather stable across different network\nconﬁgurations and types, indicating it is an intrinsic property of natural language. We also discovered\nthe self-emerged separation of syntax and semantics in a two-tower language model. We suggest that\nmore work should be invested into understanding the nature of our language so that we can engineer\nsmarter and better language processing techniques in the future.\n8\nSupplementary\n8.1\nPreference Score and Word Frequency\nThis research discovers a novel phenomenon in which preferences emerge spontaneously over diverse\nwords when a two-tower language model is used. We deﬁne preference scores and demonstrate them\nas qualitatively stable across different types of deep neural networks. We also ﬁnd the preference\nscore correlates with POS types and ICA word groups, and ﬁnd out that the primary tower prefers to\n9\nFigure 7: Examples of typical word clusters in the form of the word cloud. The size of the word is\nproportional to its frequency.\nTable 3: Preference score and its rank of typical word clusters for Bert-large, ELMo, and GPT,\nrespectively.\nTypical Words\nBert-large\nELMo\nGPT\npref-score\nrank\npref-score\nrank\npref-score\nrank\nwhich, who, what, where, when\n2.16\n1\n0.79\n6\n0.81\n5\nbut, only, than, however, because\n2.16\n2\n0.98\n2\n1.11\n2\nalso, not, however, both, then\n1.82\n3\n0.83\n4\n0.86\n4\nis, was, were, be, been\n1.77\n4\n1.16\n1\n1.12\n1\ncan, may, would, could, will\n1.46\n5\n0.94\n3\n0.85\n5\nonly, especially, simply, directly, probably\n1.38\n6\n0.31\n11\n0.31\n10\none, two, three, each, another\n1.28\n7\n0.41\n10\n0.48\n7\nother, more, later, same, different\n1.25\n8\n0.67\n8\n0.45\n9\nhim, them, us, men, whom\n1.08\n9\n0.78\n7\n1.02\n3\n##ing, being, using, having, following\n0.98\n10\n0.56\n9\n0.28\n11\nused, called, made, based, known\n0.96\n11\n0.81\n5\n0.60\n6\nmay, year, season, march, day\n0.70\n12\n0.14\n14\n0.37\n9\n##s, ##ed, ##ly, ##ing, ##ism\n0.65\n13\n-1.16\n21\n-1.78\n21\nnew, high, general, early, different\n0.43\n14\n0.26\n12\n0.20\n12\npeople, human, life, home, children\n0.35\n15\n0.06\n15\n0.04\n14\njohn, christian, william, david, charles\n0.30\n21\n-0.99\n21\n-0.85\n19\nﬁlm, book, law, history, theory\n0.22\n16\n0.24\n13\n0.05\n13\ncultural, language, music, art, song\n0.03\n17\n-0.40\n17\n-0.36\n16\nwar, people, government, military, president\n0.02\n18\n-0.58\n19\n-0.62\n18\nhead, cell, hand, body, brain\n-0.16\n19\n-0.17\n16\n-0.24\n15\nmetal, air, ice, iron, stone\n-0.28\n20\n-0.57\n18\n-0.47\n17\nworld, states, american, british, french\n-0.69\n21\n-1.05\n20\n-1.37\n20\nmodel VERB, ADP, ADV, AUX, and more syntactic word groups, while the secondary tower prefers\nto model PROPN and more semantic word groups.\nAnother salient feature of the word that is known to be correlated with word function is word\nfrequency. As we know, functional words tend to have a higher frequency, and more frequent words\ntend to have more solid semantic meaning. Then we ask, are we just rediscovering word frequency\nwith a two-tower language model? The answer is no.\nFig. 8 shows the preference score v.s. word index sorted with descending word frequency of three\ntypes of the tow-tower language model, namely Bert-large (blue), ELMo (orange), and GPT (green).\nThe plot shows the most frequent 1000 words for simplicity. It can be seen from the ﬁgure that, except\nfor the ﬁrst several tens of words, preference score and word frequency have no obvious correlation.\nWe also check the correlation quantitatively. Table 4 shows Spearman’s rank correlation among Bert-\nlarge, ELMo, GPT preference score rank, and word frequency rank. It can be seen that the correlation\nbetween word frequency rank and preference score rank of language models is signiﬁcantly smaller\nthan the correlation of preference score rank between the two language models. These results conﬁrm\nthat the preference score pattern described in this paper cannot be solely explained by word frequency\npattern.\n10\nFigure 8: Word preference score v.s. word index which is sorted in descending order. Blue, orange,\nand green lines are for Bert-large, ELMo, and GPT respectively.\nTable 4: Pair-wise Spearman’s rank correlation among Bert-large, ELMo, GPT, and frequency rank,\ncalculated on the most frequent 5000 words.\nModel Type\nELMo\nGPT\nBert-large\nFrequency Rank\nELMo\n1.0\n0.93\n0.56\n0.16\nGPT\n0.93\n1.0\n0.65\n0.22\nBert-large\n0.56\n0.65\n1.0\n0.18\nFrequency Rank\n0.16\n0.22\n0.18\n1.0\nReferences\nS.-i. Amari, A. Cichocki, and H. Yang. A new learning algorithm for blind signal separation. Advances\nin neural information processing systems, 8, 1995.\nA. Baevski, S. Edunov, Y. Liu, L. Zettlemoyer, and M. Auli. Cloze-driven pretraining of self-attention\nnetworks. arXiv preprint arXiv:1903.07785, 2019.\nR. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg,\nA. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint\narXiv:2108.07258, 2021.\nJ. W. Brown and J. Jaffe. Hypothesis on cerebral dominance. Neuropsychologia, 13(1):107–110,\n1975.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information\nprocessing systems, 33:1877–1901, 2020.\nK. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning. Electra: Pre-training text encoders as\ndiscriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.\nBert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nN. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat,\net al. Glam: Efﬁcient scaling of language models with mixture-of-experts. In International\nConference on Machine Learning, pages 5547–5569. PMLR, 2022.\n11\nW. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with\nsimple and efﬁcient sparsity, 2021.\nA. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In\n2013 IEEE international conference on acoustics, speech and signal processing, pages 6645–6649.\nIeee, 2013.\nS. Havrylov, G. Kruszewski, and A. Joulin. Cooperative learning of disjoint syntax and semantics.\narXiv preprint arXiv:1902.09393, 2019.\nG. Hickok and D. Poeppel. Dorsal and ventral streams: a framework for understanding aspects of the\nfunctional anatomy of language. Cognition, 92(1-2):67–99, 2004.\nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\n1997.\nL. Hou, Z. Huang, L. Shang, X. Jiang, X. Chen, and Q. Liu. Dynabert: Dynamic bert with adaptive\nwidth and depth. Advances in Neural Information Processing Systems, 33:9782–9793, 2020.\nT. Isomura and T. Toyoizumi. On the achievability of blind source separation for high-dimensional\nnonlinear source mixtures. Neural Computation, 33(6):1433–1468, 2021.\nP. Izsak, M. Berchansky, and O. Levy. How to train bert with an academic budget. arXiv preprint\narXiv:2104.07705, 2021.\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,\nand D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\nZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. Albert: A lite bert for\nself-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\nF. Liu, I. Vuli´c, A. Korhonen, and N. Collier. Fast, effective, and self-supervised: Transform-\ning masked language models into universal lexical and sentence encoders.\narXiv preprint\narXiv:2104.08027, 2021.\nP. Michel, O. Levy, and G. Neubig. Are sixteen heads really better than one? Advances in neural\ninformation processing systems, 32, 2019.\nP. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston,\nO. Kuchaiev, G. Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740,\n2017.\nM. Mishkin, L. G. Ungerleider, and K. A. Macko. Object vision and spatial vision: two cortical\npathways. Trends in neurosciences, 6:414–417, 1983.\nJ. Nivre, M.-C. De Marneffe, F. Ginter, Y. Goldberg, J. Hajic, C. D. Manning, R. McDonald, S. Petrov,\nS. Pyysalo, N. Silveira, et al. Universal dependencies v1: A multilingual treebank collection.\nIn Proceedings of the Tenth International Conference on Language Resources and Evaluation\n(LREC’16), pages 1659–1666, 2016.\nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances\nin neural information processing systems, 32, 2019.\nD. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. So, M. Texier, and\nJ. Dean. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350,\n2021.\n12\nM. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep\ncontextualized word representations. In Proceedings of the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 2227–2237, New Orleans, Louisiana, June 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/N18-1202. URL https://aclanthology.\norg/N18-1202.\nP. Qi, Y. Zhang, Y. Zhang, J. Bolton, and C. D. Manning. Stanza: A Python natural language\nprocessing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics: System Demonstrations, 2020. URL https:\n//nlp.stanford.edu/pubs/qi2020stanza.pdf.\nA. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by\ngenerative pre-training. 2018.\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu, et al.\nExploring the limits of transfer learning with a uniﬁed text-to-text transformer. J. Mach. Learn.\nRes., 21(140):1–67, 2020.\nJ. Rasley, S. Rajbhandari, O. Ruwase, and Y. He. Deepspeed: System optimizations enable training\ndeep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining, pages 3505–3506, 2020.\nJ. Russin, J. Jo, R. C. O’Reilly, and Y. Bengio. Compositional generalization in a deep seq2seq model\nby separating syntax and semantics. arXiv preprint arXiv:1904.09708, 2019.\nV. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster,\ncheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\nR. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni. Green ai. Communications of the ACM, 63(12):\n54–63, 2020.\nS. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye,\nG. Zerveas, V. Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b,\na large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\nAttention is all you need. Advances in neural information processing systems, 30, 2017.\nT. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\nM. Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing.\narXiv preprint arXiv:1910.03771, 2019.\nZ. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le. Xlnet: Generalized\nautoregressive pretraining for language understanding. Advances in neural information processing\nsystems, 32, 2019.\nO. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat. Q8bert: Quantized 8bit bert. In 2019 Fifth\nWorkshop on Energy Efﬁcient Machine Learning and Cognitive Computing-NeurIPS Edition\n(EMC2-NIPS), pages 36–39. IEEE, 2019.\nY. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books\nand movies: Towards story-like visual explanations by watching movies and reading books. In\nProceedings of the IEEE international conference on computer vision, pages 19–27, 2015.\n13\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2022-10-13",
  "updated": "2022-10-13"
}