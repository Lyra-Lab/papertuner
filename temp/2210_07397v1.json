{
  "id": "http://arxiv.org/abs/2210.07397v1",
  "title": "A Concise Introduction to Reinforcement Learning in Robotics",
  "authors": [
    "Akash Nagaraj",
    "Mukund Sood",
    "Bhagya M Patil"
  ],
  "abstract": "One of the biggest hurdles robotics faces is the facet of sophisticated and\nhard-to-engineer behaviors. Reinforcement learning offers a set of tools, and a\nframework to address this problem. In parallel, the misgivings of robotics\noffer a solid testing ground and evaluation metric for advancements in\nreinforcement learning. The two disciplines go hand-in-hand, much like the\nfields of Mathematics and Physics. By means of this survey paper, we aim to\ninvigorate links between the research communities of the two disciplines by\nfocusing on the work done in reinforcement learning for locomotive and control\naspects of robotics. Additionally, we aim to highlight not only the notable\nsuccesses but also the key challenges of the application of Reinforcement\nLearning in Robotics. This paper aims to serve as a reference guide for\nresearchers in reinforcement learning applied to the field of robotics. The\nliterature survey is at a fairly introductory level, aimed at aspiring\nresearchers. Appropriately, we have covered the most essential concepts\nrequired for research in the field of reinforcement learning, with robotics in\nmind. Through a thorough analysis of this problem, we are able to manifest how\nreinforcement learning could be applied profitably, and also focus on\nopen-ended questions, as well as the potential for future research.",
  "text": "A Concise Introduction to Reinforcement Learning in Robotics\nAkash Nagaraj\nDepartment of Computer Science\nPES University\nBangalore, India\nakashn1897@gmail.com\nMukund Sood\nDepartment of Computer Science\nPES University\nBangalore, India\nmukundsood2013@gmail.com\nBhagya M Patil\nDepartment of Computer Science\nPES University\nBangalore, India\nbhagyampatil@pes.edu\nAbstract— One of the biggest hurdles robotics faces is the\nfacet of sophisticated and hard-to-engineer behaviors. Rein-\nforcement learning offers a set of tools, and a framework to\naddress this problem. In parallel, the misgivings of robotics\noffer a solid testing ground and evaluation metric for advance-\nments in reinforcement learning. The two disciplines go hand-\nin-hand, much like the ﬁelds of Mathematics and Physics. By\nmeans of this survey paper, we aim to invigorate links between\nthe research communities of the two disciplines by focusing on\nthe work done in reinforcement learning for locomotive and\ncontrol aspects of robotics. Additionally, we aim to highlight\nnot only the notable successes but also the key challenges of\nthe application of Reinforcement Learning in Robotics.\nThis paper aims to serve as a reference guide for researchers\nin reinforcement learning applied to the ﬁeld of robotics.\nThe literature survey is at a fairly introductory level, aimed\nat aspiring researchers. Appropriately, we have covered the\nmost essential concepts required for research in the ﬁeld of\nreinforcement learning, with robotics in mind. Through a\nthorough analysis of this problem, we are able to manifest\nhow reinforcement learning could be applied proﬁtably, and\nalso focus on open-ended questions, as well as the potential for\nfuture research.\nKeywords- reinforcement learning in robotics, robotic\nlearning, learning locomotion, learning actions.\nI. INTRODUCTION\nReinforcement Learning (RL) is an important type of\nMachine Learning where an agent learns how to behave in a\nparticular environment by performing actions and observing\nthe results of these actions. Based on these results the agent\nis assigned a reward.\nNumerous challenging problems have seen recent success\nwhen addressed with model-free Reinforcement Learning\nalgorithms. These have proven to make very effective prob-\nlems to evaluate algorithms because of the fact that they\nare entirely observable, manipulable and have the ability\nto run quickly and simultaneously gather vast amounts of\nexperience for the agent. They have the potential for not\nonly structural, but also perpetual similarities to real-world\nproblems.\nNow, tasks pertaining to the real-world serve as a target for\nReinforcement Learning algorithms, especially in the ﬁeld\nof robotics which bear both control problems and difﬁcult\nperception. Recent notable contributions include motor con-\ntrol directly from pixels for manipulation, continuous control\nfor simulated robots, and simulation-to-real transfer, also for\nmanipulation.\nRecent work has also focused on addressing the task of\nnavigation in 3D environments in simulation, such as maze\nnavigation using imagery from a perspective view. In the\nﬁeld of robotics, agents have also been trained to navigate\nwith the help a number of different sensors. Visual sensing\nis seen as an established direction. It includes work on ﬁne-\ntuning in the real world as well as target-driven image-based\nnavigation in small grid worlds with a simulation. Structure-\nbased sensors are also used because of their ability to exhibit\nminor difference between simulation and reality than seen in\nvisual sensing.\nTo understand the need for a reward, we will use an\nanalogy. Assume there is a child in a room with a ﬁreplace.\nAs the child moves closer to the ﬁreplace it feels the warmth,\nand likes it. This would be considered a positive reward.\nHowever, if the child were to go and touch the ﬁre, he\nwould burn himself. This would be considered a negative\nreward. Therefore, we can say that the child has learned from\nthe observations obtained from the actions he took. This is\nthe fundamental need for a reward system in Reinforcement\nLearning.\nThe way a Reinforcement Learning task is deﬁned is as\nfollows:\n• An agent receives state S0 from the environment\n• Based on the state S0, the agent will take an action A0\n• The environment then transitions to a new state S1\n• The environment returns some reward R0 to the agent\nThis way, the Reinforcement Learning model returns a\nsequence of state, action, and reward with the goal of the\nagent to maximize the expected cumulative reward.\nThis whole process can also be called a Markov Decision\nProcess (MDP). An MDP is established using the Markov\nProperty, which states that given the present, the future is\nindependent of the past. In mathematical terms, a state St\nhas the Markov Property, if and only if:\nP(St+1|St) = P(St+1|S1, ..., St)\nFormally, and MDP is used to describe the environment for\nRL, where the environment is fully observable. Almost all\nRL problems can be formalized as MDPs.\nMathematically, we can deﬁne the cumulative reward\nstated above as a Markov Reward Process by the equations:\nGt =\n∞\nX\nk=0\nγkRt+k+1; where γϵ[0, 1)\narXiv:2210.07397v1  [cs.RO]  13 Oct 2022\nRt+1 + γRt+2 + γ2Rt+3...\nIt is logical to assume while calculating the cumulative\nreward, that earlier rewards have a higher probability of\nhappening, as they are more predictable than the long-term\nfuture reward. Hence, it is important to discount rewards,\nand this is done by introducing a discount rate gamma whose\nvalue must be between 0 and 1. Another need for the discount\nrate is that it is mathematically convenient and guarantees\nthat the algorithm will converge, avoiding inﬁnite returns\nin loopy Markov Processes. As seen in the equation, the\naddition of gamma can be interpreted in two ways:\n• A larger gamma results in a smaller discount, implying\nthe agent is more worried about the long-term reward\n• A smaller gamma results in a larger discount, implying\nthe agent is more worried about the short-term reward.\nII. SETTING UP A REINFORCEMENT LEARNING TASK\nWITH A REAL-WORLD ROBOT\nThe aim of this work is to address the question of how\nto set up a real-world robotic task so that an off-the-\nshelf implementation of a standard RL method can perform\neffectively and reliably. Rupam et al. designed a Reacher\ntask with the UR5 Robot (called UR5 Reacher). The task of\nthe agent is to learn to reach arbitrary target positions with\nlow-level actuations of a robot arm using a reinforcement\nlearning approach.\nThrough this paper, they try to highlight the difference\nbetween simulations of RL tasks and their real-world coun-\nterparts. They then go on to discuss the steps and elements\nof setting up real-world RL tasks including a medium\nof data transmission, concurrency, ordering and delays of\ncomputation, low-level actuation types, and frequencies of\noperating them. By varying these elements, they study and\nreport their individual contributions and the difﬁculty with\nrobot learning. They show that by accounting for the effects\nof these variations and addressing them with care during the\nset-up of the robot for the RL task, it is possible to achieve\nnot only effective performance, but also repeat-ability of\nlearning from scratch in a highly reliable manner even when\nthe repeats are run for hours on different times using different\nphysical robots.\nBelow is a summary of each of the elements involved\nin a real-world setup and the effect that they have on each\nexperiment:\nA. Concurrency, ordering, and delays of computation\nDuring simulations of the same task, it is natural to\nperform agent and environment-related computations syn-\nchronously, which may not be desirable in real-world tasks.\nSimulated tasks can comply with the Markov Decision\nProcess (MDP) framework where time does not advance\nbetween observing and acting, which is not the case in the\nreal-world. In the real-world time marches on during each\nagent and environment-related computation. Hence, the agent\nis always operating on delayed sensorimeter information.\nThis can lead to misplaced synchronization and ordering of\ncomputations, which may result in a more difﬁcult learning\nproblem and reduced potential for responsive control. There-\nfore, a design objective should be to manage and minimize\nthese delays.\nIn UR5 Reacher, the computational steps by distributing\nthem into two asynchronous processes: the robot communi-\ncation process and the reinforcement learning (RL) process.\nThe robot communication process is a device driver which\ncollects the sensorimotor data in a sensor thread and send\nthe actuation commands in a separate actuator thread. The\nRL process however, contains an environment thread that\nchecks spatial boundaries, computed the observation vector\nand the reward function that is based on the sensorimotor\npackets and further updates the actuation command for the\nactuator thread. It also contains an agent thread to deﬁne task\ntime steps and determining the action cycle time. Hence,\nsplitting the RL process into two threads allows checking\nsafety constraints faster than, and concurrently with action\nupdates.\nB. Medium of data transmission\nHere Reference [1] highlights the natural consideration\nof a scientist to pair a mobile robot with limited on-board\ncomputing power with a more computationally powerful base\nstation via a WiFi or Bluetooth module as opposed to the\nstandard USB or Ethernet option available. This however,\ncomes with its own drawbacks.\nOften, WiFi brings in variability in the inter-arrival time of\nstreamed packets. In the task setup of the Reference [1], they\ninitially set-up a baseline model using an Ethernet connection\nfor the robot communication process, before switching to\ncommunication over a TCP/IP connection.\nC. Action cycle time\nThis is the time between two subsequent action updates by\nthe agent’s policy, it is also commonly called the time-step\nduration.\nReference [1] highlight how choosing a cycle time for the\ntask at hand is not obvious, and the available literature lacks\nguidelines for a task set-up. A shorter cycle time may include\nsuperior policies with ﬁner control, however, if changes in\nsubsequent observation vectors with too short cycle times are\nnot perceptible to the agent, it results in a learning problem\nthat is very difﬁcult and near impossible. At the other end,\na longer cycle time will limit the set of possible policies\nand the control one has, however, it may make the learning\nproblem easier.\nHence, it is clear that a trade-off must be made during\nconduction of the real-world task to obtain an optimal output.\nD. The action space: position v/s velocity control\nWhen the learning process is in its infant stages, the initial\npolicies learned by the robot resulted in sequences of angular\nposition commands that caused violent, abrupt movements\nand emergency stops. Hence, initially it is possible to limit\nthe speeds the robot can take, thus avoiding the sudden jerks.\nModulation of position and velocity commands is an\nimportant aspect.\nIII. SOLVING SPARSE REWARD TASKS FROM SCRATCH\nReference [2] deﬁnes the problem to be for a robot to\nperform tasks such as opening a box and placing a block\ninside it, stacking blocks on top of each other etc. We must\nnote that while deﬁning the ﬁnal rewards are easy, such as\nassigning a reward of 1 when a block is placed in a box\nsuccessfully, or stacking one block on another, the complete\naction has several intermediate steps that must also be given\nrewards. Hence, this becomes a problem with sparse rewards,\nand the learning process becomes harder.\nA new algorithm is deﬁned for robots to learn such\nproblems through RL, called Scheduled Auxiliary Control\n(SAC-X). Their approach however, is based on four main\nprinciples, as described in their paper. Those being:\n• Every state-action pair is paired with a vector of rewards\n(which are of two types - internal auxiliary rewards and\nexternal rewards)\n• Each reward entry has an assigned policy, called an\nintention, that is trained to maximize its corresponding\ncumulative reward.\n• A high-level scheduler will select and execute the\nindividual intentions with the goal of improving the\nperformance of the agent on the external tasks.\n• Learning is performed asynchronously from policy ex-\necution and the experience between intentions is shared\nso as to use the information effectively.\nA. Scheduled Auxiliary Control (SAC-X)\nThe algorithm is a hierarchical approach to reinforcement\nlearning for learning from sparse rewards. The goal deﬁned\nis to both\n• train all auxiliary intention policies as well as the main\ntask policies to achieve their respective goals.\n• utilize all intentions for fast exploration in the main\nsparse-reward Markov Decision Process.\nThe second point was achieved by using a hierarchical\nobjective for policy training that decomposes into two parts\n• Learning the intentions - the ﬁrst part of the hierarchical\nobjective is given by joint policy improvement objective\nfor all intentions (auxiliary and external)\n• Learning the scheduler - the second part of the hier-\narchical objective deals with learning a scheduler that\nsequences intention-policies.\nB. Conclusion\nThe new algorithm described in Reference [2] is run on\na variety of experiments. They compare the performance of\ntheir novel method to the previous ones already deﬁned and\nin existence. [6]\nThe experiments that they carried out were predominantly\nvariations of stacking experiments (stacking a smaller object\non top of a larger object and vice-versa, stacking irregularly\nshaped objects on top of other objects, clean-up tasks etc.).\nThey also show that they can learn certain tasks that were\nunable to be learned by these previous algorithms.\nThey prove that by adding a scheduler, which based on the\ncurrent state and information from the external world, maps\nout a sequence of events that will give the highest reward,\nthe robot is able to learn better and most importantly, more\nefﬁciently.\nIV. LEARNING SYNERGIES BETWEEN PREHENSILE AND\nNON-PREHENSILE ACTIONS\nPrehensile and non-prehensile actions are an integral part\nof robotics. Prehensile and non-prehensile go hand-in-hand,\nfor instance, pushing (prehensile) can help rearrange clut-\ntered objects in order to provide space for arms and ﬁngers of\nthe robot to facilitate a successful grasp. Likewise, grasping\n(non-prehensile) is useful to displace objects to facilitate\ncollision-free as well as precise pushing motions.\nAnother approach to this is the hard-coding, which exploits\ndomain-speciﬁc knowledge, however, this severely limits the\nvarious types of synergistic behaviors between prehensile and\nnon-prehensile actions that can be performed. It must also be\nduly noted that pushing actions, are considered useful only\nif, in time, they facilitate grasping actions.\nReference [3] which lies in the intersecting domains of\nmachine learning, robotic manipulation, and computer vision,\nis one that focuses on learning of the synergy between\npushing and grasping with the help of self-supervised Deep\nReinforcement Learning. The primary contribution of [3]\nis that it shines light on a completely new perspective to\nbridging the gap between data-driven prehensile and non-\nprehensile manipulation. Reference [3] has an architecture\nthat comprises of two, fully convolutional networks that are\nused to map visual observations of a camera to robotic\nactions; prehensile and non-prehensile. The ﬁrst network\ninfers the usefulness of a grasp in a dense sampling (done\nFig. 1.\nLearning performance in different UR5 Reacher setups. a) Our baseline setup. b) Artiﬁcial delays. c) Action cycle times resulted in worse learning\nperformance. d) Velocity control.\npixel-wise) end-effector orientations and locations, while the\nsecond fully convolutional network does the exact same, but\nin this case for pushing. FCNs φp (pushing) and φg (grasp-\ning) share the same network architecture: two parallel 121-\nlayer DenseNets pre-trained on ImageNet[7], proceeded by\nchannel-wise concatenation and two 1x1 convolutional lay-\ners interleaved using nonlinear activation functions (ReLU),\nand spatial batch normalization, and ﬁnally bi-linearly up-\nsampled. A small learning rate is prescribed to the network at\n10−5 and gradients are back-propagated through the network\nafter each executed action.\nThe system is trained jointly in a Q-learning [10] frame-\nwork and are entirely self-supervised by trial and error,\nand rewards are provided based on successful grasps. The\npolicies are trained end-to-end using a deep network that\naccepts visual observations and outputs the expected return\n(in the form of Q values) for potential pushing and grasping\nactions, given the visual observation. In all, 32 pixel-wise\nmaps of Q values (16 for prehensile actions (pushes) in dif-\nferent directions, and 16 for non-prehensile actions (grasps)\nat different orientations) are considered. The joint policy is\nused to choose the action with the maximum Q value in order\nto maximize the chance of successful current/future grasps.\nThis system is also proven to generalize very well to novel\nobjects.\nThe Markov decision process representation of the entire\nsystem is as follows: at any given state st at time t, the agent\n(i.e. robot) chooses and executes an action at according to a\npolicy π(st), then transitions to a new state st+1 and receives\nan immediate corresponding reward Rat (st, st+1). The goal\nof our robotic reinforcement learning problem is to arrive\nat an optimal policy π∗that ensures maximization of the\nexpected sum of future rewards.\nRg(st, st+1) = 1 is awarded in the event of a successful\ngrasp, which is computed by calculating the threshold of the\nantipodal distances between the gripper ﬁngers following a\ngrasp attempt. Similarly, Rp(st, st+1) = 0.5 for pushes that\ncause detectable changes to the environment.\nThe system models each state st as an RGB-D height-\nmap image representation of the scene at a particular time t.\nFor the purpose of computation of this height-map, RGB-\nD images are captured with the help of a ﬁxed-mount\ncamera. These images are then projected onto a 3D point\ncloud, and back-projected, orthographically against in the\ndirection of gravity (upwards) to construct a height-map\nimage representation with both height-from-bottom (D) as\nwell as colour (RGB) channels.\nV. LOCOMOTIVE BEHAVIOURS IN RICH ENVIRONMENTS\nOne of the important and immanent aspects of robotics\nis locomotion. Locomotive movements in robots are notori-\nously known to be behaviours that are very sensitive to the\nchoice of reward. Agents can be taught to perform various\nactions such as running, crouching, jumping and turning, as\nneeded by the environment in the absence of explicit reward-\nbased guidance.\nReference [4] is a paper that shows great promise in\nthis area of research by building upon robust, pre-existing\npolicy gradient algorithms, for instance: Trust Region Policy\nOptimization (TRPO) [8] and Proximal Policy Optimization\n(PPO) [9]. These algorithms operate by bounding each\nparameter update to a trust region, thereby ensuring stability\nby restricting the amount by which a particular update can\nchange the policy. Additionally, similar to the A4C algorithm\n[11] and its related approaches, computation is distributed\nover numerous distributed, parallel instances of both the\nenvironment, and the agent. This distributed implementation\nof Proximal Policy Optimization (PPO) shows a signiﬁcant\nimprovement over TRPO with respect to wall clock time,\nthat being said, we still see minor differences when it comes\nto robustness of the algorithms. It also shows an improved\nperformance over many existing implementations of the\nA3C algorithms, using the same number of workers, with\ncontinuous actions provided.\nIn every iteration of TRPO given the current parameters\n(denoted by θold), a comparatively large batch of data is\ncollected, which aims to optimize the surrogate loss. The\nconstraint is placed on the magnitude that the policy is\nallowed to change, and we express this in terms of the\nKullback-Leibler divergence.\nDistributed PPO is very similar to TRPO in terms of\nperformance. DPPO scales very well proportional to the\nFig. 2.\nOverview of the system and Q-learning formulation\nnumber of workers used, possible resulting in signiﬁcant\nreduction of wall clock time required. Since DPPO is a\nfully gradient based algorithm, therefore, as shown in the\nMemory Reacher Task [4], it can also be used with Recurrent\nNetworks.\nThe experimental setup [4] comprises of a physical simu-\nlation environment similar to a platform game, implemented\nin Mujoco.\nThree different torque-controlled bodies are considered: ”\n• Planar walker: a simple walking body with 9 Degrees\nof Freedom (DoF), and 6 actuated joints constrained to\nthe plane.\n• Quadruped: a simple three-dimensional quadrupedal\nbody with 12 DoF and 8 actuated joints.\n• Humanoid: a three-dimensional humanoid with 21 ac-\ntuated dimensions and 28 DoF. ”\nThe rewards for all tasks are kept consistent and simple\nacross all terrains. The reward comprises of a main compo-\nnent, which is in proportion to the velocity along the x-axis,\nplus a small term penalizing torques. This encourages the\nagent to make progress in the forward direction along the\ntrack. The agent receives two collections of observations:\n1) a collection of egocentric, ”proprioceptive” features\ncontaining joint angles as well as angular velocities.\nIn cases of the Humanoid and Quadraped, they also\ncontain the readings for a gyroscope, accelerometer,\nand a velocimeter that are placed at the torso, in\naddition to contact sensors attached to the feet and\nlegs.\n2) a collection of “exteroceptive” features which contains\ninformation that is task-relevant, such as the proﬁle of\nthe terrain ahead, as well as the position of the agent\nwith respect to the center of the track.\nIt must also be noted that the walker also occasionally\novercomes obstacles by chance. The probability associated\nwith this change event is minute when the height of the\nobstacles are very high.\nVI. ONE-SHOT REINFORCEMENT LEARNING FOR\nNAVIGATION\nChallenging problems have been solved by learning from\nan agent’s extensive interaction with the environment by us-\ning model-free reinforcement learning algorithms. Reference\n[5] present a new method to teach an agent to learn to\nnavigate to a ﬁxed destination or goal in a set environment.\nA primary priority of any robotic learning system is to be\nable to learn a behaviour or task while still managing to\nminimize the interaction needed with the environment. ”\nIn order to learn to navigate towards a ﬁxed destination or\ngoal in a known, real-world environment reliably, Reference\n[5] proposes a method that performs an interactive replay of a\nsingle traversal of the said environment. To accomplish this,\nthe learning model uses pre-trained visual features, and also\naugments the training set with various stochastic observations\nto demonstrate zero-shot transfer to variations in the real-\nworld environment which are unseen during training.\nIn model-free Reinforcement Learning, the policy is im-\nplemented with the help of a function approximator, a deep\nneural network is commonly used for this application. Cus-\ntomarily, the system is usually trained either to approximate\nthe probability distribution over the actions of the agent\ndirectly, ensuring the maximum expected reward (as in\npolicy search) or to map actions and observations to the\nexpected future reward (as in Q-learning). Coming to value-\nbased Reinforcement Learning methods such as Q-learning,\nthe policy consists of a value estimate (the Q function)\nbeing used to choose actions with the highest return. In\ncomparison, in the case of policy search methods, the policy\nis parameterized directly and the parameters of the policy are\nupdated in the direction of positive outcomes using a value\nfunction. In the case of [5], a value-based method known as\nbootstrapped Q-learning is used.” ”\nInteractive Replay is a concept in which an agent uses\na single traversal of the environment to memorize a rough\nworld model. This provides the agent with an opportunity\nto interact with the model and generate s signiﬁcant number\nof diverse trajectories. The purpose of this is to learn how\nto navigate, while also minimizing the amount of real-\nworld data required by the agent. Using interactive replay,\nthe agent learns to navigate through the environment as a\nsingle trajectory through a real environment comprises of a\nvirtual environment. In addition, a validation environment\nis also constructed using a second pass of the environment\nhowever with different environmental conditions, which is\napproximately aligned with the training environment.\nFirst, the agent executes a complete traversal of the\nenvironment, and records all the sensor data during the\ntraversal. Second, the recorded data is arranged in the form\nof a topological map of the environment. In the experimental\nsetup in [5], minimal human interaction is required to align\nthe data to a topological map. In the map, a human is\nshown the approximate poses of the agent in 2-Dimensional\nspace, loops are closed, and duplicated regions of space\nare removed. Generally, both phases can be accomplished\nautonomously using only Simultaneous Localization and\nMapping (SLAM) techniques without human intervention. A\npose graph consisting of sensory snapshots taken at intervals\nof ∆pos meters is created by dicretizing the environmental\nspace.\nCollection of the validation environment is simpler when\ncompared to the collection of a training environment. The\nagent conducts a second traversal of the environment, which\nneed not necessarily be in the order as the ﬁrst traver-\nsal. Accordingly, the training environment is used with an\napproximate localization method to associate the training\nenvironment to nodes in the pose graph, with the closest\nmatching pose. In [5], laser-based localization is used as\nthe approximate localization system. This is coupled with a\n360◦camera used to align the image’s orientation, but pose\ncorrespondences could as well be annotated by hand in the\ncase when a localization system is unavailable. ”\nOne of the consequences of training your model on a\nsmall set of real-world experience is that over-ﬁtting to the\nconstrained environment. Over-ﬁtting could also manifest in\nthe learning of visual features for environment identiﬁcation.\nMost of the current methods in model-free RL aim at\nlearning feature extractors from absolutely nothing. They use\njust one traversal of a small environment, and this proves an\nextremely limited variety of visual structure. The lack of\ndiversity in the training environment can be compensated by\nusing a visual encoder network trained on a very large vision\ndataset.\nAnother solution for the issue of limited data is data\naugmentation. It i a proven and widely used technique in\nmachine learning. [5] exploits this technique by augmenting\nthe training environment with stochastic observations to\naid in learning. These techniques have been established to\ncompensate for the lack of extensive training samples, in\nwhich several random transformations are applied to the\ntraining samples to facilitate the improvement of the diversity\nof the training data.\nModel-free Reinforcement Learning algorithms involve\na double dueling n-step Q-learning framework established\nusing NQ parallel Q-Function heads. Each individual head is\na Deep Neural Network in it’s own-self, with recurrent layers\nand shared visual encoders, but with its own Q-function\noutput layers.\nReference [5] trains the bootstrapped Q-Network to min-\nimize the traditional Q-learning loss function:\nL = ΣbatchQ(f(xt)) −Rt)2\nwhere, Rt is the sum of exponentially-discounted future\nrewards. The ensemble nature of bootstrapped Q-learning is\nthat it’s ensemble nature enables the interpretation of the\ndistinct Q-estimates as a function of the probability distribu-\ntion, which in turn enables reasoning about the uncertainty\nin the mode, which is especially beneﬁcial in the context of\nrobotics. All the experiments conducted in [5] had a common\ngoal location. Previous work done on goal-driven navigation\nincludes the goal location in the visual feed, however, in\nthe approach presented in [5], the location of the goal is not\nvisible on the images. The robot is instead trained to identify\nthe environmental landmarks only from photographic inputs.\nThe experimental setup in [5] compares the following 3\ntypes of RL algorithms that learn from parallel experiences:\nA. Advantage-Actor-Critic (A2C)\nThis algorithm is a synchronous implementation of A3C\nin which the same model is used to choose parallel actions. It\nis a policy-search method that aims to optimize a probability\ndistribution over possible actions.\nB. n-step Q-learning\nThis algorithm is a value-based method similar to the\nbootstrapped approach but one which maintains a single Q-\nfunction estimate.\nC. n-step bootstrapped Q-learning\nBootstrapping is the use of one or more estimated values\nin the update step for the similar kinds of estimated values.\nThe performance of the bootstrapped Q-learning algorithm\nis signiﬁcantly better, compared to the other 2 algorithms.\nFuture work as described in [5] will be focusing on transfer\nto situations with agent and robot in a loop. Validation\nset performance provides evidence to show that closed-loop\ntransfer is a feasible solution, although several issues are yet\nto be addressed.\nVII. CONCLUSION\nThis paper aims to serve as a reference guide for re-\nsearchers looking to get into research in reinforcement learn-\ning applied to the ﬁeld of robotics. The literature survey is\nat a fairly introductory level, aimed at aspiring researchers.\nAppropriately, we have covered the most essential concepts\nrequired for research in the ﬁeld of reinforcement learning,\nwith robotics in mind. Right from setting up of an experi-\nmental setup for real-world application to methods such as\ninteractive replay, we also cover other concepts integral to\nthe ﬁeld of robotics, such as robots learning to solve tasks,\nlearning synergies between prehensile and non-prehensile\nactions, and learning of locomotive behaviors in various\nterrains, all of which have been presented after much testing,\nin a well-thought-out experimental setup.\nREFERENCES\n[1] Mahmood, A.R., Korenkevych, D., Komer, B.J. and Bergstra, J., 2018.\nSetting up a Reinforcement Learning Task with a Real-World Robot.\narXiv preprint arXiv:1803.07067.\n[2] Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J.,\nVan de Wiele, T., Mnih, V., Heess, N. and Springenberg, J.T., 2018.\nLearning by Playing-Solving Sparse Reward Tasks from Scratch. arXiv\npreprint arXiv:1802.10567.\n[3] J. U. Duncombe, `OInfrared navigation ˜NPart I: An assessment of\nfeasibility (Periodical style), ´O IEEE Trans. Electron Devices, vol. ED-\n11, pp. 34-39, Jan. 1959.\n[4] S. Chen, B. Mulgrew, and P. M. Grant, A clustering technique\nfor digital communications channel equalization using radial basis\nfunction networks, ´O IEEE Trans. Neural Networks, vol. 4, pp. 570-\n578, July 1993.\n[5] R. W. Lucky, Automatic equalization for digital communication, Bell\nSyst. Tech. J., vol. 44, no. 4, pp. 547-588, Apr. 1965.\n[6] Gu, Shixiang, Holly, Ethan, Lillicrap, Timothy, and Levine, Sergey.\nDeep reinforcement learning for robotic manipulation with asyn-\nchronous off-policy updates. In IEEE International Conference on\nRobotics and Automation (ICRA), 2017.\n[7] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K. and Fei-Fei, L.,\n2009, June. Imagenet: A large-scale hierarchical image database. In\nComputer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE\nConference on (pp. 248-255). Ieee.\n[8] Schulman, J., Levine, S., Abbeel, P., Jordan, M. and Moritz, P., 2015,\nJune. Trust region policy optimization. In International Conference on\nMachine Learning (pp. 1889-1897).\n[9] Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov,\nO., 2017. Proximal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\n[10] Watkins, C.J. and Dayan, P., 1992. Q-learning. Machine learning, 8(3-\n4), pp.279-292.\n[11] Dorigo, M. and Di Caro, G., 1999. Ant colony optimization: a new\nmeta-heuristic. In Proceedings of the 1999 congress on evolutionary\ncomputation-CEC99 (Cat. No. 99TH8406) (Vol. 2, pp. 1470-1477).\nIEEE.\n",
  "categories": [
    "cs.RO",
    "cs.AI"
  ],
  "published": "2022-10-13",
  "updated": "2022-10-13"
}