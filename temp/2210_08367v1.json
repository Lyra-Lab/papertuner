{
  "id": "http://arxiv.org/abs/2210.08367v1",
  "title": "Active Learning with Neural Networks: Insights from Nonparametric Statistics",
  "authors": [
    "Yinglun Zhu",
    "Robert Nowak"
  ],
  "abstract": "Deep neural networks have great representation power, but typically require\nlarge numbers of training examples. This motivates deep active learning methods\nthat can significantly reduce the amount of labeled training data. Empirical\nsuccesses of deep active learning have been recently reported in the\nliterature, however, rigorous label complexity guarantees of deep active\nlearning have remained elusive. This constitutes a significant gap between\ntheory and practice. This paper tackles this gap by providing the first\nnear-optimal label complexity guarantees for deep active learning. The key\ninsight is to study deep active learning from the nonparametric classification\nperspective. Under standard low noise conditions, we show that active learning\nwith neural networks can provably achieve the minimax label complexity, up to\ndisagreement coefficient and other logarithmic terms. When equipped with an\nabstention option, we further develop an efficient deep active learning\nalgorithm that achieves $\\mathsf{polylog}(\\frac{1}{\\epsilon})$ label\ncomplexity, without any low noise assumptions. We also provide extensions of\nour results beyond the commonly studied Sobolev/H\\\"older spaces and develop\nlabel complexity guarantees for learning in Radon $\\mathsf{BV}^2$ spaces, which\nhave recently been proposed as natural function spaces associated with neural\nnetworks.",
  "text": "arXiv:2210.08367v1  [cs.LG]  15 Oct 2022\nActive Learning with Neural Networks: Insights from\nNonparametric Statistics\nYinglun Zhu\nUniversity of Wisconsin-Madison\nyinglun@cs.wisc.edu\nRobert Nowak\nUniversity of Wisconsin-Madison\nrdnowak@wisc.edu\nAbstract\nDeep neural networks have great representation power, but typically require large numbers of train-\ning examples. This motivates deep active learning methods that can signiﬁcantly reduce the amount of\nlabeled training data. Empirical successes of deep active learning have been recently reported in the liter-\nature, however, rigorous label complexity guarantees of deep active learning have remained elusive. This\nconstitutes a signiﬁcant gap between theory and practice. This paper tackles this gap by providing the\nﬁrst near-optimal label complexity guarantees for deep active learning. The key insight is to study deep\nactive learning from the nonparametric classiﬁcation perspective. Under standard low noise conditions,\nwe show that active learning with neural networks can provably achieve the minimax label complexity,\nup to disagreement coeﬃcient and other logarithmic terms. When equipped with an abstention option,\nwe further develop an eﬃcient deep active learning algorithm that achieves polylog( 1\nε) label complexity,\nwithout any low noise assumptions. We also provide extensions of our results beyond the commonly stud-\nied Sobolev/Hölder spaces and develop label complexity guarantees for learning in Radon BV2 spaces,\nwhich have recently been proposed as natural function spaces associated with neural networks.\n1\nIntroduction\nWe study active learning with neural network hypothesis classes, sometimes known as deep active learning.\nActive learning agent proceeds by selecting the most informative data points to label: The goal of active\nlearning is to achieve the same accuracy achievable by passive learning, but with much fewer label queries\n(Settles, 2009; Hanneke, 2014). When the hypothesis class is a set of neural networks, the learner further\nbeneﬁts from the representation power of deep neural networks, which has driven the successes of passive\nlearning in the past decade (Krizhevsky et al., 2012; LeCun et al., 2015). With these added beneﬁts, deep\nactive learning has become a popular research area, with empirical successes observed in many recent papers\n(Sener and Savarese, 2018; Ash et al., 2019; Citovsky et al., 2021; Ash et al., 2021; Kothawade et al., 2021;\nEmam et al., 2021; Ren et al., 2021). However, due to the diﬃculty of analyzing a set of neural networks,\nrigorous label complexity guarantees for deep active learning have remained largely elusive.\nTo the best of our knowledge, there are only two papers (Karzand and Nowak, 2020; Wang et al., 2021)\nthat have made the attempts at theoretically quantifying active learning gains with neural networks. While\ninsightful views are provided, these two works have their own limitations.\nThe guarantees provided in\nKarzand and Nowak (2020) only work in the 1d case where data points are uniformly sampled from [0, 1]\nand labeled by a well-seperated piece-wise constant function in a noise-free way (i.e., without any labeling\nnoise).\nWang et al. (2021) study deep active learning by linearizing the neural network at its random\ninitialization and then analyzing it as a linear function; moreover, as the authors agree, their error bounds\nand label complexity guarantees can in fact be vacuous in certain cases. Thus, it’s fair to say that up to now\nresearchers have not identiﬁed cases where deep active learning are provably near minimax optimal (or even\nwith provably non-vacuous guarantees), which constitutes a signiﬁcant gap between theory and practice.\nIn this paper, we bridge this gap by providing the ﬁrst near-optimal label complexity guarantees for deep\nactive learning. We obtain insights from the nonparametric setting where the conditional probability (of\ntaking a positive label) is assumed to be a smooth function (Tsybakov, 2004; Audibert and Tsybakov,\n1\n2007).\nPrevious nonparametric active learning algorithms proceed by partitioning the action space into\nexponentially many sub-regions (e.g., partitioning the unit cube [0, 1]d into ε−d sub-cubes each with volume\nεd), and then conducting local mean (or some higher-order statistics) estimation within each sub-region\n(Castro and Nowak, 2008; Minsker, 2012; Locatelli et al., 2017, 2018; Shekhar et al., 2021; Kpotufe et al.,\n2021). We show that, with an appropriately chosen set of neural networks that globally approximates the\nsmooth regression function, one can in fact recover the minimax label complexity for active learning, up to\ndisagreement coeﬃcient (Hanneke, 2007, 2014) and other logarithmic factors. Our results are established by\n(i) identifying the “right tools” to study neural networks (ranging from approximation results (Yarotsky, 2017,\n2018) to complexity measure of neural networks (Bartlett et al., 2019)), and (ii) developing novel extensions\nof agnostic active learning algorithms (Balcan et al., 2006; Hanneke, 2007, 2014) to work with a set of neural\nnetworks.\nWhile matching the minimax label complexity in nonparametric active learning is existing, such minimax\nresults scale as Θ(poly( 1\nε)) (Castro and Nowak, 2008; Locatelli et al., 2017) and do not resemble what is\npractically observed in deep active learning: A fairly accurate neural network classiﬁer can be obtained by\ntraining with only a few labeled data points. Inspired by recent results in parametric active learning with\nabstention (Puchkin and Zhivotovskiy, 2021; Zhu and Nowak, 2022), we develop an oracle-eﬃcient algorithm\nshowing that deep active learning provably achieves polylog( 1\nε) label complexity when equipped with an\nabstention option (Chow, 1970). Our algorithm not only achieves an exponential saving in label complexity\n(without any low noise assumptions), but is also highly practical: In real-world scenarios such as medical\nimaging, it makes more sense for the classiﬁer to abstain from making prediction on hard examples (e.g.,\nthose that are close to the boundary), and ask medical experts to make the judgments.\n1.1\nProblem setting\nLet X denote the instance space and Y denote the label space. We focus on the binary classiﬁcation problem\nwhere Y := {+1, −1}. The joint distribution over X × Y is denoted as DX Y. We use DX to denote the\nmarginal distribution over the instance space X, and use DY|x to denote the conditional distribution of Y with\nrespect to any x ∈X. We consider the standard active learning setup where x ∼DX but its label y ∼DY|x is\nonly observed after issuing a label query. We deﬁne η(x) := Py∼DY|x(y = +1) as the conditional probability\nof taking a positive label. The Bayes optimal classiﬁer h⋆can thus be expressed as h⋆(x) := sign(2η(x) −1).\nFor any classiﬁer h : X →Y, its (standard) error is calculated as err(h) := P(x,y)∼DXY(h(x) ̸= y); and its\n(standard) excess error is deﬁned as excess(h) := err(h) −err(h⋆). Our goal is to learn an accurate classiﬁer\nwith a small number of label querying.\nThe nonparametric setting.\nWe consider the nonparametric setting where the conditional probability\nη is characterized by a smooth function. Fix any α ∈N+, the Sobolev norm of a function f : X →R\nis deﬁned as ∥f∥Wα,∞:= maxα,|α|≤α ess supx∈X|Dαf(x)|, where α = (α1, . . . , αd), |α| = Pd\ni=1 αi and Dαf\ndenotes the standard α-th weak derivative of f. The unit ball in the Sobolev space is deﬁned as Wα,∞\n1\n(X) :=\n{f : ∥f∥Wα,∞≤1}. Following the convention of nonparametric active learning (Castro and Nowak, 2008;\nMinsker, 2012; Locatelli et al., 2017, 2018; Shekhar et al., 2021; Kpotufe et al., 2021), we assume X = [0, 1]d\nand η ∈Wα,∞\n1\n(X) (except in Section 4).\nNeural Networks.\nWe consider feedforward neural networks with Rectiﬁed Linear Unit (ReLU) activation\nfunction, which is deﬁned as ReLU(x) := max{x, 0}. Each neural network fdnn : X →R consists of several\ninput units (which corresponds to the covariates of x ∈X), one output unit (which corresponds to the\nprediction in R), and multiple hidden computational units. Each hidden computational unit takes inputs\n{xi}N\ni=1 (which are outputs from previous layers) and perform the computation ReLU(PN\ni=1 wixi + b) with\nadjustable parameters {wi}N\ni=1 and b; the output unit performs the same operation, but without the ReLU\nnonlinearity. We use W to denote the total number of parameters of a neural network, and L to denote the\ndepth of the neural network.\n2\n1.2\nContributions and paper organization\nNeural networks are known to be universal approximators (Cybenko, 1989; Hornik, 1991). In this paper,\nwe argue that, in both passive and active regimes, the universal approximatability makes neural networks\n“universal classiﬁers” for classiﬁcation problems: With an appropriately chosen set of neural networks, one can\nrecover known minimax rates (up to disagreement coeﬃcients in the active setting) in the rich nonparametric\nregimes.1 We provide informal statements of our main results in the sequel, with detailed statements and\nassociated deﬁnitions/algorithms deferred to later sections.\nIn Section 2, we analyze the label complexity of deep active learning under the standard Tsybakov noise\ncondition with smoothness parameter β ≥0 (Tsybakov, 2004). Let Hdnn be an appropriately chosen set of\nneural network classiﬁers and denote θHdnn(ε) as the disagreement coeﬃcient (Hanneke, 2007, 2014) at level\nε. We develop the following label complexity guarantees for deep active learning.\nTheorem 1 (Informal). There exists an algorithm that returns a neural network classiﬁer bh ∈Hdnn with\nexcess error eO(ε) after querying eO(θHdnn(ε\nβ\n1+β ) · ε−d+2α\nα+αβ ) labels.\nThe label complexity presented in Theorem 1 matches the active learning lower bound Ω(ε−d+2α\nα+αβ ) (Locatelli\net al., 2017) up to the dependence on the disagreement coeﬃcient (and other logarithmic factors). Since\nθHdnn(ε) ≤ε−1 by deﬁnition, the label complexity presented in Theorem 1 is never worse than the passive\nlearning rates eΘ(ε−d+2α+αβ\nα+αβ\n) (Audibert and Tsybakov, 2007).\nWe also discover conditions under which\nthe disagreement coeﬃcient with respect to a set of neural network classiﬁers can be properly bounded,\ni.e., θHdnn(ε) = o(ε−1) (implying strict improvement over passive learning) and θHdnn(ε) = o(1) (implying\nmatching active learning lower bound).\nIn Section 3, we develop label complexity guarantees for deep active learning when an additional abstention\noption is allowed (Chow, 1970; Puchkin and Zhivotovskiy, 2021; Zhu and Nowak, 2022). Suppose a cost (e.g.\n0.49) that is marginally smaller than random guessing (which has expected cost 0.5) is incurred whenever\nthe classiﬁer abstains from making a predication, we develop the following label complexity guarantees for\ndeep active learning.\nTheorem 2 (Informal). There exists an eﬃcient algorithm that constructs a neural network classiﬁer bhdnn\nwith Chow’s excess error eO(ε) after querying polylog( 1\nε) labels.\nThe above polylog( 1\nε) label complexity bound is achieved without any low noise assumptions. Such exponen-\ntial label savings theoretically justify the great empirical performances of deep active learning observed in\npractice (e.g., in Sener and Savarese (2018)): It suﬃces to label a few data points to achieve a high accuracy\nlevel. Moreover, apart from an initialization step, our algorithm (Algorithm 4) developed for Theorem 2\ncan be eﬃciently implemented in eO(ε−1) time, given a convex loss regression oracle over an appropriately\nchosen set of neural networks; in practice, the regression oracle can be approximated by running stochastic\ngradient descent.\nTechnical contributions.\nBesides identifying the “right tools” (ranging from approximation results (Yarot-\nsky, 2017, 2018) to complexity analyses (Bartlett et al., 2019)) to analyze deep active learning, our theoretical\nguarantees are empowered by novel extensions of active learning algorithms under neural network approxima-\ntions. In particular, we deal with approximation error in active learning under Tsybakov noise, and identify\nconditions that greatly relax the approximation requirement in the learning with abstention setup; we also\nanalyze the disagreement coeﬃcient, both classiﬁer-based and value function-based, with a set of neural\nnetworks.These analyses together lead to our main results for deep active learning (e.g., Theorem 1 and\nTheorem 2). More generally, we establish a bridge between approximation theory and active learning; we\nprovide these general guarantees in Appendix B (under Tsybakov noise) and Appendix D (with the absten-\ntion option), which can be of independent interests. Beneﬁted from these generic algorithms and guarantees,\n1As a byproduct, our results also provide a new perspective on nonparametric active learning through the lens of neural\nnetwork approximations. Nonparametric active learning was previously tackled through space partitioning and local estimations\nover exponentially many sub-regions (Castro and Nowak, 2008; Minsker, 2012; Locatelli et al., 2017, 2018; Shekhar et al., 2021;\nKpotufe et al., 2021).\n3\nin Section 4, we extend our results into learning smooth functions in the Radon BV2 space (Ongie et al.,\n2020; Parhi and Nowak, 2021, 2022a,b; Unser, 2022), which is recently proposed as a natural space to analyze\nneural networks.\n1.3\nRelated work\nActive learning concerns about learning accurate classiﬁers without extensive human labeling. One of the\nearliest work of active learning dates back to the CAL algorithm proposed by Cohn et al. (1994), which set\nthe cornerstone for disagreement-based active learning. Since then, a long line of work have been developed,\neither directly working with a set classiﬁer (Balcan et al., 2006; Hanneke, 2007; Dasgupta et al., 2007;\nBeygelzimer et al., 2009, 2010; Huang et al., 2015; Cortes et al., 2019) or work with a set of regression\nfunctions (Krishnamurthy et al., 2017, 2019).\nThese work mainly focus on the parametric regime (e.g.,\nlearning with a set of linear classiﬁers), and their label complexities rely on the boundedness of the so-called\ndisagreement coeﬃcient (Hanneke, 2007, 2014; Friedman, 2009). Active learning in the nonparametric regime\nhas been analyzed in Castro and Nowak (2008); Minsker (2012); Locatelli et al. (2017, 2018); Kpotufe et al.\n(2021). These algorithms rely on partitioning of the input space X ⊆[0, 1]d into exponentially (in dimension)\nmany small cubes, and then conduct local mean (or some higher-order statistics) estimation within each\nsmall cube.\nIt is well known that, in the worst case, active learning exhibits no label complexity gains over the passive\ncounterpart (Kääriäinen, 2006). To bypass these worst-case scenarios, active learning has been popularly\nanalyzed under the so-called Tsybakov low noise conditions (Tsybakov, 2004). Under Tsybakov noise condi-\ntions, active learning has been shown to be strictly superior than passive learning in terms of label complexity\n(Castro and Nowak, 2008; Locatelli et al., 2017). Besides analyzing active learning under favorable low noise\nassumptions, more recently, researchers consider active learning with an abstention option and analyze its\nlabel complexity under Chow’s error (Chow, 1970). In particular, Puchkin and Zhivotovskiy (2021); Zhu\nand Nowak (2022) develop active learning algorithms with polylog( 1\nε) label complexity when analyzed under\nChow’s excess error. Shekhar et al. (2021) study nonparametric active learning under a diﬀerent notion\nof the Chow’s excess error, and propose algorithms with poly( 1\nε) label complexity; their algorithms follow\nsimilar procedures of those partition-based nonparametric active learning algorithms (e.g., Minsker (2012);\nLocatelli et al. (2017)).\nInspired by the success of deep learning in the passive regime, active learning with neural networks has been\nextensively explored in recent years (Sener and Savarese, 2018; Ash et al., 2019; Citovsky et al., 2021; Ash\net al., 2021; Kothawade et al., 2021; Emam et al., 2021; Ren et al., 2021). Great empirical performances\nare observed in these papers, however, rigorous label complexity guarantees have largely remains elusive\n(except in Karzand and Nowak (2020); Wang et al. (2021), with limitations discussed before). We bridge the\ngap between practice and theory by providing the ﬁrst near-optimal label complexity guarantees for deep\nactive learning. Our results are built upon approximation results of deep neural networks (Yarotsky, 2017,\n2018; Parhi and Nowak, 2022b) and VC/pseudo dimension analyses of neural networks with given structures\n(Bartlett et al., 2019).\n2\nLabel complexity of deep active learning\nWe analyze the label complexity of deep active learning in this section. We ﬁrst introduce the Tsybakov\nnoise condition in Section 2.1, and then identify the “right tools” to analyze classiﬁcation problems with\nneural network classiﬁers in Section 2.2 (where we also provide passive learning guarantees). We establish\nour main active learning guarantees in Section 2.3.\n2.1\nTsybakov noise condition\nIt is well known that active learning exhibits no label complexity gains over the passive counterpart without\nadditional low noise assumptions (Kääriäinen, 2006). We next introduce the Tsybokov low noise condition\n(Tsybakov, 2004), which has been extensively analyzed in active learning literature.\n4\nDeﬁnition 1 (Tsybakov noise). The marginal distribution DX satisﬁes the Tsybakov noise condition with\nparameter β ≥0 and universal constants c ≥1 if, ∀t > 0,\nPx∼DX (x ∈X : |η(x) −1/2| ≤τ) ≤c τ β.\nThe case with β = 0 corresponds to the general case without any low noise conditions, where no active\nlearning algorithm can outperform the passive counterpart (Audibert and Tsybakov, 2007; Locatelli et al.,\n2017). We use P(α, β) to denote the set of distributions satisfying: (i) the smoothness conditions introduced\nin Section 1.1 with parameter α > 0; and (ii) the Tsybakov low noise condition (i.e., Deﬁnition 1) with\nparameter β ≥0. We assume DX Y ∈P(α, β) in the rest of Section 2. As in Castro and Nowak (2008);\nHanneke (2014), we assume the knowledge of noise/smoothness parameters.\n2.2\nApproximation and expressiveness of neural networks\nNeural networks are known to be universal approximators (Cybenko, 1989; Hornik, 1991): For any continuous\nfunction g : X →R and any error tolerance κ > 0, there exists a large enough neural network fdnn such that\n∥fdnn −g∥∞:= supx∈X |fdnn(x) −g(x)| ≤κ. Recently, non-asympototic approximation rates by ReLU neural\nnetworks have been developed for smooth functions in the Sobolev space, which we restate in the following.2\nTheorem 3 (Yarotsky (2017)). Fix any κ > 0. For any f ⋆= η ∈Wα,∞\n1\n([−1, 1]d), there exists a neural\nnetwork fdnn with W = O(κ−d\nα log 1\nκ) total number of parameters arranged in L = O(log 1\nκ) layers such that\n∥fdnn −f ⋆∥∞≤κ.\nThe architecture of the neural network fdnn appearing in the above theorem only depends on the smooth\nfunction space Wα,∞\n1\n([−1, 1]d), but otherwise is independent of the true regression function f ⋆; also see\nYarotsky (2017) for details. Let Fdnn denote the set of neural network regression functions with the same\narchitecture. We construct a set of neural network classiﬁers by thresholding the regression function at 1\n2,\ni.e., Hdnn := {hf := sign(2f(x) −1) : f ∈Fdnn}. The next result concerns about the expressiveness of the\nneural network classiﬁers, in terms of a well-known complexity measure: the VC dimension (Vapnik and\nChervonenkis, 1971).\nTheorem 4 (Bartlett et al. (2019)). Let Hdnn be a set of neural network classiﬁers of the same architecture\nand with W parameters arranged in L layers. We then have\nΩ(WL log(W/L)) ≤VCdim(Hdnn) ≤O(WL log(W)).\nWith these tools, we can construct a set of neural network classiﬁers Hdnn such that (i) the best in-class\nclassiﬁer ˇh ∈Hdnn has small excess error, and (ii) Hdnn has a well-controlled VC dimension that is proportional\nto smooth/noise parameters. More speciﬁcally, we have the following proposition.\nProposition 1. Suppose DX Y ∈P(α, β). One can construct a set of neural network classiﬁer Hdnn such\nthat the following two properties hold simultaneously:\ninf\nh∈Hdnn err(h) −err(h⋆) = O(ε)\nand\nVCdim(Hdnn) = eO(ε−\nd\nα(1+β) ).\nWith the approximation results obtained above, to learn a classiﬁer with O(ε) excess error, one only needs to\nfocus on a set of neural networks Hdnn with a well-controlled VC dimension. As a warm-up, we ﬁrst analyze\nthe label complexity of such procedure in the passive regime (with fast rates).\nTheorem 5. Suppose DX Y ∈P(α, β). Fix any ε, δ > 0. Let Hdnn be the set of neural network classiﬁers\nconstructed in Proposition 1. With n = eO(ε−d+2α+αβ\nα(1+β) ) i.i.d. sampled points, with probability at least 1 −δ,\nthe empirical risk minimizer bh ∈Hdnn achieves excess error O(ε).\n2As in Yarotsky (2017), we hide constants that are potentially α-dependent and d-dependent into the Big-Oh notation.\n5\nThe label complexity results obtained in Theorem 5 matches, up to logarithmic factors, the passive learning\nlower bound Ω(ε−d+2α+αβ\nα(1+β) ) established in Audibert and Tsybakov (2007), indicating our proposed learning\nprocedure with a set of neural networks is near minimax optimal.\n2.3\nDeep active learning and guarantees\nThe passive learning procedure presented in the previous section treats every data point equally, i.e., it\nrequests the label of every data point.\nActive learning reduces the label complexity by only querying\nlabels of data points that are “more important”. We present deep active learning results in this section. Our\nalgorithm (Algorithm 1) is inspired by RobustCAL (Balcan et al., 2006; Hanneke, 2007, 2014) and the seminal\nCAL algorithm (Cohn et al., 1994); we call our algorithm NeuralCAL to emphasize that it works with a set\nof neural networks.\nFor any accuracy level ε > 0, NeuralCAL ﬁrst initialize a set of neural network classiﬁers H0 := Hdnn such\nthat (i) the best in-class classiﬁer ˇh := arg minh∈Hdnn err(h) has excess error at most O(ε), and (ii) the\nVC dimension of Hdnn is upper bounded by eO(ε−\nd\nα(1+β) ) (see Section 2.2 for more details).\nNeuralCAL\nthen runs in epochs of geometrically increasing lengths. At the beginning of epoch m, based on previously\nlabeled data points, NeuralCAL updates a set of active classiﬁer Hm such that, with high probability, the\nbest classiﬁer ˇh remains uneliminated. Within each epoch m, NeuralCAL only queries the label y of a data\npoint x if it lies in the region of disagreement with respect to the current active set of classiﬁer Hm, i.e.,\nDIS(Hm) := {x ∈X : ∃h1, h2 ∈Hm s.t. h1(x) ̸= h2(x)}. NeuralCAL returns any classiﬁer bh ∈Hm that\nremains uneliminated after M −1 epoch.\nAlgorithm 1 NeuralCAL\nInput: Accuracy level ε ∈(0, 1), conﬁdence level δ ∈(0, 1).\n1: Let Hdnn be a set of neural networks classiﬁers constructed in Proposition 1.\n2: Deﬁne T := ε−2+β\n1+β · VCdim(Hdnn), M := ⌈log2 T ⌉, τm := 2m for m ≥1 and τ0 := 0.\n3: Deﬁne ρm := O\n\u0012\u0010\nVCdim(Hdnn)·log(τm−1)·log(M/δ)\nτm−1\n\u0011 1+β\n2+β \u0013\nfor m ≥2 and ρ1 := 1.\n4: Deﬁne bRm(h) := Pτm−1\nt=1\nQt\n1(h(xt) ̸= yt) with the convention that P0\nt=1 . . . = 0.\n5: Initialize H0 := Hdnn.\n6: for epoch m = 1, 2, . . . , M do\n7:\nUpdate active set Hm :=\nn\nh ∈Hm−1 : bRm(h) ≤infh∈Hm−1 bRm(h) + τm−1 · ρm\no\n8:\nif epoch m = M then\n9:\nReturn any classiﬁer bh ∈HM.\n10:\nfor time t = τm−1 + 1, . . . , τm do\n11:\nObserve xt ∼DX . Set Qt :=\n1(xt ∈DIS(Hm)).\n12:\nif Qt = 1 then\n13:\nQuery the label yt of xt.\nSince NeuralCAL only queries labels of data points lying in the region of disagreement, its label complexity\nshould intuitively be related to how fast the region of disagreement shrinks. More formally, the rate of collapse\nof the (probability measure of) region of disagreement is captured by the (classiﬁer-based) disagreement\ncoeﬃcient (Hanneke, 2007, 2014), which we introduce next.\nDeﬁnition 2 (Classiﬁer-based disagreement coeﬃcient). For any ε0 and classiﬁer h ∈H, the classiﬁer-based\ndisagreement coeﬃcient of h is deﬁned as\nθH,h(ε0) := sup\nε>ε0\nPx∼DX (DIS(BH(h, ε)))\nε\n∨1,\nwhere BH(h, ε) := {g ∈H : P(x ∈X : g(x) ̸= h(x)) ≤ε}. We also deﬁne θH(ε0) := suph∈H θH,h(ε0).\n6\nThe guarantees of NeuralCAL follows from a more general analysis of RobustCAL under approximation. In\nparticular, to achieve fast rates (under Tsybakov noise), previous analysis of RobustCAL requires that the\nBayes classiﬁer is in the class (or a Bernstein condition for every h ∈H) (Hanneke, 2014). These requirements\nare stronger compared to what we have in the case with neural network approximations. Our analysis extends\nthe understanding of RobustCAL under approximation. We defer such general analysis to Appendix B, and\npresent the following guarantees.\nTheorem 6. Suppose DX Y ∈P(α, β). Fix any ε, δ > 0. With probability at least 1 −δ, Algorithm 1 returns\na classiﬁer bh ∈Hdnn with excess error eO(ε) after querying eO(θHdnn(ε\nβ\n1+β ) · ε−d+2α\nα+αβ ) labels.\nWe next discuss in detail the label complexity of deep active learning proved in Theorem 6.\n• Ignoring the dependence on disagreement coeﬃcient, the label complexity appearing in Theorem 6\nmatches, up to logarithmic factors, the lower bound Ω(ε−d+2α\nα+αβ ) for active learning (Locatelli et al.,\n2017). At the same time, the label complexity appearing in Theorem 6 is never worse than the passive\ncounterpart (i.e., eΘ(ε−d+2α+αβ\nα(1+β) ) since θHdnn(ε\nβ\n1+β ) ≤ε−\nβ\n1+β .\n• We also identify cases when θHdnn(ε\nβ\n1+β ) = o(ε−\nβ\n1+β ), indicating strict improvement over passive learning\n(e.g., when DX is supported on countably many data points), and when θHdnn(ε\nβ\n1+β ) = O(1), indicating\nmatching the minimax active lower bound (e.g., when DX Y satisﬁes conditions such as decomposibility\ndeﬁned in Deﬁnition 4. See Appendix C.2 for detailed discussion).3\nOur algorithm and theorems lead to the following results, which could beneﬁt both deep active learning and\nnonparametric learning communities.\n• Near minimax optimal label complexity for deep active learning. While empirical successes of\ndeep active learning have been observed, rigorous label complexity analysis remains elusive except for two\nattempts made in Karzand and Nowak (2020); Wang et al. (2021). The guarantees provided in Karzand\nand Nowak (2020) only work in very special cases (i.e., data uniformly sampled from [0, 1] and labeled by\nwell-separated piece-constant functions in a noise-free way). Wang et al. (2021) study deep active learning\nin the NTK regime by linearizing the neural network at its random initialization and analyzing it as a\nlinear function; moreover, as the authors agree, their error bounds and label complexity guarantees are\nvacuous in certain cases. On the other hand, our guarantees are minimax optimal, up to disagreement\ncoeﬃcient and other logarithmic factors, which bridge the gap between theory and practice in deep active\nlearning.\n• New perspective on nonparametric learning. Nonparametric learning of smooth functions have\nbeen mainly approached by partitioning-based methods (Tsybakov, 2004; Audibert and Tsybakov, 2007;\nCastro and Nowak, 2008; Minsker, 2012; Locatelli et al., 2017, 2018; Kpotufe et al., 2021) : Partition the\nunit cube [0, 1]d into exponentially (in dimension) many sub-cubes and conduct local mean estimation\nwithin each sub-cube (which additionally requires a strictly stronger membership querying oracle). Our\nresults show that, in both passive and active settings, one can learn globally with a set of neural networks\nand achieve near minimax optimal label complexities.\n3\nDeep active learning with abstention: Exponential speedups\nWhile the theoretical guarantees provided in Section 2 are near minimax optimal, the label complexity scales\nas poly( 1\nε), which doesn’t match the great empirical performance observed in deep active learning. In this\nsection, we ﬁll in this gap by leveraging the idea of abstention and provide a deep active learning algorithm\n3We remark that disagreement coeﬃcient is usually bounded/analyzed under additional assumptions on DXY, even for\nsimple cases with a set of linear classiﬁers (Friedman, 2009; Hanneke, 2014). The label complexity guarantees of partition-based\nnonparametric active algorithms (e.g., Castro and Nowak (2008)) do not depend on the disagreement coeﬃcient, but they are\nanalyzed under stronger assumptions, e.g., they require the strictly stronger membership querying oracle. See Wang (2011) for\na discussion. We left a comprehensive analysis of the disagreement coeﬃcient with a set of neural network classiﬁers for future\nwork.\n7\nthat achieves exponential label savings. We introduce the concepts of abstention and Chow’s excess error in\nSection 3.1, and provide our label complexity guarantees in Section 3.2.\n3.1\nActive learning without low noise conditions\nThe previous section analyzes active learning under Tsybakov noise, which has been extensively studied in\nthe literature since Castro and Nowak (2008). More recently, promising results are observed in active learning\nunder Chow’s excess error, but otherwise without any low noise assumption (Puchkin and Zhivotovskiy, 2021;\nZhu and Nowak, 2022). We introduce this setting in the following.\nAbstention and Chow’s error (Chow, 1970).\nWe consider classiﬁer of the form bh : X →Y ∪{⊥}\nwhere ⊥denotes the action of abstention. For any ﬁxed 0 < γ < 1\n2, the Chow’s error is deﬁned as\nerrγ(bh) := P(x,y)∼DXY(bh(x) ̸= y,bh(x) ̸= ⊥) + (1/2 −γ) · P(x,y)∼DXY(bh(x) = ⊥).\nThe parameter γ can be chosen as a small constant, e.g., γ = 0.01, to avoid excessive abstention: The price\nof abstention is only marginally smaller than random guess (which incurs cost 0.5). The Chow’s excess error\nis then deﬁned as excessγ(bh) := errγ(bh) −err(h⋆) (Puchkin and Zhivotovskiy, 2021).\nAt a high level, analyzing with Chow’s excess error allows slackness in predications of hard examples (e.g.,\ndata points whose η(x) is close to 1\n2) by leveraging the power of abstention. Puchkin and Zhivotovskiy (2021);\nZhu and Nowak (2022) show that polylog( 1\nε) is always achievable in the parametric settings. We generalize\ntheir results to the nonparametric setting and analyze active learning with a set of neural networks.\n3.2\nExponential speedups with abstention\nIn this section, we work with a set of neural network regression functions Fdnn : X →[0, 1] (that approximates\nη) and then construct classiﬁers h : X →Y ∪{⊥} with an additional abstention action. To work with a set\nof regression functions Fdnn, we analyze its “complexity” from the lenses of pseudo dimension Pdim(Fdnn)\n(Pollard, 1984; Haussler, 1989, 1995) and value function disagreement coeﬃcient θval\nFdnn(ι) (for some ι > 0)\n(Foster et al., 2020). We defer detailed deﬁnitions of these complexity measures to Appendix D.1.\nWe now present NeuralCAL++ (Algorithm 2), a deep active learning algorithm that leverages the power of\nabstention. NeuralCAL++ ﬁrst initialize a set of set of neural network regression functions Fdnn by applying a\npreprocessing step on top of the set of regression functions obtained from Theorem 3 with a carefully chosen\napproximation level κ. The preprocessing step mainly contains two actions: (1) clipping fdnn : X →R into\nˇfdnn : X →[0, 1] (since we obviously have η(x) ∈[0, 1]); and (2) ﬁltering out fdnn ∈Fdnn that are clearly\nnot a good approximation of η. After initialization, NeuralCAL++ runs in epochs of geometrically increasing\nlengths. At the beginning of epoch m ∈[M], NeuralCAL++ (implicitly) constructs an active set of regression\nfunctions Fm that are “close” to the true conditional probability η. For any x ∼DX , NeuralCAL++ constructs\na lower bound lcb(x; Fm) := inff∈Fm f(x) and an upper bound ucb(x; Fm) := supf∈Fm f(x) as a conﬁdence\nrange of η(x) (based on Fm). An empirical classiﬁer with an abstention option bhm : X →{+1, −1, ⊥} and\na query function gm : X →{0, 1} are then constructed based on the conﬁdence range (and the abstention\nparameter γ). For any time step t within epoch m, NeuralCAL++ queries the label of the observed data point\nxt if and only if Qt := gm(xt) = 1. NeuralCAL++ returns bhM as the learned classiﬁer.\nNeuralCAL++ is adapted from the algorithm developed in Zhu and Nowak (2022), but with novel extensions.\nIn particular, the algorithm presented in Zhu and Nowak (2022) requires the existence of a f ∈F such\nthat ∥f −η∥∞≤ε (to achieve ε Chow’s excess error), Such an approximation requirement directly leads\nto poly( 1\nε) label complexity in the nonparametric setting, which is unacceptable. The initialization step of\nNeuralCAL++ (line 1) is carefully chosen to ensure that Pdim(Fdnn), θval\nFdnn(γ/4) = eO(poly(1/γ)); together with\na sharper analysis of concentration results, these conditions help us derive the following deep active learning\nguarantees (also see Appendix D for a more general guarantee).\n8\nAlgorithm 2 NeuralCAL++\nInput: Accuracy level ε ∈(0, 1), conﬁdence level δ ∈(0, 1), abstention parameter γ ∈(0, 1/2).\n1: Let Fdnn be a set of neural network regression functions obtained by (i) applying Theorem 3 with\nan appropriate approximation level κ (which satisﬁes\n1\nκ = poly( 1\nγ ) polylog( 1\nε γ )), and (ii) applying a\npreprocessing step on the set of neural networks obtained from step (i). See Appendix E for details.\n2: Deﬁne T :=\nθval\nFdnn(γ/4)·Pdim(Fdnn)\nε γ\n, M := ⌈log2 T ⌉, and Cδ := O(Pdim(Fdnn) · log(T/δ)).\n3: Deﬁne τm := 2m for m ≥1, τ0 := 0, and βm := 3(M −m + 1)Cδ.\n4: Deﬁne bRm(f) := Pτm−1\nt=1\nQt( bf(xt) −yt)2 with the convention that P0\nt=1 . . . = 0.\n5: for epoch m = 1, 2, . . . , M do\n6:\nGet bfm := arg minf∈Fdnn\nPτm−1\nt=1\nQt(f(xt) −yt)2.\n7:\n(Implicitely) Construct active set Fm :=\nn\nf ∈Fdnn : bRm(f) ≤bRm( bfm) + βm\no\n.\n8:\nConstruct classiﬁer bhm : X →{+1, −1, ⊥} as\nbhm(x) :=\n(\n⊥,\nif [lcb(x; Fm) −γ\n4 , ucb(x; Fm) + γ\n4] ⊆\n\u0002 1\n2 −γ, 1\n2 + γ\n\u0003\n;\nsign(2 bfm(x) −1),\no.w.\nand query function gm(x) :=\n1\n\u0000 1\n2 ∈\n\u0000lcb(x; Fm) −γ\n4 , ucb(x; Fm) + γ\n4\n\u0001\u0001\n·\n1(bhm(x) ̸= ⊥).\n9:\nif epoch m = M then\n10:\nReturn classiﬁer bhM.\n11:\nfor time t = τm−1 + 1, . . . , τm do\n12:\nObserve xt ∼DX . Set Qt := gm(xt).\n13:\nif Qt = 1 then\n14:\nQuery the label yt of xt.\nTheorem 7. Fix any ε, δ, γ > 0. With probability at least 1 −δ, Algorithm 2 (with an appropriate initial-\nization at line 1) returns a classiﬁer bh with Chow’s excess error eO(ε) after querying poly( 1\nγ ) · polylog( 1\nε δ)\nlabels.\nWe discuss two important aspects of Algorithm 2/Theorem 7 in the following, i.e., exponential savings and\ncomputational eﬃciency. We defer more detailed discussions to Appendix F.1.\n• Exponential speedups. Theorem 7 shows that, equipped with an abstention option, deep active learn-\ning enjoys polylog( 1\nε) label complexity. This provides theoretical justiﬁcations for great empirical results\nof deep active learning observed in practice. Moreover, Algorithm 2 outputs a classiﬁer that abstains\nproperly, i.e., it abstains only if abstention is the optimal choice; such a property further implies polylog( 1\nε)\nlabel complexity under standard excess error and Massart noise (Massart and Nédélec, 2006).\n• Computational eﬃciency. Suppose one can eﬃciently implement a (weighted) square loss regression\noracle over the initialized set of neural networks Fdnn: Given any set S of weighted examples (w, x, y) ∈\nR+ × X × Y as input, the regression oracle outputs bfdnn := arg minf∈Fdnn\nP\n(w,x,y)∈S w(f(x) −y)2 .4\nAlgorithm 2 can then be eﬃciently implemented with poly( 1\nγ ) · 1\nε oracle calls.\nWhile the label complexity obtained in Theorem 7 has desired dependence on polylog( 1\nε), its dependence on\nγ can be of order γ−poly(d). Our next result shows that, however, such dependence is unavoidable even in\nthe case of learning a single ReLU function.\nTheorem 8. Fix any γ ∈(0, 1/8). For any accuracy level ε suﬃciently small, there exists a problem instance\nsuch that (1) η ∈W1,∞\n1\n(X) and is of the form η(x) := ReLU(⟨w, x⟩+ a) + b; and (2) for any active learning\nalgorithm, it takes at least γ−Ω(d) labels to identify an ε-optimal classiﬁer, for either standard excess error\nor Chow’s excess error (with parameter γ).\n4In practice, one can approximate this oracle by running stochastic gradient descent.\n9\n4\nExtensions\nPrevious results are developed in the commonly studied Sobolev/Hölder spaces. Our techniques, however,\nare generic and can be adapted to other function spaces, given neural network approximation results. In this\nsection, we provide extensions of our results to the Radon BV2 space, which was recently proposed as the\nnatural function space associated with ReLU neural networks (Ongie et al., 2020; Parhi and Nowak, 2021,\n2022a,b; Unser, 2022).5\nThe Radon BV2 space.\nThe Radon BV2 unit ball over domain X is deﬁned as R BV2\n1(X) := {f :\n∥f∥R BV2(X ) ≤1}, where ∥f∥R BV2(X ) denotes the Radon BV2 norm of f over domain X.6 Following Parhi\nand Nowak (2022b), we assume X = {x ∈Rd : ∥x∥2 ≤1} and η ∈R BV2\n1(X).\nThe Radon BV2 space naturally contains neural networks of the form fdnn(x) = PK\nk=1 vi · ReLU(w⊤\ni x + bi).\nOn the contrary, such fdnn doesn’t lie in any Sobolev space of order α ≥2 (since fdnn doesn’t have second\norder weak derivative). Thus, if η takes the form of the aforementioned neural network (e.g., η = fdnn),\napproximating η up to κ from a Sobolev perspective requires eO(κ−d) total parameters, which suﬀers from\nthe curse of dimensionality. On the other side, however, such bad dependence on dimensionality goes away\nwhen approximating from a Radon BV2 perspective, as shown in the following theorem.\nTheorem 9 (Parhi and Nowak (2022b)). Fix any κ > 0. For any f ⋆∈R BV2\n1(X), there exists a one-hidden\nlayer neural network fdnn of width K = O(κ−2d\nd+3 ) such that ∥f ⋆−fdnn∥∞≤κ.\nEquipped with this approximation result, we provide the active learning guarantees for learning a smooth\nfunction within the Radon BV2 unit ball as follows.\nTheorem 10. Suppose η ∈R BV2\n1(X) and the Tsybakov noise condition is satisﬁed with parameter β ≥0.\nFix any ε, δ > 0. There exists an algorithm such that, with probability at least 1 −δ, it learns a classiﬁer\nbh ∈Hdnn with excess error eO(ε) after querying eO(θHdnn(ε\nβ\n1+β ) · ε−\n4d+6\n(1+β)(d+3) ) labels.\nCompared to the label complexity obtained in Theorem 6, the label complexity obtained in the above theorem\ndoesn’t suﬀer from the curse of dimensionality: For d large enough, the above label complexity scales as ε−O(1)\nyet label complexity in Theorem 6 scales as ε−O(d). Active learning guarantees under Chow’s excess error in\nthe Radon BV2 space are similar to results presented in Theorem 7, and are thus deferred to Appendix G.\n5\nDiscussion\nWe provide the ﬁrst near-optimal deep active learning guarantees, under both standard excess error and\nChow’s excess error.\nOur results are powered by generic algorithms and analyses developed for active\nlearning that bridge approximation guarantees into label complexity guarantees. We outline some natural\ndirections for future research below.\n• Disagreement coeﬃcients for neural networks. While we have provided some results regarding\nthe disagreement coeﬃcients for neural networks, we believe a comprehensive investigation on this topic\nis needed. For instance, can we discover more general settings where the classiﬁer-based disagreement\ncoeﬃcient can be upper bounded by O(1)? It is also interesting to explore sharper analyses on the value\nfunction disagreement coeﬃcient.\n• Adaptivity in deep active learning. Our current results are established with the knowledge of some\nproblem-dependent parameters, e.g., the smoothness parameters regarding the function spaces and the\nnoise levels. It will be interesting to see if one can develop algorithms that can automatically adapt to\nunknown parameters, e.g., by leveraging techniques developed in Locatelli et al. (2017, 2018).\n5Other extensions are also possible given neural network approximation results, e.g., recent results established in Lu et al.\n(2021).\n6We provide more mathematical backgrounds and associated deﬁnitions in Appendix G.\n10\nAcknowledgements\nThe authors would like to thank Rahul Parhi for many helpful discussions regarding his papers. We also\nwould like to thank anonymous reviewers for their constructive comments. This work is partially supported\nby NSF grant 1934612 and AFOSR grant FA9550-18-1-0166.\nReferences\nAlekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the\nmonster: A fast and simple algorithm for contextual bandits. In International Conference on Machine\nLearning, pages 1638–1646. PMLR, 2014.\nMartin Anthony. Uniform glivenko-cantelli theorems and concentration of measure in the mathematical\nmodelling of learning. Research Report LSE-CDAM-2002–07, 2002.\nJordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Sham Kakade. Gone ﬁshing: Neural active learning\nwith ﬁsher embeddings. Advances in Neural Information Processing Systems, 34, 2021.\nJordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch\nactive learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671, 2019.\nJean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classiﬁers. The Annals of\nstatistics, 35(2):608–633, 2007.\nMaria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In Proceedings of\nthe 23rd international conference on Machine learning, pages 65–72, 2006.\nPeter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension and\npseudodimension bounds for piecewise linear neural networks. The Journal of Machine Learning Research,\n20(1):2285–2301, 2019.\nAlina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learning. In Proceed-\nings of the 26th annual international conference on machine learning, pages 49–56, 2009.\nAlina Beygelzimer, Daniel J Hsu, John Langford, and Tong Zhang. Agnostic active learning without con-\nstraints. Advances in neural information processing systems, 23, 2010.\nStéphane Boucheron, Olivier Bousquet, and Gábor Lugosi. Theory of classiﬁcation: A survey of some recent\nadvances. ESAIM: probability and statistics, 9:323–375, 2005.\nRui M Castro and Robert D Nowak. Minimax bounds for active learning. IEEE Transactions on Information\nTheory, 54(5):2339–2353, 2008.\nCK Chow. On optimum recognition error and reject tradeoﬀ. IEEE Transactions on information theory, 16\n(1):41–46, 1970.\nGui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin Rostamizadeh,\nand Sanjiv Kumar. Batch active learning at scale. Advances in Neural Information Processing Systems,\n34, 2021.\nDavid Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Machine learning,\n15(2):201–221, 1994.\nCorinna Cortes, Giulia DeSalvo, Mehryar Mohri, Ningshan Zhang, and Claudio Gentile. Active learning\nwith disagreement graphs. In International Conference on Machine Learning, pages 1379–1387. PMLR,\n2019.\nGeorge Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals\nand systems, 2(4):303–314, 1989.\n11\nSanjoy Dasgupta, Daniel J Hsu, and Claire Monteleoni. A general agnostic active learning algorithm. Ad-\nvances in neural information processing systems, 20, 2007.\nZeyad Ali Sami Emam, Hong-Min Chu, Ping-Yeh Chiang, Wojciech Czaja, Richard Leapman, Micah Gold-\nblum, and Tom Goldstein. Active learning at the imagenet scale. arXiv preprint arXiv:2111.12880, 2021.\nDylan Foster, Alekh Agarwal, Miroslav Dudík, Haipeng Luo, and Robert Schapire. Practical contextual\nbandits with regression oracles.\nIn International Conference on Machine Learning, pages 1539–1548.\nPMLR, 2018.\nDylan J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu. Instance-dependent complexity\nof contextual bandits and reinforcement learning: A disagreement-based perspective.\narXiv preprint\narXiv:2010.03104, 2020.\nDavid A Freedman. On tail probabilities for martingales. the Annals of Probability, pages 100–118, 1975.\nEric Friedman. Active learning for smooth problems. In COLT. Citeseer, 2009.\nSteve Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the 24th\ninternational conference on Machine learning, pages 353–360, 2007.\nSteve Hanneke. Theory of active learning. Foundations and Trends in Machine Learning, 7(2-3), 2014.\nDavid Haussler.\nDecision theoretic generalizations of the pac model for neural net and other learning\napplications. 1989.\nDavid Haussler. Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik-chervonenkis\ndimension. Journal of Combinatorial Theory, Series A, 69(2):217–232, 1995.\nJuha Heinonen. Lectures on Lipschitz analysis. Number 100. University of Jyväskylä, 2005.\nKurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):251–257,\n1991.\nTzu-Kuo Huang, Alekh Agarwal, Daniel J Hsu, John Langford, and Robert E Schapire.\nEﬃcient and\nparsimonious agnostic active learning. Advances in Neural Information Processing Systems, 28, 2015.\nMatti Kääriäinen. Active learning in the non-realizable case. In International Conference on Algorithmic\nLearning Theory, pages 63–77. Springer, 2006.\nMina Karzand and Robert D Nowak. Maximin active learning in overparameterized model classes. IEEE\nJournal on Selected Areas in Information Theory, 1(1):167–177, 2020.\nSuraj Kothawade, Nathan Beck, Krishnateja Killamsetty, and Rishabh Iyer. Similar: Submodular infor-\nmation measures based active learning in realistic scenarios. Advances in Neural Information Processing\nSystems, 34, 2021.\nSamory Kpotufe, Gan Yuan, and Yunfan Zhao. Nuances in margin conditions determine gains in active\nlearning. arXiv preprint arXiv:2110.08418, 2021.\nAkshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daumé III, and John Langford. Active learning\nfor cost-sensitive classiﬁcation. In International Conference on Machine Learning, pages 1915–1924. PMLR,\n2017.\nAkshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daumé III, and John Langford. Active learning\nfor cost-sensitive classiﬁcation. Journal of Machine Learning Research, 20:1–50, 2019.\nAlex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. Advances in neural information processing systems, 25, 2012.\nYann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.\n12\nGene Li, Pritish Kamath, Dylan J Foster, and Nathan Srebro. Eluder dimension and generalized rank. arXiv\npreprint arXiv:2104.06970, 2021.\nAndrea Locatelli, Alexandra Carpentier, and Samory Kpotufe. Adaptivity to noise parameters in nonpara-\nmetric active learning. In Proceedings of the 2017 Conference on Learning Theory, PMLR, 2017.\nAndrea Locatelli, Alexandra Carpentier, and Samory Kpotufe. An adaptive strategy for active learning with\nsmooth decision boundary. In Algorithmic Learning Theory, pages 547–571. PMLR, 2018.\nJianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang.\nDeep network approximation for smooth\nfunctions. SIAM Journal on Mathematical Analysis, 53(5):5465–5506, 2021.\nPascal Massart and Élodie Nédélec. Risk bounds for statistical learning. The Annals of Statistics, 34(5):\n2326–2366, 2006.\nStanislav Minsker. Plug-in approach to active learning. Journal of Machine Learning Research, 13(1), 2012.\nGreg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view of bounded norm\ninﬁnite width relu nets: The multivariate case. In International Conference on Learning Representations,\n2020.\nRahul Parhi and Robert D Nowak. Banach space representer theorems for neural networks and ridge splines.\nJ. Mach. Learn. Res., 22(43):1–40, 2021.\nRahul Parhi and Robert D Nowak. What kinds of functions do deep neural networks learn? insights from\nvariational spline theory. SIAM Journal on Mathematics of Data Science, 4(2):464–489, 2022a.\nRahul Parhi and Robert D Nowak. Near-minimax optimal estimation with shallow relu neural networks.\nIEEE Transactions on Information Theory, 2022b.\nD Pollard. Convergence of Stochastic Processes. David Pollard, 1984.\nNikita Puchkin and Nikita Zhivotovskiy. Exponential savings in agnostic active learning through abstention.\narXiv preprint arXiv:2102.00451, 2021.\nPengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and\nXin Wang. A survey of deep active learning. ACM Computing Surveys (CSUR), 54(9):1–40, 2021.\nDaniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration.\nIn NIPS, pages 2256–2264. Citeseer, 2013.\nOzan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In\nInternational Conference on Learning Representations, 2018.\nBurr Settles. Active learning literature survey. 2009.\nShubhanshu Shekhar, Mohammad Ghavamzadeh, and Tara Javidi. Active learning for classiﬁcation with\nabstention. IEEE Journal on Selected Areas in Information Theory, 2(2):705–719, 2021.\nAlexander B Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of Statistics,\n32(1):135–166, 2004.\nMichael Unser. Ridges, neural networks, and the radon transform. arXiv preprint arXiv:2203.02543, 2022.\nVN Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their\nprobabilities. Theory of Probability and its Applications, 16(2):264, 1971.\nMartin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge\nUniversity Press, 2019.\nLiwei Wang. Smoothness, disagreement coeﬃcient, and the label complexity of agnostic active learning.\nJournal of Machine Learning Research, 12(7), 2011.\n13\nZhilei Wang, Pranjal Awasthi, Christoph Dann, Ayush Sekhari, and Claudio Gentile. Neural active learning\nwith performance guarantees. Advances in Neural Information Processing Systems, 34, 2021.\nAndrew Chi-Chin Yao. Probabilistic computations: Toward a uniﬁed measure of complexity. In 18th Annual\nSymposium on Foundations of Computer Science (sfcs 1977), pages 222–227. IEEE Computer Society,\n1977.\nDmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:103–114,\n2017.\nDmitry Yarotsky. Optimal approximation of continuous functions by very deep relu networks. In Conference\non learning theory, pages 639–649. PMLR, 2018.\nYinglun Zhu and Robert Nowak. Eﬃcient active learning with abstention. arXiv preprint arXiv:2204.00043,\n2022.\n14\nA\nOmitted details for Section 2.2\nProposition 1. Suppose DX Y ∈P(α, β). One can construct a set of neural network classiﬁer Hdnn such\nthat the following two properties hold simultaneously:\ninf\nh∈Hdnn err(h) −err(h⋆) = O(ε)\nand\nVCdim(Hdnn) = eO(ε−\nd\nα(1+β) ).\nProof. We take κ = ε\n1\n1+β in Theorem 3 to construct a set of neural network classiﬁers Hdnn with W =\nO(ε−\nd\nα(1+β) log 1\nε) total parameters arranged in L = O(log 1\nε) layers. According to Theorem 4, we know\nVCdim(Hdnn) = O(ε−\nd\nα(1+β) · log2(ε−1)) = eO(ε−\nd\nα(1+β) ).\nWe now show that there exists a classiﬁer h ∈Hdnn with small excess error. Let h = hf be the classiﬁer such\nthat ∥f −η∥∞≤κ. We can see that\nexcess(h) = E\n\u0002\n1(h(x) ̸= y) −\n1(h⋆(x) ̸= y)\n\u0003\n= E\n\u0002\n|2η(x) −1| ·\n1(h(x) ̸= h⋆(x))\n\u0003\n≤2κ · Px∼DX (x ∈X : |η(x) −1/2| ≤κ)\n= O(κ1+β)\n= O(ε),\nwhere the third line follows from the fact that h and h⋆disagrees only within region {x ∈X : |η(x)−1/2| ≤κ}\nand the incurred error is at most 2κ on each disagreed data point. The fourth line follows from the Tsybakov\nnoise condition and the last line follows from the selection of κ.\nBefore proving Theorem 5, we ﬁrst recall the excess error guarantee for empirical risk minimization under\nTsybakov noise condition.\nTheorem 11 (Boucheron et al. (2005)). Suppose DX Y satisﬁes Tsybakov noise condition with parameter\nβ ≥0. Consider a datatset Dn = {(xi, yi)}n\ni=1 of n points i.i.d. sampled from DX Y. Let bh ∈H be the\nempirical risk minimizer on Dn. For any constant ρ > 0, we have\nerr(bh) −min\nh∈H err(h)\n≤ρ · (min\nh∈H err(h) −err(h⋆)) + O\n \n(1 + ρ)2\nρ\n·\n\u0012VCdim(H) · log n\nn\n\u0013 1+β\n2+β\n+ log δ−1\nn\n!\n,\nwith probability at least 1 −δ.\nTheorem 5. Suppose DX Y ∈P(α, β). Fix any ε, δ > 0. Let Hdnn be the set of neural network classiﬁers\nconstructed in Proposition 1. With n = eO(ε−d+2α+αβ\nα(1+β) ) i.i.d. sampled points, with probability at least 1 −δ,\nthe empirical risk minimizer bh ∈Hdnn achieves excess error O(ε).\nProof. Proposition 1 certiﬁes minh∈Hdnn err(h)−err(h⋆) = O(ε) and VCdim(Hdnn) = O\n\u0010\nε−\nd\nα(1+β) · log2(ε−1)\n\u0011\n.\nTake ρ = 1 in Theorem 11, leads to\nerr(bh) −err(h⋆) ≤O\n \nε +\n\u0012\nε−\nd\nα(1+β) · log2(ε−1) · log n\nn\n\u0013 1+β\n2+β\n+ log δ−1\nn\n!\n,\nTaking n = O(ε−d+2α+αβ\nα(1+β)\n· log(ε−1) + ε−1 · log(δ−1)) = eO(ε−d+2α+αβ\nα(1+β) ) thus ensures that err(bh) −err(h⋆) =\nO(ε).\n15\nB\nGeneric version of Algorithm 1 and its guarantees\nWe present Algorithm 3 below, a generic version of Algorithm 1 that doesn’t require the approximating\nclassiﬁers to be neural networks. The guarantees of Algorithm 3 are provided in Theorem 12, which is\nproved in Appendix B.2 based on supporting lemmas provided in Appendix B.1.\nAlgorithm 3 RobustCAL with Approximation\nInput: Accuracy level ε ∈(0, 1), conﬁdence level δ ∈(0, 1).\n1: Let H be a set of approximating classiﬁers such that infh∈H err(h) −err(h⋆) = O(ε).\n2: Deﬁne T := ε−2+β\n1+β · VCdim(H), M := ⌈log2 T ⌉, τm := 2m for m ≥1 and τ0 := 0.\n3: Deﬁne ρm := O\n\u0012\u0010\nVCdim(H)·log(τm−1)·log(M/δ)\nτm−1\n\u0011 1+β\n2+β \u0013\nfor m ≥2 and ρ1 := 1.\n4: Deﬁne bRm(h) := Pτm−1\nt=1\nQt\n1(h(xt) ̸= yt) with the convention that P0\nt=1 . . . = 0.\n5: Initialize H0 := H.\n6: for epoch m = 1, 2, . . . , M do\n7:\nUpdate active set Hm :=\nn\nh ∈Hm−1 : bRm(h) ≤infh∈Hm−1 bRm(h) + τm−1 · ρm\no\n8:\nif epoch m = M then\n9:\nReturn any classiﬁer bh ∈HM.\n10:\nfor time t = τm−1 + 1, . . . , τm do\n11:\nObserve xt ∼DX . Set Qt :=\n1(xt ∈DIS(Hm)).\n12:\nif Qt = 1 then\n13:\nQuery the label yt of xt.\nWe provide guarantees for Algorithm 3, and then specialize them to the settings with neural network ap-\nproximation, i.e., in Theorem 6 and Theorem 10. As discussed before, our analysis is based on the analysis\nRobustCAL, but with novel extensions in removing the requirements that the Bayes classiﬁer is in the class\n(or a Bernstein condition for every h ∈H).\nTheorem 12. Fix ε, δ > 0. With probability at least 1 −δ, Algorithm 3 returns a classiﬁer bh ∈H with\nexcess error eO(ε) after querying\neO\n\u0010\nθH(ε\nβ\n1+β ) · ε−\n2\n1+β · VCdim(H)\n\u0011\nlabels.\nB.1\nSupporting lemmas\nWe ﬁrst recall that Tsybakov noise condition leads to the so-called Bernstein condition (with respect to Bayes\nclassiﬁer h⋆).\nLemma 1 (Tsybakov (2004)). Suppose DX Y satisﬁes the Tsybakov noise condition with parameter β ≥0,\nthen there exists an universal constant c′ > 0 such that we have\nPx∼DX (h(x) ̸= h⋆(x)) ≤c′(err(h) −err(h⋆))\nβ\n1+β\nfor any h : X →Y.\nWe next present a lemma in the passive learning setting, which will later be incorporated into the active\nlearning setting. We ﬁrst deﬁne some notations. Suppose Dn = {(xi, yi)}n\ni=1 are n i.i.d. data points drawn\nfrom DX Y. For any h : X →Y, we denote Rn(h) := Pn\ni=1\n1(h(xi) ̸= yi) as the empirical error of h over\ndataset Dn. We clearly have E[Rn(h)] = n · err(h) by i.i.d. assumption.\nLemma 2. Fix ε, δ > 0.\nSuppose DX Y satisﬁes Tsybakov noise condition with parameter β ≥0 and\nerr(ˇh) −err(h⋆) = O(ε), where ˇh = arg maxh∈H err(h) and h⋆is the Bayes classiﬁer. Let Dn = {(xi, yi)}n\ni=1\n16\nbe a set of n i.i.d. data points drawn from DX Y. If β > 0, suppose n satisﬁes\nn ≤ε−2+β\n1+β · VCdim(H)\n2+2β\nβ\n· log(δ\n−1) · (log n)\n2+2β\nβ .\nWith probability at least 1 −δ, we have the following inequalities hold:\nn · (err(h) −err(h⋆)) ≤2 · (Rn(h) −Rn(ˇh)) + n · ρ(n, δ),\n∀h ∈H,\n(1)\nRn(ˇh) −min\nh∈H Rn(h) ≤n · ρ(n, δ),\n(2)\nwhere ρ(n, δ) = C ·\n \u0010\nVCdim(H)·log n·log δ\n−1\nn\n\u0011 1+β\n2+β + ε\n!\nwith a universal constant C > 0.7\nProof. Denote H := H∪{h⋆}. We know that VCdim(H) ≤VCdim(H)+1 = O(VCdim(H)). From Lemma 1,\nwe know Bernstein condition is satisﬁed with respect to H and h⋆∈H. Invoking Lemma 3.1 in Hanneke\n(2014), with probability at least 1 −δ\n2, ∀h ∈H, we have\nn · (err(h) −err(h⋆)) ≤max\n\b\n2 · (Rn(h) −Rn(h⋆)), n · ρ(n, δ)\n\t\n,\n(3)\nRn(h) −min\nh∈H\nRn(h) ≤max\n\b\n2n · (err(h) −err(h⋆)), n · ρ(n, δ)\n\t\n,\n(4)\nwhere ρ(n, δ) = O\n \u0010\nVCdim(H)·log n+log δ\n−1\nn\n\u0011 1+β\n2+β\n!\n= O\n \u0010\nVCdim(H)·log n·log δ\n−1\nn\n\u0011 1+β\n2+β\n!\n.\nEq. (2) follows by taking h = ˇh in Eq. (4) and noticing that\nRh(ˇh) −min\nh∈H Rn(h) ≤Rn(ˇh) −min\nh∈H\nRn(h)\n≤max\n\b\n2n · O(ε), n · ρ(n, δ)\n\t\n,\nwhere we use the assumption that err(ˇh) −err(h⋆) = O(ε).\nTo derive Eq. (1), we ﬁrst notice that applying Eq. (3) for any h ∈H, we have\nn · (err(h) −err(h⋆)) ≤2 · (Rn(h) −Rn(ˇh) + Rn(ˇh) −Rn(h⋆)) + n · ρ(n, δ).\nWe next only need to upper bound Rn(ˇh) −Rn(h⋆), and show that it is order-wise smaller than n · ρ(n, δ).\nWe consider random variable gi :=\n1(ˇh(xi) ̸= yi) −\n1(h⋆(xi) ̸= yi). We have\nV(gi) ≤E[g2\ni ]\n= E[1(ˇh(xi) ̸= h⋆(xi)]\n= O\n\u0010\nε\nβ\n1+β\n\u0011\n,\nwhere the last line follows from Lemma 1 and the assumption that err(ˇh) −err(h⋆) = O(ε). Denote g =\n1\nn\nPn\ni=1 gi = 1\nn(Rn(ˇh) −Rn(h⋆)), and notice that E[g] = err(ˇh) −err(h⋆). Applying Bernstein inequality on\n−g, with probability at least 1 −δ\n2, we have\ng −E[g] ≤O\n\n\n \nε\nβ\n1+β log δ\n−1\nn\n! 1\n2\n+ log δ\n−1\nn\n\n,\nwhich further leads to\nRn(ˇh) −Rn(h⋆) ≤n · O\n\nε +\n \nε\nβ\n1+β log δ\n−1\nn\n! 1\n2\n+ log δ\n−1\nn\n\n.\n7The logarithmic factors in this bound might be further optimized. We don’t focus on optimizing logarithmic factors.\n17\nThe RHS is order-wise smaller than ρn when β = 0. We consider the case when β > 0 next. Since log(δ\n−1)/n\nis clearly a lower-order term compared to ρn, we only need to show that\n\u0012\nε\nβ\n1+β log δ−1\nn\n\u0013 1\n2\nis order-wise smaller\nthan ρn. We can easily check that\n \nε\nβ\n1+β log δ\n−1\nn\n! 1\n2\n≤\n \nVCdim(H) · log n · log δ\n−1\nn\n! 1+β\n2+β\nwhenever n satisﬁes the following condition\nn ≤ε−2+β\n1+β · VCdim(H)\n2+2β\nβ\n· log(δ\n−1) · (log n)\n2+2β\nβ .\nWe denote ˇh = arg minh∈H err(h). By assumption of Theorem 12, we have err(ˇh) −err(h⋆) = O(ε). For any\nh ∈H, we also use the shorthand Rm(h) = Rτm−1(h) := Pτm−1\nt=1\n1(h(xt) ̸= yt). Note that Rm is only used\nin analysis since some yt are not observable.\nLemma 3. With probability at least 1 −δ\n2, the following holds true for all epochs m ∈[M]:\n1. ˇh ∈Hm.\n2. err(h) −err(h⋆) ≤3ρm, ∀h ∈Hm.\nProof. For each m = 2, 3, . . . , M, we invoke Lemma 2 with n = τm−1 and δ = δ/2M, which guarantees that\nτm−1 · (err(h) −err(h⋆)) ≤2 · (Rm(h) −Rm(ˇh)) + τm−1 · ρm,\n∀h ∈H,\n(5)\nRm(ˇh) −min\nh∈H Rm(h) ≤τm−1 · ρm.\n(6)\nNote that the choice T chosen in Algorithm 3 clearly satisﬁes the requirement needed (for n = τm−1) in\nLemma 2 when β > 0; and ensures that the second term in ρ(τm−1, δ/2M) (i.e., ε, see Lemma 2 for deﬁnition\nof ρ(τm−1, δ/2M)) is a lower-order term compared to the ﬁrst term.\nWe use E to denote the good event where Eq. (5) and Eq. (6) hold true across m = 2, 3, . . ., M. This good\nevent happens with probability at least 1 −δ\n2. We analyze under E in the following.\nWe prove Lemma 3 through induction. The statements clearly hold true for m = 1. Suppose the statements\nhold true up to epoch m, we next prove the correctness for epoch m + 1.\nWe know that ˇh ∈Hm by assumption. Based on the querying criteria of Algorithm 3, we know that\nbRm+1(ˇh) −bRm+1(h) = Rm+1(ˇh) −Rm+1(h),\n∀h ∈Hm\n(7)\nFrom Eq. (6), we also have\nRm+1(ˇh) −min\nh∈Hm Rm+1(h) ≤Rm+1(ˇh) −min\nh∈H Rm+1(h)\n≤τm · ρm+1.\nCombining the above two inequalities shows that\nbRm+1(ˇh) −bRm+1(h) ≤τm · ρm+1,\nimplying that ˇh ∈Hm+1 (due to the construction of Hm+1 in Algorithm 3).\nBased on Eq. (7), the construction Hm+1 and the fact that ˇh ∈Hm, we know that, for any h ∈Hm+1 ⊆Hm,\nRm+1(h) −Rm+1(ˇh) = bRm+1(h) −bRm+1(ˇh)\n≤bRm+1(h) −min\nh∈Hm\nbRm+1(h)\n≤τm · ρm+1.\n18\nPlugging the above inequality into Eq. (5) (at epoch m + 1) leads to err(h) −err(h⋆) ≤3ρm+1 for any\nh ∈Hm+1. We thus prove the desired statements at epoch m + 1.\nB.2\nProof of Theorem 12\nTheorem 12. Fix ε, δ > 0. With probability at least 1 −δ, Algorithm 3 returns a classiﬁer bh ∈H with\nexcess error eO(ε) after querying\neO\n\u0010\nθH(ε\nβ\n1+β ) · ε−\n2\n1+β · VCdim(H)\n\u0011\nlabels.\nProof. Based on Lemma 3, we know that, with probability at least 1 −δ\n2, we have\nerr(bh) −err(h⋆) ≤3ρM\n= O\n \u0012VCdim(H) · log(τM−1) · log(M/δ)\nτM−1\n\u0013 1+β\n2+β !\n= eO(ε),\nwhere we use the deﬁnition of T and τM.\nWe next analyze the label complexity of Algorithm 3. Since Algorithm 3 stops and the beginning at epoch\nM, we only need to calculated the label complexity in the ﬁrst M −1 epochs. We have\nτM−1\nX\nt=1\nQt =\nM−1\nX\nm=1\n(τm −τm−1) ·\n1(xt ∈DIS(Hm))\n≤\nM−1\nX\nm=1\n(τm −τm−1) ·\n1\n\u0010\nxt ∈DIS(BH(h⋆, c′(3ρm)\nβ\n1+β ))\n\u0011\n,\nwhere on the last line we use the facts (1) err(h) −err(h⋆) ≤3ρm, ∀h ∈Hm from Lemma 3; and (2)\nP(x : h(x) ̸= h⋆(x)) ≤c′(err(h) −err(h⋆))\nβ\n1+β from Lemma 2 (with the same constant c′). Suppose err(ˇh) −\nerr(h⋆) = c′′ε (with another universal constant c′′ by assumption). Applying Lemma 2 on ˇh leads to the\nfact that h⋆∈BH(ˇh, c′(c′′ε)\nβ\n1+β ). Since P(x : h(x) ̸= ˇh(x)) ≤P(x : h(x) ̸= h⋆(x)) + P(x : h⋆(x) ̸= ˇh(x)), we\nfurther have\nτM−1\nX\nt=1\nQt ≤\nM−1\nX\nm=1\n(τm −τm−1) ·\n1\n\u0010\nxt ∈DIS(BH(ˇh, c · ρm\nβ\n1+β ))\n\u0011\n,\nwith a universal constant c > 0. Noticing that the RHS is a sum of independent Bernoulli random variables,\napplying a Bernstein-type bound (e.g., Lemma 5), on a good event E′ that happens with probability at least\n1 −δ\n2, we have\nτM−1\nX\nt=1\nQt ≤2\nM−1\nX\nm=1\n(τm −τm−1) · P\n\u0010\nx ∈DIS(BH(ˇh, c · ρm\nβ\n1+β ))\n\u0011\n+ 4 log(4/δ)\n≤2\nM−1\nX\nm=2\nτm−1 · θH,ˇh\n\u0012\nc · ρ\nβ\n1+β\nm\n\u0013\n· c · ρ\nβ\n1+β\nm\n+ 4 log(4/δ) + 4\n≤2M · θH,ˇh\n\u0012\nc · ρ\nβ\n1+β\nM\n\u0013\n·\n\u0012\nc · τM−1 · ρ\nβ\n1+β\nM\n\u0013\n+ 4 log(4/δ) + 4,\n19\nwhere the second using the deﬁnition of disagreement coeﬃcient; and the last line follows from the fact that\nρm is non-increasing and τm−1 · ρm is increasing. Basic algebra and basic properties of the disagreement\ncoeﬃcient (i.e., Theorem 7.1 and Corollary 7.2 in Hanneke (2014)) shows that\nτM−1\nX\nt=1\nQt ≤eO\n\u0010\nθH(ε\nβ\n1+β ) · ε−\n2\n1+β · VCdim(H)\n\u0011\n,\nunder event E ∩E′, which happen with probability at least 1 −δ.\nC\nOmitted details for Section 2.3\nWe prove Theorem 6 in Appendix C.1 and discuss the disagreement coeﬃcient in Appendix C.2.\nC.1\nProof of Theorem 6\nTheorem 6. Suppose DX Y ∈P(α, β). Fix any ε, δ > 0. With probability at least 1 −δ, Algorithm 1 returns\na classiﬁer bh ∈Hdnn with excess error eO(ε) after querying eO(θHdnn(ε\nβ\n1+β ) · ε−d+2α\nα+αβ ) labels.\nProof. Construct Hdnn based on Proposition 1 such that minh∈Hdnn err(h)−err(h⋆) = O(ε) and VCdim(Hdnn) =\neO(ε−\nd\nα(1+β) ). Taking such Hdnn into Theorem 12 leads to the desired result.\nC.2\nDiscussion on disagreement coeﬃcient in Theorem 6\nWe discuss cases when the (classiﬁer-based) disagreement coeﬃcient with respect to a set of neural networks\nis well-bounded. As mentioned before, even for simple classiﬁers such as linear functions, the disagreement\ncoeﬃcient has been analyzed under additional assumptions (Friedman, 2009; Hanneke, 2014). In this section,\nwe analyze the disagreement coeﬃcient for a set of neural networks under additional assumptions on DX Y\nand Hdnn (assumptions on Hdnn can be implemented via proper preprocessing steps).\nWe leave a more\ncomprehensive investigation of the disagreement coeﬃcient for future work.\nThe ﬁrst case is when DX is supported on countably many data points. The following result show strict\nimprovement over passive learning.\nDeﬁnition 3 (Disagreement core). For any hypothesis class H and classiﬁer h, the disagreement core of h\nwith respect to H under DX Y is deﬁned as\n∂Hh := lim\nr→0 DIS(BH(h, r)).\n(8)\nProposition 2 (Lemma 7.12 and Theorem 7.14 in Hanneke (2014)). For any hypothesis class H and classiﬁer\nh, we have θh(ε) = o(1/ε) if and only if DX (∂Hh) = 0. In particular, this implies that θH(ε) = o(1/ε)\nwhenever DX is supported on countably many data points.\nWe now discuss conditions under which we can upper bound the disagreement coeﬃcient by O(1), which\nensures results in Theorem 6 matching the minimax lower bound for active learning, up to logarithmic factors.\nWe introduce the following decomposable condition.\nDeﬁnition 4. A marginal distribution DX is ε-decomposable if its (known) support supp(DX ) can be de-\ncomposed into connected subsets, i.e., supp(DX ) = ∪i∈IXi, such that\nDX (∪i∈I′Xi) = O(ε),\nwhere I′ := {i ∈I : DX (Xi) ≤ε}.\nRemark 1. Note that Deﬁnition 4 permits a decomposition such that |I| = Ω( 1\nε) where I = I \\ I′. Deﬁni-\ntion 4 requires no knowledge of the index set I or any Xi; it also places no restrictions on the conditional\nprobability on each Xi.\n20\nWe ﬁrst give results for a general hypothesis class H as follows, and then discuss how to bound the disagree-\nment coeﬃcient for a set of neural networks.\nProposition 3. Suppose DX is decomposable (into ∪i∈IXi) and the hypothesis class H consists of classiﬁers\nwhose predication on each Xi is the same, i.e., |{h(x) : x ∈Xi}| = 1 for any h ∈H and i ∈I. We then have\nθH(ε) = O(1) for ε suﬃciently small.\nProof. Fix any h ∈H. we know that for any h′ ∈BH(h, ε), we must have DIS({h, h′}) ⊆∪i∈I′Xi since\nDX {x ∈X : h(x) ̸= h′(x)} ≤ε; and {h(x) : x ∈Xi} = 1 for any h ∈H and any Xi. This further implies\nthat P(DIS(BH(h, ε)) = O(ε), and thus θH(ε) = O(1).\nWe next discuss conditions under which we can satisfy the prerequisites of Proposition 3. Suppose DX Y ∈\nP(α, β). We assume that DX is (ε\nβ\n1+β )-decomposable, and, for the desired accuracy level ε, we have\n|η(x) −1/2| ≥2ε\n1\n1+β ,\n∀x ∈supp(DX ).\n(9)\nWith the above conditions satisﬁed, we can ﬁlter out neural networks that are clearly not “close” to η.\nSpeciﬁcally, with κ = ε\n1\n1+β and Fdnn be the set of neural networks constructed from Proposition 1, we\nconsider\neFdnn := {f ∈Fdnn : |f(x) −1/2| ≥ε\n1\n1+β , ∀x ∈supp(DX )},\n(10)\nwhich is guaranteed to contain f ∈Fdnn such that ∥f −η∥∞≤ε\n1\n1+β . Now focus on the subset\ne\nHdnn := {hf : f ∈eFdnn}.\n(11)\nWe clearly have hf ∈e\nHdnn (which ensures an O(ε)-optimal classiﬁer) and VCdim( e\nHdnn) ≤VCdim(Hdnn)\n(since e\nHdnn ⊆Hdnn). We upper bound the disagreement coeﬃcient θ e\nHdnn(ε\nβ\n1+β ) next.\nProposition 4. Suppose DX Y ∈P(α, β) such that DX is (ε\nβ\n1+β )-decomposable and Eq. (9) is satisﬁed (with\nthe desired accuracy level ε). We then have θ e\nHdnn(ε\nβ\n1+β ) = O(1).\nProof. The proof is similar to the proof of Proposition 3. Fix any h = hf ∈e\nHdnn. We ﬁrst argue that, for\nany i ∈I, under Eq. (9), |{hf(x) : x ∈Xi}| = 1, i.e., for x ∈Xi, hf(x) equals either 1 or 0, but not both:\nThis can be seen from the fact that any f ∈eFdnn is continuous and satisﬁes |f(x) −1/2| ≥ε\n1\n1+β for any\nx ∈Xi.\nFix any h ∈e\nHdnn. We know that for any h′ ∈H e\nHdnn(h, ε\nβ\n1+β ), we must have DIS({h, h′}) ⊆∪i∈I′Xi due\nto similar reasons argued in the proof of Proposition 3. This further implies that P(DIS(B e\nHdnn(h, ε\nβ\n1+β )) =\nO(ε\nβ\n1+β ), and thus θ e\nHdnn(ε\nβ\n1+β ) = O(1).\nWe next argue that Eq. (9) is only needed in an approximate sense. We deﬁne the approximate decomposable\ncondition in the following.\nDeﬁnition 5. A marginal distribution DX is (ε, δ)-decomposable if there exists a known subset X ⊆\nsupp(DX ) such that\nDX (X ) ≥1 −δ,\n(12)\nand it can be decomposed into connected subsets, i.e., X = ∪i∈IXi, such that\nDX (∪i∈I′Xi) = O(ε),\nwhere I′ := {i ∈I : DX (Xi) ≤ε}.\n21\nSuppose DX Y ∈P(α, β). We assume that DX is (ε\nβ\n1+β , ε\nβ\n1+β )-decomposable (wrt X ⊆DX ), and, for the\ndesired accuracy level ε, we have\n|η(x) −1/2| ≥2ε\n1\n1+β ,\n∀x ∈X.\n(13)\nWith the above conditions satisﬁed, we can ﬁlter out neural networks that are clearly not “close” to η.\nSpeciﬁcally, with κ = ε\n1\n1+β and Fdnn be the set of neural networks constructed from Proposition 1, we\nconsider\nFdnn := {f ∈Fdnn : |f(x) −1/2| ≥ε\n1\n1+β , ∀x ∈X},\n(14)\nwhich is guaranteed to contain f ∈Fdnn such that ∥f −η∥∞≤ε\n1\n1+β . Now focus on the subset\nHdnn := {hf : f ∈Fdnn}.\n(15)\nWe clearly have hf ∈Hdnn (which ensures an O(ε)-optimal classiﬁer) and VCdim(Hdnn) ≤VCdim(Hdnn)\n(since Hdnn ⊆Hdnn). We upper bound the disagreement coeﬃcient θHdnn(ε\nβ\n1+β ) next.\nProposition 5. Suppose DX Y ∈P(α, β) such that DX is (ε\n1\n1+β , ε)-decomposable (wrt known X ⊆supp(DX ))\nand Eq. (13) is satisﬁed (with the desired accuracy level ε). We then have θHdnn(ε\nβ\n1+β ) = O(1).\nProof. The proof is the same as the proof of Proposition 5 except for any h′ ∈HHdnn(h, ε\nβ\n1+β ), we must have\nDIS({h, h′}) ⊆(∪i∈I′Xi) ∪(supp(DX ) \\ X). Based on the assumption that DX is (ε\n1\n1+β , ε)-decomposable,\nthis also leads to θHdnn(ε\nβ\n1+β ) = O(1)\nD\nGeneric version of Algorithm 2 and its guarantees\nThis section is organized as follows. We ﬁrst introduce some complexity measures in Appendix D.1. We\nthen provide the generic algorithm (Algorithm 4) and state its theoretical guarantees (Theorem 14) in\nAppendix D.2.\nD.1\nComplexity measures\nWe ﬁrst introduce pseudo dimension (Pollard, 1984; Haussler, 1989, 1995), a complexity measure used to\nanalyze real-valued functions.\nDeﬁnition 6 (Pseudo dimension). Consider a set of real-valued function F : X →R. The pseudo dimension\nPdim(F) of F is deﬁned as the VC dimension of the set of threshold functions {(x, ζ) 7→\n1(f(x) > ζ) : f ∈\nF}.\nAs discussed in Bartlett et al. (2019), similar results as in Theorem 4 holds true for Pdim(F) as well.\nTheorem 13 (Bartlett et al. (2019)). Let Fdnn be a set of neural network regression functions of the same\narchitecture and with W parameters arranged in L layers. We then have\nΩ(WL log(W/L)) ≤Pdim(Fdnn) ≤O(WL log(W)).\nWe now introduce value function disagreement coeﬃcient, which is proposed by Foster et al. (2020) in\ncontextual bandits and then adapted to active learning by Zhu and Nowak (2022) with additional supreme\nover the marginal distribution DX to deal with distributional shifts caused by selective sampling.\nDeﬁnition 7 (Value function disagreement coeﬃcient). For any f ⋆∈F and γ0, ε0 > 0, the value function\ndisagreement coeﬃcient θval\nf ⋆(F, γ0, ε0) is deﬁned as\nsup\nDX\nsup\nγ>γ0,ε>ε0\n\u001aγ2\nε2 · PDX\n\u0000∃f ∈F : |f(x) −f ⋆(x)| > γ, ∥f −f ⋆∥DX ≤ε\n\u0001\u001b\n∨1,\nwhere ∥f∥2\nDX := Ex∼DX [f 2(x)]. We also deﬁne θval\nF (γ0) := supf ⋆∈F,ε0>0 θval\nf ⋆(F, γ0, ε0).\n22\nD.2\nThe generic algorithm and its guarantees\nWe present Algorithm 4, a generic version of Algorithm 2 that doesn’t require the approximating classiﬁers\nto be neural networks.\nAlgorithm 4 NeuralCAL++ (Generic Version)\nInput: Accuracy level ε ∈(0, 1), conﬁdence level δ ∈(0, 1), abstention parameter γ ∈(0, 1/2).\n1: Let F : X →[0, 1] be a set of regression functions such that there exists a regression function f ∈F\nwith ∥f −η∥∞≤κ ≤γ/4.\n2: Deﬁne T := θval\nF (γ/4)·Pdim(F)\nε γ\n, M := ⌈log2 T ⌉, and Cδ := O(Pdim(F) · log(T/δ)).\n3: Deﬁne τm := 2m for m ≥1, τ0 := 0, and βm := 3(M −m + 1)Cδ.\n4: Deﬁne bRm(f) := Pτm−1\nt=1\nQt( bf(xt) −yt)2 with the convention that P0\nt=1 . . . = 0.\n5: for epoch m = 1, 2, . . . , M do\n6:\nGet bfm := arg minf∈F\nPτm−1\nt=1\nQt(f(xt) −yt)2.\n7:\n(Implicitely) Construct active set Fm :=\nn\nf ∈F : bRm(f) ≤bRm( bfm) + βm\no\n.\n8:\nConstruct classiﬁer bhm : X →{+1, −1, ⊥} as\nbhm(x) :=\n(\n⊥,\nif [lcb(x; Fm) −γ\n4 , ucb(x; Fm) + γ\n4] ⊆\n\u0002 1\n2 −γ, 1\n2 + γ\n\u0003\n;\nsign(2 bfm(x) −1),\no.w.\nand query function gm(x) :=\n1\n\u0000 1\n2 ∈\n\u0000lcb(x; Fm) −γ\n4 , ucb(x; Fm) + γ\n4\n\u0001\u0001\n·\n1(bhm(x) ̸= ⊥).\n9:\nif epoch m = M then\n10:\nReturn classiﬁer bhM.\n11:\nfor time t = τm−1 + 1, . . . , τm do\n12:\nObserve xt ∼DX . Set Qt := gm(xt).\n13:\nif Qt = 1 then\n14:\nQuery the label yt of xt.\nWe next state the theoretical guarantees for Algorithm 4.\nTheorem 14. Suppose θval\nF (γ/4) ≤θ and the approximation level κ ∈(0, γ/4] satisﬁes\n\u0012432θ · M 2\nγ2\n\u0013\n· κ2 ≤1\n10.\n(16)\nWith probability at least 1−δ, Algorithm 4 returns a classiﬁer bh : X →{+1, −1, ⊥} with Chow’s excess error\nexcessγ(bh) = O\n\u0012\nε · log\n\u0012θ · Pdim(F)\nε γ δ\n\u0013\u0013\n,\nafter querying at most\nO\n\u0012M 2 · Pdim(F) · log(T/δ) · θ\nγ2\n\u0013\nlabels.\nTheorem 14 is proved in Appendix D.3, based on supporting lemmas and theorems established in Ap-\npendix D.2.1 and Appendix D.2.2. The general result (Theorem 14) will be used to prove results in speciﬁc\nsettings (e.g., Theorem 7 and Theorem 18).\nD.2.1\nConcentration results\nThe Freedman’s inequality is commonly used in the ﬁeld of active learning and contextual bandits, e.g.,\n(Freedman, 1975; Agarwal et al., 2014; Krishnamurthy et al., 2019; Foster et al., 2020). We thus state the\nresult without proof.\n23\nLemma 4 (Freedman’s inequality). Let (Xt)t≤T be a real-valued martingale diﬀerence sequence adapted to\na ﬁltration Ft, and let Et[·] := E[· | Ft−1]. If |Xt| ≤B almost surely, then for any η ∈(0, 1/B) it holds with\nprobability at least 1 −δ,\nT\nX\nt=1\nXt ≤η\nT\nX\nt=1\nEt[X2\nt ] + log δ−1\nη\n.\nLemma 5. Let (Xt)t≤T to be real-valued sequence of random variables adapted to a ﬁltration Ft. If |Xt| ≤B\nalmost surely, then with probability at least 1 −δ,\nT\nX\nt=1\nXt ≤3\n2\nT\nX\nt=1\nEt[Xt] + 4B log(2δ−1),\nand\nT\nX\nt=1\nEt[Xt] ≤2\nT\nX\nt=1\nXt + 8B log(2δ−1).\nProof. This is a direct consequence of Lemma 4.\nWe now deﬁne/recall some notations. Denote nm := τm −τm−1. Fix any epoch m ∈[M] and any time step\nt within epoch m. We have f ⋆= η. For any f ∈F, we denote Mt(f) := Qt((f(xt) −yt)2 −(f ⋆(xt) −yt)2),\nand bRm(f) := Pτm−1\nt=1\nQt(f(xt) −yt)2.\nRecall that we have Qt = gm(xt).\nWe deﬁne ﬁltration Ft :=\nσ((x1, y1), . . . , (xt, yt)),8 and denote Et[·] := E[· | Ft−1]. We next present concentration results with respect\nto a general set of regression function F with ﬁnite pseudo dimension.\nLemma 6 (Krishnamurthy et al. (2019)). Consider an inﬁnite set of regression function F.\nFix any\nδ ∈(0, 1). For any τ, τ ′ ∈[T ] such that τ < τ′, with probability at least 1 −δ\n2, we have\nτ ′\nX\nt=τ\nMt(f) ≤\nτ ′\nX\nt=τ\n3\n2Et[Mt(f)] + Cδ,\nand\nτ ′\nX\nt=τ\nEt[Mt(f)] ≤2\nτ ′\nX\nt=τ\nMt(f) + Cδ,\nwhere Cδ = C ·\n\u0010\nPdim(F) · log T + log\n\u0010\nPdim(F)·T\nδ\n\u0011\u0011\nwith a universal constant C > 0.\nD.2.2\nSupporting lemmas for Theorem 14\nFix any classiﬁer bh : X →{+1, −1, ⊥}. For any x ∈X, we use the notion\nexcessγ(bh; x) :=\nPy|x\n\u0000y ̸= sign(bh(x))\n\u0001\n·\n1\n\u0000bh(x) ̸= ⊥\n\u0001\n+\n\u00001/2 −γ\n\u0001\n· 1\n\u0000bh(x) = ⊥\n\u0001\n−Py|x\n\u0000y ̸= sign(h⋆(x))\n\u0001\n=\n1\n\u0000bh(x) ̸= ⊥\n\u0001\n·\n\u0000Py|x\n\u0000y ̸= sign(bh(x))\n\u0001\n−Py|x\n\u0000y ̸= sign(h⋆(x))\n\u0001\u0001\n+\n1\n\u0000bh(x) = ⊥\n\u0001\n·\n\u0000\u00001/2 −γ\n\u0001\n−Py|x\n\u0000y ̸= sign(h⋆(x))\n\u0001\u0001\n(17)\nto represent the excess error of bh at point x ∈X.\nExcess error of classiﬁer bh can be then written as\nexcessγ(bh) := errγ(bh) −err(h⋆) = Ex∼DX [excessγ(bh; x)].\n8yt is not observed (and thus not included in the ﬁltration) when Qt = 0. Note that Qt is measurable with respect to\nσ((Ft−1, xt)).\n24\nWe let E denote the good event considered in Lemma 6, we analyze under this event through out the rest\nof this section. Most lemmas presented in this section are inspired by results provided Zhu and Nowak\n(2022). Our main innovation is an inductive analysis of lemmas that eventually relaxes the requirements for\napproximation error for Theorem 14.\nGeneral lemmas.\nWe introduce some general lemmas for Theorem 14.\nLemma 7. For any m ∈[M], we have gm(x) = 1 =⇒w(x; Fm) > γ\n2 .\nProof. We only need to show that ucb(x; Fm) −lcb(x; Fm) ≤\nγ\n2\n=⇒\ngm(x) = 0.\nSuppose otherwise\ngm(x) = 1, which implies that both\n1\n2 ∈\n\u0010\nlcb(x; Fm) −γ\n4 , ucb(x; Fm) + γ\n4\n\u0011\nand\nh\nlcb(x; Fm) −γ\n4 , ucb(x; Fm) + γ\n4\ni\n⊈\n\u00141\n2 −γ, 1\n2 + γ\n\u0015\n.\n(18)\nIf 1\n2 ∈\n\u0000lcb(x; Fm) −γ\n4, ucb(x; Fm) + γ\n4\n\u0001\nand ucb(x; Fm)−lcb(x; Fm) ≤γ\n2 , we must have lcb(x; Fm) ≥1\n2 −3\n4γ\nand ucb(x; Fm) ≤1\n2 + 3\n4γ, which contradicts with Eq. (18).\nLemma 8. Fix any m ∈[M]. Suppose f ∈Fm, we have excessγ(bhm; x) ≤0 if gm(x) = 0.\nProof. Recall that\nexcessγ(bh; x) =\n1\n\u0000bh(x) ̸= ⊥\n\u0001\n·\n\u0000Py|x\n\u0000y ̸= sign(bh(x))\n\u0001\n−Py|x\n\u0000y ̸= sign(h⋆(x))\n\u0001\u0001\n+\n1\n\u0000bh(x) = ⊥\n\u0001\n·\n\u0000\u00001/2 −γ\n\u0001\n−Py|x\n\u0000y ̸= sign(h⋆(x))\n\u0001\u0001\n.\nWe now analyze the event {gm(x) = 0} in two cases.\nCase 1: bhm(x) = ⊥.\nSince f(x) ∈[lcb(x; Fm), ucb(x; Fm)] and κ ≤γ\n4 by assumption, we know that η(x) = f ⋆(x) ∈[ 1\n2 −γ, 1\n2 + γ]\nand thus Py\n\u0000y ̸= sign(h⋆(x))\n\u0001\n≥1\n2 −γ. As a result, we have excessγ(bhm; x) ≤0.\nCase 2: bhm(x) ̸= ⊥but 1\n2 /∈(lcb(x; Fm) −γ\n4, ucb(x; Fm) + γ\n4).\nSince f(x) ∈[lcb(x; Fm), ucb(x; Fm)] and κ ≤γ\n4 by assumption, we clearly have sign(bhm(x)) = sign(h⋆(x))\nwhen 1\n2 /∈(lcb(x; Fm) −γ\n4 , ucb(x; Fm) + γ\n4 ). We thus have excessγ(bhm; x) ≤0.\nInductive lemmas.\nWe prove a set of statements for Theorem 14 in an inductive way. Fix any epoch\nm ∈[M], we consider\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbRm(f) −bRm(f ⋆) ≤Et\nh\nQt\n\u0000f(xt) −f ⋆(xt)\n\u00012i\n+ Cδ ≤3\n2Cδ\nf ∈Fm\nτm−1\nX\nt=1\nEt[Mt(f)] ≤4βm, ∀f ∈Fm\nτm−1\nX\nt=1\nE[Qt(xt)(f(xt) −f(xt))2] ≤9βm, ∀f ∈Fm\nFm ⊆Fm−1\n,\n(19)\nEx∼DX [1(gm(x) = 1)] ≤144βm\nτm−1 γ2 · θval\nf\n\u0010\nF, γ/4,\np\nβm/τm−1\n\u0011\n≤144βm\nτm−1 γ2 · θ,\n(20)\n25\nand\nEx∼DX [1(gm(x) = 1) · w(x; Fm)] ≤72βm\nτm−1γ · θval\nf\n\u0010\nF, γ/4,\np\nβm/τm−1\n\u0011\n≤72βm\nτm−1γ · θ.\n(21)\nLemma 9. Fix any m = [M]. When m = 1, 2 or when Eq. (20) holds true for epochs m = 2, 3, . . ., m −1,\nthen Eq. (19) holds true for epoch m = m.\nProof. The statements in Eq. (19) clearly hold true for m = m = 1 since, by deﬁnition, F0 = F and\nP0\nt=1 . . . = 0.\nWe thus only need to consider the case when m ≥2.\nWe next prove each of the ﬁve\nstatements in Eq. (19) for epoch m = m.\n1. In the case when m = 2, from Lemma 6, we know that\nbRm(f) −bRm(f ⋆) ≤\nτm−1\nX\nt=1\n3\n2 · Et\nh\nQt\n\u0000f(xt) −f ⋆(xt)\n\u00012i\n+ Cδ\n≤3 + Cδ ≤3\n2Cδ,\nwhere the second line follows from the fact that τ1 = 2 (without loss of generality, we assume Cδ ≥6\nhere).\nWe now focus on the case when m ≥3. We have\nbRm(f) −bRm(f ⋆) ≤\nτm−1\nX\nt=1\n3\n2 · Et\nh\nQt\n\u0000f(xt) −f ⋆(xt)\n\u00012i\n+ Cδ\n≤3\n2\nm−1\nX\nˇm=1\nn ˇmEx∼DX [1(g ˇm(x) = 1)] · κ2 + Cδ\n≤3\n2\n \n2 +\nm−1\nX\nˇm=2\nn ˇm\n144β ˇm · θ\nτ ˇm−1γ2\n!\n· κ2 + Cδ\n≤\n \n3 + 144θ\nγ2\n·\n m−1\nX\nˇm=2\nβ ˇm\n!!\n· κ2 + Cδ\n≤\n\u0012\n3 + 432θ · M 2\nγ2\n· Cδ\n\u0013\n· κ2 + Cδ\n≤3\n2Cδ,\nwhere the ﬁrst line follows from Lemma 6; the second line follows from the fact that ∥f −f ⋆∥∞≤κ; the\nthird line follows from Eq. (20); the forth line follows from n ˇm = τ ˇm−1; the ﬁfth line follows from the\ndeﬁnition of β ˇm; and the last line follows from the choice of κ in Eq. (16)\n2. Since Et[Mt(f)] = Et[Qt(f(xt)−f ⋆(xt))2], by Lemma 6, we have bRm(f ⋆) ≤bRm(f)+Cδ/2 for any f ∈F.\nCombining this with statement 1 leads to\nbRm(f) ≤bRm(f) + 2Cδ\n≤bRm(f) + βm\nfor any f ∈F, where the second line follows from the deﬁnition of βm. We thus have f ∈Fm based on\nthe elimination rule.\n26\n3. Fix any f ∈Fm. We have\nτm−1\nX\nt=1\nEt[Mt(f)] ≤2\nτm−1\nX\nt=1\nMt(f) + Cδ\n= 2 bRm(f) −2 bRm(f ⋆) + Cδ\n≤2 bRm(f) −2 bRm(f) + 4Cδ\n≤2 bRm(f) −2 bRm( bfm) + 4Cδ\n≤2βm + 4Cδ\n≤4βm,\nwhere the ﬁrst line follows from Lemma 6; the third line follows from statement 1; the fourth line follows\nfrom the fact that bfm is the minimizer of bRm(·); and the ﬁfth line follows from the fact that f ∈Fm.\n4. Fix any f ∈Fm. We have\nτm−1\nX\nt=1\nEt[Qt(xt)(f(xt) −f(xt))2] =\nτm−1\nX\nt=1\nEt[Qt(xt)((f(xt) −f ⋆(xt)) + (f ⋆(xt) −f(xt)))2]\n≤2\nτm−1\nX\nt=1\nEt[Qt(xt)(f(xt) −f ⋆(xt))2] + 2Cδ\n= 2\nτm−1\nX\nt=1\nEt[Mt(f)] + 2Cδ\n≤8βm + 2Cδ\n≤9βm,\nwhere the second line follows from (a + b)2 ≤2(a2 + b2) and (the proof of) statement 1 on the second\nline; and the fourth line follows from statement 3.\n5. Fix any f ∈Fm. We have\nbRm−1(f) −bRm−1( bfm−1) ≤bRm−1(f) −bRm−1(f ⋆) + Cδ\n2\n= bRm(f) −bRm(f ⋆) −\nτm−1\nX\nt=τm−2+1\nMt(f) + Cδ\n2\n≤bRm(f) −bRm(f) + 3\n2Cδ −\nτm−1\nX\nt=τm−2+1\nEt[Mt(f)]/2 + Cδ\n≤bRm(f) −bRm( bfm) + 5\n2Cδ\n≤βm + 3Cδ\n≤βm−1,\nwhere the ﬁrst line follows from Lemma 6; the third line follows from statement 1 and Lemma 6; the\nfourth line follows from the fact that bfm is the minimizer with respect to bRm and Lemma 6; the last line\nfollows from the construction of βm.\nWe introduce more notations. Denote (X, Σ, DX ) as the (marginal) probability space, and denote X m :=\n{x ∈X : gm(x) = 1} ∈Σ be the region where query is requested within epoch m. Under the prerequisites\nof Lemma 10 and Lemma 11 (i.e., Eq. (19) holds true for epochs m = 1, 2, . . . , m), we have Fm ⊆Fm−1 for\nm = 1, 2, . . . , m, which leads to X m ⊆X m−1 for m = 1, 2, . . ., m. We now deﬁne a sub probability measure\n27\nµm := (DX )|X m such that µm(ω) = DX (ω ∩X m) for any ω ∈Σ. Fix any epoch m ≤m and consider any\nmeasurable function F (that is DX integrable), we have\nEx∼DX [1(gm(x) = 1) · F(x)] =\nZ\nx∈X m\nF(x) dDX (x)\n≤\nZ\nx∈X m\nF(x) dDX (x)\n=\nZ\nx∈X\nF(x) dµm(x)\n=: Ex∼µm[F(x)],\n(22)\nwhere, by a slightly abuse of notations, we use Ex∼µ[·] to denote the integration with any sub probability\nmeasure µ. In particular, Eq. (22) holds with equality when m = m.\nLemma 10. Fix any epoch m ≥2. Suppose Eq. (19) holds true for epochs m = 1, 2, . . ., m, we then have\nEq. (20) holds true for epoch m = m.\nProof. We prove Eq. (20) for epoch m = m. We know that\n1(gm(x) = 1) =\n1(gm(x) = 1)·\n1(w(x; Fm) > γ/2)\nfrom Lemma 7. Thus, for any ˇm ≤m, we have\nEx∼DX [1(gm(x) = 1)] = Ex∼DX [1(gm(x) = 1) ·\n1(w(x; Fm) > γ/2)]\n≤Ex∼µ ˇ\nm[1(w(x; Fm) > γ/2)]\n≤Ex∼µ ˇ\nm\n\u0010\n1\n\u0000sup\nf∈Fm\n\f\ff(x) −f(x)\n\f\f > γ/4\n\u0001\u0011\n,\n(23)\nwhere the second line uses Eq. (22) and the last line follows from the facts that f ∈Fm (by Eq. (19)) and\nw(x; Fm) > γ/2 =⇒∃f ∈Fm, |f(x) −f(x)| > γ/4.\nFor any time step t, let m(t) denote the epoch where t belongs to. From Eq. (19), we know that, ∀f ∈Fm,\n9βm ≥\nτm−1\nX\nt=1\nEt\nh\nQt\n\u0000f(xt) −f(xt)\n\u00012i\n=\nτm−1\nX\nt=1\nEx∼DX\nh\n1(gm(t)(x) = 1) ·\n\u0000f(x) −f(x)\n\u00012i\n=\nm−1\nX\nˇm=1\nn ˇm · Ex∼µ ˇ\nm\nh\u0000f(x) −f(x)\n\u00012i\n= τm−1Ex∼νm\nh\u0000f(x) −f(x)\n\u00012i\n,\n(24)\nwhere we use Qt = gm(t)(xt) =\n1(gm(t)(x) = 1) and Eq. (22) on the second line, and deﬁne a new sub\nprobability measure\nνm :=\n1\nτm−1\nm−1\nX\nˇm=1\nn ˇm · µ ˇm\non the third line.\nPlugging Eq. (24) into Eq. (23) leads to the bound\nEx∼DX [1(gm(x) = 1)]\n≤Ex∼νm\n\u0014\n1\n\u0010\n∃f ∈F,\n\f\ff(x) −f(x)\n\f\f > γ/4, Ex∼νm\nh\u0000f(x) −f(x)\n\u00012i\n≤9βm\nτm−1\n\u0011\u0015\n,\n28\nwhere we use the deﬁnition of νm again (note that Eq. (23) works with any ˇm ≤m).\nBased on the\nDeﬁnition 7,9 we then have\nEx∼DX [1(gm(x) = 1)]\n≤144βm\nτm−1 γ2 · θval\nf\n\u0010\nF, γ/4,\np\n9βm/2τm−1\n\u0011\n≤144βm\nτm−1 γ2 · θval\nf\n\u0010\nF, γ/4,\np\nβm/τm−1\n\u0011\n≤144βm\nτm−1 γ2 · θ.\nLemma 11. Fix any epoch m ≥2. Suppose Eq. (19) holds true for epochs m = 1, 2, . . ., m, we then have\nEq. (21) holds true for epoch m = m.\nProof. We prove Eq. (21) for epoch m = m. Similar to the proof of Lemma 10, we have\nEx∼DX [1(gm(x) = 1) · w(x; Fm)] = Ex∼DX [1(gm(x) = 1) ·\n1(w(x; Fm) > γ/2) · w(x; Fm)]\n≤Ex∼µ ˇ\nm[1(w(x; Fm) > γ/2) · w(x; Fm)]\nfor any ˇm ≤m. With νm :=\n1\nτm−1\nPm−1\nˇm=1 n ˇm · µ ˇm, we then have\nEx∼DX [1(gm(x) = 1) · w(x; Fm)]\n≤Ex∼νm[1(w(x; Fm) > γ/2) · w(x; Fm)]\n≤Ex∼νm\n\"\n1\n \nsup\nf∈Fm\n\f\ff(x) −f(x)\n\f\f > γ/4\n!\n·\n \nsup\nf,f ′∈Fm\n|f(x) −f ′(x)|\n!#\n≤2Ex∼νm\n\"\n1\n \nsup\nf∈Fm\n\f\ff(x) −f(x)\n\f\f > γ/4\n!\n·\n \nsup\nf∈Fm\n|f(x) −f(x)|\n!#\n≤2\nZ 1\nγ/4\nEx∼νm\n\"\n1\n \nsup\nf∈Fm\n\f\ff(x) −f(x)\n\f\f ≥ω\n!#\nd ω\n≤2\nZ 1\nγ/4\n1\nω2 d ω ·\n\u0012 9βm\nτm−1\n· θval\nf\n\u0010\nF, γ/4,\np\n9βm/2τm−1\n\u0011\u0013\n≤72βm\nτm−1 γ · θval\nf\n\u0010\nF, γ/4,\np\nβm/τm−1\n\u0011\n≤72βm\nτm−1 γ · θ,\nwhere we follow similar steps as in the proof of Lemma 10 and use some basic arithmetic facts.\nLemma 12. Eq. (19), Eq. (20) and Eq. (21) hold true for all m ∈[M].\nProof. We ﬁrst notice that, by Lemma 9, Eq. (19) holds true for epochs m = 1, 2 unconditionally. We also\nknow that, by Lemma 10 and Lemma 11, once Eq. (19) holds true for epochs m = 1, 2, . . ., m, Eq. (20) and\nEq. (21) hold true for epochs m = m as well; at the same time, by Lemma 9, once Eq. (20) holds true for\nepochs m = 2, 3, . . ., m, Eq. (19) will hold true for epoch m = m + 1.\nWe thus can start the induction procedure from m = 2, and make sure that Eq. (19), Eq. (20) and Eq. (21)\nhold true for all m ∈[M].\n9Note that analyzing with a sub probability measure ν does not cause any problem. See Zhu and Nowak (2022) for detailed\ndiscussion.\n29\nD.3\nProof of Theorem 14\nTheorem 14. Suppose θval\nF (γ/4) ≤θ and the approximation level κ ∈(0, γ/4] satisﬁes\n\u0012432θ · M 2\nγ2\n\u0013\n· κ2 ≤1\n10.\n(16)\nWith probability at least 1−δ, Algorithm 4 returns a classiﬁer bh : X →{+1, −1, ⊥} with Chow’s excess error\nexcessγ(bh) = O\n\u0012\nε · log\n\u0012θ · Pdim(F)\nε γ δ\n\u0013\u0013\n,\nafter querying at most\nO\n\u0012M 2 · Pdim(F) · log(T/δ) · θ\nγ2\n\u0013\nlabels.\nProof. We analyze under the good event E deﬁned in Lemma 6, which holds with probability at least 1 −δ\n2.\nNote that all supporting lemmas stated in Appendix D.2.2 hold true under this event.\nFix any m ∈[M]. We analyze the Chow’s excess error of bhm, which is measurable with respect to Fτm−1.\nFor any x ∈X, if gm(x) = 0, Lemma 8 implies that excessγ(bhm; x) ≤0. If gm(x) = 1, we know that\nbhm(x) ̸= ⊥and 1\n2 ∈(lcb(x; Fm) −γ\n4 , ucb(x; Fm) + γ\n4 ). Since f ∈Fm by Lemma 12 (with Eq. (19)) and\nsupx∈X |f(x) −f ⋆(x)| ≤κ ≤γ/4 by construction. The error incurred in this case is upper bounded by\nexcess(bhm; x) ≤2|f ⋆(x) −1/2|\n≤2κ + 2|f(x) −1/2|\n≤2κ + 2w(x; Fm) + γ\n2\n≤4w(x; Fm),\nwhere we use Lemma 7 in the last line.\nCombining these two cases together, we have\nexcess(bhm) ≤4 Ex∼DX [1(gm(x) = 1) · w(x; Fm)].\nTake m = M and apply Lemma 12 (and Eq. (21)) leads to the following guarantee.\nexcess(bhM) ≤576βM\nτM−1γ · θval\nf\n\u0010\nF, γ/4,\np\nβM/τM−1\n\u0011\n≤O\n\u0012Pdim(F) log(T/δ)\nT γ\n· θ\n\u0013\n= O\n\u0012\nε · log\n\u0012θ · Pdim(F)\nε γ δ\n\u0013\u0013\n,\nwhere we use the fact that T = θ·Pdim(F)\nε γ\n.\nWe now analyze the label complexity (note that the sampling process of Algorithm 4 stops at time t = τM−1).\nNote that E[1(Qt = 1) | Ft−1] = Ex∼DX [1(gm(x) = 1)] for any epoch m ≥2 and time step t within epoch m.\n30\nCombine Lemma 12 with Eq. (20) (and Lemma 12) leads to\nτM−1\nX\nt=1\n1(Qt = 1) ≤3\n2\nτM−1\nX\nt=1\nE[1(Qt = 1) | Ft−1] + 4 log(2/δ)\n≤3 + 3\n2\nM−1\nX\nm=2\n(τm −τm−1) · 144βm\nτm−1 γ2\n· θ + 4 log(2/δ)\n≤3 + 4 log(2/δ) + O\n\u0012M 2 · Pdim(F) · log(T/δ) · θ\nγ2\n\u0013\n= O\n\u0012M 2 · Pdim(F) · log(T/δ) · θ\nγ2\n\u0013\nwith probability at least 1 −δ (due to another application of Lemma 5 with conﬁdence level δ/2); where we\nuse the fact that T = θ·Pdim(F)\nε γ\n.\nE\nProof of Theorem 7\nWe provide prerequisites in Appendix E.1 and the preprocessing procedures in Appendix E.2. We give the\nproof of Theorem 7 in Appendix E.3.\nE.1\nPrerequisites\nE.1.1\nUpper bound on the pseudo dimension\nWe present a result regarding the approximation and an upper bound on the pseudo dimension (i.e., Deﬁni-\ntion 6).\nProposition 6. Suppose DX Y ∈P(α, β). One can construct a set of neural network regression functions\nFdnn such that the following two properties hold simultaneously:\n∃f ∈Fdnn s.t. ∥f −f ⋆∥∞≤κ,\nand\nPdim(Fdnn) ≤c · κ−d\nα log2(κ−1),\nwhere c > 0 is a universal constant.\nProof. The result follows by combining Theorem 3 and Theorem 13.\nE.1.2\nUpper bounds on the value function disagreement coeﬃcient\nWe derive upper bounds on the value function disagreement coeﬃcient (i.e., Deﬁnition 7). We ﬁrst introduce\nthe (value function) eluder dimension, a complexity measure that is closely related to the value function\ndisagreement coeﬃcient Russo and Van Roy (2013); Foster et al. (2020).\nDeﬁnition 8 (Value function eluder dimension). For any f ⋆∈F and γ0 > 0, let ˇef ⋆(F, γ) be the length of\nthe longest sequence of data points x1, . . . , xm such that for all i, there exists f i ∈F such that\n|f i(xi) −f ⋆(xi)| > γ,\nand\nX\nj<i\n(f i(xj) −f ⋆(xj))2 ≤γ2.\nThe value function eluder dimension is deﬁned as ef ⋆(F, γ0) := supγ>γ0 ˇef ⋆(F, γ).\nThe next result shows that the value function disagreement coeﬃcient can be upper bounded by eluder\ndimension.\nProposition 7 (Foster et al. (2020)). Suppose F is a uniform Glivenko-Cantelli class. For any f ⋆: X →\n[0, 1] and γ, ε > 0, we have θval\nf ⋆(F, γ, ε) ≤4 ef ⋆(F, γ).\n31\nWe remark here that the requirement that F is a uniform Glivenko-Cantelli class is rather weak: It is satisﬁed\nas long as F has ﬁnite pseudo dimension (Anthony, 2002).\nIn the following, we only need to derive upper bounds on the value function eluder dimension, which upper\nbounds on the value function disagreement coeﬃcient.10 We ﬁrst deﬁne two deﬁnitions: (i) the standard def-\ninition of covering number (e.g., see Wainwright (2019)), and (ii) a newly-proposed deﬁnition of approximate\nLipschitzness.\nDeﬁnition 9. An ι-covering of a set X with respect to a metric ρ is a set {x1, . . . , xN} ⊆X such that\nfor each x ∈X, there exists some i ∈[N] such that ρ(x, xi) ≤ι. The ι-covering number N(ι; X, ρ) is the\ncardinality of the smallest ι-cover.\nDeﬁnition 10. We call a function f : X →R (L, κ)-approximate Lipschitz if\n|f(x) −f(x′)| ≤L · ∥x −x′∥2 + κ\nfor any x, x′ ∈X.\nWe next provide upper bounds on value function eluder dimension and value function disagreement coeﬃcient.\nTheorem 15. Suppose F is a set of (L, κ/4)-approximate Lipschitz functions. For any κ′ ≥κ, we have\nsupf∈F ef(F, κ′) ≤17 · N( κ′\n8L; X, ∥·∥2).\nProof. Fix any f ∈F and κ ≥κ′. We ﬁrst give upper bounds on ˇef(F, κ).\nWe construct G := F −f, which is a set of (2L, κ/2)-Lipschitz functions. Fix any eluder sequence x1, . . . , xm\nat scale κ and any ˇx ∈X. We claim that |{xj}j≤m ∩S| ≤17 where S := {x ∈X : ∥x −ˇx∥2 ≤\nκ\n8L}. Suppose\n{xj}j≤m ∩S = xj1, . . . , xjk (ji is ordered based on the ordering of {xj}j≤m). Since xjk is added into the\neluder sequence, there must exists a gjk ∈G such that\n|gjk(xjk)| > κ,\nand\nX\nj<jk\n(gjk(xj))2 ≤κ2.\n(25)\nSince gjk is (2L, κ/2)-Lipschitz, κ ≥κ′ ≥κ and xjk ∈S, we must have gjk(x) ≥κ\n4 for any x ∈S. As a\nresult, we must have |{xj}j<jk ∩Si| ≤16 as otherwise the second constraint in Eq. (25) will be violated.\nWe cover the space X with N( κ\n8L; X, ∥·∥2) balls of radius\nκ\n8L. Since the eluder sequence contains at most 17\ndata points within each ball, we know that ˇef(F, κ) ≤17 · N( κ\n8L; X, ∥·∥2).\nThe desired result follows by noticing that 17 · N( κ\n8L; X, ∥·∥2) is non-increasing in κ.\nCorollary 1. Suppose X ⊆Bd\nr := {x ∈Rd : ∥x∥2 ≤r} and F is a set of (L, κ/4)-approximate Lipschitz\nfunctions. For any κ′ ≥κ, we have θval\nF (κ′) := supf∈F,ι>0 θval\nf (F, κ′, ι) ≤c · ( Lr\nκ′ )d with a universal constant\nc > 0.\nProof. It is well-known that N(ι; Bd\nr, ∥·∥2) ≤(1 + 2r/ι)d (Wainwright, 2019). The desired result thus follows\nfrom combining Theorem 15 with Proposition 7.\nE.2\nThe preprocessing step: Clipping and ﬁltering\nLet η : X →[0, 1] denote the true conditional probability and Fdnn denote a set of neural network regression\nfunctions (e.g., constructed based on Theorem 3). We assume that (i) η is L-Lipschitz, and (ii) there exists\na f ∈F such that ∥f −η∥∞≤κ for some approximation factor κ > 0. We present the preprocessing step\nbelow in Algorithm 5.\n10We focus on Euclidean geometry on X (i.e., using ∥·∥2 norm) in deriving the upper bound. Slightly tighter bounds might\nbe possible with other norms.\n32\nAlgorithm 5 The Preprocessing Step: Clipping and Filtering\nInput: A set of regression functions F, Lipschitz parameter L > 0, approximation factor κ > 0.\n1: Clipping. Set ˇF := { ˇf : f ∈F}, where, for any f ∈F, we denote\nˇf(x) :=\n\n\n\n\n\n1,\nif f(x) ≥1;\n0,\nif f(x) ≤0;\nf(x),\no.w.\n2: Filtering. Set eF := { ˇf ∈ˇF : ˇf is (L, 2κ)-approximate Lipschitz}\n3: Return eF.\nProposition 8. Suppose η is L-Lipschitz and Fdnn is a set of neural networks (of the same architecture)\nwith W parameters arranged in L layers such that there exists a f ∈Fdnn with ∥f −η∥∞≤κ. Let eFdnn be the\nset of functions obtained by applying Algorithm 5 on Fdnn, we then have (i) Pdim( e\nFdnn) = O(WL log(W)),\nand (ii) there exists a ef ∈eFdnn such that ∥ef −η∥∞≤κ.\nProof. Suppose f is a neural network function, we ﬁrst notice that the “clipping” step can be implemented\nby adding one additional layer with O(1) additional parameters for each neural network function. More\nspeciﬁcally, ﬁx any f : X →R, we can set ˇf(x) := ReLU(f(x)) −ReLU(f(x) −1). Set ˇFdnn := { ˇf : f ∈Fdnn},\nwe then have Pdim( ˇFdnn) = O(WL log(W)) based on Theorem 13. Let eFdnn be the ﬁltered version of ˇFdnn.\nSince eFdnn ⊆ˇFdnn, we have Pdim( eFdnn) = O(WL log(W)).\nSince η : X →[0, 1], we have ∥ˇf −η∥∞≤∥f −η∥∞, which implies that there must exists a ˇf ∈ˇFdnn\nsuch ∥ˇf −η∥∞≤κ. To prove the second statement, it suﬃces to show that the ˇf ∈ˇF that achieves κ\napproximation error is not removed in the “ﬁltering” step, i.e., ˇf is (L, 2κ)-approximate Lipschitz. For any\nx, x′ ∈X, we have\n| ˇf(x) −ˇf(x′)| = | ˇf(x) −η(x) + η(x) −η(x′) + η(x′) −ˇf(x′)|\n≤L∥x −x′∥2 + 2κ,\nwhere we use the L-Lipschitzness of η and the fact that ∥ˇf −η∥∞≤κ.\nProposition 9. Suppose η is L-Lipschitz and X ⊆Bd\nr. Fix any κ ∈(0, γ/32]. There exists a set of neural\nnetwork regression functions Fdnn such that the followings hold simultaneously.\n1. Pdim(Fdnn) ≤c · κ−d\nα log2(κ−1) with a universal constant c > 0.\n2. There exists a f ∈Fdnn such that ∥f −η∥∞≤κ.\n3. θval\nFdnn(γ/4) := supf∈Fdnn,ι>0 θval\nf (Fdnn, γ/4, ι) ≤c′ · ( Lr\nγ )d with a universal constant c′ > 0.\nProof. Let Fdnn be obtained by (i) invoking Theorem 3 with approximation level κ, and (ii) invoking Algo-\nrithm 5 on the set of functions obtained in step (i). The ﬁrst two statements follow from Proposition 8, and\nthe third statement follows from Corollary 1 (note that to achieve guarantees for disagreement coeﬃcient at\nlevel γ/4, we need to have κ ≤γ/32 when invoking Theorem 3).\nE.3\nProof of Theorem 7\nTheorem 7. Fix any ε, δ, γ > 0. With probability at least 1 −δ, Algorithm 2 (with an appropriate initial-\nization at line 1) returns a classiﬁer bh with Chow’s excess error eO(ε) after querying poly( 1\nγ ) · polylog( 1\nε δ)\nlabels.\nProof. Let line 1 of Algorithm 2 be the set of neural networks Fdnn generated from Proposition 9 with approx-\nimation level κ ∈(0, γ/32] (and constants c, c′ speciﬁed therein). To apply results derived in Theorem 14,\n33\nwe need to satisfying Eq. (16), i.e., specifying an approximation level κ ∈(0, γ/32] such that the following\nholds true\n1\nκ2 ≥\n4320 · c′ · ( Lr\nγ )d ·\n\u0012\u0018\nlog2\n\u0012\nc′·( Lr\nγ )d·c·(κ−d\nα log2(κ−1))\nε γ\n\u0013\u0019\u00132\nγ2\nFor the setting we considered, i.e., X = [0, 1]d and η ∈Wα,∞\n1\n(X), we have r =\n√\nd = O(1) and L ≤\n√\nd = O(1)\n(e.g., see Theorem 4.1 in Heinonen (2005)).11 We thus only need to select a κ ∈(0, γ/32] such that\n1\nκ ≥c ·\n\u0012 1\nγ\n\u0013 d\n2 +1\n·\n\u0012\nlog 1\nε γ + log 1\nκ\n\u0013\n,\nwith a universal constant c > 0 (that is possibly d-dependent and α-dependent). Since x ≥2a log a =⇒\nx ≥a log x for any a > 0, we can select a κ > 0 such that\n1\nκ = ˇc ·\n\u0012 1\nγ\n\u0013 d\n2 +1\n· log 1\nε γ\nwith a universal constant ˇc > 0. With such choice of κ, from Proposition 9, we have\nPdim(Fdnn) = O\n\n\n\u0012 1\nγ\n\u0013 d2+d\n2α\n· polylog\n\u0012 1\nε γ\n\u0013\n.\nPlugging this bound on Pdim(Fdnn) and the upper bound on θval\nFdnn(γ/4) from Proposition 9 into the guarantee\nof Theorem 14 leads to excessγ(bh) = O(ε · log(\n1\nε γ δ)) after querying\nO\n\n\n\u0012 1\nγ\n\u0013d+2+ d2+d\n2α\n· polylog\n\u0012 1\nε γ δ\n\u0013\n\nlabels.\nF\nOther omitted details for Section 3\nWe discuss the proper abstention property of classiﬁer learned in Algorithm 2 the computational eﬃciency\nof Algorithm 2 in Appendix F.1. We provide the proof of Theorem 8 in Appendix F.2.\nF.1\nProper abstention and computational eﬃciency\nF.1.1\nProper abstention\nWe ﬁrst recall the deﬁnition of proper abstention proposed in Zhu and Nowak (2022).\nDeﬁnition 11 (Proper abstention). A classiﬁer bh : X →Y ∪{⊥} enjoys proper abstention if and only\nif it abstains in regions where abstention is indeed the optimal choice, i.e.,\n\b\nx ∈X : bh(x) = ⊥\n\t\n⊆\n\b\nx ∈X : η(x) ∈\n\u0002 1\n2 −γ, 1\n2 + γ\n\u0003\t\n=: Xγ.\nWe next show that the classiﬁer bh returned by Algorithm 4 enjoys the proper abstention property. We also\nconvert the abstaining classiﬁer bh : X →Y ∪{⊥} into a standard classiﬁer ˇh : X →Y and quantify its\nstandard excess error. The conversion is through randomizing the prediction of bh over its abstention region,\ni.e., if bh(x) = ⊥, then its randomized version ˇh(x) predicts +1/ −1 with equal probability (Puchkin and\nZhivotovskiy, 2021).\n11Recall that we ignore constants that can be potentially α-dependent and d-dependent.\n34\nProposition 10. The classiﬁer bh returned by Algorithm 4 enjoys proper abstention. With randomization\nover the abstention region, we have the following upper bound on its standard excess error\nerr(ˇh) −err(h⋆) = errγ(bh) −err(h⋆) + γ · Px∼DX (x ∈Xγ).\n(26)\nProof. The proper abstention property of bh returned by Algorithm 4 is achieved via conservation: bh will\navoid abstention unless it is absolutely sure that abstention is the optimal choice (also see the proof of\nLemma 8.\nLet ˇh : X →Y be the randomized version of h : X →{+1, −1, ⊥} (over the abstention region {x ∈X :\nbh(x) = ⊥} ⊆Xγ). We can see that, compared to the Chow’s abstention error 1/2 −γ, the additional error\nincurred over the abstention region is exactly γ · Px∼DX (x ∈Xγ). We thus have\nerr(bh) −err(h⋆) ≤errγ(bh) −err(h⋆) + γ · Px∼DX (x ∈Xγ).\nTo characterize the standard excess error of classiﬁer with proper abstention, we only need to upper bound\nthe term Px∼DX (x ∈Xγ), which does not depends on the (random) classiﬁer bh. Instead, it only depends on\nthe marginal distribution.\nWe next introduce the Massart (Massart and Nédélec, 2006), which can be viewed as the extreme version of\nthe Tsybakov noise by sending β →∞.\nDeﬁnition 12 (Massart noise). The marginal distribution DX satisﬁes the Massart noise condition with\nparameter τ0 > 0 if P(x ∈X : |η(x) −1/2| ≤τ0) = 0.\nProposition 11. Suppose Massart noise holds. By setting the abstention parameter γ = τ0 in Algorithm 4\n(and randomization over the abstention region), with probability at least 1 −δ, we obtain a classiﬁer with\nstandard excess error eO(ε) after querying poly( 1\nτ0 ) · polylog( 1\nε δ) labels.\nProof. This is a direct consequence of Theorem 7 and Proposition 10.\nF.1.2\nComputational eﬃciency\nWe discuss the eﬃcient implementation of Algorithm 4 and its computational complexity in the section. The\ncomputational eﬃciency of Algorithm 4 mainly follows from the analysis in Zhu and Nowak (2022). We\nprovide the discussion here for completeness.\nRegression orcale.\nWe introduce the regression oracle over the set of initialized neural networks Fdnn (line\n1 at Algorithm 2). Given any set S of weighted examples (w, x, y) ∈R+ × X × Y as input, the regression\noracle outputs\nbfdnn := arg min\nf∈Fdnn\nX\n(w,x,y)∈S\nw(f(x) −y)2.\nWhile the exact computational complexity of such oracle with a set of neural networks remains elusive, in\npractice, running stochastic gradient descent often leads to great approximations. We quantify the compu-\ntational complexity in terms of the number of calls to the regression oracle. Any future analysis on such\noracle can be incorporated into our guarantees.\nWe ﬁrst state some known results in computing the conﬁdence intervals with respect to a general set of\nregression functions F.\n35\nProposition 12 (Krishnamurthy et al. (2017); Foster et al. (2018, 2020)). Consider the setting studied in\nAlgorithm 4. Fix any epoch m ∈[M] and denote Bm := {(xt, Qt, yt)}τm−1\nt=1 . Fix any ι > 0. For any data\npoint x ∈X, there exists algorithms Alglcb and Algucb that certify\nlcb(x; Fm) −ι ≤Alglcb(x; Bm, βm, ι) ≤lcb(x; Fm)\nand\nucb(x; Fm) ≤Algucb(x; Bm, βm, ι) ≤ucb(x; Fm) + ι.\nThe algorithms take O( 1\nι2 log 1\nι ) calls of the regression oracle for general F and take O(log 1\nι ) calls of the\nregression oracle if F is convex and closed under pointwise convergence.\nProof. See Algorithm 2 in Krishnamurthy et al. (2017) for the general case; and Algorithm 3 in Foster et al.\n(2018) for the case when F is convex and closed under pointwise convergence.\nWe now state the computational guarantee of Algorithm 4, given the regression oracle introduced above.\nTheorem 16. Algorithm 4 can be eﬃciently implemented via the regression oracle and enjoys the same\ntheoretical guarantees stated in Theorem 7. The number of oracle calls needed is poly( 1\nγ ) · 1\nε; the per-example\ninference time of the learned bhM is eO( 1\nγ2 ·polylog( 1\nε γ )) for general F, and eO(polylog( 1\nε γ )) when F is convex.\nProof. Fix any epoch m ∈[M]. Denote ι :=\nγ\n8M and ιm := (M−m)γ\n8M\n. With any observed x ∈X, we construct\nthe approximated conﬁdence intervals c\nlcb(x; Fm) and d\nucb(x; Fm) as follows.\nc\nlcb(x; Fm) := Alglcb(x; Bm, βm, ι) −ιm\nand\nd\nucb(x; Fm) := Algucb(x; Bm, βm, ι) + ιm.\nFor eﬃcient implementation of Algorithm 4, we replace lcb(x; Fm) and ucb(x; Fm) with c\nlcb(x; Fm) and\nd\nucb(x; Fm) in the construction of bhm and gm.\nBased on Proposition 12, we know that\nlcb(x; Fm) −ιm −ι ≤c\nlcb(x; Fm) ≤lcb(x; Fm) −ιm\nand\nucb(x; Fm) + ιm ≤d\nucb(x; Fm) ≤ucb(x; Fm) + ιm + ι.\nSince ιm + ι ≤γ\n8 for any m ∈[M], the guarantee stated in Lemma 7 can be modiﬁed as gm(x) = 1 =⇒\nw(x; Fm) ≥γ\n4. The guarantee stated in Lemma 8 also holds true since we have c\nlcb(x; Fm) ≤lcb(x; Fm) and\nd\nucb(x; Fm) ≥ucb(x; Fm) by construction. Suppose Fm ⊆Fm−1 (as in Lemma 9), we have\nc\nlcb(x; Fm) ≥lcb(x; Fm) −ιm −ι ≥lcb(x; Fm−1) −ιm−1 ≥c\nlcb(x; Fm−1)\nand\nd\nucb(x; Fm) ≤ucb(x; Fm) + ιm + ι ≤ucb(x; Fm−1) + ιm−1 ≤d\nucb(x; Fm−1),\nwhich ensures that\n1(gm(x) = 1) ≤\n1(gm−1(x) = 1).\nThus, the inductive lemmas appearing in Ap-\npendix D.2.2 can be proved similarly with changes only in constant terms (also change the constant terms\nin the deﬁnition of θ and in Eq. (16), since γ\n2 is replaced by γ\n4 in Lemma 7). As a result, the guarantees\nstated in Theorem 14 (and Theorem 7) hold true with changes only in constant terms.\nWe now discuss the computational complexity of the eﬃcient implementation. At the beginning of each epoch\nm. We use one oracle call to compute bfm := arg minf∈F\nPτm−1\nt=1\nQt(f(xt) −yt)2. The main computational\ncost comes from computing c\nlcb and d\nucb at each time step.\nWe take ι = ι :=\nγ\n8M into Proposition 12,\nwhich leads to O( (log T )2\nγ2\n· log( log T\nγ\n)) calls of the regression oracle for general F and O(log( log T\nγ\n)) calls of the\nregression oracle for any convex F that is closed under pointwise convergence. This also serves as the per-\nexample inference time for bhM. The total computational cost of Algorithm 4 is then derived by multiplying\nthe per-round cost by T and plugging T = θ Pdim(F)\nε γ\n= eO(poly( 1\nγ ) · 1\nε) into the bound .\n36\nF.2\nProof of Theorem 8\nFor ease of construction, we suppose the instance space is X = Bd\n1 := {x ∈Rd : ∥x∥2 ≤1}. Part of our\nconstruction is inspired by Li et al. (2021).\nTheorem 8. Fix any γ ∈(0, 1/8). For any accuracy level ε suﬃciently small, there exists a problem instance\nsuch that (1) η ∈W1,∞\n1\n(X) and is of the form η(x) := ReLU(⟨w, x⟩+ a) + b; and (2) for any active learning\nalgorithm, it takes at least γ−Ω(d) labels to identify an ε-optimal classiﬁer, for either standard excess error\nor Chow’s excess error (with parameter γ).\nProof. Fix any γ ∈(0, 1/8). We ﬁrst claim that we can ﬁnd a discrete subset X ⊆X with cardinality\n|X| ≥(1/8γ)d/2 such that ∥xi∥2 = 1 and ⟨x1, x2⟩≤1 −4γ for any xi ∈X . To prove this, we ﬁrst notice\nthat ∥x1 −x2∥2 ≥τ ⇐⇒⟨x1, x2⟩≤1 −τ 2/2. Since the τ-packing number on the unit sphere is at least\n(1/τ)d, setting τ = √8γ leads to the desired claim.\nWe set DX := unif(X ) and Fdnn := {ReLU(⟨w, ·⟩−(1−4γ))+(1/2−2γ) : w ∈X }. We have Fdnn ⊆W1,∞\n1\n(X)\nsince ∥w∥2 ≤for any w ∈X. We randomly select a w⋆∈X and set f ⋆(·) = η(·) = ReLU(⟨w⋆, ·⟩−(1 −\n4γ)) + (1/2 −2γ). We assume that the labeling feedback is the conditional expectation, i.e., η(x) is provided\nif x is queried.\nWe see that f ⋆(x) = 1/2 −2γ for any x ∈X but x ̸= w⋆, and f ⋆(w⋆) = 1/2 + 2γ.\nWe can see that mistakenly select the wrong bf ̸= f ⋆leads to γ\n4 ·\n2\n|X| =\nγ\n2|X| excess error. Note that the\nexcess error holds true in both standard excess error and Chow’s excess error (with parameter γ) since\nDX (x ∈X : η(x) ∈[1/2 −γ, 1/2 + γ]) = 0 by construction.\nWe suppose the desired access error ε is suﬃciently small (e.g., ε ≤\nγ\n8|X|). We now show that, with label\ncomplexity at most K := ⌊|X|/2⌋= Ω(γ−d/2), any active learning algorithm will, in expectation, pick\na classiﬁer that has Ω(ε) excess error. Since the worst case error of any randomized algorithm is lower\nbounded by the expected error of the best deterministic algorithm against a input distribution (Yao, 1977),\nwe only need to analyze a deterministic learner. We set the input distribution as the uniform distribution\nover instances with parameter w⋆∈X . For any deterministic algorithm, we use s := (xi1, . . . , xiK) to denote\nthe data points queried under the constraint that at most K labels can be queried. We denote bf ∈F as the\nlearned classiﬁer conditioned on s. Since w⋆∼unif(X), we know that, with probability at least 1\n2, w⋆/∈s.\nConditioned on that event, we know that, with probability at least 1\n2, the learner will output bf ̸= f ⋆since\nmore than half of the data points remains unqueried. The deterministic algorithm thus outputs the wrong\nbf ̸= f ⋆with probability at least 1\n2 · 1\n2 =\n1\n4, which has\nγ\n2|X| excess error as previously discussed. When\nε ≤\nγ\n8|X|, this leads to Ω(ε) excess error in expectation.\nG\nOmitted details for Section 4\nWe provide mathematical backgrounds for the Radon BV2 space in Appendix G.1, derive approximation\nresults and passive learning results in Appendix G.2, and derive active learning results in Appendix G.3.\nG.1\nThe Radon BV2 space\nWe provide explicit deﬁnition of the ∥f∥R BV2(X ) and associated mathematical backgrounds in this section.\nAlso see Ongie et al. (2020); Parhi and Nowak (2021, 2022a,b); Unser (2022) for more discussions.\nWe ﬁrst introduce the Radon transform of a function f : Rd →R as\nR{f}(γ, t) :=\nZ\n{x:γ⊤x=t}\nf(x) ds(x),\n(γ, t) ∈Sd−1 × R,\nwhere s denotes the surface measure on the hyperplane {x : γ⊤x = t}. The Radon domain is parameterized\nby a direction γ ∈Sd−1 and an oﬀset t ∈R. We also introduce the ramp ﬁlter as\nΛd−1 := (−∂2\nt )\nd−1\n2 ,\n37\nwhere ∂t denotes the partial derivative with respect to the oﬀset variable, t, of the Radon domain, and the\nfractional powers are deﬁned in terms of Riesz potentials.\nWith the above preparations, we can deﬁne the R TV2-seminorm as\nR TV2(f) := cd∥∂2\nt Λd−1Rf∥M(Sd−1×R),\nwhere cd = 1/(2(2π)d−1) is a dimension-dependent constant, and ∥·∥M(Sd−1×R) denotes the total variation\nnorm (in terms of measures) over the bounded domain Sd−1 × R. The R BV2 norm of f over Rd is deﬁned\nas\n∥f∥R BV2(Rd) := R TV2(f) + |f(0)| +\nd\nX\nk=1\n|f(ek) −f(0)|,\nwhere {ek}d\nk=1 denotes the canonical basis of Rd. The R BV2(Rd) space is then deﬁned as\nR BV2(Rd) := {f ∈L∞,1(Rd) : R BV2(f) < ∞},\nwhere L∞,1(Rd) is the Banach space of functions mapping Rd →R of at most linear growth. To deﬁne the\nR BV2 norm of f over a bounded domain X ⊆Rd, we use the standard approach of considering restrictions\nof functions in R BV2(Rd), i.e.,\n∥f∥R BV2(X ) :=\ninf\ng∈R BV2(Rd)∥g∥R BV2(Rd)\ns.t.\ng|X = f.\nIn the rest of Appendix G, we use P(β) to denote the set of distributions that satisfy (1) Tsybakov noise\ncondition with parameter β ≥0; and (2) η ∈R BV2\n1(X).\nG.2\nApproximation and passive learning results\nProposition 13. Suppose DX Y ∈P(β). One can construct a set of neural network classiﬁer Hdnn such that\nthe following two properties hold simultaneously:\nmin\nh∈Hdnn err(h) −err(h⋆) = O(ε)\nand\nVCdim(Hdnn) = eO(ε−\n2d\n(1+β)(d+3) ).\nProof. We take κ = ε\n1\n1+β in Theorem 9 to construct a set of neural network classiﬁers Hdnn with W =\nO(ε−\n2d\n(1+β)(d+3) ) total parameters arranged in L = O(1) layers. According to Theorem 4, we know\nVCdim(Hdnn) = O(ε−\n2d\n(1+β)(d+3) · log(ε−1)) = eO(ε−\n2d\n(1+β)(d+3) ).\nWe now show that there exists a classiﬁer h ∈Hdnn with small excess error. Let h = hf be the classiﬁer such\nthat ∥f −η∥∞≤κ. We can see that\nexcess(h) = E\n\u0002\n1(h(x) ̸= y) −\n1(h⋆(x) ̸= y)\n\u0003\n= E\n\u0002\n|2η(x) −1| ·\n1(h(x) ̸= h⋆(x))\n\u0003\n≤2κ · Px∼DX (x ∈X : |η(x) −1/2| ≤κ)\n= O(κ1+β)\n= O(ε),\nwhere the third line follows from the fact that h and h⋆disagrees only within region {x ∈X : |η(x)−1/2| ≤κ}\nand the incurred error is at most 2κ on each disagreed data point. The fourth line follows from the Tsybakov\nnoise condition and the last line follows from the selection of κ.\n38\nTheorem 17. Suppose DX Y ∈P(β). Fix any ε, δ > 0. Let Hdnn be the set of neural network classiﬁers\nconstructed in Proposition 13. With n = eO(ε−4d+6+β(d+3)\n(1+β)(d+3) ) i.i.d. sampled data points, with probability at least\n1 −δ, the empirical risk minimizer bh ∈Hdnn achieves excess error O(ε).\nProof. Proposition 13 certiﬁes minh∈Hdnn err(h)−err(h⋆) = O(ε) and VCdim(Hdnn) = O\n\u0010\nε−\n2d\n(1+β)(d+3) · log(ε−1)\n\u0011\n.\nTake ρ = 1 in Theorem 11, leads to\nerr(bh) −err(h⋆) ≤O\n \nε +\n\u0012\nε−\n2d\n(1+β)(d+3) · log(ε−1) · log n\nn\n\u0013 1+β\n2+β\n+ log δ−1\nn\n!\n,\nTaking n = O(ε−4d+6+β(d+3)\n(1+β)(d+3) ·log(ε−1)+ε−1·log(δ−1)) = eO(ε−4d+6+β(d+3)\n(1+β)(d+3) ) thus ensures that err(bh)−err(h⋆) =\nO(ε).\nG.3\nActive learning results\nTheorem 10. Suppose η ∈R BV2\n1(X) and the Tsybakov noise condition is satisﬁed with parameter β ≥0.\nFix any ε, δ > 0. There exists an algorithm such that, with probability at least 1 −δ, it learns a classiﬁer\nbh ∈Hdnn with excess error eO(ε) after querying eO(θHdnn(ε\nβ\n1+β ) · ε−\n4d+6\n(1+β)(d+3) ) labels.\nProof. Construct Hdnn based on Proposition 13 such that minh∈Hdnn err(h)−err(h⋆) = O(ε) and VCdim(Hdnn) =\neO(ε−\n2d\n(1+β)(d+3) ). Taking such Hdnn as the initialization of Algorithm 3 (line 1) and applying Theorem 12 leads\nto the desired result.\nTo derive deep active learning guarantee with abstention in the Radon BV2 space, we ﬁrst present two\nsupporting results below.\nProposition 14. Suppose DX Y ∈P(β). One can construct a set of neural network regression functions\nFdnn such that the following two properties hold simultaneously:\n∃f ∈Fdnn s.t. ∥f −f ⋆∥∞≤κ,\nand\nPdim(Fdnn) ≤c · κ−2d\nd+3 log2(κ−1),\nwhere c > 0 is a universal constant.\nProof. The result follows by combining Theorem 9 and Theorem 13.\nProposition 15. Suppose η is L-Lipschitz and X ⊆Bd\nr. Fix any κ ∈(0, γ/32]. There exists a set of neural\nnetwork regression functions Fdnn such that the followings hold simultaneously.\n1. Pdim(Fdnn) ≤c · κ−2d\nd+3 log2(κ−1) with a universal constant c > 0.\n2. There exists a f ∈Fdnn such that ∥f −η∥∞≤κ.\n3. θval\nFdnn(γ/4) := supf∈Fdnn,ι>0 θval\nf (Fdnn, γ/4, ι) ≤c′ · ( Lr\nγ )d with a universal constant c′ > 0.\nProof. The implementation and proof are similar to those in Proposition 9, except we use Proposition 14\ninstead of Proposition 6.\nWe now state and prove deep active learning guarantees in the Radon BV2 space.\nTheorem 18. Suppose η ∈R BV2\n1(X).\nFix any ε, δ, γ > 0. There exists an algorithm such that, with\nprobability at least 1 −δ, it learns a classiﬁer bh with Chow’s excess error eO(ε) after querying poly( 1\nγ ) ·\npolylog( 1\nε δ) labels.\n39\nProof. The result is obtained by applying Algorithm 4 with line 1 be the set of neural networks Fdnn generated\nfrom Proposition 15 with approximation level κ ∈(0, γ/32] (and constants c, c′ speciﬁed therein). The rest\nof the proof proceeds in a similar way as the proof Theorem 7. Since we have r = 1 and L ≤1 (Parhi and\nNowak, 2022b), we only need to choose a κ > 0 such that\n1\nκ = ˇc ·\n\u0012 1\nγ\n\u0013 d\n2 +1\n· log 1\nε γ\nwith a universal constant ˇc > 0. With such choice of κ, we have\nPdim(Fdnn) = O\n\n\n\u0012 1\nγ\n\u0013 d2+2d\nd+3\npolylog\n\u0012 1\nε γ\n\u0013\n.\nPlugging this bound on Pdim(Fdnn) and the upper bound on θval\nFdnn(γ/4) from Proposition 15 into the guarantee\nof Theorem 14 leads to excessγ(bh) = O(ε · log(\n1\nε γ δ)) after querying\nO\n\n\n\u0012 1\nγ\n\u0013d+2+ d2+2d\nd+3\n· polylog\n\u0012 1\nε γ δ\n\u0013\n\nlabels.\n40\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2022-10-15",
  "updated": "2022-10-15"
}