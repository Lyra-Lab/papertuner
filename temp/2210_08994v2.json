{
  "id": "http://arxiv.org/abs/2210.08994v2",
  "title": "Knowledge Representation for Conceptual, Motivational, and Affective Processes in Natural Language Communication",
  "authors": [
    "Seng-Beng Ho",
    "Zhaoxia Wang",
    "Boon-Kiat Quek",
    "Erik Cambria"
  ],
  "abstract": "Natural language communication is an intricate and complex process. The\nspeaker usually begins with an intention and motivation of what is to be\ncommunicated, and what effects are expected from the communication, while\ntaking into consideration the listener's mental model to concoct an appropriate\nsentence. The listener likewise has to interpret what the speaker means, and\nrespond accordingly, also with the speaker's mental state in mind. To do this\nsuccessfully, conceptual, motivational, and affective processes have to be\nrepresented appropriately to drive the language generation and understanding\nprocesses. Language processing has succeeded well with the big data approach in\napplications such as chatbots and machine translation. However, in human-robot\ncollaborative social communication and in using natural language for delivering\nprecise instructions to robots, a deeper representation of the conceptual,\nmotivational, and affective processes is needed. This paper capitalizes on the\nUGALRS (Unified General Autonomous and Language Reasoning System) framework and\nthe CD+ (Conceptual Representation Plus) representational scheme to illustrate\nhow social communication through language is supported by a knowledge\nrepresentational scheme that handles conceptual, motivational, and affective\nprocesses in a deep and general way. Though a small set of concepts,\nmotivations, and emotions is treated in this paper, its main contribution is in\narticulating a general framework of knowledge representation and processing to\nlink these aspects together in serving the purpose of natural language\ncommunication for an intelligent system.",
  "text": "arXiv:2210.08994 v2 [cs.AI] 19 Oct 2022 \n \n \nKnowledge Representation for Conceptual, Motivational, and \nAffective Processes in Natural Language Communication \nSeng-Beng Ho  \nDeptartment of Social and \nCognitive Computing \nInstitute of High Performance \nComputing \nSingapore \nhosb@ihpc.a-star.edu.sg \n \nZhaoxia Wang \nSchool of Computing and \nInformation Systems \nSingapore Management \nUniversity \nSingapore \nzxwang@smu.edu.sg \n \nBoon-Kiat Quek \nDepartment of Social and \nCognitive Computing \nInstitute of High Performance \nComputing \nSingapore \nquekbk@ihpc.a-star.edu.sg \n \nErik Cambria \nSchool of Computer Science and \nEngineering \nNanyang Technological \nUniversity \nSingapore \ncambria@ntu.edu.sg\n \nAbstract—Natural language communication is an intricate and \ncomplex process. The speaker usually begins with an intention and \nmotivation of what is to be communicated, and what effects are \nexpected from the communication, while taking into consideration \nthe listener’s mental model to concoct an appropriate sentence. \nThe listener likewise has to interpret what the speaker means, and \nrespond accordingly, also with the speaker’s mental state in mind. \nTo do this successfully, conceptual, motivational, and affective \nprocesses have to be represented appropriately to drive the \nlanguage generation and understanding processes. Language \nprocessing has succeeded well with the big data approach in \napplications such as chatbots and machine translation. However, \nin human-robot collaborative social communication and in using \nnatural language for delivering precise instructions to robots, a \ndeeper representation of the conceptual, motivational, and \naffective processes is needed. This paper capitalizes on the \nUGALRS \n(Unified \nGeneral \nAutonomous \nand \nLanguage \nReasoning System) framework and the CD+ (Conceptual \nDependency Plus) representational scheme to illustrate how social \ncommunication through language is supported by a knowledge \nrepresentational scheme that handles conceptual, motivational, \nand affective processes in a deep and general way. Though a small \nset of concepts, motivations, and emotions is treated in this paper, \nits main contribution is in articulating a general framework of \nknowledge representation and processing to link these aspects \ntogether \nin \nserving \nthe \npurpose \nof \nnatural \nlanguage \ncommunication for an intelligent system. \nKeywords—natural language communication, natural language \nunderstanding, knowledge representation in communication, \nmotivational processes in communication, affective processes in \ncommunication \nI. INTRODUCTION \nIn current AI research, natural language understanding is \ntypically treated separately from natural language generation, \nand also natural language understanding is typically processed \nat a superficial level, with no deep representation of meaning \n[1], [2]. Despite the fact that there is typically no deep meaning \nrepresentation present in these language processing systems, \nconversational systems such as chatbots and various machine \ntranslation systems have achieved quite significant degrees of \ncommercial successes [3].  \nHowever, there are certain applications of natural language \nunderstanding that require a deeper or grounded level of \nrepresentations. For example, in using natural language \ninstructions to instruct a robot to carry out certain actions, the \nrobotic system involved has to comprehend the instructions to a \nlevel at which the grounded or “true” meanings are recovered, \nrepresented, and put to use in the form of actions and behaviors. \nAlso, motivational and affective processes feature prominently \nin human language communication [4]. For language \ngeneration, firstly there has to be some motivations involved, \nmodulated by the ongoing emotional states, before an utterance \nis constructed and emitted. For language understanding, a model \nof the utterer’s motivational and emotional states has to be \npresent in the listener’s “mind,” namely the internal language \nunderstanding processes, before a sentence spoken can be \nappropriately understood with the attendant subsequent correct \nresponses by the listener. The understanding of sentiments \npresent in sentences has been an extensively researched area in \nAI in the form of sentiment analysis [5]–[11]. However, there \nhas been no integrated natural language communication \nframework proposed that links language generation and \nunderstanding processes together in a principled manner, at the \nsame time incorporating a principled treatment of motivational \nand emotional processes which are intimately connected with \nlanguage generation and understanding [1], [2]. This paper \nattempts to address these issues using the framework of the \nUGALRS architecture proposed by Ho [12]. The remainder of \nthis paper is organized as follows: Section II discusses the \nmotivation and background; Section III presents the basic \narchitectural and representational constructs; Section IV \ndescribes the core of the representations for language \ncommunication; finally, Section V proposes concluding \nremarks. \nII. MOTIVATION AND BACKGROUND \nNatural language communication is a two-way process – \ngeneration and understanding. When a human emits an \nutterance, it is with a listener in mind, and it is also at the end of \na long process of beginning with a motivation and intention to \ncommunicate about something, and then, depending on the \ncontext and emotional state of the human involved and the \nintended effect on the listener, an appropriate sentence is then \nconcocted to precisely communicate the information to the \nintended listener [29]. To do this successfully, the utterer must \nalso have a good mental model of the mental and emotional \nstates of the listener.   \nCurrent big data approaches to language processing, such as \nthe approach taken by the GPT-3 model [3], though successful \nin many applications, do not model these internal processes. The \ndownside of that is that certain explanations and subsequent \nrelated corrective actions or suggestions are not possible. For \nexample, suppose Person A asks Person B, “Why do you say \nthat to him?” [II.1] And Person B replies, “I want him to hurt.” \n[II.2] Person A may then suggest, “Well, I would suggest a better \nway to do that, which is to…” [III.3]. Utterance II.2 is an \nexplanation of why Person B said something, and Person B, \nhaving a model of her own mental processes beginning with her \nmotivation for uttering the earlier sentence Person A is asking \nher about, is able to give Person A an explanation of what \n“causes” her to say the earlier thing that was intended to hurt a \ncertain person. Person A, who presumably also has an internal \nmodel of herself, Person B, and perhaps also the person whom \nPerson B wants to hurt, is then able to reason about and \nunderstand the various mental causalities involved and propose \na different way to generate hurt to the person Person B intends \nto hurt. Whether her suggestion will be a malicious one intended \nto go along with and please Person B or a benign one to placate \nthe situation for the betterment of all involved will depend on \nthe background of their conversation and other intentions of \nPerson A to start with. Processes such as these are present when \nhumans communicate with robots to instruct them to carry out \ncertain tasks, or when robots are communicating with each other \nin collaborating to perform certain tasks, and a robotic or AI \nsystem would benefit from a fuller model of the internal \nprocesses of language generation, communication, and \nunderstanding. \nA \nrobotic \nsystem \nthat \nis \ninvolved \nin \nlanguage \ncommunication is basically an autonomous system, and \nlanguage generation and understanding processes involve a \nnumber of components in an autonomous system. Robotic \nsystems that are in communication and collaboration with other \nsystems are social agents. Therefore, to fully elucidate the \nprocesses involved in language communication, it will be \nbeneficial to look at related work in which an internal operating \narchitecture of an autonomous system or social agent is \nproposed.  Also, the issues of grounded language representation \nneed to be addressed. Ho [12] provides an autonomous system \narchitecture, the Unified General Autonomous and Language \nReasoning System (UGALRS), that elucidates various essential \noperating components, as well as a grounded meaning \nrepresentation framework based on an enhanced version of the \nconceptual dependency (CD) theory [13]–[15] called CD+, that \noperates within UGALRS. In this paper, we will use UGALRS \nand CD+ as our representational framework. \nAs elucidated earlier, motivational and affective processes \nare intimately involved in language communication processes, \nwhich have not been adequately addressed in most language-\nrelated work, whether they are work in linguistics [16]–[19] or \ncomputational linguistics and natural language processing [1], \n[2], [20]. As will be illustrated, the UGALRS plus CD+ \nframework provides the necessary representational constructs to \nhandle motivational and affective processes as well. \nMotivational and affective processes are essential components \nin the functioning of a social agent involved in communication \nand collaboration, thus, UGALRS is also an architecture for a \nsocial agent. \nQuek [21]–[23] has articulated an architecture that \nincorporates motivation and emotion in the functioning of an \nautonomous system. Though his work is directed toward the \nusual robotic actions, driven by motivational and affective \nprocesses, it dovetails with the current work in which the \n“actions” involved would correspond to language generation in \nthe communication process. \nAs each of the domains of conceptual, motivational, and \naffective processes is extensive in itself, in this paper we focus \non using a small subset of each aspect to articulate and elucidate \nthe intricate connections between them in a general framework \nthat can be extended in future work. \nIII. BASIC ARCHITECTURAL AND REPRESENTATIONAL \nCONSTRUCTS \nAs mentioned above, our framework is based on the \nUGALRS architecture and the associated CD+ representational \nscheme. In this section, we will discuss them accordingly. \nA. The CD+ Concept Representational Framework \nHo [12] developed a general representational framework that \ncan be used to represent a large variety, if not all, concepts. It is \nhighlighted in [12] that many of the often-encountered concepts \nare functional in nature. The framework is termed CD+, which \nis in turn derived from Schank’s CD conceptual representational \nframework [13]–[15]. The representations used in CD+ are \ncognitive, causal, and grounded. \n \n(a) \n(b) \nFig. 1. Examples of CD+ representations. (a) “Person pushes the door open.” \n(b) Person wants something. \nFig. 1(a) shows the representation of the sentence, or \nconceptualization, “Person pushes the door open.” The \nhorizontal double arrow links the subject, Person, to the object \n(o), Door. This is termed a conceptualization. In CD+, the two \nmain constructs being enhanced over CD are the Structure \nAnchor (SA) and the CD+ Elaboration (CD+E) [12]. SA is a \ndetailed structural or analogical representation of the object the \nsymbol refers to, which provides grounding for the symbol. In \nthis case, for a Person, there would be the representational \ndetails of the various parts and the overall structure, specified to \nthe details of every point, limbs, and joints, as shown. A \nconvenient way to implement this representational model would \nbe the kind of representation used for objects in computer \ngraphics, which is an analogical representation in the form of a \nhigh density point cloud consisting of points corresponding to \nevery point of the object involved, or some vectorial \nrepresentations that represent the loci of these points. The \nvarious parts on the object involved, say, in the case of Person, \nthe various body parts such as the head, torso, and limbs, may \nbe movable relative to each other, and these have to be captured \nin the model. CD+E is used to elaborate on the symbols that \nrepresent certain actions, such as in this case, PUSH. PUSH \ninvolves a number of sub-steps, such as “first place palm flat on \nDoor near center of Door,” “then exert strength in the direction \nperpendicular to Door’s surface,” etc. Here we use English \nsentences to describe these steps for the ease of illustration, but \neach of these steps is in turn representable in CD+ form. There \nis a hierarchy of details until a “ground level” is reached in \nwhich the concepts used are “ground level concepts.” A number \nof ground level concepts is listed in [12] that is supposed to be \nthe common ground for all (or at least a large majority of) \nconcepts. In the two English sentences above used to describe \nthe CD+E involved, every concept used in their representations \nhas to be clearly defined, in further elaboration in the form of \nCD+E, or in the form of a ground level concept (which could be \na SA). So, all the symbols, “first,” “place,” “palm,” “near,” \n“center,” “of,” and “Door” in the sentence “first place palm near \ncenter of Door” have to be defined and the CD+ framework that \nmakes this possible is described in detail in [12]. For the rest of \nthe paper, we will omit the details of the SAs and some of the \nCD+Es to avoid clutter. The use of the SAs and some of the \nCD+Es are more relevant to other tasks as discussed in [12]. \nThe sentence “Person pushes the door open” connotes some \ncausality that is not explicit in the sentence. The implicit concept \ninvolved is “Person pushes the door and it causes the door to \nopen.” Therefore, in Fig. 1(a), there is a vertical arrow with a \nline down in the middle representing the causality involved. The \nhorizontal double arrow with a line down in the middle \nrepresents the “state” of something, in this case, the state of \nDoor is Open after being pushed. The concept of Open also \nneeds to be grounded, in this case in the form of an SA. \nIn Fig. 1(b), a more complex conceptualization is shown. It \ninvolves the concept of WANT. In a sentence such as “Person \nwants X,” in which X could be an object (say, “ice cream”) or it \ncould be another conceptualization (say, “the house to be \ndemolished”), the hidden causality is that “if X is obtained or \ncan be realized, Person will be pleased.” (The object of WANT \nis indicated with a link labelled with an “o”) “Pleased” is a \nfundamental ground level concept capturing the emotional state \nof the person involved. It is considered a basic emotional state. \nIn Fig. 1(b), the vertical double arrow with a gray box around \nit represents any conceptualization (such as “House be \nDemolished”), and when Person “WANT” that, the concept of \nWANT is CD+ Elaborated into a causal connection between, \nsay, “House be Demolished” (the same conceptualization of the \nobject of WANT) and Person being in the state of Pleased. The \n“c” above the horizontal gray box represents the conditional \n(“if”), and “f” represents the future tense. I.e., “if the house is \ndemolished, it will cause Person to be Pleased.” Now, this entire \ncausation is in turn a conceptualization created in Person’s \ninternal mental processes.  \nTherefore, this entire conceptualization is the object of what \nPerson conceptualizes - CONCP (CONCePtualize). There are \nother more complex CD+ constructs discussed in [12] but these \nexamples would suffice for our subsequent discussion. \nB. The UGALRS Architecture \nAs mentioned above, CD+ representations operate within a \ngeneral autonomous system (or social agent) architecture, the \nUGALRS, for the representation of concepts [12]. Fig. 2 shows \npart of the full UGALRS architecture that focuses on the \nlanguage aspects. The focus of our attention is on the \nLANGUAGE COMMUNICATION REASONING CORE \n(LCRC) but it is by no means the only module that is important. \nThe reason why this module occupies a larger space in this \nfigure and has its details – the sub-modules – illustrated is that \nthe CD+ representations that we will be using for illustrating the \nconcepts involved in language communication reference these \nsub-modules in LCRC. The submodules in LCRC are \nPROBLEM \nSOLVING \n(PS), \nSIMULATION \n(SM), \nBUFFER(BF), and CONTROL(CT) modules. \n \nFig. 2. The UGALRS architecture for an Intellient Autonomous System \n(IAS), a robot, or a social agent. Based on [12]. See text for explanation. \nIn the full UGALRS [12] there is a corresponding module, \nREASONING CORE (REAC) to LCRC, whose major input and \noutput are the PERCEPTION and ACTION systems \nrespectively (called the SENSORY AND ACTION CORE – \nSAAC) that are non-language related, but involve the usual \nperception and action processes. The basic idea behind \nUGALRS views the language communication process as similar \nto the usual “perception and action” processes related to vision \nand robotic limb action, but in the sphere of language. In the \nlanguage sphere, “perception” is “language understanding,” and \n“action” is “language generation.”  \nThese are done through the LANGUAGE SENSORY AND \nACTION CORE (LSAC) here. To utter something, an \nintelligent autonomous system (IAS) would begin with some \nmotivation, thus, the MOTIVATION CORE (MOTC). (On the \nREAC side, the MOTC would drive its reasoning to either \nunderstand the perceived information or to generate physical \nactions.) PS directs a problem-solving process to concoct an \nappropriate sentence to (hopefully) satisfy the motivation \ninvolved (like concocting an appropriate physical action through \nPS on the REAC side).  \nSM is used to simulate, based on some earlier learned \nlanguage communication rules, what language responses from \nthe intended recipient of the current utterance are expected. BF \ncontains the concepts that are currently being operated on. CT \ndirects and control all other modules. \nThe \nEXPERIENTIAL \nCORE \n(EXPC) \nrecords \nall \nexperiences, linguistic or otherwise. Therefore, there is a path \nfrom the PERCEPTION module to EXPC. EXPC also records \ninternal experiences such as what takes place in PS, SM, or what \nactions (utterances) are (were) emitted. EXPC also provides the \ncontext for PS, and directs prospective simulation in SM and \nreceives its results. EXPC can be divided into 3 portions – the \npresent (PRESENT(EXPC)), the past (RETRO(EXPC)) and the \nfuture (PROSP(EXPC)). RETRO(EXPC) and PROSP(EXPC) \ncan be used to ground the concepts of past and future as \nillustrated in [12]. \nThe CONCEPTUAL CORE (CONC) stores knowledge as \nrules and scripts represented in the form of CD+. Scripts are long \ncausal sequences of events that pertain to certain knowledge \ncomplexes, as articulated in [15] and explained in the context of \nCD+ in [12]. In Fig. 2, it is shown that “Models of Other Social \nAgents” are stored in EXPC and CONC, in EXPC in the form \nof un-generalized instances, and in CONC, in the form of \ngeneralized knowledge. The dotted box of “Models of Other \nSocial Agents” is not a functioning module of UGALRS, but \nserves to indicate the knowledge involved and where it is located \nin UGALRS. The sources of external knowledge for EXPC and \nCONC go through the PERCEPTION module in LSAC. \nIV. REPRESENTATIONS FOR LAUGUAGE COMMUNICATION \nArmed with the devices and constructs provided by \nUGALRS and CD+, in this section we illustrate their uses in \nrepresenting the complex and intricate processes involved in \nlanguage communication. It will be seen that between just a few \nsentences, many processes take place in the internal reasoning \nand problem solving modules of the utterer, whether it be human \nor robot (i.e., an IAS or a robotic social agent). These processes \ninvolve not only the conceptual, but also the motivational and \naffective, that can be represented by CD+ within UGALRS. \nIn the following, we will represent the internal conceptual, \nmotivational, and affective processes in both Person and Robot \nusing CD+. Even though the primary purpose of AI is in \ninvestigating the computational and representational processes \nin IAS (robots), there are two purposes in elucidating similar \nprocesses in the Person involved as well. First, Person can be \nanother robot engaging in natural language communication with \nthe first robot. Second, a robot or IAS can model the “mental” \nprocesses of another person or robot as well, which is the block \nindicated in the bottom left corner of Fig. 2. Therefore, in the \nfollowing, we elucidate the processes taking place in the person \nas well as the robot. \nA. Motivation and Sentence Concoction/Generation \nBefore an utterance is made, the utterer must begin with an \nidea. This idea could be just a thought to be shared, or a want to \nbe conveyed. Suppose a person (Person) thinks of asking a robot \n(Robot) to bring her a tool, Tool(X), from the table. This would \nbe her “WANT,” which if the robot could succeed in listening \nto her and satisfying it, she would be Pleased.  \nAt the very top part of Fig. 3, this is represented after the \nsame fashion as the representation of Fig. 1(b). Firstly, here are \nSAs associated with Person and Robot, and SAs such as these \nwill be omitted in subsequent figures to avoid clutter. The \nPTRANS concept is Physical TRANSfer, used and explained in \n[13] and [12]. There is a “from” location (Loc(Table)) and a “to” \nlocation (Loc(Person)) and a Direction (D) of transfer. So, in this \ncase, Person conceptualizes (CONCP) that if Robot were to \nPTRANS Tool(X) from Table to her, she would be Pleased. “I” \non the right most side of the PTRANS representation represent \nthe “Instrument” that Robot might use for this purpose, such as \nusing its legs to propel itself along the ground. The PTRANS \nprocess involves a series of steps. Suppose Robot is currently \nnext to Person and Table is some distance away, Robot would \nfirst turn its body and face Table, viewing from far to see that \nTool(X) is on Table, then mentally, through a PS process, plot a \npath to Table, and after executing, say, a pickup action, bring \nTool(X) to Person. This sequence of events is first worked out \nin Robot’s REAC module (i.e., the usual non-language related \nproblem solving and planning process). This is the HOW in the \nCD+ Elaboration (CD+E) pointed to from PTRANS. \n \n \nFig. 3. The conceptual, motivational, and affective processes involved in the \nutterance “Robot, please bring me Tool(X) from the table.” See text for \nexplanation. The black “Motivates” arrow is not part of the representation, but \nan indication of the source of the causal link involved. \nThe desired Pleased state in the WANT conceptualization is \na motivational force that propels Person to proceed to look for \nsolutions to realize the Pleased state. Pleased is a basic and \nground level emotion, as discussed in [12] \nNow, as discussed in [12], when a WANT is conceptualized, \nthere may or may not be a solution to satisfy the object of the \nWANT. Therefore, the exact “HOW” is “IRRELEVANT” in the \nconcept of WANT. Hence, there could be a situation in which “I \nwant to get rich but I can’t.” If a solution exists, then the concept \nof CAN comes into play. So, if “Robot CAN bring Tool(X) from \nTable to Person,” then it means a solution exists. This \nrepresentation of CAN is given in [12] and will be shown in Fig. \n4. The entire CONCP encased in a box is called a \nMOTIVATION CONCEPT (M-CONC). \nNext, having the WANT of a certain event (namely the \nRobot bringing Tool(X) to the Person), Person then WANTs to \ncommunicate the concept that she WANTs this certain event to \nhappen to Robot. There is a general rule that says if an IAS (a \nhuman is a natural IAS while a robot is an artificial one) wants \nsomething, a motivation to achieve the state of “Pleased” will \ndrive the IAS to do one of three things: 1. Carry out a planning \nor problem solving process to achieve the state of Pleased by \nherself or itself; 2. Request the help of someone else to do so; \nand 3. Command a servile agent to do so. This is encoded as the \nfirst causal link near the top of the figure. Now, note that at the \nvery top of the figure, the WANT conceptualization (the \ntopmost double arrow) is encased in a gray box with a label “1.” \nThis entire conceptualization “1”, that the Person wants Robot \nto do something, is now the object of an MTRANS (Mental \nTRANSfer) process intended to “mentally” transfer the WANT \nconceptualization “1” from Person to Robot, that will make \nPerson Pleased. (I.e., Person WANTs her WANT, currently in \nher mind, to be MTRANS to Robot’s mind. In computational \nterms, “mind” is simply the internal memory and processing \nmechanisms of the human or robot involved.) \nIn order to realize this communication, Person concocts a \nsentence by Mentally BUILDing (MBUILDing) the sentence \nfrom considering the conceptualization involved (labeled “1”), \ntogether with the grammar of the language involved, the \nintended tone of the sentence, the emotional state of Person, etc. \n(the intended tone is dependent on the existing context of \ncommunication). The sentence constructed is “Robot, please \nbring me Tool(X) from the table,” as shown in the figure as \nconceptualization “2”. (The fact that Person WANTs Robot to \ndo something is not explicitly stated in the sentence, but it is \nimplied. Person could also have stated more explicitly, “Robot, \nI want you to bring me Tool(X) from the table.) The MBUILD \nconept is discussed in [13] and [12]. The MBUILDing of the \nsentence takes place in BF(LCRC(PERSON)). The precise \nprocess of converting an internal meaning representation in \nCD+ \nform \nto \na \ngrammatical \nsurface \nsentence \nfor \ncommunication is relegated to future work. \nAfter the sentence is concocted in BF(LCRC(PERSON)), it \nit MTRANSed to ACTION(LSAC(PERSON)) to be emitted as \nan utterance. The link between MBUILD and MTRANS is a \n“temporal” one, not a causal one, as the second step simply \nfollows the first step as part of the process (temporal links are \nindicated as a thickened arrow without a line running down its \nmiddle). \nWhen a sentence, whether one that is a command or request, \nor just a factual statement, is uttered toward a recipient, a state \nof ANTICIPATION is entered (which is part of the implication \nof command or request) and the utterer then ANTICIPATEs \nsomething, as shown in the bottom of Fig. 3. This is the first \naffective state that emerges in the present communication \nprocess and will be discussed in detail in the next section. This \nANTICIPATION is accompanied by HOPE as the prospect is \npositive [24].  \nB. Affective States and Illocutionary Forces \nIn the non-linguistic sphere, when a certain action is emitted \nby an IAS, it is expected to cause certain effects. Similarly, in \nthe linguistic sphere, an emitted utterance is expected to cause \nsome effects. This has been investigated by speech act theorists \n[25]. If an utterance is meant to communicate certain \ninformation to the recipient, there may be no immediate overt \nactions or responses expected in the recipient, but the \ninformation conveyed may cause future actions or responses, or \nin the least, it may cause certain changes in the beliefs of the \nrecipient. If the utterance is in the form of a command or request, \nimmediate actions and responses are expected. The utterance is \nsaid to have an “illocutionary force.” [25] \nAt the bottom of Fig. 3 we show that Person enters am \naffective state of ANTICIPATION and she ANTICIPATEs \nsomething. In Fig. 4(a) we show the functional representation of \nANTICIPATE, an action that accompanies the affective state \nANTICIPATION. If an Agent ANTICIPATEs a certain \nconceptualization, she conceptualizes that the conceptualization \ninvolved will happen in the future. It is shown in Fig. 4(a) that \nthe object of Agent’s CONCP is labeled with an “f”, which \nmeans it resides in the prospective part of the EXPC (Fig. 2), \nPROSP(EXPC). This formulation of an affective state and its \nassociated causal consequences is in consonant with the \ncognitive appraisal theory of emotion [24]. The same approach \nwill be adopted with the other affective states in subsequent \ndiscussions. \nSpecifically, in the situation depicted in Fig. 3 in which \nPerson asks Robot to bring her Tool(X) from Table, she \nANTICIPATEs both the facts that “Robot WANTs to PTRANS \nTool(X) from Table to Person so that Person is Pleased” and \n“Robot CAN PTRANS Tool(X) from Table to Person so that \nPerson is Pleased,” as shown in Fig. 4(b). \nFirst, let us consider the representation for “Robot WANTs \nto PTRANS Tool(X) from Table to Person so that Person is \nPleased.” This is a transfer of what Person WANTs to what \nRobot WANTs. Now, for Person to reasonably assume that \nRobot would WANT to Please her, it must be assumed that \nRobot has either a SERVILE or an ALTRUISTIC attitude. In \nsituations in which Robot or other recipient(s) of the utterance \nis REBELLIOUS or UNCOOPERATIVE, then this situation is \nnot obtained. In Fig. 5 we depict that Robot is indeed SERVILE \nor ALTRUISTIC and hence in Fig. 4(b) Person ANTICIPATES \nthat “Robot WANTs to PTRANS Tool(X) from Table to Person \nso that Person is Pleased.” Hence, Robot being Pleased is caused \nby Person being Pleased. CD+ can be used to represent the \nconnections between attitudes such as being SERVILE, \nALTRUISTIC, \nCOOPERATIVE, \nREBELLIOUS, \nor \nUNCOOPERATIVE and whether the entity/IAS involves \nWANTs to do certain things. The details of these are left to \nfuture work. \nThe locus of the illocutionary force is this. In any IAS, \nultimately it will do whatever Pleases it. The arrow labeled PS \nshows the flow of Robot’s actions: in order to please itself, it has \nto please Person, and in order to please Person, it has to \nPTRANS Tool(X) from Table to Person, if it CAN. \nAs mentioned above, there is a difference between WANT \nand CAN [12]. The primary difference is that WANTing \nsomething to happen (e.g., PTRANSing something from one \nplace to another) does not imply that a solution exists for the \nthing to happen, but CAN implies that the solution exists. \nTherefore, there could be a situation that “I want to go from here \nto there but I can’t”. Hence, the representation for “Robot CAN \nPTRANS Tool(X) from Table to Person so that Person is \nPleased” shown in Fig. 4(b) is that PS(REAC(ROBOT)) returns \na solution (Solution(X)) for Robot to PTRANS Tool(X) from \nTable to Person. (EXTRANS stands for EXistential \nTRANSformation in which something goes from non-existence \nto existence or vice versa – which is used to represent the \nexistence of a Solution(X) – see [12]) \n \n(a) \n \n(b) \nFig. 4. (a) The general representation of ANTICIPATION. There is a state of \nANTICIPATION, and an object referred to by the ANTICIPATE action. (b) \nThe specific case of ANTICIPATION at the bottom of Fig. 3. \nC. Sentence Understanding, Actions, and Affective States \n Now that the utterance has been made and presumably \nreceived by Robot, the first step of the process on Robot’s side \nis to MTRANS the received utterance into BF(LCRC(ROBOT)) \nfor further processing as shown in Fig. 5. This causes Robot to \nMBUILD the conceptualization corresponding to the received \nutterance, based on the grammar of the language, the tone \npresent in the sentence, the current emotional state of Robot and \nthe perceived emotional state of Person, etc. This MBUILDed \nconceptualization is labeled “1”, which is the same as \nconceptualization “1” in Fig. 3. Assuming Robot has either a \nSERVILE, COOPERATIVE, or ALTRUISTIC attitude, this \ncauses it to create the next conceptualization capturing the fact \nthat Robot will be Pleased if Person is Pleased due to Robot \ncarrying out a certain task. This then motivates Robot to seek a \nsolution to conceptualization “4”, which is “Robot PTRANS \nTool(X) from Table to Person.” This conceptualization is \nMTRANS from BF(REAC(ROBOT)) to PS(REAC(ROBOT)).  \n \nFig. 5. Robot’s internal processing in response to Person’s utterance in Fig. 4. \nSee text for explanation. Unlike in Fig. 3, ANTICIPATE here is  accompanined \nby FEAR as it is anticipating anegative prospect [24]. \nSuppose PS(REAC(ROBOT)) cannot find a solution \nsubsequently. This situation is represented in CD+ using \nEXTRANS showing that a solution does not exist (see [12] for \nthe concept of CANNOT). It causes Robot to enter states of \nFRUSTRATED, DISPLEASED and FEAR (unlike for the case \nof ANTICIPATION in Figs. 3 and 4(a), these are not shown in \nFig. 5 to avoid clutter) and it is also FRUSTRATED, \nDISPLEASED, and FEARful about the un-attainment of \nconceptualization “4” as shown in Fig. 5 (“un-attainment’ is \nrepresented as a slash across the conceptualization and is \ndirectly related to the concept of CANNOT). FRUSTRATION \nand the other emotions can also arise if PS(REAC(ROBOT)) \ncan find a solution but Robot cannot execute it due to other \nsituations that are not anticipated in the PS process. \nThe state of FRUSTRATION always follows the situation \nwhen an Agent WANTs something but it cannot be obtained, as \nshown in Fig. 6. The state of Displeased also accompanies this \nbased on a rule that states that if the object of a WANT is not \nachievable or satisfiable, the IAS involved will be Displeased. \nFEAR comes from the fact that Robot has a model of Person’s \nnegative response to the un-attainment of her WANT, and it is \nreflected in its ANTICIPATION of conceptualization “5”, \nwhich is shown in Fig. 7 as “Person is DISAPPOINTED and \nDISPLEASED that conceptualization “4” is not attainable.” \nFEAR is the anticipation of a negative consequence that may \nhappen to the agent itself [24]. It is not shown here that a \nDISAPPOINTED and DISPLEASED Person toward Robot may \ntake negative actions toward it in some way. \n \nFig. 6. The meaning and representation of FRUSTRATED. \nOther than the consequences above, another response to not \nbeing able to find a solution includes Robot communicating this \nto Person by uttering “I cannot bring Tool(X) from the table to \nyou” as shown in Fig. 5. The symbol “SAY” has a CD+E that is \nthe same processes in Fig. 3 when Person concocts and utters a \nsentence, but we omit the detailed CD+E here. If instead Robot \nis able to find a solution, it would go ahead to carry out the task \nand say “Here is Tool(X)” when handing it to Person. What \nmotives Robot to explain its failure is the communication rule \nthat states: “if others are displeased with your failure to do \nsomething on request or command, do communicate about it, \nincluding explaining the reason involved, because this will \nplacate the other person, which in turn should reduce your own \nfrustration, displeasure, and fear.” This entire rule could be \nstated in CD+ for the system to interpret and execute.  \nD. Continuing Communication \n Following Robot reporting that it is not able to bring \nTool(X) to Person, Person enters the state of being \nDISAPPOINTED and DISPLEASED and the object of the \nDISAPPOINTment and DISPLEASure is the un-attainment of \nconceptualization “4”, as shown in Fig. 7. Instead of just keeping \nquiet, which is a possible response on the part of Person if she is \nno longer concerned about the un-attainment of “4” or she is \ntaking some time to ponder her response, a typical immediate \nresponse on the part of Person is to try and understand the cause \nof the un-attainment of “4.” To this end, Person asks “Why can’t \nyou bring Tool(X) to me?” as shown in Fig. 7. \nAs the UGALRS and CD+ representational framework as \narticulate in [12] is a fully explainable framework, when in the \nproblem solving process, PS(REAC(ROBOT)) fails to return a \nsolution, because the steps of processing everywhere in \nUGALRS using CD+ representations are explicit, the cause(s) \nof the PS failure is easily identified. Hence, the Robot would \nresponse with “Because Tool(X) is not on the table.”  \nWhat causes Robot to respond is the illocutionary force \npresent in Person asking the Why question (i.e., it is in Robot’s \nCONC, where general knowledge is stored – Fig. 2 – that Person \nwould be Pleased if her Why question is answered to, and would \nbe Displeased if this is no so. This knowledge is also represented \nin CD+ form in CONC. These are the representations of the \nillocutionary forces involved.) \n \nFig. 7. Possible communication continued from Fig 5. \nRobot \nmay \nfeel \nfurther \nRELIEVED \nfrom \nbeing \nFRUSTRATED, DISPLEASED, and FEARful after providing \nthis explanation, because providing an explanation may cause \nPerson to be more Pleased. \nV. DISCUSSION AND CONCLUSION \nAs illustrated in the foregoing discussions, many complex \nand intricate processes take place even between a small number \nof relatively simple utterances by an intelligent system, and \nthese involve conceptual, motivational, and affective processes. \nWe used the UGALRS and CD+ framework to elucidate some \nof these processes. For the sake of clarity and to avoid clutter, \nthere are some processes that have been omitted in these \ndiagrams, but some of them have been discussed in the texts, \nespecially some of the communication rules underlying the \ngeneration of sentences. Future research could further develop \nin this direction to elucidate the explicit representations of the \nrules. Suffice it here to note that the CD+ representation is \npowerful enough to represent these rules and the situations under \nwhich they are triggered – i.e., the reasoning processes \nthemselves are also representable using CD+, as has also been \namply illustrated in [12]. \nPsychologists have identified up to 161 types of motivations \nin humans [26]. For robots, the number of motivations could be \nsimpler and smaller in number [21]–[23]. However, for an IAS \nor robot to understand humans and hence be able to interact with \nthem effectively, it has to have a model of the humans’ \nmotivations, as shown in Fig. 2 and discussed in this paper. Due \nto the limitation of space, in this paper we have only dealt with \na small number of motivations. Future work certain calls for \nextension in this direction, and as demonstrated in [12], it is \npossible to do this within a UGALRS plus CD+ framework. \nThere is also a large number of emotions which are useful \nfor characterizing and communicating about internal states in \nrobots and humans than has been discussed here [10], [27], [28]. \nOrtony’s cognitive appraisal theory of emotion [24], which \nlends itself conveniently for computational treatment, has been \ncapitalized here to some extent, but there is a fully set that has \nnot been dealt with in this paper. Therefore, a fuller set of \nemotions and a more complete treatment of affective processes \nwould allow the IAS involved to deal with a more complete \nrange of communicative scenarios. \nDespite the fact that this paper only covers a subset of these \nvast conceptual, motivational, and affective spaces, its main \ncontribution is to articulate a general framework of knowledge \nrepresentation and processing to link these together and \nelucidate the respective functions they serve in the complex \nprocess underlying natural language communication. \nOther \nimportant \nfuture \nwork \nincludes: \n1. \nThe \ntransformations between the surface sentences illustrated in \nmany places in this paper and their corresponding deep level \n“meaning” representations (Section IV(A)). This has also not \nbeen fully developed in Schank’s original CD work [13]–[15]. \nThe transformation must take into consideration grammar, tone, \nemotion, etc. 2. The roles, representations and causal \nconsequences of various attitudes such as SERVILE, \nUNCOOPERATIVE, etc. (Section IV(B)). 3. Extension of the \ncurrent framework and paradigm to cover a wider range of \ncommunication. 4. A computational implementation of the \nrepresentation and processes involved. 5. The learning of the \nvarious representations illustrated in this paper. \nThat CD+ is a computationally viable representational \nscheme for this specific situation and other more general \ndomains, even though a computational implementation is not \nreported in this paper, is reflected in the fact that in the original \nwork of Schank and his associates [13]–[15], it has already been \ndemonstrated that a computational implementation of CD could \nhandle \nnatural \nlanguage \nquestion-answering \nand \ncommunication processes that benefit from the deep meaning \nrepresentations of CD. It follows then that the enhanced form of \nCD, the CD+, as promulgated in [12] and in this paper, is also \ncomputationally implementable. \nThe learning of the representations discussed in this paper is \nalso of paramount importance, as any system that cannot learn \ncannot be scaled up and is not viable as a practical system. Ho \n[12] discusses how learning could be done in the framework of \nCD+. However, it is also important to understand the kind of \nrepresentations that are needed for intelligent processes, in this \ncase, language communication processes, before we understand \nwhat it is that is to be learned. This paper hence contributes to \nthe elucidation of the intricate and complex conceptual, \nmotivational, and affective processes involved in natural \nlanguage communication between social agents, which, when \nappropriately extended, would hopefully bring about a fuller \ncharacterization of language communication in general. \nREFERENCES \n[1] \nA. Clark, C. Fox, and S. Lappin, Eds., The Handbook of \nComputational Linguistics and Natural Language Processing. \nHoboken, New Jersey: Wiley-Blackwell, 2012. \n[2] \nR. Mitkov, Ed., The Oxford Handbook of Computational Linguistics. \nOxford: Oxford University Press, 2005. \n[3] \nT. B. Brown and et. al., “Language Models are Few-Shots Learners,” \n2020. doi: 10.48550/arXiv.2005.14165. \n[4] \nJ. Reeve, Understanding Motivation and Emotion, 5th ed. Hoboken, \nNew Jersey: John Wiley & Sons, Inc., 2009. \n[5] \nE. Cambria, A. Hussain, C. Havasi, and C. Eckl, “Sentic computing: \nexploitation of common sense for the development of emotion-\nsensitive systems,” in Development of Multimodal Interfaces: Active \nListening and Synchrony, Springer, 2010, pp. 148–156. \n[6] \nE. Cambria, Q. Liu, S. Decherchi, F. Xing, and K. Kwok, “SenticNet \n7: A Commonsense-based Neurosymbolic AI Framework for \nExplainable Sentiment Analysis,” in Proceedings of LREC, 2022, pp. \n3829–3839. \n[7] \nR. Mao, Q. Liu, K. He, W. Li, and E. Cambria, “The Biases of Pre-\nTrained Language Models: An Empirical Study on Prompt-Based \nSentiment Analysis and Emotion Detection,” IEEE Transactions on \nAffective Computing, 2022. \n[8] \nK. He, R. Mao, T. Gong, C. Li, and E. Cambria, “Meta-based Self-\ntraining and Re-weighting for Aspect-based Sentiment Analysis,” \nIEEE Transactions on Affective Computing, 2022. \n[9] \nA. Kumar, T. Trueman, and E. Cambria, “Gender-Based Multi-\nAspect Sentiment Detection using Multilabel Learning,” Information \nSciences 606, pp. 453-468, 2022. \n[10] Z. Wang, S.-B. Ho, and E. Cambria, “A review of emotion sensing: \nCategorization models and algorithms,” Multimedia Tools and \nApplications, vol. 79, pp. 35553–35582, 2020. \n[11] Z. Wang, S.-B. Ho, and E. Cambria, “Multi-level fine-scaled \nsentiment analysis with ambivalence handling,” International \nJournal of Uncertainty, Fuzziness and Knowledge-Based Systems, \nvol. 28, no. 4, pp. 683–697, 2020. \n[12] S.-B. Ho, “A general framework for the representation of function \nand affordance: A cognitive, causal, and grounded approach, and a \nstep toward AGI,” 2022. doi: 10.48550/arXiv.2206.05273. \n[13] R. C. Schank, “Identification of conceptualizations underlying \nnatural language,” in Computer Models of Thought and Language, R. \nC. Schank and K. M. Colby, Eds. San Francisco: W. H. Freemann \nand Company, 1973, pp. 187–247. \n[14] R. C. Schank, Conceptual Information Processing. Amsterdam: \nNorth-Holland Publishing Company, 1975. \n[15] R. C. Schank and R. P. Abelson, Scripts, Plans, Goals, and \nUnderstanding: An Inquiry into Human Knowledge Structure. \nMahwah, New Jersey: Lawrence Erlbaum Associates, 1977. \n[16] L. Talmy, Toward a Cognitive Semantics Volume I and II. \nCambridge, Massachusetts: The MIT Press, 2000. \n[17] V. Evans and M. Green, Cognitive Linguistics: An Introduction. \nMahwah, New Jersey: Lawrence Erlbaum Associates, 2006. \n[18] R. W. Langacker, Cognitive Grammar: A Basic Introduction. \nOxford: Oxford University Press, 2008. \n[19] D. Geeraerts, Theories of Lexical Semantics. Oxford: Oxford \nUniversity Press, 2010. \n[20] J. van Eijck and C. Unger, Computational Semantics with Functional \nProgramming. Cambridge: Cambridge University Press, 2010. \n[21] B.-K. Quek, “A Survivability Framework for Autonomous Systems,” \nPh.D. Thesis, National University of Singapore, 2008. \n[22] K. Quek, J. Ibañez-Guzmán, and K.-W. Lim, “Attaining operational \nsurvivability in an autonomous unmanned ground surveillance \nvehicle,” in 32nd Annual Conference on IEEE Industrial Electronics, \n2006, pp. 3969–3974. doi: 10.1109/IECON.2006.348001. \n[23] K. Quek, J. Ibañez-Guzmán, and K.-W. Lim, “A survivability \nframework for the development of autonomous unmanned systems,” \nin 9th International Conference on Control, Automation, Robotics \nand Vision, 2006, pp. 1–6. doi: 10.1109/ICARCV.2006.345336. \n[24] A. Ortony, G. L. Clore, and A. Collins, The Cognitive Structure of \nEmotions. Cambridge: Cambridge University Press, 1990. \n[25] J. R. Searle, Speech Acts: An Essay in the Philosophy of Language. \nCambridge: Cambridge University Press, 1970. \n[26] J. Talevich, S. Read, D. Walsh, R. Iyer, G. Chopra, “Toward a \ncomprehensive taxonomy of human motives,” PLoS ONE, 12, 2017. \n[27] R. Plutchik, Emotions and Life: Perspectives from Psychology, \nBiology, and Evolution. American Psychological Association, 2002. \n[28] Y. Susanto, A. Livingstone, B. Ng, and E. Cambria, “The Hourglass \nModel Revisited,” IEEE Intelligent Systems 35 (5), pp. 96-102, 2020. \n[29] N. Howard and E. Cambria, “Intention Awareness: Improving upon \nSituation Awareness in Human-Centric Environments,” Human-\ncentric Computing and Information Sciences 3(9) 2013. \n \n",
  "categories": [
    "cs.AI"
  ],
  "published": "2022-09-26",
  "updated": "2022-10-20"
}