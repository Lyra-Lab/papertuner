{
  "id": "http://arxiv.org/abs/2210.10595v1",
  "title": "DIAMBRA Arena: a New Reinforcement Learning Platform for Research and Experimentation",
  "authors": [
    "Alessandro Palmas"
  ],
  "abstract": "The recent advances in reinforcement learning have led to effective methods\nable to obtain above human-level performances in very complex environments.\nHowever, once solved, these environments become less valuable, and new\nchallenges with different or more complex scenarios are needed to support\nresearch advances. This work presents DIAMBRA Arena, a new platform for\nreinforcement learning research and experimentation, featuring a collection of\nhigh-quality environments exposing a Python API fully compliant with OpenAI Gym\nstandard. They are episodic tasks with discrete actions and observations\ncomposed by raw pixels plus additional numerical values, all supporting both\nsingle player and two players mode, allowing to work on standard reinforcement\nlearning, competitive multi-agent, human-agent competition, self-play,\nhuman-in-the-loop training and imitation learning. Software capabilities are\ndemonstrated by successfully training multiple deep reinforcement learning\nagents with proximal policy optimization obtaining human-like behavior. Results\nconfirm the utility of DIAMBRA Arena as a reinforcement learning research tool,\nproviding environments designed to study some of the most challenging topics in\nthe field.",
  "text": "DIAMBRA Arena: a New Reinforcement Learning Platform for Research and\nExperimentation\nAlessandro Palmas - DIAMBRA - alex@diambra.ai\nAbstract\nThe recent advances in reinforcement learning have led to eﬀective methods able to obtain above human-level performances in\nvery complex environments. However, once solved, these environments become less valuable, and new challenges with diﬀerent\nor more complex scenarios are needed to support research advances. This work presents DIAMBRA Arena, a new platform for\nreinforcement learning research and experimentation, featuring a collection of high-quality environments exposing a Python API\nfully compliant with OpenAI Gym standard. They are episodic tasks with discrete actions and observations composed by raw\npixels plus additional numerical values, all supporting both single player and two players mode, allowing to work on standard\nreinforcement learning, competitive multi-agent, human-agent competition, self-play, human-in-the-loop training and imitation\nlearning. Software capabilities are demonstrated by successfully training multiple deep reinforcement learning agents with proximal\npolicy optimization obtaining human-like behavior. Results conﬁrm the utility of DIAMBRA Arena as a reinforcement learning\nresearch tool, providing environments designed to study some of the most challenging topics in the ﬁeld.\nKeywords: reinforcement learning, transfer learning, multi-agent, games\n1. Introduction\nIn the past few years, advances combining deep learing (DL)\nwith reinforcement learning (RL) reached a few outstanding\nmilestones (Silver et al., 2016, 2017; Berner et al., 2019), show-\ning that general algorithms can obtain top level performance\neven when facing complex tasks, while treating them as closed-\nboxes and without any problem-speciﬁc knowledge.\nIn order to compare diﬀerent approaches, and to measure\ntheir rate of improvement, the research community needs a\nvalid set of benchmarks. In RL the role of such benchmarks\nis played by software packages providing problems, also called\nenvironments, that are challenging for RL to solve. In recent\nyears, appearance of such tools relevantly contributed to the\nrapid development of this domain, making available problems\nof diﬀerent complexity and high scalability.\nSince simulation environments are the fundamental building\nblock on which the RL community relays for testing its algo-\nrithms, their quality is of critical importance. In spite of that,\nthe literature focused on this highly relevant aspect is not as\ncomprehensive and developed as the one concerning algorithms\nand methods (Juliani et al., 2018).\nA large portion of current RL research platforms is based\non popular video games or game engines such as Atari 2600,\nQuake III, Doom, and Minecraft. It has been a long time since\nartiﬁcial intelligence (AI) research started leveraging games to\nﬁnd meaningful, well posed and challenging problems, starting\nfrom chess and checkers (Shannon, 1950; Samuel, 1959) and\nthe application of RL to the game of Backgammon (Tesauro,\n1995). This trend continues today, as the AI research com-\nmunity shares the idea that creating algorithms able to reach\nhuman-level performances on these games, eﬃciently solving\nthe same complex challenges, notably contributes to the pursue\nof machine intelligence (Laird and VanLent, 2001).\nAs deep reinforcement learning (DeepRL) research ﬁnds new\nalgorithms and above human-level performance is obtained, the\nbenchmarks based on existing environments become less valu-\nable and their impact as research drivers becomes marginal.\nThere is a deep and entangled relation between algorithmic\nprogress and new environments release, where the latter is key\nto favor the former. If on one side the research community\nis expected to advance the RL state-of-the-art developing bet-\nter performing algorithms, who is in charge of delivering new\nhigh-quality environments is way less obvious. The task is in\nfact as challenging as algorithms development, requiring a rel-\nevant amount of time for design and implementation, as well as\nspecialized domain knowledge. Proper care must be devoted to\nthe creation of environments that pose meaningful challenges\nto learning systems, preferring properties that intercept main\nresearch interests to make them valuable benchmarks.\nFigure 1: Snapshots of some DIAMBRA Arena environments\narXiv:2210.10595v1  [cs.LG]  19 Oct 2022\nWithin this context, this paper presents DIAMBRA Arena, a\nbrand new software package featuring a collection of carefully\nselected, high-quality environments for RL research and exper-\nimentation. It has been developed with the goal of opening a\nhigh-quality curated stream of ever new environments, focus-\ning on key aspects of RL research: competitive / cooperative\nmulti-agents, agents transfer learning and generalization, hier-\narchical learning, human-agent cooperation / competition and\nhuman-in-the-loop training.\nIt also aims at allowing everyone willing to approach RL to\nhave access to state-of-the-art environments and to be able to\nuse them. This means to design and develop something that\nis not extremely demanding in terms of hardware requirements,\nthat runs on the majority of operating systems (OSs), well docu-\nmented and provided with a comprehensive collection of work-\ning examples.\nFinally, it moves in the direction of applying on RL re-\nsearch, study and experimentation, the current gamiﬁcation\ntrend, which is being successfully adopted in diﬀerent ﬁelds\nof science lately (Feger et al., 2018, 2019; Kalogiannakis et al.,\n2021; Gari et al., 2018; Hursen and Bas, 2019). Favoring ex-\nciting and rewarding experiences, this will be a key aspect in\nmotivating and getting more people involved in the ﬁeld.\nThe remainder of the paper is structured as follows: Section\n2 discusses the state of the art of RL environments and pro-\nvides an overview of relevant related work. Then, Section 3,\npresents DIAMBRA Arena software package and its most im-\nportant technical details. In Section 4 it is shown how DeepRL\nagents have been successfully trained in the games provided,\npresenting their architecture, the adopted training strategy and\nachieved performances and results. In Section 5, one ﬁnds a\ndiscussion focusing on research directions that are enabled by\nDIAMBRA Arena, current limitations and the features roadmap\nplanned for the near future. Finally, Section 6 is where conclu-\nsions about this work are presented.\n2. Background and Related Work\nRL problems, formulated in the standard form, assume there\nis an agent that is able to interact with an environment in itera-\ntive steps. Each iteration, the agent selects an action to execute,\nand it receives an observation and a reward from the environ-\nment. A RL algorithm aims at maximizing some measure of the\nagent total reward, as the agent interacts with the environment.\nIn the RL literature, this formulation is typically formalized as\na partially observable Markov decision process (POMDP).\nIn the episodic declination of RL, the agent experience is\nsplit into a series of episodes. Each episode starts from an ini-\ntial state and the interaction proceeds until a terminal state is\nreached. The goal in episodic RL is maximizing the expec-\ntation of total cumulative reward per episode, and to achieve\na high level of performance in the lowest number of episodes\npossible.\nAround year 2012, the ﬁrst benchmarks have started to arise\nin standard form, with the aim of creating a common test-bed\nto measure algorithms performances and to stimulate RL re-\nsearch. Since then, a few very notable environments and soft-\nware packages have been developed and used to demonstrate\ntremendous improvements and outstanding breakthroughs in\nthe RL domain, proving how important their role is in driving\nadvancements in this ﬁeld. Here is a list of the most relevant\nones currently used in RL research, all publicly available:\n• Arcade Learning Environment (ALE) (Bellemare et al.,\n2012): provides an interface to hundreds of Atari 2600\ngame environments with distinctive features. It has been\nand is still widely adopted as rigorous test-bed for eval-\nuating and comparing approaches and algorithms in RL,\nrepresenting the ﬁrst important eﬀort made in this stan-\ndardization direction.\n• OpenAI Gym and OpenAI Retro (Brockman et al., 2016;\nNichol et al., 2018): OpenAI Gym includes a collection of\nbenchmark environments that expose a common interface,\nproviding diﬀerent categories of problems like classic con-\ntrol and toy text, algorithmic, Atari and 2D and 3D robots.\nOpenAI Retro creates RL environments from various em-\nulated video games.\n• DeepMind Lab (Beattie et al., 2016): ﬁrst-person 3D game\nplatform, it can be used to study autonomous artiﬁcial\nagents learning complex tasks in large, partially observed,\nand visually diverse worlds. Powered by a fast and widely\nrecognized game engine, features a simple and ﬂexible\nAPI that enables creative task-designs.\n• Starcraft II Learning Environment (Vinyals et al., 2017):\nbased on the game StarCraft II, it represented a new grand\nchallenge for RL, providing a more diﬃcult class of prob-\nlems than those considered in most previous research. Its\nmain features are: multi-agent setting with multiple play-\ners interacting, imperfect information and partial observ-\nability, large action space, large state space, and delayed\ncredit assignment requiring long-term strategies over thou-\nsands of steps.\n• Unity ML-Agents and Obstacle Tower (Juliani et al., 2018,\n2019): Unity ML-Agents is open source project enabling\nto create simulated environments using the Unity Editor,\nand interact with them via a Python API. It includes a set\nof example environments together with state of the art RL,\nimitation learning and self-play algorithms in both sym-\nmetric and asymmetric games. Obstacle Tower is a high\nﬁdelity, 3D, 3rd person, procedurally generated environ-\nment. It provides both low-level control and high-level\nplanning problems in tandem, learning from pixels and a\nsparse reward signal, as well as performance evaluation\nbased on unseen instances of the environment.\n• VizDoom (Kempka et al., 2016): based on the classical\nﬁrst-person shooter video game, Doom, uses raw visual\ninformation employing the ﬁrst-person perspective in a\nsemi-realistic 3D world. It is lightweight, fast, and highly\ncustomizable via a convenient mechanism of user scenar-\nios.\n2\n• MuJoCo (Todorov et al., 2012): physics engine focused\non robotics, biomechanics, graphics and animation, and\nother areas where fast and accurate simulations are needed.\nIts key features are speed, accuracy and modeling power,\nspeciﬁcally designed for model-based optimization, and in\nparticular optimization through contacts. It has been one\nof the main references in the RL domain for robotics ap-\nplications.\nThese environments are the most important actors in the ﬁeld\nof RL platforms for research and experimentation. Section 3\nintroduces DIAMBRA Arena, which aims at becoming a new\nplayer in this domain, providing new problems and challenges\nto help RL community advancing state-of-the-art. How it com-\npares with those presented in this section in terms of features,\nspeed and memory footprint is discussed in Section 3.4.\n3. DIAMBRA Arena\nThis section presents DIAMBRA Arena software package\nand its most important technical details. It starts providing a\nhigh level overview, then it describes its technical details in\nterms of its basic usage, available settings, observation and\naction spaces, the reward wrappers and environment wrap-\npers. Next subsection describes its advanced features, such as\nhuman-in-the-loop training and multi-agent and self-play, and\nthe ﬁnal subsection compares DIAMBRA Arena with other RL\nenvironments in terms of features and performances.\n3.1. Overview\nDIAMBRA Arena is a software package featuring a collec-\ntion of high-quality environments for RL research and exper-\nimentation. It provides a standard interface to popular arcade\nemulated video games, oﬀering a Python API fully compliant\nwith OpenAI Gym format, that makes its adoption smooth and\nstraightforward.\nIt is accompanied by a comprehensive documentation (DI-\nAMBRA, 2022a) and its repository (DIAMBRA, 2022b) comes\nwith a collection of examples covering main use cases of inter-\nest that can be run in just a few steps. It supports all major OSs\n(Linux, Windows and MacOS) and can be easily installed via\nPython PIP, as described in the installation section of the doc-\numentation. It is completely free to use, the user only needs to\nregister on the oﬃcial website.\nThe ﬁrst version of the software focuses on ﬁghting games,\ncreating a robust and consistent package.\nAll of them are\nepisodic RL environments, with discrete actions that represent\ngamepad buttons. The observations to be used for control are\ncomposed by a screen pixels buﬀer plus additional data made\nof RAM values representing game elements such as characters\nhealth values or characters stage side. The problem can thus\nbe framed as ”control from pixels” only, or as a more general\none, taking advantage of RAM numerical values too. It is worth\nnoticing that RAM values usage is guaranteed to be fair, mean-\ning that they only provide redundant information that can be\nretrieved from the game screen alone, and no ”hidden” state is\ncontained in it.\nEvery environment supports both single player (1P) as well\nas two players (2P) mode. The former is the ”classic” arcade\nrace for clearing the game achieving the score record that can\nbe used for standard RL. The latter adds three new, orthogo-\nnal dimensions to these environments, making them suitable\nfor research and experimentation in the domains of competitive\nmulti-agent as well as for human-agent cooperation / competi-\ntion, and allowing to make use of self-play for training.\nIn addition, it can be easily set up to explore human-in-the-\nloop training, covering applications like human assisted re-\nwards and the natively supported imitation learning, for which\nit provides tools to record human expert demonstrations and a\nspeciﬁc class to seamlessly load and use them for agent train-\ning.\nAll interfaced games have been selected among the most\npopular and successful ﬁghting retro-games. They have been\nchosen so that, while sharing the same fundamental mechan-\nics, they provide slightly diﬀerent challenges, with speciﬁc fea-\ntures such as diﬀerent number of characters to be used at the\nsame time, how to handle combo moves, possibility to recharge\nhealth bars or not, and similar.\nWhenever possible, games are released with all hidden/bonus\ncharacters unlocked. For every released title, extensive test-\ning has been carried out, being the minimum requirement for a\ngame to be released in beta. After that, the next internal step is\ntraining a DeepRL agent to play, and possibly complete it, mak-\ning sure the 1P mode is playable with no bugs up until game\nend. This is the condition under which titles are moved from\nbeta to stable status.\n3.2. Technical Details\n3.2.1. Basic Usage and Settings\nDIAMBRA Arena usage follows the standard RL interac-\ntion framework: the agent sends an action to the environment,\nwhich processes it and performs a transition accordingly, from\nthe starting state to the new state, returning the observation and\nthe reward to the agent to close the interaction loop. Figure 2\nshows this typical interaction scheme and data ﬂow.\nFigure 2: Environment interaction framed under the standard RL POMDP\nparadigm\n3\nThe shortest snippet for a complete basic execution of the\nenvironment consists of just 11 lines of code, and is presented\nin Listing 1 below.\nimport\ndiambra . arena\nenv = diambra . arena . make ( ” doapp ” )\nobs = env . r e s e t ( )\nwhile\nTrue :\nenv . r en de r ( )\na c t i o n s = env . a c t i o n s p a c e . sample ( )\nobs ,\nrew ,\ndone ,\ni n f o = env . s t e p ( a c t i o n s )\ni f\ndone :\nobs = env . r e s e t ( )\nbreak\nenv . c l o s e ( )\nListing 1: DIAMBRA Arena basic usage\nA trained agent is expected to replace random action sam-\npling with the result of its policy prediction based on the obser-\nvation.\nAll environments share the list of options presented in Table\n1, allowing to handle many diﬀerent aspects of them, controlled\nby key-value pairs in a Python dictionary. The high level pre-\nsentation reported here is detailed in depth in the documenta-\ntion.\n3.2.2. Observation Space\nEnvironment observations are composed by two main ele-\nments: a visual one (the game frame) and an aggregation of\nquantitative values called RAM states (stage number, health\nvalues, etc.). In the default mode, both of them are exposed\nthrough an observation space of type gym.spaces.Dict1.\nFigure 3 (on the left) shows an example of Dead Or Alive++\nobservation where some of the RAM states are highlighted, su-\nperimposed on the game frame.\nAn alternative conﬁguration is also available, triggered by\nthe hardcore setting: when set to True the observation space\nis composed only by the game frame, discarding all additional\nnumerical values, thus it is of type gym.spaces.Box. Addi-\ntional details can be found in the documentation.\n3.2.3. Action Space(s)\nActions of the interfaced games can be grouped in two\ncategories:\nmove actions (up, left, etc.)\nand attack ones\n(punch, kick, etc.).\nDIAMBRA Arena provides four dif-\nferent action spaces:\nthe main distinction is between dis-\ncrete and multi-discrete ones.\nThe former is a single list\ncomposed by the union of move and attack actions (of type\ngym.spaces.Discrete), while the latter consists of two sets\ncombined, for move and attack actions respectively (of type\ngym.spaces.MultiDiscrete).\n1For details on Gym spaces, see https://github.com/openai/gym/\ntree/master/gym/spaces/\nFigure 3: Representation of Dead Or Alive++ observation space (left) and ac-\ntion spaces (right). Observations are made by the game frame plus RAM states\n(shown in the call-outs). The four action spaces are represented as sets of cor-\nrespondent gamepad buttons.\nFor each of the two options, there is an additional diﬀeren-\ntiation available: if to use attack buttons combinations or not.\nThis option is mainly available to reduce the action space size\nas much as possible, since combinations of attack buttons can\nbe seen as additional attack buttons.\nThe complete visual description of available action spaces\nis shown in Figure 3 (on the right), where all four choices are\npresented via the correspondent gamepad buttons conﬁguration.\nWhen run in 2P mode,\nthe environment is provided\nwith an action space of type gym.spaces.Dict pop-\nulated\nwith\ntwo\nitems,\nidentiﬁed\nby\nkeys\n\"P1\"\nand\n\"P2\", whose values are either gym.spaces.Discrete or\ngym.spaces.MultiDiscrete as described above. Additional\ndetails on the action spaces are reported in the documentation.\n3.2.4. Reward Function\nThe reward is deﬁned as a function of characters’ health val-\nues so that, qualitatively, damage suﬀered by the agent corre-\nsponds to a negative reward, and damage inﬂicted to the op-\nponent corresponds to a positive reward. The quantitative, gen-\neral, and formal reward function deﬁnition deﬁned by Equation\n1:\nRt =\n0,Nc\nX\ni\n\u0010 ¯Ht−\ni −¯Ht\ni −\n\u0010 ˆHt−\ni −ˆHt\ni\n\u0011\u0011\n(1)\nwhere:\n•\n¯H and ˆH are health values for opponent’s character(s) and\nagent’s one(s) respectively;\n• t−and t are used to indicate conditions at ”state-time” and\nat ”new state-time” (that is before and after environment\nstep);\n• Nc is the number of characters taking part in a round. Usu-\nally is Nc = 1 but there are some games where multiple\ncharacters are used, with the additional possible option of\nalternating them during gameplay, like Tekken Tag Tour-\nnament where 2 characters have to be selected and two\nopponents are faced every round (thus Nc = 2);\n4\nTable 1: Environment settings\nSetting\nDescription\nPlayer\nAllows to select single player (1P) or two players (2P) mode. In 1P mode,\nthe same parameter is used to select on which side to play, P1 (left) or P2 (right)\nStep ratio\nDefines how many steps the game (emulator) performs for every environment step.\nThis value has a direct impact on how frequently actions are sent to the game\nFrame shape\nResizes game frame to prescribed height and width, keeping it RGB or making it grayscale\nContinue game\n(1P mode only)\nDefines if and how to allow ”Continue” when the agent is about to face the game over condition\nDifficulty\n(1P mode only)\nSelects game difficulty\nCharacter(s)\nSelects the character(s) the agent will use\nCharacters outfit\nDefines the number of outfits to draw from at character selection\nAction space\nSpecifies the action spaces to be used\nAttack buttons\ncombination\nSpecifies if to include attack buttons combinations or not as available actions\nHardcore\nLimits the observation to the screen pixels discarding additional RAM states\nThe lower and upper bounds for the episode total cumulative\nreward are deﬁned in Equations 2. They consider the default\nreward function (Equation 1) for game execution with continue\ngame option set equal to 0.0 (no continue allowed).\nmin\n0,Ts\nX\nt\nRt = −Nc ((Ns −1) (Nr −1) + Nr) ∆H\nmax\n0,Ts\nX\nt\nRt = NcNsNr∆H\n(2)\nwhere:\n• Nr is the number of rounds to win (or lose) in order to win\n(or lose) a stage;\n• Ts is the terminal state, reached when either Nr rounds are\nlost (for both 1P and 2P mode) or game is cleared (for 1P\nmode only);\n• t represents the environment step and for an episode goes\nfrom 0 to Ts;\n• Ns is the maximum number of stages the agent can play\nbefore the game reaches Ts.\n• ∆H = Hmax −Hmin is the diﬀerence between the maximum\nand minimum health values for the given game; usually,\nbut not always, Hmin = 0.\nFor 1P mode Ns is game-dependent, while for 2P mode Ns =\n1, meaning the episode always ends after a single stage (so after\nNr rounds have been won / lost be the same player, either P1 or\nP2).\nFor 2P mode, P1 reward is deﬁned as R in Equation 1 and P2\nreward is equal to −R (zero-sum games). Equation 1 describes\nthe default reward function. It is of course possible to tweak it\nat will by means of custom reward wrappers (see Section 3.2.5\nbelow). Additional details can be found in the documentation.\n3.2.5. Wrappers\nDIAMBRA Arena comes with a large number of ready-to-\nuse wrappers and examples showing how to apply them. They\ncover a wide spectrum of use cases, and also provide reference\ntemplates to develop custom ones. In order to activate wrappers\none has just to add an additional dictionary to the environment\ncreation method, having properly populated it. A summary of\navailable wrappers is presented in Table 2, additional details\ncan be found in the oﬃcial documentation.\n3.3. Advanced Features\n3.3.1. Human-in-the-Loop Training\nGuiding the learning process by leveraging human input\nis desirable as taking advantage of humans domain expertise\nboosts learning, and helps the agent learning to behave as hu-\nmans would expect. Diﬀerent approaches have been studied\nby researchers (Zhang et al., 2019) such as humans provid-\ning evaluative feedback to the agent (Knox and Stone, 2008),\nhumans manipulating the agent’s observed states and actions\n(Abel et al., 2016), or learning to imitate expert trajectories (Ho\nand Ermon, 2016).\nThe success of the former two families of strategies strongly\ndepends on how the human interfaces with the agent during\nlearning, that is typically very diﬃcult, if not impossible, with\nthe majority of RL platforms and environments, since it re-\nquires low-level access to the environment mechanics that is\nrarely available.\nDIAMBRA Arena can be easily set up to support an interac-\ntive and collaborative learning process between the human and\n5\nTable 2: Ready-to-use wrappers\nWrapper\nDescription\nFrame warping\nResizes game frame to prescribed height and width, keeping it RGB or making it grayscale\nObservation scaling\nScales elements of the observation space, with a specific operation per each observation type\nFrame stack with\noptional dilation\nStacks latest N frames together piling them along the third dimension. Using the dilation factor, it allows\nto stack only one every M frames, covering a broader temporal span for the same amount of memory\nActions stacking\nStacks latest N actions together\nObservation dictionary\nflattening and filtering\nFlattens the observation dictionary nested levels and filters them keeping only the prescribed subset\nReward normalization\nScales the reward dividing it by the product of the scaling factor K and ∆H, difference between\nmaximum and minimum health, as previously described\nReward clipping\nClips the reward applying the sign () function\nNo-op reset\nPerforms a maximum of N no-op actions at the beginning of the episode (that is after reset)\nActions sticking\nRepeats the action sent to the environment for N steps. It is activated when N > 1 and can be applied\nonly when Step Ratio setting is equal to 1\nthe agent. The great ﬂexibility given by environment wrappers,\nallows to easily include human contribution in the training pro-\ncess. It is possible for a human to have real-time access and\ninteract with the agent during training in order to, for exam-\nple, pause the scene, and control of the agent via keyboard or\ngamepad commands.\nImitating expert trajectories is a signiﬁcantly more common\napproach in research. DIAMBRA Arena provides advanced\ntools to leverage the imitation learning technique (Hussein\net al., 2017). Very useful to speed up / bootstrap learning, usu-\nally in the very early training stages, it requires to store human\ngameplay in order to use it to train the agent policy, typically\nadopting either the supervised learning approach (behavioural\ncloning) or using the human to guide exploration in the ”clas-\nsic” RL setting. Recordings must therefore store both observa-\ntions and actions sent by the human player at least in the former\ncase, and rewards too in the latter.\nThe package comes with a wrapper dedicated to this purpose\nand one speciﬁc example showing how to set it up. In order to\nactivate it, one has just to add an additional dictionary to the\nenvironment creation method.\nHuman players are requested to play using a USB gamepad,\nthat is interfaced via a custom class named DiambraGamepad.\nTwo main settings needs to be provided:\nfile path and\nuser name. The former is the local absolute path where to save\nrecorded trajectories, provided as a string, while the latter is a\nstring variable that can be used to diﬀerentiate between users in\ncase recordings comes from multiple players.\nBehavioral cloning approach. Having a dataset (the collected\nplayer experience) with features (environment observations, for\nexample the game frame) and targets (action(s) selected by the\nhuman player), one can train the agent policy network as typi-\ncally done in supervised learning classiﬁcation problems. The\nresult would be an agent whose behavior replicates the one\nof the player, that’s where the term behavioral cloning comes\nfrom.\nGuided exploration approach.\nSince rewards are stored\nalongside observations and actions, recorded trajectories can\nbe used while remaining inside the RL paradigm: the human\nplayer has here the role of ”guidance” in the exploration phase,\nproviding the algorithm with a ”meaningful” experience of\nthe environment (from a human perspective) in the form of\ntrajectories. This is expected to signiﬁcantly speedup training,\noptimizing exploration towards zones of greater relevance in\nthe observation/action domain space.\nimport\ndiambra . arena\ns e t t i n g s = { ” t r a j\nf i l e s\nl i s t ” :\n[ ” path / to / r e c F i l e 1 ” ,\n” path / to / r e c F i l e 2 ” ,\n. . . ] ,\n” t o t a l c p u s ” :\n2}\nenv = diambra . arena . I m i t a t i o n L e a r n i n g (** s e t t i n g s )\nobs = env . r e s e t ( )\nwhile\nTrue :\nobs ,\nrew ,\ndone ,\ni n f o = env . s t e p ( 0 )\ni f\ndone :\nobs = env . r e s e t ( )\nbreak\nenv . c l o s e ( )\nListing 2: DIAMBRA Arena imitation learning example\nIn order to do so, the software package provides a dedicated\nclass named ImitationLearning with the speciﬁc purpose of\nloading trajectories recorded with the wrapper provided, and\nstepping through them. Listing 2 shows a code snippet with all\nbasic instructions required to run it.\nIt should be noted how in this case the action selection step is\nnot needed anymore since the ”history” of environment transi-\n6\ntions is already written in stored human player experience ﬁles.\n3.3.2. Multi-Agent and Self-play\nAs already mentioned, all environments can be run in both\nsingle player and two players mode, the latter making DI-\nAMBRA Arena, de-facto, a software package that can be used\nfor research in the ﬁelds of competitive multi-agent and human-\nagent competition.\nThis feature also allows to implement self-play (Baker et al.,\n2020; Bansal et al., 2018), that consists, roughly speaking, in\ntraining an agent by making it play (or ﬁght) against itself. In\nfact, supporting the 2P mode, the library allows the same agent\n(or two diﬀerent agents) to play on both sides, one against the\nother.\nThis advanced technique oﬀers many advantages: the algo-\nrithm always faces an opponent of similar skill level, enabling\nmore eﬃcient learning; it can be adopted also when no single\nplayer mode is available for a given game; it gives the agent\nthe freedom to explore and ﬁnd optimal strategies beyond those\nknown by expert human players.\n3.4. Environments Comparison\nThis section aims at comparing DIAMBRA Arena with the\nmost important tools currently available in the literature that\nprovide similar capabilities. Two main aspects are considered:\nfeatures exposed by the software and its performances in terms\nof speed and memory footprint. The former is fundamental\nfor measuring how broadly it can be applied, determining the\nrange of research topics it enables to study. The latter is strictly\nlinked with one of the most important limitations of current RL,\nsample complexity: the amount of data required by RL algo-\nrithms to achieve optimal performances is still very large, thus\nrequiring very eﬃcient environments that allow to generate the\nneeded experience quickly and to perform a large number of\nexperiments in reasonable time.\n3.4.1. Features\nTable 3 compares diﬀerent environments in terms of the most\nimportant characteristics for problems in this domain: type of\nobservation spaces, type of action spaces, if they provide ready-\nto-use wrappers, and if they provide multi-agent and advanced\nfeatures.\nRegarding the observation spaces, DIAMBRA Arena covers\nall options provided by other tools but the full RAM state. This\nhas been an explicit design choice, aiming to push the research\ntowards studying algorithms able to learn in a more human-like\ncondition, so avoiding the option of directly read the complete\nRAM states. Nonetheless, it can be easily added, if relevant, in\nfuture work.\nIn its current form, DIAMBRA Arena does not provide a\ncontinuous action space. This is mainly related to the nature\nof interfaced games, that are meant to be played with digital\ncontrollers (gamepads). Also in this case, extension to games\nfeaturing such type of input (for example mouse cursor posi-\ntion) will be subject of future work.\nDIAMBRA Arena is one of the few environments providing\nready-to-use wrappers. This is a key element in speeding up\nthe implementation of an interface with third party RL libraries,\nsuch as Stable Baselines or Ray RLlib.\nLastly, a very relevant aspect is that DIAMBRA Arena al-\nlows to study many advanced topics, such as competitive agent-\nagent, human expert demonstration recording, imitation learn-\ning, transfer learning and cross-task generalization. In addition,\nit also natively covers human assisted reward and competitive\nhuman-agent settings, unique features not provided by the oth-\ners, to the best knowledge of the author.\n3.4.2. Performances\nTable 4 provides a comparison for the diﬀerent environments\nin terms of execution speed, expressed in frames per second\n(FPS), and memory footprint for a single environment instance,\nmeasured in megabytes (MB). All environments have been run\nin the same machine, a standard low-medium level desktop also\nused to train DeepRL agents that is described in a later section.\nOnly most relevant environments are reported in the table,\nin particular those similar in terms of observation spaces, ac-\ntion spaces and advanced features. By looking at the values\nreported, one notes that both performance measures for DI-\nAMBRA Arena are similar and around the same order of mag-\nnitude of the other RL environments. If compared with the most\nsimple ones (ALE, VizDoom, Retro), the speed is between 2 to\n6 times slower, while the memory footprint is between 1.5 to 4\ntimes larger. This slightly lower eﬃciency is a fair price to pay\nto have the vast set of features DIAMBRA Arena provides that\nare not available in the other environments, as shown in the pre-\nvious subsection. In fact, when compared with an environment\nof similar complexity (Unity Obstacle Tower), it becomes clear\nhow faster DIAMBRA Arena is (about 10×), and how reason-\nable the memory footprint turns out to be.\n4. Trained DeepRL Agent\nIn order to validate and conﬁrm that the environments pro-\nvided can be learned, many tests have been performed, training\nmultiple DeepRL agents on diﬀerent games, with diﬀerent se-\ntups in terms of game settings, wrappers used, observation and\nactions spaces.\nThis section describes in detail how these agents have been\ntrained to play diﬀerent games in single player mode, maximiz-\ning the total cumulative reward, where the immediate reward is\ndeﬁned by Equation 1.\n4.1. Problem Framing and Algorithm\nTo provide useful context for the discussion, in what follows\nthe agent trained on Dead Or Alive++ will be considered. A\nreward normalization wrapper has been applied to it, using a\nscaling factor equal to K = 0.5, the game has a ∆H = (Hmax −\nHmin) = 208, resulting in a normalization term equal to K∆H =\nK(Hmax−Hmin) = 0.5∗208 = 104. It has 8 stages, the last always\nagainst the same ﬁnal boss and the second last always against\nthe same character that depends on the one used by the agent.\nTwo rounds are needed to win a stage, and a single character is\nused. Therefore, with respect to quantities in Equations 2 and\n7\nTable 3: Environments features comparison\nEnvironment\nObservation\nspaces\nAction\nspaces\nReady-to-use\nwrappers\nMulti-agent\nready\nAdvanced\nFeatures ready\nDIAMBRA\nArena\nRaw pixels (2D/3D),\nRaw pixels (2D/3D) +\nVector data\nDiscrete,\nMulti-discrete\n√\nCompetitive agent-agent,\nCompetitive human-agent\nHuman expert\ndemonstration recording,\nImitation learning,\nHuman assisted reward,\nTransfer learning,\nCross-task generalization\nUnity\nML-Agents\nVector data\nDiscrete,\nMulti-discrete,\nContinuous\nX\nCompetitive agent-agent\nImitation learning,\nTransfer learning,\nCross-task generalization\nStarCraft LE\nRaw Pixels (2D),\nFeatures layers,\nVector data\nCompound\n√\nCompetitive agent-agent\nHuman expert\ndemonstration recording\nVizDoom\nRaw pixels (3D)\nDiscrete\nX\nCompetitive agent-agent\nHuman expert\ndemonstration recording\nUnity\nObstacle Tower\nRaw pixels (2D/3D)\nDiscrete,\nMulti-discrete\nX\n-\nTransfer learning,\nCross-task generalization\nDMLab\nRaw pixels (3D),\nVector data\nDiscrete,\nContinuous\nX\n-\nTransfer learning,\nCross-task generalization\nALE\nRaw pixels (2D),\nRAM state\nDiscrete\nX\n-\n-\nGym\nRaw pixels (2D),\nVector data\nRAM State\nDiscrete,\nMulti-discrete,\nContinuous\n√\n-\n-\nRetro\nRaw pixels (2D),\nRAM state\nDiscrete,\nMulti-discrete\nX\n-\n-\nMuJoCo\nVector data\nContinuous\nX\n-\n-\nTable 4: Environments performances comparison\nEnvironment\nSpeed [FPS]\nMemory [MB]\nDIAMBRA\nArena\n0.5k\n140\nUnity\nObstacle Tower\n0.06k\n150\nRetro\n1.1k\n86\nDMLab\n1.2k\n40\nVizDoom\n1.5k\n35\nALE\n3k\n70\nthe normalization term deﬁned above, one has Nc = 1, Ns = 8,\nNr = 2, resulting in episode total cumulative reward bounds\nequal to min P0,Ts\nt\nRt = −18, max P0,Ts\nt\nRt = 32.\nThe selected game has four diﬀerent diﬃculty levels, they do\nnot aﬀect other game settings. The native game frame resolu-\ntion is 480 × 512 × 3 and there are four diﬀerent outﬁts for each\ncharacter (see Figure 4).\nFigure 4: Kasumi available outﬁts in Dead Or Alive++\nEnvironment settings have been selected so that the agent\nlearns to play with a speciﬁc character (Kasumi), as both P1\nand P2 (it will randomly start the episodes in one of the two\npositions with equal probability), while using both the ﬁrst two\noutﬁts for the selected character, sending actions once every 6\ngame steps (actions frequency 10 Hz, at ﬁxed points in ”time”).\n8\nSettings are summarized in Table 5.\nTable 5: Environment settings used during training\nSetting\nValue\nPlayer\n”Random”\nStep Ratio\n6\nFrame Shape\n[128,128,1]\nContinue Game\n0.0\nDifficultya\n3→4 of 4\nCharacter\n”Kasumi”\nCharacter Outfit\n2\nAction Space\n”Discrete”\nAttack Buttons Combination\nFalse\nHardcore\nFalse\naDiﬃculty level has been set equal to 3 at the beginning of training, and has\nbeen raised to 4 when the agent was able to complete the game in the majority of\nevaluation episodes using the initial diﬃculty value.\nThe selected observation space is made of the latest game\nframe plus the RAM states. For training this agent only some\nelements of the latter have been selected, speciﬁcally: own/op-\nponent health, own/opponent side, stage number and actions.\nThe discrete action space was selected, with no attack buttons\ncombination, for a total of 12 actions.\nIn addition to the previously mentioned reward normaliza-\ntion wrapper, a number of additional ones have been applied:\nlatest 4 frames are stacked together with no dilation; latest 12\nactions are stacked together; observation scaling is applied; and\nno reward clipping, no-op reset nor actions sticking are applied.\nAll settings are reported in Table 6.\nTable 6: Wrappers settings used during training\nWrapper Settings\nValue\nObservation Scaling\nTrue\nFrame Stacking with Dilation\n[4, 1]\nAction Stacking\n12\nReward Normalization\nTrue\n(Scaling factor K = 0.5)\nReward Clipping\nFalse\nNo-Op Reset\n0\nActions Sticking\n1\nThe proximal policy optimization (PPO) algorithm (Schul-\nman et al., 2017) has been used, leveraging the open source RL\nlibrary Stable Baselines (Hill et al., 2018). The PPO2 model im-\nplemented therein has been interfaced with DIAMBRA Arena\nby means of a speciﬁc environment wrapper, mainly related to\nthe management of observations format. A custom deep neural\nnetwork has been designed for the policy and value networks,\nand provided to the PPO2 class as model to train. All details\nabout networks model architecture, training strategy with hy-\nperparameters and performances and results are described in\nthe following three subsections.\n4.2. Model Architecture\nThe policy and the value networks share all layers up to\nthe latent space, where they bifurcate in two diﬀerent tails.\nThe shared part is composed by two diﬀerent data process-\ning pipelines, both performing information extraction, one from\nframes (frame encoder) and the other one from the RAM states\n(RAM states encoder). The architecture of the two networks is\ndescribed below, and also detailed in Table 7 and in Figure 5.\nTable 7: Model architecture details\nPart\nLayer Type\nDetails\nFrame\nIn\n128 × 128 × 4 tensor\nFrame\nConv\n8 × 8 kernel, 32 filters, relu\nFrame\nConv\n4 × 4 kernel, 64 filters, relu\nFrame\nConv\n3 × 3 kernel, 64 filters, relu\nFrame\nFC/Out\n256 neurons (latent)\nRAM states\nInput\n161 × 1 × 1 tensor\nRAM states\nFC\n64 neurons, tanh\nRAM states\nFC/Out\n64 neurons (latent), tanh\nConcat\nIn\n256 × 1 × 1 + 64 × 1 × 1 tensor\nConcat\nOut\n320 × 1 × 1 tensor\nValue\nIn\n320 × 1 × 1 tensor\nValue\nOut\n1 value\nPolicy\nIn\n320 × 1 × 1 tensor\nPolicy\nOut\n12 values (actions)\nThe frame encoder is almost the same as the one used in the\nNature paper by Mnih et al. (2015), usually referred as Nature\nCNN. It is composed by three convolution layers plus a fully\nconnected one; the input tensor is of size 128×128×4 and a total\nof 256 latent features are generated by the ﬁnal fully connected\nlayer, which receives 9216 values obtained from ﬂattening third\nconvolution layer output tensor (12 × 12 × 64). It uses relu as\nactivation function.\nThe RAM states encoder is composed by two fully connected\nlayers, generating 64 additional latent features. It uses tanh as\nactivation function.\nLatent features extracted by the two encoders are concate-\nnated together obtaining a 1D tensor of size 320.\nThe policy and value network tails are both made of a sin-\ngle fully connected layer connecting the 320 latent features to,\nrespectively: the set of 12 available actions followed by a soft-\nmax block for action selection (policy net), and the single out-\nput value (value net).\n4.3. Training Strategy\nA fair amount of tests has been performed to identify a train-\ning strategy able to obtain good results.\nThe resulting con-\n9\nFigure 5: Model architecture scheme. Both the value function and the policy share the same backbone that extracts latent features from inputs, which are then\nforwarded to their speciﬁc layers.\nﬁguration consist of: 128 × 128 frame resolution, grayscale\nframe depth, 4 frames stack, 12 actions stack, 16 parallel en-\nvironments, 128 steps per update, batch size of 256, 4 training\nepochs per update, discount factor of 0.94, learning rate sched-\nule 2e−4 →2e−6, clipping factor schedule 0.15 →0.025.\nAdditional details are presented in Table 8 and Figure 6.\nThey show the list of hyperparameters values with their descrip-\ntion, and the training process block diagram. Three clusters of\nhyperparameters can be identiﬁed related to observation space,\nrollouts and training updates.\nThe ﬁrst four hyperparameters (frame resolution, frame\ndepth, frame stack and actions stack) have a direct impact on the\nsize of the observation space. Diﬀerent tests have been carried\nout, especially for frame resolution and frame stack, reaching\nvalues up to [256 × 256] and 6 respectively. The selected val-\nues, reported in Table 8, allow to obtain a good performance\nwhile maintaining a reasonable training time, granting a 6×\nspeedup with respect to the largest conﬁguration tested.\nThe next three hyperparameters (parallel environments, en-\nvironment steps per update and batch size) are strictly related\nto rollouts generation. They deﬁne the amount and the diver-\nsiﬁcation of the experience collected by the agent, and also in\nthis case, a trade-oﬀis needed in order to limit RAM and GPU\nmemory requirements. Having many environments running in\nparallel is particularly important to diversify the collected ex-\nperience samples, minimizing their correlation, especially for\nstandard on-policy methods (as PPO) that cannot leverage (op-\ntionally prioritized) memory replay buﬀers.\nThe batch size\nplays a role too in this regard, while the number of environment\nsteps per update must be chosen with care, taking into account\nin particular rewards sparsity, and making sure the value is high\nenough to collect informative experience samples, that is sam-\nples where collected rewards have relevant eﬀects.\nThe last four hyperparameters (training epochs number, dis-\ncount factor, learning rate and clipping factor) inﬂuence more\nlow level aspects of training updates. Two key aspects to note\nof the adopted training strategy are: A) a linearly decreasing\nschedule, function of training steps, for both the learning rate\nand the clipping factor to favor ﬁne convergence towards an\noptimal policy; B) the selected value for rewards discounting\n10\nFigure 6: Block diagram representation of the training strategy\n(0.94), which is again a parameter to be chosen with care. In\nfact, it determines how far back in time the rewards are propa-\ngated, thus it must be set taking into account rewards sparsity,\npast actions importance, and game speciﬁc dynamics.\nThe block scheme representing the complete training process\n(Figure 6) describes in detail the infrastructure: it consists of a\nsingle workstation in which 16 rollout workers collect training\nexperience in parallel, store experience collected in 128 envi-\nronment steps and discount rewards across them. 10 evaluation\nworkers evaluate agent performances at regular intervals during\ntraining. Agent model parameters are shared between the two\ngroups of workers. Rollout data samples are moved in batches\nto the single GPU to leverage CUDA computing, and model\nupdates are performed accordingly to hyperparameters setup.\n4.4. Performances and Results\nTwo diﬀerent hardware setups have been used to perform\ntraining tests, Table 9 provides details in terms of compo-\nnents and training speed.\nThey can be considered low-to-\nmedium level home desktop conﬁgurations in terms of comput-\ning power. Agent performance as a function of training steps is\npresented in Figure 7, where the 10-episodes averaged total cu-\nmulative reward for the PPO agent is plotted together with the\nrandom agent one. After 25M training steps, the trained agent\naverage reward has reached a value of around 12, notably larger\nthan the random agent one, a clear demonstration it is learning\nhow to play in order to maximize episode total cumulative re-\nward. Visual comparison between the random agent and the\ntrained one demonstrates, even more than the chart, how well\nthe agent policy learned a playing strategy at least very good, if\nnot optimal.\nA video comparison showing the agent at three diﬀerent\ntraining stages (10M steps, 25M steps and 50M steps) can be\nfound at this link2. There are a few elements clearly showing\nagent improvement, two that can be easily noted are: A) the\ntrained agent stops performing attack moves when the opponent\nis far and out of reach. Instead, it starts to wait for the oppo-\nnent not only to be close enough, but even to stand up when\nlaying on the ground after being hit; B) the trained agent timely\nperforms counter-moves when appropriate, evading opponent’s\nattacks.\nAn identical approach has been successfully adopted to train\nthe same DeepRL agent in the other games and omitted here\nfor conciseness. Visual results are publicly available for Street\nFighter III and Tekken Tag Tournament at this link3 and this\nlink4 respectively.\nAchieving these results within a training time of less than\ntwo and a half days, using a medium-low level standard home\ndesktop, conﬁrms these environments, while featuring games\nwith complex mechanics, can be successfully used by everyone.\n5. Discussion\nThe presentation of DIAMBRA Arena (Section 3) and\nDeepRL Agent training results (Section 4), provides many in-\nteresting insights.\nOne important advantage of the software\n2DeepRL Agent in Dead Or Alive: https://www.youtube.com/watch?\nv=2IXsMAdAEBU\n3DeepRL Agent in Street Fighter: https://www.youtube.com/watch?\nv=dw72POyqcqk\n4DeepRL Agent in Tekken Tag https://www.youtube.com/watch?v=\nXEJ9QfmmzwM\n11\nTable 8: Training hyperparameters\nHyperparameter\nValue\nDetails\nFrame size\n128 × 128\nGame frame warping resolution\nFrame depth\n1 Channel\nGame frame warping color compression\nFrame stack\n4\nNumber of most recent frames experienced by the agent stacked\ntogether and used as input to the Policy and Value Networks\nActions stack\n12\nNumber of most recent actions executed by the agent stacked together\nand used in the RAM states as input to the Policy and Value Networks\nParallel\nenvironments\n16\nNumber of environments collecting training rollouts in parallel\nEnvironment\nsteps per update\n128\nNumber of environment steps run per update\nBatch size\n256\nNumber of training steps over which each Adam Optimizer update is computed\nTraining epochs\nper update\n4\nNumber of training epochs run per update\nDiscount factor\n0.94\nDiscount factor used in return discounting, constant through training\nLearning rate\n2e−4 →2e−6\nLearning rate used by Adam Optimizer update, linear scheduler function\nof training steps\nClipping factor\n0.15 →0.025\nClipping factor used by PPO surrogate objective clipping, linear scheduler\nfunction of training steps\nTable 9: Hardware specs and training numbers\nSetup #1\nSetup #2\nCPU\ni5 4 cores/\n8 threads\nAMD Rayzen 9\n12 cores/24 threads\nRAM\n16Gb\n16Gb\nGPU\nGTX 1050 4Gb\nGTX 1080Ti 11Gb\nEnv steps / day\n∼2.6M\n∼10.2M\nTraining time\n∼10.2 Days 24/7\n∼2.36 Days 24/7\npackage is the high degree of customization in terms of action\nspaces, observation space and environments mechanics. For ex-\nample, it is possible to make the tasks purely visual (using the\nscreen buﬀer alone as observation) or to consider a selection of\n(fair) RAM values too as available features, to easily extend the\nactions set or to use multiple characters.\nThe software comes with ready-to-use wrappers of diﬀerent\nkind, covering observation, action and reward wrapping classes.\nThese are very important elements when it comes to interfac-\ning environments with RL libraries, taking advantage of already\nimplemented ones results in a major speedup.\nDIAMBRA Arena guarantees environments accessibility and\nusability for a very broad audience. Thanks to minimal require-\nments in terms of computing power, even CPU-only training is\nFigure 7: Mean episode reward plot as a function of environment training steps,\nwith random agent score as a reference baseline\na viable option. The comparison with similar tools currently\navailable in the literature, carried out in Section 3.4, demon-\nstrated a comparable performance in terms of speed and mem-\nory footprint, while providing more advanced environments en-\nabling research on more advanced and complex RL topics. In\naddition, Section 4 showed only a few days are needed to train\nend-to-end a DeepRL agent with a low-medium level worksta-\ntion. It can be easily interfaced with third party libraries, espe-\ncially for RL training, as done in this work with Stable Base-\nlines.\n12\nIn its current form, DIAMBRA Arena provides ﬁghting\ngames as RL tasks, covering many diﬀerent interesting options\nfor modern RL research that are discussed below. Moreover,\nadditional types of environments are already planned for the\nnear future.\nAgent generalization. One potential problem of the major-\nity of RL benchmarks is that they are not ideal for testing\ngeneralization between similar tasks. This may cause falling\ninto the pitfall of ”training RL algorithms on the test set”,\nmeasuring model performance on the same environment(s) it\nwas trained on.\nIn order to prevent this, RL environments\nneed to provide means by which they can replicate the con-\ncept of ”train” and ”test” split, as typically done for supervised\nlearning datasets. Few-shot learning, cross-task generalization,\nexploration-maximization, are all topics that would relevantly\nbeneﬁt from such a feature. DIAMBRA Arena already sup-\nports this need, providing diﬀerent tasks with similar scope and\nmechanics; it allows to run episodes from game start to end,\nas a sequence of diﬀerent stages; and allows to select diﬀerent\ngame diﬃculty levels.\nThese are all features that can be used to test agent general-\nization capabilities. To stress this aspect even more, the future\naddition of new games will favor environments consisting of\nmany similar tasks sampled from a single task distribution cre-\nating opportunities to learn how to explore on some levels and\ntransfer this ability to other levels.\nCurriculum and transfer learning. Typically, RL challenges\nand tasks are approached with the aim of solving them from\nscratch. One of the most interesting directions of current re-\nsearch considers sequences of tasks, training the algorithm on\none task after the other. These sequences are made of tasks with\nan increasing level of diﬃculty, and are meant to be solved in\norder. As for agent generalization, also these applications are\nalready supported and more eﬀort will be put to extend features\nin this speciﬁc direction.\nHuman-in-the-loop training. Functionalities currently avail-\nable in the software package allow to easily provide feedback to\nthe agent and to modify the environment during training, thus\nsupporting exploration in this ﬁeld of research. Also in this\ncase, new types of environments are planned for development\nin the near future, with the aim of making human-in-the-loop\ntraining even more accessible for research and experimentation.\nMulti-agent. Current version of DIAMBRA Arena already\nsupports multi-agent applications in the competitive ﬂavor\n(both agent-agent and agent-human). One of the future devel-\nopment directions is to add new environments in which also\nthe cooperative ﬂavor is available, for both agent-agent as well\nas human-agent settings. Additional directions are being dis-\ncussed, also touching the so called ”value alignment problem”,\none of the aspects involved in typical reasonings around exis-\ntential concerns for AI.\nReal-world operation. The need to validate RL algorithms\nin the real world is of paramount importance, and would repre-\nsent a major achievement that could lead to the realization of a\ngreat potential. Almost certainly, it will involve aspects related\nto agent-human cooperation/competition, thus requiring robust\nand scalable training where agents play against (or in coopera-\ntion with) humans. The aim of developing tools and infrastruc-\nture to enable the study of agent-human interaction at scale is\nalso in the road map.\n6. Conclusions\nThis paper presented DIAMBRA Arena, a novel software\npackage for RL research and experimentation. It provides high-\nquality environments, complemented by a very broad set of\ntools enabling many diﬀerent lines of research that focus on ma-\njor challenges the scientiﬁc community is facing, as standard\nRL, competitive multi-agent, human-agent competition, human-\nin-the-loop training, imitation learning and self-play. Support-\ning all major OSs, and implementing the standard OpenAI Gym\nPython API interface, its adoption is easy and straightforward.\nMultiple DeepRL agents have been trained in the provided en-\nvironments, using PPO algorithm through Stable Baselines li-\nbrary, obtaining optimal performances and human-like behav-\nior. Results achieved conﬁrm DIAMBRA Arena utility as a\nreinforcement learning research platform, providing environ-\nments designed to study the most challenging topics in the ﬁeld.\nAcknowledgments\nThe author thanks Marco Meloni for his insightful comments\nand suggestions for editing an earlier draft of this manuscript.\n13\nReferences\nAbel, D., Salvatier, J., Stuhlmuller, A., Evans, O., 2016.\nAgent-agnostic\nhuman-in-the-loop reinforcement learning, in: NIPS Workshop on the Fu-\nture of Interactive Learning Machines. URL: http://arxiv.org/abs/\n1701.04079, arXiv:1701.04079.\nBaker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., Mor-\ndatch, I., 2020. Emergent tool use from multi-agent autocurricula, in: Inter-\nnational Conference on Learning Representations.\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., Mordatch, I., 2018. Emer-\ngent complexity via multi-agent competition, in: International Conference\non Learning Representations.\nBeattie, C., Leibo, J.Z., Teplyashin, D., Ward, T., Wainwright, M., K¨uttler, H.,\nLefrancq, A., Green, S., Vald´es, V., Sadik, A., Schrittwieser, J., Anderson,\nK., York, S., Cant, M., Cain, A., Bolton, A., Gaﬀney, S., King, H., Has-\nsabis, D., Legg, S., Petersen, S., 2016. DeepMind Lab. arXiv e-prints ,\narXiv:1612.03801arXiv:1612.03801.\nBellemare, M.G., Naddaf, Y., Veness, J., Bowling, M., 2012.\nThe Arcade\nLearning Environment: An Evaluation Platform for General Agents. arXiv\ne-prints , arXiv:1207.4708arXiv:1207.4708.\nBerner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C.,\nFarhi, D., Fischer, Q., Hashme, S., Hesse, C., Jozefowicz, R., Gray, S.,\nOlsson, C., Pachocki, J., Petrov, M., Pinto, H.P.d.O., Raiman, J., Sali-\nmans, T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I., Tang, J.,\nWolski, F., Zhang, S., 2019. Dota 2 with large scale deep reinforcement\nlearning.\narXiv e-prints URL: http://arxiv.org/abs/1912.06680,\narXiv:1912.06680.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J.,\nTang, J., Zaremba, W., 2016.\nOpenAI Gym.\narXiv e-prints ,\narXiv:1606.01540arXiv:1606.01540.\nDIAMBRA, 2022a. Arena Documentation. https://docs.diambra.ai.\nDIAMBRA, 2022b. Arena GitHub. https://github.com/diambra/arena.\nFeger, S., Dallmeier-Tiessen, S., Wo´zniak, P., Schmidt, A., 2018. Just not the\nusual workplace: Meaningful gamiﬁcation in science, in: Dachselt, R., We-\nber, G. (Eds.), Mensch und Computer 2018 - Workshopband, Gesellschaft\nf¨ur Informatik e.V., Bonn. doi:10.18420/muc2018-ws03-0366.\nFeger, S.S., Dallmeier-Tiessen, S., Wo´zniak, P.W., Schmidt, A., 2019. Gami-\nﬁcation in science: A study of requirements in the context of reproducible\nresearch, in: Proceedings of the 2019 CHI Conference on Human Factors in\nComputing Systems, Association for Computing Machinery, New York, NY,\nUSA. p. 1–14. URL: https://doi.org/10.1145/3290605.3300690.\nGari, M.R.N., Walia, G.S., Radermacher, A.D., 2018. Gamiﬁcation in com-\nputer science education: a systematic literature review, in: 2018 ASEE An-\nnual Conference & Exposition, ASEE Conferences, Salt Lake City, Utah.\ndoi:10.18260/1-2--30554. https://peer.asee.org/30554.\nHill, A., Raﬃn, A., Ernestus, M., Gleave, A., Kanervisto, A., Traore, R.,\nDhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford,\nA., Schulman, J., Sidor, S., Wu, Y., 2018.\nStable baselines.\nhttps:\n//github.com/hill-a/stable-baselines.\nHo, J., Ermon, S., 2016.\nGenerative adversarial imitation learning, in:\nLee, D., Sugiyama, M., Luxburg, U., Guyon, I., Garnett, R. (Eds.),\nAdvances in Neural Information Processing Systems, Curran Associates,\nInc.\nURL: https://proceedings.neurips.cc/paper/2016/file/\ncc7e2b878868cbae992d1fb743995d8f-Paper.pdf.\nHursen, C., Bas, C., 2019. Use of gamiﬁcation applications in science edu-\ncation. International Journal of Emerging Technologies in Learning (iJET)\n14, 4–23. URL: https://online-journals.org/index.php/i-jet/\narticle/view/8894.\nHussein, A., Gaber, M.M., Elyan, E., Jayne, C., 2017. Imitation learning: A\nsurvey of learning methods. ACM Comput. Surv. 50. URL: https://doi.\norg/10.1145/3054912, doi:10.1145/3054912.\nJuliani,\nA.,\nBerges,\nV.P.,\nTeng,\nE.,\nCohen,\nA.,\nHarper,\nJ.,\nElion,\nC., Goy, C., Gao, Y., Henry, H., Mattar, M., Lange, D., 2018.\nUnity:\nA General Platform for Intelligent Agents.\narXiv e-prints ,\narXiv:1809.02627arXiv:1809.02627.\nJuliani, A., Khalifa, A., Berges, V.P., Harper, J., Teng, E., Henry, H.,\nCrespi, A., Togelius, J., Lange, D., 2019.\nObstacle Tower: A Gener-\nalization Challenge in Vision, Control, and Planning.\narXiv e-prints ,\narXiv:1902.01378arXiv:1902.01378.\nKalogiannakis, M., Papadakis, S., Zourmpakis, A.I., 2021. Gamiﬁcation in\nscience education. a systematic review of the literature.\nEducation Sci-\nences 11. URL: https://www.mdpi.com/2227-7102/11/1/22, doi:10.\n3390/educsci11010022.\nKempka, M., Wydmuch, M., Runc, G., Toczek, J., Ja´skowski, W., 2016.\nVizdoom: A doom-based ai research platform for visual reinforcement\nlearning, in: 2016 IEEE Conference on Computational Intelligence and\nGames (CIG), pp. 1–8. URL: https://ui.adsabs.harvard.edu/abs/\n2016arXiv160502097K, doi:10.1109/CIG.2016.7860433.\nKnox, W.B., Stone, P., 2008. Tamer: Training an agent manually via evaluative\nreinforcement, in: 2008 7th IEEE International Conference on Development\nand Learning, pp. 292–297.\nLaird, J., VanLent, M., 2001. Human-level ai’s killer application: Interactive\ncomputer games. AI Magazine 22, 15. URL: https://ojs.aaai.org/\nindex.php/aimagazine/article/view/1558,\ndoi:10.1609/aimag.\nv22i2.1558.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare,\nM.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., Petersen,\nS., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,\nD., Legg, S., Hassabis, D., 2015. Human-level control through deep rein-\nforcement learning. Nature 518, 529–533. URL: https://doi.org/10.\n1038/nature14236, doi:10.1038/nature14236.\nNichol, A., Pfau, V., Hesse, C., Klimov, O., Schulman, J., 2018.\nGotta\nLearn Fast: A New Benchmark for Generalization in RL. arXiv e-prints\n, arXiv:1804.03720arXiv:1804.03720.\nSamuel, A.L., 1959.\nSome studies in machine learning using the game\nof checkers.\nIBM Journal of Research and Development 3, 210–229.\ndoi:10.1147/rd.33.0210.\nSchulman,\nJ.,\nWolski,\nF.,\nDhariwal,\nP.,\nRadford,\nA.,\nKlimov,\nO.,\n2017.\nProximal Policy Optimization Algorithms.\narXiv e-prints ,\narXiv:1707.06347arXiv:1707.06347.\nShannon, C.E., 1950.\nXxii. programming a computer for playing chess.\nThe London,\nEdinburgh,\nand Dublin Philosophical Magazine and\nJournal of Science 41,\n256–275.\nURL: https://doi.org/10.\n1080/14786445008521796,\ndoi:10.1080/14786445008521796,\narXiv:https://doi.org/10.1080/14786445008521796.\nSilver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van den Driess-\nche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,\nDieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lil-\nlicrap, T., Leach, M., Kavukcuoglu, K., Graepel, Thoreand Hassabis, D.,\n2016. Mastering the game of go with deep neural networks and tree search.\nNature 529, 484–489. URL: https://doi.org/10.1038/nature16961,\ndoi:10.1038/nature16961.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,\nA., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T.,\nHui, F., Sifre, L., van den Driessche, G., Graepel, T., Hassabis, D., 2017.\nMastering the game of go without human knowledge. Nature 550, 354–\n359. URL: https://doi.org/10.1038/nature24270, doi:10.1038/\nnature24270.\nTesauro, G., 1995. Temporal diﬀerence learning and td-gammon. J. Int. Com-\nput. Games Assoc. 18, 88. doi:10.3233/ICG-1995-18207.\nTodorov, E., Erez, T., Tassa, Y., 2012. Mujoco: A physics engine for model-\nbased control, in: 2012 IEEE/RSJ International Conference on Intelligent\nRobots and Systems, pp. 5026–5033. doi:10.1109/IROS.2012.6386109.\nVinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Sasha Vezhnevets,\nA., Yeo, M., Makhzani, A., K¨uttler, H., Agapiou, J., Schrittwieser, J.,\nQuan, J., Gaﬀney, S., Petersen, S., Simonyan, K., Schaul, T., van Has-\nselt, H., Silver, D., Lillicrap, T., Calderone, K., Keet, P., Brunasso,\nA., Lawrence, D., Ekermo, A., Repp, J., Tsing, R., 2017.\nStarCraft\nII: A New Challenge for Reinforcement Learning.\narXiv e-prints ,\narXiv:1708.04782arXiv:1708.04782.\nZhang, R., Torabi, F., Guan, L., Ballard, D.H., Stone, P., 2019. Leveraging\nhuman guidance for deep reinforcement learning tasks, in: Proceedings of\nthe Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence,\nIJCAI-19, International Joint Conferences on Artiﬁcial Intelligence Orga-\nnization. pp. 6339–6346.\nURL: https://doi.org/10.24963/ijcai.\n2019/884, doi:10.24963/ijcai.2019/884.\n14\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-10-19",
  "updated": "2022-10-19"
}