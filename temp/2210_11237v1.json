{
  "id": "http://arxiv.org/abs/2210.11237v1",
  "title": "Emerging Threats in Deep Learning-Based Autonomous Driving: A Comprehensive Survey",
  "authors": [
    "Hui Cao",
    "Wenlong Zou",
    "Yinkun Wang",
    "Ting Song",
    "Mengjun Liu"
  ],
  "abstract": "Since the 2004 DARPA Grand Challenge, the autonomous driving technology has\nwitnessed nearly two decades of rapid development. Particularly, in recent\nyears, with the application of new sensors and deep learning technologies\nextending to the autonomous field, the development of autonomous driving\ntechnology has continued to make breakthroughs. Thus, many carmakers and\nhigh-tech giants dedicated to research and system development of autonomous\ndriving. However, as the foundation of autonomous driving, the deep learning\ntechnology faces many new security risks. The academic community has proposed\ndeep learning countermeasures against the adversarial examples and AI backdoor,\nand has introduced them into the autonomous driving field for verification.\nDeep learning security matters to autonomous driving system security, and then\nmatters to personal safety, which is an issue that deserves attention and\nresearch.This paper provides an summary of the concepts, developments and\nrecent research in deep learning security technologies in autonomous driving.\nFirstly, we briefly introduce the deep learning framework and pipeline in the\nautonomous driving system, which mainly include the deep learning technologies\nand algorithms commonly used in this field. Moreover, we focus on the potential\nsecurity threats of the deep learning based autonomous driving system in each\nfunctional layer in turn. We reviews the development of deep learning attack\ntechnologies to autonomous driving, investigates the State-of-the-Art\nalgorithms, and reveals the potential risks. At last, we provides an outlook on\ndeep learning security in the autonomous driving field and proposes\nrecommendations for building a safe and trustworthy autonomous driving system.",
  "text": "Emerging Threats in Deep Learning-Based Autonomous Driving: A\nComprehensive Survey\nCao Huia, Zou Wenlonga, Wang Yinkunb, Song Tingc and Liu Mengjuna,∗\naSchool of Education,Hubei University,Youyi Road No.368,Wuhan,430062,Hubei,P.R.China\nbInstitute of Information Engineering,Chinese Academy of Sciences, Beijing,100093,P.R.China\ncSchool of Foreign Languages,Hubei University,Youyi Road,No.368, Wuhan,430062,Hubei,P.R.China\nA R T I C L E I N F O\nKeywords:\nTrustworthy AI\nDeep Learning\nArtiﬁcial Intelligence\nAutonomous Driving\nCyber Security\nAdversarial Examples\nAbstract\nSince the 2004 DARPA Grand Challenge, the autonomous driving technology has witnessed nearly\ntwo decades of rapid development. Particularly, in recent years, with the application of new sensors\nand deep learning technologies extending to the autonomous ﬁeld, the development of autonomous\ndriving technology has continued to make breakthroughs. Thus, many carmakers and high-tech giants\ndedicated to research and system development of autonomous driving. However, as the foundation\nof autonomous driving, the deep learning technology faces many new security risks. The academic\ncommunity has proposed deep learning countermeasures against the adversarial examples and AI\nbackdoor, and has introduced them into the autonomous driving ﬁeld for veriﬁcation. Deep learning\nsecurity matters to autonomous driving system security, and then matters to personal safety, which is\nan issue that deserves attention and research.This paper provides an summary of the concepts, devel-\nopments and recent research in deep learning security technologies in autonomous driving. Firstly, we\nbrieﬂy introduce the deep learning framework and pipeline in the autonomous driving system, which\nmainly include the deep learning technologies and algorithms commonly used in this ﬁeld. Moreover,\nwe focus on the potential security threats of the deep learning based autonomous driving system in\neach functional layer in turn. We reviews the development of deep learning attack technologies to\nautonomous driving, investigates the State-of-the-Art algorithms, and reveals the potential risks. At\nlast, we provides an outlook on deep learning security in the autonomous driving ﬁeld and proposes\nrecommendations for building a safe and trustworthy autonomous driving system.\n1. Introduction\nResearch about Autonomous Land Vehicles (ALVs) be-\ngan as early as 1980s with funding from the US Depart-\nment of Defense(DoD). In the 21st century, DARPA con-\nducted the Grand Challenge that launched a new generation\nof autonomous driving. The development of artiﬁcial in-\ntelligence(AI) technology is driving the rapid progress of\nautonomous vehicles with an increasing expectation from\nthe public. Currently, many traditional carmakers, such as\nuniversal Motors, Toyota, Volvo, BMW and Audi have car-\nried out researches into the autonomous driving system. On\nanother hand, not to be outdone, most of high-tech giants,\nGoogle Waymo, Tesla, Baidu and Huawei, devoted them-\nselves to autonomous driving technology. Along with arti-\nﬁcial intelligence technology, autonomous driving has seen\nrapid development and is expected to enter the practical stage.\n⋆This document is the results of the research project funded by the\nNational Natural Science Foundation of China: Research on secure data\nmanagement mechanism of new college entrance examination comprehen-\nsive quality evaluation: A security enhancement of block chain empower-\nment(No. 72204077)\n⋆⋆This document is the results of the research project funded bt the\nHubei Natural Science Foundation project:Research on the data security\nmanagement mechanism of new College Entrance Examination compre-\nhensive quality evaluation based on blockchain.(No. 2021CFB470)\n∗Corresponding author\n∗∗First two author equal contribution\ncao-hui@whu.edu.cn (C. Hui); lmj_whu@163.com (L. Mengjun)\nhttps://scholar.google.com/citations?user=1XoXUTYAAAAJ&hl=en\n(C. Hui)\nORCID(s):\nHowever, security is a major concern in the application\nof the autonomous driving system, because there are new\ntypes of security risks associated with autonomous driving\nsystem that depends heavily on deep learning. On the one\nhand, from the perspective of technical threat on AI secu-\nrity and privacy protection, new countermeasures have been\nproposed successively, including adversarial examples [1,\n2], data poisoning and AI Backdoor[3], model extraction[4],\nmodel inversion[5], and membership privacy inference[6].\nOn the another hand, from the social trust perspective of AI,\nissues about fairness, AI abuse, environment, compliance,\nand ethic, have also received attention and research. Cur-\nrently, there is some literature[7, 8, 9, 10, 11, 12, 13] sum-\nmarized AI security threats in the general environment. Dif-\nferent from that, this paper focuses on the environment of\nautonomous driving system, it reveals the new security risks\nposed by AI technologies bringing new security challenges\nto autonomous driving. Unlike other applications of deep\nlearning, the autonomous driving system is a more complex\nAI architecture consisting of dozens of functional modules,\nand diﬀerent environment modules with diﬀerent character-\nistics, raising diﬀerent requirements for AI security attack\nand mitigation techniques, including:\n• Physical world requirements. AI threats of autonomous\ndriving system should be able to take eﬀect in the real\nphysical world, and not only in the digital world and\ncomputer simulation systems. Techniques speciﬁc to\nadversarial examples attacks in the physical world are\nthe focus of this paper.\nHui Cao et al.: Preprint submitted to Elsevier\nPage 1 of 28\narXiv:2210.11237v1  [cs.CR]  19 Oct 2022\nEmerging Threats in Deep Learning-Based Autonomous Driving\n• Robustness requirements. The environment is un-\ncertain and often varies to a large extent in autonomous\ndriving. On the one hand, the images collected under\ndiﬀerent weather, light and other natural conditions\ncan vary; on the other hand, changes in a long dis-\ntance and large angle range also make image acquisi-\ntion highly variable due to the high-speed movement\nof vehicles. Therefore, AI threats need to be able to\ntake eﬀect continuously and stably under a variety of\nconditions, which raises very high demands on the ro-\nbustness of attacks, such as adversarial examples and\nAI backdoor. This paper focuses on robustness en-\nhancement methods.\n• Fusion environment requirements. Autonomous driv-\ning system often employs multi-modal fusion sensing\ntechniques that combine diﬀerent types of information\nfrom multiple RGB cameras, LiDAR, RaDAR, etc.,\nto sense the fused images. The autonomous driving\nenvironment requires that adversarial examples coun-\ntermeasures and other related threat technologies can\nbe stabilized to remain in eﬀect in the fused environ-\nment. Artiﬁcial intelligence threats in multi-modal fu-\nsion environments are also the focus of this paper.\nDue to the above concerns and requirements, AI safety tech-\nnologies in the ﬁeld of autonomous driving have continued\nto develop, and some research results and breakthroughs have\nbeen achieved. This paper introduces the latest research progress\nrelevant to unique technologies and reveals the AI security\nrisks in autonomous driving systems. This paper faces the\nabove challenges of autonomous driving systems, rather than\nin the general environment. Section 1 brieﬂy introduces the\ninfrastructure and key technologies of AI in autonomous driv-\ning; section 2 oﬀers a glimpse of the AI risks in the sensor\nlayer; section 3 comprehensively reviewed the AI risk in the\nperception layer, introduced the idea and detail of important\nalgorithms; section 4 provides the potential deep leaning risk\nand attack technology in decision layer in autonomous driv-\ning; section 5 focus on new threat of V2X that based on fed-\neration learning in the future; section 6 gives a summary and\noutlook.\n1.1. Basic Concepts of Autonomous Driving\nEssentially, autonomous driving is making driving deci-\nsions through artiﬁcial intelligence techniques or other au-\ntomated decision-making methods. According to the So-\nciety of Automotive Engineers (SAE) standard J3016[14],\nautonomous driving can be categorized into the following\nclasses.\n• L0 – No Driving Automation: driving is carried out\nentirely by a person, but warnings and system assis-\ntance are available during the journey.\n• L1 – Driver Assistance: based on the perception of\nthe driving environment, only a single aspect of au-\ntomation, which system operates the steering wheel\nor acceleration and deceleration assists the driver with\nADAS, while other driving operations are performed\nby the human driver.\n• L2 – Partial Driving Automation: based on the per-\nception of the driving environment, the system oper-\nates both the steering wheel and acceleration or decel-\neration. However, it requires a human driver to remain\nconstantly alert and ready to take full control with lit-\ntle or no warning.\n• L3 – Conditional Driving Automation: based on the\nperception of the driving environment, autonomous\ndriving system can perform all driving operations un-\nder the supervision of a human driver.\n• L4 – High Driving Automation: under certain envi-\nronmental conditions, autonomous driving system can\nperform all driving operations unsupervised.\n• L5 – Full Driving Automation: the autonomous driv-\ning system can perform all driving operations unsu-\npervised in all environmental conditions.\nFor autonomous driving system, there are diﬀerent views\nand concepts, as well as diﬀerent development and evolu-\ntionary routes. One is focus on intelligentization and cyber-\nization of vehicle components, mainly researching on sen-\nsors, in-vehicle communication, vehicle-to-everything (V2X),\nand et al, which main participant by traditional car-makers.\nThe other is focus on autonomous diving decisions, mainly\nresearching artiﬁcial intelligence and autonomous driving,\nand the main participants include: UC Berkeley, Google WayMo,\nBaidu, Apollo, Intel Carla, NVIDIA and other artiﬁcial in-\ntelligence companies. However, whether it starts from the\nvehicle moving towards AI or the other way round, auto-\nmated driving decision is the core mission in autonomous\ndriving, and safety based on AI driving decisions making is\na necessary prerequisite for the safety of autonomous driv-\ning system. The higher the level of autonomous driving, the\nhigher the reliance on AI technology represented by deep\nlearning, which lead to higher the requirements for the safety\nand robustness of deep learning itself.\n1.2. Architecture of Autonomous Driving System\nIn terms of the autonomous driving architecture and ma-\nchine learning technologies based, autonomous driving sys-\ntems can be divided into end-to-end (E2E) and modular ar-\nchitectures.\n• The modular autonomous driving system divides an\nindividual set of autonomous driving functions into\nseveral parts, each of which is completed by one or\na group of artiﬁcial intelligence models, usually in-\ncluding: positioning and projecting, target recogni-\ntion, trajectory prediction, road planning & driving\ndecision making, vehicle control, and other functions.\nThese functional modules contain the sensing layer,\nthe perception layer, the decision layer and the vehi-\ncle networking layer.\nHui Cao et al.: Preprint submitted to Elsevier\nPage 2 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\n(a) Modular Autonomous Driving Framework\n(b) E2E Autonomous Driving Framework\nFigure 1: Autonomous Driving Framework\n• The End-to-End autonomous driving system often con-\nsists of a large number of complex judgment functions\nin driving decisions performed by one or a group of ar-\ntiﬁcial intelligence models that make the ﬁnal driving\ndecision based on the environment and cloud inputs.\n1.3. Sensing Layer\nThe sensing layer includes a variety of sensors that col-\nlect information about the environment for the autonomous\ndriving system. Common sensors used in autonomous driv-\ning vehicles compromise RGB cameras, LiDAR (Light De-\ntection and Ranging), RaDAR (Radio Waves to Determine\nthe Distance), GPS, and ultrasonic sensors. Here are the\ncharacteristics of diﬀerent sensors:\n• The advantages of RGB cameras are: 1) lower cost,\nand 2) relatively mature recognition technology; their\nlimitation is that the distance is dependent on estima-\ntion.\n• The advantage of LiDAR is that it is accurate; its lim-\nitation is that it is susceptible to interference from the\nweather.\n• The advantage of radar is that it is relatively immune\nto weather interference; its limitation is that it has in-\nsuﬃcient imaging capability.\nThere are a number of existing works that provide a de-\ntailed comparison of sensors for autonomous driving vehi-\ncles, which will not be the emphasis of this paper. There are\nsome survey papers related to the sensing layer[15, 16, 17].\nMost companies have chosen autonomous driving tech-\nnology solutions that multi-modal fusion, while some have\nchosen solutions that rely primarily on RGB cameras. How-\never, it needs to be emphasized that, regardless of the choice\nof sensor conﬁguration solution, the various advanced sen-\nsors only fulﬁll the function of raw information collection\nand do not replace the key role played by artiﬁcial intelli-\ngence in the perception and decision-making of autonomous\ndriving system, and are equally unable to avoid the new safety\nrisks posed by AI.\n1.4. Perception Layer\nThe perception layer perceives and identiﬁes things like\nobject perceiving and identiﬁcation, segmentation, depth es-\ntimation and localization, which are based on the vehicle’s\nstate and road information collected by the sensors in the\nsensor layer. The commonly used techniques are given as\nfollows, which include 2D object recognition, 3D object recog-\nnition, multi-modal fusion, trajectory prediction, and so on.\nFigure 2: Sensor in Autonomous Driving\n• 2D Objection Recognition is based on a ﬂat image\nto identify the presence or absence of a speciﬁc tar-\nget in the image and locate it. technologically, 2D\nobject recognition can be divided into two classiﬁca-\ntions: two-stage objection recognition algorithms and\none-stage objection recognition algorithms. The two-\nstage algorithms ﬁrst ﬁnd a series of region proposals,\nand then classify the objects in the proposals by Con-\nvolutional Neural Networks(CNN). Commonly used\ntwo-stage algorithms include FasterRCNN[18] and MaskRCNN[19\ncharacterized by relatively high accuracy and high con-\nsumption. One-stage algorithms do not generate a sep-\narate region proposal but return to the predicted class\nand location of the target directly. Commonly used\none-stage algorithms include: SSD[20]and Yolo v3[21].\nIn 2017, Lin et al.[22] proposed a new loss function -\n\"Focal Loss\", which can signiﬁcantly improve the ac-\ncuracy of dense target recognition, and this technique\nwas ﬁrst applied to the ﬁeld of face recognition. It is\nnow applied to many target recognition ﬁelds, among\nwhich, in 2021, Yosuke Shinya et al.[23] proposed\nHui Cao et al.: Preprint submitted to Elsevier\nPage 3 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nUniverseNet, a target detection algorithm that applies\nFocal Loss, which can achieve better results in dense\ntarget and small target scenarios. A detailed compari-\nson of current mainstream 2D target recognition tech-\nniques can be found in references[15, 16, 17]\n• Multi-Modal Fusion. A single type of sensor cannot\ncapture all of the environmental information needed to\nsupport autonomous driving, while autonomous driv-\ning systems require information from several types and\na large number of sensors to make integrated deci-\nsions, which leads us to make multi-modal fusion. De-\npending on occurred times[24], the fusion can be di-\nvided into three modes: pre-fusion, post-fusion, and\ndeep fusion. Pre-fusion combines the data collected\nby all types of sensors and then makes a comprehen-\nsive decision. Post-fusion to make decisions on the\ndata collected by diﬀerent sensors and then aggregate\nthe sub-decisions. Deep fusion constitute by the fu-\nsion of data, features and decision integration, and can\nbe subdivided into ﬁve types: data in data out, data\nin feature out, feature in feature out, feature in de-\ncision out, and decision in decision out[25, 26]. An\nin-depth analysis and comparison of the various inte-\ngration methods can be found in the literature.[26, 27,\n28, 26]\n• 3D Objection Detection and Segmentation. Because\n2D images have no depth information that is needed in\nautonomous driving, such as path planning and colli-\nsion avoidance in autonomous driving, therefore 3D\nobjection detection plays a key role. Classiﬁed by the\ndetected information, 3D target detection has 3 bases:\n2D image, 3D point cloud map and multi-modal fu-\nsion image. Among them, 3D target detection based\non 2D images often uses 3D target matching and depth\nestimation to estimate the 3D target bounding box for\ntargets in 2D images using algorithms like Mono3D[29],\n3DVP[30], Deepmanta[31], and SVGA-Net[32]. 3D\ntarget recognition based on 3D point cloud maps is\nthe recognition of targets in the images with 3D in-\nformation and marks the target outline. Commonly\nused algorithms include: VeloFCN[33] , BirdNet[34],\n3DFCN[35] , PointNet++[36] and VoxelNet[37]. 3D\ntarget detection based on multi-modal integration im-\nages is to use diﬀerent integration modes to identify\n3D targets. Commonly used algorithms include: MV3D[38],\nAVOD[39], and F-PointNet[40]. A comparison and\nin-depth study of various 3D target detection algorithms\ncan be found in the literature[41, 42].\nOther deep learning research directions in the perception layer\ninclude Pedestrian Detection, Lane Detection, Traﬃc Sign\nRecognition, Pedestrian Attribute Recognition, Fast Vehi-\ncle Detection, Pedestrian Density Estimation, Plate Recog-\nnition, etc. There is detail on the leaderboard[43].\n(a) Pre-fusion\n(b) Post-fusion\n(c) Deep fusion\nFigure 3: Fusion of Autonomous Driving\n1.5. Decision-Making Layer\nDriving decision-making is the core of autonomous driv-\ning, and machine learning methods are often used, with two\ntechnical routes available: Imitation Learning and Reinforce-\nment Learning.\n• Imitation Learning. Imitation learning refers to the\nlearning behavior of agents who acquire the ability\nto perform a speciﬁc task by observing and imitating\nthe behavior of human experts[44]. Imitation learn-\ning has been successful in the ﬁeld of autonomous\ndriving[45] Imitation learning tends to collect a large\namount of environmental state 푆푖(environmental data\ncollected by various sensors, including 3D point cloud\nmaps, RGB images, etc.) as features and record the\nHui Cao et al.: Preprint submitted to Elsevier\nPage 4 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nactions performed by the human experts at the same\ntime. 퐴푖is used as a label to form a training data set\n퐷∶(푠1, 푎1), (푠2, 푎2), (푠3, 푎3), .... Using speciﬁc imi-\ntation learning algorithms, artiﬁcial intelligence mod-\nels are trained and used to make future driving deci-\nsions. The famous imitation learning methods include\nthe E2E autonomous driving algorithm based on con-\nditional imitation learning [46], and the ChauﬀeurNet[47].\n• Deep Reinforcement Learning. Deep reinforcement\nlearning simulates the self-learning model of organ-\nisms in nature. To be concrete, an agent monitors its\nown behavior and the resulting environmental changes,\nsets the reward value for diﬀerent changes, and then\ncontinuously optimizes the model and its own behav-\nior based on this. In 2013, Mnih et al.[48] combined\ndeep learning with reinforcement learning and pro-\nposed the Deep Q Learning(DQN) method. DQN is\nbased on a set of Q values in a reward table. The sys-\ntem’s driving status 푆푖and the driving operation 푎푖to\nobtain the corresponding reward value 푟푖, which auto-\nmatically generates training data 퐷∶((푠1, 푎1), 푟1), ((푠2, 푎2), 푟2), ((푠3, 푎3), 푟3), ....\nThe reinforcement learning model is then trained by\nspeciﬁc algorithms, while reinforcement learning is\nsupplemented with current operational data to con-\ntinuously optimize the model. Nowadays, deep re-\ninforcement learning has been rapidly developed and\nwidely used, with subsequently emerged Deep Recur-\nrent Q Networks (DRQNs)[49], attention mechanism\ndeep recurrent Q networks[50], asynchronous/synchronous\ndominant actor-critic (A3C/A2C)[51], and reinforce-\nment learning for unsupervised and unassisted tasks[52],\nwhich are widely used in e-Sports, health & medicine,\nrecommendation system and other ﬁelds. There are\nsome surveys of deep reinforcement learning[53, 54].\nA variety of deep reinforcement learning frameworks\nand algorithms are widely used in the ﬁeld of autonomous\ndriving vehicles. For example, Feng et al.[55], Al-\nizadeh et al.[56], Mirchevska et al.[57], and Quek et\nal.[58] apply deep reinforcement learning techniques\nto driving decisions; Holen et al.[59] use deep rein-\nforcement learning for autonomous driving roadway\nrecognition; Feng et al.[60] utilize deep reinforcement\nlearning techniques for traﬃc light optimization con-\ntrol. Some researchers have also proposed an autonomous\ndriving solution with the fusion of imitation learning\nand reinforcement learning[61, 62].\n1.6. Vehicle Networks\nWith the development of communications and AI tech-\nnology, vehicle networks are increasingly playing an impor-\ntant role in autonomous driving, especially the vehicle net-\nworks construction, which supports a distributed AI model\nand provides a novel type of AI technology in autonomous\ndriving, while also bringing new security risks.\n• Vehicle-to-Everything (V2X). V2X is a multi-layered\nnetwork system designed to enhance collaboration be-\ntween pedestrians, vehicles and transport infrastruc-\nture. It is universally composed of Vehicle-to-Vehicle\n(V2V) networks, Vehicle-to-Infrastructure (V2I) net-\nworks, Vehicle-to-Pedestrian (V2P) networks and Vehicle-\nto-Road side units (V2R) networks[63]. The commu-\nnication technologies used in the vehicular internet of\nthings can be broadly classiﬁed into two categories,\nDedicated Short Range Communication (DSRC) and\nLong-Term Evolution (LTE) cellular communication,\ncalled cellular-V2X or C-V2X for short[64].\n• Federated Learning. The vehicular internet of things\nprovides the network foundation for distributed artiﬁ-\ncial intelligence. Federated Learning is a distributed\nAI framework that replaces sensitive data interactions\nwith model interactions, enabling more eﬃcient and\nbetter privacy for knowledge sharing and transition.\nBased on the V2X, the federated learning can provide\ndistributed and interactive AI services[65, 66, 67] for\nautonomous driving system. This paper focuses on\nthe novel security risks posed by Federated Learning\nin the vehicular internet of things, and reviews related\nsecurity technology developments.\n1.7. Summary\nWe concluded the major AI application used in autonomous\ndriving in Table1\n2. Emerging Threats in Sensors\nSensors are foundational part for the autonomous driv-\ning system, which provide raw environmental information\nfor autonomous driving decision-making. The security of\nsensors directly aﬀects the safety of autonomous driving sys-\ntem. We classify attacks against sensors into two categories,\nwhere attacks that aim to compromise the usability of the\nsensing are classiﬁed as Jamming Attacks and attacks that\naim to compromise the integrity of the information collected\nby the sensors are classiﬁed as Spooﬁng Attacks.\n2.1. Jamming Attacks\nThe Jamming Attack means that attackers take some ac-\ntions to reduce the quality of data collected by the sensor,\neven making sensor unavailable. In 2015, Petit et al.[114]\nattempted a jamming attack on autonomous driving sensors\nby artiﬁcially setting up bright light interference that could\n\"blind\" the camera. In 2016, Yan et al.[115] experimented\nwith blind attacks on ultrasonic sensors. Similarly, a va-\nriety of in-vehicle sensors such as RGB cameras, LiDAR,\nRaDAR, gyroscopic sensors and GPS sensors could be sub-\nject to jamming attacks[116, 117, 118, 119].\n2.2. Spooﬁng Attacks\nThe Spooﬁng Attacks means that attackers injecting fake\nsignals to aﬀect the normal behaviour of the autonomous\ndriving system. In 2015, Petit et al.[114] attempted to send\nspeciﬁc spoofed laser signals, causing the LiDAR systems\nHui Cao et al.: Preprint submitted to Elsevier\nPage 5 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nTable 1\nMajor Deep Learning-based Tasks in Autonomous Driving\nLayer\nTask\nMajor typical deep learning algorithm\nSensor\n3D PointCloud Registration\n3DFeat-Net[68], FCGF[69], D3Feat-pred[70]\nPre-Fusion\nMulti-Frame Fusion[71],\nMTF4VT[72],TransFuser[73],DeepFusion[74]\nPerception\n2D Object Detection\nFast-RCNN[75], Faster R-CNN[75], Mask R-CNN[19],\nD-RFCN[76], Yolov4[77], YOLOv7[78],FD-SwinV2[79]\n3D Object Detection\nPointRCNN[80], PV-RCNN[81],Se-SSD[82],\nGLENet-VR[83]\nLane Detection\nSCNN[84] LaneATT[85] CLRNet[86]\nTraﬃc Sign Recognition\nCNN with 3 Spatial Transformer[87], Mask R-CNN\nwith adaptations and augmentations[19], TSR-SA[88]\nFast Vehicle Detection\nYOLOv3-tiny[89], LittleYolo-SPP[90]\nPedestrian detection\nSA-FastRCNN[91],RPN+BF[92],Pedestron[93],\nSemantic Segmentation\nFCN[94], PSPNet[95], DRAN[96],Swin trasformer[97],\nViT-Adapter[98]\nObject Tracking\nM2-Track[99],BAT[100]\nMultiple Object Tracking\nQDTrack[98], RetinaTrack[? ]\nDecision\nTrajectory Prediction\nNSP-SFM[101], Y-Net[102],Trajectron++[103],Social\nGAN[104],SoPhie[105]\nMotion Forecasting\nVI LaneIter[106], Wayformer[107]\nDeep Reinforcement Learning\nDeep Q-Learning[108]),Deep recurrent\nq-learning[49],Deep attention recurrent\nQ-network[50],Double Q-learning[109],A3C/A2C[51]\nImitation Learning\nGenerative adversarial imitation learning[110],\nConditional Imitation Learning[46, 111], Self-Imitation\nLearning[112], Chauﬀeurnet[47]\nV2X\nFederated Learning\nFedAvg[113]\nto be misled. Later, Park et al.[120] conducted similar ex-\nperiments on in-vehicle IR sensors. Yan et al.[115] worked\non gyroscopic sensors and RaDAR. Nassi et al.[121] con-\nducted combined experiments on RGB cameras, LiDAR and\nRaDAR. Psiaki et al.[122], Meng et al.[123] conducted spoof-\ning experiments on GPS for multiple environments.\nCurrently, most attacks against sensors of vehicle are trend\ntowards physical attack rather than attack on deep learning.\nIn this paper, we only give a general overview, there are more\ndetails in the surveys[17, 15].\n3. Emerging Threats in Perceptual Layer\nBased on the information captured by various types of\nsensors in the sensor layer, the perception layer performs\nrecognition and perception. These tasks, such as objective\nrecognition, segmentation, and depth estimation, are often\ndiﬃcult to accomplish through simple computing based on\nsome certain rules. Artiﬁcial intelligence is also subject to\nnew types of security threats. For example, attackers could\nuse Adversarial Examples or AI Backdoor attacks, which\ncan mislead to wrong predictions that be controlled by at-\ntackers, which leads to dangerous driving decisions. Attack-\ners may also use Model Extraction to obtain the parameters\nor Hyper-parameters of the AI model, resulting in model\nleakage and loss of intellectual property. And then attackers\nwould use Model Inversion or Membership Privacy attacks\nleading to sensitive training data leakage and privacy risks.\nDiﬀerent from some existing surveys that introduce gen-\neral adversarial examples or AI backdoors in cyberspace,\nthis paper focuses on advanced research in the physical world.\nAttacking in the physical world has faced higher demands,\nespecially, on attack constancy, high success rate, and ro-\nbustness on environmental uncertainties.\n3.1. Physical World Adversarial Examples for\nStatic Objective Detection\nIn 2014, researchers discovered that adding a small amount\nof speciﬁc interference, which is imperceptible to human be-\nings, may still cause machine learning to be misled by attack-\ners. This could cause serious security even safety problem,\nif machine learning be applied to a critical domain. Such an\nattack is known as adversarial examples attack and can be\nformalized as\n푎푟푔푚푖푛푥′ ‖‖푥′ −푥‖‖푝푠.푡.푓(푥′) = ̂푦\n(1)\nwhere 푓denotes a machine learning model, 푥denotes a\ntest example, and 푥′ denotes an adversarial example gener-\nated based on the addition of a small amount of interference,\nwith 푐for the prediction result of the model for a normal ex-\nHui Cao et al.: Preprint submitted to Elsevier\nPage 6 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nample, and 푐′ for the prediction result of the model on the\nadversarial examples.\nAfter the ﬁrst work, adversarial examples technique has\nbeen widely studied and has seen rapid development. FGSM\nalgorithm proposed by Goodfellow et al.[2] FGSM works by\ncalculating the gradient of the loss function between the in-\nput and target classiﬁcation and creating a small perturbation\nin terms of the sign vector coeﬃcient of this gradient as:\n푥퐴푑푣= 푥+ 훼푠푖푔푛(▿푥퐽(푥, 푦))\n(2)\nwhere 푥푎푑푣denotes the corresponding adversarial exam-\nple of 푥, 훼is a speciﬁc constant, 푠푖푔푛() is a sign function,\n푦푡푟푢푒is the the corresponding true label of 푥, 퐽() denotes\nthe loss function used to train the model, and ▿푥denotes\nthe the gradient of 푥. The algorithm can be used to add a\ncertain amount of adversarial noise to an image that is nor-\nmally predicted as a panda, so that the machine learning\nmodel can predicted it as a gibbon with a high conﬁdence.\nThis algorithm could achieve both untargeted attacks and tar-\ngeted attacks. There are more concerned adversarial exam-\nples algorithms be proposed in following. In 2016, Paper-\nnot et al.[124] proposed a black-box attack using an alterna-\ntive model approach. In 2017, Moosavi-Dezfooli et al.[125]\nproposed an universal adversarial example attack, where a\nparticular adversarial interference is able to inﬂuence the\nclassiﬁcation of multiple or even all examples in machine\nlearning. In the same year, Carlini et al.[126] proposed an\noptimization-based C&W algorithm to improve the adver-\nsarial examples attack. In 2018, Zhao et al.[127] found that\nnot need to be ﬁlled with artiﬁcial interference, but found dif-\nferent distribution in nature and can cause misclassiﬁcation\nof machine learning models, which is called Natural Adver-\nsarial Examples.\nIn the real physical world, the environment of autonomous\ndriving is more complex, so there are more challenges for\nattacker to generate adversarial examples in the physical-\nworld[128]. The requirements of adversarial examples in the\nphysical world are as follows.\n• Physical generatible. In the physical world, it is in-\nsuﬃcient that only adding perturbations via cyber space.\nSuch perturbations must be capable of being physi-\ncally generated by printing, 3D printing or spraying,\netc.\n• Local generatible. The local nature of the adversar-\nial example. In an adversarial example of the digital\nworld, the attacker can add perturbations to any pixel\nwithin the range of the image; however, in a physi-\ncal world attack, there are often only local areas of\nthe target that are available, and in many cases, the\nbackground areas of the image are diﬃcult to use to\ngenerate a physical world adversarial example.\n• Robustness. In the physical world, especially in the\nﬁeld of autonomous driving, it is often required that\nthe adversarial examples can continuously produce mis-\nleading eﬀects on machine learning models during mul-\ntiple angle changes. At the same time, the adversar-\nial examples need to be continuously eﬀective against\nmainstream target recognition algorithms under cer-\ntain distance and angle ranges, multiple natural en-\nvironments, and multiple resolution sensor devices.\nThis puts forward higher requirements on the persis-\ntence and universality of adversarial examples, which\ngreatly increases the complexity of the generation of\nadversarial examples.\nTherefore, more adversarial example robustness enhance-\nment measures are often required to realize the physical world\nadversarial example attacks; and the speciﬁc measures ap-\nplied to vary according to diﬀerent scenarios and attack tar-\ngets. In accordance with the scenarios and objects for which\ncountermeasures are set, this paper divides the recognition\ntargets in the autonomous driving system into three cate-\ngories: vehicles including various motor vehicles, pedestri-\nans such as walkers and cyclists, and static targets like road\nfacilities, traﬃc signs, markings, roadside advertising signs\nand other static objects.\nThus, at present, the physical world in the autonomous\ndriving ﬁeld universally has 3 types of targets for adversarial\nexample techniques: static objective recognition, pedestrian\nrecognition, and vehicle recognition.\n• Physical world adversarial examples for static targets.\nSuch attack targets include a variety of static target\nrecognition systems such as traﬃc signs, traﬃc sig-\nnals, and traﬃc markings, which are characterized by\nthe requirement that the adversarial examples can con-\ntinuously and steadily interfere with the judgments of\nmachine learning models over a large range of angles\nand distances. In 2017, Lu et al.[129] successfully\nperformed adversarial example generation in the phys-\nical world for the popular objective recognition algo-\nrithm FasterRCNN. To achieve better distance and an-\ngle range adaptation, in 2018, Eykholt et al.[130] pro-\nposed the Robust Physical Perturbations (RP2) algo-\nrithm. In the same year, Chen et al.[131] adopted the\nExpectation over Transformation (EoT) method[132]\nto improve the generation of adversarial examples for\ntraﬃc signs, resulting in improved adaptability of the\nadversarial examples to distance, angle, light and other\nenvironments. In 2019, Zhao et al.[133] proposed the\nfeature-interference reinforcement (FIR) algorithm and\nthe realistic constraints generation (ERG) algorithm to\nenhance the robustness of the adversarial examples.\nAt the same time, they proposed the nested-AE algo-\nrithm to improve the adaptability of the adversarial ex-\namples to long and short distances. Finally, the com-\nposite scheme is able to success attack against popular\nobjective recognition algorithms, such as YOLO v3\nand Faster-RCNN, within ±60◦angle and [1푚, 25푚]\ndistance range. Based on the above methods, hiding\nattacks and appearing attacks can be carried out.\nHui Cao et al.: Preprint submitted to Elsevier\nPage 7 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nThe hiding attack is to paste an adversarial example\non a normal traﬃc sign to make the target recognition\nsystem fail to recognize the traﬃc sign. The appear-\ning attack to paste an adversarial example on other ob-\njects, causing the target recognition system to recog-\nnize the object as a characteristic traﬃc sign or make a\nfalse recognition. In 2020, Kong et al.[134] proposed\nPhysGAN, a physical world adversarial examples at-\ntack method based on adversarial example generative\nnetworks, and the generated adversarial examples of\nadvertising signs have better robustness and invisibil-\nity.\n• A physical world adversarial example for pedestrian\nrecognition. In autonomous driving system, a missed\ndetection of pedestrians by the objective recognition\nsystem can have serious consequences. In 2020, Wu[135]\nproposed the \"invisibility cloak\" algorithm, where pedes-\ntrians are not normally detected by Yolo v2 and Yolo\nv3 objective detection models when wearing sprayed\nadversarial example clothing. In the same year, Wang\net al.[136] conducted a similar study.\n• Physical world adversarial examples for vehicle recog-\nnition. These examples are often pasted or painted\non the vehicle body with a speciﬁc pattern, so that\nthe vehicle detection system can not identify the ve-\nhicle or mistakenly identify the vehicle as other ob-\njects. Such physical world adversarial examples try to\nmaintain attack eﬀectiveness under high speed move-\nment, various light and other external conditions, es-\npecially in the 360-degree view and within the detec-\ntion range of vehicle identiﬁcation system, which puts\nforward higher requirements for the robustness of ad-\nversarial samples. At the same time, at diﬀerent an-\ngles, the camera may only be able to acquire part of\nthe images of he adversarial example in the vehicle\nbody, which in turn places a local requirement on the\nadversarial example, i.e., part of the adversarial ex-\nample can also achieve the attack. In 2019, Zhang\net al.[137] proposed a vehicle body painting method\nof black box adversarial example based on transition\nmodels so that the example vehicle could not be iden-\ntiﬁed by autonomous driving vehicle detection sys-\ntem. In 2020, Wu et al.[135] proposed the discrete\nsearching algorithm to eﬃciently generate adversar-\nial patches, and then proposed the Enlarge-and-Repeat\n(ER) algorithm to extend the adversarial patches to the\nwhole body using body images collected from all an-\ngles. Both are with pretty good adversarial results.\nThe following highlights the key algorithms in the physi-\ncal world adversarial example enhancement described above.\nAlgorithm1. Expectation over Transformation (EoT)\n[132]\nThe core idea of the EoT algorithm is to add a certain\nrandom perturbation to each iteration of the adversarial ex-\nample generation process, so that the ﬁnal generated adver-\nsarial example has better robustness, with speciﬁc transfor-\nmations including: projecting, rotation, and scaling. In the\nformula, the operation 푀푡(푥푏, 푥표) is deﬁned to project the\ntarget image 푥표onto the background image 푥푏through some\ntransformation 푡, and then the EoT is optimized as follows.\n̂푝= arg\nmin\n푥′∈ℝℎ×푤×3 피푥∼푋,푡∼푇[퐿(퐹(푀푡(푥, 푡푎푛ℎ(푥′))]\n+ 푐⋅‖‖푡푎푛ℎ(푥′) −푥‖‖\n(3)\nwhere 푋denote the training set of background images,\nand 퐹denote the target network.\nAlgorithm2. Adversarial Patch [138]\nBased on the EoT transform, the attacker could generate\nan adversarial patch ̂푝which the image with the this adver-\nsarial patch attacked by adversarial examples. The adversar-\nial patch can be any shape, transformed by EoT methods such\nas random projecting, rotation, and scaling, and then gener-\nated by optimization methods such as gradient descent. An\nadversarial patch generation task 퐴(푝, 푥, 푙, 푡) can be formally\ndescribed as: for any particular 푥∈ℝ푤× ℎ×푐to generate ad-\nversarial patch 푝through the EoT transformation 푡at posi-\ntion 푙, then the adversarial path 푝is continuously optimized\nby the following optimization algorithm:\n̂푝= arg\nmax\n푥∈ℝℎ×푤×푐피푥∼푋,푡∼푇,푙∼퐿[log 푃푟( ̂푦|퐴(푝, 푥, 푙, 푡))] (4)\nwhere 푋denotes the training set of background images,\n푇denotes the distribution of the EoT transform used by the\npatch, and 퐿denotes the distribution of the locations of the\nadversarial patches.\nAlgorithm3. Feature-inference Reinforcement[133]\nAdversarial example generation algorithms often require\nan objective function, or called it as loss function, designed\nto minimize the diﬀerence between the predicted and ex-\npected values of a deep learning model. The neural net-\nwork extracts features of the objects in the image and makes\nclassiﬁcation predictions based on these extracted features.\nThe researchers found that generating adversarial examples\nwith perturbations further forward in the hidden layer in the\nneural network would make the adversarial examples more\nrobust. The core idea of the FIR algorithm is therefore to\nminimize the diﬀerence between the feature images of the\nlayers of the adversarial examples and the normal examples,\nexcept for the use of the adversarial examples to mislead the\nprediction results of the neural network.\nFirst, attacker obtain the feature images 푄푛and 푄′\n푛gen-\nerated by each hidden layer of the neural network for cor-\nresponding categories 푦and 푦′. Next, generate the feature\nvectors 푣with 푣′ from the feature images 푄푛and 푄′\n푛. Fi-\nnally, attacker optimize with the a composited loss function\nuntil convergence, then return the adversarial example. Such\na vector loss can be deﬁned as 퐿표푠푠푓= ∑||푣−푣′||. The FIR\nalgorithm can be described formally as follows.\nwhere 퐶푏표푥\n푁\ndenotes the conﬁdence level of the target de-\ntection system for the target area; 푦푁denotes the conﬁdence\nHui Cao et al.: Preprint submitted to Elsevier\nPage 8 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nAlgorithm: 3. Feature-inference Reinforcement\nInput: normal example 푥, target objective detection\nmodel 푓\nOutput: adversarial example 푥′\n1 while until convergence do\n2\nforeach 푖th hidden layer in 푓do\n3\n푞푖←푓(푥) //get feature map 푞푖of normal\nexamples 푥with prediction 푦;\n4\n푞′\n푖←푓(푥′) // get feature map 푞′\n푖of\nadversarial examples 푥′ with prediction 푦′ ;\n5\nend\n6\n푣←푄∶{푞1, 푞2, ..., 푞푛\n} // generate normal\nfeature vector from feature maps\n7\n푣′ ←푄′ ∶{푞′\n1, 푞′\n2, ..., 푞′\n푛\n} // generate adversarial\nfeature vector from feature maps\n8\nOptimize 푥′ to minimize the loss function:\n훼퐶푏표푥\n푁\n+ 훽푝푁(푦푁|푆) + 푐(푙표푠푠푓)−1\n9 end\nlevel that example 푥is judged by the neural network to be\nclass 푁; 푆denotes the distribution space of conﬁdence lev-\nels; 푙표푠푠푓denotes the vector loss; 훼, 훽and 푐are three con-\nstants representing the weights respectively.\nAlgorithm4. Nested adversarial examples (Nested-\nAE)[133]\nMost objective detectors were designed to use multi-scales,\neach works better at diﬀerent distances individually. It means\nthat at diﬀerent distances, diﬀerent scales play diﬀerent roles.\nIn order to make the generated adversarial examples achieve\nadversarial eﬀects over a large distance range and multiple\nangles, the Nested-AE algorithm considers many scales for\ndiﬀerent distances and angles to generate many adversarial\npatches. Then Nested-AE obtained the adversarial patches\nto synthesize an adversarial example. Thus nested adver-\nsarial examples are more adapted to the environment of ve-\nhicle movement in autonomous driving, which can achieve\nthe purpose of a continuous adversary eﬀect on the objective\ndetection system. The nested adversarial examples could be\nformally described as\n푋푎푑푣\n푖+1 = 퐶푙푖푝\n{\n푋푖+ 휀푠푖푔푛(퐽(푋푖)),\n푆푃≤푆푡ℎ푟푒푠\n푋푖+ 휀푀푐푒푛푡푒푟푠푖푔푛(퐽(푋푖)),\n푆푃> 푆푡ℎ푟푒푠\n}\n(5)\nwhere 푋푖denotes the original example, 푋푎푑푣\n푖+1 denotes\nthe example with adversarial perturbation, 퐽() represents the\ngradient of input 푋푖and 퐶푙푖푝() refers to regularize the input\nto the [0, 255] interval.\nIf the adversarial example size 푆푝is less than or equal to\nthe threshold 푆푡ℎ푟푒푠, then it is considered a long-range ad-\nversarial attack at that point and the whole will be perturbed\n; otherwise it is a close-range attack and only the central re-\ngion will be perturbed. This decomposes the adversarial at-\ntack at diﬀerent distances into two sub-tasks, which are per-\nturbed and optimized separately.\nThe objective detection systems usually divides each video\nframe into a grid that consisted of 푚× 푛boxes. Base on the\nprediction of each box, attacker could ﬁnd the decisive box\nfor each scales, and then add the adversarial perturbation in\nthis box. As the predicted output is usually tensors, there are\nonly need the tensor of the index where the adversarial ex-\nample is located. Therefore, it can calculate the index by the\nsize of the example and the position of the centre region, so\nthat we can get the tensor representing the example region,\nwhich is denoted as 푁푝. Then 푁푝needs to be calculated in\neach video frame. The nested adversarial example algorithm\ncan be optimized using the following loss function.\n푁푝= 푓(푝푠푖푧푒, 푃푝표푠푖푡푖표푛), 1−퐶푏표푥\n푁푝+훽\n∑|||푝푁푝, 푗−푦푗|||\n2 (6)\n3.2. Physical World Adversarial Example for\nPedestrian Detection\nPedestrian detection often places high demands on the\nuniversality, portability, robustness and feasibility of the ad-\nversarial examples. To due this problem, some algorithms\nwas proposed. As a typical example, Invisibility Cloak algorithms[135]\ncan generate adversarial examples in the physical world, so\nthat pedestrians wearing clothes painted with speciﬁc adver-\nsarial examples cannot be recognized properly. If the similar\nmethod be used in pedestrian, serious consequences of au-\ntonomous driving system will be lead.\nAlgorithm 5. Invisibility Cloak[135]\nThe policy of the invisibility cloak algorithm is to use\na large number of images containing people to train against\npatches; in each iteration, a random batch of images is se-\nlected and sent to the objective detection system to obtain\nthe bounding box of people. The critical idea is that place a\nrandomly transformed patch on each detected person so that\nthe score that feature images are detected to have people’s\npresence is minimized.\nThe patch 푃∈ℝ푤×ℎ×3 is projected to the target image\n퐼by transformation function 푅휃which performs data aug-\nmentation for lighting, contrast changes, distortion, with 휃\nas the parameter, in addition to scaling the patch to ﬁt the\nsize of image 퐼.\nAdditionally, two eﬀective advanced methods for physi-\ncal world attack were proposed as auxiliary. One was called\nTotal-Variation (TV) loss. Add a TV penalty function to\nthe patch to make adversarial example smoother to improve\nrobustness of physical world adversarial examples.\nwith the ﬁnal loss function as:\n퐿표푏푗(푃) = 푙표푠푠+ 훾⋅푇푉(푃)\n(7)\nAnother is Ensemble training. In the black-box case,\nthe attacker cannot obtain the gradient of the target detection\nmodel under attack, so a possible solution is to collect mul-\ntiple similar white-box models for ensemble training. The\nHui Cao et al.: Preprint submitted to Elsevier\nPage 9 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nloss function is\n퐿푒푛푠(푃) = 휃, 핀\n∑\n푖,푗\n{\n푆(푗)\n푖(푅휃(퐼, 푃)) + 1, 0\n}\n(8)\nwhere 푆(푗) represents the objective detection model of\ntarget 푗. It is formally described as follows.\nAlgorithm: 5. Invisibility Cloak\nInput: normal example 푥, target objective detection\nmodel 푓\nOutput: adversarial example 푥′\n1 퐼= 푃(푥) // generate projection\n2 ;\n3 푅휃←휃// deﬁne a transfer function 푅휃\n4 ;\n5 while until convergence do\n6\nOptimize 푥′ to minimize the loss function:\n퐿표푏푗(푃) = 푅휃(퐼, 푃)\n7 end\n8 or Auxiliary1: TV Loss\n퐿표푏푗(푃) = 푅휃(퐼, 푃) + 훾⋅푇푉(푃)\n9 or Auxiliary2: Ensemble training\n퐿표푏푗(푃) = 피휃,퐼\n∑\n푖\nmax {푆푖(푅휃(퐼, 푃)) + 1, 0}2\n3.3. Physical World Adversarial Example for\nVehicle Detection\nThe aim of the adversarial examples for vehicle detec-\ntion is evade or mislead other vehicle detector by adversar-\nial spraying. Diﬀerent from other scenarios, the challenge\nis that the spraying need works for the detector at all angles.\nDiscrete Searching is a typical and eﬀect algorithm.\nAlgorithm 6. Discrete Searching[139]\nThe discrete searching algorithm is essentially a black-\nbox adversarial example generation algorithm based on a\ngenetic algorithm that continuously optimizes the adversar-\nial examples through mutation and selection. The discrete\nsearching algorithm that mutation-based search method de-\nﬁnes two mutation strategies. One is random mutation, where\na point within the circle with 휖as the radius, is randomly se-\nlected as the direction and advanced a step length as the new\nmutation point. If the current mutation optimization fails to\noutperform the original one, the choice will be made to con-\ntinue with the random mutation. The other is directed muta-\ntion, in which the candidate’s best mutation point is selected\nwithin a particular angular expansion of the current direc-\ntion, advancing a random step length. If the current varia-\ntion outperforms the original one, the choice will be made\nto continue with the directed mutation.\nAlgorithm: 6. Discrete Searching\nInput: normal example 푥, target objective detection\nmodel 푓\nOutput: adversarial example 푥′\n1 while until convergence do\n2\nforeach 푖∈{0, ..., 푁푎−1} do\n3\n퐶푗푤\n푖,\n퐶푗푤\n푖\n= 퐶푙푖푝(퐶푖+푅푎푛푑표푚(퐻, 푊, 3)⋅휖1 ⋅훿)\n//generate candidate point\n4\nselect ̂퐶푖in 퐶푗푤\n푖\n5\nif ̂퐶푖is better than 퐶푖then\n6\n퐶푖= ̂퐶푖\n7\nelse\n8\n퐶푖remains unchanged\n9\nend\n10\nend\n11 end\n3.4. Adversarial Examples on LiDAR and RaDAR\nAdversarial examples are related to the characteristics of\ndeep learning itself, and both RGB image-based and LiDAR-\nor RaDAR-based objective recognition systems are likely\nto suﬀer from adversarial examples attacks. In 2019, Cao\net al.[140] proposed an adversarial examples attack method\nfor LiDAR target recognition. The attacker emits a small\namount of perturbation laser at the LiDAR system, which\nled to a small perturbation in the imaging of the LiDAR sys-\ntem, and such perturbation made the LiDAR-based 3D ob-\njective recognition erroneous. Then, an adversarial object\nattack method LiDAR-Adv was proposed for LiDAR objec-\ntive recognition, in which the attacker could construct a cer-\ntain special shape of objects, causing the LiDAR objective\nrecognition system to mispredict special objects. Such an\nattack can be targeted, i.e., the real object is recognized as\nthe speciﬁed by the attacker, so this is more easier to exploit\nfor the attacker. As the ﬁgure shows, some specially shaped\nobjects can be misidentiﬁed as \"pedestrians\" by the 3D ob-\njective recognition system, while others are not recognized\nproperly by the 3D objective recognition system, posing a\nsecurity risk. The above method was successfully tested on\nBaidu’s autonomous driving system Apollo. In 2020, after\nresearch and improvement, SUN et al.[141] implemented a\nblack-box attack of the above method, which was success-\nfully tested on Intel’s autonomous driving simulation sys-\ntem Carla. In real autonomous driving environments, the\ndetection of dynamic targets often takes a multi-modal fu-\nsion of RGB images, LiDAR, and RaDAR for target recog-\nnition, which improves the robustness of the system to some\nextent.\nHigher demands are placed on the adversarial example\nfor multi-modal environments, mainly in terms of:\n• Adversarial perturbation needs to be able to be physi-\ncally generated in both the RGB image and the LiDAR\nsystem environment. Traditional RGB adversarial ex-\nHui Cao et al.: Preprint submitted to Elsevier\nPage 10 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nFigure 4: Adversarial Examples on LiDAR[140]\namples usually change the RGB values of some pixels\nin the image, however, this method cannot works on\nthe 3D cloud point map generated from the LiDAR.\nOn another hand, emit a speciﬁc adversarial laser at\nLiDAR can interfere with the LiDAR system, but it is\nalso diﬃcult to eﬀectively inﬂuence the RGB objec-\ntive recognition system.\n• The adversarial examples need to be able to physically\nand continuously work on both the RGB-based system\nand the LiDAR-based system. In a real vehicle envi-\nronment, the RGB system and the LiDAR system need\nto attack successfully and continuously at a long dis-\ntance and at diﬀerent angles.\n• Adversarial examples need to be able to adapt to dif-\nferent data preprocessing between RGB-based systems\nand LiDAR-based systems. RGB image acquisition\nsystem and LiDAR data acquisition system both have\ncertain data preprocessing, which will have impact on\nadversarial examples. The algorithm of adversarial\nexamples generation needs to have strong robustness\nto diﬀerent preprocessing.\nMeanwhile, after optimized by some speciﬁc algorithms ,\nthe threat to the fused objective recognition system by ad-\nversarial examples is still exists. In 2021, Cao et al.[142]\nproposed the MSF-ADV method with LiDAR and RGB im-\nage fusion environment as an example, and successfully im-\nplemented the physical world adversarial examples.\nAlgorithm 7. MSF-ADV Algorithm[142]\nTo accommodate the above challenges, MSF-ADV ﬁrst\ngenerates 3D objects of diﬀerent shapes so that they can si-\nmultaneously aﬀect the LiDAR-based 3D point cloud imag-\ning and also the RGB colour values of the pixels in the RGB\nimage. Secondly, MSF-ADV uses an optimization algorithm\nto generate the 3D shapes with the best adversarial eﬀect. Fi-\nnally, MSF-ADV uses a 3D printer for physical generation.\nThe loss function of the optimization algorithm can be de-\nscribed as\n푚푖푛푆푎피푡∼푇[푎(푡(푆푎); 푙, 푐, , )+휆⋅푟(푆푎, 푆)] (9)\nwhere 푆denotes the original examples the 푆푎denotes\nthe adversarial examples, s denotes the fusion algorithm,\nand 푐is the derivative projection function used to represent\nRGB image based prediction. 푙is the derivative projection\nfunction used to represent the LiDAR prediction. is the\nultimate output of the objective recognition system.\nThrough thus optimization, the trade-oﬀbetween RGB\ncolour and LiDAR shape was found.\n3.5. Adversarial Examples on Object Tracking &\nTrajectory Prediction\nUsually, autonomous driving system relies on object track-\ning and trajectory prediction, to determine and predict target\nstates, and to support driving decisions. Object trajectory\ntracking can be divided into Single-Object Tracking (SOT)\nand Multi-Object Tracking (MOT). With the application of\nobject tracking in critical cyber systems, adversarial exam-\nples attacks on it are also rising. Among them, the main pur-\npose of the adversarial examples attack on SOT is to achieve\nobjective evasion. In 2020, Chen et al.[143] proposed the\none-shot adversarial attack, which only adds a weak pertur-\nbation to the initial frame in the video, and the tracked object\nmay not be able to track the trajectory in subsequent frames.\nIn the same year, Yan et al.[144] proposed the cooling-shrinking\nattack, which perturbs the object search area by adding spe-\nciﬁc adversarial noises, so that the tracker cannot identify the\nobject and interrupted the trajectory tracking. In the next\nyear, Jia et al.[145] proposed the IoU Attack, the idea of\nwhich is to reduce the fractional diﬀerence between the nor-\nmal object border and the adversarial object border in object\ntracking, thus enabling the trajectory oﬀset using SOT sys-\ntem.\nThe autonomous driving system more often uses MOT\nsystems. An adversarial example attack on MOT may achieve\nboth evasion and object obfuscation. In 2020, Jia et al.[146]\ngenerated an adversarial example on an autonomous driving\nobject tracking system that minutely deviation the normal\ntarget identiﬁcation bounding box of the attacked target in a\nspeciﬁc direction, causing the tracker to assign the wrong ve-\nlocity to the attacked trajectory, resulting in the target track-\ning system not being able to associate with the target prop-\nerly, thus achieving an escape attack. In 2021, Lin et al.[147]\nproposed a new adversarial example scheme that mainly uses\nthe \"PullPush Loss\" algorithm and \"Center Leaping\" algo-\nrithm. The scheme leads to object tracking system confuses,\nwhen objects cross each other.\nAlgorithm 8. Push-Pull Loss [147]\nA video 푉consists of a series of frames, which can be\nmarked as 푉= {퐼1, 퐼2, ..., 퐼푁\n}, The trajectories of target푖\nand 푗are respectively 푇푖=\n{\n푂푖\n푠푖, ..., 푂푖\n푡, ..., 푂퐸푖\n푖\n}\nand 푇푗=\n{\n푂푗\n푠푗, ..., 푂푗\n푡, ..., 푂퐸푗\n푗\n}\nThe attacker’s target generates a series of adversarial frames\n̂푉, as ̂푉= {퐼1, ..., 퐼푡−1, ̂퐼푡, ..., ̂퐼푡+푛−1, 퐼푡+푛, ..., 퐼푁\n}, such that\nfrom the moment of time 푡, an adversarial misdirection of\nthe trajectory of 푖and 푗occurs. Then there is the formula,\nHui Cao et al.: Preprint submitted to Elsevier\nPage 11 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\n̂푇푖=\n{\n푂푖\n푠푖, ..., 푂푖\n푡−1, 푂푖\n푡, ..., 푂푗\n푡+푛−1, 푂푗\n푡+푛, ..., 푂푗\n푒푗\n}\nwhere 푂푖\n푡\nindicates the target identiﬁed as 푖at the moment of time 푡.\nUse the PushPull loss function to optimize and realize:\n퐿푝푢푙푙푝푢푠ℎ(푎푖\n푡−1, 푎푗\n푡−1, 푓푒푎푡푖\n푡, 푓푒푎푡푗\n푡)\n=\n∑\n푘∈{푖,푗}\n푑푓푒푎푡(푎푘\n푡−1, 푓푒푎푡̃푘\n푡) −푑푓푒푎푡(푎푘\n푡−1, 푓푒푎푡푘\n푡) (10)\nwhere 푑푓푒푎푡() denotes the cosine distance and 푎푖\n푡−1 and\n푎푗\n푡−1 represent the trajectory features of object 푖and 푗while\n푓푒푎푡푖\n푡and denotes the features of object 푖and object 푗. Af-\nter continuous optimization, it make the adversarial feature\n푓푒푎푡푗\n푡instead of 푓푒푎푡푗\n푡be classiﬁed as trajectory 푘.\nFigure 5: Push-pull Loss Function [147]\nAlgorithm 9. Center Leaping [147]\nWith the PushPull loss function optimization described\nabove, it is able to make the object tracking misled against\nattacks, which is still diﬃcult to succeed when the diﬀer-\nence between the two objects is large. The idea of the Cen-\nter Leaping algorithm is to ﬁrst mislead the objective recog-\nnition link so that the objective candidate box identiﬁed by\nthe target recognition system is shifted towards the target to\nbe misled, thus achieving a better attack success rate when\nthere is a large deviation in the distance and size diﬀerence\nbetween the two tracked objects. The loss function is\n퐿= 푚푖푛\n∑\n푘∈{푖,푗}\n푑푏표푥(퐾(푚̃푘\n푡−1, 푏표푥푘\n푡))\n= 푚푖푛\n∑\n푘∈{푖,푗}\n푑(푐푒푛푡(퐾(푚̃푘\n푡−1, 푏표푥푘\n푡))\n+ 푚푖푛\n∑\n푘∈{푖,푗}\n푑(푠푖푧푒(퐾(푚̃푘\n푡−1, 푏표푥푘\n푡))\n+ 푚푖푛\n∑\n푘∈{푖,푗}\n푑(표푓푓(퐾(푚̃푘\n푡−1, 푏표푥푘\n푡)))\n(11)\nOf which 푚푘\n푡and 푏표푥푘\n푡respectively represent the trajec-\ntory state and the candidate frame of target 푘at time 푡; 푐푒푛푡(),\n푠푖푧푒(), and 표푓푓() respectively represent the centre point po-\nsition, the size, and the oﬀset of the candidate box; and 푑()\nrepresents distance 퐿1.\nThe central leaping algorithm can be expressed as\n퐿푐푙=\n∑\n푘∈{푖,푗}\n(\n∑\n(푥,푦)∈퐵푐−>̃푘\n(1 −푀훾\n푥,푦푙표푔(푀푥,푦)+\n∑\n(푥,푦)∈퐵푐−>푘\n(푀훾\n푥,푦푙표푔(1 −푀푃푥, 푦)))\n(12)\n푀(푥, 푦) denotes the heat value of (푥, 푦), 푐푘denotes the\ncenter of the object, 푐→̃푘denotes the direction from 푐푘to\n푐푒푛푡(퐾(푚̃푘\n푡−1)). During the optimization process, the center\npoint will move to the adjacent grid along this direction. In\nthe object recognition and tracking system, the heat value of\nthe original target center will drop, and the heat value in the\ndirection close to the object will rise, so as to achieve the\ngoal of the candidate frame approaching the object.\nSimilarly, it is able to integrate the size loss function and\nthe oﬀset loss function into a A novel composite loss func-\ntion.\n퐿푟푒푔= 퐿푠푖푧푒+ 퐿표푓푓\n=\n∑\n푘∈{푖,푗}\n퐿푠푚표표푡ℎ\n1\n(푠푖푧푒(퐾(푚̃푘\n푡−1), 푠푖푧푒(푏표푥푘\n푡))\n+\n∑\n푘∈{푖,푗}\n퐿푠푚표표푡ℎ\n1\n(표푓푓(퐾(푚̃푘\n푡−1), 표푓푓(푏표푥푘\n푡))\n(13)\nwhere 퐿푠푚표표푡ℎ\n1\nis smooth loss function based on 퐿1:\n퐿푠푚표표푡ℎ\n1\n(푎, 푏) =\n{\n0.5 ⋅(푎−푏)2\n푖푓|푎−푏| < 1\n|푎−푏| −0.5\n푒푙푠푒\n(14)\n3.6. AI Backdoor & Poisoning on ADS\nArtiﬁcial intelligence models are often generated from a\ncertain amount of training data. Some scholars have found\nthat if the training data is not trustworthy, it may lead to the\ngeneration of models with \"backdoor\", which can be ma-\nnipulated by attackers in the subsequent use of the models,\ncausing serious security risks. Currently, the concepts of\n\"AI backdoor\", \"AI model poisoning\" and \"AI Trojan horse\"\nhave some similarities, but are not expressed in the same way\nin diﬀerent literature. One type of attack is called Training-\nonly attacks, or Poisoning Attacks are usually deﬁned as at-\ntacker contaminating part of the training data or modifying\nthe labels of the training data. On the contrary, another type\nof attack is called Backdoor Attacks or AI Trojans[13], in\nwhich attackers must participate in both training and testing.\nBoth poisoning attacks and AI backdoor attacks can cause\nserious security threats to autonomous driving system, and\nthis paper refers to these two types of attacks as \"AI backdoor\nattacks\", where a speciﬁc malicious modiﬁcation is made to\na target model in a speciﬁc way, causing the model to make\nharmful judgments about a speciﬁcally predicted example.\nThere are similarities and diﬀerences between backdoor at-\ntacks and adversarial example attacks. Adversarial examples\nusually do not change the model itself and will not damage\nHui Cao et al.: Preprint submitted to Elsevier\nPage 12 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nFigure 6: Centre Leaping Principle[147]\nthe integrity of the AI model, but mainly interfere with the\ntest examples and aﬀect the availability and correctness of\nthe machine learning model. On another hand, AI backdoor\ntakes the form of modiﬁcation of the AI model, poison of the\ntraining data, aggregation of the backdoor model, etc., caus-\ning tiny changes to the AI model, aﬀecting both the integrity\nof the AI model. AI backdoor attacks tend to be more hid-\nden, highly universal, and more damaging. So far, there are\ntwo major methods to implement AI backdoor, one is Data\nPoisoning Attack, and the other is Model Poisoning Attack.\nData Poisoning Attack means attacker adds a small amount\nof poisoning data into training dataset, so that the resulting\nAI model has a backdoor, and the AI model may make a\nspeciﬁc judgement when the predicted example contains a\n\"trigger\". Model Poisoning Attack means the attacker di-\nrectly modiﬁes the model or indirectly fuses the target model\nwith a harmful model by using model integration, federated\nlearning, and transition learning, causing the model to make\na directed and erroneous judgment on a speciﬁc prediction\nexample.\nIt has been argued that AI backdoor attacks already ex-\nist in traditional machine learning. In 2008, Nelson et al.\n[148, 149] proposed the backdoor attack on Bayesian net-\nworks. In 2012 Biggio et al.[150] proposed backdoor attack\non SVMs. In 2016, Alfeld et al.[151] proposed backdoor on\nauto-regressive prediction models. In 2017, Gu et al.[3] ﬁrst\nproposed backdoor attack on deep learning, then AI back-\ndoor became a promising research topic. The BadNet al-\ngorithm adds a small number of training data with the pre-\ndesigned pattern into the training data and labels such train-\ning examples with a speciﬁc target, then the trained model\nis likely to predict examples with \"Trigger\" according to the\nattacker. In the same year, Muñoz-González et al.[152] pro-\nposed a gradient-based algorithm for AI data poisoning. How-\never, for autonomous driving system, the basic AI backdoor\nalgorithms described above have two limitations.\n• Control right of training data by attacker.As it requires\nthe attacker to be able to contaminate a certain amount\nof training data, this requires the attacker to have some\ncontrol over the training data; at the very least the at-\ntacker needs to have background knowledge of the tar-\nget model’s structure, parameters, etc., which places\ncertain requirements on the attacker.\n• Concealment of attack. It relies on contaminating part\nof the training data by adding a ’pattern’ or changing\nthe label of the data. Although the pattern may be rela-\ntively insidious, forcing patterns into normal examples\nmay cause a certain amount of unnaturalness that may\nbe detected by humans, or possibly by automated de-\ntection through some anomaly identiﬁcation method.\nIt may also lead to human feel a sense of inharmonious\nif the attacker modiﬁes excessive label of the training\ndata.\nIn response to these limitations, researchers have made a\nnumber of subsequent improvements. On the one hand, at-\ntackers have improved the concealment of poisoning attacks\nby enhancing the concealment of the patterns in the exam-\nple or minimizing the impact on the integrity of the label.\nOne of the research directions is \"clean label\", which aims\nto keep the label of poisoned example semantically correct\nwhile realizing data poisoning. In 2018, Shafahi et al.[153]\nproposed the Poison Frogs algorithm, which was the ﬁrst to\nimplement the Clean Label attack for deep learning. In the\nsame year, Truner et al.[154] proposed two methods of data\ngeneration based on adversarial network and adversarial ex-\nample to achieve a label-consistent \"clean example\" attack.\nAnother research direction is \"Hidden Trigger\", also known\nas \"Invisible Trigger\", which aims to optimize the trigger\npattern to make it as invisible as possible to escape detec-\ntion by humans and machines. In 2018, Suciu et al.[155]\n, and in 2019 Saha et al.[156] put forward \"hidden trigger\"\nwhich can generate trigger patterns that humans are unable\nto directly perceive through the senses. In 2020, Wallace et\nal.[157] devised a \"hidden trigger\" poisoning attack in the\nﬁeld of natural language processing. In the same year, Li et\nal.[158] used information hiding and regularization methods\nto improve the bad net algorithm to improve the invisibility\nof the trigger pattern.\nOn the other hand, the attacker reduces the proportion\nof contaminated training data as much as possible, or even\nimplements a black-box attack that does not require contam-\ninating data, thus reducing the background knowledge re-\nquired for the attack and lowering the threshold for imple-\nmenting the attack. In 2017, Liu et al. [159] implemented a\nblack-box approach to generate backdoor by exploiting the\nmigratory nature of the attack, but such backdoor mainly ex-\nists in the fully connected layer at the end of the AI model,\nHui Cao et al.: Preprint submitted to Elsevier\nPage 13 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nwhich can easily fail once the model is ﬁne-tuned; in the\nsame year, Chen et al.[38] proposed a machine learning-\nbased approach to generate AI backdoor, which eliminates\nthe need for attackers to understand the structure of the target\nsystem and other information, and reduces the background\nknowledge requirement; in 2019, Yao et al.[160] proposed\n\"latent triggers\", which are ﬁrst generated in the \"teacher\nmodel\" and then migrated to the \"student model\" through\ntransition learning. The backdoor is not only found in the\nlast fully connected layer of the student model, but also in\nall of its layers and thus the diﬃculty of detecting the \"back-\ndoor\" through analysis is increased. In the same year, Zhu\net al.[161] investigated the migratory nature of clean label\nattacks and used knowledge transition to realize a black-box\nclean label attack.\nIn the ﬁeld of autonomous driving, in 2018, Liu et al.[162]\nrealized AI backdoor attacks in a variety of environments, in-\ncluding simulated autonomous driving platforms. In 2019,\nRehman et al.[163] implemented an AI backdoor attack on\ntraﬃc signs in the physical world; Barni et al.[164] con-\nducted a clean label poisoning attack on traﬃc signs; Ding\net al. from Nanjing University[165] designed a \"natural trig-\nger\" for autonomous driving system to trigger AI model back-\ndoor in special weather like a rainy day, to make red lights\nincorrectly identiﬁed as green lights and numbers incorrectly\nidentiﬁed in a speciﬁc way; Yao et al.[160] from the Univer-\nsity of Chicago used their proposed \"latent trigger\" method\nto generate backdoor traﬃc signs for a variety of models,\ngenerating human-imperceptible triggers on physical traﬃc\nsigns. In 2021, Tian et al.[166] achieved a clean label attack\non 3D cloud point map. In 2022, Udeshi et al.[167] pro-\nposed an anti-backdoor attack method that can be used in\ntraﬃc sign recognition scenarios, which achieved avoiding\nAI backdoor attacks by ﬁltering the triggers in the captured\nimages and correcting the prediction examples.\nAlgorithm 9. Feature Collisions\nFeature collision is the more common method of AI back-\ndoor generation, where the attacker ﬁrst selects a target in-\nstance from the test set. To achieve poisoning, the attacker\nchooses a base class instance from the base class and makes\nimperceptible changes to it, thereby generating a poisoned\ninstance that is injected into the training data later; then, dur-\ning the training phase, the model is trained using a poisoned\ndata set consisting of a clean data set plus poisoned instance;\nin the reasoning phase, this causes the target instance to be\nmistaken by the misclassiﬁcation model for being in the base\nclass during testing. It is described formally as follows.\n푓(푥) represents the neural network’s prediction on input\nexample 푥. Example 푥colliding with the target is found in\nthe feature space and then computed to be close to the base\ninstance 푏. The target function is\n푝= 푎푟푔min ‖푓(푥) −푓(푡)‖2\n2 + 훽‖푥−푏‖2\n2\n(15)\n푝is the poisoning instance, which will be misdirected as\nthe attack target.\n3.7. Summary\nThe risk of adversarial example and AI backdoor is brought\nby the characteristics of deep learning itself. Whether an au-\ntonomous driving system uses RGB cameras, LiDAR, RaDAR\nor other sensors as the source of information collection, it\noften dependent on deep learning for perception and driv-\ning decisions. Going with it, there are new safety risks as-\nsociated with artiﬁcial intelligence. At the same time, the\nautonomous driving system is a huge system, and in the per-\nception layer alone, they consist of many links that rely on\ndeep learning technologies, such as target recognition, image\nsegmentation, depth estimation and target tracking, which\nconstitute a complex decision-making process, and each link\nis also subject to diﬀerent types of AI security threats. It is\nnecessary to ensure the safety of each link to constitute a set\nof safe autonomous driving system.\nGenerated adversarial examples are hard to consistently\nfool neural network classiﬁers in the physical world. In this\nchapter, we introduced emphatically method of physical world\nadversarial examples enhancement, summarized it in Table\n2.\nAs the foundation, some major general adversarial ex-\namples algorithms list in Appendix A.\n4. Emerging Threats of Decision-Making\nLayer\nThe major function of the decision layer is to make the\ncorrect driving decision based on sensing and perception.\nIn common autonomous driving architectures, the trajectory\nof dynamic objects, such as vehicles or pedestrians, must\npredicted. If the prediction process is maliciously interfered\nwith by an attacker, vehicle may under security threat.\n4.1. Emerging Threat on Prediction-oriented\nattack techniques\nIn general, autonomous driving systems need to predict\nthe short-term or long-term spatial coordinates of various\nroad agents such as cars, buses, pedestrians, rickshaws, and\nanimals, etc. Predicting usually base on Recurrent Neural\nNetwork (RNN) techniques, the algorithms of which include\nLSTM, and Sequence to Sequence. Researchers have pro-\nposed attack methods for recurrent neural network algorithms.\nIn 2016, Papernot et al.[124] proposed an RNN-oriented ad-\nversarial example attack, and many subsequent researchers\nhave continued to improve the attack method and enhance\nthe attack eﬀect[172, 173, 174, 175, 176].\n4.2. Emerging Threat in Imitation Learning\nImitation Learning and Reinforcement Learning are the\ntwo main approaches to driving decision making. Imita-\ntion Learning is a data-driven approach that imitates expert\ndriver policies to make decisions[177] and some end-to-end\nautonomous driving systems use an imitation learning framework[178].\nReinforcement learning, on the other hand, uses deep re-\ninforcement learning algorithms to optimize the model and\nmake the best decisions. Whether a autonomous driving sys-\ntem adopts an imitation learning or a reinforcement learning\nHui Cao et al.: Preprint submitted to Elsevier\nPage 14 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nTable 2\nPhysical World Adversarial Examples Enhance Methods\nMethod\nContributions\nScenarios in Autonomous\nDriving\nEoT[132],\nEoT generate adversarial examples over a chosen\ndistribution of transformations.EoT is the ﬁrst algorithm\nthat produces robust adversarial examples, which single\nadversarial examples to an entire distribution of\ntransformations\nObject detection\nAdversarial Patch[138]\nThis attack generates an image-independent patch that\ncan then be placed anywhere within the ﬁeld of view of\nthe classiﬁer, and causes the classiﬁer to output a targeted\nclass.\nObject detection\nFIR[168]\nFIR generated adversarial examples to impact both hidden\nlayers and the ﬁnal layer. Therefore, the misclassiﬁcation\nfor adversarial examples depends more on the prior layers\nin the neural-network, which lead to be more robust in\nphysical scenarios.\nTraﬃc sign detection\nNested-AE[168]\nNested-AE contains two or more Adversarial examples\ninside that for diﬀerent distances or angles. It signiﬁcantly\nimprove the robustness of adversarial attack at the various\nposition.\nTraﬃc sign detection\nRandomly Transformed\nPatch[135]\nThese transforms are a composition of brightness,\ncontrast, rotation, translation, and sheering transforms\nthat help make patches robust to variations caused by\nlighting and viewing angle that occur in the real world.\nPedestrian detection\nTV Loss[169, 170, 135]\nTV Loss ensures a more smooth patch in which all pixels\nin the patch get optimized.\nObject detection\nEnsemble training[135]\nEnsemble training fool an ensemble of detectors that were\nnot used for training.\nObject detection\nUPC [171]\nUPC optimization constraint to make generated patterns\nlook natural to human observers.\nObject detection\nNPS [169]\nNPS deal with the diﬀerence in digital RGB-values and the\nability of real printers to reproduce these values.\nObject detection\nDiscrete Search[139]\nDiscrete Search improve the black-box attack by iteratively\nreﬁning the camouﬂage using a mutation-based search\nmethod.\nphysical-world Black-box\nattack\nMSF-Adv[142]\nMSF-Adv generate adversarial examples in Lidar, RaDar,\nand fusion.\nVehicle detection\nSpatial Transformer Layer\n(STL) to project[170]\nMany kinds of Projects imitate the form changes for\nrectangula adversarial patches after placing it in physical\nworld.\nObject detection\nSticker Projection[170]\nProject the obtained adversarial examples with small\nperturbations in the projection parameters to make the\nattack more robust.\nObject detection\narchitecture, an attacker could interfere with the AI model,\nthereby aﬀecting normal driving decisions to pose a risk to\nthe autonomous driving system.\nImitation learning can be described as a process[44], and\nhuman expert experience can be described as a tuple such as\n(푠, 푎, 푟, 푠′), where s is the state of driving, 푎is the behavior of\nthe human expert, 푟is the reward created by the behaviour 푎,\nand 푠′ is the resulting new state. Imitation learning generates\npolicy 휋through machine learning, based on the captured set\nof behaviour of the human experts 퐷= (푥푖, 푦푖).\n푢(푡) = 휋(푥(푡), 푡, 훼)\n푢is the predicted behavior given by the machine, 푥is the\nfeature vector of the state of the environment 푠, 푡is the time,\nand 훼is the set of parameters for a set of policies.\nImitation learning is still a branch of deep learning and\nbased on deep neural networks (DNNs), also it is equally\nthreatened by adversarial examples, AI backdoor and other\nforms of attack. In the autonomous driving ﬁeld in 2020,\nBoloor et al.[179] proposed an adversarial example genera-\ntion algorithm based on Bayesian optimization that can at-\nHui Cao et al.: Preprint submitted to Elsevier\nPage 15 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\ntack end-to-end (E2E) autonomous driving trajectory pre-\ndiction system, which was successfully experimented on In-\ntel’s Carla simulation platform. This approach sprayed spe-\ncial adversarial patterns generated with the algorithm on the\nroad, interfering with the autonomous driving system’s pre-\ndiction of its own vehicle, to induce the autonomous driv-\ning system to make a wrong driving decision. In the same\nyear, Yang et al.[180] proposed two adversarial attack algo-\nrithms for vehicle trajectory prediction, which improved the\nabove method, reducing the number of optimization rounds\nrequired for adversarial example generation and improving\neﬃciency.\nAlgorithm 10. Bayesian Optimization (BO) algorithm\nThe objective of the BO algorithm is to generate adver-\nsarial examples suitable for end-to-end autonomous driving\nsystem and ﬁnd the best adversarial perturbation through op-\ntimization 훿, with an loss function of\n훿∗= arg max\n훿\n푓(훿)\n(16)\nwhere 훿∗∈푑, assuming that the autonomous driving\nmodel 푓prediction conforms to a Gaussian process, it can\nbe written as 퐺푃(푓, 휇(훿), 푘(훿, 훿′)), and let the mathematical\nexpectation be 0, then 휇(훿) = 0, with the variance is the\nMattern covariance function 퐾.\n푘(훿, 훿′) = (1 +\n√\n5푟\n푙\n+ 5푟2\n3푙2 )푒푥푝(−\n√\n5푟\n푙\n)\n(17)\nwhere 푟denotes the Euclidean distance and 푙is a factor\ncoeﬃcient. In such a manner, we can consider is the adver-\nsarial perturbation.\nFigure 7: BO Algorithm [179]\n4.3. Emerging Threat on Reinforcement Learning\nReinforcement learning (deep reinforcement learning) is\nwidely used in ﬁelds such as autonomous decision-making,\nelectronic combat and competition. Combined with a num-\nber of other techniques such as deep search trees, deep re-\ninforcement learning has enabled AlphaGo to explore some\nof the blind spots of human cognition. Adopting reinforce-\nment learning technique for autonomous driving decisions is\none of the major technology routes in academia and indus-\ntry. Reinforcement learning security has recently received\nextensive attention and research.\nReinforcement learning can be described as a Markov\nDecision Process(MDP)[54, 44]. A ﬁnite state decision mak-\ning process consists of the tuple(푆, 퐴, 푇, 푅) where 푆is the\nset of ﬁnite states, 퐴is the possible behaviour of the system,\nand 푇is the State Transition Probabilities consisting of a set\nof probabilities 푃푠,푎. It indicates that when behaviour 푎is\ntaken, the probability of reaching state 푠is achieved, and re-\nward function 푅can return the reward value 푌which can be\nobtained by the reward policy 푅(푠푘, 푎푘, 푠푘+1) where the re-\nward value 푌denotes changing the state to state 푠푘+1 when\ntaking behaviour 푎푘at the state 푠푘. Reinforcement learning\nis the process of starting with a random policy, receiving a\nreward based on the execution of that policy, and then con-\ntinuously optimizing the policy by maximizing the reward.\nFigure 8: Adversarial Attacks in Reinforcement Learning [181]\nReinforcement learning is a process of self-optimization,\nwhere models evolve and improve, but the process is also\nthreatened by AI security risks[181, 182]. In the ﬁeld of re-\ninforcement learning, it is diﬃcult to distinguish between the\nconcepts of adversarial examples and AI backdoor, which\nare collectively referred to as \"adversarial attacks\". Based\non attack paths, Kiourti et al.[183] categorize the attacks\nagainst reinforcement learning into environmental adversar-\nial attacks, reward adversarial attacks, and adversarial policy\nattacks.\n• Environmental Adversarial Attacks\nEnvironment-based adversarial attacks are those that\nadd perturbations to the environment perceived in re-\ninforcement learning, thereby aﬀecting the system’s\nperception of state 푠, which in turn incorrectly matches\nthe attacker’s speciﬁed policy 푠′ to ﬁnally manipu-\nlate the decisions of the reinforcement learning system\nin a given state. In 2017, Huang et al.[184] imple-\nmented an environmental adversarial attack on a rein-\nforcement learning system by adding adversarial per-\nturbations to the external environment image frames\nin reinforcement learning based on the white-box FGSM\nalgorithm; in the same year, Lin et al.[185] proposed\nan optimized environmental adversarial attack method\ntargeting at the best behavior in a speciﬁc state; Be-\nHui Cao et al.: Preprint submitted to Elsevier\nPage 16 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nhzadan et al.[186] veriﬁed the transition of adversar-\nial attacks among reinforcement learning models and\nthus proposed a transition-based black-box attack. In\n2019, Xiao et al.[187] proposed a method for estimat-\ning model gradients based on frame consistency infor-\nmation, thus enabling the ﬁrst adversarial black-box\nattack in reinforcement learning. In 2020, Kiourti et\nal.[183] proposed TrojDRL, which describes a reward-\nbased adversarial attack as one that is performed by\nadding minute speciﬁc perturbations to the environ-\nmental state to enables ̂푠= 푠+훿that eventually makes\nthe behaviour given by the reinforcement learning model\nmisunderstood, i.e. ̂퐴(푠, 푚, 훿) ≠퐴(푠, 푚, 훿). In the area\nof autonomous driving, Behzadan et al.[188] in 2019\nveriﬁed that in the environment of autonomous driv-\ning, using reward adversarial attacks, an attacker could\ncause a direct collision or malicious manipulation of\nthe trajectory of the autonomous driving vehicles.\n• Reward Adversarial Attacks\nIf an attacker is able to maliciously tamper with some\nof the rewards, it may lead to the policy of the rein-\nforcement learning system being manipulated by the\nattacker, thus posing a severe threat to the system. Kiourti\net al.[183] validated a reward adversarial attack where\nan attacker would set the corresponding reward to 1\nwhen its target state 푠is reached, and otherwise set\nthe reward to -1, to create a strong attack scenario. In\n2022, Islam et al.[189] proposed a reward adversarial\nattack applicable to the UAV environment.\n• Adversarial Policy Attacks\nUnlike the reward adversarial attack approach, an ad-\nversarial policy attack does not need to tamper with\nthe victim’s reward or policy; instead, in an adver-\nsarial environment, the attacker quickly ﬁnds a policy\nto defeat the victim by analyzing the victim’s policy\nor behavior, ﬁnding its vulnerabilities and exploiting\nthem. Tretschk et al.[190] proposed the adversarial\npolicy of Adversarial Transformer Networks (ATN).\nIn 2020, Gleave et al.[191] proposed Adversarial Poli-\ncies, in which an attacker generates targeted adversar-\nial policies based on the behavior of the victim, pro-\nducing seemingly random and uncoordinated behav-\nior to defeat or disrupt the victim. Such policies are\nmore successful in high-dimensional environments and\nhave been validated in real eSports environments. In\n2021, Wang et al.[192] synthesized reward adversarial\nattacks with adversarial policy attacks and proposed\nBackDooRK, which signiﬁcantly improved the suc-\ncess rate of attackers in defeating their victims.\nAlgorithm 11. Adversarial Policies\nFor multiple (in the case of two) participants in a rein-\nforcement learning environment, it is assumed that the vic-\ntim’s policy 휋푣has been determined, and here the victim’s\npolicy determines its behaviour 푎푣∼휋푣(⋅∣푠). And the at-\ntacker continuously optimizes its own policy 휋훼based on the\nvictim’s policy and behaviour. It then be described as in this\nMarkov decision process 푀푎= (푆, 퐴훼, 푇훼, 푅′\n훼), considered\nwithin state transition probabilities 푇훼and rewards 푅훼, the\nintegration of the victim policy 휋푣yields\n푇훼(푠, 푎훼) = 푇(푠, 푎훼,푎푣)\nand\n푅′\n훼(푠, 푎훼,푠′) = 푅훼(푠, 푎훼,푎푣,푠′)\nThe attacker ﬁnds an adversarial policy against the vic-\ntim by optimizing the following loss function:\narg max\n∞\n∑\n푡=0\n훾푡푅훼(푠푡, 푎푡\n훼, 푠푡+1)\n(18)\nwhere it is subject to 푠푡+1 ∼푇훼(푠푡, 푎푡\n훼) and 푎훼∼휋(⋅∣푠푡)\n4.4. Summary\nThis chapter introduces some technologies that may pose\na security risk to the autonomous driving AI decision-making\nlayer and brieﬂy describes the technical principles. The main\nfunction of the sensor layer is to recognize the raw infor-\nmation collected by sensors, while the main function of the\ndecision layer is to make driving decisions based on the per-\nceived state of the environment. Each layer has its own au-\ntonomous driving function, and a threat to any of these lay-\ners could aﬀect the overall safety of the autonomous driving\nsystem.\n5. Emerging Threats in Federated\nLearning-based Vehicular Internet of\nThings\nWith the rapid development of smart vehicles, the vehi-\ncle is no longer an isolated single point, but increasingly a\nterminal worker in the pan-vehicle network. In many coun-\ntries and regions around the world, the vehicular internet of\nthings is already under rapid construction and its security\nbased on traditional cyber security technologies has been\nwidely studied[15]. However, with the development of emerg-\ning technologies such as arithmetic networks and privacy\ncomputing, new generation technologies such as deep learn-\ning technologies, edge computing and federated learning will\nbe further integrated into the environment of the vehicular\ninternet of things, giving rise to new business forms. Still,\nnew technologies and new business forms also bring new\nsecurity risks, and this chapter focuses on the new risks that\nthey may bring.\nIt is well known that current artiﬁcial intelligence tech-\nnology is a data-driven approach[193, 194, 195]. In AI ap-\nplications, a large amount of information needs to be col-\nlected in advance as the training data. Traditional methods\nof data collection and information interaction face a num-\nber of limitations: First, traditional data collection and trans-\nmission are often ineﬃcient. Secondly, traditional data col-\nlection often leads to invasion of user privacy. Therefore,\nHui Cao et al.: Preprint submitted to Elsevier\nPage 17 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nFederated Learning (FL)[196, 197, 198] is a new distributed\nlearning framework that does not require data collection by a\ncentral worker, providing a relatively more eﬃcient and pri-\nvate way of interaction. In federated learning, each worker\nis trained with local data sets to obtain local gradients or\nweights through machine learning algorithms, and then up-\nloads local gradients instead of local sensitive data, enabling\nknowledge interaction instead of data interaction. Federated\nlearning has been widely used in the Internet, mobile termi-\nnals and other ﬁelds. At the same time, federated learning\nhas also been introduced into the ﬁeld of the vehicular in-\nternet of things [199, 67, 200, 66] and has become a future\ntrend.\nThe federated learning environment oﬀers more new at-\ntack methods[201, 202, 203]. With more workers partici-\npating in federated learning and the trustworthiness of each\nworker with the cloud diﬃcult to guarantee, malicious work-\ners in the vehicular IoT may attack the federated learning\nsystem in a variety of ways, while the privacy of the workers\nmay also be at risk.\n5.1. Byzantine Attack on Federated Learning\nThe Byzantine Attack in Federated Learning refers to\nworkers attacked by a malicious Byzantine worker to con-\nstruct harmful gradients which after aggregation will make\nthe global model diﬃcult to aggregate, thus making the sys-\ntem unusable or generating a global model with a malicious\nbackdoor. Distinguished from traditional Data Posioning at-\ntacks, the above attacks are also known as Model Posioning\nattacks. In 2017, Blanchard et al.[204] ﬁrst proposed the\nByzantine attack in a machine learning environment. The\nprinciple is that in round 푡, a non-Byzantine worker 푝in fed-\nerated learning will locally compute the unbiased estimate\n푉푡\n푝of its gradient ▽푄(푥푡) and send it to the aggregation\nworker which according to some aggregation rule 퐹, aggre-\ngates the received gradient estimates, then in round 푡+1, the\nweight of the global model is\n푥푡+1 = 푥푡−훾푡⋅퐹(푉푡\n1, ..., 푉푡\n푛)\nwhere 훾푡is the learning rate. While the malicious Byzan-\ntine worker cleverly constructs destructive local gradient es-\ntimates:\n푉푛= 1\n휆푛\n⋅푈−\n푛−1\n∑\n푖=1\n휆푖\n휆푛\n푉푖\nwhere 훾푡is the weight the gradient estimate of worker 푛at\nthe time of aggregation. This will cause the global gradient\nto become any harmful gradient 푈provided by the Byzan-\ntine worker after the ﬁnal aggregation.\nThe above describes a Byzantine worker that poisons in\nonly some round of aggregation 푡, which is called a \"single-\nshot attack\". Generally, its limitations are as follows.\n• Attack’s Capability Is Limited.\nAttack just is a\nsingle worker or a single aggregation that has limited\ninﬂuence on its attacking power.\nFigure 9: Schematic of Byzantine attack [204].\nThe black\ndashed line represents the gradient estimate for non-Byzantine\nworkers, the blue solid line represents the global gradient after\nnormal aggregation, and the red dashed line represents the\ngradient estimate submitted by the Byzantine worker.\n• Poor Concealment. A single worker attack or a single-\nshot attack on an aggregator usually makes the state of\nthe poisoned worker signiﬁcantly diﬀerent from a nor-\nmal worker, and that leads to easily detected.\n• Prone to Recession. After multiple aggregations, the\neﬀect of poisoning a single worker during an aggrega-\ntion round tends to fade, even does not continue to be\neﬀective.\nBlanchard et al.[204] also proposed the concept of Byzan-\ntine tolerance for measuring the robustness of federated learn-\ning models against Byzantine attacks. To improve Byzantine\nrobustness, some scholars have proposed novel aggregation\nalgorithms that can be used for federated learning[205, 206].\nTo overcome these limitations, an attacker can adopt to\nRepeated Attacks or Collusion Attacks. Repeated Attacks\nmean that the attacker can perform poisoning in multiple\nrounds of aggregation; Collusion Attacks are joint poison-\ning of multiple Byzantine workers. Experiments show that\nrepeated attacks and collusion attacks can enhance the ca-\npability of Byzantine attacks and can signiﬁcantly improve\nthe concealment and recession resistance of the attacks[207].\nXie et al.[208] proposed a distributed backdoor attack that\ncan be performed in a federated learning environment.\nAlgorithm 12. Distributed Backdoor Attack (DBA)[208]\nDistributed backdoor attacks provide a eﬃcient way for\nmultiple malicious workers in federated learning to conspire\nto an attack. The DBA algorithm takes advantage of the local\ndata opacity in federated learning, with multiple malicious\nworkers each adding more minute malicious perturbations\nin multiple rounds to improve concealment. The malicious\npermissions of each worker are optimally generated in the\nHui Cao et al.: Preprint submitted to Elsevier\nPage 18 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nfollowing manner.\n푤∗\n푖= arg max\n푤푖(\n∑\n푗∈푠푖\n푝표푖\n푃[퐺푡+1(푅(푋푗\n푖, 휙∗\n푖)) = 휏; 훾; 퐼]\n+\n∑\n푗∈푆푖\n푐푙푛\n푃[퐺푡+1(푥푖\n푗) = 푦푖\n푗]), ∀푖∈[푀]\n(19)\nwhere 휙∗\n푖= {휙, 푂(푖)} represents the local poisoning pol-\nicy of attacker 푚푖, and ∀is the global trigger.\nFigure 10: Schematic of Byzantine Attack [201]. The black\ndashed line denotes the gradient estimate for non-Byzantine\nworkers, the blue solid line denotes the global gradient after\nnormal aggregation, and the red dashed line denotes the gra-\ndient estimate submitted by the Byzantine worker. [201]\n5.2. Privacy Inference on Federated Learning\nThe interior and exterior images of an autonomous driv-\ning vehicle may include sensitive information such as faces\nand license plate numbers. So local user data may reﬂect\nthe user’s location and trajectory, in-vehicle behavior and\ndriving habits, which are also often considered sensitive, and\ntherefore direct user data capture may lead to user privacy vi-\nolations. One of the aims of Federated Learning is to avoid\nthe direct leakage of sensitive user data, thereby achieving\nuser privacy protection. The eﬀects of user privacy in feder-\nated learning have received extensive attention and researches[209,\n113, 210, 211, 201, 212, 213, 214] . However, it shows that\nlocal gradients are highly correlated with the user’s data and\nthat data may still be inferred when local gradients are ob-\ntained. Techniques such as Model Inversion and Member-\nship Privacy Inference may help infer local user data.\n• Model Inversion The model inversion method replaces\nthe pixels in the initial random images one by one,\nthen classiﬁes the constructed images with the help of\nsome model, and iteratively optimizes the constructed\nimage based on the results of the classiﬁcation, result-\ning in a constructed image that is highly similar to the\ntarget image. To speed up aggregation, model inver-\nsion algorithms mostly utilize the distribution of the\ntarget image as prior knowledge to participate in the\noptimization process. In 2015, Fredrikson et al.[5]\nﬁrst proposed a model inversion attack; in 2016, Wu et\nal.[215] proposed a black-box model inversion attack\nalgorithm; in 2017, Hitaj et al.[216] proposed a model\ninversion algorithm based on adversarial generative\nnetwork techniques, which achieved better reconstruc-\ntion results, also known as \"Adversarial Generative\nNetwork Reconstruction Attacks\" (GAN Reconstruc-\ntion Attacks). Mai et al.[217] extended model inver-\nsion to the ﬁeld of face recognition to recover from\nface recognition features for face image data, propos-\ning the Face Recovery Attack, and Razzhigaev et al.[218,\n219] continuously optimized the black box face recov-\nery attack.\n• Membership Privacy Inference Member privacy infer-\nence means that an attacker exploits the special feed-\nback of the overﬁtting phenomenon and tries to infer\nwhether the target data is in the training set or not.\nTypically, attackers ﬁrst construct a Shadow Model\nsimilar to the target model; then uses the shadow model\nto generate training data; furthermore, used the data to\ntrain an Attack Model; last, the attackers use the At-\ntack Model to construct the complete attack process.\nIn 2017, Shokri et al.[6] at Cornell University ﬁrst\nproposed the member privacy inference attack. Once\nproposed, the member privacy inference method has\nbeen continuously researched and further optimized\nand improved[214, 213, 220]. In 2018, Yeom et al.[221]\nanalyzed in depth the relationship between overﬁtting\nand the risk of member privacy leakage.\nIn 2019,\nSalem et al.[222] incorporated a data transition attack\nmethod that reduces the attacker’s reliance on back-\nground knowledge and makes the \"shadow model\" less\nnecessary. Sablayrolles et al.[223] improved the mem-\nbership privacy inference attack using a Bayesian opti-\nmization policy. Zhang et al.[224] extended the mem-\nber privacy inference attack to the recommender sys-\ntem domain. Hui et al.[225] proposed the BlINDMI\nalgorithm, which ﬁrst generates a certain amount of\nnon-membership data, and then iteratively generates a\ncomparison between non-membership data and mem-\nbership data to improve the accuracy of membership\nprivacy inference.\nIn a federated learning, the data distribution of work-\ners is broadly similar. That providing more contex-\ntual knowledge and creating better conditions for ma-\nlicious workers and the cloud to conduct member pri-\nvacy inference attacks. In 2019, Nasr et al.[226] val-\nidated the membership privacy inference attack risk\nin federated learning. In 2020, Chen et al.[227] fur-\nther improved the success rate of member privacy in-\nference attacks in federated learning using adversarial\ngenerative networks for data augmentation. In 2021,\nHu et al.[228] proposed Source Inference Attack for\nfederated learning, which uses Bayesian methods to\ninfer the training data of federated learning workers\nand in the same year, Gupta et al.[229] extended the\ntraditional membership privacy inference attack from\nclassiﬁcation tasks to regression tasks, and also veri-\nHui Cao et al.: Preprint submitted to Elsevier\nPage 19 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nﬁed that their method is equally applicable to regres-\nsion tasks in a federated learning environment.\nAlgorithm 13. General Model Inversion[5]\nModel inversion used be recover the training dataset that\nprobability includes sensitive data. The general model in-\nversion algorithm ﬁrst computes each possible target fea-\nture vector 푣for feature 푥1 and then evaluates its probability\nof being correct. Also, since the class distribution of deep\nlearning models tends to obey a Gaussian Distribution, so\nadapt a Gaussian function as a penalty function can accel-\nerate the convergence. The algorithm can be described as\nfollows.\nAlgorithm: 13. Model Inversion\nInput: Target model 푓\nOutput: An example of training data 푥′\n1 initialized 푥0 ;\n2 foreach feature 푥푖∈the feature vector 푋do\n3\nforeach the possible value 푣∈푥푖do\n4\n푥′ = 푣, 푥2, 푥3, ..., 푥푛\n5\n푟푣←푒푟푟(푦, 푓(푥′)) ⋅Π푖푝푖(푥푖)\n6\nend\n7 end\nAlgorithm 14. Membership Privacy Inference[6]\nImplementing a member privacy inference attack requires\nseveral processes, starting with the generation of data for\ntraining the shadow model. If the attacker has some back-\nground knowledge and possesses some homogeneous distri-\nbution data, it can be used directly for shadow model train-\ning. If the attacker does not have the appropriate background\nknowledge, data with a high conﬁdence level can be selected\nas integrated training data by querying the target model. Each\nclassiﬁcation category 푐will be initially recorded as 푥ran-\ndomly and iterated as follows: sequentially select the record\ndata classiﬁed by the target model as 푐with the maximum\nconﬁdence level 푦푐which is ensured to be greater than a cer-\ntain threshold on 푓, into the integrated data set. Once a\nrecord 푥is selected into the integrated data set, randomly\nchange the features as many as 푘based on 푥to generate a\nnew record 푋∗. This is iterated until a certain amount of\ntraining data is generated. The second process is to gener-\nate a shadow model. Based on the generated training data\nset퐷푡푟푎푖푛\n푠ℎ푎푑표푤푖, after training, shadow models 푠ℎ푎푑표푤푖will be\ngenerated. The third step is to train the attack model. Query\nthe prediction vector of the records (푥, 푦) ∈퐷푡푟푎푖푛\n푠ℎ푎푑표푤in the\ntraining data set of shadow models 푠ℎ푎푑표푤푖, then record\n(푦, ̂푦, 푖푛) can be generated. And calculate the prediction vector ̂푦=\n푓푖\n푠ℎ푎푑표푤(푥) in the shadow model test data set ∀(푥, 푦) ∈퐷푡푒푠푡\n푠ℎ푎푑표푤푖\nthen vector ̂푦= 푓푖\n푠ℎ푎푑표푤(푥) can be obtained. Next the two\ncorresponding sets of vectors of each category 푐are aggre-\ngated into the training data 퐷푡푟푎푖푛\n푎푡푡푎푐푘of the attack model. On\nthis basis, a classiﬁer is trained to determine whether the data\nis included in the training data.\n5.3. Summary\nWith the fusion and development of technologies, such\nas federated learning , edge computing and etc., the intelli-\ngent vehicular internet of things is gradually growing and be-\ncoming a future trend[230]. However, the security and user\nprivacy risks associated with federated learning and other\ntechnologies are also a concern. In the vehicular IoTs, the\ndata distribution of each end is similar, providing attackers\nwith certain background knowledge. Once a end is con-\ntrolled by an attacker, the whole IoT networks may be subject\nto Byzantine attacks, and the risk of user privacy analysis is\ngreatly increased. While enjoying the convenience oﬀered\nby vehicular IoT and AI, we should not ignore the associ-\nated risks, but rather conduct relevant research and security\nprotection.\n6. Conclusion\nAutonomous driving is a complex system based on arti-\nﬁcial intelligence technology. A number of artiﬁcial intel-\nligence applications, such as objective detection, segmenta-\ntion, speech recognition and driving decision-making, play\nan important role in autonomous driving. Safety is a key\nconcern in autonomous driving systems. AI security is cru-\ncial and directly aﬀects autonomous driving system secu-\nrity, and leads to personal safety, which is far beyond tra-\nditional network security and basic software security. The\nnovel technologies, such as AI, bring emerging risks.\nThis paper brieﬂy introduces the AI technology route\nand AI functional modules in the autonomous driving sys-\ntem, and analyses the origins, development, and current ap-\npropriate AI security technologies for autonomous driving.\nSimilar to general AI, autonomous driving is under threats\nof adversarial examples attacks, including AI backdoor at-\ntacks, model inversion and member privacy. Despite there\nare some defense methods that can be useful against these\nthreats, ensuring safety for complex autonomous driving sys-\ntems requires not only single-point defense techniques against\ncertain threats but also building a complete trusted AI system[231,\n232], as following:\n• Trustworthy AI evaluation system. The safety of\nAI in autonomous driving system requires a complete\nevaluation and validation system, which covering data\npreparation, model training, model deployment, sys-\ntem application and other parts of the AI model life cy-\ncle. Especially, there are some noteworthy issues, in-\ncluding: AI adversarial robustness assessment, cross-\ndomain data robustness assessment, model safety val-\nidation, training data safety validation, and data adver-\nsarial example detection.\n• Trustworthy AI Architecture. Further more, we need\nto improve autonomous driving security from just re-\nducing some speciﬁc threats to building Trustworthy\nAI Architecture. There is something beyond adversar-\nial detecting need to do to build an AI architecture with\nhuman agency and oversight, robustness and safety,\nHui Cao et al.: Preprint submitted to Elsevier\nPage 20 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nTable 3\nFoundational Adversarial Examples Algorithms\nMethod\nmethod\nMajor algorithms\nWhite-Box\nGradient sign-based\nFGSM[2], BIM[? ], MIM[233],PGD[234]\nOptimization-based\nCW[126],\nOthers\n...\nBlack-Box\nTransfer-Based\nDIM[235], TI[236]\nApproximate Gradient\nBPDA[237],EoT[132]\nScore-based\nZOO[238], NES[239], SPSA[240], 푁Attack[241]\nDecision-based\nBoundary Attack[242], Evolutionary Attack[243]\nprivacy and data governance, transparency, diversity,\nnon-discrimination and fairness, societal and environ-\nmental well-being, and accountability.\nTo summarize, this paper appeals for attention and fo-\ncus on emerging technologies such as artiﬁcial intelligence\nthat bring new safety risks to autonomous driving systems.\nIt is necessary that construct a safer and trusted autonomous\ndriving system through the establishment of a trusted artiﬁ-\ncial intelligence technology system.\n7. Appendix A. Summary table\nHere we present a table containing a summary of the ad-\nversarial examples algorithms as the foundation in this pa-\nper.\nReferences\n[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,\nDumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing prop-\nerties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n[2] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.\nEx-\nplaining and harnessing adversarial examples.\narXiv preprint\narXiv:1412.6572, 2014.\n[3] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets:\nIdentifying vulnerabilities in the machine learning model supply\nchain. arXiv preprint arXiv:1708.06733, 2017.\n[4] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas\nRistenpart. Stealing machine learning models via prediction {APIs}.\nIn 25th USENIX security symposium (USENIX Security 16), pages\n601–618, 2016.\n[5] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inver-\nsion attacks that exploit conﬁdence information and basic counter-\nmeasures. In Proceedings of the 22nd ACM SIGSAC conference on\ncomputer and communications security, pages 1322–1333, 2015.\n[6] Reza Shokri,\nMarco Stronati,\nCongzheng Song,\nand Vitaly\nShmatikov. Membership inference attacks against machine learn-\ning models. In 2017 IEEE symposium on security and privacy (SP),\npages 3–18. IEEE, 2017.\n[7] Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, and Jinwen He.\nTowards security threats of deep learning systems: A survey. IEEE\nTransactions on Software Engineering, 2020.\n[8] Shouling Ji, tianyu Du, Jinfeng Li, Chao Shen, and Bo Li. Secu-\nrity and privacy of machine learning models: A survey. Journal of\nSoftware, 32(1), 2021.\n[9] Han Xu, Yaxin Li, Wei Jin, and Jiliang Tang. Adversarial attacks and\ndefenses: frontiers, advances and practice. In Proceedings of the\n26th ACM SIGKDD International Conference on Knowledge Dis-\ncovery & Data Mining, pages 3541–3542, 2020.\n[10] Jiao Li, Yang Liu, Tao Chen, Zhen Xiao, Zhenjiang Li, and Jianping\nWang. Adversarial attacks and defenses on cyber–physical systems:\nA survey. IEEE Internet of Things Journal, 7(6):5103–5115, 2020.\n[11] Jiliang Zhang and Chen Li. Adversarial examples: Opportunities\nand challenges. IEEE transactions on neural networks and learning\nsystems, 31(7):2578–2593, 2019.\n[12] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial ex-\namples: Attacks and defenses for deep learning. IEEE transactions\non neural networks and learning systems, 30(9):2805–2824, 2019.\n[13] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi\nSchwarzschild, Dawn Song, Aleksander Madry, Bo Li, and Tom\nGoldstein. Data security for machine learning: data poisoning, back-\ndoor attacks, and defenses. arXiv e-prints, pages arXiv–2012, 2020.\n[14] SAE On-Road Automated Vehicle Standards Committee et al. Tax-\nonomy and deﬁnitions for terms related to on-road motor vehicle\nautomated driving systems. SAE Standard J, 3016:1–16, 2014.\n[15] Kui Ren, Qian Wang, Cong Wang, Zhan Qin, and Xiaodong Lin.\nThe security of autonomous driving: Threats, defenses, and future\ndirections. Proceedings of the IEEE, 108(2):357–372, 2019.\n[16] Yu Huang and Yue Chen. Survey of state-of-art autonomous driving\ntechnologies with deep learning. In 2020 IEEE 20th International\nConference on Software Quality, Reliability and Security Compan-\nion (QRS-C), pages 221–228. IEEE, 2020.\n[17] Yao Deng, Tiehua Zhang, Guannan Lou, Xi Zheng, Jiong Jin, and\nQing-Long Han. Deep learning-based autonomous driving systems:\na survey of attacks and defenses. IEEE Transactions on Industrial\nInformatics, 17(12):7897–7912, 2021.\n[18] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster\nr-cnn: Towards real-time object detection with region proposal net-\nworks.\nAdvances in neural information processing systems, 28,\n2015.\n[19] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.\nMask r-cnn. In Proceedings of the IEEE international conference\non computer vision, pages 2961–2969, 2017.\n[20] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy,\nScott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single\nshot multibox detector. In European conference on computer vision,\npages 21–37. Springer, 2016.\n[21] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improve-\nment. arXiv preprint arXiv:1804.02767, 2018.\n[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr\nDollár. Focal loss for dense object detection. In Proceedings of\nthe IEEE international conference on computer vision, pages 2980–\n2988, 2017.\n[23] Yosuke Shinya. Usb: Universal-scale object detection benchmark.\narXiv preprint arXiv:2103.14027, 2021.\n[24] Di Feng, Christian Haase-Schütz, Lars Rosenbaum, Heinz Hertlein,\nClaudius Glaeser, Fabian Timm, Werner Wiesbeck, and Klaus Di-\netmayer.\nDeep multi-modal object detection and semantic seg-\nmentation for autonomous driving: Datasets, methods, and chal-\nlenges. IEEE Transactions on Intelligent Transportation Systems,\n22(3):1341–1360, 2020.\nHui Cao et al.: Preprint submitted to Elsevier\nPage 21 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\n[25] Belur V Dasarathy. Sensor fusion potential exploitation-innovative\narchitectures and illustrative applications. Proceedings of the IEEE,\n85(1):24–38, 1997.\n[26] Jamil Fayyad, Mohammad A Jaradat, Dominique Gruyer, and\nHomayoun Najjaran. Deep learning sensor fusion for autonomous\nvehicle perception and localization: A review. Sensors, 20(15):4220,\n2020.\n[27] Yingjie Wang, Qiuyu Mao, Hanqi Zhu, Yu Zhang, Jianmin Ji, and\nYanyong Zhang. Multi-modal 3d object detection in autonomous\ndriving: a survey. arXiv preprint arXiv:2106.12735, 2021.\n[28] De Jong Yeong, Gustavo Velasco-Hernandez, John Barry, Joseph\nWalsh, et al. Sensor and sensor fusion technology in autonomous\nvehicles: A review. Sensors, 21(6):2140, 2021.\n[29] Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fi-\ndler, and Raquel Urtasun. Monocular 3d object detection for au-\ntonomous driving. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages 2147–2156, 2016.\n[30] Yu Xiang, Wongun Choi, Yuanqing Lin, and Silvio Savarese. Data-\ndriven 3d voxel patterns for object category recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern recog-\nnition, pages 1903–1911, 2015.\n[31] Florian Chabot, Mohamed Chaouch, Jaonary Rabarisoa, Céline\nTeuliere, and Thierry Chateau. Deep manta: A coarse-to-ﬁne many-\ntask network for joint 2d and 3d vehicle analysis from monocular\nimage. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 2040–2049, 2017.\n[32] Qingdong He, Zhengning Wang, Hao Zeng, Yi Zeng, Shuaicheng\nLiu, and Bing Zeng. Svga-net: Sparse voxel-graph attention net-\nwork for 3d object detection from point clouds.\narXiv preprint\narXiv:2006.04043, 2020.\n[33] Bo Li, Tianlei Zhang, and Tian Xia.\nVehicle detection from\n3d lidar using fully convolutional network.\narXiv preprint\narXiv:1608.07916, 2016.\n[34] Jorge Beltrán, Carlos Guindel, Francisco Miguel Moreno, Daniel\nCruzado, Fernando Garcia, and Arturo De La Escalera. Birdnet:\na 3d object detection framework from lidar information. In 2018\n21st International Conference on Intelligent Transportation Systems\n(ITSC), pages 3517–3523. IEEE, 2018.\n[35] Bo Li. 3d fully convolutional network for vehicle detection in point\ncloud. In 2017 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 1513–1518. IEEE, 2017.\n[36] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas.\nPointnet++: Deep hierarchical feature learning on point sets in a\nmetric space. Advances in neural information processing systems,\n30, 2017.\n[37] Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point\ncloud based 3d object detection. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, pages 4490–4499,\n2018.\n[38] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-\nview 3d object detection network for autonomous driving. In Pro-\nceedings of the IEEE conference on Computer Vision and Pattern\nRecognition, pages 1907–1915, 2017.\n[39] Jason Ku, Melissa Moziﬁan, Jungwook Lee, Ali Harakeh, and\nSteven L Waslander. Joint 3d proposal generation and object detec-\ntion from view aggregation. In 2018 IEEE/RSJ International Con-\nference on Intelligent Robots and Systems (IROS), pages 1–8. IEEE,\n2018.\n[40] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas.\nFrustum pointnets for 3d object detection from rgb-d data. In Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 918–927, 2018.\n[41] Duarte Fernandes, António Silva, Rafael Névoa, Claudia Simoes,\nDibet Gonzalez, Miguel Guevara, Paulo Novais, Joao Monteiro, and\nPedro Melo-Pinto. Point-cloud based 3d object detection and classi-\nﬁcation methods for self-driving applications: A survey and taxon-\nomy. Information Fusion, 68:161–191, 2021.\n[42] Rui Qian, Xin Lai, and Xirong Li.\n3d object detection for au-\ntonomous driving: a survey.\nPattern Recognition, page 108796,\n2022.\n[43] Browse\nstate-of-the-art.\nhttps://paperswithcode.com/area/\ncomputer-vision/autonomous-vehicles/. Accessed May 18, 2022.\n[44] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and\nChrisina Jayne. Imitation learning: A survey of learning methods.\nACM Computing Surveys (CSUR), 50(2):1–35, 2017.\n[45] Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee,\nXinyan Yan, Evangelos A Theodorou, and Byron Boots. Imitation\nlearning for agile autonomous driving. The International Journal of\nRobotics Research, 39(2-3):286–302, 2020.\n[46] Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun,\nand Alexey Dosovitskiy. End-to-end driving via conditional imita-\ntion learning. In 2018 IEEE international conference on robotics\nand automation (ICRA), pages 4693–4700. IEEE, 2018.\n[47] Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauﬀeurnet:\nLearning to drive by imitating the best and synthesizing the worst.\narXiv preprint arXiv:1812.03079, 2018.\n[48] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A\nRusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Ried-\nmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-\nlevel control through deep reinforcement learning.\nnature,\n518(7540):529–533, 2015.\n[49] Matthew Hausknecht and Peter Stone. Deep recurrent q-learning\nfor partially observable mdps. In 2015 aaai fall symposium series,\n2015.\n[50] Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov,\nand Anastasiia Ignateva. Deep attention recurrent q-network. arXiv\npreprint arXiv:1512.01693, 2015.\n[51] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex\nGraves, Timothy Lillicrap, Tim Harley, David Silver, and Koray\nKavukcuoglu. Asynchronous methods for deep reinforcement learn-\ning. In International conference on machine learning, pages 1928–\n1937. PMLR, 2016.\n[52] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom\nSchaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Rein-\nforcement learning with unsupervised auxiliary tasks. arXiv preprint\narXiv:1611.05397, 2016.\n[53] Richard S Sutton and Andrew G Barto. Reinforcement learning: An\nintroduction. MIT press, 2018.\n[54] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and\nAnil Anthony Bharath. Deep reinforcement learning: A brief survey.\nIEEE Signal Processing Magazine, 34(6):26–38, 2017.\n[55] Xidong Feng, Jianming Hu, Yusen Huo, and Yi Zhang. Autonomous\nlane change decision making using diﬀerent deep reinforcement\nlearning methods. In CICTP 2019, pages 5563–5575. 2019.\n[56] Ali Alizadeh, Majid Moghadam, Yunus Bicer, Nazim Kemal Ure,\nUgur Yavas, and Can Kurtulus. Automated lane change decision\nmaking using deep reinforcement learning in dynamic and uncer-\ntain highway environment. In 2019 IEEE Intelligent Transportation\nSystems Conference (ITSC), pages 1399–1404. IEEE, 2019.\n[57] Branka Mirchevska, Christian Pek, Moritz Werling, Matthias Al-\nthoﬀ, and Joschka Boedecker.\nHigh-level decision making for\nsafe and reasonable autonomous lane changing using reinforcement\nlearning. In 2018 21st International Conference on Intelligent Trans-\nportation Systems (ITSC), pages 2156–2162. IEEE, 2018.\n[58] Yang Thee Quek, Li Ling Koh, Ngiap Tiam Koh, Wai Ann Tso,\nand Wai Lok Woo. Deep q-network implementation for simulated\nautonomous vehicle control.\nIET Intelligent Transport Systems,\n15(7):875–885, 2021.\n[59] Martin Holen, Rupsa Saha, Morten Goodwin, Christian W Omlin,\nand Knut Eivind Sandsmark. Road detection for reinforcement learn-\ning based autonomous car. In Proceedings of the 2020 the 3rd In-\nternational Conference on Information Science and System, pages\n67–71, 2020.\n[60] Yanming Feng and Yongrong Wu. Environmental adaptive urban\ntraﬃc signal control based on reinforcement learning algorithm. In\nJournal of Physics: Conference Series, volume 1650, page 032097.\nHui Cao et al.: Preprint submitted to Elsevier\nPage 22 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nIOP Publishing, 2020.\n[61] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ah-\nmad A Al Sallab, Senthil Yogamani, and Patrick Pérez. Deep rein-\nforcement learning for autonomous driving: A survey. IEEE Trans-\nactions on Intelligent Transportation Systems, 2021.\n[62] Szilárd Aradi. Survey of deep reinforcement learning for motion\nplanning of autonomous vehicles. IEEE Transactions on Intelligent\nTransportation Systems, 2020.\n[63] Maurice Pope. Etsi. universal mobile telecommunications system\n(umts); lte; architecture enhancements for v2x services (3gpp ts\n23.285 version 14.2. 0 release 14), 2016.\n[64] Apostolos Papathanassiou and Alexey Khoryaev. Cellular v2x as\nthe essential enabler of superior global connected transportation ser-\nvices. IEEE 5G Tech Focus, 1(2):1–2, 2017.\n[65] Xiaobo Ma, Jiahao Peng, Lei Xue, and Xiaohong Guan. ntegrated\nsecurity of cyber-physical vehicle networked systems in the age of\n5g (in chinese). Sci Sin Inform, 2019.\n[66] Zhaoyang Du, Celimuge Wu, Tsutomu Yoshinaga, Kok-Lim Alvin\nYau, Yusheng Ji, and Jie Li. Federated learning for vehicular internet\nof things: Recent advances and open issues. IEEE Open Journal of\nthe Computer Society, 1:45–61, 2020.\n[67] Jason Posner, Lewis Tseng, Moayad Aloqaily, and Yaser Jararweh.\nFederated learning in vehicular networks: opportunities and solu-\ntions. IEEE Network, 35(2):152–159, 2021.\n[68] Zi Jian Yew and Gim Hee Lee. 3dfeat-net: Weakly supervised lo-\ncal 3d features for point cloud registration. In Proceedings of the\nEuropean conference on computer vision (ECCV), pages 607–623,\n2018.\n[69] Christopher Choy, Jaesik Park, and Vladlen Koltun. Fully convolu-\ntional geometric features. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 8958–8966, 2019.\n[70] Lei Li, Siyu Zhu, Hongbo Fu, Ping Tan, and Chiew-Lan Tai. End-\nto-end learning local multi-view descriptors for 3d point clouds. In\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 1919–1928, 2020.\n[71] Zhile Ren, Orazio Gallo, Deqing Sun, Ming-Hsuan Yang, Erik B\nSudderth, and Jan Kautz. A fusion approach for multi-frame optical\nﬂow estimation. In 2019 IEEE Winter Conference on Applications\nof Computer Vision (WACV), pages 2077–2086. IEEE, 2019.\n[72] Yikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun\nSun, and Yunhe Wang. Multimodal token fusion for vision trans-\nformers. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12186–12195, 2022.\n[73] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin\nRenz, and Andreas Geiger. Transfuser: Imitation with transformer-\nbased sensor fusion for autonomous driving.\narXiv preprint\narXiv:2205.15997, 2022.\n[74] Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Jiquan\nNgiam, Daiyi Peng, Junyang Shen, Yifeng Lu, Denny Zhou, Quoc V\nLe, et al. Deepfusion: Lidar-camera deep fusion for multi-modal\n3d object detection. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 17182–17191,\n2022.\n[75] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 1440–1448, 2015.\n[76] Diganta Misra. Mish: A self regularized non-monotonic neural ac-\ntivation function. arXiv preprint arXiv:1908.08681, 4(2):10–48550,\n2019.\n[77] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors: De-\ntecting objects with recursive feature pyramid and switchable atrous\nconvolution. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 10213–10224, 2021.\n[78] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao.\nYolov7: Trainable bag-of-freebies sets new state-of-the-art for real-\ntime object detectors. arXiv preprint arXiv:2207.02696, 2022.\n[79] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jian-\nmin Bao, Dong Chen, and Baining Guo. Contrastive learning rivals\nmasked image modeling in ﬁne-tuning via feature distillation. arXiv\npreprint arXiv:2205.14141, 2022.\n[80] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d\nobject proposal generation and detection from point cloud. In Pro-\nceedings of the IEEE/CVF conference on computer vision and pat-\ntern recognition, pages 770–779, 2019.\n[81] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xi-\naogang Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set\nabstraction for 3d object detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages\n10529–10538, 2020.\n[82] Wu Zheng, Weiliang Tang, Li Jiang, and Chi-Wing Fu.\nSe-ssd:\nSelf-ensembling single-stage object detector from point cloud. In\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 14494–14503, 2021.\n[83] Yifan Zhang, Qijian Zhang, Zhiyu Zhu, Junhui Hou, and Yixuan\nYuan. Glenet: Boosting 3d object detectors with generative label\nuncertainty estimation. arXiv preprint arXiv:2207.02466, 2022.\n[84] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio\nPuglielli, Rangharajan Venkatesan, Brucek Khailany, Joel Emer,\nStephen W Keckler, and William J Dally. Scnn: An accelerator for\ncompressed-sparse convolutional neural networks. ACM SIGARCH\ncomputer architecture news, 45(2):27–40, 2017.\n[85] Lucas Tabelini, Rodrigo Berriel, Thiago M Paixao, Claudine Badue,\nAlberto F De Souza, and Thiago Oliveira-Santos. Keep your eyes\non the lane: Real-time attention-guided lane detection. In Proceed-\nings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 294–302, 2021.\n[86] Tu Zheng, Yifei Huang, Yang Liu, Wenjian Tang, Zheng Yang, Deng\nCai, and Xiaofei He. Clrnet: Cross layer reﬁnement network for lane\ndetection. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 898–907, 2022.\n[87] Álvaro Arcos-García, Juan A Alvarez-Garcia, and Luis M Soria-\nMorillo. Deep neural network for traﬃc sign recognition systems:\nAn analysis of spatial transformers and stochastic optimisation meth-\nods. Neural Networks, 99:158–165, 2018.\n[88] Junzhou Chen, Kunkun Jia, Wenquan Chen, Zhihan Lv, and Ronghui\nZhang. A real-time and high-precision method for small traﬃc-signs\nrecognition. Neural Computing and Applications, 34(3):2233–2245,\n2022.\n[89] Esther Rani et al. Littleyolo-spp: A delicate real-time vehicle detec-\ntion algorithm. Optik, 225:165818, 2021.\n[90] Pranav Adarsh, Pratibha Rathi, and Manoj Kumar. Yolo v3-tiny:\nObject detection and recognition using one stage improved model.\nIn 2020 6th International Conference on Advanced Computing and\nCommunication Systems (ICACCS), pages 687–694. IEEE, 2020.\n[91] Jianan Li, Xiaodan Liang, ShengMei Shen, Tingfa Xu, Jiashi Feng,\nand Shuicheng Yan. Scale-aware fast r-cnn for pedestrian detection.\nIEEE transactions on Multimedia, 20(4):985–996, 2017.\n[92] Liliang Zhang, Liang Lin, Xiaodan Liang, and Kaiming He. Is faster\nr-cnn doing well for pedestrian detection? In European conference\non computer vision, pages 443–457. Springer, 2016.\n[93] Irtiza Hasan, Shengcai Liao, Jinpeng Li, Saad Ullah Akram, and\nLing Shao. Generalizable pedestrian detection: The elephant in the\nroom. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11328–11337, 2021.\n[94] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convo-\nlutional networks for semantic segmentation. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages\n3431–3440, 2015.\n[95] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and\nJiaya Jia. Pyramid scene parsing network. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages\n2881–2890, 2017.\n[96] Jun Fu, Jing Liu, Jie Jiang, Yong Li, Yongjun Bao, and Hanqing\nLu.\nScene segmentation with dual relation-aware attention net-\nwork. IEEE Transactions on Neural Networks and Learning Systems,\n32(6):2547–2560, 2020.\n[97] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,\nHui Cao et al.: Preprint submitted to Elsevier\nPage 23 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nStephen Lin, and Baining Guo.\nSwin transformer: Hierarchical\nvision transformer using shifted windows. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages\n10012–10022, 2021.\n[98] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng\nDai, and Yu Qiao. Vision transformer adapter for dense predictions.\narXiv preprint arXiv:2205.08534, 2022.\n[99] Chaoda Zheng, Xu Yan, Haiming Zhang, Baoyuan Wang, Shenghui\nCheng, Shuguang Cui, and Zhen Li. Beyond 3d siamese tracking:\nA motion-centric paradigm for 3d single object tracking in point\nclouds. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8111–8120, 2022.\n[100] Chaoda Zheng, Xu Yan, Jiantao Gao, Weibing Zhao, Wei Zhang,\nZhen Li, and Shuguang Cui. Box-aware feature enhancement for sin-\ngle object tracking on point clouds. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 13199–13208,\n2021.\n[101] Jiangbei Yue, Dinesh Manocha, and He Wang.\nHuman tra-\njectory prediction via neural social physics.\narXiv preprint\narXiv:2207.10435, 2022.\n[102] Karttikeya Mangalam, Yang An, Harshayu Girase, and Jitendra Ma-\nlik. From goals, waypoints & paths to long term human trajectory\nforecasting. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 15233–15242, 2021.\n[103] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco\nPavone. Trajectron++: Dynamically-feasible trajectory forecasting\nwith heterogeneous data. In European Conference on Computer Vi-\nsion, pages 683–700. Springer, 2020.\n[104] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and\nAlexandre Alahi. Social gan: Socially acceptable trajectories with\ngenerative adversarial networks. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, pages 2255–2264,\n2018.\n[105] Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose,\nHamid Rezatoﬁghi, and Silvio Savarese. Sophie: An attentive gan\nfor predicting paths compliant to social and physical constraints. In\nProceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 1349–1358, 2019.\n[106] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Motion\ntransformer with global intention localization and local movement\nreﬁnement. arXiv preprint arXiv:2209.13508, 2022.\n[107] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel,\nKhaled S Refaat, and Benjamin Sapp. Wayformer: Motion fore-\ncasting via simple & eﬃcient attention networks.\narXiv preprint\narXiv:2207.05844, 2022.\n[108] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,\nIoannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Play-\ning atari with deep reinforcement learning.\narXiv preprint\narXiv:1312.5602, 2013.\n[109] Hado Hasselt. Double q-learning. Advances in neural information\nprocessing systems, 23, 2010.\n[110] Jonathan Ho and Stefano Ermon. Generative adversarial imitation\nlearning. Advances in neural information processing systems, 29,\n2016.\n[111] Jeﬀrey Hawke, Richard Shen, Corina Gurau, Siddharth Sharma,\nDaniele Reda, Nikolay Nikolov, Przemysław Mazur, Sean Mickleth-\nwaite, Nicolas Griﬃths, Amar Shah, et al. Urban driving with con-\nditional imitation learning. In 2020 IEEE International Conference\non Robotics and Automation (ICRA), pages 251–257. IEEE, 2020.\n[112] Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-\nimitation learning. In International Conference on Machine Learn-\ning, pages 3878–3887. PMLR, 2018.\n[113] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith.\nFederated learning: Challenges, methods, and future directions.\nIEEE Signal Processing Magazine, 37(3):50–60, 2020.\n[114] Jonathan Petit, Bas Stottelaar, Michael Feiri, and Frank Kargl. Re-\nmote attacks on automated vehicles sensors: Experiments on camera\nand lidar. Black Hat Europe, 11(2015):995, 2015.\n[115] Chen Yan, Wenyuan Xu, and Jianhao Liu. Can you trust autonomous\nvehicles: Contactless attacks against sensors of self-driving vehicle.\nDef Con, 24(8):109, 2016.\n[116] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin\nZhang, and Wenyuan Xu.\nDolphinattack: Inaudible voice com-\nmands. In Proceedings of the 2017 ACM SIGSAC Conference on\nComputer and Communications Security, pages 103–117, 2017.\n[117] Bing Shun Lim, Sye Loong Keoh, and Vrizlynn LL Thing.\nAu-\ntonomous vehicle ultrasonic sensor vulnerability and impact assess-\nment. In 2018 IEEE 4th World Forum on Internet of Things (WF-\nIoT), pages 231–236. IEEE, 2018.\n[118] Yunmok Son, Hocheol Shin, Dongkwan Kim, Youngseok Park, Juh-\nwan Noh, Kibum Choi, Jungwoo Choi, and Yongdae Kim. Rocking\ndrones with intentional sound noise on gyroscopic sensors. In 24th\nUSENIX Security Symposium (USENIX Security 15), pages 881–\n896, 2015.\n[119] Gorkem Kar, Hossen Mustafa, Yan Wang, Yingying Chen, Wenyuan\nXu, Marco Gruteser, and Tam Vu. Detection of on-road vehicles em-\nanating gps interference. In Proceedings of the 2014 ACM SIGSAC\nconference on computer and communications security, pages 621–\n632, 2014.\n[120] Youngseok Park, Yunmok Son, Hocheol Shin, Dohyun Kim, and\nYongdae Kim. This ain’t your dose: Sensor spooﬁng attack on med-\nical infusion pump. In 10th USENIX workshop on oﬀensive tech-\nnologies (WOOT 16), 2016.\n[121] Dudi Nassi, Raz Ben-Netanel, Yuval Elovici, and Ben Nassi. Mo-\nbilbye:\nattacking adas with camera spooﬁng.\narXiv preprint\narXiv:1906.09765, 2019.\n[122] Mark L Psiaki and Todd E Humphreys. Protecting gps from spoofers\nis critical to the future of navigation. IEEE spectrum, 10, 2016.\n[123] Qian Meng, Li-Ta Hsu, Bing Xu, Xiapu Luo, and Ahmed El-\nMowafy. A gps spooﬁng generator using an open sourced vector\ntracking-based receiver. Sensors, 19(18):3993, 2019.\n[124] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha,\nZ Berkay Celik, and Ananthram Swami. Practical black-box attacks\nagainst machine learning. In Proceedings of the 2017 ACM on Asia\nconference on computer and communications security, pages 506–\n519, 2017.\n[125] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi,\nand Pascal Frossard. Universal adversarial perturbations. In Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1765–1773, 2017.\n[126] Nicholas Carlini and David Wagner. Towards evaluating the robust-\nness of neural networks. In 2017 ieee symposium on security and\nprivacy (sp), pages 39–57. IEEE, 2017.\n[127] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natu-\nral adversarial examples. In International Conference on Learning\nRepresentations, 2018.\n[128] Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial\nexamples in the physical world, 2016.\n[129] Jiajun Lu, Hussein Sibai, and Evan Fabry. Adversarial examples that\nfool detectors. arXiv preprint arXiv:1712.02494, 2017.\n[130] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rah-\nmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn\nSong. Robust physical-world attacks on deep learning visual classi-\nﬁcation. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 1625–1634, 2018.\n[131] Shang-Tse Chen,\nCory Cornelius,\nJason Martin,\nand Duen\nHorng Polo Chau. Shapeshifter: Robust physical adversarial attack\non faster r-cnn object detector. In Joint European Conference on\nMachine Learning and Knowledge Discovery in Databases, pages\n52–68. Springer, 2018.\n[132] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok.\nSynthesizing robust adversarial examples. In International confer-\nence on machine learning, pages 284–293. PMLR, 2018.\n[133] Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Ob-\nject detection with deep learning: A review. IEEE transactions on\nneural networks and learning systems, 30(11):3212–3232, 2019.\nHui Cao et al.: Preprint submitted to Elsevier\nPage 24 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\n[134] Zelun Kong, Junfeng Guo, Ang Li, and Cong Liu. Physgan: Gener-\nating physical-world-resilient adversarial examples for autonomous\ndriving. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 14254–14263, 2020.\n[135] Zuxuan Wu, Ser-Nam Lim, Larry S Davis, and Tom Goldstein. Mak-\ning an invisibility cloak: Real world adversarial attacks on object de-\ntectors. In European Conference on Computer Vision, pages 1–17.\nSpringer, 2020.\n[136] Yi Wang, Jingyang Zhou, Tianlong Chen, Sijia Liu, Shiyu Chang,\nChandrajit Bajaj, and Zhangyang Wang. Can 3d adversarial logos\ncloak humans? arXiv preprint arXiv:2006.14655, 2020.\n[137] Yang Zhang, PD Hassan Foroosh, and Boqing Gong. Camou: Learn-\ning a vehicle camouﬂage for physical adversarial attack on object\ndetections in the wild. ICLR, 2019.\n[138] Tom B Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and\nJustin Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665,\n2017.\n[139] Tong Wu, Xuefei Ning, Wenshuo Li, Ranran Huang, Huazhong\nYang, and Yu Wang. Physical adversarial attack on vehicle detector\nin the carla simulator. arXiv preprint arXiv:2007.16118, 2020.\n[140] Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won\nPark, Sara Rampazzi, Qi Alfred Chen, Kevin Fu, and Z Morley Mao.\nAdversarial sensor attack on lidar-based perception in autonomous\ndriving. In Proceedings of the 2019 ACM SIGSAC conference on\ncomputer and communications security, pages 2267–2281, 2019.\n[141] Qi Sun, Xufeng Yao, Arjun Ashok Rao, Bei Yu, and Shiyan Hu.\nCounteracting adversarial attacks in autonomous driving.\nIEEE\nTransactions on Computer-Aided Design of Integrated Circuits and\nSystems, 2022.\n[142] Yulong Cao, Ningfei Wang, Chaowei Xiao, Dawei Yang, Jin Fang,\nRuigang Yang, Qi Alfred Chen, Mingyan Liu, and Bo Li. Invisi-\nble for both camera and lidar: Security of multi-sensor fusion based\nperception in autonomous driving under physical-world attacks. In\n2021 IEEE Symposium on Security and Privacy (SP), pages 176–\n194. IEEE, 2021.\n[143] Xuesong Chen, Xiyu Yan, Feng Zheng, Yong Jiang, Shu-Tao Xia,\nYong Zhao, and Rongrong Ji. One-shot adversarial attacks on visual\ntracking with dual attention. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages 10176–\n10185, 2020.\n[144] Bin Yan, Dong Wang, Huchuan Lu, and Xiaoyun Yang. Cooling-\nshrinking attack: Blinding the tracker with imperceptible noises. In\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 990–999, 2020.\n[145] Shuai Jia, Yibing Song, Chao Ma, and Xiaokang Yang. Iou attack:\nTowards temporally coherent black-box adversarial attack for visual\nobject tracking. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 6709–6718, 2021.\n[146] Yunhan Jia Jia, Yantao Lu, Junjie Shen, Qi Alfred Chen, Hao Chen,\nZhenyu Zhong, and Tao Wei Wei. Fooling detection alone is not\nenough: Adversarial attack against multiple object tracking. In Inter-\nnational Conference on Learning Representations (ICLR’20), 2020.\n[147] Delv Lin, Qi Chen, Chengyu Zhou, and Kun He. Trasw: Tracklet-\nswitch adversarial attacks against multi-object tracking.\narXiv\npreprint arXiv:2111.08954, 2021.\n[148] Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D\nJoseph, Benjamin IP Rubinstein, Udam Saini, Charles Sutton,\nJ Doug Tygar, and Kai Xia. Exploiting machine learning to subvert\nyour spam ﬁlter. LEET, 8(1-9):16–17, 2008.\n[149] Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug\nTygar.\nThe security of machine learning.\nMachine Learning,\n81(2):121–148, 2010.\n[150] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks\nagainst support vector machines. arXiv preprint arXiv:1206.6389,\n2012.\n[151] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks\nagainst autoregressive models. In Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, volume 30, 2016.\n[152] Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea\nPaudice, Vasin Wongrassamee, Emil C Lupu, and Fabio Roli. To-\nwards poisoning of deep learning algorithms with back-gradient op-\ntimization. In Proceedings of the 10th ACM workshop on artiﬁcial\nintelligence and security, pages 27–38, 2017.\n[153] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu,\nChristoph Studer, Tudor Dumitras, and Tom Goldstein.\nPoison\nfrogs! targeted clean-label poisoning attacks on neural networks.\nAdvances in neural information processing systems, 31, 2018.\n[154] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-\nlabel backdoor attacks. 2018.\n[155] Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III,\nand Tudor Dumitras. When does machine learning {FAIL}? gen-\neralized transferability for evasion and poisoning attacks. In 27th\nUSENIX Security Symposium (USENIX Security 18), pages 1299–\n1316, 2018.\n[156] Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash.\nHidden trigger backdoor attacks. In Proceedings of the AAAI con-\nference on artiﬁcial intelligence, volume 34, pages 11957–11965,\n2020.\n[157] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer\nSingh. Universal adversarial triggers for attacking and analyzing nlp.\narXiv preprint arXiv:1908.07125, 2019.\n[158] Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and\nXinpeng Zhang. Invisible backdoor attacks on deep neural networks\nvia steganography and regularization. IEEE Transactions on De-\npendable and Secure Computing, 18(5):2088–2105, 2020.\n[159] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving\ninto transferable adversarial examples and black-box attacks. arXiv\npreprint arXiv:1611.02770, 2016.\n[160] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent\nbackdoor attacks on deep neural networks. In Proceedings of the\n2019 ACM SIGSAC Conference on Computer and Communications\nSecurity, pages 2041–2055, 2019.\n[161] Chen Zhu, W Ronny Huang, Hengduo Li, Gavin Taylor, Christoph\nStuder, and Tom Goldstein. Transferable clean-label poisoning at-\ntacks on deep neural nets. In International Conference on Machine\nLearning, pages 7614–7623. PMLR, 2019.\n[162] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai,\nWeihang Wang, and Xiangyu Zhang. Trojaning attack on neural\nnetworks. 2017.\n[163] Huma Rehman, Andreas Ekelhart, and Rudolf Mayer.\nBack-\ndoor attacks in neural networks–a systematic evaluation on multi-\nple traﬃc sign datasets. In International Cross-Domain Conference\nfor Machine Learning and Knowledge Extraction, pages 285–300.\nSpringer, 2019.\n[164] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor\nattack in cnns by training set corruption without label poisoning. In\n2019 IEEE International Conference on Image Processing (ICIP),\npages 101–105. IEEE, 2019.\n[165] Shaohua Ding, Yulong Tian, Fengyuan Xu, Qun Li, and Sheng\nZhong. Trojan attack on deep generative models in autonomous driv-\ning. In International Conference on Security and Privacy in Com-\nmunication Systems, pages 299–318. Springer, 2019.\n[166] Guiyu Tian, Wenhao Jiang, Wei Liu, and Yadong Mu. Poisoning\nmorphnet for clean-label backdoor attack to point clouds.\narXiv\npreprint arXiv:2105.04839, 2021.\n[167] Sakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell Loh, Louth\nRawshan, and Sudipta Chattopadhyay.\nModel agnostic defence\nagainst backdoor attacks in machine learning. IEEE Transactions\non Reliability, 2022.\n[168] Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen, Shengzhi\nZhang, and Kai Chen. Seeing isn’t believing: Towards more robust\nadversarial attack against real world object detectors. In Proceedings\nof the 2019 ACM SIGSAC Conference on Computer and Communi-\ncations Security, pages 1989–2004, 2019.\n[169] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K\nReiter. Accessorize to a crime: Real and stealthy attacks on state-\nHui Cao et al.: Preprint submitted to Elsevier\nPage 25 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nof-the-art face recognition. In Proceedings of the 2016 acm sigsac\nconference on computer and communications security, pages 1528–\n1540, 2016.\n[170] Stepan Komkov and Aleksandr Petiushko. Advhat: Real-world ad-\nversarial attack on arcface face id system. In 2020 25th International\nConference on Pattern Recognition (ICPR), pages 819–826. IEEE,\n2021.\n[171] Lifeng Huang, Chengying Gao, Yuyin Zhou, Cihang Xie, Alan L\nYuille, Changqing Zou, and Ning Liu. Universal physical camou-\nﬂage attacks on object detectors. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages\n720–729, 2020.\n[172] Weiwei Hu and Ying Tan. Black-box attacks against rnn based mal-\nware detection algorithms. In Workshops at the Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\n[173] Kuei-Huan Chang, Po-Hao Huang, Honggang Yu, Yier Jin, and\nTing-Chi Wang. Audio adversarial examples generation with recur-\nrent neural networks. In 2020 25th Asia and South Paciﬁc Design\nAutomation Conference (ASP-DAC), pages 488–493. IEEE, 2020.\n[174] Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, and Cho-\nJui Hsieh.\nSeq2sick: Evaluating the robustness of sequence-to-\nsequence models with adversarial examples. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, volume 34, pages 3601–\n3608, 2020.\n[175] Nicholas Carlini and David Wagner. Audio adversarial examples:\nTargeted attacks on speech-to-text. In 2018 IEEE Security and Pri-\nvacy Workshops (SPW), pages 1–7. IEEE, 2018.\n[176] Felix Kreuk, Yossi Adi, Moustapha Cisse, and Joseph Keshet. Fool-\ning end-to-end speaker veriﬁcation with adversarial examples. In\n2018 IEEE international conference on acoustics, speech and sig-\nnal processing (ICASSP), pages 1962–1966. IEEE, 2018.\n[177] Xiaolin Song, Xin Sheng, Haotian Cao, Mingjun Li, Binlin Yi, and\nZhi Huang. Lane-change behavior decision-making of intelligent\nvehicle based on imitation learning and reinforcement learning(in\nchinese). Automotive Engineering, 43(1):59–67, 2021.\n[178] Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee,\nXinyan Yan, Evangelos Theodorou, and Byron Boots.\nAgile au-\ntonomous driving using end-to-end deep imitation learning. arXiv\npreprint arXiv:1709.07174, 2017.\n[179] Adith Boloor, Karthik Garimella, Xin He, Christopher Gill, Yev-\ngeniy Vorobeychik, and Xuan Zhang. Attacking vision-based per-\nception in end-to-end autonomous driving models. Journal of Sys-\ntems Architecture, 110:101766, 2020.\n[180] Jinghan Yang, Adith Boloor, Ayan Chakrabarti, Xuan Zhang, and\nYevgeniy Vorobeychik. Finding physical adversarial examples for\nautonomous driving with fast and diﬀerentiable image compositing.\narXiv preprint arXiv:2010.08844, 2020.\n[181] Inaam Ilahi, Muhammad Usama, Junaid Qadir, Muhammad Umar\nJanjua, Ala Al-Fuqaha, Dinh Thai Huang, and Dusit Niyato. Chal-\nlenges and countermeasures for adversarial attacks on deep rein-\nforcement learning. IEEE Transactions on Artiﬁcial Intelligence,\n2021.\n[182] Jinyin Chen, Yan Zhang, Wang Xueke, Cai Hongbin, Jue Wang, and\nShouling Ji. A survey of attack, defense and related security analysis\nfor deep reinforcement learning(in chinese). Acta Automatica Sinica,\n48(1):21–39, 2022.\n[183] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li.\nTrojdrl: evaluation of backdoor attacks on deep reinforcement learn-\ning. In 2020 57th ACM/IEEE Design Automation Conference (DAC),\npages 1–6. IEEE, 2020.\n[184] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and\nPieter Abbeel. Adversarial attacks on neural network policies. arXiv\npreprint arXiv:1702.02284, 2017.\n[185] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih,\nMing-Yu Liu, and Min Sun. Tactics of adversarial attack on deep\nreinforcement learning agents.\narXiv preprint arXiv:1703.06748,\n2017.\n[186] Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforce-\nment learning to policy induction attacks. In International Confer-\nence on Machine Learning and Data Mining in Pattern Recognition,\npages 262–275. Springer, 2017.\n[187] Chaowei Xiao, Xinlei Pan, Warren He, Jian Peng, Mingjie Sun,\nJinfeng Yi, Mingyan Liu, Bo Li, and Dawn Song.\nCharac-\nterizing attacks on deep reinforcement learning.\narXiv preprint\narXiv:1907.09470, 2019.\n[188] Vahid Behzadan and Arslan Munir. Adversarial reinforcement learn-\ning framework for benchmarking collision avoidance mechanisms in\nautonomous vehicles. IEEE Intelligent Transportation Systems Mag-\nazine, 13(2):236–241, 2019.\n[189] Shafkat Islam, Shahriar Badsha, Ibrahim Khalil, Mohammed\nAtiquzzaman, and Charalambos Konstantinou. A triggerless back-\ndoor attack and defense mechanism for intelligent task oﬄoading in\nmulti-uav systems. IEEE Internet of Things Journal, 2022.\n[190] Edgar Tretschk, Seong Joon Oh, and Mario Fritz. Sequential at-\ntacks on agents for long-term adversarial goals.\narXiv preprint\narXiv:1805.12487, 2018.\n[191] Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey\nLevine, and Stuart Russell. Adversarial policies: Attacking deep\nreinforcement learning. arXiv preprint arXiv:1905.10615, 2019.\n[192] Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and\nDawn Song. Backdoorl: Backdoor attack against competitive rein-\nforcement learning. arXiv preprint arXiv:2105.00579, 2021.\n[193] Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning.\nnature, 521(7553):436–444, 2015.\n[194] Yoshua Bengio, Yann Lecun, and Geoﬀrey Hinton. Deep learning\nfor ai. Communications of the ACM, 64(7):58–65, 2021.\n[195] Bo Zhang, Jun Zhu, and Hang Su. Toward the third generation of\nartiﬁcial intelligence (in chinese). Sci Sin Inform, 50(9):1281–1302,\n2020.\n[196] Jakub Konečn`y, H Brendan McMahan, Daniel Ramage, and Peter\nRichtárik. Federated optimization: Distributed machine learning for\non-device intelligence. arXiv preprint arXiv:1610.02527, 2016.\n[197] Jakub Konečn`y, H Brendan McMahan, Felix X Yu, Peter Richtárik,\nAnanda Theertha Suresh, and Dave Bacon.\nFederated learning:\nStrategies for improving communication eﬃciency. arXiv preprint\narXiv:1610.05492, 2016.\n[198] H Brendan McMahan,\nEider Moore,\nDaniel Ramage,\nand\nBlaise Agüera y Arcas. Federated learning of deep networks using\nmodel averaging.(2016). arXiv preprint arXiv:1602.05629, 2016.\n[199] Ahmet M Elbir, Burak Soner, and Sinem Coleri. Federated learning\nin vehicular networks. arXiv preprint arXiv:2006.01412, 2020.\n[200] Hongyi Zhang, Jan Bosch, and Helena Holmström Olsson. Real-\ntime end-to-end federated learning: An automotive case study. In\n2021 IEEE 45th Annual Computers, Software, and Applications\nConference (COMPSAC), pages 459–468. IEEE, 2021.\n[201] Lingjuan Lyu, Han Yu, and Qiang Yang. Threats to federated learn-\ning: A survey. arXiv preprint arXiv:2003.02133, 2020.\n[202] Viraaji Mothukuri, Reza M Parizi, Seyedamin Pouriyeh, Yan Huang,\nAli Dehghantanha, and Gautam Srivastava. A survey on security and\nprivacy of federated learning. Future Generation Computer Systems,\n115:619–640, 2021.\n[203] Xuefei Yin, Yanming Zhu, and Jiankun Hu. A comprehensive survey\nof privacy-preserving federated learning: A taxonomy, review, and\nfuture directions.\nACM Computing Surveys (CSUR), 54(6):1–36,\n2021.\n[204] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien\nStainer. Machine learning with adversaries: Byzantine tolerant gra-\ndient descent. Advances in Neural Information Processing Systems,\n30, 2017.\n[205] Rachid Guerraoui, Sébastien Rouault, et al. The hidden vulnerability\nof distributed learning in byzantium. In International Conference on\nMachine Learning, pages 3521–3530. PMLR, 2018.\n[206] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett.\nByzantine-robust distributed learning: Towards optimal statistical\nrates.\nIn International Conference on Machine Learning, pages\n5650–5659. PMLR, 2018.\nHui Cao et al.: Preprint submitted to Elsevier\nPage 26 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\n[207] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin,\nand Vitaly Shmatikov. How to backdoor federated learning. In Inter-\nnational Conference on Artiﬁcial Intelligence and Statistics, pages\n2938–2948. PMLR, 2020.\n[208] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed\nbackdoor attacks against federated learning. In International Con-\nference on Learning Representations, 2019.\n[209] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bel-\nlet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary\nCharles, Graham Cormode, Rachel Cummings, et al. Advances and\nopen problems in federated learning. Foundations and Trends® in\nMachine Learning, 14(1–2):1–210, 2021.\n[210] Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael\nMoeller. Inverting gradients-how easy is it to break privacy in feder-\nated learning? Advances in Neural Information Processing Systems,\n33:16937–16947, 2020.\n[211] Mohammad Al-Rubaie and J Morris Chang. Privacy-preserving ma-\nchine learning: Threats and solutions. IEEE Security & Privacy,\n17(2):49–58, 2019.\n[212] Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan\nLi, Xu Liu, and Bingsheng He. A survey on federated learning sys-\ntems: vision, hype and reality for data privacy and protection. IEEE\nTransactions on Knowledge and Data Engineering, 2021.\n[213] Chunyi Zhou, Dawei Chen, Shan Wang, Anmin Fu, and Yansong\nGao. Research and challenge of distributed deep learning privacy\nand security attack(in chinese). Journal of Computer Research and\nDevelopment, 58(5):927, 2021.\n[214] Junxu Liu and Xiaofeng Meng. Survey on privacy-preserving ma-\nchine learning(in chinese). Journal of Computer Research and De-\nvelopment, 57(2):346, 2020.\n[215] Xi Wu, Matthew Fredrikson, Somesh Jha, and Jeﬀrey F Naughton. A\nmethodology for formalizing model-inversion attacks. In 2016 IEEE\n29th Computer Security Foundations Symposium (CSF), pages 355–\n370. IEEE, 2016.\n[216] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. Deep\nmodels under the gan: information leakage from collaborative deep\nlearning. In Proceedings of the 2017 ACM SIGSAC conference on\ncomputer and communications security, pages 603–618, 2017.\n[217] Guangcan Mai, Kai Cao, Pong C Yuen, and Anil K Jain. On the re-\nconstruction of face images from deep face templates. IEEE trans-\nactions on pattern analysis and machine intelligence, 41(5):1188–\n1202, 2018.\n[218] Anton Razzhigaev, Klim Kireev, Edgar Kaziakhmedov, Nurislam\nTursynbek, and Aleksandr Petiushko. Black-box face recovery from\nidentity features.\nIn European Conference on Computer Vision,\npages 462–475. Springer, 2020.\n[219] Anton Razzhigaev, Klim Kireev, Igor Udovichenko, and Aleksandr\nPetiushko. Darker than black-box: Face reconstruction from simi-\nlarity queries. arXiv preprint arXiv:2106.14290, 2021.\n[220] Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip S\nYu, and Xuyun Zhang. Membership inference attacks on machine\nlearning: A survey. ACM Computing Surveys (CSUR), 2021.\n[221] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha.\nPrivacy risk in machine learning: Analyzing the connection to over-\nﬁtting. In 2018 IEEE 31st computer security foundations symposium\n(CSF), pages 268–282. IEEE, 2018.\n[222] Ahmed Salem, Yang Zhang, Mathias Humbert, Mario Fritz, and\nMichael Backes. Ml-leaks: Model and data independent member-\nship inference attacks and defenses on machine learning models. In\nNetwork and Distributed Systems Security Symposium 2019. Internet\nSociety, 2019.\n[223] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ol-\nlivier, and Hervé Jégou. White-box vs black-box: Bayes optimal\nstrategies for membership inference. In International Conference on\nMachine Learning, pages 5558–5567. PMLR, 2019.\n[224] Minxing Zhang, Zhaochun Ren, Zihan Wang, Pengjie Ren, Zhun-\nmin Chen, Pengfei Hu, and Yang Zhang.\nMembership inference\nattacks against recommender systems. In Proceedings of the 2021\nACM SIGSAC Conference on Computer and Communications Secu-\nrity, pages 864–879, 2021.\n[225] Bo Hui, Yuchen Yang, Haolin Yuan, Philippe Burlina, Neil Zhen-\nqiang Gong, and Yinzhi Cao.\nPractical blind membership infer-\nence attack via diﬀerential comparisons. In ISOC Network and Dis-\ntributed System Security Symposium (NDSS), 2021.\n[226] Milad Nasr, Reza Shokri, and Amir Houmansadr.\nComprehen-\nsive privacy analysis of deep learning: Passive and active white-\nbox inference attacks against centralized and federated learning. In\n2019 IEEE symposium on security and privacy (SP), pages 739–753.\nIEEE, 2019.\n[227] Jiale Chen, Jiale Zhang, Yanchao Zhao, Hao Han, Kun Zhu, and\nBing Chen. Beyond model-level membership privacy leakage: an\nadversarial approach in federated learning. In 2020 29th Interna-\ntional Conference on Computer Communications and Networks (IC-\nCCN), pages 1–9. IEEE, 2020.\n[228] Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, and\nXuyun Zhang. Source inference attacks in federated learning. In\n2021 IEEE International Conference on Data Mining (ICDM), pages\n1102–1107. IEEE, 2021.\n[229] Umang Gupta, Dimitris Stripelis, Pradeep K Lam, Paul Thompson,\nJosé Luis Ambite, and Greg Ver Steeg. Membership inference at-\ntacks on deep regression models for neuroimaging. In Medical Imag-\ning with Deep Learning, pages 228–251. PMLR, 2021.\n[230] Anh Nguyen, Tuong Do, Minh Tran, Binh X Nguyen, Chien Duong,\nTu Phan, Erman Tjiputra, and Quang D Tran. Deep federated learn-\ning for autonomous driving. arXiv preprint arXiv:2110.05754, 2021.\n[231] Davinder Kaur, Suleyman Uslu, Kaley J Rittichier, and Arjan Dur-\nresi. Trustworthy artiﬁcial intelligence: A review. ACM Computing\nSurveys (CSUR), 55(2):1–38, 2022.\n[232] Luciano Floridi. Establishing the rules for building trustworthy ai.\nNature Machine Intelligence, 1(6):261–262, 2019.\n[233] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu,\nXiaolin Hu, and Jianguo Li. Boosting adversarial attacks with mo-\nmentum. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 9185–9193, 2018.\n[234] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris\nTsipras, and Adrian Vladu. Towards deep learning models resistant\nto adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n[235] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang,\nZhou Ren, and Alan L Yuille. Improving transferability of adversar-\nial examples with input diversity. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages\n2730–2739, 2019.\n[236] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading de-\nfenses to transferable adversarial examples by translation-invariant\nattacks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4312–4321, 2019.\n[237] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated\ngradients give a false sense of security: Circumventing defenses to\nadversarial examples. In International conference on machine learn-\ning, pages 274–283. PMLR, 2018.\n[238] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui\nHsieh. Zoo: Zeroth order optimization based black-box attacks to\ndeep neural networks without training substitute models. In Pro-\nceedings of the 10th ACM workshop on artiﬁcial intelligence and\nsecurity, pages 15–26, 2017.\n[239] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin.\nBlack-box adversarial attacks with limited queries and information.\nIn International Conference on Machine Learning, pages 2137–\n2146. PMLR, 2018.\n[240] Jonathan Uesato, Brendan O’donoghue, Pushmeet Kohli, and Aaron\nOord. Adversarial risk and the dangers of evaluating against weak\nattacks. In International Conference on Machine Learning, pages\n5025–5034. PMLR, 2018.\n[241] Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, and Boqing\nGong.\nNattack: Learning the distributions of adversarial exam-\nples for an improved black-box attack on deep neural networks. In\nHui Cao et al.: Preprint submitted to Elsevier\nPage 27 of 28\nEmerging Threats in Deep Learning-Based Autonomous Driving\nInternational Conference on Machine Learning, pages 3866–3876.\nPMLR, 2019.\n[242] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-\nbased adversarial attacks: Reliable attacks against black-box ma-\nchine learning models. arXiv preprint arXiv:1712.04248, 2017.\n[243] Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong\nZhang, and Jun Zhu. Eﬃcient decision-based black-box adversarial\nattacks on face recognition. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages 7714–\n7722, 2019.\nHui Cao et al.: Preprint submitted to Elsevier\nPage 28 of 28\n",
  "categories": [
    "cs.CR",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2022-10-19",
  "updated": "2022-10-19"
}