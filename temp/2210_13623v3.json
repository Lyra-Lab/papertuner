{
  "id": "http://arxiv.org/abs/2210.13623v3",
  "title": "Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook",
  "authors": [
    "Baihan Lin"
  ],
  "abstract": "In recent years, reinforcement learning and bandits have transformed a wide\nrange of real-world applications including healthcare, finance, recommendation\nsystems, robotics, and last but not least, the speech and natural language\nprocessing. While most speech and language applications of reinforcement\nlearning algorithms are centered around improving the training of deep neural\nnetworks with its flexible optimization properties, there are still many\ngrounds to explore to utilize the benefits of reinforcement learning, such as\nits reward-driven adaptability, state representations, temporal structures and\ngeneralizability. In this survey, we present an overview of recent advancements\nof reinforcement learning and bandits, and discuss how they can be effectively\nemployed to solve speech and natural language processing problems with models\nthat are adaptive, interactive and scalable.",
  "text": "REINFORCEMENT LEARNING AND BANDITS FOR SPEECH AND\nLANGUAGE PROCESSING: TUTORIAL, REVIEW AND OUTLOOK\nA GENTLE INTRODUCTION\nBaihan Lin\nColumbia University\nNew York, NY 10027\nbaihan.lin@columbia.edu\nOctober 20, 2023\nABSTRACT\nIn recent years, reinforcement learning and bandits have transformed a wide range of real-world\napplications including healthcare, finance, recommendation systems, robotics, and last but not\nleast, the speech and natural language processing. While most speech and language applications of\nreinforcement learning algorithms are centered around improving the training of deep neural networks\nwith its flexible optimization properties, there are still many grounds to explore to utilize the benefits\nof reinforcement learning, such as its reward-driven adaptability, state representations, temporal\nstructures and generalizability. In this survey, we present an overview of recent advancements of\nreinforcement learning and bandits, and discuss how they can be effectively employed to solve speech\nand natural language processing problems with models that are adaptive, interactive and scalable.\nKeywords Reinforcement Learning · Bandits · Speech Processing · Natural Language Processing · Survey · Perspective\n1\nIntroduction\nAs two cornerstones of modern day technologies, speech processing and natural language processing (NLP) are innately\nsequence learning problems to extract information from these linguistic or speech signals and provide insights into\ninteractive systems to communicate in human understandable languages. The sequential and interactive nature of these\nproblems can make them well-suited into the algorithmic framework of reinforcement learning (RL). In a reinforcement\nlearning setting, an agent interacts with an environment through observations and actions, and based on the reward\nfeedback attributed by the underlying reward function of this environment, the agent learns how to perform the task of\ninterest through trials and errors. While the successful applications of reinforcement learning have been highlighted\nby a wide range of surveys in many real-world engineering domains such as robotics [1], vision [2], finance [3],\nhealthcare [4], linguistics [5], and energy management [6], there have not been one for the rich community of both the\nspeech and language domains. This is the first survey that emphasizes the synergy among the growing fields of the\nspeech processing, natural language processing and the reinforcement learning. We aim to fill this gap by adopting\na complete, timely and classical view of the reinforcement learning problems and their connections to speech and\nlanguage processing.\nThis survey distinguishes itself from previous ones by introducing, for the first time, the application of bandits and online\nlearning to speech processing tasks. While deep reinforcement learning has gained popularity for reward-driven policy\noptimization in neural network architectures, other speech or language domains may be better suited to lightweight\nrepresentations in the bandits and online learning framework. Additionally, over the past two years, there have been\nsignificant developments in different variants of reinforcement learning scenarios, such as inverse reinforcement learning,\noffline reinforcement learning, imitation learning, and behavioral cloning. Despite the successful applications outside\nthe speech domain, these new problems can benefit from more exposure to the speech and language communities due\nto their unique perspectives and advantages. This survey provides a comprehensive overview of these novel problem\nsettings as well as their state-of-the-art solutions (including the latest work in the field of large language models).\narXiv:2210.13623v3  [cs.AI]  19 Oct 2023\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nAs this interdisciplinary field is still a fast growing research area at its early stages, the intended target audience for\nthis survey are not just new researchers in the field, but also specialists of the speech and language domains that\nhopefully can gain insights and inspirations from the recent advances in reinforcement learning. Accompanying\nthe INTERSPEECH 2022 tutorial “Reinforcement Learning and Bandits for Speech and Language Processing”, the\ngoal of this survey is not to provide an exhaustive review of the fields, but to first provide an applied tutorial of the\nmethodologies in reinforcement learning, and then guide the readers through a series of prototypical examples of how\nreinforcement learning can be effectively applied to major speech and language tasks. We hope these case studies\nmotivate the readers to rethink their daily tasks as reinforcement learning problems and encourage new discussions.\nContents\n1\nIntroduction\n1\n2\nWhy do we want reinforcement learning in speech and language processing\n3\n3\nA Concise Tutorial of Reinforcement Learning and Bandits\n5\n3.1\nPreliminaries\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.2\nMulti-Armed Bandits (MAB) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.3\nContextual bandits\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.4\nReinforcement learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.5\nInverse reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.6\nImitation learning and behavioral cloning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n4\nReinforcement Learning Formulation for Speech and Language Applications\n20\n4.1\nAutomatic speech recognition (ASR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.2\nSpeaker recognition and diarization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.3\nSpoken language understanding (SLU) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.4\nNatural language understanding (NLU)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.5\nSequence generation and text-to-speech (TTS) synthesis\n. . . . . . . . . . . . . . . . . . . . . . . .\n28\n4.6\nNatural language generation (NLG)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n4.7\nLarge language models (LLM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.8\nConversational recommendation systems (CRS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n5\nEmerging Reinforcement Learning Strategies\n35\n5.1\nDeep reinforcement learning and bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n5.2\nBatched and offline reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5.3\nTransfer learning in reinforcement learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n6\nOpen Questions and Challenges\n38\n6.1\nMulti-agent settings in speech and language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n6.2\nMulti-objective training and human priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n7\nSummary and Resources\n41\n8\nNote and Acknowledgements\n42\n2\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 1: Example of a speech and language processing system\nPreview. The survey is organized in the following way: First, we briefly introduce the basic concept of reinforcement\nlearning and bandits, as well as the major variant problem settings in this machine learning domain. Second, we translate\nvarious speech and language tasks into the reinforcement learning problems and show the key challenges. Third, we\nintroduce some reinforcement learning and bandit techniques and their varieties for speech and language tasks and their\nmachine learning formulations. Fourth, we present several state-of-the-art applications of reinforcement learning in\ndifferent fields of speech and language. Lastly, we will discuss some open problems in reinforcement learning and\nbandits to show how to further develop more advanced algorithms for speech and language research in the future.\n2\nWhy do we want reinforcement learning in speech and language processing\nA speech and natural language processing system usually involves many components. For instance, Figure 1 is an\nexample of a voice command system to play music. It starts with a speech recognition engine to transcribe the speech\ninto text. A speech language understanding components first uses natural language processing techniques to parse\nout the semantic structure, its intents (e.g. command actions) and then utilizes available knowledge graphs to extract\nmachine understandable symbolic relationships for downstream information retrieval. Then the system will locate the\nentry in the databases that most closely matches the entity of query. Finally, the music app is activated playing the entry.\nThese components are often data-rich machine learning models or data analytical tools pretrained on existing data.\nThe iterative process of such a speech or language-based system is a cycle from more users to more data to smarter\nalgorithms to better products and finally back to more users (Figure 2A).\nFrom a industrial product point of view, these system components (which are sometimes called services) interact with\nthe underlying databases in various ways. Figure 2B shows an example relationship between the services and the\nFigure 2: (A) Cycle of the machine learning-based product development. (B) Example relationships among multiple\nservices and multiple databases, each with their own update schedules or frequencies.\n3\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 3: Data uncertainty in speech and language processing.\nFigure 4: (A) The reinforcement learning problem. (B) The exploration vs exploitation tradeoff.\ndatabases they train on. One service might depend on the information flow from another or several other services.\nOne database might be accessed by one or more services, and also at the same time linked with another databases\n(e.g. in relational databases). These databases are usually updated in different frequencies. For instance, one database\nmight updates regularly every 6 hours, while another database updates only sporadically whenever a new user signs\nup. One database might update its record everytime another database updates. Since we train our service models on\nthese databases, one challenge of model training for different services is to determine when or how often they are\ntrained given the updates in the databases. One might wish to train the service model everytime the available pool\nof historical data is updated, so as to get the best performing model. On the other hand, given the large size of the\nhistorical data and model parameters in industrial setting, iteratively retraining all the models at each database update\nwould be too computationally expensive and environmentally irresponsible. As a result, we want our models to ideally\nbe able to learn incrementally, and strategically given its performance or metric. Such a model would ideally deal with\nthe uncertainty in the data: exploit what we have learned so far to make the best decision in deployment, while also\nexploring enough rounds in relatively unfamiliar knowledge domains to gain better understanding of all possible actions\nit can take. This strategy of dealing with uncertainty is usually called the Exploration vs. Exploitation dilemma tradeoff.\nUncertainty is everywhere in speech and language domains. Figure 3 provides a few examples. For instance, the same\nwords or sentences can have ambiguious meanings. The speech properties and language patterns can vary significantly\nwhen the contexts are different (e.g. a podcast interview vs. a commencement speech). The same statements can also\nhave different levels of confidence, depending on their contexts and wording. New users or data streams might introduce\ndistributional shifts and cold start issues. Other than innate noises or uncertainty in the data, there are also systematic\nuncertainty in machine learning systems. For instance, the model might not be flexible to handel the variability in real\nworld situations. There might be errors and noises in the measurement systems, both in deployment and in training\n(e.g. errors in the labels in the training data). Since there are many hyperparameters to design the model, techniques\nlike neural architecture search (and graduate student descent) can introduce uncertainty in model structures. Even with\nthe same models, different training procedures can give rise to different performance and fidelity. Finally, during the\ninference phase, models can give prediction with different confidence levels. [7] covers different approaches to measure\nand quantify uncertainty in neural networks.\nReinforcement learning is the learning of trials and errors to deal with uncertainty in data. Usually, a reinforcement\nlearning problem involves two subjects: an agent and an environment. The agent interact with the environment and the\nenvironment provides reinforcement to help the agent learn. More formally, at each time step t, the agent takes an action\nAt to affect the environment, and the environment transitions to the next state St and reveals a reward feedback Rt to\nthe agent for it to update its policy (Figure 4A). The exploration vs. exploitation tradeoff then resides on a spectrum: on\nthe one end, if we exploit, we obtain short-term rewards by reaching good deployment performance given what we\n4\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 5: Five classes of reinforcement learning-related problems. The blocks are models or systems. The circles are\nintermediate scalars or vectors. The white ones are unknown quantities or systems. The grey ones are observed variable,\nor sometimes, available historical data. The orange ones are the objectives to solve.\nhave known so far; on the other end, if we explore, we obtain long-term rewards because we are building more reliable\nmodels by collecting knowledges in uncertain problem spaces (Figure 4B). Reinforcement learning algorithms can\nnavigate between these two ends to get a dynamic strategy according to the problem and performance. It is adaptive,\nso it can continually learn to accomodate for the changes in the data. It is about optimizing for the expected future\nrewards, so it can innately predict or plan ahead for future events. Since it is mostly driven by its reward feedback, it is\nalso generalizable to new tasks or uncertainty patterns by modifying its reward structure. These algorithmic properties\nsuggest that speech and language models may benefit from adopting a reinforcement learning approach.\nOne useful perspective to approach an applied task of interest is to think the following questions: what are the costs and\nbenefits in this learning system? When should this system explore (as opposed to exploit)? And how to properly explore\nthe problem space? These three questions can help us better formulate the applied problems into RL framework.\n3\nA Concise Tutorial of Reinforcement Learning and Bandits\nIn this section, we will give an applied tutorial of five classes of reinforcement learning-related problems. Figure 5\noutlines the difference in their problem settings, where the circles are intermediate scalars or vectors, the white blocks\nare quantity or system that are unknown, the grey blocks are the observed variable or available historical data, and the\norange blocks are the objectives that we are attempting to solve. We note that this visual representation is similar to a\nprobabilistic graphical model, but that is not intended nor implied.\n3.1\nPreliminaries\nMost of the times, solving different classes of reinforcement learning can be modeled by a mathematical framework\ncalled the Markov decision processes (MDP) [8]. An MDP is defined by the tuple (S, A, T , R, γ):\n• S: a set of possible states\n• A: a set of actions\n• T : a transition function, defined as T (s, a, s′) = Pr(s′|s, a), where s, s′ ∈S and a ∈A\n• R : a reward function, S × A × S 7→R\nThis is a generalized formulation of the MDP. As we will see in the following the sections, some problems might only\ninvolve a subset of these notations (e.g. the bandits may not have states). Typically, the objective of the learning process\nis to maximize the long-term reward, assuming an infinite-horizon decision process. In other words, we want to find a\n5\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nAlgorithm 1 Multi-Armed Bandits Problem\n1: for t = 1, 2, 3, · · · , T do\n2:\nr(t) is drawn according to Pr\n3:\nPlayer chooses an action at = πt(t)\n4:\nFeedback rat,t(t) for the arm at is revealed\n5:\nPlayer updates its policy πt\n6: end for\npolicy function, π : S 7→A, which specifies the action to take in a given state (which in the bandit scenario, might be a\ncontext or nothing), so that the cumulative reward is maximized.\n3.2\nMulti-Armed Bandits (MAB)\nConsider yourself at a casino, and in front of you, there are many bandit machines, each with an arm. For starter, we\nassume that they each have a static reward payoff, which is unknown to you. To gain better understanding of a certain\nbandit arm, you will need to pull that arm to gain a reward feedback revealed only for that arm. You want to play for T\nrounds, and maximize your cumulative rewards. Back to our previous introduction to the Exploration vs Exploitation\ntradeoff: if we exploit entirely, we make long-term sacrifices by potentially missing unknown optimal action, because\nwe are only choosing the best arm given the current information; if we explore entirely, we make short-term sacrifices\nby missing out known rewards, because we are always gathering more information. Due to the effectiveness of bandit\nalgorithms in balancing the exploration vs exploitation tradeoff, they are widely applied in finance [9, 10], epidemic\ncontrol [11, 12], hyperparameter tuning [13, 14], recommendation systems [15, 16], clinical prescription [17, 18],\nhuman behavioral modeling [19, 20, 21] and A/B testing [22, 23].\nAs in algorithm 1, for each round, the player chooses an action at given its policy πt, and ra, the feedback for only the\nchosen arm is revealed, which is often called bandit feedback. The player then updates its policy πt given the feedback.\nIf the player plays for T trials, the goal of the agent is to maximize the total payoff (sum of rewards) P\nt∈1...T rt. In\nanother term, we can rephrase the performance metric to be minimizing total regret P\nt∈1...T (r∗−rt), where r∗is the\nbest arm reward. Other evaluation metrics include maximizing the average reward, and maximize the percentage of\noptimal action selection.\nHere the agent is an algorithm, and the environment is the bandit task. Since they are both stochastic, the goal of this\nbandit task is empirically to maximize the expected total sum of rewards, i.e. E[P\nt∈1...T rt]. This learning process of\ncontinually updating its policy by sequentially interacting with the bandit environment is also called online learning. To\nreport the results from an online learning environment, we can plot the above performance metrics over time steps (or\ntrials). From this learning curve, we may compare different algorithms and select the best one.\nWe will introduce a few strategies how bandit algorithms deal with the exploration vs exploitation tradeoff.\nThe first one is simply explore first (by taking random actions for N rounds) and then exploit (by choosing the arm\nwith the highest estimated expected reward, the greedy approach). The estimated expected reward is defined by a value\nfunction called Q, where ˆQt(a) is the estimated expected reward for action arm a at time t. A simple formulation of the\nˆQt(a) is its average past observed rewards:\nˆQt(a) := ˆµa = r1 + r2 + ... + rna\nna\n(1)\nwhere ri is the observed reward for action arm a, and na is the number of times action arm a has been played. ˆQt=0\na\n= 0.\nThere are many variants to this strategy by setting N = f(N). For instance, one can explore once (N = 1) for each\naction first, or the first ϵ proportion of rounds (i.e. the first ϵ · T steps, where 0 ≤ϵ ≤1). In the textbook “Introduction\nto reinforcement learning” [8], a bandit simulation example of 10 arms with different average rewards trained over 1000\ntime steps compares six algorithms: Random (choosing randomly at each round), Greedy (choosing only the arm with\nthe maximum Q value), Explore-First-20 (randomly choosing for only the first 20 rounds as pure exploration, and then\nchoosing the arm with the maximum Q value), Explore-First-100, Explore-First-200, Explore-First-500. The random\nand purely greedy solutions both yield sub-optimal performance. Among the Explore-First-N agents, the higher the\nN (the exploration steps), the better the long-term cumulative rewards they get, but at the short-term initial rounds,\nespecially when they are purely exploring, they receive rewards at the chance level, which might be a risky take for\nreal-world problems.\n6\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nAlgorithm 2 Upper Confidence Bound (UCB) Algorithm\n1: Initialize ˆQa′ = 0, na′ = 0, ∀a′ ∈A\n2: for t = 1, 2, 3, · · · , T do\n3:\nChoose at =\n(\narg maxa′∈A ˆQa′ +\nq\n2 ln t\nna′ ,\nif na′ ̸= 0\n−∞,\notherwise\n4:\nObserve rat,t(t) for the arm at\n5:\nˆQa′ =\nˆ\nQa′·nat+rat,t(t)\nnat+1\n6:\nnat = nat + 1\n7: end for\nA better strategy would be to dynamically switching between exploiting (taking the greedy action) and exploring (taking\nthe random action). ϵ-Greedy algorithm describes a coin flipping process that, for a probability of ϵ (0 ≤ϵ ≤1), the\nagent take a random action at the current round, and for a probability of 1 −ϵ, it takes the greedy action [24, 25].\nThis approach doesn’t explicitly specify a exploration phase and a exploitation phase, and generally yields a more\nbalanced learning process. In the bandit simulation example, if we compare ϵ-Greedy (ϵ = 0), ϵ-Greedy (ϵ = 0.01)\nand ϵ-Greedy (ϵ = 0.1), we observe a better cumulative long-term reward for larger ϵ, i.e. more exploration. In the\nshort-term, similarly, smaller ϵ can temporarily overtake larger ones by exploiting early, but they reach their plateau\nfaster at lower payoff.\nIntuitively, at early rounds, the agent know little about the reward distribution of each arm, so it makes sense to explore\nmore at early rounds. However, when we use methods like ϵ-Greedy, the explored arms will have their Q values\nestimated in the first few steps larger than those of the unexplored arms (which are usually initialized zero), which\ndiscourages exploration. One strategy to encourage exploration in the early rounds, is to set all Q values with a large\nnumber to start with. This strategy of optimistic initial values, can innately inject exploration (even in the greedy turns)\nto under-explored action arms [24, 26]. Empirically (in bandit simulations), this trick yield better cumulative long-term\nrewards by trading off the rewards in very early rounds.\nAnother strategy would be to set a schedule for the exploration factor ϵ in ϵ-Greedy, such that it reflects certain priors\nthat can improve the learning. For instance, if we wish to encourage exploration in initial stages, we can use a decaying\nϵ function (e.g. ϵ(t) ∝1\nt ). Other variants of an ϵ schedule can be a linear function, a step function, or oscillatory\nfunction if there are periodic changes in the environment. Despite proven poly-logarithmic bounds for these variants, it\nwas reported limited advantages in these heuristics [27]. One can also potentially bind this scheduling with detection\nmechanism for paradigm shifts settings.\nWhile useful, how to decide which scheduling function to use for the exploration? What if the environment is non-\nstationary and has irregular changes of reward distributions? And how best to motivate random exploration in a principle\nway? Probability matching (PM) is a decision making strategy that choose actions given the probability of their reward\nin a stochastic setting [28, 29]. In other words, the probability of choosing arm a at time t:\npt(a) ∝\nˆQt(a)\nP\na′∈A ˆQt(a′)\n(2)\ni.e. how likely the action arm is to be optimal. Boltzmann exploration (or the SoftMax strategy) is such a formulation\nthat samples a random choice according to the Gibbs distribution:\npt(a) =\ne\nˆ\nQt(a)\nτ\nP\na′∈A e\nˆ\nQt(a′)\nτ\n(3)\nThe hyperparameter τ is called temperature, which is a user-specified quantity to control the degree of exploration. If τ\nis set to be zero, it is full exploitation. If it is set to be infinity, it is full exploration.\nSimilar to ϵ-Greedy, we can also build variants of Boltzmann exploration with schedules of the temperature τ (e.g.\nhaving the temperature decays with the number of rounds played). However, the dependency of the user to choose the\nexploration level (either ϵ or τ) at each step can be problematic (e.g. unlucky initial experience) and a principled way to\nselect them remains elusive. One strategy would be to use other quantity to guide exploration, such as adopting the\nuncertainty-guided exploration. [30] proposes the family of Upper Confidence Bound (UCB) algoritms as a elegant\n7\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nAlgorithm 3 Thompson Sampling (TS) Algorithm\n1: Initialize Sa′ = 0, Fa′ = 0, ∀a′ ∈A\n2: for t = 1, 2, 3, · · · , T do\n3:\nˆQt(a′) ∼Beta(Sa′ + 1, Fa′ + 1)∀a′ ∈A\n4:\nChoose at = arg maxa′∈A ˆQt(a′)\n5:\nObserve rat,t(t) for the arm at\n6:\nSat = Sat + rat,t(t)\n7:\nFat = Fat + (1 −rat,t(t))\n8: end for\nApproach\nExploitation\nExploration\nSwitching to random exploration\nat = arg maxa∈A ˆQta\nat = random action\nUncertainty-guided exploration\nat = arg maxa∈A ˆQta + measure of uncertainty\nProbability matching\nUse ˆQta to define pt(a)\nSelect at according to pt(a)\nTable 1: Summary of bandit action selection strategies\nalgorithmic implementation of the idea of optimism in the face of uncertainty by [31]. As in algorithm 2, each action\narm is represented by their estimated expected reward ˆQa and a confidence bound\nq\n2lnN\nna as their uncertainty. The\nagent simply chooses the action that maximize the upper confidence bound:\nat = arg max\na′∈A\nˆQa′ +\nr\n2 ln N\nna′\n(4)\nwhere N is the number of rounds played so far and na is the number of time this arm a has been chosen. We quickly\nnotice that, the first term is the exploitation term and the second term is the exploration term. (Upper Confidence\nBound-1) UCB1 is the most common implementation, which first let each arm to be played at least once before adopting\nthe upper confidence bound action selection policy. Similar to ϵ-Greedy and Boltzmann exploration, one can create\nvariants of UCB1 by introducing a schedule parameter C to the exploration: ˆQa + C ·\nq\n2 ln N\nna .\nThe UCB algorithms are specified from a frequentist point of view, one can also adopt a Bayesian approach to this\nstrategy. A Bayesian bandit would represent the action value Q function as a probability distribution p( ˆQt(a)) and\ndirectly compute the probability distribution of the Q values using the Bayes rule:\np( ˆQt(a)|Da) ∝p(Da| ˆQt(a)) · p( ˆQt(a))\n(5)\nwhere Da is the observed data (past rewards revealed when this arm is selected): Da = {r1, r2, r3, · · · , rna}. The\naction selection strategy for Bayesian bandits would be, instead of using the Q values directly, the agent first sample\nfrom the probability distribution p( ˆQt(a)) of the Q value for each action arm. Thompson sampling [32] (TS) is one\nsuch algorithm that shows a competitive performance with other approaches [33]. In the Bernoulli bandit problem\n(where the rewards are drawn from a Bernoulli distribution: Ra ∼Ber(pa ∈[0, 1])), Thompson sampling is proven to\nbe asymptotically optimal [34]. The implementation is very straightforward. As in algorithm 3, we can express our\nuncertainty about ˆQt(a) with:\np( ˆQt(a)) ∼Beta(Sa + 1, Fa + 1)\n(6)\nwhere Sa and Fa are two values stand for “success” and “failure”. If the reward is 1, then we increment the “success”\nSa by 1. And if the reward is 0, then we increment the “failure” Fa by 1. Since this is a class of probability matching\nmethod, we select the action based on p( ˆQa|Da) where Da = {r ∈[0, 1]}. In our case, we randomly sample an\nestimate ˆQt(a′) from the posterior, i.e. the Beta distribution of each arm parameterized by Sa and Fa, and then choose\nthe action arm at that has the maximum ˆQt(a′).\nAs a recap, we cover five strategies: exploration first, optimistic initial values, parameter schedule, guided exploration\nand probability matching. We may represent the action value functions ˆQt(a) in two approaches: the Frequentist\n8\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nAlgorithm 4 Contextual Bandits (CB) Problem\n1: for t = 1, 2, 3, · · · , T do\n2:\n(x(t), r(t)) is drawn according to Px,r\n3:\nContext x(t) is revealed to the player\n4:\nPlayer chooses an action at = πt(x(t))\n5:\nFeedback rat,t(t) for the arm at is revealed\n6:\nPlayer updates its policy πt\n7: end for\nAlgorithm 5 Linear Upper Confidence Bound (LinUCB) Algorithm\n1: Initialize ct ∈R+, Aa = Id, ba = 0d×1, ∀a ∈A\n2: for t = 1, 2, 3, · · · , T do\n3:\nObserve features xt ∈Rd\n4:\nfor all a ∈A do\n5:\nˆθa = A−1\na ba\n6:\npt,a = ˆθ⊤\na xt + ct\nq\nx⊤\nt A−1\na xt\n7:\nChoose arm at = arg maxa∈A pt,a\n8:\nObserve feedback rat,t\n9:\nAat = Aat + xtx⊤\nt\n10:\nbat = bat + rat,txt\n11: end for\napproach ( ˆQt(a) := ˆµa =\nr1+r2+...+rna\nna\n) and the Bayesian approach (express the uncertainty about ˆQt(a) with\np( ˆQt(a))). As summarized in table 1, there are three action selection strategies. We can explicitly separate the\nexploration (by taking random action) and the exploitation (by taking greedy action) into different phases. We can use\nuncertainty to encourage optimism and guide exploration by combining the exploitation (the action estimate) and the\nexploration (the measure of uncertainty) together into the argmax criterion. Finally, in probability matching approach,\nwe use ˆQta to define pt(a) (exploitation) and select at according to pt(a) (exploration). Some effective algorithms\ninclude ϵ-Greedy, upper confidence bound (UCB), and Thompson sampling (TS).\nWe wish to point out that, the bandits by itself is a rich field, and there are many variants to these solutions. The\nreward distributions can be independent or correlated [35], stochastic or adversarial [36, 37, 38], static or non-stationary\n[39, 40]. The action space can be discrete or continuous [41, 42], finite or infinite number of arms [43], single or\nmulti-dimensional (combinatorial) [44, 45]. The time horizon can be infinite or finite [46]. There can be a single agent\nor a population of bandit agents [12]. The feedbacks can be bandit or partially full feedback [47], always revealed or\nonly sporadically revealed (sparse or missing feedback) [48], and sometimes even budgeted [49, 45, 50]. As such, one\ncan choose from many different subproblems of bandits, such as adversarial bandits, combinatorial bandits, dueling\nbandits, bandits with a budget or knapsack, multi-play bandits, bandits with dependent arms and finally the most\nimportant variant of all, the contextual bandits. For readers with more interests, [51] is a good introduction to different\nclasses of bandit algorithms.\n3.3\nContextual bandits\nThe contextual bandits (CB) describe the scenario of multi-armed bandits that also receives side information as decision\nmaking contexts [52]. As in algorithm 4, the contextual bandits problem is defined as follows. At each time point (or\niteration) t, before choosing the arm at ∈A, agent observes an M-dimensional context, xt, as a feature vector of M\nvariables. The reward function r(t) is drawn according to Px,r, where ra(t) ∈[0, 1] is a reward at time t associated\nwith the arm a ∈A and the context at the current iteration xt (or alternatively, we can define a joint distribution over\n(x, r)). The agent uses this context as a side information, along with the rewards of the arms played in the past, to\nchoose which arm to play in the current iteration. The objective of this variant problem is to learn the relationship\nbetween the context and reward, in order to find the best arm-selection policy for maximizing cumulative reward over\nthe time horizon.\nFor introduction purpose, here we assume a Bernoulli bandit with binary reward, i.e. r ∈[0, 1]. We can adopt a linear\nassumption [53], that the expected reward is a linear function of the context:\n9\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nAlgorithm 6 Contextual Thompson Sampling (CTS) Algorithm\n1: Initialize: Ba = Id, ˆθa = 0d, fa = 0d, ∀a ∈A.\n2: for t = 1, 2, 3, · · · , T do\n3:\nReceive context xt\n4:\nfor ∀a ∈A, sample ˜θa from the N(ˆθa, v2B−1\na )\n5:\nChoose arm at = arg maxa∈A x(t)⊤˜θa\n6:\nReceive reward rat\n7:\nBat = Bat + xtxT\nt\n8:\nfat = fat + xtrat\n9:\nˆ\nθat = B−1\nat fat\n10: end\nE[ra|xt] = θ⊤\na xt\n(7)\nwhere µa is an unknown weight vector associated with the arm k. The Linear Upper Confidence Bound (LinUCB)\nalgorithm is the contextual variants of the UCB algorithm [53]. The action value function now becomes ˆQt(xt, a) and\ncan be used to estimate E[Ra|xt]\nWe can formulate the problem as a least-square ridge regression problem on its reward function:\nRa(xt) = fa(xt) + ϵ = x⊤\nt θ + a + ϵ\n(8)\nwhere fa is an arbitrary function that maps the context to the reward, θa ∈Rd and ϵ is the noise or error. At each time\nstep t and given the context xt, the weight for the action value solution becomes:\nˆQt(xt, a) = x⊤\nt ˆθa\n(9)\nˆθa = (D⊤\na Da + Id)−1D⊤\na Ca\n(10)\nwhere Da and Ca are the historical contexts and rewards observed when this arm a is chosen: Da =\n\n\nx1\nx2\n...\nxn\n\n, Ca =\n\n\nr1\nr2\n...\nrn\n\n.\nThe action selection will then depends on the estimated rewards with the mapping weight given the context:\nˆSt(xt, a) =\nq\nx⊤\nt (D⊤\na Da + Id)−1xt\n(11)\nat = arg max\na∈A\n( ˆQt(xt, a) + α · St(xt, a))\n(12)\nwhere similar to the non-contextual version of UCB, here we have two terms in the argmax option, the first term\nˆQt(xt, a) is for exploitation and the second term St(xt, a) is for the uncertainty-guided exploration, modulated by the\nexploration factor α.\nThis is the analogy to the linear regression problem, while empirically one can simply incrementally compute the\nsolution based on online feedback as in algorithm 5: we have matrix Aa which characterizes the covariance of\ncontexts, and ba which characterizes the reward mapping; and at the action selection stage, we take the argmax of\n(A−1\na ba)⊤xt + ct\nq\nx⊤\nt A−1\na xt; at the update stage, we simply incrementally update the covariance matrix Aa and\nreward mapping ba for the selected arm at.\nSimilarly, we can formulate a linear contextual version of the Thompson sampling algorithm, the Contextual Thompson\nSampling [54] algorithm. We consider the general Thompson Sampling, where the reward rat for choosing arm at at\ntime t follows a parametric likelihood function p(rt|˜θa). The posterior distribution at time t:\np(˜θi|rt) ∝Pr(rt|˜θi)\n(13)\n10\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nAlgorithm 7 Reinforcement Learning (RL) Problem\n1: for t = 1, 2, 3, · · · , T do\n2:\nAgent makes an observation o(t) to the state s(t)\n3:\nAgent chooses an action given the current state or observation: at = πt(o(t))\n4:\nEnvironment progresses to the next step st+1 given the action at\n5:\nAgent receives a reward feedback rat,st(t) for the action at taken on state st\n6:\nAgent updates its policy πt\n7: end for\nTo incorporate contextual information, p(˜θi) is given by a multivariate Gaussian distribution N(ˆθa, v2B−1\na ), where\nBa(t) = Id + Pt−1\nτ=1 xτx⊤\nτ , and where d is the size of the context vectors xt, v = R\nq\n24\nϵ d ln( 1\nγ ) with r > 0, ϵ ∈[0, 1],\nγ ∈[0, 1], and ˆθa(t) = Ba(t)−1(Pt−1\nτ=1 xτrτ). At every step t for each action arm a, the algorithm samples a\nd-dimensional context weight vector ˜µi from N( ˆθa(t), v2Ba(t)−1). The agent would select the arm at that maximizes\nx⊤\nt ˜θa, and obtains reward rt. [55] is a survey of different contextual bandits algorithms.\n3.4\nReinforcement learning\nReinforcement learning is a general-purpose framework for decision-making. Reinforcement learning is for an agent\nwith the capacity to act. Each action taken by the agent influences the agent’s future state. The success of this problem\nis measured by a scalar reward signal. As in the bandits and contextual bandits cases, the goal is to select actions to\nmaximize cumulative future rewards.\nAs hinted in the section of “Preliminaries”, reinforcement learning defines a class of algorithms for solving problems\nmodeled as Markov decision processes (MDP). Other than previously defined tuple (S, A, T , R), γ ∈[0, 1] is a\ndiscounting factor for past or future rewards (the further into the past or future, the less impact of that reward has\non current action choice). Typically, the objective is to maximize the discounted long-term reward, assuming an\ninfinite-horizon decision process. In other words, we wish to find an optimal policy function π : S 7→A, which\nspecifies the action to take in a given state, so that the cumulative reward is maximized:\nmax\nπ\n∞\nX\nt=0\nγt∇(st, at, st+1)\n(14)\nThe experience of a reinforcement learning agent is a sequence of observations, actions and rewards:\no1, a1, r1, · · · , rt−1, ot, at, rt\n(15)\nThe state is a summary of the experience defined above:\nst = f(o1, a1, r1, · · · , rt−1, ot, at, rt)\n(16)\nwhich in a full observed environment, can be:\nst = f(ot)\n(17)\nA reinforcement learning agent usually involve one or more of three components: a policy (which the agent’s behavior\nfunction), a value function (which evaluates how good each state and/or action is), and a model (which is the agent’s\nrepresentation of the environment). More specifically, the policy maps from the state to action, which can be deterministic\n(a = π(s)) or stochastic (π(a|s) = p(a|s)). The model is learned from experience, and acts as a proxy for the\nenvironment. With its model, the agent can make sense how world changes given agent’s action in two ways: the\ntransition or dynamics model predicts the agent’s next state given an action p(st+1 = s′|st = s, at = a); and the\nreward model predicts the immediate reward at state given an action r(st = s, at = a) = E[rt|st = s, at = a].\nThe value function is a prediction of future reward, i.e. what rewards will the agent gets by taking an action a in state s.\nQ-value functions gives the expected total reward from state s via action a under policy π with discount factor γ:\nQπ(s, a) = E[rt+1 + γrt+2 + γ2rt+3 + · · · |s, a]\n(18)\n11\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 6: (A) Categorization of reinforcement learning approaches. (B) Sub-processes of reinforcement learning.\n(Re-created from David Silver’s lecture).\nwhich can be decomposed into a Bellman equation:\nQπ(s, a) = Es′,a′[r + γQπ(s′, a′)|s, a]\n(19)\nwhere an optimal value function can be further obtained by taking the maximum achievable value:\nQ∗(s, a) = max\nπ\nQπ(s′, a′) = Qπ∗(s, a)\n(20)\nWith this optimal value function Q∗, the agent can act optimally:\nπ∗(s) = arg max\na\nQ∗(s, a)\n(21)\nMaximizing over all possible decisions, Q∗(s, a) can also be decomposed into a Bellman equation:\nQ∗(s, a) = rt+1 + γ max\nat+1 rt+2 + γ2 max\nat+2 rt+3 + · · ·\n(22)\n= rt+1 + γ max\nat+1 Q∗(st+1, at+1)\n(23)\n= Es′[r + γ max\na′ Q∗(s′, a′)|s, a]\n(24)\nThere are different ways to categorize reinforcement learning approaches. Figure 6A positions several approaches\nwith respect to their spectrum on the three major model components. Figure 6B also includes experience as a landing\npoint for sub-processes such as acting, model training and plannings. Overall there are model-based reinforcement\nlearning and model-free reinforcement learning. Model-based methods construct a full model of the environment\nexplicitly and then plan ahead by rolling out future steps using the model. It may or may not have an explicit policy\nand/or value function. Model-free methods, on the other hand, doesn’t have a model, and has explicit functions for the\nvalue and/or policy. The model-free reinforcement learning can be further separated into value-based and policy-based\nreinforcement learning. Value-based reinforcement learning estimate the optimal value function Q∗(s, a), which is the\nmaximum value achievable under any policy. Policy-based reinforcement learning search directly for the optimal policy\nπ∗, and use that policy to achieve the maximum future reward. There are methods which are both value-based and\npolicy-based, such as the actor-critic algorithm, which we will cover in a bit.\nQ-learning is a model-free value-based reinforcement learning algorithm. The Q-value of a state-action pair, Q(s, a),\nis key to this algorithm, representing the expected future discounted reward for taking action a ∈A in state s ∈S.\n12\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nAlgorithm 8 Q-Learning Algorithm\n1: Initialize: Qs,a = 0∀s ∈S, ∀a ∈A\n2: for each episode e do\n3:\nInitialize state s\n4:\nRepeat for each step t of the episode e\n5:\nTake action a = arg maxa′ Q(s, a′), and\n6:\nObserve s′ ∈S, r ∈R(s)\n7:\ns = s′\n8:\nQ(s, a) := ˆQ(s, a) + αt(r + γ maxa′ ˆQ(s′, a′) −ˆQ(s, a))\n9:\nuntil s is the terminal state\n10: end for\nThe action selection strategy is straightforward: at = arg maxa Qπ(s, a). Previously, we have shown that the optimal\nvalue function can be translated into a Bellman equation Q∗(s, a) = Es′[r + γ maxa′ Q∗(s′, a′)|s, a]. Therefore, we\ncan potentially approximate Q∗by directly attempting to solve the optimal Bellman equation. To do so, we define the\nBellman error as the update to our expected return when we observe the next state s′:\nr(st, at) + γ max\na\nQ(st+1, a) −Q(st, at)\n(25)\nThe first half of this error term, r(st, at) + γ maxa Q(st+1, a) is the right hand side of the Bellman equation, and\nsometimes also related to the notion of the temporal difference (TD) target. As in algorithm 8, Q-learning is the\nalgorithm that repeatedly adjusts Q to minimize the Bellman error. Ideally, when the policy is converged to the optimal\nsolution, the Bellman error would be zero. Each time, we sample consecutive states and actions to update the Q values:\nQ(st, at) = Q(st, at) + α[r(st, at) + γ max\na\nQ(st+1, a) −Q(st, at)]\n(26)\nwhere α is the learning rate. A common method to handle very large state spaces is to approximate the Q function as a\nlinear function of some features representing the observation or state space [56]. We denote ψ(s, a) as the relevant\nfeatures of the state-action pair ⟨s, a⟩. Assuming Q(s, a) = θ · ψ(s, a), where θ is an unknown weight vector to be\nlearned by interacting with the environment. Then each time step the agent takes action a at state s, obtains immediate\nreward r and arrives at the new state s′, the parameter θ is updated with the temporal:\nTD = (r + γ max\na′ Q(s′, a′)) −Q(s, a)\nθi = θi + α · TD · ψi(s, a),\n(27)\nAs in the bandit strategies, a common approach to balance the exploration vs exploitation tradeoff here is to use the\nϵ-Greedy strategy. The agent, using the ϵ-Greedy to gradually update its weight parameter θ according until convegence\nor some pre-specified maximal number of steps.\nPolicy gradient is a model-free policy-based method that learns the policy directly with a parameterized function respect\nto parameter θ. The value of the reward (the objective) function depends on this policy π(a|s), and then different\nalgorithms can be applied to optimize θ for the best reward [57]. The modeled reward function as the optimization\nobjective is defined as:\nJ(θ) =\nX\ns∈S\ndπ(s)V π(s) =\nX\ns∈S\ndπ(s)\nX\na∈A\nQπ(s, a)πθ(a|s)\n(28)\nwhere V (s) is the state-value function measures the expected return of state, V π(s) is the value of state s under policy\nπ, dπ(s) is the on-policy state under the policy π given by the stationary distribution of the Markov transition model.\nUnder the policy gradient theorem [26], the gradient ∇θJ(θ) can be computed as:\n13\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nAlgorithm 9 REINFORCE Algorithm\n1: Initialize: θ arbitrarily\n2: for each episode (on-policy trajectories) {s1, a1, r1, · · · , sT , aT , rt} ∼πθ do\n3:\nfor t = 1, 2, 3, · · · , T do\n4:\nθ = θ + αV π(s)∇θ ln πθ(a|s)\n5:\nend for\n6: end for\nAlgorithm 10 Actor-Critic Algorithm\n1: Initialize: θ arbitrarily\n2: for each episode (on-policy trajectories) {s1, a1, r1, · · · , sT , aT , rt} ∼πθ do\n3:\nfor t = 1, 2, 3, · · · , T do\n4:\nUpdate policy parameter θ: θ = θ + αθQw(s, a)∇θ ln πθ(a|s)\n5:\nCompute TD error δ: δt = rt + γQw(st+1, at+1) −Qw(s, a)\n6:\nUpdate critic parameter w: w = w + αwδt∇wQw(s, a)\n7:\nend for\n8: end for\n∇θJ(θ) = ∇θ\nX\ns∈S\ndπ(s)\nX\na∈A\nQπ(s, a)πθ(a|s)\n(29)\n∝\nX\ns∈S\ndπ(s)\nX\na∈A\nQπ(s, a)∇θπθ(a|s)\n(30)\n=\nX\ns∈S\ndπ(s)\nX\na∈A\nπθ(a|s)Qπ(s, a)∇θπθ(a|s)\nπθ(a|s)\n(31)\n= Eπ\ns∼dπ,a∼πθ[Qπ(s, a)∇θ ln πθ(a|s)]\n(32)\n[58] is a nice introduction to policy gradient objective optimization, that covers general advantage estimation to control\nthe variance and bias in this general form. More specifically, REINFORCE, also known as Monte-Carlo policy gradient,\nis an effective algorithm to perform policy gradient reinforcement learning [59]. As in algorithm 9, we can sometimes\nsimplify the term Qπ(s, a) in our gradient with the state value function V π(s), which is the value of state when we\nfollow a policy π, assuming the chosen action at is the optimal one so far that yields the best estimate of the expected\nfuture reward starting from this state st. Using episode samples, the algorithm applies Monte Carlo methods to estimate\nthe expected return, i.e. the Q function Qπ(s, a), to update the policy parameter θ given the gradient ascent update. A\nempirical strategy to improve the learning in REINFORCE is to use the advantage function A(s, a) = Q(s, a) −V (s)\ninstead of the value function in the gradient ascent update, because it reduces the variance of the gradient estimation\nwhile maintaining the bias necessary for learning [57].\nThe actor-critic algorithm is a model-free algorithm that is both policy-based and value-based [60]. On top of the\npolicy gradient methods introduced above, the actor-critic algorithm also learns the value function in addition to its\npolicy. This is another strategy to reduce the variance in the gradient in the general form of policy gradients. The critic\nupdates the parameters w of its value function (which can be either the action-value Qw(s, a) or the state-value Vw(s)).\nThe actor updates the parameter θ for the policy πθ(a|s) along the gradient direction suggested by the critic. As in\nalgorithm 10, other than the update of the policy parameter θ, we also compute the TD error as in Q-learning with\nfunction approximation, and update the weight w for Q function given the TD error. We can define two learning rate,\nαθ and αw separately for the value and policy updates.\nThere are many other variants of the policy gradient methods that have the equivalent forms for optimizations. Some\nexamples are:\n∇θJ(θ) = Eπ\ns∼dπ,a∼πθ[∇θ ln πθ(a|s)V π(s)]\nREINFORCE\n(33)\n= Eπ\ns∼dπ,a∼πθ[∇θ ln πθ(a|s)Qw(s, a)]\nQActor −Critic\n(34)\n= Eπ\ns∼dπ,a∼πθ[∇θ ln πθ(a|s)Aw(s, a)]\nAdvantageActor −Critic\n(35)\n= Eπ\ns∼dπ,a∼πθ[∇θ ln πθ(a|s)δ]\nTDActor −Critic\n(36)\n14\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 7: Comparison of the rollout search space (or backup diagrams) of Monte Carlo (MC), Temporal Difference\n(TD) learning and Dynamic Programming (DP) for state value functions (from David Silver’s RL lecture). Additional\nnotation: Gt is the final return, or expected future reward, Gt = P\nτ γτrt+τ+1 [26].\nAlgorithm 11 Inverse Reinforcement Learning (IRL) Problem\n1: Initialize w, the parameter for the reward function\n2: for each episode (on-policy expert trajectories) in the demonstration policy {(s∗\n1, a∗\n1), · · · , (s∗\nT , a∗\nT )} ∼π∗do\n3:\nwhile not converged, or a maximal step is not reached\n4:\nSolve the MDP environment using the current reward function Rw(s, a) to generate reinforcement learned\npolicy π or behaviors\n5:\nOptimize for reward function parameter w by minimizing the inconsistency between the observed behavior π∗\n(from the expert) and the reinforcement learned behavior π\n6:\nend while\n7: end for\nwhich can all be trained with stochastic gradient descent. In the variants that have a critic, policy evaluation involves\nusing Monte Carlo or TD learning methods to estimate Qπ(s, a), Aπ(s, a), V π(s).\nThere are other types of reinforcement learning algorithms worth noting. One categorization of reinforcement learning\nmethods depend on the depth in exploration search space, which consists of Monte Carlo (MC), Temporal Difference\n(TD) learning and Dynamic Programming (DP) (Figure 7). The Monte-Carlo (MC) methods learns from episodes of raw\nexperience without modeling the environmental dynamics. It computes the observed mean return as an approximation\nof the expected return. Similar to MC, the Temporal-Difference (TD) Learning, which we already covers several\nvariants (e.g. Q-Learning, Actor-Critic), is model-free approach that learns from episodes of experience. However,\nunlike MC, TD learning can learn from incomplete episodes and hence don’t need to keep track of all the episodes all\nthe way up to the termination stage. When the model is fully known or estimated, following Bellman equations, we can\nuse Dynamic Programming methods (such as value iteration) to iteratively evaluate value function and improve policy.\nIt is a model-based methods. For interested readers, [26] is the most respected textbook on reinforcement learning that\ncovers all these topics.\n3.5\nInverse reinforcement learning\nIf we compare the pipeline between the standard reinforcement learning and the inverse reinforcement learning in\nFigure 5, they look very similar, both with the agent interacting with the environment with state emission, action taking\nand reward feedback. However, instead of given a simulation environment with pre-set reward functions for the agent\nto interact with, in inverse reinforcement learning, we only have the historical trajectories or experience of an expert\ninteracting with an unknown environment, and the goal is to discover the reward functions of this environment (what is\nthis world environment about? what motivates the agent to act the way it did? Can we reason about the goal or objective\nthe expert is aiming for?). In addition, it is usually easier to obtain historical data of expert’s action, rather than having\nan interactive systems to train forever. Thus, using these priors can potentially be powerful.\nThe inverse reinforcement learning is connected to the concept of inverse optimal control [61, 62], a framework to\nmodel or to “imitate” human behaviors. However, it is different from the standard imitation learning in the following\nway. standard imitation learning only copy the actions performed by the expert, without any reasoning about the\noutcomes or implications of the actions they imitated. It also doesn’t necessarily characterize the salient properties of\nthe behaviors. For instance, an expert can have multiple skills at different contexts. Inverse reinforcement learning, or\n15\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 8: Comparison of training reinforcement learning using the conventional reinforcement learning approaches vs\nusing the inverse reinforcement learning.\nhere in this context, the “human imitation learning” aims to copy the intent of the expert, and thus, the policy it learns\nmight take very different actions despite having the same intent.\nWe are given the state and action space, historical trajectories or samples from the expert policy π∗, and sometimes\nalso its dynamic model, and the goal is to recover reward function, and sometimes also to use the learned reward to\nget an optimal policy (as in the reinforcement learning). Other than points covered above, here are several additional\nbenefits. Since the policy is a mixture of the reward and the dynamics, to enable better transfer learning across tasks\nwhen learning from the demonstration, we want to decouple the goal from the underlying dynamics [63]. Inverse\nreinforcement learning might be a solution to extract out the reward part. Another user scenario would be to use inverse\nreinforcement learning to create additional training systems with learned rewards, as in Figure 8. If our end goal is to\ntrain a reinforcement learning agent that mimics the expert policy, but we don’t have the same environment to train\non, one way would be to first learn the reward functions from expert trajectories and then train our agent in the new\nenvironment we create using the learned reward functions.\nThe framework of inverse reinforcement learning is straightforward. As in algorithm 11, we model the observed\nbehavior from the expert as the solution of an Markov Decision Process with unknown reward functions that is gradually\noptimized to converge to the underlying reward functions. We first initialize a parameter w for the reward function Rw\nwe wish to optimize. This can be a linear weight, a neural net, a distribution over rewards, or any function approximation\nas we show before. Then, we iteratively optimize for Rw: at each step, we solve the MDP environment using the current\nreward function Rw(s, a) to generate the reinforcement learned policy π or behavioral trajectories; we compute the\ninconsistency between the observed behavior π∗(from the expert) and the reinforcement learned behavior π as a loss,\nfor any optimization engine of w to minimize against. We continue this two steps until it converged to a reasonable\nlevel of behavioral inconsistency.\nInferring the underlying reward functions directly from demonstration can be challenging. First, since many reward\nfunctions can explain the same behavior, it is an underspecified problem. Second, these historical trajectories of reward\nfunctions usually have unknown dynamics and large state-action space, which makes the optimization difficult. Third, it\ncan be difficult to evaluate a learned reward, so we need to specify a reasonable metric. And lastly, the demonstrations\nmay not be precisely optimal, and thus, it might be impossible to uniquely decompose the policy of irrational or\nsuboptimal agents into the underlying reward function [64].\nDespite these challenges, several strategies are proposed to solve inverse reinforcement learning. The first strategy is to\noptimize to maximize margin. Similar to the intuition of support vector machines, the idea is to learn a reward function\nthat better explains the demonstrated expert policy than alternative policies by a margin as large as possible. One simple\nformulation of the margin is the sum of the discrepancy between the expected value of the optimal action and the next\nbest action over all states, given by [62]:\n16\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nAlgorithm 12 Maximum Entropy (MaxEnt) Algorithm\n1: Initialize w, the parameter for the reward function\n2: for each episode (on-policy expert trajectories) in the demonstration policy {(s∗\n1, a∗\n1), · · · , (s∗\nT , a∗\nT )} ∼π∗do\n3:\nwhile not converged, or a maximal step is not reached\n4:\nSolve for optimal policy π(a|s) with value iteration in the environment parameterized by the current reward\nfunction Rw(s, a).\n5:\nSolve for the state visitation frequencies p(s|w)\n6:\nCompute gradient ∇wL =\n1\nM\nP\nT ∈D xT −P\ns∈S p(s|w)xT\n7:\nw = w + α∇wL\n8:\nend while\n9: end for\nX\ns∈S\nQπ(s, a∗) −\nmax\n{a∈A|a̸=a∗Qπ(s, a)\n(37)\nwhere a∗is the optimal action at state s. If we adopt the feature-based approach with function approximation, the\nmargin is the difference between the expected value of the behavior from the observed expert trajectory and the largest\nexpected values of the the behaviors of other trajectories that can be used to learn the feature weights w. We can obtain\nthe expected value of a policy by multiplying the empirical state visitation frequency (SVF) from the demonstration\ndata, which we denote p(s|w), with the weighted features of the state obtained from the trajectory, which we denote xs.\nAs such, the margin becomes:\nX\n(s,a)∈T\np(s|w)w⊤xs −\nmax\n{T ′∈(S×A)l|T ′̸=T p(s|w)w⊤xs\n(38)\nwhere T denotes the trajectories, and (S ×A)l is the set of all possible trajectories of length l defined by the state-action\nspace. One can solve this optimization method with a linear program to retrieve the reward function that maximize this\nmargin [65]. This margin optimization approach can produce the given policy as optimal output from the complete MDP\n[66]. Assuming that each demonstration trajectory is a distinct policy, the structured maximum margin prediction (MMP,\n[67]) improves the optimization process by solving a quadratic program that is constrained to have positive margin\nvalue in equation 38 and regularized with a loss term lp(s|w) which measures the closeness between the demonstrated\nand alternative behaviors.\nHowever, as in the overall challenges of IRL, in these maximum margin methods, matching of the feature counts can be\nambiguous because a given policy can be optimal for many different reward functions, while the same feature counts\ncan be obtained by many different policies. This approach improves upon these limitations of the maximum margin\napproaches by solving the ambiguity of suboptimality with a maximum entropy principle to learn a distribution over\nbehaviors parameterized by the reward function weights. The idea is to learn the reward function that provides the\ntrajectory distribution constrained by the observed demonstrations with the maximum entropy:\nmax\nX\nT ∈(S×A)l\n−p(T ) log p(T )\n(39)\nTo avoid the exponential growth of the state-action search space, an alternative would be to find the reward function that\nmaximizes the entropy of the distribution of all policies:\nmax\nX\nπ∈(S×A)l\n−p(π) log p(π)\n(40)\nA popular algorithm to optimize for this, is the Maximum Entropy (MaxEnt) [68] algorithm, which introduces two\nconstraints to equation 40: the distribution of all demonstration trajectories should follow a probability distribution; and\nthe expected feature count of the demonstration trajectories should satisfy:\nX\nT ∈D\np(T )\nl\nX\nt=1\nγtxt\ns = 1\nM\nM\nX\ni=1\n∞\nX\nt=0\nγtxt\ns\n(41)\n17\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 9: Comparison of the generative adversarial networks and inverse reinforcement learning\nAlgorithm 13 Generative adversarial imitation learning (GAIL) Algorithm\n1: Initialize θ0, ψ0, the initial parameters for the policy and the discriminator\n2: for i = 0, 1, 2, · · · do\n3:\nSample trajectories from the generator Ti ∼πθi\n4:\nUpdate discriminator from ψi to ψi+1 with the gradient:\nˆETi[∇ψ log(Dψ(s, a))] + ˆETi[∇ψ log(1 −Dψ(s, a))]\n(45)\n5:\nUpdate policy from θi to θi+1 with the gradient:\nˆETi[∇θ log(πθ(a|s)Q(s, a))] −λ∇θH(πθ)\n(46)\n6: end for\nwhere D is our available demonstration data, xt\ns is the feature of the state s at time t, and M is the number of trajectories.\nThere are two assumptions. First, the reward of the trajectory is a linear combination of the feature counts:\nRT\nw = w⊤xT =\nX\ns∈T\nw⊤xs\n(42)\nwhere xT is the feature count of the trajectory T . And the the probability of a demonstrated trajectory should be\nexponentially higher for higher rewards than lower rewards:\np(T ) ∝eRT\nw\n(43)\nAs in algorithm 12, the algorithm solves for a convex nonlinear optimization problem:\narg max\nw\nX\nT ∈D\nlog p(T ; w), where p(T ; w) = e\nP\n(s,a)∈T w⊤xs\nZ(w)\n(44)\nThere is a connection between the maximum entropy IRL with the generative adversarial networks (GAN), which\nis a family of generative models which jointly and adversarially train a discriminator of synthetic and data samples\n18\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nAlgorithm 14 Imitation Learning (IL) or Behavioral Cloning (BC) Problem\n1: Initialize policy parameter θ\n2: for each episode (on-policy expert trajectories) in the demonstration data {(s∗\n1, a∗\n1), · · · , (s∗\nT , a∗\nT )} ∼D∗do\n3:\nfor batches of state-action pairs (s∗, a∗) do\n4:\nMakes an observation o∗(t) to the state s∗(t)\n5:\nChooses an action given the current state or observation: at = πθ(o∗(t))\n6:\nUpdate policy parameter θ using supervised learning by minimizing the loss function D(a∗\nt , at)\n7:\nend for\n8: end for\nAlgorithm 15 Dataset Aggregation (DAgger) Algorithm\n1: Initialize policy parameter θ\n2: for multiple episodes do\n3:\ntrain πθ(at|st) from demonstration data {(s∗\n1, a∗\n1), · · · , (s∗\nT , a∗\nT )} ∼D∗\n4:\nrun πθ(at|st) to get new dataset Dπ = s1, · · · , st\n5:\nask human annotators to label Dπ with actions at\n6:\naggregate D∗= D∗∪Dπ\n7: end for\nand a generator of synthetic samples given a cost function trained by the discriminator [69]. In particular, there is a\nmathematical equivalence between sample-based maximum entropy algorithm and the generative adversarial networks,\nwith respect to their similarity of both using the probability density of the generator as an evaluation metric and input to\nthe dueling discriminator [70]. As in Figure 9, similar to a GAN, inverse reinforcement learning attempts to optimize for\na reward function that can generate or parameterize a policy which can sample out trajectories that is as indiscriminable\nto an expert trajectory sample from the demonstration as possible, and the evaluation metric for that has to be also\ntrained as a discriminator which compares the samples under some principled or learned strategy (e.g. maximum\nentropy or margin). It involves a special form of discriminator, which in optimal case:\nD∗(T ) =\np(T )\np(T ) + q(T )\n(47)\nwhere we assume the generator is fixed with a density q(T ) and the p(T ) is the actual distribution of the demonstration\ndata. Generative adversarial imitation learning (GAIL, [71]) and guilded cost learning (GCL, [72]) are two IRL\nalgorithms that optimize using a GAN-like setting. As in algorithm 13, for each iteration, we first sampel the trajectories\nfrom the generator Ti ∼πθi, update the discriminator given the relative entropy (equation 45) and then update the policy\nusing the Trust region policy optimization (TRPO) rule [73] on the cost function log(Dψ(s, a)) (a KL-constrained\nnatural gradient step as in equation 46).\nFor interested readers, [66] is a survey on various approaches of inverse reinforcement learning.\n3.6\nImitation learning and behavioral cloning\nThe imitation learning or behavioral cloning aims to solve the policy learning with a supervised learning approach. As\nin algorithm 14, we simply divide the expert demonstration into batches of state-action pairs, and treat these pairs as\ni.i.d. training examples for supervised learning using a loss function that characterizes the difference between the action\ntaken by the expert and the action recommended by the supervised learning policy. Since the algorithmic space of this\nclass of methods is largely in supervised learning, which can be highly customized to the specific problem setting and\napplication domain, we will refer the readers to [74, 75] for a survey on different types of supervised learning imitation\nlearning methods.\nAn innate challenge for imitation learning is the distribution shift problem. As we are attempting to mimic the action\ntrajectory, even a small change of actions in early rounds might lead to a large variance in later actions due to the\namplification effect from the earlier actions. In other words, the pdata(ot) can be very different from pπθ(ot). To\ntackle this distributional shift problem, the Dataset Aggregation (DAgger, [76]) is an iterative algorithm to trains a\ndeterministic policy with additional collections of dataset. The idea is simple, instead of trying to craft θ such that\npπθ(ot) can be as close to pdata(ot) as possible, we can simply collect more on-policy data such that pdata(ot) are\ncloser to pπθ(ot). As in algorithm 15, the goal is to collect additional on-policy training data from pπθ(ot) by simply\nrunning πθ(at|ot) and then asks human annotators to label at. These labeled actions combined with the state transition\n19\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nAlgorithm 16 Behavioral Cloning with Demonstration Reward (BCDR) Algorithm\n1: for each episode (on-policy expert trajectories) in the demonstration data {(s∗\n1, a∗\n1), · · · , (s∗\nT , a∗\nT )} ∼D∗do\n2:\nfor t = 1, 2, 3, · · · , T do\n3:\nAgent makes an observation o∗(t) to the state s∗(t)\n4:\nAgent chooses an action given the current state or observation: at = πt(o∗(t))\n5:\nEnvironment progresses to the next step s∗\nt+1 given the action a∗\nt\n6:\nAgent receives a reward feedback about both the environment reward and the consistency between its action\nat and the expert’s action a∗\nt at this state state s∗\nt : ra∗\nt ,at,s∗\nt (t) = renv(at, s∗\nt ) + λD(a∗\nt , at)\n7:\nAgent updates its policy πt\n8:\nend for\n9: end for\ndata collected from the learning policy πθ are then appended or aggregated to the training dataset. Iteratively, the dataset\ncollected would be closer and closer to an on-policy dataset that avoids the distributional shift issue.\nHowever, we don’t always have access to human annotations. One strategy is to provide goals as contexts. Another\nstrategy is to formulate supervision as rewards. The Behavior Cloning with Demonstration Rewards (BCDR, [77]) is a\nreinforcement learning approach to directly optimize against the trajectory difference. In this setting, the agent first\ngoes through a constraint learning phase. As in algorithm 16, during the learning, the agent is allowed to query the\nstates and actions in the available demonstration data, and receive feedback about both the environment reward and\nwhether or not the selected action at matches the teacher’s action a∗\nt (from the demonstration data):\nra∗\nt ,at,s∗\nt (t) = renv(at, s∗\nt ) + λD(a∗\nt , at)\n(48)\nboth and the consistency between its action at and the expert’s action at this state state s∗\nt : During the deployment\nor testing phase, the goal of the agent is to maximize both the environment reward renv(at, s∗\nt ), and a unobserved\nD(a∗\nt , at), which models whether or not the chosen action matches which action the expert would have taken. Through\nthe learning, the behavioral cloning algorithm aims to train reinforcement learning agents to mimic the expert behaviors\nin the demonstration.\nFor interested readers, [74] is a survey on various methods in imitation learning. [78] compares the imitation learning\nwith offline reinforcement learning (a similar but different method we will introduce later in the emerging strategies\nsection) and discusses the specific environments and dataset compositions to use one over the other.\n4\nReinforcement Learning Formulation for Speech and Language Applications\nIn this section, we will cover a series of prototypical examples of how reinforcement learning can be effectively applied\nto major speech and language tasks, followed by a brief summary of other works in the specific task domain. Table 2\nsummarizes and compares the reinforcement learning formulations for several common natural language processing\ntasks. By examining the objective, reward, action space, and state space for each task, we can gain insight into the\ncommonalities and differences between these tasks. For example, many of the tasks involve extracting meaning from\nnatural language input, but the specific form of the input and output can vary widely. Additionally, different tasks have\ndifferent reward signals and action spaces depending on the specifics of the problem. Overall, this table provides a\nuseful reference for understanding how natural language processing tasks can be framed as reinforcement learning\nproblems. We hope these case studies in the following sections motivate the readers to rethink their daily tasks as\nreinforcement learning problems and encourage discussions in these new research avenues.\n4.1\nAutomatic speech recognition (ASR)\nAutomatic speech recognition (ASR) is an important component of many practical language processing systems.\nHowever, there are several challenges that can make it difficult to achieve accurate and reliable performance. One major\nchallenge is the difficulty of training effective models for low-resource or marginalized languages. This is because\nthere is often a lack of available data to train these models, which can result in poor performance and inaccurate results.\nAnother challenge is the impact of noisy environments on ASR systems. In real-world scenarios, speech signals can be\ndistorted or obscured by background noise, making it difficult for the system to accurately recognize and transcribe\nspoken language. This can lead to errors and decreased performance, particularly in situations where high accuracy\nis critical. Finally, collecting human feedback to improve ASR systems can be valuable, but can also be expensive\n20\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nTable 2: The reinforcement learning formulations in different speech and language applications\nTask\nObjective\nReward\nAction Space\nState Space\nSpeech Recogni-\ntion\nTranscribe audio\nsignal to text\nAccuracy of tran-\nscription\nSequence\nof\nphonemes/words\nCurrent audio in-\nput or transcrip-\ntion\nSpeaker Recogni-\ntion / Diarization\nIdentify speakers\nand group speech\ninto segments\nAccuracy\nof\nspeaker\niden-\ntification\nand\nsegmentation\nSpeaker labels for\neach segment\nCurrent\naudio\ninput,\nspeaker\nlabels,\nspeaker\ncharacteristics or\nspeech features\nSpoken Language\nUnderstanding\nExtract meaning\nfrom\nspoken\nlanguage\nand\nmap to a machine-\nreadable represen-\ntation\nAccuracy of ex-\ntracted meaning\nSet of possible\nmeanings\nCurrent\naudio\ninput,\ndialogue\nhistory, external\nknowledge\nNatural Language\nUnderstanding\nExtract meaning\nfrom natural lan-\nguage\ntext\nand\nmap to a machine-\nreadable represen-\ntation\nAccuracy of ex-\ntracted meaning\nSet of possible\nmeanings\nCurrent\ntext\ninput,\ndialogue\nhistory, external\nknowledge\nSequence Gener-\nation\n/\nText-to-\nSpeech Synthesis\nGenerate natural-\nsounding speech\nfrom text input\nNaturalness and\nclarity of gener-\nated speech\nSequence\nof\nspeech signals\nCurrent\ntext\ninput,\ndialogue\nhistory, external\nknowledge\nNatural Language\nGeneration\nGenerate natural\nlanguage\ntext\nin response to a\ngiven prompt or\ninput\nQuality,\nco-\nherence,\nand\nrelevance\nof\ngenerated text\nSet of possible\nnatural language\nsentences\nCurrent\ninput\nprompt\nor\ndia-\nlogue\nhistory,\nexternal\nknowl-\nedge\nLarge Language\nModels\nImprove language\nmodel\nbehavior\nand responses\nRelevance,\nengagement,\ndiversity,\nand\nfluency\nSet of possible\nwords,\nphrases,\nor sentence struc-\ntures\nContext informa-\ntion, previous dia-\nlogue history, and\ngenerated text\nConversational\nRecommenda-\ntion System\nProvide personal-\nized recommenda-\ntions in response\nto user queries or\npreferences\nAccuracy and rel-\nevance of recom-\nmendations, user\nengagement and\nsatisfaction\nSet\nof\nrec-\nommended\nitems/actions\nCurrent\nuser\npreferences,\nsearch\nqueries,\nhistorical\ndata,\nuser feedbacks\nand time-consuming. This can make it difficult to obtain large amounts of annotated data that can be used to train and\nimprove the system.\nReinforcement learning can be a powerful tool for addressing some of these challenges in speech recognition. For\nexample, one approach is to use curriculum learning to improve the effective learning from relatively small samples.\nThis involves training the model on a gradually increasing level of difficulty, starting with simple examples and gradually\nmoving towards more complex and challenging inputs. Another approach is to use speech enhancement techniques to\ngenerate cleaned and undistorted speech signals, which can improve the accuracy and robustness of the system in noisy\nenvironments. This can involve using signal processing techniques such as denoising and filtering, or training separate\nmodels to clean and preprocess the audio input before it is passed to the ASR system. Finally, domain adaptation can be\nused to transfer learned representations from previous batches of audio signals to future ones. This can be particularly\nuseful for low-resource or marginalized languages, where there may be limited data available to train the system. By\nusing transfer learning techniques, it may be possible to leverage information from related languages or domains to\nimprove the performance of the ASR system. Here are some case studies:\nCase study [83]. In this work, the authors study the usage of curriculum learning to improve model training on\nautomatic speech recognition on low-resource languages such as Turkish. Curriculum learning feed to the ASR model\n21\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 10: Examples on speaker recognition: (A) speech enhancement using reinforcement learning [79] (B) acoustic\nmodel selection using reinforcement learning [80] (C) semi-supervised learning with reinforcement learning [81, 82].\nbatches of data samples in a particular way such that they maximize the incremental learning performance at each batch.\nTo do so, a bandit-based curriculum learning framework is proposed, where the reward feedback to the bandit is the\nprediction gain, rP G = L(x, θ) −L(x, θ′), which is the loss before and after training on the current batch. By using an\nadversarial bandit algorithm, the Exponential-weight algorithm for Exploration and Exploitation (or more commonly\nknown as EXP3) [30], the curriculum generation procedure manage to effectively navigate among the exploration\nvs exploitation trade-off in using different batches of speech-text pairs which likely provides a performance boost in\nnon-stochastic degree, and speed up the learning in ASR task with limited data.\nWhile curriculum learning with RL can help improve model training, the approach may be sensitive to the choice of\nreward function and the specific curriculum generation strategy. Designing an effective reward function that accurately\nmeasures learning progress can be challenging and require domain expertise. Moreover, the effectiveness of this\napproach may be limited by the complexity of the ASR task and the quality of the training data.\nCase study [79]. In this work, the authors introduce a reinforcement learning-based speech enhancement system for\nautomatic speech recognition. As shown in Figure 10A, the raw audio signal can be noisy, and therefore, transforming\nthem into the Fourier space, filtering the Fourier spectrum with a binary mask, and then applying inverse Fourier\ntransform, can potentially generate a cleaned and un-distorted version of the speech signal. However, it is an innate\nchallenge to find the ideal binary mask, since the audio data stream might come from different sources and thus contain\ndifferent noise profiles. The solution is to first cluster the binary masks into major groups, and use a deep reinforcement\nlearning agent to identify the ideal pool of binary masks to minimize the recognition errors. The reward signal here\nto update the agent is a scaled difference between the utterance-based error rates of the ASR result from the noisy\noriginal data and the ASR result from the speech enhanced samples spectrum-filtered by the binary mask picked by the\nreinforcement learning agent.\nThe RL-based speech enhancement approach can rely on the assumption that the pre-defined binary masks can effectively\nclean and enhance the speech signal. However, the performance of this approach heavily depends on the ability of the\ndeep reinforcement learning agent to identify the optimal binary masks for various noise scenarios. Mismatched or\nincorrect mask choices could lead to suboptimal enhancement results and adversely affect ASR performance.\nCase study [80]. In this work, the authors propose a reinforcement learning-based hypothesis selection mechanism to\npick the optimal hyperparameter for the acoustic processing embedding in batch-evaluated ASR systems. As in Figure\n10B, they assume the data come in batches (e.g. batches of phone calls at different times of the day) and each batch\nmay exhibit a slightly different voice pattern that would affect the effectiveness of model’s domain adaption from one\nbatch to another. As a result, they formulate the challenge as a reinforcement learning problem, where the actions are a\nfew options of the update coefficient τ for a GMM-HMM acoustic embedding modulated by a deep learning-based\nMAP adaptation model. To specify the reward, they create two rival systems (one deep neural net that updates using\nreinforcement learning, and a baseline that doesn’t). For each input utterance, a recognition hypothesis is sampled from\neach of two competing systems, and both of them are presented to the user. Then, the user provides a human-in-the-loop\nfeedback by selecting the better hypothesis (which is the text prediction from the speech) among the two models. The\nreward feedback to the reinforcement learning agent is 1 if the deep reinforcement learning model is selected, and 0 if\nthe baseline is selected by the user as providing the better hypothesis.\nUsing RL to select optimal hyperparameters for acoustic processing in ASR may introduce additional computational\noverhead, as the agent needs to explore the action space of hyperparameters. Moreover, the efficacy of reinforcement\nlearning for hyperparameter selection can be influenced by the complexity of the task, the available labeled data, and\n22\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 11: Examples on speaker diarization: (A) The reinforcement learning problem [87] (B) the bandit solution for\ncold-start user problem [88] (C) MiniVox: online speaker diarization benchmark for reinforcement learning agents [89].\nthe design of the reward signal. The performance gains achieved by this approach might be limited by the available\nlabeled data and the diversity of voice patterns in different batches.\nCase study [81, 82]. In these two works with similar ideas, the authors use the reinforcement learning as a semi-\nsupervised pre-training mechanism to augment the ASR training with the unlabelled speech data. As in Figure 10C,\ntraditional supervised learning ASR system needs paired data of the speech and their text transcripts in order to complete\nthe supervised training. This component can also be accomplished using a reinforcement learning agent because, the\nsupervised training procedure can be handled in terms of policy-based reinforcement learning because the gradient of\ncross-entropy loss for the one-hot target output of word tokens can be formulated as a special case of policy gradient:\n∇θ log πθ(yt|xt) = ∇θ log πθ(at|st) · 1.0\n(49)\nOn the other hand, the reinforcement learning can also handle unlabelled speech corpus without corresponding text\nlabels. This is because reinforcement learning agents don’t require speech-to-text paired corpus but requires a reward\nfunction, which can be a more relaxed condition. As a result, in face of unlabelled speech data, a reward function\ncan still be effectively defined using a perplexity-based soft reward, which anti-correlates with the character error rate\n(CER) commonly used in ASR training.\nAlthough RL offers a flexible way to use unlabelled speech data for pre-training ASR models, the definition of a reward\nfunction for unlabelled data may be challenging. The reward function needs to effectively correlate with the ASR\ntask objective, such as minimizing character error rate (CER), without the direct supervision provided by labeled data.\nAdditionally, the performance improvement achieved by using unlabelled data through reinforcement learning might be\nlimited compared to fully supervised training.\nOther works. [84] trains a sequence-to-sequence model for ASR via policy gradient taking the negative Levenshtein\ndistance as the reward. [85] further improves this system by using token-level rewards instead of sentence-level rewards.\n[86] uses policy gradient to train an encoder-decoder ASR system with sequence-level evaluation metric as its reward.\nPracticality and limitations. Applying reinforcement learning to speech recognition faces challenges such as designing\nsuitable reward functions, identifying optimal hyperparameters, and ensuring effective transfer of learned representations.\nThe choice of reward function and the design of the reinforcement learning agent’s action space can significantly impact\nthe performance gains achieved. Moreover, reinforcement learning approaches may be limited by the quality and\nquantity of available data, making them more effective in scenarios with ample labeled or reward-driven data. Despite\nthe potential benefits, these challenges underscore the need for careful consideration and evaluation when applying\nreinforcement learning to address the inherent complexities of ASR tasks.\n4.2\nSpeaker recognition and diarization\nSpeaker recognition and diarization are important tasks used in a wide range of applications such as speaker identification,\nspeaker verification, and speech-based interaction. However, there are several challenges that can make it difficult\nto achieve accurate and reliable performance. One major challenge is that existing speaker recognition models are\nusually pre-trained on large scale pre-registered user profiles. This can make it difficult to adapt these models to new\nor unknown speakers, which can result in poor performance in real-world scenarios. Additionally, modern multi-user\nteleconferences often have cold start new users, which can make it difficult for the system to accurately identify and track\nindividual speakers. Another challenge is that collecting human feedback to improve speaker recognition models can be\n23\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\ndifficult and expensive, particularly when the rewards are highly sparse. In addition, labelled data for low-resource\npopulations may be very rare, which can make it difficult to train accurate models for these groups. Finally, diarization\nresults can be hard to generalize to out-of-distribution environments, such as different contexts or acoustic conditions.\nReinforcement learning can be a useful tool for addressing some of these challenges in speaker recognition and\ndiarization. For example, one approach is to use interactive learning with human feedback to improve the accuracy and\nperformance of the system. This involves actively soliciting feedback from users and using it to update and refine the\nmodel in real-time. Another approach is to make speaker recognition and diarization models lightweight and capable of\nlearning on the fly. This can help improve the adaptability and flexibility of the system, particularly in scenarios with\nnew or unknown speakers. Finally, transfer learning or semi-supervised learning can be used to improve the accuracy\nand robustness of speaker recognition and diarization models, particularly in low-resource or marginalized populations.\nBy leveraging information from related domains or data sources, it may be possible to improve the performance of the\nsystem even when labelled data is scarce. Here are some case studies:\nCase study [87, 89, 88, 90]. In this work series, the authors consider the online speaker diarization task as a\nreinforcement learning (or fully online learning) problem, where at each time step, the agent uses the window-sliced\nacoustic features as its context or state to decide which user is speaking at the current moment (as its actions, Figure\n11A). The innate challenge for this problem is that, they assume the users can come and go at any moment, and the\nsystem can be deployed without any pretraining on users’ voice profiles. The corresponding solution is to formulate it as\na bandit problem with infinite arms, where one action arm always stands for a “new user”, and some under-used action\narms can demise with a recency principle (Figure 11B). The online speaker diarization system updates the reinforcement\nlearning policy with user feedbacks in a human-in-the-loop setting, which introduces an additional challenge that these\nreward feedback can be implicit and highly sparse. To effectively learn from sparse feedback, they propose to use the\nBackground Episodic Reward LinUCB (or BerlinUCB [48]), which is a semi-supervised learning bandit algorithm that\nuses clustering as self-supervision to provide pseudo-reward and update only contextual representations when the real\nreward is missing. To encourage the field to evaluate online speaker diarization as the reinforcement learning problem,\nthey provide a testing benchmark called MiniVox ([89, 88], Figure 11B).\nIf historical data is available for pre-training, supervised and unsupervised approaches are still favored for their overall\nbetter performance, comparing to the on-the-fly reinforcement learning solution. A drawback of using RL for online\nspeaker diarization is the challenge of collecting accurate reward feedback from users. Since the reward signal is based\non implicit and sparse user feedback, there can be uncertainty in understanding the true reasons behind user preferences\nor decisions. This can affect the reliability of the reward signal used for updating the reinforcement learning policy,\npotentially leading to suboptimal learning outcomes. Moreover, designing a suitable reward function that accurately\ncaptures the quality of the diarization results and aligns with user preferences is critical, but can be challenging in\ndynamic and real-time scenarios.\nOther works. It is a relatively new idea to apply reinforcement learning to solve speaker diarization directly. Other\nthan the above works which use semi-supervised bandits on online speaker diarization problem, the follow-up work\n[91] summarizes a complete spectrum of reinforcement learning approaches in the speaker diarization problem.\nPracticality and limitations. Applying reinforcement learning to speaker recognition and diarization tasks offers\npotential benefits in addressing challenges such as adaptability to new speakers, handling cold start scenarios, and\nimproving performance for low-resource or marginalized populations. However, a common drawback in these\ncase studies is the challenge of obtaining accurate and reliable reward feedback, which is crucial for guiding the\nreinforcement learning process effectively. Sparse and implicit reward feedback can introduce uncertainty and potential\nbiases, impacting the learning process and ultimately the system’s performance. Despite the promise of reinforcement\nlearning, these challenges highlight the need for carefully designed reward mechanisms and exploration strategies to\nensure the success of the approach in speaker recognition and diarization applications.\n4.3\nSpoken language understanding (SLU)\nSpoken language understanding (SLU) is an essential component of modern day smart speakers, and is used to extract\nmeaning and intent from spoken language input. However, there are several challenges that can make it difficult to\nachieve accurate and reliable performance. One major challenge is the difficulty of transferring SLU models to new\nlanguages or domains. This can be particularly challenging if the grammatical structures of the new language differ\nsignificantly from the training data, which can result in poor performance and inaccurate results. Additionally, SLU is a\ncomplex task with many components, which can make modeling the state and action spaces very complex, particularly\nwhen the system is only partially observable. Another challenge is that human refinement of SLU parsers can be\ninterfering across different dialogue acts (DAs), which are utterances serving functions in the dialog. This can make it\ndifficult to accurately assign and identify DAs, which can result in poor performance and inaccurate results.\n24\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 12: Examples on spoken language understanding: (A) grammar slot matching with reinforcement learning in\nNMT language transference for SLU [92]. (B) SLU components in Apple Siri [93] and (C) its training pipeline.\nReinforcement learning can be a powerful tool for addressing some of these challenges in spoken language understanding.\nFor example, one approach is to use reinforcement learning to auxiliarily optimize grammar slot matching. This involves\ntraining the model to identify and fill in missing or incorrect slots in the input, which can help improve the accuracy\nand completeness of the results. Another approach is to consider the stages of training model components, and to use\nreinforcement learning to improve the performance of each component separately. This can involve training the ASR\nand NLU components separately and then integrating them using reinforcement learning to optimize performance and\naccuracy. Finally, interactive learning can be used to adaptively assign dialogue acts, which can improve the accuracy\nand relevance of the results. This involves actively soliciting feedback from users and using it to update and refine the\nmodel in real-time, which can help improve the accuracy and effectiveness of the system. Here are some case studies:\nCase study [92]. In this work, the authors adapts a neural machine translation (NMT) model to a new language using a\npolicy-gradient reinforcement learning approach such that auxiliary requirements can be included as part of the reward\nsignals (Figure 12A). In language transference between languages that are grammatically different, keeping the entity\nslots of word tokens aligned can be challenging. To tackle this issue, they propose a source-critical class-based method\nthat directly measures the slot keeping ratio (SKR, a metric for evaluating the performance of slot transferring in SLU\nacross language) as an incremental loss to bind to the reward function. More specifically, by optimizing over\n(r(ws, x) −r(wb, x))∇θ log pθ(ws)\n(50)\nwhere x is the speech signal (source input to the NMT model), wb and ws are the word token generated by the baseline\nNMT model and the NMT model adapted with a reinforcement learning agent. The reinforced translation model updates\ngiven this final policy gradient by rewarding the translation candidates that generates a higher SKR over baseline. By\nusing the relative SKR as the reward, the reinforcement learning agent adapts to obtain target language translations\nwhich can maintain both the semantic meanings and the slot information of the SLU-labeled sentences in the source\nlanguage.\nA drawback of using RL to adapt NMT models for new languages is the challenge of defining appropriate reward\nfunctions. Designing reward signals that accurately reflect slot keeping ratio (SKR) can be complex and require\nthorough understanding of the semantics and grammatical structures of both the source and target languages, as well as\ntheir innate ambiguity and dialects. An inaccurate or misaligned reward signal can lead to suboptimal adaptations and\ndecreased translation quality.\nCase study [93]. In this work, the author discusses the engineering choices made in the Siri system (Apple’s personal\nintelligent assistant). The SLU system in Siri involves many critical components, ranging from speech recognition,\nnatural language understanding, dialogue act recognition, dialogue management, action selection, to finally the speech\nsynthesis to interact with the user using human-like voice feedback (Figure 12B). The training goal is to utilize statistical\n25\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\n(e.g. deep learning-based) methods to the language understanding unit by integrating data-driven evidence collected\nfrom suitable training data in an optimal order, such that the intent of the user can be inferred efficiently and accurately.\nHowever, the feedback to the system are usually implicit and all computational components are proxies to the underlying\ninferred properties that can affect downstream tasks. As a result, this is a partially observable Markov decision process\n(POMDP) problem to the spoken dialogue systems. Performing a reinforcement learning training here is challenging\nbecause: (1) The internal state of the users that we wish to model is a complicated mixture of the user’s goal, the user’s\ninput on the interface, and the historical dialogues and interactions. (2) This is further compounded by the speech-level\nuncertainty in the user’s utterances and the systematic errors in upstream systems which can amplify and propagate the\nuncertainty into other computational entities. (3) For smooth user experience, the system should have a large enough\naction space that cover every possible system response, and as a result, the reinforcement learning policies should map\nfrom the complex and uncertain dialogue states to a large search space of possible actions.\nOne strategy they point out in this work is to first train rule-based system separately in order to gain more dialogue data\nto boost POMDP training for the reinforcement learning agent (Figure 12C).\nCase study [94]. In this work, the authors aims to exploit the usage of user’s annotating feedback for sequential training\nand refining of a zero-shot SLU semantic parser in an online way. The interactive system is formulated such that at\neach round of SLU, the user gets recommendations for their predicted intents, and they can be asked to do one of the\nthree annotation tasks to help refine this SLU model: 1) Skip, meaning that the the user don’t have to do anything; 2)\nYesNoQuestion, meaning that the user gets to confirm or negate the detected DAs in the best semantic hypothesis; and\n3) AskAnnotation, meaning that the user is asked to annotate the incoming utterance. The reward signal to maximize is\na mixture of the system improvement and the user effort.\nA drawback of using RL for interactive SLU with user feedback is the potential uncertainty and variability in user\nannotations. The reliance on user annotations for refining the SLU model introduces noise and subjectivity in the reward\nsignal, which can lead to challenges in accurately updating the reinforcement learning policy. Ensuring consistent and\nreliable user feedback is essential to avoid incorrect learning signals.\nOther works. [95, 96] propose an interactive SLU system that uses a reinforcement learning agent to select both the\noptimal speech enhancement units to process the speech signals and the candidate intent prediction. The system can\nevaluate the correctness of the prediction by computing a similarity score between the intent prediction and the sparse\nannotation of the actual intent from the user. The reward signals to update the agent comes from the difference between\nthe score of the agent and that of the baseline models that randomly sample predictions from the candidate pool. [97]\nuses a deep reinforcement learning approach to track speech dialogue state for online task-oriented spoken dialogue\nsystems.\nPracticality and limitations. While reinforcement learning holds promise for improving SLU systems, these case\nstudies highlight several common drawbacks. Designing appropriate reward functions that accurately reflect system\nperformance or user preferences can be challenging, especially in scenarios involving complex language structures and\nuser interactions. Additionally, the complexity of dealing with partially observable states in dialogue systems, as well\nas uncertainty and variability in user feedback, can introduce noise and errors in the reinforcement learning process.\nThese challenges emphasize the need for careful reward engineering, exploration strategies, and handling uncertainties\nto ensure the success of reinforcement learning approaches in enhancing SLU performance.\n4.4\nNatural language understanding (NLU)\nNatural language understanding (NLU) is a critical task used to extract meaning and intent from text-based information.\nHowever, there are several challenges that can make it difficult to achieve accurate and reliable performance. One major\nchallenge is that in many applied problems, the intents or interpretations from NLU are usually implicit or simply not\ngiven. This can make it difficult to accurately identify and extract the meaning and intent from the input, which can\nresult in poor performance and inaccurate results. Additionally, NLU is usually a critical step to provide contexts for\ndownstream tasks, and training it independently might not guarantee its performance in downstream tasks, as even a\nsmall variation can be amplified in later steps.\nReinforcement learning can be a useful tool for addressing some of these challenges in natural language understanding.\nFor example, one approach is to train the whole system as a reinforcement learning problem and implicitly learn the\nnatural language understanding task. This involves training the model to optimize a specific reward function, which\ncan help improve the accuracy and effectiveness of the system in a wide range of applications and use cases. Another\napproach is to provide contexts and personalization to natural language understanding training by training it together\nwith downstream tasks in an end-to-end optimization. This can help improve the accuracy and robustness of the system,\nparticularly in scenarios where the downstream tasks are closely related to the NLU task. Finally, natural language\nunderstanding can in turn be used to improve reinforcement learning algorithms, by providing additional insights and\n26\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 13: Examples on natural language understanding: (A) NLU learned by deep reinforcement learning in text-based\ngames [98]: top panel – model architecture; bottom panel – encoder word embedding. (B) NLU in recommendation\nsystems with implicit feedback and ambiguous entities [99] (C) NLU-informed reinforcement learning [100].\ncontext that can be used to improve the performance and accuracy of the system. This can help unlock the potential for\nnatural language processing in a wide range of applications and use cases, by improving the efficiency and effectiveness\nof the algorithms and models used in these systems. Here are some case studies:\nCase study [98]. In this work, the authors study the neural network representation of deep reinforcement learning in\ntext-based games and find that meaningful clusters of words emerge from it. They propose a long short-term network\n(LSTM)-based deep Q network (DQN) that has a separate Representation Generator that takes a stream of words\nobserved in the game’s state s as input and generates a vector representation, which is then fed into a an actor network\nconsisting of scores for all actions and argument objects (Figure 13A). The game states are hidden from the player, who\nonly receives a varying textual description. In this reinforcement learning problem, the action space has two dimensions,\nthe action and its argument object, whose policies are jointly trained with the representation generator given game\nrewards. The dimension reduction of the state encoder representation (Figure 13A botton panel) shows that the words\nare grouped by contexts which can potentially be used for natural language understanding.\nOne potential drawback of using deep RL to extract neural network representation for specific tasks is the challenge\nof interpretability. While meaningful clusters of words may emerge from the representation, understanding the exact\nsemantics and relationships between these clusters can be difficult, especially for deep RL-based decision making\nsystems trained for high-stake scenarios such as finance, forensic or health-related data.\nCase study [99]. In this work, the authors use bandits to personalize the NLU given user features and implicit feedback.\nConsider a virtual assistant system, we give the system a speech command, it transcribes it using ASR, and detects\nintents (e.g. play music) with NLU and parse out necessary further information such as slots (e.g. album) and the slot\nvalues (Figure 13B). Before executing the command of retrieving an entity (e.g. play a certain song), the system needs\nto perform the entity resolution (ER) process, which finds the best entity of the given type (e.g. album) an the slot values\n(e.g. “dark side of the moon”). However, they can be ambiguous in all three levels (alternative ASR transcripts, multiple\nNLU intents, and multiple entities matching the same criterion). It is a bandit problem because we only receive implicit\nand bandit feedback (only revealed for the picked NLU interpretations). It is a contextual bandit problem, because\nthe best NLU interpretation is dependent on the user’s interest. We can also train it with feature-based reinforcement\nlearning because the natural language understanding are usually trained with manually labelled utterances which can be\nadditional signals to the agent.\nSimilar to the aforemnetioned bandit-based speaker diarization solution, the challenge of using bandit-based personal-\nization for NLU is the uncertainty and subjectivity in sparse and implicit user feedback. Implicit feedback can be less\ninformative and may not always accurately reflect the user’s true intent or preferences. Incorrect or noisy feedback\nsignals can lead to suboptimal personalization decisions and decreased performance in entity resolution and intent\ndetection tasks.\n27\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nCase study [100]. In this work, the authors summarize different ways NLU can better inform reinforcement learning\ntasks (Figure 13C). First, we have language-conditional reinforcement learning systems, where the text information are\ndirectly related to the task, such as following text instructions, parsing rewards from instructions, and reading languages\nin the observation or action spaces. Second, we have language-assisted reinforcement learning systems, where the\nlanguages are auxiliary but useful information not directly related to the task goal, such as communicating domain\nknowledge, and structuring policies. Finally, we have task-independent natural language understanding, such as finding\nreal-world semantics and grammar, or finding the underlying storylines or intents of characters.\nA potential challenge of using NLU to inform RL tasks is the complexity and diversity of natural language. Extracting\nmeaningful and relevant information from natural language text can be challenging, especially when the language is\nambiguous, context-dependent, or involves domain-specific jargon. Inaccurate or incomplete extraction of information\ncan lead to incorrect or biased reinforcement learning decisions, such as in a medical or legal decision making scenario.\nOther works. [101] proposes a reinforcement learning solution that learns to execute navigation instructions expressed\nin natural language. [98] finds that LSTMs, if combined with reinforcement learning optimization, can learn a better\nrepresentation in text understanding than bag-of-words approaches in capturing the underlying semantics of sentences.\nPracticality and limitations. While reinforcement learning offers promising approaches to address challenges in\nnatural language understanding (NLU), these case studies highlight some common drawbacks. The complexity of\nlanguage, potential ambiguity, and the challenge of designing accurate reward functions can limit the effectiveness of\nreinforcement learning in NLU tasks. Additionally, the uncertainty and subjectivity of implicit feedback and the lack\nof interpretability in neural network representations can introduce noise and uncertainty into the learning process. To\nsuccessfully leverage reinforcement learning in NLU, careful consideration of these challenges and the development of\nstrategies to mitigate them are crucial.\n4.5\nSequence generation and text-to-speech (TTS) synthesis\nSequence generation is an important task used in a wide range of applications such as text-to-speech (TTS) synthesis,\nlanguage translation, and image captioning. However, there are several challenges that can make it difficult to achieve\naccurate and reliable performance. One major challenge is that many Seq2Seq models suffer from exposure bias, which\nrefers to the error accumulation during the output generation at test time since the model has never been exclusively\nexposed to its own predictions during training. This can result in poor performance and inaccurate results, particularly\nin scenarios where the output sequence is long and complex. In addition, in the evaluation phase, many Seq2Seq models\ncan suffer from an inconsistency between the training and testing datasets or domains. This can make it difficult to\naccurately generalize the model to new data or scenarios, which can result in poor performance and inaccurate results.\nFinally, deep learning-based solutions in sequence generation can be slow in both training and deployment, which can\nmake them unsuitable for real-time applications such as incremental text-to-speech (TTS) problems.\nReinforcement learning can be a useful tool for addressing some of these challenges in sequence generation and\ntext-to-speech synthesis. For example, one approach is to use reinforcement learning to provide a more generalizable\nsolution to sequence generation models by remembering long-term memories with state information. This can help\nimprove the accuracy and effectiveness of the model, particularly in scenarios where the output sequence is long and\ncomplex. Another approach is to use reinforcement learning to allocate resources in a dynamic way, which can enable\nhigh-quality real-time deployment of sequence generation and TTS models. This involves training the model to optimize\na specific reward function, which can help improve the efficiency and effectiveness of the system in real-time scenarios.\nHere are some case studies:\nCase study [102]. In this work, the authors argue that deep reinforcement learning can address two common issues\nin training sequence-to-sequence (Seq2Seq) models (Figure 14A). The first issue is exposure bias, which is the error\naccumulation during the sequence output generation during the test phase. This bias exists due to the difference\nof feeding sequence in the training and testing phases, as the model usually isn’t exposed to its own predictions\nduring training. The second issue is the inconsistency between the training and testing measurements and objectives.\nReinforcement learning solves them by remembering long-term memories with a state representation, and thus\nremoving the ground-truth dependency during training and using only the model distribution to minimize the objective\n(or equivalently, maximizing the rewards). It provides a nice overview of the related sequence generation applications\nin machine translation, text summarization, dialogue generation and many more. Figure 14B provide a few examples\nof formulating common sequence generation tasks into the policy, actions and rewards in reinforcement learning.\nSome common reinforcement learning agents used in this domain are policy gradient, REINFORCE, actor-critic, and\nQ-learning.\nCase study [103]. In this work, the authors study the incremental text-to-speech (TTS) synthesis problem (Figure 14C),\nwhere instead of outputting the acoustic waveform after the entire text sentence is fed into the TTS model, the goal is to\n28\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 14: Examples on sequence generation and text-to-speech (TTS) synthesis: (A) Seq2Seq model and (B) the\nreinforcement learning formulations of Seq2Seq models in sequence generation [102]. (C) The incremental text-to-\nspeech (TTS) problem and (D) its reinforcement learning solution [103].\nsynthesis the acoustic waveform as soon as possible in real-time (i.e. synthesis by each word or phoneme). Since the\ntext-to-acoustic features and speech synthesis both take time, the computing device can only perform one task at the\nsame moment, reading the text, or speaking the speech. This is a challenging task for two reasons. First, since the TTS\nmodel cannot see the full sentence before the synthesis, the model has to have some sort of intelligence to predict future\ntexts in order to generate speech that makes sense in its intonations and accents. Second, most state-of-the-art speech\nsynthesis engines are usually deep learning-based, which can create latency that doesn’t meet the real-time requirement.\nAs a result, for time-sensitive tasks like simultaneous interpretation, solutions usually emphasize non-neural and more\ntraditional architectures. As in Figure 14D, deciding when to read and when to speak can be a well-defined action space\nfor a reinforcement learning solution. The reinforcement learning agent then learns from the reward function which is a\nmixed combination that trades off between the latency incurred during the synthesis and the quality of the synthesised\nspeech output.\nA drawback of using RL for real-time incremental TTS synthesis is the trade-off between latency and quality. Optimizing\nthe reward function to strike the right balance between generating speech quickly and maintaining high-quality output\ncan be non-trivial. The RL agent may need to explore a wide range of latency-quality trade-offs before converging to an\noptimal solution, which makes certain application scenarios impractical.\nOther works. [104] proposes an interactive training paradigm that updates a reinforcement learning-based emotional\ntext-to-speech synthesis model rewarded by high emotion discriminability measured by a speech emotion recognition\nsystem. [105] uses a multistage reinforcement learning improves the sample-by-sample tree coding of speech by\nmodulating the exploration vs exploitation tradeoff in training the speech analysis and synthesis processes. [106] uses\ndeep reinforcement learning to generate music accompaniment as a duet.\nPracticality and limitations. Reinforcement learning offers potential solutions to challenges in sequence generation\nand text-to-speech synthesis tasks, such as exposure bias, inconsistency, and real-time deployment. However, the\ndesign and optimization of reward functions can be complex and require careful consideration. Balancing competing\n29\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 15: Examples on natural language generation: (A) The inverse reinforcement learning framework of text\ngeneration [107] and (B) its sub-components. (C) The reinforcement learning problem in dialogue generation [108].\nobjectives, such as generating high-quality output while minimizing latency, can be challenging. Additionally, the\neffectiveness of reinforcement learning in addressing these challenges depends on the specific task and domain, and\ncareful experimentation and tuning may be necessary to achieve optimal results.\n4.6\nNatural language generation (NLG)\nNatural language generation (NLG) is a critical task used to generate human-readable text sequences from non-linguistic\nstatistical representations of information. However, there are several challenges that can make it difficult to achieve\naccurate and reliable performance. One major challenge is that text generation engines like Seq2Seq models can\ngenerate short and dull responses, such as “idk” or “not sure.” This can make it difficult to generate interesting and\nengaging responses, particularly in scenarios where the output needs to be engaging and informative. In addition, NLG\nmodels can be short-sighted and only base their responses on the last few utterances. This can result in responses\nthat are not fully contextualized or that lack nuance and depth. Furthermore, the maximum likelihood objective is not\nnecessarily representative of how humans converse with one another, and training NLG models can be expensive and\ntime-consuming, requiring full supervision and labeled data. Finally, generated texts or dialogues can be repetitive and\nlack diversity, which can make it difficult to generate engaging and interesting content.\nReinforcement learning can be a useful tool for addressing some of these challenges in natural language generation. For\nexample, one approach is to use inverse reinforcement learning to create more dense rewards and encourage diversity.\nThis involves training the model to optimize a specific reward function, which can help improve the accuracy and\neffectiveness of the system in a wide range of applications and use cases. Another approach is to use reinforcement\nlearning to train dialogue models with customized rewards based on the problem being solved. This can help improve\nthe accuracy and relevance of the responses, particularly in scenarios where the output needs to be tailored to the\nspecific needs and preferences of the user or application. Here are some case studies:\n30\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nCase study [107]. In this work, the authors uses the inverse reinforcement learning to learn the underlying reward\nfunction or driving forces of the generative process of a text corpus and use it to generate realistic natural language\nsamples (Figure 15A). Existing NLG solutions are mostly based on adversarial generative models (e.g. SeqGAN [109])\nfollowed by a reinforcement learning model, because as we points out in the sequence generation section, they can\navoid the problem of exposure bias. However, the sequence generated by these GAN + RL solutions usually suffer\nfrom mode collapse in the adversarial generator and reward sparsity due to a perfect discriminator at the end of the\ntraining. Inverse reinforcement learning solves both by producing more dense reward signals (from the inferred reward\nfunctions) and encouraging more diversified texts as it uses entropy-regularized policy gradient (e.g. MaxEnt). As we\npointed out earlier, inverse reinforcement learning is connected to GAN and can be formulated in similar architecture\n(Figure 15A, B) by using a reward approximated generator followed by a discriminator to distinguish synthetic and real\non-policy trajectories (which are text sequences). Under this architecture, the reward functions and the text sequence\ngenerator are jointly trained in an adversarial way.\nOne potential drawback of using inverse RL for text generation is the difficulty of accurately inferring the underlying\nreward function from the given text corpus, especially if the text corpus are not considered an expert policy and thus,\ncan lead to equivalent solutions under a underspecified problem [64]. For instance, if the demonstration trajectories\nbelong to human subjects with clinical conditions, as in [110], inverse RL can have a hard time reaching to an optimal\nsolution. If the inferred reward function does not capture the nuances and complexities of human-generated text, it may\nlead to suboptimal results and fail to generate high-quality, realistic text samples.\nCase study [108]. When we think of dialogue generation, one might assume a necessity to use multiple (or two) agents\nto learn their interactions. However, due to the flexibility of the reward functions for policy gradient methods, in this\nwork the authors aim to generate dialogue directly using a single deep reinforcement learning policy (Figure 15C). The\nreinforcement learning problem is formulated the following way. The states are the concatenation of the previous two\ndialogue turns (which are also the inputs to the encoder in the Seq2Seq model). The actions are the dialogue utterances\nto generate (which have a infinite action space). The policy is a stochastic one, implicitly defined in the parameters of\nthe encoder-decoder architecture. The reward has a combination of three desired properties: 1) the ease of answering\n(which is the the negative of likelihood of dull responses, such as “Idk” or “no idea”); 2) the information flow or low\nrepetitiveness in the generated dialogue (which is the information flow of the encoder representations); and 3) the\nsemantic coherence (which is the pairwise mutual information between the two correspondents in a dialogue pair).\nA potential drawback of training a single-agent dialogue generation model using deep RL is that it may not fully\ncapture the complexities of multi-agent interactions in real-world dialogues. The simplified approach of using a single\nagent to generate dialogues might overlook the intricate dynamics that emerge from the interactions between multiple\nparticipants.\nOther works. [111] proposes a paraphrase generation model that uses reinforcement learning to fine-tune deep\nnetworks of a generator and uses inverse reinforcement learning to train an evaluator on the similarity between phrases.\n[112, 113] uses reinforcement learning to generate navigation instructions with rewards measured from either a hidden\nMarkov model or a Bayesian network. [114, 115] proposes to use an adversarial bandit to adapt the NLG model online\nthrough direct interactions with user feedbacks. [116] proposes a reinforcement learning solution to generate structured\nqueries from natural language with rewards from in-the-loop query execution over the database.\nPracticality and limitations. As in the case studies, reinforcement learning offers promising solutions to challenges in\nNLG, such as generating engaging responses and ensuring context-awareness. However, using inverse reinforcement\nlearning to infer reward functions for text generation may be prone to inaccuracies, potentially leading to suboptimal\nresults. Similarly, training single-agent dialogue generation models may not fully capture the complexities of multi-\nagent interactions that occur in real-world conversations. These challenges highlight the need for careful formulation of\nreward functions and model architectures to ensure the effectiveness and accuracy of reinforcement learning-based\nsolutions for natural language generation tasks.\n4.7\nLarge language models (LLM)\nThere is a growing interest of large language models (LLMs) such as GPT-3 [120], PaLM [121], and ChatGPT which\nperform human-level performance in NLG task, which is why we separate it out as an individual section. These LLMs\nare typically trained using maximum likelihood estimation (MLE) to generate text that matches a given input. However,\nMLE-based methods suffer from various limitations, such as generating repetitive or uninteresting responses, and not\ntaking into account the broader context of the conversation.\nRL provides a way to improve the performance of large language models by training them to optimize a specific reward\nfunction. In the context of NLG, the reward function can be defined based on various metrics, such as diversity, fluency,\nrelevance, and engagement. For example, the reward function can be designed to encourage the model to generate\n31\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 16: Examples on large language models: (A) The three main steps of reinforcement learning with human\nfeedback in InstructGPT [117]. (B) The self-critique process of constitutional AI [118]. (C) The SafeguardGPT\nframework and its reinforcement learning alignment pipeline [119].\ndiverse and engaging responses, while avoiding repetitive or irrelevant content. RL can also enable human feedbacks to\nfine-tune the LLMs to be more human-like and conversational in question answering type of scenarios.\nCase study [117]. With the introduction of human moderators or annotators, the LLM can be tuned with Reinforcement\nLearning from Human Feedback (RLHF) [122, 123, 117], which involves using human feedback in the form of rewards\nto update the parameters of an LLM. As one of the first work in this direction, InstructGPT [117] was proposed to align\nGPT-3 with users’ intended outcomes. As in Figure 16A, the process involves assembling a collection of human-crafted\ninstances showcasing the desired output behavior, and then, based on these demonstrations, GPT-3 is fine-tuned first\nusing supervised learning. Following this, a reward model is constructed through the ranking of model-generated results\nranging from the most favorable to the least. Employing this reward model, further refinement of the model is achieved\nvia RL utilizing PPO. Results suggest that if trained with RLHF, smaller LLMs (e.g. 1.3B parameters) can generate\nmore desirable results comparing to significantly larger ones (e.g. 175B).\nA potential drawback of using RLHF for tuning LLMs is the need for human-generated rewards and rankings.\nConstructing a reliable reward model based on human preferences can be subjective and time-consuming. The quality\nof the reward signal heavily depends on the accuracy of human annotators, which may introduce bias or inconsistencies\nin the training process.\nCase study [118]. Similar to RLHF, one can also tune LLMs using Reinforcement Learning from AI Feedback (RLAIF).\nConstitutional AI [118] refers to AI systems that are designed to comply with a set of ethical principles, similar to\nhow democratic societies are governed by a constitution. The authors suggest using AI feedback as a mechanism for\nensuring that the AI system remains within the boundaries of its ethical principles. Similar to RLHF, it involves both\na supervised learning stage and a RL stage (Figure 16B). In the supervised stage, the model is refined based on the\nrevisions generated alongside the output samples and self-critiques. In the RL stage, a secondary model is used to\n32\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nassess the qualities of both the original output sample and the output sample from the refined model. The difference\nbetween this two samples are treated as a guiding reward signal to train the LLM in a RL process.\nIn the case of RLAIF for ethical LLMs, a potential drawback is the challenge of defining the boundaries and principles\nthat guide the AI system’s behavior. Ensuring that the AI feedback accurately reflects the ethical principles can be\ncomplex and may require ongoing human oversight to prevent unintended consequences.\nCase study [119]. As a special hybrid case of RLAIF and RLHF, SafeguardGPT [119] is proposed to adjust chatbot\nLLM behaviors by using psychotherapy as a framework to identify and mitigate toxicity. The SafeguardGPT framework\ninvolves human moderators and 4 different AI/LLM agents, including an AI Chatbot, an AI User, an AI Therapist,\nand an AI Critic. The Chatbot and User interact in the chat room, while the Therapist guides the Chatbot through a\ntherapy session in the therapy room. Human moderators can control the sessions and diagnose the Chatbot’s state in the\ncontrol room. Lastly, the Critic evaluates the quality of the conversation and provides feedback for improvement in the\nevaluation room as an RL process (Figure 16C), either entirely closed loop or human-in-the-loop). Since it involves\nan self-adaptive autonomous agent consisting of a group of AI agents, and thus, can benefit from group thinking and\nself-reflection through cross-talking among the agents. By incorporating psychotherapy and feedback mechanisms,\nresults suggest that SafeguardGPT improves chatbots’ communication skills, empathy, and emotional intelligence.\nWhile the SafeguardGPT framework offers a comprehensive approach to improve chatbot behavior, it involves multiple\nAI agents and human moderators, making it potentially complex to implement and manage. Coordinating interactions\nbetween different AI agents and ensuring their alignment with human moderators’ intentions can be challenging and\nmay require continuous monitoring and adjustment.\nOther works. Other than using collaborative principles as in SafeguardGPT and Constitutional AI, one can train LLMs\nusing adversarial techniques, as proposed in the Red Teaming [124], where one LLM is trained to identify and expose\nweaknesses in another LLM’s language generation capabilities, as a more punitive approach. In this example, RL is\nused to maximize the expected harmfulness elicited in the Red LLM.\nIn adversarial approaches like Red Teaming, where one LLM is trained to expose weaknesses in another LLM, there\nis a risk of generating harmful or inappropriate content. The adversarial nature of the training process might lead to\nunexpected and unintended negative behaviors, reducing the reliability and safety of the generated outputs.\nIn addition to these teaming and grouping of AI agents and human users, we can use RL to personalize LLMs with prior\nknowledge, such as existing datasets (e.g., psychotherapy transcripts, social forum interactions, online rating websites)\nto pre-train individual LLMs used as the AI Therapist, AI User, and AI Critic in the example work above. This can help\ndevelop more effective, safe, and ethical AI chatbots that can be integrated into various domains, such as customer\nservice, education, and healthcare.\nPracticality and limitations.\nWhile reinforcement learning offers promising ways to enhance the performance and behavior of large language models,\nthere are potential drawbacks and challenges. These include the subjectivity and bias in human-generated rewards, the\ncomplexity of defining and enforcing ethical boundaries, the challenges of coordinating interactions among multiple AI\nagents and human moderators, and the potential for unintended negative outcomes in adversarial training approaches.\nBalancing these drawbacks with the benefits of improved LLM behavior requires careful consideration and continuous\nmonitoring.\n4.8\nConversational recommendation systems (CRS)\nConversational recommendation systems (CRS) are designed to enable dynamic communication between users and\nrecommendation systems via natural language or speech interactions, and involve multiple language understanding\ncomponents such as preference query, multi-turn conversational recommendations, and dialogue understanding and\ngeneration. However, there are several challenges that can make it difficult to achieve accurate and reliable perfor-\nmance in conversational recommendation systems. One major challenge is that recommendation systems are often\nindependently trained from natural language or speech components, which can limit their ability to take into account\nthe contexts afforded by ambiguity. This can result in recommendations that are not tailored to the specific needs\nand preferences of the user. Another challenge is that deep recommendation systems using existing natural language\nand speech components can be effective, but can be hard to generalize to cold-start users or items. This can limit\nthe usefulness and applicability of the system, particularly in scenarios where new users or items are being added\nto the system over time. Finally, directly applying the RL strategies in non-conversational recommendation systems\nto conversational ones may not handle sparse rewards very well in dialogue systems. This can make it difficult to\naccurately optimize the system to provide accurate and relevant recommendations to the user.\n33\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 17: Examples on conversational recommendation systems: (A) The major components of conversational\nrecommendation systems [125] and (B) a bandit formulation of it [126]. (C) The major components of a dialogue topic\nrecommendation in psychotherapy setting and (D) its reinforcement learning formulation [127].\nReinforcement learning can be a useful tool for addressing some of these challenges in conversational recommendation\nsystems. For example, one approach is to use bandits or reinforcement learning solutions to handle cold-start problems\nin recommendation systems. This involves training the system to optimize a specific reward function, which can help\nimprove the accuracy and effectiveness of the system in a wide range of scenarios. Another approach is to use speech\nand natural language components to parse real-time features as dense reward for reinforcement learning. This involves\ntraining the system to optimize a specific reward function, which can help improve the accuracy and effectiveness of the\nsystem in a wide range of scenarios, particularly in scenarios where the user is interacting with the system in real-time.\nHere are some case studies:\nCase study [125]. In this work, the authors summarize a bandit approach to tackle the conversational recommendation\nsystem setting (Figure 17A). In this example, the user interacts with a user interface powered by natural language\nunderstanding and generation units with queries and responses to the system. The recommendation model must\nstrategize questions, recommendations and explanations in human-understandable multi-turn dialogues which balances\nthe exploration vs exploitation trade-off. Intuitively, it can be formulated as a bandit problem where a specific set of\nquestions or items can be selected as action arms in order to minimize the number of questions asked, maximize the\nclick rate or maximize the smooth experience in the interactions [128]. This objective relates to the knowledge of\nthe user’s preferences and the questions’ quality in different aspects. A contextual bandit solution (such as LinUCB)\ncan effectively a reward mapping from these properties by asking the user about one or more attributes and treating\nthem as contexts [129]. Other than using reinforcement learning to select recommendation action, one can also use\nreinforcement learning to determine the timing to ask attributes or make recommendation. [126] proposes a bandit\n34\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nsolution to dynamically and automatically alternate asking questions about attributes with recommending items given\nthe user embeddings (Figure 17B).\nSimilar to the previous examples of bandit-based solutions in speaker diarization and SLU, a potential drawback of using\na bandit approach to tackle conversational recommendation systems is that it may not fully capture the complexities of\ndynamic user preferences and evolving conversation contexts. The bandit model might struggle to adapt quickly to\nchanging user preferences during the conversation, leading to suboptimal recommendations that do not align with the\nuser’s true interests.\nCase study [127, 130, 131]. In this work, the authors propose a real-time conversational recommendation system\nin psychotherapy to recommend discussion topics to the therapist given previous patient-therapist dialogues. An\nunsupervised learning inference method [132, 133, 134] is applied to annotate the therapeutic working alliance between\nthe patient and therapist in each dialogue turn which predicts long-term clinical outcome and serve as the reward for\neach dialogue pair. The action space are the most common topics mined from neural topic models over historical\ndata [135] and each dialogue turn are labelled by the topic label in a maximum likelihood principle (Figure 17C).\nThey train a deep reinforcement learning policy to directly map from previous and current dialogue state to the best\ntopic to recommend for the next turn, in order to maximize the cumulative therapeutic working alliance. As in Figure\n17D, an interesting perspective to view this conversational recommendation system is that while the recommendation\nagent is driven by reinforcement learning, the therapist (and even patient) have their own agencies governed under the\nreinforcement learning principles. As such, the feedback loop are two folds: the patient can directly offer feedback\nto the therapists, given the feedback, the therapist may adjust his or her internal model to weigh on the quality of the\nsuggestions made by the recommendation agent.\nWhile using RL to optimize the selection of discussion topics in psychotherapy is promising, it may not fully capture the\nnuances and complexities of therapeutic interactions and disease progression. The model’s recommendations might not\nfully account for the sensitivity and emotional states of patients and therapists, potentially leading to recommendations\nthat are technically relevant but emotionally inappropriate. Societal implications and ethical considerations [136] should\nbe taken carefully when interpreting the insights from these clinically deployed RL models.\nOther works. [137] proposes a reinforcement learning solution that tackles a combinatorial recommendation problems\nby predicting the popularity of social media posts as the values of interdependent sub-actions. [138] is a follow up work\nthat uses a two-stage Q learning approach and incorporate the global context represented by discussions in an external\nknowledge source into the state representations.\nPracticality and limitations. Reinforcement learning provides valuable solutions to challenges in conversational\nrecommendation systems, including handling cold-start issues and real-time interactions. However, applying bandit or\nreinforcement learning strategies may struggle to fully capture the dynamics of user preferences and the emotional\nnuances in human conversations. The simplified decision-making process of reinforcement learning models might not\nalways align with the complex and context-dependent nature of conversational interactions. These challenges emphasize\nthe importance of integrating human-centric considerations and domain expertise when designing reinforcement learning\nsolutions for conversational recommendation systems.\n5\nEmerging Reinforcement Learning Strategies\n5.1\nDeep reinforcement learning and bandits\nDeep Reinforcement Learning methods combines the recent advancements of deep learning with reinforcement learning\nsolutions. In the implementation level, these algorithms use deep neural networks to represent the value function,\nthe policy function, or the world model. To optimize these models from the data, one can apply most deep learning\noptimization strategies, such as stochastic gradient descent, to optimize the value function, the policy function or the\nmodel in an end-to-end fashion.\nFor instance, the deep learning variant of the Q-learning algorithm is the deep Q networks (DQN) [139]. Intuitively, it\nrepresent the Q value function by a deep Q network with weight w: Q(s, a, w) ≃Qπ(s, a). The objective function is\ndefined by the mean squared errors in Q values:\nL(w) = E[(r + γ max\na′ Q(s′, a′, w) −Q(s, a, w))2]\n(51)\nwhere r + γ maxa′ Q(s′, a′, w) is the target that we want our Q function to converge to. Then, we can compute the\ngradient for Q-learning:\n35\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\n∂L(w)\n∂w\n= E[(r + γ max\na′ Q(s′, a′, w) −Q(s, a, w))∂Q(s, a, w)\n∂w\n]\n(52)\nwhich can be optimized end-to-end with most deep learning optimization methods, such as the stochastic gradient\ndescent. However, the naive Q-learning approach for deep Q network can be unstable and reach oscillatory or diverging\nsolutions, for three reasons. First, the data is sequential, and in other words, the experience trajectories are non-iid and\ncorrelated with their successors. Second, the policy can change drastically with very small changes to the Q-values, and\nas a result, yielding oscillating policies and data distributions that swing back and forth. Third, the scale of the rewards\nand Q values are usually unknown, and thus can be amplified to very large values when trained with backpropagation.\nThere are a few solutions to solve these stability issues when training deep Q networks as Q-learning. Experience\nreplay is a strategy that pools many episodes of experience at each time steps together as a replay memory to train the\nQ-learning in a off-policy way. By learning from all past policies with these replay memories, it uses iid samples and\nbreaks the correlation of experience trajectories. The second strategy is to freeze the target by using two networks, a Q\nevaluation network and a target network which we freeze during training. This strategy avoid the oscillation in our\nsolutions and break the correlations between the Q-network and the target network. The third strategy is to clip the\nrewards or normalize the networks, such that the networks can learn from robust gradients.\nThere are many deep reinforcement learning algorithms growing as a popular field, so we will only briefly cover a\nfew. The deterministic policy gradient (DPG, [140]) solves reinforcement learning problems with continuous action\nspaces using the deterministic policy gradient, i.e. the expected gradient of the action-value function integrated over the\nstate space, or over both the state and action space in stochastic case. The deep deterministic policy gradient (DDPG,\n[141]) extends DQN and DPG into a model-free actor-critic method that replace DPG’s stepwise optimization with\nexperience replay and a “soft” target network which slowly tracks the learned network weights. Using an off-policy\nbatch-based optimization approach, DDPG introduces exploration by injecting noise to the actor policy. The trust\nregion policy optimization (TRPO, [73]) introduces the KL divergence between the new policy and the old policy as a\ntrust region constraint, and solve the constrained optimization problem approximately using sample estimates from\neither single-path or rollout trajectories. The proximal policy optimization (PPO, [142]) improves upon TRPO by using\nKL-divergence as a penalty instead of a constraint.\nSimilarly, we can also introduce deep learning into bandit solutions. [143] proposes a deep contextual multi-armed\nbandit by binding a Thompson sampling mechanism on top of a Bayesian neural network such that the inference time\ndropout and weight posterior sampling are modeled as the exploration vs exploitation tradeoff. [144] proposes a similar\nmethod of deep Bayesian bandit in recommendation system setting that approximates the uncertainty measurements of\nthe bandit predictions by employing a bootstrapped neural network with multiple heads and dropout units. Another\napproach is to directly use a neural network to represent the context. For instance, [145] proposes a neural contextual\nbandit which replaces LinUCB’s linear mapping with a neural network-based random feature mapping.\nFor interested readers, [146] is a good introduction to various types of deep reinforcement learning algorithms.\n5.2\nBatched and offline reinforcement learning\nMost use scenarios of reinforcement learning that we introduce earlier are on-policy and purely interactive. However,\nmany real-world reinforcement learning application systems have access to a large set of historical data, such as the\nprior user’s behavioral trajectories, chat dialogues or purchase history. It would be a waste to not use them for off-policy\ntraining. Offline or batched reinforcement learning studies the reinforcement learning methods that use previously\ncollected data without additional online data collection from on-policy interactions with the environment [147]. In this\nsetting, usually we have access to the experience data collected once with some (potentially unknown) behavior policy\n(which we denote πβ, where β refers to the data buffer, or also known as the replay buffer). In the training phase, we\nuse the buffer data from the data buffer β to learn a policy π. This training process doesn’t have access to or interact\nwith the MDP. This learned policy π would only be only deployed into the task MDP after being fully trained. One\nrelevant example is training a deep reinforcement learning from human dialogue data [148].\nThe offline reinforcement learning is the data-driven version of the reinforcement learning problem. And the optimization\nobjective is still to maximize the expected future reward. What differentiate it with our reinforcement learning\nformulation earlier is that it doesn’t have the ability to interact with the environment to collect additional transition\nexperience using the demonstration behavioral policy. Instead, the transition data are given as a static historical records\nof experience for the agent to learn its best policy. On the first glimpse, it resembles the imitation learning which we\ndiscuss in earlier sections, which simply adopts a supervised learning approach to learn from the demonstration. Offline\nreinforcement learning, on the other hand, needs to first comprehend the dynamical system underlying the unknown\n36\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nMDP from the static transition data and then potentially use this world model to learn an optimal policy which can\nobtain the largest cumulative reward in deployment phase.\nIt is a nontrivial problem because existing reinforcement learning solutions which we previously introduce, despite\nits flexibility to learn from off-policy dataset, often fall short of their performance to learn entirely from offline data\nwithout online interaction. This is due to a few reasons. First, they cannot effectively explore the states that are rare or\nnot available in the transition history and the actions that lead to those states. If the dataset doesn’t contain connected\nstate transition trajectories that touch high-reward regions, the agent would be unlikely to find those high-reward region.\nOne possible way would be to manually inject exploration, but that would face our second issue: they cannot correct for\nout-of-distribution estimate via interactions. For an effective exploration, agents need to perform counterfactual queries,\ni.e. the question that what may happen if the agent were to take a series of actions different from the ones they have\nseen in the dataset. However, this is problematic, because as we discuss in the last section, deep reinforcement learning\nsolutions use mechanisms like experience replay to create iid samples, which may limit the power of our learned policy\nto generate good yet potentially different actions, from those observed in our available dataset. In online setting, we can\nsimply try it out and correct for it. In offline setting, we don’t have this luxury. Third, like the challenge we discuss in\nimitation learning, we would face the problem of distributional shift when we perform a series of counterfactual queries.\nIn other words, since we might not know the distribution of our behavioral policy in the dataset, it is likely that our agent\n(characterized by its policy function, value function and world model) is trained under one distribution, but evaluated in\nan entirely different distribution, since the small changes in visited states for the new policy can be amplified over time\n(or steps) such that the two distributions vary by a large degree. Lastly, the deep learning-based function approximation\nwe use in these agents can exacerbate these issues due to their high-dimensional and expressive nature.\nIn order to evaluate how well a reinforcement learning perform given the historical data, importance sampling is used\nto perform offline evaluation with respect to either the return of a given policy, or the policy gradient of the offline\npolicy gradient methods. In short, we use importance sampling to compute J(π) with the trajectories sampled from πβ\nas the off-policy evaluation. There are multiple variants of importance sampling estimators that people use, such as\nper-decision importance estimator normalized by the weights [149], doubly robust estimator incorporating the function\napproximators [150] and marginalized importance sampling using the state-marginal importance ratio dπθ (s)\ndπβ (s) (where θ\nis the model parameter) [151]. Similarly, to compute off-policy policy gradient, one can use above importance sampling\nestimators. Other than reducing the variance using self-normalization, one can apply regularization (such as softmax as\nin [152] or KL-divergence as in [142]) to constrain the learned policy πθ to not deviate too far away from the behavioral\npolicy πβ.\nThere are several strategies to solve offline reinforcement learning problems. One can perform off-policy value function\nestimation with linear least squared methods [153, 154] but they can suffer from distributional drift, unless we have some\nsort of policy constraint that the distribution over the actions πθ(a′|s′) that we compute the target value from should\nbe close to the behavioral distribution πβ(a′|s′). This constraint makes sure that the Q function are only queried on\nin-distribution actions, such that generalization results should still hold as the errors in Q function are not accumulated.\nAlthough in this case, the Q function is evaluated on the same states as the ones in the training set, the action inputs\nare still flexible enough to be out of distribution. Methods such as Advantage Weighted Actor Critic (AWAC, [155])\nuses the KL-divergence between the learned and behavioral policies as an implicit divergence constraints. If using a\npolicy penalty algorithm, the reward function can be considered to be augmented as r∗(s, a) = r(s, a) −αf(πθ, πβ)\nwhere f(·) is the divergence function of the polices. Similar to our discussion in bandits, we can also use uncertainty\nestimation as another constraint, because out-of-distribution actions tend to have large uncertainty and constraining the\nuncertainty can produce conservative target values, as in [156]. The conservative Q-learning (CQL, [157]) regularizes\nthe value function or Q function directly to penalize large values and avoid overestimation for out-of-distribution actions.\nLastly, model-based offline reinforcement learning (MoREL, [158]) modifies the MDP model learned from data to\ninduce conservative behavior by penalizing the visitation of states under the model where the model is likely to be\nincorrect.\nFor interested readers, [147] is a good review on offline reinforcement learning techniques.\n5.3\nTransfer learning in reinforcement learning\nTransfer learning is a set of training techniques to train models with greater generalization capabilities by utilizing\nresources from other domains or resources meant for other purposes [159]. The other tasks or purposes that collect the\nresources from are called source tasks, and the resources from other domains are called the source domains. Our task\nand domain of interest are called the target task or domain. The goal of transfer learning is to effectively perform a\ntarget task on a target domain dataset utilizing the features and weights learned from the source task or dataset. In our\ncontext of reinforcement learning, we aim to answer the question that, can reinforcement learning effectively use the\nprior knowledge to facilitate learning in a new task domain? The intuition is straightforward: if our models have solved\n37\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nprior tasks, they might acquire useful (i.e. reusable) knowledge or insights for solving a new task. The knowledge\nstored in a reinforcement learning model can come from different components: the Q value function informs us which\nactions or states are good comparing to the alternatives; the policy function informs us which actions are potentially\nuseful comparing to the alternatives (since some actions in a given state are rarely useful); the world model informs us\nimportant understanding of the rules that govern the world (e.g. the laws of physics, which can be generalizable to other\nphysically grounded system); and features, weights and hidden states, which inform us a good representation to use\ndirectly, or kick start the training as the initialization for fine-tuning.\nGiven these potentially useful knowledge in reinforcement learning systems, the transfer learning might help by using\nthe past experience from one set of tasks for faster learning and better performance on a new task. In our context of\nreinforcement learning, the tasks are usually formulated as Markov decision processes. If we can directly run a policy\ntrained in the source domain in the new domain, we call it a zero-shot transfer learning, where the “shots” refer to the\nnumber of attempts in the target task or domain. Similarly goes for trying the target task once or a few times, which\ncorrespond to one-shot or few-shot transfer learning.\nThere are two main classes of transfer learning methods, the transductive transfer learning and inductive transfer\nlearning [160]. In transductive transfer learning, the source and target tasks are the same, but labeled data is only\navailable (or much more abundantly available) in the source domain. Here we translate it in reinforcement learning\nsetting: perhaps the reward feedback is not available or only very sparsely revealed in the task in the target domain,\nbut we have access to the historical trajectories or interaction environment of the same task but in the source domain.\nDomain adaption, as mentioned in one of our earlier examples, is a transductive transfer learning technique. In inductive\ntransfer learning, the source and the target tasks are different and the labelled data is only available (or more abundantly\navailable) in the target domain. Here we translate it in reinforcement learning setting: perhaps we have the historical\naction-state pairs in a source task, but don’t have access to the reward feedback. The inductive transfer learning can\nbe further separated into sequential transfer learning (STL, where multiple tasks are trained sequentially, either in a\ncross-domain or cross-task fashion), and the multi-task learning (MTL, where multiple tasks are trained at the same\ntime, in a parallel fashion).\nMore specifically, here we outline a few common strategies. In forward or sequential transfer learning, we simply train\nthe reinforcement learning model on one task, and directly transfer to a new task. One approach is to apply domain\nadaption in reinforcement learning training. Alternatively, we can also keep the reinforcement learning mechanism\nintact, but transfer the visual, speech or linguistic representations first. In multi-task transfer learning, we train the\nreinforcement learning model on many tasks at the same time and transfer to a new task. We can share the neural\nnetwork representations and layers across tasks in multi-task learning. Alternatively, we can also use contextual policies\nby baking context-relevant information of the task or domain into the model. Finally, we can transfer the world models\nand value functions by either using model-based reinforcement learning (which innately serve as an mechanism for\ntransfer), or using temporally correlated features such as successor features and representations.\nFor interested readers, [161] is a survey on transfer learning techniques applied in reinforcement learning, and [162, 163]\nare two modern surveys on transfer learning in multi-agent reinforcement learning.\n6\nOpen Questions and Challenges\n6.1\nMulti-agent settings in speech and language\nThe end goal of the speech and language is to communicate. As we formulate the generative processes of these\nsequence-based signals as reinforcement learning agents, it is a natural step to simulate and model the mechanistic\ninteractions of these agents in a multi-agent settings. We have shown in an earlier section the example of modeling the\ndialogue generation task as a single-agent reinforcement learning optimization problem [108], but in reality, there is an\ninformation asymmetry among the communicating individuals that might be more suitable for a multi-agent system\n(MAS) solution. The multiagent systems are self-organized systems with multiple interacting agents with computational\nintelligence [164]. For instance, each communicating individual undergoes an partially isolated information flow via\nmultiple processing units (e.g. language understanding, dialogue policy, and language generation), and have feedback\nloops in multiple steps which might yield complicated dynamic interactions (Figure 18).\nThese interaction trajectories can also change over different time steps, and as a result, modeling the data-generating\nmechanism of such a system would require modeling a dynamic graphs with changing nodes (as communicating\nindividuals) and edges (as communication types). There can be interesting group dynamics emerge from communications\nsuch as leadership, cohesion and conflicts [165]. One can develop solutions to measure these social dynamics, common\ngoals and collective intelligence in population and individual levels.\n38\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 18: Open question 1: multi-agent settings in speech and language.\nFrom the RL point-of-view, these multi-agent interactions suggest that the actions of one agent can affect the rewards\nand outcomes of other agents. One challenge in multi-agent settings is that the agents can have conflicting objectives or\npreferences, which can lead to suboptimal outcomes. For example, in a dialogue system where the system and the user\nare both agents, the system may prioritize providing accurate recommendations, while the user may prioritize ease of\nuse and naturalness of the conversation. Another challenge is that the agents may have different levels of information\nand understanding about the environment and each other, which can make it difficult to coordinate their actions and\noptimize their behaviors. For example, in a multi-speaker speech recognition system, different speakers may have\ndifferent accents or speech patterns that can affect the performance of the system.\nIf we only consider the users as our agents to model, the agents in this human-only multi-agent system can be cooperative\n[166] or competitive [167]. Each communicating individuals, if modeled by reinforcement learning systems, might\nhave different observation models and reward perceptions governed by competing or collaborative goals. Modeling\n(and potentially mimicking) these social sub-processes is a nontrivial task, as in [168], which compares the interaction\ntrajectories of the human data and the reinforcement learning algorithms in social dilemma setting. [169] demonstrates\nthat deep reinforcement learning can interact with natural language in a visual task to share information cooperatively.\nTo address these challenges, various approaches have been proposed to perform RL in multi-agent settings in NLP\nand speech tasks. One approach is to use coordination mechanisms to encourage agents to work together and achieve\ncommon goals. Another approach is to use adversarial training, where agents compete against each other in a zero-sum\ngame to improve their performance and achieve optimal outcomes. New speech or language-related concepts can arise\nfrom multi-agent modeling. For instance, grounded compositional language can emerge from multi-agent populations\nof NLP agents, represented as segments of abstract discrete symbols uttered by the communicating agents [170], as\nfurther summarized in this review [171]. A follow up work suggests that these compositional languages has to be\nconstrained in a specific curriculum in multi-agent setting in order to emerge [172]. Lastly, computational techniques of\nmulti-agent systems can also help improve temporal modeling of speech recognition systems, as in [173, 174].\n6.2\nMulti-objective training and human priors\nWhile we have covered many important sub-tasks in speech and language processes, many real-world applications\ninvolve the incorporation of multiple processing systems that interact with and depend on one another. For instance, a\nquestion answering system might need to understand the intent of the user to craft its objective functions, the descriptive\nentities as its retrieval constraints, and at the same time, analyze the contexts of the user scenario, reason the relationship\nof different entities, and learn a planning and prediction model to best interpolate or extrapolate users’ past experience\nto maximize or minimize the estimated objective function given different dimensions of success criteria (Figure 19). A\nconversational recommendation system, on the other hand, would need to understand the dialogue objective, infer the\nuser’s transactional state, estimate the user’s dynamic propensities, reason the compatibility and sequential relationship\nof available inventories, recommend the best items or actions to guide the users towards their transactional destination,\nall at the same time. The sub-components of such hybrid systems would have different priors on their affordance, i.e. the\n39\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\nFigure 19: Open question 2: multi-objective training and human priors.\ndefinitive quality of this processing component about how it can or should be used. And thus, training them together in\nan end-to-end solution can potentially benefit from specific training curriculum to balance the exploration vs exploitation\ntrade-off of all sub-components. One possible strategy would be to tune the reward function as a multi-objective\noptimization problem [175], such that the learning objectives for certain sub-components can be emphasized at certain\ntraining phases.\nAnother strategy would be to learn from human priors. Originally proposed as a neuroscience-inspired algorithm, the\nreinforcement learning solution has been widely studied over the years by neuroscientists and psychologists to further\nunderstand the biological constraints and mechanisms of this type of learning, in terms of reward processing [176],\nanatomical separation [177], abnormal states [178] and many others. One direction is to consider these biological\nvariants and constraints as a result from natural evolution, i.e. the algorithmic variants that serve a beneficial purpose\n(with a high fitness) at certain scenarios. Under this assumption, [179] studies the reward processing mechanism\nthat distinguishes a series of human psychiatric conditions and introduces it into the reinforcement learning model\ndirectly. If given human behavioral data, one can use mechanistic simulation, hierarchical Bayesian inference, or\ninverse reinforcement learning to build neuromorphic reinforcement learning models that mimic human behaviors\n[110]. [19, 20] further unifies these human behavioral agents at all three levels of bandits, contextual bandits and\nreinforcement learning. Empirical results suggest that these biologically plausible models are advantageous over\nexisting reinforcement learning models in AI tasks. Other than reward processing, reinforcement learning methods can\nalso model other human priors, such as attention [180]. From the anatomic separation of the biological brains, one can\npotentially map different speech and language processing components into different brain regions, and pre-program the\ninformation flow and training curriculum of these computational components, given the biological and cognitive priors\nfrom their corresponding neural correlates.\nApplied systems with speech and language processing units can have different states and contexts, which governs the\npriority of the sub-components in the system. For instance, if the user asks the virtual assistant to play music, the\npriority of the music recommendation engine should precede that of the conversational engine, as opposed to the case\nwhere the user talks to the customer support. As a result, the dynamics and transition of the different contexts would\nserve as an anchor or boundary in the multi-objective optimization problem. in other words, we can potentially train a\nuniversal model that take into account all different contexts. And then, given a certain context, the model would attempt\nto find the pareto-optimal frontier by projecting affordance onto the context plane.\nEither creating an agent with generalized intelligence across these contexts, or creating specialized agent one context\nat a time, we need to deal with multi-objective training, the situation where multiple objectives need to be optimized\nsimultaneously, which is a common scenario in NLP and speech tasks. For example, in machine translation, the system\nneeds to generate translations that are both fluent and accurate, which are two separate objectives that need to be\nbalanced. One challenge in multi-objective training is that the objectives can be in conflict with each other, making it\ndifficult to find a single solution that optimizes all of them simultaneously. This can lead to suboptimal performance,\nwhere the system fails to fully optimize any of the objectives, or it prioritizes one objective over the other, leading to\npoor performance on the neglected objective. Another challenge is that the objectives can be difficult to quantify, and\ncan vary depending on the context and task at hand. This can make it difficult to define the reward function that is used\nto train the RL model, which can affect the quality of the model and the effectiveness of the system. To address these\nchallenges, various approaches have been proposed to perform multi-objective training in NLP and speech tasks using\nRL. One approach is to use a weighted sum of objectives to combine the multiple objectives into a single objective,\nwhich can simplify the optimization process. Another approach is to use a Pareto-based approach, which involves\nfinding a set of solutions that represents the best trade-offs between the multiple objectives.\n40\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\n7\nSummary and Resources\nWe starts our survey by motivating the exploration vs exploitation trade-off problem in real-world speech and natural\nlanguage processing systems with innate data uncertainty and batch-based model training using large-scale relational\ndatabases. We formulate the reinforcement learning problem into five sub-classes of methodologies: Multi-armed\nbandits and contextual bandits are both optimization methods that aim to maximize rewards based on actions taken by\nan agent. While multi-armed bandits select the best action arm to maximize cumulative long-term rewards, contextual\nbandits use side information or features available to the agent to personalize their action strategies. Reinforcement\nlearning algorithms, on the other hand, are formulated as Markov decision processes and rely on state representations to\nupdate policies based on reward feedback received from the environment. Inverse reinforcement learning algorithms\nlearn the reward function of an unknown environment from demonstration trajectories to train reinforcement learning\nagents, while imitation learning and behavioral cloning use supervised learning to directly learn the mapping from state\nto action based on historical data of state-action pairs. These optimization methods are essential for improving the\nperformance of agents in various NLP and speech tasks.\nIn the application domains, we have reviewed speech and language tasks including speech recognition, speaker\ndiarization, spoken language understanding, natural language understanding, sequence generation, text-to-speech\nsynthesis, natural language generation, large language models and conversational recommendation systems, as well\nas presented case studies regarding the following problems: The use of reinforcement learning (RL) has been shown\nto be effective in improving various aspects of natural language processing (NLP) and speech tasks. In automatic\nspeech recognition (ASR), RL has been applied to improve low-resource training, speech enhancement, batch-wise\nadaptation, and model training using augmented unlabelled datasets. RL has also been applied to speaker diarization\nand spoken language understanding (SLU) to enable online interactive label speaker profiles and introduce slot stability\nby rewarding argmax policy. In NLP, RL has been applied to improve natural language understanding (NLU) in game\nsettings and introduce personalization by adaptively selecting NLU interpretation. RL has also been shown to be useful\nin determining resource allocation in incremental text-to-speech problems and improving text generation and dialogue\ngeneration through inverse RL and combining rewards from two dialogue agents. RL with human or AI feedbacks\ncan help train large language models to align with human values and adopt ethical principles, as well as reach better\nperformance in generating realistic responses. Finally, RL has been applied in conversational recommendation systems\nto provide recommendations based on conversational dialogues and real-time NLP-parsed elements. These findings\ndemonstrate the potential of RL in improving various NLP and speech tasks, and highlight the importance of combining\nNLP and RL techniques to achieve more effective and efficient systems.\nIn the emerging topics, we cover the advances in using deep learning techniques or representations in reinforcement\nlearning and bandit settings, using historical data for off-policy training with offline reinforcement learning, and using\ntransfer learning training techniques to effectively leverage resources or knowledge of other domains or tasks.\nIn the open questions, we describe how the speech and language studies can potentially benefit from adopting a\nmulti-agent perspective of a population of communicating individuals as well as several active research directions. We\nposition existing real-world speech and language applications into the challenge of building a multi-system reasoning\nmachine that have interacting sub-components with different training objectives. We propose to use insights from\ncognitive science or neuroscience as human priors to build better reinforcement learning models that can train faster and\nmore human-like. We describe the possibility of unifying different task contexts as temporal states where pareto-optimal\nfrontier can be located in the multi-objective optimization.\nThis survey is related to several fast growing fields. Some relevant reviews and textbooks include:\n• Introduction to reinforcement learning [8]\n• Introduction to multi-armed bandits [51]\n• A survey of inverse reinforcement learning: Challenges, methods and progress [66]\n• Imitation learning: A survey of learning methods [74]\n• Deep reinforcement learning: An overview [146]\n• Offline reinforcement learning: Tutorial, review, and perspectives on open problems [147]\n• Survey on applications of multi-armed and contextual bandits [181]\n• A survey on transfer learning for multiagent reinforcement learning systems [162]\n• Transfer learning for multiagent reinforcement learning systems [163]\n• Transfer learning for reinforcement learning domains: A survey [161]\n• Deep reinforcement learning for sequence-to-sequence models [102]\n41\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\n• Reinforcement learning based recommender systems: A survey [182]\n• A survey of the usages of deep learning for natural language processing [183]\n• Literature survey of statistical, deep and reinforcement learning in natural language processing [184]\n• Deep representation learning in speech processing: Challenges, recent advances, and future trends [185]\n• Survey on reinforcement learning for language processing [5]\n• A survey on compositional generalization in applications [171]\nOne good way to get onboard the research is to try out code examples. Some useful GitHub repositories include:\n• Bandit algorithms\n– https://github.com/doerlbh/BanditZoo (Python package)\n– https://github.com/johnmyleswhite/BanditsBook (example)\n• Reinforcement learning algorithms\n– https://github.com/tensorflow/agents (Python package)\n– https://github.com/facebookresearch/ReAgent (Python package)\n– https://github.com/aikorea/awesome-rl (resources)\n– https://github.com/dennybritz/reinforcement-learning (example)\n– https://github.com/udacity/deep-reinforcement-learning (example)\n– https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow (example)\n– https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch (example)\n• Offline Reinforcement learning algorithms\n– https://github.com/takuseno/d3rlpy (Python package)\n– https://github.com/hanjuku-kaso/awesome-offline-rl (resources)\n• Reinforcement learning applications\n– https://github.com/AI4Finance-Foundation/FinRL (RL + finance)\n– https://github.com/microsoft/recommenders (recommendation systems)\n– https://github.com/doerlbh/MiniVox (RL + speaker diarization)\n– https://github.com/doerlbh/awesome-diarization (speaker diarization)\n– https://github.com/doerlbh/MentalRL (RL + psychiatry behavioral modeling)\n– https://github.com/doerlbh/DilemmaRL (RL + multi-agent behavioral modeling)\n8\nNote and Acknowledgements\nThis survey accompanies the tutorial session “Reinforcement Learning and Bandits for Speech and Language Processing”\nheld by the author at INTERSPEECH 2022. We would like to thank the conference organizers and attended audience\nfor valuable feedback and attention. The materials of the tutorial were inspired by and partially borrows from previous\ntutorials on reinforcement learning, offline reinforcement learning and recommendation systems by Sergey Levine,\nDavid Silver, Xiangyu Zhao, Andrea Barraza-Urbina, Dorota Glowacka, Chelsea Finn, Aviral Kumar, Lilian Weng and\nEmma Brunskill. The author would like to thank their inspirations to the creation of our tutorial and this survey.\nReferences\n[1] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International\nJournal of Robotics Research, 32(11):1238–1274, 2013.\n[2] Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki, Khoa Luu, and Marios Savvides. Deep reinforcement\nlearning in computer vision: a comprehensive survey. Artificial Intelligence Review, pages 1–87, 2021.\n[3] Thomas G Fischer. Reinforcement learning in financial markets-a survey. Technical report, FAU Discussion\nPapers in Economics, 2018.\n[4] Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare: A survey.\nACM Computing Surveys (CSUR), 55(1):1–36, 2021.\n42\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\n[5] Victor Uc-Cetina, Nicolas Navarro-Guerrero, Anabel Martin-Gonzalez, Cornelius Weber, and Stefan Wermter.\nSurvey on reinforcement learning for language processing. Artificial Intelligence Review, 56(2):1543–1575,\n2023.\n[6] Zidong Zhang, Dongxia Zhang, and Robert C Qiu. Deep reinforcement learning for power system applications:\nAn overview. CSEE Journal of Power and Energy Systems, 6(1):213–225, 2019.\n[7] Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang\nFeng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural\nnetworks. arXiv preprint arXiv:2107.03342, 2021.\n[8] Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT press\nCambridge, 1998.\n[9] Weiwei Shen, Jun Wang, Yu-Gang Jiang, and Hongyuan Zha. Portfolio choices with orthogonal bandit learning.\nIn Twenty-fourth international joint conference on artificial intelligence, 2015.\n[10] Arthur Charpentier, Romuald Elie, and Carl Remlinger. Reinforcement learning in economics and finance.\nComputational Economics, pages 1–38, 2021.\n[11] Baihan Lin and Djallel Bouneffouf. Optimal epidemic control as a contextual combinatorial bandit with budget.\nIn 2022 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), pages 1–8. IEEE, 2022.\n[12] Baihan Lin. Evolutionary multi-armed bandits with genetic thompson sampling. In 2022 IEEE Congress on\nEvolutionary Computation (CEC). IEEE, 2022.\n[13] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A\nnovel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research,\n18(1):6765–6816, 2017.\n[14] Jack Parker-Holder, Vu Nguyen, and Stephen J Roberts. Provably efficient online hyperparameter optimization\nwith population-based bandits. Advances in Neural Information Processing Systems, 33:17200–17211, 2020.\n[15] Liu Yang, Bo Liu, Leyu Lin, Feng Xia, Kai Chen, and Qiang Yang. Exploring clustering of bandits for online\nrecommendation system. In Fourteenth ACM Conference on Recommender Systems, pages 120–129, 2020.\n[16] Lu Wang, Chengyu Wang, Keqiang Wang, and Xiaofeng He. Biucb: A contextual bandit algorithm for cold-start\nand diversified recommendation. In 2017 IEEE International Conference on Big Knowledge (ICBK), pages\n248–253. IEEE, 2017.\n[17] Maryam Aziz, Emilie Kaufmann, and Marie-Karelle Riviere. On multi-armed bandit designs for dose-finding\nclinical trials. Journal of Machine Learning Research, 22(1-38):4, 2021.\n[18] Sofía S Villar, Jack Bowden, and James Wason. Multi-armed bandit models for the optimal design of clinical\ntrials: benefits and challenges. Statistical science: a review journal of the Institute of Mathematical Statistics,\n30(2):199, 2015.\n[19] Baihan Lin, Guillermo Cecchi, Djallel Bouneffouf, Jenna Reinen, and Irina Rish. Unified models of human\nbehavioral agents in bandits, contextual bandits and RL. arXiv preprint arXiv:2005.04544, 2020.\n[20] Baihan Lin, Guillermo Cecchi, Djallel Bouneffouf, Jenna Reinen, and Irina Rish. Models of human behavioral\nagents in bandits, contextual bandits and rl. In Human Brain and Artificial Intelligence: Second International\nWorkshop, HBAI 2020, Held in Conjunction with IJCAI-PRICAI 2020, Yokohama, Japan, January 7, 2021,\nRevised Selected Papers 2, pages 14–33. Springer, 2021.\n[21] Djallel Bouneffouf, Irina Rish, and Guillermo A Cecchi. Bandit models of human behavior: Reward processing\nin mental disorders. In International Conference on Artificial General Intelligence, pages 237–248. Springer,\n2017.\n[22] Suhrid Satyal, Ingo Weber, Hye-young Paik, Claudio Di Ciccio, and Jan Mendling. Ab testing for process\nversions with contextual multi-armed bandit algorithms. In International Conference on Advanced Information\nSystems Engineering, pages 19–34. Springer, 2018.\n[23] Ding Xiang, Rebecca West, Jiaqi Wang, Xiquan Cui, and Jinzhou Huang. Multi armed bandit vs. a/b tests\nin e-commence-confidence interval and hypothesis test power perspectives. In Proceedings of the 28th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining, pages 4204–4214, 2022.\n[24] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal\nof artificial intelligence research, 4:237–285, 1996.\n[25] Nicolo Cesa-Bianchi and Paul Fischer. Finite-time regret bounds for the multiarmed bandit problem. In ICML,\nvolume 98, pages 100–108. Citeseer, 1998.\n43\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\n[26] Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA,\nUSA, 1st edition, 1998.\n[27] Joannes Vermorel and Mehryar Mohri. Multi-armed bandit algorithms and empirical evaluation. In European\nconference on machine learning, pages 437–448. Springer, 2005.\n[28] R Duncan Luce. Individual choice behavior: A theoretical analysis. Courier Corporation, 2012.\n[29] David R Shanks, Richard J Tunney, and John D McCarthy. A re-examination of probability matching and rational\nchoice. Journal of Behavioral Decision Making, 15(3):233–250, 2002.\n[30] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit\nproblem. SIAM Journal on Computing, 32(1):48–77, 2002.\n[31] T. L. Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied\nMathematics, 6(1):4–22, 1985.\n[32] W.R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two\nsamples. Biometrika, 25:285–294, 1933.\n[33] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in neural\ninformation processing systems, pages 2249–2257, 2011.\n[34] Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In\nCOLT 2012 - The 25th Annual Conference on Learning Theory, June 25-27, 2012, Edinburgh, Scotland, pages\n39.1–39.26, 2012.\n[35] Alessandro Lazaric, Emma Brunskill, et al. Online stochastic optimization under correlated bandit feedback. In\nInternational Conference on Machine Learning, pages 1557–1565. PMLR, 2014.\n[36] Peter Auer and Nicolò Cesa-Bianchi. On-line learning with malicious noise and the closure algorithm. Ann.\nMath. Artif. Intell., 23(1-2):83–99, 1998.\n[37] Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit\nproblem. SIAM J. Comput., 32(1):48–77, 2002.\n[38] Djallel Bouneffouf and Raphaël Féraud. Multi-armed bandit problem with known trend. Neurocomputing,\n205:16–21, 2016.\n[39] Aurélien Garivier and Eric Moulines. On upper-confidence bound policies for non-stationary bandit problems.\narXiv preprint arXiv:0805.3415, 2008.\n[40] Baihan Lin, Djallel Bouneffouf, Guillermo A Cecchi, and Irina Rish. Contextual bandit with adaptive feature\nextraction. In 2018 IEEE International Conference on Data Mining Workshops (ICDMW), pages 937–944. IEEE,\n2018.\n[41] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process optimization in the\nbandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995, 2009.\n[42] Francesco Trovò, Stefano Paladino, Marcello Restelli, and Nicola Gatti. Budgeted multi–armed bandit in\ncontinuous action space. In Proceedings of the Twenty-second European Conference on Artificial Intelligence,\npages 560–568, 2016.\n[43] Yizao Wang, Jean-Yves Audibert, and Rémi Munos. Algorithms for infinitely many-armed bandits. Advances in\nNeural Information Processing Systems, 21, 2008.\n[44] Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework and applications.\nIn International conference on machine learning, pages 151–159. PMLR, 2013.\n[45] Baihan Lin and Djallel Bouneffouf. Optimal epidemic control as a contextual combinatorial bandit with budget.\narXiv preprint arXiv:2106.15808, 2021.\n[46] Tor Lattimore. Regret analysis of the finite-horizon gittins index strategy for multi-armed bandits. In Conference\non Learning Theory, pages 1214–1245. PMLR, 2016.\n[47] Tomáš Kocák, Gergely Neu, Michal Valko, and Rémi Munos. Efficient learning by implicit exploration in bandit\nproblems with side observations. Advances in Neural Information Processing Systems, 27, 2014.\n[48] Baihan Lin. Online semi-supervised learning in contextual bandits with episodic reward. In Australasian Joint\nConference on Artificial Intelligence, pages 407–419. Springer, 2020.\n[49] Wenkui Ding, Tao Qin, Xu-Dong Zhang, and Tie-Yan Liu. Multi-armed bandit with budget constraint and\nvariable costs. In Twenty-Seventh AAAI Conference on Artificial Intelligence, 2013.\n44\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\n[50] Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks. Journal of the\nACM (JACM), 65(3):1–55, 2018.\n[51] Aleksandrs Slivkins et al. Introduction to multi-armed bandits. Foundations and Trends® in Machine Learning,\n12(1-2):1–286, 2019.\n[52] John Langford and Tong Zhang. Epoch-greedy algorithm for multi-armed bandits with side information.\nAdvances in Neural Information Processing Systems (NIPS 2007), 20:1, 2007.\n[53] Wei Chu, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandits with linear payoff functions. In\nGeoffrey J. Gordon, David B. Dunson, and Miroslav Dudik, editors, AISTATS, volume 15 of JMLR Proceedings,\npages 208–214. JMLR.org, 2011.\n[54] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In ICML (3),\npages 127–135, 2013.\n[55] Li Zhou. A survey on contextual multi-armed bandits. arXiv preprint arXiv:1508.03326, 2015.\n[56] D.P. Bertsekas and J.N. Tsitsiklis. Neuro-dynamic programming. Athena Scientific, 1996.\n[57] Lilian Weng. Policy gradient algorithms. lilianweng.github.io, 2018.\n[58] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous\ncontrol using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\n[59] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for\nreinforcement learning with function approximation. Advances in neural information processing systems, 12,\n1999.\n[60] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information processing systems,\n12, 1999.\n[61] Stuart Russell. Learning agents for uncertain environments. In Proceedings of the eleventh annual conference on\nComputational learning theory, pages 101–103, 1998.\n[62] Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml, volume 1, page 2,\n2000.\n[63] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven\nreinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n[64] Stuart Armstrong and Sören Mindermann. Occam’s razor is insufficient to infer the preferences of irrational\nagents. Advances in neural information processing systems, 31, 2018.\n[65] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of\nthe twenty-first international conference on Machine learning, page 1. ACM, 2004.\n[66] Saurabh Arora and Prashant Doshi. A survey of inverse reinforcement learning: Challenges, methods and\nprogress. Artificial Intelligence, 297:103500, 2021.\n[67] Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. Maximum margin planning. In Proceedings of the\n23rd international conference on Machine learning, pages 729–736, 2006.\n[68] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement\nlearning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008.\n[69] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020.\n[70] Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial\nnetworks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852, 2016.\n[71] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural information\nprocessing systems, 29, 2016.\n[72] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy\noptimization. In International conference on machine learning, pages 49–58. PMLR, 2016.\n[73] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy\noptimization. In International conference on machine learning, pages 1889–1897. PMLR, 2015.\n[74] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: A survey of\nlearning methods. ACM Computing Surveys (CSUR), 50(2):1–35, 2017.\n[75] Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al. An\nalgorithmic perspective on imitation learning. Foundations and Trends® in Robotics, 7(1-2):1–179, 2018.\n45\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\n[76] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction\nto no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence\nand statistics, pages 627–635. JMLR Workshop and Conference Proceedings, 2011.\n[77] Baihan Lin, Djallel Bouneffouf, and Guillermo Cecchi. Online learning in iterated prisoner’s dilemma to mimic\nhuman behavior. In Pacific Rim International Conference on Artificial Intelligence. Springer, 2022.\n[78] Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine. Should i run offline reinforcement learning or\nbehavioral cloning? In International Conference on Learning Representations, 2021.\n[79] Yih-Liang Shen, Chao-Yuan Huang, Syu-Siang Wang, Yu Tsao, Hsin-Min Wang, and Tai-Shih Chi. Rein-\nforcement learning based speech enhancement for robust speech recognition. In ICASSP 2019-2019 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6750–6754. IEEE, 2019.\n[80] Taku Kala and Takahiro Shinozaki. Reinforcement learning of speech recognition system based on policy\ngradient and hypothesis selection. In 2018 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5759–5763. IEEE, 2018.\n[81] Hoon Chung, Hyeong-Bae Jeon, and Jeon Gue Park. Semi-supervised training for sequence-to-sequence speech\nrecognition using reinforcement learning. In 2020 International Joint Conference on Neural Networks (IJCNN),\npages 1–6. IEEE, 2020.\n[82] Thejan Rajapakshe, Rajib Rana, Siddique Latif, Sara Khalifa, and Björn W Schuller. Pre-training in deep\nreinforcement learning for automatic speech recognition. arXiv preprint arXiv:1910.11256, 2019.\n[83] Anastasia Kuznetsova, Anurag Kumar, and Francis M Tyers. A bandit approach to curriculum generation for\nautomatic speech recognition. arXiv preprint arXiv:2102.03662, 2021.\n[84] Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura. Sequence-to-sequence asr optimization via reinforcement\nlearning. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages\n5829–5833. IEEE, 2018.\n[85] Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura. End-to-end speech recognition sequence training with\nreinforcement learning. IEEE Access, 7:79758–79769, 2019.\n[86] Shigeki Karita, Atsunori Ogawa, Marc Delcroix, and Tomohiro Nakatani. Sequence training of encoder-decoder\nmodel using policy gradient for end-to-end speech recognition. In 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages 5839–5843. IEEE, 2018.\n[87] Baihan Lin and Xinxin Zhang. Voiceid on the fly: A speaker recognition system that learns from scratch. In\nINTERSPEECH, 2020.\n[88] Baihan Lin and Xinxin Zhang. Speaker diarization as a fully online learning problem in minivox. arXiv preprint\narXiv:2006.04376, 2020.\n[89] Baihan Lin and Xinxin Zhang. Speaker diarization as a fully online bandit learning problem in minivox. In Asian\nConference on Machine Learning, pages 1660–1674. PMLR, 2021.\n[90] Baihan Lin. Voice2Alliance: automatic speaker diarization and quality assurance of conversational alignment. In\nINTERSPEECH, 2022.\n[91] Baihan Lin and Xinxin Zhang. A reinforcement learning framework for online speaker diarization. arXiv preprint\narXiv:2302.10924, 2023.\n[92] He Bai, Yu Zhou, Jiajun Zhang, Liang Zhao, Mei-Yuh Hwang, and Chengqing Zong. Source-critical re-\ninforcement learning for transferring spoken language understanding to a new language.\narXiv preprint\narXiv:1808.06167, 2018.\n[93] Jerome R Bellegarda. Spoken language understanding for natural interaction: The siri experience. Natural\ninteraction with robots, knowbots and smartphones, pages 3–14, 2014.\n[94] Emmanuel Ferreira, Alexandre Reiffers Masson, Bassam Jabaian, and Fabrice Lefèvre. Adversarial bandit for\nonline interactive active learning of zero-shot spoken language understanding. In 2016 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pages 6155–6159. IEEE, 2016.\n[95] Baihan Lin and Xinxin Zhang. ispeak: Interactive spoken language understanding system for children with\nspeech and language disorders. In 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2022.\n[96] Baihan Lin and Xinxin Zhang. Interactive spoken language understanding system for children with speech and\nlanguage disorders. arXiv preprint, 2023.\n[97] Zhi Chen, Lu Chen, Xiang Zhou, and Kai Yu. Deep reinforcement learning for on-line dialogue state tracking.\narXiv preprint arXiv:2009.10321, 2020.\n46\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\n[98] Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based games using\ndeep reinforcement learning. arXiv preprint arXiv:1506.08941, 2015.\n[99] Fabian Moerchen, Patrick Ernst, and Giovanni Zappella. Personalizing natural language understanding using\nmulti-armed bandits and implicit feedback. In Proceedings of the 29th ACM International Conference on\nInformation & Knowledge Management, pages 2661–2668, 2020.\n[100] Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefenstette,\nShimon Whiteson, and Tim Rocktäschel. A survey of reinforcement learning informed by natural language.\narXiv preprint arXiv:1906.03926, 2019.\n[101] Adam Vogel and Dan Jurafsky. Learning to follow navigational directions. In Proceedings of the 48th annual\nmeeting of the association for computational linguistics, pages 806–814, 2010.\n[102] Yaser Keneshloo, Tian Shi, Naren Ramakrishnan, and Chandan K Reddy. Deep reinforcement learning for\nsequence-to-sequence models. IEEE transactions on neural networks and learning systems, 31(7):2469–2489,\n2019.\n[103] Devang S Ram Mohan, Raphael Lenain, Lorenzo Foglianti, Tian Huey Teh, Marlene Staib, Alexandra Torresquin-\ntero, and Jiameng Gao. Incremental text to speech for neural sequence-to-sequence models using reinforcement\nlearning. arXiv preprint arXiv:2008.03096, 2020.\n[104] Rui Liu, Berrak Sisman, and Haizhou Li. Reinforcement learning for emotional text-to-speech synthesis with\nimproved emotion discriminability. arXiv preprint arXiv:2104.01408, 2021.\n[105] Jerry Gibson and Hoontaek Oh. A reinforcement learning approach to speech coding. Information, 13(7):331,\n2022.\n[106] Nan Jiang, Sheng Jin, Zhiyao Duan, and Changshui Zhang. Rl-duet: Online music accompaniment generation\nusing deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,\npages 710–718, 2020.\n[107] Zhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing Huang. Toward diverse text generation with inverse\nreinforcement learning. arXiv preprint arXiv:1804.11258, 2018.\n[108] Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. Deep reinforcement learning\nfor dialogue generation. arXiv preprint arXiv:1606.01541, 2016.\n[109] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy\ngradient. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017.\n[110] Baihan Lin, Djallel Bouneffouf, and Guillermo Cecchi. Split Q Learning: Reinforcement Learning with Two-\nStream Rewards. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence,\nIJCAI-19, pages 6448–6449. AAAI Press, International Joint Conferences on Artificial Intelligence Organization,\n7 2019.\n[111] Zichao Li, Xin Jiang, Lifeng Shang, and Hang Li. Paraphrase generation with deep reinforcement learning.\narXiv preprint arXiv:1711.00279, 2017.\n[112] Nina Dethlefs and Heriberto Cuayáhuitl. Combining hierarchical reinforcement learning and bayesian networks\nfor natural language generation in situated dialogue. In Proceedings of the 13th European Workshop on Natural\nLanguage Generation, pages 110–120, 2011.\n[113] Nina Dethlefs and Heriberto Cuayáhuitl. Hierarchical reinforcement learning and hidden markov models for\ntask-oriented natural language generation. In Proceedings of the 49th Annual Meeting of the Association for\nComputational Linguistics: Human Language Technologies, pages 654–659, 2011.\n[114] Matthieu Riou, Bassam Jabaian, Stéphane Huet, and Fabrice Lefèvre. Reinforcement adaptation of an attention-\nbased neural natural language generator for spoken dialogue systems. Dialogue & Discourse, 10:1–19, 2019.\n[115] Matthieu Riou, Bassam Jabaian, Stéphane Huet, and Fabrice Lefèvre. Online adaptation of an attention-based\nneural network for natural language generation. In Conference of the International Speech Communication\nAssociation (Interspeech), 2017.\n[116] Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language\nusing reinforcement learning. arXiv preprint arXiv:1709.00103, 2017.\n[117] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human\nfeedback. arXiv preprint arXiv:2203.02155, 2022.\n47\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\n[118] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen,\nAnna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback.\narXiv preprint arXiv:2212.08073, 2022.\n[119] Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi, and Kush R Varshney. Towards healthy AI: Large language\nmodels need therapists too. arXiv preprint arXiv:2304.00416, 2023.\n[120] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877–1901, 2020.\n[121] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022.\n[122] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement\nlearning from human preferences. Advances in neural information processing systems, 30, 2017.\n[123] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario\nAmodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information\nProcessing Systems, 33:3008–3021, 2020.\n[124] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat\nMcAleese, and Geoffrey Irving.\nRed teaming language models with language models.\narXiv preprint\narXiv:2202.03286, 2022.\n[125] Chongming Gao, Wenqiang Lei, Xiangnan He, Maarten de Rijke, and Tat-Seng Chua. Advances and challenges\nin conversational recommender systems: A survey. AI Open, 2:100–126, 2021.\n[126] Shijun Li, Wenqiang Lei, Qingyun Wu, Xiangnan He, Peng Jiang, and Tat-Seng Chua. Seamlessly unifying\nattributes and items: Conversational recommendation for cold-start users. ACM Transactions on Information\nSystems (TOIS), 39(4):1–29, 2021.\n[127] Baihan Lin, Guillermo Cecchi, and Djallel Bouneffouf. SupervisorBot: NLP-Annotated Real-Time Recom-\nmendations of Psychotherapy Treatment Strategies with Deep Reinforcement Learning. In Proceedings of the\nThirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23. International Joint Conferences\non Artificial Intelligence Organization, 8 2023.\n[128] Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. Towards conversational recommender\nsystems. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data\nmining, pages 815–824, 2016.\n[129] Xiaoying Zhang, Hong Xie, Hang Li, and John CS Lui. Conversational contextual bandit: Algorithm and\napplication. In Proceedings of the web conference 2020, pages 662–672, 2020.\n[130] Baihan Lin, Guillermo Cecchi, and Djallel Bouneffouf. Helping therapists with NLP-annotated recommendation.\nIn Joint Proceedings of the ACM IUI Workshops, 2023.\n[131] Baihan Lin, Guillermo Cecchi, and Djallel Bouneffouf. Psychotherapy AI companion with reinforcement\nlearning recommendations and interpretable policy dynamics. In Proceedings of the Web Conference 2023, 2023.\n[132] Baihan Lin, Guillermo Cecchi, and Djallel Bouneffouf. Deep annotation of therapeutic working alliance in\npsychotherapy. In International Workshop on Health Intelligence. Springer, 2023.\n[133] Baihan Lin, Guillermo Cecchi, and Djallel Bouneffouf. Working alliance transformer for psychotherapy dialogue\nclassification. arXiv preprint arXiv:2210.15603, 2022.\n[134] Baihan Lin. Knowledge management system with NLP-assisted annotations: A brief survey and outlook. In\nCIKM Workshops, 2022.\n[135] Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi, and Ravi Tejwani. Neural topic modeling of psychotherapy\nsessions. In International Workshop on Health Intelligence. Springer, 2023.\n[136] Baihan Lin. Computational inference in cognitive science: Operational, societal and ethical considerations.\narXiv preprint arXiv:2210.13526, 2022.\n[137] Ji He, Mari Ostendorf, Xiaodong He, Jianshu Chen, Jianfeng Gao, Lihong Li, and Li Deng. Deep reinforcement\nlearning with a combinatorial action space for predicting popular reddit threads. arXiv preprint arXiv:1606.03667,\n2016.\n[138] Ji He, Mari Ostendorf, and Xiaodong He. Reinforcement learning with external knowledge and two-stage\nq-functions for predicting popular reddit threads. arXiv preprint arXiv:1704.06217, 2017.\n48\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\n[139] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and\nMartin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n[140] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic\npolicy gradient algorithms. In International conference on machine learning, pages 387–395. PMLR, 2014.\n[141] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver,\nand Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971,\n2015.\n[142] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\n[143] Mark Collier and Hector Urdiales Llorens.\nDeep contextual multi-armed bandits.\narXiv preprint\narXiv:1807.09809, 2018.\n[144] Dalin Guo, Sofia Ira Ktena, Pranay Kumar Myana, Ferenc Huszar, Wenzhe Shi, Alykhan Tejani, Michael Kneier,\nand Sourav Das. Deep bayesian bandits: Exploring in online personalized recommendations. In Fourteenth ACM\nConference on Recommender Systems, pages 456–461, 2020.\n[145] Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration. In\nInternational Conference on Machine Learning, pages 11492–11502. PMLR, 2020.\n[146] Yuxi Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274, 2017.\n[147] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review,\nand perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n[148] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones,\nShixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human\npreferences in dialog. arXiv preprint arXiv:1907.00456, 2019.\n[149] Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty\nPublication Series, page 80, 2000.\n[150] Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In International\nConference on Machine Learning, pages 652–661. PMLR, 2016.\n[151] Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem of off-policy\ntemporal-difference learning. The Journal of Machine Learning Research, 17(1):2603–2631, 2016.\n[152] Sergey Levine and Vladlen Koltun. Guided policy search. In International conference on machine learning,\npages 1–9. PMLR, 2013.\n[153] Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesvári, and Eric\nWiewiora. Fast gradient-descent methods for temporal-difference learning with linear function approximation.\nIn Proceedings of the 26th annual international conference on machine learning, pages 993–1000, 2009.\n[154] Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. The Journal of Machine Learning\nResearch, 4:1107–1149, 2003.\n[155] Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement learning\nwith offline datasets. arXiv preprint arXiv:2006.09359, 2020.\n[156] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement\nlearning. In International Conference on Machine Learning, pages 104–114. PMLR, 2020.\n[157] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement\nlearning. arXiv preprint arXiv:2006.04779, 2020.\n[158] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline\nreinforcement learning. arXiv preprint arXiv:2005.05951, 2020.\n[159] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data\nengineering, 22(10):1345–1359, 2009.\n[160] Sebastian Ruder. Neural transfer learning for natural language processing. PhD thesis, NUI Galway, 2019.\n[161] Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal of\nMachine Learning Research, 10(7), 2009.\n[162] Felipe Leno Da Silva and Anna Helena Reali Costa. A survey on transfer learning for multiagent reinforcement\nlearning systems. Journal of Artificial Intelligence Research, 64:645–703, 2019.\n49\nReinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook\n[163] Felipe Leno Silva and Anna Helena Reali Costa. Transfer learning for multiagent reinforcement learning systems.\nIn Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI), pages\n3982–3983. Springer, 2016.\n[164] Michael Wooldridge. An introduction to multiagent systems. John wiley & sons, 2009.\n[165] Gabriel Murray, Giuseppe Carenini, and Shafiq Joty. Nlp for conversations: Sentiment, summarization, and\ngroup dynamics. In Proceedings of the 27th International Conference on Computational Linguistics: Tutorial\nAbstracts, pages 1–4, 2018.\n[166] Liviu Panait and Sean Luke. Cooperative multi-agent learning: The state of the art. Autonomous agents and\nmulti-agent systems, 11(3):387–434, 2005.\n[167] Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent complexity via\nmulti-agent competition. arXiv preprint arXiv:1710.03748, 2017.\n[168] Baihan Lin, Djallel Bouneffouf, and Guillermo Cecchi. Online learning in iterated prisoner’s dilemma to mimic\nhuman behavior. In Pacific Rim International Conference on Artificial Intelligence. Springer, 2022.\n[169] Abhishek Das, Satwik Kottur, José MF Moura, Stefan Lee, and Dhruv Batra. Learning cooperative visual dialog\nagents with deep reinforcement learning. In Proceedings of the IEEE international conference on computer\nvision, pages 2951–2960, 2017.\n[170] Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent populations.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n[171] Baihan Lin, Djallel Bouneffouf, and Irina Rish. A survey on compositional generalization in applications. arXiv\npreprint arXiv:2302.01067, 2023.\n[172] Satwik Kottur, José MF Moura, Stefan Lee, and Dhruv Batra. Natural language does not emerge naturally in\nmulti-agent dialog. arXiv preprint arXiv:1706.08502, 2017.\n[173] Michael Walsh, Robert Kelly, Greg MP O’Hare, Julie Carson-Berndsen, and Tarek Abu-Amer. A multi-agent\ncomputational linguistic approach to speech recognition. In The 18th International Joint Conference on Artificial\nIntelligence (IJCAI-03), 9th-15th August, Acapulco, Mexico, 2003. IJCAI, 2003.\n[174] Zalimhan Nagoev, Larisa Lyutikova, and Irina Gurtueva. Model for automatic speech recognition using multi-\nagent recursive cognitive architecture. Procedia computer science, 145:386–392, 2018.\n[175] Kalyanmoy Deb. Multi-objective optimization. In Search methodologies, pages 403–449. Springer, 2014.\n[176] Samuel M McClure, Michele K York, and P Read Montague. The neural substrates of reward processing in\nhumans: the modern role of FMRI. The Neuroscientist, 10(3):260–268, 2004.\n[177] Daeyeol Lee, Hyojung Seo, and Min Whan Jung. Neural basis of reinforcement learning and decision making.\nAnnual review of neuroscience, 35:287, 2012.\n[178] Tiago V Maia and Michael J Frank. From reinforcement learning models to psychiatric and neurological\ndisorders. Nature neuroscience, 14(2):154–162, 2011.\n[179] Baihan Lin, Guillermo Cecchi, Djallel Bouneffouf, Jenna Reinen, and Irina Rish. A story of two streams:\nReinforcement learning models from human behavior and neuropsychiatry. In Proceedings of the Nineteenth In-\nternational Conference on Autonomous Agents and Multi-Agent Systems, AAMAS-20, pages 744–752, Auckland,\nNew Zealand, 5 2020. International Foundation for Autonomous Agents and Multiagent Systems.\n[180] Zhibo Yang, Lihan Huang, Yupei Chen, Zijun Wei, Seoyoung Ahn, Gregory Zelinsky, Dimitris Samaras, and\nMinh Hoai. Predicting goal-directed human attention using inverse reinforcement learning. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, pages 193–202, 2020.\n[181] Djallel Bouneffouf, Irina Rish, and Charu Aggarwal. Survey on applications of multi-armed and contextual\nbandits. In 2020 IEEE Congress on Evolutionary Computation (CEC), pages 1–8. IEEE, 2020.\n[182] M Mehdi Afsar, Trafford Crump, and Behrouz Far. Reinforcement learning based recommender systems: A\nsurvey. ACM Computing Surveys (CSUR), 2021.\n[183] Daniel W Otter, Julian R Medina, and Jugal K Kalita. A survey of the usages of deep learning for natural\nlanguage processing. IEEE transactions on neural networks and learning systems, 32(2):604–624, 2020.\n[184] Akanksha Rai Sharma and Pranav Kaushik. Literature survey of statistical, deep and reinforcement learning in\nnatural language processing. In 2017 International Conference on Computing, Communication and Automation\n(ICCCA), pages 350–354. IEEE, 2017.\n[185] Siddique Latif, Rajib Rana, Sara Khalifa, Raja Jurdak, Junaid Qadir, and Björn W Schuller. Deep representation\nlearning in speech processing: Challenges, recent advances, and future trends. arXiv preprint arXiv:2001.00378,\n2020.\n50\n",
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.LG",
    "cs.SD",
    "eess.AS"
  ],
  "published": "2022-10-24",
  "updated": "2023-10-19"
}