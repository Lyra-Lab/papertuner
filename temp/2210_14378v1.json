{
  "id": "http://arxiv.org/abs/2210.14378v1",
  "title": "Bilingual Lexicon Induction for Low-Resource Languages using Graph Matching via Optimal Transport",
  "authors": [
    "Kelly Marchisio",
    "Ali Saad-Eldin",
    "Kevin Duh",
    "Carey Priebe",
    "Philipp Koehn"
  ],
  "abstract": "Bilingual lexicons form a critical component of various natural language\nprocessing applications, including unsupervised and semisupervised machine\ntranslation and crosslingual information retrieval. We improve bilingual\nlexicon induction performance across 40 language pairs with a graph-matching\nmethod based on optimal transport. The method is especially strong with low\namounts of supervision.",
  "text": "Bilingual Lexicon Induction for Low-Resource Languages\nusing Graph Matching via Optimal Transport\nKelly Marchisio1, Ali Saad-Eldin3, Kevin Duh1,4,\nCarey Priebe2,4, Philipp Koehn1\nDepartment of 1Computer Science, 2Department of Applied Mathematics and Statistics,\n3Department of Biomedical Engineering, 4Human Language Technology Center of Excellence\nJohns Hopkins University\n{kmarc,asaadel1}@jhu.edu\nkevinduh@cs.jhu.edu, {cep, phi}@jhu.edu\nAbstract\nBilingual lexicons form a critical component\nof various natural language processing applica-\ntions, including unsupervised and semisuper-\nvised machine translation and crosslingual in-\nformation retrieval. We improve bilingual lexi-\ncon induction performance across 40 language\npairs with a graph-matching method based on\noptimal transport. The method is especially\nstrong with low amounts of supervision.\n1\nIntroduction\nBilingual lexicon induction (BLI) from word em-\nbedding spaces is a popular task with a large body\nof existing literature (e.g. Mikolov et al., 2013;\nArtetxe et al., 2018; Conneau et al., 2018; Patra\net al., 2019; Shi et al., 2021). The goal is to ex-\ntract a dictionary of translation pairs given sepa-\nrate language-speciﬁc embedding spaces, which\ncan then be used to bootstrap downstream tasks\nsuch as cross-lingual information retrieval and\nunsupervised/semi-supervised machine translation.\nA great challenge across NLP is maintaining\nperformance in low-resource scenarios. A com-\nmon criticism of the BLI and low-resource MT\nliterature is that while claims are made about di-\nverse and under-resourced languages, research is\noften performed on down-sampled corpora of high-\nresource, highly-related languages on similar do-\nmains (Artetxe et al., 2020). Such corpora are not\ngood proxies for true low-resource languages ow-\ning to data challenges such as dissimilar scripts,\ndomain shift, noise, and lack of sufﬁcient bitext\n(Marchisio et al., 2020). These differences can lead\nto dissimilarity between the embedding spaces (de-\ncreasing isometry), causing BLI to fail (Søgaard\net al., 2018; Nakashole and Flauger, 2018; Ormaz-\nabal et al., 2019; Glavaš et al., 2019; Vuli´c et al.,\n2019; Patra et al., 2019; Marchisio et al., 2020).\nThere are two axes by which a language dataset\nis considered “low-resource\". First, the language it-\nself may be a low-resource language: one for which\nlittle bitext and/or monolingual text exists. Even\nfor high-resource languages, the long tail of words\nmay have poorly trained word embeddings due rar-\nity in the dataset (Gong et al., 2018; Czarnowska\net al., 2019).\nIn the data-poor setting of true\nlow-resource languages, a great majority of words\nhave little representation in the corpus, resulting in\npoorly-trained embeddings for a large proportion\nof them. The second axis is low-supervision. Here,\nthere are few ground-truth examples from which to\nlearn. For BLI from word embedding spaces, low-\nsupervision means there are few seeds from which\nto induce a relationship between spaces, regardless\nof the quality of the spaces themselves.\nWe bring a new algorithm for graph-matching\nbased on optimal transport (OT) to the NLP and\nBLI literature. We evaluate using 40 language\npairs under varying amounts of supervision. The\nmethod works strikingly well across language pairs,\nespecially in low-supervision contexts. As low-\nsupervision on low-resource languages reﬂects the\nreal-world use case for BLI, this is an encouraging\ndevelopment on realistic scenarios.\n2\nBackground\nThe typical baseline approach for BLI from word\nembedding spaces assumes that spaces can be\nmapped via linear transformation. Such methods\ntypically involve solutions to the Procrustes prob-\nlem (see Gower et al. (2004) for a review). Alter-\nnatively, a graph-based view considers words as\nnodes in undirected weighted graphs, where edges\nare the distance between words. Methods taking\nthis view do not assume a linear mapping of the\nspaces exists, allowing for more ﬂexible matching.\nBLI from word embedding spaces\nAssume\nseparately-trained monolingual word embedding\nspaces: X ∈Rn×d, Y ∈Rm×d where n/m are\nthe source/target language vocabulary sizes and d is\nthe embedding dimension. We build the matrices X\narXiv:2210.14378v1  [cs.CL]  25 Oct 2022\nand Y of seeds from X and Y, respectively, such\nthat given s seed pairs (x1, y1), (x2, y2), ...(xs, ys),\nthe ﬁrst row of X is x1, the second row is x2, etc.\nWe build Y analogously for the y-component of\neach seed pair. The goal is to recover matches for\nthe X \\ X and/or Y \\ Y non-seed words.\nProcrustes\nMany BLI methods use solutions to\nthe Procrustes problem (e.g. Artetxe et al., 2019b;\nConneau et al., 2018; Patra et al., 2019). These\ncompute the optimal transform W to map seeds:\nmin\nW∈Rd×d ||XW −Y||2\nF\n(1)\nOnce solved for W, then XW and Y live in a\nshared space and translation pairs can be extracted\nvia nearest-neighbor search. Constrained to the\nspace of orthogonal matrices, Equation 1 has a\nsimple closed-form solution (Schönemann, 1966):\nW = VUT\nUΣV = SVD(Y\nT X)\nGraph View\nHere, words are nodes in mono-\nlingual graphs Gx, Gy ∈Rn×n, and cells in\nGx, Gy are edge weights representing distance be-\ntween words. As is common in NLP, we use co-\nsine similarity. The objective function is Equa-\ntion 2, where Π is the set of permutation matri-\nces.1 Intuitively, PGyPT ﬁnds the optimal rela-\nbeling of Gy to align with Gx. This “minimizes\nedge-disagreements\" between Gx and Gy. This\ngraph-matching objective is NP-Hard. Equation 3\nis equivalent.\nmin\nP∈Π ||Gx −PGyPT||2\nF\n(2)\nmax\nP∈Π trace(GxTPGyPT)\n(3)\nEx. Take source words x1, x2. We wish to recover\nvalid translations yx1, yx2. If distance(x1, x2)=\ndistance(yx1, yx2), a solution P can have an edge-\ndisagreement of 0 here. We then extract yx1, yx2\nas translations of x1, x2. In reality, though, it is un-\nlikely that distance(x1, x2) = distance(yx1, yx2).\nBecause Equation 2 ﬁnds the ideal P to minimize\nedge disagreements over the entire graphs, we hope\nthat nodes paired by P are valid translations. If Gx\nand Gy are isomorphic and there is a unique solu-\ntion, then P correctly recovers all translations.\nGraph-matching is an active research ﬁeld and\nis computationally prohibitive on large graphs,\n1A permutation matrix represents a one-to-one mapping:\nThere is a single 1 in each row and column, and 0 elsewhere.\nbut approximation algorithms exist. BLI involves\nmatching large, non-isomorphic graphs—among\nthe greatest challenges for graph-matching.\n2.1\nFAQ Algorithm for Graph Matching\nVogelstein et al. (2015)’s Fast Approximate\nQuadratic Assignment Problem algorithm (FAQ)\nuses gradient ascent to approximate a solution to\nEquation 2. Motivated by “connectonomics\" in\nneuroscience (the study of brain graphs with bio-\nlogical [groups of] neurons as nodes and neuronal\nconnections as edges), FAQ was designed to per-\nform accurately and efﬁciently on large graphs.\nFAQ relaxes the search space of Equation 3 to al-\nlow any doubly-stochastic matrix (the set D). Each\ncell in a doubly-stochastic matrix is a non-negative\nreal number and each row/column sums to 1. The\nset D thus contains Π but is much larger. Relaxing\nthe search space makes it easier to optimize Equa-\ntion 3 via gradient ascent/descent.2 FAQ solves\nthe objective with the Frank-Wolfe method (Frank\net al., 1956) then projects back to a permutation\nmatrix.\nAlgorithm\n1\nis\nFAQ;\nf (P)\n=\ntrace(GxTPGyPT).\nThese may be built as\nGx = XXT and Gy = YYT. Gx and Gy need\nnot have the same dimensionality. Step 2 ﬁnds a\npermutation matrix approximation Q{i} to P {i} in\nthe direction of the gradient. Finding such a P re-\nquires approximation when P is high-dimensional.\nHere, it is solved via the Hungarian Algorithm\n(Kuhn, 1955; Jonker and Volgenant, 1987), whose\nsolution is a permutation matrix.\nFinally, P n\nis projected back onto to the space of permuta-\ntion matrices.\nSeeded Graph Matching (SGM;\nFishkind et al., 2019) is a variant of FAQ allow-\ning for supervision, and was recently shown to be\neffective for BLI by Marchisio et al. (2021). The\ninterested reader may ﬁnd Vogelstein et al. (2015)\nand Fishkind et al. (2019) enlightening for descrip-\ntive intuitions of the FAQ and SGM algorithms.3\nStrengths/Weaknesses\nFAQ/SGM perform well\nsolving the exact graph-matching problem: where\ngraphs are isomorphic and a full matching exists.\nIn reality, large graphs are rarely isomorphic. For\nBLI, languages have differing vocabulary size, syn-\n2“descent\" for the Quadratic Assignment Problem, “as-\ncent\" for the Graph Matching Problem. The optimization ob-\njectives are equivalent: See Vogelstein et al. (2015) for a proof.\n3’Section 3: Fast Approximate QAP Algorithm’ (Vogel-\nstein et al., 2015), ’Section 2.2. From FAQ to SGM’ (Fishkind\net al., 2019).\nAlgorithm 1 FAQ Algorithm for Graph Matching\nLet: Gx, Gy ∈Rn×n, P {0} ∈D (dbl-stoch.)\nwhile stopping criterion not met do\n1. Calculate ∇f(P {i}):\n∇f(P {i}) = GxP {i}GT\ny + GT\nx P {i}Gy\n2. Q{i} = permutation matrix approx. to ∇f(P {i})\nvia Hungarian Algorithm\n3. Calculate step size:\narg max\nα∈[0,1]\nf (αP {i} + (1 −α)Q{i})\n4. Update P {i+1} := αP {i} + (1 −α)Q{i}\nend while\nreturn permutation matrix approx. to P {n} via Hung. Alg.\nonyms/antonyms, and idiosyncratic concepts; it is\nmore natural to assume that an exact matching be-\ntween word spaces does not exist, and that mul-\ntiple matchings may be equally valid. This is an\ninexact graph-matching problem. FAQ generally\nperforms poorly ﬁnding non-seeded inexact match-\nings (Saad-Eldin et al., 2021).\n2.2\nGOAT\nGraph Matching via OptimAl Transport (GOAT)\n(Saad-Eldin et al., 2021) is a new graph-matching\nmethod which uses advances in OT. Similar to\nSGM, GOAT amends FAQ and can use seeds.\nGOAT has been successful for the inexact graph-\nmatching problem on non-isomorphic graphs:\nwhereas FAQ rapidly fails on non-isomorphic\ngraphs, GOAT maintains strong performance.\nOptimal Transport\nOT is an optimization prob-\nlem concerned with the most efﬁcient way to trans-\nfer probability mass from distribution µ to distri-\nbution v. Discrete4 OT minimizes the inner prod-\nuct of a transportation “plan\" matrix P with a cost\nmatrix C, as in Equation 4. ⟨·, ·⟩is the Frobenius\ninner product.\nP ∗= arg min\nP∈U(r,c)\n⟨P, C⟩\n(4)\nP is an element of the “transportation polytope\"\nU(r, c)—the set of matrices whose rows sum to r\nand columns sum to c. The Hungarian Algorithm\napproximately solves OT, but the search space is\nrestricted to permutation matrices.\nSinkhorn: Lightspeed OT\nCuturi (2013) intro-\nduce Sinkhorn distance, an approximation of OT\ndistance that can be solved quickly and accurately\nby adding an entropy penalty h to Equation 4.\n4As ours is, as we compute over matrices.\nAdding h makes the objective easier and more efﬁ-\ncient to compute, and encourages “intermediary\"\nsolutions similar to that seen in the Intuition sub-\nsection.\nP λ = arg min\nP∈U(r,c)\n⟨P, C⟩−1\nλh(P)\n(5)\nAs λ →∞, P λ approaches the ideal transportation\nmatrix P ∗. Cuturi (2013) show that Equation 5 can\nbe computed using Sinkhorn’s algorithm (Sinkhorn,\n1967). The interested reader can see details of\nthe algorithm in Cuturi (2013); Peyré and Cuturi\n(2019). Unlike the Hungarian Algorithm, Sinkhorn\nhas no restriction to a permutation matrix solution\nand can be solved over any U(r, c).\nSinkhorn in GOAT\nGOAT uses Cuturi (2013)’s\nalgorithm to solve Equation 5 over U(1, 1), the set\nof doubly-stochastic matrices D. They call this\nthe “doubly stochastic OT problem\", and the algo-\nrithm that solves it “Lightspeed Optimal Transport\"\n(LOT). Although Sinkhorn distance was created for\nefﬁciency, Saad-Eldin et al. (2021) ﬁnd that using\nthe matrix P λ that minimizes Sinkhorn distance\nalso improves matching performance on large and\nnon-isometric graphs. Algorithm 2 is GOAT.\nAlgorithm 2 GOAT\nLet: Gx, Gy ∈Rn×n, P {0} ∈D (dbl-stoch.)\nwhile stopping criterion not met do\n1. Calculate ∇f(P {i}):\n∇f(P {i}) = GxP {i}GT\ny + GT\nx P {i}Gy\n2. Q{i} = dbl-stoch. approx. to ∇f(P {i}) via LOT.\n3. Calculate step size:\narg max\nα∈[0,1]\nf (αP {i} + (1 −α)Q{i})\n4. Update P {i+1} := αP {i} + (1 −α)Q{i}\nend while\nreturn permutation matrix approx. to P {n} via Hung. Alg.\nIntuition\nThe\ncritical\ndifference\nbetween\nSGM/FAQ and GOAT is how each calculates step\ndirection based on the gradient. Under the hood,\neach algorithm maximizes trace(QT ∇f(P {i}) to\ncompute Q{i} (the step direction) in Step 2 of their\nrespective algorithms. See Saad-Eldin et al. (2021)\nor Fishkind et al. (2019) for a derivation. FAQ uses\nthe Hungarian Algorithm and GOAT uses LOT.\nFor ∇f(P {i}) below, there are two valid permu-\ntation matrices Q1 and Q2 that maximize the trace.\nWhen multiple solutions exist, the Hungarian Algo-\nrithm chooses one arbitrarily. Thus, updates of P\nin FAQ are constrained to be permutation matrices.\n∇f(P {i}) =\n\n\n0\n3\n0\n2\n1\n2\n0\n0\n0\n\n\nQ1 =\n\n\n0\n1\n0\n0\n0\n1\n1\n0\n0\n\n,\nQ2 =\n\n\n0\n1\n0\n1\n0\n0\n0\n0\n1\n\n\ntrace(QT\n1 ∇f(P {i})) = trace(QT\n2 ∇f(P {i})) = 5\nConcerningly, Saad-Eldin et al. (2021) ﬁnd that\nseed order inﬂuences the solution in a popular im-\nplementation of the Hungarian Algorithm. Since\nBLI is a high-dimensional many-to-many task, ar-\nbitrary choices could meaningfully affect the result.\nGOAT, on the other hand, can step in the direc-\ntion of a doubly-stochastic matrix. Saad-Eldin et al.\n(2021) prove that given multiple permutation matri-\nces that equally approximate the gradient at P {i},\nany convex linear combination is a doubly stochas-\ntic matrix that equally approximates the gradient:\nPλ =\nn\nX\ni\nλiPi\ns.t. λ1+...+λn = 1; λi ∈[0, 1]\nPλ is a weighted combination of many valid\nsolutions—obviating the need to arbitrarily select\none for the gradient-based update. LOT’s output\nof a doubly-stochastic matrix in Step 2 is similar\nto ﬁnding a Pλ in that it needn’t discretize to a sin-\ngle permutation matrix. In this way, GOAT can\nbe thought of as taking a step that incorporates\nmany possible permutation solutions. For instance,\nGOAT may select Qds = 1\n2Q1 + 1\n2Q2, which also\nmaximizes trace(QT ∇f(P {i}).\nQds =\n\n\n0\n1\n0\n1\n2\n0\n1\n2\n1\n2\n0\n1\n2\n\n\ntrace(QT\nds∇f(P {i})) = 5\nThus\nwhereas\nFAQ\ntakes\nnon-deterministic\n“choppy\" update steps, GOAT optimizes smoothly\nand deterministically. Figure 1 is an illustration.\n3\nExperimental Setup\nWe run Procrustes, SGM, and GOAT on 40 lan-\nguage pairs. We also run system combination ex-\nperiments similar to Marchisio et al. (2021). We\nevaluate with the standard precision@1 (P@1).\nFigure 1: Optimization step of FAQ vs. GOAT. FAQ\narbitrarily chooses the direction of a permutation ma-\ntrix. GOAT averages permutation matrices to take a\nsmoother path.\nWe induce lexicons using (1) the closed-form\nsolution to the orthogonal Procrustes problem of\nEquation 1, extracting nearest neighbors using\nCSLS (Conneau et al., 2018), (2) SGM, solving the\nseeded version of Equation 2, and (3) GOAT. Word\ngraphs are Gx = XXT, Gy = YYT.\nSystem Combination\nWe perform system com-\nbination experiments analogous to those of Marchi-\nsio et al. (2021), incorporating GOAT. Figure 2\nshows the system, which is made of two compo-\nnents: GOAT run in forward and reverse directions,\nand “Iterative Procrustes with Stochastic-Add\"\nfrom Marchisio et al. (2021). This iterative version\nof Procrustes runs Procrustes in source→target and\ntarget→source directions and feeds H random hy-\npotheses from the intersection of both directions\ninto another run of Procrustes with the gold seeds.\nThe process repeats for I iterations, adding H more\nrandom hypotheses each time until all are chosen.\nWe set H = 100 and I = 5, as in the original work.\n3.1\nData & Software\nWe use publicly-available fastText word embed-\ndings (Bojanowski et al., 2017)5 which we normal-\nize, mean-center, and renormalize (Artetxe et al.,\n2018; Zhang et al., 2019) and bilingual dictionaries\nfrom MUSE6 ﬁltered to be one-to-one.7 For lan-\nguages with 200,000+ embeddings, we use the ﬁrst\n5https://fasttext.cc/docs/en/pretrained-vectors.html\n6https://github.com/facebookresearch/MUSE\n7For each source word, keep the ﬁrst unused target word.\nTargets are in arbitrary order, so this is random sampling.\nFigure 2: Combination system: Iterative Procrustes\n(IterProc) & GOAT. (1) run GOAT in forward/reverse\ndirections (2) intersect hypotheses, pass to IterProc, (3)\nrun IterProc forward & reverse (4) intersect hypothe-\nses, pass to Step (1). Repeat N cycles. (End) Get ﬁnal\ntranslations from forward run of GOAT or IterProc.\n200,000. Dictionary and embeddings space sizes\nare in Appendix Table A1. Each language pair has\n∼4100-4900 translation pairs post-ﬁltering. We\nchoose 0-4000 pairs in frequency order as seeds\nfor experiments, leaving the rest as the test set.8\nFor SGM and GOAT, we use the publicly-available\nimplementations from the GOAT repository9 with\ndefault hyperparameters (barycenter initialization).\nWe set reg=500 for GOAT. For system combination\nexperiments, we amend the code from Marchisio\net al. (2021)10 to incorporate GOAT.\n3.2\nLanguages\nThe languages chosen reﬂect various language fam-\nilies and writing systems. The language families\nrepresented are:\n• Austroasiatic: Vietnamese\n• Austronesian: Indonesian, Malay\n• Dravidian: Tamil\n• Japonic: Japanese\n• Sino-Tibetan: Chinese\n• Uralic: Estonian\n• Indo-European\nBalto-Slavic: Macedonian, Bosnian, Russian\nGermanic: English, German\nIndo-Iranian: Bengali, Persian\nRomance: French, Spanish, Portuguese, Italian\nThe chosen languages use varying writing systems,\nincluding those using or derived from Latin, Cyril-\nlic, Arabic, Tamil, and character-based scripts.\n8Ex. En-De with 100 seeds has 4803 test items. With 1000\nseeds, the test set contains 3903 items.\n9https://github.com/neurodata/goat.\nSome exps.\nused\nSGM from Graspologic (github.com/microsoft/graspologic;\nChung et al., 2019), but they are mathematically equal.\n10https://github.com/kellymarchisio/euc-v-graph-bli\n4\nResults\nResults of Procrustes vs. SGM vs. GOAT are in\nTable 1, visualized in Figure 3.\nProcrustes\nvs.\nSGM\nMarchisio\net\nal.\n(2021) conclude that SGM strongly outper-\nforms\nProcrustes\nfor\nEnglish→German\nand\nRussian→English with 100+ seeds. We ﬁnd that\nthe trend holds across language pairs, with the\neffect even stronger with less supervision. SGM\nperforms reasonably with only 50 seeds for nearly\nall languages, and with only 25 seeds in many.\nChinese↔English and Japanese↔English perform\nrelatively worse, and highly-related languages\nperform best: French, Spanish, Italian, and Por-\ntuguese. German↔English performance is low\nrelative to some less-related languages, which\nhave surprisingly strong performance from SGM:\nIndonesian↔English and Macedonian↔English\nscore P@1 ≈50-60, even with low supervision.\nExcept for the aforementioned highly-related lan-\nguage pairs, Procrustes does not perform above\n∼10 for any language pair with ≤100 seeds,\nwhereas SGM exceeds P@1 = 10 with only 25\nseeds for 33 of 40 pairs.\nSGM vs. GOAT\nGOAT improves considerably\nover SGM for nearly all language pairs, and the ef-\nfect is particularly strong with very low amounts\nof seeds and less-related languages. GOAT im-\nproves upon SGM by +19.0, +8.5, and +7.9 on\nEnglish→Bengali with 25, 50, and 75 seeds, re-\nspectively. As the major use case of low-resource\nBLI and MT is dissimilar languages with low su-\npervision, this is an encouraging result. It gener-\nally takes 200+ seeds for SGM to achieve similar\nscores to GOAT with just 25 seeds.\n4.1\nIsomorphism of Embedding Spaces\nEigenvector similarity (EVS; Søgaard et al.,\n2018) measures isomorphism of embedding spaces\nbased on the difference of Laplacian eigenvalues.\nGromov-Hausdorff distance (GH) measures dis-\ntance based on nearest neighbors after an optimal\northogonal transformation (Patra et al., 2019). EVS\nand GH are symmetric, and lower means more iso-\nmetric spaces. Refer to the original papers for math-\nematical descriptions. We compute the metrics over\nthe word embedding using scripts from Vuli´c et al.\n(2020)11 and show results in Table 2. We observe a\n11https://github.com/cambridgeltl/iso-study/scripts\nSeeds\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nEnglish-to-*\nBengali\nBosnian\nEstonian\nPersian\n25\n0.0\n12.9\n31.9\n(+19.0)\n0.1\n37.0\n48.1\n(+11.1)\n0.1\n37.5\n47.0\n(+9.5)\n0.2\n35.2\n42.7\n(+7.5)\n200\n4.4\n30.9\n35.4\n(+4.5)\n7.0\n44.5\n49.9\n(+5.4)\n9.0\n46.0\n49.6\n(+3.6)\n7.0\n40.0\n44.4\n(+4.4)\n1000\n30.6\n40.7\n41.3\n(+0.6)\n39.1\n54.4\n55.1\n(+0.7)\n45.5\n57.0\n57.3\n(+0.3)\n40.9\n48.2\n49.9\n(+1.7)\n2000\n36.9\n45.9\n45.5\n(-0.4)\n45.9\n58.1\n57.1\n(-1.0)\n53.1\n63.4\n64.0\n(+0.6)\n47.6\n54.1\n54.7\n(+0.6)\nIndonesian\nMacedonian\nMalay\nTamil\n25\n0.1\n49.2\n51.3\n(+2.1)\n0.1\n51.0\n55.8\n(+4.8)\n0.3\n24.9\n36.6\n(+11.7)\n0.1\n1.5\n1.8\n(+0.3)\n200\n13.0\n54.1\n55.9\n(+1.8)\n12.2\n53.3\n57.6\n(+4.3)\n11.5\n43.3\n46.0\n(+2.7)\n4.1\n26.7\n31.0\n(+4.3)\n1000\n58.0\n64.5\n63.6\n(-0.9)\n51.0\n60.3\n61.8\n(+1.5)\n48.9\n58.9\n58.3\n(-0.6)\n26.9\n36.2\n36.2\n(+0.0)\n2000\n65.0\n70.7\n69.5\n(-1.2)\n56.3\n63.9\n64.6\n(+0.7)\n55.3\n65.0\n63.0\n(-2.0)\n32.3\n40.6\n39.8\n(-0.8)\nVietnamese\nChinese\nJapanese\nRussian\n25\n0.2\n0.4\n0.4\n(+0.0)\n0.3\n8.7\n7.9\n(-0.8)\n0.0\n1.1\n1.0\n(-0.1)\n0.4\n49.6\n55.7\n(+6.1)\n200\n2.9\n34.8\n40.6\n(+5.8)\n12.0\n22.7\n17.3\n(-5.4)\n8.3\n20.7\n15.6\n(-5.1)\n16.5\n54.3\n57.3\n(+3.0)\n1000\n37.4\n53.7\n54.9\n(+1.2)\n44.4\n34.5\n29.5\n(-5.0)\n47.8\n35.6\n27.4\n(-8.2)\n58.3\n61.7\n61.5\n(-0.2)\n2000\n50.7\n60.7\n61.1\n(+0.4)\n49.3\n45.6\n41.7\n(-3.9)\n56.0\n45.1\n40.3\n(-4.8)\n65.8\n67.4\n67.5\n(+0.1)\nGerman\nFrench\nSpanish\nItalian\n25\n0.3\n44.8\n48.5\n(+3.7)\n0.4\n58.6\n62.4\n(+3.8)\n0.3\n59.5\n62.8\n(+3.3)\n0.4\n60.0\n63.0\n(+3.0)\n200\n16.1\n47.5\n50.2\n(+2.7)\n24.9\n60.9\n63.2\n(+2.3)\n27.2\n62.1\n63.9\n(+1.8)\n24.9\n63.2\n64.4\n(+1.2)\n1000\n57.2\n54.9\n54.5\n(-0.4)\n67.9\n68.3\n67.9\n(-0.4)\n69.6\n68.8\n69.0\n(+0.2)\n69.8\n70.5\n70.3\n(-0.2)\n2000\n63.1\n61.4\n61.8\n(+0.4)\n72.5\n72.9\n72.2\n(-0.7)\n74.1\n75.2\n74.9\n(-0.3)\n75.6\n75.9\n76.0\n(+0.1)\n*-to-English\nBengali\nBosnian\nEstonian\nPersian\n25\n0.2\n37.2\n44.4\n(+7.2)\n0.2\n44.7\n54.6\n(+9.9)\n0.3\n55.9\n63.2\n(+7.3)\n0.1\n37.1\n45.1\n(+8.0)\n200\n7.6\n43.0\n46.6\n(+3.6)\n6.8\n50.4\n56.2\n(+5.8)\n12.6\n60.7\n64.8\n(+4.1)\n9.9\n42.7\n46.8\n(+4.1)\n1000\n39.0\n51.5\n51.3\n(-0.2)\n44.7\n59.2\n61.6\n(+2.4)\n54.6\n67.7\n70.0\n(+2.3)\n45.9\n48.5\n49.6\n(+1.1)\n2000\n45.2\n55.2\n54.2\n(-1.0)\n54.5\n64.8\n65.9\n(+1.1)\n62.6\n72.8\n74.2\n(+1.4)\n50.3\n53.8\n53.6\n(-0.2)\nIndonesian\nMacedonian\nMalay\nTamil\n25\n0.1\n54.6\n58.0\n(+3.4)\n0.2\n60.1\n63.2\n(+3.1)\n0.2\n10.0\n38.6\n(+28.6)\n0.2\n35.2\n44.4\n(+9.2)\n200\n13.3\n55.7\n58.3\n(+2.6)\n16.5\n62.0\n64.5\n(+2.5)\n15.5\n56.2\n58.9\n(+2.7)\n7.4\n43.4\n45.8\n(+2.4)\n1000\n58.5\n63.2\n64.1\n(+0.9)\n58.5\n66.1\n67.9\n(+1.8)\n55.7\n61.6\n62.3\n(+0.7)\n37.1\n49.6\n50.3\n(+0.7)\n2000\n66.0\n70.1\n70.1\n(+0.0)\n62.6\n71.7\n71.6\n(-0.1)\n60.2\n67.4\n67.2\n(-0.2)\n45.2\n55.1\n53.5\n(-1.6)\nVietnamese\nChinese\nJapanese\nRussian\n25\n0.1\n1.0\n3.3\n(+2.3)\n0.1\n6.8\n9.3\n(+2.5)\n0.2\n8.0\n31.2\n(+23.2)\n0.3\n49.1\n54.4\n(+5.3)\n200\n1.7\n41.3\n48.6\n(+7.3)\n12.0\n19.8\n16.1\n(-3.7)\n19.1\n34.6\n34.2\n(-0.4)\n16.6\n52.5\n55.2\n(+2.7)\n1000\n29.1\n52.4\n57.2\n(+4.8)\n44.5\n33.2\n31.9\n(-1.3)\n47.7\n42.7\n38.9\n(-3.8)\n56.6\n58.1\n60.3\n(+2.2)\n2000\n56.3\n64.1\n66.3\n(+2.2)\n50.8\n41.5\n41.8\n(+0.3)\n54.0\n48.6\n47.6\n(-1.0)\n62.7\n67.1\n68.5\n(+1.4)\nGerman\nFrench\nSpanish\nItalian\n25\n0.2\n30.3\n34.1\n(+3.8)\n0.3\n62.5\n65.3\n(+2.8)\n0.4\n61.4\n64.4\n(+3.0)\n0.4\n63.6\n66.9\n(+3.3)\n200\n10.6\n45.0\n48.3\n(+3.3)\n26.8\n64.2\n66.4\n(+2.2)\n32.6\n65.0\n65.9\n(+0.9)\n28.2\n66.6\n68.7\n(+2.1)\n1000\n52.8\n53.1\n53.3\n(+0.2)\n71.4\n70.1\n70.4\n(+0.3)\n69.5\n69.8\n70.6\n(+0.8)\n71.6\n71.8\n73.0\n(+1.2)\n2000\n60.7\n60.5\n60.4\n(-0.1)\n76.2\n75.3\n74.8\n(-0.5)\n72.9\n74.5\n73.9\n(-0.6)\n76.3\n75.6\n75.8\n(+0.2)\n*-to-*\nGerman-Spanish\nItalian-French\nSpanish-Portuguese\nPortuguese-German\n25\n0.5\n67.4\n70.5\n(+3.1)\n1.1\n85.4\n87.3\n(+1.9)\n2.5\n94.3\n94.9\n(+0.6)\n0.5\n72.5\n76.2\n(+3.7)\n200\n26.0\n67.9\n71.0\n(+3.1)\n57.3\n86.7\n87.7\n(+1.0)\n74.1\n94.9\n95.2\n(+0.3)\n36.3\n74.8\n76.9\n(+2.1)\n1000\n71.1\n74.0\n75.4\n(+1.4)\n86.1\n89.1\n89.4\n(+0.3)\n93.3\n95.8\n96.0\n(+0.2)\n75.5\n78.9\n79.6\n(+0.7)\n2000\n76.1\n78.8\n79.1\n(+0.3)\n88.8\n89.6\n90.4\n(+0.8)\n94.5\n97\n97.1\n(+0.1)\n80\n81.9\n82.0\n(+0.1)\nSpanish-German\nFrench-Italian\nPortuguese-Spanish\nGerman-Portuguese\n25\n0.5\n62.1\n66.2\n(+4.1)\n0.6\n88.9\n90.2\n(+1.3)\n1.1\n89.7\n90.5\n(+0.8)\n0.3\n72.9\n76.2\n(+3.3)\n200\n27.7\n65.2\n67.2\n(+2.0)\n58.4\n90.4\n91.2\n(+0.8)\n70.0\n90.4\n90.7\n(+0.3)\n24.3\n73.9\n77.1\n(+3.2)\n1000\n68.8\n70.6\n70.6\n(+0.0)\n89.6\n92.1\n92.7\n(+0.6)\n89.5\n90.4\n91.1\n(+0.7)\n73.7\n79.8\n80.8\n(+1.0)\n2000\n73.6\n74.0\n74.0\n(+0.0)\n91.8\n93.9\n94.1\n(+0.2)\n90.9\n91.7\n92.3\n(+0.6)\n78.7\n80.6\n82.2\n(+1.6)\nAverages\nEnglish-to-*\n*-to-English\nNon-English\nOverall\n25\n0.2\n33.2\n38.6\n(+5.4)\n0.2\n38.6\n46.3\n(+7.7)\n0.9\n79.2\n81.5\n(+2.3)\n0.3\n37.1\n41.9\n(+4.8)\n200\n12.6\n44.1\n46.4\n(+2.3)\n14.8\n50.2\n52.8\n(+2.6)\n23.4\n40.3\n41.1\n(+0.8)\n16.9\n44.8\n46.8\n(+2.0)\n1000\n49.6\n54.3\n53.7\n(-0.6)\n52.3\n57.4\n58.3\n(+0.9)\n40.5\n41.9\n42.2\n(+0.3)\n47.5\n51.2\n51.4\n(+0.2)\n2000\n56.2\n60.4\n59.6\n(-0.8)\n59.8\n63.6\n63.7\n(+0.1)\n42.1\n43.0\n43.2\n(+0.2)\n52.7\n55.7\n55.5\n(-0.2)\nTable 1: P@1 of Procrustes (P), SGM (S) or GOAT (G). ∆is gain/loss of GOAT vs. SGM. Full results in Appendix.\nFigure 3 is a visualization of these results.\nFigure 3: Visualization of Table 1 for select languages. Procrustes (dashed) vs. SGM (dotted) vs. GOAT (solid).\nX-axis: # of seeds (log scale). Y-axis: Precision@1 (↑is better). GOAT is typically best.\nEVS\nGH\nEVS\nGH\nbn\n37.79\n0.49\nit\n22.42\n0.20\nbs\n35.93\n0.41\nja\n894.20\n0.55\nde\n11.49\n0.31\nmk\n151.02\n0.19\nes\n9.91\n0.21\nms\n153.42\n0.49\net\n35.22\n0.68\nru\n14.19\n0.46\nfa\n86.98\n0.39\nta\n56.66\n0.26\nfr\n27.92\n0.17\nvi\n256.28\n0.42\nid\n188.98\n0.39\nzh\n519.82\n0.61\nTable 2: Degree of isomorphism of embedding spaces\nin relation to English.\nEVS=Eigenvector Similarity.\nGH=Gromov-Hausdorff Distance. ↓:more isomorphic.\nmoderate correlation between EVS and GH (Spear-\nman’s ρ = 0.434, Pearson’s r = 0.44).\nFigure 4 shows the relationship between relative\nisomorphism of each language vs. English, and per-\nformance of Procrustes/GOAT at 200 seeds. Trends\nindicate that higher isomorphism varies with higher\nprecision from Procrustes and GOAT. GH shows\na moderate to strong negative Pearson’s correla-\ntion with performance from Procrustes and GOAT:\nr = −0.47 and r = −0.53, respectively, for *-\nto-en and -0.55 and -0.61 for en-to-*. EVS corre-\nlates weakly negatively with performance from Pro-\ncrustes (*-to-en: -0.06, en-to-*: -0.28) and strongly\nnegatively with GOAT (*-to-en: -0.67, en-to-*:\n−0.75). As higher GH/EVS indicates less isomor-\nphism, negative correlations imply that lower de-\nFigure 4:\nX-axis:\nEigenvector Similarity (EVS) /\nGromov-Hausdorff (GH) Distance of language com-\npared to English. Y-axis: Precision@1 from Procrustes\n& GOAT with 200 seeds. ↓EVS/GH = ↑isomorphic.\ngrees of isomorphism correlate with lower scores\nfrom Procrustes/GOAT.\n4.2\nSystem Combination\nSystem combination results are in Table 3. Sim-\nilar to Marchisio et al. (2021)’s ﬁndings for their\ncombined Procrustes/SGM system, we ﬁnd (1) our\ncombined Procrustes/GOAT system outperforms\nProcrustes and GOAT alone, (2) ending with the It-\nerative Procrustes is best for moderate amounts of\nseeds, (3) ending with GOAT is best for very low\nor very high number of seeds.\nWhether we end with Iterative Procrustes vs.\nGOAT is critically important for the lowest seed\nsizes: -EndGOAT (-EG) usually fails with 25 seeds;\nall language pairs except German↔English and\nRussian↔English score P@1 < 15.0, and most\nscore P@1 < 2.0. Simply switching the order of\nprocessing in the combination system, however,\nboosts performance dramatically: ex. from 0.6\nfor StartProc-EndGOAT to 61.5 for StartGOAT-\nEndProc for Bosnian→English with 25 seeds.\nThere are some language pairs such as\nEnglish→Persian and Russian↔English where a\nprevious experiment with no seeds had reasonable\nperformance, but the combined system failed. It is\nworth investigating where this discrepancy arises.\n5\nDiscussion\nWe have seen GOAT’s strength in low-resource sce-\nnarios and in non-isomorphic embedding spaces.\nAs the major use case of low-resource BLI and\nMT is dissimilar languages with low supervision,\nGOAT’s strong performance is an encouraging\nresult for real-world applications. Furthermore,\nGOAT outperforms SGM. As the graph-matching\nobjective is NP-hard so all algorithms are approx-\nimate, GOAT does a better job by making a bet-\nter calculation of step direction. Chinese↔English\nand Japanese↔English are outliers, which is wor-\nthy of future investigation.\nNotably, these lan-\nguages have very poor isomorphism scores in rela-\ntion to English.\nWhy might graph-based methods work?\nThe\ngoal for Procrustes is to ﬁnd the ideal linear trans-\nformation Wideal ∈Rdxd to map the spaces, where\nd is the word embedding dimension. Seeds in\nProcrustes solve Equation 1 to ﬁnd an approxima-\ntion W to Wideal. Accordingly, the seeds can be\nthought of as samples from which one deduces\nthe optimal linear transformation. This is a su-\npervised learning problem, so when there are few\nseeds/samples, it is difﬁcult to estimate Wideal. Fur-\nthermore, the entire space X is mapped by W to\na shared space with Y meaning that every point in\nX is subject to a potentially inaccurate mapping\nW: the mapping extrapolates to the entire space.\nAs graph-based methods, GOAT and SGM do not\nsuffer this issue and can induce non-linear relation-\nships. Graph methods can be thought of as a semi-\nsupervised learning problem: even words that don’t\nserve as seeds are incorporated in the matching pro-\ncess. The graph manifold provides additional infor-\nmation that can be exploited.\nSecondly, the dimension of the relationship be-\ntween words in GOAT/SGM is much lower than\nfor Procrustes. For the former, the relationship is\none-dimensional: distance. As words for the Pro-\ncrustes method are embedded in d-dimensional Eu-\nclidean space, their relationships have a magnitude\nand a direction: they are {d+1}-dimensional. It is\npossible that the lower dimension in GOAT/SGM\nmakes them robust to noise, explaining why the\ngraph-based methods outperform Procrustes in low-\nresource settings. This hypothesis should be inves-\ntigated in follow-up studies.\n6\nRelated Work\nBLI\nRecent years have seen a proliferation of the\nBLI literature (e.g. Ruder et al., 2018; Aldarmaki\net al., 2018; Joulin et al., 2018; Doval et al., 2018;\nArtetxe et al., 2019a; Huang et al., 2019; Patra\net al., 2019; Zhang et al., 2020; Biesialska and Ruiz\nCosta-Jussà, 2020). Many use Procrustes-based so-\nlutions, which assume that embedding spaces are\nroughly isomorphic. Wang et al. (2021) argue that\nthe mapping can only be piece-wise linear, and\ninduce multiple mappings. Ganesan et al. (2021)\nlearn an “invertible neural network\" as a non-linear\nmapping of spaces, and Cao and Zhao (2018)\nalign spaces using point set registration. Many\napproaches address only high-resource languages.\nThe tendency to evaluate on similar languages with\nhigh-quality data from similar domains hinders ad-\nvancement in the ﬁeld (Artetxe et al., 2020).\nBLI with OT\nMost similar to ours are BLI ap-\nproaches which incorporate OT formulations us-\ning the Sinkhorn and/or Hungarian algorithms (e.g.\nAlvarez-Melis and Jaakkola, 2018; Alaux et al.,\n2018). Grave et al. (2019) optimize “Procrustes in\nWasserstein Distance\", iteratively updating a lin-\near transformation and permutation matrix using\nFrank-Wolfe on samples from embedding spaces\nX and Y. Zhao et al. (2020b) and Zhang et al.\n(2017) also use an iterative procedure. Ramírez\net al. (2020) combine Procrustes and their Itera-\ntive Hungarian algorithms. Xu et al. (2018) use\nSinkhorn distance in the loss function, and (Zhang\net al., 2017) use Sinkhorn to minimize distance be-\ntween spaces. Haghighi et al. (2008) use the Hun-\nPrev\n-EP\n-EG\nPrev\n-EP\n-EG\nPrev\n-EP\n-EG\nPrev\n-EP\n-EG\nPrev\n-EP\n-EG\nPrev\n-EP\n-EG\nSeeds\nen-bn\nbn-en\nen-bs\nbs-en\nen-de\nde-en\n25\n31.9 44.3\n0.5\n44.4 53.1\n6.2\n48.1 58.4\n0.4\n54.6 61.5\n0.6\n48.5 61.7 59.1\n34.1 59.3 56.8\n75\n33.1 44.7 39.5\n45.2 53.5 49.1\n47.9 58.1 55.1\n54.6 61.7 57.2\n48.8 62.3 59.7\n47.0 59.4 57.1\n100\n33.8 45.3 39.7\n45.5 53.9 48.1\n47.7\n58.1 55.4\n55.5 61.3 57.5\n49.0 62.2 59.7\n47.6 59.4 56.8\n2000\n45.9 48.7 49.3\n55.2 56.6 56.3\n58.1 59.9 60.5\n65.9 66.6 69.1\n63.1 66.3 69.4\n60.7\n65.1 67.9\n4000\n60.3 50.5 61.2\n65.9 55.2 68.9\n70.6 63.4 71.8\n86.1 69.7 85.7\n74.2 72.9 79.5\n71.4 67.2 77.6\nen-et\net-en\nen-fa\nfa-en\nen-id\nid-en\n25\n47.0 60.4\n5.9\n63.2 70.2 15.0\n42.7\n54.4\n4.5\n45.1 55.1\n2.0\n51.3 65.8\n0.6\n58.0 66.7\n1.8\n75\n47.5 60.6 58.6\n64.2 69.8 66.5\n42.9 54.1 51.9\n45.8 55.3 52.0\n53.6 66.0 63.3\n57.0 66.7 64.2\n100\n47.3 61.1 59.0\n64.3 70.2 66.7\n43.0 54.3 52.3\n45.6 55.5 52.7\n54.3 66.2 63.2\n57.8 67.1 63.9\n2000\n64.0 66.6 67.4\n74.2 74.1 75.0\n54.7\n58.0 58.4\n53.8 57.5 56.9\n70.7\n72.1 74.2\n70.1 72.5 72.9\n4000\n77.4 71.7 80.2\n86.4 80.7 87.2\n65.9 62.4 67.4\n65.5 60.1 67.0\n84.3 76.8 86.2\n80.5 78.2 83.7\nen-mk\nmk-en\nen-ms\nms-en\nen-ru\nru-en\n25\n55.8 63.9\n8.0\n63.2 68.8\n0.6\n36.6 62.6\n0.9\n38.6 65.3\n0.2\n55.7\n67.7 66.1\n54.4 63.9 62.0\n75\n56.5 64.3 63.3\n63.6 68.8 67.9\n42.6 62.6 59.8\n56.8 65.6 62.8\n55.7\n68.1 67.0\n54.8 63.9 61.6\n100\n56.2 64.1 64.2\n64.4 69.4 67.5\n42.9 63.0 58.6\n57.7\n65.8 63.1\n55.9 67.9 66.4\n55.1 63.8 61.3\n2000\n64.6 66.8 67.9\n71.7\n71.0 73.1\n65.0 67.0 68.5\n67.4 69.4 69.7\n67.5 72.6 74.2\n68.5 69.3 72.5\n4000\n75.6 68.9 77.1\n88.8 74.1 91.1\n79.3 70.7 79.5\n77.4 70.0 79.1\n83.3 79.3 86.5\n89.3 77.4 89.3\nen-ta\nta-en\nen-vi\nvi-en\nen-zh\nzh-en\n25\n1.8\n2.2\n0.6\n44.4 51.4\n2.4\n0.4\n0.4\n0.2\n3.3\n5.3\n0.2\n8.7\n52.7\n1.7\n9.3 48.1\n0.8\n75\n30.5 40.4 35.7\n45.1 52.4 46.9\n22.9 55.0\n1.2\n45.2 59.6 54.4\n17.4 51.6 46.4\n14.1 51.1 48.0\n100\n30.6 40.2 36.8\n45.4 52.5 47.9\n30.2 55.6 36.2\n47.1 59.3 56.3\n18.5 51.6 47.3\n15.2 51.0 48.0\n2000\n40.6 42.7 44.2\n55.1 55.3 56.2\n61.1 67.3 65.8\n66.3 73.5 71.5\n49.3 58.0 57.7\n41.8 57.6 56.8\n4000\n49.1 44.0 51.7\n73.8 65.3 71.6\n77.9 73.0 80.5\n82.1 80.1 84.3\n67.5 66.4 75.1\n66.2 65.3 73.3\nTable 3: P@1 of Combination Exps. -EP starts with GOAT, ends with IterProc. -EG: IterProc, ends with GOAT.\nPrev is previous best of prior experiments. Some seed sizes omitted for brevity (see Appendix).\ngarian Algorithm for BLI from text. Lian et al. and\nAlaux et al. (2018) align all languages to a common\nspace for multilingual BLI. The latter use Sinkhorn\nto approximate a permutation matrix in their for-\nmulation. Zhao et al. (2020a) incorporate OT for\nsemi-supervised BLI.\n7\nConclusion\nWe perform bilingual lexicon induction from word\nembedding spaces of 40 language pairs, utiliz-\ning the newly-developed GOAT algorithm for\ngraph-matching. Performance is strong across all\npairs, especially on dissimilar languages with low-\nsupervision. As the major use case of low-resource\nBLI and MT is dissimilar languages with low su-\npervision, the strong performance of GOAT is an\nencouraging result for real-world applications.\nLimitations\nAlthough we evaluate GOAT on 40 language pairs,\nthis does not capture the full linguistic diversity of\nworld languages. Languages of Eurasia are over-\nrepresented, particularly the Indo-European fam-\nily. Each pair has at least one high-resource Indo-\nEuropean language which uses the Latin script. Fu-\nture work should examine GOAT’s performance\nwhen both languages are low-resource, and on an\neven broader diversity of languages from around\nthe world. Furthermore, graph-matching methods\nare also considerably slower than calculating the\nsolution to the orthogonal Procustes problem on\nGPU, potentially limiting the former’s usefulness\nwhen one must match large sets of words. Future\nwork might examine the speed/accuracy trade-off\nbetween methods as embedding space size scales.\nAcknowledgements\nThis material is based on research sponsored by the\nAir Force Research Laboratory and Defense Ad-\nvanced Research Projects Agency (DARPA) under\nagreement number FA8750-20-2-1001. The U.S.\nGovernment is authorized to reproduce and dis-\ntribute reprints for Governmental purposes notwith-\nstanding any copyright notation thereon. The views\nand conclusions contained herein are those of the\nauthors and should not be interpreted as necessarily\nrepresenting the ofﬁcial policies or endorsements,\neither expressed or implied, of the Air Force Re-\nsearch Laboratory and DARPA or the U.S. Govern-\nment. This material is based upon work supported\nby the United States Air Force under Contract No.\nFA8750-19-C-0098. Any opinions, ﬁndings, and\nconclusions or recommendations expressed in this\nmaterial are those of the author(s) and do not nec-\nessarily reﬂect the views of the United States Air\nForce and DARPA.\nReferences\nJean Alaux, Edouard Grave, Marco Cuturi, and Ar-\nmand Joulin. 2018. Unsupervised hyper-alignment\nfor multilingual word embeddings. In International\nConference on Learning Representations.\nHanan Aldarmaki, Mahesh Mohan, and Mona Diab.\n2018. Unsupervised word mapping using structural\nsimilarities in monolingual embeddings.\nTransac-\ntions of the Association for Computational Linguis-\ntics, 6:185–196.\nDavid Alvarez-Melis and Tommi Jaakkola. 2018.\nGromov-Wasserstein alignment of word embedding\nspaces. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1881–1890, Brussels, Belgium. Association\nfor Computational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.\nA robust self-learning method for fully unsupervised\ncross-lingual mappings of word embeddings. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 789–798, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2019a.\nBilingual lexicon induction through unsu-\npervised machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5002–5007, Florence,\nItaly. Association for Computational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2019b. An effective approach to unsupervised ma-\nchine translation.\nIn Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 194–203, Florence, Italy. Associ-\nation for Computational Linguistics.\nMikel Artetxe,\nSebastian Ruder,\nDani Yogatama,\nGorka Labaka, and Eneko Agirre. 2020. A call for\nmore rigor in unsupervised cross-lingual learning.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n7375–7388, Online. Association for Computational\nLinguistics.\nMagdalena Marta Biesialska and Marta Ruiz Costa-\nJussà. 2020.\nReﬁnement of unsupervised cross-\nlingual word embeddings. In ECAI 2020, 24th Eu-\nropean Conference on Artiﬁcial Intelligence:\n29\nAugust–8 September 2020, Santiago de Compostela,\nSpain: including 10th Conference on Prestigious Ap-\nplications of Artiﬁcial Intelligence (PAIS 2020): pro-\nceedings, pages 1–4. Ios Press.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nHailong Cao and Tiejun Zhao. 2018. Point set registra-\ntion for unsupervised bilingual lexicon induction. In\nIJCAI, pages 3991–3997.\nJaewon Chung, Benjamin D Pedigo, Eric W Bridge-\nford, Bijan K Varjavand, Hayden S Helm, and\nJoshua T Vogelstein. 2019. Graspy: Graph statistics\nin python. Journal of Machine Learning Research,\n20(158):1–7.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. In 6th Inter-\nnational Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Conference Track Proceedings. OpenRe-\nview.net.\nMarco Cuturi. 2013. Sinkhorn distances: Lightspeed\ncomputation of optimal transport. Advances in neu-\nral information processing systems, 26:2292–2300.\nPaula Czarnowska, Sebastian Ruder, Édouard Grave,\nRyan Cotterell, and Ann Copestake. 2019.\nDon’t\nforget the long tail!\na comprehensive analysis of\nmorphological generalization in bilingual lexicon in-\nduction. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n974–983.\nYerai Doval, Jose Camacho-Collados, Luis Espinosa-\nAnke, and Steven Schockaert. 2018.\nImproving\ncross-lingual word embeddings by meeting in the\nmiddle. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 294–304, Brussels, Belgium. Association for\nComputational Linguistics.\nDonniell E Fishkind, Sancar Adali, Heather G Patsolic,\nLingyao Meng, Digvijay Singh, Vince Lyzinski, and\nCarey E Priebe. 2019. Seeded graph matching. Pat-\ntern recognition, 87:203–215.\nMarguerite Frank, Philip Wolfe, et al. 1956. An algo-\nrithm for quadratic programming. Naval research\nlogistics quarterly, 3(1-2):95–110.\nAshwinkumar Ganesan, Francis Ferraro, and Tim\nOates. 2021. Learning a reversible embedding map-\nping using bi-directional manifold alignment.\nIn\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 3132–3139, On-\nline. Association for Computational Linguistics.\nGoran Glavaš, Robert Litschko, Sebastian Ruder, and\nIvan Vuli´c. 2019. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, com-\nparative analyses, and some misconceptions. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 710–721,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei\nWang, and Tie-Yan Liu. 2018. Frage: Frequency-\nagnostic word representation.\narXiv preprint\narXiv:1809.06858.\nJohn C Gower, Garmt B Dijksterhuis, et al. 2004. Pro-\ncrustes problems, volume 30.\nOxford University\nPress on Demand.\nEdouard Grave, Armand Joulin, and Quentin Berthet.\n2019. Unsupervised alignment of embeddings with\nwasserstein procrustes. In The 22nd International\nConference on Artiﬁcial Intelligence and Statistics,\npages 1880–1890. PMLR.\nAria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,\nand Dan Klein. 2008. Learning bilingual lexicons\nfrom monolingual corpora. In Proceedings of ACL-\n08: HLT, pages 771–779, Columbus, Ohio. Associa-\ntion for Computational Linguistics.\nJiaji Huang, Qiang Qiu, and Kenneth Church. 2019.\nHubless nearest neighbor search for bilingual lexi-\ncon induction. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4072–4080, Florence, Italy. Associa-\ntion for Computational Linguistics.\nRoy Jonker and Anton Volgenant. 1987.\nA shortest\naugmenting path algorithm for dense and sparse lin-\near assignment problems.\nComputing, 38(4):325–\n340.\nArmand Joulin, Piotr Bojanowski, Tomas Mikolov,\nHervé Jégou, and Edouard Grave. 2018.\nLoss in\ntranslation: Learning bilingual word mapping with\na retrieval criterion.\nIn Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2979–2984, Brussels, Bel-\ngium. Association for Computational Linguistics.\nHarold W Kuhn. 1955. The hungarian method for the\nassignment problem. Naval research logistics quar-\nterly, 2(1-2):83–97.\nXin Lian, Kshitij Jain, Jakub Truszkowski, Pascal\nPoupart, and Yaoliang Yu. Unsupervised multilin-\ngual alignment using wasserstein barycenter.\nKelly Marchisio, Kevin Duh, and Philipp Koehn. 2020.\nWhen does unsupervised machine translation work?\nIn Proceedings of the Fifth Conference on Machine\nTranslation, pages 571–583, Online. Association for\nComputational Linguistics.\nKelly Marchisio, Youngser Park, Ali Saad-Eldin, An-\nton Alyakin, Kevin Duh, Carey Priebe, and Philipp\nKoehn. 2021. An analysis of Euclidean vs. graph-\nbased framing for bilingual lexicon induction from\nword embedding spaces. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2021,\npages 738–749, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013.\nExploiting similarities among languages for ma-\nchine translation. arXiv preprint arXiv:1309.4168.\nNdapa Nakashole and Raphael Flauger. 2018. Charac-\nterizing departures from linearity in word translation.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n2: Short Papers), pages 221–227, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nAitor Ormazabal, Mikel Artetxe, Gorka Labaka, Aitor\nSoroa, and Eneko Agirre. 2019. Analyzing the lim-\nitations of cross-lingual word embedding mappings.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n4990–4995, Florence, Italy. Association for Compu-\ntational Linguistics.\nBarun Patra, Joel Ruben Antony Moniz, Sarthak Garg,\nMatthew R. Gormley, and Graham Neubig. 2019.\nBilingual lexicon induction with semi-supervision\nin non-isometric embedding spaces.\nIn Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 184–193, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nGabriel Peyré and Marco Cuturi. 2019. Computational\noptimal transport. Foundations and Trends in Ma-\nchine Learning, 11(5-6):355–607.\nGuillem Ramírez, Rumen Dangovski, Preslav Nakov,\nand Marin Soljaˇci´c. 2020. On a novel application\nof wasserstein-procrustes for unsupervised cross-\nlingual learning. arXiv preprint arXiv:2007.09456.\nSebastian Ruder, Ryan Cotterell, Yova Kementched-\njhieva, and Anders Søgaard. 2018. A discriminative\nlatent-variable model for bilingual lexicon induction.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n458–468, Brussels, Belgium. Association for Com-\nputational Linguistics.\nAli\nSaad-Eldin,\nBenjamin\nD\nPedigo,\nCarey\nE\nPriebe, and Joshua T Vogelstein. 2021.\nGraph\nmatching via optimal transport.\narXiv preprint\narXiv:2111.05366.\nPeter H Schönemann. 1966. A generalized solution of\nthe orthogonal procrustes problem. Psychometrika,\n31(1):1–10.\nHaoyue Shi, Luke Zettlemoyer, and Sida I. Wang. 2021.\nBilingual lexicon induction via unsupervised bitext\nconstruction and word alignment. In Proceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 813–826, Online.\nAssociation for Computational Linguistics.\nRichard Sinkhorn. 1967. Diagonal equivalence to ma-\ntrices with prescribed row and column sums. The\nAmerican Mathematical Monthly, 74(4):402–405.\nAnders Søgaard, Sebastian Ruder, and Ivan Vuli´c.\n2018. On the limitations of unsupervised bilingual\ndictionary induction. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 778–\n788, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nJoshua T Vogelstein, John M Conroy, Vince Lyzin-\nski, Louis J Podrazik, Steven G Kratzer, Eric T\nHarley, Donniell E Fishkind, R Jacob Vogelstein,\nand Carey E Priebe. 2015.\nFast approximate\nquadratic programming for graph matching. PLOS\none, 10(4):e0121002.\nIvan Vuli´c, Goran Glavaš, Roi Reichart, and Anna Ko-\nrhonen. 2019.\nDo we really need fully unsuper-\nvised cross-lingual embeddings? In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4407–4418, Hong Kong,\nChina. Association for Computational Linguistics.\nIvan Vuli´c, Sebastian Ruder, and Anders Søgaard.\n2020. Are all good word vector spaces isomorphic?\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3178–3192, Online. Association for Computa-\ntional Linguistics.\nHaozhou Wang, James Henderson, and Paola Merlo.\n2021.\nMulti-adversarial learning for cross-lingual\nword embeddings. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 463–472, Online. Asso-\nciation for Computational Linguistics.\nRuochen Xu, Yiming Yang, Naoki Otani, and Yuexin\nWu. 2018.\nUnsupervised cross-lingual transfer of\nword embedding spaces. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2465–2474, Brussels, Bel-\ngium. Association for Computational Linguistics.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017. Earth mover’s distance minimization for\nunsupervised bilingual lexicon induction.\nIn Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1934–\n1945, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nMozhi Zhang, Yoshinari Fujinuma, Michael J. Paul,\nand Jordan Boyd-Graber. 2020.\nWhy overﬁtting\nisn’t always bad: Retroﬁtting cross-lingual word\nembeddings to dictionaries. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 2214–2220, Online. As-\nsociation for Computational Linguistics.\nMozhi Zhang, Keyulu Xu, Ken-ichi Kawarabayashi,\nStefanie Jegelka, and Jordan Boyd-Graber. 2019.\nAre girls neko or sh¯ojo? cross-lingual alignment of\nnon-isomorphic embeddings with iterative normal-\nization.\nIn Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 3180–3189, Florence, Italy. Association\nfor Computational Linguistics.\nXu Zhao, Zihao Wang, Hao Wu, and Yong Zhang.\n2020a. Semi-supervised bilingual lexicon induction\nwith two-way interaction.\nIn Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2973–2984,\nOnline. Association for Computational Linguistics.\nXu Zhao, Zihao Wang, Yong Zhang, and Hao Wu.\n2020b. A relaxed matching procedure for unsuper-\nvised BLI. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3036–3041, Online. Association for Computa-\ntional Linguistics.\nAppendix\n*-to-en\nen-to-*\nFull\n1-1\nFull\n1-1\n# Embs\nbn\n7588\n4299\n8467\n4556\n145350\nbs\n6164\n4294\n8153\n4795\n166505\nde\n10866\n4451\n14677\n4903\n200000\nen\nn/a\nn/a\nn/a\nn/a\n200000\nes\n8667\n4445\n11977\n4866\n200000\net\n6509\n4352\n8261\n4738\n200000\nfa\n8510\n4582\n8869\n4595\n200000\nfr\n8270\n4548\n10872\n4827\n200000\nid\n9677\n4563\n9407\n4573\n200000\nit\n7364\n4478\n9657\n4815\n200000\nja\n6819\n4112\n7135\n4351\n200000\nmk\n7197\n4259\n10075\n4820\n176947\nms\n8140\n4650\n7394\n4454\n155629\nru\n7452\n4084\n10887\n4812\n200000\nta\n6850\n4225\n8091\n4744\n200000\nvi\n7251\n4775\n6353\n4507\n200000\nzh\n8891\n4450\n8728\n4381\n200000\nTable A1: Size of train/test sets before (Full) & after\nmaking one-to-one (1-1), with # of embeddings used.\nen-de\nru-en\nSeeds\nRand.\nBary.\nRand.\nBary.\n100\n45.7\n45.9\n49.6\n50.4\n200\n47.4\n47.5\n52.5\n52.5\n500\n52.3\n51.9\n55.4\n55.6\n1000\n54.6\n54.9\n58.3\n58.1\n2000\n61.5\n61.4\n67.1\n67.1\n4000\n74.2\n74.2\n89.3\n89.3\nTable A2: SGM with barycenter vs. randomized initial-\nization for languages used in Marchisio et al. (2021).\nThe difference is negligible.\nUnsupervised Performance\nFor some highly-\nrelated languages, GOAT performs well even with\nno seeds (unsupervised). GOAT scores 48.8 on\nEnglish→German, 34.5 on German→English,\n62.4\non\nEnglish→Spanish,\nand\n19.6\non\nSpanish→English with no supervision. Particu-\nlarly striking is the unsupervised performance on\nhighly-related languages: >87 on Italian↔French\nand >90 for Spanish↔Portuguese. We suspect that\nthat the word embedding spaces are highly isomor-\nphic for these language pairs, allowing GOAT (and\nsometimes SGM) to easily recover the translations.\nIterative\nResults of Iterative Procrustes (Iter-\nProc), Iterative SGM (IterSGM), and Iterative\nGOAT (IterGOAT) are in Table A5. We run the It-\nerative Procrustes and Iterative SGM procedures of\nMarchisio et al. (2021) with stochastic-add. Here,\nProcrustes [or SGM] is run in source↔target di-\nrections, hypotheses are intersected, and H random\nhypotheses are added to the gold seeds and fed into\nsubsequent runs of Procrustes [SGM]. The next it-\neration adds 2H hypotheses, repeating until all hy-\npotheses are chosen. We set H = 100 and create\nan analogous iterative algorithm for GOAT, which\nwe call Iterative GOAT.\nIterSGM/GOAT perform similarly across con-\nditions, with a few exceptions where either per-\nforms very strongly with no supervision: Iter-\nGOAT scores 49.2, 45.2, 34.4, 58.2, and 55.9\nfor En-De, En-Fa, De-En, Id-En, and Ru-En, re-\nspectively, and IterSGM scores 57.3 for En-Ru.\nOn Chinese↔English, IterGOAT underperforms\nIterSGM, similar to GOAT’s underperformance of\nSGM in the single run.\nSimilar to Marchisio et al. (2021), we ﬁnd that\nIterProc compensates for an initial poor ﬁrst run\nand outperforms IterSGM with a moderate amount\nof seeds (100+). Extending to the very lowest seeds\nsizes (0-75), however, IterSGM/IterGOAT are supe-\nrior. With 25 seeds, IterProc fails for all language\npairs except En↔De and En↔Ru, scoring P@1 <\n5. IterSGM and IterGOAT, however, perform rea-\nsonably well for most language pairs with 25 seeds,\nsuggesting that the graph-based framing is the bet-\nter approach for low-seed levels. At the highest su-\npervision level (2000+ seeds), IterSGM/IterGOAT\nagain tends to be superior.\nDifferences are minor btwn using GOAT or\nSGM in the iterative or system combination experi-\nments. This results suggests that mixing Procrustes\nand graph-based framings is helpful for BLI, re-\ngardless of which algorithm one picks. It is inter-\nesting to contemplate what other problems might\nbeneﬁt from examination from multiple mathemat-\nical framings in one solution, as each may have\ncomplementary beneﬁts.\nen-bn\nen-bs\nen-et\nen-fa\nen-id\nen-mk\nen-ms\nen-ta\nSeeds P\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\n0\n0.0 0.0 0.1 (+0.1) 0.0 0.0 0.0 (+0.0) 0.0 0.0 0.2 (+0.2) 0.0 0.2 0.5 (+0.3) 0.0 0.2 2.9 (+2.7) 0.1 0.0 0.1 (+0.1) 0.0 0.0 0.1 (+0.1)\n0.0 0.0 0.0 (+0.0)\n25\n0.0 12.9 31.9 (+19.0) 0.1 37.0 48.1 (+11.1) 0.1 37.5 47.0 (+9.5) 0.2 35.2 42.7 (+7.5) 0.1 49.2 51.3 (+2.1) 0.1 51.0 55.8 (+4.8) 0.3 24.9 36.6 (+11.7) 0.1 1.5 1.8 (+0.3)\n50\n0.2 24.7 33.2 (+8.5) 0.3 39.6 47.9 (+8.3) 0.7 39.1 47.4 (+8.3) 0.6 37.0 42.6 (+5.6) 0.4 46.8 52.9 (+6.1) 0.4 51.2 56.2 (+5.0) 0.5 29.3 38.9 (+9.6)\n0.2 21.5 30.5 (+9.0)\n75\n0.5 25.2 33.1 (+7.9) 0.6 40.4 47.9 (+7.5) 1.3 40.6 47.5 (+6.9) 0.9 37.1 42.9 (+5.8) 1.1 49.8 53.6 (+3.8) 1.0 51.0 56.5 (+5.5) 1.2 37.0 42.6 (+5.6)\n0.7 22.8 30.5 (+7.7)\n100\n0.7 27.8 33.8 (+6.0) 0.7 40.7 47.7 (+7.0) 1.7 41.2 47.3 (+6.1) 1.6 36.7 43.0 (+6.3) 2.3 51.1 54.3 (+3.2) 1.6 52.0 56.2 (+4.2) 1.9 39.1 42.9 (+3.8)\n1.0 23.6 30.6 (+7.0)\n200\n4.4 30.9 35.4 (+4.5) 7.0 44.5 49.9 (+5.4) 9.0 46.0 49.6 (+3.6) 7.0 40.0 44.4 (+4.4) 13.0 54.1 55.9 (+1.8) 12.2 53.3 57.6 (+4.3) 11.5 43.3 46.0 (+2.7)\n4.1 26.7 31.0 (+4.3)\n500\n20.3 37.9 39.7 (+1.8) 26.1 50.6 52.7 (+2.1) 31.1 53.2 54.4 (+1.2) 26.8 44.7 46.8 (+2.1) 43.8 60.2 59.2 (-1.0) 39.0 58.4 60.3 (+1.9) 37.1 52.6 51.5 (-1.1) 17.0 33.5 33.9 (+0.4)\n1000 30.6 40.7 41.3 (+0.6) 39.1 54.4 55.1 (+0.7) 45.5 57.0 57.3 (+0.3) 40.9 48.2 49.9 (+1.7) 58.0 64.5 63.6 (-0.9) 51.0 60.3 61.8 (+1.5) 48.9 58.9 58.3 (-0.6) 26.9 36.2 36.2 (+0.0)\n2000 36.9 45.9 45.5 (-0.4) 45.9 58.1 57.1 (-1.0) 53.1 63.4 64.0 (+0.6) 47.6 54.1 54.7 (+0.6) 65.0 70.7 69.5 (-1.2) 56.3 63.9 64.6 (+0.7) 55.3 65.0 63.0 (-2.0) 32.3 40.6 39.8 (-0.8)\n4000 38.3 60.3 59.0 (-1.3) 48.4 70.6 69.8 (-0.8) 59.3 77.1 77.4 (+0.3) 51.1 65.5 65.9 (+0.4) 71.4 84.1 84.3 (+0.2) 59.9 75.6 75.5 (-0.1) 58.6 79.3 79.1 (-0.2) 33.5 49.1 48.5 (-0.6)\nen-vi\nen-zh\nen-ru\nen-de\nen-fr\nen-es\nen-it\nen-ja\nSeeds P\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\n0\n0.1 0.1 0.0\n(-0.1)\n0.0 0.0 0.0 (+0.0) 0.0 0.0 0.0 (+0.0) 0.0 0.4 48.8 (+48.4) 0.0 0.5 0.4 (-0.1) 0.0 0.2 62.4 (+62.2) 0.0 0.1 1.3 (+1.2) 0.0 0.0 0.0 (+0.0)\n25\n0.2 0.4 0.4 (+0.0) 0.3 8.7 7.9\n(-0.8)\n0.4 49.6 55.7 (+6.1) 0.3 44.8 48.5 (+3.7) 0.4 58.6 62.4 (+3.8) 0.3 59.5 62.8 (+3.3) 0.4 60.0 63.0 (+3.0) 0.0 1.1 1.0 (-0.1)\n50\n0.1 1.6 4.0 (+2.4) 0.6 15.6 15.0 (-0.6)\n1.0 50.2 55.6 (+5.4) 0.8 44.6 48.7 (+4.1) 1.1 58.1 62.6 (+4.5) 1.1 58.3 62.4 (+4.1) 1.4 58.3 62.4 (+4.1) 0.4 9.1 9.3 (+0.2)\n75\n0.3 13.8 22.9 (+9.1) 1.6 17.4 14.8 (-2.6)\n1.9 50.5 55.7 (+5.2) 2.6 45.3 48.8 (+3.5) 3.0 59.2 62.5 (+3.3) 3.0 60.2 63.6 (+3.4) 3.4 61.7 63.7 (+2.0) 0.9 10.3 10.7 (+0.4)\n100\n0.5 18.3 30.2 (+11.9) 3.1 18.5 15.3 (-3.2)\n3.1 51.7 55.9 (+4.2) 3.6 45.9 49.0 (+3.1) 5.8 59.4 62.3 (+2.9) 5.0 60.5 63.3 (+2.8) 5.9 60.7 63.3 (+2.6) 1.3 14.4 13.0 (-1.4)\n200\n2.9 34.8 40.6 (+5.8) 12.0 22.7 17.3 (-5.4) 16.5 54.3 57.3 (+3.0) 16.1 47.5 50.2 (+2.7) 24.9 60.9 63.2 (+2.3) 27.2 62.1 63.9 (+1.8) 24.9 63.2 64.4 (+1.2) 8.3 20.7 15.6 (-5.1)\n500\n19.2 43.4 47.9 (+4.5) 33.9 30.3 22.5 (-7.8) 45.8 58.7 59.4 (+0.7) 44.9 51.9 52.4 (+0.5) 57.9 65.2 65.4 (+0.2) 59.4 65.7 66.1 (+0.4) 57.3 67.9 67.5 (-0.4) 33.3 28.3 20.5 (-7.8)\n1000 37.4 53.7 54.9 (+1.2) 44.4 34.5 29.5 (-5.0) 58.3 61.7 61.5 (-0.2) 57.2 54.9 54.5 (-0.4) 67.9 68.3 67.9 (-0.4) 69.6 68.8 69.0 (+0.2) 69.8 70.5 70.3 (-0.2) 47.8 35.6 27.4 (-8.2)\n2000 50.7 60.7 61.1 (+0.4) 49.3 45.6 41.7 (-3.9) 65.8 67.4 67.5 (+0.1) 63.1 61.4 61.8 (+0.4) 72.5 72.9 72.2 (-0.7) 74.1 75.2 74.9 (-0.3) 75.6 75.9 76.0 (+0.1) 56.0 45.1 40.3 (-4.8)\n4000 59.4 77.9 77.7 (-0.2) 58.5 67.5 67.5 (+0.0) 73.4 83.3 81.8 (-1.5) 70.8 74.2 74.1 (-0.1) 78.2 82.3 82.3 (+0.0) 78.6 83.1 82.7 (-0.4) 77.5 84.3 83.9 (-0.4) 61.5 68.7 68.9 (+0.2)\nbn-en\nbs-en\net-en\nfa-en\nid-en\nmk-en\nms-en\nta-en\nSeeds P\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\n0\n0.0 0.0 0.0 (+0.0) 0.0 0.0 0.0 (+0.0) 0.0 0.1 0.0 (-0.1) 0.0 0.0 0.1 (+0.1) 0.0 0.1 1.0 (+0.9) 0.0 0.0 0.0 (+0.0) 0.0 0.0 0.0 (+0.0)\n0.0 0.0 0.5 (+0.5)\n25\n0.2 37.2 44.4 (+7.2) 0.2 44.7 54.6 (+9.9) 0.3 55.9 63.2 (+7.3) 0.1 37.1 45.1 (+8.0) 0.1 54.6 58.0 (+3.4) 0.2 60.1 63.2 (+3.1) 0.2 10.0 38.6 (+28.6) 0.2 35.2 44.4 (+9.2)\n50\n0.5 39.2 45.0 (+5.8) 0.3 45.2 54.6 (+9.4) 0.8 56.0 63.3 (+7.3) 0.3 38.2 45.3 (+7.1) 0.5 52.1 57.1 (+5.0) 1.2 60.1 63.5 (+3.4) 0.6 51.1 57.2 (+6.1)\n0.6 37.7 45.1 (+7.4)\n75\n1.0 39.9 45.2 (+5.3) 1.1 47.7 54.6 (+6.9) 1.7 57.9 64.2 (+6.3) 0.8 39.7 45.8 (+6.1) 1.4 52.3 57.0 (+4.7) 1.8 60.5 63.6 (+3.1) 1.9 51.9 56.8 (+4.9)\n1.1 40.2 45.1 (+4.9)\n100\n1.4 40.3 45.5 (+5.2) 1.9 48.5 55.5 (+7.0) 3.2 58.3 64.3 (+6.0) 1.5 39.8 45.6 (+5.8) 2.9 53.2 57.8 (+4.6) 4.4 61.0 64.4 (+3.4) 2.9 53.7 57.7 (+4.0)\n2.1 39.9 45.4 (+5.5)\n200\n7.6 43.0 46.6 (+3.6) 6.8 50.4 56.2 (+5.8) 12.6 60.7 64.8 (+4.1) 9.9 42.7 46.8 (+4.1) 13.3 55.7 58.3 (+2.6) 16.5 62.0 64.5 (+2.5) 15.5 56.2 58.9 (+2.7)\n7.4 43.4 45.8 (+2.4)\n500\n26.7 48.1 48.7 (+0.6) 28.0 55.4 58.9 (+3.5) 40.0 64.4 67.1 (+2.7) 33.4 46.6 48.0 (+1.4) 47.3 60.1 61.1 (+1.0) 48.6 65.2 66.3 (+1.1) 43.7 58.6 60.7 (+2.1) 25.5 47.0 48.2 (+1.2)\n1000 39.0 51.5 51.3 (-0.2) 44.7 59.2 61.6 (+2.4) 54.6 67.7 70.0 (+2.3) 45.9 48.5 49.6 (+1.1) 58.5 63.2 64.1 (+0.9) 58.5 66.1 67.9 (+1.8) 55.7 61.6 62.3 (+0.7) 37.1 49.6 50.3 (+0.7)\n2000 45.2 55.2 54.2 (-1.0) 54.5 64.8 65.9 (+1.1) 62.6 72.8 74.2 (+1.4) 50.3 53.8 53.6 (-0.2) 66.0 70.1 70.1 (+0.0) 62.6 71.7 71.6 (-0.1) 60.2 67.4 67.2 (-0.2) 45.2 55.1 53.5 (-1.6)\n4000 44.8 65.9 64.9 (-1.0) 54.4 83.7 86.1 (+2.4) 67.0 86.4 86.1 (-0.3) 52.1 65.5 64.8 (-0.7) 73.2 80.5 80.5 (+0.0) 65.3 88.4 88.8 (+0.4) 63.1 77.4 77.1 (-0.3) 48.4 73.8 73.3 (-0.5)\nTable A3: Full Results (1 of 2): P@1 of Procrustes (P), SGM (S) or GOAT (G). ∆is gain/loss of GOAT over SGM.\nvi-en\nzh-en\nru-en\nde-en\nSeeds\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\n0\n0.0\n0.0\n0.1\n(+0.1)\n0.0\n0.0\n0.0\n(+0.0)\n0.0\n0.4\n2.8\n(+2.4)\n0.0\n0.6\n34.5\n(+33.9)\n25\n0.1\n1.0\n3.3\n(+2.3)\n0.1\n6.8\n9.3\n(+2.5)\n0.3\n49.1\n54.4\n(+5.3)\n0.2\n30.3\n34.1\n(+3.8)\n50\n0.2\n27.4\n41.7\n(+14.3)\n0.6\n13.1\n12.5\n(-0.6)\n1.0\n49.5\n54.8\n(+5.3)\n0.6\n42.5\n47.6\n(+5.1)\n75\n0.3\n29.8\n45.2\n(+15.4)\n1.3\n12.6\n14.1\n(+1.5)\n2.1\n49.9\n54.8\n(+4.9)\n1.5\n42.6\n47.0\n(+4.4)\n100\n0.5\n34.4\n47.1\n(+12.7)\n2.4\n15.2\n14.9\n(-0.3)\n4.1\n50.4\n55.1\n(+4.7)\n3.0\n41.9\n47.6\n(+5.7)\n200\n1.7\n41.3\n48.6\n(+7.3)\n12.0\n19.8\n16.1\n(-3.7)\n16.6\n52.5\n55.2\n(+2.7)\n10.6\n45.0\n48.3\n(+3.3)\n500\n8.5\n45.7\n51.7\n(+6.0)\n33.0\n27.9\n25.0\n(-2.9)\n45.3\n55.6\n57.3\n(+1.7)\n39.5\n49.1\n50.3\n(+1.2)\n1000\n29.1\n52.4\n57.2\n(+4.8)\n44.5\n33.2\n31.9\n(-1.3)\n56.6\n58.1\n60.3\n(+2.2)\n52.8\n53.1\n53.3\n(+0.2)\n2000\n56.3\n64.1\n66.3\n(+2.2)\n50.8\n41.5\n41.8\n(+0.3)\n62.7\n67.1\n68.5\n(+1.4)\n60.7\n60.5\n60.4\n(-0.1)\n4000\n70.3\n81.9\n82.1\n(+0.2)\n58.4\n66.2\n66.2\n(+0.0)\n67.9\n89.3\n89.3\n(+0.0)\n63.6\n71.4\n71.0\n(-0.4)\nfr-en\nes-en\nit-en\nja-en\nSeeds\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\n0\n0.0\n0.2\n1.1\n(+0.9)\n0.0\n0.6\n19.6\n(+19.0)\n0.0\n0.1\n0.4\n(+0.3)\n0.1\n0.0\n0.0\n(+0.0)\n25\n0.3\n62.5\n65.3\n(+2.8)\n0.4\n61.4\n64.4\n(+3.0)\n0.4\n63.6\n66.9\n(+3.3)\n0.2\n8.0\n31.2\n(+23.2)\n50\n1.4\n62.8\n65.5\n(+2.7)\n2.4\n61.9\n65.0\n(+3.1)\n1.3\n63.5\n67.0\n(+3.5)\n1.6\n22.0\n30.5\n(+8.5)\n75\n4.6\n62.5\n65.7\n(+3.2)\n4.7\n62.8\n64.9\n(+2.1)\n3.1\n64.2\n67.5\n(+3.3)\n3.0\n24.1\n31.1\n(+7.0)\n100\n6.8\n63.0\n65.9\n(+2.9)\n7.8\n63.5\n65.0\n(+1.5)\n7.2\n64.9\n67.9\n(+3.0)\n5.7\n27.9\n31.9\n(+4.0)\n200\n26.8\n64.2\n66.4\n(+2.2)\n32.6\n65.0\n65.9\n(+0.9)\n28.2\n66.6\n68.7\n(+2.1)\n19.1\n34.6\n34.2\n(-0.4)\n500\n62.4\n67.7\n68.5\n(+0.8)\n61.5\n67.1\n68.2\n(+1.1)\n61.9\n69.8\n70.4\n(+0.6)\n38.3\n39.9\n37.2\n(-2.7)\n1000\n71.4\n70.1\n70.4\n(+0.3)\n69.5\n69.8\n70.6\n(+0.8)\n71.6\n71.8\n73.0\n(+1.2)\n47.7\n42.7\n38.9\n(-3.8)\n2000\n76.2\n75.3\n74.8\n(-0.5)\n72.9\n74.5\n73.9\n(-0.6)\n76.3\n75.6\n75.8\n(+0.2)\n54.0\n48.6\n47.6\n(-1.0)\n4000\n82.7\n90.0\n89.6\n(-0.4)\n74.4\n86.3\n85.8\n(-0.5)\n81.8\n86.8\n87.2\n(+0.4)\n54.5\n72.3\n70.5\n(-1.8)\nde-es\nit-fr\nes-pt\npt-de\nSeeds\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\n0\n0.0\n0.1\n0.8\n(+0.7)\n0.0\n0.1\n87.6\n(+87.5)\n0.0\n94.5\n94.9\n(+0.4)\n0.0\n0.0\n0.0\n(+0.0)\n25\n0.5\n67.4\n70.5\n(+3.1)\n1.1\n85.4\n87.3\n(+1.9)\n2.5\n94.3\n94.9\n(+0.6)\n0.5\n72.5\n76.2\n(+3.7)\n50\n1.6\n67.6\n70.0\n(+2.4)\n4.4\n85.1\n87.3\n(+2.2)\n10.4\n94.4\n95.0\n(+0.6)\n1.6\n72.5\n75.8\n(+3.3)\n75\n4.2\n65.7\n69.5\n(+3.8)\n12.9\n85.3\n87.2\n(+1.9)\n23.7\n94.4\n94.9\n(+0.5)\n3.4\n72.8\n76.0\n(+3.2)\n100\n7.0\n65.8\n69.8\n(+4.0)\n21.6\n85.8\n87.4\n(+1.6)\n38.6\n94.8\n95.1\n(+0.3)\n6.4\n72.9\n76.4\n(+3.5)\n200\n26.0\n67.9\n71.0\n(+3.1)\n57.3\n86.7\n87.7\n(+1.0)\n74.1\n94.9\n95.2\n(+0.3)\n36.3\n74.8\n76.9\n(+2.1)\n500\n59.9\n72.6\n73.5\n(+0.9)\n81.5\n87.8\n88.4\n(+0.6)\n90.6\n95.4\n95.4\n(+0.0)\n68.0\n77.2\n77.7\n(+0.5)\n1000\n71.1\n74.0\n75.4\n(+1.4)\n86.1\n89.1\n89.4\n(+0.3)\n93.3\n95.8\n96.0\n(+0.2)\n75.5\n78.9\n79.6\n(+0.7)\n2000\n76.1\n78.8\n79.1\n(+0.3)\n88.8\n89.6\n90.4\n(+0.8)\n94.5\n97.0\n97.1\n(+0.1)\n80.0\n81.9\n82.0\n(+0.1)\n4000\n79.3\n88.6\n89.1\n(+0.5)\n90.0\n92.8\n92.9\n(+0.1)\n96.3\n98.9\n98.9\n(+0.0)\n82.8\n88.6\n87.9\n(-0.7)\nes-de\nfr-it\npt-es\nde-pt\nSeeds\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\nP\nS\nG\n∆\n0\n0.0\n0.0\n0.0\n(+0.0)\n0.0\n89.0\n90.3\n(+1.3)\n0.1\n1.1\n90.4\n(+89.3)\n0.0\n0.1\n0.2\n(+0.1)\n25\n0.5\n62.1\n66.2\n(+4.1)\n0.6\n88.9\n90.2\n(+1.3)\n1.1\n89.7\n90.5\n(+0.8)\n0.3\n72.9\n76.2\n(+3.3)\n50\n1.3\n62.4\n66.1\n(+3.7)\n3.6\n89.2\n90.6\n(+1.4)\n8.3\n89.7\n90.7\n(+1.0)\n1.4\n72.5\n76.3\n(+3.8)\n75\n4.2\n63.0\n66.2\n(+3.2)\n10.2\n89.6\n91.0\n(+1.4)\n15.7\n89.4\n90.7\n(+1.3)\n3.4\n71.6\n76.4\n(+4.8)\n100\n7.8\n63.6\n66.2\n(+2.6)\n20.9\n89.8\n91.0\n(+1.2)\n25.4\n89.7\n90.8\n(+1.1)\n4.8\n71.7\n76.3\n(+4.6)\n200\n27.7\n65.2\n67.2\n(+2.0)\n58.4\n90.4\n91.2\n(+0.8)\n70.0\n90.4\n90.7\n(+0.3)\n24.3\n73.9\n77.1\n(+3.2)\n500\n59.9\n67.5\n68.4\n(+0.9)\n83.9\n91.6\n91.8\n(+0.2)\n86.6\n90.2\n90.8\n(+0.6)\n62.4\n76.9\n78.6\n(+1.7)\n1000\n68.8\n70.6\n70.6\n(+0.0)\n89.6\n92.1\n92.7\n(+0.6)\n89.5\n90.4\n91.1\n(+0.7)\n73.7\n79.8\n80.8\n(+1.0)\n2000\n73.6\n74.0\n74.0\n(+0.0)\n91.8\n93.9\n94.1\n(+0.2)\n90.9\n91.7\n92.3\n(+0.6)\n78.7\n80.6\n82.2\n(+1.6)\n4000\n75.3\n83.8\n83.8\n(+0.0)\n91.8\n96.2\n96.2\n(+0.0)\n91.5\n93.9\n93.8\n(-0.1)\n83.4\n92.9\n93.3\n(+0.4)\nTable A4: Full Results (2 of 2): P@1 of Procrustes (P), SGM (S) or GOAT (G). ∆is gain/loss of GOAT over SGM.\nIP\nIS\nIG\nIP\nIS\nIG\nIP\nIS\nIG\nIP\nIS\nIG\nIP\nIS\nIG\nIP\nIS\nIG\nSeeds\nen-bn\nbn-en\nen-bs\nbs-en\nen-de\nde-en\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.1\n0.0\n0.0\n0.0\n0.0\n0.1\n0.1\n0.0\n0.3 49.2\n0.1\n0.3 34.4\n25\n0.6 37.3 35.6\n3.2 47.3 46.8\n0.2\n52.3 51.2\n0.1\n56.5\n56.1\n61.5 49.8 49.7\n58.8 47.6 47.2\n50\n1.6 37.8 35.8\n24.3 47.5 47.2\n48.3\n52.2 51.4\n4.2\n57.2\n56.2\n61.5 49.9 49.6\n59.0 47.8 47.9\n75\n38.8 37.3 36.7\n51.9 48.3 47.6\n54.7\n52.3 51.5\n58.4\n57.1\n56.4\n61.5 50.4 49.9\n59.0 48.5 47.9\n100\n44.4 37.7 36.5\n53.2 48.2 47.9\n54.6\n52.5 51.1\n59.0\n57.1\n56.8\n62.1 50.2 50.0\n59.4 48.4 48.1\n200\n43.3 38.9 37.2\n52.1 48.0 48.1\n55.1\n52.9 52.4\n59.3\n58.8\n57.7\n62.0 50.9 50.4\n59.7 49.3 48.8\n500\n43.9 40.3 39.5\n51.8 49.7 48.8\n53.7\n54.4 53.0\n59.8\n60.1\n59.7\n62.8 52.3 52.0\n60.4 50.2 50.1\n1000\n43.4 42.2 41.3\n50.8 51.1 51.4\n53.4\n55.9 55.0\n58.8\n61.4\n61.4\n63.5 54.5 54.2\n61.4 53.8 53.6\n2000\n42.8 45.7 45.2\n49.8 54.6 54.4\n52.3\n57.8 57.4\n59.8\n65.9 65.7\n65.2 61.8 61.2\n64.0 60.5 60.4\n4000\n40.3 60.3 58.8\n45.2 65.9 66.6\n51.9\n70.4 69.9\n56.8 83.7\n86.4\n71.7 74.1 74.4\n64.5 71.4 71.2\nSeeds\nen-et\net-en\nen-fa\nfa-en\nen-id\nid-en\n0\n0.0\n0.9\n0.4\n0.0\n0.0\n0.0\n0.0\n0.1 45.2\n0.0\n0.2\n0.1\n0.0\n0.0 15.5\n0.1\n0.4 58.2\n25\n4.2 51.8 51.3\n3.1 66.4 65.9\n1.0\n46.7 45.4\n1.3\n47.6 46.7\n0.8 55.9 54.8\n1.4 59.5 58.5\n50\n59.3 52.8 51.6\n66.3 66.2 66.0\n52.9\n46.5 45.7\n9.3\n47.7\n47.0\n64.7 56.6 55.4\n65.4 59.0 58.5\n75\n58.8 52.8 51.6\n66.6 66.2 66.2\n53.1\n46.8 46.0\n54.0\n47.7\n47.1\n64.9 57.0 55.9\n65.8 59.6 58.7\n100\n59.3 53.1 51.9\n66.3 66.6 65.9\n53.0\n47.3 46.2\n54.1\n47.8\n47.0\n65.2 57.0 55.7\n66.0 59.2 58.8\n200\n59.0 53.4 51.9\n66.3 66.5 66.7\n53.3\n47.2 46.3\n54.2\n47.9\n47.6\n64.9 57.3 56.8\n66.5 60.1 59.5\n500\n59.4 55.4 54.1\n67.1 68.5 68.0\n53.3\n48.4 48.2\n54.3\n48.3\n48.4\n66.4 60.2 59.1\n67.4 62.6 61.9\n1000\n60.1 58.3 57.7\n67.6 69.8 70.4\n52.9\n49.8 49.9\n54.2\n49.8\n49.4\n67.8 63.6 62.6\n67.6 64.2 64.0\n2000\n59.6 64.2 63.2\n67.3 74.1 74.2\n53.3\n54.8 54.6\n54.5\n54.5\n54.2\n68.7 69.6 69.2\n69.3 70.2 70.2\n4000\n61.7 77.0 77.0\n67.0 86.4 86.4\n52.1\n65.5 65.5\n53.4\n65.3\n64.1\n72.1 84.1 83.8\n72.8 80.5 80.6\nSeeds\nen-mk\nmk-en\nen-ms\nms-en\nen-ru\nru-en\n0\n0.0\n0.1\n0.0\n0.0\n0.0\n0.1\n0.0\n0.1\n0.2\n0.0\n0.0\n0.1\n0.0 57.3\n0.1\n0.0\n1.1 55.9\n25\n1.0 59.4 58.6\n1.0 65.2 64.8\n0.7\n46.1 43.8\n0.7\n59.4 58.6\n66.2 57.7 57.0\n63.0 55.9 55.5\n50\n62.1 59.2 58.6\n66.3 65.6 65.4\n58.3\n46.4 43.7\n64.3\n59.4\n58.7\n66.5 58.2 57.3\n63.4 56.0 56.0\n75\n61.7 59.1 59.1\n66.8 65.4 65.5\n59.9\n47.0 45.3\n64.0\n58.7\n58.5\n66.7 58.3 57.6\n62.7 55.9 56.1\n100\n61.8 59.3 59.3\n66.2 65.6 65.5\n60.9\n47.7 45.6\n64.1\n59.5\n58.7\n66.8 57.8 57.6\n62.7 56.3 55.6\n200\n62.3 60.0 59.2\n67.0 66.0 65.7\n60.6\n48.5 47.2\n64.5\n59.5\n58.9\n66.6 58.9 57.8\n63.1 56.4 56.5\n500\n62.2 60.5 60.6\n66.7 66.9 66.5\n60.6\n52.6 51.0\n65.0\n61.4\n61.1\n67.7 60.0 59.3\n63.7 58.0 58.0\n1000\n62.0 62.5 62.5\n66.6 68.0 67.9\n61.1\n58.5 57.4\n65.0\n62.5\n63.2\n68.3 62.0 61.8\n64.0 60.3 61.1\n2000\n61.8 65.0 64.8\n66.1 71.4 71.8\n60.4\n63.9 62.6\n64.9\n67.5\n67.5\n69.5 67.2 67.6\n65.7 68.2 68.5\n4000\n62.1 75.7 75.6\n64.5 88.4 88.0\n59.9\n79.7 79.5\n64.8\n77.8\n77.5\n74.5 81.9 82.4\n69.0 89.3 89.3\nSeeds\nen-ta\nta-en\nen-vi\nvi-en\nen-zh\nzh-en\n0\n0.0\n0.0\n0.0\n0.0\n0.2\n0.9\n0.0\n0.0\n0.0\n0.0\n0.0\n0.1\n0.0\n0.3\n0.0\n0.0\n0.1\n0.1\n25\n0.3\n5.8\n4.3\n0.9 45.8 46.1\n0.1\n0.3\n0.3\n0.2\n23.5\n44.1\n3.3 13.7\n7.3\n2.0 14.6\n7.9\n50\n5.5 33.6 32.5\n18.0 46.6 46.4\n0.3\n22.0 42.6\n1.0\n48.9\n46.3\n52.9 19.0 12.3\n52.5 17.2 11.6\n75\n38.3 33.8 32.4\n50.0 46.9 46.2\n44.6\n46.5 43.3\n8.4\n49.4 48.7\n53.5 19.7 13.7\n51.9 17.0 12.9\n100\n39.5 33.3 32.5\n50.5 47.2 46.8\n3.4\n47.8 43.8\n59.4\n49.6\n48.8\n53.2 20.5 13.1\n52.1 18.3 13.4\n200\n40.2 34.0 32.8\n50.8 47.0 47.1\n57.8\n49.1 45.9\n60.1\n50.9\n49.7\n53.4 22.4 15.0\n52.6 20.2 15.0\n500\n40.0 35.4 34.0\n50.0 49.0 48.1\n59.2\n52.8 50.6\n61.4\n54.5\n53.1\n53.3 29.3 22.1\n52.8 28.3 24.4\n1000\n38.2 37.0 36.0\n49.7 50.5 50.3\n59.7\n57.1 56.4\n64.3\n58.8\n58.9\n54.2 34.1 29.6\n54.3 34.4 30.8\n2000\n38.2 40.7 38.8\n49.3 54.3 53.8\n60.6\n61.5 62.1\n68.0\n65.8\n66.3\n56.0 44.5 42.4\n54.5 42.2 41.3\n4000\n34.1 48.8 49.6\n48.9 72.9 73.3\n62.5 77.7\n78.3\n73.2\n82.3\n82.3\n59.1 67.2 66.9\n58.4 66.4 65.8\nTable A5: P@1 of Iterative Procrustes (IP), Iterative SGM (IS), and Iterative GOAT (IG). Highest per row.\nIterSGM/IterGOAT are italicized when outperforming IterProc but are not highest in row.\nen-to-*\n*-to-en\nen-to-*\n*-to-en\nSGM\nGOAT\nSGM\nGOAT\nSGM\nGOAT\nSGM\nGOAT\nSeeds\nPrev -PP -PS -PP -PG\nPrev -PP -PS -PP -PG\nPrev -PP -PS -PP -PG\nPrev -PP -PS -PP -PG\nbn\nmk\n0\n0.1\n0.0\n0.1\n0.0\n0.1\n0.1\n0.0\n0.0\n0.1\n0.0\n0.1\n0.0\n0.1\n0.0\n0.0\n0.1\n0.1\n0.0\n0.0\n0.1\n25\n37.3 44.0\n0.5 44.3\n0.5\n47.3 52.8\n8.4 53.1\n6.2\n59.4 64.4\n1.2 63.9\n8.0\n65.2 68.7\n1.1 68.8\n0.6\n50\n37.8 43.1\n2.0 44.3\n3.1\n47.5 52.1 14.4 53.0 48.2\n62.1 64.0 63.6 63.9 62.8\n66.3 68.4 68.1 68.5 67.8\n75\n38.8 44.7 38.8 44.7 39.5\n51.9 53.2 49.2 53.5 49.1\n61.7 63.8 63.7 64.3 63.3\n66.8 69.2 67.3 68.8 67.9\n100\n44.4 44.3 40.4 45.3 39.7\n53.2 52.4 48.5 53.9 48.1\n61.8 64.4 65.1 64.1 64.2\n66.2 69.3 68.2 69.4 67.5\n200\n43.3 45.4 40.9 45.2 40.8\n52.1 53.8 50.4 54.0 50.6\n62.3 64.1 64.5 64.9 64.2\n67.0 69.5 67.8 69.7 68.0\n500\n43.9 46.8 43.9 46.8 43.2\n51.8 54.3 50.3 55.1 51.4\n62.2 65.3 65.7 65.3 65.5\n66.9 69.6 69.1 70.0 68.9\n1000\n43.4 47.1 45.4 47.1 45.3\n51.5 55.3 53.2 55.5 53.9\n62.5 65.8 66.5 65.5 66.5\n68.0 69.8 69.9 69.9 69.5\n2000\n45.9 48.9 49.3 48.7 49.3\n55.2 56.6 57.2 56.6 56.3\n65.0 67.0 68.2 66.8 67.9\n71.8 71.2 72.5 71.0 73.1\n4000\n60.3 52.0 61.2 50.5 61.2\n66.6 55.2 68.9 55.2 68.9\n75.7 69.0 77.2 68.9 77.1\n88.8 74.1 91.1 74.1 91.1\nbs\nms\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.1\n0.0\n0.0\n0.0\n0.0\n0.2\n0.0\n0.0\n0.0\n0.0\n0.1\n0.0\n0.0\n0.1\n0.0\n25\n52.3 57.4\n0.2 58.4\n0.4\n56.5 61.7\n0.8 61.5\n0.6\n46.1 62.1\n0.4 62.6\n0.9\n59.4 65.6\n0.5 65.3\n0.2\n50\n52.2 57.4 25.7 57.4 53.8\n57.2 61.2\n6.0 61.9 42.3\n58.3 61.8\n6.4 62.7 58.8\n64.3 65.6 63.0 65.6 63.1\n75\n54.7 57.5 54.6 58.1 55.1\n58.4 61.6 57.9 61.7 57.2\n59.9 62.4 59.0 62.6 59.8\n64.0 65.5 64.0 65.6 62.8\n100\n54.6 57.4 55.0 58.1 55.4\n59.0 61.8 59.0 61.3 57.5\n60.9 62.6 59.7 63.0 58.6\n64.1 66.0 63.9 65.8 63.1\n200\n55.1 57.8 56.9 58.6 56.4\n59.3 61.9 58.9 62.4 58.3\n60.6 63.1 60.6 63.1 59.6\n64.5 66.7 64.3 66.4 63.9\n500\n54.4 58.5 57.1 59.1 56.7\n60.1 63.1 61.1 63.8 61.0\n60.6 63.4 61.1 64.1 61.0\n65.0 67.4 65.9 66.8 65.8\n1000\n55.9 59.8 59.1 59.6 58.7\n61.6 63.6 63.4 64.7 62.9\n61.1 65.9 64.8 66.0 64.4\n65.0 67.3 67.0 67.7 66.9\n2000\n58.1 59.2 60.7 59.9 60.5\n65.9 66.9 68.8 66.6 69.1\n65.0 67.4 68.8 67.0 68.5\n67.5 69.1 69.8 69.4 69.7\n4000\n70.6 63.0 72.2 63.4 71.8\n86.4 69.7 84.7 69.7 85.7\n79.7 70.9 79.7 70.7 79.5\n77.8 70.3 79.2 70.0 79.1\nde\nru\n0\n49.2\n0.1\n0.1 61.8\n0.0\n34.5\n0.5\n0.2 59.2\n0.3\n57.3\n0.0\n0.0\n0.1\n0.1\n55.9\n0.0\n0.0\n4.4\n0.0\n25\n61.5 61.9 59.3 61.7 59.1\n58.8 59.5 56.7 59.3 56.8\n66.2 67.8 66.3 67.7 66.1\n63.0 64.4 62.6 63.9 62.0\n50\n61.5 62.1 59.9 62.1 59.3\n59.0 59.6 56.5 59.1 56.5\n66.5 67.6 66.0 67.5 66.5\n63.4 64.4 62.5 63.7 61.5\n75\n61.5 62.0 59.6 62.3 59.7\n59.0 59.5 56.6 59.4 57.1\n66.7 67.8 66.6 68.1 67.0\n62.7 64.2 62.6 63.9 61.6\n100\n62.1 62.3 60.1 62.2 59.7\n59.4 59.5 57.1 59.4 56.8\n66.8 68.3 67.2 67.9 66.4\n62.7 64.0 61.6 63.8 61.3\n200\n62.0 62.5 60.7 62.4 60.3\n59.7 60.0 58.1 60.0 57.6\n66.6 68.7 67.5 68.6 67.1\n63.1 64.5 62.2 64.4 61.9\n500\n62.8 63.8 61.9 63.8 61.8\n60.4 61.1 59.3 61.0 59.5\n67.7 69.0 68.8 68.9 68.2\n63.7 65.0 63.8 64.8 64.1\n1000\n63.5 64.1 63.0 64.1 63.1\n61.4 62.3 61.4 62.3 61.9\n68.3 70.3 69.9 70.4 69.8\n64.0 66.5 67.1 66.7 66.5\n2000\n65.2 66.6 69.6 66.3 69.4\n64.0 65.4 68.2 65.1 67.9\n69.5 72.2 74.0 72.6 74.2\n68.5 69.5 72.7 69.3 72.5\n4000\n74.4 73.2 79.7 72.9 79.5\n71.4 67.0 77.6 67.2 77.6\n83.3 78.3 86.5 79.3 86.5\n89.3 77.4 89.3 77.4 89.3\net\nta\n0\n0.9\n0.0\n0.0\n0.2\n0.0\n0.1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.1\n0.9\n0.1\n0.0\n0.6\n0.0\n25\n51.8 60.0\n2.6 60.4\n5.9\n66.4 69.4\n8.9 70.2 15.0\n5.8\n2.1\n0.5\n2.2\n0.6\n46.1 51.0\n2.0 51.4\n2.4\n50\n59.3 61.0 59.2 60.9 58.4\n66.3 69.4 66.2 70.2 66.6\n33.6 40.5\n2.1 40.2 10.5\n46.6 50.7 47.9 52.0 46.3\n75\n58.8 60.7 58.7 60.6 58.6\n66.6 68.8 66.4 69.8 66.5\n38.3 40.0 33.5 40.4 35.7\n50.0 52.0 47.6 52.4 46.9\n100\n59.3 61.0 58.3 61.1 59.0\n66.6 69.8 66.7 70.2 66.7\n39.5 40.5 35.2 40.2 36.8\n50.5 51.5 48.2 52.5 47.9\n200\n59.0 61.3 60.4 61.5 59.9\n66.7 70.2 67.7 70.5 67.3\n40.2 40.3 36.6 40.5 36.2\n50.8 52.2 48.1 52.9 49.3\n500\n59.4 62.9 61.3 63.2 60.3\n68.5 70.8 69.7 71.5 69.0\n40.0 41.5 38.7 40.9 39.1\n50.0 53.0 51.4 53.0 50.9\n1000\n60.1 64.3 63.9 64.3 63.4\n70.4 72.5 69.9 72.5 70.1\n38.2 41.3 41.7 41.6 40.7\n50.5 53.0 52.3 53.7 51.2\n2000\n64.2 66.0 67.7 66.6 67.4\n74.2 73.5 74.8 74.1 75.0\n40.7 43.5 43.7 42.7 44.2\n55.1 55.6 56.6 55.3 56.2\n4000\n77.4 72.5 80.2 71.7 80.2\n86.4 80.7 87.2 80.7 87.2\n49.6 43.7 52.4 44.0 51.7\n73.8 64.4 71.6 65.3 71.6\nfa\nvi\n0\n45.2\n0.1\n0.0\n0.5\n0.0\n0.2\n0.2\n0.0\n0.1\n0.0\n0.1\n0.0\n0.1\n0.0\n0.1\n0.1\n0.1\n0.4\n0.1\n0.9\n25\n46.7 54.3\n1.0 54.4\n4.5\n47.6 55.1\n1.8 55.1\n2.0\n0.4\n0.4\n0.1\n0.4\n0.2\n44.1\n1.3\n0.3\n5.3\n0.2\n50\n52.9 54.0 51.6 53.8 51.2\n47.7 55.2 51.6 54.9 52.3\n42.6\n2.4\n0.2\n5.2\n0.4\n48.9 58.8\n1.2 59.1\n2.1\n75\n53.1 54.5 52.0 54.1 51.9\n54.0 55.4 52.7 55.3 52.0\n46.5 54.5\n8.8 55.0\n1.2\n49.4 59.3 28.0 59.6 54.4\n100\n53.0 54.2 52.5 54.3 52.3\n54.1 55.0 52.5 55.5 52.7\n47.8 55.4 22.3 55.6 36.2\n59.4 59.1 56.6 59.3 56.3\n200\n53.3 54.3 52.3 54.8 52.2\n54.2 55.2 53.0 54.6 51.9\n57.8 58.0 55.4 57.8 56.0\n60.1 60.7 58.7 61.2 57.7\n500\n53.3 55.5 53.4 55.5 53.1\n54.3 55.7 53.4 56.1 52.7\n59.2 59.6 59.0 60.0 58.3\n61.4 63.4 61.9 63.5 60.8\n1000\n52.9 56.0 54.8 56.3 54.8\n54.2 55.7 54.2 55.6 54.1\n59.7 63.5 61.2 63.6 61.2\n64.3 67.1 65.8 68.1 65.3\n2000\n54.8 57.7 58.6 58.0 58.4\n54.5 56.9 57.7 57.5 56.9\n62.1 66.7 66.7 67.3 65.8\n68.0 73.2 71.8 73.5 71.5\n4000\n65.9 62.9 67.2 62.4 67.4\n65.5 61.0 67.7 60.1 67.0\n78.3 72.8 80.3 73.0 80.5\n82.3 81.0 84.6 80.1 84.3\nid\nzh\n0\n15.5\n0.6\n0.0\n4.4\n0.0\n58.2\n0.0\n0.0\n1.2\n0.0\n0.3\n0.2\n0.0\n0.1\n0.0\n0.1\n0.0\n0.0\n0.0\n0.0\n25\n55.9 65.9\n1.4 65.8\n0.6\n59.5 66.8\n0.4 66.7\n1.8\n13.7 52.2\n1.7 52.7\n1.7\n14.6 51.7\n0.8 48.1\n0.8\n50\n64.7 65.8 63.4 66.0 63.2\n65.4 66.6 64.4 67.0 64.0\n52.9 52.6 47.6 52.6 46.2\n52.5 51.7 48.0 50.5 47.3\n75\n64.9 66.1 64.0 66.0 63.3\n65.8 66.6 64.1 66.7 64.2\n53.5 52.3 48.3 51.6 46.4\n51.9 51.2 48.5 51.1 48.0\n100\n65.2 66.6 63.6 66.2 63.2\n66.0 67.0 64.4 67.1 63.9\n53.2 51.9 49.4 51.6 47.3\n52.1 51.1 48.1 51.0 48.0\n200\n64.9 66.7 64.8 66.8 65.0\n66.5 67.4 64.9 67.3 65.0\n53.4 52.7 49.6 52.6 49.2\n52.6 51.6 49.3 51.4 48.9\n500\n66.4 68.0 66.8 68.0 66.7\n67.4 68.9 67.1 69.1 67.2\n53.3 54.0 52.0 53.1 51.1\n52.8 53.0 50.9 52.6 50.4\n1000\n67.8 69.9 69.8 70.1 69.7\n67.6 70.0 68.9 70.0 68.3\n54.2 54.9 53.2 55.4 52.2\n54.3 54.7 53.0 54.3 52.0\n2000\n70.7 72.2 74.9 72.1 74.2\n70.2 72.2 73.0 72.5 72.9\n56.0 59.2 59.1 58.0 57.7\n54.5 57.6 57.8 57.6 56.8\n4000\n84.3 77.1 86.2 76.8 86.2\n80.6 78.0 84.0 78.2 83.7\n67.5 66.9 74.5 66.4 75.1\n66.4 65.8 72.9 65.3 73.3\nTable A6: Full Results: P@1 of Combination Experiments. SGM-PP starts with SGM, ends with Procrustes.\nSGM-PS: IterProc then SGM. GOAT-PP: start GOAT, end Proc. GOAT-PG: IterProc then GOAT. Previous best of\nall other experiments is in the Prev column. Prev here includes iterative results from Table A5.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2022-10-25",
  "updated": "2022-10-25"
}