{
  "id": "http://arxiv.org/abs/2210.14473v1",
  "title": "Benchmarking Language Models for Code Syntax Understanding",
  "authors": [
    "Da Shen",
    "Xinyun Chen",
    "Chenguang Wang",
    "Koushik Sen",
    "Dawn Song"
  ],
  "abstract": "Pre-trained language models have demonstrated impressive performance in both\nnatural language processing and program understanding, which represent the\ninput as a token sequence without explicitly modeling its structure. Some prior\nworks show that pre-trained language models can capture the syntactic rules of\nnatural languages without finetuning on syntax understanding tasks. However,\nthere is limited understanding of how well pre-trained models understand the\ncode structure so far. In this work, we perform the first thorough benchmarking\nof the state-of-the-art pre-trained models for identifying the syntactic\nstructures of programs. Specifically, we introduce CodeSyntax, a large-scale\ndataset of programs annotated with the syntactic relationships in their\ncorresponding abstract syntax trees. Our key observation is that existing\nlanguage models pretrained on code still lack the understanding of code syntax.\nIn fact, these pre-trained programming language models fail to match the\nperformance of simple baselines based on positional offsets and keywords. We\nalso present a natural language benchmark to highlight the differences between\nnatural languages and programming languages in terms of syntactic structure\nunderstanding. Our findings point out key limitations of existing pre-training\nmethods for programming languages, and suggest the importance of modeling code\nsyntactic structures.",
  "text": "Benchmarking Language Models for Code Syntax Understanding\nDa Shen1, Xinyun Chen2†, Chenguang Wang3†, Koushik Sen4, Dawn Song4\n1University of Maryland, College Park, 2Google Research, Brain Team\n3Washington University in St. Louis, 4University of California, Berkeley\ndashen@terpmail.umd.edu, xinyunchen@google.com, chenguangwang@wustl.edu,\n{ksen,dawnsong}@cs.berkeley.edu\nAbstract\nPre-trained language models have demon-\nstrated impressive performance in both natu-\nral language processing and program under-\nstanding, which represent the input as a to-\nken sequence without explicitly modeling its\nstructure.\nSome prior works show that pre-\ntrained language models can capture the syn-\ntactic rules of natural languages without ﬁne-\ntuning on syntax understanding tasks. How-\never, there is limited understanding of how\nwell pre-trained models understand the code\nstructure so far.\nIn this work, we perform\nthe ﬁrst thorough benchmarking of the state-\nof-the-art pre-trained models for identifying\nthe syntactic structures of programs. Speciﬁ-\ncally, we introduce CodeSyntax, a large-scale\ndataset of programs annotated with the syn-\ntactic relationships in their corresponding ab-\nstract syntax trees.\nOur key observation is\nthat existing language models pretrained on\ncode still lack the understanding of code syn-\ntax.\nIn fact, these pre-trained programming\nlanguage models fail to match the performance\nof simple baselines based on positional offsets\nand keywords. We also present a natural lan-\nguage benchmark to highlight the differences\nbetween natural languages and programming\nlanguages in terms of syntactic structure un-\nderstanding. Our ﬁndings point out key limita-\ntions of existing pre-training methods for pro-\ngramming languages, and suggest the impor-\ntance of modeling code syntactic structures.1\n1\nIntroduction\nLarge-scale pre-training of language models has\nbecome the de-facto paradigm for a variety of natu-\nral language processing tasks. Furthermore, recent\nstudies show that models pre-trained on a massive\namount of code also achieve competitive perfor-\nmance on many tasks, e.g., code generation and\n†Corresponding authors.\n1Our code and dataset are available at https://github.\ncom/dashends/CodeSyntax.\nThere were many pioneer PC contributors.\nresult = object.function(argument)\nroot\nexpl\nnn\nnn\namod\nnsubj\nAssign\nAttribute\nCall\n(a)\n(b)\nFigure 1: Examples of syntactic relations for (a) natural\nlanguages (NL) and (b) programming languages (PL).\nEach relation is represented by an arrow. The relations\nin PL represent the syntax of code in a way similar to\nthose in NL.\nOffset\nBERT\nRoBERTa CodeBERT\nEnglish Syntax Understanding\n47.1\n64.6\n62.7\n59.3\nOffset\nCuBERT CodeBERT RoBERTa\nPython Syntax Understanding\n43.6\n39.2\n33.1\n34.5\nFigure 2: A preview of the model performance com-\nparison on NL and PL syntax understanding tasks. Pre-\ntrained models capture NL syntax relatively well, but\nperform worse in understanding PL syntax. The Offset\nbaseline picks the token using a ﬁxed positional offset.\nWe use BERT-large and RoBERTa-base conﬁgurations\n(corresponding to the conﬁgurations of CuBERT and\nCodeBERT). The plot shows top-1 scores. See Tables 3\nand 4 for the full results.\ncode classiﬁcation. These tasks are closely related\nto natural language (NL) tasks in their problem\nformulation. Nowadays, the common practice for\nsolving these coding tasks is to utilize the language\nmodel architectures and training schemes that are\noriginally designed for NL. The design principle of\nthese neural language models is signiﬁcantly differ-\nent from the classic rule-based program generation\narXiv:2210.14473v1  [cs.CL]  26 Oct 2022\nsystems. Speciﬁcally, neural language models take\nthe program as a token sequence, while classic\nprogram generation systems utilize the language\ngrammar and code structure. Despite the advanced\nperformance of pre-trained language models on\ncode understanding tasks, what these models have\nlearned from the code corpus remains unclear.\nIn this work, we investigate whether large-scale\npre-training is all we need for code representation\nlearning. In particular, we conduct the ﬁrst system-\natic study to analyze how the pre-trained language\nmodels understand the syntactic structures of pro-\ngrams. To this end, we introduce CodeSyntax, a\nlarge-scale benchmark consisting of programs an-\nnotated with the syntactic relationships between\ndifferent tokens. The ground truth syntactic rela-\ntionships are extracted from edges in the abstract\nsyntax trees (AST) of the programs.\nFigure 1\nshows some examples. These syntactic relations\nare function-wise similar to dependency relations\nfor NL, where prior work has demonstrated that\nthe attention heads of pre-trained language models\ncan help to identify NL relation types (Clark et al.,\n2019; Raganato et al., 2018). To measure how well\nthe pre-trained language models capture the code\nsyntactic structures, we adopt the approach to the\nPL domain. We focus on investigating the zero-\nshot capability of existing pre-training methods in\nour experiments, and we evaluate these pre-trained\nmodels without ﬁnetuning them on our benchmark.\nWe evaluate the state-of-the-art pre-trained lan-\nguage models for code representation learning, in-\ncluding CuBERT (Kanade et al., 2020) and Code-\nBERT (Feng et al., 2020). A common character-\nistic of these models is that they share the same\nTransformer-based architectural design as NL mod-\nels (Vaswani et al., 2017; Devlin et al., 2019). This\nallows us to directly compare their performance\nin capturing the syntax structure. We present a\npreview of our key results in Figure 2. Our main\nobservation is that pre-training is insufﬁcient for\nlearning the syntactic relations in code. First, we\nﬁnd that the models pre-trained on code do not al-\nways outperform models pre-trained on NL corpus\nalone. Surprisingly, compared to CodeBERT which\nis trained on both text and code corpora, RoBERTa\nachieves better performance without training on\nany code with identical model architecture. This\nindicates that pre-training on programs as token\nsequences does not help learn the syntactic rela-\ntions. On the contrary, without dependency rela-\ntions, pre-training still enables language models to\nunderstand the NL syntax to some extent.\nMoreover, for code syntax understanding, the\npre-trained models even perform worse than simple\nbaselines that pick the tokens with a ﬁxed offset.\nFor example, always selecting the (p+2)-th token as\nthe p-th token’s dependency yields higher accuracy\nthan any attention head for several relation types.\nOn the other hand, the same model architectures\npre-trained on text corpora achieve decent accuracy\nin identifying the dependency relations in the NL\ndomain, where the performance of the same simple\nbaselines is far behind.\nOur analysis reveals several key differences be-\ntween NL and PL that lead to different capabilities\nof understanding the syntax for pre-trained mod-\nels. First, programs are more structured than NL\nsentences. Programs usually contain hierarchical\nstructures representing long-term dependencies be-\ntween code tokens. Consequently, a large num-\nber of syntactic relation types are between distant\ntokens, which can be difﬁcult to recognize for at-\ntention heads. On the contrary, the dependency\nrelations in NL sentences mostly connect nearby\ntoken pairs, and in this case the attention heads are\nmore capable of identifying the correct relations.\nMeanwhile, language models are good at recog-\nnizing keyword-based relations, such as picking\nthe corresponding else keyword for an if token.\nInterestingly, we ﬁnd that the inclusion of tokens\nsuch as newlines and semicolons notably affects\nthe performance in the code domain.\nOur ﬁndings suggest that existing pre-trained\nmodels perform quite differently in PL and NL do-\nmains in terms of the ability to understand syntax.\nThus, directly applying training paradigms devel-\noped for NL could be suboptimal for program learn-\ning, and we consider designing better approaches\nto model the code structure as future work.\n2\nCodeSyntax: Benchmarking Code\nSyntax Understanding\nWe construct the CodeSyntax benchmark to eval-\nuate the performance of language models on code\nsyntax understanding. We focus on Python and\nJava languages, on which the publicly released\nmodel checkpoints of both CuBERT (Kanade et al.,\n2020) and CodeBERT (Feng et al., 2020) are pre-\ntrained. We obtain the code samples from Code-\nSearchNet (Husain et al., 2019), which is a large-\nscale dataset consisting of code in different pro-\nRelation\nCount\nExplanation\nCode Example\nhead→dependent\nPython\nJava\nPython\nJava\nAssign:\ntarget→value\n78,482\n13,384\nAssigning a value to a target\nvariable.\ntarget = 10\nint target = 10;\nCall:\nfunc→args\n110,949\n50,890\nCalling a function with some\narguments.\nfunction(arg)\nfunction(arg);\nFor:\nfor→body\n8,704\n1,864\nA for loop repeatedly executes\nthe body block for some itera-\ntions.\nfor target in iter:\nbody\nfor (initializers;\ntest; updaters) {\nbody;\n}\nIf:\nif→else\n11,024\n5,038\nAn if statement conditionally\nexecutes a body based upon\nsome criteria. The dependent\nis the else keyword.\nif condition:\nbody1\nelse:\nbody2\nif (condition) {\nbody1;\n} else {\nbody2;\n}\nIf:\nif→body\n34,250\n22,392\nAn if statement. The depen-\ndent is the body block.\nif condition:\nbody1\nelse:\nbody2\nif (condition) {\nbody1;\n} else {\nbody2;\n}\nIf:\nbody→orelse\n11,024\n4,976\nAn if statement. The head is\nthe body block and the depen-\ndent is the body of the else\nblock.\nif condition:\nbody1\nelse:\nbody2\nif (condition) {\nbody1;\n} else {\nbody2;\n}\nWhile:\ntest→body\n743\n975\nThe while loop repeatedly exe-\ncutes the body block as long as\nthe speciﬁed condition is true.\nwhile condition:\nbody\nwhile (condition) {\nbody;\n}\nTable 1: Dataset statistics of selected relation types in CodeSyntax. For each relation type, we highlight the head\nand dependent nodes in the examples in bold, with the head in blue and the dependent in red. We defer the full\nstatistics of all relation types to Table 8 in the appendix.\ngramming languages. Its training set is also part\nof the pre-training data of CodeBERT, so we re-\nmove the data samples that are included in the\npre-training data of either CuBERT or CodeBERT.\nThus, none of the programs in CodeSyntax has\nbeen seen by CuBERT or CodeBERT in the pre-\ntraining phase.\nIn total, CodeSyntax contains 18,701 code sam-\nples annotated with 1,342,050 relation edges in\n43 relation types for Python, and 13,711 code\nsamples annotated with 864,411 relation edges\nin 39 relation types for Java.\nEach code sam-\nple is an entire function consisting of multiple\nstatements, which is analogous to a paragraph\nin NL. Each relation corresponds to an edge in\nthe program AST; speciﬁcally, we utilize the\nPython ast module (Foundation, 2021) and the Java\norg.eclipse.jdt.core.dom.ASTParser class (Contrib-\nutors, 2014) to parse a code sample into an AST.\nWe present some examples of relation types in Ta-\nble 1, and we defer the description of all relation\ntypes to Table 8 in the appendix. More details about\nrelation extraction are discussed in Appendix A.\nNote that we can easily extend the dataset to cover\nmore languages since the workﬂow for extracting\nrelations is automated and AST parsers are avail-\nable for most popular programming languages.\nWe observe several characteristics of relations\nin CodeSyntax. First, the keywords in PL play an\nimportant role in recognizing the code structure.\nSpeciﬁcally, some relation types have ﬁxed key-\nwords as the edge nodes, such as the If:if→else\nrelation. Meanwhile, compared to the dependency\nrelations in NL, the relation edges in the program\nAST tend to connect nodes that are much farther\naway from each other. As shown in Figure 3, the\naverage offset between head and dependent nodes\nis no more than 10 for dependency relations in\nNL, while the average offset for a relation type\ncan be more than 100 code tokens. Speciﬁcally, in\nCodeSyntax, there are 22 near dependency types\nwhose average offsets are less than 10, and 12 far\n0\n20\n40\n60\n80\n100 120 140 160 180\noffset\n0\n5\n10\n15\n20\nCount\npython\njava\n(a) CodeSyntax.\n10\n8\n6\n4\n2\n0\n2\n4\n6\n8\n10\noffset\n0\n2\n4\n6\n8\n10\nCount\nEnglish\nGerman\n(b) Natural language corpus.\nFigure 3: Offset distribution of relation types in (a)\nCodeSyntax and (b) NL corpus. The x axis is the av-\nerage positional offset distance between heads and de-\npendents for each relation. The y axis is the number\nof relations that has the average offset value. See Sec-\ntion 3 for more details on the NL corpus.\ndependency types whose average offsets are above\n10.\n3\nEvaluation Setup\nDo pre-trained language models capture the code\nstructure without direct supervision of the syntac-\ntic information? To investigate this question, we\nevaluate several pre-trained language models with-\nout ﬁnetuning, and compare their performance in\nunderstanding the syntax for NL and PL.\nNatural language benchmark.\nTo compare the\nperformance on CodeSyntax to NL syntax under-\nstanding, we construct the NL benchmark that\nincludes English and German. Speciﬁcally, we\nuse the English News Text Treebank: Penn Tree-\nbank Revised (Bies et al., 2015) labeled with Stan-\nford Dependencies (de Marneffe and Manning,\n2008a,b), and German Hamburg Dependency Tree-\nbank (Foth et al., 2014) labeled with Universal De-\npendencies (de Marneffe et al., 2021). In total, the\nEnglish dataset has 48,883 sentences, 43 relation\ntypes, and 1,147,526 relation edges; the German\ndataset has 18,459 sentences, 35 relation types, and\n307,791 relation edges.\nAttention\nprobing\napproach.\nSome\nprior\nworks demonstrate that a Transformer archi-\ntecture (Vaswani et al., 2017) pre-trained on a\ntext corpus, such as BERT (Devlin et al., 2019),\ncontains attention heads that specialize in certain\ndependency relations in NL (Raganato et al., 2018;\nClark et al., 2019). Speciﬁcally, in the Transformer\narchitecture, each vector ei for an input token\nis transformed into the query and key vectors qi\nand ki via some linear transformations, and the\ntransformations vary among different attention\nheads. For the i-th token, the attention weight\nassigned to the j-th token is\nαi,j =\nexp(qT\ni kj)\nP\nl exp(qT\ni kl)\nThe attention weight indicates how important\nthe j-th token is with respect to the i-th token.\nTypically, different attention heads learn differ-\nent weights between input tokens. Therefore, to\nmeasure the correctness of recognizing a relation\ntype r, for each edge <h, t, r> in the program\nAST where h is the head node and t is the de-\npendent node, we enumerate all attention heads to\ncompute the attention weight αh,t. If an attention\nhead tends to assign high attention weights that\nconnect the pair of tokens belonging to the relation\ntype r, we consider the relation type to be captured.\nWe defer more implementation details of attention\nmap extraction to Appendix B.\nMetrics.\nWe use the unlabeled attachment score\n(UAS) to measure the syntax understanding perfor-\nmance, and we consider top-k scores with different\nvalues of k. To compute top-k scores for language\nmodels, for each attention head, given the head to-\nken h in a relation edge <h, t, r>, we compute\nthe attention weight over all tokens in the input\ncode, and we consider the prediction to be correct\nif the attention weight over the dependent token\nt is among the top-k tokens with the highest at-\ntention weights. For each relation, we select the\nbest-performing attention head and use its score as\nthe model’s score for that relation. We calculate a\nmodel’s average score over all relations as the ﬁnal\nscore of the model.\nIn NL dependency parsing problems, the depen-\ndent node t usually corresponds to a single word.\nHowever, in PL, the dependent can be a block that\ncontains multiple code tokens. For example, in the\nIf:if→body relation, the head is the keyword if,\nwhile the dependent is the entire body block. There-\nfore, we measure three metrics. First-token metric\nand last-token metric: the prediction is deemed\ncorrect if it successfully predicts the ﬁrst and last\ntoken of the dependent block, respectively; Any-\ntoken metric: the prediction is considered correct\nif it can predict any token within the dependent\nblock. While we agree that these are not perfect\nmetrics and one single metric may be incomplete,\nwe observe that our ﬁndings generally hold for all\nthe three metrics we evaluated. Note that the ﬁrst-\ntoken metric is stricter than the any-token metric by\ndesign. Unless otherwise speciﬁed, we report the\ntop-k scores using the ﬁrst-token metric by default.\nModel architectures.\nTable 2 summarizes the\nmodels evaluated in this work.\nFor language\nmodels over code, we consider CuBERT (Kanade\net al., 2020) and CodeBERT (Feng et al., 2020),\nand we evaluate their released pre-trained check-\npoints. Both of them are based on architectures\ninitially designed for NL. Speciﬁcally, CuBERT\nutilizes the BERT (Devlin et al., 2019) architec-\nture, and CodeBERT (Feng et al., 2020) utilizes\nthe RoBERTa (Liu et al., 2019) architecture. For\nNL models, we also evaluate multilingual variants\nof BERT and RoBERTa on the German dataset,\ni.e., Multilingual BERT (Pires et al., 2019) and\nXLM-RoBERTa (Conneau et al., 2020). Both of\nthe two code language models are cased, so we also\nevaluate the cased versions of the NL models.\nProgramming Languages\nNatural Languages\nCuBERT\nBERT\nMultilingual BERT\nCodeBERT\nRoBERTa\nXLM-RoBERTa\nTable 2: Model architectures evaluated on PL and NL\nbenchmarks. Models in the same row share the same\narchitecture, but are pre-trained on different corpora.\nBaselines.\nTo examine how well the attention\nperforms through comparisons, we design a sim-\nple offset baseline and a simple keyword baseline.\nThe offset baseline with an offset value of i always\nselects the token after i positions of the input to-\nken as its prediction when i > 0, and selects i\npositions before the input token when i < 0. The\nkeyword baseline with a keyword of key always\npredicts the next key token as its prediction. In our\nexperiments, we evaluate offset baselines with each\npossible offset value between 0 and 512 for PL, and\n-512 to 512 for NL. We use all Python and Java key-\nwords for the keyword baselines on Python and\nJava datasets respectively, including tokens such\nas if, for, in, etc. To evaluate the top-k scores\nfor baselines where k ≥2, we combine k simple\nbaselines with different offset (keyword) values to\ngive k predictions. To select k offset (keyword)\nvalues, we repeatedly and greedily include the next\nvalue that yields the highest performance increase\nfor the relation type under consideration.\n4\nExperiments\nIn this section, we present the results of pre-trained\nlanguage models for both PL and NL syntax un-\nderstanding tasks, and discuss the key observations\nthat distinguish PL from NL.\n4.1\nMain Results\nLanguage\nModel\nTop-k Score\nk=1\nk=3\nk=10\nk=20\nPython\nOffset\n43.6\n63.7\n87.3\n94.9\nKeyword\n15.7\n21.9\n23.6\n23.8\nCombined\n49.4\n69.7\n90.1\n96.3\nCuBERT\n39.2\n58.4\n81.3\n91.4\nCodeBERT\n33.1\n51.8\n78.6\n89.2\nRoBERTa\n34.5\n56.9\n82.5\n91.3\nDiff (Model - Baseline)\n-10.2\n-11.3\n-8.8\n-4.9\nJava\nOffset\n52.7\n71.5\n87.1\n94.3\nKeyword\n22.4\n27.3\n30.2\n30.6\nCombined\n60.4\n77.2\n90.0\n96.1\nCuBERT\n39.7\n59.8\n80.0\n90.2\nCodeBERT\n36.3\n57.1\n78.3\n88.8\nRoBERTa\n34.7\n57.8\n80.3\n90.5\nDiff (Model - Baseline)\n-20.7\n-17.4\n-10.0\n-5.9\nTable 3: Top-k scores for code syntax understanding.\nFor each language, the upper block contains the re-\nsults of baselines, including: (1) Offset: always picking\nthe token with a ﬁxed positional offset; (2) Keyword:\nmatching a ﬁxed keyword nearby; and (3) Combined:\ncombining the best option from Offset and Keyword.\nScore differences are calculated as the best attention\nscore - best baseline score for each language, where\na positive value indicates that the language model sur-\npasses the baseline.\nWe present our main results to compare the per-\nformance in syntactic relation understanding on PL\nand NL in Tables 3 and 4, respectively. First, on\nCodeSyntax, language models generally perform\nworse than simple offset baseline and its combi-\nnation with the keyword baseline, which indicates\nLanguage\nModel\nTop-k Score\nk=1\nk=3\nk=10\nk=20\nEnglish\nOffset\n47.1\n72.7\n91.0\n96.6\nBERT-large\n64.6\n83.2\n96.3\n99.3\nRoBERTa-base\n62.7\n84.3\n96.9\n99.4\nCodeBERT\n59.3\n79.7\n95.2\n99.1\nDiff (Model - Baseline)\n17.5\n11.6\n5.9\n2.8\nGerman\nOffset\n36.3\n58.0\n83.1\n95.1\nMultilingual BERT\n62.6\n81.9\n96.5\n99.6\nXLM-RoBERTa-base\n67.4\n85.5\n97.1\n99.7\nDiff (Model - Baseline)\n31.1\n27.5\n14.0\n4.6\nTable 4: Top-k scores for NL syntax understanding.\nNote that BERT-large and CuBERT share the same\nmodel conﬁguration, and CodeBERT and RoBERTa-\nbase have the same model architecture. Unlike Table 3,\nwe exclude Keyword and Combined baselines because\nthey do not add upon the Offset baseline in terms of the\nperformance.\nLanguage\nModel\nTop-k Score (Any-token Metric)\nk=1\nk=3\nk=10\nk=20\nPython\nOffset\n63.6\n85.4\n96.7\n98.9\nKeyword\n22.2\n31.3\n34.9\n35.2\nCombined\n66.8\n88.4\n98.2\n99.6\nCuBERT\n64.3\n82.7\n96.1\n99.2\nCodeBERT\n56.0\n76.5\n93.5\n97.9\nRoBERTa\n49.4\n74.7\n94.4\n98.5\nDiff (Model - Baseline)\n-2.5\n-5.7\n-2.1\n-0.4\nJava\nOffset\n69.4\n86.5\n96.8\n99.0\nKeyword\n40.9\n44.9\n46.7\n47.0\nCombined\n75.7\n90.0\n98.2\n99.6\nCuBERT\n72.1\n87.4\n97.5\n99.5\nCodeBERT\n62.7\n81.1\n93.9\n97.6\nRoBERTa\n59.8\n81.4\n94.9\n98.4\nDiff (Model - Baseline)\n-3.6\n-2.6\n-0.7\n-0.1\nTable 5: Top-k scores for code syntax understanding\nusing the any-token metric.\nthat the attention heads of PL pre-trained models do\nnot effectively capture the syntactic relations in pro-\ngrams. The comparison between CodeBERT and\nRoBERTa further shows that pre-training on a large-\nscale code corpus, in addition to the text corpus for\nRoBERTa pre-training, does not yield a notably bet-\nter understanding of code syntax. In comparison,\nlanguage models substantially outperform offset\nbaselines in recognizing the dependency relations\nin NL, demonstrating that the attention heads learn\nto be specialized for different relation types via\nlarge-scale pre-training on text.\nMeanwhile, we present the any-token results on\nCodeSyntax in Table 5. Although the best com-\nbined baseline still outperforms language models,\nthe performance gap shrinks drastically. In par-\nticular, CuBERT achieves better scores than the\noffset baseline, and the improvement on Java is\nmore notable. We defer the full results of different\ntop-k scores on both PL and NL benchmarks to\nAppendix D. In the following sections, we discuss\nthe key factors that affect prediction performance.\n4.2\nCase Studies: The Effect of Keywords\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.4\n0.6\n0.8\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(a) With semicolons (default).\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(b) Without semicolons.\nFigure 4: Top-k scores for Java syntax understanding\nusing the last-token metric.\nTo examine why the offset baseline outperforms\nCodeBERT and CuBERT, and why the relative per-\nformance differences get smaller when using the\nany-token metric, we conducted case studies and\nerror analysis in Section 4.2 and Section 4.3, which\nboth quantitatively and qualitatively categorize the\nerror patterns.\nFirstly, we investigate the most frequently at-\ntended code tokens, and we observe that the atten-\ntion heads tend to recognize the reserved tokens\nand keywords in PL. For example, CuBERT and\nCodeBERT get an improved score on Java because\nthe semicolon token is part of the ground truth de-\npendent node, which is a popular token attended\nto by language models. Based on this observation,\nwe perform an ablation study on the presence of\nthe semicolon in ground truth annotations. When\nthe semicolon tokens are removed from ground\ntruth dependent nodes, we also disable the lan-\nguage models to attend to semicolons in the in-\nput code. Since the semicolon appears at the end\nof each Java statement, here we compute the last-\ntoken score which may be signiﬁcantly affected by\nsemicolons. As shown in Figure 4, CuBERT sub-\nstantially outperforms baselines when semicolons\nare included in the ground truth labels. On the other\nhand, CuBERT reaches lower scores than baselines\nwhen semicolons are excluded from ground truth\nlabels and predictions. The comparison suggests\nthat attention heads are more capable of identifying\nfrequent keywords in the model input. We defer\nthe full ablation study on both Python and Java to\nAppendix F.\nWe further discuss the breakdown results with\nrespect to relation types, and we select some rep-\nresentative relations for Python that highlight the\nperformance differences between CuBERT and the\noffset baseline in Table 6. First, the attention is\nhighly capable of performing keyword matching,\nwhich leads to decent accuracy on relations that\nconnect popular keywords, such as If:if→else.\nHowever, when the head and dependent tokens\nare diverse, it becomes challenging for the lan-\nguage model to recognize the relation. For ex-\nample, in relation types Assign:target→value\nand Call:func→args, both head and dependent\nnodes can take various identiﬁer names deﬁned by\ndifferent programmers. In particular, CuBERT can\nnot effectively utilize the relative positions of to-\nkens to learn the relations, even if the dependent\nnode is near the head node. In such situations, the\noffset baseline with a ﬁxed offset value of 2 already\nsurpasses the pre-trained model. The full break-\ndown results of all relation types on both Python\nand Java can be found in Appendix G.\nRelation\nScore\nOffset Diff\nCuBERT Offset\nIf:if→else\n92.7\n5.7\n17\n87.1\nIf:body→orelse\n29.2\n7.1\n12\n22.0\nIf:if→body\n31.5\n23.1\n7\n8.4\nFor:for→body\n30.4\n32.7\n7\n-2.3\nAssign:target→value 39.8\n71.2\n2\n-31.4\nWhile:test→body\n16.2\n48.5\n4\n-32.4\nCall:func→args\n59.3\n93.2\n2\n-33.9\nTable 6: The comparison of top-1 ﬁrst-token scores\nbetween CuBERT and the offset baseline with the\nbest ﬁxed offset for selected relation types on Python\ndataset.\n4.3\nError Analysis\nRelation\nError Situation\nCount\nPython Java\nIf:\nif→else\nNested if statements or multiple\nif statements close to each other.\n34\n42\nPredicts other keywords inside\nbody block, e.g., if and while.\n11\n4\nOther.\n5\n4\nIf:\nbody→orelse\nPredicts another token with the\nsame name as head token itself.\n38\n21\nPredicts keywords inside body\nblock, e.g., if, ; and while.\n0\n14\nPredicts a long string or doc-\nstring.\n7\n7\nOther.\n5\n8\nIf:\nif→body\nPredicts blank space or tab.\n32\n0\nPredicts { or }.\n0\n27\nPredicts return.\n0\n19\nPredicts \\n or :.\n15\n0\nOther.\n3\n4\nFor:\nfor→body\nPredicts blank space or tab.\n46\n0\nPredicts { or }.\n0\n29\nOther.\n4\n21\nAssign:\ntarget→value\nPredicts a token that comes be-\nfore =, e.g. a[0] and a.b\n16\n30\nPredicts =.\n22\n5\nOther.\n12\n15\nWhile:\ntest→body\nPredicts a token in the test block.\n48\n36\nOther.\n2\n14\nCall:\nfunc→args\nPredicts ( or ).\n45\n37\nPredicts another token with the\nsame name as head token itself.\n0\n10\nOther.\n5\n3\nTable 7: Error analysis using CuBERT.\nTo categorize the wrong predictions of the at-\ntention, we manually examine 50 error cases for\neach relation selected in Table 6, and present the\nerror situations in Table 7. Again, we observe that\nthe attention often incorrectly selects frequently\noccurring tokens such as brackets. Moreover, the\nmodel has difﬁculty capturing the hierarchical code\nstructure, thus it often attends to nearby keywords\nregardless of logical code blocks.\nTake the relation If:if→else as an example,\non which the language model generally achieves\nthe best performance. Shown in Figure 5 are two\nsample if-statements, where the ﬁrst one does not\ncontain nested ﬂow control blocks while the second\none contains a keyword while inside the if-body.\n\"...\" denotes that some code is omitted. Visualiz-\ning their corresponding attention weights of the\nattention head that performs the best on the relation\nIf:if→else, we observe that the attention head\n(a) Python code.\n'if'\n'if'\n'e'\n'e'\n'.'\n'.'\n'errno'\n'errno'\n'=='\n'=='\n'errno'\n'errno'\n'.'\n'.'\n'EEXIST'\n'EEXIST'\n':'\n':'\n'\\n'\n'\\n'\n'            '\n'            '\n'pass'\n'pass'\n'\\n'\n'\\n'\n''\n''\n'else'\n'else'\n':'\n':'\n'\\n'\n'\\n'\n'if'\n'if'\n'('\n'('\n'len'\n'len'\n'('\n'('\n'self'\n'self'\n'.'\n'.'\n'stack'\n'stack'\n')'\n')'\n'>'\n'>'\n'0'\n'0'\n')'\n')'\n':'\n':'\n'\\n'\n'\\n'\n'            '\n'            '\n'while'\n'while'\n'...'\n'...'\n'else'\n'else'\n':'\n':'\n(b) Attention weights.\nFigure\n5:\nTwo\nsample\ncases\nfor\nthe\nrelation\nIf:if→else and corresponding attention weights of\nCuBERT’s head 17-2.\ncorrectly attends to the else token in the ﬁrst ex-\nample, while it wrongly attends to the while token\ninside the if-body in the second example. More\nexamples like these can be found in Appendix E.\n5\nRelated Work\nTransformer-based language models have been\nwidely used for natural language processing (De-\nvlin et al., 2019; Liu et al., 2019; Wang et al., 2020,\n2021; Shen et al., 2022; Wang et al., 2022). Hewitt\nand Manning (2019) show that syntax trees are im-\nplicitly embedded in BERT’s word representation\nspace via a structural probe. Another line of work\nstudies what is learned by the attention in language\nmodels (Clark et al., 2019; Raganato et al., 2018;\nVoita et al., 2019; Michel et al., 2019; Vig, 2019;\nBurns et al., 2018; Marecek and Rosa, 2018; Voita\net al., 2018). In particular, Clark et al. (2019) eval-\nuate the attention heads of BERT on dependency\nparsing tasks using the English Penn Treebank cor-\npus, where the attention signiﬁcantly outperforms\noffset baselines. On the contrary, we demonstrate\nthat attention-based models largely perform worse\nthan offset baselines on code syntax understanding.\nThe success of Transformer-based models for\nnatural language processing leads to their applica-\ntion in the PL domain (Kanade et al., 2020; Feng\net al., 2020; Rozière et al., 2020, 2021; Clement\net al., 2020; Dehghani et al., 2019). Chen et al.\n(2021) evaluate the model performance by mea-\nsuring the functional correctness on unit tests.\nChirkova and Troshin (2021) empirically shows\nthat Transformers can utilize syntactic information\nto make predictions in some code processing tasks,\nwhile we analyze attention’s ability to understand\nsyntactic relations. Karmakar and Robbes (2021)\nprobe pre-trained models on four code understand-\ning tasks. They focus more on code classiﬁcation,\ne.g., they train a classiﬁer for predicting the AST\nnode tag and the code length. On the contrary,\nwe probe the attention heads for syntactic relation\nunderstanding, and we aim to present a comprehen-\nsive study of the differences between pre-trained\nlanguage models on NL and PL for capturing the\nsyntax structures.\nThere have been some efforts that try to take\ncode structure into account during pre-training\nof Transformer-based models for code. For ex-\nample, GraphCodeBERT (Guo et al., 2021) uti-\nlizes data ﬂow for pretraining; i.e., the relation of\n\"where-the-value-comes-from\" for variables. On\nour Python benchmark, GraphCodeBERT achieves\na top-1 ﬁrst-token score of 39.3, which is better\nthan 33.1 of CodeBERT, and comparable to 39.2 of\nCuBERT. However, such a score is still worse than\n43.6 of the offset baseline. This trend is consistent\nwhen evaluating with other metrics. These results\nshow that pre-training on data ﬂow helps improve\nthe model’s ability to understand code syntax, but\nthere is still large room for improvement.\n6\nConclusion\nIn this work, we introduce CodeSyntax, a large-\nscale benchmark for measuring the performance of\ncode syntax understanding. Based on CodeSyntax,\nwe conduct the ﬁrst comprehensive study to ana-\nlyze the capability of pre-trained language models\non understanding the code syntactic structures with-\nout further ﬁnetuning. We demonstrate that while\nthe attention heads of pre-trained language models\nare able to identify dependency relations in NL to\nsome extent, they have difﬁculty recognizing the\nsyntactic relations in programs. Pre-trained models\neven generally perform worse than simple offset\nbaselines, and they tend to attend to frequently\noccurring nearby tokens without taking the hierar-\nchical code structure into consideration.\nWe also analyze the differences between NL and\nPL from the perspectives of pre-trained models.\nOur evaluation suggests that PL has unique char-\nacteristics that distinguish them from NL, such as\nthe long-term dependency between code tokens,\nand the hierarchy in the syntactic structures. There-\nfore, simply taking a program as a token sequence\nis insufﬁcient for modeling the program structure,\nwhich could eventually limit the potential of lan-\nguage models for code understanding tasks. We\nconsider developing new model architectures and\npre-training algorithms to leverage and represent\nthe code structure and dependency graph as impor-\ntant future work.\n7\nLimitations\nFor the limitations of our benchmark, the gold an-\nnotations are based on the AST parsers. Adding\nnew programming languages whose parsers are un-\navailable will require additional labeling efforts. A\nlimitation in our experimental setup is that we have\nonly benchmarked six models across two kinds\nof natural languages and programming languages.\nFinally, the main focus of our study is to probe\nthe language models for code understanding. As\na result, we have not proposed models that could\ndeal with the code syntax in natural language and\nprogramming language applications. Future work\ncould include developing such models that capture\nboth semantics and structures.\n8\nEthical Considerations\nWe hereby acknowledge that all of the co-authors of\nthis work are aware of the provided ACM Code of\nEthics and honor the code of conduct. The follow-\nings give the aspects of both our ethical considera-\ntions and our potential impacts to the community.\nThis work creates a benchmark to test the code syn-\ntax understanding of pre-trained language models.\nInstead of natural language, the programming lan-\nguage is used for pre-training. We do not anticipate\nthe production of harmful outputs after using our\nbenchmark and existing models, especially towards\nvulnerable populations.\n9\nEnvironmental Considerations\nWe use several pre-trained language models. Ac-\ncording to the estimation in (Strubell et al., 2019),\npre-training a model with a similar size as used in\nthe work costs 1,507 kWh·PUE and emits 1,438 lb\nCO2. This work focuses on inference. Therefore,\nour energy cost and CO2 emissions are relatively\nsmall.\nAcknowledgements\nWe would like to thank the anonymous reviewers\nfor their suggestions and comments. This material\nis in part based upon work supported by Berke-\nley DeepDrive and Berkeley Artiﬁcial Intelligence\nResearch.\nReferences\nAnn Bies, Justin Mott, and Colin Warner. 2015. En-\nglish news text treebank: Penn treebank revised.\nPhiladelphia: Linguistic Data Consortium.\nKaylee Burns, Aida Nematzadeh, Erin Grant, Alison\nGopnik, and Thomas L. Grifﬁths. 2018.\nExploit-\ning attention to reveal shortcomings in memory mod-\nels.\nIn Proceedings of the Workshop: Analyzing\nand Interpreting Neural Networks for NLP, Black-\nboxNLP@EMNLP 2018, Brussels, Belgium, Novem-\nber 1, 2018, pages 378–380. Association for Com-\nputational Linguistics.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harrison Edwards, Yuri Burda, Nicholas\nJoseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott\nGray, Nick Ryder, Mikhail Pavlov, Alethea Power,\nLukasz Kaiser, Mohammad Bavarian, Clemens Win-\nter, Philippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. 2021. Evalu-\nating large language models trained on code. CoRR,\nabs/2107.03374.\nNadezhda Chirkova and Sergey Troshin. 2021.\nEm-\npirical study of transformers for source code.\nIn\nProceedings of the 29th ACM Joint Meeting on Eu-\nropean Software Engineering Conference and Sym-\nposium on the Foundations of Software Engineer-\ning, ESEC/FSE 2021, page 703–715, New York, NY,\nUSA. Association for Computing Machinery.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does bert look\nat? an analysis of bert’s attention. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP. The\nAssociation for Computational Linguistics.\nColin B. Clement, Dawn Drain, Jonathan Timcheck,\nAlexey Svyatkovskiy, and Neel Sundaresan. 2020.\nPymt5: multi-mode translation of natural language\nand python code with transformers. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, On-\nline, November 16-20, 2020, pages 9052–9065. As-\nsociation for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nIn\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 8440–8451. Associa-\ntion for Computational Linguistics.\nEclipse Contributors. 2014.\nRational software archi-\ntect realtime edition 9.5.0. https://www.ibm.com/\ndocs/en/rsar/9.5?topic=SS5JSH_9.5.0/org.\neclipse.jdt.doc.isv/reference/api/org/\neclipse/jdt/core/dom/package-use.html.\nMarie-Catherine de Marneffe and Christopher D. Man-\nning. 2008a. Stanford dependencies manual.\nMarie-Catherine de Marneffe and Christopher D. Man-\nning. 2008b. The stanford typed dependencies rep-\nresentation.\nIn Proceedings of the workshop on\nCross-Framework and Cross-Domain Parser Evalu-\nation@COLING 2008, Manchester, UK, August 23,\n2008, pages 1–8. Coling 2008 Organizing Commit-\ntee.\nMarie-Catherine de Marneffe, Christopher D. Manning,\nJoakim Nivre, and Daniel Zeman. 2021. Universal\ndependencies. Comput. Linguistics, 47(2):255–308.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020.\nCodebert: A\npre-trained model for programming and natural lan-\nguages. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, Online Event, 16-\n20 November 2020.\nKilian A. Foth, Arne Köhn, Niels Beuck, and Wolf-\ngang Menzel. 2014. Because size does matter: The\nhamburg dependency treebank. In Proceedings of\nthe Ninth International Conference on Language Re-\nsources and Evaluation, LREC 2014, Reykjavik, Ice-\nland, May 26-31, 2014, pages 2326–2333. European\nLanguage Resources Association (ELRA).\nPython Software Foundation. 2021. Python 3.10.0 doc-\numentation, ast — abstract syntax trees.\nhttps:\n//docs.python.org/3/library/ast.html.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,\nDuyu Tang, Shujie Liu, Long Zhou, Nan Duan,\nAlexey Svyatkovskiy, Shengyu Fu, Michele Tu-\nfano, Shao Kun Deng, Colin B. Clement, Dawn\nDrain, Neel Sundaresan, Jian Yin, Daxin Jiang, and\nMing Zhou. 2021.\nGraphcodebert:\nPre-training\ncode representations with data ﬂow.\nIn 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net.\nJohn Hewitt and Christopher D. Manning. 2019.\nA\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4129–4138. Association for Computa-\ntional Linguistics.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019.\nCode-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nAditya Kanade, Petros Maniatis, Gogul Balakrishnan,\nand Kensen Shi. 2020. Learning and evaluating con-\ntextual embedding of source code. In International\nConference on Machine Learning, pages 5110–5121.\nPMLR.\nAnjan Karmakar and Romain Robbes. 2021.\nWhat\ndo pre-trained code models know about code?\nIn\n36th IEEE/ACM International Conference on Auto-\nmated Software Engineering, ASE 2021, Melbourne,\nAustralia, November 15-19, 2021, pages 1332–1336.\nIEEE.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nDavid Marecek and Rudolf Rosa. 2018.\nExtract-\ning syntactic trees from transformer encoder self-\nattentions.\nIn Proceedings of the Workshop: An-\nalyzing and Interpreting Neural Networks for NLP,\nBlackboxNLP@EMNLP 2018, Brussels, Belgium,\nNovember 1, 2018, pages 347–349. Association for\nComputational Linguistics.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one?\nIn Ad-\nvances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-\n14, 2019, Vancouver, BC, Canada, pages 14014–\n14024.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual bert?\nIn Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 4996–5001. Association for Computa-\ntional Linguistics.\nAlessandro Raganato, Jörg Tiedemann, et al. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation.\nIn Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP. The As-\nsociation for Computational Linguistics.\nBaptiste\nRozière,\nMarie-Anne\nLachaux,\nLowik\nChanussot, and Guillaume Lample. 2020.\nUnsu-\npervised translation of programming languages. In\nAdvances in Neural Information Processing Systems\n33:\nAnnual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nBaptiste Rozière,\nJie M. Zhang,\nFrançois Char-\nton, Mark Harman, Gabriel Synnaeve, and Guil-\nlaume Lample. 2021.\nLeveraging automated unit\ntests for unsupervised code translation.\nCoRR,\nabs/2110.06773.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-\nmany, Volume 1: Long Papers. The Association for\nComputer Linguistics.\nJianhao Shen, Chenguang Wang, Linyuan Gong, and\nDawn Song. 2022.\nJoint language semantic and\nstructure embedding for knowledge graph comple-\ntion. In COLING.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019.\nEnergy and policy considerations for\ndeep learning in NLP. In ACL, pages 3645–3650.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJesse Vig. 2019. Visualizing attention in transformer-\nbased language representation models.\nCoRR,\nabs/1904.02679.\nElena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018.\nContext-aware neural machine trans-\nlation learns anaphora resolution.\nIn Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics, ACL 2018, Melbourne,\nAustralia, July 15-20, 2018, Volume 1: Long Pa-\npers, pages 1264–1274. Association for Computa-\ntional Linguistics.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Conference of the Association for Computa-\ntional Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n5797–5808. Association for Computational Linguis-\ntics.\nChenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong,\nJie Tang, and Dawn Song. 2021. Zero-shot informa-\ntion extraction as a uniﬁed text-to-triple translation.\nIn EMNLP.\nChenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong,\nJie Tang, and Dawn Song. 2022. DeepStruct: Pre-\ntraining of language models for structure prediction.\nIn ACL.\nChenguang Wang, Xiao Liu, and Dawn Song. 2020.\nLanguage models are open knowledge graphs.\narXiv preprint arXiv:2010.11967.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.\nLe,\nMohammad Norouzi,\nWolfgang Macherey,\nMaxim Krikun,\nYuan Cao,\nQin Gao,\nKlaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. CoRR, abs/1609.08144.\nA\nMore Details on CodeSyntax\nConstruction\nSince the code search net dataset does not come\nwith syntactic relation labels, we come up with\na way of extracting syntactic relations. We ﬁrst\nutilize python’s tokenize module and javalang mod-\nule to produce code tokens from source code, and\nthen label these code tokens with syntactic relations\nby using AST parsers on source code. We utilize\nPython ast module (Foundation, 2021) and Java\norg.eclipse.jdt.core.dom.ASTParser class (Contrib-\nutors, 2014) to parse source code into ast nodes.\nThe AST structure captures syntactical relations.\nAn AST node has children AST nodes and a name\nthat denotes its class.\nWe use the class of the\nnode as label and children nodes as dependents\nand heads when generating annotations. For exam-\nple, the source code A = B, which means assign-\ning value B to target variable A, is parsed into the\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(a) Python (First Token Metric)\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(b) Java (First Token Metric)\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.6\n0.7\n0.8\n0.9\n1.0\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(c) Python (Any Token Metric)\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.7\n0.8\n0.9\n1.0\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(d) Java (Any Token Metric)\nFigure 6: PL Top-k Scores On Test Set\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAverage Score\nBERT-base\nBERT-large\nRoBERTa-base\nRoBERTa-large\nOffset\nFigure 7: English Top-k Scores\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.4\n0.6\n0.8\n1.0\nAverage Score\nMultilingual BERT\nXLM-RoBERTa-base\nXLM-RoBERTa-large\nOffset\nFigure 8: German Top-k Scores\nAST node Assign(targets=[Name(id=’A’)],\nvalue=Name(id=’B’)). It gives us a syntactic rela-\ntion whose head is A and dependent is B, annotated\nwith the relation type label Assign. Full statistics\nof CodeSyntax are displayed in Table 8.\nB\nMore Details on Attention Map\nExtraction for Code Language Models\nOur experiments follow the work of Clark et al.\n(2019). They evaluate the attention heads of BERT\non dependency parsing tasks on an English dataset,\nwhile we extend the work to the PL domain. We\nadopt and extend some of their code, such as the\nfunctions for extracting attention from BERT and\nplotting attention weights. The main differences\nbetween our work and theirs are that we construct\na novel dataset for syntax understanding tasks for\nPL and come up with related evaluation metrics to\naccommodate the characteristics of PL.\nB.1\nModel Input\nEach of our code samples is an entire Python or\nJava function. To prepare the input to be fed to the\nmodels, we run CuBERT and CodeBERT tokeniza-\ntion to obtain sequences of input ids for each code\nsample. We insert a [CLS] token at the beginning\nand append a [SEP] token at the end. If the input\nlength is longer than 512 tokens (the maximum\nnumber of tokens allowed), we discard that code\nsample. We never split a long code sample into\nseveral input sentences because the span of some\ndependency relations is very long within a function.\nFor example, for an if statement, the else block\nmay be far away from the keyword if. If we split\nthem into two input sentences, then attention will\nnot be able to understand and predict the relation\nbetween them. To avoid uncommon data points,\nwe remove a code sample from both CuBERT and\nCodeBERT’s input if it is longer than 512 tokens\nafter either one of CuBERT or CodeBERT’s tok-\nenization.\nB.2\nToken Alignment And Word-level\nAttention\nBERT uses WordPiece tokenization (Wu et al.,\n2016) and RoBERTa uses byte-level Byte-Pair En-\ncoding (BPE) (Sennrich et al., 2016), which may\nsplit a word into several subtokens. Additionally,\nCuBERT imposes some special rules when produc-\ning program vocabulary. However, our dataset’s\nlabels use code tokens generated by the tokenize\nmodule and the javalang module. Therefore, there\nexists a need to align CuBERT/CodeBERT subto-\nkens with code tokens in order to evaluate the mod-\nels on our dataset. We ﬁrst generate such an align-\nment that maps each code token to a set of Cu-\nBERT/CodeBERT subtokens, and then convert the\noriginal subtoken-level attention to word-level at-\ntention. We follow (Clark et al., 2019) to combine\nthe attention weights of subtokens, i.e., we sum up\ntheir attention weights.\nC\nMore Reproducibility Information\nHere we provide more information according to the\nEMNLP 2022 Reproducibility Criteria.\n• Train/validation/test splits for datasets used:\nWe do not ﬁnetune the pre-trained models\non our benchmark.\nThe validation set of\nCodeSyntax contains the code samples that\ncome from the validation set of CodeSearch-\nNet, and our test set contains the samples from\nCodeSearchNet’s test set. We use our test par-\ntition to probe the pre-trained attention heads\nwhile the validation set is not used.\n• Number of parameters in each model: Cu-\nbert and BERT-large have 340M parameters.\nCodeBert and RoBERTa-base have 125M pa-\nrameters. XLM-RoBERTa-base has 250M pa-\nrameters. Multilingual BERT-base has 110M\nparameters.\n• The average runtime for each model or algo-\nrithm: Running the pipeline to construct the\nCodeSyntax dataset takes about four hours\nassuming that dependencies and required\ndatasets have been downloaded. The algo-\nrithm to probe a pre-trained model on one\nprogramming language of CodeSyntax takes\nabout twelve hours on our machine using one\nNvidia 1080Ti GPU.\nD\nMore Results on Top-k Scores\nPL top-k scores are plotted in ﬁgure 6. NL scores\nare plotted in ﬁgure 7 (English) and ﬁgure 8 (Ger-\nman).\nE\nExamples of Correct and Incorrect\nPredictions\nIn this section, we present some visualization ex-\namples where attention correctly or incorrectly pre-\ndicts the dependents. The heads chosen in these ex-\namples are the best-performing heads of CuBERT\nevaluated using the ﬁrst-token metric. We feed the\nentire function as input to the transformer, however,\nwe only present relevant snippets here for simplic-\nity. In the source code displayed, \"...\" denotes that\nthe remaining part of the code is omitted. As a re-\nsult, the attention from a token may not sum up to\none in these ﬁgures because the rest of the function\nis omitted.\nRelation Call: func →args.\nThe correspond-\ning attention weights are visualized in Table 9 for\nPython and 10 for Java.\n• Python correct case.\nn = len(x)\nn_fft = len(win_sq)\nAttention correctly predicts the arguments x\nand win_sq, respectively.\n• Python error case.\nre.findall(pattern,text)\nThe function findall is called. The correct\nprediction should be the ﬁrst argument, which\nis pattern; however, attention incorrectly\npredicts the parenthesis (.\n• Java correct case.\nsubscriber.onError(ex);\nThe token ex has the largest weight in atten-\ntion, which is a correct prediction.\n• Java error case.\nisBug(error)\nThe function isBug is called. The correct pre-\ndiction should be the argument, error; how-\never, attention incorrectly predicts ).\n'n'\n'n'\n'='\n'='\n'len'\n'len'\n'('\n'('\n'x'\n'x'\n')'\n')'\n'\\n'\n'\\n'\n'n_fft'\n'n_fft'\n'='\n'='\n'len'\n'len'\n'('\n'('\n'win_sq'\n'win_sq'\n')'\n')'\n're'\n're'\n'.'\n'.'\n'findall'\n'findall'\n'('\n'('\n'pattern'\n'pattern'\n','\n','\n'text'\n'text'\n')'\n')'\nFigure 9: Python Head 15-11 Call: func →args.\n'subscriber'\n'subscriber'\n'.'\n'.'\n'onError'\n'onError'\n'('\n'('\n'ex'\n'ex'\n')'\n')'\n';'\n';'\n'isBug'\n'isBug'\n'('\n'('\n'error'\n'error'\n')'\n')'\nFigure 10: Java Head 19-9 Call: func →args\nRelation Assign: target →value.\nThe cor-\nresponding attention weights are visualized in Ta-\nble 11 for Python and 12 for Java.\n• Python correct case.\nvalue = round(value,\nprecision)\nThe assigned value round is correctly pre-\ndicted.\n• Python error case.\nd[\"_text\"] = r.text\nThe value assigned is r.text, but attention\nincorrectly predicts [.\n• Java correct case.\nint p = parallelism();\nAttention has the largest weight for the head\ntoken parallelism, which correctly predicts\nthe relation.\n• Java error case.\nthis.defaultProcessor\n= processor;\nThe value assigned is processor, but atten-\ntion incorrectly predicts ;.\n'value'\n'value'\n'='\n'='\n'round'\n'round'\n'('\n'('\n'value'\n'value'\n','\n','\n'precision'\n'precision'\n')'\n')'\n'\\n'\n'\\n'\n'd'\n'd'\n'['\n'['\n'\"_text\"'\n'\"_text\"'\n']'\n']'\n'='\n'='\n'r'\n'r'\n'.'\n'.'\n'text'\n'text'\nFigure 11: Python Head 15-10 Assign: target →value\n'int'\n'int'\n'p'\n'p'\n'='\n'='\n'parallelism'\n'parallelism'\n'('\n'('\n')'\n')'\n';'\n';'\n'this'\n'this'\n'.'\n'.'\n'defaultProcessor'\n'defaultProcessor'\n'='\n'='\n'processor'\n'processor'\n';'\n';'\nFigure 12: Java Head 20-10 Assign: target →value\nRelation If: if →else.\nThe corresponding\nattention weights are visualized in Table 13 for\nJava.\n• Java correct case.\nif (t instanceof Error) {\nthrow (Error) t;\n} else {\n...\nIt correctly identiﬁes the keyword else.\n• Java error case.\nif(error.addThrowable(ex)) {\nif ...\n} else {\n...\nThere is another if statement inside the body\nof the ﬁrst if statement. The correct prediction\nshould be keyword else, but it predicts the\ninner if.\nRelation For: for →body.\nThe corresponding\nattention weights are visualized in Table 14 for\nPython and 15 for Java.\n• Python correct case.\n'if'\n'if'\n'('\n'('\n't'\n't'\n'instanceof'\n'instanceof'\n'Error'\n'Error'\n')'\n')'\n'{'\n'{'\n'\\n'\n'\\n'\n'throw'\n'throw'\n'('\n'('\n'Error'\n'Error'\n')'\n')'\n't'\n't'\n';'\n';'\n'\\n'\n'\\n'\n'}'\n'}'\n'else'\n'else'\n'{'\n'{'\n'if'\n'if'\n'('\n'('\n'error'\n'error'\n'.'\n'.'\n'addThrowable'\n'addThrowable'\n'('\n'('\n'ex'\n'ex'\n')'\n')'\n')'\n')'\n'{'\n'{'\n'\\n'\n'\\n'\n'if'\n'if'\n'...'\n'...'\n'\\n'\n'\\n'\n'}'\n'}'\n'else'\n'else'\nFigure 13: Java Head 9-10 If: if →else\nfor el in predictions:\nif 0 in el:\n...\n• Python error case.\nfor pass_ in\nself.working_list:\nret.append(...\nThe correct prediction should be the ﬁrst to-\nken within the body, which is ret; however,\nattention incorrectly predicts the blank space\n\"\n\" before ret.\n• Java correct case.\nfor(BehaviorSubscription<T>\ns : array) {\nif (...\n• Java error case.\nfor (;;) {\nCacheSubscription ...\nThe\ncorrect\nprediction\nshould\nbe\nthe\nﬁrst token within the body,\nwhich is\nCacheSubscription;\nhowever,\nattention\nincorrectly predicts {.\n'for'\n'for'\n'el'\n'el'\n'in'\n'in'\n'predictions'\n'predictions'\n':'\n':'\n'\\n'\n'\\n'\n'        '\n'        '\n'if'\n'if'\n'0'\n'0'\n'in'\n'in'\n'el'\n'el'\n':'\n':'\n'\\n'\n'\\n'\n'for'\n'for'\n'pass_'\n'pass_'\n'in'\n'in'\n'self'\n'self'\n'.'\n'.'\n'working_list'\n'working_list'\n':'\n':'\n'\\n'\n'\\n'\n'            '\n'            '\n'ret'\n'ret'\n'.'\n'.'\n'append'\n'append'\n'('\n'('\nFigure 14: Python Head 18-4 For: for →body\n'for'\n'for'\n'('\n'('\n'BehaviorSubscription'\n'BehaviorSubscription'\n'<'\n'<'\n'T'\n'T'\n'>'\n'>'\n's'\n's'\n':'\n':'\n'array'\n'array'\n')'\n')'\n'{'\n'{'\n'\\n'\n'\\n'\n'if'\n'if'\n'('\n'('\n'for'\n'for'\n'('\n'('\n';'\n';'\n';'\n';'\n')'\n')'\n'{'\n'{'\n'\\n'\n'\\n'\n'CacheSubscription'\n'CacheSubscription'\nFigure 15: Java Head 16-5 For: for →body\nF\nMore Results on the Ablation Study of\nDelimiter Tokens\nThe ablation study on java dataset is shown in Fig-\nure 17 (any-token metric) and Figure 16 (last-token\nmetric). Results with ﬁrst-token metric are not af-\nfected at all because semicolons and newlines are\nnever used as the ﬁrst token of dependents. We\nfound that attention performs very well with last-\ntoken metric because it can ﬁnd semicolons and\nnewlines.\nThe ablation study on python dataset is shown\nin Figure 18 (any-token metric) and Figure 19 (last-\ntoken metric).\nG\nMore Breakdown Results on Different\nRelation Types\nMore results on comparisons of top-1 scores be-\ntween CuBERT and the offset baseline are pre-\nsented in Tables 9, 10, 11, and 12.\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.4\n0.6\n0.8\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(a) Original\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.4\n0.6\n0.8\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(b) With Newline\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(c) Without Semicolons\nFigure 16: Top-k scores for Java syntax understanding\nusing the last-token metric.\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.7\n0.8\n0.9\n1.0\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(a) Original\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.7\n0.8\n0.9\n1.0\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(b) With Newline\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.7\n0.8\n0.9\n1.0\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(c) Without Semicolons\nFigure 17: Ablation Study (Java Any-Token Metric).\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.6\n0.7\n0.8\n0.9\n1.0\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(a) Original\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.6\n0.7\n0.8\n0.9\n1.0\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(b) With Newline\nFigure 18: Ablation Study (Python Any-Token Metric).\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(a) Original\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\nk\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAverage Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(b) With Newline\nFigure 19: Ablation Study (Python Last-Token Metric).\nRelation:head→dependent\nCount\nExplanation\nCode Example\nPython\nJava\nPython\nJava\nAssign:target→value\n78482 13384 Assigning a value to a target\nvariable.\ntarget = 10\nint target = 10;\nAttribute:value→attr\n158797 84215\nAccessing the attribute (member\nﬁeld or member function) of an\nvalue.\nvalue.attribute\nline.setLength(2);\nAugAssign:target→value\n3150\n/\nAn\nassignment\naugmented\nwith an operation.\nFor Java,\nthis\ncase\nis\nincluded\nin\nAssign:target→value.\nx += 2\n/\nBinOp:left→right\n26035\n/ A binary operation.\na + b\n/\nBoolOp:value→value\n5783\n/ A boolean operation\nTrue or False\n/\nCall:args→keywords\n9256\n/ Calling a function with some\narguments (and keywords).\nfunction(arg, key=1) function(arg);\nCall:func→args\n110949 50890\nCall:func→keywords\n16274\n/\nCompare:left→comparator\n25852\n/ A comparison between values.\na < b\n/\nDict:key→value\n7787\n/ Initializing a dictionary.\n{count : 10 }\n/\nDictComp:key→generator\n359\n/\nDictComp:key→value\n359\n/ Dictionary comprehension.\n{i: 2*i for i in list}\n/\nDictComp:value→generator\n359\n/\nDo:body→test\n/\n38 The do loop repeatedly executes\nthe body block as long as the\ncondition is true.\ndo\nstatement;\nwhile (condition);\nDo:do→body\n/\n45\n/\nDo:do→test\n/\n38\nFor:for→body\n8704\n1864\nA for loop repeatedly executes\nthe body block for some\niterations.\nfor target in iter:\nbody\nfor (initializers;\ntest; updaters) {\nbody;\n}\nFor:for→initializers\n/\n1650\nFor:for→iter\n8704\n/\nFor:for→target\n8704\n/\nFor:for→test\n/\n1296\nFor:for→updaters\n/\n1682\nFor:initializers→body\n/\n1781\nFor:initializers→test\n/\n1286\nFor:initializers→updaters\n/\n1670\nFor:iter→body\n8704\n/\nFor:target→body\n8704\n/\nFor:target→iter\n8704\n/\nFor:test→body\n/\n1789\nFor:test→updaters\n/\n1678\nFor:updaters→body\n/\n1685\nGeneratorExp:elt→generator\n685\n/ A generator expression.\n(2*i for i in list)\n/\nIf:body→orelse\n11024\n4976\nAn if statement conditionally\nexecutes a body based upon\nsome criteria.\nif condition:\nbody1\nelse:\nbody2\nif (condition) {\nbody1;\n} else {\nbody2;\n}\nIf:if→body\n34250 22392\nIf:if→else\n11024\n5038\nIf:if→test\n34250 19323\nIf:test→body\n34250 22392\nIf:test→orelse\n11024\n5007\nIfExp:body→orelse\n1262\n1173\nAn if expression (conditional\nexpression).\nx if condition else y\n(condition) ? x : y\nIfExp:body→test\n1262\n/\nIfExp:test→body\n/\n1218\nIfExp:test→orelse\n1262\n1173\nInﬁxExpr:left→right\n/ 35170 Inﬁx expression of the form\nleftOperand InﬁxOperator right-\nOperand.\n/\na + b\nInstanceofExpr:expr→type\n/\n1367 Checking whether an expression\nis some type.\n/\ninput instanceof String\nLabeledStatement:label→body\n/\n10 A statement labeled with an\nidentiﬁer.\n/\nIdentiﬁer : Statement\nListComp:elt→generator\n2691\n/ List comprehension.\n[x for x in list1]\n/\nSetComp:elt→generator\n67\n/ Set comprehension.\n{x for x in list1}\n/\nSlice:lower→upper\n731\n/ A slice used in subscript of lists. A[2:6]\n/\nSubscript:value→slice\n39271\n4555 Accessing parts of an array or\ndata structure through subscript.\nA[2:6]\nA[0]\nContinued on next page.\nContinued from previous page.\nRelation:head→dependent\nCount\nExplanation\nCode Example\nPython\nJava\nPython\nJava\nSwitch:expr→statement\n/\n385 A switch statement chooses a\nbranch to execute based upon\nconditions.\nswitch (inputExpr) {\nStatement;\n}\nSwitch:switch→expr\n/\n320\n/\nSwitch:switch→statement\n/\n385\nTry:body→ﬁnalbody\n135\n474\nA try statement for handling\nexceptions.\ntry:\nbody1\nexcept Exception:\nbody2\nelse:\nbody3\nﬁnally:\nbody4\ntry {\nbody1;\n} catch (Exception e) {\nbody2;\n} ﬁnally {\nbody3;\n}\nTry:body→handler\n3020\n2011\nTry:body→orelse\n181\n/\nTry:handler→ﬁnalbody\n48\n186\nTry:handler→orelse\n181\n/\nWhile:test→body\n743\n975 The while loop repeatedly\nexecutes the body block as long\nas the condition is true.\nwhile condition:\nbody\nwhile (condition) {\nbody;\n}\nWhile:while→body\n743\n975\nWhile:while→test\n743\n416\nWith:item→body\n1239\n/ A with statement with built-in\ncontext manager.\nwith open(\"ﬁle\") as f:\ncontent = f.read()\n/\nchildren:parent→child\n652417 569499 Any pair of AST nodes that are\nparent and child in the parse\ntree.\n/\n/\ncomprehension:target→iter\n3881\n/ A for clause to iterate over some\nsequences.\n[x for x in list1]\n/\nTable 8: Full dataset statistics table of CodeSyntax. For each relation type, we highlight the head and dependent\nnodes in the examples in bold, with the head in blue and the dependent in red. If a node can be either head or\ndependent in different relations, we color it in green. For more explanation about the syntax, please refer to the\ndocumentation of Python ast module (Foundation, 2021) and Java org.eclipse.jdt.core.dom.ASTParser (Contribu-\ntors, 2014)\nRelation\nScore\nOffset\nDifference\nCuBERT\nOffset\nIf:if→else\n92.7\n5.7\n17\n87.1\nIfExp:body→orelse\n46.4\n13.6\n6\n32.8\nTry:body→handler\n39.1\n7.9\n8\n31.3\nIf:body→orelse\n29.2\n7.1\n12\n22.0\nBoolOp:value→value\n33.3\n22.4\n2\n10.9\nTry:body→ﬁnalbody\n20.5\n10.3\n7\n10.3\nIf:if→body\n31.5\n23.1\n7\n8.4\nCall:func→keywords\n38.0\n34.1\n2\n4.0\nIf:test→orelse\n7.5\n5.1\n18\n2.4\nCompare:left→comparator\n46.5\n45.7\n2\n0.8\nIf:if→test\n98.8\n98.8\n1\n-0.0\nFor:for→target\n99.4\n99.4\n1\n0.0\nWhile:while→test\n99.3\n99.3\n1\n0.0\nTry:body→orelse\n10.5\n10.5\n33\n0.0\nFor:for→body\n30.4\n32.7\n7\n-2.3\nTry:handler→orelse\n13.2\n15.8\n16\n-2.6\nIfExp:test→orelse\n23.2\n26.7\n2\n-3.5\nchildren:parent→child\n26.3\n30.7\n2\n-4.4\nAttribute:value→attr\n76.7\n82.3\n2\n-5.6\nFor:target→body\n26.4\n32.7\n6\n-6.3\nIf:test→body\n16.3\n23.2\n6\n-6.9\nSubscript:value→slice\n59.4\n66.4\n2\n-7.1\nBinOp:left→right\n31.5\n45.2\n2\n-13.7\nTry:handler→ﬁnalbody\n8.3\n25.0\n21\n-16.7\nFor:target→iter\n58.3\n77.4\n2\n-19.1\nAugAssign:target→value\n49.4\n70.3\n2\n-21.0\nFor:iter→body\n11.7\n34.8\n4\n-23.1\nIfExp:body→test\n17.2\n42.1\n2\n-24.9\nWhile:while→body\n22.1\n48.5\n5\n-26.5\nAssign:target→value\n39.8\n71.2\n2\n-31.4\nWhile:test→body\n16.2\n48.5\n4\n-32.4\nCall:func→args\n59.3\n93.2\n2\n-33.9\nCall:args→keywords\n20.6\n54.9\n2\n-34.4\nFor:for→iter\n34.1\n77.4\n3\n-43.3\nTable 9: Attention vs. offset baseline with ﬁxed offset for each relation on Python dataset using ﬁrst-token metric.\nIn the score column, we present the accuracy score for CuBERT and offset baseline. In the offset column, the\nchosen offset is shown. Score differences are calculated as CuBERT score - offset baseline score for each relation,\nwhere a positive value indicates that the language model surpasses the baseline performance. Since CuBERT\nalways outperforms Codebert, we only include results for CuBERT.\nRelation\nScore\nOffset\nDifference\nCuBERT\nOffset\nIf:if→else\n92.7\n5.7\n17\n87.1\nIfExp:body→orelse\n52.4\n21.3\n10\n31.1\nFor:target→iter\n98.8\n77.4\n2\n21.4\nFor:target→body\n97.2\n81.4\n14\n15.8\nIf:if→body\n77.0\n61.8\n11\n15.2\nIf:body→orelse\n60.7\n48.7\n18\n12.0\nFor:for→body\n93.2\n81.3\n15\n11.8\nCompare:left→comparator\n56.4\n45.7\n2\n10.7\nTry:body→orelse\n57.9\n52.6\n62\n5.3\nWhile:while→body\n95.6\n92.6\n14\n2.9\nchildren:parent→child\n45.3\n42.6\n2\n2.7\nIf:if→test\n100.0\n98.8\n1\n1.1\nBinOp:left→right\n46.0\n45.2\n2\n0.8\nFor:for→target\n99.4\n99.4\n1\n0.0\nWhile:while→test\n99.3\n99.3\n1\n0.0\nTry:body→ﬁnalbody\n25.6\n25.6\n56\n0.0\nAssign:target→value\n78.2\n79.7\n4\n-1.5\nCall:func→keywords\n62.4\n64.7\n4\n-2.3\nIf:test→body\n59.2\n61.7\n10\n-2.5\nBoolOp:value→value\n47.8\n50.6\n8\n-2.8\nAugAssign:target→value\n66.8\n70.3\n2\n-3.6\nSubscript:value→slice\n61.7\n66.4\n2\n-4.8\nFor:for→iter\n72.2\n77.4\n3\n-5.2\nAttribute:value→attr\n76.7\n82.3\n2\n-5.6\nIfExp:body→test\n34.8\n42.1\n2\n-7.3\nWhile:test→body\n85.3\n92.6\n13\n-7.4\nIf:test→orelse\n36.5\n44.9\n31\n-8.4\nTry:body→handler\n41.6\n51.2\n17\n-9.7\nIfExp:test→orelse\n30.0\n40.7\n4\n-10.7\nTry:handler→orelse\n31.6\n47.4\n25\n-15.8\nCall:func→args\n75.3\n93.2\n2\n-17.8\nFor:iter→body\n63.6\n83.5\n12\n-19.8\nCall:args→keywords\n49.2\n74.4\n4\n-25.2\nTry:handler→ﬁnalbody\n16.7\n58.3\n21\n-41.7\nTable 10: Attention vs. offset baseline with ﬁxed offset for each relation on Python dataset using any-token metric.\nRelation\nScore\nOffset\nDifference\nCuBERT\nOffset\nIf:if→else\n87.0\n7.0\n15\n80.0\nSwitch:switch→statement\n100.0\n75.2\n5\n24.8\nIf:if→body\n48.8\n26.2\n7\n22.5\nFor:test→updaters\n76.0\n53.4\n4\n22.5\nIf:body→orelse\n28.7\n9.4\n10\n19.3\nTry:body→handler\n24.4\n5.5\n8\n18.9\nDo:body→test\n13.3\n6.7\n74\n6.7\nTry:body→ﬁnalbody\n7.0\n4.3\n9\n2.6\nDo:do→test\n6.7\n6.7\n34\n0.0\nIfExp:test→orelse\n24.3\n24.5\n7\n-0.2\nInstanceofExpr:expr→type\n89.8\n91.9\n2\n-2.1\nFor:for→initializers\n97.5\n100.0\n2\n-2.5\nAttribute:value→attr\n81.2\n83.9\n2\n-2.7\nchildren:parent→child\n34.2\n36.8\n2\n-2.7\nIf:test→orelse\n2.6\n6.8\n15\n-4.2\nFor:initializers→updaters\n39.8\n45.7\n9\n-6.0\nSubscript:value→slice\n72.3\n78.8\n2\n-6.5\nIfExp:body→orelse\n44.2\n52.5\n2\n-8.2\nFor:for→updaters\n36.2\n45.5\n11\n-9.3\nTry:handler→ﬁnalbody\n19.5\n29.9\n16\n-10.4\nInﬁxExpr:left→right\n50.7\n61.6\n2\n-10.9\nSwitch:switch→expr\n89.0\n100.0\n2\n-11.0\nIf:test→body\n11.3\n26.2\n5\n-14.9\nIfExp:test→body\n21.2\n37.5\n5\n-16.3\nFor:initializers→body\n21.0\n37.5\n13\n-16.6\nSwitch:expr→statement\n58.1\n75.2\n3\n-17.1\nFor:for→body\n16.1\n36.0\n15\n-19.9\nWhile:while→body\n13.2\n42.7\n9\n-29.6\nWhile:test→body\n9.4\n42.7\n7\n-33.3\nAssign:target→value\n35.3\n68.7\n2\n-33.4\nCall:func→args\n63.6\n98.7\n2\n-35.1\nFor:test→body\n13.9\n49.2\n8\n-35.3\nIf:if→test\n58.2\n96.5\n2\n-38.3\nWhile:while→test\n41.9\n82.0\n2\n-40.1\nFor:updaters→body\n19.4\n88.9\n4\n-69.5\nFor:initializers→test\n15.1\n85.4\n5\n-70.3\nDo:do→body\n26.7\n100.0\n2\n-73.3\nFor:for→test\n10.5\n84.8\n7\n-74.3\nTable 11: Attention vs. offset baseline with ﬁxed offset for each relation on Java dataset using ﬁrst-token metric.\nRelation\nScore\nOffset\nDifference\nCuBERT\nOffset\nIf:if→else\n87.0\n7.0\n15\n80.0\nFor:test→updaters\n86.9\n53.6\n5\n33.3\nIf:if→body\n79.6\n62.1\n11\n17.5\nIf:body→orelse\n69.1\n52.1\n16\n17.0\nWhile:while→test\n98.8\n82.0\n2\n16.9\nAssign:target→value\n81.4\n68.7\n2\n12.6\nTry:body→ﬁnalbody\n20.0\n13.0\n37\n7.0\nIfExp:body→orelse\n59.5\n52.5\n2\n7.0\nFor:for→updaters\n53.6\n46.8\n11\n6.7\nDo:body→test\n33.3\n26.7\n81\n6.7\nDo:do→test\n33.3\n26.7\n81\n6.7\nFor:for→test\n95.3\n89.5\n9\n5.8\nIfExp:test→orelse\n45.9\n42.4\n10\n3.5\nWhile:while→body\n91.4\n89.0\n17\n2.4\nIf:if→test\n98.7\n96.5\n2\n2.3\nchildren:parent→child\n44.0\n42.4\n2\n1.6\nInstanceofExpr:expr→type\n93.1\n91.9\n2\n1.1\nFor:initializers→updaters\n47.2\n47.1\n9\n0.1\nSwitch:switch→statement\n100.0\n100.0\n21\n0.0\nFor:for→initializers\n99.4\n100.0\n2\n-0.6\nTry:body→handler\n44.0\n44.7\n25\n-0.7\nSubscript:value→slice\n77.9\n78.8\n2\n-0.9\nSwitch:switch→expr\n99.0\n100.0\n2\n-1.0\nWhile:test→body\n87.4\n89.5\n15\n-2.2\nAttribute:value→attr\n81.2\n83.9\n2\n-2.7\nFor:updaters→body\n96.8\n99.6\n9\n-2.8\nSwitch:expr→statement\n96.6\n100.0\n12\n-3.4\nInﬁxExpr:left→right\n58.2\n61.6\n2\n-3.4\nFor:test→body\n92.3\n96.1\n14\n-3.9\nIf:test→body\n57.4\n62.1\n9\n-4.7\nFor:for→body\n90.3\n94.9\n23\n-4.7\nIf:test→orelse\n42.3\n47.7\n27\n-5.4\nDo:do→body\n93.3\n100.0\n2\n-6.7\nFor:initializers→body\n87.7\n94.9\n21\n-7.2\nCall:func→args\n91.1\n98.7\n2\n-7.6\nFor:initializers→test\n74.5\n90.2\n7\n-15.6\nIfExp:test→body\n25.9\n50.0\n5\n-24.1\nTry:handler→ﬁnalbody\n26.0\n53.2\n20\n-27.3\nTable 12: Attention vs. offset baseline with ﬁxed offset for each relation on Java dataset using any-token metric.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-10-26",
  "updated": "2022-10-26"
}