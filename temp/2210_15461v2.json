{
  "id": "http://arxiv.org/abs/2210.15461v2",
  "title": "LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation",
  "authors": [
    "Hongcheng Guo",
    "Jiaheng Liu",
    "Haoyang Huang",
    "Jian Yang",
    "Zhoujun Li",
    "Dongdong Zhang",
    "Zheng Cui",
    "Furu Wei"
  ],
  "abstract": "Multimodal Machine Translation (MMT) focuses on enhancing text-only\ntranslation with visual features, which has attracted considerable attention\nfrom both natural language processing and computer vision communities. Recent\nadvances still struggle to train a separate model for each language pair, which\nis costly and unaffordable when the number of languages increases in the real\nworld. In other words, the multilingual multimodal machine translation\n(Multilingual MMT) task has not been investigated, which aims to handle the\naforementioned issues by providing a shared semantic space for multiple\nlanguages. Besides, the image modality has no language boundaries, which is\nsuperior to bridging the semantic gap between languages. To this end, we first\npropose the Multilingual MMT task by establishing two new Multilingual MMT\nbenchmark datasets covering seven languages. Then, an effective baseline LVP-M3\nusing visual prompts is proposed to support translations between different\nlanguages, which includes three stages (token encoding, language-aware visual\nprompt generation, and language translation). Extensive experimental results on\nour constructed benchmark datasets demonstrate the effectiveness of LVP-M3\nmethod for Multilingual MMT.",
  "text": "LVP-M3: Language-aware Visual Prompt for Multilingual\nMultimodal Machine Translation\nHongcheng Guo*1, Jiaheng Liu*1, Haoyang Huang2, Jian Yang1,\nZhoujun LiB1, Dongdong Zhang2 , Zheng Cui2, Furu Wei2\n1Beihang University\n2Microsoft Research Asia\n{hongchengguo,liujiaheng,jiaya,lizj}@buaa.edu.cn\n{haohua,dozhang,zhcui,fuwei}@microsoft.com\nAbstract\nMultimodal Machine Translation (MMT) fo-\ncuses on enhancing text-only translation with\nvisual features, which has attracted consider-\nable attention from both natural language pro-\ncessing and computer vision communities. Re-\ncent advances still struggle to train a sepa-\nrate model for each language pair, which is\ncostly and unaffordable when the number of\nlanguages increases in the real world. In other\nwords, the multilingual multimodal machine\ntranslation (Multilingual MMT) task has not\nbeen investigated, which aims to handle the\naforementioned issues by providing a shared\nsemantic space for multiple languages.\nBe-\nsides, the image modality has no language\nboundaries, which is superior to bridging the\nsemantic gap between languages. To this end,\nwe ﬁrst propose the Multilingual MMT task\nby establishing two new Multilingual MMT\nbenchmark datasets covering seven languages.\nThen, an effective baseline LVP-M3 using vi-\nsual prompts is proposed to support transla-\ntions between different languages, which in-\ncludes three stages (token encoding, language-\naware visual prompt generation, and language\ntranslation).\nExtensive experimental results\non our constructed benchmark datasets demon-\nstrate the effectiveness of LVP-M3 method for\nMultilingual MMT.\n1\nIntroduction\nMultimodal Machine Translation (MMT) extends\nthe conventional text-based machine translation\nby taking corresponding images as additional in-\nputs (Lin et al., 2020; Li et al., 2022) to mitigate\nthe problems of data sparsity and ambiguity (Ive\net al., 2019; Yang et al., 2022) when compared\nwith purely text-based machine translation. Simi-\nlar to other multimodal tasks (e.g., visual question\nanswering (Antol et al., 2015; Shih et al., 2016),\nimage captioning (Vinyals et al., 2015; Jia et al.,\n* First two authors contributed equally.\nB Corresponding author.\nTarget Language\nDe: Ein Junge taucht in...\nMMT\nModel (En-De)\nMMT\nModel (En-Fr)\nMMT\nModel (En-Cs)\nTarget Language\nDe: Ein Junge taucht in...\nFr: Un garc¸on plonge...\nCs: Chlapec ska´ce do...\n...\nMultilingual\nMMT Model\nFr: Un garc¸on plonge...\nCs: Chlapec ska´ce do...\nEn: A boy dives into...\nSource Language\nImage\n(a) MMT\n(b) Multilingual MMT\nEn: A boy dives into...\nSource Language\nImage\nFigure 1: Comparison of MMT and Multilingual MMT.\n(a) For MMT, we need to train different MMT mod-\nels to support translations between different language\npairs (e.g., “En-De” represents to translate the English\nto German). (b). For Multilingual MMT, we only need\none single model to translate the source language to dif-\nferent target languages.\n2015) and video-text retrieval (Liu et al., 2022d)),\nMMT aims to exploit the effectiveness of vision\ninformation for the machine translation task.\nMoreover, MMT has broad applications (Zhou\net al., 2018), such as multimedia news and movie\nsubtitles in different languages.\nHowever, as shown in Fig. 1(a), previous MMT\nmodels (e.g., DCCN (Lin et al., 2020)) can handle\na single language translation pair (e.g., English →\nGerman, English →French) well, but training a\nseparate model for each language pair is unafford-\nable considering there are thousands of languages\nin the world. A straightforward solution to reduce\ncomputational cost is to use one model for handling\nthe translations of multiple languages as shown in\nFig. 1(b). Meanwhile, multilingual machine trans-\nlation has been investigated for many years (Con-\nneau et al., 2020), but these existing methods only\nconsider the language as the input, where the vi-\nsion context has been ignored. Therefore, in our\nwork, we ﬁrst propose the Multilingual Multimodal\narXiv:2210.15461v2  [cs.CL]  28 Nov 2022\nMachine Translation (Multilingual MMT) task to\nachieve the translations for multiple languages us-\ning one single model.\nTo eliminate the above limitations, we propose\na simple and effective LVP-M3 method, including\nToken Encoding, Language-aware Visual Prompt\nGeneration (LVPG), and Language Translation.\nSpeciﬁcally, in the token encoding stage, we use\nthe pre-trained vision encoder to extract the visual\ntokens. Then, we follow (Johnson et al., 2017) to\nutilize the Transformer to encode the textual to-\nkens. In LVPG, inspired by (Yang et al., 2019) and\n(Tian et al., 2020), a controller network in Fig. 3 is\nleveraged to dynamically generate the parameters\nof the mapping network conditioned on the target\nlanguage. Further, the mapping network outputs\nthe language-aware visual prompts. After that, dur-\ning the language translation, following the works\n(e.g., ViLBERT (Lu et al., 2019)), we utilize co-\nTransformer to generate the vision-guided language\ntokens. Then the Transformer decoder is adopted\nto predict the translation results.\nExtensive experiments are conducted on our pro-\nposed benchmark datasets for LVP-M3. Results\nshow that our model achieves the state-of-the-art\nperformance in all translation directions, especially\noutperforming the text-only multilingual model by\n4.3 BLEU scores on average.\nThe contributions of this work are summarized\nas follows:\n• We ﬁrst propose the Multilingual Multimodal\nMachine Translation (Multilingual MMT) to\nhandle the translations for multiple language\npairs, which investigates the effect of vision\nmodality for multilingual translation and re-\nduces the computation costs of existing MMT\nmethods for multiple languages.\n• For Multilingual MMT, we propose an effec-\ntive language-aware visual prompt generation\nstrategy to produce different visual prompts\nfor different target languages based on the vi-\nsion modality and type of the target language.\n• We establish two Multilingual MMT bench-\nmark datasets to nourish the further research\non Multilingual MMT, and extensive experi-\nments on these datasets demonstrate the effec-\ntiveness of our proposed LVP-M3 method.\n2\nRelated Works\nMultimodal Machine Translation.\nThe multi-\nmodal context plays a key role in Multimodal Ma-\nchine Translation (MMT). Recent MMT methods\ncan be divided into three categories: (1) Using\nglobal visual features directly (Calixto and Liu,\n2017). For instance, Huang et al. (2016) proposes\nto concatenate global and regional visual features\nwith source sequences. (2) Exploiting visual fea-\ntures via attention scheme (Libovick`y and Helcl,\n2017; Helcl et al., 2018). Calixto et al. (2017) in-\ntroduces the visual features into the MMT model by\nusing an independent attention module. (3) Com-\nbining other vision tasks with the translation task by\nmultitask learning (Calixto et al., 2019; Yin et al.,\n2020). Elliott and Kádár (2017) decomposes mul-\ntimodal translation into two sub-tasks (i.e., transla-\ntion and visual grounding). Recently, (Huang et al.,\n2020) focuses on unsupervised setting for MMT,\nwhich utilizes pseudo visual pivoting and visual\ncontent to improve the cross-lingual alignments\nin latent space. In contrast, LVP-M3 considers\nfully-supervised multilingual setting by mapping\nvision embeddings into different feature spaces and\nachieving the purpose of using one MT model for\nhandling translations of multiple languages. Be-\nsides, reducing computation cost is vital for many\ntasks (Liu et al., 2021, 2022c,a) and we focus on\nthe Multilingual MMT task by using one single\nmodel for efﬁciency.\nMultilingual Language Models. Pre-trained mul-\ntilingual Transformer-based language models (e.g.,\nmBERT (Kenton and Toutanova, 2019) and XLM-\nR (Conneau et al., 2020)) utilize the same pre-\ntraining strategies as their respective monolingual\ncounterparts (e.g., BERT (Kenton and Toutanova,\n2019) and RoBERTa (Liu et al., 2019)). They are\npre-trained via the masked language modeling ob-\njective (MLM) Strategy. Artetxe et al. (2020) pro-\nposes a method to transfer monolingual representa-\ntions to new languages in an unsupervised fashion\nand provide new insights into the generalization\nabilities of multilingual models. Hu et al. (2020)\nintroduces the Cross-lingual Transfer Evaluation\nof Multilingual Encoders (XTREME) benchmark\nto evaluate the cross-lingual generalization capa-\nbilities, Karthikeyan et al. (2020) also provides\na comprehensive study of the contribution of dif-\nferent components in M-BERT to its cross-lingual\nability. Rust et al. (2021) shows that monolingually\nadapted tokenizers can robustly improve the mono-\nlingual performance of multilingual models. Over-\nall, when compared with these methods, we focus\non the multilingual setting for MMT, which has not\nbeen investigated before.\nVision-Language Models. The success of vision-\nlanguage models can be credited to the following\nthree important reasons: Transformers (Liu et al.,\n2022b; Vaswani et al., 2017), contrastive repre-\nsentation learning (Radford et al., 2021; Li et al.,\n2020), and large-scale training datasets (Sharma\net al., 2018; Miech et al., 2019).\nPrevious\nTransformer-based multimodal models ( (Tan and\nBansal, 2019; Chen et al., 2020; Gan et al., 2020;\nBugliarello et al., 2021)) jointly encode text to-\nkens and image region features by preprocessing\nimages using object detection models. The image\nregion features are projected into the joint embed-\nding space of the multimodal Transformer, and\nthen the multi-head attention attends to all text\nand image inputs to learn a joint representation of\nboth modalities. Besides, Kamath et al. (2021)\navoids using object detectors as a black box for\npre-extracting these region features and incorpo-\nrates the object detector end-to-end with the multi-\nmodal Transformer to achieve ﬂexibility and better\nrepresentation capacity. Recently, a representative\napproach CLIP (Radford et al., 2021) is proposed,\nwhich trains two neural network-based encoders\nusing a contrastive loss to match pairs of images\nand texts. After consuming 400 million data pairs,\nthe CLIP model demonstrates a remarkable zero-\nshot image recognition capability, and has been\napplied to many downstream tasks. For example,\nShen et al. (2022) proposes to leverage the CLIP\nmodel for different vision-language models across\nvarious tasks (e.g., image caption, visual question\nanswering). In our work, we aim to investigate\nthe effectiveness of the multimodal information for\nMultilingual MMT.\n3\nDatasets\nWe introduce two Multilingual MMT benchmark\ndatasets (i.e., M3-Multi30K, M3-AmbigCaps) us-\ning Multi30K (Elliott et al., 2016) and Ambig-\nCaps (Li et al., 2021). Here, we descried the details\nof the M3-Multi30K and M3-AmbigCaps.\nData Construction. The widely-used Multi30K\ndataset for the MMT task is based on the Flickr30K\nEntities dataset (Plummer et al., 2017). For each\nimage of Multi30K, one of the English (En) descrip-\ntions is selected in Flickr30K Entities. Currently,\nEn: A child is splashing in the water\nDe: Ein Kind plantscht im Wasser\nCs: Dítě se šplouchá ve vodě\nTr: Bir çocuk suya sıçratıyo\nHi: एक बÅा पानी मU छKटे मार रहा है\nLv: Bērns plunčājas ūdenī\nFr: Un enfant éclabousse dans l'eau\nFigure 2: Example of an image with its descriptions of\nseven different languages.\nLanguage\nISO\nFamily\nSpeakers\nEnglish\nEn\nGermanic\n400M\nGerman\nDe\nGermanic\n95M\nFrench\nFr\nRomance\n250M\nCzech\nCs\nSlavic\n11M\nHindi\nHi\nIndo-Aryan\n800M\nTurkish\nTr\nTurkic\n65M\nLatvian\nLv\nBaltic\n2M\nTable 1:\nLanguages covered by our proposed M3-\nMulti30K and M3-AmbigCaps datasets.\nthe English description of each image is translated\ninto German (De), French (Fr), and Czech (Cs) (El-\nliott et al., 2017; Barrault et al., 2018). To support\nmore languages from different language families\nand various language distributions for Multilingual\nMMT, we extend the existing Multi30K dataset\nwith additional three languages as shown in Table 1,\nwhere one sample of the M3-Multi30K dataset is\nshown in Fig. 2.\nSpeciﬁcally, in the annotation process, based\non the recent state-of-the-art multilingual machine\ntranslation model XLM-R (Conneau et al., 2020),\nwe ﬁrst translate the English description into Hindi\n(Hi), Turkish (Tr), and Latvian (Lv) for each im-\nage in Multi30K. Then, we hire independent native\nspeakers to verify and improve the quality of the\ntranslation results of different languages. In addi-\nDe: Ein Junge taucht in...\nFr: Un garc¸on plonge...\nCs: Chlapec ska´ce do...\n...\nToken Encoding\nVision\nEncoder\nLVPG\nEn: A boy dives into...\nSource language\n(De)\nTransformer\nEncoder\nTarget Language\nController\nCo-TRM\nTransformer\nDecoder\nLanguage Translation\nImage\nTRM\nMapping\nTRM\nFigure 3: The overall framework of our proposed LVP-M3 method for Multilingual MMT task, which includes\nthree stages (i.e., token encoding and language-ware visual prompt generation (LVPG) and language translation).\nHere, we take an example by translating English (En) to German (De). “TRM” and “Co-TRM” represent the\nTransformer and co-Transformer models, respectively.\ntion, as the original AmbigCaps (Li et al., 2021)\ndataset only contains two types of languages, we\nuse a similar way to extend AmbigCaps into addi-\ntional ﬁve languages in M3-AmbigCaps.\nData Splits.\nIn M3-Multi30K, the number of\nimage-translation pairs for training and testing data\nare 29000, 1000, respectively. In M3-AmbigCaps,\nthe number of image-translation pairs for training\nand testing data are 89600, 1000, respectively. We\nwill released these datasets.\n4\nMethod\n4.1\nMultilingual MMT\nSupposing we have M languages {Lm}M\nm=1 and N\nbilingual corpora {Dn}N\nn=1 under the multilingual\nsetting, the dataset Dn consists of K parallel sen-\ntences {(xk\nLi, xk\nLj)}K\nk=1 between language Li and\nLj, where K is the number of training instances\nand each instance has the corresponding image\nzk. Given the corpora, we can train a Multilingual\nMMT model that enables the translation among dif-\nferent languages with the help of image modality.\nThe training objective of the Multilingual MMT is\nlearnt with a combination of different languages:\nLmt = −\nX\ni,j,k\n1ogP(xk\nLi; xk\nLj; zk),\n(1)\nwhere the Multilingual MMT model uses a com-\nplete shared model for all translation directions.\nIn this work, we adopt Transformer as the back-\nbone model for language encoding and pre-trained\nvision branch of the CLIP model (Radford et al.,\n2021) for vision modality. A target language token\ntLj is preﬁxed to each source sentence to indicate\nthe translation direction (Johnson et al., 2017).\n4.2\nLVP-M3\nAs shown in Fig. 3, our proposed LVP-M3 method\nincludes three stages: token encoding, language-\naware visual prompt generation and language trans-\nlation. Speciﬁcally, in training, give each image\nzk, the parallel sentences {(xk\nLi; xk\nLj)} from source\nlanguage Li and target language Lj, and the target\nlanguage token embedding tLj, in token encoding\nstage, we ﬁrst use the vision encoder to extract\nthe visual token features {vm}M\nm=1 based on zk,\nwhere M is the number of visual tokens. Then,\nwe utilize the Transformer encoder to extract the\nsource language tokens {sf}F\nf=1, where F is the\nnumber of language tokens. In language-aware vi-\nsual prompt generation (LVPG) stage, we map the\n{vm}M\nm=1 into the language-aware visual prompt\n{pm}M\nm=1 conditioned on tLj to generate differ-\nent visual prompts for different target languages,\nwhere we propose to adopt the controller network\nto dynamically generate the parameters of the map-\nping network. In language translation stage, we\nﬁrst adopt the co-attention strategy to generate the\nvision-guided language tokens {qf}F\nf=1 based on\n{pm}M\nm=1 and {sf}F\nf=1. Then, we use the {qf}F\nf=1\nas the input of the Transformer decoder to pre-\ndict the translation results and compute the loss in\nEq. 1 using the predicted translation results and the\nground-truth target language xk\nLj.\n4.2.1\nToken Encoding\nFor each image zk, we directly use the vision back-\nbone (e.g., the pre-trained vision branch of the\nwidely-used CLIP model (Radford et al., 2021))\nas the vision encoder to extract the visual tokens\nfor zk as follows:\n{vm}M\nm=1 = H(zk),\n(2)\nwhere H denotes the vision encoder and M is the\nnumber of visual tokens.\nSimilarly, given the source language xk\nLi, based\non the Transformer encoder E, the source language\ntokens {sf}F\nf=1 are extracted as follows:\n{sf}F\nf=1 = E(xk\nLi),\n(3)\nwhere F is deﬁned as the number of source lan-\nguage tokens.\n4.2.2\nLanguage-aware Visual Prompt\nGeneration\nIn language-aware visual prompt generation stage\nof Fig. 3, motivated by recent works (e.g., dy-\nnamic ﬁlter networks (Jia et al., 2016) and Cond-\nConv (Yang et al., 2019)) based on conditional\nconvolutions, where the ﬁlters of conditional con-\nvolutions are conditioned on the input and are dy-\nnamically generated by another network to improve\nthe capacity of the neural network, we extend this\nidea to generate the visual prompt conditioned on\nthe target language type tLj (e.g., German) to map\nthe extracted the visual tokens into different embed-\nding spaces for different target language. Specif-\nically, in Fig. 3, based on the embedding of the\ntarget language token tLj, we utilize a controller\nnetwork C implemented by two fully-connected\nlayers with ReLU (Nair and Hinton, 2010) activa-\ntion function to generate the parameters θ of the\nmapping network M as follows:\nθ = C(tLj).\n(4)\nAfter that, we generate the language-aware visual\nprompt {pm}M\nm=1 as follows:\n{pm}M\nm=1 = M({vm}M\nm=1, θ).\n(5)\nθ is the generated parameters in Eq. 4, which is\nassigned to the mapping network M. In this way,\nwhen translating source language into different tar-\nget languages, the θ will be generated according\nto type of target language tokens, and the visual\ntokens {vm}M\nm=1 can be mapped into different vi-\nsual prompts according to the type of the target\nlanguage.\n4.2.3\nLanguage Translation\nIn Fig. 3, based on the source language to-\nkens {sf}F\nf=1 and language-aware visual prompt\n{pm}M\nm=1, we ﬁrst generate the vision-guided\nsource language tokens based on co-attention strat-\negy, which are widely used for fusing the infor-\nmation from another modality in vision-language\nmodels (Lu et al., 2019). Then, we predict the\ntranslation results using the Transformer decoder.\nSpeciﬁcally, we utilize the Transformer module\nimplemented by self-attention to fuse the informa-\ntion from other tokens within each modality for\n{sf}F\nf=1 and {pm}M\nm=1, respectively, and we repre-\nsent the updated source language tokens and visual\nprompt as S and P, respectively. Then, we take S\nas the query, and the P as the key and value in the\nco-attention module to generate the vision-guided\nsource language tokens {qf}F\nf=1 as follows:\n{qf}F\nf=1 =\nH\r\r\nh=1\nSF\n \nφh\nQ(S)φh\nK(P)⊤\n√\nC\n!\nφh\nV (P),\n(6)\nwhere ∥H\nh=1 is the concatenation of the H atten-\ntive features along the channel dimension. SF rep-\nresents the softmax operation. φh\nQ(·), φh\nK(·) and\nφh\nV (·) are the corresponding linear projection op-\nerations of the h-th head for the query, the key\nand the value, respectively. C denotes the num-\nber of feature channels. After the operation of\nEq. 6, other operations (e.g., FFN, layer normal-\nization (Ba et al., 2016)) of standard attention\nscheme (Vaswani et al., 2017) are used.\nFinally, at inference, based on {qf}F\nf=1, we use\nthe Transformer decoder to predict the target lan-\nguage sequence in our LVP-M3.\n5\nExperiments\nWe evaluate our proposed LVP-M3 method on the\nmultilingual dataset including 7 languages and 6\ntranslation directions. In all experiments, English\n(En) is treated as the pivot language for Multilin-\ngual MMT setting.\n5.1\nExperimental Setting\nImplementation Details. Our implementation is\nbased on the Fairseq (Ott et al., 2019) toolbox.\nWe utilize Sentencepiece tokenizer. The model in\nFig. 3 consists of 6 Transformer encoder/decoder\nlayers. The number of attention heads in all Trans-\nformer layers is set as 12. For training, we take\nthe Adam optimizer (Kingma and Ba, 2015) with\nModel (En→X)\nFr\nCs\nDe\nLv\nHi\nTr\nAvgall\nText-only Multilingual MT Systems\nText Transformer (Fan et al., 2021)\n61.8\n32.8\n40.6\n51.2\n59.0\n53.8\n49.8\nMultilingual MMT Systems\nVision Matters (Gated fusion) (Li et al., 2021)\n62.5\n32.9\n41.2\n52.1\n59.6\n54.2\n50.4\nVision Matters (Concatenation) (Li et al., 2021)\n59.7\n33.1\n39.8\n50.3\n57.6\n51.4\n48.6\nLVP-M3 (w/o LVPG)\n62.2\n33.4\n40.9\n51.6\n59.3\n54.0\n50.2\nLVP-M3 (Our method)\n63.7\n34.6\n43.2\n53.5\n61.4\n55.6\n52.0\nTable 2: The BLEU scores of different methods on M3-Multi30K test set. Five multilingual baselines are compared\nby us. The bottom part shows the results of the multilingual models trained with text and vision modalities. The\nbest results are highlighted.\nModel (En→X)\nFr\nCs\nDe\nLv\nHi\nTr\nAvgall\nText-only Multilingual MT Systems\nText Transformer (Fan et al., 2021)\n62.3\n47.8\n49.0\n46.6\n52.4\n35.9\n49.0\nMultilingual MMT Systems\nVision Matters (Gated fusion) (Li et al., 2021)\n64.3\n50.3\n51.2\n48.5\n54.1\n38.7\n51.2\nVision Matters (Concatenation) (Li et al., 2021)\n62.6\n47.6\n48.7\n45.9\n52.7\n36.0\n48.9\nLVP-M3 (w/o LVPG)\n63.4\n49.2\n50.3\n47.9\n52.4\n37.1\n50.1\nLVP-M3 (Our method)\n65.7\n52.9\n53.7\n51.6\n56.3\n42.7\n53.8\nTable 3: The BLEU scores of different methods on M3-AmbigCaps test set. Five multilingual baselines are\ncompared by us. The bottom part shows the results of the multilingual models trained with text and vision modali-\nties.The best results are highlighted.\nβ1 = 0.9 and β2 = 0.98. The learning rate warms\nup from 1e-7 to 1e-4 in 2000 steps and then de-\ncays based on the inverse square root of the update\nnumber. The maximum number of tokens in each\nmini-batch is 4096. Dropout and label-smoothing\nrate are set as 0.3 and 0.1, respectively. For vision\nencoder, by default, we adopt the vision branch of\nCLIP based on the ViT-L/14 model. The effect of\ndifferent vision backbones is discussed in our ab-\nlation study. All models are trained for 30 epochs\nand evaluated on one single linux machine with 8\nNVIDIA A100 GPUs (80G).\nEvaluation. We compute the cumulative 4-gram\nBLEU scores to evaluate the quality of translation.\nDuring inference, the beam search strategy is per-\nformed with a beam size of 5 for the target sentence\ngeneration. We set the length penalty as 1.0.\nBaseline Methods. As we are the ﬁrst multilin-\ngual method in this area, we reproduce methods\nincluding Text Transformer (Fan et al., 2021), the\nVision Matters (Gated fusion) (Li et al., 2021),\nand the Vision Matters (Concatenation) (Li et al.,\n2021) in the multilingual translation setting for a\nfair comparison. Besides, we also report the results\nof LVP-M3 (w/o LVPG), where we directly adopt\nthe co-attention strategy in Lu et al. (2019) to gen-\nerate the vision-guided language tokens using the\nsource tokens with the visual features.\n5.2\nResults on M3-Multi30K\nTo demonstrate the effectiveness of LVP-M3, we\ncompare our method with baseline methods on\nM3-Multi30K under the multilingual MMT set-\nting in Table 2. It should be mentioned that the\nVision Matters (Gated fusion) and the Vision Mat-\nters (Concatenation) are originally proposed in the\nbilingual setting, and we reproduce these meth-\nods in the multilingual setting for a fair compar-\nison. In Table 2, our LVP-M3 achieves the best\nBLEU scores in all translation directions. Speciﬁ-\ncally, ﬁrst, when compared with text Transformer\nwith only text information, LVP-M3 outperforms\nby +2.2 BLEU scores on average, which demon-\nstrates the effectiveness of visual context for Multi-\nModel (En→X)\nFr\nCs\nDe\nLv\nHi\nTr\nAvgall\nLVP-M3 (Static)\n62.0\n33.1\n41.1\n51.7\n59.6\n54.2\n50.3\nLVP-M3 (LVPG)\n63.7\n34.6\n43.2\n53.5\n61.4\n55.6\n52.0\nTable 4: Comparison of different vision prompt generation methods with BLEU scores.\nModel (En→X)\nFr\nCs\nDe\nLv\nHi\nTr\nAvgall\nLVP-M3+ResNet50\n62.3\n33.3\n41.7\n52.3\n61.1\n54.0\n50.8\nLVP-M3+ResNet101\n62.8\n33.8\n42.1\n52.5\n60.7\n54.2\n51.1\nLVP-M3+ViT-L/14\n63.7\n34.6\n43.2\n53.5\n61.4\n55.6\n52.0\nTable 5: Comparison different visual backbones with BLEU scores.\nlingual MMT. Second, when compared with base-\nline method LVP-M3 (w/o LVPG), LVP-M3 also\nachieves better performance on all settings, which\nveriﬁes the effectiveness of our proposed language-\naware prompt generation module for Multilingual\nMMT. Among all translation directions, the task of\nEn→De achieves the most improvement. Because\nEnglish and German are from the same language\nfamily, both languages can share the similar seman-\ntic knowledge by cross-lingual transfer.\n5.3\nResults on M3-AmbigCaps\nResults of M3-AmbigCaps are presented in Table\n3. When compared with other baseline methods,\nwe observe that our proposed LVP-M3 method also\nachieves signiﬁcant performance improvements in\nall translation directions. In Table 3, we observe\nthat our proposed method LVP-M3 outperforms\nby +4.8 BLEU scores on average when the visual\nmodality is used, which is larger than that in M3-\nMulti30K.\n5.4\nAblation Study\nIn this section, we conduct comprehensive ablation\nstudy to demonstrate the effectiveness of different\ncomponents in our proposed LVP-M3 method on\nthe test set of M3-Multi30K.\nEffect of LVPG.\nIn Table 2 and Table 3, we ob-\nserve that our language-aware visual prompt gener-\nation (LVPG) brings signiﬁcant improvements for\nMultilingual MMT. To demonstrate the effective-\nness of LVPG, we further propose two alternative\nmethods (i.e., LVP-M3 (Static) and LVP-M3 (Co-\nCoOp)) to generate the visual prompts in Table 4.\nSpeciﬁcally, in LVP-M3 (Static), we directly gen-\nerate visual prompts by mapping the visual tokens\n{vm}M\nm=1 using the mapping network, where the\nparameters of the mapping network are static after\ntraining and not conditioned on the target language\ntoken embedding tLj. In Table 4, we observe that\nour LVP-M3 outperforms these alternative methods\na lot, which guides the visual clues to bridge the\nsemantic gap between multiple languages.\nEffect of Different Vision Backbones.\nIn Ta-\nble 5, we compare the results of LVP-M3 by using\nthe visual tokens extracted by different vision back-\nbones (He et al., 2016; Dosovitskiy et al., 2020)\nin CLIP. In Table 5, we observe that our LVP-M3\nachieves best results when using ViT-L/14 as the\nvision encoder. Thus, we use ViT-L/14 as the vi-\nsion encoder by default. Moreover, we observe\nthat the performance is better when the capacity of\nthe vision backbone is better. It is also reasonable\nbecause the quality of the visual tokens is better\nwhen using more powerful vision backbones.\n5.5\nFurther Analysis\nVisualization of Different Masking Ratios.\nAs\nshown in Fig. 4, we compare our LVP-M3 method\nwith the alternative method LVP-M3 (w/o vision)\nto analyze the effectiveness of visual context when\nusing different masking ratios on the source lan-\nguage. Speciﬁcally, in LVP-M3 (w/o vision), we\nonly use the Transformer encoder to process the\nsource language with the target language embed-\nding and then adopt the Transformer decoder to\npredict the target language for multilingual MT,\nwhere the vision encoder and LVPG are both not\nused in LVP-M3 (w/o vision).\nIn Fig. 4, we report the results of these meth-\nods by translating from English (En) to French (Fr)\nand Turkish (Tr). First, when the ratio of masking\nincreases, BLEU scores drop whether the visual\ncontents are added or not, and our LVP-M3 still\n(a) En-Fr\n(b) En-Tr\nFigure 4: Translation results of LVP-M3 under different masking ratios on the source language. Results are\nevaluated on the M3-Multi30K test set by translating English (En) to other languages (i.e., Fr and Tr).\nSRC (En) : A man in a pink shirt is sitting in the grass and a ball is in the air. \n                              SRC (En) with MASK: A [MASK] in a [MASK] shirt is sitting in the grass and a [MASK] is in the air. \nTGT (De): Ein Mann in einem pinken Hemd sitzt auf dem Gras und ein Ball ist in der Luft.\nPRE (De): Ein Mann in einem rosa Hemd sitzt im Gras und ein Ball liegt in der Luft. \nTGT (Fr): Un homme en polo rose est assis dans l'herbe et un ballon est en l'air..\nPRE (Fr): Un homme en chemise rose est assis dans l'herbe et la balle est en l'air.\nFigure 5: A qualitative example by translating English (En) to German (De) and French (Fr) with the help of vision\nmodality. Tokens in red denotes correct translation. Tokens in blue denotes good synonyms, which have the similar\nmeaning with the ground-truth of target language. SRC denotes the source language. MASK means the masked\ncontents in the source language. PRE and TGT represent the predicted translation results and the ground-truth of\nthe target language, respectively.\noutperforms LVP-M3 (w/o vision) a lot. Second,\nthe performance gap between LVP-M3 and LVP-\nM3 (w/o vision) is larger when the mask ratio is\nbetween 20% and 40%, which shows that the vi-\nsual information improves the robustness of the\ntranslation model. Third, when the mask ratio is\nlarger, the results of these methods on all settings\ndegrade. When the mask ratio is set as 80%, the\nresults of LVP-M3 (w/o vision) are close to those\nof LVP-M3. It is also reasonable, because most\ntokens in each source language are masked and it\nis difﬁcult to translate well for both methods under\nthese extreme scenarios.\nQualitative Analysis.\nTo further explore the ne-\ncessity of visual modality for machine translation,\nwe compare the predictions results (i.e., De and\nFr) of a sample source language (i.e., En) with the\nground truth of these target languages in Fig. 5.\nSpeciﬁcally, the “man” (noun), “pink” (adjective),\nand “ball” (noun) are masked, and these masked\ntokens describe the saliency regions in the corre-\nsponding left image. We have the following ob-\nservations. First, we observe that even though\nthe “man” is masked, the prediction results of Ger-\nman and French on this token are still right, which\nmeans that visual modality is complementary rather\nthan redundant if the text is insufﬁcient. Second,\nour model translates some tokens to their synonyms\nin the target language. For example, although the\ntranslated word “rosa” in German is evaluated as a\nbad translation for the masked token “pink” in En-\nglish, it represents the same meaning as the word\n“pinken” in German. Besides, “la balle” in French\nis also the synonym of “ball” in English, which\nfurther demonstrates the effectiveness of the vision\nmodality.\n5.6\nDiscussion on LVP-M3\nIn our proposed LVP-M3 method, ﬁrst, both en-\ncoders (vision and text) and decoder are shared\nfor all language pairs, while previous methods on\nMMT usually adopt different models for different\nlanguage pairs. Second, to generate different vi-\nsual prompts for different language pairs with min-\nimal additional parameters, we just use controller\nnetwork to generate the parameters of mapping\nnetwork to map the vision embeddings. Third, dif-\nferent language translation directions are used in\ntraining, where the target language token is also\npreﬁxed to each source sentence for denoting the\ntranslation direction. Last, training separated mod-\nels will result in huge training costs when compared\nwith the multilingual models as discussed in many\nmultilingual methods.\n6\nConclusion\nIn our work, we ﬁrst propose the Multilingual\nMMT task to support the multilingual multimodal\nmachine translations between different language\npairs using one single model. Then, we propose an\neffective LVP-M3 baseline method for the Multilin-\ngual MMT task, where a language-aware prompt\ngeneration module is proposed to generate visual\nprompts for different target languages dynamically.\nComprehensive experimental results on our es-\ntablished Multilingual MMT benchmark datasets\ndemonstrate the effectiveness of our proposed LVP-\nM3 method for Multilingual MMT.\n7\nLimitations\nAlthough our proposed LVP-M3 method has\nachieved substantial improvements for Multilin-\ngual MMT, we ﬁnd that there still exists some\nhyper-parameters (e.g., the number of encoder and\ndecoder layers,) to tune for better results, which\nmay be time-consuming. Besides, in our estab-\nlished datasets, we support seven languages cur-\nrently, and we will extend to support more lan-\nguages and more translation directions for Multi-\nlingual MMT in the future work.\nAcknowledgments\nThis work was supported in part by the National\nNatural Science Foundation of China (Grant Nos.\n62276017, U1636211, 61672081), the 2022 Ten-\ncent Big Travel Rhino-Bird Special Research Pro-\ngram, and the Fund of the State Key Laboratory\nof Software Development Environment (Grant No.\nSKLSDE-2021ZX-18).\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. 2015.\nVqa: Visual question an-\nswering. In ICCV 2015, pages 2425–2433.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In ACL 2020, pages 4623–\n4637.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016.\nLayer normalization.\narXiv preprint\narXiv:1607.06450.\nLoïc Barrault, Fethi Bougares, Lucia Specia, Chiraag\nLala, Desmond Elliott, and Stella Frank. 2018. Find-\nings of the third shared task on multimodal machine\ntranslation. In WMT 2018, pages 304–323.\nEmanuele\nBugliarello,\nRyan\nCotterell,\nNaoaki\nOkazaki, and Desmond Elliott. 2021. Multimodal\npretraining unmasked:\nA meta-analysis and a\nuniﬁed framework of vision-and-language berts.\nTransactions of the Association for Computational\nLinguistics, 9:978–994.\nIacer Calixto and Qun Liu. 2017. Incorporating global\nvisual features into attention-based neural machine\ntranslation. In EMNLP 2017, pages 992–1003.\nIacer Calixto, Qun Liu, and Nick Campbell. 2017.\nDoubly-attentive decoder for multi-modal neural\nmachine translation.\nIn ACL 2017, pages 1913–\n1924.\nIacer Calixto, Miguel Rios, and Wilker Aziz. 2019. La-\ntent variable model for multi-modal translation. In\nACL 2019, pages 6392–6405.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020.\nUniter: Universal image-text\nrepresentation learning. In ECCV 2020, pages 104–\n120. Springer.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nIn\nACL 2020.\nAlexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk\nWeissenborn,\nXiaohua\nZhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers for\nimage recognition at scale. In ICLR 2020.\nDesmond Elliott, Stella Frank, Loïc Barrault, Fethi\nBougares, and Lucia Specia. 2017. Findings of the\nsecond shared task on multimodal machine transla-\ntion and multilingual image description. In WMT\n2017, pages 215–233.\nDesmond Elliott, Stella Frank, Khalil Sima’an, and Lu-\ncia Specia. 2016. Multi30K: Multilingual English-\nGerman image descriptions. In Proceedings of the\n5th Workshop on Vision and Language, pages 70–\n74.\nDesmond Elliott and Ákos Kádár. 2017. Imagination\nimproves multimodal translation. In IJCNLP 2017,\npages 130–141.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, Naman Goyal, Tom Birch, Vitaliy\nLiptchinsky, Sergey Edunov, Michael Auli, and Ar-\nmand Joulin. 2021. Beyond english-centric multi-\nlingual machine translation. J. Mach. Learn. Res.,\n22:107:1–107:48.\nZhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu,\nYu Cheng, and Jingjing Liu. 2020. Large-scale ad-\nversarial training for vision-and-language represen-\ntation learning.\nAdvances in Neural Information\nProcessing Systems, 33:6616–6628.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In CVPR 2016, pages 770–778.\nJindˇrich Helcl, Jindˇrich Libovick`y, and Dusan Varis.\n2018. Cuni system for the wmt18 multimodal trans-\nlation task. In WMT 2018, pages 616–623.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham\nNeubig, Orhan Firat, and Melvin Johnson. 2020.\nXtreme: A massively multilingual multi-task bench-\nmark for evaluating cross-lingual generalisation. In\nICML 2020, pages 4411–4421. PMLR.\nPo-Yao Huang, Junjie Hu, Xiaojun Chang, and Alexan-\nder Hauptmann. 2020.\nUnsupervised multimodal\nneural machine translation with pseudo visual piv-\noting.\nIn Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 8226–8237, Online. Association for Computa-\ntional Linguistics.\nPo-Yao Huang, Frederick Liu, Sz-Rung Shiang, Jean\nOh, and Chris Dyer. 2016. Attention-based multi-\nmodal neural machine translation.\nIn WMT 2016,\npages 639–645.\nJulia Ive, Pranava Madhyastha, and Lucia Specia. 2019.\nDistilling translations with visual awareness.\nIn\nACL 2019, pages 6525–6538.\nXu Jia, Bert De Brabandere, Tinne Tuytelaars, and\nLuc V Gool. 2016. Dynamic ﬁlter networks. Ad-\nvances in neural information processing systems, 29.\nXu Jia, Efstratios Gavves, Basura Fernando, and Tinne\nTuytelaars. 2015. Guiding the long-short term mem-\nory model for image caption generation. In ICCV\n2015, pages 2407–2415.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\net al. 2017. Google’s multilingual neural machine\ntranslation system: Enabling zero-shot translation.\nACL 2017, 5:339–351.\nAishwarya Kamath, Mannat Singh, Yann LeCun,\nGabriel Synnaeve, Ishan Misra, and Nicolas Carion.\n2021.\nMdetr-modulated detection for end-to-end\nmulti-modal understanding. In ICCV 2021, pages\n1780–1790.\nK Karthikeyan, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multilin-\ngual bert: An empirical study. In ICLR 2020.\nJacob\nDevlin\nMing-Wei\nChang\nKenton\nand\nLee Kristina Toutanova. 2019.\nBert: Pre-training\nof deep bidirectional transformers for language\nunderstanding.\nIn NAACL-HLT 2019,\npages\n4171–4186.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR 2015.\nBei Li, Chuanhao Lv, Zefan Zhou, Tao Zhou, Tong\nXiao, Anxiang Ma, and JingBo Zhu. 2022. On vi-\nsion features in multimodal machine translation. In\nACL 2022, pages 6327–6337.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and\nDaxin Jiang. 2020.\nUnicoder-vl: A universal en-\ncoder for vision and language by cross-modal pre-\ntraining. In AAAI 2020, volume 34, pages 11336–\n11344.\nJiaoda Li, Duygu Ataman, and Rico Sennrich. 2021.\nVision matters when it should: Sanity checking mul-\ntimodal machine translation models.\nIn EMNLP\n2021, pages 8556–8562.\nJindˇrich Libovick`y and Jindˇrich Helcl. 2017. Attention\nstrategies for multi-source sequence-to-sequence\nlearning. In ACL 2017, pages 196–202.\nHuan Lin, Fandong Meng, Jinsong Su, Yongjing Yin,\nZhengyuan Yang, Yubin Ge, Jie Zhou, and Jiebo\nLuo. 2020.\nDynamic context-guided capsule net-\nwork for multimodal machine translation. In ACM\nMM 2020, pages 1320–1329.\nJiaheng Liu, Jinyang Guo, and Dong Xu. 2022a. Ap-\nsnet: Toward adaptive point sampling for efﬁcient\n3d action recognition. IEEE Transactions on Image\nProcessing, 31:5287–5302.\nJiaheng Liu, Jinyang Guo, and Dong Xu. 2022b.\nGeometrymotion-transformer:\nAn\nend-to-end\nframework for 3d action recognition.\nIEEE\nTransactions on Multimedia, pages 1–13.\nJiaheng Liu, Haoyu Qin, Yichao Wu, Jinyang Guo,\nDing Liang, and Ke Xu. 2022c. Coupleface: Rela-\ntion matters for face recognition distillation. In Pro-\nceedings of the European Conference on Computer\nVision.\nJiaheng Liu, Tan Yu, Hanyu Peng, Mingming Sun, and\nPing Li. 2022d. Cross-lingual cross-modal consol-\nidation for effective multilingual video corpus mo-\nment retrieval. In NAACL 2022, pages 1854–1862.\nJiaheng Liu, Shunfeng Zhou, Yichao Wu, Ken Chen,\nWanli Ouyang, and Dong Xu. 2021. Block proposal\nneural architecture search.\nIEEE Transactions on\nImage Processing, 30:15–25.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. Advances in neural information processing\nsystems, 32.\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand Tapaswi, Ivan Laptev, and Josef Sivic.\n2019. Howto100m: Learning a text-video embed-\nding by watching hundred million narrated video\nclips. ICCV 2019, pages 2630–2640.\nVinod Nair and Geoffrey E Hinton. 2010.\nRectiﬁed\nlinear units improve restricted boltzmann machines.\nIn ICML 2010.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019.\nfairseq: A fast, extensible\ntoolkit for sequence modeling.\nIn NAACL-HLT\n2019, pages 48–53.\nBryan A. Plummer, Liwei Wang, Christopher M. Cer-\nvantes, Juan C. Caicedo, Julia Hockenmaier, and\nSvetlana Lazebnik. 2017. Flickr30k entities: Col-\nlecting region-to-phrase correspondences for richer\nimage-to-sentence models. IJCV 2017, 123:74–93.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021.\nLearning transferable visual models\nfrom natural language supervision. In ICML 2021,\npages 8748–8763.\nPhillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian\nRuder, and Iryna Gurevych. 2021. How good is your\ntokenizer? on the monolingual performance of mul-\ntilingual language models. In ACL 2021.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018.\nConceptual captions:\nA\ncleaned, hypernymed, image alt-text dataset for auto-\nmatic image captioning. In ACL 2018, pages 2556–\n2565.\nSheng Shen, Liunian Harold Li, Hao Tan, Mohit\nBansal, Anna Rohrbach, Kai-Wei Chang, Zhewei\nYao, and Kurt Keutzer. 2022. How much can CLIP\nbeneﬁt vision-and-language tasks? In ICLR 2022.\nKevin J Shih, Saurabh Singh, and Derek Hoiem. 2016.\nWhere to look: Focus regions for visual question an-\nswering. In CVPR 2016, pages 4613–4621.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers.\nIn EMNLP-IJCNLP 2019, pages 5100–\n5111.\nZhi Tian, Chunhua Shen, and Hao Chen. 2020. Con-\nditional convolutions for instance segmentation. In\nECCV 2020, pages 282–298. Springer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS 2017.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and\nDumitru Erhan. 2015. Show and tell: A neural im-\nage caption generator. In CVPR 2015, pages 3156–\n3164.\nBrandon Yang, Gabriel Bender, Quoc V Le, and Jiquan\nNgiam. 2019. Condconv: Conditionally parameter-\nized convolutions for efﬁcient inference. Advances\nin Neural Information Processing Systems, 32.\nJian Yang, Yuwei Yin, Shuming Ma, Dongdong Zhang,\nShuangzhi Wu, Hongcheng Guo, Zhoujun Li, and\nFuru Wei. 2022. UM4: uniﬁed multilingual multi-\nple teacher-student model for zero-resource neural\nmachine translation.\nIn IJCAI 2022, pages 4454–\n4460.\nYongjing Yin, Fandong Meng, Jinsong Su, Chulun\nZhou, Zhengyuan Yang, Jie Zhou, and Jiebo Luo.\n2020. A novel graph-based multi-modal fusion en-\ncoder for neural machine translation. In ACL 2020,\npages 3025–3035.\nMingyang Zhou, Runxiang Cheng, Yong Jae Lee, and\nZhou Yu. 2018. A visual attention grounding neu-\nral model for multimodal machine translation.\nIn\nEMNLP 2018, pages 3643–3653.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2022-10-19",
  "updated": "2022-11-28"
}