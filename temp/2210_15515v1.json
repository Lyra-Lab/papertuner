{
  "id": "http://arxiv.org/abs/2210.15515v1",
  "title": "Meta-Reinforcement Learning Using Model Parameters",
  "authors": [
    "Gabriel Hartmann",
    "Amos Azaria"
  ],
  "abstract": "In meta-reinforcement learning, an agent is trained in multiple different\nenvironments and attempts to learn a meta-policy that can efficiently adapt to\na new environment. This paper presents RAMP, a Reinforcement learning Agent\nusing Model Parameters that utilizes the idea that a neural network trained to\npredict environment dynamics encapsulates the environment information. RAMP is\nconstructed in two phases: in the first phase, a multi-environment\nparameterized dynamic model is learned. In the second phase, the model\nparameters of the dynamic model are used as context for the multi-environment\npolicy of the model-free reinforcement learning agent.",
  "text": "Meta-Reinforcement Learning Using Model Parameters\nGabriel Hartmann1,2 and Amos Azaria2\nAbstract— In\nmeta-reinforcement\nlearning,\nan\nagent\nis\ntrained in multiple different environments and attempts to learn\na meta-policy that can efﬁciently adapt to a new environment.\nThis paper presents RAMP, a\nReinforcement learning Agent\nusing Model Parameters that utilizes the idea that a neural\nnetwork trained to predict environment dynamics encapsulates\nthe environment information. RAMP is constructed in two\nphases: in the ﬁrst phase, a multi-environment parameterized\ndynamic model is learned. In the second phase, the model\nparameters of the dynamic model are used as context for the\nmulti-environment policy of the model-free reinforcement learn-\ning agent. We show the performance of our novel method in\nsimulated experiments and compare them to existing methods.\nI. INTRODUCTION\nCommon approaches for developing controllers do not rely\non machine learning. Instead, engineers manually construct\nthe controller based on general information about the world\nand the problem. After repetitively testing the controller in\nthe environment, the engineer improves the controller based\non the feedback from these tests. That is, a human is an\nessential part of this iterative process. Reinforcement Learn-\ning (RL) reduces human effort by automatically learning\nfrom interaction with the environment. Instead of explicitly\ndesigning and improving a controller, the engineer develops\na general RL agent that learns to improve the controller’s\nperformance without human intervention. The RL agent is\nusually general and does not include speciﬁc information\nabout the target environment; this allows it to adapt to dif-\nferent environments. Indeed, RL agents may achieve higher\nperformance compared to human-crafted controllers [1]–[3].\nHowever, RL agents usually require training from the ground\nup for every new environment, which requires extensive\ninteraction in the new environment.\nOne solution to speed up the training time is to explicitly\nprovide human-crafted information about the environment\n(context) to the RL agent [4]. However, such a solution\nrequires explicitly analyzing the target environment, which\nmay be challenging and time-consuming.\nInstead of relying on the human understanding of the\nproblem for providing such context, a meta-Reinforcement\nLearning (meta-RL) agent can learn to extract a proper envi-\nronmental context. To that end, a meta-RL agent is trained on\nextended interaction in multiple different environments, and\nthen, after a short interaction in a new, unseen environment, it\nThis research was supported, in part, by the Ministry of Science &\nTechnology, Israel.\n1 Department of Mechanical Engineering and Mechatronics, Ariel Uni-\nversity, Israel\n2 Department of Computer Science, Ariel University, Israel\ngabrielh@ariel.ac.il, amos.azaria@ariel.ac.il\nis required to perform well in it [5], [6]. Speciﬁcally, a meta-\nRL algorithm that is based on context extraction is composed\nof two phases. First, in the meta-learning phase, the agent\nlearns a general policy suitable to all environments given a\ncontext. Additionally, in this phase, the meta-RL agent learns\nhow to extract a context from samples obtained from an\nenvironment. Secondly, in the adaptation phase, the meta-RL\nagent conducts a short interaction in the new environment,\nand the context is extracted from it. This context is then fed\nto the general policy, which acts in the new environment.\nOne common approach for context extraction is using\na Recurrent Neural Network (RNN). That is, the RNN\nreceives the history of the states, actions, and rewards and\nis trained to output a context that is useful for the general\npolicy. However, the RNN long-term memory capability\nusually limits the effective history length [7]. Additionally,\nsince the context vector is not explicitly explainable, it is\ndifﬁcult to examine the learning process and understand if\nthe RNN learned to extract the representative properties of\nthe environments.\nIn this paper, we introduce RAMP – a Reinforcement\nlearning Agent using Model Parameters. We utilize the idea\nthat a neural network trained to predict environment dynam-\nics encapsulates the environment properties; therefore, its pa-\nrameters can be used as the context for the policy. During the\nmeta-RL phase, RAMP learns a neural network that predicts\nthe environment dynamic for each environment. However,\nsince the number of the neural network’s parameters is\nusually high, it is challenging for the policy to use the entire\nset of parameters as its context. Therefore, the majority of\nthe model’s parameters are shared between all environments,\nand only a small set of parameters are trained separately\nin each environment. In that way, the environment-speciﬁc\nparameters represent the speciﬁc environment properties.\nConsequently, a general policy uses only these parameters as\ncontext and outputs actions that are suitable for that particular\nenvironment. One advantage of RAMP is that the history\nlength used for the context extraction is not limited because\nthe context is extracted from a global dynamic model.\nAdditionally, the combination of model learning and RL in\nRAMP makes the training process more transparent since it\nis possible to evaluate the performance of the model learning\nprocess independently. We demonstrate the effectiveness of\nRAMP in several simulated experiments in Sec. V.\nTo summarize, the contributions of this paper are:\n• Suggesting a novel method for meta-reinforcement\nlearning.\n• Presenting a multi-environment dynamic model learning\nmethod that adapts to new environments by updating\narXiv:2210.15515v1  [cs.LG]  27 Oct 2022\nonly a few parameters.\n• Using the dynamic model parameters directly as a\ncontext for the general policy.\n• Combining model-based and model-free RL.\nII. RELATED WORK\nRL has shown success in numerous domains, such as\nplaying Atari games [1], [8], playing Go [9], and driving\nautonomous vehicles [3], [10]. Some are designed for one\nspeciﬁc environment [11], [12], while others can learn to\nmaster multiple environments [2], [8]; however, many algo-\nrithms require separate training for each environment.\nSeveral approaches were proposed to mitigate the need\nfor long training times by using meta-RL methods. We\nbegin by describing methods that, similarly to ours, learn\na context-conditioned, general policy. However, they con-\nstructed the context vector in different ways. We note that\nsome previous works term the different training environments\n“tasks\" since they emphasize the changes in the reward\nfunction. However, since our work focuses on environments\nwith different dynamics (transition functions), we use the\nterm “environments”. In [13], the environment properties\nare predicted by a neural network based on a ﬁxed, small\nnumber of steps. However, this approach requires explicitly\ndeﬁning the representative environment properties. More-\nover, it assumes that these properties can be estimated based\non the immediate environmental dynamics. Rasool et al.\n[14] introduce TD3-context, a TD3-based RL agent that\nuses a recurrent neural network (RNN) to create a context\nvector, which receives the recent states and rewards as input.\nHowever, even though types of RNNs such as LSTM [15]\nand GRU [16] are designed for long-term history, in practice,\nthe number of previous states considered by the RNN is\nlimited [7]. Therefore, if an event that deﬁnes an environment\noccurs too early, the RNN will “forget\" it and not provide\nan accurate context to the policy. In our method, RAMP,\nthe context consists of the parameters of a global, dynamic\nmodel, which is not limited by the history length. Other\napproaches use the RNN directly as a policy, based on the\ntransitions and rewards during the previous episode [6], [17],\ninstead of creating a context vector for a general policy.\nThese approaches are also vulnerable to this RNN memory\nlimitation.\nFinn et al. [5] proposed a different principle for meta-\nlearning termed “Model-Agnostic Meta-Learning (MAML).\"\nIn MAML, the neural network parameters are trained such\nthat the model will be adapted to a new environment by\nupdating all parameters only with a low number of gradient-\ndescent steps. However, the training process of MAML\nmay be challenging [18]. Furthermore, MAML uses on-\npolicy RL and therefore is unsuitable for the more sampling-\nefﬁcient off-policy methods as in our approach. Nevertheless,\nsince MAML can also be used for regression, we compare\nour multi-environment dynamic model learning method to\nMAML in Sec. V-A.\nSome proposed meta-RL methods are suitable for off-\npolicy learning [14], [19]. Meta-Q-learning (MQL) [14]\nupdates the policy to new environments by using data from\nmultiple previous environments stored in the replay buffer.\nThe transitions from the replay buffer are reweighed to match\nthe current environment. We compare our method, RAMP,\nto MQL in our testing environment in Sec. V-B.2.\nAs opposed to all these meta-RL methods, which are\nmodel-free, also model-based meta-RL methods were pro-\nposed. In model-based meta-RL, the agent learns a model\nthat can quickly adapt to the dynamics of a new environment.\nIgnasi et al. [20] propose to use recurrence-based or gradient-\nbased (MAML) online adaptation for learning the model.\nSimilarly, Lee et al. [21] train a model that is conditioned on\nthe encoded, previous transitions. In contrast to model-free\nRL, which learns a direct mapping (i.e., a policy) between\nthe state and actions, model-based RL computes the actions\nby planning (using a model-predictive controller) based on\nthe learned model. In our work, we combine the model-free\nand model-based approaches resulting in rapid learning of\nthe environment dynamic model and a direct policy without\nthe need for planning.\nIII. PROBLEM DEFINITION\nWe consider a set of N environments that are modeled\nas a Markov Decision Processes Mk = {S, A, T k, R},\nk = {1, . . . , N}. All environments share the same state space\nS, action space A, and reward function R and differ only by\ntheir unknown transition function T . These N environments\nare randomly split into training environments Mtrain and\ntesting environments Mtest.\nThe meta-RL agent is trained on the Mtrain environments\nand must adapt separately to each of the Mtest environments.\nThat is, the agent is permitted to interact with the Mtrain\nenvironments for an unlimited number of episodes. Then,\nthe meta-RL agent is given only a short opportunity to\ninteract with each of the Mtest environments (e.g., a single\nepisode, a number of time steps, etc.), and update its policy\nbased on this interaction. Overall, the agent’s goal is to\nmaximize the average expected discounted for each of the\nMtest environments.\nIV. RAMP\nRAMP is constructed in two phases: in the ﬁrst phase,\na multi-environment dynamic model is learned, and in the\nsecond phase, the model parameters of the dynamic model\nare used as context for the multi-environment policy of\nthe reinforcement learning agent. The following sections\nﬁrst describe how the multi-environment dynamic model is\nlearned by exploiting the environments’ common structure.\nIn the second part, we describe the reinforcement learning\nagent.\nA. Multi-Environment Dynamic Model\nAttempting to approximate the transition function of each\nenvironment T k by an individual neural network is likely\nto work well for the training environments. However, it is\nunlikely to generalize to the testing environments, as we have\nonly a limited set of data points for them. However, since\nthe environments share a common structure, it will be more\nefﬁcient to train a neural network that has shared components\nbetween all the environments. Namely, we intend to train a\ngeneral neural network based on the training environments\nsuch that it can be adapted to each testing environment using\nonly a limited set of data points.\nIn addition, since RAMP’s second phase uses the neural\nnetwork’s parameters’ values directly as a context for the RL\nagent, we wish to use only a small number of parameters that\nshould represent the properties of each speciﬁc environment\ndynamics. Therefore, the general neural network shares the\nvast part of the parameters between all environments and\nincludes only a small set of environment-speciﬁc parameters.\nThe environment-speciﬁc parameters are, in fact, a compact\nrepresentation of each environment; therefore, they can be\nused by RAMP as a context vector (as described in Sec.\nIV-B).\nWe approximate the transition function of all environments\nby a neural network with parameters indexed by ϕ, which\nare split to environment-speciﬁc parameters indexes ω ⊆ϕ,\nand to the remaining parameters indexes σ = ϕ \\ ω. The\nvalues of the parameters of each environment k are denoted\nby ˆϕk and the environment-speciﬁc parameters’ values by\nˆωk. The shared parameters’ values, which do not depend\non a speciﬁc environment, are denoted by ˆσ. Our multi-\nenvironment dynamic model is denoted by fˆσ,ˆωk. The multi-\nenvironment dynamic model is given a state s and action a\nand outputs a prediction of the state at the following time\nstep s′ for each environment Mk, i.e., s′ = fˆσ,ˆωk(s, a).\nWe now describe how to select ω and how to train the\nneural network parameters ˆσ and ˆωk. At ﬁrst, we gather\nsufﬁcient data in the form of Dk = {(s, a, s′)}, for each\nenvironment k. At the beginning of the training process\nω = ∅and σ = ϕ. The network is trained using the\ngradient descent algorithm to minimize the loss, which is\nthe squared error between the predicted and real next state\nfor each environment:\nL(Dk, ˆσ, ˆωk) =\nX\ns,a,s′∈Dk\n(s′ −fˆσ,ˆωk(s, a))2.\n(1)\nInitially, ˆσ are trained in all environments to achieve an\naverage model prediction:\nˆσ = arg min\nˆσ\n|Mtrain|\nX\nk=1\nL(Dk, ˆσ, ˆωk)\n(2)\nAfter the initial training phase, the parameters ω are\nselected from ϕ by the algorithm, one-at-a-time. Intuitively,\nthe algorithm should select parameters for ω that have the\ngreatest impact on the difference between the environments.\nTherefore, at each gradient step, the gradient of the loss\nfunction L(Dk, ˆσ, ˆωk) relative to ˆϕk is computed for each\nenvironment k:\ngk = ∇ˆϕkL(Dk, ˆσ, ˆωk),\n(3)\nand the parameter with the highest variance between all\ngradients gk is added to ω:\nω ←ω ∪arg max\ni∈ϕ\\ω\nvar(g0\ni , g1\ni , . . . , g|Mtrain|\ni\n).\n(4)\nThen, the network is trained to minimize the loss function\nin all environments:\nmin\nˆσ\n|Mtrain|\nX\nk=1\nmin\nˆωk L(Dk, ˆσ, ˆωk).\n(5)\nThat is achieved by updating the environment-speciﬁc pa-\nrameters ˆωk by the corresponding gradient:\nˆωk ←ˆωk −αωgk,\n(6)\nand updating the shared parameters by the average gradient:\nˆσ ←ˆσ −ασ\n1\n|Mtrain|\n|Mtrain|\nX\ni=0\ngi,\n(7)\nwhere αω and ασ are the learning rates. During the training,\nparameters continue to be added to |ω| until it reaches\na predeﬁned size nω. Algorithm 1 summarizes the multi-\nenvironment dynamic model learning.\nFinally, at the end of the training process (after achieving\na low loss value), only parameters ω need to be adjusted for\na new environment to get an accurate dynamic model, while\nparameters σ remain constant. That is,\nˆωk = arg min\nˆωk\nL(Dk, ˆσ, ˆωk).\n(8)\nAlgorithm 1 Model learning with RAMP\nRequire: nω ▷Number of environment-speciﬁc parameters\nRequire: ninit\n▷Number of steps for initial training\nRequire: ntot\n▷Number of total training steps\nRequire: αω, ασ\n▷Learning rates\nRequire: {D0, . . . , D|Mtrain|} ▷Data from the environments\nω ←∅\nfor i ←1,number of training steps do\nfor Mk ∈Mtrain do\nsample a batch of transitions bk ∈Dk\ngk ←∇ˆϕkL(bk, ˆσ, ˆωk)\nˆωk ←ˆωk −αωgk\nˆσ ←ˆσ −ασ · avg(g0, . . . , g|Mtrain|)\nif |ω| ≤nω and i > ninit then\nω ←ω ∪arg maxi∈ϕ\\ω var(g0\ni , g1\ni , . . . , g|Mtrain|\ni\n)\nB. Reinforcement Learning With Model Parameters Context\nThe multi-environment dynamic model parameters, de-\nscribed in the previous section, are used as a context for\nthe RL agent. That is, RAMP concatenates the environment-\nspeciﬁc parameters’ values ˆωk to the state s for training the\nRL agent.\nUnfortunately, the environment-speciﬁc parameters ˆωk do\nnot necessarily converge to the same value when trained in\nthe same environment since the amount of these parameters\nmay be greater than the degree of freedom between the envi-\nronments. That is, there may be more than one way (i.e., sin-\ngle parameters’ values) to minimize the multi-environment\ndynamic model network. Therefore, the RL agent should be\ntrained on multiple possible representations of each environ-\nment. To achieve this, the environment-speciﬁc parameters’\nvalues ˆωk are retrained every H episodes for each environ-\nment k by collecting data from a single episode with the\ncurrent policy. These values are stored in Ωk. The shared\nparameters, ˆσ, remain constant during the entire RL multi-\nenvironment training phase.\nIf the RL algorithm uses a replay buffer, If the RL\nalgorithm uses a replay buffer, the current environment index\nk is added to each tuple in addition to the standard data\nstored in the replay buffer (i.e., state, action, next state,\nreward, and done). When sampling from the replay buffer,\na context vector is concatenated to each state according to\nthe tuple’s environment index k. That context vector, which\nis the values of the environment-speciﬁc parameters ˆωk, is\nrandomly sampled from Ωk.\nWe note that RAMP can be used with any RL algorithm\nand also supports off-policy algorithms, which are considered\nto be more efﬁcient. In this work, we use TD3 [22], which\nis an off-policy, actor-critic RL algorithm. The TD3 agent\ncontains critic neural networks that estimate the action-value\nfunction. The critic is trained by minimizing the Bellman\nfunction. The actor, which is a policy represented by a neural\nnetwork, aims to maximize the expected discounted inﬁnite\nepisode reward by maximizing the action-value function.\nRAMP using the TD3 algorithm is summarized in 2.\nV. EXPERIMENTAL EVALUATION\nWe evaluate RAMP on two domains. The ﬁrst domain, a\nsine waves regression test, evaluates the ﬁrst phase of RAMP\nalone, i.e., the multi-environment dynamic model learning\nalgorithm. The second domain is the vehicle target-reaching\ndomain, in which vehicles with different dynamics aim to\nreach a target. The vehicle target-reaching domain tests the\ncomplete RAMP algorithm, composed of both phases.\nA. Sine Waves Regression\nWe used a sine waves regression test similar to [5]. The\nmulti-environment dynamic model was trained on random\nsamples of a sine wave function with different amplitudes A\nand phases φ:\ny = A sin (x + φ).\n(9)\nThe input to the function, x, is sampled uniformly from the\nrange [−5, 5]. The amplitudes of the different functions, are\nsampled from A ∈[0.1, 5], and the phases are sampled from\nφ ∈[0, π]. The network consists of two fully connected\nhidden layers, with 40 neurons in each layer and ReLU\nactivation. The size of the environment-speciﬁc parameters is\nlimited to 10, i.e. nω = 10, out of a total 1761 parameters.\nContrary to the dynamic model prediction, which receives\nan action in addition to the current state to predict the next\nstate, in this simple sine regression problem, there is a single\ninput and a single output. The multi-environment model\nAlgorithm 2 RAMP (using TD3)\nRequire: ntot\n▷Number training steps\nRequire: Mtrain\n▷Training environments\nRequire: ˆωk, k = {1, |Mtrain|}\nInitialize critic, actor, and replay buffer B\nAdd ˆωk to Ωk for all k = {1, |Mtrain|}\nwhile i < ntot do\nSelect environment Mk from Mtrain randomly\nSelect parameters ˆωk from Ωk randomly\nwhile not done do\nObserve s\nExecute action a = πΦ(s, ˆωk) + ϵ), ϵ ∼N(0, σ)\nObserve new state s′, reward r and done ﬂag d\nAdd (s, a, s′, r, d, k) to replay buffer B\nif i mod H = 0 then\nfor all Mk ∈Mtrain do\nSelect random parameters ˆωk from Ωk\nSample one episode from Mk:\nDk ←{(st, π(st, ˆωk), st+1)}t={1,T }\nRetrain ˆωk\nnew with Dk and add to Ωk\nSample random batch b ⊂B\nfor all (s, a, s′, r, d, k) ∈b do\nSample ˆωk from Ωk\nSet s′ ←(s′, ˆωk)\nSet s ←(s, ˆωk)\nUpdate critics using b\nif i mod ntraining = 0 then\nUpdate actor using b\ni ←i + 1\nwas trained on 100 random sine waves with 10 samples\neach. It was then retrained by updating only environment-\nspeciﬁc parameters, ω, on 10 samples of new sine waves. We\ncompare the multi-environment dynamic model of RAMP to\na small network composed of only 10 parameters trained on\neach new sine wave separately and to MAML [5], which\nupdates the entire network (1761 parameters).\nThe multi-environment dynamic model achieved a Mean\nSquared Error (MSE) of 0.021. This result is slightly lower\nthan MAML, which achieved an MSE of 0.037. Neverthe-\nless, since MAML uses 1761 parameters, it is impractical to\nuse them as a context for the RL agent. As expected, the\nnetwork that contains only 10 parameters resulted in a very\nhigh average MSE, 19.25. When training on all sine waves\ntogether (i.e., all model’s parameters are shared without\nenvironment-speciﬁc parameters), the MSE was 1.9. Figure\n1 depicts the performance of the multi-environment dynamic\nmodel of RAMP on the test set.\nB. Vehicle Target-Reaching Domain\nThe vehicle target-reaching domain is a simple domain\nthat enables us to provide a precise analysis of RAMP’s\nbehavior and demonstrate the concepts behind RAMP. In this\ndomain, an agent controls the vehicle’s throttle and brake and\naims to reach a target line in a minimum time. The vehicle\n4\n2\n0\n2\n4\n4\n3\n2\n1\n0\n1\n2\n3\n4\nFig. 1: Evaluation of sine functions with different amplitudes\nand phases. The solid lines represent the ground-truth func-\ntions, and the dots are the predictions.\nmust reach the target line at a speed of at most vmax. The\nstate, s = {v, d}, consists of the current vehicle’s speed, v,\nand the distance to the target d. v ranges from 0 to 30 m/s,\nthe distance to the target at the beginning of the episode\nis d = 40 m, and the desired maximal speed at the target\nline vmax is 5 m/s. The continuous throttle/brake action, a\nranges from −1 to 1. The sampling frequency is 25 Hz. The\nreward function returns −0.002 at each non-terminal step.\nWhen approaching the target line with a higher speed, than\nvmax the reward is 0.01(v −vmax)2 otherwise 0.\nWe construct 24 vehicle target-reaching environments,\nsplit into 22 for training the multi-environment model and\ntwo for testing. All vehicles from the different environments\nhave identical acceleration but a different deceleration, which\nis unknown to the agent. Speciﬁcally, the throttle command\na = [0, 1] causes an acceleration value ˙v = [0, 42] m/s2 in\nall environments. However, the brake command a = [−1, 0)\ncauses a deceleration value ˙v = [0, 42ka] that is scaled down\nby the braking factor ka, which has a value between 0.1 and\n1. The braking factor in the test environment is ka = 0.925\nand ka = 0.175, which are close to the extremes of all\nfactors.\nWe begin by evaluating the performance of the multi-\nenvironment dynamic model learning process, and then we\nevaluate the performance of the RL learning procedure. Fi-\nnally, we show the adaptation process in a new environment.\n1) Multi-Environment Dynamic Model Learning:\nThe\nmulti-environment neural network is identical to the network\nused for the sine wave regression. The dynamic model state\nconsists only of the vehicle’s speed, and the network predicts\nthe difference between the current and new states. In each\nenvironment, 100 points are randomly sampled for training,\nand only 10 points are sampled from new environments for\nthe adaptation process. Figure 2 shows speeds of 3 different\nvehicles. All accelerate at the same rate until reaching the\nmaximal speed, and then, each vehicle applies a maximal\nbraking action (a = −1), resulting in different decelera-\ntion values. The points represent the predicted speeds, and\nthe solid lines are the real speeds. Our multi-environment\ndynamic model results in an MSE of 0.00029 on new\nenvironments compared to an MSE of 0.045 when trained\non all environments together.\n0\n1\n2\n3\n4\n5\nTime [s]\n0\n5\n10\n15\n20\n25\n30\nVelocity [m/s]\nFig. 2: Speeds prediction of different vehicles. Points are the\npredictions, and solid lines are the ground-truth speeds.\nRecall that the environment-speciﬁc parameters ω are\nretrained multiple times during the RL training process as\ndescribed in Sec. IV-B. Figure 3 shows the values of each of\nthe 10 parameters for every environment, in different colors.\nThe different parameter sets are slightly shifted along the\nhorizontal axis. As depicted by the ﬁgure, the environment-\nspeciﬁc parameters converged to similar values; this can be\nseen by the consistency of the values between the parameter\nsets. In addition, the ﬁgure shows that the different envi-\nronments result in noticeable, different values for the ﬁrst\nthree parameters. In contrast, the remaining parameters show\nonly a minor variance between the environments. This result\nseems reasonable, since not all parameters are required to\ndetermine the variance of the vehicle dynamics, which in\nfact, has only one degree of freedom.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nParameter index\n1.0\n0.5\n0.0\n0.5\n1.0\nParameter Value\nFig. 3: Values of the 10 environment-speciﬁc parameters.\nDifferent colors represent different environments. Multiple\nparameter sets are shown with a shift along the horizontal\naxis.\nRecall that in the RL training phase, the general policy\nmust extract the properties of the environments from only\nthe environment-speciﬁc parameters. Therefore, beyond the\nlow loss of the prediction, we tested that it is possible to\ndirectly predict the braking factor from the environment-\nspeciﬁc parameters. To that end, we trained a dedicated\nregressor, which is not used by RAMP, on 58 sets of the\ntrained environment-speciﬁc parameters created during the\nRL training process. 40 environments were used as a training\nset, and the 18 remaining environments were used as a test\nset. The regressor is composed of a neural network with two\nhidden layers with 100 neurons each. Figure 4 shows the\nprediction error distribution of the braking factor. As depicted\nby the ﬁgure, the regressor predicts the braking factor, which\nranges from 0.1 to 1.0, with an average error of −0.0077.\n0.10\n0.05\n0.00\n0.05\n0.10\n0.15\nPrediction error ka\n0\n10\n20\n30\n40\n50\n60\n70\nFig. 4: Error distribution of predicting the real braking factor\nof environment k, based only on the environment-speciﬁc\nparameters ˆωk.\n2) Multi-Environment Reinforcement Learning: We com-\npared RAMP to the following 3 other RL agents. The\nOracle RL receives explicit information about the vehicle;\nthat is, the braking factor is added to the state. With full\nknowledge of the environmental properties, the Oracle RL is\nexpected to ﬁnd a nearly optimal solution. Next, we consider\na basic RL that is trained in all environments together,\nwithout any identiﬁcation input, and thus cannot distinguish\nbetween different vehicles. Therefore, it is expected to learn a\nconservative policy that enables safe deceleration to the target\nline, even for the vehicle with the lowest braking capability.\nThe third RL agent is the meta-Q-learning (MQL) algorithm\n[14].\nThe training process of all methods was repeated 5 times\nwith different random seeds and is shown in Fig. 5. Our\nmethod’s performance during the training process is compa-\nrable to the Oracle RL, achieving consistently higher episode\nrewards than the basic RL and MQL.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTraining steps\n1e6\n0.50\n0.45\n0.40\n0.35\n0.30\n0.25\n0.20\n0.15\n0.10\nReward\nOracle RL\nBasic RL\nRAMP\nMQL\nFig. 5: Comparison between the training processes. RAMP\nis close to the Oracle RL.\nTable I summarizes the average performance at the end of\nthe training procedure. The table shows for all agents: the\naverage reward in both test environments, the time to reach\nthe target line by the vehicles with a low and high braking\nfactor, and the average time. As depicted by the table, RAMP\nreaches an average reward that is very close to the Oracle’s\nand also has a very similar average time.\nAverage\nReward\nLow ka\nTime [s]\nHigh ka\nTime [s]\nAverage\nTime [s]\nBasic RL\n-0.1656\n3.160\n3.376\n3.268\nOracle RL\n-0.1226\n3.0\n1.976\n2.488\nRAMP\n-0.1230\n3.048\n2.04\n2.544\nMQL\n-0.1510\n3.18\n2.62\n2.90\nTABLE I: The performance of all RL agents on the test\nenvironments. The table shows for each agent: 1. The loss,\naveraged over the test episodes following the 5 separate\ntraining processes; 2. The average time achieved by the\nvehicle with the low braking factor; 3. The average time\nachieved by the vehicle with the high braking factor; 4. The\naverage between these times.\nNext, we analyze the speed proﬁles of different vehicles\ndriven by policies trained by the different RL agents. The\nspeed proﬁle of the basic RL, Oracle RL, RAMP, and MQL\nare shown in Figures 6a, 6b, 6c, and 6d respectively. The\norange lines represent speed proﬁles of vehicles with a high\nbraking factor ka = 0.925, and the blue lines represent\nlow braking factors ka = 0.175. These values are close\nto the extremes of the braking factor range to demonstrate\nthe difference between the environments. The bold columns\nrepresent the maximal permitted speed at the target for\neach of the two environments. As depicted by Fig. 6a, the\nbasic RL begins to brake on both vehicles at the same\npoint in time. This happens because the agent cannot know\nif the vehicle has a higher braking capability that allows\nbraking later or not, which leads to a conservative policy.\nAs shown in Fig. 6b, the Oracle RL begins braking on\ntime in both environments and arrives at the destination at\nthe required maximum target speed. As shown in Fig. 6c,\nRAMP results in a similar speed proﬁle as the Oracle RL.\nHowever, unlike the Oracle agent, RAMP does not receive\nany explicit information about the environment; instead, it\nlearns this information from the trajectory sampled during\none episode. Figure 6d illustrates that the MQL agent can\ndistinguish between the vehicles’ braking differences because\nthe vehicle with the higher braking factor is allowed to gain\nmore speed. However, MQL’s speed proﬁle is not as good\nas RAMP’s since the MQL agent does not accelerate and\ndecelerate at the maximal values, therefore resulting in longer\ndriving times.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nTime [s]\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nVelocity [m/s]\nka = 0.175\nka = 0.925\n(a)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTime [s]\n0\n5\n10\n15\n20\n25\n30\nVelocity [m/s]\nka = 0.175\nka = 0.925\n(b)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTime [s]\n0\n5\n10\n15\n20\n25\n30\nVelocity [m/s]\nka = 0.175\nka = 0.925\n(c)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTime [s]\n0\n5\n10\n15\n20\n25\nVelocity [m/s]\nka = 0.175\nka = 0.925\n(d)\nFig. 6: Speed proﬁles that were achieved by our method and the other agents. Blue - low braking factor, orange - high\nbraking factor, bold line - the maximal desired speed at the target. (a) The basic RL agent resulted in a conservative solution.\n(b) The Oracle RL agent achieved optimal speed proﬁles. (c) Our agent, RAMP, achieves similar optimal results without\nprior knowledge about the vehicle dynamics. (d) MQL resulted in sub-optimal results compared to RAMP.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTime [s]\n0\n5\n10\n15\n20\n25\n30\nVelocity [m/s]\nBefore\nAfter\n(a)\n0.0\n0.5\n1.0\n1.5\n2.0\nTime [s]\n0\n5\n10\n15\n20\n25\n30\nVelocity [m/s]\nBefore\nAfter\n(b)\nFig. 7: RAMP evaluation during two subsequent episodes.\nThe dashed line represents the speed proﬁle used for col-\nlecting data in the ﬁrst episode and the solid line in the\nsecond. (a) Low braking factor: the agent learned that this\nvehicle must brake earlier. (b) High braking factor: the agent\nlearned that this vehicle can brake later.\nTo conclude the evaluation of RAMP’s performance in\nthe target-reaching domain, we analyze RAMP’s adaptation\nprocess. As opposed to the Oracle RL, which is given the\nbraking factor information, RAMP must learn it from the\ndriving experience. That is, in the ﬁrst episode, RAMP\ncollects data points, and the environment-speciﬁc parameters\nare trained on it; in the second episode, RAMP drives the\nvehicle with the updated context. Figure 7 shows the speed\nproﬁle of a vehicle during two subsequent episodes for\nthe low braking factor vehicle (Fig. 7a) and for the high\nbreaking-factor (Fig. 7b). As depicted by Fig. 7a, in the ﬁrst\nepisode (represented by the dashed line), the vehicle brakes\ntoo late and therefore crosses the target line at too high a\nspeed. In the second episode (represented by a solid line),\nthe vehicle brakes earlier and cross the target line at a speed\nthat is within the speed limit. Similarly, for the vehicle with\na higher braking factor, RAMP learns that the vehicle can\nbrake later. Therefore, in the second episode, it crosses the\nﬁnish line earlier than in the ﬁrst episode.\nVI. CONCLUSIONS AND FUTURE WORK\nThis paper presented RAMP, a novel meta-reinforcement\nlearning algorithm. RAMP is constructed in two phases:\nlearning a multi-environment dynamic model and train-\ning a general reinforcement learning policy that uses the\nmodel parameters as context. The multi-environment dy-\nnamic model is trained on data from multiple environments.\nThe shared parameters are updated by the average gradient\ncomputed from the loss resulting from all environments,\nand the environment-speciﬁc parameters are trained sepa-\nrately on data from each environment. The low number of\nenvironment-speciﬁc parameters allows direct use of them as\ncontext for the general policy. That general policy is trained\nby TD3, an actor-critic, off-policy RL algorithm.\nWe evaluated the performance of RAMP in simulated\nexperiments. First, we tested the multi-environment dynamic\nmodel performance by a sine-wave regression test which we\nshow to achieve a slightly lower loss compared to MAML\n[5]. Then, we tested RAMP in a simple driving domain\nwhere every vehicle had a different deceleration rate. We\nshowed that RAMP achieved similar performance to an\nOracle RL agent, which is provided with full knowledge of\nthe environment properties.\nIn future work, we plan to test RAMP in more challenging\ndomains, such as controlling the steering of an autonomous\nvehicle and following a given path. Recall that RAMP\nassumes that the environments differ by their dynamics and\nnot by their reward function, while most previous works\nconsider the opposite. In order to adapt to environments\nthat also differ by their reward functions, a future extension\ncan be to learn a reward prediction function and uses its\nparameters as a context in addition to the multi-environment\nmodel parameters.\nREFERENCES\n[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., “Human-level control through deep reinforcement learning,”\nnature, vol. 518, no. 7540, pp. 529–533, 2015.\n[2] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,\nM. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al., “A general\nreinforcement learning algorithm that masters chess, shogi, and go\nthrough self-play,” Science, vol. 362, no. 6419, pp. 1140–1144, 2018.\n[3] F. Fuchs, Y. Song, E. Kaufmann, D. Scaramuzza, and P. Dürr, “Super-\nhuman performance in gran turismo sport using deep reinforcement\nlearning,” IEEE Robotics and Automation Letters, vol. 6, no. 3, pp.\n4257–4264, 2021.\n[4] G. Hartmann, Z. Shiller, and A. Azaria, “Deep reinforcement learning\nfor time optimal velocity control using prior knowledge,” in 2019 IEEE\n31st International Conference on Tools with Artiﬁcial Intelligence\n(ICTAI).\nIEEE, 2019, pp. 186–193.\n[5] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning\nfor fast adaptation of deep networks,” in International conference on\nmachine learning.\nPMLR, 2017, pp. 1126–1135.\n[6] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and\nP. Abbeel, “Rl2: Fast reinforcement learning via slow reinforcement\nlearning,” arXiv preprint arXiv:1611.02779, 2016.\n[7] A. Graves, G. Wayne, and I. Danihelka, “Neural turing machines,”\narXiv preprint arXiv:1410.5401, 2014.\n[8] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,\nD. Wierstra, and M. Riedmiller, “Playing atari with deep reinforcement\nlearning,” arXiv preprint arXiv:1312.5602, 2013.\n[9] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot, et al., “Mastering the game of go with deep neural\nnetworks and tree search,” nature, vol. 529, no. 7587, pp. 484–489,\n2016.\n[10] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. Yo-\ngamani, and P. Pérez, “Deep reinforcement learning for autonomous\ndriving: A survey,” IEEE Transactions on Intelligent Transportation\nSystems, vol. 23, no. 6, pp. 4909–4926, 2022.\n[11] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,\nA. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al., “Mastering\nthe game of go without human knowledge,” nature, vol. 550, no. 7676,\npp. 354–359, 2017.\n[12] F.-H. Hsu, Behind Deep Blue: Building the computer that defeated the\nworld chess champion.\nPrinceton University Press, 2002.\n[13] W. Yu, J. Tan, C. K. Liu, and G. Turk, “Preparing for the unknown:\nLearning a universal policy with online system identiﬁcation,” in\nRobotics: Science and Systems XIII, Massachusetts Institute of\nTechnology, Cambridge, Massachusetts, USA, July 12-16, 2017,\n2017. [Online]. Available: http://www.roboticsproceedings.org/rss13/\np48.html\n[14] R. Fakoor, P. Chaudhari, S. Soatto, and A. J. Smola, “Meta-q-learning,”\nin International Conference on Learning Representations, 2020.\n[Online]. Available: https://openreview.net/forum?id=SJeD3CEFPH\n[15] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\nComputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[16] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation\nof gated recurrent neural networks on sequence modeling,” arXiv\npreprint arXiv:1412.3555, 2014.\n[17] J. X. Wang, Z. Kurth-Nelson, H. Soyer, J. Z. Leibo, D. Tirumala,\nR. Munos, C. Blundell, D. Kumaran, and M. M. Botvinick, “Learning\nto reinforcement learn,” in CogSci, 2017. [Online]. Available:\nhttps://mindmodeling.org/cogsci2017/papers/0252/index.html\n[18] A.\nAntoniou,\nH.\nEdwards,\nand\nA.\nStorkey,\n“How\nto\ntrain\nyour\nMAML,”\nin\nInternational\nConference\non\nLearning\nRepresentations, 2019. [Online]. Available: https://openreview.net/\nforum?id=HJGven05Y7\n[19] K. Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen, “Efﬁcient off-\npolicy meta-reinforcement learning via probabilistic context variables,”\nin International conference on machine learning.\nPMLR, 2019, pp.\n5331–5340.\n[20] I. Clavera, A. Nagabandi, S. Liu, R. S. Fearing, P. Abbeel,\nS. Levine, and C. Finn, “Learning to adapt in dynamic, real-world\nenvironments through meta-reinforcement learning,” in International\nConference on Learning Representations, 2019. [Online]. Available:\nhttps://openreview.net/forum?id=HyztsoC5Y7\n[21] K. Lee, Y. Seo, S. Lee, H. Lee, and J. Shin, “Context-aware dynamics\nmodel for generalization in model-based reinforcement learning,”\nin Proceedings of the 37th International Conference on Machine\nLearning, ser. Proceedings of Machine Learning Research, vol.\n119.\nPMLR, 13–18 Jul 2020, pp. 5757–5766. [Online]. Available:\nhttps://proceedings.mlr.press/v119/lee20g.html\n[22] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approxi-\nmation error in actor-critic methods,” in International conference on\nmachine learning.\nPMLR, 2018, pp. 1587–1596.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-10-27",
  "updated": "2022-10-27"
}