{
  "id": "http://arxiv.org/abs/2210.16782v1",
  "title": "Unsupervised Learning of Structured Representations via Closed-Loop Transcription",
  "authors": [
    "Shengbang Tong",
    "Xili Dai",
    "Yubei Chen",
    "Mingyang Li",
    "Zengyi Li",
    "Brent Yi",
    "Yann LeCun",
    "Yi Ma"
  ],
  "abstract": "This paper proposes an unsupervised method for learning a unified\nrepresentation that serves both discriminative and generative purposes. While\nmost existing unsupervised learning approaches focus on a representation for\nonly one of these two goals, we show that a unified representation can enjoy\nthe mutual benefits of having both. Such a representation is attainable by\ngeneralizing the recently proposed \\textit{closed-loop transcription}\nframework, known as CTRL, to the unsupervised setting. This entails solving a\nconstrained maximin game over a rate reduction objective that expands features\nof all samples while compressing features of augmentations of each sample.\nThrough this process, we see discriminative low-dimensional structures emerge\nin the resulting representations. Under comparable experimental conditions and\nnetwork complexities, we demonstrate that these structured representations\nenable classification performance close to state-of-the-art unsupervised\ndiscriminative representations, and conditionally generated image quality\nsignificantly higher than that of state-of-the-art unsupervised generative\nmodels. Source code can be found at https://github.com/Delay-Xili/uCTRL.",
  "text": "Preprint. Under review.\nUNSUPERVISED LEARNING OF STRUCTURED REPRE-\nSENTATIONS VIA CLOSED-LOOP TRANSCRIPTION\nShengbang Tong1∗\nXili Dai1,2 *†\nYubei Chen3\nMingyang Li5\nZengyi Li1\nBrent Yi1\nYann LeCun3,4\nYi Ma1,5\n1University of California, Berkeley\n2Hong Kong University of Science and Technology (Guangzhou)\n3Center for Data Science, New York University\n4Courant Inst., New York University\n5Tsinghua-Berkeley Shenzhen Institute (TBSI)\nABSTRACT\nThis paper proposes an unsupervised method for learning a uniﬁed representation\nthat serves both discriminative and generative purposes. While most existing unsu-\npervised learning approaches focus on a representation for only one of these two\ngoals, we show that a uniﬁed representation can enjoy the mutual beneﬁts of having\nboth. Such a representation is attainable by generalizing the recently proposed\nclosed-loop transcription framework, known as CTRL, to the unsupervised setting.\nThis entails solving a constrained maximin game over a rate reduction objective\nthat expands features of all samples while compressing features of augmentations\nof each sample. Through this process, we see discriminative low-dimensional\nstructures emerge in the resulting representations. Under comparable experimental\nconditions and network complexities, we demonstrate that these structured repre-\nsentations enable classiﬁcation performance close to state-of-the-art unsupervised\ndiscriminative representations, and conditionally generated image quality signiﬁ-\ncantly higher than that of state-of-the-art unsupervised generative models. Source\ncode can be found at https://github.com/Delay-Xili/uCTRL.\n1\nINTRODUCTION\nIn the past decade, we have witnessed an explosive development in the practice of machine learning,\nparticularly with deep learning methods. A key driver of success in practical applications has been\nmarvelous engineering endeavors, often focused on ﬁtting increasingly large deep networks to input\ndata paired with task-speciﬁc sets of labels. Brute-force approaches of this nature, however, exert\ntremendous demands on hand-labeled data for supervision and computational resources for training\nand inference. As a result, an increasing amount of attention has been directed toward using self-\nsupervised or unsupervised techniques to learn representations that can not only learn without human\nannotation effort, but also be shared across downstream tasks.\nDiscriminative versus Generative. Tasks in unsupervised learning are typically separated into two\ncategories. Discriminative ones frame high-dimensional observations as inputs, from which low-\ndimensional class or latent information can be extracted, while generative ones frame observations as\ngenerated outputs, which should often be sampled given some semantically meaningful conditioning.\nUnsupervised learning approaches targeted at discriminative tasks are mainly based on a key idea: to\npull different views from the same instance closer while enforcing a non-collapsed representation\nby either contrastive learning techniques (Chen et al., 2020b; He et al., 2020; Grill et al., 2020a),\ncovariance regularization methods (Bardes et al., 2021; Zbontar et al., 2021), or using architecture\ndesign (Chen & He, 2020; Grill et al., 2020b). Their success is typically measured by the accuracy of a\nsimple classiﬁer (say a shallow network) trained on the representations that they produce, which have\nprogressively improved over the years. Representations learned from these approaches, however, do\nnot emphasize much about the intrinsic structure of the data distribution, and have not demonstrated\nsuccess for generative purposes.\nIn parallel, generative methods like GANs (Goodfellow et al., 2014) and VAEs (Kingma & Welling,\n2013) have also been explored for unsupervised learning. Although generative methods have made\n∗Equal contribution\n†Work done during visiting at Berkeley\n1\narXiv:2210.16782v1  [cs.CV]  30 Oct 2022\nPreprint. Under review.\nstriking progress in the quality of the sampled or autoencoded data, when compared to the aforemen-\ntioned discriminative methods, representations learned with these approaches demonstrate inferior\nperformance in classiﬁcation.\nToward A Uniﬁed Representation? The disparity between discriminative and generative approaches\nin unsupervised learning, contrasted against the fundamental goal of learning representations that\nare useful across many tasks, leads to a natural question that we investigate in this paper: in\nthe unsupervised setting, is it possible to learn a uniﬁed representation that is effective for both\ndiscriminative and generative purposes? Further, do they mutually beneﬁt each other? Concretely,\nwe aim to learn a structured representation with the following two properties:\n1. The learned representation should be discriminative, such that simple classiﬁers applied to\nlearned features yield high classiﬁcation accuracy.\n2. The learned representation should be generative, with enough diversity to recover raw inputs,\nand structure that can be exploited for sampling and generating new images.\nThe fact that human visual memory serves both discriminative tasks (for example, detection and\nrecognition) and generative or predictive tasks (for example, via replay) (Keller & Mrsic-Flogel,\n2018; Josselyn & Tonegawa, 2020; Ven et al., 2020) indicates that this goal is achievable. Beyond\nbeing possible, these properties are also highly practical – successfully completing generative tasks\nlike unsupervised conditional image generation (Hwang et al., 2021), for example, inherently requires\nthat learned features for different classes be both structured for sampling and discriminative for\nconditioning. On the other hand, the generative property can serve as a natural regularization to avoid\nrepresentation collapse.\nClosed-Loop Transcription via a Constrained Maximin Game. The class of linear discriminative\nrepresentations (LDRs) has recently been proposed for learning diverse and discriminative features\nfor multi-class (visual) data, via optimization of the rate reduction objective (Chan et al., 2022). In the\nsupervised setting, these representations have been shown to be be both discriminative and generative\nif learned in a closed-loop transcription framework via a maximin game over the rate reduction utility\nbetween an encoder and a decoder (Dai et al., 2022). Beyond the standard joint learning setting,\nwhere all classes are sampled uniformly throughout training, the closed-loop framework has also\nbeen successfully adapted to the incremental setting (Tong et al., 2022), where the optimal multi-class\nLDR is learned one class at a time. In the incremental (supervised) learning setting, one solves a\nconstrained maximin problem over the rate reduction utility which keeps learned memory of old\ntasks intact (as constraints) while learning new tasks. It has been shown that this new framework can\neffectively alleviate the catastrophic forgetting suffered by most supervised learning methods.\nContributions. In this work, we show that the closed-loop transcription framework proposed for\nlearning LDRs in the supervised setting (Chan et al., 2022) can be adapted to a purely unsupervised\nsetting. In the unsupervised setting, we only have to view each sample and its augmentations as a\n“new class” while using the rate reduction objective to ensure that learned features are both invariant\nto augmentation and self-consistent in generation; this leads to a constrained maximin game that\nis similar to the one explored for incremental learning (Tong et al., 2022). Our overall approach is\nillustrated in Figure 1.\nAs we experimentally demonstrate in Section 4, our formulation beneﬁts from the mutual beneﬁts of\nboth discriminative and generative properties. It bridges the gap between two formerly distinct set of\nmethods: by standard metrics and under comparable experimental conditions, it enables classiﬁcation\nperformance on par with and unsupervised conditional generative quality signiﬁcantly higher than\nstate-of-the-art techniques. Coupled with evidence from prior work, this suggests that the closed-loop\ntranscription through the (constrained) maximin game between the encoder and decoder has the\npotential to offer a unifying framework for both discriminative and generative representation learning,\nacross supervised, incremental, and unsupervised settings.\n2\nRELATED WORK\nOur work is mostly related to three categories of unsupervised learning methods: (1) self-supervised\nlearning via discriminative models, (2) self-supervised learning via generative models, and (3)\nunsupervised conditional image generation. Table 1 compares the capabilities of models learned by\nvarious representative unsupervised learning methods.\n2\nPreprint. Under review.\nFigure 1: Overall framework of closed-loop transcription for unsupervised learning. Two additional\nconstraints are imposed on the Binary-CTRL method proposed in prior work (Dai et al., 2022): 1)\nself-consistency for sample-wise features zi and ˆzi, say zi ≈ˆzi; and 2) invariance/similarity among\nfeatures of augmented samples zi and zi\na, say zi ≈zi\na = f(τ(xi), θ), where xi\na = τ(xi) is an\naugmentation of sample xi via some transformation τ(·).\nMethod\nLinear Probe\nImage Generation\nUCIG\nSimCLR (Chen et al., 2020b)\n\u0014\n\u0017\n\u0017\nMOCO-V2 (He et al., 2020)\n\u0014\n\u0017\n\u0017\nContraD (Jeong & Shin, 2021)\n\u0014\n\u0014\n\u0017\nPATCH-VAE (Parmar et al., 2021)\n\u0014\n\u0014\n\u0017\nCTRL-Binary (Dai et al., 2022)\n\u0014\n\u0014\n\u0017\nSLOGAN (Hwang et al., 2021)\n\u0017\n\u0014\n\u0014\nU-CTRL (ours)\n\u0014\n\u0014\n\u0014\nTable 1: Comparison of the downstream task capabilities of different unsupervised learning methods. UCIG\nrefers to Unsupervised Conditional Image Generation (Hwang et al., 2021).\nSelf-Supervised Learning for Discriminative Models. On the discriminative side, works like\nSimCLR (Chen et al., 2020b), MoCo (He et al., 2020), and BYOL (Grill et al., 2020a) have recently\nshown overwhelming effectiveness in learning discriminative representations of data. MoCo (He\net al., 2020) and SimCLR (Chen et al., 2020b) seek to learn features by pulling together features\nof augmented versions of the same sample while pushing apart features of all other samples, while\nBYOL (Grill et al., 2020a) trains a student network to predict the representation of a teacher network\nin a contrastive setting. BarlowTwins (Zbontar et al., 2021) and TCR (Li et al., 2022) learn by\nregularizing the covariance matrix of the embedding. However, features learned by this class of\nmethods are typically highly compressed, and not designed to be used for generative purposes.\nSelf-Supervised Learning with Generative Models. On the generative side, the original GAN\n(Goodfellow et al., 2014) can be viewed as a natural self-supervised learning task. With an additional\nlinear probe, works like DCGAN (Radford et al., 2015) have shown that features in the discriminator\ncan be used for discriminative tasks. To further enhance the features, extensions like BiGAN\n(Donahue et al., 2016) and ALI (Dumoulin et al., 2016) introduce a third network into the GAN\nframework, aimed at learning an inverse mapping for the generator, which when coupled with\nlabeled images can be used to study and supervise semantics in learned representations. Other\nworks like SSGAN (Chen et al., 2019), SSGAN-LA (Hou et al., 2021), and ContraD (Jeong & Shin,\n2021) propose to put augmentation tasks into GAN training to facilitate representation learning.\nOutside of GANs, variational autoencoders (VAEs) have been adapted to generate more semantically\nmeaningful representations by trading off latent channel capacity and independence constraints\nwith reconstruction accuracy (Higgins et al., 2016), an idea that has also been incorporated into\nrecognition improvements using patch-level bottlenecks (Gupta et al., 2020), which encourage a VAE\nto focus on useful patterns in images. By incorporating data-augmentation, VAE is also shown to\nachieve fair discriminative performance (Falcon et al., 2021). Recently, works like MAE (He et al.,\n2021) and CAE (Chen et al., 2022) have learned representations by solving masked reconstruction\ntasks using vision transformers. Autogressive approaches like iGPT (Chen et al., 2020a) have\nalso demonstrated decent self-supervised learning performance, which improves further with the\nincorporation of contrastive learning (Kim et al., 2021). However, unless supervised, features learned\nby those previously mentioned methods either do not have strong discriminative performance, or\ncannot be directly exploited to condition the generative task.\n3\nPreprint. Under review.\nUnsupervised Conditional Image Generation (UCIG). For generative models, we often want to\nbe able to generate images conditioned on a certain class or style, even in a completely unsupervised\nsetting. This requires that the learned representations have structures that correspond to the desired\nconditioning. InfoGAN (Chen et al., 2016) proposes to learn interpretable representations by maxi-\nmizing the mutual information between the observation and a subset of the latent code. ClusterGAN\n(Mukherjee et al., 2019) assumes a discrete Gaussian prior where discrete variables are deﬁned as a\none-hot vector and continuous variables are sampled from Gaussian distribution. Self-Conditioned\nGAN (Liu et al., 2020) uses clustering of discriminative features as labels to train. SLOGAN (Hwang\net al., 2021) proposes a new conditional contrastive loss (U2C) to learn latent distribution of the data.\nNote that compared to our work, ClusterGAN and SLOGAN introduce an additional encoder that\nleads to increased computational complexity. On the VAE side, works like VaDE (Jiang et al., 2016)\ncluster based on the learned feature of a supervised ResNet. Variational Cluster (Prasad et al., 2020)\nsimultaneously learns a prior that captures the latent distribution of the images and a posterior to help\ndiscriminate between data points in an end-to-end unsupervised setting. In this work, we will see\nhow clusters can be estimated in a principled way in a more uniﬁed framework, by optimizing the\nsame type of objective function that we use for learning features.\n3\nMETHOD\n3.1\nPRELIMINARIES: RATE REDUCTION AND CLOSED-LOOP TRANSCRIPTION\nAssumptions on Data.\nOur work, as well as prior work in closed-loop transcription (Dai et al.,\n2022; Tong et al., 2022), considers a set of N images X = [x1, x2, ..., xN] ⊂RD sampled from k\nclasses. Borrowing notation from (Yu et al., 2020), the membership of the N samples in the k classes\nis denoted using k diagonal matrices: Π = {Πj ∈RN×N}k\nj=1, where the diagonal entry Πj(i, i) of\nΠj is the probability of sample i belonging to subset j. Let Ω.= {Π | P Πj = I, Πj ≥0.} be the\nset of all such matrices. WLOG, we may assume that classes are separable, with images for each\nbelonging to a low-dimensional submanifold in the space RD.\nUnsupervised Discriminative Autoencoding.\nThe goal of transcription is to learn a uniﬁed rep-\nresentation, with the structure required to both classify and generate images from these k classes.\nConcretely, this is achieved by learning two continuous mappings: (1) an encoder parametrized by θ:\nf(·, θ) : x 7→z ∈Rd with d ≪D such that all samples are mapped to their features as X\nf(x,θ)\n−−−−→Z\nwith Z = [z1, z2, ..., zN] ⊂Rd, and (2) an inverse map g(·, η) : z 7→ˆx ∈RD such that x and\nˆx = g(f(x)) is close. In other words, X\nf(x,θ)\n−−−−→Z\ng(z,η)\n−−−−→ˆ\nX forms an autoencoding.\nIn this work, we speciﬁcally learn this mapping in an entirely unsupervised fashion, without knowing\nthe ground-truth class labels Π at all. As stated in the introduction, a both discriminative and\ngenerative representation is difﬁcult to achieve by standard generative methods like VAEs and GANs.\nThis is one of the motivations for the closed-loop transcription framework (CTRL) proposed by (Dai\net al., 2022), which we will generalize to the unsupervised setting.\nMaximizing Rate Reduction.\nThe CTRL framework (Dai et al., 2022) was proposed for the\nsupervised setting, where it aims to map each class onto an independent linear subspace. As shown in\n(Yu et al., 2020), such a linear discriminative representation (LDR) can be achieved by maximizing a\ncoding rate reduction objective, known as the MCR2 principle:\n∆R\n\u0000Z|Π) .= 1\n2 log det\n\u0012\nI +\nd\nNϵ2 ZZ⊤\n\u0013\n|\n{z\n}\nR(Z)\n−\nk\nX\nj=1\ntr(Πj)\n2N\nlog det\n\u0012\nI +\nd\ntr(Πj)ϵ2 ZΠjZ⊤\n\u0013\n|\n{z\n}\nRc\n. (1)\nwhere each Πj encodes the membership of the N samples described before. As discussed in (Chan\net al., 2022), the ﬁrst term R(Z) measures the total rate (volume) of all features whereas the second\nterm Rc measures the average rate (volume) of the k components. Our work adapts this formula to\ndesign meaningful objectives in the unsupervised setting.\nClosed-Loop Transcription.\nTo learn the autoencoding X\nf(x,θ)\n−−−−→Z\ng(z,η)\n−−−−→ˆ\nX, a fundamental\nquestion is how we measure the difference between X and the regenerated ˆ\nX = g(f(X)). It is\ntypically very difﬁcult to put a proper distance measure in the image space (Wang et al., 2004). To\n4\nPreprint. Under review.\nbypass this difﬁculty, the closed-loop transcription framework (Dai et al., 2022) proposes to measure\nthe difference between X and ˆ\nX through the difference between their features Z and ˆZ mapped\nthrough the same encoder:\nX\nf(x,θ)\n−−−−−−→Z\ng(z,η)\n−−−−−−→ˆ\nX\nf(x,θ)\n−−−−−−→ˆZ.\n(2)\nThe difference can be measured by the rate reduction between Z and ˆZ, a special case of (1) with\nk = 2 classes:\n∆R\n\u0000Z, ˆZ\n\u0001 .= R\n\u0000Z ∪ˆZ\n\u0001\n−1\n2\n\u0000R\n\u0000Z) + R\n\u0000 ˆZ)\n\u0001\n.\n(3)\nSuch a ∆R is a principled distance between subspace-like Gaussian ensembles, with the property\nthat ∆R(Z, ˆZ) = 0 iff Cov(Z) = Cov( ˆZ) (Ma et al., 2007).\nAs shown in (Dai et al., 2022), applying this measure in the closed-loop CTRL formulation can already\nlearn a decent autoencoding, even without class information. This is known as the CTRL-Binary\nprogram:\nmax\nθ\nmin\nη\n∆R(Z, ˆZ)\n(4)\nHowever, note that (4) is practically limited because it only aligns the dataset X and the regenerated\nˆ\nX at the distribution level. There is no guarantee that for each sample x would be close to the\ndecoded ˆx = g(f(x)). For example, (Dai et al., 2022) shows that a car sample can be decoded into a\nhorse; the so obtained (autoencoding) representations are not sample-wise self-consistent!\n3.2\nSAMPLE-WISE CONSTRAINTS FOR UNSUPERVISED TRANSCRIPTION\nTo improve discriminative and generative properties of representations learned in the unsupervised\nsetting, we propose two additional mechanisms for the above CTRL-Binary maximin game (4). For\nsimplicity and uniformity, here these will be formulated as equality constraints over rate reduction\nmeasures, but in practice they can be enforced softly during optimization.\nSample-wise Self-Consistency via Closed-Loop Transcription.\nFirst, to address the issue that\nCTRL-Binary does not learn a sample-wise consistent autoencoding, we need to promote ˆx to be\nclose to x for each sample. In the CTRL framework, this can be achieved by enforcing that their\ncorresponding features z = f(x) and ˆz = f(ˆx) are the same or close. To promote sample-wise\nself-consistency, where ˆx = g(f(x)) is close to x , we want the distance between z and ˆz to be zero\nor small, for all N samples. This can be formulated using rate reduction; note that this again avoids\nmeasuring differences in the image space:\nX\ni∈N\n∆R(zi, ˆzi) = 0.\n(5)\nSelf-Supervision via Compressing Augmented Samples.\nSince we do not know any class label\ninformation between samples in the unsupervised settings, the best we can do is to view every\nsample and its augmentations (say via translation, rotation, occlusion etc) as one “class” — a basic\nidea behind almost all self-supervised learning methods. In the rate reduction framework, it is\nnatural to compress the features of each sample and its augmentations. In this work, we adopt the\nstandard transformations in SimCLR (Chen et al., 2020b) and denote such a transformation as τ.\nWe denote each augmented sample xa = τ(x), and its corresponding feature as za = f(xa, θ).\nFor discriminative purposes, we hope the classiﬁer is invariant to such transformations. Hence it\nis natural to enforce that the features za of all augmentations are the same as that z of the original\nsample x. This is equivalent to requiring the distance between z and za, measured in terms of rate\nreduction again, to be zero (or small) for all N samples:\nX\ni∈N\n∆R(zi, zi\na) = 0.\n(6)\n3.3\nUNSUPERVISED REPRESENTATION LEARNING VIA CLOSED-LOOP TRANSCRIPTION\nSo far, we know the CTRL-Binary objective ∆R(Z, ˆZ) in (4) helps align the distributions while\nsample-wise self-consistency (5) and sample-wise augmentation (6) help align and compress features\nassociated with each sample. Besides consistency, we also want learned representations are maximally\ndiscriminative for different samples (here viewed as different “classes”). Notice that the rate distortion\nterm R(Z) measures the coding rate (hence volume) of all features. It has been observed in (Li et al.,\n2022) that by maximizing this term, learned features expand and hence become more discriminative.\n5\nPreprint. Under review.\nUnsupervised CTRL.\nPutting these elements together, we propose to learn a representation via\nthe following constrained maximin program, which we refer to as unsupervised CTRL (U-CTRL):\nmax\nθ\nmin\nη\nR(Z) + ∆R(Z, ˆZ)\n(7)\nsubject to\nX\ni∈N\n∆R(zi, ˆzi) = 0, and\nX\ni∈N\n∆R(zi, zi\na) = 0.\nIn practice, the above program can be optimized by alternating maximization and minimization\nbetween the encoder f(·, θ) and the decoder g(·, η). We adopt the following optimization strategy\nthat works well in practice, which is used for all subsequent experiments on real image datasets:\nmax\nθ\nR(Z) + ∆R(Z, ˆZ) −λ1\nX\ni∈N\n∆R(zi, zi\na) −λ2\nX\ni∈N\n∆R(zi, ˆzi);\n(8)\nmin\nη\nR(Z) + ∆R(Z, ˆZ) + λ1\nX\ni∈N\n∆R(zi, zi\na) + λ2\nX\ni∈N\n∆R(zi, ˆzi),\n(9)\nwhere the constraints P\ni∈N ∆R(zi, ˆzi) = 0 and P\ni∈N ∆R(zi, zi\na) = 0 in (7) have been converted\n(and relaxed) to Lagrangian terms with corresponding coefﬁcients λ1 and λ2.1\nUnsupervised Conditional Image Generation via Rate Reduction.\nThe above representation is\nlearned without class information. In order to facilitate discriminative or generative tasks, it must\nbe highly structured. As we will see via experiments, speciﬁc and unique structure indeed emerges\nnaturally in the representations learned using U-CTRL: globally, features of images in the same class\ntend to be clustered well together and separated from other classes (Figure 2); locally, features around\nindividual samples exhibit approximately piecewise linear low-dimensional structures (Figure 5).\nThe highly-structured feature distribution also suggests that the learned representation can be very\nuseful for generative purposes. For example, we can organize the sample features into meaningful\nclusters, and model them with low-dimensional (Gaussian) distributions or subspaces. By sampling\nfrom these compact models, we can conditionally regenerate meaningful samples from computed\nclusters. This is known as unsupervised conditional image generation (Hwang et al., 2021).\nTo cluster features, we exploit the fact that the rate reduction framework (1) is inspired by unsupervised\nclustering via compression (Ma et al., 2007), which provides a principled way to ﬁnd the membership\nΠ. Concretely, we maximize the same rate reduction objective (1) over Π, but ﬁx the learned\nrepresentation Z instead. We simply view the membership Π as a nonlinear function of the features\nZ, say hπ(·, ξ) : Z 7→Π with parameters ξ. In practice, we model this function with a simple neural\nnetwork, such as an MLP head right after the output feature z. To estimate a “pseudo” membership\nˆΠ of the samples, we solve the following optimization problem over Π:\nˆΠ = arg max\nξ\n∆R(Z|Π(ξ)).\n(10)\nExperiments in Section 4.2 demonstrate that conditional image generation from clusters produced in\nthis manner result in high-quality images that are highly similar in style.\n4\nEXPERIMENTS\nWe now evaluate the performance of the proposed U-CTRL framework and compare it with represen-\ntative unsupervised generative and discriminative methods. The ﬁrst set of experiments (Section 4.1\nshow that despite being a generative method in nature, U-CTRL can learn discriminative representa-\ntions competitive with state-of-the-art discriminative methods. The second set (Section 4.2) show\nthat the learned generative representation can signiﬁcantly boost the performance of unsupervised\nconditional image generation. Finally, the third set (Section 4.3) study how the advantages that\ngenerative represeentations have over discriminative ones.\nWe conduct experiments on the following datasets: CIFAR-10 (Krizhevsky et al., 2014), CIFAR-100\n(Krizhevsky et al., 2009), and Tiny ImageNet (Deng et al., 2009). Standard augmentations for\nself-supervised learning are used across all datasets (Chen et al., 2020b).\n1Notice that computing the rate reduction terms ∆R for all samples or a batch of samples requires computing\nthe expensive log det of large matrices. In practice, from the geometric meaning of ∆R for two vectors, ∆R\ncan be approximated with an ℓ2 norm or the cosine distance between two vectors.\n6\nPreprint. Under review.\nMethod\nCIFAR-10\nCIFAR-100\nTiny-ImageNet\nAccuracy\nAccuracy\nAccuracy\nGAN based methods\nSSGAN-LA(Hou et al., 2021)\n0.803\n0.543\n0.344\nDAGAN+(Antoniou et al., 2017)\n0.772\n0.519\n0.224\nContraD(Jeong & Shin, 2021)\n0.852\n0.514\n-\nVAE based methods\nPATCH-VAE (Parmar et al., 2021)\n0.471\n0.325\n-\nβ-VAE (Higgins et al., 2016)\n0.531\n0.315\n-\nCTRL based methods\nCTRL-Binary(Dai et al., 2022)\n0.599\n-\n-\nU-CTRL (ours)\n0.874\n0.552\n0.360\nTable 2: Comparison of classiﬁcation accuracy on CIFAR-10, CIFAR-100, and Tiny-ImageNet with other\ngenerative self-supervised learning methods. U-CTRL is clearly better.\nMethod\nCIFAR-10\nCIFAR-100\nTiny-ImageNet\nAccuracy\nAccuracy\nAccuracy\nSIMCLR\n0.869\n0.545\n0.359\nMoCoV2\n0.872\n0.589\n0.365\nBYOL\n0.883\n0.581\n0.371\nU-CTRL (ours)\n0.874\n0.552\n0.360\nTable 3: Comparison of classiﬁcation accuracy on CIFAR-10, CIFAR-100, and Tiny-ImageNet with purely\ndiscriminative self-supervised learning methods. U-CTRL is on par with these non-generative methods.\nWe design all experiments to ensure that comparisons against U-CTRL are fair. For all methods that\nwe compare against, we ensure that experiments are conducted with similar model sizes. If code for\nsimilar size structure can not be found, we uniformly use ResNet-18 to reproduce results for baselines,\nwhich is larger than the network used by our method. Details about network architectures and the\nexperimental setting are given in Appendix A. All methods have runned 400 epochs or equivalent\niterations (because generative models often count in iteration).\n4.1\nDISCRIMINATIVE QUALITY OF LEARNED REPRESENTATIONS\nTo evaluate the discriminative quality of the learned representations, we follow the standard practice\nof evaluating the accuracy of a simple linear classiﬁer trained on the learned representation. Table\n2 compares our method against SOTA generative self-supervised learning methods, and Table 3\ncompares our method against SOTA discriminative self-supervised methods. Experimental and\ntraining details are given in Appendix A.\nQuantitative Comparisons of Classiﬁcation Performance. From Table 2, we observe that on all\nchosen datasets, our method achieves substantial improvements compared to existing generative\nself-supervised learning methods. This includes more complex datasets like CIFAR-100 and Tiny-\nImageNet, where we surpass the current SOTA models. From Table 3, our method achieves similar\nperformance compared to SOTA discriminative self-supervised models. These results echo our goal of\nseeking a more unifed generative and discriminative representations: despite resembling a generative\nmethod architecturally, our method still produces highly discriminative representations. In addition,\nthese results lead us to ask a fundamental question: when is incorporating both discriminative and\ngenerative properties a whole greater than the sum of its parts, particularly outside of the context of\ncomputational efﬁciency? We provide preliminary answers in Section 4.3.\nQualitative Visualization of Learned Representations. To explain the classiﬁcation performance\nof our method, we visualize the incoherence between features learned for the training datasets.\nFigure 2 shows cosine similarity heatmaps between the learned features, organized by ground-truth\nclass labels. A block-diagonal pattern emerges automatically from U-CTRL training for all three\ndatasets, similar to those observed in features learned in a supervised setting (Dai et al., 2022). In this\ncase, however, these blocks emerge and correspond with classes labels despite the absence of any\nsupervision at all.\n7\nPreprint. Under review.\n(a) CIFAR-10\n(b) CIFAR-100\n(c) Tiny ImageNet\nFigure 2: Emergence of block-diagonal structures of |Z⊤Z| in the feature space for CIFAR-10 (left), 10\nrandom classes from CIFAR-100 (middle), and 10 random classes from Tiny ImageNet (right).\n(a) CIFAR-10 X\n(b) CIFAR-10 ˆ\nX\nFigure 3: Sample-wise self-consistency: visualization of images X and reconstructed ˆ\nX on CIFAR-10 dataset.\n4.2\nIMPROVED UNSUPERVISED CONDITIONAL GENERATION QUALITY\nTo evaluate the quality of unsupervised conditional image generation, we measure performance on\ntwo axes: cluster quality and image quality. We estimate clusters by optimizing (10), and show results\nand comparisons with both recent and classical methods in Table 4. Training details of our method\nfor the additional MLP head can be found in the Appendix A.\nCluster Quality. We measure normalized mutual information (NMI) and clustering accuracy for\ncluster quality on CIFAR-10 clustered into 10 classes and CIFAR-100(20), which is clustered into 20\nsuper-classes. From Table 4, we observe that on CIFAR-10, U-CTRL results in an NMI that is almost\ndouble that of the existing SOTA on both GAN-based and VAE-based methods, with signiﬁcantly\nimproved clustering accuracy. Unlike many baselines, we also demonstrate that our method scales to\nthe more challenging CIFAR-100(20) dataset, where it also signiﬁcantly outperforms alternatives.\nOur improved clustering quality suggests potential for improving unsupervised conditional image\ngeneration, which relies on ﬁrst ﬁnding statistically (and hence visually) meaningful clusters.\nImage Quality. We use Frechet Inception Distance (FID) (Heusel et al., 2017) and Inception Score\n(IS) (Salimans et al., 2016) to measure image quality. From Table 4, it is evident that U-CTRL\nmaintains competitive image quality compared to other methods, measured both by FID and IS.\nWe also compare original images against reconstructed ones in Figure 3, where we see that the\noriginal X is very similar to the reconstructed ˆ\nX; U-CTRL indeed achieves very good sample-wise\nself-consistency.\nUnsupervised Conditional Image Generation. In Figure 4, we visualize images generated from\nthe ten unsupervised clusters from (10). Each block represents one cluster and each row represents\none principal component for each cluster. Despite learning and training without labels, the model not\nonly organizes samples into correct clusters, but is also able to preserve statistical diversities within\neach cluster/class. We can easily recover the diversity within each cluster by computing different\nprincipal components and then sample and generate accordingly! More detailed illustrations with\nmore samples is provided in Appendix B.\n4.3\nBENEFITS OF U-CTRL’S STRUCTURED REPRESENTATION\nAs shown in the previous section, on datasets like CIFAR-10, CIFAR-100, and Tiny-ImageNet, our\nframework is able to achieve representation quality on par with the best discriminative self-supervised\nlearning methods. A clear advantage of this is computational efﬁciency; only a single representation\nneeds to be trained for a much broader set of tasks. This subsection aims to provide additional insights\non how a uniﬁed model can be more beneﬁcial for a broader range of tasks.\nDomain Transfer. Regenerating images is demanding on the encoder, which is required to produce\na more informative representation than contrastive training would. We hypothesize that the encoder\n8\nPreprint. Under review.\nFigure 4: Unsupervised conditional image generation from each cluster of CIFAR-10, using U-CTRL. Images\nfrom different rows mean generation from different principal components of each cluster.\nMethod\nCIFAR-10\nCIFAR-100(20)\nNMI\nAccuracy FID↓IS↑\nNMI\nAccuracy FID↓IS↑\nGAN based methods\nSelf-Conditioned GAN (Liu et al., 2020)\n0.333\n0.117\n18.0\n7.7\n0.214\n0.092\n24.1\n5.2\nSLOGAN (Hwang et al., 2021)\n0.340\n-\n20.6\n-\n-\n-\n-\n-\nVAE based methods\nGMVAE(Dilokthanakul et al., 2016)\n-\n0.247\n-\n-\n-\n-\n-\n-\nVariational Clustering\n-\n0.445\n-\n-\n-\n-\n-\n-\nCTRL based methods\nU-CTRL (ours)\n0.658\n0.799\n17.4\n8.1\n0.374\n0.433\n20.1\n7.7\nTable 4: Comparison of the quality of UCIG on CIFAR-10 and CIFAR-100(20). Many of the methods compared\ndo not provide code that scales up to CIFAR-100(20), in which case we leave the corresponding table cell blank.\ntrained with generative task may retain more information about the image and allow the representation\nto generalize better. To verify this, we compare the accuracy on CIFAR-100 using models learned\nfrom CIFAR-10 in Table 5. When compared to purely discriminative self-supervised learning models,\nwe observe that U-CTRL is 4 percent better than other methods on classiﬁcation accuracy.\nVisualization of Emerged Structures. The representations learned by U-CTRL are signiﬁcantly\ndifferent from those learned from previous either discriminative and generative methods. To illustrate\nthis, we use t-SNE (Van der Maaten & Hinton, 2008) to visualize the learned representation in 2D.\nFigure 5 compares the t-SNE of representations learned for CIFAR-10 by U-CTRL and MoCoV2,\nrespectively. It is clear that the representation learned by U-CTRL are much more structured and\nbetter organized: classes are more evident, and features within each class form clear piecewise linear\nstructures.\nMethod\nSIMCLR\nMoCoV2\nBYOL\nU-CTRL\nAccuracy\n0.422\n0.436\n0.437\n0.481\nTable 5: Comparing the transfer ability with purely discriminative self-supervised learning methods. All\nmethods are trained unsupervised on CIFAR-10 and tested on CIFAR-100.\n(a) U-CTRL\n(b) MoCoV2\nFigure 5: t-SNE visualizations of learned features of CIFAR-10 with different models.\n5\nCONCLUSION AND DISCUSSION\nIn this work, we proposed an unsupervised formulation of the closed-loop transcription frame-\nwork (Dai et al., 2022). We experimentally demonstrate that it is possible to learn a uniﬁed represen-\ntation for both discriminative and generative purposes, resulting in highly structured representations.\n9\nPreprint. Under review.\nFurther, we show that these two purposes mutually beneﬁt each other in various tasks, e.g., conditional\nimage generation and domain tranfers. Compared to the more specialized representations learned in\nprior works, our results suggest that such a uniﬁed representation has the potential in supporting and\nbeneﬁting a wider range of new tasks. In future work, we believe the learned representations can be\nfurther improved by jointly optimizing the feature representation and feature clusters, as suggested\nin the original rate reduction paper (Chan et al., 2022). Features with high likelihood of belonging\nto the same cluster can be further linearized and compressed. Due to its unifying nature and the\nsimplicity of the underlying concepts, this new framework may be extended beyond image data, such\nas sequential or dynamical observations.\n6\nACKNOWLEDGEMENTS\nYi Ma acknowledges support from ONR grants N00014-20-1-2002 and N00014-22-1-2102, the joint\nSimons Foundation-NSF DMS grant #2031899, as well as partial support from Berkeley FHL Vive\nCenter for Enhanced Reality and Berkeley Center for Augmented Cognition, Tsinghua-Berkeley\nShenzhen Institute (TBSI) Research Fund, and Berkeley AI Research (BAIR).\nREFERENCES\nAntreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial\nnetworks. arXiv preprint arXiv:1711.04340, 2017.\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization\nfor self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.\nKwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma. ReduNet:\nA white-box deep network from the principle of maximizing rate reduction. Journal of Ma-\nchine Learning Research, 23(114):1–103, 2022. URL http://jmlr.org/papers/v23/\n21-0631.html.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In International Conference on Machine Learning, pp. 1691–\n1703. PMLR, 2020a.\nTing Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised gans via\nauxiliary rotation loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 12154–12163, 2019.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning, pp.\n1597–1607. PMLR, 2020b.\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-\ngan: Interpretable representation learning by information maximizing generative adversarial nets.\nAdvances in neural information processing systems, 29, 2016.\nXiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han,\nPing Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representation\nlearning. arXiv preprint arXiv:2202.03026, 2022.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2020.\nXili Dai, Shengbang Tong, Mingyang Li, Ziyang Wu, Michael Psenka, Kwan Ho Ryan Chan,\nPengyuan Zhai, Yaodong Yu, Xiaojun Yuan, Heung-Yeung Shum, et al. Ctrl: Closed-loop\ntranscription to an ldr via minimaxing rate reduction. Entropy, 24(4):456, 2022.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009.\n10\nPreprint. Under review.\nNat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni, Kai\nArulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture varia-\ntional autoencoders. arXiv preprint arXiv:1611.02648, 2016.\nJeff Donahue, Philipp Krähenbühl, and Trevor Darrell. Adversarial feature learning. arXiv preprint\narXiv:1605.09782, 2016.\nVincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky,\nand Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.\nWilliam Falcon, Ananya Harsh Jha, Teddy Koker, and Kyunghyun Cho. Aavae: Augmentation-\naugmented variational autoencoders. arXiv preprint arXiv:2107.12329, 2021.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information\nprocessing systems, 27, 2014.\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\net al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural\nInformation Processing Systems, 33:21271–21284, 2020a.\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi\nAzar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent:\nA new approach to self-supervised learning. In NeurIPS, 2020b.\nKamal Gupta, Saurabh Singh, and Abhinav Shrivastava. Patchvae: Learning local latent codes\nfor recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 4746–4755, 2020.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 9729–9738, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked\nautoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural\ninformation processing systems, 30, 2017.\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,\nShakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a\nconstrained variational framework. arXiv preprint arXiv:1804.03599, 2016.\nLiang Hou, Huawei Shen, Qi Cao, and Xueqi Cheng. Self-supervised gans with label augmentation.\nAdvances in Neural Information Processing Systems, 34, 2021.\nUiwon Hwang, Heeseung Kim, Dahuin Jung, Hyemi Jang, Hyungyu Lee, and Sungroh Yoon. Stein\nlatent optimization for generative adversarial networks. arXiv preprint arXiv:2106.05319, 2021.\nJongheon Jeong and Jinwoo Shin.\nTraining gans with stronger augmentations via contrastive\ndiscriminator. arXiv preprint arXiv:2103.09742, 2021.\nZhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep embed-\nding: An unsupervised and generative approach to clustering. arXiv preprint arXiv:1611.05148,\n2016.\nSheena A. Josselyn and Susumu Tonegawa. Memory engrams: Recalling the past and imagining the\nfuture. Science, 367, 2020.\nGeorg B Keller and Thomas D Mrsic-Flogel. Predictive processing: A canonical cortical computation.\nNeuron, 100(2):424–435, October 2018.\n11\nPreprint. Under review.\nSaehoon Kim, Sungwoong Kim, and Juho Lee. Hybrid generative-contrastive representation learning.\narXiv preprint arXiv:2106.06162, 2021.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\nThe CIFAR-10 dataset.\nonline:\nhttp://www.cs.toronto.edu/kriz/cifar.html, 55, 2014.\nAlex Krizhevsky et al. Learning multiple layers of features from tiny images. arXiv preprint\narXiv:1312.6114, 2009.\nZengyi Li, Yubei Chen, Yann LeCun, and Friedrich T Sommer. Neural manifold clustering and\nembedding. arXiv preprint arXiv:2201.10000, 2022.\nSteven Liu, Tongzhou Wang, David Bau, Jun-Yan Zhu, and Antonio Torralba. Diverse image\ngeneration via self-conditioned gans. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 14286–14295, 2020.\nYi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of multivariate mixed data via\nlossy data coding and compression. PAMI, 2007.\nSudipto Mukherjee, Himanshu Asnani, Eugene Lin, and Sreeram Kannan. Clustergan: Latent space\nclustering in generative adversarial networks. In Proceedings of the AAAI conference on artiﬁcial\nintelligence, volume 33, pp. 4610–4617, 2019.\nGaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive generative\nautoencoder. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 823–832, 2021.\nVignesh Prasad, Dipanjan Das, and Brojeshwar Bhowmick. Variational clustering: Leveraging\nvariational autoencoders for image clustering. In 2020 International Joint Conference on Neural\nNetworks (IJCNN), pp. 1–10. IEEE, 2020.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans. Advances in neural information processing systems, 29,\n2016.\nShengbang Tong, Xili Dai, Ziyang Wu, Mingyang Li, Brent Yi, and Yi Ma. Incremental learning of\nstructured memory via closed-loop transcription. arXiv:2202.05411, 2022.\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\nGido M Ven, Hava T Siegelmann, Andreas S Tolias, et al. Brain-inspired replay for continual learning\nwith artiﬁcial neural networks. Nature Communications, 11(1):1–14, 2020.\nZhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error\nvisibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600–612, 2004.\ndoi: 10.1109/TIP.2003.819861.\nYaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and\ndiscriminative representations via the principle of maximal coding rate reduction. Advances in\nNeural Information Processing Systems, 33:9422–9434, 2020.\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised\nlearning via redundancy reduction. In International Conference on Machine Learning, pp. 12310–\n12320. PMLR, 2021.\n12\nPreprint. Under review.\nA\nTRAINING DETAILS\nA.1\nNETWORK ARCHITECTURES\nTable 6, 7 and Figure 6 give details on the network architecture for the decoder and the encoder\nnetworks used for experiments. The black rectangle marked with \"conv, s=2\" means a convlutional\nlayer with stride 2. The orange rectangle marked with \"dconv, s=2\" means a deconvolutional layer\nwith stride 2. The \"x k\" besides red frame means we regard these layers in red frame as a block\nand stack it k times. All α values in Leaky-ReLU (i.e. lReLU) of the encoder are set to 0.2. We\nset (nz = 128, nc = 3, k = 3) for CIFAR-10, (nz = 256, nc = 3, k = 4) for CIFAR-100, and\n(nz = 256, nc = 3, k = 4) for Tiny-ImageNet. As a comparison, ResNet-18 contains around 11\nmillion parameters, whereas our encoder only contains between 4 and 6 million parameters depending\non the choice of k.\nTable 8 gives details of the network architecture for the linear classiﬁer and Table 9 gives details\nof the network architecture for the additional MLP head used for unsupervised conditional image\ngeneration training.\nz ∈R1×1×nz\nResBlockUp. 256\nResBlockUp. 128\nResBlockUp. 64\n4 × 4, stride=2, pad=1 deconv. 1 Tanh\nTable 6: Network architecture of the decoder g(·, η).\nImage x ∈R32×32×nc\nResBlockDown 64\nResBlockDown 128\nResBlockDown 256\n4 × 4, stride=1, pad=0 conv nz\nTable 7: Network architecture of the encoder f(·, θ).\n(a) ResBlock Up architecture\n(b) ResBlock Down architecture\nFigure 6: Architecture of two ResBlock.\nA.2\nOPTIMIZATION\nFor all experiments, we use Adam (Kingma & Ba, 2014) as our optimizer, with hyperparameters\nβ1 = 0.5, β2 = 0.999. The learning rate is set to be 0.0001. We choose ϵ2 = 0.2. For all experiments,\nwe adopt augmentation from SimCLR (Chen et al., 2020b).\n13\nPreprint. Under review.\nz ∈R1×1×nz\nLinear(nz, number of class)\nTable 8: Network architecture of the linear classiﬁer.\nz ∈R1×1×nz\nLinear(nz, nz) ReLU\nLinear(nz, number of clusters)\nTable 9: Network architecture of the MLP head for unsupervised conditional image generation\nFor CIFAR-10, CIFAR-100, and Tiny ImageNet, we train our framework with a batch size of 1024\nover 20,000 iterations. All experiments are conducted with at most 4 RTX 3090 GPUs. Methods that\nare compared against in Table 3 are trained with the batch size of 256, because Chen et al. (2020b)\nobserve that purely discriminative methods tend to perform better with smaller batch sizes. Table 2\nmethods have used their optimal parameters in their github code.\nFor training of the MLP head for unsupervised conditional image generation(10), we again use Adam\n(Kingma & Ba, 2014) as our optimizer with hyperparameters β1 = 0.5, β2 = 0.999. We choose the\nlearning rate to be 0.0001 and ϵ2 as 0.2, with batch size 1024 over 5000 iterations.\nFor training of the linear classiﬁer, we use Adam (Kingma & Ba, 2014) as our optimizer with\nhyperparameters β1 = 0.5, β2 = 0.999. We choose learning rate to be 0.0001, with batch size 1024\nover 5000 iterations.\nB\nADDITIONAL UNSUPERVISED CLUSTERING AND GENERATION RESULTS\nB.1\nCLUSTER RECONSTRUCTION\nIn this subsection, we visualize the reconstruction of ten clusters that are predicted and generated by\nU-CTRL on the CIFAR-10 training set. Each block in Figure 7 contains both a random sample of\nreconstructed data in a cluster and the total number of samples within it. Note that CIFAR-10 contains\n50,000 training samples, split across 10 classes. As we see in Figure 7, the number of samples in\neach cluster are very close to 5,000, with the largest deviator (cluster 9) containing 3,942 samples.\nWithout any cues, one can easily identify correspond each unsupervised cluster with a CIFAR-10\nclass. For a class like ‘bird’, we observe that the model is able to group images of standing birds,\nﬂying birds, and bird heads, despite their visual differences.\nB.2\nUNSUPERVISED CONDITIONAL IMAGE GENERATION\nBuilding on U-CTRL’s ability to cluster CIFAR-10 samples, we demonstrate the model’s ability to\nperform unsupervised conditional image generation in Figure 8. In contrast to reconstruction, where\nimages are regenerated from features corresponding to real samples, we generate images based on\nthe feature sampling technique proposed in (Dai et al., 2022). From these results, we observe that\nthe U-CTRL framework maintains in-cluster diversity, and that the diversity can be recovered and\nvisualized via simple principal component analysis.\nC\nABLATION STUDIES\nC.1\nTHE IMPORTANCE OF EACH TERM IN U-CTRL FORMULATION\nIn this section, we study the signiﬁcance of the sample-wise constraints and extra rate distortion term\nin the formulation 7. Table 10 presents the following objectives that we study:\n• Objective I is the constrained U-CTRL maximin 7.\n• Objective II is the constrained maximin without the augmentation compression constraint 6.\n14\nPreprint. Under review.\n(a) Cluster 1\n(b) Cluster 2\n(c) Cluster 3\n(d) Cluster 4\n(e) Cluster 5\n(f) Cluster 6\n(g) Cluster 7\n(h) Cluster 8\n(i) Cluster 9\n(j) Cluster 10\nFigure 7: More result on the reconstruction of clusters in CIFAR-10\n• Objective III is the constrained maximin without the sample-wise self-consistency constraint 5.\n• Objective IV is the constrained maximin without the extra rate distortion term.\n• Objective V is the U-CTRL without the augmentation compression constraint and sample-wise\nself-consistency constraint.\n• Objective VI is the CTRL-Binary maximin formulation 4.\nTable 11 shows the result of a linear probe for representations trained using each objective on CIFAR-\n10. From the table, it is evident that both constraints and the rate distortion term are pivotal to the\nsuccess of our framework.\nObjective I:\nmaxθ minη R(Z) + ∆R(Z, ˆZ) s.t. P\ni∈N ∆R(zi, ˆzi) = 0, and P\ni∈N ∆R(zi, zi\na) = 0\nObjective II:\nmaxθ minη R(Z) + ∆R(Z, ˆZ) s.t. P\ni∈N ∆R(zi, ˆzi) = 0\nObjective III:\nmaxθ minη R(Z) + ∆R(Z, ˆZ) s.t. P\ni∈N ∆R(zi, zi\na) = 0\nObjective IV:\nmaxθ minη ∆R(Z, ˆZ) s.t. P\ni∈N ∆R(zi, ˆzi) = 0, and P\ni∈N ∆R(zi, zi\na) = 0\nObjective V:\nmaxθ minη R(Z) + ∆R(Z, ˆZ)\nObjective VI:\nmaxθ minη ∆R(Z, ˆZ)\nTable 10: Five different objective functions for U-CTRL.\n15\nPreprint. Under review.\n(a) Cluster 1\n(b) Cluster 2\n(c) Cluster 3\n(d) Cluster 4\n(e) Cluster 5\n(f) Cluster 6\n(g) Cluster 7\n(h) Cluster 8\n(i) Cluster 9\n(j) Cluster 10\nFigure 8: Unsupervised conditional image generation on CIFAR-10. Each block represents a cluster, within\nwhich each row represents one principal component direction in the cluster, and samples along each row represent\ndifferent noises applied in that principal direction.\nC.2\nTHE IMPORTANCE OF MCR2 IN U-CTRL FORMULATION\nIn this section, we verify the signiﬁcance of MCR2 term ∆R(Z, ˆZ) in our method. We do ablation\nstudy on CIFAR-10 with the same network and training condition. If we take away MCR2 from our\n16\nPreprint. Under review.\nMethod\nObjective I\nObjective II\nObjective III\nObjective IV\nObjective V\nObjective VI\nAccuracy\n0.874\n0.578\n0.644\n0.522\n0.633\n0.599\nTable 11: Ablation study on the signiﬁcance of different terms in U-CTRL.\nformulation, it changes (11). For simplicity, we call it U-CTRL-noMCR2\nmax\nθ\nmin\nη\nR(Z)\n(11)\nsubject to\nX\ni∈N\n∆R(zi, ˆzi) = 0, and\nX\ni∈N\n∆R(zi, zi\na) = 0.\nTable 12 shows that U-CTRL without the MCR2 not only learns worse representation but also\ngeneralizes worse to out of distribution data. Figure 9 visualizes the reconstructed ˆ\nX by U-CTRL-\nnoMCR2. It is clear from the image ﬁgure that without the MCR2, the decoder fails to reconstruct\nhigh-quality images.\nAccuracy on CIFAR-10\nTransfer Accuracy on CIFAR-100\nU-CTRL\n0.874\n0.481\nU-CTRL-noMCR2\n0.836\n0.418\nTable 12: Ablation study on the signiﬁcance of MCR2 in U-CTRL.\n(a) CIFAR-10 X\n(b) CIFAR-10 ˆ\nX\nFigure 9: Visualization of images trained by U-CTRL-noMCR2: X and reconstructed ˆ\nX on CIFAR-10 dataset.\nIt follows our discussion in the introduction that discriminative tasks and generative tasks together\nlearn feature that beniﬁts each other.\nD\nRANDOM SEED SENSITIVITY\nIn this section, we verify the stability of our method against different random seeds. We report in\nTable 13 the accuracy of U-CTRL on CIFAR-10 with different seeds. We observe that the choice of\nseed has very little impact on performance.\nRandom Seed\n1\n5\n10\n15\n100\nAccuracy\n0.874\n0.876\n0.870\n0.874\n0.871\nTable 13: Ablation study on varying random seeds.\n17\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2022-10-30",
  "updated": "2022-10-30"
}