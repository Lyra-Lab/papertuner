{
  "id": "http://arxiv.org/abs/2210.17368v1",
  "title": "Teacher-student curriculum learning for reinforcement learning",
  "authors": [
    "Yanick Schraner"
  ],
  "abstract": "Reinforcement learning (rl) is a popular paradigm for sequential decision\nmaking problems. The past decade's advances in rl have led to breakthroughs in\nmany challenging domains such as video games, board games, robotics, and chip\ndesign. The sample inefficiency of deep reinforcement learning methods is a\nsignificant obstacle when applying rl to real-world problems. Transfer learning\nhas been applied to reinforcement learning such that the knowledge gained in\none task can be applied when training in a new task. Curriculum learning is\nconcerned with sequencing tasks or data samples such that knowledge can be\ntransferred between those tasks to learn a target task that would otherwise be\ntoo difficult to solve. Designing a curriculum that improves sample efficiency\nis a complex problem. In this thesis, we propose a teacher-student curriculum\nlearning setting where we simultaneously train a teacher that selects tasks for\nthe student while the student learns how to solve the selected task. Our method\nis independent of human domain knowledge and manual curriculum design. We\nevaluated our methods on two reinforcement learning benchmarks: grid world and\nthe challenging Google Football environment. With our method, we can improve\nthe sample efficiency and generality of the student compared to tabula-rasa\nreinforcement learning.",
  "text": "Master Thesis\nIn partial fulﬁlment of the requirements for the Master of Science in\nEngineering\nTeacher-student curriculum\nlearning for reinforcement\nlearning\nYanick Schraner\nSpring Term 2022\nThesis Advisor:\nProf. Dr. Manfred Vogel, University of Applied Sciences Northwestern\nSwitzerland\narXiv:2210.17368v1  [cs.LG]  31 Oct 2022\nDeclaration of Originality\nI hereby declare that the written work I have submitted entitled\nTeacher-student curriculum learning for reinforcement learning\nis original work which I alone have authored and which is written in my own\nwords.\nAuthor\nYanick\nSchraner\nSupervisor\nManfred\nVogel\nWith the signature I declare that I have been informed regarding normal academic\ncitation rules. The citation conventions usual to the discipline in question here\nhave been respected.\nPlace and date\nSignature\nContents\nAbstract\niv\nAcknowledgements\nv\nSymbols\nvi\n1\nIntroduction\n1\n1.1\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.2\nPrevious Work\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.3\nProblem Statement . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2\nBackground\n7\n2.1\nReinforcement Learning\n. . . . . . . . . . . . . . . . . . . . . . .\n7\n2.1.1\nFinite Markov Decision Processes . . . . . . . . . . . . . .\n8\n2.1.2\nPolicy and Value Functions\n. . . . . . . . . . . . . . . . .\n9\n2.1.3\nDeep Reinforcement Learning . . . . . . . . . . . . . . . .\n11\n2.1.4\nPolicy Gradient Methods . . . . . . . . . . . . . . . . . . .\n11\n2.1.5\nProximal Policy Optimization . . . . . . . . . . . . . . . .\n13\n2.1.6\nChallenges of Deep Reinforcement Learning\n. . . . . . . .\n14\n2.2\nTransfer Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.2.1\nEvaluation Metrics for Transfer Learning . . . . . . . . . .\n16\n2.3\nCurriculum\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n2.4\nCurriculum Learning . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.4.1\nCurriculum Learning Categorization\n. . . . . . . . . . . .\n20\n2.4.2\nTask sequencing . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3\nMethods\n23\n3.1\nCurriculum Learning . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.1.1\nTransfer Method\n. . . . . . . . . . . . . . . . . . . . . . .\n24\n3.2\nTeacher Curriculum Markov Decision Process\n. . . . . . . . . . .\n25\n3.2.1\nTeacher Observation\n. . . . . . . . . . . . . . . . . . . . .\n26\n3.2.2\nReward Signal . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.2.3\nAction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.2.4\nNetwork Architecture . . . . . . . . . . . . . . . . . . . . .\n29\n3.3\nBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n3.4\nEvaluation Protocol . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n4\nExperiments and Results\n31\n4.1\nGrid World\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nii\n4.1.1\nGrid World Environments . . . . . . . . . . . . . . . . . .\n32\n4.1.2\nState & Observations . . . . . . . . . . . . . . . . . . . . .\n32\n4.1.3\nActions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n4.1.4\nRewards . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n4.1.5\nMDP Statement . . . . . . . . . . . . . . . . . . . . . . . .\n34\n4.2\nGrid World Experiments . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.2.1\nExperimental Setup . . . . . . . . . . . . . . . . . . . . . .\n35\n4.2.2\nResults Overview . . . . . . . . . . . . . . . . . . . . . . .\n37\n4.2.3\nTransfer Method\n. . . . . . . . . . . . . . . . . . . . . . .\n39\n4.2.4\nTeacher Reward Signal . . . . . . . . . . . . . . . . . . . .\n44\n4.2.5\nChoosing Alpha for Exponential Moving Average\n. . . . .\n45\n4.2.6\nSample Eﬃciency . . . . . . . . . . . . . . . . . . . . . . .\n47\n4.2.7\nGenerality of Agents . . . . . . . . . . . . . . . . . . . . .\n49\n4.3\nGoogle Football Environment\n. . . . . . . . . . . . . . . . . . . .\n51\n4.3.1\nState & Observations . . . . . . . . . . . . . . . . . . . . .\n52\n4.3.2\nActions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n4.3.3\nRewards . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n4.3.4\nFootball Academy Scenarios . . . . . . . . . . . . . . . . .\n53\n4.3.5\nMDP Statement . . . . . . . . . . . . . . . . . . . . . . . .\n53\n4.4\nGoogle Football Experiments\n. . . . . . . . . . . . . . . . . . . .\n55\n4.4.1\nExperimental Setup . . . . . . . . . . . . . . . . . . . . . .\n55\n4.4.2\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n5\nDiscussion\n60\n6\nConclusion\n63\nBibliography\n69\nA Appendix\n74\nA.1 Grid World CNN Architecture . . . . . . . . . . . . . . . . . . . .\n74\nA.2 Grid World Student Hyperparameters . . . . . . . . . . . . . . . .\n75\nA.3 Google Football Student Hyperparameters . . . . . . . . . . . . .\n76\nA.4 Teacher Hyperparameters\n. . . . . . . . . . . . . . . . . . . . . .\n77\nA.5 MLP vs. LSTM Teacher Model . . . . . . . . . . . . . . . . . . .\n78\nAbstract\nReinforcement learning (rl) is a popular paradigm for sequential decision making\nproblems. The past decade’s advances in rl have led to breakthroughs in many\nchallenging domains such as video games, board games, robotics, and chip design.\nThe sample ineﬃciency of deep reinforcement learning methods is a signiﬁcant\nobstacle when applying rl to real-world problems. Transfer learning has been\napplied to reinforcement learning such that the knowledge gained in one task can\nbe applied when training in a new task. Curriculum learning is concerned with\nsequencing tasks or data samples such that knowledge can be transferred between\nthose tasks to learn a target task that would otherwise be too diﬃcult to solve.\nDesigning a curriculum that improves sample eﬃciency is a complex problem. In\nthis thesis, we propose a teacher-student curriculum learning setting where we\nsimultaneously train a teacher that selects tasks for the student while the student\nlearns how to solve the selected task.\nOur method is independent of human\ndomain knowledge and manual curriculum design. We evaluated our methods on\ntwo reinforcement learning benchmarks: grid world and the challenging Google\nFootball environment. With our method, we can improve the sample eﬃciency\nand generality of the student compared to tabula-rasa reinforcement learning.\niv\nAcknowledgements\nI want to thank everyone who supported me throughout this thesis. I am thankful\nfor the moral support, open door for my thoughts and concerns, and advice during\nmy thesis work.\nFirstly, I would like to extend my special thanks to my advisor Prof. Dr. Manfred\nVogel, who supported me in my thesis and throughout my master’s studies. I am\nthankful for his criticism, humor, and his guidance during this time.\nI would also like to thank Christian Scheller and Lukas Neukom for their proof-\nreading and valuable comments on this thesis.\nv\nSymbols\nSymbols\nThe following list describes important notations used in this work.\nE[X]\nexpectation of a random variable X\nα, β\nstep-size parameters\nγ\ndiscount-rate parameter\ns, s′ ∈S\nstates, s′ is subsequent step of s\na ∈A\nan action\nr ∈R\na reward\nS\nset of all states\nA(s)\nset of all actions available in state s\nR\nset of all possible rewards, a ﬁnite subset of R\nt\ndiscrete time step\nT\nﬁnal time step of an episode\nat\naction at time t\nst\nstate at time t\nrt\nreward at time t\nπ\npolicy\nπ(s)\naction taken in state s under deterministic policy π\nπ(a|s)\nprobability of taking action a in state s under stochastic policy π\np(s′|s, a)\nprobability of transition to state s′, from state s taking action a\nr(s, a)\nexpected immediate reward from state s after action a\nV π(s)\nstate-value function for policy π\nV ∗(s)\noptimal state-value function\nQπ(s, a)\naction-value function under policy π\nQ∗(s, a)\noptimal action-value function\nθ, θt\nparameter vector of target policy\nπ(a|s, θ)\nprobability of taking action a in state s given parameter vector θ\nπθ\npolicy corresponding to parameter θ\n∇π(a|s, θ) column vector of partial derivatives of π(a|s, θ) with respect to θ\nJ(θ)\nperformance measure for the policy πθ\n∇J(θ)\ncolumn vector of partial derivatives of J(θ) with respect to θ\nV\nSet of all task used in curriculum learning\nvi\nAcronyms and Abbreviations\nALP\nAbsolute learning progress section 3.2.1\nEMA\nExponential moving average section 3.2.1\nFS-EMA Fast and slow exponential moving average section 3.2.1\nLP\nLearning progress section 3.2.1\nPTR\nPrevious task reward section 3.2.1\nRH\nReward history section 3.2.1\nRL\nReinforcement Learning section 2.1\nSMM\nSuper Mini Map section 4.3\nMDP\nMarkov decision problem section 4.3.5\nPOMDP Partially observable Markov decision problem\nPPO\nProximity Policy Optimization section 2.1.5\nChapter 1\nIntroduction\nReinforcement learning is based on the idea of learning through interaction with\nan environment. Learning through interaction is a natural way to gain new skills\nand a concept underlying nearly all theories of learning and intelligence. Infants\nlearn a lot about the cause and eﬀect of their actions by observing the results\nof their actions. We learn how to achieve speciﬁc goals solely by observing the\njoy of actions leading to a pleasant state in our environment (like grabbing our\nfavorite toys) or getting harmed by other actions (like touching a hot plate).\nReinforcement learning (RL) builds upon this idea of learning through interac-\ntion. As supervised learning (SL) and unsupervised learning (UL), reinforcement\nlearning is a machine learning paradigm. Due to its generality, researchers apply\nRL to a broad range of tasks. The ongoing rise of deep learning enabled rein-\nforcement learning on high-dimensional input and highly complex environments.\nSuccessful applications are board games [Silver et al., 2016, 2017, 2018], video\ngames [Mnih et al., 2015, OpenAI et al., 2019a, Vinyals et al., 2019] and robotics\n[OpenAI et al., 2019b].\nThe use of reinforcement learning in a practical setting is often not realistic\ndue to the sample ineﬃciency of RL. RL agents usually require a large number\nof interactions with the environment to learn proper behavior. For example, the\ntraining of OpenAI Five [OpenAI et al., 2019a] took two months with 150 PFlops\n/ day to surpass professional human-level performance. Training Agent57 [Badia\net al., 2020] for the Atari benchmark required 90 billion environment frames to\nreach human performance.\nThere are multiple ways to improve the sample eﬃciency of reinforcement learn-\ning. Imitation learning methods make use of an existing expert demonstration\ndataset to learn from expert priors. The simplest form of imitation learning is be-\nhavior cloning, where a policy is learned in a supervised learning fashion [Pomer-\nleau, 1989]. This method suﬀers from the distribution mismatch of the training\ndata and the actual environment. A slight divergence of trajectories present in\nthe dataset during inference leads to situations unknown to the trained policy.\nRoss et al. [2011] use a human in the loop process to counter the distribution mis-\nmatch by continuously labeling the data for those unknown situations. Another\noption is to use reinforcement learning to ﬁne-tune the initial policy [Scheller\net al., 2020, Vinyals et al., 2019]. A diﬀerent popular strategy to make RL more\n1\n2\n1.1. Related Work\nsample eﬃcient is to exploit the hierarchical structure of the given problem [Sut-\nton et al., 1998, Dayan and Hinton, 1993, Sutton et al., 1999a]. One can imagine\ntraining a sub-policy for every individual task and a general policy that chooses\none of those sub-policies. Another way to improve sample eﬃciency is curriculum\nlearning (CL), where the learning process is structured so that new concepts are\nlearned in a sequence and an agent can leverage what he previously learned. Let\nus consider maths as an analogy: We ﬁrst have to learn basic arithmetic before\nwe can do linear algebra. The idea of using such a structured learning process to\ntrain artiﬁcial agents dates back to Elman [1993] for grammar learning.\nIn this work, we build upon the idea of curriculum learning to improve the sample\neﬃciency of our RL approach. There are many ways to deﬁne a curriculum. We\nwill specify those in section 2.3. Our work focuses on how we can automatically\ndeﬁne such a curriculum during training by phrasing the task sequencing problem\nas a meta Markov decision process.\nWe present a novel curriculum learning\nframework where no domain knowledge for the task sequencing is required. This\nframework aims to increase the sample eﬃciency as well as the asymptotic reward\ncompared to vanilla reinforcement learning and a set of heuristic-based baselines.\nThis work builds upon a previous project where we applied manually designed\ncurriculum learning on a football simulation. In this previous work, we were able\nto increase the sample eﬃciency of our algorithm and the asymptotic reward to 1\ncompared to vanilla reinforcement learning with a reward of -1.4 on the 11 vs. 11\nhard environment. In section 1.2 we provide a brief recapitulation of the previous\nwork and its ﬁndings.\n1.1\nRelated Work\nOur work lies in the ﬁeld of curriculum learning in reinforcement learning. CL\ncomprises three key elements: transfer learning, task sequencing, and task gen-\neration. In this work, we focus on transfer learning and task sequencing.\nTransfer learning\nis a ﬁeld in machine learning which studies how knowledge\ngained by solving a source task can be transferred to a target problem such that\nthe target task can be solved faster. In reinforcement learning, transfer learning\nincreases the sample eﬃciency or the performance on the target task. Knowledge\ncan be transferred by selecting samples collected on a source task and use them as\ninput for batch reinforcement learning [Lazaric et al., 2008, Lazaric and Restelli,\n2011]. Options [Sutton et al., 1999b] or macro-actions can be extracted on a\nsource task and included in the action space of the target task [Soni and Singh,\n2006, Vezhnevets et al., 2016]. Models of the transition and reward function of\na source task can be transferred to allow a hybrid approach of model-free and\nmodel-based RL on the target task [Fachantidis et al., 2013]. The parameters of\n3\n1.1. Related Work\na learned policy or value function can also be used to initialize the policy or value\nfunction in the target task [Fern´andez et al., 2010, Taylor et al., 2007, Taylor and\nStone, 2005].\nThose methods make diﬀerent assumptions about the source and target Markov\nDecision Processes (MDP). The action or state space must be shared to allow\nthe initialization of a policy or value function. Alternatively, a task mapping for\nstates and actions of the source task to their equivalents of the target task is\nneeded in tabular reinforcement learning. For some methods, we need access to\na model of the source task, and not all methods allow the use of multiple source\ntasks.\nIn our work, we focus on transfer learning through policy transfer and reward\nshaping.\nWe refer to the surveys on transfer learning for reinforcement learning provided\nby Taylor and Stone [2009], Lazaric [2012] for more insights.\nTask sequencing\nis concerned with how the tasks can be sequenced in order\nto provide a curriculum. The sequencing of those tasks is crucial in CL. If the\ntasks at the beginning of the sequence are too hard, then the agent may fail to\nlearn and is unable to solve the following tasks. The goal is to sequence the task\ninto a curriculum which allows the agent to learn with a higher sample eﬃciency\nand ﬁnal performance compared to an agent trained solely on the target ask. A\nsuitable task sequence depends on the set of tasks, the agent’s characteristics,\nthe target task, and is domain-speciﬁc.\nTo perform task sequencing, we must control the environment to a certain de-\ngree to create diﬀerent tasks. Depending on the level of control we have, we can\napply diﬀerent task sequencing methods. In practice, the ﬁrst approach to task\nsequencing is made manually. We focus on how we can automatically generate a\ntask sequence for our curriculum. The simplest form is a single task curriculum\nwhere we reorder recorded experience without changing the MDP, referred to as\nsample sequencing. We collect this experience into an experience buﬀer during\nagent-environment interactions. This idea has its roots in supervised learning,\nwhere the training data is sampled in a speciﬁc order to speed up the training\nprocess [Bengio et al., 2009]. One example of sample sequencing in RL is Pri-\noritized Experience Replay for DQN and its follow-up work [Schaul et al., 2016,\nAndrychowicz et al., 2017, Ren et al., 2018, Kim and Choi, 2018].\nIn multi-agent environments, we can create a curriculum by controlling the in-\nteraction of agents in the same environment. This approach to task sequencing\nis called co-learning. In its simplest form, one performs self-play where an agent\ncompetes against or acts cooperatively with an older version of itself. The agent\nto train and his counterpart gets progressively better on the task and therefore\n4\n1.1. Related Work\ncreates an implicit curriculum. This setup has proven to be successful in the\nwell-known Go AI Alpha Go and its successors [Silver et al., 2016, 2018]. In\nVinyals et al. [2019] the idea of self-play was extended to a league setting. The\ngoal of this league setup is that the agent faces increasingly stronger opponents\nand opponents trained to perform speciﬁc strategies to exploit weaknesses and\nprevent mode collapse.\nIn co-learning and sample sequencing, the target MDP is not changed, and no\nspeciﬁc level of control over the environment is required.\nIf control over the\nenvironment is possible, one can change the target MDP by changing the initial\nstate distribution or the reward function to create a suitable task sequence. One\nexample of this approach is the reverse curriculum generation, where a robot is\nlearning to reach a goal from a set of starting positions increasingly far from the\ngoal [Florensa et al., 2017].\nFoglino et al. [2019a] uses metaheuristic search methods such as beam search,\ntabu search, genetic algorithms, or ant colony search in order to solve the task\nsequencing problem. In their follow-up paper, they compare those metaheuristic\nsearch methods to their Heuristic Task Sequencing for Cumulative Return (HTS-\nCR) algorithm [Foglino et al., 2019b].\nIn our work, we treat the task sequencing as an MDP [Narvekar et al., 2017,\nMatiisen et al., 2019, Narvekar and Stone, 2019] where we use reinforcement\nlearning in order to train a teacher agent to perform the task sequencing online.\nIn this setting, we formalize curriculum learning as an interaction of two MDPs.\nThe standard MDP modeling the learning agent interacting with the environment\nis referred to as student MDP. A meta level MDP for the curriculum agent to\nperform the task sequencing referred to teacher MDP.\nAt the time of writing, no MDP-based task sequencing method where both stu-\ndent and teacher use neural network-based function approximators and where\nboth are trained using reinforcement learning is known to the author. Therefore\nthe key focus is to propose such a framework and analyze the eﬃciency of such\nan approach.\nTask generation\nspeciﬁes how and when the source tasks for the curriculum\nare deﬁned. The quality of its source tasks heavily inﬂuences the quality of a\ncurriculum. The tasks can either be created beforehand or online during training.\nThe goal of task generation is to create a set of source tasks that allow a knowledge\ntransfer through them such that it is easier to solve the target task.\nIn Narvekar et al. [2016], a method for creating a set of source tasks by speci-\nfying task descriptors, that are controlling the degrees of freedom of the task, is\nintroduced. Those task descriptors specify the environment like the environment\nsize, action set, opponent type, initial states, et cetera. In Powerplay [Schmid-\n5\n1.2. Previous Work\nhuber, 2013] a framework where new tasks are generated online is introduced.\nThe system searches for new source tasks such that the agent becomes more and\nmore general. A ﬁxed computation budget is applied to force the task generator\nto create new tasks that are only slightly more diﬃcult than the previous ones.\nJiang et al. [2020] introduced the idea of prioritized level replay. In reinforcement\nlearning environments, there is usually a level identiﬁer. This could be a level\nindex or a random seed. Usually, the level to use is sampled uniformly. Which\nlevel is sampled can inﬂuence the diﬃculty of the task as well as the environment\ndynamics. This diversity among levels implies that diﬀerent levels hold diﬀerent\nlearning potentials for RL agents at any point in training. Prioritized level re-\nplay introduces a new level sampling strategy to prioritize levels based on their\nlearning potential creating an implicit curriculum.\n1.2\nPrevious Work\nIn previous work, we tried to reproduce the work of Kurach et al. [2019] on the\nGoogle Research Football environment. We also implemented and evaluated ex-\ntensions to the Proximal Policy Optimization (PPO) algorithm such as Actor with\nVariance Estimated Critic (AVEC) [Flet-Berliac et al., 2020]. Additionally, we\ncarried out experiments with curriculum learning in the Google football environ-\nment. Detailed information about this environment can be found in section 4.3.\nWe evaluated two manually deﬁned curricula as well as two automatically gen-\nerated curricula, and a prioritized level replay curriculum. We used the weights\nobtained by training on the source task as an initialization for the policy on the\ntarget task as a transfer method.\nThe results of our previous work are summarized in table 1.1 for a comparison in\nsection 4.4.2. Scenarios curriculum and 11 vs 11 curriculum are both manually\ndeﬁned curricula, the ﬁrst one over both a set of football academy tasks (see\ntable 4.8) and the 11 vs 11 full game on diﬃculty easy, medium and hard. The\nsecond curriculum only uses the 11 vs 11 full game environments. The increasing\ncurriculum consistently increases the game diﬃculty parameter δ by 0.05 starting\nfrom 0.01 and ﬁnishing at 0.95. A δ value of 0.95 is equivalent to the 11 vs 11\nhard environment.\nAt the ﬁrst task change, we increase δ from 0.01 to 0.05\ninstead. The diﬃculty is increased automatically after 100 training iterations.\nSmooth increasing curriculum is similar to the increasing curriculum. Instead of\nadapting the diﬃculty depending on training iterations, we increase the diﬃculty\nas soon as the average return is greater than 0.9. Additionally, we increment δ\nby 0.001, start with a value of 0.001 and end with a δ value of 0.95.\nAlthough four out of ﬁve curricula improve our results, only the smooth increasing\ncurriculum can outperform the baseline signiﬁcantly. Using only the 11 vs. 11\ntasks includes too hard tasks at the early stages of learning. The smooth increas-\n6\n1.3. Problem Statement\ning curriculum is superior over the increasing curriculum as the task diﬃculty\nincreases more evenly. Scenarios curriculum includes task switches with signiﬁ-\ncant changes in their initial state distribution and observation space, making this\ntype of curriculum worse than the increasing ones.\nExperiment\nReturn\nPPO\n-1.4\nBest scenarios curriculum\n-0.85\nBest 11 vs 11 curriculum\n-2.08\nBest increasing curriculum\n-0.48\nBest smooth increasing curriculum\n1\nPrioritized level replay\n-1.05\nBaseline\n-1.39\nTable 1.1: Comparison of our diﬀerent curriculum learning approaches, the aver-\nage return over 100 episodes on the 11 vs 11 hard environment is reported.\n1.3\nProblem Statement\nIn this work, we are developing a teacher-student curriculum learning setup fo-\ncusing on online task sequencing and transfer learning. We manually do the task\ngeneration and focus on the task sequencing problem formulated as a curriculum\nMDP [Narvekar et al., 2017]. Both the teacher and student are policy gradient\nreinforcement learning agents using neural networks as function approximators\nand are trained with PPO. We aim to answer the following research questions:\n• Is it possible to train both teacher and student with the curriculum MDP\n(CMDP) setting from scratch such that the sample eﬃciency or average\nreward on the target task increases compared to vanilla PPO on the target\ntask?\n• What is a suitable reward function for our teacher agent to increase the\nsample eﬃciency or average reward on the target task?\n• How can we deﬁne observations in the CMDP such that the sample eﬃ-\nciency or average reward on the target task increases?\n• What is a suitable transfer method for the speciﬁed setting?\n• Are we able to improve the results of our previous work on the Google\nResearch Football environment with this curriculum learning framework?\nChapter 2\nBackground\nIn this chapter, we introduce the core ideas and theoretical foundations used\nin this work.\nWe introduce the reinforcement learning paradigm focusing on\npolicy gradient methods and highlight some core ideas important in this work.\nWe introduce the proximal policy optimization (PPO) algorithm and additional\nRL improvements used in this work. Parts of this chapter were written in our\nprevious work [Schraner, 2020] and added to this thesis for a coherent document.\n2.1\nReinforcement Learning\nReinforcement learning is a popular framework suited to solve sequential decision-\nmaking processes. An agent learns how to act in an environment by observing\na numerical reward signal. The agent has to learn a policy to predict an action\nbased on the environment’s state, such that the cumulative reward is maximized.\nAt the early stages of learning, RL is very similar to trial and error learning.\nBy making progress, the learner usually continuously observes new states of the\nenvironment and learns how to act in those new states without forgetting about\nthe correct behavior in the early stages. The only indication wether a single or\na series of actions leads to preferable changes in the environment is the reward\nsignal. After every action, the agent receives a representation of the new environ-\nment state (observation) and a reward signal. This signal can be either positive\nor negative, and it does not tell the agent exactly which series of actions lead to\nthat signal. The reward can be delayed, for example, students receive a negative\nreward signal while studying for their exams, but they receive a large positive\nreward signal after successfully passing the exam. This way of learning makes\nRL a general framework, as stated by the reward hypothesis:\nThat all of what we mean by goals and purposes can be well thought\nof as the maximization of the expected value of the cumulative sum\nof a received scalar signal (called reward) [Sutton and Barto, 2018].\nBiological systems inspire the use of a reward signal to learn: through experienc-\ning pleasure or pain, we know what actions are good in an immediate sense.\nIn contrast to RL, supervised learning uses a training set of labeled examples.\nWith supervised learning, we are trying to capture the knowledge represented in\nthe training set. The use of a training set proved to be very powerful to learn\n7\n8\n2.1. Reinforcement Learning\nintelligent behavior, but it also makes supervised learning hard for interactive\nproblems. Often it is impractical or costly to obtain a useful set of examples\nof the desired behavior that is both correct and representative for all situations.\nThere is an intersection of RL and supervised learning called imitation learning.\nIn imitation learning, the agent receives an initial set of labeled demonstrations\nof actions in the environment and uses it in a supervised fashion. The agent may\nbe improved further with standard reinforcement learning.\nWith unsupervised learning, one can ﬁnd structure hidden in a large amount of\nunlabeled data. There is no reward signal to maximize in unsupervised learning,\nwhich diﬀerentiates it from RL.\n2.1.1\nFinite Markov Decision Processes\nMarkov decision processes (MDP) is a formalisation for sequential decision-making\nprocesses. A Markov decision process is a 4-tuple (S, A, p, r), where\n• S is a set of states\n• A is a set of actions\n• p(s′|s, a) = P[st+1 = s′|st = s, at = a] is the probability of transitioning\nfrom state s to state s′ when taking action a.\n• r(s, a) = E[rt+1|st = s, at = a] is the expected reward received by taking\naction a in state s.\nMDPs are an idealized form of the reinforcement learning problem and are used\nto formulate mathematically precise theoretical statements [Sutton and Barto,\n2018].\nFigure 2.1: The agent-environment interaction in a Markov decision process [Sut-\nton and Barto, 2018].\nThe agent-environment interaction in a MDP is shown in ﬁg. 2.1. The decision-\nmaker is called an agent. The agent interacts with the environment by following\na policy denoted by π.\nThe policy maps from state s to action a ∈A and\ncan either be deterministic π(s) = a or stochastic π(a|s) = Pπ[at = a|st =\ns]. Everything outside the agent is the environment. Taking an action in the\nenvironment leads to a change in the environment’s state. For example in the\n9\n2.1. Reinforcement Learning\ngrid world environment, an action can be moving forward, picking up a key, or\nopening a door. The environment receives the action of the agent and returns a\nreward r and a representation of its new state s ∈S. If the entire state of the\nenvironment is not observable, e.g., the agent has only a limited ﬁeld of view,\nthe agent receives an observation o ∈O instead of a state. The observation only\npartially describes the environment’s state. The environment can be deterministic\nor stochastic and may change itself without interactions from the agent. Agent\nand environment are interacting in a sequence of discrete-time steps t. t starts\nfrom 0 in the ﬁrst time-step, and goes up to T, with T being the last time-step.\nAt each time step, the agent receives a representation of the environment state\nst and a reward rt based on which he selects and executes action at. This leads\nto a sequence of interactions called trajectory: s0a0r1s1a1r2s2a2r3 . . . statrt. A\ntrajectory has to be ﬁnite.\nIn a ﬁnite MDP, the sets of states, actions and rewards all have a ﬁnite number\nof elements. The next state is only dependent on the preceding state-action pair.\nTherefore we have a well deﬁned discrete probability distributions for the next\nstate dependent only on the preceding state-action pair [Sutton and Barto, 2018]:\nP[st+1|st, at] = P[st+1|s1, a1, . . . , st, at]\n(2.1)\nThis is called the Markov property. To fulﬁll the Markov property, the probabili-\nties given by p can only depend on the preceding state and action and completely\ncharacterize the dynamics of the environment. Therefore, a single state must in-\nclude information about every aspect of the past agent-environment interactions\nthat make a diﬀerence for the future.\n2.1.2\nPolicy and Value Functions\nIn reinforcement learning, agents select their actions according to their policy\nfunction. A policy has to be time-independent, the trajectory up to the time\nstep t does not inﬂuence the action probabilities. The policy π(a|s) outputs a\nprobability distribution over all possible actions given a state:\nπ(a|s) = P[at = a|st = s]\n(2.2)\nThe objective of this policy is to maximize the cumulative future reward, also\ncalled return Gt:\nGt = rt+1 + γrt+2 + γ2rt+3 + · · · =\nT−1\nX\nk=t\nγkrt+k+1\n(2.3)\n10\n2.1. Reinforcement Learning\nThe discount factor γ ∈[0, 1] serves multiple purposes:\n• future rewards may have higher uncertainty\n• future rewards do not provide immediate beneﬁts. In some cases, immediate\nrewards are of more value, like in economics where we prefer the money now\nover later as we could invest it in maximizing future earnings.\n• the discount factor provides mathematical convenience, as it solves prob-\nlems with inﬁnite MDPs or loops in the state transition graph [Sutton and\nBarto, 2018]\nThe performance of a policy is measured by its state- and action-value functions.\nThose value functions for the policy π are the expectation of what return the\nagent receives by following policy π from state s ∈S. This estimation is called\nstate-value function V (s) (how good is it to be in a given state) or action-value\nfunction Q(s, a) (how good it is to perform a given action in a given state). The\nstate-value function is the expected return when following policy π from state s:\nV π(s) .= Eπ[Gt|st = s]\n(2.4)\nThe action-value function is the expected return when taking action a in state s\nand then following policy π:\nQπ(s, a) .= Eπ[Gt|st = s, at = a]\n(2.5)\nA policy π is considered better than another policy π′ if for all states s ∈S the\nexpected return is larger:\nV π(s) > V π′(s), ∀s ∈S\n(2.6)\nAn optimal policy π∗is a policy that is better or as good as any other policy in\nany state:\nV π∗(s) ≥V π(s), ∀s ∈S ∧∀π ∈Π\n(2.7)\nwhere Π is the set of all possible policies.\nThe optimal state-value function V ∗is the maximum expected return over all\npolicies when being in state s:\nV ∗(s) = maxπV π(s)\n(2.8)\nThe optimal action-value function Q∗is the maximum expected return over all\npolicies when being in state s and taking action a:\nQ∗(s, a) = maxπQπ(s, a)\n(2.9)\n11\n2.1. Reinforcement Learning\nTherefore, the optimal policy π∗can be obtained by acting greedily according to\nthe action that maximizes Q∗(s, a):\nπ∗(at|st) =\n(\n1,\nif at = argmaxa∈AQ∗(st, a)\n0,\notherwise\nStandard old fashion approaches to ﬁnd the optimal state-value function V ∗or\noptimal action-value function Q∗are Dynamic Programming, Monte-Carlo Meth-\nods, or Temporal-Diﬀerence Learning. We will not cover those methods in this\nreport and continue with deep reinforcement learning and policy gradient meth-\nods. See Sutton and Barto [2018] for an introduction of those traditional methods.\n2.1.3\nDeep Reinforcement Learning\nTabular methods work well for problems with a small state and action space. For\nsuch environments, it is easy to build a table with value or action-value estimates\nand act according to this table. Those problems are not very interesting and\nfar away from real-world applications. With growing state or action spaces, the\nmemory and computation needs are growing exponentially, with respect to the\nstate or action space, for tabular methods. It is also not feasible to visit every\nstate to ﬁll a table with value estimates.\nWith stochastic environments, the\nproblem gets even worse, as we would have to visit every state multiple times\nto calculate statistics. This raises the need for state approximation to generalize\nbetween similar states.\nThere are numerous ways to approximate states, but\nthe advances in deep learning made neural networks the preferred option as a\nfunction approximator. The work of Mnih et al. [2015] can be considered as the\nbreakthrough of mixing deep learning with reinforcement learning and allowing a\nsingle algorithm to learn how to play Atari games. One challenge of deep learning\nwith RL is that the optimization problem is non-stationary because the agent\nencounters new states in the environment during the ongoing learning progress.\n2.1.4\nPolicy Gradient Methods\nInstead of learning value functions and selecting actions based on those functions,\nwe can directly learn a parameterized policy that selects actions without consult-\ning a value function. This approach is called policy gradient method. In policy\ngradient methods the policy πθ is parametrized with a parameter vector θ ∈Rd.\nWhen using a neural network as a function approximator for the optimal policy,\nthe last layer of this network is usually a softmax layer in case of discrete actions.\nThe probability distribution will be close to a uniform distribution in the early\nlearning stages, leading to natural exploration. If the optimal policy is stochastic,\nthen the softmax distribution will approximate the optimal stochastic policy. If\n12\n2.1. Reinforcement Learning\nthe optimal policy is deterministic, then softmax distribution degenerates to a\nnearly deterministic policy. There is no need to specify that beforehand. Another\nadvantage of policy gradient methods is that we directly optimize for what we\ncare about, which is the optimal policy. It may be simpler to learn the policy\ndirectly than to estimate the state or state-action value.\nGiven a performance measure for πθ in the form of a diﬀerentiable objective\nfunction J(θ), we can perform gradient ascent to update the pararameters θ:\nθ ←θ + α∇θJ(θ)\n(2.10)\nThe learning rate is speciﬁed by α and needs to be tuned.\nThe objective of reinforcement learning is to maximize the expected sum of total\ndiscounted rewards. Policy gradient methods take this as an objective function\nto maximize:\nJ(θ) = Eπθ[\nT−1\nX\nt=0\nγtrt]\n(2.11)\nThe objective function in this form is usually not diﬀerentiable as it is dependent\non the stationary state distribution of the environment and thus can not be used\nfor gradient ascent. By the policy gradient theorem [Sutton and Barto, 2018] we\ncan rewrite J(θ) to a diﬀerentiable form:\n∇J(θ) = Eπθ[∇θlogπθ(at|st)Qπθ(st, at)]\n(2.12)\nActor-critic methods are policy gradient methods that are also learning a value\nfunction. This value function is used as a critic. The critic is used for bootstrap-\nping as in temporal-diﬀerence learning [Sutton and Barto, 2018]. Using one or\nmore estimated values, in this case, an estimate for the action-value, in the update\nstep for the same kind of estimated value is called bootstrapping. Actor-critic\nmethods use a bootstrapped n-step return or directly estimate the action-value\nfunction. The advantage of bootstrapping is variance reduction of the estimates\nand allowing us to take updates on partial episodes. However, it comes at the\ncost of a bias towards the learned critic as it usually does not match the real\naction-value function.\nIn advantage actor-critic methods, the same critic used for bootstrapping is also\nused as a baseline function. The baseline (estimated action-value) is subtracted\nfrom the sampled n-step return resulting in a term called advantage. The advan-\ntage tells the agent how much better or worse his policy performs than currently\nestimated by the critic. The actor updates the policy parameters θ for πθ(at|st),\nin the direction suggested by the critic. If the observed return is less than the\nexpected return, we want to lower the probability of taking the given action;\notherwise, we want to assign that action a higher probability. Using a baseline\nleads to further variance reduction and thus increases the convergence speed.\n13\n2.1. Reinforcement Learning\n2.1.5\nProximal Policy Optimization\nProximal policy optimization (PPO) is a policy gradient method for reinforcement\nlearning by Schulman et al. [2017]. Similar to trust region policy optimization\n(TRPO) [Schulman et al., 2015] a constraint on the policy update is added such\nthat parameter updates do not change the policy too much. This improves train-\ning stability as it prevents large policy updates. In TRPO an objective function\nis maximized, while a constraint on the size of policy updates is enforced. The\nKullback–Leibler (KL) divergence of the old policy πθold and the policy after the\nupdated πθ is used for such a constraint:\nmaximizeθE[ πθ(at|st)\nπθold(at|st)At(st, at)]\n(2.13)\nsubject to E[KL[πθold(·|st), πθ(·|st)]] ≤δ\n(2.14)\nwhere θold is the vector of policy parameters before the update, and A is the\nadvantage.\nThis constraint can be transformed to a penalty in order to solve an unconstrained\noptimization problem:\nmaximizeθE[ πθ(at|st)\nπθold(at|st)At −βKL[πθold(·|st), πθ(·|st)]]\n(2.15)\nwhere β is a scaling coeﬃcient. In TRPO, the hard constraint is used instead of\na penalty because it is hard to choose a value of β that performs well. Especially\non problems where the characteristics change throughout learning, as is the case\nin RL.\nPPO builds upon this idea of a trust region but implements a more straightfor-\nward constraint. A clipped surrogate objective is used to archive this simpliﬁca-\ntion. The ratio between old an new policies is denoted by φ(θ):\nφ(θ) = πθ(at|st)\nπθold(at|st)\n(2.16)\nThe TRPO objective is, therefore:\nJTRPO(θ) = E[φ(θ)At(st, at)]\n(2.17)\nPPO now imposes the constraint by forcing φ(θ) to stay within a trust region\nof [1 −ϵ, 1 + ϵ] where ϵ is a hyperparameter. Simply clipping updates force the\npolicy to stay in the trust region:\nJCLIP(θ) = E[min(φ(θ)A, clip(φ(θ), 1 −ϵ, 1 + ϵ)A)]\n(2.18)\n14\n2.1. Reinforcement Learning\nTherefore if the objective value is not in the trust region, the clipped value will\nbe used.\nWhen PPO is used in a network architecture with shared parameters for pol-\nicy (actor) and value (critic) functions, an additional entropy term (blue) is in-\ntroduced to encourage exploration. Furthermore, the error term on the value\nestimation (red) is part of the PPO objective function.\nJCLIP(θ) = E[JCLIP(θ) −c1(Vθ(st) −Vtarget(st))2 + c2H(st, πθ(·, st))]\n(2.19)\nwhere both c1 and c2 are scaling parameters for those losses which need to be\ntuned.\nPPO increases the sample eﬃciency of the RL algorithm empirically, and we\nhence use it in this work. Hsu et al. [2020] has shown that in some cases, PPO\nfails to converge to a bad local optimum:\n• Using standard PPO with a continuous action space, training becomes un-\nstable when rewards vanish outside the trust region. This can happen due\nto a bad Gaussian policy update, where PPO fails to recover.\n• On high-dimensional discrete action spaces, clipping might converge to sub-\noptimal actions with standard softmax policy parametrization. This hap-\npens when the policy sees only bad actions (reward 0) and suboptimal\nactions without observing the optimal action. PPO then tends to increase\nthe probability of suboptimal actions and not exploring new actions.\n• PPO can converge to suboptimal actions if they are close to the initializa-\ntion.\n2.1.6\nChallenges of Deep Reinforcement Learning\nThere are a lot of unsolved challenges in deep reinforcement learning. The most\nimportant two are the exploration vs. exploitation dilemma and the deadly triad.\nThe exploration vs. exploitation dilemma also exists in the real world. We might\nhave our favorite ski resort, where we go skiing every year. However, there are a\nlot of other ski resorts which we have not visited yet. It could be that one of the\nnew resorts would please us more than our current favorite. If we do not try out\ndiﬀerent resorts, we may never ﬁnd the optimal one, but we risk skiing in a place\nwith only easy slopes and bad restaurants if we try out new ones. The best long-\nterm strategy might involve short-term sacriﬁces to ﬁnd the optimal ski resort.\nThe diﬃculty is to ﬁnd the optimal ratio of exploring new places vs. exploiting\nthe current best one. This analogy can be adapted to reinforcement learning. We\nhave to explore new actions to learn more about their eﬀectiveness and maximize\nthe expected return in the long run by exploiting new better actions found by\n15\n2.1. Reinforcement Learning\nexploring. In stochastic tasks, this dilemma is even worse because we have to\nexplore the same action multiple times due to the stochastic changes. According\nto Sutton and Barto [2018] this problem remains unresolved and is speciﬁc to RL\nas it does not occur in supervised or unsupervised learning.\nThe deadly triad is a deﬁnition by Sutton and Barto [2018]. It states that in-\nstability and even divergence while optimizing arises whenever we combine the\nfollowing three elements:\n• Function approximation like neural networks. This element is needed\nto solve problems with large state and action spaces.\n• Bootstrapping Using one or more estimated values in the update step for\nthe same kind of estimated value [Sutton and Barto, 2018]. Bootstrapping\nadds a bias towards our start estimation, but the updates result in high\nvariance for long trajectories without bootstrapping. Without bootstrap-\nping, we need more samples before our estimate converges, leading to a loss\nin data eﬃciency and a rise in computational cost.\n• Oﬀ-policy training We call our training oﬀ-policy when we update an-\nother policy (target policy) than the one we followed to generate the training\ntrajectories. We need to train multiple value functions and policies in par-\nallel for some use-cases and are therefore oﬀ-policy. Many of the current\nstate-of-the-art RL algorithms are oﬀ-policy algorithms. Being oﬀ-policy is\nalso useful as it allows us to run a learned policy in parallel distributed on\nhundreds of actors and having a centralized learner who uses the generated\ntrajectory to update a slightly oﬀ-policy policy.\nAll of the three elements are very useful, and one does not want to give them\nup.\nIn practice, many RL architectures successfully use all three elements of\nthe deadly triad, like DQN [Mnih et al., 2015]. In the example of DQN, they\nuse many tricks to cope with the instability and prevent the estimates from\ndivergence through training with experience replay and occasionally freeze the\ntarget network.\n16\n2.2. Transfer Learning\n2.2\nTransfer Learning\nIn reinforcement learning, an agent usually starts with a random policy. The\nagent then has to learn an optimal policy for the target task using no prior\nknowledge. For challenging target tasks, for example, due to sparse reward signals\nor poor state representations, the agent might learn very slowly or entirely fails\nto learn at all.\nTransfer learning is one area of research that tries to speed up the training of\nRL agents by transferring knowledge from one or more source task MDPs to a\ntarget task. Instead of learning to solve the target task tabula rasa, the agent ac-\nquires knowledge on one or more source tasks. The knowledge can be transferred\nin form of samples [Lazaric et al., 2008], options [Soni and Singh, 2006], poli-\ncies [Fern´andez et al., 2010], models [Fachantidis et al., 2013] or value functions\n[Taylor and Stone, 2005]. In the case of policy and value function transfer, the\nparameters of a policy or value function obtained by training on one or multiple\nsource tasks can be used to initialize the policy or value function of the agent.\nTransferring the policy or value function leads to a bias in the action selection\ntowards the experience acquired in the source task.\n2.2.1\nEvaluation Metrics for Transfer Learning\nTo quantify the beneﬁts gained from transfer learning, we need meaningful met-\nrics. Typically we compare the learning curve on the target task for an agent\nafter transfer with a tabula rasa agent. We consider the following three metrics:\n• Time to Threshold: The time to threshold computes how much faster an\nagent with knowledge transfer reaches a reward threshold compared to a\ntabula rasa agent. The time can be measured by CPU / GPU instructions,\nwall clock time, episodes, or steps.\n• Jumpstart: This measurement quantiﬁes the initial performance boost we\ngain as a result of knowledge transfer.\n• Asymptotic Performance: The asymptotic performance compares the\nﬁnal performance increase at the end of training.\nWhen comparing tabula rasa agents to agents that use transfer learning, we need\nto specify if we want to include the time spent learning the source tasks in our\nmetrics. We talk about weak transfer when we do not include the costs of training\nin source tasks. If we consider the time spent in the source tasks when calculation\nthe metrics, we measure the strong transfer. In ﬁg. 2.2 we illustrate the three\nmetrics once with a weak transfer and once with a strong transfer. As the strong\ntransfer includes costs for training in the source task, the transfer curve starts\nwith a delayed training time in ﬁg. 2.2 (b).\n17\n2.2. Transfer Learning\n(a) Weak transfer\n(b) Strong transfer\nFigure 2.2: Performance metrics for transfer learning using (a) weak transfer and\n(b) strong transfer (ﬁgure by Narvekar et al. [2020]).\n18\n2.3. Curriculum\n2.3\nCurriculum\nWe deﬁne a curriculum as a concept that organizes past experiences and schedules\nfuture experiences by training on tasks.\nEvery task is modeled as a Markov\nDecision Process.\nIn the following we use the curriculum deﬁnition of Narvekar et al. [2020]:\nDeﬁnition 2.3.1 (Curriculum). Let T be a set of tasks, where mi = (Si, Ai, pi, ri)\nis a task in T and i is the task identiﬁer. Let DT be the set of all possible transi-\ntion samples from tasks in T : DT = {(s, a, r, s′)|∃mi ∈T s.t. s ∈Si, a ∈Ai, s′ ∼\npi(·|s, a), r ←ri(s, a, s′)}. A curriculum C = (V, E, g, T ) is a directed acyclic\ngraph, where V is the set of vertices, E ⊆{(x, y)|(x, y) ∈V × V ∧x ̸= y} is the\nset of directed edges, and g : V →P(DT ) is a function that associates vertices to\nsubsets of samples in DT , where P(DT ) is the power set of DT . A directed edge\n(vj, vk) in C indicates that samples associated with vj ∈V should be trained on\nbefore samples associated with vk ∈V. All paths terminate on a single sink node\nvt ∈V.\nIf the curriculum is created online, then the edges are added dynamically during\nthe agent’s training. If the curriculum is created oﬄine, then the graph is created\nbeforehand.\nOne simpliﬁcation to the curriculum deﬁnition is the single-task curriculum,\nwhere all samples stem from a single task. In a single-task curriculum, we re-\narrange the order in which we train on the experience samples as in prioritized\nexperience replay [Schaul et al., 2016].\nDeﬁnition 2.3.2 (Single-task Curriculum). A single-task curriculum is a cur-\nriculum C where the cardinality of the set of tasks considered for extraction\nsamples |T | = 1, and consists of only the target task mt.\nA second simpliﬁcation of the curriculum deﬁnition is the task-level curriculum.\nIn the task-level curriculum, we deﬁne a directed acyclic graph of intermediate\ntasks. The main challenge here is how to order the intermediate tasks such that\nthe agent can constantly learn to solve more complex tasks while preventing\ncatastrophic forgetting on already solved tasks. The mapping function g deter-\nmines the set of samples DT\ni\nthat are available at the next vertex. There are\nmultiple works on task-level curricula [Svetlik et al., 2017, Matiisen et al., 2019].\nDeﬁnition 2.3.3 (Task-level Curriculum). For each task mi ∈T , let DT\ni\nbe\nthe set of all samples associated with task mi : DT\ni\n= {(s, a, r, s′)|s ∈Si, a ∈\nAi, s′ ∼pi(·|s, a), r ←ri(s, a, s′)}. A task-level curriculum is a curriculum C =\n(V, E, g, T ) where each vertex is associated with samples from a single task in T .\nThus, the mapping function g is deﬁned as g : C →{DT\ni |mi ∈T }.\n19\n2.4. Curriculum Learning\nThe simplest form of a curriculum is the sequence curriculum. In the sequence\ncurriculum, all nodes have an indegree and outdegree of at most 1. If combined\nwith the task-level curriculum, we end up with the task-level sequence curriculum,\nwhich is an ordered list of tasks [m1, m2, . . . mt]\nDeﬁnition 2.3.4 (Sequence Curriculum). A sequence curriculum is a curriculum\nC where the indegree and outdegree of each vertex v in the graph C is at most\n1, and there is exactly one source node and one sink node.\n2.4\nCurriculum Learning\nIn curriculum learning, we try to ﬁnd the optimal order in which we feed ex-\nperience from diﬀerent source tasks to the agent in order to maximize one of\nthe metrics deﬁned in section 2.2.1. The intuition behind curriculum learning\nis that through generalization over source tasks and knowledge transfer through\nincreasingly more complex tasks, we can increase the sample eﬃciency or asymp-\ntotic performance of our training algorithms. There are three key elements to\ncurriculum learning:\n• Task Generation: In order to produce a beneﬁcial curriculum, we need a\ngood set of source tasks. The tasks should provide an increasing diﬃculty.\nThe tasks have to be similar to the target task. Otherwise, we might end\nup with a negative transfer, and using a curriculum over such tasks is\nhurtful for solving the desired target task. Task generation is, therefore, an\nimportant part of curriculum learning. In a task-level curriculum, the tasks\nare the nodes of the curriculum graph. The task may be generated online\nduring training or pre-speciﬁed.\n• Sequencing: Sequencing is concerned with the ordering of the experience\nsamples. According to the graph deﬁnition in section 2.3, sequencing deﬁnes\nthe vertices in our curriculum graph. These vertices can either be deﬁned\nonline during training or oﬄine before training the agent. In section 2.4.2\nwe go into details of this aspect to curriculum learning.\n• Transfer Learning: In order to beneﬁt from a curriculum of tasks, we need\na method to transfer knowledge through the curriculum. In section 2.2 we\nexplained multiple methods for transfer knowledge from one or more source\ntasks to the target task. In curriculum learning, we repeatedly transfer\nknowledge from task to task, whereas we have only one transfer step in\nstandard transfer learning.\nWe visualized the interaction between the three key elements in ﬁg. 2.3.\nWhen evaluating the curriculum, we use the same metrics as in section 2.2.1.\nThose metrics have to be extended as we have to consider the costs of building\n20\n2.4. Curriculum Learning\nFigure 2.3: We visualize the interaction between the three key elements of cur-\nriculum learning. Task generation is concerned with generate a set of tasks for the\ncurriculum. Task sequencing selects a task out of the task set for the agent. The\nagent use knowledge obtained by training on previously selected task through\ntransfer learning. After or during training on the new task, the obtained knowl-\nedge is stored.\nThe task generation and sequencing can be done online and\ndependent on the student’s current performance or oﬄine before training.\nthe curriculum. It is not clear how the work for the task sequencing or task\ngeneration done by humans should be included into the strong transfer metrics.\nIn our work, we ignore those costs for the sake of simplicity.\n2.4.1\nCurriculum Learning Categorization\nWe can categorize a curriculum learning approach along six dimensions, organized\nby attributes (in bold) and their respective values (in italics). This categorization\nwas introduced in Narvekar et al. [2020]:\n1. Intermediate task generation: target / automatic / domain experts /\nnaive users. The set of tasks can be either deﬁned oﬄine before training\nor online during training. One can also specify a single task curriculum\ncalled target where only the target task is used. The tasks can be speciﬁed\nby a human, either a domain expert [Schraner, 2020] or a naive user with\nno special domain knowledge.\nThere are also methods to automatically\ngenerate tasks using a set of rules or a generative process as in Wang et al.\n[2019].\n2. Curriculum representation: single / sequence / graph. The simplest\nway to represent a curriculum is a single task curriculum, where we simply\nreorder the recorded experience [Schaul et al., 2016, Andrychowicz et al.,\n2017]. When using multiple tasks we can either represent the curriculum\n21\n2.4. Curriculum Learning\nas a simple sequence of tasks [Schraner, 2020] or as directed acyclic graph\nof tasks [Svetlik et al., 2017].\nIn the task representation, we can allow\nmany-to-one, one-to-many, and many-to-many knowledge transfer.\n3. Transfer method: policies / value function / task model / partial policies\n/ shaping reward / other / no transfer. In section 2.2 we speciﬁed diﬀerent\nforms of knowledge transfer. The weights of policy [Scheller et al., 2020]\nor value [Fern´andez et al., 2010, Taylor et al., 2007, Taylor and Stone,\n2005] functions can be transferred from task to task. One can learn task\nmodels [Fachantidis et al., 2013] and transfer those from task to task, learn\nan auxiliary reward function, or extract options [Sutton et al., 1999b] and\ntransfer those to the next task.\n4. Curriculum sequencer: automatic / domain experts / naive users. The\ncurriculum sequencing is concerned with the task switches during training.\nThe switch can happen automatic upon speciﬁc rules, through a teacher\nor other methods. In section 2.4.2 we go into detail about this aspect of\ncurriculum learning. We can also sequence the task manually by domain\nexperts or naive users.\n5. Curriculum adaptivity: static / adaptive. The adaptivity of a curricu-\nlum speciﬁes if the curriculum is completely deﬁned before training or if\nit is dynamically adapted online during training. A static curriculum is\ndeﬁned before training, a adaptive curriculum is changed during training.\nAdaptive curricula can use the learning progress to estimate, e.g., if a task\nis easy or hard to learn at the current stage. Static curricula incorporate\nproblem-speciﬁc knowledge.\n6. Evaluation metric: time to threshold / asymptotic / jumpstart / total\nreward.\nIn section 2.2.1 we introduced metrics to quantify the beneﬁts\ngained from curriculum learning. When calculating those metrics, we have\nto decide if we want to measure the weak or strong transfer.\n22\n2.4. Curriculum Learning\n2.4.2\nTask sequencing\nTask sequencing is concerned with how the tasks can be sequenced in order to\nprovide a curriculum. The tasks need to be sequenced in a way such that the\ncurrent task at hand is just hard enough to solve. It might also be of interest\nto add an already learned task that the agent forgets about to allow live long\nlearning.\nIn section 1.1 we already introduced a few methods for task sequencing. In this\nsection, we detail two sequencing methods that use the teacher-student setup\nwith the MDP formulation.\nNarvekar et al. [2017] formulates curriculum learning as the interaction of two\nMDPs. A student MDP describes the currently selected task, and a teacher MDP\nmodels the selection of the next task for the student. They denote the teacher\nMDP as a fully observable MDP, where the state space is the set of policies the\nlearning agent can represent. The state is represented as the parameters of the\npolicy. The ﬁnal states are deﬁned as policies where the return on the target\ntask is higher than a speciﬁc threshold. The action space is the set of tasks a\nstudent agent can train on. Taking an action results in the student training on\nthe selected task for a ﬁxed number of steps or until convergence. The transition\nfunction describes how the student agents policy changes as a result of learning\na task. The reward function is deﬁned as the time needed by the student agent\nto learn a policy that results in a return surpassing a certain threshold on the\ntarget task (time to threshold).\nThe teacher wants to minimize this time to\nthreshold. Therefore the reward is encoded as the negative of the expected time\nneeded to learn the target task starting from a given policy. A recursive Monte-\nCarlo algorithm optimizes the teacher agent, and the student is a tabular RL\nagent with tile coding, trained with Sarsa(λ) and a value function transfer. In\ntheir follow-up paper, they investigated reward shaping as an additional transfer\nmethod [Narvekar and Stone, 2019].\nMatiisen et al. [2019] uses a similar approach, but the state is not fully observable,\nmaking the MDP a partially observable Markov decision process (POMDP). The\nstate and action space of the teacher MDP are the same as in Narvekar et al.\n[2017], but the teacher has no access to the internal parameters of the student\nagent. The observation is the reward of the student obtained on the currently\nselected task. The reward is the average reward of the student evaluated on all\ntasks at the end of a teacher step. One could also optimize for the reward of the\ntarget task, but initially, the student might not archive a reward on the target\ntask leaving the teacher without a meaningful signal.\nA comprehensive survey of task sequencing methods is provided by Narvekar\net al. [2020].\nChapter 3\nMethods\nIn this chapter, we describe the methods used in this thesis. We characterize our\ncurriculum learning approach along the six dimensions introduced in section 2.4.\nNext, we detail our transfer method and the automated curriculum sequencing\napproach. We propose multiple types of observations and reward signals for our\ncurriculum MDP. Finally, we describe our set of baselines and the evaluation\nprotocol.\n3.1\nCurriculum Learning\nIn this section, we characterize our curriculum learning approach. We use a task-\nlevel sequence curriculum. A combination of the task-level curriculum deﬁned in\nDeﬁnition 2.3.3 and a sequence curriculum (see Deﬁnition 2.3.4). Our curriculum\nlearning has the following properties:\n1. Intermediate task generation: The tasks, represented as nodes in the\ncurriculum graph, are predeﬁned before training. We use a subset of the\ntasks provided by the grid world environment and the Google Research\nFootball environment. We initially selected the tasks and kept this selec-\ntion ﬁxed throughout this thesis. The exact task selection is described in\nsection 4.2.1 and section 4.4.1.\n2. Curriculum representation: We represent the curriculum as a task-level\nsequence. We only allow one-to-one knowledge transfer between our source\ntasks and the target task. A source task can be visited multiple times in\nthe sequence. Therefore the agent is allowed to retrain on already known\ntasks.\n3. Transfer method: We experiment with three transfer methods. First, we\ncopy the policy and value function weights from task to task to transfer\nthe learned policy and value function. Second, the learned value function,\nobtained by training on the previous task, calculates an additional reward\nsignal that we added to the environment’s reward signal. Third, we exper-\niment with a combination of the ﬁrst and second method. In section 3.1.1\nwe go into detail about our knowledge transfer methods.\n4. Curriculum sequencer: In this work, we experiment with automated\n23\n24\n3.1. Curriculum Learning\ntask sequencers. We formulate the task sequencing problem as a curriculum\nMarkov Decision Process where we have full control over the student’s MDP.\nIn section 3.2 we formalize the CMDP and explain our approach in detail.\n5. Curriculum adaptivity: We use an adaptive curriculum. The teacher\nperforms task sequencing online to select tasks with a suitable learning\npotential for the current learning stage. Our set of source tasks is deﬁned\nbeforehand and kept ﬁx throughout training.\n6. Evaluation metric: We evaluate our agents using a weak transfer with\nthe asymptotic performance improvement. We compare our work against\nagents trained in Schraner [2020], which uses manually deﬁned curricula.\n3.1.1\nTransfer Method\nWe use two types of knowledge transfer methods: policy and value function\ntransfer and reward shaping. Our student agent starts with a random initialized\npolicy and value function. After training for a ﬁxed amount of steps at time-step\nt on a task mt, we switch to a new source task mt+1. The weights of the policy\nand value function obtained by training on task mt are used to initialize the agent\nbefore training on task mt+1. Our student uses weight sharing for the policy and\nvalue function. Therefore we copy all neural network weights from task to task.\nWe assume that both the policy and value function transfer from task to task as\nthe dynamics of the environment does not change in our set of tasks V. Therefore,\na state with a true high value in one environment mi ∈V has also a true high\nvalue in another environment mj ∈V. The same applies to our policy. The\nenvironments in our set of source tasks diﬀer in their initial state distribution,\nstate space and complexity.\nAll tasks in V share the same set of actions A,\nenvironment dynamics function p and reward function r.\nIt would also be possible to only transfer the weights of the shared embedding\ntogether with either the policy head or the value head from task to task. Like\nthis, either the policy or the value are initialized randomly for each task. The\ngradients for the policy and the value function ﬂow through the same model in\nour network architecture with weight sharing. If either the policy or the value\nfunction is initialized randomly, this could lead to catastrophic updates to the\nshared weights. Therefore, we transfer all the neural network weights from task\nto task.\nAdditionally to the policy and value function transfer, we experiment with a\nshaped reward signal. We use the value function vmt obtained by training on\ntask mt as an additional reward signal when training on the next task mt+1.\nThe reward ri at time-step i is therefore ri = ri + vmt(si) We expect that the\nvalue function can help to overcome the spares reward signal provided by the\n25\n3.2. Teacher Curriculum Markov Decision Process\nenvironment. For the ﬁrst task m1 in our curriculum, we do not add a shaped\nreward signal as we do not have a value function at hand. Only the value function\nof the previous task is added as a shaped reward. One could also use the average\nof the last n value functions as a shaped reward signal. We leave this experiment\nto future work.\nThe two transfer methods described can be combined, leading to a transfer\nmethod with policy and value function transfer and a shaped reward signal. We\nalso experiment with this combined transfer method.\n3.2\nTeacher Curriculum Markov Decision Pro-\ncess\nIn this section, we formulate the sequencing problem as a Markov Decision Pro-\ncess. In this formulation, we deﬁne curriculum learning as an interaction between\ntwo types of MDPs. The ﬁrst MDP is the standard RL MDP modeling the stu-\ndent interacting with a task. The second one is a higher level meta-MDP called\ncurriculum MDP (CMDP). We use the CMDP to model a teacher selecting tasks\nfor the student. The CMDP is a 4-tuple (S, A, p, r), where S is the set of all\npossible states equal to all possible policies the student can represent. The ac-\ntion space A is the set of tasks the teacher can assign to the student. Taking an\naction in the CMDP equals to an entire training cycle on the selected task in the\nstudent MDP for a ﬁxed amount of steps. The environment’s dynamics function\np models the transition from one student policy to the next student policy after\ntaking an action in the CMDP. The dynamics function is unknown. The reward\nfor the CMDP is deﬁned by the reward function r. In section 3.2.2 we detail two\nreward signals used in this research.\nThe interaction between the CMDP and the student MDP is shown in ﬁg. 3.1.\nThe teacher selects a task, and the student trains for n steps on the selected task.\nAfter the student training step, we evaluate the student on a set of evaluation\ntasks and feed the evaluation result to the CMDP. In our research the set of\nevaluation tasks Veval is equal to the learning tasks Vlearn.\nNow that we have deﬁned the sequencing problem as a CMDP, we can use re-\ninforcement learning to ﬁnd an optimal policy. We are using PPO to learn a\nteacher policy while training the student at the same time. With this approach,\nwe have to deal with noise from the student training when performing suboptimal\ntask switches, especially at the beginning of our training procedure. The teacher\nhas to learn how to perform task sequencing while the student has to learn how\nto solve the selected environment. Both are acting randomly, which may lead to\nthe teacher selecting too tricky tasks, and the student has a hard time learning\nfrom that task.\n26\n3.2. Teacher Curriculum Markov Decision Process\nFigure 3.1: Teacher-student interaction for task sequencing.\n3.2.1\nTeacher Observation\nWe experiment with diﬀerent types of observations for our teacher agent. The\nstate of the CMDP is the current policy of the student agent. We are using a\nneural network to approximate the optimal student policy π∗\ns. Therefore the stu-\ndent weights θs are the state of the CMDP, and they fulﬁll the Markov property.\nIt is unclear how we should feed this state representation into our teacher agent.\nWe could ﬂatten the weights θs to a feature vector, but this would leave us with\nan enormous input vector. In our case, we have roughly 400′000 weights in our\nstudent network. Using such a large input vector is not feasible due to memory\nlimits. Additionally, we assume that such a feature vector is not easy to interpret\nfor our teacher agent. It is hard to relate θs to the student’s performance and\nthe optimal next task to select.\nWe use principal component analysis (PCA) [F.R.S., 1901] to ﬁnd a reduced rep-\nresentation for θs. We build a training set containing student weights at diﬀerent\ntraining stages and then ﬁt PCA to this dataset. This dataset is obtained by\nsaving the weights of the student network for diﬀerent tasks at diﬀerent learning\nstages. At the end of each student training iteration, we do dimensionality re-\nduction to the ﬁrst n principal components and use this reduced representation\nas an input for our teacher.\nThere are other ways to get a reduced representation for θs like model distillation\nor auto-encoders, each with its downsides. We leave the question for an optimal\nway to bring θs into a meaningful representation to future work. In this work,\nwe refer to this observation type as pca input (PCA).\nIn our work we also experiment with partial observable curriculum MDPs. We\nwork with six diﬀerent types of manual deﬁned observation types: reward his-\ntory (RH), previous task reward (PTR), learning progress (LP), absolute learning\nprogress (ALP), exponential moving average (EMA), and fast and slow exponen-\ntial moving average (FS-EMA).\n27\n3.2. Teacher Curriculum Markov Decision Process\nReward History\n: Instead of using the students weights θs we try to represent\nthe learning potential per task.\nWe can use a tuple of the student’s average\nreward, obtained in the evaluation cycle at the end of a CMDP step, on each\ntask together with the time-step when this task was last sampled:\nRH = {(rm\neval, tm\nlast sampled)|m ∈V}\n(3.1)\nWhere rm\neval is the average reward on all evaluation episodes for task m and\ntm\nlast sampled is the CMDP time-step when task m last has been sampled. If the\ntask m never has been sampled tm\nlast sampled equals to 0.\nPrevious Task Reward\n: Instead of using the average rewards of all tasks,\nwe can input the last selected task, one-hot encoded, together with the average\nreward on that task, obtained in the evaluation cycle at the end of a CMDP step.\nLearning Progress\n: The learning progress is deﬁned as the diﬀerence be-\ntween the average reward, obtained in the evaluation cycle at the end of a CMDP\nstep, of the current and the previous time step per task:\nLP = {(rm\nt −rm\nt−1)|m ∈V}\n(3.2)\nWhere rm\nt\nis the average reward on the task m at time-step t and rm\nt−1 is the\naverage reward on task m at the previous time-step t −1.\nAbsolute Learning Progress\n: ALP is simply the absolute value of LP:\nALP = {(|rm\nt −rm\nt−1|)|m ∈V}\n(3.3)\nThe intuition behind using the absolute value is that a task at the stage of\nforgetting, resulting in a negative LP, should be treated similarly to a task with a\nsteep learning curve. This learning progress representation is inspired by Portelas\net al. [2019].\nExponential Moving Average\n: We can interpret the history of evaluation\nrewards after every CMDP cycle as a time-series. We use an exponential moving\naverage over the history of rewards to estimate the next reward. Depending on\nthe α ∈[0, 1] value used to calculate the EMA we assign more weight on recent\nsamples than on old ones. The exponential moving average over a vector x is\ndeﬁned as:\nema(xt) =\n(\nα ∗xt + (1 −α)ema(xt−1),\nt > 1\nx1,\nt = 1\n(3.4)\nWhere ema(xt) is the value of the EMA at any time period t and xt is the value\nat a time period t. Therefore the EMA input is:\nEMA = {[ema(xm\nt )]|m ∈V}\n(3.5)\n28\n3.2. Teacher Curriculum Markov Decision Process\nxm is the history of the average reward, obtained in the evaluation cycle at the\nend of a CMDP step, for task m at all CMDP time-steps. The last time step is\ndenoted as t.\nFast and Slow Exponential Moving Average\n: Kanitscheider et al. [2021]\nintroduce a smoothed EMA version, where they calculate a fast and a slow EMA\nwith a high α and a low α respectively. The fast and slow EMA is then deﬁned\nas the diﬀerence between the two EMAs:\nFS-EMA = {[emafast(xm\nt ) −emaslow(xm\nt )]|m ∈V}\n(3.6)\nIn our work, we wanted to test if this method is superior to a normal EMA with\na tuned α value.\n3.2.2\nReward Signal\nIn reinforcement learning, a meaningful reward signal is crucial for success. We\nneed to deﬁne a reward signal for our CMDP that encodes our intention and is\nrich enough for the teacher agent to learn fast. The goal of the teacher agent\nis to perform task sequencing such that the asymptotic reward on the target\ntask increases. Therefore we can use the student’s average reward on the target\ntask after a CMDP step as a reward signal. We call this reward signal target\ntask reward. Typically, the target task is hard to solve. Therefore the reward\nat the beginning of training is usually 0. This reward signal is not meaningful\nfor the teacher as it does not tell if the student is making progress in easier\nenvironments or if the student is completely lost. Such a sparse reward signal\nleads to less sample eﬃciency and as samples in the CMDP are extremely costly,\nwe want our teacher to learn fast.\nWe assume that the source tasks in V are related to the target task. Therefore, if\nour student achieves a reward on the source tasks, this is a step towards solving\nthe target task. If this is true, then we can overcome the sparse reward signal by\ndeﬁning a new reward signal source task reward:\nrteacher =\nX\nm∈V\nrm\n(3.7)\nWhere rm is the average reward on task m obtained in the evaluation cycle at\nthe end of a CMDP step.\n29\n3.2. Teacher Curriculum Markov Decision Process\n3.2.3\nAction\nThe action space in the CMDP contains all tasks in V. Taking an action equals to\nselecting a task m ∈V, changing the student’s environment to the selected task,\nand then train the student for a predeﬁned amount of steps. After the training,\nthe student is evaluated on all tasks in V and the evaluation results are passed\nto the CMDP agent. The evaluation is carried out according to the section 3.4.\nIn our experiment, we train the student for 100′000 steps. One could also train the\nstudent until convergence, surpassing a threshold, or make the number of steps\npart of the teacher’s action space. It is unclear when the student will converge,\nwhich could lead to very compute-intensive CMDP steps, and if the agent can\nsurpass the threshold, which could lead to an inﬁnite CMDP step. Therefore,\nwe do not consider these two approaches. We want to keep the teacher’s action\nspace as simple as possible. Therefore we leaf integrating the number of student\ntraining steps into the action space for future work.\n3.2.4\nNetwork Architecture\nWe keep the teacher network architecture simple. We use a multilayer perceptron\n(MLP) with three hidden layers with 64, 128, and 64 nodes, respectively, and a\nReLU [Agarap, 2018] activation function. We make use of weight sharing as we\ndo it for our student agent. The policy and value function are represented as\nindividual heads on top of the MLP feature embedding.\nWe also experiment with LSTMs [Hochreiter and Schmidhuber, 1997] in order\nto provide the teacher a memory. In this setting, we use the same MLP for the\nfeature embedding and then feed the feature vector into an LSTM with a hidden\nstate of 128. The hidden state is then used as an input for the policy and value\nfunction heads.\nDue to computation limits, we were not able to tune the network architecture.\nAnother network architecture likely yields better results.\n30\n3.3. Baselines\n3.3\nBaselines\nWe compare our teacher task sequencing approach against four baselines: uni-\nform, LP sampling, window and thompson sampling. The last two are introduced\nin Matiisen et al. [2019].\nUniform\n: This is the simplest baseline. We select the next task by uniformly\nsampling them.\nLP sampling\n: In this baseline, we select the task with the highest learning\nprogress as deﬁned in eq. (3.2).\nIf two or more tasks have the same LP, we\nsampling one of those tasks uniformly.\nThompson sampling\n: Similar to LP sampling we sample the next task\naccording to a learning progress metric. In the Thompson sampling baseline, we\nsample the task with the highest average reward after the last CMDP step.\nWindow\n: In the window algorithm, we approximate the learning progress\nwith linear regression. We store the history of average evaluation rewards. At\neach CMDP step, we ﬁt a linear regression to the reward history. The regression\ncoeﬃcient per task is used to estimate the steepness of the learning curve. The\ntask with the steepest learning curve is selected.\n3.4\nEvaluation Protocol\nThe goal of the teacher task is to improve the asymptotic performance.\nWe\nevaluate our approach using a weak transfer to allow comparison with the results\non the Google Research Football environment from our previous work [Schraner,\n2020].\nThe asymptotic performance is deﬁned as the increase in the average\nreward at the end of training.\nAdditionally, we evaluate how well the trained student agent generalizes across\nthe environments. The agent is trained overall environments, and it is interesting\nhow well the agent performs on those environments. To measure the generality,\nwe calculate the total average return, which is the sum of all average returns\nfor all tasks in V, and the percentage of environments solved. We declare an\nenvironment as solved when the average return is greater than zero.\nAfter every CMDP step, we evaluate the student on every task in V for 100\nepisodes.\nChapter 4\nExperiments and Results\nIn this chapter we describe the experiments used to evaluate our methods. We\nbegin with an introduction of the environments used, the experimental setup and\nthen provide results of our baselines and teacher student experiments. In depth\nexperiments are carried out on a grid world environment, the most promising\nsettings are then evaluated on the Google Research Football environment. In\nthe following chapters, we use the terms task and environment interchangeably.\nIn our case, the environment is equal to the task to solve, e.g., there is only a\nsingle task per environment. The description of the Google Research Football\nenvironment is taken from our previous work [Schraner, 2020].\n4.1\nGrid World\nGrid world is a simple, lightweight, and fast environment for reinforcement learn-\ning. In a Grid world environment, the agent must reach a target destination by\nnavigating through a maze. The diﬃculty of the environment ranges from very\nsimple empty grids to complex mazes where the agent has to ﬁnd keys in a spe-\nciﬁc order to unlock the target destination. We use the minimalistic grid world\n(MiniGrid) implementation by Chevalier-Boisvert et al. [2018].\nIn MiniGrid, the world is an NxN grid of tiles. Each tile in the grid world contains\nzero or one object, and each object has an associated discrete color and type. The\nobject types are wall, ﬂoor, door, key, ball, and goal. Doors have a state open,\nclosed, or locked and behave according to this state. To open a locked door, the\nagent has to carry a key matching the door’s color. The agent can pick up and\ncarry exactly one object (e.g., ball or key).\nThe simplicity of the grid world environment allows fast iteration and testing of\nmultiple ideas. Training a student agent in the CMDP setting in the grid world\nenvironment takes around 18 hours. Training a teacher in the proposed CMDP\nsetting is computationally expensive as we have to train a student agent. The\nset of environments with their diﬀerent levels of complexity is helpful for the\ncurriculum learning setting.\nDepending on the selected environment, the maximum number of steps changes.\nAn epoch in the simplest environment, Empty-5x5, ends after 100 steps, the most\ndiﬃcult environment, KeyCorridor-S6R3, terminates after 1080 steps.\n31\n32\n4.1. Grid World\n4.1.1\nGrid World Environments\nWe describe the MiniGrid environments in table 4.1 and illustrate them in ﬁg. 4.1.\nIn a grid world environment, the agent has to navigate through a maze and solve\nsome puzzles to reach the green goal square. In the KeyCorridor environment,\nthe agent has to pick up a hidden ball behind a locked door. To unlock that\ndoor, the agent must ﬁnd the matching key, which is hidden in another room.\nThe agent has to explore the environment and move through open doors to ﬁnd\nthe hidden key. The bottom row of ﬁg. 4.1 displays two diﬀerent KeyCorridor\nenvironments.\nFigure 4.1: Visualization of a subset of the MiniGrid environments used in this\nwork. The environment names from top left to bottom right: Empty-6x6, Four-\nRooms, DoorKey-16x16, MultiRoom-N4S5, KeyCorridor-S3R2, KeyCorridor-\nS3R3, and KeyCorridor-S6R3.\n4.1.2\nState & Observations\nMiniGrid supports a variety of observation types. In our work, we use a fully\nobservable view of the environment. This view has a dimension of N × N × 3,\nwhere N is the dimension of the grid world. These values are not pixels, the last\nchannel is encoding the tile at the (X, Y) location. The tile encoding is a three-\ndimensional tuple: (OBJECT INDEX, COLOR INDEX, STATE). Only doors\nand agents have a state value other than 0. The door state 0 represents an open,\n1 a closed, and 2 a locked door. The agents’ state indicates the direction of the\nagent.\nWe are transferring the policy and value function weights from environment to\nenvironment. Therefore the input dimension must stay the same for every en-\nvironment. We apply padding to have a 25 × 25 grid world independent of the\nselected environment. As padding values, we use (1, 0, 0), which is equivalent to\na wall.\n33\n4.1. Grid World\nName\nDescription\nEmpty-[X]x[X]\nThis environment is an empty room. Upon reaching\nthe green goal square, the student receives a sparse\nreward. The agent is starting in a random position.\nThe value of X deﬁnes the grid size. We use a 5 by\n5, 6 by 6, 8 by 8, and 16 by 16 grid.\nFourRooms\nThe agent must navigate in a maze of four rooms.\nFour gaps in the walls connect the rooms. To obtain\na reward, the agent must reach the green goal square.\nBoth the agent and the goal square are placed ran-\ndomly in any of the four rooms.\nDoorKey-[X]x[X]\nThe agent must pick up a key in order to unlock a\ndoor and then reach the green goal square. Due to\nthe sparse reward signal, this environment is diﬃcult\nto solve. The door, wall, agent and green goal square\nare placed randomly. The value of X deﬁnes the grid\nsize. We use a 5 by 5, 6 by 6, 8 by 8, and 16 by 16\ngrid.\nMultiRoom-N[X]-\nS[Y]\nThe environment has a series of connected rooms\nwith doors that must be opened in order to get to\nthe next room.\nThe ﬁnal green goal square is lo-\ncated in the last room. The rooms are all created\nrandomly. This environment is challenging to solve\nusing RL alone. The value of X deﬁnes the number\nof rooms and X the room size. We use N2-S4, N4-S5,\nand N6-S10 for our experiments.\nKeyCorridor-\nS[X]R[Y]\nThe agent has to pick up an object which is behind\na locked door. The key is hidden in another room,\nand the agent has to explore the environment to ﬁnd\nit.\nThe key, ball, and agent are placed randomly.\nDue to the exploration required this is a challenging\nenvironment. The value of X deﬁnes the room size\nand Y number of rows (see ﬁg. 4.1). We use an S3R1,\nS3R2, S3R3, S4R3, S5R3, S6R3 grid.\nTable 4.1: Description of the MiniGrid environments used in this thesis. All\nenvironments impose a penalty for the number of steps taken until reaching the\ntarget.\n34\n4.1. Grid World\n4.1.3\nActions\nThe action space in MiniGrid consists of six actions: Turn left, turn right, move\nforward, pickup, drop and toggle (open door, interact with objects).\n4.1.4\nRewards\nThe agent receives a reward of 1 for reaching the goal square and 0 otherwise. A\npenalty for the number of steps required to reach the target location is imposed.\nIf the agent takes more steps to reach the target, the reward decreases. The grid\nworld reward upon success is calculated according to this equation:\nr = 1 −0.9 ∗(step count/max steps)\n(4.1)\nWhere step count is equal to the number of steps taken to reach the goal square\nand max steps is the maximum number of steps allowed per episode. In ﬁg. 4.2 we\nilustrated the reward calculation for two diﬀerent trajectories in the DoorKey-8x8\nenvironment.\nFigure 4.2: A blue line illustrates the agent’s trajectory in the DoorKey-8x8\nenvironment. The maximum number of steps in the DoorKey-8x8 environment\nis 640. If the agent takes the direct path, he takes 18 steps until he reaches the\ngreen goal square. Therefore the agents reward is 1−0.9∗(18/640) = 0.9747. The\nagent on the right takes 28 steps, leaving him with a reward of 1−0.9∗(28/640) =\n0.9606.\n4.1.5\nMDP Statement\nDepending on the type of observation used, the grid world environment does\nnot fulﬁll the Markov property. Some observation types only provide a limited\nﬁeld of view.\nTo ease the problem, we only use a fully observable input for\nall of our student agents. Each observation at every time step fully describes\nthe environment’s state space, and the following environment state is entirely\ndependent on the current state.\n35\n4.2. Grid World Experiments\n4.2\nGrid World Experiments\n4.2.1\nExperimental Setup\nIn this section, we detail the experimental setup applied to all our grid world\nexperiments.\nNetwork architecture\nOur student agent uses a fully connected network with\nReLU activation functions and separate policy and value function heads. The\nnetwork architecture is depicted in ﬁg. 4.3.\nFigure 4.3: Student neural network architecture for the MiniGrid environments.\nThe input is a 25×25×3 fully observable representation of the environments state.\nAfter every fully connected layer we use a ReLU activation function. The policy\nhead uses a softmax activation function for the action probability distribution.\nThe value head does not use an activation function.\nWe also evaluated a CNN network architecture. In the appendix A.1 we provide\ndetails about the CNN architecture. The 25 × 25 × 3 grid world observation has\nlocation-dependent features. Therefore by intuition we expect that CNN models\nwork better than MLP models. Flattening the observation into a vector makes\nit harder to discover patterns and generalize over the state space. As we can see\nin ﬁg. 4.4, using a CNN network architecture instead of the MLP architecture\nimproves results on harder environments. MLP models are less noisy than CNN\nmodels, especially in the case of DoorKey experiments. We value stability over\nbest possible results in our thesis because reward changes in our CMDP setting\nalso lead to a lot of noise, therefore we use MLP models for the rest of our\nexperiments.\n36\n4.2. Grid World Experiments\nEmpty-5x5\nEmpty-6x6\nEmpty-8x8\nEmpty-16x16\nFourRooms\nDoorKey-5x5\nDoorKey-6x6\nDoorKey-8x8\nKeyCorridorS3R1\nMultiRoom-N2-S4\nEnvironment\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReturn Average\nCNN vs MLP model comparison\nModel\nMLP\nCNN\nFigure 4.4: Comparison of the average return over 100 episodes at the end of\ntraining between MLP and CNN models. The agent is trained with PPO for 10\nmillion steps on a single environment.\nNo Curriculum Experiments\nFor all MiniGrid environments, we trained an\nagent with PPO for 10 million steps and evaluated the agent for 100 episodes at\nthe end of training. We used these experiments to tune hyperparameters as well\nas the network architecture. In appendix A.2 we report the hyperparameters.\nAdditionally, we used the results to select a subset of the MiniGrid environments\nfor our curriculum learning experiments. We removed the group of MiniGrid\nenvironments where our trained agents failed in solving the environment. In the\ncase of KeyCorridor and MultiRoom, training an agent for the simplest version\nof those environments succeeded. Therefore we kept all KeyCorridor and Multi-\nRoom environments in our task set V. All experiments were repeated three times\nunder diﬀerent random seeds. In our results, we report maximum and standard\ndeviations.\nCurriculum learning\nFor both the baseline and the CMDP experiments, we\nperform 1′000 teacher steps, where for each teacher step, the student is trained for\n10′000 steps in the selected environment. Therefore, the student agent is trained\nfor 10 million steps in total. After each teacher step, the student is evaluated for\n100 episodes in each task in V. We ﬁxed those number of steps to have the same\n37\n4.2. Grid World Experiments\ntraining steps as we used in our no curriculum learning experiments. We had to\nbalance the number of teacher updates and the number of student updates per\nCMDP step. Using fewer teacher steps might not be suﬃcient for the teacher to\nlearn how to perform task sequencing but would allow our student more time to\nconverge in the selected environment. Using more teacher steps is beneﬁcial for\nthe teacher, but on the other hand, the student has less time to learn the selected\ntask. This would provide the teacher with a noisy reward signal. We selected\nthe teacher and student steps because they seem reasonable. In future work, the\ninﬂuence of those values should be evaluated.\nThe network architecture for the teacher agent is described in section 3.2.4. We\nevaluated the MLP and LSTM network architecture in appendix A.5. The MLP\narchitecture is superior to the LSTM architecture.\nThe hyperparameters for\nthe baselines as well as the CMDP teachers are described in appendix A.2 and\nappendix A.4.\nAll experiments were repeated three times under diﬀerent random seeds. In our\nresults, we report maximum and standard deviations.\n4.2.2\nResults Overview\nTable 4.2 shows the best agent’s performance on each MiniGrid environment\nused in this thesis. For each environment, we report the average return of 100\nevaluation episodes. The total mean return is deﬁned as the sum over all av-\nerage returns. The percentage of environments solved is deﬁned as the number\nof environments with an average reward greater than zero divided by all envi-\nronments. Overall, using no curriculum results in a higher average return on 8\nout of the 19 tested tasks. In contrast, curriculum learning agents outperform\nstandard RL agents in more complex tasks such as Empty-16x16, DoorKey16x16,\nand KeyCorridor-S6R3. Comparing the best CMDP agent to the best baseline\nagents we see, the CMDP agent outperforms the baselines regarding the total\nmean return and percentage of solved environments.\nIn the curriculum learning approach, the agent spends less training time in a single\nenvironment than with no curriculum. In easy environments, using no curriculum\nlearning and training on a single task yields better results than the curriculum\nlearning experiments. Training for 10 Million steps in a single environment is\nbetter than learning in a curriculum without speciﬁcally targeting those easy\nenvironments. In the CMDP setting using the source task reward, the teacher is\nencouraged to maximize the student’s average return over all environments and\nnot a single environment, as is the case when using no curriculum.\nThe following sections evaluate diﬀerent teacher reward signals, transfer methods,\nobservation types, hyperparameters as well as the sample eﬃciency and generality\nof trained agents.\n38\n4.2. Grid World Experiments\nEnvironment / Metric\nNone\nUniform\nLP\nThompson\nWindow\nCMDP\nTotal mean return\n-\n1.75\n2.71\n2.83\n2.27\n4.44\n% environments solved\n50%\n44%\n39%\n50%\n33%\n55%\nEmpty-5x5\n0.96\n0.83\n0.75\n0.57\n0.37\n0.93\nEmpty-6x6\n0.97\n0.51\n0.83\n0.51\n0.32\n0.9\nEmpty-8x8\n0.96\n0.0\n0.0\n0.62\n0.0\n0.93\nEmpty-16x16\n0.0\n0.0\n0.93\n0.92\n0.67\n0.82\nFourRooms\n0.17\n0.07\n0.09\n0.09\n0.0\n0.09\nDoorKey-5x5\n0.96\n0.17\n0.02\n0.02\n0.0\n0.13\nDoorKey-6x6\n0.94\n0.04\n0.0\n0.04\n0.81\n0.13\nDoorKey-8x8\n0.29\n0.07\n0.0\n0.05\n0.0\n0.14\nDoorKey-16x16\n0.0\n0.03\n0.04\n0.0\n0.07\n0.17\nMultiRoom-N2-S4\n0.14\n0.02\n0.04\n0.02\n0.03\n0.15\nMultiRoom-N4-S5\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nMultiRoom-N6-S10\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nKeyCorridor-S3R1\n0.07\n0.0\n0.0\n0.0\n0.0\n0.05\nKeyCorridor-S3R2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nKeyCorridor-S3R3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nKeyCorridor-S4R3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nKeyCorridor-S5R3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nKeyCorridor-S6R3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\nTable 4.2: Highest average return over 100 episodes for each teacher type at the\nend of 1000 teacher steps. The None results are the average return over 100\nepisodes obtained when training only the environment of that row.\n39\n4.2. Grid World Experiments\n4.2.3\nTransfer Method\nIn this section, we evaluate the eﬀect of the transfer methods introduced in\nsection 3.1.1. We evaluate policy transfer, reward shaping transfer, and both\ntransfers combined. All teacher agents use the source task reward, as deﬁned in\nsection 3.2.2. The results for all seven teacher observation types are reported in\ntable 4.3 and the four baselines in table 4.4.\nFigure 4.5 shows the learning curves of the reinforcement learning agents. All\nagents with policy transfer surpass agents with reward shaping or the combined\nknowledge transfer. The learning curve is noisy in policy transfer due to the en-\nvironment changes, but the performance is steadily increasing. The performance\nof agents with policy transfer did not converge after 1000 teacher steps. For both\nreward transfer and the combined transfer, the performance converged during\nthe ﬁrst 50 to 100 CMDP steps and failed to improve from then on. The agent’s\nperformance with reward transfer scatters around a total mean return of 1 during\nthe whole training.\nFigure 4.5: Learning curves for diﬀerent teacher observations and transfer meth-\nods. After each teacher step the sum of the students average return over 100\nepisodes for each environment in V is plotted. We plot the learning curve with\nthe highest total reward at the end of training for every conﬁguration.\nApproximating the entire student state space by encoding the students’ weights\nwith PCA as described in section 3.2.1 fails. The results of the experiments with\nPCA observation space are very similar to the uniform baseline results.\nWe,\ntherefore, discard this method for the rest of our experiments.\nThe best results are obtained using the PTR, EMA, and LP observations with\na policy transfer. Using the fast-slow EMA as described in section 3.2.1 is not\nsuperior to standard EMA. In section 4.2.5 we evaluate the best α value for the\n40\n4.2. Grid World Experiments\nEMA observation. Using the reward history (RH) is signiﬁcantly worse than the\nother observation types when using policy transfer. RH resulted in the highest\ntotal mean return when using reward transfer but suﬀers from a high standard\ndeviation. In general, the results with policy transfer are more stable compared\nto the results with reward transfer.\nEnvironment / Metric\nRH\nPTR\nLP\nALP\nEMA\nFS-EMA\nPCA\nPolicy\nTotal mean return\n2.34 ± 0.24\n4.44 ± 0.33\n4.17 ± 0.33\n3.35 ± 0.33\n4.35 ± 0.42\n3.72 ± 0.22\n1.64 ± 0.24\n% environments solved\n55%\n55%\n55%\n55%\n55%\n61%\n44%\nEmpty-16x16\n0.58 ± 0.27\n0.07 ± 0.03\n0.9 ± 0.27\n0.08 ± 0.03\n0.82 ± 0.34\n0.91 ± 0.38\n0.0 ± 0.0\nFourRooms\n0.11 ± 0.03\n0.13 ± 0.03\n0.11 ± 0.02\n0.1 ± 0.04\n0.08 ± 0.01\n0.06 ± 0.03\n0.08 ± 0.02\nDoorKey-16x16\n0.05 ± 0.01\n0.04 ± 0.02\n0.07 ± 0.03\n0.13 ± 0.05\n0.17 ± 0.06\n0.06 ± 0.03\n0.03 ± 0.01\nMultiRoom-N2-S4\n0.1 ± 0.02\n0.06 ± 0.02\n0.09 ± 0.01\n0.06 ± 0.03\n0.15 ± 0.05\n0.09 ± 0.04\n0.0 ± 0.0\nKeyCorridor-S3R3\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nReward\nTotal mean return\n1.38 ± 0.85\n0.68 ± 0.3\n1.01 ± 0.32\n1.11 ± 0.63\n0.95 ± 0.1\n1.22 ± 0.58\n0.63 ± 0.21\n% environments solved\n50%\n44%\n61%\n50%\n11%\n50%\n34%\nEmpty-16x16\n0.56 ± 0.28\n0.0 ± 0.0\n0.46 ± 0.23\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nFourRooms\n0.04 ± 0.002\n0.03 ± 0.01\n0.05 ± 0.03\n0.03 ± 0.01\n0.0 ± 0.0\n0.03 ± 0.01\n0.0 ± 0.0\nDoorKey-16x16\n0.0 ± 0.0\n0.0 ± 0.0\n0.03 ± 0.01\n0.02 ± 0.01\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nMultiRoom-N2-S4\n0.0 ± 0.0\n0.0 ± 0.0\n0.06 ± 0.03\n0.0 ± 0.0\n0.0 ± 0.0\n0.02 ± 0.001\n0.0 ± 0.0\nKeyCorridor-S3R3\n0.003 ± 0.001\n0.003 ± 0.002\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.004 ± 0.001\n0.0 ± 0.0\nBoth\nTotal mean return\n0.0 ± 0.0\n0.47 ± 0.24\n0.0 ± 0.0\n0.32 ± 0.16\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n% environments solved\n0%\n17%\n0%\n17%\n0%\n0%\n0%\nEmpty-16x16\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nFourRooms\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.02 ± 0.01\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nDoorKey-16x16\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nMultiRoom-N2-S4\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nKeyCorridor-S3R3\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nTable 4.3: We compare the diﬀerent knowledge transfer methods for each teacher\nobservation type. Each conﬁguration is run with three diﬀerent random seeds.\nThe maximum return over 100 episodes at the end of 1’000 teacher steps is\nreported.\nAdditionally, we provide the standard deviation between the three\nruns.\nAgents with the combined knowledge transfer are failing to learn at all. In ﬁg. 4.5\nwe see that the average return drops from around 0.7 to 0 during the ﬁrst ten\nteacher steps and fails to recover from there. We visualized the synthetic reward\nsignal added by the reward transfer in ﬁg. 4.6 for the Empty-8x8 and Empty-\n16x16 environment. Each square in the image equals a square in the grid world\nenvironment. The black squares are wall objects, and the element in the bottom\nright is the green goal square. High state values estimated by the value function\nobtained in experiments with only reward transfer (images on the left) are scat-\ntered randomly around the grid. The estimated state value is between 0.12 and\n0.16. In the value function with a combined knowledge transfer, the state values\nare almost identical for all states and have an estimated value of 10 Million. We\n41\n4.2. Grid World Experiments\nbelieve that the agent starts to optimize and estimate its previous value function\nand this leads to an ever increasing value estimate. This extremely high addi-\ntional reward overshadows the actual reward signal of the environment, making\nit impossible for the agent to learn how to solve the environment successfully.\nEmpty-8x8 - Reward transfer\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nEmpty-8x8 - Both transfers\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n1e7\nEmpty-16x16 - Reward transfer\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nEmpty-16x16 - Both transfers\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n1e7\nFigure 4.6: We visualize the added reward, when using reward transfer (left\ncolumn) and combined transfer (right column). For every state in the Empty-\n8x8 (top row) and Empty-16x16 (bottom row) grid world environment we plot\nthe added reward. A light color encodes a high added reward, black equals to\nzero added reward. Dark tiles at the end of the grid are walls, the black tile in\nthe bottom right corner is the green goal square.\nIn ﬁg. 4.7 we plot the teacher’s sampling probability distribution over the teacher\ntimesteps next to the teacher’s learning curve. During the ﬁrst 100 episodes, the\nteacher samples tasks similar to a uniform distribution. The teacher favors the\nfamily of Empty grid world environments during the ﬁrst 200 episodes. From\nthen on, the teacher selects more challenging environments such as KeyCorridor\nand DoorKey environments. After roughly 400 teacher steps, the sample dis-\ntribution does not show the noisy changes in the sample probabilities as at the\nbeginning of the training. We interpret this as the teacher getting more conﬁdent\nin performing task sequencing. The teacher’s learning curve is noisy but steadily\nincreasing. There are some drastic drops in performance, probably due to select-\ning too challenging environments. The student always manages to recover after\nthose harmful teacher actions.\nTable 4.4 shows the same properties as described above. The Thompson sam-\n42\n4.2. Grid World Experiments\nFigure 4.7: We visualize the sample probability distribution and learning curve\nof one teacher agent over its training time. The colors in the legends are ordered\nupside down to the order in the plot.\npling experiment with reward transfer is interesting. This is the only experiment\nwhere the knowledge transfer with a reward signal surpassed agents with a policy\ntransfer. Therefore, this approach to transfer learning is feasible but very noisy,\nwhich is also indicated by the high standard deviation for the Thompson sam-\npling experiments with reward transfer. While uniformly sampling environments\nis a strong baseline, it still shows the worst performance across the baselines.\nIn general, sampling environments with the highest learning progress estimate\nyields the strongest baseline results.\n43\n4.2. Grid World Experiments\nEnvironment / Metric\nUniform\nLP\nThompson\nWindow\nPolicy\nTotal mean return\n1.75 ± 0.24\n2.71 ± 0.33\n1.94 ± 0.21\n2.28 ± 0.32\n% environments solved\n44%\n39%\n22%\n34%\nEmpty-16x16\n0.0 ± 0.0\n0.92 ± 0.23\n0.0 ± 0.0\n0.67 ± 0.14\nFourRooms\n0.07 ± 0.02\n0.09 ± 0.02\n0.0 ± 0.0\n0.0 ± 0.0\nDoorKey-16x16\n0.03 ± 0.01\n0.04 ± 0.01\n0.0 ± 0.1\n0.07 ± 0.03\nMultiRoom-N2-S4\n0.02 ± 0.01\n0.04 ± 0.01\n0.03 ± 0.01\n0.03 ± 0.01\nKeyCorridor-S3R3\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nReward\nTotal mean return\n0.8 ± 0.34\n0.51 ± 0.23\n2.83 ± 1.1\n0.45 ± 0.12\n% environments solved\n44%\n11%\n50%\n22%\nEmpty-16x16\n0.0 ± 0.0\n0.0 ± 0.0\n0.92 ± 0.36\n0.0 ± 0.0\nFourRooms\n0.03 ± 0.01\n0.0 ± 0.0\n0.09 ± 0.05\n0.0 ± 0.0\nDoorKey-16x16\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nMultiRoom-N2-S4\n0.06 ± 0.02\n0.0 ± 0.0\n0.02 ± 0.01\n0.0 ± 0.0\nKeyCorridor-S3R3\n0.01 ± 0.01\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nBoth\nTotal mean return\n0.47 ± 0.21\n0.0 ± 0.0\n0.0 ± 0.0\n0.52 ± 0.22\n% environments solved\n17%\n0%\n0%\n17%\nEmpty-16x16\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nFourRooms\n0.05 ± 0.01\n0.0 ± 0.0\n0.0 ± 0.0\n0.02 ± 0.01\nDoorKey-16x16\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nMultiRoom-N2-S4\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nKeyCorridor-S3R3\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nTable 4.4: We compare the diﬀerent knowledge transfer methods for each baseline.\nEach conﬁguration is run with three diﬀerent random seeds.\nThe maximum\nthe average students return of 100 episodes at the end of 1’000 teacher steps\nis reported. Additionally, we provide the standard deviation between the three\nruns.\n44\n4.2. Grid World Experiments\n4.2.4\nTeacher Reward Signal\nIn this section we evaluate the source task reward and the target reward. Both\nreward signals are deﬁned in section 3.2.2. We use the KeyCorridor-S3R3 envi-\nronment as the target task. The agents use a policy knowledge transfer between\ntasks. In table 4.5 we report experiments for both reward signals with six obser-\nvation types.\nFor all observation types, the reported results are better when using the source\ntask reward except for the RH experiments.\nThe reported total mean return\nhas a lower standard deviation over three diﬀerent random seeds when using the\nsource task reward instead of the target reward.\nEnvironment / Metric\nRH\nPTR\nLP\nALP\nEMA\nFS-EMA\nTotal Eval. Reward\nTotal mean return\n2.34 ± 0.24\n4.44 ± 0.33\n4.17 ± 0.33\n3.35 ± 0.33\n4.35 ± 0.42\n3.72 ± 0.22\n% environments solved\n55%\n55%\n55%\n55%\n55%\n61%\nEmpty-16x16\n0.58 ± 0.27\n0.07 ± 0.03\n0.9 ± 0.27\n0.08 ± 0.03\n0.82 ± 0.34\n0.91 ± 0.38\nFourRooms\n0.11 ± 0.03\n0.13 ± 0.03\n0.11 ± 0.02\n0.1 ± 0.04\n0.08 ± 0.01\n0.06 ± 0.03\nDoorKey-16x16\n0.05 ± 0.01\n0.04 ± 0.02\n0.07 ± 0.03\n0.13 ± 0.05\n0.17 ± 0.06\n0.06 ± 0.03\nMultiRoom-N2-S4\n0.1 ± 0.02\n0.06 ± 0.02\n0.09 ± 0.01\n0.06 ± 0.03\n0.15 ± 0.05\n0.09 ± 0.04\nKeyCorridor-S3R3\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nTarget Reward\nTotal mean return\n3.6 ± 0.49\n3.6 ± 0.36\n3.38 ± 0.62\n2.35 ± 0.66\n3.41 ± 0.51\n3.38 ± 0.42\n% environments solved\n55%\n55%\n55%\n55%\n55%\n61%\nEmpty-16x16\n0.58 ± 0.27\n0.07 ± 0.03\n0.24 ± 0.08\n0.5 ± 0.1\n0.72 ± 0.21\n0.63 ± 0.08\nFourRooms\n0.11 ± 0.03\n0.13 ± 0.03\n0.11 ± 0.04\n0.1 ± 0.04\n0.05 ± 0.01\n0.06 ± 0.02\nDoorKey-16x16\n0.05 ± 0.004\n0.02 ± 0.01\n0.0 ± 0.0\n0.08 ± 0.02\n0.09 ± 0.03\n0.04 ± 0.01\nMultiRoom-N2-S4\n0.07 ± 0.01\n0.11 ± 0.04\n0.01 ± 0.003\n0.06 ± 0.03\n0.02 ± 0.01\n0.02 ± 0.01\nKeyCorridor-S3R3\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\n0.0 ± 0.0\nTable 4.5: We compare the diﬀerent teacher reward signals for each teacher\nobservation type. Each conﬁguration is run with three diﬀerent random seeds.\nThe maximum return over 100 episodes at the end of 1’000 teacher steps is\nreported.\nAdditionally, we provide the standard deviation between the three\nruns.\nIn ﬁg. 4.8 we plot the target reward signal along the steps in the CMDP. The\nreward signal for the EMA, FS-EMA, LP, and ALP experiments are very close\nto zero during training, therefore not providing the teacher agent with a helpful\nsignal. For RH and PTR, the reward for the ﬁrst 200 teacher steps scatters around\n0.01 and 0.8 and then degenerates to the same reward signal as for the other\nobservation types. Surprisingly, this reward signal provided enough information\nto the teacher to better perform task sequencing than the baselines.\nThe target reward pushes the teacher agent towards solving the KeyCorridor-\n45\n4.2. Grid World Experiments\nS3R3 environment, but at the end of training the agent fails to solve this envi-\nronment with both reward signals. With both approaches, we did not manage to\nsolve the challenging KeyCorridor-S3R3 environment.\nFigure 4.8: Visualization of the target reward signal for each teacher observation\ntype over the teachers training cycle.\n4.2.5\nChoosing Alpha for Exponential Moving Average\nIn this section, we optimize the α value for the exponential moving average. An\nα value close to one favors current values over old values in a time series. We\nexperimented with α ∈[0.1, 0.3, 0.5, 0.7, 0.9] and reported the results in table 4.6.\nThe teacher agents use a policy transfer and the source task reward.\nBy looking at the results in table 4.6 it is hard to settle for an α value. While\nan α of 0.5 or 0.9 have the highest total mean returns, the results for challenging\nenvironments are the best when selecting an alpha value of 0.3. The experiments\nwith a value of 0.7 and 0.9 suﬀer from a high standard deviation in the reported\ntotal mean return.\nIn ﬁg. 4.9 we plot the average learning curve over three diﬀerent random seeds\nfor all α values. An α value of 0.1 yields the worst results. The learning curve\nof a value with 0.3 is below higher α values but surpassed most of the other\nexperiments in the last 20 CMDP steps. The learning curves for an α value of\n0.5, 0.7, and 0.9 follow each other closely. Our experiments do not allow us to\nselect a clear winner. Because of the highest total mean return combined with a\nrelatively small standard deviation, we propose to use an alpha value of 0.5.\n46\n4.2. Grid World Experiments\nEnvironment / Metric\n0.1\n0.3\n0.5\n0.7\n0.9\nTotal mean return\n3.49 ± 0.43\n3.01 ± 0.39\n4.35 ± 0.42\n3.89 ± 0.58\n4.23 ± 0.95\n% environments solved\n50%\n61%\n55%\n55%\n61%\nEmpty-16x16\n0.32 ± 0.06\n0.58 ± 0.27\n0.82 ± 0.34\n0.97 ± 0.4\n0.94 ± 0.28\nFourRooms\n0.08 ± 0.01\n0.13 ± 0.03\n0.08 ± 0.01\n0.09 ± 0.04\n0.06 ± 0.01\nDoorKey-16x16\n0.0 ± 0.0\n0.05 ± 0.02\n0.17 ± 0.06\n0.0 ± 0.0\n0.02 ± 0.01\nMultiRoom-N2-S4\n0.15 ± 0.06\n0.08 ± 0.03\n0.15 ± 0.05\n0.13 ± 0.02\n0.29 ± 0.12\nKeyCorridor-S3R3\n0.0 ± 0.0\n0.01 ± 0.004\n0.0 ± 0.0\n0.0 ± 0.0\n0.004 ± 0.002\nTable 4.6: We compare the diﬀerent α values for the EMA observation type.\nEach conﬁguration is run with three diﬀerent random seeds.\nThe maximum\nreturn of 100 episodes at the end of 1’000 teacher steps is reported. Additionally,\nwe provide the standard deviation between the three runs.\nFigure 4.9: The average learning curve of three runs with three diﬀerent random\nseeds for each α value.\n47\n4.2. Grid World Experiments\n4.2.6\nSample Eﬃciency\nIn this section, we analyze the sample eﬃciency of an agent when trained in the\nteacher-student setting compared to trained directly on the target task. Learning\ncurves compare the agent’s experience with their performance. We use the teacher\nstep on the x-axis to measure the experience. One teacher step equals 10’000 steps\nin an environment. The performance is measured by the average return on 100\nevaluation episodes during the training. In ﬁg. 4.10 we plot three learning curves,\none for each of the following environments: Empty-16x16, DoorKey16x16, and\nFourRooms. The sample eﬃciency of an agent can be measured as the area under\nthe curve with transfer minus the area under the curve without transfer. We do\nthe evaluation manually by looking at the curves.\nIn the Empty16x16 environment, the sample eﬃciency of the agents with curricu-\nlum learning is negative for the ﬁrst 400 teacher steps. After 400 teacher steps,\nthe agent trained directly in the Empty16x16 environment suﬀers from a catas-\ntrophic update and fails to recover from then on. It seems like the agent got stuck\nin a bad local optimum. This is probably due to a bad choice in the hyperpa-\nrameters. We note that agents trained directly on the Empty16x16 environment\nwith three random seeds showed the same behavior.\nIn the DoorKey16x16 environment, all curriculum learning agents have a positive\nsample eﬃciency. The learning curve is very noisy due to the training in diﬀerent\nenvironments. Although we notice an improvement in the sample eﬃciency, the\nagent’s performance is not monotonically increasing and dropping to the same\nperformance level as when trained directly in the DoorKey16x16 environment.\nOne could perform early stopping when crossing a reward threshold to solve this\nissue, but deﬁning such a threshold is not straightforward.\nThere is no improvement in the sample eﬃciency in the FourRooms environment.\nCurriculum learning with the PTR teacher shows the best results with a slight\nimprovement in sample eﬃciency during some learning stages.\n48\n4.2. Grid World Experiments\n0\n200\n400\n600\n800\n1000\nTeacher Step\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Return\nLearning Curve - Empty-16x16\nRH\nPTR\nLP\nALP\nEMA\nFS-EMA\nNone\n0\n200\n400\n600\n800\n1000\nTeacher Step\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAverage Return\nLearning Curve - DoorKey-16x16\nRH\nPTR\nLP\nALP\nEMA\nFS-EMA\nNone\n0\n200\n400\n600\n800\n1000\nTeacher Step\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nAverage Return\nLearning Curve - FourRooms\nRH\nPTR\nLP\nALP\nEMA\nFS-EMA\nNone\nFigure 4.10:\nWe visualize the learning curves of our agent when trained in\nthe teacher-student setup and directly on the target task for the Empty-16x16,\nDoorKey16x16, and FourRooms environment. We use a teacher with policy trans-\nfer, source task reward signal, and the six partially observable inputs. The learn-\ning curve of the agent trained directly on the task is labeled with ”None”.\n49\n4.2. Grid World Experiments\n4.2.7\nGenerality of Agents\nWe compare the generality of agents trained with curriculum learning with agents\ntrained in a single environment. To measure the generality, we evaluate the agent\nat the end of training on ﬁve diﬀerent random seeds for 100 episodes on each\nenvironment in the task-set V. After the evaluation, we calculate the total mean\nreturn and the percentage of environments solved with ﬁve random seeds and\nreport the median of both measurements in table 4.7.\nAgents trained with curriculum learning are more general than agents trained in a\nsingle environment. With every teacher, we could solve more environments than\ntraining in one environment. The agent trained solely in Empty-6x6 scored a total\nmean return of 3.25. Most of the obtained reward is received in the easy Empty\nenvironment family.\nSome environments are especially useful to generalize to\nother environments. The FourRooms and MultiRoom-N2-S4 environment solve\na high percentage of environments.\n50\n4.2. Grid World Experiments\nEnvironment trained on\nTotal mean return\n% environments solved\nEmpty-5x5\n2.1\n33%\nEmpty-6x6\n3.25\n28%\nEmpty-8x8\n0.32\n22%\nEmpty-16x16\n0.02\n6%\nFourRooms\n2.41\n56%\nDoorKey-5x5\n1.89\n28%\nDoorKey-6x6\n2.58\n39%\nDoorKey-8x8\n1.0\n44%\nDoorKey-16x16\n0.6\n39%\nMultiRoom-N2-S4\n1.48\n56%\nMultiRoom-N4-S5\n0.66\n39%\nMultiRoom-N6-S10\n0.99\n33%\nKeyCorridor-S3R1\n0.47\n28%\nKeyCorridor-S3R2\n0.53\n28%\nKeyCorridor-S3R3\n0.32\n22%\nKeyCorridor-S4R3\n0.21\n11%\nKeyCorridor-S5R3\n0.11\n11%\nKeyCorridor-S6R3\n0.72\n28%\nCurriculum RH\n2.64\n56%\nCurriculum PTR\n4.14\n61%\nCurriculum LP\n3.9\n61%\nCurriculum ALP\n3.06\n67%\nCurriculum EMA\n3.93\n61%\nCurriculum FS-EMA\n3.59\n61%\nTable 4.7: We report the median measurements obtained by evaluating each\nagent with ﬁve random seed for 100 episodes on every environment in V.\n51\n4.3. Google Football Environment\n4.3\nGoogle Football Environment\nGoogle Research Football environment [Kurach et al., 2019] is a novel 3D RL en-\nvironment to provide a highly optimized, stochastic, and open-source simulation.\nThe environment provides single-agent RL, where the agent controls all players\nof his team, and multi-agent RL, where a separate agent controls each player. It\nis also possible to research the eﬀect of self-play, where the agent plays against\ndiﬀerent versions of itself.\nThe environment provides a comprehensive set of\nprogressively more demanding and diverse scenarios with the Football Academy.\nThese scenarios enable us to analyze our algorithm on a range of tasks requiring\ndiﬀerent levels of abstractions and diﬀerent tactics.\nThe engine implements a full 11 vs. 11 football game with the standard rules\nincluding goal kicks, corner kicks, yellow and red cards, oﬀsides, handballs, and\npenalty kicks as shown in ﬁg. 4.11. This full 11 vs. 11 football game, consisting\nof 3000 frames, is called Football Benchmark.\nPlayers have diﬀerent characteristics like speed or accuracy, but both teams have\nthe same set of players. Further, players are getting tired over time, which inﬂu-\nences their behavior and skills.\nFigure 4.11: The Google Football Engine is a football simulation which supports\nthe major football rules like kickoﬀs (top left), goals (top right), fouls, cards\n(bottom left), corner and penalty kicks (bottom right), and oﬀside.\n[Kurach\net al., 2019]\n52\n4.3. Google Football Environment\n4.3.1\nState & Observations\nThere are three ways to represent the environment state at the current time step\nto the reinforcement learning agent (called observation).\npixel. The representation is a 1280 x 720 x 3 tensor corresponding to the rendered\nscreen. The scoreboard and a mini-map at the bottom of the image are present\nin this representation. The mini-map tells the location of the ball and the players\nof both teams.\nSuper Mini Map (SMM). The SMM is a 72 x 96 x 4 tensor encoding information\nabout both teams, the ball, and the currently active player. The encoding is\nbinary and indicates whether there is a player or the ball at the given coordination\nor not.\nFloats. This representation uses a 115-dimensional vector to capture the game\nstate, player coordinates, ball coordinates, possession, and the active player.\n4.3.2\nActions\nThe agent can execute one of 20 actions per time step. The currently active\nplayer executes all of those actions, except the keeper rush action. The active\nplayer moves by selecting one of eight dedicated moving actions. There are four\ndiﬀerent ways to kick the ball (short pass, high pass, long pass, shot). The move,\nsprint, and dribble actions are sticky and have to be ended explicitly by their\nrespective stop action. Finally, there are sliding and do-nothing actions.\n4.3.3\nRewards\nThe environment provides two diﬀerent reward functions to choose from. It is\nalso possible to deﬁne custom reward functions to look into reward shaping. In\nthis work, we used the out-of-the-box reward functions.\nSCORING rewards the agent when scoring a goal with a +1 reward and a -1 reward\nwhen conceding one. This reward signal is sparse and can lead to no signal during\nthe early stages of learning where the agent does not know how to overcome the\nopponent’s defense.\nCHECKPOINT is an additional shaped reward designed to overcome the sparsity of\nthe SCORING reward signal. Once per episode, the agent receives a +0.1 reward\nfor getting closer to the opponent’s goal measured by the Euclidean distance. The\nopponent’s ﬁeld is divided into ten checkpoint regions, and the agent receives the\n+0.1 reward once for every region. The agent also receives all non-collected check-\npoint rewards when scoring to avoid penalizing agents that do not go through all\nthe checkpoints before scoring.\n53\n4.3. Google Football Environment\n4.3.4\nFootball Academy Scenarios\nIn addition to the full 11 vs. 11 football game, the environment allows agents\nto train on a set of 11 progressively more complex scenarios. This set of tasks is\ncalled Football Academy where it is possible to deﬁne custom scenarios to train\nagents for a particular situation. In table 4.8 the available scenarios are described.\n4.3.5\nMDP Statement\nThe Google research Football\nenvironment does not fulﬁll the Markov prop-\nerty if we only consider the pixel input at time step t as observation ot. We do not\nknow in which direction the ball or the players are moving from a single observa-\ntion. It is possible to have two identical observations, but in one observation, the\nball moves to the left of the pitch, and in the other, it moves to the right. The\ndirection and the velocity of the ball can not be inferred based on a single image.\nThe agent only partially observes its environment’s state, making it a partially\nobservable Markov decision process (POMDP).\nThere are a few possibilities to overcome this issue:\n• We can use another state representation than pixels. This representation\nwould have to encode the direction and velocity of the ball and the players.\n• We can use a long short-term memory (LSTM) [Hochreiter and Schmid-\nhuber, 1997] inside the agents function approximator. The observation ot\ntogether with the hidden state of the LSTM ht are enough to overcome the\nuncertainty about the current state st of the environment.\n• We can use the last k observations to approximate the true state st. This\nis the solution proposed by Mnih et al. [2015].\nIn our work, we used the last four observations stacked on top of each other,\nillustrated in ﬁg. 4.12, as input to approximate the true Markov state.\nFigure 4.12: To approximate the Markov property we use a frame stack of the\nlast 4 frames as an input for our neural networks.\n54\n4.3. Google Football Environment\nName\nDescription\nEmpty Goal Close\nOur player starts inside the box with the ball and\nneeds to score against an empty goal.\nEmpty Goal\nOur player starts in the middle of the ﬁeld with the\nball and needs to score against an empty goal.\nRun to Score\nOur player starts in the middle of the ﬁeld with the\nball and needs to score against an empty goal. Five\nopponent players chase ours from behind.\nPass and Shoot\nwith Keeper\nTwo of our players try to score from the edge of the\nbox. One is on the side with the ball and next to a\ndefender. The other is at the center, unmarked and\nfacing the opponent keeper.\nRun, Pass and\nShoot with Keeper\nTwo of our players try to score from the edge of the\nbox. One is on the side with the ball and unmarked.\nThe other is at the center, next to a defender, and\nfacing the opponent keeper.\nEasy\nCounter-Attack\n4 versus 1 counter-attack with keeper; all the remain-\ning players of both teams run back towards the ball.\nHard\nCounter-Attack\n4 versus 2 counter-attack with keeper; all the remain-\ning players of both teams run back towards the ball.\n11 versus 11 with\nLazy Opponents\nFull 11 versus 11 game, where the opponents cannot\nmove but intercept the ball if it is close enough to\nthem. Our center-back defender has the ball at ﬁrst.\nThe maximum duration of the episode is 3000 frames\ninstead of 400 frames.\n11 versus 11 easy\nFull 11 versus 11 game, with a duration of 3000\nframes per episode.\nThe game starts with a kick-\noﬀ, the team starting with the kick-oﬀis assigned\nrandomly. The agent receives a reward of -1 when\nreceiving a goal and a reward of +1 when scoring\none. The opponents diﬃculty is set to easy.\n11 versus 11\nmedium\nSame as 11 versus 11 easy, but the opponents diﬃ-\nculty is set to medium.\n11 versus 11 hard\nSame as 11 versus 11 easy, but the opponents diﬃ-\nculty is set to hard.\nTable 4.8: Description of the Google Football environments taken directly from\nKurach et al. [2019]. All scenarios end after 400 frames, or if the ball is lost, a\nteam scores or the game stops (e.g., if the ball leaves the pitch or a free kick is\nawarded).\n55\n4.4. Google Football Experiments\n4.4\nGoogle Football Experiments\n4.4.1\nExperimental Setup\nThe experimental setup for the Google Football environment is similar to the\ngrid world environment. As in grid world, we train the teacher agent for 1′000\nsteps, and for each teacher step, we train the student agent for 10′000 steps in\nthe selected environment. The student agent is therefore trained for 10 million\nsteps in total. After each step, we evaluate the student on all environments in\nV for 100 episodes. In our previous work [Schraner, 2020], we train the student\nagent for 50 million steps. Due to a limit in computational resources, we limit the\ntraining in this thesis to 10 million student steps and only one random seed per\nexperiment. Executing one experiment with the described amount of steps takes\naround ﬁve days. We ﬁxed the knowledge transfer to policy transfer and used the\nsource task reward signal. The hyperparameters are reported in appendix A.3.\nWe use all Google Football environments listed in table 4.8. The student agent\nreceives the SCORING reward signal.\nNetwork architecture\nThe network architecture for the teacher agent is de-\nscribed in section 3.2.4. The student’s network architecture is the same as in our\nprevious work [Schraner, 2020] and depicted in ﬁg. 4.13.\nActor-critic algorithms can share the network torso and represent the policy and\nvalue function as individual heads. This allows weight sharing between the two\nfunction approximators. Sharing weights can improve the training time and forces\nthe shared part to be more general. This may prevent over-ﬁtting. As we share\nnetwork weights, we get diﬀerent gradients from diﬀerent loss functions. In our\nwork, we made use of sharing weights.\nEach network uses the SMM observation, with a shape of 72×96×4. The football\nﬁeld has a dimension of 72 × 96, and there is a separate channel for each team,\nthe ball, and the currently active player. To approximate the Markov property,\nwe use a frame stack of the last four frames, as shown in ﬁg. 4.12.\n56\n4.4. Google Football Experiments\nFigure 4.13: CNN network architecture with the stacked Super Mini Map obser-\nvation as input. The one-hot encoded previous action at−1 is used as an additional\ninput. The policy and value function are two heads sharing the network torso.\n57\n4.4. Google Football Experiments\n4.4.2\nResults\nWe transfer the best CMDP teacher settings from the MiniGrid environment to\nthe Google Research Football environment. The four baseline methods and our\nresults in our previous work are used to compare the results in table 4.9. We\nreport the average return of 100 episodes per environment after training on one\nrandom seed. Therefore the results should be interpreted with care.\nThe LP and Thompson baseline are by far the worst. In the case of Thompson\nsampling, only one out of eleven environments were solved.\nThe LP teacher\nfails for the 11 vs. 11 full game. Sampling environments uniformly works well\non easy environments but fails for more challenging ones such as the counter-\nattack and 11 vs. 11 environments. The window baseline is the strongest of all\nbaselines. With our proposed teacher-student setting we surpass our baselines on\nall environments. The LP based teacher had the best results on empty goal close,\nempty goal and run to score, but the improvement over the other experiments is\nmarginal.\nCompared to the results in our previous work [Schraner, 2020] reported in ta-\nble 4.10 we do not match the results. Training optimized PPO for 50 million\nsteps leads to a return of -1.4 in the 11 vs. 11 hard environment. The highest\nscore we obtained in this environment was -1.45 with the window baseline and\nLP teacher. Using a manually deﬁned curriculum, we archive a return between\n-2.08 and 1, depending on the curriculum.\nUsing the ALP, RH, PTR, and LP observation types leads to the best results.\nWhereas using ALP gives the highest total average and PTR the highest number\nof solved environments. The EMA-based teachers fall behind, probably due to a\nnot-tuned α value for the Google Football environments.\n58\n4.4. Google Football Experiments\nEnvironment\n/ Metric\nUniform\nLP\nThompson\nWindow\nRH\nPTR\nLP\nALP\nEMA\nFS-EMA\nTotal mean\nreturn\n-3.26\n-11.62\n-2.67\n-0.81\n-0.3\n-0.28\n-0.5\n1.84\n-1.36\n-1.51\n% environments\nsolved\n45%\n27%\n9%\n55%\n55%\n64%\n45%\n45%\n45%\n45%\nEmpty Goal\nClose\n0.99\n1\n-0.05\n1\n0.96\n0.98\n1\n0.94\n0.79\n1\nEmpty Goal\n0.84\n1\n0.64\n0.93\n0.85\n0.67\n0.98\n0.93\n0.6\n0.76\nRun to Score\n0.88\n0.98\n0.0\n0.76\n0.95\n0.7\n0.05\n0.98\n0.67\n0.91\nPass and Shoot\nwith Keeper\n0.12\n0.0\n-0.02\n0.0\n0.33\n0.03\n0.0\n0.51\n0.0\n-0.07\nRun, Pass and\nShoot with\nKeeper\n0.02\n0.0\n0.0\n0.0\n-0.11\n-0.02\n0.0\n0.05\n0.0\n-0.17\nEasy\nCounter-Attack\n0.0\n-0.02\n0.0\n0.23\n0.33\n0.17\n0.0\n0.0\n0.02\n0.0\nHard\nCounter-Attack\n0.0\n0.0\n0.0\n0.22\n0.18\n0.05\n0.03\n0.0\n-0.05\n0.02\n11 vs. 11 lazy\n0.0\n-0.1\n0.0\n0.05\n0.0\n0.02\n0.16\n0.0\n0.12\n0.08\n11 vs. 11 easy\n-1.55\n-3.8\n-0.55\n-0.53\n-0.42\n0.0\n0.0\n-0.23\n-0.85\n-0.55\n11 vs. 11\nmedium\n-1.98\n-5.28\n-1.48\n-1.23\n-1.75\n-1.2\n-1.08\n-0.65\n-1.58\n-1.78\n11 vs. 11 hard\n-2.63\n-4.98\n-1.5\n-1.45\n-1.88\n-1.85\n-1.45\n-1.7\n-1.9\n-1.7\nTable 4.9: Google football results obtained on one random seed after training for\n1’000 CMDP steps, equal to 10 million student steps. The average return over\n100 episodes is reported. We use policy transfer between the source tasks and\nthe source task reward signal.\nExperiment\nReturn\nPPO\n-1.4\nBest scenarios curriculum\n-0.85\nBest 11 vs 11 curriculum\n-2.08\nBest increasing curriculum\n-0.48\nBest smooth increasing curriculum\n1\nPrioritized level replay\n-1.05\nTable 4.10: Comparison of our diﬀerent curriculum learning approaches of our\nprevious work Schraner [2020], the average return over 100 episodes on the 11 vs\n11 hard environment is reported.\n59\n4.4. Google Football Experiments\nIn table 4.11 we report our results obtained in our previous work [Schraner, 2019]\nby training an agent with IMPALA in a single Google Football environments.\nWith our curriculum learning approach, we can match and surpass the results of\nour previous work by only using a ﬁfth of the samples. Compared to the baselines\npublished with the Google Football environment [Kurach et al., 2019] we archived\nbetter results on the easy environments (Empty Goal, Run to Score, Pass and\nShoot with Keeper) but slightly worse on the diﬃcult environments (Run, Pass\nand Shoot with Keeper, Easy Counter-Attack, Hard Counter-Attack, and 11 vs.\n11 lazy).\nScenario\nOurs@50M\nGoogle Baseline@50M\nEmpty Goal Close\n0.99\n1.0\nEmpty Goal\n0.84\n0.86\nRun to Score\n0.88\n0.88\nPass and Shoot with Keeper\n0.0\n0.66\nRun, Pass and Shoot with Keeper\n-0.05\n0.18\nEasy Counter-Attack\n0.0\n0.5\nHard Counter-Attack\n-0.02\n0.2\n11 vs. 11 lazy\n0.01\n0.2\n11 vs. 11 easy\n-1.59\n-0.35\n11 vs. 11 medium\n-1.6\n-0.79\n11 vs. 11 hard\n-2.78\n-1.16\nTable 4.11: Results on the Football Academy scenarios obtained in our previous\nwork [Schraner, 2019] and the Google baseline [Kurach et al., 2019] with the\nIMPALA algorithm. The reported results is the average reward of 100 episodes\nafter a training on 50 million frames.\nChapter 5\nDiscussion\nRecent work in curriculum learning for rl has shown promising results in improv-\ning sample eﬃciency and asymptotic performance in challenging environments.\nMost of these works focused on automatic task generation e.g., by using pro-\ncedural generated environments [Wang et al., 2019], changing the initial state\ndistribution to easy starts [Florensa et al., 2017], creating additional synthetic\ngoals to guide the student [Racani`ere et al., 2020] or generating auxiliary tasks\n[Riedmiller et al., 2018]. Using self-play to create a curriculum was a core idea\nin AlphaStar [Vinyals et al., 2019]. Methods to automatically select tasks in a\nteacher-student setting are also popular. Most of those methods use heuristics\nor sampling-based approaches to select the tasks [Matiisen et al., 2019, Foglino\net al., 2019a,b, Narvekar et al., 2017, Narvekar and Stone, 2019, Kanitscheider\net al., 2021]. In our thesis, we propose a new approach to formulate the task\nsequencing problem as an MDP and train a teacher rl agent to perform the task\nsequencing while training the student simultaneously.\nOur experiments show that performing task sequencing with an rl teacher agent\nis superior to heuristic-based task sequencing. On the grid world environment,\nwe were able to outperform all baselines signiﬁcantly. Our approach failed to\nincrease the asymptotic performance on most of the MiniGrid when compared to\ntablua-rasa rl. Training an agent directly in easy grid world environments leads\nto better performance compared to curriculum learning approaches. Our Google\nFootball experiments improved the asymptotic performance on easy environments\nsuch as Empty Goal, Run to Score, and Pass and Shoot with Keeper. We believe\nthat the root cause of failing on challenging environments lies in the teacher’s\nreward signal. Curriculum learning with the source task reward signal is in some\ncases beneﬁcial for more challenging environments such as DoorKey-16x16, but\nit is diﬃcult to target the teacher agent towards speciﬁc environments. Using\na targeted reward signal to guide the teacher agent towards more challenging\nenvironments is often insuﬃcient, as the teacher faces a sparse reward signal\nin that setting. When providing the teacher with a sparse reward signal, our\nteacher-student settings collapses towards a uniform sampling approach.\nThe teacher agent’s learning curves did not converge after 1’000 steps. Training\nboth agents beyond 1’000 CMDP steps might increase the asymptotic perfor-\nmance and help the student agent solve more challenging environments. This\nwould increase the asymptotic performance at the cost of sample eﬃciency. We\n60\n61\nﬁxed the CMDP and student steps to match the number of training steps used\nin our no curriculum learning experiments to carry out a fair comparison. When\nspecifying the number of teacher and student steps, we have to balance the\nteacher’s and student’s performance.\nUsing fewer teacher steps might not be\nsuﬃcient for the teacher to learn how to perform task sequencing but would\ngive the student more time to converge in the selected environment. Using more\nteacher steps is beneﬁcial for the teacher, but on the other hand, the student has\nless time to learn the selected task. Using fewer student steps might lead to a\nnoisy reward signal for the teacher agent. In future work, it would be interest-\ning to analyze the importance and eﬀect of those values. One could also train\nthe student until convergence or make the number of student steps part of the\nteacher’s action space.\nWhen comparing the three transfer methods policy, reward and combined, it\nturns out that the policy transfer leads to the best results.\nReward transfer\nis volatile for both the rl teacher agents and the baselines. Combining reward\ntransfer with policy transfer fails. After a few CMDP steps, the added reward\nbonus covers the environment’s reward, forcing the student agent to maximize\nits reward signal.\nWe compared six partial observable state representations and one approximately\nfully observable state representation (PCA) for the CMDP state. The PCA rep-\nresentation type failed utterly. There are other ways to represent neural network\nweights in a compact embeddings, such as autoencoders or network distillations.\nIn future work, we can investigate other methods to represent the student weights.\nThe results of the six partial observable representations did not diﬀer too much\nfrom each other. Overall the PTR, LP, and EMA representation resulted in the\nhighest asymptotic rewards and the most general student agents for the Mini-\nGrid environments. In our Google Football environment RH, PTR, LP and ALP\nachived the highest asymptotic reward and percentage of solved environments. In\nthat environment tuning the α value for the EMA representation might improve\nthe results.\nThe proposed curriculum learning approach is beneﬁcial for multi-task reinforce-\nment learning. The reward across all tasks and the overall percentage of solved\nenvironments are signiﬁcantly higher compared to our baselines or when training\nan agent on a single environment.\nWe investigated the sample eﬃciency of our teacher-student setup in three grid\nworld environments. The sample eﬃciency improves in the Empty-16x16 and\nDoorKey16x16 environment but remains similar in the FourRooms environment.\nWe discovered that training in the proposed curriculum setup is noisy regard-\ning the student’s performance throughout training. The sample eﬃciency on the\nGoogle Football environments increased considerably. By only using one-ﬁfth of\nthe samples, we almost matched or surpassed the performance on 8 out of 11 en-\n62\nvironments compared to directly training in those environments. Training agents\nfor the 11 vs. 11 football game remains diﬃcult with our proposed curriculum\nlearning approach. The asymptotic performance remained the same compared\nto training directly in an environment. Using a careful, manually deﬁned cur-\nriculum improves the asymptotic performance compared to our teacher-student\nsetup. Our previous work concluded that deﬁning such a curriculum by hand\nrequires a lot of tweaking and domain knowledge.\nIn this work, we analyzed diﬀerent knowledge transfer methods, teacher obser-\nvation types, and reward signals. Using policy transfer combined with the PTR,\nLP, or EMA representation and the source task reward reward signal, we observe\nthe following:\n• The generality of our agent improves.\n• The asymptotic performance in some environments increases compared to\nusing no curriculum.\n• Training an rl agent to perform task sequencing is superior over our heuristic-\nbased baselines.\n• The sample eﬃciency compared to no curriculum does increase in one out\nof three analyzed grid world environments.\n• The sample eﬃciency compared to no curriculum does increase by a factor\nof ﬁve on 8 out of 11 Google Football environments.\nThe improvement in the generality and sample eﬃciency with curriculum has\nto be investigated more carefully in future research. Another weakness of our\nwork is the limited hyperparameter search. The eﬀect of training teachers or\nstudents for more steps remains unclear. Other types of rl algorithms such as\nDQN or traditional tabular methods for the teacher agent have to be tested. The\nteacher network architecture should be tuned further. Our agents trained without\ncurriculum learning on the grid world environments results in zero rewards for\nhard environments. To perform more meaningful analysis, we propose to tune\nthose baseline agents and then transfer them to the teacher-student setting. It\nremains unclear if other PPO hyperparameters for the teacher training would\nimprove performance and how well these parameters transfer across all methods.\nChapter 6\nConclusion\nIn this thesis, we successfully layout a teacher-student curriculum learning ap-\nproach with automated online task sequencing to improve the generality and sam-\nple eﬃciency of our rl agents. Our CMDP framework allows training a teacher\nand a student agent simultaneously, where the teacher selects tasks for the stu-\ndent.\nThe student’s asymptotic performance does not increase for most grid\nworld and Google Football environments compared to non-cl agents. We evalu-\nated multiple transfer methods, teacher reward signals, and CMDP observations\non the MiniGrid and the Google Research Football environment. This work is a\nstep towards successful multi-task reinforcement learning agents.\nWe found that knowledge transfer through policy transfer works well and is robust\nacross a variety of teachers. In future work, it might be interesting to experiment\nwith additional knowledge transfer methods such as macro actions [Vezhnevets\net al., 2016] and task models [Shao et al., 2019]. The choice of teacher observation\nis not as important as the transfer method. PTR, LP, ALP, and EMA proved to\nprovide useful observations. The teacher’s reward signal is essential for training\nthe teacher fast. The source task reward provides the teacher agent with a rich\nreward signal and encourages to ﬁnd a general student agent. With this reward\nsignal, it is diﬃcult to target speciﬁc environments to solve, but using the target\nreward signal is too sparse. One could use a weighted form of the source task\nreward to lay focus on a subset of the tasks while still providing a rich reward\nsignal. Another option would be a combination of both reward signals, where\nthe source task reward signal is used at the beginning of training and slowly\nexchanged with the target reward.\nFor future work, it would be interesting to experiment with diﬀerent rl algorithms\nto train the teacher. One could use traditional rl methods such as Q learning or\noﬀ-policy algorithms with experience replay such as algorithms from the DQN\nfamily. More work must be dedicated to the hyperparameter tuning of the rl\nalgorithms, teacher-student step settings, and network architectures.\nAlthough our work did not increase the asymptotic performance on most of the\nMiniGrid and Google Football environments, we increased the overall perfor-\nmance across the tasks and the sample eﬃciency. Future work should experi-\nment with other domains such as robotics and further tune the hyperparameters\nto better understand our ﬁndings.\n63\nBibliography\nD. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche,\nJ. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman,\nD. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach,\nK. Kavukcuoglu, T. Graepel, and D. Hassabis, “Mastering the game of go\nwith deep neural networks and tree search,” Nature, vol. 529, no. 7587, pp.\n484–489, 2016.\nD. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,\nT. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre,\nG. van den Driessche, T. Graepel, and D. Hassabis, “Mastering the game of\ngo without human knowledge,” Nature, vol. 550, no. 7676, pp. 354–359, 2017.\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,\nM. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. Lillicrap, K. Simonyan, and\nD. Hassabis, “A general reinforcement learning algorithm that masters chess,\nshogi, and go through self-play,” Science, vol. 362, no. 6419, pp. 1140–1144,\n2018.\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,\nA. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen,\nC. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra,\nS. Legg, and D. Hassabis, “Human-level control through deep reinforcement\nlearning,” Nature, vol. 518, no. 7540, pp. 529–533, 2015.\nOpenAI, :, C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Den-\nnison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, R. J´ozefowicz, S. Gray,\nC. Olsson, J. Pachocki, M. Petrov, H. P. de Oliveira Pinto, J. Raiman, T. Sali-\nmans, J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski, and\nS. Zhang, “Dota 2 with large scale deep reinforcement learning,” 2019.\nO. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung,\nD. H. Choi, R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss,\nI. Danihelka, A. Huang, L. Sifre, T. Cai, J. P. Agapiou, M. Jaderberg, A. S.\nVezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Budden, Y. Sulsky,\nJ. Molloy, T. L. Paine, C. Gulcehre, Z. Wang, T. Pfaﬀ, Y. Wu, R. Ring,\nD. Yogatama, D. W¨unsch, K. McKinney, O. Smith, T. Schaul, T. Lillicrap,\nK. Kavukcuoglu, D. Hassabis, C. Apps, and D. Silver, “Grandmaster level in\nstarcraft ii using multi-agent reinforcement learning,” Nature, vol. 575, no.\n7782, pp. 350–354, 2019.\n64\n65\nBibliography\nOpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew,\nA. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak,\nJ. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang, “Solv-\ning rubik’s cube with a robot hand,” 2019.\nA. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi, D. Guo, and\nC. Blundell, “Agent57: Outperforming the atari human benchmark,” 2020.\nD. A. Pomerleau, ALVINN: An Autonomous Land Vehicle in a Neural Network.\nSan Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1989, p. 305–313.\nS. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning\nand structured prediction to no-regret online learning,” ser. Proceedings\nof Machine Learning Research, G. Gordon, D. Dunson, and M. Dud´ık,\nEds., vol. 15.\nFort Lauderdale, FL, USA: JMLR Workshop and Conference\nProceedings, 11–13 Apr 2011, pp. 627–635.\nC. Scheller, Y. Schraner, and M. Vogel, “Sample eﬃcient reinforcement learning\nthrough learning from demonstrations in minecraft,” ser. Proceedings of\nMachine Learning Research, H. J. Escalante and R. Hadsell, Eds., vol. 123.\nVancouver, CA: PMLR, 08–14 Dec 2020, pp. 67–76.\nR. S. Sutton, D. Precup, and S. P. Singh, “Intra-option learning about temporally\nabstract actions,” in Proceedings of the Fifteenth International Conference on\nMachine Learning, ser. ICML ’98.\nSan Francisco, CA, USA: Morgan Kauf-\nmann Publishers Inc., 1998, p. 556–564.\nP. Dayan and G. E. Hinton, “Feudal reinforcement learning,” in Advances in\nNeural Information Processing Systems 5, S. J. Hanson, J. D. Cowan, and\nC. L. Giles, Eds.\nMorgan-Kaufmann, 1993, pp. 271–278.\nR. S. Sutton, D. Precup, and S. Singh, “Between mdps and semi-mdps:\nA\nframework for temporal abstraction in reinforcement learning,” Artif. Intell.,\nvol. 112, no. 1–2, p. 181–211, Aug. 1999.\nJ. L. Elman, “Learning and development in neural networks: The importance of\nstarting small,” Cognition, vol. 48, no. 1, pp. 71–99, 1993.\nA. Lazaric, M. Restelli, and A. Bonarini, “Transfer of samples in batch\nreinforcement learning,” in Proceedings of the 25th International Conference\non Machine Learning, ser. ICML ’08.\nNew York, NY, USA: Association for\nComputing Machinery, 2008, p. 544–551.\nA. Lazaric and M. Restelli, “Transfer from multiple mdps,” in Advances\nin Neural Information Processing Systems,\nJ. Shawe-Taylor,\nR. Zemel,\nP. Bartlett, F. Pereira, and K. Q. Weinberger, Eds., vol. 24.\nCurran\nAssociates, Inc., 2011.\n66\nBibliography\nR. Sutton, D. Precup, and S. Singh, “Between mdps and semi-mdps: A framework\nfor temporal abstraction in reinforcement learning,” Artiﬁcial Intelligence, vol.\n112, pp. 181–211, 1999.\nV. Soni and S. P. Singh, “Using homomorphisms to transfer options across\ncontinuous reinforcement learning domains,” in Proceedings, The Twenty-First\nNational Conference on Artiﬁcial Intelligence and the Eighteenth Innovative\nApplications of Artiﬁcial Intelligence Conference, July 16-20, 2006, Boston,\nMassachusetts, USA.\nAAAI Press, 2006, pp. 494–499.\nA. S. Vezhnevets, V. Mnih, J. Agapiou, S. Osindero, A. Graves, O. Vinyals,\nand K. Kavukcuoglu, “Strategic attentive writer for learning macro-actions,”\nin Proceedings of the 30th International Conference on Neural Information\nProcessing Systems, ser. NIPS’16.\nRed Hook, NY, USA: Curran Associates\nInc., 2016, p. 3494–3502.\nA. Fachantidis, I. Partalas, G. Tsoumakas, and I. Vlahavas, “Transferring\ntask models in reinforcement learning agents,” Neurocomputing, vol. 107, pp.\n23–32, 2013, timely Neural Networks Applications in Engineering.\nF. Fern´andez,\nJ. Garc´ıa,\nand M. Veloso,\n“Probabilistic policy reuse for\ninter-task transfer learning,” Robotics and Autonomous Systems, vol. 58,\nno. 7, pp. 866–871, 2010, advances in Autonomous Robots for Service and\nEntertainment.\nM. E. Taylor, P. Stone, and Y. Liu, “Transfer learning via inter-task mappings for\ntemporal diﬀerence learning,” Journal of Machine Learning Research, vol. 8,\nno. 1, pp. 2125–2167, 2007.\nM. E. Taylor and P. Stone, “Behavior transfer for value-function-based reinforce-\nment learning,” in The Fourth International Joint Conference on Autonomous\nAgents and Multiagent Systems, F. Dignum, V. Dignum, S. Koenig, S. Kraus,\nM. P. Singh, and M. Wooldridge, Eds. New York, NY: ACM Press, July 2005,\npp. 53–59.\n——, “Transfer learning for reinforcement learning domains: A survey,” Journal\nof Machine Learning Research, vol. 10, no. 56, pp. 1633–1685, 2009.\nA. Lazaric, “Transfer in reinforcement learning: A framework and a survey,” in\nAdaptation, Learning, and Optimization.\nSpringer Berlin Heidelberg, 2012,\npp. 143–173.\nY. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,” in\nInternational Conference on Machine Learning, ICML, 2009.\n67\nBibliography\nT. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience replay,”\nin International Conference on Learning Representations, Puerto Rico, 2016.\nM. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,\nB. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba, “Hindsight\nexperience replay,” in Advances in Neural Information Processing Systems,\nI. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,\nand R. Garnett, Eds., vol. 30.\nCurran Associates, Inc., 2017.\nZ. Ren, D. Dong, H. Li, and C. Chen, “Self-paced prioritized curriculum learning\nwith coverage penalty in deep reinforcement learning,” IEEE Transactions on\nNeural Networks and Learning Systems, vol. 29, no. 6, pp. 2216–2226, 2018.\nT. Kim and J. Choi, “Screenernet: Learning curriculum for neural networks,”\nCoRR, vol. abs/1801.00904, 2018.\nC. Florensa, D. Held, M. Wulfmeier, M. Zhang, and P. Abbeel, “Reverse\ncurriculum generation for reinforcement learning,” in Proceedings of the 1st\nAnnual Conference on Robot Learning, ser. Proceedings of Machine Learning\nResearch, S. Levine, V. Vanhoucke, and K. Goldberg, Eds., vol. 78.\nPMLR,\n13–15 Nov 2017, pp. 482–495.\nF. Foglino, C. C. Christakou, and M. Leonetti, “An optimization framework\nfor task sequencing in curriculum learning,” in 2019 Joint IEEE 9th Inter-\nnational Conference on Development and Learning and Epigenetic Robotics\n(ICDL-EpiRob), 2019, pp. 207–214.\nF. Foglino, C. Coletto Christakou, R. Luna Gutierrez, and M. Leonetti,\n“Curriculum learning for cumulative return maximization,” in Proceedings\nof the Twenty-Eighth International Joint Conference on Artiﬁcial Intelli-\ngence, IJCAI-19.\nInternational Joint Conferences on Artiﬁcial Intelligence\nOrganization, 7 2019, pp. 2308–2314.\nS. Narvekar, J. Sinapov, and P. Stone, “Autonomous task sequencing for\ncustomized curriculum design in reinforcement learning,” in Proceedings of\nthe Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence,\nIJCAI-17, 2017, pp. 2536–2542.\nT. Matiisen, A. Oliver, T. Cohen, and J. Schulman, “Teacher-student curriculum\nlearning,” IEEE Transactions on Neural Networks and Learning Systems, pp.\n1–9, 2019.\nS. Narvekar and P. Stone, “Learning curriculum policies for reinforcement learn-\ning,” in Proceedings of the 18th International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS), May 2019.\n68\nBibliography\nS. Narvekar, J. Sinapov, M. Leonetti, and P. Stone, “Source task creation for\ncurriculum learning,” in Proceedings of the 15th International Conference on\nAutonomous Agents and Multiagent Systems (AAMAS 2016), Singapore, May\n2016.\nJ. Schmidhuber, “Powerplay: Training an increasingly general problem solver by\ncontinually searching for the simplest still unsolvable problem,” Frontiers in\nPsychology, vol. 4, p. 313, 2013.\nM. Jiang, E. Grefenstette, and T. Rockt¨aschel, “Prioritized level replay,” 2020.\nK. Kurach, A. Raichuk, P. Stanczyk, M. Zajac, O. Bachem, L. Espeholt,\nC. Riquelme, D. Vincent, M. Michalski, O. Bousquet, and S. Gelly, “Google\nresearch football: A novel reinforcement learning environment,” 2019.\nY. Flet-Berliac, R. Ouhamma, O.-A. Maillard, and P. Preux, “Is standard devi-\nation the new standard? revisiting the critic in deep policy gradients,” 2020.\nY. Schraner, ReinforcementLearning on the GoogleFootball environment, 2020.\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed.\nThe MIT Press, 2018.\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal\npolicy optimization algorithms,” CoRR, vol. abs/1707.06347, 2017.\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region\npolicy optimization,” ser. Proceedings of Machine Learning Research, F. Bach\nand D. Blei, Eds., vol. 37.\nLille, France:\nPMLR, 07–09 Jul 2015, pp.\n1889–1897.\nC. C. Hsu, C. Mendler-D¨unner, and M. Hardt, “Revisiting design choices in\nproximal policy optimization,” CoRR, vol. abs/2009.10897, 2020.\nS. Narvekar, B. Peng, M. Leonetti, J. Sinapov, M. E. Taylor, and P. Stone,\n“Curriculum learning for reinforcement learning domains: A framework and\nsurvey,” J. Mach. Learn. Res., vol. 21, pp. 181:1–181:50, 2020.\nM. Svetlik, M. Leonetti, J. Sinapov, R. Shah, N. Walker, and P. Stone,\n“Automatic curriculum graph generation for reinforcement learning agents,”\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 31, no. 1,\nFeb. 2017.\nR. Wang, J. Lehman, J. Clune, and K. O. Stanley, “Poet:\nOpen-ended\ncoevolution of environments and their optimized solutions,” in Proceedings\nof the Genetic and Evolutionary Computation Conference, ser. GECCO ’19.\nNew York, NY, USA: Association for Computing Machinery, 2019, p. 142–151.\n69\nBibliography\nK. P. F.R.S., “Liii. on lines and planes of closest ﬁt to systems of points in space,”\nThe London, Edinburgh, and Dublin Philosophical Magazine and Journal of\nScience, vol. 2, no. 11, pp. 559–572, 1901.\nR. Portelas, C. Colas, K. Hofmann, and P. Oudeyer, “Teacher algorithms for\ncurriculum learning of deep RL in continuously parameterized environments,”\nin 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan,\nOctober 30 - November 1, 2019, Proceedings, ser. Proceedings of Machine\nLearning Research, L. P. Kaelbling, D. Kragic, and K. Sugiura, Eds., vol. 100.\nPMLR, 2019, pp. 835–853.\nI. Kanitscheider, J. Huizinga, D. Farhi, W. H. Guss, B. Houghton, R. Sampedro,\nP. Zhokhov, B. Baker, A. Ecoﬀet, J. Tang, O. Klimov, and J. Clune,\n“Multi-task curriculum learning in a complex,\nvisual,\nhard-exploration\ndomain: Minecraft,” CoRR, vol. abs/2106.14876, 2021.\nA. F. Agarap, “Deep learning using rectiﬁed linear units (relu),” 2018, cite\narxiv:1803.08375Comment: 7 pages, 11 ﬁgures, 9 tables.\nS. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computa-\ntion, vol. 9, no. 8, pp. 1735–1780, 1997.\nM. Chevalier-Boisvert, L. Willems, and S. Pal, “Minimalistic gridworld environ-\nment for openai gym,” https://github.com/maximecb/gym-minigrid, 2018.\nY. Schraner, IP7 - Reinforcement Learning on the Google Football environment,\n2019.\nS. Racani`ere, A. K. Lampinen, A. Santoro, D. P. Reichert, V. Firoiu, and\nT. P. Lillicrap, “Automated curriculum generation through setter-solver\ninteractions,” in 8th International Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\nOpenReview.net,\n2020.\nM. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. van de Wiele,\nV. Mnih, N. Heess, and J. T. Springenberg, “Learning by playing solving\nsparse reward tasks from scratch,” in Proceedings of the 35th International\nConference on Machine Learning, ser. Proceedings of Machine Learning\nResearch, J. Dy and A. Krause, Eds., vol. 80.\nPMLR, 10–15 Jul 2018, pp.\n4344–4353.\nK. Shao, Y. Zhu, and D. Zhao, “Starcraft micromanagement with reinforcement\nlearning and curriculum transfer learning,” IEEE Transactions on Emerging\nTopics in Computational Intelligence, vol. 3, no. 1, pp. 73–84, 2019.\nList of Figures\n2.1\nThe agent-environment interaction in a Markov decision process\n[Sutton and Barto, 2018]. . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.2\nPerformance metrics for transfer learning using (a) weak transfer\nand (b) strong transfer (ﬁgure by Narvekar et al. [2020]). . . . . .\n17\n2.3\nWe visualize the interaction between the three key elements of\ncurriculum learning. Task generation is concerned with generate a\nset of tasks for the curriculum. Task sequencing selects a task out\nof the task set for the agent. The agent use knowledge obtained\nby training on previously selected task through transfer learning.\nAfter or during training on the new task, the obtained knowledge is\nstored. The task generation and sequencing can be done online and\ndependent on the student’s current performance or oﬄine before\ntraining.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.1\nTeacher-student interaction for task sequencing. . . . . . . . . . .\n26\n4.1\nVisualization of a subset of the MiniGrid environments used in\nthis work. The environment names from top left to bottom right:\nEmpty-6x6, FourRooms, DoorKey-16x16, MultiRoom-N4S5, KeyCorridor-\nS3R2, KeyCorridor-S3R3, and KeyCorridor-S6R3. . . . . . . . . .\n32\n4.2\nA blue line illustrates the agent’s trajectory in the DoorKey-8x8\nenvironment. The maximum number of steps in the DoorKey-8x8\nenvironment is 640. If the agent takes the direct path, he takes 18\nsteps until he reaches the green goal square. Therefore the agents\nreward is 1−0.9∗(18/640) = 0.9747. The agent on the right takes\n28 steps, leaving him with a reward of 1 −0.9 ∗(28/640) = 0.9606.\n34\n4.3\nStudent neural network architecture for the MiniGrid environ-\nments. The input is a 25×25×3 fully observable representation of\nthe environments state. After every fully connected layer we use\na ReLU activation function. The policy head uses a softmax acti-\nvation function for the action probability distribution. The value\nhead does not use an activation function. . . . . . . . . . . . . . .\n35\n4.4\nComparison of the average return over 100 episodes at the end\nof training between MLP and CNN models. The agent is trained\nwith PPO for 10 million steps on a single environment. . . . . . .\n36\n70\n71\nList of Figures\n4.5\nLearning curves for diﬀerent teacher observations and transfer meth-\nods. After each teacher step the sum of the students average return\nover 100 episodes for each environment in V is plotted. We plot the\nlearning curve with the highest total reward at the end of training\nfor every conﬁguration. . . . . . . . . . . . . . . . . . . . . . . . .\n39\n4.6\nWe visualize the added reward, when using reward transfer (left\ncolumn) and combined transfer (right column). For every state\nin the Empty-8x8 (top row) and Empty-16x16 (bottom row) grid\nworld environment we plot the added reward. A light color encodes\na high added reward, black equals to zero added reward. Dark tiles\nat the end of the grid are walls, the black tile in the bottom right\ncorner is the green goal square.\n. . . . . . . . . . . . . . . . . . .\n41\n4.7\nWe visualize the sample probability distribution and learning curve\nof one teacher agent over its training time.\nThe colors in the\nlegends are ordered upside down to the order in the plot. . . . . .\n42\n4.8\nVisualization of the target reward signal for each teacher observa-\ntion type over the teachers training cycle. . . . . . . . . . . . . . .\n45\n4.9\nThe average learning curve of three runs with three diﬀerent ran-\ndom seeds for each α value.\n. . . . . . . . . . . . . . . . . . . . .\n46\n4.10 We visualize the learning curves of our agent when trained in\nthe teacher-student setup and directly on the target task for the\nEmpty-16x16, DoorKey16x16, and FourRooms environment. We\nuse a teacher with policy transfer, source task reward signal, and\nthe six partially observable inputs. The learning curve of the agent\ntrained directly on the task is labeled with ”None”. . . . . . . . .\n48\n4.11 The Google Football Engine is a football simulation which supports\nthe major football rules like kickoﬀs (top left), goals (top right),\nfouls, cards (bottom left), corner and penalty kicks (bottom right),\nand oﬀside. [Kurach et al., 2019]\n. . . . . . . . . . . . . . . . . .\n51\n4.12 To approximate the Markov property we use a frame stack of the\nlast 4 frames as an input for our neural networks. . . . . . . . . .\n53\n4.13 CNN network architecture with the stacked Super Mini Map ob-\nservation as input. The one-hot encoded previous action at−1 is\nused as an additional input. The policy and value function are two\nheads sharing the network torso. . . . . . . . . . . . . . . . . . . .\n56\nA.1 Student CNN neural network architecture for the MiniGrid envi-\nronments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\nList of Tables\n1.1\nComparison of our diﬀerent curriculum learning approaches, the\naverage return over 100 episodes on the 11 vs 11 hard environment\nis reported.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.1\nDescription of the MiniGrid environments used in this thesis. All\nenvironments impose a penalty for the number of steps taken until\nreaching the target. . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.2\nHighest average return over 100 episodes for each teacher type at\nthe end of 1000 teacher steps. The None results are the average\nreturn over 100 episodes obtained when training only the environ-\nment of that row. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n4.3\nWe compare the diﬀerent knowledge transfer methods for each\nteacher observation type.\nEach conﬁguration is run with three\ndiﬀerent random seeds. The maximum return over 100 episodes at\nthe end of 1’000 teacher steps is reported. Additionally, we provide\nthe standard deviation between the three runs. . . . . . . . . . . .\n40\n4.4\nWe compare the diﬀerent knowledge transfer methods for each\nbaseline. Each conﬁguration is run with three diﬀerent random\nseeds. The maximum the average students return of 100 episodes\nat the end of 1’000 teacher steps is reported.\nAdditionally, we\nprovide the standard deviation between the three runs. . . . . . .\n43\n4.5\nWe compare the diﬀerent teacher reward signals for each teacher\nobservation type. Each conﬁguration is run with three diﬀerent\nrandom seeds. The maximum return over 100 episodes at the end\nof 1’000 teacher steps is reported. Additionally, we provide the\nstandard deviation between the three runs. . . . . . . . . . . . . .\n44\n4.6\nWe compare the diﬀerent α values for the EMA observation type.\nEach conﬁguration is run with three diﬀerent random seeds. The\nmaximum return of 100 episodes at the end of 1’000 teacher steps is\nreported. Additionally, we provide the standard deviation between\nthe three runs.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n4.7\nWe report the median measurements obtained by evaluating each\nagent with ﬁve random seed for 100 episodes on every environment\nin V. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n4.8\nDescription of the Google Football environments taken directly\nfrom Kurach et al. [2019]. All scenarios end after 400 frames, or\nif the ball is lost, a team scores or the game stops (e.g., if the ball\nleaves the pitch or a free kick is awarded).\n. . . . . . . . . . . . .\n54\n72\n73\nList of Tables\n4.9\nGoogle football results obtained on one random seed after training\nfor 1’000 CMDP steps, equal to 10 million student steps.\nThe\naverage return over 100 episodes is reported. We use policy transfer\nbetween the source tasks and the source task reward signal. . . . .\n58\n4.10 Comparison of our diﬀerent curriculum learning approaches of our\nprevious work Schraner [2020], the average return over 100 episodes\non the 11 vs 11 hard environment is reported. . . . . . . . . . . .\n58\n4.11 Results on the Football Academy scenarios obtained in our previous\nwork [Schraner, 2019] and the Google baseline [Kurach et al., 2019]\nwith the IMPALA algorithm. The reported results is the average\nreward of 100 episodes after a training on 50 million frames.\n. . .\n59\nA.1 Hyperparameters used for training a student in the CMDP setting\non the grid world environments. . . . . . . . . . . . . . . . . . . .\n75\nA.2 Important ﬁxed student hyperparameters used in our Google Foot-\nball CMDP experiments. . . . . . . . . . . . . . . . . . . . . . . .\n76\nA.3 Hyperparameters used for training a teacher in the CMDP setting\nused in both the grid world and Google Football experiments.\n. .\n77\nA.4 Comparison of the MLP and LSTM teacher architecture. We re-\nport the total average return over 100 episodes on all environments\nin V at the end of training. . . . . . . . . . . . . . . . . . . . . . .\n78\nAppendix A\nAppendix\nA.1\nGrid World CNN Architecture\nIn ﬁg. A.1 we depict the CNN network architecture for grid world environments.\nThe network input is a 25 × 25 × 3 fully observable representation of the envi-\nronment’s state. We use four convolution blocks with [16, 16, 32, 64] channels, a\n[2×2, 3×3, 3×3, 3×3] kernel and a stride of [1, 1, 1, 3]. A ReLU activation func-\ntion follows each convolution block. After the convolution blocks, we use a fully\nconnected layer with 128 units. The policy and value function are separate heads\non top of the fully connected layer. The policy head uses a softmax activation\nfunction.\nFigure A.1: Student CNN neural network architecture for the MiniGrid environ-\nments.\n74\n75\nA.2. Grid World Student Hyperparameters\nA.2\nGrid World Student Hyperparameters\nWe optimized a limited number of hyperparameters for our students in the grid\nworld environment. The hyperparameters were tuned by training an rl agent\nwith PPO on a single environment for 10 million steps. We evaluate all hyper-\nparameters on all environments in V. The parameters are reported in table A.1.\nParameters with only a single ”range” value in table A.1 were set to common\nvalues used for PPO.\nParameter\nRange\nBest\nLearning rate\n[0.01, 0.03, 0.001, 0.003, 0.0001, 0.0003]\n0.0003\nDiscount\n[0.97, 0.99, 0.997, 0.999]\n0.999\nEntropy loss coeﬃcient\n0.01\n0.01\nValue loss coeﬃcient\n0.5\n0.5\nGradient norm clip\n0.5\n0.5\nGAE λ\n0.95\n0.95\nClipping range\n0.2\n0.2\nNormalize advantage\nTrue\nTrue\nMinibatches\n1\n1\nEpochs\n[1,2,4]\n2\nOptimizer\nadam\nadam\nTraining steps\n10′000\n10′000\nUnroll length\n[256, 512]\n256\nNumber of actors\n40\n40\nTable A.1: Hyperparameters used for training a student in the CMDP setting on\nthe grid world environments.\n76\nA.3. Google Football Student Hyperparameters\nA.3\nGoogle Football Student Hyperparameters\nIn table A.2 we report the hyperparameters used in our Google Football CMDP\nexperiments. These values were tuned in our previous work [Schraner, 2020].\nTherefore we do not tune any of those parameters in this thesis.\nParameter\nValue\nLearning rate\n0.0011879\nDiscount\n0.997\nEntropy loss coeﬃcient\n0.00155\nGradient norm clip\n0.76\nValue loss coeﬃcient\n0.5\nGAE λ\n0.95\nClipping range\n0.115\nNormalize advantage\nTrue\nMinibatches\n4\nEpochs\n2\nOptimizer\nadam\nReward\nScoring\nObservation\nSMM\nTraining steps\n10′000\nUnroll length\n512\nNumber of actors\n40\nTable A.2: Important ﬁxed student hyperparameters used in our Google Football\nCMDP experiments.\n77\nA.4. Teacher Hyperparameters\nA.4\nTeacher Hyperparameters\nWe optimized a limited number of hyperparameters for our teacher on the grid\nworld environment. The hyperparameters were tuned by training a teacher agent\nfor 1000 steps with PPO, using policy transfer, total average return and the RH\nobservation. The parameters are reported in table A.3 and used for both the grid\nworld and Google Football experiments. Parameters with only a single ”range”\nvalue in table A.3 were set to common values used for PPO.\nParameter\nRange\nBest\nLearning rate\n[0.1, 0.3, 0.01, 0.03]\n0.03\nLinear learning rate schedule\nTrue\nTrue\nDiscount\n1\n1\nEntropy loss coeﬃcient\n0.01\n0.01\nValue loss coeﬃcient\n0.5\n0.5\nGradient norm clip\n0.5\n0.5\nGAE λ\n0.95\n0.95\nClipping range\n0.2\n0.2\nNormalize advantage\nTrue\nTrue\nMinibatches\n1\n1\nEpochs\n4\n4\nOptimizer\nadam\nadam\nTraining steps\n1000\n1000\nUnroll length\n4\n4\nTable A.3: Hyperparameters used for training a teacher in the CMDP setting\nused in both the grid world and Google Football experiments.\n78\nA.5. MLP vs. LSTM Teacher Model\nA.5\nMLP vs. LSTM Teacher Model\nWe evaluated teachers with an MLP network architecture and an LSTM archi-\ntecture as described in section 3.2.4.\nExecuting experiments with the LSTM\narchitecture takes around 1.5 times as long as using the MLP architecture. In\ntable A.4 we compare both network architectures. We report the total average\nreturn. Each teacher is trained for 1’000 CMDP steps, where for each step, the\nstudent is trained for 10’000 frames in the selected environment. The total aver-\nage return reward signal and policy transfer are used in all experiments. We do\nnot report results for LP and EMA for the LSTM model, as those observation\ntypes were developed after this evaluation has been carried out. The MLP archi-\ntecture is superior to the LSTM architecture for all experiments except the RH\nobservation type. Therefore, we use the MLP architecture for the experiments\nreported in this thesis.\nModel\nRH\nPTR\nLP\nALP\nEMA\nFS-EMA\nMLP\n2.34\n4.44\n4.17\n3.35\n4.35\n3.72\nLSTM\n3.12\n2.97\n-\n2.95\n-\n2.76\nTable A.4: Comparison of the MLP and LSTM teacher architecture. We report\nthe total average return over 100 episodes on all environments in V at the end of\ntraining.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-10-31",
  "updated": "2022-10-31"
}