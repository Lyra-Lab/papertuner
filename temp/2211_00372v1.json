{
  "id": "http://arxiv.org/abs/2211.00372v1",
  "title": "Meta-Learning for Unsupervised Outlier Detection with Optimal Transport",
  "authors": [
    "Prabhant Singh",
    "Joaquin Vanschoren"
  ],
  "abstract": "Automated machine learning has been widely researched and adopted in the\nfield of supervised classification and regression, but progress in unsupervised\nsettings has been limited. We propose a novel approach to automate outlier\ndetection based on meta-learning from previous datasets with outliers. Our\npremise is that the selection of the optimal outlier detection technique\ndepends on the inherent properties of the data distribution. We leverage\noptimal transport in particular, to find the dataset with the most similar\nunderlying distribution, and then apply the outlier detection techniques that\nproved to work best for that data distribution. We evaluate the robustness of\nour approach and find that it outperforms the state of the art methods in\nunsupervised outlier detection. This approach can also be easily generalized to\nautomate other unsupervised settings.",
  "text": "Under Review\nMETA-LEARNING FOR UNSUPERVISED OUTLIER DE-\nTECTION WITH OPTIMAL TRANSPORT\nPrabhant Singh & Joaquin Vanschoren\nEindhoven University of Technology\nEindhoven 5600 MB, Netherlands\n{p.singh,j.vanschoren}@tue.nl\nABSTRACT\nAutomated machine learning has been widely researched and adopted in the ﬁeld\nof supervised classiﬁcation and regression, but progress in unsupervised settings\nhas been limited. We propose a novel approach to automate outlier detection\nbased on meta-learning from previous datasets with outliers. Our premise is that\nthe selection of the optimal outlier detection technique depends on the inherent\nproperties of the data distribution. We leverage optimal transport in particular, to\nﬁnd the dataset with the most similar underlying distribution, and then apply the\noutlier detection techniques that proved to work best for that data distribution. We\nevaluate the robustness of our approach and ﬁnd that it outperforms the state of\nthe art methods in unsupervised outlier detection. This approach can also be easily\ngeneralized to automate other unsupervised settings.\n1\nINTRODUCTION\nOutlier detection(OD) is the process of identifying data points that are signiﬁcantly different from\nthe rest of the data. These data points can be caused by errors in the data collection process, incorrect\nvalues, or unusual events. Outlier detection can be used to improve the quality of the data or to ﬁnd\nunusual events that could be interesting to different business and scientiﬁc domains . The term\n”outlier detection” can be interchangeably used with ”anomaly detection”. For consistency, we will\nuse the term ”outlier detection” in this paper. Outlier detection has multiple applications such as\nmedicine (Chow & keung Tse, 1990; Ma et al., 2021b), chemistry (Egan & Morgan, 1998) and\nmolecular biology (Cho & Eo, 2016). Outlier detection has been a particularly hard problem. A\nnumber of Outlier detection algorithms have been introduced in the last two decades (Aggarwal,\n2013). Unsupervised outlier detection is a very challenging task with no universally good model\nwhich works optimally on every task (Campos et al., 2015).\nAutoML (Hutter et al., 2019) has shown reliable performance and beneﬁts in model selection and\nhyperparameter optimization (Hutter et al., 2019; Feurer et al., 2015; Thornton et al., 2013). The\nresearch in Automated machine learning has been highly focused on supervised machine learning\nwhere we can focus on the performance on the hold-out dataset to deﬁne an optimization metric\nfor the search algorithm which ﬁnds the optimal algorithms by iterating over the search space. This\nsetting is very reliable (Feurer et al., 2015) but the research on unsupervised setting is rather limited.\nIn recent years frameworks like MetaOD (Zhao et al., 2021) have appeared which attempt to solve\nautomated outlier detection via meta-learning (Vanschoren, 2018).\nIn this work we propose an automated framework for unsupervised machine learning tasks LO-\nTUS(Learning to learn with Optimal Transport for Unsupervised Scenarios), which leverages meta-\nlearning (Vanschoren, 2018) and optimal transport distances (Peyr´e & Cuturi, 2019; Scetbon &\nCuturi, 2022). In this work we use LOTUS to perform model selection on a given unsupervised\noutlier detection task. We also develop GAMAOD, an extension to the popular AutoML framework\nGAMA (Gijsbers & Vanschoren, 2021) for supervised outlier detection problems. In summary, we\nmake the following 4 contributions:\n• A Meta-learner for outlier detection: We propose LOTUS: Learning to learn with Opti-\nmal Transport for Unsupervised Scenarios, an optimal transport based meta-learner which\n1\narXiv:2211.00372v1  [cs.LG]  1 Nov 2022\nUnder Review\nrecommends an optimal outlier detection algorithm based on a historical collection of\ndatasets and models in a zero-shot learning scenario. Our solution can be used in cold\nstart settings for model selection on unsupervised outlier detection.\n•\nA supervised automated framework for unsupervised outlier detection: We develop\nGAMAOD which is an extension of an AutoML tool GAMA. GAMAOD can use different\nsearch algorithms to ﬁnd optimal unsupervised outlier detection algorithms for a given\ndataset when labels are available. We develop GAMAOD to populate the meta-data for\nmodel selection.\n• Experiments and results: We empirically evaluate LOTUS in combination with existing\nstate of the art methods. We demonstrate the robustness of our approach against existing\nstate of the art meta-learners and learners.\n• Open source: We open-source the code for LOTUS and GAMAOD for researchers to\nuse and reproduce our experiments. Our tools can be extended with new datasets and\nalgorithms.\n2\nBACKGROUND\nThis section describes related work regarding Automated Machine learning for unsupervised outlier\ndetection, optimal transport and meta-learning.\n2.1\nAUTOML FOR OUTLIER DETECTION\nAutoML (Hutter et al., 2019) for unsupervised outlier detection is an extremely hard problem due\nlack of an optimization metric to perform algorithm selection. One can argue that the use of internal\nmetrics like Excess-Mass (Goix, 2016), Mass-Volume (Goix, 2016) and IREOS (Marques et al.,\n2015) can make algorithm selection possible. Ma et al. (2021a) shows in their experiments that these\ninternal metrics are computationally very expensive and do not scale well for large datasets. This\nmakes it unfeasible to use these metrics in AutoML tools for most real world scenarios.\nThere has been recent research on AutoML for outlier detection. PyODDS and MetaOD (Li et al.,\n2020; Zhao et al., 2021) are among the few tools which have been shown to automate outlier detec-\ntion.\nTo the best of our knowledge MetaOD (Zhao et al., 2021) is the current state of the art meta-\nlearner for model selection on outlier detection tasks for tabular data. MetaOD uses meta-learning\nas a recommendation engine using landmark meta-features and model based meta-features with\ncollaborative ﬁltering (Stern et al., 2010) to perform model selection for a given task.\n2.2\nMETA LEARNING\nMeta-learning or ”learning to learn” in AutoML (Vanschoren, 2019) is the study of learning from\nhistorical performances of machine learning models on a variety of tasks and using this knowledge\nto ﬁnd better models for new tasks. Meta-learning can help to speed up the model selection process\nand ﬁnd better architectures. Meta-learning is often proposed as a solution to cold start problem,\nby initializing the hyperparameters or search space for the AutoML algorithm. This is often called\nwarm-starting for AutoML.\nMeta-learning in existing AutoML tools: Different AutoML tools use different meta-learning\nschemes to solve this cold start problem. AutoSklearn-2.0 (Feurer et al., 2020) learns pipeline\nportfolios, MetaOD (Zhao et al., 2021) trains a collaborative ﬁltering based algorithm (Stern et al.,\n2010) with landmark-based and model-based metafeatures (Castiello et al., 2005), FLAML uses\nin-built meta-learned defaults for warm starting. MetaBu (Rakotoarison et al., 2022) uses Fused\nGromov Wasserstein with proximal gradient method on landmark meta-features for warm-starting\nAutoSklearn(More discussion about LOTUS vs MetaBu is provided in the section 5.3.2).\n2\nUnder Review\n2.3\nOPTIMAL TRANSPORT AND DATASET DISTANCES\nOptimal transport(OT) theory deals with the problem of ﬁnding an optimal transport map between\ntwo probability measures, often on different metric spaces. It is closely related to Monge’s problem\n(Villani, 2008), in which one searches for the optimal transport map between two given measures.\nAn Optimal transport problem consists of minimizing the cost of transporting mass from one dis-\ntribution to another. For cost function(ground metric) between pair of points, we calculate the cost\nmatrix C with dimensionality n×m, the OT problem minimizes the loss function Lc(P) := ⟨C, P⟩\nw.r.t a coupling matrix P. Most common approach with practitioners is to use a regularized ap-\nproach which is computationally more efﬁcient Lϵ\nc(P) := ⟨C, P⟩+ ϵr(P) where r is negative\nentropy sinkhorn algorithm (Cuturi, 2013) which is computationally more efﬁcient. A discrete OT\nproblem can be deﬁned with two ﬁnite pointclouds, {x(i)}n\ni=1 ,{y(j)}m\nj=1, x(i), y(j) ∈Rd, which\ncan be described as two empirical distributions: µ := Pn\ni=1 aiδx(i), ν := Pm\nj=1 bjδy(j). Here a\nand b are the probability vectors of size n and m. In this work we are interested in the the Gromov\nWasserstein(GW) distance between these two discrete probability distributions. Gromomv Wasser-\nstein allows us to match points taken within different metric spaces. This problem can be written as\na function of (a, A), (b, B) (Villani, 2008; Scetbon et al., 2022):\nGW((a, A), (b, B)) =\nmin\nP ∈Πa,b QA,B(P)\n(1)\nWhere Πa,b := {P ∈Rn×m\n+\n|P1m = a, P T 1n = b}\nthe energy QA,B is a quadratic function of P which can be described as\nQA,B(P) :=\nX\ni,j,i′,j′\n(Ai,i′ −Bj,j′)2Pi,jPi′,j′\n(2)\nIn this work we are interested in the Entropic Gromov Wasserstein cost (Peyr´e et al., 2016):\nGWε((a, A), (b, B)) =\nmin\nP ∈Πa,b QA,B(P) −ε H(P)\n(3)\nwhere GWϵ is the Entropic Gromov Wasserstein cost between our distributions A and B, and εH(P)\nis the shannon entropy. The problem with Gromov Wasserstein is that it is NP-hard and the entropic\napproximation of GW still has cubic complexity. To speed up the computations and use it in a realis-\ntic AutoML settings we use the Low-Rank Gromov Wasserstein (GW-LR) approximation (Scetbon\net al., 2021; Scetbon & Cuturi, 2022; Scetbon et al., 2022), which reduces the computational cost\nfrom cubic to linear time. Scetbon et al. (2022) consider the GW problem with low-rank couplings,\nlinked by a common marginal g. Therefore, the set of possible transport plans is restricted to those\nadopting the factorization of the form Pr = Qdiag(1/g)RT . In this form Q and R are thin matrices\nwith dimensionality of n × r, r × m respectively and g is a r−dimensional probability vector. The\nGW-LR distance is be described as:\nGW-LR(r)((a, A), (b, B)) :=\nmin\n(Q,R,g)∈Ca,b,r QA,B(Qdiag(1/g)RT )\n(4)\nOur primary inspiration for LOTUS comes from two different works.\n1. Alvarez-Melis & Fusi (2020) proposes optimal transport dataset distance(OTDD) which\nuses optimal transport to learn a mapping over the joint feature and label spaces. Alvarez-\nMelis & Fusi (2020) proposed that optimal transport distances can be used as a similarity\nmetric between different datasets from different domains and subdomains.\n2. Work of Nies et al. (2021) argues that optimal transport measures can be used as a correla-\ntion measure between two random variables via transport dependency.\nThere have been other studies exploring the space of dataset and task similarity with distance mea-\nsures. Gao & Chaudhari (2021) proposes “coupled transfer distance” which utilises optimal trans-\nport distances as a transfer learning distance metric. Achille et al. (2021) explores connections\nbetween Deep Learning, Complexity Theory, and Information Theory through their proposed asym-\nmetric distance on tasks.\n3\nUnder Review\n3\nMETHODOLOGY (META LEARNING FOR UNSUPERVISED OUTLIER\nDETECTION)\n3.1\nPROBLEM STATEMENT\nIn this section, we formally describe the problem of model selection for unsupervised outlier detec-\ntion.\nProblem Statement: Given a new dataset without any labels, our meta-learner needs to selects\nan optimal algorithm with associated hyperparameters from a collection of previously evaluated\npipeline. In this setting, we cannot optimize the given model for the dataset as there are no given\nlabels. This problem becomes from a Combined model selection and hyperparameter optimization\nproblem to a zero-shot model reccomendation problem.\nGiven a new input dataset (i.e., detection task) Dnew = (Xnew) without any labels, Select a model\nA∗\nλ∗∈A to employ on Xnew. Where A∗\nλ∗is a tuned model for a similar dataset to Xnew.\n3.2\nLOTUS+GAMAOD: A COMBINED FRAMEWORK FOR AUTOMATED UNSUPERVISED\nOUTLIER DETECTION\nIn this work, we present a meta-learner for outlier detection (LOTUS) and a tool to collect meta-data\nfor automated supervised outlier detection (GAMAOD). We call the training phase of GAMAOD\nmeta-training, where different datasets are supplied to GAMAOD with labels. This meta-data con-\ntains two major attributes:\n• A collection of n meta datasets Dmeta = {D1, ..., Dn} with labels, test and train splits\nsuch that Di = (Xtrain\ni\n, ytrain), (Xtest, ytest)\n• A collection of n optimized algorithm(s) with associated hyperparamters for every dataset\nin Dmeta; A = {A∗\nλ∗\n1, ..., A∗\nλ∗n}\nAn overview of our system can be found in Figure 1. We will discuss the building of our meta-data\nbefore describing our meta-learning approach.\nFigure 1: An overview of LOTUS\n3.2.1\nGAMAOD: AUTOMATED SUPERVISED LEARNING FOR OUTLIER DETECTION\nProblem Formulation: A Combined model selection and hyperparameter optimization problem\n(Thornton et al., 2013) for a supervised learning task is as follows:\nA∗\nλ∗= argmin\n∀Aj∈A\n∀λ∈ΛA\n1\nk\nk\nX\nf=1\nL\n\u0010\nAj\nλ,\n\b\nXf\ntrain, yf\ntrain\n\t\n,\n\b\nXf\nval, yf\nval\n\t\u0011\n(5)\n4\nUnder Review\nFigure 2: An overview of GAMAOD\nin equation 5, A∗\nλ∗is an optimal combination of learning algorithm from search space A with asso-\nciated hyperparameter space ΛA over k cross validation folds of dataset D where D = {X, y} with\ntraining and validation splits. L is our evaluation measure.\nThe CASH problem from equation 5 relies on the validation split to optimise for the optimal conﬁg-\nuration. However, in unsupervised outlier detection scenario the learning algorithm does not have\naccess to labels but the AutoML framework does. We cannot do cross validation folds in the un-\nsupervised outlier model selection setting as the learning algorithms are trained without labels and\nevaluated with labels. Hence performing k-fold CV is not useful in this setting. Our modiﬁed CASH\nformulation to select the optimal unsupervised algorithm with access to labels is as follows:\nA∗\nλ∗= argmin\n∀Aj∈A\n∀λ∈ΛA\nL\n\u0010\nAj\nλ,\n\b\nXtrain}\n\b\nytrain\n\t\u0011\n(6)\nGAMAOD: To populate our meta-data we develop an extension on top of GAMA (Gijsbers &\nVanschoren, 2021), we call this extension GAMAOD. GAMAOD’s search space consists of outlier\ndetectors from PyOD (Zhao et al., 2019), PyOD is a Python library for detecting outlying objects\nin multivariate data. GAMAOD can use different metrics to optimise for the given task like AUC\nscore and PR.\n3.2.2\nLOTUS: LEARNING TO LEARN WITH OPTIMAL TRANSPORT FOR UNSUPERVISED\nSCENARIOS\nWe describe LOTUS approach in algorithm 1. Our meta-learning approach has two components.\nFirst, we have a transformation function that is applied to the dataset. Let’s call this function φ,\nwhich is applied on given datasets Da and Db. In the second phase, we calculate the dataset similar-\nity O based on some distance metric ψ in equation 7. Because our distributions are lie on different\nmetric spaces we calculate the Low Rank Gromov Wasserstein distance from equation 4 on these\ntransformed distributions in equation 8.\nO = ψ(φ(Da)φ(Db))\n(7)\nO = GW-LR(r)(φ(Da)φ(Db))\n(8)\nTo ﬁnd the most similar dataset Γ, we take the smallest distance from datasets from our distance\nmatrix {O1, ..., On} between a new dataset Dnew and existing datasets from Dmeta. which results\nin:\nΓ = argmin{O1, ..., On}\n(9)\n5\nUnder Review\nOur meta-approach for capturing the similarity metric between datasets can be described as:\nΓΓ∈Dmeta = argmin(GW-LR(r)(φ(Dnew)ι{(φ(DD∈Dmeta))}))\n(10)\nwhere ι is the iterator over Dmeta.\nLOTUS then accesses the optimal pipeline from A: A∗\nλ∗new = A∗\nλ∗Γ Where A∗\nλ∗new is the optimal\npipeline for Dnew.\nAlgorithm 1 Pseudocode for LOTUS\nInputs: Dnew, Dmeta, A\nwhile Di ∈Dmeta do\nOi ←ψ(φ(Dnew, Di))\n▷Distance calculation\nΓ ←Dmeta[argmin{O1, ..., On}]\n▷Retrieval of most similar dataset\nA∗\nλ∗new ←A∗\nλ∗Γ\n▷Model Selection\n4\nEXPERIMENTS ON ADBENCH\nFor our experiments, we use ADBench (Han et al., 2022) and retrieve all tabular datasets. This\ncollection consists of 46 datasets. As we do not have access to multiple benchmarks we use the\nN-1 strategy for the evaluation of our system, i.e., we take one dataset at a time from ADBench and\nuse the other datasets in the meta-data. This ensures independent meta-training on the following\ndatasets. We compare our approach with 7 outlier detection algorithms available in PyOD (Zhao\net al., 2019) and the current state of the art meta-learner for outlier detection MetaOD (Zhao et al.,\n2021). From PyOD we compare our approach with the following algorithms: IForest (Liu et al.,\n2008), ABOD (Kriegel et al., 2008), OCSVM (Sch¨olkopf et al., 1999), LODA (Pevn´y, 2015),\nKNN (Angiulli & Pizzuti, 2002; Ramaswamy et al., 2000), HBOS (Goldstein & Dengel, 2012).\nFor experimental consistency, we use the same search space for GAMAOD pretraining as MetaOD\n(A.3) to ensure a fair comparison. To populate our meta-data, we run GAMAOD for 2 hours on\nevery ADBench dataset to ﬁnd the optimal pipeline for a given dataset. We use an asynchronous\nevolutionary algorithm to iterate over the search space and return the optimal pipeline. GAMAOD\nuses AUC score as internal metric for model selection.\nImplementation details: We use Independent Component Analysis(ICA) (Hyv¨arinen & Oja, 2000)\nfrom scikit-learn (Pedregosa et al., 2011) as our transformation function φ. We use OTT-JAX\nlibrary (Cuturi et al., 2022) library to implement Low Rank Gromov Wassersstein distance. For\nthis experiment, we set the rank parameter of Low Rank Gromov Wasserstein to 6. The model\nselection phase of LOTUS in our experiments is as follows: First the datasets are transformed via\nICA and then converted into JAX (Bradbury et al., 2018) pointclouds geometry objects 1 and then\nwe turn these distributions into a quadratic regularized optimal transport problem (Peyr´e et al.,\n2016). We input this quadratic problem to our Gromov Wasserstein Low Rank solver which returns\nus the distance(cost) between two datasets. When a new dataset is given to LOTUS, the pipeline\ncorresponding to the dataset with the lowest distance(except the new dataset itself) is chosen from\nthe optimal pipeline database.\n5\nRESULTS AND DISCUSSION\n5.1\nEXPERIMENTAL RESULTS\nWe use the Bayesian Wilcoxon signed-rank test (or ROPE test, Benavoli et al. (2017; 2014)) to\nanalyze the results of our experiments. ROPE deﬁnes an interval wherein the differences in model\nperformance are considered equivalent to the null value. Using this test allows us to compare model\nperformances in a more practical sense. We set the ROPE value to 1% for our experiments. We use\nthe baycomp library (Benavoli et al., 2017) to run and visualize the analyses.\n1https://ott-jax.readthedocs.io/en/latest/_autosummary/ott.geometry.\npointcloud.PointCloud.html\n6\nUnder Review\nEstimator name\np(LOTUS)\np(rope)\np(Estimator)\nIForest\n0.99954\n0.0\n0.00046\nABOD\n1.0\n0.0\n0.0\nOCSVM\n1.0\n0.0\n0.0\nLODA\n1.0\n0.0\n0.0\nKNN\n1.0\n0.0\n0.0\nHBOS\n0.99982\n0.0\n0.00018\nCOF\n1.0\n0.0\n0.0\nTable 1: Rope testing results with LOTUS vs PyOD estimators with rope=1%\nLOTUS vs MetaOD: The ROPE test on AUC scores comparing LOTUS and MetaOD are shown\nin Figure 3. There is a 74.2 % probability that LOTUS is better than MetaOD. LOTUS proves\nto be more robust than MetaOD, since p(LOTUS) > p(MetaOD). We show the per-dataset\nperformances in Appendix A.1.\nFigure 3: ROPE test LOTUS vs MetaOD.\nResults against other estimators: ROPE testing on the results of ROCAUC performance of LO-\nTUS vs other estimators are shown in Table 1. LOTUS proves to be signiﬁcantly better than other\ntechniques, with default parameters, in PyOD. In this case P(LOTUS) >> P(PyODestimators),\nFigure 5 shows the simplex plots of MetaOD vs other estimators. We also include a Critical Differ-\nence Plot of performances between LOTUS and PyOD estimators (lower is better) in Figure 4. The\ndetailed experimental results are reported in appendix A.1 table 3.\n5.2\nUSING OPTIMAL TRANSPORT DISTANCES AS A SIMILARITY MEASURE\nIn our experiments, we show that LOTUS is more robust and better than current state of the art\nmeta-learner MetaOD for unsupervised outlier detection tasks and other outlier detection algorithms\nin default conﬁguration.\nFigure 4: Comparison of average rank (lower is better) of methods w.r.t. performance across datasets\nin ADBench.\n7\nUnder Review\n(a) LOTUS vs ABOD\n(b) LOTUS vs HBOS\n(c) LOTUS vs COF\n(d) LOTUS vs IForest\n(e) LOTUS vs LODA\n(f) LOTUS vs KNN\n(g) LOTUS vs OCSVM\nFigure 5: ROPE test result of LOTUS vs (a) ABOD (b) HBOS (c) COF (d) IForest (e) LODA (f)\nKNN (g) OCSVM\nIn our method we experimentally show that using optimal transport distances like GW-LR is a\nfeasible approach for dataset similarity and meta-learning. We would like to emphasize that this\nsimilarity measure should only be used as a relative similarity measure, for e.g. in our case where\nwe use this similarity measure to ﬁnd the most similar dataset from a collection of datasets in Dmeta.\nTo estimate to what degree datasets are similar Nies et al. (2021) proposes optimal transport based\ncorrelation measures that can be leveraged. Our approach assumes that Wasserstein distances can\ncapture intrinsic properties of datasets and can capture the similarity between them, Alvarez-Melis\n& Fusi (2020) also proposes their approach with optimal transport distances to provide some sort of\ndistance between dataset.\n5.3\nRELATED WORKS\nIn this section we will discuss the difference between closest approaches to LOTUS which are\nMetaOD and MetaBu.\n5.3.1\nLOTUS VS METAOD\nLOTUS and MetaOD solve the same problem of model selection problem for unsupervised outlier\ndetection. The major difference in LOTUS and MetaOD is meta-feature generation. LOTUS aims to\ncapture the similarity of the given source and target representations via optimal transport. MetaOD\ncaptures similarity with a combination of landmark-features and model-based features and uses\na rank-based criteria called discounted cumulative gain for model selection. MetaOD also uses\nstochastic algorithms such as Isolation Forest and LODA for model-based meta-feature generation\nwhich means that the absolute dataset similarity and ranking can differ based on the number of runs.\nLOTUS on the other hand uses non stochastic methods for distance calculation. Our approach is\ngeneralises more than MetaOD as well for different unsupervised tasks as it aims to ﬁnd similar\ndataset independent of task, whereas MetaOD’s similarity is highly coupled with the task of outlier\ndetection.\n8\nUnder Review\n5.3.2\nLOTUS VS METABU\nMetaBu (Rakotoarison et al., 2022) was proposed as a solution to cold start problem in supervised\nlearning scenario. Rakotoarison et al. (2022) uses Fused-Gromov-Wasserstein distance with multi\ndimensional scaling (Cox & Cox, 2008) by ﬁrst extracting meta-features from the target representa-\ntion and source representation and proximal gradient method (Xu et al., 2020). LOTUS on the other\nhand uses independent component analysis and Low Rank Gromov Wasserstein. LOTUS is much\nfaster than MetaBu simply because of computational complexity differences between Low Rank\nGromov Wasserstein(O(n)) and Fused Gromov Wasserstein(O(n3)). LOTUS is a simpler approach\nas well as compared to MetaBu(2 phases vs 5 phases). Lastly, LOTUS is a solution for unsupervised\nsetting whereas MetaBu relies on landmark features from PyMFE (Alcobac¸a et al., 2020) which are\nmore reliable for datasets with labels. Similar to MetaOD, MetaBu setting is limited to only one task\n(supervised classiﬁcation) as it relies on landmark-features which require labels. LOTUS therefore\ncan generalize to more tasks than MetaBu.\n5.4\nLIMITATIONS\n1. LOTUS: LOTUS depends on the quality of meta-data, i.e. range of datasets and algorithms\nin our case. In the worst case scenario, if there are no similar datasets in the Dmeta, LOTUS\ncan recommend a dataset which is not sufﬁciently similar to new dataset. On the other hand,\nit is expected to improve as more benchmarks and datasets with different properties become\navailable.\n2. OT distance: The computation cost of GW-LR on really large datasets can still be very\nhigh. In these cases we recommend using stratiﬁed sampling or random sampling depend-\ning on the nature of dataset and problem.\n3. Parameter tuning: Tuning rank of GW-LR can be tricky. Low rank can result in faster\ncomputation but high loss and high rank can result in less efﬁcient algorithm. Scetbon et al.\n(2022) states an experiment where they study the affect of rank of GW-LR. This rank can\nalso be tuned by minimizing the loss between GW and GW-LR.\n6\nCONCLUSION AND FUTURE WORK\nModel selection for unsupervised outlier detection is a challenging task. We do not have efﬁcient\ninternal metrics for evaluating an algorithm without ground truth. In this work, we proposed a new\nmeta-learner: LOTUS, which uses optimal transport distances to capture the similarity between\ndatasets and uses that similarity measure to recommend pipelines from a meta-data. To run these ex-\nperiments, we also developed another tool GAMAOD which is an extension of GAMA and allows\nusers to ﬁnd optimal outlier detection algorithms in a supervised setting. Through our experiments,\nwe demonstrate that LOTUS outperforms MetaOD and other built-in estimators in PyOD. The LO-\nTUS approach also enables researchers to use a simpliﬁed meta-learning framework as compared to\nother landmark and model-based meta-features methods where meta-features are highly specialized\naccording to the domain.\nWe believe that our approach can be easily extended as a meta-learner to perform model selection\nin other unsupervised machine learning tasks as well. These include clustering, distance metric\nlearning, density estimation and covariance estimation. This approach can also be used as a meta-\nlearner to warm-start neural architecture search(NAS) problems.\n7\nREPRODUCIBILTY STATEMENT\nWe opensource both LOTUS and GAMAOD with hyperparameters used for this experiment. We\nalso provide scripts which can be used to perform these experiment for just one dataset without\nmaking the meta-data ﬁrst(not reccomended). We aim to provide modularity to researchers therefore\nwe users them to save and retrieve meta-data in whatever format they want. More information\nabout reproducing our experiments can be found in the README.md of the supplementary code\nrepository. To reproduce LOTUS for other tasks and dataset, users are simply required to change the\ndatasets and algorithms in meta-data. The approach works out of the box for other scenarios. While\nreproducing the experiments, the results can differ due to stochasticity of few algorithms.\n9\nUnder Review\nREFERENCES\nAlessandro Achille, Giovanni Paolini, Glen Mbeng, and Stefano Soatto. The information complexity\nof learning tasks, their structure and their distance. Information and Inference: A Journal of the\nIMA, 10(1):51–72, 01 2021. ISSN 2049-8772. doi: 10.1093/imaiai/iaaa033. URL https:\n//doi.org/10.1093/imaiai/iaaa033.\nCharu C. Aggarwal. Outlier analysis. In Springer New York, 2013.\nEdesio Alcobac¸a, Felipe Siqueira, Adriano Rivolli, Lu´ıs P. F. Garcia, Jefferson T. Oliva, and Andr´e\nC. P. L. F. de Carvalho. Mfe: Towards reproducible meta-feature extraction. Journal of Machine\nLearning Research, 21(111):1–5, 2020. URL http://jmlr.org/papers/v21/19-348.\nhtml.\nDavid Alvarez-Melis and Nicol´o Fusi. Geometric dataset distances via optimal transport. ArXiv,\nabs/2002.02923, 2020.\nFabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In PKDD,\n2002.\nAlessio Benavoli, Giorgio Corani, Francesca Mangili, Marco Zaffalon, and Fabrizio Ruggeri. A\nbayesian wilcoxon signed-rank test based on the dirichlet process. In Eric P. Xing and Tony\nJebara (eds.), Proceedings of the 31st International Conference on Machine Learning, volume 32\nof Proceedings of Machine Learning Research, pp. 1026–1034, Bejing, China, 22–24 Jun 2014.\nPMLR. URL https://proceedings.mlr.press/v32/benavoli14.html.\nAlessio Benavoli, Giorgio Corani, Janez Demˇsar, and Marco Zaffalon. Time for a change: a tuto-\nrial for comparing multiple classiﬁers through bayesian analysis. Journal of Machine Learning\nResearch, 18(77):1–36, 2017. URL http://jmlr.org/papers/v18/16-305.html.\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:\n//github.com/google/jax.\nGuilherme Oliveira Campos, Arthur Zimek, J¨org Sander, Ricardo J. G. B. Campello, Barbora Mi-\ncenkov´a, Erich Schubert, Ira Assent, and Michael E. Houle. On the evaluation of unsupervised\noutlier detection: measures, datasets, and an empirical study. Data Mining and Knowledge Dis-\ncovery, 30:891–927, 2015.\nCiro Castiello, Giovanna Castellano, and Anna Maria Fanelli. Meta-data: Characterization of input\nfeatures for meta-learning. In Vicenc¸ Torra, Yasuo Narukawa, and Sadaaki Miyamoto (eds.),\nModeling Decisions for Artiﬁcial Intelligence, pp. 457–468, Berlin, Heidelberg, 2005. Springer\nBerlin Heidelberg. ISBN 978-3-540-31883-5.\nHyungJun Cho and Soo-Heang Eo. Outlier detection for mass spectrometric data. Methods in\nmolecular biology, 1362:91–102, 2016.\nShein-Chung Chow and Siu keung Tse. Outlier detection in bioavailability/bioequivalence studies.\nStatistics in medicine, 9 5:549–58, 1990.\nMichael A. A. Cox and Trevor F. Cox. Multidimensional Scaling, pp. 315–347. Springer Berlin Hei-\ndelberg, Berlin, Heidelberg, 2008. ISBN 978-3-540-33037-0. doi: 10.1007/978-3-540-33037-0\n14. URL https://doi.org/10.1007/978-3-540-33037-0_14.\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NIPS, 2013.\nMarco Cuturi, Laetitia Meng-Papaxanthos, Yingtao Tian, Charlotte Bunne, Geoff Davis, and\nOlivier Teboul. Optimal transport tools (ott): A jax toolbox for all things wasserstein. ArXiv,\nabs/2201.12324, 2022.\nWilliam J. Egan and Stephen L. Morgan. Outlier detection in multivariate analytical chemical data.\nAnalytical chemistry, 70 11:2372–9, 1998.\n10\nUnder Review\nMatthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and\nFrank Hutter. Efﬁcient and robust automated machine learning. In C. Cortes, N. Lawrence,\nD. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Sys-\ntems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.\ncc/paper/2015/file/11d0e6287202fced83f79975ec59a3a6-Paper.pdf.\nMatthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and Frank Hutter. Auto-\nsklearn 2.0: Hands-free automl via meta-learning. arXiv:2007.04074 [cs.LG], 2020.\nYansong Gao and Pratik Chaudhari. An information-geometric distance on the space of tasks. In Ma-\nrina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine\nLearning, volume 139 of Proceedings of Machine Learning Research, pp. 3553–3563. PMLR,\n18–24 Jul 2021. URL https://proceedings.mlr.press/v139/gao21a.html.\nPieter Gijsbers and Joaquin Vanschoren. Gama: A general automated machine learning assistant. In\nYuxiao Dong, Georgiana Ifrim, Dunja Mladeni´c, Craig Saunders, and Soﬁe Van Hoecke (eds.),\nMachine Learning and Knowledge Discovery in Databases. Applied Data Science and Demo\nTrack, pp. 560–564, Cham, 2021. Springer International Publishing. ISBN 978-3-030-67670-4.\nNicolas Goix. How to evaluate the quality of unsupervised anomaly detection algorithms?, 2016.\nURL https://arxiv.org/abs/1607.01152.\nMarkus Goldstein and Andreas R. Dengel. Histogram-based outlier score (hbos): A fast unsuper-\nvised anomaly detection algorithm. 2012.\nSongqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao. ADBench: Anomaly\ndetection benchmark.\nIn Thirty-sixth Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track, 2022.\nURL https://openreview.net/forum?id=\nfoA_SFQ9zo0.\nFrank Hutter, Lars Kotthoff, and Joaquin Vanschoren.\nAutomated machine learning: Methods,\nsystems, challenges. Automated Machine Learning, 2019.\nAapo Hyv¨arinen and Erkki Oja. Independent component analysis: algorithms and applications.\nNeural networks : the ofﬁcial journal of the International Neural Network Society, 13 4-5:411–\n30, 2000.\nHans-Peter Kriegel, Matthias Schubert, and Arthur Zimek. Angle-based outlier detection in high-\ndimensional data.\nIn Proceedings of the 14th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pp. 444–452, 2008.\nISBN 9781605581934.\ndoi:\n10.1145/1401890.1401946.\nYuening Li, Daochen Zha, Na Zou, and Xia Hu. Pyodds: An end-to-end outlier detection system\nwith automated machine learning. Companion Proceedings of the Web Conference 2020, 2020.\nFei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. 2008 Eighth IEEE International\nConference on Data Mining, pp. 413–422, 2008.\nMartin Q. Ma, Yue Zhao, Xiaorong Zhang, and Leman Akoglu. A large-scale study on unsupervised\noutlier model selection: Do internal strategies sufﬁce?\nCoRR, abs/2104.01422, 2021a. URL\nhttps://arxiv.org/abs/2104.01422.\nZhiwei Ma, Daniel S. Reich, Sarah Dembling, Jeff H. Duyn, and Alan P. Koretsky. Outlier detection\nin multimodal mri identiﬁes rare individual phenotypes among 20,000 brains. bioRxiv, 2021b.\nHenrique O. Marques, Ricardo J. G. B. Campello, Arthur Zimek, and Jorg Sander. On the internal\nevaluation of unsupervised outlier detection. In Proceedings of the 27th International Conference\non Scientiﬁc and Statistical Database Management, SSDBM ’15, New York, NY, USA, 2015.\nAssociation for Computing Machinery. ISBN 9781450337090. doi: 10.1145/2791347.2791352.\nURL https://doi.org/10.1145/2791347.2791352.\nThomas Giacomo Nies, Thomas Staudt, and Axel Munk. Transport dependency: Optimal transport\nbased dependency measures. 2021.\n11\nUnder Review\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-\nhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\nE. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,\n12:2825–2830, 2011.\nTom´as Pevn´y. Loda: Lightweight on-line detector of anomalies. Machine Learning, 102:275–304,\n2015.\nGabriel Peyr´e and Marco Cuturi. Computational optimal transport. Found. Trends Mach. Learn.,\n11:355–607, 2019.\nGabriel Peyr´e, Marco Cuturi, and Justin Solomon. Gromov-wasserstein averaging of kernel and\ndistance matrices.\nIn Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of\nThe 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine\nLearning Research, pp. 2664–2672, New York, New York, USA, 20–22 Jun 2016. PMLR. URL\nhttps://proceedings.mlr.press/v48/peyre16.html.\nHerilalaina Rakotoarison, Louisot Milijaona, Andry RASOANAIVO, Michele Sebag, and Marc\nSchoenauer. Learning meta-features for autoML. In International Conference on Learning Rep-\nresentations, 2022. URL https://openreview.net/forum?id=DTkEfj0Ygb8.\nSridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efﬁcient algorithms for mining outliers\nfrom large data sets. SIGMOD Rec., 29(2):427–438, may 2000. ISSN 0163-5808. doi: 10.1145/\n335191.335437. URL https://doi.org/10.1145/335191.335437.\nMeyer Scetbon and Marco Cuturi. Low-rank optimal transport: Approximation, statistics and debi-\nasing. NeurIPS 2022, abs/2205.12365, 2022.\nMeyer Scetbon, Marco Cuturi, and Gabriel Peyr´e. Low-rank sinkhorn factorization. In Marina Meila\nand Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,\nvolume 139 of Proceedings of Machine Learning Research, pp. 9344–9354. PMLR, 18–24 Jul\n2021. URL https://proceedings.mlr.press/v139/scetbon21a.html.\nMeyer Scetbon, Gabriel Peyr´e, and Marco Cuturi. Linear-time gromov Wasserstein distances us-\ning low rank couplings and costs. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba\nSzepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Con-\nference on Machine Learning, volume 162 of Proceedings of Machine Learning Research,\npp. 19347–19365. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/\nv162/scetbon22b.html.\nBernhard Sch¨olkopf, Robert C. Williamson, Alex Smola, John Shawe-Taylor, and John C. Platt.\nSupport vector method for novelty detection. In NIPS, 1999.\nDavid Stern,\nRalf Herbrich,\nThore Graepel,\nHorst Samulowitz,\nLuca Pulina,\nand Ar-\nmando Tacchella.\nCollaborative expert portfolio management.\nIn Proceedings of\nthe Twenty-Fourth AAAI Conference on Artiﬁcial Intelligence AAAI-10 (to appear), July\n2010.\nURL https://www.microsoft.com/en-us/research/publication/\ncollaborative-expert-portfolio-management/.\nChris Thornton, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Auto-WEKA: Combined\nselection and hyperparameter optimization of classiﬁcation algorithms. In 19th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining, pp. 847–855, 2013. ISBN\n9781450321747. doi: 10.1145/2487575.2487629.\nJoaquin Vanschoren. Meta-learning: A survey. ArXiv, abs/1810.03548, 2018.\nJoaquin Vanschoren. Meta-learning. Hutter et al. (2019), pp. 39–68.\nC´edric Villani. Optimal transport: Old and new. 2008.\nHongteng Xu, Dixin Luo, Ricardo Henao, Svati Shah, and Lawrence Carin. Learning autoencoders\nwith relational regularization. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th\nInternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learning\nResearch, pp. 10576–10586. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.\npress/v119/xu20e.html.\n12\nUnder Review\nYue Zhao, Zain Nasrullah, and Zheng Li. Pyod: A python toolbox for scalable outlier detection. J.\nMach. Learn. Res., 20:96:1–96:7, 2019.\nYue Zhao, Ryan Rossi, and Leman Akoglu. Automatic unsupervised outlier model selection. In\nM. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Ad-\nvances in Neural Information Processing Systems, volume 34, pp. 4489–4502. Curran Asso-\nciates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/\n23c894276a2c5a16470e6a31f4618d73-Paper.pdf.\n13\nUnder Review\nDataset\nLOTUS\nMetaOD\n19 landsat\n0.7902\n0.5931\n25 musk\n0.9895\n0.9655\n24 mnist\n1.0000\n1.0000\n32 shuttle\n0.9216\n0.9163\n23 mammography\n0.6434\n0.6477\n42 WBC\n0.8521\n0.8655\n15 Hepatitis\n0.9353\n0.9353\n43 WDBC\n0.8548\n0.9671\n12 fault\n0.9246\n0.9043\n10 cover\n0.9463\n0.9436\n34 smtp\n0.2744\n0.5212\n11 donors\n0.8064\n0.8049\n29 Pima\n0.8804\n0.7197\n37 Stamps\n0.9275\n0.9339\n44 Wilt\n0.7765\n0.5327\n40 vowels\n0.8491\n0.9355\n8 celeba\n0.9908\n0.9906\n1 ALOI\n0.8954\n0.8957\n30 satellite\n0.8913\n0.7890\n26 optdigits\n0.9996\n0.9997\n2 annthyroid\n0.8472\n0.8445\n41 Waveform\n0.9758\n0.9413\n28 pendigits\n0.8597\n0.9265\n4 breastw\n0.7466\n0.7438\n21 Lymphography\n0.9441\n0.9861\n20 letter\n0.9701\n0.9891\n39 vertebral\n0.7634\n0.8424\n47 yeast\n0.9089\n0.9097\n3 backdoor\n1.0000\n1.0000\n13 fraud\n0.9646\n0.8904\n45 wine\n0.9841\n0.9481\n22 magic.gamma\n0.9322\n0.8122\n9 census\n0.9819\n1.0000\n7 Cardiotocography\n0.9392\n0.9378\n35 SpamBase\n0.9446\n0.9015\n46 WPBC\n0.7811\n0.8088\n36 speech\n1.0000\n0.4344\n6 cardio\n0.9794\n0.9793\n31 satimage-2\n0.9552\n0.8100\n18 Ionosphere\n0.8072\n0.8338\n27 PageBlocks\n0.7164\n0.7668\n5 campaign\n0.9922\n0.9996\nTable 2: AUC scores of MetaOD vs LOTUS on ADBench\nA\nAPPENDIX\nA.1\nPERFORMANCES\nTable 2 contains the performances of LOTUS and MetaOD on 42 datasets, We had to eliminate\n4 datasets from this experiment because MetaOD returned invalid models for these datasets(i.e.\nmodels with invalid values). Scores are in bold where AUC of LOTUS > MetaOD or differ by less\nthan a %. The dataset names are as they were in ADBench (Han et al., 2022).\nTable 3 reports the auc scores over datasets from ADBench. The bold number shows scores where\nLOTUS is better than all other estimators in PyOD.\n14\nUnder Review\nDataset\nIForest\nABOD\nOCSVM\nLODA\nKNN\nHBOS\nCOF\nLOTUS\n44 Wilt\n0.471963\n0.568222\n0.301310\n0.408280\n0.472095\n0.281412\n0.544269\n0.7765\n6 cardio\n0.943738\n0.498576\n0.939676\n0.892753\n0.741544\n0.865343\n0.544550\n0.9794\n43 WDBC\n0.987241\n0.987241\n0.989655\n0.987586\n0.960345\n0.998966\n0.771034\n0.8548\n4 breastw\n0.976321\n0.976321\n0.778694\n0.981964\n0.947386\n0.969329\n0.381366\n0.7466\n42 WBC\n0.993567\n0.993567\n0.994103\n0.995980\n0.911954\n0.991691\n0.754757\n0.8521\n47 yeast\n0.431011\n0.417114\n0.448353\n0.492504\n0.413668\n0.410032\n0.428639\n0.9089\n45 wine\n0.735205\n0.735205\n0.681612\n0.923158\n0.471241\n0.891757\n0.412289\n0.9841\n5 campaign\n0.692549\n0.642977\n0.645556\n0.566477\n0.696817\n0.771387\n0.564588\n0.9922\n46 WPBC\n0.522489\n0.522489\n0.475911\n0.562133\n0.419170\n0.555259\n0.495170\n0.7811\n7 Cardiotocography\n0.752439\n0.539423\n0.810433\n0.785916\n0.582569\n0.623355\n0.572511\n0.9392\n8 celeba\n0.757810\n0.757810\n0.761861\n0.718291\n0.632204\n0.805965\n0.393545\n0.9908\n9 census\n0.598140\n0.598140\n0.523211\n0.325589\n0.650628\n0.633393\n0.413254\n0.9819\n39 vertebral\n0.377788\n0.377788\n0.427308\n0.284423\n0.417163\n0.282356\n0.321923\n0.7634\n41 Waveform\n0.669757\n0.698172\n0.474443\n0.611266\n0.782120\n0.639714\n0.804121\n0.9758\n38 thyroid\n0.979620\n0.979620\n0.867786\n0.699534\n0.951152\n0.952834\n0.871991\n0.7910\n40 vowels\n0.708373\n0.956714\n0.532701\n0.655924\n0.971722\n0.646130\n0.849763\n0.8491\n3 backdoor\n0.734361\n0.734361\n0.802264\n0.708914\n0.738679\n0.665487\n0.728995\n1.0000\n32 shuttle\n0.996250\n0.618768\n0.987461\n0.951075\n0.678578\n0.994925\n0.557606\n0.9216\n31 satimage-2\n0.996844\n0.762625\n0.983527\n0.987126\n0.909884\n0.985936\n0.451384\n0.9552\n26 optdigits\n0.771433\n0.525541\n0.527237\n0.623480\n0.398194\n0.852822\n0.423611\n0.9996\n1 ALOI\n0.501898\n0.609567\n0.532848\n0.549594\n0.555634\n0.478001\n0.635583\n0.8954\n35 SpamBase\n0.657074\n0.390792\n0.520510\n0.273952\n0.515358\n0.651507\n0.416468\n0.9446\n36 speech\n0.469975\n0.729473\n0.462061\n0.448529\n0.473192\n0.476358\n0.553156\n1.0000\n34 smtp\n0.696899\n0.670223\n0.018006\n0.372124\n0.744582\n0.878626\n0.890630\n0.2744\n22 magic.gamma\n0.704407\n0.799144\n0.594241\n0.635940\n0.823228\n0.681717\n0.663549\n0.9322\n23 mammography\n0.859409\n0.859409\n0.854704\n0.814810\n0.859614\n0.871755\n0.792004\n0.6434\n24 mnist\n0.794443\n0.750330\n0.834765\n0.743575\n0.828259\n0.619057\n0.733384\n1.0000\n20 letter\n0.581556\n0.880889\n0.485185\n0.627407\n0.867111\n0.540593\n0.829704\n0.9701\n30 satellite\n0.707795\n0.538013\n0.605468\n0.609243\n0.646056\n0.768130\n0.556999\n0.8913\n19 landsat\n0.495534\n0.500057\n0.374050\n0.382382\n0.577134\n0.556768\n0.542057\n0.7902\n37 Stamps\n0.909527\n0.909527\n0.878255\n0.944582\n0.746473\n0.928582\n0.636364\n0.9275\n18 Ionosphere\n0.867847\n0.867847\n0.765359\n0.858325\n0.862297\n0.667416\n0.850478\n0.8072\n21 Lymphography\n0.997003\n0.997003\n0.993506\n0.667582\n0.512862\n0.995005\n0.934316\n0.9441\n25 musk\n0.999923\n0.085936\n0.818675\n0.959047\n0.701124\n1.000000\n0.400387\n0.9895\n17 InternetAds\n0.700473\n0.673305\n0.710028\n0.580881\n0.712320\n0.704318\n0.693902\n1.0000\n16 http\n1.000000\n1.000000\n0.995308\n0.000000\n0.001340\n0.994638\n0.583110\n0.7106\n15 Hepatitis\n0.742736\n0.742736\n0.722262\n0.772817\n0.467871\n0.813292\n0.425388\n0.9353\n14 glass\n0.818496\n0.818496\n0.459264\n0.632274\n0.740799\n0.791758\n0.882668\n0.8374\n13 fraud\n0.934023\n0.941569\n0.914391\n0.751185\n0.916394\n0.941169\n0.914591\n0.9646\n11 donors\n0.794215\n0.794215\n0.723436\n0.260784\n0.829936\n0.763981\n0.720262\n0.8064\n12 fault\n0.571477\n0.676490\n0.494426\n0.436072\n0.713079\n0.479224\n0.612146\n0.9246\n2 annthyroid\n0.824922\n0.824922\n0.606069\n0.305845\n0.730291\n0.691522\n0.704828\n0.8472\n27 PageBlocks\n0.889696\n0.684494\n0.892650\n0.753280\n0.769997\n0.788657\n0.673234\n0.7164\n28 pendigits\n0.949714\n0.673023\n0.938642\n0.951140\n0.705836\n0.921169\n0.475639\n0.8597\n29 Pima\n0.660016\n0.660016\n0.580166\n0.606169\n0.685681\n0.713573\n0.566752\n0.8804\n10 cover\n0.914310\n0.767605\n0.886407\n0.866889\n0.899776\n0.795243\n0.870260\n0.9463\nTable 3: AUC Scores: LOTUS vs PyOD estimators with default conﬁguration\n15\nUnder Review\nA.2\nBASLINES\nThe 8 baslines estimators and frameworks are listed below with brief description from PyOD’s\n(Zhao et al., 2019) documentation for reference here:\n1. MetaOD: MetaOD is the ﬁrst automated tool for outlier detection. MetaOD use collabora-\ntive ﬁltering, landmark and model based meta-features to recommend the model for given\ntask.\n2. IForest: IsolationForest ‘isolates’ observations by randomly selecting a feature and then\nrandomly selecting a split value between the maximum and minimum values of the selected\nfeature.\n3. LOF:The anomaly score of each sample is called Local Outlier Factor. It measures the\nlocal deviation of density of a given sample with respect to its neighbors. It is local in that\nthe anomaly score depends on how isolated the object is with respect to the surrounding\nneighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is\nused to estimate the local density. By comparing the local density of a sample to the local\ndensities of its neighbors, one can identify samples that have a substantially lower density\nthan their neighbors. These are considered outliers.\n4. ABOD:For an observation, the variance of its weighted cosine scores to all neighbors could\nbe viewed as the outlying score.\n5. HBOS: Histogram- based outlier detection assumes the feature independence and calcu-\nlates the degree of outlier by building histograms.\n6. KNN: kNN class for outlier detection. For an observation, its distance to its kth nearest\nneighbor could be viewed as the outlying score.\n7. COF: Connectivity-Based Outlier Factor uses the ratio of average chaining distance of data\npoint and the average of average chaining distance of k nearest neighbor of the data point,\nas the outlier score for observations.\n8. LDOA: Lightweight on-line detector of anomalies detects anomalies in a dataset by com-\nputing the likelihood of data points using an ensemble of one-dimensional histograms.\n9. OCSVM: One class support vector machines unsupervised outlier Detection. Estimate the\nsupport of a high-dimensional distribution.\nA.3\nLOTUS+GAMAOD SEARCH SPACE AND METAOD REPRODUCIBILITY\nWe implement the same searchspace as MetaOD github repository for a fair comparison. 2, MetaOD\nalso uses all the existing datasets from ADbench. We believe that we have fairly evaluated MetaOD\nagainst out baseline. We believe that our Benchmark setting was more challenging than the one\nevaluated in Zhao et al. (2021) where it take child and parent datasets. 3\n2https://github.com/yzhao062/MetaOD/blob/master/metaod/models/base_\ndetectors.py\n3https://github.com/yzhao062/MetaOD/blob/2a8ed2761468d2f8ee2cd8194ce36b0f817576d1/\nmetaod/models/train_metaod.py\n16\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-11-01",
  "updated": "2022-11-01"
}