{
  "id": "http://arxiv.org/abs/2211.03263v2",
  "title": "AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages",
  "authors": [
    "Bonaventure F. P. Dossou",
    "Atnafu Lambebo Tonja",
    "Oreen Yousuf",
    "Salomey Osei",
    "Abigail Oppong",
    "Iyanuoluwa Shode",
    "Oluwabusayo Olufunke Awoyomi",
    "Chris Chinenye Emezue"
  ],
  "abstract": "In recent years, multilingual pre-trained language models have gained\nprominence due to their remarkable performance on numerous downstream Natural\nLanguage Processing tasks (NLP). However, pre-training these large multilingual\nlanguage models requires a lot of training data, which is not available for\nAfrican Languages. Active learning is a semi-supervised learning algorithm, in\nwhich a model consistently and dynamically learns to identify the most\nbeneficial samples to train itself on, in order to achieve better optimization\nand performance on downstream tasks. Furthermore, active learning effectively\nand practically addresses real-world data scarcity. Despite all its benefits,\nactive learning, in the context of NLP and especially multilingual language\nmodels pretraining, has received little consideration. In this paper, we\npresent AfroLM, a multilingual language model pretrained from scratch on 23\nAfrican languages (the largest effort to date) using our novel self-active\nlearning framework. Pretrained on a dataset significantly (14x) smaller than\nexisting baselines, AfroLM outperforms many multilingual pretrained language\nmodels (AfriBERTa, XLMR-base, mBERT) on various NLP downstream tasks (NER, text\nclassification, and sentiment analysis). Additional out-of-domain sentiment\nanalysis experiments show that \\textbf{AfroLM} is able to generalize well\nacross various domains. We release the code source, and our datasets used in\nour framework at https://github.com/bonaventuredossou/MLM_AL.",
  "text": "AfroLM: A Self-Active Learning-based Multilingual Pretrained Language\nModel for 23 African Languages\nBonaventure F. P. Dossou1,2,∗, Atnafu Lambebo Tonja3,∗, Oreen Yousuf4,∗, Salomey Osei5,∗,\nAbigail Oppong6,∗, Iyanuoluwa Shode7,∗, Oluwabusayo Olufunke Awoyomi8,∗, Chris Chinenye Emezue1,9,∗\n∗Masakhane NLP, 1Mila Quebec AI Institute, Canada, 2 McGill University, Canada, 3Instituto Politécnico Nacional, Mexico,\n4Uppsala University, Sweden, 5Universidad de Deusto, Spain 6Ashesi University, Ghana, 7Montclair State University, USA,\n8The College of Saint Rose, USA, 9Technical University of Munich, Germany\nAbstract\nIn recent years, multilingual pre-trained lan-\nguage models have gained prominence due\nto their remarkable performance on numer-\nous downstream Natural Language Process-\ning tasks (NLP). However, pre-training these\nlarge multilingual language models requires\na lot of training data, which is not avail-\nable for African Languages.\nActive learn-\ning is a semi-supervised learning algorithm,\nin which a model consistently and dynami-\ncally learns to identify the most beneﬁcial sam-\nples to train itself on, in order to achieve bet-\nter optimization and performance on down-\nstream tasks. Furthermore, active learning ef-\nfectively and practically addresses real-world\ndata scarcity. Despite all its beneﬁts, active\nlearning, in the context of NLP and especially\nmultilingual language models pretraining, has\nreceived little consideration.\nIn this paper,\nwe present AfroLM, a multilingual language\nmodel pretrained from scratch on 23 African\nlanguages (the largest effort to date) using\nour novel self-active learning framework. Pre-\ntrained on a dataset signiﬁcantly (14x) smaller\nthan existing baselines, AfroLM outperforms\nmany multilingual pretrained language mod-\nels (AfriBERTa, XLMR-base, mBERT) on\nvarious NLP downstream tasks (NER, text\nclassiﬁcation, and sentiment analysis).\nAd-\nditional out-of-domain sentiment analysis ex-\nperiments show that AfroLM is able to gen-\neralize well across various domains. We re-\nlease the code source, and our datasets used in\nour framework at https://github.com/\nbonaventuredossou/MLM_AL.\n1\nIntroduction\nWith the appearance of Transformer models\n(Vaswani et al., 2017), the ﬁeld of Natural Lan-\nguage Processing (NLP) has seen the emergence of\npowerful multilingual pre-trained language models\n(MPLMs), such as mBERT (Devlin et al., 2018),\nXLM-RoBERTa (XML-R) (Conneau et al., 2019),\nand mT5 (Xue et al., 2021). These prominent mod-\nels have helped achieve state-of-the-art (SOTA)\nperformance in many downstream NLP tasks such\nas named entity recognition (NER) (Alabi et al.,\n2022a; Adelani et al., 2021a; Devlin et al., 2018;\nConneau et al., 2019), text classiﬁcation (Kelechi\net al., 2021), and sentiment analysis (Alabi et al.,\n2022a; Adelani et al., 2021a; Devlin et al., 2018;\nConneau et al., 2019). However they usually re-\nquire a large amount of unlabeled text corpora\nfor good performance: mBERT was trained on\nWikipedia (2,500M words) and BookCorpus (Zhu\net al., 2015) (800M words) across 104 languages\n- 5 of which are African; mT5 supports 101 lan-\nguages (13 African) and XLM-R supports 100\nlanguages (8 African), and were trained on mC4\n(Xue et al., 2021) and CommonCrawl data (Wen-\nzek et al., 2019), respectively. This requirement\nfor large-scale datasets contrasts sharply with the\nscarcity of available text corpora for African lan-\nguages, which has pushed them into low-resource\nsettings and largely excluded them from the pre-\ntraining phase of these large pre-trained models\n(Joshi et al., 2020; Adelani et al., 2022a). This ex-\nclusion, leads very often to a poor performance on\nlanguages unseen during pre-training (Alabi et al.,\n2022a) which eventually leads to inability to carry\nout the required NLP task.\nActive learning is a semi-supervised machine\nlearning algorithm that makes use of only a few\ninitial training data points to achieve better per-\nformance of a given model M. The optimization\nis done by iteratively training M, and using an-\nother model N, usually referred to as the oracle,\nto choose new training samples that will help M\nﬁnd better conﬁgurations while improving its per-\nformance (e.g., prediction accuracy). This makes\nactive learning a prevalent paradigm to cope with\ndata scarcity. The efﬁciency of active learning (i.e.\nits ability to produce better performance despite\nbeing trained on a smaller training data) has been\narXiv:2211.03263v2  [cs.CL]  23 Nov 2022\nLanguages\nFamily\nWriting System\nAfrican Region\nNo of Speakers\nInitial # of Sentences\nSource\nSize (MB)\nAmharic (amh)\nAfro-Asiatic/Semitic\nGe’ez script\nEast\n57M\n655,079\n ,?,#\n279\nAfan Oromo (orm)\nAfro-Asiatic/Cushitic\nLatin script\nEast\n37.4M\n50,105\n?\n9.87\nBambara (bam)\nNC/Manding\nLatin, Arabic(Ajami), N’ko\nWest\n14M\n6,618\n \n1.00\nGhomálá’ (bbj)\nNC/Grassﬁelds\nLatin script\nCentral\n1M\n4,841\n \n0.50\nÉwé (ewe)\nNC/Kwa\nLatin (Ewe alphabet)\nWest\n7M\n5,615\n \n0.50\nFon (fon)\nNC/Volta-Niger\nLatin script\nWest\n1.7M\n5,448\n \n1.00\nHausa (hau)\nAfro-Asiatic/Chadic\nLatin (Boko alphabet)\nWest\n63M\n1,626,330\n ,?,#\n208\nIgbo (ibo)\nNC/Volta-Niger\nLatin (Önwu alphabet)\nWest\n27M\n437,737\n ,?,#\n63\nKinyarwanda (kin)\nNC/Rwanda-Rundi\nLatin script\nCentral\n9.8M\n84,994\nº,?, \n37.70\nLingala (lin)\nNC/Bang\nLatin script\nCentral & East\n45M\n398,440\n \n45.90\nLuganda (lug)\nNC/Bantu\nLatin script (Ganda alphabet)\nEast\n7M\n74,754\n?, \n8.34\nLuo (luo)\nNilo-Saharan\nLatin script\nEast\n4M\n8,684\n?\n1.29\nMooré (mos)\nNC/Gur\nLatin script\nWest\n8M\n27,908\n ,?\n5.05\nChewa (nya)\nNC/Nyasa\nLatin script\nSouth & East\n12M\n8,000\n \n1.66\nNaija (pcm)\nEnglish-Creole\nLatin script\nWest\n75M\n345,694\n ,?,#\n101\nShona (sna)\nNC/Bantu\nLatin script (Shona alphabet)\nSoutheast\n12M\n187,810\n ,?\n32.80\nSwahili (swa)\nNC / Bantu\nLatin script (Roman Swahili alphabet)\nEast & Central\n98M\n1,935,485\n ,?,#\n276\nSetswana (tsn)\nNC / Bantu\nLatin (Tswana alphabet)\nSouth\n14M\n13,958\n ,?\n2.21\nAkan/Twi (twi)\nNC / Kwa\nLatin script\nWest\n9M\n14,701\n \n1.61\nWolof (wol)\nNC / Senegambia\nLatin (Wolof alphabet)\nWest\n5M\n13,868\n?\n2.20\nXhosa (xho)\nNC/Zunda\nLatin (Xhosa alphabet)\nSouth\n20M\n93,288\n ,?\n17.40\nYor˚ubá (yor)\nNC / Volta-Niger\nLatin (Yorùbá alphabet)\nWest\n42M\n290,999\n ,?,#\n45.9\nisiZulu (zul)\nNC / Bantu\nLatin (Zulu alphabet)\nSouth\n27M\n194,562\n ,?\n33.70\nTable 1: Languages Corpora Details. Legends: (Adelani et al., 2022a) → , (Alabi et al., 2022a) →?, (Kelechi\net al., 2021) →#, (Niyongabo et al., 2020) →º.\nproven in tasks such as biological sequence de-\nsign (Jain et al., 2022), chemical sampling (Smith\net al., 2018), and Deep Bayesian (DB) approaches\non image data (Gal et al., 2017). Also, most of\nthe work on deep active learning focuses on image\nclassiﬁcation with Convolutional Neural Networks\n(CNNs). It should be noted that active learning has\nbeen greatly explored and used to perform classi-\nﬁcation tasks, but not in language generation and\nunderstanding, and this is what we hope to address.\nA study of active learning in the context of\nNLP has been carried out by (Siddhant and Lip-\nton, 2018). In their study, it is shown that active\nlearning with DB networks coupled with uncer-\ntainty measures and acquisition function outper-\nforms several i.i.d baselines. They showed that\nwith only 20% of samples labeled, their approach\nreached an accuracy of 98-99% on the Named En-\ntity Recognition (NER) task, while i.i.d tasks re-\nquired 50% of labelled data to achieve compara-\nble performance. In their study on clinical texts,\n(Chen et al., 2015) also proved that active learning\nalgorithms outperformed other learning methods.\n(Ein-Dor et al., 2020; Tonneau et al., 2022) on their\nworks with BERT model(s) (for n different lan-\nguages, there were n different BERT-based models)\nwent further by showing that active learning works\nwith a balanced and unbalanced dataset. They also\nshowed that the different active learning methods\nperformed relatively the same.\nIn our work, we ﬁxed M=N (hence the title self-\nactive learning). In our framework, we give M\nthe ability to query itself, and use the knowledge\nacquired during each active learning round to con-\nstruct new data points (from existing ones) that will\nbe used for the next active learning round.\nWe considered a diverse set of 23 African lan-\nguages spread across the African continent. The\nselected languages are spoken in the south, cen-\ntral, east, and western regions of Africa. The lan-\nguages cover four language families: Afro-Asiatic\n(e.g., Amharic, Hausa, Afan Omoro), Niger-Congo\n(NC) (e.g., Yorùbá, Bambara, Fon), English-Creole\n(Naija) and Nilo-Saharan (Luo) (see Appendix A\nfor details). For each language, a dataset was col-\nlected from the news domain, which encompassed\nmany topics such as health, politics, society, sport,\nenvironment, etc.\nOur primary contribution to this work is our\nproposal of a self-active learning framework\nin which we pre-train the biggest Multilingual\nAfrican Language Model (for the number of\nlanguages covered) to date, and we show that\nour setup is very data-efﬁcient and provides im-\nprovements on downstream NLP tasks such as\nNER, text classiﬁcation, and sentiment analysis\n(even on out-of-domain experiments).\n2\nRelated Works on MPLMs for African\nLanguages\nLanguage adaptive ﬁne-tuning (LAFT) is one of\nthe best approaches to adapt MPLMs to a new\nlanguage. This entails ﬁne-tuning an MPLM on\nmonolingual texts of the said language with the\nsame pre-training objective. However, this can-\nnot be efﬁciently applied to African languages fac-\ning data-scarcity. (Alabi et al., 2022b) proposed a\nnew adaptation method called Multilingual adap-\nFigure 1: Self-Active Learning Framework). The process is designed in 4 stages (fully explained and detailed in\nAlgorithm 1): (1) ■Dataset split for current Active Learning round, (2) ■Active Learning round training, (3) ■\nGeneration of new sentence samples for the current round, and (4) ■Augmentation of the datasets of all languages.\ntive ﬁne-tuning (MAFT), as an approach to adapt\nMPLMs to many African languages with a single\nmodel. Their results show that MAFT is competi-\ntive to LAFT while providing a single model rather\nthan many models that are speciﬁc for individual\nlanguages. Nevertheless, Alabi et al. (2022b)’s ap-\nproach still works under the assumption that one\ndoes not need to train a model from scratch for lan-\nguages in the low-resource settings, as they could\nbeneﬁt from high-resource languages. We ﬁnd that\nthis is not always the case.\n(Kelechi et al., 2021) introduced AfriBERTa,\na multilingual language model trained on less\nthan 1GB of data from 11 African languages.\nTraining AfriBERTa from scratch showcased how\nAfrican languages can beneﬁt from being included\nin the pre-training stage of MPLMs. AfriBERTa\nproduced competitive results compared to exist-\ning MPLMs (e.g., mBERT, XLM-R), and outper-\nformed them on text classiﬁcation and NER tasks.\nRather than relying on high-resource languages for\ntransfer-learning, AfriBERTa leverages the linguis-\ntic similarity between languages with low-resource\nsettings to produce promising results. (Kelechi\net al., 2021) empirically demonstrates that this is\nmore beneﬁcial to these languages and is crucial\nin assessing the viability of language models pre-\ntrained on small datasets.\n(Antoine and Niyongabo, 2022) went beyond\nthe linguistic taxonomy in creating KinyaBERT,\na morphology-aware language model for Kin-\nyarwanda. Trained on a 2.4GB corpus contain-\ning news articles from 370 websites registered\nbetween 2011 and 2021, KinyaBERT boasts a\nTransformer-like architecture that helps the repre-\nsentation of morphological compositionality. Their\nexperiments outperformed solid baseline results\nfor tasks such as NER and machine-translated\nGLUE on the Kinyarwanda language. These re-\nsults demonstrated the effectiveness of not relying\non transfer learning from high resource languages\nand rather explicitly incorporating morphological\ninformation of the African languages in their pre-\ntraining stage.\nIn the next section, we will describe our self-\nactive learning framework, and the core details of\nour approach.\n3\nSelf-Active Learning Framework\nIn this section, we describe our self-active learn-\ning framework (Figure 1). In Algorithm 1, we\npresent a single active learning loop. In our current\nwork, our model is trained only with a Masked Lan-\nguage Modeling (MLM) objective (Conneau et al.,\n2019; Conneau and Lample, 2019; Devlin et al.,\n2018). We plan to further incorporate Translation\nLanguage Modeling (TLM) objective to improve\ntranslations of low-resource languages with rela-\ntively few thousands of data points 1. This will be\nuseful for both supervised and unsupervised trans-\nlation (Adelani et al., 2022a; Conneau et al., 2019).\n1https://github.com/facebookresearch/XLM\nWe used a shared Sentence Piece vocabulary\nwith 250, 000 BPE codes. The subword shared\nvocabulary intends to improve alignment in the\nembedding space across languages (see languages\ndescription in Appendix A and corpora details in\nTable 1) that are linguistically similar in features\nsuch as script/alphabet, morphology, etc. (Conneau\net al., 2019), reﬂecting our focus languages. Addi-\ntionally, (Conneau et al., 2019) showed that scaling\nthe size of the shared vocabulary (e.g. from 36,000\nto 256,000) improved the performance of multilin-\ngual models on downstream tasks. Our vocabulary\nis deﬁned jointly across all 23 languages and ﬁxed\nduring training, as opposed to random training and\nheld-out dataset selection at each active learning\nround.\nThe motivations behind the randomness in the\nselection of the training and held-out datasets are:\n(1) to make efﬁcient use of the limited dataset we\nhave, and (2) to expose the model step by step,\ninstead of simultaneously, to a variety of samples\nacross different news sub-domains. We believe\nthis would help in domain-shift adaptation and the\nrobustness of the model.\nAs extensively detailed in Algorithm 1, at each\nround we randomly select m sentences per lan-\nguage, from the held-out dataset of the language.\nFor a language, to generate a new sentence s′, given\nan original sentence s, we proceed as follows (more\ndetails can be found in Algorithm 1):\n1. select an initial ordered (left to right) set of\nwords from s as prompt,\n2. add a mask token at the end of the ordered set\nor sequence of words,\n3. query the model to predict the masked token,\n4. choose the best word, add it to the prompt,\n5. repeat 2-4 until we reach the length of s.\nThe process described above will produce m new\ndata points that will be added to the language\ndataset. The new dataset obtained is used to re-train\nthe model from scratch at the next active learning\nround.\n4\nExperiments, Results and Discussion\nExperiments:\nWe\nuse\nthe\nXLM-RoBERTa\n(XLM-R) architecture in our experiments based\non previous works utilizing the model to achieve\nstate-of-the-art performance in various downstream\nAlgorithm 1 Self-Active Learning Training Round\nRequire:\n• Masked Language Modeling (MLM) objective\nπθ with masking probability p = 0.15\n• Vocabulary V, Model M, Tokenizer T\n• Set of languages L = S\ni∈[1,23]{l}\n• Overall Dataset D = S\nl∈L Dl with Dl the\ndataset of language l\n• Training Dataset Dt with k% randomly se-\nlected sentences from Dl, l ∈L\n• Held-out Dataset H with 1 −k% samples for\neach language: H = S\nl∈L Hl\n• proportion t of words to successively mask in\na sentence (from left to right)\nEnsure:\n• Initialize M, and T with V\n• k ←80\n• t ←15\n• Train M with policy πθ\nGenerate set Gl of new samples for each lan-\nguage:\nfor l ∈L do\nGl ←{}\n• Build Sl with m = |Hl| sentences randomly\nchosen from Hl\n▷we choose m this way to\ncope with small size datasets\nfor s ∈Sl do\nn ←len(s), s = S\ni∈[1,n]{wi}\nts ←\n\u0006 n∗t\n100\n\u0007\n+ 1\nprompt ←S\ni∈[1,n−ts]{wi}\nwhile ts ̸= 0 do\nprompt ←prompt ∪{<mask>}\nwp ←M(prompt):\n▷predicted\nmasked word\nprompt ←prompt ∪{wp}\nts ←ts −1\nend while\nGl ←Gl ∪{prompt}\nend for\nDl ←Dl ∪Gl\n▷new samples added to the\nlanguage dataset\nend for\nModel\nHyper-parameters\nValues\nAfroLM-Large\nsequence maximum length\n256\nhidden size\n768\nattention heads\n6\nhidden layers\n10\nlearning rate\n1e-4\nbatch size\n32\n# of Parameters\n264M\ntotal initial training examples\n5,137,026\nvocabulary size\n250,000\ngradient accumulation steps\n8\nwarming steps\n40,000\ntraining steps\n500,000\nTable 2: Hyper-parameters summary\ntasks. Following the work and results of (Kelechi\net al., 2021), we trained XLM-R-based models\nfrom scratch.\nIn our current work we trained\nthe model with 3 self-active learning rounds (we\nstopped at 3 due to computational resources). We\nused 80% and 20% of languages data for the train-\ning and held-out datasets respectively. We designed\n2 versions of AfroLM: AfroLM-Large (without\nself-active learning) and AfroLM-Large (with self-\nactive learning) with the hyper-parameters speci-\nﬁed in Table 2. All training experiments were done\nusing the HuggingFace Transformers library (Wolf\net al., 2019).\nAfroLM (without self-active learning) is one\nof our baselines. We trained an XLM-R model\non the entire dataset, and the held-out dataset was\njust used for evaluation. For AfroLM-Large mod-\nels, we used Google Cloud with a single 48GB\nNVIDIA A100 GPU. An active learning round took\n≈260 hours of training. We evaluated AfroLM-\nLarge models on three downstream tasks:\n• NER: we evaluated the performance of our\nmodel pre-trained using our self-active learn-\ning framework on the MasakhaNER dataset\n(Adelani et al., 2021a). The dataset contains\nten African languages: Amharic, Hausa, Igbo,\nKinyarwanda, Luganda, Luo, Nigerian Pid-\ngin, Swahili, Wolof, and Yorùbá. (Adelani\net al., 2021a; Alabi et al., 2022a) also pro-\nvided strong baselines with pre-trained lan-\nguage models like mBERT and XLM-R on\nMasakhaNER.\n• Text Classiﬁcation: we tested our models\non Hausa and Yorùbá news text classiﬁcation\ndataset from (Hedderich et al., 2020), where\nthe authors have also built strong baselines on\nmBERT and XLM-R models.\n• Sentiment Analysis: we tested the the out-\nof-domain performance of our model in two\ndomains different from news:\n1. Movies: we directly ﬁne-tuned and\nevaluated AfroLM-Large on the YOSM\ndataset (Shode et al., 2022), which con-\ntains reviews of Yorùbá movies.\n2. Twitter →Movies: in this setup,\nwe ﬁnetuned on the training and valida-\ntion set of NaijaSenti (Muhammad et al.,\n2022), and evaluated on YOSM. Nai-\njaSenti contains human annotated tweets\nin Hausa, Yoruba, Igbo and Nigerian Pid-\ngin. However, we were not able to eval-\nuate AfroLM-Large on it because the\nauthors have not yet released the test set.\nResults & Discussion:\nTables 1 and 3 show that\nour framework includes a large variety of African\nLanguages. Table 4, and Table 5 (with 11 addi-\ntional languages from MasakhaNER 2.0 dataset\n(Adelani et al., 2022b)) show the results of our\nmethod in comparison with other baselines on\nNER task. We can notice that AfroLM-Large (w/\nAL) outperforms AfriBERTa-Large, mBERT and\nXLMR-base (≈2.5 TB of data); while being pre-\ntrained on signiﬁcantly smaller dataset (≈0.73\nGB (80% of 0.91 GB initial dataset)). AfriBERTa-\nLarge has been pretrained from scratch on 11\nAfrican languages, while mBERT and XLMR-base\n(with existing pretrained weights) were ﬁnetuned\non the MasakhaNER dataset.\nTable 6 and Table 7 show that, on the text classi-\nﬁcation and sentiment analysis tasks, our method\noutperforms many existing baselines. Additionally,\nout-of-domain experiments and analyses show that\nour method is robust and provides good results in\nout-of-domain settings.\nWhile AfroXLMR-base in average, slightly out-\nperforms our approach, it is important to notice that\nit has been pretrained on a dataset 14x bigger than\nour set. Furthermore, AfroLM-Large has been\ntrained on ≈0.73 GB of data (80% of 0.91 GB\ninitial dataset), which is less than the size of the\ncorpus used to train AfriBERTa (0.939 GB). This\nallows us to conﬁdently afﬁrm that our approach is\ndata-efﬁcient, while being very competitive.\nIt is important to note that the margin of per-\nformance from AfroLM-Large (w/ AL) does not\ncome from the fact that it has been trained on more\nlanguages. Our results show that AfroLM-Large\nLanguage\nIn\nIn\nIn\nIn\nIn\nAfriBERTa?\nAfroLM?\nAfroXLMR\nmBERT?\nXLMR?\namh\n\u0013\n\u0013\n\u0013\n\u0017\n\u0013\nhau\n\u0013\n\u0013\n\u0013\n\u0017\n\u0013\nibo\n\u0013\n\u0013\n\u0013\n\u0017\n\u0017\nkin\n\u0013\n\u0013\n\u0013\n\u0017\n\u0017\nlug\n\u0017\n\u0013\n\u0017\n\u0017\n\u0017\nluo\n\u0017\n\u0013\n\u0017\n\u0017\n\u0017\npcm\n\u0013\n\u0013\n\u0013\n\u0017\n\u0017\nswa\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\nwol\n\u0013\n\u0013\n\u0013\n\u0017\n\u0017\nyor\n\u0013\n\u0013\n\u0013\n\u0013\n\u0017\nTable 3: Information about languages included in each language model. We can notice that AfroLM includes the\nmost of them.\nLanguage\nAfriBERTa-Large\nAfroLM-Large\nAfroLM-Large\nAfroXLMR-base\nmBERT\nXLMR-base\n(w/o AL)\n(w/ AL)\namh\n73.82\n43.78\n73.84\n76.10\n00.00\n70.96\nhau\n90.17\n84.14\n91.09\n91.10\n87.34\n87.44\nibo\n87.38\n80.24\n87.65\n87.40\n85.11\n84.51\nkin\n73.78\n67.56\n72.84\n78.00\n70.98\n73.93\nlug\n78.85\n72.94\n80.38\n82.90\n80.56\n80.71\nluo\n70.23\n57.03\n75.60\n75.10\n72.65\n75.14\npcm\n85.70\n73.23\n87.05\n89.60\n87.78\n87.39\nswa\n87.96\n74.89\n87.67\n88.60\n86.37\n87.55\nwol\n61.81\n53.58\n65.80\n67.40\n66.10\n64.38\nyor\n81.32\n73.23\n79.37\n82.10\n78.64\n77.58\navg\n79.10\n68.06\n80.13\n81.90\n71.55\n79.16\navg (excl. amh)\n79.69\n70.76\n80.83\n82.54\n79.50\n80.07\nTable 4: NER Performances: F1-scores on languages test sets after 50 epochs averaged over 5 seeds. These results\ncover all 4 tags in the MasakhaNER dataset: PER, ORG, LOC, DATE. XLM-R and mBERT results obtained from\n(Adelani et al., 2021b). AfroLM-Large (w/ AL) outperforms AfriBERTa, and the initial MasakhaNER baselines.\nThe bold numbers represent the performance of the model with the lowest pretrained data. AfroXMLR-\nbase = XLMR-Large + MAFT (Alabi et al., 2022a) with 272M parameters. MAFT gives similar performance to\nindividual LAFT models (Alabi et al., 2022a) (LAFT results in single model per language).\n(w/ AL) outperforms models trained on signiﬁ-\ncantly larger datasets and number of languages.\nMoreover, the comparison of AfroLM-Large (w/\nAL) to AfroLM-Large (w/o AL) shows a signiﬁ-\ncant improvement in performance, which implies\nthat our self-active learning framework is efﬁcient,\nand leads to a better performance. This is expected,\nbecause the idea of our self-active learning (and of\nactive learning in general) is that AfroLM consis-\ntently and dynamically, identiﬁes during the train-\ning phase, the most beneﬁcial sample(s) to learn\nfrom in order to boost the performance.\nIn our current algorithm, a sentence sample is\ngenerated by iterative next-token prediction: the\ngenerated sentence is the result of the concatenation\nof each best token. Diversity in sample generation\nand selection is paramount, and we believe, could\nimprove the performance of our framework. In\nthe limitation section (section 6), we propose a\nway of selecting diverse sentences (after sentence\ngeneration). We also propose a new weighted loss,\nthat we believe will be more balanced across the\nentire dataset.\n5\nFuture works and Conclusion\nIn conclusion, we propose AfroLM, a self-active\nlearning-based multilingual language model sup-\nporting 23 African Languages; the largest to date.\nOur language datasets are collected from the news\ndomain and span across different parts of the\nAfrican continent. Our experimental results on\nNLP downstream tasks (NER, text classiﬁcation,\nand out-of-domain sentiment analysis), prove the\ndata-efﬁciency of AfroLM (as it has been trained\non a dataset 14x smaller than its competitors),\nand its competitiveness as it outperforms many\nModel\nbam\nbbj\newe\nfon\nmos\nnya\nsna\ntsn\ntwi\nxho\nzul\nAVG\nMPLMs pre-trained on from scratch on African Languages\nAfriBERTa-Large\n78.60\n71.00\n86.90\n79.90\n71.40\n88.60\n92.40\n83.20\n75.70\n85.00\n81.70\n81.31\nAfroLM-Large (w/ AL)\n80.40\n72.91\n88.14\n80.48\n72.14\n90.25\n94.46\n85.38\n77.89\n87.50\n86.31\n83.26\nMPLMs adapted to African Languages\nAfroXLMR-base\n79.60\n73.30\n89.20\n82.30\n74.40\n91.90\n95.70\n87.70\n78.90\n88.60\n88.40\n84.55\nmBERT\n78.90\n60.60\n86.90\n79.90\n71.40\n88.60\n92.40\n86.40\n75.70\n85.00\n81.70\n80.68\nXLMR-base\n78.70\n72.30\n88.50\n81.90\n72.70\n89.90\n93.60\n86.10\n78.70\n87.00\n84.60\n83.09\nTable 5: NER Baselines on MasakhaNER2.0 (Adelani et al., 2022b). We compare MPLMs trained from scratch\non African languages, and MPLMs adapted to African Languages. The average of scores are over 5 runs. The bold\nnumbers represent the performance of the model with the lowest pretrained data.\nLanguage\nAfriBERTa-Large\nAfroLM-Large\nAfroLM-Large\n(w/o AL)\n(w/ AL)\nhau\n90.86\n85.57\n91.00\nyor\n83.22\n75.30\n82.90\nTable 6: Text Classiﬁcation Performances: F1-scores on the languages test sets. The bold numbers represent\nthe performance of the model with the lowest pretrained data.\nModels\nYoruba F1-score\nAfroLM-Large (w/o AL)\nMovies\n83.12\nTwitter →Movies\n41.28\nAfroLM-Large (w/ AL)\nMovies\n85.40\nTwitter →Movies\n68.70\nAfriBERTa-Large\nMovies\n82.70\nTwitter →Movies\n65.90\nTable 7: Out-Of-Domain Sentiment Analysis Performance: F1-scores on YOSM test set after 20 epochs av-\neraged over 5 seeds. The bold numbers represent the performance of the model with the lowest pretrained\ndata.\nMPLMs (AfriBERTa, mBERT, XLMR-base) while\nbeing very competitive to AfroXLMR-base. We\nalso show that AfroLM is also able to general-\nize across various domains. For future work, we\nintend to: (1) explore and understand the rela-\ntionship between the number of active learning\nsteps and the MPLMs performance on downstream\ntasks, and (2) integrate a new weighted loss, and\nmore diversity in new data points generation and\nselection as we explain in the limitation section\n(see section 6). Our datasets, and source code are\npublicly available at https://github.com/\nbonaventuredossou/MLM_AL.\n6\nLimitations and Approach of Solution\nCurrently, the loss of the model across the train-\ning dataset (across all 23 languages), appears to\nbe the average of the individual (cross-entropy)\nlosses. Due to the disparate sizes of our corpora\nper language, the training will be biased toward the\nlanguages whose sizes predominate the training set.\nTherefore, we suggest a strategy to re-weight the\ncross entropy loss per language by the ratio of the\nsize of the dataset for that language to the size of\nthe entire training set:\nL = 1\nN\nX\nl\n|Dl\nD |Ll\nwhere |Dl\nD | is the weight of the training dataset of\nthe language l, Ll is the loss of the model on a\ngiven language l, and N is the total number of lan-\nguages (23 in our case). We believe this adjusts\nwell overall loss by using the right weighted loss of\neach language, which can be seen as their respec-\ntive contribution to the general loss.\nAnother limitation of our current framework is\nthat the samples that are generated from prompts\nmight not be diverse. Given a batch B of generated\nsamples, and a set S of initial samples, we want the\nsamples selected to be substantially different from\nthe majority of samples present in S. We think\nthat performing the following two steps will help\nto ensure this:\n1. Increasing the number of words, in a sentence,\nto be masked: this implies that the length\nof the prompt is shortened, and that we pro-\nvide less (or short) context in the input to our\nmodel. Long-range semantics is still a chal-\nlenge in natural language generation and un-\nderstanding, and large language models (GPT-\n2, DialoGPT) have insufﬁciently learned the\neffect of distant words on next-token predic-\ntion (Malkin et al., 2022). Therefore, we be-\nlieve that providing a short context will in-\ncrease the choices of the model and lead to\nthe generation of more various tokens. This\nhas been shown by (Malkin et al., 2022) where\nthey also introduced the coherence boosting\napproach to increase the focus of a language\nmodel on a long context.\n2. Using the Word Error Rate (WER) as a sim-\nple diversity measurement. The WER is an\nadaptation of the Levenshtein distance (also\ncalled edit distance), working at the word\nlevel instead of the phoneme level. Ideally,\nwe want high WER. Let W = S\ni∈[1,ts]{wi},\nthe set of words from a sentence s that we\ncut off for the next-token prediction loop de-\nscribed in section 3 and in Algorithm 1. Let\nW ′ = S\ni∈[1,ts]{w′\ni}, the set of words pre-\ndicted by the model. Then, for a pair (s, s′) of\nthe original sentence and new generated sen-\ntence (s′ = prompt ∪W ′), we can deﬁne a\ndiversity score ds,s′ = WER(W, W ′). Given\nthe deﬁnition of d, for a language l, we can\ndeﬁne a diverse batch\nBl\ndiverse =\n[\ni∈[1,|Hl|]\n{s′\ni | dsi,s′\ni ≥t}\nwhere t is an hyper-parameter, representing\nan error threshold. t can be tuned because a\nsmall t will result in a less diverse batch, while\na very huge value will result in an empty or\nalmost empty batch.\n7\nEthics Statement\nAs any modern technology, machine learning al-\ngorithms are subject to potential dual good or bad\nusage. Our work is motivated by the desire of\nmaking AI (in general, NLP in particular) applica-\ntions to be inclusive to the low-resourced languages\n(which are the vast majority of existing living lan-\nguages), hence beneﬁting to humanity and society.\nWe strongly discourage bad and unethical use of\nour work (and its derivations).\nReferences\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neu-\nbig, Daniel D’souza, Julia Kreutzer, Constantine\nLignos, Chester Palen-Michel, Happy Buzaaba,\nShruti Rijhwani, Sebastian Ruder, Stephen May-\nhew, Israel Abebe Azime, Shamsuddeen H. Muham-\nmad, Chris Chinenye Emezue, Joyce Nakatumba-\nNabende,\nPerez\nOgayo,\nAremu\nAnuoluwapo,\nCatherine Gitau, Derguene Mbaye, Jesujoba Al-\nabi, Seid Muhie Yimam, Tajuddeen Rabiu Gwad-\nabe, Ignatius Ezeani, Rubungo Andre Niyongabo,\nJonathan Mukiibi, Verrah Otiende, Iroro Orife,\nDavis David, Samba Ngom, Tosin Adewumi, Paul\nRayson, Mofetoluwa Adeyemi, Gerald Muriuki,\nEmmanuel Anebi, Chiamaka Chukwuneke, Nkiruka\nOdu,\nEric Peter Wairagala,\nSamuel Oyerinde,\nClemencia Siro, Tobius Saul Bateesa, Temilola\nOloyede, Yvonne Wambui, Victor Akinode, Deb-\norah Nabagereka,\nMaurice Katusiime,\nAyodele\nAwokoya, Mouhamadane MBOUP, Dibora Gebrey-\nohannes, Henok Tilaye, Kelechi Nwaike, Degaga\nWolde, Abdoulaye Faye, Blessing Sibanda, Ore-\nvaoghene Ahia, Bonaventure F. P. Dossou, Kelechi\nOgueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,\nAdewale Akinfaderin, Tendai Marengereke, and Sa-\nlomey Osei. 2021a.\nMasakhaNER: Named Entity\nRecognition for African Languages.\nTransactions\nof the Association for Computational Linguistics,\n9:1116–1131.\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neubig,\nDaniel D’souza, Julia Kreutzer, Constantine Lignos,\nChester Palen-Michel, Happy Buzaaba, Shruti Rijh-\nwani, Sebastian Ruder, et al. 2021b. Masakhaner:\nNamed entity recognition for african languages.\nTransactions of the Association for Computational\nLinguistics, 9:1116–1131.\nDavid Ifeoluwa Adelani, Jesujoba Oluwadara Alabi,\nAngela Fan, Julia Kreutzer, Xiaoyu Shen, Machel\nReid, Dana Ruiter, Dietrich Klakow, Peter Nabende,\nErnie Chang, Tajuddeen Gwadabe, Freshia Sackey,\nBonaventure F. P. Dossou, Chris Chinenye Emezue,\nColin Leong, Michael Beukman, Shamsuddeen Has-\nsan Muhammad, Guyo Dub Jarso, Oreen Yousuf,\nAndre Niyongabo Rubungo, Gilles HACHEME,\nEric Peter Wairagala, Muhammad Umair Nasir,\nBenjamin Ayoade Ajibade, Oluwaseyi Ajayi Ajayi,\nYvonne Wambui Gitau, Jade Abbott, Mohamed\nAhmed, Millicent Ochieng, Anuoluwapo Aremu,\nPerez Ogayo, Jonathan Mukiibi, Fatoumata Ouoba\nKabore, Godson KofﬁKALIPE, Derguene Mbaye,\nAllahsera Auguste Tapo,\nVictoire Memdjokam\nKoagne, Edwin Munkoh-Buabeng, Valencia Wag-\nner, Idris Abdulmumin, and Ayodele Awokoya.\n2022a. A few thousand translations go a long way!\nleveraging pre-trained models for african news trans-\nlation. In NAACL-HLT.\nDavid Ifeoluwa Adelani, Graham Neubig, Sebas-\ntian Ruder, Shruti Rijhwani, Michael Beukman,\nChester Palen-Michel,\nConstantine Lignos,\nJe-\nsujoba O. Alabi, Shamsuddeen H. Muhammad,\nPeter Nabende, Cheikh M. Bamba Dione, An-\ndiswa Bukula, Rooweither Mabuya, Bonaventure\nF. P. Dossou, Blessing Sibanda, Happy Buzaaba,\nJonathan Mukiibi, Godson Kalipe, Derguene Mbaye,\nAmelia Taylor,\nFatoumata Kabore,\nChris Chi-\nnenye Emezue, Anuoluwapo Aremu, Perez Ogayo,\nCatherine Gitau, Edwin Munkoh-Buabeng, Vic-\ntoire M. Koagne, Allahsera Auguste Tapo, Tebogo\nMacucwa, Vukosi Marivate, Elvis Mboning, Tajud-\ndeen Gwadabe, Tosin Adewumi, Orevaoghene Ahia,\nJoyce Nakatumba-Nabende, Neo L. Mokono, Ig-\nnatius Ezeani, Chiamaka Chukwuneke, Mofetoluwa\nAdeyemi, Gilles Q. Hacheme, Idris Abdulmumin,\nOdunayo Ogundepo, Oreen Yousuf, Tatiana Moteu\nNgoli, and Dietrich Klakow. 2022b.\nMasakhaner\n2.0: Africa-centric transfer learning for named en-\ntity recognition.\nJesujoba O Alabi, David Ifeoluwa Adelani, Marius\nMosbach, and Dietrich Klakow. 2022a.\nMulti-\nlingual language model adaptive ﬁne-tuning:\nA\nstudy on african languages.\narXiv preprint\narXiv:2204.06487.\nJesujoba O Alabi, Adelani David Ifeoluwa, Mosbach\nMarius, and Klakow Dietrich. 2022b. Multilingual\nLanguage Model Adaptive Fine-Tuning:\nA case\nstudy on African Languages. COLING.\nNzeyimana Antoine and Rubungo Andre Niyongabo.\n2022.\nKinyaBERT:a Morphology-aware Kin-\nyarwanda Language Model. ACL.\nHounkpati B. C. Capo. 1991. A comparative phonol-\nogy of Gbe. Foris Publications.\nYukun Chen,\nThomas A. Lasko,\nQiaozhu Mei,\nJoshua C. Denny, and Hua Xu. 2015. A study of ac-\ntive learning methods for named entity recognition\nin clinical text. Journal of Biomedical Informatics,\n58:11–18.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nDavid M. Eberhard, Gary F. Simons, and Charles\nD. Fennig (eds.). 2020. Ethnologue: Languages of\nthe world. twenty-third edition.\nLiat Ein-Dor, Alon Halfon, Ariel Gera, Eyal Shnarch,\nLena Dankin, Leshem Choshen, Marina Danilevsky,\nRanit Aharonov, Yoav Katz, and Noam Slonim.\n2020.\nActive Learning for BERT: An Empirical\nStudy. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP), pages 7949–7962, Online. Associa-\ntion for Computational Linguistics.\nYarin Gal, Riashat Islam, and Zoubin Ghahramani.\n2017.\nDeep bayesian active learning with image\ndata. In Proceedings of the 34th International Con-\nference on Machine Learning - Volume 70, ICML’17,\npage 1183–1192. JMLR.org.\nMichael A. Hedderich, David Adelani, Dawei Zhu, Je-\nsujoba Alabi, Udia Markus, and Dietrich Klakow.\n2020.\nTransfer learning and distant supervision\nfor multilingual transformer models: A study on\nAfrican languages. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 2580–2591, Online. As-\nsociation for Computational Linguistics.\nMoksh Jain, Emmanuel Bengio, Alex-Hernandez Gar-\ncia, Jarrid Rector-Brooks, Bonaventure F. P. Dossou,\nChanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal\nKilgour, Dinghuai Zhang, Lena Simine, Payel Das,\nand Yoshua Bengio. 2022. Biological sequence de-\nsign with gﬂownets.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld.\nIn Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6282–6293, Online. Association for Computa-\ntional Linguistics.\nOgueji Kelechi, Zhu Yuxin, and Lin Jimmy. 2021.\nSmall Data? No Problem! Exploring the Viability of\nPretrained Multilingual Language Models for Low-\nresourced Languages. EMNLP, pages 116–126.\nClaire Lefebvre and Anne-Marie Brousseau. 2002. A\ngrammar of Fongbe. Mouton de Gruyter.\nNikolay Malkin, Zhen Wang, and Nebojsa Jojic. 2022.\nCoherence boosting:\nWhen your pretrained lan-\nguage model is not paying enough attention. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 8214–8236, Dublin, Ireland. Associ-\nation for Computational Linguistics.\nShamsuddeen Hassan Muhammad, David Ifeoluwa\nAdelani, Sebastian Ruder, Ibrahim Said Ahmad,\nIdris Abdulmumin,\nBello Shehu Bello,\nMono-\njit\nChoudhury,\nChris\nChinenye\nEmezue,\nSa-\nheed Salahudeen Abdullahi, Anuoluwapo Aremu,\nAlipio Jeorge, and Pavel Brazdil. 2022. Naijasenti:\nA nigerian twitter sentiment corpus for multilingual\nsentiment analysis.\nRubungo Andre Niyongabo, Qu Hong, Julia Kreutzer,\nand Li Huang. 2020. KINNEWS and KIRNEWS:\nBenchmarking cross-lingual text classiﬁcation for\nKinyarwanda and Kirundi.\nIn Proceedings of\nthe 28th International Conference on Computa-\ntional Linguistics, pages 5507–5521, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nIyanuoluwa Shode, David Ifeoluwa Adelani, and Anna\nFeldman. 2022. YOSM: A NEW YORUBA SENTI-\nMENT CORPUS FOR MOVIE REVIEWS. In 3rd\nWorkshop on African Natural Language Processing.\nAditya Siddhant and Zachary C. Lipton. 2018. Deep\nbayesian active learning for natural language pro-\ncessing: Results of a large-scale empirical study.\nJustin S. Smith, Benjamin Tyler Nebgen, Nick Lub-\nbers, Olexandr Isayev, and Adrian E. Roitberg. 2018.\nLess is more: sampling chemical space with ac-\ntive learning. The Journal of chemical physics, 148\n24:241733.\nManuel Tonneau, Dhaval Adjodah, Joao Palotti, Nir\nGrinberg, and Samuel Fraiberger. 2022.\nMultilin-\ngual detection of personal employment status on\nTwitter. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 6564–6587, Dublin,\nIreland. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2019. Ccnet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mt5: A massively\nmultilingual pre-trained text-to-text transformer. In\nNAACL.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\nA\nLanguage Characteristics\nAmharic\n(amh)\nalso\ncalled\nAmarinya\nor\nAmerigna, is a Semitic language, an ofﬁcial\nlanguage of Ethiopia, and is also spoken in Eritrea.\nAmharic is written with a modiﬁed version of\nthe Ge’ez script, known as Fidel, consisting of\n33 basic characters, each of them with at least 7\nvowel sequences. Unlike Central and Northwest\nSemitic languages such as Arabic, Hebrew and\nAssyrian Aramaic, Amharic is written from left to\nright. The language has a variety of local dialects,\nall of which are mutually intelligible.\nThere\nare three major dialects: Gondar, Gojjami, and\nShowa. There are specially marked differences in\npronunciation, vocabulary, and grammar between\nthe northern Gojjami and the southern Showa\ndialects.\nAfan Oromo (oro)\nis an Afroasiatic language\nthat belongs to the Cushitic branch spoken by about\n30 million people in Ethiopia, Kenya, Somalia and\nEgypt, and it is the third largest language in Africa.\nThe Oromo people are the largest ethnic group in\nEthiopia and account for more than 40% of the pop-\nulation. They can be found all over Ethiopia, and\nparticularly in Wollega, Shoa, Illubabour, Jimma,\nArsi, Bale, Hararghe, Wollo, Borana and the south-\nwestern part of Gojjam2. Afan Oromo is written\nwith a Latin alphabet called Qubee. Like most other\nEthiopian languages, whether Semitic, Cushitic, or\nOmotic, Oromo has a set of ejective consonants,\nthat is, voiceless stops or affricates that are accom-\npanied by glottalization and an explosive burst of\nair. Afan Oromo has another glottalized phone\nthat is more unusual, an implosive retroﬂex stop,\n\"dh\" in Oromo orthography, a sound that is like\nan English \"d\" produced with the tongue curled\nback slightly and with the air drawn in so that a\nglottal stop is heard before the following vowel\nbegins. It is retroﬂex in most dialects, though it\nis not strongly implosive and may reduce to a ﬂap\n2https://omniglot.com/writing/oromo.htm\nbetween vowels3. In the Qubee alphabet, letters\ninclude the digraphs ch, dh, ny, ph, sh. Gemina-\ntion is not obligatorily marked for digraphs, though\nsome writers indicate it by doubling the ﬁrst ele-\nment: qopphaa’uu ’be prepared’. Afan Oromo has\nﬁve vowel phonemes, i.e., sounds that can differ-\nentiate word meaning. They can be short or long.\nThe length of the vowel makes a difference in word\nmeaning e.g., laga ‘river’ and laagaa ‘roof of the\nmouth’. Afan Oromo has 25 consonant phonemes,\ni.e., sounds that make a difference in word meaning.\nLike its close relative, Somali, native Oromo words\ndo not have the consonants /p/, /v/, and /z/.\nBambara (bam)\nis a Western Mande language\nwith about 14 million speakers mainly in Mali, and\nalso in Senegal, Niger, Mauritania, Gambia and\nCôte d’Ivoire. It is spoken principally among the\nBambara ethnic group in Mali, where it is the na-\ntional language and the most widely understood\none. Bambara is usually written with the Latin al-\nphabet, though the N’Ko and Arabic alphabets are\nalso used to some extent. It uses seven vowels a, e,\nE, i, o, O, and u each of which can be nasalized, pha-\nryngealized and murmured, giving a total number\nof 21 vowels.\nGhomalá’ (bbj)\nis a major Bamileke language\nspoken in Cameroon. It is spoken by an estimated\n1.1 million people in two main population groups.\nÉwé (ewe)\nis a language spoken in Togo and\nsoutheastern Ghana by approximately 20 million\npeople mainly in West Africa in the countries of\nGhana, Togo, and Benin. It is recognised as a\nnational language in Ghana, where English is the\nofﬁcial language, and in Togo, where French is the\nofﬁcial language. ’Ewe’ is also the name of the\ntribal group that speaks this language. Éwé has\nthree distinguishable dialects. Most of the differ-\nences among the dialects have to do with phonol-\nogy. All dialects are mutually intelligible. Éwé\nis written in the African reference alphabet, ﬁrst\nproposed by a UNESCO-organized conference in\n1978. It is a version of the Latin alphabet adapted\nto represent Éwé sounds. Some sounds are rep-\nresented by two-letter sequences, e.g., dz, ts, gb,\nkp, ny. Éwé has seven oral and ﬁve nasal vowels.\nNasal vowels are produced by lowering the soft\npalate so that air escapes both through the mouth\nand the nose. Nasal vowels are marked by a tilde.\n3https://en.wikipedia.org/wiki/Oromo_\nlanguage\nFon (fon)\nalso known as Fongbé is a native lan-\nguage of Benin Republic. It is spoken in average\nby 1.7 million people. Fon belongs to the Niger-\nCongo-Gbe languages family. It is a tonal, isolating\nand left-behind language according to (Joshi et al.,\n2020), with an Subject-Verb-Object (SVO) word\norder. Fon has about 53 different dialects, spoken\nthroughout Benin (Lefebvre and Brousseau, 2002;\nCapo, 1991; Eberhard et al., 2020). Its alphabet is\nbased on the Latin alphabet, with the addition of\nthe letters: ª, ¡, ¢, and the digraphs gb, hw, kp, ny,\nand xw. There are 10 vowels phonemes in Fon: 6\nsaid to be closed [i, u, ˜ı, ˜u], and 4 said to be opened\n[¢, ª, a, ã]. There are 22 consonants (m, b, n, ¡, p,\nt, d, c, j, k, g, kp, gb, f, v, s, z, x, h, xw, hw, w). Fon\nhas two phonemic tones: high and low. High is re-\nalized as rising (low–high) after a consonant. Basic\ndisyllabic words have all four possibilities: high-\nhigh, high-low, low-high, and low-low. In longer\nphonological words, like verb and noun phrases, a\nhigh tone tends to persist until the ﬁnal syllable. If\nthat syllable has a phonemic low tone, it becomes\nfalling (high–low). Low tones disappear between\nhigh tones, but their effect remains as a downstep.\nRising tones (low–high) simplify to high after high\n(without triggering downstep) and to low before\nhigh (Lefebvre and Brousseau, 2002; Capo, 1991).\nHausa (hau)\nbelongs to the West Chadic branch\nof the Afro-Asiatic language family. It is one of the\nlargest languages on the African continent, spoken\nas a ﬁrst language by the original Hausa people\nand by people of Fula ancestry. Hausa is the major-\nity language of much of northern Nigeria and the\nneighboring Republic of Niger. In addition, there\nis a sizable Hausa-speaking community in Sudan4.\nIt has an alphabet of 29 letters containing 5 vow-\nels and 24 consonants. Hausa alphabet is a Latin\nscript/Roman alphabet/English letters except (x, v,\np, and q) and also added six extra letters (á, â, Î, sh,\nts and ¯ (Adelani et al., 2021b). Hausa is an agglu-\ntinative language, i.e., it adds sufﬁxes to roots for\nexpressing grammatical relations without fusing\nthem into one unit, as is the case in Indo-European\nlanguages.\nÌgbò (ibo)\nis one of the largest languages of West\nAfrica, is spoken by 18 million people in Nigeria.\nIt belongs to the Benue-Congo group of the Niger-\nCongo language family. The language is thought to\nhave originated around the 9th century AD in the\n4https://www.mustgo.com/worldlanguages/hausa/\narea near the conﬂuence of the Niger and Benue\nrivers, and then spread over a wide area of south-\neastern Nigeria 5. Igbo is a national language of\nNigeria and is also recognised in Equatorial Guinea.\nIgbo is written in an expanded version of the Latin\nalphabet. Igbo is made up of many different di-\nalects which aren’t mutually intelligible to other\nIgbo speakers at times.\nKinyarwanda (kin)\nis part of the Bantu sub-\ngroup of the central branch of the Niger-Congo\nlanguage family. It is closely related to Kirundi,\nthe language of Burundi. The Rwanda language is\nmutually intelligible with Kirundi, which is spoken\nin neighboring Burundi6. It has only 18/19 conso-\nnants, as X and Q are not found in the alphabet. L\nis often replaced by R, but due to the appearance of\nimported words in the language, that is not always\nthe case. It has ﬁve vowel phonemes, i.e., sounds\nthat make a difference in word meaning.\nLingala (lin)\nis a Central Bantu language that be-\nlongs to the largest African languages phylum: the\nNiger-Congo. Lingala is spoken as a ﬁrst, second,\nand third language primarily in the Democratic Re-\npublic of Congo (DRC), the Republic of Congo\n(Congo-Brazzaville), and in parts of ﬁve neighbor-\ning central African states: Northwestern Angola,\neastern Gabon, southern Central African Republic,\nand southwestern Sudan. The estimated number\nof speakers ranges from twenty to twenty ﬁve mil-\nlion7. It is written with the Latin alphabet. The\nseven vowels are represented by ﬁve symbols. The\northographic symbols ’e’ and ’o’ each represent\ntwo sounds. There are two tones in Lingala. High\ntone is represented with an acute accent, while low\ntone is unmarked.\nLuganda (lug)\nis a Bantu language spoken in the\nAfrican Great Lakes region. It is one of the major\nlanguages in Uganda and is spoken by more than\n10 million Baganda and other people principally in\ncentral Uganda including the capital Kampala of\nUganda. Its alphabet is composed of twenty-four\nletters; 18 consonants (b, p, v, f, m, d, t, l, r, n, z, s,\nj, c, g, k, ny, N), 5 vowels ( a, e, i, o, u) and 2 semi-\nvowels(w, y). Since the last consonant N) does\nnot appear on standard typewriters or computer\nkeyboards, it is often replaced by the combination\nng’. All consonants are pronounced as if with letter\n5https://www.mustgo.com/worldlanguages/igbo/\n6https://nalrc.indiana.edu/doc/brochures/kinyarwanda.pdf\n7https://nalrc.indiana.edu/doc/brochures/lingala.pdf\n‘a’ or ‘ah’ at the end. For example, bah, cah, jah,\ngah, kah, mah, pah, lah, zah, e.t.c\nLuo (luo)\nare spoken by the Luo peoples in\nan area ranging from southern Sudan to south-\nern Kenya, with Dholuo extending into north-\nern Tanzania and Alur into the Democratic\nRepublic of the Congo.\nLuo has a CVC\nor VC structure—consonant/vowel/consonant or\nvowel/consonant. This is unlike Bantu languages,\nwhere words must end in a vowel. Luo language\nis therefore more similar to English articulation,\nwhile Bantu languages are more like Italian8.\nMooré (mos)\nis a Gur language of the Oti–Volta\nbranch and one of two ofﬁcial regional languages\nof Burkina Faso. It is the language of the Mossi\npeople, spoken by approximately 8 million people\nin Burkina Faso, plus another 1M+ in surround-\ning countries such as Ghana, Cote D’ivoire, Niger,\nMali and Togo as a native language, but with many\nmore L2 speakers. Mooré is spoken as a ﬁrst or\nsecond language by over 50% of the Burkinabè\npopulation.\nChewa (nya)\nis a Bantu language spoken in\nmuch of Southern, Southeast and East Africa,\nnamely the countries of Malawi and Zambia, where\nit is an ofﬁcial language, and Mozambique and Zim-\nbabwe where it is a recognised minority language.\nChewa has ﬁve vowel sounds: /a, E, i, O, u/; these\nare written a, e, i, o, u.\nNaija (pcm)\nis an English-based creole language\nspoken as a lingua franca across Nigeria. The lan-\nguage is sometimes referred to as \"Pijin\" or Broken\n(pronounced \"Brokun\").\nShona (sna)\nis a Bantu language of the Shona\npeople of Zimbabwe. All syllables in Shona end\nin a vowel. Consonants belong to the next syllable.\nFor example, mangwanani (\"morning\") is syllabi-\nﬁed as ma.ngwa.na.ni; \"Zimbabwe\" is zi.mba.bwe.\nNo silent letters are used in Shona.\nSwahili (swa)\nalso known by its native name\nKiswahili, is a Bantu language and the native lan-\nguage of the Swahili people native primarily to\nTanzania. Swahili has become a second language\nspoken by tens of millions in four African Great\nLakes countries (Kenya, DRC, Uganda, and Tan-\nzania), where it is an ofﬁcial or national language,\n8https://owlcation.com/humanities/Luo-language-of-\nKenya-Conversation-Basics\nwhile being the ﬁrst language for many people in\nTanzania especially in the coastal regions of Tanga,\nPwani, Dar es Salaam, Mtwara and Lindi. Standard\nSwahili has ﬁve vowel phonemes: /a/, /E/, /i/, /O/,\nand /u/.\nSetswana (tsn)\nis a Bantu language spoken\nin Southern Africa by about 14 million people.\nSetswana is an ofﬁcial language and lingua franca\nof Botswana and South Africa.\nAkan/Twi\nis a dialect of the Akan language spo-\nken in southern and central Ghana by several mil-\nlion people, mainly of the Akan people, the largest\nof the seventeen major ethnic groups in Ghana. Twi\nexcludes consonants such as c, j, q, v, x and z. It has\n15 consonants and 7 vowels. Apart from [a], [e],\n[i], [o] and [u], Twi also has 2 additional vowels;\n[E] and [O].\nWolof (wol)\nis a language of Senegal, Maurita-\nnia, and the Gambia, and the native language of\nthe Wolof people. Wolof is the most widely spoken\nlanguage in Senegal, spoken natively by the Wolof\npeople (40% of the population) but also by most\nother Senegalese as a second language.\nXhosa (xho)\nalso isiXhosa as an endonym, is a\nNguni language and one of the ofﬁcial languages of\nSouth Africa and Zimbabwe. The Xhosa language\nemploys 26 letters from the Latin alphabet. Xhosa\nhas an inventory of ten vowels: [a], [¢ e], [i], [ª o]\nand [u] written a, e, i, o and u in order, all occurring\nin both long and short. The /i/ vowel will be long\nin the penultimate syllable and short in the last\nsyllable.\nYorùbá (yor)\nhas 25 Latin letters without the\nLatin characters (c, q, v, x and z) and with addi-\ntional letters (e., gb,s., o.).Yorùbá is a tonal language\nwith three tones: low (\"\\\"), middle (\"—\", optional)\nand high (\"/\"). The Latin letters 〈c〉, 〈q〉, 〈v〉, 〈x〉,\n〈z〉are not used as part of the ofﬁcial orthography\nof Standard Yorùbá, however, they exist in several\nYorùbá dialects. The tonal marks and underdots\nare referred to as diacritics and they are needed for\nthe correct pronunciation of a word. Yorùbá is a\nhighly isolating language and the sentence structure\nfollows subject-verb-object (Adelani et al., 2021b).\nZulu (zul)\nis the mother tongue of the Zulu\npeople, South’s Africa largest ethnic group, who\ncreated an empire in the 19th century.Zulu has\na 7-vowel system. Each vowel can be long or\nshort. Zulu has close to 50 consonants including\nclicks, ejectives and implosives. Clicks originated\nin Khoisan languages and then spread into some\nneighboring Bantu ones. In Zulu they have three\nplaces of articulation: central alveolar, lateral alveo-\nlar and palatal combined with ﬁve accompaniments\n(plain, aspirated, voiced, nasal, and voiced nasal).\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2022-11-07",
  "updated": "2022-11-23"
}