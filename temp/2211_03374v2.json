{
  "id": "http://arxiv.org/abs/2211.03374v2",
  "title": "Deep Causal Learning: Representation, Discovery and Inference",
  "authors": [
    "Zizhen Deng",
    "Xiaolong Zheng",
    "Hu Tian",
    "Daniel Dajun Zeng"
  ],
  "abstract": "Causal learning has garnered significant attention in recent years because it\nreveals the essential relationships that underpin phenomena and delineates the\nmechanisms by which the world evolves. Nevertheless, traditional causal\nlearning methods face numerous challenges and limitations, including\nhigh-dimensional, unstructured variables, combinatorial optimization problems,\nunobserved confounders, selection biases, and estimation inaccuracies. Deep\ncausal learning, which leverages deep neural networks, offers innovative\ninsights and solutions for addressing these challenges. Although numerous deep\nlearning-based methods for causal discovery and inference have been proposed,\nthere remains a dearth of reviews examining the underlying mechanisms by which\ndeep learning can enhance causal learning. In this article, we comprehensively\nreview how deep learning can contribute to causal learning by tackling\ntraditional challenges across three key dimensions: representation, discovery,\nand inference. We emphasize that deep causal learning is pivotal for advancing\nthe theoretical frontiers and broadening the practical applications of causal\nscience. We conclude by summarizing open issues and outlining potential\ndirections for future research.",
  "text": " \n \n \n \n \n \n \n \n \n \n \nDeep Causal Learning: Representation, Discovery and Inference \n \nZIZHEN DENG, XIAOLONG ZHENG*, School of Artificial Intelligence, University of Chinese Academy of \nSciences; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese \nAcademy of Sciences, China \nHU TIAN, Guanghua School of Management, Peking University, China \nDANIEL DAJUN ZENG, School of Artificial Intelligence, University of Chinese Academy of Sciences; State Key \nLaboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, China \nCausal learning has garnered significant attention in recent years because it reveals the essential relationships that underpin phenomena \nand delineates the mechanisms by which the world evolves. Nevertheless, traditional causal learning methods face numerous challenges \nand limitations, including high-dimensional, unstructured variables, combinatorial optimization problems, unobserved confounders, \nselection biases, and estimation inaccuracies. Deep causal learning, which leverages deep neural networks, offers innovative insights \nand solutions for addressing these challenges. Although numerous deep learning-based methods for causal discovery and inference have \nbeen proposed, there remains a dearth of reviews examining the underlying mechanisms by which deep learning can enhance causal \nlearning. In this article, we comprehensively review how deep learning can contribute to causal learning by tackling traditional \nchallenges across three key dimensions: representation, discovery, and inference. We emphasize that deep causal learning is pivotal for \nadvancing the theoretical frontiers and broadening the practical applications of causal science. We conclude by summarizing open issues \nand outlining potential directions for future research. \nCCS CONCEPTS â€¢ Computing methodologies â†’ Machine learning; Neural network; Causal reasoning and diagnostics; \nMathematics of computing â†’ Causal networks \nAdditional Keywords and Phrases: Deep learning, Causal representation learning, Causal discovery, Causal inference \n1 INTRODUCTION \nCausality has always been a very important part of scientific research [1-3]. It has been studied in many fields, such as \nbiology [4-5], medicine [6-10], economics [11-15], epidemiology [16-18], and sociology [19-23]. For the construction of \ngeneral artificial intelligence systems, causality is also indispensable [2, 24-25]. For a long time, causal discovery and \ncausal inference have been the main research directions of causal learning. Causal discovery (CD) [26-27] is to find \ncausal relationships from observational or intervention data, usually in the form of a causal graph. Causal inference (CI) \n[3, 28] is to estimate the causal effect between variables, which can be further divided into causal identification and \ncausal estimation [1]. In causal identification, whether the causal effect can be estimated based on the existing \ninformation is determined, and in causal estimation, specific causal effect values are obtained. \nDespite the existence of numerous methods in the field of causal learning, many problems remain unsolved. For \ncausal discovery, most methods require strong assumptions, such as causal Markov conditions, and faithfulness \nassumptions [29-31], which are often impossible to verify. In addition, most traditional causal discovery methods are \n \n*Authorsâ€™ address: Z. Deng, X. Zheng (corresponding author) and D. Zeng, State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of \nAutomation, Chinese Academy of Sciences, Beijing 100190, China; emails: {dengzizhen2021, xiaolong.zheng, dajun.zeng}@ia.ac.cn. H. Tian, Guanghua \nSchool of Management, Peking University, Beijing, 100871, China; emails: tianhu@pku.edu.cn. \n \n2 \nbased on combinatorial optimization [27], a process that becomes comparatively intractable as the number of nodes \nincreases. In causal inference, the absence of counterfactual data and the impracticality of randomized controlled trials \n(RCTs) often necessitate the use of observational data to estimate causal effects. The key problem with causal inference \nfrom observational data is selection bias [3], including confounding bias (from confounders) and data selection bias \n(from colliders). Selection bias can lead us to observe false causality or mistake correlation for causation. Traditional \ncausal inference methods often have significant estimation bias due to limited fitting ability [32-33]. There are also \nseveral persistent issues in both causal discovery and causal inference, such as unknown interventions [34], unobserved \nconfounders [35], and missing data [26]. In addition, causal discovery and inference have predominantly focused on \nstructured data. Nevertheless, as application domains have broadened, there is a growing need to process unstructured \ndata types, such as images, text, and video [36-40]. When dealing with unstructured data, the high-dimensional, low-\nlevel unstructured representations need to be transformed into low-dimensional, high-level structured causal \nrepresentations. \nWe review the use of deep learning methods to address the aforementioned problems in the causal learning field. The \nthree core strengths of deep learning for causal learning are strong representational capabilities, strong fitting \ncapabilities and the ability to approximate data generation mechanisms. Firstly, when dealing with unstructured data, \nthe representational power of neural networks ensures that they can obtain a suitable causal representation for causal \ndiscovery and inference [24, 41-42]. Secondly, due to their general fitting ability and flexibility of neural networks, they \nhave become the primary method for continuous optimization to solve the long-standing combinatorial optimization \nproblem in causal discovery, and they can theoretically cope with large-scale data. The universal approximation theorem \nindicates that neural networks can learn extremely complex functions for estimating heterogeneous treatment effects \nusing low-bias estimators [43-44]. Thirdly, deep learning can explicitly or implicitly model data generation mechanisms. \nFor instance, adversarial learning implicitly generates counterfactuals, commonly implemented with generative \nadversarial networks (GANs) [45], while disentanglement mechanisms that explicitly model the data generation process \nto generate proxy/latent variables, commonly implemented with variational autoencoders (VAEs) [46]. These advantages \nrespectively solve or mitigate various challenges in causal learning, thus providing compelling reasons for integrating \ndeep learning techniques into the field. In this article, we argue that causal representation learning, deep causal discovery, \nand deep causal inference together constitute the field of deep causal learning, as these three parts cover the general \nprocess of exploring causality: representation, discovery, and inference. Figure 1 shows the main differences between \ntraditional causal learning and deep causal learning. We can clearly see the improvements that deep learning brings to \ncausal learning. \nIn the past few years, many deep learning-based causal discovery and causal inference methods have been proposed. \nThere have been many reviews related to causal learning, but few have summarized how to take advantage of deep \nlearning techniques to improve causal learning methods. Guo et al. [25] reviewed causal discovery and causal inference \nmethods with observational data, but very little of the content was related to deep learning. Yao et al. [28] focused on \ncausal inference based on the potential outcome framework. It also mentioned some causal inference methods based on \nneural networks, but the description was not systematic. Nogueira et al. [47] summarized causal discovery and causal \ninference datasets, software, evaluation metrics and running examples without focusing on the theoretical level. Glymour \net al. [26] mainly reviewed traditional causal discovery methods, and Vowels et al. [27] focused on continuous \noptimization discovery methods. Gong et al. [48] summarized the temporal causal discovery methods in-depth, but did \nnot involve non-temporal data and causal inference. There have been reviews combining causality with machine learning  \n3 \n \nFigure 1: The difference between causal learning and deep causal learning. The comparison between (a) and (b) shows the theoretical \nadvantages of deep causal learning. In the framework of deep causal learning, unstructured data can be processed with the \nrepresentational power of neural networks. With the modeling capabilities of neural networks, in causal discovery, observational data \nand (known or unknown) intervention data can be comprehensively used in the presence of hidden confounders to obtain a causal graph \nthat is closer to the facts. With the fitting ability of neural networks, the estimation bias of causal effects can be reduced in causal \ninference. The 4 orange arrows represent the neural network's empowerment of representation, discovery, and inference. (c) and (d) \ndemonstrate the advantages of deep causal learning in more detail by exploring examples of the effect of exercise on blood pressure. We \nassume that the ground truth of exercise on blood pressure is ğ¸(ğ‘‹3|ğ‘‘ğ‘œ(ğ‘‹2 = ğ‘¥)) = âˆ’1.1ğ‘¥+ 84. \n[24, 49-50], but these survey papers have mainly explored how causality can be used to solve problems in the machine \nlearning community. The work of Koch et al. [51] and Li et al. [52] are more similar to our starting point, and focused on \nthe improvements that deep learning brings to causal learning. However, they only considered the combination of deep \nlearning and causal inference and did not address other aspects of the field of causal learning, such as the representation \nof causal variables, causal discovery. \nWe present a more comprehensive and detailed review of the changes that deep learning brings to causal learning. \nThe rest of this article is organized as follows. Section 2 provides basic concepts related to causality. Section 3 reviews \ncausal representation learning using observation and intervention data when dealing with unstructured data. Section 4 \nreviews deep causal discovery methods using i.i.d data and time series data. Section 5 reviews the deep causal inference \nmethods based on covariate balance, adversarial training and proxy variables. Section 6 introduces new frontiers of \ncausal learning. Section 7 provides a conclusion and discusses the future directions of deep causal learning. Figure 2 \nillustrates the overall framework of this survey. \n  \n  \n  \nStructured data\n  \n  \n  \nDiscovery\nInference\nInference\nData\nCausal Relation \nCausal Effect \n  \n  \n  \n  \n  \n  \n  \n  \n  \nUnstructured data\nObservation \ndata\nIntervention\ndata\nHidden\nConfounder\nRepresentation\n  \n  \n  \n  \nDiscovery\nInference\nInference\nCausal Learning\nDeep Causal Learning\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \nNeural \nNetwork\n  :Age\n  :Exercise\n  :Blood Pressure\n (      =  \n= âˆ’ .   +   \n  \n  \n  \n  :Age\n  :Exercise\n  :Blood Pressure\n (      =  \n= âˆ’ +   \n  \n  \n  \n  \nText: Iâ€™m  so tired\nImage:\n  :Mood\nDiscovery\nInference\nDiscovery\nRepresentation\nInference\n(a)\n(b)\n(c)\n(d)\nExample:\nExample:\n4 \n \nFigure 2: An overview of the main structure of the survey. \nDeep Causal \nLearning\nDeep Causal \nDiscovery\nDeep Causal \nInference\nNew Frontiers\nTime Series Data\nCovariates \nBalance\nAdversarial \nTraining\nProxy Variable\nSupervised \nCausal Learning\nEnd to End \nCausal Learning\nLLM for Causal \nLearning\nI.I.D Data\nIntervention \nHidden Confounder \nNoTears, DAG-GNN, GAE, Gran-DAG, DAG-NoCurl, DCD-FG, AEQ, \nCAREFL, CASTLE, DiffAN, SGM \nCAN, CORE, ENCO, SDI \nCGNN, SAM, ICL, NOCADILAC, N-\nADMG, MissDAG\nNGC, TCDF, NAVAR, e-SRU, GVAR, CRVAE, GrID-Net, NTS-Notears, \nRhino, GANF, InGRA, ACD\nIntervention \nSample Frequency \nCRN, IDYNO,LIN \nCUTS, NGM, SCOTCH \nBNN, CFR, BRNN, BWCRF, CARW, FlexTENet, HTCE, SCRNet, \nDragonNet, DONUT, DCN, Deep-treat, RSB-Net, SITE, DESCN,\nCausal-StoNet\nMultiple-treatment \nContinuous-treatment \nTime-varying\nPM, RCFR, NCoRE, SCP\nDRNet, VCNet, ADMIT, CMSM, E2B\nTSD, MSN, DSW, SyncTwin\nGANITE, SCIGAN, MatchGAN, CF-NET, NCM, DeepMatch, CRN, \nABCEI, CETransformer, CBRE, TransTEE\nCEVAE, TEDVAE, TVAE, VSR, DeR-CFR, CFDiVAE, DMSE, \nCredence, DeepIV, CVAE-IV\nML4S, ML4C, AVICI, CSIvA\nDSCM, ABCI, DECI, CInA\nEmpirical studies of LLM on causal discovery and causal inference tasks\nCausal \nRepresentation \nLearning\nObservational \nData\nIntervention Data\nIM, IRS, CausalVAE, GEASS, CBCD,\nLEAP\nMT-CRL, IM-DRCFR, EDNIL, iMSDA, \nDCDAN, iCaRL, LLCP\nDIR, ILCMs, Rep4Ex, CITRIS, iCITRIS\nDisentanglement \nInvariance \n5 \n2 PRELIMINARIES \nIn this section, we briefly introduce the basic concept of causal discovery and causal inference. Table 1 presents the basic \nnotations used in this article. Further comprehensive and detailed causal concepts and knowledge can be found in the \nliterature [1, 3, 31, 53]. \nTable 1: Basic notations and their corresponding descriptions \nNotation \nDescription \nNotation \nDescription \nğ‘‹ğ‘– \nCovariates of the sample ğ‘–. \nğ‘‰ \nThe set of endogenous variables. \nğ‘‡ğ‘– \nTreatment of the sample ğ‘–. \nğ‘ˆ \nThe set of exogenous variables. \nğ‘Œğ‘–\nğ¹ \nFactual outcome of the sample ğ‘–. \nğ¹ \nThe set of mapping functions. \nğ‘Œğ‘–\nğ¶ğ¹ \nCounterfactual outcome of the sample ğ‘–. \nğ‘ƒğ‘ğ‘£ \nThe parent nodes of  ğ‘£. \nğ‘’(ğ‘‹ğ‘–) \nPropensity score of ğ‘‹ğ‘–. \nğ‘ƒ(ğ‘ˆ) \nDistribution of exogenous variables. \nğ›·(ğ‘‹ğ‘–) \nCovariates representation of ğ‘‹ğ‘–. \nğ¼ \nInstrumental variable. \nğœƒ \nNeural network parameters. \nğ‘ \nHidden/latent factors. \nâ„(âˆ™) \nNeural network mapping. \nğ‘‘ğ‘–ğ‘ (âˆ™) \nDistance measure. \nğ¼ğ‘ƒğ‘€ğº(âˆ™) \nIntegral probability metrics. \nğ¸ğ‘ \nThe expectation based on data distribution ğ‘. \nâ€–âˆ™â€–ğ‘ \nğ‘-norm. \nğ‘(ğœ‡, ğœ2) Gaussian distribution with mean ğœ‡ and variance ğœ2. \nğ¿ğ‘œğ‘ ğ‘ (âˆ™) \nLoss function. \nğ›¼, ğ›½, ğ›¾ \nHyperparameters. \n2.1 Causal Model \nStructure Causal Model. A structural causal model (SCM) is a 4-tuple < ğ‘‰, ğ‘ˆ, ğ¹, ğ‘ƒ(ğ‘ˆ) >, where ğ‘‰ is the set of \nendogenous variables, ğ‘ˆ is the set of exogenous variables, ğ¹ is the set of mapping functions accomplish the mapping \nfrom the parent node ğ‘ƒğ‘ğ‘£ to ğ‘£, and ğ‘ƒ(ğ‘ˆ) is the distribution of exogenous variables [3, 53]. \nIn essence, SCM is a subjective abstraction of the objective world, and the involved endogenous and exogenous \nvariables are heavily dependent on the researchers' prior knowledge. That is, the definitions of these variables themselves \nare not necessarily accurate, or the most essential variables cannot be observed due to various limitations. For example, \nwhen studying the effect of a person's family status on their academic performance, we might use the family's annual \nincome as a proxy variable, although this variable may not be entirely appropriate or even correct. \nUsually, each SCM model has a corresponding causal graph ğº, which is typically a directed acyclic graph (DAG) [3]. \nIn fact, the casual graph can be seen as an integral part of SCM, in addition to counterfactual logic. As shown in Figure 3, \nthere are three basic structures in a causal graph: Chain (a), Fork (b) and Collider (c). These three basic structures \nconstitute a variety of causal graphs. In Figure 3 (e), ğ‘‹ represent covariates, ğ‘Œ represent outcome, ğ‘‡ represent treatment, \nğ¶ represent confounders, ğ¼ represent instrumental variable, ğ‘ˆğ‘¡ represent the exogenous variable of ğ‘‡ and ğ‘ˆğ‘¦ represent the \nexogenous variable of ğ‘Œ. \nd-separation. In a causal graph ğº, we say that two sets of nodes ğ‘‹ and ğ‘Œ are d-separated by a third set of nodes ğ’, \nwhere ğ‘‹, ğ‘Œ and ğ’ are pairwise disjoint, if ğ’ block all the path between nodes in ğ‘‹ and ğ‘Œ [3].  \nIntervention. Simply put, intervening on node ğ‘‹ is to remove all edges pointing to ğ‘‹ on the causal graph ğº. The \nintervention distribution can be expressed as ğ‘ƒ(ğ‘Œ|ğ‘‘ğ‘œ(ğ‘‹= ğ‘¥)), which means assign ğ‘‹= ğ‘¥. \nBack-door adjustment. In the causal graph corresponding to the SCM, there is a pair of ordered variables (ğ‘‹, ğ‘Œ). If \nthe variable set ğ‘ satisfies the condition that there is no descendant node of ğ‘‹ in ğ‘, and ğ‘ blocks every path between ğ‘‹ \nand ğ‘Œ pointing to the ğ‘‹ path, then ğ‘ is said to satisfy the backdoor criterion [1] about (ğ‘‹, ğ‘Œ), as shown in Figure 3 (d). If \nthe variable set ğ‘ satisfies the backdoor criterion of (ğ‘‹, ğ‘Œ), then the causal effect of ğ‘‹ on ğ‘Œ can be calculated using the \nfollowing formula: \n6 \n \nFigure 3: Three basic DAGs, a simple structure causal model, and two adjustment criteria. \nğ‘ƒ(ğ‘Œ= ğ‘¦|ğ‘‘ğ‘œ(ğ‘‹= ğ‘¥)) = âˆ‘ğ‘ƒ(ğ‘Œ= ğ‘¦|ğ‘‹= ğ‘¥, ğ‘= ğ‘§)ğ‘ƒ(ğ‘= ğ‘§)\nğ‘§\n.\n(1) \nFront-door adjustment. As shown in Figure 3 (f), variable set ğ‘ is said to satisfy the front door criterion [1] for an \nordered variable pair (ğ‘‹, ğ‘Œ), if the following conditions are satisfied: 1) ğ‘§ cuts off all directed paths from ğ‘‹ to ğ‘Œ; 2) ğ‘‹ to \nğ‘ has no backdoor paths; and 3) all ğ‘ backdoor paths to ğ‘Œ are blocked by ğ‘‹. If ğ‘ satisfies the front door criterion for the \nvariable pair (ğ‘‹, ğ‘Œ), and ğ‘ƒ(ğ‘¥, ğ‘§) > 0, then the causal effect of ğ‘‹ on ğ‘Œ is identifiable and is calculated by: \nğ‘ƒ(ğ‘Œ= ğ‘¦|ğ‘‘ğ‘œ(ğ‘‹= ğ‘¥)) = âˆ‘ğ‘ƒ(ğ‘§|ğ‘¥)\nğ‘§\nâˆ‘ğ‘ƒ(ğ‘¦|ğ‘¥â€², ğ‘§)ğ‘ƒ(ğ‘¥â€²)\nğ‘¥â€²\n.\n(2) \nCausal sufficiency. For a pair of observed variables ğ‘‡ and ğ‘Œ, all their common cause must be observed in the data, \nwhich means all common cause modeled in the causal graph ğº [31].  \n2.2 Causal Discovery \nUsing observational data to discover causal relationships between variables is a fundamental and important problem. \nCausal relations among variables are usually described using a DAG, with the nodes representing variables and the edges \nindicating probabilistic relations among them. During causal discovery, the causal Markov conditions and faithfulness \nassumptions are often needed.  \nCausal Markov condition. In the graph ğº of SCM, variables are conditionally independent given their cause variables \n[1]. \nFaithfulness. Any conditional independencies in the joint distribution are implied by the graph ğº, according to the d-\nseparation properties [31].  \nOnly when both the Causal Markov condition and the Faithfulness assumption are satisfied, the DAG ğº can be \nregarded as a causal graph. The traditional causal discovery methods typically include constraint-based methods [31], \nscore-based methods [54], function-based methods [55-56] and other classes of methods [57-59]. Constraint-based \nmethods infer the causal graph by analyzing conditional independence in the data. Classical algorithms include PC and \nFCI [31]. The score-based methods search through the space of all possible DAGs representing the causal structure based \non some form of scoring function for network structures. The classic scoring method for combinatorial optimization is \n7 \nGES [54], and the classic scoring method for continuous optimization is NoTears [60]. The function-based method \nrequires assumptions about the data generation mechanism between variables and then the causal direction is judged \nthrough the asymmetry of the residuals. Essential algorithms include LiNGAM [55] and ANM [56]. For temporal causal \ndiscovery, in addition to the above methods, there are also methods based on Granger causality [11, 48, 61]. \n2.3 Causal Inference \nTo calculate various causal effects in the SCM framework, we must understand three forms of data corresponding to the \nPearlâ€™s causal hierarchy (PCH) proposed by Judea Pearl [1, 62]: observation data, intervention data and counterfactual \ndata. Observational data represent passive collection without any intervention. Causal effects cannot be calculated by \nrelying solely on observational data without making any assumptions. Intervention refers to changing the value or \ndistribution of a few variables, and the average treatment effect (ATE) may be calculated. Counterfactual data are \nunavailable in the real world. However, under various assumptions, individual treatment effects (ITE) can be calculated \nwith the help of counterfactual theory. The capabilities of the three models were discussed in more detail in previous \nwork [3]. \nCalculating causal effects is the core of causal inference. The average treatment effect (ATE) under the SCM \nframework is a common indicator to measure causal effects. It is defined as follows: \nğ´ğ‘‡ğ¸= ğ¸[ğ‘Œ|ğ‘‘ğ‘œ(ğ‘¡= 1)] âˆ’ğ¸[ğ‘Œ|ğ‘‘ğ‘œ(ğ‘¡= 0)].\n(3) \nAs shown in Equation (3), the key to calculating the causal effect is to calculate the probability under the intervention. \nATE may not capture differences in causal effects across subpopulations, which is overcome by the conditional average \ntreatment effect (CATE): \nğ¶ğ´ğ‘‡ğ¸= ğ¸[ğ‘Œ|ğ‘‘ğ‘œ(ğ‘¡= 1), ğ‘‹= ğ‘¥] âˆ’ğ¸[ğ‘Œ|ğ‘‘ğ‘œ(ğ‘¡= 0), ğ‘‹= ğ‘¥].\n(4) \nIn the SCM framework, individual treatment effects (ITE) are generally not discussed, but in fact it is a special case \nof CATE. Here we borrow the POM (Potential Outcome Framework) [7, 18] to express the individual treatment effects \n(SCM and POM are equivalent in principle, but only from a different perspective [3]): \nğ¼ğ‘‡ğ¸= ğ‘Œğ‘–(t = 1) âˆ’ğ‘Œğ‘–(t = 0)\n(5) \n3 CAUSAL REPRESENTATION LEARNING \nCausal discovery and causal inference always adhering to a basic assumption: the causal variables under study should be \nidentified and available at the beginning of the analysis. However, sometimes we cannot directly obtain low-dimensional, \nhigh-level, meaningful causal variables, but high-dimensional, low-level non-causal variables, such as words in text, \npixels in images and videos. Therefore, we need to transform unstructured variables into structured variables for causal \ndiscovery (CD) and causal inference (CI), a process known as Causal Representation Learning (CRL). In other words, \nCRL is not a necessary prerequisite for CD and CI, but a countermeasure when dealing with unstructured data. \nIn fact, the original intention behind developing CRL is to deal with the generalization and interpretability problems \nin representation learning [24]. Variables' causal representations are obtained as intermediate products in this process, \nenabling the handling of unstructured data issues in causal discovery and causal inference. This section discusses only \nthose causal representation learning methods that explicitly generate causal representations. \n8 \nGenerally speaking, in order to identify causal representations, the distribution of the data must be altered to find an \ninvariant model within it that can determine causality. Based on the source of the distribution change, we categorize the \nexisting methods into two types: observational data, where domain changes cause distribution shifts, and intervention \ndata, where interventions cause distribution shifts. \n3.1 Causal Representation Learning using Observational data  \nThe starting point of most causal representation learning is to obtain causal representations, thereby improving the \ngeneralization and interpretability of the model, supporting intervention, reasoning, and planning. Many studies have \nachieved this goal from different perspectives. This section will introduce how different methods can obtain causal \nrepresentation from the perspectives of disentanglement and invariance. \nDisentanglement perspective. IM [63] is a modular system that autonomously discerns independent mechanisms \nfrom unlabeled, shifted data. It can be adapted to various scenarios without prior knowledge of the number of underlying \nmechanisms. Using a multi-expert approach, it leverages competition among smaller neural networks to uncover causal \nmodels. During training, only the winning expert updates its weights, while others remain static. IRS [64] introduces a \nrobustness metric for assessing disentangled representation learning, viewing disentanglement as a causal process rather \nthan a mere heuristic feature in data generation. CausalVAE [65] assumes linear causality, utilizing a causal layer to \ntransform independent exogenous variables into causally significant endogenous ones. It encodes observed variables to \ngenerate exogenous inputs, which are then processed by the causal layer for endogenous causal impact. GEASS [66] \nemploys a neural network to detect sparse features with Granger causality, ensuring causal feature recovery through \nmaximizing modified transfer entropy with sparsity. This approach uses a stochastic gate layer for sparse feature subset \nselection, applicable to high-dimensional data. CBCD [67] uses VAE to derive binary causal variables from unstructured \ndata, explaining classifier outcomes. It's a partial causal discovery technique, focusing on identifying causal variables \nwithout exploring their interrelationships. Yao et al. [68] developed a nonparametric causal process identifiability theory, \nexploring how distribution shifts can enhance decoupling. The framework assumes reversible nonlinear mixing and \ndecomposes distribution shifts under both constant and variable causal relationships. It recovers time-delayed causal \nvariables and identifies their relationships from time series data in stationary environments with distribution shifts. LEAP \n[69] targets the recovery and relationship determination of latent causal variables with time lags from time series data. It \nintroduces two verifiable conditions for temporal causal process identification from nonlinear mixtures and lays a \ntheoretical groundwork for estimating latent temporal causality. \nInvariance perspective. MT-CRL [70] seeks to capture multi-task knowledge with decoupled neural modules and \nrobust task-routing through invariance-based regularization. The modules, realized via a multi-gated Mixture-of-Experts, \nfocus on distinct data aspects. A dual-layer optimization approach is employed: the outer layer refines the encoder and \nrouting, while the inner layer enhances task-specific predictors. This enables shared representation learning alongside \ntask-specific accuracy. IM-DRCFR [71] employs mutual information minimization for learning counterfactual regression \nof disentangled representations, ensuring their independence. EDNIL [72] features a dual-module multi-head network, \nwith an environment inference model to infer labels and mitigate bias, and an invariant learning model for identifying \ncross-environment invariant features. Kong et al. [73] introduced iMSDA, a domain adaptation framework using a \nvariational autoencoder to learn separable representations with an invariant component serving as a causal representation. \nDCDAN [74] acquires domain-invariant causal representations by simulating target domain samples and reweighting \nsource samples to focus on causal effects over correlations. iCaRL [75] builds invariant predictors by identifying latent \n9 \nvariables with NF-iVAE (non-factored identifiable variational autoencoder), discovering causal graphs, and learning \nbased on these direct causes within a diverse training setup.  \nMultimodal data involves concurrent observations from related sources like images and their captions. Imant et al. \n[76] explored identifiability in multimodal contrastive learning, revealing that it can block-identify shared latent factors \nacross modalities without pinpointing individuals. LLCP [77], a temporal multivariate model, leverages causal \nmodularity for local processes, enabling root cause identification and counterfactual outcome prediction. It comprises an \nencoder for latent space mapping, a decoder for variable generation, and a module for causal dynamics modeling.  \n3.2 Causal Representation Learning using Intervention data \nCompared with the natural distribution shift caused by domain changes in observed data, the information about \ndistribution changes induced by intervention is clearer and more conducive to identifying invariant causal representations. \nDIR [78] generates interventional distributions to identify stable causal rationales, separating causal from non-causal \nsubgraphs and intervening to discern invariant causal elements. ILCMs [79] employ a weakly supervised method with \nunlabeled paired samples, utilizing variational autoencoders to model causal variables and structures without explicit \ngraph optimization. Sorawit et al.â€™s Rep4Ex [80] leverages autoencoders for invariant latent mappings and control \nfunctions to estimate intervention effects, capturing latent causal representations. \nCITRIS [81] targets causal representation learning from image time series potentially influenced by multidimensional \ncausal factors, such as 3D object interactions. It employs normalization flows to disentangle representations from a pre-\ntrained autoencoder. iCITRIS [82] extends this by accommodating immediate effects in observable interventions, \nidentifying latent multidimensional causal variables and their graphs through differential causal discovery methods. It's \nbuilt on a variational autoencoder framework with convolutional encoders and decoders for reversible data mapping, \nenabling distinct latent space representations for each causal variable. \n4 DEEP CAUSAL DISCOVERY \nThis section introduces causal discovery methods based on deep neural networks. Traditional causal discovery methods \nare combinatorial optimization problems, and the technique commonly used in training NNs is continuous optimization \n[83-84]. In recent years, numerous methods using neural networks for causal discovery have emerged, and the field of \ncausal discovery has been expanded and supplemented from different angles. Generally speaking, deep causal discovery \ncan be discussed from two perspectives: independent and identically distributed data (i.i.d.) and time series data. \n4.1 Causal Discovery from I.I.D. Data \nThis subsection introduces methods for causal discovery on observational i.i.d. data using deep neural networks. These \nmethods usually convert the constraints of acyclicity into constraints on the trace of the adjacency matrix. NOTEARS \n[60] was the first to transform causal discovery from a combinatorial optimization problem to a continuous optimization \nproblem, setting the stage for the subsequent introduction of neural networks. Specifically, the previous causal discovery \nmethods usually require the obtained causal graph to be a DAG. However, with the increase in the number of nodes, the \ncomplexity of the combinatorial optimization problem increases very rapidly, and the speed of solving the problem is \ngreatly reduced, which seriously limits the solvability size of the problem. The transformation of the form is as follows. \nIn Equation (6), ğ´ represents the adjacency matrix of the causal graph, ğ‘‘ represents the number of nodes, and â„(âˆ™) is a \nsmooth function over real matrices. This derivation is concise and powerful but also ingenious. Most of the subsequent \n10 \ngradient-based methods are extensions of this. Note that in the formula, the value of â„(ğ´) is usually small but not 0, so \nthe setting of the threshold is required in most cases. \nğº(ğ´) âˆˆğ·ğ´ğºğ‘ â‡”â„(ğ´) = 0 â‡” ğ‘¡ğ‘Ÿ(ğ‘’ğ´Â°ğ´) âˆ’ğ‘‘= 0.\n(6) \nNOTEARS has a good performance under the assumption of the SEM, and many subsequent works have extended it \nto the nonlinear field. DAG-GNN [85] integrates neural networks into causal discovery for nonlinear scenarios, using \nencoder-decoder architecture to derive adjacency matrices. Following the idea of encoder-decoder, GAE [86] utilizes the \nidea of graph self-encoder to better utilize graph structure information for causal graph structure discovery. Another \nmethod that uses neural networks to adapt to nonlinear scenarios is GraN-DAG [87]. It can handle the parameter families \nof various conditional probability distributions. The idea behind it is similar to that of DAG-GNN, but it achieves better \nresults in experiments. DAG-NoCurl [88] combines GNN with the augmented Lagrangian method for nonlinear SEMs. \nDCD-FG [89] employs a Gaussian nonlinear low-rank SEM model with MLPs for factor nodes, using Gumbel-softmax \nsampling to prevent self-loops. A new indicator based on the reconstruction error of the autoencoder is proposed in AEQ \n[90]. Different indicator values are used to distinguish the causal directions, and the identification effect is better under \nunivariate conditions. CASTLE [91] uses the adjacency matrix of the DAG is learned in the process of continuous \noptimization and embeds it into the input layer of the FNN.  \nAt the same time, there are a large number of deep generative models used for causal discovery, modeling complex \nrelationships between variables. CAREFL [92] delves into the link between normalizing flows and causality, connecting \nautoregressive flows with identifiable causal models and introducing a likelihood-based causal direction metric for \nmultivariate data. SGM [93] performs generative modeling through data distribution gradient estimation using standard \nReLU neural networks. DiffAN [94] employs diffusion probability models to learn score functions for causal graph \ntopological sorting, offering efficiency over DAG space searches and scalability for large datasets with many variables \nand samples.  \nIntervention data. Intervention data reveal more information about data-generating mechanisms and allow us to obtain \na more accurate causal graph. CAN [95] generates diverse samples from conditional and interventional distributions, \nusing mask vectors for intervention selection. However, it does not guarantee finding true causal graphs due to potential \ndata assumption mismatches. CORE [96] treats causal discovery as a Markov decision process, using a Deep Q Network \nfor structural updates and interventions in a state space of causal models. ENCO [97] sidesteps acyclicity constraints by \noptimizing independent edge likelihoods as an edge direction parameter problem, scaling to larger graphs with gradient \nestimators. It employs neural networks to model conditional distributions, scores graphs on intervention data, and \noptimizes graph parameters with an objective function combining log-likelihood and edge probability regularization.  \nInterventions may also be unknown, which means not known which variables were intervened. A new class of \napproaches is needed to deal with this situation. SDI [34] facilitates concurrent discovery of causal diagrams and \nstructural equations amidst unknown interventions. It acknowledges the interdependence of structural and functional \nparameters, training the DAG's structural representation and independent causal mechanisms' functional representation in \ntandem. The method begins with parameterization, distinguishing between structural (adjacency matrix) and functional \n(M functions) parameters. A multilayer perceptron refines these parameters against observed data. The graph score, \nderived from intervention data, incorporates a cyclicality penalty.  \nHidden confounders. In practice, we do not necessarily have access to all variables because not all variables \ncontribute to causal discovery. However, when confounding variables are not observed, causal discovery will face \nvarious challenges. One of the most serious problems is that the unobserved confounders can lead to an observed \n11 \nassociation between two variables that do not have a causal relationship between them. In traditional methods, there are \nsome methods to deal with unobserved confounders, such as FCI. However, most of them are based on combinatorial \noptimization [98], which is not efficient enough. Here, we introduce some causal discovery methods based on deep \nneural networks, which can more efficiently and accurately find causal relationships in the presence of unobserved \nconfounders. In CGNN [99], the prior form of the function is not set and MMD is used as a metric to calculate the score \nof each graph to evaluate how well each causal graph fits the observed data. The most important contribution of this \nmethod is the formal definition of a functional causal model with latent variables; an exogenous variable ğ‘ˆğ‘–ğ‘— that affects \nvariables ğ‘– and ğ‘— is set to solve with the presence of unobserved confounders and proved that it is still possible to learn \nwith backpropagation. The maximum mean discrepancy (MMD) is defined as shown in Equation (7), and ğ‘˜(âˆ™) is the \nGaussian kernel: \nğ‘€ğ‘€ğ·\nÌ‚ ğ‘˜(ğ·, ğ·Ì‚) = 1\nğ‘›2 âˆ‘ğ‘˜(ğ‘¥ğ‘–, ğ‘¥ğ‘—) + 1\nğ‘›2 âˆ‘ğ‘˜(ğ‘¥Ì‚ğ‘–, ğ‘¥Ì‚ğ‘—) âˆ’\nğ‘›\nğ‘–,ğ‘—=1\n2\nğ‘›2 âˆ‘ğ‘˜(ğ‘¥ğ‘–, ğ‘¥Ì‚ğ‘—)\nğ‘›\nğ‘–,ğ‘—=1\nğ‘›\nğ‘–,ğ‘—=1\n.\n(7) \nSAM [100] solves the computational limitation of CGNN. Its equation differs from structural equations in that it \nincorporates all variables other than itself, hence it is termed \"structural agnostic.\" In SAM, noise matrices instead of the \nnoise variable of variable pairs. The introduction of differentiable parameters in the correlation matrix allows the SAM \nstructure to efficiently and automatically exploit the relation between variables, thus providing SAM with a way to \ndiscover correlations from unobserved variables. \nğ‘‹ğ‘—= ğ‘“Ì‚ğ‘—(ğ‘‹, ğ‘ˆğ‘—) = âˆ‘ğ‘šğ‘—,ğ‘˜âˆ…ğ‘—,ğ‘˜(ğ‘‹, ğ‘ˆğ‘—)ğ‘§ğ‘—,ğ‘˜+ ğ‘šğ‘—,0\nğ‘›â„\nğ‘˜=1\n,\n(8) \nwhere ğ‘“Ì‚ğ‘— is the nonlinear function, âˆ…ğ‘—,ğ‘˜ is the feature, ğ‘§ğ‘—,ğ‘˜ is the Boolean vector and ğ¸ğ‘— is the noise variable. \nICL [101] addresses hidden variables by iteratively imputing missing data, utilizing GAN for interpolation and VAE \nfor causal structure learning to uncover joint distributions and causality.  NOCADILAC [102] employs a nonlinear \ncausal model with observed and unobserved variables, distinguishing between direct causal effects and confounder \ninfluences, using VAE for parameter learning and causal graph construction. N-ADMG [103] identifies potential \nconfounding under acyclic directed mixed graph assumptions and nonlinear noise models, establishing ADMG \nidentifiability conditions and proposing an autoregressive flow-based neural model for learning. MissDAG [104], \ngrounded in the EM framework, maximizes observed data likelihood for causal graph and parameter estimation, \nintegrating diverse causal discovery methods and adeptly managing various ANMs. Table 2 shows all the mentioned \ndeep causal discovery methods from i.i.d data. \n4.2 Causal Discovery from Time Series Data \nCausal discovery in time series is the key to many fields in science [105]. In addition to common problems such as \nhidden confounders, time series data has its own unique challenges, such as non-stationary state, instantaneous causal \neffect, etc. [48, 106-107]. There are three types of causal graphs for time series: full time graphs, window causal graphs \nand summary causal graphs. Full time graphs depict the causal relationship between variables at each moment, window \ncausal graph is simplified in units of maximum delay and summary causal graphs depict the causal relationship during \nthis time. Therefore, the summary causal graph obtained from the time series is likely to have cycles, which does not \nsatisfy the DAG condition. \n12 \nTable 2: Deep causal discovery methods from i.i.d data. \nMethod \nYear \nNNs \nHid. Con. \nInter. \nOutput \nCGNN [99] \n2017 \nVAE \nY \nY \nDAG \nDAG-GNN [85] \n2019 \nVAE, GNN \nN \nN \nDAG \nGAE [86] \n2019 \nGAE \nN \nN \nDAG \nGran-DAG [87] \n2019 \nMLP \nN \nN \nDAG \nSDI [34] \n2019 \nMLP \nN \nY \nDAG \nSAM [100] \n2019 \nMLP \nY \nN \nDAG \nAEQ [90] \n2020 \nAE \nN \nN \nPair \nCASTLE [91] \n2020 \nAE \nN \nN \nDAG \nCAREFL [92] \n2020 \nMLP \nN \nY \nDAG \nICL [101] \n2020 \nGAN, VAE \nY \nY \nDAG \nCAN [95] \n2020 \nGAN \nN \nY \nDAG \nDAG-NoCurl [88] \n2021 \nGNN, MLP \nN \nN \nDAG \nENCO [97] \n2021 \nMLP \nN \nY \nDAG \nDiffAN [94] \n2022 \nMLP \nN \nN \nDAG \nDCD-FG [89] \n2022 \nMLP \nN \nY \nf-DAG \nMissDAG [104] \n2022 \nMLP \nY \nN \nDAG \nSGM [93] \n2023 \nMLP \nN \nN \nDAG \nN-ADMG [103] \n2023 \nNAF \nY \nY \nADMG \nNOCADILAC [102] \n2023 \nVAE \nY \nN \nDAG \nCORE [96] \n2024 \nDQN, MLP \nN \nY \nDAG \nTable 2. â€˜Hid. Con.â€™ indicates if there are hidden confounders. â€˜Inter.â€™ indicates if there are intervention data. \nThe most common method for discovering causal relationships in time series is the Granger causal (GC) [108]. \nAlthough the GC can achieve common-sense results under linear assumptions, the results of the GC in nonlinear \nscenarios are often unsatisfactory. There are many variations of GC-based methods that address the nonlinear problem \n[61, 109-110]. NGC [61] separates the functional representation of each variable to achieve an effective distinction \nbetween cause and effect to a certain extent. NGC provides a neural network for each variable ğ‘– to calculate the influence \nof other variables on it. If a column of the obtained weight matrix is 0, it means that the corresponding variable has no \nGranger causality to the variable ğ‘–. The core of the NGC is to design a structured sparsity-induced penalty to achieve \nregularization so that the Granger causality and the corresponding time delay ğ‘¡ can be selected at the same time: \nğ‘šğ‘–ğ‘›ğ‘Šâˆ‘(ğ‘¥ğ‘–ğ‘¡âˆ’ğ‘”ğ‘–(ğ‘¥(ğ‘¡âˆ’1):(ğ‘¡âˆ’ğ¾)))\n2\n+ ğœ†âˆ‘ğ›º(ğ‘Š:ğ‘—\n1)\nğ‘ƒ\nğ‘—=1\nğ‘‡\nğ‘¡=ğ¾\n,\n(9) \nwhere Î©(â‹…) is the penalty, W is the weights matrix and ğ‘”ğ‘– is the function of the relationships among variables. \nIn addition to NGC, there are many other deep temporal causal discovery methods based on Granger causality. TCDF \n[111] uses an attention-based convolutional neural network combined with a causal verification step to learn the causal \ngraph structure. The architecture of TCDF includes multiple independent attention-based CNNs, each of which uses the \nsame architecture but has different target time series. By interpreting the internal parameters of the convolutional \nnetwork, it is able to discover the time delay between the cause and the effect. NAVAR [112] employs an additive \nframework emphasizing the unique contributions of each input variable to outputs, quantifying and ranking their impacts \nthrough independent nonlinear functions. e-SRU [113] advances time series forecasting with a component-based \napproach using Statistical Recurrent Units for nonlinear dynamics, curbing overfitting by minimizing trainable \nparameters and employing random projections for efficient recursive computations. GVAR [114] presents a self-\ninterpretable neural network framework for Generalised Vector Autoregression, analyzing both the direction and \ntemporal dynamics of GC effects under nonlinear influences. CRVAE [115] features an encoder and multi-head decoder, \nwith each head generating a time series dimension. It promotes sparse weight matrices to encode Granger causality \n13 \nthrough a sparsity penalty, and generates time series by sampling independent noise sets and iterating through the \ndecoder. GrID-Net [116] enhances GNN architecture for Granger causal inference in single-cell data, enabling lagged \nmessage passing on DAGs to accumulate past information and proposing a neural network-tailored formula for \nrecovering nonlinear long-term dependencies. \nIn addition to the ideas based on Granger causality, the scoring-based method is also closely combined with neural \nnetworks. NTS-NOTEARS [117] captures nonlinear and time-lagged relationships with one-dimensional convolutional \nneural networks (1D CNNs), training separate networks to predict target variables from their temporal context, and \nderiving graph structures from the first layer's kernel weights. Rhino [118] combines vector autoregression, deep learning, \nand variational inference to model nonlinear relationships with immediate effects and flexible history-dependent noise. It \nis able to capture nonlinear relationships between variables as well as immediate effects due to slow sampling intervals. \nGANF [119] introduces a flow model for causality, using Bayesian networks to decompose joint probabilities into \nconditional probabilities, with DAG and flow parameters estimated jointly. \nIn many real-world systems, we often encounter a large amount of multivariate time series data collected from \ndifferent individuals, which share some commonalities but also have heterogeneity. Existing methods usually train a \nunique model for each individual, which faces the problems of inefficiency and overfitting. InGRA [120] analyzes \nmultivariate time series data from various individuals to identify common causal patterns and predict future values for \neach person's target variables, while reconstructing unique causal structures for each. Similar ideas, ACD [121] focuses \non the notion that different causal graphs within the same dynamical system share common information, seeking a model \nfor causal discovery across such graphs. It samples from multiple graphs of the same system, treating each as a training \ninstance, and extends the amortized encoder to predict additional variables and account for unobserved confounders \nusing structural knowledge. By extending the amortized encoder, it is possible to predict an additional variable, \ncombined with structural knowledge, which can be used to represent unobserved confounders.  \nIntervention data. CRN [122] tackles causal discovery in dynamic systems by training a learner to sample from \nvaried causal mechanisms, synthesizing intervention data distributions to infer causal graphs. This approach facilitates \ndomain adaptation and accumulates knowledge for structural learning. Each episode begins with a new graph target, \nrandom interventions, and neural network training on the adjacency matrix, enabling structural learning through diverse \ninterventions. IDYNO [123] operates within a continuous optimization framework with acyclicity constraints, suitable \nfor both observational and interventional time series. It extends DYNOTEARS, supports nonlinear objectives via neural \nmodels, and features a modified function for varied intervention target distributions. LIN [124] clusters data into domains, \neach representing a potential intervention area, using neural networks to parameterize variable conditional distributions \nwithin clusters. Parameters are derived from parent node values, and a lower triangular matrix captures conditional \ndistribution similarities across clusters, ensuring identical distributions share the same matrix row values.  \nSampling frequency and continuous data. In the real world, time series data often have random missing or non-\nuniform sampling frequency due to sensor limitations or transmission losses. Apart from that, the observation of time \nseries data is usually continuous, but most existing identifiability results and learning algorithms assume that the \nunderlying dynamics are discrete-time, and few studies explicitly define dependencies in infinitesimal intervals of time, \nwhich poses a challenge to existing causal discovery methods. CUTS [125] focuses on causal discovery from irregular \ntime series, employing a neural Granger causal algorithm within an iterative framework to perform data interpolation and \ngraph construction. It features two complementary modules: the latent data prediction module, which forecasts and \nregisters high-dimensional, complexly distributed irregular data, and the causal graph fitting module, which constructs a \nsparse causal adjacency matrix using the interpolated data. NGM [126] proposes a score-based graphical model learning \n14 \nalgorithm for learning dynamic systems, and proves that for a class of vector fields parameterized by large neural \nnetworks, a directed graph of local independence in a system of stochastic differential equations (SDE) can be \nconsistently recovered through least squares optimization and an adaptive regularization scheme. SCOTCH [127] uses \ncontinuous-time SDE to simulate the dynamic changes of data, which enables it to more accurately capture the inherent \nrandomness of the system, and it can handle irregularly sampled time series data and simulate systems with continuous \nprocesses. Table 3 shows all the mentioned deep causal discovery methods from time series data. \nTable 3: Deep causal discovery methods from time series data. \nMethod \nYear \nNNs \nHid. Con. \nInter. \nInstan. \nOutput graph \nNGC [61] \n2018 \nMLP, LSTM \nN \nN \nN \nSummary \nTCDF [111] \n2019 \nCNN, Attention \nY \nN \nN \nWindow \nNAVAR [112] \n2020 \nMLP, LSTM \nN \nN \nN \nSummary \ne-SRU [113] \n2020 \nSRU \nN \nN \nY \nSummary \nACD [128] \n2020 \nGNN \nY \nN \nN \nSummary \nCRN [122] \n2020 \nLSTM \nN \nN \nY \nSummary \nGVAR [114] \n2021 \nSENN \nN \nN \nY \nSummary \nInGRA [120] \n2021 \nLSTM \nN \nN \nN \nSummary \nNTS-NOTEARS [117] \n2021 \nCNN \nN \nN \nY \nWindow \nGANF [119] \n2022 \nGANF \nN \nN \nY \nSummary \nGrID-Net [116] \n2022 \nGNN \nN \nN \nN \nWindow \nIDYNO [123] \n2022 \nMLP \nN \nY \nY \nWindow \nNGM [126] \n2022 \nCNN \nN \nN \nN \nSummary \nRhino [118] \n2023 \nMLP \nN \nN \nY \nWindow \nCRVAE [115] \n2023 \nRNN, VAE \nN \nN \nY \nSummary \nLIN [124] \n2023 \nMLP \nN \nY \nN \nSummary \nCUTS [125] \n2023 \nDSGNN \nN \nN \nN \nSummary \nSCOTCH [127] \n2024 \nDiffusion \nN \nN \nY \nWindow \nTable 3. â€˜Hid. Con.â€™ indicates if there are hidden confounders. â€˜Inter.â€™ indicates if there are intervention data. â€˜Instan.â€™ indicates if there \nhave instantaneous causal effect. â€˜Output graphâ€™ indicates whether the output is a window causal graph or a summary causal graph.  \n5 DEEP CAUSAL INFERENCE \nMost traditional causal inference methods are applied to the original low-dimensional feature space. When the \ninteractions among covariates are complex, this can result in substantial errors in estimating causal effects. As deep \nlearning has gained popularity, numerous studies have started leveraging the robust fitting capabilities of neural networks \nto investigate the relationships between treatments and outcomes. \nThe core problems of causal inference are the missing counterfactual data and selection bias, as shown in Figure 4. \nThe former is the more fundamental issues of the two, since once counterfactual data are available, the estimation of \ncausal effects becomes very simple and straightforward. Therefore, from the perspective of solving the fundamental \nproblems, the methods of causal inference can be divided into selection bias-oriented and counterfactual data-oriented \nmethods. Further, existing deep learning-based causal inference methods can be roughly divided into three categories: \ncovariate balance-based methods adjust covariates to balance the distribution of covariates in different treatment groups, \nthereby eliminating selection bias; adversarial training-based methods utilize adversarial training to make the \ndiscriminator unable to distinguish between the real data and the data generated by the generator, thereby realizing the \ngeneration of implicit counterfactual data; proxy variable-based methods model the data generation mechanism as the \njoint action between multiple latent variables to achieve explicit counterfactual generation. The relationship between the \ntwo core problems and three main methods is shown in Figure 4. \n15 \n \nFigure 4: The methods to solve the fundamental problems of causal inference using deep learning. The most fundamental problem in \ncausal inference is the missing counterfactual data. Due to the lack of counterfactual data, only observational data can be used to \nestimate causal effects, leading to selection bias. In essence, if the data generation mechanism can be modeled, then the \"counterfactual \ndata\" can be approximated, and the problem of causal inference can be solved. In â€œSelection Biasâ€, gray and white nodes represent \nindividuals with different covariates. In â€œMissing Counterfactual Dataâ€, white and gray nodes represent outcomes under two treatments, \ni.e., fact and counterfactual, and only one of them can be observed. \n5.1 Covariate Balance-based Causal Inference \nIn this subsection, we focus on causal inference methods based on the covariate balance perspective. The core of \ntraditional methods for causal inference is to balance the covariates to estimate causal effects. Neural networks have a \nstrong fitting ability so that through much training, the quantitative relationship between treatment ğ‘‡ and effect ğ‘Œ can be \nfound. This is also due to the strong fitting ability of the neural network; there must be regular terms to balance the \ncovariates ğ‘‹ when used to estimate causal effects. Using neural networks, the counterfactual result (ğ‘¦ğ‘¡=1\nğ¶ğ¹) of the control \ngroup (ğ‘¡= 0) with a certain value of the covariate (ğ‘‹= ğ‘¥) can be predicted, thus obtaining causal effects ğ‘¦ğ‘¡=1\nğ¶ğ¹âˆ’ğ‘¦ğ‘¡=0\nğ¹\n. \nSuch methods typically use neural networks to obtain the representations of covariates or the propensity scores and then \ntrain estimators by minimizing the differences in representations between treatment and control groups. In the loss \nfunction, these balance ideas are usually achieved through regularization terms. The advantage of the neural network is \nthat it can flexibly use various forms of regularization to balance the distribution of covariates to eliminate the influence \nof confounders. At the same time, it has a strong estimation ability and high accuracy. The general architecture of \ncovariates balance-based causal inference methods is usually as shown in Figure 5. \nThe first method to use neural networks to achieve counterfactual predictions was BNN [129], which considers the \nproblem of counterfactual inference from a domain adaptation perspective. The neural network is used to learn the \nrepresentation of covariables, and then match is implemented in the representation space, i.e., selecting the nearest \nobservation effect as its counterfactual effect for training. For example, for a sample ğ‘¥ğ‘– in the treatment group (ğ‘¡= 1), \nthe observation effect of sample ğ‘¥ğ‘— nearest to the covariate representation in the control group (ğ‘¡= 0) is chosen as its \ncounterfactual effect ğ‘¦ğ‘–\nğ¶ğ¹= ğ‘¦ğ‘—(ğ‘–)\nğ¹. During training, each sample has two estimate outcomes â„(ğ›·(ğ‘¥ğ‘–), ğ‘¡ğ‘–) and â„(ğ›·(ğ‘¥ğ‘–), 1 âˆ’\nğ‘¡ğ‘–). In Equation (10), the first term and third term represent the difference between the observed outcome and the  \n16 \n \nFigure 5: The framework of covariates balance-based causal inference. \npredicted outcome. The discrepancy distance ğ‘‘ğ‘–ğ‘ ğ» is used to measure the difference between the two distributions. BNN \nwas also the earliest causal representation learning method to balance latent space. The important contribution of the \nmethod is making a trade-off between balance and accuracy. The optimization objective is as follows: \nğµğ»,ğ›¼,ğ›¾(ğ›·, â„) = 1\nğ‘›âˆ‘|â„(ğ›·(ğ‘¥ğ‘–), ğ‘¡ğ‘–) âˆ’ğ‘¦ğ‘–\nğ¹|\nğ‘›\nğ‘–=1\n+ ğ›¼ğ‘‘ğ‘–ğ‘ ğ»(ğ‘ƒÌ‚ğ›·\nğ¹, ğ‘ƒÌ‚ğ›·\nğ¶ğ¹) + ğ›¾\nğ‘›âˆ‘|â„(ğ›·(ğ‘¥ğ‘–), 1 âˆ’ğ‘¡ğ‘–) âˆ’ğ‘¦ğ‘—(ğ‘–)\nğ¹|\nğ‘›\nğ‘–=1\n,\n(10) \nwhere ğ›· is the learned representation and ğ‘‘ğ‘–ğ‘ ğ»(âˆ™,âˆ™) is the distance measure. By minimizing the loss function equation \n(10), the BNN can simultaneously accomplish counterfactual inference and covariate balance. In the BNN, the network \nstructure only has one head. This means that the treatment assignment information ğ‘¡ğ‘– needs to be concatenated to the \nrepresentation of covariate ğ›·(ğ‘¥ğ‘–). In most cases, ğ›·(ğ‘¥ğ‘–) is high-dimensional, so the information of ğ‘¡ğ‘– might be lost during \ntraining. \nTo address the problem of the BNN, a new architecture was proposed in the CFR [130], which has two separate heads \nrepresenting the control group and the treatment group, which share a representation network. This architecture avoids \nthe loss of treatment variable ğ‘¡ during network training. In the actual training process, according to the value of ğ‘¡ğ‘–, each \nsample is used to update the parameters of the corresponding header. Using the integral probability metric (IPM) [131-\n132] to measure the distance of control and treated distributions ğ‘(ğ‘¥|ğ‘¡= 0) and ğ‘(ğ‘¥|ğ‘¡= 1) was also proposed. In \nBRNN [43], the MSE is decomposed into bias and variance, and the estimation ability of the multi-head model is \ncompared with that of the single-head model. A new regularization term, PRG, was introduced to assess differences \nbetween the treatment and control groups. Here, BWCRF [133] proposed a function called balancing weights to make a \ntrade-off between balance and predictability. It is not a direct balance between groups, instead a weighted balance of \nrepresentations. In CARW [134], the context-aware weighting scheme that leverages the importance sampling technique \nbased on [130] to better solve the selection bias is integrated. FlexTENet [135] as a new multi-task learning architecture, \nit is able to adaptively learn what information is shared between potential outcome functions, and improves the \nestimation of heterogeneous treatment effects by introducing different inductive biases in neural networks. In order to \n17 \nprevent redundancy between shared and private layers and ensure that they encode different information of input features, \nHTCE [136] proposes an orthogonal regularization loss.  \nUsing the neural network to directly fit the relationship of covariate ğ‘‹ to outcome ğ‘Œ can cause many problems. For \nexample, the neural network may use all the variables for ğ‘Œ prediction. In fact, these variables are not needed and should \nnot all be used for estimating causal effects. Covariates can be divided into instrumental variables (only affecting \ntreatment), adjustment variables (only affecting outcome), irrelevant variables (having no effect on either treatment or \noutcome), and confounders (the cause of both treatment and outcome). When learning representations, SCRNet [137] \nonly balances the representations of confounders. It is then concatenated with the representation of the adjustment \nvariable. This approach reduces computational overhead and increases efficiency in practical applications. However, the \ndivision of variables is usually subjective, especially when the true causal graph cannot be obtained. DragonNet [138] is \na method for using neural networks to find those covariates that are associated with treatment and only use these \nvariables to predict the outcome. First, a deep neural network is trained to predict ğ‘‡, and then the last prediction layer is \nremoved to obtain the representation ğ›·. Next, similar to the CFR [130], two separate DNNs are used to predict the \noutcome at ğ‘¡= 0 and ğ‘¡= 1. Essentially, ğ›· stands for the representation related only to ğ‘‡, i.e., represents the propensity \nscore. The objective function is: \nğœƒÌ‚ = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›ğœƒğ‘…Ì‚(ğœƒ; ğ‘‹), ğ‘¤â„ğ‘’ğ‘Ÿğ‘’\n(11) \nğ‘…Ì‚(ğœƒ; ğ‘‹) = 1\nğ‘›âˆ‘[(ğ‘„ğ‘›ğ‘›(ğ‘¡ğ‘–, ğ‘¥ğ‘–; ğœƒ) âˆ’ğ‘¦ğ‘–)2 + ğ›¼ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(ğ‘”ğ‘›ğ‘›(ğ‘¥ğ‘–; ğœƒ), ğ‘¡ğ‘–)] ,\n(12) \nwhere ğ‘„ğ‘›ğ‘›(â‹…) is the DNN to model the outcome and  ğ‘”ğ‘›ğ‘›(â‹…) is the DNN to model the propensity score model. ğ›¼ is the \nhyperparameter to weight the loss term. Simultaneous training to predict propensity scores and outcomes ensures that the \nfeatures used are treatment-relevant. Compared to CFR [130], DragonNet has part of the predicted propensity score. It \nmakes a clear distinction between the prediction of outcomes and estimation of causal effects because accurate \npredictions do not mean that causal effects can be accurately estimated. In DragonNet,  ğ‘”ğ‘›ğ‘›(â‹…) is used to find the \nconfounding factors in the covariates so that the resulting representation contains only the part related to ğ‘‹ (according to \nthe adequacy of the propensity score).  \nDONUT [139] uses neural networks to learn potential outcomes that are orthogonal to the treatment assignments, \ncombining factual loss and orthogonal regularization loss. It adopts the basic architecture of CFR [130] for the outcome \nmodel, and uses logistic regression to estimate the propensity score model. In DCN [140], the architecture of predicting \noutcome is similar to DragonNet, but it has a separate neural network to model the propensity score. For each head of the \npredicted outcome DNN, there is a certain probability of dropout [141-142] at each training, and the probability depends \non the propensity score. This occurs through dropout, implicitly reflecting the balance effect of the propensity score in \nthe neural network. Deep-treat [143] divides the counterfactual prediction problem into two steps: the first step uses an \nautoencoder to learn representations that trade off bias and information loss; then the DNN is used on the transformed \ndata to achieve treatment allocation. RSB-Net [144] tackles selection bias with an autoencoder that learns two distinct \nsets of latent variables: one for selection bias and another for outcome prediction. It employs a loss function regularized \nby the Pearson Correlation Coefficient to encourage decorrelation between these sets, effectively reducing selection bias \nand focusing on variables pertinent to individual treatment effect estimation. The balance of covariates helps to eliminate \nselection bias, but paying too much attention to the balance will affect the counterfactual prediction performance. SITE \n[145] is a balanced representation learning technique that maintains local similarity while ensuring global balance and \npreserving predictive power, thus reducing selection bias. It operates in mini-batches, selecting triplet pairs and relying \n18 \non two main components: PDDM for local similarity preservation and MPDM for balanced latent space distributions. \nAdditionally, a prediction network calculates potential outcome prediction losses. DESCN [146] integrates information \nabout treatment propensity, response, and pseudo treatment effect through an end-to-end multi-task learning approach. \nThis structure not only alleviates the treatment bias and sample imbalance problems, but also the shared network can \nsimultaneously learn the propensity score and the control response with pseudo treatment effects. Causal-StoNet [147] \nintegrates a visible treatment variable within the hidden layer, maintaining computational efficiency. The model adeptly \ndecomposes the joint distribution, capturing conditional probabilities for outcomes, latent factors, treatments, and \ncovariates, thus facilitating precise approximations of both outcome and propensity score functions. \nMultiple treatments. The previous methods are only applicable to 0-1 treatment case. However, in reality, multiple \ntreatments are often required. For example, when treating patients with multiple chronic diseases or systemic diseases, \nmultiple drugs may need to be used simultaneously. PM [148] extends the approach to multiple treatment settings by \nusing the balanced propensity score to make the match and estimating the counterfactual outcomes using the nearest \nneighbor. Here, it use the minibatch level, rather than the dataset level, which can reduce variance. RCFR [149] \nalleviates the bias of BNN [129] when sample sizes are large. It is reweighted according to imbalance terms and variance. \nNCoRE [150] leverages a conditional neural network with treatment interaction regulators to deduce causal generative \nprocesses for various treatment combinations, explicitly modeling cross-interactions through sequential, conditional \nsubnetwork influence. SCP [151] excels in scenarios requiring simultaneous intervention on multiple variables, \nestimating potential outcomes from single-factor changes using observed data. It expands the dataset through sampling \nand prediction to enhance balance and sample size, subsequently estimating multi-factor treatment effects on the \naugmented dataset.  \nContinuous treatments. Estimating the average dose-response function (ADRF) of continuous treatments is an \nimportant problem in fields such as healthcare, public policy, and economics. Existing parametric methods are limited in \nmodel space, and previous approaches to enhance model expressiveness using neural networks rely on splitting the \ncontinuous treatment into chunks and using a separate head for each chunk, which in practice produces discontinuous \nADRFs. DRNet [152] incorporates treatment indicators and dosage parameters into the latent representation through a \nhierarchical structure, using the nearest neighbor similar MISE for model selection without true counterfactuals. It \nevaluates model performance with metrics like RMSE, dose policy error, and policy error to assess dose-response curve \nrecovery and optimal treatment decisions. VCNet [153] is a variable coefficient neural network that enhances model \nexpressiveness and maintains ADRF continuity, employing B-splines to model treatment level changes continuously. To \ntheoretically support the mitigation of selection bias in a continuous processing setting, ADMIT [154] introduces a \ntheoretical framework to mitigate selection bias in continuous settings, deriving a counterfactual loss upper bound using \nreweighting and IPM distance. It trains a reweighting network alongside an inference network by minimizing a combined \nobjective function. CMSM [155] is a new marginal sensitivity model of continuous treatment effects, which is based on \nmodeling a set of conditional distributions of potential outcomes and assuming that there is a certain degree of deviation \nbetween these distributions, which is bounded by parameters. It is applicable to high-dimensional and large-scale \ndatasets. E2B [156] leverages entropy balancing for end-to-end optimized weights to maximize causal inference \naccuracy, using pseudo responses and random functions for known response curve estimation.  \nTime-varying. TSD [157] leverages a time series factor model powered by a recurrent neural network to estimate \ntreatment effects amidst numerous hidden confounders. This model employs multi-task outputs to infer latent variables \nthat render treatment assignments conditionally independent, utilizing these variables for causal inference as proxies for \nthe unobserved confounders. SyncTwin [158] addresses irregularly sampled data, particularly for infrequent treatment \n19 \nchanges. It employs Seq2Seq to learn temporal covariate representations that capture individualized factors impacting \noutcomes. An optimization approach identifies weights to construct synthetic twins from control group contributors. The \npotential outcome is calculated as a weighted average of these contributors' outcomes, with ITE estimated by the \ndifference between the target's outcome and that of their synthetic twin. R-MSN [159] based on the marginal structure \nmodeling framework to learn time-varying treatment responses directly from observational data. It consists of two parts: \none is a series of propensity networks used to calculate the treatment probability; the other is a prediction network used \nto determine the treatment response given a set of planned interventions. DSW [160] harnesses gated recurrent units \n(GRUs) and attention to discern hidden confounders in time-varying contexts for individual treatment effect estimation. \nIt starts by embedding initial features into a lower-dimensional space and then refines these with a GRU to capture \nconfounder representations. The attention mechanism highlights key historical data, while maximum pooling in the RNN \nretains early information as time series expand. A final fully connected layer forecasts treatment assignments across time \npoints. \n5.2 Adversarial Training-based Causal Inference \nWhen there are many covariates and their relationship are particularly complex, sometime the method based on balance \nis ineffective. This is because the feature representation obtained by the neural network is not reasonable due to the \ncomplex relationship among covariates and the limited number of samples. In contrast, the framework based on \nadversarial training can handle this problem very well. Causal inference methods based on adversarial training \nmechanisms utilize generative-adversarial networks to implicitly learn counterfactual distributions. Generators are \ntypically used to generate counterfactual results, and then a discriminator is used to determine whether the input data \ncome from the factual distribution or the generate distribution. The basic architecture of adversarial training-based causal \ninference is shown in Figure 6. It consists of potential outcome generator ğº and potential outcome discriminator ğ·. The \npotential outcome generator ğº generates potential outcomes ğ‘¦Ìƒ based on covariates ğ‘¥, treatments ğ‘¡, and exogenous noise \nğ‘¢; the generated potential outcomes ğ‘¦Ìƒ and factual outcomes ğ‘¦ are then fed into the discriminator ğ·, which tries to \ndistinguish which is the factual outcome and which is the generated outcome. \n \nFigure 6: The framework of adversarial training-based causal inference. \nGANITE [161] uses the adversarial idea to generate the counterfactual outcomes for a given set of covariates and it \ncan deal with multiple treatment situations. It has two blocks: the counterfactual block was used to impute the â€œcompleteâ€ \ndataset, and the ITE block was used to estimate the causal effect. Due to the advantages of the GANITE framework, the \ninformation used for prediction is not lost. At the same time, although there are multiple treatment variables to choose \nfrom, only one of them can be selected to match the actual situation. A drawback of GANITE is that it cannot deal with \ncontinuous-valued interventions. The SCIGAN [162] addresses this problem very well and provides a theoretical \nverification of causal estimation under the GAN framework. However, the disadvantage is that it requires thousands of \nx\nu\nt\nG\ny\ny~\nD\nLoss\nix\niu\nit\nâˆ’\n1\nG\nF\niy\nCF\niy~\nITE\nG\n20 \ntraining samples. MatchGAN [163] maps the original samples into the GAN latent space and selects the samples of \ndifferent categories with the closest distance to enable removal of bias while preserving critical features. CF-Net [164] \npresents an end-to-end framework for extracting confounder-invariant features that respect the inherent correlation \nbetween confounders and outcomes. Its CP module effectively removes confounder influence in feature extraction, \nshowing promise in medical applications. NCMs [165] establish that neural causal models can capture the structural \nconstraints for counterfactual reasoning and propose an algorithm for joint identification and estimation of counterfactual \ndistributions. They introduce a GAN-based approach for robust inference in high-dimensional settings. There is also a \nclass of methods that consider time-related confounding factors, which can help to understand how results change over \ntime. The previously mentioned static-based methods are often not directly applicable to time series scenarios. CRN [166] \nutilizes a recurrent neural network with adversarial training to generate time-invariant feature representations, effectively \nneutralizing time-related confounders. This method is well-suited for precision medicine, guiding critical decisions on \ntreatment timing, cessation, and dosage determination. \nThere are also approaches that combine adversarial and balanced approaches, i.e., using adversarial training to \nproduce covariate-balanced representations [167]. DeepMatch [168] uses adversarial training to balance covariates in \nsuch situations. It uses the discriminative discrepancy metric in the context of NNs and requires a few further \ndevelopments of alternating gradient approaches similar to GAN. ABCEI [167] employs mutual information to gauge the \nloss during the covariate-to-latent representation transformation, optimizing this measure with a neural network to retain \nmaximum information. It balances group distributions in the latent space through adversarial training, independent of \ntreatment assignment assumptions. CETransformer [169] leverages the Transformer architecture's self-attention \nmechanism to capture covariate correlations, enhancing feature representation robustness. To address training challenges \nwith limited samples, it uses self-supervised learning with an autoencoder to augment data. Adversarial learning balances \ngroup representations, while a two-branch network predicts outcomes based on features and treatment. CBRE [170] \nintroduces a cyclic balanced representation framework, using adversarial training for robust group balance and \nmaintaining data properties through information loops, reducing information loss in latent space. Wasserstein GANs \nstabilize training, minimizing distribution disparity between groups, with the encoder constrained by a decoder module to \npreserve data integrity. TransTEE [171] offers a versatile Transformer-based framework for diverse data types, including \ntabular and sequences, managing various treatment modalities. It selectively utilizes covariates via attention mechanisms, \nwith multi-head cross-attention for adaptive covariate selection. \n5.3 Proxy Variable-based Causal Inference \nFor causal inference, there is generally an â€œunconfoundernessâ€ assumption, meaning that there are no unobserved \nconfounders. Most existing methods of estimating causal effects are based on this assumption, include traditional \nmethods or the balance-based methods mentioned earlier. However, in many cases, the assumptions are not satisfied. \nOnce there are unobserved confounders, the previous method will have large bias and even lead to incorrect conclusions. \nRecent proximal inference provides an alternative approach that can identify causal effects even in the presence of \nunobserved confounders as long as a sufficiently rich set of proxy variables is measured. It may not be possible to \nobserve all confounders, there is generally a way to measure the proxy variables of the confounders. Proxy variable-\nbased methods utilize proxy variables to separate different types of variables using disentangle representations. Most \nproxy variables required by the confounders are covered by collecting a large number of observed variables. This is easy \nto achieve in today's era of big data. Exactly how these proxy variables are used depends on their relationship to the \nunobserved confounders, treatments, and outcomes. \n21 \nDeep latent variable techniques can use noisy proxy variables to infer unobserved confounders [172]. CEVAE [173] \nuses latent variable generative models to discover unobserved confounders from the perspective of maximum likelihood. \nOne of the typical scenarios is shown in Figure 7 (a). This approach requires fairly weak assumptions about the data \ngeneration process and the structure of unobserved confounders. It uses the VAE [174-175] architecture and contains \nboth an inference network and a model network. The inference network is equivalent to an encoder, and the model \nnetwork is equivalent to a decoder. The core of this approach is the use of variational inference to obtain the probability \ndistribution needed to estimate causal effects, ğ‘(ğ‘‹, ğ‘, ğ‘¡, ğ‘¦). We use ğ‘ to denote the hidden variable, and ğ‘‹ is the proxy \nvariable that has no effect on outcome ğ‘¦ and treatment ğ‘¡. Actually, ğ‘ can be seen as the latent variable in VAE. The \ninference network was used to obtain ğ‘(ğ‘¡|ğ‘¥) and ğ‘(ğ‘§|ğ‘¡, ğ‘¦, ğ‘¥). ğ‘(ğ‘¡|ğ‘¥) can be seen as the propensity score. The model \nnetwork was used to obtain ğ‘(ğ‘¡|ğ‘§) , ğ‘(ğ‘¥|ğ‘§) and ğ‘(ğ‘¦|ğ‘¡, ğ‘§). Then, the framework is used to model the generation \nmechanism when unobserved confounders exist: \nğ¿= âˆ‘ğ¸ğ‘(ğ‘§ğ‘–|ğ‘¥ğ‘–, ğ‘¡ğ‘–, ğ‘¦ğ‘–)[ğ‘™ğ‘œğ‘”ğ‘(ğ‘¥ğ‘–, ğ‘¡ğ‘–|ğ‘§ğ‘–) + ğ‘™ğ‘œğ‘”ğ‘(ğ‘¦ğ‘–|ğ‘¡ğ‘–, ğ‘§ğ‘–) + ğ‘™ğ‘œğ‘”ğ‘(ğ‘§ğ‘–) âˆ’ğ‘™ğ‘œğ‘”ğ‘(ğ‘§ğ‘–|ğ‘¥ğ‘–, ğ‘¡ğ‘–, ğ‘¦ğ‘–)]\nğ‘\nğ‘–=1\n,\n(13) \nğ¹ğ¶ğ¸ğ‘‰ğ´ğ¸= ğ¿+ âˆ‘(ğ‘™ğ‘œğ‘”ğ‘(ğ‘¡ğ‘–= ğ‘¡ğ‘–\nâˆ—|ğ‘¥ğ‘–\nâˆ—) + ğ‘™ğ‘œğ‘”ğ‘(ğ‘¦ğ‘–= ğ‘¦ğ‘–\nâˆ—|ğ‘¥ğ‘–\nâˆ—, ğ‘¡ğ‘–\nâˆ—))\nğ‘–=1\n,\n(14) \nIn practice, to better estimate the distribution of parameters, two terms are added to the lower variable boundary as \nğ¹ğ¶ğ¸ğ‘‰ğ´ğ¸. ğ‘¦ğ‘–\nâˆ—, ğ‘¥ğ‘–\nâˆ—, ğ‘¡ğ‘–\nâˆ— are all the input observed values.  \n \nFigure 7: Typical scenarios for proxy variable-based causal inference methods. Gray nodes represent unknown variables or variables \nthat cannot be observed. \nIn causal effect estimation, an excess of control variables can weaken estimation power, while including non-essential \nones may lead to suboptimal nonparametric estimation. High-dimensional scenarios often involve non-confounders that \nshould be omitted to avoid exacerbating bias and variance, thus diminishing estimation precision. The challenge lies in \nstriking a balance: too many variables can inflate bias and variance, while too few might overlook confounders, inducing \nselection bias. To solve this problem, TEDVAE [176] divides covariates into three categories: confounders, instrumental \nfactors and risk factors. Confounders affect both cause and effect, instrumental factors only affect the cause, and risk \nfactors only affect the effect. The rest of the architecture is similar to CEVAE and can also be used for continuous \ntreatment variables. TVAE [172] improved TEDVAE by combining the method of target learning and maximum \nlikelihood estimation training. Since the causal graph used may vary when making causal inferences, the errors brought \nby different methods are different, TVAE tends to have a smaller error even though the causal graph is wrong. The \npurpose of introducing targeted regularization is to make the outcome ğ‘¦ and the treatment assignment  ğ‘¡ as independent \nas possible. TVAE can be seen as the combination of DragonNet and TEDVAE. \n22 \nTable 4: Deep causal inference methods. \nType \nMethod \nYear \nNetwork \nTreatment \nHid. Con. \nTarget \nBalance \nBNN [129] \n2016 \nMLP \nB \nN \nITE \nCFR [130] \n2016 \nMLP \nB \nN \nITE \nDCN [140] \n2017 \nMLP \nB \nN \nITE, ATE \nSITE [145] \n2018 \nMLP \nB \nN \nITE \nDeep-treat [143] \n2018 \nAE, MLP \nB \nN \nITE \nCARW [134] \n2019 \nMLP \nB \nN \nITE \nRSB-Net [144] \n2019 \nAE \nB \nN \nITE \nDragonNet [138] \n2019 \nMLP \nB \nN \nATE \nBRNN [43] \n2020 \nMLP \nB \nN \nITE \nBWCRF [133] \n2020 \nAE \nB \nN \nCATE \nSCRNet [137] \n2020 \nMLP \nB \nN \nITE \nDONUT [139] \n2021 \nMLP \nB \nN \nATE \nFlexTENet [135] \n2021 \nMLP \nB \nN \nCATE \nHTCE [136] \n2022 \nMLP \nB \nN \nCATE \nDESCN [146] \n2022 \nMLP \nB \nN \nITE \nCausal-StoNet [147] \n2024 \nMLP \nB \nY \nATE \nPM [148] \n2018 \nMLP \nM \nN \nITE \nRCFR [149] \n2018 \nMLP \nM \nN \nCATE \nNCoRE [150] \n2021 \nMLP \nM \nN \nITE \nSCP [151] \n2021 \nMLP \nM \nN \nCATE \nDRNet [152] \n2019 \nMLP \nC \nN \nITE \nE2B [156] \n2021 \nMLP \nC \nN \nCATE \nVCNet [153] \n2021 \nMLP \nC \nY \nCATE \nADMIT [154] \n2022 \nMLP \nC \nN \nCATE \nCMSM [155] \n2022 \nMLP \nC \nY \nCATE \nR-MSN [159] \n2018 \nLSTM \nT \nN \nATE \nTSD [157] \n2019 \nRNN \nT \nY \nITE \nDSW [160] \n2020 \nGRU, Attention \nT \nY \nITE \nSyncTwin [158] \n2021 \nLSTM \nT \nN \nITE \nAdversarial \nGANITE [161] \n2018 \nGAN \nB \nY \nITE \nDeepMatch [168] \n2018 \nGAN \nB \nN \nATE, CATE \nABCEI [167] \n2019 \nGAN, AE \nB \nN \nCATE \nSCIGAN [162] \n2020 \nGAN \nC \nN \nCATE \nCF-NET [164] \n2020 \nGAN \nB \nN \nFeature \nMatchGAN [163] \n2021 \nGAN \nB \nN \nITE \nCETransformer [169] \n2021 \nGAN, Transformer \nB \nY \nITE, ATE \nCBRE [170] \n2021 \nGAN, AE \nB \nN \nITE \nCRN [166] \n2022 \nRNN, MLP \nB \nY \nITE \nNCM [165] \n2022 \nGAN \nB, M, C \nY \nITE, ATE \nTransTEE [171] \n2022 \nTransformer \nB, M, C \nN \nATE, CATE \nProxy \nCEVAE [173] \n2017 \nVAE \nB \nY \nITE, ATE \nDeepIV [177] \n2017 \nMLP \nB, C \nY \nITE \nDeR-CFR [178] \n2020 \nMLP \nB \nY \nITE \nTEDVAE [176] \n2020 \nVAE \nB \nY \nATE, CATE \nTVAE [172] \n2020 \nVAE \nB \nY \nATE, CATE \nVSR [179] \n2020 \nVAE \nB \nY \nITE \nCVAE-IV [180] \n2022 \nVAE \nC \nY \nITE \nDMSE [181] \n2022 \nMLP \nB \nY \nITE, ATE \nCredence [182] \n2022 \nVAE \nB \nY \nATE \nCFDiVAE [183] \n2023 \nVAE \nB \nY \nATE \nTable 4. â€˜Treatmentâ€™ indicates the type of treatment. â€˜Bâ€™ represents 0-1 variables, â€˜Mâ€™ represents multi-variables, â€˜Câ€™ represents \ncontinuous variables, and â€˜Tâ€™ represents time-varying variables. â€˜Hid. Con.â€™ indicates if there are hidden confounders.  \n \n23 \nVSR [179] introduces a variational sample reweighting technique to eliminate confounding bias by decorrelating \ntreatment and confounders, utilizing a VAE for latent treatment representation and deep neural networks for weight \nestimation. DeR-CFR [178] identifies and balances confounders for treatment effect estimation through decomposition \nrepresentation, employing reweighting and predictive modeling for counterfactual outcomes. CFDiVAE [183] presents \nconditional front-gate adjustment (CFD), with theorems ensuring causal effect identifiability, using deep generative \nmodels to directly learn CFD variables from data with a provably identifiable VAE. DMSE [181] regards confounders as \nlatent variables, employing conditional independence and variational inference to manage missing data during training \nand inference for causal effect estimation. Credence[182] assesses causal inference method performance through \nsynthetic data generation, considering user-defined causal effects and biases, based on VAEs that learn data distributions \nto replicate and evaluate causal inference effectiveness. \n Instrumental variable (IV) [184-185] methods look for proxy latent variables for causal inference instead of finding \nhidden confounders [18]. The IV framework has a long history, especially in economics [186]. The typical scenario using \nthe instrumental variable method is shown in Figure 7 (b). A more powerful instrumental variable approach \nincorporating deep learning is introduced. DeepIV [177] uses instrumental variables and defines a counterfactual error \nfunction to implement neural network-based causal inference in the presence of unobserved confounders. The method \ncan verify the accuracy of the out-of-distribution sample, which is very beneficial and affects the need for \nhyperparameters for neural network tuning. It is implemented in two steps: the first step is to learn the treatment \ndistribution using a neural network: ğ¹Ì‚ = ğ¹Ñ„(ğ‘¡|ğ‘¥, ğ‘§), where ğ‘¥ is the covariate, ğ‘¡ is the treatment variable, and ğ‘§ is the \ninstrumental variable. The second step is to use the outcome network to predict the counterfactual outcomes. The \nobjective function is: \nğ¿(ğ·; ğœƒ) = |ğ·|âˆ’1 âˆ‘(ğ‘¦ğ‘–âˆ’âˆ«â„ğœƒ(ğ‘¡, ğ‘¥ğ‘–)ğ‘‘ğ¹Ì‚Ñ„(ğ‘¡|ğ‘¥ğ‘–, ğ‘§ğ‘–))\n2\nğ‘–\n,\n(15) \nwhere â„ is the prediction function, ğ¹Ì‚Ñ„ is the treatment distribution obtained from the first step, and ğ· is the dataset. \nConfounded instruments violate a key condition in the instrumental variable assumptionâ€”that unmeasured confounders \nand instrumental variables are conditionally independent given observed covariates. To deal with this situation, CVAE-\nIV [180] proposed an alternative hidden confounder by considering conditional independence to meet the strong \nignorability criterion, thereby allowing unbiased estimation of ITE. Table 4 shows all the mentioned deep causal \ninference methods. \n6 NEW FRONTIERS OF CAUSAL LEARNING \nIn addition to the deep causal learning methods previously described, there are several cutting-edge causal learning areas \nthat warrant close attention. These fields have made rapid progress recently and have achieved outstanding performance \nin some tasks. \n6.1 Supervised Causal learning \nExisting causal discovery mainly relies on artificial prior knowledge to determine causal relationships, or uses non-causal \nsupervisory information to learn causal relationships, rather than learning from data with causal supervisory information. \nFor example, the constraint-based method uses conditional independence tests to determine whether there is a causal \nrelationship between variable pairs, which causes a large amount of information in the data that may be related to \ncausality to be omitted, resulting in low accuracy and efficiency; the score-based method uses the minimum \n24 \nreconstruction loss to learn causal relationships, resulting in some correlations being used to determine whether causal \nrelationships exist or not. \nCausal supervisory information has rarely been used in previous causal discovery methods because the causal graph \ncorresponding to real observation data is very difficult to obtain. In the field of computer vision, obtaining supervisory \ninformation may be labeling images; in the field of natural language processing, obtaining supervisory information may \nbe making emotional judgments on sentences; and in the field of causal discovery, obtaining \"causal graphs\" as \nsupervisory information requires a full understanding of the generation mechanism behind the variables involved, so \ncausal graph supervisory information has not been taken seriously in the field of causal discovery. \nRecently, many studies have introduced the paradigm of supervised learning into the field of causal discovery. This \nparadigm is called supervised causal learning. Supervised causal learning synthesizes a large amount of data with real \ncausal graphs to train a model that can identify causal relationships, allowing the model to learn what the real causal \nrelationship is from a large amount of data. ML4S [187] and ML4C [188] use supervised learning to learn skeleton and \nv-structure respectively. Lorch et al. [189] proposed the amortized variational inference model, which uses the inductive \nbias of a specific domain represented by the graph scoring function, uses the synthesized causal graph and the \ncorresponding sampled data for training, and uses continuous optimization to obtain the inference model. The model \narchitecture uses the alternating attention mechanism [190] to permute and equivariant the sample dimension and \nvariable dimension of the dataset respectively, and proves that the supervised causal learning paradigm is generalizable \nand feasible on independent and identically distributed data. Ke et al. [191] proposed the CSIvA model, which also uses \nsynthetic data generated by different causal graphs to train the model, and also learns the mapping from data to graph \nstructure based on the alternating attention mechanism. At the same time, it extends the supervised learning paradigm to \nalso use intervention data, thereby achieving greater flexibility. \nAlthough supervised causal learning methods can define causality more \"objectively\" due to their data-driven form, \nthey need to additionally address the generalization challenge that all supervised methods face, namely the distribution \nbias between training samples and test samples. This will be the main research direction of supervised causal learning in \nthe future. \n6.2 End to End Causal learning \nExisting causal discovery and causal inference methods often develop independently. In order to obtain the causal effect \nbetween variables, it is necessary to have some understanding of the causal relationship between the variables, that is, it \nis based on the causal graph that has been obtained, which leads to the performance of the causal discovery task affecting \nthe performance of the causal inference task. With the in-depth integration of neural networks and causal learning, some \nend-to-end deep causal learning algorithms have recently been developed to achieve simultaneous causal discovery and \ncausal inference based on observational or intervention data. \n By fully combining the deep mechanism with SCM, DSCM [192] can use exogenous variables for counterfactual \ninference via variational inference. Three types of mechanisms combined with SCM have been discussed: explicit \nlikelihood, amortized explicit likelihood, and amortized implicit likelihood. These three mechanisms may need \nvariational inference and normalizing flows to model. DSCM can also finish the three steps of counterfactual inference \ndepicted by Pearl [1], which are abduction, action, and prediction. Toth et al. [127] proposed a new framework called \nActive Bayesian Causal Inference (ABCI), which aims to integrate causal discovery and causal reasoning through active \nlearning. Compared with the traditional two-stage method (first inferring the causal graph and then estimating the causal \neffect of the intervention), ABCI is more efficient, especially in terms of actively collected intervention data. It mainly \n25 \nconsidering hard interventions, and providing corresponding intervention probability calculation methods. ABCI mainly \nconsiders nonlinear additive Gaussian noise models. From a Bayesian perspective, causal queries are regarded as \npotential quantities and posterior inferences are performed, while uncertainty is considered in the process. DECI [193] \nuses additive noise model to describe the causal relationship between variables, where each variable is a function of its \nparent variable and exogenous noise. Based on autoregressive flow, variational inference is used to learn the posterior \ndistribution of the causal graph, which is capable of learning complex nonlinear relationships between variables and non-\nGaussian exogenous noise distributions. The learned generative model is used to evaluate the expectations under the \nintervention distribution, thereby estimating ATE and CATE. At the same time, in order to adapt to real data, the model \nis extended to support mixed type (continuous and discrete) data and the processing of missing values. Zhang et al. [194] \nexplored the duality of causal inference and attention mechanisms in Transformer-based architectures, demonstrated the \nprimal-dual connection between optimal covariate balancing and self-attention, and proved that trained self-attention can \nfind the optimal balancing weights for a given dataset under appropriate self-supervised losses. They propose CInA to \nuse multiple unlabeled datasets for self-supervised causal learning and achieve zero-shot causal inference on unseen \ntasks and new data. The model uses self-attention as its last layer and is trained to learn the optimal covariate balancing \nweights. Once the model is trained on multiple data sources, it can perform zero-shot causal inference by simply \nextracting the key-value tensor of the last layer of the model. \n6.3 Large Language Models for Causal Learning \nWith the recent development of large language models (LLMs), iterative progress has been promoted in various \nscientific fields. Due to its powerful representation ability, understanding of complex data structures, and rich world \nknowledge, LLMs can provide assistance in many aspects of causal learning.  \nThere are now many studies investigating the performance of LLMs on various causal tasks. KÄ±cÄ±man et al. [195] \nconducted an in-depth analysis of LLMs, distinguishing different types of causal reasoning tasks. They found that \nalgorithms based on GPT-3.5 and 4 achieved new state-of-the-art accuracy on multiple causal benchmarks. Specifically, \nthey surpassed existing algorithms on paired causal discovery tasks, counterfactual reasoning tasks (92% accuracy, an \nincrease of 20 percentage points), and actual causality (86% accuracy in determining necessary and sufficient causes in \ncases). Zhang et al. [196] argue that current LLMs can combine existing causal knowledge to answer causal questions \nlike domain experts, but they cannot yet provide satisfactory, precise answers for discovering new knowledge or making \nhigh-stakes decision-making tasks. The possibility of enabling explicit and implicit causal modules and deep causal-\naware LLMs is discussed, which will not only enable LLMs to answer more different types of causal questions, but also \nmake LLMs more trustworthy and efficient in general. Cai et al. [197] demonstrated through experiments that the causal \nreasoning ability of LLMs depends on the context and domain-specific knowledge provided. When appropriate \nknowledge is provided, LLMs exhibit causal reasoning capabilities consistent with human logic and common sense; in \nthe absence of knowledge, LLMs can still use existing numerical data to perform a certain degree of causal reasoning, \nbut there are certain limitations. \nLLMs for causal discovery. Long et al. [198] explored how to use the expert knowledge of LLM to improve data-\ndriven causal discovery, especially how to use the expert knowledge to determine the direction of causal relationships \nbetween variables when experts may provide wrong information. KÄ±cÄ±man et al. [195] found that LLMs outperformed \nexisting data-driven causal discovery algorithms on paired causal tasks. LLMs are able to infer causal relationships by \nanalyzing the metadata of variables (such as natural language descriptions of variable names and problem contexts), \nwhich is a different approach from previous causal discovery algorithms that rely on data values. In full-graph causal \n26 \ndiscovery tasks, LLMs are able to handle more complex data sets and are comparable to domain experts and deep \nlearning-based methods. Thomas [199] proposed a method for causal discovery using LLMs based on breadth-first \nsearch, which can improve the performance of causal discovery tasks under different causal graph sizes by only using \nlinear query times. Ahmed et al. [200] proposed a framework that combines the metadata-based reasoning capabilities of \nlarge language models with data-driven modeling of deep structural causal models for causal discovery tasks. First, a \ncausal graph is generated using the world knowledge of the LLM, and a data-driven approach is used to calculate the \nmodel's fit. Long et al. [201] used four causal graphs representing known medical literature as ground truth. For each \ncausal graph, a loop was traversed over each ordered variable pair and GPT-3 was asked to score both directions \nseparately. The experiment found that the accuracy of GPT-3 in confirming the direction of two variables depends on the \nlanguage used to describe the relationship. At the same time, the update of LLMs and the data they are trained on lag \nbehind the availability of new medical literature and may not be suitable for constructing DAGs for new diseases. Tu et \nal. [202] assessed ChatGPT's causal discovery capabilities in neuropathic pain diagnosis, testing it with 50 true and 50 \nfalse causal relationship pairs from a dataset. They found that while ChatGPT could provide plausible responses, it \nlacked a true grasp of the subject matter, such as the human nervous system. The model exhibited performance \nvariability, offering inconsistent answers to identical questions over time. It struggles with comprehending new concepts \nbeyond its training data, highlighting a need for enhanced consistency and reliability. \nLLMs for causal inference. CURE [203] is based on the Transformer model and uses a pre-training and fine-tuning \napproach to convert structured longitudinal patient data into sequence inputs by temporally aligning all observed \ncovariates. It is pre-trained on large-scale unlabeled patient data, uses unsupervised learning objectives to generate \ncontextualized patient information representations, and fine-tunes the pre-trained model on downstream labeled \ntreatment and outcome data. Jin et al. [204] proposed a new task CORR2CAUSE, created the first benchmark dataset for \ntesting the pure causal reasoning ability of LLMs, containing more than 200K samples, and used this dataset to evaluate \nseventeen existing LLMs. The CORR2CAUSE task requires LLMs to accept a set of correlation statements and \ndetermine the causal relationship between variables. Experiments show that existing LLMs perform poorly on causal \ninference tasks, close to random levels. Even though fine-tuning can improve performance to a certain extent, these \nmodels still cannot generalize well to out-of-distribution settings. Jin et al. [205] created a benchmark dataset called \nCLADDER to explore the causal inference ability of LLMs, which contains 10,000 samples. These samples are based on \na series of causal graphs and queries (including associations, interventions, and counterfactuals), and a prophetic causal \ninference engine generates symbolic questions and real answers, which are then translated into natural language. The \nperformance of multiple LLMs on the CLADDER dataset was evaluated, and a customized thought chain prompt \nstrategy called CAUSALCOT was introduced and evaluated. Through experiments on the CLADDER dataset, the \nauthors found that the most advanced models face significant challenges in causal reasoning tasks.  \n7 CONCLUSION AND FUTURE DIRECTIONS \nDeep learning models show great advantages in various fields, and an increasing number of researchers are trying to \ncombine deep learning with causal learning. It is hoped that the powerful representation and learning abilities of neural \nnetworks can enhance existing causal learning methods in various ways. This article reviews three aspects of the \nimprovements that deep learning brings to causal learning: causal representation learning, deep causal discovery, and \ndeep causal inference. Causal representation learning uses neural networks to learn the representation of causal variables, \nespecially in the case of unstructured high-dimension data. We reviewed deep causal discovery methods based on neural \nnetworks based on different data types. Then, we reviewed causal inference methods using deep learning from three \n27 \nperspectives: covariate balance-based, adversarial training-based, and proxy variable-based methods. Finally, we \nshowcase some of the most advanced areas of causal learning.  \nAlthough deep learning has brought about significant changes to causal learning, there are still many problems that \nmust be addressed. In this section, we pose these questions and included a brief discussion, with the hope of offering \nresearchers some future directions. \nScarcity of causal knowledge and utilizing strong assumptions. Although many of us are devoted to the field of \ncausal learning, as researchers, we must constantly reflect on whether what we think of as causality is truly causality and \nwhether there is a more suitable form of studying causality than the causal graph or potential outcomes. Because of this \nlack of causal knowledge, we rely heavily on untestable assumptions when studying causality. In the causal discovery \nfield, the correctness of the causal Markov condition assumption and faithfulness assumption still needs to be fully \nverified [206]. SUTVA, unconfounderness, and positivity assumptions are required when making causal inference. The \nconditions under which these assumptions apply and fail need to be fully studied (e.g., due to social connections, one \nperson's treatment outcomes may affect another person through social activities.). Most existing methods and \napplications of causal inference are based on the assumption of directed acyclic graphs [207], but in reality, there may be \ncausal feedback between variables, leading to the emergence of cyclic graphs [105]. In addition, different methods are \nbased on different assumptions about the distribution of noise [31, 208]. How to reasonably relax the assumptions while \nensuring the accuracy is a very challenging problem. Although these assumptions are convenient, they also bring many \nrisks, and care must be taken when using them. \nLack of casual benchmarks and suitable metrics. Although there are some commonly used datasets in the fields of \ncausal discovery and causal inference [27, 47], the scale and number of these datasets make it difficult to convince \npeople of the model's capabilities. Moreover, most of the existing datasets are synthetic data, so it is crucial to collect \nmore real datasets and develop high-quality simulators. At the same time, the metrics used to estimate causal effects on \ndifferent datasets remain inconsistent. For example, ğœ–ğ‘ƒğ¸ğ»ğ¸ used in IHDP is only suitable for binary treatment variables, \nwhile the metric ğ‘…ğ‘ğ‘œğ‘™ used in the Jobs dataset is highly targeted and does not have universality. Rich and diverse causal \n\"loss functions\" similar to the current stage of deep learning [209-211] would be very helpful for the rapid improvement \nof the performance of deep causal learning algorithms. \nCombine data-driven with knowledge-driven. Current empirical studies on LLMs have shown that they currently rely \nmainly on the world knowledge behind the metadata to complete causal learning related tasks. It is a very important \ndirection to further verify whether LLMs have true causal inference capabilities and their degree of dependence on \nknowledge and data respectively. At the same time, most of the current causal discovery and causal inference methods \nare data-driven. How to effectively combine these methods with knowledge-driven LLMs depth is also worthy of study. \nCausality for deep learning. In this article, we mainly discussed the changes that deep learning brought to causal \nlearning. At the same time, causal learning is profoundly changing the field of deep learning. Many studies have focused \non how causality can help deep learning address long-standing issues such as interpretability [37-38, 40, 212-213], \ngeneralization [214-216], robustness [217-219], and fairness [220-222]. \nACKNOWLEDGMENTS \nWe thank Xingwei Zhang and Songran Bai for their precious suggestions and comments on this work. We also thank \nHaitao Huang and Gang Zhou for invaluable discussion. This work is supported by the Ministry of Science and \nTechnology of China under Grant No. 2020AAA0108401ï¼Œand the Natural Science Foundation of China under Grant \nNos. 72225011 and 71621002. \n28 \nREFERENCES \n[1] \nJudea Pearl. 2009. Causality. Cambridge university press.  \n[2] \nBernhard SchÃ¶lkopf. 2022. Causality for machine learning. Probabilistic and Causal Inference: The Works of Judea Pearl (2022), 765-804. \n[3] \nJudea Pearl. 2009. Causal inference in statistics: An overview. Statistics Surveys 3 (2009), 96-146. \n[4] \nMd Vasimuddin and Srinivas Aluru. 2017. Parallel Exact Dynamic Bayesian Network Structure Learning with Application to Gene Networks. In \n2017 IEEE 24th International Conference on High Performance Computing (HiPC), Dec 18-21, 2017. Jaipur, India. 42-51. \n[5] \nSofia Triantafillou, Vincenzo Lagani, Christina Heinze-Deml, Angelika Schmidt and Ioannis Tsamardinos. 2017. Predicting Causal Relationships \nfrom Biological Data: Applying Automated Casual Discovery on Mass Cytometry Data of Human Immune Cells. Scientific Reports 7, 1 (2017), 1-12. \n[6] \nSteffen L Lauritzen and David J Spiegelhalter. 1988. Local computations with probabilities on graphical structures and their application to expert \nsystems. Journal of the Royal Statistical Society: Series B (Methodological) 50, 2 (1988), 157-194. \n[7] \nGuido W Imbens and Donald B Rubin. 2015. Causal inference in statistics, social, and biomedical sciences. Cambridge University Press.  \n[8] \nStefan Wager and Susan Athey. 2018. Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American \nStatistical Association 113, 523 (2018), 1228-1242. \n[9] \nSubramani Mani and Gregory F Cooper. 2000. Causal discovery from medical textual data. In Proceedings of the AMIA Symposium, 2000. 542-546. \n[10] Cross-Disorder Group of the Psychiatric Genomics Consortium. 2013. Identification of risk loci with shared effects on five major psychiatric \ndisorders: a genome-wide analysis. The Lancet 381, 9875 (2013), 1371-1379. \n[11] Clive WJ Granger. 1969. Investigating causal relations by econometric models and cross-spectral methods. Econometrica: journal of the Econometric \nSociety 37 (1969), 424-438. \n[12] Kevin D Hoover. 2006. Causality in economics and econometrics. Springer.  \n[13] Alberto Abadie and Guido W Imbens. 2016. Matching on the estimated propensity score. Econometrica 84, 2 (2016), 781-807. \n[14] Guido W Imbens. 2004. Nonparametric estimation of average treatment effects under exogeneity: A review. Review of Economics and statistics 86, \n1 (2004), 4-29. \n[15] Serge Darolles, Yanqin Fan, Jean-Pierre Florens and Eric Renault. 2011. Nonparametric instrumental regression. Econometrica 79, 5 (2011), 1541-\n1565. \n[16] Miguel Ãngel HernÃ¡n, Babette Brumback and James M Robins. 2000. Marginal structural models to estimate the causal effect of zidovudine on the \nsurvival of HIV-positive men. Epidemiology 11, 5 (2000), 561-570. \n[17] James M Robins, Miguel Angel Hernan and Babette Brumback. 2000. Marginal structural models and causal inference in epidemiology. \nEpidemiology 11, 5 (2000), 550-560. \n[18] Miguel A HernÃ¡n and James M Robins. 2010. Causal inference: What If. CRC Press.  \n[19] Miguel A HernÃ¡n. 2018. The C-word: scientific euphemisms do not improve causal inference from observational data. American journal of public \nhealth 108, 5 (2018), 616-619. \n[20] Michael P Grosz, Julia M Rohrer and Felix Thoemmes. 2020. The taboo against explicit causal inference in nonexperimental psychology. \nPerspectives on Psychological Science 15, 5 (2020), 1243-1255. \n[21] MJ Vowels. 2020. Limited functional form, misspecification, and unreliable interpretations in psychology and social science. arXiv:2009.10025. \nRetrieved from https://arxiv.org/abs/2009.10025. \n[22] Michael E Sobel. 1998. Causal inference in statistical models of the process of socioeconomic achievement: A case study. Sociological Methods & \nResearch 27, 2 (1998), 318-348. \n[23] Cosma Rohilla Shalizi and Andrew C Thomas. 2011. Homophily and contagion are generically confounded in observational social network studies. \nSociological methods & research 40, 2 (2011), 211-239. \n[24] Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal and Yoshua Bengio. 2021. Toward \nCausal Representation Learning. Proceedings of the IEEE 109, 5 (2021), 612-634. \n[25] Ruocheng Guo, Lu Cheng, Jundong Li, P. Richard Hahn and Huan Liu. 2021. A Survey of Learning Causality with Data. ACM Computing Surveys \n53, 4 (2021), 1-37. \n[26] Clark Glymour, Kun Zhang and Peter Spirtes. 2019. Review of Causal Discovery Methods Based on Graphical Models. Frontiers in Genetics 10, \n(2019), 524-539. \n[27] Matthew J Vowels, Necati Cihan Camgoz and Richard Bowden. 2021. D'ya like dags? A survey on structure learning and causal discovery. \narXiv:2103.02582. Retrieved from https://arxiv.org/abs/2103.02582. \n[28] Liuyi Yao, Zhixuan Chu, Sheng Li, Yaliang Li, Jing Gao and Aidong Zhang. 2021. A survey on causal inference. ACM Transactions on Knowledge \nDiscovery from Data (TKDD) 15, 5 (2021), 1-46. \n[29] Markus Kalisch and Peter BÃ¼hlman. 2007. Estimating high-dimensional directed acyclic graphs with the PC-algorithm. Journal of Machine Learning \nResearch 8 (2007), 613-636. \n[30] Joseph Ramsey, Jiji Zhang and Peter L Spirtes. 2012. Adjacency-faithfulness and conservative causal inference. arXiv:1206.6843. Retrieved from \nhttps://arxiv.org/abs/1206.6843. \n[31] Peter Spirtes, Clark N Glymour, Richard Scheines and David Heckerman. 2000. Causation, prediction, and search. MIT press.  \n[32] Rohit Bhattacharya, Razieh Nabi and Ilya Shpitser. 2020. Semiparametric inference for causal effects in graphical models with hidden variables. \n29 \narXiv:2003.12659. Retrieved from https://arxiv.org/abs/2003.12659. \n[33] Niki Kiriakidou and Christos Diou. 2022. An improved neural network model for treatment effect estimation. arXiv:2205.11106. Retrieved from \nhttps://arxiv.org/abs/2205.11106. \n[34] Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard SchÃ¶lkopf, Michael C Mozer, Chris Pal and Yoshua \nBengio. 2019. Learning neural causal models from unknown interventions. arXiv:1910.01075. Retrieved from https://arxiv.org/abs/1910.01075. \n[35] Christina Heinze-Deml, Marloes H Maathuis and Nicolai Meinshausen. 2018. Causal structure learning. Annual Review of Statistics and Its \nApplication 5 (2018), 371-391. \n[36] Youngseo Son, Nipun Bayas and H Andrew Schwartz. 2018. Causal explanation analysis on social media. arXiv:1809.01202. Retrieved from \nhttps://arxiv.org/abs/1809.01202. \n[37] Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer and Stuart Shieber. 2020. Investigating gender bias in \nlanguage models using causal mediation analysis. In Advances in Neural Information Processing Systems, Dec 6-12, 2020. 12388-12401. \n[38] Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen and Yonatan Belinkov. 2021. Causal analysis of syntactic \nagreement mechanisms in neural language models. arXiv:2106.06087. Retrieved from https://arxiv.org/abs/2106.06087. \n[39] Dong Zhang, Hanwang Zhang, Jinhui Tang, Xian-Sheng Hua and Qianru Sun. 2020. Causal intervention for weakly-supervised semantic \nsegmentation. Advances in Neural Information Processing Systems 33, (2020), 655-666. \n[40] Pranoy Panda, Sai Srinivas Kancheti and Vineeth N Balasubramanian. 2021. Instance-wise Causal Feature Selection for Model Interpretation. In \nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 1756-1759. \n[41] Krzysztof Chalupka, Frederick Eberhardt and Pietro Perona. 2016. Causal feature learning: an overview. Behaviormetrika 44, 1 (2016), 137-164. \n[42] Yoshua Bengio, Aaron Courville and Pascal Vincent. 2013. Representation learning: A review and new perspectives. IEEE transactions on pattern \nanalysis and machine intelligence 35, 8 (2013), 1798-1828. \n[43] Mehrdad Farajtabar, Andrew Lee, Yuanjian Feng, Vishal Gupta, Peter Dolan, Harish Chandran and Martin Szummer. 2020. Balance regularized \nneural network models for causal effect estimation. arXiv:2011.11199. Retrieved from https://arxiv.org/abs/2011.11199. \n[44] Jean Kaddour, Yuchen Zhu, Qi Liu, Matt J Kusner and Ricardo Silva. 2021. Causal effect inference for structured treatments. In Advances in Neural \nInformation Processing Systems, Dec 6-14, 2021. 24841-24854. \n[45] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville and Yoshua Bengio. 2014. \nGenerative adversarial nets. In Advances in neural information processing systems, Dec 8-13, 2014. MontrÃ©al Canada. 2672-2680. \n[46] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv:1312.6114. Retrieved from https://arxiv.org/abs/1312.6114. \n[47] Ana Rita Nogueira, Andrea Pugnana, Salvatore Ruggieri, Dino Pedreschi and JoÃ£o Gama. 2022. Methods and tools for causal discovery and causal \ninference. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 12, 2 (2022), e1449. \n[48] Chang Gong, Di Yao, Chuzhe Zhang, Wenbin Li and Jingping Bi. 2023. Causal discovery from temporal data: An overview and new perspectives. \narXiv:2303.10112. Retrieved from https://arxiv.org/abs/2303.10112. \n[49] Bernhard SchÃ¶lkopf. 2019. Causality for machine learning. Probabilistic and Causal Inference The Works of Judea Pearl (2019), 765-804. \n[50] Jean Kaddour, Aengus Lynch, Qi Liu, Matt J Kusner and Ricardo Silva. 2022. Causal Machine Learning: A Survey and Open Problems. \narXiv:2206.15475. Retrieved from https://arxiv.org/abs/2206.15475. \n[51] Bernard Koch, Tim Sainburg, Pablo Geraldo, Song Jiang, Yizhou Sun and Jacob Gates Foster. 2021. Deep learning of potential outcomes. \narXiv:2110.04442. Retrieved from https://arxiv.org/abs/2110.04442. \n[52] Zongyu Li, Zhenfeng Zhu, Shuai Zheng, Zhenyu Guo, Siwei Qiang and Yao Zhao. 2022. A survey of deep causal models and their industrial \napplications. arXiv:2209.08860. Retrieved from https://arxiv.org/abs/2209.08860. \n[53] Jonas Peters, Dominik Janzing and Bernhard SchÃ¶lkopf. 2017. Elements of causal inference: foundations and learning algorithms. The MIT Press.  \n[54] David Maxwell Chickering. 2002. Optimal structure identification with greedy search. Journal of machine learning research 3, Nov (2002), 507-554. \n[55] Shohei Shimizu, Patrik O Hoyer, Aapo HyvÃ¤rinen, Antti Kerminen and Michael Jordan. 2006. A linear non-Gaussian acyclic model for causal \ndiscovery. Journal of Machine Learning Research 72, 7 (2006), 2003-2030. \n[56] Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters and Bernhard SchÃ¶lkopf. 2008. Nonlinear causal discovery with additive noise models. \nIn Advances in neural information processing systems, Dec 8-10, 2008. Vancouver, B.C., Canada. 689-696. \n[57] Diego Colombo, Marloes H Maathuis, Markus Kalisch and Thomas S Richardson. 2012. Learning high-dimensional directed acyclic graphs with \nlatent and selection variables. The Annals of Statistics 40, 1 (2012), 294-321. \n[58] Dominik Janzing, Joris Mooij, Kun Zhang, Jan Lemeire, Jakob Zscheischler, Povilas DaniuÅ¡is, Bastian Steudel and Bernhard SchÃ¶lkopf. 2012. \nInformation-geometric approach to inferring causal directions. Artificial Intelligence 182 (2012), 1-31. \n[59] Antti Hyttinen, Patrik O Hoyer, Frederick Eberhardt and Matti Jarvisalo. 2013. Discovering cyclic causal models with latent variables: A general \nSAT-based procedure. arXiv:1309.6836. Retrieved from https://arxiv.org/abs/1309.6836. \n[60] Xun Zheng, Bryon Aragam, Pradeep K Ravikumar and Eric P Xing. 2018. Dags with no tears: Continuous optimization for structure learning. In \nAdvances in Neural Information Processing Systems, Dec 2-8, 2018. MontrÃ©al Canada. 9492-9503. \n[61] Alex Tank, Ian Covert, Nicholas Foti, Ali Shojaie and Emily Fox. 2021. Neural granger causality. IEEE Transactions on Pattern Analysis and \nMachine Intelligence 44, 8 (2021), 4267-4279. \n[62] Ilya Shpitser and Judea Pearl. 2008. Complete Identification Methods for the Causal Hierarchy. Journal of Machine Learning Research 9, 9 (2008), \n1941-1979. \n30 \n[63] Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla and Bernhard SchÃ¶lkopf. 2018. Learning independent causal mechanisms. In \nInternational Conference on Machine Learning, Jul 10-15, 2018. StockholmsmÃ¤ssan, Stockholm Sweden. 4036-4044. \n[64] Raphael Suter, Djordje Miladinovic, Bernhard SchÃ¶lkopf and Stefan Bauer. 2019. Robustly disentangled causal mechanisms: Validating deep \nrepresentations for interventional robustness. In International Conference on Machine Learning, Jun 9-15, 2019. Long Beach, California. 6056-6065. \n[65] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao and Jun Wang. 2021. CausalVAE: Disentangled representation learning via \nneural structural causal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, December 6-12, 2021. \n9593-9602. \n[66] Mingze Dong and Yuval Kluger. 2022. GEASS: Neural causal feature selection for high-dimensional biological data. In The Eleventh International \nConference on Learning Representations, 2022. 1-19. \n[67] Thien Q Tran, Kazuto Fukuchi, Youhei Akimoto and Jun Sakuma. 2021. Unsupervised Causal Binary Concepts Discovery with VAE for Black-box \nModel Explanation. arXiv:2109.04518. Retrieved from https://arxiv.org/abs/2109.04518. \n[68] Weiran Yao, Guangyi Chen and Kun Zhang. 2022. Temporally disentangled representation learning. In Advances in Neural Information Processing \nSystems, 2022. 26492-26503. \n[69] Weiran Yao, Yuewen Sun, Alex Ho, Changyin Sun and Kun Zhang. 2021. Learning temporally causal latent processes from general temporal data. \narXiv:2110.05428. Retrieved from https://arxiv.org/abs/2110.05428. \n[70] Ziniu Hu, Zhe Zhao, Xinyang Yi, Tiansheng Yao, Lichan Hong, Yizhou Sun and Ed Chi. 2022. Improving multi-task generalization via regularizing \nspurious correlation. In Advances in Neural Information Processing Systems, 2022. 11450-11466. \n[71] Mingyuan Cheng, Xinru Liao, Quan Liu, Bin Ma, Jian Xu and Bo Zheng. 2022. Learning disentangled representations for counterfactual regression \nvia mutual information minimization. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in \nInformation Retrieval, 2022. 1802-1806. \n[72] Bo-Wei Huang, Keng-Te Liao, Chang-Sheng Kao and Shou-De Lin. 2022. Environment Diversification with Multi-head Neural Network for \nInvariant Learning. In Advances in Neural Information Processing Systems, 2022. 915-927. \n[73] Lingjing Kong, Shaoan Xie, Weiran Yao, Yujia Zheng, Guangyi Chen, Petar Stojanov, Victor Akinwande and Kun Zhang. 2022. Partial \ndisentanglement for domain adaptation. In International conference on machine learning, 2022. 11455-11472. \n[74] Raha Moraffah, Kai Shu, Adrienne Raglin and Huan Liu. 2019. Deep causal representation learning for unsupervised domain adaptation. \narXiv:1910.12417. Retrieved from https://arxiv.org/abs/1910.12417. \n[75] Chaochao Lu, Yuhuai Wu, JosÃ© Miguel HernÃ¡ndez-Lobato and Bernhard SchÃ¶lkopf. 2021. Invariant causal representation learning for out-of-\ndistribution generalization. In International Conference on Learning Representations, 2021. 1-32. \n[76] Imant Daunhawer, Alice Bizeul, Emanuele Palumbo, Alexander Marx and Julia E Vogt. 2023. Identifiability results for multimodal contrastive \nlearning. arXiv:2303.09166. Retrieved from https://arxiv.org/abs/2303.09166. \n[77] Guangyi Chen, Yuke Li, Xiao Liu, Zijian Li, Eman Al Suradi, Donglai Wei and Kun Zhang. 2023. LLCP: Learning Latent Causal Processes for \nReasoning-based Video Question Answer. In The Twelfth International Conference on Learning Representations, 2023. 1-23. \n[78] Ying-Xin Wu, Xiang Wang, An Zhang, Xiangnan He and Tat-Seng Chua. 2022. Discovering invariant rationales for graph neural networks. \narXiv:1906.02226. Retrieved from https://arxiv.org/abs/1906.02226. \n[79] Johann Brehmer, Pim De Haan, Phillip Lippe and Taco S Cohen. 2022. Weakly supervised causal representation learning. In Advances in Neural \nInformation Processing Systems, 2022. 38319-38331. \n[80] Sorawit Saengkyongam, Elan Rosenfeld, Pradeep Ravikumar, Niklas Pfister and Jonas Peters. 2023. Identifying representations for intervention \nextrapolation. arXiv:2310.04295. Retrieved from https://arxiv.org/abs/2310.04295. \n[81] Phillip Lippe, Sara Magliacane, Sindy LÃ¶we, Yuki M Asano, Taco Cohen and Stratis Gavves. 2022. Citris: Causal identifiability from temporal \nintervened sequences. In International Conference on Machine Learning, 2022. 13557-13603. \n[82] Phillip Lippe, Sara Magliacane, Sindy LÃ¶we, Yuki M Asano, Taco Cohen and Efstratios Gavves. 2022. Causal representation learning for \ninstantaneous and temporal effects in interactive systems. arXiv:2206.06169. Retrieved from https://arxiv.org/abs/2206.06169. \n[83] Antonin Chambolle and Thomas Pock. 2016. An introduction to continuous optimization for imaging. Acta Numerica 25 (2016), 161-319. \n[84] Niclas AndrÃ©asson, Anton Evgrafov and Michael Patriksson. 2020. An introduction to continuous optimization: foundations and fundamental \nalgorithms. Courier Dover Publications.  \n[85] Yue Yu, Jie Chen, Tian Gao and Mo Yu. 2019. DAG-GNN: DAG structure learning with graph neural networks. In International Conference on \nMachine Learning, June 9-15, 2019. Long Beach, California. 7154-7163. \n[86] Ignavier Ng, Shengyu Zhu, Zhitang Chen and Zhuangyan Fang. 2019. A graph autoencoder approach to causal structure learning. arXiv:1911.07420. \nRetrieved from https://arxiv.org/abs/1911.07420. \n[87] SÃ©bastien Lachapelle, Philippe Brouillard, Tristan Deleu and Simon Lacoste-Julien. 2019. Gradient-based neural dag learning. arXiv:1906.02226. \nRetrieved from https://arxiv.org/abs/1906.02226. \n[88] Yue Yu, Tian Gao, Naiyu Yin and Qiang Ji. 2021. DAGs with no curl: An efficient DAG structure learning approach. In International Conference on \nMachine Learning, 2021. 12156-12166. \n[89] Romain Lopez, Jan-Christian HÃ¼tter, Jonathan Pritchard and Aviv Regev. 2022. Large-scale differentiable causal discovery of factor graphs. In \nAdvances in Neural Information Processing Systems, 2022. 19290-19303. \n[90] Tomer Galanti, Ofir Nabati and Lior Wolf. 2020. A critical view of the structural causal model. arXiv:2002.10007. Retrieved from \nhttps://arxiv.org/abs/2002.10007. \n31 \n[91] Trent Kyono, Yao Zhang and Mihaela van der Schaar. 2020. Castle: Regularization via auxiliary causal graph discovery. In Advances in Neural \nInformation Processing Systems, Dec 6-12, 2020. 1501-1512. \n[92] Ilyes Khemakhem, Ricardo Monti, Robert Leech and Aapo Hyvarinen. 2021. Causal autoregressive flows. In International conference on artificial \nintelligence and statistics, 2021. 3520-3528. \n[93] Zhenyu Zhu, Francesco Locatello and Volkan Cevher. 2024. Sample Complexity Bounds for Score-Matching: Causal Discovery and Generative \nModeling. In Advances in Neural Information Processing Systems, 2024. 1-29. \n[94] Pedro Sanchez, Xiao Liu, Alison Q O'Neil and Sotirios A Tsaftaris. 2022. Diffusion models for causal discovery via topological ordering. \narXiv:2210.06201. Retrieved from https://arxiv.org/abs/2210.06201. \n[95] Raha Moraffah, Bahman Moraffah, Mansooreh Karami, Adrienne Raglin and Huan Liu. 2020. Causal adversarial network for learning conditional \nand interventional distributions. arXiv:2008.11376. Retrieved from https://arxiv.org/abs/2008.11376. \n[96] Andreas WM Sauter, NicolÃ² Botteghi, Erman Acar and Aske Plaat. 2024. CORE: Towards Scalable and Efficient Causal Discovery with \nReinforcement Learning. arXiv:2401.16974. Retrieved from https://arxiv.org/abs/2401.16974. \n[97] Phillip Lippe, Taco Cohen and Efstratios Gavves. 2021. Efficient neural causal discovery without acyclicity constraints. arXiv:1906.02226. Retrieved \nfrom https://arxiv.org/abs/1906.02226. \n[98] Bernhard H Korte, Jens Vygen, B Korte and J Vygen. 2011. Combinatorial optimization. Springer.  \n[99] Olivier Goudet, Diviyan Kalainathan, Philippe Caillou, Isabelle Guyon, David Lopez-Paz and Michele Sebag. 2018. Learning functional causal \nmodels with generative neural networks. Explainable and interpretable models in computer vision and machine learning (2018), 39-80. \n[100] Diviyan Kalainathan. 2019. Generative Neural Networks to infer Causal Mechanisms: algorithms and applications. UniversitÃ© Paris Saclay  \n[101] Yuhao Wang, Vlado Menkovski, Hao Wang, Xin Du and Mykola Pechenizkiy. 2020. Causal discovery from incomplete data: a deep learning \napproach. arXiv:2001.05343. Retrieved from https://arxiv.org/abs/2001.05343. \n[102] David Kaltenpoth and Jilles Vreeken. 2023. Nonlinear causal discovery with latent confounders. In International Conference on Machine Learning, \n2023. 15639-15654. \n[103] Matthew Ashman, Chao Ma, Agrin Hilmkil, Joel Jennings and Cheng Zhang. 2023. Causal reasoning in the presence of latent confounders via neural \nadmg learning. arXiv:1906.02226. Retrieved from https://arxiv.org/abs/1906.02226. \n[104] Erdun Gao, Ignavier Ng, Mingming Gong, Li Shen, Wei Huang, Tongliang Liu, Kun Zhang and Howard Bondell. 2022. Missdag: Causal discovery \nin the presence of missing data with continuous additive noise models. In Advances in Neural Information Processing Systems, 2022. 5024-5038. \n[105] Jakob Runge, Sebastian Bathiany, Erik Bollt, Gustau Camps-Valls, Dim Coumou, Ethan Deyle, Clark Glymour, Marlene Kretschmer, Miguel D \nMahecha and Jordi MuÃ±oz-MarÃ­. 2019. Inferring causation from time series in Earth system sciences. Nature communications 10, 1 (2019), 1-13. \n[106] David Danks and Sergey Plis. 2013. Learning causal structure from undersampled time series. In Twenty-seventh Conference on Neural Information \nProcessing Systems, Dec 5-10, 2013. Harrahs and Harveys, Lake Tahoe 1-10. \n[107] Antti Hyttinen, Sergey Plis, Matti JÃ¤rvisalo, Frederick Eberhardt and David Danks. 2016. Causal discovery from subsampled time series data by \nconstraint optimization. In Conference on Probabilistic Graphical Models, Sep 6-9, 2016. Lugano, Switzerland. 216-227. \n[108] C. W. J. Granger. 1980. Testing for causality: A personal viewpoint. Journal of Economic Dynamics and Control 2, (1980), 329-352. \n[109] Lionel Barnett and Anil K Seth. 2014. The MVGC multivariate Granger causality toolbox: a new approach to Granger-causal inference. Journal of \nneuroscience methods 223, (2014), 50-68. \n[110] Belkacem Chikhaoui, Mauricio Chiazzaro and Shengrui Wang. 2015. A new granger causal model for influence evolution in dynamic social \nnetworks: The case of dblp. In Proceedings of the AAAI Conference on Artificial Intelligence, January 25-30, 2015. Austin Texas, USA. 51-57. \n[111] Meike Nauta, Doina Bucur and Christin Seifert. 2019. Causal discovery with attention-based convolutional neural networks. Machine Learning and \nKnowledge Extraction 1, 1 (2019), 1-28. \n[112] Bart Bussmann, Jannes Nys and Steven LatrÃ©. 2021. Neural additive vector autoregression models for causal discovery in time series. In Discovery \nScience: 24th International Conference, DS 2021, Halifax, NS, Canada, October 11â€“13, 2021, Proceedings 24, 2021. 446-460. \n[113] Saurabh Khanna and Vincent YF Tan. 2019. Economy statistical recurrent units for inferring nonlinear granger causality. arXiv:1911.09879. \nRetrieved from https://arxiv.org/abs/1911.09879. \n[114] RiÄards MarcinkeviÄs and Julia E Vogt. 2021. Interpretable models for granger causality using self-explaining neural networks. arXiv:1906.02226. \nRetrieved from https://arxiv.org/abs/1906.02226. \n[115] Hongming Li, Shujian Yu and Jose Principe. 2023. Causal recurrent variational autoencoder for medical time series generation. In Proceedings of the \nAAAI Conference on Artificial Intelligence, 2023. 8562-8570. \n[116] Rohit Singh, Alexander P Wu and Bonnie Berger. 2022. Granger causal inference on DAGs identifies genomic loci regulating transcription. \narXiv:2210.10168. Retrieved from https://arxiv.org/abs/2210.10168. \n[117] Xiangyu Sun, Oliver Schulte, Guiliang Liu and Pascal Poupart. 2021. NTS-NOTEARS: Learning nonparametric DBNs with prior knowledge. \narXiv:2109.04286. Retrieved from https://arxiv.org/abs/2109.04286. \n[118] Wenbo Gong, Joel Jennings, Cheng Zhang and Nick Pawlowski. 2022. Rhino: Deep causal temporal relationship learning with history-dependent \nnoise. arXiv:2210.14706. Retrieved from https://arxiv.org/abs/2210.14706. \n[119] Enyan Dai and Jie Chen. 2022. Graph-augmented normalizing flows for anomaly detection of multiple time series. arXiv:2202.07857. Retrieved \nfrom https://arxiv.org/abs/2202.07857. \n[120] Yunfei Chu, Xiaowei Wang, Jianxin Ma, Kunyang Jia, Jingren Zhou and Hongxia Yang. 2020. Inductive granger causal modeling for multivariate \n32 \ntime series. In 2020 IEEE International Conference on Data Mining (ICDM), 2020. 972-977. \n[121] Sindy LÃ¶we, David Madras, Richard Zemel and Max Welling. 2020. Amortized causal discovery: Learning to infer causal graphs from time-series \ndata. arXiv:2006.10833. Retrieved from https://arxiv.org/abs/2006.10833. \n[122] Nan Rosemary Ke, Jane Wang, Jovana Mitrovic, Martin Szummer and Danilo J Rezende. 2020. Amortized learning of neural causal representations. \narXiv:2008.09301. Retrieved from https://arxiv.org/abs/2008.09301. \n[123] Tian Gao, Debarun Bhattacharjya, Elliot Nelson, Miao Liu and Yue Yu Idyno. Learning nonparametric dags from interventional dynamic data. In \nInternational Conference on Machine Learning, 6988-7001. \n[124] Chenxi Liu and Kun Kuang. 2023. Causal structure learning for latent intervened non-stationary data. In International Conference on Machine \nLearning, 2023. 21756-21777. \n[125] Yuxiao Cheng, Runzhao Yang, Tingxiong Xiao, Zongren Li, Jinli Suo, Kunlun He and Qionghai Dai. 2023. Cuts: Neural causal discovery from \nirregular time-series data. arXiv:2302.07458. Retrieved from https://arxiv.org/abs/2302.07458. \n[126] Alexis Bellot, Kim Branson and Mihaela van der Schaar. 2021. Neural graphical modelling in continuous-time: consistency guarantees and \nalgorithms. arXiv:2105.02522. Retrieved from https://arxiv.org/abs/2105.02522. \n[127] Benjie Wang, Joel Jennings and Wenbo Gong. 2023. Neural Structure Learning with Stochastic Differential Equations. arXiv:2311.03309. Retrieved \nfrom https://arxiv.org/abs/2311.03309. \n[128] Sindy LÃ¶we, David Madras, Richard Zemel and Max Welling. 2022. Amortized causal discovery: Learning to infer causal graphs from time-series \ndata. In Conference on Causal Learning and Reasoning, 2022. 509-525. \n[129] Fredrik Johansson, Uri Shalit and David Sontag. 2016. Learning representations for counterfactual inference. In International conference on machine \nlearning, Jun 19-24, 2016. New York City, NY, USA. 3020-3029. \n[130] Uri Shalit, Fredrik D Johansson and David Sontag. 2017. Estimating individual treatment effect: generalization bounds and algorithms. In \nInternational Conference on Machine Learning, Aug 6-11, 2017. Sydney, Australia. 3076-3085. \n[131] CÃ©dric Villani. 2009. Optimal transport: old and new. Springer.  \n[132] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard SchÃ¶lkopf and Alexander Smola. 2012. A kernel two-sample test. The Journal of \nMachine Learning Research 13, 1 (2012), 723-773. \n[133] Serge Assaad, Shuxi Zeng, Chenyang Tao, Shounak Datta, Nikhil Mehta, Ricardo Henao, Fan Li and Lawrence Carin. 2021. Counterfactual \nrepresentation learning with balancing weights. In International Conference on Artificial Intelligence and Statistics, Apr 13-15, 2021. 1972-1980. \n[134] Negar Hassanpour and Russell Greiner. 2019. CounterFactual Regression with Importance Sampling Weights. In the 28th International Joint \nConference on Artificial Intelligence, Aug 10-16, 2019. Macao, China. 5880-5887. \n[135] Alicia Curth and Mihaela Van der Schaar. 2021. On inductive biases for heterogeneous treatment effect estimation. In Advances in Neural \nInformation Processing Systems, 2021. 15883-15894. \n[136] Ioana Bica and Mihaela van der Schaar. 2022. Transfer learning on heterogeneous feature spaces for treatment effects estimation. In Advances in \nNeural Information Processing Systems, 2022. 37184-37198. \n[137] Liu Qidong, Tian Feng, Ji Weihua and Zheng Qinghua. 2020. A new representation learning method for individual treatment effect estimation: Split \ncovariate representation network. In Asian Conference on Machine Learning, Nov 17-19, 2020. 811-822. \n[138] Claudia Shi, David Blei and Victor Veitch. 2019. Adapting neural networks for the estimation of treatment effects. arXiv:1906.0212. Retrieved from \nhttps://arxiv.org/abs/1906.0212. \n[139] Tobias Hatt and Stefan Feuerriegel. 2021. Estimating average treatment effects via orthogonal regularization. In Proceedings of the 30th ACM \nInternational Conference on Information & Knowledge Management, 2021. 680-689. \n[140] Ahmed M Alaa, Michael Weisz and Mihaela Van Der Schaar. 2017. Deep counterfactual networks with propensity-dropout. arXiv:1706.05966. \nRetrieved from https://arxiv.org/abs/1706.05966. \n[141] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural \nnetworks from overfitting. The journal of machine learning research 15, 1 (2014), 1929-1958. \n[142] Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In International \nconference on machine learning, Jun 19-24, 2016. New York City, NY, USA. 1050-1059. \n[143] Onur Atan, James Jordon and Mihaela Van der Schaar. 2018. Deep-treat: Learning optimal personalized treatments from observational data using \nneural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, Feb 2-7, 2018. New Orleans, Louisiana, USA. 2071-2078. \n[144] Zichen Zhang, Qingfeng Lan, Lei Ding, Yue Wang, Negar Hassanpour and Russell Greiner. 2019. Reducing selection bias in counterfactual \nreasoning for individual treatment effects estimation. arXiv:1912.09040. Retrieved from https://arxiv.org/abs/1912.09040. \n[145] Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao and Aidong Zhang. 2018. Representation learning for treatment effect estimation from \nobservational data. In Advances in Neural Information Processing Systems, Dec 2-8, 2018. MontrÃ©al Canada. 2638-2648. \n[146] Kailiang Zhong, Fengtong Xiao, Yan Ren, Yaorong Liang, Wenqing Yao, Xiaofeng Yang and Ling Cen. 2022. Descn: Deep entire space cross \nnetworks for individual treatment effect estimation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data \nMining, 2022. 4612-4620. \n[147] Yaxin Fang and Faming Liang. 2024. Causal-StoNet: Causal Inference for High-Dimensional Complex Data. arXiv:1906.02226. Retrieved from \nhttps://arxiv.org/abs/1906.02226. \n[148] Patrick Schwab, Lorenz Linhardt and Walter Karlen. 2018. Perfect match: A simple method for learning representations for counterfactual inference \n33 \nwith neural networks. arXiv:1810.00656. Retrieved from https://arxiv.org/abs/1810.00656. \n[149] Fredrik D Johansson, Nathan Kallus, Uri Shalit and David Sontag. 2018. Learning weighted representations for generalization across designs. \narXiv:1802.08598. Retrieved from https://arxiv.org/abs/1802.08598. \n[150] Sonali Parbhoo, Stefan Bauer and Patrick Schwab. 2021. Ncore: Neural counterfactual representation learning for combinations of treatments. \narXiv:2103.11175. Retrieved from https://arxiv.org/abs/2103.11175. \n[151] Zhaozhi Qian, Alicia Curth and Mihaela van der Schaar. 2021. Estimating multi-cause treatment effects via single-cause perturbation. In Advances in \nNeural Information Processing Systems, 2021. 23754-23767. \n[152] Patrick Schwab, Lorenz Linhardt, Stefan Bauer, Joachim M Buhmann and Walter Karlen. 2020. Learning counterfactual representations for \nestimating individual dose-response curves. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020. 5612-5619. \n[153] Lizhen Nie, Mao Ye, Qiang Liu and Dan Nicolae. 2021. Vcnet and functional targeted regularization for learning causal effects of continuous \ntreatments. arXiv:2103.07861. Retrieved from https://arxiv.org/abs/2103.07861. \n[154] Xin Wang, Shengfei Lyu, Xingyu Wu, Tianhao Wu and Huanhuan Chen. 2022. Generalization bounds for estimating causal effects of continuous \ntreatments. In Advances in Neural Information Processing Systems, 2022. 8605-8617. \n[155] Andrew Jesson, Alyson Douglas, Peter Manshausen, MaÃ«lys Solal, Nicolai Meinshausen, Philip Stier, Yarin Gal and Uri Shalit. 2022. Scalable \nsensitivity and uncertainty analyses for causal-effect estimates of continuous-valued interventions. In Advances in Neural Information Processing \nSystems, 2022. 13892-13907. \n[156] Taha Bahadori, Eric Tchetgen Tchetgen and David Heckerman. 2022. End-to-end balancing for causal continuous treatment-effect estimation. In \nInternational Conference on Machine Learning, 2022. 1313-1326. \n[157] Ioana Bica, Ahmed Alaa and Mihaela Van Der Schaar. 2020. Time series deconfounder: Estimating treatment effects over time in the presence of \nhidden confounders. In International conference on machine learning, 2020. 884-895. \n[158] Zhaozhi Qian, Yao Zhang, Ioana Bica, Angela Wood and Mihaela van der Schaar. 2021. Synctwin: Treatment effect estimation with longitudinal \noutcomes. In Advances in Neural Information Processing Systems, 2021. 3178-3190. \n[159] Bryan Lim. 2018. Forecasting treatment responses over time using recurrent marginal structural networks. In Advances in neural information \nprocessing systems, 2018. 1-11. \n[160] Ruoqi Liu, Changchang Yin and Ping Zhang. 2020. Estimating individual treatment effects with time-varying confounders. In 2020 IEEE \nInternational Conference on Data Mining (ICDM), 2020. 382-391. \n[161] Jinsung Yoon, James Jordon and Mihaela Van Der Schaar. 2018. GANITE: Estimation of individualized treatment effects using generative \nadversarial nets. In International Conference on Learning Representations, Apr 30- May 3, 2018. Vancouver Canada. 1-22. \n[162] Ioana Bica, James Jordon and Mihaela van der Schaar. 2020. Estimating the effects of continuous-valued interventions using generative adversarial \nnetworks. In Advances in Neural Information Processing Systems, Dec 6-12, 2020. 16434-16445. \n[163] Chandan Singh, Guha Balakrishnan and Pietro Perona. 2021. Matched sample selection with GANs for mitigating attribute confounding. \narXiv:2103.13455. Retrieved from https://arxiv.org/abs/2103.13455. \n[164] Qingyu Zhao, Ehsan Adeli and Kilian M Pohl. 2020. Training confounder-free deep learning models for medical applications. Nature \ncommunications 11, 1 (2020), 1-9. \n[165] Kevin Muyuan Xia, Yushu Pan and Elias Bareinboim. 2022. Neural causal models for counterfactual identification and estimation. In The Eleventh \nInternational Conference on Learning Representations, 2022. 1-14. \n[166] Ioana Bica, Ahmed M Alaa, James Jordon and Mihaela van der Schaar. 2020. Estimating counterfactual treatment outcomes over time through \nadversarially balanced representations. arXiv:2002.04083. Retrieved from https://arxiv.org/abs/2002.04083. \n[167] Xin Du, Lei Sun, Wouter Duivesteijn, Alexander Nikolaev and Mykola Pechenizkiy. 2021. Adversarial balancing-based representation learning for \ncausal effect inference with observational data. Data Mining and Knowledge Discovery 35, 4 (2021), 1713-1738. \n[168] Nathan Kallus. 2020. Deepmatch: Balancing deep covariate representations for causal inference using adversarial training. In International \nConference on Machine Learning, Jul 12-18, 2020. 5067-5077. \n[169] Zhenyu Guo, Shuai Zheng, Zhizhe Liu, Kun Yan and Zhenfeng Zhu. 2021. Cetransformer: Casual effect estimation via transformer based \nrepresentation learning. In Pattern Recognition and Computer Vision: 4th Chinese Conference, PRCV 2021, Beijing, China, October 29â€“November 1, \n2021, Proceedings, Part IV 4, 2021. 524-535. \n[170] Guanglin Zhou, Lina Yao, Xiwei Xu, Chen Wang and Liming Zhu. 2022. Cycle-balanced representation learning for counterfactual inference. In \nProceedings of the 2022 SIAM International Conference on Data Mining (SDM), 2022. 442-450. \n[171] Yi-Fan Zhang, Hanlin Zhang, Zachary Lipton, Li Erran Li and Eric P Xing. 2022. Can transformers be strong treatment effect estimators? \narXiv:2202.01336. Retrieved from https://arxiv.org/abs/2202.01336. \n[172] Matthew James Vowels, Necati Cihan Camgoz and Richard Bowden. 2020. Targeted VAE: Structured inference and targeted learning for causal \nparameter estimation. arXiv.2009.13472. Retrieved from https://arxiv.org/abs/2009.13472. \n[173] Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel and Max Welling. 2017. Causal effect inference with deep latent-variable \nmodels. In Advances in neural information processing systems, Dec 4-9, 2017. Long Beach, CA, USA. 6449-6459. \n[174] Carl Doersch. 2016. Tutorial on variational autoencoders. arXiv:1606.05908. Retrieved from https://arxiv.org/abs/1606.05908. \n[175] Diederik P Kingma and Max Welling. 2019. An introduction to variational autoencoders. arXiv:1906.02691. Retrieved from \nhttps://arxiv.org/abs/1906.02691. \n34 \n[176] Weijia Zhang, Lin Liu and Jiuyong Li. 2020. Treatment effect estimation with disentangled latent factors. arXiv:2001.10652. Retrieved from \nhttps://arxiv.org/abs/2001.10652. \n[177] Jason Hartford, Greg Lewis, Kevin Leyton-Brown and Matt Taddy. 2017. Deep IV: A flexible approach for counterfactual prediction. In \nInternational Conference on Machine Learning, Aug 6-11, 2017. Sydney, Australia. 1414-1423. \n[178] Anpeng Wu, Kun Kuang, Junkun Yuan, Bo Li, Runze Wu, Qiang Zhu, Yueting Zhuang and Fei Wu. 2020. Learning decomposed representation for \ncounterfactual inference. arXiv:2006.07040. Retrieved from https://arxiv.org/abs/2006.07040. \n[179] Hao Zou, Peng Cui, Bo Li, Zheyan Shen, Jianxin Ma, Hongxia Yang and Yue He. 2020. Counterfactual prediction for bundle treatment. In Advances \nin Neural Information Processing Systems, 2020. 19705-19715. \n[180] Haotian Wang, Wenjing Yang, Longqi Yang, Anpeng Wu, Liyang Xu, Jing Ren, Fei Wu and Kun Kuang. 2022. Estimating individualized causal \neffect with confounded instruments. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. 1857-\n1867. \n[181] Shachi Deshpande, Kaiwen Wang, Dhruv Sreenivas, Zheng Li and Volodymyr Kuleshov. 2022. Deep multi-modal structural equations for causal \neffect estimation with unstructured proxies. In Advances in Neural Information Processing Systems, 2022. 10931-10944. \n[182] Harsh Parikh, Carlos Varjao, Louise Xu and Eric Tchetgen Tchetgen. 2022. Validating causal inference methods. In International conference on \nmachine learning, 2022. 17346-17358. \n[183] Ziqi Xu, Debo Cheng, Jiuyong Li, Jixue Liu, Lin Liu and Kui Yu. 2023. Causal Inference with Conditional Front-Door Adjustment and Identifiable \nVariational Autoencoder. arXiv:2310.01937. Retrieved from https://arxiv.org/abs/2310.01937. \n[184] Jeffrey Wooldridge. 2009. Should instrumental variables be used as matching variables. Citeseer.  \n[185] Judea Pearl. 2012. On a class of bias-amplifying variables that endanger effect estimates. arXiv:1203.3503. Retrieved from \nhttps://arxiv.org/abs/1203.3503. \n[186] Olav ReiersÃ¸l. 1945. Confluence analysis by means of instrumental sets of variables. Almqvist & Wiksell.  \n[187] Pingchuan Ma, Rui Ding, Haoyue Dai, Yuanyuan Jiang, Shuai Wang, Shi Han and Dongmei Zhang. 2022. Ml4s: Learning causal skeleton from \nvicinal graphs. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. 1213-1223. \n[188] Haoyue Dai, Rui Ding, Yuanyuan Jiang, Shi Han and Dongmei Zhang. 2023. Ml4c: Seeing causality through latent vicinity. In Proceedings of the \n2023 SIAM International Conference on Data Mining (SDM), 2023. 226-234. \n[189] Lars Lorch, Scott Sussex, Jonas Rothfuss, Andreas Krause and Bernhard SchÃ¶lkopf. 2022. Amortized inference for causal structure learning. In \nAdvances in Neural Information Processing Systems, 2022. 13104-13118. \n[190] Jannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Thomas Rainforth and Yarin Gal. 2021. Self-attention between datapoints: Going beyond \nindividual input-output pairs in deep learning. In Advances in Neural Information Processing Systems, 2021. 28742-28756. \n[191] Nan Rosemary Ke, Silvia Chiappa, Jane Wang, Anirudh Goyal, Jorg Bornschein, Melanie Rey, Theophane Weber, Matthew Botvinic, Michael \nMozer and Danilo Jimenez Rezende. 2022. Learning to induce causal structure. arXiv:2204.04875. Retrieved from https://arxiv.org/abs/2204.04875. \n[192] Nick Pawlowski, Daniel Coelho de Castro and Ben Glocker. 2020. Deep structural causal models for tractable counterfactual inference. In Advances \nin Neural Information Processing Systems, Dec 6-12, 2020. 857-869. \n[193] Tomas Geffner, Javier Antoran, Adam Foster, Wenbo Gong, Chao Ma, Emre Kiciman, Amit Sharma, Angus Lamb, Martin Kukla and Nick \nPawlowski. 2022. Deep end-to-end causal inference. arXiv:2202.02195. Retrieved from https://arxiv.org/abs/2202.02195. \n[194] Jiaqi Zhang, Joel Jennings, Cheng Zhang and Chao Ma. 2023. Towards causal foundation model: on duality between causal inference and attention. \narXiv:2310.00809. Retrieved from https://arxiv.org/abs/2310.00809. \n[195] Emre KÄ±cÄ±man, Robert Ness, Amit Sharma and Chenhao Tan. 2023. Causal reasoning and large language models: Opening a new frontier for \ncausality. arXiv:2305.00050. Retrieved from https://arxiv.org/abs/2305.00050. \n[196] Cheng Zhang, Stefan Bauer, Paul Bennett, Jiangfeng Gao, Wenbo Gong, Agrin Hilmkil, Joel Jennings, Chao Ma, Tom Minka and Nick Pawlowski. \n2023. \nUnderstanding \ncausality \nwith \nlarge \nlanguage \nmodels: \nFeasibility \nand \nopportunities. \narXiv:2304.05524. \nRetrieved \nfrom \nhttps://arxiv.org/abs/2304.05524. \n[197] Hengrui Cai, Shengjie Liu and Rui Song. 2023. Is Knowledge All Large Language Models Needed for Causal Reasoning? arXiv:2401.00139. \nRetrieved from https://arxiv.org/abs/2401.00139. \n[198] Stephanie Long, Alexandre PichÃ©, Valentina Zantedeschi, Tibor Schuster and Alexandre Drouin. 2023. Causal discovery with language models as \nimperfect experts. arXiv:2307.02390. Retrieved from https://arxiv.org/abs/2307.02390. \n[199] Thomas Jiralerspong, Xiaoyin Chen, Yash More, Vedant Shah and Yoshua Bengio. 2024. Efficient causal graph discovery using large language \nmodels. arXiv:2402.01207. Retrieved from https://arxiv.org/abs/2402.01207. \n[200] Ahmed Abdulaal, Nina Montana-Brown, Tiantian He, Ayodeji Ijishakin, Ivana Drobnjak, Daniel C Castro and Daniel C Alexander. 2023. Causal \nModelling Agents: Causal Graph Discovery through Synergising Metadata-and Data-driven Reasoning. In The Twelfth International Conference on \nLearning Representations, 2023. 1-52. \n[201] Stephanie Long, Tibor Schuster, Alexandre PichÃ©, UniversitÃ© de Montreal and ServiceNow Research. 2023. Can large language models build causal \ngraphs? arXiv:2303.05279. Retrieved from https://arxiv.org/abs/2303.05279. \n[202] Ruibo Tu, Chao Ma and Cheng Zhang. 2023. Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis. \narXiv:2301.13819. Retrieved from https://arxiv.org/abs/2301.13819. \n[203] Ruoqi Liu, Pin-Yu Chen and Ping Zhang. 2024. CURE: A deep learning framework pre-trained on large-scale patient data for treatment effect \nestimation. Patterns 5, 6 (2024), 1-12. \n35 \n[204] Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona Diab and Bernhard SchÃ¶lkopf. 2023. Can large \nlanguage models infer causation from correlation? arXiv:2306.05836. Retrieved from https://arxiv.org/abs/2306.05836. \n[205] Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, LYU Zhiheng, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner and \nMrinmaya Sachan. 2023. Cladder: Assessing causal reasoning in language models. In Thirty-seventh Conference on Neural Information Processing \nSystems, 2023. 1-28. \n[206] David Freedman and Paul Humphreys. 1999. Are there algorithms that discover causal structure? Synthese 121, 1 (1999), 29-54. \n[207] Thomas C Williams, Cathrine C Bach, Niels B Matthiesen, Tine B Henriksen and Luigi Gagliardi. 2018. Directed acyclic graphs: a tool for causal \nstudies in paediatrics. Pediatric research 84, 4 (2018), 487-493. \n[208] Shohei Shimizu, Patrik O Hoyer, Aapo HyvÃ¤rinen, Antti Kerminen and Michael Jordan. 2006. A linear non-Gaussian acyclic model for causal \ndiscovery. Journal of Machine Learning Research 7, 10 (2006), 2003-2030. \n[209] Ahmed Alaa and Mihaela Van Der Schaar. 2019. Validating causal inference models via influence functions. In Proceedings of the 36th International \nConference on Machine Learning, Jun 9-15, 2019. 191-201. \n[210] Yu Luo, David A Stephens, Daniel J Graham and Emma J McCoy. 2021. Bayesian doubly robust causal inference via loss functions. \narXiv:2103.04086. Retrieved from https://arxiv.org/abs/2103.04086. \n[211] Moritz Willig, Matej ZeÄeviÄ‡, Devendra Singh Dhami and Kristian Kersting. 2021. The Causal Loss: Driving Correlation to Imply Causation. \nRetrieved from https://arxiv.org/abs/2110.12066. \n[212] Tanmayee Narendra, Anush Sankaran, Deepak Vijaykeerthy and Senthil Mani. 2018. Explaining deep learning models using causal inference. \narXiv:1811.04376. Retrieved from https://arxiv.org/abs/1811.04376. \n[213] Ãlvaro Parafita and Jordi VitriÃ . 2019. Explaining visual models by causal attribution. In 2019 IEEE/CVF International Conference on Computer \nVision Workshop (ICCVW), Oct 27-28, 2019. Seoul, Korea. 4167-4175. \n[214] Martin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani and David Lopez-Paz. 2019. Invariant risk minimization. arXiv:1907.02893. Retrieved from \nhttps://arxiv.org/abs/1907.02893. \n[215] Divyat Mahajan, Shruti Tople and Amit Sharma. 2021. Domain generalization using causal matching. In International Conference on Machine \nLearning, Jul 18-24, 2021. 7313-7324. \n[216] Yue He, Zimu Wang, Peng Cui, Hao Zou, Yafeng Zhang, Qiang Cui and Yong Jiang. 2022. CausPref: Causal Preference Learning for Out-of-\nDistribution Recommendation. In Proceedings of the ACM Web Conference 2022, Apr 25-29, 2022. Lyon France 410-421. \n[217] Cheng Zhang, Kun Zhang and Yingzhen Li. 2020. A causal view on robustness of neural networks. In Advances in Neural Information Processing \nSystems, 2020. 289-301. \n[218] Sreya Francis, Irene Tenison and Irina Rish. 2021. Towards causal federated learning for enhanced robustness and privacy. arXiv:2104.06557. \nRetrieved from https://arxiv.org/abs/2104.06557. \n[219] Haiteng Zhao, Chang Ma, Xinshuai Dong, Anh Tuan Luu, Zhi-Hong Deng and Hanwang Zhang. 2022. Certified Robustness Against Natural \nLanguage Attacks by Causal Intervention. arXiv:2205.12331. Retrieved from https://arxiv.org/abs/2205.12331. \n[220] Yiquan Wu, Kun Kuang, Yating Zhang, Xiaozhong Liu, Changlong Sun, Jun Xiao, Yueting Zhuang, Luo Si and Fei Wu. 2020. De-biased courtâ€™s \nview generation with causality. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Nov 16-20, \n2020. 763-780. \n[221] Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla and Anupam Datta. 2020. Gender bias in neural natural language processing. Springer.  \n[222] Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly, Ed H Chi and Alex Beutel. 2019. Counterfactual fairness in text classification through \nrobustness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, 2019. 219-226. \n \n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-11-07",
  "updated": "2024-07-30"
}