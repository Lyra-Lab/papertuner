{
  "id": "http://arxiv.org/abs/2211.03464v2",
  "title": "A Survey on Quantum Reinforcement Learning",
  "authors": [
    "Nico Meyer",
    "Christian Ufrecht",
    "Maniraman Periyasamy",
    "Daniel D. Scherer",
    "Axel Plinge",
    "Christopher Mutschler"
  ],
  "abstract": "Quantum reinforcement learning is an emerging field at the intersection of\nquantum computing and machine learning. While we intend to provide a broad\noverview of the literature on quantum reinforcement learning - our\ninterpretation of this term will be clarified below - we put particular\nemphasis on recent developments. With a focus on already available noisy\nintermediate-scale quantum devices, these include variational quantum circuits\nacting as function approximators in an otherwise classical reinforcement\nlearning setting. In addition, we survey quantum reinforcement learning\nalgorithms based on future fault-tolerant hardware, some of which come with a\nprovable quantum advantage. We provide both a birds-eye-view of the field, as\nwell as summaries and reviews for selected parts of the literature.",
  "text": "A Survey on Quantum Reinforcement Learning\nNico Meyer, Christian Ufrecht, Maniraman Periyasamy, Daniel D. Scherer, Axel Plinge,\nand Christopher Mutschler\nFraunhofer IIS, Fraunhofer Institute for Integrated Circuits IIS, Nuremberg, Germany\n{firstname.lastname|daniel.scherer2}@iis.fraunhofer.de\nJanuary 1, 2024\nAbstract\nQuantum reinforcement learning is an emerging field at the intersection of quantum com-\nputing and machine learning. While we intend to provide a broad overview of the literature\non quantum reinforcement learning – our interpretation of this term will be clarified below –\nwe put particular emphasis on recent developments. With a focus on already available noisy\nintermediate-scale quantum devices, these include variational quantum circuits acting as func-\ntion approximators in an otherwise classical reinforcement learning setting. In addition, we sur-\nvey quantum reinforcement learning algorithms based on future fault-tolerant hardware, some\nof which come with a provable quantum advantage. We provide both a birds-eye-view of the\nfield, as well as summaries and reviews for selected parts of the literature.\nContents\n1\nIntroduction and Overview\n2\n2\nClassical Reinforcement Learning\n3\n3\nThe Quantum Computing Paradigm\n7\n4\nQuantum Reinforcement Learning Algorithms\n11\n4.1\nQuantum-Inspired Reinforcement Learning based on Amplitude Amplification . . . .\n13\n4.2\nQuantum Reinforcement Learning with Variational Quantum Circuits\n. . . . . . . .\n15\n4.2.1\nValue-Function Approximation . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n4.2.2\nPolicy Approximation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.2.3\nCombined Approximations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n4.2.4\nOffline Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n4.2.5\nAlgorithmic and Conceptual Extensions . . . . . . . . . . . . . . . . . . . . .\n42\n4.2.6\nApplication-Focused Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n4.3\nProjective Simulation for Quantum Reinforcement Learning . . . . . . . . . . . . . .\n52\n4.4\nBoltzmann Machines for Quantum Reinforcement Learning\n. . . . . . . . . . . . . .\n54\n4.5\nQuantum Policy and Value Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n4.6\nQuantum Reinforcement Learning with Oracularized Environments . . . . . . . . . .\n60\n5\nOutlook\n64\n1\narXiv:2211.03464v2  [quant-ph]  8 Mar 2024\n1\nIntroduction and Overview\nWith recent advances in the fabrication and control of hardware for quantum information processing,\nthe possibilities of merging quantum computing (QC) with machine learning (ML) have received a\nhuge amount of attention within the growing research community. Hereby, reinforcement learning\n(RL) is the third paradigm besides supervised and unsupervised learning. In this survey article,\nwe provide an overview over so-called quantum reinforcement learning (QRL) algorithms.\nWe\nunderstand these as quantum-assisted approaches, that solve a particular task (be they classical or\nquantum in nature) by employing quantum resources (either in simulation and/or in experiment).\nIn order to keep this contribution as self-contained as possible, we provide the necessary back-\ngrounds before venturing into the QRL literature. We start out with a brief recap of the essentials\nof the RL paradigm in the fully classical setting in Sec. 2. Further, in Sec. 3 we provide a quick\nintroduction to QC and variational quantum circuits (VQCs). Readers familiar with either of the\ntopics may safely skip these sections.\nIn Sec. 4 we turn our attention to the emerging field of QRL, starting out with a quick overview\nof the literature. Then we delve into summaries of the most prominent contributions. This se-\nlection is necessarily subjective and reflects our own research interests – overall we identified 177\nrelevant manuscripts, of which we reviewed 120 explicitly. For a detailed overview on paper counts\nsee Tab. 1. We organized our summaries into several blocks, that are ordered by what one could call\nan increasing degree of ‘quantiziation’. The first of these blocks in Sec. 4.1 covers what we refer to as\n‘quantum-inspired’ RL algorithms. The second block in Sec. 4.2 takes a rather detailed look at QRL\nalgorithms that employ so-called VQCs as function approximators. In many cases, the correspond-\ning algorithms are obtained by simply replacing a standard neural network function approximator\n(or any other sort) by an appropriate VQC. We provide detailed summaries for most papers in this\nFigure 1: A possible classification matrix for QRL algorithms, where we took into account only\nthose variants of QRL which we focus on in Sec. 4. The algorithm classes are ordered according to\ntheir degree of quantum-classical hybridization, ranging from purely classical to purely quantum. A\nmore detailed review of the 22 selected works on quantum-inspired reinforcement learning (QiRL)-\nalgorithms can be found in Sec. 4.1. VQC-based approaches are summarized in quite some detail\nin Sec. 4.2 – comprising of 68 papers. QRL-algorithms employing post-noisy intermediate-scale\nquantum (NISQ) quantum algorithms as subroutines or even fully quantum approaches to QRL are\ndescribed in Sec. 4.3, Sec. 4.4, Sec. 4.5 and Sec. 4.6, based on 30 selected manuscripts. The dashed\nvertical line between classical and NISQ compute resources indicates that presently it is unclear\nwhether QRL with NISQ-compatible algorithms offers robust quantum advantage on a broad range\nof learning problems. The solid vertical line distinguishes post-NISQ algorithms from both classical\nand NISQ-compatible algorithms, as they typically come with guaranteed quantum advantage (at\nleast relative to their classical counterparts).\n2\n≤2018\n2019\n2020\n2021\n2022\n2023\nΣ\nQuantum-inspired QRL\n12\n1\n2\n5\n2\n0\n22\nVQC-based QRL\n0\n2\n2\n9\n21\n34\n68\nQRL applicationa\n0\n0\n0\n1\n7\n18\n26\nPost-NISQ QRL\n12\n2\n2\n6\n4\n4\n30\naThe QRL application papers are VQC-based and also counted towards that category.\nTable 1: The year-wise paper count for the different classes of QRL. The data not necessarily reflects\npublication, but rather the first public availability, e.g. via preprint servers.\ncategory, as variational quantum algorithms are believed to offer the potential to obtain quantum\nadvantage despite the limitations of present day NISQ hardware. In Secs. 4.3 and 4.4, we take a look\nat realizations of QRL based on so-called projective simulation and the use of Boltzmann machines\nas function approximators, respectively. In Sec. 4.5 we move to a class of approaches that employ\nquantum algorithms as subroutines. The corresponding hardware requirements will likely be com-\npatible only with universal, fault-tolerant and error-corrected quantum processing units (QPUs).\nFinally, Sec. 4.6 provides a summary for a formal approach to QRL, which treats all components of\nRL ‘quantumly’. From our point of view, the highest degree of quantization can thus be found in\nthese approaches. Fig. 1 gives an overview of the QRL literature as understood in this survey.\nFinally, in Sec. 5 we state our concluding thoughts on the current state-of-the-art of QRL. Before\nmoving to more technical content, we would like to express our hope that this literature survey on\nQRL will be of use to colleagues and collaborators and the wider QC research community.\nIt\nrepresents our effort to familiarize ourselves with QRL and its main research directions.\n2\nClassical Reinforcement Learning\nCompared to the methods of supervised and unsupervised learning, which are typically implemented\nas passive learning, RL falls into the class of interaction-based learning [SB18]. On an abstract level,\nthe learner interacts with its environment, the state of which it can either fully or only partially\nobserve through a corresponding observation obtained after executing an action according to an\nunderlying policy. In the RL paradigm, the learner is therefore appropriately referred to as an\nagent: it can - be it in simulation or in the real world - interact with its environment according\nto its abilities.\nThe aim of RL is to learn a policy through the interaction of the agent with\nthe environment, which is optimal with regard to a reward adapted to the problem.\nIn other\nwords, the agent should find an optimal policy during the learning process in the abstract space\nof all policies, which maximizes the expected cumulative reward.\nThe theoretical basis for RL\nis formed by so-called Markov decision processes (MDPs) and the associated Bellman equation,\nwhich represents a consistency equation for the so-called value function. In turn, an optimal policy\ncan be extracted from the optimal value function. Alternatively, the optimal policy can also be\nlearned directly. Under certain conditions, the elements of RL can be mapped to their respective\nequivalents in control theory, where typically a dynamic optimization problem is solved by gradient-\nbased methods with simulation of the corresponding model dynamics. On the RL side, there are\nboth model-based and model-free approaches. The model-free approach in particular is one of the\nstrengths of the RL method, since in many cases state and action spaces are too high-dimensional\nto design realistic dynamical models and simulate them to efficiently find optimal control strategies.\nThe large dimensions of the spaces that occur in realistic problems make the use of approximation\nmethods for the value function necessary.\nDriven by the breakthroughs in deep learning (DL),\nartificial neural networks (NNs) have established themselves as function approximators for both\nvalue function and policy (understood as a deterministic or probabilistic mapping of states to\n3\nactions), thus establishing the field of deep reinforcement learning (DRL).\nIn the following, we will introduce the various notions pertaining to RL in a more formal way\nand provide the background necessary to understand the basic RL terminology. In an RL scenario,\nthe algorithm, also referred to as agent, generates its own data by interacting with an environment.\nThis interaction happens over some discrete timesteps t, which are accumulated to episodes with\neither finite or infinite horizon. In each timestep, the agent is able to make an observation st ∈S\nof the environment. Based on this state information, an action at ∈A acting on the environment\nis selected according to a policy. Based on the (usually unknown) environment dynamics, the next\nstate st+1 ∈S is observed from the environment and the agent receives a reward rt ∈R for its\nchoice. The agent should select the actions in such a way that some objective is optimized, usually\nrelated to the long term reward. A sketch of this pipeline can be found in Fig. 2. In this survey\narticle, we follow the formalism and notation of Sutton et al. [SB18], with small adaptions wherever\nwe feel that it eases comprehension.\nFigure 2: Interaction between agent and environment for one timestep of a RL task.\nReinforcement Learning as a Markov Decision Process\nMore formally, this setup is usually\ndescribed as an MDP. A finite MDP is a 5-tuple (S, A, R, p, γ), where the sets S, A and R are\nfinite. It is defined by the following components:\n• A set of states S the agent can observe from the environment\n• A set of actions A the agent can execute in the environment\n• A set of rewards R ⊂R the agent can receive from the environment\n• The environment dynamics p : S × R × A × S →[0, 1]; The value p(s′, r|s, a) := Pr{st+1 =\ns′, rt = r|st = s, at = a} gives the probability that the environment transitions to state st+1\nand the agent receives reward rt, if the agents executes action at in state st at time t.\n• The discount factor 0 ≤γ ≤1, more on this below;\nThe dynamics of the environment are often not accessible to the agent, otherwise the task collapses\nto (not necessarily trivial) dynamic programming.\nThe function p satisfies the properties of a\nprobability density function (PDF), i.e., it holds P\ns′∈S,r∈R p(s′, r|s, a) = 1, for all choices of s ∈S\nand a ∈A. According to the Markov property, the dynamics are completely described by p, i.e., the\nconsecutive state st+1 and reward rt depend solely on the directly preceding state st and action at.\nWith this framework in mind, the interaction between agent and environment can be described\nas a trajectory τ. For a finite or infinite horizon H, one episode is therefore given by the sequence\nτ = [s0, a0, r0, s1, a1, r1, s2, · · · , sH−1, aH−1, rH−1] ,\n(1)\nwith st ∈S, at ∈A, and rt sampled following the environment dynamics for each timestep t.\n4\nLong Term Reward as Objective\nThe agent gets feedback from the environment through the\nimmediate rewards rt. However, instead of maximizing these short-term rewards, it is much more\nappropriate to use some long term measure as objective. A natural choice is to go for the cumulative\nreward, also referred to as the expected return\nGt := rt + rt+1 + rt+2 + · · · + rH−1.\n(2)\nFor episodic tasks (H < ∞) it is often desirable and for continuous tasks (H = ∞) it is necessary\nto use a discount factor γ. This leads to the discounted (expected) return\nGt :=\nH−1\nX\nt′=t\nγt′−t · rt′,\n(3)\nwhere each choice of γ defines a different MDP. For γ < 1 the value of Gt is guaranteed to be finite\nand emphasis on individual rewards decreases with distance from the current time-step. For γ = 0\nthe sum reduces to just the immediate reward, so an appropriate choice of this hyperparameter is\ncrucial for the potential success of the RL agent.\nPolicy, Value Functions and Optimality\nIn order to describe a meaningful RL setup, there\nare still some concepts missing. As described above, the agent needs to decide for an action in every\ntimestep, depending on the state information that is observed. This decision making process can\nbe understood as a (stochastic) policy\nπ (a|s) := Pr{at = a|st = s},\n(4)\nwhere P\na∈A π (a|s) = 1 holds for all s ∈A. The overall task of RL is to derive an optimal policy\nπ∗w.r.t. some metric.\nA suitable tool to define optimality and also to simplify updates is the notion of value functions.\nThe state value function of state s under the current policy π is defined as\nVπ(s) := Eπ [Gt|st = s] .\n(5)\nIt describes the expected returns when starting in state s and following policy π from there on,\nwith the value for a terminal state always zero by definition. It can be interpreted as a measure of\nhow good it is to be in a certain state, where quality is measured w.r.t. expected return. Explicitly\nseparating the first step in the definition above gives rise to the Bellman (expectation) equation\nVπ(s) =\nX\na∈A\nπ (a|s)\nX\ns′∈S,r∈R\np\n\u0000s′, r|s, a\n\u0001 \u0002\nr + γ · Vπ(s′)\n\u0003\n,\n(6)\nfor all s ∈S. Consequently, the value function Vπ can be viewed as the unique solution to this\nBellman equation. Alternatively, one can define the state-action value function as the expected\nreturn when starting in state s, executing action a, and following policy π from there on. It is\ndefined as\nQπ(s, a) := Eπ [Gt|st = s, at = a] ,\n(7)\nfor all s ∈S and a ∈A. It is straightforward to see that it holds Vπ(s) = P\na∈A π (a|s) Qπ(s, a) for\nall s ∈S. This identity can be used to give the Bellman equation for the state-action value function\nas Qπ(s, a) = P\ns′∈S,r∈R p(s′, r|s, a)\n\u0002\nr + γ · P\na′∈A π(a′|s′)Qπ(s′, a′)\n\u0003\n.\nThe value function allows to explicitly define and evaluate the quality of policies, i.e., the policy\nπ is better or equal to another policy π′, iff Vπ(s) ≥Vπ′(s) for all s ∈S. If a policy is better or\nequal to all others, it is considered an optimal policy π∗. All optimal policies share the same optimal\nstate-value function\nVπ∗(s) := V ∗(s) := max\nπ\nVπ(s),\n(8)\n5\nfor all s ∈S. A similar notion of optimality for the action-value function is given by\nQ∗(s, a) := max\nπ\nQπ(s, a),\n(9)\nfor all s ∈S and a ∈A.\nIt is straightforward to formulate the connection of both quantities\nas V ∗(s) = max\nπ\n\u0000P\na∈A π (a|s) Qπ(s, a)\n\u0001\n= max\na∈A Q∗(s, a). With this it is possible to derive the\nBellman optimality equation for the value function as\nV ∗(s) = max\na∈A\nX\ns′∈S,r∈R\np\n\u0000s′, r|s, a\n\u0001 \u0002\nr + γ · V ∗(s′)\n\u0003\n,\n(10)\nfor all s ∈S. Using the stated connection this can be reformulated to extend to the state-action\nvalue function as Q∗(s, a) = P\ns′∈S,r∈R p(s′, r|s, a)\n\u0014\nr + γ · max\na′∈A Q∗(s′, a′)\n\u0015\nfor all s ∈S and a ∈A.\nSolving and Approximating the Bellman Equation\nOne topic that has to be addressed\nis the actual representation of the policy and value functions. The most intuitive approach is to\njust store the values for all state-action pairs in a table, also referred to as the tabular approach.\nWhile this formulation offers nice convergence and optimality guarantees for several scenarios, it\nhas some serious drawbacks. Most prominently, it is intractable once the state-action space gets to\nlarge, which is the case for most real-world problems. A workaround is to use parametric function\napproximators, which results in the parameterized functions πθ, Vθ, or Qθ, respectively. The typical\nchoice is a NN [HSW89], in Sec. 4.2 the usage of VQCs for this task is considered from several\nangles. As there now is an approximation in the defining quantities, also convergence guarantees\nare much less straightforward than for the tabular case. The remaining parts of this section can be\nunderstood both for the tabular and parameterized case, although details might vary a bit.\nThe Bellman optimality equation offers a tool to derive an optimal policy. It has to be noted\nthat the given formulation makes use of the environment dynamics p. Therefore, solution methods\nsolving the equation with dynamic programming are referred to as model-based.\nThe two most\nprominent examples include value iteration [Bel57] and policy iteration [RN94; PRD96].\nThere is also a whole range of model-free approaches, where the agent does not make use of any\nmodel that represents the environment dynamics. Instead, all information is directly acquired by\ninteraction with the environment. One prominent representative is the Q-learning approach [WD92],\nwhich basically is an approximation of Q-value iteration using samples. Starting with a random\ninitialization, the update rule\nQ(s, a) ←Q(s, a) + α\n\u0012\nrt + γ · max\na′∈A Q(s′, a′) −Q(s, a)\n\u0013\n(11)\ndirectly derives from the Bellman equation, where α is a learning rate hyperparameter. The policy\nis usually defined to act epsilon-greedily w.r.t. the current action-value function, i.e.\nπ(s) :=\n\n\n\narg max\na∈A\nQ(s, a)\nwith probability 1 −ε,\nuniformly at random from A\nwith probability ε.\n(12)\nAn alternative approach is the policy gradient idea [Sut+99], which directly aims to learn the policy.\nBased on an parameterized policy πθ, it performs updates\nθ ←θ + α∇θJ(θ)\n(13)\nvia gradient ascent, where J(θ) is a performance measure, usually J(θ) = Vπθ(s0). Unfortunately\nthe desired gradient likely depends on some environment dynamics, which are not known. The policy\n6\ngradient theorem [Sut+99] describes a quantity proportional to ∇θVπθ, which is easier to obtain. It\nis given by\n∇θVπθ(s0) ∝\nX\ns∈S\nµ(s)\nX\na∈A\nQπθ(s, a)∇θπθ (a|s) ,\n(14)\nwhere µ(s) is a function that expresses the fraction of time that is spend in state s. An concrete\ninstance of this idea is the REINFORCE algorithm [Wil92], where a Monte Carlo method is used\nto estimate the quantity described in the equation above. Furthermore, the training procedure can\nbe stabilized by introducing a suitable baseline function that reduces the variance of the expected\nreturn [Zha+11].\nOverall, there are several extensions and modifications of the described concepts. One method\nworth mentioning is the actor-critic approach [KT03], which combines ideas form policy gradient\nand value functions. As for smaller modifications, there is double Q-learning, which introduces\nan additional target action-value function to reduce some bias caused by the standard Q-learning\nprocedure [Has10]. Similarly, the introduction of an experience replay buffer [Lin92] should improve\nstability and sample efficiency. This finally leads to offline or batch RL [EGW05], where the agent\nis not allowed to directly interact with the environment. Instead, it only has access to a set of\npreviously collected experiences. This formulation is especially relevant in practice, as generating\ndata is sometimes quite expensive. There is still a wide range of topics this summary did not touch.\nWhere necessary, additional details will also be introduced in the upcoming chapters. For a more\nbroad introduction to the topic one can refer to Ref. [SB18], more recent developments are e.g.\nreviewed in Refs. [Aru+17; NLH20].\n3\nThe Quantum Computing Paradigm\nThe foundations of QC were established at the beginning 20th century when the modern theory\nof quantum physics was developed. Benioff and Feynman proposed the idea of taking advantage\nof quantum mechanical systems for computing in the early 1980s [Ben80; Fey82]. QC challenges\nthe strong Church-Turing hypothesis, as it potentially provides efficient solutions to classically\nintractable problems [NL16]. This section gives a pragmatic introduction to the basics of QC, and\nalso provides an extension to quantum machine learning (QML) (here understood as ML with VQCs\nas a new class of models) with a focus on QRL.\nSingle and Multi-Qubit Systems\nSimilar to RL, notation and conventions regarding quantum\ncomputing vary quite a bit throughout the literature. Regarding notation, we closely follow the\ntextbook by Nielsen and Chuang [NL16].\nFor the moment, let us consider the basic unit of information for classical information processing.\nA single bit is either in state 0 or state 1, consequently, a sequence of n bits can represent 2n unique\nvalues. Obviously, the bit register can only be in one of these 2n states at any point in time.\nA qubit is the quantum version of a bit. We use the Dirac notation [NL16] to define |0⟩and\n|1⟩as two distinct, orthogonal states of the qubit system. These basis states span a 2-dimensional\nHilbert space H ∼= C2, which contains all 1-qubit (pure) quantum states. The qubits are subject to\nthe laws of quantum mechanics and can be realized with, e.g., spin systems of subatomic particles\n[PMV02], ion traps [BBA14], neutral atoms [SWM10], or superconducting circuits [YN06]. This\ngives rise to some interesting properties. In fact, a qubit can not only be in either state |0⟩or |1⟩,\nbut in a superposition of both. An arbitrary 1-qubit state is given as\n|ψ⟩= α |0⟩+ β |1⟩.\n(15)\nThe amplitudes α and β are complex numbers, which must satisfy |α|2 + |β|2 = 1. To get a nice\n7\nFigure 3: Bloch sphere representation of a 1-qubit state.\nvisual representation, Eq. (15) can be reformulated as\n|ψ⟩= eiγ\n\u0012\ncos θ\n2 |0⟩+ eiϕ sin θ\n2 |1⟩\n\u0013\n,\n(16)\nwith γ, θ, ϕ ∈R. As any global phase has no observable effect [NL16], the prefactor eiγ in Eq. (16)\ncan be omitted. This representation makes it possible to visualize the state of a 1-qubit system on\nthe surface of the Bloch sphere, see Fig. 3. The north and south poles w.r.t. the z-axis correspond to\nthe basis states |0⟩and |1⟩, which are also referred to as computational basis states of a single qubit.\nAnother, less commonly used basis is given by the poles w.r.t. the x-axis, the elements are related\nby |+⟩= |0⟩+|1⟩\n√\n2\nand |−⟩= |0⟩−|1⟩\n√\n2 . Similarly, one could also use |R⟩= |0⟩+i|1⟩\n√\n2\nand |L⟩= |0⟩−i|1⟩\n√\n2\n.\nAn alternative representation associates quantum states with amplitude vectors:\n|0⟩→\n\u00141\n0\n\u0015\nand |1⟩→\n\u00140\n1\n\u0015\n(17)\nMultiple-qubit systems are the point where things get interesting. An n-qubit system gives\naccess to the 2n-dimensional Hilbert space, in which an arbitrary pure quantum state is defined as\n|ψ⟩= c0 |00 · · · 00⟩+ c1 |00 · · · 01⟩+ · · · + c2n−1 |11 · · · 11⟩,\n(18)\nwith ci ∈C and P2n−1\ni=0\n|ci|2 = 1. The basis states, e.g. |00 · · · 01⟩= |0⟩⊗|0⟩⊗· · ·⊗|0⟩⊗|1⟩, consist\nof tensor products of the individual qubits. The state |ψ⟩→[c0, c1, · · · , cN−1]t possesses N = 2n\ncomplex amplitudes, whose absolute squared values must sum up to one. Due to the principle of\nsuperposition, an n-qubit system is able to encode and process information scaling in O (2n), while\nfor a classical setting, it is limited to O (n).\nEvolution of Closed Quantum Systems\nIn order for computation to be possible, there must\nbe some method to manipulate quantum states. Exactly this is achieved by operators acting on the\nHilbert space H. By definition, all operators, which describe the time evolution of a closed quantum\nsystem are reversible. Hence, they can be represented as unitary matrices, i.e., for an operator U it\nmust hold that U†U = I. This constraint also conveys length preserving properties, i.e., applying a\nunitary operator to a quantum state will again yield a valid quantum state satisfying Eq. (18).\nIn the following, explicit matrix representations of operators are specified in the computational\nbasis. Starting simple, consider the bit-flip operator σx. This operator just flips the amplitudes\nof the |0⟩and |1⟩basis state, on the Bloch sphere this is equivalent to a rotation by π about the\n8\nx-axis. The corresponding operators also exist for y-axis and z-axis, in matrix notation those are\ngiven as\nX := σx =\n\u00140\n1\n1\n0\n\u0015\n,\nY := σy =\n\u00140\n−i\ni\n0\n\u0015\n,\nZ := σz =\n\u00141\n0\n0\n−1\n\u0015\n.\n(19)\nFigure 4: Circuit symbols of various quantum operators (gates).\nAllowing an additional degree of freedom, one can define an operator for arbitrary rotation with\nθ about axis i as\nRi(θ) = e−i θ\n2 σi,\nfor i ∈{x, y, z}.\n(20)\nThe last 1-qubit operator we introduce is the Hadamard matrix:\nH =\n1\n√\n2\n\u00141\n1\n1\n−1\n\u0015\n,\n(21)\nwhich basically performs a change of basis with H |0⟩= |+⟩and H |1⟩= |−⟩. By employing the\ntensor product for operators, we can extend 1-qubit operators to act on single qubits comprising a\nmulti-qubit system. We now move to genuine multi-qubit operators, acting non-trivially on two or\nmore qubits. For our purposes, the most relevant 2-qubit operators are the controlled X (CX) and\ncontrolled Z (CZ), where one qubit acts as the control and the other as the target. More concretely,\nthe CX-gate flips the amplitudes of the target qubit, iff the control is in state |1⟩. Similar to this,\nthe CZ operator performs a conditional phase flip. The matrix notations are given by\nCX =\n\n\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n\n\nand\nCZ =\n\n\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n−1\n\n.\n(22)\nQuantum circuit diagrams are a nice way to visualize what is going on in a quantum algorithm.\nThe individual qubits are represented as wires, where the order of operators, also called gates, is\ndefined by their relative position. To be more precise, the top wire gets associated with the leftmost\nqubit. A few common circuit symbols for the operators introduced so far are depicted in Fig. 4.\nExtracting Classical Information via Measurements\nIn classical computing, it is trivial to\nobserve the exact states of all bits. For quantum systems, in order to extract information, an ob-\nservable quantity has to be measured. To build the bridge to quantum computing, for each physical\nobservable there exists a Hermitian operator O [NL16], i.e., it holds O† = O. The eigenstates of O\ndefine a basis of the quantum system’s Hilbert space.\n9\nFigure 5: Variational quantum circuit consisting of feature map, variational layer, and measurement.\nOnce an observable O is measured, the corresponding measurement device outputs an eigen-\nvalue of O. The post-measurement state of the system is given by the eigenstate corresponding\nto the eigenvalue that is measured. The most commonly used observable might be Pauli-Z, which\ncorresponds to a measurement in the computational basis for a single qubit, see also Eq. (19). It\nhas eigenvalues λ1 = +1, λ2 = −1 and corresponding eigenstates v1 = [1 0]t, v2 = [0 1]t.\nThe consequences for quantum computing are quite sobering, as observing superpositions w.r.t.\nthe basis defined by the observable is impossible. Rather, one of the postulates of quantum mechan-\nics states the Born rule, which defines a probabilistic relationship between quantum state and mea-\nsurement output. Let |0⟩, |1⟩, ..., |N −1⟩be the basis defined by observable O and c0, c1, ..., cN\nthe corresponding amplitudes of state |ψ⟩expressed in this basis. It holds, that measuring O will\nresult in the measurement outcome λi with probability |ci|2. Consequently, having obtained λi, the\npost-measurement state of the system is |i⟩.\nThe first algorithm claiming provable quantum advantage, i.e., an improvement w.r.t. some\ncomplexity metric compared to any classical approach, was published in 1992 by Deutsch and\nJosza [DJ92] for a specially constructed problem. Most famous might be Shor’s algorithm [Sho97],\nwhich provides an exponential speedup for tasks like prime factorization. Unfortunately, it requires\nlarge-scale, fault-tolerant and error-corrected quantum computers. All current hardware can be\nconsidered NISQ devices, which makes the execution of these algorithms infeasible. Despite this,\nthe first claim of experimental quantum advantage was published just two years ago [Aru+19].\nYet, the considered problem was quite far from general practical applicability. A demonstration\nfor achievable quantum supremacy on a practically relevant problem has still to be given. There\nare some promising candidates like quantum chemistry and material science. Recently, ideas have\nbeen put forward on combining quantum computing and machine learning [Ben+20; SP18]. These\nalgorithms are expected to bypass at least some of the problems with execution on presently available\nNISQ hardware.\nQuantum Machine Learning with Variational Quantum Circuits\nThe research on QML\njust really took off in the last two decades, yet there exists already a variety of approaches. As a\nrough clue, the hoped-for benefit of QML relies, to a large extent, on the access to the high dimen-\nsional Hilbert space granted by quantum systems. Here, we want to briefly collect the background\nfor the summaries of VQC-based QRL approaches in Sec. 4.2.\nQML frequently deals with expectation values of quantum measurements. The expectation value\nof an observable O w.r.t. the quantum state |ψ⟩is denoted as\n⟨O⟩ψ := ⟨ψ |O| ψ⟩.\n(23)\nWhile VQCs define a new class of ML models, one can make the case for the loose analogy to NNs,\nwhere the relation of in- and output depends on a set of weights. An example for a parameterized\nquantum operator is given in Eq. (20). The corresponding gate applies a rotation about a specific\naxis by some angle θ0. Multiple rotation gates form a quantum circuit, where θ summarizes all free\nparameters. Varying these values gives the possibility to determine the evolution of the quantum\nsystem. Let Uθ denote the corresponding unitary. An schematic example of a VQC is displayed in\nFig. 5. Most RL tasks use the concept of states, based on which an informed decision should be\ntaken. This state information is encoded into the quantum system with an appropriate feature map.\nIn general, the inputs s are pre-processed with some mapping function Φ. The results Φ(s) can be\n10\nneatly integrated into the quantum circuit via the unitary UΦ(s). To enhance the expressive power of\nthe VQC, one can use more sophisticated data encoding routines like data re-uploading [Pér+20] or\nincremental data-uploading [Per+22]. Eventually, some observable has to be measured. A common\nchoice is the computational basis with O = Z⊗n. Overall, the output of the VQC-model can be\ndescribed as\n⟨O⟩s,θ =\nD\n0\n\f\f\f\n\u0000UθUΦ(s)\n\u0001† OUθUΦ(s)\n\f\f\f 0\nE\n:=\nD\n0\n\f\f\fU†\ns,θOUs,θ\n\f\f\f 0\nE\n.\n(24)\nFor most tasks, this value is post-processed using some function f. Keeping things as general as\npossible, one can define a loss function L on f\n\u0010\n⟨O⟩s,θ\n\u0011\n(based on the concrete problem at hand).\nThe update of the parameters can be performed using, e.g., gradient-based techniques:\nθ ←θ + α · ∇θL\n\u0010\nf(⟨O⟩s,θ)\n\u0011\n(25)\nThe required gradient can be obtained using the parameter-shift rule [Cro19; Wie+22b], or SPSA-\nbased approximations [Wie+23].\n4\nQuantum Reinforcement Learning Algorithms\nIn QML, there are approaches that either aim to stabilize the coherent function of the QPU using\nML methods, or use the structure of a hybrid variational algorithm for ML purposes. Very often,\nRL is used to generate a solution for a quantum control problem, e.g., to learn quantum error cor-\nrection strategies [Fös+18] or to generate control policies at a lower level [Zha+19; Dal+20]. Other\nwork considers RL as the optimizer of a variational quantum algorithm (VQA) [Kha+19; Kha+20].\nWhile this represents a fascinating research topic in itself, here we will focus on the application of\nQRL algorithms for solving specific tasks, be they classical or quantum. Research in the field of\nQML has so far mostly focused on supervised and unsupervised learning. However, the literature\nalready proposes quite a few theoretical concepts and even some small-scale experimental realiza-\ntions for QRL. Recent developments mostly focus on employing VQCs as function approximators.\nWhen transferring from RL to QRL, i.e., the ‘quantization’ of the RL paradigm, there are various\npossibilities of how quantum computing enters the game. This has led to the development of differ-\nent QRL variants. A few works exist, that review current progress in QRL [KSG21; ML22; Kun22;\nLam23; NHP23] and the more general correspondence of RL and QC [ML21]. There is also recent\nwork towards a fair comparison of RL and QRL in restricted settings [MK21; Fra+22].\nQuantum-Inspired Approaches. The earliest idea for combining RL with a quantum routine relies\non the method of amplitude amplification, as it is used in Grover-type search algorithms [CDC06;\nDon+08b; Don+06b; Che+06; Don+06a; CD08; Don+08a; CD10; CFD12; Fak+13; NGC15;\nLi+20a; Nir+21; LAD21; Yin+21; Hu+21; Ren+22; Cho+23].\nSeveral qubit registers embed\nthe states and actions relevant for the RL system in a suitable Hilbert space.\nStarting from a\nuniform superposition, amplitudes favored by the reward or the value function are selectively am-\nplified. The action selection is based on Born’s rule, i.e., a measurement is carried out on the qubit\nregister with regard to the ‘action-basis’. The algorithm was also investigated independently of\nQPUs [Don+12] and recently further developed [GH19]. An introduction to this concept is also\nprovided in Ref. [Raj+21]. As it turns out, these early variants should rather be considered a set\nof QiRL algorithms, that do not offer an intrinsic potential for quantum advantage. Recently, the\ntechnique was transferred to sampling from the experience replay buffer in Q-learning [Wei+21]. A\nsummary and review of this type of QiRL can be found in Sec. 4.1.\nVQC-Based Function Approximation. In DRL, deep neural networks (DNNs) are employed as pow-\nerful function approximators. Typically, the approximation either happens in policy space (actor),\nin value space (critic), or both, resulting in so-called actor-critic approaches. Recently, VQCs were\n11\nproposed and analyzed in their role as function approximators in the RL setting – an extensive\noverview is provided in Sec. 4.2. On the one hand, this approach basically replaces a more or less\nwell understood heuristic with a poorly understood heuristic. For the quantum heuristic many open\nquestions regarding computational power, scalability and trainability remain. On the other hand,\nVQCs nonetheless have spurred the hope for quantum advantage already with NISQ devices. The\nearliest work in this direction proposed VQC-based approximation in value space, which is covered\nin Sec. 4.2.1. This so-called VQC-based Q-learning was introduced in Ref. [Che+20], and extended\nin Refs. [LS20; LS21; Lok+22; Che23c; CCC23; FP+23; SJD22; Sko+23; LXJ23]. A method to\nefficiently evaluate the Q-function is discussed in Ref. [San+23], which is however not entirely NISQ-\nfeasible. The complimentary approach of approximation in policy space is discussed in Sec. 4.2.2.\nOriginally proposed in Ref. [Jer+21a], several extensions have bee discussed in Refs. [Kun22; BAQ23;\nSSB23; Jer+23; Mey21; Mey+23b; Mey+23a]. Combinations of value and policy approximation\nare covered in Sec. 4.2.3, with (soft) actor-critic approaches in Refs. [Wu+23; Kwa+21; Ree23;\nChe23a; Lan21], and multi-agent formulations in Refs. [Yun+22; YPK23]. The setting of offline\nquantum reinforcement learning is considered in Sec. 4.2.4 by Refs.[Per+23; Che+23a]. A collection\nof algorithmic and conceptual extensions that are relevant for a wide range of approaches is com-\nposed in Sec. 4.2.5, based on Refs. [Che23d; Che23b; Kim+21; Hsi+22; Dră+22; Kru+23; SMT23;\nACN23; PPR20; Che+22; DS23; Köl+23].\nA collection of application-focused work is summa-\nrized in Sec. 4.2.6, comprising Refs. [Acu+22; Hei+22; Cob23; BYK22; SMK23; Hic+23; KCP23;\nCor+23; San+22; ACN22; Liu+23; Kum+23; Rai+23; SH23; RKM22; Yan+22; Par+23a; NS+23;\nPar+23b; PK23; Yun+23; Ans+23; Che+23c; Yan23; CRC23; Che23e].\nProjective Simulation.\nAnother QRL method is based on projective simulation (PS), which in\nthe broadest sense is a particular learning paradigm and similar in spirit to RL [BD12]. Based\non experiences made through interaction with the environment, a memory network is created by\nthe agent. The network has a directed structure with adaptive weights between the nodes of the\nnetwork. The learning process and action selection are based on a random process (more precisely,\na random walk) on the graph of the network, with the transition probabilities between nodes being\ngiven by the respective adaptive weights. PS can be ‘quantized’ by replacing the random walk\nwith a so-called quantum random walk\n[Pap+14; Tei21; TRC21; Mel+17].\nA formal analysis\nof convergence properties was given in Ref. [Boy+20]. In fact, there is already work on a proof-\nof-principle implementation in the laboratory [DFB15; Sri+18] and proposals for quantum-optics\nimplementations [Fla+23]. Possible quantum advantages over classical PS lie in the acceleration of\nthe process of action selection, also referred to as deliberation in the literature. A more detailed\nsummary is provided in Sec. 4.3.\nQuantum Boltzmann Machines. Another line of research proposes to use Boltzmann machines as\nfunction approximators. These models are assumed to be advantageous compared to typical NNs\nin environments with large action spaces. Ref. [Jer+21b] demonstrates, that Boltzmann machines\nare closely related to energy-based models. For specific instances, those allow for a quantum rep-\nresentation, which enables potential quantum speed-up for post-NISQ devices. A similar concept\nis also proposed for the annealing-based QC paradigm [Cra+18; Sch+22; Lev+17]. A summary of\nthese ideas can be found in Sec. 4.4.\nQuantum Subroutines. Another approach to go from RL to QRL replaces certain subroutines in\nexisting RL approaches.\nOne idea is to replace policy or value iteration with some quantum-\nenhanced analogues. While this approach is limited to universal, fault-tolerant and error-corrected\nquantum hardware, several such algorithms have been proposed and analyzed [Wie21; Wie+22a;\nWan+21a; CKP23; Gan+23; Zho+23; GA23].\nMost importantly, these algorithms come with\nguarantees regarding speed-up, compared to their classical counterparts. QRL in these settings\nis often limited to the tabular case and assumes a quantum version of the RL environment, i.e.,\noracle access. Our summaries and reviews can be found in Sec. 4.5.\nFull-Quantum Formulation. An approach which not only ‘quantizes’ certain subroutines, but all\n12\ncomponents of the pipeline, is considered in Refs. [DTB15; DTB16; DTB17; Dun+18].\nExten-\nsions [HDW21; HW22], applied to specific problems [Wan+21b; Wan+23], and small-scale exper-\nimental realizations [Sag+21a] were presented. An alternative route to fully quantized QRL was\ntaken in [Cor18]. For our review of this line of research, see Sec. 4.6.\nVarious Concepts.\nFor the sake of completeness, we mention different approaches found in the\nliterature. We note, however, that we did not pursue a detailed review for those works, typically\nbecause we focused on what we identified as the most considered lines of research. While some of\nthe works listed in the following simply do not fit directly with the learning-based QRL approach,\nfor others it might not seem obvious how to generalize their particular setting to a broader class\nof problems. While quantum algorithms for dynamic programming have been discussed [Ron19;\nAmb+19], it currently remains unclear how to move from dynamic programming to a learning-\nbased approach such as RL. Similarly, quantum algorithms have been employed to solve planning\ntasks [NW05], but again the transfer to a learning-based approach is far from obvious. Closer related\nto the typical RL setting is the task of imitation learning [Che+23b]. A series of papers discussed\nQRL in the setting of photonic circuits, see and Refs. [HH19a; HH19c; HH19b; HH19d; SH20] and\nRefs. [Fla+20; Lam21; Sag+21b; Nag+21; Shi+22], with the connection to superconducting qubits\nestablished in Ref. [Lam17; Cár+18]. Another approach, which we did not review in detail, is given\nby combining RL with the paradigm of quantum annealing [Neu+17; AHF20; Neu+20; Mül+21;\nFH23; NY23].\nStrategies have been developed to address the classical and quantum version of\ncontextual bandits [LHT22; LJW22; BLT23; BKS23].\nFurthermore, a quantum version of the\nclassical RL benchmark environment CartPole has been formulated [WAU20; Mei+23]. Similarly,\nvarious interpretations of QRL for specialized tasks in the quantum domain exist [Alv+16; Alv+18;\nBha+19; Alb+18; Alb+20; She+20; Oli+20; Liu+22; ÇY23].\nDifferent approaches have been\nproposed for combining RL with quantum walks [Che+19; Dal+22; MVB22].\nFurther work on\noptimization tasks rather than RL, such as Ref. [Ram17; Jaš+19; Bel+20], have not been reviewed\nin detail. An interesting interpretation of self-learning physical machines is discussed in [LM23],\nwhich potentially could be brought into line with QRL.\n4.1\nQuantum-Inspired Reinforcement Learning based on Amplitude Amplifica-\ntion\nQuantum reinforcement learning, Dong et al. (2008) and related work\nSummary. Ref. [Don+08b] discusses a new RL algorithm that is inspired by the superposition prin-\nciple of quantum mechanics. The authors propose an algorithm that modifies the action-selection\nprocedure and balances exploration and exploitation in a novel way. The authors present their ideas\nin modified form in a sequence of papers, see Refs. [Don+08b; Don+06a; Don+06b; Che+06; CD08;\nCDC06; Don+08a; CD10; Don+12; CFD12; Fak+13; NGC15; GH19; Li+20a; LAD21; Hu+21], for\nan overview see also [Raj+21]. The original work [Don+08b] discusses how to execute the proposed\nalgorithm on actual quantum devices – which, however, did not exist at this time. As discussed\nalso below, it is not clear how to run the algorithm in quantum superposition, and if this is possible\nin practice without taking away potential quantum advantage. Despite these doubts the proposed\nconcepts enhance classical RL with ideas from QC, which leads us to view this approach as QiRL.\nAlgorithmic Concepts and Extensions. Initially, the algorithm is formulated as merely quantum\ninspired in Ref. [CD08] (i.e., it is developed for a classical computer that simulates a quantum\nsuperposition). The motivation is to design an algorithm with better exploration-exploitation trade-\noff compared to e.g. ϵ-greedy action selection. The underlying routine is a modification of temporal\ndifference (TD), more concretely TD(0) in the following way: For each state the set of possible\nactions is in a ‘superposition’ and the agent (in state s) now selects an action with a given probability.\nThe action is taken and the new state s′ and reward r is observed. Afterwards, the probability of\nthe taken action is increased by k(r + V (s′)), where V (s′) is the value function of state s′, and k\nis a hyperparameter. The term r + V (s′) samples a quantity similar to Q(s, a). Consequently, the\n13\nCitation\nFirst Author\nTitle\n[Don+08b]\nD. Dong\nQuantum reinforcement learning\n[Don+06a]\nD. Dong\nQuantum mechanics helps in learning for more intelligent\nrobots\n[CDC06]\nC.-L. Chen\nQuantum computation for action selection using reinforcement\nlearning\n[Don+06b]\nD. Dong\nQuantum Robot: Structure, Algorithms and Applications\n[Che+06]\nC.-L. Chen\nSuperposition-Inspired Reinforcement Learning and Quantum\nReinforcement Learning\n[CD08]\nC.-L. Chen\nA Quantum Reinforcement Learning Method for Repeated\nGame Theory\n[Don+08a]\nD. Dong\nIncoherent Control of Quantum Systems With Wavefunction-\nControllable Subspaces via Quantum Reinforcement Learning\n[CD10]\nC.-L. Chen\nComplexity analysis of Quantum reinforcement learning\n[Don+12]\nD. Dong\nRobust Quantum-Inspired Reinforcement Learning for Robot\nNavigation\n[CFD12]\nC. Chunlin\nHybrid control of uncertain quantum systems via fuzzy esti-\nmation and quantum reinforcement learning\n[Fak+13]\nP. Fakhari\nQuantum inspired reinforcement learning in changing environ-\nment\n[NGC15]\nS. Nuuman\nA quantum inspired reinforcement learning technique for be-\nyond next generation wireless networks\nTable 2: [Part 1] Work considered for “QiRL based on amplitude Amplification” (Sec. 4.1)\nupdate creates a probability distribution, where for a given state the probability to select an action\nincreases as the value of Q(s, a) increases. Therefore, this action selection process corresponds to\nsampling from a stochastic policy dependent on the value of the state-action pairs.\nNow the algorithm is translated to be run on a quantum computer. The stochastic policy is\nreplaced by a quantum superposition. That is, for each state s the possible actions are represented\nby the eigenstates of some observable and a superposition of these states is created. If the observable\nis measured, the state will collapse to an eigenstate associated with an action which will be taken\nby the agent and therefore constitutes the selection process. After receiving the reward and the\nnew state, the Grover operator is applied L = min{k(r + V (s′)), Lmax} times to a copy of the\nsuperposition state to enhance the amplitude corresponding to the previous selected action. The\nvariable Lmax guarantees that the Grover operator is not applied too many times. Note that repeated\napplication of the procedure requires a new copy of the state after each measurement. Due to the no\ncloning theorem, this could be realized by many different independent copies of the initial memory,\nor by a purely classical representation of the states. The latter realization reduces the algorithm to\nthe initial proposal of a quantum-inspired action selection process.\nIn Ref. [Don+12] the QiRL algorithm is applied to robot navigation. It is stated explicitly that\nQiRL is a classical action-selection method that differs from the ideas of QRL, which in principle\ncould benefit from a quantum computer.\nIn Ref. [GH19] the algorithms are generalized to Q-\nlearning and double- and multiple Q-learning. Also these approaches should be understood in the\ncontext of QiRL. Finally, Refs. [Li+20a; Nir+21] apply QiRL to human decision making behavior,\n14\nCitation\nFirst Author\nTitle\n[GH19]\nM. Ganger\nQuantum Multiple Q-Learning\n[Li+20a]\nJ.-A. Li\nQuantum reinforcement learning during human decision-\nmaking\n[LAD21]\nY. Li\nIntelligent Trajectory Planning in UAV-Mounted Wireless\nNetworks: A Quantum-Inspired Reinforcement Learning Per-\nspective\n[Raj+21]\nK. Rajagopal\nQuantum Amplitude Amplification for Reinforcement Learn-\ning\n[Nir+21]\nD. Niraula\nQuantum deep reinforcement learning for clinical decision sup-\nport in oncology: application to adaptive radiotherapy\n[Wei+21]\nQ. Wei\nDeep Reinforcement Learning With Quantum-Inspired Expe-\nrience Replay\n[Yin+21]\nL. Yin\nQuantum deep reinforcement learning for rotor side converter\ncontrol of double-fed induction generator-based wind turbines\n[Hu+21]\nY. Hu\nQuantum-enhanced reinforcement learning for control: a pre-\nliminary study\n[Ren+22]\nY. Ren\nNFT-Based Intelligence Networking for Connected and Au-\ntonomous Vehicles: A Quantum Reinforcement Learning Ap-\nproach\n[Cho+23]\nB. Cho\nQuantum bandit with amplitude amplification exploration in\nan adversarial environment\nTable 3: [Part 2] Work considered for “QiRL based on amplitude Amplification” (Sec. 4.1)\nRef. [Yin+21] to a complex control task, and Ref. [Ren+22] to autonomous vehicles. Recently, the\nquantum-inspired approach to action selection in RL was transferred to experience replay buffer\nsampling in Q-learning [Wei+21].\nRemarks. Although it is mentioned in Ref. [Don+08b; Don+06a; CD08; CDC06] that the whole\nalgorithm could be run in quantum superposition on a quantum device, no details of such kind of\ngenuine QRL algorithm are given. Overall, it is unclear if such an algorithm might exist. Indeed,\nsubsequent work focuses on the QiRL paradigm.\nThe claims made in Refs. [Don+08b; Don+06a; Don+06b; CD08; CDC06; Don+08a; Don+12;\nGH19; Li+20a; LAD21; Cho+23] can be summarized as follows: speed-up in learning by better\nbalancing exploration-exploitation; less GPU power needed on classical computer compared to al-\ngorithms like classical Q-learning; more robust against changes of learning rate. More experiments\non larger environments for deeper insights into the scaling of the algorithm and a rigorous complexity\nanalysis would be an interesting topic for future work.\n4.2\nQuantum Reinforcement Learning with Variational Quantum Circuits\nThis section summarizes the state-of-the-art on VQC-based RL. Several ideas have been proposed\nin this field, with extensions in different directions. Their common ground is the usage of a VQC\nas parameterized function approximator.\nThe typical hybrid pipeline is summarized in Fig. 6. It was originally proposed for Q-function ap-\nproximation by Chen et al. [Che+20] and extended to policy approximation by Jerbi et al. [Jer+21a].\n15\nOther work proposes several modifications to this pipeline, which we will describe in the respec-\ntive summaries.\nThe algorithm must be understood as hybrid, as a lot of the work, especially\nthe optimization, is executed on classical hardware. The agent observes the current state of the\nenvironment st, and applies some pre-processing ϕ. The result is encoded using the feature map\nUϕ(s). With the current variational parameters θt, a quantum state is prepared and a (potentially\naction-dependent) observable Oa is measured. The expectation value ⟨Oa⟩s,θ can be post-processed\nto represent, e.g., a state-action value function Qθ(s, a), or the policy πθ(a|s). Depending on the\ninstance, the agent employs this function to sample an action at and executes it in the environment.\nThe reward rt (and potentially also the consecutive state st+1) is observed by the classical optimizer.\nTo enable gradient-based parameter updates, an additional hybrid module uses the parameter-shift\nrule [Cro19; Wie+22b] to compute the gradients of the VQC outputs w.r.t. the variational param-\neters θt. The classical optimizer determines the new parameter set θt+1 and instantiates the VQC\nwith these updated parameters. This overall iterative procedure of environment interaction, func-\ntion approximation, and parameter update is repeated for several episodes, in the same way as for,\ne.g., DRL.\nUnfortunately, thus far there is no guaranteed quantum advantage for this approach, apart from\nsome cryptography inspired artificial datasets [Jer+21a; SJD22]. However, several of the papers\nand preprints summarized in this section demonstrate promising experimental results.\nFigure 6: Hybrid quantum-classical agent in a typical VQC-based RL pipeline.\nThis idea was\nfirst proposed by Chen et al. [Che+20] for Q-function approximation and extended by Jerbi et\nal. [Jer+21a] to policy approximation. The QPU is used to approximate the respective function,\nwhile pre- and post-processing and optimization happens on classical hardware. The interaction with\nthe environment depends on the concrete problem instance (e.g. classical or quantum environment).\n4.2.1\nValue-Function Approximation\nThis section covers VQC-based approximations in value space, as described for the instance of\nclassical Q-learning in Eqs. (11) and (12). The work by Chen et al. [Che+20] was indeed the first\nproposal of this type of approximation-based techniques, which was reproduced and extended in\nRefs. [Lok+22; CCC23; Che23c; FP+23]. A modification of the state encoding procedure has been\n16\nCitation\nFirst Author\nTitle\n[Che+20]\nS. Y.-C. Chen\nVariational Quantum Circuits for Deep Reinforcement Learn-\ning\n[Lok+22]\nS. Lokes\nImplementation of Quantum Deep Reinforcement Learning\nUsing Variational Quantum Circuits\n[Che23c]\nS. Y.-C. Chen\nQuantum deep Q learning with distributed prioritized experi-\nence replay\n[CCC23]\nH.-Y. Chen\nDeep-Q Learning with Hybrid Quantum Neural Network on\nSolving Maze Problems\n[FP+23]\nG. Fikadu Tilaye\nInvestigating the Effects of Hyperparameters in Quantum-\nEnhanced Deep Reinforcement Learning\n[LS20]\nO. Lockwood\nReinforcement Learning with Quantum Variational Circuits\n[LS21]\nO. Lockwood\nPlaying Atari with Hybrid Quantum-Classical Reinforcement\nLearning\n[SJD22]\nA. Skolik\nQuantum agents in the Gym: a variational quantum algorithm\nfor deep Q-learning\n[Sko+23]\nA. Skolik\nRobustness of quantum reinforcement learning under hardware\nerrors\n[LXJ23]\nY. Liu\nReinforcement Learning for Continuous Control: A Quantum\nNormalized Advantage Function Approach\nTable 4: Work considered for “QRL with VQCs – Value-Function Approximation” (Sec. 4.2.1)\ndiscussed in Lockwood and Si [LS20], and was up-scaled in another work by the same authors [LS21].\nA slight reformulation of the technique – which comes with a provable advantage for very specific\nscenarios – can be found in Skolik et al. [SJD22]. An analysis of noise influence for this framework is\ndiscussed in Ref. [Sko+23]. An extension to environments with continuous action spaces is proposed\nin Ref. [LXJ23]. Ideas based on amplitude amplification to efficiently evaluate the approximated\nQ-function have been introduced in Ref. [San+23], which however can not be realized given the\ncurrent hardware restrictions.\nVariational Quantum Circuits for Deep Reinforcement Learning, Chen et al. (2020)\nand related work\nSummary. This paper by Chen et al. [Che+20] represents the first attempt to utilize VQCs for\nRL. This is done in the context of using VQCs as function approximators for the state-action value\nfunction. The authors perform simulations on simple benchmark environments and report.\nHybrid Algorithm. The algorithm is inspired by deep Q-learning (DQL) [Mni+15], where a DNN\nrepresents the Q-function. The authors replace the DNN by a VQC. The update is performed w.r.t.\nthe mean square error (MSE) loss function L(θ) = E[\n\u0000rt + γ · maxa′ Qθ′(st+1, a′) −Qθ(st, at)\n\u00012]\nusing, e.g., gradient descent. Additionally, experience replay and target networks (second set of\nparameters θ\n′) are employed to address the instabilities stemming from bootstrapping the value\nfunction, forming a double deep Q-learning (DDQL) algorithm. Fig. 7 gives the complete algorithm.\nVQC Architecture. The feature map uses simple computational basis encoding on individual qubits.\nMore concretely, the RL state is interpreted as bitstring, which can be encoded using the identity\nRz(π)Rx(π) |0⟩= |1⟩.\nThe entanglement structure connects nearest neighbors with CZ gates.\n17\nFigure 7: Hybrid algorithm proposed by and taken from Chen et al. [Che+20]; This algorithm uses\na VQC to approximate the state-action value function and follows the typical steps of DQL. Note,\nthat the authors notation for the Q-function slightly deviates from our conventions.\nThe variational parameters are incorporated in single qubit rotations about the x, y, and z axis.\nThe state-action value is decoded by measuring Pauli-Z observables on a number of qubits, that\ncorresponds to the number of actions in the environment. The full VQC is visualized in Fig. 8.\nFigure 8: VQC proposed by and taken from Chen et al. [Che+20]; The Rx and Rz gates are used\nfor state encoding. Several parameterized layers (dashed box) are repeated to form the Q-function\napproximator. The values of the function are decoded using 1-qubit Pauli-Z observables.\nExperimental Results and Discussion.\nThe proposed VQC-DQL algorithm is simulated for two\nenvironments. The first one is FrozenLake, with 16 states and an 4 actions. The second one is\nCognitiveRadio, which is adapted to VQCs sizes of 2 to 5 qubits. The authors report that their\nVQC-based agent performs at least equally well as a NN. Moreover, they claim that this requires\nfewer parameters (about one order of magnitude compared to DNNs), which points towards potential\nquantum advantage. The model is tested on actual quantum hardware with competitive results.\nRemarks. The employed encoding scheme (computational basis encoding) could be simplified by\nomitting the RZ rotations, as these only introduce a global phase. The CognitiveRadio environ-\nment might be oversimplified. We also note that the claim on reduced parameter count should be\nsubstantiated by experiments with environments of different scale.\nReproduction. A reproduction study by Lokes et al. [Lok+22] conducts an extended hyperparameter\nsearch for the described setup. The results and claims are overall consistent with [Che+20], but no\nnovel findings could were reported.\n18\nExtension. In the work by S. Y.-C. Chen [Che23c] the quantum Q-learning framework introduced\nin [Che+20] is extended by incorporating prioritized experience replay.\nAdditionally, an asyn-\nchronous training routine is employed, similar to the one discussed in [Che23a]. Both techniques\nreduce the overall sampling complexity and therefore allow for solving more complex tasks with the\nsame underlying quantum model. This is validated with numerical simulations on several versions\nof the CartPole environment.\nHybrid Model. The work by Chen et al. [CCC23] extends the quantum models used in [Che+20] with\nclassical neural networks, to produce more expressive function approximators. With that extension,\nthe quantum agent is able to solve a 20 × 20 gridworld maze, which should clearly be more complex\nthan the originally considered FrozenLake environment. However, with the provided analysis it in\nunclear to which extend the performance can be contributed to the quantum part of the model.\nHyperparameter Analysis. A hyperparamter analysis is conducted by Fikadu Tilaye and Pandey [FP+23],\nwith a focus on the Q-learning framework introduced in [Che+20]. The authors conclude, that\ndeeper quantum circuits lead to a better overall performance, while a larger learning rate speeds up\nthe overall process. However, the analysis is superficial and quite small-scale, so further investiga-\ntions are necessary to allow for more general statements.\nAlgorithmic Characteristics - Chen et al. [Che+20]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGatesa\nFrozenLake\nDDQL\nQ-function\ndiscrete\ndiscrete\n4\n4 × 2 (encoding)\n(OpenAI Gym)\n16\n4\n4 × 4 × 3 (weights)\nCognitiveRadio\nDDQL\nQ-function\ndiscrete\ndiscrete\nn\nn × 2 (encoding)\n(see [Che+20])\nn2\nn\nn × 4 × 3 (weights)\na encoding gates: qubits × per_qubit; variational gates: qubits × layers × per_qubit_per_layer;\nReinforcement Learning with Quantum Variational Circuits, Lockwood and Si (2020)\nSummary. The work by Lockwood and Si [LS20] modifies several aspects of the routine proposed\nby Chen et al. [Che+20]. Most importantly, they introduce two new encoding schemes to deal with\na continuous state space.\nModification of Architecture. The first proposed encoding is denoted as scaled encoding. It scales\nthe RL state values to the range [0, 2π), which are then encoded using some 1-qubit parameterized\nrotations. The second on (so-called directional encoding) only encodes the sign of the value. More\nconcretely, if a state variable is positive, Rx and Rz rotations by π are applied to the encoding qubit\n(following a similar idea as the computational state encoding [Che+20]).\nThe architecture for the variational layer consists of an entangling block (nearest-neighbor CX\ngates) and parameterized 1-qubit rotations about x, y, and z axis. This block is repeated three\ntimes. For decoding the state-action value, the authors employ two different strategies. The first\none feeds the measurement result into a classical fully-connected layer where the number of outputs\ncorresponds to the number of possible actions.\nIn the other case, a so-called quantum pooling\noperation, condenses the information of the quantum state into a subset of the qubits [CCL19]. This\nallows for a more flexible architecture, independent of the number of actions in the environment.\nExperimental Results. The proposed algorithm and the encoding schemes are benchmarked on the\nCartPole and Blackjack environment. While the former one uses a combination of scaled and\ndirectional encoding, the second one only employs scaled encoding. Their findings agree with those\nreported previously in the literature, namely that VQC-based models achieve similar performance\n19\nto NN-based function approximators. As also stated by Chen et al. [Che+20], the usage of VQCs\nreduces the required parameter complexity.\nRemarks. While the scaled encoding should be a sound choice, the directional encoding could be\ninappropriate for most environments. Usually, not only the sign of a specific state is relevant, but\nthe concrete state contains relevant information. With this encoding, this information is lost, which\nshould lead to a drop in performance for more complex environments. As stated previously, the\nreduced parameter complexity should be investigated for larger problem instances.\nAlgorithmic Characteristics - Lockwood and Si [LS20]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGatesa\nCartPole\nDDQL\nQ-function\ncontinuous\ndiscrete\n4\n4 × 2 (encoding)\n(OpenAI Gym)\n4-dim\n2\n4 × 3 × 3 (weights)\nBlackjack\nDDQL\nQ-function\ndiscrete\ndiscrete\n3\n3 × 2 (encoding)\n(OpenAI Gym)\n31×11×2\n2\n3 × 3 × 3 (weights)\na encoding gates: qubits × per_qubit; variational gates: qubits × layers × per_qubit_per_layer;\nPlaying Atari with Hybrid Quantum-Classical Reinforcement Learning, Lockwood and\nSi (2021)\nSummary. This work by Lockwood and Si [LS21] extends their previous paper [LS20], which, in\nturn, was based on Chen et al. [Che+20], where Q-learning with VQC function approximation has\nbeen introduced. The paper considers the Atari environments Pong and Breakout, with continuous\nstate space of dimensionality 28.224 (the observations are cropped and converted to images with\n84×84×4 pixels). This environment complexity is not tractable with previously introduced encoding\nschemes, which require one qubit for each dimension. The proposed workaround uses a classical NN\nto reduce the state dimensionality before encoding it into the VQC.\nUnderlying Algorithm and Simulation. Similar to Refs. [Che+20; LS20], the concept of DDQL is\nused. The pipeline is modified by replacing the pure VQC function approximator with a hybrid\nmodel. Several different choices are considered, the most important details are highlighted below.\nThe training is performed in an end-to-end manner, i.e., the gradients w.r.t. the VQC parameters\nare propagated back through the classical encoding network.\nModel Architecture. The VQC architecture is, as usually, composed of three parts (i.e. state encod-\ning, variational layers, and action decoding). To encode the state, the raw data is first fed through\na classical NN. This outputs a number of values equal to the number of parameters in the feature\nmap, which itself consists of 1-qubit parameterized rotations. The authors compare the performance\nof a densely connected and a convolutional neural network (CNN) for this task (the concrete archi-\ntecture of these networks are not specified). Apart from that, encoding layers of different sizes (and\ntherefore different number of parameters) from 5 to 15 qubits are compared.\nThe variational layers itself consists of two parts, where the first one is a quantum convolutional\nneural network (QCNN) [CCL19]. The authors state two motivations for this choice: First, it should\nhelp capture the spatial structure of the input images (but it is unclear, whether the encoding part\nretains the spatial structure). Second, QCNNs help to avoid barren plateaus [Pes+21] (while the\nexperiments show no sign of barren plateaus, it is not clear if this is due to this choice, or the limited\nsize of the employed circuits). After this QCNN there are three repetitions of entanglement gates\nand parameterized rotations, similar to those also used for state encoding.\nThe paper proposes two methods to deal with the problem of measurement for unequal num-\nber of qubits and actions.\nThe first method performs Pauli-Z measurements on all qubits and\n20\nuses an appended dense NN. Alternatively, quantum pooling operations [CCL19] are used, which\nsubsequently compress the measurement of two qubits into one.\nExperimental Results and Discussion. To demonstrate the basic functionality of the model, initial\nexperiments are conducted on the CartPole environment. The results demonstrate a similar perfor-\nmance to Lockwood and Si [LS20]. On the two Atari environments, the paper considers 12 different\nhybrid architectures (dense vs. convolutional encoding, 5 vs. 10 vs. 15 qubits, dense vs. pooling\ndecoding), which are compared to a well-established classical architecture.\nIt turns out, that the hybrid models are not able to learn at all.\nThe authors state, that\nthis is down to the lack of expressibility of the hybrid models, which only make use of about 104\nparameters, while the classical model uses about 106. It is expected, that for more expressive models\nthe performance improves, as learning on the much simpler CartPole environment was successful.\nRemarks. The experiments are conducted with a restricted set of hybrid models. Consequently,\nthe claim that these results do not demonstrate the inapplicability of QRL to more complex envi-\nronments like Atari is reasonable. The assumption that this approach could be made to work on\ncomplex environments, as it succeeds on e.g. CartPole, should be sustained with additional experi-\nments. For a modified architecture succeeding on the Atari environments, it is not completely clear,\nwhich part of the work is done by the classical and quantum part of the model. This is a typical\ncaveat, whenever quantum and classical architectures are combined.\nAlgorithmic Characteristics - Lockwood and Si [LS21]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGates\nCartPole\n(OpenAI Gym)\nDDQL\nQ-function\ncontinuous\n4-dim\ndiscrete\n2\n5\nN/A (classical)a\nO\n\u0000101\u0001\n(encoding)\nO\n\u0000102\u0001\n(weights)\nPong-v0\n(OpenAI Gym)\nDDQL\nQ-function\ncontinuous\ndiscrete\n6\n5 to 15\nO\n\u0000106\u0001\n(classical)\n28224-\nO\n\u0000102\u0001\n(encoding)\ndimb\nO\n\u0000104\u0001\n(weights)\nBreakout-v0\n(OpenAI Gym)\nDDQL\nQ-function\ncontinuous\ndiscrete\n4\n5 to 15\nO\n\u0000106\u0001\n(classical)\n28224-\nO\n\u0000102\u0001\n(encoding)\ndimb\nO\n\u0000104\u0001\n(weights)\na potentially also uses a classical NN for pre-processing, details are not stated;\nb dimensionality of feature space is reduced with a NN to fit size of feature map;\nQuantum agents in the Gym: a variational quantum algorithm for deep Q-learning,\nSkolik et al. (2022)\nSummary. This work by Skolik et al. [SJD22] proposes another instance of Q-learning with VQCs\nas function approximators.\nBeing aware of preceding literature, the authors set out to analyze\nthe role of architecture design, RL state encoding schemes, and observables for action decoding.\nWith regard to the previous work, the authors remark that the CartPole environment cannot be\nconsidered solved.\nImportance of Architecture Design. In terms of architecture choices, the problem of barren plateaus\nis emphasized: Architectures with many qubits and layers (which naively is required for high ex-\npressivity) are hard to train. Contrarily, over-parameterized architectures are easier to train, but\nprobably less expressive and therefore less effective on a given task.\nThe authors chose a hardware-efficient ansatz, despite being known to run into the barren\nplateau problem for large circuits. For the small circuit sizes considered in the present work, the\nbarren-plateau problem does not appear to be relevant.\n21\nEncoding Schemes. As for encoding schemes, discrete RL states are encoded in the computational\nbasis. Continuous states are scaled to the finite interval [−π/2, +π/2] by applying arctan to the\nraw observations. The result serves as the rotation angle for an Rx rotation, which is very similar\nto the scaled encoding proposed by Lockwood and Si [LS20]. In order to increase expressivity w.r.t.\nto the input, the encoding layer can be repeated through the circuit, forming a data re-uploading\nstructure [Pér+20]. Effectively, this allows to learn and approximate a Fourier sum of a certain\norder, where the order is tied to the number of repetitions of the encoding layer [SSM21]. The\nencoding is further modified by introducing learnable re-scaling parameters, that are multiplied\nwith the raw states before computing the arctan.\nExperimental Results and Discussion. The authors benchmark their architecture choices on the\nFrozenLake and CartPole environment. The performance on CartPole is compared to a small NN\nwith the same number of parameters, which seems to be inferior. Further, the range of Q-values\nthat can be encountered in the two benchmark environments is investigated.\nFor FrozenLake,\nrepresenting the Q-value with the expectation values of 1-qubit Z-operators is sufficient. For the\nCartPole environment, this strategy is found not to be adaptable enough. Instead, they chose the\nexpectation values of the parities (of 2 non-overlapping pairs of qubits) and allow for additional\ntrainable classical weights that set the scale for the Q-value approximation.\nRemarks. The authors emphasize the critical role of architectural choices at the outset of their\nmanuscript.\nWhile they offer valuable insights into this topic, also open questions remain for\nfuture work in this direction. For the CartPole environment, several trainable classical weights are\nincorporated in the algorithm. Therefore, it is not completely clear, what part of the training is\nachieved by which part of the hybrid model.\nError Analysis. The work by Skolik et al. [Sko+23] analysis the influence of hardware noise on the\nquantum Q-learning framework introduced in [SJD22], but also quantum policy gradient (QPG) ap-\nproaches discussed in Sec. 4.2.2. The results are numerically validated on the CartPole environment\nand a version of the Travelling Salesperson Problem. The results indicate, that the performance is\nvery much dependent on the inherent structure of the noise. For some instances, the robustness of\nthe learned policy is actually increased if noise is encountered during training. However, e.g. for\nstrong incoherent noise the performance decreases quite substantially. Interesting from a practical\npoint of view is especially the analysis of shot noise, which indicates that a low number of repetitions\nis enough to get a reliable estimate of the Q-function – an explicit algorithm to exploit this property\nis proposed in this work.\nContinuous Action Spaces. A Q-learning approach based on [SJD22] that incorporates continuous\naction spaces is discussed by Liu et al. [LXJ23]. They use normalized advantage functions which\nallows for continuous action selection. An alternative would be to additionally use a policy function\napproximator to form an actor-critic approach, as discussed in Sec. 4.2.3.\nAlgorithmic Characteristics - Skolik et al. [SJD22]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGatesa\nCartPole\n(OpenAI Gym)\nDDQL\nQ-function\ncontinuous\n4-dim\ndiscrete\n2\n4\n4 × 1 (encoding)\n4 × 15 × 2 (weights)\nN/A (classical)b\nFrozenLake\nDDQL\nQ-function\ndiscrete\ndiscrete\n4\n4 × 1 (encoding)\n(OpenAI Gym)\n16\n4\n4 × 15 × 2 (weights)\na encoding gates: qubits × per_qubit; variational gates: qubits × layers × per_qubit_per_layer;\nb model incorporates classical weights after measurement, details are not stated;\n22\nCitation\nFirst Author\nTitle\n[Jer+21a]\nS. Jerbi\nParameterized Quantum Policies for Reinforcement Learning\n[Kun22]\nL. Kunczik\nReinforcement Learning with Hybrid Quantum Approxima-\ntion in the NISQ Context\n[BAQ23]\nQuafu Group\nQuafu-RL: The Cloud Quantum Computers based Quantum\nReinforcement Learning\n[SSB23]\nA. Sequeira\nPolicy gradients using variational quantum circuits\n[Jer+23]\nS. Jerbi\nQuantum Policy Gradient Algorithms\n[Mey+23b]\nN. Meyer\nQuantum Policy Gradient Algorithm with Optimized Action\nDecoding\n[Mey+23a]\nN. Meyer\nQuantum Natural Policy Gradients: Towards Sample-Efficient\nReinforcement Learning\nTable 5: Work considered for “QRL with VQCs – Policy Approximation” (Sec. 4.2.2)\n4.2.2\nPolicy Approximation\nThis section covers VQC-based approximations in policy space, as described for the instance of\nclassical policy gradients in Eqs. (13) and (14). The concept was introduced by Jerbi et al. [Jer+21a],\nshortly followed by a slight reformulations in Ref. [Kun22], and an extension to allow for faster\ncomputation in Ref. [BAQ23]. Several modifications, including formulating full-quantum interaction\nwith a quantum control environment, have been introduced in Sequeira et al. [SSB23] – with a closer\nanalysis of quantum-accessible environments revealing potential advantage compared to certain\nclassical routines in Ref. [Jer+23].\nAlgorithmic extensions to the QPG setup were proposed in\nRef. [Mey21].\nDetails on a therein introduced classical post-processing function to improve RL\nperformance are discussed in Meyer et al. [Mey+23b], and quantum natural gradients to enhance\ntrainability are covered by the same authors in [Mey+23a].\nParameterized Quantum Policies for Reinforcement Learning, Jerbi et al. (2021) and\nrelated work\nSummary. The paper by Jerbi et al. [Jer+21a] starts out with a small summary of VQC-based ML\nmodels. They cite several reports of quantum advantage in the supervised and unsupervised QML.\nThis motivates their approach to go beyond the scope of Q-function approximation [Che+20; LS20;\nLS21; SJD22], and use the VQC to directly approximated the policy.\nQuantum Policy Gradient. After a brief recap of policy gradient methods for solving RL problems,\nthe authors extend those ideas to a QPG approach. More concretely, they quantize the REINFORCE\nalgorithm [Wil92] with value-function baselines by using VQCs as function approximators for the\n(stochastic) policy. The define two families of VQC-based policies: (1) A RAW-VQC policy, where the\naction selection follows Born’s rule. It is defined as πθ(a|s) = ⟨Pa⟩s,θ, where Pa are the projectors\non the elements of the computational basis. This allows action selection with only one evaluation of\nthe quantum circuit; (2) A SOFTMAX-VQC policy, defined as πθ(a|s) = eβ⟨Oa⟩s,θ/ P\na′ eβ⟨Oa′⟩s,θ. The\nmeasurement result of an action-dependent observable Oa is fed into a single-parameter softmax-\nfunction, to form a PDF. The inverse-temperature parameter β allows to adjust the peak-width of\nthe distribution, i.e., the greediness of the policy.\nCircuit Architecture. The ansatz for the VQC is chosen to be hardware-efficient, i.e., only single\nand two-qubit gates. The RL state is encoded with 1-qubit rotations. To increase the expressivity\nof the model, the authors introduce additional learnable state-scaling parameters λ.\nThose are\n23\nmultiplied to the rotational parameter denoting the state value, i.e., λi · si is the value of a 1-qubit\nrotation. This also helps circumvent the problem of being restricted to a finite set of frequencies in\nsuch an encoding scheme [SSM21]. The feature map is repeated several times, alternating with the\nvariational layer, which forms a data re-uploading structure [Pér+20]. A variational layer consists\nof CZ-gates for creating entanglement in an circular structure. The learnable parameters are used\nin 1-qubit parameterized rotation gates. Depending on the policy type, measurements are either\nconducted in the computational basis, or more complex observables are measured.\nExperimental Results. Overall, all agents are able to learn meaningful behavior in the OpenAI Gym\nenvironments CartPole, MountainCar, and Acrobot. Further experiments are reported, which serve\nthe purpose of assessing the importance of the various design choices: (1) Circuit depth increases\nperformance and learning speed, where SOFTMAX-VQC policies outperform RAW-VQC policies in\nall instances; (2) Incorporating learnable state scaling parameters increases learning performance,\ntrainable classical weights (in case of SOFTMAX-VQC) multiplied to expectation values leads to\nincrease in performance; (3) The performance gap between RAW-VQC and softmax-VQC policies\nseems to stem from the ability to adjust greediness.\nProvable and Empirical Quantum Advantage. To the best of our knowledge, this work is the first\nto corroborate the idea quantum advantage with VQCs in the RL setting. Therefore, the authors\ndevise RL environments (based on the discrete logarithm problem (DLP)), which are supposed to\nbe classically intractable.\nAny classical algorithm would need a number of samples that scales\nexponential in the problem size to achieve a low generalization error. A VQC-based algorithm with\na very specific architecture only requires a polynomial amount of data. This implies an exponential\nadvantage w.r.t. sample complexity, assuming it is infeasible to efficiently simulate the VQC on\nclassical hardware for large problem instances. The construction of the environment is inspired\nby previous results from QML, where similar learning separations between classical and quantum\nmodels have been demonstrated [LAT21].\nFurther, the authors report numerical evidence of potential quantum advantage for environ-\nments based on expectation values sampled from VQCs. The motivation lies in the (potential)\nintractability of simulating the given VQC classically for large systems. More concretely, one uses\na VQC to define a labeling function (in the sense of a classification task) over the domain [0, 2π]2\n(so-called SL-VQC). This synthetic classification dataset is then rephrased as a RL environment\nby incorporating some temporal structure (denoted as Cliffwalk-VQC). Numerically, the authors\nobserve a performance separation of models with classical DNNs and VQC-based policies. They\nclaim, that this is likely due to the oscillatory structure in the labeling function.\nRemarks. While the proposal of provable quantum advantage is obviously quite encouraging, the\npractical realization is probably out of reach for the NISQ-era. The idea of solving the task efficiently\non quantum hardware is based on Shor’s algorithm. Formulated as a VQC-based RL problem, this\nwould require circuits of complexity far beyond current scope.\nWe think it requires also some\nmore large-scale experiments, to support the empirical learning separation on the SL-VQC and\nCliffwalk-VQC environments. A comparison to other hybrid models [Che+20; LS20] shows, that\nthe proposed QPG approach is superior in terms of RL performance on various environments.\nAlternative Formulation. In the PhD thesis by L. Kunczik [Kun22] a slightly different formulation\nof the QPG framework is introduced, where the output of the quantum circuit is compounded with\na classical weight vector. However, the underlying routine is very similar to [Jer+21a]. Empirical re-\nsults are reported to verify an desirable scaling of VQC-based (as opposed to NN-based) approaches.\nHowever, experiments are to small-scale for reliable statements regarding this correlation.\nCloud Computing. The work by the BAQIS Quafu Group [BAQ23] realizes the framework introduced\nin Sec. 4.2.2 and executed it on the quantum devices provided via the Quafu cloud services. The\nresults are ambiguous, as the agents trained on hardware are not really able to learn meaningful\nbehaviour – but are also only trained for a very limited number of timesteps, as also acknowledged\nby the authors.\n24\nAlgorithmic Characteristics - Jerbi et al. [Jer+21a]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGatesa\nCartPole\nREINFORCE\nPolicy\ncontinuous\ndiscrete\n4\n30\n(OpenAI Gym)\n4-dim\n2\nMountainCar\nREINFORCE\nPolicy\ncontinuous\ndiscrete\n2\n36\n(OpenAI Gym)\n2-dim\n3\nAcrobot\nREINFORCE\nPolicy\ncontinuous\ndiscrete\n6\n72\n(OpenAI Gym)\n6-dim\n3\nSL-VQC\nREINFORCE\nPolicy\ncontinuous\n2-dim\ndiscrete\n2\n2\n37\nCliffwalk-VQC\n(see [Jer+21a])\nCognitiveRadio\nREINFORCE\nPolicy\ndiscrete\ndiscrete\nn\n30 to 75\n(see [Che+20])\nn2\nn\nfor n = 2 to 5\na this entails encoding, scaling, and variational parameters; the SOFTMAX-VQC also uses classical parameters;\nPolicy gradients using variational quantum circuits, Sequeira et al. (2023) and related\nwork\nSummary. The article by Sequeira et al. [SSB23] proposes a quantum version of the REINFORCE\nalgorithm with a VQC-based function approximator, very similar to Jerbi et al. [Jer+21a]. The\nmethods are applied to the classical environments CartPole and Acrobot but also to a simple\nquantum control problem. It proposes an initialization technique for the variational parameters of\na VQC. Following the experimental results, a quantum advantage w.r.t. the number of required\nparameters and trainability of the models is claimed.\nUnderlying Reinforcement Learning Algorithm. As in Jerbi et al. [Jer+21a], the policy is defined as\nπθ(a|s) = eβ·⟨Oa⟩θ/ P\na′ eβ·⟨Oa′⟩θ, and REINFORCE updates are performed. Hereby, the expectation\nvalues ⟨Oa⟩θ for action a is defined as the expectation ⟨σa\nz⟩, i.e., the expectation value of 1-qubit\nPauli-Z observable measured on the a-th qubit.\nVQC Architecture. The architecture follows the typical three-part structure. In the beginning, the\nstates are encoded with Rx rotations, with the state values normalized to the range [−π, π). Conse-\nquently, the number of qubits has to correspond to max{|A|, |S|}. There are several parameterized\nlayers (see Fig. 9) which incorporate variational parameters in 1-qubit Ry and Rz rotations. The\nentanglement structure can be described as CX[i, (i + l) mod n], where n is the number of qubits,\nand l the index of the layer. The measurement of 1-qubit Pauli-Z observables is a deviation to the\nprocedure proposed by Jerbi et al. [Jer+21a], where multi-qubit observables were used.\nComplexity of Gradient Estimation.\nThe paper gives an estimation of the required number of\nsamples to get an ϵ-approximation of the log-policy gradient. According to this consideration, for a\nsuccess probability of 1 −δ, the number of required measurements is bounded by c · (1−ϵ)2\nϵ2\n· log\n\u0000 k\nδ\n\u0001\n.\nHereby, c is a constant depending on algorithmic hyperparameters and k is the number of variational\nparameters. It is important to state, that this refers to the number of samples / data points required\nto get a good approximation of the true policy gradient, but not the explicit estimation of the\ngradients themself via e.g. the parameter-shift rule.\nInitialization Technique. There is some work proposing a technique for parameter initialization to\navoid barren plateaus [Gra+19]. However, a technique to boost the overall performance has not yet\nbeen proposed. Inspired by classical ML, the authors aim to break symmetries between different\nneurons (as usually initialization with constant values is a bad choice). A typical strategy is to\nselect values uniformly at random from [−π, π], or drawn them following a Gaussian distribution.\n25\nFigure 9: VQC architecture proposed by and taken from Sequeira et al. [SSB23]; It deviates from\nthe typical circular entanglement structure.\nInspired by the classical Glorot initialization scheme [GB10], the paper proposed to use a nor-\nmal distribution N(0, std2) with std = g ·\np\n2/ (fanin + fanout). Here, g is a constant multiplicative\nfactor, fanin is the number of embedded features, and fanout is the number of computational basis\nmeasurements. This technique demonstrates some promising experimental results, but no theoreti-\ncal justification is given.\nAnalysis of Fisher Information Spectrum. The paper analyzes the spectrum of the Fisher informa-\ntion matrix (FIM), which serves as a tool to quantify the trainability of a model. The empirical\nFIM is computed as F(θ) = 1\nT\nPT\nt=1 ∇θ log π(at|st, θ)∇θ log π(at|st, θ)t. A similar analysis has also\nbeen proposed for QML [Abb+21].\nThe results show, that the spectrum of the FIM associated with the quantum model exhibits\nsignificantly larger averaged eigenvalues. The compared NN was optimized over several architec-\ntures, but not many details are provided in the paper. The authors conclude, that the quantum\nmodels are beneficial in terms of trainability, and might be resilient to barren plateaus.\nExperimental Results and Discussion of Potential Quantum Advantage. The proposed algorithm\nis tested on the classical benchmark environments CartPole and Acrobot.\nThe performance is\ncompared to the best classical NN (it is not clear, what best means in this case, and to what extend\nthis holds). The authors claim a significant advantage in terms of convergence speed.\nAdditional experiments are conducted with the proposed Quantum-Glorot initialization tech-\nnique. In the two environments CartPole and Acrobot, this technique demonstrates to be beneficial\nin terms of convergence speed and training stability.\nFinally, the experiments are extended to a QuantumControl environment. It requires to learn\nthe mapping |0⟩→|1⟩via the time dependent Hamiltonian H(t) = 4J(t)σz +hσx. This is converted\nto a set of unitary gates U(t), such that |ψt+1⟩= U(t) |ψ⟩. The reward is defined as the overlap\nbetween the prepared state and |1⟩, i.e. rt = |⟨ψt|1⟩|2. The agent has to decide between the two\nactions 0 ˆ= no pulse and 1 ˆ= apply pulse.\nThe usage of a quantum environment removes the\nnecessity of encoding classical states. Unfortunately, it is not described, how |ψt⟩is incorporated in\nthe VQC (a 1-qubit parameterized circuit is apparently used to solve the task). The results on this\nenvironment suggest, that the agent is able to learn the optimal pulses in a low number of epochs.\nSummarizing the experiments, the authors claim an advantage in convergence speed compared\nto classical approaches (questionable, as there should be NNs which perform much better). Addi-\ntionally, there seems to be a clear advantage in terms of parameter complexity.\nRemarks. The authors claim, that it is possible to estimate the log-policy gradient with only an\nlogarithmic amount of samples (in the number of variational parameters).\nWhile this certainly\nholds for simulation, it is not clear, if such a technique can be applied on quantum hardware\n(e.g., some kind of sparse or perturbed gradients).\nThe introduced initialization strategy gives\nsome good experimental results, although some additional experiments and theoretical justifications\n26\nwould be desirable. The formulation of the empirical FIM drops the dependency on the prior state\ndistribution, which potentially renders the considered spectrum less representative of the model\nthan for a generic supervised learning problem. The claim of quantum advantage w.r.t. parameter\ncomplexity and absence of barren plateaus should be supported with experiments on larger-scale\nenvironments.\nQuantum-Accessible Environments. An explicit analysis of quantum-accessible environments is con-\nducted in Jerbi et al. [Jer+23]. One instance of such an environment is considered in [SSB23], but\nalso [Wu+23] uses a related formulation. The paper derives explicit quadratic advantages in sam-\npling complexity, if the learned policy satisfies certain regularity conditions. We consider this to be a\nvery important step toward identifying the actual potential of QRL. Interestingly, the stated results\nsuggest that most of the scenarios studied in literature actually satisfy the smoothness conditions.\nAn open problem is the identification of practically relevant problems that can be formulated in the\ndescribed quantum-accessible setting.\nAlgorithmic Characteristics - Sequeira et al. [SSB23]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGates\nCartPole\nREINFORCE\nPolicy\ncontinuous\ndiscrete\n4\n4 (encoding)\n(OpenAI Gym)\n4-dim\n2\n24 (weights)\nAcrobot\nREINFORCE\nPolicy\ncontinuous\ndiscrete\n6\n6 (encoding)\n(OpenAI Gym)\n6-dim\n3\n36 (weights)\nQuantumControl\nREINFORCE\nPolicy,\nquantum\ndiscrete\nN/A\n0 (encoding)a\n(see [SSB23])\nEnvironment\n2\nN/A (weights)\na the RL state is a quantum state, i.e. no classical information has to be encoded;\nQuantum Policy Gradient Algorithm with Optimized Action Decoding, Meyer et al.\n(2023)\nSummary.\nThe work by Meyer et al. [Mey+23b] builds upon the QPG framework introduced\nin [Jer+21a]. It takes a closer look at the introduced RAW-VQC policy and – based on measurements\nin the computational basis – introduces a classical post-processing function for action selection. By\noptimizing this function w.r.t. a novel quality measure, significant performance improvements can be\nmade. The introduced procedure is also suited for problems with large action spaces. Experiments\non a 5-qubit quantum device represent the first successful training of a VQC-based RL routine on\nactual quantum hardware.\nClassical Post-Processing. The work focuses on the RAW-VQC policy, i.e. πθ(a|s) = ⟨Pa⟩s,θ. For\nmeasurements in the computational basis, this can be viewed as a partitioning of all possible\nbitstrings C.\nThis allows the definition of a classical post-processing function fC : {0, 1}n →\n{0, 1, · · · , |A| −1}, such that fC(b) = a, iff b ∈Ca.\nThe policy can therefore be expressed as\nπθ(a|s) ≈1\nK · PK−1\nk=0 δfC(b(k))=a where b(k) is the bitstring observed in the k-th shot.\nGlobality Measure. The formulation in terms of a classical post-processing function allows for the\ndefinition of a quality measure on the explicitly used partitioning of C. The authors start out with the\nextracted information EIfC(b), which denotes the number of bits necessary to get an unambiguous\nassignment of the bitstring b to the set Ca it is contained in. This is extended to a globality measure\nby averaging over all possible bitstrings, i.e. GfC :=\n1\n2n\nP\nb∈{0,1}n EIfC(b). This measure quantifies,\nhow much information is used on average to make an decision for an action. While this measure\nis hard to compute in general, the authors discuss an explicit construction of a post-processing\nfunction, that guarantees saturating the globality measure (which is trivially upper-bounded by\n27\nthe number of involved qubits). Based on that construction, an optimal post-processing function\nis given by fC(b) =\nh\nb0 · · · bm−1\n\u0010Ln−1\ni=m bi\n\u0011i\n10, where [·]10 refers to the decimal representation and\nm = log2(|A|) −1.\nExperimental Results.\nThe claim that a high value of the globality measure correlates with a\ngood RL performance is experimentally demonstrated on several environments. Experiments on\nthe CartPole benchmark with globality values ranging from GfC = 1.0 to the maximum possible\nGfC = 4.0 show a clear correlation between the measure and the actual performance of the resulting\nalgorithm. It is noted, that the construction of the post-processing function explicitly is detached\nfrom the complexity of the actual quantum model, and therefore is a very efficient way to improve\nthe performance.\nThe QRL agents with GfC > 3.0 also outperform the SOFTMAX-VQC policy,\nwhich was originally conjectured to be superior in [Jer+21a]. These results are strengthened by\nexperiments on FrozenLake and ContextualBandits environments. Empirical results regarding\neffective dimension and the Fisher information spectrum [Abb+21] also demonstrate an improved\nexpressivity and trainability of models with high globality measure.\nTraining on Quantum Hardware. Using this enhanced QPG algorithm, the authors execute a full\ntraining routine on an 8-state and 2-action ContextualBandits environment on quantum hardware.\nThey employ a 3-qubit sub-topology of the 5-qubit IBM quantum device ibmq_manila [IBM23]. The\nresults confirm, that training VQC-based QRL algorithms on actual hardware is indeed possible.\nHowever, there is still a deterioration of performance compared to the noise-free simulation, which\nis explained by the currently inevitable hardware noise.\nVerification of the learned parameters\ndemonstrates, that the agent actually identifies the optimal action in all cases, only the certainty\nof that decision is less pronounced compared to simulation.\nRemarks. The described action decoding procedure is easy to extend to problems with large action\nspaces.\nHowever, some additional engineering is necessary to account for action spaces of size\nthat cannot be expressed as a power of two. It is left open, at which point the benefit of using\na post-processing function with high globality is out-weighted by the likely occurrence of barren\nplateaus [Cer+21]. Potentially the flexible definition of the post-processing function can be used\nto balance those two objectives. While the demonstration of trainability on quantum hardware is\ncertainly pretty small-scale, it can be considered an important step towards the practical usability\nof these type of algorithms.\nAlgorithmic Characteristics - Meyer et al. [Mey+23b]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGatesa\nCartPole\nREINFORCE\nPolicy\ncontinuous\ndiscrete\n4\n24 to 40b\n(OpenAI Gym)\n4-dim\n2\nFrozenLake\nREINFORCE\nPolicy\ndiscrete\ndiscrete\n4\n24 to 40\n(OpenAI Gym)\n16\n4\nContextualBandits\nREINFORCE\nPolicy\ndiscrete\ndiscrete\n5\n70\n(see [SB18])\n32\n8\nContextualBandits\nREINFORCE\nPolicy\ndiscrete\ndiscrete\n3\n30\n(see [SB18])c\n8\n2\na this entails encoding, scaling, and variational parameters;\nb the SOFTMAX-VQC also uses additional classical parameters;\nc hardware experiment: modified circuit structure to reduce transpilation overhead, details in [Mey+23b];\n28\nQuantum Natural Policy Gradients: Towards Sample-Efficient Reinforcement Learn-\ning, Meyer et al. (2023)\nSummary. The paper by Meyer et al. [Mey+23a] proposes an enhanced training routine for the\nframework proposed in [Jer+21a] and extended in [Mey+23b]. A second-order extension – based on\nso-called quantum natural gradients – is employed to define the quantum natural policy gradient\n(QNPG) algorithm.\nThe modified technique is experimentally demonstrated to have preferable\nproperties regarding trainability, and is also verified on actual quantum hardware.\nNatural Gradients. The original QPG algorithm is trained based on first-order updates, i.e. ∆θ =\nα∇θL(θ). This update structure has the shortcoming, that it is closely tied to the Euclidean geome-\ntry and does not take into account the actual curvature of the loss landscape. This can be mitigated\nby using the FIM F(θ), which describes the local curvature of the parameter space around a given\npoint. This can be used to define a natural gradient update as ∆θ = αF −1(θ)∇θL(θ) [Ama98].\nFigure 10: QNPG pipeline proposed by Meyer et al. [Mey+23a]; The pseudoinverse of the quantum\nFIM is used to perform training in an undistorted neighborhood of the loss landscape.\nQuantum Natural Policy Gradients. In order to employ this concept for training in the quantum\nrealm, the paper employs a generalization of the classical FIM. This quantum FIM (derived from the\nFubini-Study metric tensor [Che10]) g(θ) is hard to compute in general – however, a block-diagonal\napproximation can be estimated efficiently in hardware [Sto+20]. Based on that the paper defines\nthe QNPG update rule as ∆θ = αg†(θ)∇θL(θ). Additionally, a regularized version of the QNPG\nalgorithm is introduced, to counter instabilities encountered during inverting the quantum FIM. It\nhas to be highlighted, that the overhead of incorporating these second-order update rule is almost\nnegligible compared to the anyways necessary computation of first-order gradients. The pipeline of\nthe overall algorithm is visualized in Fig. 10.\nExperimental Results.\nThe effectiveness of the training routine is demonstrated on different in-\nstances of ContextualBandits. On a small-scale setting with only a single qubit and two trainable\nparameters, it is shown that the (regularized) QNPG algorithm converges significantly faster for\nrandom initializations compared to the original QPG formulation. For specific initializations it is\nmoreover validated, that the second-order extension does what it was designed for and helps to\ntraverse distorted regions of the loss landscape. An up-scaled experiment with a 12-qubit VQC\nunderlines the efficiency of the introduced routine.\nTraining on Quantum Hardware. To demonstrate the practical feasibility of the QPG approach\nthe authors train an medium-scale instance on actual quantum hardware. The experiment em-\n29\nploys a 12-qubit sub-topology of the 27-qubit system ibmq_ehningen [IBM23]. The results demon-\nstrate, that the quantum agent is actually able to learn meaningful behavior in the 4096-state\nContextualBandits environment.\nThere is some deterioration of the performance compared to\nnoise-free simulation, which is not caused by the training routine itself, as demonstrated by ex-\nperiments with analytically optimal parameters. However, the learned policy identifies the correct\naction in a majority of the cases, similar to the hardware results in [Mey+23b].\nRemarks. The paper demonstrates the effectiveness of the QNPG routine for ContextualBandits\nenvironment, the extension to more generic problems is however left for future work. A very inter-\nesting consideration is the influence of quantum natural gradients on the barren plateau problem,\nwhich is discussed with different results in Refs. [HK21; Tha+23]. The hardware experiment using\n12 qubits is a big improvement upon the results in [Mey+23b] and can be considered as the currently\nlargest-scale practical demonstration of VQC-based QRL.\nAlgorithmic Characteristics - Meyer et al. [Mey+23a]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGates\nContextualBandits\nQNPG\nPolicy\ndiscrete\ndiscrete\n1\n1 (encoding)\n(see [SB18])\n2\n2\n2 (weights)\nContextualBandits\nQNPG\nPolicy\ndiscrete\ndiscrete\n12\n12 (encoding)\n(see [SB18])a\n4096\n2\n36 (weights)\na hardware experiment: hardware-native circuit structure, details in [Mey+23a];\n4.2.3\nCombined Approximations\nIt is possible to combine the approach of approximation in value space from Sec. 4.2.1 and in policy\nspace from Sec. 4.2.2. This is formulated in an actor-critic approach in Wu et al. [Wu+23], which\nis re-implemented and extended in Refs. [Kwa+21; Ree23]. An asynchronous training routine is\nproposed by S. Y.-C. Chen [Che23a]. A soft actor-critic formulation is described by Q. Lan [Lan21].\nAn extension to multiple agents is proposed in Yun et al. [Yun+22] and extended in Ref. [YPK23].\nAn overview of progress in the field of quantum multi-agent RL can be found in Ref. [ZY23].\nCitation\nFirst Author\nTitle\n[Wu+23]\nS. Wu\nQuantum reinforcement learning in continuous action space\n[Kwa+21]\nY. Kwak\nIntroduction to Quantum Reinforcement Learning:\nTheory\nand PennyLane-based Implementation\n[Ree23]\nV. Reers\nTowards Performance Benchmarking for Quantum Reinforce-\nment Learning\n[Che23a]\nS. Y.-C. Chen\nAsynchronous training of quantum reinforcement learning\n[Lan21]\nQ. Lan\nVariational Quantum Soft Actor-Critic\n[Yun+22]\nW. J. Yun\nQuantum Multi-Agent Reinforcement Learning via Variational\nQuantum Circuit Design\n[YPK23]\nW. J. Yun\nQuantum Multi-Agent Meta Reinforcement Learning\nTable 6: Work considered for “QRL with VQCs – Combined Approximations” (Sec. 4.2.3)\n30\nQuantum reinforcement learning in continuous action space, Wu et al. (2023)\nSummary. This paper by Wu et al. [Wu+23] extends the concept of VQC-based RL to continuous\naction spaces. The authors choose a quantum control environment, more concretely one that encodes\nan eigenvalue problem.\nThis allows to interpret the action as a (parameterized) unitary.\nThe\nexperimental results suggest an exponential reduction in model complexity compared to classical\napproaches.\nEigenvalue Problem as RL Environment. The RL agent has to solve an eigenvalue problem, i.e., find\nthe eigenvalue of a given Hamiltonian. This should be done in the following iterative procedure: Let\nH be the Hamiltonian of an n-qubit quantum system E and |s0⟩an initial state from E. The system\nshould be driven towards the eigenstate of H, denoted as |u0⟩. Also the corresponding eigenvalue\nλ0 should be returned. Although not explicitly stated in the paper, we assume the agent should\nsearch for the eigenstate with the associated smallest eigenvalue, as this corresponds to the ground\nstate.\nThe observation for this environment is the current quantum state |st⟩, which is provided to the\nagent via some quantum channel. The actions the agent can execute correspond to parameterized\nunitaries U(θt), where θt are classical parameters sampled from the VQC via measurements. Once\ninstantiated, this unitary is applied to |st⟩to evolve the state U(θt) |st⟩= |st+1⟩. The agent receives\na classical reward, which describes the closeness of the current state to the searched eigenstate of\nthe Hamiltonian.\nThe authors state, that their proposed technique has some parallels to Grover’s search. More\nconcretely, the trained agent provides an alternative to the amplitude amplification procedure, which\ncould alternatively be used to solve the task at hand.\nModel Architecture and Underlying RL Algorithm. The overall approach can be considered hybrid,\nas the optimization of the VQC parameters is still conducted on classical hardware. A schematic\ndescription of the approach is given in Fig. 11.\nThe agent observes a quantum state from the\nenvironment, which is used as the initial state |st⟩of the VQC function approximator. Measurements\non the prepared quantum state determine the parameters |θt⟩. Those are then fed into the unitary\noperator |U(θt)⟩and applied to the environment state. The new state |st+1⟩, combined with an\nancilla reward qubit initialized to |0⟩, is then evolved using some user-defined reward unitary Ur.\nMeasurements are performed on this state to determine the reward produced by the executed action.\nThis procedure repeats several timesteps, with the objective to approximate the eigenstate |u0⟩.\nThe VQC architecture does not incorporate a feature map, as the observation |st⟩is used as the\ninitial state |Φ⟩. Each parameterized layer consists of 1-qubit rotations and a circular entanglement\nstructure. For every element of the action parameters θj, there is an associated observable Bj, which\nis measured on the prepared quantum state. (The paper does not mention, how the action unitary\nU(θ) is explicitly constructed.)\nFollowing this step, a phase estimation circuit implements the\nreward unitary Ur = UPE. This transforms the state to the basis of eigenstates, i.e., UPE |0⟩|st+1⟩=\nPn\nk=1 αt+1,k |λk⟩|uk⟩. With a measurement of the eigenvalue phase register, the desired eigenvalue\nλ0 is observed with a probability of pt+1 = |αt+1,0|2 = | ⟨st+1|u0⟩|2. The reward can then be defined\nas e.g. rt+1 = pt+1 −pt. Obviously, for pt+1 →1, the state |st+1⟩converges to |u0⟩.\nThe underlying RL routine is an actor-critic method. Therefore, the paper combines a policy-\nVQC as actor and a Q-function-VQC as critic to a so-called quantum deep deterministic policy\ngradient (QDDPG) algorithm.\nThe experience of the agent, i.e., tuples (|st⟩, θt, rt, |st+1⟩), are\nstored in a replay buffer to prevent overfitting.\nAdditionally, target networks are employed for\nboth, the actor and the critic.\nExperimental Results and Model Complexity. All experimental results in the paper are based on\nclassical simulations. For training, the Hamiltonian H = 1\n4(sxσx + syσy + szσz + I) is instantiated\nwith the coefficients (sx, sy, sz) = (0.13, 0.28, 0.95). Concrete details on the training procedure,\ne.g., the number of episodes, are not stated. The trained model is applied to 1000 random initial\nstates. The overlap with the respective |u0⟩is approaching one, consequently the agent is able to\n31\n(a) The QRL model. Each iterative step can be\ndescribed by the following loop: (1) at step t, the\nagent receives |st⟩and generates the action\nparameter θt according to the current policy; (2)\nthe agent generates |st+1⟩= U(θt) |st⟩(3) based on\n|st⟩and |st+1⟩, a reward rt+1 is calculated and fed\nback to the agent, together with |st+1⟩; (4) based\non st+1 and rt+1, the policy is updated and then\nused to generate θt+1.\n(b) The quantum circuit for our QRL framework at\neach iteration. The entire QRL process includes two\nstages, so we give the circuit separately. In stage 1,\nthe circuit includes two registers: the reward\nregister, initialized |0⟩, and the environment register\n|st⟩. Upolicy is generated by the quantum neural\nnetwork, and determines the action unitary U(θt).\nUr and M are designed to generate the reward rt+1.\nIn stage 2, the circuit has only environment register\nand does not need to feedback the reward value and\nupdate the policy.\nFigure 11: Hybrid model for quantum environment proposed by and taken from Wu et al. [Wu+23]\n(including subcaptions); We note an ambiguity in notation, as the parameters θt,i must not be\nconfused with the parameters of the action unitary U(θ). The first set are the ones updated by the\nRL algorithm, the other ones are extracted via measurements from the quantum state prepared by\nthe VQC.\nget quite close to the desired eigenstate in all cases. The trained model shows good generalization\ncapabilities, i.e., it can be applied to various initial states. This is in contrast to e.g. a variational\nquantum eigensolver (VQE), where the control pulse for one initial state is meaningless for other\nones.\nThe overall gate complexity for one RL episode is stated as O(m · polylog(N)). Here, m is\nthe number of shots for sampling expectation values and N denotes the number of qubits. This\nstatement assumes that H can be efficiently simulated as otherwise the complexity of UPE would\nexceed O(polylog(N)).\nAdditionally, all VQCs in the method are also assumed to have a gate\ncomplexity of at most O(polylog(N)). With this perquisites, the authors claims an exponential\nadvantage in model complexity compared to classical approaches.\nGeneralization to Discrete Action Spaces.\nThe paper also generalizes the presented concept to\ndiscrete action spaces, with the FrozenLake environment as an example.\nThe observations are\nencoded as basis states into the VQC via computational encoding, similar to Chen et al. [Che+20].\nThe movements applied by the actions are formulated as unitaries acting on the VQC state. A\nslight generalization of Chen et al. [Che+20] is used for this, which allows to perform the transforms\n|0⟩→|1⟩and |1⟩→|0⟩in a parameterized manner. The reward unitary is formulated in a similar\nfashion. It is stated that experiments with this configuration were successful, but no concrete results\nare provided.\nRemarks. There are some caveats and ambiguities we identified regarding the proposed approach.\nFirst, the algorithm requires knowledge of and ability to prepare the desired eigenstate |u0⟩for\nthe training procedure. With this state already known, the whole procedure of reproducing it is a\nsomewhat circular task. However, as the learned model seems to generalize to different input states,\nthe technique offers clear advantage over approaches like quantum phase estimation. Second, the\nmodel requires repeated preparation of the environment state |st⟩, as it is disturbed by measurements\nto extract the reward information.\nThis should be doable, as one knows the state preparation\n32\nroutine |st⟩= U(θt−1) · · · U(θ0) |s0⟩.\nThe influence of this additional overhead is unfortunately\nnot considered in the complexity considerations discussed above. Third, the claim of exponential\nquantum advantage w.r.t. model complexity (i.e. O(polylog(N)) for all VQCs) should be supported\nby larger-scale experiments.\nAlgorithmic Characteristics - Wu et al. [Wu+23]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGates\nQuantum\nActor-Critic\n“QDDPG”\nQ-function,\nquantum\nconti-\nnuousa\nn\n0 (encoding)b\nn × d × 3 (weights)c\nEigenvalues\nPolicy,\n(see [Wu+23])\nEnvironment\nFrozenLake\nActor-Critic\nQ-function,\ndiscreted\ndiscreted\nn\n0 (encoding)b\n(OpenAI Gym)\n“QDDPG”\nPolicy\n16\n4\nN/A (weights)\na output is interpreted as parameters of a unitary, i.e. a quantum operation applied to the environment;\nb the RL state is a quantum state, i.e. no classical information has to be encoded;\nc variational gates: qubits × layers × per_qubit_per_layer; details are not specified;\nd state and action space are encoded into the quantum realm for a neat integration into the pipeline;\nIntroduction to Quantum Reinforcement Learning: Theory and PennyLane-based Im-\nplementation, Kwak et al. (2021)\nSummary. The paper by Kwak et al. [Kwa+21] gives a short introduction to both RL and (varia-\ntional) QC. This is followed up by a tutorial on how to implement a VQC-enhanced RL algorithm\nwith PennyLane to solve the CartPole environment.\nHybrid RL Agent. The paper employs the typical hybrid structure, with the VQC as a function ap-\nproximator. The optimization of the parameters and the interaction with the CartPole environment\nis executed on classical hardware. The underlying algorithm uses an actor-critic approach, where\nthe actor is quantum and the critic is classical. A set of 1-qubit rotations is used to encode the state\nof the CartPole environment into the four-qubit system. This encoding layer is followed by 4 layers\nwith learnable 1-qubit rotations and an unspecified entanglement structure. The result is extracted\nfrom the measurement of 2 qubits in the computational basis and the respective expectation values\nare interpreted as the action-value function.\nRemarks.\nThe agent is able to surpass random behavior, but lacks behind other hybrid ap-\nproaches [LS20; Jer+21a]. To the best of our understanding, the implemented quantum actor-critic\napproach deviates in some details from previously considered approaches. Most importantly, a hy-\nbrid approach is used, where the actor is represented with a VQC and the critic employs a classical\nDNN. A benchmark analysis of the described setup is proposed and conducted by V. Reers [Ree23].\nAlgorithmic Characteristics - Kwak et al. [Kwa+21]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGatesa\nCartPole\nActor-Criticb\nQ-function\ncontinuous\ndiscrete\n4\n4 × 1 (encoding)\n(OpenAI Gym)\n4-dim\n2\n4 × 4 × 3 (weights)\na encoding gates: qubits × per_qubit; variational gates: qubits × layers × per_qubit_per_layer;\nb only the actor employs a VQC, the critic uses a classical DNN;\n33\nAsynchronous training of quantum reinforcement learning, S. Y.-C. Chen (2023)\nSummary. This work by S. Y.-C. Chen [Che23a] introduces an actor-critic approach, that is trainable\nin an asynchronous fashion.\nThis yields the big advantage, that training could be spread out\nover several classical simulators or quantum hardware devices. The efficiency of the introduced\nquantum asynchronous advantage actor critic (QA3C) algorithm compared to previous formulations\nis demonstrated on several benchmark environments.\nQuantum A3C. The underlying concept is based on the classical A3C algorithm [Mni+16]. This\nframework makes use of a global shared memory and a process-specific memory for each individual\nagent. Each agent interacts with the environment independently, and only once certain criteria are\nmet the global model is updated using the information provided by the local agents. This enables\na distributed and therefore easy parallelizable training routine. The approximator for Q-function\nand policy both are realized using VQCs with classical neural networks pre- and appended to form\na hybrid model.\nExperimental Results. The proposed QA3C algorithm is executed on the environments Acrobot,\nCartPole, and MiniGrid-SimpleCrossing. It is observed over all instances, that the hybrid quan-\ntum model is competitive with a much larger classical model. Moreover it is demonstrated, that\nQA3C outperforms classical A3C employing classical models of comparable complexity.\nRemarks. The distribution of the training among several workers is certainly an important consid-\neration taking the current access modalities of quantum hardware providers into account. However,\nit is not clear if training practically can be distributed considering the long queue waiting times.\nMoreover, it has to be taken into account, that it is not clear what actually is the role of the VQC,\ndue to the appended neural networks. However, the comparison to full-classical agents of similar\nsize is an interesting consideration. As usually it has to be highlighted that the experiments were\nto small-scale to make meaningful statements on potential quantum advantage.\nAlgorithmic Characteristics - S. Y.-C. Chen et al. [Che23a]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGatesa\nAcrobot\n(OpenAI Gym)\nActor-critic\n”QA3C“\nQ-function,\nPolicy\ncontinuous\n6-dimc\ndiscrete\n3c\n8\nN/A (encoding)\n48 (weights)b\n148 (classical)\nCartPole\n(OpenAI Gym)\nActor-critic\n”QA3C“\nQ-function,\nPolicy\ncontinuous\n4-dimc\ndiscrete\n2c\n8\nN/A (encoding)\n48 (weights)b\n107 (classical)\nSimpleCrossing\n(OpenAI Gym)\nActor-critic\n”QA3C“\nQ-function,\nPolicy\ncontinuous\n127-dimc\ndiscrete\n6c\n8\nN/A (encoding)\n48 (weights)b\n2431 (classical)\na the training process is distributed over 80 workers, which incorporate a local copy of the parameters;\nb actor and the critic are composed of an individual hybrid model, i.e. the number of weights are doubled;\nc action and state-spaces are mapped to the required dimensionality by using classical neural networks;\nVariational Quantum Soft Actor-Critic, Q. Lan (2021)\nSummary. The paper by Q. Lan [Lan21] introduces a quantum version of a soft actor-critic (SAC)\napproach. The advantage of this algorithm, compared to previous suggestions, is the possibility to\nwork with a continuous action space. The algorithm is tested on the Pendulum environment.\nSoft Actor-Critic for Continuous Control. The term continuous control refers to a setup, in which the\nagent acts in a continuous action space. Most publications in the context of QRL deal with discrete\n34\naction spaces, while a few others discuss continuous control for quantum environments [Wu+23;\nSSB23]. This work focuses on classical environments, which requires some kind of action decoding\nbased on measurements of the quantum state. Instead of directly selecting the actions based on\nmeasurement results, the parameterized hybrid model learns the parameters of a distribution, from\nwhich the action is sampled. The VQC, and a downstream NN, are used to represent mean µ and\nvariance σ of a Gaussian distribution. This allows the agent to act in a continuous action space in\na straightforward manner.\nIn contrast to the standard RL setup, SAC [Haa+18] not only aims to optimize the expected\nreturn, but also the policy entropy [Zie+08; Haa+17]. Therefore, the expected return is defined as\nGt = P∞\ni=t γi−t(r(si, ai)+αH[πθ(·|si)]), where H[p] = −\nR\nR p(x) log p(x)dx is the differential entropy\nfor the probability density function p(x).\nAmong other advantages, this entropy normalization\npotentially enhances exploration by encouraging more stochastic policies [Haa+17].\nVQC Architecture. The paper considers two different VQC architectures. The first one uses the\ntypical three-part structure of rotational encoding, variational layers, and measurements.\nThe\nsecond architecture is more complex, as it uses data re-uploading [Pér+20], and a more complex\nencoding structure [SJD22; Jer+21a]. It can be expected, that the second choice gives rise to more\nexpressive models, which usually correlates with RL performance.\nExperimental Results. The experimental section of the paper compares the performance of the two\nresulting quantum SAC approaches to a classical NN on the Pendulum environment. On the one\nhand, the quantum model with the simple VQC architecture is inferior to the other two approaches.\nOn the other hand, the quantum model with data re-uploading performs similar to the classical\nmodel, and both are able to learn near-optimal behavior. The quantum model incorporates only 41\nparameters, while the classical one uses 1250. This is interpreted as an quantum advantage w.r.t.\nparameter complexity.\nSome additional architecture experiments are conducted, mainly focusing one the depth of the\nunderlying VQCs. It is observed, that a certain number of variational layers is required to enable\ntraining.\nOverall, the performance is strongly correlated with the concrete architecture choice,\nwhich is in line with the results known from literature [Fra+22].\nRemarks. To substantiate the claim of quantum advances w.r.t. parameter complexity, more ex-\nperiments with increasing environment size should be performed. By using NNs in combination\nwith VQCs, it is not completely clear, which part of the learning is actually conducted by the\nquantum part. The differing performance of the two architecture choices highlight the importance\nof designing a sophisticated data encoding scheme.\nAlgorithmic Characteristics - Q. Lan [Lan21]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGatesa\nPendulum\nQuantum-\nQ-function\ncontinuous\nconti-\nnuous\n3\n3 to 12 (encoding)\n(OpenAI Gym)\nSAC\n3-dim\n36 (weights)\na the hybrid model also incorporates additional classical parameters in an appended NN;\nQuantum Multi-Agent Reinforcement Learning via Variational Quantum Circuit De-\nsign, Yun et al. (2022)\nSummary. This paper by Yun et al. [Yun+22] introduces a quantum multi-agent reinforcement\nlearning (QMARL) approach. It is applied to an environment inspired by wireless communica-\ntion. The authors achieve results that are competitive with classical NNs with higher parameter\ncomplexity.\n35\nQMARL Framework and VQC Architecture. The approach is inspired by the classical method of cen-\ntralized training with decentralized execution (CTDE). This approach deals with the problems intro-\nduced by a non-stationary reward structure, caused by the interaction of multiple agents [Low+17].\nThe actor-critic structure employs only a single critic (i.e. represented by a single VQC), which\nreceives the rewards. A naive implementation would would increase the qubit count with the number\nof agents. To resolve this problem, the state encoding routine is modified, such that only one qubit\nis required for each agent.\nThe general VQC architecture follows the typical three-part structure. The states are encoded\nusing a feature map with 1-qubit rotations. The state space of the environment is four-dimensional.\nConsequently, four qubits are used to represent the actor associated to each of the four agents. For\nthe critic, all rotations for the state of one agent are applied to a single qubit. This implies a qubit\ncount equal to the number of agents (i.e. implemented for 4 qubits in the article). The following\nlearnable layer(s) consist of 1-qubit rotations and some unspecified entanglement structure. The\nchoice of the measured observables M are not explicitly stated.\nExperimental Results and Discussion. The QMARL algorithm is applied to a communication task\nreferred to as Single-Hop Offloading environment. It simulates two clouds, between which pack-\nages have to be distributed along four edges. Each cloud and edge has a queue with a certain\ncapacity. One agent is used to learn the actions of its associated edge. The objective is to minimize\nthe overflow and underflow of queues.\nThe paper compares four different multi-agent reinforcement learning (MARL) and QMARL\nframeworks: (1) The described version, where actor and critic are represented with a VQC; (2) A\nmodified pipeline, where the critic is represented with a classical NN; (3) A small-scale classical\nMARL approach; All three setups contain 50 trainable parameters each. (4) A large-scale classical\nMARL algorithm with over 40000 trainable parameters.\nThe results demonstrate, that the QMARL approach (1) is competitive with the large-scale\nMARL algorithm (4). In contrast, the hybrid QMARL method (2) and also the small-scale classical\nMARL seem to lack expressivity to solve this task. The authors conclude, that QMARL yields some\nquantum advantage, as the parameter complexity is drastically reduced.\nRemarks. Potentially, compressing all observations of one agent into one qubit is not sufficient\nto represent the information in a lossless manner. Therefore, larger-scale experiments should be\nconducted to get more insights into the proposed quantum multi-agent architecture. The same\nholds for the reduced parameter complexity compared to classical models.\nAlgorithmic Characteristics - Yun et al. [Yun+22]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGates\nSingle-Hop\nMulti-Agent\nQ-function\nPolicy\ncontinuous\n4-dim\ndiscrete\n4\n4\n4 or 16 (encoding)a\nN/A (weights)\nOffloading\nActor-Critic\n(see [Yun+22])\n“QMARL”\na the 4 quantum actors use 4 encoding parameters each; the quantum centralized critic contains 16;\nQuantum Multi-Agent Meta Reinforcement Learning, Yun et al. (2023)\nSummary. The second paper by Yun et al. [YPK23] extends their previous approach [Yun+22]\nwith various new techniques for QMARL. It proposes to use meta-learning by pre-training only one\nindividual agent. This is followed by a fine-tuning the multi-agent scenario. Therefore, two different\ntypes of trainable parameters are used, i.e. trainable measurements are introduced to complement\nthe typical variational parameters.\nThe approach is also extended to continual learning, where\nmeta-learning is performed on multiple environments at once.\n36\nVQC Architecture and meta-QMARL. The underlying RL algorithm employs an SAC approach\nwith the VQC as function approximator for the action-value function. The quantum circuit uses\nthe three-layer structure of 1-qubit rotation data encoding, variational layers with entanglement\ngates, and measurement. The paper applies QRL to multi-agent problems and extends the original\nproposal on quantum CTDE by Yun et al. [Yun+22]. An additional step is introduced for the\ntraining procedure, resulting in a meta-learning approach.\nIn order to realize these concepts, the authors define two different sets of parameters. First,\nthere are the typical variational parameters ϕ, usually parameterizing 1-qubit rotations. Second,\nit is also possible to parameterize and train the measurement observables. The paper proposes to\nuse Mθ(m)\n1,2 = R†\nx(θ1) · R†\ny(θ2) · Z · Ry(θ2) · Rx(θ1) as observable on the m-th qubit, i.e. two trainable\nparameters for each 1-qubit observable. Basically, this trainable observable introduces a change of\nbasis, as final measurements are always performed in the computational basis. The instantiated\nobservable can be visualized on the Bloch sphere as the angle w.r.t. which the measurement is\nperformed.\nBoth parameter sets are trained in alternating steps, where the first one is referred to as meta\nquantum neural network (QNN) angle training, and focuses exclusively on the variational parameters\nϕ. This step trains only a single quantum agent, which interacts with several other agents in the\nmulti-agent environment. Unfortunately, the authors do not state how this interaction is actually\nrealized. We assume, that the quantum agent interacts with other classical agents in this initial\ntraining phase. During training, the pole parameters θ are not updated, but they can be varied with\nsome randomly selected value to form a kind of angle-to-pole regularization. The second phase, the\nlocal QNN pole training, focuses on the parameterized observables. Those are fine-tuned individually\nfor each copy of the meta-trained QNN, corresponding to the all-quantum agents interacting in the\nmulti-agent environment.\nThe authors propose, that by meta-training the network, it is more\nefficient to fine-tune the individual agents. This is justified with the lower parameter complexity, as\nthe variational parameters remain constant in the second training phase. The loss function is the\nsum of all Q-learning losses of the individual agents.\nAdditionally, the paper introduces the concept of pole memory, which refers to storing the trained\npole parameters for the individual agents. As these sets are much smaller than the set of variational\nparameters, it is more efficient to store the full configuration.\nExperimental Results. The introduced training routing is executed on a two-step two-agent environ-\nment. It is observed, that the meta-training convergence is slower than direct training of a QMARL\nagent. However, once this training has converged, finetuning is much more efficient. Overall, the\nauthors conclude, that the additional step of meta-training enhances convergence in a multi-agent\nenvironment.\nExtension to Continual Learning. The above setting is also extended to continual learning, i.e.\ntraining in more than one environment (or typically the same environment with slightly altered\ndynamics).\nThe investigation focuses on the difference in performance with and without the use of pole\nmemory. The results suggest that resetting the poles to the initial state (i.e. the parameter setting\nwith which meta training was conducted) benefits convergence speed and stability in an environment\nwith alternating dynamics. Meta training with a higher degree of angle-to-pole regularization seems\nto enhance the generalization performance of the meta-QNN.\nRemarks. The paper does not state explicitly how exactly the initial meta training is conducted.\nConsidering the results, the VQCs seem to have some capability w.r.t. transfer learning, as which\nthe meta-learning and continual training can be interpreted.\nThe idea of employing trainable\nobservables has also potential for other approaches, as it partially avoids the necessity to explicitly\npre-select an action decoding scheme. Practically, these trainable observables are introduced by\nadding an additional layer to the VQC which learns a specific measurement. A significant difference\nto pre-existing procedures is that these parameters are not trained simultaneously with the typical\n37\nvariational parameters.\nIt is not completely clear, whether this two-step training procedure is\nbeneficial in a general setup.\nAlgorithmic Characteristics - Yun et al. [YPK23]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGatesa\nSingle-Hop\nMeta-Multi-\nQ-function\ncontinuous\n4-dim\ndiscrete\n4\n4\n4 (encoding)\nN/A (weights)\nOffloading\nAgent SAC\n(see [Yun+22])\n“Meta-QMARL”\nTwo-Step\nMeta-Multi-\nQ-function\ncontinuous\n4-dim\ndiscrete\n2\n2\n2 (encoding)\nN/A (weights)\nGame\nAgent SAC\n(see [YPK23])\n“Meta-QMARL”\na the parameter counts are denoted for a single agent;\n4.2.4\nOffline Methods\nOffline reinforcement learning [Lev+20] deals with the setting, when no direct interaction with the\nenvironment is possible. Instead, the agent is trained on a set of pre-acquired data. Two alternative\nformulations for the quantum realm have been proposed in Periyasamy et al. [Per+23] and Cheng\net al. [Che+23a].\nCitation\nFirst Author\nTitle\n[Per+23]\nM. Periyasamy\nBatch Quantum Reinforcement Learning\n[Che+23a]\nZ. Cheng\nOffline Quantum Reinforcement Learning in a Conservative\nManner\nTable 7: Work considered for “QRL with VQCs – Offline Methods” (Sec. 4.2.4)\nBatch Quantum Reinforcement Learning, Periyasamy et al. (2023)\nSummary. In this work, Periyasamy et al. [Per+23] propose batch-constrained quantum Q-learning\n(BCQQ), a offline QRL algorithm based on the classical discrete batch-constrained deep Q-learning\n(BCQ) algorithm by Fujimoto et al. [Fuj+19]. Furthermore, the authors introduce a novel data\nre-uploading (DRU) scheme, which they call cyclic DRU. Experiments are executed in the OpenAI\nCartPole environment.\nAlgorithm. The key idea in BCQ is that in order to avoid a distributional shift from training to\ntesting, a trained policy should induce at test time a similar state-action visitation to that observed\nin the the offline training data, the so-called batch. Hence, the name batch-constrained.\nTo achieve this, BCQ trains a generative model Gω to pre-select likely actions based on the batch.\nThrough this selection, the policy is constrained to only choose from a subset of actions. In the case\nof a discrete action space, the generative model can be understood as a map Gω : S →∆(A) that\ntakes the current environment state as input and outputs the probability with which each action\nwould occur in the batch. In particular, if the batch is filled using transitions from a policy πb then\nthe generative model should imitated this policy, i.e. Gω(a|s) ≈πb(a|s). Therefore, Gω is called\nimitator.\n38\nThrough using this imitator, actions can be pre-selected by discarding actions whose probability\nrelative to the most likely one is below a threshold τ\n˜\nA(s) =\n(\na ∈A\n\f\f\f\f\f\nGω(a|s)\nmaxˆa∈AGω(ˆa|s) > τ\n)\n.\n(26)\nThe actions selected by the imitator are then evaluated by a Q-network, which is trained by only\nconsidering the selected actions in the loss computation.\nThe imitator itself is trained with a\nstandard cross-entropy loss\nl(ω) = −\nX\n(s,a)∈B\nlog (Gω(a|s)) .\nAdditionally, to address the overestimation bias of Q-learning towards state transitions that are\nunderrepresented in the batch, double DQN [VGS16] is employed.\nFinally, the BCQQ algorithm is obtained by applying the variational quantum deep Q-networks\n(VQ-DQN) proposed by Franz et al. [Fra+22] as function approximators for both the imitator and\nQ-network. Moreover, for the model training the authors use the AMSgrad optimizer in combination\nwith gradients approximated via SPSA. Wiedmann et al. [Wie+23] demonstrated that SPSA can\nbe used to efficiently train medium-sized VQCs with a reduced number of circuit runs, compared\nto the commonly used parameter-shift rule.\nModel Architecture. The VQC used as the function approximator for the imitator and Q-network\nis shown in Fig. 12. Each entry of the four-dimensional state vector returned by the CartPole\nenvironment is encoded using a single qubit Rx gate on an individual qubit. The variational block\ncomprises five layers containing four parameterized Ry, and four parameterized Rz gates each. In\naddition to the parameterized rotational gates, each layer also includes two-qubit CZ entanglement\ngates with nearest-neighbor connectivity.\nThe CartPole environment has two discrete actions.\nTherefore, the expectation value of the Pauli-ZZ observable on qubits 1 and 2 and Pauli-ZZ\nobservable on qubits 3 and 4 are used to decode the Q-values from the VQC. Furthermore, trainable\nclassical weights are applied on both expectation values to increase the range of possible Q-values.\nPeriyasamy et al. [Per+22] established that spreading encoding gates for the feature vector of a\ngiven data point throughout the quantum circuit results in an improved representation of the data\nwhen the expectation values are measured for observables containing all Pauli strings. Following\nthis, the authors use a re-uploading scheme, which exposes each qubit to all the entries of the\ncurrent input state vector. Contrary to the standard data re-uploading, where the encoding scheme\nis re-introduced after each variational layer as such, the encoding scheme is re-introduced with the\ninput state vector shifted by one step in a round-robin fashion. The structure of a VQC with this\ncyclic DRU is shown on the right of Fig. 12.\nFigure 12: Left: VQC that is used as function approximator in the discrete BCQQ algorithm. Right:\nVQC with cyclic DRU. Note: Each ⃗θ block represents the repetition of the variational layer ansatz\nwith different trainable parameters. Both taken from [Per+23].\nExperimental Results and Discussion. In order to evaluate the performance of BCQQ, the authors\ntrain policies on buffers with varying sizes, filled with randomly sampled environment interactions.\nAs a classical benchmark the authors train neural networks instead of VQCs on the same buffers.\n39\nFor this benchmark, they first use a fully connected neural network with a total number of 67270\nparameters and second a smaller network with just 55 parameters. The number of parameters in\nthe smaller network is much more comparable to the VQC. The authors find that the BCQQ agent\nis able to learn an optimal policy, achieving the maximum reward of 500, from a buffer of just 100\nrandom environment interactions. Interestingly, the classical agents fail to learn a policy in this low\ndata regime, suggesting a potential quantum advantage in terms of the sample efficiency.\nMoreover, the cumulative reward these models can achieve beyond 500 is tested, which shows\nthat the VQC with cyclic DRU out-performs the VQC with standard DRU. All these experiments\nwere performed using an early stopping criteria, where during training the current policy is evaluated\nin the actual environment to save computational resources. Strictly speaking, this makes the training\nnot fully offline. In a second experiment however, the authors train the VQC with cyclic DRU on\na buffer filled with 100 interactions obtained from an optimal policy with noise. From this, the\nauthors show that without early stopping the BCQQ agent can learn an optimal policy from this\nnoisy buffer.\nRemarks.\nIt remains to be shown that the observed sample efficiency scales to more complex\nenvironments. Furthermore, a more elaborate analysis of the effectiveness of cyclic DRU could give\ninsights for future VQC design.\nAlgorithmic Characteristics - Periyasamy et al. [Per+23]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGatesa\nCartPole\n(OpenAI Gym)\nBCQ\nImitator,\nQ-function\ncontinuous\n4-dim\ndiscrete\n2\n4\n4 × 1 (encoding)\n4 × 15 × 2 (weights)\nN/A (classical)b\na encoding gates: qubits × per_qubit; variational gates: qubits × layers × per_qubit_per_layer;\nb model incorporates classical weights after measurement, details are not stated;\nOffline Quantum Reinforcement Learning in a Conservative Manner, Cheng et al. (2023)\nSummary. This work by Cheng et al. [Che+23a] introduces the offline QRL algorithm, conservative\nquantum Q-learning (CQ2L). In contrast to online RL, offline RL is used in scenarios where the\nagent cannot interact with the environment during training and is hence trained purely data-driven\nfrom a set of previously collected data. The proposed algorithm is based on the classical conservative\nQ-learning (CQL) algorithm by Kumar et al. [Kum+20]. Experiments are conducted in the OpenAI\nCartPole, Acrobot and MountainCar environments.\nAlgorithm. The objective of offline RL is to learn a near-optimal policy from a fixed dataset D\nsampled with a behavior policy πb, without further environment interactions. A major challenge in\nthis setting is that the fundamental assumption that agents can sample data online is violated. This\nmeans that agents have to learn a policy or value function from out-of-distribution (OOD) data,\nwhich is nontrivial. This distributional shift makes it hard to evaluate and consequently improve\ncurrent Q-value functions, leading to an extrapolation error [Kos+21].\nUnder the online setting, agents obtain corrective feedback through environment interactions.\nHowever, for offline training, the extrapolation error means that agents could overestimate Q-values\nfor unseen state-action pairs, which could lead to poor performance. Hence, CQL suppresses the\noverestimation problem in offline RL by learning a conservative Q-value function. In particular,\nthis is achieved via double Q-learning [VGS16] and a penalty term to update the Q-values in a\n40\nconservative manner. The resulting conservative update target is obtained as\nargmin\nQ\nα · E\ns∼D\n \nlog\nX\na∈A\nexp\n\u0000Q(s, a; θA\nk )\n\u0001\n−E\na∼πbQ(s, a; θA\nk ))\n!\n+\nE\n(s,a,r,s′)∼D\n\u0010\nY DoubleQ\nk\n−Q(s, a)\n\u00112\n,\n(27)\nwith the double Q-learning target update\nY DoubleQ\nk\n:= r + γ · Q(s′, argmax\na∈A\nQ(s′, a; θA\nk ); θB\nk ).\n(28)\nHere, θA\nk and θB\nk denote two independent sets of parameters, which are updated similarly to the\ntarget network in the deep Q-network (DQN) algorithm, by symmetrically exchanging the roles\nof θA\nk and θB\nk in Eq. (28). Having these independent parameters helps to compute unbiased Q-\nvalue estimates. CQ2L is then obtained by implementing the Q-value function via the variational\nVQ-DQN proposed by Franz et al. [Fra+22].\nModel Architecture. VQCs with 5 layers to represent Q-value functions are used. For CartPole,\nAcrobot and MountainCar 4, 6 and 2 qubit systems are used, respectively. According to the feasible\nactions in these environments, quantum observables [Z0Z1, Z2Z3], [Z0, Z1, Z2], and [Z0, Z0Z1, Z1]\nare chosen, where Zi denotes the readout of a Pauli Z gate on the ith qubit. Input data are encoded\nwith X rotation gates, while the variational part includes X, Y, and Z rotation gates. Moreover,\nqubits are entangled in a circular topology. The variational part, entanglement, and data encoding\nare repeated several times, which is then measured by Pauli Z gates to determine the Q-values.\nExperimental Results and Discussion. To evaluate the offline QRL algorithm, the authors create\noffline data sampled by a DQN agent with epsilon-greedy policy, interacting with the corresponding\nenvironment. The sampled data are recorded in a replay buffer with length 1 × 106 and then saved\nfor offline QRL. The logged data contain tuples of (st, at, rt, st+1, d), where d indicates whether an\nepisode terminates. For training, a single trajectory from the collected buffer is selected.\nThe authors compare the performance of CQ2L with the off-policy VQ-DQN trained offline on\nthe same data. These experiments show that CQ2L is able to solve all given environments and\noutperform offline VQ-DQN. The latter indicates that it is not feasible to directly extend off-policy\nQRL algorithms like VQ-DQN to the offline setting. Furthermore, the authors find that CQ2L\nperforms only marginally worse than online VQ-DQN in CartPole. Interestingly, online VQ-DQN\nfails to solve Acrobot and MountainCar and is clearly outperformed by CQ2L.\nFinally, the performance is compared to classical CQL, where a fully connected neural network\nwith a similar number of parameters as the VQC is used. The results indicate that CQ2L could\nachieve comparable performance to the classical one.\nBesides, no significant advantages in the\nsample efficiency or the parameter size are observed. The authors hypothesize that this may indicate\nthat the current structure of VQCs or the limited number of qubits is not sufficient to exhibit\nquantum advantages for QRL.\nRemarks. The performance is compared to classical CQL, where a fully connected neural network\nwith a similar number of parameters as the VQC is used. The results indicate that CQ2L could\nachieve comparable performance to the classical one.\nBesides, no significant advantages in the\nsample efficiency or the parameter size are observed. The authors hypothesize that this may indicate\nthat the current structure of VQCs or the limited number of qubits is not sufficient to exhibit\nquantum advantages for QRL. This result contradicts other observations in the literature, where at\nleast for small system sizes some improvement w.r.t. parameter complexity was observed. However,\nwe agree with the statement, that such performance improvements might strongly depend on the\nspecific VQC architecture.\n41\nAlgorithmic Characteristics - Cheng et al. [Che+23a]\nEnvironment\nAlgorithm\nQuantum\nState\nAction\nQubits\nParameterized\nType\nComponent\nSpace\nSpace\nGatesa\nCartPole\n(OpenAI Gym)\nCQL\nQ-function\ncontinuous\n4-dim\ndiscrete\n2\n4\n4 × 1 (encoding)\n4 × 15 × 2 (weights)\nN/A (classical)b\nAcrobot\n(OpenAI Gym)\nCQL\nQ-function\ncontinuous\n6-dim\ndiscrete\n3\n6\n4 × 1 (encoding)\n4 × 15 × 2 (weights)\nN/A (classical)b\nMountainCar\n(OpenAI Gym)\nCQL\nQ-function\ncontinuous\n2-dim\ndiscrete\n3\n2\n4 × 1 (encoding)\n4 × 15 × 2 (weights)\nN/A (classical) b\na encoding gates: qubits × per_qubit; variational gates: qubits × layers × per_qubit_per_layer;\nb model incorporates classical weights after measurement, details are not stated;\n4.2.5\nAlgorithmic and Conceptual Extensions\nThis section describes extensions to the VQC-based QRL framework, that have relevance for multiple\nof the previously classified methods. This entails tools to deal with partially observable (quantum)\nenvironments discussed in Kimura et al. [Kim+21]. A big emphasis is put on the explicit design\nof model architectures. Work by Hsiao et al. [Hsi+22; Tru+23] demonstrates that this is indeed\nan important topic, as otherwise everything could be easily emulated with classical architectures.\nDifferent approaches to this design task are discussed in Refs. [Che23d; Che23b; Dră+22; Kru+23;\nSMT23; ACN23; PPR20]. Avoiding the typical gradient-based training routines, a evolutionary\napproach is proposed by Chen et al. [Che+22] and also discussed in Refs. [DS23; Köl+23].\nVariational Quantum Circuit-Based Reinforcement Learning for POMDP and Experi-\nmental Implementation, Kimura et al. (2021)\nSummary. The paper by Kimura et al. [Kim+21] extends the concept of VQC-based RL to par-\ntially observable environments. The approach is inspired by classical model-free, complex-valued\nRL [HSS06]. Additionally, a novel VQC architecture (novel with regard to measurement procedure)\nis proposed. A detailed description of the gradient computation with backpropagation techniques\nis provided (it is not quite clear how this method generalizes to quantum hardware).\nPartially Observable MDP. A partially observable Markov decision process (POMDP) is described\nas a tuple (S, A, T, R, Ω, O) and is a generalization of a MDP. The variable S denotes a discrete\nstate space, A is a discrete set of actions, T(s′|s, a) describes the state transition probabilities and\nR(s, a) is a reward function. Extending the fully-observable case, Ωis a discrete set of observations\nand O(o|s, a) is an observation probability matrix with o ∈Ω.\nOne caveat of partially observable environments is the perceptual aliasing problem. This refers to\nthe property, that the agent cannot distinguish two different states due to the limited observation\nability. An example of such an environment is the partially observable maze used in Kimura et\nal. [Kim+21]. Similar to most gridworld environments, the task is to navigate from the start state\nto the goal state on the shortest path possible. However, the observations provided to the agent are\nambiguous as several cells return the same state indicator.\nSolving POMDPs with Complex Valued RL. One way to bypass this state ambiguity is to introduce\na belief distribution over possible states. Unfortunately, this is computationally expensive. An\nalternative approach is complex-valued RL [HSS06; MNM17]. It incorporates time series information\ninto the action-value function, which represented as complex numbers. More concretely, the complex\n42\nCitation\nFirst Author\nTitle\n[Kim+21]\nT. Kimura\nVariational Quantum Circuit-Based Reinforcement Learning\nfor POMDP and Experimental Implementation\n[Hsi+22]\nJ.-Y. Hsiao\nUnentangled quantum reinforcement learning agents in the\nOpenAI Gym\n[Tru+23]\nN. Truong\nInvestigating Quantum Reinforcement Learning structure to\nthe CartPole control task\n[Che23d]\nS. Y.-C. Chen\nQuantum deep recurrent reinforcement learning\n[Che23b]\nS. Y.-C. Chen\nEfficient quantum recurrent reinforcement learning via quan-\ntum reservoir computing\n[Dră+22]\nT.-A. Drăgan\nQuantum Reinforcement Learning for Solving a Stochastic\nFrozen Lake Environment and the Impact of Quantum Ar-\nchitecture Choices\n[Kru+23]\nG. Kruse\nVariational Quantum Circuit Design for Quantum Reinforce-\nment Learning on Continuous Environments\n[SMT23]\nY. Sun\nDifferentiable Quantum Architecture Search for Quantum Re-\ninforcement Learning\n[ACN23]\nE. Andrés\nEfficient Dimensionality Reduction Strategies for Quantum\nReinforcement Learning\n[Che+22]\nS. Y.-C. Chen\nVariational quantum reinforcement learning via evolutionary\noptimization\n[DS23]\nL. Ding\nMulti-objective evolutionary search for parameterized quan-\ntum cirucits\n[Köl+23]\nM. Kölle\nMulti-Agent Quantum Reinforcement Learning using Evolu-\ntionary Optimization\nTable 8:\nWork considered for “QRL with VQCs – Algorithmic and Conceptual Extensions”\n(Sec. 4.2.5)\n˙Q-function ( ˙x denotes complex values) encodes the history of the agent, i.e. the previously visited\nstates. The cumulative reward value is expressed by the absolute value of ˙Q-function, while the\npath length of the propagated reward is represented by the phase of the ˙Q-function on the complex\nplane. Therefore, ˙Q-function-Learning keeps continuity w.r.t. the described internal reference value.\nThis helps distinguish states which are affected by the perceptual aliasing problem. Formally, this\nis achieved by updating the complex values in the opposite phase direction. The complex-valued ˙Q-\nfunction can be represented with tabular methods [HSS06], or with complex-valued NNs [MNM17].\nThe update mechanism represents a generalized Q-learning approach, i.e. the objective is to\noptimize the loss function Lθ = 1\n2 · | ˙Q(ot−k, at−k) −(rt+1 + γ · ˙Qmax(t)) ˙ut(k)|. Here, ˙ut(k) = ˙βk+1 is\na complex hyperparameter and k is the trace length. The NN is replaced with a VQC as action-value\nfunction approximator in the following.\nVQC Architecture and Gradient Computation. The paper deviates in several design choices from\nthe standard method. Most importantly, the ˙Q-values for the different actions are not extracted\nfrom the same circuit (e.g. measurement on different qubits corresponding to different actions).\nInstead, the actions are encoded into the VQC with a feature map similar to the one used for state\nencoding. Consequently, different circuits have to be evaluated for each action. This encoding can\n43\nhappen either directly in the feature map, or alternatively into the decoding unitary. However, the\nthree-part structure of the circuit is preserved.\nFigure 13: Encoding and decoding architectures proposed by and taken from [Kim+21];\nThe encoding unitary Uencoder consists of simple parameterized 1-qubit rotations, where three\ndifferent concrete encodings are considered as shown in Fig. 13. The Type 1 feature map encodes\nthe observations directly with an arcsin function. Type 2 uses a computational encoding for the\nobservations, basically equivalent to the one proposed by Chen et al. [Che+20]. Type 3 also uses\nan arcsin transform, but directly encodes the action information into the feature map.\nThe variational part repeats several layers of parameterized 1-qubit rotations, followed by a\ncircular entanglement structure. In the experimental part, the authors consider different circuit\ndepths.\nThe output of the circuit is evaluated with the Hadamard Test, which measures the prepared\nstate against an output unitary Uout. This introduces an overhead since a controlled version of the\nunitary Uout needs to be implemented (details in Fig. 13). Additionally, an ancilla qubit, and three\n1-qubit gates are required. The output unitary itself consists also of learnable 1-qubit rotations. If\nencoding unitaries of Type 1 or Type 2 are used, it additionally encodes the action information.\nThe real and imaginary output of the Hadamard test are used to construct the complex-valued\n˙Q-function.\nThe evaluation of the circuits is straightforward on quantum hardware. However, this does not\napply to evaluating gradients w.r.t. the parameters, which is necessary for training. The paper gives\na detailed derivation on how to compute the gradients via simulation on classical hardware. The\nidea is inspired by classical backpropagation and somewhat looks like the adjoint method [Luo+20].\nThis makes it infeasible, at least in the given form, for actual quantum hardware.\nExperimental Results and Discussion. The paper compares the training results (on the described\nmaze environment) for the three types of quantum agents to different classical agents. The classical\ntabular approach outperforms all other methods, as the underlying algorithm guarantees an optimal\nsolution. The authors argue, that there seems to be some intrinsic advantage of the Type 2 quantum\ncircuits, as these perform better then the other approximate algorithms.\nRemarks. We think there needs to be some further investigation regarding the applicability of the\nalgorithm to actual quantum hardware. Currently, we propose to consider the approach as QiRL.\nWe agree, that QC offers great potential for complex-valued RL, as QC itself deals with complex\nnumbers. However, there are still open questions regarding the most promising way to exploit this\nconnection. A quantum version of a POMDP is discussed in Ref. [BBA14], which might provide for\nan interesting extension of this paper.\n44\nUnentangled quantum reinforcement learning agents in the OpenAI Gym, Hsiao et\nal. (2022)\nSummary. The paper by Hsiao et al. [Hsi+22] uses an hybrid proximal policy optimization (PPO)\nalgorithm, with a combination of VQC and NN as policy function approximator. The quantum\ncircuit architecture is untypical, as it only uses 1-qubit rotations. Consequently, no entanglement\nis created, and all qubits can be considered as independent systems. Still, the resulting RL agent is\nable to learn good policies on some standard environments (CartPole, Acrobot, and LunarLander).\nThe learned parameters are ported to quantum hardware and tested with sophisticating results.\nUnderlying RL Algorithm and Model Architecture. The classical RL algorithm is PPO, i.e. an policy-\nbased approach. It follows the typical hybrid setup, as the VQC is used as function approximator,\nand parameter updates are computed on classical hardware. To enhance the expressivity of the\nmodel, a classical NN is appended. It uses the measured expectation values as inputs. The outputs\nof the network are post-processed using a softmax function.\nThe structure of the hybrid model is displayed in Fig. 14. The feature map consists of 1-qubit\nrotations, which is a common choice in the literature. The variational (‘parameter’ in Fig. 14) layer\nincorporates 1-qubit parameterized rotations. It is important to highlight that the circuit does not\ncontain any multi-qubit gates. Consequently, no entanglement between the qubits is created. As\nefficient classical simulation of the circuit is possible, the approach should be counted towards QiRL.\nDespite this, the authors demonstrate, that a good RL training performance can be achieved with\nthis model.\nFigure 14: Hybrid quantum-classical model proposed by and taken from Hsiao et al. [Hsi+22];\nExperimental Results.\nThe described hybrid agent is trained on three tasks from the OpenAI\nGym, i.e. CartPole, Acrobot, and LunarLander. The quantum agents outperforms several classical\narchitectures. As this is achieved with much fewer parameters, the authors claim that the approach\npoints towards potential advantage.\nThe results on LunarLander are remarkable in that regard, that it might be the most complex\nenvironment solved with VQC-based RL thus far. While the classical simulability prohibits any\nintrinsic quantum advantage, the models still are able to achieve a good performance. This gives\nrise to the questions, whether one can draw inspiration from quantum mechanics for purely classical\napproaches.\nTesting on Quantum Hardware. Once the models are trained, they are tested with the learned\nparameters on IBMQ hardware (with up to 8 qubits, depending on the environment). The models\nare able to replicate the learned near-optimal behavior.\nRemarks. As without entanglement the VQCs can be simulated classically, we agree with the authors\nthat the proposed algorithm should be considered as a QiRL approach. As the proposed model\n45\nincorporates also a classical network, it is nor clear, what part of the learning is conducted with the\nVQC. The simple circuit structure might also explain, that the results for testing on hardware are\nstable. Usually, a big portion of the noise is caused by two-qubit gates, which are not present in\nthe used VQC. A partial re-implementation of this work can also be found in Ref. [Tru+23].\nCompendium of Architecture Discussions\nAs demonstrated by the previously discussed work of Hsiao et al. [Hsi+22], it is important to\nput careful consideration into the design of the employed quantum model architecture.\nIn the\nfollowing, we briefly summarize several works that make contributions in that direction. The idea\nof incorporating the information of multiple timesteps via recurrent networks is discussed by S. Y.-C.\nChen [Che23d] and extended in Ref. [Che23b]. Several explicit VQC architectures are compared and\nanalyzed by Drˇagan et al. [Dră+22]. An automated approach for architecture generation is proposed\nin Sun et al. [SMT23].\nDifferent encoding techniques are discussed by Andrés et al. [ACN23].\nDrawing a connection to a different context, the work by Park et al. [PPR20] proposes to vary the\narchitecture itself, by dynamically in- and excluding two-qubit gates.\nRecurrent Quantum Neural Networks. The work by S. Y.-C. Chen [Che23d] proposes the use of\nquantum recurrent neural networks (QRNNs) in the Q-learning setting (see Sec. 4.2.1), specifically\nquantum long short-term memory (QLSTM) [CYF22]. This enables the agent to also incorporate\ninformation from previous timesteps into the decision process. In is experimentally demonstrated\non the CartPole environment, that the QRNN is at least least competitive – if not superior – to\npurely classical models of similar size. It is also discussed that the method might be well suited\nfor partially observable environments, establishing a connection to [Kim+21]. A continuation of\nthis line of research in Ref. [Che23b] proposes a more efficient training routine for QRNN, based on\nreservoir computing [LJ09] and the QA3C approach discussed in Ref. [Che23a].\nExplicit Architecture Comparison.\nA study by Drˇagan et al. [Dră+22] compares various circuit\narchitectures for a modified version of the FrozenLake environment. The underlying algorithm is\na quantum version of PPO (see Sec. 4.2.2) and the VQCs are combined with classical NNs to a\nhybrid model. The results suggest that the performance is strongly dependent on the choice of VQC\narchitecture. Measures like expressibility [SJA19], entanglement capability [SJA19], and effective\ndimension [Abb+21] provide an a priori indicator for the potential suitability of the architecture.\nHowever, there seems to be no clear correlation between the concrete value of these measures and\nthe RL performance.\nContinuous Environments and Encoding. The work by Kruse et al. [Kru+23] extends the actor-critic\nparadigm (discussed e.g. in Ref. [Dră+22]) to continuous action spaces. The authors demonstrate\nthat the quantum agent is able to learn in the environments Pendulum-v1 and LunarLander-v2. It\nis conjectured, that applying an arctan function to data points – as often done in literature – is\nindeed counter-productive for the overall performance. Moreover, a stacked encoding is proposed,\nwhich uses angle encoding on multiple qubits for a single data point. This allows to avoid pre-\nprocessing with a classical neural network, ensuring potential performance improvements can really\nbe attributed to the quantum agent. On both benchmarks a reduction in parameter complexity\ncompared to classical agents is reported. However, this only holds true for certain design choices,\nwhich again highlights the importance of architecture selection.\nAutomatic Generation of Architectures.\nSun et al. [SMT23] propose an automated tool for the\ngeneration of QRL-suitable circuit architectures.\nThe method is based on differential quantum\narchitecture search (DQAS) [Zha+22], i.e. the architecture itself is trained using gradient-based\nmethods. The approach is studied within the framework of quantum Q-learning (see Sec. 4.2.1) on\nthe FrozenLake environment. Using DQAS, the authors are able to identify a VQC architecture that\nseems to be very well-suited for the given problem and outperforms some typically used problem-\nagnostic circuit designs.\n46\nEncoding Considerations.\nThe work by Andrés et al. [ACN23] compares different strategies for\nencoding data into the VQC, all within the context of quantum Q-learning (see Sec. 4.2.1). Evalu-\nations are conducted on three environments within the energy-efficiency and management context.\nThe authors compare three different architecture layouts: (1) classical data is pre-processed and\nreduced in dimensionality using a NN and encoded via rotational parameters; (2) similar, but\ndata re-uploading [Pér+20] is employed; (3) classical data is normalized and encoded via ampli-\ntude encoding [SP18], output is post-processed with a NN; The authors claim superior performance\ncompared to classical models of similar size, especially using amplitude encoding. However, it has\nto be noted, that the experiments were quite small-scale. The combination with NNs complicates\nstatements on the actual contribution of the quantum part. It also has to be noted, that amplitude\nencoding might not be NISQ-compatible in the general case.\nVariational quantum reinforcement learning via evolutionary optimization, Chen et\nal. (2022)\nSummary. The main focus of the paper by Chen er al. [Che+22] is the investigation of gradient-\nfree evolutionary optimization for Q-learning with VQCs. This routine is tested in two different\nscenarios, for each of which also a state encoding scheme is proposed. More concretely, amplitude\nencoding is applied to the CartPole environment. For the gridworld environment MiniGrid with\nlarger state space (147 dimensional), the paper proposes a hybrid model with an encoding mechanism\nbased on tensor network (TN) techniques.\nAmplitude Encoding. The observation space of the CartPole environment is 4-dimensional. The\nstate values are continuous. This allows the use of amplitude encoding, i.e. two qubits can be used\nto encode the (re-scaled) values into the four amplitudes of the system. The authors follow the\nmethod described in Schuld and Petruccione [SP18]. This works fine for small systems, but requires\nnot NISQ-compatible operators for bigger instances.\nTN-based Encoding. The MiniGrid environment is similar to FrozenLake, as the goal in both envi-\nronments is to navigate from a start to a goal state on the shortest way possible. The paper uses sim-\nple environment configurations, with state spaces of size 5×5, 6×6, and 8×8. The observation space\nis of dimensionality 7×7×3. The agent has to decide between 6 actions, of which only 4 are relevant\nin the simplified scenario. The reward is defined as 1 −0.9 · number_steps/max_number_steps.\nApart from the larger observation space, we assume this environment to be about the same com-\nplexity as FrozenLake.\nThe paper addresses the problem of encoding the 147-dimensional state into a quantum feature\nmap with just 8 variational parameters. Other work uses e.g. CNNs to reduce the dimensionality\nof the feature space [LS21]. As the encoding networks have to be pre-trained, it is not quite clear,\nwhat part of the work is really done by the VQC. The authors suggest to use a hybrid encoding\nscheme based on TNs, similar to Chen et al. [Che+21]. The proposed TN technique encodes the\nobservation [v1, · · · , v147]t into the product state [1 −v1, v1]t ⊗[1 −v2, v2]t ⊗· · · ⊗[1 −vN, vN]t,\nwhere the individual elements are normalized. Those encoded states represented by the red nodes\nin Fig. 15a.\nThe trainable part of the matrix product state (MPS) outputs an 8-dimensional\ncompressed feature vector. This is represented by the 147 + 1 blue nodes and the open leg (i.e.\noutgoing edge) in Fig. 15a. The bond dimension is a hyperparameter of the MPS, which correlates\nwith the number of trainable parameters [Per+06].\nVQC Architecture. The model follows the typical three-part architecture, i.e. first the feature map,\nthen the variational part, and finally some measurements. For the CartPole environment, a simple\n2-qubit circuit with amplitude encoding and 4 variational layers is used. Both qubits are measured\nin the Pauli-Z basis and the action corresponding to the higher expectation value is selected. For the\nMiniGrid environment, the 8-qubit circuit with just one repetition of the variational layer is used.\nThe encoding is done with the TN-compressed state, i.e. the output from the TN is encoded into\n47\n(a) TN for performing dimensionality\nreduction;\n(b) VQC with feature map, several variational layers, and\n1-qubit measurements;\nFigure 15: Components of the architecture proposed by and taken from Chen et al. [Che+22].\nthe circuit as shown in Fig. 15b. As the environment has 6 actions, the top 6 qubits are measured,\nand the action corresponding to the highest expectation value is executed.\nRL with Evolutionary Optimization. The underlying algorithm is a Q-learning RL approach. The\nupdates of the QNN representing the action-value function are conducted via evolutionary optimiza-\ntion. This implies, that no gradients have to be computed. Usually, this is one major bottleneck of\nVQC-based RL, which might be circumvented by this approach.\nThe paper uses a simplistic instance of an evolutionary algorithm, where mutation, but no\nrecombination operations are employed. An initial population of M individuals is generated, which\nare used to simulate some episodes on the environment. The best T agents (the ones producing\nthe highest reward averaged over several runs) are selected as parents for the next generation.\nRandom Gaussian noise is applied to this parents (mutation), until M −1 children are generated.\nAdditionally, the best individual from the previous generation is kept, i.e. again M individuals.\nThis procedure is repeated until a certain convergence criteria is met, e.g. a high enough reward.\nExperimental Findings and Discussion.\nThe paper applies the two different encoding methods,\ncombined with the evolutionary optimization idea, to the respective environments. All experiments\nare conducted as noiseless simulations. On the CartPole environment, the 2-qubit architecture\nachieves an near-optimal performance with only 26 parameters, which is significantly less than in\nmost state-of-the-art NNs. The authors claim, that with their method the number of parameters\ncan be reduced to O(polylog(n)). In contrast, classical ML requires O(poly(n)) parameters.\nThe experiments on the MiniGrid environment employ the described hybrid TN-based architec-\nture. Results are compared to an encoding based on a simple NN, presumably similar to Lockwood\nand Si [LS21]. All approaches achieve a near-optimal performance. Overall, the TN-model (with\nlarge enough bond dimension) slightly outperforms the classical approach. The authors consider\nthis as a proof-of-principle for effectiveness of the MPS encoding for RL learning.\nRemarks. The amplitude encoding is currently not feasible for more complex problems, due to\nthe lack of an NISQ-compatible state-preparation routine. The evolutionary optimization approach\ncould circumvent some of the problems typically associated with gradient based techniques. Exper-\niments on larger-scale environments might be an interesting direction for future work, to investigate\nhow the evolutionary algorithm deals with more complex optimization landscapes. We suggest to\nincorporate some recombination procedures into the evolutionary algorithm, to enhance its perfor-\nmance.\nMulti-Objective Formulation.\nRelated work by Ding and Spector [DS23] proposes a version of\nevolutionary search for the automated generation of QRL architectures (see also the discussions on\nVQC architecture above in Ref. [Hsi+22] and related work). The training itself is done with a QPG\napproach [Jer+21a] (see Sec. 4.2.2) and nested with evolutionary architecture search [DS22]. This\n48\nprocedure is conducted w.r.t. several objectives, including enforcing a as-small-as-possible model\nsize and several noise-related considerations. The approach is validated on the three benchmark\nenvironments CartPole, MountainCar, and Acrobot. The results demonstrate improved training\nbehavior – with smaller model size – compared to previous work [Jer+21a]. The authors also further\nanalyze the learned architectures for recurring patterns. However, it is acknowledge that larger-scale\nexperiments are necessary to identify a general guideline for architecture selection.\nMulti-Agent Scenario. The work by Kölle et al. [Köl+23] extends the framework of Ref. [Che+22]\nto the multi-agent setting (see Sec. 4.2.3). The authors compare different evolutionary strategies,\nincluding mutation-only and two different setups with additional recombination steps. The evalua-\ntion is conducted on the CoinGame environment and yields results that are competitive with classical\napproaches – using significantly fewer parameters. It has to be noted, that the experiments are too\nsmall-scale to make reliable statements about the scaling behaviour of this approach. While evolu-\ntionary optimization is certainly an interesting consideration compared to gradient-based techniques,\nthe stated advantage regarding reduced proneness to barren plateaus is not sufficiently documented\nand should therefore be viewed with some scepticism.\n4.2.6\nApplication-Focused Work\nThis section summarizes work that discusses VQC-based QRL techniques for specific applications.\nOn the one hand, this is a very important area of research, in order to identify practically relevant\nQRL one day. On the other hand, it has to be noted, that all current work is limited to relatively\nsmall problem setups. This can be justified by current hardware restrictions – but also casts some\ndoubt on the scalability of the stated results. Nonetheless, an overview of the considered ideas\nmight be beneficial for further research:\nApplications related to robotics and similar control tasks are discussed in Refs. [Acu+22; Hei+22;\nCob23; BYK22; SMK23; Hic+23; KCP23].\nPlanning tasks of different form are the focus of\nRefs. [Cor+23; San+22; ACN22; Liu+23; Kum+23; Rai+23; SH23; RKM22]. Collaborative en-\nvironments are addressed with multi-agent methods in Refs. [Yan+22; Par+23a; NS+23; Par+23b;\nPK23; Yun+23; Ans+23]. The field of finances is discussed in Refs. [Che+23c; Yan23]. A back-\nto-the-roots work considers QRL for board games in Ref. [CRC23]. Last but not least, the task of\ndesigning VQC architectures is addressed in Ref. [Che23e].\nQRL for Robotics and other Control Tasks\nThe work by Acuto et al. [Acu+22] applies the quantum SAC approach proposed in Ref. [Lan21] to\nthe control of an robotic arm. The environment is implemented as an extension of the Acrobot-v1\nenvironment. On this small-scale setup the hybrid quantum model demonstrates reduced parameter\ncomplexity compared to classical methods.\nA robot navigation scenario is discussed by Heimann et al. [Hei+22] in a simulated environment.\nThe quantum Q-learning (see Sec. 4.2.1) approach demonstrates parameter reduction compared to\nclassical approaches. The setup is extended to a more complex environment by J. Cobussen [Cob23].\nA similar robot navigation task is considered in Bar et al. [BYK22], which employs the Q-learning\nmethod proposed in Ref. [Che+20]. The authors report a reduction in the number of parameters,\nwhich however also yields a decreased success rate for the considered scenarios.\nCollision-free navigation of self-driving cars is considered in Sinha et al. [SMK23]. The authors em-\nploy an actor-critic quantum A2C approach, which is similar to the QA3C introduced by Ref. [Che23a].\nOn a small 4-qubit toy environment the proposed approach shows improved training stability\ncompared to classical A2C. A similar problem is considered with tools from quantum Q-learning\n(see Sec. 4.2.1) by Hickmann et al. [Hic+23].\n49\nCitation\nFirst Author\nTitle\n[Acu+22]\nA. Acuto\nVariational Quantum Soft Actor-Critic for Robotic Arm Con-\ntrol\n[Hei+22]\nD. Heimann\nQuantum Deep Reinforcement Learning for Robot Navigation\nTasks\n[Cob23]\nJ. Cobussen\nQuantum Reinforcement Learning for Sensor-Assisted Robot\nNavigation Tasks\n[BYK22]\nN. F. Bar\nAn Approach Based on Quantum Reinforcement Learning for\nNavigation Problems\n[SMK23]\nA. Sinha\nNav-Q: Quantum Deep Reinforcement Learning for Collision-\nFree Navigation of Self-Driving Cars\n[Hic+23]\nM. L. Hickmann\nPotential analysis of a Quantum RL controller in the context\nof autonomous driving\n[KCP23]\nG. S. Kim\nRealizing\nStabilized\nLanding\nfor\nComputation-Limited\nReusable Rockets:\nA Quantum Reinforcement Learning\nApproach\n[Cor+23]\nR. Correll\nQuantum Neural Networks for a Supply Chain Logistics Ap-\nplication\n[San+22]\nF. Sanches\nShort quantum circuits in reinforcement learning policies for\nthe vehicle routing problem\n[ACN22]\nE. Andrés\nOn the Use of Quantum Reinforcement Learning in Energy-\nEfficiency Scenarios\n[Liu+23]\nD. Liu\nMulti-agent quantum-inspired deep reinforcement learning for\nreal-time distributed generation control of 100% renewable en-\nergy systems\n[Kum+23]\nM. Kumar\nBlockchain Based Optimized Energy Trading for E-Mobility\nUsing Quantum Reinforcement Learning\n[Rai+23]\nS. Rainjonneau\nQuantum Algorithms applied to Satellite Mission Planning for\nEarth Observation\n[SH23]\nM. Shahid\nIntroducing Quantum Variational Circuit for Efficient Man-\nagement of Common Pool Resources\n[RKM22]\nF. Rezazadeh\nTowards Quantum-Enabled 6G Slicing\nTable 9: [Part 1] Work considered for “QRL with VQCs – Application-Focused Work” (Sec. 4.2.6)\nThe task of steering reusable rockets is considered in Kim et al. [KCP23]. The unspecified QRL\nmethod demonstrates reduced memory requirements (by requiring fewer parameters) on an 8-qubit\ntoy environment.\nQRL for Planning Tasks\nThe vehicle routing problem (VRP) is considered by Correll et al. [Cor+23] via an quantum-\nenhanced attention mechanism. Several parts of a classical encoder-decoder model with attention\nmechanism [KVW18] are replaced with medium-scale VQCs (up to 10 qubits). With using quantum\nmethods to implement orthogonal NNs [KLM21], a potential speed-up during inference is reported.\n50\nCitation\nFirst Author\nTitle\n[Yan+22]\nR. Yan\nA Multiagent Quantum Deep Reinforcement Learning Method\nfor Distributed Frequency Control of Islanded Microgrids\n[Par+23a]\nS. Park\nQuantum Multi-Agent Actor-Critic Networks for Cooperative\nMobile Access in Multi-UAV System\n[NS+23]\nB. Narottama\nLayerwise Quantum Deep Reinforcement Learning for Joint\nOptimization of UAV Trajectory and Resource Allocation\n[Par+23b]\nS. Park\nQuantum\nMulti-Agent\nReinforcement\nLearning\nfor\nAu-\ntonomous Mobility Cooperation\n[PK23]\nS. Park\nQuantum Reinforcement Learning for Large-Scale Multi-\nAgent Decision-Making in Autonomous Aerial Networks\n[Yun+23]\nW. J. Yun\nQuantum Multi-Agent Actor-Critic Neural Networks for\nInternet-Connected Multi-Robot Coordination in Smart Fac-\ntory Management\n[Ans+23]\nJ. A. Ansere\nQuantum Deep Reinforcement Learning for Dynamic Resource\nAllocation in Mobile Edge Computing-based IoT Systems\n[Che+23c]\nE. A. Cherrat\nQuantum Deep Hedging\n[Yan23]\nJ. Yang\nApply Deep Reinforcement Learning with Quantum Comput-\ning on the Pricing of American Options\n[CRC23]\nJ. Chao\nQuantum Enhancements for AlphaZero\n[Che23e]\nS. Y.-C. Chen\nQuantum Reinforcement Learning for Quantum Architecture\nSearch\nTable 10: [Part 2] Work considered for “QRL with VQCs – Application-Focused Work” (Sec. 4.2.6)\nExperimental on a simple instance of the traveling salesman problem (TSP) are conducted to sup-\nport this claim. A simpler approach for the same task is considered in Sanches et al. [San+22],\nwhere only the attention heads are replaced with 4-qubit VQCs.\nThe work by Andrés et al. [ACN22] considers different planing tasks related to energy-efficiency\nscenarios.\nThe authors employ quantum actor-critic methods (see Sec. 4.2.2) to address these\ntasks. The authors report a slower convergence compared to classical methods, however therefore\na reduced parameter complexity. Similar scenarios within the energy context are also discussed by\nLiu et al. [Liu+23] and Kumar et al. [Kum+23].\nThe task of satellite mission planning is formulated as a scheduling problem and addressed by\nRainjonneau et al. [Rai+23]. The authors apply two different quantum-enhanced methods within\nthis context: (1) policy approximation (see Sec. 4.2.2) with VQCs; (2) replacing several components\nof AlphaZero with quantum components, similar as to discussed in Ref. [CRC23]; The experiments\nwith 4-qubit circuits demonstrate a clear improvement compared to straightforward greedy methods.\nThe problem of distributing common pool resources is discussed by Shahid and Hassan [SH23].\nQuantum-enhanced Q-learning (see Sec. 4.2.1) is applied to an 8-qubit toy environment, and superior\ntraining performance compared to classical models of similar size is reported.\nA task from mobile communication (6G slicing) is considered in Rezazadeh et al. [RKM22]. The\nauthors employ the VQC-based Q-learning approach proposed in Ref. [Che+20] and claim improve-\nments w.r.t. parameter complexity and the potential for distributed computing.\n51\nQRL in Collaborative Scenarios\nDifferent tasks that are based on the collaboration of multiple entities are discussed in a series\nof work by Yan et al. [Yan+22], Park et al. [Par+23a; Par+23b; PK23], Yun et al. [Yun+23],\nNarottama et al. [NS+23], and Ansere et al. [Ans+23]. The foundation is the multi-agent approach\nQMARL proposed in Ref. [Yun+22] with smaller extensions. On respective toy environments, the\napproaches demonstrate faster convergence and reduced parameter complexity compared to classical\nimplementations.\nQRL for Finances\nThe work by Cherrat et al. [Che+23c] addresses the task of deep hedging with distributional actor-\ncritic methods. Classical methods are modified with quantum-enhanced orthogonal NNs [KLM21],\nwhich promises speed-ups during inference. This is supported by medium-scale hardware test on\nup to 16 qubits – which makes this one of the largest-scale demonstrations of VQC-based QRL.\nAnother work within the context of finances, conducted by J. Yang [Yan23], proposes the use of\nquantum Q-learning (see Sec. 4.2.1) to speed up calculations.\nQRL for Games\nThe work by Chao et al. [CRC23] thinks back to the origins of classical RL and consider es the\nboard game Orthello, which basically is a simplified version of Go. To solve this toy environment,\nthe authors modify two components of AlphaZero [Sil+18]: (1) replacing function approximators\nwith VQCs; (2) using tensor network methods for feature extraction; For simulations on up to 12\nqubits, the methods show performance compared to classical approaches.\nQRL for Architecture Design\nS. Y.-C. Chen [Che23e] addresses the task of quantum circuit design. The author uses the actor-\ncritic method QA3C [Che23a] to generate circuits that prepare 2-qubit Bell states and GHZ states\non up to 3 qubits.\n4.3\nProjective Simulation for Quantum Reinforcement Learning\nProjective simulation for artificial intelligence, Briegel et al. (2012) and related work\nSummary. Projective simulation for artificial intelligence by Briegel et al. [BD12] is the first in a\nseries of articles, which propose a learning scheme for creative behavior. This is understood in the\nsense that the agent can deal with unseen experiences by relating to other conceivable situations.\nThe method is developed for classical agents. There is only a brief final paragraph, outlining a\nquantum-mechanical implementation. Since subsequent papers ‘quantize’ the original idea heavily, a\nbrief summary is in order: The approach is based on a random walk on a previous-experience network\n(memory), simulating an agent pondering its next action. More specifically, previous experiences\ncompose a network of clips, which is dynamically modified by new experiences. It is important to\nnote that clips, in contrast to actual experiences, are e.g. remembered observations, states or actions.\nTo select the next action, an observation of the agent activates a clip, followed by a random walk\nthrough the network (projective simulation). This is repeated until an action is ‘excited’ and coupled\nout from the network and the action is selected. It is worthwhile noting that the term projective as\nused here is not related to its use in quantum physics, such as in projective measurement.\nAction Selection. The process of action selection is slightly more sophisticated than described above.\nIf a percept s is observed, a random walk through the network starts from the corresponding percept\nclip. After some deliberation time the random walk reaches an action clip, which is only out-coupled\nand taken in reality if the percept-action pair (s, a) was rewarded in the past (i.e. tagged positively).\n52\nCitation\nFirst Author\nTitle\n[BD12]\nH. J. Briegel\nProjective simulation for artificial intelligence\n[Mel+17]\nA. A. Melnikov\nProjective simulation with generalization\n[Boy+20]\nW. L. Boyajian\nOn the convergence of projective-simulation–based reinforce-\nment learning in Markov decision processes\n[Pap+14]\nG. D. Paparo\nQuantum Speedup for Active Learning Agents\n[Tei21]\nM. Teixeira\nQuantum Reinforcement Learning Applied to Games\n[TRC21]\nM. Teixeira\nQuantum Reinforcement Learning Applied to Board Games\n[DFB15]\nV. Dunjko\nQuantum-enhanced deliberation of learning agents using\ntrapped ions\n[Sri+18]\nT. Sriarunothai\nSpeeding-up the decision making of a learning agent using an\nion trap quantum processor\n[Fla+23]\nF. Flamini\nTowards interpretable quantum machine learning via single-\nphoton quantum walks\nTable 11: Work considered for “Projective Simulation for QRL” (Sec. 4.3)\nIf not, a new simulation is started. This process repeats until an action clip with positive tag, or a\npredefined reflection time is reached; in the latter case the action is out-coupled irrespective of the\ntag.\nLearning Procedure. The actual learning process can be summarized as follows:\n1. If a transition (s, a) is rewarded, increase the network weight of the direct transition s →a.\n(Note that the agent might have chosen s →a after many steps of PS; by reinforcing the\ndirect transition, it might be exploited directly next time);\n2. Increase the weights of the indirect transition (all weights of the network that led to the\ntransition s →a in the random walk through the network). Thus, the agent discovers useful\nactions after deliberation of fictitious clips;\n3. Introduce damping of all weights to let the agent forget, in order to be able to adapt to new\nsituations (as appearing for example in a time-dependent environment);\n4. If a new situation is discovered, a corresponding clip is added to the network and directed\nedges from all the other clips to the new one are added;\n5. Additional extensions can be implemented, such as modifications of clips and creation of\ncompletely fictitious compositions of episodes;\nThis line of research has been continued in Ref. [Mel+17] (generalization) and [Boy+20] (conver-\ngence). In the last paragraph of Ref. [BD12] a quantum version of the algorithm is briefly discussed.\nThe idea is to replace the random walk on the network by a quantum walk. A number of subsequent\npapers investigate the quantum approach more rigorously:\nIn order to define a quantum walk algorithm as done in Ref. [Pap+14], the PS approach is\nviewed slightly different. The given clip network with the percept set S is separated in |S| disjoint\nnetworks. Thus one obtains a directed weighted graph (a Markov chain) for each percept with\naction clips as absorber states. Each of the actions is initially flagged (corresponding to the emotion\ntags of the initial projected simulation proposal). If an actual out-coupled action did not lead to\na reward, this particular flag is removed. Now the action selection proceeds in the following way:\n53\nIf the agent observes a percept s, a random walk starts through the graph (deliberation) until\nan action is reached, which is out-coupled only if the action is flagged (reflection). Thus, action\nselection corresponds to sampling from the conditional probability distribution over the flagged\naction space. Given the transition matrix P of the Markov chain, subsequent applications of P to\nthe initial state (probability one for the percept clip) realizes the approximate stationary distribution\n(subsequently referred to as diffusion). Sampling from this distribution and disregarding un-flagged\nactions produces the correct samples. As ps is the probability to sample a flagged action from the\nequilibrium distribution obtained by diffusion, one needs to repeat the sampling process O(1/ps)\ntimes until a flagged action is sampled. The quantum random walk search algorithm is closely related\nto Grover’s algorithm. By elevating the transition matrix to a diffusion operator and introducing\nan oracle that marks flagged actions, the quantum algorithm only needs O(1/√ps) oracle calls.\nConsequently, a quadratic speed-up for the deliberation process can be achieved. Therefore, this\nquantum algorithm speeds up the agent’s internal computation time for action selection.\nThis\ntechnique is extended and applied to board games in Refs. [Tei21; TRC21].\nExperimental Implementation. In Ref. [DFB15] the authors investigate the implementation of the\nalgorithm proposed by Ref. [Pap+14] on an ion trap quantum computer. Results are also backed\nup by numerical simulations. The actual proof-of-principle experiment with two qubits is discussed\nin Ref. [Sri+18], where signatures of the quadratic speed up are observed. Ref. [Fla+23] proposes\na quantum-optics based implementation of the projective simulation paradigm. Here, the random\nwalk through the clip network is promoted to a quantum walk of a single photon through an optical\ninterferometer. Outcoupling of an action then corresponds to an occupation number measurement\nof output modes.\n4.4\nBoltzmann Machines for Quantum Reinforcement Learning\nCitation\nFirst Author\nTitle\n[Jer+21b]\nS. Jerbi\nQuantum Enhancements for Deep Reinforcement Learning in\nLarge Spaces\n[Cra+18]\nD. Crawford\nReinforcement Learning Using Quantum Boltzmann Machines\n[Sch+22]\nM. Schenk\nHybrid actor-critic algorithm for quantum reinforcement learn-\ning at CERN beam lines\n[Lev+17]\nA. Levit\nFree energy-based reinforcement learning using a quantum\nprocessor\nTable 12: Work considered for “Boltzmann Machines for QRL” (Sec. 4.4)\nQuantum Enhancements for Deep Reinforcement Learning in Large Spaces, Jerbi et\nal. (2021) and related work\nSummary. The work presented in Ref. [Jer+21b] investigates an alternative NN architecture to\nthose often used for learning the Q-function (or more generally the merit function) in RL tasks. The\nauthors argue that these alternative models perform advantageously in large action spaces. This is\ndue to their capability to represent multimodal functions better than standard network architectures,\nwhile using a similar number of parameters. It is further found that these alternative architectures\nare closely related to energy-based models, some of which admit quantum representations. In turn,\nthis allows quantum evaluations, enabling a provable quantum speed-up for fault-tolerant quantum\ncomputing.\n54\nMotivation. The standard architecture for Q-learning with NNs is depicted in Fig. 16 (upper part).\nThe representation of a state is fed into a NN, which outputs the values of the so-called merit function\n(the Q-value in case of Q-learning) for each possible action (given the state). The policy can be\nderived from this function by with softmax post-processing. The effective-temperature parameter\nis decreased over time to reduce exploration and enhance exploitation.\nThe authors argue that this NN architecture is not suited for large action spaces. It has to output\na high dimensional function, i.e. the merit functions for all actions simultaneously for a given state.\nThe authors argue that this network is unable to approximate a multimodal merit function in case\nof complex state-action correlations. Instead, the authors discuss the NN structure shown in the\nlower part of Fig. 16. Here, the state and action is fed to the NN, which outputs the corresponding\nmerit function. Action selection is done by sampling from the probability distribution, given by\na softmax function on the values of the merit function. Therefore, sampling requires |A| forward\npasses, where A is the action set, making action selection a computationally expensive task for large\naction spaces.\nExperiments are conducted on a generalized GridWorld environment with a large set of actions.\nThe associated complex transition function gives rise to one optimal and many sub-optimal policies.\nThe authors find that the NN architecture shown in the lower part of Fig. 16 indeed performs better,\nbut at the cost of the expensive sampling described before.\nEnergy-based Models. The potential for quantum speed up comes from the observation that the\nsecond architecture in Fig. 16 is equivalent to a certain kind of energy-based model. Energy-based\nfunction approximators are used for generative modeling of probability distributions based on the\nBoltzmann-Gibbs distribution with respect to an energy functional. Boltzmann machines are one\ninstance of such energy-based models where the energy functional is given by a spin-spin interaction\nmodel. However, Boltzmann machines are hard to train which led to the development of restricted\nBoltzmann machines where a special interaction structure with a hidden layer enables more efficient\ntraining. In Ref. [Jer+21b] the authors observe that the lower architecture in Fig. 16 is equivalent\nto a generalized form of restricted Boltzmann machines.\nFigure 16: Difference between architectures in Q-learning (upper part of the figure) and energy-\nbased models (lower part of the figure) as shown in Jerbi et al. [Jer+21b]\nQuantum Speed-Up. Inspired by this insight, the authors next investigate quantum energy-based\nmodels. Here, the classical spin-spin interaction energy is promoted to a spin Hamiltonian, known\nas quantum Boltzmann machines and restricted quantum Boltzmann machines.\nSome of these\n55\nmodels allow efficient training, while the hardness of sampling remains.\nTo speed up sampling\nin the classical and quantum setting, the following quantum subroutines for a RL algorithm are\ndiscussed:\n(1) Quantum Gibbs sampling: The Gibbs-Boltzmann distribution is prepared as a qsample, from\nwhich expectations values can be sampled with quadratic speed-up, compared to classical Monte-\nCarlo sampling methods. (2) Gibbs-state preparation by Hamilton simulation: Using Hamilton-\nsimulation techniques, an approximation to the Gibbs qsample can be prepared, leading to quadratic\nspeed-up compared to exact sampling (calculating all energies and explicitly normalizing the prob-\nability distribution). (3) Quantum simulated annealing: This method uses a quantum method for\nthe approximate Monte-Carlo sampling of the Gibbs state itself by leveraging quantum random\nwalks on graphs.\nAll methods discussed so far need oracularized access to the Hamiltonian and it is unlikely\nthat they could be realized on current hardware. A realization on near-term hardware might be\nachieved by (4) Variational Gibbs-state preparation: Here, a variational circuit can be employed\nto approximate a Gibbs qsample, using the free energy as an objective. Any quantum speed up,\nhowever, for this method is heuristic and has not been made rigorous so far.\nRemarks.\nRelated work [Cra+18; Sch+22; Lev+17] proposes models based on quantum Boltz-\nmann machines for quantum annealing hardware. Since this literature survey focuses on algorithms\nproposed for gate-based QC, we do not include a detailed summary here.\n4.5\nQuantum Policy and Value Iteration\nSo far, we have considered QRL algorithms that employ QC for function approximation or propose\nquantum approaches to alternative learning frameworks such as PS. We now turn to proposals that\nreplace subroutines of existing RL frameworks by quantum algorithms such as amplitude estimation,\nquantum maximum finding and, respectively, quantum matrix inversion. As a result, the proposed\nQRL algorithms guarantee improved sample or computational complexity. As these methods need\noracular access to the environment, they should be categorized as post-NISQ algorithms.\nCitation\nFirst Author\nTitle\n[Wan+21a]\nD. Wang\nQuantum algorithms for reinforcement learning with a gener-\native model\n[Gan+23]\nB. Ganguly\nQuantum Computing Provides Exponential Regret Improve-\nment in Episodic Reinforcement Learning\n[Zho+23]\nH. Zhong\nProvably Efficient Exploration in Quantum Reinforcement\nLearning with Logarithmic Worst-Case Regret\n[GA23]\nB. Ganguly\nQuantum Acceleration of Infinite Horizon Average-Reward Re-\ninforcement Learning\n[CKP23]\nE. A. Cherrat\nQuantum Reinforcement Learning via Policy Iteration\n[Wie+22a]\nS. Wiedemann\nQuantum Policy Iteration via Amplitude Estimation and\nGrover Search - Towards Quantum Advantage for Reinforce-\nment Learning\nTable 13: Work considered for “Quantum Policy and Value Iteration” (Sec. 4.5)\nQuantum algorithms for reinforcement learning with a generative model, Wang et\nal. (2021)\n56\nSummary. The work in Ref. [Wan+21a] proposes two algorithms for RL with a generative model\nand rigorously derives bounds for their sample complexity.\nClassical Generative Models. Classically, the term generative model describes a simulator, which\nqueried with a state-action pair (s, a), produces a sample s′ ∼P(·|s, a). Thus, by repeated sampling\nfor each state-action pair, one can estimate the transition matrix of the underlying MDP. This allows\nto subsequently obtain an approximation of the optimal policy by means of value iteration. Over\nthe years there has been tremendous effort devoted to improving sample efficiency (defined as the\nnumber of times the simulator has to be queried). This performance metric is meaningful if one\nassumes that every query of the simulator is costly. The best classical algorithm [Li+20b] requires\na total number of O(|S||A|Γ3/ϵ2) samples, where |S| and |A| are the number of states and actions,\nΓ = 1/(1 −γ) is the effective horizon of the MDP, and ϵ is the deviation of the optimal value\nfunction from the approximation. The sample complexity is linear in the product |S||A|, since the\ntransition matrix has to be estimated for each (s, a). The factor 1/ϵ2 originates from Hoeffding’s\ninequality (indeed bounding the deviation of a sample average from its real value by ϵ, requires\nO(1/ϵ2) samples).\nThe origin of the third power of Γ, in contrast, is less intuitive.\nNote that\nthe sample complexity of the classical algorithm is also a lower bound (in the classical case) and\ntherefore optimal.\nIncorporating Quantum Subroutines. As shown in Ref. [Wan+21a], the classical sample complexity\ncan be reduced by replacing the classical mean-estimation subroutine in Ref. [Li+20b] by a quantum\nroutine based on the quantum mean-estimation algorithm [Bra+02].\nEven though the optimal\nclassical algorithm is more sophisticated as outlined above and so is its quantization, the following\ndiscussion captures the essential features. The quantum subroutine requires the generative model\nin oracle form and can then be used to estimate the expectation value E(V ) = P\ns′ P(s′|s, a)V (s′)\n(which appears in the Bellman equation) individually for every pair (s, a) in time O(1/ϵ). This\nquadratic speed-up originates from Grover’s algorithm, on which the quantum-mean estimation\nalgorithm is based upon. As a consequence, the quantum-policy iteration algorithm achieves the\nsample complexity O(|S||A|Γ1.5/ϵ) with an quadratic improvement in Γ and ϵ.\nThe dependence on the size of the action space can be further reduced by using quantum max-\nimum finding [Mon15] to calculate the maximum over actions in the Bellman optimality equation.\nHowever, using this quantum routine, one can not fully exploit the power of the classical optimal\nalgorithm. Hence, while the dependence on |A| is reduced quadratically and the improvement in\nϵ is kept, the improvement in Γ is lost. As a result, the algorithm based on both quantum-mean\nestimation and quantum maximum finding achieves a sample complexity O(|S|\np\n|A|Γ3/ϵ).\nFinally, the lower bound O(|S||A|Γ1.5/ϵ) is derived and possible improvements of the algorithm\nto reach this limit are discussed.\nQuantum computing provides exponential regret improvement in episodic reinforce-\nment learning, Ganguly et al. (2023)\nSummary. In Ref. [Gan+23] and independently in Ref. [Zho+23] the authors consider the problem of\nan agent operating in a finite-horizon episodic tabular MDP and investigate if quantum computing\ncan alleviate the exploration-exploitation trade-off. This problem has been considered for the case\nof bandits [Wan+23; LHT22; LZ22] but is here generalized to the full multi-state RL problem. In\nthe online setting, the agent only has access to the next state and reward given its current state\nand chosen action. In contrast, previous work [Wan+21a] assumed access to a generative model,\nwhich can be queried with arbitrary state-action pairs producing samples of the next state and\nreward. This setting does not consider the exploration-exploitation trade-off that arises from online\ninteraction with the environment. Here, the agent must learn to discover high-reward states by a\nsuitable exploration strategy. The performance of the agent in this problem can be measured by\nthe regret, which is defined as the cumulative difference between the optimal value function and its\napproximation after K episodes. The goal is to design an algorithm with the weakest scaling of the\n57\nregret in K, indicating a more effective trade-off between exploration and exploitation. The classical\nUCB-VI algorithm achieves the lower bound Ω(\n√\nK) [JOA10; AOM17] of the regret. The proposed\nquantum algorithm in Ref. [Gan+23] builds upon this classical algorithm by replacing the mean\nestimation routine with a quantum algorithm. Given a state-action pair, the quantum algorithm\nassumes a ‘transition oracle’ which generates a quantum superposition over all possible next states\nwith amplitudes given by the square root of the respective transition probabilities. A similar oracle\nis used for generating rewards. The algorithm utilizes the quantum multivariate mean estimation\nalgorithm [Ham21], which reduces the number of samples required to satisfy a given error bound\nfor mean estimation quadratically. The result is a decrease of the regret of the quantum algorithm\nfrom O(\n√\nK) to O(1) up to logarithmic factors. This is an exponential improvement over classical\nresults. In a follow-up work by the same authors [GA23], the results were extended to infinite horizon\nproblems, where an exponential reduction in regret from O(\n√\nT) to O(1) (T being the total number\nof time steps) is achieved. Additionally, Ref. [Zho+23] consideres linear function approximation and\ndemonstrates that the exponential improvement is maintained.\nQuantum Reinforcement Learning via Policy Iteration, Cherrat et al. (2023)\nSummary.\nRef. [CKP23] proposes a quantum algorithm for an iterative scheme of Q-value evaluation and policy\nimprovement. The algorithm evaluates the Q-value on a quantum computer, with the state vector\nrepresenting the Q-values, being extracted by measurements. The policy afterwards is improved on\na classical device. The algorithm can achieve quantum advantage in certain situations.\nTo set up the general framework, the authors first formulate the Bellman equation for Q-value\nevaluation as a matrix equation [LP03]\nQ = R + γPΠQ .\nDenoting the size of the action and state space as |A| and |S|, the |A||S| dimensional vectors Q and\nR represent the Q-values and the reward vector, respectively; the environment transition function\nis the |A||S| × |S| dimensional matrix P; the policy is represented by an |S| × |A||S|-dimensional\nmatrix Π; γ denotes the usual discounting factor; The authors propose to compute (11 −γPΠ)−1R\non a quantum device.\nQuantum Subroutine: Block Encodings and Linear Algebra. To perform this task, Ref. [CKP23]\nrelies on so-called block encodings of matrices [Gil+19].\nThis powerful framework gives rise to\nvarious quantum algorithms for encoding general complex (not necessarily rectangular) matrices\nin the leading principal block of a larger unitary matrix.\nOnce the data has been loaded, the\nframework further provides linear-algebra routines such as matrix multiplication, addition [Gil+19]\nand inversion [CKS17]. The encoding algorithms need quantum access to the data, i.e. via oracles.\nTherefore, the methods can be attributed to the post-NISQ algorithms category. A well-known data-\nloading scheme is the sparse-input model, viable for sparse matrices. The authors of Ref. [CKP23]\napply a more general scheme, the so-called µp(A) [CGJ19] block encoding of a matrix A. Here, the\nquality (i.e. the probability to obtain the correct output of the algorithm, e.g. after matrix-vector\nmultiplication and a subsequent measurement) of the encoding depends on the maximum of the\ncolumn and row norms of the matrix. The aforementioned norm is a function of p and can be\nchosen freely to optimize the encoding quality. Based on this formalism, the authors show that\npolicy evaluation requires time\nO(µP Γpolylog(|S||A|Γ/ϵ)) .\n(29)\nIn Eq. (29), the parameter Γ = (1−γ)−1, ϵ denotes the accuracy of the matrix inversion subroutine.\nThe term µP describes the quality of the encoding of the environment-transition matrix, which\ndepends on the structure of the environment. In the worst case it scales as\np\n|S||A|. Due to the\nsparsity of the transition function of many environments, a better scaling is often expected. As\ndiscussed below, for the frozen-lake environment one even finds µP = O(1). The complexity in\nEq. (29) assumes an efficient loading routine for the matrices. To achieve efficient loading also for\nthe policy matrix, a QRAM data structure for the policy needs to be constructed. This needs to\n58\nhappen in time O(|S||A|) for each policy-evaluation step. Afterwards, the matrix can be loaded\nefficiently for each cycle of the measurement protocol.\nClassical Subroutine: Policy Improvement.\nThe policy improvement step on a classical device\nrequires reading out the Q-vector from the quantum computer after matrix inversion.\nNaively,\none would expect that the measurement process introduces exponential overhead. However, since\nconvergence results for the Bellman equations are based on the maximum norm (L∞norm), the\nauthors employ L∞-norm state tomography [KP20]. This is efficient, i.e. requires O(1/ϵ2) shots,\nwhere ϵ now is the target accuracy for the optimal Q-values (under L∞-norm). Consequently, the\noverall time complexity (neglecting logarithmic terms) of the algorithm is\nO(|S||A| + µP Γ/ϵ2) .\n(30)\nIn Eq. (30) the factor 1/ϵ2 appears in the second term since the matrix inversion subroutine is called\nfor each of the 1/ϵ2 shots. The first term is the classical complexity of calculating the argmax\nfunction for policy improvement and construction of the policy oracle prior to each evaluation step.\nExample Environments. The authors consider the FrozenLake and the InvertedPendulum environ-\nments as examples. We will briefly discuss the insights from the former here: The simple form of\nthe environment allows choosing µP = 1/2, which thus is independent of the size of the action and\nstate space. Note that the gate complexity is still of the order of |S||A|. It only becomes efficient\nfor special structured instances of the environment such as all ‘holes’ on the diagonal of the grid.\nQuantum advantage. The leading term in Eq. (30) is linear in |S| and |A|, showing a speed-up\nwith respect to classical linear-system of equations solvers. These exhibit complexity O((|S||A|)ω),\nwith ω > 1, and vanilla Q-value iteration with complexity O(|S|2|A|). Even though a more detailed\ncharacterization of possible quantum advantage is not provided in Ref. [CKP23], it is clear that the\nspeed up can be at most polynomial.\nLeast-Squares Policy Iteration. Finally, the authors generalize the method to least-squares policy\niteration [LP03], where the Q-vector is approximated by a set of basis functions. For details refer\nto Refs. [LP03; CKP23].\nQuantum Policy Iteration via Amplitude Estimation and Grover Search - Towards\nQuantum Advantage for Reinforcement Learning, Wiedemann et al. (2022)\nSummary.\nIn the QRL scheme proposed in Refs. [Wie+22a; Wie21], a policy is evaluated by\nconstructing a superposition of all possible trajectories of an MDP with fixed-horizon and with\nfinite action and state space. Making use of amplitude estimation [Bra+02], the number of calls to\na state-transition oracle for estimation of the value function (up to some fixed additive error) can be\nquadratically reduced. A second algorithm finds the optimal policy in the policy space quadratically\nfaster compared to direct policy search by means of Grover’s algorithm.\nFirst Algorithm. The first algorithm assumes access to a policy oracle Π and an environment oracle\nE which act on an initial state |s⟩as\nΠ(|s⟩|0⟩A) =\nX\na\np\nπ(a|s)|s⟩|a⟩\nE(|s⟩|a⟩|0⟩R|0⟩S) =\nX\nr,s′\np\np(r, s′|s, a)|s⟩|a⟩|r⟩|s′⟩.\nApplying these operators sequentially on partially fresh registers as shown in Fig. 17 results in a\nsuperposition of all possible trajectories\n|t⟩= |s0⟩|a0⟩|r1⟩|s1⟩. . . |rH⟩|sH⟩\n59\nwhere H is the horizon of the MDP, such that the quantum state reads\n|ψπ⟩=\nX\nt\n√pt|t⟩|Gt⟩.\nHere, pt is the probability of trajectory t. An additional unitary operator has been applied that\ncalculates the return Gt of trajectory t and encodes the value into an additional register entangled\nwith the corresponding trajectory. The superscript π on |ψπ⟩denotes that the state corresponds to\nthe superposition of trajectories for a given policy π.\nFigure 17: Sequence of policy and environment operator application to an initial state s. This con-\nstructs a superposition of all possible trajectories for a fixed horizon MDP as shown in Wiedemann\net al. [Wie21].\nThe next step of the algorithm attaches an ancilla qubit. With bit-by-bit rotations of the state\nthe digital encoding of Gt is transformed into amplitude encoding (assuming here for simplicity\nGt ∈[0, 1]). A simple calculation reveals that the probability of finding the ancilla qubit in state\n|1⟩is given by the average return, that is the value function of the initial state s. With this insight\nin mind, the authors propose amplitude estimation [Bra+02]. This involves the phase-estimation\nalgorithm, to extract the value function.\nWhile classically sampling from the superposition of\ntrajectories would require O(1/ϵ2) preparations of the state, the quantum algorithm achieves the\nsame error with O(1/ϵ), resulting in a quadratic speed-up. Hereby, ϵ denotes the fixed additive\nerror to which the value function is to be determined.\nSecond Algorithm. The second algorithm shown in Ref. [Wie+22a] is a quantum version of direct\npolicy search. The authors propose to create a superposition\n1\np\n|P|\nX\nπ\n|π⟩|ψπ⟩\nwhere |π⟩is a digital representation of the policy, |ψπ⟩the superposition of all trajectories corre-\nsponding to policy π as before, and |P| the size of the policy space. Quantum minimum finding\n[DH96] can now be applied to find the optimal policy (the one with maximal expected return\nstarting from initial state s), requiring only O(\np\n|P|) preparations of the state. This is opposed\nby O(|P|) in classical direct policy search. Note, however, that the space of all policies scales as\nO(|A||S|), where |A| and |S| are the sizes of action and state space, respectively. Consequently,\nthe quantum algorithm scales exponentially worse compared to policy iteration where the Bellman\noptimality equation is iterated with polynomial complexity in |S| and |A|. The method proposed\nin Refs. [Wie+22a; Wie21] therefore should be seen as a quantum version of direct policy search.\n4.6\nQuantum Reinforcement Learning with Oracularized Environments\nIn this final section we summarize work that proposes fully quantum-mechanical approaches to\nQRL. In the articles we survey below, the environment is a quantum system or oracle that can\n60\nbe queried by superpositions of states and actions. Interactions with a quantum-mechanical agent\ncreate superpositions of trajectories as input for subroutines like Grover search, quantum-maximum\nfinding, and amplitude estimation. Provable quantum advantage renders some of these proposals\ninteresting candidates for the post-NISQ era.\nCitation\nFirst Author\nTitle\n[DTB16]\nV. Dunjko\nQuantum-Enhanced Machine Learning\n[DTB15]\nV. Dunjko\nFramework for learning agents in quantum environments\n[DTB17]\nV. Dunjko\nAdvances in quantum reinforcement learning\n[HDW21]\nA. Hamann\nQuantum-accessible reinforcement learning beyond strictly\nepochal environments\n[Wan+21b]\nD. Wang\nQuantum exploration algorithms for multi-armed bandits\n[Wan+23]\nZ. Wan\nQuantum Multi-Armed Bandits and Stochastic Linear Bandits\nEnjoy Logarithmic Regrets\n[Sag+21a]\nV. Saggio\nExperimental quantum speed-up in reinforcement learning\nagents\n[HW22]\nA. Hamann\nPerformance analysis of a hybrid agent for quantum-accessible\nreinforcement learning\n[Cor18]\nA. Cornelissen\nQuantum gradient estimation and its application to quantum\nreinforcement learning\nTable 14: Work considered for “QRL with Oracularized Environments” (Sec. 4.6)\nQuantum-Enhanced Machine Learning, Dunjko et al. (2016) and related work\nSummary. In Ref. [DTB16] and in a more detailed preprint [DTB15] a general framework of an\nagent-environment interaction where both entities are quantum-mechanical systems is developed.\nTo query the environment by a superposition of action states (intuitively the agent learns in par-\nallel), clearly the environment must be modeled by some form of an oracle. As it turns out, this\noracularization is much more involved than one might naively think.\nThe focus of the work is\ntherefore:\n• Formalizing a quantum mechanical version of agent-environment interaction\n• Investigation of the classical limit\n• Properties of the general quantum mechanical set-up\n• Treatment of special oracularizable environments\n• Identification of quantum advantage for these environments\nGeneral Setup. The interaction between agent and environment is modeled as shown in Fig. 2a in\nRef. [DTB15]. The register RA processes the computations of the agent, while the register RE repre-\nsents the environment. The communication register stores one action and one state. The interaction\nis described by completely positive trace preserving (CPTP) maps or, if we wish, unitary maps on\na larger system. The first map ME\n1 outputs the initial state and stores it into the communication\nregister. The map MA\n1 (modeling the agent) reads this state and, after some processing on RA,\noutputs an action state which is added to the communication register. Now this action processed\n61\nby ME\n2 , which outputs a new state. Consecutively, the previous state in the communication register\nis overwritten, and so on. The particular form of the states of RC (if in superpositions of action or\nnot) will be discussed later.\nWhile RC only contains a state-action pair, the agent’s register stores all previous states and\nactions (because the next action proposed by the learning algorithm depends on all actions and\nstates encountered before, note here the distinction between algorithm and policy). The same is\ntrue for the (in general non-Markovian) environment.\nNext, as shown in Fig. 18, a tester register RT is introduced, which is designed to ‘observe’ the\nelapsed history (all encountered states and actions during a learning sequence). This copying from\nRC to RT is modeled by controlled unitaries (so they do not modify RC). Each of them act on a\nfresh part of the register RT .\nThe term copying the register here means that a superposition of computational basis states is\nconcatenated with a second register, on which then each basis state is copied to. This produces\nin general a highly entangled state, which cannot be factorized into the initial state on the first\nregister and a copy on the second (note the no-cloning theorem only rules out a transformation\nproducing this factorized copy for a general initial state). The most general form of the tester\ninteraction treated in this work allows additional unitary transformations, such that the copying\ncan be described in the form of controlled unitaries. A tester interaction that merely copies the\nstates will be referred to as classical.\nAfter training, the register RT contains the sequence of actions and states, the so-called his-\ntory. Any metric measuring performance of learning can be phrased as a function of the history\nprobabilities. Therefore, it can be formulated as the expectation value of an observable on RT .\nFigure 18: Adding a tester as proposed in Dunjko et al. [DTB16].\nClassical Limit. For recovering the classical learning set-up, the notion of classical interaction is\ndefined by restricting the form of the maps, such that the state in RA −RC −RE remains separable\n(note that no entanglement between the registers does not prohibit entangled agent or environment\nstates, thus quantum mechanical environments and agents equipped with a quantum computer\nare not excluded). Additionally, the tester interaction is supposed to be classical (in the sense as\ndefined above). For this setup it is shown that for every scenario with separable register state there\nexists a classical environment and a classical agent that produce the same history. Consequently,\nno quantum improvements are possible. Hence, there can be no improvement in the figure of merit,\neven when the agent has access to a quantum computer.\nGeneral Quantum-Mechanical Set-Up.\nWhat happens when we allow general maps and general\nstates on the registers? The authors prove that the state on RT is still an incoherent mixture, and\ntherefore no quantum advantage can be expected. The reason for this result lies in the memory of\nagent and possibly the environment: The agent in general has to remember all previous encountered\nstates and actions, because the learning algorithm run by the agent is a function of that particular\nelapsed history. The quantum state therefore is a superposition of histories entangled with a state,\nwhich describes the agent that has seen this particular history. The states of this agent are orthog-\nonal, since a different agent state translates into a different bit state of the memory. Thus, when\ntracing out these degrees of freedoms, the resulting reduced density matrix on RT is an incoherent\nmixture and no quantum advantage can be achieved in the figure of merit. (Side remark: This does\n62\nnot exclude a quantum advantage in terms of computational complexity in the internal processing\nof the agent. The result is about exploiting the ‘quantumness’ of the environment-agent interaction)\nWe note that one has to be careful with the interpretation of density matrices. One might\nbe inclined to think that an incoherent mixture of history states weighted by their probability in\nsome sense corresponds to traversing all of the histories simultaneously but note that the correct\nexpectation value with respect to this density matrix is only obtained in the limit of infinitely many\nruns corresponding to sampling trajectories one after another.\nOracularization of Environments. The next part of the work focuses on a special class of envi-\nronments and learning setting without memory, which overcome the decoherence problem. These\noracularized environments are of the following form:\n• episodic with fixed horizon →fixed sequence of interactions\n• deterministic →action sequence fully determines the history, states can be disregarded\n• binary rewards issued at final state →allows use of Grover search\nQuantum Advantage.\nWith these assumptions a proper oracle can be constructed, that can be\nqueried with a superposition of actions.\nThis allows to use it as a phase flip oracle, as in the\nDeutsch-Jozsa or Grover algorithm. The time required for finding a rewarded-action sequence is\ntherefore quadratically reduced. Consequently, this setting is meaningful for learning tasks, where\nthe reward is very sparse. That is, the agent cannot learn until it has first seen a reward. After this\ninitial exploration phase, the agent can now be further trained in simulation. Finally, some of the\nassumptions are relaxed. The authors also show, how stochastic oracles can be constructed.\nFurther Work. There is further work that builds upon the results of Refs. [DTB15; DTB16]. In\nRef. [DTB17], the algorithm is applied to the optimization of parameters describing the properties\nof the agent (hyperparameter). It also discusses the notion of register hijacking, where the agent\nhas access to hidden memory registers of the environment. This assumption allows the oraculariza-\ntion of more general environments, which is also discussed in Ref. [Dun+18]. This class is further\ngeneralized in Ref. [HDW21] beyond episodic environments. A closer investigation of amplitude\namplification techniques for the special case of multi-armed bandits environments is conducted in\nRefs. [Wan+21b; Wan+23].\nIn Ref. [Sag+21a], the learning setting is implemented experimen-\ntally for a two-qubit system and an experimental quantum advantage is observed.\nFinally, the\nperformance of an agent in this setting is investigated in Ref. [HW22].\nQuantum gradient estimation and its application to quantum reinforcement learning,\nCornelissen (2018)\nSummary. The master’s thesis [Cor18] considers model-based RL and develops quantum algorithms\nfor policy evaluation and policy optimization. For the former method a quadratic improvement in\nsample complexity is found.\nQuantum Policy Evaluation. A quantum algorithm for quantum policy evaluation is presented in\nSec. 6.2 of the thesis and will be summarized in the following: The algorithm is executed on a\nregister that is capable to store T states and actions of a T-step MDP. To generate a sequence,\na transition-probability oracle and a policy oracle are defined. They generate a superposition of\nall possible action-state sequences of the Markov problem, weighted by the square root of the\ncorresponding probabilities. Note that the state is normalized as the probabilities sum up to one.\nNext, a reward oracle is defined which, when acting on a state-action pair, multiplies the state with a\nphase factor. The phase is the discounted reward for this state action pair. The discount factors are\nintroduced by making use of fractional phase oracles. This is discussed in detail in Sec. 4 and 5 of the\nthesis, which are based on Refs. [GAW19; Gil+19]. The fractional reward oracle is applied to every\nstate-action pair in the register, resulting in the product of phase factors containing the individual\n63\ndiscounted rewards. Thus, when merging the exponentials to one exponential, the full quantum\nstate is a superposition of all state-action sequences, weighted by the square root of the individual\nprobability and a phase factor containing the corresponding return. Next, it is shown how the phase\nfactor can be encoded in the amplitude by a controlled operation on an ancilla qubit. Consequently,\nthe probability of measuring the ancilla in, say, state |0⟩is given by the expectation value of the\nreturn, that is the value function. It can be measured using quantum-amplitude estimation, which\nworks based on the phase estimation algorithm. The amplitude-estimation algorithm is a Grover-\ntype algorithm.\nHence, it is not surprising that the quadratic speed up compared to classical\nMonte-Carlo sampling results from this algorithmic step.\nQuantum Policy Optimization. In Sec. 6.4 of the thesis a policy optimization algorithm is developed.\nThis method can be seen as a quantum analogue of policy gradient. First of all, the policy needs\nto be parameterized. This is done by introducing the parameters xsa such that π(a|s) = xsa for\nall a but one arbitrarily chosen a∗and π(a∗|s) = 1 −P\na xsa otherwise. By that definition, the\npolicy is properly normalized and all xsa ∈[0, 1]. Consequently, the expected return is a high-\ndimensional polynomial in the parameters xsa. For taking the derivative of this objective, Jordan’s\nquantum gradient algorithm [Jor05] in it’s advanced form [GAW19] is employed. This leads to a\nfinite-difference approximation of the gradients, written in a phase factor, which can be read out\nafter applying phase estimation. Following Ref. [Gil+19], significant amount of work is devoted\nto transform the probability oracle for the policy and the transition matrix described above into a\nphase oracle. Once the superposition of state-action sequences is prepared, an oracle call multiplies\neach state in the superposition by the corresponding discounted reward. Consecutively, the gradient\nestimation algorithm is applied and the gradients can be read out. This step is followed by adapting\nthe policy through gradient ascent.\nIt is concluded in the thesis that this policy optimization\nalgorithm does not necessarily lead to quantum speed-up. However, as the author argues, it is\nconceivable that improvement of the algorithm might lead to a quantum speed-up.\n5\nOutlook\nWe have given a rather detailed account of the various instances QRL that have appeared throughout\nthe literature. We observed, that the dichotomy found at the hardware level, i.e., currently available\nNISQ devices vs. fault-tolerant and error-corrected QPUs, manifests also at the algorithmic level.\nWith NISQ devices in mind, VQCs have been suggested as function approximators.\nThese\nreplace their classical counterparts in RL algorithms with function approximation in policy space,\nvalue space, or both. Here, one typically replaces a classical learning heuristic by a learning heuristic\nwith a quantum component. Any sort of potential quantum advantage, however, is not immediately\napparent. We eventually can obtain theoretical insight into the properties of VQCs viewed as ML\nmodels and function approximators. However, a direct comparison to their classical cousins, such\nas neural networks, is anything but easy and might strongly depend on the chosen metric. How\ncan we meaningfully deploy an agent trained with VQC-components? What are the requirements\nfor quantum advantage in such a heuristic setting? What does non-simulability of quantum circuits\nimply for e.g. generalization bounds of VQCs as ML models? Can we scale VQCs while maintaining\ntheir desirable properties? What is the intrinsic inductive bias of VQCs viewed as ML models?\nWhat are the implications for RL and its application domains? All these questions are currently\nbeing investigated in the research community, and we are looking forward to new results.\nWhile quantum algorithms for fault-tolerant and error-corrected QPUs have been put forward,\nwe are still far from being able to deploy these algorithms for meaningful problem sizes. Given the\nnecessary advancements of hardware platforms, it will be exciting to see whether these types of\nquantum algorithms will become competitive with classical learning approaches in practice.\nWe hope that our survey on the QRL literature and the various types of QRL algorithms will\nhelp guide newcomers to the field and will serve as a valuable reference for researchers.\n64\nAcknowledgments\nWe acknowledge collaboration and exchange with M. Franz, L. Wolf, M. Schönberger and W.\nMauerer as well as M. J. Hartmann on the topic of quantum reinforcement learning. We further\nacknowledge exchange and discussion with W. Hauptmann, D. Hein, S. Udluft, V. Tresp, Y. Ma,\nA. Auer, M. Weber, B. Bisgin, L. Bleiziffer, C. Mendl, S. Wiedemann, S. Wölk, J. M. Lorenz, M.\nMonnet, T.-A. Dragan, G. Kruse and G. Kontes. We would like to thank M. Leib for feedback on\nan early version of the manuscript. This work was supported by the German Federal Ministry of\nEducation and Research (BMBF), funding program “quantum technologies – from basic research to\nmarket”, grant number 13N15645.\n65\nList of Abbreviations\nBCQ batch-constrained deep Q-learning. 38, 40\nBCQQ batch-constrained quantum Q-learning. 38–40\nCNN convolutional neural network. 20, 47\nCPTP completely positive trace preserving. 61\nCQ2L conservative quantum Q-learning. 40, 41\nCQL conservative Q-learning. 40–42\nCTDE centralized training with decentralized execution. 36, 37\nDDQL double deep Q-learning. 17, 19–22\nDL deep learning. 3\nDLP discrete logarithm problem. 24\nDNN deep neural network. 11, 17, 18, 24, 33\nDQAS differential quantum architecture search. 46\nDQL deep Q-learning. 17, 18\nDQN deep Q-network. 41\nDRL deep reinforcement learning. 4, 11, 16\nDRU data re-uploading. 38–40\nFIM Fisher information matrix. 26, 27, 29\nMARL multi-agent reinforcement learning. 36\nMDP Markov decision process. 3–5, 42, 57, 59, 60, 63\nML machine learning. 2, 7, 10, 11, 23, 25, 48, 64\nMPS matrix product state. 47, 48\nMSE mean square error. 17\nNISQ noisy intermediate-scale quantum. 2, 3, 10, 12, 24, 47, 48, 56, 58, 61, 64\nNN neural network. 3, 6, 10, 12, 18, 20–22, 24, 26, 35, 36, 43, 45–48, 50, 52, 54, 55\nPDF probability density function. 4, 23\nPOMDP partially observable Markov decision process. 42, 44\nPPO proximal policy optimization. 45, 46\nPS projective simulation. 12, 53, 56\nQA3C quantum asynchronous advantage actor critic. 34, 46, 49, 52\n66\nQC quantum computing. 2, 3, 7, 11–13, 33, 44, 56\nQCNN quantum convolutional neural network. 20\nQDDPG quantum deep deterministic policy gradient. 31, 33\nQiRL quantum-inspired reinforcement learning. 2, 11, 13–15, 44, 45\nQLSTM quantum long short-term memory. 46\nQMARL quantum multi-agent reinforcement learning. 35–38, 52\nQML quantum machine learning. 7, 10, 11, 23, 24, 26\nQNN quantum neural network. 37, 48\nQNPG quantum natural policy gradient. 29, 30\nQPG quantum policy gradient. 22–24, 27–29, 48\nQPU quantum processing unit. 3, 11, 16, 64\nQRL quantum reinforcement learning. 2, 3, 7, 10–15, 17, 21, 23, 27, 28, 30, 32, 34, 37, 38, 40–43,\n46, 48–54, 56, 59–61, 64\nQRNN quantum recurrent neural network. 46\nRL reinforcement learning. 2–5, 7, 10–13, 15–17, 19, 21–24, 27, 28, 30–33, 35, 37, 40, 42, 44–46,\n48, 52, 54, 56, 57, 63, 64\nSAC soft actor-critic. 34, 35, 37, 38, 49\nTD temporal difference. 13\nTN tensor network. 47, 48\nTSP traveling salesman problem. 51\nVQ-DQN variational quantum deep Q-networks. 39, 41\nVQA variational quantum algorithm. 11\nVQC variational quantum circuit. 2, 6, 7, 10–12, 15–21, 23–43, 45–52, 64\nVQE variational quantum eigensolver. 32\nVRP vehicle routing problem. 50\n67\nReferences\n[Abb+21]\nA. Abbas et al. “The power of quantum neural networks”. Nat. Comput. Sci. 1.6 (2021),\n403. doi: 10.1038/s43588-021-00084-1.\n[ACN22]\nE. Andrés, M. P. Cuéllar, and G. Navarro. “On the use of quantum reinforcement\nlearning in energy-efficiency scenarios”. Energies 15.16 (2022), 6034. doi: 10.3390/\nen15166034.\n[ACN23]\nE. Andrés, M. Cuellar, and G. Navarro. “Efficient Dimensionality Reduction Strategies\nfor Quantum Reinforcement Learning”. IEEE Access 11 (2023), 104534. doi: 10.1109/\nACCESS.2023.3318173.\n[Acu+22]\nA. Acuto et al. “Variational quantum soft actor-critic for robotic arm control”.\narXiv:2212.11681 (2022). doi: 10.48550/arXiv.2212.11681.\n[AHF20]\nR. Ayanzadeh, M. Halem, and T. Finin. “Reinforcement Quantum Annealing: A Hy-\nbrid Quantum Learning Automata”. Sci. Rep. 10.1 (2020), 1. doi: 10.1038/s41598-\n020-64078-1.\n[Alb+18]\nF. Albarrán-Arriagada et al. “Measurement-based adaptation protocol with quantum\nreinforcement learning”. Phys. Rev. A 98.4 (2018), 042315. doi: 10.1103/PhysRevA.\n98.042315.\n[Alb+20]\nF. Albarrán-Arriagada et al. “Reinforcement learning for semi-autonomous approxi-\nmate quantum eigensolver”. Mach. learn.: sci. technol. 1.1 (2020), 015002. doi: 10.\n1088/2632-2153/ab43b4.\n[Alv+16]\nU. Alvarez-Rodriguez et al. “Artificial Life in Quantum Technologies”. Sci. Rep. 6.1\n(2016), 1. doi: 10.1038/srep20956.\n[Alv+18]\nU. Alvarez-Rodriguez et al. “Quantum Artificial Life in an IBM Quantum Computer”.\nSci. Rep. 8.1 (2018), 1. doi: 10.1038/s41598-018-33125-3.\n[Ama98]\nS.-I. Amari. “Natural Gradient Works Efficiently in Learning”. Neural Comput. 10.2\n(1998), 251. doi: 10.1162/089976698300017746.\n[Amb+19]\nA. Ambainis et al. “Quantum Speedups for Exponential-Time Dynamic Programming\nAlgorithms”. Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete\nAlgorithms (2019), 1783. doi: 10.1137/1.9781611975482.107.\n[Ans+23]\nJ. A. Ansere et al. “Quantum Deep Reinforcement Learning for Dynamic Resource\nAllocation in Mobile Edge Computing-based IoT Systems”. IEEE Trans. Wirel. Com-\nmun. (2023). doi: 10.1109/TWC.2023.3330868.\n[AOM17]\nM. G. Azar, I. Osband, and R. Munos. “Minimax Regret Bounds for Reinforcement\nLearning”. Proceedings of the 34th International Conference on Machine Learning.\nVol. 70. 2017, 263. url: https://dl.acm.org/doi/10.5555/3305381.3305409.\n[Aru+17]\nK. Arulkumaran et al. “Deep reinforcement learning: A brief survey”. IEEE Signal\nProcess. Mag. 34.6 (2017), 26. doi: 10.1109/MSP.2017.2743240.\n[Aru+19]\nF. Arute et al. “Quantum Supremacy using a Programmable Superconducting Proces-\nsor”. Nature 574 (2019), 505. doi: 10.1038/s41586-019-1666-5.\n[BAQ23]\nBAQIS Quafu Group. “Quafu-RL: The Cloud Quantum Computers based Quantum\nReinforcement Learning”. arXiv:2305.17966 (2023). doi: 10.48550/arXiv.2305.\n17966.\n[BBA14]\nJ. Barry, D. T. Barry, and S. Aaronson. “Quantum partially observable Markov deci-\nsion processes”. Phys. Rev. A 90.3 (2014), 032311. doi: 10.1103/PhysRevA.90.032311.\n68\n[BD12]\nH. J. Briegel and G. De las Cuevas. “Projective simulation for artificial intelligence”.\nSci. Rep. 2.1 (2012), 1. doi: 10.1038/srep00400.\n[Bel+20]\nD. Beloborodov et al. “Reinforcement learning enhanced quantum-inspired algorithm\nfor combinatorial optimization”. Mach. Learn.: Sci. Technol. 2.2 (2020), 025009. doi:\n10.1088/2632-2153/abc328.\n[Bel57]\nR. Bellman. “A Markovian decision process”. J. math. mech. 6.5 (1957), 679. url:\nhttps://www.jstor.org/stable/24900506.\n[Ben+20]\nM. Benedetti et al. “Parameterized quantum circuits as machine learning models”.\nQuantum Sci. Technol. 5 (2020), 019601. doi: 10.1088/2058-9565/ab4eb5.\n[Ben80]\nP. Benioff. “The computer as a physical system: A microscopic quantum mechanical\nHamiltonian model of computers as represented by Turing machines”. J. Stat. Phys.\n22 (1980), 563. doi: 10.1007/BF01011339.\n[Bha+19]\nK. Bharti et al. “How to Teach AI to Play Bell Non-Local Games: Reinforcement\nLearning”. arXiv:1912.10783 (2019). doi: 10.48550/arXiv.1912.10783.\n[BKS23]\nS. Buchholz, J. M. Kübler, and B. Schölkopf. “Multi armed bandits and quantum\nchannel oracles”. arXiv:2301.08544 (2023). doi: 10.48550/arXiv.2301.08544.\n[BLT23]\nS. Brahmachari, J. Lumbreras, and M. Tomamichel. “Quantum contextual bandits and\nrecommender systems for quantum data”. arXiv:2301.13524 (2023). doi: 10.48550/\narXiv.2301.13524.\n[Boy+20]\nW. L. Boyajian et al. “On the convergence of projective-simulation–based reinforce-\nment learning in Markov decision processes”. Quantum Mach. Intell. 2.13 (2020), 1.\ndoi: 10.1007/s42484-020-00023-9.\n[Bra+02]\nG. Brassard et al. “Quantum amplitude amplification and estimation”. Contemp. Math.\n305 (2002), 53. url: http://www.ams.org/books/conm/305/.\n[BYK22]\nN. F. Bar, H. Yetis, and M. Karakose. “An Approach Based on Quantum Reinforce-\nment Learning for Navigation Problems”. 2022 International Conference on Data An-\nalytics for Business and Industry (ICDABI). 2022, 593. doi: 10.1109/ICDABI56818.\n2022.10041570.\n[Cár+18]\nF. A. Cárdenas-López et al. “Multiqubit and multilevel quantum reinforcement learn-\ning with quantum technologies”. PloS one 13.7 (2018), e0200455. doi: 10 . 1371 /\njournal.pone.0200455.\n[CCC23]\nH.-Y. Chen, Y.-J. Chang, and C.-R. Chang. “Deep-Q Learning with Hybrid Quantum\nNeural Network on Solving Maze Problems”. arXiv:2304.10159 (2023). doi: 10.48550/\narXiv.2304.10159.\n[CCL19]\nI. Cong, S. Choi, and M. D. Lukin. “Quantum convolutional neural networks”. Nat.\nPhys. 15.12 (2019), 1273. doi: 10.1038/s41567-019-0648-8.\n[CD08]\nC.-L. Chen and D. Dong. “Superposition-Inspired Reinforcement Learning and Quan-\ntum Reinforcement Learning”. Reinforcement Learning. 2008. Chap. 4. doi: 10.5772/\n5275.\n[CD10]\nC. Chen and D. Dong. “Complexity analysis of quantum reinforcement learning”.\nProceedings of the 29th Chinese Control Conference. 2010, 5897. url: https : / /\nieeexplore.ieee.org/abstract/document/5572589.\n[CDC06]\nC.-L. Chen, D. Dong, and Z. Chen. “Quantum computation for action selection us-\ning reinforcement learning”. Int. J. Quantum Inf. 4.06 (2006), 1071. doi: 10.1142/\nS0219749906002419.\n69\n[Cer+21]\nM. Cerezo et al. “Variational quantum algorithms”. Nat. Rev. Phys. 3.9 (2021), 625.\ndoi: 10.1038/s42254-021-00348-9.\n[CFD12]\nC. Chunlin, J. Frank, and D. Daoyi. “Hybrid control of uncertain quantum systems\nvia fuzzy estimation and quantum reinforcement learning”. Proceedings of the 31st\nChinese Control Conference. 2012, 7177. url: https : / / ieeexplore . ieee . org /\nabstract/document/6391208.\n[CGJ19]\nS. Chakraborty, A. Gilyén, and S. Jeffery. “The Power of Block-Encoded Matrix Pow-\ners: Improved Regression Techniques via Faster Hamiltonian Simulation”. 46th In-\nternational Colloquium on Automata, Languages, and Programming (ICALP 2019).\nVol. 132. 2019, 33:1. doi: 10.4230/LIPIcs.ICALP.2019.33.\n[Che+06]\nC. Chen et al. “A quantum reinforcement learning method for repeated game theory”.\n2006 International Conference on Computational Intelligence and Security. Vol. 1.\n2006, 68. doi: 10.1109/ICCIAS.2006.294092.\n[Che+19]\nC.-C. Chen et al. “Hybrid classical-quantum linear solver using Noisy Intermediate-\nScale Quantum machines”. Sci. Rep. 9.1 (2019), 1. doi: 10.1038/s41598-019-52275-\n6.\n[Che+20]\nS. Y.-C. Chen et al. “Variational Quantum Circuits for Deep Reinforcement Learning”.\nIEEE Access 8 (2020), 141007. doi: 10.1109/access.2020.3010470.\n[Che+21]\nS. Y.-C. Chen et al. “An end-to-end trainable hybrid classical-quantum classifier”.\nMach. learn.: sci. technol. 2.4 (2021), 045021. doi: 10.1088/2632-2153/ac104d.\n[Che+22]\nS. Y.-C. Chen et al. “Variational quantum reinforcement learning via evolutionary\noptimization”. Mach. learn.: sci. technol. 3.1 (2022), 015025. doi: 10.1088/2632-\n2153/ac4559.\n[Che+23a]\nZ. Cheng et al. “Offline quantum reinforcement learning in a conservative manner”.\nProceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. 6. 2023, 7148.\ndoi: 10.1609/aaai.v37i6.25872.\n[Che+23b]\nZ. Cheng et al. “Quantum Imitation Learning”. IEEE Trans. Neural Netw. Learn. Syst.\n(2023), 1. doi: 10.1109/TNNLS.2023.3275075.\n[Che+23c]\nE. A. Cherrat et al. “Quantum Deep Hedging”. Quantum 7 (2023), 1191. doi: 10.\n22331/q-2023-11-29-1191.\n[Che10]\nR. Cheng. “Quantum Geometric Tensor (Fubini-Study Metric) in Simple Quantum\nSystem: A pedagogical Introduction”. arXiv:1012.1337 (2010). doi: 10.48550/arXiv.\n1012.1337.\n[Che23a]\nS.\nY.-C.\nChen.\n“Asynchronous\ntraining\nof\nquantum\nreinforcement\nlearning”.\narXiv:2301.05096 (2023). doi: 10.48550/arXiv.2301.05096.\n[Che23b]\nS. Y.-C. Chen. “Efficient quantum recurrent reinforcement learning via quantum reser-\nvoir computing”. arXiv:2309.07339 (2023). doi: 10.48550/arXiv.2309.07339.\n[Che23c]\nS. Y.-C. Chen. “Quantum Deep Q-Learning with Distributed Prioritized Experience\nReplay”. IEEE International Conference on Quantum Computing and Engineering\n(QCE). Vol. 2. 2023, 31. doi: 10.1109/QCE57702.2023.10180.\n[Che23d]\nS. Y.-C. Chen. “Quantum deep recurrent reinforcement learning”. ICASSP 2023-2023\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\n2023, 1. doi: 10.1109/ICASSP49357.2023.10096981.\n[Che23e]\nS. Y.-C. Chen. “Quantum Reinforcement Learning for Quantum Architecture Search”.\nProceedings of the 2023 International Workshop on Quantum Classical Cooperative.\n2023, 17. doi: 10.1145/3588983.3596692.\n70\n[Cho+23]\nB. Cho et al. “Quantum bandit with amplitude amplification exploration in an adver-\nsarial environment”. IEEE Transactions on Knowledge and Data Engineering (2023).\ndoi: 10.1109/TKDE.2023.3279207.\n[CKP23]\nE. A. Cherrat, I. Kerenidis, and A. Prakash. “Quantum reinforcement learning via\npolicy iteration”. Quantum Mach. Intell. 5.2 (2023), 30. doi: 10.1007/s42484-023-\n00116-1.\n[CKS17]\nA. M. Childs, R. Kothari, and R. D. Somma. “Quantum Algorithm for Systems of\nLinear Equations with Exponentially Improved Dependence on Precision”. SIAM J.\nComput. 46.6 (2017), 1920. doi: 10.1137/16M1087072.\n[Cob23]\nJ. G. H. Cobussen. “Quantum Reinforcement Learning for Sensor-Assisted Robot Nav-\nigation Tasks”. MA thesis. Lund University, 2023. url: https://lup.lub.lu.se/\nstudent-papers/search/publication/9141398.\n[Cor+23]\nR. Correll et al. “Quantum Neural Networks for a Supply Chain Logistics Application”.\nAdv. Quantum Technol. 6.7 (2023), 2200183. doi: 10.1002/qute.202200183.\n[Cor18]\nA. Cornelissen. “Quantum gradient estimation and its application to quantum rein-\nforcement learning”. MA thesis. Delft University of Technology in collaboration with\nCentrum Wiskunde & Informatica, 2018. url: https://repository.tudelft.nl/\nislandora/object/uuid:26fe945f-f02e-4ef7-bdcb-0a2369eb867e.\n[Cra+18]\nD. Crawford et al. “Reinforcement Learning Using Quantum Boltzmann Machines”.\nQuantum Inf. Comput. 18.1–2 (2018), 51. url: https://www.rintonpress.com/\njournals/doi/QIC18.1-2-3.html.\n[CRC23]\nJ. Chao, R. Rodriguez, and S. Crowe. “Quantum Enhancements for AlphaZero”. Pro-\nceedings of the Companion Conference on Genetic and Evolutionary Computation.\n2023, 2179. doi: 10.1145/3583133.3596302.\n[Cro19]\nG. E. Crooks. “Gradients of parameterized quantum gates using the parameter-shift\nrule and gate decomposition”. arXiv:1905.13311 (2019). doi: 10.48550/arXiv.1905.\n13311.\n[ÇY23]\nE. Çağlar and İ. Yilmaz. “Secure Communication Based On Key Generation With\nQuantum Reinforcement Learning”. Int. J. Inf. Secur. 12.2 (2023), 22. doi: 10.55859/\nijiss.1264169.\n[CYF22]\nS. Y.-C. Chen, S. Yoo, and Y.-L. L. Fang. “Quantum long short-term memory”.\nICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE. 2022, 8622.\n[Dal+20]\nM. Dalgaard et al. “Global optimization of quantum dynamics with AlphaZero deep\nexploration”. NPJ Quantum Inf. 6.1 (2020), 1. doi: 10.1038/s41534-019-0241-0.\n[Dal+22]\nN. Dalla Pozza et al. “Quantum reinforcement learning: the maze problem”. Quantum\nMach. Intell. 4.1 (2022), 1. doi: 10.1007/s42484-022-00068-y.\n[DFB15]\nV. Dunjko, N. Friis, and H. J. Briegel. “Quantum-enhanced deliberation of learning\nagents using trapped ions”. New J. Phys. 17.2 (2015), 023006. doi: 10.1088/1367-\n2630/17/2/023006.\n[DH96]\nC. Durr and P. Hoyer. “A quantum algorithm for finding the minimum”. arXiv:quant-\nph/9607014 (1996).\n[DJ92]\nD. Deutsch and R. Jozsa. “Rapid Solution of Problems by Quantum Computation”.\nProc. R. Soc. Lond. 439.1907 (1992). doi: 10.1098/rspa.1992.0167.\n[Don+06a]\nD. Dong et al. “Quantum mechanics helps in learning for more intelligent robots”.\nChinese Phys. Lett. 23.7 (2006), 1691. doi: 10.1088/0256-307X/23/7/010.\n71\n[Don+06b]\nD. Dong et al. “Quantum robot: structure, algorithms and applications”. Robotica 24.4\n(2006), 513. doi: 10.1017/S0263574705002596.\n[Don+08a]\nD. Dong et al. “Incoherent control of quantum systems with wavefunction-controllable\nsubspaces via quantum reinforcement learning”. IEEE Transactions on Systems, Man,\nand Cybernetics, Part B (Cybernetics) 38.4 (2008), 957. doi: 10.1109/TSMCB.2008.\n926603.\n[Don+08b]\nD. Dong et al. “Quantum reinforcement learning”. IEEE Trans. Syst. Man Cybern.,\nPart B (Cybernetics) 38.5 (2008), 1207. doi: 10.1109/TSMCB.2008.925743.\n[Don+12]\nD. Dong et al. “Robust Quantum-Inspired Reinforcement Learning for Robot Navi-\ngation”. IEEE/ASME Trans Mechatron 17.1 (2012), 86. doi: 10.1109/TMECH.2010.\n2090896.\n[Dră+22]\nT.-A. Drăgan et al. “Quantum Reinforcement Learning for Solving a Stochas-\ntic Frozen Lake Environment and the Impact of Quantum Architecture Choices”.\narXiv:2212.07932 (2022). doi: 10.48550/arXiv.2212.07932.\n[DS22]\nL. Ding and L. Spector. “Evolutionary quantum architecture search for parametrized\nquantum circuits”. Proceedings of the Genetic and Evolutionary Computation Confer-\nence Companion. 2022, 2190. doi: 10.1145/3520304.3534012.\n[DS23]\nL. Ding and L. Spector. “Multi-Objective Evolutionary Architecture Search for Pa-\nrameterized Quantum Circuits”. Entropy 25.1 (2023), 93. doi: 10.3390/e25010093.\n[DTB15]\nV. Dunjko, J. M. Taylor, and H. J. Briegel. “Framework for learning agents in quantum\nenvironments”. arXiv:1507.08482 (2015). url: https://arxiv.org/abs/1507.08482.\n[DTB16]\nV. Dunjko, J. M. Taylor, and H. J. Briegel. “Quantum-Enhanced Machine Learning”.\nPhys. Rev. Lett. 117.13 (2016), 130501. doi: 10.1103/PhysRevLett.117.130501.\n[DTB17]\nV. Dunjko, J. M. Taylor, and H. J. Briegel. “Advances in quantum reinforcement\nlearning”. 2017 IEEE International Conference on Systems, Man, and Cybernetics\n(SMC) (2017), 282. doi: 10.1109/SMC.2017.8122616.\n[Dun+18]\nV. Dunjko et al. “Exponential improvements for quantum-accessible reinforcement\nlearning”. arXiv:1710.11160 (2018). doi: 10.48550/arXiv.1710.11160.\n[EGW05]\nD. Ernst, P. Geurts, and L. Wehenkel. “Tree-based batch mode reinforcement learning”.\nJ. Mach. Learn. Res. 6 (2005), 503. url: http://jmlr.org/papers/v6/ernst05a.\nhtml.\n[Fak+13]\nP. Fakhari et al. “Quantum inspired reinforcement learning in changing environment”.\nNew Math. Nat. Comput. 9.03 (2013), 273. doi: 10.1142/S1793005713400073.\n[Fey82]\nR. P. Feynman. “Simulating physics with computers”. Int. J. Theor. Phys. 21.6/7\n(1982), 467. doi: 10.1007/BF02650179.\n[FH23]\nJ. Fernández-Villaverde and I. J. Hull. Dynamic Programming on a Quantum An-\nnealer: Solving the RBC Model. Working Paper 31326. National Bureau of Economic\nResearch, 2023. doi: 10.3386/w31326.\n[Fla+20]\nF. Flamini et al. “Photonic architecture for reinforcement learning”. New J. Phys. 22.4\n(2020), 045002. doi: 10.1109/PN50013.2020.9166962.\n[Fla+23]\nF. Flamini et al. “Reinforcement learning and decision making via single-photon quan-\ntum walks”. arXiv:2301.13669 (2023). doi: 10.48550/arXiv.2301.13669.\n[Fös+18]\nT. Fösel et al. “Reinforcement Learning with Neural Networks for Quantum Feedback”.\nPhys. Rev. X 8.3 (2018), 031084. doi: 10.1103/PhysRevX.8.031084.\n72\n[FP+23]\nG. Fikadu Tilaye, A. Pandey, et al. “Investigating the effects of hyperparameters in\nquantum-enhanced deep reinforcement learning”. Quantum Eng. 2023 (2023). doi:\n10.1155/2023/2451990.\n[Fra+22]\nM. Franz et al. “Uncovering instabilities in variational-quantum deep Q-networks”. J.\nFranklin Inst. (2022). doi: 10.1016/j.jfranklin.2022.08.021.\n[Fuj+19]\nS. Fujimoto et al. “Benchmarking batch deep reinforcement learning algorithms”.\narXiv:1910.01708 (2019). doi: 10.48550/arXiv.1910.01708.\n[GA23]\nB. Ganguly and V. Aggarwal. “Quantum Acceleration of Infinite Horizon Average-\nReward Reinforcement Learning”. arXiv:2310.11684 (2023). doi: 10.48550/arXiv.\n2310.11684.\n[Gan+23]\nB. Ganguly et al. “Quantum Computing Provides Exponential Regret Improvement in\nEpisodic Reinforcement Learning”. arXiv:2302.08617 (2023). doi: 10.48550/arXiv.\n2302.08617.\n[GAW19]\nA. Gilyén, S. Arunachalam, and N. Wiebe. “Optimizing quantum optimization algo-\nrithms via faster quantum gradient computation”. Proceedings of the Thirtieth An-\nnual ACM-SIAM Symposium on Discrete Algorithms (2019), 1425. doi: 10.1137/1.\n9781611975482.87.\n[GB10]\nX. Glorot and Y. Bengio. “Understanding the difficulty of training deep feedforward\nneural networks”. J. Mach. Learn. Res. 9 (2010), 249. url: https://proceedings.\nmlr.press/v9/glorot10a.html.\n[GH19]\nM. Ganger and W. Hu. “Quantum Multiple Q-Learning”. International Journal of\nIntelligence Science 9.01 (2019), 1. doi: 10.4236/ijis.2019.91001.\n[Gil+19]\nA. Gilyén et al. “Quantum singular value transformation and beyond: exponential\nimprovements for quantum matrix arithmetics”. Proceedings of the 51st Annual ACM\nSIGACT Symposium on Theory of Computing (2019), 193. doi: 10.1145/3313276.\n3316366.\n[Gra+19]\nE. Grant et al. “An initialization strategy for addressing barren plateaus in\nparametrized quantum circuits”. Quantum 3 (2019), 214. doi: 10.22331/q-2019-\n12-09-214.\n[Haa+17]\nT. Haarnoja et al. “Reinforcement learning with deep energy-based policies”. Proceed-\nings of Machine Learning Research 70 (2017), 1352. url: https://proceedings.mlr.\npress/v70/haarnoja17a.html.\n[Haa+18]\nT. Haarnoja et al. “Soft actor-critic algorithms and applications”. arXiv:1812.05905\n(2018). doi: 10.48550/arXiv.1812.05905.\n[Ham21]\nY. Hamoudi. “Quantum Sub-Gaussian Mean Estimator”. 29th Annual European Sym-\nposium on Algorithms (ESA 2021). Vol. 204. 2021, 50:1. doi: 10.4230/LIPIcs.ESA.\n2021.50.\n[Has10]\nH.\nHasselt.\n“Double\nQ-learning”.\nNeurIPS\n23.2\n(2010),\n2613.\nurl:\nhttps\n:\n/\n/\nproceedings\n.\nneurips\n.\ncc\n/\npaper\n/\n2010\n/\nhash\n/\n091d584fced301b442654dd8c23b3fc9-Abstract.html.\n[HDW21]\nA. Hamann, V. Dunjko, and S. Wölk. “Quantum-accessible reinforcement learning\nbeyond strictly epochal environments”. Quantum Mach. Intell. 3.22 (2021), 1. doi:\n10.1007/s42484-021-00049-7.\n[Hei+22]\nD. Heimann et al. “Quantum deep reinforcement learning for robot navigation tasks”.\narXiv:2202.12180 (2022). doi: 10.48550/arXiv.2202.12180.\n[HH19a]\nW. Hu and J. Hu. “Q Learning with Quantum Neural Networks”. Natural Science\n11.01 (2019), 31. doi: 10.4236/ns.2019.111005.\n73\n[HH19b]\nW. Hu and J. Hu. “Distributional Reinforcement Learning with Quantum Neural\nNetworks”. Intelligent Control and Automation 10.02 (2019), 63. doi: 10.4236/ica.\n2019.102004.\n[HH19c]\nW. Hu and J. Hu. “Reinforcement Learning with Deep Quantum Neural Networks”.\nJournal of Quantum Information Science 9.01 (2019), 1. doi: 10.4236/jqis.2019.\n91001.\n[HH19d]\nW. Hu and J. Hu. “Training a Quantum Neural Network to Solve the Contextual\nMulti-Armed Bandit Problem”. Natural Science 11 (2019), 17. doi: 10.4236/ns.\n2019.111003.\n[Hic+23]\nM. L. Hickmann et al. “Potential analysis of a Quantum RL controller in the con-\ntext of autonomous driving”. 31st European Symposium on Artificial Neural Net-\nworks, Computational Intelligence and Machine Learning, ESANN 2023. 2023, 263.\ndoi: 10.14428/esann/2023.ES2023-22.\n[HK21]\nT. Haug and M. Kim. “Optimal training of variational quantum algorithms without\nbarren plateaus”. arXiv:2104.14543 (2021). doi: 10.48550/arXiv.2104.14543.\n[Hsi+22]\nJ.-Y. Hsiao et al. “Unentangled quantum reinforcement learning agents in the OpenAI\nGym”. arXiv:2203.14348 (2022). doi: 10.48550/arXiv.2203.14348.\n[HSS06]\nT. Hamagami, T. Shibuya, and S. Shimada. “Complex-valued reinforcement learning”.\n2006 IEEE International Conference on Systems, Man and Cybernetics 5 (2006), 4175.\ndoi: 10.1109/ICSMC.2006.384789.\n[HSW89]\nK. Hornik, M. Stinchcombe, and H. White. “Multilayer feedforward networks are uni-\nversal approximators”. Neural Netw. 2.5 (1989), 359. doi: 10.1016/0893-6080(89)\n90020-8.\n[Hu+21]\nY. Hu et al. “Quantum-enhanced reinforcement learning for control: a preliminary\nstudy”. Control. Theory Technol. 19 (2021), 455. doi: 10.1007/s11768-021-00063-x.\n[HW22]\nA. Hamann and S. Wölk. “Performance analysis of a hybrid agent for quantum-\naccessible reinforcement learning”. New J. Phys. 24.3 (2022), 033044. doi: 10.1088/\n1367-2630/ac5b56.\n[IBM23]\nIBM Quantum. Qiskit Runtime Service, Sampler primitive (Version 0.9.1). https:\n//quantum-computing.ibm.com/. 2023.\n[Jaš+19]\nJ. Jašek et al. “Experimental hybrid quantum-classical reinforcement learning by boson\nsampling: how to train a quantum cloner”. Optics Express 27.22 (2019), 32454. doi:\n10.1364/OE.27.032454.\n[Jer+21a]\nS. Jerbi et al. “Parametrized Quantum Policies for Reinforcement Learning”. Adv.\nNeural Inf. Process. Syst. 34 (2021), 28362. doi: 10.5281/zenodo.5833370.\n[Jer+21b]\nS. Jerbi et al. “Quantum Enhancements for Deep Reinforcement Learning in Large\nSpaces”. Phys. Rev. X Quantum 2.1 (2021), 010328. doi: 10.1103/PRXQuantum.2.\n010328.\n[Jer+23]\nS. Jerbi et al. “Quantum Policy Gradient Algorithms”. 18th Conference on the Theory\nof Quantum Computation, Communication and Cryptography (TQC 2023). 2023, 13:1.\ndoi: 10.4230/LIPIcs.TQC.2023.13.\n[JOA10]\nT. Jaksch, R. Ortner, and P. Auer. “Near-optimal Regret Bounds for Reinforcement\nLearning”. J. Mach. Learn. Res. 11.51 (2010), 1563. url: http://jmlr.org/papers/\nv11/jaksch10a.html.\n[Jor05]\nS. P. Jordan. “Fast Quantum Algorithm for Numerical Gradient Estimation”. Phys.\nRev. Lett. 95.5 (2005), 050501. doi: 10.1103/PhysRevLett.95.050501.\n74\n[KCP23]\nG. S. Kim, J. Chung, and S. Park. “Realizing Stabilized Landing for Computation-\nLimited\nReusable\nRockets:\nA\nQuantum\nReinforcement\nLearning\nApproach”.\narXiv:2310.06541 (2023). doi: 10.48550/arXiv.2310.06541.\n[Kha+19]\nS. Khairy et al. “Reinforcement-Learning-Based Variational Quantum Circuits Op-\ntimization for Combinatorial Problems”. arXiv:1911.04574 (2019). doi: 10.48550/\narXiv.1911.04574.\n[Kha+20]\nS. Khairy et al. “Learning to Optimize Variational Quantum Circuits to Solve Com-\nbinatorial Problems”. Proceedings of the AAAI Conference on Artificial Intelligence\n34.03 (2020), 2367. doi: 10.1609/aaai.v34i03.5616.\n[Kim+21]\nT. Kimura et al. “Variational Quantum Circuit-Based Reinforcement Learning for\nPOMDP and Experimental Implementation”. Math. Probl. Eng. 2021 (2021), 3511029.\ndoi: 10.1155/2021/3511029.\n[KLM21]\nI. Kerenidis, J. Landman, and N. Mathur. “Classical and quantum algorithms for\northogonal neural networks”. arXiv:2106.07198 (2021). doi: 10.48550/arXiv.2106.\n07198.\n[Köl+23]\nM. Kölle et al. “Multi-Agent Quantum Reinforcement Learning using Evolutionary\nOptimization”. arXiv:2311.05546 (2023). doi: 10.48550/arXiv.2311.05546.\n[Kos+21]\nI. Kostrikov et al. “Offline reinforcement learning with fisher divergence critic regu-\nlarization”. International Conference on Machine Learning. PMLR. 2021, 5774. url:\nhttps://proceedings.mlr.press/v139/kostrikov21a.html.\n[KP20]\nI. Kerenidis and A. Prakash. “A quantum interior point method for LPs and SDPs”.\nACM Transactions on Quantum Computing 1.1 (2020), 1. doi: 10.1145/3406306.\n[Kru+23]\nG. Kruse et al. “Variational Quantum Circuit Design for Quantum Reinforcement\nLearning on Continuous Environments”. arXiv:2312.13798 (2023). doi: 10.48550/\narXiv.2312.13798.\n[KSG21]\nK. Kashyap, D. Shah, and L. Gautam. “From Classical to Quantum: A Review of\nRecent Progress in Reinforcement Learning”. 2021 2nd International Conference for\nEmerging Technology (INCET) (2021), 1. doi: 10.1109/INCET51464.2021.9456218.\n[KT03]\nV. Konda and J. N. Tsitsiklis. “On Actor-Critic Algorithms”. SIAM J. Control Optim.\n42.4 (2003), 1143. doi: https://doi.org/10.1137/S0363012901385691.\n[Kum+20]\nA.\nKumar\net\nal.\n“Conservative\nq-learning\nfor\noffline\nreinforcement\nlearn-\ning”.\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems\n33\n(2020),\n1179.\nurl:\nhttps : / / proceedings . neurips . cc / paper / 2020 / hash /\n0d2b2061826a5df3221116a5085a6052-Abstract.html.\n[Kum+23]\nM. Kumar et al. “Blockchain Based Optimized Energy Trading for E-Mobility Using\nQuantum Reinforcement Learning”. IEEE Trans. Veh. Technol. 72.4 (2023), 5167. doi:\n10.1109/TVT.2022.3225524.\n[Kun22]\nL. Kunczik. “Reinforcement Learning with Hybrid Quantum Approximation in the\nNISQ Context”. PhD thesis. Universität der Bundeswehr München, 2022. doi: 10.\n1007/978-3-658-37616-1.\n[KVW18]\nW. Kool, H. Van Hoof, and M. Welling. “Attention, learn to solve routing problems!”\narXiv:1803.08475 (2018). doi: 10.48550/ARXIV.1803.08475.\n[Kwa+21]\nY. Kwak et al. “Introduction to Quantum Reinforcement Learning: Theory and\nPennyLane-based Implementation”. 2021 International Conference on Information\nand Communication Technology Convergence (ICTC) (2021), 416. doi: 10 . 1109 /\nICTC52510.2021.9620885.\n75\n[LAD21]\nY. Li, A. H. Aghvami, and D. Dong. “Intelligent Trajectory Planning in UAV-Mounted\nWireless Networks: A Quantum-Inspired Reinforcement Learning Perspective”. IEEE\nWireless Commun. Lett. 10.9 (2021), 1994. doi: 10.1109/LWC.2021.3089876.\n[Lam17]\nL. Lamata. “Basic protocols in quantum reinforcement learning with superconducting\ncircuits”. Sci. Rep. 7.1 (2017), 1. doi: 10.1038/s41598-017-01711-6.\n[Lam21]\nL. Lamata. “Quantum Reinforcement Learning with Quantum Photonics”. Photonics\n8.2 (2021), 33. doi: 10.3390/photonics8020033.\n[Lam23]\nL. Lamata. “Quantum Machine Learning Implementations: Proposals and Experi-\nments”. Adv. Quantum Technol. 6.7 (2023), 2300059. doi: 10.1002/qute.202300059.\n[Lan21]\nQ. Lan. “Variational Quantum Soft Actor-Critic”. arXiv:2112.11921 (2021). doi: 10.\n48550/arXiv.2112.11921.\n[LAT21]\nY. Liu, S. Arunachalam, and K. Temme. “A rigorous and robust quantum speed-up\nin supervised machine learning”. Nat. Phys. 17 (2021), 1013. doi: 10.1038/s41567-\n021-01287-z.\n[Lev+17]\nA. Levit et al. “Free energy-based reinforcement learning using a quantum processor”.\narXiv:1706.00074 (2017). doi: 10.48550/arXiv.1706.00074.\n[Lev+20]\nS. Levine et al. “Offline reinforcement learning: Tutorial, review, and perspectives on\nopen problems”. arXiv:2005.01643 (2020). doi: 10.48550/arXiv.2005.01643.\n[LHT22]\nJ. Lumbreras, E. Haapasalo, and M. Tomamichel. “Multi-armed quantum bandits:\nExploration versus exploitation when learning properties of quantum states”. Quantum\n6 (2022), 749. doi: 10.22331/q-2022-06-29-749.\n[Li+20a]\nJ.-A. Li et al. “Quantum reinforcement learning during human decision-making”. Nat.\nHum. Behav. 4.3 (2020), 294. doi: 10.1038/s41562-019-0804-2.\n[Li+20b]\nG. Li et al. “Breaking the sample size barrier in model-based reinforcement\nlearning with a generative model”. Adv. Neural Inf. Process Syst. 33 (2020),\n12861. url: https : / / proceedings . neurips . cc / paper / 2020 / hash /\n96ea64f3a1aa2fd00c72faacf0cb8ac9-Abstract.html.\n[Lin92]\nL.-J. Lin. “Self-improving reactive agents based on reinforcement learning, planning\nand teaching”. Mach. Learn. 8.3 (1992), 293. doi: 10.1007/BF00992699.\n[Liu+22]\nW. Liu et al. “A quantum system control method based on enhanced reinforcement\nlearning”. Soft Comput. 26.14 (2022), 6567. doi: 10.1007/s00500-022-07179-5.\n[Liu+23]\nD. Liu et al. “Multi-agent quantum-inspired deep reinforcement learning for real-time\ndistributed generation control of 100% renewable energy systems”. Eng. Appl. Artif.\nIntell. 119 (2023), 105787. doi: 10.1016/j.engappai.2022.105787.\n[LJ09]\nM. Lukoševičius and H. Jaeger. “Reservoir computing approaches to recurrent neural\nnetwork training”. Comput. Sci. Rev. 3.3 (2009), 127. doi: 10.1016/j.cosrev.2009.\n03.005.\n[LJW22]\nY.-P. Liu, Q.-S. Jia, and X. Wang. “Quantum reinforcement learning method and\napplication based on value function”. IFAC-PapersOnLine 55.11 (2022), 132. doi:\n10.1016/j.ifacol.2022.08.061.\n[LM23]\nV. Lopez-Pastor and F. Marquardt. “Self-learning machines based on Hamiltonian\necho backpropagation”. Phys. Rev. X 13.3 (2023), 031020. doi: 10.1103/PhysRevX.\n13.031020.\n[Lok+22]\nS. Lokes et al. “Implementation of Quantum Deep Reinforcement Learning Using\nVariational Quantum Circuits”. 2022 International Conference on Trends in Quantum\nComputing and Emerging Business Technologies (TQCEBT). 2022, 1. doi: 10.1109/\nTQCEBT54229.2022.10041479.\n76\n[Low+17]\nR. Lowe et al. “Multi-agent actor-critic for mixed cooperative-competitive environ-\nments”. Adv. Neural Inf. Process. Syst. 31 (2017), 6382. url: https://dl.acm.org/\ndoi/10.5555/3295222.3295385.\n[LP03]\nM. G. Lagoudakis and R. Parr. “Least-squares policy iteration”. J. Mach. Learn. Res.\n4 (2003), 1107. url: https://www.jmlr.org/papers/v4/lagoudakis03a.html.\n[LS20]\nO. Lockwood and M. Si. “Reinforcement Learning with Quantum Variational Circuit”.\nProceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital\nEntertainment 16.1 (2020), 245. url: https://ojs.aaai.org/index.php/AIIDE/\narticle/view/7437.\n[LS21]\nO. Lockwood and M. Si. “Playing Atari with Hybrid Quantum-Classical Reinforce-\nment Learning”. NeurIPS 2020 Workshop on Pre-registration in Machine Learning\n148 (2021), 285. url: https://proceedings.mlr.press/v148/lockwood21a.html.\n[Luo+20]\nX.-Z. Luo et al. “Yao. jl: Extensible, efficient framework for quantum algorithm design”.\nQuantum 4 (2020), 341. doi: 10.22331/q-2020-10-11-341.\n[LXJ23]\nY. Liu, C. Xu, and S. Jin. “Reinforcement Learning for Continuous Control: A Quan-\ntum Normalized Advantage Function Approach”. 2023 IEEE International Conference\non Quantum Software (QSW). 2023, 83. doi: 10.1109/QSW59989.2023.00020.\n[LZ22]\nT. Li and R. Zhang. “Quantum Speedups of Optimizing Approximately Con-\nvex Functions with Applications to Logarithmic Regret Stochastic Convex Ban-\ndits”. Advances in Neural Information Processing Systems. Vol. 35. 2022, 3152.\nurl: https : / / proceedings . neurips . cc / paper _ files / paper / 2022 / hash /\n14f75513f0f1ca01de1e826b52e6b840-Abstract-Conference.html.\n[Mei+23]\nK. Meinerz et al. “The Quantum Cartpole: A benchmark environment for non-linear\nreinforcement learning”. arXiv:2311.00756 (2023). doi: 10.48550/arXiv.2311.00756.\n[Mel+17]\nA. A. Melnikov et al. “Projective simulation with generalization”. Sci. Rep. 7.1 (2017),\n1. doi: 10.1038/s41598-017-14740-y.\n[Mey+23a]\nN. Meyer et al. “Quantum Natural Policy Gradients: Towards Sample-Efficient Re-\ninforcement Learning”. IEEE International Conference on Quantum Computing and\nEngineering (QCE). Vol. 2. 2023, 36. doi: 10.1109/QCE57702.2023.10181.\n[Mey+23b]\nN. Meyer et al. “Quantum Policy Gradient Algorithm with Optimized Action Decod-\ning”. International Conference on Machine Learning (ICML). Vol. 202. PMLR. 2023,\n24592. url: https://proceedings.mlr.press/v202/meyer23a.html.\n[Mey21]\nN. Meyer. “Variational Quantum Circuits for Policy Approximation”. MA thesis.\nFriedrich-Alexander-Universität Erlangen-Nürnberg, 2021.\n[MK21]\nM. Moll and L. Kunczik. “Comparing quantum hybrid reinforcement learning to clas-\nsical methods”. Hum. Intell. Syst. Integr. 3.1 (2021), 15. doi: 10.1007/s42454-021-\n00025-3.\n[ML21]\nJ. D. Martín-Guerrero and L. Lamata. “Reinforcement Learning and Physics”. Appl.\nSci. 11.18 (2021), 8589. doi: 10.3390/app11188589.\n[ML22]\nJ. D. Martín-Guerrero and L. Lamata. “Quantum Machine Learning: A tutorial”. Neu-\nrocomputing 470 (2022), 457. doi: 10.1016/j.neucom.2021.02.102.\n[Mni+15]\nV. Mnih et al. “Human-level control through deep reinforcement learning”. Nature\n518.7540 (2015), 529. doi: 10.1038/nature14236.\n[Mni+16]\nV. Mnih et al. “Asynchronous methods for deep reinforcement learning”. International\nConference on Machine Learning (ICML). Vol. 48. PMLR, 2016, 1928. url: https:\n//proceedings.mlr.press/v48/mniha16.html.\n77\n[MNM17]\nM. Mochida, H. Nakano, and A. Miyauchi. “A complex-valued reinforcement learning\nmethod using complex-valued neural networks”. IEICE Technical Report; IEICE Tech.\nRep. 117.112 (2017), 1. url: https://ken.ieice.org/ken/paper/20170629ebuV/\neng/.\n[Mon15]\nA. Montanaro. “Quantum speedup of Monte Carlo methods”. Proc. Math. Phys. Eng.\nSci. 471.2181 (2015), 20150301. doi: 10.1098/rspa.2015.0301.\n[Mül+21]\nT. Müller et al. “Towards Multi-Agent Reinforcement Learning using Quantum Boltz-\nmann Machines”. arXiv:2109.10900 (2021). doi: 10.48550/arXiv.2109.10900.\n[MVB22]\nT. Mullor, D. Vigouroux, and L. Bethune. “Efficient circuit implementation for coined\nquantum walks on binary trees and application to reinforcement learning”. IEEE/ACM\nSymposium on Edge Computing (SEC). 2022, 436. doi: 10.1109/SEC54971.2022.\n00066.\n[Nag+21]\nD. Nagy et al. “Photonic quantum policy learning in OpenAI Gym”. IEEE Interna-\ntional Conference on Quantum Computing and Engineering (QCE). 2021, 123. doi:\n10.1109/QCE52317.2021.00028.\n[Neu+17]\nF. Neukart et al. “Traffic flow optimization using a quantum annealer”. Front. ICT 4\n(2017), 29. doi: 10.3389/fict.2017.00029.\n[Neu+20]\nN. M. Neumann et al. “Multi-agent Reinforcement Learning Using Simulated Quantum\nAnnealing”. International Conference on Computational Science (2020), 562. doi: 10.\n1007/978-3-030-50433-5_43.\n[NGC15]\nS. Nuuman, D. Grace, and T. Clarke. “A quantum inspired reinforcement learning\ntechnique for beyond next generation wireless networks”. 2015 IEEE Wireless Com-\nmunications and Networking Conference Workshops (WCNCW). 2015, 271. doi: 10.\n1109/WCNCW.2015.7122566.\n[NHP23]\nN. M. Neumann, P. B. de Heer, and F. Phillipson. “Quantum reinforcement learn-\ning: Comparing quantum annealing and gate-based quantum computing with classical\ndeep reinforcement learning”. Quantum Inf. Process. 22.2 (2023), 125. doi: 10.1007/\ns11128-023-03867-9.\n[Nir+21]\nD. Niraula et al. “Quantum deep reinforcement learning for clinical decision support\nin oncology: application to adaptive radiotherapy”. Sci. Rep. 11.1 (2021), 1. doi: 10.\n1038/s41598-021-02910-y.\n[NL16]\nM. A. Nielsen and C. I. L. Quantum Computation and Quantum Information\n(10th Anniversary edition). Cambridge University Press, 2016. doi: 10 . 1017 /\nCBO9780511976667.\n[NLH20]\nR. Nian, J. Liu, and B. Huang. “A review On reinforcement learning: Introduction and\napplications in industrial process control”. Comput. Chem. Eng. 139 (2020), 106886.\ndoi: 10.1016/j.compchemeng.2020.106886.\n[NS+23]\nB. Narottama, S. Y. Shin, et al. “Layerwise Quantum Deep Reinforcement Learning\nfor Joint Optimization of UAV Trajectory and Resource Allocation”. IEEE Internet\nThings J. (2023). doi: 10.1109/JIOT.2023.3285968.\n[NW05]\nS. Naguleswaran and L. B. White. “Quantum search in stochastic planning”. Noise\nand Information in Nanoelectronics, Sensors, and Standards III 5846 (2005), 34. doi:\n10.1117/12.609962.\n[NY23]\nE. E. Nuzhin and D. Yudin. “Quantum-enhanced policy iteration on the example of a\nmountain car”. arXiv:2308.08348 (2023). doi: 10.48550/arXiv.2308.08348.\n78\n[Oli+20]\nJ. Olivares-Sánchez et al. “Measurement-Based Adaptation Protocol with Quantum\nReinforcement Learning in a Rigetti Quantum Computer”. Quantum Reports 2.2\n(2020), 293. doi: 10.3390/quantum2020019.\n[Pap+14]\nG. D. Paparo et al. “Quantum Speedup for Active Learning Agents”. Phys. Rev. X 4.3\n(2014), 031002. doi: 10.1103/PhysRevX.4.031002.\n[Par+23a]\nC. Park et al. “Quantum Multi-Agent Actor-Critic Networks for Cooperative Mobile\nAccess in Multi-UAV Systems”. IEEE Internet Things J. 10.22 (2023), 20033. doi:\n10.1109/JIOT.2023.3282908.\n[Par+23b]\nS. Park et al. “Quantum Multi-Agent Reinforcement Learning for Autonomous Mo-\nbility Cooperation”. IEEE Commun. Mag. (2023). doi: 10.1109/MCOM.020.2300199.\n[Per+06]\nD. Perez-Garcia et al. “Matrix product state representations”. arXiv:0608197 (2006).\ndoi: 10.48550/arXiv.quant-ph/0608197.\n[Pér+20]\nA. Pérez-Salinas et al. “Data re-uploading for a universal quantum classifier”. Quantum\n4 (2020), 226. doi: 10.22331/q-2020-02-06-226.\n[Per+22]\nM. Periyasamy et al. “Incremental Data-Uploading for Full-Quantum Classification”.\nIEEE International Conference on Quantum Computing and Engineering (QCE).\n2022, 31. doi: 10.1109/QCE53715.2022.00021.\n[Per+23]\nM. Periyasamy et al. “Batch Quantum Reinforcement Learning”. arXiv:2305.00905\n(2023). doi: 10.48550/arXiv.2305.00905.\n[Pes+21]\nA. Pesah et al. “Absence of barren plateaus in quantum convolutional neural networks”.\nPhys. Rev. X 11.4 (2021), 041011. doi: 10.1103/PhysRevX.11.041011.\n[PK23]\nS. Park and J. Kim. “Quantum Reinforcement Learning for Large-Scale Multi-Agent\nDecision-Making in Autonomous Aerial Networks”. 2023 VTS Asia Pacific Wireless\nCommunications Symposium (APWCS). 2023, 1. doi: 10.1109/APWCS60142.2023.\n10233966.\n[PMV02]\nV. Privman, D. Mozyrsky, and I. Vagner. “Quantum computing with spin qubits in\nsemiconductor structures”. Comput. Phys. Commun. 146 (2002), 331. doi: 10.1016/\nS0010-4655(02)00424-1.\n[PPR20]\nD. K. Park, J. Park, and J.-K. K. Rhee. “Quantum-classical reinforcement learning\nfor decoding noisy classical parity information”. Quantum Mach. Intell. 2.1 (2020), 1.\ndoi: 10.1007/s42484-020-00019-5.\n[PRD96]\nE. Pashenkova, I. Rish, and R. Dechter. Value iteration and policy iteration algorithms\nfor Markov decision problem. Citeseer, 1996. url: https://www.researchgate.net/\npublication/2605845_Value_iteration_and_policy_iteration_algorithms_\nfor_Markov_decision_problem.\n[Rai+23]\nS. Rainjonneau et al. “Quantum algorithms applied to satellite mission planning for\nEarth observation”. IEEE Journal of Selected Topics in Applied Earth Observations\nand Remote Sensing 16 (2023), 7062. doi: 10.1109/JSTARS.2023.3287154.\n[Raj+21]\nK. Rajagopal et al. “Quantum amplitude amplification for reinforcement learning”.\nHandbook of Reinforcement Learning and Control 325 (2021), 819. doi: 10.1007/978-\n3-030-60990-0_26.\n[Ram17]\nA. Ramezanpour. “Optimization by a quantum reinforcement algorithm”. Phys. Rev.\nA 96.5 (2017), 052307. doi: 10.1103/PhysRevA.96.052307.\n[Ree23]\nV. Reers. “Towards Performance Benchmarking for Quantum Reinforcement Learn-\ning”. INFORMATIK 2023 - Designing Futures: Zukünfte gestalten. (2023), 1135. doi:\n10.18420/inf2023_126.\n79\n[Ren+22]\nY. Ren et al. “NFT-based intelligence networking for connected and autonomous ve-\nhicles: A quantum reinforcement learning approach”. IEEE Network 36.6 (2022), 116.\ndoi: 10.1109/MNET.107.2100469.\n[RKM22]\nF. Rezazadeh, S. Kahvazadeh, and M. Mosahebfard. “Towards Quantum-Enabled 6G\nSlicing”. arXiv:2212.11755 (2022). doi: 10.48550/arXiv.2212.11755.\n[RN94]\nG. A. Rummery and M. Niranjan. On-line Q-learning using connectionist systems.\nVol. 37. Citeseer, 1994. url: https : / / www . researchgate . net / publication /\n2500611_On-Line_Q-Learning_Using_Connectionist_Systems.\n[Ron19]\nP. Ronagh. “The Problem of Dynamic Programming on a Quantum Computer”.\narXiv:1906.02229 (2019). doi: 10.48550/arXiv.1906.02229.\n[Sag+21a]\nV. Saggio et al. “Experimental quantum speed-up in reinforcement learning agents”.\nNature 591.7849 (2021), 229. doi: 10.1038/s41586-021-03242-7.\n[Sag+21b]\nV. Saggio et al. “Quantum speed-ups in reinforcement learning”. Quantum Nanopho-\ntonic Materials, Devices, and Systems 2021 11806 (2021), 40. doi: 10.1117/12.\n2593720.\n[San+22]\nF. Sanches et al. “Short quantum circuits in reinforcement learning policies for the\nvehicle routing problem”. Phys. Rev. A 105.6 (2022), 062403. doi: 10.1103/PhysRevA.\n105.062403.\n[San+23]\nA. Sannia et al. “A hybrid classical-quantum approach to speed-up Q-learning”. Sci.\nRep. 13.1 (2023), 3913. doi: 10.1038/s41598-023-30990-5.\n[SB18]\nR. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. Second Edi-\ntion. The MIT Press, 2018. url: http://incompleteideas.net/book/the-book-\n2nd.html.\n[Sch+22]\nM. Schenk et al. “Hybrid actor-critic algorithm for quantum reinforcement learning at\ncern beam lines”. arXiv:2209.11044 (2022). doi: 10.48550/arXiv.2209.11044.\n[SH20]\nE. Sorensen and W. Hu. “Practical Meta-Reinforcement Learning of Evolutionary\nStrategy with Quantum Neural Networks for Stock Trading”. Journal of Quantum\nInformation Science 10.3 (2020), 43. doi: 10.4236/jqis.2020.103005.\n[SH23]\nM. Shahid and M. A. Hassan. “Introducing Quantum Variational Circuit for Efficient\nManagement of Common Pool Resources”. IEEE Access 11 (2023), 110862. doi: 10.\n1109/ACCESS.2023.3322144.\n[She+20]\nK. S. Shenoy et al. “Demonstration of a measurement-based adaptation protocol with\nquantum reinforcement learning on the IBM Q experience platform”. Quantum Inf.\nProcess. 19 (2020), 1. doi: 10.1007/s11128-020-02657-x.\n[Shi+22]\nH. Shinkawa et al. “Bandit approach to conflict-free multi-agent Q-learning in view\nof photonic implementation”. arXiv:2212.09926 (2022). doi: 10.48550/arXiv.2212.\n09926.\n[Sho97]\nP. W. Shor. “Polynomial-Time Algorithms for Prime Factorization and Discrete Loga-\nrithms on a Quantum Computer”. SIAM J. Comput. 26.5 (1997), 1484. doi: 10.1137/\ns0097539795293172.\n[Sil+18]\nD. Silver et al. “A general reinforcement learning algorithm that masters chess, shogi,\nand Go through self-play”. Science 362.6419 (2018), 1140. doi: 10.1126/science.\naar6404.\n[SJA19]\nS. Sim, P. D. Johnson, and A. Aspuru-Guzik. “Expressibility and entangling capabil-\nity of parameterized quantum circuits for hybrid quantum-classical algorithms”. Adv.\nQuantum Technol. 2.12 (2019), 1900070. doi: 10.1002/qute.201900070.\n80\n[SJD22]\nA. Skolik, S. Jerbi, and V. Dunjko. “Quantum agents in the Gym: a variational quan-\ntum algorithm for deep Q-learning”. Quantum 6 (2022), 720. doi: 10.22331/q-2022-\n05-24-720.\n[Sko+23]\nA. Skolik et al. “Robustness of quantum reinforcement learning under hardware errors”.\nEPJ Quantum Technol. 10.1 (2023), 1. doi: 10.1140/epjqt/s40507-023-00166-1.\n[SMK23]\nA. Sinha, A. Macaluso, and M. Klusch. “Nav-Q: Quantum Deep Reinforcement Learn-\ning for Collision-Free Navigation of Self-Driving Cars”. arXiv:2311.12875 (2023). doi:\n10.48550/arXiv.2311.12875.\n[SMT23]\nY. Sun, Y. Ma, and V. Tresp. “Differentiable Quantum Architecture Search for Quan-\ntum Reinforcement Learning”. IEEE International Conference on Quantum Comput-\ning and Engineering (QCE). Vol. 2. 2023, 15. doi: 10.1109/QCE57702.2023.10177.\n[SP18]\nM. Schuld and F. Petruccione. Supervised Learning with Quantum Computers. Vol. 17.\nSpringer, 2018. url: https://link.springer.com/book/10.1007/978-3-319-\n96424-9.\n[Sri+18]\nT. Sriarunothai et al. “Speeding-up the decision making of a learning agent using an\nion trap quantum processor”. Quantum Sci. Technol. 4.1 (2018), 015014. doi: 10.\n1088/2058-9565/aaef5e.\n[SSB23]\nA. Sequeira, L. P. Santos, and L. S. Barbosa. “Policy gradients using variational quan-\ntum circuits”. Quantum Mach. Intell. 5.1 (2023), 18. doi: 10.1007/s42484- 023-\n00101-8.\n[SSM21]\nM. Schuld, R. Sweke, and J. J. Meyer. “Effect of data encoding on the expressive power\nof variational quantum-machine-learning models”. Phys. Rev. A 103.3 (2021), 032430.\ndoi: 10.1103/physreva.103.032430.\n[Sto+20]\nJ. Stokes et al. “Quantum Natural Gradient”. Quantum 4 (2020), 269. doi: 10.22331/\nq-2020-05-25-269.\n[Sut+99]\nR. S. Sutton et al. “Policy gradient methods for reinforcement learning with function\napproximation”. NeurIPS 12 (1999). url: https://papers.nips.cc/paper/1999/\nhash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html.\n[SWM10]\nM. Saffman, T. G. Walker, and K. Mølmer. “Quantum information with Rydberg\natoms”. Rev. Mod. Phys. 82.3 (2010), 2313. doi: 10.1103/RevModPhys.82.2313.\n[Tei21]\nM. A. B. Teixeira. “Quantum Reinforcement Learning Applied to Games”. PhD thesis.\nUniversidade do Porto (Portugal), 2021. url: https://repositorio-aberto.up.pt/\nbitstream/10216/135628/2/487581.pdf.\n[Tha+23]\nS. Thanasilp et al. “Subtleties in the trainability of quantum machine learning models”.\nQuantum Mach. Intell. 5.1 (2023), 21. doi: 10.1007/s42484-023-00103-6.\n[TRC21]\nM. Teixeira, A. P. Rocha, and A. J. Castro. “Quantum Reinforcement Learning Ap-\nplied to Board Games”. IEEE/WIC/ACM International Conference on Web Intelli-\ngence and Intelligent Agent Technology. 2021, 343. doi: 10.1145/3486622.3493944.\n[Tru+23]\nN. Truong Thu Ngo et al. “Investigating Quantum Reinforcement Learning structure\nto the CartPole control task”. Proceedings of the 9th International Conference of Asian\nSociety for Precision Engineering and Nanotechnology (ASPEN2022). 2023, 227. url:\nhttps://eprints.qut.edu.au/239327/.\n[VGS16]\nH. Van Hasselt, A. Guez, and D. Silver. “Deep reinforcement learning with double\nq-learning”. Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 30. 1.\n2016. doi: 10.1609/aaai.v30i1.10295.\n81\n[Wan+21a]\nD. Wang et al. “Quantum algorithms for reinforcement learning with a generative\nmodel”. International Conference on Machine Learning (ICML) 139 (2021), 10916.\nurl: https://proceedings.mlr.press/v139/wang21w.html.\n[Wan+21b]\nD. Wang et al. “Quantum exploration algorithms for multi-armed bandits”. Proceedings\nof the AAAI Conference on Artificial Intelligence. Vol. 35. 11. 2021, 10102. doi: 10.\n1609/aaai.v35i11.17212.\n[Wan+23]\nZ. Wan et al. “Quantum multi-armed bandits and stochastic linear bandits enjoy\nlogarithmic regrets”. Proceedings of the AAAI Conference on Artificial Intelligence.\nVol. 37. 8. 2023, 10087. doi: 10.1609/aaai.v37i8.26202.\n[WAU20]\nZ. T. Wang, Y. Ashida, and M. Ueda. “Deep reinforcement learning control of quantum\ncartpoles”. Phys. Rev. Lett. 125.10 (2020), 100401. doi: 10.1103/PhysRevLett.125.\n100401.\n[WD92]\nC. J. C. H. Watkins and P. Dayan. “Q-learning”. Mach. Learn. 8.3 (1992), 279. doi:\n10.1007/BF00992698.\n[Wei+21]\nQ. Wei et al. “Deep Reinforcement Learning With Quantum-Inspired Experience Re-\nplay”. IEEE Trans. Cybern. 52.9 (2021), 9326. doi: 10.1109/TCYB.2021.3053414.\n[Wie+22a]\nS. Wiedemann et al. “Quantum Policy Iteration via Amplitude Estimation and Grover\nSearch–Towards Quantum Advantage for Reinforcement Learning”. arXiv:2206.04741\n(2022). doi: 10.48550/arXiv.2206.04741.\n[Wie+22b]\nD. Wierichs et al. “General parameter-shift rules for quantum gradients”. Quantum 6\n(2022), 677. doi: 10.22331/q-2022-03-30-677.\n[Wie+23]\nM. Wiedmann et al. “An Empirical Comparison of Optimizers for Quantum Machine\nLearning with SPSA-based Gradients”. IEEE International Conference on Quantum\nComputing and Engineering (QCE). Vol. 1. 2023, 450. doi: 10.1109/QCE57702.2023.\n00058.\n[Wie21]\nS. Wiedemann. “Modelling and Solving Reinforcement Learning Problems on Quan-\ntum Computers”. MA thesis. Technische Universität München, 2021.\n[Wil92]\nR. J. Williams. “Simple statistical gradient-following algorithms for connectionist re-\ninforcement learning”. Mach. Learn. 8.3 (1992), 229. doi: 10.1007/BF00992696.\n[Wu+23]\nS. Wu et al. “Quantum reinforcement learning in continuous action space”.\narXiv:2012.10711 (2023). doi: 10.48550/arXiv.2012.10711.\n[Yan+22]\nR. Yan et al. “A multiagent quantum deep reinforcement learning method for dis-\ntributed frequency control of islanded microgrids”. IEEE Trans. Control Netw. Syst.\n9.4 (2022), 1622. doi: 10.1109/TCNS.2022.3140702.\n[Yan23]\nJ. Yang. “Apply Deep Reinforcement Learning with Quantum Computing on the Pric-\ning of American Options”. Internet Finance and Digital Economy. 2023, 675. doi:\n10.1142/9789811267505_0050.\n[Yin+21]\nL. Yin et al. “Quantum deep reinforcement learning for rotor side converter control\nof double-fed induction generator-based wind turbines”. Engineering Applications of\nArtificial Intelligence 106 (2021), 104451. doi: 10.1016/j.engappai.2021.104451.\n[YN06]\nJ. Q. You and F. Nori. “Superconducting Circuits and Quantum Information”. Phys.\nToday 58.11 (2006), 42. doi: 10.1063/1.2155757.\n[YPK23]\nW. J. Yun, J. Park, and J. Kim. “Quantum multi-agent meta reinforcement learning”.\nProceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. 9. 2023, 11087.\ndoi: 10.1609/aaai.v37i9.26313.\n82\n[Yun+22]\nW. J. Yun et al. “Quantum multi-agent reinforcement learning via variational quantum\ncircuit design”. 2022 IEEE 42nd International Conference on Distributed Computing\nSystems (ICDCS). 2022, 1332. doi: 10.1109/ICDCS54860.2022.00151.\n[Yun+23]\nW. J. Yun et al. “Quantum Multi-Agent Actor-Critic Neural Networks for Internet-\nConnected Multi-Robot Coordination in Smart Factory Management”. IEEE Internet\nThings J. 10.11 (2023), 9942. doi: 10.1109/JIOT.2023.3234911.\n[Zha+11]\nT. Zhao et al. “Analysis and Improvement of Policy Gradient Estimation”. Adv. Neural\nInf. Process. Syst. 24 (2011), 118. doi: 10.1016/j.neunet.2011.09.005.\n[Zha+19]\nX.-M. Zhang et al. “When does reinforcement learning stand out in quantum control?\nA comparative study on state preparation”. NPJ Quantum Inf. 5.1 (2019), 85. doi:\n10.1038/s41534-019-0201-8.\n[Zha+22]\nS.-X. Zhang et al. “Differentiable quantum architecture search”. Quantum Sci. Technol.\n7.4 (2022), 045023. doi: 10.1088/2058-9565/ac87cd.\n[Zho+23]\nH. Zhong et al. “Provably Efficient Exploration in Quantum Reinforcement Learn-\ning with Logarithmic Worst-Case Regret”. arXiv:2302.10796 (2023). doi: 10.48550/\narXiv.2302.10796.\n[Zie+08]\nB. D. Ziebart et al. “Maximum entropy inverse reinforcement learning”. AAAI 8\n(2008), 1433. url: https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf.\n[ZY23]\nJ. Zhao and W. Yu. “Quantum Multi-Agent Reinforcement Learning as an Emerging\nAI Technology: A Survey and Future Directions”. TechRxiv (2023). doi: 10.36227/\ntechrxiv.24563293.v1.\n83\n",
  "categories": [
    "quant-ph",
    "cs.LG"
  ],
  "published": "2022-11-07",
  "updated": "2024-03-08"
}