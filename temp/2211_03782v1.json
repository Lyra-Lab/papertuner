{
  "id": "http://arxiv.org/abs/2211.03782v1",
  "title": "On minimal variations for unsupervised representation learning",
  "authors": [
    "Vivien Cabannes",
    "Alberto Bietti",
    "Randall Balestriero"
  ],
  "abstract": "Unsupervised representation learning aims at describing raw data efficiently\nto solve various downstream tasks. It has been approached with many techniques,\nsuch as manifold learning, diffusion maps, or more recently self-supervised\nlearning. Those techniques are arguably all based on the underlying assumption\nthat target functions, associated with future downstream tasks, have low\nvariations in densely populated regions of the input space. Unveiling minimal\nvariations as a guiding principle behind unsupervised representation learning\npaves the way to better practical guidelines for self-supervised learning\nalgorithms.",
  "text": "ON MINIMAL VARIATIONS FOR UNSUPERVISED REPRESENTATION LEARNING\nVivien Cabannes\nAlberto Bietti\nRandall Balestriero\nMeta AI\nABSTRACT\nUnsupervised representation learning aims at describing raw\ndata eﬃciently to solve various downstream tasks.\nIt has\nbeen approached with many techniques, such as manifold\nlearning, diﬀusion maps, or more recently self-supervised\nlearning. Those techniques are arguably all based on the under-\nlying assumption that target functions, associated with future\ndownstream tasks, have low variations in densely populated\nregions of the input space. Unveiling minimal variations as a\nguiding principle behind unsupervised representation learning\npaves the way to better practical guidelines for self-supervised\nlearning algorithms.\nIndex Terms— Self-supervised learning, unsupervised\nlearning, minimal variations, ﬁrst principles.\n1. INTRODUCTION\nData is everywhere, but it is often too unstructured or high\ndimensional to leverage classical statistics on their raw form.\nRecent advances in machine learning have succeeded in exploit-\ning parts of the millions terabytes of unlabeled data contained\non the internet. This was achieved by creating self-supervised\ntasks to be solved by the machine, inciting it to learn good\nrepresentations of text [1, 2]. Those “foundational” representa-\ntions are now being leveraged to solve several “downstream”\ntasks on languages [3]. Similar developments have been made\non other high-dimensional data such as images, videos or audio\nspeeches [4, 5, 6]. Despite their rapid progress, the training\nof self-supervised learning (SSL) models remains challenging\nand lacks theoretical foundations.\nLearning without supervision has been historically referred\nto as unsupervised learning. While at ﬁrst sights, the literature\nbodies on unsupervised learning and self-supervised learning\nseem relatively disjoint, connections have been made between\nthe two [7, 8]. This work provides further insights on their\nlinks through the concept of minimal variations, detailed in\nSection 3. Theory is veriﬁed on synthetic experiments in\nSection 4. This understanding could be leveraged in future\nwork to improve SSL algorithms in practical settings with\nlimited compute resources.\n2. SETTING AND CONTEXT\nIn the following, X shall be a Hilbert space (i.e. endowed with\na scalar product) and Y an output space. A distribution 𝜌𝑋is\nassumed to have generated a dataset (𝑋𝑖)𝑖∈[𝑛] of independent\nsamples 𝑋𝑖∼𝜌𝑋for 𝑖∈[𝑛].1 Our goal is to ﬁnd a repre-\nsentation 𝜑: X →R𝑝for a small 𝑝, such that for relevant\ndownstream distributions 𝜌on pairs of input/output and loss\nfunctions ℓ: Y × Y →R, one can eﬃciently minimize the\nsubsequent population risk\nR( 𝑓; 𝑝, ℓ) = E(𝑋,𝑌)∼𝜌[ℓ( 𝑓(𝑋),𝑌)] ,\n(1)\nbased on i.i.d. samples (𝑋𝑖,𝑌𝑖). More exactly, the optimal\nfunctions 𝑓should easily be approached under the form 𝑓=\n𝑔◦𝜑for 𝑔in a small class of functions. Typically, 𝑔would be\na linear function, a.k.a. a linear probe. Reducing the search of\n𝑓: X →Y in a potentially big function space to the search of\n𝑔: R𝑝→Y in a much smaller one, will drastically improve\nsample eﬃciency [9].\nTo learn the representation 𝜑, SSL leverages augmenta-\ntions of data. It deﬁnes 𝑡: X × Ξ →X a transformation\nparameterized by 𝜉∈Ξ. For example, Ξ could be X and\n𝑡(𝑋, 𝜉) = 𝑋+ 𝜉. Assuming that transformations 𝑡(𝑋, 𝜉) do not\nfundamentally change the semantic of the input, any pairs of\naugmented and original points should be close in the features\nspace 𝜑(X). This is put in equations through the minimization\nof the variational quantity\nE𝑋,𝜉[𝑑(𝜑(𝑡(𝑋, 𝜉)), 𝜑(𝑋))] ,\n(2)\nfor 𝑑a notion of similarity in R𝑝, e.g. the square loss 𝑑(𝑥, 𝑥′) =\n∥𝑥−𝑥′∥2. In practice, 𝜑is often taken as a neural network,\nand its learning is conducted through the optimization of its\nparameters. Equation (2) is trivially minimized by setting 𝜑\nto a constant. To avoid such a “collapse” phenomenon, one\nshould encourage diversity in the representation, for instance\nby using the constraint\nE𝑋[𝜑(𝑋)𝜑(𝑋)⊤] = 𝐼.\n(3)\nClassical self-supervised techniques such as SimCLR [4],\nBarlow Twins [10] and VICReg [11] can be understood as\nimplementing diﬀerent speciﬁcations of such a scheme [8]\n(respectively, 𝑑would be the cosine similarity, some cross-\ncorrelation measure, and the square hinge loss).\n1The set {1, · · · , 𝑛} is denoted [𝑛].\narXiv:2211.03782v1  [cs.LG]  7 Nov 2022\n3. MINIMAL VARIATIONS\n3.1. Classical hypothesis\nThis section reviews classical assumptions about the nature\nof downstream tasks with respect to the input distribution 𝜌𝑋.\nIt shows how those assumptions are related to the idea that\nfuture target functions have low variations on highly populated\nregions of the input space.\nOften praised in semi-supervised learning setting, the\ncluster assumption states that the support of 𝜌𝑋have several\nconnected components and that downstream classiﬁcation tasks\nare likely to respect this structure, i.e. labels shall be constant\nover each connected component a.k.a. cluster [12]. In other\nterms, one expects the decision boundary2 between classes\nto be situated in regions of the input space where there is no\ndensity. Yet, on big or poorly curated datasets, classes might\nnot be separated by no-density regions. In such a setting, the\ncluster assumption is relaxed as the low-density separation\nhypothesis, assuming that downstream decision boundaries\nwill fall in low-density regions (i.e. where 𝜌𝑋is small). For\nexample, in a balanced binary classiﬁcation problem where\nY = {−1, 1} and d𝜌(𝑥| 𝑦) ∝exp(−∥𝑥+ 𝑦𝜇∥/𝜎2) d𝑥, the\noptimal decision boundary is the hyperplane 𝜇⊥which does\nminimize the value of 𝜌𝑋(𝐴) for any hyperplane 𝐴that cross\n[−𝜇, +𝜇].\nIn regression settings, it is often assumed that the down-\nstream target functions will be smooth on densely populated\nregions of the input space [13]. Variations of functions are\nmeasured through quantities such as\nJ ( 𝑓) = E [∥∇𝑓(𝑋)∥𝑞] ,\n(4)\nfor 𝑞= 1 (total variation), 𝑞= 2 (Dirichlet energy) or higher\n(𝑞-Laplacian). Interestingly, binary classiﬁcation problems\nare often approached through the learning of a continuous\nsurrogate function 𝑓whose sign is taken as the classiﬁcation\nrule [14]. From a classiﬁcation perspective, variations of 𝑓\nare only needed in order for 𝑓to change sign, and the low-\nvariation hypothesis states that those variations should take\nplace in sparsely populated areas of the input space. This is\ncoherent with the low-density separation hypothesis stating\nthat 𝑓should change sign in sparsely populated regions.\n3.2. Embedding for minimal variations\nThis section extends on classical unsupervised techniques as\naiming to minimize the criterion (4) under the orthonormal\nconstraint (3).\nAssuming low-variation of downstream tasks, it is natural\nto design 𝜑in order to represent a maximum number of low-\nvariation functions as 𝑔◦𝜑. Considering linear probes, the\n2For a predictor 𝑓: X →Y, the input space X is partitioned into decision\nregions X𝑦= {𝑥| 𝑓(𝑥) = 𝑦} indexed by 𝑦∈Y, “decision boundaries” refers\nto the boundaries of those regions.\nspan of (𝜑𝑖)𝑖∈[𝑝] could be searched as a 𝑝-dimension space of\nfunctions with minimal variations according to the criterion (4).\nPut in equations with 𝜑: X →R𝑝, this reads\narg min\n𝜑;s.t. (3)\nmax\n𝑤∈R𝑝;∥𝑤∥=1 J (𝑤⊤𝜑(·)),\nunder the coverage constraint (3). Such a 𝜑is an ideal data\nrepresentation to solve downstream tasks with linear probes, as\nlong as solutions verify the low-variation hypothesis. With 𝐷\ndenoting the Jacobian, the formulation with 𝑞= 2 translates as\n𝜑= arg min\n𝜑s.t. (3)\nE\n\u0002\n∥𝐷𝜑(𝑋)∥2\n𝐹\n\u0003\n.\n(5)\nIn practice, this formulation is favored for analytical reasons.\nBy making J ( 𝑓) a quadratic form, it reveals the operator\nL that represents it.3\nIn particular, equation (5) is solved\nexplicitly with 𝜑𝑖the 𝑖-th eigenfunctions of L.4 The proof is a\nsimple application of the Rayleigh-Ritz formula, that deﬁnes\neigenfunctions recursively through the formula\n𝜑𝑖= arg min\n𝜑:X→R\n⟨𝜑, L𝜑⟩= E\n\u0002\n∥∇𝜑(𝑋)∥2\u0003\ns.t. E\n\u0002\n𝜑𝑖(𝑋)𝜑𝑗(𝑋)\n\u0003\n= 𝛿𝑖𝑗\n∀𝑗< 𝑖.\nIn the literature, this approach is often referred to as\nspectral embedding (the space is embedded through the spectral\ndecomposition of the operator). It is particularly well suited\nfor the cluster assumption, since the null space of L is nothing\nbut the span of the indicator functions of each connected\ncomponent of 𝜌𝑋, which has motivated its use for clustering\nand manifold regularization [15]. Under mild assumptions,\nL is indeed a diﬀusion operator (when 𝜌𝑋has a density and\ncompact support L 𝑓= −Δ 𝑓+ ⟨∇log 𝜌𝑋, ∇𝑓⟩), which links it\nto diﬀusion maps [16], label propagation [17] and Langevin\ndynamics [18].\nSince the 2000s, the criterion (4) has been approached in a\nnon-parametric fashion based on ﬁnite diﬀerences, leveraging\ngraph Laplacians [17, 19]. Based on samples (𝑋𝑖)𝑖∈[𝑛], it aims\nat minimizing\nE𝑔(𝜑) =\n∑︁\n𝑖,𝑗∈[𝑛]\nh\n𝑘𝜎(𝑋𝑖, 𝑋𝑗)\n\r\r𝜑(𝑋𝑖) −𝜑(𝑋𝑗)\n\r\r2i\n,\n(6)\nwith 𝑘a notion of similarity to perform ﬁnite diﬀerences and 𝜎\na scaling parameter (e.g. 𝑘𝜎(𝑥, 𝑥′) = exp(−∥𝑥−𝑥′∥2 /𝜎2)),\nand subject to the empirical version of the constraint (3),\n1\n𝑛2\n∑︁\n𝑖,𝑗∈[𝑛]\n𝜑(𝑋𝑖)𝜑(𝑋𝑖)⊤= 𝐼.\n3Some minor mathematical precautions should be taken to deal with this\nweighted Sobolev pseudo-norm, we will omit them in this paper.\n4The solution of 𝜑is unique up to orthonormal transformations 𝑈𝜑for\n𝑈∈R𝑝×𝑝orthogonal, and to permutation of eigenfunctions associated with\nthe 𝑝-th eigenvalue of L.\n3.3. Consistency results\nThis section discusses limiting behaviors of the methods de-\nscribed previously, namely SSL (2), graph Laplacian (6) and\nDirichlet energy (5).\nGraph Laplacians have arguably two convergence proper-\nties. On the one hand, keeping the scale 𝜎constant, as the\nnumber of samples 𝑛goes to inﬁnity, the empirical minimizer\nminimizes the following measure of variations\nE\n\u0002\n∥𝑑(𝑋) 𝑓(𝑋) −𝑘𝜎∗𝑓(𝑋)∥2\u0003\n, with\n𝑘𝜎∗𝑓(𝑥) = E[𝑘𝜎(𝑥, 𝑋) 𝑓(𝑋)],\n𝑑= 𝑘𝜎∗1,\nwhich can be seen as a smoothed, reweighted version of the\nDirichlet energy. The convergence happens relatively fast,\ntypically in 𝑂(𝑛−1/2) in 𝐿2-norm [20]. On the other hand, with\nthe right scaling of 𝜎, this ﬁnite diﬀerence method is able to\nconverge towards the ideal solution deﬁned by (5). Yet the\nconvergence rates are much worse, e.g. in 𝑂(𝑛−1/𝑑) for 𝑑the\ndimension of the data manifold (𝑑= dim supp 𝜌𝑋) [21]. This\nmay be understood intuitively, to measure variations with ﬁnite\ndiﬀerences, the number of points needed grows exponentially\nwith dimension [22].\nAlternatively, (4) might be estimated directly with empirical\nsamples and a parametric model such as neural networks or\nkernel methods. This enables fast convergence towards the\nsolution of (5) within the search space of functions for 𝜑. By\nnot suﬀering from the curse of dimension and converging to\nthe ideal operator, this approach is statistically superior [23].\nYet, it requires optimization over derivatives which can lead to\ncomputational drawbacks.\n3.4. Insights for SSL\nWe argue that SSL objectives such as (2) can be seen as\nmeasures of variations. In particular, since 𝑡(𝑥, 𝜉) is supposed\nto be closed to 𝑥(at least semantically speaking), it behaves\nas a random variable (with respect to 𝜉) to compute ﬁnite\ndiﬀerences at a point 𝑥∈X.\nTherefore, we expect SSL\nalgorithms either, when keeping the scale of 𝜉constant,5 to\nconverge fast to the minimizer of some smoothed version of\na functional that measure variations (4) (depending on 𝑡and\nthe distance 𝑑), or, with the right decreasing scale, to converge\nslowly to the ideal functional itself.\n4. EXPERIMENTS\nPrior sections have introduced three techniques to learn 𝜑, SSL\n(2), graph Laplacian (6) and empirical Dirichlet energy (5).\nWe have argued that they all aimed at learning the same type of\nfunctions, i.e. orthogonal functions that minimize variations.\nAfter implementation details, proof-of-concept experiments\nverify this claim.\n5Here, Ξ is implicitly assumed to be a Banach space and the transformation\nto verify 𝑡(𝑥+ 𝜉) = 𝑥+ 𝑜( ∥𝜉∥).\n4.1. Implementation details\nThis section reviews implementation details based on empirical\nsamples (𝑋𝑖)𝑖∈[𝑛]. Experiments were made with the following\nspeciﬁcation of the self-supervised learning objective\nE𝑠(𝜑) = 1\n𝑛\n∑︁\n𝑖∈[𝑛]\n∥𝜑(𝑋𝑖) −𝜑(𝑋𝑖+ 𝜎𝜉𝑖)∥2 ,\n(2)\nwith 𝜉𝑖a random unit Gaussian variable, and 𝜎a scale param-\neter. The empirical version of Dirichlet energy reads\nE𝑒(𝜑) = 1\n𝑛\n∑︁\n𝑖∈[𝑛]\n∥∇𝜑(𝑋𝑖)∥2 ,\n(5)\nwhich, in the case of deep networks, is related to double\nbackpropagation [24] and has been used in other contexts [25,\n26]. Finally, the graph Laplacian objective is nothing but (6).\nIn experiments, the orthogonality constraints was relaxed\nas a penalty reading\nΩ(𝜑) =\n\r\r 1\n𝑛2\n∑︁\n𝑖∈[𝑛]\n𝜑(𝑋𝑖)𝜑(𝑋𝑖)⊤−𝐼\n\r\r2,\nwhile the ﬁnal objective is E(𝜑) + 𝜆Ω(𝜑).\nSelf-supervised learning is known to be quite unstable\nto changes in hyperparameters. In experiments, the scale\nparameters (standard deviation of augmentation in SSL, and\nkernel scaling in graph Laplacian) were set to match the\nwidth of the half-moon dataset (which was itself generated\nwith Gaussian noise). Stochastic gradient descent parameters\n(learning rate scheduling, batch size) were tuned to succeed\nthe sole minimization of Ω.6 Finally, the regularizer 𝜆was\nset to approximately balance the penalty and the objective at\nhand (the learning rate was divided by 𝜆accordingly). The\nrepresentation 𝜑was parameterized with a fully connected\nneural network with ﬁve hidden layers, each containing a\nhundred neurons.\nThe code is available online at https:\n//github.com/VivienCabannes/laplacian.\n4.2. Consistency results.\nThis section checks the claim that the three objectives (2), (5)\nand (5) are learning similar functions. It proceeds with the two\nhalf-moons dataset (Figure 1).\nIn this setting, the eigenfunctions of L are related to the\nFourier basis on the union of two segments and are relatively\nstable under smoothing of the diﬀerential functional. Beside\nthe constant function, the null space of L is made of 𝜑1 the\ndiﬀerence of the indicator functions of both half-moons. The\nsecond two eigenfunctions 𝜑2 and 𝜑3 are ﬁrst-mode waves on\neach component. Figure 1 reports the learned 𝜑for the three\n6Note that because the expectation is inside the norm in Ω a naive mini-\nbatch strategy does not provide unbiased stochastic estimate of its gradient.\nWe overcame this issue by considering large batches.\nFig. 1: Functions learned with 𝑝= 2 for SSL (2) on the right, graph Laplacian (6) in the middle, and Dirichlet energy (5) on the right.\nDatapoints (𝑋𝑖)𝑖∈[𝑛] are represented as black dots, while the functions 𝜑𝑖(𝑋) are represented through the level regions in diﬀerent colors.\nmethods with 𝑝= 2. All methods recover 𝜑1 (top), the ﬁrst two\nrecover 𝜑2 while the third one recovers a mixture cos(𝜃)𝜑2 +\nsin(𝜃)𝜑3 for some 𝜃∈[0, 2𝜋], which also minimizes (5).\nTable 1 is concerned with 𝑝= 5, and the downstream task\nthat consists in predicting if 𝑥was in the top (or bottom) of\nthe left (or right) half-moon. This generates a classiﬁcation\nproblem with four diﬀerent classes. Such a task is ideal to\nevaluate our argumentation since the ﬁrst ﬁve eigenfunctions\nof the diﬀusion operator L discriminate those four parts of the\nspace with linear probing. The results are satisfying.\nSSL (2)\nenergy (5)\ngraph (6)\n96.14 ± 0.16\n97.32 ± 0.16\n95.15 ± 0.19\nTable 1: Accuracy on downstream task with linear probing to\ncheck eigenspace retrieval (random is 0.25).\n4.3. Discussion\nWhile the previous experiments are made on small synthetic\ndata, some behaviors are worse mentioning. First, the SSL\nobjective (2) and the graph Laplacian (6) lead to similar results.\nYet, graph Laplacian only uses samples on the data manifold,\nwhile augmented data in SSL gets out of it. At ﬁrst sight, it\nseems better to restrict computations of ﬁnite diﬀerences to the\nmanifold: the method would scale with the intrinsic dimension\nof data instead of the explicit input dimension [21]. In practice,\non the contrary, people do use aggressive color jittering leading\nto unnatural augmented images. We notice in experiments\nthat this prevents neural networks from taking arbitrary values\noutside the support of the data. On the other hand, graph\nLaplacian can exhibit high values outside the manifold, making\nit vulnerable to distribution shift or adversarial attacks [27].7\nIn high dimensional input space, the Dirichlet energy\nmethod (5) is supposed to exhibit much better statistical prop-\nerties [23]. In practice, however, it suﬀers from some com-\nputational drawbacks. More speciﬁcally, for neural networks\nwith two hidden layers with both one hundred neurons, graph\nLaplacian and SSL ﬁnd similar solutions as the one in Figure 1\nwhile the Dirichlet energy method tends to collapse to basic\northogonal functions such as 𝜑𝑖= cos(2𝜋𝜔𝑖⟨𝑒𝑖, 𝑥⟩) for some\nsmall 𝜔and some unit vector 𝑒𝑖. This behavior vanishes with\ndeeper networks.\nFinally, when 𝑝gets big, the diﬀerent functions 𝜑𝑖learned\nare hard to parse visually. While a solution for 𝜑are waves with\nincreasing modes, in practice the networks learn an orthogonal\ntransformation of it, i.e. 𝜑←𝑈𝜑for 𝑈∈R𝑝×𝑝a random\northogonal matrix. If those diﬀerent modes were to correspond\nto features in the original data, it would be natural to ask for (𝜑𝑖)\nto describe those local regions of the input space associated\nwith features. This suggests room for future improvements of\nSSL methods.\n5. CONCLUSION\nThis paper unveiled the link between novel self-supervised\nlearning techniques and classical unsupervised learning ones.\nKey to all those methods is the low-variation hypothesis. In\nfuture work, we hope to leverage this understanding to provide\npractical guidelines to design self-supervised learning algo-\nrithms and deploy them in the wild without having to rely on\nexpensive hyperparameters validation.\n7Additionally, rote that SSL gets fresh samples for each 𝜉𝑖at each opti-\nmization epoch, which reduces in-samples bias.\n6. REFERENCES\n[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova, “BERT: Pre-training of deep bidirectional\ntransformers for language understanding,” in NAACL,\n2019.\n[2] Aakanksha Chowdhery et al., “PaLM: Scaling language\nmodeling with Pathways,” 2022.\n[3] Tom Brown et al.,\n“Language models are few-shot\nlearners,” 2020.\n[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoﬀrey Hinton, “A simple framework for contrastive\nlearning of visual representations,” in ICML, 2020.\n[5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,\nPiotr Bojanowski, and Armand Joulin, “Unsupervised\nLearning of Visual Features by Contrasting Cluster As-\nsignments,” in NeuRIPS, 2020.\n[6] Alec Radford, “Robust speech recognition via large-scale\nweak supervision,” 2022.\n[7] JeﬀZ. HaoChen, Colin Wei, Adrien Gaidon, and Tengyu\nMa, “Provable guarantees for self-supervised deep learn-\ning with spectral contrastive loss,” in NeurIPS, 2021.\n[8] Randall Balestriero and Yann LeCun, “Contrastive and\nnon-contrastive self-supervised learning recover global\nand local spectral embedding methods,” in NeurIPS,\n2022.\n[9] Vladimir Vapnik, The Nature of Statistical Learning,\nSpringer, 1995.\n[10] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and\nStephane Deny, “Barlow Twins: Self-supervised learning\nvia redundancy reduction,” in ICML, 2021.\n[11] Adrien Bardes, Jean Ponce, and Yann Lecun, “VICReg:\nVariance-invariance-covariance regularization for self-\nsupervised learning,” in ICLR, 2022.\n[12] Philippe Rigollet, “Generalization error bounds in semi-\nsupervised classiﬁcation under the cluster assumption,”\nJMLR, 2007.\n[13] Jesper van Engelen and Holger Hoos,\n“A survey of\nsemi-supervised learning,” Machine Learning, 2020.\n[14] Peter Bartlett, Michael Jordan, and Jon Mcauliﬀe, “Con-\nvexity, classiﬁcation, and risk bounds,” JASA, 2006.\n[15] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani,\n“Manifold regularization: A geometric framework for\nlearning from labeled and unlabeled examples,” JMLR,\n2006.\n[16] Ronald Coifman and Stéphane Lafon, “Diﬀusion maps,”\nApplied and Computational Harmonic Analysis, 2006.\n[17] Xiaojin Zhu, Zoubin Ghahramani, and John Laﬀerty,\n“Semi-supervised learning using Gaussian ﬁelds and har-\nmonic functions,” in ICML, 2003.\n[18] Dominique Bakry, Ivan Gentil, Michel Ledoux, et al.,\nAnalysis and geometry of Markov diﬀusion operators, vol.\n103, Springer, 2014.\n[19] Mikhail Belkin and Partha Niyogi, “Laplacian eigenmaps\nfor dimensionality reduction and data representation,”\nNeural Comp., 2003.\n[20] Ulrike von Luxburg, Mikhail Belkin, and Olivier Bous-\nquet, “Consistency of spectral clustering,” Annals of\nStat., 2008.\n[21] Matthias Hein, Jean-Yves Audibert, and Ulrike von\nLuxburg, “Graph Laplacians and their convergence on\nrandom neighborhood graphs,” JMLR, 2007.\n[22] Yoshua Bengio, Olivier Delalleau, and Nicolas Le\nRoux, “Label propagation and quadratic criterion,” Semi-\nSupervised Learning, 2006.\n[23] Vivien Cabannes, Loucas Pillaud-Vivien, Francis Bach,\nand Alessandro Rudi,\n“Overcoming the curse of di-\nmensionality with Laplacian regularization in semi-\nsupervised learning,” in NeurIPS, 2021.\n[24] Harris Drucker and Yann Le Cun, “Improving generaliza-\ntion performance using double backpropagation,” IEEE\ntransactions on neural networks, 1992.\n[25] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vin-\ncent Dumoulin, and Aaron Courville, “Improved training\nof Wasserstein GANs,” in NeuRIPS, 2017.\n[26] Alberto Bietti, Grégoire Mialon, Dexiong Chen, and\nJulien Mairal, “A kernel perspective for regularizing\ndeep neural networks,” in ICML, 2019.\n[27] Judy Hoﬀman, Daniel A. Roberts, and Sho Yaida, “Ro-\nbust learning with Jacobian regularization,” 2019.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML",
    "68Q32",
    "G.3"
  ],
  "published": "2022-11-07",
  "updated": "2022-11-07"
}