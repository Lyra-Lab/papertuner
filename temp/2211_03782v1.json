{
  "id": "http://arxiv.org/abs/2211.03782v1",
  "title": "On minimal variations for unsupervised representation learning",
  "authors": [
    "Vivien Cabannes",
    "Alberto Bietti",
    "Randall Balestriero"
  ],
  "abstract": "Unsupervised representation learning aims at describing raw data efficiently\nto solve various downstream tasks. It has been approached with many techniques,\nsuch as manifold learning, diffusion maps, or more recently self-supervised\nlearning. Those techniques are arguably all based on the underlying assumption\nthat target functions, associated with future downstream tasks, have low\nvariations in densely populated regions of the input space. Unveiling minimal\nvariations as a guiding principle behind unsupervised representation learning\npaves the way to better practical guidelines for self-supervised learning\nalgorithms.",
  "text": "ON MINIMAL VARIATIONS FOR UNSUPERVISED REPRESENTATION LEARNING\nVivien Cabannes\nAlberto Bietti\nRandall Balestriero\nMeta AI\nABSTRACT\nUnsupervised representation learning aims at describing raw\ndata eï¬ƒciently to solve various downstream tasks.\nIt has\nbeen approached with many techniques, such as manifold\nlearning, diï¬€usion maps, or more recently self-supervised\nlearning. Those techniques are arguably all based on the under-\nlying assumption that target functions, associated with future\ndownstream tasks, have low variations in densely populated\nregions of the input space. Unveiling minimal variations as a\nguiding principle behind unsupervised representation learning\npaves the way to better practical guidelines for self-supervised\nlearning algorithms.\nIndex Termsâ€” Self-supervised learning, unsupervised\nlearning, minimal variations, ï¬rst principles.\n1. INTRODUCTION\nData is everywhere, but it is often too unstructured or high\ndimensional to leverage classical statistics on their raw form.\nRecent advances in machine learning have succeeded in exploit-\ning parts of the millions terabytes of unlabeled data contained\non the internet. This was achieved by creating self-supervised\ntasks to be solved by the machine, inciting it to learn good\nrepresentations of text [1, 2]. Those â€œfoundationalâ€ representa-\ntions are now being leveraged to solve several â€œdownstreamâ€\ntasks on languages [3]. Similar developments have been made\non other high-dimensional data such as images, videos or audio\nspeeches [4, 5, 6]. Despite their rapid progress, the training\nof self-supervised learning (SSL) models remains challenging\nand lacks theoretical foundations.\nLearning without supervision has been historically referred\nto as unsupervised learning. While at ï¬rst sights, the literature\nbodies on unsupervised learning and self-supervised learning\nseem relatively disjoint, connections have been made between\nthe two [7, 8]. This work provides further insights on their\nlinks through the concept of minimal variations, detailed in\nSection 3. Theory is veriï¬ed on synthetic experiments in\nSection 4. This understanding could be leveraged in future\nwork to improve SSL algorithms in practical settings with\nlimited compute resources.\n2. SETTING AND CONTEXT\nIn the following, X shall be a Hilbert space (i.e. endowed with\na scalar product) and Y an output space. A distribution ğœŒğ‘‹is\nassumed to have generated a dataset (ğ‘‹ğ‘–)ğ‘–âˆˆ[ğ‘›] of independent\nsamples ğ‘‹ğ‘–âˆ¼ğœŒğ‘‹for ğ‘–âˆˆ[ğ‘›].1 Our goal is to ï¬nd a repre-\nsentation ğœ‘: X â†’Rğ‘for a small ğ‘, such that for relevant\ndownstream distributions ğœŒon pairs of input/output and loss\nfunctions â„“: Y Ã— Y â†’R, one can eï¬ƒciently minimize the\nsubsequent population risk\nR( ğ‘“; ğ‘, â„“) = E(ğ‘‹,ğ‘Œ)âˆ¼ğœŒ[â„“( ğ‘“(ğ‘‹),ğ‘Œ)] ,\n(1)\nbased on i.i.d. samples (ğ‘‹ğ‘–,ğ‘Œğ‘–). More exactly, the optimal\nfunctions ğ‘“should easily be approached under the form ğ‘“=\nğ‘”â—¦ğœ‘for ğ‘”in a small class of functions. Typically, ğ‘”would be\na linear function, a.k.a. a linear probe. Reducing the search of\nğ‘“: X â†’Y in a potentially big function space to the search of\nğ‘”: Rğ‘â†’Y in a much smaller one, will drastically improve\nsample eï¬ƒciency [9].\nTo learn the representation ğœ‘, SSL leverages augmenta-\ntions of data. It deï¬nes ğ‘¡: X Ã— Î â†’X a transformation\nparameterized by ğœ‰âˆˆÎ. For example, Î could be X and\nğ‘¡(ğ‘‹, ğœ‰) = ğ‘‹+ ğœ‰. Assuming that transformations ğ‘¡(ğ‘‹, ğœ‰) do not\nfundamentally change the semantic of the input, any pairs of\naugmented and original points should be close in the features\nspace ğœ‘(X). This is put in equations through the minimization\nof the variational quantity\nEğ‘‹,ğœ‰[ğ‘‘(ğœ‘(ğ‘¡(ğ‘‹, ğœ‰)), ğœ‘(ğ‘‹))] ,\n(2)\nfor ğ‘‘a notion of similarity in Rğ‘, e.g. the square loss ğ‘‘(ğ‘¥, ğ‘¥â€²) =\nâˆ¥ğ‘¥âˆ’ğ‘¥â€²âˆ¥2. In practice, ğœ‘is often taken as a neural network,\nand its learning is conducted through the optimization of its\nparameters. Equation (2) is trivially minimized by setting ğœ‘\nto a constant. To avoid such a â€œcollapseâ€ phenomenon, one\nshould encourage diversity in the representation, for instance\nby using the constraint\nEğ‘‹[ğœ‘(ğ‘‹)ğœ‘(ğ‘‹)âŠ¤] = ğ¼.\n(3)\nClassical self-supervised techniques such as SimCLR [4],\nBarlow Twins [10] and VICReg [11] can be understood as\nimplementing diï¬€erent speciï¬cations of such a scheme [8]\n(respectively, ğ‘‘would be the cosine similarity, some cross-\ncorrelation measure, and the square hinge loss).\n1The set {1, Â· Â· Â· , ğ‘›} is denoted [ğ‘›].\narXiv:2211.03782v1  [cs.LG]  7 Nov 2022\n3. MINIMAL VARIATIONS\n3.1. Classical hypothesis\nThis section reviews classical assumptions about the nature\nof downstream tasks with respect to the input distribution ğœŒğ‘‹.\nIt shows how those assumptions are related to the idea that\nfuture target functions have low variations on highly populated\nregions of the input space.\nOften praised in semi-supervised learning setting, the\ncluster assumption states that the support of ğœŒğ‘‹have several\nconnected components and that downstream classiï¬cation tasks\nare likely to respect this structure, i.e. labels shall be constant\nover each connected component a.k.a. cluster [12]. In other\nterms, one expects the decision boundary2 between classes\nto be situated in regions of the input space where there is no\ndensity. Yet, on big or poorly curated datasets, classes might\nnot be separated by no-density regions. In such a setting, the\ncluster assumption is relaxed as the low-density separation\nhypothesis, assuming that downstream decision boundaries\nwill fall in low-density regions (i.e. where ğœŒğ‘‹is small). For\nexample, in a balanced binary classiï¬cation problem where\nY = {âˆ’1, 1} and dğœŒ(ğ‘¥| ğ‘¦) âˆexp(âˆ’âˆ¥ğ‘¥+ ğ‘¦ğœ‡âˆ¥/ğœ2) dğ‘¥, the\noptimal decision boundary is the hyperplane ğœ‡âŠ¥which does\nminimize the value of ğœŒğ‘‹(ğ´) for any hyperplane ğ´that cross\n[âˆ’ğœ‡, +ğœ‡].\nIn regression settings, it is often assumed that the down-\nstream target functions will be smooth on densely populated\nregions of the input space [13]. Variations of functions are\nmeasured through quantities such as\nJ ( ğ‘“) = E [âˆ¥âˆ‡ğ‘“(ğ‘‹)âˆ¥ğ‘] ,\n(4)\nfor ğ‘= 1 (total variation), ğ‘= 2 (Dirichlet energy) or higher\n(ğ‘-Laplacian). Interestingly, binary classiï¬cation problems\nare often approached through the learning of a continuous\nsurrogate function ğ‘“whose sign is taken as the classiï¬cation\nrule [14]. From a classiï¬cation perspective, variations of ğ‘“\nare only needed in order for ğ‘“to change sign, and the low-\nvariation hypothesis states that those variations should take\nplace in sparsely populated areas of the input space. This is\ncoherent with the low-density separation hypothesis stating\nthat ğ‘“should change sign in sparsely populated regions.\n3.2. Embedding for minimal variations\nThis section extends on classical unsupervised techniques as\naiming to minimize the criterion (4) under the orthonormal\nconstraint (3).\nAssuming low-variation of downstream tasks, it is natural\nto design ğœ‘in order to represent a maximum number of low-\nvariation functions as ğ‘”â—¦ğœ‘. Considering linear probes, the\n2For a predictor ğ‘“: X â†’Y, the input space X is partitioned into decision\nregions Xğ‘¦= {ğ‘¥| ğ‘“(ğ‘¥) = ğ‘¦} indexed by ğ‘¦âˆˆY, â€œdecision boundariesâ€ refers\nto the boundaries of those regions.\nspan of (ğœ‘ğ‘–)ğ‘–âˆˆ[ğ‘] could be searched as a ğ‘-dimension space of\nfunctions with minimal variations according to the criterion (4).\nPut in equations with ğœ‘: X â†’Rğ‘, this reads\narg min\nğœ‘;s.t. (3)\nmax\nğ‘¤âˆˆRğ‘;âˆ¥ğ‘¤âˆ¥=1 J (ğ‘¤âŠ¤ğœ‘(Â·)),\nunder the coverage constraint (3). Such a ğœ‘is an ideal data\nrepresentation to solve downstream tasks with linear probes, as\nlong as solutions verify the low-variation hypothesis. With ğ·\ndenoting the Jacobian, the formulation with ğ‘= 2 translates as\nğœ‘= arg min\nğœ‘s.t. (3)\nE\n\u0002\nâˆ¥ğ·ğœ‘(ğ‘‹)âˆ¥2\nğ¹\n\u0003\n.\n(5)\nIn practice, this formulation is favored for analytical reasons.\nBy making J ( ğ‘“) a quadratic form, it reveals the operator\nL that represents it.3\nIn particular, equation (5) is solved\nexplicitly with ğœ‘ğ‘–the ğ‘–-th eigenfunctions of L.4 The proof is a\nsimple application of the Rayleigh-Ritz formula, that deï¬nes\neigenfunctions recursively through the formula\nğœ‘ğ‘–= arg min\nğœ‘:Xâ†’R\nâŸ¨ğœ‘, Lğœ‘âŸ©= E\n\u0002\nâˆ¥âˆ‡ğœ‘(ğ‘‹)âˆ¥2\u0003\ns.t. E\n\u0002\nğœ‘ğ‘–(ğ‘‹)ğœ‘ğ‘—(ğ‘‹)\n\u0003\n= ğ›¿ğ‘–ğ‘—\nâˆ€ğ‘—< ğ‘–.\nIn the literature, this approach is often referred to as\nspectral embedding (the space is embedded through the spectral\ndecomposition of the operator). It is particularly well suited\nfor the cluster assumption, since the null space of L is nothing\nbut the span of the indicator functions of each connected\ncomponent of ğœŒğ‘‹, which has motivated its use for clustering\nand manifold regularization [15]. Under mild assumptions,\nL is indeed a diï¬€usion operator (when ğœŒğ‘‹has a density and\ncompact support L ğ‘“= âˆ’Î” ğ‘“+ âŸ¨âˆ‡log ğœŒğ‘‹, âˆ‡ğ‘“âŸ©), which links it\nto diï¬€usion maps [16], label propagation [17] and Langevin\ndynamics [18].\nSince the 2000s, the criterion (4) has been approached in a\nnon-parametric fashion based on ï¬nite diï¬€erences, leveraging\ngraph Laplacians [17, 19]. Based on samples (ğ‘‹ğ‘–)ğ‘–âˆˆ[ğ‘›], it aims\nat minimizing\nEğ‘”(ğœ‘) =\nâˆ‘ï¸\nğ‘–,ğ‘—âˆˆ[ğ‘›]\nh\nğ‘˜ğœ(ğ‘‹ğ‘–, ğ‘‹ğ‘—)\n\r\rğœ‘(ğ‘‹ğ‘–) âˆ’ğœ‘(ğ‘‹ğ‘—)\n\r\r2i\n,\n(6)\nwith ğ‘˜a notion of similarity to perform ï¬nite diï¬€erences and ğœ\na scaling parameter (e.g. ğ‘˜ğœ(ğ‘¥, ğ‘¥â€²) = exp(âˆ’âˆ¥ğ‘¥âˆ’ğ‘¥â€²âˆ¥2 /ğœ2)),\nand subject to the empirical version of the constraint (3),\n1\nğ‘›2\nâˆ‘ï¸\nğ‘–,ğ‘—âˆˆ[ğ‘›]\nğœ‘(ğ‘‹ğ‘–)ğœ‘(ğ‘‹ğ‘–)âŠ¤= ğ¼.\n3Some minor mathematical precautions should be taken to deal with this\nweighted Sobolev pseudo-norm, we will omit them in this paper.\n4The solution of ğœ‘is unique up to orthonormal transformations ğ‘ˆğœ‘for\nğ‘ˆâˆˆRğ‘Ã—ğ‘orthogonal, and to permutation of eigenfunctions associated with\nthe ğ‘-th eigenvalue of L.\n3.3. Consistency results\nThis section discusses limiting behaviors of the methods de-\nscribed previously, namely SSL (2), graph Laplacian (6) and\nDirichlet energy (5).\nGraph Laplacians have arguably two convergence proper-\nties. On the one hand, keeping the scale ğœconstant, as the\nnumber of samples ğ‘›goes to inï¬nity, the empirical minimizer\nminimizes the following measure of variations\nE\n\u0002\nâˆ¥ğ‘‘(ğ‘‹) ğ‘“(ğ‘‹) âˆ’ğ‘˜ğœâˆ—ğ‘“(ğ‘‹)âˆ¥2\u0003\n, with\nğ‘˜ğœâˆ—ğ‘“(ğ‘¥) = E[ğ‘˜ğœ(ğ‘¥, ğ‘‹) ğ‘“(ğ‘‹)],\nğ‘‘= ğ‘˜ğœâˆ—1,\nwhich can be seen as a smoothed, reweighted version of the\nDirichlet energy. The convergence happens relatively fast,\ntypically in ğ‘‚(ğ‘›âˆ’1/2) in ğ¿2-norm [20]. On the other hand, with\nthe right scaling of ğœ, this ï¬nite diï¬€erence method is able to\nconverge towards the ideal solution deï¬ned by (5). Yet the\nconvergence rates are much worse, e.g. in ğ‘‚(ğ‘›âˆ’1/ğ‘‘) for ğ‘‘the\ndimension of the data manifold (ğ‘‘= dim supp ğœŒğ‘‹) [21]. This\nmay be understood intuitively, to measure variations with ï¬nite\ndiï¬€erences, the number of points needed grows exponentially\nwith dimension [22].\nAlternatively, (4) might be estimated directly with empirical\nsamples and a parametric model such as neural networks or\nkernel methods. This enables fast convergence towards the\nsolution of (5) within the search space of functions for ğœ‘. By\nnot suï¬€ering from the curse of dimension and converging to\nthe ideal operator, this approach is statistically superior [23].\nYet, it requires optimization over derivatives which can lead to\ncomputational drawbacks.\n3.4. Insights for SSL\nWe argue that SSL objectives such as (2) can be seen as\nmeasures of variations. In particular, since ğ‘¡(ğ‘¥, ğœ‰) is supposed\nto be closed to ğ‘¥(at least semantically speaking), it behaves\nas a random variable (with respect to ğœ‰) to compute ï¬nite\ndiï¬€erences at a point ğ‘¥âˆˆX.\nTherefore, we expect SSL\nalgorithms either, when keeping the scale of ğœ‰constant,5 to\nconverge fast to the minimizer of some smoothed version of\na functional that measure variations (4) (depending on ğ‘¡and\nthe distance ğ‘‘), or, with the right decreasing scale, to converge\nslowly to the ideal functional itself.\n4. EXPERIMENTS\nPrior sections have introduced three techniques to learn ğœ‘, SSL\n(2), graph Laplacian (6) and empirical Dirichlet energy (5).\nWe have argued that they all aimed at learning the same type of\nfunctions, i.e. orthogonal functions that minimize variations.\nAfter implementation details, proof-of-concept experiments\nverify this claim.\n5Here, Î is implicitly assumed to be a Banach space and the transformation\nto verify ğ‘¡(ğ‘¥+ ğœ‰) = ğ‘¥+ ğ‘œ( âˆ¥ğœ‰âˆ¥).\n4.1. Implementation details\nThis section reviews implementation details based on empirical\nsamples (ğ‘‹ğ‘–)ğ‘–âˆˆ[ğ‘›]. Experiments were made with the following\nspeciï¬cation of the self-supervised learning objective\nEğ‘ (ğœ‘) = 1\nğ‘›\nâˆ‘ï¸\nğ‘–âˆˆ[ğ‘›]\nâˆ¥ğœ‘(ğ‘‹ğ‘–) âˆ’ğœ‘(ğ‘‹ğ‘–+ ğœğœ‰ğ‘–)âˆ¥2 ,\n(2)\nwith ğœ‰ğ‘–a random unit Gaussian variable, and ğœa scale param-\neter. The empirical version of Dirichlet energy reads\nEğ‘’(ğœ‘) = 1\nğ‘›\nâˆ‘ï¸\nğ‘–âˆˆ[ğ‘›]\nâˆ¥âˆ‡ğœ‘(ğ‘‹ğ‘–)âˆ¥2 ,\n(5)\nwhich, in the case of deep networks, is related to double\nbackpropagation [24] and has been used in other contexts [25,\n26]. Finally, the graph Laplacian objective is nothing but (6).\nIn experiments, the orthogonality constraints was relaxed\nas a penalty reading\nÎ©(ğœ‘) =\n\r\r 1\nğ‘›2\nâˆ‘ï¸\nğ‘–âˆˆ[ğ‘›]\nğœ‘(ğ‘‹ğ‘–)ğœ‘(ğ‘‹ğ‘–)âŠ¤âˆ’ğ¼\n\r\r2,\nwhile the ï¬nal objective is E(ğœ‘) + ğœ†Î©(ğœ‘).\nSelf-supervised learning is known to be quite unstable\nto changes in hyperparameters. In experiments, the scale\nparameters (standard deviation of augmentation in SSL, and\nkernel scaling in graph Laplacian) were set to match the\nwidth of the half-moon dataset (which was itself generated\nwith Gaussian noise). Stochastic gradient descent parameters\n(learning rate scheduling, batch size) were tuned to succeed\nthe sole minimization of Î©.6 Finally, the regularizer ğœ†was\nset to approximately balance the penalty and the objective at\nhand (the learning rate was divided by ğœ†accordingly). The\nrepresentation ğœ‘was parameterized with a fully connected\nneural network with ï¬ve hidden layers, each containing a\nhundred neurons.\nThe code is available online at https:\n//github.com/VivienCabannes/laplacian.\n4.2. Consistency results.\nThis section checks the claim that the three objectives (2), (5)\nand (5) are learning similar functions. It proceeds with the two\nhalf-moons dataset (Figure 1).\nIn this setting, the eigenfunctions of L are related to the\nFourier basis on the union of two segments and are relatively\nstable under smoothing of the diï¬€erential functional. Beside\nthe constant function, the null space of L is made of ğœ‘1 the\ndiï¬€erence of the indicator functions of both half-moons. The\nsecond two eigenfunctions ğœ‘2 and ğœ‘3 are ï¬rst-mode waves on\neach component. Figure 1 reports the learned ğœ‘for the three\n6Note that because the expectation is inside the norm in Î© a naive mini-\nbatch strategy does not provide unbiased stochastic estimate of its gradient.\nWe overcame this issue by considering large batches.\nFig. 1: Functions learned with ğ‘= 2 for SSL (2) on the right, graph Laplacian (6) in the middle, and Dirichlet energy (5) on the right.\nDatapoints (ğ‘‹ğ‘–)ğ‘–âˆˆ[ğ‘›] are represented as black dots, while the functions ğœ‘ğ‘–(ğ‘‹) are represented through the level regions in diï¬€erent colors.\nmethods with ğ‘= 2. All methods recover ğœ‘1 (top), the ï¬rst two\nrecover ğœ‘2 while the third one recovers a mixture cos(ğœƒ)ğœ‘2 +\nsin(ğœƒ)ğœ‘3 for some ğœƒâˆˆ[0, 2ğœ‹], which also minimizes (5).\nTable 1 is concerned with ğ‘= 5, and the downstream task\nthat consists in predicting if ğ‘¥was in the top (or bottom) of\nthe left (or right) half-moon. This generates a classiï¬cation\nproblem with four diï¬€erent classes. Such a task is ideal to\nevaluate our argumentation since the ï¬rst ï¬ve eigenfunctions\nof the diï¬€usion operator L discriminate those four parts of the\nspace with linear probing. The results are satisfying.\nSSL (2)\nenergy (5)\ngraph (6)\n96.14 Â± 0.16\n97.32 Â± 0.16\n95.15 Â± 0.19\nTable 1: Accuracy on downstream task with linear probing to\ncheck eigenspace retrieval (random is 0.25).\n4.3. Discussion\nWhile the previous experiments are made on small synthetic\ndata, some behaviors are worse mentioning. First, the SSL\nobjective (2) and the graph Laplacian (6) lead to similar results.\nYet, graph Laplacian only uses samples on the data manifold,\nwhile augmented data in SSL gets out of it. At ï¬rst sight, it\nseems better to restrict computations of ï¬nite diï¬€erences to the\nmanifold: the method would scale with the intrinsic dimension\nof data instead of the explicit input dimension [21]. In practice,\non the contrary, people do use aggressive color jittering leading\nto unnatural augmented images. We notice in experiments\nthat this prevents neural networks from taking arbitrary values\noutside the support of the data. On the other hand, graph\nLaplacian can exhibit high values outside the manifold, making\nit vulnerable to distribution shift or adversarial attacks [27].7\nIn high dimensional input space, the Dirichlet energy\nmethod (5) is supposed to exhibit much better statistical prop-\nerties [23]. In practice, however, it suï¬€ers from some com-\nputational drawbacks. More speciï¬cally, for neural networks\nwith two hidden layers with both one hundred neurons, graph\nLaplacian and SSL ï¬nd similar solutions as the one in Figure 1\nwhile the Dirichlet energy method tends to collapse to basic\northogonal functions such as ğœ‘ğ‘–= cos(2ğœ‹ğœ”ğ‘–âŸ¨ğ‘’ğ‘–, ğ‘¥âŸ©) for some\nsmall ğœ”and some unit vector ğ‘’ğ‘–. This behavior vanishes with\ndeeper networks.\nFinally, when ğ‘gets big, the diï¬€erent functions ğœ‘ğ‘–learned\nare hard to parse visually. While a solution for ğœ‘are waves with\nincreasing modes, in practice the networks learn an orthogonal\ntransformation of it, i.e. ğœ‘â†ğ‘ˆğœ‘for ğ‘ˆâˆˆRğ‘Ã—ğ‘a random\northogonal matrix. If those diï¬€erent modes were to correspond\nto features in the original data, it would be natural to ask for (ğœ‘ğ‘–)\nto describe those local regions of the input space associated\nwith features. This suggests room for future improvements of\nSSL methods.\n5. CONCLUSION\nThis paper unveiled the link between novel self-supervised\nlearning techniques and classical unsupervised learning ones.\nKey to all those methods is the low-variation hypothesis. In\nfuture work, we hope to leverage this understanding to provide\npractical guidelines to design self-supervised learning algo-\nrithms and deploy them in the wild without having to rely on\nexpensive hyperparameters validation.\n7Additionally, rote that SSL gets fresh samples for each ğœ‰ğ‘–at each opti-\nmization epoch, which reduces in-samples bias.\n6. REFERENCES\n[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova, â€œBERT: Pre-training of deep bidirectional\ntransformers for language understanding,â€ in NAACL,\n2019.\n[2] Aakanksha Chowdhery et al., â€œPaLM: Scaling language\nmodeling with Pathways,â€ 2022.\n[3] Tom Brown et al.,\nâ€œLanguage models are few-shot\nlearners,â€ 2020.\n[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoï¬€rey Hinton, â€œA simple framework for contrastive\nlearning of visual representations,â€ in ICML, 2020.\n[5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,\nPiotr Bojanowski, and Armand Joulin, â€œUnsupervised\nLearning of Visual Features by Contrasting Cluster As-\nsignments,â€ in NeuRIPS, 2020.\n[6] Alec Radford, â€œRobust speech recognition via large-scale\nweak supervision,â€ 2022.\n[7] Jeï¬€Z. HaoChen, Colin Wei, Adrien Gaidon, and Tengyu\nMa, â€œProvable guarantees for self-supervised deep learn-\ning with spectral contrastive loss,â€ in NeurIPS, 2021.\n[8] Randall Balestriero and Yann LeCun, â€œContrastive and\nnon-contrastive self-supervised learning recover global\nand local spectral embedding methods,â€ in NeurIPS,\n2022.\n[9] Vladimir Vapnik, The Nature of Statistical Learning,\nSpringer, 1995.\n[10] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and\nStephane Deny, â€œBarlow Twins: Self-supervised learning\nvia redundancy reduction,â€ in ICML, 2021.\n[11] Adrien Bardes, Jean Ponce, and Yann Lecun, â€œVICReg:\nVariance-invariance-covariance regularization for self-\nsupervised learning,â€ in ICLR, 2022.\n[12] Philippe Rigollet, â€œGeneralization error bounds in semi-\nsupervised classiï¬cation under the cluster assumption,â€\nJMLR, 2007.\n[13] Jesper van Engelen and Holger Hoos,\nâ€œA survey of\nsemi-supervised learning,â€ Machine Learning, 2020.\n[14] Peter Bartlett, Michael Jordan, and Jon Mcauliï¬€e, â€œCon-\nvexity, classiï¬cation, and risk bounds,â€ JASA, 2006.\n[15] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani,\nâ€œManifold regularization: A geometric framework for\nlearning from labeled and unlabeled examples,â€ JMLR,\n2006.\n[16] Ronald Coifman and StÃ©phane Lafon, â€œDiï¬€usion maps,â€\nApplied and Computational Harmonic Analysis, 2006.\n[17] Xiaojin Zhu, Zoubin Ghahramani, and John Laï¬€erty,\nâ€œSemi-supervised learning using Gaussian ï¬elds and har-\nmonic functions,â€ in ICML, 2003.\n[18] Dominique Bakry, Ivan Gentil, Michel Ledoux, et al.,\nAnalysis and geometry of Markov diï¬€usion operators, vol.\n103, Springer, 2014.\n[19] Mikhail Belkin and Partha Niyogi, â€œLaplacian eigenmaps\nfor dimensionality reduction and data representation,â€\nNeural Comp., 2003.\n[20] Ulrike von Luxburg, Mikhail Belkin, and Olivier Bous-\nquet, â€œConsistency of spectral clustering,â€ Annals of\nStat., 2008.\n[21] Matthias Hein, Jean-Yves Audibert, and Ulrike von\nLuxburg, â€œGraph Laplacians and their convergence on\nrandom neighborhood graphs,â€ JMLR, 2007.\n[22] Yoshua Bengio, Olivier Delalleau, and Nicolas Le\nRoux, â€œLabel propagation and quadratic criterion,â€ Semi-\nSupervised Learning, 2006.\n[23] Vivien Cabannes, Loucas Pillaud-Vivien, Francis Bach,\nand Alessandro Rudi,\nâ€œOvercoming the curse of di-\nmensionality with Laplacian regularization in semi-\nsupervised learning,â€ in NeurIPS, 2021.\n[24] Harris Drucker and Yann Le Cun, â€œImproving generaliza-\ntion performance using double backpropagation,â€ IEEE\ntransactions on neural networks, 1992.\n[25] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vin-\ncent Dumoulin, and Aaron Courville, â€œImproved training\nof Wasserstein GANs,â€ in NeuRIPS, 2017.\n[26] Alberto Bietti, GrÃ©goire Mialon, Dexiong Chen, and\nJulien Mairal, â€œA kernel perspective for regularizing\ndeep neural networks,â€ in ICML, 2019.\n[27] Judy Hoï¬€man, Daniel A. Roberts, and Sho Yaida, â€œRo-\nbust learning with Jacobian regularization,â€ 2019.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML",
    "68Q32",
    "G.3"
  ],
  "published": "2022-11-07",
  "updated": "2022-11-07"
}