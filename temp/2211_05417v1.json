{
  "id": "http://arxiv.org/abs/2211.05417v1",
  "title": "Can Transformers Reason in Fragments of Natural Language?",
  "authors": [
    "Viktor Schlegel",
    "Kamen V. Pavlov",
    "Ian Pratt-Hartmann"
  ],
  "abstract": "State-of-the-art deep-learning-based approaches to Natural Language\nProcessing (NLP) are credited with various capabilities that involve reasoning\nwith natural language texts. In this paper we carry out a large-scale empirical\nstudy investigating the detection of formally valid inferences in controlled\nfragments of natural language for which the satisfiability problem becomes\nincreasingly complex. We find that, while transformer-based language models\nperform surprisingly well in these scenarios, a deeper analysis re-veals that\nthey appear to overfit to superficial patterns in the data rather than\nacquiring the logical principles governing the reasoning in these fragments.",
  "text": "arXiv:2211.05417v1  [cs.CL]  10 Nov 2022\nCan Transformers Reason in Fragments of Natural Language?\nViktor Schlegel 1,2\nviktor_schlegel@asus.com\nKamen V. Pavlov 2\npavloffkamen@gmail.com\nIan Pratt-Hartmann 2,3\nian.pratt@manchester.ac.uk\n1ASUS Intelligent Cloud Services (AICS), Singapore\n2Department of Computer Science, University of Manchester, United Kingdom\n3Instytut Informatyki Uniwersytet Opolski, Poland\nAbstract\nState-of-the-art\ndeep-learning-based\nap-\nproaches to Natural Language Processing\n(NLP) are credited with various capabilities\nthat involve reasoning with natural language\ntexts. In this paper we carry out a large-scale\nempirical study investigating the detection\nof formally valid inferences in controlled\nfragments of natural language for which the\nsatisﬁability problem becomes increasingly\ncomplex.\nWe ﬁnd that, while transformer-\nbased language models perform surprisingly\nwell in these scenarios, a deeper analysis re-\nveals that they appear to overﬁt to superﬁcial\npatterns in the data rather than acquiring the\nlogical principles governing the reasoning in\nthese fragments.\n1\nIntroduction\nThe recent success of neural networks in a range\nof tasks connected with logical inference in natu-\nral language is remarkable. Foremost among such\nsystems are those employing transformer-based\nlanguage models (Vaswani et al., 2017) optimised\non large corpora in an unsupervised manner (De-\nvlin et al., 2019) and then further ﬁne-tuned on\ntask-speciﬁc datasets (Bowman et al., 2015; Ra-\njpurkar et al., 2016).\nHowever, concerns per-\nsist regarding so-called “data-set artefacts” (Guru-\nrangan et al., 2018; Schlegel et al., 2022, 2020).\nHave netural networks really acquired the princi-\nples of reasoning with natural language, or are\nthey merely responding to superﬁcial patterns in\nthe data?\nIn fact, two strands of research may be dis-\ncerned in recent work on natural language infer-\nence (NLI). The ﬁrst formulates the central task\nas follows: given a pair of sentences in some nat-\nural language, a premise, P and a hypothesis, H,\ndetermine whether P (i) entails or (ii) contradicts\n(i.e. entails the negation of) H; or (iii) P is neu-\ntral with respect to H. For example, the premise\nReasoning pattern of interest (e.g. chaining)\n1. Every artist is a beekeeper.\n2. Every beekeeper is a carpenter.\n3. No carpenter is a dentist.\n4. Some artist is a dentist.\nIs this collection of sentences satisﬁable or contradictory?\n(equivalent)\nSentence\ngenerator\nSet of sentences\nSet of formulas\nTheorem\nprover\nLabelled\nsentence set\nTraining\ncorpus\nTest\ncorpus\nModel\nTrain\nTrained\nModel\nEvaluate\nFigure 1: Depiction of our approach to evaluate reason-\ning capabilities in neural networks.\nA person rides his bicycle in the sand beside the\nocean is taken to entail the hypothesis A person is\non a beach. The primary issue is the possibility\nof learning such a mapping on the basis of some\ndata set consisting of many such sentence pairs,\neach tagged with the a ‘correct’ (i.e. gold standard)\nlabel as determined by human judgement, typi-\ncally obtained by crowd-sourcing. Examples in-\nclude the RTE dataset (Bentivogli et al., 2009) and\nthe (much larger) SNLI and MNLI datasets (Bow-\nman et al., 2015; Williams et al., 2018). Neural-\nnetwork models achieve impressive accuracy on\nthis task (Chen et al., 2017; Devlin et al., 2019).\nThe nature of the challenge here is (deliberately)\nmixed: on the one hand, solving the NLI prob-\nlem requires a grasp of the meaning of various\nclosed-class words (a, not, . . . ) together with an\nappreciation of the semantics of the grammatical\nconstructions involved. On the other hand, almost\nall the entailments or contradictions encountered\nrely on common sense knowledge (for example,\nthat racing a bicycle involves riding it), which it\nis the job of the system to acquire. The supposi-\ntion is that these pieces of commonsense knowl-\nedge mediate the entailment of the hypothesis (or\nits negation) from the premise.\nThe logical ba-\nsis of the inferences, so reconstructed, is typically\nstraightforward, amounting to simple syllogism-\nlike inferences; but the mediating commonsense\nknowledge is coded in ‘soft’ form, matching the\napproximate, probabilistic nature of natural lan-\nguage inference.\nThe operative notion of infer-\nence thus eludes a formal deﬁnition (Sugawara\net al., 2018),ultimately resulting in debatable la-\nbels (Schlegel et al., 2020).\nThesecond strand of research investigates the\nability of neural networks to recognize formally\nvalid entailments in sensu stricto, but nevertheless\ncouched in fragments of natural language. Exam-\nples include Salvatore et al. (2019); Richardson\net al. (2019); Richardson and Sabharwal (2021)\nand Talmor et al. (2020).\nHere the focus is on\nlearning the (potentially complex) inferential pat-\nterns inherent in the logical syntax of the closed-\nclass words and grammatical constructions deﬁn-\ning the fragment of language under investigation.\nFor example, the premises Every doctor who is\nnot a philosopher is a baker, John is a doctor and\nJohn is not a baker entail the hypothesis John is\na philosopher. No commonsense knowledge is re-\nquired here: the given premises either entail the\ngiven hypothesis formally, or they do not. Since\nentailment in a formally deﬁned fragment of lan-\nguage is a matter of mathematical fact, not hu-\nman judgement, inferential problems may be con-\nstructed artiﬁcially, using random sampling over a\ngrammar, with the correct answers determined by\nan automated theorem prover (ATP). The question\nis not whether neural networks can be trained to\nmimic human annotators’ judgements, but rather,\nwhether they can learn the logical principles gov-\nerning language in question. Note that, because it\nis formal validity rather than commonsense plausi-\nbility that is at issue here, inference tasks of inter-\nest typically involve not a single premise P, but\nrather, a collection of premises.\nIndividual sen-\ntences seldom yield any non-trivial formal entail-\nments.\nThese two strands of research reﬂect a differ-\nence in motivation.\nThe former aims to under-\nstand inferences apparently performed by humans\nin everyday linguistic settings—inferences which\nare necessarily messy and approximate in charac-\nter.\nFrom a formal point of view, many of the\nalleged entailments or contradictions are in fact\njudgements of probability. Thus, for example the\npremise Alice is holding a dog does not, logically\nspeaking, contradict the hypothesis Alice is hold-\ning a cat, even assuming that no dog is a cat; yet\nsuch labellings abound in NLI datasets, reﬂecting\nthe judgements made by (and instructions given\nto) the human annotators who generate them. By\ncontrast, the second (strictly logical) strand of re-\nsearch is motivated principally by a desire to un-\nderstand the rule-based character of natural lan-\nguage syntax and semantics, and in particular, by\nthe question of whether a neural network can learn\nthe rules in question.\nIn this paper, we report on a large-scale empiri-\ncal study to investigate whether transformer-based\nlanguage models can learn the logical syntax of\nnatural language.\nSpeciﬁcally, we consider per-\nformance on the problem of determining the sat-\nisﬁability (logical consistency) of sets of English\nsentences featuring the determiners every, some,\nno, the negative adverb not, and relative clauses,\nin the context of a vocabulary of count nouns\nand transitive verbs.\nImportantly, the sentences\nin question are drawn from various different frag-\nments of English, each characterized by a particu-\nlar range of the grammatical constructions investi-\ngated. In each case, data sets—both for training\nand evaluation—are artiﬁcially generated in order\nto test the system’s grasp of the underlying logic.\nFigure 1 illustrates the approach, giving an exam-\nple problem instance for the very simplest of the\nfragments considered. We ﬁnd that state-of-the-art\ndeep learning-based approaches achieve impres-\nsive performance on this task. However, in-depth\nanalysis of their generalisation capabilities reveals\na more nuanced pattern, suggesting a tendency to\noverﬁt to the parameters that control the data gen-\neration, rather than learning the underlying logical\nprinciples.\n2\nFragments of Language\nBy a fragment of a natural language, we under-\nstand a collection of sentences forming a naturally\ndelineated subset of that language, and equipped\nwith a truth-conditional semantics commanding\nthe assent of its native speakers. For example, con-\nsider the fragment of natural language correspond-\ning to the classical syllogistic given to us by Aris-\ntotle (1989, Book A). In its English-language ver-\nsion, it consists of the sentence forms\nEvery p is a q\nNo p is a q\nSome p is a q\nSome p is not a q,\nwith schematic variables p and q substituted by\ncommon (count) nouns. This fragment, which we\nhere denote S, can be used to formulate examples\nsuch as the one shown in Figure 1.\nFor present purposes, we may consider the truth\nconditions of an English sentence to be given by\ntranslation to a formal language such as ﬁrst-order\nlogic, in a way which uncontroversially recon-\nstructs the operative notion of logical entailment.\nThus, for example, the sentence forms S corre-\nspond to the respective logical forms ∀x(p(x) →\n±q(x)) and ∃x(p(x) ∧±q(x)), where ± indicates\neither the presence or absence of negation (¬). An\nargument in S is regarded as valid precisely when\nthe ﬁrst-order translations of its premises entail—\nin the familiar logical sense—the ﬁrst-order trans-\nlation of its conclusion.\nConsider now the argument:\nSome artist hates no beekeeper; every bee-\nkeeper hates some artist; therefore some artist\nis not a beekeeper.\n(1)\nThis argument is again intuitively valid (though\nthis takes a little thought to see).\nOn the other\nhand, it cannot be sensibly cast in the syllogistic,\nbecause it so obviously hinges on intrinsically re-\nlational information. (Observe the alternation of\nsubject and object of the verb hates in the two\npremises.) Is there, then, a larger fragment of En-\nglish in which it might be expressed? Take the re-\nlational syllogistic, denoted R, to be the fragment\nof English obtained by adding to S the sentence-\nforms:\nEvery p rs every/some q\nSome p rs every/some q\nNo p rs every/any q\nSome p does not r every/any q,\nwhere p and q are common count nouns and r a\ntransitive verb. As with S, so too with R: the truth-\nconditions of sentences can be captured by transla-\ntion to ﬁrst-order logic in a way which faithfully\nreconstructs the intuitive notion of validity. For\nexample, “Some artist hates no beekeeper” may\nbe rendered as ∃x(artist(x) ∧∀y(beekeeper(y) →\n¬hate(x, y))), and so on. Under these semantics,\nargument (1) is conﬁrmed as a valid argument in\nthe fragment R.\nThere are other ways to extend the fragment S,\nof course. One (very modest) such extension is\nactually featured in the works of Aristotle (1963,\nCh. 10). Let us say that the extended classical syl-\nlogistic, denoted S+, is the fragment of English\nwhich adds to S the sentence-forms\nEvery non-p is a q\nSome non-p is a q\nNo non-p is a q\nSome non-p is not a q,\ncorresponding\nto\nthe\nﬁrst-order\nformulas\n∀x(¬p(x) →±q(x)) and ∃x(¬p(x)∧±q(x)). As\nwe might say, S+ adds ‘noun-level negation’ to S.\nFollowing in the same vein, we can extend R with\nnoun-level negation, yielding sentences such as\n“No non-carpenter admires any non-artist” and so\non. We call this fourth fragment the extended rela-\ntional syllogistic, denoted R+. Again, translation\ninto ﬁrst-order logic is completely standard.\nLet L be a fragment of some natural language.\nA set of L-sentences is said to be satisﬁable if\nthere is a structure making the logical translations\nof these sentences true. The satisﬁability problem\nfor L, denoted Sat(L), is the problem of determin-\ning whether a given ﬁnite set of sentences of L is\nsatisﬁable. Provided L is equipped with a mech-\nanism for sentence negation (as are all the frag-\nments considered here), any procedure for solv-\ning Sat(L) immediately yields a procedure for rec-\nognizing logical entailments in L, since an argu-\nment is valid just in case its premises together\nwith the negation of its conclusion is not satisﬁ-\nable. Therefore, we have no use for the familiar\nclassiﬁcation of natural language inference prob-\nlems as entailment, contradiction and neutral (and\nstill less the four-way classiﬁcation of Tian et al.\n(2021)): the satisﬁability problem is as general as\nwe need. As the fragment L becomes more ex-\npressive, the corresponding satisﬁability problem\nSat(L) will, in general, become more increasingly\ndifﬁcult. However, as we shall see, the details of\nthe resulting trade-off between expressiveness and\nease of inference are rather intricate. The focus of\nthe present paper is whether neural networks can\nlearn to solve problems such as Sat(L) for various\nfragments L such as S, S+, R and R+.\nWe remark that the approach taken here is par-\nallel to that taken with respect to the fragments\nGRL and RCL in (Richardson and Sabharwal,\n2021). There are two notable differences, however,\nFirstly, the fragments considered here are, from a\ngrammatical point of view, more basic, and less\nclearly a natural language version of propositional\nlogic clauses. In particular, GRL includes the ‘sen-\ntence’ If carrot and not steak then apples; and even\nthe more natural RCL is limited to the construc-\ntions Every X who is (not) a Y is (not) a Z. To al-\nlow comparison between this work and the present\nstudy we consider the minimal extension of S by\nmeans of relative clauses, thus allowing the addi-\ntional sentence-forms\nEvery o who is a p is a q\nNo o who is a p is a q\nSome o who is a p is a q\nSome o who is a p is not a q\nDenote this fragment by Sr. Similarly, denote by\nSrn the same fragment additionally allowing nega-\ntive relative clauses, such as “Every o who is not a\np is a q” and so on. The fragment Srn is actually\nan extension of RCL.\nAn important motivation for the rather more\ngeneral approach taken here is that the satisﬁabil-\nity problem Sat(L) for various fragments L of En-\nglish may be studied from a purely complexity-\ntheoretic point of view.\nThus, for example,\nit is known that the problems Sat(S), Sat(S+)\nand Sat(R) are NLOGSPACE-complete, while\nSat(R+) is EXPTIME-complete (Pratt-Hartmann\nand Moss, 2009). Similarly, the satisﬁability prob-\nlems for GRL and RCL are easily seen to be\nNPTIME-complete, as is the problem Sat(Srn);\nby contrast,\nthe problem Sat(Sr) is PTIME-\ncomplete (Pratt-Hartmann, 2014, Theorem 7).\nThe question naturally arises as to whether the abil-\nity of neural networks to learn to solve these var-\nious satisﬁability problems correlates with these\ncomplexity-theoretic differences.\nTable 1shows all templates used to generate the\ndatasets for each of the fragments. Code to gener-\nate the datasets, allowing control of the parameters\ndiscussed above, is included in the supplementary\nmaterial available on github1.\n3\nRandom problems\nIn this section we outline the construction of col-\nlections of sets of formulas in the fragments S+,\nR, R+, Sr and Srn, in which each generated set is\nlabelled as satisﬁable or unsatisﬁable. (The frag-\nment S is not interestingly different from S+, and\nwill not be considered in the experiments reported\nhere.)These collections are partitioned into train-\ning and evaluation sets, enabling us to test the abil-\nity of neural networks to learn to recognise satisﬁ-\nability under a range of conditions.\nAny sentence in S+ translates to a formula hav-\ning either of the forms ∀x(±p(x) →±q(x)) or\n∃x(±p(x) ∧±q(x)). We may thus generate a sen-\ntence of S+ pseudo-randomly by selecting a uni-\nversal sentence with probability pu, a negated sub-\nject with probability p¯s and a negated object with\nprobability p¯o, and then choosing p and q at ran-\ndom from some collection of n nouns. By carry-\ning out this process s times, we obtain a random\n1https://github.com/schlevik/nlr\nset Φ of S+-sentences (|Φ| = s). For deﬁnite-\nness, we set p¯s = p¯o = 0.5 and pu = 0.8. In\naddition, we remove any inconsistent sentences,\nsuch as “Some p is not a p.”\nIn choosing the\nvarious parameters, we ﬁx n/s = 0.8, which, as\nwe have empirically established, keeps the propor-\ntion of satisﬁable instances at roughly 50%. Col-\nlections of such problem instances are created for\nvarious values of s. The ratio n/s = 0.8 corre-\nsponds (for S+) roughly to the critical region of\nSAT problems studied in (Richardson and Sabhar-\nwal, 2021), where it was shown that learning on\ndata from this region is more effective than learn-\ning from uniformly sampled data. However, we\nneed to be wary of assuming that such instances\nare difﬁcult—an issue addressed in Section 4.\nFor R+, we proceed similarly, generating s sen-\ntences at random over a ﬁxed collection of n nouns\nand v transitive verbs.\nEvery sentence in R+\nis either a sentence in S+ or translates to a for-\nmula having one of the forms ∀x(±p(x) →γ) or\n∃x(±p(x) ∧γ), where γ is either ∀y(±q(y) →\n±r(x, y)) or ∃y(±q(y) ∧±r(x, y)).\nCall sen-\ntences in R+ \\ S+ relational sentences. We may\nthus generate a sentence of R+ pseudo-randomly\nby choosing to produce a relational sentence with\nprobability pr.\nIf we choose to produce a non-\nrelational sentence (i.e. a sentence in S+), we pro-\nceed as above; otherwise a negated verb is cho-\nsen with probability p¯v, a universally quantiﬁed γ\nwith probability puu; the other parameters, n, v,\npu, ps and po are interpreted as before. By setting\np¯s = p¯o = 0, we guarantee that every generated\nrelational sentence has a non-negated subject and\na non-negated object, and hence is a sentence of\nR. By repeating this process s times, we obtain\nan set Φ of sentences in R+ (or R). When gener-\nating instances of Sat(R), we thus set pr = 0.2,\np¯s = p¯o = 0, p¯v = 0.5 and pu = puu = 0.8;\nin addition, we ﬁx n/s = 0.6 and v/s = 0.15,\nwhich, as we have empirically established, keeps\nthe ratio of satisﬁable to non-satisﬁable instances\nat roughly 50%. Likewise, when generating in-\nstances of Sat(R+), we set p¯s = p¯v = 0.5, with\nthe other parameters as for R; however, we adjust\nn/s to 0.64 in order to maintain the ratio of satisﬁ-\nable to non-satisﬁable instances.\nFinally, for fragments with relative clauses, Sr\nand Srn, in addition to the parameters introduced\nfor S+, we control the probability of negated rela-\ntive clauses by the parameter p¯r. By setting p¯r = 0\nFrag.\nTemplates\nExample sentence\nS+\nEvery/No (non-)p is a q.\nEvery artist is a beekeeper.\nSome (non-)p is (not) a q.\nSome carpenter is not a dentist.\nR\nall of S and:\nEvery/Some p rs every/some q.\nEvery artist chases some beekeeper.\nSome p does not r every/any q.\nSome beekeeper does not chase any artist.\nNo p rs every/any q.\nNo beekeeper bewitches any artist.\nR+\nall of S+ and:\nEvery/Some (non-)p rs every/some (non-)q.\nEvery non-artist chases some beekeeper.\nSome (non-)p does not r every/any (non-)q.\nSome beekeeper does not chase any non-\nartist.\nNo (non-)p rs every/any (non-) q.\nNo non-beekeeper bewitches any non-artist.\nSr\nEvery/Some/No o who is a p is a q.\nEvery artist who is a dentist is a carpenter.\nSome o who is a p is not a q.\nSome dentist who is a hunter is not a spy.\nSrn\nall of Sr and:\nEvery/Some/No o who is not a p is a q.\nEvery artist who is a not dentist is a carpenter.\nSome o who is a not p is not a q.\nSome dentist who is a not hunter is not a spy.\nTable 1: Templates used to generate the problem instances for all ﬁve fragments. Round brackets () denote option-\nals, forward slashes / denote alternatives.\nwe guarantee that every generated sentence has an\nun-negated relative clause, and hence is a sentence\nof Sr. We additionally set p¯o = 0.5 and pu = 0.8,\nand for Srn, we set p¯r = 0.5. We empirically es-\ntablish that setting (n −0.225)/s = 0.59 yields\na probability of satisﬁability of approximately 0.5\nfor both fragments.\nThe satisﬁability of a generated problem in-\nstance is determined using the Vampire automated\ntheorem prover (ATP) (Kovács and Voronkov,\n2013), which (assuming termination) either re-\nports that the input set is satisﬁable or outputs a\nproof of a contradiction. We record, for each gen-\nerated problem instance, whether it is satisﬁable,\nand, if not, the number l of lines in the discov-\nered proof of a contradiction (proof length) as well\nas the number of input sentences, d, used in that\nproof. The ATP terminated on all generated prob-\nlem instances; there is, however, no general guar-\nantee that the proofs found are the shortest possi-\nble.\nGenerated sentences are created in the relevant\nfragments of English with templates depicted in\nTable 1 and realised with dictionaries of nouns\nthat describe categories for unary predicates (e.g.\n“artist”, “beekeeper”) and transitive verbs for bi-\nnary predicates (e.g. “admires”, “bewitches”). We\nuse distinct vocabularies for training and evalua-\ntion data and use words with non-overlapping se-\nmantic ﬁelds. Unless stated otherwise, to main-\ntain comparability between the different datasets,\nwe generate 60000 examples for training and 8000\nfor evaluating model accuracy. This is achieved by\ngenerating 3750 and 500 (for training and evalua-\ntion sets, respectively) examples for each number\nof sentences s between 15 and 30.\n4\nConstructed problems\nOne attractive feature of the fragments S, S+ and\nR is that their satisﬁable sets of formulas admit\nof a simple graph-theoretic characterization. This\ngives us an additional means of creating data sets\ncomprising challenging problem instances.\nWe illustrate with S. Let a set Φ of S-sentences\nbe given, and let p1, . . . , pn be the common nouns\n(predicates) occurring in Φ. Now let V be the set\nof expressions pi(x) or ¬pi(x) (1 ≤i ≤n). We\ncall the elments of V literals and let the variables ℓ\nand m range over V . If ℓ∈V , denote by ¯ℓthe op-\nposite literal obtained by adding or removing the\nnegation sign as appropriate. Now let E be the set\nof ordered pairs of literals (ℓ, m) such that Φ con-\ntains a sentence formalized as either ∀x(ℓ→m)\nor as ∀x( ¯m →¯ℓ). Thus, GΦ = (V, E) is a di-\nrected graph. Write ℓ⇒Φ m if there is a path in\nGΦ from ℓto m. It can be shown (Pratt-Hartmann\nand Moss, 2009, Sec. 3) that a set of S-formulas\nΦ is satisﬁable if and only if Φ contains no sen-\ntence ∃x(ℓ∧m) such that either: (i) ℓ⇒Φ ¯ℓ; (ii)\nm ⇒Φ ¯m or (iii) ℓ⇒Φ ¯m. Thus, determining\n(un)satisﬁability in S amounts to detecting certain\nforbidden conﬁgurations (in this case: paths) in\nthe directed graph GΦ. We regard the length of the\npath (if it exists) as the size of the forbidden conﬁg-\nuration. Satisﬁability in S+ is characterized simi-\nlarly: we simply have to check that, in addition, V\ncontains no pair of opposite literals o and ¯o such\nthat o ⇒¯o and ¯o ⇒o. Again, if a set of sentences\nof S+ is inconsistent, then it contains a forbidden\nconﬁguration having a well-deﬁned size.\nThis gives us controlled way to generate hard\nproblem instances. Consider the fragment S+. We\nbegin by simply constructing a forbidden conﬁgu-\nration having a given size, d. To obtain an unsat-\nisﬁable problem instance Φ of size s, we then add\ns −d random sentences, checking (using a sim-\nple algorithm) that doing so does not create any\nsmaller forbidden conﬁgurations. To obtain a sat-\nisﬁable problem instance of size s, we reverse one\nof the implications in the forbidden conﬁguration,\nand check that the added s −d random sentences\ndo not cause an unsatisﬁability. In effect, d is a\nguaranteed difﬁculty level; it yields a lower bound\non the proof length required to establish unsatisﬁ-\nability. At the same time, the satisﬁable and unsat-\nisﬁable instances thus generated are not easily dis-\ntinguished by any superﬁcial characteristics. We\ndenote the problem Sat(S+) constructed as just\ndescribed with d in a speciﬁed range as Sat(S+\n[·,·]).\nThus, for example, in S+\n[2,6], unsatisﬁable problem\ninstances have 2 ≤d ≤6.\nFor R, the corresponding characterization of un-\nsatisﬁability in terms of forbidden conﬁgurations\ninvolves several cases (Pratt-Hartmann and Moss,\n2009, Sec. 4). For simplicity, we generate difﬁcult\ninstances by focusing on just one of these types\nof forbidden conﬁguration, which we call an ∀∀-\nconﬁguration. We begin by constructing an ∀∀-\nconﬁguration of speciﬁc size 6d (for d ≥1). (Such\na collection of sentences is always unsatisﬁable.)\nTo obtain an unsatisﬁable problem instance of size\ns, we add s −6d randomly generated sentences,\nagain checking that the resulting unsatisﬁability is\ndue entirely to the forbidden conﬁguration. Sat-\nisﬁable instances are then obtained by reversing\none of the implications in the ∀∀-conﬁguration,\nand checking that this does not lead to unsatisﬁ-\nability. Denote the problem Sat(R) constructed\nas just described with d in a speciﬁed range as\nSat(R⟨·,·⟩). For example, in R⟨1,2⟩, inconsistent\nproblem instances are inconsistent because of ∀∀-\nconﬁgurations of size 6 or 12.\nIt is not possible to characterize Sat(R+) in\nterms of forbidden conﬁgurations in this simple\nway. As a substitute, we use the proof-lengths of\nthe proofs found by the ATP as a rough guide to\ndifﬁculty. To generate difﬁcult instances of R+,\ntherefore, we ﬁrst generate random instances as in\nSec. 3; we then ﬁlter out those unsatisﬁable in-\nstances with short proof-lengths (as reported by\nthe ATP), and then remove satisﬁable instances at\nrandom to preserve the proportion of satisﬁable in-\nstances overall.\n5\nExperimental Setup\nIn the following study, we investigate whether\nneural networks, and in particular state-of-the-art\ntransformer-based language models (Devlin et al.,\n2019) can learn to perform satisﬁability checks\non examples representing the selected fragments.\nFirst, we investigate whether they can do so in prin-\nciple, by optimising and evaluating classiﬁers on\ntraining and evaluation data drawn from the same\ndistribution. Second, in an attempt to understand\nwhether they reliably learn the underlying logical\nprinciples that govern (un-)satisﬁability, we eval-\nuate their generalisation capabilities on out-of-\ndistribution evaluation data. We do so by altering\nvarious parameters of the training and evaluation\ndata generation to control the structure of the gen-\nerated problem instances.\nMore speciﬁcally, we cast the problem of de-\ntermining satisﬁability as binary text classiﬁca-\ntion and conduct experiments with three trans-\nformer based language models, RoBERTa, XLnet\nand Electra (Liu et al., 2019; Yang et al., 2019;\nClark et al., 2020a).which are further described in\nthe Appendix. We employ pre-trained language\nmodels because initial experiments showed that\nthese tend to converge faster compared to train-\ning from scratch, despite the fact that their pre-\ntraining objectives bears little similarity to the task\nat hand. Following Devlin et al. (2019), we repre-\nsent each problem in plain English text prepended\nby the special [CLS] token as input to the lan-\nguage model. The text is embedded using the lan-\nguage model, and the embedding of the [CLS]\ntoken, as output by the ﬁnal layer of the language\nmodel, is projected into a two-dimensional space,\nrepresenting the odds of the problem being satisﬁ-\nable. During inference, we pick the highest logit\nas the model’s prediction and during training we\nProblem size\nS+\nR\nSr\nSrn\nR+\n15 ≤s ≤30\n76\n93\n94\n80\n74\n30 ≤s ≤401\n64\n89\n94\n82\n72\n40 ≤s ≤45\n63\n86\n-\n-\n71\nTable 2: Accuracy of optimised models trained on ran-\ndom examples consisting of 15 to 30 sentences (s) and\nevaluated on longer random problem instances.\nminimise the cross-entropy loss between the log-\nits and the expected class, thus optimising the pa-\nrameters of the language model to produce the ex-\npected prediction conditioned on the input. We\nkeep the choice of hyper-parameters (detailed in\nAppendix) consistent across experiments.\n6\nResults and Analysis\nWe report and analyse the results of the conducted\nempirical study. For all results we measure the\nerror as a conﬁdence interval at α = 0.05, us-\ning asymptotic normal approximation and omit re-\nportage for brevity, as all measures are in the range\nof at most two percent points. We average results\nobtained for all three language models.\nTransformers perform well on random exam-\nples in all fragments. To seek evidence for the\nﬁrst question we train and evaluate separate mod-\nels on randomly generated problems instances of\neach of the ﬁve fragments. The results are reported\nthe ﬁrst row of Table 2 and suggest that the mod-\nels perform well on randomly generated problems\nin all fragments, even on the EXPTIME-complete\nR+ fragment. Surprisingly, the obtained accura-\ncies do not seem to correspond to the complexity\nclasses of the elicited fragments.\nNote that the\nresults reported in this table are not necessarily\nthat of the best-performing model; for example,\nby training the RoBERTa model longer and on a\nlarger dataset representing the R+ fragment, we\nobtain accuracy scores of up to 81% (from origi-\nnal 79%, see Table 8 in the Appendix). However,\nto maintain comparability and as the difference is\nmarginal, we use the same training budget for all\nmodels optimised on different fragments. The task\nappears non-trivial, as a simple LSTM-based clas-\nsiﬁer was not able to outperform the majority class\nbaseline even on the simplest S+ fragment and af-\nter explicit hyper-parameter optimisation.\nTo investigate whether this performance gener-\nalises with the size of the problem instances, we\n137 for Sr and Srn to ﬁt transformers’ 512-token limit\ngenerate evaluation sets with 30 ≤s ≤45 sen-\ntences for longer problems. Note that we are con-\nstrained to problems of the size of up to 512 to-\nkens, as a technical constraint of the pre-trained\nlanguage model, hence we do not investigate gen-\neralisation capabilities to problems beyond 45 (37\nfor Sr, Srn) sentences. The remainder of Table 2\nshows that models generalise consistently to prob-\nlems larger than seen during training, with the no-\ntable exception of the model optimised on S+,\nwhich exhibits the most signiﬁcant drop of over\n10 percent points.\nSuperﬁcially, Table 2 appears to indicate good\ngeneralisation performance. However, it is impor-\ntant to realise that simply increasing the number of\nsentences does not make the reasoning problems\nharder, as witnessed by the fact that the number of\nsentences, d, required to prove contradiction does\nnot increase: on average, d = 3.50 for examples\nwith 15 to 30 sentences, and d = 3.58 for prob-\nlems with 30 to 45 sentences. Thus, the forbidden\nconﬁgurations the network is identifying are, for\nthe most part, small, even for large numbers of sen-\ntences. A complimentary analysis in the Appendix\nreveals further, that these forbidden conﬁgurations\nare likely to be common between different frag-\nments.\nTransformers are not robust to distribution\nshifts.\nMoving on to the second question, we\ninvestigate whether the optimised models truly\npick up the reasoning patterns as intended or\nrather overﬁt to their training data as an artefact\nof the conﬁguration of parameters controlling the\nstochastic generation. Approximating the “hard-\nness” of a problem instance by its proof length\nl, we ﬁnd that for all fragments, the overwhelm-\ning majority (ranging from 29% in S to 86% in\nSrn) of contradictory examples have short proof\nlengths of 12 or less, indicating that random prob-\nlems are, in fact, unsatisﬁable for trivial reasons\nwhich are easy to show (See also histogram in\nAppendix). Thus, we collect “hard” examples by\n(over-) generating a large body of examples and\nthen ﬁltering by proof length and refer to them as\nrandom hard problem instances. Naturally, in this\nway we can only capture the hardness of inconsis-\ntent examples, therefore the evaluation focuses on\nthose.\nWhen comparing the (out-of-distribution) per-\nformance of models on “easy” and “hard” non-\nsatisﬁable problem instances with proof length of\nTrain l\nEval l\nS+\nR\nSr\nSrn\nR+\nl ≥6\n(easy)\nsatisﬁable\n61\n87\n90\n53\n58\nl ≤12\n98\n99\n99\n70\n98\nl ≥42\n84\n74\n70\n42\n70\nl ≥22\n(hard)\nsatisﬁable\n57\n89\n86\n50\n52\nl ≤12\n68\n61\n33\n61\n72\nl ≥42\n100\n94\n99\n96\n73\nTable 3: Accuracy of models trained on random satis-\nﬁable and easy/hard insatisﬁable examples, and evalu-\nated on random satisﬁable and easy/hard insatisﬁable\nexamples with proof length l ≤12 and l ≥42.\nat least 42 (Table 3, second and third rows), on\naverage, there is a gap of 25 percent points, This\nsuggests that models in fact overﬁt to simple prob-\nlems that tend to dominate the randomly gener-\nated datasets without picking up the general prin-\nciples governing reasoning in the corresponding\nfragments. Models optimised on “hard” training\nexamples (proof length ≥22, complemented with\nan equal number of random consistent problems),\ngeneralise well to even harder problems (proof\nlength ≥42, Table 3, last row). This suggests that\nthe models can learn to classify hard problem in-\nstances when presented explicitly by supplying ap-\npropriate training data. This performance does not\ncarry over to simpler problems, however, as mod-\nels optimised on harder problems exhibit a drop\nin accuracy when evaluated on simpler problems,\nas the penultimate row of Table 3 shows. In con-\njunction, these observations suggest that the mod-\nels tend to overﬁt to patterns in the generated data\narising from the parameterisation of the generation\nalgorithm, rather than learning to perform satisﬁa-\nbility checking in a more general sense. In other\nwords, just like a bad student of logic, they appear\nto “learn the proofs” rather than the logical princi-\nples behind the proofs required for successful sys-\ntematic generalisation.\nTransformers seem unable to reliably learn the\ndistinct reasoning patterns. Finally, as determin-\ning non-satisﬁability for the fragments S+ and R\ninvolves detection of one of a handful of forbid-\nden conﬁgurations, we generate data that exposes\nprecisely these forbidden conﬁgurations.\nThe four top rows of Table 4 show that models\noptimised on random and random hard datasets\nin S+ and R pick these patterns up to a vary-\ning degree: while all models perform better than\nchance, the evaluation performance drops consid-\nerably, when comparing to in-distribution perfor-\nmance. In the case of S+, breaking down the per-\nformance by chain depth, as shown in Figure 2,\nreveals that models optimised on random data per-\nform best on examples with chain length two (es-\nsentially detecting inconsistencies of the form “All\np are q”, “All q are s”, “Some p are not s”) and fail\nto generalise beyond that, while models trained on\nrandom hard data perform best on examples with\nchain length ﬁve, with their performance deterio-\nrating for examples with longer and shorter chains.\nThis reinforces the previous point that these mod-\nels appear to have learned to identify problems that\nhave proofs with similar structure to those in their\ntraining data rather than learning to solve the prob-\nlem in a more general sense.\nWhen we both optimise and evaluate models\non the constructed datasets, we ﬁnd that they are\ncapable of learning these inconsistency patterns\nwell, and generalise to harder problems unseen\nduring training: the RoBERTa model optimised on\nS+ with chain lengths between two and six per-\nforms just as well on evaluation data with chain\nlengths of up to 10 (Table 4, row ﬁve). However,\nother models fail to learn these patterns from con-\nstructed S+ data, suggesting that it is a challeng-\ning task. Similarly, all models optimised on R gen-\neralise to unseen lengths of the ∀∀-conﬁgurations\n(Table 4, row six).\nHowever, even when seem-\ningly learning these patterns, models fail to re-\nliably transfer these capabilities beyond the con-\nstructed cases, as evidenced by the poor gener-\nalisation performance when trained on the con-\nstructed datasets and evaluated on randomly gen-\nerated data (Table 4, bottom).\nFor R, the bad\ngeneralisation capability is expected, as the con-\nstructed dataset does not contain inconsistency pat-\nterns other than the ∀∀-conﬁgurations (e.g. exam-\nples where only the non-relational statements are\ninconsistent). However, this is inconsistent with\nthe case of S+, as all possible inconsistency pat-\nterns are covered in the constructed dataset, yet the\noptimised RoBERTa models fail to transfer to ran-\ndom S+ data.\n7\nRelated Work and Conclusion\nThe work reported in this paper represents a con-\ntinuation of various studies carried out on “chal-\nlenge sets”, proposed to investigate the ability of\n(optimised) neural networks to perform different\n2Results for RoBERTa only, as other models failed to con-\nverge on constructed S+ data.\nTrain On\nEvaluate On\nAccuracy\nS+\nS+\n[2,6]\n65\nS+\nl≥22\nS+\n[2,6]\n57\nR\nR⟨1,2⟩\n53\nRl≥22\nR⟨1,2⟩\n58\nS+\n[2,6]\nS+\n[2,10]\n962\nR⟨1,2⟩\nR⟨1,3⟩\n100\nS+\n[2,6]\nS+\n542\nR⟨1,2⟩\nR\n48\nS+\n[2,6]+R⟨1,2⟩\nR\n882\nTable 4: Accuracy of model optimised and evaluated on\nrandomly generated and constructed datasets in S+and\nR. The datasets S+\nl≥22 and Rl≥22 have unsatisﬁable ex-\namples with at least proof length 22, while S+\n[·,·], R⟨·,·⟩\nhave constructed chains in the bracketed range.\n2\n3\n4\n5\n6\n40\n60\n80\n100\nChain Length\nAccuracy\nS+\nS+l≤22\nRandom Guess\nFigure 2: Accuracy by chain length for RoBERTa mod-\nels optimised on S+ and S+l≤22, evaluated on S+[2,6].\nkinds of reasoning. Such studies range from mono-\ntonicity entailments (Geiger et al., 2020; Yanaka\net al., 2020) to probing lexical knowledge (Glock-\nner et al., 2018) to logical connectives (Salvatore\net al., 2019; Richardson et al., 2019) or verb sym-\nmetry (Mitra et al., 2020). These studies are re-\nlated to ours, in that they seek to isolate capa-\nbilities of interest and perform controlled exper-\niments using synthetic datasets.\nThey have in\ncommon that models optimised on crowd-sourced\ndatasets, such as MNLI, (Williams et al., 2018),\nperform poorly on the challenge set data exhibit-\ning the elicited phenomenon, but ﬁne-tuning the\noptimised model on portions of these data im-\nproves the performance.\nHowever, as Rozen et al. (2019) show, good\nscores after the ﬁne-tuning probably stem from\nthe fact that the investigated model has learned to\nadapt to the regularities of the challenge set rather\nthan learning a general notion of the investigated\nphenomenon. Our analysis is similar: transformer-\nbased neural networks perform well on randomly\ngenerated data, but this performance is brittle, and\nthe models overﬁt to the problem space as set out\nby the dataset generation method.\nThey do not\ngeneralise well to examples outside of that space,\nsuggesting that they do not generalise systemati-\ncally (Fodor and Pylyshyn, 1988), i.e. they strug-\ngle to identify a ﬁnite set of rules and to apply\nthem repeatedly. Similar to our ﬁndings, research\non systematic generalisation suggests that neural\nnetworks tend to generalise without systematicity\nin supervised learning scenarios (Johnson et al.,\n2017; Lake and Baroni, 2017; Goodwin et al.,\n2020), although these studies, unlike ours, did not\nconcern pre-trained models.\nOne might argue that it is unfair to expect a sta-\ntistical model that relies on correlations to learn\npatterns that are not obviously present in the data.\nHowever, it seems that such claims are being made\nin the literature (Clark et al., 2020b). While we\nobserve that models optimised on constructed ex-\namples generalise well to harder problems unseen\nduring training, we also show that this capability\nappears not to transfer to examples that are only\nsuperﬁcially different. Thus, in our experiments,\nsimilar to Richardson and Sabharwal (2021), we\nﬁnd that the optimised models are not able to reli-\nably disentangle and acquire the different reason-\ning patterns required to successfully complete the\ntask of determining satisﬁability.\nOur study highlights one of the issues with em-\npirically postulating neural networks various capa-\nbilities by means of good performance on chal-\nlenge sets.\nThey have only negative predictive\npower (Gardner et al., 2020): while low perfor-\nmance indicates the lack of a capability, the con-\nverse does not necessarily hold.\nThis can be\ntaken as a motivation to develop formal veriﬁ-\ncation methods for neural networks (Shi et al.,\n2020), or investigate worst-case bounds for dif-\nferent phenomena (Raghunathan et al., 2018; Jia\net al., 2019).\nOur formulated task naturally allows us to ex-\npand the scope of the controlled experiments: for\nexample, by increasing the closed-class vocabu-\nlary. Another possible avenue is to focus on im-\nproving the systematic generalisation of neural ap-\nproaches, for example by providing the formulas\nrequired to prove that a set of sentences is unsatisﬁ-\nable as additional supervision signal, or by relying\non modular or model-based approaches (Andreas\net al., 2016; Lake et al., 2013).\nLimitations\nThe design of the study limits our ﬁndings by de-\nsign - by removing the need of inducing mean-\ning postulates (i.e. commonsense reasoning and\nworld knowledge) we explicitly focus on logico-\nsyntactic capabilities, analogous to how “reason-\ning” is deﬁned for symbolic approaches to AI. Ar-\nguably, In “real world” application scenarios, com-\nmonsense reasoning and world knowledge cannot\nbe fully disconnected from the requirement to per-\nform reasoning, which allows our inquiry to draw\nfundamental conclusions about the capabilities of\ntransformer-based language models rather than to\nmake recommendations which are of immediate\nrelevance to practitioners.\nOur study also suffers from the inductive\ndilemma. We ﬁnd that multiple transformer-based\nlanguage models follow the trends reported in this\npaper, speciﬁcally that they fail to robustly iden-\ntify the reasoning patterns necessary for reliably\ndetermining satisﬁability in the elicited fragments.\nHowever, due to the empirical nature of this re-\nsearch, this ﬁnding is of course not a guarantee\nthat some neural architecture (transformer-based\nor otherwise) could still perform well, when tested\nin our out-of-distribution evaluation settings.\nAcknowledgements\nThe authors would like to acknowledge the use of\nthe Computational Shared Facility at The Univer-\nsity of Manchester and thank the anonymous re-\nviewers from the ARR December 2021 cycle for\ntheir valuable feedback.\nReferences\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and\nDan Klein. 2016. Neural Module Networks.\nAristotle. 1963. Aristotle’s Categories and De Inter-\npretatione. Clarendon Press, Oxford. (J.R. Ackrill,\nTr.).\nAristotle. 1989. Prior Analytics. Hackett, Indianapolis,\nIN. (R. Smith, Tr.).\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009.\nThe\nﬁfth pascal recognizing textual entailment challenge.\nIn In Proc Text Analysis Conference (TAC’09.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Stroudsburg, PA, USA. Association for\nComputational Linguistics.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced LSTM for\nNatural Language Inference. ACL 2017 - 55th An-\nnual Meeting of the Association for Computational\nLinguistics, Proceedings of the Conference (Long\nPapers), 1:1657–1668.\nKevin Clark,\nMinh-Thang Luong,\nGoogle Brain,\nQuoc V Le Google Brain, and Christopher D Man-\nning. 2020a.\nELECTRA: Pre-training Text En-\ncoders as Discriminators Rather Than Generators.\nIn International Conference on Learning Represen-\ntations (ICLR).\nPeter Clark, Oyvind Tafjord, and Kyle Richardson.\n2020b. Transformers as Soft Reasoners over Lan-\nguage.\nIn Proceedings of the Twenty-Ninth Inter-\nnational Joint Conference on Artiﬁcial Intelligence,\npages 3882–3890. International Joint Conferences\non Artiﬁcial Intelligence.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Stroudsburg, PA, USA. Associa-\ntion for Computational Linguistics.\nJerry A. Fodor and Zenon W. Pylyshyn. 1988. Connec-\ntionism and cognitive architecture: A critical analy-\nsis. Cognition, 28(1-2):3–71.\nMatt Gardner, Yoav Artzi, Victoria Basmov, Jonathan\nBerant, Ben Bogin, Sihao Chen, Pradeep Dasigi,\nDheeru Dua, Yanai Elazar, Ananth Gottumukkala,\nNitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco,\nDaniel Khashabi, Kevin Lin, Jiangming Liu, Nel-\nson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer\nSingh, Noah A. Smith, Sanjay Subramanian, Reut\nTsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.\n2020. Evaluating Models’ Local Decision Bound-\naries via Contrast Sets. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020,\npages 1307–1323, Stroudsburg, PA, USA. Associa-\ntion for Computational Linguistics.\nAtticus Geiger, Kyle Richardson, and Christopher\nPotts. 2020. Neural natural language inference mod-\nels partially embed theories of lexical entailment\nand negation.\nIn Proceedings of the Third Black-\nboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP, pages 163–173. Associ-\nation for Computational Linguistics.\nMax Glockner, Vered Shwartz, and Yoav Goldberg.\n2018. Breaking NLI Systems with Sentences that\nRequire Simple Lexical Inferences.\nIn Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 650–655, Stroudsburg, PA, USA. Asso-\nciation for Computational Linguistics.\nEmily Goodwin, Koustuv Sinha, and Timothy J.\nO’Donnell. 2020.\nProbing Linguistic Systematic-\nity.\nIn Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics,\npages 1958–1969, Stroudsburg, PA, USA. Associa-\ntion for Computational Linguistics.\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel Bowman, and Noah A\nSmith. 2018. Annotation Artifacts in Natural Lan-\nguage Inference Data. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers),\npages 107–112, Stroudsburg, PA, USA. Association\nfor Computational Linguistics.\nRobin Jia, Aditi Raghunathan, Kerem Göksel, and\nPercy Liang. 2019. Certiﬁed Robustness to Adver-\nsarial Word Substitutions.\nIn Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4127–4140, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nJustin\nJohnson,\nLi\nFei-Fei,\nBharath\nHariharan,\nC Lawrence Zitnick, Laurens Van Der Maaten,\nand Ross Girshick. 2017.\nCLEVR: A Diagnostic\nDataset for Compositional Language and Elemen-\ntary Visual Reasoning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 2901–2910.\nLaura Kovács and Andrei Voronkov. 2013. First-Order\nTheorem Proving and Vampire.\nLecture Notes\nin Computer Science (including subseries Lecture\nNotes in Artiﬁcial Intelligence and Lecture Notes in\nBioinformatics), 8044 LNCS:1–35.\nBrenden M. Lake and Marco Baroni. 2017. General-\nization without systematicity: On the compositional\nskills of sequence-to-sequence recurrent networks.\n35th International Conference on Machine Learning,\nICML 2018, 7:4487–4499.\nBrenden M. Lake, Ruslan Salakhutdinov, and Joshua B.\nTenenbaum. 2013.\nOne-shot learning by invert-\ning a compositional causal process. University of\nToronoto web domain, 26.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In Interna-\ntional Conference on Learning Representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arxiv:1907.11692.\nArindam Mitra, Ishan Shrivastava, and Chitta Baral.\n2020. Enhancing Natural Language Inference Us-\ning New and Expanded Training Data Sets and New\nLearning Models. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in PyTorch.\nIn Autodiff Workshop @ NIPS 2017.\nI. Pratt-Hartmann and L. Moss. 2009. Logics for the\nrelational syllogistic.\nReview of Symbolic Logic,\n2(4):647–683.\nIan Pratt-Hartmann. 2014. Semantic complexity in nat-\nural language. In The Handbook of Contemporary\nSemantic Theory, 2nd edition edition, pages 429–\n454. Wiley Blackwell.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020.\nExploring\nthe Limits of Transfer Learning with a Uniﬁed Text–\nto-Text Transformer. Journal of Machine Learning\nResearch, 21:1–67.\nAditi Raghunathan, Jacob Steinhardt, and Percy Liang.\n2018. Semideﬁnite relaxations for certifying robust-\nness to adversarial examples. In Advances in Neu-\nral Information Processing Systems 31: Annual Con-\nference on Neural Information Processing Systems,\npages 10900–10910.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 2383–2392,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nKyle Richardson, Hai Hu, Lawrence S. Moss, and\nAshish Sabharwal. 2019. Probing Natural Language\nInference Models through Semantic Fragments. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence.\nKyle Richardson and Ashish Sabharwal. 2021. Push-\ning the limits of rule reasoning in transformers\nthrough natural language satisﬁability.\n(forthcom-\ning in AAAI 22).\nOhad Rozen, Vered Shwartz, Roee Aharoni, and Ido\nDagan. 2019. Diversify Your Datasets: Analyzing\nGeneralization via Controlled Variance in Adversar-\nial Datasets.\nIn Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 196–205, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.\nFelipe Salvatore, Marcelo Finger, and Roberto Hi-\nrata\nJr.\n2019.\nA\nlogical-based\ncorpus for\ncross-lingual evaluation.\nIn Proceedings of the\n2nd Workshop on Deep Learning Approaches for\nLow-Resource NLP (DeepLo 2019), pages 22–30,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nViktor Schlegel, Goran Nenadic, and Riza Batista-\nNavarro. 2022. A survey of methods for revealing\nand overcoming weaknesses of data-driven Natural\nLanguage Understanding. Natural Language Engi-\nneering, pages 1–31.\nViktor Schlegel, Marco Valentino, André Andre Fre-\nitas, Goran Nenadic, and Riza Batista-Navarro.\n2020.\nA Framework for Evaluation of Machine\nReading Comprehension Gold Standards.\nIn Pro-\nceedings of The 12th Language Resources and Eval-\nuation Conference, pages 5359–5369, Marseille,\nFrance. European Language Resources Association.\nZhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie\nHuang, and Cho-Jui Hsieh. 2020. Robustness Ver-\niﬁcation for Transformers. In 8th International Con-\nference on Learning Representations, ICLR 2018 -\nConference Track Proceedings.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and Policy Considerations for\nDeep Learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nSaku Sugawara, Kentaro Inui, Satoshi Sekine, and\nAkiko Aizawa. 2018. What Makes Reading Com-\nprehension Questions Easier? In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4208–4219, Strouds-\nburg, PA, USA. Association for Computational Lin-\nguistics.\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-\nberg, and Jonathan Berant. 2020. Leap-Of-Thought:\nTeaching Pre-Trained Models to Systematically Rea-\nson Over Implicit Knowledge. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems.\nJidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao,\nHao He, and Yaohui Jin. 2021.\nDiagnosing the\nﬁrst-order logical reasoning ability through Logic-\nNLI.\nIn Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3738–3747, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A Broad-Coverage Challenge Corpus for Sen-\ntence Understanding through Inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122, Stroudsburg, PA,\nUSA. Association for Computational Linguistics.\nHitomi Yanaka, Koji Mineshima, Daisuke Bekki, and\nKentaro Inui. 2020. Do Neural Models Learn Sys-\ntematicity of Monotonicity Inference in Natural Lan-\nguage?\nIn Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6105–6117, Stroudsburg, PA, USA. Associa-\ntion for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding. In In Proceedings of Ad-\nvances in Neural Information Processing Systems 32\n(NeurIPS 2019), pages 5753–5763.\nA\nAdditional Details on the\nExperimental Setup\nRegarding models, We choose RoBERTa, Electra\nand XLnet as representatives of “BERTology” as\nthey all employ different pretraining objectives to\nobtain the contextualised representations. We do\nnot perform explicit hyperparemeter optimisation\nfor each of the models for each of the datasets,\nas we are not invested in ﬁnding a best perform-\ning model, but rather, we are concerned with more\ngeneral questions about the learnability of the pre-\nsented problems. We ﬁnd that our obtained results\nare similar, and in this regard we expect results to\nbe similar for other transformer-based approaches,\nas their performance stems from the amount of pre-\ntraining data and language model size, rather than\narchitectural choices (Raffel et al., 2020). Finally,\ninvestigating multiple models (e.g.\nmore trans-\nformer models or hyperparemeter optimisation) in-\ncreases the amount of computation and thus the\ncarbon footprint (Strubell et al., 2019), which we\ndeem unnecessary given the research questions.\nAs both data generation and model optimisa-\ntion are stochastic processes, we are concerned\nwith the impact of chance on the results of our\nexperiments. To investigate whether model con-\nvergence is impacted, we optimise ﬁve models on\nR datasets generated from different random seeds.\nFor model performance, we evaluate one opti-\nmised model on ﬁve different R evaluation sets.\nThe results are summarised in Table 5: the vari-\nance of evaluation scores is negligible, while for\noptimisation, the inﬂuence of randomness is more\nnoticeable. The presented loss variance translates\nto differences in accuracy of up to 2 percent points.\nB\nAdditional training details\nWe implement the training and inference in Py-\nTorch 1.10.0 (Paszke et al., 2017).\nWe use\nthe pre-trained language models available in\nExperiment\nMetric\nMean\nStd. dev.\ntrain stability\nLoss\n0.07696\n0.008\neval stability\nAccuracy\n0.953\n0.0013\nTable 5: Impact of randomness in the data generation\nprocess on model optimisation (ﬁrst row) and on model\nperformance (second row).\n[0; 10) [10; 20)[20; 30)[30; 40)[40; ∞)\n0\n20\n40\n60\n80\nproof lengths d\nProportion in %\nS+\nR\nSr\nSrn\nR+\nFigure 3: Proof length distribution.\nthe transformers2 library.\nWe train the\nroberta model on an Nvidia V100 GPU with\n16 GB of RAM. to keep a consistent set of hy-\nperparamters, We train the XLNet models on an\nNvidia A100 GPU with 80GB of RAM since these\nmodels do not support gradient checkpointing, and\nthe chosen batch size results in large (≥16GB)\nmemory requirements during training.\nWe ﬁx the random seed to maintain determin-\nistic behaviour and the hyper-parameters used for\ntraining all models are\n• Batch size: relying on gradient checkpoint-\ning, we are able to set the batch size to 56.\n• Learning Rate: We schedule the learning\nrate to linearly warm up from zero to 4·10−6,\nlinearly decaying it to zero, as it was found to\nperform best across all fragments. We use the\nADAM optimiser with the default parameters\nǫ = 1 × 10−8, β1 = 0.99 and β2 = 0.999.\nNote that this comparatively low learning rate\nprohibits us from using mixed precision opti-\nmisation.\n• Train Epochs: We train the models for 6\nepochs on all fragments to maintain the same\ntraining budget.\n• Maximal sequence length:\nAs the input\nlength varies for different fragments, we en-\nsure that the sequence lengths are set in a way\nthat allows to embed. In practice this varies\nfrom 288 to 432 tokens. Note that padding in-\nput sequences to max length does not impact\nthe training procedure as the padding tokens\nare not attended to when calculating the em-\n2https://github.com/huggingface/transformers\nbedding of the [CLS] input token (nor for\nany other input tokens).\nWe ﬁnd this setting works well for all conducted\nexperiments, thus we keep the same set of hyper-\nparameters to maintain comparability. To replicate\nour experiments, please see the separately sup-\nplied code.\nC\nAdditional Results\nWe further investigate whether transformers—\nperhaps due to their pre-training—can generalise\nto fragments they have not encountered during\ntraining. To that end, we evaluate the models on\nall fragments, not only those they have been op-\ntimised on. The results are reported in Table 6\nand reveal counter-intuitive patterns. Surprisingly,\nnone of the trained models generalises particularly\nwell to the simpler S+ fragment (Table 6, ﬁrst\nrow). While the results of models trained on the\nSr and Srn fragments could be explained by the\ndifferent problem structure (during training, these\nmodels do not encounter sentences as they appear\nin S+), the same explanation is not valid for the R\nand R+ fragments: problems in these fragments\nconsist of 80% syllogistic statements on average.\nIn fact, an estimated 90% of the unsatisﬁable prob-\nlems in R and R+ contain an inconsistency in the\nnon-relational statements. Therefore, it is reason-\nable to expect models optimised on R and R+ to\nperform well on S+, which appears not to be the\ncase. Contrariwise, models optimised on the sim-\npler fragments S+ fragment do generalise well to\nthe harder fragments R and R+, as one would\nexpect given their high number of non-relational\ninconsistencies. This contradictory evidence sug-\ngests that the models struggle to reliably identify\nthe conﬁgurations leading to insatisﬁability. An-\nother observation that eludes a simple explanation\nis the good performance on the R fragment of the\nmodel optimised on Sr, as these fragments are gen-\nerated from non-overlapping sentence templates.\nTraining the model on a combined sample of\n12000 problem instances from all fragments re-\nsults in accuracy scores of 73%, 89%, 94%, 94%\nand 74%, for the fragments S+, R, Sr, Srn and\nR+, respectively.\nThe distribution of different proof lengths for\nrandomly generated data is shown in Figure 3 and\nsupports the hypothesis that most randomly gener-\nated inconsistent problem instances are “easy” in\nEvaluated on ↓\nTrained on →\nmaj. class\nS+\nR\nSr\nSrn\nR+\nS+\n54\n86\n61\n61\n55\n70\nR\n52\n79\n95\n78\n54\n85\nSr\n55\n45\n48\n96\n90\n49\nSrn\n53\n47\n50\n78\n91\n48\nR+\n53\n73\n63\n60\n55\n79\nTable 6: Accuracy of RoBERTa models evaluated on\nall fragments. Grey results denote generalisation per-\nformance to harder fragments.\nProblem size\nS+\nR\nSr\nSrn\nR+\n15 ≤s ≤30\n82\n96\n95\n91\n82\n30 ≤s ≤40\n72\n92\n94\n93\n77\n40 ≤s ≤45\n70\n89\n-\n-\n76\nTable 7: Accuracy of optimised Electra models trained\non random examples consisting of 15 to 30 sentences\n(s) and evaluated on longer random problem instances.\nProblem size\nS+\nR\nSr\nSrn\nR+\n15 ≤s ≤30\n86\n95\n96\n91\n79\n30 ≤s ≤40\n62\n92\n95\n94\n76\n40 ≤s ≤45\n61\n90\n-\n-\n75\nTable 8:\nAccuracy of optimised RoBERTa models\ntrained on random examples consisting of 15 to 30 sen-\ntences (s) and evaluated on longer random problem in-\nstances.\nthe sense that they have short proof lengths that\nlead to refutation.\nFigure 2 shows the breakdown by chain length\nfor models optimised on random and random hard\nS+ data and evaluated on constructed S+ exam-\nples.\nFinally, Tables 7-15 report all performances dis-\ncussed in the main paper broken down by model.\nWe see similar trends across models, with Elec-\ntra performing best and XLNet often performing\nworst. We caution to over-interpret these differ-\nences, as these could be due to the amount of pre-\ntraining of each architecture and the resulting sen-\nsitivity to hyper-parameters (Lan et al., 2020).\nD\n∀∀-conﬁgurations in R\nIn the fragment R, satisﬁability is characterized\nby a ﬁnite number (half a dozen or so) of so-\ncalled forbidden conﬁgurations: families of un-\nsatisﬁable sets of formulas, with the instances of\neach family characterized by a numerical param-\neter related to that instance’s cardinality. It can\nProblem size\nS+\nR\nSr\nSrn\nR+\n15 ≤s ≤30\n61\n89\n92\n60\n62\n30 ≤s ≤40\n60\n83\n93\n60\n63\n40 ≤s ≤45\n59\n80\n-\n-\n64\nTable 9: Accuracy of optimised XLNet models trained\non random examples consisting of 15 to 30 sentences\n(s) and evaluated on longer random problem instances.\nTrain l\nEval l\nS+\nR\nSr\nSrn\nR+\nl ≥6\n(easy)\nsatisﬁable\n72\n92\n91\n86\n71\nl ≤12\n99\n100\n100\n100\n98\nl ≥42\n92\n77\n79\n48\n66\nl ≥22\n(hard)\nsatisﬁable\n74\n92\n89\n51\n65\nl ≤12\n57\n64\n30\n59\n64\nl ≥42\n100\n95\n99\n96\n79\nTable 10: Accuracy of Electra models trained on ran-\ndom satisﬁable and easy/hard insatisﬁable examples,\nand evaluated on random satisﬁable and easy/hard in-\nsatisﬁable examples.\nTrain l\nEval l\nS+\nR\nSr\nSrn\nR+\nl ≥6\n(easy)\nsatisﬁable\n78\n92\n92\n87\n72\nl ≤12\n99\n99\n99\n99\n97\nl ≥42\n71\n68\n64\n37\n56\nl ≥22\n(hard)\nsatisﬁable\n60\n90\n86\n48\n53\nl ≤12\n64\n63\n28\n67\n72\nl ≥42\n100\n94\n98\n99\n75\nTable 11: Accuracy of RoBERTa models trained on ran-\ndom satisﬁable and easy/hard insatisﬁable examples,\nand evaluated on random satisﬁable and easy/hard in-\nsatisﬁable examples.\nTrain l\nEval l\nS+\nR\nSr\nSrn\nR+\nl ≥6\n(easy)\nsatisﬁable\n33\n77\n87\n53\n31\nl ≤12\n95\n99\n99\n70\n98\nl ≥42\n91\n78\n68\n41\n90\nl ≥22\n(hard)\nsatisﬁable\n37\n85\n84\n50\n38\nl ≤12\n84\n55\n32\n57\n81\nl ≥42\n100\n93\n99\n92\n67\nTable 12: Accuracy of XLNet models trained on ran-\ndom satisﬁable and easy/hard insatisﬁable examples,\nand evaluated on random satisﬁable and easy/hard in-\nsatisﬁable examples.\nbe shown that any unsatisﬁable set of R-formulas\ncontains an instance of one of these forbidden con-\nﬁgurations. The ∀∀-conﬁguration is the most com-\nplex of these, and, therefore, the hardest to learn\nto recognize. Hard unsatisﬁable formula sets in\nR were constructed in the experiments reported\nhere using the ∀∀-conﬁguration. We brieﬂy out-\nline its form here.\nWe use abbreviated logical\nTrain On\nEvaluate On\nAccuracy\nS+\nS+\n[2,6]\n70\nS+\nl≥22\nS+\n[2,6]\n58\nR\nR⟨1,2⟩\n51\nRl≥22\nR⟨1,2⟩\n61\nS+\n[2,6]\nS+\n[2,10]\n−\nR⟨1,2⟩\nR⟨1,3⟩\n100\nS+\n[2,6]\nS+\n−\nR⟨1,2⟩\nR\n48\nS+\n[2,6]+R⟨1,2⟩\nR\n−\nTable 13: Accuracy of Electra models optimised and\nevaluated on random and constructed datasets in S+,R.\nTrain On\nEvaluate On\nAccuracy\nS+\nS+\n[2,6]\n68\nS+\nl≥22\nS+\n[2,6]\n58\nR\nR⟨1,2⟩\n57\nRl≥22\nR⟨1,2⟩\n58\nS+\n[2,6]\nS+\n[2,10]\n96\nR⟨1,2⟩\nR⟨1,3⟩\n100\nS+\n[2,6]\nS+\n54\nR⟨1,2⟩\nR\n48\nS+\n[2,6]+R⟨1,2⟩\nR\n88\nTable 14: Accuracy of RoBERTa models optimised and\nevaluated on random and constructed datasets in S+,R.\nTrain On\nEvaluate On\nAccuracy\nS+\nS+\n[2,6]\n56\nS+\nl≥22\nS+\n[2,6]\n56\nR\nR⟨1,2⟩\n52\nRl≥22\nR⟨1,2⟩\n55\nS+\n[2,6]\nS+\n[2,10]\n−\nR⟨1,2⟩\nR⟨1,3⟩\n100\nS+\n[2,6]\nS+\n−\nR⟨1,2⟩\nR\n48\nS+\n[2,6]+R⟨1,2⟩\nR\n−\nTable 15: Accuracy of XLNet models optimised and\nevaluated on random and constructed datasets in S+,R.\nnotation, writing ∀(p, q) instead of ∀x(p(x) →\nq(x)), or ∀(p, ∃(q, ¬r)) instead of ∀x(p(x) →\n∃y(¬r(x, y) ∧q(y))), and so on.\nAn instance of a ∀∀-conﬁguration with parame-\nter d consists of six sets of d formulas (hence, 6d\nin all). The ﬁrst two lists entail that all ps are o1s\nand all ps are o2s:\n∀(p, p1), . . . , ∀(pd−1, o1)\n∀(p, p′\n1), . . . , ∀(p′\nd−1, o2).\nIt follows that, if some ps exist, then some o1s are\no2s. The second two lists entail that all qs are re-\nlated by r to all o1s and to no o2s:\n∀(q, q1), . . . , ∀(qd−2, qd−1), ∀(qd−1, ∀(o1, r))\n∀(q, q′\n1), . . . , ∀(q′\nd−2, q′\nd−1), ∀(q′\nd−1, ∀(o2, ¬r)).\nIt follows that, if some qs exist, then no o1s are o2s.\nHence, these four lists entail that, if some ps exist,\nthen no qs exist. The ﬁnal two lists entail that there\nare both ps and qs:\n∃(u0, u1), ∀(u1, ∃(u2, ±r)), . . . , ∀(ud−1, ∃(p, ±r))\n∃(u′\n0, u′\n1), ∀(u′\n1, ∃(u′\n2, ±r)), . . . , ∀(u′\nd−1, ∃(q, ±r)).\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2022-11-10",
  "updated": "2022-11-10"
}