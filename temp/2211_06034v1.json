{
  "id": "http://arxiv.org/abs/2211.06034v1",
  "title": "Does Deep Learning REALLY Outperform Non-deep Machine Learning for Clinical Prediction on Physiological Time Series?",
  "authors": [
    "Ke Liao",
    "Wei Wang",
    "Armagan Elibol",
    "Lingzhong Meng",
    "Xu Zhao",
    "Nak Young Chong"
  ],
  "abstract": "Machine learning has been widely used in healthcare applications to\napproximate complex models, for clinical diagnosis, prognosis, and treatment.\nAs deep learning has the outstanding ability to extract information from time\nseries, its true capabilities on sparse, irregularly sampled, multivariate, and\nimbalanced physiological data are not yet fully explored. In this paper, we\nsystematically examine the performance of machine learning models for the\nclinical prediction task based on the EHR, especially physiological time\nseries. We choose Physionet 2019 challenge public dataset to predict Sepsis\noutcomes in ICU units. Ten baseline machine learning models are compared,\nincluding 3 deep learning methods and 7 non-deep learning methods, commonly\nused in the clinical prediction domain. Nine evaluation metrics with specific\nclinical implications are used to assess the performance of models. Besides, we\nsub-sample training dataset sizes and use learning curve fit to investigate the\nimpact of the training dataset size on the performance of the machine learning\nmodels. We also propose the general pre-processing method for the physiology\ntime-series data and use Dice Loss to deal with the dataset imbalanced problem.\nThe results show that deep learning indeed outperforms non-deep learning, but\nwith certain conditions: firstly, evaluating with some particular evaluation\nmetrics (AUROC, AUPRC, Sensitivity, and FNR), but not others; secondly, the\ntraining dataset size is large enough (with an estimation of a magnitude of\nthousands).",
  "text": "Does Deep Learning REALLY Outperform Non-deep Machine\nLearning for Clinical Prediction on Physiological Time Series?\nKe Liao1,2, Wei Wang1, Armagan Elibol2, Lingzhong Meng3, Xu Zhao4, and Nak Young Chong2\nAbstract— Machine learning has been widely used in health-\ncare applications to approximate complex models, for clinical\ndiagnosis, prognosis, and treatment. As deep learning has the\noutstanding ability to extract information from time series, its\ntrue capabilities on sparse, irregularly sampled, multivariate,\nand imbalanced physiological data are not yet fully explored.\nIn this paper, we systematically examine the performance\nof machine learning models for the clinical prediction task\nbased on the EHR, especially physiological time series. We\nchoose Physionet 2019 challenge public dataset [1] to predict\nSepsis outcomes in ICU units. Ten baseline machine learning\nmodels are compared, including 3 deep learning methods and\n7 non-deep learning methods, commonly used in the clinical\nprediction domain. Nine evaluation metrics with speciﬁc clinical\nimplications are used to assess the performance of models.\nBesides, we sub-sample training dataset sizes and use learning\ncurve ﬁt to investigate the impact of the training dataset\nsize on the performance of the machine learning models.\nWe also propose the general pre-processing method for the\nphysiology time-series data and use Dice Loss [2] to deal\nwith the dataset imbalanced problem. The results show that\ndeep learning indeed outperforms non-deep learning, but with\ncertain conditions: ﬁrstly, evaluating with some particular\nevaluation metrics (AUROC, AUPRC, Sensitivity, and FNR),\nbut not others; secondly, the training dataset size is large enough\n(with an estimation of a magnitude of thousands).\nI. INTRODUCTION\nWith the widespread adoption of Electronic Health\nRecords (EHR), there is an increased emphasis on predic-\ntive models that can help clinical decisions and treatment\nrecommendation. EHR contains various types of data, in-\ncluding demographics information, laboratory values, vital\nsigns, physiology signals and treatment records, which are\ngenerally sparse, irregularly sampled, and multivariate. With\nthe development of machine learning methods, especially\ndeep learning and neural networks, more research focuses\non clinical prediction or physiology management estimation\nwith EHR using machine learning methods [3]. Deep learn-\ning shows excellent capabilities to extract information and\nfeatures from big data, but the performance of deep learning\n1K.\nLiao\nand\nW.\nWang\nare\nwith\nthe\nRicoh\nSoftware\nRe-\nsearch\nCenter\n(Beijing)\nCo.,\nLtd.,\nBeijing,\nChina\n({ke.liao,\nwei.wang5}@cn.ricoh.com).\n2K. Liao is a Ph.D. student of the Japan Advanced Institute of Science and\nTechnology (JAIST). A. Elibol and N.Y. Chong are with the School of In-\nformation Science, JAIST, Ishikawa, Japan ({s1820033, aelibol,\nnakyoung}@jaist.ac.jp).\n3L.\nMeng\nis\nwith\nthe\nDepartment\nof\nAnesthesiology,\nYale\nUniversity\nSchool\nof\nMedicine,\nNew\nHaven,\nCT\n06520,\nUSA\n(lingzhong.meng@yale.edu).\n4X. Zhao is with the Department of Anesthesiology, The Second Xiangya\nHospital, Central South University, Changsha, Hunan Province, China\n(zhaoxu109@hotmail.com).\ndepends on the training dataset size, model structure, data\nquality, and similar other related conditions. Many research\nor actual work fails to reach the ideal target when utilizing\ndeep learning models [4]. In some cases, non-deep learning,\nsuch as XGBoost has a similar or even better performance\nwith deep neural networks [5].\nIn this paper, we systematically analyze the machine learn-\ning models’ performance on the clinical prediction domain,\nusing EHR, especially the physiology time series data. We\npropose a general framework to handle the raw multivariate\ntime series data. We utilize the Physionet 2019 challenge\npublic dataset [1] to predict Sepsis outcomes in ICU units,\ndeﬁned as a binary classiﬁcation problem. And we train\nand evaluate the datasets with 10 baseline machine learning\nmodels: 3 deep learning models: (1) InceptionTime [6], (2)\nTCN (Temporal CNN) [7] and (3) Transformer [8] and 7\nnon-deep learning models: (4) LR (Logistic Regression),\n(5) SVM (Support Vector Machines), (6) KNN (K Nearest\nNeighbors), (7) GP (Gaussian Process), (8) DT (Decision\nTree), (9) RF (Random Forests) and (10) XGBoost [5]. Then,\nwe evaluate the performance of different machine learning\nmodels using 9 different evaluation metrics with specialized\nclinical meanings. Also, as the training dataset size is a\ncrucial aspect for the models’ performance, we sub-sample\nthe training dataset and present a quantitative analysis of\nthe relationship between the training dataset size and the\nperformance of machine learning models using the learning\ncurve ﬁt method.\nBased on the proposed systematic and quantitative analysis\nframework, we ﬁnd that deep learning indeed outperforms\nnon-deep learning only under certain conditions.\n• The four evaluation metrics: AUROC, AUPRC, Sen-\nsitivity, and FNR show that deep learning outperforms\nnon-deep learning. For example, AUROC is greater than\n0.9 for deep learning models, while it is less than 0.8\nfor non-deep learning models.\n• Labeled training dataset size: The larger training dataset\nimproves the performance of the machine learning mod-\nels, and deep learning models need a certain number of\ntraining dataset sizes to reach superior performance. The\nthreshold value depends on different evaluation metrics.\nThe overall estimate of training dataset sizes is about a\nmagnitude of thousands.\nII. RELATED WORK\nThere is an extensive body of clinical prediction based\non EHR data, especially the physiology time series. Hatib\net al. [9] use feature engineering to extract features from\narXiv:2211.06034v1  [cs.LG]  11 Nov 2022\narterial pressure waveform to the prediction of an upcoming\nhypotensive event. Lee et al. [10] extract basic statistical\nfeatures and then uses feedforward neural networks to predict\nmortality.\nSome recent works try to extract the information or fea-\ntures from raw time series data, without feature engineering\nor statistical analysis. Kok et al. [7] use TCN for automated\nprediction of Sepsis outcome. Che et al. [2] develop LSTM\nfor Sepsis prediction with missing values in raw data. Fawaz\net al. [6] provide InceptionTime (a CNN-based model) to\ndeal with time series classiﬁcation problem. Zhao et al. [4]\nuse the InceptionTime model to analyze the intraoperative\ntime-series monitoring data, predicting the post-hysterectomy\nquality of recovery. Wu et al. [8] focus on the Transformer\nmodel and use it in the inﬂuenza prevalence prediction case.\nIII. METHODS\nA. Data analysis and preprocessing\n1) Data characteristics: This work is motivated by the\nanalysis of physiological time series data in EHR, which\nare generally sparse, irregularly sampled, and multivariate.\nThe EHR data contains the real-time physiological status of\npatients [3], such as vital signs or other physiology signals,\nlaboratory values, demographics information, treatments, and\noutcomes. The data contains continuous and categorical data\nwith varied time lengths and different sampling frequencies.\nThe physiology time series are usually sparse, with missing\ndata both in the time dimension and feature dimension. Also,\nthe positive sample size is generally much smaller than\nthe negative one in most clinical prediction domains. The\nimbalanced dataset is a fundamental problem in the clinical\nprediction ﬁeld.\n2) Public datasets:\nPhysionet 2019 challenge dataset\n(early prediction of Sepsis from clinical data) includes hourly\nphysiological time series data consisting of 8 vital signs,\n26 laboratory values, and 6 demographic details of 40,336\nsamples from the MIMIC III database [1]. The time length\nvaries from 8 to 336 time-steps. The outcome is the onset of\nsepsis.\n3) Preprocessing:\na) Missing data: The physiology data is inconsistent\nin the timeline. Meanwhile, missing values are correlated\nwith feature labels. As the physiology data reﬂect the clinical\nstatus of patients, we assume that the values vary little within\na small time window. Thus, we use forward and backward ﬁll\nfor the missing points in the time domain. When the missing\nvalue relates to the feature labels, we ﬁll other speciﬁc values\nwith a special meaning, e.g., -1 or N/A.\nb) Various data length: Data length depends on time\nlength and sampling frequencies. The time lengths of phys-\niology time series are signiﬁcantly different among features\nor patients. Also, the sampling frequencies are different\namong physiology features. The data ﬁlling process includes\ntwo steps: 1) ﬁrstly, utilize forward and backward ﬁlling\naccording to the max sampling frequency. 2) ﬁll each sample\naccording to the max length of all samples with a special\nmeaning, e.g., -1 or N/A.\nc) Continuous and categorical data: Generally, vital\nsigns and laboratory values are continuous data and we use\nnormalization to maintain the contribution of each feature\nand keep the model unbiased. Demographics information\nis mainly categorical data and we use one-hot encoding to\nconvert the categorical data. After the ﬁlling, normalization,\nand one-hot encoding, processed EHR data is formatted for\ntraining and testing.\nd) Imbalanced dataset: The imbalance issue is a com-\nmon problem in the clinical domain: positive samples are\nmuch less than negative ones. For example, the Sepsis\npredction dataset has 40,336 samples with 2,932 positive\npatients, with a rate of 7.27%. Two main methods deal\nwith the imbalance dataset: data augmentation and special\nloss functions. As the data augmentation methods affect the\ndataset size, we use Dice Loss [2] as the loss function of the\nprediction problem.\nB. Models\n1) Deep learning models: Deep learning models are based\non neural networks that are composed of a large number\nof layers and parameters, which can be potentially used for\nextracting features from time series and downstream tasks\nsuch as regression, classiﬁcation, forecasting, and feature\nembedding. We employ 3 different deep learning models\nto directly tackle the physiology time series for Sepsis\nprediction task, including InceptionTime [6], TCN (Temporal\nCNN) [7] and Transformer [8].\na) InceptionTime: InceptionTime is a 1D-CNN-based\ndeep neural network. It consists of a series of Inception mod-\nules followed by a Global Average Pooling layer and a Dense\nlayer with a softmax activation function. In each Inception\nmodule, multi-dimension time series data are transformed\ninto 1D data. Three one-dimension ﬁlters with lengths of 10,\n20, and 40, respectively, are applied simultaneously to the\noutput. Residual blocks are added to mitigate the vanishing\ngradient problem.\nb) TCN: TCN is a variant of the CNN and employs or-\ndinary convolutions and dilations. It is suitable for sequential\ndata with temporality and large receptive ﬁelds. The latent\ncorrelation among series can be learned from this model.\nc) Transformer:\nThe Transformer-based time series\nmodel consists of Transformer encoder layers.The network is\ncomposed of an input layer, a positional encoding layer, and a\nstack of transformer encoder layers. We use one transformer\nencoder layer in this work.\n2) Non-deep learning models: For Non-deep learning\nmodels, we evaluate 7 models with the statistical information\nextracted from raw time series data. Some machine learning\nmodels are inconvenient to be used with raw time-series\ndata, such as logistic regression. Usually, the preprocessing\nis needed to extract the basic (descriptive) statistical infor-\nmation or feature from the time-series data to be used in\nthe machine learning methods [9],[10]. The statistical infor-\nmation includes the maximum, minimum, average, median,\nstandard deviation, quantiles, moments, etc.\nC. Training and Evaluation\n1) Training process: We choose 36 features (8 vital signs,\n26 laboratory values, and 2 demographic details) to predict\nthe sepsis outcome as a binary classiﬁcation. We divided the\ntraining dataset into training sets and test sets. The test sets\nhave 4,033 samples (10% of all dataset sizes) and the training\nsets have 36,303 samples (90% of all dataset sizes). Also,\nthe training sets are sub-sampled from 100 to 36,303 sample\nsize: 100, 200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000,\n10000, 15000, 20000, 25000, 30000, 36303.\n2) Performance metrics and evaluation: For the classi-\nﬁcation task, the most commonly reported metric is the\narea under the receiver operator characteristic curve (AU-\nROC), which combines the information from sensitivity\nand speciﬁcity into a single value. We also report AUPRC\nsince it is a good metric for describing model performance\ngiven imbalanced datasets. And we evaluate the accuracy,\nsensitivity (recall, TPR (true positive rate)), speciﬁcity (TNR,\n1-FPR (false positive rate)), FNR, FPR, NPV, PPV, which\nrepresent actual clinical signiﬁcance [11].\n3) Training dataset size impact: The evaluation results of\nmachine learning models are largely dependent on the train-\ning dataset size. We choose the learning curve approach [12]\nto explore the relationship between the performance and the\ntraining dataset size. The curve model is represented as an\ninverse power law function given by\ny = f(x, b) = 100 + b1 · xb2.\n(1)\nUsing the evaluation results of the models trained with\ndifferent training datasets sizes, we calculate the unknown\nparameters (b1 and b2) with nonlinear weighted least squares\noptimization using the following equation.\nE(b) =\nm\nX\np=1\n(ωp · (tp −yp))2\n=\nm\nX\np=1\n(ωp · (tp −f(xp, b)))2\n=\nm\nX\np=1\n(ωp · rp(b)2)\n= RT WR,\n(2)\nwhere rp is the residual between the real value and the ﬁtted\nfunction.\nIV. RESULT AND DISCUSSION\nA. Comparison of machine learning models’ performance\nFig. 1 contains the evaluation results of the Sepsis predic-\ntion task (a binary classiﬁcation problem) based on different\nmachine learning models.\nOur ﬁndings include:\n• The deep learning models outperform the non-deep\nlearning methods for the AUROC, AUPRC, sensitivity,\nand FNR.\n• For the accuracy, speciﬁcity, NPV, and FPR, both deep\nlearning models and non-deep learning models have\nsimilar performance results.\n• The performance of PPV evaluation metric differs from\ndiverse machine learning models.\n• All machine learning models could be divided into three\ngroups based on performance: (1) deep learning models;\n(2) XGBoost, Random Forest, and Decision Tree; (3)\nGaussian process, KNN, SVM, and logistic regression.\nThe performance is ranked in the order of best to worst:\nGroup 1, Group 2, Group 3.\n• Among non-deep learning models, XGBoost performs\nbest compared to the other non-deep learning models.\nXGBoost could not perform as well as deep learning\nmethods though.\nB. Training dataset size and learning curves\nWe also analyze the relationship between the training\ndataset size and the model performance. Fig. 2 depicts the\nimpact of the training dataset size on the performance of the\nmodels, for the AUROC (upper left), AUPRC (upper right),\nSensitivity (below left), and FNR (below right). Each line\nrepresents a model’s performance according to the increasing\nnumber of training dataset sizes.\nOur ﬁndings include:\n• When the training dataset size is small, the deep learn-\ning models do not outperform the non-deep learning\nmodels, especially for the sensitivity evaluation metrics.\n• With the increased training dataset size, the deep learn-\ning models show an excellent ability comparing with\nnon-deep learning models for all four evaluation met-\nrics.\n• The increased number of training dataset size has a\nlarger impact on Group 1 than Group 2, while no\ninﬂuence on Group 3.\n• With the increased training dataset size, all the models’\nperformance approach saturation.\nV. CONCLUSIONS\nOur systematical and quantitative study showed that deep\nlearning indeed outperforms non-deep learning with spe-\nciﬁc training dataset size (an estimation of a magnitude of\nthousands) and some particular evaluation metrics (AUROC,\nAUPRC, Sensitivity, and FNR). No general effect of machine\nlearning models or deep learning models was observed with\nsmall sample sizes (below estimation of a magnitude of\nthousands). Therefore, deep learning has the potential to be\nused as a tool for clinical prediction.\nThe future direction of our work will extend our quan-\ntitative analysis to investigate the relationship between the\nclinical research performance and the scale of the network,\ndata points, the sampling frequency of raw data, augmenta-\ntion of data, etc. It can assist researchers to determine the\nmodels and the dataset quality for clinical prediction research\nbased on machine learning approaches.\nModel\nAUROC\nAUPRC\nAccuracy\nSensitivity\nSpeciﬁcity\nPPV\nNPV\nFPR\nFNR\nTCN\n0.924\n0.698\n0.956\n0.659\n0.979\n0.715\n0.973\n0.021\n0.341\nInceptionTime\n0.907\n0.713\n0.957\n0.635\n0.982\n0.735\n0.972\n0.018\n0.365\nTransformer\n0.909\n0.572\n0.947\n0.584\n0.976\n0.653\n0.968\n0.024\n0.416\nXGBoost\n0.746\n0.444\n0.955\n0.502\n0.99\n0.803\n0.962\n0.01\n0.498\nRandom Forests\n0.75\n0.459\n0.957\n0.509\n0.992\n0.832\n0.963\n0.008\n0.461\nDecision Tree\n0.726\n0.271\n0.923\n0.495\n0.957\n0.474\n0.96\n0.043\n0.505\nGaussian Process\n0.622\n0.226\n0.937\n0.253\n0.991\n0.679\n0.944\n0.009\n0.747\nKNN\n0.625\n0.248\n0.94\n0.256\n0.994\n0.758\n0.945\n0.006\n0.744\nSVM\n0.609\n0.244\n0.94\n0.222\n0.997\n0.844\n0.942\n0.003\n0.778\nLogistic Regression\n0.631\n0.279\n0.943\n0.266\n0.996\n0.848\n0.945\n0.004\n0.734\nFig. 1.\nPerformance comparison of machine learning models.\n(a) AUROC\n(b) AUPRC\n(c) Sensitivity\n(d) FNR\nFig. 2.\nAUROC, AUPRC, Sensivitity and FNR Learning Curve of different models.\nACKNOWLEDGMENT\nThe authors would like to thank Luyun Qin (SRCB-Ricoh)\nfor his work in data preprocessing.\nREFERENCES\n[1] M. A. Reyna, C. Josef, S. Seyedi, R. Jeter, S. P. Shashikumar, M. B.\nWestover, A. Sharma, S. Nemati, and G. D. Clifford, “Early prediction\nof sepsis from clinical data: the physionet/computing in cardiology\nchallenge 2019,” in 2019 Computing in Cardiology (CinC).\nIEEE,\n2019, pp. Page–1.\n[2] T. Vicar, P. Novotna, J. Hejc, M. Ronzhina, and R. Smisek, “Sepsis\ndetection in sparse clinical data using long short-term memory network\nwith dice loss,” in 2019 Computing in Cardiology (CinC).\nIEEE,\n2019, pp. Page–1.\n[3] H. Harutyunyan, H. Khachatrian, D. C. Kale, G. Ver Steeg, and\nA. Galstyan, “Multitask learning and benchmarking with clinical time\nseries data,” Scientiﬁc data, vol. 6, no. 1, pp. 1–18, 2019.\n[4] X. Zhao, K. Liao, W. Wang, J. Xu, and L. Meng, “Can a\ndeep learning model based on intraoperative time-series monitoring\ndata predict post-hysterectomy quality of recovery?” Perioperative\nMedicine,\nvol.\n10,\nno.\n1,\np.\n8,\n2021.\n[Online].\nAvailable:\nhttps://doi.org/10.1186/s13741-021-00178-4\n[5] Y. Wang, B. Xiao, X. Bi, W. Li, J. Zhang, and X. Ma, “Prediction of\nSepsis from Clinical Data Using LSTM and XGBoost,” p. 4, 2019.\n[6] H. I. Fawaz, B. Lucas, G. Forestier, C. Pelletier, D. F. Schmidt,\nJ. Weber, G. I. Webb, L. Idoumghar, P.-A. Muller, and F. Petitjean,\n“Inceptiontime: Finding alexnet for time series classiﬁcation,” Data\nMining and Knowledge Discovery, vol. 34, no. 6, pp. 1936–1962,\n2020.\n[7] C. Kok, V. Jahmunah, S. L. Oh, X. Zhou, R. Gururajan, X. Tao, K. H.\nCheong, R. Gururajan, F. Molinari, and U. R. Acharya, “Automated\nprediction of sepsis using temporal convolutional network,” Computers\nin Biology and Medicine, vol. 127, p. 103957, 2020.\n[8] N. Wu, B. Green, X. Ben, and S. O’Banion, “Deep transformer models\nfor time series forecasting: The inﬂuenza prevalence case,” arXiv\npreprint arXiv:2001.08317, 2020.\n[9] F. Hatib, Z. Jian, S. Buddi, C. Lee, J. Settels, K. Sibert, J. Rinehart, and\nM. Cannesson, “Machine-learning algorithm to predict hypotension\nbased on high-ﬁdelity arterial pressure waveform analysis,” Anesthe-\nsiology, vol. 129, no. 4, pp. 663–674, 2018.\n[10] C. K. Lee, I. Hofer, E. Gabel, P. Baldi, and M. Cannesson, “Develop-\nment and validation of a deep neural network model for prediction of\npostoperative in-hospital mortality,” Anesthesiology, vol. 129, no. 4,\npp. 649–662, 2018.\n[11] H. B. Wong and G. H. Lim, “Measures of diagnostic accuracy: sensi-\ntivity, speciﬁcity, ppv and npv,” Proceedings of Singapore healthcare,\nvol. 20, no. 4, pp. 316–318, 2011.\n[12] J. Cho, K. Lee, E. Shin, G. Choy, and S. Do, “How much data is\nneeded to train a medical image deep learning system to achieve\nnecessary high accuracy?” arXiv preprint arXiv:1511.06348, 2015.\nREFERENCES\n[1] M. A. Reyna, C. Josef, S. Seyedi, R. Jeter, S. P. Shashikumar, M. B.\nWestover, A. Sharma, S. Nemati, and G. D. Clifford, “Early prediction\nof sepsis from clinical data: the physionet/computing in cardiology\nchallenge 2019,” in 2019 Computing in Cardiology (CinC).\nIEEE,\n2019, pp. Page–1.\n[2] T. Vicar, P. Novotna, J. Hejc, M. Ronzhina, and R. Smisek, “Sepsis\ndetection in sparse clinical data using long short-term memory network\nwith dice loss,” in 2019 Computing in Cardiology (CinC).\nIEEE,\n2019, pp. Page–1.\n[3] H. Harutyunyan, H. Khachatrian, D. C. Kale, G. Ver Steeg, and\nA. Galstyan, “Multitask learning and benchmarking with clinical time\nseries data,” Scientiﬁc data, vol. 6, no. 1, pp. 1–18, 2019.\n[4] X. Zhao, K. Liao, W. Wang, J. Xu, and L. Meng, “Can a\ndeep learning model based on intraoperative time-series monitoring\ndata predict post-hysterectomy quality of recovery?” Perioperative\nMedicine,\nvol.\n10,\nno.\n1,\np.\n8,\n2021.\n[Online].\nAvailable:\nhttps://doi.org/10.1186/s13741-021-00178-4\n[5] Y. Wang, B. Xiao, X. Bi, W. Li, J. Zhang, and X. Ma, “Prediction of\nSepsis from Clinical Data Using LSTM and XGBoost,” p. 4, 2019.\n[6] H. I. Fawaz, B. Lucas, G. Forestier, C. Pelletier, D. F. Schmidt,\nJ. Weber, G. I. Webb, L. Idoumghar, P.-A. Muller, and F. Petitjean,\n“Inceptiontime: Finding alexnet for time series classiﬁcation,” Data\nMining and Knowledge Discovery, vol. 34, no. 6, pp. 1936–1962,\n2020.\n[7] C. Kok, V. Jahmunah, S. L. Oh, X. Zhou, R. Gururajan, X. Tao, K. H.\nCheong, R. Gururajan, F. Molinari, and U. R. Acharya, “Automated\nprediction of sepsis using temporal convolutional network,” Computers\nin Biology and Medicine, vol. 127, p. 103957, 2020.\n[8] N. Wu, B. Green, X. Ben, and S. O’Banion, “Deep transformer models\nfor time series forecasting: The inﬂuenza prevalence case,” arXiv\npreprint arXiv:2001.08317, 2020.\n[9] F. Hatib, Z. Jian, S. Buddi, C. Lee, J. Settels, K. Sibert, J. Rinehart, and\nM. Cannesson, “Machine-learning algorithm to predict hypotension\nbased on high-ﬁdelity arterial pressure waveform analysis,” Anesthe-\nsiology, vol. 129, no. 4, pp. 663–674, 2018.\n[10] C. K. Lee, I. Hofer, E. Gabel, P. Baldi, and M. Cannesson, “Develop-\nment and validation of a deep neural network model for prediction of\npostoperative in-hospital mortality,” Anesthesiology, vol. 129, no. 4,\npp. 649–662, 2018.\n[11] H. B. Wong and G. H. Lim, “Measures of diagnostic accuracy: sensi-\ntivity, speciﬁcity, ppv and npv,” Proceedings of Singapore healthcare,\nvol. 20, no. 4, pp. 316–318, 2011.\n[12] J. Cho, K. Lee, E. Shin, G. Choy, and S. Do, “How much data is\nneeded to train a medical image deep learning system to achieve\nnecessary high accuracy?” arXiv preprint arXiv:1511.06348, 2015.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-11-11",
  "updated": "2022-11-11"
}