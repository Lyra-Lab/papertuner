{
  "id": "http://arxiv.org/abs/2211.09705v1",
  "title": "A Review of Deep Learning Techniques for Protein Function Prediction",
  "authors": [
    "Divyanshu Aggarwal",
    "Yasha Hasija"
  ],
  "abstract": "Deep Learning and big data have shown tremendous success in bioinformatics\nand computational biology in recent years; artificial intelligence methods have\nalso significantly contributed in the task of protein function classification.\nThis review paper analyzes the recent developments in approaches for the task\nof predicting protein function using deep learning. We explain the importance\nof determining the protein function and why automating the following task is\ncrucial. Then, after reviewing the widely used deep learning techniques for\nthis task, we continue our review and highlight the emergence of the modern\nState of The Art (SOTA) deep learning models which have achieved groundbreaking\nresults in the field of computer vision, natural language processing and\nmulti-modal learning in the last few years. We hope that this review will\nprovide a broad view of the current role and advances of deep learning in\nbiological sciences, especially in predicting protein function tasks and\nencourage new researchers to contribute to this area.",
  "text": "A Review of Deep Learning Techniques for Protein \nFunction Prediction \n \nDivyanshu Aggarwal \nDept of Biotechnology,  \nDelhi Technological University, \nNew Delhi, India \ndivyanshuggrwl@gmail.com \n \n \nYasha Hasija \nDept of Biotechnology,  \nDelhi Technological University, \nNew Delhi, India \nyashahasija06@gmail.com (Corresponding author) \n \n \nAbstract—Deep Learning and big data have shown \ntremendous success in bioinformatics and computational biology \nin recent years; artificial intelligence methods have also \nsignificantly contributed in the task of protein function \nclassification. \nThis \nreview \npaper \nanalyzes \nthe \nrecent \ndevelopments in approaches for the task of predicting protein \nfunction using deep learning. We explain the importance of \ndetermining the protein function and why automating the \nfollowing task is crucial. Then, after reviewing the widely used \ndeep learning techniques for this task, we continue our review \nand highlight the emergence of the modern State of The Art \n(SOTA) \ndeep \nlearning \nmodels \nwhich \nhave \nachieved \ngroundbreaking results in the field of computer vision, natural \nlanguage processing and multi-modal learning in the last few \nyears. We hope that this review will provide a broad view of the \ncurrent role and advances of deep learning in biological \nsciences, especially in predicting protein function tasks and \nencourage new researchers to contribute to this area. \nIndex Terms—Bioinformatics, Big Data, Protein Function \nClassification, Deep Learning, Computational Biology \nI. \nINTRODUCTION \nProteins play a large role in the cellular machinery of the \nliving organism. It is very important to know the function \nprotein while conducting any proteomic research on that \nparticular protein. However, more than 40% of the NCBI \ndatabase's protein sequences have no assigned function as of \n2013 [1]. This can be induced by the fact that it is expensive, \nhigh in processing time, and challenging to determine a \nprotein's function using functional assays. [1] This arises the \nneed for a computational method to determine a protein's \nfunction from the raw data obtained from the high \nthroughput techniques, including but not limited to protein \nsequence, protein structure, gene expression profile and \nprotein-protein interaction data. \nTraditional approaches to classification for protein \nfunction attempt to identify the evolutionary relationship \nbetween a new protein and a query protein [1]. A high \nsequence similarity score can suggest a high probability of 2 \nproteins originating from a single evolutionary source [1]. \nHowever, it is well established that proteins with high \nsequence alignment or sequence similarity score may or may \nnot show similar functions.[2] Such erroneous annotations \nmay lead to propagation and amplification in large databases \nquickly. \nSuch errors are not only due to basic transfer strategies \nbased on homology, but also due to the manual data analysis \n \nGraph 1. Growth of Gene Bank [3] \nProcess. Such problems have also been tried to tackle \nusing different data, including but not limited to 3D structure \nsimilarity, gene expression profile similarity and genomic \nexpression profile similarity. \nIn the past few years, the growth of biological databases \nand significant advancements in computing resources creates \nan opportunity for the scientific community to build novel \ndeep learning models and architectures to tackle such \nproblems. As the latest trend in recent developments in \nnatural language processing research and sequential data \npreprocessing \nusing \ndeep \nneural \nnetworks \n(DNN), \nconvolutional neural networks (CNN), recurrent neural \nnetworks (RNN), long- term and short-term memory \n(LSTM), and attention-based transformer models, it is \nimportant to evaluate the suitability, feasibility, sustainability \nand explain ability of such models for the given task. In the \nsense of protein function classification, this analysis aims to \nprovide an overview of these techniques. \n \nGraph 2. Growth of Protein Data Bank [4] \nThis paper outlines the latest trends in the development \nof automated annotation of protein function from raw data \nusing deep learning. The flow of paper will be as follows; \nfirst, we discuss about the approached for prediction protein \nfunction from raw without using deep learning. (section 2). \nNext, we describe the recent deep learning and machine \nlearning advances and how they are used in the protein \n2021 2nd International Conference for Emerging Technology (INCET) \nBelgaum, India. May 21-23, 2021\n978-1-7281-7029-9/21/$31.00 ©2021 IEEE\n1\nfunction classification perspective (Section 3). We discuss \nrelated work and concluding remarks in the section after that \n(Section 4 and 5, respectively). \nII. \nCLASSICAL COMPUTATIONAL TECHNIQUES \nFOR PROTEIN FUNCTION PREDICTION \nUnderstanding how the cell functions enables one to \nlearn how the protein works. Usually, a protein function is \nspecified as the molecular function of the protein; material \ntransport, gene regulation, catalysis of biochemical reactions \n(enzymes) and others are some protein functions. The protein \nfunctions in many ways; it can interact with other proteins in \norder to perform its roles biologically. The mutation of the \namino acid sequence may result in certain diseases from a \nphenotypical perspective. Factors like, but not limited to, \nsuch as, the environment of the cell, for example, \ntemperature, subcellular protein position, may also influence \nthe role of a protein. For standardizing functional annotation \nand enabling computer processing to explain various aspects \nof protein functions, several classification schemes have \nsuggested specific vocabulary. Gene Ontology is the most \ngenerally recognized (GO). Three aspects of protein \nfunctions are defined by GO: molecular structure, biological \nmechanism, and cellular positioning. An ontology is \nconstructed as a directed acyclic graph, here vertices reflect \nfunction of protein and their relationships are represented by \nthe edges. For their practical predictions from this machine-\nreadable vocabulary rather than natural language style, \nmethods of prediction using computation should have a \nuniform, output, which can show errors within the annotation \nlevel. [5], [6], [7] \nThe most common means of identifying a protein's \nfunction by using the already annotated sequences is through \nBLAST. The query sequence is matched with the sequences \nfrom the database, and the similarity score is used to estimate \nthe function of the protein with different sequences. BLAST \nuses heuristic algorithms to match strings. The genes with \nhigh similarity scores are said to be homologs. It is assumed \nthat protein sequences with similar homology have a similar \nfunction, however, sequence matching is not the most \naccurate way of determining the protein's function; 2 highly \nsimilar protein sequences are not supposed to have same \nGene Ontology in all the scenarios [8]. \nGene fusion and phylogenetic profile are another 2 \nfeatures which can give good accuracy in the task of \nprediction protein function. Gene neighborhood strategies \nuse the idea that proteins whose translated genes are located \nnear each other on a chromosome are deemed to be linked in \nfunctional way in different genomes. In one genome that is \nfused into another genome to form a single gene, the idea \nthat group of genes are required focuses on gene fusion-\nbased approaches. Another ground-breaking method for \npredicting functional relationships is to compare the \nphylogenetic profiles of proteins. A collection of bits that \nindicate a homologous presence or absence in each genome \nis known as protein phylogenetic profile. Similar profiling \nmay also imply that gene which has produced the proteins \nhave \nevolved \ntogether \nand \nmay \nindicate \nfunction \nconservation. [9], [10] \nIt is also possible to use motifs and domain details. A \ndomain is a part of the amino acid sequence of proteins that \nfolds independently of the rest of the structure into a stable \nstructure. A motif is a very short stretch of sequences of \namino acids that theoretically encodes the structure of the \nprotein. These are used for the metadata of protein, such as \nlength of sequence, composition of amino acids, and \nphysicochemical properties of a sequence. [2] \nHowever, the structure of the protein is the most \ninformative aspect of a protein to describe the function of a \nprotein. Due to the Structural Genomics Initiative, we today \nhave a database called protein data bank, which contains the \nstructure of millions of protein sequences. The query protein \nis structurally matched. The protein sequences are matched \nin 2 ways, globally and locally. In global alignment, the \nentire structure is matched, which in local structural \nalignment, local regions like motifs and domains are \nmatched since they are a better criterion for identifying the \nprotein's function.  The motifs and domains can capture the \nintramolecular interactions of a protein, while global \nalignment methods are able to identify the intermolecular \ninteractions. Intermolecular interactions are generally \nperformed on the surface, which are responsible for a \nprotein's biochemical properties. [11] \n \nFig. 1. . \nWith \nthe \nemergence \nof \nhigh-throughput \nprotein \nsequencing technologies, vast quantities of data have been \nmade available in biological gene expression and protein-\nprotein interaction databases. The gene expression is \nmeasured by the amount of the protein produced by a gene in \nspecific conditions that performs a particular gene. It is \nprobable that proteins whose genes are co-expressed have \nthe same role. Proteins subsequently perform a particular \nrole by communicating with other proteins; hence similar \nprotein-protein interactions can lead to similar protein \nfunctions. \nIII. \nUSING DEEP LEARNING FOR PROTEIN \nFUNCTION PREDICTION \nDeep learning has achieved significant momentum in the \npast few years with the advancements in the computing \npower of the central processing units and graphical \nprocessing units and the rapid expansion of biological \ndatabases. Deep learning algorithms are capable of \nextracting hidden features and patterns from the data are also \nable to form non-linear classification boundaries. [12], [13] \nDeep learning also scales well with the growth in data \nvolumes, and its application to computer vision and natural \nlanguage processing can be adapted to the structure of 3D \nproteins and protein sequences, respectively. With the recent \nadvances in neural graph networks, they can also be applied \nin protein-protein interaction and phylogenetic databases. \n2\nA. Data Sources \nThere are multiple data sources from where the data can    \nbe obtained to train the model from such tasks. Uniprot KB \nand PDB remains to be the open databases that can be freely \naccessed. [14], [15] The data from such databases can be \ndownloaded in bulk and can be cleaned and preprocessed to \ntrain such models. However, the CAFA dataset released by \nThe Function Special Interest Group is a dataset mainly \nmeant for the challenge of building computational models for \nprotein function classification. [16] However some models \nhave also used STRING db in the past. \nB. Deep Learning Paradigm \nThis paper presents the analysis of modern deep learning \ntechniques in the context of protein function classification. \nThe classification of the function of a protein can be \nclassified as   a supervised machine learning task. Since \nneural networks in general settings cannot use unsupervised \nlearning tasks, we cannot use clustering techniques or other \nunsupervised techniques. Deep learning research has \ngathered momentum in the past few years. This can be \nattributed to the advancements in modern machines' \ncomputational capacity and availability of large publicly \navailable databases. The sector has had a major influence, \nincluding, to name a few, banking, healthcare, social media. \nRecent developments in the processing of natural language \nand computer vision by deep learning can have an immense \nimpact on genomics and proteomics research. [13] \nRecent developments in algorithm development, such as \nDNNs, RNNs, CNNs, LSTMs, and attention models for \nimage and text data, the State of the art and near-human \nprecision have been achieved already, we can only wonder \nhow they  can do in such biological tasks where the \nenormous amounts of similar data are accessible. However, \nthe challenge of data representation arises while solving such \ntasks using deep learn- ing. While there are pre-trained word \nembeddings available for natural language, it is difficult to \nsay so for biological sequence databases. However, \nresearchers have provided some protein embedding in recent \nyears, and the quality of such models is not on par with \nnatural language models, trained with similar volumes of \ndata. The reason might be the relatively small vocabulary \nsize of protein sequences with 20 amino acids known, \nwhereas more than 100,000 words are taken in the \nvocabulary while training a natural language model. [17] The \nsubsequent sections will explore the recent advancements in \ndeep learning and their implication in the protein \nclassification task. \nC. Protein Function prediction using sequence only \nWith the advances in text classification tasks using deep \nlearning, one can only get curious about how these \ntechniques \ncan \nbe \napplied \nto \nbiological \nsequence \nclassification. Related biological problems have been further \naddressed by developments in word embedding methods, 1D \nCNN, FastText word embedding, DNNs, and RNNs by \nadapting methods from natural language processing tasks. \nWe will address some of the methods mentioned in this \nsubsection to predict a protein's function solely from raw \nsequence data. \n1) Combination of 1D Convolutional Neural Network \nand Deep Neural Networks: Convolutional Neural Networks \nare the deep learning algorithms capable of extracting unique \ngeographic features from the data. They are also used to \nreduce the data's size by amplifying and representing \nlocalized features in the final representation. The \nconvolutional Kernel translates over an image or text and \ncaptures the localized information during translation. A 1D \nConvolutional Neural Network convolves through a protein \nsequence representation in 1 dimension. [18], [19], [20] \n \n \nFig. 2. Illustration of CNN and DNN for Function Prediction \nThe convolutional Kernel is translated over the word \nembedding of the sequence, concatenated word byword for a \nsequence. There are several alternatives available for natural \nlanguages like FastText or word2vec, while for protein \nsequences, ProtVec or prot2vec is a popular choice. [21], \n[22], [23] \n2) Using Recurrent Neural Networks: Recurrent neural \nnetworks are neural networks that can identify the temporal \ncharacteristics of the sequence, i.e., a network graph is a \ngraph where a directed graph forms connection between \nnodes along a time sequence. There are dynamic networks. \nThey are useful in learning features from variable length \nfeatures. This makes them useful for processing variable-\nlength sequences in the input. Such models can also be used \nfor sequence-to-sequence tasks like machine translation. \n \nThe modified version of RNN is known as Long Term \nShort Memory (LSTM). LSTMs are capable of storing \nparticular temporal features in the hidden State of the cell. \n[24] This makes it more accurate for longer sequences and \nsequences with high length variability. The LSTM also \ntackles the problem of vanishing and exploding gradient \nproblem better than vanilla RNNs. [12], [24], [25] \n \nFig. 3. Schematic Diagram of protein function classification using \nRecurrent Neural Networks \nBi-LSTMs, as the name suggests, are bidirectional LSTM \nneural network architectures. These can capture the bidi- \nrectional features of the sequence. Hence, they can have \nadvantages in tasks with bidirectional nature like sequence \nclassification. These models can be used by feeding \nembedding layers directly to the LSTM models, where they \nextract out  the sequential features, and they can be \nconcatenated or added further. The resulting vector is then \nfed to a feed-forward layer or multiple feed-forward layers, \nwhich can then be passed through a SoftMax layer for the \nclassification task. \n3) Transformers for Protein Sequence Classification: \nWith the introduction of attention-based models in 2017 by \ngoogle research, natural language processing has changed \ndrastically. The attention-based transformers can process \nlong sequences much better than LSTMs and can utilize the \nparallel GPU architecture for faster and more cost-effective \ntraining. The Transformer models can extract out location-\nspecific features and give weights to tokens based on \n3\ncontext relevance. [26] \nThe transformer models scale up well with the increasing \ndata and the increasing number of parameters. With GPT and \nBERT's introduction, large language models are becoming \nmore and more mainstream with time. [27], [28] The models \ncan grasp the link between sequence tokens very well and    \nare the present State of the art for most NLP assignments. \nMoreover, these models can also form word embeddings \nthrough self-supervised tasks like masked language modeling \nand next-word prediction. [17] These representations can \nthen be used to for finetuning for the number of other \nmainstream tasks. Such self-supervised learning techniques \ncan also be used for protein sequences as demonstrated by \nprotBERT and proBERTa. [29], [30] \nD. Multi-modal Deep Learning for Protein Function \nClassifi-cation \nIn past few years, multi-modal methods of deep learning \nhave gained huge traction. A basic multi-modal deep \nlearning technique can be performed by combining the image \nfeatures and text features. The researchers have previously \nused sequence data, genomic expression, 3D structures, and \ndata on protein- protein interaction to predict the activity of \nthe protein. With the advances in Natural Language \nProcessing and Computer Vision, such techniques seem \npromising for the Particular Task. The Machines can learn in \ncognizance with different data representations to make \npredictions. The sequence data can provide information \nabout the amino acid sequences and their positioning, while \n3D structure can show structural domains and motifs, which \nare a significant indicator of the protein's function. The \ngenomic expression data can capture the patterns in the \ngenomic expression and the corresponding GO annotation, \nwhile protein-protein interaction databases are useful for \nfinding the patterns in protein-to-protein interactions. [31], \n[32], [33], [34] \n \nFig. 4. Multi-modal Deep Learning Illustration. \nThe Multi-modal deep learning techniques, given the \nhigh compute power available these days, seems promising \nfor such tasks. With the increase in high throughput \ntechnologies, the database for such features is only expected \nto grow further, providing us with more data for training \nthese models. There is also a significant momentum gained \nin multi-modal deep learning research, which will further \ndrive such models' accuracy. \nE. Using Autoencoders \nA few researchers in the past have also tried using \ndenoising autoencoders for extracting features from the \nsequence and structure alike. [31], [35], [36], [37] These \nfeatures are then fed into any other classifier of choice. This \ndenoising autoencoder is trained to produce denoised outputs \nthat are generated randomly with some average and standard \ndeviation according to a normal distribution. However, the \ntechnique is not just limited to multi-modal techniques. \nMasked language modeling is shown in protBERT, and \nProtBERTa is also denoising autoencoder task applied on \nraw protein sequence only. [29], [30] \n \nFig. 5. Autoencoder model architecture. \nIV. \nRELATED WORK \nWe reviewed 9 recent models built by researchers around \nthe world for this task. We reported AUC score of each \nmodel with respect to the CAFA challenge and human and \nyeast STRING networks metric. [16], [38] We have reported \nthe methodologies and the resulting Fmax score in table 1. \nAccording to figure 6, the 1D CNN combined with DNN \nseems to be the clear preference for function prediction task. \nThe models are simple and perform well in the sequence \nclassification tasks. At the same time, the transformer models \nseem to be least explored for this task. \n \n \nFig. 6. Distribution of all the techniques used by the model reviewed \nmodels \nFrom the above data we can infer that simple techniques \nlike 1D CNN + DNN perform well on the raw sequence data. \nThey can be improved by combining with PPI data; gene \nexpression data are alike. Around the same time, SOTA NLP \nmodels such as transformers are not on par with other \nconventional techniques. However, there is not much \nresearch that is done on transformer models for such \ndownstream tasks. It shall be interesting to see the \nimplications of such models further in the future for such \ndownstream tasks, especially the use of protBERT and \nPRoBERTa pre-trained models finetuned for protein function \nclassification. [29], [30] \n \n \n4\nTABLE I.  \nRECENT MODELS AND THEIR PERFORMANCE IN THE PROTEIN FUNCTION CLASSIFICATION \nModel \nAuthors \nData Published \nMethods Used \nFmax \nValidation Dataset \nDeepGO \nKulmanov et al. \nFebruary 2018 \nCNN+DNN \n0.47 \nCAFA3 \nDeepNF \nGligorijevic et al. \nJune 2018 \nMulti-modal Deep Learn- \n0.42 \nSTRING \ning \nDeepPred \nRifaioglu et al. \nMay 2019 \nHierarchical DNN \n0.50 \nCAFA3 \nDeepGOPlus \nKulmanov et al. \nJuly 2019 \nCNN+DNN \n0.54 \nCAFA3 \nUDSMProt \nStrodthoff et al. \nJanuary 2020 \nLSTMs \n0.58 \nCAFA3 \nSDN2GO \nCai et al. \nApril 2020 \nCNN+DNN \n0.56 \nCAFA3 \nMultiPredGO \nGiri et al. \nSeptember 2020 \nMulti-modal Deep Learn- \n0.37 \nCAFA3 \ning \nTALE+ \nCao et al. \nSeptember 2020 \nTransformer \n0.67 \nCAFA3 \nFmax Scores are based on the Molecular Function Prediction of the models. \n \nV. \nCONCLUDING REMARKS \nIn the world of bioinformatics, developments in deep \nlearn- ing have had a huge influence. The researchers can \nnow make sophisticated deep learning Models that can \nderive concealed characteristics from raw biological data. \nThis can remove the requirement of hand-crafted features for \nmachine learning tasks. Moreover, such models can be \nscaled well with increasing data and compute capacity. [42] \nThus, such models can be monumental in building an \nautomated system for annotating protein functions through \nraw data. \nIt would be interesting in the future to see how some pre-\ntrained models like prot-BERT [30] and PRoBERTa [43] \nperforms when finetuned for the downstream tasks for \nprotein \nfunction \nprediction. \nSuch \nprotein \nvector \nrepresentations are said to be SOTA in the self-supervised \ntask for masked language modelling. The recent ESM-1b \nprotein language mode by Facebook AI Research is also \nanother contender. [44]  With recent advances in multi-\nmodal deep learning and the production of multi-dimensional \ndata in biological databases, the characteristics derived from \nvarious types of raw data, such as 3D structure, protein-\nprotein interaction, are used to predict protein function, the \ngene expression profile can be paired with current SOTA \nprotein sequence representations, which is likely to boost the \nFmax scores on the CAFA challenge further. \nREFERENCES \n[1] \nJ. Bernardes and C. Pedreira, \"A Review of  Protein  Function \nPrediction  Under  Machine  Learning  Perspective,\" Recent   Patents \non Biotechnology, vol. 7, no. 2, pp. 122–141,  2013.  [Online]. \nAvailable: \n10.2174/18722083113079990006;https://dx.doi.org/10.2174/ \n18722083113079990006 \n[2] \nF. Eisenhaber, 2013. \n[3] \nW. \nGenbank \nand \nStatistics, \n2021. \n[Online]. \nAvailable: \nhttps://www.ncbi. nlm.nih.gov/genbank/statistics/.Accessed28 \n[4] \n2021. [Online]. Available: http://www.wwpdb.org/stats/download. \nAccessed28 \n[5] \nM. Ashburner, C. A. Ball, J. A. Blake, D. Botstein, H. Butler, J.  M.  \nCherry,  A.  P.   Davis,  K.  Dolinski,  S.  S.  Dwight,  J.  T. Eppig,  \nM.  A.  Harris,  D.  P.  Hill,  L.  Issel-Tarver,  A.  Kasarskis,  S.  \nLewis, J.  C.   Matese,  J.  E.   Richardson,  M.   Ringwald,  G.  M.   \nRubin,  and G. Sherlock, \"Gene Ontology: tool for the unification of \nbiology,\" Nature Genetics, vol. 25, no. 1, pp. 25–29, 2000. [Online]. \nAvailable: 10.1038/75556;https://dx.doi.org/10.1038/75556 \n[6] \nA. D. Diehl, J. A. Lee, R. H. Scheuermann, and J. A. Blake, \"Ontology \ndevelopment for biological systems: immunology,\" Bioinformatics,    \nvol.    23,    no.    7,    pp.    913–  915, 2007. [Online]. Available: \n10.1093/bioinformatics/btm029;https: \n//dx.doi.org/10.1093/bioinformatics/btm029 \n[7] \nJ. D. Osborne, J. Flatow, M. Holko, S. M. Lin, W. A. Kibbe, L. Zhu, \nM. I. Danila, G. Feng, and R. L. Chisholm, \"Annotating the human \ngenome with Disease Ontology,\" BMC Genomics, vol. 10, no. Suppl \n1, pp. S6–S6, 2009. [Online]. Available: 10.1186/1471-2164-10-s1-\ns6;https: //dx.doi.org/10.1186/1471-2164-10-s1-s6 \n[8] \nS. F. Altschul, W. Gish, W. Miller, E. W. Myers, and D. J. Lipman, \n\"Basic local alignment search tool,\" Journal of Molecular Biology, \nvol. 215, no. 3, pp. 403–410, 1990. [Online]. Available: \n10.1016/s0022- \n2836(05)80360-2;https://dx.doi.org/10.1016/s0022-\n2836(05)80360-2 \n[9] \nJ. Huerta-Cepas, D. Szklarczyk, K. Forslund, H. Cook, D. Heller, M.  \nC.  Walter,  T.   Rattei,  D.  R.  Mende,  S.  Sunagawa,  M.  Kuhn, L. \nJ. Jensen, C. von Mering, and P.  Bork, \"eggNOG  4.5:  a  hierarchical \northology framework with improved functional annotations for \neukaryotic, prokaryotic and viral sequences,\" Nucleic Acids \nResearch, vol. 44, no. D1, pp. D286–D293, 2016. [Online]. Available: \n10.1093/nar/gkv1248;https://dx.doi.org/10.1093/nar/gkv1248 \n[10] B. E, \"Genome-scale phylogenetic function annotation of large and \ndiverse protein families,\" Genome Res, vol. 21, pp. 1969–1980. \n[11] D. Cozzetto, F. Minneci, H. Currant, and D. T. Jones, \"FFPred 3: \nfeature-based function prediction for all Gene Ontology domains,\" \nScientific Reports, vol. 6, no. 1, 2016. [Online]. Available: 10.1038/ \nsrep31865;https://dx.doi.org/10.1038/srep31865 \n[12] Y. LeCun, Y. Bengio, and G. Hinton, \"Deep learning,\" Nature,  \nvol. 521, no. 7553, pp. 436–444, 2015. [Online]. Available: \n10.1038/nature14539;https://dx.doi.org/10.1038/nature14539 \n[13] C. Cao, F. Liu, H. Tan,  D.  Song,  W.  Shu,  W.  Li,  Y. Zhou, X. Bo, \nand Z. Xie, \"Deep Learning and Its Applications in Biomedicine,\" \nProteomics Bioinforma, vol. 16, pp. 17–32, 2018. \n[14] R. Apweiler, \"The Universal Protein resource (UniProt),\" Nucleic \nAcids Res, vol. 36, 2008. \n[15] H. Berman, K. Henrick, and H. Nakamura, \"Announcing  the  \nworldwide Protein Data Bank,\" Nature Structural &  Molecular \nBiology, vol. 10, no. 12, pp. 980–980, 2003. [Online]. Available: \n10.1038/nsb1203-980;https://dx.doi.org/10.1038/nsb1203-980 \n[16] A. N. Zhou, Y. Jiang, T. R. Bergquist, A. J. Lee, B. Z. Kacsoh, W. \nCrocker, K. A. Lewis, G. Georghiou, H. N. Nguyen, M. N. Hamid, L. \nDavis, T. Dogan, V.  Atalay, A. S. Rifaioglu, A. Dalkiran, R. Cetin-\nAtalay, C. Zhang, R. L. Hurto, P.  L. Freddolino, Y.  Zhang, P. Bhat, \nF. Supek, J. M. Fernández, B. Gemovic, V. R. Perovic, R. S. \nDavidovic´, N. Sumonja, N. Veljkovic, E. Asgari, M. Mofrad, G. \nProfiti, C.  Savojardo,  P.  L.  Martelli,  R.  Casadio,  F.  Boecker,  I.  \nKahanda, N. Thurlby, A. C. Mchardy, A. Renaux, R. Saidi, J. Gough, \nA. A. Freitas, M. Antczak, F. Fabris, M. N. Wass, J. Hou, J. Cheng, J. \nHou, Z. Wang,  A. E. Romero, A. Paccanaro, H. Yang,  T.  Goldberg, \nC. Zhao, L. Holm, P. Törönen, A. J. Medlar, E. Zosa, I. Borukhov, I. \nNovikov, A. Wilkins, O. Lichtarge, P. H. Chi, W. C. Tseng, M. \nLinial, P. W. Rose,C. Dessimoz, V. Vidulin, S. Dzeroski, I. Sillitoe, \nS. Das, J. G. Lees, C. T. Jones, C. Wan, D. Cozzetto, R. Fa, M. \nTorres, A. W. Vesztrocy, J.  M.  Rodriguez,  M.  L.  Tress,  M.  \nFrasca,  M.  Notaro,  G.  Grossi, A. Petrini, M. Re, G. Valentini, M. \nMesiti, D. B. Roche, J. Reeb, D. W. Ritchie, S. Aridhi, S. Z. Alborzi, \nM. D. Devignes, D. Koo, R. Bonneau, V.  Gligorijevic´,  M.  Barot,  \nH.  Fang,  S.  Toppo,  E.  Lavezzo,  M.  Falda, M.  Berselli,  S.  \nTosatto,  M.  Carraro,  D.  Piovesan,  H.  U.  Rehman, Q. Mao, S. \nZhang, S. Vucetic, G. S. Black, J. D. Larsen, D. J. Omdahl, A. R. \nSagers, L. W.  Suh, E. Dayton, J. B. Mcguffin, L. J.  Brackenridge, D. \nA. Babbitt, P.  C. Yunes, J. M. Fontana, P.  Zhang, F. Zhu, S. You, R. \nZhang, Z. Dai, S. Yao, S. Tian, W. Cao, R. Chandler, C. Amezola, M.  \nJohnson,  D.  Chang,  J.  M.  Liao,  W.  H.  Liu,  Y.   W.  Pascarelli, S. \nFrank, Y. Hoehndorf, R. Kulmanov, M. Boudellioua, I. Politano, G, \nD. Carlo, S. Benso, A. Hakala, K. Ginter, F. Mehryary, F. Kaewphan, \nS.  Björne,  J.  Moen,  H.  Tolvanen,  M.  Salakoski,  T.  Kihara,  D. \n5\nJain, A. Šmuc, T. Altenhoff, A. Ben-Hur, A. Rost, B. Brenner, S. E. \nOrengo, C. A. Jeffery, C. J. Bosco, G. Hogan, D. A. Martin, M. J. \nO'donovan, C. Mooney, S. D. Greene, C. S. Radivojac, P. Friedberg, \nand I, 2019. [Online]. Available: https://doi.org/10.1101/653105 \n[17] J. Vig, A. Madani, L. R. Varshney, C. Xiong, R. Socher, and N. F. \nRajani, 2020. \n[18] M. Kulmanov and R. Hoehndorf, \"DeepGOPlus: Improved protein \nfunction prediction from sequence,\" Bioinformatics, vol. 36, pp. 422– \n429, 2020. \n[19] M. Kulmanov, M. A. Khan, and R. Hoehndorf, \"DeepGO: predicting \nprotein functions from sequence and interactions using a deep \nontology-aware classifier,\" Bioinformatics, vol. 34, no. 4, pp.  660–  \n668, 2018. [Online]. Available: 10.1093/bioinformatics/btx624;https: \n//dx.doi.org/10.1093/bioinformatics/btx624 \n[20] Y. Cai, J. Wang, and L. Deng, \"SDN2GO: An Integrated Deep \nLearning Model for Protein Function Prediction,\" Frontiers in  \nBioengineering and Biotechnology, vol. 8, pp. 1–11, 2020. [Online]. \nAvailable: \n10.3389/fbioe.2020.00391;https://dx.doi.org/10.3389/fbioe.2020.0039\n1 \n[21] E. Asgari and M. R. K. Mofrad, \"Continuous Distributed \nRepresentation of  Biological  Sequences  for  Deep  Proteomics  and  \nGenomics,\" PLOS ONE, vol. 10, no. 11, pp. e0 141 287–e0 141 287,  \n2015. \n[Online]. \nAvailable: \n10.1371/journal.pone.0141287;https://dx.doi.org/ \n10.1371/journal.pone.0141287 \n[22] X. Lin, \"DeepGS: Deep Representation Learning of Graphs and Se- \nquences for Drug-Target Binding Affinity Prediction,\" Front Artif \nIntell Appl, vol. 325, pp. 1301–1308, 2020. \n[23] 2021. [Online]. Available: https://github.com/yotamfr/prot2vec. \nAccessed28 \n[24] X. L. Liu, 2017. [Online]. Available: https://doi.org/10.1101/103994 \n[25] S. Hochreiter and J. Schmidhuber, \"Long Short-Term  Memory,\" \nNeural Computation, vol. 9, no. 8, pp. 1735–1780, 1997. [Online]. \nAvailable: \n10.1162/neco.1997.9.8.1735;https://dx.doi.org/10.1162/neco. \n1997.9.8.1735 \n[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. \nGomez, Ł. Kaiser, and I. Polosukhin, \"Attention is all you need,\" \nAdvances in Neural Information Processing Systems. Neural \ninformation processing systems foundation, pp. 5999–6009, 2017. \n[27] A. Radford, J. Wu, R. Child, D. Luan, and D. Amodei.  \n[28] J. Devlin, M. W. Chang, K. Lee, K. T.  Google, A. I. Language, and \nBert.  \n[29] S. C. Huang, L. Martinez-Nunez, R. T. Rupani, H. Platé, M. Niranjan, \nM. Chambers, R. C. Howarth, P. H. Sanchez-Elsner, T. Azodi, C. B. \nTang, J. Shiu, S. H. Senior, A. W.  Evans, R. Jumper, J. Kirkpatrick, \nJ. Sifre, L. Green, T. Qin, C. Žídek, A. Nelson, A. Bridgland, A. \nPene- dones, H. Petersen, S. Simonyan, K. Crossan, S. Kohli, P. \nJones, D. T. Silver, D. Kavukcuoglu, K. Hassabis, D. Vig, J.  Madani,  \nA. Varsh- ney,  L.  R.  Xiong,  C.  Socher,  R.  Rajani,  N.  F.  Rives,  \nA.  Goyal, S. Meier, J. Guo, D. Ott, M. Zitnick, C. L. Ma, J. Fergus, \nR. Elnaggar, A. Heinzinger, M. Dallago, C. Rost, B. Shanehsazzadeh, \nA. Belanger, D. Dohan, D. Only, U. Madani, A. Mccann, B. Naik, N. \nKeskar, N. S. Anand, N. Eguchi, R. R. Huang, P.  S. Socher, R. Lu, \nA. X. Zhang, H. Ghassemi, M. Moses, A. Nambiar, A. Liu, S. Heflin, \nM. Maslov, S. Hopkins, and M, pp. 1–11. \n[30] A. Elnaggar, M. Heinzinger, C. Dallago, G. Rehawi, Y. Wang, L. \nJones, Gibbs, T. Feher, C. Angerer, M. Steinegger, D. Bhowmik, B. \nRost, and Prottrans. \n[31] V.  Gligorijevic',  M.  Barot,  and  R.  Bonneau, \"deepNF:  deep  \nnetwork fusion for protein function prediction,\" Bioinformatics, vol. \n34, \nno. \n22, \npp. \n3873–3881, \n2018. \n[Online]. \nAvailable: \n10.1093/bioinformatics/ \nbty440;https://dx.doi.org/10.1093/bioinformatics/bty440 \n[32] T. Baltrušaitis, C. Ahuja, L. P. Morency, and Multimodal. \n[33] S. J. Giri, P. Dutta, P. Halani, and S. Saha, \"MultiPredGO: Deep \nMulti-Modal Protein Function Prediction by Amalgamating Protein \nStructure, Sequence, and Interaction,\" IEEE Journal of Biomedical \nand Health Informatics, vol. 2194, pp. 1–1, 2020. [Online]. Available: \n10. \n1109/jbhi.2020.3022806;https://dx.doi.org/10.1109/jbhi.2020.302280\n6 \n[34] Q. Mcnamara, D. L. Vega, A. Yarkoni, and T, \"Developing a compre- \nhensive framework for multi-modal feature extraction,\" Proceedings \nof the ACM SIGKDD International Conference on Knowledge \nDiscovery and Data Mining, pp. 1567–1574, 2017. \n[35] R. Cao, C. Freitas, L. Chan, M. Sun, H. Jiang, and Z. Chen, \n\"ProLanGO: Protein Function Prediction Using Neural Machine \nTranslation Based  on a Recurrent Neural Network,\" Molecules, vol. \n22, no. 10, pp. 1732– 1732, 2017. [Online]. Available: \n10.3390/molecules22101732;https: \n//dx.doi.org/10.3390/molecules22101732 \n[36] R. Bonetta and G. Valentino, \"Machine  learning  techniques  for \nprotein function prediction,\" Proteins: Structure, Function, and \nBioinformatics, vol. 88, no. 3, pp. 397–413, 2020. [Online]. \nAvailable: 10.1002/prot.25832;https://dx.doi.org/10.1002/prot.25832 \n[37] Q. Meng, D. Catchpoole, D. Skillicorn, and P. J. Kennedy, Relational \nAutoencoder for Feature Extraction. \n[38] D. Szklarczyk, A. L. Gable, D. Lyon, A. Junge, S. Wyder, J. Huerta- \nCepas, M. Simonovic, N. T. Doncheva, J. H. Morris, P. Bork, L. J. \nJensen, and C. von Mering, \"STRING v11: protein–protein \nassociation networks with increased coverage, supporting functional \ndiscovery in genome-wide experimental datasets,\" Nucleic Acids \nResearch, vol. 47, no. D1, pp. D607–D613, 2019. [Online]. Available: \n10.1093/nar/gky1131;https://dx.doi.org/10.1093/nar/gky1131 \n[39] A.   S.   Rifaioglu,   T.   Dog˘an,   M.   J.   Martin,   R.   Cetin-Atalay,   \nand V. Atalay, \"DEEPred: Automated Protein Function Prediction \nwith Multi-task Feed-forward Deep Neural Networks,\" Scientific \nReports, vol. 9, no. 1, pp. 1–16, 2019. [Online]. Available: \n10.1038/s41598- \n019-43708-3;https://dx.doi.org/10.1038/s41598-\n019-43708-3 \n[40] N. Strodthoff, P. Wagner, M. Wenzel, and W. Samek, 2019. [Online]. \nAvailable: https://doi.org/10.1101/704874 \n[41] Y. \nCao \nand \nY. \nShen, \n2020. \n[Online]. \nAvailable: \nhttps://doi.org/10.1101/ 2020.09.27.315937 \n[42] J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. \nKianinejad, Patwary, Y. Yang, and Y. Zhou, 2017. \n[43] A. Nambiar, S. Liu, M. Hopkins, M. Heflin, S. Maslov, and A. Ritz, \n2020. \n[44] A. Rives, S. Goyal, J. Meier, D. Guo, M. Ott, C. L. Zitnick, J. Ma, \nand R. Fergus, pp. 622 803–622 803, 2019. \n \n6\n",
  "categories": [
    "q-bio.BM",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2022-10-27",
  "updated": "2022-10-27"
}