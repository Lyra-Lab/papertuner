{
  "id": "http://arxiv.org/abs/2211.10168v1",
  "title": "Language-Conditioned Reinforcement Learning to Solve Misunderstandings with Action Corrections",
  "authors": [
    "Frank Röder",
    "Manfred Eppe"
  ],
  "abstract": "Human-to-human conversation is not just talking and listening. It is an\nincremental process where participants continually establish a common\nunderstanding to rule out misunderstandings. Current language understanding\nmethods for intelligent robots do not consider this. There exist numerous\napproaches considering non-understandings, but they ignore the incremental\nprocess of resolving misunderstandings. In this article, we present a first\nformalization and experimental validation of incremental action-repair for\nrobotic instruction-following based on reinforcement learning. To evaluate our\napproach, we propose a collection of benchmark environments for action\ncorrection in language-conditioned reinforcement learning, utilizing a\nsynthetic instructor to generate language goals and their corresponding\ncorrections. We show that a reinforcement learning agent can successfully learn\nto understand incremental corrections of misunderstood instructions.",
  "text": "Language-Conditioned Reinforcement Learning to\nSolve Misunderstandings with Action Corrections\nFrank Röder∗\nInstitute for Data Science Foundations\nHamburg University of Technology\nfrank.roeder@tuhh.de\nManfred Eppe\nInstitute for Data Science Foundations\nHamburg University of Technology\nmanfred.eppe@tuhh.de\nAbstract\nHuman-to-human conversation is not just talking and listening. It is an incremen-\ntal process where participants continually establish a common understanding to\nrule out misunderstandings. Current language understanding methods for intelli-\ngent robots do not consider this. There exist numerous approaches considering\nnon-understandings, but they ignore the incremental process of resolving misun-\nderstandings. In this article, we present a ﬁrst formalization and experimental\nvalidation of incremental action-repair for robotic instruction-following based\non reinforcement learning. To evaluate our approach, we propose a collection\nof benchmark environments for action correction in language-conditioned rein-\nforcement learning, utilizing a synthetic instructor to generate language goals and\ntheir corresponding corrections. We show that a reinforcement learning agent\ncan successfully learn to understand incremental corrections of misunderstood\ninstructions.\n1\nIntroduction\nIn real-world human-to-human communications, misunderstandings happen very frequently. Ambi-\nguity, a lack of common ground, and incorrect statements are three examples of situations that cause\nmisunderstandings. For example, you may ask your colleague to “give me the item on the right”,\nreferring to the object on her right, while your conversation partner assumes you refer to your right.\nHumans are extremely proﬁcient in resolving such ambiguities by performing conversational repair,\ne.g., “No, not to my right, I mean your right!”. Such repair commands enable conversation partners\nto incrementally establish a common understanding of what was said.\nThe state-of-the-art in robotic language understanding considers non-understandings [Bohus and\nRudnicky, 2008, Bordes et al., 2017], but it entirely ignores the incremental resolution of misunder-\nstandings with conversational repair.\nThe concept of action correction is not only important when the system is being deployed into the\nreal world but should be, as we propose in this article, a fundamental part of the learning procedure.\nHowever, even ﬂagship approaches like SayCan struggle with ambiguities and negations [Ahn et al.,\n2022], despite using large language models with a rich semantic knowledge of the world.\nRecent advances in controlling robots with language achieved remarkable results in simulation\n[Lynch and Sermanet, 2021, Huang et al., 2022] and also made considerable progress in real-world\napplications [Shridhar et al., 2021]. Especially reinforcement learning (RL) turns out to be a suitable\nframework for training robots paired with large language models [Ahn et al., 2022]. But it remains\nunclear how we can build an intelligent robot capable of resolving ambiguities, as we depict in\nFigure 1.\nIn this article, we propose an extension of goal-conditioned RL, where goals are speciﬁed as word\n∗Corresponding author (https://www.dsf.tuhh.de/index.php/team/frankroeder/)\nAccepted to the 2nd Workshop on Language in Reinforcement Learning, (NeurIPS 2022). Do not distribute.\narXiv:2211.10168v1  [cs.LG]  18 Nov 2022\nembeddings that can be dynamically extended with repair commands while a robot is executing a\ncertain task or after causing changes in the environment (see Figure 1). We hypothesize that our\napproach enables robotic agents to react appropriately to the following three yet unexplored cases of\nmisunderstandings (see Appendix A.3 for examples in our environment):\nAmbiguity\nInstructions are ambiguous if they are underspeciﬁed. As an example, consider Figure\n1. Here, the instruction “grasp the cube” is imprecise as there are two cubes and the instruction is\nreferring to a categorical feature of the present objects that are identical for two entities.\nLack of common ground\nWhen operator and robot have a lack of common ground, the robot\nmisunderstands the instruction due to unknown words used (out-of-distribution) or insufﬁcient\ntraining. For example, if the robot’s object detection method misclassiﬁes a red apple for a tomato,\nthe operator needs to use action corrections to interrupt the robot’s interactions with the apple and\nguide it to the tomato.\nInstruction Correction\nIt can happen that the error is completely on the operator’s side, e.g., when\nshe/he accidentally utters an unintended instruction. For example, the operator might say “reach the\nred cube”, while having intended to instruct the robot to reach the green cube. This requires an action\ncorrection like “No, sorry, I mean the green cube”.\nIn the following, we ﬁrst highlight that the state of the art in robotic language understanding lacks\nmethods to address such misunderstandings. Then we brieﬂy describe our method, and, ﬁnally,\nwe present our environment extension. We conclude the article with our experiments and results,\nshowing how our method successfully ﬁlls this gap.\nFigure 1: Consider the present scene with a green cube, a green cuboid, and a red cube. An instructor\nassigns the robot to “grasp the cube”. The robot might grasp an object by chance (let us assume\nthe red cube). Now an action correction like “actually, the green” could resolve the robot’s mistake\ncaused by the ambiguity of the instruction. The robot should be still aware of the “grasping” task\nand initial shape word “cube”, to ideally ignore the green cuboid and approach the green cube.\n2\nRelated Work\nNatural language is the most important tool of communication, therefore researchers investigate its\nusage for robots for more than a decade [Steels, 2008] and it is still actively researched [Tellex et al.,\n2020]. Speciﬁcally grounded language is a key ingredient for true understanding because it is not\nalways possible to transmit information without the background knowledge and considerations of\nother modalities [Bisk et al., 2020]. The notion of controlling intelligent agents with language has\nalso found its way into deep RL research [Hill et al., 2019, Akakzia et al., 2021, Chevalier-Boisvert\net al., 2019]. The review of Luketina et al. [2019] outlines the usage of language in RL where it is\napplied to assist or instruct an agent to solve a task. RL is an embodied setup and therefore provides\nthe grounding of language through rewards [Hermann et al., 2017, Akakzia et al., 2021, Röder et al.,\n2022]. Furthermore, its compositional representation allows decomposing a task, e.g., “remove the\nplate and clean the table”, into two subtasks, namely “remove plate” and “clean table”. We assume\nthat action corrections are of great beneﬁt if such a compositional representation is utilized by the\nagent because a misunderstanding could originate in misinterpretation of one single word.\n2\n3\nMethod\nWe outline the general notion of action correction as dynamic goal extension, where we augment the\nepisodic goal by the action correction. For a general example, consider the illustration in Figure 2.\nHere, the agent interacts with the world under the consideration of the instruction gℓ2. The instructor\nis observing the environment state for interactions with wrong objects, on which an action correction\nis returned immediately or, like in the real world, with a small delay, after some actions potentially\nalready changed the world state st. The following goal is now a concatenation of the original goal\nand the action correction, gℓ◦gac. The fact that the policy π(st, gℓ) is conditioned on the current\nstate st and instruction gℓ, requires it to notice the connection the between action correction gac, the\ninitial state st and the state st+1 the correction appears.\nInstruction\nobserves\nobserves\nAction Correction\nFigure 2: We illustrate the general case of an action correction. The episodic conversation starts with\nan instruction gℓuttered by the instructor (left box). The agent, being in state st−1 executes an action\nat to receive a scalar reward rt and the next world state st. However, the instructor detects with the\nhelp of the task-speciﬁc condition function from Equation 1 a wrong behavior. Following, an action\ncorrection gac is returned, which is concatenated to the original goal gℓ◦gac (right box). Under the\nconsideration of the new linguistic goal, the agent attempts to correct its behavior.\nFormally, the reward function Rg is determined by at least one or more task-speciﬁc conditions CT\n[Röder et al., 2022], that are evaluated at each step of the simulation (see Equation 1).\nrt = Rg(st+1, gℓ) =\n\u001a0,\nif CT (st+1, gℓ)\n−1,\notherwise\n(1)\nThe synthetic instructor uses the same function CT from Equation 1 to also detect interactions with\nthe non-goal objects. In case of a misunderstanding, the instructor returns an action correction, while\nhaving the full knowledge of the goal object and non-goal object’s position and properties in the\nscene. Figure 1 illustrates an example of an ambiguity where the robot interacts with the wrong object\nbecause of a spatial setup (grasping the closer object) or by chance. Besides, the correction in this\nexample is referring to a unique property of the actual goal object and is not making use of negations\nlike “not the red”. We are aware of a recent experiment with negations in language-conditioned RL\n[Hill et al., 2020], but the authors are not utilizing them for action correction. They rather instruct\nan agent to go to an object that has not a speciﬁc property. However, we assume this to be learned\nin our scenarios when the corrections involve negations. In the following section, we give a brief\ndescription of our environments for action correction.\n3.1\nLearning Environment for Action Correction\nWe present an extension of the publicly available language-conditioned RL environment LANRO\n[Röder et al., 2022] 3. It provides a selection of 4 tasks, such as reach, push, grasp, and lift. An\nepisode consists of two or three objects on a table (see Figure 1) and a linguistic goal generated by a\nsynthetic instructor. We provide the formal deﬁnition of the templates to generate the instructions\nin Appendix A.1. Like Röder et al. [2022], we make use of a condition-based reward function\ncorresponding to the language goal (see Equation 1). We use the task-speciﬁc conditions to determine\n2We omit the subscript t to emphasize the episodic language goal gℓ, regardless of the time.\n3https://github.com/frankroeder/lanro-gym\n3\nthe moment we return an action correction for, e.g., an interaction with a wrong object (see Figure 2).\nFrom this point on, the original goal gℓis extended by the action correction gac. In the end, we receive\na concatenation of strings, hence gℓ◦gac. To bootstrap the learning of simple instruction-following,\nwe challenge the agent to solve action corrections in only 50% of the episodes. Since we actually\ndelegate the agent to fulﬁll the task of two different linguistic goals, the episode limit is increased\nto 100 steps. In summary, the action correction episodes contain at most two goals that need to be\nsolved in sequence. The instructor can resolve ambiguous and erroneous statements immediately\n(longer initial instruction goal) or after recognizing the wrong behavior (extended goal given changed\nworld state). Finally, the lack of common ground could be resolved by both repeating the goal object\nproperties or providing a negation after interacting with a wrong object.\n4\nExperiments\nFor our initial experiments, we make use of a language-conditioned Soft Actor-Critic baseline\n[Akakzia et al., 2021, Röder et al., 2022], that we pair like Röder et al. [2022] with a GRU as\nlanguage encoder [Cho et al., 2014]. We conduct all the experiments with two and three objects on\nthe table. The objects are sampled uniformly from all attribute combinations, but we put speciﬁc\nconstraints on the overlapping features. Our goal object has at most 1 property in common with the\nnon-goal objects. This allows us to create scenarios like the one in Figure 1, where the agent needs\nto consider every part of the language goal. Furthermore, it assures that there is a unique and valid\nsolution after the instructor proposes the correction.\nIn Figure 3, we show the mean and the standard deviation of 3 random seeds in the “reach” and\n“push” tasks. Next to the default action corrections without negations (AC, red lines in Figure 3), we\nshowcase the episodes with action corrections including negations (ACN, purple lines in Figure 3).\nAs we sample 50% of the episodes with an action correction challenge, we provide additional plots\nfor those episodes in Appendix A.2.\n0.00\n0.25\n0.50\n0.75\n1.00\ntime steps\n×108\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nsuccess rate\nReachShape2\nAC\nACN\n(a)\n0.00\n0.25\n0.50\n0.75\n1.00\ntime steps\n×108\n0.0\n0.2\n0.4\n0.6\n0.8\nsuccess rate\nPushShape2\nAC\nACN\n(b)\n0.00\n0.25\n0.50\n0.75\n1.00\ntime steps\n×108\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nsuccess rate\nPushColor3\nAC\nACN\n(c)\n0.00\n0.25\n0.50\n0.75\n1.00\ntime steps\n×108\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nsuccess rate\nPushShape3\nAC\nACN\n(d)\nFigure 3: Our experiments showcase the success rate of a selection of “reach” and “push” experi-\nments with different object conﬁgurations. On the x-axis, we show the total environment steps. The\ny-axis shows the mean success rate of solving both the episodes with single language goals and those\nincluding the action correction. In Appendix A.2, we depict the action correction success rates only.\nAs expected, learning action correction with negations is slightly more difﬁcult than corrections\ncreated with editing terms and directly naming the desired goal object properties. Learning action\ncorrections with just two objects in the scene is basically switching to the other object, after the\naction correction appears. However, it might still help with learning a robust representation because\nthe agent nevertheless needs to grasp the nuances of the language. In Figure 3c and Figure 3d, the\nperformance in the “push” task with 3 objects is shown. Although the success rate is increasing over\ntime, the baseline requires millions of environment steps to surpass the threshold of only solving\nthe single goal instructions shown in Figure 3c. We excluded the tasks that were too challenging for\nour baseline, but we expect addressing this with methods like hindsight learning (Appendix A.4),\nintrinsic motivation [Röder et al., 2020, Colas et al., 2020b, Akakzia et al., 2021] or hierarchical RL\n[Jiang et al., 2019, Eppe et al., 2022].\nIn summary, it still takes additional effort to solve action correction challenges efﬁciently. However,\nwe postulate our environment and initial experimental results to be valuable insight for future work.\n5\nConclusion\nThis paper is the ﬁrst to formalize and experimentally validate the use of incremental action correction\nfor robotic instruction-following based on reinforcement learning. We present a novel collection of\nbenchmark environments for which we show initial experimental results, using a common language-\nconditioned baseline algorithm.\n4\nAcknowledgments\nFrank Röder and Manfred Eppe acknowledge funding by the DFG through the LeCAREbot\n(433323019) and IDEAS/MoReSpace (402776968) projects.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian\nIbarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth,\nNikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey\nLevine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek\nRettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev,\nVincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan. Do As I Can,\nNot As I Say: Grounding Language in Robotic Affordances, 2022.\nAhmed Akakzia, Cédric Colas, Pierre-Yves Oudeyer, Mohamed Chetouani, and Olivier Sigaud.\nGrounding Language to Autonomously-Acquired Skills via Goal Generation. In International Con-\nference on Learning Representations, Virtual (formerly Vienna, Austria), 2021. OpenReview.net.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella\nLapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian.\nExperience Grounds Language. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors,\nConference on Empirical Methods in Natural Language Processing, pages 8718–8735, Online,\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.703.\nDan Bohus and Alexander I. Rudnicky. Sorry, I Didn’t Catch That! In Laila Dybkjær and Wolfgang\nMinker, editors, Recent Trends in Discourse and Dialogue, volume 39, pages 123–154. Springer\nNetherlands, Dordrecht, 2008. ISBN 978-1-4020-6820-1 978-1-4020-6821-8. doi: 10.1007/\n978-1-4020-6821-8_6.\nAntoine Bordes, Y. Lan Boureau, and Jason Weston. Learning End-to-End Goal-Oriented Dialog. In\nInternational Conference on Learning Representations, Toulon, France, 2017.\nMaxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia,\nThien Huu Nguyen, and Yoshua Bengio. BabyAI: A Platform to Study the Sample Efﬁciency of\nGrounded Language Learning. In International Conference on Learning Representations, New\nOrleans, Louisiana, USA, 2019. OpenReview.net.\nKyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. Learning Phrase Representations using RNN Encoder–Decoder\nfor Statistical Machine Translation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans,\neditors, Conference on Empirical Methods in Natural Language Processing, pages 1724–1734,\nDoha, Qatar, 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179.\nCédric Colas, Ahmed Akakzia, Pierre-Yves Oudeyer, Mohamed Chetouani, and Olivier Sigaud.\nLanguage-Conditioned Goal Generation: A New Approach to Language Grounding in RL, 2020a.\nCédric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Ford\nDominey, and Pierre-Yves Oudeyer. Language as a Cognitive Tool to Imagine Goals in Curiosity-\nDriven Exploration. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina\nBalcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems, vol-\nume 33, pages 3761–3774, Vancouver, Canada, 2020b. Curran Associates, Inc.\nManfred Eppe, Christian Gumbsch, Matthias Kerzel, Phuong D. H. Nguyen, Martin V. Butz, and Ste-\nfan Wermter. Intelligent problem-solving as integrated hierarchical reinforcement learning. Nature\nMachine Intelligence, 4(1):11–20, 2022. ISSN 2522-5839. doi: 10.1038/s42256-021-00433-9.\nKarl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David\nSzepesvari, Wojciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin, Marcus Wainwright,\nChris Apps, Demis Hassabis, and Phil Blunsom. Grounded Language Learning in a Simulated 3D\nWorld. arXiv:1706.06551 [cs, stat], 2017.\n5\nFelix Hill, Stephen Clark, Karl Moritz Hermann, and Phil Blunsom. Understanding Early Word\nLearning in Situated Artiﬁcial Agents. arXiv:1710.09867 [cs], page online, 2019.\nFelix Hill, Andrew Lampinen, Rosalia Schneider, Stephen Clark, Matthew Botvinick, James L.\nMcClelland, and Adam Santoro. Environmental drivers of systematicity and generalization in a\nsituated agent. In International Conference on Learning Representations, Virtual (formerly Addis\nAbaba, Ethiopia), 2020.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language Models as Zero-Shot\nPlanners: Extracting Actionable Knowledge for Embodied Agents. arXiv:2201.07207 [cs], 2022.\nYiDing Jiang, Shixiang (Shane) Gu, Kevin P Murphy, and Chelsea Finn. Language as an Abstraction\nfor Hierarchical Deep Reinforcement Learning. In Hanna M. Wallach, Hugo Larochelle, Alina\nBeygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in\nNeural Information Processing Systems, volume 32, pages 9419–9431, Vancouver, Canada, 2019.\nCurran Associates, Inc.\nJelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob N. Foerster, Jacob Andreas, Edward\nGrefenstette, Shimon Whiteson, and Tim Rocktäschel. A Survey of Reinforcement Learning\nInformed by Natural Language. In International Joint Conference on Artiﬁcial Intelligence,\npages 6309–6317, Macao, China, 2019. International Joint Conferences on Artiﬁcial Intelligence\nOrganization. ISBN 978-0-9992411-4-1. doi: 10.24963/ijcai.2019/880.\nCorey Lynch and Pierre Sermanet. Language Conditioned Imitation Learning Over Unstructured\nData. In Dylan A. Shell, Marc Toussaint, and M. Ani Hsieh, editors, Robotics: Science and\nSystems, Virtual, 2021. doi: 10.15607/RSS.2021.XVII.047.\nFrank Röder, Manfred Eppe, Phuong D. H. Nguyen, and Stefan Wermter. Curious Hierarchical Actor-\nCritic Reinforcement Learning. In International Conference on Artiﬁcial Neural Networks, volume\n12397, pages 408–419, Canceled (formerly Bratislava, Slovakia), 2020. Springer International\nPublishing. ISBN 978-3-030-61616-8. doi: 10.1007/978-3-030-61616-8_33.\nFrank Röder, Manfred Eppe, and Stefan Wermter. Grounding Hindsight Instructions in Multi-Goal\nReinforcement Learning for Robotics. In International Conference on Development and Learning,\npages 170–177, London, UK, 2022. IEEE. ISBN 978-1-66541-310-7.\nMohit Shridhar, Lucas Manuelli, and Dieter Fox. CLIPort: What and Where Pathways for Robotic\nManipulation. In Aleksandra Faust, editor, Conference on Robot Learning, Proceedings of Machine\nLearning Research, pages 894–906, London, UK, 2021. PMLR.\nLuc Steels. The symbol grounding problem has been solved. So what’s next?\nIn Symbols and\nEmbodiment: Debates on Meaning and Cognition, pages 223–244. Oxford University Press, New\nYork, NY, US, ﬁrst edition, 2008. ISBN 0-19-921727-0 978-0-19-921727-4.\nStefanie Tellex, Nakul Gopalan, Hadas Kress-Gazit, and Cynthia Matuszek. Robots That Use\nLanguage. Annual Review of Control, Robotics, and Autonomous Systems, 3(1):25–55, 2020. doi:\n10.1146/annurev-control-101119-071628.\n6\nA\nAppendix\nA.1\nGrammar Templates\nA.1.1\nInstruction Templates\n⟨INSTRUCTION⟩\n|=\n⟨TASKVERB⟩⟨ARTICLE⟩⟨OBJECT⟩\n⟨OBJECT⟩\n|=\n⟨COLOR⟩⟨SHAPE⟩| ⟨SHAPE⟩object | ⟨COLOR⟩object\n⟨SHAPE⟩\n|=\n⟨CUBE⟩| ⟨CUBOID⟩| ⟨RECTANGLE⟩\n⟨COLOR⟩\n|=\nred | green | blue | yellow | purple | orange | pink | cyan | brown\n⟨TASKVERB⟩\n|=\n(reach | touch | contact) | (push | move | shift)\n⟨CUBE⟩\n|=\ncube | box | block\n⟨CUBOID⟩\n|=\ncuboid | brick | oblong\n⟨CYLINDER⟩\n|=\ncylinder | barrel | tophat\n⟨ARTICLE⟩\n|=\nthe\nFigure 4: Backus normal form (BNF) for our instruction set inspired by Chevalier-Boisvert et al.\n[2019]. Here we show the verbs for the “reach” and “push” task only.\nA.1.2\nAction Correction Templates\n⟨EXTENDED INSTRUCTION⟩\n|=\n⟨INSTRUCTION⟩⟨CORRECTION⟩\n⟨CORRECTION⟩\n|=\n⟨BEGINNING⟩⟨ARTICLE⟩⟨OBJECT⟩\n⟨BEGINNING⟩\n|=\n⟨EXCUSE⟩| ⟨NEGATION⟩| ⟨EDIT⟩\n⟨EXCUSE⟩\n|=\nsorry | excuse me | no i meant | pardon\n⟨NEGATION⟩\n|=\nnot\n⟨EDIT⟩\n|=\nactually\nFigure 5: The extended BNF grammar for our action corrections paired with the primal instruction.\nThese deﬁnitions apply to all tasks.\n7\nA.2\nAction Correction Successes\nIn our experimental design, we decide to sample 50% of the episodes with the need to solve a second\ngoal, our action correction. We provide additional correction success rate plots to make clear how\ncapable the agent is in solving the language-conditioned tasks augmented by a misunderstanding\nchallenge.\n0.00\n0.25\n0.50\n0.75\n1.00\ntime steps\n×108\n0.1\n0.2\n0.3\n0.4\n0.5\ncorrection success rate\nReachShape2\nAC\nACN\n(a)\n0.00\n0.25\n0.50\n0.75\n1.00\ntime steps\n×108\n0.0\n0.2\n0.4\n0.6\n0.8\ncorrection success rate\nPushShape2\nAC\nACN\n(b)\n0.00\n0.25\n0.50\n0.75\n1.00\ntime steps\n×108\n0.1\n0.2\n0.3\n0.4\ncorrection success rate\nPushColor3\nAC\nACN\n(c)\n0.00\n0.25\n0.50\n0.75\n1.00\ntime steps\n×108\n0.1\n0.2\n0.3\ncorrection success rate\nPushShape3\nAC\nACN\n(d)\nFigure 6: These ﬁgures depict the corresponding correction success rates of the experiment in\nFigure 3.\n8\nA.3\nAction Correction Example\nThis section showcases that all types of misunderstanding from Section 1 are covered by the proposed\nenvironment setup. In Figure 7, we present a selected scene for this purpose.\nAmbiguity\nFor the case of an ambiguity, the operator instructs the robot to “reach the cuboid”\n(Figure 7a-c). However, given the two present cuboids, this instruction is ambiguous, and the robot\nmight reach for the red cuboid (Figure 7d). As the instructor’s intention was it to reach the blue\ncuboid, the ambiguous situation is resolved by the action correction “sorry, the blue cuboid”, after\nwhich the robot carries out the new goal successfully (Figure 7e-h). Instead of an action correction,\nwe could also return an action correction with a negation like “sorry, not the red one”.\nLack of common ground\nConsider the case where the instructor utters a rare word like “azure” in\nthe instruction “reach the azure cuboid”. However, the robot still tries to make a guess and reaches\nfor the closer cuboid (Figure 7a-c). Subsequently, the instructor can return an action correction that\ncan resolve the uncertainty with, e.g. “actually, the blue cuboid”, or express additional information\nin the form of a negation like “no, not the red”.\nInstruction Correction\nFor an instruction correction (where the operator’s intention and instruction\nmismatches), an initial instruction like “reach the red cuboid” (Figure 7a-c) gets corrected after the\ninstructor recognizes the wrong object being touched. Following, an action correction “actually, the\nblue” is returned (Figure 7d). Finally, the agent switches to the other object and ﬁnishes the task\n(Figure 7e-h).\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\nFigure 7: An action correction scene from LANRO [Röder et al., 2022]\n9\nA.4\nHindsight Learning for Action Correction\nWe argue that language-conditioned RL has many properties with goal-conditioned RL in common\nand could beneﬁt from its insights. In Colas et al. [2020a], the authors even speak for goal-agnostic\nagents where the goal originates in modalities like images, Cartesian Coordinates, language or other\nrepresentations. The challenge of correcting a behavior is to interpret the action correction in context\nof the past actions, initial instruction and changes of the state. Because one can consider the initial\ninstruction to be an abstract goal description, the action correction acquaints about an alternation of\nthe episodic goal.\nEspecially methods of hindsight learning are valuable to apply to language-conditioned RL [Jiang\net al., 2019, Colas et al., 2020b, Akakzia et al., 2021, Röder et al., 2022] because they help to improve\nthe sample-efﬁciency by utilizing failures as imaginary successes for learning. For this article, we\nkeep it a question for future work on how to combine this with action correction.\nA.5\nHyperparameters\nThe hyperparameters are a slight alternation of the ones used by Akakzia et al. [2021] and Röder et al.\n[2022].\nTable 1\nParameter.\nValues.\nbatch size\n256\nhidden size of MLPs\n256\nlearning rate actor, critic and entropy tuning\n1e−3\nnumber of workers\n16\ntrade-off coefﬁcient α\n0.2\nbuffer size\n1e+6\nword embedding size\n32\ndiscount γ\n0.98\nnumber of critics\n1\npolyak ρ\n0.95\n10\n",
  "categories": [
    "cs.LG",
    "cs.RO"
  ],
  "published": "2022-11-18",
  "updated": "2022-11-18"
}