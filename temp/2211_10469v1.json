{
  "id": "http://arxiv.org/abs/2211.10469v1",
  "title": "Hub-VAE: Unsupervised Hub-based Regularization of Variational Autoencoders",
  "authors": [
    "Priya Mani",
    "Carlotta Domeniconi"
  ],
  "abstract": "Exemplar-based methods rely on informative data points or prototypes to guide\nthe optimization of learning algorithms. Such data facilitate interpretable\nmodel design and prediction. Of particular interest is the utility of exemplars\nin learning unsupervised deep representations. In this paper, we leverage hubs,\nwhich emerge as frequent neighbors in high-dimensional spaces, as exemplars to\nregularize a variational autoencoder and to learn a discriminative embedding\nfor unsupervised down-stream tasks. We propose an unsupervised, data-driven\nregularization of the latent space with a mixture of hub-based priors and a\nhub-based contrastive loss. Experimental evaluation shows that our algorithm\nachieves superior cluster separability in the embedding space, and accurate\ndata reconstruction and generation, compared to baselines and state-of-the-art\ntechniques.",
  "text": "Hub-VAE: Unsupervised Hub-based Regularization of Variational Autoencoders\nPriya Mani∗\nCarlotta Domeniconi†\nAbstract\nExemplar-based methods rely on informative data points or\nprototypes to guide the optimization of learning algorithms.\nSuch data facilitate interpretable model design and predic-\ntion.\nOf particular interest is the utility of exemplars in\nlearning unsupervised deep representations. In this paper,\nwe leverage hubs, which emerge as frequent neighbors in\nhigh-dimensional spaces, as exemplars to regularize a vari-\national autoencoder and to learn a discriminative embed-\nding for unsupervised down-stream tasks. We propose an\nunsupervised, data-driven regularization of the latent space\nwith a mixture of hub-based priors and a hub-based con-\ntrastive loss. Experimental evaluation shows that our algo-\nrithm achieves superior cluster separability in the embed-\nding space, and accurate data reconstruction and genera-\ntion, compared to baselines and state-of-the-art techniques.\nKeywords: representation learning; regularization; exem-\nplar selection; hubness phenomenon\n1\nIntroduction\nTraditional exemplar-based machine learning methods,\nsuch as the Parzen window estimator Parzen (1962),\nnearest neighbor methods Cover and Hart (1967), and\nexemplar-SVMs Malisiewicz, Gupta, and Efros (2011),\nrely on a set of exemplars and on a distance metric\ndeﬁned on the training data to learn a predictive model.\nExemplars are instances selected from the training\ndata which are informative for a given learning task.\nExemplar-based methods improve the discriminative\nability of a model, and support interpretable model\ndesign and prediction. In deep learning, exemplar-based\nmethods are typically used to explain post-training\nthe predictions of supervised black-box models (e.g.,\nRibeiro, Singh, and Guestrin (2016), Guidotti et al.\n(2019)).\nSuch methods are model-agnostic, as the\nlearned exemplars do not contribute to the training of\na black-box model, but are instead used to make the\nmodel interpretable. A diﬀerent approach in exemplar-\nbased deep learning (the focus of this paper) is to\nleverage exemplars during the training of a deep neural\nnetwork to regularize the latent embedding (e.g., Snell,\nSwersky, and Zemel (2017), Norouzi, Fleet, and Norouzi\n∗George Mason University, USA.\n†George Mason University, USA.\n(2020), Bautista et al. (2016)).\nIn this paper, we\nleverage exemplars to train an unsupervised generative\nmodel, which learns discriminative latent embeddings\nsuitable for unsupervised down-stream tasks.\nVariational\nautoencoders\n(VAEs,\nKingma\nand\nWelling (2014)) are a popular deep learning framework\nfor unsupervised generative models. A VAE maps an in-\nput data point x into a distribution qφ(z|x) in the latent\nspace (known as the variational posterior), through an\nencoder network parameterized by φ. The latent rep-\nresentation z is mapped into a distribution of the re-\nconstructed input pθ(x|z) through a decoder network\nparameterized by θ. A VAE regularizes the latent em-\nbedding by modeling a prior distribution p(z) over the\nlatent space. The objective when training a VAE is to\nmaximize a lower bound on the log marginal likelihood\nof x, derived by variational inference:\nL(φ, θ; x) = Eqφ(z|x)[log pθ(x|z)] −DKL(qφ(z|x)||pθ(z)),\nwhere DKL(·) is the KL-divergence between distribu-\ntions.\nVarious approaches have been proposed in the liter-\nature to regularize the latent space of a VAE, e.g. learn-\ning a disentangled latent representation Higgins et al.\n(2017), Kim and Mnih (2018), and learning an infor-\nmative prior distribution Norouzi, Fleet, and Norouzi\n(2020), Tomczak and Welling (2018).\nThe predictive\nperformance of a model which uses latent embeddings\nfor an unsupervised down-stream task depends on the\nseparability of data clusters in the latent space.\nIn\nthis work we propose to use hubs as exemplars to drive\nthe regularization of the latent space.\nThe hubness\nphenomenon Radovanovic, Nanopoulos, and Ivanovic\n(2010) refers to the emergence of few data points that\nare frequent neighbors of the data. Such phenomenon\naﬀects nearest neighbor computations and the cluster\nassumption Chapelle, Schlkopf, and Zien (2010).\nTo compute hubs in a collection of data, the concept\nof hubness score is used. The hubness score Nk(x) of a\ndata point x is deﬁned as the number of points that have\nx as their k-nearest neighbor. The data points which\ncontribute to the hubness of x are termed the reverse\nk-nearest neighbors of x. A hub is a data point whose\nhubness score exceeds by a certain threshold the mean\nhubness score of the data: Nk > µNk + λσNk where\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\narXiv:2211.10469v1  [cs.LG]  18 Nov 2022\nµNk and σNk are the mean and standard deviation\nof the distribution of hubness scores within the data.\nWhether a hub has a positive or negative inﬂuence on\ndata clustering depends on the degree of label mismatch\nbetween the hub and its reverse k-nearest neighbors.\nBad hubs with greater than 50% label mismatch appear\nnear cluster boundaries and can cause the merging of\nunrelated clusters, while good hubs are located near\nthe true cluster centers, and can be useful seeds to\nguide data clustering Tomasev et al. (2014); Mani and\nDomeniconi (2020).\nPrevious work on regularization\ndoesn’t account for hubness in their objectives.\nIn this paper, we regularize a VAE by leveraging\ngood hubs as exemplars learned in the latent space. We\nlearn a discriminative latent embedding by modeling a\nmixture of hub-based priors, and an adversarial margin\nbetween bad neighbors (i.e. k-nearest neighbors with\na label mismatch) via contrastive learning in the latent\nspace Schroﬀ, Kalenichenko, and Philbin (2015).\n1.1\nDesign Considerations Hubs exhibit charac-\nteristics which make them well-suited to be selected as\nexemplars to learn an embedding space for clustering.\nIn fact, good hubs facilitate the generation of prototypes\nof a given class, due to their proximity to cluster centers.\nA recent approach called Exemplar-VAE Norouzi, Fleet,\nand Norouzi (2020) uses exemplars chosen at random\nto regularize a VAE by learning a mixture of exemplar-\nbased priors distribution. As such, the resulting com-\nponents of the prior distribution may include outliers\nor data points at the boundaries of clusters, which may\nmerge clusters in the learned embedding space. Further-\nmore, the chosen exemplars do not depend on the evolv-\ning latent space of the VAE. In contrast, our proposed\napproach computes hubs in the VAE latent space which\nevolves with training. To further enhance the learned\nembeddings, Hub-VAE also optimizes a contrastive loss\nterm that increases the margin between points and their\nnearest neighbors with a mismatch in estimated labels.\nFig. 1(a) shows a sample of hubs selected as exem-\nplars by Hub-VAE from the class digit “3” (Dynamic\nMNIST dataset). Similarly, Fig. 1(b) shows a sample\nof exemplars chosen by Exemplar-VAE. In Fig. 1(b), we\nhighlight the images which could be confused as mem-\nbers of another class (e.g. class 8) and those which show\na large variation from the prototypical image of digit 3.\nThe hubs chosen by Hub-VAE are closely aligned to\nprototypical shapes of digit 3, and have a better image\nquality, while the images chosen by Exemplar-VAE have\na large stroke variability. In Fig. 1(c)-1(d), we show the\nencoding of the test data for DynamicMNIST obtained\nby Hub-VAE and Exemplar-VAE, respectively.\nAs a\nconsequence of the chosen hubs as exemplars, the clus-\nters learned by Hub-VAE are more compact and better\nseparated. We computed the clustering accuracy of digit\n3 on the encoded embedding of test data. We used k-\nmeans to cluster the data and assigned each cluster to\nthe majority class label in the cluster. Exemplar-VAE\nresults in an average accuracy of 0.83 (±0.01) for digit\nclass 3 and Hub-VAE gives an average accuracy of 0.87\n(±0.06). This experiment shows the potential of a hub-\nbased regularization of VAEs, particularly for clustering\ntasks which can beneﬁt from the properties of hubs.\n1.2\nContributions We present a novel approach\n(Hub-VAE) for the design of variational autoencoders,\nand leverage the hubness phenomenon to guide the reg-\nularization of the latent space via hub exemplars. This\ngives an embedding space well-aligned with the cluster-\ning structure of the data, which also lead to accurate\ndata reconstruction and generation. Our main contri-\nbutions are:\n1. Unsupervised\nregularization\nand\nhub-\nbased\npriors:\nWe propose an unsupervised\nand exemplar-based regularization framework for\nVAEs, built on a data-driven selection of hubs in\nthe latent space. We use a mixture of hub-based\npriors to regularize the VAE latent space by\nselecting informative hubs as exemplars.\n2. Unsupervised contrastive loss:\nWe further\nregularize the VAE latent space via an adversarial\nmargin between data points and neighbors with\nmismatched estimated labels, via a contrastive loss.\n3. Improved cluster separability and generative\nmodeling: We empirically evaluate our method\non data clustering, representation learning, and\ngenerative modeling. Our experiments show that\nHub-VAE is competitive against the state-of-the-\nart.\n2\nRelated Work\nVariational auto-encoders are versatile, deep generative\nmodels with a wide range of applications (van den Oord,\nVinyals, and Kavukcuoglu, 2017; Sch¨onfeld et al., 2019;\nGregor et al., 2019).\nVAEs have a strong theoretical\nfoundation and are easier to train and optimize than\nother classes of generative models (Radford, Metz, and\nChintala, 2016).\nIn this section we brieﬂy discuss\nprevious work on VAEs.\n2.1\nVAE Regularization Several approaches have\nbeen proposed to learn a disentangled representation in\nthe latent space and make the latent dimensions inde-\npendent. The β-VAE (Higgins et al., 2017) weights the\nKL-divergence term by a factor β to trade oﬀlearn-\ning an accurate reconstruction with disentanglement,\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\n(a)\n(b)\n(c)\n(d)\nFigure 1: A sample of digit 3 exemplars learned by (a) Hub-VAE and (b) Exemplar-VAE. Red bounding boxes\nindicate images with noise or defects and yellow bounding boxes indicate images that have a large stroke variation.\nFor example, one of the highlighted images can be mistaken as a member of class 8. (c)-(d) show the TSNE plots\nof DynamicMNIST test data embeddings learned by (c) Hub-VAE and (d) Exemplar-VAE. The TSNE plots\nare color-coded according to ground-truth. The clusters obtained by Hub-VAE are more compact and better\nseparated, which lead to a superior clustering accuracy for digit 3: 87% (Hub-VAE) vs. 83% (Exemplar-VAE).\nwhile Factor-VAE (Kim and Mnih, 2018) encourages\nthe representations to be factorial.\nAnother way to\nlearn an expressive and interpretable latent represen-\ntation is to modify the prior distribution of the VAE.\n(Johnson et al., 2016; Hoﬀman and Johnson, 2016) have\nshown that the prior is essential to improving the perfor-\nmance of the VAE and several methods have proposed\nusing a mixture of Gaussians prior. However, they do\nnot allow for a closed-form optimization of the VAE\nobjective. The authors in (VAE-Vampprior, (Tomczak\nand Welling, 2018)) learn an approximation of the opti-\nmal mixture prior as a variational mixture of posteriors\nbased on pseudo-inputs which are learned through the\nVAE optimization. The recent work in (Exemplar-VAE,\n(Norouzi, Fleet, and Norouzi, 2020)) learns a mixture of\nexemplar-based priors, where the exemplars are chosen\nfrom the training data, and proposes a framework for\nconditional image generation. The work in (Tran, Pan-\ntic, and Deisenroth, 2021) introduces a mixture prior\nwith a Cauchy-Schwarz divergence term to regularize\nthe VAE. The work (ByPE-VAE, (Ai et al., 2021)) con-\nditioned the mixture prior on a Bayesian pseudo-coreset\nwhich is learned along with the optimization of VAE.\n2.2\nContrastive Learning Contrastive learning is\nclosely\nrelated\nto\nlarge\nmargin\nnearest\nneighbors\n(LMNN, (Weinberger, Blitzer, and Saul, 2005)) in tra-\nditional metric learning and optimizes a similar objec-\ntive in a self-supervised or supervised deep-learning set-\nting. A common approach is to select positive and neg-\native samples with respect to an anchor data point, and\nto optimize the latent space so that positive samples\nare near each other in the learned representation, while\nnegative samples are pushed farther away from the an-\nchor. Several variants of contrastive losses have been\nproposed, such as to select positive samples by data\naugmentation (Chen et al., 2020), co-occurrence (Tian,\nKrishnan, and Isola, 2020), and to choose important\ntriplets for learning (Bengio et al., 2009). (Li, Kan, and\nHe, 2020) proposed an unsupervised deep metric learn-\ning method where a contrastive ratio between cluster\ncenters is minimized.\n2.3\nHubness\nPhenomenon Radovanovic,\nNanopoulos,\nand\nIvanovic\n(2010)\nprovides\na\nde-\ntailed theoretical study of the hubness phenomenon,\nits emergence, and its impact on learning tasks. Much\nof the previous work on hubness consider hubs as\ndetrimental to their learning task (e.g., music retrieval\n(Berenzweig, 2007), ﬁnger-print identiﬁcation (Hicklin,\nWatson, and Ulery, 2005), zero-shot learning (Zhang,\nXiang, and Gong, 2017)), and aim to reduce hubness\nin data ((Feldbauer and Flexer, 2019)). However, few\napproaches have attempted to leverage hubs for unsu-\npervised learning.\n(Tomasev et al., 2014) empirically\nanalyzed the role of hubs w.r.t k-means clustering.\nThis work identiﬁed good hubs as cluster prototypes\nand designed several hub-based variants of k-means to\nleverage hubs as cluster centroids in k-means iterations.\nMani and Domeniconi (2020) identiﬁed good hubs\nusing a pre-trained classiﬁer and leveraged them to\noptimize a subspace clustering algorithm.\n3\nHub-based VAE Regularization\nThe architecture of Hub-VAE is given in Fig. 2. The\nmodel consists of an encoder which maps data and\nhubs into their respective distributions in the latent\nspace. The encoded data and hubs are reconstructed\nusing a shared decoder. The encoder and the decoder\ncan be viewed as parametric functions which learn the\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\nFigure 2: Hub-VAE architecture\nparameters of the variational posterior and hub-based\nprior, and the generative model respectively.\nWe next formally deﬁne the components of Hub-\nVAE and how they are combined together.\nLet x =\n{xi}n\ni=1 be the input data and H = {xh}m\nh=1, m < n\nbe the input representation of the hubs selected from\nthe latent embedding of x, where m is the number\nof components in the hub-based prior.\nEach data\npoint xi is encoded into a variational posterior dis-\ntribution qφ(z|xi) = N(z|µφ(xi), Σφ(xi)), while each\nhub xh is encoded into a component prior distribu-\ntion rφ(z|xh) = N(z|µφ(xh), σ2I) in the latent space,\nwhere N(.) denotes the Gaussian distribution, µφ and\nΣφ denote parametric mean and covariance functions\nwhich are learned by the encoder network with parame-\nter set φ. The sampled latent space variable is mapped\nback into the input space through a decoder using a\ngenerative model pθ(xi|z).\nTo simplify the presenta-\ntion, we denote Q(φ)\ni\n= qφ(z|xi), R(φ)\nh\n= rφ(z|xh) and\nP(θ)\ni\n= pθ(xi|z). The encoder networks share the same\nparameter set φ. The prior distribution is deﬁned as\nan isotropic Gaussian with scalar variance σ2, while\nthe variational posterior has a diagonal covariance Σ.\nThe mean and variance of the variational posterior, as\nwell as the mean of the mixture of hub-based priors are\nlearned. The speciﬁc form of the distribution of the de-\ncoder depends on the type of data; typically a Bernoulli\ndistribution is used for binary or discrete data and a\nGaussian distribution is used for continuous data. §3.1,\n§3.2 and §3.3 describe each component of Hub-VAE in\ndetail. §3.4 describes the overall objective function of\nHub-VAE. The pseudocode for Hub-VAE is given in Al-\ngorithm 1 in the supplementary material.\n3.1\nHub Selection We compute the hubs at the be-\nginning of each training epoch, and use them to learn a\nmixture prior distribution. Hubs are computed within\neach mini-batch of an epoch from the k-nearest neigh-\nbor graph (k-NN) of the data, as deﬁned in §1.\nA\nvariational encoder learns data distributions instead of\npoint estimates in the latent space. Hence, we construct\nthe k-NN graph based on the 2-Wasserstein Givens and\nShortt (1984) distance between the learned distributions\nin the latent space. Given two distributions pi(µi, Σi)\nand pj(µj, Σj) with means µi, µj, and diagonal covari-\nances Σi, Σj respectively, the 2-Wasserstein distance\nbetween the distributions is deﬁned as W(pi, pj) =\n(||µi −µj||2\n2 + ||Σ\n1/2\ni\n−Σ\n1/2\nj ||2\nF )\n1/2 where ||.||F denotes\nthe Frobenius norm. We set λ = 0.5 and k =\n√\nB to\ncompute the hubness scores Nk (deﬁned in §1), where\nB is the number of data points in a mini-batch.\nAs discussed in §1, not all hubs exhibit useful\nclustering properties. While good hubs appear near the\ncenters of the true clusters within data, a bad hub has\na signiﬁcant label mismatch with its reverse k-nearest\nneighbors (RkNN), and can negatively aﬀect tasks\ninvolving similarity or nearest neighbor computations.\nA bad hub is a hub whose label mismatches with more\nthan 50% of its reverse k-nearest neighbors.Identifying\nbad hubs without access to class labels is a challenge.\nWe address this problem by formulating a scoring\nfunction to ﬁlter bad hubs based on the characteristics\nexhibited by hubs in the latent space. Good hubs tend\nto be located near dense regions; hence the pairwise\ndistances between the distributions of good hubs and\ntheir reverse nearest neighbors tend to be smaller then\nthose of bad hubs.\nAs a consequence, the average\nprobability of reconstructing a good hub from the\ndistribution of its reverse k-nearest neighbors is higher\nthan for bad hubs. Based on this, we formulate a scoring\nfunction for good hubs:\nG(xh) =\nP\nr∈RkNN(zh) pθ(xh|zr)\nP\nr∈RkNN(zh) W(qφ(z|xr), qφ(z|xh)),\nwhere qφ and pθ denote the distributions learned by the\nencoder and decoder respectively; xh and xr denote the\ninput representations of a hub and its reverse k-nearest\nneighbor; zh and zr denote the data points sampled in\nthe latent space from their encoded distributions.\nTo motivate the choice of the scoring function\nabove, consider the plots given in Figure 3 which show\nthe hubs computed in an epoch and their characteristics\nfor the datasets used in our experiments.\nThe x-\naxis in each sub-plot of Figure 3 denotes normalized\n(µNk = 0, σNk = 1) hubness scores.\nThe y-axis\ndenotes bad hubness. The bad hubness score of a hub\nis computed as the percentage of its RkNN with a\nlabel mismatch.\nFor the purpose of this analysis, we\nuse the ground truth labels to compute bad hubness.\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\n(a) Dynamic MNIST\n(b) Fashion MNIST\n(c) Dynamic MNIST\n(d) Fashion MNIST\nFigure 3: Scatter plots of characteristics of hubs. The x-axis in each sub-plot denotes normalized (µNk = 0, σNk\n= 1) hubness scores. The y-axis denotes bad hubness. The hubs are color-coded by the sum of the pairwise\ndistances to their reverse k-nearest neighbors (RkNN) in plots (a)-(b), and by their reconstruction probabilities\nw.r.t. the distributions of their RkNN in plots (c)-(d). Hubs with high pairwise distances (top-right quadrant\nof (a)-(b)) and low reconstruction probabilities (bottom-left quadrant of (c)-(d)) with respect to their RkNN are\nstrong bad hubs.\nThe hubs are color-coded by the sum of the pairwise\ndistances to their RkNN in plots (a)-(d), and by their\nreconstruction probabilities w.r.t. the distributions of\ntheir RkNN in plots (e)-(h).\nWe can see that the\nhubs with high pairwise distances (top-right quadrant of\n(a)-(d)) and low reconstruction probabilities (bottom-\nleft quadrant of (e)-(h)) with respect to their reverse\nk-nearest neighbors are strong bad hubs.The above\ncharacteristics exhibited by hubs in the latent space\nof VAE, motivate the coice of the scoring function\nG(xh) above. We use a threshold to select good hubs\nbased on G(xh), setting the threshold adaptively to\nmax(z-score(G))\n2\n.\nPrior to each training epoch, we pre-compute a pool\nof good hubs by selecting them from each mini-batch in\nthe epoch. The data in each mini-batch of an epoch are\nencoded into the latent space using the conﬁguration\nof the VAE model trained in the previous epoch. The\ntraining of the VAE in the current epoch is then carried\nout, and uses the selected pool of hubs to regularize the\nlatent space.\n3.2\nHub-based Prior Distribution We learn a\nmixture of good hub-based priors to regularize the VAE.\nEach mini-batch in a training epoch samples a sub-\nset of hubs from the selected pool of hubs for that\nepoch.\nWe deﬁne the component prior distribution\ncorresponding to each hub as an isotropic Gaussian\nwith a mean function which depends on the chosen\nhub and shared covariance among all of the hub dis-\ntributions, as also done in previous work on exem-\nplar based priors Norouzi, Fleet, and Norouzi (2020);\nTomczak and Welling (2018):\nR(φ)\nh\n= rφ(z|xh) =\nN(z|µφ(xh), σ2I), where σ is a scalar.\nBased on\nthese distributions, we deﬁne the KL-divergence loss\nbetween the variational posterior and the hub-based\nprior as LKL(xi, φ) = DKL(Q(φ)\ni\n|| 1\nm\nPm\nj=1 R(φ)\nhj ). The\nreconstruction loss component of the VAE objective\nLR(xi, φ, θ) = −EQ(φ)\ni [log P(θ)\ni ] further regularizes the\nlatent space. The complete loss function for Hub-VAE\nis given in § 3.4. In Hub-VAE a hub is chosen from the\nselected pool of hubs in the latent space, and is encoded\ninto a latent representation using the hub-based prior.\nThe decoder then reconstructs the latent representation\nas a new sample. For generative modeling, Hub-VAE re-\nquires the decoder network as well as the learned pool\nof hubs and the hub-based prior rφ.\n3.3\nHub-based Contrastive Learning We regu-\nlarize the VAE to learn a discriminative latent space\nembedding by increasing the margin between a point\nand its neighbor when a label mismatch is detected.\nThis regularization is useful in high-dimensional latent\nspaces which are aﬀected by the emergence of bad hubs.\nWe achieve this through contrastive learning within the\nk-neighborhood of each point.\nContrastive learning\nworks similarly to the hinge loss in Weinberger, Blitzer,\nand Saul (2005). Positive and negative samples are se-\nlected w.r.t an anchor point, depending on the label\nmatch/mismatch between the anchor and the samples.\nThe distances between each triplet (anchor, positive,\nand negative sample) are adjusted such that the dis-\ntance of the anchor to the negative sample is larger than\nthat of the positive sample by a certain margin.\nContrastive learning is typically applied in a su-\npervised setting where the class labels of data points\nare known.\nOur model is unsupervised which brings\nforth the challenging problem of selecting good and bad\nneighbors without access to their labels. To overcome\nthis issue, we cluster the data in the latent space and\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\nuse the clustering labels as a proxy for the ground truth.\nSince good hubs appear near cluster centers, we seed\nthe clustering process using the hub sample to obtain a\nhigher clustering quality. In each mini-batch, the hubs\nare clustered using k-means1, and the rest of the data\nare assigned to the cluster label of its nearest hub, as\ncomputed by the 2-Wasserstein distance between their\ndistributions.\nGiven the cluster labels, we choose triplets from\nthe neighborhood of each data point as follows: each\ndata point is an anchor, the farthest neighbor that\nshares the same cluster label with the anchor is chosen\nas the positive sample, and the neighbors with label\nmismatch are chosen as the negative samples.\nThe\ncontrastive loss of a given anchor point xa with t triplets\n(xa, xp, xni), i = 1, 2, . . . t is deﬁned as:\nLC(xa, φ) =\nt\nX\ni=1\n[W(Q(φ)\na , Q(φ)\np ) −W(Q(φ)\na , Q(φ)\nni ) + 1]+\n{xp, xni} ∈kNN(xa), i = 1, 2, ... . . . t\nl(xa) ̸= l(xni), l(xa) = l(xp)\nwhere xp is the positive sample, xni are negative\nsamples associated with the anchor, and l(x) is the\nlabel of x.\nThe loss is minimized when the distance\nfrom the anchor to each negative example is greater\nthen the distance to the positive example by 1. Hence,\ntraining using this loss makes the anchor (i.e.\nits\nembedding) signiﬁcantly closer to the positive example\nthan to negative examples2.\nA training mini-batch is a sample of data points,\nand the distribution of cluster densities in the sample\ncould vary. Hence we compute adaptive neighbors for\neach data point to reduce over-stepping cluster bound-\naries due to the selection of k nearest neighbors. We se-\nlect triplets from an adaptive neighborhood of each data\npoint in the latent space. The adaptive neighbors are\ncomputed as follows: we construct a k-nearest neighbor\ngraph and compute mean µD and standard deviation\nσD of the distribution of kth-neighbor distances across\nthe data points, and eliminate the neighbors whose dis-\ntances exceed µD + 2σD.\nThe hub seeded clustering\ntogether with the adaptive neighborhood facilitates se-\nlection of more accurate triplets for contrastive learning.\n3.4\nHub-VAE Objective Function The objective\nfunction minimized by Hub-VAE is given below.\nIt\nincludes the following terms: (1) the reconstruction loss\n1Number of clusters in k-means is set to the number of classes.\n2Since we use cluster labels to choose xp and xni, each anchor\nxa will have a positive sample within its neighborhood. When\n{xni} is an empty set, LC(xa, φ) = 0.\nbetween the data point given in input to the encoder\nand the output of the decoder; (2) the KL-divergence\nbetween the variational posterior and the hub-based\nprior; (3) the contrastive loss within the neighborhood\nof the input data point.\nLHUB-VAE(xa; φ, θ) =\nLR(xa, φ, θ) + LC(xa, φ) + βLKL(xa, φ)\nThe training of Hub-VAE is conducted in mini-batches,\nand each mini-batch minimizes LHUB-VAE over the\ndata points in that mini-batch.\nThe reparameteriza-\ntion Kingma and Welling (2014) is applied to qφ(z|xa)\nto enable back-propagation through the encoder. Hub-\nVAE performs stochastic optimization using the Adam\nalgorithm Kingma and Ba (2015) with normalized gra-\ndients Yu et al. (2017). Training is performed for 100\nepochs and a validation loss is computed at the end of\neach epoch using the current model conﬁguration and a\nseparate validation data. Similarly to previous models\nproposed in the literature Norouzi, Fleet, and Norouzi\n(2020); Tomczak and Welling (2018), the validation loss\nuses all of the learned hubs as exemplars and does not\nincorporate the contrastive loss term in its objective3\nLval(xv; φ, θ) = LR(xv, φ) + βDKL(Q(φ)\nv || 1\n|H|\n|H|\nX\nj=1\nR(φ)\nj\n)\nwhere H is the pool of selected good hubs and xv is a\nvalidation instance. The conﬁguration which results in\nthe minimum validation loss is used as the ﬁnal model.\n4\nExperiments\nWe compare the methods on four well-known bench-\nmark datasets: Dynamic MNIST (DMNIST) Salakhut-\ndinov and Murray (2008),\nFashion MNIST (FM-\nNIST) Xiao, Rasul, and Vollgraf (2017), USPS4, and\nCaltech101Silhouettes5\n(Caltech101).\nWe evaluate\nHub-VAE against a baseline standard-VAE with a fac-\ntored Gaussian distribution which is compared in Tom-\nczak and Welling (2018), and state-of-the-art VAE regu-\nlarization methods which modify the prior distribution:\nByPE-VAE Ai et al. (2021), Exemplar-VAE Norouzi,\nFleet, and Norouzi (2020), denoted as Ex-VAE, and\nVAE-Vampprior Tomczak and Welling (2018). For all\n3LC isn’t included in the validation objective as its computa-\ntion requires knowledge of the number of classes in the data. Fur-\nthermore, LC acts as an additional regularizer, and the learning\nof VAE can be approximated by Lval, without sacriﬁcing model\nperformance.\n4https://pytorch.org/vision/stable/datasets.html\n5https://people.cs.umass.edu/~marlin/data/caltech101_\nsilhouettes_28_split1.mat.\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\nData\nVAE-Gaussian\nVAE-Vamp\nEx-VAE\nByPE-VAE\nHub-VAE\nDMNIST\n0.52 (0.02)\n0.58 (0.01)\n0.64 (0.01)\n0.66 (0.01)\n0.73 (0.02)\nUSPS\n0.52 (0.01)\n0.64 (0.01)\n0.69 (0.01)\n0.68 (0.01)\n0.76 (0.01)\nFMNIST\n0.57 (0.01)\n0.58 (0.01)\n0.59 (0.01)\n0.61 (0.01)\n0.64 (0.01)\nCaltech101\n0.59 (0.002)\n0.60 (0.002)\n0.60 (0.002)\n0.61 (0.001)\n0.60 (0.004)\nTable 1: k-means V-measure. We show mean (std) over 10 runs. Statistically signiﬁcant results are underlined.\n(a) Hub-VAE\n(b) Exemplar-VAE\n(c) ByPE-VAE\n(d) Hub-VAE\n(e) Exemplar-VAE\n(f) ByPE-VAE\nFigure 4: Conditional image generation from classes 3 in DMNIST and ’Sneaker’ in FMNIST. The inset shows\nreference images.\nexperiments, we set the number of components m =\n1000 in the prior distribution, and the latent space di-\nmensionality d = 40 6. All methods are run for a maxi-\nmum of 100 epochs. We use early-stopping with a look-\nahead of 50 epochs. The initial value of β in LHUB-VAE\nis set to 1 and is annealed as the epochs progress. Re-\nsults are shown on average over 10 runs and statistical\nsigniﬁcant results are underlined. All experiments are\nrun on a A100 GPU on the Google Cloud Platform.\nHub-VAE takes on average 28 sec to run an epoch for\nDMNIST, 37 sec for FMNIST, and 5 sec for USPS and\nCaltech101. Dataset summary and comparison of run-\nning times across all methods are given in the supple-\nmentary material.\n4.1\nRepresentation Learning We evaluate the eﬃ-\ncacy of the learned latent embeddings for unsupervised\ntasks using the V-measure Rosenberg and Hirschberg\n(2007) and KNN purity. The V-measure is an entropy-\nbased evaluation metric computed as the harmonic\nmean of homogeneity and completeness score of a given\nclustering. Homogeneity measures the degree to which\nthe members of a cluster belong to the same class, while\nthe completeness score measures the degree to which the\nmembers of a class belong to the same cluster. KNN pu-\nrity measures the % of k-nearest neighbors which share\nthe same class label, i.e. the degree to which the cluster\nassumption holds on a given test data. The number of\nclasses for k-means clustering is set equal to the true\n6Number of coresets in ByPE-VAE, and the number of pseudo-\ninputs in VAE-Vampprior are set to 1000 for fair comparison.\nnumber of classes in the data.\nTables 1 show the results on V-measure. The results\nfor KNN purity are given in the supplementary material.\nHub-VAE outperforms baselines by at least 8% for V-\nmeasure on DMNIST, FMNIST, and USPS, and attains\ncomparable performance on Caltech101.\nHub-VAE\nfurther achieves superior performance in V-Measure\ncompared to Ex-VAE and ByPE-VAE for the large\nmajority of the datasets, thus showing the advantage of\nusing hubs as exemplars to improve cluster separability\nin the embedding space. We provide an ablation study,\nt-SNE plots, and additional results on CIFAR-10 data\nin the supplementary material.\n4.2\nReconstruction and Generation In Fig. 4,\nwe compare the quality of exemplar-based generative\nmodeling using conditional image generation on Hub-\nVAE, Exemplar-VAE, and ByPE-VAE. We depict gen-\nerated images from class 3 for DMNIST and from class\n’Sneaker’ for FMNIST. We observe that some of the\nimages generated by Exemplar-VAE and ByPE-VAE re-\nsemble digits 8 or 5. These classes exhibit features which\ncan confound their members, and they are embedded in\nclose proximity in the latent space learned by Exemplar-\nVAE and ByPE-VAE. In contrast, Hub-VAE results in\naccurate generations and closely follow the shape of the\nreference image (shown in the inset). Hub-VAE sepa-\nrates digits with varied strokes within a class into sep-\narate clusters, which helps obtain more accurate recon-\nstructions and generations.\nSimilarly, Exemplar-VAE\ndoes not fully capture the shape of ’Sneaker’ and also\ngenerates blurred images. The above results show that\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\nData\nHub-VAE\nEx-VAE\nByPE-VAE\nDMNIST\n8.45 (0.2)\n9.49 (0.6)\n21.39 (0.6)\nUSPS\n16.44 (0.3)\n20.12 (11)\n37.80 (2.6)\nFMNIST\n25.25 (0.2)\n25.15 (0.3)\n54.90 (0.6)\nCaltech101\n91.45 (0.6)\n87.01 (2.2)\n124 (3.1)\nTable 2: FID on reconstructed images. Lower values\nsignify higher quality.\nStatistically signiﬁcant results\nare underlined.\nNumber of components\nV-measure\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0\n500\n1000\n1500\nDMNIST\nFMNIST\nUSPS\nCaltech\nFigure 5: Sensitivity analysis on number of components.\na hub-based regularization of VAEs is useful to improve\nthe quality of the generative modeling.\nWe compute Fretchet Inception Distance (FID)\nHeusel et al. (2017) between original and reconstructed\nimages to evaluate their quality. In Table 2, we observe\nthat Hub-VAE has higher reconstruction quality than\nits competitors for DMNIST and USPS and similar\nquality for FMNIST.\n5\nSensitivity Analysis\nWe conduct sensitivity analysis to assess the inﬂuence\nof the number of components of the mixture prior\ndistribution on VAE regularization. Figure 5 shows the\nvalues of k-means V-measure across diﬀerent number of\ncomponents m ∈{100, 500, 1000, 2000}. While we see a\nnotable increase in V-measure for FMNIST when m is\nincreased from 500 to 1000, the trend is stable for the\nother datasets for this range of m values.\n6\nConclusion\nWe proposed a data-driven, hub-based regularization\napproach for VAEs to learn a discriminative and in-\nterpretable latent embedding using a mixture of hub\nbased priors and leveraging the useful clustering prop-\nerties of hubs. We further enhanced the regularization\nby introducing a hub-based contrastive loss to increase\nthe margin between bad neighbors in the latent space.\nOur method achieves competitive results against state-\nof-the-art VAE regularization methods.\nReferences\nAi, Q.; HE, L.; LIU, S.; and Xu, Z. 2021. ByPE-VAE:\nBayesian Pseudocoresets Exemplar VAE. In NeurIPS\n2021.\nBautista, M. ´A.; Sanakoyeu, A.; Tikhoncheva, E.; and\nOmmer, B. 2016.\nCliqueCNN: Deep Unsupervised\nExemplar Learning. In NeurIPS.\nBengio, Y.; Louradour, J.; Collobert, R.; and Weston,\nJ. 2009. Curriculum learning. In ICML 2009.\nBerenzweig, A. 2007. Anchors and Hubs in Audio-based\nMusic Similarity. Ph.D. thesis, Columbia University.\nChapelle, O.; Schlkopf, B.; and Zien, A. 2010. Semi-\nSupervised Learning. The MIT Press, 1st edition.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. E.\n2020. A Simple Framework for Contrastive Learning\nof Visual Representations. In ICML 2020.\nCover, T. M.; and Hart, P. E. 1967. Nearest neighbor\npattern classiﬁcation.\nIEEE Trans. Inf. Theory,\n13(1): 21–27.\nFeldbauer, R.; and Flexer, A. 2019. A comprehensive\nempirical comparison of hubness reduction in high-\ndimensional spaces. Knowl. Inf. Syst., 59(1): 137–\n166.\nGivens, C. R.; and Shortt, R. M. 1984.\nA class of\nwasserstein metrics for probability distributions. The\nMichigan Mathematical Journal, 31(2): 231–240.\nGregor, K.; Papamakarios, G.; Besse, F.; Buesing, L.;\nand Weber, T. 2019. Temporal Diﬀerence Variational\nAuto-Encoder. In ICLR 2019.\nGuidotti, R.; Monreale, A.; Matwin, S.; and Pedreschi,\nD. 2019. Black Box Explanation by Learning Image\nExemplars in the Latent Feature Space. In ECML\nPKDD 2019.\nHeusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.;\nand Hochreiter, S. 2017. GANs Trained by a Two\nTime-Scale Update Rule Converge to a Local Nash\nEquilibrium. In NeurIPS, volume 30.\nHicklin, A.; Watson, C.; and Ulery, B. 2005. The myth\nof goats: How many people have ﬁngerprints that\nare hard to match. Internal Report 7271, National\nInstitute of Standards and Technology (NIST).\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\nHiggins, I.; Matthey, L.; Pal, A.; Burgess, C.; Glorot,\nX.; Botvinick, M.; Mohamed, S.; and Lerchner, A.\n2017.\nbeta-VAE: Learning Basic Visual Concepts\nwith a Constrained Variational Framework. In ICLR.\nHoﬀman, M. D.; and Johnson, M. J. 2016. Elbosurgery:\nYet another way to carve up the variational evidence\nlower bound. In Workshop in Advances in Approxi-\nmate Bayesian Inference, NIPS.\nJohnson, M. J.; Duvenaud, D.; Wiltschko, A. B.;\nAdams, R. P.; and Datta, S. R. 2016.\nComposing\ngraphical models with neural networks for structured\nrepresentations and fast inference. In NeurIPS 2016,\n2946–2954.\nKim, H.; and Mnih, A. 2018. Disentangling by Factoris-\ning. In ICML 2018, 2649–2658.\nKingma, D. P.; and Ba, J. 2015. Adam: A Method for\nStochastic Optimization. In ICLR.\nKingma, D. P.; and Welling, M. 2014. Auto-Encoding\nVariational Bayes. In ICLR.\nLi, Y.; Kan, S.; and He, Z. 2020.\nUnsupervised\nDeep Metric Learning with Transformed Attention\nConsistency and Contrastive Clustering Loss.\nIn\nECCV 2020.\nMalisiewicz, T.; Gupta, A.; and Efros, A. A. 2011.\nEnsemble of exemplar-SVMs for object detection and\nbeyond. In ICCV 2011, 89–96.\nMani, P.; and Domeniconi, C. 2020. Hub-based sub-\nspace clustering. Neurocomputing, 413: 193–209.\nNorouzi, S.;\nFleet, D. J.;\nand Norouzi, M. 2020.\nExemplar VAE: Linking Generative Models, Nearest\nNeighbor Retrieval, and Data Augmentation.\nIn\nNeurIPS 2020.\nParzen, E. 1962. On estimation of a probability density\nfunction and mode. Annals of Mathematical Statis-\ntics.\nRadford, A.; Metz, L.; and Chintala, S. 2016. Unsu-\npervised Representation Learning with Deep Convo-\nlutional Generative Adversarial Networks. In Bengio,\nY.; and LeCun, Y., eds., ICLR 2016.\nRadovanovic, M.; Nanopoulos, A.; and Ivanovic, M.\n2010. Hubs in Space: Popular Nearest Neighbors in\nHigh-Dimensional Data.\nJ. Mach. Learn. Res., 11:\n2487–2531.\nRibeiro, M. T.; Singh, S.; and Guestrin, C. 2016. ”Why\nShould I Trust You?”: Explaining the Predictions of\nAny Classiﬁer. In ACM SIGKDD, 1135–1144.\nRosenberg, A.; and Hirschberg, J. 2007. V-Measure: A\nConditional Entropy-Based External Cluster Evalu-\nation Measure.\nIn Eisner, J., ed., EMNLP-CoNLL\n2007, 410–420.\nSalakhutdinov, R.;\nand Murray, I. 2008.\nOn the\nquantitative analysis of deep belief networks.\nIn\nICML 2008.\nSch¨onfeld, E.; Ebrahimi, S.; Sinha, S.; Darrell, T.; and\nAkata, Z. 2019.\nGeneralized Zero- and Few-Shot\nLearning via Aligned Variational Autoencoders. In\nCVPR 2019, 8247–8255.\nSchroﬀ, F.; Kalenichenko, D.; and Philbin, J. 2015.\nFaceNet: A uniﬁed embedding for face recognition\nand clustering. In CVPR 2015, 815–823.\nSnell, J.; Swersky, K.; and Zemel, R. S. 2017. Proto-\ntypical Networks for Few-shot Learning. In NeurIPS,\n4077–4087.\nTian, Y.; Krishnan, D.; and Isola, P. 2020. Contrastive\nMultiview Coding. In ECCV 2020. Springer.\nTomasev, N.; Radovanovic, M.; Mladenic, M.; and\nIvanovic, M. 2014. The Role of Hubness in Clustering\nHigh-Dimensional Data. IEEE Trans. Knowl. Data\nEng., 26(3): 739–751.\nTomczak, J. M.; and Welling, M. 2018.\nVAE with\na VampPrior.\nIn AISTATS 2018, Playa Blanca,\nLanzarote, Canary Islands, Spain.\nTran, L.; Pantic, M.; and Deisenroth, M. P. 2021.\nCauchy-Schwarz Regularized Autoencoder.\nCoRR,\nabs/2101.02149.\nvan den Oord, A.; Vinyals, O.; and Kavukcuoglu, K.\n2017. Neural Discrete Representation Learning. In\nNeurIPS, 6306–6315.\nWeinberger, K. Q.; Blitzer, J.; and Saul, L. K. 2005.\nDistance Metric Learning for Large Margin Nearest\nNeighbor Classiﬁcation. In Advances in Neural Infor-\nmation Processing Systems 2005, 1473–1480.\nXiao,\nH.;\nRasul,\nK.;\nand\nVollgraf,\nR.\n2017.\nFashion-MNIST: a Novel Image Dataset for Bench-\nmarking Machine Learning Algorithms.\nCoRR,\nabs/1708.07747.\nYu, A. W.; Lin, Q.; Salakhutdinov, R.; and Carbonell,\nJ. G. 2017.\nNormalized Gradient with Adaptive\nStepsize Method for Deep Neural Network Training.\nCoRR, abs/1707.04822.\nZhang, L.; Xiang, T.; and Gong, S. 2017. Learning a\nDeep Embedding Model for Zero-Shot Learning. In\nCVPR 2017.\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\nHub-VAE: Unsupervised Hub-based Regularization of Variational Autoencoders\nSupplementary Material\nPriya Mani∗\nCarlotta Domeniconi†\n1\nAlgorithm\nAlgorithm 1 shows the pseudo-code of training epoch\nfor Hub-VAE.\n2\nAdditional Experiments\nData Table 1 gives the summary statistics for the\ndata.\nWe evaluated our method on an additional\ndataset, CIFAR-10 1.\n2.1\nRepresentation Learning We evaluate k-NN\npurity of data in Table 2. The value of k for computing\nthe KNN purity is set to k = √ntest, where ntest is\nthe number of instances in the test data.\nHub-VAE\noutperforms baselines by at least 1% for KNN purity on\nDMNIST, FMNIST, and USPS, and attains comparable\nperformance on Caltech101.\nFig. 1 shows the t-SNE plot of test data of DMNIST\nfor ByPE-VAE. Similar to Exemplar-VAE, ByPE-VAE\ndoes not form compact clusters.\nIn Table 3, we evaluate k-means V-measure and\nKNN purity for CIFAR-10 across 5 independent training\nruns.\nWe trained each method for 500 epochs for\nCIFAR-10. Hub-VAE outperforms the baseline and has\nnearly similar performance to ByPE-VAE.\n2.2\nAblation Study We evaluate the inﬂuence of\ndiﬀerent components of Hub-VAE on its objective func-\ntion. We compute k-means V-measure and KNN purity\nfor our model without applying contrastive loss (Hub-\nVAE-NoContrastive), and without applying hub selec-\ntion (Hub-VAE-NoSelection). Table 4 and Table 5show\nthe k-means V-measure and KNN purity for these vari-\nants.\nUSPS and FMNIST show a signiﬁcant decrease in\nperformance for HuB-VAE variants. The performance\ndrop is more pronounced for Hub-VAE-NoContrastive,\nwhich suggests that contrastive learning is useful in\nlearning the underlying clustering structure of data\nby separating bad neighbors in the data.\nHub-VAE-\nNoSelection results in lower V-measure and KNN purity\n∗George Mason University, USA.\n†George Mason University, USA.\n1https://www.cs.toronto.edu/ kriz/cifar.html\nAlgorithm 1 Hub-VAE\n1: input: Data x = {xi}n\ni=1, number of components\nin the prior m, # epochs =100, mini-batch size\nB = 100, β = 1, variance of prior distribution σ2,\nthe number of clusters K, learned hubs H\n2: output: Learned parameters φ, θ, τ\n3: Initialize parameters φ, θ, τ\n4: for each training epoch do\n5:\nfor each mini-batch {xi}B\ni=1 in the epoch do\n6:\nCompute distributions R(φ)\ni\nand P(θ)\ni\nfor each\nxi in the batch\n7:\n// Compute pairwise 2-Wasserstein distances\nfor all points in the batch\n8:\ndij ←W(R(φ)\ni\n, R(φ)\nj\n), i, j ∈[B]\n9:\n// Compute hubs\n10:\nH ←{xh | N√\nB(xh) > µ + 0.5σ}\n11:\n// Estimate good hubness score\nG(xh) ←\nP\nr∈RkNN(zh) pθ(xh|zr)\nP\nr∈RkNN(zh) drh\nfor h ∈H\n12:\nend for\n13:\nEliminate bad hubs xhj from the pool {xh}:\nH ←{h ∈H | z-score(G(xh)) < max(z-score(G))\n2\n}\n14:\nfor each mini-batch {xi}B\ni=1 in epoch do\n15:\nCompute distribution Q(φ)\ni\nfor each xi in the\nbatch\n16:\nLet h1, . . . , hm be a uniformly random sample\nfrom H\n17:\nLet l(xhj) be the label after using K-Means\nclustering on the hubs xh1, . . . , xhm, where K\ndenotes the number of clusters in the data\n18:\n// Assign clustering labels {li} to the closest\nhub\nFor each i ∈[B], let l(xi) ←l(xhj), where\nj ←argminj ˜dihj, where ˜dihj = W(Q(φ)\ni\n, R(φ)\nhj )\n19:\nFor each a ∈[B], compute the contrastive loss\nLC(xa, φ)\n20:\nCompute distribution P(θ)\na\nfor all a ∈[B]\n21:\nCompute the loss LHUB-VAE(xa; φ, θ, τ)\n22:\nBack-propagate\nand\nupdate\nparameters\n(φ, θ, τ) to minimize LHUB-VAE(xa; φ, θ, τ)\n23:\nend for\n24: end for\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\narXiv:2211.10469v1  [cs.LG]  18 Nov 2022\nData\n# instances\n# input features\n# classes\nDMNIST\n70000\n784\n10\nFMNIST\n70000\n784\n10\nUSPS\n9298\n256\n10\nCaltech101\n8671\n784\n101\nCIFAR-10\n60000\n3072\n10\nTable 1: Dataset information\nData\nVAE-Gaussian\nVAE-Vamp\nEx-VAE\nByPE-VAE\nHub-VAE\nDMNIST\n72.77 (0.83)\n86.18 (0.12)\n88.24 (0.44)\n90.09 (0.42)\n89.36 (0.97)\nUSPS\n71.17 (0.68)\n80.94 (0.28)\n84.02 (0.59)\n81.33 (0.19)\n85.79 (0.57)\nFMNIST\n69.71 (0.19)\n71.55 (0.06)\n71.69 (0.10)\n71.37 (0.22)\n72.92 (0.15)\nCaltech101\n41.80 (0.06)\n41.89 (0.08)\n42.16 (0.07)\n42.52 (0.08)\n42.20 (0.11)\nTable 2: KNN Purity. We show mean (std) over 10 runs. Statistically signiﬁcant results are underlined.\nMetric\nVAE-Gaussian\nVAE-Vamp\nEx-VAE\nByPE-VAE\nHub-VAE\nk-means V-measure\n0.11 (0.002)\n0.10 (0.001)\n0.10 (0.004)\n0.12 (0.004)\n0.11 (0.001)\nKNN Purity\n21.65 (0.07)\n22.15 (0.05)\n21.9 (0.06)\n22.71 (0.16)\n22.05 (0.04)\nTable 3: Evaluation of representation quality of CIFAR-10 embeddings.\nWe show mean (std) over 5 runs.\nStatistically signiﬁcant results are underlined.\nthan Hub-VAE, which shows that use of good hubs as\nexemplars in the mixture prior plays a signiﬁcant role in\nlearning the clustering structure of data. DMNIST and\nCaltech101 did not show much change in performance\non the variants. A lack of decrease in performance of\nHub-NoSelection compared to Hub-VAE indicates that\nDMNIST and Caltech101 have fewer strong bad hubs\nin the data which can negatively inﬂuence the neigh-\nbor computation and regularization. The lack of per-\nformance drop without contrastive learning (Hub-VAE-\nNoContrastive) further indicates that these datasets\nhave relatively fewer bad neighbors than FMNIST and\nUSPS.\n2.3\nReconstruction and Generation In Fig. 2,\nwe compare the reconstruction of images in DMNIST\nfor each method.\nThe images are chosen through\na random sampling of the latent space.\nFig. 2(a)\nand (e) shows the original training images for the\ndata being reconstructed, and (b)-(d), (f)-(h) show the\nreconstructions obtained by the diﬀerent methods. We\nobserve that Hub-VAE reconstructed images are less\nblurred and have fewer inaccuracies compared to its\ncompetitors.\nThe reconstructions for FMNIST in Fig. 3 show an\noverall lesser variability among Hub-VAE and Ex-VAE.\nFigure 1: t-SNE plot of DMNIST test data for ByPE-\nVAE.\nHowever, it can be observed that ByPE-VAE results\nin blurred images and fails to capture the ﬁner details\nof the images which were captured by Hub-VAE and\nEx-VAE (for example, class ’Sneaker’, ’Ankle boots’).\nSimilar results can be observed for USPS, where ByPE-\nVAE incorrectly reconstructs digit 8 instead of digit 3,\nas shown by red bounding box in Fig. 4.\nIn Fig. 5, we compare the quality of conditional\nimage generation on Hub-VAE and Ex-VAE.We plot\nthe classes ’Sandals’, ’Bag’, ’Ankle Boots’ and ’Shirt’\nfor Fashion-MNIST. The classes ’Sandals’ and ’Ankle\nBoots’ can be confused with each other.\nWe again\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\nData\nHub-VAE\nHub-VAE\nHub-VAE\n-NoSelection\n-NoContrastive\nDMNIST\n0.73 (0.02)\n0.73 (0.002)\n0.73 (0.02)\nUSPS\n0.73 (0.01)\n0.68 (0.01)\n0.76 (0.01)\nFMNIST\n0.62 (0.01)\n0.60 (0.005)\n0.64 (0.01)\nCaltech101\n0.60 (0.001)\n0.60 (0.004)\n0.60 (0.004)\nTable 4: k-means V-measure on Hub-VAE variants. We show mean (std) over 10 runs. Statistically signiﬁcant\nresults are underlined.\nData\nHub-VAE\nHub-VAE\nHub-VAE\n-NoSelection\n-NoContrastive\nDMNIST\n89.07 (0.02)\n90.36 (0.002)\n89.36 (0.97)\nUSPS\n84.17 (0.46)\n83.92 (0.45)\n85.79 (0.57)\nFMNIST\n72.30 (0.14)\n71.83 (0.0)\n72.92 (0.15)\nCaltech101\n41.97 (0.03)\n42.28 (0.0)\n42.20 (0.11)\nTable 5: KNN purity on Hub-VAE variants. We show mean (std) over 10 runs. Statistically signiﬁcant results\nare underlined.\n(a) Original Image\n(b) Hub-VAE\n(c) Exemplar-VAE\n(d) ByPE-VAE\n(e) Original Image\n(f) Hub-VAE\n(g) Exemplar-VAE\n(h) ByPE-VAE\nFigure 2: Reconstruction of randomly sampled images of digits 3 and 4 of Dynamic MNIST. (a) shows the original\nimages of the reconstructed data. Red bounding boxes show inaccurate reconstructions of digits with respect to\ntheir original images (e.g. some of the digit 4 images in ByPE-VAE could be misconstrued as digit 9).\nobserve the superior generative quality of Hub-VAE\ncompared to Ex-VAE which generates blurred images\nand does not accurately capture the ﬁne details of the\nreference image.\n2.4\nRunning Time We compare the running time\nin seconds of each method for an epoch in Table 6.\nVAE-Gaussian has the lowest running time. We observe\nthat while Hub-VAE has a higher running time than\nthe compared methods, the time is feasible from a\npractical standpoint. The bottleneck is the computation\nof hubs in each epoch, which involves pairwise distance\ncomputations. However, approximate nearest neighbor\ncomputation can be used to speedup the runtime of\nHub-VAE.\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\n(a) Original Image\n(b) Hub-VAE\n(c) Ex-VAE\n(d) ByPE-VAE\nFigure 3: Reconstruction of randomly sampled images of FMNIST from the latent space. (a) shows the original\nimages of the reconstructed data.\n(a) Original Image\n(b) Hub-VAE\n(c) Ex-VAE\n(d) ByPE-VAE\nFigure 4: Reconstruction of randomly sampled images of USPS from the latent space. (a) shows the original\nimages of the reconstructed data. Red bounding box on ByPE-VAE shows an incorrect reconstruction.\n(a) Hub-VAE\n(b) Hub-VAE\n(c) Hub-VAE\n(d) Hub-VAE\n(e) Exemplar-VAE\n(f) Exemplar-VAE\n(g) Exemplar-VAE\n(h) Exemplar-VAE\nFigure 5: Conditional image generation from chosen classes in FMNIST for (a)-(d) Hub-VAE and (e)-(f) Ex-VAE.\nThe reference images (exemplar) are shown in the inset.\n2.5\nHub Characteristics We show the hub charac-\nteristics for USPS and Caltech101 in Fig. 6\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\nData\nVAE-Gaussian\nVAE-Vamp\nEx-VAE\nByPE-VAE\nHub-VAE\nDMNIST\n5.24\n6.58\n7.63\n7.01\n28.16\nFMNIST\n5.46\n6.64\n11.12\n7.65\n33.62\nUSPS\n1.61\n1.77\n1.99\n2.06\n4.26\nCaltech101\n1.45\n1.56\n1.75\n1.73\n4.67\nCIFAR-10\n4.7\n5.83\n7.93\n5.78\n28.36\nTable 6: Running time (in seconds) per epoch.\n(a) USPS\n(b) Caltech101Silhouettes\n(c) USPS\n(d) Caltech101Silhouettes\nFigure 6: Scatter plots of characteristics of hubs. The x-axis in each sub-plot denotes normalized (µNk = 0, σNk\n= 1) hubness scores. The y-axis denotes bad hubness. The hubs are color-coded by the sum of the pairwise\ndistances to their reverse k-nearest neighbors (RkNN) in plots (a)-(b), and by their reconstruction probabilities\nw.r.t. the distributions of their RkNN in plots (c)-(d). Hubs with high pairwise distances (top-right quadrant\nof (a)-(b)) and low reconstruction probabilities (bottom-left quadrant of (c)-(d)) with respect to their RkNN are\nstrong bad hubs.\nCopyright © 2023 by SIAM\nUnauthorized reproduction of this article is prohibited\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2022-11-18",
  "updated": "2022-11-18"
}