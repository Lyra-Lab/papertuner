{
  "id": "http://arxiv.org/abs/2211.12280v1",
  "title": "Transformer Based Multi-Grained Features for Unsupervised Person Re-Identification",
  "authors": [
    "Jiachen Li",
    "Menglin Wang",
    "Xiaojin Gong"
  ],
  "abstract": "Multi-grained features extracted from convolutional neural networks (CNNs)\nhave demonstrated their strong discrimination ability in supervised person\nre-identification (Re-ID) tasks. Inspired by them, this work investigates the\nway of extracting multi-grained features from a pure transformer network to\naddress the unsupervised Re-ID problem that is label-free but much more\nchallenging. To this end, we build a dual-branch network architecture based\nupon a modified Vision Transformer (ViT). The local tokens output in each\nbranch are reshaped and then uniformly partitioned into multiple stripes to\ngenerate part-level features, while the global tokens of two branches are\naveraged to produce a global feature. Further, based upon offline-online\nassociated camera-aware proxies (O2CAP) that is a top-performing unsupervised\nRe-ID method, we define offline and online contrastive learning losses with\nrespect to both global and part-level features to conduct unsupervised\nlearning. Extensive experiments on three person Re-ID datasets show that the\nproposed method outperforms state-of-the-art unsupervised methods by a\nconsiderable margin, greatly mitigating the gap to supervised counterparts.\nCode will be available soon at https://github.com/RikoLi/WACV23-workshop-TMGF.",
  "text": "Transformer Based Multi-Grained Features\nfor Unsupervised Person Re-Identification\nJiachen Li1, Menglin Wang2, and Xiaojin Gong*\n1,2,∗College of Information Science and Electronic Engineering, Zhejiang University, China\n1,∗{lijiachen isee, gongxj}@zju.edu.cn,\n2lynnwang6875@gmail.com;\nAbstract\nMulti-grained features extracted from convolutional neu-\nral networks (CNNs) have demonstrated their strong dis-\ncrimination ability in supervised person re-identification\n(Re-ID) tasks. Inspired by them, this work investigates the\nway of extracting multi-grained features from a pure trans-\nformer network to address the unsupervised Re-ID prob-\nlem that is label-free but much more challenging. To this\nend, we build a dual-branch network architecture based\nupon a modified Vision Transformer (ViT). The local to-\nkens output in each branch are reshaped and then uniformly\npartitioned into multiple stripes to generate part-level fea-\ntures, while the global tokens of two branches are averaged\nto produce a global feature. Further, based upon offline-\nonline associated camera-aware proxies (O2CAP) that is a\ntop-performing unsupervised Re-ID method, we define of-\nfline and online contrastive learning losses with respect to\nboth global and part-level features to conduct unsupervised\nlearning.\nExtensive experiments on three person Re-ID\ndatasets show that the proposed method outperforms state-\nof-the-art unsupervised methods by a considerable mar-\ngin, greatly mitigating the gap to supervised counterparts.\nCode will be available soon at https://github.com/\nRikoLi/WACV23-workshop-TMGF.\n1. Introduction\nPurely unsupervised person re-identification (Re-ID)\naims to learn a Re-ID model without using any identity\nlabels.\nThis task has attracted extensive research inter-\nest because of its label-free manner, which makes it more\npractical and scalable to real-world deployments.\nOver\nthe past few years, significant progress has been made,\nmainly due to the leverage of pseudo labeling [24] and con-\ntrastive learning [5, 15, 44] techniques. Existing unsuper-\n*The corresponding author.\nvised methods often focus on the design of various con-\ntrastive losses [3, 8, 38, 39] and the refinement of noisy\npseudo labels [7, 39, 43, 51]. Most of them pay little atten-\ntion to the improvement of their feature extraction networks,\nwhich are crucial for identification as well.\nOn the contrary, the architecture of feature extrac-\ntion backbones has been extensively investigated in super-\nvised person Re-ID. For example, besides bag of tricks\n(BoT) [27], partition-based [6,35] or multi-granularity [37,\n52] networks are developed to capture fine-grained cues,\nand attention schemes [4, 22, 33] are integrated to concen-\ntrate on discriminative parts.\nRecently, self-attention or\ntransformer mechanisms [17,21,23,28,32,50,58] have also\nbeen successfully applied to supervised Re-ID. Some of\nthem [21, 23, 50] integrate transformers with convolutional\nneural networks (CNNs) and the others [17,28,32,58] con-\nstruct pure transformer architectures to explore long-range\ncontexts. It has been validated that both fine-grained cues\nand long-range contexts greatly boost the performance of\nsupervised Re-ID.\nInspired by the techniques developed in supervised Re-\nID, especially by the CNN-based Multiple Granularity Net-\nwork (MGN) [37] and the pure transformer networks [17,\n28, 32, 58], we intend to investigate the way of extract-\ning multi-grained features from a pure transformer network\nto address the more challenging unsupervised Re-ID prob-\nlem. To this end, we take the TransReID-SSL [17, 28] net-\nwork as our backbone, which builds upon the Vision Trans-\nformer (ViT) [9] but slightly modifies ViT to adapt to the\nRe-ID task.\nThen, we construct a dual-branch architec-\nture based on the transformer backbone. Each branch du-\nplicates the last transformer layer to produce outputs inde-\npendently. The output local tokens in each branch, which\nare corresponding to input patches [2,32], are reshaped and\nuniformly partitioned into a number of stripes to learn part-\nlevel features. Meanwhile, the output global tokens of both\nbranches are averaged to generate a global feature. By this\narXiv:2211.12280v1  [cs.CV]  22 Nov 2022\nmeans, multi-grained features are effectively learned from\nthe pure transformer network.\nIn order to take advantage of the learned multi-grained\nfeatures for unsupervised Re-ID, we choose offline-online\nassociated camera-aware proxies (O2CAP) [39] as our\nlearning framework. O2CAP [39] is a state-of-the-art un-\nsupervised Re-ID method based on a CNN backbone. We\nextend it by replacing its backbone with our transformer-\nbased dual-branch network for feature extraction. In addi-\ntion, we define offline and online contrastive learning losses\nwith respect to both global and part-level features.\nAlthough some recent works also attempt to learn part-\nlevel features from pure transformer networks [32, 58] for\nsupervised Re-ID or leverage CNN-based part-level fea-\ntures for unsupervised Re-ID [7,13,25,47,48], our method\ndistinguishes itself from them in the following aspects:\n• Inspired by MGN [37], we design a dual-branch ar-\nchitecture appended to a pure transformer network to\nlearn features at multiple granularities, which is sim-\nple but effective to mine fine-grained cues and capture\nlong-range contexts at the same time.\n• Based upon O2CAP [39], we additionally define part-\nfeature based offline and online contrastive learning\nlosses to leverage the learned multi-grained features\nfor unsupervised Re-ID, boosting the performance sig-\nnificantly.\n• Extensive experiments on three person Re-ID datasets\nshow that our method outperforms state-of-the-art un-\nsupervised methods by a considerable margin, greatly\nmitigating the gap to supervised counterparts.\n2. Related Work\n2.1. Unsupervised Person Re-ID\nPrevious unsupervised Re-ID methods can be roughly\ngrouped into unsupervised domain adaptation (UDA)-\nbased [11, 14, 18, 42, 53] or purely unsupervised [3, 8, 24,\n38, 39, 46, 49] categories. In recent years, the purely un-\nsupervised Re-ID has attracted more research interest due\nto its promising performance and no use of extra labeled\ndatasets. Most unsupervised methods are clustering-based,\ntaking clustering to produce pseudo labels and training a\nRe-ID model under the supervision of pseudo labels itera-\ntively. Advanced performance has been recently achieved\nvia the design of clustering techniques [49,55], the leverage\nof contrastive learning techniques [8,38,39], the refinement\nof noisy pseudo labels [7, 43, 51], etc. Although numer-\nous methods have been developed, almost all of them leave\ntheir CNN-based feature extraction backbones unchanged.\nAn exception is TransReID-SSL [28], which attempts to ex-\nploit a pure transformer network for unsupervised Re-ID\nand significantly boosts the performance.\n2.2. Part-based Person Re-ID\nPart-based features can encode fine-grained cues that\nare shown to be helpful for discriminating IDs under the\nsupervised setting. Various methods, which adopt direct\npartition [6, 35], multiple granularities [37, 52], attention\nschemes [22, 33], or transformer-based techniques [21, 23,\n32, 50, 58], have been developed to boost the performance\nof supervised Re-ID. In contrast, there are only a few stud-\nies on exploiting part-level features for unsupervised per-\nson Re-ID. These unsupervised methods either design part-\nfeature based losses [13,47,48], or integrate part-level fea-\ntures for similarity measurement [25], or use part features\nto refine pseudo labels [7]. In these unsupervised methods,\npart-level features are all extracted from CNN backbones.\n2.3. Transformer-based Person Re-ID\nTransformer has demonstrated its great potential in var-\nious vision tasks [9, 26]. It has also been applied to person\nRe-ID in recent years, mostly under the supervised setting.\nInitial methods [21, 23, 50] often integrate transformer lay-\ners with CNN backbones to capture fine-grained cues and\nlong-range contexts. For instance, TPM [21] constructs a\ntransformer-based module to adaptively merge parts gen-\nerated from CNN-based networks such as PCB [35] or\nMGN [37].\nHAT [50] designs a hierarchical aggrega-\ntion transformer upon ResNet-50 [16] to integrate low-\nlevel details with high-level semantics. PAT [23] appends\na part-aware transformer after a CNN backbone to dis-\ncover diverse parts. Recently, pure transformer architec-\ntures, such as TransReID [17], AAformer [58], and LA-\nTransformer [32] are also developed for supervised Re-ID.\nAAformer [58] introduces part tokens to learn part features\nin transformer while LA-Transformer [32] designs a PCB-\nlike strategy to extract part-level features. In contrast to\nthem [32, 58], we construct a dual-branch architecture ap-\npended to a pure transformer to learn features at multiple\ngranualities and utilize the multi-grained features for unsu-\npervised Re-ID.\n3. The Proposed Method\nThis work aims to investigate the way of extracting\nmulti-grained features from a pure transformer network to\naddress unsupervised Re-ID. To this end, we take a modi-\nfied Vision Transformer (ViT) [9, 17, 28] as our backbone\nand construct a dual-branch architecture appended to the\nbackbone for feature extraction. The last transformer layer\nis duplicated for each branch to produce outputs indepen-\ndently. Then, the local tokens in each branch are reshaped\nand uniformly partitioned into multiple stripes to yield part-\nlevel features, and meanwhile the global tokens of both\nbranches are averaged to get a global feature. Further, based\nupon O2CAP [39], offline and online contrastive learning\nlosses with respect to both global and part-level features are\ndefined for unsupervised learning. During test time, only\nthe global feature obtained from two global tokens is used\nfor inference. Figure 1 illustrates the entire framework.\n3.1. Transformer Backbone\nWe choose the pure transformer architecture used in\nTransReID-SSL [28] as our backbone. It builds upon Vi-\nsion Transformer (ViT) [9] but makes two modifications\nto adapt to person Re-ID. Specifically, as shown in Fig-\nure 2, an input image x ∈RH×W ×3 is first passed through\nan Instance-Batch Normalization (IBN)-based convolution\nstem to produce a feature map x′ ∈R\nH\n2 × W\n2 ×C, in which\nH and W are the height and width of the image and C is\nthe number of channels. The IBN-based convolution stem,\ninspired by IBN-Net [29] that is extensively used in CNN-\nbased Re-ID methods, takes place of the vanilla convolution\nstem [45] which is previously added in ViT to further im-\nprove training stability and generalization ability. Then, the\nfeature map is split into N = HW\nP 2 non-overlapping patches\nand each patch is in size of P\n2 × P\n2 . Further, each patch is\nprojected into a D-dimensional feature f ∈RD as an em-\nbedded token. A learnable class token cls is prepended to\nthe sequence of patch tokens. Finally, position embeddings,\ntogether with additional camera embeddings [17, 28], are\nadded to the embedded patch tokens to form the input for a\ntransformer network. The entire procedure of tokenization\nis formally defined as follows:\n  \\ be gin {ali g n e d}  \\ma th\nbf  {f}_i  &=  \\ps i _ i  \\ left (\\mathrm {ICS}\\left (\\mathbf {x}\\right ) \\right ), \\ \\ i = 1, ..., N; \\\\ \\mathbf {z}^0 &= \\left [{cls};\\ \\mathbf {f}_1;\\ ...;\\ \\mathbf {f}_N \\right ] + \\mathbf {p} + \\lambda _c \\mathbf {c}. \\end {aligned} \n(1)\nHere, ICS denotes the IBN-based convolution stem and ψi\nis the partition and projection operation. p ∈R(N+1)×D\nis the position embedding, c ∈R(N+1)×D is the camera\nembedding, and λc is a hyper-parameter for weighting.\nThe embedded tokens z0 are then input to the trans-\nformer network composed of L transformer layers exactly\nas ViT [9]. Each layer consists of Multi-head Self-Attention\n(MSA) and Multi-Layer Perceptron (MLP) modules. The\ncomputation of layer l ∈{1, ..., L} is defined as follows:\n  \\la b el {eq:MSA_FF N } \\be\ngi n  {aligned} \\ha t  {\\mathbf {z}}^{l-1} &= \\mathrm {MSA}(\\mathrm {LN}(\\mathbf {z}^{l-1})) + \\mathbf {z}^{l-1}, \\\\ \\mathbf {z}^{l} &= \\mathrm {MLP}(\\mathrm {LN}(\\hat {\\mathbf {z}}^{l-1})) + \\hat {\\mathbf {z}}^{l-1}, \\end {aligned} \n(2)\nin which LN denotes layer normalization. Therefore, we\ndenote the final output of the transformer network as\n  \\\nm\nathbf  {\nz } ^L =  \\\nl\ne\nft [{cls}^L;\\ \\mathbf {f}_1^L;\\ ...;\\ \\mathbf {f}_N^L \\right ]. \n(3)\n3.2. Multiple Granularity Architecture\nInspired by the CNN-based Multiple Granularity Net-\nwork (MGN) [37], we construct a dual-branch architecture\nto extract features at multiple granularities. The architecture\nis illustrated in Figure 1. We modify the above-introduced\ntransformer backbone by duplicating the L-th transformer\nlayer while keeping all previous layers unchanged. Each\nbranch has one copy of the L-th transformer layer, which\noutputs one global token clsL together with N local tokens\n\u0002\nf L\n1 ; ...; f L\nN\n\u0003\n. As pointed out in [2, 32], the local tokens\nsubstantially correspond to the original input patches and\nhence encode fine-grained local information. We therefore\nreshape the local tokens in each branch and partition them\ninto horizontal stripes to generate part-level features.\nMore specifically, we denote the output local tokens in\nthe i-th branch as\n\u0002\nf L\ni,1; ...; f L\ni,N\n\u0003\n, which are reshaped into\na feature map f ′\ni in size of H\nP × W\nP × D. Then, the reshaped\nfeature map is uniformly split into Ki horizontal parts and\neach part gets a D-dimensional feature by average pooling\nall features belonging to this part. Formally, this procedure\nis described as\n  \n\\ b egin {a\nli\ng n\ned} \\mat h b\nf {\nf}\n' _ i  & = \n\\mat h rm {Res hape}\\ le f\nt ( \\ l e f t [\\ma thbf {f}_{i,1}^L;\\ ...;\\ \\mathbf {f}_{i,N}^L \\right ] \\right ), \\ \\ i = 1, 2; \\\\ \\mathbf {h}_{i,k} &= \\mathrm {AvgPool}\\left (\\mathrm {Split}\\left (\\mathbf {f}'_i, k\\right )\\right ), \\ \\ k = 1, ..., K_i. \\end {aligned} \n(4)\nHere, hi,k denotes the k-th part-level feature in the i-th\nbranch. With a bit change of the subscripts, we denote the\nset of all part features as {hk}K\nk=1, where K = K1 + K2.\nNote that each branch also outputs a global token. There-\nfore, different from MGN [37] that additionally constructs a\nglobal branch, we get a global feature by simply averaging\nthe global tokens produced in both branches. That is,\n  \\\nm\na\nthbf\n { g} =\n \n\\\nfrac {1}{2} \\left (cls_1^L + cls_2^L\\right ). \n(5)\nBoth the global feature and part-level features are further\npassed through a batch normalization (BN) layer and a L2\nnormalization layer to have unit norms.\n3.3. Unsupervised Re-ID Losses\nWe intend to leverage multi-grained features to promote\nthe performance of unsupervised Re-ID. In this work, we\nbuild our losses upon O2CAP [39], which is a state-of-the-\nart unsupervised method using CNN-extracted global fea-\ntures. We extend it to include both global and part-level\nfeatures for learning.\nGiven an unlabeled dataset, O2CAP performs the un-\nsupervised learning by conducting a clustering step and a\nmodel learning step alternatively and iteratively. In each\nclustering step, it utilizes DBSCAN [10] to cluster all im-\nages with respect to their global features, and then split\neach cluster into multiple camera-aware proxies according\nto camera information. The proxies are taken as pseudo la-\nbels for supervision. Let us denote the dataset with pseudo\nlabels as D = {(xi, ˜yi)}NI\ni=1, where image xi extracts a\nglobal feature gi and a set of part-level features {hk\ni }K\nk=1.\ncls token \nLocal token \nBN: Batch Normalization \nL2: L2-Normalization \nGAP: Global Average Pooling  \nGAP, BN, L2 \nMean, BN, L2 \nGAP, BN, L2 \nTransformer Layer \n \nReshape, 3-split \nTransformer Layer \n \nReshape, 2-split \n𝐱∈ℝ𝐻×𝑊×3 \n𝐳𝐿∈ℝ𝑁+1 ×𝐷 \n𝐡𝒌∈ℝ𝐷 \n𝐠∈ℝ𝐷 \n... \nℒ𝑜𝑓𝑓\n𝑔\n+ ℒ𝑜𝑛\n𝑔 \n... \nTransformer Layer \n \nℒ𝑜𝑓𝑓\n𝑘\n+ ℒ𝑜𝑛\n𝑘 \nTokenization \nTransformer \nBackbone \nFigure 1. An overview of the proposed method. It constructs a dual-branch architecture appended to a pure transformer backbone to extract\nfeatures at multiple granularities. Besides, offline and online contrastive learning losses defined with respect to both global and part-level\nfeatures are employed for unsupervised learning.\nTransformer Layer 𝟏 \nTransformer Layer 𝑳 \n0 \n1 \n2 \nN \n... \nIBN-based \nConvolution \nStem \n... \nProjection Layer \n... \n... \nPosition  \nEmbedding \nCamera  \nEmbedding \ncls \nFigure 2. An illustration of the transformer backbone [28].\nIt\nbuilds upon ViT [9] but makes the following modifications: 1)\nreplace the vanilla convolution stem [45] by the IBN-based convo-\nlution stem and 2) additionally include camera embeddings.\nBesides, ˜yi ∈{1, · · · , Np} is a generated pseudo label, Np\nis the number of proxies and NI is the number of images.\nThen, a proxy-level memory bank K ∈RNp×D is con-\nstructed, in which each entry stores the centroid of a proxy’s\nglobal feature. During back-propagation, when image xi is\ninput, the entry corresponding to the pseudo class ˜yi is up-\ndated via\n  \\mat h cal {K} [ \\t i lde {y}_i] \\leftarrow \\mu \\mathcal {K}[\\tilde {y}_i] + (1 - \\mu ) \\mathbf {g}_i, \\label {eq:mu} \n(6)\nwhere K[˜yi] denotes the ˜yi-th entry of the memory bank and\nµ ∈[0, 1] is an updating rate.\nAssisted with the memory bank, two contrastive learning\nlosses are designed based on offline and online associations\nrespectively. For image xi, the offline association retrieves\na positive proxy set P1 directly according to the offline clus-\ntering and splitting results while gets a negative set Q1 from\nthe remaining hard negative proxies. P1 and Q1, respec-\ntively, store the indexes of associated positive and negative\nproxies. Then, the offline contrastive loss is defined as\n  \n\\sm a l\nl\n \n\\ma\nt\nh\nc a\nl {L\n}\n^g_{\noff\n}=-\\ sum\n \n_{i=\n1}^{ B}\\ l e\nft (\n\\fra c {\n1\n}\n{ |\\mathcal {P}_1|}\\sum _{u \\in \\mathcal {P}_1} \\log \\frac {S(u, \\mathbf {g}_i)}{\\sum \\limits _{p \\in \\mathcal {P}_1} S(p, \\mathbf {g}_i) + \\sum \\limits _{q \\in \\mathcal {Q}_1} S(q, \\mathbf {g}_i)}\\right ), \\label {eq_inter1} \n(7)\nin which S(u, gi) = exp(K[u]T gi/τ), τ is a temperature\nfactor. Moreover, | · | denotes the cardinality of a set and B\nis the batch size.\nHowever, offline association is noisy due to the imper-\nfect clustering results. To remedy the noise, an online asso-\nciation strategy is proposed. It employs an instance-proxy\nbalanced similarity and a camera-aware nearest neighbor\nscheme to select a positive proxy set P2 and a negative set\nQ2 for each anchor image xi on the fly. The online con-\ntrastive loss is defined as\n  \n\\s m a\nl\nl\n \\m\na\nt\nh c\nal {\nL\n}^g_\n{on\n}=-\\ sum\n \n_{i=\n1}^{ B}\\ l e\nft (\n\\fra c {\n1\n}\n{ |\\mathcal {P}_2|}\\sum _{u \\in \\mathcal {P}_2} \\log \\frac {S(u, \\mathbf {g}_i)}{\\sum \\limits _{p \\in \\mathcal {P}_2} S(p, \\mathbf {g}_i) + \\sum \\limits _{q \\in \\mathcal {Q}_2} S(q, \\mathbf {g}_i)}\\right ). \\label {eq_inter2} \n(8)\nThe above-introduced losses are defined with respect to\nglobal features. In order to leverage part-level features, we\nadditionally construct memory banks and define both types\nof losses for each part, while keeping the clustering step\nunchanged (i.e. only using the global features for clustering\nas in O2CAP [39]). The entire loss for training is as follows:\n  \\m\nath c al\n { L } \n=\n \n\\\nm\nath\nc\nal\n {L } ^g\n_{\no\nff} + \\mathcal {L}^g_{on} + \\lambda _p \\frac {1}{K} \\sum _{k=1}^{K}\\left (\\mathcal {L}^k_{off} + \\mathcal {L}^k_{on}\\right ), \n(9)\nwhere λp is a weighting factor to balance global and part-\nbased losses.\n4. Experiments\n4.1. Datasets and Evaluation Metrics\nTo validate the proposed method, we conduct a se-\nries of experiments on three person Re-ID datasets: Mar-\nket1501 [54], DukeMTMC-reID [31], and MSMT17 [41].\nThe former two datasets are captured on university cam-\npus with outdoor scenarios only, while MSMT17 [41] con-\ntains both indoor and outdoor scenarios and therefore is\nmore challenging. The number of cameras, together with\nthe number of IDs and images contained in training, query,\nand gallery sets on three datasets are listed in Table 1. The\ntraining sets are used for unsupervised learning. During test\ntime, each image in query is matched to similar images in\ngallery sets.\nDataset\nTraining Set\nQuery Set\nGallery Set\n#Camera\n#ID\n#Image\n#ID\n#Image\n#ID\n#Image\nMarket1501\n6\n751\n12,936\n750\n3,368\n751\n15,913\nDukeMTMC-reID\n8\n702\n16,522\n702\n2,228\n1,110\n17,661\nMSMT17\n15\n1,041\n32,621\n3,060\n11,659\n3,060\n82,161\nTable 1. The number of cameras, IDs and images on three datasets.\nAs the common practice [3, 39], we employ the exten-\nsively used mean Average Precision (mAP) and Cumula-\ntive Matching Characteristic (CMC) for performance eval-\nuation. The CMC metric is reported via Rank-1, Rank-5,\nand Rank-10. Besides, no post-processing techniques (e.g.\nRe-ranking [57]) are used.\n4.2. Implementation Details\nOur transformer backbone is built upon the ViT-Small/16\nmodel [9,28], which has L = 12 transformer layers and the\nnumber of heads in each MSA is 6, the feature dimension\nis D = 384. The backbone is pre-trained on a large-scale\nunlabeled dataset LUPerson [12]. Each image is resized to\n384 × 128 and augmented with random horizontal flipping,\ncropping and erasing [56]. Patch size P = 16, that is, each\nfeature map produced after the IBN-based convolution stem\nis split into patches in size of 8 × 8. The weight for camera\nembedding is λc = 3. The numbers of parts in two branches\nare K1 = 2 and K2 = 3 unless otherwise specified.\nIn unsupervised learning, all involved hyper-parameters\nare set the same as those in O2CAP [39]. That is, the up-\ndating rate µ = 0.2, the temperature factor τ = 0.07, and\nthe batch size B = 32. Moreover, the weight of part-based\nlosses is λp = 0.1. The model is trained by SGD optimizer\nfor 50 epochs, with a momentum of 0.9, a learning rate of\n0.00035 and a weight decay of 0.0005. The learning rate\nis regulated by a warmup scheduler that multiplies 0.01 on\nthe initial learning rate and linearly enlarges it to the stan-\ndard value in previous 10 epochs. At epoch 20 and 40, the\nlearning rate is divided by 10. The entire algorithm is im-\nplemented in PyTorch [30]. To accelerate training, we use\nhalf-precision floating point (FP16) computation in training\nwhile use full precision (FP32) for test. All experiments are\nrun on a single NVIDIA GTX 1080 GPU.\n4.3. Ablation Studies\nWe first conduct a series of experiments to validate the\neffectiveness of the proposed method, which is referred to\nas the Transformer-based Multi-Grained Feature (TMGF)\nmethod. Different variants of our model are investigated\nand all experiments are conducted on MSMT17 [41].\nEffectiveness of Two Branches. In order to investigate the\neffectiveness of our dual-branch architecture, we compare\nthe full model with three variants, including: 1) TMGF1: a\nbaseline model that simply adopts the original transformer\nbackbone and uses the global token as the image feature for\nlearning; 2) TMGF2: a model with a single branch and the\nlocal tokens are partitioned into two parts, and meanwhile\nboth global and part-level features are taken for learning;\nand 3) TMGF3: a model with a single branch similar to\nTMGF2 but with three partitions for local tokens. The per-\nformance of all model variants are reported in Table 2. From\nthe results we observe that the leverage of local tokens can\nconsistently improve the performance, and 2-partition out-\nperforms 3-partition if only a single branch is constructed.\nWhen two branches are working together, the mAP perfor-\nmance is further boosted while Rank-1 is slightly dropped\n(comparing TMGF vs. TMGF2).\nModels\nDuplicate Layer L\nPartition\nMSMT17\n2-split\n3-split\nmAP\nRank-1\nTMGF1 (Baseline)\n53.5\n81.7\nTMGF2 (Single B.)\n✓\n57.8\n83.6\nTMGF3 (Single B.)\n✓\n54.7\n82.0\nTMGF4 (w/o Dup.)\n✓\n✓\n56.8\n82.8\nTMGF (Full)\n✓\n✓\n✓\n58.2\n83.3\nTable 2. Comparison of the proposed full model and its variants\nin different architectures. “Single B.” denotes “single branch” and\n“w/o Dup.” denotes “without duplication”.\nEffectiveness of Duplicating Layer L. In our dual-branch\narchitecture, the last transformer layer (i.e. layer L) is dupli-\ncated so that each branch has one copy to produce indepen-\ndent outputs for different partition. An alternative design\nis to simply append two branches after the original trans-\nformer backbone without duplication, which performs two\ndifferent partitions on the same output local tokens. We re-\nfer to this design as the TMGF4 model, whose performance\nis also presented in Table 2. When different partitions are\nconducted on the same outputs, the network may mix up\nthe learning of local cues, leading to a degenerated perfor-\nmance. Therefore, it is necessary to duplicate the last trans-\nformer layer for better learning.\nImpact of Feature Granularity. The granularity of part-\nlevel features may influence the performance as well. Thus,\nwe investigate three different partition ways, which split lo-\ncal tokens of two branches into 2 and 3 parts, or 2 and 4\nparts, or 3 and 4 parts respectively. Table 3 lists the perfor-\nmance under different partitions. We observe that the best\nperformance is achieved by the one with 2 and 3 parts in\ntwo branches. Splitting local tokens into more parts does\nnot bring improvement. On the contrary, more fine-grained\npartition degenerates the performance.\nGranularities\nMSMT17\nmAP\nRank-1\n[2, 3]\n58.2\n83.3\n[2, 4]\n57.1\n82.9\n[3, 4]\n56.6\n82.7\nTable 3. The performance under different partitions. [K1, K2] de-\nnotes that the local tokens in the first and second branches are split\ninto K1 and K2 parts respectively.\nImpact of the Fusion of Global Tokens. Each branch of\nour dual-branch architecture yields a global token. It im-\nplies that there are different ways to generate the global fea-\nture for learning. For example, we can choose the global\ntoken produced in either branch as the global feature, or av-\nerage both global tokens as introduced in Section 3.2. In\nthis experiment we investigate the different ways and com-\npare their performance in Table 4. The results show that\nthe way of averaging both global tokens outperforms the\nother ways by a considerable margin, because it encourages\na more balanced learning of two branches.\nModels\nGlobal Feature\nMSMT17\nmAP\nRank-1\nTMGF5\ng = clsL\n1\n55.9\n82.5\nTMGF6\ng = clsL\n2\n56.8\n82.8\nTMGF(Full)\ng = 1\n2\n\u0000clsL\n1 + clsL\n2\n\u0001\n58.2\n83.3\nTable 4. The performance of the models using different global fea-\ntures. clsL\n1 and clsL\n2 denote the global tokens output in the first\nand second branch respectively.\n4.4. Visualization of Attention Maps\nTo further illustrate the effectiveness of our dual-branch\narchitecture, we visualize attention maps via the Attention\nRollout scheme [1]. Figure 3 presents the attention maps\nobtained by the full model (TMGF), together with the maps\nobtained by the baseline model (TMGF1) for comparison.\nWe notice that, with the exploitation of multi-grained fea-\ntures, our full model can attend to person regions more com-\npletely (e.g. examples in the first row) and focus more on\ndiscriminative regions (e.g. examples in the fourth row).\nMeanwhile, the full model is less distracted by background\n(e.g. examples in the second row) or occluded regions (e.g.\nexamples in the third row). These properties enable our\nmodel to achieve a promising Re-ID performance even if\nthe dataset is very challenging.\n(a) \n(b) \n(c) \n(a) \n(b) \n(c) \nFigure 3. Visualization of attention maps. (a) color images, (b)\nattention maps obtained by the baseline model (TMGF1), and (c)\nattention maps obtained by our full model (TMGF).\n4.5. Comparison to State-of-the-arts\nFinally, we compare our approach with state-of-the-art\nmethods on three person Re-ID datasets. Table 5 presents\nthe comparison results.\nMethods\nReference\nMarket1501\nDukeMTMC-reID\nMSMT17\nmAP\nRank-1\nRank-5\nRank-10\nmAP\nRank-1\nRank-5\nRank-10\nmAP\nRank-1\nRank-5\nRank-10\nPurely unsupervised methods\nBUC [24]\nAAAI19\n38.3\n66.2\n79.6\n84.5\n27.5\n47.4\n62.6\n68.4\n-\n-\n-\n-\nHCT [49]\nCVPR20\n56.4\n80.0\n91.6\n95.2\n50.7\n69.6\n83.4\n87.4\n-\n-\n-\n-\nClusterContrast [8]\narxiv21\n83.0\n92.9\n97.2\n98.0\n73.6\n85.5\n92.2\n94.3\n31.2\n61.5\n71.8\n76.7\nCAP [38]\nAAAI21\n79.2\n91.4\n96.3\n97.7\n67.3\n81.1\n89.3\n91.8\n36.9\n67.4\n78.0\n81.4\nIICS [46]\nCVPR21\n72.9\n89.5\n95.2\n97.0\n64.4\n80.0\n89.0\n91.6\n26.9\n56.4\n68.8\n73.4\nRLCC [51]\nCVPR21\n77.7\n90.8\n96.3\n97.5\n69.2\n83.2\n91.6\n93.8\n27.9\n56.5\n68.4\n73.1\nICE [3]\nICCV21\n82.3\n93.8\n97.6\n98.4\n69.9\n83.3\n91.5\n94.1\n38.9\n70.2\n80.5\n84.4\nMGH [43]\nMM21\n81.7\n93.2\n96.8\n98.1\n70.2\n83.7\n92.1\n93.7\n40.6\n70.2\n81.2\n84.5\nMCRN [42]\nAAAI22\n80.8\n92.5\n-\n-\n69.9\n83.5\n-\n-\n31.2\n63.6\n-\n-\nMGCE-HCL [34]\nACPR22\n79.6\n92.1\n-\n-\n67.5\n82.5\n-\n-\n-\n-\n-\n-\nO2CAP [39]\narxiv22\n82.7\n92.5\n96.9\n98.0\n71.2\n83.9\n91.3\n93.4\n42.4\n72.0\n81.9\n85.4\nPPLR [7]\nCVPR22\n84.4\n94.3\n97.8\n98.6\n-\n-\n-\n-\n42.2\n73.3\n83.5\n86.5\nTransReID-SSL† [28]\narxiv21\n89.6\n95.3\n-\n-\n-\n-\n-\n-\n50.6\n75.0\n-\n-\nTMGF†\nThis work\n89.5\n95.5\n98.0\n98.7\n76.8\n86.7\n92.9\n94.1\n58.2\n83.3\n90.2\n92.1\nUDA-based methods\nSSG [13]\nICCV19\n58.3\n80.0\n90.0\n92.4\n53.4\n73.0\n80.6\n83.2\n13.3\n32.2\n-\n51.2\nSpCL [14]\nNIPS20\n76.7\n90.3\n96.2\n97.7\n68.8\n82.9\n90.1\n92.5\n26.5\n53.1\n65.8\n70.5\nIsobe et al. [19]\nICCV21\n83.4\n94.2\n-\n-\n70.8\n83.5\n-\n-\n36.3\n66.6\n-\n-\nDARC [18]\nAAAI22\n85.1\n94.1\n97.6\n98.7\n-\n-\n-\n-\n35.2\n64.5\n76.2\n80.4\nFully supervised methods\nABD-Net [4]\nICCV19\n88.3\n95.6\n-\n-\n78.6\n89.0\n-\n-\n60.8\n82.3\n90.6\n-\nst-ReID [36]\nAAAI19\n86.7\n97.2\n99.3\n99.5\n82.8\n94.0\n97.0\n97.8\n-\n-\n-\n-\nPAT† [23]\nCVPR21\n88.0\n95.4\n-\n-\n78.2\n88.8\n-\n-\n-\n-\n-\n-\nTransReID† [17]\nICCV21\n89.5\n95.2\n-\n-\n82.6\n90.7\n-\n-\n69.4\n86.2\n-\n-\nLA-Transformer† [32]\narxiv21\n94.5\n98.3\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nPFD† [40]\nAAAI22\n89.7\n95.5\n-\n-\n83.2\n91.2\n-\n-\n-\n-\n-\n-\nTransReID-SSL(w/ GT)† [28]\narxiv21\n91.3\n96.2\n-\n-\n-\n-\n-\n-\n68.1\n86.1\n-\n-\nTMGF(w/ GT)†\nThis work\n91.9\n96.3\n98.9\n99.3\n83.1\n92.3\n96.4\n97.4\n70.3\n88.2\n94.1\n95.4\nTable 5. Comparison with state-of-the-art methods. † indicates that the method is using a transformer backbone.\nComparison with Unsupervised Methods.\nWe include\n17 representative or recent unsupervised person Re-ID\nmethods for comparison, among which 13 methods are\npurely unsupervised and 4 methods are UDA-based. Ex-\ncept TransReID-SSL [28], all previous methods are based\non CNN backbones pre-trained on ImageNet [20].\nIt\ncan be seen that the transformer feature extraction back-\nbone [28] greatly boosts the performance because of its\nnetwork architecture and the pre-train on LUPerson [12].\nOur method is built upon TransReID-SSL [28]. In contrast\nto TransReID-SSL that uses contrastive learning losses de-\nfined in ClusterContrast [8], we design a dual-branch ar-\nchitecture to extract multi-grained features and define both\nglobal and part-based contrastive learning losses in the form\nof O2CAP [39]. On the most challenging dataset MSMT17,\nour method surpasses TransReID-SSL by 7.6% mAP and\n8.3% Rank-1. When compared to the CNN-based O2CAP\nmethod, we improve the performance by a significant mar-\ngin. Especially on MSMT17, 15.8% mAP and 11.3% Rank-\n1 improvements are achieved.\nComparison with Fully Supervised Methods. We include\nseven recent fully supervised Re-ID methods for reference,\nin which ABD-Net [4] and st-ReID [36] are CNN-based and\nall others [17, 23, 28, 32, 40] are transformer-based. Mean-\nwhile, the performance of our model trained with ground-\ntruth labels is also provided, which indicates the upper\nbound performance our method can achieve. From the re-\nsults we see that our unsupervised model has already per-\nformed better than several supervised methods on Market-\n1501.\nThe performance gap between our unsupervised\nmodel and the supervised ABD-Net [4] is very small on\nDukeMTMC-reID and MSMT17. In addition, our model\ntrained with ground-truth performs better than the super-\nvised TransReID-SSL [36], validating the effectiveness of\nour multi-grained feature extraction.\n5. Conclusion\nIn this work, we have presented an approach to extract\nmulti-grained features from a pure transformer network and\nleverage the multi-grained features for unsupervised person\nRe-ID. The designed dual-branch architecture is simple but\neffective for feature extraction. Benefited from the multi-\ngrained features and part-feature based learning losses, our\nmethod outperforms existing unsupervised Re-ID methods\nby a considerable margin, greatly mitigating the perfor-\nmance gap to supervised counterparts.\nReferences\n[1] Samira Abnar and Willem Zuidema.\nQuantifying atten-\ntion flow in transformers. arXiv preprint arXiv:2005.00928,\n2020.\n[2] Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew\nZhai, and Dmitry Kislyuk. Toward transformer-based object\ndetection. arXiv preprint arXiv:2012.09958, 2020.\n[3] Hao Chen, Benoit Lagadec, and Francois Bremond.\nIce:\nInter-instance contrastive encoding for unsupervised person\nre-identification. In ICCV, pages 14960–14969, 2021.\n[4] Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang\nChen, Yang Yang, Zhou Ren, and Zhangyang Wang. Abd-\nnet: Attentive but diverse person re-identification. In ICCV,\npages 8351–8361, 2019.\n[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In ICML, pages 1597–1607, 2020.\n[6] De Cheng, Yihong Gong, Sanping Zhou, Jinjun Wang, and\nNanning Zheng. Person re-identification by multi-channel\nparts-based cnn with improved triplet loss function.\nIn\nCVPR, pages 1335–1344, 2016.\n[7] Yoonki Cho, Woo Jae Kim, Seunghoon Hong, and Sung-Eui\nYoon. Part-based pseudo label refinement for unsupervised\nperson re-identification. In CVPR, pages 7308–7318, 2022.\n[8] Zuozhuo Dai, Guangyuan Wang, Weihao Yuan, Xiaoli Liu,\nSiyu Zhu, and Ping Tan. Cluster contrast for unsupervised\nperson re-identification. arXiv preprint arXiv:2103.11568,\n2021.\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021.\n[10] Martin Ester, Hans-Peter Kriegel, J¨org Sander, Xiaowei Xu,\net al. A density-based algorithm for discovering clusters in\nlarge spatial databases with noise. In KDD, pages 226–231,\n1996.\n[11] Hao Feng, Minghao Chen, Jinming Hu, Dong Shen, Haifeng\nLiu, and Deng Cai. Complementary pseudo labels for un-\nsupervised domain adaptation on person re-identification.\nIEEE Transactions on Image Processing, 30:2898–2907,\n2021.\n[12] Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu\nYuan, Lei Zhang, Houqiang Li, and Dong Chen.\nUnsu-\npervised pre-training for person re-identification. In CVPR,\npages 14750–14759, 2021.\n[13] Yang Fu, Yunchao Wei, Guanshuo Wang, Yuqian Zhou,\nHonghui Shi, and Thomas S Huang. Self-similarity group-\ning: A simple unsupervised cross domain adaptation ap-\nproach for person re-identification. In ICCV, pages 6112–\n6121, 2019.\n[14] Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, et al. Self-\npaced contrastive learning with hybrid memory for domain\nadaptive object re-id. In NeurIPS, pages 11309–11321, 2020.\n[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In CVPR, pages 9729–9738, 2020.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\npages 770–778, 2016.\n[17] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang.\nTransreid: Transformer-based object re-\nidentification. In ICCV, pages 15013–15022, 2021.\n[18] Zhengdong Hu, Yifan Sun, Yi Yang, and Jianguang Zhou.\nDivide-and-regroup clustering for domain adaptive person\nre-identification. In AAAI, 2022.\n[19] Takashi Isobe, Dong Li, Lu Tian, Weihua Chen, Yi Shan,\nand Shengjin Wang. Towards discriminative representation\nlearning for unsupervised person re-identification. In ICCV,\npages 8526–8536, 2021.\n[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In NeurIPS, 2012.\n[21] Shenqi Lai, Zhenhua Chai, and Xiaolin Wei. Transformer\nmeets part model: Adaptive part division for person re-\nidentification. In ICCVW, pages 4150–4157, 2021.\n[22] Wei Li, Xiatian Zhu, and Shaogang Gong. Harmonious at-\ntention network for person re-identification. In CVPR, pages\n2285–2294, 2018.\n[23] Yulin Li, Jianfeng He, Tianzhu Zhang, Xiang Liu, Yongdong\nZhang, and Feng Wu. Diverse part discovery: Occluded per-\nson re-identification with part-aware transformer. In CVPR,\npages 2898–2907, 2021.\n[24] Yutian Lin, Xuanyi Dong, Liang Zheng, Yan Yan, and Yi\nYang. A bottom-up clustering approach to unsupervised per-\nson re-identification. In AAAI, pages 8738–8745, 2019.\n[25] Yutian Lin, Lingxi Xie, Yu Wu, Chenggang Yan, and Qi\nTian.\nUnsupervised person re-identification via softened\nsimilarity learning. In CVPR, pages 3390–3399, 2020.\n[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, pages 10012–10022, 2021.\n[27] Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei\nJiang. Bag of tricks and a strong baseline for deep person\nre-identification. In CVPRW, 2019.\n[28] Hao Luo, Pichao Wang, Yi Xu, Feng Ding, Yanxin Zhou, Fan\nWang, Hao Li, and Rong Jin. Self-supervised pre-training for\ntransformer-based person re-identification.\narXiv preprint\narXiv:2111.12084, 2021.\n[29] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two\nat once: Enhancing learning and generalization capacities\nvia ibn-net. In ECCV, 2018.\n[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An\nimperative style, high-performance deep learning library. In\nNeurIPS, 2019.\n[31] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,\nand Carlo Tomasi. Performance measures and a data set for\nmulti-target, multi-camera tracking. In ECCV, 2016.\n[32] Charu Sharma, Siddhant R Kapil, and David Chapman. Per-\nson re-identification with a locally aware transformer. arXiv\npreprint arXiv:2106.03720, 2021.\n[33] Jianlou Si, Honggang Zhang, Chun-Guang Li, Jason Kuen,\nXiangfei Kong, Alex C. Kot, and Gang Wang. Dual attention\nmatching network for context-aware feature sequence based\nperson re-identification. In CVPR, pages 5363–5372, 2018.\n[34] He Sun, Mingkun Li, and Chun-Guang Li.\nHybrid con-\ntrastive learning with cluster ensemble for unsupervised per-\nson re-identification.\nIn ACPR, pages 532–546. Springer,\n2022.\n[35] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\nWang. Beyond part models: Person retrieval with refined\npart pooling (and a strong convolutional baseline). In ECCV,\npages 480–496, 2018.\n[36] Guangcong Wang, Jianhuang Lai, Peigen Huang, and Xiao-\nhua Xie. Spatial-temporal person re-identification. In AAAI,\nvolume 33, pages 8933–8940, 2019.\n[37] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi\nZhou. Learning discriminative features with multiple granu-\nlarities for person re-identification. In ACM MM, pages 274–\n282, 2018.\n[38] Menglin Wang, Baisheng Lai, Jianqiang Huang, Xiaojin\nGong, and Xian-Sheng Hua. Camera-aware proxies for un-\nsupervised person re-identification. In AAAI, pages 2764–\n2772, 2021.\n[39] Menglin Wang, Jiachen Li, Baisheng Lai, Xiaojin Gong, and\nXian-Sheng Hua.\nOffline-online associated camera-aware\nproxies for unsupervised person re-identification.\narXiv\npreprint arXiv:2201.05820, 2022.\n[40] Tao Wang, Hong Liu, Pinhao Song, Tianyu Guo, and Wei\nShi. Pose-guided feature disentangling for occluded person\nre-identification based on transformer. In AAAI, pages 2540–\n2549, 2022.\n[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\nPerson transfer gan to bridge domain gap for person re-\nidentification. In CVPR, pages 79–88, 2018.\n[42] Yuhang Wu, Tengteng Huang, Haotian Yao, Chi Zhang,\nYuanjie Shao, Chuchu Han, Changxin Gao, and Nong Sang.\nMulti-centroid representation network for domain adaptive\nperson re-id. In AAAI, pages 2750–2758, 2022.\n[43] Yiming Wu, Xintian Wu, Xi Li, and Jian Tian. Mgh: Meta-\ndata guided hypergraph modeling for unsupervised person\nre-identification. In ACM MM, pages 1571–1580, 2021.\n[44] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination. In CVPR, pages 3733–3742, 2018.\n[45] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr\nDoll´ar, and Ross Girshick. Early convolutions help trans-\nformers see better. NeurIPS, 34:30392–30400, 2021.\n[46] Shiyu Xuan and Shiliang Zhang. Intra-inter camera similar-\nity for unsupervised person re-identification. In CVPR, pages\n11926–11935, 2021.\n[47] Qize Yang, Hong-Xing Yu, Ancong Wu, and Wei-Shi Zheng.\nPatch-based discriminative feature learning for unsupervised\nperson re-identification. In CVPR, pages 3633–3642, 2019.\n[48] Zizheng Yang, Xin Jin, Kecheng Zheng, and Feng Zhao.\nUnleashing potential of unsupervised pre-training with intra-\nidentity regularization for person re-identification. In CVPR,\npages 14298–14307, 2022.\n[49] Kaiwei Zeng, Munan Ning, Yaohua Wang, and Yang Guo.\nHierarchical clustering with hard-batch triplet loss for person\nre-identification. In CVPR, pages 13657–13665, 2020.\n[50] Guowen Zhang, Pingping Zhang, Jinqing Qi, and Huchuan\nLu. Hat: Hierarchical aggregation transformers for person\nre-identification. In ACM MM, pages 516–525, 2021.\n[51] Xiao Zhang, Yixiao Ge, Yu Qiao, and Hongsheng Li. Refin-\ning pseudo labels with clustering consensus over generations\nfor unsupervised object re-identification.\nIn CVPR, pages\n3436–3445, 2021.\n[52] Feng Zheng, Cheng Deng, Xing Sun, Xinyang Jiang, Xi-\naowei Guo, Zongqiao Yu, Feiyue Huang, and Rongrong Ji.\nPyramidal person re-identification via multi-loss dynamic\ntraining. In CVPR, pages 8514–8522, 2019.\n[53] Kecheng Zheng, Wu Liu, Lingxiao He, Tao Mei, Jiebo Luo,\nand Zheng-Jun Zha. Group-aware label transfer for domain\nadaptive person re-identification.\nIn CVPR, pages 5310–\n5319, 2021.\n[54] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-\ndong Wang, and Qi Tian. Scalable person re-identification:\nA benchmark. In ICCV, pages 1116–1124, 2015.\n[55] Yi Zheng, Shixiang Tang, Guolong Teng, Yixiao Ge, Kai-\njian Liu, Jing Qin, Donglian Qi, and Dapeng Chen.\nOn-\nline pseudo label generation by hierarchical cluster dynamics\nfor adaptive person re-identification. In ICCV, pages 8371–\n8381, 2021.\n[56] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In AAAI, pages\n13001–13008, 2020.\n[57] Zhun Zhong, Liang Zheng, and Shaozi Li. Re-ranking per-\nson re-identification with k-reciprocal encoding. In CVPR,\n2017.\n[58] Kuan Zhu, Haiyun Guo, Shiliang Zhang, Yaowei Wang,\nGaopan Huang, Honglin Qiao, Jing Liu, Jinqiao Wang, and\nMing Tang. Aaformer: Auto-aligned transformer for person\nre-identification. arXiv preprint arXiv:2104.00921, 2021.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2022-11-22",
  "updated": "2022-11-22"
}