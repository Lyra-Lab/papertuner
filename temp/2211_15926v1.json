{
  "id": "http://arxiv.org/abs/2211.15926v1",
  "title": "Interpretations Cannot Be Trusted: Stealthy and Effective Adversarial Perturbations against Interpretable Deep Learning",
  "authors": [
    "Eldor Abdukhamidov",
    "Mohammed Abuhamad",
    "Simon S. Woo",
    "Eric Chan-Tin",
    "Tamer Abuhmed"
  ],
  "abstract": "Deep learning methods have gained increased attention in various applications\ndue to their outstanding performance. For exploring how this high performance\nrelates to the proper use of data artifacts and the accurate problem\nformulation of a given task, interpretation models have become a crucial\ncomponent in developing deep learning-based systems. Interpretation models\nenable the understanding of the inner workings of deep learning models and\noffer a sense of security in detecting the misuse of artifacts in the input\ndata. Similar to prediction models, interpretation models are also susceptible\nto adversarial inputs. This work introduces two attacks, AdvEdge and\nAdvEdge$^{+}$, that deceive both the target deep learning model and the coupled\ninterpretation model. We assess the effectiveness of proposed attacks against\ntwo deep learning model architectures coupled with four interpretation models\nthat represent different categories of interpretation models. Our experiments\ninclude the attack implementation using various attack frameworks. We also\nexplore the potential countermeasures against such attacks. Our analysis shows\nthe effectiveness of our attacks in terms of deceiving the deep learning models\nand their interpreters, and highlights insights to improve and circumvent the\nattacks.",
  "text": "PREPRINT\n1\nInterpretations Cannot Be Trusted: Stealthy and\nEffective Adversarial Perturbations against\nInterpretable Deep Learning\nEldor Abdukhamidov\n, Mohammed Abuhamad\n, Simon S. Woo\n, Eric Chan-Tin\n, and Tamer ABUHMED\nAbstract—Deep learning methods have gained increased atten-\ntion in various applications due to their outstanding performance.\nFor exploring how this high performance relates to the proper use\nof data artifacts and the accurate problem formulation of a given\ntask, interpretation models have become a crucial component in\ndeveloping deep learning-based systems. Interpretation models\nenable the understanding of the inner workings of deep learning\nmodels and offer a sense of security in detecting the misuse\nof artifacts in the input data. Similar to prediction models,\ninterpretation models are also susceptible to adversarial inputs.\nThis work introduces two attacks, AdvEdge and AdvEdge+,\nthat deceive both the target deep learning model and the\ncoupled interpretation model. We assess the effectiveness of\nproposed attacks against two deep learning model architectures\ncoupled with four interpretation models that represent different\ncategories of interpretation models. Our experiments include\nthe attack implementation using various attack frameworks. We\nalso explore the potential countermeasures against such attacks.\nOur analysis shows the effectiveness of our attacks in terms of\ndeceiving the deep learning models and their interpreters, and\nhighlights insights to improve and circumvent the attacks.\nIndex Terms—Adversarial Images, Deep Learning, Security,\nTransferability, Interpretability\nI. INTRODUCTION\nD\nEEP Neural Networks (DNNs) have been increasingly\nincorporated into a wide range of applications due to\ntheir high predictive accuracy in various machine learning\ntasks, including image classiﬁcation [2], and natural language\nprocessing [3]. Despite these extraordinary achievements, it\nis not yet entirely evident how the DNN models make\ncertain decisions due to their complex network architecture.\nAdditionally, the susceptibility to adversarial manipulations is\nanother shortcoming of DNN models. For example, adversarial\nexamples, maliciously-crafted inputs, can lead to unexpected\nmodel behavior during decision-making.\nRecently, researchers and practitioners have used inter-\npretability as an indispensable tool to describe the behavior\nof DNN models in an understandable form to humans. More-\nover, the high performance of the security-sensitive models is\nEldor Abdukhamidov and Tamer ABUHMED are with Department of\nComputer Science and Engineering, Sungkyunkwan University, Suwon, South\nKorea.(E-mail: abdukhamidov@skku.edu, E-mail: tamer@skku.edu). Simon\nS. Woo is with Department of Artiﬁcial Intelligence and Department of\nApplied Data Science, Sungkyunkwan University, Suwon, South Korea.(E-\nmail: swoo@g.skku.edu). Mohammed Abuhamad and Eric Chan-Tin are with\nDepartment of Computer Science, Loyola University, Chicago, USA.(E-mail:\nmabuhamad@luc.edu, E-mail: chantin@cs.luc.edu).\n• An earlier version of this work has appeared in CSoNET 2021 [1]\nInput \nInterpretation \nInput \nInterpretation \n(a) Benign\n(b) Adversarial \n(c) Dual Adv.\nFig. 1: Examples of (a) benign, (b) standard adversarial and\n(c) dual adversarial images together with their interpretations\nbased on ResNet-50 classiﬁer and CAM interpreter.\nstrongly related to interpretability. Several studies have been\nconducted to improve the security of DNNs by providing\ninterpretability at different levels (e.g., overall model [4], [5]\nand instance levels [6]–[8]). For instance, Figure 1 (a) displays\nthe causal relationship of an attribution map highlighting\nimportant regions of an image based on the prediction output.\nGenerally, the use of interpretations assists in understanding\nthe internal structure of models to debug them, perform\nsecurity analysis, and identify adversarial examples. Figure 1\n(b) shows successful adversarial examples producing different\ninterpretation maps from the benign interpretation maps.\nIn the interpretable deep learning system workﬂow, a clas-\nsiﬁer (DNN model) and its coupled interpreter (interpretation\nmodel) construct interpretable deep learning systems (IDLS).\nIDLSes offer additional reliability and security in human-\nassisted decision-making processes, since experts can deter-\nmine if an interpretation map ﬁts the prediction of the DNN\nmodels. Nevertheless, interpretability is also vulnerable to\nadversarial modiﬁcation. It is both possible and practical to\ncreate an adversarial input to fool the target DNN model and\nits coupled interpretation model simultaneously. Figure 1 (c)\nshows dual adversarial examples resulting from targeting the\nDNN model and the coupled interpreter. The dual adversarial\nexamples generate interpretation maps indistinguishable from\ntheir benign interpretation maps.\narXiv:2211.15926v1  [cs.CR]  29 Nov 2022\nIn this work, we proposed optimized adversarial attacks\nthat target IDLSes, i.e., can fool the prediction model and\nthe coupled interpreter simultaneously. We call these attacks\nAdvEdge and AdvEdge+. In particular, the attacks exploit\nthe image’s edge information to enable perturbation to be\nattached to the edges of the image areas spotlighted by the\ninterpretation model’s attribution map. This allows for a far\nmore stealthy adversarial attack since the crafted adversarial\ninputs are difﬁcult to identify, even with the interpretation and\nhuman involvement. Furthermore, the proposed attacks gener-\nate effective adversarial examples with minimal perturbation.\nOur Contribution. In this work, we show that adversarial\nsamples can be used to fool the existing IDLSes. We present\ntwo attacks, i.e.,AdvEdge AdvEdge+, that mislead the target\nDNN and deceive the coupled interpreter using adversarial\ninputs that have a high attack success rate with less amount\nof noise. We also employ several attack frameworks, i.e., PGD,\nC&W, and StAdv, for our attacks and evaluate the performance\nagainst four interpreters, i.e., Grad, CAM, RTS, and MASK.\nWe evaluate the attacks on the ImageNet dataset as well as\ncompare them with the existing attack ADV2 [9]. To present\nthe validity of the attacks, we include additional evaluation\nmetrics in terms of classiﬁcation, interpretation, and noise\ngeneration. In addition, we show that our attacks provide\nsigniﬁcantly better transferability over ADV2 by testing the\nattack transferability across different interpreters. Finally, we\npropose two possible countermeasures against our attacks.\nOur contributions are summarized as follows:\n• We propose two novel attacks against IDLSes. Our\nproposed attacks,AdvEdge and AdvEdge+, utilize edge\ninformation to enhance interpretation-derived attacks. We\nevaluate the effectiveness of attacks when various DNN\nmodels (e.g., ResNet and DenseNet) or interpretation\nmodels (e.g., Grad, CAM, RTS, and MASK) are involved.\n• We evaluated our attacks on three frameworks (i.e., PGD,\nC&W, and StAdv) to show that the attacks can be ﬂexibly\nconstructed using various frameworks.\n• Experimental\nﬁndings\nindicate\nthat\nAdvEdge\nand\nAdvEdge+ are equally effective as ADV2 in terms of\nmisclassiﬁcation success rate, and our attacks outperform\nADV2 in creating adversarial samples with interpretation\nmaps that are similar to the benign interpretation maps.\nFurthermore, this level of effectiveness is maintained with\nvery little noise.\n• We explore the transferability of the attacks across dif-\nferent interpreters. Our results show that perturbation\ngenerated using one interpreter can be transferred to\nanother and successfully achieve the adversarial objective\nin most cases.\n• We propose two countermeasures based on DNNs and\ninterpreters. We present multiple-interpreter-based detec-\ntor to identify adversarial samples. We show that the de-\ntector improves the adversarial detection process against\nthe proposed attacks. We also present interpretation-\nbased robust training as a defense technique to train\na DNN classiﬁer against the worst-case interpretation\ndiscrepancy, and indicate that the technique improves the\nrobustness of both a DNN model and its interpreter.\nOrganization. The rest of the paper is organized as fol-\nlows: §II highlights the relevant literature; §III presents the\nfundamental concepts; §IV and §V provide description of\nthe proposed attacks and the implementation details when\nused against four interpreters; §VI shows the experiments and\nevaluation of attacks; §VII presents speciﬁc cases in which the\nexisting and our attacks cannot perform well; §VIII discusses\npossible countermeasures; and §IX offers our conclusion.\nII. RELATED WORK\nIn this section, we present the related works in three cate-\ngories: adversarial attacks, transferability, and interpretability.\nAdversarial Attacks. Machine learning (ML) models have\noften become targets for malicious attacks, as they are being\nincreasingly utilized in security-related domains. In general,\nadversarial attacks are divided into two primary groups in\nterms of malicious data manipulations: 1) poisoning the train-\ning dataset to degrade the targeted models (i.e., poisoning\nattack [10]) and 2) altering the sample input to cause the target\nmodel to misbehave (i.e., evasion attack [11]). Attacking DNN\nmodels has proven to be more complicated because of their\ncomplex network architecture as compared with traditional\nML models [12], [13]. There are several pieces of research\nthat focus on attacking DNNs based on new evasion attacks\n[14], [15]. This work particularly investigates attacks against\nIDLSes that utilize interpretability as defense mechanisms.\nTransferability. Transferability is an interesting property of\nadversarial attacks [15]: e.g., adversarial inputs generated for\na speciﬁc DNN can be effective against other DNNs. This\nproperty is used in black-box scenarios [16]: e.g., generating\nadversarial inputs based on known white-box DNN and ap-\nplying them against the target black-box DNN [17], [18]. In\na research study [19], a technique that perturbs the hidden\nlayers of a model to improve the transferability of adversarial\ninputs, is proposed. Another work [20] applies differentiable\ntransformations to input samples for enhanced transferability.\nIn this work, we investigate the transferability of adversarial\ninputs across different interpreters.\nInterpretability. Interpretable models have been utilized to\nprovide the interpretability for DNN models. Such inter-\npretability can be derived using various methods: back-\npropagation [8], [21], intermediate representations [22], [23],\ninput perturbation [7], and meta models [6].\nInterpretability can contribute to system security in the\ndecision-making process involving an expert who can in-\nspect/verify the attribution maps contributing to certain out-\nputs. The interpretation of adversarial inputs is expected to\ndiffer remarkably from the interpretation of their benign in-\nputs. There have been many works on interpretability to debug\nDNNs [24], identifying the adversarial inputs [25], [26], etc.\nAnother recent study [9] shows the feasibility of manip-\nulating IDLSes. It offers a novel attacking method to fool\nDNN models and their corresponding interpretation models\nsimultaneously, demonstrating that the improved interpretabil-\nity guarantees only limited security and protection. This work\nintroduces two optimized versions of the ADV2 attack [9]\nagainst IDLSes to fool the DNN models and mislead the\ncoupled interpretation models.\n2\nIII. FUNDAMENTAL CONCEPTS\nThis section introduces and deﬁnes common concepts and\nterms used throughout the study. We note that even though\nthis work focuses on IDLSes designed for classiﬁcation tasks,\nsuch as image classiﬁcation, the attacks can be generalized to\nother modeling tasks.\nLet a classiﬁer f(x) = y ∈Y (i.e., DNN classiﬁer f) that\nassigns a sample input (x) to a category (y) from a collection\nof predetermined categories (Y ). Let an interpreter g(x; f) =\nm (i.e., interpretation model g) that produces an interpretation\nmap (m) that highlights the feature importance in the sample\ninput (x) based on the prediction of the DNN model (f). The\nvalue of the i-th component in m (i.e., m[i]) represents the\nimportance of the i-th component in the sample x (i.e., x[i])).\nFrom this perspective, we consider that model interpretation\ncan be obtained by two basic methods: 1 Post-hoc interpre-\ntation: This can be accomplished by adjusting the complexity\nof DNNs or by employing post-training techniques. This strat-\negy necessitates the development of a new model to provide\ninterpretations for the present model [4]–[7], [27]. 2 Intrinsic\ninterpretation: This type of interpretability can be produced\nby developing self-interpretable DNNs, which intrinsically\nembed the interpretability feature into their structures [4], [5].\nIn this study, we focus on the ﬁrst method of interpretability,\nin which an interpreter (g) obtains an attribution map m about\nhow a DNN classiﬁer f identiﬁes the sample input x.\nIn a typical adversarial setting, DNN models are vulnerable\nto adversarial examples, which are maliciously crafted samples\naimed to fool the model [15], [28]. More speciﬁcally, an\nadversarial input (ˆx) can be generated by manipulating the\nbenign input (x) using one of the well-known attacks (PGD\n[14], BIM [29], C&W [30], StAdv [31], etc.) to make the\nmodel misclssify ˆx to an output f(ˆx) = yt ̸= f(x). The ma-\nnipulations, e.g., also known as adversarial perturbations, are\nusually bounded by a norm ball Bε(x) = {∥ˆx −x∥∞≤ε},\nwhere ε is the threshold, to ensure its success and evasiveness.\nIn this work, we adopt the following three well-known\nattack frameworks in our paper:\nPGD. A ﬁrst-order adversarial attack applies a sequence of\nproject gradient descent on the loss function:\nˆx(i+1) =\nY\nBε(x)\n\u0010\nˆx(i) −α. sign(∇ˆxℓprd(f(ˆx(i)), yt))\n\u0011\n(1)\nHere, Q is a projection operator, Bε is a norm ball restrained\nby a pre-ﬁxed ε, α is a learning rate, x is the benign sample,\nˆx(i) is the ˆx at the iteration i, and ℓprd is a loss function that\ncalculates the difference between f(ˆx) and yt.\nC&W. C&W attack framework considers the process of gener-\nating adversarial inputs as a optimization problem of Lp-norm\nregarding the distance metric δ with respect to the given input\nx, which can be described as:\n||δ||p =\n n\nX\ni=1\n|δi|p\n!1/p\n; δi = ˆxi −xi\n(2)\nminimize ||δ||p + c · ℓ(x + δ)\ns.t.\nx + δ ∈[0, 1]n,\n(3)\nwhere δ is the perturbation added to the input x, ℓis a loss\nfunction that is to solve optimization problem using gradient\ndescent, and c is a constant number. We utilize L2-norm based\nC&W in this work.\nStAdv. The attack is based on the spatial transformation\nof pixels to improve the perceptual quality of adversarial\nperturbations. The per-pixel ﬂow is expressed as the ﬂow\nvector: ri := (∆u(i), ∆v(i)), speciﬁcally, ∆u(i) = u(i)\nx −u(i)\nˆx\nand ∆v(i) = v(i)\nx\n−v(i)\nˆx , where (u(i)\nx , v(i)\nx ) and (u(i)\nˆx , v(i)\nˆx )\nrepresent the 2D coordinates of the i-th pixel of the benign\nand the adversarial inputs: x and ˆx. Obtaining the pixel values\nof the adversarial input is described as:\nˆx(i) =\nX\nq∈N(u(i)\nx ,v(i)\nx )\nx(q)(1 −|u(i)\nx −u(q)\nx |)(1 −|v(i)\nx −v(q)\nx |),\n(4)\nwhere N(u(i)\nx , v(i)\nx ) represents the indices of the four pixel\nneighbors (top/bottom-left/right) of (u(i)\nx , v(i)\nx ). Following two\nloss functions are used to formulate the search of adversarial\ndeformations as an optimization problem and they are de-\nscribed as follows:\nℓprd(x, r) := max\n\u001a\nmax\ni̸=t (fi(x + r)) −ft(x + r), κ\n\u001b\n,\nℓflow(r) :=\nall pixels\nX\np\nX\nq∈N(p)\nq\n||∆u(p) −∆u(q)||2\n2 + ||∆v(p) −∆v(q)||2\n2\nThe optimization problem is balanced with a factor of λ\nbetween the two losses to encourage the adversarial input to be\nclassiﬁed as the target label while preserving a high perceptual\nquality, as follows:\nmin\nr\n{ℓprd(x, r) + λℓflow(r)}\n(5)\nIV. ADVEDGE ATTACK\nThe section provides the details of implementing the pro-\nposed attacks against four different types of interpreters.\nA. Attack Deﬁnition\nThe attack’s primary goal is to fool the DNN models f and\ntheir coupled interpretation models g in IDLSes. To achieve\nthis goal, an adversarial sample ˆx is created by adding noise to\nthe benign version x in order to meet the following conditions:\n1) A DNN model f misclassiﬁes the adversarial sample ˆx\nto yt: f(ˆx) = yt such that f(ˆx) = yt ̸= f(x).\n2) The adversarial sample\nˆx causes the interpretation\nmodel g to generate the target interpretation map mt:\ng(ˆx; f) ≈g(x; f).\n3) The adversarial and benign samples are indistinguishable.\nThe proposed approach aims to discover a minimal pertur-\nbation, so the adversarial examples satisfy these conditions.\nThe optimization framework to characterize the attack is:\nmin\nˆx\n: ∆(ˆx, x) s.t.\n\u001a f(ˆx) = yt\ng(ˆx; f) = mt\n(6)\nEq. 6 can be reformulated to be more appropriate for\noptimization as follows:\n3\nmin\nˆx\n: ℓprd(f(ˆx), yt) + λ. ℓint(g(ˆx; f), mt) s.t. ∆(ˆx, x) ≤ε,\n(7)\nwhere ℓprd and ℓint denote the classiﬁcation loss as in Eq.\n1 and the interpretation loss, respectively. ℓint quantiﬁes the\ndissimilarity between the adversarial and benign attribution\nmaps, i.e., g(ˆx; f) and mt = g(x; f), respectively. The hyper-\nparameter λ is used to balance the two components (i.e., ℓprd\nand ℓint). We build Eq. 7 based on the PGD framework, and\nit can be applied to the other attack frameworks with few\nmodiﬁcations (i.e., considering Eq. 3 and Eq. 5) as follows:\nmin\nˆx\n: ||δ||p + c ℓprd(f(ˆx), yt) + λ. ℓint(g(ˆx; f), mt)\n(8)\nmin\nˆx\n: ℓprd(f(ˆx), yt) + λ. ℓint(g(ˆx; f), mt) + λℓflow(r)\n(9)\nThe terms are deﬁned as:\nℓprd(f(ˆx), yt) = −log(fyt(ˆx)),\n∆(ˆx, x) = ∥ˆx −x ∥∞, and\nℓint(g(ˆx; f), mt) = ∥g(ˆx; f) −mt∥2\n2.\nIn PGD, the perturbation in Eq. 1 is controlled as:\nˆx(i+1) =\nY\nBε(x)\n\u0010\nˆx(i) −Nw α. sign(∇ˆxℓadv(ˆx(i)))\n\u0011\n(10)\nFor C&W and StAdv attack frameworks, we control the\nperturbation in Eq. 2 and Eq. 5 as follows:\nδ = Nw (ˆx −x),\n(11)\nr := Nw (∆u, ∆v),\n(12)\nwhere, Nw is the perturbation method that determines both\nthe amount and position of noise injected to the sample\nconsidering the edge weights w of the benign sample. The\nentire loss equation (i.e., Eq. 7) is represented as ℓadv.\nAdvEdge. In Eq. 10, we use the Nw term to optimize the\nposition and amount of applied perturbation. In AdvEdge, we\nlimit the added perturbation to the edges of the sample that\noverlap with the interpretation map provided by the interpreter.\nIn other words, we learn the critical portions of a given\ninput sample based on the overall loss (i.e., DNN loss and\ninterpretation model loss) and then inject noise on the edges\nof those areas. We deﬁne the edges based on the edge weights\nacquired by applying the Sobel ﬁlter [32]) to the sample image.\nA pixel-wise edge weights matrix is deﬁned as E : e →\nRh×w for an image of height h and width w using a typical\nedge extraction method (e.g., Sobel ﬁlter in our experiments).\nThese weights are applied to the sign of the gradient update\n(PGD framework) as follows: E(x)⊗α. sign(∇ˆxℓadv(ˆx(i))),\nwhere the operator ⊗represents the Hadamard product. By\ndoing so, the attack ampliﬁes the noise on the edges while\nreducing or constricting its presence otherwise in the image.\nConsidering a straightforward application of the edge\nweight matrix, i.e., Nw = E(x), we can rewrite Eq. 10, Eq.\n11, and Eq. 12 (respectively) as:\nˆx(i+1) =\nY\nBε(x)\n\u0010\nˆx(i) −E(x) α. sign(∇ˆxℓadv(ˆx(i)))\n\u0011\n(13)\nδ = E(x) (ˆx −x)\n(14)\nr := E(x) (∆u, ∆v)\n(15)\nAdvEdge+. This technique, like AdvEdge, leverages the edge\nweights to optimize the perturbation. Instead of adjusting the\nnoise based on the edge weights in the attack, we apply\nthe perturbations only on deﬁned edges that pass a certain\nthreshold. This is performed by a binarization operation (e.g.,\nbin(Eδ(x) →ei := {0 if ei ≤δ; 1 otherwise ∀i})) to obtain\na binary edge matrix Eδ : e →[0, 1]h×w, where h and w\nare the height and width of an input respectively. The hyper-\nparameter δ sets the threshold for binarizing edge weights,\nand can be adjusted based on the attack objective. The rest\nof the implementation is similar to AdvEdge, e.g., the PGD\nattack can be formulated as: Eδ(x) ⊗α. sign(∇ˆxℓadv(ˆx(i))).\nThis enables perturbations to be applied only on the edges.\nGiven the constraint on perturbation locality and amount, the\nthreshold δ is adjusted to 0.1 to improve the effectiveness.\nThe implementation of attacks in Equations (10, 11, 12,\n13, 14, and 15) against the representatives of four categories\nof interpreters is discussed in the following subsections:\n1) back-propagation-guided interpretation, 2) representation-\nguided interpretation, 3) model-guided interpretation, and 4)\nperturbation-guided interpretation.\nB. Back-Propagation-Guided Interpretation\nThis category of interpretation models computes the gradi-\nent of a DNN model’s prediction with respect to the provided\nsample input. This highlights the signiﬁcance of each feature\nin the input sample. Larger values suggest higher importance\nof the features to the model prediction. In this paper, we\ndiscuss gradient saliency (Grad) [27] as an example of back-\npropagation-guided interpretation. The Grad interpreter gen-\nerates an attribution map m given the model f prediction to\na given input x with a given class y. This attribution map is\nestimated as follows:\nm =\n\f\f\f\f\n∂fy(x)\n∂x\n\f\f\f\f\nLooking for the optimal adversarial sample ˆx for IDLSes\nbased on Grad via a sequence of gradient descent updates (as\nin applying Eq. 10) is ineffective, since DNN models with\nReLU activation functions cause the Hessian matrix compu-\ntation result to be all-zero. The challenge can be resolved by\ncomputing the smoothed value of the gradient of ReLU (r(z))\nusing a function h(z) with the following formula:\nh(z) ≜\n(\n(z +\n√\nz2 + τ)′ = 1 +\nz\n√\nz2+τ\nfor\nz < 0\n(\n√\nz2 + τ)′ =\nz\n√\nz2+τ\nfor\nz ≥0\n4\nHere, τ is a constant parameter, and h(z) approximates\nthe values of r(z) and its gradients are non-zero. Another\nalternative is to use the sigmoid function (σ(z) =\n1\n1+e−z ). We\nnote that this method can be applied to other back-propagation-\nguided interpreters ( e.g., LRP [33], SmoothGrad [34]) as they\nare based on gradient-centric formulations [35].\nC. Representation-Guided Interpretation\nThis type of interpreter extracts feature maps from inter-\nmediate layers of DNNs to provide attribution maps. The\nrepresentative of representation-guided interpretation in this\nstudy is Class Activation Map (CAM) [23]. CAM performs\nglobal average pooling on the convolutional feature maps, and\nutilizes the outputs as features for a fully-connected layer for\nthe model prediction. The signiﬁcance of the sample input\nareas can be determined by projecting the output layer weights\nback onto the convolutional feature maps.\nSpeciﬁcally, let ai(k, l) be the activation of channel i in\nthe last convolutional layer at (k, l) spatial location, and\nP\nk,l ai(k, l) as the result of global average pooling. The\nactivation map mc for an input x and class c is given by:\nmc(x, y) =\nX\ni\nwi,cai(k, l),\nwhere c is c-th output of the linear layer and wi,c is the weight\ncorresponding to class c for i-th channel. We construct m by\nextracting and concatenating interpretation maps from f up\nto the ﬁnal convolutional layer and a fully connected layer,\nsimilar to the work of [9]. We exploit the interpretation model\nby utilizing gradient descent updates to ﬁnd the optimal ˆx as in\nEq. 10. The attack can also be used against various interpreters\nof this category (e.g., Grad-CAM [22]).\nD. Model-Guided Interpretation\nBy masking signiﬁcant areas in any input, a model-guided\ninterpretation model is trained to predict the attribution map\nin a single forward pass directly. We consider Real-Time\nImage Saliency (RTS) [6] for this category. RTS generates\nthe attribution map m for any given class c and input x by\nresolving the following problem:\nmin\nm :\nλ1rtv(m) + λ2rav(m)\n−log(fc(Φ(x; m)))\n+ λ3fc(Φ(x; 1 −m))λ4 s.t. 0 ≤m ≤1\n(16)\nHere, the regularizers {λi}4\ni=1 balance the involved factors,\nthe rav(m) represents the average value of m. The rtv(m)\nexpresses the variation of m that enforces mask smoothness,\nand it is deﬁned simply as: rtv(m) = P\ni,j(mi,j −mi,j+1)2 +\nP\ni,j(mi,j−mi+1,j)2. Furthermore, Φ is the operator that uses\nm as a mask to blend x with random color noise and Gaussian\nblur, and it is deﬁned as: Φ(x, m) = x⊗m+r⊗(1−m), where\n⊗denotes Hadamard product and r is an alternative image.\nFor applying Φ, r can be chosen to be a highly blurred version\nof in the input x. In general, Eq. 16 ﬁnds the important parts\nof x in terms of which f predicts f(x) with high conﬁdence.\nDuring the inference, computing Eq. 16 is expensive. There-\nfore, RTS trains a DNN to predict the attribution map for any\ninput so that RTS does not have to access the DNN f after\ntraining. This can be achieved by composing a ResNet [2], a\npre-trained model as an encoder to extract feature maps of the\ngiven inputs and a U-Net [36] as the masking model which is\ntrained to optimize the framework in Eq. 16. For this work, we\nconsider the composition of the encoder and masking model\nas the interpreter g.\nAttacking RTS by employing Eq. 10, Eq. 11 or Eq. 12\ndirectly, has been found to be inefﬁcient for ﬁnding the optimal\nadversarial samples [9]. The main reason for this is that the\ninterpretation model uses both the masking model and the\nencoder (enc(·)). To address this issue, we utilize an additional\nloss term ℓenc(enc(ˆx), enc(yt)) to Eq. 7, Eq. 8 and Eq. 9 to\ncompute the difference between the encoder’s outcome with\nthe adversarial sample input ˆx and the target class yt. Then,\nwe perform a series of gradient descent updates to choose the\nbest the optimal adversarial sample input ˆx.\nE. Perturbation-Guided Interpretation\nThis type of interpretation model investigates the interpre-\ntation maps by introducing a minimal amount of noise to the\ninput and evaluating the changes in the model’s prediction. We\nconsider MASK [7] to be the class representative.\nMASK identiﬁes the attribution map for the given input x by\naffecting the maximally informative input regions. Speciﬁcally,\nthe model f(x) generates a vector of scores for distinct\nhypotheses about the input x (e.g., as a softmax probability\nlayer in a DNN), then MASK explores the smallest mask\nm to make the model performance drop signiﬁcantly: i.e.,\nfy(Φ(x; m)) ⩽fy(x). We note that the mask m in this\nscenario is binary, where m[i] = 0 or 1 to indicate whether\nthe i-th feature is replaced with Gaussian noise. The optimal\nmask can be obtained by solving the following problem:\nmin\nm : fy(Φ(x; m)) + λ. ∥1 −m∥1 s.t. 0 ≤m ≤1,\n(17)\nwhere λ encourages most of the mask to be sparse. By solving\nthe Eq. 17, we ﬁnd the most informative and necessary regions\nof the input x with reference to the model prediction f(x).\nWe cannot directly advance the Eq. 7, Eq. 8 and Eq. 9\nwith the Eq. 10, Eq. 11 and Eq. 12, respectively since the\ninterpretation model g is built as an optimization method.\nA bi-level optimization framework [9] can be used to\nsolve this problem. The loss function is rewritten as follows:\nℓadv(x, m) ≜ℓprd(f(x), yt) + λ. ℓint(m, mt) by including\nbenign attribution map m as a new variable. Let ℓmap(m; x)\ndenote the object function of Eq. 17 and m∗(x) = argminm :\nℓmap(m; x) be the attribution map generated by MASK for\nthe input x. Then, we have the following framework:\nmin\nx\n: ℓadv(x, m∗(x)) s.t. m∗(x) = argmin\nm\nℓmap(m; x)\n(18)\nFor every update on the input x, solving the bi-level\noptimization in Eq. 18 is expensive. As a solution for the\nissue, an approximate iterative procedure was proposed by\n[9] to optimize x and the mask m by alternating between\nℓadv and ℓmap. Brieﬂy, given x(i−1) at the i-th iteration,\n5\nCAM\nGrad\nMASK\nRTS\nImage\nBenign \n2\nADV\nAdvEdge\nAdvEdge+\nFig. 2: Examples of benign and adversarial attribution maps\ngenerated by Grad, CAM, MASK, and RTS interpreters on\nthe ResNet model. Adversarial attribution maps are based on\nADV2, AdvEdge, and AdvEdge+ attacks.\nthe attribution map m(i) is computed by updating m(i−1) on\nℓmap(m(i−1); x(i−1)), then m(i) is ﬁxed and x(i) is attained\nby reducing ℓadv after one gradient descent step with respect\nto m(i). To update x(i), the objective function is formulated\nas follows:\nℓadv\n\u0010\nx(i−1), m(i) −α. ∇m. ℓmap(m(i); x(i−1))\n\u0011\n,\n(19)\nwhere, α is the learning rate. Implementation details are\nprovided in the following section.\nV. EXPERIMENTAL SETTING\nThis\nsection\ndescribes\nthe\nimplementation\ndetails\nof\nAdvEdge and AdvEdge+, and the methods used to maximize\ntheir effectiveness on target interpreters (Grad, CAM, MASK,\nand RTS). Both AdvEdge and AdvEdge+ are built based on\nthe PGD, C&W and StAdv attack frameworks. We use the\nvalues α = 1/255, and ε = 0.031 from the prior work [9].\nℓ∞is used to calculate the perturbation proportion. To improve\nthe attack’s efﬁciency, we use a method that adds noise to\nthe edges of the images with a ﬁxed number of iterations\n(#iterations = 300).\nOptimization Steps. zero gradient of the prediction loss\nhinders ﬁnding the desired result with correct interpretation\n(Grad). To address this issue, a label smoothing strategy\nusing cross-entropy is used. The approach samples the pre-\ndiction loss using a uniform distribution U(1 −ρ, 1) and\nthe value of ρ is substantially decreased during the attacking\nprocess. Considering yc =\n1−yt\n|Y |−1, we derive ℓprd(f(x), yt) =\n−P\nc∈Y yc log fc(x).\nIn solving the Eq. 19, multiple steps of gradient descent to\nupdate m and to compute the m∗(x) for faster convergence are\napplied. Additionally, the average gradient is utilized to update\nm and to support the stable optimization. More precisely, at\nthe i-th iteration with multistep gradient descent, {m(i)\nj } is\nthe sequence of maps. To calculate the gradient in order to\nupdate m, the interpretation loss P\nj ∥m(i)\nj\n−mt∥2\n2 is used.\nTo improve the convergence, the learning rate is also changed\ndynamically. For stable learning, x is updated in two steps: 1\nupdating x based on the prediction loss ℓprd; 2 updating x\nbased on the interpretation loss ℓint.\nIn the second step, x’s conﬁdence score is checked whether\nit is still above a speciﬁc threshold after the perturbation by\nsearching the largest step size with applying the binary search.\nUpdating the estimation of the attribution map in terms of\nℓmap results in the signiﬁcant deviation from the original map\ngenerated by MASK as the number of steps increases in the\nupdate process. To keep the effectiveness of the attack, the\ngenerated map is replaced with the map g(x; f) which is the\noutput of the MASK in terms of the latest adversarial input at\na speciﬁc iteration point (e.g., 50 iterations). Simultaneously,\nAdam step parameter is reset for the correct state.\nDataset. For our experiment, we use ImageNetV2 Top-Images\n[37] dataset. ImageNetV2 is a new test set collected based\non the ImageNet benchmark and was mainly published for\ninference accuracy evaluation. The dataset contains 10,000\nimages based on 1,000 different classes that are similar to\nthe classes in the original ImageNet dataset. All images are\ncropped to 224 × 224 pixels, and the pixels are normalized\nbetween [0, 1]. As the test set, we use 1,000 images that are\nselected randomly and classiﬁed correctly by the classiﬁer f.\nPrediction Models. The study employs two state-of-the-art\nDNN models ResNet-50 [2] and DenseNet-169 [38], which\ndemonstrate 22.85% and 22.08% top-1 error rates on the\nImageNet dataset, respectively. Those DNN models differ\nin capacity (50 and 169 layers, respectively) and network\narchitecture (residual blocks and dense blocks, respectively).\nUsing those various DNN models assist in computing the\nefﬁcacy of the proposed attacks.\nInterpretation Models. We explored the attacks against the\nGrad [27], CAM [23], RTS [6], and MASK [7] interpreters\nas representative of back-propagation-guided, representation-\nguided, model-guided, and perturbation-guided interpretation\nmodels, respectively. In our experiment, we employed the\ninterpreters’ original open-source implementation.\nAttack Framework. We implement our attack on the basis\nof PGD, C&W, and StAdv attack frameworks. We note that\nattack frameworks other than the used in this work (e.g., BIM\n[29]) can also be implemented for the attack. We also assume\nthat our proposed attacks are based on non-targeted attacks, in\nwhich DNN models are forced to misclassify the adversarial\ninput ˆx to a certain and randomly-selected class. We make\ncomparisons between the proposed approaches (AdvEdge and\nAdvEdge+) and ADV2 attack, which is assumed a novel attack\nto produce adversarial samples for target DNNs and their\nlinked interpreters. We have the same hyperparameters (learn-\ning rate, number of iterations, step size, etc.) and experimental\nsettings as in ADV2 [9].\nVI. ATTACK EVALUATION\nFor the evaluation, we answer the following questions: 1\nAre the AdvEdge and AdvEdge+ attacks effective against\nDNN models? 2 Are the AdvEdge and AdvEdge+ attacks\nefﬁcient at deceiving interpreters? 3 Does transferability exist\nin the attacks against the interpreters using the proposed\n6\nTABLE I: Attack success rate of ADV2, AdvEdge, and AdvEdge+ against different classiﬁers (ResNet-50 and DenseNet-169)\nand interpreters (Grad, CAM, RTS, and MASK) testing on 1,000 images and using PGD, C&W, and stAdv frameworks.\nGrad\nCAM\nMASK\nRTS\nADV2\nAdvEdge\nAdvEdge+\nADV2\nAdvEdge\nAdvEdge+\nADV2\nAdvEdge\nAdvEdge+\nADV2\nAdvEdge\nAdvEdge+\nPGD\nResnet\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nDensenet\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nC&W\nResnet\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.99\n0.99\n0.99\nDensenet\n1.00\n0.99\n0.99\n0.94\n0.94\n0.94\n1.00\n1.00\n1.00\n0.99\n0.99\n0.99\nStAdv\nResnet\n0.99\n0.99\n0.99\n1.00\n1.00\n1.00\n0.99\n0.96\n0.95\n0.99\n0.99\n0.99\nDensenet\n0.97\n0.97\n0.96\n0.94\n0.94\n0.94\n0.99\n0.98\n0.97\n0.99\n0.99\n0.99\nTABLE II: Misclassiﬁcation conﬁdence of ADV2, AdvEdge, and AdvEdge+ against different classiﬁers (ResNet-50 and\nDenseNet-169) and interpreters (Grad, CAM, RTS, and MASK) testing on 1,000 images and using PGD, C&W, and stAdv.\nGrad\nCAM\nMASK\nRTS\nADV2\nAdvEdge\nAdvEdge+\nADV2\nAdvEdge\nAdvEdge+\nADV2\nAdvEdge\nAdvEdge+\nADV2\nAdvEdge\nAdvEdge+\nPGD\nResnet\n0.927\n0.936\n0.923\n0.553\n0.554\n0.560\n0.535\n0.597\n0.534\n0.715\n0.705\n0.717\nDensenet\n0.903\n0.921\n0.910\n0.501\n0.491\n0.512\n0.585\n0.631\n0.628\n0.566\n0.584\n0.574\nC&W\nResnet\n0.925\n0.930\n0.931\n0.833\n0.835\n0.840\n0.401\n0.410\n0.397\n0.614\n0.665\n0.644\nDensenet\n0.930\n0.929\n0.930\n0.789\n0.790\n0.792\n0.439\n0.482\n0.450\n0.593\n0.596\n0.593\nStAdv\nResnet\n0.885\n0.877\n0.870\n0.947\n0.950\n0.949\n0.401\n0.405\n0.397\n0.839\n0.843\n0.844\nDensenet\n0.866\n0.853\n0.849\n0.870\n0.883\n0.873\n0.439\n0.482\n0.446\n0.769\n0.771\n0.773\nattacks? 4 Do the AdvEdge and AdvEdge+ attacks make it\neasier to attack IDLSes? To answer these questions, we per-\nform several experiments evaluating AdvEdge and AdvEdge+,\nand comparing the ﬁndings to ADV2 in [9]. We utilize the\noriginal implementation of ADV2 presented by the authors.\nEvaluation Metrics. We used several evaluation metrics to\nassess the attack success against the used DNN classiﬁers and\ninterpretation models. To begin, we use the following metrics\nto evaluate the attacks based on misleading the DNN models:\n• Attack success rate: We calculate the proportion of\nsuccessfully-misclassiﬁed test inputs to the total number of\ntest samples, which is calculated as: #successful trials\n#total trials\n.\n• Misclassiﬁcation conﬁdence: We check conﬁdence in pre-\ndicting the targeted category, which is the probability given\nto the class yt by a DNN model.\nFurthermore, we assess the attacks based on their capability\nin deceiving the interpretation model. This is accomplished by\nassessing the interpretation maps of adversarial samples. This\ntask is challenging since there is a lack of standard metrics to\nevaluate the interpretation maps provided by the interpreters.\nTherefore, we use the following metrics:\n• Qualitative comparison: We use this measurement to check\nwhether the results of our approach are perceptually indis-\ntinguishable. We compare the interpretations maps of benign\nand adversarial inputs qualitatively.\n• Lp Measure: We observe the difference between benign\nand adversarial attribution maps using the L1 distance. All\nvalues of attribution maps are standardized to [0, 1] to obtain\nthe results.\n• IoU Test (Intersection-over-Union): This is another quan-\ntitative metric for determining the similarity of attribu-\ntion maps. This metric is commonly used to compare\nthe prediction outcome with the ground truth: IoU(m) =\n|O(m) T O(m◦)| / |O(m) S O(m◦)|, where m is adver-\nsarial attribution map and m◦is the benign attribution map.\nThis metric is also known as Jaccard index.\nThe following metric is used to quantify the amount of noise\nto produce the adversarial inputs:\n• Structural Similarity (SSIM): The mean structural simi-\nlarity index [39] between the benign and adversarial sample\ninputs is used to calculate added the noise. SSIM is a tech-\nnique to measure the image quality based on its distortion-\nfree reference image:\nSSIM(x, ˆx) =\n(2µxµˆx + c1)(2σxˆx + c2)\n(µ2x + µ2\nˆx + c1)(σ2x + σ2\nˆx + c2),\nwhere µx and µˆx are the averages of x and ˆx, σx and σˆx\nare the variances of x and ˆx, σxˆx is the covariance of x\nand ˆx. c1 and c2 are the variables to stabilize the division\nwith weak denominator. To calculate the non-similarity rate\n(also known as the distance or noise rate), we subtract the\nSSIM value from one (noise rate = 1 −SSIM).\nFinally, we also consider the time for the attacks:\n• Average Time: We calculate the time that is taken to\ngenerate adversarial inputs by the attacks.\nA. Attack Effectiveness against DNNs\nIn terms of fooling the target DNN models, we ﬁrst evaluate\nAdvEdge and AdvEdge+ along with comparing the results\nwith the previous work (ADV2 [9]). We build existing method\nupon C&W and StAdv frameworks for comparison purposes.\nThe results are shown in Tables I and II, and presented using\nattack success rate and misclassiﬁcation conﬁdence.\nTable I summarizes the results of the attack success rate\nof ADV2, AdvEdge, and AdvEdge+ against various DNN\nmodels and interpreters based on 1,000 images. As it is\nshown, the attack success rate for C&W and StAdv is not\nas high as PGD. The reason for the case is that we applied\nthe same hyperparameter values of PGD attack for the other\nattacks as fair comparison(step size, number of iterations,\nlearning rate, etc.). In general, AdvEdge and AdvEdge+ are as\neffective as ADV2. Additionally, Table II presents the results\nof misclassiﬁcation conﬁdence on 1,000 images in terms of\nthree methods against various DNN and interpretation models.\nDespite the fact that the main goal is to apply a minimal\namount of noise to certain parts of the sample images, the\nperformance on three attack frameworks is quite better than\n7\nGrad\nCAM\nMASK\nRTS\nGrad\nCAM\nMASK\nRTS\nGrad\nCAM\nMASK\nRTS\nGrad\nCAM\nMASK\nRTS\nGrad\nCAM\nMASK\nRTS\nGrad\nCAM\nMASK\nRTS\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n2\nADV\nAdvEdge\n+\nAdvEdge\nResNet \nDenseNet\nPGD\nC&W\nStAdv\nFig. 3: L1 distance of attribution maps generated by different interpreters when applying ADV2, AdvEdge, and AdvEdge+\nfrom those of corresponding benign samples on ResNet-50 and DenseNet-169 using the PGD, C&W, and stAdv frameworks.\nGrad\nCAM\nMASK\nRTS\nGrad\nCAM\nMASK\nRTS\nGrad\nCAM\nMASK\nRTS\nPGD\nC&W\nStAdv\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\nResNet \nDenseNet\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n2\nADV\nAdvEdge\n+\nAdvEdge\nGrad\nCAM\nMASK\nRTS\nGrad\nCAM\nMASK\nRTS\nGrad\nCAM\nMASK\nRTS\nFig. 4: IoU scores of attribution maps generated by ADV2, AdvEdge, and AdvEdge+ using the four interpreters on ResNet-50\nand DenseNet-169. Our attacks achieve higher IoU scores in comparison with ADV2.\nADV2 in terms of Grad (ResNet), CAM (DenseNet) and RTS\n(DenseNet) in three attack frameworks. In other circumstances,\nthe conﬁdence levels of the DNN models are comparable. The\nresults show that AdvEdge and AdvEdge+ are effective in\ndeceiving the target DNNs. Restricting perturbations to edges,\nthe proposed methods are as effective as ADV2.\nB. Attack Effectiveness against Interpreters\nWe assess the capability of AdvEdge and AdvEdge+ attacks\nto create interpretation maps that are comparable to the benign\ninterpretation maps. We begin with a qualitative evaluation to\nobserve the similarity of interpretations produced by adver-\nsarial samples (i.e., generated by AdvEdge and AdvEdge+)\nand the corresponding benign samples. By making qualitative\ncomparison of the interpretations of benign and adversarial\nsamples, AdvEdge and AdvEdge+ generated interpretations\nthat are visually indistinguishable from their corresponding be-\nnign sample inputs. When compared to ADV2, both proposed\napproaches produced interpretation maps that are similar to\nthe benign inputs. Figure 2 shows some examples of observed\nattribution maps obtained using Grad, CAM, RTS, and MASK.\nThe examples show the adversarial interpretations and the\ncorresponding benign. As shown in the ﬁgure, the adversarial\nand benign attribution maps are highly similar.\nIn addition to qualitative analysis, we employ Lp to quantify\nthe similarity of generated interpretation maps. Figure 3 sum-\nmarizes the results of L1 measurement. As displayed in the\nﬁgure, when compared to ADV2, AdvEdge and AdvEdge+\nproduce adversarial sample inputs with interpretation maps\nthat are similar to those obtained for the benign samples.\nThe results are consistent among interpretation models with\nall DNNs. We observe that the efﬁciency of our attack (against\n8\n2\nADV\nAdvEdge\n+\nAdvEdge\n0.14\nCAM\nRTS\nMASK\nGrad\n0\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nCAM\nRTS\nMASK\nGrad\n0\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nCAM\nRTS\nMASK\nGrad\n0\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nCAM\nRTS\nMASK\nGrad\n0\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nCAM\nRTS\nMASK\nGrad\n0.14\n0\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nCAM\nRTS\nMASK\nGrad\n0\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nResNet\nDenseNet\nPGD\nC&W\nStAdv\nFig. 5: Noise rate of adversarial inputs generated by ADV2, AdvEdge, and AdvEdge+ on ResNet-50 and DenseNet-169.\nTABLE III: Average time (in seconds) to generate a single adversarial input by ADV2, AdvEdge and AdvEdge+ across different\ninterpreters on ResNet-50 and DenseNet-169. Best results are given in bold.\nGrad\nCAM\nMASK\nRTS\nADV2\nAdvEdge\nAdvEdge+\nADV2\nAdvEdge\nAdvEdge+\nADV2\nAdvEdge\nAdvEdge+\nADV2\nAdvEdge\nAdvEdge+\nPGD\nResnet\n32.68\n32.69\n30.02\n15.37\n16.05\n15.83\n188.97\n188.14\n194.56\n8.75\n8.45\n8.39\nDensenet\n33.02\n33.01\n31.45\n49.42\n49.38\n49.37\n340.49\n343.40\n340.53\n12.43\n12.37\n16.73\nC&W\nResnet\n75.95\n75.68\n75.64\n4.68\n4.68\n4.72\n174.18\n185.32\n139.37\n8.78\n8.96\n8.95\nDensenet\n173.70\n173.09\n173.26\n4.46\n4.46\n4.54\n201.81\n201.78\n202.91\n12.29\n12.23\n12.56\nStAdv\nResnet\n80.16\n79.88\n79.96\n13.13\n13.12\n13.17\n987.88\n731.46\n732.35\n7.48\n7.68\n7.82\nDensenet\n179.03\n179.17\n179.52\n8.14\n8.29\n8.24\n293.12\n309.89\n311.12\n10.23\n10.72\n10.40\ninterpretation models) differs with the interpretation models.\nFurthermore, as reﬂected in the Figure 3, there are several\ncases that the attacks could not achieve high attack success\nrate in terms of interpreters, which are known as outliers. We\nwill analyse them in §VII.\nThe IoU score is another quantitative metric for com-\nparing the similarity of interpretation maps. We binarized\nthe values of interpretation maps to compute the IoU as\nthey are originally real values. Figure 4 displays the IoU\nscores of attribution maps generated by ADV2, AdvEdge, and\nAdvEdge+ adopting PGD, C&W, and StAdv with regard to\nfour interpreter models on ResNet and DenseNet. As shown\nin the ﬁgure, AdvEdge and AdvEdge+ performed better than\nADV2. We observe that AdvEdge and AdvEdge+ achieved\nsigniﬁcantly better results in terms of the interpreters Grad,\nCAM, and MASK. AdvEdge and AdvEdge+ are effective in\ndeceiving different interpreters. Based upon the qualitative and\nquantitative measures, the attribution maps of adversarial and\nbenign samples are almost indistinguishable.\nC. Adversarial Perturbation Rate\nThe SSIM metric helps assess the pixel-level perturbation\napplied by AdvEdge and AdvEdge+ to generate the adversar-\nial samples. We measure the noise amount from images’ areas\nthat are not identical to the original images using SSIM. Figure\n5 presents the perturbation amount generated by different at-\ntacks (i.e., ADV2, AdvEdge, and AdvEdge+) to succeed under\ndifferent frameworks. As shown, AdvEdge and AdvEdge+\nrequired signiﬁcantly lower noise than ADV2 in terms of\nPGD and C&W. The results become more obvious when using\nthe MASK interpreter (PGD attack). AdvEdge+ generates the\nleast amount of noise to fool the target DNN model and the\nMASK interpreter. In terms of StAdv, limiting the perturbation\nfor only edges of speciﬁc regions caused the attack to use\nmore noise to have successful attack as compared with ADV2.\nThis can be more noticeable, when MASK interpreter is used.\nGenerally, AdvEdge and AdvEdge+ achieve high success rate\nwith less perturbation.\nD. Average Time\nTo compare the attacks in terms of time, we calculated\nthe time to generate adversarial inputs. As shown in Table\nIII, CAM and RTS interpreters took less amount of time for\nthe generation process in comparison of Grad and MASK in\ngeneral. Among the four interpreters, MASK is the interpreter\nthat takes considerable time to achieve successful attack rate.\nIn terms of attack frameworks adopted (PGD, StAdv, and\nC&W), C&W based on CAM and MASK is faster while PGD\nbased on Grad generates adversarial in shorter time period. In\naccordance with the attack types, there is not any signiﬁcant\ntime difference among ADV2, AdvEdge and AdvEdge+. In\nseveral cases, our attacks took less time to search in adversarial\nspace (Grad). This can clearly be observed in the time taken by\nAdvEdge and AdvEdge+ using StAdv on MASK interpreter\nand ResNet classiﬁer. To conclude, AdvEdge and AdvEdge+\nprovide comparable time to craft adversarial inputs.\nE. Attack Transferability\nOne interesting property of adversarial inputs is their trans-\nferability. Speciﬁcally, one adversarial input effective against\na DNN can be effective against other DNNs [17], [18], [28].\nIn this part, we evaluate the transferability in our attacks\nagainst interpreters. Speciﬁcally, for a given interpreter g,\na set of adversarial inputs generated against g is selected\nrandomly to compute their attribution maps using the other\ninterpreters g′. Figure 6 illustrates the attribution maps of\nadversarial inputs on g and g′ on the basis of PGD attack. We\n9\nBenign\n2\nADV\nAdvEdge\nBenign\n2\nADV\nAdvEdge\nBenign\n2\nADV\nBenign\n2\nADV\nGrad\nCAM\nMASK\nRTS\nGrad\nCAM\nMASK\nRTS\nSource interpreters\nTarget interpreters\nAdvEdge+\nAdvEdge+\nAdvEdge\nAdvEdge+\nAdvEdge\nAdvEdge+\nFig. 6: Visualization of transferability for attribution maps of adversarial inputs across different interpreters on ResNet-50\nadopting PGD attack framework. Heatmap areas and shapes of the results generated by AdvEdge and AdvEdge+ share high\nsimilarity with the benign cases. Our attacks provide signiﬁcantly better transferability as compared with ADV2.\nGrad\nCAM\nMASK\nRTS\nSource interpreters\nGrad\nCAM\nMASK\nRTS\nTarget interpreters \nBenign\n2\nADV\nAdvEdge\nBenign\n2\nADV\nAdvEdge\nBenign\n2\nADV\nBenign\n2\nADV\nAdvEdge+\nAdvEdge+\nAdvEdge\nAdvEdge+\nAdvEdge\nAdvEdge+\nFig. 7: Limitations in transferability among attribution maps of adversarial inputs across different interpreters on ResNet. Our\nattack results still provide meaningful interpretation in attack transferability.\nprovide the example for PGD attack, as it adds a considerable\namount of noise to the inputs for high attack success, which\nis highly likely for attribution maps to deviate from their\noriginal attribution maps (see Figure 5). Furthermore, we\ncompare the attribution maps of adversarial inputs against their\ncorresponding original attribution maps. As shown in Figure\n6, interpretation transferability is valid for the attacks, and it\nis more obvious in AdvEdge and AdvEdge+ than in ADV2.\nAdditionally, we continue our observation quantitatively.\nTable IV shows the L1 distance between attribution maps\nof adversarial (ADV2, AdvEdge and AdvEdge+) and benign\nsamples across different interpreters. Observe that our meth-\nods generate better-quality interpretation maps on different\ninterpretation models g as compared to the samples produced\nby ADV2. Overall, AdvEdge and AdvEdge+ maintain high\ntransferability across interpreters for most of classes.\nLimitations. We note that some adversarial inputs crafted\non a speciﬁc interpreter g may not be transferable to an-\nother interpreter g′ as different interpreters utilize different\nproperties of DNNs to generate attribution maps. Figure 7\ndisplays some cases where using interpretations from one\ninterpreter to another produces attribution maps that are not\nentirely similar to benign cases. However, the attribution maps\nof adversarial inputs from different interpretation models are\nstill focused on the object in the image. Figure 7 shows\nsome of these cases across different attacks (ADV2, AdvEdge\nand AdvEdge+). In summary, AdvEdge and AdvEdge+ may\nnot generate transferable adversarial input in some cases.\nHowever, the interpretation focus (highlighted area) is still in\nthe object. Thus, the attribution map can be valid if the user\ndoes not have knowledge about the benign interpretation.\nVII. DISCUSSION\nAs displayed in Figure 3, there are several cases that\nthe attacks lack searching proper perturbation to mislead the\ntarget DNN and their coupled interpreters. We provide some\nexamples that are considered as outliers for different attack\nframeworks on both ResNet and DenseNet in Figure 8. It is\nobserved that as the interpretation (heatmap) is scattered across\nthe whole image region (cases in MASK, Grad, and RTS),\nthe attacks ADV2, AdvEdge and AdvEdge+ cannot ﬁnd the\nbest adversarial space to meet the requirements of classiﬁers\nand interpreters. Additionally, samples that are located near\ndecision boundaries are likely to jump into another class space.\nAdding small amount of perturbation causes the focus to be\nchanged totally into another object in the image, attribution\nmaps of which are totally different from the benign ones and it\ncan be seen as the example of CAM interpreter. AdvEdge and\nAdvEdge+ may lack in ﬁnding the proper adversarial space\n10\nTABLE IV: L1 distance of transferability among attribution maps of adversarial (ADV2, AdvEdge, and AdvEdge+) across\ninterpreters on ResNet-50 and DenseNet-169 (row/column as source/target interpreters). Best results are given in bold.\nGrad\nCAM\nMASK\nRTS\nADV2\nAdvEdge\nAdvEdge+\nADV2\nAdvEdge\nAdvEdge+\nADV2\nAdvEdge\nAdvEdge+\nADV2\nAdvEdge\nAdvEdge+\nPGD\nResnet\nGrad\n0.081\n0.077\n0.074\n0.246\n0.241\n0.241\n0.578\n0.563\n0.556\n0.092\n0.077\n0.080\nCAM\n0.093\n0.092\n0.091\n0.063\n0.063\n0.062\n0.523\n0.523\n0.523\n0.097\n0.097\n0.096\nMASK\n0.117\n0.116\n0.103\n0.246\n0.234\n0.262\n0.545\n0.529\n0.482\n0.099\n0.087\n0.069\nRTS\n0.097\n0.096\n0.096\n0.281\n0.273\n0.273\n0.626\n0.613\n0.617\n0.098\n0.097\n0.097\nDensenet\nGrad\n0.085\n0.082\n0.079\n0.219\n0.222\n0.220\n0.606\n0.596\n0.575\n0.086\n0.072\n0.073\nCAM\n0.102\n0.101\n0.102\n0.093\n0.092\n0.092\n0.573\n0.563\n0.564\n0.100\n0.099\n0.099\nMASK\n0.117\n0.118\n0.115\n0.250\n0.246\n0.253\n0.546\n0.523\n0.511\n0.101\n0.086\n0.086\nRTS\n0.104\n0.104\n0.103\n0.262\n0.251\n0.252\n0.567\n0.558\n0.551\n0.110\n0.110\n0.109\nC&W\nResnet\nGrad\n0.042\n0.041\n0.041\n0.244\n0.240\n0.242\n0.546\n0.545\n0.542\n0.069\n0.069\n0.069\nCAM\n0.050\n0.060\n0.049\n0.328\n0.307\n0.329\n0.573\n0.570\n0.574\n0.038\n0.037\n0.038\nMASK\n0.091\n0.089\n0.089\n0.221\n0.220\n0.219\n0.473\n0.474\n0.475\n0.081\n0.081\n0.081\nRTS\n0.092\n0.091\n0.092\n0.308\n0.310\n0.311\n0.639\n0.638\n0.638\n0.086\n0.085\n0.085\nDensenet\nGrad\n0.045\n0.044\n0.044\n0.241\n0.241\n0.237\n0.562\n0.562\n0.561\n0.056\n0.055\n0.054\nCAM\n0.065\n0.070\n0.065\n0.335\n0.333\n0.335\n0.570\n0.572\n0.571\n0.035\n0.040\n0.035\nMASK\n0.092\n0.092\n0.093\n0.220\n0.220\n0.218\n0.479\n0.481\n0.476\n0.080\n0.079\n0.079\nRTS\n0.095\n0.094\n0.094\n0.289\n0.286\n0.291\n0.558\n0.558\n0.559\n0.092\n0.091\n0.091\nStAdv\nResnet\nGrad\n0.036\n0.035\n0.035\n0.259\n0.229\n0.231\n0.550\n0.539\n0.540\n0.032\n0.035\n0.035\nCAM\n0.078\n0.076\n0.077\n0.141\n0.114\n0.115\n0.542\n0.536\n0.537\n0.023\n0.026\n0.026\nMASK\n0.084\n0.086\n0.086\n0.246\n0.243\n0.246\n0.511\n0.501\n0.502\n0.074\n0.075\n0.076\nRTS\n0.079\n0.078\n0.078\n0.255\n0.248\n0.249\n0.550\n0.548\n0.547\n0.022\n0.022\n0.022\nDensenet\nGrad\n0.041\n0.040\n0.040\n0.250\n0.221\n0.222\n0.547\n0.538\n0.539\n0.034\n0.036\n0.036\nCAM\n0.088\n0.085\n0.085\n0.172\n0.151\n0.151\n0.545\n0.541\n0.541\n0.062\n0.064\n0.062\nMASK\n0.085\n0.085\n0.084\n0.229\n0.216\n0.219\n0.493\n0.485\n0.489\n0.067\n0.067\n0.068\nRTS\n0.082\n0.081\n0.081\n0.241\n0.235\n0.235\n0.543\n0.539\n0.540\n0.028\n0.029\n0.029\nAdvEdge+\n2\nADV\nBenign\nAdvEdge\nGrad\nCAM\nMASK\nRTS\nFig. 8: Outliers of adversarial inputs with interpretations\nsimilar to benign. Grad and MASK are based on StAdv and\nPGD, respectively (DenseNet-169). CAM and RTS interpreters\nare based on StAdv and C&W, respectively (Resnet-50).\nin cases when the sample is near the decision boundary or the\ninterpretation is spread over the image region.\nVIII. POTENTIAL COUNTERMEASURES\nIn this section, we discuss the potential countermeasure\nagainst our proposed attack. Based on the results provided in\nthe above sections, the adversarial interpretations of AdvEdge\nand AdvEdge+ share a high similarity with benign attribution\nmaps. It is also mentioned that different interpretations utilize\ndifferent behavioral aspects of DNN models. Using ensem-\nble interpretation techniques can be encouraging to defend\nagainst interpretation-based attacks. However, aggregating an\ninterpretation via the ensemble technique is quite challenging\nas the interpretations of different interpreters are contrasting.\nAdditionally, the proposed attack can be adaptive by optimiz-\ning the interpretation loss with respect to all the interpreters;\nnevertheless, it is computationally expensive to generate an\nadversarial sample considering all the interpreters. Taking\nthose into consideration, we recommend some countermea-\nsures against the attack.\nAdversarial Detector. We suggest a multiple-interpreter-based\ndetector that utilizes multiple interpretations of a single input\nto check whether the given input is adversarial or benign.\nDetectors can be efﬁcacious at detecting adversarial samples\nvia the usage of multiple interpreters to some extent. For our\nexperiment, we generated adversarial samples based on one\ninterpreter and produce attribution maps of those samples via\ntwo interpreters including the target interpreter. For example,\nadversarial samples are generated based on Grad interpreter\nand attributions maps of those samples are extracted from both\nGrad and CAM interpreters. We repeated the same process by\ntargeting CAM and applying Grad as a secondary interpreter.\nThe generated attribution maps are based on single-channel,\ntherefore, we stacked single-channel attribution maps of two\ninterpreters to convert into benign and adversarial two-channel\ndata correspondingly. 2,000 benign and 2,000 adversarial\nsamples are produced for each experiment. As the dataset size\nis small, we adopted pre-trained DNN model VGG-16 [40]\nto extract feature vectors from convolutional layers and we\nadopted ML model Gradient boosting classiﬁer as a ﬁnal layer\ninstead of a fully-connected layer. The main reason for the\napproach is that the benign and adversarial attribution maps\nshare high similarity and it is a complex process enough to\nclassify the samples. Gradient boosting classiﬁer is a group\nof machine learning algorithms that combines several weak\nlearning models to form a strong predictive model.\nAs mentioned earlier, each attribution map is a one-channel\nimage. As a result, we replaced the input and the output layer\n11\nTABLE V: The results of the two-interpretation-based detector.\nThe upper part relates to 2-channel samples and the lower part\nshows the results of 3-channel samples.\n2-channel samples\nGrad\nCAM\nMASK\nRTS\nGrad\n-\n-\n-\n-\nCAM\n0.896\n-\n-\n-\nMASK\n0.869\n0.866\n-\n-\nRTS\n0.762\n0.716\n0.701\n-\n3-channel samples\nGrad\n-\n-\n-\n-\nCAM\n0.890\n-\n-\n-\nMASK\n0.911\n0.894\n-\n-\nRTS\n0.744\n0.712\n0.714\n-\nTABLE VI: Results of the three-interpretation-based detector.\nCombinations of interpreters\nAccuracy\nGrad, CAM, MASK\n0.975\nGrad, CAM, RTS\n0.861\nGrad, MASK, RTS\n0.838\nCAM, MASK, RTS\n0.848\nof VGG-16 and trained only those layers to adjust the weights\nof the model and to generate 2-channel samples. For the\nsecond case, we stacked the multiplication of attribution maps\ncollected from two interpreters as the third channel. In Table\nV, the results of both cases are provided. As shown Table V,\nthe detector could be used to improve the adversarial detection\nprocess by generating enough samples. We also created an\nexperiment by adopting three different interpreters and stacked\nthe attribution maps of those interpretation models into a 3-\nchannel sample. Table VI shows results of the detector that is\nbased on the combination of three interpreters.\nInterpretation-based Robust Training. The defense tech-\nnique is based on the method in [41]. The technique considers\na generic form of L1 2-class interpretation difference:\nDL1(x, ˆx) = (1/2)(∥gy(x, f) −gy(ˆx, f)∥1\n+∥gyt(x, f) −gyt(ˆx, f)∥1)\n(20)\nIn Eq. 20 creates a perturbation-independent lower bound\nfor any adversarial attack that makes hard to fool a clas-\nsiﬁer and evade the interpretation discrepancy. Training a\nclassiﬁer against the worst-case interpretation difference is\nrecommended by the min-max optimization problem:\nmin\nθ\nEx\n\u0002\nftrain(θ; x, y) + γDworst(x, ˆx)\n\u0003\n(21)\nwhere ftrain is the training loss (i.e., cross-entropy loss),\nDworst denotes a measurement of the highest interpretation\ndifference between benign and adversarial samples x and ˆx,\nand γ balances the accuracy and model interpretability. For the\nexperiment, we adopted PGD attack framework to generate\nadversarial samples. The results of the robust training are\nprovided in Table VII. In the table, the results are produced by\nthe Wide-ResNet [42] model with coupled CAM interpreter on\nCIFAR-10 [43] dataset. Interpretability results are calculated\nby applying Kendall’s Tau order rank correlation between\nthe original and adversarial interpretation maps. As shown\nin the Table VII, the adversarial-trained model provided high\nTABLE VII: The results of interpretation-based robust training\nwith different perturbation size ϵ. NT stands for Normal Train-\ning and AT denotes Adversarial Training. Testing accuracy is\nbased on the robustness of the classiﬁer (upper part) and lower\npart provides the results of similarity of attribution maps.\nϵ\n0\n2/255\n4/255\n6/255\n8/255\n9/255\n10/255\nTesting Accuracy\nNT\n0.785\n0.190\n0.090\n0.080\n0.085\n0.085\n0.080\nAT\n0.685\n0.565\n0.465\n0.335\n0.280\n0.265\n0.220\nAttack against interpretability\nNT\n0.542\n0.132\n-0.06\n-0.100\n-0.083\n-0.183\nAT\n0.870\n0.782\n0.736\n0.703\n0.706\n0.676\nclassiﬁcation and interpretation robustness as compared with\nthe normal-trained model.\nIX. CONCLUSION\nThis paper proposes two attack methods (AdvEdge and\nAdvEdge+) to improve the adversarial attacks on interpretable\ndeep learning systems (IDLSes). Those methods use edge\ninformation of image inputs to optimize the ADV2 attack,\nwhich provides adversarial samples to mislead the target DNN\nmodels and their coupled interpretation models at the same\ntime. Through empirical examination of a large dataset on two\ndistinct DNN architectures by adopting three different attack\nframeworks (PGD, StAdv, and C&W), we showed the validity\nand efﬁcacy of AdvEdge and AdvEdge+ attacks. We presented\nour ﬁndings in comparison to four distinct types of interpreters\n(i.e., Grad, CAM, MASK, and RTS). The results indicated\nthat AdvEdge and AdvEdge+ effectively produce adversarial\nsamples that are capable of misleading DNN models and\ntheir interpreters. We also showed that the transferability of\nattribution maps generated by our attack methods provides\nsigniﬁcantly better results than existing methods. The results\nconﬁrmed that the interpretability of IDLSes provides a limited\nsense of security in the decision-making process.\nACKNOWLEDGMENT\nThis research was supported by the MSIT (Ministry of\nScience and ICT), Korea, under the ICT Creative Consilience\nProgram (IITP-2021-2020-0-01821) supervised by the IITP\n(Institute for Information & communications Technology Plan-\nning & Evaluation), and the National Research Foundation of\nKorea (NRF) grant funded by the Korea government (MSIT)\n(No. 2021R1A2C1011198).\nREFERENCES\n[1] E. Abdukhamidov, M. Abuhamad, F. Juraev, E. Chan-Tin, and\nT. AbuHmed, “Advedge: Optimizing adversarial perturbations against\ninterpretable deep learning,” in Computational Data and Social Net-\nworks, D. Mohaisen and R. Jin, Eds.\nCham: Springer International\nPublishing, 2021, pp. 93–105.\n[2] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770–778.\n[3] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning\nwith neural networks,” arXiv preprint arXiv:1409.3215, 2014.\n[4] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between\ncapsules,” arXiv preprint arXiv:1710.09829, 2017.\n[5] Q. Zhang, Y. N. Wu, and S.-C. Zhu, “Interpretable convolutional neural\nnetworks,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2018, pp. 8827–8836.\n12\n[6] P. Dabkowski and Y. Gal, “Real time image saliency for black box\nclassiﬁers,” arXiv preprint arXiv:1705.07857, 2017.\n[7] R. C. Fong and A. Vedaldi, “Interpretable explanations of black boxes\nby meaningful perturbation,” in Proceedings of the IEEE International\nConference on Computer Vision, 2017, pp. 3429–3437.\n[8] K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside convolutional\nnetworks: Visualising image classiﬁcation models and saliency maps,”\narXiv preprint arXiv:1312.6034, 2013.\n[9] X. Zhang, N. Wang, H. Shen, S. Ji, X. Luo, and T. Wang, “Interpretable\ndeep learning under ﬁre,” in 29th {USENIX} Security Symposium\n({USENIX} Security 20), 2020.\n[10] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support\nvector machines,” arXiv preprint arXiv:1206.6389, 2012.\n[11] N. Dalvi, P. Domingos, S. Sanghai, and D. Verma, “Adversarial clas-\nsiﬁcation,” in Proceedings of the tenth ACM SIGKDD international\nconference on Knowledge discovery and data mining, 2004, pp. 99–108.\n[12] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[13] F. Juraev, E. Abdukhamidov, M. Abuhamad, and T. Abuhmed, “Depth,\nbreadth, and complexity: Ways to attack and defend deep learning\nmodels,” in Proceedings of the 2022 ACM on Asia Conference on\nComputer and Communications Security, 2022, pp. 1207–1209.\n[14] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards\ndeep learning models resistant to adversarial attacks,” arXiv preprint\narXiv:1706.06083, 2017.\n[15] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,\nand R. Fergus, “Intriguing properties of neural networks,” arXiv preprint\narXiv:1312.6199, 2013.\n[16] E. Abdukhamidov, F. Juraev, M. Abuhamad, and T. Abuhmed, “Black-\nbox and target-speciﬁc attack against interpretable deep learning sys-\ntems,” in Proceedings of the 2022 ACM on Asia Conference on Computer\nand Communications Security, 2022, pp. 1216–1218.\n[17] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into transfer-\nable adversarial examples and black-box attacks,” arXiv preprint\narXiv:1611.02770, 2016.\n[18] N. Papernot, P. McDaniel, and I. Goodfellow, “Transferability in ma-\nchine learning: from phenomena to black-box attacks using adversarial\nsamples,” arXiv preprint arXiv:1605.07277, 2016.\n[19] Q. Huang, I. Katsman, H. He, Z. Gu, S. Belongie, and S.-N. Lim,\n“Enhancing adversarial example transferability with an intermediate\nlevel attack,” in Proceedings of the IEEE/CVF international conference\non computer vision, 2019, pp. 4733–4742.\n[20] C. Xie, Z. Zhang, Y. Zhou, S. Bai, J. Wang, Z. Ren, and A. L. Yuille,\n“Improving transferability of adversarial examples with input diversity,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 2730–2739.\n[21] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller,\n“Striving for simplicity: The all convolutional net,” arXiv preprint\narXiv:1412.6806, 2014.\n[22] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\nD. Batra, “Grad-cam: Visual explanations from deep networks via\ngradient-based localization,” in Proceedings of the IEEE international\nconference on computer vision, 2017, pp. 618–626.\n[23] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning\ndeep features for discriminative localization,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2016, pp. 2921–\n2929.\n[24] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are easily\nfooled: High conﬁdence predictions for unrecognizable images,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2015, pp. 427–436.\n[25] N. Liu, H. Yang, and X. Hu, “Adversarial detection with model in-\nterpretation,” in Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, 2018, pp. 1803–\n1811.\n[26] G. Tao, S. Ma, Y. Liu, and X. Zhang, “Attacks meet interpretabil-\nity: Attribute-steered detection of adversarial samples,” arXiv preprint\narXiv:1810.11580, 2018.\n[27] K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside convolutional\nnetworks: Visualising image classiﬁcation models and saliency maps,”\n2014.\n[28] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Univer-\nsal adversarial perturbations,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2017, pp. 1765–1773.\n[29] A. Kurakin, I. Goodfellow, S. Bengio et al., “Adversarial examples in\nthe physical world,” 2016.\n[30] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural\nnetworks,” in 2017 ieee symposium on security and privacy (sp). IEEE,\n2017, pp. 39–57.\n[31] C. Xiao, J.-Y. Zhu, B. Li, W. He, M. Liu, and D. Song, “Spatially\ntransformed adversarial examples,” arXiv preprint arXiv:1801.02612,\n2018.\n[32] I. Sobel, R. Duda, and P. Hart, “Sobel-feldman operator.”\n[33] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. M¨uller, and\nW. Samek, “On pixel-wise explanations for non-linear classiﬁer deci-\nsions by layer-wise relevance propagation,” PloS one, vol. 10, no. 7, p.\ne0130140, 2015.\n[34] D. Smilkov, N. Thorat, B. Kim, F. Vi´egas, and M. Wattenberg,\n“Smoothgrad:\nremoving\nnoise\nby\nadding\nnoise,”\narXiv\npreprint\narXiv:1706.03825, 2017.\n[35] M. Ancona, E. Ceolini, C. ¨Oztireli, and M. Gross, “Towards better\nunderstanding of gradient-based attribution methods for deep neural\nnetworks,” arXiv preprint arXiv:1711.06104, 2017.\n[36] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in International Conference on\nMedical image computing and computer-assisted intervention. Springer,\n2015, pp. 234–241.\n[37] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar, “Do imagenet\nclassiﬁers generalize to imagenet?” in International Conference on\nMachine Learning.\nPMLR, 2019, pp. 5389–5400.\n[38] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\nconnected convolutional networks,” in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, 2017, pp. 4700–4708.\n[39] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image\nquality assessment: from error visibility to structural similarity,” IEEE\ntransactions on image processing, vol. 13, no. 4, pp. 600–612, 2004.\n[40] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[41] A. Boopathy, S. Liu, G. Zhang, C. Liu, P.-Y. Chen, S. Chang, and\nL. Daniel, “Proper network interpretability helps adversarial robustness\nin classiﬁcation,” in International Conference on Machine Learning.\nPMLR, 2020, pp. 1014–1023.\n[42] S. Zagoruyko and N. Komodakis, “Wide residual networks,” arXiv\npreprint arXiv:1605.07146, 2016.\n[43] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features\nfrom tiny images,” 2009.\n13\n",
  "categories": [
    "cs.CR",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2022-11-29",
  "updated": "2022-11-29"
}