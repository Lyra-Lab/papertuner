{
  "id": "http://arxiv.org/abs/2212.00253v1",
  "title": "Distributed Deep Reinforcement Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox",
  "authors": [
    "Qiyue Yin",
    "Tongtong Yu",
    "Shengqi Shen",
    "Jun Yang",
    "Meijing Zhao",
    "Kaiqi Huang",
    "Bin Liang",
    "Liang Wang"
  ],
  "abstract": "With the breakthrough of AlphaGo, deep reinforcement learning becomes a\nrecognized technique for solving sequential decision-making problems. Despite\nits reputation, data inefficiency caused by its trial and error learning\nmechanism makes deep reinforcement learning hard to be practical in a wide\nrange of areas. Plenty of methods have been developed for sample efficient deep\nreinforcement learning, such as environment modeling, experience transfer, and\ndistributed modifications, amongst which, distributed deep reinforcement\nlearning has shown its potential in various applications, such as\nhuman-computer gaming, and intelligent transportation. In this paper, we\nconclude the state of this exciting field, by comparing the classical\ndistributed deep reinforcement learning methods, and studying important\ncomponents to achieve efficient distributed learning, covering single player\nsingle agent distributed deep reinforcement learning to the most complex\nmultiple players multiple agents distributed deep reinforcement learning.\nFurthermore, we review recently released toolboxes that help to realize\ndistributed deep reinforcement learning without many modifications of their\nnon-distributed versions. By analyzing their strengths and weaknesses, a\nmulti-player multi-agent distributed deep reinforcement learning toolbox is\ndeveloped and released, which is further validated on Wargame, a complex\nenvironment, showing usability of the proposed toolbox for multiple players and\nmultiple agents distributed deep reinforcement learning under complex games.\nFinally, we try to point out challenges and future trends, hoping this brief\nreview can provide a guide or a spark for researchers who are interested in\ndistributed deep reinforcement learning.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n1\nDistributed Deep Reinforcement Learning: A\nSurvey and A Multi-Player Multi-Agent Learning\nToolbox\nQiyue Yin, Tongtong Yu, Shengqi Shen, Jun Yang, Meijing Zhao, Kaiqi Huang, Bin Liang, Liang Wang\nAbstract—With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential\ndecision-making problems. Despite its reputation, data inefﬁciency caused by its trial and error learning mechanism makes deep\nreinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efﬁcient deep\nreinforcement learning, such as environment modeling, experience transfer, and distributed modiﬁcations, amongst which, distributed\ndeep reinforcement learning has shown its potential in various applications, such as human-computer gaming, and intelligent\ntransportation. In this paper, we conclude the state of this exciting ﬁeld, by comparing the classical distributed deep reinforcement\nlearning methods, and studying important components to achieve efﬁcient distributed learning, covering single player single agent\ndistributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learning.\nFurthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many\nmodiﬁcations of their non-distributed versions. By analyzing their strengths and weaknesses, a multi-player multi-agent distributed\ndeep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment,\nshowing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under\ncomplex games. Finally, we try to point out challenges and future trends, hoping this brief review can provide a guide or a spark for\nresearchers who are interested in distributed deep reinforcement learning.\nIndex Terms—Deep reinforcement learning, distributed machine learning, self-play, population-play, toolbox.\n!\n1\nINTRODUCTION\nW\nITH\nthe\nbreakthrough\nof\nAlphaGo\n[1],\n[2],\nan\nagent that wins plenty of professional Go players\nin human-computer gaming, deep reinforcement learning\n(DRL) comes to most researchers’ attention, which becomes\na recognized technique for solving sequential decision mak-\ning problems. Plenty of algorithms are developed to solve\nchallenging issues that lie between DRL and real world\napplications, such as exploration and exploitation dilemma,\ndata inefﬁciency, multi-agent cooperation and competition.\nAmong all these challenges, data inefﬁciency is the most\ncriticized due to the trial and error learning mechanism of\nDRL, which requires a huge amount of interactive data.\nTo alleviate the data inefﬁciency problem, several re-\nsearch directions are developed [3]. For example, model\nbased deep reinforcement learning constructs environment\nmodel for generating imaginary trajectories to help reduce\ntimes of interaction with the environment. Transfer rein-\nforcement learning mines shared skills, roles, or patterns\nfrom source tasks, and then uses the learned knowledge to\naccelerate reinforcement learning in the target task. Inspired\nfrom distributed machine learning techniques, which has\nbeen successfully utilized in computer vision and natural\nlanguage processing [4], distributed deep reinforcement\n•\nQiyue Yin (qyyin@nlpr.ia.ac.cn), Tongtong Yu, Shengqi Shen, Meijing\nZhao, Kaiqi Huang and Liang Wang are with Institute of Automation,\nChinese Academy of Sciences, Beijing, China, 100190.\n•\nJun Yang and Bin Liang are with the Department of Automation, Ts-\ninghua University, Beijing, China, 100084.\n•\nCorresponding\nauthors:\nkqhuang@nlpr.ia.ac.cn\n(Kaiqi\nHuang);\nyangjun603@tsinghua.edu.cn (Jun Yang)\nlearning (DDRL) is developed, which has shown its poten-\ntial to train very successful agents, e.g., Suphx [5], OpenAI\nFive [6], and AlphaStar [7].\nGenerally, training DRL agents consists of two main\nparts, i.e., pulling policy network parameters to generate\ndata by interacting with the environment, and updating\npolicy network parameters by consuming data. Such a\nstructured pattern makes distributed modiﬁcations of DRL\nfeasible, and plenty of DDRL algorithms have been de-\nveloped. For example, the general reinforcement learning\narchitecture [8], likely the ﬁrst DDRL architecture, divides\nthe training system into four components, i.e., parameter\nserver, learners, actors and replay buffer, which inspires\nsuccessive more data efﬁcient DDRL architectures. The\nrecently proposed SEED RL [9], an improved version of\nIMPALA [10], is claimed to be able to produce and consume\nmillions of frames per second, based on which, AlphaStar\nis successfully trained within 44 days (192 v3 + 12 128 core\nTPUs, 1800 CPUs) for beating professional human players.\nTo make distributed modiﬁcations of DRL be able to use\nmultiple machines, several engineering problems should be\nsolved such as machines communication and distributed\nstorage. Fortunately, several useful toolboxes have been\ndeveloped and released, and revising codes of DRL to a\ndistributed version usually requires a small amount of code\nmodiﬁcation, which largely promotes the development of\nDDRL. For example, Horovod [11], released by Uber, makes\nfull use of ring allreduce technique, and can properly use\nmultiple GPUs for training acceleration by just adding a few\nlines of codes compared with the single GPU version. Ray\narXiv:2212.00253v1  [cs.LG]  1 Dec 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n2\n[12], a distributed framework of machine learning released\nby UC Berkeley RISELab, provides a RLlib [13] for efﬁcient\nDDRL, which is easy to be used due to its reinforcement\nlearning abstraction and algorithm library.\nConsidering the big progress of DDRL, it is emergent to\ncomb out the course of DDRL techniques, challenges and\nopportunities, so as to provide clues for future research.\nRecently, Samsami and Alimadad [14] gave a brief review of\nDDRL, but their aim is single player single agent distributed\nreinforcement learning frameworks, and more challenging\nmultiple agents and multiple players DDRL is absent. Czech\n[15] made a short survey on distributed methods for re-\ninforcement learning, but only several speciﬁc algorithms\nare categorized and no key techniques, comparison and\nchallenges are discussed. Different from previous summary,\nthis paper shows a more comprehensive survey by com-\nparing the classical distributed deep reinforcement learning\nmethods, and studying important components to achieve\nefﬁcient distributed learning, covering single player single\nagent distributed deep reinforcement learning to the most\ncomplex multiple players multiple agents distributed deep\nreinforcement learning.\nThe rest of the paper is organized as follows. In Section\n2, we brieﬂy describe background of DRL, distributed learn-\ning, and typical testbeds for DDRL. In section 3, we elabo-\nrate on taxonomy of DDRL, by dividing current algorithms\nbased on the learning frameworks and players and agents\nparticipating in. In Section 4, we compare current DDRL\ntoolboxes, which help achieve efﬁcient DDRL a lot. In Sec-\ntion 5, we introduce a new multi-player multi-agent DDRL\ntoolbox, which provides a useful DDRL tool for complex\ngames. In Section 6, we summarize the main challenges and\nopportunities for DDRL, hoping to inspire future research.\nFinally, we conclude the paper in Section 7.\n2\nBACKGROUND\n2.1\nDeep Reinforcement Learning\nReinforcement learning is a typical kind of machine learning\nparadigm, the essence of which is learning via interaction.\nIn a general reinforcement learning method, an agent in-\nteracts with an environment by posing actions to drive the\nenvironment dynamics, and receiving rewards to improve\nits policy for chasing long-term outcomes. To learn a good\nagent that can make sequential decisions, there are two\ntypical kinds of algorithms, i.e., model-free methods that use\nno environment models, and model-based approaches that\nuse the pre-given or learned environment models. Plenty\nof algorithms have been proposed, and readers can refer to\n[16], [17] for a more thorough review.\nIn reality, applications naturally involve the participa-\ntion of multiple agents, making multi-agent reinforcement\nlearning a hot topic. Generally, multi-agent reinforcement\nlearning is modeled as a stochastic game, and obeys similar\nlearning paradigm with conventional reinforcement learn-\ning. Based on the game setting, agents can be fully coop-\nerative, competitive and a mix of the two, requiring rein-\nforcement learning agents to emerge abilities that can match\nthe goal. Various key problems of multi-agent reinforcement\nlearning have been raised, e.g., communication and credit\nassignment. Readers can refer to [18], [19] for a detailed\nintroduction.\nWith the breakthrough of deep learning, deep rein-\nforcement learning becomes a strong learning paradigm by\ncombining representation learning ability of deep learning\nand decision making ability of reinforcement learning, and\nseveral successful deep reinforcement learning agents have\nbeen proposed. For example, AlphaGo [1], [2], a Go agent\nthat can beat professional human players, is based on single\nagent deep reinforcement learning. OpenAI Five, a dota2\nagent that wins champion players in an e-sport for the ﬁrst\ntime, relies on multi-agent deep reinforcement learning. In\nthe following, unless otherwise stated, we do not distin-\nguish single agent or multiple agents deep reinforcement\nlearning.\n2.2\nDistributed Learning\nThe success of deep learning is inseparable from huge data\nand computing power, which leads to huge demand of\ndistributed learning that can handle data intensive and com-\npute intensive computing. Due to the structured computa-\ntion pattern of deep learning algorithms, some successful\ndistributed learning methods are proposed for parallelism\nin deep learning [20], [21]. An early popular distributed\ndeep learning framework is DistBelief [22], designed by\nGoogle, where concepts of parameter server and A-SGD are\nproposed. Based on DistBelief, Google released the second\ngeneration of distributed deep learning framework, Tensor-\nﬂow [23], which becomes a widely used tool. Other typical\ndistributed deep learning frameworks, such as PyTorch,\nMXNet, and Caffe2 are also developed and used by the\nresearch and industrial communities.\nBen-Nun and Hoeﬂer [24] gave an in-depth concurrency\nanalysis of parallel and distributed deep learning. In the\nsurvey, the authors gave different types of concurrency for\ndeep neural networks, covering the bottom level opera-\ntors, and key factors such as network inference and train-\ning. Finally, several important topics such as asynchronous\nstochastic optimization, distributed system architectures,\ncommunication schemes are discussed, providing clues for\nfuture directions of distributed deep learning. Nowadays,\ndistributed learning is widely used in various ﬁelds, such\nas wireless networks [25], AIoT service platform [26] and\nhuman-computer gaming [27]. In short, DDRL is a special\ntype of distributed deep learning. Instead of focusing on\ndata parallelism and model parallelism in conventional\ndeep learning, DDRL aims at improving data throughput\ndue to the characteristics of reinforcement learning.\n2.3\nTesting Environments\nWith the huge success of AlphaGo [1], DDRL is widely\nused in games, especially human-computer gaming. Those\ngames provide an ideal testbeds for development of DDRL\nalgorithms or frameworks, from single player single agent\nDDRL to multiple players multiple agents DDRL.\nAtari is a popular reinforcement learning testbed be-\ncause it has the similar high dimensional visual input\ncompared to human [28]. Besides, several environments\nconfront challenging issues such as long time horizon and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n3\nsparse rewards [29]. Plenty of DDRL algorithms are com-\npared in Atari games, showing training acceleration against\nDRL without parallelism. However, typical Atari games are\ndesigned for single player single agent problems.\nWith the emerging of multi-agent reinforcement learn-\ning in multi-agent games, StarCraft Multi-Agent Challenge\n(SMAC) [30] becomes a recognized testbed for single player\nmulti-agent reinforcement learning. Speciﬁcally, SMAC is\na sub-task of StarCraft by focusing on micromanagement\nchallenges, where a team of units is controlled to ﬁght\nagainst build-in opponents. Several typical multi-agent re-\ninforcement learning algorithms are released along with\nSMAC, which support parallel data collection in reinforce-\nment learning.\nApart from the above single player single agent and\nsingle player multiple agents testing environments, there are\na few multiple players environments for deep reinforcement\nlearning algorithms [31]. Even though huge success has\nbeen made for games like Go, StarCraft, dota2 and honor\nof kings, those multiple players environments are used\nfor a few researchers due to the huge game complexity.\nOverall, those multiple player single agent and multiple\nagents environments largely promote the development of\nDDRL.\n3\nTAXONOMY\nOF\nDISTRIBUTED\nDEEP\nREIN-\nFORCEMENT LEARNING\n3.1\nTaxonomic Basis\nPlenty of DDRL algorithms or frameworks are developed\nwith representatives such as GORILA [8], A3C [32], APE-\nX [33], IMPALA [10], Distributed PPO [34], R2D2 [35] and\nSeed RL [9], based on which, we can draw the key compo-\nnents of a DDRL, as shown in Fig. 1. We sometimes use the\nframeworks instead of algorithms or methods because these\nframeworks are not targeted to a speciﬁc reinforcement\nlearning algorithm, and they are more like a distributed\nframework for various reinforcement learning methods.\nGenerally, there are mainly three parts for a basic DDRL\nalgorithm, which forms a single player single agent DDRL\nmethod:\n•\nActors: produce data (trajectories or gradients) by\ninteracting with the environment.\n•\nLearners: consume data (trajectories or gradients) to\nperform neural network parameters updating.\n•\nCoordinators: coordinate data (parameters or trajec-\ntories) to control the communication between learn-\ners and actors.\nActors pull neural network parameters from the learn-\ners, receive states from the environments, and perform\ninference to obtain actions, which drive the dynamics of\nenvironments to the next states. By repeating the above\nprocess with more than one actor, data throughput can\nbe increased and enough data can be collected. Learners\npull data from actors, perform gradients calculation or post-\nprocessing, and update the network parameters. More than\none learner can alleviate the limited storage of a GPU by\nutilizing multiple GPUs with tools such as ring allreduce or\nparameter-server [11]. By repeating above process, the ﬁnal\nreinforcement learning agent can be obtained.\nLearners\nActors\nData: used for actor to inference\nData: used for learner to update\nCoordinator\nCoordinator\nFig. 1. Basic framework of DDRL.\nAgents cooperation\nLearners\nActors\nCoordinators\nSingle Player Single \nAgent DDRL\nPlayers evolution\nSingle Player Multiple \nAgents DDRL\nMultiple Player Multiple \nAgents DDRL\nMultiple Players Single \nAgent DDRL\nFig. 2. Single player single agent DDRL to multiple players multiple\nagents DDRL.\nCoordinators are important for the DDRL algorithms,\nwhich control the communication between learners and\nactors. For example, when the coordinators are used to syn-\nchronize the parameters updating and pulling (by actors),\nthe DDRL algorithm is synchronous. When the parameters\nupdating and pulling (by actors) are not strictly coordinated,\nthe DDRL algorithm is asynchronous. So a basic classiﬁca-\ntion of DDRL algorithms can be based on the coordinators\ntypes.\n•\nSynchronous: global policy parameters updating is\nsynchronized, and pulling policy parameters (by ac-\ntors) is synchronous, i.e., different actors share the\nsame latest global policy.\n•\nAsynchronous: Updating the global policy parame-\nters is asynchronous, or policy updating (by learners)\nand pulling (by actors) are asynchronous, i.e., actors\nand learners usually have different policy parame-\nters.\nWith the above basic framework, a single player single\nagent DDRL algorithm can be designed. However, when\nfacing multiple agents or multiple players, the basic frame-\nwork is unable to train usable RL agents. Based on current\nDDRL algorithms that support large system level AI such\nas AlphaStar [7], OpenAI Five [6] and JueWU [36], two\nkey components are essential to build multiple players and\nmultiple agents DDRL, i.e., agents cooperation and players\nevolution, as shown in Fig. 2.\nModule of agents cooperation is used to train multiple\nagents based on multi-agent reinforcement learning algo-\nrithms [18]. Generally, multi-agent reinforcement learning\ncan be classiﬁed into two categories, i.e., independent train-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n4\nSingle Player Single Agent DDRL\n※ Coordinators type\n•\nAsynchronous based\n•\nSynchronous based\n※ Players \nevolution\n•\nSelf-play based for all players\n•\nPopulation-play based for all players\n※ Agents \ncooperation\n•\nIndependent training for each agent\n•\nJoint training for all agents\nSingle Player multiple Agents DDRL\nMultiple Players Single Agent DDRL\nMultiple Players Multiple Agents DDRL\nDDRL components\nLearners\nActors\nCoordinators\nFig. 3. The taxonomy of distributed deep reinforcement learning.\ning and joint training, based on how to perform agents\nrelationship modeling.\n•\nIndependent training: train each agent indepen-\ndently by considering other learning agents as part\nof the environment.\n•\nJoint training: train all the agents as a whole, con-\nsidering factors such as agents communication, re-\nward assignment, and centralized training with dis-\ntributed execution.\nModule of players evolution is designed for agents itera-\ntion for each player, where agents of other players are learn-\ning at the same time, leading to more than one generation\nof agents to be learned for each player like in AlphaStar,\nand OpenAI Five. Based on current mainstream players\nevolution techniques, players evolution can be divided into\ntwo types:\n•\nSelf-play based: different players share the same\npolicy networks, and the player updates the current\ngeneration of policy by confronting its past versions.\n•\nPopulation-play based: different players have differ-\nent policy networks, or called populations, and a\nplayer updates its current generation of policy by\nconfronting other players or/and its past versions.\nFinally, based on the above key components for DDRL,\nthe taxonomy of DDRL is shown in Fig. 3. In the following,\nwe will summarize and compare representative methods\nbased on their main characteristics.\n3.2\nCoordinators Types\nBased on the coordinators types, DDRL algorithms can be\ndivided into asynchronous based and synchronous based.\nFor a asynchronous based DDRL method, there are two\ncases: the updating of global policy parameters is asyn-\nchronous; the global policy parameters updating (by learn-\ners) and pulling (by actors) are asynchronous. For a syn-\nchronous based DDRL method, global policy parameters\nupdating is synchronized, and pulling policy parameters\n(by actors) is synchronous.\nLearners\nParameter server\nTarget Q network\nActors\nQ Network-1\nEnvironment-1\nEnvironment-n\nQ network -n\n…\n…\naction\nstate\naction\nstate\nTrajectory\nQ network\nPull\nmodel\nGradient\nT\nT\nFig. 4. Basic framework of Gorila.\n3.2.1\nAsynchronous based\nNair et al. [8] proposed probably the ﬁrst massively dis-\ntributed architecture for deep reinforcement learning, Go-\nrila, which builds the basis of the succeeding DDRL algo-\nrithms. As shown in Fig. 4, a distributed deep Q-Network\n(DQN) algorithm is implemented. Apart from the basic\nDQN algorithm that mains a Q network and a target Q\nnetwork, the distribution lies in: parallel actors to generate\ntrajectories and send them to the Q network and target Q\nnetwork of the learners, and learners to calculate gradients\nfor parameters updating based on a parameter server tool\nthat can store a distributed neural network. The algorithm\nis asynchronous because neural network parameters up-\ndating of learners and trajectories collecting of actors are\nasynchronously performed without waiting. In their paper,\nthe implemented distributed DQN reduces the wall-time\nrequired to achieve compared or super results by an order\nof magnitude on most 49 games in Atari compared to non-\ndistributed DQN.\nSimilar with [8], Horgan et al. [33] introduced distributed\nprioritized experience replay, i.e., APE-X, to enhance the Q-\nlearning based distributed reinforcement learning. Specif-\nically, prioritized experience replay is used to sample the\nmost important trajectories, which are generated by all the\nactors. Accordingly, a shared experience replay memory\nshould be introduced to store all the generated trajectories.\nIn the experiments, a fraction of the wall-clock training time\nis achieved on the Arcade Learning Environment. To further\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n5\nLearners\nActors\nAgent\nGlobal network\nLocal network -1\nEnvironment-1\nEnvironment-n\nLocal network -n\n…\n…\naction\nstate\naction\nstate\nGradient\nGradient\nPull\nmodel\nPull\nmodel\nT\nT\nFig. 5. Basic framework of A3C.\nenhance [33], Kapturowski et al. [35] proposed recurrent\nexperience replay in distributed reinforcement learning, i.e.,\nR2D2, by introducing RNN-based reinforcement learning\nagents. The authors investigate the effects of parameter lag\nand recurrent state staleness problems on the performance,\nobtaining the ﬁrst agent to exceed human-level performance\nin 52 of the 57 Atari games with the designed training\nstrategy.\nMnih et. al [32] proposed Asynchronous Advantage\nActor-Critic (A3C) framework, which can make full use of\nthe multi-core CPU instead of the GPU, leading to cheap dis-\ntribution of reinforcement learning algorithm. As shown in\nFig. 5, each actor calculates gradient of the samples (mainly\nstates, actions and rewards used for regular reinforcement\nlearning algorithms), send them to the learners, and then\nupdate the global policy. The updating is asynchronous\nwithout synchronization among gradients from different\nactors. Besides, parameters (maybe not the latest version)\nare pulled by each actor to generate data with environ-\nments. In their paper, four speciﬁc reinforcement learning\nalgorithms are established, i.e., asynchronous one-step Q-\nlearning, asynchronous one-step Sarsa, asynchronous n-step\nQ-learning and asynchronous advantage actor-critic. Exper-\niments show that half the time on a single multi-core CPU\ninstead of a GPU is obtained on the Atari domain.\nTo make use of the GPU’s computational power instead\nof just the multi-core CPU as in A3C, Babaeizadeh et al. [37]\nproposed asynchronous advantage actor-critic on a gpu, i.e.,\nGA3C, which is a hybrid CPU/GPU version of the A3C. As\nshown in Fig. 6, the learner consists of three parts: predictor\nto dequeue prediction requests and obtain actions by the\ninference, trainer to dequeue batches of trajectories for the\nagent model, and the agent model to update the parameters\nwith the trajectories. Noted that the threads of predictor and\ntrainer are asynchronously executed. With the above multi-\nprocess, multi-thread CPU for environments rollout and a\nGPU, GA3C achieves a signiﬁcant speed up compared to\nA3C.\nPlacing gradient calculation in the actor side will limit\nthe data throughput of the whole DDRL system, i.e., tra-\njectories collected per time unit, so Espeholt et al. [10]\nproposed Importance Weighted Actor-Learner Architecture\n(IMPALA) to alleviate this problem. As shown in 7, parallel\nactors communicate with environments, collect trajectories,\nand send them to the learners for parameters updating.\nSince gradients calculation is put in the learners side, which\ncan be accelerated with GPUs, the framework is claimed\nto scale to thousands of machines without sacriﬁcing data\nefﬁciency. Considering that the local policy used to generate\nLearners\nActors\n…\nModel\nGlobal network\nGradient\nEnvironment-1\nEnvironment-n\nTrainers\nPredictors\nStates, Rewards\nStates\nState\nState\nActions\nAction\nT\nT\nFig. 6. Basic framework of GA3C.\nLearners\nActors\nAgent\nGlobal network\nLocal network -1\nEnvironment-1\nEnvironment-n\nLocal network -n\n…\n…\naction\nstate\naction\nstate\nTrajectory\nPull\nmodel\nTrajectory\nGradient\nPull\nmodel\nT\nT\nFig. 7. Basic framework of impala.\ntrajectories are behind the global policy in the learners due\nto the asynchrony between learner and actors, a V-trace\noff-policy actor-critic algorithm is introduced to correct the\nharmful discrepancy. Experiments on DMLab-30 and Atari-\n57 show that IMPALA can achieve better performance with\nless data compared with previous agents.\nBy using synchronized sampling strategy for actors in-\nstead of the independent sampling of IMPALA, Stooke and\nAbbeel [38] proposed a novel accelerated method, which\nconsists of two main parts, i.e, synchronized sampling\nand synchronous/asynchronous multi-GPU optimization.\nAs shown in 8, individual observations of each environment\nare gathered into a batch for inference, which largely reduce\nthe inference times compared with approaches that generate\ntrajectories for each environment independently. However,\nsuch synchronized sampling may suffer from slowdown\nwhen different environments in different processes have\nlarge execution differences, which is alleviated by tricks\nsuch as allocating available CPU cores used for environ-\nments evenly. As for the learners, they server as a parameter\nserver, whose parameters are pushed by actors, and then\nupdated asynchronously among other actors. The imple-\nmented asynchronous version of PPO, i.e., APPO, learn\nsuccessful policies in Acari games in mere minutes.\nWith the above synchronized sampling in [38], inference\nLearners\nActors\nAgent\nGlobal network\nLocal network -1\nLocal network -n\n…\nactions\nstates\naction\nstate\nPull \nmodel\nEnvironment-1\nEnvironment-n\n…\nEnvironment-1\nEnvironment-n\n…\n…\nPush \nmodel\nPush \nmodel\nGradient\nGradient\nT\nT\nFig. 8. Basic framework of APPO.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n6\nLearners\nActors\nEnvironment-1\nEnvironment-n\n…\nEnvironment-1\nEnvironment-n\n…\n…\nModel\nGlobal network\nTrajectory queue\nStates\nActions\nActions\nGradient\nBatches\nCache\nT\nT\nFig. 9. Basic framework of SEEDRL.\nLearners\nActors\n…\nModel\nGlobal network\nTrajectories pool\nEnvironment-1\nEnvironment-n\nBatch \nstates\nDistribute \nactions\nT\nT+1\nGradient\nT\nFig. 10. Basic framework of PAAC.\ntimes will be largely reduced, but the communication bur-\nden between learners and actors will be a big problem when\nthe networks are huge. Espeholt et al. [9] proposed Scalable,\nEfﬁcient, Deep-RL (SEEDRL), which features centralized in-\nference and an optimized communication layer called gRPC.\nAs shown in Fig. 9, the communication between learners\nand actors are mere states and actions, which will reduce\nlatency with the proposed high performance RPC library\ngRPC. The authors implemented policy gradients and Q-\nlearning based algorithms and tested them on the Atari-57,\nDeepMind Lab and Google Research Football environments,\nand a 40% to 80% cost reduction is obtained, showing great\nimprovements.\n3.2.2\nSynchronous based\nAs an alternative to asynchronous advantage actor-critic\n(A3C), Clemente et al. [39] found that a synchronous ver-\nsion, i.e., advantage actor-critic (A2C), can better use the\nGPU resources, which should perform well with more ac-\ntors. In the implementation of A2C, i.e., PAAC, a coordinator\nis utilized to wait for all gradients of the actors before\noptimizing the global network. As shown in Fig. 10, learners\nupdate the policy parameters before all the trajectories are\ncollected, i.e., the job of actors is done, and when the\nlearners are updating the policy, the trajectory sampling is\nstopped. As a result, all actors are coordinated to obtain the\nsame global network to interact with environments in the\nfollowing steps.\nAs an alternative of advantage actor-critic (A2C) algo-\nrithm in handling continuous action space, PPO algorithm\n[40] shows great potential due to its trust region con-\nstraint. Heess et al. [34] proposed large scale reinforcement\nlearning with distributed PPO, i.e., DPPO, which has both\nsynchronous and asynchronous versions, and shows better\nperformance with the synchronous update. As shown in\nFig. 11, implementation of DPPO is similar to A3C but with\nsynchronization when updating the policy neural network.\nHowever, the synchronization will limit the throughout of\nLearners\nActors\nModel\nGlobal network\nLocal network -1\nEnvironment-1\nEnvironment-n\nLocal network -n\n…\n…\naction\nstate\naction\nstate\nGradient\nGradient\nParameter\nT\nT+1\nT+1\n×\nFig. 11. Basic framework of DPPO.\nLearner + Actor\nLearner + Actor\nEnvironment-1\nEnvironment-n\nModel\nGlobal network\n…\nModel\nGlobal network\nEnvironment-1\nEnvironment-n\n…\nGradient\nGradient\naction\nstate\nstate\naction\nstate\nstate\nGradient\nT+1\nT\nT\nFig. 12. Basic framework of DDPPO.\nthe whole system due to different rhythm of the actors. The\nauthors use a threshold for the number of actors whose\ngradients must be available for the learners, which makes\nthe algorithm scale to large number of actors.\nDifferent from DPPO algorithm that a server is applied\nfor neural networks updating, Wijmans et al. [41] further\nproposed a decentralized DPPO framework, i.e., DDPPO,\nwhich exhibits near-liner scaling to the GPUs. As shown\nin Fig. 12, a learner and an actor are bundled together,\nas a unit, to perform trajectories collection and gradients\ncalculation. Then gradients from all the units are gathered\ntogether through some reduce operations, e.g., ring allre-\nduce, to update the neural networks, which make sure that\nparameters are the same for all the units. Noted that to\nalleviate the synchronization overhead when performing\ntrajectories collection in parallel units, similar strategies like\nin DPPO is used to discard certain percentages of trajectories\nin several units. Experiments show a speedup of 107x on 128\nGPUs over a serial implementation.\n3.2.3\nDiscussion\nSingle machine or multiple machines. In the beginning\nof developing DDRL algorithms, researchers make previ-\nous non-distributed DRL methods distributed using one\nmachine. For example, the parallel of several typical actor-\ncritic algorithms are designed to use the multi-process of\nCPUs, e.g., A3C [32], and PAAC [39]. Lately, researchers aim\nat improving data throughput of the whole DDRL system,\ne.g., IMPALA [10], and SEEDRL [9], which serves as a basic\ninfrastructure for training complex games AI like AlphaStar\nand OpenAI Five. These systems usually can make use\nof multiple machines. However, early DDRL algorithms\ndesigned for a single machine can also be deployed in multi-\nple machines when communications between machines are\nsolved, which is relatively simple by using open soured\ntools (will be introduced in Section 4).\nExchange trajectories or gradients. Learners and actors\nserve as basic components for DDRL algorithms, and the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n7\ndata communicated between them can be trajectories or gra-\ndients based on whether to put the gradient calculation on\nthe actor or learner side. For example, actors of A3C [32] are\nin charge of trajectories collection and gradients calculation,\nand the gradients are then sent to learners for policy update,\nwhich just make simple operations such as sum operation.\nSince gradients calculation is time-consuming, especially\nthe policy model is large, the calculating load between the\nlearners and actors will be unbalanced. Accordingly, more\nand more DDRL algorithms put gradients calculation in the\nlearners side by using more suitable devices, i.e., GPUs. For\nexample, in the higher data throughput DDRL frameworks\nlike IMPALA [10], learners are in charge of policy updating\nand actors are in charge of trajectories collection.\nsynchronized or independent inference. When actors\nare collecting trajectories by interacting with the environ-\nments, actions should be inferred. Basically, when perform-\ning a step on an environment, there should be one times\ninference. Previous DDRL methods usually maintain an en-\nvironment for an actor, where action inference is performed\nindependently from other actors and environments. With\nthe increasing number of environments to collect trajec-\ntories, it is resource consuming especially only CPUs are\nused in the actor side. By putting the inference in the GPU\nside, the resources are also largely wasted because the batch\nsize of the inference is one. To cope with above problems,\nplenty of DDRL frameworks use an actor to manage several\nenvironments, and perform actions inference synchronized.\nFor example, APPO [38] and SEEDRL [9] introduce synchro-\nnization to collect states and distribute actions obtained by\nenvironments and actor policy, respectively.\nAsynchronous or synchronous DDRL. In regarding\nsynchronous based and asynchronous based DDRL algo-\nrithms, different methods share advantages and disadvan-\ntages. For asynchronous DDRL algorithms, the global policy\nusually does not need to wait all the trajectories or gradients,\nand data collection conventionally does not need to wait the\nlatest policy parameters. Accordingly, the data throughput\nof the whole DDRL system will be large. However, there\nexists a lag between the global policy and behavior policy,\nand such a lag is usually a trouble for on-policy based\nreinforcement learning algorithms. DDRL frameworks such\nas IMPALA [10] introduces V-trace, and GA3C [37] brings\nin small term ε to alleviate the problem, but those kinds\nof methods will be unstable when the lags are large. For\nsynchronous DDRL algorithms, synchronization among tra-\njectories or gradients is required before updating the policy.\nAccordingly, waiting time is wasted for actors or learners\nwhen one side is working. However, synchronization makes\nthe training stable, and it is easier to be implemented such\nas DPPO [34] and DDPPO [41].\nOthers. Usually, multiple actors can be implemented\nwith no data exchange, because their jobs, i.e., trajectory col-\nlection, can be independent. As for learners, most methods\nonly maintain one learner, which will be enough due to lim-\nited model size and especially the limited trajectory batch\nsize. However, large batch size is claimed to be important\nfor complex games [6], and accordingly multiple learners\nbecome necessary. In the multiple learners case, usually a\nsynchronization should be performed before updating the\nglobal policy network. Generally, a sum operation can han-\ndle the synchronization, but it is time consuming when the\nlearners are distributed in multiple machines. An optimal\nchoice is proposed in [41], where ring allreduce operation\ncan nicely deal with the synchronization problem, and an\nimplementation of [41] is easy by using toolbox such as\nHorovod [11]. On the other hand, when the model size is\nlarge and a GPU can not load the whole model, a parameter-\nserver framework [8], [33] based learner can be a choice,\nwhich may be combined with the ring allreduce operation to\nhandle the large model size and large batch size challenge.\nBrief summary. Finally, when a DDRL algorithm is\nrequired, how to select a proper or efﬁcient method largely\nrely on the computing resources can be used, the policy\nmodel resource occupancy, and the environment resource\noccupancy. If there is only one machine with multiple CPU\ncores and GPUs, no extra communication is required except\nfor the data exchange between CPU and GPUs. But, if there\nare multiple machines, the data exchange should be con-\nsidered, which may be the bottleneck of the whole system.\nWhen the policy model is large, exchange of model between\nmachines is time consuming, so methods such as SEEDRL\n[9] is proper due to only states and actions being exchanged.\nHowever, if the policy model is small, frequently exchange\nthe trajectories will be time consuming, and methods such\nas DDPPO [41] will be a choice. When the environment re-\nsource occupancy is large, massive resources will be used to\nstart-up environments, and limited GPUs maybe competent\nat the policy updating. Accordingly, DDRL methods such as\nIMPALA [10] will be suitable because a high data through-\nput can be obtained. Finally, there may be no best DDRL\nmethods for any learning environments, and researchers can\nchoose one that best suits their tasks.\n3.3\nAgents Cooperation Types\nWhen confronting single agent reinforcement learning, pre-\nvious DDRL algorithms can be easily used. But, when there\nare multiple agents, distributed multi-agent reinforcement\nlearning algorithms are required to train multiple agents si-\nmultaneously. Accordingly, previous DDRL algorithms may\nneed to be revised to handle the multiple agents case. Based\non current training paradigms for multi-agent reinforcement\nlearning, agents cooperation types can be classiﬁed into two\ncategories, i.e., independent training and joint training, as\nshown in Fig. 13. Usually, an agents manager is added to\ncontrol all the agents in a game. Independent training trains\neach agent by considering other learning agents as part of\nthe environment, and joint training trains all the agents as a\nwhole by using typical multi-agent reinforcement learning\nalgorithms.\n3.3.1\nIndependent training\nIndependent training makes a n agents training as a training\nof n independent training, and accordingly previous DDRL\nalgorithms can be used with a few modiﬁcations. The agents\nmanager is mainly used to bring other agents’ information,\ne.g., actions, into current DDRL training of an agent, because\ndynamic of an environment should be driven by all the\nagents. Considering the requirement of agents cooperation,\nindependent training makes more contribution on how to\npromote cooperation among independent agents.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n8\nSingle Player Single Agent DDRL\nAgents Manager\nAgent n\nAgent 1\n…\nAs a whole like a \nsingle agent\nJoint \ntraining\nIndependent\ntraining\nn agents = n copy \nof single agent\nFig. 13. Basic framework of agents training.\nJaderberg et al. [42] proposed FTW agents for Quake III\nArena in Capture the Flag (CTF) mode, where several agents\ncooperate to ﬁght another camp. To train scalable agents\nthat can cooperate with any other agents even for unseen\nagents, the authors train agents independently, where a\npopulation of independent agents are trained concurrently,\nwith each participating thousands of parallel matches. To\nhandle thousands of parallel environments, an IMPALA [10]\nbased framework is used1. As for the cooperation problem,\nthe authors design rewards based on several marks between\nthe agents cooperated, so as to promote the emergence of\ncooperation. More speciﬁcally, All the agents share the same\nﬁnal global reward, i.e., win or lose. Besides, intermediate\nrewards are learned based on several events that consider-\ning teammates’ action such as teammates capturing the ﬂag,\nand teammate picking up the ﬂag.\nBerner et al. [6] proposed OpenAI Five for Dota2, where\nﬁve heroes cooperate together to ﬁght another cooperated\nﬁve heroes. In their AI, each hero is modeled as an agent and\ntrained independently. To deal with large parallel environ-\nments so as to generate a batch size of more than a million\nof time steps, a SEEDRL [9] framework is used. Unlike [42]\nutilizing different policy networks for different agents, Ope-\nnAI Five uses the same policy for different agents, which\nmay promote the emergence of cooperation. The actions\ndifferences lie in the features designing, where different\nagents in Dota2 share almost the same features but with\nspeciﬁc features such as hero ID. Finally, similar with [42]\nthat designs rewards to promote cooperation, the authors\nuse a weighted sum of individual and team rewards, which\nare given by following experience of human players, e.g.,\ngaining resources, and killing enemies.\nYe et al. [36] proposed JueWu2 for Honor of Kings, which\nis a similar game compared to Dota2 but played in mobile\ndevices instead of computer devices. Like in [6], a SEEDRL\n[9] framework is adopted. Besides, the authors also use the\nsame policy for different agents as in [6]. The policy network\nis kind of different, where ﬁve value heads are used due\nto a deeper consideration of the game characteristics. Key\ndifference between [6] is the training paradigm used to scale\nto a large number of heroes, which is not the main scope of\nthis paper, and researchers can refer to the original paper for\nmore details.\nZha et al. [43] proposed DouZero for DouDiZhu, where\n1. Mainly based on their codes released.\n2. A recognized name.\na Landlord agent and two Peasant agents are confronting\nfor a win. Three agents using three policy networks are\ntrained independently like in [42]. A Gorila [8] based DDRL\nalgorithm is used for the three agents learning in a single\nserver. Cooperation between the Peasants agents emerges\nwith the increasing of training epochs.\nBaker et al. [44] proposed multi-agent autocurricula for\ngame hide-and-seek to study the emergent tool use. Like\nin [6], a SEEDRL [9] framework is used, and the same\npolicy for different agents are used for training. Besides,\nthe authors test using distinct policies for different agents,\nshowing similar results but reduced sample efﬁciency.\n3.3.2\nJoint training\nJoint training trains all the agents as a whole using typical\nmulti-agent reinforcement learning algorithms like a single\nagent. The difference is the trajectories collected, which have\nall the agents’ data instead of just an agent. The agents\nmanager can be designed to handle multi-agent issues, such\nas communication, and coordination, to further accelerate\ntraining. However, current multi-agent DDRL algorithms\nonly consider a simple way, i.e., actor parallelization to col-\nlect enough trajectories. Accordingly, most previous DDRL\nalgorithms can be easily implemented.\nThe implementation of QMIX [45], a popular Q value\nfactorisation based multi-agent reinforcement learning al-\ngorithm, is implemented using multi-processing to interact\nwith the environments [46]. Another example is the RLlib\n[13], a part of the open source Ray project [12], which makes\nabstractions for DDRL and implements several jointly\ntrained multi-agent reinforcement learning algorithms, e.g.,\nQMIX and PPO with centralized critic. Generally speaking,\njoint training is similar with single agent training in the\nﬁeld of DDRL, but consideration of parallelized training\nfor issues such as communication and coordination among\nagents, the training speed may further accelerated.\n3.3.3\nDiscussion\nAs for independent training, even though different agents\nare trained independently, different methods take into ac-\ncount problems such as the feature engineering, and reward\nreshaping, so as to promote cooperation. Since different\nagents are trained by making other agents as part of the\nenvironment, conventional DDRL algorithms can be used\nwithout many modiﬁcations. From the successful agents\nsuch as OpenAI Five and JueWu, we can see that SeedRL or\nits revised versions are a good choice. As for joint training,\nit is far from satisfactory, because there is a huge room to\nimprove parallelism among agents by properly considering\nthe multi-agent issues such as communication when design-\ning actors and learners.\n3.4\nPlayer Evolution Types\nIn most case, we have no opponents to drive the capacity\ngrowth for a player3. To handle such a problem, the player\nusually ﬁghts against itself to increase its ability, such as\nAlphaGo [1], which uses DDRL and self-play for super-\nhuman AI learning. Based on current learning paradigms\n3. Here player means the a side for a game, which may controls one\nagent like Go or multiple agents like Dota2\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n9\nSingle Player Single Agent DDRL\nPlayer 1\nt=T\nPlayer 1 \nt=1\n…\nMaintain n players and \ntheir past versions\nPopulation-play \nbased\nSelf-play \nbased\nPlayer n \nt=1\nPlayer n \nt=T\n…\n…\n…\nMaintain a player and its \npast versions\nPlayers Manager\nFig. 14. Basic framework of player iteration.\nfor players evolution, current methods can be classiﬁed\ninto two categories, i.e., self-play and population-play, as\nshown in Fig 14. To maintain the players for evolution, a\nplayers manager is required for the DDRL algorithms for\none or multiple players. Self-play maintains a player and its\npast versions, whereas, population-play maintains several\ndistinct players and their past versions.\n3.4.1\nSelf-play based evolution\nSelf-play becomes a popular tool since the success of Al-\nphaGo series [1], [2], [47], which train a player by ﬁghting\nagainst itself. In an iteration or called generation of the\nplayer, the current version is trained based on previous\nDDRL algorithms by using one or some of its previous\nversions as opponents. The players manager decides which\nprevious versions are used as the opponents.\nIn JueWu [36] developed for Honor of Kings, a naive self-\nplay is used for two players (each controls ﬁve agents) using\nthe same policy. A SEEDRL [9] DDRL algorithm is used, and\nthe self-play is used in the ﬁxed lineup and random lineup\nstages for handling large hero pool size. Players trained\nfor hide-and-seek [44] is similar with JueWu [36], where a\nSEEDRL [9] DDRL algorithm and a naive self-play are used\nto prove the multi-agent auto-curricula. Another similar\nexample is Suphx [5] proposed for Mahjong, which uses\nself-play for a player to confront the other three players (use\nthe same policy). As for the DDRL algorithm, an IMPALA\n[10] framework is applied for training each generation.\nIn OpenAI Five [6] designed for Dota2, a more complex\nself-play is used for two players (each controls ﬁve agents)\nusing the same policy. In each generation, instead of ﬁghting\nagainst the current generation like naive self-play, the player\ntrains the current policy against itself for 80% of games, and\nagainst its previous versions for 20% of games. Speciﬁcally,\na dynamic sampling system is designed to select the past\nversions based on their dynamically generated quality score,\nwhich claims to alleviate cyclic strategies problem. As for\nthe basic DDRL algorithm, a SEEDRL [9] framework is used\nfor all the generations of players training.\n3.4.2\nPopulation-play based evolution\nPopulation-play can be seen as an advanced self-play, where\nmore than one player and their past generations should be\nmaintained for players evolution. It can be used for several\ncases: the policy used is different for different players (e.g.,\na Landlord and two Peasant players in DouZero); some\nauxiliary players are introduced for the target player to\novercome game-theoretic challenges (e.g., main exploiter\nand league exploiter players in AlphaStar); parallel players\nare used with consistent roles to support concurrent training\nand to alleviate unstabitily of self-play (e.g., populations in\nFTW).\nIn DouZero [43] designed for DouDiZhu, a Landlord and\ntwo Peasant players are trained simultaneously, where their\ncurrent generations ﬁght against each other to collect trajec-\ntories and to train the players. The basic DDRL algorithm is\nGorila [8] running on a single machine, based on which, all\nthe three players are trained asynchronously.\nIn AlphaStar [7] developed for StarCraft, the players\nmanager maintains three main players for three different\nraces, i.e., Protoss, Terran, and Zerg. Besides, for each race,\nseveral auxiliary players are designed, i.e., one main ex-\nploiter player and two league exploiter players. Those aux-\niliary players help the main player ﬁnd out weaknesses and\nhelp all the players ﬁnd systemic weaknesses. The authors\nclaim that using such a population addresses the complexity\nand game-theoretic challenges of the StarCraft. As for the\nDDRL algorithm, SEEDRL [9] is utilized to support large\nsystem training. Commander [48] is similar with [7], and\nmore exploiter players are used.\nIn FTW [42] designed for CTF, the players manager\nmaintains a population of players, who cooperate and con-\nfront with each other to learn scalable bots. The positions\nof all the players are the same, and a population-based\ntraining method is designed to adjust players with worse\nperformance, so as to improve the ability of all the players.\nAs for the basic DDRL algorithm, an IMPALA [10] method is\nused to have large data throughput to train tens of players.\n3.4.3\nDiscussion\nSelf-play has a long history in multi-agent settings, where\nearly work explored it in genetic algorithms [49]. It becomes\nvery popular since the success of AlphaGo series [1], [2] and\nthen be used for AI systems such as Libratus [50], DeepStack\n[51] and OpenAI Five [6]. Combining DDRL, it can be used\nto solve very complex games. On the other side, population-\nplay can be seen as an advanced self-play, which maintains\nmore players to achieve ability improvement. Current works\nuse population-play to accelerate training, overcome game-\ntheoretic challenges, or just handle the problem that requires\ndistinct players. Compared with self-play, population-play\nis more ﬂexible, and can handle diverse situations, whereas,\nself-play is easy to be implemented, and has proved its\npotential in complex games. So, there is no conclusion\nwhich one is better, and researchers can select self-play or\npopulation-play DDRL based on their request.\n4\nTYPICAL DISTRIBUTED DEEP REINFORCEMENT\nLEARNING TOOLBOXES\nDDRL is important for complex problems using reinforce-\nment learning as solvers, and several useful toolboxes have\nbeen released to help researchers reduce development costs.\nIn this section, we analysis several typical toolboxes, hoping\nto give a clue when researchers are making a selection\namong them.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n10\n4.1\nTypical Toolboxes\nRay [12] is a distributed framework consisting of two main\nparts, i.e., a system layer to implement tasks scheduling and\ndata management, and an application layer to provide high-\nlevel API for various applications. Using Ray, researchers\ncan easily implement a DDRL method without considering\nthe nodes/machines communications and how to sched-\nule different calculations. The API is user-friendly, and by\nadding @ray.remote, users can obtain a remote function\nthat can be executed in parallel. A RLLib [13] package\nis speciﬁcally introduced to handle reinforcement learning\nproblems such as A3C, APEX and IMPALA. Furthermore,\nseveral built-in multi-agent DDRL algorithms are provided\nsuch as QMIX [45] and MADDPG [52].\nAcme [53] is designed to enable distributed reinforce-\nment learning to promote development of novel RL agents\nand their applications. It involves many separate (parallel)\nacting, learning, as well as diagnostic and helper processes,\nwhich are key building blocks for a DDRL system. One of\nthe main contributions is the in-memory storage system,\ncalled Reverb, which is a high-throughput data system that\nare suitable for experience replay based reinforcement learn-\ning algorithms. With the aim of supporting agents at various\nscales of execution, plenty of mainstream DDRL algorithms\nare implemented, i.e., online reinforcement learning algo-\nrithms such as Deep Q-Networks [28], R2D2 [35] and IM-\nPALA [10], ofﬂine reinforcement learning such as behavior\ncloning and TD3 [54], imitation learning such as adversarial\nimitation learning [55] and soft Q imitation learning [56].\nTianshou [57] is a highly modularized Python library\nthat uses PyTorch for DDRL. Its main characteristic is the\ndesign of building blocks that support more than 20 classic\nreinforcement learning algorithms with distributed version\nthrough a uniﬁed interface. Since Tianshou focuses on small-\nto medium-scale applications of DDRL with only parallel\nsampling, it is a lightweight platform that is research-\nfriendly. It is claimed that Tianshou is easy to install, and\nusers can apply Pip or Conda to accomplish installation on\nplatforms covering Windows, macOS and Linux.\nTorchBeast [58] is another DDRL toolbox that bases on\nPytorch to support for fast, asynchronous and parallel train-\ning of reinforcement learning agents. The authors provide\ntwo versions, i.e., a pure-Python MonoBeast and a multi-\nmachine high-performance PolyBeast with several parts\nbeing implemented with C++. Users only require Python\nand Pytorch to implement DDRL algorithms. In the toolbox,\nIMPALA is supported and tested with the classic Atari suite.\nMALib [59] is a scalable and efﬁcient computing frame-\nwork for population-based multi-agent reinforcement learn-\ning algorithms. Using a centralized task dispatching model,\nit supports self-generated tasks and heterogeneous policy\ncombinations. Besides, by abstracting DDRL algorithms us-\ning Actor-Evaluator-Learner, a higher parallelism for learn-\ning and sampling is achieved. The authors also claimed to\nhave an efﬁcient code reuse and ﬂexible deployments due\nto the higher-level abstractions of multi-agent reinforcement\nlearning. In the released code, several popular reinforcement\nlearning environments such as Google Research Football\nand SMAC are supported and typical population based\nalgorithms such as policy space response oracle (PSRO) [60]\nand Pipeline-PSRO [61] are implemented.\nSeedRL [9] is a scalable and efﬁcient deep reinforcement\nlearning toolbox, as described in Section 3.2.1. Generally, it is\nveriﬁed on the tensor processing unit (TPU) device, which is\na special chip customized by Google for machine learning.\nTypical DDRL algorithms are implemented, e.g., IMPALA\n[10], and R2D2 [35], which are tested on four classical\nenvironments, i.e., Arati, DeepMind lab, Google research\nfootball and Mujoco. Distributed training is supported using\ncloud machine learning engine of Google.\n4.2\nDiscussions\nBefore comparing different kinks of toolboxes, we want to\nclaim that there are no best DDRL toolboxes for any re-\nquirements, but the most suitable one depending on speciﬁc\ngoals.\nTianshou and TorchBeast are lightweight platforms that\nsupport several typical DDRL algorithms. Users can easily\nuse and revise the released codes for developing reinforce-\nment learning algorithms with the PyTorch deep learning\nlibrary. The user-friendly features make these toolboxes\npopular. However, even though those toolboxes are highly\nmodularized, the scalability to large number of machines for\nperforming large learner parallel and actor parallel are not\ntested, and bottleneck may appear with increasing number\nof machines.\nRay, Acme and SeedRL are relatively large toolboxes\nthat can theoretically support any DDRL algorithms with\ncertain modiﬁcations. Using their open projects, users can\nutilize multiple machines to implement high data through-\nput DDRL algorithms. Moreover, multiple agents training,\nand multiple players evolution can be achieved, such as\nfor AlphaStar. However, the modiﬁcations are not easy\nwhen revising the DDRL algorithms due to the code nest-\ning, especially for complex functions such as self-play and\npopulation-play.\nMALib is similar with Ray, Acme and SeedRL, which\nis a specially designed DDRL toolbox for population-based\nmulti-agent reinforcement learning. With their APIs, users\nmay easily implement population based multi-agent rein-\nforcement learning algorithms such as ﬁctitious self-play\n[62] and PSRO. Even though experiments for large number\nof machines are not tested, this toolbox is fully functional\n(APIs provided) for various requirements of DDRL algo-\nrithms from single player single agent DDRL to multiple\nplayers multiple agents DDRL.\nIn summary, current DDRL toolboxes provide a good\nsupport for DDRL algorithms, and several typical testing\nenvironments are applied for performance validation. How-\never, those DDRL toolboxes are either lightweight or heavy,\nand not tested for complex games. In the following, we will\ndesign a new toolbox, which focuses on multiple players\nand multiple agents DDRL training on complex games.\n5\nA MULTI-PLAYER MULTI-AGENT REINFORCE-\nMENT LEARNING TOOLBOX\nIn this section, we open a multi-player multi-agent rein-\nforcement learning toolbox, M2RL, to support populations\nof players (with each may control several agents) for com-\nplex games, e.g., Wargame [63]. Noted that this project is on\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n11\nPlayers Manager\nPlayer-1\nPlayer-2\nPlayer-N\nPlayer-1 Population\nPlayer-2 Population\nPlayer-N Population\nExperience \nBuffer\nLearner\nActor\nEnv\nMy agent\nAnother \nplayer’s \nagent\nExperience \nBuffer\nLearner\nActor\nEnv\nMy agent\nAnother \nplayer’s \nagent\nExperience \nBuffer\nLearner\nActor\nEnv\nMy agent\nAnother \nplayer’s \nagent\n… …\n… …\nEvaluation->Player-X vs (Player-1,……,Player-N)\nChoosing Algorithm-> Player-X’s opponent chosen probabilities\nMy agent\nMy agent\nMy agent\nMy agent\nMy agent\nMy agent\nFig. 15. Basic framework of the proposed multi-player multi-agent reinforcement learning toolbox M2RL.\nPlayer1\nStep Sampling\nStep Sampling\nPlayers Manager\nEvaluate Manager\nPayoff Data\nAll Players Data\nChoosen Player Data\nPlayer test\nOpponent Choosing\nActor\nStep Trajectories\nActor\nActor\nExperience Buffer\nBatch Sampling\nLearner\nUnfinished Buffer\nFinished Buffer\nEnv\nEnv\nEnv\nOur Obs\nOpponent Obs\nReward Engineering\nobservation\nParameters\nModel\nTrain\nBatch Trajectories\nParameter Server\nParameters Pulling\naction\nStep Sampling\nAgent\nAgent\nAgent\naction\nobservation\nStep Trajectories\nParameters\nParameter \nBuffer\nSave/Load Player\nPlayer Data\nAll Experience\nWin Experience\nEpisode \nTrajectories\nPlayer Data\nMask Engineering\nFeature Engineering\nModel\nInference\nAction Engineering\nStep Counter\nProbability/Value\nmask\nfeature\nOpponent Actor\nModel\nInference\nfeature\n+mask\nAgent\nAgent\nAgent\nProbability\n/Value\nPlayer2\nSave/Load Player\nPlayer Data\nPlayerN\nSave/Load Player\nFig. 16. Speciﬁc details of the proposed multi-player multi-agent reinforcement learning toolbox M2RL.\ngoing, so the main purpose is a preliminary introduction,\nand we will continue to improve this project. Hyperlink of\nthe project is m2rl-V1.0.\n5.1\nOverall Framework\nThe overall framework is shown in Fig 15. Each player, con-\nsisting one or multiple agents, has three key components:\nlearner, actor and experience buffer. The multiple concur-\nrently executed actors produce data for learners, which use\nthe current player and other players as opponents based on\nthe choice of players manager. The experience buffer is used\nto store trajectories of the player to support asynchronous or\nsynchronous training. The learner for each player is used to\nupdate parameters of the player and send parameters to the\nactors. Apart from the above basic factors, Players manager\nmaintains self-play and population-play, which has two key\nparts: evaluating players and choosing opponent players for\neach player.\nMore speciﬁc details of M2RL is shown in Fig 16. To\nmake M2RL easy to be used for complex games, we design\neach parts in a relatively ﬂexible manner.\n•\nThe players manager evaluates all saved players (in-\ncluding their past versions) using their confrontation\nresults, based on which, various opponents selection\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n12\n300\n500\n700\n900\n1100\n1300\n1500\n1700\n5.5\n16.5\n27.5\n38.5\n49.5\n60.5\n71.5\n82.5\n93.5\n104.5\nElo Rating\nTraining Hours\nElo_Red\nM2RL\nDemo\nKnowledge_1\nKnowledge_2\nKnowledge_3\n300\n500\n700\n900\n1100\n1300\n1500\n1700\n5.5\n16.5\n27.5\n38.5\n49.5\n60.5\n71.5\n82.5\n93.5\n104.5\nElo Rating\nTraining Hours\nElo_Blue\nM2RL\nDemo\nKnowledge_1\nKnowledge_2\nKnowledge_3\nFig. 17. Elo results of the trained AI bots (red and blue players) based on M2RL. Knowledge 1, Knowledge 2 and Knowledge 3 are three\nprofessional level AI bots. Demo is an AI with a strategy to select the highest priority action when it is possible.\nmethods can be implemented to promote players\nevolution, e.g., revised self-play in OpenAI Five [6],\nand prioritized ﬁctitious self-play in AlphaStar [7].\n•\nEach player maintains its own learner, actor and\nexperience buffer, making distinct players training\npossible, e.g., red and blue players in Wargame [63].\nConsidering that the game is complex with different\nobservation and action spaces compared to OpenAI\ngym, feature engineering and mask engineering are\nused in the framework. Besides, experience buffer\nis revised to change unﬁnished buffer to ﬁnished\nbuffer, which is very useful for asynchronous multi-\nagent cooperation [64].\n•\nAll the codes are based on the user-friendly frame-\nwork Ray, which is easy to be deployed, revised\nand used. More speciﬁcally, we can make full use\nof computing resources by segmenting a GPU to\nseveral parts and assigning each part to different\ntasks, which is important for complex games under\nlimited computing resources.\n5.2\nA Case\nWargame, a complex game like Dota2 and StarCraft, is a\npopular testing environment for verifying artiﬁcial intel-\nligence [27]. In a Wargame map4, the red player controls\nseveral ﬁghting units to confront the blue player who also\ncontrols several units. The game is asymmetric because\nplayers have distinct strategies space, and usually the blue\nplayer has more forces, while the red player has vision ad-\nvantage. Please refer to [63] for more details of the Wargame.\nWe can naturally model Wargame as a two players mul-\ntiple agents problem, where each ﬁghting unit is regarded\nas an agent. To train two AI bots for the red and blue\nplayers, respectively, we use several widely adopted settings\nlike in OpenAI Five [6], JueWu [36] and AlphaStar [7],\ne.g., shared PPO policy for each agent, dual-clip for the\nPPO, and prioritized ﬁctitious self-play. Each player trains\nits bot using about 200,000 games, and uses 9,500 games\nfor the players manager to evaluate each generation of the\nplayer. The computing resources used here are: 2×Intel(R)\nXeon(R) Gold 6240R CPU @ 2.40GHz, 4× NVIDIA GeForce\n4. wargame.ia.ac.cn, ID=2010431153\nRTX 2080 Ti, and 500GB memory. With above resources,\nthe training lasts for ﬁve days, and we ﬁnally obtain 20\ngenerations for each player. To evaluate the performance of\nthese bots, we use the build-in demo agent as baseline, and\nbring in three professional level AI bots designed by teams\nwho have studied Wargame for several years, represented\nas Knowledge 1, Knowledge 2, and Knowledge 3, respec-\ntively. It should be noted that those professional AI bots do\nnot participated in training. Similar with the evaluation for\nAlphaGo [1] and AlphaStar [7], we use Elo as metrics. The\nresults are shown in Fig 17.\nFrom Fig 17, it can be seen that with the increasing\nof players evolution, the learned policy for each player is\nbecoming stronger. Since Wargame is a complex game and\nprevious toolboxes are not speciﬁcally designed for complex\ngames, the comparison is not performed due to a hard\ntransfer on these toolboxes. Overall, the results show the\nability of the proposed M2RL to some extent. Since this\nproject is an on going item, so the main purpose of this\npart is an introduction, and we will continue improve this\nproject (m2rl-V1.0).\n6\nCHALLENGES AND OPPORTUNITIES\nPlenty of DDRL algorithms and toolboxes are proposed,\nwhich largely promote the study of reinforcement learning\nand its applications. We think current methods still suffer\nseveral challenges, which may be the future directions.\nFirstly, current methods rarely consider accelerating com-\nplex reinforcement learning algorithms, such as those study-\ning exploration, communication and generalization prob-\nlems. Secondly, current approaches mainly use ring allre-\nduce or parameter and server for learners, which seldom\nhandle large model size and batch size situations simulta-\nneously. Thirdly, self-play or population-play are important\nmethods for multiple players and multiple agents training,\nwhich are also ﬂexible without strict restrictions, but deeper\nstudy is deﬁcient. Fourthly, several famous DDRL toolboxes\nare developed, but none of them is veriﬁed with large scale\ntraining, e.g., tens of machines for complex games.\nDDRL with advanced reinforcement learning algo-\nrithms. The research and application of reinforcement learn-\ning show explosive growth since the success of AlphaGo.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n13\nNew topics emerge such as hierarchical deep reinforce-\nment learning, model based reinforcement learning, multi-\nagent reinforcement learning, off-line reinforcement learn-\ning, meta reinforcement learning [16], [18], but DDRL meth-\nods rarely consider those new research area. Distributed\nimplementation is kind of engineering but not naive. For ex-\nample, when considering information communication for a\nmulti-agent reinforcement learning algorithm, agents man-\nager should reasonably parallelize agents communication\ncalculation so as to improve data throughput. Accordingly,\nhow to accelerate advanced reinforcement learning algo-\nrithms with distributed implementation is an important\ndirection.\nDDRL with big model size and batch size. With the\nsuccess of foundation models in the ﬁeld of computer vision\nand natural language processing, big model in reinforce-\nment learning will be a direction [27]. This requires that\nthe DDRL methods can handle big model size and batch\nsize situations simultaneously. Currently, the learners in\nDDRL are based on techniques such as ring allreduce or\nparameter-server, with each has its advantages. For exam-\nple, parameter-server can store big model in different GPUs,\nand ring allreduce can quickly exchange gradients between\ndifferent GPUs. However, none of them are applied for\nbig model size and batch size in reinforcement learning.\nAccordingly, how to combine these techniques to ﬁt DDRL\nalgorithms for efﬁcient training is a future direction.\nSelf-play and population-play based DDRL methods.\nSelf-play and population-play are mainstream reinforce-\nment learning agents evolution methods, which are widely\nadopted in current professional human-level AI systems,\ne.g., OpenAI Five [6] and AlphaStar [7]. Generally, self-\nplay and population-play have no strict restrictions on the\nplayers, which means a player can ﬁght against any past\nversions for the same player or different players. Those\nheuristic design makes exploring the best conﬁguration a\nhard question, which also makes designing templates for\na toolbox a tricky problem. In the future, self-play and\npopulation-play based DDRL methods are worthy of further\nstudy, e.g., adaptively ﬁnding out the best conﬁguration.\nToolboxes construction and validation. Several famous\nscientiﬁc research institutions such as DeepMind, OpenAI,\nand UC Berkeley have released toolboxes to support DDRL\nmethods. Most of them use gym to test the performance,\nsuch as data throughput, and linearity. However, environ-\nments in gym are relatively small compared with envi-\nronments in real world applications. On the other hand,\nmost of the testing uses one or two nodes/machines with\nlimited numbers of CPU and GPU devices, making the\ntesting insufﬁcient to discover bottleneck of the toolboxes.\nAccordingly, even though most current DDRL toolboxes\nare highly modularized, the scalability to large number of\nmachines for performing large learner parallel and actor\nparallel for complex environments are not fulled tested.\nFuture bottlenecks of the toolboxes may be discovered with\nlarge testing.\n7\nCONCLUSION\nIn this paper, we have surveyed representative distributed\ndeep reinforcement learning methods. By summarizing key\ncomponents to form a distributed deep reinforcement learn-\ning system, single player single agent distributed deep\nreinforcement learning methods are compared based on\ndifferent types of coordinators. Furthermore, by introducing\nagents cooperation and players evolution, multiple players\nmultiple agents distributed deep reinforcement learning\napproaches are elaborated. To support easy codes imple-\nmentation, some popular distributed deep reinforcement\nlearning toolboxes are introduced and discussed, based on\nwhich, a new multiple players and multiple agents learning\ntoolbox is developed, hoping to assist learning for complex\ngames. Finally, we discuss the challenges and opportunities\nof this exciting ﬁled. Through this paper, we hope it be-\ncomes a reference for researchers and engineers when they\nare exploring novel reinforcement learning algorithms and\nsolving practical reinforcement learning problems.\nREFERENCES\n[1]\nD. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre et al.,\n“Mastering the game of go with deep neural networks and tree\nsearch,” Nature, vol. 529, pp. 484–489, 2016.\n[2]\nD. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang\net al., “Mastering the game of go without human knowledge,”\nNature, vol. 550, pp. 354–359, 2017.\n[3]\nY. Yu, “Towards sample efﬁcient reinforcement learning,” in In-\nternational Joint Conference on Artiﬁcial Intelligence, 2018, pp. 5739–\n5743.\n[4]\nX. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai et al., “Pre-trained models for\nnatural language processing: A survey,” Science China Technological\nSciences, pp. 1–26, 2020.\n[5]\nJ. Li, S. Koyamada, Q. Ye, G. Liu, C. Wang et al., “Suphx: mastering\nmahjong with deep reinforcement learning,” arXiv:2003.13590v2,\n2020.\n[6]\nC.\nBerner,\nG.\nBrockman,\nB.\nChan,\nV.\nCheung,\nP.\nDebiak\net al., “Dota 2 with large scale deep reinforcement learning,”\narXiv:1912.06680v1, 2019.\n[7]\nO.\nVinyals,\nI.\nBabuschkin,\nW.\nM.\nCzarnecki,\nM.\nMathieu,\nA. Dudzik et al., “Grandmaster level in starcraft ii using multi-\nagent reinforcement learning,” Nature, vol. 575, pp. 350–354, 2019.\n[8]\nA. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon et al.,\n“Massively parallel methods for deep reinforcement learning,”\nin Deep Learning Workshop, International Conference on Machine\nLearning, 2015.\n[9]\nL. Espeholt, R. Marinier, P. Stanczyk, K. Wang et al., “Seed rl:\nScalable and efﬁcient deep-rl with accelerated central inference,”\nin International Conference on Learning Representations, 2020.\n[10] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih et al.,\n“Impala: scalable distributed deep-rl with importance weighted\nactor-learner architectures,” arXiv:1802.01561, 2018.\n[11] A. Sergeev and M. Del Balso, “Horovod: fast and easy distributed\ndeep learning in tensorﬂow,” arXiv:1802.05799, 2018.\n[12] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw et al.,\n“Ray: A distributed framework for emerging {AI} applications,”\nin 13th {USENIX} Symposium on Operating Systems Design and\nImplementation ({OSDI} 18), 2018, pp. 561–577.\n[13] E. Liang, R. Liaw, P. Moritz, R. Nishihara, R. Fox et al., “Rllib: Ab-\nstractions for distributed reinforcement learning,” in International\nConference on Machine Learning, 2018.\n[14] M. R. Samsami and H. Alimadad, “Distributed deep reinforcement\nlearning: An overview,” in Reinforcement Learning Algorithms: Anal-\nysis and Applications, 2021.\n[15] J. Czech, “Distributed methods for reinforcement learning survey,”\nin Reinforcement Learning Algorithms: Analysis and Applications,\n2021.\n[16] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A.\nBharath, “Deep reinforcement learning: A brief survey,” IEEE\nSignal Processing Magazine, vol. 34, no. 6, pp. 26–38, 2017.\n[17] T. M. Moerland, J. Broekens, and C. M. Jonker, “Model-based\nreinforcement learning: A survey,” arXiv:2006.16712, 2020.\n[18] S. Gronauer and K. Diepold, “Multi-agent deep reinforcement\nlearning: a survey,” Artiﬁcial Intelligence Review, vol. 55, pp. 895–\n943, 2022.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n14\n[19] Y. Yang and J. Wang, “Multi-agent deep reinforcement learning: a\nsurvey,” arXiv:2011.00583v3, 2021.\n[20] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A.\nBharath, “Demystifying parallel and distributed deep learning: An\nin-depth concurrency analysis,” Ben-Num, Tai and Torsten, Hoeﬂer,\nvol. 52, no. 4, pp. 1–43, 2020.\n[21] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang et al., “Terngrad: Ternary\ngradients to reduce communication in distributed deep learning,”\nin Advances in Neural Information Processing Systems, 2017, pp.\n1509–1519.\n[22] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin et al., “Large\nscale distributed deep networks,” in Advances in Neural Information\nProcessing Systems, 2012, pp. 1232–1240.\n[23] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen et al.,\n“Tensorﬂow: Large-scale machine learning on heterogeneous dis-\ntributed systems,” https://arxiv.org/abs/1603.04467, 2016.\n[24] T. Ben-Nun and T. Hoeﬂer, “Demystifying parallel and distributed\ndeep learning: An in-depth concurrency analysis,” ACM Comput-\ning Surveys, vol. 52, no. 4, pp. 1–43, 2020.\n[25] J. Park, S. Samarakoon, A. Elgabli, J. Kim, M. Bennis et al.,\n“Communication-efﬁcient and distributed learning over wireless\nnetworks: principles and applications,” Proceedings of the IEEE, vol.\n109, no. 5, pp. 796–819, 2021.\n[26] T.-C. Chiu, Y.-Y. Shih, A.-C. Pang, C.-S. Wang, W. Weng et al.,\n“Semisupervised distributed learning with non-iid data for aiot\nservice platform,” IEEE Internet of Things Journal, vol. 7, no. 10, pp.\n9266–9277, 2020.\n[27] Q. Yin, J. Yang, K. Huang, M. Zhao, W. Ni et al., “Ai in human-\ncomputer gaming: techniques, challenges and opportunities,”\narXiv:2111.07631v2, 2022.\n[28] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, and J. Veness,\n“Human-level control through deep reinforcement learning,” Na-\nture, vol. 518, pp. 529–533, 2015.\n[29] Y.\nBurda,\nH.\nEdwards,\nA.\nStorkey,\nand\nO.\nKlimov,\n“Exploration\nby\nrandom\nnetwork\ndistillation,”\nhttps://doi.org/10.48550/arXiv.1810.12894, 2018.\n[30] M.\nSamvelyan,\nT.\nRashid,\nC.\nS.\nd.\nWitt,\nG.\nFarquhar,\nN.\nNardelli\net\nal.,\n“The\nstarcraft\nmulti-agent\nchallenge,”\nhttps://doi.org/10.48550/arXiv.1902.04043, 2019.\n[31] M. Lanctot, E. Lockhart, J.-B. Lespiau, V. Zambaldi, S. Upadhyay\net al., “Openspiel: A framework for reinforcement learning in\ngames,” arXiv:1908.09453v6, 2020.\n[32] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap et al.,\n“Asynchronous methods for deep reinforcement learning,” in\nInternational Conference on Machine Learning, 2016, pp. 1928–1937.\n[33] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel et al.,\n“Distributed prioritized experience replay,” in International Confer-\nence on Learning Representations, 2018.\n[34] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel et al., “Ai in\nhuman-computer gaming: techniques, challenges and opportuni-\nties,” arXiv:1707.02286, 2017.\n[35] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney,\n“Recurrent experience replay in distributed reinforcement learn-\ning,” in International Conference on Learning Representations, 2019.\n[36] D. Ye, G. Chen, W. Zhang, S. Chen, B. Yuan et al., “Towards playing\nfull moba games with deep reinforcement learning,” in Neural\nInformation Processing Systems, 2020.\n[37] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, and J. Kautz, “Re-\ninforcement learning through asynchronous advantage actor-critic\non a gpu,” in International Conference on Learning Representations,\n2017.\n[38] A. Stooke and P. Abbeel, “Accelerated methods for deep reinforce-\nment learning,” arXiv:1803.02811v2, 2019.\n[39] A. V. Clemente, H. N. Castej´on, and A. Chandra, “Efﬁcient paral-\nlel methods for deep reinforcement learning,” arXiv:1705.04862v2,\n2017.\n[40] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal policy optimization algorithms,” arXiv:1707.06347,\n2017.\n[42] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever\net al., “Human-level performance in 3d multiplayer games with\npopulationbased reinforcement learning,” Science, vol. 364, pp.\n859–865, 2019.\n[41] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa et al., “Dd-\nppo: Learning near-perfect pointgoal navigators from 2.5 billion\nframes,” in International Conference on Learning Representations,\n2020.\n[43] D. Zha, J. Xie, W. Ma, S. Zhang, X. Lian, X. Hu, and J. Liu,\n“Douzero: Mastering doudizhu with self-play deep reinforcement\nlearning,” in International Conference on Machine Learning, 2021.\n[44] B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell et al., “Emer-\ngent tool use from multi-agent autocurricula,” arXiv:1909.07528,\n2019.\n[45] T. Rashid, M. Samvelyan, C. S. d. Witt, G. Farquhar, J. N. Foerster,\nand S. Whiteson, “Qmix: Monotonic value function factorisation\nfor deep multi-agent reinforcement learning,” in International Con-\nference on Machine Learning, 2018, pp. 4292–4301.\n[46] M. Samvelyan, T. Rashid, C. S. d. Witt, G. Farquhar, N. Nardelli\net al., “The starcraft multi-agent challenge,” arXiv:1902.04043v5,\n2019.\n[47] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai et al.,\n“A general reinforcement learning algorithm that masters chess,\nshogi, and go through self-play,” Science, vol. 362, pp. 1140–1144,\n2018.\n[48] X. Wang, J. Song, P. Qi, P. Peng, Z. Tang et al., “Scc: an efﬁcient\ndeep reinforcement learning agent mastering the game of starcraft\nii,” in International Conference on Machine Learning, 2021.\n[49] J. Paredis, “Coevolutionary computation,” Artiﬁcial life, vol. 2,\nno. 4, pp. 355–375, 1995.\n[50] N. Brown and T. Sandholm, “Superhuman ai for heads-up no-\nlimit poker: Libratus beats top professionals,” Science, vol. 359, pp.\n418–424, 2018.\n[51] M. Moravˇc´ık, M. Schmid, N. Burch, V. Lis´y, D. Morrill et al.,\n“Deepstack: Expert-level artiﬁcial intelligence in heads-up no-\nlimit poker,” Science, vol. 356, pp. 508–513, 2017.\n[52] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch,\n“Multi-agent actor-critic for mixed cooperative-competitive envi-\nronments,” arXiv:1706.02275v4, 2017.\n[53] M. W. Hoffman, B. Shahriari, J. Aslanides, G. Barth-Maron,\nN. Momchev et al., “Acme: A research framework for distributed\nreinforcement learning,” arXiv:2006.00979v2, 2020.\n[54] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approx-\nimation error in actor-critic methods,” in International Conference on\nMachine Learning, 2018, pp. 1587–1596.\n[55] J. Ho and S. Ermon, “Generative adversarial imitation learning,”\nin Advances in Neural Information Processing Systems, 2016.\n[56] S.\nReddy,\nA.\nD.\nDragan,\nand\nS.\nLevine,\n“Sqil:\nImitation\nlearning\nvia\nreinforcement\nlearning\nwith\nsparse\nrewards,”\nhttps://doi.org/10.48550/arXiv.1905.11108, 2019.\n[57] J. Weng, H. Chen, D. Yan, K. You, A. Duburcq et al., “Tianshou: A\nhighly modularized deep reinforcement learning library,” Journal\nof Machine Learning Research, vol. 23, no. 267, pp. 1–6, 2022.\n[58] H. K¨uttler, N. Nardelli, T. Lavril, M. Selvatici, V. Sivaku-\nmar et al., “Torchbeast: A pytorch platform for distributed rl,”\narXiv:1910.03552v1, 2019.\n[59] M. Zhou, Z. Wan, H. Wang, M. Wen, R. Wu et al., “Malib: A par-\nallel framework for population-based multi-agent reinforcement\nlearning,” arXiv:2106.07551, 2021.\n[60] P. Muller, S. Omidshaﬁei, M. Rowland, K. Tuyls, J. Perolat\net al., “A generalized training approach for multiagent learning,”\nhttps://doi.org/10.48550/arXiv.1909.12823, 2019.\n[61] S. Mcaleer, J. Lanier, R. Fox, and P. Baldi, “Pipeline psro: A scalable\napproach for ﬁnding approximate nash equilibria in large games,”\nin Advances in Neural Information Processing Systems, 2020.\n[62] J. Heinrich, M. Lanctot, and D. Silver, “Fictitious self-play in\nextensive-form games,” in International Conference on Machine\nLearning, 2015, pp. 805–813.\n[63] Q. Yin, M. Zhao, W. Ni, J. Zhang, and K. Huang, “Intelligent\ndecision making technology and challenge of wargame,” Acta\nAutomatica Sinica, vol. 47, 2021.\n[64] H. Jia, Y. Hu, Y. Chen, C. Ren, T. Lv et al., “Fever basketball: A\ncomplex, ﬂexible, and asynchronized sports game environment\nfor multi-agent reinforcement learning,” arXiv:2012.03204, 2020.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.MA"
  ],
  "published": "2022-12-01",
  "updated": "2022-12-01"
}