{
  "id": "http://arxiv.org/abs/2212.03419v1",
  "title": "JamPatoisNLI: A Jamaican Patois Natural Language Inference Dataset",
  "authors": [
    "Ruth-Ann Armstrong",
    "John Hewitt",
    "Christopher Manning"
  ],
  "abstract": "JamPatoisNLI provides the first dataset for natural language inference in a\ncreole language, Jamaican Patois. Many of the most-spoken low-resource\nlanguages are creoles. These languages commonly have a lexicon derived from a\nmajor world language and a distinctive grammar reflecting the languages of the\noriginal speakers and the process of language birth by creolization. This gives\nthem a distinctive place in exploring the effectiveness of transfer from large\nmonolingual or multilingual pretrained models. While our work, along with\nprevious work, shows that transfer from these models to low-resource languages\nthat are unrelated to languages in their training set is not very effective, we\nwould expect stronger results from transfer to creoles. Indeed, our experiments\nshow considerably better results from few-shot learning of JamPatoisNLI than\nfor such unrelated languages, and help us begin to understand how the unique\nrelationship between creoles and their high-resource base languages affect\ncross-lingual transfer. JamPatoisNLI, which consists of naturally-occurring\npremises and expert-written hypotheses, is a step towards steering research\ninto a traditionally underserved language and a useful benchmark for\nunderstanding cross-lingual NLP.",
  "text": "JamPatoisNLI: A Jamaican Patois Natural Language Inference Dataset\nRuth-Ann Armstrong\nJohn Hewitt\nChristopher Manning\nDepartment of Computer Science\nStanford University\n{ruthanna,johnhew,manning}@cs.stanford.edu\nAbstract\nJamPatoisNLI provides the ﬁrst dataset for nat-\nural language inference in a creole language,\nJamaican Patois.\nMany of the most-spoken\nlow-resource languages are creoles.\nThese\nlanguages commonly have a lexicon derived\nfrom a major world language and a distinc-\ntive grammar reﬂecting the languages of the\noriginal speakers and the process of language\nbirth by creolization. This gives them a dis-\ntinctive place in exploring the effectiveness\nof transfer from large monolingual or multi-\nlingual pretrained models.\nWhile our work,\nalong with previous work, shows that trans-\nfer from these models to low-resource lan-\nguages that are unrelated to languages in their\ntraining set is not very effective, we would\nexpect stronger results from transfer to cre-\noles.\nIndeed, our experiments show con-\nsiderably better results from few-shot learn-\ning of JamPatoisNLI than for such unrelated\nlanguages, and help us begin to understand\nhow the unique relationship between creoles\nand their high-resource base languages affect\ncross-lingual transfer.\nJamPatoisNLI, which\nconsists of naturally-occurring premises and\nexpert-written hypotheses, is a step towards\nsteering research into a traditionally under-\nserved language and a useful benchmark for\nunderstanding cross-lingual NLP.\n1\nIntroduction\nThe extensive progress that has been made in\nNLP research in recent years has largely been con-\nstrained to around 20 of the 7000 languages spoken\naround the world (Magueresse et al., 2020). Creole\nlanguages, which emerge as a result of contact be-\ntween speakers of different vernaculars, are even\nfurther underexplored (Lent et al., 2022b).\nThis work contributes to addressing this gap. We\npresent JamPatoisNLI, the ﬁrst natural language in-\nference dataset in Jamaican Patois, which is an\nEnglish-based creole spoken in the Caribbean. Ad-\nditionally, to our knowledge, no other natural lan-\nMi\nnuh\nlike\nwait\nMi\nhate\nfi\nwait\nEnglish \nlexical \noverlap\nEnglish \nlexical \noverlap\ngrammatical \nparticle \nmeaning 'to'\nNegation \nmarker \nFirst \nperson \nsingular \npronoun\nN\npremise\nhypothesis\nEnglish \nlexical \noverlap\nC\nE\nFigure 1:\nLinguistic features relevant for textual en-\ntailment classiﬁcation for Jamaican Patois and lexical\noverlap with English.\nguage inference corpus exists for any other creole\nlanguage.\nJamaican Patois is one of over 100 creole lan-\nguages spoken by millions of inhabitants of dif-\nferent regions across the world, including Africa,\nthe Caribbean, the Americas, islands in the In-\ndian Ocean and the Paciﬁc Ocean (including Aus-\ntralia and the Philippines) and South Asia (Ro-\nmaine, 2017; Bakker and Daval-Markussen, 2013).\nThough there has been a recent spike in interest\nin work on low-resource languages in the NLP\ncommunity (Kuriyozov et al., 2022; Kumar et al.,\n2022; Ebrahimi et al., 2021; Inuwa-Dutse, 2021;\nHasan et al., 2020; Agi´c and Vuli´c, 2019; Chowd-\nhury et al., 2018; Kumar et al., 2019; Das et al.,\n2017; Adewumi, 2022), creoles in particular are ex-\ntremely under-explored in spite of the prevalence of\ntheir usage globally (Lent et al., 2022b). Working\nmore with this class of languages is an important\nstep in ensuring that the beneﬁts of NLP technology\nare more equitably distributed globally.\nAdditionally, the class of creole languages is\na uniquely interesting point of study within the\nspace of multilingual NLP. Though creoles like\nJamaican Patois have distinct morphosyntactic fea-\ntures, they often share signiﬁcant lexical overlap\nwith the high-resource base languages from which\nthey are derived. This makes it possible to study\narXiv:2212.03419v1  [cs.CL]  7 Dec 2022\ncross-lingual transfer between high-resource and\nlow-resource languages that are distinct, but share\nsimilar lexicons. In particular, JamPatoisNLI pro-\nvides a benchmark for NLP researchers working\nto understand cross-lingual transfer to languages\noutside the training data of large pretrained mul-\ntilingual models. Creole languages like Jamaican\nPatois have the unique property of being outside\nthe pretraining data of these models, yet highly re-\nlated to their base languages, which are present in\nthe datasets used to train the models.1\nJamPatoisNLI was constructed using both natu-\nrally occurring and newly constructed utterances\nof Jamaican Patois rather than through translation.\nThis mitigates the problem of skewed cross-lingual\ntransfer results which arises when the test dataset\nconsists of translated examples but the training\ndataset does not (Artetxe et al., 2020). This also\nenhances the ecological validity (de Vries et al.,\n2020) of the dataset, as it is grounded in real world\nusage of the language and is thus a more relevant,\nrealistic benchmark. These two features mean that\nwork done with the dataset will be particularly use-\nful for moving towards developing technologies for\nspeakers of the language.\nWe run studies on JamPatoisNLI transfer-\nring from monolingual English BERT, multi-\nlingual BERT, monolingual English RoBERTa\nand multilingual XLM-RoBERTa, ﬁnetuned on\nthe Multi-NLI dataset, in zero-shot and few-\nshot settings.\nWe ﬁnd that monolingual En-\nglish RoBERTa (76.50%) and multilingual XLM-\nRoBERTa (75.17%) achieve similar accuracies\nwhen we use the entire few-shot JamPatoisNLI\ntraining dataset with 250 examples for further ﬁne-\ntuning. We also ﬁnd that the monolingual English\nBERT model (66.17 %) and the multilingual BERT\nmodel (65.33 %), achieve similar accuracies when\nwe use the entire few-shot JamPatoisNLI train-\ning dataset. In our experiments, the RoBERTa-\nbased models strongly outperform the BERT-based\nmodels. Additionally, we ﬁnd that few-shot per-\nformance on JamPatoisNLI increases much faster\n(with respect to the number of few-shot training ex-\namples) than on languages in AmericasNLI, which\nhave no strong connection to a high-resource lan-\nguage (Ebrahimi et al., 2021). Lastly, we run quali-\ntative experiments which leverage the relatedness\n1In large web scrapes, there likely is some Jamaican Patois\nlanguage in the resulting text, but it is not, e.g., one of the\nlanguages with a Wikipedia large enough to be included in\nMultilingual BERT.\nbetween Jamaican Patois and English to understand\nwhich differences between the languages boost or\ninhibit the effectiveness of cross-lingual transfer.\nWe hope that JamPatoisNLI prompts long-term\nresearch into building NLP tools that consider the\nparticular difﬁculties and opportunities of NLP for\nJamaican Patois and creole languages in general.\n2\nRelated Work\nNatural Language Inference Datasets.\nNatural\nlanguage inference (NLI), or recognizing textual\nentailment, is a standard benchmark task for natural\nlanguage understanding (Consortium et al., 1996;\nDagan et al., 2005; Storks et al., 2019).\nThe input to the task is a pair of sentences: the\npremise and the hypothesis. The goal is to output\na label – entailment, neutral or contradiction – to\ndescribe the relationship between the pair. Various\napproaches have been used to create NLI corpora.\nThe Stanford NLI (SNLI) (Bowman et al., 2015),\nMulti-NLI (MNLI) (Williams et al., 2018) and Ad-\nversarial NLI (ANLI) (Williams et al., 2020) En-\nglish datasets, esXNLI Spanish dataset (Artetxe\net al., 2020) Original Chinese Natural Language In-\nference (OCNLI) dataset (Hu et al., 2020) and code-\nmixed Hindi-English dataset (Khanuja et al., 2020)\nall consist of a mixture of pre-existing sentences\nand crowdsourced sentences. In the Japanese Re-\nalistic Textual Entailment Corpus, a collection of\npre-existing sentences are ﬁltered and paired using\nmachine learning methods then manually annotated\nwith labels (Yanaka and Mineshima, 2021).\nOther NLI corpora have been made using trans-\nlation techniques. The Natural Language Infer-\nence in Turkish (NLI-TR) dataset (Budur et al.,\n2020) was created using Amazon Translate on\nSNLI and MNLI. The Cross-Lingual NLI (XNLI)\nCorpus (Conneau et al., 2018) was created by col-\nlecting and crowd-sourcing 750 examples then hir-\ning human translators to translate the sentences\ninto 15 languages.\nExtensions of this dataset\nto low-resource languages such as AmericasNLI\n(Ebrahimi et al., 2021) and IndicXNLI (Aggar-\nwal et al., 2022) have been created using human\nand machine translation methods. However, sub-\nsequent research has found that translation-based\napproaches to creating datasets can introduce sub-\ntle artifacts which can lead to skewed accuracies\nfor cross-lingual transfer methods (Artetxe et al.,\n2020). JamPatoisNLI mitigates this problem by\nusing original rather than translated examples.\nIn spite of the examples given above, generally,\nthere is a relative dearth of datasets and research\ninto methods for low-resource languages across\nNLI and other tasks. Low-resource languages can\nbe deﬁned as those which are ‘less studied, re-\nsource scarce, less computerized, less privileged,\nless commonly taught or low density’ (Magueresse\net al., 2020).\nCreole Languages in NLP.\nCreole languages\nare typically low-resource. These languages arise\nthrough the process of creolization of another class\nof languages called pidgins. Pidgins emerge as a\nresult of contact between two or more groups of\nspeakers which do not have a common language. A\npidgin evolves to become a creole when it becomes\nthe native language of the children of its speakers\n(Muysken et al., 1995).2\nWithin the NLP community, a few datasets for\ndifferent tasks have been created for creoles using a\nvariety of methods. NaijaSenti is a Twitter human-\nannotated sentiment analysis dataset which is partly\ncomprised of 14,000 tweets in Nigerian-Pidgin or\nNaija, which is an English-based creole (Muham-\nmad et al., 2022).\nThe authors ﬁnd that code-\nswitching between these languages and English\nis a common feature in the dataset. They explore\nlanguage adaptive ﬁnetuning and zero-shot cross\nlingual transfer from multilingual pretrained mod-\nels, and achieve promising results. Cross-lingual\nChoice of Plausible Alternatives (XCOPA) (Ponti\net al., 2020) is a multilingual dataset for causal com-\nmon sense reasoning in 11 languages, one of which\nis Haitian Creole, that was created by translating\nEnglish COPA. The authors ﬁnd that across the lan-\nguages in the dataset, translation based-approaches\noutperform methods which employ multilingual\npretraining and ﬁnetuning. A part-of-speech tag-\nging and dependency parsing corpus for Colloquial\nSingaporean English (Singlish), an English-based\ncreole, has also been created (Wang et al., 2017)\nand further expanded (Wang et al., 2019) using\nthe Universal Dependencies (Nivre et al., 2020)\nscheme. The dataset was created by crawling pages\non online Singaporean forums.\nOther work has also explored using machine\nlearning methods for identifying and generating\ncreole text. Chang et al. (2022) use contrastive\nlearning to ﬁnetune BART (Lewis et al., 2019) so\nthat the model produces novel dialogue texts in\n2We discuss the process of creolization for Jamaican Patois\nfurther in Section 3.\nNaija and Yaounde (both English-based creoles).\nSoto (2020) uses a FastText (Joulin et al., 2016)\nbased supervised classiﬁer to identify instances of\nsentences in Guadeloupean Creole within a multi-\nlingual dataset.\nThe use of machine learning models on creole\nlanguages has also been investigated. Lent et al.\n(2021) ﬁnd that standard language models work\nbetter than distributionally robust ones on creoles,\nwhich shows that these languages are relatively\nstable. Lent et al. (2022a) show that ancestor-to-\ncreole transfer is non-trivial.\n3\nJamaican Patois\n3.1\nDescription of the Language\nJamaican Patois (or Jamaican Creole) is an English-\nbased creole spoken by over 3 million inhabitants\non the island and by Jamaicans across the diaspora\nglobally (Mair, 2003). Jamaican Patois resulted\nfrom contact between enslaved Africans brought to\nthe island in the 17th century and British colonists.\nBecause it is a hybrid of the languages spoken by\nthe two groups of people that came in contact, it\nexists on a continuum that ranges from more dis-\nsimilar to less dissimilar to English (Davidson and\nSchwartz, 1995). The terms for the classes in the\ncontinuum are the acrolect (variations which are\nclosest to English), the basilect (variations which\nare furthest from English) and the mesolect (varia-\ntions which are in between) (Patrick, 2019)\nExamples of each are shown in Table 1.\nClass\nExample\nBasilect\nMe a nyam di bickle weh dem gi mi.\nMesolect\nMe a eat di food weh dem gi mi.\nAcrolect\nI’m eating the food that they gave me.\nTable 1: Different translations of ‘I’m eating the food\nthat they gave me’ in Jamaican Patois. The basilectal\nextreme of the continuum consists of words that are\nnearly exclusively non-English. On the acrolectal ex-\ntreme of the spectrum (or Jamaican Standard English),\nthe example is identical to English.\n3.2\nRelevant Linguistic Features\nUnstandardized Orthography.\nJamaican Pa-\ntois is primarily a spoken language. Though there\nhave been efforts to develop a formal writing sys-\ntem for the language, none that have been devel-\noped are widely used by speakers of Patois.\nInstead, speakers use spelling patterns that re-\nﬂect how words in Patois are pronounced. This is\nillustrated in Table 2. In the table, ‘I want’ is spelt\nboth ‘Me wah’ and ‘Mi waa’: though the phrases\nyield similar pronunciations, different spellings are\nused.\nJamaican Patois\nEnglish\nMe wah bawl.\nI want to cry.\nMi waa cook.\nI want to cook.\nTable 2:\nExample of varied spelling of Patois words\npresent in the dataset.\nVocabulary Overlap with English.\nSince Ja-\nmaican Patois is English-based, there is a high\ndegree of overlap between the vocabularies used\nby the two languages, in spite of differences in\nspelling, tense and structure.\nWe present an example of this in the quote below.\nStrictly non-English vocabulary (including words\nsuch as ‘a’ that have different meanings in English)\nwhich are highlighted in bold, account for less than\none-third of the words in the sentence.\nIt look like more tourist start come\nsince dem loosen up di restrictions dem.\nMi frighten fi see how di beach full wen\nmi go a Negril weh day.\nTherefore, JamPatoisNLI will be useful for evalu-\nating the efﬁcacy of methods for linguistic transfer\nin scenarios where there is a high degree of overlap\nbetween the source and target language.\nNegation.\nCommon markers of negation used\nin Jamaican Patois and their English equivalents\nwhich feature in the dataset are presented in Table\n3. Examples of these markers in the dataset are\npresented in Table 17 in the Appendix.\nNegation markers are important linguistic fea-\ntures in the context of NLI datasets, as their pres-\nence and interaction with other sentence compo-\nnents are highly relevant to the determination of the\nright classiﬁcation for a given textual entailment\nexample (Gururangan et al., 2018).\nJamaican Patois\nEnglish\nnuh\nnot/don’t/doesn’t\ncyaa/cyaan\ncan’t\nneva\nnever\nTable 3: Markers of negation in Jamaican Patois.\n4\nConstructing JamPatoisNLI\nFor each example in the dataset, we pulled the\npremise from a pre-existing text source. Then, a\nlabel was randomly selected and a corresponding\nhypothesis was written by the ﬁrst author, who\nspeaks and writes Jamaican Patois ﬂuently. Our\nmethodology mirrors that of both MNLI (Williams\net al., 2018) and ANLI (Williams et al., 2020).\nJamPatoisNLI consists of 650 examples split\nacross training, development and validation. Statis-\ntics for the corpus are shown in Table 5. A lim-\nited availability of native speakers to construct and\nannotate a large number of examples is a current\nproblem in low-resource NLP (Magueresse et al.,\n2020). However, for the purposes of our exper-\niments, the sizes of the training, validation and\ntesting sets are sufﬁcient for exploring few-shot\nﬁnetuning techniques and obtaining useful signals\nabout the effectiveness of different methods.\n4.1\nPremise Collection\nSince Jamaican Patois is primarily a spoken lan-\nguage, there is a limited number of textual sources\nof Patois that are readily available online. How-\never, Patois speakers regularly use the language\nfor communication on social media, and in litera-\nture. These are the sources that were used for the\npremises in the dataset. Around 97% of examples\nare drawn from Twitter and the remaining examples\nare drawn from a cultural website, jamaicans.com,\nand from literature by Jamaican poets, Dr. Louise\nBennett-Coverley and Shelley Sykes-Coley. The\nnumber of examples per source is outlined in Table\n13 in the Appendix.\nThis method of construction also makes the\ndataset less prone to effects from translation ar-\ntifacts which can skew the effectiveness of dif-\nferent cross-lingual transfer techniques. Artetxe\net al. (2020) ﬁnd that when the test dataset is\nmade using translated examples, there is a slight\noverestimation of the cross-lingual transfer gap as\nwell as the efﬁcacy of the TRANSLATE-TRAIN3 tech-\nnique, and an underestimation of the efﬁcacy of\nthe TRANSLATE-TEST4 technique. None of these\neffects are present when the test dataset is com-\nposed of original examples which were not cre-\nated through translation. Additionally, because the\n3The TRANSLATE-TRAIN technique involves translating the\ntraining dataset to the target language.\n4The TRANSLATE-TEST technique involves translating the\ntesting dataset to the source language.\nPremise\nLabel\nHypothesis\nI decided that Christmas hafﬁketch me inna good\nmood!\nentailment\nE E\nMe determined ﬁhappy wen Christmas come!\nA dem ﬁget the money\ncontradiction\nC C\nDem nuh deserve di money\nmi must make chicken alfredo when mi go home\ndoe\nneutral\nN N\nmi love ﬁeat chicken alfredo\nRaisin a get soak in a red label wine ﬁmake cake\nneutral\nC N\nMi granny nuh normally mek har cake dem wid\nraisin\nI was in juicy beef and yuh know say mi stress out\ntil mi phone drop\nentailment\nE E\nMi phone drop wen mi did deh inna juicy beef\nTable 4: Random sample selected from the 100 double annotated examples in the corpus, with their gold labels\nand validation labels (abbreviated E, N, C) by each of the annotators.\nStatistic\nEnt.\nNeu.\nCon.\nTotal\n#Train\n84\n83\n83\n250\n#Dev\n66\n67\n67\n200\n#Test\n67\n66\n67\n200\nAvg. Premise Length\n12.2\n13.6\n11.8\n12.5\nAvg. Hypothesis Length\n10.3\n11.9\n10.7\n11.0\n#Distinct Words\n1210\n1401\n1187\n2612\nTable 5:\nStatistics across the 650 examples in the\ndataset, by class and in aggregate.\npremises of JamPatoisNLI are drawn from natural\noccurrences of Jamaican Patois written by various\nspeakers of the language, the dataset better reﬂects\nthe natural writing patterns of speakers than those\ncreated using machine or human translation tech-\nniques.\n4.2\nHypothesis Construction\nThe set of hypotheses in the corpus is comprised\nof novel sentences constructed by our ﬁrst author,\nwho is a native speaker of Jamaican Patois. For\neach premise, a corresponding hypothesis was writ-\nten so that the pair’s classiﬁcation would be either\nentailment, neutral or contradiction. The\ncriteria used for assignment of pairs to each class\nis shown in Figure 4 in the Appendix.\nThe constructed hypothesis in each example\nmimics the diverse spelling conventions and writ-\ning patterns used in the corresponding pre-existing\npremise. As such, the non-standardized nature of\nJamaican Patois is reﬂected in both the collected\nand constructed sentences in the dataset.\nIn order to maximize the linguistic diversity of\nexamples in the dataset, each premise was used\nto generate a single hypothesis (rather than three\nhypotheses generated per premise, which was done\nfor MNLI (Williams et al., 2018)).\nMetric\nAccuracy\nCounts\nFleiss K\n88.99%\n100\n% Accuracy\n89.00%\n100\nNeutral % Accuracy\n75.76%\n33\nEntailment % Accuracy\n100.00%\n34\nContradiction % Accuracy\n90.91%\n33\nTable 6: Inter-annotator agreement. We count a classi-\nﬁcation as accurate if both annotators agreed with the\noriginal annotations in the dataset.\n4.3\nLabel Validation\nA random sample of 100 sentence pairs evenly\ndistributed across the three classes was double an-\nnotated by ﬂuent speakers of Jamaican Patois. We\nrecruited volunteer annotators by reaching out to\nfriends and colleagues. The labelling criteria given\nto the annotators were the same as those used to\ngenerate the hypotheses, and are outlined in Ap-\npendix Figure 4. In Table 6, we present statistics for\ninter-annotator agreement for these examples. The\nFleiss Kappa accuracy for the dataset was 88.99%\nwhile the percentage accuracy was 89.00%.\n5\nExperiments and Results\nAcross our experiments, our goals are to:\n1. Provide benchmarks for JamPatoisNLI thus\ndetermining the difﬁculty of the dataset and\neffectiveness of cross-lingual transfer.\n2. Compare the effectiveness of cross-lingual\ntransfer on JamPatoisNLI (a language that is\nrelated to language(s) present in the training\ncorpus of each of the pretrained models we\nexamine), to cross-lingual transfer on Amer-\nicasNLI (which contains languages that are\nunrelated to any language(s) present in the\ntraining corpus of each pretrained model).\n3. Leverage the nature of Jamaican Patois as\na creole to further understand cross-lingual\ntransfer.\nThe experiments that we conduct are done in the\nzero-shot and few-shot settings.\n5.1\nGeneral Setup\nIn our experiments, we use English BERT, multilin-\ngual BERT (Devlin et al., 2018), English RoBERTa\n(Liu et al., 2019) and XLM-RoBERTa (Conneau\net al., 2019a) as our base pretrained models. We\nuse a two-layer perceptron with ReLU activations\nfor the classiﬁcation head, and ﬁrst ﬁnetune on the\nMNLI training dataset. We use cased and uncased\nversions of each BERT-based pretrained model, and\nexperiment with frozen and unfrozen versions,5 for\na total of eight types of BERT-based models. For\nour RoBERTa-based models, we also experiment\nwith frozen and unfrozen versions for a total of four\ntypes of RoBERTa-based models. Throughout our\nexperiments with the twelve model types, we make\ncomparisons among the BERT-based models and\nthe RoBERTa-based models separately.\nTo select the twelve MNLI ﬁnetuned models that\nwe use for our few-shot experiments, we conduct a\nhyperparameter search over dropouts in the range\n[0.2, 0.5], batch sizes in the range [8, 32], learning\nrates in the range [1e-05, 1e-06] and epoch counts\nin the range [2, 10] and pick those that achieved\nreasonable accuracies on the MNLI development\ndataset (above 86% for unfrozen models and above\n62% for frozen models).\nAmong the twelve selected models ﬁnetuned\non MNLI, we evaluate the zero-shot and few-shot\nperformance on each of our target datasets to de-\ntermine which model types produce the highest\naccuracy. To compare the types of models, we ﬁx\nthe hyperparameters to the values in Table 16 in\nthe Appendix, and average over three experiments\nwith different seeds. Then, from among the eight\nﬁnetuned BERT-based models, we pick the type\nthat achieved the highest scores for the maximum\nnumber of few-shot training examples for each our\nvalidation datasets (JamPatoisNLI and Americas-\n5In our frozen model, all parameters of the pretrained\nbase models are ﬁxed during ﬁnetuning so that only the NLI\nclassiﬁcation head is updated, while for our unfrozen models,\nall model parameters are allowed to update.\nHyperparameter\nBest Model on\nJamPatoisNLI\nBest Model on\nAmericasNLI\nFinetune epoch ct.\n5\n5\nFinetune batch size\n16\n16\nFinetune learning rate\n1e-05\n1e-05\nFinetune dropout\n0.3\n0.3\nFew shot # of iter.\n200\n100\nFew shot batch size\n16\n8\nFew shot learning rate\n5e-05\n1e-05\nFew shot dropout\n0.25\n0.25\nTable 7:\nFinal hyperparameters for best BERT-based\nmodel on JamPatoisNLI (bert-uncased-unfrozen)\nand AmericasNLI (mbert-cased-unfrozen).\nHyperparameter\nBest Model on\nJamPatoisNLI\nBest Model on\nAmericasNLI\nFinetune epoch ct.\n3\n5\nFinetune batch size\n32\n16\nFinetune learning rate\n1e-05\n1e-05\nFinetune dropout\n0.2\n0.3\nFew shot # of iter.\n200\n100\nFew shot batch size\n16\n16\nFew shot learning rate\n1e-05\n1e-05\nFew shot dropout\n0.25\n0.25\nTable 8:\nFinal hyperparameters for best RoBERTa-\nbased model on JamPatoisNLI (roberta-unfrozen)\nand AmericasNLI (xlm-unfrozen).\nNLI). We also do the same for the four ﬁnetuned\nRoBERTa-based models.\nAfter we select the best out of the model types\namong the models ﬁnetuned on MNLI and further\nﬁnetuned on the target fewshot datasets, we per-\nform a ﬁnal hyperparameter sweep. Tables 7 and\n8 show the ﬁnal set of hyperparameters that we\narrived at after we conducted our sweep for the\nbest models on the JamPatoisNLI and Americas-\nNLI validation sets among our BERT-based models\nand RoBERTa-based models.\nIn our few-shot ﬁnetuning setup, we select one\nexample from each class for each “shot”. For in-\nstance, using this convention, two-shot ﬁnetuning\ninvolves ﬁnetuning using six examples in total: two\nfrom each of the three NLI classes. Additionally,\nduring few-shot ﬁnetuning, we keep all layers of\nthe base model unfrozen.\n5.2\nBenchmarks for JamPatoisNLI\nSetup.\nFor JamPatoisNLI, the best BERT-based\nmodel type was the unfrozen uncased English\nBERT model (bert-uncased-unfrozen) based\non accuracies on the validation set.\nUs-\ning the hyperparameters in Table 7, we also\nmake comparisons to a hypothesis only baseline\n# of Fewshot\nClass Triples\nMaj. Base.\nHyp. Only Base.\n(bert-uncased-\nunfrozen)\nbert-uncased-\nunfrozen\nmbert-uncased-\nunfrozen\nroberta-\nunfrozen\nxlm-unfrozen\n0\n33.50\n38.50\n56.00\n50.00\n67.50\n56.00\n1\n33.50\n38.17\n54.50\n52.17\n68.17\n57.50\n2\n33.50\n37.17\n56.83\n53.33\n69.17\n58.17\n4\n33.50\n37.00\n51.00\n52.33\n66.83\n57.67\n8\n33.50\n35.83\n52.17\n51.17\n68.83\n57.50\n16\n33.50\n38.83\n56.17\n53.50\n70.17\n58.83\n32\n33.50\n38.50\n61.17\n63.83\n73.00\n70.00\n64\n33.50\n46.33\n64.50\n65.17\n76.33\n72.50\n83\n33.50\n43.33\n66.17\n65.33\n76.50\n75.17\nTable 9: Zero-shot and few-shot accuracies for different models evaluated on JamPatoisNLI averaged over three\nexperiments with different seeds. The best models were chosen based on results for the validation set.\n(bert-uncased-unfrozen), as well as the best\nmultilingual BERT-based model on JamPatois-\nNLI, which was the unfrozen uncased multilingual\nBERT model (mbert-uncased-unfrozen).\nThe\nbest\nRoBERTa-based\nmodel\ntype\nwas the unfrozen English RoBERTa model\n(roberta-unfrozen). We also include results for\nthe best multilingual RoBERTa-based model on the\ndataset, which was the unfrozen XLM-RoBERTa\nmodel (xlm-unfrozen). The hyperparameters that\nwe used are listed in Table 8.\nResults.\nOur\nresults\non\nthe\ntest\nset\nare\npresented\nin\nTable\n9.\nWe\nfound\nthat\nwith\nthe\nmaximum\nnumber\ntraining\nof\nexamples,\nbert-uncased-unfrozen\nand\nmbert-uncased-unfrozen had relatively similar\naccuracies when all few-shot examples were used\n(66.17% and 65.33% respectively). We also found\nthat\nroberta-unfrozen\nand\nxlm-unfrozen\nachieve similar accuracies on the full fewshot\ndataset (76.50% and 75.17%) respectively.\nThe two RoBERTa-based models signiﬁcantly\noutperformed the two BERT-based models – in fact,\nthe zero-shot accuracy on the roberta-unfrozen\nmodel (67.50%) outperforms both BERT based\nmodels when they are ﬁnetuned on the full few-\nshot dataset.\nFor our best model (xlm-unfrozen), the stan-\ndard deviation in percentage accuracy for the maxi-\nmum number of few-shot examples across ten ex-\nperiments was 0.75% when evaluated on the val-\nidation set and 1.43% when evaluated on the test\nset.\n5.3\nComparisons with AmericasNLI\nSetup.\nA natural comparison point for JamPatois-\nNLI is AmericasNLI (Ebrahimi et al., 2021) as it is\nalso a low-resource NLI dataset. However, unlike\nJamaican Patois, the languages in the corpus are\nnot closely related to any high-resource languages\nfor which there are large pretrained language mod-\nels or large natural language inference training\ndatasets. In particular, the languages in Ameri-\ncasNLI do not belong to the same family as any\nof the languages in the two most commonly used\nmultilingual pretrained language models – multilin-\ngual BERT (Devlin et al., 2018) and XLM-R (Con-\nneau et al., 2019b). JamPatoisNLI is unseen from\nthe perspective of existing pretrained monolingual\nor multilingual models but related to the source\nlanguage(s) involved in transfer learning, whereas\nAmericasNLI is both unseen and unrelated.\nFor our experiments, we use ﬁve of the lan-\nguages in the AmericasNLI dataset, and create a\nrandomly selected 250-200-200 train-dev-test split\nfrom among the examples in the original develop-\nment dataset for each language (shown in Table 14\nin the Appendix) to mirror the number of examples\npresent in each of the splits in JamPatoisNLI.\nFor the AmericasNLI languages, the best BERT-\nbased model type based on results on the vali-\ndation set was the unfrozen cased multilingual\nBERT model (mbert-cased-unfrozen). The best\nRoBERTa-based model type was the unfrozen\nXLM-RoBERTa model (xlm-unfrozen).\nResults.\nWe present the results of our experi-\nments on the test set in Table 10. We found that\nthere was a signiﬁcant gap in accuracies on JamPat-\noisNLI and AmericasNLI. Across all experiments,\nboth zero-shot and few-shot accuracies for the Jam-\nPatoisNLI dataset exceeded those for the Amer-\nicasNLI dataset. The best JamPatoisNLI model\nachieved a zero-shot accuracy of 67.50% while\nthe best AmericasNLI model achieved a zero-shot\nAvg. AmericasNLI\nAccuracy\nPatois\nAccuracy\nNum.\nmbert-\ncased-\nunfrozen\nxlm-\nunfrozen\nbert-\nuncased-\nunfrozen\nroberta-\nunfrozen\n0\n42.00\n39.60\n56.00\n67.50\n1\n41.83\n39.17\n54.50\n68.17\n2\n42.67\n39.50\n56.83\n69.17\n4\n42.67\n40.03\n51.00\n66.83\n8\n42.70\n39.93\n52.17\n68.83\n16\n43.63\n42.77\n56.17\n70.17\n32\n46.40\n46.07\n61.17\n73.00\n64\n48.87\n47.40\n64.50\n76.33\n83\n49.23\n48.83\n66.17\n76.50\nTable 10:\nTest set accuracies for best BERT-\nbased and RoBERTa-based models on the Jam-\nPatoisNLI\ndataset\n(bert-uncased-unfrozen,\nroberta-unfrozen) and on the AmericasNLI dataset\n(mbert-cased-unfrozen,\nxlm-unfrozen).\nExper-\niments are averaged over three seeds and the best\nmodels were chosen based on results for the validation\nset.\nFigure 2:\nPlots for the best AmericasNLI model\n(mbert-cased-unfrozen) on each language, and the\nbest JamPatoisNLI model (bert-uncased-unfrozen).\nExperiments are averaged over three seeds and the best\nmodels were chosen based on results for the val. set.\naccuracy of 42.00% (both compared to a 33.50%\nmajority baseline).\nThis shows that the language relatedness be-\ntween Jamaican Patois and English signiﬁcantly\nboosts the effectiveness of cross-lingual transfer\nlearning even in the zero-shot case. For the few-\nshot setting, the highest accuracy achieved on the\nJamPatoisNLI dataset was 76.50%. The highest\naverage accuracy achieved on the AmericasNLI\ndataset was 49.23%.\nThe plots comparing the best JamPatoisNLI\nmodel to the best AmericasNLI model on each\nof the respective datasets for BERT-based models\nand RoBERTa-based models are shown in Figures\n2 and 3. For the BERT-based models, we see that\ncross-lingual transfer augmented by few-shot learn-\n0\n100\n101\n102\nNum Examples\n0.4\n0.5\n0.6\n0.7\nAccuracies\nAccuracies for JamPatoisNLI and AmericasNLI\naym\nbzd\ncni\npatois\nquy\ntar\nFigure 3:\nPlots for the best AmericasNLI model\n(xlm-unfrozen) on each language, and the best Jam-\nPatoisNLI model (roberta-unfrozen). Experiments\nare averaged over three seeds and the best models were\nchosen based on results for the val. set.\ning is quite effective for JamPatoisNLI, whereas\nthe gains for AmericasNLI languages are rather\nmodest. Tabulated results for these experiments\ncan be found in Appendix Tables 18 and 19.\n5.4\nExperiments with Transitioning from\nJamaican Patois to English\nSetup.\nA key characteristic of Jamaican Patois is\nthat it exists on a spectrum that ranges from highly\ndissimilar to English (the basilect), to highly simi-\nlar to English (the acrolect). We experiment with\n83-shot classiﬁcation (the full set of examples in\nour few-shot training dataset) on an augmented test\ndataset derived from pairs that were incorrectly\nclassiﬁed by at least two of the three models in\nour original few-shot experiments. To construct\nthis dataset, we picked a single example for each\ntype of misclassiﬁcation with respect to the three\nNLI labels, for a total of 6 examples from the origi-\nnal dataset (which mostly fell on various points on\nthe mesolectal range of the creole spectrum). We\nthen wrote English translations for each of these\nexamples (which would fall on the acrolectal end\nof the creole spectrum) and hand-wrote interme-\ndiate translations between them that are all valid\nJamaican Patois to qualitatively study whether (and\nfor what changes) along the path the label becomes\ncorrect. We conduct few-shot ﬁnetuning using our\noriginal training set for three models with different\nseeds using the parameters for the best BERT-based\nJamPatoisNLI model (bert-uncased-unfrozen),\nlisted in Table 7.\nResults.\nWe present a qualitative example of this\nexperiment in Table 11. Here, changing the verb\nfrom Jamaican Patois to English caused the models\nto switch to the correct classiﬁcation. The three\nChange\nPremise\nHypothesis\nTgt.\nM1 M2 M3\n-\nAny day mi master\nLumaFusion,\nmi lef my work.\nAs soon as mi good\nwid LumaFusion,\nmi a quit mi job\nE\nC\nC\nC\nPronoun:\nmi →I\nAny day I master\nLumaFusion,\nmi lef my work\nAs soon as I’m good\nwid LumaFusion,\nmi a quit my job\nE\nC\nC\nC\nVerb:\nlef/quit →leaving/quitting\nAny day I master\nLumaFusion, mi\nleaving my work\nAs soon as I’m good\nwid LumaFusion,\nmi quitting this job\nE\nE\nE\nE\nPronoun:\nmi →I\nAny day I master\nLumaFusion, I’m\nleaving my job\nAs soon as I’m good\nwid LumaFusion,\nI’m quitting my job.\nE\nE\nE\nE\nDeterminer/Preposition:\nAny day/wid →The day that/with\nThe day that I master\nLumaFusion, I’m\nleaving my job.\nAs soon as I’m good\nwith LumaFusion,\nI’m quitting my job.\nE\nE\nE\nE\nTable 11: Sample from Jamaican Patois to English transition dataset. The ﬁnal example is in English, and we\npresent predictions made by three models ﬁnetuned with our Patois few-shot training dataset using the parameters\nfor the best JamPatoisNLI model in Table 7.\n.\nmodels switched to the correct prediction for a\nchange prior to the full translation of the Jamaican\nPatois example to English for all but one of the orig-\ninally misclassiﬁed examples in our experiments.\n6\nDiscussion\nWe see that the relatedness between Jamaican Pa-\ntois and English strongly contributes to the effec-\ntiveness of cross-lingual transfer in both zero-shot\nand few-shot settings. Additionally, although natu-\nral language inference is a higher order reasoning\ntask, our models achieved relatively high accuracy\non the JamPatoisNLI dataset by learning the task\nfrom MNLI examples in English.\nA natural question that arises based on these\nresults, is whether vocabulary overlap is the pri-\nmary factor that led to the boost in effectiveness of\ntransfer learning in these experiments, or whether\na higher order notion of similarity is a larger factor.\nComparing zero-shot and few-shot accuracies for\nother languages that are closely related to English\nbut do not share the same degree of vocabulary\noverlap as an English-based creole (such as Ger-\nman) might be an interesting line of future research.\nInterestingly, though Jamaican Patois developed\nas a result of contact between speakers of English\nand speakers of West African languages (some\nof which are present in multilingual BERT’s and\nXLM-RoBERTa’s training corpus), the multilin-\ngual models were not more effective base pre-\ntrained language models than the monolingual mod-\nels. Another possible direction for future research\nmight be to determine whether there are methods\nthat allow for more effective leveraging of the mul-\ntilingual characteristic of the models during ﬁne-\ntuning for creole target languages.\n7\nConclusion\nJamPatoisNLI is a natural language inference\ndataset in an English-based creole, constructed\nfrom existing and novel examples of Jamaican Pa-\ntois. Our experiments show that the language’s\nrelatedness to English signiﬁcantly boosts the ef-\nfectiveness of cross-lingual transfer, even for the\nhigher order task of natural language inference in\nboth zero-shot and few-shot settings. We hope\nthat the creation of this dataset encourages further\nresearch in the ﬁeld on methods to improve cross-\nlingual transfer for creole target languages, and the\ncreation of other low-resource language and creole\nlanguage datasets.\nAcknowledgements\nWe thank Roxanne Dobson, Ghawayne Calvin,\nDanielle Roberts, Khaesha Brooks, Dominique\nLyew and Ana-Katrina Donaldson for volunteer-\ning to be dataset annotators. We also thank Prof.\nChristopher Potts (cgpotts@stanford.edu) for com-\nments on the paper. RA was partially supported by\na Siebel Scholarship, a Google Generation Scholar-\nship and the SWE Motorola Solutions Foundation\nEngineering Scholarship. JH was supported by an\nNSF Graduate Research Fellowship under grant\nnumber DGE-1656518.\n8\nLimitations\nOne limitation of our research is related to the fact\nthat Jamaican Patois is a low-resource language.\nThe size of the dataset splits (particularly, the val-\nidation and test sets) are much smaller than those\nof high-resource language datasets.\nFurther, the differences observed between the\nAmericasNLI and JamPatoisNLI datasets are not\nnecessarily solely due to differences in language\nsimilarity to the source languages: another con-\ntributing factor might be differences in difﬁculty\nfor the two datasets.\nReferences\nTosin Adewumi. 2022.\nItakúroso: Exploiting cross-\nlingual transferability for natural language genera-\ntion of dialogues in low-resource, African languages.\nIn 3rd Workshop on African Natural Language Pro-\ncessing.\nDivyanshu\nAggarwal,\nV.\nGupta,\nand\nAnoop\nKunchukuttan. 2022.\nIndicXNLI: Evaluating\nmultilingual inference for Indian languages. ArXiv,\nabs/2204.08776.\nŽeljko Agi´c and Ivan Vuli´c. 2019. JW300: A wide-\ncoverage parallel corpus for low-resource languages.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n3204–3210, Florence, Italy. Association for Compu-\ntational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2020.\nTranslation artifacts in cross-lingual transfer learn-\ning. CoRR, abs/2004.04721.\nPeter Bakker and Aymeric Daval-Markussen. 2013.\nCreole studies in the 21st century: A brief presen-\ntation of the special issue on creole languages. Acta\nLinguistica Hafniensia, 45(2):141–150.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nCoRR, abs/1508.05326.\nEmrah Budur, Riza Özçelik, Tunga Güngör, and\nChristopher Potts. 2020. Use of machine translation\nto obtain labeled datasets for resource-constrained\nlanguages. CoRR, abs/2004.14963.\nErnie Chang, Jesujoba Oluwadara Alabi, David Ife-\noluwa Adelani, and Vera Demberg. 2022. Dialogue\npidgin text adaptation via contrastive ﬁne-tuning. In\n3rd Workshop on African Natural Language Process-\ning.\nKoel Dutta Chowdhury, Mohammed Hasanuzzaman,\nand Qun Liu. 2018.\nMultimodal neural machine\ntranslation for low-resource language pairs using\nsynthetic data. In Proceedings of the Workshop on\nDeep Learning Approaches for Low-Resource NLP,\npages 33–42.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019a. Unsupervised\ncross-lingual representation learning at scale. CoRR,\nabs/1911.02116.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019b. Unsupervised\ncross-lingual representation learning at scale. CoRR,\nabs/1911.02116.\nAlexis Conneau, Guillaume Lample, Ruty Rinott, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018.\nXNLI: evaluat-\ning cross-lingual sentence representations.\nCoRR,\nabs/1809.05053.\nThe Fracas Consortium, Robin Cooper, Dick Crouch,\nJan Van Eijck, Chris Fox, Josef Van Genabith,\nJan Jaspars, Hans Kamp, David Milward, Manfred\nPinkal, Massimo Poesio, Steve Pulman, Ted Briscoe,\nHolger Maier, and Karsten Konrad. 1996. Using the\nframework.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005.\nThe pascal recognising textual entailment\nchallenge.\nIn Machine learning challenges work-\nshop, pages 177–190. Springer.\nArjun Das, Debasis Ganguly, and Utpal Garain. 2017.\nNamed entity recognition with word embeddings\nand Wikipedia categories for a low-resource lan-\nguage.\nACM Transactions on Asian and Low-\nResource Language Information Processing (TAL-\nLIP), 16(3):1–19.\nCecelia Davidson and Richard G Schwartz. 1995. Se-\nmantic boundaries in the lexicon: Examples from Ja-\nmaican Patois. Linguistics and Education, 7(1):47–\n64.\nHarm de Vries, Dzmitry Bahdanau, and Christo-\npher D. Manning. 2020.\nTowards ecologically\nvalid research on language user interfaces. CoRR,\nabs/2007.14435.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018.\nBERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nAbteen Ebrahimi, Manuel Mager, Arturo Oncevay,\nVishrav Chaudhary, Luis Chiruzzo, Angela Fan,\nJohn Ortega, Ricardo Ramos, Annette Rios, Ivan\nVladimir, Gustavo A. Giménez-Lugo, Elisabeth\nMager, Graham Neubig, Alexis Palmer, Rolando\nA. Coto Solano, Ngoc Thang Vu, and Katharina\nKann. 2021.\nAmericasNLI: Evaluating zero-shot\nnatural language understanding of pretrained mul-\ntilingual models in truly low-resource languages.\nCoRR, abs/2104.08726.\nSuchin Gururangan,\nSwabha Swayamdipta,\nOmer\nLevy, Roy Schwartz, Samuel R. Bowman, and\nNoah A. Smith. 2018. Annotation artifacts in natu-\nral language inference data. CoRR, abs/1803.02324.\nTahmid Hasan, Abhik Bhattacharjee, Kazi Samin, Ma-\nsum Hasan, Madhusudan Basak, M Sohel Rah-\nman, and Rifat Shahriyar. 2020. Not low-resource\nanymore: Aligner ensembling, batch ﬁltering, and\nnew datasets for Bengali-English machine transla-\ntion. arXiv preprint arXiv:2009.09359.\nHai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra\nKübler, and Lawrence Moss. 2020. OCNLI: Orig-\ninal Chinese Natural Language Inference. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020, pages 3512–3526, Online. As-\nsociation for Computational Linguistics.\nIsa Inuwa-Dutse. 2021. The ﬁrst large scale collection\nof diverse Hausa language datasets. arXiv preprint\narXiv:2102.06991.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, Hervé Jégou, and Tomás Mikolov.\n2016. Fasttext.zip: Compressing text classiﬁcation\nmodels. CoRR, abs/1612.03651.\nSimran\nKhanuja,\nSandipan\nDandapat,\nSunayana\nSitaram, and Monojit Choudhury. 2020.\nA new\ndataset for natural language inference from code-\nmixed conversations. CoRR, abs/2004.05051.\nGokul Karthik Kumar, Abhishek Singh Gehlot, Sa-\nhal Shaji Mullappilly, and Karthik Nandakumar.\n2022.\nMucot:\nMultilingual contrastive training\nfor question-answering in low-resource languages.\narXiv preprint arXiv:2204.05814.\nRashi Kumar, Piyush Jha, and Vineet Sahula. 2019. An\naugmented translation technique for low resource\nlanguage pair: Sanskrit to Hindi translation.\nIn\nProceedings of the 2019 2nd International Confer-\nence on Algorithms, Computing and Artiﬁcial Intel-\nligence, pages 377–383.\nElmurod Kuriyozov, Sanatbek Matlatipov, Miguel A\nAlonso, and Carlos Gómez-Rodríguez. 2022. Con-\nstruction and evaluation of sentiment datasets for\nlow-resource languages: The case of Uzbek. In Lan-\nguage and Technology Conference, pages 232–243.\nSpringer.\nHeather Lent, Emanuele Bugliarello, and Anders Sø-\ngaard. 2022a.\nAncestor-to-creole transfer is not\na walk in the park.\nIn Proceedings of the Third\nWorkshop on Insights from Negative Results in NLP,\npages 68–74, Dublin, Ireland. Association for Com-\nputational Linguistics.\nHeather Lent, Kelechi Ogueji, Miryam de Lhoneux,\nOrevaoghene Ahia, and Anders Søgaard. 2022b.\nWhat a creole wants, what a creole needs. ArXiv\npreprint arXiv:2206.00437.\nHeather C. Lent,\nEmanuele Bugliarello,\nMiryam\nde Lhoneux,\nChen Qiu,\nand Anders Søgaard.\n2021.\nOn language models for creoles.\nCoRR,\nabs/2109.06074.\nMike\nLewis,\nYinhan\nLiu,\nNaman\nGoyal,\nMar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2019. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. CoRR, abs/1910.13461.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nAlexandre Magueresse, Vincent Carles, and Evan\nHeetderks. 2020.\nLow-resource languages: A re-\nview of past work and future challenges.\nCoRR,\nabs/2006.07264.\nChristian Mair. 2003.\nLanguage, code, and symbol:\nThe changing roles of Jamaican Creole in diaspora\ncommunities.\nAAA: Arbeiten aus Anglistik und\nAmerikanistik, 28(2):231–248.\nShamsuddeen Hassan Muhammad, David Ifeoluwa\nAdelani, Sebastian Ruder, Ibrahim Said Ahmad,\nIdris Abdulmumin, Bello Shehu Bello, Monojit\nChoudhury, Chris Chinenye Emezue, Saheed Abdul-\nlahi Salahudeen, Aremu Anuoluwapo, Alípio Jeorge,\nand Pavel Brazdil. 2022.\nNaijaSenti: A Nigerian\nTwitter sentiment corpus for multilingual sentiment\nanalysis. CoRR, abs/2201.08277.\nPieter Muysken, Norval Smith, et al. 1995. The study\nof pidgin and creole languages. Pidgins and creoles:\nAn introduction, pages 3–14.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Hajic, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis M. Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection.\nCoRR, abs/2004.10643.\nPeter Patrick. 2019. Jamaican Creole. In The Mouton\nWorld Atlas of Variation in English, pages 126–136.\nDe Gruyter.\nEdoardo Maria Ponti, Goran Glavas, Olga Majewska,\nQianchu Liu, Ivan Vulic, and Anna Korhonen. 2020.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. CoRR, abs/2005.00333.\nSuzanne Romaine. 2017. Pidgin and creole languages.\nRoutledge.\nWilliam Soto. 2020. Language Identiﬁcation of Guade-\nloupean Creole.\nIn 2èmes journées scientiﬁques\ndu Groupement de Recherche Linguistique Informa-\ntique Formelle et de Terrain (LIFT), pages 54–59,\nMontrouge (virtuel), France. CNRS.\nShane Storks, Qiaozi Gao, and Joyce Yue Chai. 2019.\nCommonsense reasoning for natural language under-\nstanding: A survey of benchmarks, resources, and\napproaches. ArXiv, abs/1904.01172.\nHongmin Wang, Jie Yang, and Yue Zhang. 2019. From\ngenesis to creole language: Transfer learning for\nSinglish Universal Dependencies parsing and POS\ntagging. ACM Trans. Asian Low-Resour. Lang. Inf.\nProcess., 19(1).\nHongmin Wang, Yue Zhang, GuangYong Leonard\nChan, Jie Yang, and Hai Leong Chieu. 2017. Univer-\nsal Dependencies parsing for colloquial Singaporean\nEnglish. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1732–1744, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nAdina Williams, Tristan Thrush, and Douwe Kiela.\n2020. ANLIzing the adversarial natural language in-\nference dataset. CoRR, abs/2010.12729.\nHitomi Yanaka and Koji Mineshima. 2021.\nAssess-\ning the generalization capacity of pre-trained lan-\nguage models through Japanese adversarial natural\nlanguage inference.\nIn Proceedings of the Fourth\nBlackboxNLP Workshop on Analyzing and Interpret-\ning Neural Networks for NLP, pages 337–349, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021.\nBitﬁt:\nSimple parameter-efﬁcient\nﬁne-tuning for transformer-based masked language-\nmodels. CoRR, abs/2106.10199.\nA\nAppendix\nA.1\nFinetuning with BitFit\nBitFit is a sparse parameter efﬁcient ﬁnetuning\nmethod introduced for use with small-to-medium\nsized training datasets which involves ﬁnetuning\nonly the bias terms of a pretrained language model\n(Zaken et al., 2021). As an initial approach for few-\nshot ﬁnetuning, we experimented with using BitFit\nusing the same hyperparameters described in our\nprior experiments (in Table 7) for the best JamPat-\noisNLI model (English BERT uncased unfrozen),\nbut increasing the learning rate by one order of\nmagnitude as the authors do in the paper to 5e-04.\nIn Table 12, we present the results for few-shot\nﬁnetuning using the BitFit method (Zaken et al.,\n2021) in comparison with the vanilla ﬁnetuning\nmethod (in which all model parameters are left un-\nfrozen). In the zero-shot setting and in the cases\nwhere there are a small number of few-shot ex-\namples, the two techniques perform similarly, but\nBitFit begins to underperform relative to the vanilla\nmethod with more few-shot examples.\nNum Examples\nJam\nJam-BitFit\n0\n56.00\n56.00\n1\n54.50\n55.83\n2\n56.83\n55.67\n4\n51.00\n55.83\n8\n52.17\n55.83\n16\n56.17\n55.83\n32\n61.17\n54.67\n64\n64.50\n58.00\n83\n66.17\n58.67\nTable 12: Comparison for zero-shot and few-shot ﬁne-\ntuning using BitFit and the vanilla ﬁnetuning technique.\nExperiments are averaged over three seeds, and are re-\nported on the test dataset.\nSource\nExamples\nTwitter\n634\nAnthology: Shelley Sykes-Coley\n6\nPoetry: Rt. Hon. Dr. Louise Bennett-Coverley\n4\nOnline blog\n6\nTable 13: Sources for premises in the dataset.\nLanguage\nISO\nFamily\nDev\nTest\nAymara\naym\nAymaran\n743\n750\nAsháninka\ncni\nArawak\n658\n750\nBribri\nbzd\nChibchan\n743\n750\nQuechua\nquy\nQuechuan\n743\n750\nRarámuri\ntar\nUto-Aztecan\n743\n750\nTable 14:\nLanguages used from the AmericasNLI\ndataset and the sizes of the original splits.\nHyperparameter\nValues\nBatch size\n8, 16\nLearning rate\n1e-05, 5e-05\nNumber of iterations\n100, 200\nTable 15:\nValues used for few-shot hyperparameter\nsweep. Experiments are averaged over three seeds.\nHyperparameter\nValue\nBatch size\n8\nLearning rate\n1e-05\nNumber of iterations\n100\nDropout\n0.25\nTable 16: Hyperparameters used for model type selec-\ntion. Experiments are averaged over three seeds.\nEntailment.\n(a) Given the premise, a reasonable reader would\nconclude that the hypothesis must also be true.\n(b) The hypothesis is necessarily consistent with\nthe premise.\n(c) If a speaker holds the sentiment or opinion\nexpressed in premise, then a reasonable reader\nwould conclude that they also hold the sentiment\nor opinion expressed in hypothesis.\nContradiction.\n(a) Given the premise, a reasonable reader would\nconclude that the hypothesis must be false.\n(b) The hypothesis is necessarily inconsistent\nwith the premise.\n(c) If a speaker holds the sentiment or opinion\nexpressed in premise, then a reasonable reader\nwould conclude that they do not hold the\nsentiment or opinion expressed in hypothesis.\nNeutral\n(a) Given the premise, a reasonable reader would\nconclude that the hypothesis could be either true\nor false.\n(b) The hypothesis is neither necessarily inconsis-\ntent nor necessarily consistent with the premise.\n(c) If a speaker holds the sentiment or opinion\nexpressed in premise, then a reasonable reader\nwould conclude that it may or may not be true\nthat they hold the sentiment or opinion expressed\nin hypothesis.\nFigure 4: Labelling criteria used to generate each hy-\npothesis based on the premise, and given as labelling\nguidelines to dataset validators.\nPremise\nHypothesis\nLabel\nJason mi deh cook and me nah\nmek u mek di likkle bickle\nbun up!\nJason neva eat cook food\nfrom da restaurant deh inna\nim life\nneutral\nAnd if dem tek everything\nand all mi have a my breathe ,\nmi happy same way\nNuh matta weh dem waa\ntek from mi glad as long as\nmi have life\nentailment\nMi nuh bada waa get married...\never\nMi cyaa wait ﬁget married\ncontradiction\nTable 17: Examples of negation markers in examples from each of the three classes in the dataset.\nNum Examples\naym\nbzd\ncni\nquy\ntar\njam\n0\n42.00\n44.50\n43.00\n40.50\n40.00\n56.00\n1\n42.33\n46.17\n40.33\n41.50\n38.83\n54.50\n2\n42.33\n46.83\n43.00\n41.00\n40.17\n56.83\n4\n44.33\n47.17\n42.17\n41.00\n38.67\n51.00\n8\n46.17\n45.83\n41.67\n41.17\n38.67\n52.17\n16\n47.83\n46.83\n39.50\n42.83\n41.17\n56.17\n32\n51.67\n47.67\n46.50\n43.67\n42.50\n61.17\n64\n53.67\n48.33\n49.50\n49.17\n43.67\n64.50\n83\n53.17\n49.50\n49.17\n50.67\n43.67\n66.17\nTable 18: Zero-shot and few-shot plot for the best BERT-based AmericasNLI model (mbert-cased-unfrozen)\naccuracies\nfor\neach\nlanguage\nin\nthe\ndataset\nand\nthe\nbest\nBERT-based\nJamPatoisNLI\nmodel\n(bert-uncased-unfrozen).\nExperiments are averaged over three seeds and the best models were chosen\nbased on results for the validation set.\nNum Examples\naym\nbzd\ncni\nquy\ntar\njam\n0\n42.50\n38.50\n42.50\n37.00\n37.50\n67.50\n1\n42.00\n41.00\n40.17\n37.00\n35.67\n68.17\n2\n41.00\n41.67\n42.50\n38.50\n33.83\n69.17\n4\n42.33\n41.33\n42.00\n39.00\n35.50\n66.83\n8\n43.33\n41.67\n41.67\n38.00\n35.00\n68.83\n16\n48.17\n41.33\n44.83\n43.83\n35.67\n70.17\n32\n52.33\n49.67\n45.67\n45.83\n36.83\n73.00\n64\n52.67\n49.67\n47.33\n47.00\n40.33\n76.33\n83\n50.17\n54.00\n51.33\n46.83\n41.83\n76.50\nTable 19: Zero-shot and few-shot plot for the best RoBERTa-based AmericasNLI model (xlm-unfrozen) accu-\nracies for each language in the dataset and the best RoBERTa-based JamPatoisNLI model (roberta-unfrozen).\nExperiments are averaged over three seeds and the best models were chosen based on results for the validation set.\n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "I.2.7"
  ],
  "published": "2022-12-07",
  "updated": "2022-12-07"
}