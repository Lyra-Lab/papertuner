{
  "id": "http://arxiv.org/abs/2212.03435v1",
  "title": "Improve Bilingual TTS Using Dynamic Language and Phonology Embedding",
  "authors": [
    "Fengyu Yang",
    "Jian Luan",
    "Yujun Wang"
  ],
  "abstract": "In most cases, bilingual TTS needs to handle three types of input scripts:\nfirst language only, second language only, and second language embedded in the\nfirst language. In the latter two situations, the pronunciation and intonation\nof the second language are usually quite different due to the influence of the\nfirst language. Therefore, it is a big challenge to accurately model the\npronunciation and intonation of the second language in different contexts\nwithout mutual interference. This paper builds a Mandarin-English TTS system to\nacquire more standard spoken English speech from a monolingual Chinese speaker.\nWe introduce phonology embedding to capture the English differences between\ndifferent phonology. Embedding mask is applied to language embedding for\ndistinguishing information between different languages and to phonology\nembedding for focusing on English expression. We specially design an embedding\nstrength modulator to capture the dynamic strength of language and phonology.\nExperiments show that our approach can produce significantly more natural and\nstandard spoken English speech of the monolingual Chinese speaker. From\nanalysis, we find that suitable phonology control contributes to better\nperformance in different scenarios.",
  "text": "IMPROVE BILINGUAL TTS USING DYNAMIC LANGUAGE AND PHONOLOGY\nEMBEDDING\nFengyu Yang, Jian Luan, Yujun Wang\nXiaomi AI Lab, Beijing, China\nABSTRACT\nIn most cases, bilingual TTS needs to handle three types of input\nscripts: ﬁrst language only, second language only, and second lan-\nguage embedded in the ﬁrst language. In the latter two situations,\nthe pronunciation and intonation of the second language are usually\nquite different due to the inﬂuence of the ﬁrst language. Therefore,\nit is a big challenge to accurately model the pronunciation and into-\nnation of the second language in different contexts without mutual\ninterference. This paper builds a Mandarin-English TTS system to\nacquire more standard spoken English speech from a monolingual\nChinese speaker. We introduce phonology embedding to capture the\nEnglish differences between different phonology. Embedding mask\nis applied to language embedding for distinguishing information be-\ntween different languages and to phonology embedding for focusing\non English expression. We specially design an embedding strength\nmodulator to capture the dynamic strength of language and phonol-\nogy. Experiments show that our approach can produce signiﬁcantly\nmore natural and standard spoken English speech of the monolingual\nChinese speaker. From analysis, we ﬁnd that suitable phonology\ncontrol contributes to better performance in different scenarios.\nIndex Terms: bilingual, speech synthesis, phonology, embedding\nmask, embedding strength modulator\n1. INTRODUCTION\nNowadays, a bilingual text-to-speech(TTS) system is necessary for\nmany application scenarios like voice assistant. For example, the\nnames of English songs and movies are often directly embedded in\nChinese responses. A straightforward way to build a bilingual TTS\nsystem is by collecting speech data from bilingual speakers. [1] pro-\nposed a shared hidden Markov model (HMM)-based bilingual TTS\nsystem, using a Mandarin-English corpus recorded by a bilingual\nspeaker. [2] presented a TTS system using a speaker and language\nfactorized deep neural network(DNN) with a corpus of three bilin-\ngual speakers. However, mixed-lingual corpora are scarce while a\nlarge number of monolingual corpora are easily accessible.\nAnother way is to leverage monolingual speech data from dif-\nferent speakers [3, 4, 5, 6, 7, 8]. [5] proposes a polyglot synthe-\nsis method adapting the shared HMM states to the target speaker,\ntrained on monolingual corpora. [6] proposes to factorize speaker\nand language based on an HMM-based parametric TTS system. [7]\nutilizes a combined phonetic space in two languages to build a code-\nswitched TTS system based on HMM. [8] maps the senones be-\ntween two monolingual corpora in two languages with a speaker-\nindependent DNN ASR output based on HMM TTS.\nEnd-to-end TTS systems also extend to multilingual tasks using\nmonolingual speech[9, 10, 11, 12, 13, 14, 15, 16]. [13] used Uni-\ncode bytes as a uniﬁed new language representation for multilingual\nTTS. 125 hours of speech were used and their system can read code-\nswitching text, despite the problem of speaker inconsistency when\ncross-language. [14] trained with designed loss terms preserving the\nspeaker’s identity in multiple languages based on the VoiceLoop ar-\nchitecture [17]. The trained speech is recorded by 410 monolingual\nspeakers speech from English, Spanish and German. [15] used an\nadversarial loss term to disentangle speaker identity from the speech\ncontent, which trained with 550 hours of speech from 92 monolin-\ngual speakers. Limited by corpus size, [16] proposed tone embed-\nding and tone classiﬁer for tone preservation to generate utterances\nin a proper prosodic accent of the target language.\nGenerally, each speaker speaks only one language, leading to\nspeaker and language characteristics being highly correlated. Using\nonly monolingual corpora for bilingual or multilingual TTS easily\nleads to heavy accent carry-over in synthesized speech or incon-\nsistent voice between languages. Actually, bilingual corpus helps\ndeal with the problem.\n[18] trained a TTS system transforming\nspeaker embedding between languages from a bilingual speaker to\nother monolingual speakers for a high degree of naturalness.\nIn\nthis paper, we expect to utilize scarce bilingual corpora to acquire\nmore standard spoken English from a monolingual speaker, which is\nhighly correlated with phonology learning.\nFor example, in mixed-lingual utterances, the pronunciation of\nEnglish by a non-native speaker, like Chinese, is strongly inﬂuenced\nby their native language and is most often different from the standard\nEnglish pronunciation[19]. Mandarin derives pronunciation directly\nfrom the spellings of the word with different tones, which have a\nhigh grapheme-to-phoneme(g2p) correlation. In contrast, English is\nan alphabetic and highly non-phonemic language. In Consequence,\nnative phonemic language speakers, whose pronunciation is inﬂu-\nenced by the spelling of the word, often pronounce English words\ndifferently from standard English speakers[20]. In mixed-lingual ut-\nterances, these speakers, despite qualiﬁed bilingual speakers, gen-\nerally replace some English phonemes with the closest phoneme in\ntheir native language, resulting in mispronunciation and differences\nin phonology like articulation change and intonation variation[21].\nGiven these challenges, building a state-of-the-art bilingual TTS\nsystem requires special designs handling the English differences in\nphonology between mixed-lingual and monolingual utterances. In\nthis paper, our contributions include: (1) introducing phonology em-\nbedding to capture the English differences between mixed-lingual\nand monolingual utterances; (2) proposing embedding mask to lan-\nguage embedding for distinguishing information between different\nlanguages and phonology embedding for focusing on expression\nbetween different phonology of English; (3) designing embedding\nstrength modulator(ESM) to capture the dynamic information of lan-\nguage and phonology, which helps to generate more standard spoken\nEnglish speech; (4) experiments showing that static and dynamic\ncomponents in ESM can control different attributes of phonology.\nPhonology decomposition and control can make a contribution to\nmore standard spoken English expression and better performance in\ndifferent scenarios.\narXiv:2212.03435v1  [cs.SD]  7 Dec 2022\nText sequence\nDecoder\nMel\nSpectrogram\nGMM\nAttention\nText Encoder\nGradient \nReversal\nSpeaker \nClassifier\nLanguage \nEmbedding\nSpeaker \nEmbedding\nConcat\nPhonology\nEmbedding\nEmbedding Strength \nModulator\nEmbedding Strength \nModulator\nAdd\nAdd\nAdversarial Loss\nFig. 1. Overview of the proposed bilingual architecture with spe-\ncially designed modules marked in yellow color.\n2. MODEL STRUCTURE\nFig. 1 illustrates the proposed bilingual TTS architecture.\nThe\nencoder-attention-decoder backbone with speaker and language em-\nbedding will be described in Sec. 2.1. The phonology embedding\nand specially designed masks for language and phonology embed-\nding respectively will be described in Sec. 2.2.\nThe embedding\nstrength modulator will be described in Sec. 2.3.\n2.1. Baseline\nOur baseline system adopted from [22] is a popular Tacotron2[22]-\nbased multilingual TTS architecture. It uses attention to bridge en-\ncoder and decoder. Language and speaker information are embed-\nded in separate look-up tables. They are combined with the encoder\noutput to distinguish different languages and speakers. Besides, an\nadversarially-trained speaker classiﬁer is employed to disentangle\ntext encoder output from speaker information. Mel-Lpcnet adopted\nfrom [23] is used as a vocoder to reconstruct waveform from given\nmel-spectrogram.\nThe architecture takes phoneme sequences as inputs for both En-\nglish and Mandarin. Their phoneme sets are simply concatenated\nand no phoneme is shared across. Tone or stress tokens are inserted\ninto the phoneme sequence at the end of each syllable. For Man-\ndarin, there are 4 lexicon tones and one neutral tone. Instead, there\nare 4 stress types for English includes the sentence, primary, sec-\nondary, and none. Moreover, prosodic break tokens are inserted into\nthe input sequence as well. Finally, the expanded phoneme set con-\ntains: 73 Mandarin phonemes, 39 English phonemes, 5 Mandarin\ntones, 4 English stresses, Mandarin character boundary, English syl-\nlable boundary, English liaison symbol and 4 shared prosodic break\ntypes, i.e. prosodic word (PW), prosodic phrase (PPH), intonation\nphrase (IPH) and silence at the beginning or end.\n2.2. Embedding mask\nFig. 2 shows an example of embedding mask in language and\nphonology embedding.\nInstead of broadcasting language embedding to all the tokens\nof the input sequence, the proposed method applies language em-\nbedding only to the token types shared across languages, i.e. PW,\nPPH, IPH and /sil/. Because other token types are language-speciﬁc\nalready and need no additional information to distinguish language.\nOn the other hand, to capture the English differences between\nthe mixed-lingual and monolingual utterances, a special phonology\nembedding is designed. To focus on English expression, it is applied\nsil DH AH 6 #1 K AO 7 L #2 z an 4 sil\nLanguage embedding\nPhonology embedding\nThe call赞。\nFront-end\nEncoder\nFig. 2. An illustration of how to mask embedding. Language and\nphonology embedding only applied to the highlighted position of\nencoder outputs. The symbols #1, #2, #3 and /sil/ denote 4 shared\nprosodic break types. The numbers 1-5 denote tones of the previous\nChinese syllable. The numbers 6-9 denote stresses of the previous\nEnglish syllable.\nto all English-speciﬁc tokens, including 4 types of stresses, syllable\nboundary and liaison symbol.\n2.3. Embedding strength modulator\nEven though the language and phonology embedding have been lim-\nited to only part of input tokens by masks, we think their strength\nshould vary for different contexts. To capture the dynamic strength\nof languages and phonology, we propose an attention-based embed-\nding strength modulator, whose framework is similar to [24].\nThe structure of the ESM is shown in Fig. 3. There are two sub-\nnetworks in ESM: multi-head attention and a feed-forward network.\nThe layer normalization and residual connection are applied to both\nof the sub-networks. Formally, from the encoder output with scaled\npositional encoding Eo, and the language or phonology embedding\nLP, the ﬁrst sub-network Mo and the second sub-network Fo are\ncalculated as:\nMo = MH(Eo, LN(LP), LN(LP)) + LP,\n(1)\nFo = FFN(LN(Mo)) + Mo.\n(2)\nwhere MH(query, key, value), FFN(·) and LN(·) are multi-head at-\ntention, feed-forward network and layer normalization respectively.\nSince the attention key and value (LP) have only one item, the en-\nergy need not be normalized by softmax operation. Instead, each\nhead in multi-head attention is computed by:\nheadh = αh·Vh =\nQh·Kh\n∥Qh∥∥Kh∥·Vh,\n(3)\nwhere ∥·∥is the L2 norm of the last dimension, {Q, K, V } represent\nquery, key and value through linear transformation respectively and\nthe strength α is a scaled cosine similarity between the query and\nkey to be in the range of [-1, 1].\nIn particular, there are two components in Fig. 3 marked in yel-\nlow color. The original embedding learned for each language and\nphonology is regarded as a static component. While the output of\nmulti-head attention, the static embedding multiplied by a dynamic\nweight, is regarded as a dynamic component. We will analyze the\nroles each component of language and phonology embedding play\nin Sec. 3.3.\n3. EXPERIMENTS\n3.1. Basic setups\nModels are trained with proprietary datasets composing three kinds\nof high-quality speech:\n(1) bilingual corpus from two Chinese\nLanguage or \nPhonology \nEmbedding\nMultiHead \nAttention\nLN\nLN\nFeedForward \nModule\nAdd\nAdd\nStatic Component\nDynamic Component\nEncoder\nOutputs\nScaled \nPositional \nEncoding\nFig. 3. The structure of the embedding strength modulator of both language and phonology embedding.\nspeakers, 45000 and 25000 Mandarin utterances for female and\nmale speakers respectively, 9000 mixed-lingual utterances and 9000\nEnglish utterances for both speakers; (2) English corpus from two\nAmerican speakers, 9000 and 25000 English utterances for female\nand male speakers respectively; (3) Mandarin corpus from a fe-\nmale Chinese speaker, 9000 Mandarin utterances for cross-lingual\nexperiments.\nLabels of the above corpora in language and phonology em-\nbedding are described in Tab. 1. Mandarin utterances are all from\nChinese speakers. Their language is labeled Mandarin and no En-\nglish phonology label is required. For plain English utterances, the\ncorpora recorded by both American and Chinese speakers are la-\nbeled as English for language and Standard-English for phonology.\nThese Chinese speakers pronounce English well and the corpora\nrecording by American speakers are used as supplementary English\ndatasets and are beneﬁcial to speaker learning. Particularly, since\nEnglish parts in mixed-lingual utterances are in a small amount and\nare mostly words and abbreviations with heavy Chinese phonol-\nogy, we label them Mandarin for language and Chinese-English for\nphonology, treated as Mandarin utterances.\nThe additional inputs of the learned speaker (64-dim), language\nand phonology embedding (both 512-dim same with the dimen-\nsions of encoder output) are injected into the backbone. In ESM,\nthe ﬁrst sub-network includes 8-head multi-head attention and the\nfeed-forward sub-network consists of two convolution networks\nwith 2048 and 512 hidden units. Linguistic inputs have been in-\ntroduced in Sec. 2.2 and for acoustic features, we use an 80-band\nmel-spectrogram extracted from 16kHz waveforms. We built the\nfollowing systems for comparison:\n• BASE: Baseline system with senteneial language embedding\nas described in Sec. 2.1;\n• EM: Baseline system with specially designed language and\nphonology embedding as described in Sec. 2.2;\n• ESM: Baseline system with specially designed language and\nphonology embedding through ESM as described in Sec. 2.3.\n3.2. Subjective evaluation\nWe conduct Mean Opinion Score (MOS) evaluations of speech nat-\nuralness and speaker similarity via subjective listening tests.\n20\nspeakers are asked to listen to the generated 20 English utterances\nand 10 mixed-lingual utterances. MOS results are reported in Tab. 2.\nExcept for parts of samples in listing tests, generated Mandarin de-\nmos of this monolingual speaker are also shown in demo pages1.\nWe can ﬁnd that the EM system with masked embedding brings\nbetter performance on both speech naturalness and speaker similar-\nity than the conventional BASE system. It indicates that masked em-\nbedding captures features that better represent language and phonol-\n1Samples can be found from: https://fyyang1996.github.io/esm/\nogy. For the further proposed embedding strength modulator, we\nﬁnd that by capturing the dynamic strength of language and phonol-\nogy system ESM achieves signiﬁcantly better performance than the\nEM system. It demonstrates that the dynamic strength of language\nand phonology is beneﬁcial to speech naturalness and speaker simi-\nlarity of generated speech.\n3.3. ESM component analysis\nAs mentioned above, the output of ESM may be regarded as the\ncombination of a static component and a dynamic component. One\nsimple method of analyzing the contribution of each component is\nto condition the model on only one component at each time. In\nthe generation phase, we replace the static or dynamic component\nfrom Mandarin label to English label for language embedding or\nfrom Chinese-English phonology label to Standard-English phonol-\nogy label for phonology embedding respectively. Fig. 4 shows the\nspectrogram and F0 contour, extracted by parselmouth[25], of the\nsame sentence synthesized with six kinds of label combinations as\ndescribed below:\n(a) Base combination:\nusing Mandarin and Chinese-English\nphonology labels both in dynamic and static components;\n(b) Reference combination: using English and Standard-English\nphonology labels both in dynamic and static components;\n(c) Based on (a), replacing dynamic phonology embedding from\nChinese-English to Standard-English phonology.\n(d) Based on (a), replacing static phonology embedding from\nChinese-English to Standard-English phonology;\n(e) Based on (a), replacing dynamic language embedding from\nMandarin to English;\n(f) Based on (a), replacing static language embedding from Man-\ndarin to English;\nEmpirically, we ﬁnd that each component represents articula-\ntion, intonation, speaking rate and pause duration changes respec-\ntively, which inﬂuence phonology collectively. Listing to the sam-\nples of (a) and (c) in the demo page, we can easily hear about artic-\nulation changes between them, which is difﬁcult to be caught sight\nof. Perceptually, the trend of F0 values in Fig. 4(d) is different from\nthat in Fig. 4(a), showing that static phonology embedding major af-\nfects intonation. Fig. 4(e) shows that replacing the dynamic language\nembedding from Mandarin to English causes a gradual compression\nof the spectrogram and F0 values in the time domain. We believe\nthat the dynamic language embedding encodes the information cor-\nrelated with speaking rate variation. Besides, syllables in Fig. 4(f)\nhave distinct intervals compared with that in Fig. 4(a), which demon-\nstrates that static language embedding represents the average dura-\ntion of pauses. More demos and be found in the demo page.\nTable 1. Labels of trained corpora in language and phonology embedding.\nCorpus\nLanguage embedding\nPhonology embedding\n(1) Chinese speaker\nTrain\nMandarin\nMandarin\nNone\nMixed-lingual\nMandarin\nChinese-English\nEnglish\nEnglish\nStandard-English\n(2) American speaker\nTrain\nEnglish\nEnglish\nStandard-English\n(3) Chinese speaker\nTest\nMandarin\nMandarin\nNone\nTable 2. The MOS of different systems with conﬁdence intervals of\n95%.\nModel\nBASE\nEM\nESM\nNaturalness\n3.81±0.12\n4.03±0.10\n4.39±0.08\nSimilarity\n3.79±0.12\n3.91±0.11\n4.04±0.10\n0\n50\n100\n150\n200\n250\n300\nFrame\n0\n10\n20\n30\n40\n50\n60\n70\n80\nMel channel\n150\n175\n200\n225\n250\n275\n300\n325\n350\nFundamental frequency [Hz]\n(a) Base combination\n0\n50\n100\n150\n200\n250\n300\nFrame\n0\n10\n20\n30\n40\n50\n60\n70\n80\nMel channel\n150\n175\n200\n225\n250\n275\n300\n325\n350\nFundamental frequency [Hz]\n(b) Reference combination\n0\n50\n100\n150\n200\n250\n300\nFrame\n0\n10\n20\n30\n40\n50\n60\n70\n80\nMel channel\n150\n175\n200\n225\n250\n275\n300\n325\n350\nFundamental frequency [Hz]\n(c) Dynamic phonology embedding\n0\n50\n100\n150\n200\n250\n300\nFrame\n0\n10\n20\n30\n40\n50\n60\n70\n80\nMel channel\n150\n175\n200\n225\n250\n275\n300\n325\n350\nFundamental frequency [Hz]\n(d) Static phonology embedding\n0\n50\n100\n150\n200\n250\n300\nFrame\n0\n10\n20\n30\n40\n50\n60\n70\n80\nMel channel\n150\n175\n200\n225\n250\n275\n300\n325\n350\nFundamental frequency [Hz]\n(e) Dynamic language embedding\n0\n50\n100\n150\n200\n250\n300\nFrame\n0\n10\n20\n30\n40\n50\n60\n70\n80\nMel channel\n150\n175\n200\n225\n250\n275\n300\n325\n350\nFundamental frequency [Hz]\n(f) Static language embedding\nFig. 4. Spectrogram and F0 of a test sentence generated by different\ncombinations, which refers to 2.1 in demo page.\n3.4. Control\nTo validate the above analysis, we conduct MOS evaluations of\nspeech naturalness and speaker similarity via subjective listening\ntests. 20 speakers are asked to listen to the generated 15 English ut-\nterances for enhancing English expressiveness and 15 mixed-lingual\nutterances for smooth mixed-lingual transition. Demos can be found\nin 3 and 4 on the demo pages.\nEnhance expressiveness To enhance the expressiveness of a\nplain English text, we double the dynamic components of both lan-\nguage and phonology embedding while remaining their static com-\nponents. The ”double” herein means that the ﬁnal vector has a dou-\nble distance of the reference vector to the base vector. For language\nembedding, the reference is English and the base is Mandarin. While\nfor phonology embedding, the reference is Standard English and the\nbase is Chinese-English. Fig. 5 shows the results of MOS evalua-\n27%\n36%\n26%\n4%\n47%\n60%\nSimilarity\nNaturalness\nProposed\nNo Preference\nControlled\nFig. 5. A/B preference results for control in enhancing expressive-\nness or not with conﬁdence intervals of 95% and p-value<0.0001\nfrom the t-test.\n17%\n22%\n30%\n27%\n53%\n51%\nSimilarity\nNaturalness\nProposed\nNo Preference\nControlled\nFig. 6. A/B preference results for control in smooth transition or\nnot with conﬁdence intervals of 95% and p-value<0.0001 from the\nt-test.\ntions. We ﬁnd that by the ”double” operation herein system ESM\nachieves signiﬁcantly better performance than the ESM system on\nspeech naturalness. It indicates that the control operation enhances\nEnglish expressiveness signiﬁcantly.\nSmooth transition\nWhen synthesizing a mixed-lingual text,\nwe modify the language labels of embedded English words from\nMandarin to English while phonology labels of them from Chinese-\nEnglish to Standard-English.\nParticularly, their static component\nof phonology embedding remains Chinese-English.\nIn this way,\nthe English words will have standard-English articulation but more\ncompatible intonation with the context of Chinese words. Fig. 6\nshows the results of MOS evaluations.\nIt can be found that the\ncontrolled ESM system brings better performance on both speech\nnaturalness and speaker similarity than the proposed ESM system.\nIt demonstrates that the control operation is beneﬁcial to smooth\nmixed-lingual transition.\n4. CONCLUSIONS\nThis paper builds a Mandarin-English TTS system for a monolingual\nChinese speaker. We introduce phonology embedding and a special\ndesigned mask for language and phonology embedding. They are\nemployed to distinguish two languages and the English phonological\ndifferences between monolingual and embedded cases respectively.\nFurthermore, the proposed embedding strength modulator enables\nlanguage and phonology embedding to be variable with token con-\ntext. Experiments show that our approach can produce signiﬁcantly\nmore natural and standard spoken English speech than baseline. Ab-\nlation analysis on different components demonstrates that English\nphonology can be tuned effectively for various scenarios.\n5. REFERENCES\n[1] Yao Qian, Hui Liang, and Frank K Soong, “A cross-language\nstate sharing and mapping approach to bilingual (mandarin–\nenglish) tts,” IEEE Transactions on Audio, Speech, and Lan-\nguage Processing (TASLP), vol. 17, no. 6, pp. 1231–1239,\n2009.\n[2] Yuchen Fan, Yao Qian, Frank K Soong, and Lei He, “Speaker\nand language factorization in dnn-based tts synthesis,” in 2016\nIEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP). IEEE, 2016, pp. 5540–5544.\n[3] Javier Latorre, Koji Iwano, and Sadaoki Furui, “New approach\nto the polyglot speech generation by means of an hmm-based\nspeaker adaptable synthesizer,” Speech Communication, vol.\n48, no. 10, pp. 1227–1242, 2006.\n[4] Ivan Himawan, Sandesh Aryal, Iris Ouyang, Sam Kang, Pierre\nLanchantin, and Simon King, “Speaker adaptation of a multi-\nlingual acoustic model for cross-language synthesis,” in 2020\nIEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP). IEEE, 2020, pp. 7629–7633.\n[5] Javier Latorre, Koji Iwano, and Sadaoki Furui,\n“Polyglot\nsynthesis using a mixture of monolingual corpora,” in 2005\nIEEE International Conference on Acoustics, Speech, and Sig-\nnal Processing (ICASSP). IEEE, 2005, pp. I–1.\n[6] Heiga\nZen,\nNorbert\nBraunschweiler,\nSabine\nBuchholz,\nMark JF Gales, Kate Knill, Sacha Krstulovic, and Javier La-\ntorre, “Statistical parametric speech synthesis based on speaker\nand language factorization,”\nIEEE Transactions on Audio,\nSpeech, and Language Processing (TASLP), vol. 20, no. 6, pp.\n1713–1724, 2012.\n[7] Sunayana Sitaram, Sai Krishna Rallabandi, Shruti Rijhwani,\nand Alan W Black, “Experiments with cross-lingual systems\nfor synthesis of code-mixed text.,” in SSW, 2016, pp. 76–81.\n[8] Feng-Long Xie, Frank K Soong, and Haifeng Li, “A kl di-\nvergence and dnn approach to cross-lingual tts,” in 2016 IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). IEEE, 2016, pp. 5515–5519.\n[9] Yuewen Cao, Xixin Wu, Songxiang Liu, Jianwei Yu, Xu Li,\nZhiyong Wu, Xunying Liu, and Helen Meng,\n“End-to-end\ncode-switched tts with mix of monolingual recordings,”\nin\n2019 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2019, pp. 6935–6939.\n[10] Liumeng Xue, Wei Song, Guanghui Xu, Lei Xie, and Zhizheng\nWu,\n“Building a mixed-lingual neural tts system with only\nmonolingual data,” in 2019 Annual Conference of the Interna-\ntional Speech Communication Association (INTERSPEECH).\nISCA, 2019.\n[11] Yuewen Cao, Songxiang Liu, Xixin Wu, Shiyin Kang, Peng\nLiu, Zhiyong Wu, Xunying Liu, Dan Su, Dong Yu, and Helen\nMeng, “Code-switched speech synthesis using bilingual pho-\nnetic posteriorgram with only monolingual corpora,” in 2020\nIEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP). IEEE, 2020, pp. 7619–7623.\n[12] Shengkui Zhao, Trung Hieu Nguyen, Hao Wang, and Bin Ma,\n“Towards natural bilingual and code-switched speech synthe-\nsis based on mix of monolingual recordings and cross-lingual\nvoice conversion,” in 2020 Annual Conference of the Interna-\ntional Speech Communication Association (INTERSPEECH).\n2020, pp. 2927–2931, ISCA.\n[13] Bo Li, Yu Zhang, Tara Sainath, Yonghui Wu, and William\nChan, “Bytes are all you need: End-to-end multilingual speech\nrecognition and synthesis with bytes,” in 2019 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2019, pp. 5621–5625.\n[14] Eliya Nachmani and Lior Wolf, “Unsupervised polyglot text-\nto-speech,” in 2019 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp.\n7055–7059.\n[15] Yu Zhang, Ron J Weiss, Heiga Zen, Yonghui Wu, Zhifeng\nChen, RJ Skerry-Ryan, Ye Jia, Andrew Rosenberg, and Bhu-\nvana Ramabhadran, “Learning to speak ﬂuently in a foreign\nlanguage: Multilingual speech synthesis and cross-language\nvoice cloning,” in 2019 Annual Conference of the International\nSpeech Communication Association (INTERSPEECH). ISCA,\n2019, pp. 2080–2084.\n[16] Ruolan Liu, Xue Wen, Chunhui Lu, and Xiao Chen, “Tone\nlearning in low-resource bilingual tts.,” in 2020 Annual Con-\nference of the International Speech Communication Associa-\ntion (INTERSPEECH). ISCA, 2020, pp. 2952–2956.\n[17] Yaniv Taigman, Lior Wolf, Adam Polyak, and Eliya Nach-\nmani,\n“Voiceloop: Voice ﬁtting and synthesis via a phono-\nlogical loop,” in 2018 International Conference on Learning\nRepresentations (ICLR), 2018.\n[18] Soumi Maiti, Erik Marchi, and Alistair Conkie, “Generating\nmultilingual voices using speaker space translation based on\nbilingual speaker data,” in 2020 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2020, pp. 7624–7628.\n[19] Younggun Lee, Suwon Shon, and Taesu Kim, “Learning pro-\nnunciation from a foreign language in speech synthesis net-\nworks,” arXiv preprint arXiv:1811.09364, 2018.\n[20] Arun Baby, Pranav Jawale, Saranya Vinnaitherthan, Sumukh\nBadam, Nagaraj Adiga, and Sharath Adavanne, “Non-native\nenglish lexicon creation for bilingual speech synthesis,” arXiv\npreprint arXiv:2106.10870, 2021.\n[21] Ji He, Yao Qian, Frank K Soong, and Sheng Zhao, “Turning\na monolingual speaker into multilingual for a mixed-language\ntts,” in 2012 Annual Conference of the International Speech\nCommunication Association (INTERSPEECH). 2012, pp. 963–\n966, ISCA.\n[22] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster,\nNavdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang,\nYuxuan Wang, Rj Skerrv-Ryan, et al., “Natural tts synthesis\nby conditioning wavenet on mel spectrogram predictions,” in\n2018 IEEE international conference on acoustics, speech and\nsignal processing 1(ICASSP). IEEE, 2018, pp. 4779–4783.\n[23] Jean-Marc Valin and Jan Skoglund, “Lpcnet: Improving neu-\nral speech synthesis through linear prediction,” in 2019 IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). IEEE, 2019, pp. 5891–5895.\n[24] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin\nZheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang,\nand Tieyan Liu, “On layer normalization in the transformer\narchitecture,” in 2020 International Conference on Machine\nLearning (ICML). PMLR, 2020, pp. 10524–10533.\n[25] Yannick Jadoul, Bill Thompson, and Bart De Boer,\n“Intro-\nducing parselmouth: A python interface to praat,” Journal of\nPhonetics, vol. 71, pp. 1–15, 2018.\n",
  "categories": [
    "cs.SD",
    "cs.CL",
    "eess.AS"
  ],
  "published": "2022-12-07",
  "updated": "2022-12-07"
}