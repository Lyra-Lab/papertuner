{
  "id": "http://arxiv.org/abs/2212.06123v1",
  "title": "A Survey on Reinforcement Learning Security with Application to Autonomous Driving",
  "authors": [
    "Ambra Demontis",
    "Maura Pintor",
    "Luca Demetrio",
    "Kathrin Grosse",
    "Hsiao-Ying Lin",
    "Chengfang Fang",
    "Battista Biggio",
    "Fabio Roli"
  ],
  "abstract": "Reinforcement learning allows machines to learn from their own experience.\nNowadays, it is used in safety-critical applications, such as autonomous\ndriving, despite being vulnerable to attacks carefully crafted to either\nprevent that the reinforcement learning algorithm learns an effective and\nreliable policy, or to induce the trained agent to make a wrong decision. The\nliterature about the security of reinforcement learning is rapidly growing, and\nsome surveys have been proposed to shed light on this field. However, their\ncategorizations are insufficient for choosing an appropriate defense given the\nkind of system at hand. In our survey, we do not only overcome this limitation\nby considering a different perspective, but we also discuss the applicability\nof state-of-the-art attacks and defenses when reinforcement learning algorithms\nare used in the context of autonomous driving.",
  "text": "A Survey on Reinforcement Learning Security with Application to\nAutonomous Driving\nAMBRA DEMONTIS, DIEE, University of Cagliari, Italy\nMAURA PINTOR, DIEE, University of Cagliari and Pluribus One, Italy\nLUCA DEMETRIO, DIBRIS, University of Genova and Pluribus One, Italy\nKATHRIN GROSSE, VITA Lab, École Polytechnique Fédéerale de Lausanne, Switzerland\nHSIAO-YING LIN, Huawei Technologies France, France\nCHENGFANG FANG, Huawei International, Singapore\nBATTISTA BIGGIO, DIEE, University of Cagliari and Pluribus One, Italy\nFABIO ROLI, DIBRIS, University of Genoa and Pluribus One, Italy\nReinforcement learning allows machines to learn from their own experience. Nowadays, it is used in safety-critical applications,\nsuch as autonomous driving, despite being vulnerable to attacks carefully crafted to either prevent that the reinforcement learning\nalgorithm learns an effective and reliable policy, or to induce the trained agent to make a wrong decision. The literature about the\nsecurity of reinforcement learning is rapidly growing, and some surveys have been proposed to shed light on this field. However, their\ncategorizations are insufficient for choosing an appropriate defense given the kind of system at hand. In our survey, we do not only\novercome this limitation by considering a different perspective, but we also discuss the applicability of state-of-the-art attacks and\ndefenses when reinforcement learning algorithms are used in the context of autonomous driving.\nCCS Concepts: • Computing methodologies →Machine learning; • Security and privacy;\nAdditional Key Words and Phrases: Survey, Reinforcement Learning, Autonomous Driving, Security, Adversarial Machine Learning\nACM Reference Format:\nAmbra Demontis, Maura Pintor, Luca Demetrio, Kathrin Grosse, Hsiao-Ying Lin, Chengfang Fang, Battista Biggio, and Fabio Roli. 2021.\nA Survey on Reinforcement Learning Security with Application to Autonomous Driving. In Woodstock ’18: ACM Symposium on Neural\nGaze Detection, June 03–05, 2018, Woodstock, NY. ACM, New York, NY, USA, 28 pages. https://doi.org/10.1145/1122445.1122456\n1\nINTRODUCTION\nReinforcement learning (RL) is a process that teaches machines to perform a task using a reward-based learning system,\nwhich is a technique also exploited by the human brain [17]. The machine tries to perform an action, and if it gets a\npositive (negative) reward/outcome, it will be more (less) likely to repeat that action in the future.\nFor its ability to solve complex, dynamic, and high-dimensional problems [60], RL is used in many applications.\nNotably, it is used even in the high-risk, high-value ones such as cybersecurity [60] and autonomous driving [41]. The\nfirst use RL to detect and fight against sophisticated cyberattacks such as falsified data injection in cyber-physical\nsystems [98], intrusion to host computers or networks [51] and malware [91]. While the latter makes extensive use of\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2021 Association for Computing Machinery.\nManuscript submitted to ACM\n1\narXiv:2212.06123v1  [cs.LG]  12 Dec 2022\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\nRL to accomplish different tasks such as path planning and the development of high-level driving policies for complex\nnavigation tasks [41].\nUnfortunately, RL is vulnerable to different types of attacks [36] that can alter the agents’ behavior. For example,\nan autonomous driving car that uses RL to establish its driving policies can be induced to cause a car crash or traffic\njam. As RL is frequently applied in mission-critical tasks like those mentioned above, its security is of great concern,\nand the literature about this topic is rapidly proliferating. Different surveys [4, 14, 36] have been published on this\ntopic. However, most of them [4, 14] only consider earlier, pioneering works, while the most recent taxonomy provided\nin [36] does not allow system designers to understand which defenses can be applied to protect a given RL model (based\non its characteristics) against a particular attack. Our survey aims to overcome this shortcoming. The first contribution\nprovided by our survey is a categorization that allows system designers to understand which defense they can apply\nto defend the system at hand against a precise threat. The second is to categorize all the literature about attacks and\ndefenses against RL (more than 50 papers) accordingly. Our third contribution is to examine to which extent the attacks\nand defenses proposed against RL can be applied to the autonomous-driving systems, motivated by the fact that the\nlatter is a safety-critical application. Finally, we explain that attacks and defenses against RL are strongly inspired\nby those devised many years ago against machine-learning classifiers [7]. However, RL has peculiarities that make\nthe application of these attacks not straightforward. For this reason, only a subset of the attacks previously proposed\nagainst classifiers has been tested against RL. This parallel allows us to provide our fourth contribution: a discussion\nabout open challenges and interesting research directions about the security of RL.\nWe start our review in Sect. 2, providing the reader with the background about reinforcement learning essential\nto understanding how those algorithms work and, consequently, how they can be threatened. For the same reason,\nin Sect. 3, we highlight the peculiarities of reinforcement learning algorithms used for autonomous driving. Then,\nin Sect. 4 we present a threat model that encompasses all literature about the security of reinforcement learning. In\nSect. 5 (6), we explain how attacks (defenses) can be categorized, and we present a taxonomy of the state-of-the-art\nattacks (defenses). In this sections, we review all the works about the security of reinforcement learning, discuss their\napplicability to systems developed for autonomous driving, and highlight future research directions.\nTo summarize, with this work, we provide the following contributions:\n• we propose a categorization of attacks and defenses that allows matching, for the RL system at hand, specific\nattacks with appropriate defenses;\n• we categorize more than 50 papers about the security of reinforcement learning;\n• we discuss the applicability of state-of-art attacks and defenses to RL algorithms in the context of autonomous\ndriving;\n• comparing work on the security of RL with that of ML systems, to which the first is highly inspired, we highlight\nopen challenges and interesting research directions.\n2\nREINFORCEMENT LEARNING\nReinforcement learning is a methodology that allows learning from experience. This mechanism is unconsciously used\nevery day by our brain. Humans perform actions, and if they get a positive outcome, they will repeat them in the future,\nexpecting to receive the favorable outcome again. In contrast, if these actions have an unpleasant outcome, they are\nless likely to repeat them to avoid receiving that outcome again [17].\n2\nA Survey on Reinforcement Learning Security\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nState:\ncars’ positions\nstreet signals \nlane borders\n….\nActions:\nturn right\nturn left\naccelerate\ndecelerate\nCar\n(Ego) Agent\nObservations\nSensors\nEnvironment \nReward\nFig. 1. Conceptual representation of a reinforcement learning system, considering autonomous driving as a paradigmatic example.\nThis technique has been successfully used also to teach machines to perform complex tasks in dynamic environments.\nFor example, it is used to train autonomous driving cars to drive through traffic [41], which is a highly challenging\nproblem. While learning to drive, there are, in fact, many challenges to face, such as predicting and taking into account\nthe behavior of the other cars, even when they are violating some rules, e.g., passing despite the red light.\nIn this section, we provide the reader with the background about RL. First, we describe the components of the RL\nsystem. Then, we explain how state-of-the-art systems can be categorized.\n2.1\nComponents of Reinforcement Learning Systems\nIn the following, we will describe the RL system components and explain how they interplay. The reader can find them\nillustrated in Figure 1. While describing them, we also start presenting the notation that we will use in this survey,\nwhich is summarized in Table 1.\nAgent. An agent is an entity that lives in and influences the considered scenario. For example, if we consider the\napplication of autonomous driving, each car driving on the street and each pedestrian walking on it will be an agent.\nThe agent we aim to teach something using RL techniques is usually called the ego agent.\nAction. An action 𝑎is a move an agent can make in the considered scenario. When the time in which the agent\nperforms the action is relevant, we will denote the action performed at time 𝑡as 𝑎𝑡. This action can be chosen between\na set of all possible actions the attacker can make. For example, turn right, turn left, accelerate, decelerate, and keep\ngoing without making any changes. The set of possible actions is usually named the Action Space, and denoted as A.\nEnvironment. The environment E is the considered scenario and thus the place where the agents live; e.g., the street\nwhere cars and pedestrians move on. Besides the agents, the environment can contain other elements relevant to the\nego agent; e.g., street signals and traffic lights.\nState. The state s is a collection of values that encode the environment’s conditions. For example, the position of the\nagents in it, the color of the traffic lights, etc. The agents’ actions usually alter the environment state. For example,\nwhen a car turns right, its position will change (and thus the environment state which includes it). We denote as s𝑡+1\nthe state in which the environment is induced when the agent performs the action 𝑎𝑡.\n3\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\nReward. The reward 𝑟is the feedback that can measure the success of the agent’s action and can be obtained by\nthe environment. For example, the ego car would receive a positive reward if an action reduces the distance from\nthe desired destination and a negative reward if it collides with another car or with the guardrails. For the action 𝑎𝑡\nperformed at time 𝑡, the ego agent will receive the reward denoted with 𝑟𝑡.\nObservations. The state and the reward are information that the ego agent perceives from the environment through\nits sensors. The state and reward perceived in this way by the agent are called observations (o). They may or not\ncoincide exactly with the actual state of the environment. For example, the agent sensors might not be precise enough\nto perceive the state correctly.\nReturn. When choosing the action 𝑎𝑡to perform, the ego agent should care not only about the reward that they will\nreceive immediately (𝑟𝑡) but also about the ones that they will receive in the future. To this end, they can consider the\nreturn, which we denote with R, given as the sum of the rewards they will get from time 𝑡until the future (until an\ninfinite amount of time): R𝑡= Í∞\n𝑖=𝑡𝑟𝑖. Often, we would like to make the agent learn to accomplish the task as soon as\npossible, e.g., reaching the destination by traveling for fewer kilometers. To this end, we can consider the discounted\nsum of the rewards as a return. The discounted factor, which we denote as 𝛾, is a number 0 ≤𝛾≤1 multiplied by\nthe future rewards and chosen to make the future rewards less important than future rewards enforcing short-term\nlearning. The return will be thus R𝑡= Í∞\n𝑖=𝑡𝛾𝑖𝑟𝑖.\nQ-function. As we have explained, the agent, when choosing the action to perform, should take into account the\nreturn. However, they usually do not know in advance which reward they will get from the environment when they\ndo an action 𝑎, while the environment state is s𝑡(and thus they cannot know in advance the return). The agent\ncan only estimate the return with a function that is usually called Q-function. The Q-function can be written as\n𝑄𝜋(s𝑡,𝑎) = E(R𝑡|s𝑡,𝑎), i.e., the expected return starting from state s𝑡and taking action 𝑎∈A(s𝑡).\nPolicy. The policy (𝜋) is the strategy (behavior) adopted by the agent. For example, a policy might prescribe halting\nthe ego car whenever the agent observes a red light at a crossing. The policy that led the agent to choose the action\n𝑎𝑖∈A(s𝑡) that, given the current state s𝑡, maximizes the Q-function is usually called optimal policy 𝜋∗.\nValue function. The value function (V) is a prediction of the future reward, starting from state s𝑡, and following policy\n𝜋. Therefore, can be used to evaluate the goodness of a policy 𝜋. It is obtained by computing the expectation of the dis-\ncounted rewards, i.e., the return R𝑡, from time step 𝑡over all actions according to the policy 𝜋: V𝜋(s) = E𝜋[R𝑡|s𝑡]. Thus,\nthe relationship between the value function and the Q-function can be expressed as V𝜋(s) = Í\n𝑎∈A(s𝑡) 𝜋(𝑎|s)𝑄𝜋(s,𝑎),\ni.e., the sum of the Q-functions of all actions that can be taken at state s𝑡, multiplied by the probability of taking the\naction given by the policy 𝜋.\n2.2\nSolving the Reinforcement Learning Problem\nThe problem of teaching the agent to accomplish a task can thus be formulated as the problem of finding the optimal\npolicy. The general formulation treats the problem as a Markov decision process (MDP). An MDP is a reinforcement\n4\nA Survey on Reinforcement Learning Security\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nTable 1. Summary of the notation and abbreviations used throughout the paper.\nNotation\nMeaning\nNotation\nMeaning\nReinforcement Learning\n𝑎\nAction\nR\nReturn\nE\nEnvironment\n𝛾\nDiscount factor\ns\nState\n𝜋\nPolicy\n𝑟\nReward\nV\nValue function\no\nObservations\nM\nModel\nAttack\n𝑐(𝑥)\nThe attacker changes some inputs 𝑥\n𝑚(𝑥)\nThe attacker monitors some inputs/outputs 𝑥\n∅\nAttack uses no agent\n≈M\nAttack’s agent is a surrogate (copy of the target) agent\n∈E\nAttack’s agent is within the environment\n∉E\nAttack uses agent that is not part of the environment\nreach(s𝑇)\nMake E reach a target state s𝑇\nmin(R)\nMinimize the return\nlearn(𝜋𝑇)\nMake the agent learn a target policy\nlearn(𝜋𝑇s )\nMake the agent learn to use a target policy for a subset of observations\nlearn(𝜋𝐵)\nMake the agent learn a policy containing a backdoor\nsteal(M)\nAttacker copies model M without consent\nDefense Methods\nAT\nAdversarial training\nreg.\nRegularization\ndist.\nDistillation\ndet.\nDetection\nsan.\nSanitization\nlearning task that satisfies the Markov property, i.e., the current state retains all relevant information from the past\nstates. Formally, a state s𝑡is Markov if and only if P[s𝑡+1|s𝑡] = P[s𝑡+1|s1, . . . , s𝑡]. In simpler words, the environment’s\nresponse at time 𝑡+ 1 depends only on the state s𝑡and action 𝑎𝑡(state and action at time 𝑡), independently of how the\npast history of states and actions led to s𝑡. If the state and action spaces are finite, the problem is called a finite Markov\ndecision process (finite MDP). The Bellman equation for V𝜋expresses the relationship between the value of a state and\nthe values of its successor states. It is expressed as a recursive function:\nV𝜋(s𝑡) =\n∑︁\n𝑎∈A(s𝑡)\n𝜋(𝑎|s𝑡)\n∑︁\ns′,𝑟\n𝑝(s′,𝑟|s𝑡,𝑎)[𝑟+ 𝛾V𝜋(s′)],\n(1)\nand it can be interpreted as a sum over all the values of the three variables, 𝑎, s, and 𝑟(where we use the notation s′\nfor denoting the states that can be reached from state s𝑡). For each set of these values, we compute the probability,\n𝜋(𝑎|s𝑡)𝑝(s′,𝑟|s𝑡,𝑎), and weight the reward expected along the future states by these probabilities. Solving a reinforce-\nment learning task means finding the policy that achieves the highest reward over time. There is always at least one\npolicy that is better than or equal to all other policies, and it is called the optimal policy 𝜋∗. The Bellman optimality\nequation indicates that the value of a state under an optimal policy must equal the expected return for the best action\nfrom that state:\nV∗\n𝜋(s𝑡) = max\n𝜋\nV𝜋(s𝑡) =\nmax\n𝑎∈A(s𝑡)\n∑︁\ns′,𝑟\n𝑝(s′,𝑟|s𝑡,𝑎)[𝑟+ 𝛾V∗(s′)].\n(2)\nThis equation, if solved explicitly, gives the optimal policy and thus provides the solution to the reinforcement\nlearning task. Once the optimal value function V∗is known, the optimal policy can be found by performing a one-step-\nahead search, i.e., by comparing all the values of the states reached by the actions available at state s𝑡, and picking the\none that maximizes it (i.e., which has the optimal value V∗(s𝑡+1)). Having the optimal Q-function 𝑄∗makes choosing\noptimal functions even easier. Once knowing 𝑄∗, the optimal policy can be found by assigning non-zero probabilities\nonly to the actions that have the maximum Q-function (𝑄∗(s𝑡,𝑎)). At this point, the utility of the value and Q-functions\nshould become clear; in particular, they allow expressing the optimal long-term returns as quantities available locally\nand immediately at each state.\n5\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\nApproximations. For finite MDPs, the Bellman optimality equation can be solved by defining a system of equations\nwith one equation for each state. For non-finite MDPs, however, this solution makes assumptions that are not always\npossible in real cases. The assumptions that are often difficult to satisfy are: (i) the environment is perfectly known in\nits dynamics; (ii) the problem can be solved with the available computational resources; and (iii) the problem satisfies\nthe Markov property. Among these three assumptions, the second one is the most problematic. In real scenarios, indeed,\nthe number of states might be intractable (or infinite). For example, the number of states in the game of Backgammon is\nabout 1020, and in autonomous driving it is usually infinite. This makes the MDP problem computationally unsolvable.\nTypically, in these cases, the problem is simplified with approximations that reduce the search space [83]. MDPs are\nthus usually solved with different learning algorithms that approximate the value and the Q-functions (by using linear\ncombinations of features, deep neural networks, or other functions) and attempt to generalize to unseen states (by\nobserving similarities from states seen in training). Other simplifications are given, for example, by setting an upper\nbound to the maximum number of states, i.e., by limiting the number of RL steps for which the agent can perform its\nactions. A taxonomy of these algorithms will be detailed in Section 2.3.\n2.3\nCategorization of Reinforcement Learning Systems\nIn this section, we explain how the state-of-the-art algorithms for reinforcement learning can be categorized depending\non their features, and we discuss exemplary approaches of each category.\nSingle-Agent versus Multi-Agent RL. Depending on the number of agents whose actions influence the environment,\nRL is denoted as single-agent if a single agent alters the environment, i.e., it can change its state, and multi-agent if\nmultiple agents influence the environment. Multi-agent systems are thus far more complex, as the behavior of many\ndifferent agents alters the environment and thus has an influence on the ego agent. An example in which the ego car\nshould be modeled within other cars on the road is the problem of modeling stop-and-go behavior during potential\ntraffic jams [94].\nModel-based versus Model-free RL. The environment of an RL agent can be explicitly modeled or implicitly\nlearned [39]. RL is considered as model-based if it creates a model of the environment, which in the following we will\ndenote as M, and model-free otherwise. Model-based RL learns to predict the reward and the next state and bases its\ndecisions on these predictions. Model-free RL, instead, does not try to model the environment, as it bases its decisions\nonly on the information obtained by interacting with the environment. For example, tabular certainty-equivalence\n(TCE) [54] models the environment as a Markov decision process or as a graph with associated actions, transition proba-\nbilities, and rewards. TCE is thus a model-based RL approach, where the policy is derived from this environment model.\nOn the other hand, most recent works, including deep-learning-based RL such as deep Q-networks (DQN) [57, 103]\nor proximal policy optimization (PPO) [28, 77] are model-free: they do not estimate explicit transition probabilities\nof the environment. Instead, they often model only the options available from the current state without modeling\nfuture possibilities in detail. The transitions are thus implicitly estimated, as opposed to explicitly modeled as in TCE,\nfor example. In this sense, model-based approaches have the advantage of allowing the agent to think ahead or plan.\nHowever, this comes at the price of larger sample complexity, as building a model is costly in terms of the need for data\nabout the environment. Modeling the environment can also present a challenge when this model diverges from the\nreal environment. This is a particularly relevant problem in autonomous driving, as traffic might be too complex to be\nexplicitly modeled. Consequently, most works considered in this survey are model-free approaches.\nPolicy Updates. When learning a policy, the agent interacts with the environment with a given frequency. Depending\non the frequency with which the policy is updated, RL is referred to as: (1) on-policy RL, when the policy is updated after\n6\nA Survey on Reinforcement Learning Security\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\neach interaction with the environment; (2) off-policy RL, when the policy is updated sporadically, i.e., after different\niterations with the environment; and (3) offline RL, when the agent is just presented with collected data without being\nable to interact with the environment [68]. For example, PPO [28, 77] is on-policy, and the learned policy is directly\napplied to the environment and used to take actions and thus sample further data from the environment. In contrast,\nDQN [57, 103] is an off-policy approach. Here, the policy can also be updated using data from previous policies, which\nwere different from the current policy. In this sense, all the interactions seen are saved and can be used for learning,\nleading to offline learning. Almost all algorithms considered in this survey are either on- or off-policy, except the\nplanning-based offline agent studied by Rakhsha et al. [72]. Due to the lack of evaluation benchmarks, offline RL is, in\nfact, the least used, although it is considered promising as it allows to take advantage of large, existing datasets [25].\n3\nREINFORCEMENT LEARNING FOR AUTONOMOUS DRIVING\nOne possible application area for RL is autonomous driving. This setting favors certain RL systems, whereas other\npreviously-discussed properties map to specific implementations of autonomous driving. In this section, we discuss\nthese relations in detail. In the first subsection, we set the previously introduced categorization of RL in relation to\nautonomous driving tasks. In the second subsection, we discuss the components of an autonomous car to understand\nhow RL is applied and how RL potentially interacts with additional components within the car.\n3.1\nDriving Automation and Reinforcement Learning Approaches\nIn the previous section, we categorized RL along single-agent or multi-agent systems, the kind of modelling, and the\nway the policy is updated. To understand how these categorizations relate to autonomous driving tasks, we first need\nto understand how cars are automated. To this end, we review the six levels of automation in driving as defined by the\nSociety of Automotive Engineers (SAE) [75]. We will start with the lowest level and progress towards more autonomy.\nLevel 0. No automation, but automatic systems may provide warnings or momentary assistance. Examples are\nlane departure warnings or emergency braking systems.\nLevel 1. The driver monitors the environment and controls the car, but the system supports the driver by either\nbreaking or accelerating. Examples include lane-centering or cruise control.\nLevel 2. The driver monitors the environment and controls the car, but the system supports the driver by\nbreaking and accelerating. Examples include lane centering and simultaneous cruise control.\nLevel 3. The system controls the car, but the driver may be requested to take over control at any time. Examples\ninclude a traffic jam chauffeur.\nLevel 4. The system controls the car without any intervention by a human, but it can drive only in particular\nareas. Examples include a driverless taxi restricted to a small geographic area, such as an airport).\nLevel 5. The system controls the car without any intervention by a human.\nWhen applying RL, these levels, for example, determine whether the required RL system can be considered single agent\n(levels 1-4) or has to be multi-agent (levels 4 and 5). More concretely, at low automation levels, the agent is tailored\nfor specific tasks (for example, lane keeping), that do not involve interacting with other vehicles. As the driver is still\nresponsible for controlling the car, the agent can focus on its specific task. When the agent instead fully controls the\ncar, interactions with other agents (like, for example, other vehicles or pedestrians) are unavoidable.\nConcerning the model-free and model-based distinction, we have to consider the enormous complexity of the\nenvironment, in particular at higher automation levels (level > 3). The more complex the environment is, the harder\n7\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\nPlanning\nRL Components\nModel of the environment, \nPolicy, …\nControl\nActuators\nSteering Wheel, Brake, \nAccelerator\nPerception\nSensors\nCamera, LiDAR, RaDAR,\nUltrasonic sensors\nHigh-level representations\n2D/3D object detection, Depth \nestimation, Segmentation \nDecisions\nTrajectory optimization, Path \nplanning, Traffic simulation\nDriving Actions\nStop the car, Turn left, Turn right, \nChange Lane\nAdditional Components: sensor-controller communication, controller-actuator communication, vehicle software\nFig. 2. Components of Autonomous Driving Systems.\nthe deployment of model-based RL, as the model has to explicitly model the entire environment. When it comes to the\npolicy, updating the policy offline allows for collecting data and training the agent without doing so in real traffic. This\navoids potentially dangerous situations, as otherwise, a sub-optimal policy may be used and take actions that are not\nsafe. However, there is still the possibility to train on- or off-policy, but in a virtual environment. In terms of deployed\nautonomous cars, there might be the possibility that off-policy allows the deployment of a sufficiently mature policy\nregularly updated by the vendor, as already established by car vendors nowadays.1\n3.2\nAutonomous Driving Components and the Application of Reinforcement Learning\nThe RL agent should not be seen in isolation from the vehicle it controls: to allow full (or also partial) automation,\ndifferent types of sensors and components are required. Previous works describe these components at different levels\nof abstraction [20, 26, 69]. We chose the three-level abstraction structure by Pendleton et al. [66], which divides the\ninformation flow through the car. More specifically, the information is first perceived (perception), then used to derive\nactions (planning) which are then carried out (control). We now review each of these steps with a focus on reinforcement\nlearning. We illustrate the main components of the abstraction in Figure 2.\nPerception. As a first step, the car must be capable of perceiving its environment as a basis for any future decision or\noutput. This may include the localization of the car but mainly relates to the perception of the environment, generally\nbased on sensors. We now give an overview of the commonly used sensors, inspired by Dede et al. [27]. One example is\nthe LiDAR sensor, with a range of up to 200 meters for collision avoidance. A sensor similar to LiDAR is Radar with a\nsimilar range, yet it may also be used for collision detection in the closer vicinity of the vehicle. Both inputs can be\nrepresented, for example, as point clouds. Other sensors include cameras for object detection, which typically cover\na range of up to 100 meters and yield images or videos. Ultrasonic sensors have a short range and are often used for\n1See, for example, https://www.tesla.com/support/software-updates.\n8\nA Survey on Reinforcement Learning Security\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nparking assistance. Several sensors are often used in conjunction for redundancy and thus increase the reliability of\ncollected inputs. In addition to mere sensing, this layer may incorporate tasks such as object detection, depth estimation,\nand semantic segmentation, where inputs are already processed, resulting in aggregated information. These parts are\nvulnerable to specific attacks as well [13, 96], but for the sake of this survey, we will focus on attacks targeting the RL\ncomponent specifically.\nPlanning. Receiving the previously-collected inputs and, potentially, inputs from other vehicles, an RL agent can\nbe trained to solve different tasks, including, for example, path planning. In the following sections, we will discuss\ntasks such as deriving steering output prediction from input images [8, 81, 95, 99], traffic simulation [94], and path\nfinding [54]. Depending on the task, the requirements of an RL agent may vary greatly. An agent can maneuver a car\non a highway, where different non-autonomous vehicles are present. In this case, the RL system must be guaranteed to\noperate in real-time. However, small delays may be more acceptable for a vehicle operating in an area void of humans\n(both pedestrians and drivers). Along these lines, it is crucial whether the system is multi-agent (e.g., is aware of other\nreactive agents in its environment) or whether it considers the environment as static. The latter limits a system to an\nartificial setting where no interaction with other vehicles or pedestrians takes place. The multi-agent setting can be\nconsidered the bare minimum for maneuvering realistic traffic situations.\nControl. Once target actions have been identified, the vehicle needs to act upon these. This step involves communication\nwith the actuators and path and trajectory tracking or keeping track that indented actions are carried out as previously\nplanned. While there are potentially applicable mathematical modeling tools, machine learning, and RL systems used in\nthis step, there is no previous work dealing with the RL security of this component, at least to the best of our knowledge.\nThe security of control systems in general, however, is subject to ongoing research [5].\nFurther Components. There are parts of the autonomous car that we skipped previously for simplicity. For example,\nthe application of sensors requires software. To ensure communication between individual components, each car\ncontains different CAN buses. Similar layers to translate the output of the RL agent to the car are also required. Each of\nthese components may include a security vulnerability. As we will see in the next section, some attacks assume not to\nalter the environment but the information provided to the RL agent [81, 90, 103].\n4\nTHREAT MODEL\nIn this section, we describe a threat model that can be used to characterize attacks against RL. We characterize the\nattack according to (1) the goal of the attacker, (2) the knowledge the attacker has about the target system, and (3) the\nattacker’s capabilities of manipulating input data and/or tampering with the system components.\n4.1\nGoal\nThe attacker’s goal is the result they would like to obtain by perpetrating their attack. The goals can be broadly\ncategorized according to the security violation they cause. They can cause an integrity violation, e.g., to force the agent\nto perform a particular action at a specific point in time without compromising normal system operations. For example,\nit can force an agent that models an autonomous car to steer in the wrong direction in a curve. They can cause an\navailability violation that compromises the system’s functionalities. For example, an attacker can alter a portion of the\ntraining data, making the agent unable to learn to accomplish its task or learn to perform a task different from the one\nfor which it was devised. Finally, they can cause a privacy violation by obtaining private information about the system\nor the data used to train it by reverse-engineering the learning algorithm.\n9\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\n4.2\nKnowledge\nThe attacker can have different levels of knowledge of the targeted system, including (i) the training data, (ii) the\nobservations, (iii) the RL algorithm, (iv) the learned policy, (v) the actions performed by the agent, and (vi) the reward.\nIf the attackers know everything about the system, the attack is named white-box. This setting is often unrealistic\nbut allows system designers to perform a worst-case evaluation of the security of the RL policy at hand, providing\nempirical upper bounds on the performance degradation that may incur under attack.\nThe attacker can have limited knowledge of the system; they might know only some of the components mentioned\nabove. For example, the attacker might know the observations and the reward obtained by the agent from the environ-\nment, but they might be unaware of the target RL algorithm. Attackers can also have full knowledge of the system, but\nthey might be able to acquire this knowledge only after a few seconds, i.e., when the system has already altered its\nstate. If the attacker has only a limited knowledge of the system, the attack is denoted as gray-box. To carry out attacks\nin this scenario, attackers might exploit a property of the attacks called transferability [19]. Namely, they can create a\nsurrogate RL system, devise the attack against the surrogate and then employ the crafted attack against their target.\nWhen the attacker does not have any knowledge about the RL system, the attack is named black-box.\n4.3\nCapabilities\nAttackers might have different capabilities that they can leverage to achieve their goals. Firstly, they can have or do not\nhave the ability to alter training data. If the attacker alters the training data, the attack is defined as a training-time\n(poisoning) attack. Otherwise, it is defined as a test-time attack. Moreover, they can have the ability to change (𝑐(𝑥)) or\nmonitor (𝑚(𝑥)) some input/output 𝑥of the target system. In Figure 3, we highlight in red the components of the system\nshown in Figure 1 that attackers can manipulate without having access to the RL algorithm. Attackers can directly\nmanipulate the environment, for example, by adding objects (such as the red box shown in Figure 3) to the scenario or\nby employing agents that adopt a malicious policy. Attackers can also tamper with the sensors to alter the ego agent\nobservations. In this way, the agent will perform the best action for the perceived state, which would be different from\nthe actual state of the environment. Therefore, the agent will perform an action that is not optimal for the actual state\nof the environment. Even if this might be less likely to happen, attackers may also alter the RL algorithm’s code used to\ntrain the agent, as previously done in [56].\n4.4\nUsage of an Agent\nWhen running attacks against reinforcement learning systems, attackers can also leverage an agent to achieve their\nmalicious goals. There are different ways in which this can happen, as detailed below.\n• As explained before, when the attackers do not have perfect knowledge about their target system, they can train\na surrogate model, which in the following we will denote with ≈M (i.e., a system which approximates the target\nmodel).\n• In a multi-agent RL system, multiple agents influence the environment and thus its state, which is given as an\ninput to the ego agent. Therefore attackers can use an agent included in the environment (∈E) to carry out the\nattack.\n• Attackers can also leverage an agent not included in the environment (∉E) to enhance their attack. For example,\nthey can use it to understand how to alter the environment to cause the misbehavior of their target agent.\nIn the following, we will use the symbol ∅to denote the case where the attacker does not use any malicious agent.\n10\nA Survey on Reinforcement Learning Security\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nState:\ncars’ positions\nstreet signals \nlane borders\n….\nActions:\nturn right\nturn left\naccelerate\ndecelerate\nEnvironment \nReward\nCar\nObservations\nSensors\n(Ego) Agent\nFig. 3. Conceptual representation of the components of an RL policy that models an autonomous driving car the attacker can tamper\nwithout having access to the RL algorithm.\n5\nATTACKS\nDespite the promising results reported in the literature, RL is vulnerable to different attacks. As we will explain later,\nthe attack strategies proposed against RL are a subset of those proposed against ML. In the following, we will first\nexplain how state-of-the-art attacks against RL can be categorized. Then, we will present a taxonomy of the attacks\nproposed against Single-agent and Multi-agent reinforcement learning systems. We describe them separately as the\nattacks potentially applicable to multi-agent RL systems are more complex. Indeed, in multi-agent RL, the attacker\ncan exploit other agents that impact the environment to mount the attack. For the article that considers autonomous\ndriving as their case study, we will provide further details about the supposition behind them and their applicability to\nreality. Finally, we will discuss interesting feature directions regarding developing attacks against RL.\n5.1\nCategorizing the Attacks against RL\nOur taxonomy aims to be helpful to both researchers and practitioners. Therefore, our categorization highlights the\nrelevant characteristics of the attacks and the systems to which they have been applied. This allows the readers to\nunderstand which attacks are applicable to a chosen RL system and to notice blind spots, namely the RL system for\nwhich no attacks have been devised yet.\nWe categorize the attacks against RL in terms of:\n• Policy Update. The frequency with which the policy is updated (on-policy, off-policy, offline). This is relevant\nbecause, depending on this, training time attacks may or not have an immediate influence on the system;\n• Model-based. Their eventual ability to model the environment. It is true if the RL algorithm models the environment\nand false otherwise. Systems that model the environment may be potentially attacked, also making the system\nunable to learn to predict the environment outputs;\n• Attacker’s Capability. That denotes the capabilities the attackers have to have to perpetrate the attack. It is\nsubdivided in: (1) Attacker’s Action, which is the action that must be performed by the attacker, and, as we\ndiscussed, could be to monitor the inputs of the RL system or to alter them (2) Poisoning the eventual ability of\nthe attackers to alter the input during training, which can be true if they have this capability and false otherwise.;\n11\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\nTable 2. The objectives of the state-of-the-art attack against RL categorized according to the attackers’ capability required to carry\nout them (the ability to change only the test data or also the training data), and the security violation they cause.\nIntegrity\nAvailability\nPrivacy\nTest Data\n- reach(s𝑇)\n- steal(M)\n- min(R)\nTraining Data\n- learn(𝜋𝐵)\n- learn(𝜋𝑇)\n- learn(𝜋𝑇s )\n- min(R)\n• Attacker’s Knowledge. Level of knowledge of the victim system, which may be white-, gray-, and black-box;\n• Attacker’s goal. The result the attackers would like to obtain with their attack;\n• Sequential Attack. The eventual temporal dependence of the attack on the current or previous state. Sequential\nattacks are more complex to realize as the attacker when computing the attack, should know or predict the states\nthat will occur earlier than the state in which it would like to perpetrate the attack;\n• Attacker’s Agent. Whether and how the attackers use an agent to carry out the attack. The attacks where the\nagent exploits other agents are usually more complex to implement.\nThe possible attackers’ goals depend on the type of system the attacker is targeting. In the following, we discuss\nthe attackers’ goals considered by the works that proposed attacks against RL. In Table 2, we have categorized them\naccording to the security violation they cause, which can be integrity, availability, or privacy violation (see Sect. 4 for\nfurther details) and the capability required for the attacker (if he should be able to change/monitor the training data or\nonly the test data). In the following, we first describe the attackers’ goals that can be achieved by changing/monitoring\nonly the test data, then the ones for which it is necessary also to change/monitor the training data.\nTest time attacks. The attacks that manipulate only the test data cause either integrity or privacy violations. The\nattacks that cause the former can have two different goals:\n• They can make the environment reach a target state (reach(s𝑇)); e.g., they can make a car turn in the wrong\ndirection in a curve, causing the collision with the guardrail;\n• They can force the agent to perform the action that minimizes the return (min(R)); e.g., in a two-player game,\nthe attacker can alter the input of the opponent to make it perform a silly action, increasing its likelihood to win\nthe game.\nTo perform these attacks, the attacker should have the capability to change (c) some test inputs. The attacks that cause\nprivacy violations proposed so far monitor (m) the agent’s behavior to clone its policy. This attack is called model\nstealing (steal(M)) [62].\nTraining time attacks. To perform these attacks, the attacker should have the capability to change (c) some inputs\nreceived by the reinforcement learning system at training time. The attacks that manipulate the training data proposed\nso far can cause either an availability or an integrity violation. The attacks that cause an availability violation can have\nas a goal:\n12\nA Survey on Reinforcement Learning Security\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\n• Forcing the agent to learn a target policy, namely a policy chosen by the attacker. For example, they can make\nan artificial intelligence implementing an opponent in a two-player game learn to help the player instead of\nimpeding him. We denote them as (learn(𝜋𝑇));\n• Make the agent unable to learn to accomplish its task correctly, minimizing the return (min(R)). For example,\nthey can make a car unable to learn to drive safely;\nThe attacks that cause an integrity violation:\n• Inject training data containing a particular pattern called “trigger” to alter the agent’s agent behavior at test time\nin the presence of the trigger. For example, the car can behave normally in the presence of known traffic signs\nand adopt a strange behavior only when it encounters a stop sign with a yellow sticker (the trigger) attached.\nThe attacks exploiting such a mechanism are called Backdoor poisoning [29]. We denote them as (learn(𝜋𝐵)) as\nthe agent learns a policy that contains a backdoor, which makes the agent adopt a target policy but only in the\npresence of a trigger. The attacker should be able to alter also the test data to add the trigger.\n• Force the agent to learn to employ a target policy, namely a policy chosen by the attacker only for a subset of\nstates. The agent will employ the optimal policy when receiving the other states. For example, they can make a\ncar learn to go astray only when crossing a yellow car. We define this attack as Targeted Poisoning and we denote\nthis goal as (learn(𝜋𝑇s )). The main difference with Backdoor Poisoning is that Backdoor Poisoning forces the\nagent to misbehave in the presence of states defined by the attacker and then manipulates the environment to\ncontain the trigger at test time. In contrast, Targeted Poisoning makes the agent misbehave in the presence of\nstates that do not appear frequently but are not altered ad-hoc by the attacker. Thus the attacker does not need\nthe ability to manipulate the test samples but only the training samples.\nTraining time integrity attacks are hard to spot by observing the agent’s behavior, as it behaves as expected in most\ncases. Its behavior is unusual only in the presence of the trigger (subset of states).\n5.2\nAttacks against Single-agent RL\nIn this section, we present the literature about attacks against single-agent RL systems in the following. First, in Table 3,\nwe provide a schematic overview. We then describe the attacks in more detail, subdividing them according to the\nsecurity violation they cause.\nIntegrity Violation. Integrity attacks make the system behave as expected by the attacker on particular inputs without\ncompromising the system behavior on legitimate, unmodified inputs.\nAs explained before, an attacker can cause an integrity violation by having the ability to modify only the test data.\nIn the following, we describe test-time integrity attacks. The majority of the proposed attacks alter a component\nof the system (state, action, environment, or observations) to minimize the expected reward for a particular input\n[33, 46, 56, 85, 95, 99, 100, 103]. The attacker often modifies a system component only slightly, by applying a perturbation\nbounded with an Lp-norm constraint [48, 85, 100], making the attack difficult to be detected. To carry out these attacks,\nthe attacker often tampers with the sensors. Alternatively, the authors of [85] exploit adversarial patches [9], namely,\nsmall stickers containing an evident perturbation. The advantage of adversarial patches is that they can be printed\nand used to alter the environment; therefore, attackers can perpetrate this attack even if they do not have access to\nthe sensors. The authors of [85], considering a system trained to grasp objects based on visual input, show that the\nagent can become unable to perform its task when the patch is present. They consider 100 object-grasping episodes and\n13\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\nTable 3. Attacks against single-agent reinforcement learning systems. The presence of the ✓indicates that the corresponding attribute\nis true for the attack. For the attacker’s knowledge, we use  to represent black-box, G\n# for gray-box, and # for white-box.\nAttacker’s Capability\nReference\nPolicy\nUpdate\nModel\nBased\nAttacker’s\nAction\nPoisoning\nAttacker’s\nKnowledge\nAttacker’s\nGoal\nSequential\nAttack\nAttacker’s\nAgent\n[54]\non-policy\n/✓\n𝑐(𝑟)\n✓\n#\nlearn(𝜋𝑇)\n≈M\n[102]\noff-policy\n𝑐(𝑟)\n✓\n#\nlearn(𝜋𝑇)\n∅\n[35]\noff-policy\n𝑐(𝑟)\n✓\n#/  \nlearn(𝜋𝑇)\n✓\n∅\n[72]\non-policy, offline\n✓\n𝑐(𝑟,𝑎, s)\n✓\n#\nlearn(𝜋𝑇)\n✓\n∅\n[49]\non/off-policy\n/✓\n𝑐(𝑎)\n✓\n \nlearn(𝜋𝑇)\n≈M\n[49]\non/off-policy\n/✓\n𝑐(𝑎)\n✓\n#\nlearn(𝜋𝑇)\n∅\n[73]\n–\n–\n𝑐(𝑎)\n✓\nG#/  \nlearn(𝜋𝑇)\n✓\n∉E\n[55]\non-policy\n𝑐(𝑟)\n✓\n \nmin(R)\n∅\n[40]\non-policy\n𝑐(o,𝑟)\n✓\n#\nlearn(𝜋𝐵)\n∅\n[1]\noff-policy\n𝑐(E,𝑟)\n✓\n \nlearn(𝜋𝐵)\n∅\n[23]\non-policy\n𝑐(o)\n✓\n#\nlearn(𝜋𝑇s )\n∅\n[23]\non-policy\n𝑐(o)\n✓\n#\nlearn(𝜋𝑇s )\n∅\n[53]\noff-policy\n𝑐(𝑟)\n✓\n#\nlearn(𝜋𝑇s )\n∅\n[97]\noff-policy\n𝑐(E)\n✓\n#/  \nlearn(𝜋𝑇s )\n✓\n∉E\n[100]\non-policy\n𝑐(o)\n \nmin(R)\n✓\n∉E\n[85]\non/off-policy\n𝑐(E)\n \nmin(R)\n∅\n[56]\non-policy\n𝑐(s)\n#\nmin(R)\n∅\n[46]\non/off-policy\n𝑐(𝑎)\n#\nmin(R)\n∅\n[46]\non/off-policy\n𝑐(𝑎)\n#\nmin(R)\n✓\n∉E\n[99]\noff-policy\n𝑐(o)\n#\nmin(R)\n✓\n∉E\n[99]\noff-policy\n𝑐(o)\n \nmin(R)\n✓\n≈M\n[103]\non/off-policy\n𝑐(o)\n \nmin(R)\n∅\n[95]\noff-policy\n𝑐(o)\n#/  \nmin(R)\n✓\n∅\n[95]\noff-policy\n𝑐(𝑎)\n#\nmin(R)\n≈M\n[33]\non/off-policy\n𝑐(E)\n#/  \nmin(R)\n∅\n[95]\noff-policy\n𝑐(E)\n \nreach(s𝑇)\nN/A\n∅\n[8]\non-policy\n𝑐(E)\n#\nreach(s𝑇)\n✓\n∅\noptimize the patch to minimize the cumulative sum of the reward in all the episodes. Few works alter a component of\nthe system to cause the environment to reach a target state [8, 95].\nTo facilitate the success of integrity attacks, the attacker can add some carefully-crafted training samples. With\ncarefully designed perturbation of the environment at training time, the authors of [97] make the victim agent learn to\nperform an action chosen by the attacker when the agent is in some specific states and behave normally otherwise. In\n[53], the authors focus on attacking contextual bandits, a class of RL algorithms that work well for choosing actions in\ndynamic environments where options change rapidly, and the set of available actions is limited. The attack proposed in\n[53], altering the reward, induces the agent to learn to perform the action chosen by the attacker when a particular input\nis provided to the system. The authors of [40] propose an attack against DRL that adds training samples containing a\nparticular pattern (trigger) and alters the corresponding reward assigning the highest reward to random actions. This\n14\nA Survey on Reinforcement Learning Security\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nTable 4. Attacks against multi-agent reinforcement learning systems. The presence of the ✓indicates that the corresponding attribute\nis true for the attack. For the attacker’s knowledge, we use  to represent black-box, G\n# for gray-box, and # for white-box.\nAttacker’s Capability\nReference\nPolicy\nUpdate\nModel\nBased\nAttacker’s\nAction\nPoisoning\nAttacker’s\nKnowledge\nAttacker’s\nGoal\nSequential\nAttack\nAttacker’s\nAgent\n[3]\noff-policy\n𝑐(E)\n✓\nG#\nlearn(𝜋𝑇)\n≈M\n[93]\non-policy\n𝑐(E)\n✓\n#\nlearn(𝜋𝐵)\n✓\n∈E\n[94]\noff-policy\n𝑐(𝑟)\n✓\n \nlearn(𝜋𝐵)\n∈E\n[28]\non-policy\n𝑐(E)\n✓\nG#\nmin(R)\n✓\n∈E\n[47]\non/off-policy\n𝑐(E)\n#\nreach(s𝑇)\n✓\n∉E\n[86]\non/off-policy\n𝑐(E)\n#\nmin(R)\n∅\n[81]\non/off-policy\n𝑐(o)\n#\nmin(R)\n✓\n∉E\n[81]\non/off-policy\n𝑐(o)\nG#\nmin(R)\n✓\n∉E\n[90]\noff-policy\n𝑐(o)\n#\nmin(R)\n✓\n∅\n[74]\noff-policy\n𝑐(o)\n \nmin(R)\n✓\n∉E\n[44]\non-policy\n𝑐(o)\n#\nmin(R)\n∅\n[82]\non/off-policy\n𝑐(o)\n#\nmin(R)\n∉E\n[43]\noff-policy\n𝑐(s)\n#\nmin(R)\n∅\n[37]\non/off-policy\n𝑐(o)\n \nmin(R)\n≈M\n[12]\non/off-policy\n𝑚(𝑎)\n \nsteal(M)\n✓\n∅\nmakes the agent act randomly every time the trigger is present in the input. This attack is difficult to detect because the\nagent’s behavior is sensible when the trigger is not present. The attack force the agent to assume the desired behavior\nat test time only when an input containing the trigger is presented. In [1], the authors propose an attack similar to\n[40] that makes the agent learn to perform a target policy when the trigger is present in the input. Interestingly, in\nthis paper, the authors propose the concept of in-distribution triggers. Namely, patterns that can occur due to agent\ninteraction with the environment and thus are difficult to detect when used as a trigger. The authors show that the\nattack succeeds by adding 10-20% samples containing the trigger.\nAvailability Violation. As we explained before, availability attacks, also called denial-of-service attacks, make the\nsystem unusable. The availability attacks proposed against RL modify a portion of the training data to make the agent\nunable to learn to accomplish the task correctly. In [55], the authors propose a black-box attack. They assume the\nattacker does not have any knowledge about the learned policy, but they can sometimes flip the sign of the reward\nwhen the environment reaches a chosen state. Their work focuses on on-policy RL and analyzes different exploration\nstrategies. Their experimental analysis shows that small exploration probabilities are more resilient to perturbations of\nthe reward. In [35, 54, 72, 102], the authors alter the reward to force the agent to learn a policy chosen by the attacker,\ne.g., make a robot learn to reach a location chosen by the attacker instead of the location desired by the developer of\nthe RL system.\nPrivacy Violation. So far, to the best of our knowledge, no attack has been proposed against single-agent RL algorithms.\nAs explained in the previous sections, the adaptation of attacks previously proposed against standard ML systems to RL\nis not straightforward. Therefore many of the former have not been tested yet against RL.\n15\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\n5.3\nAttacks against Multi-agent RL\nIn the following, we present the literature about attacks against multi-agent RL. Table 4 presents the taxonomy of the\nstate-of-the-art attacks against multi-agent RL, and then we describe them in more detail.\nIntegrity Violation. The majority of the proposed test-time attacks manipulate some system components to minimize\nthe expected reward [28, 37, 43, 44, 74, 81, 82, 86, 90]. In [86], the authors claim that test-time attacks against DRL, to\nbe practical, should work disregarding the current state of the system. The reason is that pre-computing an adversarial\nexample for each possible state is not feasible in practice, as the possible states are often too many, and the attacker\nwould not be able to compute the attack for the current state before the state is already changed. To solve this problem,\nthey consider universal adversarial examples [59] that can be computed offline and are effective for different states.\nOnly the work in [47] proposes test-time attacks that make the environment reach a particular state desired by the\nattacker.\nA few attacks alter the training data to make the agent perform an attacker-chosen action when a particular state\n(trigger) occurs. In particular, they exploit other agents to trigger the desired behavior. The work in [93] considers\na competitive two-player RL system and uses the opponent’s behavior to trigger the backdoor. In [94], instead, the\nauthors consider a traffic congestion control system, and assume that the attacker uses the state of the car agents in the\nneighborhood of the ego agent to trigger the backdoor.\nAvailability Violation. In [3], the authors propose a training time attack that causes an availability violation. With\nad-hoc manipulation of the environment at training time, the attack proposed in this work makes the RL learn a target\npolicy. The authors of the work in [28], instead considers a two-player game and exploits the behavior of an agent\nwithin the environment (the opponent) to make the ego agent unable to win the match. This attack is named adversarial\npolicy.\nPrivacy Violation. So far, only the work in [12] proposes an attack that compromises the privacy of RL. In particular,\nthe authors of [12] propose a methodology to perform model stealing ad-hoc for DRL algorithms. This attack creates a\nclone of a target RL policy. Many different attacks were previously proposed against deep learning, but they were not\napplicable to DRL due to its complexity, stochasticity, and limited observable information. The attack proposed by the\nauthors is based on imitation learning, a well-established solution to learning sequential decision-making policies. The\ndesiderata for the copied model can be (1) accuracy, namely the ability to match or exceed the accuracy of the original\nmodel; (2) fidelity, namely a similar behavior to the original model (even committing the same errors). The authors\nshowed the proposed attack achieves both high accuracy and fidelity.\n5.4\nAttacks on Autonomous Driving\nIn this section, we discuss attacks specifically in relation to autonomous driving. We first review attacks that specifically\ntarget autonomous vehicles and discuss which properties are required for an attack to be a real-world threat. We\nconclude this section by discussing future work.\nState of the Art. We summarize attacks that have been evaluated in driving scenarios in Table 5. Two of these six\nattacks are evaluated in a multi-agent context. Although a car that operates in a single-agent system is limited to an\nautonomy level 4, and a multi-agent system approach is required for level 5, single-agent attacks may still work on\nmulti-agent capable cars. In this sense, cars with automation level 5 are, in principle vulnerable towards more kinds of\nattacks. Of six attacks on autonomous driving tasks, four are evasion attacks, all on end-to-end systems, attempting\nto make the vehicle leave the lane [8, 81, 99] or trigger suboptimal behavior [95], all potentially triggering a crash.\n16\nA Survey on Reinforcement Learning Security\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nTable 5. Attacks evaluated on driving tasks. We list the attack, the target component (e.g., planning, or end-to-end), the attack goal in\nrelation to the car, and the attackers action to mount the attack. We further repeat from the previous tables whether the attack is a\npoisoning/training time attack, whether it is sequential, and whether the targeted system is modelled as an multi-agent system. The\nother details about these attacks are reported in Table 3 and 4 ∗The work by Wang et al. [94] targets the controller of an autonomous\nvehicle as a part of a larger system that optimizes traffic flow.\nReference\nTarget\nComponent\nAttacker’s\ngoal\nAttacker’s\nAction\nPoisoning\nSequential\nattack\nMultiagent\n[8]\nend-to-end\ncar leaves lane\npaint pattern on street\n✓\n[95]\nend-to-end\nsubptimal behavior\npaint pattern on street\n/✓\n[99]\nend-to-end\ncar leaves lane\nsensor input / FGSM\n✓\n[81]\nend-to-end\ncar leaves lane\nsensor environment / FGSM\n✓\n✓\n[54]\nplanning\nreroute car\ntraining rewards\n✓\n[94]\ncontroller*\ntraffic jam/crash\npositions and speeds\n✓\n✓\nAlthough neither is evaluated on a real vehicle, Boloor et al. [8] considers an attacker that paints lines on the street and\nXiao et al. [95] with printed perturbations. Interestingly, in [8], autonomous driving is considered as a case study. The\nattacker is assumed to manipulate the scenario, painting some black lines on the street, making the car agent turn in\nthe wrong direction. The authors show that their attack is effective using a driving simulator. Yang et al. [99] propose\ninstead to tamper with the sensor input altering them leveraging the system’s gradients with respect to the output.\nYet differently, Sun et al. [81] propose to alter the environment’s response to the agents behavior. Furthermore, Ma\net al. [54] introduces a policy-poisoning attack that affects the planning component in a toy example, resulting in the\ncar taking a sub-optimal route to the target. This is achieved by tampering with the rewards during the training of\nthe model. Finally, Wang et al. [94] introduces a backdoor attack, affecting the controller component of a connected\nautonomous vehicle to cause a traffic jam or a crash, by considering two different desired behaviors: (1) acceleration\nand deceleration to cause traffic congestion, (2) acceleration to cause a crash with the vehicle on the front. The authors\ntested the effectiveness of the proposed attack using a simulator and both a single- and a two-lane circuit. However,\nthis methodology does not target the car itself but the algorithm controlling the cars to optimize traffic flow. The\nauthors embed a backdoor into this meta-algorithm that depends on the individual cars’ position and speed in the\ntraffic scenario to cause the desired behavior.\nPractical Shortcomings. While all these attacks are useful as proofs of concepts, it is unclear how far they would\naffect actual autonomous cars. For example, Boloor et al. [8] and Xiao et al. [95] are the only ones considering an action\nan attacker could potentially carry out in the real world. More concretely, no attack is evaluated on a real vehicle. A\npossible explanation for this could be the cost of an autonomous vehicle, which usually amounts to more than 20,000$.2,\nand is probably not affordable for most researchers or their institutions. An additional (unknown) attack limitation is\nthe time overhead introduced by current attacks. For example, for all four evasion attacks [8, 81, 95, 99], it is unclear\nwhether the input perturbations could be computed fast enough to harm a car in real-time, or whether (or to what\ndegree) they would transfer, e.g., can be computed on one model and work on another [63]. None of the attacks tested\nwith autonomous driving as a case study has exploited an agent within the system. Therefore, to which extent this\n2https://qz.com/924212/what-it-really-costs-to-turn-a-car-into-a-self-driving-vehicle/\n17\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\n2021: Model Stealing [12]\n2020: Backdoor Poisoning [40]\n2018: Targeted Poisoning [53]\n2017: Availability Poisoning [3]\n2017: Test-time Integrity [47, 56]\n2017: Targeted and Backdoor Poisoning [42,29]\n2017: Membership Inference [79]\nML\nRL\nLegend\nIntegrity violation\nAvailability violation\nPrivacy violation\n2022: Training-time Sponge [15]\n2021: Test-time Sponge [80]\n2016: Model Stealing [89]\n2015: Model Inversion [24]\n2012: Availability Poisoning [6]\n2004-2005: Test-time Integrity [18,52]\nFig. 4. Timeline of attacks against reinforcement learning, compared to those proposed against machine-learning classifiers.\nmay work in this scenario is still to be understood. In addition, the question is whether sequential attacks are feasible\nor required in practice. A non-sequential attack is easier to stage, as one tampered input may be sufficient to trigger\nunwanted behavior. However, of all attacks from Table 3 and 4, slightly more than half introduce sequential attacks.\nFurthermore, of the above-mentioned attacks, only Wang et al. [94] and Xiao et al. [95] operate in a black-box setting,\nwhich seems a more realistic assumption given that currently, most autonomous car models are not open-source. Of all\nattacks from Table 3 and 4, only roughly a third operate in a black box setting.\n5.5\nFuture Research Directions\nThe attacks against RL are adaptations of attacks previously proposed against standard ML systems, particularly against\nclassifiers. In Figure 4, we highlight the connections between the attacks proposed against standard ML systems and\nthose proposed against ML. The first attacks against RL [47, 56] were proposed in 2017 and were able to make the\nsystem misbehave when a carefully crafted sample was provided as input to the system. These attacks are inspired by\nthe ones [18, 52] proposed more than ten years before (between 2004 and 2005) against classifiers to have a carefully\ncrafted sample misclassified. For example, to have a spam email misclassified as legitimate. Also, the first training time\nattack against RL was proposed in 2017 [3]. The goal of this attack was to make the system unable to work correctly,\ncausing a denial of service. In this case, the attack is also inspired by an attack [6] proposed a long time before (2012)\nagainst classifiers. In this case, to make a Support Vector Machine unable to learn to classify samples correctly. In 2018\nand 2020, researchers proposed the first poisoning attacks against RL aimed at violating the integrity of the systems. In\nparticular, in 2018 (2020), they proposed the first targeted [53] (backdoor [40]) poisoning attacks. Both were inspired by\narticles proposed in 2017. The first is [42] aimed to have some test samples misclassified by manipulating few training\nsamples. The second is [29], making the classifier learn to misclassify all the samples containing a trigger. Also, the\n18\nA Survey on Reinforcement Learning Security\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nonly attack that violates the privacy of RL systems stealing it [24], proposed in 2021, is inspired by an attack previously\n(in 2016) proposed against classifiers [89].\nHowever, different attacks proposed against classifiers have not yet been tested against RL. Also leveraging on this\nanalysis, in the following, we will highlight different interesting future directions in developing attacks against RL.\nPrivacy Violation. The only attack that compromises the system privacy tested so far against RL is model stealing.\nHowever, other attacks might be potentially adapted to work against RL. One of these attacks is model inversion [24],\nin which the attacker tries to reconstruct some training data by observing the model input and the agent behavior. The\nother is membership inference [79], where the attacker would like to understand if a given sample was or was not part\nof the training dataset.\nAvailability Violation. All the availability attacks proposed so far against RL manipulate the training samples to alter\nthe policy. However, another attack called “sponge attack” [80] was proposed against classifiers and can potentially be\nadapted to work against RL. With the attack proposed in [80], the attacker does not need to manipulate the training data\nto perpetrate this attack. At test time, the attacker provides the system inputs carefully crafted to increase the system’s\ntime response, potentially delaying the response of real-time systems to the subsequent inputs. This kind of attack that\nslows down the system was also perpetrated at training time by altering the code used to train the algorithm [15].\nAutonomous Driving. It is required to either study more black-box attacks or understand more in detail the transfer-\nability [63] of attacks in reinforcement learning. Otherwise, the computation time of the attack should be provided\nso that the real-time risk can be assessed. In addition, more realistic attacker actions that are aligned with real cars,\nat least from the design perspective, are required. In this context, it is important to evaluate attacks on real cars or\nvery close models of existing cars encompassing the full system. For example Jia et al. [38] show that only fooling\nobject detection does not suffice to alter the output of the entire objection detection pipeline within a vehicle. These\nreal-world evaluations could shed light on whether sequential or non-sequential attacks are more dangerous and\nwhether black-box access suffices to affect a car’s security.\n6\nDEFENSES\nDifferent defenses have been proposed to make RL more robust against attacks. In the following, we first explain how\nstate-of-the-art defenses against RL can be categorized. Then, we present a taxonomy of the defenses proposed to\nsecure Single-agent and Multi-agent reinforcement learning systems. Because, as we explained in the introduction,\nautonomous driving is of paramount importance, we have also investigated in detail the article that considers this\napplication. However, to our knowledge, a single defense has proposed a defense considering it. Therefore, we mainly\ndiscuss the open challenges. Finally, we discuss interesting feature directions regarding the development of defenses for\nRL).\n6.1\nCategorizing the Defenses against RL\nAll the proposed defenses techniques proposed to defend reinforcement learning adapt defense techniques previously\ndevised for classifiers. They can be subdivided into two categories: the ones that try to counteract past attacks (reactive\ndefenses) and the ones that act to prevent future attacks (proactive defenses) [7]. The former usually tries to detect\nand stop the attack (detection) and/or to remove its effect from a system or input altered by it (sanitization). The latter,\ninstead, try to make the system more robust and can be further categorized depending on the strategy they use to\nachieve this goal as:\n19\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\n• Adversarial training: iteratively retrain the system on the simulated attacks. These defenses are heuristic, with\nno formal guarantees on convergence and robustness properties.\n• Game theory: model the problem of making the classifier more robust as a Nash [10] or Stackelberg [50] game\nwhere the agent plays with an adversary, deriving formal conditions for existence and uniqueness of the game\nequilibrium, under the assumption that each player knows everything about the opponents and the game. They\nare more principled than adversarial training; however, they are more computationally costly.\n• Regularization: make the system more robust, penalizing its sensitivity to inputs’ perturbations.\n• Distillation: is a technique that was originally proposed for model compression [32]. It trains a small network\n(called student) to imitate the behavior of a larger network (teacher). This technique, if the two networks have\nthe same architecture, increases regularization [58]. A variant of distillation called \"defensive distillation\" was\nproposed in [64], but it was shown to be ineffective in [11].\nAs we have explained when presenting the categorization that we used for attacks, the goal of our taxonomy is to be\nhelpful to both researchers and practitioners. Thus, we should consider not only the peculiarities of the defenses but\nalso the ones of the reinforcement learning systems on which they have shown to be effective. Therefore, we categorize\nthe defenses proposed against RL in terms of the following:\n• Policy Update. The frequency with which the policy is updated (on-policy, off-policy, offline);\n• Defense Technique. The type of defense technique (i.e., adversarial training, detection, etc.);\n• Attacker’s Capability. That denotes the capabilities the attackers have to have to perpetrate the attack. It is\nsubdivided in: (1) Attacker’s Action, which is the action that must be performed by the attacker, and, as we\ndiscussed, could be to monitor the inputs of the RL system or to alter them (2) Poisoning the eventual ability of\nthe attackers to alter the input during training, which can be true if they have this capability and false otherwise.;\n• Attacker’s Knowledge. Level of knowledge of the victim system, which may be white-, gray-, and black-box;\n• Attacker’s goal. The result the attackers would like to obtain with their attack;\n• Sequential Attack. The eventual temporal dependence of the attack on the current or previous state;\n• Attacker’s Agent. whether and how the attacker uses an agent to carry out the attack.\nIn the following sections, we present a taxonomy of the defenses that have been proposed against single- and\nmulti-agent RL algorithms, subdividing them according to the security violation they aim to contrast.\n6.2\nDefenses for Single-agent RL\nThis section provides an overview of the defenses proposed against single-agent RL. First, we provide in Table 6 a\ncompact categorization of state-of-the-art defenses against single agents.\nIn the table, we omit the column for the ability of the system to model the environment because all the defenses at\nthe state of the art consider model-free systems.\nIn the following, we describe these works in more detail, subdividing them according to the aimed security violation.\nThe work in [2, 21, 71, 101] proposes provable defenses, i.e., defenses with provable theoretical guarantees within\nthe considered threat model.\nIntegrity Violation. Attacks that violate the system’s integrity can be carried out without altering the training data\nor altering them. To the best of our knowledge, all the defenses that face integrity violations that have been proposed\nso far for single-agent RL defend against attackers that manipulate only test data.\n20\nA Survey on Reinforcement Learning Security\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nTable 6. Defenses for single-agent reinforcement learning systems. The presence of the ✓indicates that the corresponding attribute is\ntrue for the defense. For the attacker’s knowledge, we use  to represent black-box, G\n# for gray-box, and # for white-box. All the\ndefenses proposed so far have been tested on model-free RL algorithms.\nAttacker’s Capability\nReference\nPolicy\nUpdate\nDefense\nTechnique\nAttacker’s\nAction\nPoisoning\nAttacker’s\nKnowledge\nAttacker’s\nGoal\nSequential\nAttack\nAttacker’s\nAgent\n[21]\noff-policy\nreg.\n𝑐(𝑟)\n✓\nG#\nmin(R)\n∅\n[30]\non/off-policy\ndet. + san.\n𝑐(s,𝑟)\n✓\nG#\nmin(R)\n≈M\n[2]\n-\ngame theory\n𝑐(𝑟)\n✓\n#\nlearn(𝜋𝑇)\n✓\n∅\n[67]\non-policy\nAT\n𝑐(E)\n#\nmin(R)\n✓\n∉E\n[31]\non-policy\ndet. + san.\n𝑐(o)\n#\nmin(R)\n✓\n∅\n[87]\noff-policy\nAT\n𝑐(𝑎)\n#\nmin(R)\n∉E\n[48]\non/off-policy\nreg.\n𝑐(o)\n#\nmin(R)\n✓\n∅\n[84]\non-policy\nAT\n𝑐(𝑎)\n#\nmin(R)\n∅\n[56]\non-policy\nAT\n𝑐(E, o)\n#\nmin(R)\n∅\n[65]\noff-policy\nAT\n𝑐(o)\n#\nmin(R)\n∅\n[45]\non-policy\nAT + transfer learning\n𝑐(𝑎)\n \nreach(s𝑇)\n✓\n∉E\n[71]\non-policy\nAT + ensemble\n𝑐(E)\nN.A\nN.A\nN.A\nN.A.\n[100]\non-policy\nAT\n𝑐(o)\n \nmin(R)\n✓\n∉E\n[101]\non-policy\nAT (GAN)\n𝑐(E)\n \nmin(R)\n✓\n∉E\nTable 7. Defenses for multi-agent reinforcement learning systems. The presence of the ✓indicates that the corresponding attribute is\ntrue for the defense. For the attacker’s knowledge, we use  to represent black-box, G\n# for gray-box, and # for white-box. All the\ndefenses proposed so far have been tested on model-free RL algorithms.\nAttacker’s Capability\nReference\nPolicy\nUpdate\nDefense\nTechnique\nAttacker’s\nAction\nPoisoning\nAttacker’s\nKnowledge\nAttacker’s\nGoal\nSequential\nAttack\nAttacker’s\nAgent\n[93]\non-policy\nsan.\n𝑐(E)\n✓\n#\nlearn(𝜋𝐵)\n✓\n∈E\n[92]\non/off-policy\nreg.\n𝑐(𝑟)\n✓\n \nmin(R)\n∅\n[61]\non/off-policy\nreg.\n𝑐(o)\n#\nmin(R)\n/✓\n∅\n[70]\noff-policy\n≈dist.\n𝑐(o)\n#\nmin(R)\n✓\n∅\n[22]\noff-policy\ndist. + AT\n𝑐(o)\n#\nmin(R)\n∅\n[44]\non-policy\nAT\n𝑐(o)\n#\nmin(R)\n∅\n[48]\noff-policy\ndet.\n𝑐(o)\n#\nmin(R)\n∅\nThe majority of them focus on attacks that try to minimize the expected reward by manipulating different components\nof the RL system, namely, the observations, the environment, or the action chosen by the agent. The defenses proposed\nto counter these attacks are mainly based on adversarial training [56, 65, 67, 84, 87, 100, 101], which allows increasing\nrobustness to the perturbation of a specific component. In [31], the authors propose a defense that, analyzing states\nand rewards, can detect and restore the observations altered by the attacker. The work in [101] proposes a regularizer\n21\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\nthat penalizes the sensitivity of the policy to alterations of the observations. The authors of [21], instead, try to reduce\nthe sensitivity to the perturbation of the rewards by adding noise to the rewards during training. This defense is\ncertified; namely, it provides a certificate for the maximum reward change under a bounded perturbation of the reward’s\nobservation. In [71], the authors propose using an ensemble of models trained with adversarial training to learn a policy\nrobust to parametric model errors as well unmodeled effects. However, they test the robustness of the proposed defense\nonly against random perturbations and not against attackers that craft worst-case perturbations.\nA single work has proposed a defense against targeted evasion attacks [45]. The authors propose a mechanism to\ncounter attacks that alter the action chosen by the agent to force the environment to reach a given target state. This\nwork analyzes three different strategies to perform adversarial training: (1) jointly training an untrained legitime agent\nand an adversarial agent; (2) training an untrained legitime agent that plays against an already trained adversarial agent,\n(3) fine-tuning the legitime agent while it plays against a pre-trained adversarial agent. Their experimental analysis\nshows that the third strategy is the only one that allows for increasing the robustness of the legitimate agent.\nAvailability Violation. Only a few works have proposed defenses against availability attacks. The authors of [2]\npropose using game theory to reduce the vulnerability against attacks that force the victim to learn a policy chosen by\nthe attacker altering the reward at training time. The work in [21, 30], instead, proposes defenses against attacks that\ntry to make the agent unable to accomplish the desired task, minimizing the expected return. In [21], the authors focus\non attacks that manipulate the reward to make the agent select suboptimal actions. To increase robustness against\nthis attack, they propose to make the agent randomly choose a state from a top quantile of high-reward states. They\nclaim that, in this way, adversarially placed corrupt states are likely to be avoided. The work in [30], instead, focuses on\nattacks that tamper with the states observed by the RL agents and the associate reward so that the trained model learns\nto perform sub-optimal actions. The authors consider a system that applies RL for the autonomous cyber defense of a\nnetwork under partial observability. They show that if the defender knows the nodes visible to the attacker, the number\nof compromised nodes at each step, and the attacker’s strategy can find and restore the compromised nodes.\nPrivacy Violation. To the best of our knowledge, no defense has been proposed to protect RL against model stealing,\nwhich is the only privacy-related attack proposed thus far.\n6.3\nDefenses for Multi-agent RL\nHere we provide an overview of the defenses proposed against multi-agent RL. Table 7 provides their compact overview,\nand we describe these defenses in more detail below.\nThe work in [70, 92] provides provable defenses.\nIntegrity Violation. The majority of the defenses proposed against integrity attacks focus on test-time attacks. All of\nthem consider attacks that alter some components of the RL system to minimize the expected reward for a given point\nin time. Like for single-agents, defenses based on adversarial training [44], detection, [48], and regularization [61] have\nbeen proposed. Another strategy that has been proposed only for multi-agent RL is distillation. The authors of [22]\nconsider an RL system based on DQN. They modify the system to use two different architectures, which are both trained\nonline. The original DQN, trained usually, is supported by another DQN, trained on samples perturbed with worst-case\nperturbations. The authors use the former to learn the optimal Q-function in a legitimate scenario and the latter to learn\nthe optimal Q-function in the presence of an adversary. Then they create a DQN that synthesizes the experience of both.\nIn [70], the authors propose a defense that tries to make the student robust without generating adversarial examples.\n22\nA Survey on Reinforcement Learning Security\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nTo this end, they use a loss that maximizes the probability that the student chose the same action chosen by the teacher,\nminimizes the entropy of the unwanted actions, and penalizes the sensitivity to input perturbation using a regularizer.\nTo facilitate attacks that violate the system’s integrity, attackers can also tamper with training data. A single defense\nagainst these attacks has been proposed so far. In [93], the authors focus on attacks that force the victim to learn a\ntarget policy adopted only when a backdoor, learned by the agent at training time, is triggered. The trigger considered\nby the authors is a particular action of another agent in the environment. The authors have investigated if fine-tuning\nthe agent on clean data can reduce the vulnerability to this attack. They have shown that fine-tuning on legitimate data\nis only effective in an extremely limited sense, mainly because the trigger action is seldom encountered in legitimate\ndata. Thus, it does not make the agent unlearn the trigger, and therefore, the attack remains effective. Notably, this is\nthe only defense that has been proposed so far against an attack that exploits an agent within the environment.\nAvailability Violation. The only countermeasure against attacks that violate the system’s availability has been\nproposed in [92]. This defense focuses on attacks that alter the reward to minimize the expected return. To counteract\nthis attack, they propose an unbiased estimator that, given the noisy reward, approximates the true (unperturbed)\nreward.\nPrivacy Violation. To the best of our knowledge, no defense has been proposed so far to protect RL algorithms against\nmodel stealing.\n6.4\nDefenses for Autonomous Driving\nIn this subsection, we discuss defenses specifically in relation to autonomous driving. We first summarize existing\nefforts that evaluate their techniques considering autonomous driving scenarios and then discuss requirements for\ndefenses that should be practically applicable to this scenario. We conclude the section by discussing future work.\nTo the best of our knowledge, only one defense has been evaluated in the context of autonomous driving, more\nspecifically on a toy navigation task, which is the game theory-based approach by Banihashem et al. [2]. No mitigation,\nhowever, has been tested on a task more similar to autonomous driving or on a real vehicle. One possible reason, in\nparticular for the latter, could be the cost of an actual vehicle to evaluate such defenses, which easily amounts to more\nthan 20,000$.3. This cost is not affordable for most laboratories. Albeit using a simulator can, to some degree, solve this\nproblem, there are other challenges that defenses should face to be applicable in AD, such as:\n• Requiring only hardware that is usually already on board [78]. For example, it is unlikely that an autonomous\nvehicle that is purely based on cameras will be equipped with a LiDar sensor only for defense purposes.\n• Be evaluated considering all the autonomous driving system components [78]. There is a possibility that the\ndefense interacts with other parts of the car, which has to be taken into account during development.\nIn contrast, some requirements can also be approximated in a simulator without involving an actual car. These include\nthe introduced overhead, which cannot be too large as otherwise, the car cannot react properly anymore. Some of the\ndefenses discussed in this section (for example, when based on regularization [21, 48, 92] or distillation [22, 70]) do not\nintroduce overhead neither at test nor at training time, whereas adversarial training [67, 84, 88] introduces an overhead\nonly a training time. They are thus more practical in this sense because the training can be done with a simulator.\nAlso, modifying RL systems by leveraging previously proposed techniques [34] to predict the influence of the other\nagents in the environment might increase the security of autonomous driving.\n3https://qz.com/924212/what-it-really-costs-to-turn-a-car-into-a-self-driving-vehicle/\n23\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\n6.5\nFuture Research Directions\nMany defenses have been proposed so far; however, there are still interesting research avenues regarding the defenses\nfor RL.\nPrivacy Violation. A recent attack [12] has shown that it is possible to steal a DRL model by observing states and\nactions. Namely, to create a copy of it, that: (1) matches or exceeds the accuracy of the original model; (2) has a similar\nbehavior as the original model (it commits the same errors). This attack can be used to copy a competitor’s DRL model\nor create a surrogate that attackers can use to craft attacks that transfer against their target model [19]. No defense has\nbeen proposed so far against this attack.\nIntegrity Violation. So far, the only defense proposed against training-time integrity violations is [93], which consists\nof fine-tuning the DRL algorithm on a clean dataset. However, the authors showed that this technique is ineffective;\nthus, to date, no defense has been shown to be effective against this type of attack for RL. Nevertheless, different\ndefenses proposed for classifiers could be adapted and applied, including those which examine training [86] or test data\nto detect the trigger [93], and those which increase regularization. [16]\nAvailability Violation. As we have discussed before, the only defense that defends attacks carried out exploiting an\nagent within the environment is [93]. No defense has been proposed against attacks that exploit an attack within the\nenvironment to cause an availability violation, for example, the adversarial policy attack [28].\nAutonomous Driving. Future work should clearly communicate the hardware requirements of the defense. Further-\nmore, the mitigation’s introduced time overhead during both the training and test phase should be stated clearly and in\npercent overhead of normal training/operation time or directly an appropriate time unit (e.g., seconds). It also remains\nan open question of how to evaluate defenses on the system level, or if a suitable simulator can be built or cars can be\nprovided for research purposes. Finally, given that cars are highly standardized in terms of safety and security, it might\nfurther be necessary that future defenses ultimately comply with existing or new industrial standards [76].\n7\nCONCLUDING REMARKS\nIn this survey, we analyze and categorize state-of-the-art attacks and defenses against RL algorithms. The categorization\nthat we provide, differently from the ones available in the previously published works, allows system designers to\nunderstand which defense they can apply to defend the system at hand against a precise threat. Given that autonomous\ndriving is a particularly concerning and valuable application, we have also analyzed the articles considering it more\nextensively. Our analysis has shown that the studies about the security of RL-based autonomous driving systems are at\ntheir birth. Further studies are needed to assess the applicability of existing techniques to this application. Finally, we\nhave identified which attacks that have been previously proposed against machine learning have not been adapted and\napplied yet to reinforcement learning and for which attacks no defenses have been proposed so far. This allows system\ndesigners to envision possible threats and researchers to explore interesting future research directions.\nREFERENCES\n[1] Chace Ashcraft and Kiran Karra. 2021. Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers. arXiv preprint arXiv:2106.07798\n(2021).\n[2] Kiarash Banihashem, Adish Singla, and Goran Radanovic. 2021. Defense Against Reward Poisoning Attacks in Reinforcement Learning. arXiv\npreprint arXiv:2102.05776 (2021).\n[3] Vahid Behzadan and Arslan Munir. 2017. Vulnerability of deep reinforcement learning to policy induction attacks. In International Conference on\nMachine Learning and Data Mining in Pattern Recognition. Springer, 262–275.\n24\nA Survey on Reinforcement Learning Security\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\n[4] Vahid Behzadan and Arslan Munir. 2018. The Faults in Our Pi Stars: Security Issues and Open Challenges in Deep Reinforcement Learning.\narXiv:1810.10369 [cs, stat] (Oct. 2018). http://arxiv.org/abs/1810.10369 arXiv: 1810.10369.\n[5] Deval Bhamare, Maede Zolanvari, Aiman Erbad, Raj Jain, Khaled Khan, and Nader Meskin. 2020. Cybersecurity for industrial control systems: A\nsurvey. computers & security 89 (2020), 101677.\n[6] Battista Biggio, Blaine Nelson, and Pavel Laskov. 2012. Poisoning attacks against support vector machines, In 29th Int’l Conf. on Machine Learning,\nJohn Langford and Joelle Pineau (Eds.). Int’l Conf. on Machine Learning (ICML), 1807–1814.\n[7] B. Biggio and F. Roli. 2018. Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning. Pattern Recognition 84 (2018), 317–331.\n[8] Adith Boloor, Xin He, Christopher Gill, Yevgeniy Vorobeychik, and Xuan Zhang. 2019. Simple physical adversarial examples against end-to-end\nautonomous driving models. In 2019 IEEE International Conference on Embedded Software and Systems (ICESS). IEEE, 1–7.\n[9] T. Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and J. Gilmer. 2017. Adversarial Patch. ArXiv abs/1712.09665 (2017).\n[10] Michael Brückner, Christian Kanzow, and Tobias Scheffer. 2012. Static Prediction Games for Adversarial Learning Problems. J. Mach. Learn. Res. 13\n(September 2012), 2617–2654.\n[11] Nicholas Carlini and David Wagner. 2016. Defensive Distillation is Not Robust to Adversarial Examples. Technical Report arXiv:1607.04311. arXiv.\nhttps://doi.org/10.48550/arXiv.1607.04311 arXiv:1607.04311 [cs] type: article.\n[12] Kangjie Chen, Shangwei Guo, Tianwei Zhang, Xiaofei Xie, and Yang Liu. 2021. Stealing Deep Reinforcement Learning Models for Fun and Profit.\nIn Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security. 307–319.\n[13] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Polo Chau. 2018. Shapeshifter: Robust physical adversarial attack on faster r-cnn\nobject detector. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 52–68.\n[14] Tong Chen, Jiqiang Liu, Yingxiao Xiang, Wenjia Niu, Endong Tong, and Zhen Han. 2019. Adversarial attack and defense in reinforcement\nlearning-from AI security view. Cybersecurity 2, 1 (2019), 1–22.\n[15] Antonio Emanuele Cinà, Ambra Demontis, Battista Biggio, Fabio Roli, and Marcello Pelillo. 2022. Energy-Latency Attacks via Sponge Poisoning.\narXiv:2203.08147 [cs] (March 2022). http://arxiv.org/abs/2203.08147 arXiv: 2203.08147.\n[16] Antonio Emanuele Cinà, Kathrin Grosse, Sebastiano Vascon, Ambra Demontis, Battista Biggio, Fabio Roli, and Marcello Pelillo. 2022. Backdoor\nLearning Curves: Explaining Backdoor Poisoning Beyond Influence Functions. arXiv:2106.07214 [cs] (March 2022). http://arxiv.org/abs/2106.07214\narXiv: 2106.07214.\n[17] James Clear. 2018. Atomic Habits. Random House.\n[18] Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. 2004. Adversarial classification. In Tenth ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining (KDD). Seattle, 99–108.\n[19] Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio, Alina Oprea, Cristina Nita-Rotaru, and Fabio Roli. 2019. Why Do\nAdversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks. In 28th USENIX Security Symposium (USENIX Security\n19). USENIX Association.\n[20] Yao Deng, Tiehua Zhang, Guannan Lou, Xi Zheng, Jiong Jin, and Qing-Long Han. 2021. Deep learning-based autonomous driving systems: a\nsurvey of attacks and defenses. IEEE Transactions on Industrial Informatics 17, 12 (2021), 7897–7912.\n[21] Tom Everitt, Victoria Krakovna, Laurent Orseau, and Shane Legg. 2017. Reinforcement learning with a corrupted reward channel. In Proceedings of\nthe 26th International Joint Conference on Artificial Intelligence. 4705–4713.\n[22] Marc Fischer, Matthew Mirman, Steven Stalder, and Martin Vechev. 2019. Online robustness training for deep reinforcement learning. arXiv\npreprint arXiv:1911.00887 (2019).\n[23] Harrison Foley, Liam Fowl, Tom Goldstein, and Gavin Taylor. 2022. Execute Order 66: Targeted Data Poisoning for Reinforcement Learning. arXiv\npreprint arXiv:2201.00762 (2022).\n[24] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model Inversion Attacks That Exploit Confidence Information and Basic Counter-\nmeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (CCS ’15). ACM, New York, NY, USA,\n1322–1333.\n[25] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. 2021. D4RL: Datasets for Deep Data-Driven Reinforcement Learning.\nhttps://doi.org/10.48550/arXiv.2004.07219 Number: arXiv:2004.07219 arXiv:2004.07219 [cs, stat].\n[26] Joshua Garcia, Yang Feng, Junjie Shen, Sumaya Almanee, Yuan Xia, Chen, and Qi Alfred. 2020. A comprehensive study of autonomous vehicle\nbugs. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 385–396.\n[27] DEDE Georgia, HAMON Ronan, JUNKLEWITZ Henrik, NAYDENOV Rossen, MALATRAS Apostolos, and SANCHEZ MARTIN Jose Ignacio. 2021.\nCybersecurity challenges in the uptake of Artificial Intelligence in Autonomous Driving. (2021).\n[28] Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. 2019. Adversarial Policies: Attacking Deep Reinforcement\nLearning. In International Conference on Learning Representations.\n[29] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. In\nNIPS Workshop on Machine Learning and Computer Security, Vol. abs/1708.06733.\n[30] Yi Han, David Hubczenko, Paul Montague, Olivier De Vel, Tamas Abraham, Benjamin IP Rubinstein, Christopher Leckie, Tansu Alpcan, and Sarah\nErfani. 2020. Adversarial Reinforcement Learning under Partial Observability in Autonomous Computer Network Defence. In 2020 International\nJoint Conference on Neural Networks (IJCNN). IEEE, 1–8.\n[31] Aaron J Havens, Zhanhong Jiang, and Soumik Sarkar. 2018. Online Robust Policy Learning in the Presence of Unknown Adversaries. In NeurIPS.\n25\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\n[32] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in a Neural Network. Technical Report arXiv:1503.02531. arXiv.\nhttps://doi.org/10.48550/arXiv.1503.02531 arXiv:1503.02531 [cs, stat] type: article.\n[33] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. 2017. Adversarial attacks on neural network policies. arXiv preprint\narXiv:1702.02284 (2017).\n[34] Xin Huang, Sungkweon Hong, Andreas Hofmann, and Brian C Williams. 2019. Online risk-bounded motion planning for autonomous vehicles in\ndynamic environments. In Proceedings of the International Conference on Automated Planning and Scheduling, Vol. 29. 214–222.\n[35] Yunhan Huang and Quanyan Zhu. 2019. Deceptive reinforcement learning under adversarial manipulations on cost signals. In International\nConference on Decision and Game Theory for Security. Springer, 217–237.\n[36] Inaam Ilahi, Muhammad Usama, Junaid Qadir, Muhammad Umar Janjua, Ala Al-Fuqaha, Dinh Thai Huang, and Dusit Niyato. 2021. Challenges\nand Countermeasures for Adversarial Attacks on Deep Reinforcement Learning. IEEE Transactions on Artificial Intelligence (2021), 1–1. https:\n//doi.org/10.1109/TAI.2021.3111139 Conference Name: IEEE Transactions on Artificial Intelligence.\n[37] Matthew Inkawhich, Yiran Chen, and Hai Li. 2020. Snooping Attacks on Deep Reinforcement Learning. In Proceedings of the 19th International\nConference on Autonomous Agents and MultiAgent Systems. 557–565.\n[38] Yunhan Jia Jia, Yantao Lu, Junjie Shen, Qi Alfred Chen, Hao Chen, Zhenyu Zhong, and Tao Wei Wei. 2020. Fooling detection alone is not enough:\nAdversarial attack against multiple object tracking. In International Conference on Learning Representations (ICLR’20).\n[39] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. 1996. Reinforcement learning: A survey. Journal of artificial intelligence research 4\n(1996), 237–285.\n[40] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. 2020. TrojDRL: evaluation of backdoor attacks on deep reinforcement learning.\nIn 2020 57th ACM/IEEE Design Automation Conference (DAC). IEEE, 1–6.\n[41] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A. Al Sallab, Senthil Yogamani, and Patrick Pérez. 2021. Deep Reinforcement\nLearning for Autonomous Driving: A Survey. IEEE Transactions on Intelligent Transportation Systems (2021), 1–18. https://doi.org/10.1109/TITS.\n2021.3054625 Conference Name: IEEE Transactions on Intelligent Transportation Systems.\n[42] P. W. Koh and P. Liang. 2017. Understanding Black-box Predictions via Influence Functions. In International Conference on Machine Learning\n(ICML).\n[43] Ezgi Korkmaz. 2020. Nesterov Momentum Adversarial Perturbations in the Deep Reinforcement Learning Domain.\n[44] Jernej Kos and Dawn Song. 2017. Delving into adversarial attacks on deep policies. arXiv preprint arXiv:1705.06452 (2017).\n[45] Xian Yeow Lee, Yasaman Esfandiari, Kai Liang Tan, and Soumik Sarkar. 2021. Query-based targeted action-space adversarial policies on deep\nreinforcement learning agents. In Proceedings of the ACM/IEEE 12th International Conference on Cyber-Physical Systems. 87–97.\n[46] Xian Yeow Lee, Sambit Ghadai, Kai Liang Tan, Chinmay Hegde, and Soumik Sarkar. 2020. Spatiotemporally constrained action space attacks on\ndeep reinforcement learning agents. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 4577–4584.\n[47] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. 2017. Tactics of adversarial attack on deep reinforcement\nlearning agents. In Proceedings of the 26th International Joint Conference on Artificial Intelligence. 3756–3762.\n[48] Yen-Chen Lin, Ming-Yu Liu, Min Sun, and Jia-Bin Huang. 2017. Detecting adversarial attacks on neural network policies with visual foresight.\narXiv preprint arXiv:1710.00814 (2017).\n[49] Guanlin Liu and Lifeng Lai. 2021. Provably Efficient Black-Box Action Poisoning Attacks Against Reinforcement Learning. In NeurIPS.\n[50] Wei Liu and Sanjay Chawla. 2010. Mining adversarial patterns via regularized loss minimization. Machine Learning 81, 1 (2010), 69–83.\n[51] Manuel Lopez-Martin, Belen Carro, and Antonio Sanchez-Esguevillas. 2020. Application of deep reinforcement learning to intrusion detection for\nsupervised problems. Expert Systems with Applications 141 (2020), 112963. https://doi.org/10.1016/j.eswa.2019.112963\n[52] Daniel Lowd and Christopher Meek. 2005. Adversarial Learning. In Proc. 11th ACM SIGKDD International Conference on Knowledge Discovery and\nData Mining (KDD). ACM Press, Chicago, IL, USA, 641–647.\n[53] Yuzhe Ma, Kwang-Sung Jun, Lihong Li, and Xiaojin Zhu. 2018. Data poisoning attacks in contextual bandits. In International Conference on Decision\nand Game Theory for Security. Springer, 186–204.\n[54] Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Xiaojin Zhu. 2019. Policy poisoning in batch reinforcement learning and control. Advances in Neural\nInformation Processing Systems (2019).\n[55] Rubén Majadas, Javier García, and Fernando Fernández. 2021. Disturbing Reinforcement Learning Agents with Corrupted Rewards. arXiv preprint\narXiv:2102.06587 (2021).\n[56] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li Fei-Fei, and Silvio Savarese. 2017. Adversarially robust policy learning: Active construction of\nphysically-plausible perturbations. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 3932–3939.\n[57] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari\nwith deep reinforcement learning. arXiv preprint arXiv:1312.5602 (2013).\n[58] Hossein Mobahi, Mehrdad Farajtabar, and Peter Bartlett. 2020.\nSelf-Distillation Amplifies Regularization in Hilbert Space. In Advances\nin Neural Information Processing Systems, Vol. 33. Curran Associates, Inc., 3351–3361.\nhttps://proceedings.neurips.cc/paper/2020/hash/\n2288f691b58edecadcc9a8691762b4fd-Abstract.html\n[59] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. 2017. Universal adversarial perturbations. In CVPR.\n[60] Thanh Thi Nguyen and Vijay Janapa Reddi. 2021. Deep Reinforcement Learning for Cyber Security. IEEE Transactions on Neural Networks and\nLearning Systems (2021), 1–17.\nhttps://doi.org/10.1109/TNNLS.2021.3121870 Conference Name: IEEE Transactions on Neural Networks and\n26\nA Survey on Reinforcement Learning Security\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nLearning Systems.\n[61] Tuomas Oikarinen, Tsui-Wei Weng, and Luca Daniel. 2020. Robust deep reinforcement learning through adversarial loss. arXiv preprint\narXiv:2008.01976 (2020).\n[62] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. 2019.\nKnockoff Nets: Stealing Functionality of Black-Box Models. 4954–\n4963. https://openaccess.thecvf.com/content_CVPR_2019/html/Orekondy_Knockoff_Nets_Stealing_Functionality_of_Black-Box_Models_CVPR_\n2019_paper.html\n[63] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. 2016. Transferability in machine learning: from phenomena to black-box attacks using\nadversarial samples. arXiv preprint arXiv:1605.07277 (2016).\n[64] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. 2016. Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks.\nIn 2016 IEEE Symposium on Security and Privacy (SP). 582–597. https://doi.org/10.1109/SP.2016.41\n[65] Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. 2017. Robust deep reinforcement learning with\nadversarial attacks. arXiv preprint arXiv:1712.03632 (2017).\n[66] Scott Drew Pendleton, Hans Andersen, Xinxin Du, Xiaotong Shen, Malika Meghjani, You Hong Eng, Daniela Rus, and Marcelo H Ang. 2017.\nPerception, planning, control, and coordination for autonomous vehicles. Machines 5, 1 (2017), 6.\n[67] Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. 2017. Robust adversarial reinforcement learning. In International Conference\non Machine Learning. PMLR, 2817–2826.\n[68] David L Poole and Alan K Mackworth. 2010. Artificial Intelligence: foundations of computational agents. Cambridge University Press.\n[69] Adnan Qayyum, Muhammad Usama, Junaid Qadir, and Ala Al-Fuqaha. 2020. Securing connected & autonomous vehicles: Challenges posed by\nadversarial machine learning and the way forward. IEEE Communications Surveys & Tutorials 22, 2 (2020), 998–1026.\n[70] Xinghua Qu, Yew-Soon Ong, Abhishek Gupta, and Zhu Sun. 2020. Adversary Agnostic Robust Deep Reinforcement Learning. arXiv preprint\narXiv:2008.06199 (2020).\n[71] Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine. 2017. EPOpt: Learning Robust Neural Network Policies Using\nModel Ensembles. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track\nProceedings.\n[72] Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. 2020. Policy teaching via environment poisoning: Training-time\nadversarial attacks against reinforcement learning. In International Conference on Machine Learning. PMLR, 7974–7984.\n[73] Amin Rakhsha, Xuezhou Zhang, Xiaojin Zhu, and Adish Singla. 2021. Reward poisoning in reinforcement learning: Attacks against unknown\nlearners in unknown environments. arXiv preprint arXiv:2102.08492 (2021).\n[74] Alessio Russo and Alexandre Proutiere. 2019. Optimal attacks on reinforcement learning policies. arXiv preprint arXiv:1907.13548 (2019).\n[75] J SAE. 2014. 3016 (2014). Taxonomy and definitions for terms related to on-road motor vehicle automated driving systems. Society of Automotive\nEngineers (2014).\n[76] Rick Salay, Rodrigo Queiroz, and Krzysztof Czarnecki. 2018. An analysis of ISO 26262: Machine learning and safety in automotive software. Technical\nReport. SAE Technical Paper.\n[77] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint\narXiv:1707.06347 (2017).\n[78] Junjie Shen, Ningfei Wang, Ziwen Wan, Yunpeng Luo, Takami Sato, Zhisheng Hu, Xinyang Zhang, Shengjian Guo, Zhenyu Zhong, Kang Li, et al.\n2022. SoK: On the Semantic AI Security in Autonomous Driving. arXiv preprint arXiv:2203.05314 (2022).\n[79] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. 2017. Membership Inference Attacks Against Machine Learning Models. In 2017 IEEE Symposium\non Security and Privacy (SP). 3–18.\n[80] Ilia Shumailov, Yiren Zhao, Daniel Bates, Nicolas Papernot, Robert Mullins, and Ross Anderson. 2021. Sponge Examples: Energy-Latency Attacks\non Neural Networks. In 2021 IEEE European Symposium on Security and Privacy (EuroS P). 212–231. https://doi.org/10.1109/EuroSP51992.2021.00024\n[81] Jianwen Sun, Tianwei Zhang, Xiaofei Xie, Lei Ma, Yan Zheng, Kangjie Chen, and Yang Liu. 2020. Stealthy and efficient adversarial attacks against\ndeep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 5883–5891.\n[82] Yanchao Sun, Ruijie Zheng, Yongyuan Liang, and Furong Huang. 2021. Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion\nAttacks in Deep RL. arXiv preprint arXiv:2106.05087 (2021).\n[83] Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction, by Sutton, R.S. and Barto, A.G. Vol. 3.\n[84] Kai Liang Tan, Yasaman Esfandiari, Xian Yeow Lee, Soumik Sarkar, et al. 2020. Robustifying reinforcement learning agents via action space\nadversarial training. In 2020 American control conference (ACC). IEEE, 3959–3964.\n[85] Atanas Tanev, Svetlana Pavlitskaya, Joan Sigloch, Arne Roennau, Ruediger Dillmann, and J Marius Zollner. 2021. Adversarial Black-Box Attacks\non Vision-based Deep Reinforcement Learning Agents. In 2021 IEEE International Conference on Intelligence and Safety for Robotics (ISR). IEEE,\n177–181.\n[86] Buse GA Tekgul, Shelly Wang, Samuel Marchal, and N Asokan. 2021. Real-time Attacks Against Deep Reinforcement Learning Policies. arXiv\npreprint arXiv:2106.08746 (2021).\n[87] Chen Tessler, Yonathan Efroni, and Shie Mannor. 2019. Action robust reinforcement learning and applications in continuous control. In International\nConference on Machine Learning. PMLR, 6215–6224.\n27\nWoodstock ’18, June 03–05, 2018, Woodstock, NY\nDemontis, et al.\n[88] Chen Tessler, Yonathan Efroni, and Shie Mannor. 2019. Action Robust Reinforcement Learning and Applications in Continuous Control (Proceedings\nof Machine Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, Long Beach, California, USA, 6215–6224.\nhttp://proceedings.mlr.press/v97/tessler19a.html\n[89] Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart. 2016. Stealing Machine Learning Models via Prediction APIs. In\n25th USENIX Security Symposium (USENIX Security 16). USENIX Association, Austin, TX, 601–618.\n[90] Edgar Tretschk, Seong Joon Oh, and Mario Fritz. 2018. Sequential Attacks on Agents for Long-Term Adversarial Goals. In 2. ACM Computer Science\nin Cars Symposium.\n[91] Xiaoyue Wan, Geyi Sheng, Yanda Li, Liang Xiao, and Xiaojiang Du. 2017. Reinforcement Learning Based Mobile Offloading for Cloud-Based\nMalware Detection. In GLOBECOM 2017 - 2017 IEEE Global Communications Conference. 1–6. https://doi.org/10.1109/GLOCOM.2017.8254503\n[92] Jingkang Wang, Yang Liu, and Bo Li. 2020. Reinforcement learning with perturbed rewards. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 34. 6202–6209.\n[93] Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song. 2021. BACKDOORL: Backdoor Attack against Competitive\nReinforcement Learning. arXiv preprint arXiv:2105.00579 (2021).\n[94] Yue Wang, Esha Sarkar, Wenqing Li, Michail Maniatakos, and Saif Eddin Jabari. 2021. Stop-and-Go: Exploring Backdoor Attacks on Deep\nReinforcement Learning-Based Traffic Congestion Control Systems. IEEE Transactions on Information Forensics and Security 16 (2021), 4772–4787.\nhttps://doi.org/10.1109/TIFS.2021.3114024\n[95] Chaowei Xiao, Xinlei Pan, Warren He, Jian Peng, Mingjie Sun, Jinfeng Yi, Mingyan Liu, Bo Li, and Dawn Song. 2019. Characterizing attacks on\ndeep reinforcement learning. arXiv preprint arXiv:1907.09470 (2019).\n[96] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. 2017. Adversarial examples for semantic segmentation and\nobject detection. In Proceedings of the IEEE international conference on computer vision. 1369–1378.\n[97] Hang Xu, Rundong Wang, Lev Raizman, and Zinovi Rabinovich. 2021. Transferable Environment Poisoning: Training-time Attack on Reinforcement\nLearning. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems. 1398–1406.\n[98] Yoriyuki Yamagata, Shuang Liu, Takumi Akazaki, Yihai Duan, and Jianye Hao. 2021. Falsification of Cyber-Physical Systems Using Deep\nReinforcement Learning. IEEE Transactions on Software Engineering 47, 12 (Dec. 2021), 2823–2840.\nhttps://doi.org/10.1109/TSE.2020.2969178\nConference Name: IEEE Transactions on Software Engineering.\n[99] Chao-Han Huck Yang, Jun Qi, Pin-Yu Chen, Yi Ouyang, I-Te Danny Hung, Chin-Hui Lee, and Xiaoli Ma. 2020. Enhanced adversarial strategically-\ntimed attacks against deep reinforcement learning. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 3407–3411.\n[100] Huan Zhang, Hongge Chen, Duane Boning, and Cho-Jui Hsieh. 2021. Robust reinforcement learning on state observations with learned optimal\nadversary. In ICLR.\n[101] Kaiqing Zhang, Tao Sun, Yunzhe Tao, Sahika Genc, Sunil Mallya, and Tamer Basar. 2020. Robust Multi-Agent Reinforcement Learning with Model\nUncertainty.. In NeurIPS.\n[102] Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. 2020. Adaptive reward-poisoning attacks against reinforcement learning. In International\nConference on Machine Learning. PMLR, 11225–11234.\n[103] Yiren Zhao, Ilia Shumailov, Han Cui, Xitong Gao, Robert Mullins, and Ross Anderson. 2020. Blackbox attacks on reinforcement learning agents\nusing approximated temporal information. In 2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops\n(DSN-W). IEEE, 16–24.\n28\n",
  "categories": [
    "cs.LG",
    "cs.RO"
  ],
  "published": "2022-12-12",
  "updated": "2022-12-12"
}