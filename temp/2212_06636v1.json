{
  "id": "http://arxiv.org/abs/2212.06636v1",
  "title": "Categorical Tools for Natural Language Processing",
  "authors": [
    "Giovanni de Felice"
  ],
  "abstract": "This thesis develops the translation between category theory and\ncomputational linguistics as a foundation for natural language processing. The\nthree chapters deal with syntax, semantics and pragmatics. First, string\ndiagrams provide a unified model of syntactic structures in formal grammars.\nSecond, functors compute semantics by turning diagrams into logical, tensor,\nneural or quantum computation. Third, the resulting functorial models can be\ncomposed to form games where equilibria are the solutions of language\nprocessing tasks. This framework is implemented as part of DisCoPy, the Python\nlibrary for computing with string diagrams. We describe the correspondence\nbetween categorical, linguistic and computational structures, and demonstrate\ntheir applications in compositional natural language processing.",
  "text": "Categorical Tools for Natural\nLanguage Processing\nGiovanni de Felice\nWolfson College\nUniversity of Oxford\nA thesis submitted for the degree of\nDoctor of Philosophy\nMichaelmas 2022\narXiv:2212.06636v1  [cs.CL]  13 Dec 2022\nAbstract\nThis thesis develops the translation between category theory and com-\nputational linguistics as a foundation for natural language processing.\nThe three chapters deal with syntax, semantics and pragmatics. First,\nstring diagrams provide a uniﬁed model of syntactic structures in for-\nmal grammars. Second, functors compute semantics by turning diagrams\ninto logical, tensor, neural or quantum computation. Third, the result-\ning functorial models can be composed to form games where equilibria\nare the solutions of language processing tasks. This framework is imple-\nmented as part of DisCoPy, the Python library for computing with string\ndiagrams. We describe the correspondence between categorical, linguis-\ntic and computational structures, and demonstrate their applications in\ncompositional natural language processing.\nAcknowledgements\nI would like to thank my supervisor Bob Coecke for introducing me to the\nwonderland of string diagrams, for supporting me throughout my studies\nand always encouraging me to switch topics and pursue my ideas. This\nthesis is the fruit of a thousand discussions, board sessions, smokes and\nbeers with Alexis Toumi. I am grateful to him for having the patience\nto teach Python to a mathematician, for his loyal friendship and the con-\ntinual support he has given me in both personal matters and research. I\nwant to thank my examiners Aleks Kissinger and Pawel Sobocinski for\ntheir detailed feedback on the ﬁrst version of this thesis, and their sug-\ngestion to integrate the passage from categories to Python code. Thanks\nalso to Andreas Petrossantis, Sebastiano Cultrera and Dimitri Kartsaklis\nfor valuable comments on this manuscript, to Dan Marsden and Prakash\nPanangaden for providing guidance in my early research, and to Samson\nAbramsky for prompting me to search into the deeper history of applied\ncategory theory.\nAmong fellow collaborators who have shared their wisdom and passion, for\nmany insightful discussions, I would like to thank Amar Hadzihasanovic,\nRui Soares Barbosa, David Reutter, Antonin Delpeuch, Stefano Gogioso,\nKonstantinos Meichanetzidis, Mario Roman, Elena Di Lavore and Richie\nYeung. Among my friends, who have been there for me in times of sadness\nand of joy, and made me feel at home in Oxford, Rome and Sicily, spe-\ncial thanks to Tommaso Salvatori, Pinar Kolancali, Tommaso Battistini,\nEmanuele Torlonia, Benedetta Magnano and Pietro Scammacca. Finally,\na very special thanks to Nonna Miti for hosting me in her garden and to\nmy mother and father for their loving support.\nContents\nAcknowledgements\n5\nIntroduction\n11\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n1\nDiagrams for Syntax\n19\n1.1\nArrows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n1.1.1\nCategories . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n1.1.2\nRegular grammars\n. . . . . . . . . . . . . . . . . . . . . . . .\n24\n1.1.3\ncat.Arrow . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n1.2\nTrees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n1.2.1\nOperads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n1.2.2\nContext-free grammars . . . . . . . . . . . . . . . . . . . . . .\n33\n1.2.3\noperad.Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n1.3\nDiagrams\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n1.3.1\nMonoidal categories . . . . . . . . . . . . . . . . . . . . . . . .\n40\n1.3.2\nMonoidal grammars . . . . . . . . . . . . . . . . . . . . . . . .\n43\n1.3.3\nFunctorial reductions . . . . . . . . . . . . . . . . . . . . . . .\n45\n1.3.4\nmonoidal.Diagram\n. . . . . . . . . . . . . . . . . . . . . . . .\n48\n1.4\nCategorial grammar . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n1.4.1\nBiclosed categories . . . . . . . . . . . . . . . . . . . . . . . .\n54\n1.4.2\nAjdiuciewicz . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n1.4.3\nLambek . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n1.4.4\nCombinatory\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n1.4.5\nbiclosed.Diagram . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n1.5\nPregroups and dependencies . . . . . . . . . . . . . . . . . . . . . . .\n66\n1.5.1\nPregroups and rigid categories . . . . . . . . . . . . . . . . . .\n66\n1.5.2\nDependency grammars are pregroups . . . . . . . . . . . . . .\n71\n1.5.3\nrigid.Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n1.6\nHypergraphs and coreference . . . . . . . . . . . . . . . . . . . . . . .\n80\n1.6.1\nHypergraph categories . . . . . . . . . . . . . . . . . . . . . .\n80\n1.6.2\nPregroups with coreference . . . . . . . . . . . . . . . . . . . .\n83\n1.6.3\nhypergraph.Diagram . . . . . . . . . . . . . . . . . . . . . . .\n86\n7\n2\nFunctors for Semantics\n91\n2.1\nConcrete categories in Python . . . . . . . . . . . . . . . . . . . . . .\n93\n2.1.1\nTensor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n2.1.2\nFunction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n2.2\nMontague models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n103\n2.2.1\nLambda calculus\n. . . . . . . . . . . . . . . . . . . . . . . . .\n103\n2.2.2\nTyped ﬁrst-order logic . . . . . . . . . . . . . . . . . . . . . .\n106\n2.2.3\nMontague semantics\n. . . . . . . . . . . . . . . . . . . . . . .\n107\n2.2.4\nMontague in DisCoPy\n. . . . . . . . . . . . . . . . . . . . . .\n110\n2.3\nNeural network models . . . . . . . . . . . . . . . . . . . . . . . . . .\n113\n2.3.1\nFeed-forward networks . . . . . . . . . . . . . . . . . . . . . .\n113\n2.3.2\nRecurrent networks . . . . . . . . . . . . . . . . . . . . . . . .\n116\n2.3.3\nRecursive networks . . . . . . . . . . . . . . . . . . . . . . . .\n118\n2.3.4\nAttention is all you need?\n. . . . . . . . . . . . . . . . . . . .\n120\n2.3.5\nNeural networks in DisCoPy . . . . . . . . . . . . . . . . . . .\n123\n2.4\nRelational models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n125\n2.4.1\nDatabases and queries\n. . . . . . . . . . . . . . . . . . . . . .\n125\n2.4.2\nThe category of relations . . . . . . . . . . . . . . . . . . . . .\n128\n2.4.3\nGraphical conjunctive queries . . . . . . . . . . . . . . . . . .\n129\n2.4.4\nRelational models . . . . . . . . . . . . . . . . . . . . . . . . .\n131\n2.4.5\nEntailment and question answering . . . . . . . . . . . . . . .\n134\n2.5\nTensor network models . . . . . . . . . . . . . . . . . . . . . . . . . .\n137\n2.5.1\nTensor networks . . . . . . . . . . . . . . . . . . . . . . . . . .\n137\n2.5.2\nTensor functors . . . . . . . . . . . . . . . . . . . . . . . . . .\n140\n2.5.3\nDisCoCat and bounded memory . . . . . . . . . . . . . . . . .\n143\n2.5.4\nBubbles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n145\n2.6\nKnowledge graph embeddings . . . . . . . . . . . . . . . . . . . . . .\n147\n2.6.1\nEmbeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n147\n2.6.2\nRescal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n148\n2.6.3\nDistMult . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n149\n2.6.4\nComplEx\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n150\n2.7\nQuantum models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n155\n2.7.1\nQuantum circuits . . . . . . . . . . . . . . . . . . . . . . . . .\n155\n2.7.2\nQuantum models . . . . . . . . . . . . . . . . . . . . . . . . .\n158\n2.7.3\nAdditive approximations . . . . . . . . . . . . . . . . . . . . .\n159\n2.7.4\nApproximating quantum models . . . . . . . . . . . . . . . . .\n160\n2.8\nDisCoPy in action\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n165\n3\nGames for Pragmatics\n169\n3.1\nProbabilistic models\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n171\n3.1.1\nCategorical probability . . . . . . . . . . . . . . . . . . . . . .\n171\n3.1.2\nDiscriminators . . . . . . . . . . . . . . . . . . . . . . . . . . .\n174\n3.1.3\nGenerators . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n177\n3.2\nBidirectional tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n179\n3.2.1\nLenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n179\n8\n3.2.2\nUtility functions . . . . . . . . . . . . . . . . . . . . . . . . . .\n182\n3.2.3\nMarkov rewards . . . . . . . . . . . . . . . . . . . . . . . . . .\n183\n3.3\nCybernetics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n185\n3.3.1\nParametrization . . . . . . . . . . . . . . . . . . . . . . . . . .\n185\n3.3.2\nOpen games . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n186\n3.3.3\nMarkov decisions . . . . . . . . . . . . . . . . . . . . . . . . .\n189\n3.3.4\nRepeated games . . . . . . . . . . . . . . . . . . . . . . . . . .\n191\n3.4\nExamples\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n194\n3.4.1\nBayesian pragmatics\n. . . . . . . . . . . . . . . . . . . . . . .\n194\n3.4.2\nAdversarial question answering\n. . . . . . . . . . . . . . . . .\n196\n3.4.3\nWord sense disambiguation\n. . . . . . . . . . . . . . . . . . .\n197\nReferences\n201\n9\n10\nIntroduction\nSince the very beginnings of human inquiry into language, people have investigated\nthe natural processes by which we learn, understand and produce linguistic meaning.\nOnly recently, however, the ﬁeld of linguistics has become an autonomous scientiﬁc\ndiscipline. The origins of this modern science are closely interlinked with the birth\nof mathematical logic at the end of the nineteenth century. In the United States,\nPeirce founded “semiotics” — the science of signs and their interpretation — while\ndeveloping graphical calculi for logical inference. At around the same time, in the\nUnited Kingdom, Frege and Russell developed formal languages for logic in the search\nfor a Leibnizian “characteristica universalis” while discussing the sense and reference\nof linguistic phrases.\nThese mathematical origins initiated a formal and computational approach to\nlinguistics, often referred to as the symbolic tradition, which aims at characterising\nlanguage understanding in terms of structured logical processes and the automatic\nmanipulation of symbols.\nOn the one hand, it led to the development of mathe-\nmatical theories of syntax, such as the categorial grammars stemming from the Pol-\nish school of logic [Ajd35; Lam58] and Chomsky’s inﬂuential generative grammars\n[Cho57]. On the other hand, it allowed for the development of formal approaches to\nsemantics such as Tarski’s theory of truth [Tar36; Tar43], which motivated the work\nof Davidson [Dav67a] and Montague [Mon70b] in extracting the logical form of nat-\nural language sentences. From the technological perspective, these theories enabled\nthe design of programming languages, the construction of large-scale databases for\nstoring structured knowledge and linguistic data, as well as the implementation of\nexpert computer systems driven by formal logical rules to reason about this accrued\nknowledge. Since the 1990s, the symbolic tradition has been challenged by a series of\nnew advances motivated by the importance of context and ambiguity in language use\n[BP83]. With the growing amount of data and large-scale corpora available on the\ninternet, statistical inference methods based on n-grams, Markov models or Bayesian\nclassiﬁers allowed for experiments to tackle new problems such as speech recognition\nand machine translation [MS99]. The distributional representation of meaning in vec-\ntor spaces [SW97] was found suitable for disambiguating words in context [Sch98] and\ncomputing synonymity [TP10]. Furthermore, connectionist models based on neural\nnetworks have produced impressive results in the last couple of decades, outperform-\ning previous models on a range of tasks such as language modelling [Ben+03; Mik+10;\nVas+17], word sense disambiguation [Nav09], sentiment analysis [Soc+13a; Cha+22],\nquestion answering [JM08; LHH20] and machine translation [BCB14; Edu+18].\n11\nDriven by large-scale industrial applications, the focus gradually shifted from the-\noretical enquiries into linguistic phenomena to the practical concern of building highly\nparallelizable connectionist code for beating state-of-the-art algorithms. Recently, a\ntransformer neural network with billions of parameters (GPT-3) [Bro+20] wrote a\nGuardian article on why humans have nothing to fear from AI. The reasons for how\nand why GPT-3 “chose” to compose the text in the way that it did is a mystery and\nthe structure of its mechanism remains a “black box”. Connectionist models have\nshown the importance of the distributional aspect of language and the eﬀectiveness\nof machine learning techniques in NLP. However, their task-speciﬁcity and the diﬃ-\nculty in analysing the underlying processes which concur in their output are limits\nwhich need to be addressed. Recent developments in machine learning have shown\nthe importance of taking structure into account when tackling scientiﬁc questions\nin network science [Wu+20], chemistry [Kea+16], biology [Jum+21; Zho+21]. NLP\nwould also beneﬁt from the same grounding in order to analyse and interpret the\ngrowing “library of Babel” of natural language data.\nCategory theory can help build models of language amenable to both linguis-\ntic reasoning and numerical computation. Its roots are the same as computational\nlinguistics, as categories were used to link algebra, logic and computation [Law63;\nLam86; LS86]. Category theory has since followed a solid thread of applications,\nfrom the semantics of programming languages [SRP91; AJ95] to the modelling of a\nwide range of computational systems, including knowledge-based [Spi12], quantum\n[AC07; CK17], dataﬂow [BSZ14], statistical [VKS19] and diﬀerentiable [AD19] pro-\ncesses. In the Compositional Distributional models of Coecke et al. [CCS08; CCS10;\nSCC13] (DisCoCat), categories are used to design models of language in which the\nmeaning of sentences is derived by composition from the distributional embeddings of\nwords. Generalising from this work, language can be viewed as a syntax for arranging\nsymbols together with a functor for interpreting them. Speciﬁcally, syntactic struc-\ntures form a free category of string diagrams, while meaning is computed in categories\nof numerical functions. Functorial models can then be learnt in data-driven tasks.\nThe aim of this thesis is to provide a uniﬁed framework of mathematical tools to\nbe applied in three important areas of computational linguistics: syntax, semantics\nand pragmatics. We provide an implementation of this framework in object-oriented\nPython, by translating categorical concepts into classes and methods. This transla-\ntion has lead to the development of DisCoPy [dTC21], an open-source Python toolbox\nfor computing with string diagrams and functors. We show the potential of this frame-\nwork for reasoning about compositional models of language and building structured\nNLP pipelines. We show the correspondence between categorical and linguistic no-\ntions and we describe their implementation as methods and interfaces in DisCoPy.\nThe library is available, with an extensive documentation and testing suite, at:\nhttps://github.com/oxford-quantum-group/discopy\nIn Chapter 1, on syntax, we use the theory of free categories and string diagrams to\nformalise Chomsky’s regular, context-free and unrestricted grammars [Cho56]. With\nthe same tools, the categorial grammars of Ajdiuciewicz [Ajd35], Lambek [Lam58]\n12\nand Steedman [Ste00], as well as Lambek’s pregroups [Lam99a] and Tesniere’s depen-\ndency grammars [Tes59; Gai65], are formalised. We lay out the architecture of the\nsyntactic modules of DisCoPy, with interfaces for the corresponding formal models\nof grammar and functorial reductions between them. The second chapter deals with\nsemantics. We use Lawvere’s concept of functorial semantics [Law63] to deﬁne sev-\neral NLP models, including logical, distributional and connectionist approaches. By\nvarying the target semantic category, we recover knowledge-based, tensor-based and\nquantum models of language, as well as Montague’s logical semantics [Mon70b] and\nconnectionist models based on neural networks. The implementation of these models\nin Python is obtained by deﬁning semantic classes that carry out concrete compu-\ntation. We describe the implementation of the main semantic modules of DisCoPy\nand their interface with high-performance libraries for numerical computation. This\nframework is then applied to the study of the pragmatic aspects of language use in\ncontext and the design of NLP tasks in Chapter 3. To this end, we use the recent\napplications of category theory to statistics [CJ19; Fri20], machine learning [FST19;\nCru+21] and game theory [Gha+18] to develop a formal language for modelling com-\npositions of NLP models into games and pragmatic tasks.\nThe mathematical framework developed in this thesis provides a structural under-\nstanding of natural language processing, allowing for both interpreting existing NLP\nmodels and building new ones. The proposed software is expected to contribute to\nthe design of language processing systems and their implementation using symbolic,\nstatistical, connectionist and quantum computing.\n13\nContributions\nThe aim of this thesis is to provide a framework of mathematical tools for computa-\ntional linguistics. The three chapters are related to syntax, semantics and pragmatics,\nrespectively.\nIn Chapter 1, a uniﬁed theory of formal grammars is provided in terms of free cat-\negories and functorial reductions and implemented in object-oriented Python. This\nwork started from discussions with Alexis Toumi, Bob Coecke, Mernoosh Sadrzadeh,\nDan Marsden and Konstantinos Meichanetzidis about logical and distributional mod-\nels of natural language [Coe+18; dMT20; Coe+22]. It was a driving force in the\ndevelopment of DisCoPy [dTC21]. The main contributions are as follows.\n1. A uniﬁed treatment of formal grammars is provided using the theory of string\ndiagrams in free monoidal categories. We cover Chomsky’s regular 1.1, context-\nfree 1.2 and unrestricted grammar 1.3, corresponding to free categories, free op-\nerads and free monoidal categories respectively. This picture is obtained by ag-\ngregating the results of Walters and Lambek [Wal89b; Wal89a; Lam88; STS20].\nUsing the same tools, we formalise categorial grammars 1.4, as well as pregroups\nand dependency grammars 1.5 in terms of biclosed and rigid categories. The\nlinks between pregroups and rigid categories are known since [PL07], those be-\ntween categorial grammar and biclosed categories were previously discussed by\nLambek [Lam88] but not fully worked out, while the categorical formalisation\nof dependency grammar is novel. We also introduce the notion of pregroup with\ncoreference for discourse representation in 1.6 [Coe+18] to oﬀer an alternative\nto the DisCoCirc framework of Coecke [Coe20] which can be implemented with\nreadily available tools.\nTo the best of our knowledge, this is the ﬁrst time\nthat models from the Chomskyan and categorial traditions appear in the same\nframework, and that the full correspondence between linguistic and categorical\nnotions is spelled out.\n2. Functorial reductions between formal grammars are introduced as a convenient\nintermediate notion between weak and strong equivalences, and used to com-\npute normal forms of context-free grammars in 1.3.3. We use this notion in\nthe remainder of the chapter to capture the relationship between: i) CFGs and\ncategorial grammar (Propositions 1.4.8 and ), ii) categorial grammar and bi-\nclosed categories (Propositions 1.4.7, 1.4.12 and 1.4.17), iii) categorial and pre-\ngroup grammars (Propositions 1.5.7 and 1.5.13) and iv) pregroups, dependency\ngrammars and CFGs in 1.5.2. The latter yields a novel result showing that\ndependency grammars are the structural intersection of pregroups and CFGs\n(Theorem 1.5.23).\n3. The previously introduced categorical deﬁnitions are implemented in object-\noriented Python. The structure of this chapter follows the architecture of the\nsyntactic modules of DisCoPy, as described at the end of this section. Free\ncategories and syntactic structures are implemented by subclassing cat.Arrow\n14\nor monoidal.Diagram, the core data structures of DisCoPy. Functorial reduc-\ntions are implemented by calling the corresponding Functor class. We interface\nDisCoPy with linguistic tools for large-scale parsing.\nIn Chapter 2, functorial semantics is applied to the study of natural language\nprocessing models. Once casted in this algebraic framework, it becomes possible to\nprove complexity results and compare diﬀerent NLP pipelines, while also implement-\ning these models in DisCoPy. Section 2.4 is based on joint work with Alexis Toumi\nand Konstantinos Meichanetzidis on relational semantics [dMT20]. Sections 2.5, 2.8\nand 2.7 are based on the recent quantum models for NLP introduced with Bob Co-\necke, Alexis Toumi and Konstantinos Meichanetzidis [Mei+21; Mei+20] and further\ndeveloped in [Kar+21; TdY22]. We list the main contributions of this chapter.\n1. NLP models are given a uniﬁed theory, formalised as functors from free syn-\ntactic categories to concrete categories of numerical structures. These include\ni) knowledge-based relational models 2.4 casted as functors into the category\nof relations, ii) tensor network models 2.5 seen as functors into the category\nof matrices and including factorisation models for knowledge-graph embedding\ncovered in 2.6 iii) quantum NLP models 2.7 casted as functors into the category\nof quantum circuits, iv) Montague semantics 2.2 given by functors into carte-\nsian closed categories, and v) recurrent and recursive neural network models\n2.3 which appear as functors from grammar to the category of functions on\neuclidean spaces.\n2. We prove complexity results on the evaluation of these functorial models and\nrelated NLP tasks. Expanding on [dMT20], we use relational models to deﬁne\nNP-complete entailment and question-answering problems (Propositions 2.4.26,\n2.4.31 and 2.4.33). We show that the evaluation of tensor network models is in\ngeneral #P-complete (Proposition 2.5.18) but that it becomes tractable when\nthe input structures come from a dependency grammar (Proposition 2.5.20). We\nshow that the additive approximation of quantum NLP models is a BQP-complete\nproblem (Proposition 2.7.18). We also prove results showing that Montague\nsemantics is intractable in its general form (Propositions 2.2.14, 2.2.16).\n3. Montague semantics is given a detailed formalisation in terms of free cartesian\nclosed categories. This corrects a common misconception in the DisCoCat lit-\nerature, and in particular in [SCC13; SCC14], where Montague semantics is\nseen as a functor from pregroup grammars to relations. These logical models\nare studied in 2.4, but they are distinct from Montague semantics where the\nlambda calculus and higher-order types play an important role 2.2.\n4. We show how to implement functorial semantics in DisCoPy. More precisely,\nthe implementation of the categories of tensors and Python functions is de-\nscribed in 2.1. We then give a concrete example of how to solve a knowledge-\nembedding task in DisCoPy by learning functors 2.8. We deﬁne currying and\n15\nuncurrying of Python functions 2.2 and use it to give a proof-of-concept imple-\nmentation of Montague semantics. We deﬁne sequential and parallel composi-\ntion of Tensorﬂow/Keras models [Cho+15], allowing us to construct recursive\nneural networks with a DisCoPy functor 2.3.\nIn Chapter 3, we develop formal diagrammatic tools to model pragmatic scenarios\nand natural language processing tasks.\nThis Chapter is based on our work with\nMario Roman, Elena Di Lavore and Alexis Toumi [de +21], and provides a basis\nfor the formalisation of monoidal streams in the stochastic setting [DdR22].\nThe\ncontribution for this section is still at a preliminary stage, but the diagrammatic\nnotation succeeds in capturing and generalising a range of approaches found in the\nliterature as follows.\n1. Categorical probability [Fri20] is applied to the study of discriminative and gen-\nerative language models using notions from Bayesian statistics in 3.1. We inves-\ntigate the category of lenses [Ril18a] over discrete probability distributions in\n3.2 and use it to characterise notions of context, utility and reward for iterated\nstochastic processes [DdR22] (see Propositions 3.2.4 and 3.2.6). Finally, we ap-\nply open games [BHZ19] the study of Markov decision processes and repeated\ngames in 3.3, while giving examples relevant to NLP.\n2. Three NLP applications of the developed tools are provided in 3.4: i) we discuss\nreference games [FG12; MP15; BLG16] and give a diagrammatic proof that\nBayesian inversion yields a Nash equilibrium (Proposition 3.4.1), ii) we deﬁne\na question answering game between a teacher and a student and compute the\nNash equilibria when the student’s strategies are given by relational models [de\n+21] and iii) we give a compositional approach to word-sense disambiguation\nas a game between words where strategies are word-senses.\nThe DisCoPy implementation, carried out with Alexis Toumi [dTC21], is de-\nscribed throughout the ﬁrst two chapters of this thesis. We focused on i) the passage\nfrom categorical deﬁnitions to object-oriented Python and ii) the applications of Dis-\nCoPy to Natural Language Processing. Every section of Chapter 1 corresponds to a\nsyntactic module in DisCoPy, as we detail.\n1. In 1.1, we view derivations of regular grammars as arrows in the free cate-\ngory and show how these notions are implemented via the core DisCoPy class\ncat.Arrow.\n2. In 1.2, we study context-free grammars in terms of trees in the free operad and\ngive an implementation of free operads and their algebras in DisCoPy. This\nis a new operad module which has been written for this thesis and features\ninterfaces with NLTK [LB02] for CFG and SpaCy [Hon+20] for dependencies.\n3. In 1.3, we formalise Chomsky’s unrestricted grammars in terms of monoidal\nsignatures and string diagrams and describe the implementation of the key\nDisCoPy class monoidal.Diagram.\n16\n4. In 1.4, we formalise categorial grammar in terms of free biclosed categories.\nWe give an implementation of biclosed.Diagram as a monoidal diagram with\ncurry and uncurry methods and show its interface with state-of-the-art cate-\ngorial grammar parsers, as provided by Lambeq [Kar+21].\n5. In 1.5, we show that pregroup and dependency structures are diagrams in free\nrigid categories. We describe the data structure rigid.Diagram and its interface\nwith SpaCy [Hon+20] for dependency parsing.\n6. In 1.6, we introduce a notion of pregroup grammar with coreference using\nhypergraphs to represent the syntax of text and discourse. We describe the\nhypergraph.Diagram data structure and show how to interface it with SpaCy’s\npackage for neural coreference.\nThe models studied in Chapter 2 can all be implemented using one of the four\nsemantic modules of DisCoPy which we detail.\n1. The tensor module of DisCoPy implements the category of matrices, as de-\nscribed in 2.1 where we give its implementation in NumPy [Har+20]. We use\nit in conjunction with Jax [Bra+18] in 2.8 to implement the models introduced\nin 2.4 and 2.5.\n2. The quantum module of DisCoPy implements quantum circuits 2.7 and sup-\nports interfaces with PyZX [Kv19] and TKET [Siv+20] for optimisation and\ncompilation on quantum hardware. These features are described in our recent\nwork [TdY22].\n3. The function module of DisCoPy implements the category of functions on\nPython types, as described in 2.1. We deﬁne currying and uncurrying of Python\nfunctions 2.2 and use it to give a proof-of-concept implementation of Montague\nsemantics.\n4. The neural module implements the category of neural networks on euclidean\nspaces. We describe it in 2.3 as an interface between DisCoPy and Tensor-\nFlow/Keras [Cho+15].\nA schematic view of the modules in DisCoPy and their interfaces is summarized\nin Figure 1.\n17\ncat\noperad\nmonoidal\nbiclosed\nrigid\nhypergraph\ncontext-free\npregroup\ncategorial\nNLTK\nLambeq\nSpaCy\ndependency\nKeras\nJax\nPyTorch\nNumPy\nTensorNetwork\nfunction\ntensor\nTKET\nquantum\ninterface\nfunctorial reduction\nneural\nfunctorial semantics\ncoreference\nPyZX\nFigure 1: DisCoPy: an interfaced compositional software for NLP\n18\nChapter 1\nDiagrams for Syntax\nThe word “grammar” comes from the Greek γρ´αµµα (gramma), itself from γρ´αϕϵιν\n(graphein) meaning both “to write” and “to draw”, and we will represent grammat-\nical structure by drawing diagrams. A formal grammar is usually deﬁned by a set\nof rewriting rules on strings.\nThe rewriting process, also called parsing, yields a\nprocedure for deciding whether a string of words is grammatical or not.\nThese structures were studied in mathematics since the 1910s by Thue and later\nby Post [Pos47] and Markov Jr. [Kus06]. Their investigation was greatly advanced\nby Chomsky [Cho57], who used them to generate grammatical sentences from some\nbasic rewrite rules interpreted as productions. He showed that natural restrictions\non the allowed production rules form a hierarchy, from unrestricted to regular gram-\nmars, which corresponds to models of computation of varying strengths, from Turing\nmachines to deterministic ﬁnite state automata [Cho56]. In parallel to Chomsky’s\nseminal work, Lambek developed his syntactic calculus [Lam58], reﬁning and unify-\ning the categorial grammars originated in the Polish school of logic [Ajd35]. These\nare diﬀerent in spirit from Chomsky’s grammars, but they also have tight links with\ncomputation as captured by the Lambda calculus [van87; Ste00].\nIn this chapter, we lay out the correspondence between free categorical structures\nand linguistic models of grammar. Every level in this hierarchy is implemented with\na corresponding class in DisCoPy. In 1.1, we show that regular grammars can be\nseen as graphs with a labelling homomorphism and their derivations as arrows in the\nfree category, implemented via the core DisCoPy class cat.Arrow. In 1.2, we show\nthat context-free grammars are operadic signatures and their derivations trees in the\nfree operad.\nWe give an implementation of free operads as a class operad.Tree,\ninterfaced with NLTK [LB02] for context-free parsing. In 1.3, we arrive at Chomsky’s\nunrestricted grammars, captured by monoidal signatures and string diagrams. We\ndiscuss varying notions of reduction and normal form for these grammars, and show\nthe implementation of the key DisCoPy class monoidal.Diagram. In 1.4, we show\nthat categorial grammars such as the original grammars of Ajdiuciewicz and Bar-\nHillel, the Lambek calculus and Combinatory Categorial Grammars (CCGs) can be\nseen as biclosed signatures and their grammatical reductions as morphisms in free\nbiclosed categories. We give an implementation of biclosed.Diagram as a monoidal\ndiagram with curry and uncurry methods and show its interface with state-of-the-\n19\nChapter 1\nDiagrams for Syntax\nart categorial grammar parsers, as provided by Lambeq [Kar+21]. In 1.5, we show\nthat pregroups and dependency grammars are both captured by rigid signatures, and\ntheir derivations by morphisms in the free rigid category. This leads to the data\nstructure rigid.Diagram which we interface with SpaCy [Hon+20] for state-of-the-\nart dependency parsing. Finally, in 1.6, we introduce a notion of pregroup grammar\nwith coreference using hypergraphs to represent the syntax of text and discourse, and\ngive a proof-of-concept implementation in DisCoPy.\n20\nArrows\nSection 1.1\n1.1\nArrows\nIn this section, we introduce three structures: categories, regular grammars and\ncat.Arrows. These cast light on a level-zero correspondence between algebra, lin-\nguistics and Python programming. We start by deﬁning categories and their free\nconstruction from graphs. Following Walters [Wal89a], regular grammars are deﬁned\nas graphs together with a labelling homomorphism and their grammatical sentences\nas labelled paths, i.e. arrows of the corresponding free category. This deﬁnition is\nvery similar to the deﬁnition of a ﬁnite state automaton, and we discuss the equiv-\nalence between Walters’ notion, Chomsky’s original deﬁnition and ﬁnite automata.\nWe end by introducing the cat module, an implementation of free categories and\nfunctors which forms the core of DisCoPy.\n1.1.1\nCategories\nDeﬁnition 1.1.1 (Simple signature / Directed graph). A simple signature, or directed\ngraph, G is a collection of vertices G0 and edges G1 such that each edge has a domain\nand a codomain vertex\nG0\ndom\n←−−G1\ncod\n−−→G0\n. A graph homomorphism ϕ : G →Γ is a pair of functions ϕ0 : G0 →Γ0 and\nϕ1 : G1 →Γ1 such that the following diagram commutes:\nG0\nG1\nG0\nΓ0\nΓ1\nΓ0\nϕ0\ndom\ncod\nϕ1\nϕ0\ndom\ncod\nWe denote by G(a, b) the edges f ∈G1 such that dom(f) = a and cod(f) = b. We\nalso write f : a →b to denote an edge f ∈G(a, b).\nA category is a directed graph with a composition operation, in this context\nvertices are called objects and edges are called arrows or morphisms.\nDeﬁnition 1.1.2 (Category). A category C is a graph C1 ⇒C0, where C0 is a\nset of objects, and C1 a set of morphisms, equipped with a composition operation\n· : C(a, b) × C(b, c) →C(a, c) deﬁned for any a, b, c ∈C0 such that:\n1. for any a ∈C0 there is an identity morphism ida ∈C(a, a) (identities).\n2. for any f : a →b, f · ida = f = idb · f (unit law).\n3. whenever a\nf−→b\ng−→c\nh−→d, we have f · (g · h) = (f · g) · h (associativity).\nA functor F : C →D is a graph homomorphism which respects composition and\nidentities, i.e. for any a ∈C0 F(ida) = idF(a) and whenever a\ng−→b\nf−→c in C we\nhave F(f · g) = F(f) · F(g).\n21\nChapter 1\nDiagrams for Syntax\nGiven a pair of functors F, G : C →D, a natural transformation α : F →G is a\nfamily of maps αa : F(a) →G(a) such that the following diagram commutes:\nF(a)\nG(a)\nF(b)\nG(b)\nF(f)\nαa\nG(f)\nαb\n(1.1)\nfor any f : a →b in C.\nRemark 1.1.3. The symbol →appeared remarkably late in the history of symbols\nwith the earliest use registered in Bernard Forest de Belidor’s 1737 L’architecture\nhydraulique, where it is used to denote the direction of a ﬂow of water. Arguably,\nit conveys more structured information then its predecessor: the Medieval manicule\nsymbol. Its current mathematical use as the type of a morphism f : x →y appeared\nonly at the beginning of the 20th century, the ﬁrst extensive use being registered in\nHausdorﬀ[Hau35] to denote group homomorphisms.\nExample 1.1.4 (Basic). Sets and functions form a category Set.\nMonoids and\nmonoid homomorphisms for a category Mon.\nGraphs and graph homomorphisms\nform a category Graph. Categories and functors form a category Cat.\nAn arrow f in a graph G is a sequence of edges f ∈G∗\n1 such that cod(fi) =\ndom(fi+1), it can be represented graphically as a sequence of arrows:\na0\nf−→an = a0\nf1\n−→a1 . . .\nfn\n−→an\nOr as a sequence of vertices and edges:\nf1\na0\na1\nf2\nfn\nan\n. . .\na2\nOr as a sequence of boxes and wires:\nf1\na0\na1 f2\nfn\nan\n. . .\na2\nTwo arrows with matching endpoints can be composed by concatenation.\na\nf−→b\ng−→c = a\nf·g\n−→c\nPaths on G in fact form a category, denoted C(G). C(G) has the property of being\nthe free category generated by G [Mac71].\nThe free category construction is the object part of functor C : Graph →Cat,\nwhich associates to any graph homomorphism ϕ : G →V a functor C(ϕ) : C(G) →\nC(V ) which relabels the vertices and edges in an arrow. This free construction C\nis the left adjoint of the functor U : Cat →Graph which forgets the composition\noperation on arrows. C is a left adjoint of U in the sense that there is a natural\nisomorphism:\nGraph(G, U(S)) ≃Cat(C(G), S)\n22\nArrows\nSection 1.1\nwhich says that specifying a functor F : C(G) →S is the same as specifying an arrow\nin S for every generator in G. This will have important consequences in the context\nof semantics.\nA preorder P is a category with at most one morphism between any two objects.\nGiven a, b ∈P0, the hom-set P(a, b) is either the singleton or empty, we write aPb\nfor the corresponding boolean value.\nIdentities and composition of the category,\ncorrespond to reﬂexivity and transitivity of the preorder. Following Lambek [Lam68]\nand [Str07], we can interpret a preorder as a logic, by considering the underlying set as\na set of formulae and the relation ≤as a consequence relation, which is usually denoted\n⊢(entails). Reﬂexivity and transitivity of the consequence relation correspond to the\nfollowing familiar rules of inference.\nA ⊢A\nA ⊢B\nB ⊢C\nA ⊢C\n(1.2)\nGiven a graph G we can build a preorder by taking the reﬂexive transitive closure\nof the relation G ⊆G0 × G0 induced by the graph, ≤= RTC(G) ⊆G0 × G0. We can\nthink of the edges of the graph G as a set of axioms, then the free preorder RTC(G)\ncaptures the logical consequences of these axioms, and, in this simple setting, we have\nthat a =⇒b if and only if there is an arrow from a to b in G.\nThis construction is analogous to the free category construction on a graph, and\nis in fact part of a commuting triangle of adjunctions relating topology, algebra and\nlogic.\nCat\nGraph\nPreord\nU\n∃\nRTC\nC\nI\nU\nWhere ∃C is the preorder collapse of C, i.e. a∃Cb if and only if ∃f ∈C(a, b). The\nfunctor RTC allows to deﬁne the following decision problem.\nDeﬁnition 1.1.5. ∃Path\nInput:\nG ⇒B, a, b ∈N\nOutput:\na RTC(G) b\nThe problem ∃Path is also known as the graph accessibility problem, which is\ncomplete for NL the complexity class of problems solvable by a non-deterministic\nTuring machine in logarithmic space [Imm87]. In particular, it is equivalent to 2SAT,\nthe problem of satisfying a formula in conjunctive normal form where each clause has\ntwo literals. The reduction from 2SAT to ∃Path is obtained by turning the formula\ninto its graph of implications.\nThe free construction C allows to deﬁne a proof-relevant form of the path problem,\nwhere what matters is not only whether a entails b but the way in which a entails b.\nDeﬁnition 1.1.6. Path\nInput:\nG, a, b ∈G0\nOutput:\nf ∈C(G)(a, b)\n23\nChapter 1\nDiagrams for Syntax\nFrom these considerations we deduce that Path ∈FNL, since it is the function\nproblem corresponding to ∃Path. These problems correspond to parsing problems for\nregular grammars.\n1.1.2\nRegular grammars\nWe now show how graphs and categories formalise the notion of regular grammar.\nFix a ﬁnite set of words V , called the vocabulary, and let us use V to label the edges\nin a graph G. The data for such a labelling is a function L : G1 →V from edges to\nwords, or equivalently a graph homomorphism L : G →V where V is seen as a graph\nwith one vertex and words as edges. Fix a starting vertex s0 and a terminal vertex\ns1. Given any arrow f : s0 →s1 ∈C(G), we can concatenate the labels for each\ngenerator to obtain a string L∗(f) ∈C(V ) = V ∗where L∗= C(L) is the function\nL applied point-wise to arrows. We say that a string u ∈V ∗is grammatical in G\nwhenever there is an arrow f : s0 →s1 in G such that L(f) = u. We can think of\nthe arrow f as a witness of the grammaticality of u, called a proof in logic and a\nderivation in the context of formal grammars.\nDeﬁnition 1.1.7 (Regular grammar). A regular grammar is a ﬁnite graph G equipped\nwith a homomorphism L : G →V , where V is a set of words called the vocabulary,\nand two speciﬁed symbols s0, s1 ∈G0, the bottom (starting) and top (terminating)\nsymbols. Explicitly it is given by three functions:\nG0\nG1\nG0\nV\ndom\ncod\nL\nThe language generated by G is given by the image of the labelling functor:\nL(G) = L∗(C(G)(s0, s1)) ⊆V ∗.\nA morphism of regular grammars ϕ : G →H is a graph homomorphism such that the\nfollowing triangle commutes:\nG\nH\nV\nL\nϕ\nL\nand such that ϕ(s1) = s′\n1, ϕ(s0) = s′\n0. These form a category of regular grammars\nReg which is the slice or comma category of the coslice over the points { s0, s1 } of\nthe category of signatures Reg = (2\\Graph)/V .\nDeﬁnition 1.1.8 (Regular language). A regular language is a subset of X ⊆V ∗such\nthat there is a regular grammar G with L(G) = X.\n24\nArrows\nSection 1.1\nExample 1.1.9 (SVO). Consider the regular grammar generated by the following\ngraph:\ns0\nx0\nx3\nmet\nB\nA\nwho\nB\nx1\nx2\nC\n.\nAn example of sentence in the language L(G) is “A met B who met C.”.\nExample 1.1.10 (Commuting diagrams). Commuting diagrams such as 1.1 can be\nunderstood using regular grammars. Indeed, a commuting diagram is a graph G to-\ngether with a labelling of each edge as a morphism in a category C. Given a pair of\nvertices x, y ∈G0, we get a regular language L(G) given by arrows from x to y in G.\nSaying that the diagram G commutes corresponds to the assertion that all strings in\nL(G) are equal as morphisms in C.\nWe now translate from the original deﬁnition by Chomsky to the one above. Recall\nthat a regular grammar is a tuple G = (N, V, P, s) where N is a set of non-terminal\nsymbols with a speciﬁed start symbol s ∈N, V is a vocabulary and P is a set of\nproduction rules of the form A →aB or A →a or A →ϵ where a ∈V , A, B ∈N and\nϵ denotes the empty string. We can think of the sentences generated by G as arrows\nin a free category as follows. Construct the graph Σ = P ⇒(N + { s1 }) where for\nany production rule in P of the form A →aB there is an edge A\nf−→B with L(f) = a\nand for any production rule A →a there is an edge A\nw−→s1 with L(w) = a. The\nlanguage generated by G is the image under L of the set of labelled paths s →s1 in\nΣ, i.e. L(Σ) = L∗(C(Σ)(s, s1)).\nThis translation is akin to the construction of a ﬁnite state automaton from a reg-\nular grammar. In fact, the above deﬁnition of regular grammar directly is equivalent\nto the deﬁnition of non-deterministic ﬁnite state automaton (NFA). Given a regular\ngrammar (G, L), we can construct the span\nV × G0\nL×dom\n←−−−G1\ncod\n−−→G0\nwhich induces a relation im(G1) ⊆V ×G0×G0, which is precisely the transition table\nof an NFA with states in G0, alphabet symbols V and transitions in im(G1). If we\nrequire that the relation im(G1) be a function — i.e. for any (v, a) ∈V × G0 there is\na unique b ∈G0 such that (v, a, b) ∈im(G1) — then this deﬁnes a deterministic ﬁnite\nstate automaton (DFA). From any NFA, one may build a DFA by blowing up the state\nspace. Indeed relations X ⊆V ×G0 ×G0 are the same as functions V ×G0 →P(G0)\nwhere P denotes the powerset construction. So any NFA X ⊆V × G0 × G0 can be\nrepresented as a DFA V × P(G0) →P(G0).\nNow that we have shown how to recover the original deﬁnition of regular gram-\nmars, consider the following folklore result from formal language theory.\nProposition 1.1.11. Regular languages are closed under intersection and union.\n25\nChapter 1\nDiagrams for Syntax\nProof. Suppose G and G′ are regular grammars, with starting states q0, q′\n0 and ter-\nminating states q1, q′\n1.\nTaking the cartesian product of the underlying graphs G × G′ = G1 × G′\n1 ⇒\nG0 × G′\n0 we can deﬁne a regular grammar G ∩G′ ⊆G × G′ with starting state\n(q0, q′\n0), terminating state (q1, q′\n1) and such that there is an edge between (a, a′) and\n(b, b′) whenever there are edges a\nf−→b in G and a′\nf′\n−→b′ in G′ with the same label\nL(f) = L′(f ′). Then an arrow from (q0, q′\n0) to (q1, q′\n1) in G ∩G′ is the same as a pair\nof arrows q0 →q1 in G and q′\n0 →q′\n1 in G′. Therefore L(G ∩G′) = L(G) ∩L(G′).\nProving the ﬁrst part of the statement.\nMoreover, the disjoint union of graphs G + G′ yields a regular grammar G ∪G′ by\nidentifying q0 with q′\n0 and q1 with q′\n1. Then an arrow q0 →q1 in G ∪G′ is either an\narrow q0 →q1 in G or an arrow q′\n0 →q′\n1 in G′. Therefore L(G∪G′) = L(G)∪L(G′).\nThe constructions used in this proof are canonical constructions in the category\nof regular grammars Reg. Note that Reg = 2\\Graph/V is both a slice and a coslice\ncategory. Moreover, Graph has all limits and colimits. While coslice categories reﬂect\nlimits and slice categories reﬂect colimits, we cannot compose these two statements\nto show that Reg has all limits and colimits. However, we can prove explicitly that\nthe constructions deﬁned above give rise to the categorical product and coproduct.\nWe do not know whether Reg also has equalizers and coequalizers which would yield\nall ﬁnite limits and colimits.\nProposition 1.1.12. The intersection ∩of NFAs is the categorical product in Reg.\nThe union ∪of NFAs is the coproduct in Reg.\nProof. To show the ﬁrst part. Suppose we have two morphisms of regular grammars\nf : H →G and g : H →G′. These must respect the starting and terminating\nsymbols as well as the labelling homomorphism. We can construct a homomorphism\nof signatures < f, g >: H →G ∩G′ where G ∩G′ = G ×V G′ with starting point\n(q0, q′\n0) and endpoint (q1, q′\n1) as deﬁned above.\n< f, g > is given on vertices by\n< f, g >0 (x) = (f0(x), g0(x)) and on arrows by < f, g >1 (h) = (f0(h), g0(h)). Since\nL(f(h)) = L(h) = L(g(h)), this deﬁnes a morphism of regular grammars < f, g >:\nH →G∩G′. There are projections G∩G′ →G, G′ induced by the projections π0, π1 :\nG × G′ →G, G′, and it is easy to check that π0◦< f, g >= f and π1◦< f, g >= g\nwhere π0 and π1 are the projections. Now, suppose that there is some k : H →G∩G′\nwith π0 ◦k = f and π1 ◦k = g, k then the underlying function H →G × G′ must\nbe equal to < f, g > and thus also as morphisms of regular grammar k =< f, g >.\nTherefore G ∩G′ is the categorical product in Reg.\nSimilarly the union is the coproduct in Reg. Given any pair of morphisms f : G →\nH and g : G′ →H, we may deﬁne [f, g] : G ∪G′ →H on vertices by [f, g](x) = f(x)\nif x ∈G and [f, g](x) = g(x) if x ∈G′ and on similarly on edges. We have that\n[f, g](q0) = f(q0) = g(q′\n0) = q1 since q0 and q′\n0 are identiﬁed in G ∪G′ and L([f, g](h))\nis either of these equal expressions L(f(h)) = L(h) = L(g(h)), i.e. [f, g] is a morphism\nof regular grammars. Let i : G →G ∪G′ and i′ : G′ →G ∪G′ be the injections into\nthe union. We have [f, g]◦i = f and [f, g]◦i′ = g. Moreover, for any other morphism\nk : G∪G′ →H with k◦i = f and k◦i′ = g, we must have that k(h) = f(h) whenever\n26\nArrows\nSection 1.1\nh ∈G ⊆G ∪G′ and k(h) = g otherwise, i.e. k = [f, g]. Therefore G ∪G′ satisﬁes the\nuniversal property of the coproduct in Reg.\nWe now study the parsing problem for regular grammars. First, consider the non-\nemptiness problem, which takes as input a regular grammar G and returns “no” if\nL(G) is empty and “yes” otherwise.\nProposition 1.1.13. The non-emptiness problem is equivalent to ∃Path\nProof. L(G) is non-empty if and only if there is an arrow from s0 to s1 in G.\nNow, consider the problem of recognizing the language of a regular grammar G,\nalso known as Parsing. We deﬁne the proof-relevant version which, of course, has a\ncorresponding decision problem ∃Parsing.\nDeﬁnition 1.1.14. Parsing\nInput:\nG, u ∈V ∗\nOutput:\nf ∈C(G)(s0, s1) such that L∗(f) = u.\nGiven a string u ∈V ∗, we may build the regular grammar Gu given by the path-\ngraph with edges labelled according to u, so that L(Gu) = { u }. Then the problem\nof deciding whether there is a parse for u in G reduces to the non-emptiness problem\nfor the intersection G ∩Gu. This has the following consequence.\nProposition 1.1.15. ∃Parsing is equivalent to ∃Path and is thus NL-complete. Sim-\nilarly, Parsing is FNL-complete.\nAt the end of the section, we give a simple algorithm for parsing regular grammars\nbased on the composition of arrows in a free category. In order to model the higher\nlevels of Chomsky’s hierarchy, we need to equip our categories with more structure.\n1.1.3\ncat.Arrow\nWe have introduced free categories and shown how they appear in formal language\ntheory. These structures have moreover a natural implementation in object-oriented\nPython, which we now describe. In order to implement a free category in Python\nwe need to deﬁne three classes: cat.Ob for objects, cat.Arrow for morphisms and\ncat.Box for generators. Objects are initialised by providing a name.\nListing 1.1.16. Objects in a free category.\nclass Ob:\ndef __init__(self, name):\nself.name = name\nArrows, i.e. morphisms of free categories, are given by lists of boxes with matching\ndomains and codomains. In order to initialise a cat.Arrow, we provide a domain,\na codomain and a list of boxes. The class comes with a method Arrow.then for\ncomposition and a static method Arrow.id for generating identity arrows.\n27\nChapter 1\nDiagrams for Syntax\nListing 1.1.17. Arrows in a free category.\nclass Arrow:\ndef __init__(self, dom, cod, boxes, _scan=True):\nif not isinstance(dom, Ob) or not isinstance(cod, Ob):\nraise TypeError()\nif _scan:\nscan = dom\nfor depth, box in enumerate(boxes):\nif box.dom != scan:\nraise AxiomError()\nscan = box.cod\nif scan != cod:\nraise AxiomError()\nself.dom, self.cod, self.boxes = dom, cod, boxes\ndef then(self, *others):\nif len(others) > 1:\nreturn self.then(others[0]).then(*others[1:])\nother, = others\nif self.cod != other.dom:\nraise AxiomError()\nreturn Arrow(self.dom, other.cod, self.boxes + other.boxes, _scan=False))\n@staticmethod\ndef id(dom):\nreturn Arrow(self, dom, dom, [], _scan=False)\ndef __rshift__(self, other):\nreturn self.then(other)\ndef __lshift__(self, other):\nreturn other.then(self)\nWhen _scan == False we do not check that the boxes in the arrow compose.\nThis allows us to avoid checking composition multiple times for the same Arrow. The\nmethods __rshift__ and __lshift__ allow to use the syntax f >> g and g << f for\nthe composition of instances of the Arrow class.\nFinally, generators of the free category are special instances of Arrow, initialised\nby a name, a domain and a codomain.\nListing 1.1.18. Generators of a free category.\nclass Box(Arrow):\ndef __init__(self, name, dom, cod):\nself.name, self.dom, self.cod = name, dom, cod\nArrow.__init__(self, dom, cod, [self], _scan=False)\nThe subclassing mechanism in Python allows for Box to inherit all the Arrow\nmethods, so that there is essentially no diﬀerence between a box and an arrow with\none box.\n28\nArrows\nSection 1.1\nRemark 1.1.19. It is often useful to deﬁne dataclass methods such as __repr__,\n__str__ and __eq__ to represent, print and check equality of objects in a class.\nSimilarly, other standard methods such as __hash__ may be overwritten and used as\nsyntactic gadgets. We set self.name = name although, in DisCoPy, this parameter\nis immutable and e.g. Ob.name is implemented as a @property method. In order to\nremain concise, we will omit these methods when deﬁning further classes.\nWe now have all the ingredients to compose arrows in free categories. We check\nthat the axioms of categories hold for cat.Arrows on the nose.\nListing 1.1.20. Axioms of free categories.\nx, y, z = Ob(’x’), Ob(’y’), Ob(’z’)\nf, g, h = Box(’f’, x, y), Box(’g’, y, z), Box(’h’, z, x)\nassert f >> Arrow.id(y) == f == Arrow.id(x) >> f\nassert (f >> g) >> h == f >> g >> h == (f >> g) >> h\nA signature is a pair of lists, for objects and generators. A homomorphism between\nsignatures is a pair of Python dictionnaries. Functors between the corresponding free\ncategories are initialised by a pair of mappings ob, from objects to objects, and ar\nfrom boxes to arrows. The call method of Functor allows to evaluate the image of\ncomposite arrows.\nListing 1.1.21. Functors from a free category.\nclass Functor:\ndef __init__(self, ob, ar):\nself.ob, self.ar = ob, ar\ndef __call__(self, arrow):\nif isinstance(arrow, Ob):\nreturn self.ob[arrow]\nif isinstance(arrow, Box):\nreturn self.ar[arrow]\nif isinstance(arrow, Arrow):\nreturn Arrow.id(self(arrow.dom)).then(*map(self, arrow))\nraise TypeError()\nWe check that the axioms hold.\nx, y = Ob(’x’), Ob(’y’)\nf, g = Box(’f’, x, y), Box(’g’, y, x)\nF = Functor(ob={x : y, y: x}, ar={f : g, g: f})\nassert F(f >> g) == F(f) >> F(g)\nassert F(Arrow.id(x)) == Arrow.id(F(x))\nAs a linguistic example, we use the composition method of Arrow to write a simple\nparser for regular grammars.\nListing 1.1.22. Regular grammar parsing.\n29\nChapter 1\nDiagrams for Syntax\nfrom discopy.cat import Ob, Box, Arrow, AxiomError\ns0, x, s1 = Ob(’s0’), Ob(’x’), Ob(’s1’)\nA, B, C = Box(’A’, s0, x), Box(’B’, x, x), Box(’A’, x, s1)\ngrammar = [A, B, C]\ndef is_grammatical(string, grammar):\narrow = Arrow.id(s0)\nbool = False\nfor x in string:\nfor box in grammar:\nif box.name == x:\ntry:\narrow = arrow >> box\nbool = True\nbreak\nexcept AxiomError:\nbool = False\nif not bool:\nreturn False\nreturn bool\nassert is_grammatical(\"ABBA\", grammar)\nassert not is_grammatical(\"ABAB\", grammar)\nSo far, we have only showed how to implement free categories and functors in\nPython. However, the same procedure can be repeated. Implementing a category\nin Python amounts to deﬁning a pair of classes for objects and arrows and a pair\nof methods for identity and composition. In the case of Arrow, and the syntactic\nstructures of this chapter, we are able to check that the axioms hold in Python. In\nthe next chapter, these arrows will be mapped to concrete Python functions, for which\nequality cannot be checked.\n30\nTrees\nSection 1.2\n1.2\nTrees\nContext-free grammars (CFGs) emerged from the linguistic work of Chomsky [Cho56]\nand are used in many areas of computer science. They are obtained from regular\ngrammars by allowing production rules to have more than one output symbol, result-\ning in tree-shaped derivations. Following Walters [Wal89a] and Lambek [Lam99b],\nwe formalise CFGs as operadic signatures and their derivations as trees in the corre-\nsponding free operad. Morphisms of free operads are trees with labelled nodes and\nedges. We give an implementation – the operad module of DisCoPy – which satisﬁes\nthe axioms of operads on the nose and interfaces with the library NLTK [LB02].\n1.2.1\nOperads\nDeﬁnition 1.2.1 (Operadic signature). An operadic signature is a pair of functions:\nG∗\n0\ndom\n←−−G1\ncod\n−−→G0\nwhere G1 is the set of nodes and G0 a set of objects.\nA morphism of operadic\nsignatures ϕ : G →Γ is a pair of functions ϕ0 : G0 →Γ0, ϕ1 : G1 →Γ1 such that the\nfollowing diagram commutes:\nG∗\n0\nG1\nG0\nΓ∗\n0\nΓ1\nΓ0\nϕ∗\n0\ndom\ncod\nϕ1\nϕ0\ndom\ncod\nWith these morphisms, operadic signatures form a category denoted OpSig.\nA node or box in an operadic signature is denoted b0 . . . bn\nf−→a. Nodes of the\nform a\nw−→ϵ for the empty string ϵ are called leaves.\nf\na\nb0\nbn\n. . .\nw\na\nNode\nLeaf\nDeﬁnition 1.2.2 (Operad). An operad O is an operadic signature equipped with\na composition operation · : Q\nbi∈⃗b Op(⃗ci, bi) × Op(⃗b, a) →Op(⃗c, a) deﬁned for any\na ∈O0, ⃗b,⃗c ∈O∗\n0. Given f : ⃗b →a and gi : ⃗ci →bi, the composition of ⃗g with f is\ndenoted graphically as follows.\nf\ng0\n. . .\n. . .\n. . .\na\nb0\nbn\nc1\ncm\ngn\n(1.3)\nWe ask that they satisfy the following axioms:\n31\nChapter 1\nDiagrams for Syntax\n1. for any a ∈O0 there is an identity morphism ida ∈Op(a, a) (identities).\n2. for any f : ⃗b →a, f · ida = f = ⃗\nidbi · f (unit law).\n3.\nf\ngn\ng0\n. . .\n. . .\na\nb0\nbn\nc0\ncm\ngi\n. . .\n=\nbi\nf\ngn\ng0\n. . .\n. . .\na\nb0\nbn\nc0\ncm\ngi\n. . .\nbi\n(1.4)\nAn algebra F : O →N is a morphism of operadic signature which respects composi-\ntion, i.e. such that whenever ⃗c\n⃗g−→⃗b\nf−→a in O we have F(⃗g · f) =\n⃗\nF(g) · F(f). With\nalgebras as morphisms, operads form a category Operad.\nRemark 1.2.3. The diagrammatic notation we are using is not formal yet, but it will\nbe made rigorous in the following section where we will see that operads are instances\nof monoidal categories and thus admit a formal graphical language of string diagrams.\nGiven an operadic signature G, we may build the free operad over G, denoted\nOp(G). Morphism of Op(G) are labelled trees with nodes from G and edges labelled\nby elements of the generating objects B. Two trees are equivalent (or congruent)\nif they can be deformed continuously into each other using the interchanger rules\n1.4 repeatedly. The free operad construction is part of the following free-forgetful\nadjunction.\nOpSig\nOp\n⇄\nU\nOperad\nThis means that for any operadic signature G and operad O, algebras Op(G) →O\nare in bijective correspondence with morphisms of operadic signatures G →U(O).\nExample 1.2.4 (Peirce’s alpha). Morphisms in an operad can be depicted as trees,\nor equivalently as a nesting of bubbles:\n⇐⇒\na\nb\nc\nd\nh\na\nb\nc\nd\ng\nf\nf\ng\nh\n(1.5)\nInterestingly, the nesting perspective was adopted by Peirce in his graphical formula-\ntion of propositional logic: the alpha graphs [Pei06]. We may present Peirce’s alpha\ngraphs as an operad α with a single object x, variables as leaves a, b, c, d : 1 →x,\n32\nTrees\nSection 1.2\na binary operation ∧: xx →x and a unary operation ¬ : x →x, together with\nequations encoding the associativity of ∧and ¬¬ = idx. In order to encode Peirce’s\nrules for “iteration” and “weakening”, we would need to work in a preordered operad,\nbut we omit these rules here, see [BT00] for a full axiomatisation. The main purpose\nhere is to note that the nesting notation is sometimes more practical than its tree\ncounterpart. Indeed, since ∧is associative and it is the only binary operation in α,\nwhen two symbols are put side by side on the page, it is unambiguous that one should\ntake the conjunction ∧. Therefore the nested notation simpliﬁes reasoning and we\nmay draw the propositional formula\n¬(a0 ∧¬(a1 ∧a2) ∧a3)\nas the following diagram:\na0\na1\na2\na3\n(1.6)\n1.2.2\nContext-free grammars\nGiven a ﬁnite operadic signature G, we can intepret the nodes in G as production\nrules, the generating objects as symbols, and morphisms in the free operad Op(G) as\nderivations, obtaining the notion of a context-free grammar.\nDeﬁnition 1.2.5 (Context-free grammar). A CFG is a ﬁnite operadic signature of\nthe following shape:\n(B + V )∗←G →B\nwhere B is a set of non-terminal symbols with a speciﬁed sentence symbol s ∈B, V\nis a vocabulary (a.k.a a set of terminal symbols) and G is a set of production rules.\nThe language generated by G is given by:\nL(G) = { u ∈V ∗| ∃g : u →s ∈Op(G) }\nwhere Op(G) is the free operad of labelled trees with nodes from G.\nRemark 1.2.6. Note that the direction of the arrows is the opposite of the usual\ndirection used for CFGs, instead of seeing a derivation as a tree from the sentence\nsymbol s to the terminal symbols u ∈V ∗, we see it as a tree from u to s. This, of\ncourse, does not change any of the theory.\nDeﬁnition 1.2.7 (Context-free language). A context-free language is a subset X ⊆\nV ∗such that X = L(G) for some context-free grammar G.\nExample 1.2.8 (Backus Naur Form). BNF is a convenient syntax for deﬁning\ncontext-free languages recursively. An example is the following expression:\ns ←s ∧s | ¬s | a\n33\nChapter 1\nDiagrams for Syntax\na ←a0 | a1 | a2 | a3\nwhich deﬁnes a CFG with seven production rules {s ←s ∧s , s ←¬s , s ←a a ←\na0 a ←a1 a ←a2 a ←a3} and such that trees with root s are well-formed propositional\nlogic formulae with variables in { a0, a1, a2, a3 }. An example of a valid propositional\nformula is\n¬(a0 ∧a1) ∧¬(a2 ∧a3)\nas witnessed by the following tree:\na0\na1\na2\na3\n∧\n¬\n∧\n∧¬\ns\nwhere we have omitted the types of intermediate wires for readability.\nExample 1.2.9. As a linguistic example, let B = { n, d, v, vp, np, s } for nouns,\nprepositions, verbs, verb phrases and prepositional phrases, and let G be the CFG\ndeﬁned by the following lexical rules:\nCaesar →n\nthe →d\nRubicon →n\ncrossed →v\ntogether with the production rules n · v →vp, n · d →vp, vp · pp →s. Then the\nfollowing is a grammatical derivation:\nCaesar\ncrossed\nRubicon\ns\nvp\nn\nnp\nv\nthe\nd\nn\nExample 1.2.10 (Regular grammars revisited). Any regular grammar yields a CFG.\n34\nTrees\nSection 1.2\nThe translation is given by turning paths into left-handed trees as follows:\nShakespeare\nis\nMessina\ns1\ns0\nShakespeare\na\nis\nc\nMessina\ns1\n7→\nw\nc\na\nw\nfrom\nb\nfrom\nb\nw\n(1.7)\nNot all CFGs arise in this way. For example, the language of well-bracketed expres-\nsions, deﬁned by the CFG with a single production rule G = { s ←(s) }, cannot be\ngenerated by a regular grammar. We can prove this using the pumping lemma. In-\ndeed, suppose there is a regular grammar G′ such that well-bracketed expressions are\npaths in G′. Let n be the number of vertices in G′ and consider the grammatical ex-\npression x = (. . . () . . . ) with n + 1 open and n + 1 closed brackets. If G′ parses x,\nthen there must be a path p0 in G′ with labelling (. . . ( and a path p1 with labelling\n) . . . ) such that p0 · p1 : s0 →s1 in G′. By the pigeon hole principle, the path p0 must\nhave a cycle of length k ≥1. Remove this cycle from p0 to get a new path p′\n0. Then\np′\n0 · p1 yields a grammatical expression x′ = (. . . () . . . ) with n + 1 −k open brackets\nand n + 1 closed brackets. But then x′ is not a well-bracketed expression. Therefore\nregular grammars cannot generate the language of well-bracketed expressions and we\ndeduce that regular languages are strictly contained in context-free languages.\nWe brieﬂy consider the problem of parsing context-free grammars.\nDeﬁnition 1.2.11. CfgParsing\nInput:\nG, u ∈V ∗\nOutput:\nf ∈Op(G)(u, s)\nThis problem can be solved using a pushdown automaton, and in fact any language\nrecognized by a pushdown automaton is context-free [Cho56]. The following result\nwas shown independently by several researchers at the end of the 1960s.\nProposition 1.2.12. [You67; Ear70] Context-free grammars can be parsed in cubic\ntime.\n1.2.3\noperad.Tree\nTree data structures are ubiquitous in computer science. They can implemented via\nthe inductive deﬁnition: a tree is a root together with a list of trees. Implementing\n35\nChapter 1\nDiagrams for Syntax\noperads as deﬁned in this Section, presents some extra diﬃculties in handling types\n(domains and codomains) and identities. In fact, the concept of “identity tree” is\nnot frequent in the computer science literature. Our implementation of free oper-\nads consists in the deﬁnition of classes operad.Tree, operad.Box, operad.Id and\noperad.Algebra, corresponding to morphisms (trees), generators (nodes), identities\nand algebras of free operads, respectively. A Tree is initialised by a root, instance\nof Node, together with a list of Trees called branches. Alternatively, it may be built\nfrom generating Boxs using the Tree.__call__ method, this allows for an intuitive\nsyntax which we illustrate below.\nListing 1.2.13. Tree in a free operad.\nclass Tree:\ndef __init__(self, root, branches, _scan=True):\nif not isinstance(root, Box):\nraise TypeError()\nif not all([isinstance(branch, Tree) for branch in branches]):\nraise TypeError()\nif _scan and not root.cod == [branch.dom for branch in branches]:\nraise AxiomError()\nself.dom, self.root, self.branches = root.dom, root, branches\n@property\ndef cod(self):\nif isinstance(self, Box):\nreturn self._cod\nelse:\nreturn [x for x in branch.cod for branch in self.branches]\ndef __repr__(self):\nreturn \"Tree({}, {})\".format(self.root, self.branches)\ndef __str__(self):\nif isinstance(self, Box):\nreturn self.name\nreturn \"{}({})\".format(self.root.name,\n’, ’.join(map(Tree.__str__, self.branches)))\ndef __call__(self, *others):\nif not others or all([isinstance(other, Id) for other in others]):\nreturn self\nif isinstance(self, Id):\nreturn others[0]\nif isinstance(self, Box):\nreturn Tree(self, list(others))\nif isinstance(self, Tree):\nlengths = [len(branch.cod) for branch in self.branches]\nranges = [0] + [sum(lengths[:i + 1]) for i in range(len(lengths))]\nbranches = [self.branches[i](*others[ranges[i]:ranges[i + 1]])\nfor i in range(len(self.branches))]\nreturn Tree(self.root, branches, _scan=False)\nraise NotImplementedError()\n36\nTrees\nSection 1.2\n@staticmethod\ndef id(dom):\nreturn Id(dom)\ndef __eq__(self, other):\nreturn self.root == other.root and self.branches == other.branches\nA Box is initialised by label name, a domain object dom and a list of objects cod\nfor the codomain.\nListing 1.2.14. Node in a free operad.\nclass Box(Tree):\ndef __init__(self, name, dom, cod):\nif not (isinstance(dom, Ob) and isinstance(cod, list)\nand all([isinstance(x, Ob) for x in cod])):\nreturn TypeError\nself.name, self.dom, self._cod = name, dom, cod\nTree.__init__(self, self, [], _scan=False)\ndef __repr__(self):\nreturn \"Box(’{}’, {}, {})\".format(self.name, self.dom, self._cod)\ndef __hash__(self):\nreturn hash(repr(self))\ndef __eq__(self, other):\nif isinstance(other, Box):\nreturn self.dom == other.dom and self.cod == other.cod \\\nand self.name == other.name\nif isinstance(other, Tree):\nreturn other.root == self and other.branches == []\nAn Id is a special type of node, which cancels locally when composed with\nother trees.\nThe cases in which identities must be removed are handled in the\nTree.__call__ method. The Tree.__init__ method, as it stands, does not check\nthe identity axioms. We will however always use the __call__ syntax to construct\nour trees.\nListing 1.2.15. Identity in a free operad.\nclass Id(Box):\ndef __init__(self, dom):\nself.dom, self._cod = dom, [dom]\nBox.__init__(self, \"Id({})\".format(dom), dom, dom)\ndef __repr__(self):\nreturn \"Id({})\".format(self.dom)\nWe can check that the axioms of operads hold for Tree.__call__.\nListing 1.2.16. Axioms of free operads.\n37\nChapter 1\nDiagrams for Syntax\nx, y = Ob(’x’), Ob(’y’)\nf, g, h = Box(’f’, x, [x, x]), Box(’g’, x, [x, y]), Box(’h’, x, [y, x])\nassert Id(x)(f) == f == f(Id(x), Id(x))\nleft = f(Id(x), h)(g, Id(x), Id(x))\nmiddle = f(g, h)\nright = f(g, Id(x))(Id(x), Id(x), h)\nassert left == middle == right == Tree(root=f, branches=[g, h])\nListing 1.2.17. We construct the tree from Example 1.2.9.\nn, d, v, vp, np, s = Ob(’N’), Ob(’D’), Ob(’V’), Ob(’VP’), Ob(’NP’), Ob(’S’)\nCaesar, crossed = Box(’Caesar’, n, []), Box(’crossed’, v, []),\nthe, Rubicon = Box(’the’, d, []), Box(’Rubicon’, n, [])\nVP, NP, S = Box(’VP’, vp, [n, v]), Box(’NP’, np, [d, n]), Box(’S’, s, [vp, np])\nsentence = S(VP(Caesar, crossed), NP(the, Rubicon))\nWe deﬁne the Algebra class, which implements operad algebras as deﬁned in 1.2.2\nand is initialised by a pair of mappings: ob from objects to objects and ar from nodes\nto trees. These implement functorial reductions and functorial semantics of CFGs, as\ndeﬁned in the next section and chapter respectively.\nListing 1.2.18. Algebra of the free operad.\nclass Algebra:\ndef __init__(self, ob, ar, cod=Tree):\nself.cod, self.ob, self.ar = cod, ob, ar\ndef __call__(self, tree):\nif isinstance(tree, Id):\nreturn self.cod.id(self.ob[tree])\nif isinstance(tree, Box):\nreturn self.ar[tree]\nreturn self.ar[tree.root](*[self(branch) for branch in tree.branches])\nNote that we parametrised the class algebra over a codomain class, which by default\nis the free operad Tree. We may build any algebra from the free operad to an operad\nA by providing a class cod=A with A.id and A.__call__ methods. We will see a ﬁrst\nexample of this when we interface Tree with Diagram in the next section. Further\nexamples will be given in Chapter 2.\nWe end by interfacing the operad module with the library NLTK [LB02].\nListing 1.2.19. Interface between nltk.Tree and operad.Tree.\ndef from_nltk(tree):\nbranches, cod = [], []\nfor branch in tree:\nif isinstance(branch, str):\nreturn Box(branch, Ob(tree.label()), [])\nelse:\nbranches += [from_nltk(branch)]\ncod += [Ob(branch.label())]\nroot = Box(tree.label(), Ob(tree.label()), cod)\nreturn root(*branches)\n38\nTrees\nSection 1.2\nThis code assumes that the tree is generated from a lexicalised CFG. The operad\nmodule of DisCoPy contains the more general version. We can now deﬁne a grammar\nin NLTK, parse it, and extract an operad.Tree. We check that we recover the correct\ntree for “Caesar crossed the Rubicon”.\nfrom nltk import CFG\nfrom nltk.parse import RecursiveDescentParser\ngrammar = CFG.fromstring(\"\"\"\nS -> VP NP\nNP -> D N\nVP -> N V\nN -> ’Caesar’\nV -> ’crossed’\nD -> ’the’\nN -> ’Rubicon’\"\"\")\nrd = RecursiveDescentParser(grammar)\nfor x in rd.parse(’Caesar crossed the Rubicon’.split()):\ntree = from_nltk(x)\nassert tree == sentence\n39\nChapter 1\nDiagrams for Syntax\n1.3\nDiagrams\nString diagrams in monoidal categories are the key tool that we use to represent\nsyntactic structures. In this section we introduce monoidal grammars, the equivalent\nof Chomsky’s unrestricted type-0 grammars. Their derivations are string diagrams\nin a free monoidal category. We introduce functorial reductions as a structured way\nof comparing monoidal grammars, and motivate them as a tool to reason about\nequivalence and normal forms for context-free grammar.\nString diagrams have a\nconvenient premonoidal encoding as lists of layers, which allows to implement the\nclass monoidal.Diagram as a subclass of cat.Arrow. We give an overview of the\nmonoidal module of DisCoPy and its interface with operad.\nMonoidal category\nType-0 grammar\nPython\nobjects\nstrings\nTy\ngenerators\nproduction rules\nBox\nmorphisms\nderivations\nDiagram\nfunctors\nreductions\nFunctor\n1.3.1\nMonoidal categories\nDeﬁnition 1.3.1 (Monoidal signature). A monoidal signature G is a signature of\nthe following form:\nG∗\n0\ndom\n←−−G1\ncod\n−−→G∗\n0\n. G is a ﬁnite monoidal signature if G1 is ﬁnite. A morphism of monoidal signatures\nϕ : G →Γ is a pair of maps ϕ0 : G0 →Γ0 and ϕ1 : G1 →Γ1 such that the following\ndiagram commutes:\nG∗\n0\nG1\nG∗\n0\nΓ∗\n0\nΓ1\nΓ∗\n0\nϕ∗\n0\ndom\ncod\nϕ1\nϕ∗\n0\ndom\ncod\nWith these morphisms, monoidal signatures form a category MonSig.\nElements f : ⃗a →⃗b of G1 are called boxes and are denoted by the following\ndiagram, read from top to bottom, special cases are states and eﬀects with no inputs\nand outputs respectively.\nf\n. . .\na0\nan\nb0\nbn\n. . .\nBox\nw\na\nState\nm\nb\nEﬀect\nDeﬁnition 1.3.2 (Monoidal category). A (strict) monoidal category is a category C\nequipped with a functor ⊗: C×C →C called the tensor and a speciﬁed object 1 ∈C0\ncalled the unit, satisfying the following axioms:\n40\nDiagrams\nSection 1.3\n1. 1 ⊗f = f = f ⊗1 (unit law)\n2. (f ⊗g) ⊗h = f ⊗(g ⊗h) (associativity)\nfor any f, g, h ∈C1. A (strict) monoidal functor is a functor that preserves the tensor\nproduct on the nose, i.e. such that F(f ⊗g) = F(f)⊗F(g). The category of monoidal\ncategories and monoidal functors is denoted MonCat.\nRemark 1.3.3. The general (non-strict) deﬁnition of a monoidal category relaxes the\nequalities in the unit and associativity laws to the existence of natural isomorphisms,\ncalled unitors and associators. These are then required to satisfy some coherence con-\nditions in the form of commuting diagrams, see MacLane [Mac71]. In practice, these\nnatural transformations are not used in calculations. MacLane’s coherence theorem\nensures that any monoidal category is equivalent to a strict one.\nGiven a monoidal signature G we can generate the free monoidal category MC(G),\ni.e. there is a free-forgetful adjunction:\nMonSig\nMC\n⇄\nU\nMonCat\nThis means that for any monoidal signature G and monoidal category S, functors\nMC(G) →S are in bijective correspondence with morphisms of signatures G →U(S).\nThe free monoidal category was ﬁrst characterized by Joyal and Street [JS91] who\nshowed that morphisms in MC(G) are topological objects called string diagrams. We\nfollow the formalisation of Delpeuch and Vicary [DV19a] who provided an equivalent\ncombinatorial deﬁnition of string diagrams.\nGiven a monoidal signature G, we can construct the signature of layers:\nG∗\n0\ndom\n←−−L(G) = G∗\n0 × G1 × G∗\n0\ncod\n−−→G∗\n0\nwhere for every layer l = (u, f : x →y, v) ∈L(G) we deﬁne dom(l) = uxv and\ncod(l) = uyv. A layer l ∈L(G) is denoted as follows:\nf\nx\ny\nu\nu\nv\nv\nThe set of premonoidal diagrams PMC(G) is the set of morphisms of the free category\ngenerated by the layers:\nPMC(G) = C(L(G))\nThey are precisely morphisms of free premonoidal categories in the sense of [PR97].\nMorphisms d ∈PMC(G) are lists of layers d = (d1, . . . , dn) such that cod(di) =\ndom(di+1). The data for such a diagram may be presented in a more succint way as\nfollows.\nProposition 1.3.4 (Premonoidal encoding). [DV19a] A premonoidal diagram d ∈\nPMC(G) is uniquely deﬁned by the following data:\n41\nChapter 1\nDiagrams for Syntax\n1. a domain dom(d) ∈Σ∗\n0,\n2. a codomain cod(d) ∈Σ∗\n0,\n3. a list of boxes boxes(d) ∈Gn\n1,\n4. a list of oﬀsets offsets(d) ∈Nn.\nWhere n ∈N is the length of the diagram and the oﬀsets indicate the number of\nboxes to the left of each wire. This data deﬁnes a valid premonoidal diagram if for\n0 < i ≤n we have:\nwidth(d)i ≥offsets(d)i + |dom(bi)|\nwhere the widths are deﬁned inductively by:\nwidth(d)1 = size(dom(d))\nwidth(d)i+1 = width(d)i + |cod(bi)| −|dom(bi)|\nand bi = boxes(d)i.\nAs an example consider the following diagram:\nh\ny\nu\nv\ng\nf\na\na\nb\nc\nx\nIt has the following combinatorial encoding:\n(dom = abc, cod = auyv, boxes = [f, g, h], offsets = [2, 1, 2])\nwhere f : c →xv, g : b →u and h : x →y and a, b, c, x, u, y, v ∈G0. This combi-\nnatorial encoding is the underlying data-structure of both the online proof assistant\nGlobular [BKV18] and the Python implementation of monoidal categories DisCoPy\n[dTC21].\nPremonoidal diagrams are a useful intermediate step to deﬁne the free monoidal\ncategory MC(G) over a monoidal signature G. Indeed, morphisms in MC(G) are\nequivalence classes of the quotient of PMC(G) by the interchanger rules, given by\nthe following relation:\ng\ny\nb\nf\nx\na\nu\nv\nv\nu\ng\ny\nb\nf\nx\na\nu\nv\nv\nu\n∼\n(1.8)\nThe following result was proved by Delpeuch and Vicary [DV19a], who showed how\nto transalte between the combinatorial deﬁnition of diagrams and the deﬁnition of\nJoyal and Street as planar progressive graphs up to planar isotopy [JS91].\n42\nDiagrams\nSection 1.3\nProposition 1.3.5. [DV19a]\nMC(G) ≃PMC(G)/ ∼\nGiven two representatives d, d′ : u →v ∈PMC(G) we may check whether\nthey are equal in MC(G) in cubic time [DV19a]. Assuming d and d′ are boundary\nconnected diagrams, this is done by turning d and d′ into their interchanger normal\nform, which is obtained by applying the interchanger rules 1.8 from left to right\nrepeatedly. For disconnected diagrams the normalization requires more machinery\nbut can still be performed eﬃciently, see [DV19a] for details.\n1.3.2\nMonoidal grammars\nMonoidal categories appear in linguistics as a result of the following change of termi-\nnology. Given a monoidal signature G, we may think of the objects in G∗\n0 as strings of\nsymbols, the generating arrows in G1 as production rules and morphisms in MC(G)\nas derivations. We directly obtain a deﬁnition of Chomsky’s generative grammars, or\nstring rewriting system, using the notion of a monoidal signature.\nDeﬁnition 1.3.6 (Monoidal Grammar). A monoidal grammar is a ﬁnite monoidal\nsignature G of the following shape:\n(V + B)∗\ndom\n←−−G\ncod\n−−→(V + B)∗\nwhere V is a set of words called the vocabulary, and B is a set of symbols with s ∈B\nthe sentence symbol. An utterance u ∈V ∗is grammatical if there is a string diagram\ng : u →s in MC(G), i.e. the language generated by G is given by:\nL(G) = { u ∈V ∗| ∃f ∈MC(G)(u, s) }\nThe free monoidal category MC(G) is called the category of derivations of G.\nRemark 1.3.7. Any context-free grammar as deﬁned in 1.2.5 yields directly a monoidal\ngrammar.\nProposition 1.3.8. The languages generated by monoidal grammars are equivalent\nto the languages generated by Chomsky’s unrestricted grammars.\nProof. Unrestricted grammars are deﬁned as ﬁnite relations P ⊆(V +B)∗×(V +B)∗\nwhere V is a set of terminal symbols, B a set of non-terminals and P is a set of\nprodution rules [Cho56]. The only diﬀerence between this deﬁnition and monoidal\ngrammars is that the latter allow more than one production rule between pairs of\nstrings. However, a string u is in the language if there exists a derivation g : u →s.\nTherefore the languages are equivalent.\nWe deﬁne recursively enumerable languages as those generated by monoidal gram-\nmars, or equivalently by Chomsky’s unrestricted grammars, or equivalently those\nrecognized by Turing machines as discussed in the next paragraph.\n43\nChapter 1\nDiagrams for Syntax\nDeﬁnition 1.3.9 (Recursively enumerable language). A recursively enumerable lan-\nguage is a subset X ⊆V ∗such that X = L(G) for some monoidal grammar G.\nExample 1.3.10 (Cooking recipes). As an example of a monoidal grammar, let V =\n{aubergine, tomato, parmigiana} be a set of cooking ingredients, B = { parmigiana }\nbe a set of dishes and let G be a set of cooking steps, e.g.\nstack : a p →p\ntake : t →t t\nspread : t p →p\neat : t →1.\nThen the derivations u →parmigiana in MC(G) with u ∈V ∗are cooking recipes to\nmake parmigiana. For instance, the following is a valid cooking recipe:\nstack\ntake\nspread\neat\naubergine\nparmigiana\ntomato\ntomato\nparmigiana\nparmigiana\ntomato\nRecall that a Turing machine is given by a tuple (Q, Γ, ♯, V, δ, q0, s) where Q is a\nﬁnite set of states, Γ is a ﬁnite set of alphabet symbols, ♯∈Γ is the blank symbol,\nV ⊆Γ\\ { ♯} is the set of input symbols, q0 ∈Q is the initial state, s ∈Q is the\naccepting state and δ ⊆((Q\\ { s }) × Γ) × (Q × Γ) × { L, R } is a transition table,\nspecifying the next state in Q from a current state, the symbol in Γ to overwrite the\ncurrent symbol pointed by the head and the next head movement (left or right). At\nthe start of the computation, the machine is in state q0 and the tape contains a string\nof initial symbols u ∈V ∗followed by the blank symbol ♯indicating the end of the\ntape. The computation is then performed by applying transitions according to δ until\nthe accepting state s is reached. We assume that the transition δ doesn’t overwrite\nthe blank symbol and that it leaves it at the end of the tape.\nA Turing machine may be encoded in a monoidal grammar as follows. The set of\nnon-terminal symbols is B = (Γ\\V ) + Q, the set of terminal symbols is V and the\nsentence type is s ∈B. The production rules in G are given by:\nR\nq\na\nb\nq′\nb\na′\nL\na\nq\nb\na\nb′\nq′\nq\n♯\na\nq′\n♯\na\n(1.9)\nfor all a, a′, b, b′ ∈Γ\\ { ♯}, q, q′ ∈Q such that δ((q, a), (q′, a′, R)) = 1 and δ((q, b), (q′, b′, L)) =\n1 and δ((q, ♯), (q′, ♯, L)) = 1. Note that the last rule ensures that the blank symbol\n♯is always left at the end of the tape and never overwritten. Then we have that\nmorphisms in MC(G)(q0 w ♯, u s v) are terminating runs of the Turing machine. In\norder to express these runs as morphisms w →s we may erase the content of the tape\nonce we reach the accepting state by adding a production rule xsy →s to G for any\nx, y ∈B. Using this encoding we can prove the following proposition.\n44\nDiagrams\nSection 1.3\nProposition 1.3.11. The parsing problem for monoidal grammars is undecidable.\nProof. The encoding of Turing machines into monoidal grammars given above re-\nduces the problem of parsing monoidal grammars to the Halting problem for Turing\nmachines. Therefore it is an undecidable problem.\n1.3.3\nFunctorial reductions\nWe now come to the question of reduction and equivalence for grammars. Several\ndeﬁnitions are available in the literature and we introduce three alternative notions\nof varying strengths. The most established notion of equivalence between CFGs —\nknown as weak equivalence — judges a grammar from the language it generates.\nDeﬁnition 1.3.12 (Weak reduction). Let G and G′ be monoidal grammars over\nthe same vocabulary V . We say that G reduces weakly to G′, denoted G ≤G′, if\nL(G) ⊆L(G′). G is weakly equivalent to G′ if L(G) = L(G′).\nEven if two grammars are weakly equivalent, they may generate their sentences\nin completely diﬀerent ways. This motivates the deﬁnition of a stronger notion of\nequivalence, which does not only ask for the generated languages to be equal, but\nalso for the corresponding derivations to be the same. This notion has been studied\nin a line of work connecting context-free grammars (CFGs) and algebraic signatures\n[HR76; Gog+77; BM20], which we discuss below.\nDeﬁnition 1.3.13 (Strong reduction). Let G and G′ be monoidal grammars over\nthe same vocabulary V . A strong reduction from G to G′ is a morphism of monoidal\nsignatures f : G →G′ such that f0(v) = v for any v ∈V and f0(s) = s′. With\nstrong reductions as morphisms, monoidal grammars over V form a subcategory of\nMonSig denoted GrammarV . We say that G and G′ are strongly equivalent if they\nare isomorphic in GrammarV .\nNote ﬁrst that strong reduction subsumes its weak counterpart. Indeed given a\nmorphism f : G →G′, we get a functor MC(f) : MC(G) →MC(G′) mapping\nsyntax trees of one grammar into syntax trees of the other.\nSince f0(v) = v for\nall v ∈V and f0(s) = s′, there is an induced function MC(f) : MC(G)(u, s) →\nMC(G′)(u, s′) for any u ∈V ∗, which implies that L(G) ⊆L(G′).\nA strong reduction f is a consistent relabelling of the nodes and types of the\nunderlying operadic signature. This often results in too strict of a notion, since it\nrelates very few grammars together. We introduce an intermediate notion of reduction\nbetween grammars, which we call functorial reduction.\nDeﬁnition 1.3.14 (Functorial reduction). Let G and G′ be monoidal grammars\nover the same vocabulary V .\nA functorial reduction from G to G′ is a functor\nF : MC(G) →MC(G′) such that F0(v) = v for all v ∈V and F0(s) = s′. A\nfunctorial equivalence between G and G′ is a pair of functors F : MC(G) →MC(G′)\nand F ′ : MC(G) →MC(G′). With functorial reductions as morphisms, monoidal\ngrammars over V form a category GrammarV .\n45\nChapter 1\nDiagrams for Syntax\nRemark 1.3.15. The passage from strong to functorial reductions can be seen as as\na Kleisli category construction. The free operad functor MC : MonSig →MonCat\ninduces a monad U ◦MC : MonSig →MonSig.\nWe can construct the Kleisli\ncategory Kl(U ◦MC) with objects given by operadic signatures and morphisms given\nby morphisms of signatures f : G →UMC(G′). Equivalently, a morphism G →G′\nin Kl(U ◦MC) is a functor MC(G) →MC(G′) since MC ⊣U. MC to CfgV we\nstill have an adjunction MC ⊣U and the following equivalence:\nGrammarV ≃Kl(U ◦MC).\nFunctorial reductions can be computed in logarithmic space. We give a proof, an\nalternative proof is given by the code for Functor.__call__.\nProposition 1.3.16. Funtorial reductions can be computed in log-space L.\nProof. Let G and G′ be monoidal grammars, a functorial reduction from G to G′ is\na functor F : MC(G) →MC(G′). By the universal property of the free monoidal\ncategory MC(G) the data for such a functor is a ﬁnite homomorphism of signatures\nG →U(MC(G′)), i.e. a collection of morphisms { F(f) }f∈G. Consider the problem\nwhich takes as input a diagram g : x →y ∈MC(G) and a functorial reduction\nF : G →U(MC(G′)) and outputs F(g) ∈MC(G′).\nWe assume that we have\npremonoidal representations of g and F(f) for every production rule f ∈G, i.e. they\nall come as a pair of lists for boxes and oﬀsets. In order to compute F(g) we run\nthrough the list of boxes and replace each box f of g by F(f) adding the oﬀset of f\nto every oﬀset in F(f). This can be computed using a constant number of counters\n(one for the index of the box in the list, one for the oﬀset and one for the pointer to\nF(f)) thus functorial reductions are in logspace.\nFrom the monoidal deﬁnition of weak, strong and functorial reduction we derive\nthe corresponding notions for regular and CFGs using the following diagram.\nCat\nOperad\nMonCat\nGraph\nOpSig\nMonSig\nU\nU\nU\nC\nOp\nMC\nWe can now compare regular, context-free and unrestricted grammars via the\nnotion of reduction.\nProposition 1.3.17. Any regular grammar strongly reduces to a CFG and any CFG\nto a monoidal grammar.\nProof. This is done by proving that the injections in the diagram above exist and\nmake the diagram commute.\nThe functorial notion of reduction sits in-between the weak and strong notions. As\nshown above, any strong reduction f : G →G′ induces a functorial reduction MC(f) :\nMC(G) →MC(G′) via the free construction MC : MonSig →MonCat and\n46\nDiagrams\nSection 1.3\nthe existence of such a functor induces an inclusion of the corresponding languages.\nHowever, not all functorial reductions are strong. It is well-known that any CFG can\nbe lexicalised without losing expressivity, the resulting grammar is functorially and\nnot strongly equivalent to the original CFG.\nDeﬁnition 1.3.18 (Lexicalised CFG). A lexicalised CFG is an operadic signature of\nthe following shape:\nB∗+ V ←G →B\nIn other words, all production rules involving terminal symbols in V are of the form\nw →b for w ∈V and b ∈B. These are called lexical rules.\nProposition 1.3.19. Any CFG is functorially (and not strongly) equivalent to a\nlexicalised CFG.\nProof. For any CFG (B + V )∗←G →B we can build a lexicalised CFG B′∗+ V ←\nG′ →B′ where B′ = B +V and G′ = G+{ v →v }v∈V , where we distinguish between\nthe two copies of V . There is a functor F : MC(G) →MC(G′) given on objects by\nF0(x) = x for x ∈B + V and on arrows f : ⃗y →x by F1(f) = ⃗g · f where gi : yi →yi\nis the identity if yi ∈B and is a lexical rule yi →yi if yi ∈V . Similarly for the other\ndirection, there is a functor F ′ : MC(G′) →MC(G) given on objects by F ′\n0(x) = x\nfor all x ∈B + V and on arrows by F ′\n1(f) = f for f ∈G and F1(v →v) = idv.\nTherefore G and G′ are functorially equivalent.\nNote that even the G′ is not strongly equivalent to G. Indeed strong equivalence\nwould imply that there is a bijection between the underlying sets of symbols, i.e.\n|B + V | = |B + 2V | which is only true for grammars over an empty vocabulary.\nWe now study two useful normal forms for CFGs.\nDeﬁnition 1.3.20 (Chomsky normal form). A CFG G is in Chomsky normal form\nif it has the following shape:\nB2 + V ←G →B\ni.e. all production rules are of the form w →a or bc →a for w ∈V and a, b, c ∈B.\nProposition 1.3.21. Any CFG G is weakly equivalent to a CFG G′ in Chomsky\nnormal form, such that the reduction from G to G′ is functorial.\nProof. Fix any CFG G.\nWithout loss of generality, we may assume that G is\nlexicalised B∗+ V\n←G →B.\nIn order to construct G′, we start by setting\nG′ = { f : ⃗a →b ∈G | |⃗a| ≤2 }.\nThen, for any production rule f : ⃗a →b in G\nsuch that k = |⃗a| > 2 we add k −1 new rules fi and k −2 new symbols ci to G′\ngiven by { fi : ciai+1 →ci+1 }k−2\ni=0 where c0 = a0 and ck−1 = b. This yields a CFG\nG′ in Chomsky normal form. There is a functorial reduction from G to G′ given by\nmapping production rules f in G to left-handed trees with fis as nodes, as in the\n47\nChapter 1\nDiagrams for Syntax\nfollowing example:\na0\na1\na2\nb\n7→\na0\na1\na2\nb\na3\na3\nf\nf0\nf1\nf2\nc0\nc1\n(1.10)\nThis implies that L(G) ⊆L(G′). Now suppose ⃗u ∈L(G′), i.e. there is a tree g : ⃗u →s\nin Op(G′). By construction, if some fi : ciai+1 →ci+1 appears as an node in g then\nall the fis must appear as a sequence of nodes in g, therefore g is in the image of a\ntree in Op(G)(⃗u, s) and ⃗u ∈L(G). Therefore L(G) = L(G′).\nSince the reduction from a CFG to its Chomsky normal form is functorial, the\ntranslation can be performed in logspace. Indeed, we will show in the next section\nthat the problem of applying a functor between free monoidal categories (of which\noperad algebras are an example) is in NL. We end with an even weaker example of\nequivalence between grammars.\nDeﬁnition 1.3.22 (Greibach normal form). A CFG G is in Greibach normal form\nif it has the following shape:\nV × B∗←G →B\ni.e. every production rule is of the form wb →a for a ∈B, b ∈B∗and w ∈V .\nProposition 1.3.23. [Gre65] Any CFG is weakly equivalent to one in Greibach nor-\nmal form and the conversion can be performed in poly-time.\nWe will use these normal forms in the next section, when we discuss functo-\nrial reductions between categorial grammars and their relationship with context-free\ngrammars.\n1.3.4\nmonoidal.Diagram\nWe now present monoidal, the key module of DisCoPy which allows to compute with\ndiagrams in free monoidal categories. We have deﬁned free monoidal categories via\nthe concepts of layers and premonoidal diagrams. These have a natural implemen-\ntation in object-oriented Python, consisting in the deﬁnition of classes monoidal.Ty,\nmonoidal.Layer and monoidal.Diagram, for types, layers and diagrams in free pre-\nmonoidal categories, respectively.\nTypes are tuples of objects, equipped with a\n.tensor method for the monoidal product.\nListing 1.3.24. Types of free monoidal categories.\n48\nDiagrams\nSection 1.3\nfrom discopy import cat\nclass Ty(cat.Ob):\ndef __init__(self, *objects):\nself.objects = tuple(\nx if isinstance(x, cat.Ob) else cat.Ob(x) for x in objects)\nsuper().__init__(self)\ndef tensor(self, *others):\nfor other in others:\nif not isinstance(other, Ty):\nraise TypeError()\nobjects = self.objects + [x for t in others for x in t.objects]\nreturn Ty(*objects)\ndef __matmul__(self, other):\nreturn self.tensor(other)\nLayers are instances of cat.Box, initialised by triples (u, f, v) for a pair of types\nu, v and a box f.\nListing 1.3.25. Layer in a free premonoidal category.\nclass Layer(cat.Box):\ndef __init__(self, left, box, right):\nself.left, self.box, self.right = left, box, right\ndom, cod = left @ box.dom @ right, left @ box.cod @ right\nsuper().__init__(\"Layer\", dom, cod)\nDisCoPy diagrams are initialised by a domain, a codomain, a list of boxes and a list\nof oﬀsets. It comes with methods Diagram.then, Diagram.tensor and Diagram.id\nfor composing, tensoring and generating identity diagrams. As well as a method for\nnormal_form which allows to check monoidal equality of two premonoidal diagrams.\nListing 1.3.26. Diagram in a free premonoidal category.\nclass Diagram(cat.Arrow):\ndef __init__(self, dom, cod, boxes, offsets, layers=None):\nif layers is None:\nlayers = cat.Id(dom)\nfor box, off in zip(boxes, offsets):\nleft = layers.cod[:off] if layers else dom[:off]\nright = layers.cod[off + len(box.dom):]\\\nif layers else dom[off + len(box.dom):]\nlayers = layers >> Layer(left, box, right)\nlayers = layers >> cat.Id(cod)\nself.boxes, self.layers, self.offsets = boxes, layers, tuple(offsets)\nsuper().__init__(dom, cod, layers, _scan=False)\ndef then(self, *others):\nif len(others) > 1:\nreturn self.then(others[0]).then(*others[1:])\n49\nChapter 1\nDiagrams for Syntax\nother, = others\nreturn Diagram(self.dom, other.cod,\nself.boxes + other.boxes,\nself.offsets + other.offsets,\nlayers=self.layers >> other.layers)\ndef tensor(self, other):\ndom, cod = self.dom @ other.dom, self.cod @ other.cod\nboxes = self.boxes + other.boxes\noffsets = self.offsets + [n + len(self.cod) for n in other.offsets]\nlayers = cat.Id(dom)\nfor left, box, right in self.layers:\nlayers = layers >> Layer(left, box, right @ other.dom)\nfor left, box, right in other.layers:\nlayers = layers >> Layer(self.cod @ left, box, right)\nreturn Diagram(dom, cod, boxes, offsets, layers=layers)\n@staticmethod\ndef id(dom):\nreturn Diagram(dom, dom, [], [], layers=cat.Id(dom))\ndef interchange(self, i, j, left=False):\n...\ndef normal_form(self, normalizer=None, **params):\n...\ndef draw(self, **params):\n...\nDiagrams always a carry a cat.Arrow called layers, which may be thought of as a\nwitness that the diagram is well-typed. If no layers are provided, the Diagram.__init__\nmethod computes the layers and checks that they compose. A cat.AxiomError is\nreturned when the layers do not compose. The Diagram.interchange method al-\nlows to change the order of layers in a diagram when they commute, and returns\nan InterchangerError when they don’t.\nThe Diagram.normal_form method im-\nplements the algorithm of [DV19a], see the rewriting module of DisCoPy.\nThe\nDiagram.draw method is implemented in the drawing module and allows to render a\ndiagram via matplotlib [] as well as generating a TikZ [] output for academic papers.\nFinally, a Box is initialised by a name together with domain and codomain types.\nListing 1.3.27. Box in a free monoidal category.\nclass Box(cat.Box, Diagram):\ndef __init__(self, name, dom, cod, **params):\ncat.Box.__init__(self, name, dom, cod, **params)\nlayer = Layer(dom[0:0], self, dom[0:0])\nlayers = cat.Arrow(dom, cod, [layer], _scan=False)\nDiagram.__init__(self, dom, cod, [self], [0], layers=layers)\nWe check that the axioms for monoidal categories hold up to interchanger.\n50\nDiagrams\nSection 1.3\nListing 1.3.28. Axioms of free monoidal categories\nx, y, z, w = Ty(’x’), Ty(’y’), Ty(’z’), Ty(’w’)\nf0, f1, f2 = Box(’f0’, x, y), Box(’f1’, z, w), Box(’f2’, z, w)\nd = Id(x) @ f1 >> f0 @ Id(w)\nassert f0 @ (f1 @ f2) == (f0 @ f1) @ f2\nassert f0 @ Diagram.id(Ty()) == f0 == Diagram.id(Ty()) @ f0\nassert d == (f0 @ f1).interchange(0, 1)\nassert f0 @ f1 == d.interchange(0, 1)\nFunctorial reductions are implemented via the monoidal.Functor class, initialised\nby a pair of mappings: ob from objects to types and ar from boxes to diagrams. It\ncomes with a __call__ method that scans through a diagram a replaces each box\nand identity wire with its image under the mapping.\nListing 1.3.29. Monoidal functor.\nclass Functor(cat.Functor):\ndef __init__(self, ob, ar, cod=(Ty, Diagram)):\nsuper().__init__(ob, ar)\ndef __call__(self, diagram):\nif isinstance(diagram, Ty):\nreturn self.cod[0].tensor(*[self.ob[Ty(x)] for x in diagram])\nif isinstance(diagram, Box):\nreturn super().__call__(diagram)\nif isinstance(diagram, Diagram):\nscan, result = diagram.dom, self.cod[1].id(self(diagram.dom))\nfor box, off in zip(diagram.boxes, diagram.offsets):\nid_l = self.cod[1].id(self(scan[:off]))\nid_r = self.cod[1].id(self(scan[off + len(box.dom):]))\nresult = result >> id_l @ self(box) @ id_r\nscan = scan[:off] @ box.cod @ scan[off + len(box.dom):]\nreturn result\nraise TypeError()\nWe check that the axioms hold.\nx, y, z = Ty(’x’), Ty(’y’), Ty(’z’)\nf0, f1, f2 = Box(’f0’, x, y), Box(’f1’, y, z), Box(’f2’, z, x)\nF = Functor(ob={x: y, y: z, z: x}, ar={f0: f1, f1: f2, f2: f0})\nassert F(f0 >> f1) == F(f0) >> F(f1)\nassert F(f0 @ f1) == F(f0) @ F(f1)\nassert F(f0 @ f1 >> f1 @ f2) == F(f0) @ F(f1) >> F(f1) @ F(f2)\nAny operad.Tree can be turned into an equivalent monoidal.Diagram. We show\nhow this interface is built by overriding the __call__ method of operad.Algebra.\nListing 1.3.30. Interface with operad.Tree.\nfrom discopy import operad\nclass Algebra(operad.Algebra):\ndef __init__(self, ob, ar, cod=Diagram, contravariant=False):\n51\nChapter 1\nDiagrams for Syntax\nself.contravariant = contravariant\nsuper().__init__(self, ob, ar, cod=cod)\ndef __call__(self, tree):\nif isinstance(tree, operad.Id):\nreturn self.cod.id(self.ob[tree.dom])\nif isinstance(tree, operad.Box):\nreturn self.ar[tree]\nbox = self.ar[tree.root]\nif isinstance(box, monoidal.Diagram):\nif self.contravariant:\nreturn box << monoidal.Diagram.tensor(\n*[self(branch) for branch in tree.branches])\nreturn box >> monoidal.Diagram.tensor(\n*[self(branch) for branch in tree.branches])\nreturn box(*[self(branch) for branch in tree.branches])\nob2ty = lambda ob: Ty(ob)\nnode2box = lambda node: Box(node.name, Ty(node.dom), Ty(*node.cod))\nt2d = Algebra(ob2ty, node2box, cod=Diagram)\nnode2box_c = lambda node: Box(node.name, Ty(*node.cod), Ty(node.dom))\nt2d_c = Algebra(ob2ty, node2boxc, cod=Diagram, contravariant=True)\ndef tree2diagram(tree, contravariant=False):\nif contravariant:\nreturn t2dc(tree)\nreturn t2d(tree)\n52\nCategorial grammar\nSection 1.4\n1.4\nCategorial grammar\nThe categorial tradition of formal grammars originated in the works of Ajdukiewicz\n[Ajd35] and Bar-Hillel [Bar53], their formalisms are now known as AB grammars\n[Bus16]. They analyse language syntax by assigning to every word a type generated\nfrom basic types using two operations: \\ (under) and / (over). Strings of types are\nthen parsed according to the following basic reductions:\nα (α\\β) →β\n(α/β) β →α\n(1.11)\nThe slash notation α/β, replacing the earlier fraction α\nβ, is due to Lambek who\nuniﬁed categorial grammars in an algebraic foundation known as the Lambek calculus,\nﬁrst presented in his seminal 1958 paper [Lam58]. With the advent of Chomsky’s\ntheories of syntax in the 1960s, categorial grammars were disregarded for almost\ntwenty years [Lam88]. They were revived in the 1980s by several researchers such as\nBuszowski in Poland, van Benthem and Moortgat in the Netherlands, as witnessed\nin the 1988 books [OBW88; Moo88].\nOne reason for this revival is the proximity between categorial grammars and logic.\nIndeed, the original rewrite rules (1.11) can be seen as versions of modus ponens in a\nGentzen style proof system [Lam99b]. Another reason for this revival, is the proximity\nbetween categorial grammars, the typed Lambda calculus [Chu40] and the semantic\ncalculi of Curry [Cur61] and Montague [Mon73]. Indeed, one of the best intuitions\nfor categorial grammars comes from interpreting the slash type α/β as the type of a\nfunction with input of type β and output of type α, and the reduction rules (1.11)\nas function application. From this perspective, the syntactic process of recognizing a\nsentence has the same form as the semantic process of understanding it [Ste00]. We\nwill see the implications of this philosophy in 2.2.\nAlthough he did not mention categories in his original paper [Lam58], Lambek\nhad in mind the connections between linguistics and category theory all along, as\nmentioned in [Lam88].\nIndeed the reductions in (1.11) and those introduced by\nLambek, correspond to the morphisms of free biclosed categories, which admit a neat\ndescription as deductions in a Gentzen style proof system. This leads to a fruitful\nparallel between algebra, proof theory and categorial grammar, summarized in the\nfollowing table.\nCategories\nLogic\nLinguistics\nPython\nBiclosed category\nProof system\nCategorial grammar\nDisCoPy\nobjects\nformulas\ntypes\nbiclosed.Ty\ngenerators\naxioms\nlexicon\nbiclosed.Box\nmorphisms\nproof trees\nreductions\nbiclosed.Diagram\nWe start by deﬁning biclosed categories and a general notion of biclosed gram-\nmar. Then the section will unfold as we unwrap the deﬁnition, meeting the three\nmost prominent variants of categorial grammars: AB grammars [Bus16], the Lambek\ncalculus [Lam58] and Combinatory Categorial Grammars [Ste00]. We end by giv-\ning an implementation of free biclosed categories as a class biclosed.Diagram with\nmethods for currying and uncurrying.\n53\nChapter 1\nDiagrams for Syntax\n1.4.1\nBiclosed categories\nDeﬁnition 1.4.1 (Biclosed signature). A biclosed signature G is a collection of gen-\nerators G1 and basic types G0 together with a pair of maps:\nT(G0)\ndom\n←−−G1\ncod\n−−→T(G0)\nwhere T(G0) is the set of biclosed types, given by the following inductive deﬁnition:\nT(B) ∋α = a ∈B | α ⊗α | α\\α | α/α\n(1.12)\nA morphism of biclosed signatures ϕ : G →Γ is a pair of maps ϕ1 : G1 →Γ1 and\nϕ0 : G0 →Γ0 such that the diagram with the signature morphisms commutes. The\ncategory of biclosed signatures is denoted BcSig\nDeﬁnition 1.4.2 (Biclosed category). A biclosed monoidal category C is a monoidal\ncategory equipped with two bifunctors −\\−: Cop × C →C and −/−: C × Cop →C\nsuch that for any object a, a ⊗−⊣a\\−and −⊗a ⊣−/a. Explicitly, we have the\nfollowing isomorphisms natural in a, b, c ∈C0:\nC(a, c/b) ≃C(a ⊗b, c) ≃C(b, a\\c)\n(1.13)\nThese isomorphisms are often called currying (when ⊗is replaced by \\ or /) and un-\ncurrying. With monoidal functors as morphisms, biclosed categories form a category\ndenoted BcCat.\nMorphisms in free biclosed category can be described as the valid deductions in a\nproof system deﬁned as follows. The axioms of monoidal categories may be expressed\nas the following rules of inference:\nid\na →a\na →b\nb →c ◦\na →c\na →b\nc →d ⊗\na ⊗c →b ⊗d\n(1.14)\nAdditionally, the deﬁning adjunctions of biclosed categories may be expressed as\nfollows:\na ⊗b →c\niﬀ\na →c/b\niﬀ\nb →a\\c\n(1.15)\nGiven a biclosed signature G, the free biclosed category BC(G) contains all the\nmorphisms that can be derived using the inference rules 1.15 and 1.14 from the\nsignature seen as a set of axioms for the deductive system. BC is part of an adjunction\nrelating biclosed signatures and biclosed categories:\nBcSig\nBC\n⇄\nU\nBcCat\nWe can now deﬁne a general notion of biclosed grammar, the equivalent of monoidal\ngrammars in a biclosed setting.\n54\nCategorial grammar\nSection 1.4\nDeﬁnition 1.4.3. A biclosed grammar G is a biclosed signature of the following\nshape:\nT(B + V )\ndom\n←−−G\ncod\n−−→T(B + V )\nwhere V is a vocabulary and B is a set of basic types. The language generated by a\nG is given by:\nL(G) = { u ∈V ∗| ∃g : u →s ∈BC(G) }\nwhere BC(G) is the free biclosed category generated by G. We say that G is lexicalised\nwhen it has the following shape:\nV\ndom\n←−−G\ncod\n−−→T(B)\nAs we will see, AB grammars, Lambek grammars and Combinatory Categorial\ngrammars all reduce functorially to biclosed grammars.\nHowever, biclosed gram-\nmars can have an inﬁnite number of rules of inference, obtained by iterating over\nthe isomorphism (1.15). Interestingly, these rules have been discovered progressively\nthroughout the history of categorial grammars.\n1.4.2\nAjdiuciewicz\nWe discuss the classical categorial grammars of Ajdiuciewicz and Bar-Hillel, known\nas AB grammars [Bus16]. The types of the original AB grammars are given by the\nfollowing inductive deﬁnition:\nTAB(B) ∋α = a ∈B | α\\α | α/α .\nGiven a vocabulary V , the lexicon is usually deﬁned as a relation ∆⊆V × TAB(B)\nassigning a set of candidate types ∆(w) to each word w ∈V . Given an utterance\nu = w0 . . . wn ∈V ∗, one can prove that u is a grammatical sentence by producing\na reduction t0 . . . tn →s for some ti ∈∆(wi), generated by the basic reductions in\n(1.11).\nDeﬁnition 1.4.4 (AB grammar). An AB grammar is a tuple G = (V, B, ∆, s) where\nV is a vocabulary, B is a ﬁnite set of basic types, and ∆⊆V × TAB(B) is a ﬁnite\nrelation, called the lexicon. The rules of AB grammars are given by the following\nmonoidal signature:\nRAB =\n\n\n\n\n\n\n\n\n\nb/a\na\na\\b\na\n,\nb\nb\napp\napp\n\n\n\n\n\n\n\n\n\na,b∈TAB(B)\n.\nThen the language generated by G is given by:\nL(G) = { u ∈V ∗: ∃g ∈MC(∆+ RAB)(u, s) }\n55\nChapter 1\nDiagrams for Syntax\nRemark 1.4.5. Sometimes, categorial grammarians use a diﬀerent notation where\na\\b is used in place of b\\a. We ﬁnd the notation used here more intuitive: we write\na ⊗a\\b →b instead of a ⊗b\\a →b. Ours is in fact the notation used in the original\npaper by Lambek [Lam58].\nExample 1.4.6 (Application). As an example take the vocabulary V = {Caesar, crossed, the, Rubicon}\nand basic types B = {s, n, d} for sentence, noun and determinant types. We deﬁne\nthe lexicon ∆by:\n∆(Caesar) = { n }\n∆(crossed) = { n\\(s/n) }\n∆(the) = { d }\n∆(Rubicon) = { d\\n }\nThen the sentence “John wrote a dictionnary” is grammatical as witnessed by the\nfollowing reduction:\nCaesar\ncrossed\nRubicon\ns\nn\nd\\n\nn\\(s/n)\nthe\nn\nd\ns/n\napp\napp\napp\nProposition 1.4.7. AB grammars reduce functorially to biclosed grammars.\nProof. It is suﬃcient to show that the rules RAB can be derived from the axioms of\nbiclosed categories, i.e. that they are morphisms in any free biclosed category. Let\na, b be objects in a free biclosed category. We derive the forward application rule as\nfollows.\nid\na/b →a/b\n1.15\na/b ⊗b →a\nSimilarly, one may derive backward application app< : b ⊗b\\a →b.\nAB grammars are weakly equivalent to context-free grammars [Ret05] as shown\nby the following pair of propositions.\nProposition 1.4.8. For any AB grammar there is a functorially equivalent context-\nfree grammar in Chomsky normal form.\nProof. The only diﬃculty in this proof comes from the fact that RAB is an inﬁ-\nnite set, whereas context-free grammars must be deﬁned over a ﬁnite set of symbols\nand production rules.\nGiven an AB grammar G = (V, B, ∆), deﬁne X = {x ∈\nTAB(B)|∃(w, t) ∈∆such thatx ⊆t} where we write x ⊆t to say that x is a sub-type\nof t. Note that X is ﬁnite. Now let P = {r ∈RAB|dom(r) ∈X}. Note that also P is\na ﬁnite set. Then X ←P + ∆→(X + V )∗forms a lexicalised context-free grammar\nwhere each production rule has at most arity 2, i.e. P + ∆is in Chomsky normal\nform. There is a functorial reduction MC(P + ∆) →MC(RAB + ∆) induced by the\ninjection P ,−→RAB, also there is a functorial reduction MC(RAB+∆) →MC(P +∆)\nwhich sends all the types in TAB(B)\\X to the unit and acts as the identity on the\nrest. Therefore G is functorially equivalent to P + ∆.\n56\nCategorial grammar\nSection 1.4\nProposition 1.4.9. Any context-free grammar in Greibach normal form reduces func-\ntorially to an AB grammar.\nProof. Recall that a CFG is in Greibach normal form when it has the shape:\nB ←G →V × B∗\nWe can rewrite this as a signature of the following shape:\nV ←G →B × B∗\nThis yields a relation G ⊆V × (B × B∗). We deﬁne the lexicon ∆⊆V × T(B)\nby ∆(w) = G(w)0/inv(G(w)1) where inv inverts the order of the basic types. Then\nthere is a functorial reduction MC(G) →MC(∆+ RAB) given on any production\nrule wa0 . . . ak →b in G by:\nw\n. . .\na0\nak\nb\n7→\nw\n. . .\na0\nak\nb/(ak . . . a0)\napp\napp\nb/(ak . . . a1)\nb\n1.4.3\nLambek\nIn his seminal paper [Lam58], Lambek introduced two new types of rules to the\noriginal AB grammars: composition and type-raising. Several extensions have been\nconsidered since, including non-associative [MR12c] and multimodal [MR12b] ver-\nsions. Here, we focus on the original (associative) Lambek calculus which is easier to\ncast in a biclosed setting and already captures a large fragment of natural language.\nThe types of the Lambek calculus are precisely the biclosed types given in 1.17.\nThis is not a coincidence since Lambek was aware of biclosed categories when he\nintroduced his calculus [Lam88].\nDeﬁnition 1.4.10 (Lambek grammar). A Lambek grammar is a tuple G = (V, B, ∆, s)\nwhere V is a vocabulary, B is a ﬁnite set of basic types and ∆⊆V ×T(B) is a lexicon.\nThe rules of Lambek grammars are given by the following monoidal signature:\nRLG =\n\n\n\n\n\n\n\n\n\nb/a\na\na\\b\na\n,\nb\nb\na/b b/c\nb\\c\na\\b\n,\na\\c\na/c\n,\na\na\n,\nb/(a\\b)\n(b/a)\\b\n,\napp\napp\ntyr\ntyr\ncomp\ncomp\n\n\n\n\n\n\n\n\n\na,b,c∈T(B)\n.\nThen the language generated by G is given by:\nL(G) = { u ∈V ∗: ∃g ∈MC(∆+ RLG)(u, s) }\n57\nChapter 1\nDiagrams for Syntax\nExample 1.4.11 (Composition). We adapt an example from [Ste87]. Consider the\nfollowing lexicon:\n∆(I) = ∆(Grandma) = { n }\n∆(the) = { d }\n∆(parmigiana) = { d\\n }\n∆(will) = ∆(may) = { (n\\s)/(n\\s) }\n∆(eat) = ∆(cook) = { n\\s/n }\nThe following is a grammatical sentence, parsed using the composition rule:\nGrandma\nparmigiana\nn\ncomp\nwill\nn\n(n\\s)/n\napp\ns\ncook\napp\nn\\s\n(n\\s)/(n\\s)\n(n\\s)/n\nthe\napp\nd\\n\nd\nAnd the following sentence requires the use of type-raising:\nGrandma\nparmigiana\nn\ncomp\nmay cook\nn\n(x\\x)/x\napp\ns/n\napp\nx\\x\n(n\\s)/n\ns/(n\\s)\nand\nI\nwill eat\ntyr\ns/n\ncomp\nn\n(n\\s)/n\ntyr\ns/n\napp\ns\ns/(n\\s)\nwhere x = s/n and we have omitted the composition of modalities (will, may) with\ntheir verbs (cook, eat).\nProposition 1.4.12. Lambek grammars reduce functorially to biclosed grammars.\nProof. It is suﬃcient to show that the rules of Lambek grammars RLG can be derived\nfrom the axioms of biclosed categories. We already derived forward and backward\napplication in 1.4.7. The following proof tree shows that the forward composition\nrule follows from the axioms of biclosed categories.\nid\na/b →a/b\nid\nb/c →b/c\n1.15\nb/c ⊗c →b ⊗and ◦app\na/b ⊗b/c ⊗c →a 1.15\na/b ⊗b/c →a/c\n58\nCategorial grammar\nSection 1.4\nA similar argument derives the backward composition rule comp< : a/b ⊗a\\c →c/b.\nForward type-raising is derived as follows.\nid\na\\b →a\\b\n1.15\na ⊗a\\b →b 1.15\na →b/(a\\b)\nAnd a similar argument derives backward type-raising tyr< : a →(b/a)\\b.\nIt was conjectured by Chomsky that any Lambek grammar is weakly equivalent\nto a context-free grammar, i.e. that the language recognised by Lambek calculi is\ncontext-free. This conjecture was proved in 1993 by Pentus [Pen93], calling for a\nmore expressive version of categorial grammars.\n1.4.4\nCombinatory\nIn the 1980s, researchers interested in the syntax of natural language started rec-\nognizing that certain grammatical sentences naturally involve crossed dependencies\nbetween words, a phenomenon that cannot be captured by context-free grammars.\nThese are somewhat rare in English, but they abound in Dutch for instance [Bre+82].\nAn English example is the sentence ”I explained to John maths” which is often al-\nlowed to mean ”I explained maths to John”. Modeling cross-serial dependencies was\none of the main motivations for the development the Tree-adjoining grammars of\nJoshi [Jos85] and the Combinatory Categorial grammars (CCGs) of Steedman [Ste87;\nSte00]. These were later shown to be weakly equivalent to linear indexed grammars\n[VW94], making them all “mildly-context sensitive” according to the deﬁnition of\nJoshi [Jos85].\nCCGs extend the earlier categorial grammars by adding a crossed composition\nrule which allows for controlled crossed dependencies within a sentence, and is given\nby the following pair of reductions:\nxcomp> : a/b ⊗c\\b →c\\a\nxcomp< : a/b ⊗a\\c →c/b\nWe start by deﬁning CCGs as monoidal grammars, and then discuss how they relate\nto biclosed categories.\nDeﬁnition 1.4.13 (Combinatory Categorial Grammar). A CCG is a tuple G =\n(V, B, ∆, s) where V is a vocabulary, B is a ﬁnite set of basic types and ∆⊆V ×T(B)\nis a lexicon. The rules of CCGs are given by the following monoidal signature:\nRCCG = RLG +\n\n\n\n\n\n\n\n\n\na/b a\\c\nc\\b\na/b\n,\nc\\a\nc/b\nxcomp\nxcomp\n\n\n\n\n\n\n\n\n\na,b,c∈T(B)\n.\nThen the language generated by G is given by:\nL(G) = { u ∈V ∗: ∃g ∈MC(∆+ RCCG)(u, s) }\n59\nChapter 1\nDiagrams for Syntax\nExample 1.4.14 (Crossed composition). Taking the grammar from Example and\nadding two lexical entries we may derive the following grammatical sentences:\nGradma\ncooked\nfor\ns\nn\ns\\s\nn\\(s/n)\nparmigiana\nn\ns/n\napp\napp\napp\nme\n(s\\s)/n\nn\napp\ns\nGradma\ncooked\nfor\ns/n\nn\ns\\s\nn\\(s/n)\nparmigiana\nn\ns/n\napp\napp\nxcomp\nme\n(s\\s)/n\nn\napp\ns\nNote that the ﬁrst one can be parsed already in an AB grammar, whereas the second\nrequires the crossed composition rule.\nIt was ﬁrst shown by Moortgat [Moo88] that the crossed composition rule cannot\nbe derived from the axioms of the Lambek calculus. In our context, this implies that\nwe cannot derive xcomp from the axioms of biclosed categories. Of course, CCGs may\nbe seen as biclosed grammars by adding the crossed composition rules RCCG −RLG\nas generators in the signature. However, it is interesting to note that these rules can\nbe derived from closed categories, the symmetric version of biclosed categories:.\nDeﬁnition 1.4.15 (Symmetric monoidal category). A symmetric monoidal category\nC is a monoidal category equipped with a natural transformation swap : a⊗b →b⊗a\nsatisfying:\n=\n,\n=\nf\n,\nf\na\nb\na\nb\na\nb\na\nb\nc\nb\na\nc\na\nc\nc\nb\n=\na b\nb\nc\nc\na\na\nb\nb\nc\nc\na\n.\n(1.16)\nfor any a, b, c ∈C0 and f : a →b ∈C1.\nDeﬁnition 1.4.16 (Closed category). A closed category is a symmetric biclosed cat-\negory.\nProposition 1.4.17. The crossed composition rule follows from the axioms of closed\ncategories.\nProof. Let a, b, c be objects in the free closed category with no generators.\nThe\nfollowing proof tree shows that the backward crossed composition rule follows from\nthe axioms of closed categories.\n60\nCategorial grammar\nSection 1.4\nid\na/b →a/b\nid\nc\\b →c\\b\n1.15\nc ⊗c\\b →b ⊗and ◦app\na/b ⊗c ⊗c\\b →a ◦swap\na/b ⊗c\\b ⊗c →a 1.15\na/b ⊗c\\b →a\\c\nNote the crucial use of pre-composition with swap in the penultimate inference. A\nsimilar argument derives the forward crossed composition rule a/b ⊗a\\c →c/b.\nOne needs to be very careful in adding permutations and symmetry in a formal\ngrammar. The risk is to lose any notion of word order. It was initially believed that\nadding the crossed composition rule, coupled with type-rasing, would collapse the\ncalculus to permutation invariance [Wil03]. However, as argued by Steedman this\nis not the case: out of the n! possible permutations of a string of n types, CCGs\nonly allow S(n) permutations where S(n) is the nth Large Schroder number [Ste19].\nThus the crossed composition rule only permits limited crossed dependencies within\na sentence. This claim is also veriﬁed empirically by the successful performance of\nlarge scale CCG parsers [Yos+19].\nIf we consider the complexity of parsing categorial grammars, it turns out that\nparsing the Lambek calculus is NP-complete [Sav12]. Even when the order of types is\nbounded the best parsing algorithms for the Lambek calculus run in O(n5) [Fow08].\nParsing CCGs is in P with respect to the size of the input sentence, but it becomes\nexponential when also the grammar is taken in the input [KSJ18]. This is in contrast\nto CFGs or the mildly context-sensitive tree-adjoining grammars where parsing is\ncomputable in O(n3) in both the size of input grammar and input sentence. This\ncalled for a simpler version of the Lambek calculus, with a more eﬃcient parsing\nprocedure, but still keeping the advantages of the categorial lexicalised formalism\nand the proximity to semantics.\n1.4.5\nbiclosed.Diagram\nWe wish to upgrade the monoidal.Diagram class to represent diagrams in free bi-\nclosed categories. In order to achieve this, we ﬁrst need to upgrade the Ty class,\nto handle the types T(B) of free biclosed categories. A biclosed.Ty is an instance\nof monoidal.Ty with extra methods for under \\ and over / operations. Recall the\ncontext-free grammar of biclosed types:\nT(B) ∋α = a ∈B | α ⊗α | α\\α | α/α\n(1.17)\nElements of T(B) are trees built from leaves B and binary nodes ⊗, \\ and /. It may\nbe implemented in object-oriented programming, by subclassing monoidal.Ty for the\ntensor and using the native tree structure of classes. We give a working version of\nbiclosed.Ty, detailing also some of the standard methods.\nListing 1.4.18. Types in free biclosed categories\nfrom discopy import monoidal\n61\nChapter 1\nDiagrams for Syntax\nclass Ty(monoidal.Ty):\ndef __init__(self, *objects, left=None, right=None):\nself.left, self.right = left, right\nsuper().__init__(*objects)\ndef __lshift__(self, other):\nreturn Over(self, other)\ndef __rshift__(self, other):\nreturn Under(self, other)\ndef __matmul__(self, other):\nreturn Ty(*(self @ other))\n@staticmethod\ndef upgrade(old):\nif len(old) == 1 and isinstance(old[0], (Over, Under)):\nreturn old[0]\nreturn Ty(*old.objects)\nclass Over(Ty):\ndef __init__(self, left=None, right=None):\nTy.__init__(self, self)\ndef __repr__(self):\nreturn \"Over({}, {})\".format(repr(self.left), repr(self.right))\ndef __str__(self):\nreturn \"({} << {})\".format(self.left, self.right)\ndef __eq__(self, other):\nif not isinstance(other, Over):\nreturn False\nreturn self.left == other.left and self.right == other.right\nclass Under(Ty):\ndef __init__(self, left=None, right=None):\nTy.__init__(self, self)\ndef __repr__(self):\nreturn \"Under({}, {})\".format(repr(self.left), repr(self.right))\ndef __str__(self):\nreturn \"({} >> {})\".format(self.left, self.right)\ndef __eq__(self, other):\nif not isinstance(other, Under):\nreturn False\nreturn self.left == other.left and self.right == other.right\nThe Ty.upgrade method allows for compatibility between the tensor methods __matmul__\nof biclosed and monoidal types. The upgrading mechanism is further desribed in\n62\nCategorial grammar\nSection 1.4\nthe documentation of monoidal.\nWe illustrate the syntax of biclosed types.\nx = Ty(’x’)\nassert x >> x << x == Over(Under(Ty(’x’), Ty(’x’)), Ty(’x’))\nassert x >> (x << x) == Under(Ty(’x’), Over(Ty(’x’), Ty(’x’)))\nx0, x1, y0, y1, m = Ty(’x0’), Ty(’x1’), Ty(’y0’), Ty(’y1’), Ty(’m’)\nlens = (x0 >> m @ y0) @ ( m @ x1 >> y1)\nassert lens == Ty(Under(Ty(’x0’), Ty(’m’, ’y0’)), Under(Ty(’m’, ’x1’), Ty(’y1’)))\nA biclosed.Diagram is a monoidal.Diagram with domain and codomain biclosed.Tys,\ntogether with a pair of static methods curry() and uncurry() implementing the\ndeﬁning isomorphism of biclosed categories (1.15). In fact, we can store the infor-\nmation of how a biclosed diagram is constructed by using two special subclasses of\nBox, which record every application of the currying morphisms. Thus a diagram in a\nbiclosed category is a tree built from generating boxes using id, then, tensor, Curry\nand UnCurry.\nListing 1.4.19. Diagrams in free biclosed categories\nfrom discopy import monoidal\n@monoidal.Diagram.subclass\nclass Diagram(monoidal.Diagram):\ndef curry(self, n_wires=1, left=False):\nreturn Curry(self, n_wires, left)\ndef uncurry(self):\nreturn UnCurry(self)\nclass Id(monoidal.Id, Diagram):\nclass Box(monoidal.Box, Diagram):\nclass Curry(Box):\ndef __init__(self, diagram, n_wires=1, left=False):\nname = \"Curry({})\".format(diagram)\nif left:\ndom = diagram.dom[n_wires:]\ncod = diagram.dom[:n_wires] >> diagram.cod\nelse:\ndom = diagram.dom[:-n_wires]\ncod = diagram.cod << diagram.dom[-n_wires or len(diagram.dom):]\nself.diagram, self.n_wires, self.left = diagram, n_wires, left\nsuper().__init__(name, dom, cod)\nclass UnCurry(Box):\ndef __init__(self, diagram):\nname = \"UnCurry({})\".format(diagram)\n63\nChapter 1\nDiagrams for Syntax\nself.diagram = diagram\nif isinstance(diagram.cod, Over):\ndom = diagram.dom @ diagram.cod.right\ncod = diagram.dom.left\nsuper().__init__(name, dom, cod)\nelif isinstance(diagram.cod, Under):\ndom = diagram.dom.left @ diagram.dom\ncod = diagram.dom.right\nsuper().__init__(name, dom, cod)\nelse:\nsuper().__init__(name, diagram.dom, diagram.cod)\nWe give a simple implementation of free biclosed categories which does not impose\nthe axioms of free biclosed categories, UnCurry(Curry(f)) == f and naturality. IT\ncan be implemented on syntax by adding if statement in the inits, and upgrading\nCurry and UnCurry to subclasses of biclosed.Diagram..\nFinally, a biclosed.Functor is a monoidal.Functor whose call method has a\npredeﬁned mapping for all structural boxes in biclosed. It thus allows to map any\nbiclosed.Diagram into a codomain class cod equipped with curry and uncurry\nmethods.\nListing 1.4.20. Functors from free biclosed categories\nclass Functor(monoidal.Functor):\ndef __init__(self, ob, ar, cod=(Ty, Diagram)):\nself.cod = cod\nsuper().__init__(ob, ar, ob_factory=cod[0], ar_factory=cod[1])\ndef __call__(self, diagram):\nif isinstance(diagram, Over):\nreturn self(diagram.left) << self(diagram.right)\nif isinstance(diagram, Under):\nreturn self(diagram.left) >> self(diagram.right)\nif isinstance(diagram, Ty) and len(diagram) > 1:\nreturn self.cod[0].tensor(*[\nself(diagram[i: i + 1]) for i in range(len(diagram))])\nif isinstance(diagram, Id):\nreturn self.cod[1].id(self(diagram.dom))\nif isinstance(diagram, Curry):\nn_wires = len(self(getattr(\ndiagram.cod, ’left’ if diagram.left else ’right’)))\nreturn self.cod[1].curry(\nself(diagram.diagram), n_wires, diagram.left)\nif isinstance(diagram, UnCurry):\nreturn self.cod[1].uncurry(self(diagram.diagram))\nreturn super().__call__(diagram)\nWe recover the rules of categorial grammars (in historical order) by constructing\nthem from identities in the free biclosed category with no generators.\nListing 1.4.21. Categorial grammars and the free biclosed category\n64\nCategorial grammar\nSection 1.4\n# Adjiuciewicz\nFA = lambda a, b: UnCurry(Id(a >> b))\nassert FA(x, y).dom == x @ (x >> y) and FA(x, y).cod == y\nBA = lambda a, b: UnCurry(Id(b << a))\nassert BA(x, y).dom == (y << x) @ x and BA(x, y).cod == y\n# Lambek\nproofFC = FA(x, y) @ Id(y >> z) >> FA(y, z)\nFC = Curry(proofFC, left=True)\nassert FC.dom == (x >> y) @ (y >> z) and FC.cod == (x >> z)\nBC = Curry(Id(x << y) @ BA(z, y) >> BA(y, x))\nassert BC.dom == (x << y) @ (y << z) and BC.cod == (x << z)\nTYR = Curry(UnCurry(Id(x >> y)))\nassert TYR.dom == x and TYR.cod == (y << (x >> y))\n# Steedman\nSwap = lambda a, b: Box(’Swap’, a @ b, b @ a)\nproofBX = Id(x << y) @ (Swap(z >> y, z) >> FA(z, y)) >> BA(y, x)\nBX = Curry(proofBX)\nassert BX.dom == (x << y) @ (z >> y) and BX.cod == (x << z)\nproofFX = (Swap(x, y << x) >> BA(x, y)) @ Id(y >> z) >> FA(y, z)\nFX = Curry(proofFX, left=True)\nassert FX.dom == (y << x) @ (y >> z) and FX.cod == (x >> z)\nThe assertions above are alternative proofs of Propositions 1.4.7, 1.4.12 and 1.4.17.\nWe draw the proofs for forward composition (proofFC) and backwards crossed com-\nposition (proofBX).\n65\nChapter 1\nDiagrams for Syntax\n1.5\nPregroups and dependencies\nIn his 1897 paper “The logic of relatives” [Pei97], Peirce makes an analogy between\nthe sentence “John gives John to John” and the molecule of ammonia.\nJohn\nJohn\nJohn\ngives\nH\nH\nH\nN\nThe intuition that words within a sentence are connected by “bonds”, as atoms in a\nmolecule, is the basis of Peirce’s diagrammatic approach to logical reasoning, and of\nhis analysis of the concept of valency in grammar [Ask19]. This intuition resurfaced\nin the work of Lucien Tesniére [Tes59] who analysed the valency of a large number of\nlexical items, in an approach to syntax that became known as dependency grammar\n[Hay64; Gai65]. These bear a striking resemblance to Lambek’s pregroup grammars\n[Lam08] and its developments in the DisCoCat framework of Coecke, Sadrzadeh et\nal. [CCS08; SCC13; SCC14].\nIn this section, we formalise both Pregroup Grammar (PG) and Dependency\nGrammar (DG) in the language of free rigid categories. Once casted in this alge-\nbraic framework, the similarities between PG and DG become apparent. We show\nthat dependency grammars are structurally equivalent to both pregroup grammar\nand context-free grammar, i.e. their derivations are tree-shaped rigid diagrams. We\nend by describing the implementation of the class rigid.Diagram and we interface\nit with SpaCy’s dependency parser [Hon+20].\n1.5.1\nPregroups and rigid categories\nPregroup grammars were introduced by Lambek in 1999 [Lam99a]. Arising from a\nnon-commutative fragment of Girard’s linear logic [CL02] they reﬁne and simplify the\nearlier Lambek calculus discussed in Section 1.4.3. As shown by Buszowski, pregroup\ngrammars are weakly equivalent to context-free grammars [BM07].\nHowever, the\nsyntactic structures generated by pregroup grammars diﬀer from those of a CFG.\nInstead of trees, pregroups parse sentences by assigning to them a nested pattern of\ncups or “links” as in the following example.\nBruno\ngives\nbook\nJohn\na\nIn the strict sense of the word, a pregroup is a preordered monoid where each object\nhas a left and a right adjoint [Lam99a]. We formalise pregroup grammars in terms of\nrigid categories which categorify the notion of pregroup by upgrading the preorder to\na category. Going from inequalities in a preordered monoid to arrows in a monoidal\ncategory allows both to reason about syntactic ambiguity — as discussed by Lambek\n66\nPregroups and dependencies\nSection 1.5\nand Preller [PL07] — as well as to deﬁne pregroup semantics as a monoidal functor,\nan observation which lead to the development of the compositional distributional\nmodels of Coecke et al. [CCS10] and which will form the basis of the next chapter on\nsemantics.\nGiven a set of basic types B, the set of pregroup types P(B) is deﬁned as follows.\nP(B) ∋t ::= b ∈B | tr | tl | t ⊗t.\nwe can use it to deﬁne the notion of a rigid signature.\nDeﬁnition 1.5.1 (Rigid signature). A rigid signature is a graph Σ = Σ1 ⇒P(Σ0).\nA morphism of rigid signatures Σ →Γ is a pair of maps Σ1 →Γ1 and Σ0 →Γ0\nsatisfying the obvious commuting diagram.\nRigid signatures and their morphisms\nform a category denoted RgSig.\nDeﬁnition 1.5.2 (Rigid category). A rigid category C is a monoidal category such\nthat each object a has a left adjoint al and a right adjoint ar. In other words there\nare morphisms al ⊗a →1, 1 →a ⊗al, a ⊗ar →1 and 1 →ar ⊗al, denoted as cups\nand caps and satisfying the snake equations:\n=\n=\na\nar\na\na\na\na\nal\na\n(1.18)\nThe category of rigid categories and monoidal functors is denoted RigidCat.\nProposition 1.5.3. Rigid categories are biclosed, with a\\b = ar ⊗b and b/a = b⊗al.\nGiven a rigid signature Σ we can generate the free rigid category\nRC(Σ) = MC(Σ + { cups, caps })/ ∼snake\nwhere ∼snake is the equivalence relation on diagrams induced by the snake equations\n(1.18).\nRigid categories are called compact 2-categories with one object by Lam-\nbek and Preller [PL07], who showed that RC deﬁnes an adjunction between rigid\nsignatures and rigid categories.\nRgSig\nRC\n⇄\nU\nRgCat\nWe start by deﬁning a general notion of rigid grammar, a subclass of biclosed\ngrammars.\nDeﬁnition 1.5.4. A rigid grammar G is a rigid signature of the following shape:\nP(B + V )\ndom\n←−−G\ncod\n−−→P(B + V )\nwhere V is a vocabulary and B is a set of basic types. The language generated by\nG is given by:\nL(G) = { u ∈V ∗| ∃g : u →s ∈RC(G) }\nwhere RC(G) is the free rigid category generated by G.\n67\nChapter 1\nDiagrams for Syntax\nA pregroup grammar is a lexicalised rigid grammar deﬁned as follows.\nDeﬁnition 1.5.5 (Pregroup grammar). A pregroup grammar is a tuple G = (V, B, ∆, I, s)\nwhere V is a vocabulary, B is a ﬁnite set of basic types, G ⊆V × P(B) is a lexicon\nassigning a set of possible pregroup types to each word, and I ⊆B × B is a ﬁnite set\nof induced steps. The language generated by G is given by:\nL(G) = { u ∈V ∗: ∃g ∈RC(G)(u, s) }\nwhere RC(G) := RC(∆+ I).\nExample 1.5.6. Fix the basic types B = { s, n, n1, d, d1 } for sentence, noun, plu-\nral noun, determinant and plural determinant and consider the following pregroup\nlexicon:\n∆(pair) = { dr n } , ∆(lives) = { dr\n1 n1 } , ∆(lovers) = { n1 } , ∆(starcross) = { n nl } ,\n∆(take) = { nr s nl } ,\n∆(of) = { nr n nl } ,\n∆(A) = { d } ,\n∆(their) = { d1 } .\nand one induced step I = { n1 →n }. Then the following is a grammatical sentence:\nA\npair\nof\nstarcross lovers take\ntheir lives\nwhere we omitted the types for readability, and we denoted the induced step using a\nblack node.\nThe tight connections between categorial grammars and pregroups were discussed\nin [Bus07], they are evermore apparent from a categorical perspective: since rigid cat-\negories are biclosed, there is a canonical way of mapping the reductions of a categorial\ngrammar to pregroups.\nProposition 1.5.7. For any Lambek grammar G there is a pregroup grammar G′\nwith a functorial reduction MC(G) →RC(G′).\nProof. The translation works as follows:\nb/c\na/b\nb\\c\ncomp\ncr\na\na br b cr\n7→\nb\na/b\nb\\c\napp\na\na br\nb\n7→\n,\nb\\c\na\\b\nb\\c\ncomp\nc\nal\nal b bl c\n7→\na\na\\b\nb\\c\napp\na\na\nal b\n7→\n,\n,\n,\na\nb/(a\\b)\ntyr\nbl\na\na\n7→\nb\na\na\n7→\na\n(b/a)\\b\ntyr\nb\nbr\n68\nPregroups and dependencies\nSection 1.5\nExample 1.5.8. Consider the categorial parsing of the sentence “Grandma will cook\nthe parmigiana” from Example 1.4.14.\nThe categorial type of “will” is given by\n(n\\s)/(n\\s) which translates to the pregroup type (nrs)(nrs)l = nrs sln, the tran-\nsitive verb type (n\\s)/n for “cook” translates to nr s nl, and similarly for the other\ntypes. Translating the categorial reduction according to the mapping above, we obtain\nthe following pregroup reduction:\nGrandma\nparmigiana\nwill\ncook\nthe\nOne advantage of pregroups over categorial grammars is that they can be parsed\nmore eﬃciently. This is a consequence of the following lemma, proved by Lambek in\n[Lam99a].\nProposition 1.5.9 (Switching lemma). For any pregroup grammar G and any re-\nduction t →s in RC(G), there is a type t′ such that t →s = t →t′ →s and t →t′\ndoesn’t use contractions (cups), t′ →s doesn’t use expansions (caps).\nRemark 1.5.10. Note that the equivalent lemma for categorial grammars would state\nthat all instances of the type-raising rule can appear after all instances of the compo-\nsition and application rules. This is however not the case.\nA direct corollary of this lemma, is that any sentence u ∈V ∗may be parsed using\nonly contractions (cups). This drastically reduces the search space for a reduction,\nwith the consequence that pregroup grammars can be parsed eﬃciently.\nProposition 1.5.11. Pregroup grammars can be parsed in polynomial time [DP05;\nMor11] and in linear-time in linguistically justiﬁed restricted cases [Pre07].\nAs discussed in Section 1.4.4, linguists have observed that certain grammatical\nsentences naturally involve crossed dependencies between words [Sta04]. Although\nthe planarity assumption is justiﬁed in several cases of interest [Can06], grammars\nwith crossed dependencies allow for more ﬂexibility when parsing natural languages.\nIn order to model these phenomena with pregroup grammars, we need to step out\nof (planar) rigid categories and allow for (restricted) permutations. These can be\nrepresented in the symmetric version of rigid categories, known as compact closed\ncategories.\nDeﬁnition 1.5.12 (Compact-closed). A compact-closed category is a rigid category\n(1.18) that is also symmetric (1.16).\nGiven a pregroup grammar G, we can generate the free compact-closed category\ndeﬁned by:\nCC(G) = RC(G + swap)/ ∼sym\n69\nChapter 1\nDiagrams for Syntax\nwhere ∼sym is the congruence induced by the axioms for the symmetry (1.16). Notice\nthat in a compact-closed category ar ≃al for any object a, see e.g. [HV19]. Pregroup\nreductions in the free rigid category RC(G) can of course be mapped in CC(G) via\nthe canonical rigid functor which forgets the r and l adjoint structure.\nWe cannot use free compact-closed categories directly to parse sentences. If we\nonly ask for a morphism g : u →s in CC(G) in order to show that the string u is\ngrammatical, then any permutation of the words in u would also count as grammat-\nical, and we would lose any notion of word order. In practice, the use of the swap\nmust be restricted to only special cases. These were discussed in 1.4, where we saw\nthat the crossed composition rule of combinatory grammars is suitable for modeling\nthese restricted cases.\nIn recent work [YK21], Yeung and Kartsaklis introduced a translation from CCG\ngrammars to pregroup grammars which allows to build a diagram in the free compact-\nclosed category over a pregroup grammar from any derivation of a CCG. This is useful\nin practical applications since it allows to turn the output of state-of-the-art CCG\nparsers such as [YNM17] into compact-closed diagrams. The translation is captured\nby the following proposition.\nProposition 1.5.13. For any combinatory grammar G there is a pregroup grammar\nG′ with a canonical functorial reduction G →CC(G′).\nProof. The translation is the same as 1.5.7, together with the following mapping for\nthe crossed composition rules:\nc\\b\na/b\nc\\a\nxcomp\na\ncl\na br cl b\n7→\n,\na\\c\na/b\nb\\c\nxcomp\nbr\nc\na br al c\n7→\n(1.19)\nRepresenting the crossed composition rule in compact-closed categories, allows to\nreason diagrammatically about equivalent syntactic structures.\nExample 1.5.14. As an example consider the pregroup grammar G with basic types\nB = { n, s } and lexicon given by:\n∆(cooked) = { nrsnl }\n∆(me) = ∆(Grandma) = ∆(parmigiana) = { n }\n∆(for) = { slsnl }\nThen using the grammar from Example 1.4.14, we can map the two CCG parses\nto the following compact-closed diagrams in CC(G), even though the second is not\ngrammatical in a planar pregroup grammar.\nGrandma\nme\nfor\ncooked\nparmigiana\nGrandma\nme\nfor\ncookedparmigiana\nIf we interpret the wires for words as the unit of the tensor, then these two diagrams\nare equal in CC(G) but not when seen in a biclosed category.\n70\nPregroups and dependencies\nSection 1.5\n1.5.2\nDependency grammars are pregroups\nDependency grammars arose from the work of Lucien Tesniere in the 1950s [Tes59].\nIt was made formal in the 1960s by Hays [Hay64] and Gaifman [Gai65], who showed\nthat they have the same expressive power as context-free grammars. Dependency\ngrammars are very popular in NLP, supported by large-scale parsing tools such as\nthose provided by Spacy [Hon+20]. We take the formalisation of Gaifman [Gai65] as\na starting point and show how the dependency relation may be seen as a diagram in\na free rigid category.\nLet us ﬁx a vocabulary V and a set of symbols B, called categories in [Gai65],\nwith s ∈B the sentence symbol.\nDeﬁnition 1.5.15 (Dependency grammar [Gai65]). A dependency grammar G con-\nsists in a set of rules G ⊆(B + V ) × B∗of the following shapes:\nI (x, y1 . . . yl ⋆yl+1 . . . yn) where x, yi ∈B, indicating that the symbol x may depend\non the symbols y1 . . . yl on the left and on the symbols yl+1 . . . yn on the right.\nII (w, x) for w ∈V and x ∈B, indicating that the word w may have type x.\nIII (x, s) indicating that the symbol x may govern a sentence.\nFollowing Gaifman [Gai65], we deﬁne the language L(G) generated by a depen-\ndency grammar G to be the set of strings u = w1w2 . . . wn ∈V ∗such that there\nare symbols x1, x2 . . . xn ∈B and a binary dependency relation d ⊆X × X where\nX = { x1, . . . , xn } satisfying the following conditions:\n1. (wi, xi) ∈G for all i ≤n,\n2. (xi, xi) /∈RTC(d) where RTC(d) is the reﬂexive transitive closure of d, i.e. the\ndependency relation is acyclic,\n3. if (xi, xj) ∈d an (xi, xk) ∈d then xj = xk, i.e. every symbol depends on at\nmost one head, i.e. the dependency relation is single-headed or monogamous,\n4. if i ≤j ≤k and (xi, xk) ∈RTC(d) then (xi, xj) ∈RTC(d), i.e. the dependency\nrelation is planar,\n5. there is exactly one xh such that ∀j (xh, xj) /∈d and (xh, s) ∈G, i.e. the relation\nis connected and rooted (xh is called the root and we say that xh governs the\nsentence).\n6. for every xi, let y1, . . . , yl ∈X be the (ordered list of) symbols which depend on\nxi from the left and yl+1, . . . , yn ∈X be the (ordered list of) symbols which de-\npend on xi from the right, then (x, y1 . . . yl⋆yl+1 . . . yn) ∈G, i.e. the dependency\nstructure is allowed by the rules of G.\n71\nChapter 1\nDiagrams for Syntax\nExample 1.5.16. Consider the dependency grammar with V = { Moses, crossed, the, Red, Sea },\nB = { d, n, a, s, v } and rules of type I:\n(v, n ⋆n), (a, ⋆n), (d, ⋆), (n, ⋆), (n, ad⋆) ,\nof type II:\n(Moses, n), (crossed, v), (the, d), (Red, a), (Sea, n)\nand a single rule of type III (v, s).\nThen the sentence “She tied a plastic bag” is\ngrammatical as witnessed by the following dependency relation:\nMoses\nSea\ncrossed\nthe\nRed\nThis combinatorial deﬁnition of a dependency relation has an algebraic counter-\npart as a morphism in a free rigid category. Given a dependency grammar G, let\n∆(G) ⊆V × P(B) be the pregroup lexicon deﬁned by:\n(w, yr\n1 . . . yr\nl x yl\nl+1 . . . yl\nn) ∈∆(G) ⇐⇒(w, x) ∈G ∧(x, y1 . . . yl ⋆yl+1 . . . yn) ∈G\n(1.20)\nalso, let I(G) be rules in G of the form (x, s) where x ∈B and s is the sentence\nsymbol.\nProposition 1.5.17. For any dependency grammar G, if a string of words is gram-\nmatical u ∈L(G) then there exists a morphism u →s in RC(∆(G) + I(G)).\nProof. Fix a dependency grammar G, and suppose w1 . . . wn ∈L(G) then there is a\nlist of symbols X = {x1, x2 . . . xn} with xi ∈B and a dependency relation d ⊆X ×X\nsuch that the conditions (1), . . . , (6) given above are satisﬁed. We need to show that\nd deﬁnes a diagram w1 . . . wn →s in RC(∆(G) + I(G)). Starting from the type\nw1w2 . . . wn, conditions (1) and (6) of the dependency relation ensure that there is an\nassignment of a single lexical entry in ∆(G) to each wi. Applying these lexical entries\nwe get a morphism w1w2 . . . wn →T in RC(∆(G)) where:\nT = ⊗n\ni=1(yr\ni,1 . . . yr\ni,li xi yl\ni,li+1 . . . yi,ni).\nFor each pair (xi, xj) ∈d with i ≤j, xi must appear as some yj,k with k ≤lj by\ncondition (6). Therefore we can apply a cup connecting xi and (yj,k)r in T. Similarly\nfor (xi, xj) ∈d with j ≤i, xi must appear as some yj,k with k > lj and we can apply\na cup connecting (yj,k)l and xi in T. By monogamy (3) and connectedness (5) of the\ndependency relation, there is exactly one such pair for each xi, except for the root\nxh. Therefore we can keep applying cups until only xh is left. By planarity (4) of the\ndependency relation, these cups don’t have to cross, which means that the diagram\nobtained is a valid morphism T →xh in RC(∆(G)). Finally condition (5) ensures\nthat there exists an induced step xh →s ∈I(G). Overall we have built a morphism\nw1w2 . . . wn →T →xh →s in RC(∆(G) + I(G)), as required.\n72\nPregroups and dependencies\nSection 1.5\nCorollary 1.5.18. For any dependency grammar G there is a pregroup grammar\n˜G = (V, B, ∆(G), I(G), s) such that L(G) ⊆L( ˜G).\nExample 1.5.19. An example of the translation deﬁned above is the following:\nCaesar\nRubicon\ncrossed\nthe\nCaesar\nRubicon\ncrossed\nthe\n7→\nn\nn\nd\nn\nnr\nnl\nn\nd\ndr\ns\nThe proposition above gives a structural reduction from dependency grammars to\npregroup grammars, where the dependency relation witnessing the grammaticality of\na string u is seen as a pregroup reduction u →s. This leads to a ﬁrst question: do\nall the pregroup reduction arise from a dependency relation? In [Pre07], Preller gives\na combinatorial description of pregroup reductions, which is strikingly similar to the\ndeﬁnition of dependency relation. In particular it features the same conditions for\nmonogamy (3), planarity (4) and connectedness (5). However, pregroup reductions\nare in general not acyclic, as the following example shows:\n. . .\n. . .\nxl\nx\nxl\nx\ny\nyr\nz\nz\nx\ny\nyr\ny\nzr\nzr\n(1.21)\nTherefore we do not expect that any given pregroup grammar reduces to a dependency\ngrammar. The question still remains for pregroup grammars with restricted types of\nlexicons. Indeed, the cyclic example above uses a lexicon which is not of the shape\n(1.20). We deﬁne operadic pregroups to be pregroup grammars with lexicon of the\nshape (1.20).\nDeﬁnition 1.5.20 (Operadic pregroup). An operadic pregroup is a pregroup grammar\nG = (V, B, ∆, s) such that for any lexical entry (w, t) ∈∆we have t = yrx zl for\nsome x ∈B and y, z ∈B∗.\nUsing Delpeuch’s autonomization of monoidal categories [Del19], we can show that\nreductions in an operadic pregroup always form a tree, justifying the name “operadic”\nfor these structures.\nProposition 1.5.21. Every operadic pregroup is functorially equivalent to a CFG.\nProof. It will be suﬃcient to show that the reductions of an operadic pregroup\nare trees. Fix an operadic pregroup ˜G = (V, B, ∆, I, s). We now deﬁne a functor\nF : RC( ˜G) →A(MC(G)) where A is the free rigid (or autonomous) category con-\nstruction on monoidal categories as deﬁned by Delpeuch [Del19], and G is a context-\nfree grammar with basic symbols B + V and production rules y1 . . . ylwyl+1 . . . yn →\n73\nChapter 1\nDiagrams for Syntax\nx ∈G whenever (w, yr\n1 . . . yr\nl x yl\nl+1 . . . yl\nn) ∈˜G. F is given by the identity on objects\nF(x) = x, and on lexical entries it is deﬁned by:\nw\nx\nyr\n1\nyr\nl\nyl\nl+1\nyr\nn\n. . .\n. . .\n7→\nw\nx\nyr\n1\nyr\nl\nyl\nl+1 yr\nn\n. . .\n. . .\nAs shown by Delpeuch the embedding MC(G) →A(MC(G)) is full on the sub-\ncategory of morphisms g : x →y where x, y ∈(B + V )∗.\nGiven any pregroup\nreduction g : u →s in RC( ˜G) we may apply the functor F to get a morphism\nF(g) : F(u) →F(s). By deﬁnition of F, we have that F(u) = u and F(s) = s are\nboth elements of (B + V )∗. By fullness of the embedding, all the cups and caps in\nF(g) can be removed using the snake equation, i.e. F(g) ∈MC(G). We give an\nexample to illustrate this:\nCaesar\nRubicon\ncrossed\nthe\nn\nnr\nnl\nn\nd\ndr\ns\n7→\nCaesar\nRubicon\ncrossed\nthe\n=\nCaesar\nRubicon\ncrossed\nthe\ns\nn\nd\nn\nIt is easy to see that the induced monoidal diagram has the same connectivity as the\npregroup reduction. Since it is a monoidal diagram it must be acyclic, and since all\nthe boxes have only one ouput it must be a tree, ﬁnishing the proof.\nProposition 1.5.22. Every dependency grammar is struturally equivalent to an op-\neradic pregroup.\nProof. We need to show that given a dependency grammar G and a string of words\nu ∈V ∗, dependency relations for u are in one-to-one correspondence with reductions\nu →s for the operadic pregroup ˜G = (V, B, ∆(G), I(G), s). The proof of Proposition\n1.5.17, gives an injection of dependency relations for u to pregroup reductions u →s\nfor ˜G. For the opposite direction note that, by the Lambek switching lemma 1.5.9,\nany pregroup reduction u →s can be obtained using lexical entries followed by a\ndiagram made only of cups (contractions). This deﬁnes a relation on u satisfying\nconditions (1) and (3-6) of dependency relations, see [Pre07]. It remains to show that\nalso condition (2) is satisﬁed, i.e. that reductions in an operadic pregroup are acyclic,\nbut this follows from Proposition 1.5.21.\n74\nPregroups and dependencies\nSection 1.5\nTheorem 1.5.23. Every dependency grammar is structurally equivalent to both a\npregroup and a context-free grammar.\nProof. Follows from Propositions 1.5.22 and 1.5.21.\nOverall, we have three equivalent ways of looking at the structures induced by\ndependency grammars a.k.a operadic pregroups. First, we may see them as depen-\ndency relations as ﬁrst deﬁned by Gaifman [Gai65] and reviewed above. Second, we\nmay see them as pregroup reductions (i.e. patterns of cups) as proven in Proposition\n1.5.17. Third, we may see them as trees as shown in Proposition 1.5.21.\nOn the one hand, this new algebraic perspective will allow us to give functorial\nsemantics to dependency grammars.\nIn 2.5 we interpret them in rigid categories\nusing their characterization as pregroup grammars. In 3.4.3, we interpret them in\na category of probabilistic processes (where cups and caps are not allowed) using\nthe characterization of dependency relations as trees. On the other, it allows us to\ninterface DisCoPy with established dependency parsers such as those provided by\nSpaCy [Hon+20].\n1.5.3\nrigid.Diagram\nThe rigid module of DisCoPy is often used as a suitable intermediate step between\nany grammar and tensor-based semantics. For example, the lambeq package [Kar+21]\nprovides a method for generating instances of rigid.Diagram from strings parsed\nusing a transformer-based CCG parser [clark2021]. We describe the implementation\nof the rigid module and construct an interface with SpaCy’s dependency parser\n[Hon+20]. A rigid.Ob, or basic type, is deﬁned by a name and a winding number\nz. It comes with property methods .l and .r for taking left and right adjoints by\nacting on the winding integer z.\nListing 1.5.24. Basic types and their iterated adjoints.\nclass Ob(cat.Ob):\n@property\ndef z(self):\n\"\"\" Winding number \"\"\"\nreturn self._z\n@property\ndef l(self):\n\"\"\" Left adjoint \"\"\"\nreturn Ob(self.name, self.z - 1)\n@property\ndef r(self):\n\"\"\" Right adjoint \"\"\"\nreturn Ob(self.name, self.z + 1)\ndef __init__(self, name, z=0):\nself._z = z\nsuper().__init__(name)\n75\nChapter 1\nDiagrams for Syntax\nTypes in rigid categories also come with a monoidal product, We implement them\nby subclassing monoidal.Ty and providing the deﬁning methods of rigid.Ob, note\nthat taking adjoints reverses the order of objects.\nListing 1.5.25. Pregroup types, i.e. types in free rigid categories.\nclass Ty(monoidal.Ty, Ob):\n@property\ndef l(self):\nreturn Ty(*[x.l for x in self.objects[::-1]])\n@property\ndef r(self):\nreturn Ty(*[x.r for x in self.objects[::-1]])\n@property\ndef z(self):\nreturn self[0].z\ndef __init__(self, *t):\nt = [x if isinstance(x, Ob)\nelse Ob(x.name) if isinstance(x, cat.Ob)\nelse Ob(x) for x in t]\nmonoidal.Ty.__init__(self, *t)\nOb.__init__(self, str(self))\nRigid diagrams are monoidal diagrams with special morphisms called Cup and Cap,\nsatisfying the snake equations (1.18). The requirement that the axioms are satisﬁed\nis relaxed to the availability of a polynomial time algorithm for checking equality of\nmorphisms. This is implemented in DisCoPy, with the rigid.Diagram.normal_fom\nmethod, following the algorithm of Dunn and Vicary [DV19b, Deﬁnition 2.12]. Rigid\ndiagrams are also biclosed, i.e. they can be curryed and uncurryed.\nListing 1.5.26. Diagrams in free rigid categories.\n@monoidal.Diagram.subclass\nclass Diagram(monoidal.Diagram):\n@staticmethod\ndef cups(left, right):\nreturn cups(left, right)\n@staticmethod\ndef caps(left, right):\nreturn caps(left, right)\n@staticmethod\ndef curry(self, n_wires=1, left=False):\nreturn curry(self, n_wires=n_wires, left=left)\n@staticmethod\ndef uncurry(self, n_wires=1, left=False):\nreturn uncurry(self, n_wires=n_wires, left=left)\n76\nPregroups and dependencies\nSection 1.5\ndef normal_form(self, normalizer=None, **params):\n...\nclass Box(monoidal.Box, Diagram):\n...\nclass Id(monoidal.Id, Diagram):\n...\nNote that currying and uncurrying correspond to transposition of wires in the rigid\nsetting. The class comes with its own Box instance which carry a winding number _z\nfor their transpositions. The currying and uncurrying functions are deﬁned as follows.\nfrom discopy.rigid import Id, Ty, Box, Diagram\ndef curry(diagram, n_wires=1, left=False):\nif not n_wires > 0:\nreturn diagram\nif left:\nwires = diagram.dom[:n_wires]\nreturn Diagram.caps(wires.r, wires) @ Id(diagram.dom[n_wires:])\\\n>> Id(wires.r) @ diagram\nwires = diagram.dom[-n_wires:]\nreturn Id(diagram.dom[:-n_wires]) @ Diagram.caps(wires, wires.l)\\\n>> diagram @ Id(wires.l)\ndef uncurry(diagram, n_wires=1, left=False):\nif not n_wires > 0:\nreturn diagram\nif left:\nwires = diagram.cod[:n_wires]\nreturn Id(wires.l) @ diagram\\\n>> Diagram.cups(wires.l, wires) @ Id(diagram.cod[n_wires:])\nwires = diagram.cod[-n_wires:]\nreturn diagram @ Id(wires.r)\\\n>> Id(diagram.cod[:-n_wires]) @ Diagram.cups(wires, wires.r)\nWe only showed the main methods available with rigid diagrams. The DisCoPy\nimplementation also comes with classes Cup and Cap for representing the structural\nmorphisms.\nThis allows to deﬁne rigid.Functor as a monoidal Functor with a\npredeﬁned mapping on instances of Cup and Cap.\nListing 1.5.27. Functors from free rigid categories.\nclass Functor(monoidal.Functor):\ndef __init__(self, ob, ar, cod=(Ty, Diagram)):\nsuper().__init__(ob, ar, cod=cod)\ndef __call__(self, diagram):\nif isinstance(diagram, Ty):\n...\n77\nChapter 1\nDiagrams for Syntax\nif isinstance(diagram, Cup):\nreturn self.cod[1].cups(\nself(diagram.dom[:1]), self(diagram.dom[1:]))\nif isinstance(diagram, Cap):\nreturn self.cod[1].caps(\nself(diagram.cod[:1]), self(diagram.cod[1:]))\nif isinstance(diagram, Box):\n...\nif isinstance(diagram, monoidal.Diagram):\nreturn super().__call__(diagram)\nraise TypeError()\nWe build an interface with the dependency parser of SpaCy [Hon+20]. From a\nSpaCy dependency parse we may obtain both an operad.Tree and a rigid.Diagram.\nListing 1.5.28. Interface between spacy and operad.Tree\nfrom discopy import operad\ndef find_root(doc):\nfor word in doc:\nif word.dep_ == ’ROOT’:\nreturn word\ndef doc2tree(word):\nif not word.children:\nreturn operad.Box(word.text, operad.Ob(word.dep_), [])\nroot = operad.Box(word.text, operad.Ob(word.dep_),\n[operad.Ob(child.dep_) for child in word.children])\nreturn root(*[doc2tree(child) for child in word.children])\ndef from_spacy(doc):\nroot = find_root(doc)\nreturn doc2tree(root)\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Moses crossed the Red Sea\")\nassert str(from_spacy(doc)) = ’crossed(Moses, Sea(the, Red))’\nListing 1.5.29. Interface between spacy and rigid.Diagram\nfrom discopy.rigid import Ty, Id, Box, Diagram, Functor\ndef doc2rigid(word):\nchildren = word.children\nif not children:\nreturn Box(word.text, Ty(word.dep_), Ty())\nleft = Ty(*[child.dep_ for child in word.lefts])\nright = Ty(*[child.dep_ for child in word.rights])\nbox = Box(word.text, left.l @ Ty(word.dep_) @ right.r, Ty(),\ndata=[left, Ty(word.dep_), right])\ntop = curry(curry(box, n_wires=len(left), left=True), n_wires=len(right))\n78\nPregroups and dependencies\nSection 1.5\nbot = Id(Ty()).tensor(*[doc2rigid(child) for child in children])\nreturn top >> bot\ndef doc2pregroup(doc):\nroot = find_root(doc)\nreturn doc2rigid(root)\nWe can now build pregroup reductions from dependency parses. We check that the\noutputs of the two interfaces are functorially equivalent.\ndef rewiring(box):\nif not box.data:\nreturn Box(box.name, box.dom, Ty())\nleft, middle, right = box.data[0], box.data[1], box.data[2]\nnew_box = Box(box.name, middle, left @ right)\nreturn uncurry(uncurry(new_box, len(left), left=True), len(right))\nF = Functor(ob=lambda ob: Ty(ob.name), ar=rewiring)\nassert repr(F(doc2pregroup(doc)).normal_form()) ==\\\nrepr(operad.tree2diagram(from_spacy(doc)))\ndrawing.equation(dep2pregroup(doc).normal_form(left=True),\noperad.tree2diagram(from_spacy(doc)), symbol=’->’)\n79\nChapter 1\nDiagrams for Syntax\n1.6\nHypergraphs and coreference\nCoreference resolution is the task of ﬁnding all linguistic expressions, called mentions,\nwhich refer to the same entity in a piece of text. It has been a core research topic\nin NLP [Ela06], including early syntax-based models of pronoun resolution [Hob78],\nBayesian and statistical approaches [GHC98; CM15] as well as neural-based models\n[CM16; Lee+17]. This is still a very active area of research with new state-of-the-\nart models released every year, and several open-source tools available in the web\n[Qi+20].\nIn the previous sections, we studied a range of formal grammars that capture\nthe syntactic structure of sentences. The aim of this section is to cross the sentence\nboundary and move towards an analysis of text and discourse. Assuming that the\nresolution process has been completed, we want a suitable syntactic representation of\ntext with coreference. We can obtain it using a piece of structure known as a com-\nmutative special Frobenius algebra, or more simply a “spider”. These were introduced\nin linguistics by Sadrzadeh et al. [SCC13; SCC14] as a model for relative pronouns,\nand have recently been used by Coecke [Coe20] to model the interaction of sentences\nwithin text.\nIn this section, we introduce pregroup grammars with coreference, a syntactic\nmodel which allows to represent the grammatical and referential structure of text di-\nagrammatically. This is similar in spirit to the work of Coecke [Coe20], although our\napproach preserves the pregroup formalism and adds coreference as extra structure.\nThis makes our model suited for implementation since one can ﬁrst parse sentences\nwith a pregroup grammar and then link the entities together using coreference resolu-\ntion tools. We show a proof-of-concept implementation using the hypergraph module\nof DisCoPy.\n1.6.1\nHypergraph categories\nThe term “hypergraph categories” was introduced in 2018 by Fong and Spivak [FS18b],\nto refer to categories equipped with Frobenius algebras on every object. These struc-\ntures were studied at least since Carboni and Walters [CW87] and have been applied\nto such diverse ﬁelds as databases [BSS18], control theory [BE14], quantum com-\nputing [CK17] and linguistics [SCC14]. In a recent line of work [Bon+16; Bon+20;\nBon+21], Bonchi, Sobocinski et al. developed a rewrite theory for morphisms in these\ncategories in terms of double-pushout hypergraph rewriting [Bon+20]. This makes\napparent the combinatorial nature of hypergraph categories, making them particu-\nlarly suited to implementation [WZ21]. We will not be interested here in the rewrite\ntheory for these categories, but rather in their power in representing the grammatical\nand coreferential structure of language. They will also provide us with an intermediate\nstep between syntax and the semantics of Sections 2.4 and 2.5.\nDeﬁnition 1.6.1 (Hypergraph category). [FS18b] A hypergraph category is a sym-\nmetric monoidal category such that each object a is equipped with a commutative\n80\nHypergraphs and coreference\nSection 1.6\nspecial Frobenius algebra Froba = {∆a, ϵa, ∇a, ηa} satisfying the following axioms:\n=\n=\n=\n=\n=\n=\n=\n;\n;\n;\n;\n;\n;\n.\n;\n=\n(1.22)\nWhere the unlabeled wire denotes object a.\nProposition 1.6.2. Hypergraph categories are self-dual compact-closed with cups and\ncaps given by:\n:=\n:=\n;\n.\n(1.23)\nThe axioms of commutative special Frobenius algebras (1.22), may be expressed\nin a more intuitive way as fusion rules of spiders. Spiders are deﬁned as follows:\n:=\n. . .\n. . .\n. . .\n. . .\n(1.24)\nThey are the normal form of commutative special Frobenius algebras. More precisely,\nusing the axioms (1.22), it can be shown that any connected diagram built using the\nFrobenius generators Froba can be rewritten into the right-hand side above. This\nwas shown in the context of categorical quantum mechanics [HV19] where Frobenius\nalgebras, corresponding to “observables”, play a foundational role.\nThe following\nresult is also proved in [HV19], and used in the context of the ZX calculus [van20],\nit provides a more concise and intuitive way of reasoning with commutative special\nFrobenius algebras.\nProposition 1.6.3 (Spider fusion). [HV19] The axioms of special commutative Frobe-\n81\nChapter 1\nDiagrams for Syntax\nnius algebras (1.22), are equivalent to the spider fusion rules:\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\na\nb\nc\nd\nc + d\na + b\n=\n(1.25)\nGiven a monoidal signature Σ, the free hypergraph category is deﬁned by:\nHyp(Σ) = MC(Σ + { Froba }a∈Σ0)/ ∼=\nwhere MC is the free monoidal category construction, deﬁned in 1.3 and ∼= is the\nequivalence relation generated by the axioms of commutative special Frobenius alge-\nbras (1.22), or equivalently the spider fusion rules (1.25). Without loss of generality,\nwe may assume that the monoidal signature Σ has trivial dom function. Formally we\nhave that for any monoidal signature B∗\ndom\n←−−Σ\ncod\n−−→B∗there is a monoidal signature\nof the form Σ′\ncod\n−−→B∗such that Hyp(Σ) ≃Hyp(Σ′). The signature Σ′ is deﬁned\nby taking the name of every generator in Σ, i.e. turning all the inputs into outputs\nusing caps as follows:\nf\n. . .\n. . .\nf\n. . .\n7→\n. . .\nak\na0 b0\nbn\nb0\nbn\na0\nak\nWe deﬁne a hypergraph signature as a monoidal signature where the generators have\nonly output types.\nDeﬁnition 1.6.4 (Hypergraph signature). A hypergraph signature Σ over B is a set\nof hyperedge symbols Σ together with a map σ : Σ →B∗.\nMorphisms in Hyp(Σ) for a hypergraph signature Σ have a normal form given as\nfollows.\nProposition 1.6.5 (Hypergraph normal form). Let Σ →B∗be a hypergraph signa-\nture and x, y ∈B∗. Any morphism d : x →y ∈Hyp(Σ) is equal to a diagram of the\nfollowing shape:\nb0\nb1\nbk\nH(d)\n. . .\n. . .\n. . .\nx0\nxn\ny0\nym\n82\nHypergraphs and coreference\nSection 1.6\nwhere H(d) ∈Hyp(∅) is a morphism built using only spiders, i.e. generated from\n∪bFrobb for b ∈B.\nThis normal form justiﬁes the name “hypergraph” for these categories. Indeed\nwe may think of the spiders of a diagram d as vertices, and boxes with n ports as\nhyperedges with n vertices. Then the morphism H(d) deﬁned above is the incidence\ngraph of d, indicating for each vertex (spider) the hyperedges (boxes) that contain\nit. Note however, that morphisms in Hyp(Σ) carry more data than a simple hyper-\ngraph. First, they carry labels for every hyperedge and every vertex. Second they\nare “open”, i.e. they carry input-output information allowing to compose them. If\nwe consider morphisms d : 1 →1 in Hyp(Σ), these are in one-to-one correspondence\nwith hypergraphs labeled by Σ, as exempliﬁed in the following picture:\nA\nB\nC\n⇐⇒\nA\nB\nC\n(1.26)\n1.6.2\nPregroups with coreference\nStarting from a pregroup grammar G = (V, B, ∆, I, s), we can model coreference\nby adding memory or reference types for each lexical entry, and rules that allow\nto swap, merge or discard these reference types. Formally, this is done by ﬁxing a\nset of reference types R and allowing the lexicon to assign reference types alongside\npregroup types:\n∆⊆V × P(B) × R∗\nWe can then represent the derivations for such a grammar with coreference in one\ncategory Coref(∆, I) deﬁned by:\nCoref(∆, I) = RC(∆+ I + { Frobr }r∈R , { swapr,x }r∈R , x∈P(B)+R)\nwhere Frobr contains the generators of Frobenius algebras for every reference type\nr ∈R (allowing to initialise, merge, copy and delete them), swapr,x allows to swap\nany reference type r ∈R to the right of any other type x ∈P(B) + R, i.e. reference\ntypes commute with any other object. Note that we are not quotienting Coref by\nany axioms since we use it only to represent the derivations.\nDeﬁnition 1.6.6 (Pregroup with coreference). A pregroup grammar with coreference\nis a tuple G = (V, B, R, I, ∆, s) where V is a vocabulary, B ∋s is a set of basic types,\nR is a set of reference types, I ⊆B×B is a set of induced steps, ∆⊆V ×P(B)×R∗is\na lexicon, assigning to every word w ∈V a set of types ∆(w) consisting of a pregroup\ntype in P(B) and a list of reference type in R∗. An utterance u = w0 . . . wn ∈V ∗is\nk-grammatical in G if there are types (ti, ri) ∈∆(wi) ⊆P(B) × R∗such that there\nis morphism t0 ⊗r0 ⊗t1 ⊗r1 . . . tn ⊗rn →sk in Coref(∆, I) =: Coref(G) for some\nk ∈N.\n83\nChapter 1\nDiagrams for Syntax\nExample 1.6.7. We denote reference types with red wires. The following is a valid\nreduction in a pregroup grammar with coreference, obtained from Example 1.5.6 by\nadding reference types for the words “A”, “whose” and “their”.\nA\npair\nof\nstarcross lovers take their lives\nwhose\nmisadventured\npiteousoverthrowsdoth\nwith their death\nbury theirparent’sstrife\nWe can now consider the problem of parsing a pregroup grammar with coreference.\nDeﬁnition 1.6.8. CorefParsing\nInput:\nG, u ∈V ∗, k ∈N\nOutput:\nf ∈Coref(G)(u, sk)\nNote that, seen as a decision problem, CorefParsing is equivalent to the parsing\nproblem for pregroups.\nProposition 1.6.9. The problem ∃CorefParsing is equivalent to the parsing problem\nfor pregroup grammars.\nProof. First note that for any pregroup grammar with coreference G = (V, B, R, I, ∆, s)\nthere is aa corresponding pregroup grammar ˜G = (V, B, I, ˜∆, sk) where ˜∆is obtained\nfrom ∆by projecting out the R∗component. Suppose there is a reduction d : u →sk\nin Coref(G), since there are no boxes making pregroup and reference types interact,\nwe can split the diagram d into a pregroup reduction in RC(G) and a coreference\nresolution Rn →1 where n is th number of reference types used. Therefore u is also\ngrammatical in ˜G. Now, ˜G reduces functorially to G since we can map the lexical en-\ntries in ˜∆to the corresponding entry in ∆followed by discarding the reference types\nwith counit spiders. Therefore any parsing for ˜G induces a parsing for Coref(G),\nﬁnishing the proof.\nSupposing we can parse pregroups eﬃciently, CorefParsing becomes only inter-\nesting as a function problem: which coreference resolution should we choose for a given\ninput utterance u? There is no deﬁnite mathematical answer to this question. De-\npending on the context, the same utterance may be resolved in diﬀerent ways. Promi-\nnent current approaches are neural-based such as the mention-ranking coreference\nmodels of Clark and Manning [clark2016a]. SpaCy oﬀers a neuralcoref package\nimplementing their model and we show how to interface it with the rigid.Diagram\n84\nHypergraphs and coreference\nSection 1.6\nclass at the end of this section. Being able to represent the coreference resolution\nin the diagram for a piece of text is particularly useful in semantics, for example it\nallows to build arbitrary conjunctive queries as one-dimensional strings of words, see\n2.4.\nThe image of the parsing function for pregroups with coreference may indeed be\narbitrarily omplicated, in the sense that any hypergraph d ∈Hyp(Σ) can be obtained\nas a k-grammatical reduction, where the hyperedges in Σ are seen as verbs and the\nvertices in B as nouns.\nProposition 1.6.10. For any ﬁnite hypergraph signature Σ\nσ−→B∗, there is a pregroup\ngrammar with coreference G = (V, B, R, ∆(σ), s) and a full functor J : Coref(∆(σ)) →\nHyp(Σ) with J(w) = J(s) = 1 for w ∈V .\nProof. We deﬁne the vocabulary V = Σ + B where the elements of Σ are seen as\nverbs and the elements of B as nouns. We also set the basic types to be B + { s }\nand the reference types to be R = B. The lexicon ∆(σ) ⊆V × P(B + { s }) × R∗is\ndeﬁned by:\n∆(σ) = { (w, s σ(w)l, ϵ) | w ∈Σ ⊆V } + { (w, w, w)|w ∈B ⊆V }\nwhere ϵ denotes the empty list. The functor J : Coref(∆(σ)) →Hyp(Σ) is given on\nobjects by J(w) = 1 for w ∈V , J(x) = x for x ∈B + R and J(s) = 1 and on the\nlexical entries by J(w) = w if w ∈Σ and J(w) = cupw if w ∈B.\nIn order to prove the proposition, we show that for any d : 1 →1 ∈Hyp(Σ) there\nis an utterance u ∈V ∗with a k-grammatical reduction g : u →sk in Coref(∆(σ))\nsuch that J(g) = d, where k is the number of boxes in d. Fix any diagram d : 1 →\n1 ∈Hyp(Σ). By proposition 1.6.5, it can be put in normal form:\nf1\nf2\nfk\nH(d)\n. . .\nWhere f1 . . . fk are the boxes in d and H(d) is a morphism built only from the Frobe-\nnius generators Frobb for b ∈B. Let σ(fi) = bi0 . . . bini, then for every i ∈{ 1, . . . k },\nwe may build a sentence as follows (leaving open the reference wires):\nfi\nfi\n. . .\nbi0 bini\n. . .\ns\nbi0 bini\nTensoring these sentences together and connecting them with coreference H(d) we\nget a morphism g : u →sk in Coref(∆(σ)) where u = f1σ(f1) . . . fkσ(fk), and it is\neasy to check that J(g) = d.\n85\nChapter 1\nDiagrams for Syntax\n1.6.3\nhypergraph.Diagram\nThe hypergraph module of DisCoPy, still under development, is an implementation of\ndiagrams in free hypergraph categories based on [Bon+20; Bon+21]. The main class\nhypergraph.Diagram is well documented and it comes with a method for composition\nimplemented using pushouts. We give a high-level overview of the datastrutures of\nthis module, with a proof-of-concept implementation of dependency grammars with\ncoreference. Before introducing diagrams, recall that the types of free hypergraph\ncategories are self-dual rigid types.\nclass Ty(rigid.Ty):\n@property\ndef l(self):\nreturn Ty(*self.objects[::-1])\nr = l\nWe store a hypergraph.Diagram via its incidence graph. This is given by a list\nof boxes and a list of integers wires of the same length as ports.\nport_types = list(map(Ty, self.dom)) + sum(\n[list(map(Ty, box.dom @ box.cod)) for box in boxes], [])\\\n+ list(map(Ty, self.cod))\nNote that port_types are the concatenation of the domain of the diagram, the\npairs of domains and codomains of each box, and the codomain od the diagram. We\nsee that wires deﬁnes a mapping from ports to spiders, which we store as a list of\nspider_types.\nspider_types = {}\nfor spider, typ in zip(wires, port_types):\nif spider in spider_types:\nif spider_types[spider] != typ:\nraise AxiomError\nelse:\nspider_types[spider] = typ\nspider_types = [spider_types[i] for i in sorted(spider_types)]\nThus a hypergraph.Diagram is initialised by a domain dom, a codomain cod, a\nlist of boxes boxes and a list of wires wires as characterised above. The __init__\nmethod automatically computes the spider_types, and in doing so checks that the\ndiagram is well-typed.\nListing 1.6.11. Diagrams in free hypergraph categories\nclass Diagram(cat.Arrow):\ndef __init__(self, dom, cod, boxes, wires, spider_types=None):\n...\ndef then(self, other):\n...\ndef tensor(self, other=None, *rest):\n...\n86\nHypergraphs and coreference\nSection 1.6\n__matmul__ = tensor\n@property\ndef is_monogamous(self):\n...\n@property\ndef is_bijective(self):\n...\n@property\ndef is_progressive(self):\n...\ndef downgrade(self):\n...\n@staticmethod\ndef upgrade(old):\nreturn rigid.Functor(\nob=lambda typ: Ty(typ[0]),\nar=lambda box: Box(box.name, box.dom, box.cod),\nob_factory=Ty, ar_factory=Diagram)(old)\ndef draw(self, seed=None, k=.25, path=None):\n...\nThe current draw method is based on a random spring layout algorithm of the\nhypergraph.\nOne may check whether a diagram is symmetric, compact-closed or\ntraced, using methods is_progressive, is_bijective and is_monogamous respec-\ntively. The downgrade method turns any hypergraph.Diagram into a rigid.Diagram.\nThis is by no means an optimal algorithm. There are indeed many ways to tackle the\nproblem of extraction of rigid diagrams from hypergraph diagrams.\nThe classes hypergraph.Box and hypergraph.Id are deﬁned as usual, by suclass-\ning the corresponding rigid classes and Diagram. Two special types of morphisms,\nSpider and Swap, can be deﬁned directly as subclasses of Diagram.\nListing 1.6.12. Swaps and spiders in free hypergraph categories\nclass Swap(Diagram):\n\"\"\" Swap diagram. \"\"\"\ndef __init__(self, left, right):\ndom, cod = left @ right, right @ left\nboxes, wires = [], list(range(len(dom)))\\\n+ list(range(len(left), len(dom))) + list(range(len(left)))\nsuper().__init__(dom, cod, boxes, wires)\nclass Spider(Diagram):\n\"\"\" Spider diagram. \"\"\"\ndef __init__(self, n_legs_in, n_legs_out, typ):\n87\nChapter 1\nDiagrams for Syntax\ndom, cod = typ ** n_legs_in, typ ** n_legs_out\nboxes, spider_types = [], list(map(Ty, typ))\nwires = (n_legs_in + n_legs_out) * list(range(len(typ)))\nsuper().__init__(dom, cod, boxes, wires, spider_types)\nWe now show how to build hypergraph diagrams from SpaCy’s dependency parser\nand the coreference information provided by the package neuralcoref.\nListing 1.6.13. Coreference and hypergraph diagrams\nimport spacy\nimport neuralcoref\nnlp = spacy.load(’en’)\nneuralcoref.add_to_pipe(nlp)\ndoc1 = nlp(\"A pair of starcross lovers take their life\")\ndoc2 = nlp(\"whose misadventured piteous overthrows doth with \\\ntheir death bury their parent’s strife.\")\nWe use the interface with SpaCy from the operad module to extract dependency\nparses and a rigid.Functor with codomain hypergraph.Diagram to turn the depen-\ndency trees into hypergraphs.\nfrom discopy.operad import from_spacy, tree2diagram\nfrom discopy.hypergraph import Ty, Id, Box, Diagram\nfrom discopy import rigid\nF = rigid.Functor(ob=lambda typ: Ty(typ[0]),\nar=lambda box: Box(box.name, box.dom, box.cod),\nob_factory=Ty, ar_factory=Diagram)\ntext = F(tree2diagram(from_spacy(doc1, lexicalised=True)))\\\n@ F(tree2diagram(from_spacy(doc2, lexicalised=True)))\nassert text.is_monogamous\ntext.downgrade().draw(figsize=(10, 7))\nWe can now use composition in hypergraph to add coreference boxes that link\nleaves according to the coreference clusters.\nfrom discopy.rigid import Ob\nref = lambda x, y: Box(’Coref’, Ty(x, y), Ty(x))\ndef coref(diagram, word0, word1):\npos0 = diagram.cod.objects.index(word0)\npos1 = diagram.cod.objects.index(word1)\nswaps = Id(Ty(word0)) @ \\\nDiagram.swap(diagram.cod[pos0 + 1 or pos1:pos1], Ty(word1))\ncoreference = swaps >> \\\nref(word0, word1) @ Id(diagram.cod[pos0 + 1 or pos1:pos1])\nreturn diagram >> Id(diagram.cod[:pos0]) @ coreference @ \\\nId(diagram.cod[pos1 + 1 or len(diagram.cod):])\ndef resolve(diagram, clusters):\n88\nHypergraphs and coreference\nSection 1.6\ncoref_diagram = diagram\nfor cluster in clusters:\nmain = str(cluster.main)[0]\nfor mention in cluster.mentions[1:]:\ncoref_diagram = coref(coref_diagram, Ob(main), Ob(str(mention)))\nreturn coref_diagram\ndoc = nlp(\"A pair of starcross lovers take their life, whose misadventured \\\npiteous overthrows doth with their death bury their parent’s strife.\")\nclusters = doc._.coref_clusters\nresolve(text, clusters).downgrade().draw(figsize=(11, 9), draw_type_labels=False)\nNote that the only way we have so far of applying functors to a hypergraph.Diagram\nis by ﬁrst downgrading it to a rigid.Diagram. An important direction of future work\nis the implementation of double pushout rewriting for hypergraph diagrams [Bon+20;\nBon+21]. In particular, this would allow to compute free functors directly on the hy-\npergraph representation as they are special instances of rewrites.\n89\nChapter 1\nDiagrams for Syntax\n90\nChapter 2\nFunctors for Semantics\nThe modern word “semantics” emerged from the linguistic turn of the end of the 19th\ncentury along with Peirce’s “semiotics” and Saussure’s “semiology”. It was introduced\nas “sémantique” by the French linguist Michel Breal, and has its root in the greek word\nσηµ˘ασ´ι¯α (semasia) which translates to “meaning” or “signiﬁcation”. Semantics is the\nscientiﬁc study of language meaning. It thus presupposes a deﬁnition of language as\na system of signs, a deﬁnition of meaning as a mental or computational process, and\nan understanding of how signs are mapped onto their meaning. The deﬁnition and\nanalysis of these concepts is a problem which has motivated the work of linguists,\nlogicians and computer scientists throughout the 20th century.\nIn his work on “The Semantic Conception” [Tar36; Tar43], Alfred Tarski proposed\nto found the science of semantics on the concept of truth relative to a model. As\na mathematician, Tarski focused on the formal language of logic and identiﬁed the\nconditions under which a logical formula ϕ is true in a model K. The philosophi-\ncal intuition and mathematical tools developed by Tarski and his collaborators had\na great impact on linguistics and computer science. They form the basis of David-\nson’s [Dav67b; Dav67a] truth-conditional semantics, as well as Montague’s “Universal\nGrammar” [Mon70b; Mon70a; Mon73], which translates natural language sentences\ninto logical formulae. They are also at the heart of the development of relational\ndatabases in the 1970s [Cod70; CB76; CM77], where formulas are used as queries for\na structured storage of data. These approaches adhere to the principle of composition-\nality, which we may sum up in Frege’s words: “The possibility of our understanding\nsentences which we have never heard before rests evidently on this, that we can con-\nstruct the sense of a sentence out of individual parts which correspond to words”\n[Fre14]. In Tarski’s approach, compositionality manifests itself in the notion of satis-\nfaction for a formula ϕ in a model K, which is deﬁned by induction over the formal\ngrammar from which ϕ is constructed.\nFormulas\nModel\n−−−→Truth\nIn his 1963 thesis [Law63], Lawvere introduced the concept of functorial seman-\ntics as a foundation for universal algebra. The idea is to represent syntax as a free\ncategory with products and semantics as a structure-preserving functor computing\n91\nChapter 2\nFunctors for Semantics\nthe meaning of a compound algebraic expression from the semantics of its basic op-\nerations.\nThe functorial approach to semantics is naturally compositional, but it\ngeneralises Tarski’s set-theoretic approach by allowing the semantic category to rep-\nresent processes in diﬀerent models of computation. For example, taking semantics\nin the category of complex vector spaces and linear maps allows to build a struc-\ntural understanding of quantum information protocols [AC07]. The same principles\nare used in applications of category theory to probability [Fon13; CJ19], databases\n[Spi12], chemistry [BP17] and network theory [BCR18]. Of particular interest to us\nare the Categorical Compositional Distributional (DisCoCat) models of Coecke et al.\n[CCS08; CCS10] where functors are used to compute the semantics of natural lan-\nguage sentences from the distributional embeddings of their constituent words. As\nwe will see, functors are useful for both constructing new models of meaning and\nformalising already existing ones.\nSyntax\nFunctor\n−−−−→Semantics\nIn this chapter, we use functorial semantics to characterise the expressivity and\ncomplexity of a number of NLP models, including logical, tensor network, quantum\nand neural network models, while showing how they are implemented in DisCoPy.\nWe start in 2.1 by showing how to implement semantic categories in Python, while\nintroducing the two main concrete categories we are interested in, MatS and Set,\nimplemented respectively with the Python classes Tensor and Function. In 2.2, we\nshow that Montague semantics is captured by functors from a biclosed grammar to\nthe category of sets and functions Set, through a lambda calculus for manipulating\nﬁrst-order logic formulas. We then give an implementation of Montague semantics\nby deﬁning currying and uncurrying methods for Function. In 2.3, we show that re-\ncurrent and recursive neural network models are functors from regular and monoidal\ngrammars (respectively) to a category NN of neural network architectures. We illus-\ntrate this by building an interface between DisCoPy and Tensorﬂow/Keras [Cho+15].\nIn 2.4, we show that functors from pregroup grammars to the category of relations\nallow to translate natural language sentences into conjunctive queries for a relational\ndatabase. In 2.5, we formalise the relationship between DisCoCat and tensor net-\nworks and use it to derive complexity results for DisCoCat models. In 2.7, we study\nthe complexity of our recently proposed quantum models for NLP [Coe+20; Mei+20].\nWe show how tensor-based models are implemented in just a few lines of DisCoPy\ncode, and we use them to solve a knowledge embedding task 2.8.\n92\nConcrete categories in Python\nSection 2.1\n2.1\nConcrete categories in Python\nWe describe the implementation of the main semantic modules in DisCoPy: tensor\nand function. These consists in classes Tensor and Function whose methods carry\nout numerical computation. Diagram may then be evaluated using Functor. We may\nstart by considering the monoid, a set with a unit and a product, or equivalently a\ncategory with one object. We can implement it as a subclass of cat.Box by overriding\ninit, repr, then and id. In fact, it is suﬃcient to provide an additional tensor\nmethod and we can make Monoid a subclass of monoidal.Box. Both then and tensor\nare interpreted as multiplication, id as the unit.\nListing 2.1.1. Delooping of a monoid as monoidal.Box.\nfrom discopy import monoidal\nfrom discopy.monoidal import Ty\nfrom numpy import prod\nclass Monoid(monoidal.Box):\ndef __init__(self, m):\nself.m = m\nsuper().__init__(m, Ty(), Ty())\ndef __repr__(self):\nreturn \"Monoid({})\".format(self.m)\ndef then(self, other):\nif not isinstance(other, Monoid):\nraise ValueError\nreturn Monoid(self.m * other.m)\ndef tensor(self, other):\nreturn Monoid(self.m * other.m)\ndef __call__(self, *others):\nreturn Monoid(prod([self.m] + [other.m for other in others]))\n@staticmethod\ndef id(x):\nif x != Ty():\nraise ValueError\nreturn Monoid(1)\nassert Monoid(2) @ Monoid.id(Ty()) >> Monoid(5) @ Monoid(0.1) == Monoid(1.0)\nassert Monoid(2)(Monoid(1), Monoid(4)) == Monoid(8)\nRemark 2.1.2. We deﬁne semantic classes as subclasses of monoidal.Box in order\nfor them to inherit the usual DisCoPy syntax. This can be avoided by explicitly pro-\nviding the deﬁnitions __matmul__, right and left __shift__ as well as the dataclass\nmethods.\nA weighted context-free grammar (WCFG) is a CFG where every production rule\nis assigned a weight, scores are assigned to derivations by multiplying the weights\n93\nChapter 2\nFunctors for Semantics\nof each production rule appearing in the tree. This model is equally expressive as\nprobabilistic CFGs and has been applied to range of parsing and tagging tasks [SJ07].\nWCFGs are simply functors from Tree into the Monoid class! Thus we can deﬁne\nweighted grammars as a subclass of monoidal.Functor.\nListing 2.1.3. Weighted grammars as Functor.\nfrom discopy.monoidal import Functor, Box, Id\nclass WeightedGrammar(Functor):\ndef __init__(self, ar):\nob = lambda x: Ty()\nsuper().__init__(ob, ar, ar_factory=Monoid)\nweight = lambda box: Monoid(0.5)\\\nif (box.dom, box.cod) == (Ty(’N’), Ty(’A’, ’N’)) else Monoid(1.0)\nWCFG = WeightedGrammar(weight)\nA = Box(’A’, Ty(’N’), Ty(’A’, ’N’))\ntree = A >> Id(Ty(’A’)) @ A\nassert WCFG(tree) == Monoid(0.25)\nWe can now generate trees with NLTK and evaluate them in a weighted CFG.\nListing 2.1.4. Weighted context-free grammar.\nfrom discopy.operad import from_nltk, tree2diagram\nfrom nltk import CFG\nfrom nltk.parse import RecursiveDescentParser\ngrammar = CFG.fromstring(\"\"\"\nS -> VP NP\nNP -> D N\nVP -> N V\nN -> A N\nV -> ’crossed’\nD -> ’the’\nN -> ’Moses’\nA -> ’Red’\nN -> ’Sea’\"\"\")\nrd = RecursiveDescentParser(grammar)\nparse = next(rd.parse(’Moses crossed the Red Sea’.split()))\ndiagram = tree2diagram(from_nltk(parse))\nparse2 = next(rd.parse(’Moses crossed the Red Red Red Sea’.split()))\ndiagram2 = tree2diagram(from_nltk(parse2))\nassert WCFG(diagram).m > WCFG(diagram2).m\nFunctors into Monoid are degenerate examples of a larger class of models called\nTensor models. An instance of Monoid is in fact a Tensor with domain and codomain\nof dimension 1. In Tensor models of language, words and production rules are up-\ngraded from just carrying a weight to carrying a tensor. The tensors are multiplied\naccording to the structure of the diagram.\n94\nConcrete categories in Python\nSection 2.1\n2.1.1\nTensor\nTensors are multidimensional arrays of numbers that can be multiplied along their\nindices. The tensor module of DisCoPy comes with interfaces with numpy [Har+20],\ntensornetwork [Rob+19] and pytorch [Pas+19] for eﬃcient tensor contraction as\nwell as simpy [Meu+17] and jax [Bra+18] for computing gradients symbolically and\nnumerically. We describe the implementation of the semantic class Tensor. We give\na more in-depth example in 2.8, after covering the relevant theory in 2.4 and 2.5.\nA semiring is a set S equipped with two binary operations + and · called addition\nand multiplication, and two speciﬁed elements 0, 1 such that (S, +, 0) is a commutative\nmonoid, (S, ·, 1) is a monoid, the multiplication distributes over addition:\na · (b + c) = a · b + a · c\n(a + b) · c = a · c + b · c\nand multiplication by 0 annihilates: a · 0 = 0 = 0 · a for all a, b, c ∈S. We say that S\nis commutative when a · b = b · a for all a, b ∈S. Popular examples of semirings are\nthe booleans B, natural numbers N, positive reals R+, reals R and complex numbers\nC.\nThe axioms of a semiring are the minimal requirements to deﬁne matrix multipli-\ncation and thus a category MatS with objects natural numbers n, m ∈N and arrows\nn →m given by n × m matrices with entries in S. Composition is given by matrix\nmultiplication and identities by the identity matrix. For any commutative semiring S\nthe category MatS is monoidal with tensor product ⊗given by the kronecker product\nof matrices. Note that S must be commutative in order for MatS to be monoidal,\nsince otherwise the interchanger law wouldn’t hold. When S is non-commutative,\nMatS is a premonoidal category [PR97].\nExample 2.1.5. The category of ﬁnite sets and relations FRel is isomorphic to\nMatB. The category of ﬁnite dimensional real vector spaces and linear maps is iso-\nmorphic to MatR. The category of ﬁnite dimensional Hilbert spaces and linear maps\nis isomorphic to MatC.\nMatrices f : 1 →n0 ⊗· · · ⊗nk for objects ni ∈N are usually called tensors with\nk indices of dimensions ni. The word tensor emphasizes that this is a k dimensional\narray of numbers, while matrices are usually thought of as 2 dimensional. Thus MatS\ncan be thought of as a category of tensors with speciﬁed input and output dimensions.\nThis gives rise to more categorical structure. MatS forms a hypergraph category with\nFrobenius structure (µ, ν, δ, ϵ) given by the “generalised Kronecker delta” tensors,\ndeﬁned in Einstein’s notation by:\nµk\ni,j = δj,k\ni\n=\n(\n1\nif i = j = k\n0\notherwise\nνi = ϵi = 1\n(2.1)\nIn particular, MatS is compact closed with cups and caps given by µϵ and δν. The\ntranspose f ∗of a matrix f : n →m is obtained by pre and post composing with cups\nand caps. When the semiring S is involutive, MatS has moreover a dagger structure,\ni.e. an involutive identity on objects contravariant endofunctor (see the nlab). In the\n95\nChapter 2\nFunctors for Semantics\ncase when S = C this is given by taking the conjugate transpose, corresponding to\nthe dagger of quantum mechanics.\nThe class Tensor implements the category of matrices in numpy [Har+20], with\nmatrix multiplication as then and kronecker product as tensor.\nThe categorical\nstructure of MatS translates into methods of the class Tensor, as listed below. The\ntypes of the category of tensors are given by tuples of dimensions, each entry corre-\nsponding to a wire. We can implement them as a subclass of rigid.Ty by overriding\n__init__.\nListing 2.1.6. Dimensions, i.e. types of Tensors.\nclass Dim(Ty):\n@staticmethod\ndef upgrade(old):\nreturn Dim(*[x.name for x in old.objects])\ndef __init__(self, *dims):\ndims = map(lambda x: x if isinstance(x, monoidal.Ob) else Ob(x), dims)\ndims = list(filter(lambda x: x.name != 1, dims))\n# Dim(1) == Dim()\nfor dim in dims:\nif not isinstance(dim.name, int):\nraise TypeError(messages.type_err(int, dim.name))\nif dim.name < 1:\nraise ValueError\nsuper().__init__(*dims)\ndef __repr__(self):\nreturn \"Dim({})\".format(’, ’.join(map(repr, self)) or ’1’)\ndef __getitem__(self, key):\nif isinstance(key, slice):\nreturn super().__getitem__(key)\nreturn super().__getitem__(key).name\n@property\ndef l(self):\nreturn Dim(*self[::-1])\n@property\ndef r(self):\nreturn Dim(*self[::-1])\nA Tensor is initialised by domain and codomain Dim types and an array of shape\ndom @ cod. It comes with methods then, tensor for composing tensors in sequence\nand in parallel. These matrix operations can be performed using numpy, jax.numpy\n[Bra+18] or pytorch [Pas+19] as backend.\nListing 2.1.7. The category of tensors with Dim as objects.\nimport numpy as np\n96\nConcrete categories in Python\nSection 2.1\nclass Tensor(rigid.Box):\ndef __init__(self, dom, cod, array):\nself._array = Tensor.np.array(array).reshape(tuple(dom @ cod))\nsuper().__init__(\"Tensor\", dom, cod)\n@property\ndef array(self):\nreturn self._array\ndef then(self, *others):\nif self.cod != other.dom:\nraise AxiomError()\narray = Tensor.np.tensordot(self.array, other.array, len(self.cod))\\\nif self.array.shape and other.array.shape\\\nelse self.array * other.array\nreturn Tensor(self.dom, other.cod, array)\ndef tensor(self, others):\ndom, cod = self.dom @ other.dom, self.cod @ other.cod\narray = Tensor.np.tensordot(self.array, other.array, 0)\\\nif self.array.shape and other.array.shape\\\nelse self.array * other.array\nsource = range(len(dom @ cod))\ntarget = [\ni if i < len(self.dom) or i >= len(self.dom @ self.cod @ other.dom)\nelse i - len(self.cod) if i >= len(self.dom @ self.cod)\nelse i + len(other.dom) for i in source]\nreturn Tensor(dom, cod, Tensor.np.moveaxis(array, source, target))\ndef map(self, func):\nreturn Tensor(\nself.dom, self.cod, list(map(func, self.array.flatten())))\n@staticmethod\ndef id(dom=Dim(1)):\nfrom numpy import prod\nreturn Tensor(dom, dom, Tensor.np.eye(int(prod(dom))))\n@staticmethod\ndef cups(left, right):\n...\n@staticmethod\ndef caps(left, right):\n...\n@staticmethod\ndef swap(left, right):\narray = Tensor.id(left @ right).array\nsource = range(len(left @ right), 2 * len(left @ right))\ntarget = [i + len(right) if i < len(left @ right @ left)\nelse i - len(left) for i in source]\nreturn Tensor(left @ right, right @ left,\nTensor.np.moveaxis(array, source, target))\n97\nChapter 2\nFunctors for Semantics\nTensor.np = np\nThe compact-closed structure of the category of matrices is implemented via static\nmethods cups and caps and swap. We check the axioms of compact closed categories\n(1.5.12) on a Dim object.\nListing 2.1.8. Axioms of compact closed categories.\nfrom discopy import Dim, Tensor\nimport numpy as np\nx = Dim(3, 2)\ncup_r, cap_r = Tensor.cups(x, x.r), Tensor.caps(x.r, x)\ncup_l, cap_l = Tensor.cups(x.l, x), Tensor.caps(x, x.l)\nsnake_r = Tensor.id(x) @ cap_r >> cup_r @ Tensor.id(x)\nsnake_l =\ncap_l @ Tensor.id(x) >> Tensor.id(x) @ cup_l\nassert np.allclose(snake_l.array, Tensor.id(x).array, snake_r.array)\nswap = Tensor.swap(x, x)\nassert np.allclose((swap >> swap).array, Tensor.id(x @ x).array)\nassert np.allclose((swap @ Tensor.id(x) >> Tensor.id(x) @ swap).array,\nTensor.swap(x, x @ x).array)\nFunctors into Tensor allow to evaluate any DisCoPy diagram as a tensor network.\nThey are simply monoidal.Functors with codomain (Dim, Tensor), initialised by\na mapping ob from Ty to Dim and a mapping ar from monoidal.Box to Tensor. We\ncan use them to give an example of a DisCoCat model [CCS10] in DisCoPy, these are\nstudied in 2.4 and 2.5 and used for a concrete task in 2.8.\nListing 2.1.9. DisCoCat model as Functor.\nfrom lambeq import BobcatParser\nparser = BobcatParser()\ndiagram = parser.sentence2diagram(’Moses crossed the Red Sea’)\ndiagram.draw()\n98\nConcrete categories in Python\nSection 2.1\nfrom discopy.tensor import Dim, Tensor, Functor\nimport numpy as np\ndef box_to_array(box):\nif box.name ==’Moses’:\nreturn np.array([1, 0])\nif box.name == ’Sea’:\nreturn np.array([0, 1])\nif box.name == ’crossed’:\nreturn np.array([[0, 1], [1, 0]])\nif box.name in [’the’, ’Red’]:\nreturn np.eye(2)\nraise NotImplementedError()\nob = lambda x: 2 if x == n else 1\nF = Functor(ob, box_to_array)\nassert F(diagram).array == [1.0]\n2.1.2\nFunction\nFunction is the semantic class that characterises the models studied in 2.2 and 2.3.\nIt consists in an implementation of the category Set of sets and functions, or more\nprecisely, the category of Python functions on tuples with Ty as objects. We describe\nthe basic methods of Function, that arise from the cartesian structure of the cate-\ngory of functions. In 2.2, we also deﬁne curry and uncurry methods for Function,\naccounting for the biclosed structure of this category. Alexis Toumi [Tou22] gives\ndetailed implementation and examples for this class.\nSet is a monoidal category with the cartesian product × : Set × Set →Set as\ntensor. It is moreover symmetric, there is a natural transformation σA,B : A × B →\nB×A satisfying σB,A◦σA,B = idA×B and the axioms 1.16. Set is a cartesian category.\nDeﬁnition 2.1.10 (Cartesian category). A cartesian category C is a symmetric\nmonoidal category such that the product × satisﬁes the following properties:\n1. there are projections A\nπ1\n←−A × B\nπ2\n−→B for any A, B ∈C0,\n2. any pair of function f : C →A and g : C →B induces a unique function\n< f, g >: C →A × B with π1◦< f, g >= f and π2◦< f, g >= g.\nThe structure of the category of functions is orthogonal to the structure of tensors.\nThis may be stated formally as in the following proposition, shown in the context of\nthe foundations of quantum mechanics.\nProposition 2.1.11. [Abr12] A compact-closed category that is also cartesian is triv-\nial, i.e. there is at most one morphism between any two objects.\nThe main diﬀerence comes from the presence of the diagonal map copy in Set.\nThis is a useful piece of structure which exists in any cartesian category.\n99\nChapter 2\nFunctors for Semantics\nProposition 2.1.12 (Fox [Fox76]). In any cartesian category C there is a natural\ntransformation copyA : A →A ⊗A satisfying the following axioms:\n1. Commutative comonoid axioms:\n=\n=\n=\n=\n,\n.\n,\n(2.2)\n2. Naturality of copy:\nf\nf\nf\n=\n(2.3)\nThis proposition may be used to characterise the free cartesian category Cart(Σ)\nfrom a generating monoidal signature Σ as the free monoidal category with natural\ncomonoids on every object. These were ﬁrst studied by Lawvere [Law63] who used\nthem to deﬁne algebraic theories as functors.\nDeﬁnition 2.1.13 (Lawvere theory). A Lawvere theory with signature Σ is a product-\npreserving functor F : Cart(Σ) →Set.\nWe now show how to implement these concepts in Python. A Function is ini-\ntialised by a domain dom and a codomain cod together with a Python function inside\nwhich takes tuples of length len(dom) to tuples of length len(cod). The class comes\nwith methods id, then and tensor for identities, sequential and parallel composition\nof functions, as well as a __call__ method which accesses inside.\nListing 2.1.14. The category of Python functions on tuples.\nclass Function(monoidal.Box):\ndef __init__(self, inside, dom, cod):\nself.inside = inside\nname = \"Function({}, {}, {})\".format(inside, dom, cod)\nsuper().__init__(name, dom, cod)\ndef then(self, other):\ninside = lambda *xs: other(*tuple(self(*xs)))\nreturn Function(inside, self.dom, other.cod)\ndef tensor(self, other):\ndef inside(*xs):\nleft, right = xs[:len(self.dom)], xs[len(self.dom):]\nresult = tuple(self(*left)) + tuple(other(*right))\nreturn (result[0], ) if len(self.cod @ other.cod) == 1 else result\nreturn Function(inside, self.dom @ other.dom, self.cod @ other.cod)\ndef __call__(self, *xs): return self.inside(*xs)\n@staticmethod\ndef id(x):\nreturn Function(lambda *xs: xs, x, x)\n100\nConcrete categories in Python\nSection 2.1\n@staticmethod\ndef copy(x):\nreturn Function(lambda *xs: (*xs, *xs), x, x @ x)\n@staticmethod\ndef delete(x):\nreturn Function(lambda *xs: (), x, Ty())\n@staticmethod\ndef swap(x, y):\nreturn Function(lambda x0, y0: (y0, x0), x @ y, y @ x)\nWe can check the properties of diagonal maps and projections.\nListing 2.1.15. Axioms of cartesian categories.\nX = Ty(’X’)\ncopy = Function.copy(X)\ndelete = Function.delete(X)\nI = Function.id(X)\nswap = Function.swap(X, X)\nassert (copy >> copy @ I)(54) == (copy >> I @ copy)(54)\nassert (copy >> delete @ I)(46) == (copy >> I @ delete)(46)\nassert (copy >> swap)(’was my number’) == (copy)(’was my number’)\nf = Function(lambda x: (46,) if x == 54 else (54,), X, X)\nassert (f >> copy)(54) == (copy >> f @ f)(54)\nassert (copy @ copy >> I @ swap @ I)(54, 46) == Function.copy(X @ X)(54, 46)\nThis is all we need in order to interpret diagrams as functions! Indeed, it is suﬃ-\ncient to use an instance of monoidal.Functor, with codomain cod = (Ty, Function).\nWe generate a diagram using the interface with SpaCy and we evaluate its semantics\nwith a Functor.\nListing 2.1.16. Lawvere theory as a Functor\nfrom discopy.operad import from_spacy, tree2diagram\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Fifty four was my number\")\ndiagram = tree2diagram(from_spacy(doc), contravariant=True)\ndiagram.draw()\n101\nChapter 2\nFunctors for Semantics\nfrom discopy.monoidal import Ty, Id, Box, Functor\nX = Ty(’X’)\nob = lambda x: X\ndef box_to_function(box):\nif box.name == ’was’:\nreturn Function(lambda x, y: (x == y, ), X @ X, X)\nif box.name == ’number’:\nreturn Function.id(X)\nif box.name == ’four’:\nreturn Function(lambda x: (4 + x, ), X, X)\nif box.name == ’Fifty’:\nreturn Function(lambda: (50, ), Ty(), X)\nif box.name == ’my’:\nreturn Function(lambda: (54, ), Ty(), X)\nraise NotImplementedError()\nF = Functor(ob, box_to_function, ob_factory=Ty, ar_factory=Function)\nassert F(diagram)() == (True,)\n102\nMontague models\nSection 2.2\n2.2\nMontague models\nMontague’s work appeared in three papers in the 1970s [Mon70b; Mon70a; Mon73].\nIn the ﬁrst, he characterises his endeavour in formulating a mathematically precise\ntheory of natural language semantics: “The basic aim of semantics is to character-\nize the notion of a true sentence (under a given interpretation) and of entailment”\n[Mon70b].\nOn the syntactic side, Montague’s grammar can be seen as an instance of the\ncategorial grammars studied in 1.4, see e.g.\n[MR12a].\nOn the semantic side, he\nused a blend of lambda calculus and modal logic allowing him to combine the logical\nmeaning of individual words into the meaning of a sentence.\nThis work had an\nimmediate inﬂuence on philosophy and linguistics [Par76; HL79; JZ21]. It motivated\nmuch of the work on combinatory categorial grammars (see 1.4.4) and has been used\nin the implementation of semantic parsers turning natural language into database\nqueries [KM14; AFZ14]. From the point of view of large-scale NLP, Montague models\nsuﬀer from great complexity issues and making them practical comes at the cost of\nrestricting the possibility of (exact) logical reasoning.\nIt was ﬁrst proposed by Lambek [Lam88; Lam99b], that Montague semantics\nshould be seen as a functor from categorial grammars to a cartesian closed category.\nIn this section, we make this idea precise, focusing on the ﬁrst-order logic aspects\nof Montague’s translation and leaving the intensional and modal aspects for future\nwork. We formulate Montague semantics as a pair of functors:\nG −→CCC(ΓΣ) −→Set\nwhere G is a categorial grammar, CCC(ΓΣ) is a lambda calculus for typed ﬁrst-order-\nlogic and Set is the category of sets and functions. This factorization allows to distin-\nguish between the syntactic translation from sentences to formulae G →CCC(ΓΣ)\nand the evaluation of the resulting formulae in a concrete model CCC(ΓΣ) →Set.\nWe start in 2.2.1 by introducing the lambda calculus and reviewing its relation-\nship with cartesian closed categories. In 2.2.2, we deﬁne a typed lambda calculus\nCCC(ΓΣ) for manipulating ﬁrst-order logic formulae, and in 2.2.3 we deﬁne Mon-\ntague models and discuss their complexity. Finally, in 2.2.4 we show how to imple-\nment Montague semantics in DisCoPy, by deﬁning curry and uncurry methods for\nthe Function class introduced in 2.1.\n2.2.1\nLambda calculus\nThe lambda calculus was introduced in the 1930s by Alonzo Church as part of his\nresearch into the foundations of mathematics [Chu32; Chu36]. It is a model of com-\nputation which can be used to simulate any Turing machine [Tur37]. Church also\nintroduced a simply typed version in [Chu40] which yields a weaker model of com-\nputation but allows to avoid the inﬁnite recursions of the untyped calculus. There\nis a well known correspondence — due to Curry and Howard — between the typed\nlambda calculus and intuitionistic logic, where types are seen as formulae and terms\n103\nChapter 2\nFunctors for Semantics\nas proofs. This correspondence was later extended by Lambek [LS86] who showed\nthat the typed lambda calculus has a clear characterisation as the equational theory\nof cartesian closed categories, viewing programs (or proofs) as morphisms.\nThe rules of the lambda calculus emerge naturally from the structure of the cate-\ngory of sets and functions Set. We have seen in 2.1 that Set is a monoidal category\nwith the cartesian product × : Set × Set →Set. Moreover, for any pair of sets A\nand B, the hom-set Set(A, B) is itself a set, denoted BA or A →B. In fact, there\nis a functor −B : Set →Set taking a set A to AB and a function f : A →C to a\nfunction f B : AB →CB given by f B(g) = f ◦g for any g ∈AB. This functor is the\nright adjoint of the cartesian product, i.e. there is a natural isomorphism:\nSet(A × B, C) ≃Set(A, CB)\nThis is holds in any cartesian closed category.\nDeﬁnition 2.2.1 (Cartesian closed category). A cartesian closed category C is carte-\nsian category equipped with a functor −A for any object A ∈C0 which is the right\nadjoint of the cartesian product A × −⊣−A. Explicitly, there is a natural isomor-\nphism:\nC(A × B, C) ≃C(A, CB)\n(2.4)\nProposition 2.2.2. A cartesian closed category is a cartesian category (Deﬁnition\n2.1.10) which is also biclosed (Deﬁnition 1.4.2).\nRemark 2.2.3. The categorial biclosed grammars studied in 1.4 map canonically into\nthe lambda calculus, as we will see in 2.2.3.\nA functional signature is a set Γ together with a function ty : Γ →TY (B) into\nthe set of functional types deﬁned inductively by:\nTY (B) ∋T, U = b ∈B | T ⊗U | T →U .\nwe write x : T whenever ty(x) = T for x ∈Γ. Given a functional signature Γ, we can\nconsider the free cartesian closed category CCC(Γ) generated by Γ. As ﬁrst shown\nby Lambek [LS86], morphisms of the free cartesian closed category over Γ can be\ncharacterised as the terms of the simply typed lambda calculus generated by Γ.\nWe deﬁne the lambda calculus generated by the basic types B and a functional\nsignature Γ. Its types are given by TY (B). We deﬁne the set of terms by the following\ninductive deﬁnition:\nTE ∋t, u = x | tu | λx.t | < t, u > | π1u | π2u\nA typing context is just a set of pairs of the form x : T, i.e. a functional signature Γ.\nThen a typing judgement is a triple:\nΓ ⊢t : T\n104\nMontague models\nSection 2.2\nconsisting in the assertion that term t has type T in context Γ. A typed term is a\nterm t with a typing judgement Γ ⊢t : T which is derivable from the following rules\nof inference:\nΓ, x : T ⊢x : T\n(2.5)\nΓ ⊢t : T\nΓ ⊢u : U\nΓ ⊢< t, u >: T × U\nΓ ⊢v : T × U\nΓ ⊢π1v : T\nΓ ⊢v : T × U\nΓ ⊢π2v : U\n(2.6)\nΓ, x : U ⊢t : T\nΓ ⊢λx.t : U →T\nΓ ⊢t : U →T\nΓ ⊢u : U\nΓ ⊢tu : T\n(2.7)\nWe deﬁne the typed lambda terms generated by Γ, denoted Λ(Γ) as the lambda\nterms that can be typed in context Γ. In order to deﬁne equivalence of typed lambda\nterms we start by deﬁning β-reduction →β which is the relation on terms generated\nby the following rules:\n(λx.t)u →β t[u/x]\nπ1 < t, u >→β t\nπ2 < t, u >→β u\n(2.8)\nwhere t[u/x] is the term obtained by substituting u in place of x in t, see e.g. [AT10]\nfor an inductive deﬁnition. We deﬁne β-conversion ∼β as the symmetric reﬂexive\ntransitive closure of →β. Next, we deﬁne η-conversion ∼η as the symmetric reﬂexive\ntransitive closure of the relation deﬁned by:\nt ∼η λx.tx\nv ∼η< π1v, π2v >\n(2.9)\nFinally λ-conversion, denoted ∼λ is the transitive closure of the union ∼β ∪∼η. One\nmay show that for any typed term Γ ⊢t : T, if t →β t′ then Γ ⊢t′ : T, and similarly\nfor ∼η, so that λ-equivalence is well deﬁned on typed terms. Moreover, β-reduction\nadmits strong normalisation, i.e. ever reduction sequence is terminating and leads to\na normal form without redexes. For any lambda term t we denote its normal form\nby nm(t), which of course satisﬁes t ∼λ nm(t). However normalising lambda terms is\na problem known to be not elementary recursive in general [Sta79]! We discuss the\nconsequences of this result for Montague grammar at the end of the section.\nWe can now state the correspondence between cartesian closed categories and the\nlambda calculus [LS86]. For a proof, we refer to the lecture notes [AT10] where this\nequivalence is spelled out in detail alongside the correspondence with intuitionistic\nlogic.\nProposition 2.2.4. [AT10, Section 1.6.5] The free cartesian closed category over Γ\nis equivalent to the lambda calculus generated by Γ.\nCCC(Γ) ≃Λ(Γ)/ ∼λ\nNote that a morphism f : x →y in CCC(Γ) may have several equivalent represen-\ntations as a lambda term in Λ(Γ). In the remainder of this section, by f ∈CCC(Γ)\nwe will mean any such representation, and we will write explicitly nm(f) when we\nwant its normal form.\n105\nChapter 2\nFunctors for Semantics\n2.2.2\nTyped ﬁrst-order logic\nMontague used a blend of lambda calculus and logic which allows to compose the\nlogical meaning of individual words into the meaning of a sentence. For example to\nintepret the sentence “John walks”, Montague would assign to “John” the lambda term\nJ = λx.John(x) and to “walks” the term W = λϕ.∃x · ϕ(x) ∧walks(x) so that their\ncomposition results in the closed logic formula ∃x · John(x) ∧walks(x). Note that x\nand ϕ above are symbols of diﬀerent type, x is a variable and ϕ a proposition. The\naim of this section is to deﬁne a typed lambda calculus for manipulating ﬁrst-order\nlogic formulae — akin to [LF04] and [BGG12] — which will serve as codomain for\nMontague’s mapping.\nWe start by recalling the basic notions of ﬁrst-order logic (FOL). A FOL signature\nΣ = C + F + R consist in a set of constant symbol a, b ∈C, a set of function\nsymbols f, g ∈F and a set of relational symbols R, S ∈R together with a function\nar : F + R →N assigning an arity to functional and relational symbols. The terms\nof ﬁrst-order logic over Σ are generated by the following context-free grammar:\nFT(Σ) ∋t ::= x | a | f(⃗x)\nwhere x ∈X for some countable set of variables X, a ∈C, f ∈F and ⃗x ∈(X ∪C)ar(f).\nThe set of ﬁrst-order logic formulae over Σ is deﬁned by the following context-free\ngrammar:\nFOL(Σ) ∋ϕ ::= ⊤| t | x = x′ | R(⃗x) | ϕ ∧ϕ | ∃x · ϕ | ¬ϕ\n(2.10)\nwhere t, x, x′ ∈FT(Σ), R ∈R, ⃗x ∈FT(Σ)ar(R) and ⊤is the truth symbol. Let us\ndenote the variables of ϕ by var(ϕ) ⊆X and its free variables by fv(ϕ) ⊆var(ϕ).\nFor any formula ϕ and ⃗x ∈FT(Σ)∗, we denote by ϕ(⃗x) the formula obtained by\nsubstituting the terms x1 . . . xn in place of the free variables of ϕ where n = |fv(ϕ)|.\nWe can now build a typed lambda calculus over the set of FOL formulae.\nDeﬁnition 2.2.5 (Typed ﬁrst-order logic). Given a FOL signature Σ = C + R, we\ndeﬁne the typed ﬁrst order logic over Σ as the free cartesian closed category CCC(ΓΣ)\nwhere ΓΣ is the functional signature with basic types:\nB = { X, P }n∈N\nwhere X is the type of terms and P is the type of propositions, and entries given by:\nΓΣ = { ϕ : P | ϕ ∈FOL(Σ) −FT(Σ) } + { x : X | x ∈FT(Σ) }\nIn order to recover a FOL formula from a lambda term f : T in CCC(ΓΣ), we\nneed to normalise it.\nProposition 2.2.6. For any lambda term f : P in CCC(ΓΣ), the normal form is a\nﬁrst order logic formula nm(f) ∈FOL(Σ).\nProof.\n106\nMontague models\nSection 2.2\nNote that morphisms ϕ : P in CCC(ΓΣ) are the same as ﬁrst-order logic formulae\nϕ ∈FOL(Σ), since we have adopted the convention that a morphism in CCC(Γ) is\nthe normal form of its representation as a lambda term in Λ(Γ).\nExample 2.2.7. Take Σ = { John, Mary, walks, loves } with ar(John) = ar(Mary) =\nar(walks) = 1 and ar(loves) = 2.\nThen the following are examples of well-typed\nlambda expressions in CCC(ΓΣ):\nλϕ.∃x · ϕ(x) ∧John(x) : P →P\nλxλy.loves(x, y) : X × X →P\nA model for ﬁrst-order logic formulae is deﬁned as follows.\nDeﬁnition 2.2.8 (FOL model). A model K over a FOL signature Σ, also called\nΣ-model, is given by a set U called the universe and an interpretation K(R) ⊆U ar(R)\nfor every relational symbol R ∈R and K(a) ∈U for every constant symbol c ∈C.\nWe denote by MΣ the set of Σ-models.\nGiven a model K ∈MΣ with universe U, let\neval(ϕ, K) = { v ∈U fv(ϕ) | (K, v) ⊨ϕ }\nwhere the satisfaction relation (⊨) is deﬁned by induction over (2.10) in the usual\nway.\nProposition 2.2.9. Any model K ∈MΣ induces a monoidal functor FK : CCC(ΓΣ) →\nSet such that closed lambda terms ϕ : F0 are mapped to their truth value in K.\nProof. By the universal property of free cartesian closed categories, it is suﬃcient to\ndeﬁne FK on generating objects and arrows. On objects we deﬁne:\nFK(X) = 1\nFK(P) =\nN\na\nn=0\nP(U n)\nwhere N is the maximum arity of a symbol in Σ and P(U n) is the powerset of U n.\nOn generating arrows ϕ : P and x : X in Γ, Fk is deﬁned by:\nFK(ϕ) = eval(ϕ, K) ∈FK(P)\nFK(x) = 1\n2.2.3\nMontague semantics\nWe model Montague semantics as a pair of functors G →CCC(ΓΣ) →Set for a\ngrammar G. We have already seen that functors CCC(ΓΣ) →Set can be built from\nΣ-models or relational databases. It remains to study functors G →CCC(ΓΣ), which\nwe call Montague models.\n107\nChapter 2\nFunctors for Semantics\nDeﬁnition 2.2.10 (Montague model). A Montague model is a monoidal functor\nM : G →CCC(ΓΣ) for a biclosed grammar G and a FOL signature Σ such that\nM(s) = P and M(w) = 1 for any w ∈V ⊆G0. The semantics of a grammatical\nsentence g : u →s in BC(G) is the ﬁrst order logic formula nm(M(w)) ∈FOL(Σ)\nobtained by normalising the lambda term M(w) : P in CCC(ΓΣ).\nRemark 2.2.11. Note that since CCC(ΓΣ) is biclosed, it is suﬃcient to deﬁne the\nimage of the lexical entries in G to obtain a Montague model. The structural mor-\nphisms app, comp deﬁned in 1.4.3 have a canonical intepretation in any cartesian\nclosed category given by:\nappA,B 7→λf.(π2f)(π1f) : (A × (A →B)) →B\ncompA,B,C 7→λfλa.(π2f)(π1f)a : ((A →B) × (B →C)) →(A →C)\nExample 2.2.12. Let us consider a Lambek grammar G with basic types B = { n, n′, s }\nand lexicon given by:\n∆(All) = { s/(n\\s)/n′ }\n∆(roads) = { n′ }\n∆(lead to) = { (n\\s)/n }\n∆(Rome) = { n }\nThe following is a grammatical sentence in L(G):\nAll\nroads\nRome\ns\ns/(n\\s)/n′\nn\n(n\\s)/n\nlead to\nn\\s\ns/(n\\s)\napp\napp\napp\nn′\nWe ﬁx a FOL signature Σ = R + C with one constant symbol C = { Rome } and two\nrelational symbols R = { road, leads-to } with arity 1 and 2 respectively. We can now\ndeﬁne a Montague model M on objects by:\nM(n) = X\nM(n′) = X →P\nM(s) = P\nand on arrows by:\nM(All) = λϕλψ.∀x(ϕ(x) =⇒ψ(x)) : (X →P) →((X →P) →P)\nM(roads) = λx.road(x) : X →P\nM(Rome) = Rome : X\nM(lead to) = λxλy.leads-to(x, y) : X →(X →P)\nThen the image of the sentence above normalises to the following lambda term:\nM(All roads lead to Rome →s) = ∀x · (road(x) =⇒leads-to(x, Rome))\n108\nMontague models\nSection 2.2\nWe are interested in the problem of computing the Montague semantics of sen-\ntences generated by a biclosed grammar G.\nDeﬁnition 2.2.13. LogicalForm(G)\nInput:\ng : u →s, Σ, M : G →CCC(ΓΣ)\nOutput:\nnm(M(g)) ∈FOL(Σ)\nWe may split the above problem in two steps. First, given the sentence g : u →s\nwe are asked to produce the corresponding lambda term M(g) ∈Λ(ΓΣ). Second,\nwe are asked to normalise this lambda term in order to obtain the underlying ﬁrst-\norder logic formula in FOL(Σ). The ﬁrst step is easy and may in fact be done in L\nsince it is an example of a functorial reduction, see Proposition 1.3.16. The second\nstep may however be exceptionally hard. Indeed, Statman showed that deciding β\nequivalence between simply-typed lambda terms is not elementary recursive [Sta79].\nAs a consequence, the normalisation of lambda terms is itself not elementary recursive.\nWe may moreover show that any simply typed lambda term can be obtained in the\nimage of some M : G →CCC(ΓΣ), yielding the following result.\nProposition 2.2.14. There are biclosed grammars G such that LogicalForm(G) is\nnot elementary recursive.\nProof. Let G be a biclosed grammar with one generating object a generating arrows\nswap : a ⊗a →a ⊗a, copy : a →a ⊗a and discard : a →1.\nThese gen-\nerators map canonically in any cartesian category. In fact, there is a full functor\nF : BC(G) →CCC, since all the structural morphisms of cartesian closed categories\ncan be represented in BC(G). Therefore for any morphism h ∈CCC there is a mor-\nphism f ∈BC(G) such that F(f) = h. Then for any typed lambda term g ∈Λ(∅)\nthere is a morphism f ∈BC(G) such that F(f) maps to g under the translation 2.2.4.\nTherefore normalising g is equivalent to computing nm(F(f)) = LogicalForm(G).\nTherefore the problem of normalising lambda terms reduces to LogicalForm.\nOf course, this proposition is about the worst case scenario, and deﬁnitely not\nabout human language. In fact, small to medium scale semantic parsers exist which\ntranslate natural language sentences into their logical form using Montague’s trans-\nlation [KM14; AFZ14]. It would be interesting to show that for restricted choices of\ngrammar G (e.g. Lambek or Combinatory grammars), and possibly assuming that the\norder of the lambda terms assigned to words is bounded (as in [Ter12]), LogicalForm\nbecomes tractable.\nEven if we are able to extract a logical formula eﬃciently from natural language,\nwe need to be able to evaluate it in a database in order to compute its truth value.\nThus, in order to compute the full-blown montague semantics we need to solve the\nfollowing problem.\nDeﬁnition 2.2.15. Montague(G)\nInput:\ng : u →s, Σ, M : G →CCC(ΓΣ), K ∈MΣ\nOutput:\nFK(M(g)) ⊆U fv(M(g))\n109\nChapter 2\nFunctors for Semantics\nNote that solving this problem does not necessarily require to solve LogicalForm.\nHowever, since we are dealing with ﬁrst-order logic, the problem for a general biclosed\ngrammar is at least as hard as the evaluation of FOL formulae.\nProposition 2.2.16. There are biclosed grammars G such that Montague is PSPACE-\nhard.\nProof. It was shown by Vardi that the problem of evaluating eval(ϕ, K) for a ﬁrst-\norder logic formula ϕ and a model K is PSPACE-hard [Var82]. Reducing this problem\nto Montague is easy. Take G to be a categorial grammar with a single word w of type\ns. Given any ﬁrst-order logic formula ϕ and model K, we deﬁne M : G →CCC(ΓΣ)\nby M(s) = P and M(w) = ϕ. Then we have that the desired output FK(M(w)) =\nFK(ϕ) = eval(ϕ, K) is the evaluation of ϕ in K.\nIt would be interesting to look at restrictions of this problem, such as ﬁxing the\nMontague model M, ﬁxing the input sentence g : u →s and restricting the allowed\ngrammars G. In general however, the proposition above shows that working with\nfull-blown ﬁrst-order logic is inpractical for large-scale industrial NLP applications.\nIt also shows that models of this kind are the most general and applicable if we can\nﬁnd a compromise between tractability and logical expressivity.\n2.2.4\nMontague in DisCoPy\nWe implement Montague models as DisCoPy functors from biclosed.Diagram into\nthe class Function, as introduced in 1.4 and 2.1 respectively. Before we can do this,\nwe need to upgrade the Function class to account for its biclosed structure. The only\nadditional methods we need are curry and uncurry.\nListing 2.2.17. Curry and UnCurry methods for Function.\nclass Function:\n...\ndef curry(self, n_wires=1, left=False):\nif not left:\ndom = self.dom[:-n_wires]\ncod = self.cod << self.dom[-n_wires:]\ninside = lambda *xl: (lambda *xr: self.inside(*(xl + xr)),)\nreturn Function(inside, dom, cod)\nelse:\ndom = self.dom[n_wires:]\ncod = self.dom[:n_wires] >> self.cod\ninside = lambda *xl: (lambda *xr: self.inside(*(xl + xr)),)\nreturn Function(inside, dom, cod)\ndef uncurry(self):\nif isinstance(self.cod, Over):\nleft, right = self.cod.left, self.cod.right\ncod = left\ndom = self.dom @ right\ninside = lambda *xs: self.inside(*xs[:len(self.dom)])[0](*xs[len(self.dom):])\n110\nMontague models\nSection 2.2\nreturn Function(inside, dom, cod)\nelif isinstance(self.cod, Under):\nleft, right = self.cod.left, self.cod.right\ncod = right\ndom = left @ self.dom\ninside = lambda *xs: self.inside(*xs[len(left):])[0](*xs[:len(left)])\nreturn Function(inside, dom, cod)\nreturn self\nWe can now map biclosed diagrams into functions using biclosed.Functor. We\nstart by initialising a sentence with its categorial grammar parse.\nListing 2.2.18. Example parse from a categorial grammar\nfrom discopy.biclosed import Ty, Id, Box, Curry, UnCurry\nN, S = Ty(’N’), Ty(’S’)\ntwo, three, five = Box(’two’, Ty(), N), Box(’three’, Ty(), N), Box(’five’, Ty(), N)\nplus, is_ = Box(’plus’, Ty(), N >> (N << N)), Box(’is’, Ty(), N >> S << N)\nFA = lambda a, b: UnCurry(Id(a >> b))\nBA = lambda a, b: UnCurry(Id(b << a))\nsentence = two @ plus @ three @ is_ @ five\ngrammar = FA(N, N << N) @ Id(N) @ BA(N, N >> S) >> BA(N, N) @ Id(N >> S) >> FA(N, S)\nsentence = sentence >> grammar\nsentence.draw()\nWe can now evaluate this sentence in a Montague model deﬁned as a biclosed\nfunctor.\nListing 2.2.19. Evaluating a sentence in a Montague model.\nfrom discopy.biclosed import Functor\nnumber = lambda y: Function(lambda: (y, ), Ty(), N)\nadd = Function(lambda x, y: (x + y,), N @ N, N)\nequals = Function(lambda x, y: (x == y, ), N @ N, S)\n111\nChapter 2\nFunctors for Semantics\nob = lambda x: x\nar = {two: number(2), three: number(3), five: number(5),\nis_: equals.curry().curry(left=True),\nplus: add.curry().curry(left=True)}\nMontague = Functor(ob, ar, ob_factory=Ty, ar_factory=Function)\nassert Montague(sentence)() == (True,)\n112\nNeural network models\nSection 2.3\n2.3\nNeural network models\nIn the last couple of decades, neural networks have become ubiquitous in natural lan-\nguage processing. They have been used successfully in a wide range of tasks including\nlanguage modelling [DBM15], machine translation [BCB14; Pop+20], parsing [JM08],\nquestion answering [Zho+17] and sentiment analysis [Soc+13a; KGB14].\nIn this section we show that neural network models can be formalised as functors\nfrom a grammar G to the category SetR of Euclidean spaces and functions via a\ncategory NN of neural network architectures. This includes feed-forward, recurrent\nand recursive neural networks and we discuss how the recent attention mechanisms\ncould be formalised in this diagrammatic framework. At each step, we show how these\nneural architectures are used to solve concrete tasks such as sentence classiﬁcation,\nlanguage modelling, sentiment analysis and machine translation. We keep a level of\ninformality in describing the learning process, an aspect which we will further explore\nin Chapter 3. We end the section by building an interface between DisCoPy and\nTensorﬂow/Keras neural networks [Cho+15; Mar+15], by deﬁning a class Network\nthat allows for composing and tensoring Keras models.\n2.3.1\nFeed-forward networks\nThe great advantage of neural networks comes from their ability to simulate any func-\ntion on Euclidean spaces, a series of results known as universal approximation theo-\nrems [TKG03; OK19]. We will thus give them semantics in the category SetR where\nmorphisms are functions acting on Euclidean spaces. Note that SetR is monoidal with\nproduct ⊕deﬁned on objects as the direct sum of Euclidean spaces Rn ⊕Rm = Rn+m\nand on arrows as the cartesian product f ⊕g(x, y) = (f(x), g(y)). To our knowledge,\nSetR doesn’t have much more structure than this. We will in fact not be able to\ninterpret\nIn a typical supervised machine learning problem one wants to approximate an un-\nknown function f : Rn →Rm given a dataset of pairs D = {(x, f(x))|x ∈Rn, f(x) ∈\nRm}. Neural networks are parametrized functions built from the following basic pro-\ncessing units:\n1. sum sum : n →1 for n ∈N,\n2. weights {w : 1 →1}w∈W0,\n3. biases {r : 0 →1}r∈W1\n4. and activation σ : 1 →1.\nwhere W = W0 + W1 is a set of variables. These generate a free cartesian category\nNN = Cart(W + {sum, σ})\nwhere morphisms are the diagrams of neural network architectures. For example, a\nneuron with n inputs, bias w0 ∈W and weights ⃗w ∈W ∗is given by the following\n113\nChapter 2\nFunctors for Semantics\ndiagram in NN.\nσ\nsum\n. . .\nw0\nw1\nwn\no(⃗w)\nn\n:=\n. . .\nn\n(2.11)\n“Deep” networks are simply given by composing these neurons in parallel forming a\nlayer:\no(⃗w1)\nn\n. . .\no(⃗wm)\n. . .\nm\nlayer\nm\n:=\nn\n. . .\n. . .\n. . .\n. . .\nand then stacking layers one on top of the other, as pasta sheets on a lasagna.\nFix a neural network architecture K : n →m in NN. Given a choice of param-\neters θ : W →R, the network K induces a function Iθ(K) : Rn →Rm called an\nimplementation. Iθ is in fact a monoidal functor NN →SetR deﬁned on objects by\nIθ(1) = R, and on arrows by:\n1. Iθ(sum) = {(x1, . . . , xn) 7→Pn\ni=1 xi},\n2. Iθ(w) = {x 7→θ(w) · x},\n3. Iθ(r) = {() 7→r}\n4. Iθ(σ) is a non-linearity such as sigmoid.\nFor example, the image of the neuron 2.11 is given by the function Iθ(o(⃗w)) : Rn →R\ndeﬁned as follows:\nIθ(o(⃗w))(⃗x) = σ(w0 +\nn\nX\ni=1\nwixi)\nIn order to learn f : Rn →Rm, we choose an architecture K : n →m and a loss\nfunction l : RW →R that evaluates the choice of parameters against the dataset, such\nas the mean-squared error l(θ) = P\ni(Iθ(K)(xi)−f(xi))2. The aim is to minimize the\nloss function with respect to the parameters so that Iθ(K) approximates f as closely\nas possible. The most reliable way of doing this is to compute the gradient of l w.r.t.\nθ and descending along the gradient. This boils down to computing the gradient of\nIθ(K) w.r.t θ which itself boils down to computing the gradient of each component of\nthe network. Assuming that σ : R →R is a smooth function, the image of the functor\nIθ lies in Smooth ,−→SetR, the category of smooth functions on Euclidean spaces.\nThe backpropagation algorithm provides an eﬃcient way of updating the parameters\n114\nNeural network models\nSection 2.3\nof the network step-by-step while descending along the gradient of l, see [Cru+21] for\na recent categorical treatment of diﬀerentiation.\nAlthough neural networks are deterministic functions, it is often useful to think\nof them as probabilistic models. One can turn the output of a neural network into a\nprobability distribution using the softmax function as follows. Given a neural network\nK : m →n and a choice of parameters θ : W →R we can compose Iθ(K) : Rm →Rn\nwith a softmax layer, given by:\nsoftmaxn(⃗x)i =\nexi\nPn\ni=1 exi\nThen the output is a normalised vector of positive reals of length n, which yields a\ndistribution over [n] the set with n elements, i.e. softmax has the following type:\nsoftmaxn : Rn →D([n])\nwhere D(A) is the set of probability distributions over A, as deﬁned in 3.1, where\nsoftmax is analysed in more detail. In order to simulate a probabilistic process, from\na neural network K : m →n, an element of [n] is drawn at random from the induced\ndistribution softmaxn(Iθ(K)(x)) ∈D([n]).\nFeed-forward neural networks can be used for sentence classiﬁcation: the general\ntask of assigning labels to pieces of written text. Applications include spam detection,\nsentiment analysis and document classiﬁcation. Let D ⊆V ∗×X be a dataset of pairs\n(u, x) for u an utterance and x a label. The task is to ﬁnd a function f : V ∗→D(X)\nminimizing a loss function l(f, D) which computes the distance between the predicted\nlabels and the expected ones given by D. For example one may take the mean squared\nerror l(f, D) = P\n(u,x)∈D(f(u) −|x⟩)2 or the cross entropy loss −1\n|D|\nP\n(u,x)∈D |x⟩·\nlog(f(u)), where |x⟩is the one-hot encoding of x ∈X as a distribution |x⟩∈D(X)\nand · is the inner product. Assuming that every sentence u ∈V ∗has length at most\nm and that we have a word embedding E : V →Rk, we can parametrize the set of\nfunctions Rmk →R|X| using a neural network K : mk →|X| and use softmax to get\na probability distribution over classes:\nf(x | u) = softmaxk(Iθ(K)(E∗(u))) ∈D(X)\nwhere u = v1v2 . . . vm ∈V ∗is an utterance and E∗(u) is the feature vector for u, ob-\ntained by concatenating the word embeddings E∗(u) = concat(E(v1), E(v2), . . . , E(vn)).\nIn this approach, K plays the role of a black box which takes in a list of words\nand ouputs a class.\nFor instance in a sentiment analysis task, we can set X =\n{ positive, negative } and m = 3 and expect that “not so good” is classiﬁed as “nega-\ntive”.\nK\nNot\nso\ngood\nk\nk\nk\n2\nnegative\n115\nChapter 2\nFunctors for Semantics\nA second task we will be interested in is language modelling, the task of predicting\na word in a text given the previous words. It takes as input a corpus, i.e. a set of\nstrings of words C ⊆V ∗, and outputs f : V ∗→D(V ) a function which ouputs a\ndistribution for next word f(u) ∈D(V ) given a sequence of previous words u ∈V ∗.\nLanguage modelling can be seen as an instance of the classiﬁcation task, where the\ncorpus C is turned into a dataset of pairs D such that (u, x) ∈D whenever ux is a\nsubstring of C. Thus language modelling is a self-supervised task, i.e. the supervision\nis not annotated by humans but directly generated from text. Neural networks were\nﬁrst used for language modelling by Bengio et al. [Ben+03], who trained a single\n(deep) neural network taking a ﬁxed number of words as input and predicting the\nnext, obtaining state of the art results at the time.\nFor both of these tasks, feed-forward neural networks perform poorly compared to\nthe recurrent architectures which we are about to study. This is because feed-forward\nnetworks take no account of the sequential nature of the input.\n2.3.2\nRecurrent networks\nThe most popular architectures currently used modelling language are recurrent neu-\nral networks (RNNs).\nThey were introduced in NLP by Mikolov et al.\nin 2010\n[Mik+10] and have since become widely used, see [DBM15] for a survey.\nRNNs were introduced by Ellman [Elm90] to process sequence input data. They\nare deﬁned by the following recursive equations:\nht = σ(Whxt + Uhht−1 + bh)\nyt = σ(Wyht + by)\n(2.12)\nwhere t ∈N is a time variable, ht denotes the encoder hidden vector, xt the input\nvector, yt the output, Wh and Wy are matrices of weights and bh, by are bias vectors.\nWe may draw these two components as diagrams in NN:\nX\nx\nh\nh =\nU\nx\nh\nh\nWh\nsum\nσ\nY\nh\ny =\nWy\nh\nh\nσ\nsum\nby\nbh\nRemark 2.3.1. The diagrams in the remainder of this section should be read from\nleft/top to bottom/right. One may obtain the corresponding string diagram by bending\nthe left wires to the top and the right wires to the bottom. In the diagram above we\nhave omitted the subscript t since it is determined by the left-to-right reading of the\ndiagram and we use the labels h, x, y for dimensions, i.e. objects of NN.\n116\nNeural network models\nSection 2.3\nThe networks above are usually called simple recurrent networks since they have\nonly one layer. More generally X and Y could be any neural networks of type X :\nx ⊕h →h and Y : h →y, often called recurrent network and decoder respectively.\nDeﬁnition 2.3.2. A recurrent neural network is a pair of neural networks X : x⊕h →\nh and Y : h →y in NN for some dimensions x, h, y ∈N.\nThe data of RNNs deﬁned above captures precisely the data of a functor from\na regular grammar as we proceed to show. Fix a ﬁnite vocabulary V and consider\nthe regular grammar RG with three symbols s0, h, s1 and transitions h\nw−→h for each\nword w ∈V .\nh\ns0\ns1\nw ∈V\nNote that RG parses any string of words, i.e.\nthe language generated by RG is\nL(RG) = V ∗. The derivations in RG are sequences:\nh\ns0\ns1\nw0 h w1 . . . wk h\n(2.13)\nRecurrent neural networks induce functors from this regular grammar to NN.\nProposition 2.3.3. Recurrent neural networks (X : x ⊕h →h, Y : h →y) induce\nfunctors RN : RG →NN such that |V | = x, RN(h) = h, RN(s1) = y, RN(s0) = 0\nand RN(s0 →h) = ⃗0.\nProof. Given an RNN (X, Y ) we can build a functor with RN(h\nw−→h) = (|w⟩⊕idh)·X\nwhere |w⟩is the one-hot encoding of word w ∈V as a vector of dimension n = |V |,\nand RN(h →s0) = Y .\nRemark 2.3.4. Note that a functor RN : RG →NN induces a ﬁnite family of\nneural networks RN(h\nw−→h) indexed by w ∈V . We do not think that it is always\npossible to construct a recurrent network X : |V | ⊕RN(h) →RN(h) such that\nRN(h\nw−→h) = (|w⟩⊕idn) · X, hinting that functors RN : RG →NN are a larger\nclass of processes than RNNs, but we were unable to ﬁnd a counterexample.\nGiven a recurrent neural network RN : G →NN, the image under RN of the\nderivation 2.13 is the following diagram in NN.\nw0\nw1\nwk\n. . .\nX\nX\nX\nh\nh\ny\n|V |\n|V |\n|V |\nY\nh\nh\n⃗0\nIt deﬁnes a network that runs through the input words wi with the recurrent\nnetwork and uses the decoder to output a prediction of dimension y.\n117\nChapter 2\nFunctors for Semantics\nIn order to classify sentences in a set of classes X, one may choose y = |X|, so\nthat the implementation of the diagram above is a vector of size |X| giving scores for\nthe likelihood that w0w1 . . . wk belongs to class c ∈X.\nFor language modelling, we may set y = |V |. Given a string of words u ∈V ∗with\nderivation gu : s0 →s1 ∈C(RG) and an implementation Iθ : NN →SetR, we can\ncompute the distribution over the next words softmax(Iθ(RN(gu))) ∈D |V |.\nAnother task that recurrent neural networks allow to tackle is machine translation.\nThis is done by composing a encoder recurrent network X : |V |⊕h →h with a decoder\nrecurrent network K : h →h ⊕|V |, as in the following diagram.\nw0\nwk\n. . .\nX\nX\nh\n|V |\n|V |\nK\nh\nh\n|V |\n. . .\nK\n|V |\nw′\n0\nw′\nk\nh\nh\nEncode\nDecode\n(2.14)\nOne of the drawbacks of standard RNNs is that they don’t have memory. For\ninstance in the translation architecture above, the last encoder state is unfolded into\nall the output decoder states. This often results in a translation that loses accuracy\nas the sentences grow bigger.\nLong-short term memory networks (LSTMs) were\nintroduced in order to remedy this problem [HS97]. They are a special instance of\nrecurrent neural networks, where the encoder states are split into a hidden state and\na cell state. The cell state allows to store information for longer durations and their\narchitecture was designed to avoid vanishing gradients [GSC00]. This makes them\nparticularly suited to NLP applications, see [OMK19] for example applications.\nMoreover, RNNs are well suited for processing data coming in sequences but they\nfail to capture more structured input data. If we want to process syntax trees and\nmore complex grammatical structures, what we need is a recursive neural network.\n2.3.3\nRecursive networks\nRecursive neural networks (RvNN) generalise RNNs by allowing neural networks to\nrecur over complex structures. They were introduced in the 1990s by Goller and\nKuchler [GK96] for the classiﬁcation of logical terms, and further generalised by\nSperduti et al. [SS97; FGS98] who pioneered their applications in chemistry [Bia+00;\nMSS04]. RvNNs were introduced in NLP by Socher et al. [Soc+13a], who obtained\nstate-of-the-art results in the task of sentiment analysis using tree-shaped RvNNs.\nThey have since been applied to several tasks including word sense disambiguation\n[CK15] and logical entailment [BPM15].\nIn [SS97], Sperduti and Starita provide a structured way of mapping labelled\ndirected graphs onto neural networks.\nThe main diﬃculty in their formalisation\nappears when the graph in the domain has cycles, in which case they give a procedure\nfor unfolding it into a directed acyclic graph. We review their formalisation for the\ncase of directed acyclic graphs (DAGs). A DAG is given by a set of vertices N and a set\nof edges E ⊆N×N such that the transitive closure of E does not contain loops. In any\n118\nNeural network models\nSection 2.3\nDAG (N, E), the parents of a vertex v are deﬁned by pa(v) = { v′ ∈N | (v′, v) ∈E }\nand the children by ch(v) = { v′ ∈N | (v, v′) ∈E }. A Σ-labelled DAG is a DAG\n(N, E) with a labelling function ϕ : N →Σ.\nSuppose we have a set X of Σ-labelled DAGs which we want to classify over k\nclasses, given a dataset of pairs D ⊆X × [k]. The naive way to do this is to encode\nevery graph in D as a vector of ﬁxed dimension n and learning the parameters of\na neural network K : n →k.\nSperduti and Starita propose instead to assign a\nrecursive neuron to each vertex of the graph, and connecting them according to\nthe topological structure of the graph.\nGiven a labelled DAG (N, E, ϕ) with an\nassignment θ : E + Σ →R of weights to every edge in E and every label in Σ, the\nrecursive neuron assigned to a vertex v in a DAG (N, E) is the function deﬁned by\nthe following recursive formula:\no(v) = σ(θ(ϕ(v)) +\nX\nv′∈pa(v)\nθ(v′, v)o(v′))\n(2.15)\nNote that this is a simpliﬁcation of the formula given by Sperduti et al. where we\nassume that a single weight is assigned to every edge and label, see [SS97] for details.\nThey further assume that the graph has a sink, i.e. a vertex which can be reached\nfrom any other vertex in the graph. This allows to consider the output of the neuron\nassigned to the supertarget as the score of the graph, which they use for classiﬁcation.\nWe show that the recursive neuron (2.15) deﬁnes a functor from a monoidal gram-\nmar to the category of neural networks NN. Given any set of Σ-labelled DAGs X,\nwe may construct a monoidal signature G = Σ + E∗\nϕ+in\n←−−−N\nout\n−−→E∗where N is\nthe set of all vertices appearing in a graph in X and E is the set of all edges ap-\npearing in a graph in X, with in, out : N →E∗listing the input and output edges\nof vertex v respectively and ϕ : N →Σ is the labelling function. Then the recur-\nsive neuron o given above deﬁnes a functor O : MC(G + swap) →SetR from the\nfree monoidal category generated by G and swaps to SetR. The image of a vertex\nv : l ⊕⃗e →⃗e′ is a function O(v) : R|⃗e|+1 →R|⃗e′| given by: O(v)(x) = copy(o(v)(x))\nwhere copy : R →R|⃗e′| is the diagonal map in SetR. Note that any DAG H ∈X\ngives rise to a morphism in MC(G + swap) given by connecting the boxes (vertices)\naccording to the topological structure of the graph, using swaps if necessary. Note\nthat the functor O factors through the category NN of neural networks since all the\ncomponents of Equation 2.15 are generators of NN, thus O deﬁnes a mapping from\nDAGs in X to neural networks. Generalising from this to the case where vertices in\nthe graph can be assigned multiple neurons, we deﬁne recursive neural networks as\nfollows.\nDeﬁnition 2.3.5. A recursive network model is a monoidal functor F : G →NN\nfor a monoidal grammar G, such that F(w) = 1 for w ∈V ⊆G0. Given a choice of\nparameters θ : W →R, the semantics of a parsed sentence g : u →s in MC(G) is\ngiven by Iθ(F(g)) ∈RF(s).\nRemark 2.3.6. Although it is in general useful to consider non-planar graphs as\nthe input of a recursive neural network, in applications to linguistics one can usually\n119\nChapter 2\nFunctors for Semantics\nassume that the graphs are planar. In order to recover non-planar graphs from the\ndeﬁnition above it is suﬃcient to add a swap to the signature G.\nWith this deﬁnition at hand, we can look at the applications of RvNNs in linguis-\ntics. The recursive networks for sentiment analysis of [Soc+13a] are functors from\na context-free grammar to neural networks. In this case RvNNs are shown to cap-\nture correctly the role of negation in changing the sentiment of a review. This is\nbecause the tree structure induced by the context-free grammar captures the part of\nthe phrase which is being negated, as in the following example.\nnot very good\nN\nA\nNP\nNeg\nNP\nAs shown in [Soc+13a, Figure 4] the recurrent network automtically learned to assign\na negative sentiment to the phrase above, even if the sub-phrase “very good” is posi-\ntive. Generalising from this, researchers have shown that recursive networks perform\nwell on natural language entailment tasks [BPM15].\n2.3.4\nAttention is all you need?\nThe great advantage of neural networks is also their worst handicap. Since they are\nable to approximate almost any function, we have no guarantees as to what the re-\nsulting function will look like, an issue famously put as the “black box problem”. The\nbaﬄing thing is that neural networks seem to work best when their underlying struc-\nture is unconstrained and neurons are arranged in a deep fully connected network.\nWe can see this pattern in the recent progression that neural language models have\nmade: from structured networks to attention.\nAttention layers were added to recurrent neural networks by Bahdanau et al.\nin 2014 [BCB14]. The architecture of Bahdanau et al. is composed of two recurrent\nneural networks, an encoder f and a decoder g, connected by an attention mechanism\nas in the following diagram:\nf\nxt−1\nf\nxt\nf\nxt+1\nAttention\ng\nyt−1\ng\nyt\ng\nyt+1\nht−2\nht−1\nht\nht+1\nst−2\nst−1\nst\nst+1\nct−1\nct\nct+1\n. . .\n. . .\n(2.16)\n120\nNeural network models\nSection 2.3\nwhere xi is the ith input word in the domain language, yi is the ith output word in\nthe target language, the his and sis are the hidden states of the encoder and decoder\nRNN respectively:\nhi = f(xi, hi−1)\nsi = g(si−1, yi−1, ci)\nand ci is the context vector calculated from the input ⃗x, the hidden states ⃗h and the\nlast decoder hidden state si−1 as follows:\nci =\nn\nX\nj=1\nαijhj\n(2.17)\nwhere\nαij = (softmaxn(⃗ei))j\n(⃗ei)j = a(si−1, hj)\n(2.18)\nwhere n = |⃗x| and a is a (learned) feed-forward neural network. The coeﬃcients αij\nare called attention weights, they provide information as to how much input word\nxj is relevant for determining the output word yi. In the picture above, we used a\ncomb notation for Attention informally to represent the recursive relation between\ncontext vectors c and hidden states h and s, capturing the ﬂow of information in\nthe architecture of Bahdanau et al. This diagram should be read from top-left to\nbottom-right. At each time step t, the attention mechanism computes the context\nvector ct from the last decoder hidden state st−1 and all the encoder hidden states ⃗h.\nInﬁnite comb diagrams such as the one above may be formalised as monoidal streams\nover the category of neural networks [DdR22]. A similar notation is used in Chapter\n3. Note that Bahdanau et al. model the encoder f as a bidirectional RNN [SP97]\nwhich produces hidden states hi that depend both on the previous and the following\nwords. For simplicity we have depicted f as a standard RNN.\nIn 2017, Vaswani et al. published the paper “Attention is all you need” [Vas+17]\nwhich introduced transformers.\nThey showed that the recurrent structure is not\nneeded to obtain state-of-the-art results in machine translation. Instead, they pro-\nposed a model built up from three simple components: positional encoding, attention\nand feed-forward networks, composed as in the following diagram:\nD\nwk+1\nw0\nw1\nwk\nAttention\nWQ\nWK\nWV\n. . .\nQ\nK\nV\npos(0) pos(1)\npos(k)\n(2.19)\nwhere\n121\nChapter 2\nFunctors for Semantics\n1. WQ, WK, WV are (learned) linear functions, and D is a (learned) decoder neural\nnetwork.\n2. the positional encoding pos supplements the word vector wi of dimension n\nwith another n dimensional vector pos(i) ∈Rn where pos : N →Rn is given\nby:\npos(i)j =\n(\nsin(\nj\nm2k/n)\nif j = 2k\ncos(\nj\nm2k/n)\nif j = 2k + 1\nwhere m ∈N is a hyperparameter which determines the phase of the sinusoidal\nfunction (Vaswani et al. choose m = 1000 [Vas+17]),\n3. and attention is deﬁned by the following formula:\nAttention(Q, K, V ) = softmax(QKT\n√dK\n)V\n(2.20)\nwhere Q and K are vectors of the same dimension dK called query and keys\nrespectively and V is a vector called values.\nNote that the positional encoding is needed since otherwise the network would have\nno information about the position of words in a sentence. Using these sinusoidal\nencodings turns absolute into relative position [Vas+17]. Comparing this deﬁnition\n2.20 of attention with the one used by Bahdanau et al. [BCB14], we note that in\nEquations 2.17 and 2.18 the hidden states ⃗h and ⃗s play the role of keys K and queries\nQ respectively, and the values V are again taken to be encoder hidden states ⃗h. The\nmain diﬀerence is that instead of a deep neural network (denoted a above) the queries\nand keys are pre-processed with linear operations (WK, WQ and WV above). This\narchitecture is the basis of BERT [Dev+19] and its extensions such as GPT-3 which\nuse billions of parameters to achieve state of the art results in a wide range of NLP\ntasks.\nReasoning about what happens inside these models and explaining their behaviour\nwith linguistic analysis is hard. Indeed, the same architectures where sentences are\nreplaced by images and words by parts of that image give surprisingly accurate re-\nsults in the image recognition task [Dos+20], suggesting that linguistic principles are\ninsuﬃcient for analysing these algorithms. However, viewing deep neural networks\nas end-to-end black-box learners, it becomes interesting to open the box and analyse\nthe output of the intermediate layers with the aim of understanding the diﬀerent\nfeatures that the network learns in the process. Along these lines, researchers have\nfound that neural network models automatically encode linguistic features such as\ngrammaticality [Coe+19] and dependency parsing [ACL19]. One possible line of fu-\nture research would be to study what happens in the linear world of keys, values and\nqueries. One may take the latest forms of attention [Vas+17] as the realisation that\naccurate predictions can be obtained from a linear process fracQKT√dK by using\na single softmax activation. We will give a bayesian interpretation of this activation\nfunction in Chapter 3.\n122\nNeural network models\nSection 2.3\n2.3.5\nNeural networks in DisCoPy\nWe now show how to interface DisCoPy with Tensorﬂow/Keras neural networks\n[Cho+15; Mar+15]. In order to do this, we need to deﬁne objects, identities, compo-\nsition and tensor for Keras models.\nThe objects of our category of neural networks will be instances of PRO, a subclass\nof monoidal.Ty initialised by a dimension n. A morphism from PRO(n) to PRO(k)\nis a neural network with input shape (n, ) and output shape (k, ) (we only deal\nwith ﬂat shapes for simplicity). A neural.Network is initialised by providing domain\nand codomain dimensions together with a Keras model of that type. Composition is\neasily implemented using the call method of Keras models. For tensor, we ﬁrst need\nto split the domain using keras.layers.Lambda, then we act on each subspace inde-\npendently with self and other, and ﬁnally we concatenate the outputs. Identities\nare simply Keras models with outputs = inputs, and we include a static method\nfor constructing dense layer models.\nListing 2.3.7. The category of Keras models\nfrom discopy import monoidal, PRO\nimport tensorflow as tf\nfrom tensorflow import keras\nclass Network(monoidal.Box):\ndef __init__(self, dom, cod, model):\nself.model = model\nsuper().__init__(\"Network\", dom, cod)\ndef then(self, other):\ninputs = keras.Input(shape=(len(self.dom),))\noutput = self.model(inputs)\noutput = other.model(output)\ncomposition = keras.Model(inputs=inputs, outputs=output)\nreturn Network(self.dom, other.cod, composition)\ndef tensor(self, other):\ndom = len(self.dom) + len(other.dom)\ncod = len(self.cod) + len(other.cod)\ninputs = keras.Input(shape=(dom,))\nmodel1 = keras.layers.Lambda(\nlambda x: x[:, :len(self.dom)],)(inputs)\nmodel2 = keras.layers.Lambda(\nlambda x: x[:, len(self.dom):],)(inputs)\nmodel1 = self.model(model1)\nmodel2 = other.model(model2)\noutputs = keras.layers.Concatenate()([model1, model2])\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nreturn Network(PRO(dom), PRO(cod), model)\n@staticmethod\ndef id(dim):\ninputs = keras.Input(shape=(len(dim),))\nreturn Network(dim, dim, keras.Model(inputs=inputs, outputs=inputs))\n123\nChapter 2\nFunctors for Semantics\n@staticmethod\ndef dense_model(dom, cod, hidden_layer_dims=[], activation=tf.nn.relu):\ninputs = keras.Input(shape=(dom,))\nmodel = inputs\nfor dim in hidden_layer_dims:\nmodel = keras.layers.Dense(dim, activation=activation)(model)\noutputs = keras.layers.Dense(cod, activation=activation)(model)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nreturn Network(PRO(dom), PRO(cod), model)\nAs an application, we use a monoidal.Functor to construct a Keras model from\na context-free grammar parse, illustrating the tree neural networks of [Soc+13a].\nListing 2.3.8. Tree neural networks\nfrom discopy.monoidal import Ty, Box, Functor\nn, a, np, neg, star = Ty(’N’), Ty(’A’), Ty(’NP’), Ty(’Neg’), Ty(’*’)\nnot_, very, good = Box(’not’, star, neg), Box(’very’, star, a), Box(’good’, star, n)\nP1, P2 = Box(’P1’, a @ n, np), Box(’P2’, neg @ np, np)\ndiagram = not_ @ very @ good >> Id(neg) @ P1 >> P2\ndim = 5\nob = lambda x: PRO(dim)\nar = lambda box: Network.dense_model(len(box.dom) * dim, len(box.cod) * dim)\nF = Functor(ob, ar, ob_factory=PRO, ar_factory=Network)\nkeras.utils.plot_model(F(diagram).model)\n124\nRelational models\nSection 2.4\n2.4\nRelational models\nThe formal study of relations was initiated by De Morgan in the mid 19th century\n[De 47]. It was greatly developed by Peirce who only published small fragments of his\ncalculus of relations [Pei97], although much of it was popularised in the inﬂuential\nwork of Schroder [Sch90]. In the ﬁrst half of the twentieth century, this calculus was\noften disregarded in favour of Frege’s and Russell’s approach to logic [Ane12], until\nit was revived by Tarski [Tar41] who developed it into the rich ﬁeld of model theory.\nWith the advent of computer science, the calculus of relations came to be recog-\nnized as a convenient framework for storing and accessing data, leading to the devel-\nopment of relational databases in the 1970s [Cod70]. SQL queries were introduced by\nChamberlin and Boyce in 1976 [CB76] and they are still used for accessing databases\ntoday. Conjunctive queries are an important subclass of SQL, corresponding to the\nSelect-Where-From fragment. They were introduced by Chandra and Merlin [CM77]\nwho showed that their evaluation in a relational database is an NP-complete problem,\nspawning a large ﬁeld of studies in the complexity of constraint satisfaction problems\n[DKV02]. The blend of algebra and logic oﬀered by the theory of relations is partic-\nularly suited to a categorical formalisation and it has motivated the work of Carboni\nand Walters [CW87] as well as Brady and Trimble [BT00] and Bonchi et al. [BPS17;\nBSS18] among others.\nIn this section, we start by reviewing the theory of relational databases and con-\njunctive queries from a categorical perspective. We then introduce relational models\nby transposing the deﬁnitions into a linguistic setting. This allows us to transfer\nresults from database theory to linguistics and deﬁne NP −complete entailment and\nquestion answering problems. The correspondence of terminology between databases,\ncategory theory and linguistics is summarized in the table below.\nDatabases\nAlgebra\nLinguitics\nRelational database\nCartesian bicategory\nRelational model\nattributes\nobjects\nbasic types\nschema\nsignature\nlexicon\nquery\nmorphism\nsentence\ninstance\nfunctor\nmodel\ncontainment\npreorder enrichment\nentailment\n2.4.1\nDatabases and queries\nWe introduce the basic notions of relational databases, starting with an example.\nExample 2.4.1.\nreader\nbook\nwriter\nSpinoza\nDe Causa\nBruno\nShakespeare\nWorld of Wordes\nFlorio\nFlorio\nDe Causa\nBruno\nLeibniz\nTractatus\nSpinoza\n125\nChapter 2\nFunctors for Semantics\nConsider the structure of the table above, which we denote by ρ. There is a set of\nattributes A = { reader, book, writer } which name the columns of the table and a set\nof data values Da for each attribute a ∈A,\nDr = Dw = {Spinoza, Shakespeare, Leibniz, Bruno, Florio}\nDb = {De Causa, World of Wordes, Tractatus}\nA row of the table is a tuple t ∈Q\na∈A Da, which assigns a particular value ta to each\nattribute a ∈A, e.g. (Leibniz, Tractatus, Spinoza). The table then consists in a set\nof tuples, i.e. a relation ρ ⊆Q\na∈A Da.\nA relational database is a collection of tables (relations), organised by a schema.\nGiven a set of attributes A, a schema Σ is a set of relational symbols, together with\na domain function dom : Σ →A∗. The schema serves to specify the set of names Σ\nfor the tables in a database together with the type of their columns. For example we\nmay have ρ ∈Σ for the table above with dom(ρ) = (reader, book, writer). We have\nalready encountered this type of structure in 1.6.1, where we used the term hypergraph\nsignature instead of schema.\nDeﬁnition 2.4.2. A relational database K with schema Σ is an assignment of each\nattribute a ∈A to a corresponding set of data values Da, and an assignment of each\nsymbol R ∈Σ to a relation K(R) ⊆Q\na∈dom(R) Da.\nInstead of working directly with relational databases, it is often convenient to\nwork with a simpler notion known as a relational structure. The schema is replaced\nby a relational signature, which is a set of symbols Σ equipped with an arity function\nar : Σ →N.\nDeﬁnition 2.4.3 (Relational structure). A relational structure K over a signature Σ,\nalso called a Σ-structure, is given by a set U called the universe and an interpretation\nK(R) ⊆U ar(R) for every symbol R ∈Σ. We denote by MΣ the set of Σ-structures\nwith ﬁnite universe U(K).\nGiven two Σ-structures K, K′, a homomorphism f : K →K′ is a function f :\nU(K) →U(K′) such that ∀R ∈Σ ∀⃗x ∈U ar(R) · ⃗x ∈K(R) =⇒f(⃗x) ∈K′(R).\nRemark 2.4.4. Note that relational structures are the same as relational databases\nwith only one attribute.\nAttributes a ∈A can be recovered by encoding them as\npredicates a ∈Σ of arity 1 and one may take the universe to be the union of the sets\nof data values U = ∪a∈ADa.\nWe consider the problem of ﬁnding a homomorphism between relational structures.\nDeﬁnition 2.4.5. Homomorphism\nInput:\nK, K′ ∈MΣ\nOutput:\nf : K →K′\nProposition 2.4.6. [GJ90] Homomorphism is NP −complete.\n126\nRelational models\nSection 2.4\nProof. Membership may be shown to follow from Fagin’s theorem: homomorphisms\nare deﬁned by an existential second-order logic formula. Hardness follows by reduction\nfrom graph homomorphism: take Σ = { • } and ar(•) = 2 then a Σ-structure is a\ngraph.\nThe most prominent query language for relational databases is SQL [CB76]. Con-\njunctive queries form a subset of SQL (corresponding to the Select-Where-From frag-\nment) with a convenient mathematical formulation. We deﬁne conjunctive queries and\nthe corresponding Evaluation and Containment problems. Let X be a (countable)\nset of variables, Σ a relational signature and consider the logical formulae generated\nby the following context-free grammar:\nϕ ::= ⊤| x = x′ | ϕ ∧ϕ | ∃x · ϕ | R(⃗x)\nwhere x, x′ ∈X, R ∈Σ and ⃗x ∈X ar(R).\nLet us denote the variables of ϕ by\nvar(ϕ) ⊆X, its free variables by fv(ϕ) ⊆var(ϕ) and its atomic formulae by\natoms(ϕ) ⊆`\nR∈Σ var(ϕ)ar(R), i.e. an atomic formula is given by R(x1, . . . , xar(R))\nfor some variables xi ∈X.\nThis fragment is called regular logic in the category-theory literature [FS18a]. It\nyields conjuntive queries via the prenex normal form.\nDeﬁnition 2.4.7. Conjunctive queries ϕ ∈QΣ are the prenex normal form ϕ =\n∃x0 · · · ∃xk · ϕ′ of regular logic formulae, for the bound variables { x0, . . . , xk } =\nvar(ϕ) \\ fv(ϕ) and ϕ′ = V atoms(ϕ). We denote by :\nQΣ(k) = {ϕ ∈QΣ | fv(ϕ) = k}\nthe set of conjunctive queries with k free variables.\nGiven a structure K ∈MΣ, let eval(ϕ, K) = { v ∈U(K)fv(ϕ) | (K, v) ⊨ϕ }\nwhere the satisfaction relation (⊨) is deﬁned in the usual way.\nDeﬁnition 2.4.8. Evaluation\nInput:\nϕ ∈QΣ,\nK ∈MΣ\nOutput:\neval(ϕ, K) ⊆U(K)fv(ϕ)\nDeﬁnition 2.4.9. Containment\nInput:\nϕ, ϕ′ ∈QΣ\nOutput:\nϕ ⊆ϕ′\n≡∀K ∈MΣ · eval(ϕ, K) ⊆eval(ϕ′, K)\nExample 2.4.10. Following from 2.4.1 let U := Dw ∪Db and ﬁx the schema Σ =\n{read, wrote}∪U with ar(w) = 1 for w ∈U and ar(read) = ar(wrote) = 2. Consider\nthe relational structure K ∈QΣ with universe U and K(w) = { w } ⊆U for w ∈\nΣ −{ read, wrote } and K(read) ⊆U × U given by the ﬁrst two columns of table\nρ, K(wrote) ⊆U × U given by the second two columns of ρ. The following is a\nconjunctive query with no free variables:\nϕ = ∃y, z · read(x, y) ∧Bruno(y) ∧wrote(x, z)\n127\nChapter 2\nFunctors for Semantics\nSince ϕ has one free variables, eval(ϕ, K) ⊆U. With K as deﬁned above there\nare three valuations v : var(ϕ) = { x, y, z } →U such that (v, K) ⊨ϕ, yielding\neval(ϕ, K) = { Spinoza, Florio, Galileo } ⊆U.\nDeﬁnition 2.4.11 (Canonical structure). Given a query ϕ ∈QΣ, the canonical struc-\nture CM(ϕ) ∈MΣ is given by U(CM(ϕ)) = var(ϕ) and CM(ϕ)(R) = { ⃗x ∈var(ϕ)ar(R) | R(⃗x) ∈atoms(\nfor R ∈Σ.\nThis result was used by Chandra and Merlin to reduce from Homomorphism to\nboth Evaluation and Containment.\nTheorem 2.4.12 (Chandra-Merlin [CM77]). The problems Evaluation and Containment\nare logspace equivalent to Homomorphism, hence NP −complete.\nProof. Given a query ϕ ∈QΣ and a structure K ∈MΣ, query evaluation eval(ϕ, K)\nis given by the set of homomorphisms CM(ϕ) →K. Given ϕ, ϕ′ ∈MΣ, we have ϕ ⊆\nϕ′ iﬀthere is a homomorphism f : CM(ϕ) →CM(ϕ′) such that f(fv(ϕ)) = fv(ϕ′).\nGiven a structure K ∈MΣ, we construct ϕ ∈QΣ with fv(ϕ) = ∅, var(ϕ) = U(K)\nand atoms(ϕ) = K.\n2.4.2\nThe category of relations\nLet us now consider the structure of the category of relations. A relation R : A →B\nis a subset R ⊆A × B or equivalently a predicate R : A × B →B, we write aRb for\nthe logical statement R(a, b) = 1. Given S : B →C, the composition R; S : A →C\nis deﬁned as follows:\naR; Sc ⇐⇒∃b ∈B · aRb ∧bSc .\nUnder this composition relations form a category denoted Rel.\nWe can also construct the category of relations by considering the powerset monad\nP : Set →Set deﬁned on objects by P(X) = { S ⊆X } and on arrows by f : X →Y\nby P(f) : P(X) →P(Y ) : S 7→f(S). A relation R : A →B is the same as a function\nR : A →P(B) and in fact Rel = Kl(P) is the Kleisli category of the powerset monad.\nAny function f : A →B induces a relation I(f) = { (x, f(x)) | x ∈A } ⊆A × B,\nsometimes called the graph of f. The tensor product of relations R : A →B and\nT : C →D is denoted R ⊗T : A × C →B × D and deﬁned by:\n(a, c)R ⊗T(b, d) ⇐⇒aRb ∧cTd .\nEquipped with this tensor product and unit the one-element set 1, Rel forms a\nsymmetric monoidal category, with symmetry lifted from Set. We can thus use the\ngraphical language of monoidal categories for reasoning with relations.\nNote that each object A ∈Rel is self-dual, as witnessed by the morphisms cupA :\nA × A →1 and capA : 1 →A × A, deﬁned by:\n(a, a′)cupA ⇐⇒(a = a′) ⇐⇒capA(a, a′)\n128\nRelational models\nSection 2.4\nThese denoted graphically as cups and caps and satisfy the snake equations 1.18.\nThus Rel is a compact-closed category.\nMoreover, every object A ∈Rel comes\nequipped with morphisms ∆: A →A × A and ∇: A × A →A deﬁned by:\na∆(a′, a′′) ⇐⇒(a = a′ = a′′) ⇐⇒(a, a′)∇a′′ .\nTogether with the unit η : 1 →A and the counit ϵ : A →1, deﬁned by η = A = ϵ,\nthe tuple (∆, ϵ, ∇, η) satisﬁes the axioms of special commutative frobenius algebras,\nmaking Rel a hypergraph category in the sense of Fong and Spivak [FS18b]. Note\nthat cup = ∇; ϵ and cap = η; ∆, moreover it is easy to show that the snake equations\nfollow from the axioms of special commutative Frobenius algebras. Finally, we can\nequip the hom-sets Rel(A, B) with a preorder structure given by:\nR ≤S ⇐⇒(aRb =⇒aSb) .\nIn category theory, this situation is known as a preorder enrichment. Equipped with\nthis preorder enrichment, Rel forms a Cartesian bicategory in the sense of Carboni\nand Walters [CW87].\n2.4.3\nGraphical conjunctive queries\nBonchi, Seeber and Sobocinski [BSS18] introduced graphical conjunctive queries (GCQ),\na graphical calculus where query evaluation and containment are captured by the ax-\nioms of the free Cartesian bicategory CB(Σ) generated by a relational signature Σ.\nCartesian bicategories were introduced by Carboni and Walters [CW87] as an axioma-\ntisation of categories of relations, they are hypergraph categories where every hom-set\nhas a partial order structure akin to subset inclusion between relations. We review\nthe correspondence of Bonchi et al. [BSS18], between conjunctive queries and mor-\nphisms of free cartesian bicategories. We refer to Appendix 2.4.1 for an introduction\nto relational datatabases with examples.\nDeﬁnition 2.4.13 (Cartesian bicategory). [CW87] A cartesian bicategory C is a\nhypergraph category enriched in preorders and where the preorder structure interacts\nwith the hypergraph structure as follows:\n≤\n≤\n≤\n,\n,\n,\nR\n≤\nR\nR\nR\n≤\n,\n.\na\na\na\na\na\na\nb\na\na\na\na\na\na\na\nb\nb\nb\nb\na\na\na\na\n(2.21)\nfor all objects a, b ∈C0 and morphisms R : a →b.\nA morphism of Cartesian\nbicategories is a strong monoidal functor which preserves the partial order, the monoid\nand the comonoid structure.\n129\nChapter 2\nFunctors for Semantics\nWe recall the basic notions of relational databases and conjunctive queries. A\nrelational signature is a set of symbols Σ equipped with an arity function ar : Σ →\nN. This is used to deﬁne a relational structure as a mathematical abstraction of a\ndatabases.\nDeﬁnition 2.4.14 (Relational structure). A relational structure K over a signature\nΣ, also called a Σ-structure, is given by a set U called the universe and an inter-\npretation K(R) ⊆U ar(R) for every symbol R ∈Σ. We denote by MΣ the set of\nΣ-structures with ﬁnite universe U(K).\nGiven two Σ-structures K, K′, a homomorphism f : K →K′ is a function f :\nU(K) →U(K′) such that ∀R ∈Σ ∀⃗x ∈U ar(R) · ⃗x ∈K(R) =⇒f(⃗x) ∈K′(R).\nLet X be a (countable) set of variables, Σ a relational signature and consider the\nlogical formulae generated by the following context-free grammar:\nϕ ::= ⊤| x = x′ | ϕ ∧ϕ | ∃x · ϕ | R(⃗x)\nwhere x, x′ ∈X, R ∈Σ and ⃗x ∈X ar(R).\nLet us denote the variables of ϕ by\nvar(ϕ) ⊆X, its free variables by fv(ϕ) ⊆var(ϕ) and its atomic formulae by\natoms(ϕ) ⊆`\nR∈Σ var(ϕ)ar(R), i.e. an atomic formula is given by R(x1, . . . , xar(R)) for\nsome variables xi ∈X. This fragment is called regular logic in the category-theory\nliterature [FS18a]. It yields conjuntive queries via the prenex normal form.\nDeﬁnition 2.4.15. Conjunctive queries ϕ ∈QΣ are the prenex normal form ϕ =\n∃x0 · · · ∃xk · ϕ′ of regular logic formulae, for the bound variables { x0, . . . , xk } =\nvar(ϕ) \\ fv(ϕ) and ϕ′ = V atoms(ϕ). We denote by :\nQΣ(k) = {ϕ ∈QΣ | fv(ϕ) = k}\nthe set of conjunctive queries with k free variables.\nProposition 2.4.16. There is a bijective correspondence between relational structures\nwith signature σ : Σ →N = { x }∗and monoidal functors K : Σ →Rel such that\nK(x) = U.\nProof. Given a schema dom : Σ →A∗, the data for a monoidal functor K : Σ →Rel\nis an assignment of each a ∈A to a set of data-values Da = K(a) and of each symbol\nR ∈Σ to a relation K(R) ⊆Q\na∈dom(R) Da. This is precisely the data of a relational\ndatabase. Relational structures are a sub-example with A = { x }.\nBonchi, Seeber and Sobocinski show that queries can be represented as diagrams\nin the free cartesian bicategory, and that this translation is semantics preserving.\nLet CB(Σ) be the free Cartesian bicategory generated by one object x and arrows\n{ R : 1 →xar(R) }R∈Σ, see [BSS18, def. 21].\nProposition 2.4.17. ([BSS18, prop. 9, 10]) There is a two-way translation between\nformulas and diagrams:\nΘ : QΣ ⇆CB(Σ) : Λ\nwhich preserves the semantics, i.e. such that for all ϕ, ϕ′ ∈QΣ we have ϕ ⊆ϕ′ ⇐⇒\nΘ(ϕ) ≤Θ(ϕ′), and for all arrows d, d′ ∈CB(Σ), d ≤d′ ⇐⇒Λ(d) ⊆Λ(d′).\n130\nRelational models\nSection 2.4\nProof. The translation is deﬁned by induction from the syntax of regular logic for-\nmulae to that of GCQ diagrams and back. Note that given ϕ ∈QΣ with |fv(ϕ)| = n,\nwe have Θ(ϕ) ∈CB(Σ)(0, n) and similarly we have fv(Λ(d)) = m + n for d ∈\nCB(Σ)(m, n), i.e. open wires correspond to free variables.\nExample 2.4.18. The translation works as follows. Given a morphism in CB(Σ),\nnormalized according to 1.6.5, the spiders are interpreted as variables and the boxes\nas relational symbols. For example, assuming A, B, C ∈Σ, the following morphism\nf : x →x ∈CB(Σ)\nA\nB\nC\nis mapped to the query:\nΛ(f) = ∃x1, x2 · A(x0, x1) ∧B(x0, x1, x2) ∧C(x1, x3)\nThe query from Example 2.4.10, is mapped by Θ to the diagram:\nread\nBruno\nwrote\nProposition 2.4.19. Let [CB(Σ), Rel] denote the set of morphisms of Cartesian\nbicategories, there are bijective correspondences between closed diagrams in CB(Σ),\nformulas in QΣ with no free variables and models with signature Σ.\nCB(Σ)(0, 0)\n(1)\n≃\n{ ϕ ∈QΣ | fv(ϕ) = ∅}\n(2)\n≃\nMΣ\n(3)\n≃\n[CB(Σ), Rel]\nProof. (1) follows from theorem 2.4.17, (2) from theorem 2.4.12 and (3) follows from\nproposition 2.4.16 since any monoidal functor Σ →Rel induces a morphism of Carte-\nsian bicategories CB(Σ) →Rel.\n2.4.4\nRelational models\nWe have seen that relational databases are functors K : Σ →Rel from a relational\nsignature Σ. It is natural to to generalise this notion by considering functors G →Rel\nwhere G is a formal grammar. Any of the formal grammars studied in Chapter 1 may\nbe used to build a relational model. However, it is natural to pick a grammar G\nthat we can easily interpret in Rel. In other words, we are interested in grammars\nwhich have common structure and properties with the category of relations. Recall\nthat Rel is compact-closed with the diagonal and its transpose as cups and caps.\nThis makes rigid grammars particularly suited for relational semantics, since we can\ninterpret cups and caps using the compact closed structure of Rel.\n131\nChapter 2\nFunctors for Semantics\nDeﬁnition 2.4.20 (Relational model). A relational model is a rigid monoidal functor\nF : G →Rel where G is a rigid grammar.\nWe illustrate relational models with an example.\nExample 2.4.21 (Truth values). Let us ﬁx the vocabulary V = U + { read, wrote },\nwhere U = Dw ∪Db ∪Dr is the set of data values from Example 2.4.1. Consider the\npregroup grammar deﬁned by the following lexicon:\n∆(x) = { n } ,\n∆(read) = ∆(wrote) = { nrsnl }\nfor all x ∈U ⊆V . We build a functor F : ∆→Rel, deﬁned on objects by F(n) = U,\nand F(w) = 1 = F(s) for all w ∈V , on proper nouns by F(x →n) = { x } ⊆U for\nx ∈U ⊆V and on verbs as follows:\nread\nnr s nl\nwrote\nnr s nl\nρ\nU\nU\n7→\n,\nρ\nU U\n7→\nwhere ρ : 1 →U⊗U⊗U ∈Rel is the table (relation) from Example 2.4.1. Interpreting\ncups and caps in RC(∆) with their counterparts in Rel, we can evaluate the semantics\nof the sentence g : Spinoza read Bruno →s:\nSpinoza\nBruno\nread\n7→\nS\nB\nρ\nobtaining a morphisms F(g) : 1 →1 ∈Rel, which is simply a truth value given by\nthe evaluation of the following query:\nF(g) = ⊤⇐⇒∃x, y ∈U · F(Spinoza)(x) ∧F(read)(x, y) ∧F(Bruno)(y)\nwhich is true with our deﬁnition of F.\nFrom this example, we see that relational models can be used to give a truth\ntheoretic semantics by interpreting the sentence type s as the unit of the tensor in\nRel. We can also use these models to answer questions.\nExample 2.4.22 (Questions). Add a question type q ∈B and the question word\nWho ∈V to the pregroup grammar above with ∆(Who) = q sl n. Then there is a\ngrammatical question gq : Who read De Causa →s in RC(∆) given by the following\ndiagram:\nDe Causa\nWho\nread\n?\n132\nRelational models\nSection 2.4\nLet F(q) = U and F(Who →q sl n) = capU ⊆U ⊗U. Then evaluating the question\nabove in F yields F(gq) = { Spinoza, Florio } ⊆U.\nAs shown by Sadrzadeh et al. [SCC14], we may give semantics to the relative\npronoun “that” using the Frobenius algebra in Rel.\nExample 2.4.23 (Relative pronouns). Add the following lexical entries:\n∆(that) = { nr n sl nll, nr n sl n } ,\n∆(a) = { d } ,\n∆(book) = { drn } .\n. Then there is a grammatical question g′\nq : Who read a book that Bruno wrote →q\nin RC(∆) given by the following diagram:\na\nWho\nread\nbook\nthat\nBruno wrote ?\nLet F(d) = 1, F(a →d) = ⊤, F(that →nr n sl n) = ν · δ · (δ ⊗idU) : 1 →U 3 (the\nspider with 3 outputs and 0 inputs) and F(book →dr n) = Db ⊆U. Then evaluating\nthe question above in F yields F(g′\nq) = { Spinoza, Florio, Galileo } ⊆U, as expected.\nThe ﬁrst linguistic problem that we consider is the task of computing the semantics\nof a sentence u ∈L(G) for a pregroup grammar G in a given relational model F : G →\nRel. Throughout this section and the next, we assume that G is a rigid grammar.\nDeﬁnition 2.4.24. RelSemantics(G)\nInput:\ng ∈RC(G)(u, s), F : G →Rel\nOutput:\nF(g)\nSince Rel is a cartesian bicategory, any monoidal functor from G to Rel must\nfactor through a free cartesian bicategory.\nLemma 2.4.25. Let G = (V, B, ∆, s) be a pregroup grammar. Any relational model\nF : RC(G) →Rel factors through a free cartesian bicategory RC(G) →CB(Σ) →\nRel, where Σ is obtained from ∆via the map P(B) →B∗which forgets the adjoint\nstructure.\nProof. This follows from the universal property of the free cartesian bicategory.\nThis lemma allows to reduce the problem of computing the semantics of sentences\nin Rel to Evaluation, thus proving its membership in NP.\nProposition 2.4.26. There is a logspace reduction from RelSemantics(G) to con-\njunctive query Evaluation, hence RelSemantics ∈NP.\n133\nChapter 2\nFunctors for Semantics\nProof. The factorisation K ◦L = F of lemma 2.4.25 and the translation Λ of theo-\nrem 2.4.17 are in logspace, they give a query ϕ = Λ(L(r)) ∈Q∆such that eval(ϕ, K) =\nF(r).\nThe queries that arise from a pregroup grammar are a particular subclass of\nconjunctive queries. This leads to the question: what is the complexity of Evaluation\nfor this class of conjunctive queries? We conjecture that these queries have bounded\ntreewidth, i.e. that they satisfy the tractability condition for the CSP dichotomy\ntheorem [Bul17].\nConjecture 2.4.27. For any pregroup grammar G, RelSemantics(G) is poly-time\ncomputable in the size of (u, s) ∈List(V ) × P(B) and in the size of the functor F.\n2.4.5\nEntailment and question answering\nWe have seen that relational models induce functors L : G →CB(Σ), turning sen-\ntences into conjunctive queries. Thus we can test whether a sentence u ∈L(G) entails\na second sentence u′ ∈L(G) by checking containment of the corresponding queries.\nMore generally, we may consider models in a ﬁnitely presented cartesian bicategory\nC, i.e. a cartesian bicategory equipped with a ﬁnite set of existential rules of the\nform ∀x0 · · · ∀xk · ϕ →ϕ′ for ϕ, ϕ′ ∈CB(Σ) with fv(ϕ) = fv(ϕ′) = { x0, . . . , xk }.\nThese are also called tuple-generating dependencies in database theory [Tho13]. They\nwill allow us to model more interesting forms of entailment in natural language.\nDeﬁnition 2.4.28 (CB model). A CB model for a rigid grammar G is a monoidal\nfunctor L : G →C where C is a ﬁnitely presented Cartesian bicategory.\nExample 2.4.29. Take C to be the Cartesian bicategory generated by the signature\nΣ = { Leib, Spi, inﬂ, calc, phil, ... } as 1-arrows with codomain given by the function\nar : Σ →N and the following set of 2-arrows:\nread\nLeib\n≤\ndisc\ncalc\ninﬂ\n≤\nphil\n≤\n,\n.\nThe composition of 2-arrows in C allows us to compute entailment, e.g.:\nLeib\nread\nSpin\nLeib\ninﬂ\nSpin\n≤\n≤\nSpin\ninﬂ\nLeib\n≤\nSpin\ninﬂ\nphil\ndisc\ncalc\nSpin\ninﬂ\nLeib\nLeib\n≤\nwhere the second and third inequations follow from the axioms of deﬁnition 2.4.13,\nthe ﬁrst and last from the generating 2-arrows.\n134\nRelational models\nSection 2.4\nStarting from the pregroup grammar G deﬁned in 2.4.4, and adding the following\nlexical entries:\n∆(inﬂuenced) = ∆(discovered) = { nrsnl } , ∆(calculus) = { n } , ∆(philosopher) = { drn } .\nWe may construct a functor L : G →CB(Σ) given on objects by L(w) = L(s) = 1\nand L(n) = x, and on arrows by sending every lexical entry to the corresponding\nsymbol in Σ except for the question word “who” which is interpreted as a cap and\nthe functional word “that” which is interpreted as a spider with three outputs. Then\none may check that the image of the sentence “Spinoza inﬂuenced a philosopher that\ndiscovered calculus” is grammatical in G and that the corresponding pregroup reduction\nis mapped via L to the last diagram in the derivation above.\nDeﬁnition 2.4.30. Entailment\nInput:\nr ∈RC(G)(u, s),\nr′ ∈RC(G)(u′, s),\nL : RC(G) →C\nOutput:\nL(r) ≤L(r′)\nProposition 2.4.31. Entailment is undecidable for ﬁnitely presented Cartesian bi-\ncategories. When C is freely generated (i.e. it has no existential rules), the problem\nreduces to conjunctive query Containment.\nProof. Entailment of conjunctive queries under existential rules is undecidable, see\n[BM02].\nWhen C = CB(Σ) is freely generated by a relational signature Σ, i.e.\nwith no existential rules, theorem 2.4.17 yields a logspace reduction to Containment:\nEntailment ∈NP.\nWe now consider the following computational problem: given a natural language\ncorpus and a question, does the corpus contain an answer? We show how to trans-\nlate a corpus into a relational database so that question answering reduces to query\nevaluation.\nIn order to translate a corpus into a relational database, it is not suﬃcient to\nparse every sentence independently, since the resulting queries will have disjoint sets\nof variables. The extra data that we need is a coreference resolution, which allows\nto link the common entities mentioned in these sentences. In 1.6.2, we deﬁned a\nnotion of pregroup grammar with coreference G which allows to represent a corpus\nof k sentences as one big diagram C ∈Coref(G), assuming that both the pregroup\nparsing and the coreference resolution have been performed. In order to interpret a\npregroup grammar with coreference G = (V, B, ∆, I, R, s) in a cartesian bicategory\nC, it is suﬃcient to ﬁx a CB model from the pregroup grammar (V, B, ∆, I, s) into\nC and choose an image for the reference types in R. Then the coreference resolution\nis interpreted using the Frobenius algebra in C.\nFix a pregroup grammar with coreference G and a CB model L : Coref(G) →\nCB(Σ) with L(s) = 0, i.e. grammatical sentences are mapped to closed formulae. We\nassume that L(q) = L(a) for q and a the question and answer types respectively, i.e.\nboth are mapped to queries with the same number of free variable. Lexical items such\nas “inﬂuence” and “Leibniz” are mapped to their own symbol in the relational signature\n135\nChapter 2\nFunctors for Semantics\nΣ, whereas functional words such as relative pronouns are sent to the Frobenius\nalgebra of CB(Σ), see 2.4.4 or [SCC13].\nWe describe a procedure for answering a question given a corpus. Suppose we\nare given a corpus u ∈V ∗with k sentences. Parsing and resolving the coreference\nyields a morphism C ∈Coref(G)(u, sk). Using L we obtain a relational database\nfrom C given by the canonical structure induced by the corresponding query K(C) =\nCM(L(C)), we denote by E := var(L(C)) the universe of this relational structure.\nGiven a parsed question g : v →q ∈Coref(G), we can answer the question g by\nevaluating it in the model K(C).\nDeﬁnition 2.4.32. QuestionAnswering\nInput:\nC ∈Coref(G)(u, sk), g ∈Coref(G)(v, q)\nOutput:\nEvaluation(K, L(g)) ⊆Efv(L(g))\nwhere K = CM(L(C))\nProposition 2.4.33. QuestionAnswering is NP −complete.\nProof. Membership follows immediately by reduction to Evaluation. Hardness fol-\nlows by reduction from Evaluation. Indeed ﬁx any relational structure K and query\nϕ, using Proposition 1.6.10, we may build a corpus C ∈Coref(G) and a question\ng ∈Coref(G) such that L(C) = K and L(g) = ϕ.\nNote that this is an asymptotic result. In practice, the questions we may ask are\nsmall compared to the size of the corpus. This would make the problem tractable\nsince Evaluation is only NP-complete in the combined size of database and query,\nbut it becomes polytime computable when the query is ﬁxed [Tho13].\n136\nTensor network models\nSection 2.5\n2.5\nTensor network models\nTensors arose in the work of Ricci and Levi-Civita in the end of the 19th century\n[RL00]. They were adopted by Einstein [Ein16], who used repeated indices to de-\nnote their compositions, and applied to quantum mechanics by Heisenberg [KH25] to\ndescribe the possible states of quantum systems.\nIn the 1970s, Penrose introduced a diagrammatic notation for manipulating tensor\nexpressions [Pen71]: wires represent vector spaces, nodes represent multi-linear maps\nbetween them. This work was one of the main motivations behind Joyal and Street’s\ngraphical calculus for monoidal categories [JS91], which was later adopted in the de-\nvelopment of Categorical Quantum Mechanics (CQM) [AC08]. The same notation\nis widely used in the Tensor Networks (TN) community [Eis13; BMT15; DEB19].\nUntil recently, CQM and TN remained separated ﬁelds because they were interested\nin diﬀerent aspects of these graphical networks. On the one hand, rewriting and ax-\niomatics. On the other, fast methods for tensor contraction and complexity theoretic\nguarantees. These two lines of research are now seeing a fruitful exchange of ideas\nas category theorists become more applied and vice-versa. For instance, rewriting\nstrategies developed in the context of categorical quantum mechanics can be used to\nspeed-up quantum computations [Kv20], or solve satisﬁability and counting problems\n[dKM20; TM21].\nThe tools and methods developed by these communities are ﬁnding many appli-\ncations in artiﬁcial intelligence. Tensor networks are widely used in machine learn-\ning, supported by eﬃcient contraction tools such as Google’s TensorNetwork library\n[EHL19], and are beginning to be applied to natural language processing [PV17;\nZha+19]. Distributional Compositional models of meaning [CCS08; CCS10] (Dis-\nCoCat) arise naturally from Categorical Quantum Mechanics [AC08; CK17], In a\nnutshell, CQM provides us with graphical calculi to reason about tensor networks,\nand DisCoCat provides a way of mapping natural language to these calculi so that\nsemantics is computed by tensor contraction.\nIn this section, we establish a formal connection between tensor networks and\nfunctorial models. This allows us to transfer complexity and tractability results from\nthe TN literature to the DisCoCat models of meaning. In particular, we show that\nDisCoCat models based on dependency grammars can be computed in polynomial\ntime. We end by discussing an extension of these tensor-based models where bubbles\nare used to represent non-linear operations on tensor networks.\n2.5.1\nTensor networks\nIn this section, we review the basic notions of tensor networks. We take as a starting\npoint the deﬁnition of tensor networks used in the complexity theory literature on\nTNs [AL10; OGo19; GK20].\nLet us denote an undirected graph by (V, E) where V is a ﬁnite set of vertices\nand E ⊆{ { u, v } | u, v ∈V } is a set of undirected edges. The incidence set of a\nvertex v is I(v) = { e ∈E | v ∈e } and the degree of v is the number of incident edges\ndeg(v) = |I(v)|. An order n tensor T of shape (d1, . . . , dn) with di ∈N is a function\n137\nChapter 2\nFunctors for Semantics\nT : [d1] ⊗· · · ⊗[dn] →S where [di] = { 1, . . . , di } is the ordered set with di elements\nand S is a semiring of numbers (e.g. B, N, R, C), see 2.1.\nDeﬁnition 2.5.1 (Tensor network). A tensor network (V, E, T) over a semiring S is\na undirected graph (V, E) with edge weights dim : E →N and a set of tensors T =\n{ Tv }v∈V such that Tv is a tensor of order deg(v) and shape (dim(e0), . . . , dim(edeg(v)))\nfor ei ∈I(v) ⊆E. Each edge e ∈E corresponds to an index i ∈[dim(e)] along which\nthe adjacent tensors are to be contracted.\nThe contration of two tensors T0 : [m] ⊗[d] →S and T1 : [d] ⊗[l] →S along their\ncommon dimension [d] is a tensor T0 · T1 : [k] ⊗[l] →S with:\nT0 · T1(i, j) =\nX\nk∈[d]\nT0(i, k)T1(k, l)\nIf T0 : [m] →S and T1 : [l] →S do not have a shared dimension then we still denote by\nT0·T1 : [m]⊗[l] →S, the outer (or tensor) product of T0 and T1 given by T0·T1(i, j) =\nT0(i)T1(j). Note that this is suﬃcient to deﬁne the product T0 · T1 for tensors of\narbitary shape since (d0, . . . , dn) shaped tensors are in one-to-one correspondence\nwith order 1 tensors of shape (d0d1 . . . dn).\nWe are interested in the value contract(V, E, T) of a tensor network which is the\nnumber in S obtained by contracting the tensors in (V, E, T) along their shared edges.\nWe may deﬁne this by ﬁrst looking at the order of contractions, called bubbling in\n[ALM07; AL10].\nDeﬁnition 2.5.2 (Contraction order). [OGo19] A contraction order π for (V, E, T)\nis a total order of its vertices, i.e. a bijection π : [n] →V where n = |V |.\nGiven a contraction order π for (V, E, T) we get an algorithm for computing the\nvalue of (V, E, T), given by contract(V, E, T) = An where:\nA0 = 1 ∈S\nAi = Ai−1 · Tπ(i)\nOne may check that the value An obtained is independent of the choice of contraction\norder π, although the time and space required to compute the value may very well\ndepend on the order of contractions [OGo19]. We can now deﬁne the general problem\nof contracting a tensor network.\nDeﬁnition 2.5.3. Contraction(S)\nInput:\nA tensor network (V, E, T) over S\nOutput:\ncontract(V, E, T)\nProposition 2.5.4. Contraction(S) is #P-complete for S = N, R+, R, C, it is NP-\ncomplete for S = B.\nProof. #P-hardness was proved in [BMT15] by reduction from #SAT. NP-completeness\nwhen S = B follows by equivalence with conjunctive query evaluation.\n138\nTensor network models\nSection 2.5\nEven though contracting tensor networks is a hard problem in general, it becomes\ntractable, in several restricted cases of interest.\nLet us look a bit more closely at the process of contraction. Recall that given two\nn ⊗n matrices M0, M1 the time complexity of matrix multiplication is in general n3.\nThis is the simplest instance of a tensor contraction which can be depicted graphically\nas follows:\nM0\nM1\nd\nd\nd\nThe standard way to compute this contraction is with two for loops over the outgoing\nwires for each basis vector in the middle wire, resulting in the cubic complexity.\nThere are Strassen-like algorithms with a better asymptotic runtime, but we do not\nconsider these here. Now suppose M0 and M1 are tensors with indices of dimension\nn connected by one edge, with k0 and k1 outgoing edges respectively:\nM0\nM1\n...\n...\nk0\nk1\nThen in order to contract the middle wire, we need k0 + k1 for loops for each basis\nvector in the middle wire, resulting in the complexity nk0+k1+1. In particular if we have\n0 edges connecting the tensors, i.e. we are taking the outer product, then the time\ncomplexity is O(nk0+k1). Similarly, if we have k2 parallel edges connecting the tensors,\nthe time complexity is O(nk0+k1+k2). We can speed up the computation by slicing\n[GK20], i.e. by parallelizing the computation over k2 · n GPUs, where each computes\nthe for loops for a basis vector of the middle wires, reducing the time complexity to\nnk0+k1. This is for instance enabled by Google’s Tensor Network library [EHL19].\nFor a general tensor network (V, E, T), we have seen that the contraction strategy\ncan be represented by a contraction order π : [|V |] →V , this may be viewed as a\nbubbling on the graph (V, E):\nAt each contraction step i, we are contracting the tensor Ai−1 enclosed by the bubble\nwith Tπ(i). From the considerations above we know that the time complexity will\ndepend on the number of edges crossing the bubble at each time step. For instance\nin the bubbling above, there are at most 3 edges crossing a bubble at each time step,\nWe can capture this complexity using the notion of bubblewidth [ALM07], also known\nas the cutwidth [OGo19], of a tensor network.\nDeﬁnition 2.5.5 (Bubblewidth). The bubblewidth of a contraction order π : [n] →V\nfor a graph (V, E) is given by:\nbubblewidth(π) = maxj∈[n] |{{ π(j), π(k) } ∈E | i ≤j < k}|)\n139\nChapter 2\nFunctors for Semantics\nThe bubblewidth of a tensor network (V, E) is the minimum bubblewidth of a contrac-\ntion order π for (V, E):\nBW(V, E) = minπ:[n]→V (bubblewidth(π))\nInterestingly, the bubblewidth of a graph can be used to bound from above the\nmuch better studied notions of treewidth and pathwidth. Let us denote by PW(V, E)\nand TW(V, E) the path width and tree width of a graph (V, E).\nProposition 2.5.6 (Aharonov et al. [ALM07]).\nTW(V, E) ≤PW(V, E) ≤2BW(V, E)\nThe following proposition is a weaker statement then the results of O’Gorman\n[OGo19], that we can prove using the simple considerations about contraction time\ngiven above.\nProposition 2.5.7. Any tensor network (V, E, T) can be contracted in O(nBW(d)+a)\ntime where n = maxe∈E(dim(e)) is the maximum dimension of the edges, BW(d) is\nthe bubble width of d and a = maxv∈V (deg(v)) is the maximum order of the tensors\nin T.\nProof. At each contraction step i, we are contracting the tensor Ai−1 enclosed by the\nbubble with Tπ(i). From the considerations above we know that the time complexity of\nsuch a contraction is at most O(nwi+deg(π(i))) where wi is the number of edges crossing\nthe i-th bubble and deg(π(i)) is the degree of the i-th vertex in the contraction order\nπ. The overall time complexity is the sum of these terms, which is at most of the\norder O(nBW(d)+a).\nTherefore tensor networks of bounded bubblewidth may be evaluated eﬃciently.\nCorollary 2.5.8. Contraction(S) can be computed in poly-time if the input tensor\nnetworks have bounded bubblewidth and bounded vertex degree.\n2.5.2\nTensor functors\nWe now show that tensor networks may be reformulated in categorical language as\ndiagrams in a free compact-closed category equipped with a functor into the category\nof matrices over a commutative semiring MatS, deﬁned in 2.1. From this perspective,\nthe contraction order π provides a way of turning the compact-closed diagram into a\npremonoidal one, where the order of contraction is the linear order of the diagram.\nProposition 2.5.9. There is a semantics-preserving translation between tensor net-\nworks (V, E, T) over S and pairs (F, d) of a diagram d : 1 →1 ∈CC(Σ) in a free\ncompact closed category and a monoidal functor F : Σ →MatS. This translation is\nsemantics-preserving in the sense that contract(V, E, T) = F(d).\n140\nTensor network models\nSection 2.5\nProof. Given a tensor network (V, E, T) we build the signature Σ = V\ncod\n−−→E∗where\ncod(v) = I(v) is (any ordering of) the incidence set of vertex v, i.e. vertices correspond\nto boxes and edges to generating objects.\nThe set of tensors { Tv }v∈V induces a\nmonoidal functor F : Σ →MatS given on objects e ∈E by F(e) = dim(e) and on\nboxes v ∈V by F(vi) = Ti : 1 →dim(e0) ⊗· · · ⊗dim(edeg(v)) for ei ∈cod(vi) = I(v).\nWe build a diagram d : 1 →1 ∈CC(Σ) by ﬁrst tensoring all the boxes in Σ and then\ncomposing it with a morphism made up only of cups and swaps, where there is a cap\nconnecting an output port of box v ∈Σ with an output port of v′ ∈Σ, whenever\n{ v, v′ } ∈E.\n7→\nOne can check that F(d) = contract(V, E, T) since composing T0 ⊗T1 : 1 →m ⊗\nn ⊗n ⊗l with idm ⊗cupn ⊗idl in MatS corresponds to contracting the tensors T0\nand T1 along their common dimension n. For the other direction, it is easy to see\nthat any closed diagram in CC(Σ) induces a graph with vertices Σ and edges given\nby the struture of the diagram. Then a functor F : Σ →MatS yields precisely the\ndata of the tensors { Tv }v∈Σ where Tv = F(v).\nIt is often useful to allow a special type of vertex in the tensor network. These\nare called COPY tensors in the tensor network literature [BMT15; OGo19; GK20],\nwhere they are used for optimizing tensor contraction. They also appear throughout\ncategorical quantum mechanics [CK17] — and most prominently in the ZX calculus\n[van20] — where they are called spiders. We have seen in 1.6.1 that diagrams with\nspiders are morphisms in a free hypergraph category Hyp(Σ).\nSince Hyp(Σ) ≃\nCC(Σ + Frob)/ ∼= (see 1.6.1), Proposition 2.5.9 can be used to show that there is\na semantics-preserving translation between tensor networks with COPY tensors and\npairs (F, d) of a diagram d ∈Hyp(Σ) and a functor F : Σ →MatS.\nExample 2.5.10 (Conjunctive queries as TNs). Conjunctive queries are tensor net-\nworks over B. Tensor networks also subsume probabilistic graphical models by taking\nthe underlying category to be Prob or MatR+ [Gla+19].\nWe now consider the problem of evaluating closed diagrams d : 1 →1 ∈CC(Σ)\nusing a tensor functor F : Σ →MatS\nDeﬁnition 2.5.11. FunctorEval(S)\nInput:\nΣ a monoidal signature, d : 1 →1 ∈CC(Σ), F : Σ →MatS,\nOutput:\nF(d)\nProposition 2.5.12. FunctorEval(S) is equivalent to Contraction(S)\nProof. This follows from Proposition 2.5.9.\n141\nChapter 2\nFunctors for Semantics\nThe notion of contraction order has an interesting categorical counterpart.\nIt\ngives a way of turning a compact-closed diagram into a premonoidal one, such that\nthe width of the diagram is the bubblewidth of the contraction order.\nProposition 2.5.13. There is a semantics-preserving translation between tensor net-\nworks (V, E, T) over S with a contraction order π : [|V |] →V and pairs (F, d) of a\ndiagram d : 1 →1 ∈PMC(Σ + swap) in a free premonoidal category with swaps and\na monoidal functor F : Σ →MatS. This translation is semantics-preserving in the\nsense that contract(V, E, T) = F(d). Moreover, we have that\nbubblewidth(π) = width(d)\nwith width(d) as deﬁned in 1.3.4.\nProof. We can use Proposition 2.5.9 to turn (V, E, T) into a compact closed diagram\nd : 1 →1 ∈CC(Σ) over a signature Σ with only output types for each box. Given\na contraction order π : [n] →V , we modify the signature Σ by transposing a output\nport e = { v, v′ } ∈E of v ∈V into an input whenever π−1(v) > π−1(v′) forming a\nsignature Σπ Then we can construct a premonoidal diagram\nπ(d) = ◦k\ni=1(permi ⊗bi) ∈PMC(Σπ + swap)\nwhere permi ∈PMC(swap) is a morphism containing only swaps and identities. This\nis done by ordering the boxes according to π and pushing all the wires to the left, as\nin the following example:\nv1\nv2\nv4\nv3\nv5\nv6\nv1\nv2\nv4\nv3\nv5\nv6\n7→\n(2.22)\nNote that no cups or caps need to be added since the input/output types of the\nboxes in the signature have been changed accordingly. The bubblewidth of π is then\nprecisely the width of the resulting premonoidal diagram.\nWe may deﬁne the bubblewidth of a diagram d ∈CC(Σ) as the bubblewidth of\nthe corresponding graph via Proposition 2.5.9. Also we deﬁne the dimension of a\nfunctor F : Σ →MatS as follows:\ndim(F) = maxx∈Σ0(F(x))\nWe may now derive the consequences of Proposition 2.5.8 in this categorical context.\nProposition 2.5.14. FunctorEval(S) can be computed in polynomial time if the\ninput diagrams d have bounded bubblewidth and the input functors F have bounded\ndimension.\nProof. This follows from the conjunction of Proposition 2.5.8 and the reduction from\nFunctorEval to Contraction of Proposition 2.5.12.\n142\nTensor network models\nSection 2.5\n2.5.3\nDisCoCat and bounded memory\nDisCoCat models were introduced by Coecke et al. in 2008 [CCS08; CCS10]. In the\noriginal formalism, the authors considered functors from a pregroup grammar to the\ncategory of ﬁnite dimensional real vector spaces and linear maps (i.e. MatR). In\nthis work, we follow [Coe+18; dMT20] in treating DisCoCat models as functors into\ncategories of matrices over any semiring. These in particular subsume the relational\nmodels studied in 2.4 by taking S = B. As shown in Section 2.1, MatS is a compact-\nclosed category for any commutative semiring. Therefore, the most suitable choice of\nsyntax for this semantics are rigid grammars, for which we have a canonical way of\ninterpreting the cups and caps. Thus, in this section, the grammars we consider are\neither pregroup grammars 1.5.1, or dependency grammars 1.5.2, or pregroup grammar\nwith crossed dependencies 1.5.1 or with coreference 1.6.2.\nDeﬁnition 2.5.15. A DisCoCat model is a monoidal functor F : G →MatS for\na rigid grammar G and a commutative semiring S.\nThe semantics of a sentence\ng : u →s ∈RC(G) is given by its image F(g). We assume that F(w) = 1 for w ∈V\nso that\nDistributional models F : G →MatR can be constructed by counting co-occurences\nof words in a corpus [GS11]. The image of the noun type n ∈B is a vector space\nwhere the inner product computes noun-phrase similarity [SCC13]. When applied to\nquestion answering tasks, distributional models can be used to compute the distance\nbetween a question and its answer [Coe+18].\nExample 2.5.16. As an example we may take the pregroup lexicon:\n∆(Socrate) = ∆(spettu) = { n }\n∆(jé) = { nrsnl, nrnrs }\nAnd we may deﬁne a DisCoCat model F : ∆→MatR with F(n) = 2, F(s) =\n1 and F(Socrate, n) = (1, 0) : 1 →2, F(spettu, n) = (−1, 0) and F(jé, nrsnl) =\nF(jé, nrnrs) = cap2 : 1 →2 ⊗2 in MatR. Then the following grammatical sentences:\nSocrate\njé\nspettu\nSocrate\njé\nspettu\nevaluate to a scalar in R given by the matrix multiplication:\nF(“Socrate jé spettu”) =\n\u0002\n1\n0\n\u0003 \u0014 1\n0\n0\n1\n\u0015 \u0014−1\n0\n\u0015\n= −1 = F(“Socrate spettu jé”)\nWe are interested in the complexity of the following semantics problem.\nDeﬁnition 2.5.17. TensorSemantics(G, S)\nInput:\ng ∈RC(G)(u, s), F : G →MatS\nOutput:\nF(g)\n143\nChapter 2\nFunctors for Semantics\nGiven a sentence diagram g ∈L(G), the pair (g, F) forms a tensor network. Com-\nputing the semantics of g in F amounts to contracting this tensor network. Therefore\nTensorSemantics(G, S) ∈#P. For a general rigid grammar, this problem is #P-hard,\nsince it subsumes the FunctorEval(S)(Σ) problem. When G is a pregroup grammar\nwith coreference, we can construct any tensor network as a list of sentences connected\nby the coreference, making the TensorSemantics(G, S) problem #P −complete.\nProposition 2.5.18. TensorSemantics(G, S) where G is a pregroup grammar with\ncoreference is equivalent to Contraction(S).\nProof. One direction follows by reduction to FunctorEval(S) since any model F :\nG →MatS factors through a free compact-closed category. For the other direction,\nﬁx any pair (F, d) of a diagram d : x →y ∈CC(Σ) and a functor F : Σ →MatS.\nUsing Proposition 1.6.10 we can build a corpus C : u →sk ∈Coref(G), where the\nconnectivity of the tensor network is encoded a coreference resolution, so that the\ncanonical functor Coref(G) →CC(Σ) maps C to d.\nFrom a linguistic perspective, a contraction order for a grammatical sentence\ng : u →s gives a reading order for the sentence and the bubblewidth is the maximum\nnumber of tokens (or basic types) that the reader should hold in memory in order to\nparse the sentence. Of course, in natural language there is a natural reading order\nfrom left to right which induces a “canonical” bubbling of g. In light of the famous\npsychology experiments of Miller [Mil56], we expect that the short-term memory\nrequired to parse a sentence is bounded, and more precisely that BW(g) = 7±2. For\npregroup diagrams generated by a dependency grammar, it is easy to show that the\nbubblewidth is bounded.\nProposition 2.5.19. The diagrams in RC(G) generated by a dependency grammar\nG have bubblewidth bounded by the maximum arity of a rule in G.\nProof. Dependency relations are acyclic, and the bubblewidth for an acyclic graph is\nsmaller than the maximum vertex degree of the graph, which is equal to the maximum\narity of a rule in G, i.e. the maximum nuber of dependents for a symbol plus one.\nTogether with Proposition 2.5.14, we deduce that the problem of computing the\ntensor semantics of sentences generated by a dependency grammar is tractable.\nCorollary 2.5.20. If G is a dependency grammar then TensorSemantics(G) can be\ncomputed in polynomial time.\nFor a general pregroup grammar we also expect the generated diagrams to have\nbounded bubblewidth, even though they are not acyclic. For instance, the cyclic\npregroup reductions given in 1.21 have constant bubblewidth 4, which is obtained\nby chosing a contraction order from the middle to the sides, even though the naive\ncontraction order of 1.21 from left to right has unbounded bubblewidth. We end\nwith a cojecture, since we were unable to show pregroup reductions have bounded\nbubblewidth in general.\nConjecture 2.5.21. Diagrams generated by a pregroup grammar G have bounded\nbubble width and thus TensorSemantics(G, S) can be computed in polynomial time.\n144\nTensor network models\nSection 2.5\n2.5.4\nBubbles\nWe end this section by noting that a simple extension of tensor network models\nallows to recover the full expressive power of neural networks. We have seen that\ncontraction strategies for tensor networks can be represented by a pattern of bubbles\non the diagram. These bubbles did not have any semantic interpretation and they\nwere just used as brackets, specifying the order of contraction. We could however\ngive them semantics, by interpreting bubbles as operators on tensors. As an example,\nconsider the following diagram, where each box Wi is interpreted as a matrix:\n. . .\nW0\nW1\nWk\n⃗x\nSuppose that each bubble acts on the tensor it encloses by applying a non-linearity σ\nto every entry. Then we see that this diagram speciﬁes a neural network of depth k\nand width n where n is the maximum dimension of the wires. With this representation\nof neural networks, we have more control over the structure of the network.\nC\nR\ncrossed the\nC\ncrossed\nR\nthe\nCaesar\nRubicon\ncrossed\nthe\nSuppose we have a pregroup grammar G0 and context-free grammar G1 over the same\nvocabulary V and let u ∈L(G0) ∩L(G1) with two parses g0 : u →s in RC(G0) and\ng1 : u →s in O(G1). Take the skeleton of the context-free parse g1, i.e. the tree\nwithout labels. This induces a pattern of bubbles, as described in Example 1.2.4 on\nPeirce’s alpha. Fix a tensor network model G0 →MatS for the pregroup grammar\nwith F(w) = 1 for w ∈V and choose an activation function σ : S →S. Then σ\ndeﬁnes a unary operator on homsets Sσ : Q\nx,y∈N MatS(x, y) →Q\nx,y∈N MatS(x, y)\ncalled a bubble, see [HS20] or [TYd21] for formal deﬁnitions. We can combine the\npregroup reduction with the pattern of bubbles as in the example above. We may\n145\nChapter 2\nFunctors for Semantics\nthen compute the semantics of u as follows. Starting from the inner-most bubble, we\ncontract the tensor network enclosed by it and apply the activation σ to every entry in\nthe resulting tensor. This gives a new bubbled network where the inner-most bubble\nhas been contracted into a box. And we repeat this process. The procedure described\nabove gives a way of controlling the non-linearities applied to tensor networks from\nthe grammatical structure of the sentence. While this idea is not formalised yet, it\npoints to a higher diagrammatic formalism in which diagrams with bubbles are used\nto control tensor computations. This diagrammatic notation can be traced back to\nPeirce and was recently used in [HS20] to model negation in ﬁrst-order logic and in\n[TYd21] to model diﬀerentiation of quantum circuits. We will also use it in the next\nsection to represent the non-linearity of knowledge-graph embeddings and in Chapter\n3 to represent the softmax activation function.\n146\nKnowledge graph embeddings\nSection 2.6\n2.6\nKnowledge graph embeddings\nRecent years have seen the rapid growth of web-scale knowledge bases such as Free-\nbase [Bol+08], DBpedia [Leh+14] and Google’s Knowledge Vault [Don+14]. These\nresources of structured knowledge enable a wide range of applications in NLP, in-\ncluding semantic parsing [Ber+13], entity linking [HOD12] and question answering\n[BCW14]. Knowledge base embeddings have received a lot of attention in the sta-\ntistical relational learning literature [Wan+17]. They approximate a knowledge base\ncontinuously given only partial access to it, simplifying the querying process and al-\nlowing to predict missing entries and relations — a task known as knowledge base\ncompletion.\nKnowledge graph embedding are of particular interest to us since they provide a\nlink between the Boolean world of relations and the continuous world of tensors. They\nwill thus provide us with a connection between the relational models of Section 2.4\nand the tensor network models of Section 2.5. We start by deﬁning the basic notions\nof knowledge graph embeddings. Then we review three factorization models for KG\nembedding, focusing on their expressivity and their time and space complexity. This\nwill lead us progressively from the category of relations to the category of matrices\nover the complex numbers with its convenient factorization properties.\n2.6.1\nEmbeddings\nMost large-scale knowledge bases encode information according to the Resource De-\nscription Framework (RDF), where data is stored as triples (head, relation, tail) (e.g.\n(Obama, BornIn, US)). Thus a knowledge graph is just a set of triples K ⊆E ×R×E\nwhere E is a set of entities and R a set of relations. This form of knowledge represen-\ntation can be seen as an instance of relational databases where all the relations are\nbinary.\nProposition 2.6.1. Knowledge graphs are in one-to-one correspondence with func-\ntors Σ →Rel where Σ = R is a relational signature containing only symbols of arity\ntwo.\nProof. This is easy to see, given a knowledge graph K ⊆E × R × E, we can build\na functor F : R →Rel deﬁned on objects by F(1) = E and on arrows by F(r) =\n{ (e0, e1)|(e0, r, e1) ∈K } ⊆E × E. Similarly any functor F : Σ →Rel deﬁnes a\nknowledge graph K = { (π1F(r), r, π2F(r)) | r ∈Σ } ⊆F(1) × Σ × F(1).\nHigher-arity relations can be encoded into the graph through a process known as\nreiﬁcation. To reify a k-ary relation R, we form k new binary relations Ri and a new\nentity eR so that R(e1, . . . , ek) is true iﬀ∀i we have Ri(eR, ei). Most of the literature\non embeddings focuses on knowledge graphs, and the results which we present in this\nsection follow this assumption. However, some problems with reiﬁcation have been\npointed out in the literature, and current research is aiming at extending the methods\nto knowledge hypergraphs [Fat+20], a direction which we envisage also for the present\nwork.\n147\nChapter 2\nFunctors for Semantics\nEmbedding a knowledge graph consists in the following learning problem. Starting\nfrom a knowledge graph K : E ×R×E →B, the idea is to approximate K by a scoring\nfunction X : E × R × E →R such that ∥σ(X) −K∥is minimized where σ : R →B\nis any activation function.\nThe most popular activation functions for knowledge graph embeddings are ap-\nproximations of the sign function which takes a real number to its sign ±1, where −1\nis interpreted as the Boolean 0 (false) and 1 is interpreted as the Boolean 1 (true). In\nmachine learning applications, one needs a diﬀerentiable version of sign such as tanh\nor the sigmoid function. In this section, we are mostly interested in the expressive\npower of knowledge graph embeddings, and thus we deﬁne “exact” embeddings as\nfollows.\nDeﬁnition 2.6.2 (Embedding). An exact embedding for a knowledge graph K ⊆\nE × R × E, is a tensor X : E × R × E →R such that sign(X) = K.\nEven though sign is not a homomorphism of semirings between R and B, it allows\nto deﬁne a notion of sign rank which has interesting links to measures of learnability\nsuch as the VC-dimension [AMY16].\nDeﬁnition 2.6.3 (Sign rank). Given R ∈MatB the sign rank of R, denoted rank±(R)\nis given by:\nrank±(R) = min{rank(X) | sign(X) = R , X ∈MatR}\nThe sign rank of a relation R is often much lower than its rank. In fact, we don’t\nknow any relation R : n →n with rank±(R) > √n. The identity Boolean matrix\nn →n has sign rank 3 for any n [AMY16]. Therefore any permutation of the rows\nand columns of an identity matrix has sign rank 3, an example is the relation “is\nmarried to”. For the factorization models studied in this section, this means that\nthe dimension of the embedding is potentially much smaller than then the number of\nentities in the knowledge graph.\nSeveral ways have been proposed for modeling the scoring function X, e.g. using\ntranslations [Bor+13] or neural networks [Soc+13b].\nWe are mostly interested in\nfactorization models where X : E × R × E →R is treated as a tensor with three\noutputs, i.e. a state in MatR. Assuming that X admits a factorization into smaller\nmatrices allows to reduce the search space for the embedding while decreasing the\ntime required to predict missing entries. The space complexity of the embedding is\nthe amount of memory required to store X. The time complexity of an embedding\nis the time required to compute σ(X |s, v, o⟩) given a triple.\nThese measures are\nparticularly relevant when it comes to embedding integrated large-scale knowledge\ngraphs. The problem then boils down to ﬁnding a good ansatz for such a factorization,\ni.e. an assumed factorization shape that reduces the time and space complexity of\nthe embedding.\n2.6.2\nRescal\nThe ﬁrst factorization model for knowledge graph embedding — known as Rescal —\nwas proposed by Nickel et al. in 2011 [NTK11]. It models the real tensor X with the\n148\nKnowledge graph embeddings\nSection 2.6\nfollowing ansatz:\nE\nE\nn\nn\nX\nE\nR\nE\n=\nE\nE\nR\nW\n(2.23)\nwhere E : |E| →n is the embedding for entities, W : n ⊗|R| ⊗n →1 is a real tensor,\nand n is a hyper-parameter determining the dimension of the embedding.\nThe well-known notion of rank can be used to get bounds on the dimension of an\nembedding model. It is formalised diagrammatically as follows.\nDeﬁnition 2.6.4 (Rank). The rank of a tensor X : 1 →⊗n\ni=1di in MatS is the\nsmallest dimension k such that X factors as X = ∆n\nk; ⊗n\ni=1Ei where Ei : k →di and\n∆n\nk is the spider with no inputs and n outputs of dimension k.\n. . .\nE1\nE2\nEn\nd1\nd2\ndn\nk\nk\nk\nX\n. . .\nd1\nd2\ndn\n=\nThe lowest the rank, the lowest the search space for a factorization.\nIn fact\nif X is factorized as in 2.23 then rank(X) = rank(W) ≤|R| n2.\nThe time and\nspace complexity of the embedding are thus both quadratic in the dimension of the\nembedding n.\nSince any three-legged real tensor X can be factorized as in 2.23 for some n, Rescal\nis a very expressive model and performs well. However, the quadratic complexity is\na limitation which can be avoided by looking for diﬀerent ansatze.\n2.6.3\nDistMult\nThe idea of DistMult [Yan+15] is to factorize the tensor X via joint orthogonal\ndiagonalization, i.e. the entities are embedded in a vector space of dimension n and\nrelations are mapped to diagonal matrices on Rn. Note that the data required to\nspecify a diagonal matrix n →n is just a vector v : 1 →n which induces a diagonal\nmatrix ∆(v) by composition with the frobenius algebra ∆: n →n ⊗n in MatR.\nDistMult starts from the assumption — or ansatz — that X can is factorized as\nfollows:\nE\nE\nn\nn\nX\nE\nR\nE\n=\nE\nE\nR\nW\nn\n(2.24)\n149\nChapter 2\nFunctors for Semantics\nwhere E : |E| →n and W : |R| →n and ∆: n ⊗n ⊗n →1 is the spider with\nthree legs. Note that rank±(K) ≤rank(X) = rank(∆) = n, so that both the time\nand space complexity of DistMult are linear O(n). DistMult obtained the state of\nthe art on Embedding when it was released. It performs especially well at modeling\nsymmetric relations such as the transitive verb “met” which satisﬁes “x met y” iﬀ“y\nmet x”. It is not a coincidence that DistMult is good in these cases since a standard\nlinear algebra result says that a matrix Y : n →n can be orthogonally diagonalized if\nand only if Y is a symmetric matrix. For tensors of order three we have the following\ngenralization, with the consequence that DistMult models only symmetric relations.\nProposition 2.6.5. X : 1 →|E| × |R| × |E| is jointly orthogonally diagonalizable if\nand only if it is a family of symmetric matrices indexed by R.\nProof.\nE\nE\nn\nn\nE\nE\nR\nW\nn\nE\nE\nn\nn\nE\nE\nR\nW\nn\n=\nThis called for a more expressive factorization method, allowing to represent asym-\nmetric relations such as “love” for which we cannot assume that “x loves y” implies “y\nloves x”.\n2.6.4\nComplEx\nTrouillon et al. [Tro+17] provide a factorization of any real tensor X into complex-\nvalued matrices, that allows to model symmetric and asymmetric relations equally\nwell. Their model, called Complex allows to embed any knowledge graph K in a\nlow-dimensional complex vector space. We give a diagrammatic treatment of their\nresults, working in the category MatC, allowing us to improve their results. The\ndiﬀerence between working with real-valued and complex-valued matrices is that the\nlatter have additional structure on morphisms given by conjugation. This is pictured,\nat the diagrammatic level, by an asymmetry on the boxes which allows to represent\nthe operations of adjoint, transpose and conjugation as rotations or reﬂexions of\nthe diagram. This graphical gadget was introduced by Coecke and Kissinger in the\n150\nKnowledge graph embeddings\nSection 2.6\ncontext of quantum mechanics [CK17], we summarize it in the following picture:\nE\nE\nE\nE\ndagger\ndagger\n∗\nconjugate\nconjugate\n(2.25)\nThe key thing that Trouillon et al. note is that any real square matrix is the real-\npart of a diagonalizable complex matrix, a consequence of Von Neumann’s spectral\ntheorem.\nDeﬁnition 2.6.6. Y : n →n in MatC is unitarily diagonalizable if it factorizes as:\nE\nE\nY\nw\n=\n(2.26)\nwhere w : 1 →n is a state and E : n →n is a unitary.\nDeﬁnition 2.6.7. Y is normal if it commutes with its adjoint: Y Y † = Y †Y .\nY\nY\nY\nY\n=\nTheorem 2.6.8 (Von Neumann [Von29]). Y is diagonalizable if and only if Y is\nnormal.\nProposition 2.6.9 (Trouillon [Tro+17]). Suppose Y : n →n in MatR is a real\nsquare matrix, then Z = Y + iY T is a normal matrix and Re(Z) = Y . Therefore\nthere is a unitary E and a diagonal complex matrix W such that Y = Re(EWE†).\n151\nChapter 2\nFunctors for Semantics\nGraphically we have:\nE\nE\nY\nw\n=\n(2.27)\nwhere the red bubble indicates the real-part operator.\nNote that rank(A + B) ≤rank(A) + rank(B) which implies that rank(Z) =\nrank(Y + iY T) ≤2rank(Y ).\nCorollary 2.6.10. Suppose Y : n →n in MatR and rank(X) = k, then there is\nE : n →2k and W : 2k →2k diagonal in MatC such that Y = Re(EWE†)\nGiven a binary relation R : |E| →|E|, the sign-rank gives a bound on the dimension\nof the embedding.\nCorollary 2.6.11. Suppose R : |E| →|E| in MatB and rank±(R) = k, then there is\nE : |E| →2k and W : 2k →2k diagonal in MatC such that R = sign(Re(EWE†))\nThe above reasoning works for a single binary relation R. However, given knowl-\nedge graph K ⊆E ×R×E and applying the reasoning above we will get |R| diﬀerent\nembeddings Er : |E| →nr, one for each relation r ∈R, thus obtaining only the\nfollowing factorization: K = sign(P\nr∈R ∆◦(Er ⊗W(r) ⊗Er). Which means that\nthe overall complexity of the embedding is O(|R| n) where n = maxr∈R(nr). The\nproblem becomes: can we ﬁnd a single embedding E : |E| →n such that all rela-\ntions are mapped to diagonal matrices over the same n? In their paper, Trouillon\net al. sketch a direction for this simpliﬁcation but end up conjecturing that such a\nfactorization does not exist. We answer their conjecture negatively, by proving that\nfor any real tensor X of order 3, the complex tensor Z = X + iXT is jointly unitarily\ndiagonalizable.\nDeﬁnition 2.6.12. X : n ⊗m →n in MatC is simultaneously unitarily diagonaliz-\nable if there is a unitary E : n →n and W : m →n such that:\nE\nE\nX\n=\nm\nn\nn\nm\nW\nn\nn\nn\n(2.28)\n152\nKnowledge graph embeddings\nSection 2.6\nDeﬁnition 2.6.13. X : n ⊗m →n is a commuting family of normal matrices if:\nX\nm\nn\nn\nX\nn\nX\nn\nn\nX\nn\nm\nm\n=\n(2.29)\nThe following is an extension of Von Neumann’s theorem to the multi-relational\nsetting.\nTheorem 2.6.14. [HJ12] X : n ⊗m →n in MatC is a commuting family of normal\nmatrices if and only if it is simultaneously unitarily diagonalizable.\nOur contribution is the following result.\nProposition 2.6.15. For any real tensor X : n ⊗m →n the complex tensor Z\ndeﬁned by:\nZ\nm\nn\nn\nX\nm\nn\nn\nX\nm\nn\nn\n−\n=\ni\nis a commuting family of normal matrices.\nProof. This follows by checking that the two expressions below are equal, using only\nthe snake equation:\nZ\nZ\nX\nX\nX\n=\nX\nX\nX\nX\nX\n+\n−\n+\ni\ni\nZ\nZ\nX\nX\nX\n=\nX\nX\nX\nX\nX\n+\n−\n+\ni\ni\nCorollary 2.6.16. For any real tensor X : 1 →n ⊗m ⊗n there is a unitary\nE : n →n and W : n →m in MatC such that:\nX\nn\nm\nn\nE\nE\nn\nn\nn\nn\nm\nW\nn\n=\n(2.30)\n153\nChapter 2\nFunctors for Semantics\nwhere the bubble denotes the real-part operator.\nProposition 2.6.17. For any knowledge graph K ⊆E × R × E of sign rank k =\nrank±(K), there is E : |E| →2k and W : |R| →2k in MatC such that\nK = sign(Re(∆◦(E ⊗W ⊗E∗))).\nThis results in an improvement of the bound on the size of the factorization by\na factor of |R|. Although this improvement is only incremental, the diagrammatic\ntools used here open the path for a generalisation of the factorization result to higher-\norder tensors which would allow to model higher-arity relations. This may require to\nmove into quaternion valued vector spaces where (at least) ternary symmetries can\nbe encoded.\nMoreover, as we will show in Section 2.7, quantum computers allow to speed\nup the multiplication of complex valued tensors.\nAn implementation of ComplEx\non a quantum computer was proposed by [Ma+19]. The extent to which quantum\ncomputers can be used to speed up knowledge graph embeddings is an interesting\ndirection of future work.\n154\nQuantum models\nSection 2.7\n2.7\nQuantum models\nQuantum computing is an emerging model of computation which promises speed-up\non certain tasks as compared to classical computers. With the steady growth of quan-\ntum hardware, we are approaching a time when quantum computers perform tasks\nthat cannot be done on classical hardware with reasonable resources. Quantum Nat-\nural Language Processing (QNLP) is a recently proposed model which aims to meet\nthe data-intensive requirements of NLP algorithms with the computational power of\nquantum hardware [ZC16; Coe+20; Mei+20]. These models arise naturally from the\ncategorical approaches to linguistics [CCS10] and quantum mechanics [AC08]. They\ncan in fact be seen as instances of the tensor network models studied in 2.5. We have\ntested them on noisy intermediate-scale quantum computers [Mei+20; Lor+21], ob-\ntaining promising results on small-scale datasets of around 100 sentences. However,\nthe crucial use of post-selection in these models, is a limit to their scalability as the\nnumber of sentences grows.\nIn this section, we study the complexity of the quantum models based on a map-\nping from sentences to pure quantum circuits [Mei+20; Lor+21]. Building on the work\nof Arad and Landau [AL10] on the complexity of tensor network contraction, we show\nthat the additive approximation (with scale ∆= 1) of quantum models is a complete\nproblem for BQP, the class of problems which can be solved in polynomial time by a\nquantum computer with a bounded probability of error. Note that this approxima-\ntion scheme has severe limits when the amplitude we want to approximate is small\ncompared to the scale ∆. Thus the results may be seen as a negative statement about\nthe ﬁrst generation of QNLP models. However, speciﬁc types of linguistic structure\n(such as trees) may allow to reduce the post-selection and thus the approximation\nscale. Moreover, this puts QNLP on par with well-known BQP-complete problems\nsuch as approximate counting [Bor+09] and the evaluation of topological invariants\n[FLW00; FKW02] to set a standard for the next generations of QNLP models.\nThe development of DisCoPy was driven and motivated by the implementation\nof these QNLP models. The quantum module of DisCoPy is described in [TdY22],\nit features interfaces with the tensor module for classical simulation, with PyZX\n[Kv19] for optimization and with tket [Siv+20] for compilation and evaluation on\nquantum hardware.\nIn order to run quantum machine learning routines, we also\ndeveloped diagrammatic tools for automatic diﬀerentiation [TYd21]. The pipeline for\nperforming QNLP experiments with DisCoPy has been packaged in Lambeq [Kar+21]\nwhich provides state-of-the-art categorial parsers and optimised classes for training\nQNLP models.\n2.7.1\nQuantum circuits\nIn this section, we give a brief introduction to the basic ingredients of quantum\ncomputing, and deﬁne the categories QCirc and PostQCirc of quantum circuits\nand their post-selected counterparts. A proper introduction to the ﬁeld is beyond the\nscope of this thesis and we point the reader to [CK17] and [van20] for diagrammatic\ntreatments.\n155\nChapter 2\nFunctors for Semantics\nThe basic unit of information in a quantum computer is the qubit, whose state space\nis the Bloch sphere, i.e. the set of vectors ψ = α |0⟩+ β |1⟩∈C2 with norm ∥ψ∥=\n|α|2 + |β|2 = 1. Quantum computing is inherently probabilistic, we never observe the\ncoeﬃcients α and β directly, we only observe the probabilities that outcomes 0 or 1\noccur. These probabilities are given by the Born rule:\nP(i) = |⟨i|ψ⟩|2\nwith i ∈{ 0, 1 } and where ⟨i|ψ⟩is the inner product of |i⟩and |ψ⟩in C2, also\ncalled amplitude. Note that the requirement that ψ be of norm 1 ensures that these\nprobabilities sum to 1. The joint state of n qubits is given by the tensor product\nψ1 ⊗· · · ⊗ψn and thus lives in C2n, a Hilbert space of dimension 2n. The evolution\nof a quantum system composed of n qubits is described by a unitary linear map U\nacting on the space C2n, i.e. a linear map satisfying UU † = id = U †U. Where\n† (dagger) denotes the conjugate transpose. Note that the requirement that U be\nunitary ensures that ∥Uψ∥= ∥ψ∥and so U sends quantum states to quantum states.\nThe unitary map U is usually built up as a circuit from some set of basic gates.\nDepending on the generating set of gates, only certain unitaries can be built. We say\nthat the set of gates is universal, when any unitary can be obtained using a ﬁnite\nsequence of gates from this set. The following is an example of a universal gate-set:\nGates = { CX, H, Rz(α), swap }α∈[0,2π]\n(2.31)\nwhere CX is the controlled X gate acting on 2 qubits and deﬁned on the basis of C4\nby:\nCX(|00⟩) = |00⟩\nCX(|01⟩) = |01⟩\nCX(|10⟩) = |11⟩\nCX(|11⟩) = |10⟩\nRz(α) is the Z phase, acting on 1 qubit as follows:\nRz(α)(|0⟩) = |0⟩\nRz(α)(|1⟩) = eiα |1⟩.\nH is the Hadamard gate, acting on 1 qubit as follows:\nH(|0⟩) = 1\n√\n2(|0⟩+ |1⟩)\nH(|1⟩) = 1\n√\n2(|0⟩−|1⟩) .\nand the swap gate acts on 2 qubits and is deﬁned as usual.\nWe deﬁne the category of quantum circuits QCirc = MC(Gates) as a free\nmonoidal category with objects natural numbers n and arrows generated by the gates\nin (2.31).\nDeﬁnition 2.7.1 (Quantum circuit). A quantum circuit is a diagram c : n →n ∈\nMC(Gates) = QCirc where n is the number of qubits, it maps to a corresponding\nunitary Uc : (C2)⊗n →(C2)⊗n ∈MatC.\n156\nQuantum models\nSection 2.7\nExample 2.7.2. An example of quantum circuit is the following:\nCX\nCX\nα\n(2.32)\nwhere we denote the Rz(α) gate using the symbol α and the Hadamard gate using a\nsmall white box. Note that diﬀerent quantum circuits c and c′ may correspond to the\nsame unitary, we write c ∼c′ when this is the case. For instance the swap gate is\nequivalent to the following composition:\nCX\nCX\nCX\n∼\nWhen performing a quantum computation on n qubits, the quantum system is\nusually prepared in an all-zeros states |0⟩= |0⟩⊗n, then a quantum circuit c is ap-\nplied to it and measurements are performed on the resulting state Uc |0⟩, yielding\na probability distribution over bitstrings of length n. We can then post-select on\ncertain outcomes, or predicates over the resulting bitstring, by throwing away the\nmeasurements that do not satisfy this predicate. Diagrammatically, state prepara-\ntions and post-selections are drawn using the generalised Dirac notation [CK17] as in\nthe following post-selected circuit:\nCX\nCX\nα\n0\n1\n0\n0\n0\n(2.33)\nThe category of post-selected quantum circuits is obtained from QCirc by allow-\ning state preparations |0⟩, |1⟩, and post-selections ⟨0| , ⟨1|.\nPostQCirc = MC(Gates + { ⟨i| , |i⟩}i∈0,1 + { a : 0 →0 }a∈R)\nNote that we also allow arbitrary scalars a ∈R, seen as boxes 0 →0 in PostQCirc\nto rescale the results of measurements, this is needed in order to interpret pregroup\ngrammars in PostQCirc, see 2.7.2. Post-selected quantum circuits map functorially\ninto linear maps:\nI : PostQCirc →MatC\n157\nChapter 2\nFunctors for Semantics\nby sending each gate in Gates to the corresponding unitary, state preparations and\npost-selections to the corresponding states and eﬀects in MatC. This makes post-\nselected quantum circuits instances of tensor networks over C.\nProposition 2.7.3. For any morphism d : 1 →1 in PostQCirc, there exists a\nquantum circuit c ∈QCirc such that I(d) = a · ⟨0| Uc |0⟩where a = Q\ni U(ai) is the\nproduct of the scalars appearing in d.\nProof. We start by removing the scalars ai from the diagram d and multiplying them\ninto a = Q\ni U(ai). Then we pull all the kets to the top of the diagram and all the\nbras to the bottom, using swap if necessary.\n2.7.2\nQuantum models\nQuantum models are functors from a syntactic category to the category of post-\nselected quantum circuits PostQCirc.\nThese were introduced in recent papers\n[Mei+21; Coe+20], and experiments were performed on IBM quantum computers\n[Mei+20; Lor+21].\nDeﬁnition 2.7.4 (Quantum model). A quantum (circuit) model is a functor F :\nG →PostQCirc for a grammar G, the semantics of a sentence g ∈L(G) is given\nby a distribution over bitstrings b ∈{ 0, 1 }F(s) obtained as follows:\nP(b) = |⟨b| F(g) |0⟩|2\nQuantum models deﬁne the following computational problem:\nDeﬁnition 2.7.5. QSemantics\nInput:\nG a grammar, g : u →s ∈MC(G), F : G →PostQCirc, b ∈\n{ 0, 1 }F(s)\nOutput:\n|⟨b| I(F(g)) |0⟩|2\nRemark 2.7.6. Note that quantum language models do not assume that the semantics\nof words be a unitary matrix. Indeed a word may be interpreted as a unitary with some\noutputs post-selected. However, mapping words to unitaries is justiﬁed in many cases\nof interest in linguistics. For instance, this implies that there is no loss of information\nabout “ball” when an adjective such as “big” is applied to it.\nAny of the grammars studied in Chapter 1 may be intepreted with a quantum\nmodel. For monoidal grammars (including regular and context-free grammars), this\nis simply done by interpreting every box as a post-selected quantum circuit. For\nrigid grammars, we need to choose an interpretation for the cups and caps. Since the\nunderlying category MatC is compact-closed, there is a canonical way of interpreting\nthem using a CX gate and postselection, and re-scaling the result by a factor of\n√\n2:\n158\nQuantum models\nSection 2.7\nCX\n0\n0\n7→\nCX\n0\n0\n√\n2\n7→\n,\n.\n√\n2\n(2.34)\nIn order to interpret cross dependencies we can use the swap gate, see Example 2.7.2.\nFinally, in order to interpret the Frobenius algebras in a pregroup grammar with\ncoreference, we use the following mapping:\nCX\n0\n0\n7→\nCX\n0\n√\n2\nOne can check the the image under I of the circuit above maps to the Frobenius\nalgebra in MatC.\nIn practice, quantum models are learned from data by choosing a parametrized\nquantum circuit in PostQCirc for each production rule and then tuning the param-\neters (i.e. the phases α in Rz(α)) appearing in this circuit, see [Mei+20; Lor+21] for\ndetails on ansatze and training.\n2.7.3\nAdditive approximations\nSince quantum computers are inherently probabilistic, there is no deterministic way\nof computing a function F : {0, 1}∗→C encoded in the amplitudes of a quantum\nstate. Rather what we obtain is an approximation of F. In many cases, the best\nwe can hope for is an additive approximation, which garantees to generate a value\nwithin the range [F(x)−∆(x)ϵ, F(x)+∆(x)ϵ] where ∆: { 0, 1 }∗→R+ is an approx-\nimation scale. This approximation scheme has been found suitable to describe the\nperformance of quantum algorithms for contracting tensor network [AL10], counting\napproximately [Bor+09], and computing topological invariants [FLW00] including the\nJones polynomial [FKW02].\nDeﬁnition 2.7.7 (Additive approximation). [AL10] A function F : { 0, 1 }∗→C\nhas an additive approximation V with an approximation scale ∆: { 0, 1 }∗→R+ if\nthere exists a probabilistic algorithm that given any instance x ∈{ 0, 1 }∗and ϵ > 0,\nproduces a complex number V (x) such that\nP(|V (x) −F(x)| ≥ϵ∆(x)) ≤1\n4\n(2.35)\nin a running time that is polynomial in |x| and ϵ−1.\n159\nChapter 2\nFunctors for Semantics\nRemark 2.7.8. The error signal ϵ is usually inversely proportional to a polynomial\nin the run-time t of the algorithm, i.e. we have ϵ =\n1\npoly(t).\nAn algorithm V satisfying the deﬁniton above is a solution of the following problem\ndeﬁned for any function F : { 0, 1 }∗→C and approximation scale ∆: { 0, 1 }∗→R+.\nDeﬁnition 2.7.9. Approx(F, ∆)\nInput:\nx ∈{ 0, 1 }∗, ϵ > 0\nOutput:\nv ∈C such that P(|v −F(x)| ≥ϵ∆(x)) ≤1\n4\nCompare this to the deﬁnition of a multiplicative approximation, also known as a\nfully polynomial randomised approximation scheme [JSV04].\nDeﬁnition 2.7.10 (Multiplicative approximation). A function F : { 0, 1 }∗→C has\na multiplicative approximation V if there exists a probabilistic algorithm that given\nany instance x ∈{ 0, 1 }∗and ϵ > 0, produces a complex number V (x) such that\nP(|V (x) −F(x)| ≥ϵ |F(x)|) ≤1\n4\n(2.36)\nin a running time that is polynomial in |x| and ϵ−1.\nRemark 2.7.11. This approximation is called multiplicative because V (x) is guar-\nanteed to be within a multiplicative factor (1 ± ϵ) of the optimal value F(x). For\ninstance the inequality in (2.36) implies that |F(x)| (1 −ϵ) ≤|V (x)| ≤|F(x)| (1 + ϵ))\nwith probability bigger than 3\n4.\nNote that any multiplicative approximation is also additive with approximation\nscale ∆(x) = |F(x)|. However, the converse does not hold. An additive approximation\nscheme allows for the approximation scale ∆(x) to be exponentially larger than the\nsize of the output |F(x)|, making the approximation (2.35) quite loose since the error\nparameter ϵ may be bigger than the output signal V (x).\n2.7.4\nApproximating quantum models\nIn this section, we show that the problem of additively approximating the evaluation\nof sentences in a quantum model is in BQP and that it is BQP-complete in special\ncases of interest. The argument is based on Arad and Landau’s work [AL10] on the\nquantum approximation of tensor networks. We start by reviewing their results and\nend by demonstrating the consequences for quantum language models.\nConsider the problem of approximating the contraction of tensor networks T(V, E)\nusing a quantum computer. Arad and Landau [AL10] show that this problem can be\nsolved in polynomial time, up to an additive accuracy with a scale ∆that is related\nto the norms of the swallowing operators.\nProposition 2.7.12 (Arad and Landau [AL10]). Let T(V, E) be a tensor network\nover C of dimension q and maximal node degree a, let π : [|V |] →V be a contraction\norder for T and let { Ai }i∈{ 1,...,k } be the corresponding set of swallowing operators.\n160\nQuantum models\nSection 2.7\nThen for any ϵ > 0 there exists a quantum algorithm that runs in k · ϵ−2 · poly(qa)\ntime and outputs a complex number r such that:\nP(|value(T) −r| ≥ϵ∆) ≤1\n4\nwith\n∆(T) =\nk\nY\ni=1\n∥Ai∥\nProof. Given a tensor network with a contration order π, the swallowing operators Ai\nare linear maps. For each of them, we can construct a unitary Ui acting on a larger\nspace such that post-selecting on some of its outputs yields Ai. Composing these\nwe obtain a unitary Uc = U1 · U2 . . . U|V | represented by a poly-size quantum circuit\nc ∈QCirc such that value(T) = ⟨0| Uc |0⟩. In order to compute an approximation\nof this quantity we can use an H-test, deﬁned by the following circuit:\nU\n. . .\n. . .\n(2.37)\nwhere the white boxes are hadamard gates and the subdiagram in the middle denotes\nthe controlled unitary U. It can be shown that the probability r of measuring 0 on\nthe ancillary qubit is equal to Re(⟨0| Uc |0⟩). A slightly modiﬁed version of the H-\ntest computes Im(⟨0| Uc |0⟩). Arad and Landau [AL10] show that this process can be\ndone in polynomial time and that the result of measuring the ancillary qubit gives an\nadditive approximation of value(T) with approximation scale ∆(T) = Qk\ni=1 ∥F(di)∥.\nCorollary 2.7.13. The problem Approx(Contraction(C), ∆) with ∆as deﬁned above\nis in BQP.\nRemark 2.7.14. From the Cauchy-Schwartz inequality we have that:\n|T| ≤\nk\nY\ni=1\n∥Ai∥= ∆(T) .\nIn fact we have no guarantee that the approximation scale ∆(T) is not exponentially\nlarger than |F(d)|. This is a severe limitation, since the approximations we get from\nthe procedure deﬁned above can have an error larger than the value we are trying to\napproximate.\nWe now consider the problem of approximating the amplitude of a post-selected\nquantum circuit |⟨0| Uc |0⟩|2. Note that this is an instance of the problem studied in\nthe previous paragraph, since post-selected quantum circuits are instances of tensor\nnetworks, so that this problem belong to the class BQP. For this subclass of tensor\n161\nChapter 2\nFunctors for Semantics\nnetworks the approximation scale ∆(c) can be shown to be constant and equal to\n1, with the consequence — shown by Arad and Landau [AL10] — that the additive\napproximation of post-selected quantum circuits is BQP −hard.\nIn order to show BQP-hardness of a problem F, it is suﬃcient to show that an\noracle which computes F can be used to perform universal quantum computation\nwith bounded error. More precisely, for any quantum circuit u ∈QCirc denote by\np0 the probability of obtaining outcome 0 when measuring the last qubit of c. To\nperform universal quantum computation, it is suﬃcient to distinguish between the\ncases where p0 <\n1\n3 and p0 >\n2\n3 for any quantum circuit c. Thus a problem F is\nBQP-hard if for any circuit c, there is a poly-time algorithm V using F as an oracle\nthat returns Y ES when p0 < 1\n3 with probability bigger than 3\n4 and NO when p0 > 2\n3\nwith probability bigger than 3\n4. .\nProposition 2.7.15. [AL10] The problem of ﬁnding an additive approximation of\n|⟨0| Uc |0⟩|2 with scale ∆= 1 where Uc is the unitary induced by a quantum circuit\nc ∈QCirc is BQP-complete.\nProof. Membership follows by reduction to Approx(Contraction(S), ∆), since post-\nselected quantum circuits are instances of tensor networks. To show hardenss, ﬁx any\nquantum circuit c on n qubits and denote by p0 the probability of obtaining outcome\n0 on the last qubit of c. We can construct the quantum circuit:\nUc\n. . .\n. . .\nU †\nc\nCX\n. . .\n(2.38)\nit is easy to check that ⟨0| Uq |0⟩= p0. Since q is a circuit, there is a natural con-\ntraction order given by the order of the gates, moreover the corresponding swallowing\noperators Uqi are unitary and thus ∥Uqi∥= 1, also ∥U(|0⟩)∥= ∥U(⟨0|)∥= 1 and\ntherefore the approximation scale is constant ∆(q) = 1. Suppose we have an oracle\nV that computes an approximation of ⟨0| Uq |0⟩with constant scale ∆(q) = 1. Then\nfor any circuit c ∈QCirc, we construct the circuit q and apply V to get a complex\nnumber V (q) such that:\nP(|V (q) −p0| ≥ϵ) ≤1\n4 =⇒P(p0 −ϵ ≤|V (q)| ≤p0 + ϵ) ≥3\n4\nSetting ϵ < 1\n6, we see that if |V (q)| < 1\n6 then p0 < 1\n3 with probability ≥3\n4 and similarly\n|V (q)| > 5\n6 implies p0 > 1\n3 with probability ≥3\n4. Note that this would not be possible\nif we didn’t know that the approximation scale ∆(q) is bounded. Thus V can be used\nto distinguish between the cases where p0 < 1\n3 and p0 > 2\n3 with probability of success\ngreater than 3\n4. Therefore the oracle V can be used to perform universal quantum\ncomputation with bounded error. And thus V is BQP-hard.\n162\nQuantum models\nSection 2.7\nCorollary 2.7.16. Approx(FunctorEval(C)(I), ∆= 1) where I : PostQCirc →\nMatC is the functor deﬁned in paragraph 2.7.1 is a BQP-complete problem.\nProof. Membership follows by reduction to Approx(Contraction(C), ∆) since post-\nselected quantum circuits are instances of tensor networks. Since QCirc ,−→PostQCirc\nthe problem of Proposition 2.7.15 reduces to Approx(FunctorEval(I), ∆= 1), thus\nshowing hardness.\nNote that the value v = |⟨0| Uc |0⟩| may be exponentially small in the size of the\ncircuit c, so that the approximation scale ∆= 1 is still not optimal. In this case we\nwould need exponentially many samples from the quantum computer approximate v\nup to multiplicative accuracy.\nWe end by showing BQP-completeness for the problem of approximating the se-\nmantics of sentences in a quantum model with approximation scale ∆= 1.\nDeﬁnition 2.7.17. QASemantics = Approx(QSemantics, ∆= 1)\nInput:\nG a monoidal grammar, g : u →s ∈MC(G), F : G →PostQCirc,\nb ∈{ 0, 1 }F(s), ϵ > 0\nOutput:\nv ∈C such that P(|v −⟨b| I(F(g)) |0⟩| ≥ϵ) ≤1\n4\nProposition 2.7.18. There are pregroup grammars G, such that the problem QASemantics(G)\nis BQP-complete.\nProof. Membership follows by reduction to the problem of Proposition 2.7.15, since\nthe semantics of a grammatical reduction g : u →s in RC(G) is given by the\nevaluation of |⟨b| I(F(g)) |0⟩|2 which corresponds to evaluating |⟨0| Uc |0⟩|2 where c is\ndeﬁned by Proposition 2.7.3. To show hardness, let G have only one word w of type s,\nﬁx any unitary U, and deﬁne F : G →PostQCirc by F(s) = 0 and F(w) = ⟨0| U |0⟩,\nthen evaluating the semantics of w reduces to the problem of Proposition 2.7.15 and\nis thus BQP −hard.\nNote that we were able to show completeness using the fact that the functor F is in\nthe input of the problem. It is an open problem to show that QASemantics(G)(F)\nis BQP-complete for ﬁxed choices of grammar G and functors F. In order to show\nthis, one may need to assume that G is a pregroup grammar with coreference and\nshow that there is a functor F such that any post-selected quantum circuit can be\nbuilt up using ﬁxed size circuits (corresponding to words) connected by GHZ states\n(corresponding to the spiders encoding the coreference), adapting the argument from\nProposition 2.5.12.\nMoreover, as discussed above, this approximation scheme is limited by the fact\nthat the approximation scale ∆may be too big when the output ⟨b| I(F(g)) |0⟩is\nexponentially small. This is particularly signiﬁcant at the beginning of training, when\nF is initialised as a random mapping to circuits, see [Mei+20]. This seems to be an\ninherent problem caused by the use of post-selection in the model, although methods\nto reduce the post-selection have been proposed, e.g. the snake removal scheme from\n[Mei+21].\n163\nChapter 2\nFunctors for Semantics\nOne avenue to overcome this limitation, is to consider a diﬀerent typ of mod-\nels, deﬁned as functors G →CPM(QCirc) where CPM(QCirc) is the category\nof completely positive maps induced by quantum circuits as deﬁned in [HV19]. In\nthis category, post-selection is not allowed since every map is causal. The problem\nwith these models however is that we cannot interpret the cups and caps of rigid\ngrammars. We may still be able to interpret monoidal grammars, as well as acyclic\npregroup reductions such as those induced by a dependency grammar. Exploring the\ncomplexity of these models, and testing them on quantum hardware, is left for future\nwork.\n164\nDisCoPy in action\nSection 2.8\n2.8\nDisCoPy in action\nWe now give an example of how DisCoPy can be used to solve a concrete task. We\ndeﬁne a relational model (2.4) and then learn a smaller representation of the data\nas a tensor model (2.5). Since the sentences we will deal with are all of the form\nsubject-verb-object, this means we will perform a knowledge ambedding task in the\nsense of 2.6. We start by deﬁning a simple pregroup grammar with 3 nouns and 2\nverbs.\nListing 2.8.1. Subject-verb-object language.\nfrom discopy.rigid import Ty, Id, Box, Diagram\nn, s = Ty(’n’), Ty(’s’)\nmake_word = lambda name, ty: Box(name, Ty(), ty)\nnouns = [make_word(name, n) for name in [’Bruno’, ’Florio’, ’Shakespeare’]]\nverbs = [make_word(name, n.l @ s @ n.r) for name in [’met’, ’read’]]\ngrammar = Diagram.cups(n, n.l) @ Id(s) @ Diagram.cups(n.r, n)\nsentences = [a @ b @ c >> grammar for a in nouns for b in verbs for c in nouns]\nsentences[2].draw()\nWe can now build a relational model for this language as a tensor.Functor.\nListing 2.8.2. Relational model in DisCoPy.\nfrom discopy.tensor import Dim, Tensor, Functor, Spider\nimport jax.numpy as np\nTensor.np = np\nob = {n: Dim(3), s: Dim(1)}\ndef mapping(box):\nif box.name == ’Bruno’:\nreturn np.array([1, 0, 0])\nif box.name == ’Florio’:\nreturn np.array([0, 1, 0])\nif box.name == ’Shakespeare’:\nreturn np.array([0, 0, 1])\nif box.name == ’met’:\nreturn np.array([[1, 0, 1], [0, 1, 1], [1, 1, 1]])\nif box.name == ’read’:\nreturn np.array([[1, 0, 0], [0, 1, 1], [0, 1, 1]])\nif box.name == ’who’:\n165\nChapter 2\nFunctors for Semantics\nreturn Spider(0, 3, Dim(3)).array\nT = Functor(ob, mapping)\nassert T(sentences[2]).array == [0.]\nWe use float numbers for simplicity, but one may use dtype=bool instead. Note\nthe special intepretation of “who” as a Frobenius spider, see 2.4.4.\nRelational models can be used to evaluate any conjunctive query over words. We\ncan generate a new pregroup reduction using lambeq [Kar+21] and evaluate it in T.\nListing 2.8.3. Evaluating a conjunctive query in a relational model.\nfrom lambeq import BobcatParser\nparser = BobcatParser()\ndiagram = parser.sentence2diagram(’Bruno met Florio who read Shakespeare.’)\ndiagram.draw()\nassert T(diagram) = [1.0]\nWe now show how to embed the three-dimensional data deﬁned by T as a two-\ndimensional tensor.Functor with float entries. We start by parametrising two-\ndimensional tensor functors.\nListing 2.8.4. Parametrising a tensor functor.\nimport numpy\ndef p_mapping(box, params):\nif box.name == ’Bruno’:\nreturn np.array(params[0])\nif box.name == ’Florio’:\nreturn np.array(params[1])\nif box.name == ’Shakespeare’:\nreturn np.array(params[2])\nif box.name == ’met’:\nreturn np.array([params[3], params[4]])\nif box.name == ’read’:\nreturn np.array([params[5], params[6]])\nif box.name == ’who’:\nreturn Spider(0, 3, Dim(2)).array\nob = {n: Dim(2), s: Dim(1)}\nF = lambda params: Functor(ob, lambda box: p_mapping(box, params))\nparams0 = numpy.random.rand(6, 2)\nassert F(params0)(sentences[2]).array != [0.]\n166\nDisCoPy in action\nSection 2.8\nWe obtain a prediction by evaluating F(params) on a sentence and taking sigmoid\nto get a number between 0 and 1. We can then deﬁne the loss of a functor as the\nmean squared diﬀerence between its predictions and the true labels given by T. Of\ncourse, other activation and loss functions may be used.\nListing 2.8.5. Deﬁning the loss function for a knowledge embedding task.\ndef sigmoid(x):\nsig = 1 / (1 + np.exp(-x))\nreturn sig\nevaluate = lambda F, sentence: sigmoid(F(sentence).array)\ndef mean_squared(y_true, y_pred):\nreturn np.mean((np.array(y_true) - np.array(y_pred)) ** 2)\nloss = lambda params: mean_squared(*zip(\\\n*[(T(sentence).array, evaluate(F(params), sentence)) for sentence in sentences]))\nThe Jax package [Bra+18] supports automatic diﬀerentiation grad and just-in-\ntime compilation jit for jax compatible numpy code.\nSince the code for Tensor\nis compatible, we can directly use Jax to compile a simple update function for the\nfunctor’s parameters. We run the loop and report the results obtained.\nListing 2.8.6. Learning functors with Jax.\nfrom jax import grad, jit\nfrom time import time\nstep_size = 0.1\n@jit\ndef update(params):\nreturn params - step_size * grad(loss)(params)\nepochs, iterations = 7, 30\nparams = numpy.random.rand(6, 2)\nfor epoch in range(epochs):\nstart = time()\nfor i in range(iterations):\nparams = update(params)\nprint(\"Epoch {} ({:.3f} milliseconds)\".format(epoch, 1e3 * (time() - start)))\nprint(\"Testing loss: {:.5f}\".format(loss(params)))\ny_true = [T(sentence).array for sentence in sentences]\ny_pred = [0 if evaluate(F(final_params), sentence) < 0.5 else 1\nfor sentence in sentences]\nprint(classification_report(y_true, y_pred))\n167\nChapter 2\nFunctors for Semantics\nAnd voilà! In just a few lines of code we have learnt a compressed 2D tensor\nrepresentation of the data of a 3D relational model. We have executed a simple sta-\ntistical relational learning routine [Wan+17], which can in fact be seen as a version\nof the knowledge graph embedding model Rescal [NTK11], see 2.6, we review the\ntheory of knowledge graph embeddings from a diagrammatic perspective, we show\nthe advantages of moving into the complex numbers for embedding relations. In fact,\nwe implemented and tested a similar embedding method using quantum computers\n[Mei+20; Lor+21] which naturally handle complex tensors, see 2.7. Large-scale di-\nagram parsing is now possible thanks to Lambeq [Kar+21], as we showed above.\nThere remains work to do on optimization and batching for tensor models, which\nwould allow to scale these experiments to real-world corpora.\n168\nChapter 3\nGames for Pragmatics\nThe threefold distinction between syntax, semantics and pragmatics may be traced\nback to the semiotics of Peirce an his trilogy between sign, object and interpre-\ntant. According to Peirce [Pei65], these three aspects would induce three diﬀerent\napproaches to semiotics, renewing the medieval trivium: formal grammar, logic and\nformal rhetoric. In his studies [Sta70], Stalnaker gives a ﬁner demarcation between\npragmatics and semantics through the concept of context: “It is a semantic problem\nto specify the rules for matching up sentences of a natural language with the propo-\nsitions that they express. In most cases, however, the rules will not match sentences\ndirectly with propositions, but will match sentences with propositions relative to fea-\ntures of the context in which the sentence is used. Those contextual features are part\nof the subject matter of pragmatics.”\nIn his Philosophical Investigations [Wit53], Wittgenstein introduces the concept\nof language-game (Sprachspiel) as a basis for his theory of meaning. He never gives a\ngeneral deﬁnition, and instead proceeds by enumeration of examples: “asking, thank-\ning, cursing, greeting, praying”. Thus, depending on the language-game in which it is\nplayed, the same utterance “Water!” can be interpreted as the answer to a question,\na request to a waiter or the chorus of a song. From the point of view of pragmatics,\nlanguage-games provide a way of capturing the notion of context, isolating a particular\nmeaning/use of language within an environment deﬁned by the game.\nSince Lewis’ work on conventions [Lew69], formal game theory has been used\nto model the variability of the meaning of sentences and their dependence on con-\ntext\n[Fra09; MP15; BS18]. These theoretical enquiries have also been supported\nby psycholinguistic experiments such as those of Frank and Goodman [FG12], where\na Bayesian game-theoretic model is used to predict the behaviour of listeners and\nspeakers in matching words with their referents.\nIn parallel to its use in pragmatics, game theory has been proven signiﬁcant in de-\nsigning machine learning tasks. It is at the heart of multi-agent reinforcement learning\n[TN05], where decision-makers interact in a stochastic environment. It is also used\nto improve the performance of neural network models, following the seminal work of\nGoodfellow et al. [Goo+14], and is beginning to be applied to natural language pro-\ncessing tasks such as dialogue generation [Li+16; Li+17], knowledge graph embedding\n[CW18; XHW18] and word-sense disambiguation [TN19].\n169\nChapter 3\nGames for Pragmatics\nThe aim of this chapter is to develop formal and diagrammatic tools to model\nlanguage games and NLP tasks. More precisely, we will employ the theory of lenses,\nwhich have been developed as a model for the dynamics of data-accessing programs\n[PGW17] and form the basis of the recent applications of category theory to both\ngame theory [Gha+18; BHZ19] and machine learning [FST19; Cru+21]. The results\nare still at a preliminary stage, but the diagrammatic representation succeeds in\ncapturing a wide range of pragmatic scenarios.\nIn Section 3.1, we argue that probabilistic models are best suited for analysing\npragmatics and NLP tasks and show how the deterministic models studied in Chapter\n2 may be turned into probabilistic ones. In Section 3.2, we introduce lenses and show\nhow they capture the dynamics of probabilistic systems and stochastic environments.\nIn Section 3.3, we show how parametrization may be used to capture agency in these\nenvironments and discuss the notions of optimum, best response and equilibrium for\nmulti-agent systems. Throughout, we give examples illustrating how these concepts\nmay be used in both pragmatics and NLP and in Section 3.4, we give a more in-depth\nanalysis of three examples, building on recent proposals for modelling language games\nwith category theory [HL18; de +21].\n170\nProbabilistic models\nSection 3.1\n3.1\nProbabilistic models\nUnderstanding language requires resolving large amounts of vagueness and ambiguity\nas well as inferring interlocutor’s intents and beliefs. Uncertainty is a central feature\nof pragmatic interactions which has led linguists to devise probabilistic models of\nlanguage use, see [FJ16] for an overview.\nProbabilistic modelling is also used throughout machine learning and NLP in\nparticular. For example, the language modelling task is usually formulated as the\ntask of learning the conditional probability p(xk | xk−1, . . . , x1) to hear the word xk\ngiven that words x1, . . . , xk−1 have been heard. We have seen in Section 2.3 how neural\nnetworks induce probabilistic models using the softmax function. The language of\nprobability theory is particularly suited to formulating NLP tasks, and reasoning\nabout them at a high level of abstraction.\nCategorical approaches to probability theory and Bayesian reasoning have made\nmuch progress in recent years. In their seminal paper [CJ19], Cho and Jacobs iden-\ntiﬁed the essential features of probabilistic resoning using string diagrams, including\nmarginalisation, conditionals, disintegration and Bayesian inversion. Some of their\ninsights derived from Fong’s thesis [Fon13], where Bayesian networks are formalised as\nfunctors into categories of Markov kernels. Building on Cho and Jacobs, Fritz [Fri20]\nintroduced Markov categories as a synthetic framework for probability theory, gener-\nalising several results from the dominant measure-theoretic approach to probability\ninto this high-level diagrammatic setting. These algebraic tools are powering inter-\nesting developments in applied category theory, including diagrammatic approaches\nto causal inference [JKZ18] and Bayesian game theory [BHZ19].\nIn this section, we review the basic notions of probability theory from a categor-\nical perspective and show how they can be used to reason about discriminative and\ngenerative NLP models.\n3.1.1\nCategorical probability\nWe introduce the basic notions of categorical probability theory, following [CJ19] and\n[Fri20]. For simplicity, we work in the setting of discrete probabilities, although most\nof the results we use are formulated diagrammatically and are likely to generalise to\nany Markov category.\nLet D : Set →Set be the discrete distribution monad deﬁned on objects by:\nD(X) = { p : X →R+ | dhas ﬁnite support and\nX\nx∈X\np(x) = 1 }\nand on arrows f : X →Y by:\nD(f) : D(X) →D(Y ) : (p : X →R+) 7→(y ∈Y 7→\nX\nx∈f−1(y)\np(x) ∈R+)\nWe can construct the Kleisli category for the distribution monad Prob = Kl(D)\nwith objects sets and arrows discrete conditional distributions f : X →D(Y ) we\n171\nChapter 3\nGames for Pragmatics\ndenote by f(y|x) ∈R+ the probability f(x)(y). Composition of f : X →D(Y ) and\ng : Y →D(Z) is given by:\nX\nf−→D(Y )\nD(g)\n−−→DD(Z)\nµZ\n−→D(Z)\nwhere µZ : DD(Z) →D(Z) ﬂattens a distribution of distributions by taking sums.\nExplicitly we have:\nf · g(z|x) =\nX\ny\ng(z|y)f(y|x) ∈R+\nWe may think of the objects of Prob as random variables and the arrows as condi-\ntional distributions.\nThe category Prob has interesting structure.\nFirst of all, it is a symmetric\nmonoidal category with × as monoidal product.\nThis is not a cartesian product\nsince D(X × Y ) ̸= D(X) × D(Y ). The unit of the monoidal product × is the sin-\ngleton set 1 which is terminal in Prob, i.e. for any set X there is only one map\ndelX : X →D(1) ≃1 called discard. Terminality of the monoidal unit means that\nif we discard the output of a morphism we might as well have discarded the input, a\nproperty often interpreted as causality [HV19].\nThere is a commutative comonoid structure copyX : X →X × X on each object\nX with counit !X.\nAlso there is a embedding of Set into Prob which gives the\ndeterministic maps. These are characterized by the following property\ncopyY ◦f = (f × f) ◦copyX ⇐⇒f : X →Y is deterministic\nMorphisms p : 1 →X in Prob are simply distributions p ∈D(X). Given a joint\ndistribution p : 1 →X ×Y we can take marginals of p by composing with the discard\nmap:\np\nX\nY\np\nX\nY\n,\nIn Prob the disintegration theorem holds.\nProposition 3.1.1 (Disintegration). [CJ19] For any joint distribution p ∈D(X×Y ),\nthere are channels c : X →D(Y ) and c† : Y →D(X) satisfying:\nc\np\nX\nY\nX\nY\n=\np\nX\nY\nc†\np\nX\nY\nX\nY\n=\nintegration\nintegration\ndisintegration\ndisintegration\n(3.1)\n172\nProbabilistic models\nSection 3.1\nProof. The proof is given in [CJ19] in the case of the full subcategory of Prob with\nobjects ﬁnite sets, i.e. for the category Stoch of stochastic matrices. Extending this\nproof to the inﬁnite discrete case is simple because for any distribution p ∈D(X ×Y )\nwe may construct a stochastic vector over the support of p, which is ﬁnite by deﬁnition\nof D. So we can disintegrate p in Stoch and then extend it to Prob by assigning\nprobability 0 to elements outside of the support.\nExample 3.1.2. Taking X = { 1, 2 } and Y = { A, B } an example of disintegration\nis the following:\n\n1\n1/8\n2\n7/8 ,\nA\nB\n1\n1\n0\n2\n3/7\n4/7\n\n←\nA\nB\n1\n1/8\n0\n2\n3/8\n1/2\n→\n\n\nA\nB\n1\n1/4\n0\n2\n3/4\n1\n,\nA\nB\n1/2\n1/2\n\n\nGiven a prior p ∈D(X) and a channel c : X →D(Y ), we can integrate them\nto get a joint distribution over X and Y and then disintegrate over Y to get the\nchannel c† : Y →D(X). This process is known as Bayesian inversion and c† is called\na Bayesian inverse of c along p. These satisfy the following equation, which can be\nderived from 3.1.\nc†\np\nY\nX\nY\n=\nc\nX\nc\np\nX\nY\nX\n(3.2)\nInterpreting this diagrammatic equation in Prob we get that:\nc(y|x)p(x) = c†(x|y)\nX\nx\n(c(y|x)p(x))\nknown as Bayes law.\nThe category Prob satisﬁes a slightly stronger notion of disintegration which\ndoesn’t only apply to states or joint distributions but to channels directly.\nProposition 3.1.3 (Conditionals). [Fri20] The category Prob has conditionals, i.e.\nfor any morphism f : A →D(X × Y ) in Prob there is a morphism f|X : A × X →\nD(Y ) such that:\nf\nY\nX\nA\nf\nX\nA\nf|X\nY\nY\n=\nProof. As for Proposition 3.1.1, this was proved in the case of Stoch by Fritz [Fri20]\nand it is simple to extend the proof to Prob.\n173\nChapter 3\nGames for Pragmatics\n3.1.2\nDiscriminators\nThe ﬁrst kind of probabilistic systems that we consider are discriminators. We deﬁne\nthem in general as probabilistic channels that take sentences in a language L(G) and\nproduce distributions over a set of features Y . These can be seen as solutions to the\ngeneral classiﬁcation task of assigning sentences in L(G) to classes in Y .\nDeﬁnition 3.1.4 (Discriminator). A discriminator for a grammar G in a set of\nfeatures Y is a probabilistic channel:\nc : L(G) →D(Y )\nRemark 3.1.5. Throughout this chapter we assume that parsing for the chosen gram-\nmar G can be done eﬃciently.\nWFor simplicity, we assume that we are given a\nfunction:\nparsing : L(G) →\na\nu∈V ∗\nG(u, s)\nwhere G is the category of derivations for the grammar G. This could also be made\na probabilistic channel with minor modiﬁcations to the results of this section.\nIn the previous chapters, we deﬁned NLP models as functors F : G →S where G\nis a grammar and S a semantic category. The aim for this section is to show that,\nin most instances, we can turn these models into probabilstic discriminators. We\nwill do this in two steps. Assuming that parsing can be performed eﬃciently, it is\neasy to show that functorial models F : G →S induce encoders, i.e. deterministic\nfunctions L(G) →S which assign to every sentence in the language L(G) ⊆V ∗a\ncompressed semantic representation in the sentence space S = F(s). In order to build\na discriminator c from an encoder f : L(G) →S the only missing piece of structure is\nan activation function σ : S →D(Y ), mapping semantic states to distributions over\nclasses.\nSoftmax is a useful activation function which allows to turn real valued vectors\nand tensors into probability distributions.\nsoftmaxX : RX →D(X)\nsoftmax(⃗x)i =\nexi\nPn\ni=1 exi\nThus, when the sentence space is S = RY , softmax allows to turn encoders f : X →\nRY into discriminators c = softmax ◦f : X →D(Y ). In fact all discriminators arise\nfrom encoders by post-composition with softmax as the following proposition shows.\nProposition 3.1.6. Given a channel c : X →D(Y ) and a prior distribution p ∈\nD(Y ) there is a function f : X →RY such that\nX\nf−→RY\nsoftmax\n−−−−→D(Y ) = X\nc−→D(Y )\n174\nProbabilistic models\nSection 3.1\nProof. In order to prove the existence of f, we construct the log likelihood function.\nGiven c : X →D(Y ) there is a Bayesian inverse c† : Y →D(X), then we can deﬁne\nthe likelihood l : X × Y →R+ by:\nl(x, y) = c†(x|y)p(y)\nand the log likelihood is given by\nf(x) = logY (l(x, y)) ∈RY\nwhere logY : (R+)Y →RY is the logarithm applied to each entry. Then one can\nprove using Bayes law that:\nsoftmax(f) = softmax(log(l(x, y))) =\nl(x, y)\nP\ny′ l(x, y′) = c†(x|y)p(y)\nP\ny′ c†(x|y′) = c(y|x)\n(3.3)\nNote that many functions f may induce the same channel c in this way. The\nencoder f : X →(R+)Y given in the proof is the log of the likelihood function\nl : X × Y →R+. In these instances, the encoder is well behaved probabilistically,\nsince it satisﬁes the version 3.3 of Bayes equation. However, in most gradient-based\napplications of stochastic methods, encoders X →(R)Y such as those built from a\nneural network tend to achieve a better performance.\nWe already saw in Section 2.3, that softmax can be used to turn neural networks\ninto probabilistic models. We rephrase this in the following deﬁnition.\nDeﬁnition 3.1.7. Given any recursive neural network F : G →NN(W) for a\nmonoidal grammar G with F(s) = nand F(v) = 0 for v ∈V ⊆G0, and parameters\nθ : W →R, we can build a discriminator ˜F : L(G) →D([n]) as the following\ncomposition:\n˜F = L(G)\nparsing\n−−−−→\na\nu∈V ∗\nMC(G)(u, s)\nF−→NN(W)(0, n)\nIθ\n−→Rn\nsoftmax\n−−−−→D([n])\nwhere Iθ : NN(W) →SetR is the functor deﬁned in2.3.\nSoftmax induces a function S : MatR →Prob deﬁned on objects by n 7→[n] for\nn ∈N and on arrows by:\nn\nM\n−→m\n7→\n[n]\nS(M)\n−−−→D([m]) = [n]\n|−⟩\n−−→Rn\nM·\n−→Rm\nsoftmax\n−−−−→D([m])\nWhere |−⟩is the one-hot encoding which maps each element of [n] to the correspond-\ning basis vector in Rn. Note that the mapping S is not functorial, but it is surjective\nas can readily be shown from Proposition 3.1.6.\nDeﬁnition 3.1.8. Given any tensor network model F : G →MatR for a rigid gram-\nmar G with F(s) = n and F(w) = 1 for w ∈V ⊆G0 we can build a discriminator\n˜F : L(G) →D([n]) as the following composition:\n˜F = L(G)\nparsing\n−−−−→\na\nu∈V ∗\nRC(G)(u, s)\nF−→MatR(1, n)\nS−→D([n])\n175\nChapter 3\nGames for Pragmatics\nExample 3.1.9 (Student). Consider a student who is faced with a question in L(G, q),\nand has to guess the answer. This can be seen as a probabilistic channel L(G, q) →\nD(A) where A is a set of possible answers. We can build this channel using a ten-\nsor network model F : G →MatR with F(q) = RA which induces a discriminator\n˜F : L(G, q) →D(A), as deﬁned above. The image under ˜F of the grammatical ques-\ntion “Who was the emperor of France” in L(G, q) is given by the following diagram,\nrepresenting a distribution in D(A):\nWho\nemperor\nwas\nof\nFrance\nwhere the bubble represents the unary operator on hom-sets S.\nDeﬁnition 3.1.10. Given any relational model F : G →Rel for a rigid grammar\nG with F(s) = Y and F(w) = 1 for w ∈V ⊆G0 we can build a discriminator\n˜F : L(G) →D(Y ) as follows: ˜F = L(G)\nparsing\n−−−−→`\nu∈V ∗RC(G)(u, s)\nF−→Rel(1, Y ) ≃\nP(Y )\nuniform\n−−−−→D(Y ) where uniform takes a subset of Y to the uniform distribution\nover that subset.\nExample 3.1.11 (Literal listener). Consider a listener who hears a word in W and\nneeds to choose which object in R the word refers to.\nThis example is based on\nBergen et al.\n[BLG16].\nSuppose we have a relation (called lexicon in [BLG16])\nϕ : W × R →B, where ϕ(w, r) = 1 if w could refer to r and 0 otherwise. We can\ntreat ϕ as a likelihood ϕ : W × R →R+. and model a literal listener as a classiﬁer\nl : W →D(R) deﬁned by:\nl(w|r) ∝ϕ(w, r)\nThis is the same as taking l(r) ∈D(W) to be the uniform distribution over the words\nthat could refer to r according to ϕ. Thus the literal listener does not take the context\ninto account. We can extend this model by allowing noun phrases to be uttered, instead\nof single words, i.e. W = L(G, n) for some rigid grammar G with noun type n ∈G0.\nThen we can replace the relation ϕ : W × R →B by a relational model F : G →Rel\nwith F(n) = R, so that any grammatical noun phrase g : u →n in RC(G) is mapped\nto a subset F(g) ⊆R of the objects it could refer to. Then the listener is modeled by:\nl(g|r) ∝⟨F(g)|r⟩\nwhere ⟨F(g)|r⟩= 1 if r ∈F(g) and is zero otherwise. This corresponds to taking\nl = ˜F as deﬁned in the proposition above. As we will see in Example 3.1.13 and\nSection 3.4.1, we can use a literal listener to deﬁne a pragmatic speaker which reasons\nabout the literal interpretation of its words.\n176\nProbabilistic models\nSection 3.1\n3.1.3\nGenerators\nIn order to solve NLP tasks, we need generators along-side discriminators. These are\nprobabilistic channels which produce a sentence in the language given some seman-\ntic information in S. They are used throughout NLP, and most notably in neural\nlanguage modelling [Ben+03] and machine translation [BCB14].\nDeﬁnition 3.1.12 (Generator). A generator for S in G is a probabilistic channel:\nc : S →DL(G)\nBayesian probability theory allows to exploit the symmetry between encoders and\ndecoders. Recall that Bayes law relates a conditional distribution c : X →D(Y ) with\nits Bayesian inverse c† : Y →D(X) given a prior p : 1 →D(X):\nc†(x|y) =\nc(y|x)p(x)\nP\nx′ c(y|x′)p(x′)\nThus Bayesian inversion † can be used to build a decoder c† : Y →DL(G) from an\nencoder c : L(G) →D(Y ) given a prior over the language p ∈DL(G), and viceversa.\nExample 3.1.13 (Speaker). Consider a speaker who is given an object in R (e.g. a\nblack chess piece), and needs to produce a word in W to refer to it (e.g. \"bishop\").\nWe can model it as a generator s : R →D(W). Following from Example 3.1.11,\nassume the speaker has access to a relation ϕ : R × W →B. Then she knows what\nthe literal interpretation of her words is, i.e. she can build a channel l : W →D(R)\ncorresponding to a literal speaker. In order to perform her task s = R →D(W), she\ncan take the Bayesian inverse of the literal listener s = l†. We will see in Section\n3.4.1, that this strategy for the teacher yields a Nash equilibrium in a collaborative\ngame with a listener.\nIn practice, it is often very expensive to compute the Bayesian inverse of a channel.\nExpecially when one does not have a ﬁnite table of probabilities X × Y →R+ but\nrather a log-likelihood function X →RY represented e.g. as a neural network. Thus,\nin most cases, we must resort to other tools to build generators. Recurrent neural\nnetworks are the simplest such tool. Indeed, given a recurrent network g : n →n⊕|V |\nin NN we can build a generator Rn →D(V ∗), by picking an initial encoder state\nx ∈Rn and simply iterate the recurrent network by composing it along the encoder\nspace n, see Section 2.3 for examples.\nExample 3.1.14 (Translator/Chatbot). The sequence-to-sequence (Seq2Seq) model\nof Bahdanau et al.\n[BCB14], surveyed at the end of Section 2.3 is composed of\nrecurrent encoder and decoder networks connected by an attention mechanism, see\n(2.16). It was originally used to build a translator V ∗→D(V ′∗) for two vocabularies\nV and V ′. Taking V ′ = V we can use Seq2Seq to build a chatbot V ∗→D(V ∗). We\nwill use these in 3.3.15.\n177\nChapter 3\nGames for Pragmatics\nFrom a categorical perspective, we do not understand generators as well as dis-\ncriminators. If generators arise from functors, discriminators should arise from a dual\nnotion, i.e. a concept of cofunctor, but we were unable to work out what these should\nbe. Recently, Toumi and Koziell-Pipe [TK21] introduced a notion of functorial lan-\nguage model which allows to generate missing words in a grammatical sentence using\nDisCoCat models. Indeed, if we assume that the grammatical structure is given, then\nwe may generate sentences with that structure using an activation function. We give\nan example in the case of relational models.\nExample 3.1.15 (Teacher). Consider a teacher who knows an answer (e.g. “Napoleon”),\nand needs to produce a question with that answer (e.g. “Who was emperor of France?”).\nSuppose that the teacher has access to a relational model F : G →Rel for a grammar\nG with a question type q and a noun type n with F(q) = F(n) (i.e. answers are\nnouns). Following [Coe+18; TK21], we may repackage the functor F via an embed-\nding E : N →F(n) in Rel where N = { w ∈V | (w, n) ∈G } is the set of nouns, so\nthat F(w →n) = E |w⟩. Fixing a grammatical structure g : u →q, the teacher can\ngenerate questions with that structure, by evaluating the structure in her model F and\nthen uniformly choosing which nouns to use in the question. This process is shown in\nthe following diagram:\nWho\nwas\nof\nE\nE\nwhere the bubble indicates the uniform operator on hom-sets of Rel.\nRead from\nbottom to top, the diagram above represents a channel N →D(N ×N) which induces\na channel N →D(L(G, q)). However, taking the uniform function is not a pragmatic\nchoice for the teacher. Indeed, given “Napoleon” as input, the channel above is equally\nlikely to choose the question “Who was emperor of France?” as “Who was citizen of\nFrance?”. We will see in Section 3.4.2, that the pragmatics of posing questions may\nbe captured by an adversarial scenario in which the teacher aims to ask hard questions\nto a student who tries to answer them.\n178\nBidirectional tools\nSection 3.2\n3.2\nBidirectional tools\nThe concept of reward is central in both game theory and machine learning. In the\nﬁrst, it is formalised in terms of a utility function and allows to deﬁne the optimal\nstrategies and Nash equilibria of games. In the second, where it is captured (pes-\nsimistically) using a loss function, it deﬁnes an objective that the learning system\nmust minimise. Reward is also a central concept in reinforcement learning where\none considers probabilistic processes which run through a state-space while collect-\ning rewards [How60; GS14]. In this section we show that the information ﬂow of\nrewards in a dynamic probabilistic system can be captured in a suitable category of\nlenses. The bidirectionality of lenses allows to represent the action-reaction structure\nof game-theoretic and machine learning systems.\n3.2.1\nLenses\nLenses are bidirectional data accessors which were introduced in the context of the\nview-update problem in database theory [BPV06; JRW12], although they have an-\ntecedents in Godel’s “Dialectica interpretation”. They are widely used in functional\nprogramming [PGW17] and have recently received a lot of attention in the applied\ncategory theory community, due to their applications to machine learning [FST19;\nFJ19] and game theory [Gha+18]. There are several variants on the deﬁnition of lenses\navailable in the literature. These were largely uniﬁed by Riley who introduced optics\nas generalisation of lenses from Set to any symmetric monoidal category [Ril18a]. We\nuse the term lens instead of optics for this general notion.\nDeﬁnition 3.2.1 (Lens). [Ril18a] Let X, Y, O, S be objects in a symmetric monoidal\ncategory C. A lens or optic [f, v] : (X, S) →(Y, O) in C is given by the following\ndata:\n• an object M ∈C,\n• a forward part f : X →M ⊗Y called “get”,\n• a backward part v : M ⊗O →S called “put”.\nStripped out of its set-theoretic semantics, a lens is simply seen as a pair of\nmorphisms arranged as in the following two equivalent diagrams.\nf\nv\nf, v\nX\nS\nR\nY\nY\nX\nS\nR\nM\n=\n(3.4)\nTwo lenses [f, v], [f ′, v′] : (X, S) →(Y, R) are said to be equivalent, written [f, v] ∼=\n179\nChapter 3\nGames for Pragmatics\n[f ′, v′], if there is a morphisms h : M →M ′ satisfying\nf\nv′\nY\nX\nS\nR\nM\nh\nM ′\nf ′\nv′\nY\nX\nS\nR\nM ′\n=\n=\nf\nv\nY\nX\nS\nR\nM\n(3.5)\nThe quotient of the set of lenses by the equivalence relation ∼= can be expressed\nas a coend formula [Ril18b]. Here we omit the coend notation for simplicity. This\nquotient is needed in order to show that composition of lenses is associative and\nunital. Indeed, as shown by Riley [Ril18b], equivalence classes of lenses under ∼=\nform a symmetric monoidal category denoted Lens(C). Sequential composition for\n[f, u] : (X, S) →(Y, O) and [g, v] : (Y, O) →(Z, Q) is given by [g, v] ◦[f, u] =\n[(idMf ⊗g) ◦f, v ◦(idMf ⊗u)] with Mg◦f = Mf × Mg, diagrammatically we have:\nf\nv\nY\nX\nS\nR\nMf\ng\nu\nZ\nQ\nMg\n(3.6)\nand tensor product [f, v] ⊗[g, u] is given by:\nf\nv\nY\nX\nS\nR\nMf\ng\nu\nZ\nQ\nMg\nY\nR\n(3.7)\nMoreover, there are cups allowing to turn a covariant wire in the contravariant direc-\ntion.\n,\n(3.8)\nRemark 3.2.2. This diagrammatic notation is formalised in [Hed17], where cate-\ngories endowed with this structure are called teleological. Note that LensC is not\ncompact-closed, i.e. we can only turn wires from the covariant to the contravariant\ndirection. When we draw a vertical wire as in 3.5, we actually mean a cup as in 3.8,\nthis makes the notation more compact.\n180\nBidirectional tools\nSection 3.2\nWe interpret lenses along the lines of compositional game theory [Gha+18]. A\nlens is a process which makes observations in X, produces actions or moves in Y ,\nthen it gets some reward or utility in R and gives back information as to its degree\nof satisfaction or coutility in S. Of course, this interpretation in no way exhausts\nthe possible points of vue on lenses.\nFor instance in [FST19] and [Cru+21], one\ninterprets lenses as smooth functions turning inputs in X into outputs in Y , and then\nbackpropagating the error in R = ∆Y to an error in the input S = ∆X.\nWe are particularly interested in lenses over the category Prob of conditional\nprobability distributions, which we call stochastic lenses. These have been charac-\nterised in the context of causal inference [JKZ18], where they are called combs, as\nmorphisms of Prob satisfying a causality condition.\nDeﬁnition 3.2.3 (Comb). [JKZ18] A comb is a stochastic map c : X ⊗R →Y ⊗S\nsatisfying for some c′ : X →Y , the following equation:\nX\nY\nR\nS\nc\nX\nY\nR\nc′\n=\n(3.9)\nCombs have an intuitive diagrammatic representation from which they take their\nname.\nX\nY\nR\nS\nc\n(3.10)\nWith this diagram in mind, the condition 3.9 reads: “the contribution from input R\nis only visible via output S ”[JKZ18].\nProposition 3.2.4. In Prob, combs X⊗R →Y ⊗S are in one-to-one correspondence\nwith lenses (X, S) →(Y, R).\nProof. The translation works as follows:\nf\nv\nY\nX\nS\nR\nM\n7→\nX\nY\nR\nS\nf\nv\nM\nX\nY\nR\nS\nc\n7→\nc′\nc|X⊗Y\nY\nX\nS\nR\nM\n(3.11)\nwhere c′ is the morphism deﬁned in 3.2.3 and c|X⊗Y is the conditional deﬁned by\nProposition 3.1.3. Note that c|X⊗Y is not unique in general. However any such choice\n181\nChapter 3\nGames for Pragmatics\nyields equivalent lenses since the category Prob is productive, see [DdR22, Theorem\n7.2] for a slightly more general result. Indeed, this proposition is the base case for\nthe inductive proof of [DdR22, Theorem 7.2].\nWhy should we use lenses (X, S) →(Y, R) instead of combs?\nThe diﬀerence\nbetween them is in the composition. Indeed, composing lenses corresponds to nesting\ncombs as in the following diagram:\nY\nZ\nQ\nR\ng\nu\nf\nv\nY\nX\nS\nR\nMf\ng\nu\nZ\nQ\nMg\n⇐⇒\nX\nS\nf\nv\nMg\nMf\nAs we will see, the composition of lenses allows to deﬁne a notion of feedback for\nprobabilistic systems which correctly captures their dynamics.\n3.2.2\nUtility functions\nWe analyse a special type of composition in Lens(Prob): between a lens and its\nenvironment, also called context in the open games literature [BHZ19]. This is a\ncomb in Lens(C) that ﬁrst produces an initial state in X, then receives a move in Y\nand produces a utility or reward in R.\nDeﬁnition 3.2.5 (Context). The context for lenses of type (X, S) →(Y, R) over C\nis a comb c in Lens(C) of the following shape:\nX\nS\nR\nY\nC\n(3.12)\nProposition 3.2.6. Contexts C for lenses of type (X, S) →(Y, R) in Prob are in\none-to-one correspondence with pairs [p, k] where p ∈D(X) is a distribution over\nobservations called prior and k : X × Y →D(R) is a channel called utility function.\nProof. Since the unit of the tensor is terminal in Prob, a context C of the type above\nis given by a pair of morphisms, [p′, k′] as in the following diagram:\nX\nS\nR\nY\np′\nk′\nM\nR\n(3.13)\n182\nBidirectional tools\nSection 3.2\nMoreover, by the disintegration theorem, the joint distribution p′ above may be fac-\ntored into a prior distribution p over X and a channel c : X →D(M). Therefore the\ncontext above is equivalent to to a context induced by a distribution over starting\nstate p ∈D(X) and a utility function k : X × Y →R as in the following diagram:\nX\nS\nR\nY\np\nk\nX\nR\n(3.14)\nAs we will see in Section 3.3.2, this notion of utility captures the notion from game\ntheory where utility functions assign a reward in R given the move of the player in Y\nand the moves of the other players, see also [BHZ19].\n3.2.3\nMarkov rewards\nAnother interesting form of composition in Lens(Prob) arises when considering the\nnotion of a Markov reward process (MRP) [How60]. MRPs are dynamic probabilistic\nsystems which transit through a state space X while collecting rewards in R.\nDeﬁnition 3.2.7 (MRP). A Markov reward process with state space X is given by\nthe following data:\n1. a transition function T : X →D(X),\n2. a payoﬀfunction R : X →R,\n3. and a discount factor γ ∈[0, 1).\nThis data deﬁnes lens [T, Rγ] : (X, R) →(X, R) in Prob drawn as follows:\nT\nRγ\nX\nX\nR\nR\nX\n(3.15)\nwhere Rγ : X × R →D(R) is given by the one-step discounted payoﬀ:\nRγ(x, r) = R(x) + γr\nIntuitively, the MRP observes the state x ∈X which it is in and collects a reward\nR(x), then uses the transition T to move to the next state. Given an expected future\nreward r, the MRP computes the current value given by summing the current reward\nwith the expected future reward discounted by γ. This process is called Markov,\n183\nChapter 3\nGames for Pragmatics\nbecause the state at time step t + 1 only depends on the state a time step t, i.e. xt+1\nis sampled from the distribution T(xt).\nThus an MRP is an endomorphism [T, Rγ] : (X, R) →(X, R) in the category\nof stochastic lenses. We can compose [T, Rγ] with itself n times to get a new lens\n[T, Rγ]n where the forward part is given by iteration of the transition function and\nthe backward part is given by the n-step discounted payoﬀ:\nRn\nγ\n−(x, r) =\nn−1\nX\ni=1\nγiR(T i(x)) + γnr\nSince 0 ≤γ < 1, this expression converges in the limit as n →∞if R and T\nare deterministic. When R and T are stochastic, one can show that the expectation\nE(Rn\nγ\n−) converges to the value of the MRP which yields a measure of the total reward\nexpected from this process.\nvalue([T, Rγ])(x) = E(\n∞\nX\ni=1\nγiR(T i(x))) = lim\nn→∞E(\nn\nX\ni=1\nγiR(T i(x)))\n(3.16)\nWe can represent this as an eﬀect v = value([T, Rγ]) : (X, R) →(1, 1) in Lens(Prob),\nwhich simply consists in a probabilistic channel v : X →R. This eﬀect v satiﬁes the\nfollowing Bellman ﬁxed point equation in Lens(Prob), characterising it as the iter-\nation of [T, Rγ], see [GS14].\nv(x) = E(R(x) + γv(T(x)))\nwhich we may express diagrammatically as:\nT\nRγ\nX\nX\nR\nR\nX\nv\nX\nv\n=\nE\nR\n(3.17)\nwhere E denotes the conditional expectation operator, given for any channel f : X →\nD(R) by the function E(f) : X →R deﬁned by E(f)(x) = P\ny∈R yf(y|x) (note that\nthere only ﬁnitely many non-zero terms in this discrete setting). The value v of the\nreward process is often estimated by running Monte Carlo methods which iterate\nthe transition function while collecting rewards. Note that not all stochastic lenses\n(X, R) →(X, R) have an eﬀect (X, R) →(1, 1) with which they satisfy Equation\n3.17. In fact it is suﬃcient to set T = idX, R(x) = 1 for all x ∈X and γ > 1 in order\nto get a counter-example. It would be interesting to characterise the stochastic lenses\nsatisfying 3.17 algebraically, e.g. are they closed under composition? This would\nin fact be true if the conditional expectation operator E was functorial. Given the\nresults of [Cha+09] and [AR18], we expect that conditional expectation can be made\na functor by restriciting the objects of Prob.\n184\nCybernetics\nSection 3.3\n3.3\nCybernetics\nParametrization is the process of representing functions X →Y via a parameter\nspace Π with a map Π →(X →Y ).\nIt is a fundamental tool in both machine\nlearning and game theory, since it allows to deﬁne a notion of agency, through the\nchoice of parameters. For example, players in a formal game are parametrized over\na set of strategies: there is a function X →Y , turning observations into moves, for\nany strategy in Π. In reinforcement learning, the agent is parametrized by a set of\npolicies, describing how to turn states into actions. We show that parametrized lenses\nare suitable for representing these systems and give examples relevant for NLP.\n3.3.1\nParametrization\nCategories allow to distinguish between two types of parametrization. Let S be a\nsemantic category with a forgetful functor S ,−→Set\nDeﬁnition 3.3.1 (External parametrization). An external parametrization (f, Π) of\nmorphisms X →Y in a category S, also called a family of morphisms indexed by\nΠ, is a function f : Π →S(X, Y ). These form a category Fam(S) with composition\ndeﬁned for f : Π0 →S(X, Y ) and g : Π1 →S(Y, Z) by g ◦f : Π0 × Π1 →S(X, Z)\nwith g ◦f(π0, π1) = f(π0) ◦g(π1).\nDeﬁnition 3.3.2 (Internal parametrization). An internal parametrization (f, Π) of\nmorphisms X →Y in a monoidal category S is a morphism f : Π ⊗X →Y in S.\nThese form a category Para(S) with composition deﬁned for f : Π0 ⊗X →Y and\ng : Π1 ⊗Y →Z by g ◦f : Π0 ⊗Π1 ⊗X →Z with g ◦f = f(π0) ◦g(π1).\nWhich parametrization should we prefer, Fam or Para?\nIt depends on con-\ntext. Internal parametrization is usually a stricter notion, because it imposes that\nthe parametrization be a morphism of S and not simply a function.\nFor exam-\nple, Para(Smooth) embeds in Fam(Smooth) but the embedding is not full, i.e.\nthere are external parametrizations deﬁned by non-diﬀerentiable functions. In fact,\nthe Para construction was introduced in the context of gradient-based learning\n[Cru+21], where it is very desirable that the parametrization be diﬀerentiable. Ex-\nternal parametrizations are mostly used in the compositional game theory literature\n[Gha+18], since they are more ﬂexible and allow to deﬁne a notion of best response\n(see 3.3.6). However they are also inextricably linked to Set, making them less desir-\nable from a categorical perspective, since it is harder to prove results about Fam(S)\ngiven knowledge of S.\nNote that the two notions coincide for the category of sets and functions. Indeed,\nsince Set is cartesian closed, we have that:\nΠ →Set(X, Y ) ⇐⇒Π →(X →Y ) ⇐⇒Π × X →Y\nand therefore Para(Set) ≃Fam(Set). Even though Prob is not cartesian closed,\nthese notions again coincide.\n185\nChapter 3\nGames for Pragmatics\nProposition 3.3.3. Internal and external parametrizations coincide in Prob, i.e.\nFam(Prob) ≃Para(Prob).\nProof. This follows by the following derivation in Set.\nΠ →Prob(X, Y ) ⇐⇒Π →(X →D(Y )) ⇐⇒Π × X →D(Y )\nIn fact, the derivation above holds in any Kleisli category for a commutative strong\nmonad.\nProposition 3.3.4. For any commutative strong monad M : Set →Set, the Kleisli\ncategory Kl(M) is monoidal and we have:\nFam(Kl(M)) ≃Para(Kl(M))\ni.e. the notions of internal and external parametrization for Kleisli categories coin-\ncide.\n3.3.2\nOpen games\nWe now review the theory of open games [Gha+18; BHZ19].\nStarting from the\ndeﬁnition of lenses, open games are obtained in two steps. First an open game is a\nfamily of lenses parametrized by a set of strategies: for each strategy the forward part\nof the lens says how the agent turns observations into moves and the backward part\nsays how it computes a payoﬀgiven the outcome of its actions. Second, this family\nof lenses is equipped with a best response function indicating the optimal strategies\nfor the agent in a given context.\nDeﬁnition 3.3.5 (Family of lenses). A family of lenses over a symmetric monoidal\ncategory C is a morphism in Fam(Lens(C)).\nExplicitly, a family of lenses P :\n(X, S) →(Y, R) is given by a function P : Π →Lens(C)((X, S), (Y, R)) for some\nset of parameters Π.\nRecall the deﬁnition of context for lenses given in 3.2.5. Let us use the notation\nC((X, S), (Y, R)) for the set of contexts of lenses (X, S) →(Y, R).\nDeﬁnition 3.3.6 (Best response). A best response function for a family of lenses\nP : (X, S) →(Y, R) is a function of the following type:\nB : C((X, S), (Y, R)) →P(P)\nThus B takes as input a context and outputs a predicate on the set of parameters (or\nstrategies).\nDeﬁnition 3.3.7 (Open game). [BHZ19] An open game G : (X, R) →(Y, O) over\na symmetric monoidal category C is a family of lenses { [π, vπ] }π∈PG in C equipped\nwith a best response function BG.\n186\nCybernetics\nSection 3.3\nProposition 3.3.8. [BHZ19] Open games over C form a symmetric monoidal cate-\ngory denoted Game(C).\nProof. See [BHZ19] for details on the proof and the composition of best responses in\nthis general case.\nThe category Game(C) admits a graphical calculus developed in [Hed17] which\nis the same as the one for lenses described in Section 3.2.1..\nEach morphism is\nrepresented as a box with covariant wires for observations in X and moves in Y , and\ncontravariant wires for rewards in R (called utilities in [Gha+18]) and satisfaction in\nS (called coutilities in [Gha+18]).\nG\nX\nY\nS\nR\n(3.18)\nThese boxes can be composed in parallel, indicating that the players make a move\nsimultaneously, or in sequence, indicating that the second player can observe the ﬁrst\nplayer’s move.\nG\nH\nX\nY\nZ\nS\nR\nQ\nG\nH\nX\nY\nZ\nS\nY\nQ\nR\nR\n(3.19)\nOf particular interest to us is the category of open games over Prob. Since we\nfocus on this category for the rest of the chapter we will use the notation Game :=\nGame(Prob). In this category the best response function is easier to specify, since\ncontexts factor as in Proposition 3.2.6.\nProposition 3.3.9. A context for an open game in Game(Prob) (X, S) →(Y, R)\nis given by a stochastic lens [p, k] : (1, X) →(Y, R) or explicitly by a pair of channels\np : 1 →D(M × X) and k : M × Y →D(R).\nProof. This follows from Proposition 3.2.6.\nWith this simpler notation for contexts as pairs [p, k] we can deﬁne the composition\nof best responses for open games.\nDeﬁnition 3.3.10. [BHZ19, Deﬁnition 3.18] Given open games (X, S)\nG−→(Y, R)\nH\n−→\n(Z, O) the composition of their best responses is given by the cartesian product:\nBG·H([p, k]) = BG([p, H · k]) × BH([p · G, k]) ⊆ΠG × ΠH\nwhere ΠG and ΠH are the corresponding sets of strategies.\nWe can now deﬁne a notion of utility-maximising agent\nDeﬁnition 3.3.11 (Utility-maximising agent). An (expected) utility maximising agent\nis a morphism c : (X, 1) →(Y, R) in Game given by the following data:\n187\nChapter 3\nGames for Pragmatics\n1. π ∈Π = X →D(Y ) is the set of strategies.\n2. f +\nπ : X →D(A) : x 7→π(x)\n3. f −\nπ : R →1 is the discard map.\n4. B : ([p : 1 →M × X, k : M × Y →R]) = argmaxπ∈P(E(p; idM ⊗fπ; k))\nwhere argmax : RΠ →P(Π).\nNote that it is suﬃcient to specify the input type X and output type Y in order\nto deﬁne a utility-maximising agent (X, 1) →(Y, R) in Game.\nExample 3.3.12 (Classiﬁer utility). We model a classiﬁer X →D(Y ) as a utility-\nmaximising agent c : (X, 1) →(Y, R). Assume that Y has a metric d : Y × Y →R.\nGiven a dataset of pairs, i.e. a distribution K ∈D(X × Y ), we can deﬁne a context\n[K, k] for c by setting k = −d, as shown in the following pair of equivalent diagrams:\nX\nR\nY\np\nk\nX\nR\nK|X\nc\nY\nX\nR\nY\nK\nk\nR\nc\nY\n=\nwhere (p, K|X) is the disintegration of K along X. This yields a distribution over the\nreal numbers R corresponding to the utility that classiﬁer c gets in its approximation\nof K|X. The best response function for c in this context is given by:\nB([K, k]) = argmaxcE(−K; idY ⊗c; d)\nMaximising this expectation corresponds to minimising the average distance between\nthe true label y ∈Y and the predicted label c(x) for (x, y) distributed as in the dataset\nK ∈D(X × Y ).\nExample 3.3.13 (Generator/Discriminator). Fix a grammar G, a feature set Y with\na distance function d : Y ×Y →R and a disribution p ∈D(Y ). We model a generator\nG : Y →DL(G) and a discriminator D : L(G) →D(Y ) as utility-maximising agents\nand compose them as in the following diagram:\np\nG\nD\nL(G)\nR\nY\nY\nY\nR\nd\n−\nR\nWhere −: R →R is multiplication by −1. This yields an adversarial (or zero-sum)\ngame between the generator and the discriminator. Assuming G and D are utility-\nmaximising agents, the best response function for this closed game is given by:\nargmaxG(Ey∈p(d(y, D(G(y)))) × argmaxD(−Ey∈p(d(y, D(G(y)))))\n188\nCybernetics\nSection 3.3\nwhere we use the notation Ey∈p to indicate the expectation (over the real numbers)\nwhere y is distributed according to p ∈D(Y ). Thus we can see that the game reaches\nan equilibrium when the discriminator D can always invert the generator G, and the\ngenerator cannot choose any strategy to change this. Implementing G and D as neural\nnetworks yields the generative adversarial architecture of Goodfellow et al. [Goo+14]\nwhich is used to solve a wide range of NLP tasks, see [WSL19] for a survey. In\nSection 3.4.2, we will see how this architecture can be used for question answering.\nIn the remainder of this chapter, we will work interchangeably in the category\nof families of stochastic lenses Fam(Lens(Prob)) or in Game(Prob).\nThe only\ndiﬀerence between these categories is that the latter has morphisms equipped with\na best response function and a predeﬁned way of composing them. As we will see,\nthis deﬁnition of best response is not suited to formalising repeated games and the\ncategory of families of stochastic lenses gives a more ﬂexible approach.\n3.3.3\nMarkov decisions\nWe now study Markov decision processes (MDP) [How60] as parametrized stochastic\nlenses. These are central in reinforcement learning, they model a situation in which a\nsingle agent makes decisions in a stochastic environment with the aim of maximising\nits expected long-term reward. An MDP is simply an MRP 3.2.7 parametrised by a\nset of actions.\nDeﬁnition 3.3.14 (MDP). A Markov decision process with states in X and actions\nin A is given by the following data:\n1. a transition T : A × X →D(X)\n2. a reward function R : A × X →D(R)\n3. a discount factor γ ∈[0, 1)\nA policy is a function π : X →D(Y ) which represents a strategy for the agent:\nit yields a distribution over the actions π(x) ∈D(Y ) for each state x ∈X. Given\na policy π ∈Π = X →D(Y ), the MDP induces an MRP which runs through the\nstate space X by choosing actions according to π and collecting rewards. Thus, we\ncan formalise an MDP as a family of MRPs parametrized by the policies Π, i.e.\na morphism Pπ : (X, R) →(X, R) in Fam(Lens(Prob)), given by the following\ncomposition:\nπ\nT\nR\n+γ\nX\nR\nX\nA\nA\nR\nR\n=\nX\nR\nX\nR\nPπ\n(3.20)\n189\nChapter 3\nGames for Pragmatics\nThe aim of the agent is to maximise its expected discounted reward. Given a\npolicy π, the MRP Pπ induces a value function vπ = value(Pπ) : X →R, as deﬁned\nin Section 3.2. Again, we have that the Bellman expectation equation holds:\nvπ\n=\nX\nR\nvπ\nX\nR\nX\nR\nPπ\nE\n(3.21)\nThis ﬁxed-point equation gives a way of approximating the value function by iterating\nthe transitions and rewards under a given policy. The aim is then to ﬁnd the optimal\npolicy π∗at starting state x0 by maximising the corresponding value function vπ:\nπ∗= argmaxπ∈P(vπ(x0)).\nNote that the this optimal policy is not captured by the open games formalism. If\nwe took the MDP Pπ to be a morphism in Game, i.e. if we equipped it with a best\nresponse B as deﬁned in 3.3.6, then the sequential composition Pπ · Pπ would have a\nbest response function with target Π×Π, i.e. the strategy for an MDP with two stages\nwould be a pair of policies for the ﬁrst and second stage, contradicting the idea that the\nMDP tries to ﬁnd the best policy to use at all stages of the game. Working in the more\ngeneral setting of parametrized stochastic lenses we can still reason about optimal\nstrategies, although it would be interesting to have a compositional characterization\nof these.\nExample 3.3.15 (Dialogue agents). In [Li+16], Li et al. use reinforcement learning\nto model dialogue agents. For this they deﬁne a Markov decision process with the\nfollowing components. Fix a set of words V . The states are given by pairs (p, q)\nwhere q ∈V ∗is the current sentence uttered (to which the agent needs to reply)\nand p ∈V ∗is the previous sentence uttered by the agent. An action is given by a\nsequences of words a ∈V ∗. The policy is modeled by a Seq2Seq model with attention\nbased on Bahdanau et al. [BCB14], which they pretrain on a dataset of dialogues.\nRewards are computed using three reward functions R1, R2 and R3, where R1 penalises\ndull responses a (by comparing them with a manually constructed list), R2 penalizes\nsemantic similarity between consecutive statements of the agent p and a, and R3\nrewards semantic coherence between p, q and a. These three rewards are combined\nusing a weighted sum λsum(r1, r2, r3) = λ1r1 + λ2r2 + λ3r3. We can depict the full\narchitecture as one diagram:\nSeq2Seq\nR1\nR2\nR3\nλsum\nq\np\nR\nR\na\nR\nR\nR\n190\nCybernetics\nSection 3.3\n3.3.4\nRepeated games\nThe deﬁnition of MDP given above captures a single agent interacting in a ﬁxed\nenvironment with no agency. In real-world situations however, the environment con-\nsists itself in a collection of agents which also make choices so as to maximise their\nexpected reward.\nIn order to allow many agents interacting in an environment, we break up the\ndeﬁnition of MDPs above, and isolate the part that is making decisions from the\nenvironment part. We pack the transition T, reward R and discount factor γ into\none lens called environment and given by:\nE = [T, Rγ] : (X × A, R) →(X, R)\nwhere Rγ : X × A × R →R is deﬁned by\nRγ(x, a, r) = R(x, a) + γr\nThis allows to isolate the decision part, seen as an open game of the following type:\nD : (X, 1) →(A, R)\nwhich represents an agent turning states in X into actions in A and receiving rewards\nin R. Explicitly we have the following deﬁnition.\nDeﬁnition 3.3.16 (Decision process). A decision process D with state space X,\naction space A and discount factor γ is a utility maximising agent D : (X, 1) →(A, R)\nComposing D with the environment E as in the following diagram, we get back\nprecisely the deﬁnition of MDP.\nPπ\nT\nRγ\nDπ\nX\nX\nR\nR\nA\nX\nR\n=\nX\nR\nX\nR\n(3.22)\nWe can now consider a situation in which many decision processes interact in an\nenvironment.\nStochastic games, a.k.a Markov games, were introduced by Shapley in the 1950s\n[Sha53]. They can be seen as a generalisation of MDPs, even though they appeared\nbefore the work of Bellman [Bel57].\nDeﬁnition 3.3.17 (Stochastic game). A stochastic game is given by the following\ndata:\n1. A number k of players, a set of states X and a discount factor γ,\n2. a set of actions Ai for each player i ∈{0, . . . , k −1},\n3. a transition function T : X × A0 × · · · × Ak →D(X),\n191\nChapter 3\nGames for Pragmatics\n4. a reward function R : X × A0 × · · · × Ak →Rk.\nThe game is played in stages. At each stage, every player observes the current\nstate in X and chooses an action in Ai, then “Nature” changes the state of the game\nusing transition T and every player is rewarded according to R. It is easy express\nthis repeated game as a parallel composition of decision processes Di followed by the\nenvironment [T, Rγ] in Game. As an example, we give the diagram for a two-player\nzero-sum stochastic game.\nT\nRγ\nDπ0\nX\nX\nR\nR\nA0\nX\nR\nDπ1\nA1\n−\n(3.23)\nIn this case, one can see directly from the diagram, that the zero-sum stochastic game\ninduces an MDP with action set Π0 × Π1 given by pairs of policies from players 0\nand 1. A consequence of this is that one can prove the Shapley equations [Ren19],\nthe analogue of the Bellman equation of MDPs, for determining the equilibria of the\ngame.\nNote that stochastic games only allow players to make moves in parallel, i.e. at\nevery stage the move of player i is independent of the other players’ moves. The gen-\nerality provided by this compositional approach allows to consider repeated stochastic\ngames in which moves are made in sequence, as in the following example.\nExample 3.3.18 (Wittgenstein’s builders). [Wit53] Consider a builder and an ap-\nprentice at work on the building site. The builder gives orders to his apprentice, who\nneeds to implement them by acting on the state of the building site.\nIn order to\nmodel this language game we start by ﬁxing a grammar G for orders, e.g. the regular\ngrammar deﬁned by the following labelled graph:\ns0\nx0\nx1\no\nbricks\nslabs\npillars\nlarge\nsmall\nBring\nCut\nx2\nx3\nwood\nthe\nand\nThis deﬁnes a language for orders L(G) given by the paths s0 →o in the graph above.\nThe builder observes the state of the building site S and produces an order in L(G),\nwe can model it as a channel πB : S →DL(G). We assume that the builder has\na project P, i.e. a subset of the possible conﬁgurations of the building site that he\nﬁnds satisfactory P ⊆S. We can model these preferences with a utility function\n192\nCybernetics\nSection 3.3\nuP : S →R, which computes the distance between the current state of the building\nsite and the desired states in P. The builder wants to get the project done as soon as\npossible and he discounts the expected future rewards by a discount factor γ ∈[0, 1).\nThis data deﬁnes a stochastic lens given by the following composition:\nBuilder\nL(G)\nR\nS\nR\n:=\nπB\nL(G)\nR\nS\nR\nuP\n+γ\nThe apprentice receives an order in L(G) and uses it to modify the state of the building\nsite S. We can model this as a probabilistic channel πA : L(G) × S →D(S). This\ndata deﬁnes the following stochastic lens:\nApprentice\nL(G)\nR\nS\n:=\nπA\nS\nR\nS\nS\nL(G)\nWe assume that the apprentice is satisﬁed when the builder is, i.e. the utilities of the\nbuilder are fed directly into the apprentice. This deﬁnes a repeated game, with strategy\nproﬁles given by pairs (πB, πA), given by the following diagram:\nBuilder\nApprentice\nS\nL(G)\nR\nR\nS\nS\nR\nThis example can be extended and elaborated in diﬀerent directions. For example, the\npolicy for the apprentice πA could be modeled as a functor F : G →Set with F(s0) = 1\nand F(o) = S →D(S) so that grammatical orders in L(G) are mapped to channels\nthat change the conﬁguration of the building site.\nAnother aspect is the builder’s\nstrategy πB : S →L(G, o) which could be modeled as a generation process from a\nprobabilistic context-free grammar. Also the choice of state space S is interesting, it\ncould be a discrete minecraft-like space, or a continuous space in which words like\n“cut” have a more precise meaning.\n193\nChapter 3\nGames for Pragmatics\n3.4\nExamples\n3.4.1\nBayesian pragmatics\nIn this section we study the Rational Speech Acts model of pragmatic reasoning [Fra09;\nFG12; GS13; BLG16]. The idea, based on Grice’s conversational implicatures [Gri67],\nis to model speaker and listener as rational agents who choose words attempting to\nbe informative in context. Implementing this idea involves the interaction of game\ntheory and Bayesian inference. While this model has been criticised on the ground of\nattributing excessive rationality to human speakers [Gat+13], it has received support\nby psycholinguistic experiments on children and adults [FG12] and has been applied\nsuccessfully to a referential expression generation task [MP15].\nConsider a collaborative game between speaker and listener.\nThere are some\nobjects or referents in R lying on the table. The speaker utters a word in W referring\nto one of the objects. The listener has to guess which object the word refers to. We\ndeﬁne this reference game by the following diagram in Game(Prob).\np\nSpeaker\nListener\nW\nB\nR\nR\nR\nB\n∆\n(3.24)\nWhere p is a given prior over the referents (encoding the probability that an object\nr ∈R would be referred to) and ∆(r, r′) = 1 if r = r′ and ∆(r, r′) = 0 otherwise. The\nstrategies for the speaker and listener are given by:\nP0 = R →D(W)\nP1 = W →D(R)\nThe speaker is modeled by a utility-maximising agent with strategies π0 : R →D(W)\nand best response in context [p ∈D(R × W), l : R × W →B] given by the π : R →\nD(W) in the argmax of E(l ◦(π ⊗idR) ◦p). Similarly for the listener with the roles\nof R and W interchanged. Composing speaker and listener according to (3.24) we\nobtain a closed game for which the best response is a predicate over the strategy\nproﬁles (π0 : R →D(W), π1 : W →D(R)) indicating the subset of Nash equilibria.\nargmaxπ0,π1(E(∆◦((π1 ◦π0) ⊗idR) ◦copy ◦p)\n(3.25)\nIf we assume that the listener uses Bayesian inference to recover the speaker’s intended\nreferent, then we are in a Nash equilibrium.\nProposition 3.4.1. If π1 : W →D(R) is a Bayesian inverse of π0 : R →D(W)\nalong p ∈D(R) then (π0, π1) is a Nash equilibrium for (3.24).\n194\nExamples\nSection 3.4\nProof. Since π1 is a Bayesian inverse of π0 along p, Equation (3.2) implies the following\nequality:\np\nπ0\nπ1\nW\nR\nR\nR\nB\n∆\np\nπ1\nπ1\nR\nR\nR\nW\nB\n∆\nπ0\n=\nBy deﬁnition of ∆, it is easy to see that the expectation of the diagram above is 1.\nTherefore it is in the argmax of (3.25), i.e. it is a Nash equilibrium for (3.24).\nRemark 3.4.2. We do not believe that all equilibria for this game arise through\nBayesian inversion, but were unable to ﬁnd a counterexample.\nExample 3.4.3. As an example suppose the referents are shapes R = { Bouba, Kiki }\nand the words are W = { pointy, round }. A literal listener would use the following\nchannel π0 : W →D(R) to assign words to their referents.\nBouba\nKiki\nround\n1\n0\npointy\n1/2\n1/2\nA pragmatic speaker would use the Bayesian inverse of this channel to refer to the\nletters.\nBouba\nKiki\nround\n2/3\n0\npointy\n1/3\n1\nA pragmatic listener would be inclined to use the Bayesian inverse of the pragmatic\nspeaker’s channel.\nBouba\nKiki\nround\n1\n0\npointy\n1/4\n3/4\nAnd so on. We can see that Bayesian inversion correctly captures the pragmatics of\nmatching words with their referents in this restricted context.\nFrank and Goodman [FG12] model the conditional distribution π0 : R →D(W)\nwith a likelihood based on an information-theoretic measure known as surprisal or\ninformation content.\nπ0(r)(w) =\n|R(w)|\nP\nw′∈W(r) |R(w′)|\nwhere R(w) is the set of objects to which word w could refer to and W(r) is the set\nof words that could refer to object r. Then given a prior p over the set of objects,\nthe Bayesian inverse π1 = π†\n0 can be computed using Bayes rule. In their experiment,\nthe participants were split in three groups: speaker, salience and listeners. They were\nasked questions to test respectively for the likelihood π0, the prior p and the posterior\npredictions π1 of their model.\n195\nChapter 3\nGames for Pragmatics\nNote that the experiment of Frank and Goodman involved three referents (a blue\nsquare, a blue circle and a green square) and four words (blue, green, square and cir-\ncle). The sets R(w) and W(r) were calculated by hand, and computing the Bayesian\ninverse of π0 in this case was an easy task.\nHowever, as the number of possible\nreferents and words grows, Bayesian inversion quickly becomes computationally in-\ntractable without some underlying compositional structure mediating it.\n3.4.2\nAdversarial question answering\nIn this section we deﬁne a game modelling an interaction between a teacher and a\nstudent. The teacher poses questions that the student tries to answer. We assume\nthat the student is incentivised to answer questions correctly, whereas the teacher\nis incentivised to ask hard questions, resulting in an adversarial question answering\ngame (QA). For simplicity, we work in a deterministic setting, i.e. we work in the\ncategory of in Game(Set). We ﬁrst give a syntactic deﬁnition of QA as a diagram,\nwe then instantiate the deﬁnition with respect to monoidal grammars and functorial\nmodels, which allows us to compute the Nash equilibria for the game.\nLet us ﬁx three sets C, Q, A for corpora (i.e. lists of facts), questions and an-\nswers respectively. Let U be a set of utilities, which can be taken to be R or B. A\nteacher T :\n\u0000C\n1\n\u0001\n↛\n\u0000Q×A\nU\n\u0001\nis a utility-maximising player where each strategy repre-\nsents a function turning facts from the corpus into pairs of questions and answers. A\nstudent S :\n\u0000Q\n1\n\u0001\n↛\n\u0000A\nU\n\u0001\nis a utility-maximising player where each strategy represents\na way of turning questions into answers. A marker is a strategically trivial open\ngame M:\n\u0000A×A\nU×U\n\u0001\n↛\n\u00001\n1\n\u0001\nwith trivial play function and a coplay function deﬁned as\nκM(aT , aS) = (−d(aS, aT ), d(aS, aT )) where d : A × A →U is a given metric on A.\nFinally, we model a corpus as a strategically trivial open game f :\n\u00001\n1\n\u0001\n↛\n\u0000C\n1\n\u0001\nwith\nplay function given by πf(∗) = f ∈C. All these open games are composed to obtain\na question answering game in the following way.\nCorpus\nTeacher\nStudent\nMarker\nC\nA\nQ\nA\nR\nR\n(3.26)\nIntuitively, the teacher produces a question from the corpus and gives it to the student\nwho uses his strategy to answer. The marker will receive the correct answer from the\nteacher together with the answer that the student produced, and output two utilities.\nThe utility of the teacher will be the distance between the student’s answer and the\ncorrect answer; the utility of the student will be the exact opposite of this quantity.\nIn this sense, question answering is a zero-sum game.\nWe now instantiate the game deﬁned above with respect to a pregroup grammar\nG = (B, V, D, s) with a ﬁxed question type z ∈B. The strategies of the student\nare given by relational models σ : G →Rel with σ(z) = 1, so that given a question\nq : u →z, σ(q) ∈P(1) = B is the student’s answer. In practice, the student may only\nhave a subset of models available to him so we set ΣS ⊆{σ : G →Rel : σ(z) = 1}.\n196\nExamples\nSection 3.4\nWe assume that the teacher has the particularly simple role of picking a question-\nanswer pair from a set of possible ones, i.e. we take the corpus C to be a list of\nquestion-answer pairs (q, a) for q : u →z and a ∈A. For simplicity, we assume q\nis a yes/no question and a is a boolean answer, i.e. Q = L(G, z) and A = B. The\nstrategies of the teacher are given by indices ΣT = { 0, 1, . . . n }, so that the play\nfunction πT : ΣT × (Q × A)∗→Q × A picks the question-answer pair indicated by\nthe index. The marker will compare the teacher’s answer a with the student’s answer\nσ(q) ∈B using the metric d : A × A →B :: (a0, a1) 7→(a0 = a1) and output boolean\nutilities in U = B. Plugging these open games as in (3.26), we can compute the set\nof equilibria by composing the equilibrium functions of its components.\nEG = {(j, σ) ∈ΣT × ΣS : j ∈argmax\ni∈ΣT\nai ̸= σ(qi) ∧σ ∈argmax\nσ∈ΣS\n(aj = σ(qj))}\nTherefore, in a Nash equilibrium, the teacher will ask the question that the student,\neven with his best guess, is going to answer in the worst way. The student, on the\nother hand, is going to answer as correctly as possible.\nWe analyse the possible outcomes of this game.\n1. There is a pair (qi, ai) in C that the student cannot answer correctly, i.e. ∀σ ∈\nΣS : σ(qi) ̸= ai. Then i is a winning strategy for the teacher and (i, σ) is a\nNash equilibrium, for any choice of strategy σ for the student. If no such pair\nexists, then we fall into one of the following cases.\n2. The corpus is consistent — i.e. ∃σ : G →Rel such that ∀i · σ(qi) = ai — and\nthe student has access to the model σ that answers all the possible questions\ncorrectly. Then, the strategy proﬁle (j, σ) is a Nash equilibrium and a winning\nstrategy for the student for any choice j of the teacher.\n3. For any choice i of the teacher, the student has a model σi that answers qi\ncorrectly. And viceversa, for any strategy σ of the student there is a choice j of\nthe teacher such that σ(qj) ̸= aj. Then the set EG is empty, there is no Nash\nequilibrium.\nTo illustrate the last case, consider a situation where the corpus C = { (q0, a0), (q1, a1) }\nhas only two elements and the student has only two models ΣS = { σ0, σ1 } such that\nσi(qi) = ai for i ∈{ 0, 1 } but σ0(q1) ̸= a1 and σ1(q0) ̸= a0. Then we’re in a matching\npennies scenario, both the teacher and the student have no incentive to choose any\none of their strategies and there is no Nash equilibrium. This problem can be ruled\nout if we allowed the players in the game to have mixed strategies, which can be\nachieved with minor modiﬁcations of the open game formalism [Gha+19].\n3.4.3\nWord sense disambiguation\nWord sense disambiguation (WSD) is the task of choosing the correct sense of a word\nin the context of a sentence. WSD has been argued to be an AI-complete task in the\nsense that it can be used to simulate other NLP task [Nav09]. In [TN19], Tripodi and\n197\nChapter 3\nGames for Pragmatics\nNavigli use methods from evolutionary game theory to obtain state-of-the-art results\nin the WSD task. The idea is to model WSD as a collaborative game between words\nwhere strategies are word senses. In their model, the interactions between words are\nweighted by how close the words are in a piece of text. In this section, we propose\na compositional alternative, where the interaction between words is mediated by the\ngrammatical structure of the sentence they occur in. Concretely, we show that how\nto build a functor J : GW →Game given a functorial model F : GV →Prob\nwhere GW is a grammar for words and GV a grammar for word-senses.\nGiven a\nsentence u ∈L(GV ), the functor J constructs a collaborative game where the Nash\nequilibrium is given by a choice of sense for each word in u that maximises the score\nof the sentence u in F.\nWe work with a dependency grammar G ⊆(B +V )×B∗where B is a set of basic\ntypes and V a set of words, see Deﬁnition 1.5.15. Recall from Proposition 1.5.21,\nthat dependency relations are acyclic, i.e. we can always turn the dependency graph\ninto a tree as in the following example:\nBob\ndiagram\ndraws\na\nBob\ndiagram\ndraws\na\ns\ns\n7→\nThis means that any sentence parsed with a dependency grammar induces a directed\nacyclic graph of dependencies. We may represent these parses in the free monoidal\ncategory MC( ˜G) where ˜G = B∗←V →B is the signature with boxes labelled by\nwords v ∈V with inputs given by the symbols that depend on v and a single output\ngiven by the symbol on which v depends, as shown in Proposition 1.5.21.\nTaking dependency grammars seriously, it is sensible to interpret them directly in\nthe category Prob of conditional distributions. Indeed, a functor F : ˜G →Prob is\ndeﬁned on objects by a choice of feature set F(x) for every symbol x ∈B and on words\nv : y1y2 . . . yn →x by a conditional distribution F(v) : F(y1) × F(y2) . . . F(yn) →\nD(F(x)) indicating the probability that word v has a particular feature given that\nthe words it depends on have particular features. Thus a parsed sentence is sent to\na Bayesian network where the independency constraints of the network are induced\nby the dependency structure of the sentence. One may prove formally that functors\n˜G →Prob induce Bayesian networks using the work of Fong [Fon13].\nWe are now ready to describe the WSD procedure. Fix a set W of words and V\nof word-senses with a dependency grammar GV for senses and a relation Σ ⊆W ×V ,\nassigning to each word w ∈W the set of its senses Σ(w) ⊆V .\nComposing GV\nwith Σ, we get a grammar for words GW. Assume we are given a functorial model\nF : GV →Prob with F(s) = 2, i.e. for any grammatical sentence g : u →s, its image\nF(g) ∈D(2) = [0, 1] quantiﬁes how likely it is that the sentence is true. We use F to\ndeﬁne a functor J : GW →Game(Prob) such that the Nash equilibrium of the image\n198\nExamples\nSection 3.4\nof any grammatical sentence is an assignement of each word to a sense maximising\nthe overall score of the sentence. On objects J is deﬁned by J(b) = (F(b), 2) for any\nb ∈B. Given a word w : y1 . . . yn →x in GW its image under J is given by an open\ngame J(w) : (F(y1) × F(y2) . . . F(yn), 2n) →(F(x), 2) with strategy set Σ(w) (i.e.\nstrategies for word w ∈W are given by its senses v ∈Σ(w) ⊆V ) deﬁned for every\nstrategy v ∈Σ(w) by the following lens:\n. . .\nF(y1)2\nF(yn)2\nF(x) 2\nJ(w)(v)\n:=\n. . .\nF(y1) 2\nF(yn) 2\nF(x)\n2\nF(v)\n. . .\nThe best response function in context [p, k] is given by:\nBJ(w,t)(k) = argmaxv∈Σ(w)(E(p; J(w)(v); k))\nThen given a grammatical sentence g ∈MC(GW) we get a closed game J(g) : 1 →1\nwith equilibrium given by:\nBJ(g) = argmaxvi∈Σ(wi)(F(g[wi := vi]))\nwhere u[wi := vi] denotes the sentence obtained by replacing each word wi ∈W with\nits sense vi ∈Σ(wi) ⊆V . Computing this argmax corresponds precisely to choosing\na sense for each word such that the probability that the sentence is true is maximised.\nExample 3.4.4. As an example take the following dependency grammar G with ˜G\ngiven by the following morphisms:\nBob : 1 →n , draws = n ⊗n →s , a : 1 →d , card : d →n , diagram : d →n (3.27)\nThe relation Σ ⊆W × V between words and senses is given by:\nΣ(Bob) = {Bob Coecke, Bob Ciaﬀone}\nΣ(draws) = {draws (pull), draws (picture)}\nand Σ(x) = { x } for x ∈{card, diagram, a}. The functor F : GV →Prob is deﬁned\non objects by F(d) = 1 (i.e. determinants are discarded), F(s) = 2 and F(n) =\n{Bob Coecke, Bob Ciaﬀone, card, diagram} and on arrows by:\nF(x)(y) =\n(\n1\nx = y\n0\notherwise .\nfor x ∈V \\ {draws (pull), draws (picture)}. The image of the two possible senses of\n“draw” are given by the following table:\nsubject\nobject\ndraws (picture)\ndraws (pull)\nBob Coecke\ncard\n0.1\n0.3\nBob Ciaﬀone\ncard\n0.1\n0.9\nBob Coecke\ndiagram\n0.9\n0.2\nBob Ciaﬀone\ndiagram\n0.1\n0.1\n199\nNote that a number in [0, 1] is suﬃcient to specify a distribution in D(2). We get a\ncorresponding functor J : GW →Game(Prob) which maps “Bob draws a diagram”\nas follows:\nV\n2\nV\n2\n2\n2\ndraws\nBob\ndiagram\nBob\ndiagram\na\ndraws\n7→\nn\nd\nn\ns\nComposing the channels according to the structure of the diagram we get a distribu-\ntion in D(2) parametrized over the choice of sense for each word. According to the\ntable above, the expectation of this distribution is maximised when the strategy of the\nword “Bob” is the sense “Bob Coecke” and the strategy of “draws” is the sense “draws\n(picture)”.\n200\nReferences\n[AD19]\nAbadiMartín and PlotkinGordon D. “A Simple Diﬀerentiable Programming\nLanguage”. In: Proceedings of the ACM on Programming Languages (Dec.\n2019). doi: 10.1145/3371106.\n[Abr12]\nSamson Abramsky. No-Cloning In Categorical Quantum Mechanics. Mar.\n2012. arXiv: 0910.2401 [quant-ph].\n[AC07]\nSamson Abramsky and Bob Coecke. “A Categorical Semantics of Quantum\nProtocols”. In: arXiv:quant-ph/0402130 (Mar. 2007). arXiv:\nquant-ph/0402130.\n[AC08]\nSamson Abramsky and Bob Coecke. “Categorical Quantum Mechanics”. In:\narXiv:0808.1023 [quant-ph] (Aug. 2008). arXiv: 0808.1023 [quant-ph].\n[AJ95]\nSamson Abramsky and Achim Jung. “Domain Theory”. In: Handbook of Logic\nin Computer Science (Vol. 3): Semantic Structures. USA: Oxford University\nPress, Inc., Jan. 1995, pp. 1–168.\n[AT10]\nSamson Abramsky and Nikos Tzevelekos. “Introduction to Categories and\nCategorical Logic”. In: arXiv:1102.1313 [cs, math] 813 (2010), pp. 3–94. doi:\n10.1007/978-3-642-12821-9_1. arXiv: 1102.1313 [cs, math].\n[AR18]\nTakanori Adachi and Yoshihiro Ryu. “A Category of Probability Spaces”. In:\narXiv:1611.03630 [math] (Oct. 2018). arXiv: 1611.03630 [math].\n[ALM07]\nDorit Aharonov, Zeph Landau, and Johann Makowsky. “The Quantum FFT\nCan Be Classically Simulated”. In: arXiv:quant-ph/0611156 (Mar. 2007).\narXiv: quant-ph/0611156.\n[Ajd35]\nK. Ajdukiewiz. “Die Syntaktische Konnexitat”. In: Studia Philosophica (1935),\npp. 1–27.\n[ACL19]\nAfra Alishahi, Grzegorz Chrupała, and Tal Linzen. “Analyzing and\nInterpreting Neural Networks for NLP: A Report on the First BlackboxNLP\nWorkshop”. In: arXiv:1904.04063 [cs, stat] (Apr. 2019). arXiv: 1904.04063\n[cs, stat].\n[AMY16]\nNoga Alon, Shay Moran, and Amir Yehudayoﬀ. “Sign Rank versus VC\nDimension”. In: arXiv:1503.07648 [math] (July 2016). arXiv: 1503.07648\n[math].\n[Ane12]\nIrving H. Anellis. “How Peircean Was the \"’Fregean’ Revolution\" in Logic?”\nIn: arXiv:1201.0353 [math] (Jan. 2012). arXiv: 1201.0353 [math].\n201\n[AL10]\nItai Arad and Zeph Landau. “Quantum Computation and the Evaluation of\nTensor Networks”. In: arXiv:0805.0040 [quant-ph] (Feb. 2010). arXiv:\n0805.0040 [quant-ph].\n[AFZ14]\nYoav Artzi, Nicholas Fitzgerald, and Luke Zettlemoyer. “Semantic Parsing\nwith Combinatory Categorial Grammars”. en-us. In: Proceedings of the 2014\nConference on Empirical Methods in Natural Language Processing: Tutorial\nAbstracts. Oct. 2014.\n[Ask19]\nJohn Ole Askedal. Peirce and Valency Grammar. en. De Gruyter Mouton,\nMay 2019. Chap. Signs of Humanity / L’homme et ses signes, pp. 1343–1348.\n[BCR18]\nJohn C. Baez, Brandon Coya, and Franciscus Rebro. “Props in Network\nTheory”. In: arXiv:1707.08321 [math-ph] (June 2018). arXiv: 1707.08321\n[math-ph].\n[BE14]\nJohn C. Baez and Jason Erbele. “Categories in Control”. In: arXiv:1405.6881\n[quant-ph] (May 2014). arXiv: 1405.6881 [quant-ph].\n[BP17]\nJohn C. Baez and Blake S. Pollard. “A Compositional Framework for\nReaction Networks”. In: Reviews in Mathematical Physics 29.09 (Oct. 2017),\np. 1750028. doi: 10.1142/S0129055X17500283. arXiv: 1704.02051.\n[BM02]\nJ. F. Baget and M. L. Mugnier. “Extensions of Simple Conceptual Graphs:\nThe Complexity of Rules and Constraints”. In: Journal of Artiﬁcial\nIntelligence Research 16 (June 2002), pp. 425–465. doi: 10.1613/jair.918.\narXiv: 1106.1800.\n[BCB14]\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. “Neural Machine\nTranslation by Jointly Learning to Align and Translate”. In: arXiv e-prints\n1409 (Sept. 2014), arXiv:1409.0473.\n[BKV18]\nKrzysztof Bar, Aleks Kissinger, and Jamie Vicary. “Globular: An Online\nProof Assistant for Higher-Dimensional Rewriting”. In: Logical Methods in\nComputer Science ; Volume 14 (2018), Issue 1, 18605974. doi:\n10.23638/LMCS-14(1:8)2018. arXiv: 1612.01093.\n[Bar53]\nYehoshua Bar-Hillel. “A Quasi-Arithmetical Notation for Syntactic\nDescription”. In: Language 29.1 (1953), pp. 47–58. doi: 10.2307/410452.\n[BGG12]\nChitta Baral, Marcos Alvarez Gonzalez, and Aaron Gottesman. “The Inverse\nLambda Calculus Algorithm for Typed First Order Logic Lambda Calculus\nand Its Application to Translating English to FOL”. en. In: Correct\nReasoning: Essays on Logic-Based AI in Honour of Vladimir Lifschitz. Ed. by\nEsra Erdem, Joohyung Lee, Yuliya Lierler, and David Pearce. Lecture Notes\nin Computer Science. Berlin, Heidelberg: Springer, 2012, pp. 40–56. doi:\n10.1007/978-3-642-30743-0_4.\n[BP83]\nJon Barwise and John Perry. Situations and Attitudes. MIT Press, 1983.\n[Bel57]\nRichard Bellman. “A Markovian Decision Process”. In: Journal of\nMathematics and Mechanics 6.5 (1957), pp. 679–684.\n[Ben+03]\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. “A\nNeural Probabilistic Language Model”. In: The Journal of Machine Learning\nResearch 3.null (Mar. 2003), pp. 1137–1155.\n202\n[BS18]\nAnton Benz and Jon Stevens. “Game-Theoretic Approaches to Pragmatics”.\nIn: Annual Review of Linguistics 4.1 (2018), pp. 173–191. doi:\n10.1146/annurev-linguistics-011817-045641.\n[Ber+13]\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. “Semantic\nParsing on Freebase from Question-Answer Pairs”. In: Proceedings of the 2013\nConference on Empirical Methods in Natural Language Processing. Seattle,\nWashington, USA: Association for Computational Linguistics, Oct. 2013,\npp. 1533–1544.\n[BLG16]\nLeon Bergen, Roger Levy, and Noah Goodman. “Pragmatic Reasoning\nthrough Semantic Inference”. en. In: Semantics and Pragmatics 9.0 (May\n2016), EARLY ACCESS. doi: 10.3765/sp.9.20.\n[BMT15]\nJacob D. Biamonte, Jason Morton, and Jacob W. Turner. “Tensor Network\nContractions for #SAT”. In: Journal of Statistical Physics 160.5 (Sept. 2015),\npp. 1389–1404. doi: 10.1007/s10955-015-1276-z. arXiv: 1405.7375.\n[Bia+00]\nAnna Maria Bianucci, Alessio Micheli, Alessandro Sperduti, and\nAntonina Starita. “Application of Cascade Correlation Networks for\nStructures to Chemistry”. en. In: Applied Intelligence 12.1 (Jan. 2000),\npp. 117–147. doi: 10.1023/A:1008368105614.\n[BPV06]\nAaron Bohannon, Benjamin C. Pierce, and Jeﬀrey A. Vaughan. “Relational\nLenses: A Language for Updatable Views”. In: Proceedings of the Twenty-Fifth\nACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database\nSystems. PODS ’06. New York, NY, USA: Association for Computing\nMachinery, June 2006, pp. 338–347. doi: 10.1145/1142351.1142399.\n[Bol+08]\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and\nJamie Taylor. “Freebase: A Collaboratively Created Graph Database for\nStructuring Human Knowledge”. In: Proceedings of the 2008 ACM SIGMOD\nInternational Conference on Management of Data. SIGMOD ’08. New York,\nNY, USA: Association for Computing Machinery, June 2008, pp. 1247–1250.\ndoi: 10.1145/1376616.1376746.\n[BHZ19]\nJoe Bolt, Jules Hedges, and Philipp Zahn. “Bayesian Open Games”. In:\narXiv:1910.03656 [cs, math] (Oct. 2019). arXiv: 1910.03656 [cs, math].\n[Bon+16]\nFilippo Bonchi, Fabio Gadducci, Aleks Kissinger, Pawel Sobocinski, and\nFabio Zanasi. “Rewriting modulo Symmetric Monoidal Structure”. In:\nProceedings of the 31st Annual ACM/IEEE Symposium on Logic in Computer\nScience (July 2016), pp. 710–719. doi: 10.1145/2933575.2935316. arXiv:\n1602.06771.\n[Bon+20]\nFilippo Bonchi, Fabio Gadducci, Aleks Kissinger, Pawel Sobocinski, and\nFabio Zanasi. “String Diagram Rewrite Theory I: Rewriting with Frobenius\nStructure”. In: arXiv:2012.01847 [cs, math] (Dec. 2020). arXiv: 2012.01847\n[cs, math].\n[Bon+21]\nFilippo Bonchi, Fabio Gadducci, Aleks Kissinger, Pawel Sobocinski, and\nFabio Zanasi. “String Diagram Rewrite Theory II: Rewriting with Symmetric\nMonoidal Structure”. In: arXiv:2104.14686 [cs, math] (Apr. 2021). arXiv:\n2104.14686 [cs, math].\n203\n[BPS17]\nFilippo Bonchi, Dusko Pavlovic, and Pawel Sobocinski. “Functorial Semantics\nfor Relational Theories”. In: arXiv:1711.08699 [cs, math] (Nov. 2017). arXiv:\n1711.08699 [cs, math].\n[BSS18]\nFilippo Bonchi, Jens Seeber, and Pawel Sobocinski. “Graphical Conjunctive\nQueries”. In: arXiv:1804.07626 [cs] (Apr. 2018). arXiv: 1804.07626 [cs].\n[BSZ14]\nFilippo Bonchi, Pawe\\l Sobociński, and Fabio Zanasi. “A Categorical\nSemantics of Signal Flow Graphs”. In: International Conference on\nConcurrency Theory. Springer, 2014, pp. 435–450.\n[BCW14]\nAntoine Bordes, Sumit Chopra, and Jason Weston. “Question Answering with\nSubgraph Embeddings”. In: Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP). Doha, Qatar: Association\nfor Computational Linguistics, Oct. 2014, pp. 615–620. doi:\n10.3115/v1/D14-1067.\n[Bor+13]\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Durán, Jason Weston, and\nOksana Yakhnenko. “Translating Embeddings for Modeling Multi-Relational\nData”. In: Proceedings of the 26th International Conference on Neural\nInformation Processing Systems - Volume 2. NIPS’13. Red Hook, NY, USA:\nCurran Associates Inc., Dec. 2013, pp. 2787–2795.\n[Bor+09]\nM. Bordewich, M. Freedman, L. Lovász, and D. Welsh. “Approximate\nCounting and Quantum Computation”. In: arXiv:0908.2122 [cs] (Aug. 2009).\narXiv: 0908.2122 [cs].\n[BPM15]\nSamuel R. Bowman, Christopher Potts, and Christopher D. Manning.\n“Recursive Neural Networks Can Learn Logical Semantics”. In: Proceedings of\nthe 3rd Workshop on Continuous Vector Space Models and Their\nCompositionality. Beijing, China: Association for Computational Linguistics,\nJuly 2015, pp. 12–21. doi: 10.18653/v1/W15-4002.\n[Bra+18]\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson,\nChris Leary, Dougal Maclaurin, George Necula, Adam Paszke,\nJake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable\ntransformations of Python+NumPy programs. Version 0.2.5. 2018. url:\nhttp://github.com/google/jax.\n[BT00]\nGeraldine Brady and Todd H. Trimble. “A Categorical Interpretation of C.S.\nPeirce’s Propositional Logic Alpha”. en. In: Journal of Pure and Applied\nAlgebra 149.3 (June 2000), pp. 213–239. doi:\n10.1016/S0022-4049(98)00179-0.\n[Bre+82]\nJoan Bresnan, Ronald M. Kaplan, Stanley Peters, and Annie Zaenen.\n“Cross-Serial Dependencies in Dutch”. In: Linguistic Inquiry 13.4 (1982),\npp. 613–635.\n[Bro+20]\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeﬀrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\n204\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. “Language\nModels Are Few-Shot Learners”. In: arXiv:2005.14165 [cs] (July 2020). arXiv:\n2005.14165 [cs].\n[Bul17]\nAndrei A. Bulatov. “A Dichotomy Theorem for Nonuniform CSPs”. In:\narXiv:1703.03021 [cs] (2017). doi: 10.1109/FOCS.2017.37. arXiv:\n1703.03021 [cs].\n[BM20]\nSamuele Buro and Isabella Mastroeni. “On the Semantic Equivalence of\nLanguage Syntax Formalisms”. en. In: Theoretical Computer Science 840\n(Nov. 2020), pp. 234–248. doi: 10.1016/j.tcs.2020.08.022.\n[Bus07]\nWojciech Buszkowski. “Type Logics and Pregroups”. en. In: Studia Logica 87.2\n(Dec. 2007), pp. 145–169. doi: 10.1007/s11225-007-9083-4.\n[Bus16]\nWojciech Buszkowski. “Syntactic Categories and Types: Ajdukiewicz and\nModern Categorial Grammars”. In: 2016. doi: 10.1163/9789004311763_004.\n[BM07]\nWojciech Buszkowski and Katarzyna Moroz. “Pregroup Grammars and\nContext-Free Grammars”. In: 2007.\n[CW18]\nLiwei Cai and William Yang Wang. “KBGAN: Adversarial Learning for\nKnowledge Graph Embeddings”. In: Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Papers). New Orleans,\nLouisiana: Association for Computational Linguistics, June 2018,\npp. 1470–1480. doi: 10.18653/v1/N18-1133.\n[Can06]\nR. Ferrer i Cancho. “Why Do Syntactic Links Not Cross?” en. In: EPL\n(Europhysics Letters) 76.6 (Nov. 2006), p. 1228. doi:\n10.1209/epl/i2006-10406-0.\n[CW87]\nA. Carboni and R. F. C. Walters. “Cartesian Bicategories I”. en. In: Journal of\nPure and Applied Algebra 49.1 (Nov. 1987), pp. 11–32. doi:\n10.1016/0022-4049(87)90121-6.\n[CL02]\nClaudia Casadio and Joachim Lambek. “A Tale of Four Grammars”. en. In:\nStudia Logica 71.3 (Aug. 2002), pp. 315–329. doi: 10.1023/A:1020564714107.\n[CB76]\nDonald D. Chamberlin and Raymond F. Boyce. “SEQUEL: A Structured\nEnglish Query Language”. en. In: Proceedings of the 1976 ACM SIGFIDET\n(Now SIGMOD) Workshop on Data Description, Access and Control - FIDET\n’76. Not Known: ACM Press, 1976, pp. 249–264. doi:\n10.1145/800296.811515.\n[Cha+22]\nJireh Yi-Le Chan, Khean Thye Bea, Steven Mun Hong Leow,\nSeuk Wai Phoong, and Wai Khuen Cheng. “State of the Art: A Review of\nSentiment Analysis Based on Sequential Transfer Learning”. In: Artiﬁcial\nIntelligence Review (Apr. 2022). doi: 10.1007/s10462-022-10183-8.\n[CM77]\nAshok K. Chandra and Philip M. Merlin. “Optimal Implementation of\nConjunctive Queries in Relational Data Bases”. en. In: Proceedings of the\nNinth Annual ACM Symposium on Theory of Computing - STOC ’77.\nBoulder, Colorado, United States: ACM Press, 1977, pp. 77–90. doi:\n10.1145/800105.803397.\n205\n[Cha+09]\nPhilippe Chaput, Vincent Danos, Prakash Panangaden, and Gordon Plotkin.\n“Approximating Markov Processes by Averaging”. en. In: Automata,\nLanguages and Programming. Ed. by Susanne Albers,\nAlberto Marchetti-Spaccamela, Yossi Matias, Sotiris Nikoletseas, and\nWolfgang Thomas. Lecture Notes in Computer Science. Berlin, Heidelberg:\nSpringer, 2009, pp. 127–138. doi: 10.1007/978-3-642-02930-1_11.\n[CK15]\nJianpeng Cheng and Dimitri Kartsaklis. “Syntax-Aware Multi-Sense Word\nEmbeddings for Deep Compositional Models of Meaning”. In:\narXiv:1508.02354 [cs] (Aug. 2015). arXiv: 1508.02354 [cs].\n[CJ19]\nKenta Cho and Bart Jacobs. “Disintegration and Bayesian Inversion via\nString Diagrams”. In: Mathematical Structures in Computer Science (Mar.\n2019), pp. 1–34. doi: 10.1017/S0960129518000488. arXiv: 1709.00322.\n[Cho+15]\nFrancois Chollet et al. Keras. 2015. url:\nhttps://github.com/fchollet/keras.\n[Cho56]\nNoam Chomsky. “Three Models for the Description of Language”. In: Journal\nof Symbolic Logic 23.1 (1956), pp. 71–72. doi: 10.2307/2964524.\n[Cho57]\nNoam Chomsky. Syntactic Structures. The Hague: Mouton and Co., 1957.\n[Chu32]\nAlonzo Church. “A Set of Postulates for the Foundation of Logic”. In: Annals\nof Mathematics 33.2 (1932), pp. 346–366. doi: 10.2307/1968337.\n[Chu36]\nAlonzo Church. “An Unsolvable Problem of Elementary Number Theory”. In:\nJournal of Symbolic Logic 1.2 (1936), pp. 73–74. doi: 10.2307/2268571.\n[Chu40]\nAlonzo Church. “A Formulation of the Simple Theory of Types”. In: The\nJournal of Symbolic Logic 5.2 (1940), pp. 56–68. doi: 10.2307/2266170.\n[CM15]\nKevin Clark and Christopher D. Manning. “Entity-Centric Coreference\nResolution with Model Stacking”. In: Proceedings of the 53rd Annual Meeting\nof the Association for Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers).\nBeijing, China: Association for Computational Linguistics, July 2015,\npp. 1405–1415. doi: 10.3115/v1/P15-1136.\n[CM16]\nKevin Clark and Christopher D. Manning. “Deep Reinforcement Learning for\nMention-Ranking Coreference Models”. In: Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Processing. Austin, Texas:\nAssociation for Computational Linguistics, Nov. 2016, pp. 2256–2262. doi:\n10.18653/v1/D16-1245.\n[CCS08]\nStephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh. “A Compositional\nDistributional Model of Meaning”. In: Proceedings of the Second Symposium\non Quantum Interaction (QI-2008) (2008), pp. 133–140.\n[CCS10]\nStephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh. “Mathematical\nFoundations for a Compositional Distributional Model of Meaning”. In: A\nFestschrift for Jim Lambek. Ed. by J. van Benthem, M. Moortgat, and\nW. Buszkowski. Vol. 36. Linguistic Analysis. 2010, pp. 345–384. arXiv:\n1003.4394.\n206\n[Cod70]\nE F Codd. “A Relational Model of Data for Large Shared Data Banks”. en. In:\n13.6 (1970), p. 11.\n[Coe20]\nBob Coecke. “The Mathematics of Text Structure”. In: arXiv:1904.03478\n[quant-ph] (Feb. 2020). arXiv: 1904.03478 [quant-ph].\n[Coe+18]\nBob Coecke, Giovanni de Felice, Dan Marsden, and Alexis Toumi. “Towards\nCompositional Distributional Discourse Analysis”. In: Electronic Proceedings\nin Theoretical Computer Science 283 (Nov. 2018), pp. 1–12. doi:\n10.4204/EPTCS.283.1. arXiv: 1811.03277 [cs].\n[Coe+20]\nBob Coecke, Giovanni de Felice, Konstantinos Meichanetzidis, and\nAlexis Toumi. “Foundations for Near-Term Quantum Natural Language\nProcessing”. In: arXiv:2012.03755 [quant-ph] (Dec. 2020). arXiv: 2012.03755\n[quant-ph].\n[Coe+22]\nBob Coecke, Giovanni de Felice, Konstantinos Meichanetzidis, and\nAlexis Toumi. “How to Make Qubits Speak”. In: Quantum Computing in the\nArts and Humanities: An Introduction to Core Concepts, Theory and\nApplications. Ed. by Eduardo Reck Miranda. Cham: Springer International\nPublishing, 2022, pp. 277–297. doi: 10.1007/978-3-030-95538-0_8.\n[CK17]\nBob Coecke and Aleks Kissinger. Picturing Quantum Processes: A First\nCourse in Quantum Theory and Diagrammatic Reasoning. Cambridge:\nCambridge University Press, 2017. doi: 10.1017/9781316219317.\n[Coe+19]\nAndy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce,\nFernanda Viégas, and Martin Wattenberg. “Visualizing and Measuring the\nGeometry of BERT”. In: arXiv:1906.02715 [cs, stat] (Oct. 2019). arXiv:\n1906.02715 [cs, stat].\n[Cru+21]\nG. S. H. Cruttwell, Bruno Gavranović, Neil Ghani, Paul Wilson, and\nFabio Zanasi. “Categorical Foundations of Gradient-Based Learning”. In:\narXiv:2103.01931 [cs, math] (Mar. 2021). arXiv: 2103.01931 [cs, math].\n[Cur61]\nH. B. Curry. “Some Logical Aspects of Grammatical Structure”. In: Structure\nLanguage and Its Mathematical Aspects, vol. XII. American Mathematical\nSociety, 1961, pp. 56–68.\n[DKV02]\nVíctor Dalmau, Phokion G. Kolaitis, and Moshe Y. Vardi. “Constraint\nSatisfaction, Bounded Treewidth, and Finite-Variable Logics”. en. In:\nPrinciples and Practice of Constraint Programming - CP 2002. Ed. by\nGerhard Goos, Juris Hartmanis, Jan van Leeuwen, and\nPascal Van Hentenryck. Vol. 2470. Berlin, Heidelberg: Springer Berlin\nHeidelberg, 2002, pp. 310–326. doi: 10.1007/3-540-46135-3_21.\n[Dav67a]\nDonald Davidson. “The Logical Form of Action Sentences”. In: The Logic of\nDecision and Action. Ed. by Nicholas Rescher. University of Pittsburgh Press,\n1967, pp. 81–95.\n[Dav67b]\nDonald Davidson. “Truth and Meaning”. In: Synthese 17.3 (1967),\npp. 304–323.\n[dKM20]\nNiel de Beaudrap, Aleks Kissinger, and Konstantinos Meichanetzidis. “Tensor\nNetwork Rewriting Strategies for Satisﬁability and Counting”. In:\narXiv:2004.06455 [quant-ph] (Apr. 2020). arXiv: 2004.06455 [quant-ph].\n207\n[de +21]\nGiovanni de Felice, Elena Di Lavore, Mario Román, and Alexis Toumi.\n“Functorial Language Games for Question Answering”. In: Electronic\nProceedings in Theoretical Computer Science 333 (Feb. 2021), pp. 311–321.\ndoi: 10.4204/EPTCS.333.21. arXiv: 2005.09439 [cs].\n[dMT20]\nGiovanni de Felice, Konstantinos Meichanetzidis, and Alexis Toumi.\n“Functorial Question Answering”. In: Electronic Proceedings in Theoretical\nComputer Science 323 (Sept. 2020), pp. 84–94. doi: 10.4204/EPTCS.323.6.\narXiv: 1905.07408 [cs, math].\n[dTC21]\nGiovanni de Felice, Alexis Toumi, and Bob Coecke. “DisCoPy: Monoidal\nCategories in Python”. In: Electronic Proceedings in Theoretical Computer\nScience 333 (Feb. 2021), pp. 183–197. doi: 10.4204/EPTCS.333.13. arXiv:\n2005.02975 [math].\n[De 47]\nAugustus De Morgan. Formal Logic: Or, The Calculus of Inference, Necessary\nand Probable. en. Taylor and Walton, 1847.\n[DBM15]\nWim De Mulder, Steven Bethard, and Marie-Francine Moens. “A Survey on\nthe Application of Recurrent Neural Networks to Statistical Language\nModeling”. en. In: Computer Speech & Language 30.1 (Mar. 2015), pp. 61–98.\ndoi: 10.1016/j.csl.2014.09.005.\n[DP05]\nSylvain Degeilh and Anne Preller. “Eﬃciency of Pregroups and the French\nNoun Phrase”. en. In: Journal of Logic, Language and Information 14.4 (Oct.\n2005), pp. 423–444. doi: 10.1007/s10849-005-1242-2.\n[Del19]\nAntonin Delpeuch. “Autonomization of Monoidal Categories”. In:\narXiv:1411.3827 [cs, math] (June 2019). arXiv: 1411.3827 [cs, math].\n[DV19a]\nAntonin Delpeuch and Jamie Vicary. “Normalization for Planar String\nDiagrams and a Quadratic Equivalence Algorithm”. In: arXiv:1804.07832 [cs]\n(Sept. 2019). arXiv: 1804.07832 [cs].\n[DEB19]\nSamuel Desrosiers, Glen Evenbly, and Thomas Baker. “Survey of Tensor\nNetworks”. In: (2019), F68.006.\n[Dev+19]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT:\nPre-Training of Deep Bidirectional Transformers for Language\nUnderstanding”. In: arXiv:1810.04805 [cs] (May 2019). arXiv: 1810.04805\n[cs].\n[DdR22]\nElena Di Lavore, Giovanni de Felice, and Mario Román. Monoidal Streams for\nDataﬂow Programming. Feb. 2022. doi: 10.48550/arXiv.2202.02061. arXiv:\n2202.02061 [cs, math].\n[Don+14]\nXin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao,\nKevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang.\n“Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge\nFusion”. In: Proceedings of the 20th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining. KDD ’14. New York, NY, USA:\nAssociation for Computing Machinery, Aug. 2014, pp. 601–610. doi:\n10.1145/2623330.2623623.\n208\n[Dos+20]\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,\nXiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. “An Image\nIs Worth 16x16 Words: Transformers for Image Recognition at Scale”. In:\narXiv:2010.11929 [cs] (Oct. 2020). arXiv: 2010.11929 [cs].\n[DV19b]\nLawrence Dunn and Jamie Vicary. “Coherence for Frobenius Pseudomonoids\nand the Geometry of Linear Proofs”. In: arXiv:1601.05372 [cs] (2019). doi:\n10.23638/LMCS-15(3:5)2019. arXiv: 1601.05372 [cs].\n[Ear70]\nJay Earley. “An Eﬃcient Context-Free Parsing Algorithm”. In:\nCommunications of the ACM 13.2 (Feb. 1970), pp. 94–102. doi:\n10.1145/362007.362035.\n[Edu+18]\nSergey Edunov, Myle Ott, Michael Auli, and David Grangier. “Understanding\nBack-Translation at Scale”. In: Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing. Brussels, Belgium:\nAssociation for Computational Linguistics, Oct. 2018, pp. 489–500. doi:\n10.18653/v1/D18-1045.\n[EHL19]\nStavros Efthymiou, Jack Hidary, and Stefan Leichenauer. “TensorNetwork for\nMachine Learning”. In: arXiv:1906.06329 [cond-mat, physics:physics, stat]\n(June 2019). arXiv: 1906.06329 [cond-mat, physics:physics, stat].\n[Ein16]\nAlbert Einstein. “The Foundation of the General Theory of Relativity”. en. In:\nAnnalen Phys. 49.7 (1916), pp. 769–822. doi: 10.1002/andp.200590044.\n[Eis13]\nJ. Eisert. “Entanglement and Tensor Network States”. In: arXiv:1308.3318\n[cond-mat, physics:quant-ph] (Sept. 2013). arXiv: 1308.3318 [cond-mat,\nphysics:quant-ph].\n[Ela06]\nPradheep Elango. Coreference Resolution: A Survey. 2006.\n[Elm90]\nJeﬀrey L. Elman. “Finding Structure in Time”. en. In: Cognitive Science 14.2\n(1990), pp. 179–211. doi: 10.1207/s15516709cog1402_1.\n[Fat+20]\nBahare Fatemi, Perouz Taslakian, David Vazquez, and David Poole.\n“Knowledge Hypergraphs: Prediction Beyond Binary Relations”. In:\narXiv:1906.00137 [cs, stat] (July 2020). arXiv: 1906.00137 [cs, stat].\n[Fon13]\nBrendan Fong. “Causal Theories: A Categorical Perspective on Bayesian\nNetworks”. In: arXiv:1301.6201 [math] (Jan. 2013). arXiv: 1301.6201 [math].\n[FJ19]\nBrendan Fong and Michael Johnson. “Lenses and Learners”. In:\narXiv:1903.03671 [cs, math] (May 2019). arXiv: 1903.03671 [cs, math].\n[FS18a]\nBrendan Fong and David I. Spivak. “Graphical Regular Logic”. In:\narXiv:1812.05765 [cs, math] (Dec. 2018). arXiv: 1812.05765 [cs, math].\n[FS18b]\nBrendan Fong and David I. Spivak. “Hypergraph Categories”. In:\narXiv:1806.08304 [cs, math] (June 2018). arXiv: 1806.08304 [cs, math].\n[FST19]\nBrendan Fong, David I. Spivak, and Rémy Tuyéras. “Backprop as Functor: A\nCompositional Perspective on Supervised Learning”. In: arXiv:1711.10455 [cs,\nmath] (May 2019). arXiv: 1711.10455 [cs, math].\n209\n[Fow08]\nTimothy A. D. Fowler. “Eﬃciently Parsing with the Product-Free Lambek\nCalculus”. In: Proceedings of the 22nd International Conference on\nComputational Linguistics (Coling 2008). Manchester, UK: Coling 2008\nOrganizing Committee, Aug. 2008, pp. 217–224.\n[Fox76]\nThomas Fox. “Coalgebras and Cartesian Categories”. In: Communications in\nAlgebra 4.7 (Jan. 1976), pp. 665–667. doi: 10.1080/00927877608822127.\n[FG12]\nMichael C. Frank and Noah D. Goodman. “Predicting Pragmatic Reasoning\nin Language Games”. en. In: Science 336.6084 (May 2012), pp. 998–998. doi:\n10.1126/science.1218633.\n[Fra09]\nMichael Franke. “Signal to Act: Game Theory in Pragmatics”. In: (Jan. 2009).\n[FJ16]\nMichael Franke and Gerhard Jäger. “Probabilistic Pragmatics, or Why Bayes’\nRule Is Probably Important for Pragmatics”. en. In: Zeitschrift für\nSprachwissenschaft 35.1 (June 2016), pp. 3–44. doi: 10.1515/zfs-2016-0002.\n[FGS98]\nP. Frasconi, M. Gori, and A. Sperduti. “A General Framework for Adaptive\nProcessing of Data Structures”. In: IEEE Transactions on Neural Networks\n9.5 (Sept. 1998), pp. 768–786. doi: 10.1109/72.712151.\n[FLW00]\nMichael Freedman, Michael Larsen, and Zhenghan Wang. “A Modular Functor\nWhich Is Universal for Quantum Computation”. In: arXiv:quant-ph/0001108\n(Feb. 2000). arXiv: quant-ph/0001108.\n[FKW02]\nMichael H. Freedman, Alexei Kitaev, and Zhenghan Wang. “Simulation of\nTopological Field Theories by Quantum Computers”. In: Communications in\nMathematical Physics 227.3 (June 2002), pp. 587–603. doi:\n10.1007/s002200200635. arXiv: quant-ph/0001071.\n[Fre14]\nGottlob Frege. “Letter to Jourdain”. In: The Frege Reader. Oxford: Blackwell\nPublishing, 1914, pp. 319–321.\n[Fri20]\nTobias Fritz. “A Synthetic Approach to Markov Kernels, Conditional\nIndependence and Theorems on Suﬃcient Statistics”. In: arXiv:1908.07021\n[cs, math, stat] (Mar. 2020). arXiv: 1908.07021 [cs, math, stat].\n[Gai65]\nHaim Gaifman. “Dependency Systems and Phrase-Structure Systems”. en. In:\nInformation and Control 8.3 (June 1965), pp. 304–337. doi:\n10.1016/S0019-9958(65)90232-9.\n[GJ90]\nMichael R. Garey and David S. Johnson. Computers and Intractability; A\nGuide to the Theory of NP-Completeness. New York, NY, USA: W. H.\nFreeman & Co., 1990.\n[Gat+13]\nA. Gatt, R. van Gompel, K. van Deemter, and E. J. Krahmer. “Are We\nBayesian Referring Expression Generators?” English. In: Proceedings of the\nCogSci workshop on the production of referring expressions: bridging the gap\nbetween cognitive and computational approaches to reference (PRE-CogSci\n2013) (2013), pp. 1–6.\n[GHC98]\nNiyu Ge, John Hale, and Eugene Charniak. “A Statistical Approach to\nAnaphora Resolution”. In: Sixth Workshop on Very Large Corpora. 1998.\n210\n[GSC00]\nFelix A. Gers, Jürgen Schmidhuber, and Fred Cummins. “Learning to Forget:\nContinual Prediction with LSTM”. In: Neural Computation 12.10 (Oct. 2000),\npp. 2451–2471. doi: 10.1162/089976600300015015.\n[Gha+18]\nNeil Ghani, Jules Hedges, Viktor Winschel, and Philipp Zahn. “Compositional\ngame theory”. In: Proceedings of the 33rd Annual ACM/IEEE Symposium on\nLogic in Computer Science. 2018, pp. 472–481. doi: 10.3982/ECTA6297.\n[Gha+19]\nNeil Ghani, Clemens Kupke, Alasdair Lambert, and\nFredrik Nordvall Forsberg. “Compositional Game Theory with Mixed\nStrategies: Probabilistic Open Games Using a Distributive Law”. English. In:\nApplied Category Theory Conference 2019. July 2019.\n[Gla+19]\nIvan Glasser, Ryan Sweke, Nicola Pancotti, Jens Eisert, and J. Ignacio Cirac.\n“Expressive Power of Tensor-Network Factorizations for Probabilistic\nModeling, with Applications from Hidden Markov Models to Quantum\nMachine Learning”. In: arXiv:1907.03741 [cond-mat, physics:quant-ph, stat]\n(Nov. 2019). arXiv: 1907.03741 [cond-mat, physics:quant-ph, stat].\n[Gog+77]\nJ. A. Goguen, J. W. Thatcher, E. G. Wagner, and J. B. Wright. “Initial\nAlgebra Semantics and Continuous Algebras”. In: Journal of the ACM 24.1\n(Jan. 1977), pp. 68–95. doi: 10.1145/321992.321997.\n[GK96]\nC. Goller and A. Kuchler. “Learning Task-Dependent Distributed\nRepresentations by Backpropagation through Structure”. In: Proceedings of\nInternational Conference on Neural Networks (ICNN’96). Vol. 1. June 1996,\n347–352 vol.1. doi: 10.1109/ICNN.1996.548916.\n[Goo+14]\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,\nDavid Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.\n“Generative Adversarial Nets”. In: Advances in Neural Information Processing\nSystems 27. Ed. by Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence,\nand K. Q. Weinberger. Curran Associates, Inc., 2014, pp. 2672–2680.\n[GS13]\nNoah Goodman and Andreas Stuhlmüller. “Knowledge and Implicature:\nModeling Language Understanding as Social Cognition”. In: Topics in\ncognitive science 5 (Jan. 2013), pp. 173–184. doi: 10.1111/tops.12007.\n[GS14]\nAlexander Gouberman and Markus Siegle. “Markov Reward Models and\nMarkov Decision Processes in Discrete and Continuous Time: Performance\nEvaluation and Optimization”. en. In: Stochastic Model Checking. Rigorous\nDependability Analysis Using Model Checking Techniques for Stochastic\nSystems: International Autumn School, ROCKS 2012, Vahrn, Italy, October\n22-26, 2012, Advanced Lectures. Ed. by Anne Remke and Mariëlle Stoelinga.\nLecture Notes in Computer Science. Berlin, Heidelberg: Springer, 2014,\npp. 156–241. doi: 10.1007/978-3-662-45489-3_6.\n[GK20]\nJohnnie Gray and Stefanos Kourtis. “Hyper-Optimized Tensor Network\nContraction”. In: arXiv:2002.01935 [cond-mat, physics:physics,\nphysics:quant-ph] (Feb. 2020). arXiv: 2002.01935 [cond-mat,\nphysics:physics, physics:quant-ph].\n211\n[GS11]\nEdward Grefenstette and Mehrnoosh Sadrzadeh. “Experimental Support for a\nCategorical Compositional Distributional Model of Meaning”. In: The 2014\nConference on Empirical Methods on Natural Language Processing. 2011,\npp. 1394–1404. arXiv: 1106.4058.\n[Gre65]\nSheila A. Greibach. “A New Normal-Form Theorem for Context-Free Phrase\nStructure Grammars”. In: Journal of the ACM 12.1 (Jan. 1965), pp. 42–52.\ndoi: 10.1145/321250.321254.\n[Gri67]\nHerbert Paul Grice. “Logic and Conversation”. In: Studies in the Way of\nWords. Ed. by Paul Grice. Harvard University Press, 1967, pp. 41–58.\n[HOD12]\nSherzod Hakimov, Salih Atilay Oto, and Erdogan Dogdu. “Named Entity\nRecognition and Disambiguation Using Linked Data and Graph-Based\nCentrality Scoring”. In: Proceedings of the 4th International Workshop on\nSemantic Web Information Management. SWIM ’12. New York, NY, USA:\nAssociation for Computing Machinery, May 2012, pp. 1–7. doi:\n10.1145/2237867.2237871.\n[HL79]\nPer-Kristian Halvorsen and William A. Ladusaw. “Montague’s ’Universal\nGrammar’: An Introduction for the Linguist”. In: Linguistics and Philosophy\n3.2 (1979), pp. 185–223.\n[Har+20]\nCharles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers,\nPauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor,\nSebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus,\nStephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane,\nJaime Fernández del Río, Mark Wiebe, Pearu Peterson,\nPierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser,\nHameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. “Array\nprogramming with NumPy”. In: Nature 585.7825 (Sept. 2020), pp. 357–362.\ndoi: 10.1038/s41586-020-2649-2.\n[HR76]\nW.s. Hatcher and T. Rus. “Context-Free Algebras”. In: Journal of Cybernetics\n6.1-2 (Jan. 1976), pp. 65–77. doi: 10.1080/01969727608927525.\n[Hau35]\nFelix Hausdorﬀ. Gesammelte Werke Band III: Deskripte Mengenlehre und\nTopologie. de. Ed. by Ulrich Felgner, Horst Herrlich, Mirek Husek,\nVladimir Kanovei, Peter Koepke, Gerhard Preuß, Walter Purkert, and\nErhard Scholz. Felix Hausdorﬀ- Gesammelte Werke. Berlin Heidelberg:\nSpringer-Verlag, 1935. doi: 10.1007/978-3-540-76807-4.\n[HS20]\nNathan Haydon and Pawel Sobocinski. “Compositional Diagrammatic\nFirst-Order Logic”. In: 11th International Conference on the Theory and\nApplication of Diagrams (DIAGRAMS 2020). 2020. url:\nhttps://www.ioc.ee/~pawel/papers/peirce.pdf.\n[Hay64]\nDavid G. Hays. “Dependency Theory: A Formalism and Some Observations”.\nIn: Language 40.4 (1964), pp. 511–525. doi: 10.2307/411934.\n[Hed17]\nJules Hedges. “Coherence for Lenses and Open Games”. In: arXiv:1704.02230\n[cs, math] (Sept. 2017). arXiv: 1704.02230 [cs, math].\n212\n[HL18]\nJules Hedges and Martha Lewis. “Towards Functorial Language-Games”. In:\nElectronic Proceedings in Theoretical Computer Science 283 (Nov. 2018),\npp. 89–102. doi: 10.4204/eptcs.283.7.\n[HV19]\nChris Heunen and Jamie Vicary. Categories for Quantum Theory: An\nIntroduction. Oxford University Press, 2019.\n[Hob78]\nJerry R. Hobbs. “Resolving Pronoun References”. en. In: Lingua 44.4 (Apr.\n1978), pp. 311–338. doi: 10.1016/0024-3841(78)90006-2.\n[HS97]\nSepp Hochreiter and Jürgen Schmidhuber. “Long Short-Term Memory”. In:\nNeural computation 9 (Dec. 1997), pp. 1735–80. doi:\n10.1162/neco.1997.9.8.1735.\n[Hon+20]\nMatthew Honnibal, Ines Montani, Soﬁe Van Landeghem, and Adriane Boyd.\nspaCy: Industrial-strength Natural Language Processing in Python. 2020. doi:\n10.5281/zenodo.1212303.\n[HJ12]\nRoger A Horn and Charles R Johnson. Matrix Analysis. Cambridge University\nPress, 2012.\n[How60]\nRoland Howard. Dynamic Programming and Markov Processes. MIT press,\nCambridge, 1960.\n[Imm87]\nNeil Immerman. “Languages That Capture Complexity Classes”. In: SIAM\nJournal of Computing 16 (1987), pp. 760–778.\n[JKZ18]\nBart Jacobs, Aleks Kissinger, and Fabio Zanasi. “Causal Inference by String\nDiagram Surgery”. In: arXiv:1811.08338 [cs, math] (Nov. 2018). arXiv:\n1811.08338 [cs, math].\n[JZ21]\nTheo M. V. Janssen and Thomas Ede Zimmermann. “Montague Semantics”.\nIn: The Stanford Encyclopedia of Philosophy. Ed. by Edward N. Zalta.\nSummer 2021. Metaphysics Research Lab, Stanford University, 2021.\n[JSV04]\nMark Jerrum, Alistair Sinclair, and Eric Vigoda. “A Polynomial-Time\nApproximation Algorithm for the Permanent of a Matrix with Nonnegative\nEntries”. In: Journal of the ACM 51.4 (July 2004), pp. 671–697. doi:\n10.1145/1008731.1008738.\n[JRW12]\nMichael Johnson, Robert Rosebrugh, and R. J. Wood. “Lenses, Fibrations and\nUniversal Translations†”. en. In: Mathematical Structures in Computer Science\n22.1 (Feb. 2012), pp. 25–42. doi: 10.1017/S0960129511000442.\n[Jos85]\nAravind K. Joshi. “Tree Adjoining Grammars: How Much Context-Sensitivity\nIs Required to Provide Reasonable Structural Descriptions?” In: Natural\nLanguage Parsing: Psychological, Computational, and Theoretical Perspectives.\nEd. by Arnold M. Zwicky, David R. Dowty, and Lauri Karttunen. Studies in\nNatural Language Processing. Cambridge: Cambridge University Press, 1985,\npp. 206–250. doi: 10.1017/CBO9780511597855.007.\n[JS91]\nA. Joyal and R. Street. “The Geometry of Tensor Calculus, I”. In: Advances in\nMathematics 88.1 (1991), pp. 55–112. doi: 10.1016/0001-8708(91)90003-p.\n213\n[Jum+21]\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green,\nMichael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates,\nAugustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer,\nSimon A. A. Kohl, Andrew J. Ballard, Andrew Cowie,\nBernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler,\nTrevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski,\nMartin Steinegger, Michalina Pacholska, Tamas Berghammer,\nSebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior,\nKoray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. “Highly Accurate\nProtein Structure Prediction with AlphaFold”. In: Nature 596.7873 (Aug.\n2021), pp. 583–589. doi: 10.1038/s41586-021-03819-2.\n[JM08]\nDaniel Jurafsky and James Martin. Speech and Language Processing: An\nIntroduction to Natural Language Processing, Computational Linguistics, and\nSpeech Recognition. Vol. 2. United States: Prentice Hall PTR, Feb. 2008.\n[KGB14]\nNal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. “A Convolutional\nNeural Network for Modelling Sentences”. In: Proceedings of the 52nd Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). Baltimore, Maryland: Association for Computational Linguistics,\nJune 2014, pp. 655–665. doi: 10.3115/v1/P14-1062.\n[Kar+21]\nDimitri Kartsaklis, Ian Fan, Richie Yeung, Anna Pearson, Robin Lorenz,\nAlexis Toumi, Giovanni de Felice, Konstantinos Meichanetzidis,\nStephen Clark, and Bob Coecke. “Lambeq: An Eﬃcient High-Level Python\nLibrary for Quantum NLP”. In: arXiv:2110.04236 [quant-ph] (Oct. 2021).\narXiv: 2110.04236 [quant-ph].\n[Kea+16]\nSteven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and\nPatrick Riley. “Molecular Graph Convolutions: Moving beyond Fingerprints”.\nIn: Journal of Computer-Aided Molecular Design 30.8 (Aug. 2016),\npp. 595–608. doi: 10.1007/s10822-016-9938-8.\n[Kv19]\nAleks Kissinger and John van de Wetering. “PyZX: Large Scale Automated\nDiagrammatic Reasoning”. In: arXiv:1904.04735 [quant-ph] (Apr. 2019).\narXiv: 1904.04735 [quant-ph].\n[Kv20]\nAleks Kissinger and John van de Wetering. “Reducing T-Count with the\nZX-Calculus”. In: Physical Review A 102.2 (Aug. 2020), p. 022406. doi:\n10.1103/PhysRevA.102.022406. arXiv: 1903.10477.\n[KH25]\nH. A. Kramers and W. Heisenberg. “Über die Streuung von Strahlung durch\nAtome”. de. In: Zeitschrift für Physik 31.1 (Feb. 1925), pp. 681–708. doi:\n10.1007/BF02980624.\n[KM14]\nJayant Krishnamurthy and Tom M. Mitchell. “Joint Syntactic and Semantic\nParsing with Combinatory Categorial Grammar”. In: Proceedings of the 52nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers). Baltimore, Maryland: Association for Computational\nLinguistics, June 2014, pp. 1188–1198. doi: 10.3115/v1/P14-1112.\n[KSJ18]\nMarco Kuhlmann, Giorgio Satta, and Peter Jonsson. “On the Complexity of\nCCG Parsing”. In: Computational Linguistics 44.3 (Sept. 2018), pp. 447–482.\ndoi: 10.1162/coli_a_00324.\n214\n[Kus06]\nBoris A. Kushner. “The Constructive Mathematics of A. A. Markov”. In: The\nAmerican Mathematical Monthly 113.6 (2006), pp. 559–566. doi:\n10.2307/27641983.\n[Lam86]\nJ. Lambek. “Cartesian Closed Categories and Typed λ-Calculi”. en. In:\nCombinators and Functional Programming Languages. Ed. by Guy Cousineau,\nPierre-Louis Curien, and Bernard Robinet. Lecture Notes in Computer\nScience. Berlin, Heidelberg: Springer, 1986, pp. 136–175. doi:\n10.1007/3-540-17184-3_44.\n[Lam88]\nJ. Lambek. “Categorial and Categorical Grammars”. en. In: Categorial\nGrammars and Natural Language Structures. Ed. by Richard T. Oehrle,\nEmmon Bach, and Deirdre Wheeler. Studies in Linguistics and Philosophy.\nDordrecht: Springer Netherlands, 1988, pp. 297–317. doi:\n10.1007/978-94-015-6878-4_11.\n[Lam99a]\nJ. Lambek. “Type Grammar Revisited”. en. In: Logical Aspects of\nComputational Linguistics. Ed. by Alain Lecomte, François Lamarche, and\nGuy Perrier. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer,\n1999, pp. 1–27. doi: 10.1007/3-540-48975-4_1.\n[LS86]\nJ. Lambek and P. J. Scott. Introduction to Higher Order Categorical Logic.\nVol. 7. USA: Cambridge University Press, 1986.\n[Lam08]\nJim Lambek. From Word to Sentence: A Computational Algebraic Approach to\nGrammar. Open Access Publications. Polimetrica, 2008.\n[Lam58]\nJoachim Lambek. “The Mathematics of Sentence Structure”. In: The\nAmerican Mathematical Monthly 65.3 (Mar. 1958), pp. 154–170. doi:\n10.1080/00029890.1958.11989160.\n[Lam68]\nJoachim Lambek. “Deductive Systems and Categories”. en. In: Mathematical\nsystems theory 2.4 (Dec. 1968), pp. 287–318. doi: 10.1007/BF01703261.\n[Lam99b]\nJoachim Lambek. “Deductive Systems and Categories in Linguistics”. en. In:\nLogic, Language and Reasoning: Essays in Honour of Dov Gabbay. Ed. by\nHans Jürgen Ohlbach and Uwe Reyle. Trends in Logic. Dordrecht: Springer\nNetherlands, 1999, pp. 279–294. doi: 10.1007/978-94-011-4574-9_12.\n[LF04]\nShalom Lappin and C. Fox. “An Expressive First-Order Logic for Natural\nLanguage Semantics”. en. In: Institute of Philosophy (2004).\n[LHH20]\nMd Tahmid Rahman Laskar, Jimmy Xiangji Huang, and Enamul Hoque.\n“Contextualized Embeddings Based Transformer Encoder for Sentence\nSimilarity Modeling in Answer Selection Task”. In: Proceedings of the 12th\nLanguage Resources and Evaluation Conference. Marseille, France: European\nLanguage Resources Association, May 2020, pp. 5505–5514.\n[Law63]\nF: W. Lawvere. “Functorial Semantics of Algebraic Theories”. PhD thesis.\nColumbia University, 1963.\n[Lee+17]\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. “End-to-End\nNeural Coreference Resolution”. In: Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing. Copenhagen, Denmark:\nAssociation for Computational Linguistics, Sept. 2017, pp. 188–197. doi:\n10.18653/v1/D17-1018.\n215\n[Leh+14]\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nDimitris Kontokostas, Pablo Mendes, Sebastian Hellmann, Mohamed Morsey,\nPatrick Van Kleef, Sören Auer, and Christian Bizer. “DBpedia - A\nLarge-Scale, Multilingual Knowledge Base Extracted from Wikipedia”. In:\nSemantic Web Journal 6 (Jan. 2014). doi: 10.3233/SW-140134.\n[Lew69]\nDavid K. Lewis. Convention: A Philosophical Study. Wiley-Blackwell, 1969.\n[Li+16]\nJiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and\nDan Jurafsky. “Deep Reinforcement Learning for Dialogue Generation”. en.\nIn: arXiv:1606.01541 [cs] (Sept. 2016). arXiv: 1606.01541 [cs].\n[Li+17]\nJiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean, Alan Ritter, and\nDan Jurafsky. “Adversarial Learning for Neural Dialogue Generation”. In:\narXiv:1701.06547 [cs] (Sept. 2017). arXiv: 1701.06547 [cs].\n[LB02]\nEdward Loper and Steven Bird. NLTK: The Natural Language Toolkit. May\n2002. doi: 10.48550/arXiv.cs/0205028. arXiv: cs/0205028.\n[Lor+21]\nRobin Lorenz, Anna Pearson, Konstantinos Meichanetzidis,\nDimitri Kartsaklis, and Bob Coecke. “QNLP in Practice: Running\nCompositional Models of Meaning on a Quantum Computer”. In:\narXiv:2102.12846 [quant-ph] (Feb. 2021). arXiv: 2102.12846 [quant-ph].\n[Ma+19]\nYunpu Ma, Volker Tresp, Liming Zhao, and Yuyi Wang. “Variational\nQuantum Circuit Model for Knowledge Graphs Embedding”. In:\narXiv:1903.00556 [quant-ph] (Feb. 2019). arXiv: 1903.00556 [quant-ph].\n[Mac71]\nS. Mac Lane. Categories for the Working Mathematician. Springer Verlag,\n1971.\n[MS99]\nChristopher D. Manning and Hinrich Schütze. Foundations of Statistical\nNatural Language Processing. Cambridge, MA, USA: MIT Press, 1999.\n[Mar+15]\nMartín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\nCraig Citro, Greg S. Corrado, Andy Davis, Jeﬀrey Dean, Matthieu Devin,\nSanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoﬀrey Irving,\nMichael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,\nManjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga,\nSherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens,\nBenoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,\nVincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals,\nPete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and\nXiaoqiang Zheng. TensorFlow: Large-Scale Machine Learning on\nHeterogeneous Systems. Software available from tensorﬂow.org. 2015. url:\nhttps://www.tensorflow.org/.\n[Mei+21]\nKonstantinos Meichanetzidis, Stefano Gogioso, Giovanni de Felice,\nNicolò Chiappori, Alexis Toumi, and Bob Coecke. “Quantum Natural\nLanguage Processing on Near-Term Quantum Computers”. In: Electronic\nProceedings in Theoretical Computer Science 340 (Sept. 2021), pp. 213–229.\ndoi: 10.4204/EPTCS.340.11. arXiv: 2005.04147 [quant-ph].\n216\n[Mei+20]\nKonstantinos Meichanetzidis, Alexis Toumi, Giovanni de Felice, and\nBob Coecke. “Grammar-Aware Question-Answering on Quantum Computers”.\nIn: arXiv:2012.03756 [quant-ph] (Dec. 2020). arXiv: 2012.03756 [quant-ph].\n[Meu+17]\nAaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ondřej Čertík,\nSergey B. Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov,\nJason K. Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig,\nBrian E. Granger, Richard P. Muller, Francesco Bonazzi, Harsh Gupta,\nShivam Vats, Fredrik Johansson, Fabian Pedregosa, Matthew J. Curry,\nAndy R. Terrel, Štěpán Roučka, Ashutosh Saboo, Isuru Fernando,\nSumith Kulal, Robert Cimrman, and Anthony Scopatz. “SymPy: symbolic\ncomputing in Python”. In: PeerJ Computer Science 3 (Jan. 2017), e103. doi:\n10.7717/peerj-cs.103.\n[MSS04]\nAlessio Micheli, Diego Sona, and Alessandro Sperduti. “Contextual Processing\nof Structured Data by Recursive Cascade Correlation”. eng. In: IEEE\ntransactions on neural networks 15.6 (Nov. 2004), pp. 1396–1410. doi:\n10.1109/TNN.2004.837783.\n[Mik+10]\nTomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan Cernocký, and\nSanjeev Khudanpur. “Recurrent Neural Network Based Language Model”. In:\nProceedings of the 11th Annual Conference of the International Speech\nCommunication Association, INTERSPEECH 2010. Vol. 2. Jan. 2010,\npp. 1045–1048.\n[Mil56]\nGeorge A. Miller. “The Magical Number Seven, plus or Minus Two: Some\nLimits on Our Capacity for Processing Information”. In: Psychological Review\n63.2 (1956), pp. 81–97. doi: 10.1037/h0043158.\n[MP15]\nWill Monroe and Christopher Potts. “Learning in the Rational Speech Acts\nModel”. In: arXiv:1510.06807 [cs] (Oct. 2015). arXiv: 1510.06807 [cs].\n[Mon70a]\nRichard Montague. “English as a Formal Language”. In: Linguaggi Nella\nSocieta e Nella Tecnica. Ed. by Bruno Visentini. Edizioni di Communita,\n1970, pp. 188–221.\n[Mon70b]\nRichard Montague. “Universal Grammar”. en. In: Theoria 36.3 (1970),\npp. 373–398. doi: 10.1111/j.1755-2567.1970.tb00434.x.\n[Mon73]\nRichard Montague. “The Proper Treatment of Quantiﬁcation in Ordinary\nEnglish”. In: Approaches to Natural Language. Ed. by K. J. J. Hintikka,\nJ. Moravcsic, and P. Suppes. Dordrecht: Reidel, 1973, pp. 221–242.\n[Moo88]\nMichael Moortgat. Categorial Investigations: Logical and Linguistic Aspects of\nthe Lambek Calculus. 9. Walter de Gruyter, 1988.\n[MR12a]\nRichard Moot and Christian Retoré. “Lambek Calculus and Montague\nGrammar”. en. In: The Logic of Categorial Grammars: A Deductive Account of\nNatural Language Syntax and Semantics. Ed. by Richard Moot and\nChristian Retoré. Lecture Notes in Computer Science. Berlin, Heidelberg:\nSpringer, 2012, pp. 65–99. doi: 10.1007/978-3-642-31555-8_3.\n217\n[MR12b]\nRichard Moot and Christian Retoré. “The Multimodal Lambek Calculus”. en.\nIn: The Logic of Categorial Grammars: A Deductive Account of Natural\nLanguage Syntax and Semantics. Ed. by Richard Moot and Christian Retoré.\nLecture Notes in Computer Science. Berlin, Heidelberg: Springer, 2012,\npp. 149–191. doi: 10.1007/978-3-642-31555-8_5.\n[MR12c]\nRichard Moot and Christian Retoré. “The Non-Associative Lambek Calculus”.\nen. In: The Logic of Categorial Grammars: A Deductive Account of Natural\nLanguage Syntax and Semantics. Ed. by Richard Moot and Christian Retoré.\nLecture Notes in Computer Science. Berlin, Heidelberg: Springer, 2012,\npp. 101–147. doi: 10.1007/978-3-642-31555-8_4.\n[Mor11]\nKatarzyna Moroz. “A Savateev-Style Parsing Algorithm for Pregroup\nGrammars”. en. In: Formal Grammar. Ed. by Philippe de Groote, Markus Egg,\nand Laura Kallmeyer. Lecture Notes in Computer Science. Berlin, Heidelberg:\nSpringer, 2011, pp. 133–149. doi: 10.1007/978-3-642-20169-1_9.\n[Nav09]\nRoberto Navigli. “Word Sense Disambiguation: A Survey”. In: ACM\nComputing Surveys 41.2 (Feb. 2009), 10:1–10:69. doi:\n10.1145/1459352.1459355.\n[NTK11]\nMaximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. “A Three-Way\nModel for Collective Learning on Multi-Relational Data”. In: Proceedings of\nthe 28th International Conference on International Conference on Machine\nLearning. ICML’11. Madison, WI, USA: Omnipress, June 2011, pp. 809–816.\n[OGo19]\nBryan O’Gorman. “Parameterization of Tensor Network Contraction”. In:\narXiv:1906.00013 [quant-ph] (2019), 19 pages. doi:\n10.4230/LIPIcs.TQC.2019.10. arXiv: 1906.00013 [quant-ph].\n[OBW88]\nRichard T. Oehrle, E. Bach, and Deirdre Wheeler, eds. Categorial Grammars\nand Natural Language Structures. en. Studies in Linguistics and Philosophy.\nSpringer Netherlands, 1988. doi: 10.1007/978-94-015-6878-4.\n[OK19]\nIlsang Ohn and Yongdai Kim. “Smooth Function Approximation by Deep\nNeural Networks with General Activation Functions”. en. In: Entropy 21.7\n(July 2019), p. 627. doi: 10.3390/e21070627.\n[OMK19]\nDaniel W. Otter, Julian R. Medina, and Jugal K. Kalita. “A Survey of the\nUsages of Deep Learning in Natural Language Processing”. In:\narXiv:1807.10854 [cs] (Dec. 2019). arXiv: 1807.10854 [cs].\n[Par76]\nBarbara Partee. Montague Grammar. en. Elsevier, 1976. doi:\n10.1016/C2013-0-11289-5.\n[Pas+19]\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,\nGregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein,\nLuca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,\nZachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. “PyTorch: An\nImperative Style, High-Performance Deep Learning Library”. In: Advances in\nNeural Information Processing Systems 32. Ed. by H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett. Curran Associates,\nInc., 2019, pp. 8024–8035. url:\n218\nhttp://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-\nhigh-performance-deep-learning-library.pdf.\n[Pei65]\nC. S. Peirce. “On a New List of Categories”. In: Proceedings of the American\nAcademy of Arts and Sciences 7 (1865), pp. 287–298. doi: 10.2307/20179567.\n[Pei97]\nCharles S. Peirce. “The Logic of Relatives”. In: The Monist 7.2 (1897),\npp. 161–217.\n[Pei06]\nCharles Santiago Sanders Peirce. “Prolegomena to an Apology for\nPragmaticism”. In: The Monist 16.4 (1906), pp. 492–546.\n[Pen71]\nRoger Penrose. “Applications of Negative Dimensional Tensors”. en. 1971.\n[Pen93]\nM. Pentus. “Lambek Grammars Are Context Free”. In: [1993] Proceedings\nEighth Annual IEEE Symposium on Logic in Computer Science. June 1993,\npp. 429–433. doi: 10.1109/LICS.1993.287565.\n[PV17]\nVasily Pestun and Yiannis Vlassopoulos. “Tensor Network Language Model”.\nIn: arXiv:1710.10248 [cond-mat, stat] (Oct. 2017). arXiv: 1710.10248\n[cond-mat, stat].\n[PGW17]\nMatthew Pickering, Jeremy Gibbons, and Nicolas Wu. “Profunctor Optics:\nModular Data Accessors”. In: The Art, Science, and Engineering of\nProgramming 1.2 (Apr. 2017), p. 7. doi:\n10.22152/programming-journal.org/2017/1/7. arXiv: 1703.10857.\n[Pop+20]\nMartin Popel, Marketa Tomkova, Jakub Tomek, Łukasz Kaiser,\nJakob Uszkoreit, Ondřej Bojar, and Zdeněk Žabokrtský. “Transforming\nMachine Translation: A Deep Learning System Reaches News Translation\nQuality Comparable to Human Professionals”. en. In: Nature Communications\n11.1 (Sept. 2020), p. 4381. doi: 10.1038/s41467-020-18073-9.\n[Pos47]\nEmil L. Post. “Recursive Unsolvability of a Problem of Thue”. EN. In: Journal\nof Symbolic Logic 12.1 (Mar. 1947), pp. 1–11.\n[PR97]\nJohn Power and Edmund Robinson. “Premonoidal Categories and Notions of\nComputation”. In: Mathematical Structures in Computer Science 7.5 (Oct.\n1997), pp. 453–468. doi: 10.1017/S0960129597002375.\n[Pre07]\nAnne Preller. “Linear Processing with Pregroups”. en. In: Studia Logica 87.2-3\n(Dec. 2007), pp. 171–197. doi: 10.1007/s11225-007-9087-0.\n[PL07]\nAnne Preller and Joachim Lambek. “Free Compact 2-Categories”. en. In:\nMathematical Structures in Computer Science 17.02 (Apr. 2007), p. 309. doi:\n10.1017/S0960129506005901.\n[Qi+20]\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\nChristopher D. Manning. “Stanza: A Python Natural Language Processing\nToolkit for Many Human Languages”. In: Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics: System\nDemonstrations. 2020.\n[Ren19]\nJérôme Renault. “A Tutorial on Zero-Sum Stochastic Games”. May 2019.\n[Ret05]\nChristian Retoré. The Logic of Categorial Grammars: Lecture Notes. en.\nReport. INRIA, Sept. 2005, p. 105.\n219\n[RL00]\nM. M. G. Ricci and T. Levi-Civita. “Méthodes de calcul diﬀérentiel absolu et\nleurs applications”. fr. In: Mathematische Annalen 54.1 (Mar. 1900),\npp. 125–201. doi: 10.1007/BF01454201.\n[Ril18a]\nMitchell Riley. “Categories of Optics”. In: arXiv:1809.00738 [math] (Sept.\n2018). arXiv: 1809.00738 [math].\n[Ril18b]\nMitchell Riley. “Categories of Optics”. In: arXiv:1809.00738 [math] (Sept.\n2018). arXiv: 1809.00738 [math].\n[Rob+19]\nChase Roberts, Ashley Milsted, Martin Ganahl, Adam Zalcman,\nBruce Fontaine, Yijian Zou, Jack Hidary, Guifre Vidal, and\nStefan Leichenauer. TensorNetwork: A Library for Physics and Machine\nLearning. May 2019. doi: 10.48550/arXiv.1905.01330. arXiv: 1905.01330\n[cond-mat, physics:hep-th, physics:physics, stat].\n[SCC13]\nMehrnoosh Sadrzadeh, Stephen Clark, and Bob Coecke. “The Frobenius\nAnatomy of Word Meanings I: Subject and Object Relative Pronouns”. In:\nJournal of Logic and Computation 23.6 (Dec. 2013), pp. 1293–1317. doi:\n10.1093/logcom/ext044. arXiv: 1404.5278.\n[SCC14]\nMehrnoosh Sadrzadeh, Stephen Clark, and Bob Coecke. “The Frobenius\nAnatomy of Word Meanings II: Possessive Relative Pronouns”. In: Journal of\nLogic and Computation abs/1406.4690 (2014), exu027.\n[SRP91]\nVijay A. Saraswat, Martin Rinard, and Prakash Panangaden. “The Semantic\nFoundations of Concurrent Constraint Programming”. In: Proceedings of the\n18th ACM SIGPLAN-SIGACT Symposium on Principles of Programming\nLanguages. POPL ’91. New York, NY, USA: Association for Computing\nMachinery, Jan. 1991, pp. 333–352. doi: 10.1145/99583.99627.\n[Sav12]\nYury Savateev. “Product-Free Lambek Calculus Is NP-Complete”. en. In:\nAnnals of Pure and Applied Logic. The Symposium on Logical Foundations of\nComputer Science 2009 163.7 (July 2012), pp. 775–788. doi:\n10.1016/j.apal.2011.09.017.\n[Sch90]\nSchröder. Vorlesungen über die algebra der logik. ger. Leipzig, B. G. Teubner,\n1890.\n[SP97]\nM. Schuster and K.K. Paliwal. “Bidirectional Recurrent Neural Networks”. In:\nIEEE Transactions on Signal Processing 45.11 (Nov. 1997), pp. 2673–2681.\ndoi: 10.1109/78.650093.\n[Sch98]\nHinrich Schütze. “Automatic Word Sense Discrimination”. In: Computational\nLinguistics 24.1 (1998), pp. 97–123.\n[Sha53]\nL. S. Shapley. “Stochastic Games”. en. In: Proceedings of the National\nAcademy of Sciences 39.10 (Oct. 1953), pp. 1095–1100. doi:\n10.1073/pnas.39.10.1095.\n[STS20]\nDan Shiebler, Alexis Toumi, and Mehrnoosh Sadrzadeh. Incremental\nMonoidal Grammars. Jan. 2020. doi: 10.48550/arXiv.2001.02296. arXiv:\n2001.02296 [cs].\n[Siv+20]\nSeyon Sivarajah, Silas Dilkes, Alexander Cowtan, Will Simmons,\nAlec Edgington, and Ross Duncan. “T|ket> : A Retargetable Compiler for\nNISQ Devices”. en. In: Quantum Science and Technology 6.1 (Nov. 2020).\n220\n[SJ07]\nNoah A. Smith and Mark Johnson. “Weighted and Probabilistic Context-Free\nGrammars Are Equally Expressive”. In: Computational Linguistics 33.4\n(2007), pp. 477–491. doi: 10.1162/coli.2007.33.4.477.\n[Soc+13a]\nR. Socher, Alex Perelygin, J. Wu, Jason Chuang, Christopher D. Manning,\nA. Ng, and Christopher Potts. “Recursive Deep Models for Semantic\nCompositionality Over a Sentiment Treebank”. In: EMNLP. 2013.\n[Soc+13b]\nRichard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng.\n“Reasoning with Neural Tensor Networks for Knowledge Base Completion”.\nIn: Proceedings of the 26th International Conference on Neural Information\nProcessing Systems - Volume 1. NIPS’13. Red Hook, NY, USA: Curran\nAssociates Inc., Dec. 2013, pp. 926–934.\n[SW97]\nKaren Sparck Jones and Peter Willett, eds. Readings in Information Retrieval.\nSan Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1997.\n[SS97]\nA. Sperduti and A. Starita. “Supervised Neural Networks for the\nClassiﬁcation of Structures”. In: IEEE Transactions on Neural Networks 8.3\n(May 1997), pp. 714–735. doi: 10.1109/72.572108.\n[Spi12]\nDavid I. Spivak. “Functorial Data Migration”. en. In: Information and\nComputation 217 (Aug. 2012), pp. 31–51. doi: 10.1016/j.ic.2012.05.001.\n[Sta04]\nEdward Stabler. “Varieties of Crossing Dependencies: Structure Dependence\nand Mild Context Sensitivity”. In: Cognitive Science 28 (Sept. 2004),\npp. 699–720. doi: 10.1016/j.cogsci.2004.05.002.\n[Sta70]\nRobert C. Stalnaker. “Pragmatics”. In: Synthese 22.1-2 (1970), pp. 272–289.\ndoi: 10.1007/bf00413603.\n[Sta79]\nRichard Statman. “The Typed λ-Calculus Is Not Elementary Recursive”. en.\nIn: Theoretical Computer Science 9.1 (July 1979), pp. 73–81. doi:\n10.1016/0304-3975(79)90007-0.\n[Ste87]\nMark Steedman. “Combinatory Grammars and Parasitic Gaps”. en. In:\nNatural Language & Linguistic Theory 5.3 (Aug. 1987), pp. 403–439. doi:\n10.1007/BF00134555.\n[Ste00]\nMark Steedman. The Syntactic Process. Cambridge, MA, USA: MIT Press,\n2000.\n[Ste19]\nMark Steedman. 14. Combinatory Categorial Grammar. en. De Gruyter\nMouton, May 2019. Chap. Current Approaches to Syntax, pp. 389–420.\n[Str07]\nLutz Straßurger. “What Is a Logic, and What Is a Proof?” en. In: Logica\nUniversalis. Ed. by Jean-Yves Beziau. Basel: Birkhäuser, 2007, pp. 135–152.\ndoi: 10.1007/978-3-7643-8354-1_8.\n[Tar36]\nAlfred Tarski. “The Concept of Truth in Formalized Languages”. In: Logic,\nSemantics, Metamathematics. Ed. by A. Tarski. Oxford University Press,\n1936, pp. 152–278.\n[Tar41]\nAlfred Tarski. “On the Calculus of Relations”. EN. In: Journal of Symbolic\nLogic 6.3 (Sept. 1941), pp. 73–89.\n221\n[Tar43]\nAlfred Tarski. “The Semantic Conception of Truth and the Foundations of\nSemantics”. In: Philosophy and Phenomenological Research 4.3 (1943),\npp. 341–376. doi: 10.2307/2102968.\n[Ter12]\nKazushige Terui. “Semantic Evaluation, Intersection Types and Complexity of\nSimply Typed Lambda Calculus”. In: 23rd International Conference on\nRewriting Techniques and Applications (RTA’12). Ed. by Ashish Tiwari.\nVol. 15. Leibniz International Proceedings in Informatics (LIPIcs). Dagstuhl,\nGermany: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, 2012,\npp. 323–338. doi: 10.4230/LIPIcs.RTA.2012.323.\n[Tes59]\nLucien Tesniere. Elements de Syntaxe Structurale. Paris/FRA: Klincksiek,\n1959.\n[Tho13]\nMichaël Thomazo. “Conjunctive Query Answering Under Existential Rules -\nDecidability, Complexity, and Algorithms”. en. PhD thesis. Oct. 2013. url:\nhttps://tel.archives-ouvertes.fr/tel-00925722.\n[TKG03]\nDomonkos Tikk, László T. Kóczy, and Tamás D. Gedeon. “A Survey on\nUniversal Approximation and Its Limits in Soft Computing Techniques”. en.\nIn: International Journal of Approximate Reasoning 33.2 (June 2003),\npp. 185–202. doi: 10.1016/S0888-613X(03)00021-5.\n[Tou22]\nAlexis Toumi. “Category Theory for Quantum Natural Language Processing”.\nUniversity of Oxford, 2022.\n[TdY22]\nAlexis Toumi, Giovanni de Felice, and Richie Yeung. DisCoPy for the\nQuantum Computer Scientist. May 2022. doi: 10.48550/arXiv.2205.05190.\narXiv: 2205.05190 [quant-ph].\n[TK21]\nAlexis Toumi and Alex Koziell-Pipe. “Functorial Language Models”. In:\narXiv:2103.14411 [cs, math] (Mar. 2021). arXiv: 2103.14411 [cs, math].\n[TYd21]\nAlexis Toumi, Richie Yeung, and Giovanni de Felice. “Diagrammatic\nDiﬀerentiation for Quantum Machine Learning”. In: Electronic Proceedings in\nTheoretical Computer Science 343 (Sept. 2021), pp. 132–144. doi:\n10.4204/EPTCS.343.7. arXiv: 2103.07960 [quant-ph].\n[TM21]\nAlex Townsend-Teague and Konstantinos Meichanetzidis. “Classifying\nComplexity with the ZX-Calculus: Jones Polynomials and Potts Partition\nFunctions”. In: arXiv:2103.06914 [quant-ph] (Mar. 2021). arXiv: 2103.06914\n[quant-ph].\n[TN19]\nRocco Tripodi and Roberto Navigli. “Game Theory Meets Embeddings: A\nUniﬁed Framework for Word Sense Disambiguation”. In: Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP). Hong Kong, China: Association for Computational\nLinguistics, Nov. 2019, pp. 88–99. doi: 10.18653/v1/D19-1009.\n[Tro+17]\nThéo Trouillon, Christopher R. Dance, Johannes Welbl, Sebastian Riedel,\nÉric Gaussier, and Guillaume Bouchard. “Knowledge Graph Completion via\nComplex Tensor Factorization”. In: The Journal of Machine Learning\nResearch 18.1 (2017), pp. 4735–4772. eprint: arXiv:1702.06879.\n222\n[Tur37]\nA. M. Turing. “Computability and \\Lambda-Deﬁnability”. In: Journal of\nSymbolic Logic 2.4 (1937), pp. 153–163. doi: 10.2307/2268280.\n[TP10]\nPeter D. Turney and Patrick Pantel. “From Frequency to Meaning: Vector\nSpace Models of Semantics”. In: Journal of Artiﬁcial Intelligence Research 37\n(Feb. 2010), pp. 141–188. doi: 10.1613/jair.2934. arXiv: 1003.1141.\n[TN05]\nKarl Tuyls and Ann Nowé. “Evolutionary Game Theory and Multi-Agent\nReinforcement Learning”. en. In: The Knowledge Engineering Review 20.1\n(Mar. 2005), pp. 63–90. doi: 10.1017/S026988890500041X.\n[VKS19]\nVákárMatthijs, KammarOhad, and StatonSam. “A Domain Theory for\nStatistical Probabilistic Programming”. In: Proceedings of the ACM on\nProgramming Languages (Jan. 2019). doi: 10.1145/3290349.\n[van87]\nJohan van Benthem. “Categorial Grammar and Lambda Calculus”. en. In:\nMathematical Logic and Its Applications. Ed. by Dimiter G. Skordev. Boston,\nMA: Springer US, 1987, pp. 39–60. doi: 10.1007/978-1-4613-0897-3_4.\n[van20]\nJohn van de Wetering. “ZX-Calculus for the Working Quantum Computer\nScientist”. In: arXiv:2012.13966 [quant-ph] (Dec. 2020). arXiv: 2012.13966\n[quant-ph].\n[Var82]\nMoshe Y. Vardi. “The Complexity of Relational Query Languages (Extended\nAbstract)”. In: Proceedings of the Fourteenth Annual ACM Symposium on\nTheory of Computing. STOC ’82. New York, NY, USA: Association for\nComputing Machinery, May 1982, pp. 137–146. doi: 10.1145/800070.802186.\n[Vas+17]\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You\nNeed”. In: arXiv:1706.03762 [cs] (Dec. 2017). arXiv: 1706.03762 [cs].\n[VW94]\nK. Vijay-Shanker and David J. Weir. “The Equivalence Of Four Extensions Of\nContext-Free Grammars”. In: Mathematical Systems Theory 27 (1994),\npp. 27–511.\n[Von29]\nJ. Von Neumann. “Zur Algebra Der Funktionaloperationen Und Theorie Der\nNormalen Operatoren”. In: Mathematische Annalen 102 (1929), pp. 370–427.\ndoi: 10.1007/BF01782352.\n[Wal89a]\nR. F. C. Walters. “A Note on Context-Free Languages”. en. In: Journal of\nPure and Applied Algebra 62.2 (Dec. 1989), pp. 199–203. doi:\n10.1016/0022-4049(89)90151-5.\n[Wal89b]\nR. F. C. Walters. “The Free Category with Products on a Multigraph”. en. In:\nJournal of Pure and Applied Algebra 62.2 (Dec. 1989), pp. 205–210. doi:\n10.1016/0022-4049(89)90152-7.\n[Wan+17]\nQ. Wang, Z. Mao, B. Wang, and L. Guo. “Knowledge Graph Embedding: A\nSurvey of Approaches and Applications”. In: IEEE Transactions on\nKnowledge and Data Engineering 29.12 (Dec. 2017), pp. 2724–2743. doi:\n10.1109/TKDE.2017.2754499.\n223\n[WSL19]\nWilliam Yang Wang, Sameer Singh, and Jiwei Li. “Deep Adversarial Learning\nfor NLP”. In: Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Tutorials.\nMinneapolis, Minnesota: Association for Computational Linguistics, June\n2019, pp. 1–5. doi: 10.18653/v1/N19-5001.\n[Wil03]\nEdwin Williams. Representation Theory. en. MIT Press, 2003.\n[WZ21]\nPaul Wilson and Fabio Zanasi. “The Cost of Compositionality: A\nHigh-Performance Implementation of String Diagram Composition”. In:\narXiv:2105.09257 [cs, math] (May 2021). arXiv: 2105.09257 [cs, math].\n[Wit53]\nLudwig Wittgenstein. Philosophical Investigations. Oxford: Basil Blackwell,\n1953.\n[Wu+20]\nYongji Wu, Defu Lian, Yiheng Xu, Le Wu, and Enhong Chen. “Graph\nConvolutional Networks with Markov Random Field Reasoning for Social\nSpammer Detection”. In: Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence 34.01 (Apr. 2020), pp. 1054–1061. doi:\n10.1609/aaai.v34i01.5455.\n[XHW18]\nWenhan Xiong, Thien Hoang, and William Yang Wang. “DeepPath: A\nReinforcement Learning Method for Knowledge Graph Reasoning”. In:\narXiv:1707.06690 [cs] (July 2018). arXiv: 1707.06690 [cs].\n[Yan+15]\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng.\n“Embedding Entities and Relations for Learning and Inference in Knowledge\nBases”. In: arXiv:1412.6575 [cs] (Aug. 2015). arXiv: 1412.6575 [cs].\n[YK21]\nRichie Yeung and Dimitri Kartsaklis. “A CCG-Based Version of the DisCoCat\nFramework”. In: arXiv:2105.07720 [cs, math] (May 2021). arXiv: 2105.07720\n[cs, math].\n[YNM17]\nMasashi Yoshikawa, Hiroshi Noji, and Yuji Matsumoto. “A* CCG Parsing\nwith a Supertag and Dependency Factored Model”. In: Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers). Vancouver, Canada: Association for Computational Linguistics,\nJuly 2017, pp. 277–287. doi: 10.18653/v1/P17-1026.\n[Yos+19]\nMasashi Yoshikawa, Hiroshi Noji, Koji Mineshima, and Daisuke Bekki.\n“Automatic Generation of High Quality CCGbanks for Parser Domain\nAdaptation”. In: arXiv:1906.01834 [cs] (June 2019). arXiv: 1906.01834 [cs].\n[You67]\nDaniel H. Younger. “Recognition and Parsing of Context-Free Languages in\nTime N3”. en. In: Information and Control 10.2 (Feb. 1967), pp. 189–208. doi:\n10.1016/S0019-9958(67)80007-X.\n[ZC16]\nWilliam Zeng and Bob Coecke. “Quantum Algorithms for Compositional\nNatural Language Processing”. In: Electronic Proceedings in Theoretical\nComputer Science 221 (Aug. 2016), pp. 67–75. doi: 10.4204/EPTCS.221.8.\narXiv: 1608.01406.\n[Zha+19]\nLipeng Zhang, Peng Zhang, Xindian Ma, Shuqin Gu, Zhan Su, and\nDawei Song. “A Generalized Language Model in Tensor Space”. In:\narXiv:1901.11167 [cs] (Jan. 2019). arXiv: 1901.11167 [cs].\n224\n[Zho+21]\nJie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang,\nZhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph Neural\nNetworks: A Review of Methods and Applications. Oct. 2021. arXiv:\n1812.08434 [cs, stat].\n[Zho+17]\nQingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and\nMing Zhou. “Neural Question Generation from Text: A Preliminary Study”.\nIn: arXiv:1704.01792 [cs] (Apr. 2017). arXiv: 1704.01792 [cs].\n225\n",
  "categories": [
    "cs.CL",
    "math.CT"
  ],
  "published": "2022-12-13",
  "updated": "2022-12-13"
}