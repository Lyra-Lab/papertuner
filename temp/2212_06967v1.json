{
  "id": "http://arxiv.org/abs/2212.06967v1",
  "title": "Explaining Agent's Decision-making in a Hierarchical Reinforcement Learning Scenario",
  "authors": [
    "Hugo Muñoz",
    "Ernesto Portugal",
    "Angel Ayala",
    "Bruno Fernandes",
    "Francisco Cruz"
  ],
  "abstract": "Reinforcement learning is a machine learning approach based on behavioral\npsychology. It is focused on learning agents that can acquire knowledge and\nlearn to carry out new tasks by interacting with the environment. However, a\nproblem occurs when reinforcement learning is used in critical contexts where\nthe users of the system need to have more information and reliability for the\nactions executed by an agent. In this regard, explainable reinforcement\nlearning seeks to provide to an agent in training with methods in order to\nexplain its behavior in such a way that users with no experience in machine\nlearning could understand the agent's behavior. One of these is the\nmemory-based explainable reinforcement learning method that is used to compute\nprobabilities of success for each state-action pair using an episodic memory.\nIn this work, we propose to make use of the memory-based explainable\nreinforcement learning method in a hierarchical environment composed of\nsub-tasks that need to be first addressed to solve a more complex task. The end\ngoal is to verify if it is possible to provide to the agent the ability to\nexplain its actions in the global task as well as in the sub-tasks. The results\nobtained showed that it is possible to use the memory-based method in\nhierarchical environments with high-level tasks and compute the probabilities\nof success to be used as a basis for explaining the agent's behavior.",
  "text": "Preprint accepted at the 41st International Conference of the Chilean Computer Science Society, SCCC 2022, Santiago, Chile, 2022.\nExplaining Agent’s Decision-making in a\nHierarchical Reinforcement Learning Scenario\nHugo Mu˜noz∗, Ernesto Portugal†, Angel Ayala†, Bruno Fernandes† and Francisco Cruz‡∗\n∗Escuela de Ingenier´ıa, Universidad Central de Chile, Santiago, Chile\nEmail: hugo.munoz@alumnos.ucentral.cl\n†Escola Polit´ecnica de Pernambuco, Universidade de Pernambuco, Recife, Brasil\nEmails: {eipb, aaam, bjtf}@ecomp.poli.br\n‡School of Computer Science and Engineering, University of New South Wales, Sydney, Australia\nEmail: f.cruz@unsw.edu.au\nAbstract—Reinforcement learning is a machine learning ap-\nproach based on behavioral psychology. It is focused on learning\nagents that can acquire knowledge and learn to carry out new\ntasks by interacting with the environment. However, a problem\noccurs when reinforcement learning is used in critical contexts\nwhere the users of the system need to have more information and\nreliability for the actions executed by an agent. In this regard,\nexplainable reinforcement learning seeks to provide to an agent\nin training with methods in order to explain its behavior in such\na way that users with no experience in machine learning could\nunderstand the agent’s behavior. One of these is the memory-\nbased explainable reinforcement learning method that is used to\ncompute probabilities of success for each state-action pair using\nan episodic memory. In this work, we propose to make use of\nthe memory-based explainable reinforcement learning method in\na hierarchical environment composed of sub-tasks that need to\nbe ﬁrst addressed to solve a more complex task. The end goal\nis to verify if it is possible to provide to the agent the ability\nto explain its actions in the global task as well as in the sub-\ntasks. The results obtained showed that it is possible to use the\nmemory-based method in hierarchical environments with high-\nlevel tasks and compute the probabilities of success to be used\nas a basis for explaining the agent’s behavior.\nI. INTRODUCTION\nDuring the last decade, the growth of machine learning\nalgorithms has been relevant to the point of being successful in\nareas of science as well as in the daily life of humans. Some\nareas include health, image processing, games, autonomous\ncars, recommended content, among others [1]. Many of these\nalgorithms use artiﬁcial neural networks (ANN) that are known\nas a black box system [2] or a combination of ANN and\nphenomenological models known as gray box systems [3],\n[4]. An ANN allows to infer an output value that depends\non a combination of input values, however, the process for\ndetermining the inference value is based on empirical data.\nAs a result, these systems lack credibility in environments\nwhere the decisions need to be selected in such a way so\nthat they are correct and grounded. For example, in the case\nof diagnosing illnesses, an algorithm needs to be capable of\nconsidering different alternatives and determine and catalog\nthe test correctly.\nFurthermore, there exist other autonomous learning algo-\nrithms mainly used for solving sequential decisions problems,\nknown as reinforcement learning (RL) [5]. In this type of\nalgorithm, an agent needs to learn to make a decision through\ntrial and error to solve a task formulated as a Markovian\ndecision problem. Thus, commencing from a state st, an agent\nchooses an action at that allows the transition to the next\nstate st+1, obtaining a reward signal rt+1 to evaluate the\nquality of the action selected from state st. The main idea\nis that through observation of the reward at each step, the\nagent will be capable of reﬁning a policy that allows it to\nselect actions to receive a higher reward at the end of the\ntask [6]. Similarly for the RL algorithms, a person without\nknowledge of artiﬁcial intelligence does not know the form or\naspects that are considered by the agent to select an action [7].\nIn RL methods, the hierarchical approach is based on the\nability of cognitive beings to resolve complex challenges by\ndividing them into more tractable smaller parts. In addition, it\nis possible to learn new tasks quickly through the sequence of\nthe behaviors learned, although the task requires various low-\nlevel actions [8]. For example, humans can learn new tasks\nquickly by classifying the parts learned, including even if the\ntask requires millions of low-level actions, such as muscular\ncontractions. Hierarchical reinforcement learning (HRL), an\nextension of RL, models these problems in order to make the\nagents represent complicated behaviors as a short sequence\nof high-level actions. As a result, the agent can solve more\ncomplex problems. Therefore, if some solutions require a great\nnumber of low-level actions, the hierarchical policy could be\nconverted into a sequence of high-level actions [8].\nIn this regard, explainable artiﬁcial intelligence (XAI) is\nthe area that seeks to provide the ability to those systems in\norder to be able to explain its behavior in such a way that\nit is understandable to humans [9], [10]. Likewise, explain-\nable reinforcement learning (XRL) emerges as a sub-task of\nXAI [11]. Since this subarea is focused on RL, the methods\nfor making the system capable of providing an explanation\nare based on stages from the learning process. These methods\nmay be based on: relevant features, the learning and Markov\ndecision process (MDP), or at policy level [12].\nDiverse techniques have been used in order to be able to\nexplain behavior in hierarchical environments. For instance,\njust as dividing the tasks into different levels where the highest\nlevel groups the lowest level tasks and trains an agent for each\narXiv:2212.06967v1  [cs.AI]  14 Dec 2022\nlevel [13]. Also, based on human behavior to navigate through\na room or simply that the model learns to carry our basic\ntasks and afterwards put together these basic tasks to carry\nout new ones. In this research, we sought to provide another\nalternative so that the models would be capable of providing\nan explanation in hierarchical contexts. Using the memory-\nbased explainable reinforcement learning method [14], we\nproposed analyzing the probabilities of success for different\nlow-level tasks in a hierarchy, in addition to obtaining a global\nprobability of success, for the completed task. The probability\nof success may be used as a basis for explaining the behavior\nof an autonomous agent in a hierarchical environment. The\nexplanations generated are offered in a natural language rep-\nresentation with the ability to be better understood by not only\nRL practitioners, but also by any person with no knowledge\nof the area [15].\nOur main contribution is the extension of a memory-based\nexplainability method for hierarchical scenarios. The rest of\nthe paper is structured as follows. The next section reviews\nthe background and related works. The third section introduces\nthe explainability method used in this work, i.e., hierarchical\nexplainable reinforcement learning. Section 4 describes the\nexperimental scenario and Section 5 shows the results obtained\nduring experiments. Finally, Section 6 depicts the main con-\nclusions and possible future work.\nII. RELATED WORKS\nPrevious works have studied explainability in RL algorithms\nusing explanations in distinct levels, depending on the target\naudience and the type of task involved [11], [16]. A recent\nstudy classiﬁed the XRL methods as transparent algorithms\nand post-hoc explainability. In the ﬁrst case, the algorithms\npresent a transparent architecture that allows them to explain\ndirectly from the method without the need of an external\nprocess. In the second case, the algorithms must be analyzed\nafter the execution of the method. Another study [15] was\nmore focused on the explanation of responsibilities for goal-\ndriven tasks based on a generic concept for different users in\na robotic scenario. In this approach, a probability of success is\ncomputed for a robot action from a particular state to indicate\nthe conﬁdence of reaching a ﬁnal state. These goal-driven\nexplanations are separated as follows:\n• Introspection-based [17]: where the probability of success\nis estimated directly from the Q-values obtained.\n• Learning-based [18]: where the probability of success is\nlearned during the training process.\n• Memory-based [14]: where the probability of success is\ncomputed using the total number of transitions and the\ntotal number of transition within a correct sequence.\nFor more complex tasks, studies that have been published\nhave used hierarchical reinforcement learning (HRL). HRL\nallows reducing the complexity of the task by dividing it\ninto sub-tasks to solve the problem. In [19], they assert\nthat the process of understanding a complex explanation is\nhierarchical. They use the concept of mentality to provide\nan explanation to a human through hierarchical XRL. They\ncarried out experiments set in the scavenger-hunt domain by\nlooking for a treasure where the robot looks for explanations\nat intentional and action levels. This study demonstrated that\nhumans prefer explanations with different levels of detail.\nIn the study [20], hierarchical XRL was used as the basis\nto explain the decision-making function. They used Deep\nDeterministic Policy Gradient (DDPG). DDPG employs an\nANN to predict two hierarchical levels (high level and low\nlevel), the sub-tasks and the goal. However, for researchers,\nthe explainability level for humans is still considered very\npremature. Moreover, in [21] is proposed a method to train\nan RL agent to give local explanations by deconstructing\nhierarchical goals. In their research, they used HRL to help the\nreward decomposition algorithm (drQ) to give explanations.\nUtilizing two hierarchical levels, they employed a high-level\nagent to learn the sequences of the challenges in order to solve\nthe task while the low-level agents were in charge of solving\nthe challenges.\nIn summary, in the ﬁeld of XRL, we can classify the\nmethods into two groups: transparent algorithm or post-hoc\nexplainability. Nonetheless, although a few methods have\naddressed providing explanations of RL algorithms, they differ\nfrom each other in terms of what they want to explain and to\nwhom. Particularly in the ﬁeld of HRL, the general view of\nexplainability is still missing. Our approach extends one of\nthose XRL methods to a hierarchical context.\nIII. HIERARCHICAL EXPLAINABLE REINFORCEMENT\nLEARNING\nOur proposed method of explainability was implemented for\nan hierarchical reinforcement learning algorithm (HRL). The\nbasis for an RL algorithm is that an agent needs to ﬁnd ways\nto solve a problem through trial and error by maximizing the\nreward obtained by executing a determined action. Thus, the\nproblem is formulated as a Markov decision process where the\nagent needs to ﬁnd a policy π that allows it to select an action\nat on state st to maximize the reward value obtained rt+1.\nA Q-value function allows estimating the reward that can be\nobtained in the long run for each action a given a state s, such\nas is explained in Equation (1).\nQ∗(s, a) = max\nπ E[rt+γrt+1+γ2rt+2+...|st = s, at = a, π],\n(1)\nwhere γ ∈[0, 1] is the discount factor, a rate that sets how\nmuch the future rewards are considered.\nWith regard to the hierarchical method, we deﬁne low- and\nhigh-level actions. In our scenario, the low-level actions corre-\nspond to the directions that the agent can move. Therefore, an\nestimation of the Q-value was deﬁned for each one of these\nfour actions executed. The high-level actions correspond to\nthe different tasks that the agent should solve. These are three\nhigh-level tasks in the scenario proposed. Along with HRL\nmethod, the memory-based XRL method is used to provide\nexplanations about the actions carried out by the apprentice\nagent in a determined state. Although the Q-values could be\nused to explain the behavior of the RL agent to the expert users\nin the area, our method looks for an agent capable of providing\nan explanation that makes sense to all types of users whether\nor not they know RL. The explanations use the probability of\nsuccessfully completing the task by selecting an action in a\nstate, in addition to the number of transitions the agent carries\nout to accomplish the task. Once the task in ﬁnished, the agent\nmay give an explanation based on probabilities of success; or\na counterfactual explanation of why the agent selected one\naction over another, using more comprehensible language to a\nnon-expert user.\nWe proposed an memory-based explainable reinforcement\nlearning algorithm in order to compute the probability of\nsuccess using an RL agent with episodic memory. When\naccessing the agent’s memory, its behavior may be understood\nas a result of its experience. For this a list of state-action\npairs Tlist is implemented including all of the transitions the\nagent carried out during its learning process. Moreover, in\norder to compute the probability of success, the total number\nof transitions Tt the agent made should be saved and the\nnumber of transitions in a sequences of success Ts. Tt and\nTs correspond to a matrix of state-action pairs. Each time the\nagent reaches the ﬁnal state, the probability of success Ps is\ncomputed by dividing Ts in Tt, i. e., Ps ←Ts/Tt [14].\nThe explainability method is used with the Q-learning\nalgorithm [22]. The method selects an action for state st\nusing the tuple < st, at, rt+1, st+1 >. Algorithm 1 shows\nthe memory-based explainable reinforcement learning method\nand Figure 1 illustrates more details about it. The Q-values for\neach available action is estimated by a function approximation\nbased on a neural network with 100 inputs, representing 100\npossible states, 256 neurons in the hidden layer, and four\noutputs representing the Q-values for each possible low-level\naction. Figure 2 shows the architecture of the network.\nIV. EXPERIMENTAL SCENARIO\nIn the following section, we provide a description about the\nresearch experiments carried out. This includes details about\nprobability, task hierarchies, and sub-tasks. In addition, we\ndescribe the rules of the training process. In this work, a simple\nsimulated scenario is used as this is an initial approximation\nfor this approach in the context of hierarchical reinforcement\nlearning scenarios1. Therefore, we aimed at obtaining prelim-\ninary results to analyze the method’s feasibility in hierarchical\ncontexts.\nA. Problem to resolve\nIn this work, we focused on solving the spaceship escape\nproblem. It consisted of an astronaut (agent) inside of a\nspaceship that needed to return home by crossing a wormhole.\nPrior to crossing the wormhole, the spaceship needed to ﬁrst\nput up a shield before crossing the wormhole; otherwise, it\nwould explode. Furthermore, various black holes existed in the\nenvironment. If the spaceship were to fall into one of these, it\n1Code available at https://github.com/hugo12xx/memoria\nFig. 1.\nFlowchart of the memory-based explainable reinforcement learning\nmethod. Blue boxes are part of the Q-learning algorithm and the orange boxes\nare the steps to implement the explainability method.\nAlgorithm 1 Memory-based explainable reinforcement learn-\ning method [14].\n1: Initialize Q(s, a), Tt, Ts, Ps\n2: for each episode do\n3:\nInitialize Tlist\n4:\nInitialize st ←s0\n5:\nrepeat\n6:\nChoose an action at from st using ϵ-greedy policy\n7:\nTake action at\n8:\nSave state-action transition in Tlist\n9:\nObserve reward rt+1 and next state st+1\n10:\nTt[s][a] ←Tt[s][a] + 1\n11:\nUpdate Q(st, at) using Q-learning algorithm\n12:\nst ←st+1\n13:\nuntil st is terminal or max steps reached\n14:\nif s is goal state then\n15:\nfor each s, a ∈Tlist do\n16:\nTs[s][a] ←Ts[s][a] + 1\n17:\nend for\n18:\nend if\n19:\nCompute Ps ←Ts/Tt\n20: end for\n0\n0\n0\n.\n.\n.\n1\nX =\n.\n.\n.\n.\n.\n.\n.\nQ =\nQup\nQdown\nQleft\nQright\nInput\n100 Units\nHidden Layer \n256 Units\nOutput Layer \n4 Units \nFig. 2. Artiﬁcial neural network to compute the Q-values for four possible\nlow-level actions. The network uses state st as input setting to one the\nagent’s position and zero otherwise. Subsequently, the input was processed\nwith 256 neurons in the hidden layer, and obtaining four Q-values indicating\nthe maximum future reward that may be obtained with each of them.\nwould immediately lose its course, and the spaceship would\nnot be able to escape the black hole. Therefore, it would not\nbe able to return home.\nFigure 3 illustrates a map of the possible states including the\nblack holes, the position of the shield, and the wormhole. Each\nlocation in the 10 × 10 maze grid is indexed with the number\nof states, as shown in this ﬁgure. For example, the starting\nposition for the spaceship, labeled as 0 is shown in the upper\nleft hand corner of the maze. To return home, the position\nof the shield corresponds to state 93, and the position of the\nwormhole corresponds to state 7. It is important to highlight\nthat if the spaceship reaches state 7 without ﬁrst collecting\nthe shield, then, the wormhole would have the same effect as\na black hole. States 3, 13, 20, and 22 corresponded to the\nblack holes. Therefore, initially, the spaceship was surrounded\nby black holes. As a result the ship needed to learn how to\nescape from this zone in order to later ﬁnd the shield.\nB. Hierarchy of the tasks\nIn order to solve the spaceship escape problem, we proposed\nusing hierarchical reinforcement learning for the tasks that\nseeks to divide the problem into high-level actions. The deﬁned\nactions are to reach state 31, then to reach state 93, and ﬁnally\nto reach state 7. As a result, the agent will learn how to\nsolve this sequence of high-level tasks instead of learning all\npossible actions involved at the lower level. In this context,\nselecting any of the four displacement movements allowed\nfor the spaceship corresponds to a low-level task, i.e., go up,\ndown, left, or right.\nAs illustrated in Figure 3, the spaceship starts from state 0\nwith the goal of reaching state 7 with the shield in order to\nescape and return home. The details for all of the high-level\ntasks are shown below:\nGoal ﬁrst high-level task\nGoal second high-level task\nGoal third high-level task\nWormHole\nAgent\nBlack Holes\nShield\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\nFig. 3. Possible states go from 0 to 99. State 0 (the place where the spaceship\nis located in the ﬁgure) represents the initial state. State 7 (wormhole)\nrepresents the ﬁnal state (goal). Black holes are present in states 3, 13, 20,\nand 22. It is not possible to return home if the spaceship falls into one of these\nblack holes. Furthermore, before going towards the wormhole to escape, the\nagent needs to pass through state 93 in order to collect the shield. Otherwise, if\nstate 7 is reached without the shield, the objective will not have been achieved.\n• First high-level task: the ﬁrst task the agent needed to\nperform was to escape from the upper left hand corner.\nThe black holes surrounded the spaceship in this corner.\nTherefore, if it does not learn to escape this zone, it could\nnot return home. It was assumed that the agent will learn\nhow to escape the zone with the black holes by reaching\nstate 31, just below the zone of the black holes.\n• Second high-level task: once outside the corner of the\nblack holes, the agent can move at the lower/inferior part\nof the maze. However, it may not go simply to state 7\nto escape through the wormhole. First, it must ﬁnd the\nshield that was in state 93. Thus, ﬁnding this state was\nthe second high-level task.\n• Third high-level task: once the spaceship has escaped\nfrom the black holes in the corner and collected the\nshield, the agent needed to solve the next high-level\ntask, that was to ﬁnd the wormhole that corresponded\nto exiting, or rather, reaching state 7 with the shield.\nC. Training rules\nThe spaceship problem was represented by a Markov deci-\nsion process and solved through reinforcement learning. Thus,\nsome training rules were established. These are described in\nthe following way:\n• The scenario allowed for the execution of four actions i.e.,\nup, down, left, and right. The agent may select any action\nthat moved the spaceship by one box in the direction\nchosen.\n• If the spaceship falls into a black hole, the training session\nshould stop immediately, and the agent will receive a\nnegative reward equal to a -100.\n• The agent cannot exit the 10 × 10 grid. The actions that\nwould let it exit were limited.\n• The agent should learn to escape from the zone of the\nblack holes. Once the agent is out of this zone, it receives\na reward of 200. This state is reﬂected in Figure 3 with\nthe color yellow and corresponds to state 31.\n• In order to return home, the spaceship needed to collect\nthe shield from state 93 (the color orange in Figure 3).\nAfter obtaining the shield, the agent received a reward of\n200.\n• Having collected the shield and reaching the goal state\nwhich is represented by the wormhole in state 7 (the color\nred in Figure 2), the agent received a reward equal to 500.\nOn the contrary, without the shield, the agent received a\nnegative reward of -100, and the training episode ends.\n• When the agent reached the goal or lost in some way, the\ntraining session ended, and a new one began.\n• The maximum number of steps per episode for the ﬁrst\nhigh-level task was max steps = 10, while for the\nsecond and third was max steps = 100.\nV. RESULTS\nIn the following section, the results from the experiments are\npresented. For the agent, its position was located in the starting\nposition corresponding to the high-level task that was to be\nlearned. For the experiments, the Q-learning algorithm was\nused with learning rate α = 0.00001, discount rate γ = 0.9\nand the action selection method ϵ-greedy with ϵ = 0.7. The\ntraining episodes varied for each high-level task, and they were\ndetermined empirically related to each task and to the training\nparameters.\nNext, the probabilities of success are shown for each state-\naction pair and each high-level task. Additionally, the proba-\nbilities of success for the global task are also shown. Once the\nprobabilities of success were calculated, these could be used\ntogether with a template to generate goal-driven explanations.\nA. Training for the ﬁrst high-level task\nFor the ﬁrst high-level task, the agent began from state 0 as\nthe starting point and state 31 as the goal. In this task, the agent\nonly sought to escape the black holes zone without falling into\nthem. During the training, 10,000 episodes were used with 10\niterations in each one. In spite of the existence of different\nUP\nDOWN\nLEFT\nRIGHT\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFig. 4. Heat map with the probabilities of success for the ﬁrst high-level task.\nThe Y axis represents the 100 possible states and the X axis shows the four\ndeﬁned actions. Higher probabilities of success are observed from state 21 as\nit is more possible to escape the black holes zone and reach the goal state.\nUP\nDOWN\nLEFT\nRIGHT\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFig. 5.\nHeat map of the probabilities of success for the second high-level\ntask. The Y axis represents the 100 possible states, and the X axis shows the\nfour deﬁned actions. State-action pairs that move the agent closer to state 93\nshow a higher probability of successfully collecting the shield.\nUP\nDOWN\nLEFT\nRIGHT\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFig. 6.\nHeat map of the probabilities of success for the third high-level\ntask. The Y axis represents the 100 possible states and the X axis shows the\nfour deﬁned actions. Actions that move the agent into state 7 show a higher\nprobability of success as they allow reaching the wormhole and completing\nthe mission.\nescape routes, the best paths for the agent to understand were\nstates 0, 1, 11, 21, and 31.\nFigure 4 shows the probabilities computed during the ﬁrst\nhigh-level task in a heat map. As can be seen, states 3, 13,\n20, and 22, or any action that made the agent enter any of\nthese states always had possibility of success 0 since these\nstates correspond to the black holes. In the same way, it was\nobserved that the actions that got the agent closer to the goal\npresented the greater probability. For example, in state 21, the\naction go down showed 100% probability of success since\nalways when the agent chose to move down from state 21\nwould escape the black-holes zone and complete the ﬁrst high-\nlevel task. Similarly, on state 11 the action go down had a high\nprobability of success but not of 100%. This happened because\nmoving down from state 11 reached state 21 from which the\nagent could go left or right falling into a black hole. The\naction go left or right from state 11 showed a low probability\nof success but was not null, as the agent would move away\nfrom the goal but still had a probability of escape.\nConsidering the above, when the agent is in state 11 and\nperforms the action go down, it is possible to ask it: why didn’t\nyou go left on the last action?. The agent with the probability\nof success as a basis could respond using a template in the\nfollowing way: I did not move to the left since carrying out\nthis action, I would only have a 25% probability of escaping\nthe black holes while going down, I have a 60% probability\nof escaping.\nUP\nDOWN\nLEFT\nRIGHT\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFig. 7.\nHeat map with the global probabilities of success. The highest\nprobabilities are observed as the different high-level tasks were completed.\nHowever, no state-action pair obtained a probability of success of 100%\nbecause when considering the global problem, no single action guarantees\nreturning home.\nB. Training for the second high-level task\nIn the second high-level task, the agent had state 31 as the\nstarting point and 93 as goal state where it sought to collect\nthe shield. For this task 15,000 episodes were used with a\nmaximum of 100 steps in each one. Of all of the possible\npaths, the best one the agent found included states 31, 41, 51,\n61, 71, 81, 91, 92, and 93.\nFigure 5 illustrates the probabilities of success computed\nduring the second high-level task. The closer the agent comes\nto the goal (state 93) the greater the probabilities of escaping\nthe black holes. In this case, there are three actions that\nprovided 100% probability of success, that corresponds to the\nactions that directly move the spaceship to state 93.\nThe three state-action pairs that moved to state 93 were from\nstate 92 to right, from state 83 to down, and from state 94\nto left. That is, these state-action pairs guarantee completing\nthe mission of collecting the shield. Other possible actions\nfrom those states also showed a high probability of success,\nbecause they are close to the goal. For example, from state\n83, moving down yields 100% success while up, right, and\nleft maintained a high probability of success, but it did not\nguarantee completing the mission.\nConsidering this scenario, when the agent ﬁnds itself in state\n83, and carries out the action to move down, the user could\nask: Why did you move down with the last action? Based on the\nprobabilities of success computed, the agent using a template\ncould respond: I moved down because in doing so, I have a\n100% probability of collecting the shield.\nC. Training for the third high-level task\nIn the third high-level task, the training for the agent had\nas a starting point state 93 and as a goal state 7. In this task,\nthe agent sought to cross the wormhole located in state 7 and\nreturn home. For this task, 20,000 episodes were used with\n100 iterations in each one of the episodes. Of all of the paths\nexplored, the best path found corresponds to the route between\nstates 93, 94, 95, 96, 97, 87, 77, 67, 57, 47, 37, 27, 17, and 7.\nThe probabilities of success computed during the third high-\nlevel task are illustrated in Figure 6. As the agent got closer\nto the goal (state 7), the highest probabilities of success are\nobserved. In this case, three state-action pairs provided 100%\nprobability of success. The three possibilities with which state\n7 was entered are: from state 6 to the right, from state 17\nup, and from state 8 to the left. When one of these actions\nwas executed in these states, the agent would enter into the\nwormhole. That is, it would complete the mission, and the\nspaceship would return home.\nUsing the probabilities of success as a basis, the agent one\nmore time can explain its behavior. For example, from state\n16 and executing the action to move up, it was possible to\nask the agent: Why did you not move to the left in the last\naction? Then, the agent could respond using a template in the\nfollowing way: I did not move to the left because doing so,\nI would have only have a 30% probability of reaching the\nwormhole and of returning home, while moving up I have an\n80% probability of completing the mission successfully.\nD. Global probability of success\nAs demonstrated in the previous experimental results, the\nprobability of success for each one of the high-level tasks was\nobtained. However, this does not allow the agent to show the\nprobability of success for the problem in global terms, but it\nallows showing only one high-level task at a time. Therefore,\nwe computed a matrix for the global probabilities of success\nby averaging the three individual matrices for the probabilities\nof success obtained previously. This matrix is illustrated in\nFigure 7. The matrix for the global probabilities of success,\nunlike the individual matrices, did not represent a particular\nhigh-level task, but it combined the three individual matrices\ntogether in order to obtain the general probabilities of success\nfor the global problem. Through the global probability, we\ncan better understand the agent’s behavior. For example, from\nstates 0, 1, 2, 3, 4, 5, 6, 7, 8 and 9, the action up, always had\n0% probability of success. From states 0, 10, 20, 30, 40, 50,\n60, 70, 80 and 90, the action of moving the agent to the left\nalso had a 0% probability of success. In addition, from states\n9, 19, 29, 39, 49, 59, 69, 79, 89 and 99, the action to the right,\nhad 0% probability of success as well. And ﬁnally, from states\n90, 91, 92, 93 94, 95, 96, 97, 98 and 99, the action down, had\nalso 0% probability of success as the agent was not allowed to\nexit the 10 × 10 grid. Moreover, it could be seen the behavior\nwhen entering the black holes. For example, any action that\nentered states 3, 13, 20, had a 0% probability of success.\nIt is important to highlight that in the global matrix, unlike\nin the individual matrices, no action provided 100% proba-\nbility of success because indeed no state-action pair in this\nscenario guaranteed success. It is important to remember that\nupon entering the wormhole in state 7, it is not guaranteed that\nthe spaceship could escape and return home. The escape only\noccurred if the shield had been collected beforehand. In this\ncase, the success of the mission was reduced to the question:\nUpon entering the wormhole, does the spaceship have the\nshield? If the spaceship does not had the shield, then, the\neffect would be the same as that of a black hole. With respect\nto the global probabilities of success, the same occurred with\nthe actions that made the agent enter state 93 or collect the\nshield. In the global matrix, these actions did not have 100%\nprobability of success since collecting the shield still did not\nguarantee escape. To successfully escape, the spaceship still\nneeded to cross the wormhole and, along the way, it had the\npossibility of falling into a black hole. Overall, although the\nhighest probabilities of success were obtained by completing\nthe different high-level tasks, none reached 100% success, as\npointed out above, as no action could guarantee escape.\nVI. CONCLUSIONS\nIn this work, we have demonstrated that with the memory-\nbased explainable reinforcement learning method using hier-\narchical training, it is possible for a learning agent to learn\nhow to escape a scenario while computing probabilities of\nsuccess to be used for explanations. Being a hierarchical\nscenario, the training was divided in order to complete high-\nlevel tasks. The explainability method was used to compute\nthe global and individual matrices with the probability of\nsuccess. Afterward, these were used as a basis for exploring\nagent’s behavior using explainability. A matrix was used to\nexplain high-level tasks (one matrix was created for each\ntask). However, this only resulted in an explanation of how\nto complete an individual high-level task. To observe the\nagent’s overall behavior, a global matrix was computed with\nthe general probabilities by averaging the matrices obtained\nfor each high-level task. The global matrix showed coherent\nprobabilities of success for the agent when executing an action\nstarting from a speciﬁc state being, therefore, a good basis to\nbe used for generating explanations that can be understood by\nnon-expert users. Future work includes extending our research\ninto more complex environments, especially continuous, in\norder to explore other explainability algorithms, such as the\nlearning-based or introspection-based methods for hierarchical\ntasks.\nACKNOWLEDGMENT\nThis research was partially ﬁnanced by Universidad Cen-\ntral de Chile under the research project CIP2020013, the\nCoordenac¸˜ao de Aperfeic¸oamento de Pessoal de N´ıvel Su-\nperior—Brasil (CAPES)—Finance Code 001, Fundac¸˜ao de\nAmparo a Ciˆencia e Tecnologia do Estado de Pernam-\nbuco (FACEPE), and Conselho Nacional de Desenvolvimento\nCient´ıﬁco e Tecnol´ogico (CNPq)—Brazilian research agen-\ncies.\nREFERENCES\n[1] F. K. Doˇsilovi´c, M. Brˇci´c, and N. Hlupi´c, “Explainable artiﬁcial in-\ntelligence: A survey,” in 2018 41st International convention on infor-\nmation and communication technology, electronics and microelectronics\n(MIPRO).\nIEEE, 2018, pp. 0210–0215.\n[2] J. D. Olden and D. A. Jackson, “Illuminating the “black box”: a\nrandomization approach for understanding variable contributions in\nartiﬁcial neural networks,” Ecological modelling, vol. 154, no. 1-2, pp.\n135–150, 2002.\n[3] F. Cruz, G. Acu˜na, F. Cubillos, V. Moreno, and D. Bassi, “Indirect train-\ning of grey-box models: application to a bioprocess,” in International\nsymposium on neural networks.\nSpringer, 2007, pp. 391–397.\n[4] F. C. Naranjo and G. A. Leiva, “Indirect training with error backpropa-\ngation in gray-box neural model: Application to a chemical process,” in\n2010 XXIX international conference of the Chilean Computer Science\nSociety.\nIEEE, 2010, pp. 265–269.\n[5] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[6] F. Cruz, P. W¨uppen, A. Fazrie, C. Weber, and S. Wermter, “Action\nselection methods in a robotic reinforcement learning scenario,” in 2018\nIEEE Latin American Conference on Computational Intelligence (LA-\nCCI).\nIEEE, 2018, pp. 1–6.\n[7] P. Barros, A. Tanevska, F. Cruz, and A. Sciutti, “Moody learners-\nexplaining competitive behaviour of reinforcement learning agents,” in\n2020 Joint IEEE 10th International Conference on Development and\nLearning and Epigenetic Robotics (ICDL-EpiRob).\nIEEE, 2020, pp.\n1–8.\n[8] K. Frans, J. Ho, X. Chen, P. Abbeel, and J. Schulman, “Meta learning\nshared hierarchies,” arXiv preprint arXiv:1710.09767, 2017.\n[9] D. Gunning, M. Steﬁk, J. Choi, T. Miller, S. Stumpf, and G.-Z.\nYang, “Xai—explainable artiﬁcial intelligence,” Science Robotics, vol. 4,\nno. 37, p. eaay7120, 2019.\n[10] R. Dazeley, P. Vamplew, C. Foale, C. Young, S. Aryal, and F. Cruz,\n“Levels of explainable artiﬁcial intelligence for human-aligned conver-\nsational explanations,” Artiﬁcial Intelligence, vol. 299, p. 103525, 2021.\n[11] R. Dazeley, P. Vamplew, and F. Cruz, “Explainable reinforcement\nlearning for broad-xai: A conceptual framework and survey,” arXiv\npreprint arXiv:2108.09003, 2021.\n[12] S. Milani, N. Topin, M. Veloso, and F. Fang, “A survey of explainable\nreinforcement learning,” arXiv preprint arXiv:2202.08434, 2022.\n[13] A. Heuillet, F. Couthouis, and N. D´ıaz-Rodr´ıguez, “Explainability in\ndeep reinforcement learning,” Knowledge-Based Systems, vol. 214, p.\n106685, 2021.\n[14] F. Cruz, R. Dazeley, and P. Vamplew, “Memory-based explainable\nreinforcement learning,” in Australasian Joint Conference on Artiﬁcial\nIntelligence.\nSpringer, 2019, pp. 66–77.\n[15] F. Cruz, R. Dazeley, P. Vamplew, and I. Moreira, “Explainable robotic\nsystems: Understanding goal-driven actions in a reinforcement learning\nscenario,” Neural Computing and Applications, pp. 1–18, 2021.\n[16] F. Cruz, C. Young, R. Dazeley, and P. Vamplew, “Evaluating human-\nlike explanations for robot actions in reinforcement learning scenarios,”\nin 2022 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS).\nIEEE, 2022, pp. 1–8.\n[17] A. Ayala, F. Cruz, B. Fernandes, and R. Dazeley, “Explainable deep\nreinforcement learning using introspection in a non-episodic task,” arXiv\npreprint arXiv:2108.08911, 2021.\n[18] E. Portugal, F. Cruz, A. Ayala, and B. Fernandes, “Analysis of ex-\nplainable goal-driven reinforcement learning in a continuous simulated\nenvironment,” Algorithms, vol. 15, no. 3, p. 91, 2022.\n[19] M. Zakershahrak and S. Ghodratnama, “Are we on the same page? hier-\narchical explanation generation for planning tasks in human-robot team-\ning using reinforcement learning,” arXiv preprint arXiv:2012.11792,\n2020.\n[20] B. Beyret, A. Shafti, and A. A. Faisal, “Dot-to-dot: Explainable hi-\nerarchical reinforcement learning for robotic manipulation,” in 2019\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS).\nIEEE, 2019, pp. 5014–5019.\n[21] F. Rietz, S. Magg, F. Heintz, T. Stoyanov, S. Wermter, and J. A.\nStork, “Hierarchical goals contextualize local reward decomposition\nexplanations,” Neural Computing and Applications, pp. 1–12, 2022.\n[22] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8,\nno. 3, pp. 279–292, 1992.\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2022-12-14",
  "updated": "2022-12-14"
}