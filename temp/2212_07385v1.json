{
  "id": "http://arxiv.org/abs/2212.07385v1",
  "title": "Quantum Control based on Deep Reinforcement Learning",
  "authors": [
    "Zhikang Wang"
  ],
  "abstract": "In this thesis, we consider two simple but typical control problems and apply\ndeep reinforcement learning to them, i.e., to cool and control a particle which\nis subject to continuous position measurement in a one-dimensional quadratic\npotential or in a quartic potential. We compare the performance of\nreinforcement learning control and conventional control strategies on the two\nproblems, and show that the reinforcement learning achieves a performance\ncomparable to the optimal control for the quadratic case, and outperforms\nconventional control strategies for the quartic case for which the optimal\ncontrol strategy is unknown. To our knowledge, this is the first time deep\nreinforcement learning is applied to quantum control problems in continuous\nreal space. Our research demonstrates that deep reinforcement learning can be\nused to control a stochastic quantum system in real space effectively as a\nmeasurement-feedback closed-loop controller, and our research also shows the\nability of AI to discover new control strategies and properties of the quantum\nsystems that are not well understood, and we can gain insights into these\nproblems by learning from the AI, which opens up a new regime for scientific\nresearch.",
  "text": "Master Thesis\n修士論文\nQuantum Control based on Deep\nReinforcement Learning\n(深層強化学習に基づく量子制御)\nJuly, 2019\n令和元年７月\nZhikang Wang\nワン\n王　\nヅーカン\n智康\nDepartment of Physics, the University of Tokyo\n東京大学大学院　理学系研究科　物理学専攻\narXiv:2212.07385v1  [quant-ph]  14 Dec 2022\nAbstract\nWith the development of quantum experimental techniques in recent years, artiﬁcial\nquantum systems have become more controllable, and the increased controllability has\nopened up a new regime of quantum physics that is both complex and realizable. Espe-\ncially, quantum technology has been brought close to real-world applications, including\nquantum metrology, quantum-limited sensors [1] and quantum computers [2], which are\nof great importance for future technology. However, quantum systems in the real world\nare susceptible to imperfections. They are vulnerable to noise and decoherence, and they\nare often constructed based on approximations, such as ignoring anharmonic factors or\nlong-range interactions. These deﬁciencies are closely related to the fact that, when we\ntake all these tricky factors into consideration, the quantum systems typically become too\ncomplicated to analyse and thus it is hard to ﬁnd the best setting for them. To overcome\nthis diﬃculty, deep reinforcement learning has been proposed as a universal solver to such\nproblems. An artiﬁcial intelligence (AI) technology has the advantage in that it does not\nrequire to explicitly analyse the problem, and that it automatically searches for a good\nsolution through trial and error [3]. On the other hand, deep learning as a new tool\nachieved its success only less than 10 years ago [4] and has been applied to physics over\nthe last 3 years. Probably due to a knowledge gap between physicists and AI technology,\ndeep learning has yet to be applied to many fundamental physical problems and yet to\nprove its eﬃcacy or superior performance to conventional strategies.\nIn this thesis, we consider two simple but typical control problems and apply deep\nreinforcement learning to them, i.e., to cool and control a particle which is subject to\ncontinuous position measurement in a one-dimensional quadratic potential or in a quartic\npotential. We compare the performance of reinforcement learning control and conven-\ntional control strategies on the two problems, and show that the reinforcement learning\nachieves a performance comparable to the optimal control for the quadratic case, and\noutperforms conventional control strategies for the quartic case for which the optimal\ncontrol strategy is unknown. To our knowledge, this is the ﬁrst time deep reinforcement\nlearning is applied to quantum control problems in continuous real space. Our research\ndemonstrates that deep reinforcement learning can be used to control a stochastic quan-\ntum system in real space eﬀectively as a measurement-feedback closed-loop controller,\nand our research also shows the ability of AI to discover new control strategies and prop-\nerties of the quantum systems that are not well understood, and we can gain insights\ninto these problems by learning from the AI, which opens up a new regime for scientiﬁc\nresearch.\ni\nAcknowledgements\nI thank Yuto Ashida and professor Masahito Ueda for discussion on the research. Partic-\nularly, I would like to thank Y. Ashida for bringing my attention to the importance of the\nresearch topic presented here and to thank for professor M. Ueda’s detailed comments on\nboth the research and the writing of this thesis; without them the research would not be\ncarried out and the thesis would not be completed. I also thank for ﬁnancial support by\nthe Global Science Graduate Course (GSGC) program and the support of computational\nresources by the Institute for Physics of Intelligence at the University of Tokyo, which\nhave made the research possible. I am grateful to the Stack Exchange community and\nthe Pytorch community for their valuable comments and technical helps on programming\nand implementation, which have solved myriads of the technical problems that I encoun-\ntered and taught me how to build up the program. I would also like to thank Ziyin\nLiu for discussion on deep learning and programming and thank Ryusuke Hamazaki and\nZongping Gong for discussion on the speciﬁc models investigated in the research, and I\nthank professor Mio Murao for discussion and her valuable advice which have helped me\nout to complete the research for all input cases that are discussed in Chapter 4.\nMy parents and friends have supported and encouraged me along the way in my life\nand in my research, and I wish to express my deepest gratitude to them.\nii\nContents\nAbstract\ni\nAcknowledgements\nii\n1\nIntroduction\n1\n1.1\nBackground . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.1.1\nQuantum Control . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.1.2\nDeep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.2\nCombination of Deep Learning and Quantum Control . . . . . . . . . . .\n4\n1.3\nOutline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2\nContinuous Measurement on Quantum Systems\n7\n2.1\nGeneral Model of Quantum Measurement . . . . . . . . . . . . . . . . . .\n7\n2.2\nContinuous Limit of Measurement . . . . . . . . . . . . . . . . . . . . . .\n9\n2.2.1\nUnconditional State Evolution . . . . . . . . . . . . . . . . . . . .\n9\n2.2.2\nQuantum Trajectory Conditioned on Measurement Outcomes\n. .\n10\n3\nDeep Reinforcement Learning\n16\n3.1\nDeep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.1.1\nMachine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.1.2\nFeedforward Neural Networks . . . . . . . . . . . . . . . . . . . .\n17\n3.1.3\nTraining a Neural Network . . . . . . . . . . . . . . . . . . . . . .\n20\n3.2\nReinforcement Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.2.1\nProblem Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.2.2\nQ-learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.2.3\nImplementation of Deep Q-Network Learning\n. . . . . . . . . . .\n24\n4\nControl in Quadratic Potentials\n26\n4.1\nAnalysis of Control of a Quadratic Potential . . . . . . . . . . . . . . . .\n26\n4.1.1\nGaussian Approximation . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.1.2\nEﬀective Description of Time Evolution . . . . . . . . . . . . . . .\n29\n4.1.3\nOptimal Control\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n4.2\nNumerical Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n4.2.1\nProblem Settings . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n4.2.2\nReinforcement Learning Implementation . . . . . . . . . . . . . .\n43\n4.3\nComparison of Reinforcement Learning and Optimal Control . . . . . . .\n46\n4.3.1\nPerformances . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\niii\nCONTENTS\n4.3.2\nResponse to Diﬀerent Inputs . . . . . . . . . . . . . . . . . . . . .\n48\n4.4\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n5\nControl in Quartic Potentials\n51\n5.1\nAnalysis of a Quartic Potential\n. . . . . . . . . . . . . . . . . . . . . . .\n51\n5.1.1\nBreakdown of Gaussian Approximation and Eﬀective Description\n51\n5.1.2\nApproximate Controls\n. . . . . . . . . . . . . . . . . . . . . . . .\n53\n5.1.3\nBehaviour of a Quartic Potential\n. . . . . . . . . . . . . . . . . .\n56\n5.2\nNumerical Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n5.2.1\nSimulation of the Quantum System . . . . . . . . . . . . . . . . .\n59\n5.2.2\nImplementation of Reinforcement Learning . . . . . . . . . . . . .\n62\n5.3\nComparison of Reinforcement Learning and Conventional Control . . . .\n63\n5.4\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n6\nConclusions and Future Perspectives\n65\nAppendix A Linear-Quadratic-Gaussian Control\n68\nA.1 Deterministic Linear-Quadratic Control . . . . . . . . . . . . . . . . . . .\n68\nA.1.1\nProblem Setting and the Quadratic Optimal Cost . . . . . . . . .\n68\nA.1.2\nThe Hamilton-Jacobi-Bellman Equation and the Optimal Control\n71\nA.2 Linear-Quadratic Control with Gaussian Noise . . . . . . . . . . . . . . .\n74\nA.2.1\nDeﬁnitions of the Optimal Control and Control Cost\n. . . . . . .\n74\nA.2.2\nThe LQG Optimal Control . . . . . . . . . . . . . . . . . . . . . .\n75\nAppendix B Numerical Simulation of Stochastic Diﬀerential Equation\n77\nB.1 Stochastic Itˆo–Taylor Expansion . . . . . . . . . . . . . . . . . . . . . . .\n77\nB.2 Order 1.5 Strong Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\nAppendix C Details of the Reinforcement Learning Algorithm\n86\nC.1 Extensions of DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\nC.1.1\nTarget Network and ϵ-Greedy Strategy . . . . . . . . . . . . . . .\n86\nC.1.2\nDouble Q-Learning . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\nC.1.3\nDueling DQN Structure\n. . . . . . . . . . . . . . . . . . . . . . .\n88\nC.1.4\nPrioritized Memory Replay . . . . . . . . . . . . . . . . . . . . . .\n88\nC.1.5\nNoisy DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\nC.1.6\nMulti-Step Learning\n. . . . . . . . . . . . . . . . . . . . . . . . .\n89\nC.1.7\nDistributional Learning . . . . . . . . . . . . . . . . . . . . . . . .\n90\nC.2 Hyperparameter Settings . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\nC.2.1\nGradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\nC.2.2\nPrioritized Replay Settings . . . . . . . . . . . . . . . . . . . . . .\n91\nC.2.3\nOther Reinforcement Learning Settings . . . . . . . . . . . . . . .\n91\nBibliography\n93\niv\nChapter 1\nIntroduction\nIn this chapter, we give a background review on the current developments of quantum\ncontrol and deep learning in Section 1.1, and then introduce the motivation and content\nof our research in Section 1.2, and ﬁnally present the outline of this thesis in Section 1.3.\n1.1\nBackground\n1.1.1\nQuantum Control\nIn general, a control problem is to ﬁnd a control protocol that maximizes a score or\nminimizes a cost which encodes a prescribed target of control. The control as an output\nfrom the controller is a time sequence which inﬂuences evolution of the controlled system.\nWhen the evolution of the system is deterministic, the problem can be formulated as [5]\nu∗= argmin\nu\n\u0014Z T\n0\nL(u, x, t) dt + LT(u, x)\n\u0015\n,\ndx = f(x, u)dt,\n(1.1)\nwhere u is the control variable, L stands for the loss, T is the total time considered in\nthis control, LT is a speciﬁc loss for the ﬁnal state, f represents the rule of time evolution\nof the system, and the goal of a control problem is to ﬁnd this u∗, which is called the\noptimal control, and x and u depend on time t. If u does not use information obtained\nfrom the controlled system x during its control, it is called open-loop control [6]; if it\ndepends on information obtained from the controlled system, it is called closed-loop con-\ntrol, or feedback control. The feedback control necessarily involves measurement, and\ntherefore it is, in fact, the measurement-feedback control in the quantum case. For clas-\nsical systems, the control problem can often be solved rather straightforwardly, while for\nquantum systems this is not the case. The main reason lies in the complexity of descrip-\ntion of the controlled system. To describe a classical system, a few numbers as relevant\nphysical quantities suﬃce, while for a quantum system, we need many more parameters\nto describe the superposition among diﬀerent components, and if it is a many-body state\nwe even need exponentially many parameters. This diﬃculty inhibits straightforward\nanalysis to solve quantum control problems, except for a few cases where the quantum\n1\nChapter 1. Introduction\nsystem can be simpliﬁed.\nThis quantum control problem did not attract as much attention as it deserves until\nthe development of controllable artiﬁcial quantum systems in the last two decades [7].\nExamples of the controllable systems are quantum dots in semiconductors and photonic\ncrystals, trapped ion systems, superconducting qubits, optical lattices, nitrogen-vacancy\n(NV) centers, coupling optical systems, cavity optomechanical systems and so on [2,8–14].\nThey are used as platforms for simulation of quantum dynamics or considered as poten-\ntial candidates for quantum computation devices.\nHowever, none of them is perfect.\nThey always come with sources of noise and contain small error terms that cannot be\ncontrolled in a straightforward manner. For example, superconducting qubits use the\nlowest two energy levels of its small superconducting circuit as the logical |0⟩and |1⟩\nstates for quantum computation, but there is always a non-zero probability for the state\nto jump to energy levels higher than |0⟩and |1⟩and to go out of control. To ﬁnd a control\nstrategy to suppress such problems, typically approximations and assumptions are made\nto simplify the situations, often involving use of perturbative expansions or reduced sub-\nspace eﬀective Hamiltonians, and then a control is calculated. For the superconducting\nqubit example, the control pulses are optimized so that one or two nearest energy levels\nabove the operating qubit levels are suppressed. Concerning decoherence, the dynamical\ndecoupling control is a good example, which considers the fast limit of control and series\nexpansion [15]. Another example could be the spin echo control technique, which specif-\nically deals with time-independent inhomogeneous imperfections [16]. All these methods\ncome along with important assumptions and approximations. Moreover, it should be\nmentioned that usually the analysis does not straightforwardly give the optimal control\nas deﬁned in Eq. (1.1), but only gives some hints so that reasonable control protocols\ncan be designed by hand.\nWhen the above analysis-based method does not work, numerical search algorithms\nprovide an alternative. They assume that the control is a series of pulses, or a superpo-\nsition of trigonometric waves, which can be parametrized with a sequence of parameters,\nand they consider small variations of the control parameters to get closer to the control\ntarget, such as ﬁdelity, and then they do it iteratively to gradually modify the control pa-\nrameters. These algorithms include QOCT [17], CRAB [18] and GRAPE [19]. However,\nsince these methods are either eﬀectively or directly based on gradients, if the situation\nis complicated, they can easily be trapped in local optima and do not give satisfactory\nsolutions, as exempliﬁed in Ref. [20]. Nevertheless, they have been shown to be useful\nin many simple practical scenarios where no other means are available to ﬁnd a control,\neven including simple chaotic systems [21]. One weakness of these methods is that they\ncan only be used as open-loop controls, for which the starting point and the endpoint\nof control are prescribed beforehand. Actually it is almost impossible to optimize the\ncontrol and at the same time make it conditioned on all types of measurement outcomes.\nOn the whole, it can be recognized that there is no universal approach to quantum\ncontrol, and most of the current methods are ad hoc for speciﬁc situations. Therefore, if\nthe controlled system becomes more complicated and involved, it would be much more\ndiﬃcult to ﬁnd a satisfactory control using the above strategies, and it is desirable if\n2\nChapter 1. Introduction\n⇒\nFigure 1.1: Working of a deep learning system.\nIt learns by ﬁtting its internal connection\nparameters (weights and biases) into the given data-answer pairs, such that the computation\nof the network precisely relates every data to its corresponding answer for the whole training\ndataset that it learns. After training, it is used to predict something on new data.\nwe can ﬁnd some general and better strategy to overcome the diﬃculty of analysing a\ncomplex system in order to obtain a control.\n1.1.2\nDeep Learning\nDeep learning, namely machine learning with deep neural networks, has become popular\nand used extensively in recent years, especially for tasks that were previously considered\ndiﬃcult for AI to do. For instance, deep learning has established new records on many\nproblem-solving contests [22, 23] and defeated human champions in games [24], and it\nis still being researched and developing rapidly. Deep learning will be introduced more\nformally and in detail in Chapter 3, and therefore we only give a brief introduction here\nto present the general idea.\nGenerally speaking, deep learning uses a deep neural network as a powerful function\napproximator that can learn patterns of previous data to give predictions on new data,\nas illustrated in Fig. 1.1. It learns by modifying its internal parameters to ﬁt given data-\nanswer pairs as training data. Due to the complexity and universality of deep neural\nnetworks, it has turned out that this simple learning procedure can make the neural\nnetwork correctly learn various complex relations between the data and the answer. For\nexample, based on this simple method, it can be used to recognize objects, modify images\nand do translation [25–27], and evaluate the advantages and disadvantages in chess [24].\nIt is generally believed that neural networks can almost learn any functions, as long as\nfunctions are suﬃciently smooth and do not appear to be pathological.\nHowever, as\na drawback, deep learning always gives approximate solutions and does not explain its\nreason. Deep learning is typically not precise, and is generally a blackbox technique which\nwe cannot explain well so far [28].\n3\nChapter 1. Introduction\n1.2\nCombination of Deep Learning and Quantum Con-\ntrol\nTo use deep learning for a control problem, the reinforcement learning scheme needs to\nbe implemented, and it will be explained in detail in Chapter 3. Reinforcement learning\nwith deep learning uses a neural network to evaluate which control option is good and\nwhich control option is bad during the control, and it learns by exploring its environment,\nwhich stands for the controlled system behaviour. It explores its environment to accumu-\nlate experience, and it learns the accumulated experience, and its goal is set to maximize\nthe control target when making control decisions. Overall, it learns and explores diﬀer-\nent possibilities of its environment automatically, and can learn underlying rules of the\nenvironment and often avoid local optima. Therefore, it can be seen as an alternative\nto the gradient-based quantum control algorithms in Section 1.1.1. One advantage of\nreinforcement learning based control is that, it can deal with both open-loop control and\nclosed-loop control in the same way. Since the neural network needs to take information\nabout the controlled system to give a control output, it does not matter if the controlled\nsystem is changed suddenly due to measurement-backaction: if the system changed, the\ncontrol output from the neural network is also changed and that is all. The versatility\nof AI makes all kinds of control scenarios possible without the need for human design,\nwhich is diﬃcult with only conventional methods.\nFigure 1.2: The classical cart-pole system and the controlled inverted potential quantum system\nwith measurement. In either case the system is unstable, and the particle tends to fall oﬀfrom\nthe center. The target of control is to move the cart, or apply an external force, so that the\nparticle stays at the center. For the quantum case, position measurement is necessary to prevent\nthe wavefunction from expanding, and at the same time serves as a source of random noise.\nExisting researches on deep-reinforcement-based quantum control are not many, and\nalmost all of them only consider discrete systems composed of spins and qubits, and\nmostly focus on error correction or noise-resistant manipulation under some noise mod-\nels [29–31], which are clearly for practical purposes. Also, most of them only involve\ndeterministic evolution of the states. In our research, we consider a system in continu-\nous position space subject to measurement, which is yet to be investigated, and we use\ndeep reinforcement learning to control the system and compare its control strategy with\nexisting conventional controls to gain insight into what is learned by the AI and how it\nmay outperform existing methods.\n4\nChapter 1. Introduction\nSpeciﬁcally, we consider a particle in a 1D quadratic potential under continuous posi-\ntion measurement. The measurement introduces stochasticity into the system, and makes\nthe system more realistic. When the potential of the system is upright, i.e. its minimum\nlies at the center, the system is just a usual harmonic oscillator; when the potential is\ninverted, the system is essentially an inverted pendulum and becomes analogous to the\nstandard cart-pole system [32] which is a benchmark for reinforcement learning control\n(Fig. 1.2). In the former case, the target of control is set to cool down the harmonic\noscillator, which is ground-state cooling and is important as a real problem in experi-\nments [33]. In the latter case, the target of control is to keep the particle at the center\nof the potential, which amounts to stabilizing the unstable system, and in both cases,\nthe controller uses an external force exerted on the particle to control it. We train a\nneural network following the strategy of reinforcement learning, and compare its per-\nformance on the two tasks with the performance of the optimal control obtained from\nthe linear-quadratic-Gaussian (LQG) control theory [5]. Next, we extend the problem\nto an anharmonic setting by changing the potential to be quartic, and repeat the above\nprocedure. For this case, an optimal control strategy is not known, and therefore we\nuse suboptimal control strategies and Gaussian approximations and the local approxima-\ntion of the linear control to derive several control protocols from a conventional point of\nview, and we compare their performances with the reinforcement learning control. We\nalso compare the behaviour of the controls by looking at their outputs, and we discuss\nthe properties of the underlying quantum systems to gain insights on the controllers’\nbehaviour.\n1.3\nOutline\nThe present thesis is organized as follows.\nIn Chapter 2, we present a review of continuous measurement on quantum systems.\nWe give the formulation of a general measurement, and formally derive the stochastic dif-\nferential equations that govern the evolution of a quantum state subjected to continuous\nmeasurement, where the evolution is called a quantum trajectory. We discuss both jump\nand diﬀusive trajectories, and give the equation that is used in our investigated control\nproblem.\nIn Chapter 3, we present a review of deep reinforcement learning. We start from the\nbasics of machine learning and introduce deep learning with its motivation and uses, and\nintroduce reinforcement learning, especially a particular type of reinforcement learning\ncalled Q-learning, which is used in our research. Finally we discuss the implementation\nof deep learning for a reinforcement learning problem, i.e. deep reinforcement learning.\nIn Chapter 4, we describe the quadratic control problems that are introduced in the\nlast section. We ﬁrst analyse the problems to show that they can be solved by the stan-\ndard LQG control, and then describe our problem setting and our learning system in\ndetail, and we present the results of the reinforcement learning and those of the optimal\n5\nChapter 1. Introduction\ncontrol. We compare the results, and also directly compare the output from the deep\nlearning system with that from the optimal control. We ﬁnd that, both ﬁnal performances\nand the control behaviours of the two are similar, which implies that the AI correctly\nlearned the optimal control. There also exist small traces of imperfections concerning the\nAI’s behaviour. We will make discussions on these results.\nIn Chapter 5, we describe the quartic anharmonic control problems. We follow the\nsame line of reasoning as the quadratic case and show that this quartic case cannot be\nsimpliﬁed in the same way as the quadratic one, and the system exhibits intrinsic quantum\nmechanical behaviour that cannot be modelled classically. We then discuss possible con-\ntrol strategies based on existing ideas and compare their performances with our trained\nreinforcement learning controllers, and organise the results and discussions in the same\nway as Chapter 4. We ﬁnd that when properly conﬁgured, the reinforcement learning\ncontroller could outperform all of our derived control strategies, which demonstrates the\nsupremacy and the universality of reinforcement learning.\nIn Chapter 6, we discuss the conclusions of this thesis and their implications, and we\ndiscuss the future perspectives.\nSome technical details are discussed in appendices. Appendix A reviews the linear-\nquadratic-Gaussian (LQG) control theory that is used in Chapters 4 and 5. Appendix B\nexplains the numerical methods implemented in our numerical simulation of the quantum\nsystems. Appendix C presents detailed adopted techniques and conﬁgurations of our\nreinforcement learning algorithm.\n6\nChapter 2\nContinuous Measurement on\nQuantum Systems\nIn this chapter, we review the formulation of quantum measurement and its continuous\nlimit. We review the indirect measurement model in Section 2.1, and in Section 2.2 we\ndiscuss continuous measurement of the diﬀusive type, which describes the position mea-\nsurement that we apply to the quantum system in our research in Chapter 4 and 5.\n2.1\nGeneral Model of Quantum Measurement\nIn the postulates of quantum mechanics [34], a general measurement is described by a\nset of linear operators {Mm}, with m denoting measurement outcomes. These operators\nact on the measured quantum system state space and satisfy the completeness condition\nX\nm\nM †\nmMm = I\n(2.1)\nsuch that the unconditioned post-measurement quantum state as a sum over measure-\nment outcomes is trace-preserved:\nρ′ = E (ρ) =\nX\nm\nMmρM †\nm\n⇒\ntr (ρ′) = tr\n \nρ\nX\nm\nM †\nmMm\n!\n= tr(ρ),\n(2.2)\nwhere ρ is the measured quantum state and MmρM †\nm represents the state after a mea-\nsurement outcome m is observed. This condition of trace preservation ensures that the\ntotal probability of all measurement outcomes is one. To obtain a normalized state after\na certain measurement outcome, it is divided by its outcome probability and becomes\nρm =\nMmρM †\nm\ntr\n\u0010\nMmρM †\nm\n\u0011 ,\n(2.3)\nwhere the trace tr\n\u0000MmρM †\nm\n\u0001\nis the probability of measurement outcome m for state ρ.\n7\nChapter 2. Continuous Measurement on Quantum Systems\nU\nρe\nρ\nm\nMmρM †\nm\nFigure 2.1: Indirect measurement model. We let a meter interact with a measured quantum\nstate, and we measure the meter to obtain a direct measurement result m.\nThe simplest and standard measurement is projection measurement {Pm}, satisfying\nEq. (2.1) and\n∀m,\n∀n ∈Z+,\n(Pm)n = Pm,\nP †\nm = Pm\n(2.4)\nand\nPiPj = δijPi,\nδij =\n\u001a 0\nif i ̸= j;\n1\nif i = j,\n(2.5)\nsuch that they are projectors. These projector properties ensure that after a measurement\noutcome m is obtained, if you measure it again immediately, the measurement outcome\nmust again be m and the state is not changed. This is the simplest and basic quantum\nmeasurement we have. Now we show it is possible to extend the projection measurement\nto a general measurement {Mm} as in equations (2.1) to (2.3) by using an indirect mea-\nsurement scheme.\nSuppose we want to measure a state ρ. We prepare a meter state ρe which is known\nand not entangled with ρ, and let it interact with ρ through a unitary evolution U, and\nthen we measure the state of the meter using the projection measurement as schematically\nillustrated in ﬁgure 2.1. For a measurement outcome m on the meter, the unnormalized\npost-measurement state of the initial ρ becomes\n˜ρm = tre\n\u0000(I ⊗Pm)U(ρ ⊗ρe)U †(I ⊗Pm)\n\u0001\n= tre\n\u0000(I ⊗Pm)U(ρ ⊗ρe)U †\u0001\n.\n(2.6)\nFor simplicity, we assume ρe is pure, i.e.\nρe = |ψe⟩⟨ψe|, and we decompose Pm into\nPm = P\ni |ψi⟩⟨ψi|. The above result can be written as\n˜ρm =\nX\ni⟨ψi|U (|ψe⟩⟨ψe| ⊗ρ) U †|ψi⟩.\n(2.7)\nIf we deﬁne Mm,i ≡⟨ψi|U|ψe⟩, it becomes\n˜ρm =\nX\ni\nMm,i ρM †\nm,i ,\n(2.8)\nwhich can be considered as a measurement operator set {M(m,i)} with measurement\noutcomes (m, i) as in equations (2.1) to (2.3), and we discard information on index i.\nIf the projector Pm only projects into one basis |ψm⟩⟨ψm|, i.e. i only has one choice,\n8\nChapter 2. Continuous Measurement on Quantum Systems\nthen it results in the measurement operator set {Mm}, and ρm =\nMmρM †\nm\ntr\n\u0010\nMmρM †\nm\n\u0011 . The\ncompleteness condition (2.1) can be deduced from the completeness of projectors {Pm}.\nConversely, for a given set of measurement operators {Mm} that satisfy completeness\ncondition (2.1), there exists a unitary U such that ⟨ψm|U|ψe⟩= Mm, and it allows us\nto implement the measurement {Mm} through an indirect measurement with a direct\nprojection measurement on the meter [34].\n2.2\nContinuous Limit of Measurement\n2.2.1\nUnconditional State Evolution\nWe consider the continuous limit of repeated measurements in inﬁnitesimal time. When\nthe measurement outcomes are not taken into account, the state evolution is determinis-\ntic, as ρ →E(ρ) for each measurement done. We require this deterministic evolution to\nbe continuous, that is\n0 <\n\r\r\r\r lim\ndt→0\nE(ρ) −ρ\ndt\n\r\r\r\r < ∞,\nE(ρ) =\nX\nm\nMmρM †\nm,\n(2.9)\nwhere E necessarily depends on dt.\nWe ﬁrst consider a binary measurement {M0, M1} with two measurement outcomes.\nDue to the requirements P\nm M †\nmMm = I and (E(ρ) −ρ) ∼dt, we set M0 = I −R dt\n2\nsuch\nthat\nM0ρM †\n0 = (I −R dt)ρ\n(2.10)\nwith M1 satisfying\nM1 = L1\n√\ndt,\nL†\n1L1 = R\n(2.11)\nfor the condition P\nm M †\nmMm = I. In this way, all requirements for a continuous measure-\nment are satisﬁed [35]. Then similarly, we may add more operators Mi into the operator\nset {Mm}, and they satisfy\nMi = Li\n√\ndt,\ni = 1, 2, · · · , m ,\nM0 = I −dt\n2\nm\nX\ni=1\nL†\niLi,\n(2.12)\nwhich produce the Lindblad equation\ndρ\ndt = −i\nℏ[H, ρ] +\nX\ni\nγi\n\u0012\nLiρL†\ni −1\n2{L†\niLi, ρ}\n\u0013\n,\n(2.13)\n9\nChapter 2. Continuous Measurement on Quantum Systems\nwhere a self-evolution term [H, ρ] with Hamiltonian H is taken into account, {·, ·} is the\nanticommutator, and γi characterizes the strength of measurement. The above results\ncan also be derived from the indirect measurement model by a repetition of a week uni-\ntary interaction between the state and the meter followed by a projection measurement\non the meter [36].\n2.2.2\nQuantum Trajectory Conditioned on Measurement Out-\ncomes\nWhen the measurement outcomes of a continuous measurement are observed, the quan-\ntum state conditioned on the outcomes follows a quantum trajectory. This quantum\ntrajectory can be considered as a stochastic process, and it is not necessarily continuous.\nThe probability to get a measurement outcome i in an inﬁnitesimal measurement in\ntime dt is\npi(dt) = tr(LiρL†\ni) dt ,\n(2.14)\nwhich vanishes with dt. Therefore, in an inﬁnitesimal length of time, measurement out-\ncomes other than the outcome 0 can only appear with vanishingly small probabilities.\nWe therefore take the limit that all measurement outcomes are sparse in time except for\nthe outcome 0, that is, two or more of them do not occur in the same inﬁnitesimal time\ninterval in dt. If we denote the number of measurement outcome i in dt as dNi(t), they\nobey the following:\ndNi(t) = 0 or 1,\ndNidNj = δij dNi,\nE [dNi(t)] = tr\n\u0010\nLiρ(t)L†\ni\n\u0011\ndt .\n(2.15)\nWe now write the stochastic diﬀerential equation with these random variables to describe\nthe state evolution, conditioned on these measurement outcomes [35]. For a pure state\n|ψ⟩, we have\nd|ψ(t)⟩=\n\nX\ni\ndNi(t)\n\n\nLi\nq\n⟨L†\niLi⟩\n−1\n\n+ [1 −\nX\ni\ndNi(t)]\n\n\nM0\nq\n⟨M †\n0M0⟩\n−1\n\n\n\n|ψ(t)⟩\n=\n\nX\ni\ndNi(t)\n\n\nLi\nq\n⟨L†\niLi⟩\n−1\n\n+ 1 ·\n\n\n1 −dt\n2\nPm\ni=1 L†\niLi\nq\n⟨1 −dt Pm\ni=1 L†\niLi⟩\n−1\n\n\n\n|ψ(t)⟩\n=\n\nX\ni\ndNi(t)\n\n\nLi\nq\n⟨L†\niLi⟩\n−1\n\n−dt\n \n1\n2\nm\nX\ni=1\nL†\niLi −1\n2\nm\nX\ni=1\n⟨L†\niLi⟩\n!\n|ψ(t)⟩,\n(2.16)\nwhere the term [1 −P\ni dNi(t)] is replaced by 1 due to few non-zero events of dNi. When\n10\nChapter 2. Continuous Measurement on Quantum Systems\nthe state Hamiltonian is taken into account, Eq. (2.16) reduces to\nd|ψ(t)⟩=\n\nX\ni\ndNi(t)\n\n\nLi\nq\n⟨L†\niLi⟩\n−1\n\n−dt\n \ni\nℏH + 1\n2\nm\nX\ni=1\nL†\niLi −1\n2\nm\nX\ni=1\n⟨L†\niLi⟩\n!\n|ψ(t)⟩.\n(2.17)\nThis is called a nonlinear stochastic Schr¨odinger equation (SSE). For a general mixed\nstate, the equation is\ndρ = −dt i\nℏ(Heﬀρ −ρH†\neﬀ) + dt\nm\nX\ni=1\n⟨L†\niLi⟩ρ +\nm\nX\ni=1\ndNi\n \nLiρL†\ni\n⟨L†\niLi⟩\n−ρ\n!\n,\n(2.18)\nHeﬀ:= H −iℏ\n2\nm\nX\ni=1\nL†\niLi.\n(2.19)\nNote that the expectation value ⟨·⟩depends on the current state ρ(t) or |ψ(t)⟩, and it\nintroduces nonlinearity regarding the state by the ⟨·⟩ρ and ⟨·⟩|ψ(t)⟩terms.\nPhysically, when the operators Li are far from the identity and change the quantum\nstate much, the rare non-zero dNi events are called quantum jumps, meaning that the\nstate changes suddenly during its evolution. At another limit in which the operators Li\nare close to the identity and non-zero dNi occurs more frequently, the state can evolve\nsmoothly. This is called the diﬀusive limit, and there are multiple ways to achieve this\nlimit. If we have discrete measurement outcomes 0, 1, 2 such that\nL1 =\nr\nΓ\n2 (I + l ˆa),\nL2 =\nr\nΓ\n2 (I −l ˆa),\nM0 = I −Γl2\n2 ˆa†ˆa dt ,\n(2.20)\nl →0,\nΓ →∞,\nΓl2 = γ\n2,\n(2.21)\nwhere γ is a constant. In this case, the frequency of measurement outcome i = 1, 2\nbecomes high:\nE [δN1,2(t)] = tr\n\u0010\nL1,2 ρL†\n1,2\n\u0011\ndt · δt\ndt = Γ\n2\n\u00001 ± l⟨ˆa + ˆa†⟩+ l2⟨ˆa†ˆa⟩\n\u0001\nδt\n≈Γ\n2\n\u00001 ± l⟨ˆa + ˆa†⟩\n\u0001\nδt,\n(2.22)\nwhere δNi(t) denotes the number of measurement outcome i in a small time interval δt,\nand we assume that the state does not change much during this interval. Under this\nassumption, δNi(t) is a Poisson distribution for the interval δt, and therefore at Γ →∞,\nit is non-negligible and can be approximated to be Gaussian:\nδN1,2(t) = Γ\n2\n\u00001 ± l⟨ˆa + ˆa†⟩\n\u0001\nδt +\nr\nΓ\n2\n\u0012\n1 ± l\n2⟨ˆa + ˆa†⟩\n\u0013\nδW1,2(t),\n(2.23)\n11\nChapter 2. Continuous Measurement on Quantum Systems\nδW1,2(t) ∼N\n\u0010\n0,\n√\nδt\n2\u0011\n,\n(2.24)\nwhere Γδt is large, and δWi(t) is a Wiener increment satisfying δWiδWj = δijδt. To\nproceed, we ﬁrst calculate dρ:\ndρ = −dt i\nℏ(Heﬀρ −ρH†\neﬀ) + dt\n2\nX\ni=1\n⟨L†\niLi⟩ρ +\n2\nX\ni=1\ndNi\n \nLiρL†\ni\n⟨L†\niLi⟩\n−ρ\n!\n= −dt i\nℏ[H, ρ] −dtΓ\n2\n\b\nI + l2ˆa†ˆa, ρ\n\t\n+ Γ\n\u00001 + l2⟨ˆa†ˆa⟩\n\u0001\ndt ρ\n+ dN1\n\u0012ρ + lˆaρ + lρˆa† + l2ˆaρˆa†\n1 + l⟨ˆa + ˆa†⟩+ l2⟨ˆa†ˆa⟩−ρ\n\u0013\n+ dN2\n\u0012ρ −lˆaρ −lρˆa† + l2ˆaρˆa†\n1 −l⟨ˆa + ˆa†⟩+ l2⟨ˆa†ˆa⟩−ρ\n\u0013\n= −dt i\nℏ[H, ρ] −dtΓl2\n2\n\b\nˆa†ˆa, ρ\n\t\n+ Γl2⟨ˆa†ˆa⟩dt ρ\n+ dN1\n\u0000lˆaρ + lρˆa† + l2ˆaρˆa† −l⟨ˆa + ˆa†⟩ρ + l2⟨ˆa + ˆa†⟩2ρ −l2⟨ˆa†ˆa⟩ρ −l2⟨ˆa + ˆa†⟩(ˆaρ + ρˆa†)\n\u0001\n+ dN2\n\u0000−lˆaρ −lρˆa† + l2ˆaρˆa† + l⟨ˆa + ˆa†⟩ρ + l2⟨ˆa + ˆa†⟩2ρ −l2⟨ˆa†ˆa⟩ρ −l2⟨ˆa + ˆa†⟩(ˆaρ + ρˆa†)\n\u0001\n= −dt i\nℏ[H, ρ] −dtΓl2\n2\n\b\nˆa†ˆa, ρ\n\t\n+ Γl2⟨ˆa†ˆa⟩dt ρ + (dN1 −dN2)\n\u0000lˆaρ + lρˆa† −l⟨ˆa + ˆa†⟩ρ\n\u0001\n+ (dN1 + dN2)(l2ˆaρˆa† −l2⟨ˆa†ˆa⟩ρ −l2⟨ˆa + ˆa†⟩(ˆaρ + ρˆa†) + l2⟨ˆa + ˆa†⟩2ρ),\n(2.25)\nwhere we have expanded the denominators up to O(l2). To accumulate a total of δt\ndt →∞\nsteps, we substitute Eq. (2.23) into the above. It can be checked easily that most of the\nterms are cancelled and to the leading order in l it becomes\nδρ = −δt i\nℏ[H, ρ] −δtΓ\n2\nm\nX\ni=1\n\u0002\n−2l2ˆaρˆa† + l2{ˆa†ˆa, ρ}\n\u0003\n+\n√\nΓl\nm\nX\ni=1\n\u0002\n(ˆa −⟨ˆa⟩)ρ + ρ(ˆa† −⟨ˆa†⟩)\n\u0003\nδW\n=\n\"\n−i\nℏ[H, ρ] −γ\n4\nm\nX\ni=1\n\u0000{ˆa†ˆa, ρ} −2ˆaρˆa†\u0001\n#\nδt +\nrγ\n2\nm\nX\ni=1\n\u0002\n(ˆa −⟨ˆa⟩)ρ + ρ(ˆa† −⟨ˆa†⟩)\n\u0003\nδW,\n(2.26)\nwhere the rule of calculation follows the Itˆo calculus, and we have used a single Wiener\nincrement δW to represent the term (δN1 −δN2). The above equation shows that our\ninitial assumption that ρ does not change much during a suﬃciently small time interval\nδt is true, as diverging quantities are cancelled and only non-diverging γ terms before δt\nand δW remain, which scales with the length of a chosen time interval δt.1 Rewriting δt\nas dt, we obtain the ﬁnal result\n1As can be seen from our derivation, the function before δW should be evaluated at time t but not at\ntime t + δt\n2 . This point is crucial in stochastic calculus, and in this case it is called Itˆo calculus. Another\ncaveat is that the stochastic diﬀerential equations converge in\n√\ndt rather than dt, which is diﬀerent\nfrom usual diﬀerential equations and is important when we try to prove the above result from a rigorous\nmathematical point of view.\n12\nChapter 2. Continuous Measurement on Quantum Systems\nFigure 2.2: Setup of homodyne detection. An intense laser ﬁeld (called a local oscillator) is\nsuperposed with a measured signal through a beam splitter. The two output ﬁelds are detected\nby two detectors which convert incident photons into electric currents. The electric currents are\nthen fed into a balanced detector. Reproduced from Ref. [37].\ndρ =\n\"\n−i\nℏ[H, ρ] −γ\n4\nm\nX\ni=1\n\u0000{ˆa†ˆa, ρ} −2ˆaρˆa†\u0001\n#\ndt+\nrγ\n2\nm\nX\ni=1\n\u0002\n(ˆa −⟨ˆa⟩)ρ + ρ(ˆa† −⟨ˆa†⟩)\n\u0003\ndW,\n(2.27)\nwhere dW ∼N\n\u0010\n0,\n√\ndt\n2\u0011\n. For a pure state it is\nd|ψ⟩=\n\"\n−i\nℏH −γ\n4\nm\nX\ni=1\n\u0012\nˆa†ˆa −ˆa⟨ˆa + ˆa†⟩+ 1\n4⟨ˆa + ˆa†⟩2\n\u0013#\ndt|ψ⟩\n+\nrγ\n2\nm\nX\ni=1\n\u0012\nˆa −1\n2⟨ˆa + ˆa†⟩\n\u0013\ndW|ψ⟩.\n(2.28)\nIn real experiments, the above model describes a homodyne detection using a strong\nlocal oscillator, and dN1−dN2 is the observed signal as a diﬀerential photocurrent, which\nis illustrated in Fig. 2.2, which has a mean value of Γl⟨ˆa+ˆa†⟩and a standard deviation of\n√\nΓdt regarding the measured signal. The random variable dW represents the observed\nsignal deviating from its mean value. This experimental setting explains why we need\ntwo measurement outcomes dN1 and dN2. If we only take one measurement outcome and\nblock either branch of the light that goes out from the beam splitter, it comes back to\nthe source signal and disturbs the Hamiltonian of the measured system drastically.\nIn the cases where measurement outcomes are not discrete but are real-valued, such as\nthe direct position measurement, the measured system can also have a diﬀusive quantum\ntrajectory, to which the above analysis does not apply. In such cases, we may assume the\nmeasurements are weak and performed repeatedly, so that they are averaged to obtain\n13\nChapter 2. Continuous Measurement on Quantum Systems\na total measurement, and then according to the central limit theorem, we can approxi-\nmately describe the measurement eﬀects and results as a Gaussian random process:\nMq =\n\u0012γdt\nπ\n\u0013 1\n4\ne−γdt\n2 (ˆx−q)2,\n(2.29)\nwhere we have assumed that the measured physical quantity is the position of a particle.\nNote that the completeness condition is automatically satisﬁed by the property of the\nGaussian integral:\nZ ∞\n−∞\nM †\nqMq dq = 1 .\n(2.30)\nTherefore it is a valid measurement. In this case, measurement outcomes are by them-\nselves Gaussian and can be modelled as\n\u0012\n⟨ˆx⟩+\ndW\n√2γdt\n\u0013\n, which is given in Ref. [38]. The\ndeduction follows by explicitly calculating MqρM †\nq by a straightforward expansion, and\nthe ﬁnal result is exactly the same as before, provided that ˆa and ˆa† are replaced by ˆx.\nThe result for a pure state is\nd|ψ⟩=\n\u0014\u0012\n−i\nℏH −γ\n4(ˆx −⟨ˆx⟩)2\n\u0013\ndt +\nrγ\n2(ˆx −⟨ˆx⟩)dW\n\u0015\n|ψ⟩,\n(2.31)\nwhere dW is a Wiener increment as before. This is the equation that we use in the\nsimulation of quantum systems in our research. If we express it in terms of the density\nmatrix that admits a mixed state, the equation becomes\ndρ = −i\nℏ[H, ρ]dt −γ\n4[ˆx, [ˆx, ρ]]dt +\nrγ\n2{ˆx −⟨ˆx⟩, ρ}dW.\n(2.32)\nIn order to model incomplete information on measurement outcomes, we deﬁne a mea-\nsurement eﬃciency parameter η, satisfying 0 ≤η ≤1, which represents the ratio of mea-\nsurement outcomes that are obtained. In the above equations, measurement outcomes\nare represented by a Wiener increment dW, which can be considered as an accumulated\nvalue in a small time interval. Therefore, we rewrite the Wiener increment into a series\nof smaller Wiener increments which represent repeated weak measurement results, and\nzero out a portion of those results to obtain a total incomplete measurement outcome, i.e.,\ndW =\nN\nX\ni=1\ndWi,\ndWi ∼N\n\u0012\n0, dt\nN\n\u0013\n,\n(2.33)\nwhere we have discretized the time in units of dt into N steps to obtain N Weiner incre-\nments. The original condition dW ∼N(0, dt) is clearly satisﬁed. Then after removing a\nportion 1 −η of the measurement results, the total dW becomes\ndW =\nηN\nX\ni=1\ndWi,\ndWi ∼N\n\u0012\n0, dt\nN\n\u0013\n,\n(2.34)\n14\nChapter 2. Continuous Measurement on Quantum Systems\nand therefore we have dW ∼N(0, η dt), and the time-evolution equation is\ndρ = −i\nℏ[H, ρ]dt −γ\n4[ˆx, [ˆx, ρ]]dt +\nrγη\n2 {ˆx −⟨ˆx⟩, ρ}dW,\ndW ∼N(0, dt),\n(2.35)\nwhere we have rescaled dW such that it is now a standard Wiener increment.\n15\nChapter 3\nDeep Reinforcement Learning\nIn this chapter, we brieﬂy review deep reinforcement learning. It is constituted of two\ndiﬀerent subjects: (1) deep learning and (2) reinforcement learning.\nFirst, we begin\nwith the general idea of machine learning in Section 3.1 and then introduce deep learning\nalong with its motivations and uses. Then we review reinforcement learning, especially Q-\nlearning, in Section 3.2, and discuss how it is implemented using deep learning technology.\nIn this thesis, we use the word “machine learning” to refer to the general picture of\nartiﬁcial intelligence (AI) technology, and use the word “deep learning” to refer to ma-\nchine learning systems that specially use deep learning techniques. This chapter mainly\ndiscusses the AI technology relevant to the present research, and does not cover irrelevant\ndeep learning or machine learning topics.\n3.1\nDeep Learning\n3.1.1\nMachine Learning\nGenerally speaking, learning usually refers to a process in which unpredictable becomes\npredictable, by building a model to correctly relate diﬀerent pieces of relevant informa-\ntion. Machine learning aims to automatize this process. Although humans often achieve\nlearning via a sequence of logical reasoning and validation, up to now, machines do not\nhave a good common knowledge base to achieve creative logical reasoning to learn. To\ncompensate for this deﬁciency, machine learning systems usually have a set of possible\nmodels beforehand, which represents conceivable relations among the diﬀerent pieces of\ninformation that it is going to learn. The set of conceivable relations here is formally\ncalled the hypothesis space. Then, it learns some provided example data, by looking for a\nmodel in its hypothesis space which ﬁts the observed data best, and ﬁnally use the found\nmodel as the relation among the pieces of information it learns to give prediction on new\ndata. As a result, the learned model is almost always only an approximate solution to\nthe underlying problem. Nevertheless, it still works well enough in cases where a given\nproblem cannot be modelled precisely but can be approximated easily.\nTo formally give a deﬁnition of machine learning, according to Tom M. Mitchell [39],\n16\nChapter 3. Deep Reinforcement Learning\n“A computer program is said to learn from experience E with respect to some class of\ntasks T and performance measure P, if its performance at tasks in T, as measured by P,\nimproves with experience E.”\n3.1.2\nFeedforward Neural Networks\nAs discussed, the setting of the hypothesis space of a machine learning system is cru-\ncial for the performance. Because diﬀerent problems have diﬀerent properties, before\nthe emergence of deep learning, researchers considered various approximate models to\ndescribe the corresponding real-life problems, including text-voice transform, language\ntranslation, image recognition, etc., and therefore the researchers specialized in diﬀerent\nmachine learning tasks usually worked separately. However, the deep neural network as a\ngeneral hypothesis space set outperformed all previous research results in 2012 [22], and\nstarted a deep learning boom. Below we introduce the deep neural network model, or\nprecisely, the deep feedforward neural network following the line of thoughts of the last\nsection.\nWhen we attempt to model a relation between two quantities, the simplest guess is the\nlinear relation. Although real-world problems are typically high-dimensional and more\ncomplex, we may hold on to this linearity even in a multidimensional setting, and assume\ny = Mx + b ,\n(3.1)\nwhere we model the relation between y and x . Here y and x are vectors, M is a matrix.\nb is an additional bias term as a small compromise starting from the linear guess. M and\nb are learned by ﬁtting y and x pairs into existing training data pairs {(x, y)i}. This\nprocess of learning is called linear regression [28].\nObviously, the simple linear (or aﬃne) model above would not work for realistic com-\nplex problems, as it cannot model nonlinear relations.\nTherefore, we apply a simple\nnonlinear function σ after the linear map, which is called an activation function. This\nname is an analogy to the activation function controlling ﬁrings of neurons in neuro-\nscience. For simplicity, this function is a scalar function and is applied to a vector in\nan element-wise manner, acting on every component of the vector separately. Then, the\nfunction may be constructed as\ny = f(x) = M2 · σ(M1x + b1) + b2 .\n(3.2)\nIn practice, there is almost no constraint on the activation function, as long as it is\nnonlinear. The most commonly used two functions are the ReLU (Rectiﬁed Linear Unit)\nand the sigmoid, which are shown in ﬁgure 3.1.\nThe ReLU function simply zeros all negative values and keeps all positive values, and\nthe sigmoid is a transition between zero and one, which is also known as the standard\n17\nChapter 3. Deep Reinforcement Learning\nσ(x)\n(a) ReLU\nσ(x) = max(0, x)\nx\nσ(x)\n(b) Sigmoid\nσ(x) =\n1\n1+e−x\nx\nFigure 3.1: two frequently used activation functions in deep learning\nlogistic function. Because most of the time we use the ReLU as the activation function\nσ, we assume using the ReLU in the following context unless otherwise mentioned.\nAn immediate result which can be drawn is that the function f in Eq. (3.2) is univer-\nsal, in the sense that it can approximate an arbitrary continuous mapping from x to y ,\nprovided that the parameters M1, b1, M2, b2 have suﬃciently many dimensions and are\ncomplicated enough.\nThe above argument can be shown easily for the ReLU as σ , and then similarly for\nother activation functions. For simplicity we ﬁrst consider one-dimensional x, y . First we\nnote that the ReLU just eﬀectively bends a line, and M and b can be used to replicate,\nrotate and shift the straight line x = x ; then it can be realized that the above function\nin Eq. (3.2) is constituted of three successive processes: (1) copying the line x = x ,\nplus customizable rotation and shift by M1, b1 , (2) bending each resultant line by σ,\n(3) rotating, shifting and summing all lines by M2, b2 . Therefore, with correctly picked\nM1, b1, M2, b2 , this function can be used to construct arbitrary lines that are piece-wise\nlinear with ﬁnitely many bend points. Thus, it can approximate arbitrary functions from\nx to y with arbitrary precision, provided that parameters M1, b1, M2, b2 are appropri-\nately chosen. For the case of higher dimensional x and y, instead of bended lines, the\nfunction f constructs polygons in the x space and the universality follows similarly. This\nargument of universality also holds for other types of nonlinear functions besides the\nReLU, and can be shown by similar constructive arguments.\nNow, we consider using the function in Eq. (3.2) as our hypothesis space for machine\nlearning. Given data points {(x, y)i}, if we follow the universality argument and ﬁt the\nparameters in Eq. (3.2) to make f reproduce the relation from x to y over the whole\ndataset {(x, y)i}, then, as can also be seen from the universality argument, f becomes the\nnearest-neighbour interpolation of data points {xi}. For any unseen data point x absent\nfrom the training set {(x, y)i}, to predict its corresponding y , f ﬁnds its nearest neigh-\n18\nChapter 3. Deep Reinforcement Learning\nbours in the data set {xi} and predict its y as a linear interpolation of those neighbours’\ny values. This is a direct consequence of the polygon argument in the above paragraph,\nand implies that using this parametric function as the hypothesis space is still simple\nand naive, and that it cannot easily learn complex relations between x and y unless we\nhave numerous data points in the training set to represent all possibilities of x. Thus,\nwe need further improvement so that the function can learn complex relations more easily.\nAs mentioned earlier, the nonlinear function σ between linear mappings has its biolog-\nical analogue as the activation function. This is because in neuroscience, the activation of\na single neuron is inﬂuenced by its input linearly if its input is above an activation thresh-\nold, and if the input is below the threshold, there is no activation. This phenomenon\nis exactly modelled by the ReLU function following a linear mapping, where the linear\nmapping is connected to input neurons. Although each individual neuron functions sim-\nply, when they are connected, they may show extremely complex behaviour collectively.\nMotivated by this observation, we choose to apply the functions fi(x) := σ(Mix + bi)\nsequentially and put them into the form fi ◦fj ◦fk · · ·◦fl to build a deep neural network.\nIn this case, the output vector value of every fi represents a layer of neurons, with each\nscalar in it representing a single neuron, and every layer is connected to the previous\nlayer through the weight matrix M . Note that the dimension of every fi may not be\nequal. This artiﬁcial network of neurons is called a feedforward neural network, since its\ninformation only goes in one direction and does not loops back to previous neurons. It\ncan be written as follows:\ny = f(x) = Mn (fn−1 ◦fn−2 ◦fn−3 · · · ◦f1 (x)) + bn ,\n(3.3)\nwhere\nfi(x) = σ (Mix + bi) ,\nσ = max(0, x) .\n(3.4)\nIntuitively speaking, although one layer of fi only bends and folds the line a few times,\nsuccessive fi s can fold on existing foldings and ﬁnally make the represented function\nmore complex but with a certain regular shape. This equation (Eq. 3.3) is the deep\nfeedforward neural network that we use in deep learning as our hypothesis space. For\ncompleteness, we recapitulate and deﬁne some relevant terms below.\nThe neural network is a function f that gives an output y when provided with an\ninput x as in Eq. (3.3). The depth of a deep neural network refers to the n number in Eq.\n(3.3), which is the number of linear mappings involved. fi stands for the i-th layer, and\nthe activation values of the i-th layer is the output of fi , and the width of layer fi refers\nto the dimension of its output vector, i.e. the number of scalars (or neurons) involved.\nThese involved scalars are called units or hidden units of the layer. When there is no\nconstraint on matrix M , all units of adjacent layers are connected by generally non-zero\nentries in matrix M and this case is called fully connected. Usually, we call the n-th layer\nas the top layer and the ﬁrst layer as the bottom. Concerning output y , in Eq. (3.3)\nit is a general real-valued vector, but if we know there exists some preconditions on the\n19\nChapter 3. Deep Reinforcement Learning\nproperties of y, we may add an output function at the top of the network f to constrain\ny so that the preconditions are satisﬁed. This is especially useful for image classiﬁcation\ntasks, where the neural network is supposed to output a probability distribution over\ndiscrete choices. For the classiﬁcation case, the partition function is used as the output\nfunction to change the unnormalized output into a distribution.\n3.1.3\nTraining a Neural Network\nIn the last section we deﬁned our feedforward neural network f with parameters {Mi, bi}n\ni=1 .\nDiﬀerent from the cases of Eq. (3.1) and (3.2), it is not directly clear how to ﬁnd ap-\npropriate parameters {Mi, bi}n\ni=1 to ﬁt f to a given dataset {(x, y)i}. As the ﬁrst step,\nwe need to have a measure to evaluate how well a given f ﬁts into the dataset. For this\npurpose, a loss function is used, which measures the diﬀerence between f(xi) and yi on\na dataset S ≡{(x, y)i}, and a larger loss implies a lower performance. In the simplest\ncase, the L2 loss is used:\nL = 1\n|S|\nX\n(xi,yi)∈S\n||f(xi) −yi||2 ,\n(3.5)\nwhere || · || is the usual L2 norm on vectors. This loss is also termed mean squared error\n(MSE), i.e. the average of the squared error ||f(xi) −yi||2. It is widely used in machine\nlearning problems as a fundamental measure of diﬀerence.\nWith a properly chosen loss function, the original problem of ﬁnding a f that best\nﬁts the dataset S reduces to ﬁnding a f that minimizes the loss, which is an optimiza-\ntion problem in the parameter space {Mi, bi}n\ni=1 . This optimization problem is clearly\nnon-convex and hard to solve. Therefore, instead of looking for a global minimum in the\nparameter space, we only look for a local minimum which hopefully has a low enough loss\nto accomplish the learning task well. This is done via gradient descent. Namely, we cal-\nculate the gradient of the loss with respect to all the parameters, and then modify all the\nparameters following the gradient to decrease the loss using a small step size, and then we\nrepeat this process. Denoting the parameters by θ , the iteration process is given as below:\nθ′ = θ −ϵ ∇θL (θ, {(x, y)i}) ,\n(3.6)\nwhere ϵ is the iteration step size and is called the learning rate. It is clear that, with a\nsmall enough learning rate, the above iteration indeed converges to a local minimum of L.\nThis process of ﬁnding a solution is called training in machine learning, and before train-\ning we initialize the parameters randomly. In practice, although Eq. (3.5) uses the whole\ntraining set S to deﬁne L , during training we only sample a minibatch of data points\nfrom S to evaluate the gradient, and this is called stochastic gradient descent (SGD). The\nsampling method signiﬁcantly improves eﬃciency.\n20\nChapter 3. Deep Reinforcement Learning\nAs can be seen, both training and evaluation of a neural network requires a great\namount of matrix computation.\nIn a typical modern neural network, the number of\nparameters is on the order of tens of millions and the required amount of computation\nis huge. Therefore, the potential of this neural network strategy did not attract much\nattention until the technology of GPU (Graphic Processing Unit)-based parallelized com-\nputation becomes available in recent years [40], which makes it possible to train modern\nneural networks in hours or a few days, which would previously take many months on\nCPUs. This development of technology makes large-scale deep learning possible and is\none important reason for the deep learning boom in recent years.\nIn real cases, the iteration in training process usually does not follow Eq. (3.6) ex-\nactly. This is because this iteration strategy can cause the iteration to go forth and back\ninside a valley-shaped region, or be disturbed by local noise on gradients, or be blocked\nby barriers in the searched parameter space, etc.\nTo alleviate these problems, some\nalternative algorithms have been developed as improved versions of the basic gradient\ndescent, including Adam [41], RMSprop [42], and gradient descent with momentum [28].\nBasically, these algorithms employ two strategies. The ﬁrst one is to give the iteration\nstep an inertia, so that the iteration step is not only inﬂuenced by the current gradient,\nbut also by all previous gradients, and the eﬀect of previous gradients decays exponen-\ntially every time a step is done. This is called the momentum method, and the so-called\nmomentum term is one minus the decay coeﬃcient, usually set to 0.9 , which represents\nhow much the inertia is preserved per step. This training method is actually ubiquitous\nin deep learning. The second strategy is normalization of parameter gradients, such that\nthe average of the iteration step for each parameter becomes roughly constant and not\nproportional to the magnitude of the gradient. In sparse gradient cases, this strategy\ndramatically speeds up training. RMSprop and Adam adopt this strategy.\nTo summarize, we ﬁrst train a neural network to ﬁt a given dataset S = {(x, y)i} by\nminimizing a predeﬁned loss. Then we use the neural network to predict y of new unseen\ndata points x . Concerning practical applications, fully connected neural networks as\ndescribed above are commonly used for regression tasks, of which the target is to ﬁt a\nreal-valued function, which is often multivariable. For image classiﬁcation, motivated by\nthe fact that a pixel in an image is most related to nearby pixels, we put the units of a\nneural network layer also into a pixelized space, and connect a unit only to adjacent units\nbetween layers. To extract nonlocal information as an output, we downsample the pix-\nelized units. This structure is called the convolutional neural network, and is the current\nstate-of-the-art method for image classiﬁcation tasks [26]. Many other neural network\nstructures exist, but we leave them here since they are not directly relevant to our study.\nAlthough deep neural networks work extremely well for various tasks, so far we do not\nprecisely know the reason, which is still an important open question nowadays.\n21\nChapter 3. Deep Reinforcement Learning\n3.2\nReinforcement Learning\n3.2.1\nProblem Setting\nIn a reinforcement learning (RL) task, we do not have a training dataset beforehand, but\nwe let the AI interact with an environment to accumulate experience, and learn from the\naccumulated experience with a target set to maximize a predeﬁned reward. Because the\nAI learns by exploring the environment to perform better and better, this learning pro-\ncess is called reinforcement learning, in contrast to supervised and unsupervised learning\nin which case the AI learns from a pre-existing dataset.\nReinforcement learning is important when we do not understand an environment,\nbut we can simulate the environment for an AI to interact with and gain experience\nfrom. Examples of reinforcement learning tasks include video games [43], modern e-sports\ngames [44], chess-like games [24], design problems [45] and physical optimization problems\nin quantum control [46,47]. In all situations the environment that the AI interact with\nis modelled by a Markov decision process (MDP) or a game in game theory, where there\nis a state representing the current situation of the environment, and the AI inputs the\nstate and outputs an action which inﬂuences future evolution of the environment state.\nThe goal of the AI is maximize the expected total reward in the environment, such as\nscores in games, winning probability in chess, and ﬁdelity of quantum gates. This setting\nis illustrated in ﬁgure 3.2. Note that the evolution of the environment state and actions\nof the AI are discrete in time steps.\nFigure 3.2: Setting of reinforcement learning, where an AI agent interacts with an environment.\nSubscripts t and t + 1 denote successive time steps. Reproduced from Ref. [48].\n3.2.2\nQ-learning\nThere are many learning strategies for a reinforcement learning task. The most basic one\nis brute-force search, which is to test all possible action choices under all circumstances to\nﬁnd out the most beneﬁcial strategy, in the sense of total expected reward. This strategy\ncan be used to solve small-scale problems such as tic-tac-toe (ﬁgure 3.3).\nHowever, this strategy is applicable to tic-tac-toe only because the space of the game\nstate is so small that it can be easily enumerated. In most cases, brute-force search is\nnot possible, and we need better methods, often heuristic ones, to achieve reinforcement\n22\nChapter 3. Deep Reinforcement Learning\nFigure 3.3: A tic-tac-toe game example, reproduced from Ref. [49].\nlearning. In this section we discuss the mostly frequently used method, Q-learning [50],\nwhich is used in our research in the next few chapters.\nThe Q-learning is based on a function Qπ(s, a) which is deﬁned to be the expected\nfuture reward at a state s when taking an action a, provided with a certain policy π\nto decide future actions. In addition, expected rewards in future steps are discounted\nexponentially by γ according to how far they are away from the present:\nQπ(st, at) = r(st, at) + E(at+i∼π(st+i), st+i)∞\ni=1\n\" ∞\nX\ni=1\nγi r(st+i, at+i)\n#\n,\n0 < γ < 1 ,\n(3.7)\nwhere r(s, a) is the reward, and the expectation is taken over future trajectories (st+i, at+i)∞\ni=1 .\nNote that the environment evolution (st, at) 7→st+1 is solely determined by the environ-\nmental property and cannot be controlled.\nThis Q function has a very important recursive property, that is\nQπ(st, at) = r(st, at) + γ Eat+1∼π(st+1), st+1 [Qπ(st+1, at+1)]\n(3.8)\nwhich can be shown directly from Eq. (3.7) with non-divergent Q s. If the action policy\nπ∗is optimal, Qπ∗then satisﬁes the following Bellman Eq. (3.9) [51]:\nQπ∗(st, at) = r(st, at) + γ Est+1\n\u0014\nmax\nat+1 Qπ∗(st+1, at+1)\n\u0015\n,\n(3.9)\nwhich is straightforward to show by following Eq. (3.8), using the fact that policy π∗\ntakes every action to maximize Q ; otherwise it cannot be an optimal policy as Q would\nbe increased by taking a maximum.\nThen we look for such a Q function. This function can be obtained by the following\niteration:\nQ′(st, at) = r(st, at) + γ Est+1\n\u0014\nmax\nat+1 Q(st+1, at+1)\n\u0015\n.\n(3.10)\n23\nChapter 3. Deep Reinforcement Learning\nAfter suﬃciently many iterations, Q′ converges to Qπ∗[50]. This is due to the dis-\ncount factor 0 < γ < 1 and a non-diverging reward r(s, a) , which makes iterations of\nthe above equation drop the negligible ﬁnal term Est+1\n\u0002\nmaxat+1 Q(st+1, at+1)\n\u0003\nas it would\nbe multiplied by γn after n iterations and diminishes exponentially, and the remaining\nvalue of Q′ would be purely determined by reward r(s, a) . Since this converged Q′ is\nnot inﬂuenced by the initial Q choice at the start of iterations, this Q′ must be unique,\nand therefore it is also the Qπ∗in Eq. (3.9), so this converged Q′ is both unique and\noptimal. This is rigorously proved in Ref. [50]. After obtaining the Qπ∗, we simply use\nmaxat Qπ∗(st, at) to decide the action at for a state st , and this results in the optimal\npolicy π∗.\nAn important caveat here is that, the optimality above cannot be separated from the\ndiscount factor γ , which puts exponential discount on future rewards. This γ is necessary\nfor convergence of Q , but it also results in a “time horizon” beyond which the optimal\npolicy does not consider future rewards. However, the goal of solving the reinforcement\nlearning problem is to actually maximize total accumulated reward, which corresponds\nto γ = 1 . The ideal case is that π∗converges for γ →1 ; however, this is not always true,\nand diﬀerent γ s represent strategies that have diﬀerent amounts of foresight. In addition,\na large γ such as 0.9999 often make the learning diﬃcult and hard to proceed. Therefore\nin practice, Q-learning usually does not achieve the absolute optimality that corresponds\nto γ = 1, except for the case where the reinforcement learning task is well-bounded so\nthat γ →1 does not cause any problem.\n3.2.3\nImplementation of Deep Q-Network Learning\nThe Q-learning strategy that uses a deep neural network to approximate Q function is\ncalled deep Q-network (DQN) learning. It simply constructs a deep feedforward neural\nnetwork to represent the Q function, with input s and several outputs as evaluated Q\nvalues for each action choice a . Note that now action a is a choice from a ﬁnite set and\nis not a continuous variable. The training loss is deﬁned to minimize the absolute value\nof the temporal diﬀerence error (TD error), which comes from Eq. (3.10):\nL = Q(st, at) −\n\u0012\nr(st, at) + γ max\nat+1 Q(st+1, at+1)\n\u0013\n,\n(3.11)\nwhere (st, at, r, st+1) is a sampled piece of experience of the AI interacting with the en-\nvironment. Due to the nature of sampling during training, we ignore evaluation of the\nexpectation of st+1 in the equation.\nUp to now we have obtained all the necessary pieces of information to implement\ndeep reinforcement learning. First, we initialize a feedforward neural network with ran-\ndom parameters. Second, we use this neural network as our Q function and use strategy\nmaxat Q(st, at) to take actions (which may be modiﬁed). Third, we sample from what the\nAI has experienced, i.e. (st, at, r, st+1) data tuples, and calculate the error, (Eq. 3.11),\n24\nChapter 3. Deep Reinforcement Learning\nand use gradient descent to minimize the absolute error. Fourth, we repeat the second\nand third steps until the AI performs well enough. This system can be divided into three\nparts, and diagrammatically it is shown in ﬁgure 3.4. Note that the second and the third\nsteps above can be parallelized and executed simultaneously.\nFigure 3.4: Reinforcement learning system, reproduced from Ref. [52].\nIn real scenarios, a great deal of technical modiﬁcations are applied to the above\nlearning procedure in order to make the learning more eﬃcient and to improve the per-\nformance. These advanced learning strategies include separation of the trained Q function\nand the other Q function term used in calculating the loss [53], taking random actions\non occasion [53], prioritized experience sampling strategy [54], double Q-networks to sep-\narately decide actions and Q values [55], duel network structure to separately learn the\naverage Q value and Q value change due to each action [56], using random variables\ninside network layers to induce organized random exploration in the environment [57].\nThese are the most important recent developments, and they improve the performance\nand stability of deep Q-learning, which is discussed in Appendix C. These techniques are\nincorporated together in Ref. [58] and the resultant algorithm is called Rainbow DQN.\nWe follow this algorithm in our reinforcement learning researches in the next chapters.\nThe hyperparameter settings and details of our numerical experiments are provided in\nthe corresponding chapters and Appendix C.\n25\nChapter 4\nControl in Quadratic Potentials\nIn this chapter, we discuss one particle control with continuous position measurement\nin one-dimensional quadratic potentials, including a harmonic potential and an inverted\nharmonic potential, and the target of control is to keep the particle at low energy around\nthe center. We ﬁrst analyse the system in Section 4.1.1 to simplify it, and show that it\nhas an optimal solution of control in Section 4.1.3 with reference to the Linear-Quadratic-\nGaussian (LQG) control theory which is discussed in Appendix A. We explain our ex-\nperimental setting of the simulated quantum system, train a neural network to control\nthe system, and present the results in Section 4.2. Next, we compare the resulting per-\nformance with the optimal control in Section 4.3, and then consider diﬀerent levels of\nhandicaps on controllers, comparison them with suboptimal control strategies, and dis-\ncuss what is precisely learned by the AI. Finally, in Section 4.4 we present the conclusions\nof this chapter.\n4.1\nAnalysis of Control of a Quadratic Potential\n4.1.1\nGaussian Approximation\nThe state of a particle in quadratic potentials under position measurement is known to\nbe well approximated by a Gaussian state [59,60]. We discuss this result in detail in this\nsection, and derive a suﬃcient representation of the evolution equation of a particle only\nin terms of the ﬁrst and second moments of its Wigner distribution in (x, p) phase space.\nThis signiﬁcantly simpliﬁes the problem, and reduces it to an almost classical situation.\nBy the term Gaussian state, we mean that the one-particle state has a Gaussian shape\ndistribution as its Wigner distribution in phase space. We show that a Gaussian-shaped\nWigner distribution always keeps its Gaussian property when evolving in a quadratic\npotential. First, we consider the problem in the Heisenberg picture and evaluate the time\nevolutions of operators ˆx and ˆp:\nd\ndt ˆx = i\nℏ[H, ˆx],\nd\ndt ˆp = i\nℏ[H, ˆp],\n(4.1)\n26\nChapter 4. Control in Quadratic Potentials\nH = k\n2 ˆx2 + ˆp2\n2m,\nk ∈R,\nm ∈R+,\n(4.2)\nwhere k can be both positive and negative. We do not assume a particular sign of k in\nthe following calculations. We substitute H into Eq. (4.1) and obtain\nd\ndt ˆx = ˆp\nm,\nd\ndt ˆp = −kˆx,\n(4.3)\nwhich constitute a system of diﬀerential equations as\nd\ndt\n\u0012 ˆx\nˆp\n\u0013\n=\n\u0012\n0\n1\nm\n−k\n0\n\u0013 \u0012 ˆx\nˆp\n\u0013\n.\n(4.4)\nThis is solved by eigendecomposition:\n\u0012 ˆx(t)\nˆp(t)\n\u0013\n= etA\n\u0012 ˆx(0)\nˆp(0)\n\u0013\n,\nA :=\n\u0012\n0\n1\nm\n−k\n0\n\u0013\n(4.5)\nA = QΛQ−1 =\n\u0012 −\n1\n√\n−mk\n1\n√\n−mk\n1\n1\n\u0013 \n−\nq\n−k\nm\n0\n0\nq\n−k\nm\n\n\n \n−\n√\n−mk\n2\n1\n2\n√\n−mk\n2\n1\n2\n!\n(4.6)\nM := etA = QetΛQ−1 = Q\n \ne−t√\n−k\nm\n0\n0\net√\n−k\nm\n!\nQ−1,\n(4.7)\n\u0012 ˆx(t)\nˆp(t)\n\u0013\n= M\n\u0012 ˆx(0)\nˆp(0)\n\u0013\n.\n(4.8)\nFor simplicity, we deﬁne\nλ :=\nr\n−k\nm ≡i\nr\nk\nm,\n(4.9)\nand then the matrix M can be written as\nM = 1\n2\n\u0012\netλ + e−tλ\n1\n√\n−mk(etλ −e−tλ)\n√\n−mk(etλ −e−tλ)\netλ + e−tλ\n\u0013\n,\n(4.10)\nwhich is a symplectic matrix.\nTherefore, a free evolution in a quadratic potential equivalently transforms the ˆx and\nˆp operators through a symplectic transformation. Next we show that this results in the\nsame symplectic transform of its Wigner distribution in terms of phase-space coordinates\n27\nChapter 4. Control in Quadratic Potentials\n(x, p). To show this, we use the characteristic function deﬁnition of Wigner distribu-\ntion [61], as follows:\nW(x) =\nZ\nR2\nd2ξ\n(2π)2 exp(−ixTΩξ)χ(ξ),\n(4.11)\nx =\n\u0012 x\np\n\u0013\n,\nΩ=\n\u0012\n0\n1\n−1\n0\n\u0013\n,\nx, ξ ∈R2,\n(4.12)\nχ(ξ) = tr(ρD(ξ)),\nD(ξ) := exp(iˆxTΩξ),\nˆx :=\n\u0012 ˆx\nˆp\n\u0013\n,\n(4.13)\nwhere ρ is a quantum state, χ is called the Wigner characteristic function and D is the\nWeyl operator. The Wigner distribution is essentially a Fourier transform of the Wigner\ncharacteristic function, and both the characteristic function χ and the Wigner distri-\nbution W contain complete information about the state ρ. Now we consider the time\nevolution of ρ, or equivalently, the time evolution of ˆx. According to Eqs. (4.8) and\n(4.10) and the symplectic property of M, we have\nχ(ξ, t) = tr(ρD(ξ, t)) = tr(ρ exp(iˆxTM TΩξ))\n= tr(ρ exp(iˆxTM TΩMM −1ξ))\n= tr(ρ exp(iˆxTΩM −1ξ))\n= χ(M −1ξ, 0).\n(4.14)\nNote that the matrix M has a determinant equal to 1, i.e. |M| = 1, and therefore it is\nalways invertible. Then the Wigner distribution is\nW(x, t) =\nZ\nR2\nd2ξ\n(2π)2 exp(−ixTΩξ)χ(ξ, t)\n=\nZ\nR2\nd2ξ\n(2π)2 exp(−ixTΩξ)χ(M −1ξ, 0)\n=\nZ\nR2\nd2(Mξ)\n(2π)2|M| exp(−ixTΩMξ)χ(ξ, 0)\n=\nZ\nR2\nd2(Mξ)\n(2π)2\nexp(−ixTM −1TΩξ)χ(ξ, 0)\n= W(M −1x, 0),\n(4.15)\nwhere we have changed the integration variable ξ ←Mξ in the third line and used the\ncondition of unbounded integration area R2 in the last line.\nWe see that a Wigner distribution W(x, 0) simply evolves to W(Mx, t) after time t.\nThis is merely a linear transformation of phase space coordinates, and therefore the dis-\ntribution as a whole remains unaltered, with only the position, orientation and width of\nthe distribution possibly being changed. The shape of distribution never changes. Thus,\n28\nChapter 4. Control in Quadratic Potentials\na Gaussian distribution always stays Gaussian. This fact also holds true for Hamiltonians\nincluding the terms (ˆxˆp + ˆpˆx) and ˆx and ˆp, which can be proved similarly.\nThen, if at some instants we do weak position measurements that can be approximated\nto be Gaussian on the state ρ, as in Section 2.2.2, the Gaussianity of the Wigner distri-\nbution on position coordinate x increases, and along with the rotation and movement of\nthe distribution in phase space, the Gaussianity of the whole distribution monotonically\nincreases, and as a result it is always rounded to be approximately Gaussian in the long\nterm. The main idea is that, non-Gaussianity never emerges by itself in quadratic poten-\ntials and under position measurement.\n4.1.2\nEﬀective Description of Time Evolution\nIn this case, the state can be fully described by its means and covariances as a Gaussian\ndistribution in phase space. Those quantities are\n⟨ˆx⟩,\n⟨ˆp⟩,\nVx := ⟨ˆx2⟩−⟨ˆx⟩2,\nVp := ⟨ˆp2⟩−⟨ˆp⟩2,\nC := 1\n2⟨ˆxˆp + ˆpˆx⟩−⟨ˆx⟩⟨ˆp⟩,\n(4.16)\ni.e., totally ﬁve real values.\nTherefore, when describing the time evolution of the state under continuous measure-\nment, we may only describe the time evolution of the above ﬁve quantities instead, which\nis considerably simpler. We now derive their evolution equations.\nWe use the evolution equation for a state under continuous position measurement Eq.\n(2.35) as a starting point:\ndρ = −i\nℏ[H, ρ]dt −γ\n4[ˆx, [ˆx, ρ]]dt +\nrγη\n2 {ˆx −⟨ˆx⟩, ρ}dW,\n(4.17)\ndW ∼N(0, dt),\nγ > 0,\nη ∈[0, 1].\n(4.18)\nRecall that dW is a Wiener increment and γ and η represent the measurement strength\nand eﬃciency respectively, and we use Itˆo calculus formulation. We can now evaluate the\ntime evolution of the quantities in Eq. (4.16).\n29\nChapter 4. Control in Quadratic Potentials\nd⟨ˆx⟩= tr(ˆx dρ) = tr\n\u0012 i\nℏρ[H, ˆx]dt −γ\n4[ˆx, [ˆx, ρ]]ˆxdt +\nrγη\n2 {ˆx −⟨ˆx⟩, ρ}ˆx dW\n\u0013\n= tr\n\u0012\nρ ˆp\nmdt −0 +\nrγη\n2 (2ˆx2ρ −2⟨ˆx⟩ˆxρ) dW\n\u0013\n= ⟨ˆp⟩\nm dt +\np\n2γη(⟨ˆx2⟩−⟨ˆx⟩2) dW\n= ⟨ˆp⟩\nm dt +\np\n2γηVx dW.\n(4.19)\nd⟨ˆp⟩= tr(ˆp dρ) = tr\n\u0012 i\nℏρ[H, ˆp]dt −γ\n4[ˆx, [ˆx, ρ]]ˆpdt +\nrγη\n2 {ˆx −⟨ˆx⟩, ρ}ˆp dW\n\u0013\n= tr\n\u0012\nρ(−kˆx)dt −γ\n4ρ(ˆxˆxˆp −2ˆxˆpˆx + ˆpˆxˆx) +\np\n2γη\n\u0012\nρ ˆxˆp + ˆpˆx\n2\n−ρ⟨ˆx⟩ˆp\n\u0013\ndW\n\u0013\n= −k⟨ˆx⟩dt +\np\n2γηC dW.\n(4.20)\ndVx = tr(ˆx2 dρ) −d(⟨ˆx⟩2) = tr(ˆx2 dρ) −2⟨ˆx⟩d⟨ˆx⟩−(d⟨ˆx⟩)2\n= tr(ˆx2 dρ) −2⟨ˆx⟩\n\u0012⟨ˆp⟩\nm dt +\np\n2γηVx dW\n\u0013\n−2γηV 2\nx dt.\n(4.21)\nSince it is too lengthy, we calculate tr(ˆx2 dρ) separately. In following calculations we\nneed to use symmetric properties of ρ as a Gaussian state. First we use tr((ˆx−⟨ˆx⟩)3ρ) = 0,\nwhich means that the skewness of a Gaussian distribution is zero. It leads to\ntr(ˆx3ρ) = 3⟨ˆx2⟩⟨ˆx⟩−2⟨ˆx⟩3 = 3Vx⟨ˆx⟩+ ⟨ˆx⟩3.\n(4.22)\ntr(ˆx2 dρ) = tr\n\u0012 i\nℏρ[H, ˆx2]dt −γ\n4[ˆx, [ˆx, ρ]]ˆx2dt +\nrγη\n2 {ˆx −⟨ˆx⟩, ρ}ˆx2 dW\n\u0013\n= tr\n\u0012\nρ ˆxˆp + ˆpˆx\nm\ndt +\np\n2γη\n\u0000ρˆx3 −ρ⟨ˆx⟩ˆx2\u0001\ndW\n\u0013\n= ⟨ˆxˆp + ˆpˆx⟩\nm\ndt +\np\n2γη\n\u00003Vx⟨ˆx⟩+ ⟨ˆx⟩3 −⟨ˆx⟩(Vx + ⟨ˆx⟩2)\n\u0001\ndW\n= ⟨ˆxˆp + ˆpˆx⟩\nm\ndt + 2\np\n2γηVx⟨ˆx⟩dW,\n(4.23)\ndVx = tr(ˆx2 dρ) −2⟨ˆx⟩\n\u0012⟨p⟩\nm dt +\np\n2γηVx dW\n\u0013\n−2γηV 2\nx dt\n=\n\u00122C\nm −2γηV 2\nx\n\u0013\ndt.\n(4.24)\n30\nChapter 4. Control in Quadratic Potentials\nNext we calculate Vp in a similar manner.\ndVp = tr(ˆp2 dρ) −d(⟨ˆp⟩2)\n= tr(ˆp2 dρ) −2⟨ˆp⟩\n\u0010\n−k⟨ˆx⟩dt +\np\n2γηC dW\n\u0011\n−2γηC2 dt.\n(4.25)\nWe need to use the following symmetric property:\ntr((ˆp −⟨ˆp⟩)(ˆx −⟨ˆx⟩)(ˆp −⟨ˆp⟩)ρ) = 0\n⇒\ntr\n\u0012 ˆxˆp2 + ˆp2ˆx\n2\nρ\n\u0013\n= tr((ˆpˆxˆp)ρ) = 2C⟨ˆp⟩+ Vp⟨ˆx⟩+ ⟨ˆp⟩2⟨ˆx⟩,\n(4.26)\ntr(ˆp2 dρ) = tr\n\u0012 i\nℏρ[H, ˆp2]dt −γ\n4[ˆx, [ˆx, ρ]]ˆp2dt +\nrγη\n2 {ˆx −⟨ˆx⟩, ρ}ˆp2 dW\n\u0013\n= tr\n\u0010\n−kρ(ˆxˆp + ˆpˆx)dt −γ\n4ρ(ˆxˆxˆp2 −2ˆxˆp2ˆx + ˆp2ˆxˆx)\n+\np\n2γη\n\u0012\nρ(ˆxˆp2 + ˆp2ˆx)\n2\n−ρ⟨ˆx⟩ˆp2\n\u0013\ndW\n\u0013\n= −k⟨ˆxˆp + ˆpˆx⟩dt + tr\n\u0010\n−γ\n4ρ(2iℏˆxˆp −2iℏˆpˆx)\n\u0011\n+\np\n2γη2C⟨ˆp⟩dW\n= −k⟨ˆxˆp + ˆpˆx⟩dt + γ\n2ℏ2dt +\np\n2γη2C⟨ˆp⟩dW,\n(4.27)\ndVp = tr(ˆp2 dρ) −2⟨ˆp⟩\n\u0010\n−k⟨ˆx⟩dt +\np\n2γηC dW\n\u0011\n−2γηC2 dt\n= (−2kC −2γηC2 + γ\n2ℏ2)dt.\n(4.28)\nFinally, we calculate the covariance C.\ndC = 1\n2tr ((ˆxˆp + ˆpˆx) dρ) −d(⟨ˆx⟩⟨ˆp⟩)\n= 1\n2tr ((ˆxˆp + ˆpˆx) dρ) −⟨ˆx⟩d⟨ˆp⟩−⟨ˆp⟩d⟨ˆx⟩−d⟨ˆx⟩d⟨ˆp⟩\n= 1\n2tr ((ˆxˆp + ˆpˆx) dρ) −⟨ˆx⟩(−k⟨ˆx⟩dt +\np\n2γηC dW)\n−⟨ˆp⟩\n\u0012⟨p⟩\nm dt +\np\n2γηVx dW\n\u0013\n−2γηVxC dt.\n(4.29)\nHere we need the following symmetry:\ntr((ˆx −⟨ˆx⟩)(ˆp −⟨ˆp⟩)(ˆx −⟨ˆx⟩)ρ) = 0\n⇒\ntr\n\u0012 ˆpˆx2 + ˆx2ˆp\n2\nρ\n\u0013\n= tr((ˆxˆpˆx)ρ) = 2C⟨ˆx⟩+ Vx⟨ˆp⟩+ ⟨ˆx⟩2⟨ˆp⟩.\n(4.30)\n31\nChapter 4. Control in Quadratic Potentials\n1\n2tr ((ˆxˆp + ˆpˆx) dρ) = 1\n2tr\n\u0012 i\nℏρ[H, (ˆxˆp + ˆpˆx)]dt −γ\n4[ˆx, [ˆx, ρ]](ˆxˆp + ˆpˆx)dt\n+\nrγη\n2 {ˆx −⟨ˆx⟩, ρ}(ˆxˆp + ˆpˆx) dW\n\u0013\n= 1\n2tr\n\u0012\nρ\n\u00122ˆp2\nm −2kˆx2\n\u0013\ndt −0 +\np\n2γη (2ˆxˆpˆxρ −(ˆxˆp + ˆpˆx)⟨ˆx⟩ρ) dW\n\u0013\n=\n\u0012Vp + ⟨ˆp⟩2\nm\n−k(Vx + ⟨ˆx⟩2)\n\u0013\ndt\n+ 1\n2\n\u0010p\n2γη\n\u00004C⟨ˆx⟩+ 2Vx⟨ˆp⟩+ 2⟨ˆx⟩2⟨ˆp⟩−2C⟨ˆx⟩−2⟨ˆp⟩⟨ˆx⟩2\u0001\ndW\n\u0011\n=\n\u0012Vp + ⟨ˆp⟩2\nm\n−k(Vx + ⟨ˆx⟩2)\n\u0013\ndt +\np\n2γη (C⟨ˆx⟩+ Vx⟨ˆp⟩) dW,\n(4.31)\ndC = 1\n2tr ((ˆxˆp + ˆpˆx) dρ) −⟨ˆx⟩(−k⟨ˆx⟩dt +\np\n2γηC dW)\n−⟨ˆp⟩\n\u0012⟨ˆp⟩\nm dt +\np\n2γηVx dW\n\u0013\n−2γηVxC dt\n=\n\u0012Vp\nm −kVx −2γηVxC\n\u0013\ndt.\n(4.32)\nThe results are summarized as follows:\nd⟨ˆx⟩= ⟨ˆp⟩\nm dt +\np\n2γη Vx dW,\nd⟨ˆp⟩= −k⟨ˆx⟩dt +\np\n2γη C dW,\ndVx =\n\u00122C\nm −2γηV 2\nx\n\u0013\ndt,\ndVp = (−2kC −2γηC2 + γ\n2ℏ2)dt,\ndC =\n\u0012Vp\nm −kVx −2γηVxC\n\u0013\ndt.\n(4.33)\nOur results coincide with the results presented in Ref. [62], and we have also veriﬁed\nthat our results are correct through numerical calculation. From the above equations, we\ncan see that only the average position and momentum are perturbed by the stochastic\nterm dW, and the covariances form a closed set of equations and evolve deterministically.\nTherefore, we can calculate their steady values:\n32\nChapter 4. Control in Quadratic Potentials\ndVx = dVp = dC = 0 · dt,\nVx, Vp > 0.\n⇒C = −k +\np\nk2 + γ2ηℏ2\n2γη\n, Vx =\ns\nC\nmγη, Vp = 2C\np\nmγηC + k\ns\nmC\nγη .\n(4.34)\nWe observe that a state always evolves into a steady shape in numerical simulation,\nand due to this convergence we assume that the covariances are simply ﬁxed as the above\nvalues. Then, the degrees of freedom considerably decrease, and the only remaining ones\nare the two real quantities ⟨ˆx⟩and ⟨ˆp⟩, which are the means of the Gaussian distribution\nin phase space.\nEquivalently, we may say that the degrees of freedom of the state is represented by\na displacement operator D(α), which displaces the state from the origin of phase space,\ni.e. ⟨ˆx⟩= ⟨ˆp⟩= 0. We denote the state centered at the origin of phase space by ρ0, and\nwe have\n⟨ˆx⟩Dρ0D† =\nr\nℏ\n2mω(α + α∗),\n⟨ˆp⟩Dρ0D† = i\nr\nℏmω\n2\n(α∗−α),\n(4.35)\nD(α) = eαˆa†−α∗ˆa†,\nα ∈C,\n(4.36)\nˆa :=\nrmω\n2ℏ(ˆx +\ni\nmω ˆp),\nω :=\nr\n|k|\nm ,\n(4.37)\nwhere ˆa is the annihilation operator. It has the following properties:\n[ˆa, ˆa†] = 1,\nˆx =\nr\nℏ\n2mω(ˆa† + ˆa),\nˆp = i\nr\nℏmω\n2\n(ˆa† −ˆa),\n(4.38)\nand\nD†(α)ˆaD(α) = ˆa + α,\nD†(α)ˆa†D(α) = ˆa† + α∗,\n(4.39)\nˆn := ˆa†ˆa,\nD†(α)ˆnD(α) = ˆn + (αˆa† + α∗ˆa) + |α|2,\n(4.40)\nwhere ˆn is the number operator. In the above calculations we do not assume the sign\nof k, but here it is necessary to use the positive coeﬃcient |k| to give an appropriate\ndeﬁnition for the operators, and therefore ω is always positive.\nFor the state ρ0, we have ⟨ˆx⟩ρ0 = ⟨ˆp⟩ρ0 = 0, and therefore ⟨αˆa† + α∗ˆa⟩ρ0 = 0. Then\nwe have\n⟨ˆn⟩Dρ0D† = ⟨ˆn⟩ρ0 + |α|2.\n(4.41)\n33\nChapter 4. Control in Quadratic Potentials\nTherefore, we can use |α|2 to express the expectation value of the operator\n\u0010\nˆp2\n2m + |k|\n2 ˆx2\u0011\nfor state Dρ0D†:\n\u001c ˆp2\n2m + |k|\n2 ˆx2\n\u001d\nDρ0D†\n= ℏω\n\u0012\n⟨ˆn⟩Dρ0D† + 1\n2\n\u0013\n= ℏω\n\u0012\n⟨ˆn⟩ρ0 + 1\n2 + |α|2\n\u0013\n,\n(4.42)\nwhere ⟨ˆn⟩ρ0 is a constant determined by the covariances in Eq. (4.34). We also have\n⟨ˆx⟩Dρ0D† and ⟨ˆp⟩Dρ0D† to represent the real and imaginary parts of α, so we obtain the\nfollowing formula:\n\u001c ˆp2\n2m + |k|\n2 ˆx2\n\u001d\nDρ0D†\n= ℏω\n\u0012\n⟨ˆn⟩ρ0 + 1\n2\n\u0013\n+\n \n⟨ˆp⟩2\nDρ0D†\n2m\n+ |k|\n2 ⟨ˆx⟩2\nDρ0D†\n!\n.\n(4.43)\nNow, if we want to evaluate\nD\nˆp2\n2m + |k|\n2 ˆx2E\nfor the state ρ, we can just replace the\noperators ˆp and ˆx by the means ⟨ˆp⟩and ⟨ˆx⟩.\nAs we can see, this system turns out to be very simple. This can be understood by\nthe fact that, concerning free evolution, a non-negative Wigner distribution behaves in\nphase space exactly in the same way as the corresponding classical distribution unless the\nHamiltonian contains terms that are more than quadratic or non-analytic, since its evo-\nlution equation would reduces to the Liouville equation [63,64]. In addition, the position\nmeasurement only shrinks and squeezes the distribution in the x direction, which does\nnot introduce negativity into the distribution [65], and therefore the distribution evolves\nalmost classically. The only quantumness in this system is the measurement backaction\nby dW and the uncertainty principle which introduces a constant term in dVp (see Eq.\n(4.33)).\n4.1.3\nOptimal Control\nAs the system is simpliﬁed, we now consider control of this quadratic system. The system\nis summarized by the following:\nd⟨ˆx⟩= ⟨ˆp⟩\nm dt +\np\n2γη Vx dW,\nd⟨ˆp⟩= −k⟨ˆx⟩dt +\np\n2γη C dW,\nC = −k +\np\nk2 + γ2ηℏ2\n2γη\n,\nVx =\ns\nC\nmγη,\nVp = 2C\np\nmγηC + k\ns\nmC\nγη ,\n(4.44)\nwhere the only independent degrees of freedom are ⟨ˆx⟩and ⟨ˆp⟩. We consider using an\n34\nChapter 4. Control in Quadratic Potentials\nexternal force to control the system, which is just an additional term Fconˆx added to the\ntotal Hamiltonian H. Then the time evolution becomes\nd⟨ˆx⟩= ⟨ˆp⟩\nm dt +\np\n2γη Vx dW,\nd⟨ˆp⟩= (−k⟨ˆx⟩−Fcon)dt +\np\n2γη C dW,\n(4.45)\nwhere Fcon actually gives a force in the opposite direction of its sign. We can conﬁrm\nthat the equations concerning Vx, Vp and C are not changed explicitly, or by interpreting\nthe additional Fconˆx term in the Hamiltonian as a shift of the operator ˆx by an amount\nof Fcon\nk , which clearly does not aﬀect the covariances.\nWhen k is larger than zero, the Hamiltonian H =\nˆp2\n2m + k\n2 ˆx2 represents a harmonic os-\ncillator, and here we consider controlled cooling of this system. Since the system involves\nonly one particle, cooling amounts to decreasing its energy from an arbitrarily chosen\ninitial state. Because we assume continuous measurement on this system, the previous\nanalysis and simpliﬁcation apply.1 The target of control is to minimize energy ⟨H⟩, which\namounts to minimizing the functional\n\u0010\n⟨ˆp⟩2\n2m + k\n2⟨ˆx⟩2\u0011\nwith k > 0 according to Eq. (4.43),\nunder the above time-evolution equations (4.45). As we consider a general cooling task,\nthe minimization should be considered as minimizing the time-averaged total energy. We\ncall this minimized function as a loss, which is sometimes also called a control score. It\nis denoted as L:\nL := lim\nT→∞\n1\nT\nZ T\n0\n\u0012⟨ˆp⟩2\n2m + k\n2⟨ˆx⟩2\n\u0013\ndt.\n(4.46)\nHere the inﬁnite limit of time is not crucial. It is put here for translational invariance of\nthe control in time, and suﬃcient long-term planning of the control.\nWhen the system is noise-free and deterministic, that is,\nd⟨ˆx⟩= ⟨ˆp⟩\nm dt,\nd⟨ˆp⟩= (−k⟨ˆx⟩−Fcon)dt,\n(4.47)\nthe L above can converge to 0 due to the term\n\u0010\nlim\nT→∞\n1\nT\n\u0011\nand therefore is not a proper\nmeasure of loss that we wish to minimize. Therefore, we use the above deﬁnition (4.46)\nonly when the system contains noise, and for a deterministic case we need to redeﬁne it as\nL := lim\nT→∞\nZ T\n0\n\u0012⟨ˆp⟩2\n2m + k\n2⟨ˆx⟩2\n\u0013\ndt,\n(4.48)\nwhich is not essentially diﬀerent from Eq. (4.46) but is well-behaved when we try to min-\nimize it. Since these two deﬁnitions are only diﬀerent by some mathematical subtlety,\nwe do not speciﬁcally distinguish them when it is unnecessary, and it is clear from the\n1We do not consider a measurement strength that varies in time.\n35\nChapter 4. Control in Quadratic Potentials\ncontext which one is being considered.\nAs introduced in Section 1.1.1, we now seek for the optimal strategy of controlling\nthe variable Fcon such that L is minimized. For the deterministic system Eq. (4.47),\nminimization of L can be achieved in a simple manner, by borrowing some ideas from\nphysics. First, we note that the time-evolution equations concerning ⟨ˆx⟩and ⟨ˆp⟩are\neﬀectively classical, which means that, we have a classical particle with position x and\nmomentum p satisfying xt=0 = ⟨ˆx⟩t=0 and pt=0 = ⟨ˆp⟩t=0, and the time evolution of (x, p)\ncan be the same as that of (⟨ˆx⟩, ⟨ˆp⟩) of the underlying quantum system, which can also\nbe seen from the Ehrenfest theorem concerning quadratic potentials [66]. Then, when\nlooking at the functional L as expressed in Eq. (4.48), one may recall the action and\nthe Hamilton principle, i.e., a classical trajectory of mechanical variables (x, p) minimizes\nthe total action which is the time integral of the Lagrangian, which is very similar to the\nform of Eq. (4.48). Therefore, if we construct a Lagrangian L deﬁned with x and p such\nthat minimization of\n\u0000R\nL dt\n\u0001\ncorresponds to minimization of the loss L, then we can\nobtain a trajectory of variables (⟨ˆx⟩, ⟨ˆp⟩) as the classical trajectory of (x, p) that mini-\nmizes the loss L. After we obtain the desired trajectory, an external control is applied\nto keep the quantities (⟨ˆx⟩, ⟨ˆp⟩) such that they stay on the desired trajectory. This com-\npletes a simple derivation of the so-called linear-quadratic optimal control for our system.\nFollowing this argument, we deﬁne L = m ˙x2\n2 + kx2\n2 = T −V, where the classical kinetic\nenergy is T =\np2\n2m = m ˙x2\n2\nand the potential is V = −kx2\n2 . Note that a Lagrangian must\nbe deﬁned in the form of L(q, ˙q, t) so that for this functional the Hamilton principle\nholds [67]. The action\n\u0000R\nL dt\n\u0001\nis equal to the loss L, and therefore a classical particle\ntravelling in the potential V has mechanical variables (x, p) which minimize L when they\nare substituted by (⟨ˆx⟩, ⟨ˆp⟩), as ⟨ˆx⟩and ⟨ˆp⟩are constrained by the same relation as x\nand p, i.e.\nd\ndt⟨ˆx⟩= ⟨ˆp⟩\nm (Eq. 4.47) and\nd\ndtx =\np\nm, which makes sure that when x is sub-\nstituted by ⟨ˆx⟩, p is substituted by ⟨ˆp⟩. From a viewpoint of optimization, we see that\nthe minimization of L is done under the constraint\nd\ndt⟨ˆx⟩= ⟨ˆp⟩\nm , which is achieved by the\nsame constraint\nd\ndtx = p\nm of the classical Lagrangian. Therefore, all necessary conditions\nare indeed satisﬁed, and a trajectory of (⟨ˆx⟩, ⟨ˆp⟩) which minimizes L must be a classical\nphysical trajectory of (x, p) for Lagrangian L, and we should use the control to achieve\nsuch a trajectory.\nNext, we consider what trajectories of L can be used to minimize L. Because of the\nunstable potential V = −kx2\n2 which is high at the center and low at both sides, a classical\nparticle would have a divergent total action\n\u0000R ∞L dt\n\u0001\nunless the particle precisely stops\nat the top of the potential with a zero momentum, in which case the action becomes\nnon-divergent. Therefore, we speciﬁcally look at the conditions under which it can be\nnon-divergent. In order to precisely stop at the top of the potential V, as a ﬁrst condition\nits velocity and position should have opposite signs, so that it moves towards the center,\nand as a second condition it needs to dissipate all its energy when exactly reaching the\ntop, i.e.\np2\n2m −kx2\n2 = 0. Therefore, the trajectory of the particle’s (x, p) satisﬁes\np = −\n√\nmk x.\n(4.49)\n36\nChapter 4. Control in Quadratic Potentials\nThis is the main result of our optimal control.\nThen, whenever the above condition is not satisﬁed for our state with (⟨ˆx⟩, ⟨ˆp⟩), we\napply control Fcon to inﬂuence the evolution of ⟨ˆp⟩so that it changes to satisfy the con-\ndition. If Fcon is not bounded, we can modify ⟨ˆp⟩in an inﬁnitesimal length of time to\nsatisfy the condition quickly, and then keep a moderate strength of Fcon to keep ⟨ˆp⟩al-\nways satisfying it. This is the optimal control which minimizes L if the system variables\nevolve according to Eq. (4.47), which is deterministic and does not include noise.\nThe important but diﬃcult ﬁnal step is to show that, when measurement backaction\nnoise is included as in Eq. (4.45), the above control strategy is still optimal. This is called\nthe separation theorem in the context of control theory, and it is not straightforward to\nprove. Therefore, we resort to the standard Linear-Quadratic-Gaussian (LQG) control\ntheory [5] and prove it in the context of control theory in Sec. A.2 in appendices. Since\nthe reasoning follows a diﬀerent line of thoughts, we do not discuss it here further.2\nRegarding the case of k < 0, in which the system amounts to an inverted pendulum,\nwe consider the minimization of a loss deﬁned as\nL =\nZ \u0012⟨ˆp⟩2\n2m + |k|\n2 ⟨ˆx⟩2\n\u0013\ndt\n(4.50)\nso that when it is minimized, both the position and momentum are kept close to zero,\nand therefore the particle stays stable near the origin of the x coordinate. This makes the\nproblem the same as before and produces the same optimal trajectory condition, that is\np = −\np\nm|k| x,\n(4.51)\nand we use this as the conventional optimal control strategy for the inverted harmonic\npotential problem.\n4.2\nNumerical Experiments\nIn this section, we describe the settings of our numerical experiments of the simulated\nquantum control in quadratic potentials under continuous position measurement. De-\ntailed settings concerning speciﬁc deep reinforcement learning techniques and the corre-\nsponding hyperparameters are given in Appendix C.\n4.2.1\nProblem Settings\nFirst, we describe our settings of the simulated quantum system and the control.\n2A more general and rigorous proof can be found in Ref. [68].\n37\nChapter 4. Control in Quadratic Potentials\nLoss Function\nAs discussed in previous sections, we set the targets of the control to be minimizing the\nenergy of the particle and keeping the particle near the center respectively for the har-\nmonic oscillator and the inverted harmonic potentials. Because the problem is stochastic,\nthe minimized quantity is actually the expectation of the loss, written as the following:\nE[L1] = E\n\u0014\nlim\nT→∞\n1\nT\nZ T\n0\n\u0012⟨ˆp⟩2\n2m + k\n2⟨ˆx⟩2\n\u0013\ndt\n\u0015\n,\n(4.52)\nE[L2] = E\n\u0014 1\nT\nZ T\n0\ng (⟨ˆx⟩, ⟨ˆp⟩) dt\n\u0015\n,\n(4.53)\nwhere E[·] denotes the expectation value over trajectories of its stochastic variables, and\ng is a function which judges whether the particle is away from the center (falling out)\nor is near the center (staying stable). The function g is 1 when the particle is away,\nand is 0 otherwise. In our numerical simulation of the particle, we stop our simulation\nwhen the particle is already away and take g = 1 afterwards. Therefore, we use a large\nT rather than taking its inﬁnite limit; otherwise it approaches 1. Concerning controls,\nwe use deep reinforcement learning to learn to minimize these two quantities as reviewed\nin Chapter 3. However, the optimal control discussed in the last section only applies to\nloss functions of quadratic forms, and does not directly apply to Eq. (4.53). In order\nto obtain a control strategy for the inverted potential problem, we need to artiﬁcially\ndeﬁne a diﬀerent loss function to use optimal control theory. This is done in Eq. (4.50).\nWe use the optimal control derived from the artiﬁcially deﬁned loss there and compare\nits performance with the deep reinforcement learning that directly learns the original loss.\nSimulation of the Quantum System\nAlthough we have a set of equations (4.45) which eﬀectively describes the time evolution\nof the system, we still decide to numerically simulate the original quantum state in its\nHilbert space. This is because we want to conﬁrm that reinforcement learning can di-\nrectly learn from a numerically simulated continuous-space quantum system without the\nneed of simpliﬁcation, and at the same time numerical error and computational budget\nare still acceptable. After this is conﬁrmed, we may carry our strategy to other problems\nthat are more diﬃcult and cannot be simpliﬁed. Another reason for simulating the orig-\ninal quantum state is that, we input the quantum state directly to the neural network to\nmake it learn from the state as well, and therefore we need the state.\nTo simulate the quantum system, we express the state in terms of the energy eigen-\nbasis of the harmonic potential V = |k|\n2 ˆx2. This simulation strategy is precise and ef-\nﬁcient, because the state is Gaussian and thus can be expressed in terms of squeezing\nand displacement operators, which result in exponentially small values in high energy\ncomponents of the harmonic eigenbasis. We set the energy cutoﬀof our simulated space\nat the 130-th excited state, and whenever the component on the 120-th excited state\nexceeds a norm of 10−5, we judge that the numerical error is going to be high and we\n38\nChapter 4. Control in Quadratic Potentials\nstop the simulation. We call this as failing, because the controller fails to keep the state\nstable around the center; otherwise the high energy components would not be large. This\nis used as our criterion to judge whether a control is successful or not.\nTo reduce the computational cost, we only consider pure states, and the time-evolution\nequation is Eq. (2.31), i.e.,\nd|ψ⟩=\n\u0014\u0012\n−i\nℏH −γ\n4(ˆx −⟨ˆx⟩)2\n\u0013\ndt +\nrγ\n2(ˆx −⟨ˆx⟩)dW\n\u0015\n|ψ⟩,\nwhere the Hamiltonian H is\nH = ˆp2\n2m + k\n2 ˆx2 + Fconˆx.\n(4.54)\nAlso, due to the time-evolution equations of ⟨ˆx⟩and ⟨ˆp⟩(Eq. (4.45)), incomplete informa-\ntion on measurement outcomes, i.e. η < 1, does not change the optimal control strategy,\nsince the strategy merely depends on k and m. Thus, we have the fact that the optimal\ncontrol is the same for both a pure state with complete information and a mixed state\nwith partial measurement information. As shown in Eq. (4.45), the diﬀerence between\nincomplete and complete measurement information in this problem is only at the size\nof the additive noise, which does not essentially aﬀect the system behaviour. Therefore,\nwe do not experiment on mixed states with incomplete measurement results for simplicity.\nTo numerically simulate the time-evolution equation, we discretize the time into time\nsteps and do iterative updates to the state. The implemented numerical update scheme\nis a mixed explicit-implicit 1.5 order strong convergence scheme for Itˆo stochastic diﬀer-\nential equations with additional 2nd and 3rd order corrections of deterministic terms. It\nis nontrivial and is described in Section B.2 of Appendix B. We numerically veriﬁed that\nour method has small numerical error, and speciﬁcally, the covariances of our simulated\nstate diﬀer from the calculated ones (see Eq. (4.44)) by an amount of 10−4 ∼10−6 when\nthe state is stable, and diﬀer by an amount of 10−3 ∼10−4 when the simulated system\nfails, which is always below 10−2. Therefore, we believe that our numerical simulation\nof this stochastic system is suﬃciently accurate. An example of the simulated system is\nplotted in Fig. 4.1.\nTo initialize the simulation, the state is simply set to be the ground state of the har-\nmonic eigenbasis at the beginning, and then it evolves under the position measurement\nand control. The state leaves the ground state because of the measurement backaction,\nand it continuously gains energy if no control force is applied. Thus to keep the state\nat a low energy, it is necessary to make use of the control. When the total simulation\ntime exceeds a preset threshold tmax or when the simulation fails, we stop the simulation\nand restart a new one. We call one simulation from the start to the end as an episode,\nfollowing the usual convention of reinforcement learning.\n39\nChapter 4. Control in Quadratic Potentials\n(a)\n(b)\nFigure 4.1: Examples of controlled wavefunctions ψ in the problems of cooling a harmonic oscil-\nlator (a) and stabilizing an inverted oscillator (b), plotted in x space, together with schematic\nplots of the controlled potential (grey) and the probability distribution density (red). The real\npart and the imaginary part of the wavefunctions are plotted in blue and orange, respectively.\nThe units of the horizontal axis is\nq\nℏ\nmω.\nConstraints of Control Force\nIn practice, the applied external control force Fcon must be ﬁnite and bounded. Because\nthis external force as shown in Eq. (4.54) is equivalent to shifting the center of the poten-\ntial by an amount of Fcon\nk , we compare the width of the wavefunction and the distance\nof the potential shift, and keep them to be of the same order of magnitude. The wave-\nfunctions in our experiments have standard deviations in the position space of around\n0.67 and 0.80 in units of\nq\nℏ\nmω for the harmonic oscillator problem and the inverted oscil-\nlator problem, and therefore the wavefunctions have widths of around 2.7 and 3.2. The\nallowed shifts of potentials for these two problems are correspondingly set to be [−5, +5]\nand [−10, +10]3, all in units of\nq\nℏ\nmω. In our numerical experiments, we ﬁnd that the\ninverted problem is quite unstable. On the one hand, the noise is intrinsically unbounded\nas a Gaussian variable so it can overcome any ﬁnite control force; on the other hand, in\nthe inverted potential, any deviation from the center makes the system harder to control\nsince the particle tends to move away, and when it has deviated too much, the bounded\ncontrol force may not be strong enough to work well. This is why we have set the allowed\ncontrol force for the inverted problem to be larger. In the harmonic oscillator case, no\nmatter how far the noise makes the particle deviate from the center, the particle always\ncomes back again by oscillations, and a control force can always have a favourable eﬀect\non the particle to reduce its momentum as desired. This is why we have set the allowed\ncontrol force for the harmonic oscillator problem to be small. In our experiments, we\n3We do not use penalty on large control forces to prevent the divergence of control as done in the\nusual linear-quadratic control theory. This is because we also do not put penalty on any control choice\nof our neural network output and we want to compare the two strategies fairly.\n40\nChapter 4. Control in Quadratic Potentials\nFigure 4.2: An example of the optimal control 1\nkFcon and the average position ⟨ˆx⟩in the cooling\nharmonic oscillator problem, plotted against time t of the system. The units are consistent with\nTable 4.1. It can be seen that the control force Fcon is almost random, with a mean of zero,\nand ⟨ˆx⟩ﬂuctuates around zero.\nfound that these allowed control force regions are suﬃcient to demonstrate the eﬃcient\ncooling and control of the particle, as demonstrated in Section 4.3.1.\nTo make the control practical, we do not allow the controller to change its control\nforce too many times during one oscillation period of the system, i.e. within one time\nperiod 2π\nω , where ω =\nr\n|k|\nm . This condition is imposed both for the harmonic and the\ninverted oscillator problems. We set that the controller can output 36 diﬀerent control\nforces in one oscillation period 2π\nω , which amounts to 18 controls in half an oscillation\nperiod, i.e. the particle moving from one side to the other and changing the direction,\nor 9 controls in a one-quarter period. Our choice of this speciﬁc number here is only for\ndivisibility regarding the time step of the simulated quantum system. A control force\nFcon is applied to the system as a constant before the next control force is outputted from\nthe controller. Therefore, the control forces on the system regarded as a function of time\nbecome a sequence of step functions, and we call each constant step in it as a control\nstep, in comparison to the time step of the numerical simulation.\nThen, we need to speciﬁcally consider the implementation of the optimal controls. As\ncontrol forces are bounded and discretized in time, we consider variations of the origi-\nnal continuous optimal control to satisfy these constraints, which is common in control\ntheory. First, because each force is applied as a constant during the control step, the\ntarget of the control should be adapted and set to that the particle should be on the\ndesired optimal trajectory at the end of a control step. We do so by solving for the con-\ntrol force Fcon using the time-evolution equation (4.47) and the current state (⟨ˆx⟩, ⟨ˆp⟩),\nwith the target state (⟨ˆx⟩+ d⟨ˆx⟩, ⟨ˆp⟩+ d⟨ˆp⟩) satisfying the optimal trajectory condition\nin Eq. (4.51). This is solved by simply taking dt in Eq. (4.47) to be the time of the\ncontrol step and expanding d⟨ˆx⟩and d⟨ˆp⟩up to dt2; this strategy is fairly accurate since\nwe repeat it 18 times per one half of the oscillation period. The resulting control Fcon\nis linear with respect to the system variables (⟨ˆx⟩, ⟨ˆp⟩), and it is called a linear control.\nFinally, we bound Fcon by simply clipping it within the required bounds, ignoring the\n41\nChapter 4. Control in Quadratic Potentials\nexcessive values that are beyond the constraints. An example of the resulting control is\nshown in Fig. 4.2.\nParameter Settings\nMany parameters of the simulated quantum system are redundant and can actually be\nrescaled to produce the same quantum evolution. Therefore, most of the parameters are\narbitrary and we summarize our settings in Table 4.1.\nω (ωc)\nm (mc)\nk (mcω2\nc)\ndt ( 1\nωc)\nη\nγ (mcω2\nc\nℏ)\nπ\n1\nπ\n±π\n1\n720,\n1\n1440\n1\nπ, 2π\nnmax\nFcon (\np\nℏmcω3\nc)\nNcon\ntmax ( 1\nωc)\n130\n[−5π, +5π], [−10π, +10π]\n18\n100\nTable 4.1: Parameter settings of the simulation of quadratic potentials under the position\nmeasurement. Units are shown in the parenthesis. The physical quantities mc and ωc are used\nas a reference, and the parameters without units are dimensionless. The values are separated by\na comma to show the speciﬁc settings for the harmonic (left) and the inverted (right) oscillator\nproblems.\nIn Table 4.1, η denotes the measurement eﬃciency, and because we only consider pure\nstates, it is 1; γ is the measurement strength, and we set it so that the size of the wavefunc-\ntion is about the same as that of the ground-state wavefunction of a harmonic oscillator.\nFor the harmonic oscillator γ = π, and for the inverted oscillator γ = 2π, and even if γ\nis changed, our numerical simulation is still expected to produce similar results. In both\nthe harmonic and the inverted problems we simulate the time evolution of the state only\ntill time tmax. Other parameters include nmax, the high-energy cutoﬀ, dt, the simulation\ntime step, the control forces Fcon that is shown in ranges, and Ncon, the number of output\ncontrols from the controller per unit time\n1\nωc. The oscillation period of the harmonic\nsystem here is exactly 2 ×\n1\nωc, which is our time scale. For example, for each simulation\nepisode we simulate for time 100× 1\nωc, i.e., 50 oscillation periods of the harmonic oscillator.\nEvaluation of Performance\nIn this subsection, we explain how we evaluate the performances of diﬀerent controllers.\nThe performances are evaluated according to our numerical simulation, using the con-\ntrollers to output control forces at every control step. Because the goals for the harmonic\noscillator and the inverted oscillator problems are diﬀerent, we use diﬀerent methods to\nevaluate them.\n42\nChapter 4. Control in Quadratic Potentials\nFor the problem of cooling a harmonic oscillator, we run the numerical simulation with\ncontrol for 1000 episodes4, and sample the expected energy of the state as the phonon\nnumber ⟨ˆn⟩for 4000 times in these episodes, and calculate the sample mean and esti-\nmate the standard error of the mean. We ﬁrst initialize the state and let it evolve under\ncontrol for 20 periods of oscillations, i.e. time 40 × 1\nωc, and then sample its ⟨ˆn⟩per time\n15× 1\nωc, till the end of the episode. This is to remove the eﬀect of initialization and sample\ncorrelations during sampling. The value ⟨ˆn⟩is evaluated at the control steps, which is\nfor consistency with the optimal control target that aims to put the state on the desired\ntrajectory at the end of control steps. In this way we numerically evaluate the expected\nenergy of the particle being controlled, or, the expected value of the loss function as in\nEq. (4.52).\nFor the inverted oscillator problem, we use a totally diﬀerent measure of performance.\nWe use the failure events to judge whether the controlled particle is near or away from the\ncenter, which means that, if the simulated system fails (i.e., the components on eigenbasis\nn = 120 exceeds the norm of 10−5), then the criterion function g in Eq. (4.53) equals\n1; otherwise g equals 0. In our numerical simulation, failure events correspond to the\ncenter of the wavefunction staying near x = 8 or x = 9 in units of\nq\nℏ\nmω, while the control\nforce is allowed to move the center of the potential to x = 10 at most. For simplicity, we\nconsider a failure event in some time interval as a Bernoulli distribution of “to fail” and\n“not to fail”, and we use the failing probability in one episode as our measure of control\nperformance. To evaluate a controller, we run simulations of 1000 episodes to estimate\nits failing probability, and the variance of the estimation is calculated based the formula\nof binomial distributions.\n4.2.2\nReinforcement Learning Implementation\nIn this section we describe how neural networks are trained to learn control strategies. We\nuse deep Q-learning, and since we mainly follow Section 3.2.3 which is already explained\nin detail, we only discuss our speciﬁc settings for the problems concerned here.\nWe use Pytorch [69] as our deep learning library, and when there are settings not\nmentioned in this thesis, it means that we use the default ones. For example, we use the\ndefault random initialization strategy for neural networks provided by Pytorch, and we\ndo not manually set random seeds for the main process. As required, we sample random\nnumbers in the main process to set the random seeds of subprocesses, which interact with\nthe environment as reinforcement learning actors to accumulate experiences.\nNeural Network Architecture\nNeural network architecture is mostly related to the structure of its input, and therefore\nwe need to decide what is the input to the neural network ﬁrst. We consider three cases.\n4Sometimes we simulate for more episodes.\n43\nChapter 4. Control in Quadratic Potentials\nThe ﬁrst is to use the quintuple (⟨ˆx⟩, ⟨ˆp⟩, Vx, Vp, C) as the input, because we know that\nthis quintuple completely describes a Gaussian state, and actually it is already more than\nenough to determine the optimal control. The second case is to use the raw values of the\nwavefunction as the input, by taking all real-part and imaginary-part values of its compo-\nnents on the chosen eigenbasis. The third case is to use the measurement outcomes as the\ninput. One or two measurement outcomes are not suﬃcient to determine the properties\nof the current state, and therefore we need to input many outcomes that are sequential\nin time into the neural network. In this case, the neural network is either a convolutional\nnetwork or a recurrent neural network; otherwise it cannot process so many values or-\ndered sequentially. Roughly speaking, we need the neural network to learn appropriate\ncoarse-graining strategy for such a large number of measurement outcomes.\nFor the ﬁrst and the second cases, the neural network is 4-layer fully connected feed-\nforward neural network with the numbers of internal units being (512, 256, 256+128,\n21+1), where the ﬁnal two layers are separated into two diﬀerent branches of computa-\ntion as suggested in Ref. [56] (see Sec. C.1.3). The 21 outputs are used to predict the\naction values deviating from their mean for diﬀerent control choices, and the last 1 output\nis used to predict the mean action value. For the third input case, the neural network\nis a 6-layer feedforward neural network, with its ﬁrst three layers being 1-dimensional\nconvolutional layers, and others being fully connected ones. The kernel sizes of the three\nconvolutional layers and their strides are respectively (13,5), (11,4), (9,4), with the num-\nber of ﬁlters being (32, 64, 64)5. The fully connected three layers have their numbers of\nhidden units as (256, 256+128, 21+1), which is similar to the network of the previous\ncase. Following Ref. [57], the last two layers in our networks are constructed as noisy\nlayers (see Sec. C.1.5). We notice that measurement outcomes only are not suﬃcient to\npredict the state, and we also need previous control forces that have been exerted on the\nstate. Thus, we input the force and the measurement outcome data in parallel as a time\nsequence as the network input. The input time sequence contains force and measurement\noutcome history in the last 6 ×\n1\nωc time for the cooling harmonic oscillator problem and\ncontains the last 4 ×\n1\nωc time for the inverted oscillator problem, which is due to the\ndiﬀerent measurement strength γ in the two problems.\nAs discussed in Section 3.2, to implement Q-learning we need to deﬁne the action\nchoices of the control, which is supposed to be a discrete set. However, currently we only\nhave an allowed interval of control force values. Therefore, we discretize this interval\ninto equispaced 21 diﬀerent values as our 21 diﬀerent control force choices, and then we\ndo Q-learning as introduced in Section 3.2, which is the same for the harmonic and the\ninverted oscillator problems.\n5Experimentally we found that a larger size of the layers does not necessarily perform better, probably\ndue to larger noises. Also, we cannot apply batch normalization between the convolutional layers, because\nthis is a reinforcement learning task and the training target is not static.\n44\nChapter 4. Control in Quadratic Potentials\nTraining Settings\nAs we have used many deep reinforcement techniques, the hyperparameter settings rel-\nevant for the speciﬁc techniques are put into Appendix C. In this subsection we only\ndescribe the settings and strategies that are relevant to the discussion we have presented\nso far.\nFirst, in order to optimize the neural network with the training loss, we use the\nRMSprop algorithm [42] with its initial learning rate set to 2 × 10−4 and momentum\nset to 0.9, and the training minibatch size is 512. The γ coeﬃcient that serves as the\ntime horizon of Q-learning in Eq. (3.10) is 0.99, which means that future rewards are\ndiscounted by 0.99 for each control step. As\n1\n1−0.99 = 100 and we have 36 controls per\noscillation, the time horizon of the controller is around 10 ×\n1\nωc. We assume that this\nis suﬃcient for learning the tasks, i.e. a control does not inﬂuence the behaviour of the\nstate after 5 periods of oscillation.\nThe reward of reinforcement learning is set to the control loss times a negative sign.\nFor the cooling harmonic oscillator problem, because the value of ⟨ˆn⟩decreases to a small\nnumber when the state is cooled, we multiply the loss by 5, and we also shift it by 0.4 such\nthat it is closer to zero and can be either negative or positive. However, for the second\nand the third input cases as the learning is more diﬃcult, we ﬁnd that such large loss\nvalues make the learning process noisy and unstable, and as a result ⟨ˆn⟩never decreases\nto a small value and the learning does not proceed. Therefore, for the second and third\ninput cases we only rescale the loss by 2, and we shift it by 1. In addition, we stop the\nsimulation whenever ⟨ˆn⟩> 10 for the second input case or ⟨ˆn⟩> 20 for the third input\ncase is satisﬁed. With these modiﬁcations, the learned loss values are always reasonably\nsmall and the learning can proceed. Since the learned states always have low energy, in\nthe wavefunction input case we only use the ﬁrst 40 energy eigenbasis of the state as input.\nRegarding the inverted oscillator problem, experimentally we ﬁnd that the learning\nis unstable if we give a negative reward only at the moment when the simulation fails.\nThis is probably due to the stochasticity of the system, because the stochasticity may\npush a state at the edge of failure either to fail or to turn back, which makes the training\nloss very high due to the unexpected behaviours of the states. In this case, the reward\nis called to be sparse. To alleviate this problem, we add a small portion of negative ⟨ˆn⟩\ninto the reward to facilitate the learning, which is multiplied by a factor of 0.02, with the\noriginal reward of a failure event being −10.\nWe use a memory replay buﬀer to store accumulated experiences of our reinforcement\nlearning actors, and its size is set to containing a few thousands or hundreds of tmax\nepisodes. Due to diﬀerent sizes of the inputs of diﬀerent input cases, the sizes of the\nreplay buﬀers are made diﬀerent since we have limited computer memory. The sizes for\nthe three cases are respectively 6000, 4000 and 400 episodes. To compromise, when new\nexperiences are stored into the memory replay, we take away the experiences that are\nunimportant ﬁrst, i.e. with low training loss values, so that important pieces of memory\nare preserved. We also use prioritized sampling during training as in Ref. [54], which is\n45\nChapter 4. Control in Quadratic Potentials\ncooling harmonic\noscillators\nstabilizing\ninverted\noscillators\nNetwork\nInput\n(⟨ˆx⟩, ⟨ˆp⟩, Vx, Vp, C)\n0.3252 ± 0.0024\n85.4% ± 1.1%\nwavefunction\n0.3266 ± 0.0040\n73.3% ± 1.4%\nmeasurement out-\ncomes\n0.4505 ± 0.0053\n0.0%\n(7.2%±1.2% provided\nwith examples)\noptimal control\n0.3265 ± 0.0032\n89.8% ± 1.0%\nTable 4.2: Performance results for the problem of cooling harmonic oscillators and stabilizing\ninverted oscillators. The numbers behind ± signs show the estimated standard deviations of the\nreported means. As discussed in Sec. 4.2.1, the values reported for the cooling problem are the\naverage phonon numbers ⟨ˆn⟩and for the inverted problem are the success rates in one episode,\ni.e., one minus the probability of failure.\ndescribed in more detail in Sec. C.1.4 and C.2.2.\nTo encourage exploring diﬀerent possibilities of control, we make the reinforcement\nlearning actors use the ϵ-greedy strategy to take actions [53], i.e., the an action is ran-\ndomly taken with a small probability ϵ, and with a probability 1 −ϵ it is taken to pursue\nthe highest expected reward (see Sec. C.1.1). This probability ϵ is 40% at the beginning\nof training and then decreases rapidly to 2%, and after several stages of decrease, it is\nsuppressed to 0.01% around the end of training.\nThe number of simulated episodes for training is around 10000 for the problem of cool-\ning a harmonic oscillator and is around 30000 for the problem of stabilizing an inverted\noscillator. Actually the simulation of the inverted problem fails quickly during the ﬁrst\na few thousand episodes, within an episode time length of around 1 or 2 ×\n1\nωc, and only\nthe later episodes last longer, which shows that this problem is indeed a diﬃcult problem.\n4.3\nComparison of Reinforcement Learning and Op-\ntimal Control\n4.3.1\nPerformances\nCooling Harmonic Oscillators\nWe now compare the performances of trained neural networks as described above with\nthe performances of our derived optimal controls. For the problem of cooling a harmonic\noscillator, the training was completed within one day on a Titan X GPU with 10-process\nparallelization running in Python, for each of the three input cases. The resulting per-\nformances in terms of ⟨ˆn⟩are listed in Table 4.2, compared with the optimal control.\nWe can see that the performances except for the measurement input case are within one\n46\nChapter 4. Control in Quadratic Potentials\nstandard error, and therefore we conclude that they perform approximately the same.\nFor completeness, we analytically calculate the theoretical optimal performance for this\nproblem if provided with unbounded control. Assuming that ⟨ˆp⟩= −\n√\nmk⟨ˆx⟩always\nholds6, the time evolution of ⟨ˆx⟩becomes an Ornstein–Uhlenbeck process and we can\ncalculate the expectation E [⟨ˆx⟩2] and evaluate the loss given in Eq. (4.52). The result is\n⟨ˆn⟩≈0.2565, which is actually smaller than the above values. This discrepancy comes\nfrom the ﬁnite time length of our allowed control steps. If the number of control steps\nNcon in time\n1\nωc increases to 72, the performance of our optimal control is evaluated to\nbe ⟨ˆn⟩≈0.2739 ± 0.0047, which is closer to the theoretical value and shows that our\noptimal control strategy is indeed valid. Therefore, by comparing with the optimal con-\ntrol in Table 4.2, we can argue that our reinforcement learning is successful. Note that\nactually ⟨ˆn⟩can never be reduced to zero, because the position measurement squeezes\nthe wavefunction in space and the term ⟨ˆn⟩ρ0 in Eq. (4.43) is non-zero.\nAs for the measurement-outcome-based input case, the lower performance may result\nfrom the intrinsic stochasticity of the measurement outcomes that disturb the process of\nlearning. In deep learning practices, injected random noise in training data is typically\nfound to decrease the ﬁnal performance, and therefore, the measurement input case may\nhave been aﬀected similarly. This is possibly due to the interplay among the noise and\nthe deep network structure and the gradient-based optimization. There seems to be no\nstraightforward solution, and in this case it may be diﬃcult for the reinforcement learning\nto learn from measurement outcomes alone.\nStabilizing Inverted Oscillators\nRegarding the inverted harmonic potential problem, as the system is considerably more\nstochastic, a neural network is trained for three days for each input case. The result-\ning performances represented as success rates are given in Table 4.2, i.e., one minus the\nprobability of a failure event occurring in an episode. Here we ﬁnd that only the ﬁrst\ninput case achieves a performance comparable to the optimal control, and that the per-\nformance roughly decreases with the increased diﬃculty of learning for the three input\ncases. To conﬁrm that the measurement-outcome-based network can really learn, we add\n100 episodes that are controlled by the optimal control into its memory replay as exam-\nples, such that when the network is trained, it learns both its own experiences and the\nexamples given by the optimal control. In this case, the network performed better, and\nat the end of training it achieved a success rate of 7.2%, which implies that this network\nis indeed possible to complete the given task, but with much more diﬃculty.\nIn order to make a fairer comparison between the optimal control and the reinforce-\nment learning controller, we consider a discretized version of the optimal control that has\ncontrol forces discretized in the same way as the reinforcement learning controller. In or-\nder to obtain discretized values, we set the outputs of the optimal control to their nearest\nneighbouring values among the equispaced 21 choices of forces as the neural network, and\n6From the perspective of the LQG theory, this condition can be obtained by taking the small control\ncost limit.\n47\nChapter 4. Control in Quadratic Potentials\nFigure 4.3: The control force plotted against the (⟨ˆx⟩, ⟨ˆp⟩) input for the problem of cooling a\nharmonic oscillator. The left panel shows the force of the optimal control, and the right one\nshows the control force of a trained reinforcement learning actor.\nthen use the forces as the control. The performance of this discertized controller is eval-\nuated, and the result is 89.3% ± 1.0%, within one standard deviation compared with the\ncontinuous controller, which demonstrates that the discretization does not signiﬁcantly\ndecrease the controller’s performance, and the performance of the reinforcement learning\nis not restricted by the discretization.\nFinally we consider an even more discretized control strategy, the bang-bang protocol.\nThe bang-bang protocol uses one of the extrema as its output, and in our case it outputs\neither the maximal force to the left or the maximal force to the right. If the optimal\ncontrol has an output to the left, its variation as a bang-bang protocol outputs the left\nmaximal output force, and vice versa. We then test the performance of this bang-bang\nprotocol variation. Its performance is 71.0% ± 1.4%, which is signiﬁcantly lower. This\nbang-bang protocol always has the right direction of the control force, but its strength\nis larger than the optimal control and therefore the controlled system is perturbed more\nstrongly. This shows that the inverted oscillator system is quite sensitive, and increased\nnoise and disturbance reduce the stability of the system. Obviously this bang-bang pro-\ntocol is a suboptimal control strategy, and both the best reinforcement learning control\nand our optimal control have performances superior to it.\n4.3.2\nResponse to Diﬀerent Inputs\nTo see what the reinforcement learning has learned in detail, we plot its output control\nforce against diﬀerent inputs of (⟨ˆx⟩, ⟨ˆp⟩), ﬁxing (Vx, Vp, C) at their average values. For\nthe harmonic oscillator problem, the control force regarding (⟨ˆx⟩, ⟨ˆp⟩) is given in Fig. 4.3,\nand for the inverted oscillator problem it is given in Fig. 4.4, and they are compared with\nthe optimal controls. The leftmost and rightmost tails are the regions where the state is\nboth staying away and moving away quickly, in which case it is almost destined to fail,\nand the central cliﬀof the plots is where the state stays most of the time. If the cliﬀ\nbecomes vertical, the control strategy reduces to the bang-bang protocol.\nFrom the above ﬁgures we see that, the reinforcement learning controllers have learned\n48\nChapter 4. Control in Quadratic Potentials\nFigure 4.4: The control force plotted against the (⟨ˆx⟩, ⟨ˆp⟩) input for the problem of stabilization\nin an inverted harmonic potential. The left panel shows the the optimal control, and the right\none shows our trained reinforcement learning actor.\nqualitatively the same as the optimal control, and therefore we may say that they have\nlearned the underlying properties of the system. However, some defects exist in their\ncontrol decisions, which may be attributed to noise and insuﬃcient training. Especially\nfor the inverted oscillator problem, while the controller has 21 choices of control, it prefers\noutputting 4 or 5 choices around the central cliﬀin Fig. 4.4, though all control choices\nare equispaced. This is shown more clearly in Fig. 4.5. We suspect this to be an artefact\nof reinforcement learning. The reinforcement learning actor may discover when to use\nsome of its control choices at the beginning, and then it converged to them rather than\nlearning some other choices. This artefact may be suppressed by reﬁnement of the train-\ning procedure, and the ﬁnal performance may potentially increase. One reason for this\nartefact is that, in order to conﬁrm that the performance of the neural network is lower\nthan that of the optimal control, we need one thousand episodes of simulations, which\namounts to 2 × 106 control steps, and therefore similarly, it is not easy for the reinforce-\nment learning to discover that its strategy can still be improved. Also, the time horizon\nof control is set to be\n1\n10 of an episode as discussed earlier, and therefore it may hard for\nthe reinforcement learning to learn an optimal strategy on such long-time controls.\n4.4\nConclusion\nIn this chapter, we have shown that the problems of controlling a particle near the center\nof quadratic potentials with position measurement has optimal control solutions, and we\nhave shown that a deep-reinforcement-learning-based controller can learn such a problem\nand control the system with a performance comparable to the optimal control. Also, the\ntraining of the reinforcement learning is within practical computational budgets, which is\nnot speciﬁc to this quadratic case. We expect that similar reinforcement-learning-based\ncontrollers can be trained following the same line for other potentials where no optimal\nor reasonable control strategies are known, and we may carry the same settings to those\nother problems when possible. Therefore, naturally we move to the case of quartic po-\ntentials in the next chapter.\n49\nChapter 4. Control in Quadratic Potentials\nFigure 4.5: The control force of trained reinforcement learning plotted against (⟨ˆx⟩, ⟨ˆp⟩) input\nfor the inverted harmonic potential problem. This is a ﬁner and enlarged version of the right\nﬁgure of Fig. 4.4.\n50\nChapter 5\nControl in Quartic Potentials\nIn this chapter, we follow the same line as the last chapter to discuss controlling a quan-\ntum particle in a one-dimensional quartic potential, and the target of control is to keep\nthe particle at a low energy. Unlike the last chapter, we do not consider inverted potential\nproblems, because inverted anharmonic potentials are of less practical interest. We ﬁrst\nshow that the properties of the quartic system is totally diﬀerent from the quadratic one\nand cannot be simpliﬁed in the same way in Section 5.1.1, and we derive some control\nstrategies analogous to the optimal control of the quadratic case with some approximation\nin Section 5.1.2. Our experimental settings to simulate quartic potentials are diﬀerent\nfrom the quadratic case and we explain those settings in Section 5.2. Based on the set-\ntings, we use deep reinforcement learning to control the system and present its control\nresults in Section 5.3 and compare the results to our derived approximate control strate-\ngies. Finally, in Section 5.4 we present the conclusions of this chapter.\n5.1\nAnalysis of a Quartic Potential\nIn this section we show that a quartic potential cannot be simpliﬁed in the same way\nas a quadratic potential by revisiting the arguments presented in Section 4.1. To obtain\nreasonable control strategies, we consider approximations to the time evolution of the\nsystem and present the corresponding derived controls, which serve as the comparison\ngroup as conventional control strategies. To make the properties of a quartic potential\nclear, we use ﬁgures to illustrate the behaviour of a particle inside a quartic potential\nbefore going into the next section.\n5.1.1\nBreakdown of Gaussian Approximation and Eﬀective De-\nscription\nFollowing Section 4.1, we consider the time evolutions of operators ˆx and ˆp in the Heisen-\nberg picture for the Hamiltonian with a quartic potential:\nH = ˆp2\n2m + λˆx4,\n(5.1)\n51\nChapter 5. Control in Quartic Potentials\nd\ndt ˆx = i\nℏ[H, ˆx] = ˆp\nm,\nd\ndt ˆp = i\nℏ[H, ˆp] = −4λˆx3,\n(5.2)\nwhich involves the third-order term of ˆx. Therefore, its time evolution is not a simple\nlinear transformation between operators ˆx and ˆp. It is nonlinear, and therefore the shape\nof its Wigner distribution is not preserved. Also, since the Hamiltonian is more than\nquadratic, the time-evolution equation of the Wigner distribution is diﬀerent from the\nclassical Liouville equation in phase space, and it is known that there exists no trajectory\ndescription of the Wigner distribution [70], which means that there is a non-classical\neﬀect.\nWe then consider the time evolution of the average momentum ⟨ˆp⟩. As shown in\nEq. (5.2), we have\nd\ndt⟨ˆp⟩= −4λ⟨ˆx3⟩,\n(5.3)\nand if Gaussian approximation holds, ⟨ˆx3⟩can be calculated as in Eq. (4.22) exploiting\nthe zero skewness property of the Gaussian. Therefore, we verify whether or not its zero\nskewness can always be kept zero as a Gaussian state in a quartic potential. The time\nevolution of skewness is\nd\n\n(ˆx −⟨ˆx⟩)3\u000b\n= tr\n\u0000dρ(ˆx −⟨ˆx⟩)3 + ρ d(ˆx −⟨ˆx⟩)3\u0001\n= tr\n\u0012 i\nℏρ\n\u0002\nH, (ˆx −⟨ˆx⟩)3\u0003\ndt −3ρ (ˆx −⟨ˆx⟩)2d⟨ˆx⟩\n\u0013\n= tr\n\u0012 i\nℏρ\n\u0014 ˆp2\n2m, (ˆx −⟨ˆx⟩)3\n\u0015\ndt −ρ 3⟨ˆp⟩\nm (ˆx −⟨ˆx⟩)2dt\n\u0013\n= 3 ⟨(ˆx −⟨ˆx⟩)ˆp(ˆx −⟨ˆx⟩)⟩\nm\ndt −3⟨ˆp⟩⟨(ˆx −⟨ˆx⟩)2⟩\nm\ndt\n= 3 ⟨(ˆx −⟨ˆx⟩)(ˆp −⟨ˆp⟩)(ˆx −⟨ˆx⟩)⟩\nm\ndt,\n(5.4)\nwhich means that the skewness of the phase-space Wigner distribution in the x direction\nis dependent on another skewness of it in the x, p plane. We then calculate the time\nevolution of ⟨(ˆx −⟨ˆx⟩)(ˆp −⟨ˆp⟩)(ˆx −⟨ˆx⟩)⟩:\nd ⟨(ˆx −⟨ˆx⟩)(ˆp −⟨ˆp⟩)(ˆx −⟨ˆx⟩)⟩= tr(dρ(ˆx −⟨ˆx⟩)(ˆp −⟨ˆp⟩)(ˆx −⟨ˆx⟩)\n+ ρ d(ˆx −⟨ˆx⟩)(ˆp −⟨ˆp⟩)(ˆx −⟨ˆx⟩))\n= 2 ⟨(ˆp −⟨ˆp⟩)(ˆx −⟨ˆx⟩)(ˆp −⟨ˆp⟩)⟩\nm\ndt\n−4λ\n\nˆx3(ˆx −⟨ˆx⟩)2\u000b\ndt + 4λ⟨ˆx3⟩\n\n(ˆx −⟨ˆx⟩)2\u000b\ndt\n= 2 ⟨(ˆp −⟨ˆp⟩)(ˆx −⟨ˆx⟩)(ˆp −⟨ˆp⟩)⟩\nm\ndt\n−4λ\n\n(ˆx3 −⟨ˆx3⟩)(ˆx −⟨ˆx⟩)2\u000b\ndt.\n(5.5)\nThe ﬁrst term of the above is still a skewness, but the second term is rather nontrivial.\n52\nChapter 5. Control in Quartic Potentials\nThis is because ˆx3 −⟨ˆx3⟩̸= (ˆx −⟨ˆx⟩)3. It appears to be non-zero, and we show this fact\nnow. We evaluate the term ⟨(ˆx3 −⟨ˆx3⟩)(ˆx −⟨ˆx⟩)2⟩for a Gaussian state:\n\n(ˆx3 −⟨ˆx3⟩)(ˆx −⟨ˆx⟩)2\u000b\n= ⟨ˆx5⟩−2⟨ˆx4⟩⟨ˆx⟩+ ⟨ˆx3⟩⟨ˆx⟩2 −⟨ˆx2⟩⟨ˆx⟩3 + ⟨ˆx⟩5.\n(5.6)\nUsing the fact that a Gaussian distribution is central-symmetric and therefore has odd\ncentral moments being zero, similar to the skewness, we have ⟨(ˆx−⟨ˆx⟩)5⟩= 0 and therefore\n⟨ˆx5⟩= 5⟨ˆx4⟩⟨ˆx⟩−10⟨ˆx3⟩⟨ˆx⟩2 + 10⟨ˆx2⟩⟨ˆx⟩3 −4⟨ˆx⟩5,\n(5.7)\nand Eq (5.6) becomes\n\n(ˆx3 −⟨ˆx3⟩)(ˆx −⟨ˆx⟩)2\u000b\n= 3⟨ˆx4⟩⟨ˆx⟩−9⟨ˆx3⟩⟨ˆx⟩2 + 9⟨ˆx2⟩⟨ˆx⟩3 −3⟨ˆx⟩5.\n(5.8)\nUsing the fact that excess kurtosis is zero for a Gaussian distribution, i.e. ⟨(ˆx −⟨ˆx⟩)4⟩\nV 2\nx\n= 3,\nwhere Vx is the variance of x, we have\n⟨ˆx4⟩= 4⟨ˆx3⟩⟨ˆx⟩−6⟨ˆx2⟩⟨ˆx⟩2 + 3⟨ˆx⟩4 + 3V 2\nx ,\n(5.9)\nand Eq. (5.8) becomes\n\n(ˆx3 −⟨ˆx3⟩)(ˆx −⟨ˆx⟩)2\u000b\n= 3⟨ˆx3⟩⟨ˆx⟩2 −9⟨ˆx2⟩⟨ˆx⟩3 + 6⟨ˆx⟩5 + 9V 2\nx ⟨ˆx⟩\n= 3\n\u00003Vx⟨ˆx⟩+ ⟨ˆx⟩3\u0001\n⟨ˆx⟩2 −9⟨ˆx⟩5 + 6⟨ˆx⟩5 −9Vx⟨ˆx⟩3 + 9V 2\nx ⟨ˆx⟩\n= 9V 2\nx ⟨ˆx⟩.\n(5.10)\nTherefore, whenever the mean ⟨ˆx⟩of the Gaussian distribution is not zero, it gradually\nloses its Gaussianity in the quartic potential, and therefore the Wigner distribution can-\nnot always be Gaussian.\nBesides the failure of the Gaussian approximation, it is known that a quartic system\nis hard to analyse since it corresponds to the 1-dimensional φ4 theory [71]. Therefore we\ngive up on analysing the system explicitly.\n5.1.2\nApproximate Controls\nAlthough the state can no longer be described by only a few parameters, we still hope\nto obtain some reasonable control strategies for it. In this subsection we discuss possible\ncontrol strategies.\nIf the system evolves deterministically, we can always ﬁnd an appropriate control\nby simulating the state under the control and repeatedly reﬁning the control to make\nthe system evolve into a desired state. This local search method of control is actually\n53\nChapter 5. Control in Quartic Potentials\nwidely used and it has many diﬀerent variations and names as introduced in Chapter\n1 [17–20]. In engineering, the famous ones include so-called diﬀerential dynamic pro-\ngramming (DDP) [72] and the iterative linear-quadratic-Gaussian method (ILQG) [73].\nHowever, these methods have strong limitations. First of all, since they search for pos-\nsible controlled evolutions of a given state, if there are N diﬀerent given states, then\nthey need to search N times, using one for each, and if we do not know the given state\nin advance, we would need to do the search immediately after the state is given, which\nis time-consuming and inappropriate as a realistic control strategy. On the other hand,\nas these methods are based on local search, they can hardly be applied to systems that\ncontain noise. This is because the methods investigate only one certain trajectory of the\nsystem and reﬁne that investigated speciﬁc trajectory by back-and-forth iteration, which\nwould result in a control that is inapplicable to other possibilities of noise. For a diﬀerent\nrandom realization of noise, the controller does not have an idea about how to control\nthe system and generally the control would fail. The control problem in this setting is\ncalled a stochastic optimal control (SOC) problem [74]. To deal with this, the ILQG\nalgorithm treats the eﬀect of noise as small deviations from the expected trajectory and\ndeals with the deviations perturbatively, but as already mentioned in the original paper,\nthis method only handles a small noise that does not signiﬁcantly aﬀect the overall evo-\nlution of the system [73]. In our case, the quartic system has noise-induced behaviour,\nwhich makes an actual trajectory drastically diﬀerent from the expected trajectory in\nthe long term, and therefore this perturbative ILQG method can hardly apply. On the\nother hand, machine learning strategies such as the graphical-model-based approximate\ninference control (AICO) are often used to handle these stochastic control problems [74].\nOverall, if we want to cool down a particle in a quartic potential with position measure-\nment and a random initial state, we actually have no existing well-established method\nthat can exploit the properties of the system to control it, except for machine learning.\nIn this case, to obtain control strategies other than machine learning as a comparison\ngroup, we give up looking for usual globally optimal or locally optimal solutions and\nconsider suboptimal controls and approximations of the system. As before, we use an\nexternal force to control the system, which is a linear term Fconˆx added to the system\nHamiltonian that is parametrized by the control parameter Fcon.\ndamping controller\nAs a ﬁrst control strategy, we consider the time derivative of the system energy. It is\ndE = d⟨H⟩= tr(dρ H) = tr(−i\nℏ[H + Fconˆx, ρ]H dt) = −Fcon⟨ˆp⟩\nm\ndt,\nH = ˆp2\n2m + λˆx4.\n(5.11)\nTherefore, whenever ⟨ˆp⟩> 0 is satisﬁed, we can use a positive Fcon to decrease the energy,\nand when ⟨ˆp⟩< 0 is satisﬁed, we can use a negative Fcon, which is the same for a classical\nsystem. This is called a steepest descent method. In numerical simulation, because we\nneed to use ﬁnite time control steps, the action of this controller should be set to remove\nthe particle’s momentum at each control step, i.e. satisfying ⟨ˆp⟩+ d⟨ˆp⟩= 0 with dt taken\n54\nChapter 5. Control in Quartic Potentials\nto be the time of the control step. However, in numerical simulation we ﬁnd that such a\nstrategy would prevent the state from moving to the center and keep it at a high energy.\nTherefore, we use a damping strategy instead, i.e., ⟨ˆp⟩+d⟨ˆp⟩= (1−ζ)⟨ˆp⟩with 0 ≤ζ < 1.\nWe experimentally found that ζ = 0.5 results in the best performance in our problem\nsetting and therefore we use the parameter setting ζ = 0.5. We call this the damping\ncontroller. Note that for a quadratic potential this strategy is not optimal, and it results\nin overdamping of the system.\nquadratic controller\nAs a second control strategy, we apply the usual linear-quadratic-Gaussian controller to\nthe quartic system. This is because linear-quadratic-Gaussian controllers are often used in\nreal situations where we have some quantities to minimize and do not care about the strict\noptimality of control [75], and therefore, we just follow this strategy. To apply the linear-\nquadratic-Gaussian (LQG) theory we need to set a quadratic control target, deﬁne system\nvariables, and linearise the system. After these procedures, the LQG theory produces a\ncontroller which is optimal on the transformed system, and we use this controller to\ncontrol our original system. In analogy to the harmonic problem in Chapter 4, we use\n⟨ˆx⟩and ⟨ˆp⟩as our system variables, which actually only contain partial information\nof the system, and we deﬁne the minimized loss as L =\nR \u0010\nk\n2⟨ˆx⟩2 + ⟨ˆp⟩2\n2m\n\u0011\ndt, where the\nparameter k is searched and determined by testing the controller’s performance. Following\nthe arguments presented in Chapter 4, with discretized control steps, the control force\nis determined such that (⟨ˆx⟩+ d⟨ˆx⟩, ⟨ˆp⟩+ d⟨ˆp⟩) satisﬁes the optimal trajectory condition\n⟨ˆp⟩= −\n√\nkm⟨ˆx⟩to the ﬁrst order of dt, with dt being the time of a control step. We call\nthis derived controller as the quadratic controller, because it only considers observables\nthat are suﬃcient for the quadratic problem case.\nGaussian approximation controller\nAs a fourth control strategy, we use a Gaussian approximation of the state and establish\nthe correspondence between (⟨ˆx⟩, ⟨ˆp⟩) of the quantum state and (x, p) of a classical par-\nticle, as done in the quadratic system case. By Eq. (5.3), we know that the evolution of\n⟨ˆp⟩depends on ⟨ˆx3⟩, which involves skewness and is not deﬁned for a classical point-like\nparticle. However, based on a Gaussian approximation, the state has zero skewness and\n⟨ˆx3⟩= 3Vx⟨ˆx⟩+ ⟨ˆx⟩3 as shown in Eq. (4.22). If we further assume that Vx is a constant,\nwe obtain a classical particle having (x, p) corresponding to (⟨ˆx⟩, ⟨ˆp⟩), where p evolves\naccording to dp\ndt = −12λVxx−4λx3, i.e. in a static potential V = 6λVxx2 +λx4. Then by\nfollowing the same Lagrangian argument in Section 4.1.3, we derive an optimal control\nprotocol for the classical particle in this quartic potential. In this way we can go beyond\nthe quadratic problem. Using Eq. (5.9), the loss under the above Gaussian approxima-\ntion is\n55\nChapter 5. Control in Quartic Potentials\nL =\nZ \u0012⟨ˆp2⟩\n2m + λ⟨ˆx4⟩\n\u0013\ndt\n=\nZ \u0012⟨ˆp⟩2 + Vp\n2m\n+ λ(4⟨ˆx3⟩⟨ˆx⟩−6⟨ˆx2⟩⟨ˆx⟩2 + 3⟨ˆx⟩4 + 3V 2\nx )\n\u0013\ndt\n=\nZ \u0012⟨ˆp⟩2 + Vp\n2m\n+ λ(6Vx⟨ˆx⟩2 + ⟨ˆx⟩4 + 3V 2\nx )\n\u0013\ndt,\n(5.12)\nand we take Vp and Vx as constants. These above assumptions are expected to hold when\nthe position measurement on the system is strong, so that the Gaussian measurement\ndominates the evolution of the shape of the state, and the quartic potential has little\neﬀect on the state.\nTo make the loss function as a classical action, we interpret the integrated term in\nEq. (5.12) as a Lagrangian, with the potential term being V = −6λVxx2 −λx4. Following\nthe same argument as Section 4.1.3, the optimal trajectory should satisfy\np = −\np\n2m(6λVx + λx2)x,\n(5.13)\nso that the particle exactly stops at the top of the potential. We call this controller as\nour Gaussian approximation controller.\nTo implement it in numerical simulation, we proceed similarly as before to set the\nstate (⟨ˆx⟩, ⟨ˆp⟩) on the optimal trajectory after applying a control step. Since the vari-\nance Vx may actually change in time, at each step we calculate the current variance to\ndetermine the control.\n5.1.3\nBehaviour of a Quartic Potential\nBefore proceeding into the next section, we use examples to demonstrate the typical\nbehaviour of a quartic potential according to our numerical simulation. In our simulation,\nwe initialize the state as a Gaussian wave packet near the center of the potential, and let it\nevolve in time. First we investigate the deterministic case with no position measurement\nimposed. The time evolution is plotted in Fig. 5.1. It can be seen that at the beginning\nthe wave is localized in space and shows oscillatory behaviour inside the potential, with its\ncenter of mass moving forward and backward. However, as the wave slowly spreads and\ndelocalize throughout time, the particle gradually spread in the bottom of the potential\nand delocalize, and the center of mass ceases to oscillate, as shown in Fig. 5.2.\n56\nChapter 5. Control in Quartic Potentials\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 5.1: Deterministic time evolution in a quartic potential V for a particle initialized as a\nGaussian wave packet in (a), and panels (a) to (f) are ordered chronologically as indicated by\nthe arrows. The grey arrows inside ﬁgures show the propagation directions of the density wave,\nand its movement of the center of mass becomes more and more obscured as time elapses. The\nparticle gradually delocalize and loses its forward-backward oscillation in position space. In the\nplots, blue and orange curves show the real and imaginary parts of the wave function, read ones\nshow the probability density distribution, and grey one show the quartic potential.\n57\nChapter 5. Control in Quartic Potentials\nFigure 5.2: Time evolution of the average position ⟨ˆx⟩of the wavefunction in Fig. 5.1 (orange\ncurve). The units of the time and position are consistent with those in the main text. As the\nstate delocalizes, its center of mass gradually ceases to oscillate.\nThe diﬀerent behaviour from quadratic potentials is mainly due to the irregular po-\ntential shape. The bottom of the quartic potential is a plateau, and the wave function\nevolves almost freely. However, the two sides of the potential abruptly increase in value\nand therefore the wave front is strongly reﬂected at both ends and interfere with the\ntail of the wavefunction, which can be observed from the distribution density plots in\nFig. 5.1(b) and 5.1(c). As the process repeats, the wavefunction becomes more and\nmore delocalized and tend to resemble a wavefunction in an inﬁnite-well potential, and\nits center-of-mass motion disappears. This is contrary to the quadratic case, where the\nshape of the wavefunction is preserved and no interference emerges.\nFigure 5.3: Time evolution of the average position ⟨ˆx⟩of the wavefunction initialized as in Fig.\n5.2 but subject to continuous position measurement. This ﬁgure is to be compared with Fig.\n5.2. At the beginning, the wavefunction is initialized with a low variance in position and is\naﬀected less by the measurement, but as it delocalizes, it experiences a stronger measurement\neﬀect and noise, and its center-of-mass oscillation is also increasingly more perturbed by the\nnoise. Concerning the probability density distribution of the wavefunction, we can observe a\nrough “envelope” which signiﬁes the center of mass.\n58\nChapter 5. Control in Quartic Potentials\n(a)\n(b)\nFigure 5.4: Fraction of the component in the harmonic-oscillator eigenbasis for states in (a)\nquartic and (b) quadratic potentials. The states are subject to the continuous position mea-\nsurement. The ordinate is plotted in a log scale.\nOn the other hand, when there is weak position measurement imposed, the properties\nof the system will change as illustrated in Fig. 5.3. The position measurement shrinks\nthe wavefunction and makes the wavefunction imbalance at the central plateau of the\npotential, and therefore the center of the wavefunction slowly oscillates with noise, which\nis in contrast to the case of Fig. 5.2. Without measurement we observe no such long-term\noscillatory behaviour. Due to this reason, it is indeed possible for those simple controllers\ndescribed in Section 5.1.2 to successfully cool down the quartic system when the position\nmeasurement is imposed. Otherwise the situation would be extremely unclear, because\nwithout the measurement-induced eﬀect both ⟨ˆx⟩and ⟨ˆp⟩tend to shrink to zero while\nthe energy of the system is high.\n5.2\nNumerical Experiments\nIn this section we discuss our experimental setting for simulating the quartic system.\nBecause the reinforcement learning implementation is almost the same as the quadratic\ncase in Chapter 4, we only brieﬂy discuss the parameter settings and choices.\n5.2.1\nSimulation of the Quantum System\nThe numerical simulation of a quartic potential is carried out in discretized real space.\nThis is because we ﬁnd that, as the wavefunction has become non-Gaussian, it is ineﬃ-\ncient to simulate the wavefunction using the eigenbasis of a harmonic oscillator. We plot\nthe norms of the wavefunction on high-energy components when it is simulated using the\neigenbasis of a harmonic oscillator in Fig. 5.4(a), and compare it with a state evolving\nin a quadratic potential as plotted in Fig. 5.4(b). As the ﬁgure shows, for a state in a\nquadratic potential which is eﬀectively Gaussian, the norms of its components on the\n59\nChapter 5. Control in Quartic Potentials\nhigh-energy part of the harmonic basis decrease exponentially, which is shown as the\nstraight line in the log scale plot in Fig. 5.4(b). However, the components for a state in\na quartic potential do not decrease exponentially as shown in Fig. 5.4(a), and thus sim-\nulating the state using the harmonic basis is both inaccurate and ineﬃcient. Therefore,\nwe change to the usual ﬁnite-diﬀerence time-domain (FDTD) method, which discretizes\nboth space and time to simulate the wave function. Our parameters and choices are\ndescribed as follows.\nWe use a central diﬀerence method to evaluate the derivatives of the wavefunction\nin position space, i.e. to approximate the operators ∂\n∂x and\n∂2\n∂x2. The central diﬀer-\nence method we use involves 9 points in total, and has O(d8) errors for the ﬁrst- and\nsecond-order derivative evaluations with d being the discretization grid size. The nu-\nmerical evaluations of the derivatives are found to be fairly accurate by comparing the\nresults with analytically calculated derivatives for Gaussian wave packets, with an error\nof around 10−5. Except for the discretization, for simplicity we keep most of our settings\nthe same as those in the quadratic problems in Section 4.2, including the time step, the\ncontrol step, the control force and the mass, and also the reference quantities mc and\nωc that deﬁne the units of mass and frequency. However, some other parameter settings\nneed to be changed. As a starting point, we need to determine the quartic potential\ncoeﬃcient λ. To make the quartic system of a similar size as the quadratic system, we set\nthe values of the harmonic potential and the quartic potential to be equal around position\nx = 3.5\n\u0010q\nℏ\nmcωc\n\u0011\n1, and therefore we take the value λ = π\n25\n\u0010\nm2\ncω3\nc\nℏ\n\u0011\n. Next, to avoid the nu-\nmerical divergence problem that results from high energy modes on the discretized space\ngrid and large potentials, we restrict our simulated space to be from x = −8.5\n\u0010q\nℏ\nmcωc\n\u0011\nto x = +8.5\n\u0010q\nℏ\nmcωc\n\u0011\n, and we discretize it using a step of d = 0.1\n\u0010q\nℏ\nmcωc\n\u0011\n. Note that\nbecause the quartic potential increases fast, the largest value of the simulated potential\nexceeds the harmonic potential at position x = 20\n\u0010q\nℏ\nmcωc\n\u0011\n, and therefore a properly\ncontrolled and cooled wavefunction does not have enough energy to come close to the\nborder of the space, which we have also conﬁrmed in our numerical simulation.\nEven with such a coarse grid and moderate potential values, the numerical simulation\nstill diverges after tens of thousands of time steps. To make minimal additional modiﬁca-\ntions to handle this diverging problem, we add higher order correction terms concerning\nthe Hamiltonian into our state-update equation of the numerical simulation. This is eas-\nily done by adding more Taylor-expansion terms P\nn\n1\nn!(−dt i\nℏH)n|ψ⟩with dt being our\nsimulation time step. In our case, we sum to the 5-th order correction, and the numerical\ndivergence disappears. It is important to note that such a correction does not necessarily\nincrease the precision of the simulation, because when adding these additional terms, we\ndo not consider their interactions with other non-Hamiltonian terms that would result\nin values of similar orders. On the other hand, the space discretization also limits our\nsimulation precision. The additional higher order corrections of the Hamiltonian is only\nintended to alleviate the numerical divergence, not to increase an overall precision. To\n1The expressions in parenthesis are physical units.\n60\nChapter 5. Control in Quartic Potentials\navoid large numerical errors, we claim that the simulation fails when its energy exceeds an\namount of 20 (ℏωc) or when the norm of the wavefunction exceeds 10−5 near the border\nof the space.2 When such conditions are not violated, we numerically conﬁrmed that our\nsimulation of the wavefunction has an error below 0.05%3 for simulation time 10 ×\n1\nωc,\ni.e., the time horizon of the reinforcement learning control.\nThe parameters used are summarized as follows:\nλ\n\u0010\nm2\ncω3\nc\nℏ\n\u0011\nm (mc)\nd\n\u0010q\nℏ\nmcωc\n\u0011\nx\n\u0010q\nℏ\nmcωc\n\u0011\ndt\n\u0010\n1\nωc\n\u0011\nη\nγ\n\u0010\nmcω2\nc\nℏ\n\u0011\nπ\n25\n1\nπ\n0.1\n[−8, +8]\n1\n1440\n1\n0.01π\nFcon ((mcℏ)\n1\n2ω\n3\n2c )\nNcon\ntmax\n\u0010\n1\nωc\n\u0011\n[−5π, +5π]\n18\n100\nTable 5.1: The parameter settings used in our quartic system simulations.\nThe units are\nspeciﬁed in parenthesis, and we use the same reference quantities mc and ωc as Table 4.1.\nWhen there is no unit, the quantity is dimensionless.\nIn the above Table 5.1, the parameter λ denotes the strength of our quartic potential,\nm is the mass, d is the discretization step of position space, x represents the positions\nconsidered in our simulation, dt is the time step, η is measurement eﬃciency, and γ is the\nmeasurement strength. This measurement strength is much smaller than the ones used\nin quadratic problems, because we want the wavefunction to be suﬃciently non-local in\nour quartic potential. For the control part, Fcon is the external control force, Ncon is\nthe number of control steps in a unit time\n1\nωc, and tmax is the total time of a simulation\nepisode. When we implement our derived conventional controllers as in Sec. 5.1.2, we\nalways discretize the control forces in the same way as the neural network controller be-\nfore applying the forces, which is a strategy already mentioned in Sec. 4.3.1 concerning\nthe discretized optimal control.\nA ﬁnal issue here is the initialization of the wavefunction. This is not a signiﬁcant\nissue for the case of quadratic potentials, where the state quickly evolves into a Gaus-\nsian state with ﬁxed covariances and follow simple time-evolution equations. However,\nas illustrated in Fig. 5.1, we know that a freely evolving wavefunction in a quartic po-\ntential is typically non-Gaussian, and therefore, in order to initialize a “typical” state in\nthe quartic potential, we use a two-step initialization strategy. First, we initialize the\nstate as a Gaussian wave packet as in Fig. 5.1(a). Second, we let the state evolve in the\npotential for time 15 ×\n1\nωc under the position measurement. If the resulting state has\n2Speciﬁcally, we evaluate the value of the wavefunction on the 5th leftmost point and the 5th rightmost\npoint on our space grid. This is because a point too close to the border is aﬀected by the error coming\nfrom the ﬁnite-diﬀerence-based derivative estimation, and the wavefunction may not evolve correctly.\n3This is conﬁrmed by simulating the same wavefunction using ﬁner discretization steps and then\ncomparing the simulation results. We only evaluated the deterministic evolution of the state, i.e., γ = 0.\n61\nChapter 5. Control in Quartic Potentials\nan energy below 18 (ℏωc), we accept it as an initialized state that can be used in our\nsimulation; otherwise we repeat the above initialization process to obtain other states.\nThe initialization energy is dependent on the initial Gaussian wave packet, and we set\nits wave-number to be randomly chosen from the interval [−0.4, +0.4]\n\u0000pmcωc\nℏ\n\u0001\n, and\nwe set its mean to be 0 and standard deviation to be 1\n\u0000p mcωc\nℏ\n\u0001\n. The units are the\nsame as those in Table 5.1, Fig. 5.1 and Fig. 5.2. This initialization strategy produces\ninitial energies of the states that range from 1 to 18 (ℏωc) with an average around 10 (ℏωc).\nWhen evaluating a controller’s performance, since the oscillation behaviour is much\nslower compared with the quadratic case, we sample the energy of the controlled state\nper time 20 ×\n1\nωc after the episode has started for time 40 ×\n1\nωc, i.e., using 3 samples in\neach simulated episode. Then we calculate the sample mean and the standard error.\n5.2.2\nImplementation of Reinforcement Learning\nThe reinforcement learning system is implemented basically in the same way as that de-\nscribed in Sec. 4.2.2 for the quadratic problems. One diﬀerence here is that, we do not\nconsider measurement-outcome-based controllers, since we know that they would result\nin lower performances as discussed in Sec. 4.3.\nSimilar to the case of quadratic problems, we consider two cases concerning the neural\nnetwork inputs. While the second case is simply to input the wavefunction, the ﬁrst case is\nno longer to input the quintuple (⟨ˆx⟩, ⟨ˆp⟩, Vx, Vp, C), because we know that these ﬁve val-\nues are not suﬃcient to describe the state. To proceed further, we input higher order mo-\nments of the phase space quasi-distribution of the state beyond the covariances. Namely,\nwe input the third-order central moments ⟨(ˆx −⟨ˆx⟩)3⟩, ⟨(ˆx −⟨ˆx⟩) (ˆp −⟨ˆp⟩) (ˆx −⟨ˆx⟩)⟩,\n⟨(ˆp −⟨ˆp⟩) (ˆx −⟨ˆx⟩) (ˆp −⟨ˆp⟩)⟩and ⟨(ˆp −⟨ˆp⟩)3⟩into the neural network, which are called\nskewnesses, and similarly, we input all forth- and ﬁfth-order central moments. The re-\nsulting input contains 20 values in total, and this is used as the ﬁrst input case for our\nquartic problem. For the wavefunction input case, as we know that the wavefunction\nnear the borders of the simulated position space is less relevant and contains numerical\nerror, we do not input wavefunction values near the border. Speciﬁcally, regarding the\nwavefunction amplitudes deﬁned on spatial grids, we discard 15 wavefunction values on\nboth the left and the right border, and use the rest as the neural network input, by sep-\narating the complex amplitudes into their real-valued parts and imaginary-valued parts.\nThe reward is deﬁned to be −2 times the expected energy of the state in the quar-\ntic potential, shifted by 2. Similar to the quadratic case in Sec. 4.2.2, the simulation is\nstopped when the energy is too high, with the threshold energy being 20 (ℏωc). This\nkeeps both the learned loss values reasonably small and the numerical simulation error\nof the system reasonably small, which is already mentioned in Sec. 5.2.1. The number\nof simulation episodes for training is set to be 12000, and the replay memory sizes of the\ntwo input cases are respectively set to contain 6000 and 2000 episodes that are of the\nlength of tmax.\n62\nChapter 5. Control in Quartic Potentials\ncooling quartic\noscillators\nNetwork\nInput\nphase space distri-\nbution moments\n(up to the ﬁfth)\n0.7393±0.0003\nwavefunction\n0.7575±0.0009\nDerived\nControls\ndamping control\n0.7716±0.0024\nquadratic control\n0.8211±0.0031\nGaussian approxi-\nmation\n0.7562±0.0010\nGround State Energy\n0.7177\nTable 5.2: Performance results for the problem of cooling a quartic oscillator. The numbers\nafter the ± signs show the estimated standard deviations for the above reported means. The\nvalues show the average energies of the controlled states in units of ℏωc.\n5.3\nComparison of Reinforcement Learning and Con-\nventional Control\nIn this section we present the performances of the reinforcement learning controllers and\nthe controllers described in Sec. 5.1.2. The results are summarized in Table. 5.2. Unlike\nthe quadratic case, we do not directly compare the behaviours of reinforcement learning\ncontrollers and conventional controllers as in Sec. 4.3.2, because the relevant inputs have\nbecome much more complicated and high-dimensional and can hardly be plotted.\nThe damping controller uses a damping factor of 0.5, and the quadratic controller uses\na parameter k = 2×λ·\nℏ\nmcωc, which are determined by a rough grid search of the possible\nvalues. The ground-state energy is calculated in our discretized and bounded position\nspace, by direct diagonalization of the system Hamiltonian.4 In Table 5.2, it can be seen\nthat the ﬁrst input case of the reinforcement learning controller performs best, clearly\nbetter than all the other controllers by many times of standard deviations. Also we ﬁnd\nthat the energy variance of its controlled states is much smaller than other controllers,\nwhich means that the control is very stable, and results in an extremely small standard\nerror as given in Table 5.2. Compared with the ﬁrst input case, the second input case\nperforms comparatively worse, probably because the neural network cannot easily evalu-\nate the relevant physical observables using the wavefunction, as physical observables are\nalways quadratic in terms of the wavefunctions. This diﬃculty typically induces noise\nand small error in an AI’s decisions, and can lower its ﬁnal performance slightly. In fact,\nwe indeed ﬁnd a larger variance of the energies of the controlled states, which supports\nthe above interpretation.\n4For completeness, the ﬁrst three excited states have energies that are given by 2.5718, 5.0463 and\n7.8816 in units of ℏωc. It can be seen that the controlled states are very close to the ground state.\n63\nChapter 5. Control in Quartic Potentials\n5.4\nConclusion\nIn this chapter, we have shown that the problem of cooling a particle in a one-dimensional\nquartic potential can be learned and solved well by a deep-reinforcement-learning-based\ncontroller, and the controller can outperform all conventional non-machine-learning strate-\ngies that we have considered, which serves as an example of application of deep reinforce-\nment learning to physics. We have also found that when relevant information of the\ninput cannot be extracted by the neural network precisely and easily, the neural network\ntypically performs slightly worse. This fact suggests that adaptation and modiﬁcation of\nthe neural network architecture for speciﬁc physical problems may increase the ﬁnal per-\nformance of the AI, which is a possible direction of future research concerning applying\ndeep learning to physics.\nOn the other hand, we ﬁnd that this cooling problem for quartic potentials does not\nsigniﬁcantly demonstrate a superior performance of deep reinforcement learning. This is\nprobably due to the fact that all the controllers have cooled the states to have energies\nthat are very close to the ground state, and as can be observed experimentally, all the\nresulting wavefunctions have become very similar in shape, which suggests that a pertur-\nbative solution to the problem can probably work very well after all, and that the system\neventually becomes quite static and simple. Therefore, the complexity of the properties\nof the quartic potential does not necessarily imply that conventional strategies cannot\ncool the system well, and in order to demonstrate signiﬁcant superior performance of\ndeep reinforcement learning, we actually need problems that are more complicated than\nthis cooling task.\n64\nChapter 6\nConclusions and Future Perspectives\nIn this thesis, based on the formulation of continuous measurement and the deep reinforce-\nment learning technology which are reviewed in Chapters 2 and 3, we have investigated\nthe control problems of one particle in one-dimensional quadratic and quartic poten-\ntials subject to continuous position measurement, and showed that the performance of\ndeep-reinforcement-learning based control is comparable to or better than existing non-\nmachine-learning strategies. Although this research is not the ﬁrst application of deep\nreinforcement learning to quantum control, this research has a lot of important implica-\ntions which we will summarise and discuss in this chapter.\nIn our research we have shown that when the reinforcement learning controller is\ncorrectly conﬁgured and suﬃciently trained, with the recent developments of reinforce-\nment learning technology it can achieve a performance that is satisfactory enough when\ncompared with known control strategies. Even when we give the controller redundant in-\nformation but not the directly relevant information as its input, such as the wavefunction\nor the measurement outcomes, the reinforcement-learning controller is still able to ex-\ntract relevant physical information from its input to output a reasonable control, though\nit may need more training. This is especially useful when we do not know what physical\nquantities are relevant. Even when we do not understand how to simplify or interpret the\nproblem, the AI still gives us an answer that can be veriﬁed, which is of great practical\nimportance for controlling complex quantum systems that we do not understand well.\nAlthough previous researches also proceeded along this line, as discussed in Chapter 1,\nthey mainly focused on open-loop controls in which the state evolves deterministically\nand the AI only searches for one controlled trajectory of the state. However, for such\nopen-loop control problems, there are already plenty of eﬃcient search algorithms that\ncan be used to ﬁnd the trajectory and AI technology may not be necessary. On the other\nhand, in our problem setting the state evolves stochastically due to measurement backac-\ntion, and therefore the controller is a measurement-feedback closed-loop controller, since\nit reads the post-measurement state which is conditioned on the measurement. Measure-\nment is useful in real circumstances as it acquires information of the state and puriﬁes the\nstate, and in order to accompany measurement, a closed-loop controller that works with\nmeasurement-induced stochasticity is necessary. This is where a reinforcement learning\ncontroller can be used, and explains the uniqueness and importance of our research.\n65\nChapter 6. Conclusions and Future Perspectives\nIn reality, AI-based real-time closed-loop controllers are indeed feasible by embed-\nding the AI computation into a programmed microchip to work suﬃciently fast [76],\nsuch as the application-speciﬁc integrated circuit (ASIC) and ﬁeld-programmable gate\narray (FPGA), which respond and operate in a time scale of microseconds and nanosec-\nonds [77, 78]. In this way, AI-based controllers can operate around the same time scale\nas the underlying quantum systems and become realizable in experiments and real-world\nquantum devices, which may help the development of quantum technology in a direct\nmanner. If we do not consider using the AI controller directly, we may as well design\nour own control methods based on the AI’s learned strategy, similar to what is done in\nRef. [31] concerning spin-chain controls.\nFrom a theoretical perspective, we may do reverse engineering on the AI controller\nto discover underlying properties of the controlled system. We may analyse the system\nunder control to gain insights on how the system behaves and develop eﬀective theories\nto describe it and explain AI’s control. That is, we start from the answer of the control\nproblem obtained from the AI and proceed to ﬁnd the reason for such an answer. In this\nmanner AI strategies can open up a new way of research on complex quantum systems\nand bring a lot of new possibilities. Although we have not done this for our quartic\nsystems in Chapter 5, we expect that it is possible, and this reverse-engineering strategy\nmay give us a lot of insights on the quartic system since we almost have no other methods\nor lines of research to follow to analyse the system behaviour. The same logic applies to\nother complex systems, and this is why we believe AI strategies bring us new possibilities\nof research on complex quantum systems.\nBesides the above implications concerning physics, there are also new questions to an-\nswer from the perspective of deep reinforcement learning. In our research, we simulated\nthe quantum system for 104 episodes, i.e. around 105 ∼106 oscillations, to train the rein-\nforcement learning controller on the quantum control problem, which is of a considerable\ncomputational cost. It would be desirable if we can reduce the number of simulations by\none or two orders of magnitude while achieving a similar performance of reinforcement\nlearning, which asks for a better simulation or training strategy. This can be a future\nresearch direction, improving the implementation of reinforcement learning on physics;\notherwise it would not be feasible to train reinforcement learning AI to control quantum\nsystems that require a long time to simulate. Also, there are possibilities to suitably\nmodify reinforcement learning systems for underlying physical problems to facilitate the\nlearning. For example, we know that any expectation value of physical observables are\nof a quadratic form regarding the state vector |ψ⟩, as ⟨ψ| ˆO|ψ⟩. However, the deep neural\nnetworks we used are only composed of linear mappings and rectiﬁer units, which can-\nnot express quadratic forms straightforwardly. Thus, a learning system taking the state\nvector as an input may work more eﬃciently if we design a new structure of its neural\nnetwork to make it learn quadratic forms. Similar optimizations of the learning system\ncan be many, and deep reinforcement learning has not been well adapted to physics yet.\nOn the whole, our research has demonstrated new possibilities of applying deep re-\ninforcement learning to quantum physics and real-world applications. There are many\nnew questions to answer and problems to solve concerning the interplay between deep\n66\nChapter 6. Conclusions and Future Perspectives\nleaning and physics, and in the future, AI technology may hopefully help both real-world\nproblems and theoretical physical research.\n67\nAppendix A\nLinear-Quadratic-Gaussian Control\nIn this appendix we review the linear-quadratic-Gaussian (LQG) control theory, which\nenables us to conduct an optimal control in its speciﬁc LQG problem setting. We ﬁrst\ndiscuss the linear-quadratic control problem without a Gaussian noise in Section A.1,\nand then discuss a situation in which a Gaussian noise is added in Section A.2, which\ncompletes the LQG theory. Our notations follow Ref. [5].\nA.1\nDeterministic Linear-Quadratic Control\nThe controller for the linear-quadratic control problem is called a linear-quadratic regu-\nlator (LQR) [5], and we introduce its problem setting and derivations in this section.\nA.1.1\nProblem Setting and the Quadratic Optimal Cost\nAs introduced in Chapter 1, a control problem is concerned with a controlled system\nstate, which is denoted as x, and a control parameter, which is denoted as u. Here x and\nu are vectors and we do not write them in boldfaces for simplicity. The linear-quadratic\nproblem involves a linear system in the sense that the time evolution of x is linear with\nrespect to combined x and u, i.e.\ndx = Fx dt + Gu dt,\n(A.1)\nwhere F and G are matrices, and we have suppressed time dependences of all the vari-\nables F, G, x and u. Then, the control problem considers minimization of a quadratic\ncontrol cost1 accumulated from the start of the control at time t0 to the end time T,\nwhich is formulated as\nV (x(t0), u(·), t0) =\nZ T\nt0\n\u0000u†Ru + x†Qx\n\u0001\ndt + x†(T)Ax(T),\n(A.2)\n1This quantity has many nomenclatures. It is also called a performance index or score or cost-to-go.\n68\nAppendix A. Linear-Quadratic-Gaussian Control\nwhere Q and A are non-negative matrices and R is a positive matrix, and therefore we\nhave V ≥0. In the above equation, V is dependent only on the initial condition of x, i.e.,\nx(t0), because the time-evolution equation of x is determined once we know the control\ntrajectory u(·). From now on we always assume that the control stops at time T, while\nthe starting time of control t0 may be changed.\nFrom Eq. (A.1) we can conclude that the controlled system x is memoryless, and\ntherefore for any consistent control strategy, the control u must be a function that is only\ndependent on the current values of x and t. Let us denote a possible optimal control\nstrategy of the problem by u∗, which minimizes the control cost V . The minimized cost\nis then denoted by V ∗(x(t0), t0), where u∗has been totally determined by x and t and\ntherefore disappeared. Now we prove that V ∗must have the form of\nV ∗(x(t), t) = x†(t)P(t)x(t),\nwhere matrix P(t) is independent of x, and is symmetric (self-conjugate) and non-\nnegative. First, we show that for a constant λ, we have V ∗(λx, t) = |λ|2V ∗(x, t). This\ncan be shown as follows. First,\nV ∗(λx, t) = V (λx, u∗\nλx(·), t)\n≤V (λx, λu∗\nx(·), t)\n= |λ|2V (x, u∗\nx(·), t)\n= |λ|2V ∗(x, t),\n(A.3)\nwhere we have used u∗\nx(·) to denote the optimal control for an initial state x and u∗\nλx(·)\nfor an initial state λx. Using the optimality of control u∗\nλx(·) regarding the initial state\nλx we proceed from the ﬁrst line to the second line, and using the linearity of Eq. (A.1)\nand the deﬁnition of V in Eq. (A.2) we proceed from the second line to the third line.\nNote that we have suppressed the time argument of the initial condition. Similarly, we\nhave\n|λ|2V ∗(x, t) ≤|λ|2V (x, λ−1u∗\nλx(·), t) = V (λx, u∗\nλx(·), t) = V ∗(λx, t),\n(A.4)\nand therefore we have\nV ∗(λx, t) = |λ|2V ∗(x, t).\n(A.5)\nSecondly, we show that V ∗(x1, t) + V ∗(x2, t) = 1\n2 [V ∗(x1 + x2, t) + V ∗(x1 −x2, t)]. It is\ndone similarly using the linearity of trajectory x(·) regarding the initial condition x(t)\nand control u(·) and using the quadratic property of V :\n69\nAppendix A. Linear-Quadratic-Gaussian Control\nV ∗(x1, t) + V ∗(x2, t) = 1\n4 [V ∗(2x1, t) + V ∗(2x2, t)]\n≤1\n4\n\u0002\nV ((x1 + x2) + (x1 −x2), u∗\nx1+x2 + u∗\nx1−x2, t)\n+V ((x1 + x2) −(x1 −x2), u∗\nx1+x2 −u∗\nx1−x2, t)\n\u0003\n.\n(A.6)\nFrom Eq. (A.1), we know that for an initial state x1(t0) and control u1(·) we can ob-\ntain a trajectory x1(·), and if there is another initial state x2(t0) and control u2(·) and\na resulting trajectory x2(·), we can obtain a third valid trajectory k1x1(·) + k2x2(·) with\ncontrol k1u1(·) + k2u2(·), which is the linearity. By Eq. (A.2) we have\nV ((x1 + x2), (u1 + u2)(·), t) =\nZ T\nt\n\u0002\n(u1 + u2)†(s)R(u1 + u2)(s)\n+(x1 + x2)†(s)Q(x1 + x2)(s)\n\u0003\nds\n+ (x1 + x2)†(T)A(x1 + x2)(T),\nV ((x1 −x2), (u1 −u2)(·), t) =\nZ T\nt\n\u0002\n(u1 −u2)†(s)R(u1 −u2)(s)\n+(x1 −x2)†(s)Q(x1 −x2)(s)\n\u0003\nds\n+ (x1 −x2)†(T)A(x1 −x2)(T),\n(A.7)\nand therefore\nV ((x1 + x2), (u1 + u2)(·), t) + V ((x1 −x2), (u1 −u2)(·), t)\n= 2\nZ T\nt\n\u0010\nu†\n1Ru1 + x†\n1Qx1 + u†\n2Ru2 + x†\n2Qx2\n\u0011\nds\n+ 2x†\n1(T)Ax1(T) + 2x†\n2(T)Ax2(T)\n= 2V (x1, u1(·), t) + 2V (x2, u2(·), t).\n(A.8)\nNote that the cancellation of the x1(·) and x2(·) terms is possible due to the fact that\nthey are the trajectories resulting from initial conditions x1 and x2 and controls u1 and\nu2, and therefore they are already fully determined. Using the above formula we can\ntransform Eq. (A.6) into\nV ∗(x1, t) + V ∗(x2, t) ≤1\n2\n\u0002\nV (x1 + x2, u∗\nx1+x2, t) + V (x1 −x2, u∗\nx1−x2, t)\n\u0003\n= 1\n2 [V ∗(x1 + x2, t) + V ∗(x1 −x2, t)] .\n(A.9)\nNext we perform a change of variables:\n1\n2 [V ∗(x1 + x2, t) + V ∗(x1 −x2, t)] ≤1\n4 [V ∗(2x1, t) + V ∗(2x2, t)]\n= V ∗(x1, t) + V ∗(x2, t).\n(A.10)\nTherefore, we have obtained\n70\nAppendix A. Linear-Quadratic-Gaussian Control\nV ∗(x1, t) + V ∗(x2, t) = 1\n2 [V ∗(x1 + x2, t) + V ∗(x1 −x2, t)] ,\n(A.11)\nand together with Eq. (A.5) we have the result that V ∗takes a quadratic form with\nrespect to x, as shown in Ref. [5]. Therefore it can be written as\nV ∗(x, t) = x†P(t)x,\n(A.12)\nwhere we have denoted the moment to start the control as t rather than t0, and this is be-\ncause we will do variations on this variable later. The matrix P(t) above is symmetric and\nnon-negative and independent of x. Its symmetric property can be assumed without loss\nof generality because we can equivalently rewrite it as 1\n2\n\u0000P + P †\u0001\n, and its non-negativity\nfollows from the non-negativity of V . Note that everything here is implicitly dependent\non the ending time T of the control.\nA.1.2\nThe Hamilton-Jacobi-Bellman Equation and the Optimal\nControl\nTo solve for the matrix P(t), we need to introduce the Hamilton-Jacobi-Bellman (HJB)\nequation. This equation is in essence similar to the Bellman equation for the Q-function\nin Section 3.2.2, but it has no discount factor and deals with continuous space. To pro-\nceed, we ﬁrst derive this HJB equation.\nFor an optimal control cost V ∗(x(t), t), by its optimality its control u∗should minimize\nits own expectation of its control cost, and therefore for an inﬁnitesimal time dt it satisﬁes\nV ∗(x(t), t) = min\nu {V ∗(x(t + dt), t + dt) + ℓ(x(t), u)dt} ,\n(A.13)\nwhere ℓis the integrated term in the control cost (i.e., u†Ru + x†Qx in Eq. (A.2)), and\nthe state x(t + dt) is dependent on the control u, introducing a non-trivial u dependence\ninto the minimization. By the Taylor-expansion of V ∗(x(t + dt), t + dt) up to the ﬁrst\norder of dt, we obtain\nV ∗(x(t + dt), t + dt) = V ∗(x(t), t) + ∂xV ∗(x(t), t) dx + ∂tV ∗(x(t), t) dt,\n(A.14)\nwhere ∂x and ∂t denote the diﬀerentiation with respect to the ﬁrst and the second argu-\nments of V ∗. Therefore Eq. (A.13) can be written as\n−∂tV ∗(x(t), t) dt = min\nu {∂xV ∗(x(t), t) dx + ℓ(x(t), u)dt} .\n(A.15)\nIf we express the time-evolution equation of x as dx\ndt = f(x, u), then we have the HJB\nequation as\n71\nAppendix A. Linear-Quadratic-Gaussian Control\n−∂tV ∗(x(t), t) = min\nu {∂xV ∗(x(t), t)f(x, u) + ℓ(x(t), u)} .\n(A.16)\nSometimes this equation is also written as\n˙V (x, t) + min\nu {∇V (x, t) · f(x, u) + ℓ(x, u)} = 0,\n(A.17)\nwhere the superscript stars are omitted, and the superscript dot and nabla denote ∂t and\n∂x.\nBecause the optimal control cost V ∗(x, t) is deﬁned on the whole space of x and t and\nby deﬁnition V ∗is the smallest possible control cost, we have the fact that as long as V ∗\nexists, it is unique. Therefore, if we have a function V that satisﬁes the HJB equation\n(Eq. (A.16)), it must be the unique optimal control cost V ∗, and the HJB equation be-\ncomes a suﬃcient and necessary condition for optimal control. In addition, the optimal\ncontrol strategy u∗is to take the minimization of u in the above HJB equation, which is\npossibly not unique. This suﬃcient and necessary condition provided by the HJB equa-\ntion is important because it makes “guessing” the optimal control possible.\nAfter we have obtained the HJB equation, we now turn to solve the matrix P(t) in\nEq. (A.12). By straightforward substitution, we have2\n−x† ˙Px = min\nu\n\b\nx†P(Fx + Gu) + (Fx + Gu)†Px + u†Ru + x†Qx\n\t\n= min\nu\n\b\nx†(PF + F †P)x + x†Qx + x†PGu + u†G†Px + u†Ru\n\t\n.\n(A.18)\nWe can arrange the u dependent terms into a complete square to eliminate them through\nminimization:\n−x† ˙Px = min\nu\n\b\nx†(PF + F †P)x + x†Qx + (u† + x†PGR−1)R(R−1G†Px + u)\n−x†PGR−1G†Px\n\t\n= x†(PF + F †P)x + x†Qx −x†PGR−1G†Px,\n(A.19)\nwhere we have assumed that P is symmetric, or hermitian, and that R is positive. The\nminimization in the above equation is completed by forcing the condition R−1G†Px+u =\n0, and therefore the optimal control u∗is\nu∗= −R−1G†Px,\n(A.20)\nwhere the matrix P is obtained from the diﬀerential equation (A.19) and matrices G and\nR are deﬁned in Eq. (A.1) and (A.2). The above equation concerning P can be further\nsimpliﬁed by removing the arbitrary variable x on both sides. Then we obtain the matrix\n2In usual control theory, complex values are not considered. However, for consistency with quantum\nmechanics, we have generalized the derivation to allow complex values. Therefore, one or two terms in\nour derivation may be diﬀerent from those in standard textbooks.\n72\nAppendix A. Linear-Quadratic-Gaussian Control\nRiccati equation:\n−˙P = PF + F †P −PGR−1G†P + Q.\n(A.21)\nSince this equation is a diﬀerential equation, we still need to ﬁnd its boundary condition\nto solve it. Recall that there is a ﬁnal control-independent term x†(T)Ax(T) in the deﬁ-\nnition of V in Eq. (A.2), we obtain the boundary condition for P(t), i.e. P(T) = A. In\ncases A = 0, we have P(T) = 0. This completes the optimal control for ﬁnite-horizon\ncontrol problems, i.e., T < ∞.\nNext we consider the case of T →∞. Although it is still possible to proceed with\ntime dependent matrices F, G, R and Q, for simplicity we require them to be constant\nin the following discussion.\nWhen all the matrices in the deﬁnition of the problem are time-independent, the prop-\nerty of the controlled system becomes completely time-invariant if T also goes to inﬁnity.\nIn this case, the control strategy u should be consistent in time, i.e. time-independent,\nand thus P cannot change with time3, and we have ˙P = 0, which yields the continuous-\ntime algebraic Riccati equation (CARE):\nPF + F †P −PGR−1G†P + Q = 0.\n(A.22)\nMathematical software usually provides numerical methods to solve this algebraic Riccati\nequation, and this equation is of great importance in engineering and control theory. We\nsummarise the results for the inﬁnite-time control case below.\ndx = Fx dt + Gu dt,\n(A.23)\nV (x(t0), u(·)) =\nZ ∞\nt0\n\u0000u†Ru + x†Qx\n\u0001\ndt,\nR > 0, Q ≥0,\n(A.24)\nV ∗(x(t0)) = x†(t0)Px(t0),\nP ≥0,\n(A.25)\nu∗= −R−1G†Px,\n(A.26)\nPF + F †P −PGR−1G†P + Q = 0,\n(A.27)\nwhere u∗is the optimal control, and V ∗is the optimal cost.\nFrom the above equations, we can see that a ﬁnite choice of the matrix R is necessary\nto produce a non-divergent u∗. Otherwise, when solving for u∗, typically its value goes to\n3This result can also be obtained by analysing the diﬀerential equation and take the limit that the\ntime is inﬁnitely long.\n73\nAppendix A. Linear-Quadratic-Gaussian Control\ninﬁnity. This is reasonable since the condition R = 0 implies that the control is for free\nand we can use as large control as we want, which clearly does not produce ﬁnite control\nstrengths. However, we may look at the conditions where the control u∗is precisely zero,\nand interpret the inﬁnite control strengths as pushing the state x to these stationary\npoints where u∗is zero. This justiﬁes our treatment of the control forces in Chapter 4\nwhere we have assumed R = 0. It can be seen that in this case, the time evolution of\nthe system, i.e. matrix F, is no longer relevant for this control problem, and the optimal\ncontrol is determined by the loss.\nFor completeness, we present the results of discrete linear systems with discrete con-\ntrols, i.e., xn and un, without proof.\nThe proof follows the same line as the above\ncontinuous case, and interested readers are referred to Ref. [5].\nxt+1 = Fxt + Gut,\n(A.28)\nV (xt0, u(·))) =\n∞\nX\nt=t0\n\u0010\nu†\ntRut + x†\nt+1Qxt+1\n\u0011\n,\nR > 0, Q ≥0,\n(A.29)\nV ∗(x(t0)) = x†(t0)Px(t0),\nP ≥0,\n(A.30)\nu∗= −\n\u0000G†SG + R\n\u0001−1 G†SFx,\nS = P + Q,\n(A.31)\nS = F † h\nS −SG\n\u0000G†SG + R\n\u0001−1 G†S\ni\nF + Q.\n(A.32)\nThe last equation A.32 is called the discrete-time algebraic Riccati equation (DARE).\nA.2\nLinear-Quadratic Control with Gaussian Noise\nWhen an additive Gaussian noise is introduced into the time-evolution equation, the op-\ntimal control strategy in Eqs. (A.20) and (A.22) is not changed. In this section we prove\nthis result.\nA.2.1\nDeﬁnitions of the Optimal Control and Control Cost\nWhen a Gaussian noise introduced, the time-evolution equation (A.1) becomes\ndx = Fx dt + Gu dt + ˆQ\n1\n2 dW,\n(A.33)\nwhere dW is a multi-dimensional Wiener increment with the variance dt for each dimen-\nsion, and ˆQ is a semideﬁnite covariance matrix.\n74\nAppendix A. Linear-Quadratic-Gaussian Control\nWithout noise, as long as the system is controllable, it is always possible to use u to\ncontrol the state x to become 0 and leave both x and u zero afterwards to stop the accu-\nmulation of control cost, which shows that there always exist control strategies with ﬁnite\ncontrol costs even when the limit T →∞is taken, and therefore the optimal cost V ∗must\nalso have a ﬁnite value. However, this is not true when noise is present. When the system\ncontains noise, expectation values of the state typically converge to some steady values,\nand they are not zero. Therefore, the expected control cost V would be proportional to\nthe control time T in the long term, and we have lim\nT→∞V →∞and lim\nT→∞\n1\nT V →c where\nc is some constant. Therefore, we can neither consider a minimization of lim\nT→∞V nor\nlim\nT→∞\n1\nT V to deduce the optimal control, since lim\nT→∞\n1\nT V is never aﬀected by any short-time\nchange of control u. In this case, in order to deﬁne the optimal control for an inﬁnite\ntime horizon T, we investigate the convergence of the optimal control strategy u∗at the\nlarge limit of T, rather than the convergence of the control cost V ∗. Therefore we start\nfrom a ﬁnite T to deduce V ∗and u∗.\nIn the presence of a Wiener increment, the derivation of the HJB equation (A.16)\nshould be modiﬁed. Especially, the Taylor expansion of V ∗(x(t + dt), t + dt) should be\ntaken to the second order of dx. Note that the control cost V is an expectation now. The\nTaylor expansion is\nV ∗(x(t + dt), t + dt) = E\n\u0014\nV ∗+ ∂tV ∗dt + ∂xV ∗dx + 1\n2∂2\nxV ∗dx2\n\u0015\n= V ∗+ ∂tV ∗dt + ∂xV ∗(Fx dt + Gu dt) + 1\n2tr\n\u0010\n∂2\nxV ∗ˆQ\n\u0011\ndt,\n(A.34)\nand the HJB equation (A.16) becomes\n−∂tV ∗(x(t), t) = min\nu\n\u001a\n∂xV ∗(x(t), t)(Fx + Gu) + 1\n2tr\n\u0010\n∂2\nxV ∗(x(t), t) ˆQ\n\u0011\n+ (u†Ru + x†Qx)\n\u001b\n.\n(A.35)\nAs discussed in Section A.1.2, if the solution V ∗to the above equation exists, it must be\nunique. Therefore, we try to make up such a solution by referring to the results obtained\nin the previous section.\nA.2.2\nThe LQG Optimal Control\nAs explained above, we expect V to be proportional to time T. Since Eq. (A.35) is\nonly diﬀerent from the deterministic case by the term 1\n2tr\n\u0010\n∂2\nxV ∗(x(t), t) ˆQ\n\u0011\n, without any\nchange of the optimal control, we expect this term to be a constant, which is indeed the\ncase for a V ∗quadratic in x. Its value is 1\n2tr\n\u0010\n2P(t) ˆQ\n\u0011\n= tr\n\u0010\nP(t) ˆQ\n\u0011\n. Recall that P(t)\nis constrained by the continuous algebraic Riccati equation and boundary condition and\nis independent of the control u. Therefore, the minimization in Eq. (A.35) concerning\nu is not aﬀected by the term tr\n\u0010\nP(t) ˆQ\n\u0011\n. Assuming that ∂xV ∗is the same as the deter-\n75\nAppendix A. Linear-Quadratic-Gaussian Control\nministic case, we can proceed similarly and reaches\n−∂tV ∗(x(t), t) = x†(PF + F †P)x + x†Qx −x†PGR−1G†Px + tr\n\u0010\nP(t) ˆQ\n\u0011\n,\n(A.36)\nwhere we have used the optimal control u∗= −R−1G†Px as before.\nThe above formula clearly suggests an additional term in V ∗to represents the eﬀect\nof tr\n\u0010\nP ˆQ\n\u0011\n, so that the original quadratic term x†P(t)x can be kept unaltered. It can be\nconstructed as follows:\nV ∗(x, t) = x†P(t)x +\nZ T\nt\ntr\n\u0010\nP(s) ˆQ\n\u0011\nds,\n(A.37)\nwhere P(t) satisﬁes the ﬁnite-time Riccati equation (A.21). For this constructed V ∗,\nEq. (A.36) is clearly satisﬁed, and therefore the HJB equation (A.35) is also satisﬁed, so\nthat the optimal control u∗is a valid control. As a ﬁnal step, we conﬁrm the convergence\nbehaviour of u∗at large T.\nThe convergence of u∗at large T by deﬁnition depends on the convergence of the\nmatrix P(t). However, this is already shown in Section A.1.2. We have known that\nin the deterministic case P(t) loses its dependency on t at the limit T →∞, and the\nﬁnite-time Riccati equation that governs the evolution of P(t) is the same as the equation\nfor the stochastic case here. Therefore at the large limit of T, P(t) also loses its time-\ndependence in this stochastic problem, and u∗converges. The P matrix ﬁnally satisﬁes\nthe continuous-time algebraic Riccati equation (A.22), which is time-independent. This\ncompletes our proof.\n76\nAppendix B\nNumerical Simulation of Stochastic\nDiﬀerential Equation\nTo train the reinforcement learning agent, we simulate the controlled quantum system\nfor hundreds of thousands of oscillation periods, and therefore both eﬃciency and consis-\ntency are crucial. In this appendix we introduce stochastic Itˆo–Taylor expansion and give\nthe update rule of our numerical approximation in Eq. (B.32) and (B.40). The contents\nand notations follow Ref. [79].\nB.1\nStochastic Itˆo–Taylor Expansion\nLet W(t) be a Wiener process with time argument t ∈[0, T] that is randomly chosen and\nsatisﬁes\nW(t + dt) −W(t) ∼N(0, dt),\n(B.1)\nand the quantity Xt evolves according to the Itˆo stochastic diﬀerential equation\ndXt = a(Xt) dt + b(Xt) dW,\n(B.2)\nwhere a and b are smooth functions diﬀerentiable up to some necessary orders, when their\nderivatives appear in the following context.\nWe consider a function of the variable Xt as f(Xt). By the Itˆo formula:\ndf(Xt) =\n\u0012\na(Xt) ∂\n∂xf(Xt) + 1\n2b2(Xt) ∂2\n∂x2f(Xt)\n\u0013\ndt + b(Xt) ∂\n∂xf(Xt) dWt,\n(B.3)\nwhere the partial diﬀerentials are taken with respect to the argument of function f(·),\nand\ndWt ≡W(t + dt) −W(t). All time dependences are indicated as subscripts. The Itˆo\nformula can be considered as a Taylor expansion of f(·) on its argument using the ﬁrst\n77\nAppendix B. Numerical Simulation of Stochastic Diﬀerential Equation\nand the second terms subject to the relation dW 2 = dt.\nFirst, we rewrite the stochastic diﬀerential equations in their integral forms formally:\nXt = X0 +\nZ t\n0\na(Xs) ds +\nZ t\n0\nb(Xs) dWs,\n(B.4)\nwhich is just the previous stochastic diﬀerential equation on dXt. Similarly we have\nf(Xt) = f(X0) +\nZ t\n0\n\u0012\na(Xs)f ′(Xs) + 1\n2b2(Xs)f ′′(Xs)\n\u0013\nds +\nZ t\n0\nb(Xs)f ′(Xs) dWs, (B.5)\nwhere we have written diﬀerentiation with respect to its argument as primes, which we\nwill also write in the form of f (n). We then apply the above equation to a(Xt) and b(Xt),\nand substitute them into Eq. (B.4), obtaining\nXt = X0 +\nZ t\n0\na(Xs1) ds1 +\nZ t\n0\nb(Xs1) dWs1\n= X0\n+\nZ t\n0\n\u0012\na(X0) +\nZ s1\n0\n\u0012\na(Xs2)a′(Xs2) + 1\n2b2(Xs2)a′′(Xs2)\n\u0013\nds2 +\nZ s1\n0\nb(Xs2)a′(Xs2) dWs2\n\u0013\nds1\n+\nZ t\n0\n\u0012\nb(X0) +\nZ s1\n0\n\u0012\na(Xs2)b′(Xs2) + 1\n2b2(Xs2)b′′(Xs2)\n\u0013\nds2 +\nZ s1\n0\nb(Xs2)b′(Xs2) dWs2\n\u0013\ndWs1\n= X0 +\nZ t\n0\na(X0) ds1 +\nZ t\n0\nb(X0) dWs1\n+\nZ t\n0\nZ s1\n0\n\u0012\naa′ + 1\n2b2a′′\n\u0013\nds2 ds1 +\nZ t\n0\nZ s1\n0\na′b dWs2 ds1\n+\nZ t\n0\nZ s1\n0\n\u0012\nab′ + 1\n2b2b′′\n\u0013\nds2 dWs1 +\nZ t\n0\nZ s1\n0\nbb′ dWs2 dWs1\n= X0 + a(X0) t + b(X0)(Wt −W0) + R,\n(B.6)\nwhere R stands for the remaining terms, and dWs2 and dWs1 are the Wiener increments\nfor the same Wiener process Wt. It can easily be realized that the terms\n\u0000aa′ + 1\n2b2a′′\u0001\nand a′b which depend on Xs2 can be further expanded by Eq. (B.5), and this results in\nintegrations with a constant f(X0) plus a remaining factor that scales with orders of t.\nThis recursive expansion produces the Itˆo–Taylor expansion. For simplicity, we deﬁne\ntwo operators to represent this substitutive recursion:\nL0 := a ∂\n∂x + 1\n2b2 ∂2\n∂x2,\nL1 = b ∂\n∂x,\n(B.7)\nwhere the functions a and b take the same arguments as the function that is diﬀerentiated\nby them. Then Eq. (B.5) can be written into\n78\nAppendix B. Numerical Simulation of Stochastic Diﬀerential Equation\nf(Xt) = f(X0) +\nZ t\n0\nL0f(Xs)ds +\nZ t\n0\nL1f(Xs) dWs.\n(B.8)\nTo demonstrate the recursive expansion, we expand Eq. (B.6) for a second time:\nXt = X0 + a(X0) t + b(X0)(Wt −W0) +\nZ t\n0\nZ s1\n0\nL0a(Xs2) ds2 ds1 +\nZ t\n0\nZ s1\n0\nL1a(Xs2) dWs2 ds1\n+\nZ t\n0\nZ s1\n0\nL0b(Xs2) ds2 dWs1 +\nZ t\n0\nZ s1\n0\nL1b(Xs2) dWs2 dWs1\n= X0 + a(X0) t + b(X0)(Wt −W0) +\nZ t\n0\nZ s1\n0\nL0a(X0) ds2 ds1 +\nZ t\n0\nZ s1\n0\nL1a(X0) dWs2 ds1\n+\nZ t\n0\nZ s1\n0\nL0b(X0) ds2 dWs1 +\nZ t\n0\nZ s1\n0\nL1b(X0) dWs2 dWs1\n+\nZ t\n0\nZ s1\n0\nZ s2\n0\nL0L0a(Xs3) ds3 ds2 ds1 +\nZ t\n0\nZ s1\n0\nZ s2\n0\nL1L0a(Xs3) dWs3 ds2 ds1\n+\nZ t\n0\nZ s1\n0\nZ s2\n0\nL0L1a(Xs3) ds3 dWs2 ds1 +\nZ t\n0\nZ s1\n0\nZ s2\n0\nL1L1a(Xs3) dWs3 dWs2 ds1\n+\nZ t\n0\nZ s1\n0\nZ s2\n0\nL0L0b(Xs3) ds3 ds2 dWs1 +\nZ t\n0\nZ s1\n0\nZ s2\n0\nL1L0b(Xs3) dWs3 ds2 dWs1\n+\nZ t\n0\nZ s1\n0\nZ s2\n0\nL0L1b(Xs3) ds3 dWs2 dWs1 +\nZ t\n0\nZ s1\n0\nZ s2\n0\nL1L1b(Xs3) dWs3 dWs2 dWs1,\n(B.9)\nwhere the terms other than triple integrals can be evaluated directly. Obviously, this\ninvolves permutations of multiple t and W integrals. For notational simplicity we deﬁne\nan ordered list structure to denote the permutations as\nα = (j1, j2, . . . , jl),\nji = 0, 1 ,\ni = 1, 2, . . . , l\n(B.10)\nand deﬁne their manipulation as\nα ∗˜α = (j1, j2, . . . , jl, ˜j1, ˜j2, . . . , ˜j˜l),\n(B.11)\n−α = (j2, j3, . . . , jl),\nα−= (j1, j2, . . . , jl−1),\n() ≡v,\n(B.12)\nthat is, we use v to denote an empty list. Then we deﬁne multiple Itˆo integral as\nIv[f(·)]0,t := f(t),\n(B.13)\nIα∗(0)[f(·)]0,t :=\nZ t\n0\nIα[f(·)]0,sds,\nIα∗(1)[f(·)]0,t :=\nZ t\n0\nIα[f(·)]0,sdWs.\n(B.14)\nThus they are deﬁned recursively. We give the following examples for the sake of illus-\ntration.\n79\nAppendix B. Numerical Simulation of Stochastic Diﬀerential Equation\nI(0)[f(·)]0,t =\nZ t\n0\nf(s)ds,\nI(1,1)[f(·)]0,t =\nZ t\n0\nZ s1\n0\nf(s2) dWs2 dWs1,\nI(1,1,0)[f(·)]0,t =\nZ t\n0\nZ s1\n0\nZ s2\n0\nf(s3) dWs3 dWs2 ds1.\n(B.15)\nTherefore, among the indices in the parenthesis (· · · ) 1 stands for integrating with respect\nto W, and 0 stands for integrating with respect to t, and all the integrations are multiple\nintegrals. Note that the sequence of integration should be carried out from the left to the\nright. From now on, when we do not write the [f(·)](0,t) part in the expression Iα[f(·)]0,t,\ni.e. simply as Iα, we mean Iα[1]0,t, which is a constant factor dependent only on t.\nAlso for simplicity, we use the list structure to denote multiple actions of L0 and L1:\nfv := f,\nf(j)∗α := Ljfα,\nj = 0, 1 .\n(B.16)\nFor example, we have L0L0L1f as f(0,0,1). Eq. (B.8) can be written and expanded as\nf(Xt) =f(X0) + I(0)[f(0)(·)]0,t + I(1)[f(1)(·)]0,t\n=f(X0) + I(0)\n\u0002\nf(0)(X0) + I(0)[L0f(0)(◦)]0,· + I(1)[L1f(0)(◦)]0,·\n\u0003\n0,t\n+ I(1)\n\u0002\nf(1)(X0) + I(0)[L0f(1)(◦)]0,· + I(1)[L1f(1)(◦)]0,·\n\u0003\n0,t\n=f(X0) + I(0)f(0)(X0) + I(0)\n\u0002\nI(0)[f(0,0)(◦)]0,· + I(1)[f(1,0)(◦)]0,·\n\u0003\n0,t\n+ I(1)f(1)(X0) + I(1)\n\u0002\nI(0)[f(0,1)(◦)]0,· + I(1)[f(1,1)(◦)]0,·\n\u0003\n0,t\n=f(X0) + I(0)f(0)(X0) + I(0,0)[f(0,0)(·)]0,t + I(1,0)[f(1,0)(·)]0,t\n+ I(1)f(1)(X0) + I(0,1)[f(0,1)(·)]0,t + I(1,1)[f(1,1)(·)]0,t.\n(B.17)\nThis is Eq. (B.6). Because the multiple integrals Iα decrease with the length of α and\nthe time scale t, they are supposed to decrease in value when expanded to higher orders.\nNote that a single 1 index in α provides 1\n2 order while a single 0 index provides order 1.\nWhen we expand this Itˆo–Taylor expansion, the number of its terms increases exponen-\ntially with increasing the order. We now state the Itˆo–Taylor expansion:\nf(Xt) =\nX\nα∈A\nIαfα(X0) +\nX\nα∈B(A)\nIα[fα(·)]0,t,\n(B.18)\nα ∈A and α ̸= v ⇒−α ∈A,\nB(A) = {α : α /∈A and −α ∈A},\n(B.19)\nwhere A and B are respectively called a hierarchical set and a remainder set.\n80\nAppendix B. Numerical Simulation of Stochastic Diﬀerential Equation\nWhen making a numerical approximation, we throw away the remainder part and\ntake the A part only, and we say the approximation is up to order k if B only contains\nterms of order k + 1\n2 regarding time t. We take t in the above equation as the time step dt\nfor the approximation of Xt in the deﬁning equation Eq. (B.2), so that we approximate\ndXt up to order (dt)k.\nTo calculate the multiple integrals Iα, we need an important relation:\nI(1)Iα = WtI(j1,...,jl) =\nl\nX\ni=0\nI(j1,...,ji,1,ji+1,...,jl) +\nl\nX\ni=1\nδji,1I(ji,...,ji−1,0,ji+1,...,jl),\nI(0)Iα = tI(j1,...,jl) =\nl\nX\ni=0\nI(j1,...,ji,0,ji+1,...,jl).\n(B.20)\nThat is, when we multiply the two integrals, we ﬁrst insert the index of the ﬁrst integral\ninto the index list of the second one, i.e. α, and sum over diﬀerent inserting positions,\nand then if the ﬁrst integral is a Wiener process, it cancels each Wiener integral index 1\nin the list α and produces a 0. We ﬁrst show the I(0) part:\nd(I(0)Iα) = d(tI(j1,...,jl)) = dt I(j1,...,jl) + tI(j1,...,j(l−1))dW jl,\n(B.21)\nI(0),tIα,t =\nZ t\nd(I(0),sIα,s) = I(j1,...,jl,0),t +\nZ t\nsI(j1,...,j(l−1)),sdW jl\ns ,\n(B.22)\nwhere we write dW 0 as dt. Recursively we have\nZ t\nsI(j1,...,j(l−1)),sdW jl\ns =\nZ t\nI(j1,...,j(l−1),0),sdW jl\ns +\nZ t Z s1\nsI(j1,...,j(l−2)),sdW jl−1\ns2\ndW jl\ns1\n= I(j1,...,j(l−1),0,jl),t +\nZ t Z s1\ns2I(j1,...,j(l−2)),s2dW jl−1\ns2\ndW jl\ns1,\n(B.23)\nand therefore by induction and the fact Iv = 1 we have\nI(0)Iα =\nl\nX\ni=0\nI(j1,...,ji,0,ji+1,...,jl).\n(B.24)\nThe same process applies to the case of I(1) = Wt, with the only diﬀerence being that\nd(WI(j1,...,jl)) = dW I(j1,...,jl) + WI(j1,...,j(l−1))dW jl + δjl,1dt I(j1,...,j(l−1)),\n(B.25)\ndue to the fact that dW 2 = dt. The ﬁnal result Eq. (B.20) follows straightforwardly.\nB.2\nOrder 1.5 Strong Scheme\nIn our numerical simulation, we use an order 1.5 strong approximation. The word strong\nmeans that the simulated stochastic variable approaches a certain real trajectory with\n81\nAppendix B. Numerical Simulation of Stochastic Diﬀerential Equation\nerror of order 1.5 , and this directly relates to the use of Itˆo–Taylor expansion. For con-\nsistency with Ref. [79], we rewrite Xt into Yn where n denotes the n-th time step. The\nupdate rule of Yn is essentially\nYn+1 = Yn + aI(0) + bI(1) + a(0)I(0,0) + b(0)I(0,1) + a(1)I(1,0) + b(1)I(1,1) + b(1,1)I(1,1,1), (B.26)\nwhere we have used the notation in Eq. (B.16). For completeness we present the terms\nof a and b:\na(0) = aa′ + 1\n2b2a′′,\nb(0) = ab′ + 1\n2b2b′′\na(1) = ba′,\nb(1) = bb′,\nb(1,1) = b((b′)2 + bb′′)\n(B.27)\nThen we need to evaluate the constant terms I. According to Eq. (B.20), we have\nI(0) = dt,\nI(1) = dW,\nI(0,0) = 1\n2dt2,\ndW dW = I(1)I(1) = 2I(1,1) + I(0),\ndW dt = I(1)I(0) = I(1,0) + I(0,1),\ndWI(1,1) = 3I(1,1,1) + I(1,0) + I(0,1),\n(B.28)\nwhere we have used the symbol dt to denote our iteration time step, which is sometimes\nalso written as ∆t or ∆. Thus both dt and dW are ﬁnite, and do not involve integrated\nvalues, i.e. dW 2 ̸= dt. Then we reach the result:\nI(1,1) = 1\n2(dW 2 −dt),\nI(1,0) =: dZ,\nI(0,1) = dW dt −dZ,\nI(1,1,1) = 1\n3(dW\n2 (dW 2 −dt) −dW dt) = dW(1\n6dW 2 −1\n2dt) = dW\n2\n\u0012dW 2\n3\n−dt\n\u0013\n,\n(B.29)\nwhere we have to deﬁne an additional random variable dZ. Here dZ satisﬁes\ndZ ∼N\n\u0012\n0, 1\n3dt3\n\u0013\n,\nE(dZ dW) = 1\n2dt2,\n(B.30)\nwhich is a Gaussian variable correlated to dW. Its properties are discussed in Ref. [79].\nThen we can write everything explicitly:\nYn+1 = Yn + a dt + b dW + 1\n2bb′(dW 2 −dt) + 1\n2\n\u0012\naa′ + 1\n2b2a′′\n\u0013\ndt2\n+ a′b dZ +\n\u0012\nab′ + 1\n2b2b′′\n\u0013\n(dW dt −dZ)\n+ 1\n2b\n\u0000bb′′ + (b′)2\u0001\ndW\n\u00121\n3dW 2 −dt\n\u0013\n.\n(B.31)\nThis is the order 1.5 strong Taylor scheme, and we ignore arguments when they are just\nYn. To obtain stable and precise numerical results, the deterministic second-order term\n82\nAppendix B. Numerical Simulation of Stochastic Diﬀerential Equation\nwith dt2 is also included, while the stochastic terms of the second order are not. Next,\nwe need to avoid the explicit calculation of derivatives and use the numerically evaluated\nvalues. It then becomes\nYn+1 = Yn + b dW + 1\n4 (a(Y+) + 2a + a(Y−)) dt +\n1\n4\n√\ndt\n(b(Y+) −b(Y−)) (dW 2 −dt)\n+\n1\n2\n√\ndt\n(a(Y+) −a(Y−)) dZ + 1\n2dt (b(Y+) −2b + b(Y−)) (dW dt −dZ)\n+ 1\n4dt (b(Φ+) −b(Φ−) −b(Y+) + b(Y−)) dW\n\u00121\n3dW 2 −dt\n\u0013\n,\n(B.32)\nY± = Yn + a dt ± b\n√\ndt,\nΦ± = Y+ ± b(Y+)\n√\ndt\n(B.33)\nwhich can be conﬁrmed by expanding Y± and Φ± up to order 1 of dt. This is because the\ncoeﬃcients are at least of order 1\n2, and the dt term which is supposed to approximate up\nto (dt)2 has coeﬃcient dt that is of order 1:\ndt\n4 (a(Y+) + 2a + a(Y−)) = dt\n4\n\u00004a + 2a′a dt + a′′b2 dt\n\u0001\n= a dt + dt2\n2 (aa′ + 1\n2b2a′′),\n1\n2\n√\ndt\n(b(Y+) −b(Y−)) =\n1\n2\n√\ndt\n(b′ · 2b\n√\ndt) = bb′,\n1\n2\n√\ndt\na(Y+) −a(Y−) = a′b,\n1\n2dt (b(Y+) −2b + b(Y−)) =\n1\n2dt\n\u0000b′ · 2a dt + b′′b2 dt\n\u0001\n= ab′ + 1\n2b2b′′,\n1\n2dt(b(Φ+) −b(Φ−) −b(Y+) + b(Y−)) =\n1\n2dt\n\u0010\n2b′b(Y+)\n√\ndt + 2b′′bb(Y+)dt −2bb′√\ndt\n\u0011\n=\n1\n2dt\n\u0010\n2b′(b′b\n√\ndt)\n√\ndt + 2b′′bb(Y+)dt\n\u0011\n= b(b′)2 + b2b′′.\n(B.34)\nTherefore the above update rule Eq. (B.32) is correct. However, this is not enough. In\nactual numerical approximations, the simulated vector components rotate in the com-\nplex plane, and the high-frequency components rotate faster and accumulate more error.\nBecause the error always increases the norm of the values, it accumulates and diverges\nexponentially, which is especially severe for high-frequency components that are around\nthe energy cutoﬀ. Therefore, if possible we use an implicit method that takes the update\ntarget Yn+1 into its arguments, which solves the high-frequency divergence problem by\nforcing the error not to increase the norm, and typically to shrink the values with high\nerror so that they do not accumulate or propagate. The implicit scheme update rule is\nthe following:\n83\nAppendix B. Numerical Simulation of Stochastic Diﬀerential Equation\nYn+1 = Yn + b dW + 1\n2 (a(Yn+1) + a) dt +\n1\n4\n√\ndt\n(b(Y+) −b(Y−)) (dW 2 −dt)\n+\n1\n2\n√\ndt\n(a(Y+) −a(Y−))\n\u0012\ndZ −1\n2dW dt\n\u0013\n+ 1\n2dt (b(Y+) −2b + b(Y−)) (dW dt −dZ)\n+ 1\n4dt (b(Φ+) −b(Φ−) −b(Y+) + b(Y−)) dW\n\u00121\n3dW 2 −dt\n\u0013\n,\n(B.35)\nwhere dZ, Y± and Φ± are the same as before. Note that there is an additional term\n1\n2\n√\ndt\n(a(Y+) −a(Y−))\n\u0012\n−1\n2dW dt\n\u0013\n,\nwhich equals −a′b\n2 dW dt. This is to cancel the extra b dW term inside Yn+1, which ap-\npears if we expand Yn+1 by Itˆo–Taylor expansion (see Eq. (B.31)):\nYn+1 = Yn + a dt + b dW + 1\n2bb′(dW 2 −dt) + 1\n2\n\u0012\naa′ + 1\n2b2a′′\n\u0013\ndt2 . . . ,\n(B.36)\na(Yn+1) = a + a′\n\u0012\na dt + b dW + 1\n2bb′(dW 2 −dt)\n\u0013\n+ 1\n2a′′b2 dW 2,\n(B.37)\ndt\n2 a(Yn+1) = dt\n2 a + dt2\n2 a′a + dt dW\n2\na′b + dt2\n4 a′′b2 + dt(dt −dW 2)\n4\na′′b2 + . . . ,\n(B.38)\nwhere we have ignored all stochastic terms equal to or above order 1. By taking dW 2 = dt,\nwe recover the order 1.5 Itˆo–Taylor expansion (B.31) as the explicit update scheme case.\nIn our numerical simulation of Eq. (2.31), we can split the function that governs\ndeterministic evolution as a = a1 + a2, where a1 is the Hamiltonian part a1(Y ) = HY\nwhich is linear and can be evaluated implicitly, because\nYn+1 = 1\n2(HYn+1 + HYn)dt\n⇒\n(I −dt\n2 H)Yn+1 = dt\n2 HYn\n⇒\nYn+1 = (I −dt\n2 H)−1 dt\n2 HYn.\n(B.39)\nHowever, a2 is a nonlinear part. For the case of our simulated diﬀerential equation, we\ncan only evaluate a1(Yn+1) implicitly but not a2(Yn+1), therefore we use a mix of explicit\nand implicit schemes. Because the terms\n1\n2a(Yn+1)dt −\n1\n2\n√\ndt\n(a(Y+) −a(Y−)) · 1\n2dW dt\nin the implicit scheme Eq. (B.35) replace the term\n1\n4 (a(Y+) + a(Y−)) dt\n84\nAppendix B. Numerical Simulation of Stochastic Diﬀerential Equation\nin the explicit scheme Eq. (B.32), we separately use them for a1 and a2 in our simulated\nequation, that is\n1\n2a1(Yn+1)dt −\n1\n2\n√\ndt\n(a1(Y+) −a1(Y−)) · 1\n2dW dt + 1\n4 (a2(Y+) + a2(Y−)) dt.\n(B.40)\nSubstituting it into the corresponding part 1\n4 (a(Y+) + a(Y−)) dt of Eq. (B.32), we obtain\nan update rule to use in our experiments. On the other hand, when the simulated state\nis not a vector but a density matrix, this method is hard to apply, because there seems to\nbe no readily available method to quickly solve equations of the form Y = −i[H, Y ]dt+C\nwith a banded matrix H.\nDue to the simplicity of evaluating a1, we add one more term concerning a1 into our\nupdate rule equation. First, we have linearity a′ = a′\n1 + a′\n2, and we have a′′\n1 = 0 and\na′\n1 is a constant linear mapping. We know that the values concerning a1 are typically\nlarger than those concerning a2 and b. Therefore, we add an additional deterministic\n3-rd order term 1\n6dt3(a′\n1)2a into the update rule, which corresponds to the term dt3\n6 f (3) of\nthe Taylor expansion, and it indeed also exists in the Itˆo–Taylor expansion. Because the\nimplicit method uses dt\n2 a1(Yn+1), which already includes a term dt3\n4 a′\n1a′\n1a, for implicit\nmethods we modify it by adding a term −1\n12dt3(a′\n1)2a to correct its value. This method\nturns out to reduce the numerical error, as it works in the direction to prevents the norm\nincrease.\nThis is especially important when an implicit method is not used, because\nTaylor expansion up to lower or higher orders around order 3 all results in update rules\nthat increase the norm, which would make the value diverge exponentially.\n85\nAppendix C\nDetails of the Reinforcement\nLearning Algorithm\nIn this appendix, we discuss the reinforcement learning strategies we have adopted as ex-\ntensions of the basic Deep Q-Network reinforcement learning. We have mainly followed\nthe Rainbow DQN in Ref. [58] which integrates most of the recently developed DQN\ntechniques, while there are still some diﬀerences between our implementation and theirs.\nIn the following, we introduce the involved DQN techniques one by one in Section C.1\nand present our settings in Section C.2. This appendix is intended for the background\nknowledge of Chapter 3.\nC.1\nExtensions of DQN\nC.1.1\nTarget Network and ϵ-Greedy Strategy\nAs already introduced in the earliest paper of DQN [53], it is usually necessary to keep\na target network as a reference to train the reinforcement learning network. Because the\nminimized loss in Eq. (3.11), i.e.,\nL = Q(st, at) −\n\u0012\nr(st, at) + γ max\nat+1 Q(st+1, at+1)\n\u0013\n,\n(C.1)\nis to minimize a loss of the Q-network against the Q-network itself, a direct gradient\ndescent method is found to be generally unstable. Therefore, to make it stable, in Eq.\n(C.1) we use a ﬁxed target network to evaluate the Q-function on the right and use the\ncurrently trained network to evaluate the Q-function on the left, and we use the gradient\ndescent method to modify the parameters of the currently trained network only. In order\nto synchronise the target network with the current learning progress, we update the tar-\nget network by assigning the trained network’s parameters to it after some predetermined\ntraining time, and then repeat the training. The training loss is therefore\nL = Qθ1(st, at) −\n\u0012\nr(st, at) + γ max\nat+1 Qθ2(st+1, at+1)\n\u0013\n,\n(C.2)\n86\nAppendix C. Details of the Reinforcement Learning Algorithm\nwhere θ1 represents the parameters of the currently trained neural network and θ2 repre-\nsents the target network. Only parameters θ1 are modiﬁed by the gradient descent.\nThe ϵ-greedy strategy is a strategy of taking actions for a reinforcement learning agent\n(or controller) during learning. The greedy strategy means to always take the action that\nhas the highest expected reward, and the ϵ-greedy strategy means to use the greedy strat-\negy with a probability of 1−ϵ, and with a probability of ϵ it takes a random action. This\nϵ-greedy strategy uses such an ϵ random search to explore possibilities of small deviations\nfrom the greedy strategy and encourages exploration of the action space. Typically this\nϵ hyperparameter is manually annealed during training. This technique is standard, and\nespecially it is used when no other exploration strategies are adopted.\nC.1.2\nDouble Q-Learning\nThis technique is introduced in Ref. [55]. When we look at the loss that is optimized as\nin Eq. (C.1), we can notice that there is a max operation involved, which always takes\nthe maximal value of the neural network output. As we know that the neural network\nhas ﬂuctuating parameter values, this max operation will make the neural-network esti-\nmation of the function Q always higher than what it should be. In order to make this Q\nestimation center at its correct value, we separate the decision of the best action maxat+1\nand the evaluation of the Q-function by using two diﬀerent neural networks.\nWe keep two neural networks with diﬀerent parameters θ1 and θ2. To decide the op-\ntimal action a∗\nt+1, we use the network with parameters θ1:\na∗\nt+1 = argmax\nat+1\nQθ1(st+1, at+1).\n(C.3)\nThen we evaluate this optimal action a∗\nt+1 on the other neural network θ2:\nr(st, at) + γ Qθ2(st+1, a∗\nt+1).\n(C.4)\nBecause the ﬂuctuations in the two neural networks are typically uncorrelated, this es-\ntimated Q-function does not always takes the maximum of a network’s ﬂuctuation now.\nThe above strategy does not impede training because the two networks θ1 and θ2 learn the\nreinforcement learning task simultaneously, and both of them output reasonable decisions\nthroughout training. In cases where both of them work perfectly, the decision outputted\nfrom one network would be identical to that from the other, and the loss reduces to the\nprevious case as in Eq. (C.1). In practice, it is found suﬃcient to use the target network\nin Section C.1.1 as the network θ2, and to use the trained network as θ1. Note that\nthe choice of a∗\nt+1 depends on the trained network θ1 but is not involved in the gradient\ncomputation for the gradient descent.\n87\nAppendix C. Details of the Reinforcement Learning Algorithm\nC.1.3\nDueling DQN Structure\nThis technique is introduced in Ref. [56]. As introduced in Chapter 3, deep learning usu-\nally comes with numerical imprecision and noise. Due to its properties and its gradient-\ndescent-based training, when the output value of the neural network is large, the gradients\nare large, and the numerical imprecision resulting from gradient descent is large. There-\nfore, if we have a generally large Q-function value which is the network output, it would\nbe diﬃcult for the network to learn small details of the Q-function further. However,\nsince we use the formula a∗\nt+1 = argmaxat+1 Qθ1(st+1, at+1) to decide the best action a∗\nt+1,\nit is necessary to reliably compare the Q values for diﬀerent actions at+1, which can\npossibly be very close and hard to learn. To alleviate this problem, a dueling structure\nof the DQN was proposed, which is used to separately predict the mean value of Q for\nall actions and the deviations from the mean for each action choice a. In this way, even\nthough the mean is large and has a large error, the deviations from the mean can be small\nand reliably learned, which gives a correct action choice and a better ﬁnal performance.\nC.1.4\nPrioritized Memory Replay\nThis technique is introduced in Ref. [54]. Its idea is simple: if the loss on some training\ndata is high, then there is more to learn on those data, and we should sample them\nmore often so that they are learned for more times. This strategy is especially important\nwhen the reward is sparse, or when there are only a few moments that are crucial. As\ndemonstrated in Ref. [54], in some extreme cases this strategy even accelerates the speed\nof learning for 103 times, which is striking. Therefore, it is almost always used together\nwith usual reinforcement learning.\nTo implement the prioritized sampling, we ﬁrst need to deﬁne the priority. As in\ncommon cases, we set the probability p of a data being sampled as\np ∝|L|α,\n(C.5)\nwhere L is the previously estimated loss on the data and α is a hyperparameter. In\norder to make the original optimization target unaﬀected by prioritized sampling, the\nloss of each data is rescaled by the probability of the data being sampled, so that the\nloss minimization is still done equally on the whole dataset, not merely concentrating on\nhigh loss cases.\nIn cases where the memory replay is of a limited size, when the memory replay buﬀer\nis full and new experiences should be stored, we take away the samples with the lowest\nloss from the replay buﬀer to store new experiences so that the memory is best utilized.\nBecause this strategy would potentially change the optimization target, we do so only\nwhen the memory replay can deﬁnitely not be enlarged.\n88\nAppendix C. Details of the Reinforcement Learning Algorithm\nC.1.5\nNoisy DQN\nThe noisy DQN is introduced in Ref. [57], and it is an exploration strategy that helps\nwith the ϵ-greedy exploration. As explained in Section C.1.1, the ϵ-greedy strategy only\nexplores small deviations from the deterministic greedy strategy and cannot explore to-\ntally diﬀerent long-term strategies. Instead of completely random exploration, we may\nutilize the neural network itself to explore by introducing random variables into the neu-\nral network. In Ref. [57], noisy layers are used to replace the usual network layers. It can\nbe written as\ny = (b + bnoisy ⊙ϵb) + (W + Wnoisy ⊙ϵw)x,\n(C.6)\nwhere ϵb and ϵw are sampled random variables, ⊙denotes the element-wise multiplica-\ntion, and bnoisy and Wnoisy are learned parameters that rescale the noise. In most cases\nbnoisy and Wnoisy gradually shrinks during learning, showing a self-annealing behaviour.\nConcerning implementation, we follow Ref. [57] to use factorized Gaussian noises to con-\nstruct ϵw and then rescale it by the square root function while keeping its sign.\nThere is still one technical caveat when using this strategy. During training, the noise\nparameters ϵb and ϵw should be sampled diﬀerently for diﬀerent data in a training batch;\notherwise bnoisy and Wnoisy will always experience similar gradients to those of b and W\nand it will not work correctly. For eﬃciency, one single noise can be used on multiple\ntraining data, and the number of noises used is a hyperparameter.\nAbove are the techniques that are adopted in our reinforcement learning. Two other\ntechniques in Rainbow DQN [58] are not used; however, for the sake of completeness, we\nbrieﬂy discuss and explain them in the following.\nC.1.6\nMulti-Step Learning\nAs introduced in Ref. [80], we can consider an alternative for Eq. (C.1) that learns multi-\nple time steps altogether rather than from step t to t+1. The motivation comes from the\nfact that, when the reinforcement learning learns step by step, the information of future\nrewards also propagate step by step, and if the number of steps is large, the learning\nprocess can be very slow. This results in a modiﬁed loss:\nL = Q(st, at) −\nn−1\nX\nk=0\nγkr(st+k, at+k) + γn max\nat+n Q(st+n, at+n) ,\n(C.7)\nwhere n is the number of steps that are considered altogether as a large step. This strat-\negy is heuristic, and although it can result in faster learning, it no longer preserves the\noriginal deﬁnition of the Q-function as in Section 3.2 and therefore does not theoretically\nguarantee the convergence to the optimal solution. Since in our experiments we compare\nthe optimal controls, we do not adopt this strategy.\n89\nAppendix C. Details of the Reinforcement Learning Algorithm\nC.1.7\nDistributional Learning\nAs suggested in Ref. [81], it is possible to let the neural network learn the distribution\nof future rewards rather than the Q-function which is the mean of future rewards. This\nstrategy modiﬁes the loss function drastically, changing it from the diﬀerence between two\nvalues to the diﬀerence between two distributions, as measured by the Kullbeck-Leibler\ndivergence. This strategy has the advantage that its loss function correctly represents how\nwell the AI has understood the behaviour of the environment in a stochastic setting, while\nthe loss deﬁned with the Q-function may be dominated by the variance that results from\nenvironmental noises. Therefore when combined with prioritized sampling, this distri-\nbutional learning strategy can have a signiﬁcant increase in performance in the long term.\nHowever, in order to approximately represent a probability distribution, in the origi-\nnal paper the authors discretized the space of future rewards, and used one output value\nfor each discretized point per action choice [81]. This requires us to presume the region of\nrewards that can possibly be achieved, and to apply proper discretization of the rewards.\nThese requirements contradict our original purposes of research on the quantum control\nproblems, where we have hoped to know what rewards could be achieved and how pre-\ncisely an optimal reward could be acquired. Therefore, we do not adopt this strategy.\nC.2\nHyperparameter Settings\nIn this section, we present our hyperparameter settings and discuss the important ones.\nAll our hyperparameter settings are determined empirically and we do not guarantee\ntheir optimality.\nC.2.1\nGradient Descent\nWe use the RMSprop gradient descent optimizer provided by Pytorch [42]. The learning\nrate is annealed from 2 × 104 to 1 × 106 by 5 steps. The 5 learning rates are: 2 × 10−4,\n4 × 10−5, 8 × 10−6, 2 × 10−6, 1 × 10−6. The learning rate schedules during training are\ndiﬀerent for diﬀerent problems and cases, including diﬀerent input cases, and we have\nset the learning rate schedules empirically by observing when the loss stopped to change\nand levelled oﬀ. The momentum parameter is set to be 0.9, and the ϵ parameter which\nis used to prevent numerical divergence is set to be 10−5.\nTo obtain a useful training loss for minimization, we deform the loss value L as in Eq.\n(C.1) to construct the Huber loss, i.e.,\nL′ =\n\n\n\n\n\n1\n2L2,\nif |L| < 1,\n|L| −1\n2,\nif |L| ≥1.\n(C.8)\nsuch that the training loss is a mean-squared-error when |L| is small, and it is a L1 loss\n90\nAppendix C. Details of the Reinforcement Learning Algorithm\nwhen |L| is large. Unlike the usual mean-squared-error loss, this strategy guarantees that\nlarge loss values do not result in extremely large gradients that may cause instability.\nAlso, before each update step of the parameters, we manually clip the gradient of each\nparameter such that the gradients have a maximal norm of 1.\nC.2.2\nPrioritized Replay Settings\nThe memory replay buﬀer that supports prioritized sampling is eﬃciently realized using\nthe standard Sum Tree data structure, and we have used the Sum Tree code1 released\nunder the MIT License on GitHub. Concerning the hyperparameter settings, our settings\nare diﬀerent for diﬀerent problems and they are summarized in Table C.1. Diﬀerent input\ncases for the same problem share the same settings.\nα\nβ\npϵ\nLmax\npreplace low loss\ncooling oscillators\n0.4\n0.2 →1.0\n0.001\n10\n0.9\nstabilizing inverted\noscillators\n0.8\n0.2 →1.0\n0.001\n10\n0.9\ncooling quartic os-\ncillators\n0.4\n0.2 →1.0\n0.001\n10\n0.8\nTable C.1: The prioritized replay settings used in our experiments.\nAmong the parameters in Table C.1, β denotes the extent of loss rescaling of the data\nwith diﬀerent priorities such that the optimization target is kept the same, and after each\ntraining step, it is incremented by 0.001 until it reaches 1. A small parameter pϵ is added\nto the sampling probability for every data so that all data have ﬁnite probabilities to be\nsampled. The parameter Lmax is a cutoﬀof large losses in order to compute moderate\nprobabilities, and for a probability preplace low loss we use new experience data to replace\nthe training data that have lowest losses when the replay buﬀer is full; otherwise we\nrandomly take out existing training data to store new data. Note that actually we need\nto sort out a portion of data that have the lowest losses at a time, not the single data.\nOtherwise it would be extremely computationally ineﬃcient and almost stop the training\nalgorithm. In our implementation we sort out the 1% portion of the data with lowest\nlosses each time, and then we replace them one by one.\nC.2.3\nOther Reinforcement Learning Settings\nThe period for updating the target network discussed in Section C.1.1 is set to 300 train-\ning steps. However, to facilitate learning at the initial stage, we set it to 30 steps at\nthe start, and after the simulated system has achieved a maximal time of 20 × 1\nωc during\n1https://github.com/jaromiru/AI-blog/blob/master/SumTree.py\n91\nAppendix C. Details of the Reinforcement Learning Algorithm\nwhich it does not fail, we set the target network update period to 150 steps, and after\nit has achieved 50 × 1\nωc, we set the update period to 300 steps. Note that if the training\nsucceeds, it must be able to achieve the tmax that is 100 × 1\nωc.\nConcerning the neural networks, in order to accelerate training, we follow Ref. [82] to\nseparate the learning of the weight matrices and the norms of the weight matrices. This\nstrategy is applied to both usual network layers and the noisy layers as in Section C.1.5.\nDuring training, we use a minibatch size of 512, and we sample 32 diﬀerent noises on\nthose data in the noisy layers. For computational eﬃciency, we always sample the noises\nin advance to use them later. The number of training steps is set to be proportional to\nthe number of experiences that are stored into the memory buﬀer. For the quadratic\nproblems, in the position and momentum input case, each experience is sampled and\nlearned for 8 times on average, and in other input cases, each experience is sampled for\n16 times on average. For the quartic problem, each experience is sampled for 8 times.\nTo obtain a trained neural network to evaluate, we track the performance of the\nnetworks during training. When the reinforcement learning agent performs better than\nthe current performance record, we re-evaluate it for two more times, and if the average\nperformance achieves a new performance record, we save this neural network in the\nhard disk for further evaluation after the training. Finally, we pick the 10 best trained\nnetworks to do more detailed evaluation as described in Section 4.2.1 to give our ﬁnal\nperformances. For the stabilizing inverted oscillator problem, it is hard to measure the\nperformance of a reinforcement learning agent in a single trial. Therefore, whenever the\nreinforcement learning agent succeeds in one episode, we test it for two more episodes,\nand if it succeeds for all these three trials, we store the network in the hard disk when\nsuch successful networks have already appeared for 8 times. That is, we save one network\nper eight networks that are tested to be successful. In this way we can guarantee that\nthe saved networks are not too close to each other and are roughly equispaced in the\nexperienced training time. After the training, we evaluate the performances of the 10\nlatest neural networks that are saved.\n92\nBibliography\n[1] C. L. Degen, F. Reinhard, and P. Cappellaro. Quantum sensing. Rev. Mod. Phys.,\n89:035002, Jul 2017.\n[2] Andreas Wallraﬀ, David I Schuster, Alexandre Blais, Luigi Frunzio, R-S Huang, Jo-\nhannes Majer, Sameer Kumar, Steven M Girvin, and Robert J Schoelkopf. Strong\ncoupling of a single photon to a superconducting qubit using circuit quantum elec-\ntrodynamics. Nature, 431(7005):162, 2004.\n[3] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath. Deep rein-\nforcement learning: A brief survey. IEEE Signal Processing Magazine, 34(6):26–38,\nNov 2017.\n[4] Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton.\nDeep learning.\nnature,\n521(7553):436, 2015.\n[5] Brian D. O. Anderson and John B. Moore.\nOptimal Control: Linear Quadratic\nMethods. Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1990.\n[6] Benjamin C. Kuo. Automatic Control Systems (7th Ed.). Prentice-Hall, Inc., Upper\nSaddle River, NJ, USA, 1995.\n[7] Steven Chu. Cold atoms and quantum control. Nature, 416(6877):206, 2002.\n[8] Peter Lodahl, Sahand Mahmoodian, and Søren Stobbe. Interfacing single photons\nand single quantum dots with photonic nanostructures. Rev. Mod. Phys., 87:347–400,\nMay 2015.\n[9] Ronald Hanson, Leo P Kouwenhoven, Jason R Petta, Seigo Tarucha, and Lieven MK\nVandersypen.\nSpins in few-electron quantum dots.\nReviews of modern physics,\n79(4):1217, 2007.\n[10] Iulia M Georgescu, Sahel Ashhab, and Franco Nori. Quantum simulation. Reviews\nof Modern Physics, 86(1):153, 2014.\n[11] H. H¨aﬀner, C.F. Roos, and R. Blatt. Quantum computing with trapped ions. Physics\nReports, 469(4):155 – 203, 2008.\n[12] Lucio Robledo, Lilian Childress, Hannes Bernien, Bas Hensen, Paul FA Alkemade,\nand Ronald Hanson. High-ﬁdelity projective read-out of a solid-state spin quantum\nregister. Nature, 477(7366):574, 2011.\n93\nBIBLIOGRAPHY\n[13] Shuntaro Takeda and Akira Furusawa. Perspective: Toward large-scale fault-tolerant\nuniversal photonic quantum computing. arXiv preprint arXiv:1904.07390, 2019.\n[14] Markus Aspelmeyer, Tobias J. Kippenberg, and Florian Marquardt. Cavity optome-\nchanics. Rev. Mod. Phys., 86:1391–1452, Dec 2014.\n[15] Lorenza Viola, Emanuel Knill, and Seth Lloyd. Dynamical decoupling of open quan-\ntum systems. Phys. Rev. Lett., 82:2417–2421, Mar 1999.\n[16] S. Meiboom and D. Gill. Modiﬁed spin-echo method for measuring nuclear relaxation\ntimes. Review of Scientiﬁc Instruments, 29(8):688–691, 1958.\n[17] J Werschnik and E K U Gross. Quantum optimal control theory. Journal of Physics\nB: Atomic, Molecular and Optical Physics, 40(18):R175–R211, sep 2007.\n[18] Patrick Doria, Tommaso Calarco, and Simone Montangero. Optimal control tech-\nnique for many-body quantum dynamics. Phys. Rev. Lett., 106:190501, May 2011.\n[19] Navin Khaneja, Timo Reiss, Cindie Kehlet, Thomas Schulte-Herbr¨uggen, and Stef-\nfen J. Glaser. Optimal control of coupled spin dynamics: design of nmr pulse se-\nquences by gradient ascent algorithms. Journal of Magnetic Resonance, 172(2):296\n– 305, 2005.\n[20] Ehsan Zahedinejad, Sophie Schirmer, and Barry C. Sanders. Evolutionary algorithms\nfor hard quantum control. Phys. Rev. A, 90:032310, Sep 2014.\n[21] Toshiya Takami, Hiroshi Fujisaki, and Takayuki Miyadera. Coarse-grained picture\nfor controlling quantum chaos. Advances in Chemical Physics, 130:435–458, 01 2005.\n[22] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E. Hinton. Imagenet classiﬁcation\nwith deep convolutional neural networks. In Proceedings of the 25th International\nConference on Neural Information Processing Systems - Volume 1, NIPS’12, pages\n1097–1105, USA, 2012. Curran Associates Inc.\n[23] Ewan Dunbar, Xuan Nga Cao, Juan Benjumea, Julien Karadayi, Mathieu Bernard,\nLaurent Besacier, Xavier Anguera, and Emmanuel Dupoux.\nThe Zero Resource\nSpeech Challenge 2017. arXiv e-prints, page arXiv:1712.04313, Dec 2017.\n[24] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew\nLai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel,\nTimothy Lillicrap, Karen Simonyan, and Demis Hassabis. A general reinforcement\nlearning algorithm that masters chess, shogi, and go through self-play.\nScience,\n362(6419):1140–1144, 2018.\n[25] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolf-\ngang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s\nneural machine translation system: Bridging the gap between human and machine\ntranslation. arXiv preprint arXiv:1609.08144, 2016.\n94\nBIBLIOGRAPHY\n[26] Yanping Huang, Yonglong Cheng, Dehao Chen, Hyouk Joong Lee, Jiquan Ngiam,\nQuoc V. Le, and Zhongqian Chen. Gpipe: Eﬃcient training of giant neural networks\nusing pipeline parallelism. CoRR, abs/1811.06965, 2018.\n[27] Jacob R Gardner, Paul Upchurch, Matt J Kusner, Yixuan Li, Kilian Q Weinberger,\nKavita Bala, and John E Hopcroft. Deep manifold traversal: Changing labels with\nconvolutional features. arXiv preprint arXiv:1511.06421, 2015.\n[28] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press,\n2016. http://www.deeplearningbook.org.\n[29] Murphy Yuezhen Niu, Sergio Boixo, Vadim N. Smelyanskiy, and Hartmut Neven.\nUniversal quantum control through deep reinforcement learning. npj Quantum In-\nformation, 5(1):33, 2019.\n[30] Thomas F¨osel, Petru Tighineanu, Talitha Weiss, and Florian Marquardt. Reinforce-\nment learning with neural networks for quantum feedback. Phys. Rev. X, 8:031084,\nSep 2018.\n[31] Marin Bukov, Alexandre G. R. Day, Dries Sels, Phillip Weinberg, Anatoli\nPolkovnikov, and Pankaj Mehta. Reinforcement learning in diﬀerent phases of quan-\ntum control. Phys. Rev. X, 8:031086, Sep 2018.\n[32] Shlomo Geva and Joaquin Sitte.\nThe cart-pole experiment as a benchmark for\ntrainable controllers. Technical report, Australia, 1992.\n[33] Massimiliano Rossi, David Mason, Junxin Chen, Yeghishe Tsaturyan, and Albert\nSchliesser.\nMeasurement-based quantum control of mechanical motion.\nNature,\n563(7729):53, 2018.\n[34] Michael A Nielsen and Isaac Chuang. Quantum computation and quantum infor-\nmation, 2002.\n[35] Howard M. Wiseman and Gerard J. Milburn. Quantum Measurement and Control.\nCambridge University Press, 2009.\n[36] Angel Rivas and Susana F Huelga. Open quantum systems. Springer, 2012.\n[37] Jaroslav ˇReh´aˇcek, Yong Siah Teo, Zdenˇek Hradil, and Sascha Wallentowitz. Sur-\nmounting intrinsic quantum-measurement uncertainties in gaussian-state tomogra-\nphy with quadrature squeezing. Scientiﬁc Reports, 5(1), July 2015.\n[38] L. Di´osi. Continuous quantum measurement and itˆo formalism. Physics Letters A,\n129(8):419 – 423, 1988.\n[39] Tom Michael Mitchell.\nMachine Learning.\nMcGraw-Hill International Editions.\nMcGraw-Hill, 1997.\n[40] Nvidia cuda sdk. http://www.nvidia.com/cuda, 2019.\n95\nBIBLIOGRAPHY\n[41] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\nInternational Conference on Learning Representations, 12 2014.\n[42] Tijmen Tieleman and Geoﬀrey Everest Hinton.\nNeural networks for machine\nlearning: Lecture 6a – overview of mini-batch gradient descent. http://www.cs.\ntoronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf, 2012.\n[43] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning envi-\nronment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, jun 2013.\n[44] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha\nVezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K¨uttler, John Agapiou, Julian\nSchrittwieser, John Quan, Stephen Gaﬀney, Stig Petersen, Karen Simonyan, Tom\nSchaul, Hado van Hasselt, David Silver, Timothy Lillicrap, Kevin Calderone, Paul\nKeet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and Rodney\nTsing. StarCraft II: A New Challenge for Reinforcement Learning. arXiv e-prints,\npage arXiv:1708.04782, Aug 2017.\n[45] Yue Liu, Tianlu Zhao, Wangwei Ju, and Siqi Shi. Materials discovery and design us-\ning machine learning. Journal of Materiomics, 3(3):159 – 177, 2017. High-throughput\nExperimental and Modeling Research toward Advanced Batteries.\n[46] Thomas F¨osel, Petru Tighineanu, Talitha Weiss, and Florian Marquardt. Reinforce-\nment learning with neural networks for quantum feedback. Phys. Rev. X, 8:031084,\nSep 2018.\n[47] Murphy Yuezhen Niu, Sergio Boixo, Vadim N. Smelyanskiy, and Hartmut Neven.\nUniversal quantum control through deep reinforcement learning. npj Quantum In-\nformation, 5(1):33, 2019.\n[48] T. Akinbulire, H. Schwartz, R. Falcon, and R. Abielmona. A reinforcement learn-\ning approach to tackle illegal, unreported and unregulated ﬁshing. In 2017 IEEE\nSymposium Series on Computational Intelligence (SSCI), pages 1–8, Nov 2017.\n[49] A\nsample\ntic-tac-toe\ngame.\nhttps://commons.wikimedia.org/wiki/File:\nTic-tac-toe-game-1.svg, 2007. [CC BY-SA 3.0 (http://creativecommons.org/\nlicenses/by-sa/3.0/)].\n[50] Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning,\n8(3):279–292, May 1992.\n[51] D.E. Kirk. Optimal Control Theory: An Introduction. Dover Books on Electrical\nEngineering Series. Dover Publications, 2004.\n[52] Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan,\nDavid Budden, Gabriel Barth-Maron, Hado P. van Hasselt, John Quan, Mel Ve-\ncer´ık, Matteo Hessel, R´emi Munos, and Olivier Pietquin. Observe and look further:\nAchieving consistent performance on atari. CoRR, abs/1805.11593, 2018.\n96\nBIBLIOGRAPHY\n[53] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness,\nMarc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg\nOstrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, He-\nlen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\nHuman-level control through deep reinforcement learning. Nature, 518:529 EP –,\nFeb 2015.\n[54] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized Experi-\nence Replay. arXiv e-prints, page arXiv:1511.05952, Nov 2015.\n[55] Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning\nwith Double Q-learning. arXiv e-prints, page arXiv:1509.06461, Sep 2015.\n[56] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando\nde Freitas. Dueling Network Architectures for Deep Reinforcement Learning. arXiv\ne-prints, page arXiv:1511.06581, Nov 2015.\n[57] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Os-\nband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin,\nCharles Blundell, and Shane Legg. Noisy Networks for Exploration. arXiv e-prints,\npage arXiv:1706.10295, Jun 2017.\n[58] Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski,\nWill Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow:\nCombining Improvements in Deep Reinforcement Learning. arXiv e-prints, page\narXiv:1710.02298, Oct 2017.\n[59] K. Jacobs and P. L. Knight. Linear quantum trajectories: Applications to continuous\nprojection measurements. Phys. Rev. A, 57:2301–2310, Apr 1998.\n[60] Wojciech H. Zurek, Salman Habib, and Juan Pablo Paz. Coherent states via deco-\nherence. Phys. Rev. Lett., 70:1187–1190, Mar 1993.\n[61] Christian Weedbrook, Stefano Pirandola, Ra´ul Garc´ıa-Patr´on, Nicolas J. Cerf, Tim-\nothy C. Ralph, Jeﬀrey H. Shapiro, and Seth Lloyd. Gaussian quantum information.\nRev. Mod. Phys., 84:621–669, May 2012.\n[62] A. C. Doherty and K. Jacobs. Feedback control of quantum systems using continuous\nstate estimation. Phys. Rev. A, 60:2700–2711, Oct 1999.\n[63] Francisco Soto-Eguibar and Pierre Claverie. Time evolution of the wigner function.\nJournal of Mathematical Physics, 24(5):1104–1109, 1983.\n[64] J Willard Gibbs. Elementary principles in statistical mechanics. Courier Corpora-\ntion, 2014.\n[65] M. S. Kim, F. A. M. de Oliveira, and P. L. Knight. Properties of squeezed number\nstates and squeezed thermal states. Phys. Rev. A, 40:2494–2503, Sep 1989.\n[66] Brian C. Hall. Quantum Theory for Mathematicians, volume 267. 01 2013.\n97\nBIBLIOGRAPHY\n[67] H. Goldstein, C.P. Poole, and J.L. Safko. Classical Mechanics. Addison Wesley,\n2002.\n[68] A. Lindquist. On feedback control of linear stochastic systems. SIAM Journal on\nControl, 11(2):323–343, 1973.\n[69] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,\nZachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.\nAutomatic diﬀerentiation in pytorch. In NIPS-W, 2017.\n[70] Dimitris Kakofengitis and Ole Steuernagel. Wigner’s quantum phase-space current\nin weakly-anharmonic weakly-excited two-state systems.\nThe European Physical\nJournal Plus, 132(9):381, 2017.\n[71] Carl M. Bender and Tai Tsun Wu. Anharmonic oscillator. Phys. Rev., 184:1231–\n1260, Aug 1969.\n[72] D. M. Murray and S. J. Yakowitz. Diﬀerential dynamic programming and newton’s\nmethod for discrete optimal control problems. Journal of Optimization Theory and\nApplications, 43(3):395–414, Jul 1984.\n[73] Emanuel Todorov and Weiwei Li. A generalized iterative lqg method for locally-\noptimal feedback control of constrained nonlinear stochastic systems. In Proceedings\nof the 2005, American Control Conference, 2005., pages 300–306. IEEE, 2005.\n[74] Marc Toussaint. Robot trajectory optimization using approximate inference. In Pro-\nceedings of the 26th Annual International Conference on Machine Learning, ICML\n’09, pages 1049–1056, New York, NY, USA, 2009. ACM.\n[75] Pierpaolo Benigno and Michael Woodford. Linear-quadratic approximation of opti-\nmal policy problems. Journal of Economic Theory, 147(1):1–42, 2012.\n[76] Bert Moons, Daniel Bankman, and Marian Verhelst.\nEmbedded Deep Learning:\nAlgorithms, Architectures and Circuits for Always-on Neural Network Processing.\n01 2019.\n[77] P. N. Whatmough, S. K. Lee, H. Lee, S. Rama, D. Brooks, and G. Wei. 14.3 a\n28nm soc with a 1.2ghz 568nj/prediction sparse deep-neural-network engine with\n¿0.1 timing error rate tolerance for iot applications. In 2017 IEEE International\nSolid-State Circuits Conference (ISSCC), pages 242–243, Feb 2017.\n[78] G. Desoli, N. Chawla, T. Boesch, S. Singh, E. Guidetti, F. De Ambroggi, T. Majo,\nP. Zambotti, M. Ayodhyawasi, H. Singh, and N. Aggarwal. 14.1 a 2.9tops/w deep\nconvolutional neural network soc in fd-soi 28nm for intelligent embedded systems. In\n2017 IEEE International Solid-State Circuits Conference (ISSCC), pages 238–239,\nFeb 2017.\n[79] Peter E Kloeden and Eckhard Platen. Numerical solution of stochastic diﬀerential\nequations, volume 23. Springer Science & Business Media, 2013.\n98\nBIBLIOGRAPHY\n[80] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction.\nMIT press, 2018.\n[81] Marc G. Bellemare, Will Dabney, and R´emi Munos. A Distributional Perspective\non Reinforcement Learning. arXiv e-prints, page arXiv:1707.06887, Jul 2017.\n[82] Tim Salimans and Diederik P. Kingma. Weight Normalization: A Simple Reparam-\neterization to Accelerate Training of Deep Neural Networks. arXiv e-prints, page\narXiv:1602.07868, Feb 2016.\n99\n",
  "categories": [
    "quant-ph",
    "cs.AI"
  ],
  "published": "2022-12-14",
  "updated": "2022-12-14"
}