{
  "id": "http://arxiv.org/abs/2212.08232v1",
  "title": "Offline Robot Reinforcement Learning with Uncertainty-Guided Human Expert Sampling",
  "authors": [
    "Ashish Kumar",
    "Ilya Kuzovkin"
  ],
  "abstract": "Recent advances in batch (offline) reinforcement learning have shown\npromising results in learning from available offline data and proved offline\nreinforcement learning to be an essential toolkit in learning control policies\nin a model-free setting. An offline reinforcement learning algorithm applied to\na dataset collected by a suboptimal non-learning-based algorithm can result in\na policy that outperforms the behavior agent used to collect the data. Such a\nscenario is frequent in robotics, where existing automation is collecting\noperational data. Although offline learning techniques can learn from data\ngenerated by a sub-optimal behavior agent, there is still an opportunity to\nimprove the sample complexity of existing offline reinforcement learning\nalgorithms by strategically introducing human demonstration data into the\ntraining process. To this end, we propose a novel approach that uses\nuncertainty estimation to trigger the injection of human demonstration data and\nguide policy training towards optimal behavior while reducing overall sample\ncomplexity. Our experiments show that this approach is more sample efficient\nwhen compared to a naive way of combining expert data with data collected from\na sub-optimal agent. We augmented an existing offline reinforcement learning\nalgorithm Conservative Q-Learning with our approach and performed experiments\non data collected from MuJoCo and OffWorld Gym learning environments.",
  "text": "Ofﬂine Robot Reinforcement Learning with\nUncertainty-Guided Human Expert Sampling\nAshish Kumar\nOffworld Inc., United States\nashish.kumar@offworld.ai\nIlya Kuzovkin\nOffworld Inc., United States\nilya.kuzovkin@offworld.ai\nAbstract\nRecent advances in batch (ofﬂine) reinforcement learning have shown promising\nresults in learning from available ofﬂine data and proved ofﬂine reinforcement\nlearning to be an essential toolkit in learning control policies in a model-free setting.\nAn ofﬂine reinforcement learning algorithm applied to a dataset collected by a\nsuboptimal non-learning-based algorithm can result in a policy that outperforms\nthe behavior agent used to collect the data. Such a scenario is frequent in robotics,\nwhere existing automation is collecting operational data. Although ofﬂine learning\ntechniques can learn from data generated by a sub-optimal behavior agent, there\nis still an opportunity to improve the sample complexity of existing ofﬂine rein-\nforcement learning algorithms by strategically introducing human demonstration\ndata into the training process. To this end, we propose a novel approach that\nuses uncertainty estimation to trigger the injection of human demonstration data\nand guide policy training towards optimal behavior while reducing overall sample\ncomplexity. Our experiments show that this approach is more sample efﬁcient\nwhen compared to a naive way of combining expert data with data collected from\na sub-optimal agent. We augmented an existing ofﬂine reinforcement learning\nalgorithm Conservative Q-Learning with our approach and performed experiments\non data collected from MuJoCo and OffWorld Gym learning environments.\n1\nIntroduction\nThe ﬁeld of ofﬂine reinforcement learning has emerged in the past few years as a way of training\nreinforcement learning (RL) agents from previously collected experiences and without the need for\nan interactive feedback loop with the environment [20]. Since the ﬁrst breakthrough in the ﬁeld of\ndeep reinforcement learning [24], this ﬁeld has seen considerable progress over the years, achieving\nsuper-human performance in computer games [23, 32, 31, 35], solving robotics control problem\n[2, 19, 11], improving recommendation systems [4, 13], healthcare [12], autonomous driving [25],\nand process optimization [27]. However, one of the limitations of online reinforcement learning is\nthat it relies on interaction with a dynamical system or environment, and such online interactions\ncan be expensive, especially in domains such as real-world robotics. Similar to supervised learning,\nwhere an algorithm trains a model by performing batch model updates, ofﬂine reinforcement learning\nperforms model updates by sampling batches from a dataset of state, action, reward and next state\n(SARS) tuples. The model is then trained towards the standard reinforcement learning objective of\nmaximizing expected future discounted reward, but also with a secondary objective of either keeping\nthe learned policy close to the distribution of the behavior policy that was used to collect the data\n[9, 16].\nWe propose a method that allows to reduce the overall ofﬂine learning sample complexity by tracking\nmodel uncertainty measured from an ensemble of Q-networks. High level of uncertainty shows\nthat the agent has entered an unexplored part of the state-action space indicating that introducing\nhuman demonstrations at this moment would have the most impact on learning. The algorithm\narXiv:2212.08232v1  [cs.LG]  16 Dec 2022\nmeasures the uncertainty and strategically introduces human demonstration data in-between the\nbatches sampled from sub-optima non-expert data. Through our experiments in a simulated MuJoCo\n[34] and OffWorld Gym [15] environments, we demonstrate that our approach signiﬁcantly reduces\nsample complexity compared to a model trained on a naively mixed dataset. The proposed method is\ndirectly transferable to the physical world without any additional assumptions on the properties of the\nenvironment or the learning algorithm.\nRelated Work\nOur work is most effective when applied with an algorithm that addresses some of the inherent\nissues of ofﬂine reinforcement learning like extrapolation error, q-value overestimation, and others.\nExtrapolation error is caused when there is a mismatch between the distribution of the dataset\ncollected by a behavior agent and the state-action visitation of the policy being trained. Fujimoto et al.\n[9] introduced an algorithm called Batch-Constrained Q-Learning where they propose a solution to\nreduce the extrapolation error in reinforcement learning by training a policy such that it is constrained\nto stay close to the behavior policy. Techniques like BEAR-QL [16] prevent overestimation of\nq-value by bootstrapping error reduction, while CQL, and algorithm introduced by Kumar et al. [17],\nintroduced a lower bound on the q-value to prevent over-estimation.\nThis work assumes that this algorithm is applied on a static dataset. To the best of our knowledge,\nthe current state of the art algorithms like CQL and Implicit Q-Learning (IQL) [14] have been tested\non static datasets only and there is no research work that demonstrates the application of ofﬂine\nreinforcement learning techniques on dynamic datasets.\nOver the recent years, signiﬁcant contributions have been made to the ﬁeld of ofﬂine reinforcement\nlearning towards solving such inherent challenges with the technique as deadly-triad issue [33],\nthe issue with q-value function overestimation for out-of-distribution (OOD) state-action pairs [9],\noverﬁtting and underﬁtting issues [6], to name a few. To solve the problem of q-value function\noverestimation for OOD state-action pair, algorithms like conservative Q-learning (CQL) implement\na learning objective that favors conservative estimates of the q-value function. The deadly-triad\nproblem was solved in various works, viz. Fujimoto et al. [9], Lillicrap et al. [22] and Haarnoja et al.\n[11]. The formulation of overﬁtting and underﬁtting has been established in work by Kumar et al.\n[18], where several approaches to overcome these problems have been identiﬁed.\nSince this work is based on mixing data generated by different behavior agents to create a single\ndataset, we rely on the results of Schweighofer et al. [28], Fu et al. [7], and Gulcehre et al. [10]\nthat demonstrate the feasibility of training an ofﬂine RL model on a mixed dataset that mixes SARS\ntuples obtained by different behavior policies.\nIn the online RL setting, uncertainty estimation has been leveraged for efﬁcient exploration [5], and\nfor improving exploration by reducing the uncertainty of learned implicit reward function [21]. In\nthe ofﬂine RL setting, the work by An et al. [1] quantiﬁes the uncertainty of Q-value estimates by\nusing an ensemble of Q-networks. However, in this work, uncertainty estimation is leveraged as a\npenalization term in Q-learning, while in our proposed method uncertainty acts as an indicator of\nthe learning progress. Wu et al. [36] proposed the Uncertainty Weighted Actor-Critic algorithm\nthat down-weights the contribution of OOD state-action pairs in training. This work achieves good\nperformance gains on a dataset with narrow human demonstrations, however, in their work, data\nmixing is done by mixing the human demonstration data with data generated by imitating the human\ndata. Some recent work [29], [8] have explored sampling strategies in ofﬂine RL using rank-based\nsampling or sampling prioritized experience replay where priority is given to sample transitions with\nlower epistemic uncertainty. Such methods may over-sample good samples.\n2\nPreliminaries\nReinforcement learning is a machine learning technique through which an agent learns to solve a task\nby learning from interactions with an environment. In the domain of robotic control and behavior\nmost of the algorithms are based on the Markov Decision Process. A Markov Decision Process or\nMDP is deﬁned as a tuple comprising of seven elements – (S, A, T , r, γ, S0, H), where S is the state\nspace, A is the action space, T is the state transition probability function T = P(st+1|st, at), r is the\nenvironment reward function r: S × A →R1, γ is the discount value, S0 is the start state distribution\n2\nand H is the horizon length. In this work, we consider an MDP setting with a non-zero time horizon\nand discounted cumulative rewards. A reinforcement learning algorithm interacts with an environment\nto learn a policy π, which maximizes the reinforcement learning objective J (π)=E[\nH\nP\nt=0\nγtrt]. The two\nimportant functions that help estimate the value of a state or a state and action pair are the value\nfunction Vπ(s)=E[\nH\nP\nt=0\nγtrt|s0=s] and the Q-function Qπ(s,a)=E[\nH\nP\nt=0\nγtrt|s0=s,a0=a]. There are two broad\ncategories of online reinforcement learning algorithms, viz. on-policy and off-policy. In an on-policy\nsetting, the algorithm generates an action for a particular state based on the current policy. The\nrecorded trajectory data is then used to update the current policy. Whereas in an off-policy setting\nthere is a concept of a replay buffer D, that accumulates data from different policies and the policy\nupdates are made by sampling from the replay buffer instead of generating actions from the current\npolicy. The standard Q-learning objective is to the minimize the Bellman error which is deﬁned as\nL(θ)=(Qθ(st,at)−(rt+γ maxa Q(st+1,a))) [33].\nIn ofﬂine reinforcement learning, D represents the entire replay dataset and in Q-learning setting the\nobjective is to minimize the following loss function based on Bellman error:\nL(θ) = E(s,a,r,s′)∼D[(Qθ(s, a) −(r + γEa′∼π(·|s′)[Qθ′(s′, a′)]))2]\n(1)\nwhere θ′ are the parameters of the target Q-network, softly updated for algorithm stability, while\nθ are the parameters of the running Q-network. In addition to the general reinforcement learning\nobjective or the Q-learning objective, the algorithms usually have a secondary objective of keeping\nthe learned policy close to the state-action distribution of the behavior policy. This is done to avoid\noverestimation of the Q-value, which happens when the Q-estimate error propagates during the\nbootstrapping, as shown in Equation 1. Over-estimation of Q-values can destabilize the training.\nSub-optimal agent (SOA) is represented by a behavior policy trained by QR-DQN learning algorithm\nto approximately 60% of optimal performance. While we assume that a human is an optimal (or\nclose to) agent, the SOA represents a data source that has lower performance guarantees, but is easier\nto obtain.\n3\nUncertainty-Guided Sampling in Ofﬂine RL\nThe problem we are addressing in this work is the reduction of sample complexity of existing\nofﬂine RL algorithms. In robotics, solving a task with a minimal number of samples is a goal most\npractitioners would appreciate. In many cases, there is either an absence of an optimal approach\nor a human is an expert, but creating a human demonstration dataset is prohibitively expensive.\nCombining data from expert and non-expert demonstrations into a mixed dataset can signiﬁcantly\nreduce the overall sample complexity [28], however in many practical applications the amount of\nhuman demonstrations necessary to reach close to optimal behavior remains prohibitively high.\nIn this work, we propose a method to strategically choose the order in which we pick samples from\ntwo replay buffers, one with data collected by a sub-optimal agent (DSOA) and another with data\ncollected from human demonstrations (DH). Both are available ahead of time. This allows us to boost\nthe speed of learning twice and reduce the overall sample complexity by 5 times, minimizing the\nrequired total sample size of both replay buffers required to reach close-to-optimal performance.\n3.1\nSample complexity in ofﬂine RL setting\nOfﬂine RL algorithms learn a policy by performing policy updates on data sampled\nfrom an existing dataset.\nA dataset consists of trajectories collected by performing roll-\nouts in an environment using a behavior policy µ.\nA dataset is usually of the form:\n[(s1\n0, a1\n0, r1\n0, . . . , s1\nH, a1\nH, r1\nH), . . . , (s1\n0, a1\n0, r1\n0, . . . , sN\nH, aN\nH, rN\nH)], where N is the total number of\nepisodes. A sample-efﬁcient algorithm requires a smaller dataset than an algorithm with high\nsample complexity. Sample complexity bounds are usually computed based on MDP attributes such\nas the horizon length H, the state space S, the action space A, etc. We will make the single policy\nconcentrability assumption about the behavior policy as deﬁned in [26, 37].\n3\nThe single policy concentrability assumption is deﬁned as follows: given a reference policy µ and an\noptimal policy π∗, µ is said to satisfy the assumption when\nmax\nt∈[1..H],(s,a)∈S×A\ndπ∗\nt (s, a)\ndµ\nt (s, a) ≤C∗, ∀s ∈S, a ∈A\n(2)\nfor some deterministic optimal policy π∗and coefﬁcient C∗, where d is the state-action distribution\nof a policy. Intuitively the assumption requires there is a constant C∗(concentrability coefﬁcient)\nsuch that for every possible state-action pair the ratio of probabilities of said state-action being in dπ\nand dµ is not higher than the constant C∗.\nThe concentrability coefﬁcient C∗∈[1, ∞) is the smallest value that satisﬁes Equation 2. When\nthe reference policy is exactly equal to the optimal policy µ = π∗the concentrability coefﬁcient\nC∗= 1. C∗estimates the difference between the state-action distribution density under of reference\n(behavior) policy and the optimal policy. Recent works in the ﬁeld of sample complexity analysis of\nofﬂine RL algorithms are based on the pessimism principle for value iterations [30, 37] and express\nthe sample complexity of an ofﬂine RL algorithm as a function of S, A, H and C∗, where the upper\nbound on the number of episodes in the dataset D is O(f(S, A, H, C∗)). The theoretical foundation\nof our approach relies on two assumptions. First, the algorithms need to satisfy the single policy\nconcentrability assumption, so that their sample complexity could be expressed as a function of S, A,\nH and C∗. Second, we assume that the human demonstrator is an expert and the behavior policy µH\ncorresponding to human policy is very close to an optimal policy π∗such that the following condition\nis satisﬁed,\nC∗\nµH < C∗\nµ, ∀µ ∈M\n(3)\nwhere M is a family of non-expert behavior policies. Intuitively, inequality 3 states that if an\nalgorithm’s sample complexity can be expressed in terms of concentrability, and the assumption that\nhuman expert is close to optimal is satisﬁed, then the concentrability coefﬁcient is lower when the\nalgorithm is trained on human data.\n3.2\nUncertainty estimation\nIn online reinforcement learning, uncertainty estimation has been leveraged for effective exploration\nstrategies such as upper conﬁdence bound exploration via Q-ensembles [5]. In an ofﬂine reinforcement\nlearning setting, uncertainty estimation has been leveraged to act conservatively, and select paths\nwith low uncertainty [3]. Our approach deﬁnes uncertainty in ofﬂine reinforcement learning as\nestimated variance of the Q-estimate of a given (s, a) pair over an ensemble of M Q-networks\ntrained on the same data. In an ensemble of M Q-networks that are trained to minimize the objective\ndeﬁned in Equation 1 we deﬁne point estimators for the mean µ= 1\nM\nPM\ni=1[Qθi(s,a)] and variance\nσ2= 1\nM\nM\nP\ni=1\n(Qθi(s,a)−µ(s,a))2 of the Q-estimates. The variance σ2 characterizes the uncertainty in Q-\nfunction estimates and reﬂects the state of the training. At the start of the training, the Q-function\nestimates are expected to be noisy and have high variance. As the training progresses, the Q-network\nestimates become more accurate in the parts of the state space that were visited by the learning\nalgorithm as the critic or Q-estimator improves.\n3.3\nOfﬂine reinforcement learning with Uncertainty-Guided Expert Sampling\nIntroducing expert demonstrations into the training dataset helps reduce the sample complexity\nand also helps improve performance when compared to a mixed dataset. Having deﬁned sample\ncomplexity analysis and uncertainty estimation in the previous sections, we propose the Uncertainty-\nGuided Expert Sampling (UGES) algorithm (see Supplementary Algorithm 1) that leads to more\nefﬁcient use of available human data in conjunction with ofﬂine data collected from a sub-optimal\nbehavior policy. Above-threshold uncertainty in the model ensemble triggers the algorithm to sample\nthe next SARS tuple from the replay buffer DH containing human demonstrations. Strategic sampling\nallows to keep the buffer of human demonstrations small while providing the same beneﬁt to the\noverall learning process as a naive sampling on a larger human dataset would.\n4\nAlgorithm 1 Ofﬂine Reinforcement Learning with Uncertainty-Guided Expert Sampling (UGES)\nInput: Human dataset DH\n=\n\b\n(si\nt, ai\nt, ri\nt)\n\tN,H\ni,t=1, Sub-optimal agent dataset DSOA\n=\n\b\n(si\nt, ai\nt, ri\nt)\n\tN,H\ni,t=1, E = Number of training iterations, ϵ = Uncertainty threshold, M = Number\nof Q-networks\nOutput: Qθ = Q-function, for actor-critic methods only : πφ = policy\n1: Initialization Initialize Q-Ensemble Qm\nθ = Qm\nθ0∀m ∈[1..M], for actor-critic methods only: πφ\n= πφ0, S = {} buffer for sampling\n2: for training step i in {1..E} do\n3:\nCalculate µ =\n1\nM\nPM\ni=1[Qθi(s, a)], σ2 =\n1\nM\nM\nP\ni=1\n(Qθi(s, a) −µ)2\n4:\nif σ < ϵ then\n5:\nS = Randomly sample from DSOA\n6:\nelse\n7:\nS = Randomly sample from DH\n8:\nend if\n9:\nL(θ) = Critic Loss based on an ofﬂine RL objective\n10:\nθi\nt := θi\nt−1 −ηQ∇L(θ)∀θ ∈[1..M]\n11:\nPolicy improvement for actor-critic only: φt := φt−1 −ηπEs∼S,a∼πφ(·|s)[min\n1,M Qi(s, a) −\nlog πφ(a|s)]\n12: end for\n4\nExperimental Results\nWe experimentally validate that uncertainty-based sampling strategy allows for a signiﬁcant reduction\nin overall sample complexity and explore the beneﬁts of combining the two sources of ofﬂine data.\nWe compare two ways of mixing the data during learning: a naive (random) combination strategy\nis compared against the UGES method in terms of speed of learning, resulting agent performance,\noverall sample complexity, and amount of human data required to reach close-to-optimal performance.\n(a) MuJoCo Cheetah\n(b) MuJoCo Ant\n(c) OffWorld Gym Monolith\nFigure 1: Learning environments we used to test the uncertainty-guided expert sampling approach.\nThe experiments were performed using datasets generated from three learning tasks in simulated\nenvironments shown on Figure 1. In addition to the standard MuJoCo tasks Cheetah and Ant (D4RL\nbenchmark datasets [7]), UGES was tested in a simulated version of the OffWorld Gym Monolith\nenvironment that contains a vision-based task: a mobile robot explores the environment and gets\nrewarded when it reaches a monolith (goal) in the center of the ﬁeld. A sparse reward of +1 is given\nwhen the robot is within a small distance from the goal with no step penalty.\nTo facilitate a fair comparison, we use the number of successful trajectories as the characteristic\nof a size of a dataset. A successful trajectory is deﬁned as one that leads to a positive reward (see\nSupplementary Figure 3a for a comparison between training on human demonstrations (expert data)\nversus on data collected by a sub-optimal agent. In all experiment we maintain the 5:1 ratio of\nsuboptimal to expert data. In Cheetah and Ant experiments the dataset was 320, 000 successful\ntrajectories from the suboptimal agent and 80, 000 expert ones. In the Monolith environment the\nofﬂine data consisted of 10, 000 SOA trajectories and 2, 500 expert human demonstrations. In our\n5\nexperiments, the uncertainty threshold, ϵ, was tuned experimentally and the best threshold value was\nused in all three environments.\n(a) Uncertainty-based sampling leads to\nmore than 2x faster convergence in Mu-\nJoCO Cheetah environment and reaches\nbetter overall level of performance.\n(b) Solving MuJoCo Ant environment\nwith UGES from ofﬂine data is faster\nand leads to higher performance.\n(c) In a simulation of a\nvision-based robotic task,\nUGES demonstrates more\nefﬁcient data utilization\nand higher performance\nFigure 2: Comparison between the episodic returns of CQL agents trained with UGES versus naive\nsampling of the human expert data. The conﬁdence intervals are based on 5 to 10 runs.\n4.1\nUncertainty-Guided Expert Sampling leads to a more efﬁcient expert data utilization\nAcross all experiments we can see a signiﬁcant improvement of data utilization efﬁciency when\nsampled are picked based on agent’s uncertainty estimate. Subsequent experiments in the Monolith\nenvironment that allowed the naive mixing strategy to have access to a larger dataset of human\nexpert trajectories showed that in this environment the naive strategy needed approximately 5x more\nsuccessful human expert trajectories to follow UGES’ learning curve (see Supplementary Figure 3b).\n4.2\nUncertainty-Guided Expert Sampling reaches higher performance\nWe compare the episodic returns of online evaluations performed on agents trained using UGES versus\nConservative Q-Learning (CQL) [17] agents trained on a combined dataset with 80% successful\ntrajectories coming from the SOA agent and 20% being human expert trajectories. The result shown\non Figure 2 demonstrates that already in the early stages of learning the algorithm proposed in this\nwork makes a more efﬁcient use of available samples, learning faster than a naive dataset combination\nstrategy and consistently reaching higher level of performance.\nConclusion\nGenerating a large dataset for ofﬂine RL can could be time-consuming, and algorithms that reduce\nsample requirements can make a difference. Human demonstrations provide great learning signal\nfor Ofﬂine RL, but collecting such data is prohibitively expensive, especially in real-world robotics.\nThese constraints lead us to consider using a combination of two sources of ofﬂine data: a limited\namount of “expensive\" human demonstrations with a dataset of “cheap\" autonomously collected\nexperiences. In this work we show how uncertainty estimation can guide strategic introduction of the\nhuman samples into the learning process leading to signiﬁcant reduction in overall sample complexity.\nLimitations\nThe theoretical analysis in Section 3.1 is based on the assumption that the sample complexity of the\nCQL algorithm (and other ofﬂine RL algorithms that are being used in practice) can be expressed in\nterms of a set of MDP characteristics and the concentrability coefﬁcient. However, to our knowledge,\nthis assumption has not been explicitly proven for the CQL algorithm.\nDue to a wide range of experimental conditions, we relied on data generated by interactions with a\nsimulated environment, while our main aim is to make ofﬂine RL sample complexity sufﬁciently low\nto facilitate learning in the real physical world. The robotic benchmark learning environment chosen\n6\nfor this maintains a close relationship between its real and simulated versions of the environment.\nSince our results do not depend on the properties of the environment we believe that the empirical\nresult shows in this work is directly transferable to the physical environment. Working with simulated\ndata has allowed us to experiment with a broader set of experimental conditions than would not be\npossible otherwise. Our next immediate step is to validate the method on a real robot.\nAcknowledgements\nThe authors would like to thank Dylan Wishner and Felix Lu for advice and early version of the\nTianshou code adaptation, and for suggestions and discussions they contributed during our work on\nthis manuscript.\nReferences\n[1] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based ofﬂine reinforcement\nlearning with diversiﬁed q-ensemble. Advances in Neural Information Processing Systems, 34, 2021.\n[2] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub\nPachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand\nmanipulation. The International Journal of Robotics Research, 39(1):3–20, 2020.\n[3] Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in ﬁxed-dataset\npolicy optimization. arXiv preprint arXiv:2009.06799, 2020.\n[4] Jia-Wei Chang, Ching-Yi Chiou, Jia-Yi Liao, Ying-Kai Hung, Chien-Che Huang, Kuan-Cheng Lin,\nand Ying-Hung Pu. Music recommender using deep embedding-based features and behavior-based\nreinforcement learning. Multimedia Tools and Applications, 80(26):34037–34064, 2021.\n[5] Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. Ucb exploration via q-ensembles.\narXiv preprint arXiv:1706.01502, 2017.\n[6] Dylan J Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu. Ofﬂine reinforcement\nlearning: Fundamental barriers for value function approximation. arXiv preprint arXiv:2111.10919, 2021.\n[7] Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n[8] Yuwei Fu, Di Wu, and Benoit Boulet. Benchmarking sample selection strategies for batch reinforcement\nlearning. 2021.\n[9] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without explo-\nration. In International Conference on Machine Learning, pages 2052–2062. PMLR, 2019.\n[10] Caglar Gulcehre, Sergio Gómez Colmenarejo, Ziyu Wang, Jakub Sygnowski, Thomas Paine, Konrad\nZolna, Yutian Chen, Matthew Hoffman, Razvan Pascanu, and Nando de Freitas. Regularized behavior\nvalue estimation. arXiv preprint arXiv:2103.09575, 2021.\n[11] Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine. Learning to walk\nvia deep reinforcement learning. arXiv preprint arXiv:1812.11103, 2018.\n[12] Pierre Humbert, Julien Audiffren, Clément Dubost, and Laurent Oudre. Learning from an expert. In\nPorceedings of 30th Conference on Neural Information Processing Systems (NIPS), pages 1–5, 2016.\n[13] Wacharawan Intayoad, Chayapol Kamyod, and Punnarumol Temdee. Reinforcement learning for online\nlearning recommendation system. In 2018 Global Wireless Summit (GWS), pages 167–170. IEEE, 2018.\n[14] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Ofﬂine reinforcement learning with implicit q-learning.\narXiv preprint arXiv:2110.06169, 2021.\n[15] Ashish Kumar, Toby Buckley, John B Lanier, Qiaozhi Wang, Alicia Kavelaars, and Ilya Kuzovkin. Offworld\ngym: open-access physical robotics environment for real-world reinforcement learning benchmark and\nresearch. arXiv preprint arXiv:1910.08639, 2019.\n[16] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning\nvia bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.\n7\n[17] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine\nreinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020.\n[18] Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workﬂow for ofﬂine\nmodel-free robotic reinforcement learning. arXiv preprint arXiv:2109.10813, 2021.\n[19] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning hand-eye\ncoordination for robotic grasping with deep learning and large-scale data collection. The International\njournal of robotics research, 37(4-5):421–436, 2018.\n[20] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tutorial,\nreview, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n[21] Xinran Liang, Katherine Shu, Kimin Lee, and Pieter Abbeel. Reward uncertainty for exploration in\npreference-based reinforcement learning. arXiv preprint arXiv:2205.12401, 2022.\n[22] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David\nSilver, and Daan Wierstra.\nContinuous control with deep reinforcement learning.\narXiv preprint\narXiv:1509.02971, 2015.\n[23] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra,\nand Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602,\n2013.\n[24] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through\ndeep reinforcement learning. nature, 518(7540):529–533, 2015.\n[25] Daniel Chi Kit Ngai and Nelson Hon Ching Yung. A multiple-goal reinforcement learning method for\ncomplex vehicle overtaking maneuvers. IEEE Transactions on Intelligent Transportation Systems, 12(2):\n509–522, 2011.\n[26] Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging ofﬂine reinforcement\nlearning and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems,\n34, 2021.\n[27] Manuel Schneckenreither and Stefan Haeussler. Reinforcement learning methods for operations research\napplications: The order release problem. In International Conference on Machine Learning, Optimization,\nand Data Science, pages 545–559. Springer, 2018.\n[28] Kajetan Schweighofer, Markus Hofmarcher, Marius-Constantin Dinu, Philipp Renz, Angela Bitto-Nemling,\nVihang Patil, and Sepp Hochreiter. Understanding the effects of dataset characteristics on ofﬂine reinforce-\nment learning. arXiv preprint arXiv:2111.04714, 2021.\n[29] Yichen Shen, Zhou Fang, Yunkun Xu, Yu Cao, and Jiangcheng Zhu. A rank-based sampling framework for\nofﬂine reinforcement learning. In 2021 IEEE International Conference on Computer Science, Electronic\nInformation Engineering and Intelligent Control Technology (CEI), pages 197–202. IEEE, 2021.\n[30] Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic q-learning for ofﬂine reinforcement\nlearning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890, 2022.\n[31] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc\nLanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play\nwith a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017.\n[32] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human\nknowledge. nature, 550(7676):354–359, 2017.\n[33] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n[34] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In\n2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026–5033. IEEE, 2012.\n[35] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung\nChung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft\nii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n8\n[36] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin\nGoh. Uncertainty weighted actor-critic for ofﬂine reinforcement learning. arXiv preprint arXiv:2105.08140,\n2021.\n[37] Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy ﬁnetuning: Bridging sample-\nefﬁcient ofﬂine and online reinforcement learning. Advances in neural information processing systems, 34,\n2021.\n9\nSupplementary Materials\nTables and Figures\nBehavior policy\nSuccessful trajectories\nTime Steps\nHuman\n200\n5,000\nSuboptimal agent\n300\n13,500\nSuboptimal agent\n500\n21,000\nSuboptimal agent\n900\n40,000\nSuboptimal agent\n1,400\n60,000\nSuboptimal agent\n1,600\n70,000\nTable 1: Correspondence between the number of successful trajectories and the total number of time\nsteps in the dataset collected from the OffWorld Gym environment.\n(a) With the same number of successful trajectories in\nthe dataset, the trajectories provided by a human expert\nlead to faster learning and higher ﬁnal performance.\n(b) In OffWorld Gym Monolith experiment 5x\nhuman expert data were required for a naive sam-\npling strategy to reaches a level of performance\nsimilar to UGES.\nFigure 3: Supplementary experiments. (a) Comparing learning performance using human data versus\nrelying on sub-optimal agent data only. (b) We conducted a set of experiment gradually increasing\nthe amount of ofﬂine data available to the naive strategy to mark the moment when the performance\nreaches that of UGES.\nParameters\nThe table 2 contains the values of various parameters used in our MuJoCo experiments.\nParameters used in the Monolith environment are in table 3.\n10\nParameter\nValue\nNumber of Ensemble, M\n10\nUncertainty threshold, ϵ\n16\nActor learning rate, ηπ\n0.0001\nCritic learning rate, ηQ\n0.0003\nBatch Size\n256\nNumber of iterations, E\n36\nGamma, γ\n0.99\nRL Algorithm\nSoft-Actor Critic\nHidden layer sizes\n[256, 256]\nTable 2: Values of various parameters used in experiments.\nParameter\nValue\nNumber of Ensemble, M\n10\nUncertainty threshold, ϵ\n16\nCritic learning rate, ηQ\n0.0001\nBatch Size\n32\nNumber of iterations, E\n36\nGamma, γ\n0.99\nRL Algorithm\nQR-DQN\nTable 3: Values of various parameters used in experiments.\n11\n",
  "categories": [
    "cs.LG",
    "cs.RO"
  ],
  "published": "2022-12-16",
  "updated": "2022-12-16"
}