{
  "id": "http://arxiv.org/abs/2212.09873v2",
  "title": "A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Models",
  "authors": [
    "Karin de Langis",
    "Dongyeop Kang"
  ],
  "abstract": "There is growing interest in incorporating eye-tracking data and other\nimplicit measures of human language processing into natural language processing\n(NLP) pipelines. The data from human language processing contain unique insight\ninto human linguistic understanding that could be exploited by language models.\nHowever, many unanswered questions remain about the nature of this data and how\nit can best be utilized in downstream NLP tasks. In this paper, we present\neyeStyliency, an eye-tracking dataset for human processing of stylistic text\n(e.g., politeness). We develop a variety of methods to derive style saliency\nscores over text using the collected eye dataset. We further investigate how\nthis saliency data compares to both human annotation methods and model-based\ninterpretability metrics. We find that while eye-tracking data is unique, it\nalso intersects with both human annotations and model-based importance scores,\nproviding a possible bridge between human- and machine-based perspectives. We\npropose utilizing this type of data to evaluate the cognitive plausibility of\nmodels that interpret style. Our eye-tracking data and processing code are\npublicly available.",
  "text": "A Comparative Study on Textual Saliency of Styles\nfrom Eye Tracking, Annotations, and Language Models\nKarin de Langis\nUniversity of Minnesota\ndento019@umn.edu\nDongyeop Kang\nUniversity of Minnesota\ndongyeop@umn.edu\nAbstract\nThere is growing interest in incorporating eye-\ntracking data and other implicit measures of hu-\nman language processing into natural language\nprocessing (NLP) pipelines. The data from\nhuman language processing contain unique in-\nsight into human linguistic understanding that\ncould be exploited by language models. How-\never, many unanswered questions remain about\nthe nature of this data and how it can best be\nutilized in downstream NLP tasks. In this pa-\nper, we present eyeStyliency, an eye-tracking\ndataset for human processing of stylistic text\n(e.g., politeness). We develop a variety of meth-\nods to derive style saliency scores over text us-\ning the collected eye dataset. We further inves-\ntigate how this saliency data compares to both\nhuman annotation methods and model-based\ninterpretability metrics. We find that while eye-\ntracking data is unique, it also intersects with\nboth human annotations and model-based im-\nportance scores, providing a possible bridge\nbetween human- and machine-based perspec-\ntives. We propose utilizing this type of data\nto evaluate the cognitive plausibility of models\nthat interpret style. Our eye-tracking data and\nprocessing code are publicly available.1\n1\nIntroduction\nHuman perception and understanding of text is crit-\nical in NLP. Typically, this understanding is lever-\naged in the form of ground-truth human annotations\nin supervised learning pipelines, or in the form of\nhuman evaluations of generated text. However,\nhuman language understanding is complex; mul-\ntiple cognitive processes work together to enable\nreading, many of which occur automatically and\nunconsciously (DeVito, 1970).\nBecause of the complexity, disciplines con-\ncerned with understanding and modeling how hu-\nmans read – e.g., psycholinguistics and cognitive\nscience – heavily utilize implicit measures of the\n1https://github.com/minnesotanlp/eyeStyliency\nBERT:\n@Delta.  Are you kidding?  Delayed from 7:30pm to\n11:00 - only to cancel it because the pilot is overtime.\nAnnotation:\n@Delta.  Are you kidding?  Delayed from 7:30pm to\n11:00 - only to cancel it because the pilot is overtime.\nEye Tracking:\n@Delta.  Are you kidding?  Delayed from 7:30pm to\n11:00 - only to cancel it because the pilot is overtime.\nFigure 1: Salient words for impoliteness from three different\nperspectives. We find that eye tracking data contains some\noverlap between machine and human-annotated salience.\nhuman reading experience that capture signals from\nthese automatic processes in real time. Examples\nof implicit measures include event-related poten-\ntial, reaction times, and eye movements. In con-\ntrast, explicit measures include surveys and other\nmethods that directly ask people to report their per-\nceptions and experiences. We posit that traditional\nNLP pipelines, which have widely used explicit\nmeasures of human understanding, can also benefit\nfrom implicit measures. In this paper, we focus\nspecifically on the use of eye movements as an im-\nplicit measure of textual saliency.\nRecent research in NLP has demonstrated the\nfeasibility of incorporating various types of eye\nmovement data into NLP models in order to im-\nprove performance on a number of tasks (see Ta-\nble 2 for an overview). However, this is still an\nunderexplored area: best practices remain unclear,\nand it’s not obvious whether there are tasks that\nare unsuitable for eye movement data, or how eye\nmovement data should be balanced with traditional\nannotation data. In this work, we address two main\nresearch questions: RQ1: Does eye-tracking-based\nsaliency meaningfully differ from simply gather-\ning word-level human annotations, or from model-\nbased word importance measures? RQ2: How can\nwe measure eye movements specific to a high-level\ntextual feature like style, and which eye tracking\nmetrics and data processing methods are best suited\nto capturing textual saliency?\narXiv:2212.09873v2  [cs.CL]  22 Oct 2023\nTo address these questions, we conduct an eye\ntracking case study in which participants read texts\nthe HummingBird dataset (Hayati et al., 2021).\nWe choose this dataset because it contains lexical-\nlevel human annotations indicating which words\ncontribute to the text’s style and because its do-\nmain (textual styles) has not to our knowledge been\nwidely explored for eye tracking applications – al-\nthough prior work investigates eye tracking and\nsentiment analysis, it does not extend to other lin-\nguistic styles such as politeness.\nWe collect style-specific eye movements through\na carefully designed experiment (see Section 3 for\ndetails), and we use these eye movements to de-\nrive saliency scores over the text. We compare this\neye-based saliency to human annotations as well\nas two large language model (LLM)-derived im-\nportance scores: integrated gradient scores from\na BERT model fine-tuned on style datasets (Hay-\nati et al., 2021), and word-surprisal scores from\nGPT-2 (Radford et al., 2019) (see Figure 1 for an\nexample). Our findings indicate that eye-tracking-\nbased saliency highlights some unique areas of the\ntext, but it also intersects with both saliency from\nmodel-based metrics and saliency from human an-\nnotations, making a bridge of sorts between the\nhuman- and machine-based perspectives. We dis-\ncuss some implications of these findings for NLP\nresearch.\nSpecifically, our contributions are:\n• An experimental paradigm for obtaining eye\ntracking-based signals for specific features of\ntext (in our case, textual style).\n• A first-of-its-kind eye movement dataset on\nstyle saliency, collected from 20 participants\nand consisting of both control readings and\nstyle-focused readings for polite, impolite, pos-\nitive, and negative textual styles.\n• An illustration of the distinction between this\ndataset’s explicit human annotations and im-\nplicit human eye data through a unique com-\nparison between salient text obtained via an-\nnotation and via eye tracking.\n2\nRelated Work\nEye tracking has been a staple of psycholinguis-\ntic investigations of reading for decades (Rayner,\n1978; Just and Carpenter, 1980). Eye movement\ndata is compelling because it provides realtime in-\nformation about how people process language in a\nnatural, ecologically valid setting (i.e., there is no\nNLP Area\nH M\nlearning\nfrom eye\ndata\nOurs\nTextual\nStyle\n✓✓\n✗\nKuribayashi et al. (2021) Perplexity\n✗✓\n✗\nMalmaud et al. (2020)\nQA\n✗✗Joint learning\nBolotova et al. (2020)\nQA\n✗✓\n✗\nSood et al. (2020b)\nQA\n✗✓\n✗\nSood et al. (2020a)\nParaphrasing\n✗✗Joint learning\nHollenstein et al. (2019) Sentiment\nClf., NER\n✗✗Joint learning\nBarrett et al. (2018)\nPoS tagging\n✗✗\nHMM\nTokunaga et al. (2017)\nNER\n✗✗\n✗\nKlerke et al. (2015)\nSummarization ✓✗\n✗\nGreen (2014)\nParsing\n✗✗\n✗\nTable 1: A summary of prior work applying eye tracking meth-\nods to NLP. The H column indicates whether traditional hu-\nman annotations are considered in relation to the eye tracking\ndata, and the M indicates whether model attention is consid-\nered. Most prior research has focused on either (a) comparing\nand contrasting eye movements with various models’ atten-\ntion mechanisms, or (b) using eye movements for multi-task\nlearning, where NLP task performance can be improved by a\nmodel that jointly learns to predict eye movements in addition\nto the relevant NLP task. To our knowledge, there have not\nbeen three-way comparisons between attention mechanisms\nfrom eye tracking, large language models, and manual human\nannotations.\nexplicit experimental task, such as question answer-\ning, for participants to complete) (Kaiser, 2013).\nEye data provides insight into cognitive processes\nthrough the eye-mind assumption, which posits\nthat (1) our eyes fixate on whatever our brains are\ncurrently processing, and (2) as cognitive effort\nto process an item increases, the amount of time\nthat the eyes fixate on that item also increases (Just\nand Carpenter, 1980). Analysis of eye data under\nthis framework has led to important insights into\nmany unconscious phenomena in human language\ncomprehension, e.g. the mechanisms involved in\nambiguity resolution during reading (Traxler and\nFrazier, 2008).\nEye Tracking in NLP. Due to the eye-mind\nassumption, eye-tracking data is particularly well-\nsuited to inferring patterns of reader attention, or\nsaliency, over text. This saliency information has\nso far shown promising results when integrated into\nNLP models for question answering (e.g. Malkin\net al. (2022); Sood et al. (2020a); Malmaud et al.\n(2020)). However, this is still a developing research\narea: there is limited available data, and there is\nlittle consensus regarding how to effectively collect\ndata and incorporate it into NLP pipelines. To our\nknowledge there is no previous research that inves-\ntigates saliency for style via eye tracking, nor any\nprevious research that compares saliency from eye\ntracking to human annotations (Table 1 compares\nour work with the prior work).\nOutside of textual saliency, eye-tracking data has\nbeen leveraged for a variety of NLP tasks. Mishra\net al. (2013) quantify the difficulty of sentences\nin machine translation tasks using eye movement\ndata; Mishra et al. (2016) determine whether a\nreader understands sarcasm in text, and Søgaard\n(2016) evaluate the quality of word embeddings\nand text generations, respectively. Other work uses\nexisting datasets, sometimes augmenting the data\nwith a learned gaze predictor model, and uses this\neye movement data as an additional signal when\ntraining models for various NLP tasks, includ-\ning named entity recognition (Hollenstein et al.,\n2019; Tokunaga et al., 2017), paraphrasing (Sood\net al., 2020b), part-of-speech tagging (Barrett et al.,\n2018), and sentiment analysis (see also Mathias\net al. (2020) for a review).\nSaliency in Linguistic Styles. People apply\nstyles to language in order to express attitudes, re-\nflect interpersonal intentions or goals, or convey so-\ncial standings of the speaker or listener. (Note that\nwhile many sociolinguistics theories distinguish\nbetween textual style and textual attributes, in this\nwork, we follow the common convention in recent\nNLP papers of broadly using ‘style’ to encompass\nboth of these ideas (Jin et al., 2022).) The meaning\nexpressed by textual styles can be significant; in\nfact, there is strong evidence that effective commu-\nnication requires an understanding of both style and\nliteral semantic meaning (Hovy, 1987). Although\nBERT (Devlin et al., 2018) based fine-tuned mod-\nels show strong performance on style classification,\nthere are notable differences between how BERT\nperceives style at the lexical level and how humans\nperceive it, and that using data about these differ-\nences during training improves model performance\n(Hayati et al., 2023).\n3\neyeStyliency: A Dataset of Eye\nMovement for Textual Saliency\nWe describe the data collection procedure for eye-\nStyliency dataset from 20 participants and methods\nfor computing saliency scores over text.\n3.1\nData Setups\nOur dataset consists of items from the Humming-\nbird dataset (Hayati et al., 2021) in the following\nstylistic categories: polite, impolite, positive sen-\ntiment, and negative sentiment.2 We chose this\nsubset because of the small correlation between\ncategories (other categories, e.g. anger, disgust,\nand negative sentiment are all highly correlated).\nIn this study, we limit participants’ total time\ncommitment to one hour.\nTo achieve this, the\ndataset size is 90 items across the four style cate-\ngories. (The average word count per item in the\ndataset is 21.6 overall; for the impolite, polite, neg-\native, and positive styles average word count is\n21.3, 22.8, 21.4, and 20.8, respectively.) Most par-\nticipants finished the experiment in 40-60 minutes,\ndepending on both the individual’s reading speed\nand the time needed to calibrate the individual to\nthe eye tracker.\n3.2\nEye-Tracking Measures\nMonocular eye movement data is collected with an\nEyeLink 1000 Plus3 at a rate of 1000Hz. We look\nat the following eye-tracking metrics:\n• First Fixation Duration (FFD): The duration\nof the first fixation in an interest area.\n• First Run Dwell Time (FRD): The time in-\nterval beginning with the first fixation in the\ninterest area and ending when the eye exits an\ninterest area (whether to the right or left).\n• Go Past Time (GP): The time interval begin-\nning with the first fixation in an interest area\nand ending when the eye exits the interest area\nto the left (i.e., to reread).\n• Dwell Time (DT): The total fixation duration\nfor all fixations in an interest area. Also known\nas gaze duration.\n• Reread Time (RR): The total fixation duration\nfor all fixations in an interest area after the area\nhas already been entered and exited once.\n• Pupil Size (PS): The average pupil size over\nall fixations in an interest area.\n(Note that First Run Dwell Time + Reread\nTime = Dwell Time.)\nThese measures can broadly be categorized into\nearly measures (first fixation duration, pupil size)\nthat reflect more low-level reading processes and\n2Politeness and sentiment datasets in Hummingbird are\noriginally sourced from Danescu-Niculescu-Mizil et al. (2013)\nand Socher et al. (2013).\n3Made by SR Research, Ontario, Canada; https://\nwww.sr-research.com/eyelink-1000-plus/\nApplications\nN\nFFD\nFC\nFRD\nDT\nRR\nRC\nPL\neyeStyliency (Ours)\nTextual Style\n20\n✓\n✗\n✓\n✓\n✓\n✗\n✓\nKuribayashi et al. (2021)\nLanguage model perplexity\n✗\n✗\n✗\n✗\n✓\n✗\n✗\n✗\nMalmaud et al. (2020)\nQuestion Answering\n269\n✗\n✗\n✗\n✓\n✗\n✗\n✗\nBolotova et al. (2020)\nQuestion Answering\n20\n✗\n✓\n✗\n✓\n✓\n✗\n✗\nSood et al. (2020b)\nParaphrasing\n✗\n✗\n✓\n✗\n✗\n✗\n✗\n✗\nSood et al. (2020a)\nQuestion Answering\n23\n✗\n✗\n✗\n✓\n✗\n✗\n✗\nHollenstein et al. (2019) NER, Sentiment/Relation Classification\n✗\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nBarrett et al. (2018)\nPoS tagging\n✗\n✓\n✗\n✗\n✓\n✓\n✓\n✗\nTokunaga et al. (2017)\nNamed entity recognition (NER)\n✗\n✗\n✗\n✗\n✓\n✗\n✗\n✗\nMishra et al. (2016)\nSarcasm detection\n7\n✗\n✗\n✗\n✓\n✗\n✗\n✗\nKlerke et al. (2015)\nNLG evaluation\n24\n✗\n✓\n✗\n✓\n✓\n✓\n✓\nGreen (2014)\nPhrase-structure parsing\n40\n✗\n✗\n✗\n✗\n✓\n✗\n✗\nTable 2: A comparison of prior works with respect to the eye tracking metrics studied, data processing techniques, and number\nof participants whose eye tracking data is collected. FFD = first fixation duration, FC = fixation count, RC = regression count,\nRR = reread time, PL = pupil size, N = number of participants if new eye data collected.\nlate measures (go past time, dwell time, reread\ntime) that reflect higher-level processing and mean-\ning integration (Conklin et al., 2021). Previous\neye tracking applications for NLP have commonly\nused dwell time, but a variety of measures have\nbeen examined (see Table 2). In this study, we aim\nto compare a wide variety of measures in order\nto estimate which may be best-suited to capturing\ntextual saliency. Note that to avoid redundancy, we\nchose to omit fixation counts from our analysis af-\nter finding high correlations between this measure\nand dwell time (pearson’s r = 0.93, p < 0.01).\nWe also chose to omit regression counts from our\nanalysis after finding that regression counts were\nextremely sparse – specifically, 1.8% of the dataset\nhad a non-zero regression count.\n3.3\nExperimental Procedure\nThe\nexperiment\nfollows\na\nbetween-subjects,\nblocked design. The key part of our experiment\nis the technique to isolate eye movements that are\nspecifically relevant to the text’s style. In order to\ndo this, we inform participants at the beginning of\neach block that the block will contain only stim-\nuli that share a style (polite, impolite, positive, or\nnegative) and source (Twitter, IMdB, or Stack Ex-\nchange/Wikipedia forums) – but in fact, we will\noccasionally present an incongruent style in the\nblock (e.g., present an impolite Tweet during the\npolite Tweet block). We expect that incongruency\nto cause readers to pay more attention to style-\nspecific aspects of the text, as they are unexpected.\nWe are interested in comparing the eye movements\nof participants who read a stimulus in the congruent\ncondition with those of participants who read that\nstimulus in the incongruent condition. Note that the\nexperiment has a between-subjects design, i.e. the\nsame participant does not see the same text in both\nconditions. The congruent reading of the text pro-\nvides a control. Figure 2 shows a concrete example\nof these two conditions, while Figure 3 shows a\nvisualization of these contrasted eye movements.\nFigure 4 shows a procedure of our experiments.\nThe experimental procedure is as follows (more\ndetails in Appendix A). Participants complete nine\nblocks. At the beginning of block, the participant\nis informed of the style and source, and asked to\npay attention to the style of the following texts.\nEach block contains 10 items, eight of which are\ncongruent with the target style. The remaining\ntwo items are incongruent with the target style.\nIncongruent items are counterbalanced across par-\nticipants. Blocks are presented in a random order,\nand items within the blocks are pseudorandom-\nized to ensure adequate spacing between congruent\nand incongruent trials (Egner, 2007) (there is also\na context-free text as an added control). Partici-\npants are asked True/False comprehension ques-\ntions pseudorandomly after 30% of the items in\norder to maintain motivation to read carefully. Af-\nter the experiment concludes, participants complete\nthe Perceived Awareness of Research Hypothesis\nScale (PARH) (Rubin, 2016) to evaluate whether\ndemand characterstics (Nichols and Maner, 2008)\nof the experiment may have influenced reading be-\nhavior. The study procedure was approved by the\nCongruent\nSetup\nA densely constructed, highly referential film, and an audacious return to\nform that can comfortably sit among Jean-Luc Godard's finest work. \nIncongruent\nSetup\nContext\nStimuli\nWatching its rote plot points connect\nis about as exciting as gazing at an\negg timer for 93 minutes.\nThe movie, directed by Mick Jackson,\nleaves no cliche unturned, from the\npredictable plot to the characters\nstraight out of central casting.\nAn entertaining, colorful, action-filled\ncrime story with an intimate heart.\nThe mesmerizing performances of\nthe leads keep the film grounded and\nkeep the audience riveted.\n.. highly\nreferential\nfilm, and ..\n.. highly\nreferential\nfilm, and ..\n.. highly \nreferential \nfilm, and ..\nEye-based\nsaliency\nThe following movie reviews were\nwritten by critics who disliked the film.\nThe following movie reviews were\nwritten by critics who liked the film.\nincongruent\ngaze\ncongruent gaze\n(control)\nFigure 2: Illustrative example of congruent vs incongruent\npresentation of the same stimulus. We rely on expectation\neffects to induce participants to attend to the unexpected style\n(in this case, positive sentiment); in other words, we assume\nthat the surprise regarding the style will result in longer gaze\ndurations for words that contribute to the perception of that\nstyle — in this case, words relating to positive sentiment.\ninstitutional review board (IRB).\nParticipants\nWe collect data from 20 partici-\npants (12 male, 7 female, 1 non-binary; median age\n23 years) recruited from the University community\nand word-of-mouth. An additional 6 participants\nwere recruited but unable to complete the study due\nto problems with eye calibration. Participants were\ncompensated with a $15 Amazon gift card.\nApparatus\nMonocular eye movement data is col-\nlected with an EyeLink 1000 Pro, using the desktop\nmount, at a rate of 1000Hz. Participants use a chin-\nrest while reading in order to stabilize the head.\nWe use the Experiment Builder software to present\nstimuli to participants in a 16pt serif font with 1.5\nline spacing, on our display monitor with a 508mm\ndisplay area and a 1680x1050 resolution. Partic-\nipants are seated with their eyes 50-60cm away\nfrom the display monitor.\nStudy Design Rationale\nBased on the well-\ndocumented phenomenon of expectancy effects in\ncognition (see Schwarz et al. (2016) for further\ndiscussion), we assume that the incongruent texts\nthat subvert the stylistic expectation will lead to\nparticipants reacting with surprise and increased\nprocessing difficulty in response to parts of the text\nassociated with the unexpected style.\nAlternative designs that explicitly ask partici-\npants to classify an item’s style were strongly con-\nsidered, but were rejected for two reasons: first, we\nare interested in observing a relatively natural read-\nFigure 3: Exemplary eye-tracking data showing saliency for\npolite style, with comparison to human word-level style im-\nportance highlighting. The eye-tracking data is visualized as a\nheat map showing gaze data from the incongruent style con-\ndition, with the gaze data from the congruent style (control)\ncondition subtracted.\nIntro\nBlock (specific style + medium) \nInstructions\nPractice Items\nView\nContext\nView\nStimuli\nComprehension\nQuestion\n80% congruent\n20% incongruent\nQuestions occur\nevery ~3 items\nDemand\nCharacteristic\nSurvey\nPost-survey\nRepeat 9 times\nFigure 4: Experimental procedure.\ning process and introducing a classification task\nruns counter to that goal; second, the style clas-\nsification task could increase the saliency of not\nonly the target style but also its opposing style, as\nboth can be relevant to the decision (e.g., the pres-\nence of an impolite word is relevant to the decision\nof whether a statement is polite). We also consid-\nered designs in which congruency is established\nvia explicit text labels rather than implicit expecta-\ntions, but decided to instead choose an experimen-\ntal paradigm that adheres as closely as possible to\nan ecologically valid reading task.\n3.4\nPre-processing Eye Tracking Data\nEye data was delineated into fixations and saccades\nusing the DataViewer software with EyeLink’s stan-\ndard algorithm and default velocity and accelera-\ntion thresholds. We further cleaned the data by\nremoving trials with significant track loss (i.e. tri-\nals with track loss in over 50% of the text area);\n1.5% of trials were removed due to track loss. An\noutlier analysis showed that 0.5% of fixations were\noutliers and were removed in our analysis.\n3.5\nCalculating Saliency Scores\nWe divide the text into interest areas (IAs) and\ncalculate saliency scores for each IA. We do not\nsegment the IAs such that each IA contains a single\nword, because in a single fixation people can read\na span of about 21 surrounding characters (Rayner,\n1978), meaning that many short words are not fix-\nated on, leading to difficulties with our desired\nanalyses. Instead, we use the natural language pro-\ncessing toolkit (NLTK)’s stopwords list (Bird et al.,\n2009) to define each IA such that stopwords share\nan IA with the closest non-stopword. Specifically,\neach stopword is combined with the closest non-\nstopword, with non-stopwords to the right being\npreferred in the case of a tie. We also ensure that\nno IA contains a line break.\nWe utilize two techniques for calculating each\neye tracking-based metric for each IAi. Note that\nthese techniques are applied across all eye track-\ning measures x ∈{DT, FRD, GP, DT, RR, PS} as\ndefined in Section 3.2.\n• z-score: For each participant pk, denote the\neye tracking measurement in IAi as xki. We\ncalculate the participant-specific z-score of eye\ntracking measurement from IAi as zk(IAi) =\nxki−µk\nσk\n, where µk and σk are the participant-\nspecific arithmetic mean and standard devia-\ntion, respectively. Then, the saliency score for\nIAi is given by\nPn\nk=0 zk(IAi)\nn\n.\n• raw: We aggregate the raw values of the eye\ntracking measurements from each IA. The\nsaliency score for IAi is given by\nPn\nk=0 xki\nn\n.\n4\nExperimental Results\n4.1\nComparison with Other Saliency Metrics\nWe investigate how eye tracking metrics compare\nwith other existing measures for lexical-level sig-\nnificance – namely, human annotations, integrated\ngradient scores, and large language model surprisal\nscores (see Figure 5 for a visualization of these\nscores):\n• Surprisal scores:\nFor the text in the ith\ninterest area, denoted IAi, the surprisal is\nP(IAi|IA0, IA1, ...IAi−1).\nWe obtain this\nprobability estimate from the pre-trained GPT-\n2 model (Radford et al., 2019). 4 In the event\nthat an IA includes multiple tokens, we sum\nthe surprisal of those tokens.\n• Model gradient scores: The integrated gra-\ndient method (Sundararajan et al., 2017) is\noften used to obtain scores over the input to-\nkens to a deep neural network, where a token’s\nscore reflects how much that token influenced\nthe network’s final output. We obtain these\nscores with the Captum codebase (Kokhlikyan\net al., 2020), using the fine-tuned BERT model\n4We include word-surprisal scores from GPT-2 as they\nhave previously been found to correlate with human reading\ntimes (Wilcox et al., 2020).\nfrom Hayati et al. (2021). For IAi, the in-\ntegrated gradient score is the average of the\nindividual tokens within IAi.\n• Human annotations:\nHuman annotations\ncome from the Hummingbird dataset (Hayati\net al., 2021). Three annotators per item were\nasked to highlight words that contribute to the\ntext’s style. We averaged these binary high-\nlighting scores over each annotator to arrive at\na saliency score for each interest area.\nThroughout the comparison, we answer the fol-\nlowing two questions: How much do the salient IAs\nderived from each measure overlap and how much\ndoes each measure agree on the saliency strength\nof each IA?\nTo find the overlap between salient interest ar-\neas derived from different measures, we compute\na binary saliency map over the dataset for each\nmeasure. We then compute the pairwise Jaccard\nsimilarity coefficient for each possible pairing of\nsalient text sets (Fig 6), where the Jaccard simi-\nlarity coefficient is their intersection over union.\nWe use the median saliency score as the threshold\nthat determines whether the IA is labeled “salient”\nso that each measure results in the same number\nof salient words, allowing a more straightforward\ncomparison between measures.\nWe find that the intersection over union of salient\ninterest areas from eye tracking methods and both\nintegrated gradient scores and human annotations\nfalls between 0.26 and 0.31. Critically, the three-\nway intersection over union between salient text\nfrom integrated gradients, human annotations, and\neye tracking metrics falls between 0.05 and 0.06,\nindicating that each metric captures a relatively\nunique set of text within the dataset (see Fig 7).\nWe also investigate what types of words are se-\nlected as salient by each method by performing\npart-of-speech (POS) tagging on the salient interest\nareas for each measure, finding that while distri-\nbutions of parts of speech are similar, humans se-\nlect proportionally more adjectives while eye track-\ning metrics select proportionally more verbs and\nadverbs (Figure 9). This discrepancy may be ex-\nplained by human annotators focusing more on sin-\ngle words with high stand-alone style (oftentimes\nthese are adjectives such as happy, gracious), while\npeople’s eyes attend to the context surrounding that\nword (oftentimes this context includes verbs and\nadverbs). For example, in the polite phrase “Thank\nyou for removing...,” human annotators highlight\nThank you for your kind comment. Do you have a suggestion where the portals \nshould be placed in the article? \nThank you for your kind comment. Do you have a suggestion where the portals \nshould be placed in the article? \nThank you for your kind comment.  Do you have a suggestion where the portals \nshould be placed in the article?\nThank you for your kind comment. Do you have a suggestion where the portals \nshould be placed in the article?\nAnnotation\nBERT Gradient\nGPT2 Surprisal\nEye Dwell Time\n(a) Saliency scores for politeness.\n-1: For not specifying what is to be done later with the data. If you claim \nthe question is open-ended (interview and all) then why accept an answer?\n-1: For not specifying what is to be done later with the data. If you claim \nthe question is open-ended (interview and all) then why accept an answer?\n-1: For not specifying what is to be done later with the data. If you claim \nthe question is open-ended (interview and all) then why accept an answer?\n-1: For not specifying what is to be done later with the data. If you claim \nthe question is open-ended (interview and all) then why accept an answer? \nAnnotation\nBERT Gradient\nGPT2 Surprisal\nEye Dwell Time\n(b) Saliency scores for impoliteness.\nThe movie, directed by Mick Jackson, leaves no cliche unturned, from \nthe predictable plot to the characters straight out of central casting. \nThe movie, directed by Mick Jackson, leaves no cliche unturned, from \nthe predictable plot to the characters straight out of central casting.\nThe movie, directed by Mick Jackson, leaves no cliche unturned, from\nthe predictable plot to the characters straight out of central casting.\nThe movie, directed by Mick Jackson, leaves no cliche unturned, from\nthe predictable plot to the characters straight out of central casting.\nAnnotation\nBERT Gradient\nGPT2 Surprisal\nEye Dwell Time\n(c) Saliency scores for negative sentiment.\nIt's one of those baseball pictures where the hero is stoic, the wife is patient, \nthe kids are as cute as all get-out and the odds against success are long enough \nto intimidate, but short enough to make a dream seem possible. \nIt's one of those baseball pictures where the hero is stoic, the wife is patient, \nthe kids are as cute as all get-out and the odds against success are long enough \nto intimidate, but short enough to make a dream seem possible.\nIt's one of those baseball pictures where the hero is stoic, the wife is patient, \nthe kids are as cute as all get-out and the odds against success are long enough\nto intimidate, but short enough to make a dream seem possible. \nIt's one of those baseball pictures where the hero is stoic, the wife is patient,\nthe kids are as cute as all get-out and the odds against success are long enough\nto intimidate, but short enough to make a dream seem possible.\nAnnotation\nBERT Gradient\nGPT2 Surprisal\nEye Dwell Time\n(d) Saliency scores for positive sentiment.\nFigure 5: A comparison of saliency scores from various methods: manual human annotations, language model introspection, and\neye tracking. Darker highlights indicate stronger saliency scores.\nannotations\nmodel grads\nsurprisal\ndwell time\nreread time\ngo past time\nfirst run dwell\nfirst fixation duration\npupil size\ndwell time\nreread time\ngo past time\nfirst run dwell\nfirst fixation\nduration\npupil size\nFigure 6: Confusion matrix of the Jaccard similarity score for salient text derived from each metric. (See Appendix for the\ncorrelation coefficient for saliency scores derived from each metric.)\nonly “thank you” whereas eye gaze also focuses on\nthe gerund verb “removing.”\nTo measure agreement between different mea-\nsures with respect to saliency strength, we compute\na saliency score for each IA in the dataset derived\nfrom each measure. We then compute the pair-\nwise Pearson’s r correlation coefficient, finding\nmost coefficients are near 0 (see Appendix). In\nother words, while there is some agreement across\nhuman-, machine-, and eye-based methods with\nrespect to which IAs are above median saliency,\nthere is little correlation with respect to the saliency\nscores themselves.\n4.2\nQualitative Results\nFor a qualitative visualization of saliency over the\npoliteness style, see Figure 8. In general, human an-\nnotations have a tendency to focus on segments of\ntext with clear style markers. For instance, phrases\nsuch as “please” are consistently highlighted by\nhuman annotators. Our eye tracking data indicates\nthat these phrases do not reliably draw the reader’s\ngaze during the realtime reading process. We no-\ntice that the eyes often focus on the object of the\npolitness marker rather than the politeness marker\nitself: For instance, the polite text “Thank you for\nyour kind comment,” human annotators highlight\nonly “thank you” whereas gaze data focuses on\n“your kind comment.”\n119\n102\n100\n110\n70\n75\n62\nHuman Annotations\nIntegrated Gradients\nDwell Time\nFigure 7: Venn diagram illustrating the intersection of sets\nof salient interest areas derived from Dwell Time (blue), inte-\ngrated gradients (green), and human annotations (red).\nHuman\nAnnotation \n    \nIntegrated\n   Gradients\nDwell Time\nas you wish\nto fix it\nCould you\nsorry for\ntrouble\nHi -\n@Colin \nHey, I \nthe pseudo\n@pinknumjoo\nplease\nAnniversary\nBirthday\nPlease\nHappy\nI am really\nthey work\nat around\nI should\nitself, but\nThanks,\nThank you\nchance you\npossible for\nlet me know\nHey Sai  -\nfeedback on\nat 1:10\nimportant than\n@discoverdelicio\nmuch like\nOops,\nunderstand\nlove to\npicturing it\nFigure 8: Venn diagram showing interest areas salient to the\npolite style. For each section of the Venn diagram, the interest\nareas with the top five highest saliency scores are shown.\nWe also observe that eye data, and in particular\ndwell time, shows high attention to certain nouns\n– i.e., names, usernames, and movie titles. This\ncannot be explained by word frequency effects, as\nparticipants in the control condition did not spend\nas long attending to these nouns.\n4.3\n“Eye-in-the-loop” few-shot learning\nWe utilize “eye-in-the-loop” few-shot learning in\norder to roughly probe the cognitive plausibility of\nGPT-3 (Brown et al., 2020). Our prompts present\na classification task and include zero to four ex-\namples from our dataset, including an “important\nwords” section that contains the salient text as de-\nfined by each eye-tracking measure, human anno-\ntations, and integrated gradient scores (see Sec-\ntion 3.5 for details). As a baseline, we omit the\n“important words.” We expect that if GPT-3 has a\nparticularly strong cognitive understanding of style\nHuman Annotations\nIntegrated Gradients\nDwell Time \nReread Time \nGo Past Time \nFirst Run Dwell \nFirst Fixation Duration \nPupil Size \n21%\n17%\n16%\n15%\n16%\n16%\n14%\n15%\n51%\n51%\n50%\n53%\n50%\n49%\n54%\n54%\n5%\n7%\n5%\n4%\n6%\n6%\n5%\n4%\n19%\n19%\n23%\n22%\n22%\n22%\n21%\n22%\n4%\n4%\n5%\n4%\nIN\nJJ\nNN\nRB\nVB\nother\nFigure 9: Top 5 most common parts of speech for each mea-\nsure’s salient IA set. IN: prepositions and subordinating con-\njunctions, JJ: adjectives, NN: nouns, RB: adverbs, VB: verbs.\nFigure 10: Few-shot learning classification experiment accu-\nracy scores, averaged over 5 rounds with randomly selected\ndemonstrations. Error bars indicate 95% confidence interval.\nprocessing, “important words” from eye movement\ndata may improve its task performance (in these\nexperiments, we use the text-davinci-002 model).\nResults are relatively inconsistent across each of\nthe four shots, but in most cases, it seems that in-\ncluding salient words has little effect on the model\naccuracy on the style classification task. A subset\nof the results are shown in Figure 10; see Appendix\nfor full results and prompt details.\n5\nKey Findings and Discussion\nHere we discuss the relationship between our re-\nsults and our research questions:\nRQ1: Does eye tracking data for saliency mean-\ningfully differ from simply gathering word-level\nhuman annotations, or from model-based word\nimportance measures? Our data show a substan-\ntial difference between eye-tracking-based saliency,\nmodel-based saliency, and human annotations. It\nis perhaps unintuitive that reading behavior would\ndiffer from self-reports after reading, but this is\nconsistent with findings in psycholinguistics that\nestablish strong distinctions between explicit mea-\nsures (i.e., human annotations) and implicit mea-\nsures (i.e., eye tracking) of human language pro-\ncessing. Interestingly, there is some intersection\nbetween eye tracking-based saliency and model-\nbased saliency that is not shared with human anno-\ntators. This suggests that some automatic aspects\nof human language processing, accessible through\neye tracking but not necessarily survey methods,\nmay be shared with large language models.\nRQ2: How can we measure eye movements\nspecific to a high-level textual feature like style,\nand which eye tracking metrics and data process-\ning methods are best suited to capturing textual\nsaliency? The results from our experiment indi-\ncate that our experimental paradigm exploiting con-\ngruency effects may be effective in finding eye\nmovements specific to certain text features. In a\nlinear mixed effect model analyzing the data, we\nfind significant effects of the congruency condi-\ntion on dwell time and pupil size (see Appendix\nA.2). This suggests that the congruency effect does\nimpact reading patterns – whether this impact is\ndirectly linked to the textual style is difficult to\ndefinitively answer, but given the overlap between\neye-tracking-based style saliency and other style\nsaliency measures, it seems reasonable to believe\nthat the experimental manipulation resulted in an\nimplicit measure of style perception. Experiments\nbased on congruency effects may be a promising\nroute for capturing eye movements related to other\nhigh-level textual features such as sarcasm and\nmetaphor. We find that dwell time appears to be\nthe strongest eye-tracking metrics for capturing\ntextual saliency, as it has both the highest overlap\nwith human- and machine-based saliency and most\nstrongly responded to the experimental manipula-\ntion. Using the same criteria, we also find that\nusing participant-level z-scores to represent the eye\nmovement data yields the best results.\n6\nLimitations\nIn this exploratory study, our dataset and sample\nsize are both small, limiting the possibilities for\na more thorough evaluation of the data e.g. by\nfine-tuning a language model. We also note that\nby design, our experiment presents incongruent\nitems rarely, and consequently we have consider-\nably more congruent datapoints than incongruent\ndatapoints – an inherent limitation of the proposed\nexperimental paradigm. In light of our results,\nwhich suggest that eye-tracking data contains use-\nful and unique information, we plan to develop\nmethods for collecting this kind of real-time human\nreading data at scale – i.e., without the constraints\nof costly in-person eye tracking – in future work.\nFinally, eye tracking analysis in general is lim-\nited by the eye-mind assumption, which holds that\nthe eye fixates on what the mind is currently pro-\ncessing. While there is strong evidence supporting\nthe eye-mind assumption during reading, there is a\nnotable exception: retrieval processes (i.e. access-\ning memory) are not reflected in eye movements\n(Anderson et al., 2004).\nAcknowledgements\nWe would like to thank Jeffrey Bye, Andrew Elfen-\nbein, Charles Fletcher, Shirley Hayati, Brooke Lea,\nAndreas Schramm, and Mariya Toneva for their\nvaluable feedback and insightful suggestions re-\ngarding the experimental procedure and data analy-\nsis. We would also like to thank Miguel Miguelez\nDiaz, Risako Owan, Faziel Khan, and Josh Spitzer-\nResnick for testing and critiquing the initial exper-\nimental pipeline. This research received funding\nfrom the Sony Research Innovation Award.\nReferences\nJohn R Anderson, Dan Bothell, and Scott Douglass.\n2004. Eye movements do not reflect retrieval pro-\ncesses: Limits of the eye-mind hypothesis. Psycho-\nlogical Science, 15(4):225–231.\nMaria Barrett, Joachim Bingel, Nora Hollenstein, Marek\nRei, and Anders Søgaard. 2018. Sequence classi-\nfication with human attention. In Proceedings of\nthe 22nd Conference on Computational Natural Lan-\nguage Learning, pages 302–312, Brussels, Belgium.\nAssociation for Computational Linguistics.\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Nat-\nural language processing with Python: analyzing text\nwith the natural language toolkit. O’Reilly Media,\nInc.\nValeria Bolotova, Vladislav Blinov, Yukun Zheng,\nW Bruce Croft, Falk Scholer, and Mark Sanderson.\n2020. Do people and neural nets pay attention to\nthe same words: Studying eye-tracking data for non-\nfactoid QA evaluation. In Proceedings of the 29th\nACM International Conference on Information &\nKnowledge Management, pages 85–94.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, and Amanda\net al. Askell. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Process-\ning Systems, volume 33, pages 1877–1901.\nHenry Conklin, Bailin Wang, Kenny Smith, and Ivan\nTitov. 2021. Meta-learning to compositionally gen-\neralize. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3322–3335, Online. Association for Computa-\ntional Linguistics.\nCristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan\nJurafsky, Jure Leskovec, and Christopher Potts. 2013.\nA computational approach to politeness with appli-\ncation to social factors. In 51st Annual Meeting of\nthe Association for Computational Linguistics, pages\n250–259. ACL.\nJoseph A DeVito. 1970. The Psychology of Speech\nand Language: An Introduction to Psycholinguistics.\nRandom House, New York.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nTobias Egner. 2007. Congruency sequence effects and\ncognitive control. Cognitive, Affective, & Behavioral\nNeuroscience, 7(4):380–390.\nMatthew J Green. 2014. An eye-tracking evaluation of\nsome parser complexity metrics. In Proceedings of\nthe 3rd Workshop on Predicting and Improving Text\nReadability for Target Reader Populations (PITR),\npages 38–46.\nShirley Hayati, Dongyeop Kang, and Lyle Ungar. 2021.\nDoes BERT learn as humans perceive? Understand-\ning linguistic styles through lexica. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 6323–6331.\nShirley Anugrah Hayati, Kyumin Park, Dheeraj Ra-\njagopal, Lyle Ungar, and Dongyeop Kang. 2023.\nStylex: Explaining style using human lexical annota-\ntions. In Proceedings of the 17th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics, pages 2835–2848.\nNora Hollenstein, Maria Barrett, Marius Troendle,\nFrancesco Bigiolli, Nicolas Langer, and Ce Zhang.\n2019. Advancing NLP with cognitive language pro-\ncessing signals. arXiv preprint arXiv:1904.02682.\nEduard Hovy. 1987. Generating natural language un-\nder pragmatic constraints. Journal of Pragmatics,\n11(6):689–719.\nDi Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova,\nand Rada Mihalcea. 2022. Deep learning for text\nstyle transfer: A survey. Computational Linguistics,\n48(1):155–205.\nMarcel A Just and Patricia A Carpenter. 1980. A theory\nof reading: From eye fixations to comprehension.\nPsychological Review, 87(4):329.\nElsi Kaiser. 2013. Experimental paradigms in psycholin-\nguistics. Research methods in linguistics, pages 135–\n168.\nSigrid Klerke, Héctor Martínez Alonso, and Anders\nSøgaard. 2015. Looking hard: Eye tracking for de-\ntecting grammaticality of automatically compressed\nsentences. In Proceedings of the 20th Nordic Con-\nference of Computational Linguistics (NODALIDA\n2015), pages 97–105.\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin,\nEdward Wang, Bilal Alsallakh, Jonathan Reynolds,\nAlexander Melnikov, Natalia Kliushkina, Carlos\nAraya, Siqi Yan, et al. 2020. Captum: A unified\nand generic model interpretability library for pytorch.\narXiv preprint arXiv:2009.07896.\nTatsuki Kuribayashi, Yohei Oseki, Takumi Ito, Ryo\nYoshida, Masayuki Asahara, and Kentaro Inui. 2021.\nLower perplexity is not always human-like. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 5203–5217,\nOnline. Association for Computational Linguistics.\nNikolay Malkin, Zhen Wang, and Nebojsa Jojic. 2022.\nBoosting coherence of language models. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics. (To appear).\nJonathan Malmaud, Roger Levy, and Yevgeni Berzak.\n2020. Bridging information-seeking human gaze and\nmachine reading comprehension. In Proceedings\nof the 24th Conference on Computational Natural\nLanguage Learning, pages 142–152.\nSandeep Mathias, Diptesh Kanojia, Abhijit Mishra, and\nPushpak Bhattacharya. 2020. A survey on using gaze\nbehaviour for natural language processing. In Pro-\nceedings of the Twenty-Ninth International Joint Con-\nference on Artificial Intelligence, IJCAI-20, pages\n4907–4913. International Joint Conferences on Arti-\nficial Intelligence Organization. Survey track.\nAbhijit Mishra, Pushpak Bhattacharyya, and Michael\nCarl. 2013. Automatically predicting sentence trans-\nlation difficulty. In Proceedings of the 51st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 346–351.\nAbhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal\nDey, and Pushpak Bhattacharyya. 2016. Harnessing\ncognitive features for sarcasm detection. In Proceed-\nings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1095–1104.\nAustin Lee Nichols and Jon K Maner. 2008.\nThe\ngood-subject effect: Investigating participant demand\ncharacteristics. The Journal of general psychology,\n135(2):151–166.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nKeith Rayner. 1978. Eye movements in reading and\ninformation processing.\nPsychological Bulletin,\n85(3):618.\nMark Rubin. 2016. The perceived awareness of the\nresearch hypothesis scale: Assessing the influence of\ndemand characteristics. Figshare, 10:m9.\nKatharina A Schwarz, Roland Pfister, and Christian\nBüchel. 2016. Rethinking explicit expectations: con-\nnecting placebos, social cognition, and contextual\nperception. Trends in cognitive sciences, 20(6):469–\n480.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nAnders Søgaard. 2016. Evaluating word embeddings\nwith fmri and eye-tracking. In Proceedings of the 1st\nworkshop on evaluating vector-space representations\nfor NLP, pages 116–121.\nEkta Sood, Simon Tannert, Diego Frassinelli, Andreas\nBulling, and Ngoc Thang Vu. 2020a. Interpreting\nattention models with human visual attention in ma-\nchine reading comprehension. In Proceedings of\nthe 24th Conference on Computational Natural Lan-\nguage Learning, pages 12–25.\nEkta Sood, Simon Tannert, Philipp Mueller, and An-\ndreas Bulling. 2020b. Improving natural language\nprocessing tasks with human gaze-guided neural at-\ntention. In Advances in Neural Information Process-\ning Systems, volume 33, pages 6327–6341.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Interna-\ntional conference on machine learning, pages 3319–\n3328. PMLR.\nTakenobu Tokunaga, Hitoshi Nishikawa, and Tomoya\nIwakura. 2017.\nAn eye-tracking study of named\nentity annotation. In Proceedings of the International\nConference Recent Advances in Natural Language\nProcessing, RANLP 2017, pages 758–764.\nMatthew J Traxler and Lyn Frazier. 2008. The role of\npragmatic principles in resolving attachment ambi-\nguities: Evidence from eye movements. Memory &\ncognition, 36(2):314–328.\nEthan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, Peng\nQian, and Roger Levy. 2020. On the predictive power\nof neural language models for human real-time com-\nprehension behavior. In Proceedings of the 42nd\nAnnual Meeting of the Cognitive Science Society.\nAlain F Zuur, Elena N Ieno, and Chris S Elphick. 2010.\nA protocol for data exploration to avoid common sta-\ntistical problems. Methods in ecology and evolution,\n1(1):3–14.\nA\nAppendix\nA.1\nExperimental Materials\nThe following materials were presented to partici-\npants during the experiment. Informed consent was\nobtained from each participant before the experi-\nment began. Instructions were displayed as shown\nin Figure 11.\nThe practice items, which participants completed\nafter reading the instructions and before beginning\nthe experiment, were as follows:\nText: What does this have to do with programming\n?\nAre you trying to solve this problem\nwith a program?\nQuestion: None\nText: this is source code... what is the\nquestion? Do you really think that throwing\ncode at us will solve your problem?!\nQuestion: Do you agree or disagree with the\nfollowing statement: The writer of the post\nseems upset.\nSee also Figure 11 for screenshots of the dis-\nplay shown to participants at various points in the\nexperiment.\nA.2\nMixed Effect Modeling\nWe fit linear mixed effect models to predict our\neye tracking measures, using the R packages lme4\nand lmetest. Our fixed effects are the number of\ncharacters in the interest area, the HAL frequency\nof the interest area, whether the previous interest\narea was viewed, and whether the interest area is\nin the congruent or incongruent condition. Our\nrandom effect is the participant ID. All variables\nare normalized prior to analysis.\nmodel = lmer(EYE_TRACKING_MEASURE ~ 1 +\ncongruent + previous_viewed+ LENGTH +\nHAL_FREQ + (1 | RECORDING_SESSION_LABEL))\nThe Dwell Time and Pupil Size eye tracking\nmeasure showed significance for the the fixed con-\ngruency effect. The other eye tracking measures\n– First Run Dwell Time, First Fixation Duration,\nReread Time, and Go Past Time – result in a singu-\nlar fit, likely because they are considerably more\nsparse (i.e., many interest areas have a null values\nfor these metrics).\nt value\nPr(> |t|)\nSig.\nVIF\n(Intercept)\n-19.114\n< 0.001\n∗∗∗\nfrequency\n-18.238\n< 0.001\n∗∗∗\n2.53\nlength\n31.858\n< 0.001\n∗∗∗\n2.53\ncongruency\n2.449\n< 0.05\n∗\n1.00\nprevious IA\n26.662\n<0.001\n∗\n1.00\nTable 3: Fixed Effects: predicting dwell time\nt value\nPr(> |t|)\nSig.\nVIF\n(Intercept)\n-4.098\n< 0.001\n∗∗∗\nfrequency\n1.865\n0.06\n.\n2.28\nlength\n3.056\n< 0.01\n∗∗\n2.27\ncongruency\n-8.382\n< 0.001\n∗∗∗\n1.00\nprevious IA\n9.915\n<0.001\n∗∗∗\n1.00\nTable 4: Fixed Effects: predicting pupil size\nWe tested variables for collinearity using the\nvariance inflation factor (VIF) (Zuur et al., 2010)\n(none exceeded the recommended threshold of 3).\nA.3\nAdditional Saliency Comparisons\nA.3.1\nSaliency Scores\nFigure 12 shows the Pearson’s r value for saliency\nscore over interest areas derived from each method.\nWe also include more example items from the\ndataset with associated saliency scores in Fig-\nure 5b.\nA.4\nFew-Shot Learning Experiment Details\nand Results\nThe full few-shot learning results can be found\nin Table 5. The experiment was conducted with\nthe OpenAI API5 completion endpoint and the fol-\nlowing parameters: the text-davinci-002 model, a\ntemperature of 0, and a top_p of 1.\nWe generated in-context learning prompts over\nour dataset by including important words as fol-\nlows:\nDecide whether the following text is Polite or\nImpolite.\nText: Thank you for your kind comment. Do you have a\nsuggestion where the portals should be placed?\nImportant words: thank you, suggestion\nPolite or Impolite:\n5https://openai.com/api, accessed in accordance with Ope-\nnAI’s terms of use\n(a) Experiment instructions screen.\n(b) One of the “context” screens shown at the be-\nginning of each block. This information makes\nparticipants aware of what type of text to expect in\nthe following screens.\n(c) One of the screens displaying an item from the\ndataset.\n(d) One of the comprehension question screens.\nFigure 11: Screenshots from the experiment program.\nannotations\nmodel grads\nsurprisal\ndwell time\nreread time\ngo past time\nfirst run dwell\nfirst fixation duration\npupil size\nannotations\nmodel grads\nsurprisal\ndwell time\nreread time\ngo past time\nfirst run dwell\nfirst fixation duration\npupil size\n1\n0.17\n1\n0.058 0.038\n1\n-0.0055-0.013 0.064\n1\n-0.019 -0.042\n0.04\n0.19\n1\n-0.027 -0.034 0.041\n0.23\n0.98\n1\n-0.056 0.015 0.038\n0.22\n-0.012\n0.14\n1\n-0.037 -0.035 0.065 0.076\n0.01\n0.046\n0.28\n1\n-0.019 -0.053 -0.066 -0.072 -0.021 -0.023 -0.031 -0.013\n1\nFigure 12: Correlations (Pearson’s r) between the saliency scores derived from each method.\nMetric for Saliency\nData aggregation (eye-tracking only)\nExperimental Conditions\n0-shot\n1-shot\n2-shot\n4-shot\nBaseline\nNA\nNA\n95.18\n93.98 (2.46)\n90.36 (0.96)\n95.18 (0.96)\nHuman Annotations\nNA\nNA\n93.98\n91.57 (2.89)\n90.36 (3.27)\n93.98 (1.80)\nIntegrated Gradients\nNA\nNA\n93.98\n93.98 (1.93)\n92.77 (2.46)\n96.39 (0.96)\nGPT2 Surprisal\nNA\nNA\n93.98\n92.77 (0.96)\n92.77 (0.96)\n97.59 (1.18)\nDwell Time\nz score\nAll\n93.98\n92.77 (1.80)\n93.98 (0.96)\n96.39 (2.89)\nDwell Time\nz score\nIncongruent - Congruent\n93.98\n93.98 (1.93)\n91.57 (1.93)\n95.18 (2.36)\nDwell Time\nLME\nAll\n93.98\n92.77 (0.96)\n92.77 (0.96)\n97.59 (1.18)\nDwell Time\nLME\nIncongruent - Congruent\n93.98\n92.77 (1.18)\n91.57 (1.93)\n95.18 (2.36)\nDwell Time\nraw\nAll\n93.98\n93.98 (1.18)\n95.18 (1.93)\n96.39 (1.80)\nDwell Time\nraw\nIncongruent - Congruent\n93.98\n92.77 (1.80)\n89.16 (2.89)\n95.18 (2.46)\nReread Time\nz score\nAll\n93.98\n92.77 (0.96)\n92.77 (0.96)\n97.59 (1.18)\nReread Time\nz score\nIncongruent - Congruent\n93.98\n93.98 (2.36)\n90.36 (2.36)\n97.59 (2.16)\nReread Time\nraw\nAll\n93.98\n92.77 (1.18)\n91.57 (2.46)\n93.98 (2.81)\nReread Time\nraw\nIncongruent - Congruent\n92.77\n92.77 (2.46)\n86.75 (2.81)\n96.39 (1.80)\nGo Past Time\nz score\nAll\n93.98\n92.77 (0.96)\n92.77 (0.96)\n97.59 (1.18)\nGo Past Time\nz score\nIncongruent - Congruent\n93.98\n91.57 (2.89)\n87.95 (3.86)\n92.77 (2.46)\nGo Past Time\nraw\nAll\n92.77\n92.77 (0.96)\n91.57 (2.46)\n96.39 (1.18)\nGo Past Time\nraw\nIncongruent - Congruent\n93.98\n92.77 (3.27)\n90.36 (3.20)\n93.98 (2.46)\nFirst Run Dwell Time\nz score\nAll\n93.98\n92.77 (0.96)\n92.77 (0.96)\n97.59 (1.18)\nFirst Run Dwell Time\nz score\nIncongruent - Congruent\n93.98\n92.77 (2.46)\n92.77 (1.18)\n92.77 (2.46)\nFirst Run Dwell Time\nraw\nAll\n93.98\n92.77 (1.18)\n92.77 (2.46)\n96.39 (2.36)\nFirst Run Dwell Time\nraw\nIncongruent - Congruent\n93.98\n92.77 (1.80)\n89.16 (3.54)\n93.98 (2.46)\nFirst Run Dwell Time\nLME\nAll\n93.98\n92.77 (1.93)\n92.77 (2.36)\n95.18 (2.46)\nFirst Run Dwell Time\nLME\nIncongruent - Congruent\n93.98\n92.77 (2.46)\n89.16 (3.54)\n93.98 (2.46)\nFirst Fixation Duration\nz score\nAll\n93.98\n92.77 (0.96)\n92.77 (0.96)\n97.59 (1.18)\nFirst Fixation Duration\nz score\nIncongruent - Congruent\n93.98\n92.77 (2.46)\n90.36 (0.96)\n95.18 (2.46)\nFirst Fixation Duration\nraw\nAll\n93.98\n93.98 (2.64)\n89.16 (1.80)\n96.39 (0.96)\nFirst Fixation Duration\nraw\nIncongruent - Congruent\n93.98\n92.77 (2.81)\n90.36 (3.27)\n95.18 (1.93)\nPupil Size\nz score\nAll\n93.98\n92.77 (0.96)\n92.77 (0.96)\n97.59 (1.18)\nPupil Size\nz score\nIncongruent - Congruent\n92.77\n93.98 (2.46)\n86.75 (4.31)\n93.98 (1.80)\nPupil Size\nraw\nAll\n93.98\n92.77 (1.18)\n92.77 (1.80)\n95.18 (2.81)\nPupil Size\nraw\nIncongruent - Congruent\n93.98\n91.57 (1.93)\n86.75 (4.03)\n96.39 (2.46)\nPupil Size\nLME\nAll\n93.98\n91.57 (2.46)\n95.18 (2.36)\n92.77 (1.18)\nPupil Size\nLME\nIncongruent - Congruent\n93.98\n92.77 (2.16)\n86.75 (4.03)\n96.39 (2.46)\nHybrid (Human + Dwell Time)\nz score\nAll\n95.18\n93.98 (1.18)\n93.98 (2.46)\n96.39 (1.52)\nHybrid (Human + Dwell Time)\nz score\nIncongruent - Congruent\n93.98\n92.77 (4.15)\n93.98 (3.05)\n96.39 (1.18)\nTable 5: Accuracy results on few-shot learning experiments over dataset. For 1-, 2-, and 4-shot learning, five different randomly\nselected prompts were chosen and the average accuracy is reported (the 95% confidence interval is reported in parentheses after\nthe accuracy score).\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2022-12-19",
  "updated": "2023-10-22"
}