{
  "id": "http://arxiv.org/abs/2212.10124v1",
  "title": "Image Segmentation-based Unsupervised Multiple Objects Discovery",
  "authors": [
    "Sandra Kara",
    "Hejer Ammar",
    "Florian Chabot",
    "Quoc-Cuong Pham"
  ],
  "abstract": "Unsupervised object discovery aims to localize objects in images, while\nremoving the dependence on annotations required by most deep learning-based\nmethods. To address this problem, we propose a fully unsupervised, bottom-up\napproach, for multiple objects discovery. The proposed approach is a two-stage\nframework. First, instances of object parts are segmented by using the\nintra-image similarity between self-supervised local features. The second step\nmerges and filters the object parts to form complete object instances. The\nlatter is performed by two CNN models that capture semantic information on\nobjects from the entire dataset. We demonstrate that the pseudo-labels\ngenerated by our method provide a better precision-recall trade-off than\nexisting single and multiple objects discovery methods. In particular, we\nprovide state-of-the-art results for both unsupervised class-agnostic object\ndetection and unsupervised image segmentation.",
  "text": "Image Segmentation-based Unsupervised Multiple Objects Discovery\nSandra Kara\nHejer Ammar\nFlorian Chabot\nQuoc-Cuong Pham\nUniversit´e Paris-Saclay, CEA, List, F-91120, Palaiseau, France\n{firstname.lastname}@cea.fr\nAbstract\nUnsupervised object discovery aims to localize objects\nin images, while removing the dependence on annotations\nrequired by most deep learning-based methods. To address\nthis problem, we propose a fully unsupervised, bottom-up\napproach, for multiple objects discovery. The proposed ap-\nproach is a two-stage framework. First, instances of ob-\nject parts are segmented by using the intra-image similar-\nity between self-supervised local features. The second step\nmerges and ﬁlters the object parts to form complete ob-\nject instances. The latter is performed by two CNN models\nthat capture semantic information on objects from the entire\ndataset. We demonstrate that the pseudo-labels generated\nby our method provide a better precision-recall trade-off\nthan existing single and multiple objects discovery meth-\nods. In particular, we provide state-of-the-art results for\nboth unsupervised class-agnostic object detection and un-\nsupervised image segmentation.\n1. Introduction\nDeep learning methods have shown tremendous ad-\nvances in resolving several computer vision tasks such as\nobject detection and image segmentation. However, mas-\nsive amounts of carefully labeled images are necessary to\ntrain reliable deep learning models that can reach high per-\nformances. Due to the high cost of such manual annota-\ntions, several approaches were proposed to use only limited\namounts of annotated data, such as semi-supervised learn-\ning, weakly supervised learning or few-shot learning. In\nthis work, we address the problem of localizing objects in\nimages without any supervision, called Unsupervised Ob-\nject Discovery (UOD).\nUOD can be useful for other vision tasks related to ob-\nject localization. Pseudo-labels generated without supervi-\nsion have been shown to provide reliable object priors for\nimage instance retrieval in [34]. For object detection, they\ncan be used either to initialize an object detector without\nadditional annotation [24], or in a semi-supervised setting,\nwhen combined with few labeled data [32]. Providing ro-\nbust pseudo-labels with limited noise is key to the success\nof these tasks. However, this remains a major challenge,\nespecially in a completely unsupervised context, where no\nprior knowledge is provided about the semantics and local-\nization of objects present in an image.\nMany approaches solve this problem by leveraging inter-\nimage similarities [28, 29] between pairs of object propos-\nals in different images. Without carefully designed opti-\nmization mechanisms, these methods come with a compu-\ntational cost and complexity that compromise their scalabil-\nity. Moreover, these methods have shown to be dependent\non supervised CNN features for the calculation of similari-\nties.\nRecently, vision transformers (ViT) have achieved ex-\ncellent performances, outperforming CNN architectures in\nboth supervised tasks [8, 7, 1] and self-supervised learn-\ning [2, 3].\nParticularly, strong objects localization hints\nemerge from training ViT models using a self-distillation\nscheme, in DINO [2]. These self-supervised features were\nso far explored only for solving single object discovery task\n[24, 31]. TokenCut [31] demonstrated the effectiveness of\nspectral-clustering applied on self-supervised ViT features,\nfor saliency detection, and signiﬁcantly improved the state-\nof-the-art for single object discovery.\nIn this work, we propose a new approach to address mul-\ntiple objects discovery without any supervision. We explore\nself-supervised Vision Transformer (SS-ViT) features to lo-\ncalize and segment multiple object instances in the image.\nDiscovering multiple objects in each image is not straight-\nforward as it requires a clear deﬁnition of what an object\nis. In fact, objects are either deﬁned as the annotated re-\ngions in the supervised setting, or as the salient region in\neach image, in unsupervised single object discovery ap-\nproaches. To address the localization of multiple objects\nin a fully unsupervised way, we propose to recognise ob-\nject regions using a semantic information captured at the\ndataset level. In other terms, an object is deﬁned as belong-\ning to one of the discovered semantic categories, in the im-\narXiv:2212.10124v1  [cs.CV]  20 Dec 2022\nage collection. Concretely, the semantic categories present\nin the dataset are discovered in an unsupervised way. This\ninformation is encoded using classiﬁcation models. Object\ndiscovery is then designed as the activation of object parts\nin each image using SS-ViT features, and the merging of\nthese object parts using the self-supervised classiﬁers, to\ndiscover complete object instances. The effectiveness of\nthe proposed framework is demonstrated through extensive\nexperiments on the object detection benchmarks PASCAL\nVOC [9] and MSCOCO [17]. Since by design, our method\nprovides pixel-wise mask proposals, we also show that the\nsame framework solves the unsupervised image segmenta-\ntion task.\nOur contributions can be formulated as follows :\n• We propose a fully unsupervised, bottom-up approach,\nfor multiple objects discovery. We ﬁrst discover ob-\nject parts using intra-image similarities. Object parts\nare merged using a dataset-driven information, to form\ncomplete object instances. Both stages exploit self-\nsupervised ViT features to produce instance masks. To\nthe best of our knowledge, this is the ﬁrst work that\nbuilds on SS-ViT features to solve the multi-object dis-\ncovery task.\n• We generalize the saliency-based approach in Token-\nCut [31] for the discovery of local ﬁne semantic con-\ncepts (object parts) of multiple objects in an image.\n• We propose a novel semantic object proposal method\nfor the self-supervised learning of a region classiﬁer.\nThis visual model encodes the dataset-level semantic\ninformation.\n• We improve the state-of-the-art in unsupervised multi-\nple objects discovery, unsupervised class-agnostic ob-\nject detection and unsupervised image segmentation,\non challenging object detection benchmarks.\n2. Related work\n2.1. Unsupervised object discovery/co-localization\nWe can distinguish, from previous works, two distinct\ntasks: object discovery and object co-localization. The for-\nmer consists in localizing objects in an image without any\nprior knowledge of the image content. This is the real object\ndiscovery task, which is much more challenging than ob-\nject co-localization [29]. On the other hand, co-localization\naims at localizing common objects between images, that\nshare the same semantic content. Algorithms in this setting\nare fed with perfect image clusters derived from the ground-\ntruth. It is therefore a weakly supervised version of object\ndiscovery.\nDDT [32] addresses the co-localization task and is the\nﬁrst work that demonstrated the reusability of supervised\nCNN features for object co-localization. In DDT, objects\nare selected from regions of high correlation within a given\ncluster (semantic category).\nOther methods address both tasks, and many of them\nleverage inter-image similarities between off-the-shelf re-\ngion proposals. Cho et al. [4] formulated the problem as a\nstructure and objects discovery, by iterating part-matching\nand object localization. Similarly, OSD [28] simultaneously\nlocalizes objects and discovers structures of the image col-\nlection. It formalizes the task as an optimization problem.\nAlthough OSD brought a large improvement, it has shown\nto be highly dependent on supervised proposals provided by\n[18]. The method also suffers from overlapping region pro-\nposals, which prevents it from proposing multiple objects\nper image. These limitations were addressed in rOSD [29]\nby providing unsupervised proposals corresponding to re-\ngions of high activations around local maxima, within CNN\nfeature maps. rOSD also constrains the number of proposals\nper local maximum, and performs non-maximum suppres-\nsion (NMS) [21] post-processing, to address the problem\nof overlapping proposals. Note that these methods, while\nunsupervised, are built on supervised CNN features, from\nthe ImageNet [6] classiﬁcation task. LOD [30] formalized\nthe task as a ranking problem, and focused on ensuring the\nscalability of the proposed approach. It also demonstrated\nthe utility of self-supervised CNN features for single and\nmultiple objects discovery.\nOther methods [24, 31] tackled the single object discov-\nery problem, and showed the potential of self-supervised\nfeatures, especially from ViT models, for saliency detec-\ntion. LOST [24] proposed a seed expansion heuristic based\non inter-patch correlation. TokenCut [31] investigated the\nuse of spectral-clustering on self-supervised ViT features,\nwhich are projected into a new space that allows for a more\naccurate binary clustering [23].\nIn previous multi-object discovery methods, relying on\ninter-image similarities in the computation of an objectness\nscore could favor the discovery of the most frequent ob-\njects. In our approach, even though we also use a dataset-\ndriven information, we overcome this issue by training vi-\nsual classiﬁcation models to encode the information of se-\nmantic classes. This results in a better separation between\nobject and non-object regions, and a better detection of\nunder-represented classes.\n2.2. Unsupervised image segmentation\nImage segmentation is the task of grouping all pixels of\nan image into meaningful regions, where pixels sharing the\nsame characteristics are assigned to the same region [16].\nDue to the very high cost of such a dense annotation, weakly\nsupervised and fully unsupervised methods were explored.\nIn the weakly supervised setting, [33] takes as input the\nimage-level labels of the class categories present in the im-\nage, and utilizes a vision-language embedding model to cre-\nate a rough segmentation map for each class.\nOther approaches do not use any kind of supervision. On\none hand, we ﬁnd classic methods such as k-means [11],\nthat focuses on pixels clustering based on color and texture\nfeatures, and assigns each pixel to the cluster with the near-\nest mean. Moreover, graph-based segmentation (GS) [10]\ngenerates image segments, while ensuring that these seg-\nments are not being too coarse or too detailed, based on re-\ngions comparison. More recently, methods based on unsu-\npervised learning for image segmentation have been intro-\nduced. For example, IIC [13] learns to maximize the mu-\ntual information between an image and its augmentations\non a patch-level cluster. Kim et al. [14, 16] trains a CNN\nby iterating features clustering and network parameters tun-\ning. The method is based on three criteria to maximize the\nfeatures similarity between spatially continuous pixels and\npixels assigned to the same cluster, while imposing a large\nnumber of clusters. The authors proposed two solutions for\nlabel assignment without any supervision (i) by superpixel\nextraction using simple linear iterative clustering in [14] and\n(ii) by the use of a spacial continuity loss in [16] to address\nthe limitation of ﬁxed segment boundaries.\nThese methods discover multiple objects by proposing\ndense object candidates. Several other methods address the\nsemantic segmentation task in an unsupervised way, with-\nout proposing dense object discovery. We do not consider\nthese approaches in our study as we solve a different task.\n2.3. Self-supervised vision transformers\nThe self-supervised setting aims at learning useful rep-\nresentations with no real label. It was ﬁrst used for pre-\ntraining CNN models [12, 19, 27], and showed a strong gen-\neralization ability to downstream tasks. More recently, the\nself-attention based encoding of images using transformers\nfor vision [7] was proved to be effective for a large spectrum\nof supervised vision tasks such as classiﬁcation [7], seman-\ntic segmentation [25] and dense prediction tasks [20]. ViT\nhas also become a reference architectural choice of neural\nnets for visual representation learning. In addition to the\nclassic masked auto-encoding paradigm inspired from NLP,\nMoCo-v3 [3] among others, demonstrated a strong potential\nof training ViT with a contrastive approach.\nRecently, a self-distillation scheme was used in DINO\n[2] to train ViT with no labels. The choices made during\ntraining result in effective semantic separation and local-\nglobal alignment of the learned features. In particular, the\nresulting attention maps strongly activate the object regions,\nwhich provide clues to the localization of objects in the im-\nage. SS-ViT features have been explored recently to per-\nform saliency detection and single object discovery tasks\n[24, 31].\nTo the best of our knowledge, our method is the ﬁrst to\nexploit self-supervised ViT features in a fully unsupervised\nmultiple object discovery pipeline. The method outputs ob-\nject instance masks resolving also the unsupervised image\nsegmentation task.\nSuch results can be used as pseudo-\nlabels to initialize the training of a class-agnostic object de-\ntector, without any supervision.\n3. Method\n3.1. Overview\nRecently, SS-ViT features showed to generalize well to\nsaliency-based tasks [24, 31].\nIn this work, we aim to\ndemonstrate the potential of using those features for mul-\ntiple objects discovery, without any supervision. We adopt\na bottom-up approach, illustrated in ﬁgure 1, starting with\nan intra-image analysis, for the discovery of object parts.\nAt the dataset level, two CNN models are trained in a self-\nsupervised manner, using carefully selected, and semantic\nobject proposals. These models are used to merge and ﬁlter\nobject parts, to form complete object instances.\nThe intra-image analysis can be seen as a generaliza-\ntion of TokenCut [31] to the multiple object discovery task.\nSimilar to TokenCut, we perform spectral clustering using\nSS-ViT features, to decompose the image into eigen vec-\ntors with useful information. Different from TokenCut: (i)\nSince we focus on the localization of multiple objects, we\nlook for more localization clues than just saliency. Thus\nwe use multiple eigen vectors, as the feature space to ap-\nply local clustering, instead of only using the second eigen\nvector. (ii) The number of local clusters is no longer known\nas we try to solve the multi-object discovery task (2 clus-\nters in saliency detection task). To manage this, we propose\nan algorithm for choosing an optimal number of clusters,\nwithout any knowledge about the number of objects, or se-\nmantic concepts, in each image. The algorithm is detailed\nin section 3.2 and aims at discovering multiple object parts,\nwhile limiting over-segmentation.\nThe goal of the dataset-level analysis is to build two clas-\nsiﬁers that capture the dominant semantic classes in the im-\nage collection. One classiﬁer is used for merging object\nparts, resulting from the local segmentation, and associates\na conﬁdence score to each discovered object. The second\nclassiﬁer separates foreground/background classes and is\nused to ﬁlter remaining noise after the merging phase. We\nperform image clustering to get pseudo-labels for training\nboth models.\nSince images may contain several seman-\ntic concepts, instead of using the whole images, we ap-\nply clustering on selected object proposals from Selective\nSearch [26]. For proposals selection, we build an object-\nness score detailed in section 3.3. The retained top propos-\nals are grouped into clusters, which are used for training the\nclassiﬁers.\nFinally, the classiﬁers are used in cascade to merge and\ndenoise the discovered object parts. Both stages use self-\nsupervised ViT features trained using DINO [2]. We show\nin section 3.3 how these features are particularly relevant\nto our approach because of some properties like semantic\nseparation, local-global alignment, and object regions acti-\nvation.\n3.2. Discovery of intra-image semantic concepts\nIn this step, we extend TokenCut [31] to discover mul-\ntiple objects in each image, instead of solving saliency de-\ntection. TokenCut constructs a weighted graph where nodes\nare ViT embeddings of image patches and edges correspond\nto the cosine similarity between tokens. Single object dis-\ncovery is then formalized as a normalized graph-cut (Ncut)\nproblem, which is solved using spectral clustering: features\nare projected into a new space via eigen decomposition. In\nthis space, the second smallest eigen vector provides a so-\nlution to the Ncut problem for binary clustering, as demon-\nstrated by Shi and Malik [23]. Likewise, we create a sim-\nilarity graph based on SS-ViT features. The image is then\ndecomposed into eigen vectors with useful information. We\nconsider N eigen vectors (N ≥2) for local clustering, since\nwe aim to capture multiple objects in the image. The choice\nof N is studied in section 4.6. The selected N eigen vectors\nrepresent the feature space where local clustering of image\npixels is performed: each pixel is represented with a new\nfeature vector f ′\ni of size N, where i varies between 1 and\nthe total number of pixels per image np.\nSince in a fully unsupervised setting the number of se-\nmantic concepts in each image is unknown, we determine\nan optimal number of clusters K using an iterative process\nas detailed in algorithm 1. We apply k-means clustering to\nthe image pixels, in the new feature space of eigen vectors\nF = {f ′\ni; 1 ≤i ≤np}. This partitions the image into K\ngroups, which we denote CK. We consider the background\ncluster as the one occupying the biggest area in the image.\nThe background id is denoted as b id. All the remaining\nclusters represent the objects area. K is incremented, start-\ning from K = 2, until no signiﬁcant object area is newly\nactivated. The goal is to activate multiple object regions in\nthe image, while limiting over-segmentation. Examples of\nthe results of this step are provided in ﬁgure 3, ﬁrst column.\nIn particular, we see in the last row that, in some cases, the\nalgorithm directly outputs an optimal segmentation of the\nimage. This shows its effectiveness compared to a simple\nover-segmentation, where a predeﬁned number of clusters\nis used, without adapting to the content of each image.\n3.3. Dataset-level semantic object proposals\nAs stated above, we use Selective Search [26] (SeSe)\nregion proposals as object priors to discover the semantic\nclasses in the dataset, through proposals clustering. These\nproposals provide a fairly high recall. However, their rank-\nAlgorithm 1 Iterative clustering for intra-image discovery\nof semantic concepts\n1: Initialize:\nK ←2\nCK ←Kmeans(F, K)\nb id ←arg max\nk\n{area(CK(k)), 1 ≤k ≤\nK}\nobj area ←PK\nk=1,k̸=b id area(CK(k))\nadd semantic concepts ←True\n2: while add semantic concepts do\n3:\nK ←K + 1\n4:\nCK ←Kmeans(F, K)\n5:\nnew obj area ←PK\nk=1,k̸=b id area(CK(k))\n6:\nif new obj area\nobj area\n> thresh then\n7:\nobj area ←new obj area\n8:\nelse add semantic concepts ←False\n9: return K\ning is rather naive: given an over-segmentation of the im-\nage, the regions merged ﬁrst, based on color and texture\nsimilarities, are ranked ﬁrst. This makes even the top pro-\nposals subject to a lot of noise. We thus propose a new rank-\ning of SeSe proposals, to select the most relevant ones. To\ndo this, we build an objectness score, based on assumptions\nabout object-like regions.\nNote that the objectness score is computed within each\nimage, independently from all other images in the dataset.\nConcretely, we use two main measures in this computation:\nintersection over union (IoU) and cosine similarity between\nobject proposals in the same image. Given M proposals (p1,\np2, ..., pM) in an image, we deﬁne uij as the overlap rate\nand sij as the similarity between pi and pj. For the latter,\nwe use the cosine similarity between the CLS tokens from\nthe last layer of a ViT trained using DINO. Let fi and fj be\nthe feature vectors (CLS token) that result from passing pi\nand pj respectively to the SS-ViT. The cosine similarity sij\nis deﬁned as:\nsij =\nfi.fj\n||fi||||fj||\n(1)\nThe new objectness score for object proposals re-ranking is\nthe weighted sum of three normalized terms:\nscore(pi) = α\n2 (SimL(pi) + DissimG(pi)) + (1 −α)H(pi) (2)\nEach term of this score is based on a different assumption:\nObject-like regions have high local similarity. We deﬁne\nlocal similarity for a given proposal pi as its average simi-\nlarity to its neighbouring proposals, i.e. proposals having an\nIoU with pi above a threshold t. We notice that these pro-\nposals correspond usually to parts of the same objects. We\nFigure 1. Pipeline of the method. Top left: Intra-image analysis for the discovery of local semantic concepts. Bottom: Dataset-level\nanalysis for the selection of semantic object proposals to train self-supervised classiﬁers. Top right: Using the data-driven classiﬁers on\neach image for parts merging and denoising.\nalso recall that we are using SS-ViT features learned using\nDINO, with a global local alignment objective. This means\nthat an object is close to its parts in the DINO features space.\nFrom this we deduce that a high similarity between pi and\nits neighbours increases its chance of containing an object.\nThus, we make all the neighbours of pi vote positively for\nit, in the following local similarity term:\nSimL(pi) =\nM\nX\nj=1\nsij, j ̸= i, uij ≥t\n(3)\nObject-like regions have high global dissimilarity. We\nnow consider the global similarity, i.e. the average simi-\nlarity between pi and all other proposals that do not over-\nlap with pi. Given the foreground/background imbalance in\nreal-world images, most object proposals in an image oc-\ncupy the background, and have similar visual content (e.g.\n’sky’). Objects, on the contrary, occupy regions that are dis-\ntinct in the image. If a proposal pi contains an object, then\nit has low overall similarity, as all object proposals in the\nbackground vote negatively for it. Thus, pi will be highly\ndissimilar, hence the following global dissimilarity term:\nDissimG(pi) =\nM\nX\nj=1\n(1 −sij), j ̸= i, uij < t\n(4)\nObject-like regions have high entropy. The Shannon en-\ntropy of a discrete random variable is deﬁned as:\nH(p) = −\nX\nx\nPxlog(Px)\n(5)\nThis measure is used to quantify the randomness of a vari-\nable [5]. In image processing, Px refers to the distribution\nof gray levels x in image p (or colors intensities in RGB\nimages). The previous formula associates higher entropy to\nimages with more details and colors variation. Inversely,\nhomogeneous regions are characterized by a low entropy.\nWe thus associate low-entropy proposals to background, by\nadding an entropy term in the ﬁnal objectness score.\nIt can be seen from ﬁgure 2 that the proposed ranking\nimproves the detection rate for a ﬁxed number of proposals,\ncompared to two modes of SeSe. This is especially true\nwhen a small number of proposals are selected. We also\ncompare qualitatively the retained top-1 proposals with the\ntwo rankings.\nWe recall that the aim of this new ranking is to reduce the\namount of noise in the top proposals, which will be retained\nfor clustering, as explained in section 3.4. We choose to use\nSeSe proposals for its popularity. However, the proposed\nranking should be valid with other proposals, provided that\nthey have a similar distribution, i.e. bounding boxes that\noccupy the whole image, and thus verify the background\ndominance condition, discussed above.\n3.4. Dataset-driven self-supervised region classiﬁ-\ncation\nAfter re-ranking the SeSe object proposals, the goal\nis to transform the top P object priors in each image into\npseudo-labels to train multi-class classiﬁers. These will be\nused to merge and reﬁne the discovered local semantic con-\ncepts. We use a value of P large enough to make sure that\n \n \nFigure 2. Results of ranking object proposals. Left: Comparison of the detection rate given the number of retained top proposals from\ntwo modes of SeSe, with our ranking. Right: Examples of the top1 proposal using SeSe score (top) and our score (bottom).\nwe do not only select objects but also object parts. This is\nimportant since the classiﬁer must learn to assign the same\nsemantic class to an object and its parts, for an accurate\nmerging. We use k-means clustering [11] on the SS-ViT\nfeatures of all the selected object proposals. The optimal\nnumber of clusters is chosen by ﬁnding the best silhouette\nscore [22], which minimizes the mean intra-cluster distance\nand maximizes the mean nearest-cluster distance.\nWith\nthe semantic information contained in SS-ViT features,\nsimilar semantic concepts are grouped together. Moreover,\neach cluster contains proposals of both objects and their\nparts. This is especially due to the multi-crop augmentation\ntechnique used in DINO. The obtained clusters capture\nthe global semantic information of the dataset. Note that\nthe number of the clusters is not necessarily equal to the\nnumber of classes annotated in the dataset. However, we\ncan still localize instances of undiscovered categories, such\nas ‘bottle’ and ‘plant’.\nSince some of the selected proposals may still belong to\nbackground (Bg) regions, some of the discovered pseudo-\nclasses are Bg clusters, that we aim to identify. According\nto our ranking score detailed in 3.3, the proposals having\nthe lowest scores are the ones representing most probably\nBg regions. Each of these proposals is passed to a SS-ViT\nto extract its features. The average vector of these features\nis considered as a pattern of Bg regions.\nThe clusters\nwhose center has a distance below a threshold tbg with the\nBg pattern, are considered Bg clusters.\nAfter Foreground (Fg) and Bg groups identiﬁcation, we\nassociate to the clusters two types of labels (i) Each Fg clus-\nter is assigned an id representing one discovered semantic\nclass. (ii) All clusters have a binary label indicating whether\nit belongs to Fg or Bg. These image clusters are used to train\ntwo CNN-based classiﬁers, with the cluster id as a classiﬁ-\ncation target. The ﬁrst is a multi-class classiﬁer trained us-\ning Fg clusters to assign objects and object parts to a speciﬁc\nclass. The second classiﬁer is trained using all the discov-\nered clusters, and learns to distinguish between objects and\nBg regions.\n3.5. Instance segmentation using dataset-level in-\nformation\nIn this ﬁnal step, the obtained classiﬁers are used to\nmerge and reﬁne the object parts identiﬁed in the intra-\nimage analysis. The multi-class classiﬁer is ﬁrst used on\neach segmented region: Image crops enclosing each ob-\nject part segment are passed to the CNN-based classiﬁer.\nNearby regions assigned to the same category are merged\nto form complete object instances. The image crop around\neach merged region segment is then passed to the Fg/Bg\nclassiﬁer to eliminate segments classiﬁed as Bg. This binary\nclassiﬁcation is performed second to avoid incorrect classi-\nﬁcation of small object parts as Bg, if used before merging.\nThe multi-class classiﬁer also assigns to each object a con-\nﬁdence score, which is necessary for the evaluation metrics\n(AP@50, odAP). We provide in ﬁgure 3 illustrations for\neach step of the proposed framework.\nFigure 3. Example of results. By column: results of the discovery\nof local concepts, segmentation result after parts merging, ﬁnal\ninstance mask segmentation, ﬁnal bounding boxes.\n4. Experiments\n4.1. Implementation details\nFollowing previous works [29, 30, 24] we conduct our\nexperiments on three detection and localization bench-\nmarks:\nVOC2007 trainval, VOC2012 trainval [9] and\nCOCO20k which is composed of 19817 images randomly\nchosen from COCO2014 trainval dataset [24]. We spec-\nify in the following the implementation details and hyper-\nparameters for each addressed task.\nUnsupervised multiple objects discovery. In the intra-\nimage analysis, local clustering is applied on SS-ViT fea-\ntures learned using the DINO training scheme. Based on\nthe conclusions from previous works [31, 24], we use the\nvariant ViT-S with a patch size of 16. Eigen decomposi-\ntion is performed using the keys features of the last layer.\nTo ﬁnd the optimal number of local clusters, we set as con-\nvergence criterion a fraction of newly activated area of 2%,\n(thresh = 1.02 in algorithm 1). The number of eigen vec-\ntors used for clustering is studied in section 4.6, and showed\nto be invariant to the dataset: 3 eigen vectors for PASCAL\nVOC and COCO20k. For object proposals re-ranking, the\nthree terms are found to have an equivalent impact on the\nﬁnal re-ranking, with α = 0.7, and t = 0.1. Proposals from\nSelective Search single mode are used in this work. For the\ndataset-level analysis, P = 20 top proposals in each im-\nage are selected to train the classiﬁers. A distance threshold\ntbg = 0.8 from the Bg is used to separate Fg and Bg clus-\nters. We use ResNet50 as the backbone of the two classi-\nﬁers, initialized with DINO pre-training.\nUnsupervised class-agnostic object detection. We follow\nthe same conﬁguration described in [24] for training a class-\nagnostic Faster-RCNN, with our pseudo-labels. We also use\nthe same batch-size and the number of training iterations,\nfor an objective comparison with previous works.\nUnsupervised image segmentation. Following [16], this\nexperiment is conducted on VOC2012 validation set, con-\nsisting of 1446 images. Masks resulting from multi-object\ndiscovery task are evaluated using mIOU, see section 4.2.\n4.2. Metrics and evaluation settings\nDifferent metrics are used to evaluate different tasks:\nUnsupervised multiple objects discovery. Most of multi-\nple objects discovery methods are based on ranking of ob-\nject proposals. This makes them able to produce a large\nnumber of object candidates. The question then arises as\nto how many proposals to keep for computing recall, pre-\ncision, or even the classical AP50 metric, since all of these\nwould be affected by the number of retained top proposals.\n[30] addressed this issue and proposed an new version of\nAP, adapted to the object discovery task, called odAP. odAP\nis presented as the area under the precision-recall curve,\nwhere each precision-recall point is computed for a num-\nber of retained proposals, starting from 1, to the maximum\nnumber of objects in any image in the dataset. Even though\nby design, our approach outputs a reduced number of pro-\nposals, we use odAP to compare with previous works. We\nreport the odAP50 where a detection is considered correct\nif its overlap rate with a ground truth bounding box is above\n50%. And odAP@[50 : 95], which is the average odAP for\n10 values of IoU, varying from 50% to 95%.\nClass-agnostic unsupervised object detection. A classical\nclass agnostic Average Precision (AP@50) is calculated.\nUnsupervised image segmentation. Following [16], we\nuse the mean intersection over union (mIoU) to evaluate un-\nsupervised image segmentation. mIOU is calculated as the\naverage IOU between each ground truth mask (along with\nthe background) and the detected mask that has the largest\nIOU with it, without considering any class label.\n4.3. Unsupervised multiple objects discovery\nWe follow previous works and evaluate our method using\nodAP 4.2. Note that this metric is particularly adapted to the\nmethods that propose a large number of object candidates,\nbased on a ranking of object proposals. Since our approach\nis built on image segmentation, a limited number of boxes\nare proposed: 3 per image on average in PASCAL VOC\ndataset [9]. Hence, our approach is disadvantaged by this\nmetric regarding the recall. Despite that, we show in table\n1 the superiority of our method on both odAP@50 and the\nmuch more demanding odAP[50-95] metric.\nThe higher odAP[50-95] demonstrates the accuracy of\nour returned pseudo-boxes: Since these are generated from\ninstance masks, they better enclose objects, and thus re-\nmain valid for a higher IoU threshold condition. Also, our\nmethod uses self-supervised features, which makes it fully\nunsupervised, unlike previous methods, which showed to be\ndependant on supervised features.\nMethod\nFeatures\nodAP@50\nodAP@[50-95]\nVOC07\nVOC12\nCOCO20k\nVOC07\nVOC12\nCOCO20k\nKim et al. [15, 24]\nSup\n9.5\n11.8\n3.93\n2.5\n3.1\n0.96\nDDT+ [32, 24]\nSup\n8.7\n11.1\n2.41\n3.0\n4.1\n0.73\nrOSD [29, 24]\nSup\n13.1\n15.4\n5.18\n4.3\n5.3\n1.62\nLOD [30, 24]\nSup\n13.9\n16.1\n6.63\n4.5\n5.3\n1.98\nOurs\nSelf\n15.4\n17.6\n5.44\n6.8\n8.1\n2.11\nTable 1. Multi-object discovery performance in odAP (Average\nPrecision for object discovery)\n4.4. Class-agnostic unsupervised object detection\nState-of-the-art multiple objects discovery methods\n(MOD) usually rely on a ranking of object proposals based\non inter-image similarities. These methods output a large\nnumber of object candidates and the question then arises\nas to how many bounding boxes to keep for the initializa-\ntion of an object detector. Inversely, single object discov-\nery methods (SOD) have a very limited recall. We argue\nthat our method provides a better precision/recall trade-off\nthan the previous methods in both settings. To prove this,\nwe train a class-agnostic object detector using our gener-\nated pseudo-labels. Results are presented in table 2. We\nnotice a clear improvement with our approach compared to\nMOD methods on all tested datasets. The gap however gets\nsmaller when comparing with the SOD methods on PAS-\nCAL VOC [9] dataset. This can be explained by the pres-\nence of a dataset bias in PASCAL VOC: A large proportion\nof images in this dataset contain one object, which gives a\nclear advantage to SOD methods. On the more challeng-\ning COCO20k dataset, our method exceeds both categories\n(MOD and SOD). This demonstrates the superiority of our\npseudo-labels, even for datasets with complex scenes.\nMethod\nVOC07\nVOC12\nCOCO20K\nSelective Search [26]\n3.6\n4.8\n1.8\nEdgeBoxes [35]\n2.9\n4.2\n1.6\nrOSD + CAD [29]\n24.2\n29.0\n8.4\nLOD + CAD [30]\n22.7\n28.4\n8.8\nLOST + CAD [24]\n29.0\n33.5\n9.9\nTokenCut + CAD [31]\n26.2\n35.0\n10.5\nOurs + CAD\n27.9\n36.2\n13.8\nTable 2. Class-agnostic unsupervised object detection in AP50%\n4.5. Unsupervised image segmentation\nWe further evaluate the performance of our method on\nVOC12 [9] validation set for unsupervised image segmen-\ntation task (see Table 3). Our method signiﬁcantly outper-\nforms previous state-of-the-art methods for discovering ob-\nject masks in a fully unsupervised way. More qualitative\nresults are provided in the supplementary material.\nMethod\nVOC12\nk-means clustering [11], k=2\n0.3166\nk-means clustering [11], k=17\n0.2383\nGraph-based segmentation (GS) [10], τ = 100\n0.2682\nGraph-based segmentation (GS) [10], τ = 500\n0.3647\nIIC [13], k=2\n0.2729\nIIC [13], k=20\n0.2005\nKim et al. with superpixels [14]\n0.3082\nKim et al. with continuity loss [16], ν = 5\n0.3520\nOurs\n0.4247\nTable 3. Unsupervised image segmentation results in mIOU\n4.6. Ablation study\nIn table 4, we provide an ablation study on different\nterms of the ranking score presented in 3.3. We evaluate\nthe recall@50 (recall at IoU=50%) for different numbers\nof the retained top proposals. We compare the results of\nthe overall ranking score, with the ranking obtained when\none of the terms is removed from the ﬁnal score. The best\nresults are achieved by considering all 3 terms, which sup-\nports the assumptions made in section 3.3. We also compare\nour score with the original SeSe ranking of proposals from\ntwo settings. Using this new ranking, we ensure that the top\nproposals are more reliable for the classiﬁers training.\nMethod\nRecall@50\nNumber of boxes\n1\n4\n10\n20\nSeSe Fast mode [26]\n7.5\n19.6\n30.3\n40.0\nSeSe Single mode [26]\n7.9\n19.9\n30.7\n40.9\nOurs: SimL + DissimG\n13.9\n23.5\n34.4\n44.1\nOurs: SimL + H\n12.8\n24.9\n35.9\n44.6\nOurs: Overall score\n15.7\n27.1\n36.9\n45.0\nTable 4. Ablation study on the impact of the different terms com-\nposing the re-ranking score, evaluated on VOC07 testset\nWe also provide a study of the number of eigen vectors to\nbe used in the intra-image analysis, in order to activate mul-\ntiple objects, while limiting the amount of noise. In table\n5, we evaluate the AP@50 of the generated pseudo-boxes,\nto choose the best precision/recal trade-off. We conduct the\nstudy on PASCAL VOC and COCO since they present dif-\nferent distributions of objects. Considering this study, the\nreported results are obtained with 3 eigen vectors in the\nintra-image analysis, for both datasets.\nNumber of eigen vectors\nVOC07\nCOCO20k\n2\n22.1\n5.9\n3\n22.5\n6.3\n4\n21.3\n6.0\n5\n20.3\n5.8\nTable 5. AP@50 as a function of the number of eigen vectors used\nfor local analysis\n5. Conclusion and future work\nWe presented a fully unsupervised approach for multiple\nobjects discovery. The aim of this work was to address some\nof the limitations observed in existing methods. Namely,\nlow recall in saliency detection-oriented methods, and the\nhigh amount of noise when several object candidates are\nproposed. We have shown that formulating the problem as\nan unsupervised segmentation is particularly suitable for re-\nducing the noise in the generated pseudo-boxes. This pro-\nvides a better precision-recall trade-off, which leads to a\nbetter initialization of an object detector. Still in this di-\nrection, we can further investigate the use of these pseudo-\nlabels as an initial seed in a pseudo-labelling approach.\nSimilarly, we can investigate the use of these object can-\ndidates with noise handling mechanisms.\n6. Acknowledgements\nThis work beneﬁted from the FactoryIA supercomputer\nﬁnancially supported by the Ile-deFrance Regional Council.\nReferences\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European confer-\nence on computer vision, pages 213–229. Springer, 2020.\n[2] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 9650–9660, 2021.\n[3] Xinlei Chen, Saining Xie, and Kaiming He.\nAn empiri-\ncal study of training self-supervised vision transformers. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 9640–9649, 2021.\n[4] Minsu Cho, Suha Kwak, Cordelia Schmid, and Jean Ponce.\nUnsupervised object discovery and localization in the wild:\nPart-based matching with bottom-up region proposals.\nIn\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 1201–1210, 2015.\n[5] Thomas M Cover and Joy A Thomas. Information theory\nand statistics. Elements of information theory, 1(1):279–335,\n1991.\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009.\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021.\n[8] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\nHerv´e J´egou.\nTraining vision transformers for image re-\ntrieval. CoRR, abs/2102.05644, 2021.\n[9] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\nA. Zisserman. The pascal visual object classes (voc) chal-\nlenge. International Journal of Computer Vision, 88(2):303–\n338, June 2010.\n[10] Pedro Felzenszwalb and Daniel Huttenlocher.\nEfﬁcient\ngraph-based image segmentation. International Journal of\nComputer Vision, 59:167–181, 09 2004.\n[11] J. A. Hartigan and M. A. Wong. A k-means clustering algo-\nrithm. JSTOR: Applied Statistics, 28(1):100–108, 1979.\n[12] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing\nthe dimensionality of data with neural networks. science,\n313(5786):504–507, 2006.\n[13] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant in-\nformation clustering for unsupervised image classiﬁcation\nand segmentation. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 9865–9874,\n2019.\n[14] Asako Kanezaki.\nUnsupervised image segmentation by\nbackpropagation. 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages\n1543–1547, 2018.\n[15] Gunhee Kim and Antonio Torralba. Unsupervised detection\nof regions of interest using iterative link analysis. Advances\nin neural information processing systems, 22, 2009.\n[16] Wonjik Kim, Asako Kanezaki, and Masayuki Tanaka. Unsu-\npervised learning of image segmentation based on differen-\ntiable feature clustering. IEEE Transactions on Image Pro-\ncessing, 29:8055–8068, 2020.\n[17] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision, pages 740–755.\nSpringer, 2014.\n[18] Santiago Manen, Matthieu Guillaumin, and Luc Van Gool.\nPrime object proposals with randomized prim’s algorithm.\nIn 2013 IEEE International Conference on Computer Vision,\npages 2536–2543, 2013.\n[19] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor\nDarrell, and Alexei A Efros.\nContext encoders: Feature\nlearning by inpainting.\nIn Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n2536–2544, 2016.\n[20] Jaonary Rabarisoa, Valentin Belissen, Florian Chabot, and\nQuoc-Cuong Pham. Self-Supervised Pre-training of Vision\nTransformers for Dense Prediction Tasks.\narXiv e-prints,\npage arXiv:2205.15173, May 2022.\n[21] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. Advances in neural information process-\ning systems, 28, 2015.\n[22] Peter Rousseeuw. Silhouettes: a graphical aid to the inter-\npretation and validation of cluster analysis. J. Comput. Appl.\nMath., 20(1):53–65, 1987.\n[23] Jianbo Shi and Jitendra Malik. Normalized cuts and image\nsegmentation. Proceedings of IEEE Computer Society Con-\nference on Computer Vision and Pattern Recognition, pages\n731–737, 1997.\n[24] Oriane Sim’eoni, Gilles Puy, Huy V. Vo, Simon Roburin,\nSpyros Gidaris, Andrei Bursuc, Patrick P’erez, Renaud Mar-\nlet, and Jean Ponce. Localizing objects with self-supervised\ntransformers and no labels. In BMVC, 2021.\n[25] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia\nSchmid. Segmenter: Transformer for semantic segmenta-\ntion. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 7262–7272, 2021.\n[26] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gev-\ners, and Arnold WM Smeulders. Selective search for ob-\nject recognition. International journal of computer vision,\n104(2):154–171, 2013.\n[27] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. Extracting and composing robust\nfeatures with denoising autoencoders. In Proceedings of the\n25th international conference on Machine learning, pages\n1096–1103, 2008.\n[28] Huy V Vo, Francis Bach, Minsu Cho, Kai Han, Yann LeCun,\nPatrick P´erez, and Jean Ponce. Unsupervised image match-\ning and object discovery as optimization. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8287–8296, 2019.\n[29] Huy V Vo, Patrick P´erez, and Jean Ponce. Toward unsu-\npervised, multi-object discovery in large-scale image collec-\ntions. In European Conference on Computer Vision, pages\n779–795. Springer, 2020.\n[30] Huy V. Vo, Elena Sizikova, Cordelia Schmid, Patrick P´erez,\nand Jean Ponce. Large-scale unsupervised object discovery.\nIn Advances in Neural Information Processing Systems 35\n(NeurIPS), 2021.\n[31] Yangtao Wang, Xi Shen, Shell Xu Hu, Yuan Yuan, James L\nCrowley, and Dominique Vaufreydaz. Self-supervised trans-\nformers for unsupervised object discovery using normalized\ncut. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 14543–14553,\n2022.\n[32] Xiu-Shen Wei, Chen-Lin Zhang, Yao Li, Chen-Wei Xie,\nJianxin Wu, Chunhua Shen, and Zhi-Hua Zhou. Deep de-\nscriptor transforming for image co-localization. In Proceed-\nings of the Twenty-Sixth International Joint Conference on\nArtiﬁcial Intelligence, IJCAI-17, pages 3048–3054, 2017.\n[33] Nir Zabari and Yedid Hoshen. Semantic segmentation in-\nthe-wild without seeing any segmentation examples. CoRR,\nabs/2112.03185, 2021.\n[34] Z. Zhang, L. Wang, Y. Wang, L. Zhou, J. Zhang, and F.\nChen.\nDataset-driven unsupervised object discovery for\nregion-based instance image retrieval. IEEE Transactions on\nPattern Analysis & Machine Intelligence, (01):1–1, jan 5555.\n[35] C. Lawrence Zitnick and Piotr Doll´ar. Edge boxes: Locating\nobject proposals from edges. In David Fleet, Tomas Pajdla,\nBernt Schiele, and Tinne Tuytelaars, editors, Computer Vi-\nsion – ECCV 2014, pages 391–405, Cham, 2014. Springer\nInternational Publishing.\nSupplementary material\nA. Evaluation of single object discovery methods on the multiple objects discovery task\nIn our study, we have compared our approach mainly to the multi-object discovery (MOD) methods. The comparison\nwith single object discovery (SOD) approaches has only been performed after training an object detector with pseudo labels\nfrom each method. Here we raise the question of what happens if SOD models are applied to MOD tasks. To answer this\nquestion in a quantitative aspect, we provide in table 6 a comparison of our method with LOST[24] and the more recent\nbaseline TokenCut[31]. Since these methods output a single object, odAP is no longer adapted and we use the AP50 metric\nfor comparison. Note that AP50 is more relevant when a ﬁxed number of objects are returned. Considering these results\n(table 6) and those of table 1, we conclude that our approach provides a better precision-recall trade-off than existing object\ndiscovery methods.\nMethod\nVOC07\nVOC12\nCOCO20k\nLOST [24]\n15.7\n17.7\n4.1\nTokenCut [31]\n18.6\n22.6\n5.8\nOurs\n22.5\n23.1\n6.3\nTable 6. Multiple objects discovery results in AP@50%\nIn the following, we provide additional qualitative results on VOC12 and COCOC20k datasets, for the tasks addressed in\nthis work. In particular, in ﬁgures 4, 5, we compare our results with LOST[24], a single object discovery method that achieves\nthe second best mAP after training the class-agnostic object detector (see table 2). We can see from the visual results that\nLOST usually groups several instances, even of different semantics, in complex scenes, with spatially close objects. This\nexplains the superiority of our approach in the initialization of the class-agnostic object detector. We also show in ﬁgures\n6, 7 that after training an object detector with our pseudo-labels, we improve the separation of instances, even of the same\nsemantic category.\nB. More qualitative results for multiple objects discovery on VOC12 dataset\nFigure 4. Qualitative results on VOC12 of the multiple objects discovery. By column: original image, the predicted bounding-box from\nLOST[24], our segmentation result, our pseudo-boxes.\nC. More qualitative results for multiple objects discovery on COCO20k dataset\nFigure 5. Qualitative results on COCO20K of the multiple objects discovery. By column: original image, the predicted bounding-box\nfrom LOST[24], our segmentation result, our pseudo-boxes.\nD. Qualitative results for class-agnostic object detection on VOC12 dataset\nFigure 6. Qualitative results for class-agnostic object detection on VOC12 dataset.\nE. Qualitative results for class-agnostic object detection on COCO20k dataset\nFigure 7. Qualitative results for class-agnostic object detection on COCO20k dataset\nF. Failure cases\nOur approach, although effective, shows some limitations that can be investigated in a future work. In particular, in\nthe merging process, since nearby segments belonging to the same class are merged, this leads to the merging of nearby\ninstances of the same category, see ﬁgure 8. This problem occurs in all existing object discovery methods [29, 24, 31],\nand comes from the semantic information contained in both supervised and self-supervised features. Solving this problem\nrequires, for example, the learning of an instance-variant representation, which is a challenging task. We hope that this work\nwill stimulate interest in this research direction.\nFigure 8. Examples from VOC07 dataset where our approch fails to discover objects, by merging nearby instances of the same category.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2022-12-20",
  "updated": "2022-12-20"
}