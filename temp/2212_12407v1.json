{
  "id": "http://arxiv.org/abs/2212.12407v1",
  "title": "Text classification in shipping industry using unsupervised models and Transformer based supervised models",
  "authors": [
    "Ying Xie",
    "Dongping Song"
  ],
  "abstract": "Obtaining labelled data in a particular context could be expensive and time\nconsuming. Although different algorithms, including unsupervised learning,\nsemi-supervised learning, self-learning have been adopted, the performance of\ntext classification varies with context. Given the lack of labelled dataset, we\nproposed a novel and simple unsupervised text classification model to classify\ncargo content in international shipping industry using the Standard\nInternational Trade Classification (SITC) codes. Our method stems from\nrepresenting words using pretrained Glove Word Embeddings and finding the most\nlikely label using Cosine Similarity. To compare unsupervised text\nclassification model with supervised classification, we also applied several\nTransformer models to classify cargo content. Due to lack of training data, the\nSITC numerical codes and the corresponding textual descriptions were used as\ntraining data. A small number of manually labelled cargo content data was used\nto evaluate the classification performances of the unsupervised classification\nand the Transformer based supervised classification. The comparison reveals\nthat unsupervised classification significantly outperforms Transformer based\nsupervised classification even after increasing the size of the training\ndataset by 30%. Lacking training data is a key bottleneck that prohibits deep\nlearning models (such as Transformers) from successful practical applications.\nUnsupervised classification can provide an alternative efficient and effective\nmethod to classify text when there is scarce training data.",
  "text": "Text classification in shipping industry using unsupervised models and Transformer based \nsupervised models \nYing Xie (School of Management, Cranfield University, Bedford, UK) \nYing.xie@cranfield.ac.uk \nDongping Song (School of Management, University of Liverpool, UK) \nDongping.Song@liverpool.ac.uk \n           \n \n \nObtaining labelled data in a particular context could be expensive \nand time consuming.  Although different algorithms, including \nunsupervised learning, semi-supervised learning, self-learning have \nbeen adopted, the performance of text classification varies with \ncontext. Given the lack of labelled dataset, we proposed a novel and \nsimple unsupervised text classification model to classify cargo \ncontent in international shipping industry using the Standard \nInternational Trade Classification (SITC) codes. Our method stems \nfrom representing words using pretrained Glove Word Embeddings \nand finding the most likely label using Cosine Similarity. \nTo compare unsupervised text classification model with supervised \nclassification, we also applied several Transformer models to \nclassify cargo content. Due to lack of training data, the SITC \nnumerical codes and the corresponding textual descriptions were \nused as training data. A small number of manually labelled cargo \ncontent data was used to evaluate the classification performances of \nthe unsupervised classification and the Transformer based \nsupervised classification. The comparison reveals that unsupervised \nclassification \nsignificantly \noutperforms \nTransformer \nbased \nsupervised classification even after increasing the size of the training \ndataset by 30%.  Lacking training data is a key bottleneck that \nprohibits deep learning models (such as Transformers) from \nsuccessful practical applications. Unsupervised classification can \nprovide an alternative efficient and effective method to classify text \nwhen there is scarce training data.   \nKeywords—text classification, supervised, unsupervised, \ntransformers, cargo content, shipping \nI. INTRODUCTION  \nAlongside the rapid development of digital products and \ntechnologies, container shipping services and operations are \nbeing transformed into a digital era, facilitated by applying \nartificial intelligence (including machine learning, deep \nlearning, natural language processing, etc.), robotics, 5G, \nblockchain and Internet of Things sensors. Past research has \ndeveloped machine learning models for demand prediction, \nquayside \noperation, \nvehicle \ntrajectories, \nship \nfuel \nconsumption and on time performance prediction. In \nsummary, the applications of data analytics and machine \nlearning in container ports are predominantly on port \nthroughput forecasting and vessel estimated time of arrival \nwith very few on port operations management. This paper \nproposes a Natural Language Processing (NLP) based \ntechnique to effectively classify container content. Container \ncontent is an important feature used in machine learning \nmodels to predict container destinations in port operations \nmanagement.  Classifying container content into the Standard \nInternational Trade Classification (SITC) codes reduces the \ndimensionality of container content,  which  improves the \ninterpretability of the machine learning models but without \nscarifying prediction accuracy.  \nIn real world applications, labelled text data are often \nscarce, and obtaining labelled data could be labour intense and \ncostly.  Although different algorithms, including traditional \nsupervised text classification, unsupervised classification, \nsemi-supervised classifications, deep learning models have \ndeveloped, the performance of text classification varies with \ncontext. Recent advancement in deep learning models leads to \nstate-of-the-art results in many NLP tasks, including text \nclassification, question answering, translation, text generation \nand many more. Transformers are a class of deep neural \nnetwork models that significantly improve performances on a \nwide array of standard NLP tasks, by leveraging large-scale \npre-training, transfer learning and parallel processing of the \nwords in the documents. In this research, several Transformer \nmodels were applied to classify cargo content in international \nshipping industry in a supervised manner. Due to lack of \ntraining data, the Standard International Trade Classification \n(SITC) codes and the corresponding descriptions were used as \ntraining data, but the number of training data is very small. On \nthe other hand, we proposed an innovative and simple \nunsupervised classification model which represents texts \nusing Glove Embeddings and assigns classification label \nusing Cosine Similarity. \nII. TEXT CLASSIFICAITON  \nA. Text Representation \nIn text classification, the most fundamental step is text \nrepresentation [1], i.e., a process of converting unstructured \ntext documents to numerical vectors, to make them \nmathematically computable.  The most widely applied text \nrepresentation method is the bag of words (BoW) [2] and \nTerm Frequency-Inverse Document Frequency (TF-IDF). \nBoW maps texts into feature vectors according to the \noccurrences of the words (i.e., features) in that text. TF-IDF \nevaluates how relevant a word is to a document in a collection \nof documents.  \nThe main shortfall of BoW and TF-IDF is the inability of \ncapturing context information of words or texts. This is where \nword embeddings were developed.   Word embeddings model \nwords in vectors of real numbers and capture syntactic \nfeatures and semantic word relationships [3]. Word \nembeddings are commonly used in deep learning models and \npast research showed that the performance of text \nclassification largely depends on effectiveness of the word \nembeddings [4]. Word embeddings can be either trained using \nthe embedding layer in a deep learning model, alternatively, \npre trained word embeddings can be adopted as an initialiser \nin the embedding layer, such as Google’s Word2Vec, BERT, \nStandford’s GloVe FastText developed by Facebook.  \nWord2Vec FastText and GloVe are context independent word \nembeddings that models one vector for each word by \nintegrating all the different senses of the word into one vector. \nBERT produces different word embeddings for a word by \ncapturing its context and considering word order and position \nin a text. One group of studies built text representations by \naggregating the word embeddings as document embeddings \n[4][5], while the other group built text representations by \njointly learning word/document and document embeddings \n[6]; the text representations were then fed into a classifier. \nB. Deep Learning-Based Text Classification \n• ANN, RNN and CNN \nExtracting features manually from input data is \nextremely time-consuming, and it requires strong \nknowledge of the subject as well as the domain. To \novercome this problem, deep learning models are \ndeveloped to automate the process of feature \nengineering and selection. The ability of automatically \nselecting features makes deep learning models popular \nchoices for text classification tasks [7]. Three \nrepresentative deep networks are Artificial Neural \nNetwork \n(ANN) \n[4][8], \nConvolutional \nNeural \nNetwork (CNN) [9][10] and Recurrent Neural \nNetwork (RNN). ANN use Bag of Words to represent \ntext, and the vector representations are fed into one or \nmore feed-forward layers to perform classifications \n[8]. Liu et al. [11] and Luo et al. [12] used LSTM, a \nspecific type of RNN, to learn text representation. \nKim[10] used CNN for sentence classification, while \nConneau et al. [13] applied CNNs at character level \nand achieved promising results. \n• Attention Mechanism \nTo further increase the representation flexibility of \nsuch models and better capture semantics and words \nrelationships, \nresearchers \nintroduced \nvarious \nmechanisms to Neural Network based models. \nAttention mechanisms have been introduced as an \nintegral part of models to model word associations \n[14]. Yang et al. [14] applied a hierarchical two-level \nattention network at the word and sentence level, for \ntext \nclassification. \nThis \nmodel \noutperforms \nsubstantially on text classification problems. \nZhou et al. [15] applied the hierarchical attention \nmodel to multi-lingual sentiment classification. The \nsentence-level attention model learns importance of \nsentences in a document, while the word level attention \nmodel learns the importance of words in each sentence, \nthen the overall sentiment is decided. \n• Transformers \nThe computational costs to hand sequential input data \nin RNNs, or to capture relationships between words in \nCNNs increase potentially along with the length of the \nsentence. To overcome this limitation, Transformers \nwere developed to compute an attention score for \nevery word in a sentence or document in parallel, and \nthe attention score is used to model the influence each \nhas on another [16].  \nTransformers use deep neural network architectures \n(for example, a neural network architecture with 12 or \n24 layers) and are pre-trained on much larger amounts \nof text corpora to learn contextual text representations. \nTransformers have been trained on a large amount of \ntext in a self-learning manner. Self-learning is a type \nof training where the model trains itself by leveraging \none part of the data to predict the other party, and to \ngenerate labels as the learning progresses, hence it \neliminates the necessity of data labelling and reduces \ncost and time of manual labelling [17]. One of the most \nwidely used Transformers is Bidirectional Encoder \nRepresentations for Transformers (BERT) [18]. BERT \nwas pretrained from unlabelled data extracted from \nEnglish Wikipedia and BooksCorpus, on two tasks:  \nmasked language modelling and next sentence \nprediction. As a result of pretraining, it require less \nresources for task specific fine tuning when it is \napplied to create text representations [18][19]. \nApplying deep pretrained Transformers models to \ndownstream NLP tasks has achieved state-of-the-art \nperformance, including text classification, albeit with \nscarce label sets [20][21]. \nC. Multi Class Text Classification  \nMost researchers used open-source text classification \ndatasets to train the deep learning models, and to conduct \nexperiments and analyse classification performance. These \ndatasets include: (1) sentiment analysis datasets, such as Yelp \n(185), IMDB (186), Standford Sentiment Treebank 43, \nAmazon product reviews (189); (2) News Classification \ndatasets, such as 20 Newsgroups 190, Reuters News 191, (3) \nTopic Classification datasets, such as DBpedia (195), \nOhsumed medicine database (196), ERU-lex EU Law \ndatabase 197, Web of Science (WOS) dataset (136); (4) \nNatural Language Inference dataset, such as Stanford NLI \n(208), Mutli-Genre NLI (209, etc.  Most of these datasets have \n2 -5 labels, such as the IMDb with 2 binary classes, AGNews \nhas 4 categories, and Yell news has 5 fine grained sentiment \nlabels. The ERU-lex European Union law has 19,314 \ndocuments and 3,956 categories. Past research has shown the \ndifficulty in performing large scale text classification \nincreases significantly when the number of class labels \nexceeds 100 [22]. \nDifferent from the past research that uses public open-\nsource data to test the classification models, this research \nexamines how to classify shipping cargo content using 10 \ncategory labels. The class labels are mutually exclusive, and \none cargo content is associated with only one label. The \nabsence of labelled training data makes the classification task \nvery challenging. We propose a novel and simple model for \nunsupervised text classification and evaluation. Our method \nstems from representing words using pretrained Glove Word \nEmbeddings and finding the most likely label using Cosine \nSimilarity. \nTo make a comparison, a second model is built as flat \nsupervised classification and it is trained using SITC textual \ndescriptions and manually labelled data. Three Hugging Face \nPre-trained Transformers [16], including BERT, RoBERTa \nand XLNet, are used to represent text and perform supervised \nclassification.  Hugging Face provides pre-trained models for \na variety of transformer architectures and these models have \nshown promise in improving results on tasks with a small \namount of labelled data, which is the challenge facing by this \nresearch. A comparison of transformer-based supervised \nclassification and Cosine similarity based unsupervised text \nclassification is also made. Unsupervised text classification \nproves to be effective in performing the multi class text \nclassification, with higher accuracy score. \nIII. THE NEEDS TO CLASSIFY CARGO CONTENT \nWithin the hundreds of millions of containers received by \na shipping port, many of them contain the same content but \nhave slightly content names. That means some cargo content \nis unique in names, but they belong to the same content \ncategory, for example, “HOBBY RACK BOX1” and \n“HOBBY RACK BOX2” belong to the same category. \nTherefore, when using machine learning models to analyse the \noperation of containers, such as the destinations of containers, \nor the haulage companies appointed to transport containers, it \nis necessary to classify the cargo content into the same \ncategory to reduce the feature dimensionality, to remove \nredundant information and noise, and to reduce computing \ntime in machine learning models.    \nManually labelling a higher number of text documents can \nbe a very labour intensive and intellectually challenging \nactivity; hence, the labelled training documents are often \nunavailable.  Even when subject knowledge is applied to label \ntext documents, variation could be high, especially in multi-\nclass classification where there are many labels. The variation \nin classification is caused by personal opinions and is open to \ndifferent interpretations [23]. For these reasons, the labelled \ncargo content does not exist. In the absence of labelled training \ndata, we proposed an unsupervised classification to categorise \nthe container content. \nIV. PROPOSED UNSUPERVISED CLASSIFICATION \nThe text document is a collection of unique shipping cargo \ncontent which is retrieved from a shipping port’s operation \nterminal. The 10 category labels are derived from the Level 1 \nSITC codes. SITC is a product classification of the United \nNations used for international trade.  SITC has five levels of \nclassification structures, comprising Level 1 that defines \ngoods in 10 Sections using one-digit code (0-9), Level 2 that \ndefines goods in 67 divisions using two-digit codes, Level 3 \nwith 261 groups defined using three-digit codes, Level 4 that \ndefines 1033 subgroups using four-digit codes and Level 5 \nthat defines goods in 2970 items using five-digit codes. An \nexample of the hierarchical SITC codes is given in Table 1:  \nTABLE I.  \nAn example of hierarchical Standard International Trade \nClassification codes \nLevel \nCode \nTextual Description \n1 \n0 \nFood and live animals \n2 \n00 \nLive animals \n3 \n001 \nLive animals other than animals of division 03 \n4 \n0011 \nBovine animals, live \n5 \n00111 \nPure-bred breeding animals \nTF-IDF vectorizer was applied as weights to Word \nEmbedding to create Weighted Embedding Matrix.  Some of \nthe cargo content has detailed description, such as “FROZEN \nINDIAN \nMACKEREL \nWHOLE \nROUND”, \n“BTM \nMACKEREL IN TOMATO SAUCE”, “PREPARED OR \nPRESERVED TUNAS, S”, and “CANNED TUNA”, while \nsome of the content is brief and unspecific, such as \n“FURNITURE”. To maximise the classification accuracy, we \nbuilt category word embeddings of the most detailed SITC \nLevel 5 structure to calculate Cosine similarity with the text \nword embeddings. \nA. Unsupervised classification Using SITC \nThe multiple class text classification was applied to label \nsimilar container content using the predefined Level 5 SITC \ncodes. The aim of classification is to systematically and \nobjectively transforming the large number of individual \ncontent items into meaningful categories, reducing the \ndimension size of this feature and saving computing time.   \n1) Pre-processing categorical varables using Natural \nLanguage Processing \nA pipeline of NLP steps was performed to pre-process the \ntext data contained: \na) Preprocessing using Regular Expresssions (Regex): \nRegex is used to remove non-alphanumeric characters from \nthe text and to remove extra spaces and punctuations. Regex \nis also performed to decontract words (replacing the words \nlike can’t with cannot) and to remove newlines and tabs (such \nas /, -, /n). All the texts were converted to lower case to avoid \nalgorithms interpreting same words with different cases as \ndifferent.  \nb) Removing \nstop \nwords, \ntokenisation \nand \nlemmatization: Stopwords are commonly used words that do \nnot carry much meaning or weight compared with other \nwords, such as “the”, “and”, “or”, etc. Tokenisation is the \nprocess of splitting a string into individual tokens. A sentence \ncan be tokenised into n-grams tokens (n=1, 2, 3…), i.e., a \nsequence of n words. Lemmatisation is the process of \nreducing the number of words into a single word by \ncombining common words together, for example, “blogging” \nand “blogger” are lemmatised into the root word “blog”.   \nc) Feature extraction: text data needs to be covered \ninto numerical vectors before being processed by any \nmachine learning algorithm. Word Embeddings map words \nfrom the vocabulary to low dimensional, dense and real \nvalued vectors, by capturing syntactic and semantic \ninformation. Words from the same context are clustered \ntogether and are represented by similar vectors. Each \ndimension of the vectors represents a different aspect of \nwords [24]. For example, a word embedding scheme W maps \na word to a d-dimensional vector: word→ R^d, where d is \nusually an integer of 100, 200, 300 or 500. For example, the \nwords “machine” and “learning” can be represented as: \n W(‘machine’)=(-0.406, 0.306, -0.012, …)  \n W(‘learning)=(-0.591, 0.432, 0.721,…) \n In machine learning, embeddings not only aid deep \nlearning but also help feature pre-processing [25]. The \ncommonly used pre-trained word embeddings techniques are \nGoogle’s Word2Vec [26].  Stanford’s GloVe [27] and \nFacebook’s Fasttext [4]. In this research, a pre-trained Glove \nembedding “glove.6B.100d.txt” was used to produce vector \nrepresentation of words in general cargo content and the \nSITC codes.  The pre-trained Glove embedding maps a word \nto a 100-dimensional vector: word→ R^100. Each cargo \ncontent and SITC code is represented as a N x100 matrix, \nwhere N is the number of words in the cargo content or the \nSITC code. \n2) Integrating unsupervised learning and classification \nText similarity between two pieces of text is determined \nby lexical similarity (closeness in words) and meaning \n(semantic similarity). Jia et al. [28] used unsupervised pre-\ntraining to estimate an embedding layer and represent \ncategorical variables. In a novel unsupervised text \nclassification model, elements in TF-IDF matrix were applied \nas weights to Word Embedding to create Weighted \nEmbedding Matrix. Weighted Word embedding proved to \nimprove text classification accuracy, in comparison with non-\nweighted word embedding models [29].  \nWhen comparing similarity between cargo content and the \nSITC codes, we need to determine the text similarity, i.e., how \nclose two pieces of text are both in lexical similarity and \nsemantic similarity. A commonly used similarity measure for \ntext clustering is Cosine Similarity [30].  Cosine similarity \nmeasures the similarity between vectors using the cosine value \nof the angle and determines whether two vectors are pointing \nin roughly the same direction. Compared with Euclidean \ndistance which directly measures the linear interval or length \nbetween vectors, Cosine similarity pays more attention to the \ndifference between the relative levels of the dimensions, \nhence, it has a better effect than Euclidean distance. Even if \nthe two texts are distant by the Euclidean distance, they may \nstill be oriented together in terms of Cosine similarity. The \nsmaller the angle, higher the cosine similarity. \nAssuming there are two nonzero vectors in space x  and y, \nthe Cosine similarity is defined as the Cosine of the angle \nbetween the two vectors: \nCosine similarity(|x∙y|)=xy/(‖x‖  ‖y‖ )   \n   (1) \nThe following Algorithm 1 was executed to classify \ncontainer content into predefined categories: \nAlgorithm 1: The algorithm to perform unsupervised classification \n1 \nInput: unique cargo content and a list of predefined categories of \nSITC codes \n2 \nOutput: unique cargo content labelled with a predefined SITC \ncode \n3 \nAppend the cargo content to the predefined categories of SITC \ncodes and constitute a corpus C m×n \nwhere m denotes the number of documents in C and n  is the \nnumber of unique terms in C \n4 \nObtain a TI-IDF term document matrix M with C, M ∈R m×n \n5 \nFor each document t  in C, pad it to length n, t ∈{1,…,m} \n6 \nGet the word embedding matrix E∈R n×l, where l  is the embedding \nsize of the pre-trained vectors created by GloVe model (l=100 in \nthis research) \n7 \nCreate an empty weighted document embedding matrix W∈R m×l \n8 \nFor each  t∈{0,…,m-1} do: \n9 \n     For each   j ∈{0,…,n-1} do: \n10 \n      W[t, j+1] = W[t, j]+ E[j]∘M[t][j], where ∘ denotes the element \nwise multiplication of the two  matrices \n11 \n    End for \n12 \nEnd for \n13 \nCalculate the pair wise Cosine similarity of the W \n14 \nFind the index SITC∈{1,…,m-1} that achieves the maximum \nsimilarity between W[SITC] and W[m] \n15 \nAssign SITC to the cargo content as the classification label \n16 \nRecord the classification label SITC in the Dataframe DF \nTaking a content text and append it to the end of 2970 \npredefined Level 5 SITC categories, we created a corpus C \nconsisting of m documents and n  unique terms. The TF-IDF \ndocument matrix M  ∈𝑅𝑚×𝑛 was computed and used as \nweight vectors. The weighted word embeddings were \ngenerated as follows. Every document t (such as a cargo \ncontent or a predefined category) was padded it to the length \nn, and represented as a vector 𝑡∈𝑅1×𝑛. Pre-trained Glove \nembeddings [27] was loaded, and every unique term in the \ncorpus was converted into a 1×l vector.  We used Glove \nembedding with the word vector dimension of 100, hence \nl=100. We got a word embedding matrix 𝐸∈𝑅𝑛×𝑙 to \nrepresent the whole corpus. Every document was therefore \nconverted into a matrix of 𝑡= (𝑒1, … , 𝑒𝑛) ∈𝑅𝑛×𝑙. Merging \nthe document matrix t for the whole corpus C, we got a \ndocument embedded matrix 𝐷𝐸∈𝑅𝑚×𝑛×𝑙. To calculate \npairwise Cosine similarity between the row vectors in 𝐷𝐸∈\n𝑅𝑚×𝑛×𝑙, we converted every tth row 𝐷𝐸𝑡∈𝑅𝑛×𝑙 into a single \nvector. Calculating weighted average of word vectors in 𝐷𝐸𝑡 \nusing the TF-IDF weights, we could reserve the weights by \nthe term uniqueness to the selected content and generate a \nweighted document embedding matrix 𝑊∈𝑅𝑚×𝑙.  \nPairwise Cosine similarity was calculated between the \ncargo content and one of the 2970 predefined SITC categories.  \nAfter 2970 iterations, the label of the SITC code was assigned \nas the cluster label to the cargo content, if the Cosine similarity \nbetween the instance of cargo content and SITC category is \nthe largest, as expressed in equation (2): \nLevel 5 label=argmax(Cosine similarity (cargo content, SITC \ncategory))  \n \n \n \n \n      (2) \nV. TRANSFORMER BASED SUPERVISED CLASSIFICATION \nThe supervised classification models are built on several \nTransformers implemented by HuggingFace library [31], \nincluding BERT, RoBERTa and XLNet. As shown in Figure \n1, Transformer model has two main components, namely the \nEncoder and the Decoder [16]. The Encoder is composed of 6 \nidentical layers, with each layer having two sub-layers, i.e. a \nmulti-head self-attention mechanism and a simple fully \nconnected Feed Forward network.  \nThe Encoder first performs the encoding of the sentences \nbased on its tokenizer and produces an embedding matrix with \nan embedding vector for each word. The embedding matrix of \nthe input document is concatenated with a positional \nencoding; the concatenated matrix enters the multi-head \nattention block; the multi-head attention layer chooses which \nparts of the text for the model to focus on; the output from the \nmulti-head attention block x is normalised in a normalisation \nlayer \nusing \nthis \nfunction: \nLayerNorm(x+Multi-Head \nAttention(x)); the normalised output is then fed into a fully \nconnected Feed Forward pass on the neural network and \nnormalised again before being passed to the decoder using a \nsimilar function: LayerNorm(x + FeedForward(x)).   \n \nFig. 1. The encoder-decoder architecture of a Transformer  (adapted from \n[16]) \nAs shown in Fig. 1, the Decoder also has 6 identical layers \nwith each layer having three sub-layers, including the two sub-\nlayers used in the Encoder plus a third sublayer that performs \nmasked multi-head attention directly on the target output \nsequence. Similar operations as those in the Encoder are \nemployed in each of the three sub-layers. Masked multi-head \nattention prevents positions from attending to subsequent \npositions, ensuring that the predictions for one position \ndepends only on the known outputs ahead of this position. The \nmasked multi-head attention is more useful for NLP tasks that \npredict a sequence of numbers or vectors, such as language \ntranslation or next sentence prediction. In text classification, \nour output is a single output, so the masked multi-head \nattention sublayer does not add much value.  In the Decoder, \na FeedForward network receives the final attention vectors \nand uses them to produce a single vector. Applying the \nSoftmax or other suitable activation functions to this vector \nproduces a set of probabilities belonging to each category. \nThree Transformer models were implemented in this \nresearch: BERT [32], RoBERTa [33] and XLNet [34]. All the \nthree models have demonstrated high levels of accuracy \nacross NLP tasks, and thus they were selected to classify \nshipping cargo content when only a small number of training \ndata was available. BERT and RoBERTa are examples of \nautoencoding based pretraining models where a certain \nportion of tokens are masked, and the models are trained to \nrecover the original tokens in the context. XLNet is an \nexample of autoregressive language model that predicts a \nfuture word given preceding context. BERT is pretrained over \nhundreds of millions of textual data to learn a language \nrepresentation that can be fine-tuned for special NLP tasks. \nDifferent from Word Embeddings like Glove and Word2Vec \nthat convert the word into one vector, BERT reviews the entire \nsentence and then assigns an embedding to each word based \non the contexts, hence the same word in a different context \nwould be converted to a different vector. The key methods \nused in BERT include bidirectional Transformers, Masked \nLanguage Model and Next Structure Prediction. RoBERTa \nand XLNet are improved versions of BERT. Both models \nimproved training using much larger data sets and more \ncomputational power, leading to better prediction results. \nInstead of using masked language model, XLNet introduces \npermutation language modelling where all tokens are \npredicted in random order. RoBERTa replaces the Next \nSentence Prediction task with dynamic masking so that the \nmasked token changes during the training. The set of \nparameters used in the Transformers models are given in \nTable II. \nTABLE II.  \nPARAMETERS OF TRANSFORMERS \nTransformers \nBERT \nRoBERTa \nXLNet \nTokenizer parameters \nModel name \nBert-base-\nuncased \nRoberta-base \nXlnet-base-\ncased \nMax length \n128 \n128 \n128 \npad to max \nlength \nTrue \nTrue \nTrue \nTransformers parameters \noptimizer \nAdamW \nAdamW \nAdamW \nLoss function \nCrossEntropyLoss \nCrossEntropyLoss \nCrossEntropyLoss \nMetrics \naccuracy \naccuracy \naccuracy \nVI. TRANING AND TEST DATA \nWe used real cargo container content to evaluate the \ndifferent methods described in the previous section. Two sets \nof training data were constructed: Training Dataset A is \ncomposed of 2970 Level 5 SITC textual headings  plus10 \nLevel 1 SITC textual headings, i.e., in total 2980 instances of \ndata.  The  corresponding numerical codes of these textual \nheadings are used as classification labels (see Appendix 1); \nTraining Dataset B is a bigger set, including Dataset A and \nadditional 970 manually labelled cargo content. The \npredefined list of headings reported in SITC Level 5 creates a \ncomprehensive training corpus that includes most of the \nvocabulary relevant to international traded goods. Since the \ncategory labels are derived from the Level 1 SITC codes, we \nmerged the headings at SITC Levels 1 and 5 to create Training \nDataset A. Two annotators were hired to classify 1000 \ninstances of cargo content using 10 Level 1 codes.  Inter-rater \nreliability based on the percent of agreement between \nannotators [35] was 84.8% for the entire set. The disagreement \nbetween the two annotators was discussed and content was \nreclassified accordingly. Among the 1000 labelled data, 970 \nwere added to Dataset A to create Dataset B, and the rest 30 \ninstances of manual labelled cargo content data were used to \ncreate the test dataset. In the test dataset, each instance has a \nSITC Level 1 label (0-9) assigned exclusively. Table III \npresents a description of the datasets. \nTABLE III.  \nDATASETS DESCRIPTION \nDataset \nNumber of \ndocuments \nLabels \nTraning dataset A: \nSITC Labels \n2980 \n10 \nTraning dataset B: \nSITC Labels+manual labelling \n3950 \n10 \nTest dataset \n30 \n10 \nVII. EVALUATION OF CLASSIFICATION PERFORMANCE \nThe unsupervised and supervised classification models \nwere evaluated by the metric Accuracy (ACC). Accuracy is \ncommonly used for comparing multiclass classification [36]. \nThe classification Accuracy (ACC) is expressed in terms of \nthe percentage of instances correctly classifies, as shown in \nequation (3): \nACC = C/N                                 (3) \nwhere C is the number of stances that are classified correctly, \nand N is the set of all the testing instances.  \nTo evaluate the actual performance of the unsupervised \ntext classification using Glove word embeddings and Cosine \nsimilarity, we compared the unsupervised classification \nperformances with the classification results obtained from a \nmanual labelling process, as shown in Table IV. Since the \nunsupervised classification used Level 5 labels and the manual \nlabelling process used Level 1 labels, as shown in the columns \nof “Predicted Level 5 label” and “Manual label” in Table III, \nthe Level 5 labels of the unsupervised classifications were \nconverted to Level 1 labels using the first digit in the Level 5 \ncodes. The “Similarity” column presents the Cosine Similarity \nof the classification. The Cosine similarity of the classification \nranges in [0, 0.99], showing that some content may not be \nfound in the STIC codes while others have good matches.  \nIn Table V, the unsupervised classification results are \ncompared with the performances of the supervised models.  \nDue to the lack of training data, the classification ACC of the \nthree Transformer based models are quite low, although \nfeeding additional manually labelled data using Dataset B \nslightly improved the classification performances. In \ncomparison, it is apparent that the ACC of unsupervised \nclassification is much higher. The unsupervised classification \nmodel outperforms the three Transformer based supervised \nclassification models, even after the size of the training dataset \nis significantly increased by 30%. The results show that the \nunsupervised text classification could act as a practical \nalternative to categorise cargo content in the absence of \nlabelled training data. \nTABLE IV.  \nEXAMPLES OF EVALUCATING UNSUPERVISED \nCLASSIFICATION PERFORMANCE \nCargo content \nPredicted \nLevel 5 label \nPredicted \nLevel 1 label \nMannual \nLabel \nSimilarit\ny \nAuto parts \n78439 \n7 \n7 \n0.787 \nBaby car seat \n82112 \n8 \n8 \n0.832 \nElectrically \nCalcined \nAnthracite Coal \n32121 \n3 \n3 \n0.844 \nHand sanitiser \n73515 \n7 \n5 \n0.485 \nTABLE V.  \nA COMPARISON OF SUPERVISED AND UNSUPERVISED \nCLASSIFICATION \nClassifiers \nAccuracy (ACC) \nIncrement of \nAccuracy \nUnsupervised \nclassification \n83% \n \nTraining Dataset A (2980） \nBERT \n6/30=20% \n0 \nRoBERTa \n8/30=27% \n0 \nXLNET \n7/30=23% \n0 \nTraining Dataset B （3950） \nBERT \n11/30=37% \n17% \nRoBERTa \n12/30=40% \n13% \nXLNET \n10/30=33% \n10% \nThe results in Tables IV and V show that the unsupervised \nclassification can produce good results. Further experiments \nwere conducted to classify the 98,321 instances of cargo \ncontent using the unsupervised model, and the achieved \nclassification accuracy is 82%. It shows the unsupervised \nclassification model can be applied to deal with large scale \ntext classification problems in real business context, albeit \nlacking labelled data. \nVIII. CONCLUSION, LIMITATIONS AND FUTURE WORK \nThe unsupervised classification model is built on pre-\ntrained Word Embeddings and Cosine Similarity. In the \nabsence of training data, the efficiency and effectiveness of \nthis model are verified in a real business case where the model \nis applied to classify shipping cargo content.  \nDue to the scarce training data, we applied the new state-\nof-the art Transformers to conduct self-learning and \nsupervised classification. However, the accuracies achieved \nby the supervised classification models are significantly lower \nthan the ones achieved by the unsupervised classification \nmodel. We tested the Transformers based classification across \ntwo different training datasets to illustrate the impacts of \nadditional good quality training data.  With 30% of additional \ntraining data (970 in this research), we can improve the \nclassification accuracy by 10% or more.  \nWhile Transformers can potentially reduce the amount of \nlabelled data necessary for achieving optimal levels of \nclassification accuracy, our findings suggest that good \nperformances of these model still depend on high quality and \nsufficient labelled data.  \nIn the future studies, the unsupervised classification model \ncould be used as a baseline to generate more labelled data for \nthe supervised models. Given that SITC has a large number of \nclosely related categories present in a hierarchical structure, \nhierarchical multi class text classification will be developed \nand tested. Hybrid embedding based text representation of all \ncategories in the 5-level SITC hierarchy will be developed, \nwith an aim to improve classification accuracy in the absence \nof large amount of training data. The hybrid embedding \nconsists of graph embedding of categories in the hierarchy and \ntheir word embedding of category labels. \nREFERENCES \n[1] R. Zhao and K. Mao, “Fuzzy bag-of-words model for document \nrepresentation,” IEEE transactions on fuzzy systems, vol. 26, iss. 2, \npp.794–804, 2020.  \n[2] H.T. Nguyen, P. H. Duong and E. Cambria, “Learning short-text \nsemantic similarity with word embeddings and external knowledge \nsources,” Knowledge-Based Systems, vol. 182, p. 104842, 2019.  \n[3] Y. Li and T. Yang, “Word embedding for understanding natural \nlanguage: a survey,” In Guide to Big data applications. Springer, Cham, \n2018, pp. 83–104.  \n[4] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou and T. \nMikolov, “ zip: Compressing text classification models,” preprint \narXiv: 1612.03651, 2016.  \n[5] Q. Le and T Mikolov, “ Distributed representations of sentences and \ndocuments,” Proceedings of the Internation Conference on Machine \nLearning, 2014, pp. 1188–1196. \n[6] J. Tang, M. Qu and Q. Mei, “Pte: Predictive text embedding through \nlarge-scale heterogeneous text networks,” In Proceedings of the 21th  \nACM SIGKDD international conference on knowledge discovery and \ndata mining, pp. 1165–1174, August 2015. \n[7] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenghlu and \nJ. Gao, “Deep learning—based tect classification: a comprehensive \nreview,” ACM Computing Surveys (CSUR), vol. 54, iss. 3, 2021, pp. \n1–40.  \n[8] M. Iyyer, V. Manjunatha, J. Boyd-Graber and H. Daume III, “Deep \nunordered compostion rivals syntactic methods for text classification,” \nIn Proceedings of the 53rd Annual Meeting of the Association for \nComputaional Linguistics and the 7th International Joint Conference \non Natural Language Processing, 2015, pp. 1681–1691. \n[9] N. Kalchbrenner, G. Edward and P. Blunsom, “A convolutional neural \nnetwork for modelling sentences,” In Proceedings of the 52nd Annual \nMeeting of the Association for Computational Linguistics (ACL’14),  \nDOI: http://dx.doi.org/10.3115/v1/p14-1062, arxiv:1404.2188, 2014.  \n[10] Y. Kim, “Convolutional neural networks for sentence classification,” \nIn Proceedings of the Conference on EmpiricalMethods in Natural \nLanguage \nProcessing \n(EMNLP’14), \nDOI: \nhttp://dx.doi.org/10.3115/v1/d14-1181, arxiv:1408.5882, 2014. \n[11] P. Liu, X. Qiu and X. Huang, “Recurrent neual network for text \nclassification \nwithmulti-task \nlearning,” \nretrieved \nfrom: \nhttps://arXiv:1605.05101, 2016.  \n[12] Y. Luo, “Recurrent neural networks for classifying relations in clinical \nnotes,” Journal of biomedical informatics, vol. 72, 2017, pp. 85–95. \n[13] A. Conneau, H. Schwenk, L. Barrault and Y. Lecun, “Very deep \nconvolutional networks for text classification,” retrieved from: \nhttps://arXiv:1606.01781, 2017.  \n[14] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola and E. Hovy, \n“Hierarchical attention networks for document classification,” In \nProceedings of the Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language \nTechnologies, pp. 1480–1489, 2016. \n[15] X. Zhou, X. Wan and J. Xiao, “Attention-based LSTM network for \ncross-lingual sentiment classification,” In Proceedings of the \nConference on Empirical Methods in Natural Language Processing,  \npp. 247–256, 2016. \n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. \nGomez, L. Kaiser and I. Polosukhim, “Attention is all you need,” \nAdvances in neural information processing systems, 2017, vol. 30.  \n[17] D. Hendrycks, M. Mazeika, S. Kadavath and D. Song, “Using self-\nsupervised learning can improve model robustness and uncertainty,” \nAdvances in Neural Information Processing Systems, 2017, vol. 32. \n[18] J. Devlin, M.W. Chang, K. Lee and K. Toutanova, “Bert: Pre-training \nof deep bidirectional transformers for language understanding,” \npreprint arXiv:1810.04805, 2018.  \n[19] J. Dodge, G. Ilharco, R. Schwartz, A. Farhadi, H. Hajishirzi and N. \nSmith, \n“Fine-tuning \npretrained \nlanguage \nmodels: \nWeight \ninitializations, \ndata \norders, \nand \nearly \nstopping,” \npreprint \narXiv:2002.06305, 2020.  \n[20] F. Souza, R. Nogueira and R. Lotufo, “October. BERTimbau: \npretrained BERT models for Brazilian Portuguese,” In Brazilian \nConference on Intelligent Systems, Springer, Cham, 2020, pp. 403–\n417.  \n[21] W.C. Chang, H.F. Yu, K. Zhong, Y. Yang and I.S. Dhillon, “Taming \npretrained transformers for extreme multi-label text classification,” In \nProceedings of the 26th ACM SIGKDD International Conference on \nKnowledge Discovery & Data Mining, pp. 3163–3171, August 2020. \n[22] E.L. Mencía and J. Fürnkranz, “Efficient pairwise multilabel \nclassification for large-scale problems in the legal domain,” In Joint \nEuropean Conference on Machine Learning and Knowledge Discovery \nin Databases, Springer, Berlin, Heidelberg, pp. 50–65, September  \n2008. \n[23] N. Shafiabady, L.H. Lee, R. Rajkumar, V.P. Kallimani, N.A. Akram, \nand D. Isa, “Using unsupervised clustering approach to train the \nSupport Vector Machine for text classification,” Neurocomputing, vol: \n211, 2016,  pp. 4–10.  \n[24] C. Wang, Y.  Song, H. Li, M. Zhang, and J. Han, “Text classification \nwith heterogeneous information network kernels,” In AAAI, pp. 2130–\n2136, 2016.  \n[25] M. Kraus, S. Feuerriegel and A. Oztekin, “Deep learning in business \nanalytics and operations research: Models, applications and managerial \nimplications,” European Journal of Operational Research, vol: 281, iss. \n3, 2020,  pp.628–641. \n[26] T. Mikolov, I. Sutskever, K. Chen, G.S. Corrado and J. Dean, \n“Distributed representations of words and phrases and their \ncompositionality,” Advances in neural information processing \nsystems, vol: 26, 2013. \n[27] J. Pennington, R. Socher and C.D. Manning, “Glove: Global vectors \nfor word representation,” In Proceedings of the 2014 conference on \nempirical methods in natural language processing (EMNLP), pp. 1532–\n1543, October 2014.  \n[28] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. \nGuadarrama and T. Darrell, “Caffe: Convolutional architecture for fast \nfeature embedding,” In Proceedings of the 22nd ACM international \nconference on Multimedia, pp. 675–678, November 2014. \n[29] B. Guo, C. Zhang, J. Liu and X. Ma, \"Improving text classification with \nweighted word embeddings via a multi-channel TextCNN model,\" \nNeurocomputing, vol: 363, 2019, pp. 366–374. \n[30] J. Han, M. Kamber and J. Pei, Getting to Know Your Data. In Data \nMining (Third Edition), Boston: Morgan Kaufmann, pp. 39–82, \nFebruary 2012.  \n[31] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. \nCistac, T. Rault, R. Louf, M. Funtowicz and J. Davison, \n“Huggingface's transformers: State-of-the-art natural language \nprocessing,”  preprint arXiv:1910.03771, 2019. \n[32] J. Devlin, M. Chang, K. Lee and K. Toutanova, “Bert: Pre-training of \ndeep bidirectional transformers for language understanding” preprint  \narXiv:1810.04805, 2018.  \n[33] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, \nL. Zettlemoyer and V. Stoyanov, “Roberta: A robustly optimized bert \npretraining approach”, Retrieved from https://arXiv:1907.11692, 2019.  \n[34] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R.R. Salakhutdinov and Q.V. \nLe, Xlnet: Generalized autoregressive pretraining for language \nunderstanding. Advances \nin \nneural \ninformation \nprocessing \nsystems,  vol: 32, 2019. \n[35] S.E. Stemler and J.  Tsai, “Best practices in interrater reliability: Three \ncommon approaches,” Best practices in quantitative methods, pp.29–\n49, 2008. \n[36] A. Mathur and G.M. Foody, “Multiclass and binary SVM \nclassification: Implications for training and classification users.”  IEEE \nGeoscience and remote sensing letters, vol. 5, iss. 2, 2008, pp.241–245. \n \n \n \n \n \n \n \n \n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "ACM-class: J.m"
  ],
  "published": "2022-12-21",
  "updated": "2022-12-21"
}