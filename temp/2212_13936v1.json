{
  "id": "http://arxiv.org/abs/2212.13936v1",
  "title": "On Pathologies in KL-Regularized Reinforcement Learning from Expert Demonstrations",
  "authors": [
    "Tim G. J. Rudner",
    "Cong Lu",
    "Michael A. Osborne",
    "Yarin Gal",
    "Yee Whye Teh"
  ],
  "abstract": "KL-regularized reinforcement learning from expert demonstrations has proved\nsuccessful in improving the sample efficiency of deep reinforcement learning\nalgorithms, allowing them to be applied to challenging physical real-world\ntasks. However, we show that KL-regularized reinforcement learning with\nbehavioral reference policies derived from expert demonstrations can suffer\nfrom pathological training dynamics that can lead to slow, unstable, and\nsuboptimal online learning. We show empirically that the pathology occurs for\ncommonly chosen behavioral policy classes and demonstrate its impact on sample\nefficiency and online policy performance. Finally, we show that the pathology\ncan be remedied by non-parametric behavioral reference policies and that this\nallows KL-regularized reinforcement learning to significantly outperform\nstate-of-the-art approaches on a variety of challenging locomotion and\ndexterous hand manipulation tasks.",
  "text": "On Pathologies in KL-Regularized Reinforcement\nLearning from Expert Demonstrations\nTim G. J. Rudner∗†\nUniversity of Oxford\nCong Lu∗\nUniversity of Oxford\nMichael A. Osborne\nUniversity of Oxford\nYarin Gal\nUniversity of Oxford\nYee Whye Teh\nUniversity of Oxford\nAbstract\nKL-regularized reinforcement learning from expert demonstrations has proved\nsuccessful in improving the sample efﬁciency of deep reinforcement learning al-\ngorithms, allowing them to be applied to challenging physical real-world tasks.\nHowever, we show that KL-regularized reinforcement learning with behavioral\nreference policies derived from expert demonstrations can suffer from patholog-\nical training dynamics that can lead to slow, unstable, and suboptimal online\nlearning. We show empirically that the pathology occurs for commonly chosen\nbehavioral policy classes and demonstrate its impact on sample efﬁciency and\nonline policy performance. Finally, we show that the pathology can be remedied by\nnon-parametric behavioral reference policies and that this allows KL-regularized\nreinforcement learning to signiﬁcantly outperform state-of-the-art approaches on a\nvariety of challenging locomotion and dexterous hand manipulation tasks.\n1\nIntroduction\nReinforcement learning (RL) [15, 24, 46, 47] is a powerful paradigm for learning complex behaviors.\nUnfortunately, many modern reinforcement learning algorithms require agents to carry out millions\nof interactions with their environment to learn desirable behaviors, making them of limited use\nfor a wide range of practical applications that cannot be simulated [8, 28]. This limitation has\nmotivated the study of algorithms that can incorporate pre-collected ofﬂine data into the training\nprocess, either fully ofﬂine or with online exploration, to improve sample efﬁciency, performance, and\nreliability [2, 6, 16, 23, 52, 53]. An important and well-motivated subset of these methods consists of\napproaches for efﬁciently incorporating expert demonstrations into the learning process [5, 11, 18, 42].\nReinforcement learning with Kullback-Leibler (KL) regularization is a particularly successful ap-\nproach for doing so [3, 27, 29, 31, 44, 51]. In KL-regularized reinforcement learning, the standard\nreinforcement learning objective is augmented by a Kullback-Leibler divergence term that penal-\nizes dissimilarity between the online policy and a behavioral reference policy derived from expert\ndemonstrations. The resulting regularized objective pulls the agent’s online policy towards the\nbehavioral reference policy while also allowing it to improve upon the behavioral reference policy by\nexploring and interacting with the environment. Recent advances that leverage explicit or implicit\nKL-regularized objectives, such as BRAC [51], ABM [44], and AWAC [27], have shown that KL-\nregularized reinforcement learning from expert demonstrations is able to signiﬁcantly improve the\nsample efﬁciency of online training and reliably solve challenging environments previously unsolved\nby standard deep reinforcement learning algorithms.\n∗Equal contribution. † Corresponding author: tim.rudner@cs.ox.ac.uk.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2212.13936v1  [cs.LG]  28 Dec 2022\nParametric\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10.0\n−7.5\n−5.0\n−2.5\n0.0\n2.5\n5.0\n7.5\n10.0\nNon-Parametric\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10.0\n−7.5\n−5.0\n−2.5\n0.0\n2.5\n5.0\n7.5\n10.0\n0.000\n0.003\n0.006\n0.009\n0.012\n0.015\n0.012\n0.020\n0.028\n0.036\n0.044\n0.052\nFigure 1: Predictive variances of non-parametric and parametric behavioral policies on a low dimen-\nsional representation (the ﬁrst two principal components) of a 39-dimensional dexterous hand manipula-\ntion state space (see “door-binary-v0” in Figure 5). Left: Parametric neural network Gaussian behavioral\npolicy πψ(· | s) = N(µψ(s), σ2\nψ(s)). Right: Non-parametric Gaussian process posterior behavioral policy\nπGP(· | s, D0) = GP(µ0(s), Σ0(s, s′)). Expert trajectories D used to train the behavioral policies are shown\nin black. The GP predictive variance is well-calibrated: It is small near the expert trajectories and large in other\nparts of the state space. In contrast, the neural network predictive variance is poorly calibrated: It is relatively\nsmall on the expert trajectories, and collapses to near zero elsewhere. Note the signiﬁcant difference in scales.\nContributions. In this paper, we show that despite some empirical success, KL-regularized rein-\nforcement learning from expert demonstrations can suffer from previously unrecognized pathologies\nthat lead to instability and sub-optimality in online learning. To summarize, our core contributions\nare as follows:\n• We illustrate empirically that commonly used classes of parametric behavioral policies experi-\nence a collapse in predictive variance about states away from the expert demonstrations.\n• We demonstrate theoretically and empirically that KL-regularized reinforcement learning al-\ngorithms can suffer from pathological training dynamics in online learning when regularized\nagainst behavioral policies that exhibit such a collapse in predictive variance.\n• We show that the pathology can be remedied by non-parametric behavioral policies, whose\npredictive variances are well-calibrated and guaranteed not to collapse about previously unseen\nstates, and that ﬁxing the pathology results in online policies that signiﬁcantly outperform state-\nof-the-art approaches on a range of challenging locomotion and dexterous hand manipulation\ntasks.\nThe left panel of Figure 1 shows an example of the collapse in predictive variance away from the\nexpert trajectories in parametric behavioral policies. In contrast, the right panel of Figure 1 shows\nthe predictive variance of a non-parametric behavioral policy, which—unlike in the case of the\nparametric policy—increases off the expert trajectories. By avoiding the pathology, we obtain a\nstable and reliable approach to sample-efﬁcient reinforcement learning, applicable to a wide range of\nreinforcement learning algorithms that leverage KL-regularized objectives.2\n2\nBackground\nWe consider the standard reinforcement learning setting where an agent interacts with a discounted\nMarkov Decision Process (MDP) [46] given by a 5-tuple (S, A, p, r, γ), where S and A are the state\nand action spaces, p(· | st, at) are the transition dynamics, r(st, at) is the reward function, and γ is\na discount factor. ρπ(τt) denotes the state–action trajectory distribution from time t induced by a\npolicy π(· | st). The discounted return from time step t is given by R(τt) = P∞\nk=t γkr(sk, ak) for\nt ∈N0. The standard reinforcement learning objective to be maximized is the expected discounted\nreturn Jπ(τ0) = Eρπ(τ0)[R(τ0)] under the policy trajectory distribution.\n2.1\nImproving and Accelerating Online Training via Behavioral Cloning\nWe consider settings where we have a set of expert demonstrations without reward,\nD0 = {(sn, an)}N\nn=1 = {¯S, ¯A}, which we would like to use to speed up and improve online learn-\n2Code and visualizations of our results can be found at https://sites.google.com/view/nppac.\n2\ning [5, 42]. A standard approach for turning expert trajectories into a policy is behavioral cloning [1, 4]\nwhich involves learning a mapping from states in the expert demonstrations to their corresponding\nactions, that is, π0 : S →A. As such, behavioral cloning does not assume or require access to a\nreward function and only involves learning a mapping from states to action in a supervised fashion.\nSince expert demonstrations are costly to obtain and often only available in small number, behavioral\ncloning alone is typically insufﬁcient for agents to learn good policies in complex environments\nand has to be complemented by a method that enables the learner to build on the cloned behavior\nby interacting with the environment. A particularly successful and popular class of algorithms\nused for incorporating behavioral policies into online training is KL-regularized reinforcement\nlearning [10, 37, 43, 48].\n2.2\nKL-Regularized Objectives in Reinforcement Learning\nKL-regularized reinforcement learning modiﬁes the standard reinforcement learning objective by\naugmenting the return with a negative KL divergence term from the learned policy π to a reference\npolicy π0, given a temperature parameter α. The resulting discounted return from time step t ∈N0 is\nthen given by\n˜R(τt) =\n∞\nX\nk=t\nγk\u0002\nr(sk, ak) −αDKL(π(· | sk) ∥π0(· | sk))\n\u0003\n(1)\nand the reinforcement learning objective becomes ˜Jπ(τ0) = Eρπ(τ0)[ ˜R(τ0)]. When the reference\npolicy π0 is given by a uniform distribution, we recover the entropy-regularized reinforcement\nlearning objective used in Soft Actor–Critic (SAC) [13] up to an additive constant.\nUnder a uniform reference policy π0, the resulting objective encourages exploration, while also\nchoosing high-reward actions. In contrast, when π0 is non-uniform, the agent is discouraged to\nexplore areas of the state space S where the variance of π0(· | s) is low (i.e., more certain) and\nencouraged to explore areas of the state space where the variance of π0(· | s) is high. The KL-\nregularized reinforcement learning objective can be optimized via policy–gradient and actor–critic\nalgorithms.\n2.3\nKL-Regularized Actor–Critic\nAn optimal policy π that maximizes the expected KL-augmented discounted return ˜Jπ can be learned\nby directly optimizing the policy gradient ∇π ˜Jπ. However, this policy gradient estimator exhibits\nhigh variance, which can lead to unstable learning. Actor–critic algorithms [7, 17, 32, 38] attempt to\nreduce this variance by making use of the state value function V π(st) = Eρπ(τt)[ ˜R(τt) | st] or the\nstate–action value function Qπ(st, at) = Eρπ(τt)[ ˜R(τt) | st, at] to stabilize training.\nGiven a reference policy π0(at | st), the state value function can be shown to satisfy the modiﬁed\nBellman equation\nV π(st) ˙= Eat∼π(·|st)[Qπ(st, at)] −αDKL\n\u0000π(· | st) || π0(· | st)\n\u0001\nwith a recursively deﬁned Q-function\nQπ(st, at) ˙= r(st, at) + γ Est+1∼p(·|st,at)[V π(st+1)].\nInstead of directly optimizing the objective function ˜Jπ via the policy gradient, actor–critic methods\nalternate between policy evaluation and policy improvement [7, 13]:\nPolicy Evaluation. During the policy evaluation step, Qπ\nθ (s, a), parameterized by parameters θ, is\ntrained by minimizing the Bellman residual\nJQ(θ) ˙= E(st,at)∼D\nh\n(Qθ(st, at) −(r(st, at) + γEst+1∼p(·|st,at)[V¯θ(st+1)]))2i\n,\n(2)\nwhere D is a replay buffer and ¯θ is a stabilizing moving average of parameters.\nPolicy Improvement. In the policy improvement step, the policy πφ, parameterized by parameters\nφ, is updated towards the exponential of the KL-augmented Q-function,\nJπ(φ) ˙= Est∼D [αDKL(πφ(· | st) ∥π0(· | st))] −Est∼D\n\u0002\nEat∼πφ(·|st) [Qθ(st, at)]\n\u0003\n,\n(3)\nwith states sampled from a replay buffer D and actions sampled from the parameterized online policy\nπφ. The following sections will focus on the policy improvement objective and how certain types of\nreferences policies can lead to pathologies when optimizing Jπ(φ) with respect to φ.\n3\n3\nIdentifying the Pathology\nIn this section, we investigate the effect of KL-regularization on the training dynamics. To do\nso, we ﬁrst consider the properties of the KL divergence to identify a potential failure mode for\nKL-regularized reinforcement learning. Next, we consider parametric Gaussian behavioral reference\npolicies commonly used in practice for continuous control tasks [13, 51] and show that for Gaussian\nbehavioral reference policies with small predictive variance, the policy improvement objective suffers\nfrom exploding gradients with respect to the policy parameters φ. We conﬁrm that this failure occurs\nempirically and demonstrate that it results in slow, unstable, and suboptimal online learning. Lastly,\nwe show that various regularization techniques used for estimating behavioral policies are unable to\nprevent this failure and also lead to suboptimal online policies.\n3.1\nWhen Are KL-Regularized Reinforcement Learning Objectives Meaningful?\nWe start by considering the properties of the KL divergence and discuss how these properties can lead\nto potential failure modes in KL-regularized objectives. A well-known property of KL-regularized\nobjectives in the variational inference literature is the occurrence of singularities when the support of\none distribution is not contained in the support of the other.\nTo illustrate this problem, we consider the case of Gaussian behavioral and online policies commonly\nused in practice. Mathematically, the KL divergence between two full Gaussian distributions is\nalways ﬁnite and well-deﬁned. Hence, we might hope KL-regularized reinforcement learning with\nGaussian behavioral and online policies to be unaffected by the failure mode described above.\nHowever, the support of a Gaussian online policy πφ(· | st) will not be contained in the support of\na behavioral reference policy π0(· | st) as the predictive variance σ2\n0(st) tends to zero, and hence\nDKL(πφ(· | st) ∥π0(· | st)) →∞as σ2\n0(st) →0. In other words, as the variance of a behavioral\nreference policy tends to zero and the behavioral distribution becomes degenerate, the KL divergence\nblows up to inﬁnity [25]. While in practice, Gaussian behavioral policy would not operate in the limit\nof zero variance, the functional form of the KL divergence between (univariate) Gaussians,\nDKL(πφ(· | st) ∥π0(· | st)) ∝log σ0(st)\nσφ(st) +\nσ2\nφ(st) + (µφ(st) −µ0(st))2\n2σ2\n0(st)\n,\nimplies a continuous, quadratic increase in the magnitude of the divergence as σ0(st) decreases,\nfurther exacerbated by a large difference in predictive means, |µφ(st) −µ0(st)|.\nAs a result, for Gaussian behavioral reference policies π0(· | st) that assign very low probability\nto sets of points in sample space far away from the distribution’s mean µ0(st), computing the KL\ndivergence can result in divergence values so large to cause numerical instabilities and arithmetic\noverﬂow. Hence, even for a suitably chosen behavioral reference policy class, vanishingly small\nbehavioral reference policy predictive variances can cause the KL divergence to ‘blow up’ and cause\nnumerical issues at evaluation points far away from states in the expert demonstrations.\nOne way to address this failure mode may be to lower-bound the output of the variance network\n(e.g., by adding a small constant bias). However, placing a ﬂoor on the predictive variance of the\nbehavioral reference policy is not sufﬁcient to encourage effective learning. While it would prevent\nthe KL divergence from blowing up, it would also lead to poor gradient signals, as well-calibrated\npredictive variance estimates that increase on states far away from the expert trajectories are necessary\nto keep the KL penalty from pulling the predictive mean of the online policy towards poor behavioral\nreference policy predictive means on states off the expert trajectories. Another possible solution could\nbe to use heavy-tailed behavioral reference policies distributions, for example, Laplace distributions,\nto avoid pathological training dynamics. However, in Appendix B.3 we show that Laplace behavioral\nreference policies also suffer from pathological training dynamics, albeit less severely.\nIn the following sections, we explain how an explosion in DKL(πφ(· | st) ∥π0(· | st)) caused by small\nσ2\n0(st) affects the gradients of Jπ(φ) in KL-regularized RL and discuss of how and why σ2\n0(st) may\ntend to zero in practice.\n3.2\nExploding Gradients in KL-Regularized Reinforcement Learning Objectives\nTo understand how small predictive variances in behavioral reference policies can affect—and possibly\ndestabilize—online training in KL-regularized RL, we consider the contribution of the behavioral\n4\nreference policy’s variance to the gradient of the policy objective in Equation (3). Compared to\nentropy-regularized actor–critic methods (SAC, Haarnoja et al. [13]), which implicitly regularize\nagainst a uniform policy, the gradient estimator ˆ∇φJπ(φ) in KL-regularized RL gains an extra scaling\nterm ∇at log π0(at | st), the gradient of the prior log-density evaluated actions at ∼πφ(· | s):\nProposition 1 (Exploding Gradients in KL-Regularized RL). Let π0(· | s) be a Gaussian behavioral\nreference policy with mean µ0(st) and variance σ2\n0(st), and let πφ(· | s) be an online policy with\nreparameterization at = fφ(ϵt; st) and random vector ϵt. The gradient of the policy loss with respect\nto the online policy’s parameters φ is then given by\nˆ∇φJπ(φ) =\n\u0000α∇at log πφ(at | st) −α∇at log π0(at | st)\n−∇atQ(st, at)\n\u0001\n∇φfφ(ϵt; st) + α∇φ log πφ(at | st)\n(4)\nwith ∇at log π0(at | st) = −at−µ0(st)\nσ2\n0(st)\n. For ﬁxed |at −µ0(st)|, ∇at log π0(at|st) grows as\nO(σ−2\n0 (st)); thus,\n| ˆ∇φJπ(φ) | →∞\nas\nσ2\n0(st) →0\nwhenever\n∇φfφ(ϵt; st) ̸= 0.\nProof. See Appendix A.1.\nThis result formalizes the intuition presented in Section 3.1 that a behavioral reference policy with a\nsufﬁciently small predictive variance may cause KL-regularized reinforcement learning to suffer from\npathological training dynamics in gradient-based optimization. The smaller the behavioral reference\npolicy’s predictive variance, the more sensitive the policy objective’s gradients will be to differences\nin the means of the online and behavioral reference policies. As a result, for behavioral reference\npolicies with small predictive variance, the KL divergence will heavily penalize online policies whose\npredictive means diverge from the predictive means of the behavioral policy—even in regions of the\nstate space away from the expert trajectory where the behavioral policy’s mean prediction is poor.\n3.3\nPredictive Uncertainty Collapse Under Parametric Policies\nThe most commonly used method for estimating behavioral policies is maximum likelihood estima-\ntion (MLE) [44, 51], where we seek π0 ˙= πψ⋆with ψ⋆˙= arg maxψ\n\b\nE(s,a)∼D0[log πψ(a | s)]\n\t\nfor a parametric behavioral policy πψ.\nIn practice, πψ is often assumed to be Gaussian,\nπψ(· | s) = N(µψ(s), σ2\nψ(s)), with µψ(s) and σ2\nψ(s) parameterized by a neural network.\nWhile maximizing the likelihood of the expert trajectories under the behavioral policy is a sensible\nchoice for behavioral cloning, the limited capacity of the neural network parameterization can\nproduce unwanted behaviors in the resulting policy. The maximum likelihood objective ensures that\nthe behavioral policy’s predictive mean reﬂects the expert’s actions and the predictive variance the\n(aleatoric) uncertainty inherent in the expert trajectories.\nHowever, the maximum likelihood objective encourages parametric policies to use their model\ncapacity toward ﬁtting the expert demonstrations and reﬂecting the aleatoric uncertainty in the data.\nAs a result, for states off the expert trajectories, the policy can become degenerate and collapse to\npoint predictions instead of providing meaningful predictive variance estimates that reﬂect that the\nbehavioral policy ought to be highly uncertain about its predictions in previously unseen regions\nof the state space. Similar behaviors are well-known in parametric probabilistic models and well-\ndocumented in the approximate Bayesian inference literature [33, 39].\n0\n2\n4\n6\n8\n10\nEpochs\n10−3\n10−1\nσ2\nψ(s)\nValidation Variance\nValidation Log-Likelihood\n0\n2\nlog πψ( ¯A | ¯S)\nFigure 2: Collapse in the predictive variance (in blue)\nof a Gaussian behavioral policy parameterized by a neu-\nral network when training via maximum likelihood es-\ntimation. Lines and shaded regions denote means and\nstandard deviations over ﬁve random seeds, respectively.\nFigure 1 demonstrates the collapse in predic-\ntive variance under maximum likelihood estima-\ntion in a low-dimensional representation of the\n“door-binary-v0” dexterous hand manipulation\nenvironment. It shows that while the predictive\nvariance is small close to the expert trajectories\n(depicted as black lines), it rapidly decreases\nfurther away from them. Examples of variance\ncollapse in other environments are presented\nin Appendix B.6. Figure 2 shows that the predic-\ntive variance off the expert trajectories consis-\ntently decreases during training. As shown in Proposition 1, such a collapse in predictive variance can\n5\n0\n2000\n4000\nAverage Returns\nHalfCheetah-v2\n2000\n4000\nAnt-v2\n0\n2000\n4000\nWalker2d-v2\n100\n101\n102\nDKL(π || π0)\n101\n102\n101\n102\n0K\n25K\n50K\n75K\n100K\n125K\n150K\nTimesteps\n10−3\n10−2\nE[|∇φJπ(φ)|]\n0K\n10K\n20K\n30K\n40K\n50K\nTimesteps\n10−2\n0K\n10K\n20K\n30K\n40K\n50K\nTimesteps\n10−2\n10−1\nσ2 = 1 × 10−3\nσ2 = 5 × 10−3\nσ2 = 1 × 10−2\nFigure 3: Ablation study showing the effect of predictive variance collapse on the performance of KL-regularized\nRL on MuJoCo environments. The plots show the average return of the learned policy, the magnitude of the\nKL penalty, and the magnitude of the average absolute gradients of the policy loss during online training. The\nlighter the shading, the lower the behavioral policy’s predictive variance.\nresult in pathological training dynamics in KL-regularized online learning—steering the online policy\ntowards suboptimal trajectories in regions of the state space far away from the expert demonstrations\nand deteriorating performance.\nEffect of regularization on uncertainty collapse. To prevent a collapse in the behavioral policy’s\npredictive variance, prior work proposed adding entropy or Tikhonov regularization to the MLE\nobjective [51]. However, doing so does not succeed in preventing a collapse in predictive variance off\nthe expert demonstration trajectories, as we show in Appendix A.3. Deep ensembles [20], whose\npredictive mean and variance are computed from the predictive means and variances of multiple\nGaussian neural networks, are a widely used method for uncertainty quantiﬁcation in regression\nsettings. However, model ensembling can be costly and unreliable, as it requires training multiple\nneural networks from scratch and does not guarantee well-calibrated uncertainty estimates [39, 49].\nWe provide visualizations in Appendix B.5 which show that ensembling multiple neural network\npolicies does not fully prevent a collapse in predictive variance.\n3.4\nEmpirical Conﬁrmation of Uncertainty Collapse\nTo conﬁrm Proposition 1 empirically and assess the effect of the collapse in predictive variance on\nthe performance of KL-regularized RL, we perform an ablation study where we ﬁx the predictive\nmean function of a behavioral policy to a mean function that attains 60% of the optimal performance\nand vary the magnitude of the policy’s predictive variance. Speciﬁcally, we set the behavioral\npolicy’s predictive variance to different constant values in the set {1 × 10−3, 5 × 10−3, 1 × 10−2}\n(following a similar implementation in Nair et al. [27]).3 The results of this experiment are shown\nin Figure 3, which shows the average returns, the KL divergence, and the average absolute gradients\nof the policy loss over training. The plots conﬁrm that as the predictive variance of the ofﬂine\nbehavioral policy tends to zero, the KL terms and average policy gradient magnitude explode as\nimplied by Proposition 1, leading to unstable training and a collapse or dampening in average returns.\nIn other words, even for behavioral policies with accurate predictive means, smaller predictive vari-\nances slow down or even entirely prevent learning good behavioral policies. This observation conﬁrms\nthat the pathology identiﬁed in Proposition 1 occurs in practice and that it can have a signiﬁcant\nimpact on KL-regularized RL from expert demonstrations, calling into question the usefulness of KL\nregularization as a means for accelerating and improving online training. In Appendix B.1, we show\nthat an analogous relationship exists for the gradients of the Q-function loss.\n3We attempted to use smaller values, but the gradients grew too large and caused arithmetic overﬂow.\n6\n4\nFixing the Pathology\nIn order to address the collapse in predictive uncertainty for behavioral policies parameterized by a\nneural network trained via MLE, we specify a non-parametric behavioral policy whose predictive\nvariance is guaranteed not to collapse about previously unseen states. Noting that KL-regularized RL\nwith a behavioral policy can be viewed as approximate Bayesian inference with an empirical prior\npolicy [13, 21, 40], we propose Non-Parametric Prior Actor–Critic (N-PPAC), an off-policy temporal\ndifference algorithm for improved, accelerated, and stable online learning with behavioral policies.\n4.1\nNon-Parametric Gaussian Processes Behavioral Policies\nGaussian processes (GPs) [36] are models over functions deﬁned by a mean m(·) and covariance\nfunction k(·, ·). When deﬁned in terms of a non-parametric covariance function, that is, a covariance\nfunction constructed from inﬁnitely many basis functions, we obtain a non-degenerate GP, which has\nsufﬁcient capacity to prevent a collapse in predictive uncertainty away from the training data. Unlike\nparametric models, whose capacity is limited by their parameterization, a non-parametric model’s\ncapacity increases with the amount of training data.\nConsidering a non-parametric GP behavioral policy, π0(· | s), with\nA | s ∼π0(· | s) = GP\n\u0000m(s), k(s, s′)\n\u0001\n,\n(5)\nwe can obtain a non-degenerate posterior distribution over actions conditioned on the ofﬂine data\nD0 = {¯S, ¯A} with actions sampled according to the\nA | s, D0 ∼π0(· | s, D0) = GP\n\u0000µ0(s), Σ0(s, s′)\n\u0001\n,\n(6)\nwith\nµ(s)=m(s) + k(s, ¯S)k(¯S, ¯S)−1( ¯A −m( ¯A)) and Σ(s, s′)=k(s, s′) + k(s, ¯S)k(¯S, ¯S)−1k(¯S, s′).\nTo obtain this posterior distribution, we perform exact Bayesian inference, which naively scales as\nO(N 3) in the number of training points N, but Wang et al. [50] show that exact inference in GP\nregression can be scaled to N > 1, 000, 000. Since expert demonstrations usually contain less than\n100k datapoints, non-parametric GP behavioral policies are applicable to a wide array of real-world\ntasks. For an empirical evaluation of the time complexity of using a GP prior, see Section 5.5.\nFigure 1 conﬁrms that the non-parametric GP’s predictive variance is well-calibrated: It is small in\nmagnitude in regions of the state space near the expert trajectories and large in magnitude in other\nregions of the state space. While actor–critic algorithms like SAC implicitly use a uniform prior to\nexplore the state space, using a behavioral policy with a well-calibrated predictive variance has the\nbeneﬁt that in regions of the state space close to the expert demonstrations the online policy learns to\nmatch the expert, while elsewhere the predictive variance increases and encourages exploration.\nAlgorithmic details. In our experiments, we use a KL-regularized objective with a standard actor–\ncritic implementation and Double DQN [14]. Pseudocode is provided in (Appendix C.1).\n5\nEmpirical Evaluation\nWe carry out a comparative empirical evaluation of our proposed approach vis-à-vis related methods\nthat integrate ofﬂine data into online training. We provide a detailed description of the algorithms we\ncompare against in Appendix A.4. We perform experiments on the MuJoCo benchmark suite and the\nsubstantially more challenging dexterous hand manipulation suite with sparse rewards.\nWe show that KL-regularized RL with a non-parametric behavioral reference policy can rapidly learn\nto solve difﬁcult high-dimensional continuous control problems given only a small set of expert\ndemonstrations and (often signiﬁcantly) outperforms state-of-the-art methods, including ones that\nuse ofﬂine reward information—which our approach does not require. Furthermore, we demonstrate\nthat the GP behavioral policy’s predictive variance is crucial for KL-regularized objectives to learn\ngood online policies from expert demonstrations. Finally, we perform ablation studies that illustrate\nthat non-parametric GP behavioral reference policies also outperform parametric behavioral reference\npolicies with improved uncertainty quantiﬁcation, such as deep ensembles and Bayesian neural\n7\n0\n5000\n10000\nHalfCheetah-v2\n−2500\n0\n2500\n5000\nAnt-v2\n0\n2000\n4000\n6000\nWalker2d-v2\n100K\n200K\n300K\n400K\n500K\nTimesteps\n0\n5000\n10000\n100K\n200K\n300K\n400K\n500K\nTimesteps\n−2500\n0\n2500\n5000\n100K\n200K\n300K\n400K\n500K\nTimesteps\n0\n2000\n4000\n6000\nN-PPAC (Ours)\nBRAC\nAWAC\nAWR\nABM\nSACfD\nSAC + BC\nBEAR\nDAPG\nFigure 4: Comparison of N-PPAC (ours) vs. previous baselines on standard MuJoCo benchmark tasks. Top:\nKL-based methods (dashed lines), Bottom: Non-KL-based methods (dash-dotted lines). Both top and bottom\nplots include N-PPAC (blue). BRAC uses the same actor–critic algorithm as N-PPAC, but uses a parametric\nbehavioral policy, and results in slower learning and worse ﬁnal performance.\nnetworks (BNNs) with Monte Carlo dropout, and that the difference between non-parametric and\nparametric models is exacerbated the fewer expert demonstrations are available. We use the expert\ndata from Nair et al. [27], every experiment uses six random seeds, and we use a ﬁxed KL-temperature\nfor each environment class. For further implementation details, see Appendix C.2.\n5.1\nEnvironments\nMuJoCo locomotion tasks.\nWe evaluate N-PPAC on three representative tasks:\n“Ant-v2”,\n“HalfCheetah-v2”, and “Walker2d-v2”. For each task, we use 15 demonstration trajectories col-\nlected by a pre-trained expert, each containing 1,000 steps. The behavioral policy is speciﬁed as the\nposterior distribution of a GP with a squared exponential kernel, which is well-suited for modeling\nsmooth functions.\nDexterous hand manipulation tasks. Real-world robot learning is a setting where human demon-\nstration data is readily available, and many deep RL approaches fail to learn efﬁciently. We study\nthis setting in a suite of challenging dexterous manipulation tasks [35] using a 28-DoF ﬁve-ﬁngered\nsimulated ADROIT hand. The tasks simulate challenges common to real-world settings with high-\ndimensional action spaces, complex physics, and a large number of intermittent contact forces. We\nconsider two tasks in particular: in-hand rotation of a pen to match a target and opening a door by un-\nlatching and pulling a handle. We use binary rewards for task completion, which is signiﬁcantly more\nchallenging than the original setting considered in Rajeswaran et al. [35]. 25 expert demonstrations\nwere provided for each task, each consisting of 200 environment steps which are not fully optimal\nbut do successfully solve the task. The behavioral policy is speciﬁed as the posterior distribution of a\nGP with a Matérn kernel, which is more suitable for modeling non-smooth data.\n5.2\nResults\nOn MuJoCo environments, KL-regularized RL with a non-parametric behavioral policy consistently\noutperforms all related methods across all three tasks, successfully accelerating learning from ofﬂine\ndata, as shown in Figure 4. Most notably, it outperforms methods such as AWAC [27]—the previous\nstate-of-the-art—which attempts to eschew the problem of learning behavioral policies but instead\nuses an implicit constraint. Our approach, N-PPAC, exhibits an increase in stability and higher returns\ncompared to comparable methods such as ABM and BRAC that explicitly regularize the online policy\nagainst a parametric behavioral policy and plateau at suboptimal performance levels as they are being\nforced to copy poor actions from the behavioral policy away from the expert data. In contrast, using a\nnon-parametric behavioral policy allows us to avoid such undesirable behavior.\nOn dexterous hand manipulation environments, KL-regularized RL with a non-parametric behavioral\npolicy performs on par or outperforms all related methods on both tasks, as shown in Figure 5. Most\nnotably, on the door opening task, it achieves a stable success rate of 90% within only 100,000\nenvironment interactions For comparison, AWAC requires 4× as many environment interactions to\n8\n0.00\n0.25\n0.50\n0.75\n1.00\npen-binary-v0\n0.00\n0.25\n0.50\n0.75\n1.00\ndoor-binary-v0\n100K\n200K\n300K\n400K\nTimesteps\n0.00\n0.25\n0.50\n0.75\n1.00\n100K\n200K\n300K\n400K\nTimesteps\n0.00\n0.25\n0.50\n0.75\n1.00\nN-PPAC (Ours)\nSACfD\nBRAC\nSAC + BC\nAWAC\nBEAR\nAWR\nDAPG\nABM\nFigure 5: Left & Center: Comparison of N-PPAC (ours) vs. previous baselines on dexterous hand manipulation\ntasks. Top: KL-based methods (dashes), Bottom: Non-KL-based methods (dots and dashes). Both top and\nbottom plots include N-PPAC (blue). Right: The pen-binary-v0 (top) and door-binary-v0 (bottom) environments.\nachieve the same performance and is signiﬁcantly less stable, while most other methods fail to learn\nany meaningful behaviors.\nAlternative divergence metrics underperform KL-regularization. KL-regularized RL with a\nnon-parametric behavioral policy consistently outperforms methods that use alternative divergence\nmetrics, as shown in the bottom plots of Figures 4 and 5.\n5.3\nCan the Pathology Be Fixed by Improved Parametric Uncertainty Quantiﬁcation?\n0K\n50K\n100K\n150K\n200K\nTimesteps\n10−2\n10−1\n100\ndoor-binary-v0 Ablations\nVariance Type\nNonparametric GP (Exact Posterior)\nParametric Gaussian NN (Ensemble)\nParametric Gaussian NN (MC-D BNN)\nParametric Gaussian NN (MLE + Entropy)\nParametric Gaussian NN (MLE)\nFigure 6: Post-online training success rates with\ndifferent behavioral policy variance functions.\nTo assess whether the success of non-parametric be-\nhavioral reference policies is due to their predictive\nvariance estimates—as suggested by Proposition 1—\nor due to better generalization from their predictive\nmeans, we perform an ablation study on the predictive\nvariance of the behavioral policy. To isolate the effect\nof the predictive variance on optimization, we perform\nonline training using behavioral policies with differ-\nent predictive variance functions (parametric and non-\nparametric) and identical mean functions, which we set\nto be the predictive mean of the GP posterior (which\nachieves a success rate of ~80%). If the pathology iden-\ntiﬁed in Proposition 1 can be remedied by commonly\nused parametric uncertainty quantiﬁcation methods, we\nwould expect the parametric and non-parametric behav-\nioral policy variance functions to result in similar on-\nline policy success rates. We consider the challenging\n“door-binary-v0” environment for this ablation study.\nParametric uncertainty quantiﬁcation is insufﬁcient. Figure 5 shows that parametric variance\nfunctions result in online policies that only achieve success rates of up to 20% and eventually\ndeteriorate, whereas the non-parametric variance yields an online policy that achieves a success rate\nof nearly 100%. This ﬁnding shows that commonly used uncertainty quantiﬁcation methods, such\nas deep ensembles or BNNs with Monte Carlo dropout, do not generate sufﬁciently well-calibrated\nuncertainty estimates to remedy the pathology, and better methods may be needed [9, 39, 41].\nLower-bounding the predictive variance does not remedy the pathology. The predictive variance\nof all MLE-based and ensemble behavioral reference policies in all experiments are bounded away\nfrom zero at a minimum value of ≈10−2. Hence, setting a ﬂoor on the variance is not sufﬁcient to\nprevent pathological training dynamics. This result further demonstrates the importance of accurate\npredictive variance estimation in allowing the online policy to match expert actions in regions of the\nstate space with low behavioral policy predictive variance and explore elsewhere.\n9\n5.4\nCan a Single Expert Demonstration Be Sufﬁcient to Accelerate Online Training?\n0K\n100K\n200K\n300K\n400K\n500K\nTimesteps\n0\n5000\n10000\n15 Expert Demonstrations\n0K\n100K\n200K\n300K\n400K\n500K\nTimesteps\n0\n5000\n10000\nOne Expert Demonstration\nN-PPAC (Ours)\nEnsemble Behavioral Policy\nMC-Dropout Behavioral Policy\nNo Behavioral Policy (SAC)\nFigure 7: Returns during online training with different behav-\nioral policies and varying amounts of expert demonstration data on\n“HalfCheetah-v2”.\nTo assess the usefulness of non-\nparametric behavioral reference poli-\ncies in settings where only few ex-\npert demonstrations are available, we\ninvestigate whether the difference in\nperformance between online policies\ntrained with non-parametric and para-\nmetric behavioral reference policies,\nrespectively, is exacerbated the fewer\nexpert demonstrations are available.\nTo answer this question, we consider\nthe “HalfCheetah-v2” environment\nand compare online policies trained\nwith different behavioral reference\npolicies—non-parametric GPs, deep ensembles, and BNNs with Monte Carlo dropout—estimated\neither from 15 expert demonstrations (i.e., 15 state–action trajectories, containing 15,000 samples) or\nfrom a single expert demonstration (i.e., a single state–action trajectory, containing 1,000 samples).\nA single expert demonstration is sufﬁcient for non-parametric behavioral reference policies.\nFigure 7 shows the returns for online policies trained with behavioral reference policies estimated\nfrom the full dataset (top plot) and from only a single expert state–action trajectory (bottom plot).\nOn the full dataset, we ﬁnd that all three methods are competitive and improve on the prior state-\nof-the-art but that the GP behavioral policy leads to the highest return. Remarkably, non-parametric\nGP behavioral policies perform just as well with only a single expert demonstration as with all\n15 (i.e., with 1,000 data points, instead of 15,000 data points). These results further emphasizes\nthe usefulness of non-parametric behavioral policies when accelerating online training with expert\ndemonstrations—even when only very few expert demonstrations are available.\n5.5\nAre Non-Parametric GP Behavioral Reference Policies Too Computationally Expensive?\nTable 1 presents the time complexity of KL-regularized RL under non-parametric GP and parametric\nneural network behavioral reference policies, as measured by the average time elapsed per epoch on\nthe “door-binary-v0” and “HalfCheetah-v2” environments. One epoch of online training on “door-\nbinary-v0” and “HalfCheetah-v2” requires computing the KL divergence over 1,000 mini-batches\nof size 256 and 1,024, respectively. The time complexity of evaluating the log-density of a GP\nbehavioral reference policy—needed for computing gradients of the KL divergence during online\ntraining—scales quadratically in the number of training data points and linearly in the dimensionality\nof the state and action space, respectively. As can be seen in Table 1, non-parametric GP behavioral\nreference policies only lead to a modest increase in the time needed to complete one epoch of training\nwhile resulting in signiﬁcantly improved performance as shown in Figures 4 and 5.\nTable 1: Time per epoch under different behavioral reference policies for expert demonstration data of varying\nsize computed on a GeForce RTX 3080 GPU. The ﬁrst and second value in each entry of the table give the time\nrequired when using a single parametric neural network and a GP behavioral reference policy, respectively.\nDataset\n1,000 Data Points\n5,000 Data Points\n15,000 Data Points\nHalfCheetah-v2\n12.00s / 16.06s\n11.59s / 18.31s\n12.00s / 46.54s\ndoor-binary-v0\n19.62s / 23.78s\n19.62s / 33.62s\n-\n6\nConclusion\nWe identiﬁed a previously unrecognized pathology in KL-regularized RL from expert demonstrations\nand showed that this pathology can signiﬁcantly impede and even entirely prevent online learning. To\nremedy the pathology, we proposed the use of non-parametric behavioral reference policies, which\nwe showed can signiﬁcantly accelerate and improve online learning and yield online policies that\n(often signiﬁcantly) outperform current state-of-the-art methods on challenging continuous control\ntasks. We hope that this work will encourage further research into better model classes for deep\nreinforcement learning algorithms, including and especially for reinforcement from image inputs.\n10\nAcknowledgments and Disclosure of Funding\nWe thank Ashvin Nair for sharing his code and results, as well as for providing helpful insights about\nthe dexterous hand manipulation suite. We also thank Clare Lyle, Charline Le Lan, and Angelos Filos\nfor detailed feedback on an early draft of this paper, Avi Singh for early discussions about behavioral\ncloning in entropy-regularized RL, and Tim Pearce for a useful discussion on the role of good\nmodels in RL. TGJR and CL are funded by the Engineering and Physical Sciences Research Council\n(EPSRC). TGJR is also funded by the Rhodes Trust and by a Qualcomm Innovation Fellowship. We\ngratefully acknowledge donations of computing resources by the Alan Turing Institute.\nReferences\n[1] Michael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelli-\ngence 15, pages 103–129, 1995.\n[2] Philip J Ball, Cong Lu, Jack Parker-Holder, and Stephen Roberts. Augmented world models\nfacilitate zero-shot dynamics generalization from a single ofﬂine environment. In Marina\nMeila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine\nLearning, volume 139 of Proceedings of Machine Learning Research, pages 619–629. PMLR,\n18–24 Jul 2021.\n[3] Abdeslam Boularias, Jens Kober, and Jan Peters. Relative entropy inverse reinforcement\nlearning. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence\nand Statistics, pages 182–189. JMLR Workshop and Conference Proceedings, 2011.\n[4] Ivan Bratko, Tanja Urbancic, and Claude Sammut. Behavioural cloning: phenomena, results\nand problems. IFAC Proceedings Volumes, 28(21):143–149, 1995.\n[5] Tim Brys, Anna Harutyunyan, Halit Bener Suay, Sonia Chernova, Matthew E Taylor, and\nAnn Nowé. Reinforcement learning from demonstrations through shaping. In Twenty-fourth\nInternational Joint Conference on Artiﬁcial Intelligence, 2015.\n[6] Catherine Cang, Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin. Behavioral priors\nand dynamics models: Improving performance and domain transfer in ofﬂine RL, 2021.\n[7] Thomas Degris, Martha White, and Richard S. Sutton. Off-policy actor-critic. In Proceedings of\nthe 29th International Coference on International Conference on Machine Learning, ICML’12,\npage 179–186, Madison, WI, USA, 2012. Omnipress.\n[8] Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforce-\nment learning. arXiv preprint arXiv:1904.12901, 2019.\n[9] Sebastian Farquhar, Michael A. Osborne, and Yarin Gal. Radial bayesian neural networks:\nBeyond discrete support in large-scale bayesian deep learning. In Silvia Chiappa and Roberto\nCalandra, editors, Proceedings of the Twenty Third International Conference on Artiﬁcial\nIntelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages\n1352–1362. PMLR, 26–28 Aug 2020.\n[10] Alexandre Galashov, Siddhant M. Jayakumar, Leonard Hasenclever, Dhruva Tirumala, Jonathan\nSchwarz, Guillaume Desjardins, Wojciech M. Czarnecki, Yee Whye Teh, Razvan Pascanu, and\nNicolas Heess. Information asymmetry in kl-regularized RL. In 7th International Conference\non Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.\n[11] Yang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. Reinforcement\nlearning from imperfect demonstrations. arXiv preprint arXiv:1802.05313, 2018.\n[12] CW Groetsch. The theory of Tikhonov regularization for Fredholm equations. Boston Pitman\nPublication, 1984.\n[13] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan,\nVikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic\nalgorithms and applications, 2019.\n11\n[14] Hado V. Hasselt. Double Q-learning. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S.\nZemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages\n2613–2621, 2010.\n[15] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A\nsurvey. Journal of artiﬁcial intelligence research, 4:237–285, 1996.\n[16] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. MOReL:\nModel-based ofﬂine reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.\nBalcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,\npages 21810–21823. Curran Associates, Inc., 2020.\n[17] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information\nprocessing systems, pages 1008–1014, 2000.\n[18] George Konidaris, Scott Kuindersma, Roderic Grupen, and Andrew Barto. Robot learning from\ndemonstration by constructing skill trees. The International Journal of Robotics Research, 31\n(3):360–375, 2012.\n[19] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy\nQ-learning via bootstrapping error reduction. In Advances in Neural Information Processing\nSystems 32, pages 11784–11794, 2019.\n[20] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable pre-\ndictive uncertainty estimation using deep ensembles. In I. Guyon, U. V. Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Informa-\ntion Processing Systems 30, pages 6402–6413, 2017.\n[21] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and\nreview, 2018.\n[22] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval\nTassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning.\nIn ICLR (Poster), 2016.\n[23] Cong Lu, Philip J. Ball, Jack Parker-Holder, Michael A. Osborne, and Stephen J. Roberts.\nRevisiting design choices in model-based ofﬂine reinforcement learning, 2021.\n[24] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR,\nabs/1312.5602, 2013.\n[25] Kevin P. Murphy. Machine learning: A Probabilistic Perspective. MIT Press, 2013.\n[26] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming exploration\nin reinforcement learning with demonstrations. In 2018 IEEE International Conference on\nRobotics and Automation (ICRA), pages 6292–6299, 2018.\n[27] Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online rein-\nforcement learning with ofﬂine datasets, 2020.\n[28] Nicolás Navarro-Guerrero, Cornelius Weber, Pascal Schroeter, and Stefan Wermter. Real-world\nreinforcement learning for autonomous humanoid robot docking. Robotics and Autonomous\nSystems, 60(11):1400–1407, 2012.\n[29] Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. In\nProceedings of the Seventeenth International Conference on Machine Learning, ICML ’00,\npage 663–670, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.\n[30] Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Krzysztof Choromanski, Anna Choro-\nmanska, and Michael Jordan. Learning to score behaviors for guided policy optimization. In\nHal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference\non Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages\n7445–7454. PMLR, 13–18 Jul 2020.\n12\n[31] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:\nSimple and scalable off-policy reinforcement learning, 2019.\n[32] Jan Peters, Sethu Vijayakumar, and Stefan Schaal. Natural actor-critic. In European Conference\non Machine Learning, pages 280–291. Springer, 2005.\n[33] Joaquin Quiñonero Candela and Carl Edward Rasmussen. A unifying view of sparse approxi-\nmate Gaussian process regression. J. Mach. Learn. Res., 6:1939–1959, December 2005. ISSN\n1532-4435.\n[34] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel\nTodorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement\nlearning and demonstrations. In Proceedings of Robotics: Science and Systems (RSS), 2018.\n[35] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel\nTodorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement\nlearning and demonstrations, 2018.\n[36] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine\nLearning (Adaptive Computation and Machine Learning). The MIT Press, 2005.\n[37] Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and\nreinforcement learning by approximate inference. Proceedings of Robotics: Science and Systems\nVIII, 2012.\n[38] Michael T Rosenstein, Andrew G Barto, Jennie Si, Andy Barto, and Warren Powell. Supervised\nactor-critic reinforcement learning. Learning and Approximate Dynamic Programming: Scaling\nUp to the Real World, pages 359–380, 2004.\n[39] Tim G. J. Rudner, Zonghao Chen, and Yarin Gal. Rethinking function-space variational\ninference in Bayesian neural networks. In Third Symposium on Advances in Approximate\nBayesian Inference, 2021.\n[40] Tim G. J. Rudner, Vitchyr H. Pong, Rowan Thomas McAllister, Yarin Gal, and Sergey\nLevine. Outcome-driven reinforcement learning via variational inference. In A. Beygelz-\nimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information\nProcessing Systems, 2021. URL https://openreview.net/forum?id=4bzanicqvy8.\n[41] Tim G. J. Rudner, Freddie Bickford Smith, Qixuan Feng, Yee Whye Teh, and Yarin Gal.\nContinual learning via function-space variational inference. In ICML Workshop on Theory and\nFoundations of Continual Learning, 2021.\n[42] Stefan Schaal et al. Learning from demonstration. Advances in neural information processing\nsystems, pages 1040–1046, 1997.\n[43] John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft\nQ-learning. arXiv preprint arXiv:1704.06440, 2017.\n[44] Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael\nNeunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing\nwhat worked: Behavior modelling priors for ofﬂine reinforcement learning. In International\nConference on Learning Representations, 2020.\n[45] Alex Smola, Arthur Gretton, Le Song, and Bernhard Schölkopf. A hilbert space embedding for\ndistributions. In Marcus Hutter, Rocco A. Servedio, and Eiji Takimoto, editors, Algorithmic\nLearning Theory, pages 13–31, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg.\n[46] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT\nPress, second edition, 2018.\n[47] Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM,\n38(3):58–68, 1995.\n13\n[48] Emanuel Todorov. Linearly-solvable markov decision problems. In B. Schölkopf, J. Platt, and\nT. Hoffman, editors, Advances in Neural Information Processing Systems, volume 19, pages\n1369–1376. MIT Press, 2007.\n[49] Joost van Amersfoort, Lewis Smith, Andrew Jesson, Oscar Key, and Yarin Gal. Improving\ndeterministic uncertainty estimation in deep learning for classiﬁcation and regression, 2021.\n[50] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kilian Q Weinberger, and Andrew Gor-\ndon Wilson. Exact Gaussian processes on a million data points. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d’Alché Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems, volume 32, 2019.\n[51] Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement\nlearning. arXiv preprint arXiv:1911.11361, 2019.\n[52] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea\nFinn, and Tengyu Ma. Mopo: Model-based ofﬂine policy optimization. In H. Larochelle,\nM. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information\nProcessing Systems, volume 33, pages 14129–14142. Curran Associates, Inc., 2020.\n[53] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea\nFinn. COMBO: Conservative ofﬂine model-based policy optimization, 2021.\n14\nSupplementary Material\nTable of Contents\nA Derivations and Further Technical Details\n15\nA.1\nProof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nA.2\nLaplace Parametric Behavioral Reference Policy . . . . . . . . . . . . . . . . . . .\n16\nA.3\nRegularized Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . . .\n16\nA.4\nComparison to Prior Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nB\nFurther Experimental Results\n18\nB.1\nExploding Q-function Gradients . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nB.2\nAblation Study on the Effect of KL Divergence Temperature Tuning . . . . . . . .\n18\nB.3\nAblation Study: Performance under a Laplace Parametric Behavioral Reference Policy 19\nB.4\nVisualizations of Regularized Maximum Likelihood Parametric Behavioral Policies\n20\nB.5\nVisualizations of Ensemble Maximum Likelihood Parametric Behavioral Policies .\n21\nB.6\nParametric vs. Non-Parametric Predictive Variance Visualizations Across Environments 22\nB.7\nVisual Comparison of Parametric vs. Non-Parametric Behavioral Policy Trajectories\n23\nC Further Implementation Details\n23\nC.1\nAlgorithmic Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nC.2\nHyperparameters\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nAppendix A\nDerivations and Further Technical Details\nA.1\nProof of Proposition 1\nProposition 1 (Exploding Gradients in KL-Regularized RL). Let π0(· | s) be a Gaussian behavioral\nreference policy with mean µ0(s) and variance σ2\n0(s), and let π(· | s) be an online policy with\nreparameterization at = fφ(ϵt; st) and random vector ϵt. The gradient of the policy loss with respect\nto the online policy’s parameters φ is then given by\nˆ∇φJπ(φ) =\n\u0000α∇at log πφ(at | st) −α∇at log π0(at | st)\n−∇atQ(st, at)\n\u0001\n∇φfφ(ϵt; st) + α∇φ log πφ(at | st)\n(A.1)\nwith\n∇at log π0(at | st) = −at −µ0(st)\nσ2\n0(st)\n.\n(A.2)\nFor ﬁxed |at −µ0(st)|, ∇at log π0(at | st) grows as O(σ−2\n0 (st)); thus,\n| ˆ∇φJπ(φ) | →∞\nas\nσ2\n0(st) →0,\n(A.3)\nwhen ∇φfφ(ϵt; st) ̸= 0.\nProof. The policy loss, as given in Equation (3), is:\nJπ(φ) = Est∼D\n\u0002\nDKL\n\u0000πφ(· | st) || π0(· | st)\n\u0001\u0003\n−Est∼D\n\u0002\nEat∼πφ [Qθ(st, at)]\n\u0003\n.\n(A.4)\nTo obtain a lower-variance gradient estimator, the policy is reparameterized using a neural network\ntransformation\nat = fφ(ϵt; st)\n(A.5)\n15\nwhere ϵt is an input noise vector. Following Haarnoja et al. [13], we can now rewrite Equation (A.4)\nas\nJπ(φ) = Est∼D,ϵt\n\u0002\nα\n\u0000log πφ(fφ(ϵt; st) | st) −log π0(fφ(ϵt; st) | st)\n\u0001\n−Q(st, fφ(ϵt; st))\n\u0003\n(A.6)\nwhere D is a replay buffer and πφ is deﬁned implicitly in terms of fφ. We can approximate the\ngradient of Equation (A.6) with\nˆ∇φJπ(φ) =\n\u0000α∇at log πφ(at | st) −α∇at log π0(at | st)\n−∇atQ(st, at)\n\u0001\n∇φfφ(ϵt; st) + α∇φ log πφ(at | st).\n(A.7)\nNext, consider the term ∇at log π0(at | st) for a Gaussian policy:\nlog π0(at | st) = log\n\u0012\n1\nσ0(st)\n√\n2π\n\u0013\n−1\n2\n\u0012at −µ0(st)\nσ0(st)(st)\n\u00132\n(A.8)\nThus,\n∇at log π0(at | st) = −at −µ0(st)\nσ2\n0(st)\n.\n(A.9)\nFor ﬁxed |at −µ0(st)|, ∇at log(π0(at | st)) grows as O(σ−2\n0 (st)), and so,\n| ˆ∇φJπ(φ)| →∞\nas\nσ2\n0(st) →0.\n(A.10)\nwhenever ∇φfφ(ϵt; st) ̸= 0.\nA.2\nLaplace Parametric Behavioral Reference Policy\nA Laplace behavioral reference policy may be able to mitigate some of the problems posed by Propo-\nsition 1 due to the heavy tails of the distribution. The gradient for a Laplace behavioral reference\npolicy\nπ0(at | st) ˙=\n1\n2σ0(st) exp\n\u0012\n−|at −µ0(st)|\nσ0(st)\n\u0013\n,\n(A.11)\nincreases linearly for a given distance between at and the mean µ0(st) as the scale σ0(st) tends to\nzero.\nA.3\nRegularized Maximum Likelihood Estimation\nTo address the collapse in predictive variance away from the ofﬂine dataset under MLE training seen\nin Figure 1, Wu et al. [51] in practice augment the usual MLE loss with an entropy bonus as follows:\nπ0 ˙= πψ⋆\nwith ψ⋆˙= arg max\nψ\n\b\nE(s,a)∼D[log πψ(a | s) + βH(πψ(· | s))]\n\t\n.\n(A.12)\nwhere β is temperature tuned to an entropy constraint similar to Haarnoja et al. [13]. The entropy\nbonus is estimated by sampling from the behavioral policy as\nH(πψ(· | s)) = Ea∼πψ[−log πψ(a | s)]\n(A.13)\nFigure 11 shows the predictive variances of behavioral policies trained on expert demonstrations for\nthe “door-binary-v0” environment with various entropy coefﬁcients β. Whilst entropy regularization\npartially mitigates the collapse of predictive variance away from the expert demonstrations, we\nstill observe the wrong trend similar to Figure 1 with predictive variances high near the expert\ndemonstrations and low on unseen data. The variance surface also becomes more poorly behaved,\nwith “islands” of high predictive variance appearing away from the data.\nWe may also add Tikhonov regularization [12] to the MLE objective, explicitly,\nπ0 ˙= πψ⋆\nwith ψ⋆˙= arg max\nψ\n\b\nE(s,a)∼D[log πψ(a | s) −λψ⊤ψ]\n\t\n.\n(A.14)\nwhere λ is the regularization coefﬁcient.\nFigure 12 shows the predictive variances of behavioral policies trained on expert demonstrations for\nthe “door-binary-v0” environment with varying Tikhonov regularization coefﬁcients λ. Similarly,\nTikhonov regularization does not resolve the issue with calibration of uncertainties. We also observe\nthat too high a regularization strength causes the model to underﬁt to the variances of the data.\n16\nA.4\nComparison to Prior Works\nTo assess the usefulness of KL regularization for improving the performance and sample efﬁciency\nof online learning with expert demonstrations, we compare our approach to methods that incorporate\nexpert demonstrations into online learning implicitly or explicitly via KL regularization as well as by\nmeans other than KL regularization.\nABM [44]. ABM explicitly KL-regularizes the online policy against a behavioral policy. This\nbehavioral policy can be estimated via MLE, like BRAC, or alternatively via an “advantage-weighted\nbehavioral model” where the RL algorithm is biased to choose actions that are both supported by\nthe ofﬂine data and that are good for the current task. This objective ﬁlters trajectory snippets by\nadvantage-weighting, using an n-step advantage function. We show that no carefully chosen objective\nwith additional hyperparameters is required.\nAWAC [27]. AWAC performs online ﬁne-tuning of a policy pre-trained on ofﬂine. It achieves\nstate-of-the-art results on the dexterous hand manipulation and MuJoCo continuous locomotion tasks.\nAWAC implicitly constrains the KL divergence of the online policy to be close to the behavioral\npolicy by sampling from the replay buffer, which is initially ﬁlled with the ofﬂine data. The method\nrequires additional off-policy data to be generated to saturate the replay buffer, thereby requiring\na hidden number of environment interactions that do not involve learning. Our approach does not\nrequire the ofﬂine data to be added to the replay buffer before training.\nAWR [31]. AWR approximates constrained policy search by alternating between supervised value\nfunction and policy regression steps. The objective derived is similar to AWAC but instead estimates\nthe value function of the behavioral policy which was demonstrated to be less efﬁcient than Q-\nfunction estimation via bootstrapping [27]. The method may be converted to use ofﬂine data by\nadding prior data to the replay buffer before training.\nBEAR [19]. BEAR attempts to stabilize learning from off-policy data (such as ofﬂine data) by\ntackling bootstrapping error from actions far from the training data. This is achieved by searching for\npolicies with the same support as the training distribution. This approach is too restrictive for the\nproblem considered in this paper, since only a small number of expert demonstrations is available,\nwhich requires exploration. In contrast, our approach encourages exploration away from the data by\nwider behavioral policy predictive variances. BEAR uses an alternate divergence measure to the KL\ndivergence, Maximum Mean Discrepancy [45]. Other divergences such as Wasserstein Distances [30]\nhave also been proposed for regularization in RL.\nBRAC [51]. BRAC regularizes the online policy against an ofﬂine behavioral policy as our method\ndoes. However, BRAC exhibits the pathologies we have shown by learning a poor behavioral policy\nvia MLE. To mitigate this, in practice, BRAC adds an entropy bonus to the supervised learning\nobjective which stabilizes the variance around the training set but has no guarantees away from\nthe data. We demonstrate that behavioral policy obtained via maximum likelihood estimation with\nentropy regularization exhibit a collapse in predictive uncertainty estimates way from the training\ndata, resulting in the pathology described in Proposition 1.\nDAPG [34]. DAPG incorporates ofﬂine data into policy gradients by initially pre-training with a\nbehaviorally cloned policy and then augmenting the RL loss with a supervised-learning loss. We\nsimilarly pre-train the online policy at the start to avoid noisy KLs at the beginning of training.\nHowever, training a joint loss that combines two disparate and often divergent terms can be unstable.\nSAC+BC [26]. SAC+BC represents the approach of Nair et al. [26] but uses SAC instead of\nDDPG [22] as the underlying RL algorithm. The method maintains a secondary replay buffer ﬁlled\nwith ofﬂine data that is sampled each update step, augmenting the policy loss with a supervised\nlearning loss that is ﬁltered by advantage and hindsight experience replay. Our method requires far\nfewer additional ad-hoc algorithmic design choices.\nSACfD [13]. SACfD uses the popular Soft Actor–Critic (SAC) algorithm with ofﬂine data loaded\ninto the replay buffer before online training. Our algorithm uses the same approximate policy iteration\nscheme as SAC with a modiﬁed objective. Nair et al. [27] show that including the ofﬂine data into\nthe replay buffer does not signiﬁcantly improve the training performance over the unmodiﬁed SAC\nobjective and that pre-training the online policy with ofﬂine data results in catastrophic forgetting.\nThus, a different approach is needed to integrate ofﬂine data with SAC-style algorithms.\n17\nAppendix B\nFurther Experimental Results\nB.1\nExploding Q-function Gradients\nIn Proposition 1 and Section 3.4, we showed that the policy gradient ˆ∇φJπ(φ) explodes due to the\nblow-up of the gradient of the behavioral reference policy’s log-density as the behavioral policy\npredictive variance σ0(s) tends to zero. A similar relationship holds for the Q-function gradients,\nwhich we conﬁrm empirically in Figure 8.\n0\n2000\n4000\nAverage Returns\nHalfCheetah-v2\n2000\n4000\nAnt-v2\n0\n2000\n4000\nWalker2d-v2\n100\n101\n102\nDKL(π || π0)\n101\n102\n101\n102\n0K\n25K\n50K\n75K\n100K\n125K\n150K\nTimesteps\n10−3\n10−2\nE[|∇φJπ(φ)|]\n0K\n10K\n20K\n30K\n40K\n50K\nTimesteps\n10−2\n0K\n10K\n20K\n30K\n40K\n50K\nTimesteps\n10−2\n10−1\nσ2 = 1 × 10−3\nσ2 = 5 × 10−3\nσ2 = 1 × 10−2\nFigure 8: Ablation study showing the effect of predictive variance collapse on the performance of KL-regularized\nRL on MuJoCo benchmarks. Policies shown from dark to light in order of decreasing constant predictive variance,\nsimulating training under maximum likelihood estimation. The plots show the average return of the learned\npolicy, magnitude of the KL penalty, and magnitude of the Q-function gradients during online training.\nB.2\nAblation Study on the Effect of KL Divergence Temperature Tuning\nFigure 9 shows that unlike in standard SAC [13], tuning of the KL-temperature is not necessary to\nachieve good online performance. For simplicity, we use a ﬁxed value throughout our experiments.\n0K\n100K\n200K\n300K\n400K\n500K\nTimesteps\n0\n2000\n4000\n6000\n8000\n10000\n12000\nHalfCheetah-v2\n0K\n50K\n100K\n150K\n200K\nTimesteps\n0\n1000\n2000\n3000\n4000\n5000\nAnt-v2\n0K\n100K\n200K\n300K\n400K\nTimesteps\n0\n1000\n2000\n3000\n4000\n5000\nWalker2d-v2\nFixed KL Temperature\nTuned KL Temperature\nFigure 9: Ablation study on the effect of automatic KL temperature tuning on the performance of KL-regularized\nRL with a non-parametric GP behavioral reference policy on MuJoCo locomotion tasks.\n18\nB.3\nAblation Study: Performance under a Laplace Parametric Behavioral Reference Policy\nWe use a Laplace behavioral reference policy to assess whether it is more effective at incorporating\nthe expert demonstration data into online training. Figure 10 shows empirical results using the\nLaplace behavioral reference policy compared against N-PPAC (in blue) and a SAC baseline (in green)\non three MuJoCo locomotion tasks. We use automatic KL-temperature tuning for this ablation. On\nthe Ant-v2 environment, the Laplace behavioral reference policy slightly improves upon the baseline\nSAC performance, which does not use any prior information at all. On the door and pen environment,\nthe online policy learned under the Laplace behavioral reference policy does not learn any meaningful\nbehavior.\nIn both MuJoCo locomotion tasks and the “door-binary-v0” and “pen-binary-v0” dexterous hand\nmanipulation environments, N-NPAC signiﬁcantly outperforms both the online policy learned under\nthe Laplace behavioral reference policy and the SAC baseline. We can understand the behavior under\nthe Laplace behavioral reference policy in terms of collapse of predictive variance away from data\nfor neural network parameterized policies, as it too has a decreasing variance away from the expert\ntrajectories.\n0K\n50K\n100K\n150K\n200K\nTimesteps\n0\n2000\n4000\n6000\n8000\nHalfCheetah-v2\n0K\n50K\n100K\n150K\n200K\nTimesteps\n0\n1000\n2000\n3000\n4000\n5000\nAnt-v2\n0K\n100K\n200K\n300K\n400K\nTimesteps\n0\n1000\n2000\n3000\n4000\n5000\nWalker2d-v2\nN-PPAC (Ours)\nSAC\nSAC + Laplacian Prior\nFigure 10: Ablation study using heavier-tailed Laplace behavioral reference policy on MuJoCo locomotion\ntasks.\n19\nB.4\nVisualizations of Regularized Maximum Likelihood Parametric Behavioral Policies\nMaximum Likelihood + Entropy Maximization\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10\n−5\n0\n5\n10\n(a) β = 10−2\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10\n−5\n0\n5\n10\n(b) β = 10−3\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10\n−5\n0\n5\n10\n(c) β = 10−4\n0.000\n0.004\n0.008\n0.012\n0.016\n0.020\nFigure 11: Predictive variances of parametric neural network Gaussian behavioral policies πψ(· | s) =\nN(µψ(s), σ2\nψ(s)) trained with different entropy regularization coefﬁcients β.\nMaximum Likelihood + Tikhonov Regularization\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10\n−5\n0\n5\n10\n(a) λ = 10−2\n0.000\n0.030\n0.060\n0.090\n0.120\n0.150\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10\n−5\n0\n5\n10\n(b) λ = 10−3\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10\n−5\n0\n5\n10\n(c) λ = 10−4\n0.000\n0.003\n0.006\n0.009\n0.012\n0.015\nFigure 12: Predictive variances of parametric neural network Gaussian behavioral policies πψ(· | s) =\nN(µψ(s), σ2\nψ(s)) trained with different Tikhonov regularization coefﬁcients λ.\n20\nB.5\nVisualizations of Ensemble Maximum Likelihood Parametric Behavioral Policies\nOn the “door-binary-v0” environment, we consider an ensemble of parametric neural network\nGaussian policies πψ1:K(· | s) ˙= N(µψ1:K(s), σ2\nψ1:K(s)) with\nµψ1:K(s) ˙= 1\nK\nK\nX\nk=1\nµψk(s),\nσ2\nψ1:K(s) ˙= 1\nK\nK\nX\nk=1\n\u0010\nσ2\nψk(s) + µ2\nψk(s)\n\u0011\n−µ2\nψ1:K(s)\n(B.15)\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10\n−5\n0\n5\n10\n(a) Single Model\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10\n−5\n0\n5\n10\n(b) K = 2\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10\n−5\n0\n5\n10\n(c) K = 4\n0.000\n0.016\n0.032\n0.048\n0.064\n0.080\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10\n−5\n0\n5\n10\n(d) K = 6\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10\n−5\n0\n5\n10\n(e) K = 8\n−2\n0\n2\n4\n−3\n−2\n−1\n0\n1\n2\n3\n4\n−5\n0\n5\n−10\n−5\n0\n5\n10\n(f) K = 10\n0.000\n0.016\n0.032\n0.048\n0.064\n0.080\nFigure 13: Predictive variances of ensembles of parametric neural network Gaussian behavioral policies\nπψ1:K(· | s) with each neural network in the ensemble trained via MLE. The ensemble policies are marginally\nbetter calibrated than parametric neural network policies in that their predictive variance only collapses in some\nbut not all regions away from the expert trajectories.\n21\nB.6\nParametric vs. Non-Parametric Predictive Variance Visualizations Across\nEnvironments\nFigure 14 shows the predictive variances of non-parametric and parametric behavioral policies on\nlow dimensional representations of the environments considered in Figures 4 and 5 (excluding\n“door-binary-v0”, which is shown in Figure 1).\nNon-Parametric\n−10\n0\n10\n−15\n−10\n−5\n0\n5\n10\n15\n−20\n0\n20\n−30\n−20\n−10\n0\n10\n20\n30\nParametric\n−10\n0\n10\n−15\n−10\n−5\n0\n5\n10\n15\n−20\n0\n20\n−30\n−20\n−10\n0\n10\n20\n30\n0.015\n0.048\n0.081\n0.114\n0.147\n0.180\n0.000\n1.800\n3.600\n5.400\n7.200\n9.000\n(a) pen-binary-v0\n−20\n0\n20\n−20\n−10\n0\n10\n20\n−50\n−25\n0\n25\n50\n−40\n−20\n0\n20\n40\n−20\n0\n20\n−20\n−10\n0\n10\n20\n−50\n−25\n0\n25\n50\n−40\n−20\n0\n20\n40\n0.060\n0.082\n0.104\n0.126\n0.148\n0.170\n0.000\n0.027\n0.054\n0.081\n0.108\n0.135\n(b) Ant-v2\n−25\n0\n25\n−40\n−20\n0\n20\n40\n−50\n0\n50\n−50\n0\n50\n−25\n0\n25\n−40\n−20\n0\n20\n40\n−50\n0\n50\n−50\n0\n50\n0.105\n0.135\n0.165\n0.195\n0.225\n0.255\n0.000\n0.036\n0.072\n0.108\n0.144\n0.180\n(c) HalfCheetah-v2\n−20\n0\n20\n−20\n−10\n0\n10\n20\n−50\n−25\n0\n25\n50\n−40\n−20\n0\n20\n40\n−20\n0\n20\n−20\n−10\n0\n10\n20\n−50\n−25\n0\n25\n50\n−40\n−20\n0\n20\n40\n0.000\n0.051\n0.102\n0.153\n0.204\n0.255\n0.000\n0.033\n0.066\n0.099\n0.132\n0.165\n(d) Walker-v2\nFigure 14: Predictive variances of non-parametric and parametric behavioral policies on low dimensional\nrepresentations of the environments considered in Figures 4 and 5 (excluding “door-binary-v0”, which\nis shown in Figure 1).\nLeft Column:\nNon-parametric Gaussian process posterior behavioral policy\nπGP(· | s, D0) = GP(µ0(s), Σ0(s, s′)). Right Column: Parametric neural network Gaussian behavioral pol-\nicy πψ(· | s) = N(µψ(s), σ2\nψ(s)). Expert trajectories D used to train the behavioral policies are shown in black.\nAs in Figure 1, the predictive variance of the GP is well-calibrated, whereas the predictive variance of the neural\nnetwork is not.\n22\nB.7\nVisual Comparison of Parametric vs. Non-Parametric Behavioral Policy Trajectories\nTo better understand the signiﬁcance of the behavioral policy’s model class, we sample trajectories\nfrom different behavioral policies on the door-opening task in Figure 15. We visualize the mean\ntrajectory and predictive variances of various behavioral policies showing a more sensible mean\ntrajectory and predictive variance from the non-parametric GP policy leading to better regularization\ncompared to a behavioral policy parameterized by a neural network and the implicit uniform prior in\nSAC, a state-of-the-art RL algorithm. On a randomly sampled unseen goal, we can see in Figure 15b\nthat a neural network policy trained via MLE produces a conﬁdent but incorrect trajectory. The\nstarting position is shown in black and the goal position is shown in green. We also visualize a\nuniform prior, which SAC implicitly regularizes against. Informative priors from ofﬂine data can\ngreatly accelerate the online performance of such actor-critic methods.\n(a)\n(b)\n(c)\nNonparametric GP BC Policy (Exact Posterior)\nParametric NN Gaussian BC Policy (MLE)\nUniform Prior (SAC)\nFigure 15: Left: challenging door opening task [35] which standard RL algorithms struggle on. Right and center:\n3D plots of sampled mean trajectories and predictive variances from different behavioral policies from expert\ndemonstration π0, showing a more sensible mean trajectory and predictive variance from the non-parametric GP\npolicy leading to better regularization over both: (b) a behavioral policy using a poor model class, and (c) the\nimplicit uniform prior in SAC. Starting position shown in black and goal position shown in green.\nAppendix C\nFurther Implementation Details\nC.1\nAlgorithmic Details\nPre-training On the dexterous hand manipulation tasks, before online training, the online policy is\npre-trained to minimize the KL divergence to the behavioral reference policy on the ofﬂine dataset:\nJGP(φ) ˙= Es∼D0 [DKL(πφ(· | s) ∥π0(· | s))] .\nAlgorithm 1 Non-Parametric Prior Actor–Critic\nInput: ofﬂine dataset D0, initial parameters θ1, θ2, φ, GP π0(· | s) = GP\n\u0000m(s), k(s, s′)\n\u0001\nCondition π0(· | s) on D0 to obtain π0(· | s, D0)\nfor each ofﬂine batch do\nφ ←φ −λGP ˆ∇φJGP(φ)\n▷Minimize KL between online and behavioral reference policy.\nend for\n¯θ1 ←θ1, ¯θ2 ←θ2\n▷Initialize target network weights.\nD ←∅\n▷Initialize an empty replay pool.\nfor each iteration do\nfor each environment step do\nat ∼πφ(· | st)\nst+1 ∼p(· | st, at)\nD ←D ∪{(st, at, r(st, at), st+1)}\nend for\nfor each gradient step do\nθi ←θi −λQ ˆ∇θiJQ(θi) for i ∈{1, 2}\nφ ←φ −λπ ˆ∇φJπ(φ)\n▷Minimize JQ and Jπ using GP π0(· | s, D0).\nˆθi ←τθi + (1 −τ)ˆθi for i ∈{1, 2}\n▷Update target network weights.\nend for\nend for\nOutput: Optimized parameters θ1, θ2, φ\n23\nC.2\nHyperparameters\nTable 2 lists the hyperparameters used for N-PPAC. For other hyperparameter values, we used the\ndefault values in the RLkit repository. When multiple values are given, the former refer to MuJoCo\ncontinuous control and the latter to dexterous hand manipulation tasks.\nTable 2: N-PPAC hyperparameters.\nParameter\nValue(s)\noptimizer\nAdam\nlearning rate\n3 · 10−4\ndiscount (γ)\n0.99\nreward scale\n1\nreplay buffer size\n106\nnumber of hidden layers\n{2, 4}\nnumber of hidden units per layer\n256\nnumber of samples per minibatch\n{256, 1024}\nactivation function\nReLU\ntarget smoothing coefﬁcient (τ)\n0.005\ntarget update interval\n1\nnumber of policy pretraining epochs\n400\nGP covariance function\n{RBF, Matérn}\nTable 3 lists the hyperparameters used to train the Gaussian process on the ofﬂine data. The\nhyperparameters are trained by maximizing the log-marginal likelihood. The ofﬂine data is provided\nunder the Apache License 2.0.\nTable 3: GP optimization hyperparameters.\nParameter\nValue\noptimizer\nAdam\nlearning rate\n0.1\nnumber of epochs\n500\nHyperparameter Sweep for Section 5.4. For the BNN behavioral policy trained via Monte Carlo\ndropout, a dropout probability of p = 0.1 and a weight decay coefﬁcient 1e −6 were used. These\nvalues were found via a hyperparameter search over {0.1, 0.2} for p and {1e−4, 1e−5, 1e−6, 1e−7}\nfor the dropout probability and the weight decay coefﬁcient, respectively.\nFor the deep ensemble behavioral policy, M = 15 ensemble members and a weight decay coefﬁcient\nof 1e −6 were used. The weight decay coefﬁcient was found via a hyperparameter search over\n{5, 10, 15, 20} for M and {1e −4, 1e −5, 1e −6, 1e −7} for the weight decay coefﬁcient. Each\nensemble member was trained on a different 80-20 training–validation split and initialized using\ndifferent random seeds.\n24\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ME",
    "stat.ML"
  ],
  "published": "2022-12-28",
  "updated": "2022-12-28"
}