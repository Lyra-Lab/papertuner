{
  "id": "http://arxiv.org/abs/2212.13980v1",
  "title": "Towards Learning Abstractions via Reinforcement Learning",
  "authors": [
    "Erik Jergéus",
    "Leo Karlsson Oinonen",
    "Emil Carlsson",
    "Moa Johansson"
  ],
  "abstract": "In this paper we take the first steps in studying a new approach to synthesis\nof efficient communication schemes in multi-agent systems, trained via\nreinforcement learning. We combine symbolic methods with machine learning, in\nwhat is referred to as a neuro-symbolic system. The agents are not restricted\nto only use initial primitives: reinforcement learning is interleaved with\nsteps to extend the current language with novel higher-level concepts, allowing\ngeneralisation and more informative communication via shorter messages. We\ndemonstrate that this approach allow agents to converge more quickly on a small\ncollaborative construction task.",
  "text": "Towards Learning Abstractions via Reinforcement\nLearning\nErik Jergéus1,*,†, Leo Karlsson Oinonen1,*,†, Emil Carlsson1 and Moa Johansson1\n1Chalmers University of Technology, Gothenburg, Sweden\nAbstract\nIn this paper we take the first steps in studying a new approach to synthesis of efficient communication\nschemes in multi-agent systems, trained via reinforcement learning. We combine symbolic methods with\nmachine learning, in what is referred to as a neuro-symbolic system. The agents are not restricted to only\nuse initial primitives: reinforcement learning is interleaved with steps to extend the current language\nwith novel higher-level concepts, allowing generalisation and more informative communication via\nshorter messages. We demonstrate that this approach allow agents to converge more quickly on a small\ncollaborative construction task.\nKeywords\nReinforcement learning, Multi Agent Systems, Neuro-Symbolic Systems, Emergent Communication\n1. Introduction\nLearning to communicate and coordinate efficiently via interactions, rather than relying on\nsolely supervised learning, is often viewed as a prerequisite for developing artificial agents able\nto do complex machine-to-machine and machine-to-human communication [1]. The field of\nlanguage learning and emergent communication has a long history [2, 3, 4, 5, 6], and is now\na vibrant field of research also in the deep learning community [7, 8, 9, 10]. Recent work has\nfocused on developing agents with single message communication [11, 12, 13], variable length\ncommunication [14] and compositional language [15, 16], via interactions and reinforcement\nlearning. However, a striking characteristic of human communication that has been overlooked\nin the literature is the ability to derive novel concepts and abstractions from primitives, via\ninteraction.\nIn this paper, we investigate how artificial agents can develop linguistic abstractions via\ninteraction and reinforcement learning, starting from a small set of primitive concepts and\ngradually increasing the size and efficiency of their language over time. Our motivation is\nthe builder-architect experiment in [17], investigating how humans develop communicative\nabstractions. Here, the architect is given a drawing of a shape, and has to instruct the builder\nAIC 2022, 8th International Workshop on Artificial Intelligence and Cognition\n*Corresponding author.\n†These authors contributed equally.\n\" erikjer.student@chalmers.se (E. Jergéus); leoo.student@chalmers.se (L. K. Oinonen); caremil@chalmers.se\n(E. Carlsson); jomoa@chalmers.se (M. Johansson)\n\u0012 0000-0003-2231-6869 (E. Jergéus); 0000-0003-4117-5096 (L. K. Oinonen); 0000-0002-0170-7898 (E. Carlsson);\n0000-0002-1097-8278 (M. Johansson)\n© 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\nCEUR\nWorkshop\nProceedings\nhttp://ceur-ws.org\nISSN 1613-0073\nCEUR Workshop Proceedings (CEUR-WS.org)\narXiv:2212.13980v1  [cs.AI]  28 Dec 2022\nhow to construct it from small blocks. As the experiment progressed, participants developed\nmore concise instructions after repeated attempts. Instead of talking about the positions of\nindividual blocks, they started using abstractions describing commonly seen shapes, such as\nL-shape or upside-down U, see Figure 1. Our contribution here is an initial feasibility study\nof a neuro-symbolic multi-agent reinforcement learning framework for this task. Inspired by\nneuro-symbolic program synthesis [18], the agent interleave reinforcement learning to train\ntheir neural network, with symbolic reflection to introduce new concepts for common action\nsequences. We show that agents learn to reconstruct the given shapes faster when allowed the\ncapability to introduce abstractions.\n…vertical in \nposition 2…\nHmm…?\n= L-shape\nFigure 1: Agents should periodically reflect on their experience and consider introducing abstractions,\nallowing shorter utterances for constructing commonly occurring shapes.\n2. Implementation\nOur setup mimics the one from McCarty et al. [17] where two agents, the architect and the\nbuilder, communicate about a set of geometric shapes. The agents iterate between two learning\nphases, one where the agents use reinforcement learning to develop the meaning of each\nmessage, followed by an abstraction phase, where the architect may introduce new instructions\nfor commonly seen structures. This will allow the agents to potentially solve the tasks using\nfewer messages, which gives them a higher reward as shorter interactions are preferred.\n2.1. The Environment\nThe architect’s input is a picture of the goal state alongside the current state, each of which is\nrepresented by binary 6x6 matrices, see Figure 2. Locations where there are blocks are repre-\nsented as 1’s and empty locations by 0. This is passed through a feed forward neural network,\nwhich outputs a message with instructions to the builder. The work described here focus on the\nlearning of the architect, with the builder assumed to understand the architect’s messages per-\nfectly. In the future, the builder will also be represented with a neural-network, learning to map\nmessages to the corresponding (sequence of) actions. Initially, the architect’s message-space\nconsists of 12 messages, simply the six possible positions of vertical (2 x 1) and horizontal (1 x 2)\nblocks respectively. We denote the set of messages as ℳ= {𝑉1, 𝐻1, ..., 𝑉6, 𝐻6}. These initial\nmessages have a one-to-one correspondence to the basic actions the builder can perform. Note\nGoal\nState\nFigure 2: The architect sees both the goal and the current state and decides to instruct the builder to\nplace a vertical block in position 4.\nthat the architect is allowed to introduce new messages, abstractions, during the abstraction\nphase, formally introduced in later sections.\nReward Function\nThe agents receive a reward 𝑅at each time step 𝑡when performing an\naction 𝑎, either a larger reward if the new state matches the goal exactly, or a smaller reward\nif the most recently placed block partially matches the goal. This reward function is given in\nEquation 1, where partial_match denote the number of new grid squares covered by the most\nrecently placed block matching the goal.\n𝑅𝑡(𝑠, 𝑔, 𝑎) = (0.1 * 𝑝𝑎𝑟𝑡𝑖𝑎𝑙_𝑚𝑎𝑡𝑐ℎ+ 1 * (𝑠== 𝑔)) * 0.9𝑡\n(1)\nThe architect becomes better at generalising what placing a single block entails when receiving\nan intermediate reward based on how much of that block contributes to the final goal. The larger\nreward from completing the whole structure biases the architect to always aim for a perfect\ncompletion of the goal. Finally, we encourage the architect to always use as few messages as\npossible (i.e. using abstractions) by discounting the reward based on the number of time-steps\nfurther.\n2.2. Deep Reinforcement Learning\nThe architect is modelled as a Deep Q-Network (DQN) with experience replay [19]. In short, this\nmeans that the architect, for each state and message, (𝑠, 𝑚), estimates the Q-value, or expected\ncumulative reward, for conveying message 𝑚given state 𝑠. We consider a neural network with\nlayers of sizes [72, 576, 576, 576, 36, |ℳ𝑚𝑎𝑥|], where ℳ𝑚𝑎𝑥is the maximum allowed size of\nthe message space, and we use ReLU activation between each layer. See GitHub1 for the other\nhyper-parameters and implementation.\n2.3. Abstraction Phase\nIn order to learn abstractions, we implement a version of the wake-sleep-dream framework used\nin the neuro-symbolic system DreamCoder [18]. After a wake-phase where the agents have\nengaged in reinforcement learning to learn to communicate using the current set of messages,\nthe architect enters the sleep-phase, where it is allowed to invent new messages. During the\n1https://github.com/jerge/MARL/tree/Communicative-Abstractions\nsleep phase, the architect searches for the longest common sub-sequence(s) of messages from\nthe previous reinforcement learning phase. The sub-sequence’s are rated based on their length\nand frequency and the top-rated sequence will turn into a new abstraction.\nNext follows a dream phase, which aims to quickly train the agents to use the new abstraction\ngenerated from the sleep phase. This is done by letting the agent re-experience the examples\nfrom the replay buffer, but now with the new abstraction instead of the corresponding sub-\nsequence of messages. This will lead the new abstraction towards the appropriate Q-value\nbefore starting the next reinforcement learning phase.\n3. Experimental Results\nWe conducted an initial feasibility experiment for our framework: Given a set of three re-\noccurring shapes from McCarthy’s human experiment (Fig. 3) [17], does the agents learn to\nreconstruct them faster if allowed to introduce abstractions?\nWe hypothesise that:\na) Having a language with messages also corresponding to common sequences of actions will\nfacilitate the reinforcement learning construction task.\nb) Our neuro-symbolic agent can discover and learn to use such concepts.\nFigure 3: Shapes in our experimental set: 3 upside-down U, 5 C-shapes and 3 L-shapes in all possible\ndifferent locations in the 6x6 grid.\nAgents were first pre-trained on simple randomly generated shapes to learn the basic mes-\nsage/action pairs. To establish an upper and lower bound on the agents’ performance, we\nevaluated one instance without abstraction capabilities (worst case, Fig. 4), and one instance\nwhere optimal abstractions for the shapes were already given upfront (best case, Fig. 5). The\ndifferences are large: in the worst case the agents required 350 000 epochs to successfully learn\nto construct all shapes, compared to 17 500 epochs in the best case. There is a clear advantage\nin having a richer language.\nNext, we evaluated the complete architect-agent with ability to introduce abstractions, as\nshown in Fig. 6. The agents learned to solve the construction task after 160 000 epochs, which\nis still considerably faster than the worst-case scenario. Note that the architect choose to\nintroduce only two abstractions, the upside-down U very early on, and the C-shape towards the\nend. Investigating this behaviour in more detail is further work.\nFigure 4: Worst-case bound: without the capability to create abstractions, using only initial primitives,\nlearning to build all shapes requires over 350 000 epochs.\nFigure 5: Best-case bound: If agents are given the relevant abstractions upfront, the task can be learned\nin 17 500 epochs.\n4. Conclusion and Future Work\nIn this work, we have introduced a neuro-symbolic framework for learning linguistic abstractions\nvia a combination of reinforcement learning, symbolic reasoning and interactions between\nagents. Our initial results on a small collaborative building task suggest that it is feasible for\nreinforcement learning agents to develop useful abstractions by alternating between neural\nlearning, and symbolic abstraction phases introducing new concepts. These introduced abstract\nconcepts also greatly improve the performance of the agents.\nThis is just a first step and we would like to further explore how to learn abstractions via\nreinforcement learning. One interesting direction is to extend our work to more complex\nenvironments. One issue that might arise in such scenarios is that the agents might need to\nfirst develop several intermediate abstractions, before being able to construct abstractions that\nFigure 6: With abstraction-invention, the agents need 160 000 epochs to learn to build all shapes. The\nhorizontal lines mark when the abstraction upside-down U and C-shape were introduced.\ngreatly improves the reward. Solving this type of exploration-exploitation dilemma seems like\na fundamental problem for the agents, and might require new exploration techniques.\nAnother interesting future direction is to explore scenarios where agents do not share exactly\nthe same understanding of a message, and are required to reason about each other in a recursive\nfashion.\nAcknowledgement\nErik and Leo received a Lars Pareto Travel Grant from Chalmers Univer-\nsity of Technology to present this work. Emil Carlsson was supported by CHAIR (Chalmers AI\nResearch), and Moa Johansson was supported by the Wallenberg Al, Autonomous Systems and\nSoftware Program - Humanities and Society (WASP-HS) funded by the Marianne and Marcus\nWallenberg Foundation and the Marcus and Amalia Wallenberg Foundation.\nReferences\n[1] T. Mikolov, A. Joulin, M. Baroni, A roadmap towards machine intelligence, in: Computa-\ntional Linguistics and Intelligent Text Processing, 2018, pp. 29–61.\n[2] T. Hashimoto, T. Ikegami, Emergence of net-grammar in communicating agents., Bio\nSystems 38 1 (1996) 1–14.\n[3] S. Kirby, J. R. Hurford, The Emergence of Linguistic Structure: An Overview of the Iterated\nLearning Model, Springer-Verlag, Berlin, Heidelberg, 2002, pp. 121–148.\n[4] K. Smith, S. Kirby, H. Brighton, Iterated learning: A framework for the emergence of\nlanguage, Artificial life 9 (2003) 371–86.\n[5] L. Steels, The emergence and evolution of linguistic structure: from lexical to grammatical\ncommunication systems, Connection science 17 (2005) 213–230.\n[6] L. L. Steels, The Talking Heads experiment, number 1 in Computational Models of Language\nEvolution, Language Science Press, Berlin, 2015.\n[7] J. Foerster, I. A. Assael, N. de Freitas, S. Whiteson, Learning to communicate with deep\nmulti-agent reinforcement learning, in: Advances in Neural Information Processing\nSystems, volume 29, 2016.\n[8] A. Lazaridou, M. Baroni, Emergent multi-agent communication in the deep learning era,\n2020.\n[9] F. Hill, A. K. Lampinen, R. Schneider, S. Clark, M. Botvinick, J. L. McClelland, A. Santoro,\nEnvironmental drivers of systematicity and generalization in a situated agent, in: 8th\nInternational Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020, 2020.\n[10] F. Hill, O. Tieleman, T. von Glehn, N. Wong, H. Merzic, S. Clark, Grounded language\nlearning fast and slow, 2020. URL: https://arxiv.org/abs/2009.01719.\n[11] E. Jorge, M. Kågebäck, F. D. Johansson, E. Gustavsson, Learning to Play Guess Who? and\nInventing a Grounded Language as a Consequence (2016).\n[12] A. Lazaridou, A. Peysakhovich, M. Baroni, Multi-agent cooperation and the emergence of\n(natural) language, in: 5th International Conference on Learning Representations, ICLR\n2017 - Conference Track Proceedings, 2017, pp. 1–11.\n[13] M. Kågebäck, E. Carlsson, D. Dubhashi, A. Sayeed, A reinforcement-learning approach to\nefficient communication, PLoS ONE 15 (2020) 1–26.\n[14] S. Havrylov, I. Titov, Emergence of language with multi-agent games: Learning to commu-\nnicate with sequences of symbols, in: Advances in Neural Information Processing Systems,\nvolume 30, 2017.\n[15] I. Mordatch, P. Abbeel, Emergence of grounded compositional language in multi-agent\npopulations, 2018.\n[16] J. Mu, N. Goodman, Emergent communication of generalizations, in: Advances in Neural\nInformation Processing Systems, 2021.\n[17] W. McCarthy, R. Hawkins, C. Holdaway, H. Wang, J. Fan, Learning to communicate about\nshared procedural abstractions, in: Proceedings of the 43rd Annual Conference of the\nCognitive Science Society, 2021.\n[18] K. Ellis, C. Wong, M. Nye, M. Sablé-Meyer, L. Morales, L. Hewitt, L. Cary, A. Solar-Lezama,\nJ. B. Tenenbaum, DreamCoder: Bootstrapping inductive program synthesis with wake-\nsleep library learning, in: Proceedings of the 42nd ACM SIGPLAN International Conference\non Programming Language Design and Implementation, 2021, p. 835–850.\n[19] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M. Riedmiller,\nPlaying atari with deep reinforcement learning (2013).\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2022-12-28",
  "updated": "2022-12-28"
}