{
  "id": "http://arxiv.org/abs/2212.14750v2",
  "title": "Unsupervised 4D LiDAR Moving Object Segmentation in Stationary Settings with Multivariate Occupancy Time Series",
  "authors": [
    "Thomas Kreutz",
    "Max Mühlhäuser",
    "Alejandro Sanchez Guinea"
  ],
  "abstract": "In this work, we address the problem of unsupervised moving object\nsegmentation (MOS) in 4D LiDAR data recorded from a stationary sensor, where no\nground truth annotations are involved. Deep learning-based state-of-the-art\nmethods for LiDAR MOS strongly depend on annotated ground truth data, which is\nexpensive to obtain and scarce in existence. To close this gap in the\nstationary setting, we propose a novel 4D LiDAR representation based on\nmultivariate time series that relaxes the problem of unsupervised MOS to a time\nseries clustering problem. More specifically, we propose modeling the change in\noccupancy of a voxel by a multivariate occupancy time series (MOTS), which\ncaptures spatio-temporal occupancy changes on the voxel level and its\nsurrounding neighborhood. To perform unsupervised MOS, we train a neural\nnetwork in a self-supervised manner to encode MOTS into voxel-level feature\nrepresentations, which can be partitioned by a clustering algorithm into moving\nor stationary. Experiments on stationary scenes from the Raw KITTI dataset show\nthat our fully unsupervised approach achieves performance that is comparable to\nthat of supervised state-of-the-art approaches.",
  "text": "Unsupervised 4D LiDAR Moving Object Segmentation in Stationary Settings\nwith Multivariate Occupancy Time Series\nThomas Kreutz\nMax M¨uhlh¨auser\nAlejandro Sanchez Guinea\nTelekooperation Lab, Technical University Darmstadt\n{kreutz, max, sanchez}@tk.tu-darmstadt.de\nAbstract\nIn this work, we address the problem of unsupervised\nmoving object segmentation (MOS) in 4D LiDAR data\nrecorded from a stationary sensor, where no ground truth\nannotations are involved.\nDeep learning-based state-of-\nthe-art methods for LiDAR MOS strongly depend on anno-\ntated ground truth data, which is expensive to obtain and\nscarce in existence. To close this gap in the stationary set-\nting, we propose a novel 4D LiDAR representation based\non multivariate time series that relaxes the problem of un-\nsupervised MOS to a time series clustering problem. More\nspecifically, we propose modeling the change in occupancy\nof a voxel by a multivariate occupancy time series (MOTS),\nwhich captures spatio-temporal occupancy changes on the\nvoxel level and its surrounding neighborhood. To perform\nunsupervised MOS, we train a neural network in a self-\nsupervised manner to encode MOTS into voxel-level feature\nrepresentations, which can be partitioned by a clustering al-\ngorithm into moving or stationary. Experiments on station-\nary scenes from the Raw KITTI dataset show that our fully\nunsupervised approach achieves performance that is com-\nparable to that of supervised state-of-the-art approaches.\n1. Introduction\nUnderstanding an urban environment in terms of its\nmoving or static entities is a crucial aspect for scene under-\nstanding (e.g., [1]), autonomous driving agents (e.g., [30,\n31]), consistent mapping (e.g., [7]), pedestrian safety, and\nintelligent transportation systems in smart cities (e.g., [28,\n21]).\nIn particular, LiDAR moving object segmentation\n(MOS) is a task to classify the points of a scene into be-\ning dynamic or static.\nThe research on end-to-end approaches for LiDAR ob-\nject detection, semantic segmentation, instance segmenta-\ntion, and panoptic segmentation has matured over the past\nyears [20] and large-scale autonomous driving datasets like\nSemanticKITTI [17, 3], NuScenes [5, 15], or Waymo [34,\n14] have been the essential ingredient for developing state-\nof-the-art approaches.\nUnfortunately, annotated data for\nLiDAR MOS is scarce [8]. Recently, an annotated MOS\nbenchmark dataset based on SemanticKITTI has been pro-\nposed in [7], which fostered promising research about end-\nto-end approaches for MOS in the autonomous driving set-\nting (e.g., [27, 25, 19]). However, the lack of annotated\ndatasets limits the practical application of supervised end-\nto-end MOS deep learning models to scenarios where data\nhas not been recorded with the same sensor setup [8].\nA potential solution to the described issue is unsuper-\nvised methods because they do not depend on annotated\ndata and generalize better to arbitrary data distributions [4,\n37]. For instance, self-supervised scene flow methods can\nbe used for unsupervised MOS, but their performance is in-\nferior to state-of-the-art supervised methods [25].\nIn contrast to previous work, we propose a fully un-\nsupervised 4D LiDAR MOS approach that generalizes to\ndata recorded from arbitrary stationary LiDAR sensors, and\nachieves results that are comparable to that of supervised\nstate-of-the-art approaches. Previous work has shown that\nmovement appears with occupancy change patterns in occu-\npancy time series [13]. On this basis, it can be hypothesized\nthat multivariate occupancy time series (MOTS) are an ef-\nfective data modality to identify motion in spatio-temporal\nneighborhoods of point cloud videos. In our paper, we an-\nswer the following hypothesis: Multivariate time series are\nan effective data-modality for unsupervised MOS in station-\nary LiDAR point cloud videos.\nWe propose MOTS as a novel 4D LiDAR representation\nthat allows using self-supervised representation learning to\ndistinguish between moving and static parts in a station-\nary LiDAR scene. More specifically, a voxel is represented\nby a MOTS that effectively models spatio-temporal occu-\npancy changes of the voxel and its surrounding neighbor-\nhood. Following recent advances in self-supervised learn-\ning for multivariate time series (e.g., [16, 35]), we first en-\ncode MOTS in short time windows with a neural network\nto a spatio-temporal voxel embedding. Afterward, we clus-\nter the resulting embeddings of each voxel for unsupervised\narXiv:2212.14750v2  [cs.CV]  12 Jan 2023\nMOS. Therefore, our approach relaxes MOS to a multivari-\nate time series clustering problem.\nWe show the effectiveness of MOTS for unsupervised\nMOS by quantitative evaluations on publicly available sta-\ntionary data from the Raw KITTI dataset [17] and a qualita-\ntive evaluation on stationary data we recorded with a Velo-\ndyne VLP-16 sensor. Our main contributions are:\n• A novel representation of 4D point clouds for represen-\ntation learning of spatio-temporal occupancy changes\nin a local neighborhood of stationary LiDAR point\ncloud videos, which we call MOTS\n• An unsupervised MOS approach for stationary 4D Li-\nDAR point cloud videos based on MOTS\n2. Related Work\nThe majority of closely related work on moving object\nsegmentation (MOS) can be categorized into dynamic oc-\ncupancy grid mapping (e.g., [29, 32]), scene flow (e.g.,\n[23, 2]), and moving object segmentation methods (e.g.,\n[7, 25]).\n2.1. Dynamic Occupancy Grid Mapping\nOccupancy grid mapping estimates probabilities for the\noccupancy of grid cells. Furthermore, dynamic occupancy\ngrid mapping (DOGMA) aims to learn a state vector for\neach grid cell that consists of occupancy probability and ve-\nlocity [29]. An effective dynamic occupancy grid mapping\nbased on finite random sets has been proposed in [29].\nUsing the result in [29] as a basis, various deep learning-\nbased methods that learn DOGMAs have been proposed.\nFor instance, the work in [13] uses the DOGMA from [29]\nas an input to a neural network that learns to predict bound-\ning boxes for moving objects.\nThe work in [31] learns\nDOGMAs to estimate motion of objects in the scene with\na neural network in a stationary setting.\nThey use the\nDOGMA obtained from the approach in [29] as a basis to\ntrain their model end-to-end, and their work was extended\nin [32] to the non-stationary setting.\nIn spite of their success, the described methods depend\non a DOGMA to find moving objects, and they are limited\nto 2D birds-eye view (BEV) maps. Today, in other related\ntasks such as semantic segmentation, projection based deep\nlearning methods are getting outperformed by methods that\noperate directly in the 3D or 4D domain [38]. In contrast,\nour method is designed for raw 4D point clouds and does\nnot depend on occupancy grid mapping methods.\n2.2. Scene Flow\nScene flow methods learn a displacement vector for any\npoint in frame t to frame t + 1. Hence, a scene flow method\ncan be extended to a MOS approach. For instance, cluster-\ning point positions together with their corresponding scene\nflow vectors have been used in [23] to obtain an unsuper-\nvised motion segmentation. Furthermore, the work in [2]\nshowed that scene flow based on a self-supervised method\ncan learn to segment motion as a byproduct. However, the\ndownside of scene flow methods is that (a) there is no clear\ncorrespondence between points across frames in noisy point\nclouds and (b) only using two frames might not contain\nenough information for all moving points in the scene, par-\nticularly when dealing with slow moving objects, as out-\nlined in [25].\nThese limitations can explain the inferior\nresults for scene flow-based MOS on the SemanticKITTI\nMOS benchmark [7, 25]. In comparison, our approach can\nlearn motion from a larger temporal context by including\nmore than two frames. Furthermore, a trivial correspon-\ndence between voxels across all frames exists in our ap-\nproach because it is designed on the voxel level.\n2.3. Motion/Moving Object Segmentation\nRecently, a benchmark and a supervised model (LMNet)\nbased on range images for MOS has been proposed in [7].\nThe authors extended their work with an automatic labeling\napproach that is based on a map cleaning method in [8],\nwhich makes it more robust to unseen environments and\nimproves the performance. The work in [27] proposed a\nmethod based on BEV which is faster than LMNet but with\ninferior segmentation performance.\nA method for 4D MOS has been proposed recently\nin [25], where predictions are made from a 4D volume of\nthe point cloud video. Further, a Bayesian filter taking pre-\nvious predictions into account is proposed to filter out noise.\nThe model in [25] makes use of sparse convolutions [10],\nwhich achieve better performance than projecting the point\ncloud to a two-dimensional range image representation.\nFusing semantic predictions with moving object predic-\ntions has been shown to increase the performance in [7].\nUsing semantic features for MOS has been used specifically\nin [19]. In this case, the semantic features are learned indi-\nvidually on each frame and the moving object segmentation\nmask is learned afterward jointly from sequences of the re-\nsulting semantic features and range images.\nAll the aforementioned approaches show a promising\nperformance, but they rely on annotations to train their ap-\nproach. In comparison, we are the first to propose an unsu-\npervised approach for MOS in the stationary setting which\ndoes not depend on occupancy grid mapping, map cleaning\nor scene flow methods. At the same time, our approach is\nbased on multiple frames.\n2.4. Unsupervised Segmentation of Time Series\nRecent advances in self-supervised multivariate time se-\nries representation learning (e.g., [16, 24, 35]) have shown\nthat different states of a system (measured by a multivari-\nate time series) can be learned for each time step in a self-\nFigure 1: Overview of the proposed approach\nsupervised manner.\nThe learned representations at each\ntime step can then be clustered to obtain an unsupervised\nsegmentation of the time series.\nTo the best of our knowledge, our work is the first to\nadapt this idea to the point cloud domain. We consider oc-\ncupancy states at each point in time of a voxel as discrete\ntime series measurements and exploit the dependency be-\ntween occupancy changes in the spatio-temporal neighbor-\nhood of a voxel for unsupervised MOS.\n3. Approach\n3.1. Problem setup\nGiven a point cloud video recorded from a stationary Li-\nDAR sensor, our goal is to produce an unsupervised seg-\nmentation of the scene into moving and stationary points\nwithout having to rely on annotated data. More specifically,\nthe goal is to perform unsupervised moving object segmen-\ntation (MOS) solely on raw, stationary LiDAR point cloud\nvideos.\nThis problem is of practical use in smart cities\nwhere LiDAR sensors can be mounted on, for instance,\nstreet lamps that cover a large area of the city [28] and mov-\ning objects have to be identified. Another crucial use case is\nidentifying moving objects in the traffic around a stationary\nautonomous vehicle that waits to drive onto a busy road.\n3.2. Overview\nWe propose a novel representation of point cloud videos\nin order to learn spatio-temporal representations of single\nvoxel cells. The proposed representation relaxes unsuper-\nvised MOS to a multivariate time series clustering problem.\nFigure 1 summarizes our method. At frame t, we com-\npute occupancy time series (OTS) of length w for all voxel\ncells in the frame. Given a spatial radius r, we construct\nmultivariate occupancy time series (MOTS) from the OTS\nof a voxel and all OTS in its surrounding neighborhood.\nEach channel of MOTS effectively captures the occupancy\nchange in local spatio-temporal neighborhoods of the scene.\nWe assume that movement appears similarly across MOTS\nfrom different voxels and clustering MOTS separates mov-\ning voxels from stationary voxels. Hence, with MOTS the\nMOS problem is relaxed to a multivariate time series clus-\ntering problem.\nGiven a MOTS point cloud video representation at any\nframe t, a neural network encodes all MOTS in frame t to\na feature representation that can distinguish moving from\nstationary voxel states. This kind of representation learns\nand encodes spatio-temporal occupancy changes such that\na clustering algorithm can perform unsupervised MOS.\n3.3. Multivariate Occupancy Time Series\nOur\napproach\nis\ndesigned\nfor\nvoxelized\npoint\ncloud\nvideos.\nA\nvoxelgrid\nis\na\nset\nof\nvoxels\nV ⊆Rw/m×h/m×d/m,\nwith a grid resolution of m,\nheight h, width w, and depth d. An ordered sequence of 3D\nvoxelgrids V S ∈(V1, ..., VN) can be considered a video,\nwith N being the number of frames. A voxel v ∈V can\nhave one of two states: occupied or free. The state of a\nvoxel at time t is modelled by a function S : V × N −→B,\nwith the interpretation that 0 = free and 1 = occupied.\nAssuming data recorded from a stationary LiDAR, there is\na bidirectional mapping from any voxel v ∈Vk to a voxel\nv′ ∈Vl, k ̸= l, such that v == v′. As a result, at any point\nin time t, for any voxel vi, we can define its occupancy\ntime series OTSi,t ∈Bw as\n  OTS_ { i, t} = [S(v_{i }, t -(w-1 )), . .., S (v_{i}, t-1), S(v_{i}, t)] (1)\nwith w being the time series length, and S(vi, ·) measuring\nthe occupancy of vi at each point in time.\nWe define MOTS as a multivariate collection of OTS,\nwhere we consider the OTS of voxels vj in a spatial neigh-\nborhood around vi as additional channels. Given a spa-\ntial radius of r around vi with a voxel grid resolution\nof m units in an arbitrary euclidean space, we define a\nset R = {−r, −r + m, ..., 0, ..., r −m, r} that includes all\npossible discrete distances within radius r around 0 as the\ncenter. We then compute a neighborhood distance matrix\nNr = R × R × R with a 3-fold Cartesian product over R\nby considering each element in Nr as a row. Nr holds the\ndistances to all reachable voxels within radius r considering\nan arbitrary voxel vi as the center. An element-wise addi-\ntion of each row in Nr with vi computes the neighborhood\n  \\ma th c al  {N}(v_{i}, r) = \\mathcal {N}_{r} + v_{i} \n(2)\nwith Nr + vi being the shorthand notation of adding vi\nto each row of Nr ([18]).\nGiven the neighborhood N(vi, r) of vi\nwith ra-\ndius r, we define a multivariate occupancy time series\nMOTSi,t ∈B|N(vi,r)|×w of vi as\n  MOTS_ { i, t} =  \\{  OTS_{ j, t} \\ | \\ v_{j} \\in \\mathcal {N}(v_{i}, r)\\} \n(3)\nwhere the channels of MOTSi,t are composed by the OTS\nof each vj ∈N(vi, r).\nMOTS for Non-Stationary LiDAR.\nWhile the focus of\nour work is on the stationary case, MOTS can also be com-\nputed in the non-stationary case. For a non-stationary Li-\nDAR, we assume to have the pose information given by,\ne.g., a SLAM approach [17]. Given the poses, we transform\neach frame to the pose of the first frame to again obtain a\nbidirectional mapping from any voxel v ∈Vk to v′ ∈Vl,\nk ̸= l, such that v == v′. The latter property allows com-\nputing MOTS for each voxel in a non-stationary setting.\n3.4. Efficiently Transforming 4D Point Clouds into\nMOTS\nA dense MOTS representation of 4D point clouds is in-\nefficient because most of the space in all frames is empty.\nHence, following related work (e.g., [10]) we adopt a sparse\ntensor representation and only store/compute MOTS of vox-\nels that are occupied. We can represent each frame Vt with\n0 < t ≤N of a 4D point cloud as a sparse tensor, which\nwe consider a pair (Vt\nsparse, Ft\nsparse) consisting of a set of\nvoxels Vt\nsparse that we define as\n  \n\\mathc a l {V} ^{ t }_{sp ar s e }  =  \\{ (v_{i}, t) \\ | \\ S(v_{i}, t)=1 \\wedge v_{i} \\in V_{t}\\} \n(4)\nand its corresponding set of MOTS features Ft\nsparse that\nwe define as\n  \n\\mathc a l {F}^{t } _{spa rs e }  = \\ { MOTS_{i, t} \\ | \\ S(v_{i}, t)=1 \\wedge v_{i} \\in V_{t} \\} \n(5)\nIn practice, we make use of vectorized operations and\na performant parallel hashmap Python implementation1 to\nefficiently compute the MOTS features for each voxel.\n1https://github.com/atom-moyer/getpy\nFigure 2: Architecture of our 1D CNN Autoencoder\n3.5. Unsupervised Moving Object Segmentation\nwith MOTS\nWe relax unsupervised MOS with MOTS to a time se-\nries clustering problem. Due to the enormous amount of\navailable training data and high dimensionality of the time\nseries, we leverage deep learning to learn the underlying\nstructure of the data and use an autoencoder (AE) as a fea-\nture extractor. More specifically, we use an AE based on 1D\nconvolutions to learn feature representations of MOTS.\nThe AE consists of an encoder and a decoder part. The\nencoder f : Rd 7→Re maps a d-dimensional input data\npoint x ∈Rd to an e-dimensional (latent) code represen-\ntation z ∈Re. The decoder is a function f : Re 7→Rd\nthat maps the e-dimensional code vector z back to a d-\ndimensional output ˆx, with the goal to be as similar as pos-\nsible to the input x, i.e., g(f(x)) = ˆx ≈x with f(x) = z\nand g(z) = ˆx. To this end, the AE is trained using the\nwell-known mean squared error (MSE) loss function\n  \\mathc al {L} = MSE(x, \\hat {x}) \n(6)\nin order to minimize the reconstruction error.\nUnsupervised MOS.\nFor each frame in V S, we encode\nthe MOTS of all occupied voxels with the encoder f. After-\nward, a clustering model partitions the voxel embeddings\ninto a moving or stationary state. In this work, we per-\nform unsupervised MOS by clustering the voxel embed-\ndings with a gaussian mixture model (GMM). We empiri-\ncally found the GMM to significantly perform better than,\ne.g., k-means.\n3.6. Architecture\nWe depict the architecture of the AE we use in this work\nin Figure 2. The encoder is composed of three 1D convolu-\ntional layers, each having a kernel size of three. Afterward,\nwe use two fully connected (FC) dense layers to project the\noutput of the last convolutional layer to the e-dimensional\ncode vector. The decoder consists of the reversed FC dense\nlayers and three 1D transposed convolutions to reconstruct\nthe input from the code vector. After each layer, we use the\nReLU activation function as the non-linearity. We use this\nstraightforward baseline model to highlight the effective-\nness of MOTS in distinguishing local occupancy changes.\n4. Evaluation\nIn this section, we first describe our experimental setup\nand design. Afterward, we quantitatively evaluate our ap-\nproach against supervised state-of-the-art approaches for\nMOS on stationary scenes from the KITTI dataset. In addi-\ntion, we investigate the influence of hyperparameters on the\noverall performance of our approach. Finally, we perform a\nqualitative evaluation with Velodyne VLP-16 LiDAR data.\nOur source code and data is publicly available on github\ngithub.com/thkreutz/umosmots.\n4.1. Dataset and Metric\nRaw KITTI [17].\nTo the best of our knowledge, a large-\nscale dataset with point-wise moving object annotations for\nstationary LiDAR sensors is not publicly available. The Se-\nmanticKITTI MOS dataset [3, 7] includes annotations for\nmoving objects. However, there are not enough stationary\nframes in the validation sequence (see Supplementary Sec-\ntion 1) for a strong evaluation, and the annotations for the\ntest sequences are not publicly available.\nFor a meaningful evaluation against the state-of-the-art\nin a stationary setting, we manually annotated sequences\nfrom the Raw KITTI dataset [17]. The data in Raw KITTI\nhas been recorded with a 64-beam Velodyne HDL-64E Li-\nDAR sensor at a 10Hz framerate. We manually annotated\nthree stationary scenes from the “Campus” and “City” cat-\negories summarized in Table 2. These three scenes have in\ntotal 378 frames for evaluation.\nFor a fair comparison against the state-of-the-art, our\nAE model is trained only on the training sequences\n{i | 0 ≤i < 11} \\ {8} of the SemanticKITTI dataset. One\nMOTS per voxel is one training example, which leads to an\nenormous amount of training data. For this reason, we de-\ncided to train only on the first 200 frames of each sequence.\nVelodyne VLP-16.\nFor a qualitative evaluation on a dif-\nferent sensor, we recorded eight stationary LiDAR scenes\nwith a 16-beam Velodyne VLP-16 LiDAR around the Cam-\npus of the TU Darmstadt. The data was collected at a fram-\nerate of 20Hz, which is twice the framerate of Raw KITTI.\nMetric.\nWe quantify the performance of our approach\nagainst the state-of-the-art by following related work [7]\nand use the intersection-over-union (IoU) metric. To evalu-\nate the performance over all frames, we compute the mean\nof the IoU from each frame, which is known as the mean-\nintersection-over-union (mIoU).\n4.2. Implementation Details\nWe train our AE models with the Adam optimizer,\nbatch size = 1024 , learning rate = 1e −4, and embed-\nding dimension e ∈{16, 32} for two epochs, which takes\naround 8–24 hours on an RTX A4000 16GB GPU. We eval-\nuate different window sizes w ∈{8, 10, 15, 20}, where 8\nis the best setting found in [7] and 10 in [26]. We fur-\nther evaluate the neighborhood radius settings r = {1, 2}\nfor MOTS, which lead to possible MOTS of dimensions\n{27, 125} × {8, 10, 15, 20}.\nRegarding clustering, the number of clusters for the\nGMM is evaluated between k = {10, 15, 20}. We train one\nGMM per scene on 200, 000 uniformly sampled embed-\ndings from the first ten frames of each scene to speed up the\ntraining. The final predictions are obtained by computing\nthe IoU for each cluster to the ground truth “moving” class\non the first frame of the respective sequence. We empiri-\ncally find that our approach usually predicts moving voxels\nacross 1 and 3 clusters, each having a ground truth overlap\nof at least 0.15. Therefore, we automatically map all clus-\nters with an IoU overlap of at least 0.15 to the class “mov-\ning”. In practice, this overlap can be found with minimal\neffort by a domain expert, or a small scene can be anno-\ntated.\n4.3. Experimental Design\nWe evaluate our approach against the state-of-the-art in\na stationary setting of the Raw KITTI dataset. In partic-\nular, we evaluate against the two best recent supervised\nstate-of-the-art methods for LiDAR MOS2: LMNet [7] and\n4DMOS [25]. Both approaches have been trained on Se-\nmanticKITTI data. For this reason, a comparison through\nan experiment on the Raw KITTI data against the latter ap-\nproaches can in fact be made because the sensor setup is\nequivalent. We argue that a model trained on data from se-\nquences of a mostly moving sensor (with ego-motion com-\npensation) should perform well on data recorded from a sta-\ntionary vehicle. This situation occurs naturally while a ve-\nhicle is stopping at a red light or is waiting to merge into\na busy road. To the best of our knowledge, a distinction\nbetween a moving or stationary ego-vehicle is not made\nin evaluations on large-scale autonomous driving datasets\n(e.g., SemanticKITTI, NuScenes [5], Waymo [34], Argo-\nverse [6]). For this reason, we believe the results of our\nexperiments are a valuable contribution to the community.\nIn the remainder of this evaluation, we follow related\nwork [23, 2] and remove the ground. However, because we\n2At the time of writing this work (July 2022)\nmIoU(↑)\nSetting\nApproach\nCity 1\nCity 2\nCampus 1\navg\nSupervised\nLMNet [7]\n.281\n.557\n.787\n.541\nSupervised\n4DMOS w/o BF [25]\n.639\n.743\n.940\n.774\nSupervised\n4DMOS w/ BF p=0.25 [25]\n.567\n.660\n.944\n.723\nSupervised\n4DMOS w/ BF p=0.5 [25]\n.630\n.769\n.946\n.781\nUnsupervised\nOurs r = 2, e = 32, k = 20, w = 15\n.792\n.840\n.791\n.808\nUnsupervised\nOurs r = 2, e = 32, k = 20, w = 20\n.806\n.839\n.792\n.812\nTable 1: Summary of the mIoU results on stationary KITTI scenes against the state-of-the-art\nName\n#Frames\n2011 09 26 drive 0017 sync (City 1)\n114\n2011 09 26 drive 0060 sync (City 2)\n78\n2011 09 28 drive 0016 sync (Campus 1)\n186\nTotal\n378\nTable 2: Overview of name, category, and number of frames\nfor each annotated scene\noperate in a stationary setting, where the FOV and location\nof the sensor do not change, we remove the ground by sim-\nply thresholding the z-axis to −1. We furthermore evaluate\nour approach against the state-of-the-art on the voxel level.\nTo this end, all point-wise predictions by LMNet [7] and\n4DMOS [25] are mapped to its respective voxel.\n4.4. Results on Raw KITTI\nWe compare the two best-performing configurations of\nour approach against state-of-the-art MOS approaches on\nour three annotated scenes from the Raw KITTI dataset.\nThe results in Table 1 show that our unsupervised ap-\nproach reaches comparable performance to the state-of-the-\nart w.r.t. mIoU. On average over all scenes, we achieve\nbetter performance with 0.812 compared to 0.781 of the\nstate-of-the-art method 4DMOS.\n4DMOS considerably outperforms our approach on the\n“Campus 1” scene. As shown in Figure 3, our approach\nmakes wrong segmentation predictions on the ground, some\noccluded parts while an object passes by, and not the en-\ntire vehicle is covered. 4DMOS achieves an almost perfect\noverlap with the ground truth. At the same time, LMNet\nmisses out on some pedestrians. In contrast, our approach\naccurately segments the pedestrians.\nOur unsupervised approach outperforms the supervised\napproaches on the “City 1” and “City 2” scenes. Based on\nthe visualization in Figure 4, we now pose possible expla-\nnations for the lower performance of LMNet and 4DMOS.\nEspecially for LMNet, the drop in performance is due to\nApproach Configuration k ∈{10, 15, 20}\nmIoU(↑)\nr = 1, e = 16, w = 15\n.756 ± .025\nr = 1, e = 16, w = 20\n.757 ± .066\nr = 2, e = 32, w = 15\n.759 ± .055\nr = 1, e = 32, w = 20\n.774 ± .052\nr = 2, e = 32, w = 20\n.800 ± .043\nTable 3: Top five average mIoU results over all clusters and\nscenes\n(a) wrongly segmenting vehicles that are standing still next\nto the ego-vehicle and (b) missing some pedestrians that\nare far away. We hypothesize that LMNet encounters an\nout-of-distribution scenario. Stopping at a red light with\ncars around the ego-vehicle may not appear often enough\nin the training data. However, the training data includes\nvarious highway/secondary road scenes with cars close to\nthe ego-vehicle that keep the same distance by driving at\na similar speed. In such scenarios, cars close to the ego-\nvehicle also move, and LMNet can segment them as mov-\ning objects. When the ego-vehicle is stationary, the model\nseems to not generalize well. Therefore, our experiment in-\ndicates biased training data w.r.t. non-stationary scenes in\nSemanticKITTI, which may cause the performance to drop\nfor LMNet in this stationary setting. In contrast, 4DMOS\nshows excellent generalization capability to the stationary\nsetting, but it misses a moving vehicle in the upper part of\nthe scene, which our approach segments correctly.\n4.5. Influence of Hyperparameters\nIn this section, we evaluate the influence of hyperparam-\neters e, r, k, and w on the performance of our approach. We\nconducted an experiment with different configurations. The\nresults are summarized in Figure 6 and Table 3.\nIn Figure 6, the respective x-axis of each subplot varies\ndifferent settings for (a) the number of clusters k, (b) the\nsize of radius r, (c) the embedding dimension e, and (d) the\nwindow size w. The y-axis shows the averaged mIoU per-\nformance of different configurations over the three scenes\nGround Truth\nLMNet\n4DMOS\nOurs\nFigure 3: Qualitative comparison to the ground truth and state-of-the-art approaches on the Campus 1 scene\nGround Truth\nLMNet\n4DMOS\nOurs\nFigure 4: Qualitative comparison to the ground truth and state-of-the-art approaches on the City 2 scene\nin dependency to the varied parameter on the x-axis. We\npresent the results using a boxplot to visualize the standard\ndeviation across different configurations, which serves as\nan uncertainty measure. Table 3 summarizes the five best\nconfigurations w.r.t.\nr, e, and w across different cluster\nnumbers and all scenes. We computed the mean over the\nrespective mIoU results.\nImpact of number of clusters. Figure 6 shows how\nthe number of clusters influences the performance of our\napproach. Our approach reaches good performance more\nconsistently with 15 and 20 clusters across different pa-\nrameter configurations. We attribute this result to differ-\nent patterns of the stationary or moving parts (e.g., corners,\nwalls, ground, pillars, trees) in the scene that are captured\nby MOTS. Hence, more clusters are needed to correctly par-\ntition the latent space.\nImpact of radius.\nRegarding the radius r, Figure 6\nshows that on average a higher radius yields a better mIoU.\nA higher radius implies a larger receptive field, which bene-\nfits the encoder to distinguish between moving and station-\nary patterns in MOTS. More specifically, the averaged re-\nsults over all clusters and scenes in Table 3 show that our\napproach reaches best performance with a radius of r = 2.\nImpact of embedding dimension.\nThe best perfor-\nmance across all cluster configurations is achieved with\ne = 32 as shown in Table 3. In fact, the three best configura-\ntions all used e = 32, which suggests that a higher embed-\nding dimension achieves the best performance. However,\nwe cannot conclude from our study in Figure 6 that embed-\nding dimension e = 32 consistently outperforms e = 16.\nImpact of window size. The top five results in Table 3\nshow that larger window sizes from w = 15 to w = 20 per-\nform the best overall. In Figure 6, we can observe that both\nw = 15 and w = 20 have considerably better performance\nacross all scenes. Our experiments show that context con-\ncerning the 15–20 past frames is the most effective configu-\nration for stationary scenes. Twenty past frames correspond\nto two seconds temporal context at a 10Hz framerate.\n4.6. Qualitative Results with a Velodyne VLP-16\nWe trained one model with the best parameter configu-\nration (r = 2, e = 32) on the first 200 frames of seven sta-\ntionary scenes for 5 epochs. Furthermore, we use k = 20\nclusters when training the GMM. Because the VLP-16 data\nwas recorded at 20Hz, we used a window size of w = 40\nto match the best performing temporal history of 2 sec-\nonds. This shows that our approach can even scale to tem-\nporal histories greater than 20 frames. In contrast, other\napproaches (e.g., 4DMOS) may not be able to handle such\na long history due to the enormous memory consumption.\nThe qualitative results on a leave-out test scene in Fig-\nure 5 show that our approach can be applied to different\nLiDAR sensor setups (e.g., VLP-16, HDL-64E) that even\nhave different temporal resolutions (e.g., 10Hz, 20Hz). We\ncan see that our approach accurately segments movement of\ndifferent pedestrians and a cyclist. Wrong predictions are\nobtained for tree leaves due to noisy sensor measurements\nthat probably appear similar to movement in MOTS.\nFrame t\nFrame t+10\nFrame t+20\npedestrians\nbicycle\nFigure 5: Qualitative results on data recorded from a Velodyne VLP-16 LiDAR\n10\n15\n20\nk\n0.0\n0.2\n0.4\n0.6\n0.8\nmIoU\n1\n2\nr\n16\n32\ne\n8\n10\n15\n20\nw\nCampus 1\nCity 1\nCity 2\nFigure 6: Ablation study regarding number of clusters k, radius r, embedding dimension e, and window size w\n5. Discussion and Future Work\nOur experimental evaluation shows the potential of our\napproach to segment moving objects in stationary LiDAR\npoint cloud videos without any supervision.\nThe pro-\nposed approach for stationary LiDAR sensors can outper-\nform state-of-the-art supervised models as shown in Ta-\nble 1. However, we want to emphasize that our model has\nlimitations in a non-stationary setting. In particular, points\nentering the scene or previously occluded parts that become\nvisible appear exactly the same in MOTS when compared to\na moving object. Extending the approach for non-stationary\nLiDAR data is left for future work.\nFurthermore, our approach is limited by a small recep-\ntive field. For instance, recent works on Vision Transform-\ners show that global context is essential for learning good\nfeature representations [12, 36, 9]. Increasing the MOTS\nreceptive field by a small amount implies a quadratic scal-\ning of MOTS channels, which quickly scales to thousands\nof channels. The latter results in strong performance limi-\ntations w.r.t. time and memory, because one MOTS is com-\nputed for each unique voxel in the scene for each frame. For\nthis reason, future work on our approach includes finding an\nefficient method for scaling the receptive field.\n6. Negative Sociental Impact\nSmart cities of the future will have an infrastructure to\ncollect enormous amounts of data from heterogeneous data\nsensors such as LiDAR, surveillance cameras, or temper-\nature sensors.\nThese sensors build the foundation for a\ndigital twin that can reason about all kinds of behavior in\nthe city [33]. This information will help to enhance the\nlives of citizens and have a substantial impact on intelli-\ngent transportation systems [21] in the future. However,\nusing surveillance cameras as a data source for digital twins\nraises strong privacy concerns. For instance, cameras cap-\nture color information of the natural scene, allowing per-\nson re-identification [11]. In the wrong hands, this informa-\ntion encourages tracking a specific target, potentially lead-\ning to blackmail, which results in a strong negative socien-\ntal impact. For this reason, we want to raise awareness on\nusing LiDAR as a more anonymity-preserving technology\nfor surveillance. That is because LiDAR does not record\nfacial characteristics or further details like hair and skin\ncolor [22]. In a stationary setting, LiDAR sensors can de-\ntect all kinds of objects and reason about their behavior and\nmay replace the need for RGB cameras in public places.\n7. Conclusion\nThis work addresses unsupervised moving object seg-\nmentation (MOS) in stationary LiDAR point cloud videos.\nOur approach effectively learns voxel embeddings from oc-\ncupancy changes in a spatio-temporal neighborhood. We\npropose to model the occupancy changes in the neighbor-\nhood of a voxel by a multivariate occupancy time series\n(MOTS), which in turn allows learning voxel embeddings\nthat encode motion information.\nAs a consequence, our\nMOTS voxel representation relaxes unsupervised MOS to\na multivariate time series clustering problem. We evalu-\nate our method quantitatively on stationary scenes from the\nRaw KITTI dataset and qualitatively on stationary VLP-16\ndata. We achieve comparable performance to state-of-the-\nart supervised MOS approaches in the stationary setting.\nAcknowledgement\nThis work has been funded by the LOEWE initiative\n(Hesse, Germany) within the emergenCITY centre.\nReferences\n[1] Mehmet Aygun, Aljosa Osep, Mark Weber, Maxim Maxi-\nmov, Cyrill Stachniss, Jens Behley, and Laura Leal-Taix´e.\n4d panoptic lidar segmentation.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5527–5537, 2021.\n[2] Stefan Andreas Baur, David Josef Emmerichs, Frank Moos-\nmann, Peter Pinggera, Bj¨orn Ommer, and Andreas Geiger.\nSlim: Self-supervised lidar scene flow and motion segmen-\ntation. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 13126–13136, 2021.\n[3] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke,\nC. Stachniss, and J. Gall. SemanticKITTI: A Dataset for Se-\nmantic Scene Understanding of LiDAR Sequences. In Proc.\nof the IEEE/CVF International Conf. on Computer Vision\n(ICCV), 2019.\n[4] Borna Beˇsi´c, Nikhil Gosala, Daniele Cattaneo, and Abhinav\nValada. Unsupervised domain adaptation for lidar panop-\ntic segmentation. IEEE Robotics and Automation Letters,\n7(2):3404–3411, 2022.\n[5] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom.\nnuscenes: A multi-\nmodal dataset for autonomous driving. In CVPR, 2020.\n[6] Ming-Fang Chang, John W Lambert, Patsorn Sangkloy, Jag-\njeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter\nCarr, Simon Lucey, Deva Ramanan, and James Hays. Argov-\nerse: 3d tracking and forecasting with rich maps. In Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2019.\n[7] Xieyuanli Chen, Shijie Li, Benedikt Mersch, Louis Wies-\nmann, J¨urgen Gall, Jens Behley, and Cyrill Stachniss. Mov-\ning object segmentation in 3d lidar data: A learning-based\napproach exploiting sequential data. IEEE Robotics and Au-\ntomation Letters, 6(4):6529–6536, 2021.\n[8] Xieyuanli Chen, Benedikt Mersch, Lucas Nunes, Rodrigo\nMarcuzzi, Ignacio Vizzo, Jens Behley, and Cyrill Stach-\nniss. Automatic labeling to generate training data for on-\nline lidar-based moving object segmentation. arXiv preprint\narXiv:2201.04501, 2022.\n[9] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath\nHariharan. Picie: Unsupervised semantic segmentation us-\ning invariance and equivariance in clustering. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 16794–16804, June 2021.\n[10] Christopher Choy, JunYoung Gwak, and Silvio Savarese.\n4d spatio-temporal convnets: Minkowski convolutional neu-\nral networks. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3075–\n3084, 2019.\n[11] Julia Dietlmeier, Joseph Antony, Kevin McGuinness, and\nNoel E O’Connor. How important are faces for person re-\nidentification?\nIn 2020 25th International Conference on\nPattern Recognition (ICPR), pages 6912–6919. IEEE, 2021.\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n[13] Nico Engel, Stefan Hoermann, Philipp Henzler, and Klaus\nDietmayer. Deep object tracking on dynamic occupancy grid\nmaps using rnns.\nIn 2018 21st International Conference\non Intelligent Transportation Systems (ITSC), pages 3852–\n3858. IEEE, 2018.\n[14] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi\nLiu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp,\nCharles R. Qi, Yin Zhou, Zoey Yang, Aur’elien Chouard,\nPei Sun, Jiquan Ngiam, Vijay Vasudevan, Alexander Mc-\nCauley, Jonathon Shlens, and Dragomir Anguelov.\nLarge\nscale interactive motion forecasting for autonomous driv-\ning: The waymo open motion dataset.\nIn Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 9710–9719, October 2021.\n[15] W. Fong, R. Mohan, J. Hurtado, L. Zhou, H. Caesar, O.\nBeijbom, and A. Valada. Panoptic nuscenes: A large-scale\nbenchmark for lidar panoptic segmentation and tracking. In\nICRA, 2022.\n[16] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin\nJaggi.\nUnsupervised scalable representation learning for\nmultivariate time series. Advances in neural information pro-\ncessing systems, 32, 2019.\n[17] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for Au-\ntonomous Driving?\nThe KITTI Vision Benchmark Suite.\nIn Proc. of the IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), pages 3354–3361, 2012.\n[18] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\nDeep Learning.\nMIT Press, 2016.\nhttp://www.\ndeeplearningbook.org.\n[19] Shuo Gu, Suling Yao, Jian Yang, and Hui Kong. Semantics-\nguided moving object segmentation with 3d lidar.\narXiv\npreprint arXiv:2205.03186, 2022.\n[20] Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu,\nand Mohammed Bennamoun.\nDeep learning for 3d point\nclouds: A survey. IEEE transactions on pattern analysis and\nmachine intelligence, 43(12):4338–4364, 2020.\n[21] Austin Harris, Jose Stovall, and Mina Sartipi. Mlk smart cor-\nridor: An urban testbed for smart city applications. In 2019\nIEEE International Conference on Big Data (Big Data),\npages 3506–3511. IEEE, 2019.\n[22] Velodyne\nLidar.\nLidar\nProvides\nAdvanced\nIntelli-\ngence\nto\nNext\nGeneration\nSafety\nand\nSecurity\nAp-\nplications.\nhttps://velodynelidar.com/blog/\nlidar-next-generation-security/, 2020. [On-\nline; accessed 14-March-2022].\n[23] Xingyu Liu,\nCharles R Qi,\nand Leonidas J Guibas.\nFlownet3d: Learning scene flow in 3d point clouds. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 529–537, 2019.\n[24] Kevin Luxem, Falko Fuhrmann, Johannes K¨ursch, Ste-\nfan Remy, and Pavol Bauer. Identifying behavioral struc-\nture from deep variational embeddings of animal motion.\nBioRxiv, 2020.\n[25] Benedikt Mersch, Xieyuanli Chen, Ignacio Vizzo, Lucas\nNunes, Jens Behley, and Cyrill Stachniss. Receding moving\nobject segmentation in 3d lidar data using sparse 4d convo-\nlutions. arXiv preprint arXiv:2206.04129, 2022.\n[26] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 4460–4470, 2019.\n[27] Sambit Mohapatra, Mona Hodaei, Senthil Yogamani, Stefan\nMilz, Patrick Maeder, Heinrich Gotzig, Martin Simon, and\nHazem Rashed. Limoseg: Real-time bird’s eye view based li-\ndar motion segmentation. arXiv preprint arXiv:2111.04875,\n2021.\n[28] Max M¨uhlh¨auser, Christian Meurisch, Michael Stein, J¨org\nDaubert, Julius Von Willich, Jan Riemann, and Lin Wang.\nStreet lamps as a platform. Communications of the ACM,\n63(6):75–83, 2020.\n[29] Dominik Nuss, Stephan Reuter, Markus Thom, Ting Yuan,\nGunther Krehl, Michael Maile, Axel Gern, and Klaus Diet-\nmayer. A random finite set approach for dynamic occupancy\ngrid maps with real-time application. The International Jour-\nnal of Robotics Research, 37(8):841–866, 2018.\n[30] Gheorghii Postica, Andrea Romanoni, and Matteo Mat-\nteucci. Robust moving objects detection in lidar data exploit-\ning visual cues. In 2016 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), pages 1093–1098.\nIEEE, 2016.\n[31] Marcel Schreiber, Vasileios Belagiannis, Claudius Gl¨aser,\nand Klaus Dietmayer. Motion estimation in occupancy grid\nmaps in stationary settings using recurrent neural networks.\nIn 2020 IEEE International Conference on Robotics and Au-\ntomation (ICRA), pages 8587–8593. IEEE, 2020.\n[32] Marcel Schreiber, Vasileios Belagiannis, Claudius Gl¨aser,\nand Klaus Dietmayer.\nDynamic occupancy grid mapping\nwith recurrent neural networks.\nIn 2021 IEEE Interna-\ntional Conference on Robotics and Automation (ICRA),\npages 6717–6724. IEEE, 2021.\n[33] Ehab Shahat, Chang T Hyun, and Chunho Yeom. City digital\ntwin potentials: A review and research agenda. Sustainabil-\nity, 13(6):3386, 2021.\n[34] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\nYuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han,\nJiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Et-\ntinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang,\nJonathon Shlens, Zhifeng Chen, and Dragomir Anguelov.\nScalability in perception for autonomous driving: Waymo\nopen dataset. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), June\n2020.\n[35] Sana\nTonekaboni,\nDanny\nEytan,\nand\nAnna\nGolden-\nberg.\nUnsupervised representation learning for time se-\nries with temporal neighborhood coding.\narXiv preprint\narXiv:2106.00750, 2021.\n[36] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai,\nBin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention\nfor local-global interactions in vision transformers.\narXiv\npreprint arXiv:2107.00641, 2021.\n[37] Xia Yuan, Yangyukun Mao, and Chunxia Zhao. Unsuper-\nvised segmentation of urban 3d point cloud based on lidar-\nimage. In 2019 IEEE International Conference on Robotics\nand Biomimetics (ROBIO), pages 2565–2570. IEEE, 2019.\n[38] Hui Zhou, Xinge Zhu, Xiao Song, Yuexin Ma, Zhe Wang,\nHongsheng Li, and Dahua Lin. Cylinder3d: An effective\n3d framework for driving-scene lidar semantic segmentation.\narXiv preprint arXiv:2008.01550, 2020.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2022-12-30",
  "updated": "2023-01-12"
}