{
  "id": "http://arxiv.org/abs/2301.00802v3",
  "title": "Deep Clustering of Tabular Data by Weighted Gaussian Distribution Learning",
  "authors": [
    "Shourav B. Rabbani",
    "Ivan V. Medri",
    "Manar D. Samad"
  ],
  "abstract": "Deep learning methods are primarily proposed for supervised learning of\nimages or text with limited applications to clustering problems. In contrast,\ntabular data with heterogeneous features pose unique challenges in\nrepresentation learning, where deep learning has yet to replace traditional\nmachine learning. This paper addresses these challenges in developing one of\nthe first deep clustering methods for tabular data: Gaussian Cluster Embedding\nin Autoencoder Latent Space (G-CEALS). G-CEALS is an unsupervised deep\nclustering framework for learning the parameters of multivariate Gaussian\ncluster distributions by iteratively updating individual cluster weights. The\nG-CEALS method presents average rank orderings of 2.9(1.7) and 2.8(1.7) based\non clustering accuracy and adjusted Rand index (ARI) scores on sixteen tabular\ndata sets, respectively, and outperforms nine state-of-the-art clustering\nmethods. G-CEALS substantially improves clustering performance compared to\ntraditional K-means and GMM, which are still de facto methods for clustering\ntabular data. Similar computationally efficient and high-performing deep\nclustering frameworks are imperative to reap the myriad benefits of deep\nlearning on tabular data over traditional machine learning.",
  "text": "Deep Clustering of Tabular Data by Weighted Gaussian\nDistribution Learning\nShourav B. Rabbani, Ivan V. Medri, and Manar D. Samad\nDepartment of Computer Science\nTennessee State University\nNashville, TN, USA\nmsamad@tnstate.edu\nMay 20, 2024\nABSTRACT\nDeep learning methods are primarily proposed for supervised learning of images or text with lim-\nited applications to clustering problems. In contrast, tabular data with heterogeneous features pose\nunique challenges in representation learning, where deep learning has yet to replace traditional ma-\nchine learning. This paper addresses these challenges in developing one of the first deep clustering\nmethods for tabular data: Gaussian Cluster Embedding in Autoencoder Latent Space (G-CEALS).\nG-CEALS is an unsupervised deep clustering framework for learning the parameters of multivari-\nate Gaussian cluster distributions by iteratively updating individual cluster weights. The G-CEALS\nmethod presents average rank orderings of 2.9(1.7) and 2.8(1.7) based on clustering accuracy and\nadjusted Rand index (ARI) scores on sixteen tabular data sets, respectively, and outperforms nine\nstate-of-the-art clustering methods. G-CEALS substantially improves clustering performance com-\npared to traditional K-means and GMM, which are still de facto methods for clustering tabular data.\nSimilar computationally efficient and high-performing deep clustering frameworks are imperative to\nreap the myriad benefits of deep learning on tabular data over traditional machine learning.\nKeywords tabular data, deep clustering, embedding clustering, multivariate Gaussian, autoencoder.\n1\nIntroduction\nDeep learning has replaced traditional machine learning in many data-intensive research and applications due to its\nability to perform concurrent and efficient representation learning and classification. This concurrent learning ap-\nproach outperforms traditional machine learning that requires handcrafted features for classification [1, 2]. However,\nrepresentation learning via supervisory signals from ground truth labels may be prone to overfitting [3] and adversar-\nial attacks [4]. Moreover, human annotations for supervised representation learning and classification can be hard to\nobtain and unavailable in all data domains. Therefore, representation learning via deep unsupervised clustering may\nenable deep learning of vast unlabeled data samples available in practice.\nOne of the ways to overcome the limitations of supervised representation learning is to generate pseudo labels via\nself-supervision, which does not require human-annotated supervisory signals [5, 6]. A self-supervised autoencoder\npreserves input data information in a low-dimensional embedding for data reconstruction. However, the embedding\nresulting from a data reconstruction objective may not be optimal representations for downstream classification or\nclustering tasks [7]. Therefore, deep learning methods have been jointly optimized with a clustering algorithm to\nobtain clustering friendly representations [8, 9, 10, 11, 12]. The existing embedding clustering methods use traditional\nclustering algorithms (e.g., k-means) in joint optimization, assume t-distributed clusters, and benchmark on image data\nsets. While deep representation learning of images is well studied using convolutional neural networks (CNN), similar\nmethods are not well developed for tabular data with heterogeneous feature space. The literature has strong evidence\nthat traditional machine learning outperforms deep models in the supervised learning of tabular data [13, 14, 15, 16,\n17]. However, deep learning methods have not been proposed for clustering tabular data based on a recent survey on\narXiv:2301.00802v3  [cs.LG]  17 May 2024\nA PREPRINT - MAY 20, 2024\ndeep clustering [18]. This paper reviews the assumptions made in the embedding clustering literature to propose a\nnovel deep clustering method for tabular data.\nThe remainder of this manuscript is organized as follows. Section 2 provides a review of the state-of-the-art liter-\nature on deep embedding clustering. Section 3 introduces tabular data and some theoretical contrasts between the\nembeddings for data visualization and clustering to lay the foundation for our proposed method. Section 4 outlines\nthe proposed deep clustering framework for effective cluster representation and assignments. Section 5 summarizes\nthe tabular data sets and experiments for evaluating the proposed deep clustering method. Section 6 provides the ex-\nperimental results and compares the clustering performances between the proposed and nine state-of-the-art clustering\nmethods. Section 7 summarizes the findings with additional insights into the results. The paper concludes in Section\n8.\n2\nRelated work\nA recent survey article reviews deep clustering methods for image, text, video, and graph data without any examples\napplied to tabular data sets [18]. One of the earliest methods for embedding clustering, Deep Embedded Clustering\n(DEC) [8], is inspired by the seminal work on t-distributed stochastic neighborhood embedding (t-SNE) [19]. The DEC\nmethod first trains a deep autoencoder by minimizing the data reconstruction loss. The trained encoder part (excluding\nthe decoder) is then fine-tuned by minimizing the Kullback-Leibler (KL) divergence between t-distributed clusters\non the embedding (Q) and a target distribution (P). The target distribution is a closed-form mathematical expression\nobtained by taking the derivative of the KL divergence loss with respect to P and equating it to zero. Therefore, the\ntarget distribution (P) is also a function of t-distributed Q in similar work. Later, the k-means clustering in the DEC\napproach is replaced by spectral clustering to improve the embedding quality in terms of clustering performance [20].\nThe DEC method is also enhanced by an improved DEC (IDEC) framework [9]. In IDEC, the autoencoder reconstruc-\ntion and the KL divergence losses are jointly minimized to train a pre-trained deep autoencoder. Similar strategies,\nincluding t-distributed clusters, k-means clustering, and KL divergence loss, are adopted in joint embedding and clus-\nter learning (JECL) for multimodal representation learning of text-image data pairs [21]. The Deep Clustering via Joint\nConvolutional Autoencoder (DEPICT) approach learns image embedding via a de-noising autoencoder [22]. Unlike\nearlier methods, the DEPICT method proposes a clustering head with the softmax function to obtain soft cluster as-\nsignments without a distribution assumption. However, their method aims to achieve balanced clusters when cluster\nimbalance is a common problem with tabular data. They demonstrate that a cross-entropy loss can replace the KL\ndivergence in minimizing the difference between P and Q distributions.\nThe embedding clustering literature commonly uses cluster assignments from k-means in a deep learning frame-\nwork [8, 9, 11, 10, 23, 21, 22]. The assumption of t-distributed cluster embedding made in the DEC method [8]\ncontinues to appear in subsequent studies [24, 25, 9, 20, 21, 26, 27, 18]. A t-distribution is parameterized only by\ncluster centroid, whereas a multivariate Gaussian distribution can be used to learn both cluster centroid and covari-\nance. Furthermore, the t-distribution assumption is originally made for neighborhood embedding for the t-SNE data\nvisualization algorithm [19]. We argue that the distribution assumption for data visualization may not optimally satisfy\nthe requirements for clustering.\nFurthermore, recent deep clustering methods optimized and improved on image data sets alone may not be optimal or\neven suitable for tabular data with heterogeneous feature space [28, 29, 27, 30, 31, 32, 33, 34]. Some of these models\nuse CNN-based large learning architectures for clustering large image data sets [28, 29, 31]. However, these large\nimage-based CNN architectures are not suitable for learning tabular data sets with heterogeneous features. Some of\nthese methods are sensitive to the values selected for multiple hyperparameter [31, 34]. In several studies, class labels\nare leveraged to identify an early stopping point while pretraining the autoencoder, which may violate the unsupervised\nnature of clustering algorithms [27, 34]. A few methods also used class labels in a semi-supervised step to perform\nclustering [32, 33], which may go against the unsupervised nature of clustering algorithms. There is a need for deep\nclustering methods for tabular data addressing these methodological shortcomings.\n2.1\nContributions\nThis paper proposes the first method for deep embedding clustering of tabular data to the best of our knowledge by\naddressing the shortcomings of state-of-the-art deep clustering methods. First, we replace the current assumption of t-\ndistributed embedding with a mixture of multivariate Gaussian distributions for multivariate tabular data by providing\na theoretical underpinning for this choice. Second, a new embedding clustering algorithm is proposed that can learn\nthe cluster embedding and assignments without requiring the aid of a traditional clustering method. Third, multivariate\ncluster centroids and covariance matrices are updated as trainable parameters, and the cluster distributions are adjusted\n2\nA PREPRINT - MAY 20, 2024\nby individual cluster weights to learn imbalanced tabular data better. Fourth, a deep autoencoder directly learns cluster\ndistributions using a dynamic distribution target instead of setting a mathematically closed-form target distribution or\na KL divergence loss.\n3\nTheoretical background\nThis section provides preliminaries on tabular data compared to commonly used benchmark image data sets. We draw\nmultiple contrasts between the neighborhood embedding proposed for data visualization and the deep embedding\nrequired for clustering to underpin our proposed method.\n3.1\nPreliminaries\nA tabular data set is represented in a matrix X ∈ℜn×d with n i.i.d samples in rows. Each sample (Xi) is represented\nby a d-dimensional feature vector, Xi ∈ℜd = {x1, x2, ..., xd}, where i = {1, 2, ..., n}. Compared to a homo-\ngeneous pixel distribution P(I) of an image I, tabular data contain multivariate distributions P(x1, x2, ..., xd) of\nheterogeneous features in relatively much lower dimensions. One popular example of tabular data is Electronic Health\nRecords (EHR), where individual patient follow-ups are characterized by heterogeneous clinical variables (e.g., heart\nrate, blood pressure, height) [35]. EHR data have been used to cluster patients of varying risk levels using one of the\ndeep clustering methods (IDEC) [36]. Tabular data are also used in missing value imputation literature, where cluster\nlabels are used to facilitate the imputation of missing values in heterogeneous samples [37].\nTable 1 shows several important contrasts between image and tabular data. One may argue that some high-dimensional\nsequential data, such as genomics or two-dimensional images converted to pixel vectors, can be structured in a data\ntable. However, these vector representations still include regularity or homogeneity in patterns that do not pose the\nchallenge of heterogeneity of tabular data. Therefore, tabular data with a heterogeneous feature space fail to take\nadvantage of deep learning methods because image-like sequential or spatial regularities are absent. Furthermore, the\ncurrent literature selectively uses data sets with high dimensionality and large sample sizes to demonstrate the effec-\ntiveness of deep learning methods [38, 39, 40]. In contrast, the most commonly available tabular data sets have limited\nsample sizes and dimensions (Table 1), which are rarely considered in deep representation learning. Consequently,\ntabular data sets are identified as the last unconquered castle for deep learning [16], where traditional machine learning\nmethods still appear competitive against deep neural network architectures [16, 15].\nFactors\nImage data\nTabular data\nHeterogeneity\nHomogeneous\nHeterogeneous\npixels\nvariables\nSpatial Regularity\nYes\nNo\nSample size\nLarge, >50,000\nSmall, median∼660\nBenchmark data sets\nMNIST, CIFAR\nNone\nData dimensionality\nHigh, >1000\nLow, median 18\nBest method\nDeep CNN\nTraditional\nmachine learning\nTable 1: Contrasts between image and tabular data require different deep learning architectures for tabular data.\nMedian sample size and data dimensionality are obtained from the 100 most downloaded tabular data sets from the\nUCI machine learning repository [41].\n3.2\nEmbedding for data visualization\nA neighborhood embedding is a low-dimensional map that preserves the similarity between data points (xi and xj)\nobserved in a higher dimension. Maaten and Hinton propose a Student’s t-distribution to model the similarity between\nsamples in neighborhood embedding (zi, zj) of high-dimensional data points (xi and xj) for data visualization [19].\nFirst, the similarity between two sample points (xi and xj) in the high dimension is modeled by a Gaussian distribution,\npij in Equation 1. Similar joint distribution can be defined for a pair of points in the low-dimensional embedding (zi,\nzj) as qij in Equation 2.\npij =\nexp(−||xi −xj||2/2σ2)\nP\nk̸=l exp(−||xk −xl||2/2σ2),\n(1)\nqij =\nexp(−||zi −zj||2/2σ2\nP\nk̸=l exp(−||zk −zl||2/2σ2)\n(2)\n3\nA PREPRINT - MAY 20, 2024\n30\n20\n10\n0\n10\n20\nProjected Space 1\n20\n10\n0\n10\n20\nProjected Space 2\nCNV\nDME\nDrusen\nNormal\n(a) t-SNE projected\n50\n0\n50\n100\n150\nProjected Space 1\n50\n0\n50\n100\n150\nProjected Space 2\nCNV\nDME\nDrusen\nNormal\n(b) PCA projected\nFigure 1: Two-dimensional embeddings of high dimensional image features extracted from a deep convolutional neural\nnetwork obtained from [42].\nt-SNE [19]\nDEC [8] or IDEC [9]\nPurpose\nEmbedding for data visualization in d=2\nEmbedding for clustering in d> 2\nLow-dimensional\nSampled from Gaussian\nAutoencoder\nembedding (zi)\nwith low σ2\nlatent space\nDistance or similarity\nBetween sample\nBetween point & cluster\nmeasure\npoints (xi, xj)\ncentroid (xi, µj)\nEmbedding\nt-distribution,\nt-distribution,\ndistribution (qij)\nα = 1\nα = 1\nTarget\nGaussian in high-dimensional\nA function of\ndistribution (pij)\nspace (x)\nt-distributed qij\nLearning\nzi+1 = zi + d KLD(p,q)\nd(zi)\nwi+1 = wi + d KLD(p,q)\nd(w)\nTable 2: Comparison between neighborhood embedding proposed for t-SNE data visualization [19] and cluster em-\nbedding proposed in DEC [8] inspired by t-SNE. α = degrees of freedom of t-distribution, d = dimension of low-\ndimensional embedding. W represents the trainable parameter of an autoencoder.\nNotably, the low-dimensional embedding here is not obtained via a neural network as seen in deep embedding clus-\ntering. The divergence between the target (pij) and embedding (qij) distributions is measured using a KL divergence\nloss, which is minimized to optimize the final t-SNE embedding iteratively.\nKL (P||Q) =\nX\ni\nX\nj\npijlog pij\nqij\n(3)\nTo facilitate high-dimensional data visualization in two dimensions (2D), the embedding distribution (qij) is modeled\nby a Student’s t-distribution in Equation 4. One primary justification for t-distribution is its heavier tails compared to\na Gaussian distribution. A heavier tail aids in efficiently mapping outliers observed in high-dimensional input data\nspace to a lower 2D space for data visualization.\nqij =\n(1 + ||zi −zj||)−1\nP\nk̸=l(1 + ||zk −zl||)−1\n(4)\nTherefore, data points placed at a moderate distance in high-dimension are pulled farther by a t-distribution to aid\ndata visualization in 2D space. In the context of cluster embedding, we argue that the additional separation between\npoints in low dimensions may alter their cluster assignments. This phenomenon is observed in Figure 1 where high-\ndimensional deep image features are mapped on to 1) t-SNE and 2) two principal component spaces. The scattering of\ndata points is evident in the t-SNE mapping (Figure 1 (a)), where one blue point appears on the left side of the figure,\ncorrupting its cluster assignment, unlike the PCA mapping (Figure 1 (b)). This observation is in line with the contrats\nbetween data visualization and clustering presented in Table 2.\n3.3\nEmbedding for clustering\nEmbedding for clustering is achieved by infusing cluster separation information into the low-dimensional latent space\nof a deep neural network.\n4\nA PREPRINT - MAY 20, 2024\nWhile neighborhood embedding is initialized by sampling from a Gaussian distribution, cluster embedding methods\nuse embedding learned from an autoencoder’s latent space (Zi ∈ℜm, where m ≪d). However, the current cluster\nembedding methods use the same t-distribution (Equation 4) to define the embedding distribution (qij), similar to\nneighborhood embedding. The target distribution (pij) is derived as a function of qij, as shown below.\nsij =\nq2\nij\nP\nj qij\n, pij =\nsij\nP\nj sij\n.\n(5)\nWhile pair-wise sample distances in neighborhood embedding have a complexity of O (N 2), the distances from the\ncluster centroids in embedding are O(N*K). Here, K is the number of clusters, much smaller than the number of sam-\nples (N). While an outlier point results in N large distances (extremely small pij values) in neighborhood embedding,\nthere will be much fewer (K<<N) of those large distances in cluster embedding. Therefore, the effect of outliers on\ncluster embedding can be much lower than the assumption made in neighborhood embedding for data visualization.\nSubsequently, the soft cluster assignment of the i −th sample to the j −th cluster (sij) can be obtained using a\nGaussian kernel as below, which is the negative exponent of the Mahalanobis distance (dij) (Equation 6) between the\npoint (Zi) and the j-th cluster centroid (Equation 7).\ndij\n=\nq\n(Zi −µj) Σ−1\nj\n(Zi −µj)T\n(6)\nsij\n=\nexp (−dij)\n(7)\nThe soft cluster assignments for each sample are then normalized to get cluster likelihood distribution, p′\nij (Equation\n8). The cluster prior probability is initialized as 1/K as equally weighted clusters (ωj). Subsequently, (ωj) is updated\nfrom the second training epoch using p′\nij estimated from the previous epoch, as shown below.\np′\nij =\nsij\nP\nj sij\n,\nwj = 1\nN\nN\nX\ni=1\np′\nij\n(8)\n4\nProposed method\nWe propose a novel deep clustering method, Gaussian Cluster Embedding in Autoencoder Latent Space (G-CEALS),\nas follows. First, a multivariate Gaussian distribution replaces the widely used t-distribution (Equation 4). Unlike\nt-distribution, Gaussian distribution can regulate the cluster variance or scatter. Deep learning methods involving\nmixtures of Gaussian distributions have previously shown promising results in anomaly and error detection tasks [43,\n44]. Howeever, similar methods are not adjusted for the imbalances in multiple Gaussian distributions or clusters.\nThe clustering on embedding (Zi ∈ℜm) yields K cluster assignments for individual samples. Each cluster j is\nmodeled by a centroid vector (µj ∈ℜm) and a covariance matrix (Σj ∈ℜm×m). Second, the proposed deep learning\nframework learns cluster distributions by updating µj and diagonals of Σj as trainable parameters. Therefore, the deep\nlearning framework learns Gaussian distributions on embedding space. The distribution parameters are initialized\nusing K-means clustering means and by setting Σj to an identity matrix, as shown in Figure 2.\nWe know from the Bayes’ rule that the product of the likelihood distribution p′\nij and prior probability ωj is proportional\nto the posterior probability of a sample belonging to a cluster, as shown in Equation 9.\npij\n=\nωj × p′\nij\n(9)\nAdditionally, a two-layer MLP head f(·) projects the input embedding space (Zij), which is transformed into soft\ncluster assignments qij via a softmax layer, as shown in Equation 10.\nqij\n=\nexp (f(Zi,j))\nP\nj exp (f(Zi,j)\n(10)\nThe third important contribution of the proposed method is that the cluster (P) and target (Q) distributions are inde-\npendently defined and dynamically updated, unlike existing embedding clustering methods.\n4.1\nOptimizing deep embedding for clustering\nAn autoencoder is trained to encode the input (X ∈ℜn×m) into a latent embedding (Z ∈ℜn×d), which is then\ndecoded to reconstruct the original input ( ˆ\nXi), as shown in the autoencoder’s reconstruction loss below.\nLrecon = argmin\nθ,Φ\n1\nN\nN\nX\ni=1\n||Xi −ˆ\nXi||2\n2.\n(11)\n5\nA PREPRINT - MAY 20, 2024\nEncoder\nDecoder\nCluster Loss\nReconstruction \nLoss\nLinear Layer\nReLU Activation\nACC\nARI\nMLP Head\nClustering Module\nEq. 9\nEq. 10\nFigure 2: Proposed deep clustering framework for tabular data. All samples of an unlabeled tabular data set are used to\ntrain the autoencoder in tandem with two subnetworks: a clustering module and an MLP head with a softmax output\nlayer. The final cluster distribution (P) and assignments are obtained after the clustering module. The final cluster\nassignments are evaluated using ACC, ARI, and NMI performance metrics.\nHere, θ and Φ denote the trainable parameters of the encoder and decoder, respectively. Notably, sophisticated au-\ntoencoder architectures (e.g., stacked, denoising, convolutional, variational) and popular learning tricks (e.g., data\naugmentation, dropout learning) may overshadow the contribution of a learning algorithm. Therefore, we use a stan-\ndard autoencoder architecture without data augmentation and dropout learning. Similar autoencoders have been found\neffective in the supervised classification of tabular data [45, 7]. The pretrained autoencoder network is subsequently\nfine-tuned to yield cluster-friendly embedding and cluster assignments. A clustering module with trainable parameters\n(µ and Σ) computes the cluster distribution (P) using Equation 9. A cross-entropy loss involving P and Q distribu-\ntions is minimized as the clustering loss in Equation 12. The cross-entropy loss induces separation between clusters\ninstead of collapsing to cluster centroids.\nLcluster\n=\n1\nN\nN\nX\ni=1\nK\nX\nj=1\npij log qij\n(12)\nThe pretrained autoencoder is jointly trained with the clustering loss to learn cluster distributions in the latent space\nusing Equation 13.\nL\n=\nLrecon + γ ∗Lcluster\n(13)\n=\nargmin\nθ,Φ,µ,Σ\n1\nN\nN\nX\ni=1\n||Xi −ˆ\nXi||2\n2 + γ ∗1\nN\nN\nX\ni=1\nK\nX\nj=1\npij log qij\nHere, γ is a hyperparameter to balance the contribution of clustering loss in joint learning. The effect of γ values on\nthe clustering accuracy is evaluated later in an ablation study. The computational steps of the proposed framework are\npresented in Algorithm 1.\n4.2\nCluster imbalance and convergence\nThe convergence of the proposed cluster loss is important in addition to ensuring the cluster separation after training.\nFigure 3 shows smooth convergence of clustering loss for two different γ values. A larger γ value (1.0) speeds up the\nconvergence. However, a lower γ value (0.1) helps with stable and smooth convergence at a slower pace. Therefore, a\nγ value of 0.1 is chosen for models that require this hyperparameter.\nThe effectiveness of the proposed deep clustering method in creating cluster separation is visualized using t-SNE plots\nin Figure 4. The cluster visualization identifies an issue where minority clusters may merge with majority ones after\ntraining for a long period without an early stopping. The merging of clusters happens due to cluster imbalance in\ntabular data sets. We address this issue by adopting two strategies. First, we implement an early stopping criterion\nbased on the cluster weight ωj updates, which is a measure of cluster size. The cluster weight becomes zero when it\nmerges with another cluster during training, which can be prevented by setting a threshold on cluster weight. When\nthere are K balanced clusters, the weights initially take a value of 1/K. We stop the training when at least one of the\ncluster weights drops below 50% of 1/K to prevent possible cluster merging.\n6\nA PREPRINT - MAY 20, 2024\nAlgorithm 1 Proposed G-CEALS Deep Clustering Algorithm\n1: Input: d-dimensional tabular data, X ∈ℜn×d\n2: Output: Cluster-friendly embedding, Z ∈ℜn×m, m≪d and soft cluster assignments (qij)\n3: Pre-train autoencoder ({Wencoder, Wdecoder}) ←X\n4: Embedding (Z) ←Encoder (X, Wencoder)\n5: Initialize pseudo-labels: ˜Y ←k-means(Z)\n6: Initialize j-th cluster parameters: µj ←k-means(Z), Σj ←I, ωj ←1/k\n7: Trainable cluster distribution parameters: Wcluster ←{[µ1, µ2, ..., µk], [Σ1, Σ2, ..., Σk] }\n8: Initialize: W 0 ={Wencoder, Wdecoder, Wcluster, WMLP }\n9: for t = 1 →n epochs do\n10:\nXb ←Sample mini-batch from X for uniform class distribution\n11:\nZt ←Encoder (Xb, W t\nencoder), ˆ\nXb ←Decoder (Zt, W t\ndecoder)\n12:\npij ←(Zt, W t\ncluster) using Eq. 9\n13:\nqij ←(Zt, W t\nMLP ) using Eq. 10\n14:\nL ←Lrecon + γ ∗Lcluster, Eq. 13\n15:\nW t+1 ←W t - α∇W tL, update trainable parameters minimizing the joint loss in Eq. 13\n16:\np′\nij ←(Z, W t+1\ncluster), Z ←Encoder (X, W t+1\nencoder) using Eq. 8\n17:\nωj ←update using Eq. 8\n18:\nif ωj ≤1/2k then\n19:\nStop training\n20:\nend if\n21: end for\nOpenML\nName\nSamples\nNumerical\nCategorical\nFeature dimension\nClasses\nF-S ratio\nC-score\n1063\nKc2\n522\n21\n0\n21\n2\n4.023\n0.557\n40994\nClimate-model-simulation-crashes\n540\n20\n0\n18\n2\n3.333\n0.000\n1510\nWdbc\n569\n30\n0\n30\n2\n5.272\n0.101\n1480\nIlpd\n583\n9\n1\n11\n2\n1.887\n0.036\n11\nBalance-scale\n625\n4\n0\n4\n3\n0.640\n0.000\n37\nDiabetes\n768\n8\n0\n8\n2\n1.042\n0.000\n469\nAnalcatdata dmft\n797\n0\n4\n21\n6\n2.635\n0.005\n458\nAnalcatdata authorship\n841\n70\n0\n70\n4\n8.323\n0.000\n1464\nBlood-transfusion-service-center\n748\n4\n0\n4\n2\n0.535\n0.167\n1068\nPc1\n1109\n21\n0\n21\n2\n1.894\n0.271\n1049\nPc4\n1458\n37\n0\n37\n2\n2.538\n0.063\n23\nCmc\n1473\n2\n7\n24\n3\n1.629\n0.011\n1050\nPc3\n1563\n37\n0\n37\n2\n2.367\n0.176\n40975\nCar\n1728\n0\n6\n21\n4\n1.215\n0.000\n40982\nSteel-plates-fault\n1941\n27\n0\n27\n7\n1.391\n0.046\n1067\nKc1\n2109\n21\n0\n21\n2\n0.996\n0.481\nTable 3: Summary of sixteen tabular data sets used in this study. The feature dimension combines numerical and one-\nhot encoded categorical features. F-S ratio is the ratio of features to samples. C-scores represent the mean absolute\ncorrelations across all feature pairs.\nSecond, we use mini-batch gradient descent to optimize the deep clustering model. However, mini-batches may not\ncontain all cluster samples when the data set is imbalanced, leading to inflated or biased clustering accuracy. We use\nK-means clustering to obtain the pseudo-labels and identify the minority cluster. If the minority cluster has nmin\nsamples, we randomly choose an equal number of samples from other clusters to form a batch size of 256 or lower.\nThis random sampling is performed at every epoch and repeated 1000 times to train the model. Therefore, the batch\nsize varies across the data sets depending on the size of the minority cluster.\nThe convergence of three cluster parameters: mean vectors, covariance matrices, and cluster weights is presented in\nFigure 5 for a two-cluster clustering problem. For better visualization, we use the L2 norm distance between two\nconsecutive mean vector updates and the determinant of the covariance matrices.\n5\nExperiments\nThis section identifies the tabular data sets, baseline algorithms, and metrics used to evaluate the performance of our\nproposed deep clustering method.\n7\nA PREPRINT - MAY 20, 2024\n5.1\nTabular data sets\nAll methods are evaluated on a diverse set of 16 tabular datasets sourced from OpenML-CC18 [46]. Table 3 summa-\nrizes 16 tabular data sets representing various application domains and a wide range of data statistics. The heterogene-\nity of tabular data sets is further characterized by F-S ratio and C-scores in the Table. FS-ratio represents the feature\nand sample ratio of the data set. The C-score provides a measure of feature correlations. It shows the mean of the\nabsolute correlations across all features. It is notable that prior studies on tabular data classification (not clustering)\nselectively use data sets with very large sample sizes [38, 39, 40]. In practice, most tabular data domains include\nlimited samples and features with or without the presence of categorical variables, unlike image data sets.\n5.2\nAdapting baseline methods to tabular data\nRecent surveys on deep clustering methods show no examples of clustering tabular data sets [18, 47]. Deep embedding\nclustering methods have been invariably designed and evaluated on benchmark image data sets. Therefore, existing\ndeep clustering methods may not be ideal baselines for tabular data due to the data-centric contrasts presented in\nTable 1.\nThe DEC [8] and IDEC [9] methods use a fully-connected autoencoder architecture as d–500–500–2000–10-2000-\n500-500-d. The deep k-means (DKM) [10] and AE-CM [5] methods use the same learning architecture after replacing\nthe fixed dimensionality of the embedding (10) with the number of target clusters (k). The dynamic autoencoder\n(DynAE) uses the same architecture as DEC/IDEC [11]. However, the objective function is regularized by image\naugmentation (shifting and rotation), which has to be disabled for tabular data in this paper. Several other methods are\nbuilt on convolutional neural network (CNN) architectures [48, 22], whereas fully connected neural networks are the\ndefault choice for tabular data. For example, Caron et al. have learned visual features from images using AlexNet and\nVGG-16 after Sobel filtering for color removal and contrast enhancement [6], which are not applicable to tabular data.\nTheir deepCluster architecture has five convolutional layers with up to 384 2D image filters to learn image features.\nThe transfer learning of tabular data using the image-pre-trained VGG-16 model is not trivial. The DEPICT method\nuses a convolutional denoising autoencoder for reconstructing original images from corrupted images [22]. Instead,\nwe use a standard convolutional autoencoder replacing the 2D filters with 1D kernels to learn embedding for tabular\ndata vectors because image denoising is not reproducible on tabular data.\nAll these methodological aspects are considered in adapting seven state-of-the-art deep clustering methods (DEC,\nIDEC, AE-CM, DynAE, DEPICT, DKM, DCN) as baseline deep clustering methods for tabular data sets, including\ntwo traditional clustering methods (k-means) and Gaussian Mixture Model (GMM).\n5.3\nEvaluation\nThe proposed deep clustering model training entails self-supervised data reconstruction and cluster distribution learn-\ning without involving ground truth labels. The quality of cluster embedding is evaluated in downstream clustering\nusing clustering accuracy (ACC) [49], as shown in Equation 14.\nACC = max\nm\nPN\ni=1 1{ytrue(i) = m(ypred(i))}\nN\n(14)\nHere, ytrue(i) is the ground truth class label of the i-th sample. ypred(i) is the predicted cluster label. m() finds\nthe best label mapping between the cluster and ground truth labels. The mapping can be obtained by the Hungarian\nalgorithm [49]. Notably, a cross-validation of the model is not considered in unsupervised learning where class labels\nare not involved in the training process. Therefore, we use all data samples in model training, similar to training and\nevaluating any clustering algorithms [8, 9]. The clustering accuracy takes a value between 0 (failure) and 1 (perfect\nclusters). In all experiments, the cluster number is set to the number of class labels in a given data set. The accuracy\nscores are multiplied by 100 to represent the numbers in percentage.\nLikewise, Adjusted Rand Index (ARI) quantifies the similarity between the true and predicted clusters while consider-\ning chance, as shown in Eq. 15. ARI adjusts for randomness or the expected similarity due to random chance, as shown\nbelow. ARI is particularly useful when the number of clusters is not known in advance. Its value ranges between -0.5\nand +1. ARI scores close to 0.0 indicate random cluster labels, and a higher positive value suggests better concordance\nbetween the true and predicted clusters.\nARI =\nRI −E(RI)\nmax(RI) −E(RI),\nRI = TP + TN\nNC2\n.\n(15)\n8\nA PREPRINT - MAY 20, 2024\n0\n200\n400\n600\n800\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nLoss\nReconstruction loss, Lrecon\nCluster loss, Lcluster\n(a) γ = 0.1\n0\n200\n400\n600\n800\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nLoss\nReconstruction loss, Lrecon\nCluster loss, Lcluster\n(b) γ = 1.0\nFigure 3: The reconstruction and clustering losses are obtained using the tabular data set with ID 1510 for two γ\nvalues. A higher γ value results in faster convergence of the clustering loss, slowing the reconstruction loss. However,\na lower value is preferred to ensure smooth convergence of the cluster parameters and autoencoder weights.\nHere, TP and TN denote the true positive and true negative pairs, NC2 denotes the number of possible pairs, and E(RI)\nis the expected Rand index. For multiclass classification, TP and TN are determined using the one-vs-all scheme.\n6\nResults\nAll experiments are conducted on a Dell Precision 5820 workstation running Ubuntu 20.04 with 64GB RAM and\nan NVIDIA Quadro RTX A5000 GPU with 16GB memory. We standardize numerical features using their mean\nand standard deviation and one-hot encode categorical features before model training. The source code is publicly\navailable on github1.\n6.1\nLearning architecture and implementation\nAll algorithms are implemented in Python. The proposed deep learning method is developed using the PyTorch\npackage, whereas traditional methods are implemented using scikit-learn. The baseline implementations are obtained\nfrom their respective GitHub repositories. Specifically, the DEPICT algorithm is implemented using the Theano\npackage, and other methods are implemented using the TensorFlow or Keras package.\nAs mentioned in Section 5.2, all baseline deep clustering algorithms are benchmarked on image datasets. Therefore,\nminimal modifications are made to the source code to enable the input and learning of tabular data sets in place of\nimage data. All methods, including the proposed one, use a fully connected autoencoder of the same architecture\n(d-500-500-2000-m-2000-500-500-d), where m is the embedding dimension. The DKM method presets the value m\nto the number of clusters (k). The adapted DEPICT method uses a CNN-based architecture with 1D filters for tabular\ndata. For all experiments, the learning rate is set to 0.001 with an Adam optimizer and using a batch size of 256.\nEach method pretrains an autoencoder for 1000 epochs and then fine-tunes jointly with the clustering loss (Eq. 13) for\nanother 1000 epochs.\nAn obvious benefit of deep learning methods over traditional clustering is its flexible embedding size. Therefore,\nthe embedding dimension is varied from 5 to 20 at five intervals (5, 10, 15, and 20) for each deep learning method\nand data set pair, considering the heterogeneity in tabular data sets and features. However, the DMK method sets\nthe embedding size equal to the number of clusters. The deep clustering methods are compared on the embedding\ndimension that yields the best clustering performance for a given data set.\n6.2\nCluster imbalance and convergence\nThe convergence of the proposed cluster loss is important in addition to ensuring the cluster separation after training.\nFigure 3 shows smooth convergence of clustering loss for two different γ values. A larger γ value (1.0) speeds up the\nconvergence. However, a lower γ value (0.1) helps with stable and smooth convergence at a slower pace. Therefore, a\nγ value of 0.1 is chosen for models that require this hyperparameter.\nThe effectiveness of the proposed deep clustering method in creating cluster separation is visualized using t-SNE plots\nin Figure 4. The cluster visualization identifies an issue where minority clusters may merge with majority ones after\ntraining for a long period without an early stopping. The merging of clusters happens due to cluster imbalance in\n1https://github.com/mdsamad001/G-CEALS—Deep-Clustering-for-Tabular-Data/\n9\nA PREPRINT - MAY 20, 2024\n30\n20\n10\n0\n10\n20\n30\ntsne_0\n20\n10\n0\n10\n20\n30\ntsne_1\nY = 0\nY = 1\nY = 2\nY = 3\n(a) Before clustering\n40\n30\n20\n10\n0\n10\n20\ntsne_0\n40\n30\n20\n10\n0\n10\n20\n30\ntsne_1\nY = 0\nY = 1\nY = 2\nY = 3\n(b) Clustering without early stop\n20\n10\n0\n10\n20\n30\ntsne_0\n30\n20\n10\n0\n10\n20\n30\ntsne_1\nY = 0\nY = 1\nY = 2\nY = 3\n(c) Clustering with early stop\nFigure 4: t-SNE visualization of deep clustering of data set ID 458 with and without early stopping. While the deep\nclustering method facilitates cluster separation, it may merge the minority clusters due to the cluster imbalance in\ntabular data.\n0\n250\n500\n750\n1000\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\nt\nj\nt\n1\nj\n2\n2\n1e\n5\n0\n1\n(a) Cluster centroid updates\n0\n250\n500\n750\n1000\nEpoch\n0.0\n0.5\n1.0\n1.5\nDeterminant of \nj\n0\n1\n(b) Determinant of cluster covariances\n0\n250\n500\n750\n1000\nEpoch\n0.3\n0.4\n0.5\n0.6\n0.7\nCluster weights, \nj\n0\n1\n(c) Cluster weights\nFigure 5: Convergence of cluster centroids (µj), cluster covariances (Σj), and cluster weights (ωj) for two clusters\nusing dataset ID 1510. Here, t represents epoch, and j is the cluster index.\ntabular data sets. We address this issue by adopting two strategies. First, we implement an early stopping criterion\nbased on the cluster weight ωj updates, which is a measure of cluster size. The cluster weight becomes zero when it\nmerges with another cluster during training, which can be prevented by setting a threshold on cluster weight. When\nthere are K balanced clusters, the weights initially take a value of 1/K. We stop the training when at least one of the\ncluster weights drops below 50% of 1/K to prevent possible cluster merging.\nSecond, we use mini-batch gradient descent to optimize the deep clustering model. However, mini-batches may not\ncontain all cluster samples when the data set is imbalanced, leading to inflated or biased clustering accuracy. We use\nK-means clustering to obtain the pseudo-labels and identify the minority cluster. If the minority cluster has nmin\nsamples, we randomly choose an equal number of samples from other clusters to form a batch size of 256 or lower.\nThis random sampling is performed at every epoch and repeated 1000 times to train the model. Therefore, the batch\nsize varies across the data sets depending on the size of the minority cluster.\nThe convergence of three cluster parameters: mean vectors, covariance matrices, and cluster weights is presented in\nFigure 5 for a two-cluster clustering problem. For better visualization, we use the L2 norm distance between two\nconsecutive mean vector updates and the determinant of the covariance matrices.\n6.3\nEmbedding dimension for clustering\nImages are high-dimensional data and are conventionally projected onto a low-dimensional embedding for effective\nseparation of class or clusters. Unlike image data, the feature dimension of tabular data can be considerably low and\nheterogeneous. In the absence of an effective feature extractor (like CNN used for images), it is unknown if a tabular\ndata set can be better clustered at a higher or lower dimension than its original feature space. Therefore, the optimal\nembedding size for clustering tabular data may vary across data sets or deep clustering methods. We identify the\nembedding dimension that yields the best clustering performance for a given deep clustering method and tabular data\nset. We observe that low-dimensional tabular data sets can benefit from learning a higher dimensional embedding\nusing an overcomplete autoencoder (latent space larger than the input space). Therefore, we use these best embedding\ndimensions to compare our proposed and baseline deep clustering methods in subsequent sections.\n10\nA PREPRINT - MAY 20, 2024\nDataset ID\nK-means(X)\nGMM(X)\nK-means(Z)\nGMM(Z)\nDEC\nIDEC\nDEPICT\nDynAE\nDCN\nAE-CM\nDKM\nG-CEALS\n1063\n80.3\n80.3\n68.0\n72.8\n67.0\n66.9\n66.1\n79.5\n80.3\n79.6\n79.6\n81.0\n40994\n52.4\n53.1\n57.0\n61.1\n57.0\n54.8\n50.4\n60.4\n64.6\n91.5\n60.0\n85.9\n1510\n91.2\n94.0\n92.1\n89.5\n91.4\n93.1\n93.7\n58.0\n77.7\n62.7\n80.2\n95.6\n1480\n61.4\n53.5\n64.0\n64.0\n59.3\n58.7\n62.1\n64.0\n63.7\n71.3\n65.8\n65.2\n11\n51.2\n52.6\n52.3\n55.8\n51.4\n52.3\n52.8\n51.7\n57.6\n46.1\n47.6\n69.3\n37\n66.1\n58.5\n73.3\n72.8\n71.4\n71.2\n67.4\n67.4\n64.4\n65.1\n66.4\n74.1\n469\n21.8\n21.5\n21.3\n21.6\n21.5\n21.3\n21.0\n22.3\n22.6\n20.5\n21.9\n22.0\n458\n98.2\n98.5\n83.6\n87.4\n81.5\n83.1\n78.8\n99.9\n51.0\n49.5\n64.5\n89.8\n1464\n68.3\n57.6\n73.9\n69.1\n66.4\n66.4\n56.1\n60.0\n74.1\n76.2\n64.3\n74.1\n1068\n89.2\n71.0\n92.3\n86.8\n82.5\n81.2\n59.0\n93.0\n93.0\n93.1\n93.1\n92.5\n1049\n81.9\n72.2\n84.3\n72.2\n70.0\n69.3\n66.5\n70.5\n87.1\n87.9\n87.5\n85.3\n23\n42.4\n39.6\n42.0\n43.4\n44.3\n43.8\n42.0\n38.8\n40.7\n42.7\n36.9\n44.2\n1050\n87.3\n75.0\n66.3\n76.2\n58.6\n62.5\n57.9\n89.7\n89.0\n89.7\n64.9\n83.0\n40975\n31.2\n31.2\n31.1\n34.7\n31.1\n31.1\n38.3\n46.1\n56.5\n70.0\n36.1\n49.2\n40982\n39.0\n37.5\n45.2\n46.1\n44.4\n44.8\n36.6\n47.6\n38.0\n36.6\n44.9\n41.3\n1067\n83.3\n72.6\n85.3\n77.0\n80.1\n79.3\n62.1\n77.3\n84.7\n84.7\n85.3\n85.2\nAvg. Rank\n6.7 (2.5)\n7.9 (3.6)\n5.8 (2.6)\n5.6 (2.2)\n7.6 (2.6)\n7.9 (2.7)\n9.2 (3.3)\n5.8 (3.7)\n5.1 (3.7)\n5.6 (4.8)\n6.2 (3.6)\n2.9 (1.7)\nOverall Rank\n8\n11\n5\n3\n9\n10\n12\n6\n2\n4\n7\n1\nTable 4: Comparison of clustering accuracy between the proposed G-CEALS method and baseline traditional or deep\nclustering methods on sixteen tabular data sets.\nDataset ID\nK-means(X)\nGMM(X)\nK-means(Z)\nGMM(Z)\nDEC\nIDEC\nDEPICT\nDynAE\nDCN\nAE-CM\nDKM\nG-CEALS\n1063\n0.044\n0.044\n0.127\n0.193\n0.115\n0.112\n0.102\n0.215\n0.192\n0.082\n0.007\n0.335\n40994\n0.001\n0.003\n0.017\n0.031\n0.017\n0.009\n-0.010\n0.006\n0.021\n0.000\n0.004\n0.029\n1510\n0.677\n0.774\n0.707\n0.620\n0.684\n0.744\n0.762\n0.020\n0.295\n0.000\n0.347\n0.831\n1480\n-0.072\n-0.031\n0.043\n0.031\n0.034\n0.029\n0.048\n0.031\n0.006\n-0.001\n-0.020\n0.033\n11\n0.139\n0.135\n0.140\n0.147\n0.130\n0.140\n0.120\n0.076\n0.097\n0.000\n0.083\n0.296\n37\n0.100\n0.013\n0.211\n0.205\n0.181\n0.179\n0.121\n0.110\n0.050\n0.000\n0.077\n0.200\n469\n0.004\n0.003\n0.007\n0.006\n0.007\n0.007\n0.006\n0.006\n0.009\n0.001\n0.006\n0.006\n458\n0.951\n0.959\n0.695\n0.766\n0.650\n0.705\n0.747\n0.996\n0.178\n0.060\n0.400\n0.786\n1464\n0.034\n-0.048\n0.077\n0.126\n0.061\n0.066\n0.013\n0.014\n0.009\n0.000\n0.024\n0.079\n1068\n0.184\n0.060\n0.157\n0.176\n0.105\n0.110\n0.025\n0.030\n0.080\n0.000\n0.014\n0.148\n1049\n0.090\n0.080\n0.049\n0.064\n0.055\n0.056\n0.098\n0.058\n0.051\n0.008\n0.011\n0.105\n23\n0.027\n0.003\n0.026\n0.032\n0.035\n0.031\n0.027\n-0.007\n0.018\n0.000\n-0.003\n0.033\n1050\n0.046\n0.116\n-0.048\n0.109\n-0.020\n-0.033\n0.025\n0.006\n0.060\n-0.000\n-0.057\n0.073\n40975\n0.013\n0.013\n0.011\n0.021\n0.011\n0.011\n0.013\n0.110\n0.021\n0.000\n0.029\n0.035\n40982\n0.156\n0.142\n0.217\n0.207\n0.218\n0.221\n0.125\n0.223\n0.082\n0.007\n0.192\n0.163\n1067\n0.209\n0.162\n0.185\n0.185\n0.218\n0.218\n0.058\n0.113\n0.119\n0.105\n0.179\n0.211\nAvg. Rank\n6.4 (3.1)\n7.4 (3.6)\n5.1 (3.0)\n3.6 (1.9)\n5.2 (2.8)\n5.2 (2.6)\n6.8 (3.3)\n6.4 (3.7)\n7.4 (3.2)\n11.1 (1.3)\n8.8 (2.6)\n2.8 (1.7)\nOverall Rank\n6\n10\n3\n2\n5\n4\n8\n7\n9\n12\n11\n1\nTable 5: Comparison of adjusted Rand index (ARI) scores between the proposed G-CEALS and baseline traditional\nor deep clustering methods on sixteen tabular data sets.\n6.4\nClustering of tabular data sets\nTable 4 presents the clustering accuracy (ACC) and rank ordering of the baseline and proposed methods. Similar to\nother studies on tabular data, no single method performs the best on all data sets due to data heterogeneity. The AE-CM\nmethod yields superior clustering accuracy on data sets with IDs 40994, 1480, 1464, 1068, 1049, and 40975. However,\nthis method results in some of the lowest accuracy scores on other datasets. Similarly, the DynAE method outperforms\nall methods on three data sets with ID 458, 1050, and 40982. Our proposed G-CEALS method outperforms all\nbaselines on four data sets with ID 1063, 1510, 11, and 37.\nTherefore, the tabular data literature commonly uses rank ordering to demonstrate the generalizability of a learning\nalgorithm. Our proposed G-CEALS method shows the best average rank of 2.9 (1.7) across 16 tabular data sets,\noutperforming all other competitive deep clustering baselines DCN (5.1 (3.7)) and AE-CM (5.6(4.8)). These results\nare important because traditional clustering methods have long been used as de facto methods for tabular data. A\nGMM clustering on Z space (GMM (Z)) is outperformed by only two deep clustering methods (DCN and proposed\nG-CEALS).\nThe rank-ordering results based on clustering accuracy are consistent with those obtained using ARI scores. Table 5\nranks our proposed deep clustering method as the best (an average rank of 2.8(1.7)) among all methods based on ARI\nscores. Although the AE-CM method yields competitive clustering accuracies (ACC) on multiple data sets (Table 4),\nits ARI scores are nearly zero in most cases. For almost all other baseline methods, at least one data set yields a negative\nARI score, which indicates a discordance between predicted cluster labels and ground truth labels. In contrast, none\nof the ARI scores obtained by the proposed G-CEALS method is negative.\n11\nA PREPRINT - MAY 20, 2024\nMethod\nK-means(X)\nGMM(X)\nGMM(Z)\nK-means(Z)\nDEC\nG-CEALS\nIDEC\nDKM\nAECM\nDEPICT\nDCN\nDynAE\nTime (Seconds)\n0.06\n0.07\n40.08\n40.13\n71.29\n73.16\n84.58\n97.09\n268.94\n307.17\n384.91\n1718.55\nRelative time\n0.00\n0.00\n0.55\n0.55\n0.97\n1.00\n1.16\n1.33\n3.68\n4.20\n5.26\n23.49\nTable 6: Time required in seconds to complete 1000 epochs for clustering dataset 1510. The relative time is the\ncomputation time of other baseline methods relative to the time taken by the proposed method (relative time 1.0).\n6.5\nTime complexity\nOne of the reasons traditional clustering methods have been a de facto choice for tabular data is computational time.\nEven at the expense of heavy computations, deep learning methods have not shown great success in outperforming\ntraditional machine learning on tabular data. Table 6 presents the training times for all methods when using the\ndataset with ID 1510. Relative to other competitive deep clustering methods (DCN, AE-CM), the proposed G-CEALS\nrequires three to five times less computational time. However, a little over a minute computational time yields up\nto 64% increase in clustering accuracy compared to baseline methods with faster computational time (K-means (X),\nGMM (X), GMM (Z), K-means (Z), DEC). Therefore, the proposed G-CEALS method provides superior clustering\naccuracy at a reasonably low computational cost.\n7\nDiscussion of results\nThe paper proposes one of the first deep clustering methods for tabular data when a recent survey on deep clustering\nmethods suggests no work on such data [18]. The key findings of this article are as follows. First, the proposed G-\nCEALS method is superior to eleven baseline traditional and deep clustering methods in average ranks across sixteen\ntabular data sets. Second, the proposed method demonstrates effective cluster separation on deep feature space by\nlearning Gaussian cluster parameters whereas existing models learn the mean of t-distributed clusters. Third, the\nproposed method handles cluster imbalance problem in tabular data by learning individual cluster weights instead\nof assuming balanced clusters. Fourth, the proposed method shows faster computational cost compared to other\ncompetitive deep clustering methods. Even when the proposed methods is computationally more expensive than some\ntraditional clustering methods, it offers superior clustering accuracy.\n7.1\nTraditional versus deep clustering\nA general observation in the deep classification of tabular data studies is traditional machine learning of input features\n(X) is often superior to deep learning methods [16, 17]. However, a simple autoencoder learned embedding (Z)\nachieves better clustering performance than traditional machine learning of X. Among the deep clustering methods,\nDCN amd AC-EM methods show superior clustering accuracy to traditional clustering methods. However, ARI scores\nreveal that traditional clustering (K-means or GMM) on autoencoder learned embedding (Z) is superior to all baseline\ndeep or traditional clustering methods (K-means or GMM on X). It is known that an accuracy metric may not be\nreliable in the presence of data imbalance. Data imbalance problem is usually not seen as an issue with image data sets\nas it is with tabular data sets. In this context, metrics like ARI may reveal important insights into cluster randomness.\nTherefore, current deep clustering methods (AE-CM, DKM, DCN), benchmarked on or developed for image data\nsets, may not yield robust clustering performance on tabular data compared to more traditional methods (GMM(Z),\nK-means (Z), DEC, IDEC). In contrast, proposed G-CEALS method achieves the best performance on both clustering\naccuracy and ARI scores, indicating its effectiveness on tabular data.\n7.2\nImage versus tabular data embedding\nTraditional clustering methods are obsolete in computer vision because clustering on high-dimensional homogeneous\npixel space is ineffective. In contrast, tabular data sets have smaller sample sizes and dimensionality with heteroge-\nneous features, whereas traditional clustering is still relevant and effective. In this context, our results reveal that deep\narchitectures with convolutional neural networks are not as effective in learning tabular data embedding as they are\nwith image data. Our observation confirms a preliminary study showing that state-of-the-art deep clustering methods\noptimized for image data sets do not produce satisfactory clustering accuracy on tabular data [50]. This suggests the\nneed for specialized learning algorithms and architectures for tabular data similar to the proposed G-CEALS method.\n7.3\nEffects of data statistics on clustering performance\nTabular data sets are said to be heterogeneous because of heterogeneity in feature space and data statistics. We\ndiscuss three scenarios in this context. First, categorical features extend the data dimension with additional one-hot\n12\nA PREPRINT - MAY 20, 2024\n0.2\n0.4\n0.6\n0.8\n1.0\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nACC\nID:1510\nID:1063\nID:1480\nID:40975\nID:40982\n(a) Accuracy\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\nARI\nID:1510\nID:1063\nID:1480\nID:40975\nID:40982\n(b) ARI\nFigure 6: Effects of γ values on the clustering accuracy and ARI scores of the proposed G-CEALS methods using five\ntabular data sets.\nencoded binary columns. Three (IDs 469, 23, and 40975) of the sixteen tabular data sets have categorical features\nonly. The proposed G-CEALS is the best for categorical tabular data (average rank 2.7 (0.6) followed by other deep\nlearning (average rank 6.5 (3.5)) and traditional clustering (average rank 7.3 (1.8)) methods. For twelve numerical-only\ntabular data sets, average rank orderings are similar, G-CEALS (3.0 (2.0), other deep learning 6.6 (3.5), and traditional\nclustering 7.0 (3.4). The AE-CM achieves the best clustering accuracy (71.3) for only one mixed data set (ID 1480)\nbut suffers a negative ARI score (-0.001). Conversely, G-CEALS shows a better balance in clustering accuracy (65.2)\nand ARI (0.033).\nSecond, a higher F-S ratio indicates wider data tables, whereas a lower F-S ratio refers to taller data tables. Tabular data\nsets with a higher F-S ratio are more likely to introduce the curse of dimensionality on machine learning. For datasets\nwith an F-S ratio below 1.0, G-CEALS achieves an average rank of 2.0 (1.0), whereas other deep and traditional\nmethods rank 6.4 (3.6) and 8.2 (2.8), respectively. Conversely, the average ranks for datasets with an F-S ratio above\n1.0 are 3.2 (1.8) for G-CEALS, 6.6 (3.5) for other deep learning methods, and 7.1 (3.1) for traditional clustering.\nThird, the C-score measures between-feature correlations of a tabular data set, which may affect machine learning\nperformance. Tabular data sets with high C-scores (>0.10) are best clustered using the G-CEAL method (average\nrank 3.3 (1.9)), whereas the G-CEAL method ranks (2.3 (1.2)) the best on tabular data sets with low C-scores (<\n0.10). The average rank orderings of deep learning methods are 6.6(3.3) and 65. (3.6) on low and high C-score data\nsets, respectively. Traditional clustering methods show inferior rank orderings of 7.5(3.1) on low and 7.2(3.6) on high\nC-score data sets.\n7.4\nAblation study\nThe effect of the clustering loss balancing parameter γ on clustering accuracy and ARI scores is shown in Figure 6.\nThe results indicate that clustering performance remains relatively stable across varying γ values. However, the choice\nof γ value can impact the time and stability of convergence, although a delayed convergence is expected to yield\nsimilar clustering performance. Therefore, a γ value in the lower range is preferred to ensure stable convergence and\nclustering performance.\n7.5\nLimitations\nDespite promising clustering performance, the proposed G-CEALS method has several limitations as any other\nmethod. Unsupervised learning or clustering is not trivial via deep learning, which often expects a target variable\nor a robust learning objective. There is still plenty of room to improve the cluster performance by innovating novel\nlearning objectives or targets. Furthermore, data or cluster imbalance is common with tabular data sets, which would\nrequire more algorithmic solutions instead of the proposed early stopping to avoid the clusters from merging. Several\naspects of model selection (e.g., embedding dimensions and network architecture) vary due to the heterogeneity of tab-\nular data sets, unlike image data sets in computer vision applications. The proposed method needs better approaches\nto model selection and optimization.\n8\nConclusions\nThis paper presents a novel deep clustering method for concurrently learning cluster-friendly embedding and cluster\nassignments of unlabeled tabular data. The superiority of the proposed G-CEALS method against nine number of state-\nof-the-art clustering methods suggests that multivariate Gaussian distributions learn clusters better than the widely used\n13\nA PREPRINT - MAY 20, 2024\nt-distribution. Furthermore, dynamically updating the target cluster distribution is more effective than setting a closed-\nform target for deep clustering. The cluster weight is important in ensuring proper cluster separation during cluster\nimbalance. A data-informed decision is recommended when selecting an appropriate clustering approach because one\nmethod may not be suitable for all tabular data. The proposed deep clustering shows a promising approach that may\nreplace traditional machine learning approaches for clustering tabular data.\nAcknowledgements\nThe research reported in this publication was supported by the Air Force Office of Scientific Research under Grant\nNumber W911NF-23-1-0170. The content is solely the responsibility of the authors and should not be interpreted as\nrepresenting the official policies, either expressed or implied, of the Army Research Office or the U.S. Government.\nReferences\n[1] Lei Lin, Wencheng Wu, Zhongkai Shangguan, Safwan Wshah, Ramadan Elmoudi, and Beilei Xu. Hpt-rl: Cali-\nbrating power system models based on hierarchical parameter tuning and reinforcement learning. In 2020 19th\nIEEE International Conference on Machine Learning and Applications (ICMLA), pages 1231–1237. IEEE, 2020.\n[2] M Alam, M D Samad, L Vidyaratne, A Glandon, and K M Iftekharuddin. Survey on Deep Neural Networks in\nSpeech and Vision Systems. Neurocomputing, 417:302–321, 2020.\n[3] Xue Ying. An overview of overfitting and its solutions. In Journal of physics: Conference series, volume 1168,\npage 022022. IOP Publishing, 2019.\n[4] Sandy H. Huang, Nicolas Papernot, Ian J. Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on\nneural network policies. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Workshop Track Proceedings. OpenReview.net, 2017.\n[5] Ahc`ene Boubekki, Michael Kampffmeyer, Ulf Brefeld, and Robert Jenssen. Joint optimization of an autoencoder\nfor clustering and embedding. Machine learning, 110(7):1901–1937, 2021.\n[6] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learn-\ning of visual features. In Proceedings of the European conference on computer vision (ECCV), pages 132–149,\n2018.\n[7] Shourav B. Rabbani, Ivan V. Medri, and Manar D. Samad. Attention versus contrastive learning of tabular data\n– a data-centric benchmarking, 2024.\n[8] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In 33rd\nInternational Conference on Machine Learning, ICML 2016, volume 1, pages 740–749, 2016.\n[9] Xifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. Improved deep embedded clustering with local structure\npreservation. IJCAI International Joint Conference on Artificial Intelligence, 0:1753–1759, 2017.\n[10] Maziar Moradi Fard, Thibaut Thonet, and Eric Gaussier. Deep k-means: Jointly clustering with k-means and\nlearning representations. Pattern Recognition Letters, 138:185–192, 2020.\n[11] Nairouz Mrabah, Naimul Mefraz Khan, Riadh Ksantini, and Zied Lachiri. Deep clustering with a dynamic\nautoencoder: From reconstruction towards centroids construction. Neural Networks, 130:206–228, oct 2020.\n[12] Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong. Towards K-means-friendly spaces: Simultaneous\ndeep learning and clustering. In 34th International Conference on Machine Learning, ICML 2017, volume 8,\npages 5888–5901, 2017.\n[13] Niklas Kohler, Maren Buttner, and Fabian Theis. Deep learning does not outperform classical machine learning\nfor cell-type annotation. bioRxiv, page 653907, 2019.\n[14] Aaron M. Smith, Jonathan R. Walsh, John Long, Craig B. Davis, Peter Henstock, Martin R. Hodge, Mateusz Ma-\nciejewski, Xinmeng Jasmine Mu, Stephen Ra, Shanrong Zhao, Daniel Ziemek, and Charles K. Fisher. Standard\nmachine learning approaches outperform deep representation learning on phenotype prediction from transcrip-\ntomics data. BMC Bioinformatics, 21(1):119, dec 2020.\n14\nA PREPRINT - MAY 20, 2024\n[15] Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep\nneural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.\n[16] Arlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka. Well-tuned simple nets excel on tabular\ndatasets. Advances in neural information processing systems, 34:23928–23941, 2021.\n[17] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion,\n81:84–90, may 2022.\n[18] Sheng Zhou, Hongjia Xu, Zhuonan Zheng, Jiawei Chen, Jiajun Bu, Jia Wu, Xin Wang, Wenwu Zhu, Martin\nEster, et al. A comprehensive survey on deep clustering: Taxonomy, challenges, and future directions. arXiv\npreprint arXiv:2206.07579, 2022.\n[19] Laurens Van Der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning\nResearch, 9:2579–2625, 2008.\n[20] Liang Duan, Charu Aggarwal, Shuai Ma, and Saket Sathe. Improving spectral clustering with deep embed-\nding and cluster estimation.\nProceedings - IEEE International Conference on Data Mining, ICDM, 2019-\nNovem(Icdm):170–179, 2019.\n[21] Sean T. Yang, Kuan Hao Huang, and Bill Howe. JECL: Joint embedding and cluster learning for image-text\npairs. Proceedings - International Conference on Pattern Recognition, pages 8344–8351, 2020.\n[22] Kamran Ghasedi Dizaji, Amirhossein Herandi, Cheng Deng, Weidong Cai, and Heng Huang. Deep Clustering\nvia Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization. In Proceedings of the\nIEEE International Conference on Computer Vision, volume 2017-Octob, pages 5747–5756, 2017.\n[23] Rui Zhang, Yinglong Xia, Hanghang Tong, and Yada Zhu. Robust embedded deep K-means clustering. Interna-\ntional Conference on Information and Knowledge Management, Proceedings, pages 1181–1190, 2019.\n[24] Yazhou Ren, Kangrong Hu, Xinyi Dai, Lili Pan, Steven C.H. Hoi, and Zenglin Xu.\nSemi-supervised deep\nembedded clustering. Neurocomputing, 325:121–130, jan 2019.\n[25] Joseph Enguehard, Peter O’Halloran, and Ali Gholipour. Semi-supervised learning with deep embedded cluster-\ning for image classification and segmentation. IEEE Access, 7:11093–11104, 2019.\n[26] Lirong Wu, Lifan Yuan, Guojiang Zhao, Haitao Lin, and Stan Z. Li. Deep Clustering and Visualization for End-\nto-End High-Dimensional Data Analysis. IEEE Transactions on Neural Networks and Learning Systems, pages\n1–12, 2022.\n[27] Mohammadreza Sadeghi and Narges Armanfard. Idecf: Improved deep embedding clustering with deep fuzzy\nsupervision. In 2021 IEEE International Conference on Image Processing (ICIP), pages 1009–1013. IEEE, 2021.\n[28] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. Scan:\nLearning to classify images without labels. In European conference on computer vision, pages 268–285. Springer,\n2020.\n[29] Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. In\nProceedings of the AAAI conference on artificial intelligence, volume 35, pages 8547–8555, 2021.\n[30] Junjian Li, Jin Liu, Hailin Yue, Jianhong Cheng, Hulin Kuang, Harrison Bai, Yuping Wang, and Jianxin Wang.\nDarc: Deep adaptive regularized clustering for histopathological image classification. Medical image analysis,\n80:102521, 2022.\n[31] Mohammadreza Sadeghi, Hadi Hojjati, and Narges Armanfard. C3: Cross-instance guided contrastive clustering.\nBritish Machine Vision Conference 2023, 2023.\n[32] Bingchen Zhao, Xin Wen, and Kai Han. Learning semi-supervised gaussian mixture models for generalized cat-\negory discovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16623–\n16633, 2023.\n[33] Yu Duan, Zhoumin Lu, Rong Wang, Xuelong Li, and Feiping Nie. Toward balance deep semisupervised cluster-\ning. IEEE Transactions on Neural Networks and Learning Systems, pages 1–13, 2024.\n15\nA PREPRINT - MAY 20, 2024\n[34] Mohammadreza Sadeghi and Narges Armanfard. Deep clustering with self-supervision using pairwise similari-\nties. arXiv preprint arXiv:2405.03590, 2024.\n[35] Maksims Kazijevs and Manar D Samad. Deep imputation of missing values in time series health data: A review\nwith benchmarking. Journal of Biomedical Informatics, page 104440, 2023.\n[36] Ibna Kowsar, Shourav B Rabbani, Kazi Fuad B Akhter, and Manar D Samad. Deep clustering of electronic health\nrecords tabular data for clinical interpretation. In 2023 IEEE International Conference on Telecommunications\nand Photonics (ICTP), pages 01–05. IEEE, 2023.\n[37] Manar D Samad, Sakib Abrar, and Norou Diawara. Missing value estimation using clustering and deep learning\nwithin multiple imputation framework. Knowledge-based systems, 249:108968, 2022.\n[38] Sercan ¨O Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In Proceedings of the AAAI\nconference on artificial intelligence, volume 35, pages 6679–6687, 2021.\n[39] Jannik Kossen, Neil Band, Clare Lyle, Aidan N. Gomez, Tom Rainforth, and Yarin Gal. Self-attention between\ndatapoints: Going beyond individual input-output pairs in deep learning. Advances in Neural Information Pro-\ncessing Systems, 34:28742–28756, 6 2021.\n[40] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models for\ntabular data. Advances in Neural Information Processing Systems, 23:18932–18943, 2021.\n[41] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.\n[42] Raisul Arefin, Manar D. Samad, Furkan A. Akyelken, and Arash Davanian. Non-transfer Deep Learning of\nOptical Coherence Tomography for Post-hoc Explanation of Macular Disease Classification. In 2021 IEEE 9th\nInternational Conference on Healthcare Informatics (ICHI), pages 48–52. IEEE, aug 2021.\n[43] Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen.\nDeep autoencoding gaussian mixture model for unsupervised anomaly detection. In International Conference on\nLearning Representations, 2018.\n[44] Ehsan Variani, Erik McDermott, and Georg Heigold. A gaussian mixture model layer jointly optimized with\ndiscriminative features within a deep neural network architecture. In 2015 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages 4270–4274. IEEE, 2015.\n[45] Sakib Abrar and Manar D Samad. Perturbation of deep autoencoder weights for model compression and classi-\nfication of tabular data. Neural Networks, 156:160–169, 2022.\n[46] Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Frank Hutter, Michel Lang, Rafael G. Mantovani, Jan N.\nvan Rijn, and Joaquin Vanschoren. Openml benchmarking suites. arXiv:1708.03731v2 [stat.ML], 2019.\n[47] Yazhou Ren, Jingyu Pu, Zhimeng Yang, Jie Xu, Guofeng Li, Xiaorong Pu, Philip S. Yu, and Lifang He. Deep\nClustering: A Comprehensive Survey. oct 2022.\n[48] Qinjian Huang, Yue Zhang, Hong Peng, Tingting Dan, Wanlin Weng, and Hongmin Cai. Deep subspace clus-\ntering to achieve jointly latent feature extraction and discriminative learning. Neurocomputing, 404:340–350,\n2020.\n[49] H. W. Kuhn. The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2:83–97,\n3 1955.\n[50] Sakib Abrar, Ali Sekmen, and Manar D. Samad. Effectiveness of deep image embedding clustering methods on\ntabular data. In 2023 15th International Conference on Advanced Computational Intelligence (ICACI), pages\n1–7, 2023.\n16\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-01-02",
  "updated": "2024-05-17"
}