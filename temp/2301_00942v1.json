{
  "id": "http://arxiv.org/abs/2301.00942v1",
  "title": "Deep Learning and Computational Physics (Lecture Notes)",
  "authors": [
    "Deep Ray",
    "Orazio Pinti",
    "Assad A. Oberai"
  ],
  "abstract": "These notes were compiled as lecture notes for a course developed and taught\nat the University of the Southern California. They should be accessible to a\ntypical engineering graduate student with a strong background in Applied\nMathematics.\n  The main objective of these notes is to introduce a student who is familiar\nwith concepts in linear algebra and partial differential equations to select\ntopics in deep learning. These lecture notes exploit the strong connections\nbetween deep learning algorithms and the more conventional techniques of\ncomputational physics to achieve two goals. First, they use concepts from\ncomputational physics to develop an understanding of deep learning algorithms.\nNot surprisingly, many concepts in deep learning can be connected to similar\nconcepts in computational physics, and one can utilize this connection to\nbetter understand these algorithms. Second, several novel deep learning\nalgorithms can be used to solve challenging problems in computational physics.\nThus, they offer someone who is interested in modeling a physical phenomena\nwith a complementary set of tools.",
  "text": "Deep Learning and Computational Physics\n(Lecture Notes)\nDeep Ray, Orazio Pinti and Assad A. Oberai1\n1Department of Aerospace and Mechanical Engineering, University of Southern California,\nLos Angeles, California, USA\narXiv:2301.00942v1  [cs.LG]  3 Jan 2023\nContents\nPreface\n3\n1\nIntroduction\n4\n1.1\nComputational physics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.2\nMachine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.2.1\nExamples of ML\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.2.2\nTypes of ML algorithms based leaning task\n. . . . . . . . . . . . . . . . .\n5\n1.3\nArtiﬁcial Intelligence, Machine Learning and Deep Learning . . . . . . . . . . . .\n6\n1.4\nMachine learning and computational physics . . . . . . . . . . . . . . . . . . . . .\n7\n2\nIntroduction to deep neural networks\n9\n2.1\nMLP architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.2\nActivation functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2.1\nLinear activation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.2.2\nRectiﬁed linear unit (ReLU) . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.2.3\nLeaky ReLU\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.2.4\nLogistic function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.2.5\nTanh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.2.6\nSine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.3\nExpressivity of a network\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.3.1\nUniversal approximation results . . . . . . . . . . . . . . . . . . . . . . . .\n15\n2.4\nTraining, validation and testing of neural networks . . . . . . . . . . . . . . . . .\n15\n2.5\nGeneralizability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n2.5.1\nRegularization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n2.6\nGradient descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n2.7\nSome advanced optimization algorithms\n. . . . . . . . . . . . . . . . . . . . . . .\n20\n2.7.1\nMomentum methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n2.7.2\nAdam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n2.7.3\nStochastic optimization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n2.8\nCalculating gradients using back-propagation\n. . . . . . . . . . . . . . . . . . . .\n23\n2.9\nRegression versus classiﬁcation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n3\nResidual neural networks\n27\n3.1\nVanishing gradients in deep networks . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n3.2\nResNets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.3\nConnections with ODEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.4\nNeural ODEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n1\n4\nSolving PDEs with MLPs\n33\n4.1\nFinite diﬀerence method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n4.2\nSpectral collocation method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.3\nPhysics-informed neural networks (PINNs) . . . . . . . . . . . . . . . . . . . . . .\n38\n4.4\nExtending PINNs to a more general PDE\n. . . . . . . . . . . . . . . . . . . . . .\n40\n4.5\nError analysis for PINNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n4.6\nData assimilation using PINNs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n5\nConvolutional Neural Networks\n44\n5.1\nFunctions and images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n5.2\nConvolutions of functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n5.2.1\nExample 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n5.2.2\nExample 2\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n5.3\nDiscrete convolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n5.4\nConnection to ﬁnite diﬀerences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n5.5\nConvolution layers\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n5.5.1\nAverage and Max Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n5.5.2\nConvolution for inputs with multiple channels . . . . . . . . . . . . . . . .\n51\n5.6\nConvolution Neural Network (CNN)\n. . . . . . . . . . . . . . . . . . . . . . . . .\n51\n5.7\nTranspose convolution layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n5.8\nImage-to-image transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n6\nOperator Networks\n57\n6.1\nThe problem with PINNs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n6.2\nParametrized PDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n6.3\nOperators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n6.4\nDeep Operator Network (DeepONet) Architecture\n. . . . . . . . . . . . . . . . .\n59\n6.5\nTraining DeepONets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n6.6\nError Analysis for DeepONets . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n6.7\nPhysics-Informed DeepONets\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n6.8\nFourier Neural Operators - Architecture . . . . . . . . . . . . . . . . . . . . . . .\n64\n6.9\nDiscretization of the Fourier Neural Operator . . . . . . . . . . . . . . . . . . . .\n66\n6.10 The Use of Fourier Transforms\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n7\nProbabilistic Deep Learning\n70\n7.1\nKey elements of Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n7.2\nRandom Variables\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n7.2.1\nCumulative distribution function . . . . . . . . . . . . . . . . . . . . . . .\n72\n7.2.2\nProbability density function . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n7.2.3\nExamples of Important RVs . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n7.2.4\nExpectation and variance of RVs . . . . . . . . . . . . . . . . . . . . . . .\n76\n7.2.5\nPair of RVs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n7.3\nUnsupervised probabilistic deep learning algorithms . . . . . . . . . . . . . . . . .\n79\n7.3.1\nGANs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n7.4\nSupervised probabilistic deep learning algorithms . . . . . . . . . . . . . . . . . .\n82\n7.4.1\nConditional GANs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\n2\nPreface\nThese notes were compiled as lecture notes for a course developed and taught at the University\nof the Southern California. They should be accessible to a typical engineering graduate student\nwith a strong background in Applied Mathematics.\nThe main objective of these notes is to introduce a student who is familiar with concepts\nin linear algebra and partial diﬀerential equations to select topics in deep learning. These\nlecture notes exploit the strong connections between deep learning algorithms and the more\nconventional techniques of computational physics to achieve two goals. First, they use concepts\nfrom computational physics to develop an understanding of deep learning algorithms. Not\nsurprisingly, many concepts in deep learning can be connected to similar concepts in computational\nphysics, and one can utilize this connection to better understand these algorithms. Second,\nseveral novel deep learning algorithms can be used to solve challenging problems in computational\nphysics. Thus, they oﬀer someone who is interested in modeling a physical phenomena with a\ncomplementary set of tools.\n3\nChapter 1\nIntroduction\nThis course deals with topics that lie at the interface of computational physics and machine\nlearning. Before we can appreciate the need to combine both these important concepts, we need\nto understand what each of them mean on their own.\n1.1\nComputational physics\nComputational physics plays a fundamental role in solving many problems in ﬁelds of science and\nengineering. To gain an understanding of this concept, we brieﬂy outline the key steps involved\nin solving a physical problem:\n1. Consider a physical phenomena and collect measurements of some observable of interest.\nFor example, the measurements of the water height and wave direction obtain from ocean\nbouys, when studying oceanic waves.\n2. Based on the observations, postulate a physical law. For instance, you observe that the\nmass of ﬂuid is a closed-system is conserved for all time.\n3. Write down a mathematical description of the law. This could make use of ordinary\ndiﬀerential equations (ODEs), partial diﬀerential equations (PDEs), integral equations, etc.\n4. Once the mathematical model is framed, solve for the solution of the system. There are\ntwo ways to obatin this:\n(a) In certain situations an exact analytical form of the solution can be obtained. For\ninstance one could solve ODEs/PDEs using separation of variables, Laplace transforms,\nFourier transforms or integration factors.\n(b) In most scenarios, exact expressions of the solution cannot be obtained and must\nbe suitable approximated using a numerical algorithm. For instance, one could use\nforward or backward Euler, mid-point rule, or Runge-Kutta schemes for solving\nsystems of ODEs; or one could use ﬁnite diﬀerence/volume/element methods methods\nfor solving PDEs.\n5. Once the algorithm to evaluate the solution (exactly or approximately) is designed, use it\nto validate the mathematical model, i.e., see if the predictions are in tune with the data\ncollected.\nAll these steps broadly describe what computational physics entails.\n4\n1.2\nMachine learning\nUnlike computational physics, machine learning (ML) does not require the postulation of a\nphysical law. The general steps involved are:\n1. Collect data by observing the physical phenomena, by real-time measurements of some\nobservable or by using an numerical solver approximating the phenomenon.\n2. Train a suitable algorithm using the collected data, with the aim of discovering a pattern\nor relation between the various samples. See Section 1.2.1 for some concrete examples.\n3. Once trained, use the ML algorithm to make future predictions, and validate it with\nadditional collected data.\n1.2.1\nExamples of ML\n1. Regression algorithms: Given the set of pairwise data {(xi, yi) : 1 ≤i ≤N} which\ncorresponds to some unknown function y = f(x), ﬁt a polynomial (or any other basis) to\nthis data set in order to approximate f. For instance, ﬁnd the coeﬃcients a, b of the linear\nﬁt ˜f(x; a, b) = ax + b to minimize the error\nE(a, b) =\nN\nX\ni=1\n|yi −˜f(xi)|2.\nIf (a∗, b∗) = arg mina,b E(a, b), then we can consider ˜f∗(x) := ˜f(x; a∗, b∗) to be the approxi-\nmation of f(x) (see Figure 1.1(a)).\n2. Decision trees: We are given a dataset from a sample population, containing the features:\nage, income. Furthermore, the data is divided into two groups; an individual in Group A\nowns a house while an individual in Group B does not. Then, given a features of a new\ndata point, we would like to predict the probability of this new individual owning a house.\nDecision trees can be used to solve this classiﬁcation problem. The way a typical decision\ntree works is by making cuts the maximize the group-based separation for the samples in\nthe dataset (see Figure 1.1(b)). Then, based on these cuts, the algorithm determines the\nprobability of belonging to a particular class/group for a new point.\n3. Clustering algorithms: Given a set of data with a number of features per sample, ﬁnd\ncluster/patterns in the data (see Figure 1.1(c)).\n1.2.2\nTypes of ML algorithms based leaning task\nBroadly speaking, there are four types of ML algorithms:\n1. Supervised learning: Given the data S = {(xi, yi) : 1 ≤i ≤N}, predict ˆy for some new\nˆx, such that (ˆx, ˆy) /∈S. For instance, given a set of images and image labels (e.g. dog,\ncat, cow, etc), train a classiﬁcation ML algorithm to learn the relation between images and\nlabels, and use it to predict the label of a new image.\n2. Unsupervised learning: Given the data S = {xi ∈Ωx : 1 ≤i ≤N}, ﬁnd a relation\namong diﬀerent regions of Ωx. For instance, ﬁnd clusters in the dataset, or ﬁnd an expression\nfor the probability distribution px(x) governing the spread of this data and generate new\nsamples from it.\n5\n(a) Linear regression\n(b) Decision tree\n(c) Clustering\nFigure 1.1: Examples of ML\n3. Semi-supervised learning: This family of methods falls between the supervised and\nunsupervised learning families. They typically make use of a combination of labelled and\nunlabelled data for training. For example let’s say we are given 10,000 images that are\nunlabeled and only 50 images that are labeled. Can we use this dataset to develop an\nimage classiﬁcation algorithm?\n4. Re-inforcement learning: The methods belonging to this family learn driven by rewards\nor penalties for decisions taken. Thus, a suitable path/policy is learned to maximize the\nreward. These kinds of methods are used to train algorithms to play chess or Go.\nIn this course, we will primarily focus on the ﬁrst two types of ML algorithms.\n1.3\nArtiﬁcial Intelligence, Machine Learning and Deep Learning\nAt times, the terms Artiﬁcial Intelligence (AI), ML and Deep Learning (DL) are used inter-\nchangeably. In reality, these are three related but diﬀerent concepts. This can be understood by\nlooking at the Venn diagram in Figure 1.2.\nFigure 1.2: The relation between AI, ML and DL\nAI refers to a system with human-like intelligence. While ML is a key component of an AI\nsystem, there are other ingredients involved. A self-driving car is a prototypical example of AI.\n6\nLet’s take a closer look at the design of such a system (see Figure 1.3). A car is mounted with a\ncamera which takes lives images/video of the road ahead. These frames are then passed to an\nML algorithm which performs a semantic segmentation, i.e., segments out diﬀerent regions of\nthe frame and classiﬁes the type of object (car, tree, road, sky, etc) in each segment. Once this\nsegmentation is done, it is passed to a decision system that decides the next action of the car\nshould be based on this segmented image. This information then passes though a control module\nthat actually controls the mechanical actions of the car. This entire process is mimics what a\nreal driver would do, and is thus artiﬁcial intelligence.\nOn the other hand, machine learning (ML) are the components of this system that are trained\nusing data. That is they learn through data. In the example above, the Semantic Segmenter is\none such system. There are many ML algorithms that can perform this task using data, and we\nwill learn some in this course. The Decision System could also be an ML component - where the\nappropriate decision to be made is learnt from prior data. However, it could be non-ML. Perhaps\na rules based expert system.\nFigure 1.3: Schematic of AI system for a self-driving car. Some illustration taken from [32].\nDL is a subset of ML algorithms. The simplest form of a DL architecture, known as a feed-\nforward network. It comprises a number of layers of non-linear transformations. The non-linear\ntransformations are applied (component-wise) to an aﬃne transformation of an intermediate\noutput. This architecture is loosely motivated by how signals are transmitted by the central\nnervous in living organisms. We will study the DL architecture in greater detail in Chapter 2.\n1.4\nMachine learning and computational physics\nNow that we have a better understanding of computational physics and ML, the next obvious\nquestion would be “why do we need to look at a combination of the two?” We list down a few\nmotivations below:\n• For complex patterns of “physical” data, ML provides an alternate route to representing\nmathematical laws. Consider a physical process that contains two important components.\nOf these, one is well understood and has a trusted mathematical model, and the other is\npoorly understood and does not have a mathematical description. In this scenario, one\nmay use computational physics for the ﬁrst component and ML for the second. A concrete\nexample of this would be a system governed by conservation of energy and a complex\nconstitutive model. For the former we may have a well understood mathematical model,\nwhile for the latter we may have to rely on ML to develope a model.\n• ML in general is very data hungry. But the knowledge of physics can help restrict the\n7\nmanifold on which the input and solution/predictions lie. With such constraints, we can\nreduce the amount of data required to train the ML algorithm.\n• Tools for analyzing computational physics (functional analysis, numerical analysis, notions\nof convergence to exact solutions, probabilistic frameworks) carry over to ML. Applying\nthese tools to ML helps us better understand and design better ML algorithms.\nWe brieﬂy summarize the various topics that will be covered in this course:\n• Deep Neural Networks (MLPs) and their convergence.\n• Resnets and their connections with non-linear ODEs (Neural ODEs).\n• Recurnets and their connections with nonlinear ODEs.\n• Convolutional neural networks and their connection to PDEs.\n• Stochastic gradient descent and how it is related to ODEs.\n• Deep Learning algorithms for solving PDEs.\n• Deep Learning algorithms for approximation operators.\n• Generative adversarial algorithms and their connection to computational physics.\n8\nChapter 2\nIntroduction to deep neural networks\nIn this chapter, we will take a closer look at the simplest network architecture that is available\nknown as multilayer perceptron (MLP).\n2.1\nMLP architecture\nLet us deﬁne our objective as the approximation of a function f : x ∈Rd 7→y ∈RD using an\nMLP, which we denote as F. Computing units of an MLP, called artiﬁcial neurons, are stacked\nin a number of consecutive layers. The zeroth layer of F is called the source layer, which is not\na computing layer but is only responsible for providing an input (of dimension d) to the network.\nThe last layer of F is known as the output layer, which outputs the network’s prediction (of\ndimension D). Every other layer in between is known as a hidden layer. The number of neurons\nin a layer deﬁnes the width of that layer. A schematic of an MLP with 2 hidden layers is shown\nin Figure 2.1.\nx(l)\ni\n= σ(W (l)\ni j x(l−1)\nj\n+b(l)\ni )\nx(0)\nx(1)\nx(2)\nx(3)\nAct. func.\nAct. func.\nOut. func.\nSource Layer\nHidden Layer 1\nHidden Layer 2\nOutput Layer\nFigure 2.1: MLP with 2 hidden layers\nTo understand the operations occurring inside an MLP, let us deﬁne some notations. We\nconsider a network with L hidden layers, with the width of layer (l) denoted as Hl for l =\n0, 1, ..., L+1. Note that for consistency with the function f that we are trying to approximate, we\nmust have H0 = d and HL+1 = D. Let us denote the output vector for l-th layer by x(l) ∈RHl,\nwhich will serve as the input to the next layer. We set x(0) = x ∈Rd which will be the input\nsignal provided by the input layer. In each layer l, 1 ≤l ≤L + 1, the i-th neuron performs an\n9\naﬃne transformation on that layers input x(l−1) followed by a non-linear transformation\nx(l)\ni\n= σ\n\u0010\nW (l)\nij x(l−1)\nj\n|\n{z\n}\nEinstein sum\n+b(l)\ni\n\u0011\n,\n1 ≤i ≤Hl,\n1 ≤j ≤Hl−1\n(2.1)\nwhere W (l)\nij and b(l)\ni\nare respectively known as the weights and bias associated with i-th neuron\nof layer l, while the function σ(.) is known as the activation function, and plays a pivotal role in\nhelping the network to represent non-linear complex functions. If we set W (l) ∈RHl−1×Hl to be\nthe weight matrix for layer l and b(l) ∈RHl to be the bias vector for layer l, then we can re-write\nthe action of the whole layer as\nx(l) = σ\n\u0010\nA(l)(x(l−1))\n\u0011\n,\nA(l)(x(l−1)) = W (l)x(l−1) + b(l)\n(2.2)\nwhere the activation function is applied component-wise. Thus, the action of the whole network\nF : Rd 7→RD can be mathematically seen as a composition of alternating aﬃne transformations\nand component-wise activations\nF(x) = A(L+1) ◦σ ◦A(L) ◦σ ◦A(L−1) ◦· · · ◦σ ◦A(1)(x).\n(2.3)\nWe make a few remarks here:\n1. For simplicity of the representation, we assume that the same activation function is used\nacross all layers of the network. However, this is not a strict rule. In fact, there is recent\nevidence that suggests that alternating activation function from layer to layer leads to\nbetter neural networks [33].\n2. At times, there might be an output function O instead of an activation function at the end\nof the output layer, which is typically used to reformulate the output into a suitable form.\nWe will see examples of such functions later in the course.\n3. We will use the term depth of the network to denote the number of computing layers in\nthe MLP, i.e. the number of hidden layers and the output layer, which would be L + 1 as\nper the notations used above.\nThe parameters of the network is all the weights and biases, which we will represent as\nθ = {W (l), b(l)}L+1\nl=1 ∈RNθ\nwhere Nθ denotes the total number of parameters of the network. The network F(x; θ) represents\na family of parameterized functions, where θ needs to suitably chosen such that the network\napproximates the target function f(x) at the input x.\nQuestion 2.1.1. Prove that Nθ = PL+1\nl=1 (Hl−1 + 1)Hl.\n2.2\nActivation functions\nThe activation function is perhaps the most important component of an MLP. A large number of\nactivations are available in literature, each with its own advantages and disadvantages. Let us\ntake a look at a few of these options (also see Figure 2.2).\n10\nξ\nσ(ξ)\n(a) Linear\nξ\nσ(ξ)\n(b) ReLU\nα = 0.1\nξ\nσ(ξ)\n(c) Leaky ReLU\n1\nξ\nσ(ξ)\n(d) Logistic\n1\n−1\nξ\nσ(ξ)\n(e) Tanh\n1\n−1\nξ\nσ(ξ)\n(f) Sine\nFigure 2.2: Examples of activation functions\n11\n2.2.1\nLinear activation\nThe simplest activation corresponds to σ(ξ) = ξ. Some features of this function are\n• The function is inﬁnitely smooth, but all derivatives beyond the second derivative are zero.\n• The range of the function is (−∞, ∞).\n• Using the linear activation function (in all layers) will reduce the entire network to a single\naﬃne transformation of the input x. In other words, the network will be nothing more that\na linear approximation of the target function f, which is not useful if f is highly non-linear.\n2.2.2\nRectiﬁed linear unit (ReLU)\nThis function is piecewise linear and deﬁned as\nσ(ξ) = max{0, ξ} =\n(\nξ,\nif ξ ≥0\n0,\nif ξ < 0\n(2.4)\nThis is one of the most popular activation functions used in practice. Some features of this\nfunction are:\n• The function is continuous, while its derivative will be piecewise constant with a jump\nξ = 0. The second derivative will be a dirac function concentrated at ξ = 0. In other words,\nthe higher-order derivates (greater than 1) are not well-deﬁned.\n• The range of the function is [0, ∞).\n2.2.3\nLeaky ReLU\nThe ReLU activation leads to a null output from a neuron if the aﬃne transformation of the\nneuron is negative. This can lead to the phenomena of dying neurons [15] while training a neural\nnetwork, where neurons drops out completely from the network and no longer contribute to the\nﬁnal prediction. To overcome this challenge, a leaky version ReLU was designed\nσ(ξ; α) =\n(\nξ,\nif ξ ≥0\nαξ,\nif ξ < 0\n(2.5)\nwhere α becomes a network hyper-parameter. Some features of this function are:\n• The derivates of Leaky ReLU behave in the same way as those for ReLU.\n• The range of the function is (−∞, ∞).\n2.2.4\nLogistic function\nThe Logistic or Sigmoid activation function is given by\nσ(ξ) =\n1\n1 + e−ξ\n(2.6)\nand has the following properties\n• The function is inﬁnitely smooth and monotonic.\n• The range of the function is (0, 1), i.e., the function is bounded. Such a function is useful\nin representing probabilities.\n• Since the derivative quickly decays to zero away from ξ = 0, this activation function can\nlead to slow convergence of the network while training.\n12\n2.2.5\nTanh\nThe tanh function is can be seen as a symmetric extension of the logistic function\nσ(ξ) = eξ −e−ξ\neξ + e−ξ\n(2.7)\nand has the following properties\n• The function is inﬁnitely smooth and monotonic.\n• The range of the function is (−1, 1), i.e., the function is bounded. Note that it maps zeros\ninput to zero, while pushing positive (negative) inputs to +1 (-1).\n• Similar to the logistic function, the derivative of tanh quickly decays to zero away from\nξ = 0 and can thus lead to slow convergence while training networks.\n2.2.6\nSine\nRecently, the sine function, i.e., σ(ξ) = sin(ξ) has been proposed as an eﬃcient activation function\n[27]. It has the best features of all the activation function discussed above:\n• The function is inﬁnitely smooth.\n• The range of the function is (−1, 1), i.e., the function is bounded.\n• None of the derivatives of this function decay to zero.\nQuestion 2.2.1. Can you think of an MLP architecture with the sine activation function, which\nleads to an approximation very similar to a Fourier series expansion?\n2.3\nExpressivity of a network\nLet us try to understand the eﬀects of Nθ increases. To see this, let us consider a simple example\nusing the ReLU activation function, i.e., σ(ξ) = max{ξ, 0}. We set d = D = 1, L = 1 and the\nparameters\nW (1) =\n\u00142\n1\n\u0015\n,\nb(1) =\n\u0014−2\n0\n\u0015\n,\nW (2) =\n\u0002\n1\n1\n\u0003\n,\nb(2) = 0.\nas shown in Figure 2.3(a). Then the various layer outputs are\nx(1)\n1\n= max{2x(0)\n1\n−2, 0},\nx(1)\n2\n= max{x(0)\n1 , 0},\nx(2)\n1\n= max{2x(0)\n1\n−2, 0} + max{x(0)\n1 , 0}.\nNotice that while the the output x(1) of the hidden layer (see Figures 2.3(b) and (c)) have only\none corner/kink, the ﬁnal output ends up having two kinks (see Figures 2.3(d)).\nWe generalize this formulation to a bigger network with L hidden layers each of width H.\nThen one can expect that x(1)\ni , 1 ≤i ≤H will have a single kink, with the location and angle of\nthe kink depending on the weights and bias associated with each neuron of the hidden layer. The\nvector x(1) is passed to the next hidden layer, where each neuron will combine the single kinks\nand give an output with possibly H kinks. Once again, the location and angles of the H kinks in\nthe output from each neuron of the second hidden layer will be diﬀerent. The location of the\nkinks will be diﬀerent because each neuron is allowed a diﬀerent bias, and therefore can induce\na diﬀerent shift. Continuing this argument, one can expect the number of kinks to increase as\nH, H2, H3 as it passes through the various hidden layers with width H. In general the total\nnumber of kinks can grow as HL. In other words, the networks have the ability to become more\nexpressive as the depth (and width) of the network is increased.\n13\nw = 2\nb = −2\nw = 1\nb = 0\nw = 1\nb = 0\nw = 1\n(0)\n(1)\n(2)\n(a) MLP with L = 1,W = 2\nx(0)\n1\nx(1)\n1\n0\n4\n4\nOne kink\n(b) x(1)\n1\nvs x(0)\n1\nx(0)\n1\nx(1)\n2\n0\n4\n4\nOne kink\n(c) x(1)\n2\nvs x(0)\n1\nx(0)\n1\nx(2)\n1\n0\n4\n4\nTwo kinks\n(d) x(2)\n1\nvs x(0)\n1\nFigure 2.3: Examples to understand the expressivity of neural networks\n14\n2.3.1\nUniversal approximation results\nTo quantify the expressivity of networks in a mathematically rigorous manner, we look at some\nresults about the approximation properties of MLPs. For these results, we assume K ⊂Rd is a\nclosed and bounded set.\nTheorem 2.3.1 (Pinkus, 1999 [23]). Let f : K →R, i.e., D = 1, be a continuous function.\nThen given an ϵ > 0, there exists an MLP with a single hidden layer (L = 1), arbitrary width H\nand a non-polynomial continuous activation σ such that\nmax\nx∈K |F(x; θ) −f(x)| ≤ϵ.\nTheorem 2.3.2 (Kidger, 2020 [9]). Let f : K →RD be a continuous vector-valued function.\nThen given an ϵ > 0, there exists an MLP with arbitrary number of hidden layers L, each having\nwidth H ≥d + D + 2, a continuous activation σ (with some additional mild conditions), such that\nmax\nx∈K ∥F(x; θ) −f(x)∥≤ϵ.\nTheorem 2.3.3 (Yarotsky, 2021 [33]). Let f : K →R be a function with two continuous\nderivates, i.e., f ∈C2(K). Consider an MLP with ReLU activations and H ≥2d + 10. Then\nthere exists a network with this conﬁguration such that the error converges as\nmax\nx∈K |F(x; θ) −f(x)| ≤C(Nθ)−4\nwhere C is a constant depending on the number of network parameters.\nNumerical results like those mentioned above help demystify the “black-box” nature of neural\nnetwork, and serve as useful practical guidelines when designing network architectures.\n2.4\nTraining, validation and testing of neural networks\nNow that we have a better understanding of the architecture of MLPs, we would now like to\ndiscuss how the parameters of these networks are set to approximate some target function. We\nrestrict our discussions to the framework of supervised learning.\nLet us assume that we are given a dataset of pairwise samples S = {(xi, yi) : 1 ≤i ≤N}\ncorresponding to a target function f : x 7→y. We wish to approximate this function using the\nneural network\nF(x; θ, Θ)\nwhere θ are the network parameters deﬁned before, while Θ corresponds to the hyper-parameters\nof the network such as the depth L + 1, width H, type of activation function σ, etc. The strategy\nto design a robust network involves three steps:\n1. Find the optimal values of θ (for a ﬁxed Θ) in the training phase.\n2. Find the optimal values of Θ in the validation phase.\n3. Test the performance of the network on unseen data on the testing phase.\nTo accomplish these three tasks, it is ﬁrst customary to split the dataset S into three distinct\nparts: a training set with Ntrain samples, a validation set with Nval samples and test set with\nNtest samples, with N = Ntrain + Nval + Ntest. Typically, one uses around 60% of the samples as\ntraining samples, 20% as validation samples and the remaining 20% for testing.\n15\nSplitting the dataset is necessary as neural networks are heavily over-parameterized functions.\nThe large number of degrees of freedom available to model the data can lead to over-ﬁtting\nthe data. This happens when the error or noise present in the data drives the behavior of the\nnetwork more than the underlying input-output relation itself. Thus, a part of the data is used\nto determine θ, and another part to determine the hyper-parameters Θ. The remainder of the\ndata is kept aside for testing the performance of the trained network on unseen data, i.e., the\nnetwork’s ability to generalize well.\nNow let us discuss how this split is used during the three phases in further details:\nTraining:\nTraining the network makes use of the training set Strain to solve the following\noptimization problem: Find\nθ∗= arg min\nθ\nΠtrain(θ),\nwhere\nΠtrain(θ) =\n1\nNtrain\nNtrain\nX\ni=1\n(xi,yi)∈Strain\n∥yi −F(xi; θ, Θ)∥2\nfor some ﬁxed Θ. The optimal θ∗is obtained using a suitable gradient based algorithm (will be\ndiscussed later). The function Πtrain is referred to as the loss function. In the example above we\nhave used the mean-squared loss function. Later we will consider other types of loss functions.\nValidation:\nValidation of the network involves using the validation set Sval to solve the\nfollowing optimization problem: Find\nΘ∗= arg min\nΘ\nΠval(Θ),\nwhere\nΠval(Θ) =\n1\nNval\nNval\nX\ni=1\n(xi,yi)∈Sval\n∥yi −F(xi; θ∗, Θ)∥2.\nThe optimal Θ∗is obtained using a techniques such as (random or tensor) grid search.\nTesting:\nOnce the \"best\" network is obtained, characterized by θ∗and Θ∗, it is evaluated on\nthe test set Stest to estimate the networks performance on data not used during the ﬁrst two\nphases.\nΠtest =\n1\nNtest\nNtest\nX\ni=1\n(xi,yi)∈Stest\n∥yi −F(xi; θ∗, Θ∗)∥2.\nThis testing error is also known as the (approximate) generalizing error of the network.\nLet’s see an example to better understand how such a network is obtained\nExample 2.4.1. Let us consider an MLP where all hyper-parameters are ﬁxed except for the\nfollowing ﬂexible choices\nσ ∈{ReLU, tanh},\nL ∈{10, 20}.\nWe use the following algorithm\n1. For each possible σ, L pair:\n(a) Find θ∗= arg minθ Πtrain(θ)\n(b) With this θ∗, evaluate Πval(Θ)\n2. Select Θ∗to be the one that gave the smallest value of Πval(Θ).\n3. Finally, report Πtest for this Θ∗and the corresponding θ∗.\n16\n2.5\nGeneralizability\nIf we train a network that has a small value of Πtrain and Πval, does it ensure that Πtest will be\nsmall? This question is addressed by studying the generalizability of the trained network, i.e., it\ncapability to perform well on data not seen while training/validating the network. If the network\nis trained to overﬁt the training data, the network will typically lead to poor predictions on test\ndata. Typically, if Strain, Sval and Stest are chosen from the same distribution of data, then a\nsmall value of Πtrain, Πval can lead to small values of Πtest. Let us look at the commonly used\ntechnique to avoid data overﬁtting, called regularization.\n2.5.1\nRegularization\nNeural networks, especially MLPs, are almost always over-parametrized, i.e., Nθ ≫N where N is\nthe number of training samples. This would lead to a highly non-linear network model, for which\nthe loss function Π(θ) (where we omit the subscript \"train\" for brevity) can have a landscape\nwith many local minimas (see Figure 2.4(a)). Then how do we determine which minima leads to\na better generalization? To nudge the choice of θ∗in a more favorable direction, a regularization\ntechnique can be employed.\n(a) Loss function landscape\n(b) Network sensitivity\nFigure 2.4: The eﬀect of regularization on the loss function. We have assumed a scalar θ for\neasier illustration.\nThe simplest method of regularization involves augmenting a penalty term to the loss function:\nΠ(θ) −→Π(θ) + α∥θ∥,\nα ≥0\nwhere α is a regularization hyper-parameter, and ∥θ∥is a suitable norm of the network parameters\nθ. This augmentation can change the landscape of Π(θ) as illustrated in Figure 2.4(a). In other\nwords, such a regularization encourages the selection of a minima corresponding to smaller values\nof the parameters θ.\nIt is not obvious why a smaller value of θ would be a better choice. To see why this is better,\nconsider the intermediate network output\nx(1)\n1\n= σ(W (1)\n1j x(0)\nj\n+ b(1)\n1 ),\nwhich gives\n∂x(1)\n1\n∂x(0)\n1\n= σ′(W (1)\n1j x(0)\nj\n+ b(1)\n1 )W (1)\n11 ∝W (1)\n11 .\n17\nSince this derivate scales with W (1)\n11 , this implies that |∂F(x)\n∂x(0)\n1\n| scales with W (1)\n11\nas well.\nIf\n|W (1)\n11 | ≫1, then network would be very sensitive to even small changes in the input x(0)\n1 , i.e.,\nthe network would be ill-posed. As illustrated in Figure 2.4(b), using a proper regularization\nwould help avoid over ﬁtting.\nLet us consider some common types of regularization:\n• l2 regularization: Here we use the l2 norm in the regularization term\n∥θ∥= ∥θ∥2 =\n Nθ\nX\ni=1\nθ2\ni\n!1/2\n.\n• l1 regularization: Here we use the l1 norm in the regularization term\n∥θ∥= ∥θ∥1 =\nNθ\nX\ni=1\n|θi|,\nwhich promotes the sparsity of θ.\n2.6\nGradient descent\nRecall that we wish to solve the minimization problem θ∗= arg min Π(θ) in the training phase.\nThis minimization problem can be solved using gradient descent (GD), also known as steepest\ndescent. Consider the Taylor expansion about θ0\nΠ(θ0 + ∆θ) = Π(θ0) + ∂Π\n∂θ (θ0) · ∆θ + ∂2Π\n∂θiθj\n(ˆθ)∆θi∆θj\nfor some ˆθ in a small neighbourhood of θ0. When |∆θ| is small and assuming\n∂2Π\n∂θiθj is bounded,\nwe can neglect the second order term and just consider the approximation\nΠ(θ0 + ∆θ) ≈Π(θ0) + ∂Π\n∂θ (θ0) · ∆θ.\nIn order to lower the value of the loss function as much as possible compared to its evaluation at\nθ0, i.e. minimize ∆Π = Π(θ0 + ∆θ) −Π(θ0), we need to choose the step ∆θ in the opposite\ndirection of the gradient, i.e.:\n∆θ = −η∂Π\n∂θ (θ0)\nwith the step-size η ≥0, also known as the learning-rate. This is yet another hyper-parameter\nthat we need to tune during the validation phase. This is the crux of the GD algorithm, and can\nbe summarized as follows:\n1. Initialize k = 0 and θ0\n2. While |Π(θk)| > ϵ1, do\n(a) Evaluate ∂Π\n∂θ (θk)\n(b) Update θk+1 = θk −η ∂Π\n∂θ (θk)\n(c) Increment k = k + 1\n18\nConvergence: Assume that Π(θ) is convex and diﬀerentiable, and its gradient is Lipschitz\ncontinuous with Lipschitz constant K. Then for a η ≤1/K , the GD updates converges as\n∥θ∗−θk∥2 ≤C\nk .\nHowever, in most scenarios Π(θ) is not convex. If there is more than one minima, then what\nkind of minima does GD like to pick? To answer this, consider the loss function for a scalar θ as\nshown in Figure 2.5, which has two valleys. Let’s assume that the proﬁle of Π(θ) in the each\nvalley can be approximated by a (centered) parabola\nΠ(θ) ≈1\n2aθ2\nwhere a > 0 is the curvature of each valley. Note that the curvature of the left valley is much\nsmaller than the curvature of the right valley. Let’s pick a constant learning rate η and a starting\nvalue θ0 in either of the valleys. Then,\n∂Π\n∂θ (θ0) = aθ0\nand the new point after a GD update will be θ1 = θ0(1 −aη). Similarly, it is easy to see that all\nsubsequent iterates write θk+1 = θk(1 −aη). For convergence, we need\n\f\f\f\f\nθk+1\nθk\n\f\f\f\f < 1\n=⇒|1 −aη| < 1.\nSince a > 0 in the valleys, we will need the following condition on the learning rate\n−1 < 1 −aη =⇒aη < 2.\nIf we ﬁx η, then for convergence we need the local curvature to satisfy a < 2/η. In other words,\nGD will prefer to converge to a minima with a ﬂat/small curvature, i.e., it will prefer the minima\nin the left valley. If the starting point is in the right valley, there is a chance that we will keep\novershooting the right minima and bounce oﬀthe opposite wall till the GD algorithm slingshots\nθk outside the valley. After this it will enter the left valley with a smaller curvature and gradually\nmove towards its minima.\nFigure 2.5: GD prefers ﬂatter minimas.\nWhile it is clear that GD prefers ﬂat minima, what is not clear is why are ﬂat minima better.\nThere is empirical evidence that the parameter values obtained at ﬂat minima tend to generalize\nbetter, and therefore are to be preferred.\n19\n2.7\nSome advanced optimization algorithms\nWe discussed how GD can be used to solve the optimization problem involved in training neural\nnetworks. Let us look at a few advanced and popular optimization techniques motivated by GD.\nIn general, the update formula for most optimization algorithms make use of the following\nformula\n[θk+1]i = [θk]i −[ηk]i[gk]i,\n1 ≤i ≤Nθ,\n(2.8)\nwhere [ηk]i is the component-wise learning rate and the vector-valued function g depends/approximates\nthe gradient. Note that the notation [.]i is used to denote the i-th component of the vector. Also\nnote that the learning rate is allowed to depend on the iteration number k. The GD method\nmakes use of\n[ηk]i = η,\ngk = ∂Π\n∂θ (θk).\nAn issue with the GD method is that the convergence to the minima can be quite slow if η is\nnot suitably chosen. For instance, consider the objective function landscape shown in Figure\n2.6, which has sharper gradients along the [θ]2 direction compared to the [θ]1 direction. If we\nstart from a point, such as the one shown in the ﬁgure, then if η is too large (but still within the\nstable bounds) the updates will keep zig-zagging its way towards the minima. Ideally, for the\nparticular situation shown in Figure 2.6, we would like the steps to take longer strides along the\n[θ]1 compared to the [θ]2 direction, thus reaching the minima faster.\nFigure 2.6: Zig-zagging updates with GD.\nLet us look at two popular methods that are able to overcome some of the issues faced by\nGD.\n2.7.1\nMomentum methods\nMomentum methods make use of the history of the gradient, instead of just the gradient at the\nprevious step. The formula for the update is given by\n[ηk]i = η,\ngk = β1gk−1 + (1 −β1)∂Π\n∂θ (θk),\ng−1 = 0\n20\nwhere gk is a weighted moving average of the gradient. This weighting is expected to smoothen\nout the zig-zagging seen in Figure 2.6 by cancelling out the components of gradient along the [θ]2\ndirection and move more smoothly towards the minima. A commonly used value for β1 is 0.9.\n2.7.2\nAdam\nThe Adam optimization was introduced by Kingma and Ba [10], which makes use of the history\nof the gradient as well the second moment (which is a measure of the magnitude) of the gradient.\nFor an initial learning rate η, the updates are given by\ngk = β1gk−1 + (1 −β1)∂Π\n∂θ (θk)\n[Gk]i = β2[Gk−1]i + (1 −β2)\n\u0012 ∂Π\n∂θi\n(θk)\n\u00132\n[ηk]i =\nη\np\n[Gk]i + ϵ\n(2.9)\nwhere gk and Gk are the weighted running averages of the gradients and the square of the gradients,\nrespectively. The recommended values for the hyper-parameters are β1 = 0.9, β2 = 0.999 and\nϵ = 10−8. Note that the learning rate for each component is diﬀerent. In particular, the larger\nthe magnitude of the gradient for a component the smaller is its learning rate. Referring back to\nthe example in Figure 2.6, this would mean a smaller learning rate for θ2 in comparison to θ1,\nand therefore will help alleviate the zig-zag path of the optimization algorithm.\nRemark 2.7.1. The Adam algorithm also has additional correction steps for gk and Gk to\nimprove the eﬃciency of the algorithm. See [10] for details.\n2.7.3\nStochastic optimization\nWe note that the training loss can be rewritten as\nΠ(θ) =\n1\nNtrain\nNtrain\nX\ni=1\nΠi(θ),\nΠi(θ) = ∥yi −F(xi; θ, Θ)∥2\nThus, the gradient of the loss function is\n∂Π\n∂θ (θ) =\n1\nNtrain\nNtrain\nX\ni=1\n∂Πi\n∂θ (θ)\nHowever, taking the summation of gradients can be very expensive since Ntrain is typically very\nlarge, Ntrain ∼106. One easy way to circumvent this problem is to use the following update\nformula (shown here for the GD method)\nθk+1 = θk −ηk\n∂Πi\n∂θ (θk),\n(2.10)\nwhere i is randomly chosen for each update step k.\nThis is known as stochastic gradient\ndescent. Remarkably, this modiﬁed algorithm does converge assuming that Πi(θ) is convex\nand diﬀerentiable, and ηk ∼1/\n√\nk [19]. To illustrate why ηk needs to decay, consider the toy\nfunction(s) for θ ∈R2\nΠ1(θ) = ([θ]1 −1)2 + ([θ]2 −1)2,\nΠ2(θ) = ([θ]1 + 1)2 + 0.5([θ]2 −1)2,\nΠ3(θ) = 0.7([θ]1 + 1)2 + 0.5([θ]2 + 1)2,\nΠ4(θ) = 0.7([θ]1 −1)2 + 1\n2([θ]2 + 1)2,\nΠ(θ) = 1\n4 (Π1(θ) + Π2(θ) + Π3(θ) + Π4(θ)) .\n(2.11)\n21\nThe contour plots of these functions in shown in Figure 2.7(a), where the black contour plots\ncorresponds to Π(θ). Note that the θ∗= (0, 0) is the unique minima for Π(θ). We consider\nsolving with the SGD algorithm with a constant learning rate ηk = 0.4 and a decaying learning\nrate ηk = 0.4/\n√\nk. Starting with θ0 = (−1.0, 2.0) and randomly selecting i ∈1, 2, 3, 4 for each\nstep k, we run the algorithm for 10,000 iterations. The ﬁrst 10 steps with each learning rate is\nplotted in Figure 2.7(a). We can clearly see that without any decay in the learning rate, the\nSGD algorithm keeps overshooting the minima. In fact, this behaviour continues for all future\niterations as can be seen in Figure 2.7(b) where the norm of the updates does not decay (we\nexpect it to decay to |θ∗| = 0). On the other hand, we quickly move closer to θ∗if the learning\nrate decays as 1/\n√\nk.\nThe reason for reducing the step size as we approach closer to the minima is that far away\nfrom the minima for Π the gradient vector for Π and all the individual Πi’s align quite well.\nHowever, as we approach closer to the minima for Π this is not the case and therefore one is\nrequired to take smaller steps so as not be thrown oﬀto a region far away from the minima.\n(a) Function contours and paths\n(b) Norm of updates\nFigure 2.7: SGD algorithm with and without a decay in the learning rate.\nIn practice, stochastic optimization algorithms are not used for the following reasons:\n1. Although the loss function decays with the number of iterations, it ﬂuctuates in a chaotic\nmanner close the the minima and never manages to reach the minima.\n2. While handling all samples at once can be computationally expensive, handling a single\nsample at a time severly under-utilizes the computational and memory resources.\nHowever, a compromise can be made by using mini-batch optimization. In this strategy, the\ndataset of Ntrain samples is split into Nbatch disjoint subsets known as mini-batches. Each\nmini-batch contains Ntrain = Ntrain/Nbatch samples, which also refered to as the batch-size. Thus,\nthe gradient of the loss function can be approximated by\n∂Π\n∂θ (θ) =\n1\nNtrain\nNtrain\nX\ni=1\n∂Πi\n∂θ (θ) ≈\n1\nNtrain\nX\ni∈batch(j)\n∂Πi\n∂θ (θ).\n(2.12)\n22\nNote that taking Nbatch = 1 leads to the original optimization algorithms, while take Nbatch =\nNtrain gives the stochastic gradient descent algorithm. One typically chooses a batch-size to\nmaximize the amount of data that can be loaded into the RAM at one time. We deﬁne an epoch\nas one full pass through all samples (or mini-batches) of the full training set. The following\ndescribes the mini-batch stochastic optimization algorithm:\n1. For epoch = 1, ..., J\n(a) Randomly shuﬄe the full training set\n(b) Create Nbatch mini-batches\n(c) For i = 1, · · · , Nbatch\ni. Evaluate the batch gradient using (2.12).\nii. Update θ using this gradient and your favorite optimization algorithm (gradient\ndescent, momentum, or Adam).\nRemark 2.7.2. There is an interesting study [31] that suggests that stochastic gradient descent\nmight actually help in selecting minima that generalize better. In that study the authors prove\nthat SGD prefers minima whose curvature is more homogeneous. That is, the distribution of the\ncurvature of each of the components of the loss function is sharp and centered about a small value.\nThis is contrast to minima where the overall curvature might be small; however the distribution\nof the curvature of each component of loss function is more spread out. Then they go on to show\n(empirically) that the more homogeneous minima tend to generalize better than their heterogeneous\ncounterparts.\n2.8\nCalculating gradients using back-propagation\nThe ﬁnal piece of the training algorithm that we need to understand is how the gradients are\nactually evaluated while training the network. Recall the output x(l+1) of layer l + 1 is given by\nAﬃne transform:\nξ(l+1)\ni\n= W (l+1)\nij\nx(l)\nj + b(l+1)\ni\n,\n1 ≤i ≤Hl+1\n(2.13)\nNon-linear transform:\nx(l+1)\ni\n= σ\n\u0010\nξ(l+1)\ni\n\u0011\n,\n1 ≤i ≤Hl+1.\n(2.14)\nGiven a training sample (x, y), set x(0) = x. The value of the loss/objective function (for this\nparticular sample) can be evaluated using the forward pass:\n1. For l = 1, ..., L + 1\n(a) Evaluate ξ(l) using (2.13).\n(b) Evaluate x(l) using (2.14).\n2. Evaluate the loss function for the given sample\nΠ(θ) = ∥y −F(x; θ, Θ)∥2.\nThis operation can be written succinctly in the form of a computational graph as shown in Figure\n2.8. In this ﬁgure, the lower portion of the graph represents the evaluation of the loss function Π.\nWe would of course need to repeat this step for all samples in the training set (or a mini-batch\nfor stochastic optimization). For simplicity, we restrict the discussion to the evaluation of the\nloss and its gradient for a single sample.\n23\nIn order to update the network parameters, we need ∂Π\n∂θ , or more precisely\n∂Π\n∂W (l) , ∂Π\n∂b(l) for\n1 ≤l ≤L + 1. We will derive expressions for these derivatives by ﬁrst deriving expressions for\n∂Π\n∂ξ(l) and\n∂Π\n∂x(l) .\nFrom the computational graph it is easy to see how each hidden variable in the network is\ntransformed to the next. Recognizing this, and applying the chain rule repeatedly yields\n∂Π\n∂ξ(l) =\n∂Π\n∂x(L+1) · ∂x(L+1)\n∂ξ(L+1) · ∂ξ(L+1)\n∂x(L) · · · ∂x(l+1)\n∂ξ(l+1) · ∂ξ(l+1)\n∂x(l)\n· ∂x(l)\n∂ξ(l) .\n(2.15)\nIn order to evaluate this expression we need to evaluate the following terms:\n∂Π\n∂x(L+1) = −2(y −x(L+1))T\n(2.16)\n∂ξ(l+1)\n∂x(l)\n= W (l+1)\n(2.17)\n∂x(l)\n∂ξ(l) = S(l) ≡diag[σ′(ξ(l)\n1 ), · · · , σ′(ξ(l)\nHl)],\n(2.18)\nwhere the last two relations hold for any network layer l, Hl is the width of that particular layer,\nand σ′ denotes the derivative of the activation with respect to its argument. Using these relations\nin (2.15), we arrive at,\n∂Π\n∂ξ(l) =\n∂Π\n∂x(L+1) · S(L+1) · W (L+1) · · · S(l+1) · W (l+1) · S(l).\n(2.19)\nTaking the transpose, and recognizing that Σ(l) is diagonal and therefore symmetric, we ﬁnally\narrive at\n∂Π\n∂ξ(l) = S(l)W (l+1)T S(l+1) · · · W (L+1)T S(L+1)[−2(y −x(L+1))].\n(2.20)\nThis evaluation can also be represented as a computational graph. In fact, as shown in Figure\n2.8, it can be appended to the original graph, where this part of the computation appear in the\nupper row of the graph. Note that we are now traversing in the backward direction. Hence the\nname back propagation.\nP\nA(1)\ns\nA(l+1)\ns\ns\nW(1)\nW(l+1)\nS(L+1)\n𝒙(𝟎)\n𝝃(𝟏)\n𝒙(𝟏)\n𝒙(𝒍)\n𝒙(𝒍&𝟏)\n𝒙(𝑳&𝟏)\n𝝃(𝒍)\n𝝃(𝒍&𝟏)\n𝝃(𝑳&𝟏)\n𝝏𝚷\n𝝏𝝃(𝟏)\n𝝏𝚷\n𝝏𝒙(𝟎)\n𝝏𝚷\n𝝏𝒙(𝒍)\n𝝏𝚷\n𝝏𝒙(𝒍&𝟏)\n𝝏𝚷\n𝝏𝒙(𝑳&𝟏)\n𝝏𝚷\n𝝏𝝃(𝒍)\n𝝏𝚷\n𝝏𝝃(𝒍&𝟏)\n𝝏𝚷\n𝝏𝝃(𝑳&𝟏)\n𝝏𝚷\n𝝏𝒙(𝟏)\nS(1)\nS(l)\nS(l+1)\ns\nFigure 2.8: Computational graph for computing the loss function and its derivatives with respect\nto hidden/latent vectors.\nThe ﬁnal step is to evaluate an explicit expression for\n∂Π\n∂W (l) . This can be done by recognizing,\n∂Π\n∂W (l) = ∂Π\n∂ξ(l) · ∂ξ(l)\n∂W (l) = ∂Π\n∂ξ(l) ⊗x(l−1),\n(2.21)\n24\nwhere [x ⊗y]ij = xiyj is the outer product. Thus, in order to evaluate\n∂Π\n∂W (l) we need x(l−1)\nwhich is evaluted during the forward phase and\n∂Π\n∂ξ(l) which is evaluated during back propagation.\nQuestion 2.8.1. Can you derive a similar set of expressions and the corresponding algorithm to\nevaluate\n∂Π\n∂b(l) ?\nQuestion 2.8.2. Can you derive an explicit expression for ∂x(L+1)\n∂x(0) . That is the an expression for\nthe derivative of the output of the network with respect to its input? This is a very useful quantity\nthat ﬁnds use in algorithms like physics informed neural networks and Wasserstein generative\nadversarial networks.\n2.9\nRegression versus classiﬁcation\nTill now, given the labelled dataset S = {(xi, yi) : 1 ≤i ≤N}, we have considered two types of\nlosses\n• The mean square error (MSE)\nΠ(θ) =\n1\nNtrain\nNtrain\nX\ni=1\n∥yi −F(xi; θ, Θ)∥2.\n• The mean absolute error (MAE)\nΠ(θ) =\n1\nNtrain\nNtrain\nX\ni=1\n∥yi −F(xi; θ, Θ)∥.\nNeural networks with the above losses can be used to solve various regression problems where\nthe underlying function is highly nonlinear and the inputs/outputs are multi-dimensional.\nExample 2.9.1. Given the house/apartment features such as the zip code, the number of\nbedrooms/bathrooms, carpet area, age of construction, etc, predict the outcomes such as the\nmarket selling price, or the number of days on the market.\nNow let us consider some examples of classiﬁcation problems, where the output of the network\ntypically lies in a discrete ﬁnite set.\nExample 2.9.2. Given the symptoms and blood markers of patients with COVID-19, predict\nwhether they will need to be admitted to ICU. So the input and output for this problem would be\nx = [pulse rate, temperature, SPO2, procalcitonin, ...]\ny = [p1, p2]\nwhere p1 is the probability of being admitted to the ICU, while p2 is the probability of not being\nadmitted. Note that 0 ≤p1, p2 ≤1 and p1 + p2 = 1.\nExample 2.9.3. Given a set of images of animals, predict whether the animal is a dog, cat or\nbird. In this case, the input and output should be\nx = the image\ny = [p1, p2, p2]\nwhere p1, p2, p3 is the probability of being a dog, cat or bird, respectively.\n25\nSince the output for the classiﬁcation problem corresponds to probabilities, we need to make\na few changes to the network\n1. Make use of an output function at the end of the output layer that suitably transforms the\noutput vector into the desired form, i.e, a vector of probabilities. This is typically done\nusing the softmax function\nx(L+1)\ni\n=\nexp (ξ(L+1)\ni\n)\nPC\nj=1 exp (ξ(L+1)\nj\n)\nwhere C is the number of classes (and also the output dimension). Verify that with this\ntransformation, the components of the x(L+1) form a convex combination, i.e., x(L+1)\ni\n∈[0, 1]\nand PC\ni=1 x(L+1)\ni\n= 1.\n2. The output labels for the various samples need to be one-hot encoded. In other words, for\nthe sample (x, y), the output label y should have dimension D = C, and whose component\nis 1 only for the component signifying the class x belongs to, otherwise 0. For instance, in\nExample 2.9.3\ny =\n\n\n\n\n\n[1, 0, 0]⊤\nif x is a dog,\n[0, 1, 0]⊤\nif x is a cat,\n[0, 0, 1]⊤\nif x is a pig.\n3. Although the MSE or MSA can still be used as the loss function, it is preferable to use the\ncross-entropy loss function\nΠ(θ) =\n1\nNtrain\nNtrain\nX\ni=1\nC\nX\nc=1\n−yci log(Fc(xi; θ)),\n(2.22)\nwhere yci is the c-th component of the true label for the i-th sample. The loss function in\n(2.22) treats yc and Fc as probability distributions and measures the discrepancy between\nthe two. It can be shown to be related to the Kullback-Liebler divergence between the two\ndistributions. Compared to MSE, this loss function severely penalizes strongly conﬁdent\nincorrect predictions. This is demonstrated in Example 2.9.4\nExample 2.9.4. Let us consider a binary classiﬁcation problem, i.e., C = 2. For a given x,\nlet y = [0, 1] and let the prediction be F = [p, 1 −p]. Clearly, a small value of p is preferred.\nTherefore any reasonable cost function should penalize large values of p. Now let us evaluate the\nerror using various loss functions\n• MSE Loss = (0 −p)2 + (1 −1 + p)2 = 2p2.\n• Cross-entropy Loss = −(0 log(p) + 1 log(1 −p) = −log(1 −p).\nNote that both losses penalize large values of p. Also when p = 0, both losses are zero. However,\nas p →1 (which would lead the wrong prediction), the MSE loss →2, while the cross-entropy\nloss →∞. That is, it strongly penalizes incorrect conﬁdent predictions.\n26\nChapter 3\nResidual neural networks\nResidual networks (or ResNets) were introduced by He et al. [8] in 2015. In this chapter, we will\ndiscuss what these networks are, why they were introduced and their relation to ODEs.\n3.1\nVanishing gradients in deep networks\nWhile training neural networks, the gradients\n∂Π\n∂W (l) , ∂Π\n∂b(l) might become very small. For instance,\nconsider a very deep network, say L ≥20. If\n\f\f\f\n∂Π\n∂W (l)\n\f\f\f ≪1 for l ≤¯l, then the contribution of ﬁrst\n¯l layers of the network will be negligible, as the inﬂuence of their weights on the loss function is\nsmall. Because of this depth cut-oﬀ, the beneﬁt in terms of expressivity of deep networks is lost.\nSo why does this happen? Recall from Section 2.8 that\n∂Π\n∂W (l) = ∂Π\n∂ξ(l) ⊗x(l−1)\nand\n∂Π\n∂ξ(l) = Σ(l)\nL+1\nY\nm=l+1\n(W (m)T Σ(m))\n∂Π\n∂ξ(L+1) .\n(3.1)\nFor any matrix, A, let τ(A) denote the largest singular value. Then we can bound | ∂Π\n∂ξ(l) | by\n| ∂Π\n∂ξ(l) | ≤τ(Σ(l))\nL+1\nY\nm=l+1\n(τ(W (m))τ(Σ(m)))|\n∂Π\n∂ξ(L+1) |.\n(3.2)\nRecall that Σ(m) ≡diag[σ′(ξ(m)\n1\n), · · · , σ′(ξ(m)\nHl )], where σ′ denotes the derivative of σ with\nrespect to its argument. For ReLU its value is either 0 or 1. Therefore τ(Σ(m))) = 1.\nAlso, for stability we would want τ(W (m)) < 1. Otherwise the output of the network can\nbecome unbounded. In practise this is enforced by the regularization term.\nUsing this in the equation above we have\n| ∂Π\n∂ξ(l) | ≤\nL+1\nY\nm=l+1\n(τ(W (m)))|\n∂Π\n∂ξ(L+1) |,\n(3.3)\nwhere each term in the product is a scalar less than 1. As the number of terms increases, that\nis L −l ≫1, this product can, and does, become very small. This typically happens when\n27\nL −l ≈20, in which case | ∂Π\n∂ξ(l) |, and therefore |\n∂Π\n∂W (l) |, become very small. This issue is called\nthe problem of vanishing gradients. It manifests itself in deep networks where the weights in the\ninner layers (say L −l > 20) do not contribute to the network.\nIn [8], the authors demonstrate that taking a deeper network can actually lead to an increase\nin training and validation error (see Figure 3.1). Thus, beyond a certain point, increasing the\ndepth of a network can be counterproductive. Based on our previous discussion on vanishing\ngradients we know why this is the case. Given this, we would like to come up with a network\narchitecture that addresses the problem of vanishing gradients by ensuring\n\f\f\f\n∂Π\n∂ξ(L+1)\n\f\f\f ≈\n\f\f\f ∂Π\n∂ξ(1)\n\f\f\f.\nThis means requiring that when the weights of the network approach small values, the network\nshould approach the identity mapping, and not the null mapping. This is the core idea behind a\nResNet architecture.\nFigure 3.1: Training error (left) and test error (right) on CIFAR-10 data with “plain\" deep\nnetworks (taken from [8]).\n3.2\nResNets\nFigure 3.2: ResNet of depth 6 with skip connections.\nConsider an MLP with depth 6 (as shown in Figure 3.2) with a ﬁxed width H for each hidden\nlayer. We add skip connections between the hidden layers in the following manner\nx(l)\ni\n= σ(W (l)\nij x(l−1)\nj\n+ b(l)\ni ) + x(l−1)\ni\n,\n2 ≤l ≤L.\n(3.4)\nWe can make the following observations:\n1. If all weights (and biases) were null, then x(5) = x(1), which in turn would imply\n∂Π\n∂x(1) =\n∂Π\n∂x(5) ,\n28\nP\nA(1)\ns\nA(l+1)\ns\ns\nW(1)\nW(l+1)\nS(L+1)\n𝒙(𝟎)\n𝝃(𝟏)\n𝒙(𝟏)\n𝒙(𝒍)\n𝒙(𝒍&𝟏)\n𝒙(𝑳&𝟏)\n𝝃(𝒍)\n𝝃(𝒍&𝟏)\n𝝃(𝑳&𝟏)\n𝝏𝚷\n𝝏𝝃(𝟏)\n𝝏𝚷\n𝝏𝒙(𝟎)\n𝝏𝚷\n𝝏𝒙(𝒍)\n𝝏𝚷\n𝝏𝒙(𝒍&𝟏)\n𝝏𝚷\n𝝏𝒙(𝑳&𝟏)\n𝝏𝚷\n𝝏𝝃(𝒍)\n𝝏𝚷\n𝝏𝝃(𝒍&𝟏)\n𝝏𝚷\n𝝏𝝃(𝑳&𝟏)\n𝝏𝚷\n𝝏𝒙(𝟏)\nS(1)\nS(l)\nS(l+1)\nI\nI\nI\nI\nI\nI\ns\nFigure 3.3: Computational graph for forward and backpropagation in a Resnet.\ni.e., we will not have the issue of vanishing gradients.\n2. The computational graph for forward and back-propagation of a ResNet is shown in Figure\n3.3. Looking at this graph, it is clear that the expression for ∂x(l+1)\n∂x(l)\nnow involves traversing\ntwo branches and adding their sum. Therefore, we have\n∂Π\n∂ξ(l) = Σ(l)\nL+1\nY\nm=l+1\n(I + W (m)T Σ(m))\n∂Π\n∂ξ(L+1) .\n(3.5)\nNow, if we assume that |W (m)| ≪1 via regularization, we have\n∂Π\n∂ξ(l) = Σ(l)\u0000I +\nL+1\nX\nm=l+1\nW (m) T Σ(m) + higher order terms\n\u0001\n∂Π\n∂ξ(L+1) .\n(3.6)\nIn the expression above, even if the individual matrices have small entries, their sum need\nnot approach a zero matrix. This implies that we can create a ﬁnite (and signiﬁcant)\nchange between the gradients near the input and output layers, while still requiring the\nweights to be small (via regularization).\nRemark 3.2.1. The above analysis can be extended to cases when H is not ﬁxed, but the analysis\nis not as clean. See [8] on how we can do this.\n3.3\nConnections with ODEs\nLet us ﬁrst consider the special case of a ResNet with d = D = H. Recall the relation (3.4),\nwhich we can rewrite as\nx(l) −x(l−1)\n∆t\n= 1\n∆tσ(W (l)x(l−1) + b(l)) = 1\n∆tσ(ξ(l))\n(3.7)\n29\nfor some scalar ∆t, where we note that ξ(l) is a function of x(l−1) parameterized by θ(l) =\n[W (l), b(l)]. Thus, we can further rewrite (3.7) as\nx(l) −x(l−1)\n∆t\n= V (x(l−1); θ(l)).\n(3.8)\nNow consider a ﬁrst-order system of (possibly non-linear) ODEs, where given x(0) and\n˙x ≡dx\ndt = V (x, t)\n(3.9)\nwe want to ﬁnd x(T). In order to solve this numerically, we can uniformly divide the temporal\ndomain with a time-step ∆t and temporal nodes t(l) = l∆t, 0 ≤l ≤L + 1, where (L + 1)∆t = T.\nDeﬁne the discrete solution as x(l) = x(l∆t). Then, given x(l−1), we can use a time-integrator\nto approximate the solution x(l). We can consider a method motivated by the forward Euler\nintegrator, where the the LHS of (3.9) is approximated by\nLHS ≈x(l) −x(l−1)\n∆t\n.\nwhile the RHS is approximated using a parameter θ(l) as\nRHS ≈V (x(l−1); t(l)) = V (x(l−1); θ(l)).\nwhere we are allowing the parameters to be diﬀerent at each time-step. Putting these two\ntogether, we get exactly the relation of the ResNet given in (3.8). In other words, a ResNet is\nnothing but a descritization of a non-linear system of ODEs. We make some comments to further\nstrengthen this connection.\n• In a fully trained ResNet we are given x(0) and the weights of a network, and we predict\nx(L+1).\n• In a system of ODEs, we are given x(0) and V (x, t), and we predict x(T).\n• Training the ResNet means determining the parameters θ of the network so that x(L+1) is\nas close as possible to yi when x(0) = xi, for i = 1, · · · , Ntrain.\n• When viewed from the analogous ODE point of view, training means determining the right\nhand side V (x, t) by requiring x(T) to be as close as possible to yi when x(0) = xi, for\ni = 1, · · · , Ntrain.\n• In a ResNet we are looking for \"one\" V (x, t) that will map xi to yi, for all 1 ≤i ≤Ntrain.\n3.4\nNeural ODEs\nMotivated by the connection between ResNets and ODEs, neural ODEs were proposed in [4].\nConsider a system of ODEs given by\ndx\ndt = V (x, t)\n(3.10)\nGiven x(0), we wish to ﬁnd x(T). In [4], the RHS, i.e., V (x, t), is deﬁned using a feed-forward\nneural network with parameters θ (see Figure 3.4). The input to the network is (x, t) while the\noutput is V (x, t) (having the same dimension as x). With this description, the system (3.10) is\nsolved using a suitable time-marching scheme, such as forward Euler, Runge-Kutta, etc.\n30\n𝑥!\n𝑥\"\n𝑥#$!\n𝑡\nMLP\n𝑣!\n𝑣\"\n𝑣#$!\nFigure 3.4: Feed-forward neural network used to model the right hand side in a Neural ODE.\nThe number of dependent variables = d −1.\nFigure 3.5: Analogy between regression problems and Neural ODEs.\nSo how do we use this network to solve a regression problem? Assume that you are given the\nlabelled training data S = {(xi, yi) : 1 ≤i ≤Ntrain}. Here both xi and yi are assumed to have\nthe same dimension d −1. The key idea is to think of xi as points in the d −1-dimensional space\nthat represent the initial state of the system, and to think of yi as points that represent the ﬁnal\nstate. Then the regression problem becomes ﬁnding the RHS of (3.10) that will map the initial\npoints to the ﬁnal points with minimal amount of error. In other words, ﬁnd the parameters θ\nsuch that\nΠ(θ) = 1\nN\nN\nX\ni=1\n|xi(T; θ) −yi|2\nis minimized. Here, xi(T; θ) denotes the solution (at time t = T) to (3.10) with x(0) = xi and\nthe RHS represented by a feed-forward neural network V (x, t; θ). Note that yi is the output\nvalue that is measured. There is a relatively straightforward way of extending this approach to\nthe case when xi and yi have diﬀerent dimensions. In summary, in Neural ODEs one transforms\na regression problem to one of ﬁnding the nonlinear, time-dependent RHS of a system of ODEs.\nLet us list the advantages and diﬀerences when comparing Neural ODEs to ResNets:\n• If we interpret the number of time-steps in the Neural ODE as the number of hidden\n31\nlayers L in a ResNet, then the computational cost for both methods is O(L). This is the\ncost associated with performing one forward propagation and one backward propagation.\nHowever the memory cost (the cost associated with storing the weights of each layer), is\ndiﬀerent. For the neural ODE all the weights are associated with the feed-forward network\nused to represent the function V (x, t; θ). Thus the number of weights are independent of\nthe number of time-steps used to solve the ODE. On the other hand, for a ResNet the\nnumber of weights increases linearly with the number of layers, therefore the cost of storing\nthem scales as O(L).\n• In Neural ODEs, we can take the limit ∆t →0 and study the convergence, since this\nwill not change the size of the network used to represent the RHS. However, this is not\ncomputationally feasible to do for ResNets, where ∆t →0 corresponds to the network\ndepth L →∞!\n• ResNet uses a forward Euler type method, but in a Neural ODE one can use any time-\nintegrator. Especially, other higher-order explicit time-integrator like the Runge-Kutta\nmethods that converge to the “exact” solution at a faster rate.\n32\nChapter 4\nSolving PDEs with MLPs\nA number of numerical methods exist to solve PDEs. Some of these are:\n• Finite diﬀerence methods\n• Finite volume methods\n• Finite element methods\n• Spectral Galerkin and collocation methods\n• Deep neural networks!\nTo better appreciate some of these methods, especially deep neural networks, let us consider\na simple model problem describing the scalar advection-diﬀusion problem in one-dimension:\nFind ﬁnd u(x) in the interval x ∈(0, l) such that\nadu\ndx −κd2u\ndx2 = f(x),\nx ∈(0, ℓ)\nu(0) = 0\nu(ℓ) = 1\n(4.1)\nwhere a denotes the advective velocity, κ is the diﬀusion coeﬃcient while f(x) is the source. Such\nequations are used to model many physical phenomena, such as the transport of pollutant by\nﬂuids, or modelling the ﬂow of electrons through semiconductors. The multi-dimensional version\nof this problem will take the form\na · ∇u(s) −κ∆u(s) = f(s),\ns ∈Ω\nu(s) = g(s),\ns ∈∂Ω\n(4.2)\nNote that the model problem is a linear PDE (ODE in the one-dimensional case). Replacing the\nvelocity a by u leads to the viscous Burgers equation.\nThe solution to (4.1) for f ≡0 can be analytically written as\nu(x) = 1 −exp(ax/κ)\n1 −exp(aℓ/κ)\nwhere aℓ/κ is known as the Peclet number (Pe) and measures the ratio of the strength of\nadvection to the strength of diﬀusion. We plot the solution for varying values of a and κ in\nFigure 4.1. Note that for small Pe, the solution is essentially a straight line. But as Pe increases,\nthe solution starts to bend and forming a steeper boundary layer near the right boundary. The\nthickness of this boundary layer is given by δ ≈Pe × l.\nWe will now consider a few methods to numerically solve this toy problem.\n33\n(a) Fixed κ\n(b) Fixed a\nFigure 4.1: Exact solution of (4.1) with ℓ= 1.\n4.1\nFinite diﬀerence method\nThe key steps of a ﬁnite diﬀerence scheme are as follows:\n1. Discretize the domain into a grid of points, with the goal being to ﬁnd the solution at these\npoints.\n2. Approximate the derivates with ﬁnite diﬀerence approximations at these points. This leads\nto a system of (linear or non-linear) algebraic equations.\n3. Solve this system using a suitable algorithm to ﬁnd the solution.\nApplying these steps to (4.1) leads to:\n1. Discretize the domain into N + 1 points, with xi = ih, 0 ≤i ≤N where h = ℓ/N. We\nwish to solve for u(xi) = ui. We also know from the boundary conditions that u0 = 0 and\nuN = 1.\n2. Use the approximations\ndu\ndx(xi) = ui+1 −ui−1\n2h\n+ O(h2)\nd2u\ndx2 (xi) = ui+1 −2ui + ui−1\nh2\n+ O(h2)\nNote that both the approximation used above are second order accurate. They are “central\ndiﬀerence” approximations, as they weigh points on either side of the i-th point with\nthe same magnitude. It is worth mentioning that in the limit of large Peclet number,\na central diﬀerence approximation of the advective term is not ideal since it leads to\nnumerical instability. In such a case, an “upwind” approximation is preferred. Applying\n34\nthe approximations to the PDE at xi, 1 ≤i ≤N −1\naui+1 −ui−1\n2h\n−κui+1 −2ui + ui−1\nh2\n= fi\n⇐⇒ui+1\n\u0010 a\n2h −κ\nh2\n\u0011\n|\n{z\n}\nγ\n+ui\n\u00122κ\nh2\n\u0013\n| {z }\nβ\n+ui−1\n\u0010\n−a\n2h −κ\nh2\n\u0011\n|\n{z\n}\nα\n= fi\nLooking at each node where the solution is unknown (recall that u0 = 0 and uN = 1 are\nknown),\nβu1 + γu2 = −αu0 + f1\nαui−1 + βui + γui+1 = fi,\n∀2 ≤i ≤N −2\nαuN−2 + βuN−1 = −γuN + fN−1\n(4.3)\nCombining all the N −1 equations in (4.3), we get the following linear system\nKu = f\n(4.4)\nwhere the tridiagonal matrix K and the other vectors in (4.4) are deﬁned as\nK =\n\n\nβ\nγ\n0\nα\n...\n...\n...\n...\nγ\n0\nα\nβ\n\n\n∈R(N−1)×(N−1),\nu =\n\u0002\nu1\nu2\n· · ·\nuN−2\nuN−1\n\u0003⊤∈RN−1,\nf =\n\u0002\n−αu0 + f1\nf2\nf3 · · ·\nfN−2\nfN−1\n−γuN + fN−1\n\u0003⊤∈RN−1\n3. Solve u = K−1f.\nNote that:\n• In practice, we never actually invert K as it is computationally expensive. We instead\nuse smart numerical algorithms to solve the system (4.4). For instance, one can use the\nThomas tridiagonal algorithm for this particular system, which is a simpliﬁed version of\nGaussian elimination.\n• We only obtain an approximation ui ≈u(xi). To reduce the approximation error, we\ncould reduce the mesh size h. Alternatively, we could use higher-order ﬁnite diﬀerence\napproximations which would lead to a \"wider stencil\" to approximate the derivates at each\npoint.\n• We can think of each point where we “apply” the PDE as a collocation point. This idea of\napplying the PDE at collocation points is shared by the next method we consider. It is\nalso shared by the method with uses MLPs to solve PDEs.\n4.2\nSpectral collocation method\nSpectral collocation methods seek a solution written as an expansion in terms of a set of smooth\nand global basis functions. The basis functions are chosen a priori, whereas the coeﬃcients of\nthe expansion are unknowns, and are computed by requiring that the numerical solution of the\nPDE is exact at a set of so-called collocation points. More speciﬁcally, this approach involves the\nfollowing steps.\n35\n1. Select a set of global basis functions with the following properties:\n(a) It forms complete basis in the space of functions being considered.\n(b) Is smooth enough so that derivatives can be evaluated.\n(c) Easy to evaluate.\n(d) Derivatives that are easy to evaluate.\nFor instance, one can use the Chebyshev polynomials deﬁned on ξ ∈(−1, 1), given by the\nfollowing recurrence relation\nT0(ξ) = 1,\nT1(ξ) = ξ,\nTn+1(ξ) = 2ξTn(ξ) −Tn−1(ξ)\nThe ﬁrst few Chebyshev polynomials are shown in Figure 4.2. Note that this basis satisﬁes\nall the required properties listed above. It is easy to evaluate at any point because one\ncan use the recurrence relation above and the values of the two lower-order polynomials to\nevaluate the Chebyshev polynomial of the subsequent order. One can also take derivatives\nof the recurrence relation above to evaluate a recurrence relation for derivatives of all\norders.\nFigure 4.2: First few Chebyshev polynomials.\n2. Write the solution as a linear combination of the basis functions {φn(x)}N\nn=0\nu(x) =\nN\nX\nn=0\nunφn(x)\n(4.5)\nwhere un are the basis coeﬃcients. For our toy problem (4.1) (assuming ℓ= 1), we will use\nthe Chebyshev polynomials φn(x) = Tn(2x −1), where the argument is transformed to use\nthese functions on the interval (0, 1).\n3. Evaluate the derivates for the PDE, which for our toy problem will be\ndu\ndx(x) =\nN\nX\nn=0\nunφ′\nn(x) =\nN\nX\nn=0\nun2T ′\nn(2x −1)\nd2u\ndx2 (x) =\nN\nX\nn=0\nunφ′′\nn(x) =\nN\nX\nn=0\nun4T ′′\nn(2x −1)\n(4.6)\n36\n4. Use the boundary conditions of the PDE. For the speciﬁc case of (4.1),\nu(0) = 0 =⇒\nN\nX\nn=0\nunφn(0) =\nN\nX\nn=0\nunTn(−1) = 0,\nu(1) = 1 =⇒\nN\nX\nn=0\nunφn(1) =\nN\nX\nn=0\nunTn(1) = 1\n(4.7)\nwhich leads to 2 linear equations for N + 1 coeﬃcients. We then consider a set of (suitably\nchosen) nodes xi, 1 ≤i ≤N −1 in the interior of the domain, i.e. the collocation points,\nand use the derivatives found in step 3. in the PDE evaluated at these N −1 nodes\na\nN\nX\nn=0\nunφ′\nn(xi) −κ\nN\nX\nn=0\nunφ′′\nn(xi) = f(xi)\n=⇒\nN\nX\nn=0\nun\n\u00002aT ′\nn(2xi −1) −4κT ′′\nn(2xi −1)\n\u0001\n= f(xi)\n(4.8)\nThis leads to an additional N −1 equations for the N + 1 coeﬃcients. Combining (4.7)\nand (4.8) leads to the following linear system\nKu = f\n(4.9)\nwhere\nK ∈R(N+1)×(N+1),\nu =\n\u0002\nu0\nu2\n· · ·\nuN−1\nuN\n\u0003⊤∈RN+1,\nf =\n\u0002\n0\nf(x1)\nf(x1)\n· · ·\nf(xN−2)\nf(xN−1)\n1\n\u0003⊤∈RN+1\n5. Solve u = K−1f.\nWe need to choose the collocation/quadrature points xi properly, so that K has desirable\nproperties that make the linear system (4.9) easier to solve. These include invertibility, positive-\ndeﬁniteness, sparseness, etc.\nRemark 4.2.1. The method is called a collocation method as the PDE is evaluated at the speciﬁc\ncollocation/quadrature points xi.\nRemark 4.2.2. When working with a non-linear PDE, we will end up with a non-linear systems\nof algebraic equations for the coeﬃcients u0, ..., uN, which is typically solved by Newton’s method.\nLet us look at a least-square variant for ﬁnding the coeﬃcients of the expansion of the spectral\nmethods. As done earlier, we still represent the solution using (4.5) and compute its derivates.\nThen, the coeﬃcients u are found by minimizing the following loss function\nΠ(u) = Πint(u) + λΠbc(u)\nΠbc(u) =\n\f\f\f\f\f\nN\nX\nn=0\nunφn(0) −0\n\f\f\f\f\f\n2\n+\n\f\f\f\f\f\nN\nX\nn=0\nunφn(1) −1\n\f\f\f\f\f\n2\nΠint(u) =\n1\nNtrain\nNtrain\nX\ni=1\n\f\f\f\f\fa\nN\nX\nn=0\nunφ′\nn(xi) −κ\nN\nX\nn=0\nunφ′′\nn(xi) −f(xi)\n\f\f\f\f\f\n2\n(4.10)\n37\nThis can be solved using any of the gradient-based methods we have seen in Chapter 2. This\napproach is especially useful when treating non-linear PDEs. In fact, in those cases it is not be\npossible to write a linear system in terms of the coeﬃcients such as (4.9). A few things to note\nhere\n• λ is a parameter used to scale the interior loss and boundary loss diﬀerently.\n• The number of interior point xi can be chosen independently of the number of basis\nfunctions. In other words, Ntrain does not have to be the same as N.\n• We will see in the next section how this variant of the spectral method is very similar to\nhow deep neural networks are used to solve PDEs.\n4.3\nPhysics-informed neural networks (PINNs)\nThe idea of using neural networks to solve partial diﬀerential equations was introduced in 1990-\n2000s by Lagaris et al. [11]. With the renewed interest in using machine learning tools in solving\nPDEs, this idea was rediscovered in 2019 by Raissi et al. [24], and was given the term PINNs\n(physics-informed neural networks). The basic idea of PINNs is similar to regression, except\nthat the loss function Π(θ) contains derivate operators arising in the PDE being considered. We\noutline the main steps below for a one-dimensional scalar PDE, which can easily be extended to\nmulti-dimensional systems of PDEs. We recommend that the reader thinks about the similarities\nand diﬀerences between this method and spectral collocation method described in the previous\nsection.\n1. Select a neural network as a function representation of the PDE solution:\nu = F(x; θ).\n(4.11)\nSome crucial properties required by this representation are:\n(a) Do we have completeness with the representation, i.e., can we accurately approximate\nthe necessary class of function using the representation? The answer is yes, because\nof the universal approximation theorems of neural networks (see Section 2.3.1).\n(b) Is the representation smooth? The answer is yes if the activation function is smooth,\nsuch as tanh, sin, etc. Note that we cannot use ReLU since it does not enough number\nof smooth derivatives.\n(c) Is it easy to evaluate? The answer is yes, due to a quick forward propagation pass.\n(d) Is it easy to evaluate derivates? The answer is yes, due to back-propagation. This will\nbe discussed in detail below.\n2. Given the representation (4.11), we need to ﬁnd θ such that the PDE is satisﬁed in some\nsuitable form. Compare this with spectral collocation approximation given by (4.5), where\nwe need to determine the coeﬃcients un. Note that while the dependence on the coeﬃcients\nun in (4.5) is linear, the dependence on θ in (4.11) can be highly non-linear.\n3. Next we want to ﬁnd the derivatives of the representation. Consider the computational graph\nof the network as shown in Figure 4.3. It comprises alternate steps of aﬃne transformations\nand component-wise nonlinear transformation. The derivative of the output with respect\nto the input can be evaluated by back-propagation. The graph in Figure 4.3 is obtained by\n38\nsimply setting Π = x(L+1) in the graph shown in Figure 2.8. Further, once we recognize\nthat ∂x(L+1)\n∂x(L+1) = 1, the identity matrix, we can easily read from this graph that\n∂x(L+1)\n∂x(0)\n= W (L+1)S(L+1)W (L)S(L) · · · W (2)S(2)W (1)S(1)\nHence, the evaluation of du\ndx requires the extention of the original graph with a backward\nbranch used to evaluate the derivative of the activation function for each component of\nthe vectors ξ(l) (see Figure 4.3). The second derivative d2u\ndx2 is evaluated by performing\nback-propagation of the extended graph. To evaluate higher order derivatives, the graph\nwill need to be extended further in a similar manner. This is what happens behind the\nscenes in Pytorch when a call to \"autograd\" is made.\nA(1)\ns\nA(l+1)\ns\ns\nW(1)\nW(l+1)\nS(L+1)\n𝒙(𝟎)\n𝝃(𝟏)\n𝒙(𝟏)\n𝒙(𝒍)\n𝒙(𝒍&𝟏)\n𝒙(𝑳&𝟏)\n𝝃(𝒍)\n𝝃(𝒍&𝟏)\n𝝃(𝑳&𝟏)\n𝝏𝒙(𝑳#𝟏)\n𝝏𝝃(𝟎)\n𝝏𝒙(𝑳#𝟏)\n𝝏𝒙(𝟎)\n𝝏𝒙(𝑳#𝟏)\n𝝏𝒙(𝒍)\n𝝏𝒙(𝑳#𝟏)\n𝝏𝒙(𝒍#𝟏)\n𝝏𝒙(𝑳#𝟏)\n𝝏𝒙(𝑳#𝟏) = 1 \n𝝏𝒙(𝑳#𝟏)\n𝝏𝝃(𝒍)\n𝝏𝒙(𝑳#𝟏)\n𝝏𝝃(𝒍#𝟏)\n𝝏𝒙(𝑳#𝟏)\n𝝏𝝃(𝑳#𝟏)\n𝝏𝒙(𝑳#𝟏)\n𝝏𝒙(𝟏)\nS(1)\nS(l)\nS(l+1)\ns\nFigure 4.3: Extended graph to evaluate derivatives with respect to network input.\n4. Insert the functional representation of the solution (4.11) into the PDE to ﬁnd the parame-\nters θ. To do this, we ﬁrst deﬁne a set of points S = {xi : 1 ≤i ≤Ntrain} used to train\nthe network, analogous to the set of collocation points in the spectral collocation methods.\nThereafter, we need to deﬁne the loss function (specialized to our toy problem (4.1))\nΠ(θ) = Πint(θ) + λbΠb(θ),\nΠb(θ) = (F(0; θ) −0)2 + (F(1; θ) −1)2 ,\nΠint(θ) =\n1\nNtrain\nNtrain\nX\ni=1\n\u0000aF′(xi; θ) −κF′′(xi; θ)n −f(xi)\n\u00012 .\n(4.12)\nAfter training the network, i.e. solving the minimization problem θ∗= arg min\nθ\nΠ(θ),\nthe solution writes u∗(x) = F(x; θ∗). Note that this is exactly what is done for the least\nsquares variant of the spectral collocation method, where the coeﬃcients un are solved by\nminimizing a similar loss.\nWe make a few remarks:\n• When we are able to ﬁnd θ∗for which Π(θ∗) = 0, this implies Πint(θ∗) = 0 and Πb(θ∗) = 0.\nIn other words, the PDE residuals are zero at the collocation points. This will lead to a\ngood solution as long as the collocation points cover the domain well.\n• There are various ways to improve the accuracy of PINNs, such as\n39\n– Increasing the number of collocation points.\n– Changing the hyper-parameter λb weighting the boundary loss.\n– Increasing the size of the network. That is, increasing Nθ.\n• The boundary conditions (BCs) of a diﬀerential equation carry fundamental physical\nproperties of the phenomena we are trying to describe, and it is paramount that those are\nsatisﬁed by our numerical solution. In the framework of PINNs, BCs are enforced as a\nsoft constrained via the penalization term Πb(θ). Hence, the hyper-parameter λb plays a\ncrucial role in the training of the network, as it balances the interplay between the two loss\nterms during the minimization process. If the gradients of the diﬀerent loss terms are not\nadequately scaled, the convergence to a solution that satisﬁes both the BCs and the PDE\nitself can be extremely slow. This is particularly exacerbated for stiﬀPDEs. To address\nthis issue, diﬀerent self-adaptive techniques to tune the value of λb along the training have\nbeen proposed [29, 16, 3].\n4.4\nExtending PINNs to a more general PDE\nConsider a general PDE: Find the solution u : Ω⊂Rd →RD such that\nL(u(x)) = f(x),\nx ∈Ω\nB(u(x)) = g(x),\nx ∈∂Ω\n(4.13)\nwhere L is the diﬀerential operator, f is the known forcing term, B is the boundary operator,\nand g is the non-homogeneous part of boundary condition (also prescribed).\nAs an example, we can consider the three-dimensional incompressible Navier-Stokes equation\nsolving for the velocity ﬁeld v = [v1, v2, v3] and pressure p on Ω= ΩS × [0, T]. Here ΩS is the\nthree dimensions spatial domain and [0, T] is the time interval of interest. The equation is given\nby\n∂v\n∂t + v · ∇v + ∇p −µ∆v = f,\n∀(s, t) ∈Ω\n∇· u = 0,\n∀(s, t) ∈Ω\nv = 0,\n∀(s, t) ∈∂ΩS × [0, T]\nv(s, 0) = v0(s),\n∀s ∈ΩS.\n(4.14)\nThe ﬁrst equation above is the balance of linear moment. The second equation enforces the\nconservation of mass. The third equation is the no-slip boundary condition which is used when\nthe boundary is rigid and ﬁxed. The fourth equation is the prescription of the initial velocity\nﬁeld.\nTo design a PINN for (4.13), the input to the network should be the independent variables x\nand the output should be the solution vector u. For the speciﬁc case of the Navier-Stokes system\n(4.14), the input to the network would be [s1, s2, s3, t] ∈R4, while the output vector would be\nu = [v1, v2, v3, p] ∈R4. The steps would be the following:\n1. Construct the loss functions\n• Deﬁne the interior residual R(u) = L(u) −f.\n• Deﬁne the boundary residual Rb(u) = B(u) −g.\n• Select suitable Nv collocation points in the interior of the domain and Nb points on\nthe domain boundary to evaluate the residuals. These could be chosen as based on\nquadrature rules, such as Gaussian, Lobatto, Uniform, Random, etc.\n40\nThen the loss function is\nΠ(θ) = Πint(θ) + λbΠb(θ)\nΠint(θ) = 1\nNv\nNv\nX\ni=1\n|R(F(xi; θ)|2\nΠb(θ) = 1\nNb\nNb\nX\ni=1\n|Rb(F(xi; θ)|2\n2. Train the network: ﬁnd θ∗= arg min\nθ\nΠ(θ), and set the solution as u∗= F(x; θ∗)\nWe make some remarks here:\n• It is implicitly assumed that a weight regularization term is also added to the loss Π(θ).\n• Is u∗(x) the exact solution to the PDE? The answer is No!\n– Firstly, Π(θ∗) may not be zero.\n– Even if Π(θ∗) is identically zero, it only means that the residuals vanishes at the\ncollocation points. However, that does not guarantee that the residuals will vanish\neverywhere in the domain. For that Nv, Nb →∞.\n– Also, with a ﬁxed network (Nθ ﬁxed) we cannot represent all functions. For that, we\nwill need Nθ →∞.\n• In practice, we only compute Π(θ∗). Is the solution error ∥e∥= ∥u∗−u∥related to this\nloss value? And if it is, can we say that this error will be small as long as the loss is small?\nThis is what we try to answer in next section.\n4.5\nError analysis for PINNs\nIn order to evaluate the error e = u∗−u, we need to know the exact solution u which is not\navailable in general. We consider a way of overcoming this issue, by restricting our discussion to\nlinear PDEs i.e., L and B are linear operators.\nNote that if u is the exact solution, then\nL(e) = L(u∗−u) = L(u∗) −L(u) = L(u∗) −f = R(u∗)\n(4.15)\nand\nB(e) = B(u∗−u) = B(u∗) −B(u) = B(u∗) −g = Rb(u∗)\n(4.16)\nThus, (4.15) (4.16) lead to a PDE for e driven by the residuals of the MLP solution,\nL(e) = R(u∗),\nin Ω\nB(e) = Rb(u∗),\non Ω\n(4.17)\nIf the residuals of u∗were zero, then e = 0. Unfortunately, these residuals are not zero. The\nmost that we can say is that they are small at the collocation points. However, from the theory\nof stability of well-posed PDEs, we have\n∥e∥L2(Ω) ≤C1\n\u0000∥R(u∗)∥L2(Ω) + ∥Rb(u∗)∥L2(∂Ω)\n\u0001\n(4.18)\n41\nwhere C1 is a stability constant that depends on the PDE, the domain Ω, etc. This is a condition\nthat hols for all well-posed PDEs. It says that if the terms driving the PDE are small, then the\nsolution to the PDE will also be small. This equation tells us that we can control the error if\nwe can control the residuals for the MLP solution. However, in practise we know and control\nΠint, Πb and not ∥R(u∗)∥2\nL2(Ω), ∥Rb(u∗)∥2\nL2(∂Ω). The question then becomes, are these quantities\nrelated. This is answered in the analysis below,\n∥R(u∗)∥L2(Ω) =\n\f\f\fmΩΠint(θ∗)1/2 + ∥R(u∗)∥L2(Ω) −mΩΠint(θ∗)1/2\f\f\f\n≤mΩΠint(θ∗)1/2 +\n\f\f\f∥R(u∗)∥L2(Ω) −mΩΠint(θ∗)1/2\f\f\f\n≤mΩΠint(θ∗)1/2 + C2(Nv)−α\n(4.19)\nwhere mΩis the measure of the domain Ω, C2 and α > 0 will depend on the type of quadrature\npoints chosen. In the equation above the ﬁrst line is obtained by adding and subtracting the\nterm mΩΠint(θ∗)1/2. The second line is obtained by using the triangle inequality. The third line\nis a statement of error in approximating an integral with a ﬁnite sum of values evaluated at\nquadrature points.\nSimilarly for the boundary residual\n∥Rb(u∗)∥L2(∂Ω) ≤m∂ΩΠb(θ∗)1/2 + C3(Nb)−β\n(4.20)\nwhere m∂Ωis the measure of ∂Ω, C3 and β > 0 will depend on the type of boundary quadrature\npoints.\nUsing (4.18), (4.19) and (4.20), we get\n∥e∥L2(Ω) ≤C1\n\n\nmΩΠint(θ∗)1/2 + m∂ΩΠb(θ∗)1/2\n|\n{z\n}\nreduced by Nθ↑\n+ C2(Nv)−α + C3(Nb)−β\n|\n{z\n}\nreduced by Nv,Nb↑\n\n\n\n(4.21)\nThis equation tells us that it is possible to control the error in the PINNs solution by reducing\nthe loss functions (by increasing Nθ) and by increasing the number of interior and boundary\ncollocation points. For further details about this analysis, the reader is referred to [18].\n4.6\nData assimilation using PINNs\nThe problem of data assimilation is often encountered in the science and engineering. In this\nproblem, we are able to make a few sparse measurements of a quantity, and using these we wish\nto evaluate it everywhere on a ﬁne grid. We are also given a physical principal (in the form of a\nPDE) that the variable of interest must adhere to.\nLet us assume that we are given a set of sparse measurements of some quantity u on the\ndomain Ω\nui = u(xi),\nxi ∈Ω,\n1 ≤i ≤M\nFurthermore, we are given that u satisﬁes some constraint R(u) = 0 on Ω. Then, data assimilation\ncorresponds to using this information to ﬁnd the value of u at any x ∈Ω.\nWe can solve this problem using PINNs. First, we represent u using a neural network F(x, θ).\nNext, we deﬁne a loss function\nΠ(θ) = λI\nM\nM\nX\ni=1\n(ui −F(xi, θ))2\n|\n{z\n}\ndata matching\n+ 1\nNv\nM+Nv\nX\ni=M+1\n|R(F(xi, θ))|2\n|\n{z\n}\nphysical constraint\n+ λ∥θ∥2\n| {z }\nsmoothness\n42\nwhere xi, M + 1 ≤i ≤M + Nv are some collocation points chosen to evaluate the residual,\nwhile λI, λ are hyper-parameters. Then we train the network by ﬁnding θ∗= arg min\nθ\nΠ(θ), and\nset the PINNs solution as u∗= F(x; θ∗).\n43\nChapter 5\nConvolutional Neural Networks\nIn the previous chapters, we have seen how to construct neural networks using fully-connected\nlayers. We will now look at a diﬀerent class of layers, called convolution layers, which are very\nuseful when handling inputs which are images. These tasks include classifying images into\ncategories, performing semantic segmentation on images, and transforming images from one type\nto another.\n5.1\nFunctions and images\nConsider a function u(x) deﬁned on x ∈[a, b] × [c, d] ⊂R2. Then we can visualize the discretized\nversion of this function as an image U ∈RN1×N2, where\nU[i, j] = u(ih, jh),\n1 ≤i ≤N1, 1 ≤j ≤N2, h = pixel size.\n(5.1)\nNote that the image in (5.1) deﬁnes a grayscale image where the value of u at each pixel is\njust the intensity. If we work with color images, then it would be a three-dimensional tensor,\nwith the third dimension corresponding to the red, blue and green channels. In other words,\nU ∈RN1×N2×3.\nIf we want to use a fully-connected neural network (MLP) which takes as input a colored 2D\nimage of size 100 × 100, then the input dimension after unravelling the entire image as a single\nvector would be 3 × 104, which is very large. This would in turn lead to very large connected\nlayers which is not computationally feasible. Secondly, when unravelling the image, we lose all\nspatial context of the initial image. Finally, one would expect that local operations, such as\nedge detection, would be the same in any region of the image. Consider the weights for a fully\nconnected layer. These would be represented by the matrix Wij, where the i index represent the\noutput of the linear transform and the j index represents the input. If the operation was the\nsame for every output index, we would apply the same operation for every i and therefore not\nneed the matrix. To address all these issues, we can use the convolution operator on functions.\n5.2\nConvolutions of functions\nThe convolution operator maps functions to functions. Consider the function u(x), x ∈Rd,\nand a suﬃciently smooth kernel function g(x) which decays as |x| →∞. Then the convolution\noperator is given by\nu(x) =\nZ\nRd g(y −x)u(y)dy\n(5.2)\n44\nWe can interpret the convolution operator as sampling u by varying x. For example, in 1D, let\nu(x) and g(x) be as shown in Figure 5.1, and\nu(x) =\nZ\nR\ng(y −x)u(y)dy.\nConsider a point x0. Then g(y −x0) shifts the kernel to the location x0 which will sample the\nfunction u in the orange shaded region. Similarly, for another point x1, g(y −x1) shifts the kernel\nto the location x1 which will sample the function u in the green shaded region. So as the kernel\nmoves, it samples u in diﬀerent windows. Note that the same operation is applied regardless of\nthe value of x. Lets now consider a few typical kernel functions.\n(a) Kernel g(x)\n(b) Sampling\nFigure 5.1: Sampling with shifted kernel in 1D.\n5.2.1\nExample 1\nA popular choice is the Gaussian kernel\ng(ξ) = ρ(|ξ|),\nwhere for any scalar r,\nρ(r) = exp(−r2/2σ2)\np\n(2πσ2)d\n.\nwhich is used as a smoothing/blurring ﬁlter. Here d is the dimension while σ denotes the spread.\nThis kernel in 1D is shown in 5.1(a) for σ = 0.2. Some important properties of this kernel are:\n• It is isotropic, as it only depends on the magnitude of ξ.\n• The integral over the whole domain is unity.\n• It is parameterized by σ, which represents a frequency cut-oﬀ, as scales ﬁner than σ are\nﬁltered out by the convolution. This smoothing eﬀect is demonstrated in Figure 5.2\n45\nFigure 5.2: 1D convolution with Gaussian kernel.\n5.2.2\nExample 2\nLet us consider another example of a kernel that would produce the derivative of a smooth\nversion of u. In 2D, we want this to look like\nu(x) =\n∂\n∂x1\n\u0012Z ∞\n−∞\nZ ∞\n−∞\nρ(|y −x|)u(y)dy1dy2\n\u0013\n=\nZ ∞\n−∞\nZ ∞\n−∞\n∂ρ(|y −x|)\n∂x1\nu(y)dy1dy2\n=\nZ ∞\n−∞\nZ ∞\n−∞\n\u0012\n−∂ρ(|y −x|)\n∂y1\n\u0013\n|\n{z\n}\nrequired kernel\nu(y)dy1dy2\n(5.3)\nThis kernel is shown in both 1D and 2D in Figure 5.3. Note that the action of this kernel look\nlike a smoothed ﬁnite diﬀerence operation. That is the region to the left of the center of the\nkernel is weighted by a negative value and the region to the right is weighted by a positive value.\n5.3\nDiscrete convolutions\nTo evaluate the discrete convolution in 2D, consider (5.2) and discretize it using quadrature.\nThen, we will have\nU[i, j] =\nN1\nX\nm=1\nN2\nX\nn=1\ng[m −i, n −j]U[m, n]\n(5.4)\nwhere we have absorbed the measure h2 into the deﬁnition of the kernel. As in the continuous\ncase, we will assume that g vanishes after a certain distance\ng[p, q] = 0,\n|p|, |q| > ¯N (measure of width of the kernel).\n46\n(a) 1D\n(b) 2D, with a derivative along the 1-direction\nFigure 5.3: Derivative kernel. The blue curve denotes the Gaussian kernel and the orange curve\ndenotes the derivative.\nThus, the limits of the sum are reduced by exclusing all the pixels over which the convolution\nwill be zero,\nU[i, j] =\ni+ ¯\nN\nX\nm=i−¯\nN\nj+ ¯\nN\nX\nn=j−¯\nN\ng[m −i, n −j]U[m, n].\n(5.5)\nLet m′ = m −i and n′ = n −j. Then\nU[i, j] =\n¯\nN\nX\nm′=−¯\nN\n¯\nN\nX\nn′=−¯\nN\ng[m′, n′]U[i + m′, j + n′].\n(5.6)\nThis is precisely how a convolution is applied in deep learning. Thus, the convolution is entirely\ndetermined by\ng[m, n],\n|m|, |n| ≤¯N,\nwhich become the weights of the convolution layer, with the number of weight being ( ¯N + 1)2.\nLet’s consider some examples:\n• A smoothing kernel would be\n1\n8\n\n\n1\n4\n1\n1\n4\n1\n3\n1\n1\n4\n1\n1\n4\n\n≈Gaussian kernel with some σ\n• Kernels that lead to the derivative along the x-direction and y-direction are given by\n\n\n0\n0\n0\n−1\n0\n1\n0\n0\n0\n\n\nand\n\n\n0\n1\n0\n0\n0\n0\n0\n−1\n0\n\n\n• Similarly, the second derivatives anlong the x and y-directions are given by kernels of the\nform\n\n\n0\n0\n0\n−1\n2\n−1\n0\n0\n0\n\n\nand\n\n\n0\n−1\n0\n0\n2\n0\n0\n−1\n0\n\n\n47\n• While the Laplacian is given by the kernel of the type\n\n\n0\n−1\n0\n−1\n4\n−1\n0\n−1\n0\n\n\nRemark 5.3.1. We can have diﬀerent ¯N in diﬀerent directions. That is, we can have kernels\nwith diﬀerent widths along each direction.\n5.4\nConnection to ﬁnite diﬀerences\nThere is a very strong connection between the concept of convolution and the stencil of a ﬁnite\ndiﬀerence scheme. This is made clear in the discussion below.\nSay some function u(x1, x2) is represented on a ﬁnite grid, where the grid points are indexed\nby (i, j) with a grid size h. Then we use the notation U[i, j] = u(xi\n1, xj\n2). Using Taylor series\nexpansion about (i, j)\nU[i + 1, j] −U[i −1, j]\n2h\n= u(xi+1\n1\n, xj\n2) −u(xi−1\n1\n, xj\n2)\n2h\n=\n∂\n∂x1\nu(xi\n1, xj\n2) + O(h2).\nThe operation above is identical to the operation of a discrete convolution with weights given by\n\n\n0\n0\n0\n−1\n0\n1\n0\n0\n0\n\n1\n2h ≈∂u\n∂x1\n.\nThus we may say that this convolution operation approximates a derivative along the 1-direction.\nSimilarly, we can show that\nU[i + 1, j] −2U[i, j] + U[i −1, j]\nh2\n=\n∂\n∂x2\n1\nu(xi\n1, xj\n2) + O(h2).\nand thus the convolution with the kernel given by\n\n\n0\n0\n0\n1\n−2\n1\n0\n0\n0\n\n1\nh2 ≈∂u\n∂x2\n1\n,\napproximates the computation of the second derivative along the 1-direction.\n5.5\nConvolution layers\nThe key things to remember are:\n• Each convolution layer consists of several discrete convolutions, each with its own kernel.\n• The weights of the kernel, which determine its action (smoothing, ﬁrst derivative, second\nderivative etc.), are learnable parameters and are determined when training the network.\nThus the way to think about the learning process is that the network learns the operations\n(convolutions) that are appropriate for its task. The task can be a classiﬁcation problem,\nfor example.\n48\nLet us assume we have an N1×N2 image as an input. Then, we will have multiple convolutions\nin a convolution layer, each of which will generate a diﬀerent image, as shown in Figure 5.4(a).\nThe trainable parameters of this layer are the weights of each convolution kernel. Assuming the\nwidth of the kernels is ¯N in each direction, and there are P kernels, then the number of trainable\nweights will be P × (2 ¯N + 1)2.\nNext let us consider the size of the output image after applying a single kernel operation.\nNote that we will not be able to apply the kernel on the boundary pixels since there are no\npixel-values available beyond the image boundary (see Figure 5.4(b)). Thus, we will have to\nskip ¯N pixels at each boundary when applying the kernel, leading to an output image of shape\n(N1 −¯N + 1) × (N2 −¯N + 1).\nOne way to overcome this is by padding the image with ¯N pixels with zero value on each\nedge. Now we can apply the kernel on the boundary pixels and the output image will be the\nsame size as the input image, as can be seen in Figure 5.4(c).\nAnother feature of convolutions is known as the stride which determines the number of pixels\nby which the kernel is shifted as we move over the image. In the examples above, the stride was\n1 in both directions. In practice, we can choose a stride > 1 which will further shrink the size of\nthe output image. For instance, if stride was taken as S in each direction (with zero-padding\napplied), then the output image size would reduce by a factor of S in each direction (see Figure\n5.4(d)).\n(a) Action of a convolution layer\n(b) Convolution without padding\n(c) Convolution with zero-padding\n(d) Convolution with zero-padding and stride 2\nFigure 5.4: Action of a convolution layer/kernel.\n49\n5.5.1\nAverage and Max Pooling\nPooling operations are generally used to reduce the size of an image, and allowing you to step\nthrough diﬀerent scales of the image. If applied on an image of size N × N over patches of size\nS × S, the new image will have dimensions N\nS × N\nS , where S is the stride of the pooling operation.\nThis is shown in Figure 5.5 for S = 2. Note it is typical to select the patch of pixels over which\nthe max or average is computed to be (S × S), where S is the stride. This is true for Figure 5.5\n(b) but not for 5.5 (a).\nAlso, we show in Figure 5.6 how pooling allows us to move through various scales of the\nimage, where the image gets coarser as more pooling operations are applied. Note that pooling\noperations do not have any trainable parameters. The pooling operation has strong analog\nin similar operators that are used when designing multigrid preconditioners for solving linear\nsystems of algebraic equations.\n(a) Stride 1\n(b) Stride 2\nFigure 5.5: Max pooling applied to an image over patches of size (2 × 2).\n(a) Original\n(b) After 1 pooling op.\n(c) After 2 pooling op.\n(d) After 3 pooling op.\n(e) After 4 pooling op.\n(f) After 5 pooling op.\nFigure 5.6: Max pooling applied repeatedly to an image over patches of size (2 × 2) with stride 2.\n50\n5.5.2\nConvolution for inputs with multiple channels\nAssume that the input to a convolution layer is of size N1 × N2 × C, where C is the number of\nchannels in the input image. Then a convolution layer apply P convolutions on this input and\ngive an output of size M1 × M2 × P. Note that both the spatial resolution as well as the number\nof channels of the output image might be diﬀerent from the input image. Furthermore, if a single\nconvolution in the layer uses a kernel of width k = 2 ¯N + 1, then the kernel will be of the shape\nk × k × C, i.e., the kernel will have k × k weights for each of the C input channels of the input\nimage. Each convolution will act on the input to give an output image of shape M1 × M2 × 1.\nThe output of each convolution are stacked together to give the ﬁnal output of the convolution\nlayer. This can be written as,\n¯U[i, j, k] =\n¯\nN\nX\nm=−¯\nN\n¯\nN\nX\nn=−¯\nN\nC\nX\nc=1\ngk[m, n, c]U[i + m, j + n, c],\n1 ≤i ≤M1, 1 ≤j ≤M2, 1 ≤k ≤P,\nwhere gk is the kernel of the k-th convolution in the layer. Note that the total number of trainable\nparameters will be (2 ¯N + 1) × (2 ¯N + 1) × P × C. This is the type of convolutional layer most\nfrequently encountered in a convolutional neural network, which is described in the following\nsection.\n5.6\nConvolution Neural Network (CNN)\nNow let’s put all the elements together to form the full network. Consider an image classiﬁcation\nproblem. Then the CNN will be given by y = F(x; θ) where x ∈RN1×N2×N3 is the input image\nwith N3 channels, while y ∈RC is the probability vector whose i-th component of denotes the\nprobability that the input image belongs the i-th class among a total of C classes. The y are\ntypically one-hot encoded.\nThe possible architecture of this network is shown in Figure 5.7. This consists of a number of\nconvolution layers followed by pooling layers, which will reduce the spatial resolution of the input\nimage while increasing the number of channels. The output of the ﬁnal pooling layer is ﬂattened\nto form a vector, which is then passed though a number of fully connected layers with some\nactivation function (say ReLU). The ﬁnal fully connected layer reduced the size of the vector to\nC (which is taken to be 10 in the Figure), which is then passed through a softmax function to\ngenerate the predicted probability vector y. Since we are solving a classiﬁcation problem, the\nloss function is taken to be the cross-entropy function\nΠ(θ) = −\nNtrain\nX\ni=1\nC\nX\nc=1\nh\ny(i)\nc log\n\u0010\nFc(x(i); θ)\n\u0011i\n.\nWe train the CNN by trying to ﬁnd θ∗= arg min\nθ\nΠ(θ) with the ﬁnal network being y = F(x; θ∗).\nWe make some important remarks:\n1. The convolution operation is also a linear operation on the input, as is the case for a fully\nconnected layer. The only diﬀerence is that in a fully-connected layer, the weights connect\nevery pixel in the output to every pixel of the input, while in a convolution layer the weights\nconnect one pixel of the output only to a patch of pixels in the input. Furthermore, the\nsame weights are applied on each patch of the input.\n2. In the CNN shown in Figure 5.7, the convolution layers can be interpreted as encoding\ninformation about the input image, while the fully connected layers can be interpreted as\n51\nFigure 5.7: Example of a CNN architecture for an image classiﬁcation problem, for 10 classes.\nusing this information to solve the classiﬁcation problem. This is why in the literature,\nconvolution layers are said to perform feature selection. Further the part of the network\nleading up to the “ﬂattened” vector is sometimes referred to as the encoder.\n3. Sometime, activation is also applied to the output of the convolution layer along with a\nbias\nx(l+1)[i, j, k] = σ(\nX\nm,n,c\ngk[m, n, c]x(l)[i + m, j + n, c] + bk)\nwhere a single bias bk is used for a given output channel k.\n4. In the example above we considered an image classiﬁcation problem. That is, the network\nwas a transform from an image to a class label. We can think of other similar cases. For\nexample, when the network is a transform from an image to a real number. This might\nhave several useful applications in computational physics. Consider the case where you\nwant to create an enstrophy calculator. That is a network that will take as input images of\nthe velocity components of a ﬂuid deﬁned on a grid, and produce as output the integral of\nthe square of the vorticity (called the enstrophy) over the entire domain. Another example\nwould be a network that takes as input the components of the displacement of an elastic\nsolid and produces as output the total strain energy stored within the solid.\n5. The architecture we have considered allows us to transform images to vectors, which is\nuseful in problems involving image classiﬁcation, image analysis, etc. However, there is\nanother architecture that does the opposite, i.e., maps vectors into images. This is useful in\napplications involving image synthesis. Finally, by putting these two architecture together,\nwe can transform an image to a vector and back to another image. Such image-to-image\ntransformations are useful in applications such as image semantic segmentation. These\nideas are described in the following Sections.\n6. It is worth taking a moment to analyze how convolution layers act on images and why\nthey are so useful. When dealing with images input in the context of deep learning, a ﬁrst\nnaive approach could be to ﬂatten the image and feed it to a regular fully connected MLP.\n52\nHowever, this would lead to diﬀerent problems. In fact, for regular images, the size of the\nﬂatten input would be extremely large. In that case, we would have 2 possibilities when\ndeﬁning the architecture of the network:\n(a) We can size the ﬁrst layers of the network to have a width comparable to the (large)\ndimension of the input.\n(b) We can have a sharp decrease in the width of the second hidden layer.\nEither of the strategies would lead to issues. In the ﬁrst case, the size of the network would\nbe too large. Hence, there would be too many trainable parameters which would require\nan unrealistic amount of data to train the network. In the second case, the compression\nof information happening between the ﬁrst two layers would be too aggressive, making\nit very hard for the network to learn how to extract the right features across diﬀerent\nimages. Moreover, important spatial relationship among pixels (like edges, shapes, etc.)\nare lost by ﬂattening an image. Ideally we would like to leverage these relations as much as\npossible, since they carry important spatial information. Convolution layers can solve both\nissues. They allow the input to be a 2D image, while drastically decreasing the number\nof learnable parameters needed for the feature extraction task. In fact, kernels introduce\na limited number of parameters compared to a classic fully connected layer. Since the\nsame kernel is applied at diﬀerent pixel locations in an image, .i.e. parameter sharing, they\nutilize the computational resources in an eﬃcient and smart manner.\n5.7\nTranspose convolution layers\nWe have seen how convolution and pooling layers can be used to scale down images. We now\nconsider layers that do the opposite, i.e, scale up images. To understand what this operation\nwould look like, let us look at a few examples\n1. Consider a 1D image of size 4\nInput =\n\u0002\nu1,\nu2,\nu3,\nu4\n\u0003\nConsider a kernel of size 3 × 1\nk =\n\u0002\nx,\ny,\nz\n\u0003\nConsider a convolution layer with the kernel k, stride 1 and zero-padding layer of size 1.\nThen, the output of the layer acting on the input is\nOutput =\n\u0002\nyu1 + zu2,\nxu1 + yu2 + zu3,\nxu2 + yu3 + zu4,\nxu3 + yu4\n\u0003\nThe steps involved in convolution operator are: pad, dot-product, stride. Note that using\npadding and stride 1 have ensured the output has the same size as the input.\n2. Consider another convolution with the same kernel k, zero-padding layer 1 but stride 2.\nThen, the output of the layer acting on the same input as earlier is\nOutput =\n\u0002\nyu1 + zu2,\nxu2 + yu3 + zu4\n\u0003\nNote that the size of the output has reduced by a factor of 2. In other words, increasing\nthe stride has allowed us to downsample the input. The question we want to ask is whether\nwe can perform an upsampling in a similar way? This can indeed be done by transposing\nevery operation in a convolution layer.\n53\n• Instead of using a dot-product (inner-product), we will use an outer-product.\n• Instead of skipping pixels in the input (stride) we will skip pixels in the output.\n• Instead of padding, we will need to crop the output.\n3. Let us now see an example of a transpose convolution layer. Consider a 1D input image of\nsize 2 × 1\nInput =\n\u0002\nu1,\nu2\n\u0003\nand a kernel of size 3 × 1\nk =\n\u0002\nx,\ny,\nz\n\u0003\n.\nif we perform the outer-product of the input with k, we will get\nOuter product =\n\u0014u1x\nu1y\nu1z\nu2x\nu2y\nu2z\n\u0015\n.\nIf we use a stride of 2, we will need to shift the rows of the outer-product by 2\nStriding =\n\u0014u1x\nu1y\nu1z\n0\n0\n0\n0\nu2x\nu2y\nu2z\n\u0015\n.\nAfter striding is performed we need to add the entries in each column and crop the vector\nto get the output\nOutput = Crop(\n\u0002\nu1x,\nu1y,\nu1z + u2x,\nu2y,\nu2z\n\u0003\n) =\n\u0002\nu1x,\nu1y,\nu1z + u2x,\nu2y\n\u0003\nwhere we have cropped out the last few elements (by convention) to get an output which\nhas 2 times the size of the input.\n4. We consider transpose convolution in 2D applied on a 2D image of size (2 × 2). The kernel\nis taken to be of shape (3 × 3) with stride 2 and padding (cropping). The action of this\ntranspose convolution is shown in Figure 5.8(a), where we ﬁrst obtain an image of size\n(5 × 5) which is then cropped to give an output of size (4 × 4). Note that the output\npixels get an unequal contribution from the various patches (indicated by numbers in the\nFigure), which leads to checker-boarding, which is undesirable. Checker-boarding refers to\npixel-to-pixel variations in the values of the output image.\n5. One way to avoid checker-boarding, is by ensuring that the ﬁlter size is an integer multiple\nof the stride. Let us repeat the previous example but with a (2 × 2) kernel. The operation\nis illustrated in Figure 5.8(b)In this case, we do not need to pad (crop) and each output\npixel has an equal contribution.\nWe make some remarks:\n1. Transpose convolution layers are also called fractionally-strided layers, because for every\none step in the input, we take greater than one step in the output. This is the opposite of\nwhat happens in a convolution layer. In a convolution layer, we take step greater than one\nin the input image, for step of uint one in the output image.\n2. Transpose convolutions are a tool of choice for upscaling through learnable parameters.\n3. Upscaling is typically done with a reduction in the number of channels, which is once again\nthe opposite of what is done in convolution layers.\n54\n(a) kernel size (3 × 3), checker-boarding\n(b) kernel size (2 × 2), no checker-boarding\nFigure 5.8: Example of a transpose convolution operation. The cells marked with red X’s in the\noutput are cropped out. The numbers denote the number of patches that contributed to the\nvalue in the output pixel.\n55\n5.8\nImage-to-image transformations\nImage-to-image to transformations can be seen analogous to function-to-function transformations.\nThese types of networks are typically used in computer vision, super-resolution, style transfer,\nand also in computational physics where we (say) map the source (RHS) ﬁeld to the solution of\nthe PDE.\nWe will discuss a particular type of network for such transformations, which is known as\nU-Nets [26]. In a U-Net (see Figure 5.9), there a down is a downward branch which takes an input\nimage and downscales the images using a number of convolution layers and pooling operations.\nAs we go down this branch, the number of channels typically increase. After we reach the coarsest\nlevel, we have an upward branch that scales up the image and reduced the number of channels\nusing transpose convolution type operations, to ﬁnally give the output image. In addition to\nthese branches, the U-Net also makes use of skip connections that combines information at a\nparticular scale in the downward branch to the information in the upward branch at the same\nscale. These connections are similar to what are used in ResNets. In the upscaling branch of\nthe U-net, if you consider the activation at one point, you will see they come from two diﬀerent\nsources. One of these is the from the same spatial scale in the down-scaling branch of the U-net,\nand the other is from the coarser scales of the upscaling branch of the U-net.\nRemark 5.8.1. The U-net architecture shares many common features with the V-cycle that is\ntypically used in multigrid preconditioners.\nRemark 5.8.2. We can also think of a the U-net as an encode-decoder network with the additional\nfeature of including skip connections.\nFigure 5.9: Example of a U-Net taken from [26].\n56\nChapter 6\nOperator Networks\n6.1\nThe problem with PINNs\nRecall that a typical MLP y = F(x; θ) is a function that takes as input x ∈Rd and gives an\noutput y ∈Rd with trainable weights θ. Also, as we discussed in Chapter 4, a PINN is a network\nof the form u(x) = F(x; θ) taking as input the independent variable x of the underlying PDE\nand giving the solution u(x) (of the PDE) as output. The network is trained by minimizing\nthe weighted sum of the PDE and boundary residual. However, this is just one instance of the\nsolution of the PDE for some given boundary condition and source term. For instance, if we\nconsider the PDE\n∇· (κ∇u) = f(x),\nx ∈Ω= [0, 1] × [0, 1]\nu(x) = g(x),\nx ∈∂Ω\n(6.1)\nand train a PINN F(x; θ) to minimize the loss function\nΠ(θ) = 1\nNv\nNv\nX\ni=1\n|∇· (κ∇F(xi; θ)) −f(xi)|2 + λb\nNb\nNb\nX\ni=1\n|F(xi; θ) −g(xi)|2\nThen, if θ∗= arg min\nθ\nΠ(θ), the PINN solving (6.1) will be u(x) = F(x; θ∗). However, if we\nchange f or g in (6.1), we have no reason to believe that the same trained network would work.\nIn fact, we would need to retrain the network (with perhaps the same architecture) for the new\nf and g. This can be quite cumbersome to do, and we would ideally like to avoid it. In this\nchapter, we will see ways by which we can overcome this issue.\n6.2\nParametrized PDEs\nAssume the the source term f in (6.1) is given as a parametric function f(x; α). For instance,\nwe could have\nf(x1, x2; α) = 4αx1(1 −x1)x2(1 −x2)\nThen we could train a PINN that accommodates for the parametrization by considering a network\nthat takes as input both x and α, i.e., F(x, α; θ). This is shown in Figure 6.1 This network can\nbe trained by minimizing the loss function\nΠ(θ) = 1\nNa\nNa\nX\nj=1\n\"\n1\nNv\nNv\nX\ni=1\n|∇· (κ∇F(xi, αj; θ)) −f(xi, αj)|2 + λb\nNb\nNb\nX\ni=1\n|F(xi, αj; θ) −g(xi)|2\n#\n57\nNote that we have to also consider collocation points for the parameter α while constructing\nthe loss function. If θ∗= arg min\nθ\nΠ(θ), then the solution to the parameterized PDE would\nbe u(x, α) = F(x, α; θ∗). Further, for any new value of α = ˆα we could ﬁnd the solution by\nevaluating F(x, ˆα; θ∗). We could use the same approach if there was a way of parameterizing\nthe functions κ(x) and g(x).\n𝑥!\n𝑥\"\n𝛼\nℱ\n𝑢\nFigure 6.1: Schematic of a PINN with a parameterized input.\nHowever, what if we wanted the solution for an arbitrary, non-parametric f? In order to do\nthis, we need to ﬁnd a way to approximate operators that map functions to functions.\n6.3\nOperators\nConsider a class of functions a(y) ∈A such that a : ΩY\n→RD. The functions in this class\nmight have certain properties, such as a ∈C(ΩY ) or a ∈L2(ΩY ). Also consider the operator\nN : A 7→C(ΩX), with u(x) = N(a)(x) for x ∈ΩX. Let us see some examples of operators N.\n1. Consider the PDE\n∇· (κ∇u) = f,\nx ∈Ω\nu = g,\nx ∈∂Ω\n(6.2)\nFor this PDE, ΩX = ΩY = Ωand the operator N maps the RHS f to the solution\n(temperature) u of the PDE. That is, u = N(f)(x). The input and the output to the\noperator are related by the equation above where it is assumed that κ and g are given and\nﬁxed.\n2. Consider the PDE\n∇· (κ∇u) = f,\nx ∈Ω\nu = g,\nx ∈∂Ω\n(6.3)\nwhich is the same as the previous PDE but we are assuming that the conductivity ﬁeld κ\nmight change for the model, instead of the RHS. Then, ΩX = ΩY = Ωand the operator N\nmaps the conductivity κ to the solution u of the PDE. That is, u = N(κ)(x). The input\nand the output to the operator are related by the equation above where it is assumed that\nf and g are given and ﬁxed.\n58\n3. Once again, consider the same PDE but with conductivity and the boundary condition\nbeing allowed to change\n∇· (κ∇u) = f(x),\nx ∈Ω\nu(x) = g(x),\nx ∈∂Ω\n(6.4)\nThen, the operator N maps the boundary condition g and the conductivity κ to the solution\nu of the PDE. That is, u = N(κ, g)(x). In this case the input to the operator are two\nfunctions (g, κ) and the output is a single function. Therefore ΩX = Ω, while ΩY = Ω× ∂Ω.\nThe input and the output are related through the solution to the PDE above where it is\nassumed that f is given and ﬁxed.\n4. Now consider the equations of linear isotropic elasticity posed on a three-dimensional\ndomain Ω⊂R3,\n∇·\n\u0000λ(∇· u)I + 2µ∇S(u)\n\u0001\n= f(x),\nx ∈Ω\nu(x) = g(x),\nx ∈∂Ω.\n(6.5)\nConsider the operator deﬁned by u(x) = N(f)(x). Here the input function, f : Ω→R3,\nand the output function u : Ω→R3. The two are related by the equations above where\nλ, µ, g are given and ﬁxed.\n5. Now, consider a diﬀerent PDE. In particular, the advection-diﬀusion-reaction equation,\n∂u\n∂t + a · ∇u −κ∇2u + u(1 −u) = f,\n(x, t) ∈Ω× (0, T]\nu(x, t) = g(x, t),\n(x, t) ∈∂Ω× (0, T]\nu(x, 0) = u0(x),\nx ∈Ω.\n(6.6)\nWe want to ﬁnd the operator N maps the initial condition u0 to the solution u at the ﬁnal\ntime T, i.e., u(x, T) = N(u0)(x). In this case ΩX = ΩY = Ω. Further the input and the\noutput functions are related to each other via the solution of the PDE above with a, κ, f, g\ngiven and ﬁxed.\nRemark 6.3.1. It is often useful to determine whether an operator is linear or non-linear. This\nis because if it is linear it can be well approximated by another linear operator. In the cases\nconsidered above the operators in examples 1 and 4 were linear whereas those in examples 2,3,\nand 5 were nonlinear.\nWe are interested in networks that approximate the operator N. We will see how we can do\nthis in the next section. These types of networks are often referred to as Operator Networks.\nThey are two popular versions of these networks. One is referred to as a Deep Operator Network,\nor a DeepONet, and the other is referred to as a Fourier Neural Operator. We describe the\nDeepONet in the next section.\n6.4\nDeep Operator Network (DeepONet) Architecture\nOperator networks were ﬁrst proposed by Chen and Chen [5], where they considered only shallow\nnetworks with a single hidden layer. This idea was rediscovered and extended to deep architectures\nmore recently in [14] and were called DeepONets. A standard DeepONet comprises two neural\nnetworks. We describe below its construction to approximate an operator N : A →U, where\nA is a set of functions of the form a : ΩY ⊂Rd →R while U consists of functions of the form\nu : ΩX ⊂RD →R. Furthermore, we assume that point-wise evaluations of both class of functions\nis possible. The architecture for the DeepONet for this operator is illustrated in Figure 6.2. It is\nexplained below:\n59\n• Fix M distinct sensor points y(1), ..., y(M) in ΩY .\n• Sample a function a ∈A at these sensor points to get the vector a = [a(y(1)), ..., a(y(M))]⊤∈\nRM.\n• Supply a as the input to a sub-network, called the branch net B(.; θB) : RM →Rp, whose\noutput would be the vector β = [β1(a), ..., βp(a)]⊤∈Rp. Here θB are the trainable\nparameters of the branch net. The dimension of the output of the branch is relatively small,\nsay p ≈100.\n• Supply x as an input to a second sub-network, called the trunk net T (.; θT ) : RD →Rp,\nwhose output would be the vector τ = [τ1(x), ..., τp(x)]⊤∈Rp. Here θT are the trainable\nparameters of the trunk net.\n• Take a dot product of the outputs of the branch and trunk nets to get the ﬁnal output of\nthe DeepONet e\nN(., .; θ) : RD × RM →R which will approximate the value of u(x)\nu(x) ≈e\nN(x, a; θ) =\np\nX\nk=1\nβk(a)τk(x).\n(6.7)\nwhere the trainable parameters of the DeepONet will be the combined parameters of the\nbranch and trunk nets, i.e., θ = [θT , θM].\nFigure 6.2: Schematic of a DeepONet\nIn the above construction, once the DeepONet is trained (we will discuss the training in the\nfollowing section), it will approximate the underlying operator N, and allow us to approximate\nthe value of any N(a)(x) for any a ∈A and any x ∈ΩX. Note that in the construction of the\nDeepONet, the M sensor points need to be pre-deﬁned and cannot change during the training\nand evaluation phases.\nWe can make the following observations regarding the DeepONet architecture:\n1. The expression in (6.7) has the form of representing the solution as the sum of a series of\ncoeﬃcients and functions. The coeﬃcients are determined by the branch network, while the\n60\nfunctions are determined by the trunk network. In that sense the DeepONet construction is\nsimilar to that of what is used in the spectral method or the ﬁnite element method. There\nis a critical diﬀerence though. In these methods, the basis functions are pre-determined\nand selected by the user. However, in the DeepONet these functions are determined by the\ntrunk network and their ﬁnal form depends on the data used to train the DeepONet.\n2. Architecture of the branch sub-network: When points for sampling the input function\nare chosen randomly, the appropriate architecture for the branch network comprises fully\nconnected layers. Further recognizing that the dimension of the input to this network can\nbe rather large N1 ≈104, while the output is typically small p ≈102, this network can be\nthought of as an encoder.\nWhen points for sampling the input function are chosen on a uniform grid, the appropriate\narchitecture for the branch network comprises convolutional layer layers. In that case this\nnetwork maps an image of large dimension (N1 ≈104) to a latent vector of small dimension,\np ≈102. Thus it is best represented by a convolutional neural network.\n3. Broadly speaking, there are two ways of improving the experssivity of the DeepONet. These\ninvolve increase the number of network parameters in the branch and trunk sub-networks,\nand increasing the dimension p of the latent vectors formed by these sub-networks.\n6.5\nTraining DeepONets\nTraining a DeepONet is typically supervised, and requires pairwise data. The following are the\nmain steps involved:\n1. Select N1 representative function a(i), 1 ≤i ≤N1 from the set A. Evaluate the values of\nthese functions at the M sensor points, i.e., a(i)\nj\n= a(i)(y(j)) for 1 ≤j ≤M. This gives us\nthe vectors a(i) = [a(i)(y(1)), ..., a(i)(y(M))]⊤∈RM for each 1 ≤i ≤N1.\n2. For each a(i), determine (numerically or analytically) the corresponding functions u(i) given\nby the operator N.\n3. Sample the function u(i) at N2 points in ΩX, i.e., u(i)(x(k)) for 1 ≤k ≤N2.\n4. Construct the training set\nS =\nn\u0010\na(i), x(k), u(i)(x(k))\n\u0011\n: 1 ≤i ≤N1, 1 ≤k ≤N2\no\nwhich will have N1 × N2 samples.\n5. Deﬁne the loss function\nΠ(θ) =\n1\nN1N2\nN1\nX\ni=1\nN2\nX\nk=1\n| e\nN(x(k), a(i); θ) −u(i)(x(k))|2.\n6. Training the DeepONet corresponds to ﬁnding θ∗= arg min\nθ\nΠ(θ).\n7. Once trained, then given any new a ∈A samples at the M sensor points (which gives the\nvector a ∈RM), and a new point x ∈ΩX, we can evaluate the corresponding prediction\nu∗(x) = e\nN(x, a; θ∗).\n61\nRemark 6.5.1. We need not choose the same N2 points across all i in the training set. In fact,\nthese can be chosen randomly leading to a more diverse dataset.\nRemark 6.5.2. The DeepONet can be easily extended to the case where the input comprises\nmultiple functions. In this case, the trunk network remains the same, however the branch network\nnow has multiple vectors as input. The case corresponding to two input functions, a(y) and b(y),\nwhich when sampled yield the vectors, a and b, is shown in Figure 6.3.\n𝒂\n𝒃\n𝒙\nBranch\n𝜷\nTrunk\n𝝉\n𝒖\nDot Product\nFigure 6.3: Schematic of a DeepONet with two input functions.\nRemark 6.5.3. The DeepONet can be easily extended to the case where the output comprises\nmultiple functions (say D such functions). In this case, the output of the branch and trunk\nnetwork leads to D vectors each with dimension p. The solution is then obtained by taking the\ndot product of each one of these vectors. The case corresponding to two output functions u1(x)\nand u2(x) is shown in Figure 6.3.\n𝒂\n𝒙\nBranch\n𝜷𝟐\nTrunk\n𝝉𝟏\n𝒖𝟏\nDot Product\n𝜷𝟏\n𝝉𝟐\n𝒖𝟐\nDot Product\nFigure 6.4: Schematic of a DeepONet with two output functions.\n6.6\nError Analysis for DeepONets\nThere is a universal approximation theorem for a shallow version of DeepONets by Chen and\nChen [5]\n62\nTheorem 6.6.1. Suppose ΩX and ΩY are compact sets in RD (or more generally a Banach\nspace) and Rd, respectively. Let N be a nonlinear, continuous operator mapping V ⊂C(ΩY ) into\nC(ΩX). Then given ϵ > 0, there exists a DeepONet with M sensors and a single hidden layer of\nwidth P in the branch and trunk nets such that\nmax\nx∈ΩX\na∈A\n| e\nN(x, a; θ) −N(a)(x)| < ϵ\nfor a large enough P and M.\nThis result has been extended to a deeper version of the network in [14], and generalized\nfurther by removing the compactness assumptions on the spaces [12].\nRecently in [20], the authors have developed an estimate of the error in a DeepONet that\nclearly pinpoints the diﬀerent sources of error in a DeepONet. This estimate states, the error\nmeasured in the L2(Ω) norm is bounded as\nmax\na∈A ∥e\nN(x, a; θ) −N(a)(x)∥L2(Ω) ≤C\n\u0000ϵh + √ϵt + ϵs + M−α1 + (N2)−α2\u0001\n(6.8)\nwhere ϵh is the error made by the numerical solver used to generate the approximate target\nsolutions u(i) in the training set (as compared to the exact solutions), ϵt is the ﬁnal training\nerror/loss attained, while ϵs bounds the distance of any a ∈A from the set of functions {a(i)}N1\ni=1\nused to construct the construct the training set, i.e., an estimate of how well the training samples\ncovers the input space A. Further, since the input function is evaluated at M ﬁnite sensor nodes,\nwhile the output is evaluated at N2 output nodes, this will lead to an additional discretization\n(or quadrature) error which is given by the last two terms in (6.8). Note that this is similar to\nthe error estimates obtained for PINNs in (4.21).\n6.7\nPhysics-Informed DeepONets\nRecall that DeepONets approximates u(x) = N(a)(x) ≈e\nN(x, a; θ). Assume that the pair a and\nu satisfy a PDE. For example,\n∇· (κu) = f in Ω\nu = g on ∂Ω\n(6.9)\nwhere κ and g are prescribed. To construct the operator N that maps f to u, we need to solve\nthe PDE externally. However, in addition to this, we can also use a PINN-type loss function and\nadd that to the total loss. This is the idea of Physic-Informed DeepONets proposed in [30]. So\nfor the above model PDE, the loss additional physics-based loss would would be,\nΠp(θ) = 1\n¯N1\n1\n¯N2\n¯\nN1\nX\ni=1\n¯\nN2\nX\nk=1\n\f\f\f∇x ·\n\u0010\nκ∇x e\nN(x(k), f (i); θ)\n\u0011\n−f(i)(x(k))\n\f\f\f\n2\n.\n(6.10)\nThis is in addition to the standard data-driven loss function which, for this example is given by\nΠd(θ) =\n1\nN1N2\nN1\nX\ni=1\nN2\nX\nk=1\n| e\nN(x(k), f (i); θ) −u(i)(x(k))|2.\n(6.11)\nThe total loss function is a weighted sum of these two terms:\nΠ(θ) = Πd(θ) + λΠp(θ),\n(6.12)\nwhere λ is a hyper-parameter. A few comments are in order:\n63\n1. The output sensor points used in the physics-based loss function are usually distinct from\nthe output sensor points used in the data-driven loss term. The former represent the\nlocations at which we wish to minimize the residual of the PDE, while the latter represent\nthe points at which the solution is available to us through external means. The total\nnumber of the output sensor points in the physics-based portion of the loss function is\ndenoted by ¯N2, whereas in the data-driven loss function it is denoted by N2.\n2. The set of input functions used to construct the physics-based loss function is usually\ndistinct from the set of input functions used to construct the data-driven loss function.\nThe former set represents the functions for which we wish to minimize the residual of the\nPDE, while the latter set represents the collection of input functions for which the solution\nis available to us through external means. The total number of functions in the set used to\nconstruct the the physics-based portion of the loss function is denoted by ¯N1, whereas the\ntotal number of functions in the set used to construct the data-driven portion of the loss\nfunction is denoted by N −1.\nAs earlier, we train the network by ﬁnding θ∗= arg min\nθ\nΠ(θ) and approximate the solution\nfor a new a by u∗(x) = e\nN(x, a; θ∗). The advantages of adding the extra physics-based loss are:\n1. It reduces the demand on the amount of data in the data-driven loss term. What is means\nis that we don’t have to generate as many solutions of the PDE for training the DeepONet.\n2. It makes the network more robust in that it becomes more likely to produce accurate\nsolutions for the type of input functions not included in the training set for the data-driven\nloss term.\n6.8\nFourier Neural Operators - Architecture\nWe now introduce and discuss Fourier Nerual Operators (FNOs) [13]. We discuss their architecture\nin this section, and then discuss other aspects in the following section.\nOur approach in developing the architecture for a FNO will be to begin with the architecture\nof a typical feed-forward MLP that maps a scalar to another scalar, and systematically extend it\nso that the extended version maps a scalar valued function to another scalar valued function.\nA(1)\ns\nA(l+1)\ns\ns\n𝒙(𝟎)\n𝝃(𝟏)\n𝒙(𝟏)\n𝒙(𝒍)\n𝒙(𝒍&𝟏)\n𝒙(𝑳&𝟏)\n𝝃(𝒍)\n𝝃(𝒍&𝟏)\n𝝃(𝑳&𝟏)\ns\nFigure 6.5: Computational graph for a feed-forward MLP.\nIn Figure 6.5 we have plotted the computational graph of an MLP. We are focused only on\nthe forward part (not the back-propagation) part of this network. For simplcity, we assume that\nthe input x(0) = x is a scalar and the output x(L) = y is also a scalar. Further all the other\nhidden variables (with the exception of ξ(L+1)) are vectors with H components. That is, the\nwidth of each layer is H.\nThe ﬁrst step in this process will be to replace the input and the output with functions. The\ninput will now be the function a(x) : Ω7→R1. Similarly the output is the function u(x) : Ω7→R1.\nThis leads us to the computational graph shown in Figure 6.6.\n64\nA(1)\ns\nA(l+1)\ns\ns\n𝑎(𝒙)\n𝒗(𝟏)\n𝒖(𝟏)\n𝒖(𝒍)\n𝒖(𝒍%𝟏)\n𝒗(𝒍)\n𝒗(𝒍%𝟏)\n𝒗(𝑳%𝟏)\ns\n𝑢(𝒙)\nFigure 6.6: Computational graph for a feed-forward Fourier Neural Operator (FNO) network.\nThe next step is consider the variables in the hidden layers. In the MLP, these were all\nvectors with H components. In the FNO, these will be functions with H components. That is,\nv(1), · · · , v(L), u(1), · · · , u(L) : Ω7→RH.\n(6.13)\nAs shown in Figure 6.6, v(n) and u(n) are the counterparts of ξ(n) and x(n), respectively. Further\nsince ξ(L+1) was a scalar, we will set v(L+1) to be a scalar valued function.\nWe are now done with extending the input, the output and the variables in the hidden layers\nfrom vectors to functions. Next, we need to extend the operators that transform vectors to\nvectors within an MLP to those that transform functions to functions within an FNO.\nWe begin with the operator A(1), which in an MLP is an aﬃne map from a vector with one\ncomponent to a vector with H components. Its straightforward extension to functions is,\nv(1)(x) = A(1)(a)(x),\n(6.14)\nwhere\nv(1)\ni\n(x) = W (1)\ni\na(x) + b(1)\ni ,\ni = 1, · · · , H.\n(6.15)\nHere W (1)\ni\nand b(1)\ni\nare the weights and biases associated with this layer.\nSimilarly, in an MLP the operator A(L+1) is an aﬃne map from a vector with H components\nto a vector with 1 component. It’s straightforward extension to functions is,\nv(L+1)(x) = A(L+1)(u(L))(x),\n(6.16)\nwhere\nv(L+1)(x) = W (L+1)\ni\nu(L)\ni\n(x) + b(L+1),\ni = 1, · · · , H.\n(6.17)\nHere W (L+1)\ni\nand b(L+1) are the weights and the bias associated with this layer.\nNext we describe the action of the activation on input functions. It is a simple extension of\nthe activation function applied to the point-wise values of the input function. That is,\nu(n)(x) = σ(v(n))(x),\n(6.18)\nwhere\nu(n)\ni\n(x) = σ(v(n)\ni\n(x)),\ni = 1, · · · , H.\n(6.19)\nFinally it remains to extend the operators A(n), n = 2, · · · , L to functions. These are deﬁned\nas,\nv(n+1)(x) = A(n+1)(u(n))(x),\n(6.20)\nwhere\nv(n+1)\ni\n(x)\n=\nW (n+1)\nij\nu(n)\nj\n(x) + b(n+1)\ni\n(6.21)\n+\nZ\nΩ\nκ(n+1)\nij\n(y −x)u(n)\nj\n(y)dy,\ni = 1, · · · , H.\n(6.22)\n65\nIn the equation above the summation over the dummy index j (from 1 to H) is implied. The\nnew term that appears in this equation is a convolution. It is motivated by the observation that\na large class of linear operators can be represented as convolutions. An example is the so-called\nGreen’s operator which maps the right hand side (also called the forcing function) of a linear\nPDE to its solution. The functions κ(n+1)\nij\n(z) are called the kernels of the convolution. We note\nthat there are H2 of these functions in each layer.\nIt is instructive to examine a speciﬁc case of a convolution. Let us consider Ω= [0, L1]×[0, L2],\nwhere we denote the two coordinates by either x1 and x2, or y1 or y2. In this case we may write\nthe convolution as,\nvi(x1, x2)\n=\nZ L1\n0\nZ L2\n0\nκij(y1 −x1, y2 −x2)uj(y1, y2) dy2dy1,\ni = 1, · · · , H. (6.23)\nIn the equation above, we have dropped the superscripts since they are not relevant to the\ndiscussion.\nRemark 6.8.1. We may interpret the FNO as a sequence of an aﬃne transform and convo-\nlution followed by a point-wise nonlinear activation. This combination of linear and nonlinear\n(activation) operations allows us to approximate nonlinear operator using this architecture.\nRemark 6.8.2. It is instructive to list all the trainable entities in a FNO. First we list all the\ntrainable parameters:\nW (1)\ni\n, W (2)\nij , · · · , W (L)\nij , W (L+1)\ni\n; b(1)\ni , b(2)\ni , · · · , b(L)\ni\n, b(L+1).\n(6.24)\nThereafter, all the trainable kernel functions\nκ(n)\nij (z),\nn = 2, · · · , L.\n(6.25)\nThe neural operator introduced in this section acts directly on functions and transforms them\ninto functions. However, when implementing this operator on a computer the functions have to\nbe represented discretely. This is described in the following section.\n6.9\nDiscretization of the Fourier Neural Operator\nThe functions that appear in the neural operator described in the previous section are:\na, v(1), u(1), · · · , v(L), u(L), v(L+1), u.\n(6.26)\nEach of these functions is deﬁned on the domain Ω. We discretize this domain with N uniformly\ndistributed points, and represent each function using its values on these points.\nAs an example, in two dimensions, with Ω= [0, L1] × [0, L2], we represent the function\na(x1, x2) as,\na[m, n] = a(x1m, x2n),\nm = 1 · · · , N1, n = 1 · · · , N2.\n(6.27)\nwhere\nx1m\n=\n(m −1) ×\nL1\nN1 −1\n(6.28)\nx1n\n=\n(n −1) ×\nL2\nN2 −1.\n(6.29)\nThe same representation will be used for all other functions.\n66\nWe now have to consider the discrete version of all operations on these functions as well.\nThis is described below for the special case of Ω= [0, L1] × [0, L2].\nWe begin with the operator A(1). The discretized version is\nv(1)[m, n] = A(1)(a)[m, n],\n(6.30)\nwhere\nv(1)\ni\n[m, n] = W (1)\ni\na[m, n] + b(1)\ni ,\ni = 1, · · · , H.\n(6.31)\nSimilarly, the discretized version of the operator A(L+1) is,\nv(L+1)[m, n] = A(L+1)(u(L))[m, n],\n(6.32)\nwhere\nv(L+1)[m, n] = W (L+1)\ni\nu(L)\ni\n[m, n] + b(L+1),\ni = 1, · · · , H.\n(6.33)\nNext we describe the action of the activation function on discretized input functions. It is given\nby\nu(n)[m, n] = σ(v(n))[m, n],\n(6.34)\nwhere\nu(n)\ni\n[m, n] = σ(v(n)\ni\n[m, n]),\ni = 1, · · · , H.\n(6.35)\nFinally it remains to develop the discrete version of the operators A(n), n = 2, · · · , L. These are\ndeﬁned as,\nv(p+1)[m, n] = A(p+1)(u(p))[m, n],\n(6.36)\nwhere\nv(p+1)\ni\n[m, n]\n=\nW (p+1)\nij\nu(p)\nj [m, n] + b(p+1)\ni\n(6.37)\n+\nN1\nX\nr=1\nN2\nX\ns=1\nκ(p+1)\nij\n[r −m, s −n]u(p)\nj [r, s]h1h2,\ni = 1, · · · , H,\n(6.38)\nwhere h1 =\nL1\nN1−1 and h2 =\nL2\nN2−1. Note that the integral in the convolution is now replaced\nby a sum over all the grid points.\nComputing this integral for each value of i and m, n\ninvolves O(N1N2H) ﬂops. And since this needs to be done for H diﬀerent values of i, N1\nvalues of M, and N2 values of j, the total cost of discretizing the convolution operation is\nO(N2\n1 N2\n2 H) = O(N2H2), where N = N1 × N2. The factor of N2 in this cost is not acceptable\nand makes the implementation of this algorithm impractical. In the following section we describe\nhow the use of Fourier Transforms (forward and inverse) overcomes this bottleneck and leads\nto a practical algorithm. This is also the reason that this algorithm is referred to as a “Fourier\nNeural Operator.\"\n6.10\nThe Use of Fourier Transforms\nConsider a periodic function u(x2, x2) deﬁne on Ω≡[0, L1]×[0, L2]. If this function is suﬃciently\nsmooth it may be approximated by a truncated Fourier series,\nu(x1, x2) ≈\nN1/2\nX\nm=−N1/2\nN2/2\nX\nn=−N2/2\nˆu[m, n]e2πi( mx1\nL1 + nx2\nL2 ).\n(6.39)\nHere N1 and N2 are even integers, the coeﬃcients ˆu[m, n] are the Fourier coeﬃcients and i = √−1.\nWe note that while the function u is real-valued the coeﬃcients are complex-valued. However,\n67\nsince u is real-valued, they obey the rule ˆu[−m, −n] = ˆu∗[m, n], where (.)∗denotes the complex-\nconjugate of a complex number. The approximation can be made more accurate by increasing\nN1 and N2, and as these numbers tend to inﬁnity, we recover the equality. The relation above is\noften referred to as the inverse Fourier transform, since it maps the Fourier coeﬃcients to the\nfunction in the physical space.\nThe forward Fourier transform (which maps the function in the physical space to the Fourier\ncoeﬃcients) can be obtained from the relation above by\n1. Multiplying both sides by e−2πi( rx1\nL1 + sx2\nL2 ), where r and s are integers.\n2. Integrating both sides over Ω.\n3. Recognizing that the integral\nR\nΩe2πi( (m−r)x1\nL1\n+ (n−s)x2\nL2\n)dx1dx2 is non-zero only when m = r\nand n = s, and in that case it evaluates to L1L2.\nThese steps yield the ﬁnal relation:\nˆu[r, s] =\n1\nL1L2\nZ L1\n0\nZ L2\n0\nu(x1, x2)e−2πi( rx1\nL1 + sx2\nL2 )dx1dx2.\n(6.40)\nWe now describe how Fourier transforms can be used to evaluate the convolution eﬃciently.\nTo do this we consider the special case of 2D convolution in (6.23). We begin with substituting\nuj(y1, y2) = PN1/2\nm=−N1/2\nPN2/2\nn=−N2/2 ˆuj[m, n]e2πi( my1\nL1 + ny2\nL2 ) in this equation to get,\nvi(x1, x2)\n=\nZ L1\n0\nZ L2\n0\nκij(y1 −x1, y2 −x2)\nX\nm,n\nˆuj[m, n]e2πi( my1\nL1 + ny2\nL2 ) dy2dy1\n=\nX\nm,n\nˆuj[m, n]\nZ L1\n0\nZ L2\n0\nκij(y1 −x1, y2 −x2)e2πi( my1\nL1 + ny2\nL2 ) dy2dy1\n=\nX\nm,n\nˆuj[m, n]\nZ L1−x1\n−x1\nZ L2−x2\n−x2\nκij(z1, z2)e2πi( m(z1+x1)\nL1\n+ n(z2+x2)\nL2\n) dz2dz1\n=\nX\nm,n\nˆuj[m, n]e2πi( mx1\nL1 + nx2\nL2 )\nZ L1\n0\nZ L2\n0\nκij(z1, z2)e2πi( mz1\nL1 + nz2\nL2 ) dz2dz1\n=\nL1L2\nX\nm,n\nˆuj[m, n]ˆκij[−m, −n]e2πi( mx1\nL1 + nx2\nL2 ).\n(6.41)\nIn the development above, in going from the ﬁrst to the second line we have taken the summation\noutside the integral and recognized that the coeﬃcients ˆuj[m, n] do not depend on y1 and y2.\nIn going from the second to the third line we have introduced the variables z1 = y1 −x1 and\nz2 = y2 −x2. In going from the third to the fourth line we have made use of the fact that\nthe functions κij(z1, z2) are periodic. Finally in going from the fourth to the ﬁfth line we have\nmade use of the deﬁnition of the Fourier Transform (6.40). This ﬁnal relation tells us that the\nconvolution can be computed by:\n1. Computing the Fourier Transform of uj.\n2. Computing the Fourier Transform of κij.\n3. Computing the product of the coeﬃcients of these two transforms.\n4. Computing the inverse Fourier Transform of the product.\n68\nNext, we account for the fact that we will only work with the discrete forms of the functions\nuj and κij. This means that we evaluate the inverse Fourier transform (6.39) at a ﬁnite set of\ngrid points. Further, it means that we have to approximate the integral in the Fourier transform\n(6.40). This alternate form is given by\nˆu[r, s] = h1h2\nL1L2\nN1\nX\nm=1\nN2\nX\nn=1\nu[m, n]e−2πi( rx1m\nL1 + sx2n\nL2 ).\n(6.42)\nHere h1 = L1\nN1 and h2 = L2\nN2 , x1m = (m −1)h1 and x2n = (n −1)h2.\nThe ﬁnal observation is that the evaluating the sums in (6.39) and (6.42) require O(N2)\noperations. This would make the evaluation of the convolution via the Fourier method impractical\nexcept for when N is very small. However, the use of Fast Fourier Transform (FFT) reduces this\ncost to O(N log N). Thus the cost of implementing the convolution reduces to O(N log NH2).\nThis makes the implementation of Fourier Neural Operators practical.\n69\nChapter 7\nProbabilistic Deep Learning\nSo far, we have considered regression and classiﬁcation problems, where for a given input x we\nneed to compute a single output y. However, this may not be enough for many problems of\ninterest. In fact, there may be many y’s for a given x. For example,\n1. y and x might be measured with some random noise.\n2. y and x might be inherently stochastic. For instance, y could be the pressure measured in\na turbulent ﬂow at some point x in space.\n3. inverse problems can have multiple solutions. For instance, the forward/direct problems\nwould be determining the temperature ﬁeld given the head conductivity, while the inverse\nproblem could be determining the conductivity ﬁeld given the (possibly noisy) temperature\nﬁeld.\nThus, we need to formulate a probabilistic framework to use deep learning algorithms to\nsolve such problems. Recall, that our deterministic model was given by y = F(x; θ). In the\nprobabilistic setup, y, x and θ are treated as random variables.\nBefore we can work with random variables we need to understand some key elements of the\ntheory of probability that are necessary in deﬁning random variables.\n7.1\nKey elements of Probability Theory\nA random experiment is described by a procedure and a set of one or more observa-\ntions/measurements. For example,\n1. Observe a switch and determine whether it is on or oﬀ.\n2. Toss a coin 3 times and note the sequence of heads H or tails T.\n3. Toss a coin 3 times and count the number of times H appears. Note that this is the same\nexperiment as earlier but the measurement is diﬀerent.\n4. Spin a spinner, and measure the ﬁnal angle in radians.\nThe outcome is the results of the experiment that cannot be broken down into smaller parts.\nThe sample space, denoted by S, is the set of all possible outcomes of an experiment. For each\nof the four random experiments observed above, we have\n1. S = {on, oﬀ}.\n70\n2. S = {TTT, TTH, THT, HTT, THH, HTH, HHT, HHH}.\n3. S = {0, 1, 2, 3}.\n4. S = {ψ : ψ ∈(0, 2π]}.\nNote that there is a fundamental diﬀerence between the ﬁrst 3 experiments, where S is discrete\nand countable, and the last experiment where the S is uncountable.\nAn event is a collection of outcomes, i.e., a subset of S. Typically the outcomes in an event\nsatisfy a condition. Let’s see some examples for the above experiments\n1. A = {on} or A = {on, oﬀ} = S .\n2. A are all outcomes with at least 2 H, i.e., A = {THH, HTH, HHT, HHH}.\n3. A are all outcomes with at least 2 H, i.e., A = {2, 3}. If we deﬁne B to be all outcomes\nwith 4 H, then no outcome would satisfy this condition, i.e., B = ∅the null set.\n4. A are all outcomes with ψ > π/4, i.e, A = {ψ : ψ ∈(π/4, 2π]}.\nAn event class E is a collection of all event (sets) over which probabilities can be deﬁned.\nWhen S is countable, E is all subsets of S. When S is not countable, E is the Borel ﬁeld (or\nBorel algebra), which is the collection of all open and closed sets in S.\nThe probability law is a rule that assigns a probability to all sets in an event class E . We\nlist the axioms of probability, which are the requirements of a probability law.\nConsider a sample space S for an experiment and the corresponding event class E . Let\nP : E 7→[0, 1] satisfy\n1. P[A] ≥0 for all A ∈E .\n2. P[S] = 1.\n3. If A1, A2, ... are events such that Ai ∩Aj = ∅for all i ̸= j, i.e., the events are mutually\nexclusive, then\nP[\n∞\n[\ni=1\nAi] =\n∞\nX\ni=1\nP[Ai].\nAny assignment P that satisﬁes the above conditions is said to be a valid probability law. Note\nthat probability is like mass. It is non-negative (axiom 1), conserved (total mass is always 1,\naxiom 2), and for distinct points the total mass is obtained by adding individual masses (axiom\n3).\nIf S is countable, then it is suﬃcient to deﬁne a probability law for all elements of S, i.e., for\nall elementary outcomes, while making sure that the probabilities are non-negative and add up\nto 1 (the ﬁrst two axioms). Let us try to assign probability laws for the ﬁrst three examples\nwhich have a countable S using these criteria.\n1. For some p ∈[0, 1], deﬁne P[on] = p; P[oﬀ] = 1 −p.\n2. For a fair die with no memory, P[ai] = 1/8, where ai ∈S, i = 1, · · · , 8.\n3. For a fair die with no memory, P[0] = 1/8, P[1] = 3/8, P[2] = 3/8, P[3] = 1/8.\nRemark 7.1.1. As an exercise, verify that the axioms are satisﬁed for each of these cases.\n71\nFor a continuous sample space, it is suﬃcient to deﬁne a probability law for all open and\nclosed intervals, while ensuring axioms 1 and 2. Let us consider the fourth example which\nhas an uncountable S. If the spinner is completely unbiased, then the probability is uniformly\ndistributed. Then for b ≥a, we say that P[(a, b]] = (b −a)/(2π). Note that the probability of\nsingleton sets in a continuous sample space is zero (for any distribution).\nRemark 7.1.2. From this point on, whenever we talk about the sample space S, we will im-\nplicitly assume that we are referring to the triplet (S, E , P). This triplet is also known as a\n\"measure space\".\n7.2\nRandom Variables\nA random variable X is a function deﬁned from S to the real line with the property that the set\nAb = {ξ ∈S : X(ξ) ≤b} belongs to E for all b ∈R. Note that in the measure theoretic language,\nwe are requiring X to be a measurable function. Also note that according to the deﬁnition of Ab,\nwe are enforcing the requirement that we should be able to evaluate P[Ab] for all b ∈R.\nLet us deﬁne random variables (RVs) for the above examples:\n1. For S = {on, oﬀ} with P[on] = p, P[oﬀ] = 1 −p, deﬁne the RV\nX(ξ) =\n(\n0\nif ξ = oﬀ\n1\nif ξ = on.\n(7.1)\nThis is also known as a Bernoulli Random Variable.\n2. For S = {TTT, TTH, THT, HTT, THH, HTH, HHT, HHH} with P[ai] = 1/8 for\nall ai ∈S, deﬁne\nX(ξ) = Number of heads in ξ.\n(7.2)\nNote that this is the random event that was described in Experiment 3.\n3. This random event is already a random variable.\n4. For the spinner experiment with S = {ψ : ψ ∈(0, 2π]} with P[(a, b]] = (b−a)/(2π), deﬁne\nX(ψ) = ψ\n2π.\n(7.3)\nIf X is deﬁned on a discrete sample space, it is called a discrete random variable, while if it is\ndeﬁned on a continuous sample space, it is called a continuous random variable.\nAs described above, a random variable inherits its probabilistic interpretation from the measure\nspace used to deﬁne it. In the following sections we deﬁne the probabilistic interpretation of a\nrandom variable.\n7.2.1\nCumulative distribution function\nThe cumulative distribution function (cdf) of a random variable X is given by\nFX(x) = P[ξ : X(ξ) ≤x]\nwhich deﬁnes a probability on R of X taking values in the interval (−∞, x]. Let us deﬁne the\ncdf for the above examples:\n1. For the Bernoulli RV deﬁned by (7.1)\n72\n• if x < 0, then FX(x) = P[∅] = 0\n• if 0 ≤x < 1, then FX(x) = P[{oﬀ}] = 1 −p\n• if x ≥1, then FX(x) = P[{on, oﬀ}] = 1\nThe full cdf is shown in Figure 7.1(a).\n2. For the RV deﬁned by (7.2)\n• if x < 0, then FX(x) = P[∅] = 0\n• if 0 ≤x < 1, then FX(x) = P[#ofH = 0] = P[{TTT}] = 1/8\n• if 1 ≤x < 2, then FX(x) = P[#ofH = 0, 1] = 1/8 + 3/8 = 4/8\n• if 3 ≤x < 3, then FX(x) = P[#ofH = 0, 1, 2] = 1/8 + 3/8 + 3/8 = 7/8\n• if x ≥3, the FX(x) = P[#ofH = 0, 1, 2, 3] = 1\nThe full cdf is shown in Figure 7.1(b).\n3. This random variable is the same as Example 2.\n4. For the spinner experiment with the RV deﬁned by (7.3)\nFX(x) = P[ψ : X(ψ) ≤x] = P[{ψ : ψ ≤2πx}]\n• if x < 0, then FX(x) = P[∅] = 0\n• if 0 ≤x < 1, then FX(x) = P[{ψ ∈(0, 2πx]}] = 2πx\n2π = x\n• if x ≥1, then FX(x) = P[{ψ ≤2π}] = 1\nThe full cdf is shown in Figure 7.1(c).\n(a) Bernoulli\n(b) 3 coin tosses\n(c) Spinner\nFigure 7.1: Examples of cumulative distribution functions\nLet us discuss some properties of FX:\n1. 0 ≤FX(x) ≤1.\n2. limx→∞FX(x) = 1.\n3. limx→−∞FX(x) = 0.\n73\n4. FX is monotonically increasing.\n5. The cdf is always continuous from the right\nFX(x) = lim\nh→0+ Fx(x + h).\nNote that the FX for discrete RV (see Figure 7.1) are discontinuous at ﬁnitely many x. In\nfact, the cdf for discrete RVs can be written as a ﬁnite sum of the form\nFX(x) =\nK\nX\nk=1\npkH(x −xk),\nK\nX\nk=1\npk = 1,\nwhere pk is the probability mass and H is the Heaviside function\nH(x) =\n(\n1\nif x > 0\n0\nif x ≤0 .\nRemark 7.2.1. Once we have the FX we can calculate the probability that X will take values in\n\"any\" interval in R, i.e., we can compute P[a < X ≤b]. Note that\nFX(b) = P[X ≤b] = P[(X ≤a) ∪(a < X ≤b)]\n= P[X ≤a] + P[a < X ≤b]\n(mutually exclusive events)\n= FX(a) + P[a < X ≤b].\nThus,\nP[a < X ≤b] = FX(b) −FX(a).\n7.2.2\nProbability density function\nWe deﬁne the probability density function (pdf). For a continuous FX, it is deﬁned as\nfX(x) = d\ndxFX(x)\n(7.4)\nwhich enjoys the following properties inherited from the cdf:\n1. fX(x) ≥0, ∀x ∈R, since FX is monotonically increasing.\n2. limx→−∞fX(x) = limx→∞fX(x) = 0.\n3. Integrating (7.4) from (−∞, x] gives us\nZ x\n−∞\nfX(y)dy = FX(x) −\nlim\nx→−∞FX(x) = FX(x).\n4. Also\nP[a < X ≤b] = FX(b) −FX(a) =\nZ b\n−∞\nfX(y)dy −\nZ a\n−∞\nfX(y)dy =\nZ b\na\nfX(y)dy.\nThus, the integral of a pdf in an interval gives the \"probability mass\" which is the probability\nthat the RV lies in that interval. This is the reason why the pdf is called a \"density\".\n74\n5. Furthermore,\nZ ∞\n−∞\nfX(y)dy = lim\nx→∞FX(x) −\nlim\nx→−∞FX(x) = 1.\n6. For a very small h > 0, we have the interpretation\nP[a < X ≤a + h] =\nZ a+h\na\nfX(y)dy ≈hfX(a).\nNote that as h →0+, P[a < X ≤a + h] →0. That is, for a continuous RV the probability\nof attaining a single value is zero.\n7.2.3\nExamples of Important RVs\nLet us look at some important random variables and the associated cdf, pdf (also see Figure 7.2):\n1. Uniform RV: for some interval (a, b], the pdf is given by\nfX(x) =\n(\n1\nb−a\nif x ∈(a, b]\n0\nother wise ,\nwhile the cdf is given by\nFX(x) =\n\n\n\n\n\n0\nif x < a\nx−a\nb−a\nif x ∈(a, b]\n1\nif x > b\n.\n2. Exponential RV: used to model lifetime of devices/humans after a critical event. In\nthis case, X represents the time to failure and P[X > x] = e−λx where λ > 0 is a model\nparameter which denotes the rate of failure. Thus,\nFX(x) = P[X ≤x] = 1 −P[X > x] = 1 −e−λx,\nand\nfX(x) = d\ndxFX(x) = λe−λx.\n3. Gaussian RV: used to model natural things like height, weight, etc. In fact, through the\nCentral Limit Theorem, this is also the distribution given by an aggregate of many RVs.\nThe pdf is given by\nfX(x) =\n1\n√\n2πσe−1\n2( x−µ\nσ )\n2\nwhich is parameterized by the mean µ which denotes the center of this distributions, and\nthe variance σ2 which denotes its spread. The corresponding cdf is given by\nFX(x) = 1\n2\n\u0014\n1 + erf\n\u0012x −µ\nσ\n√\n2\n\u0013\u0015\n,\nerf(x) =\n2\n√π\nZ x\n0\ne−t2dt.\nIn probabilistic Machine Learning one makes extensive use of uniform and Gaussian random\nvariables.\n75\n(a) Uniform RV (a = −1, b = 1)\n(b) Exponential RV (λ = 0.8)\n(c) Gaussian RV\nFigure 7.2: Continuous random variables\n7.2.4\nExpectation and variance of RVs\nGiven a RV X with pdf fX, we can calculate its expected value or expectation or mean as\nµX := E[X] =\nZ ∞\n−∞\nxfX(x)dx.\nThe expectation has the following properties:\n• Note that if a pdf is symmetric about x = m, then E[X] = m. To see this, note that\n(m −x)fX(x) will be anti-symmetric about m. Thus\n0 =\nZ ∞\n−∞\n(m−x)fX(x)dx = m\nZ ∞\n−∞\nfX(x)dx−\nZ ∞\n−∞\nxfX(x)dx\n=⇒\nZ ∞\n−∞\nxfX(x)dx = m.\nUsing this property, we can easily say the mean for a uniform RV is (a + b)/2, while for a\nGaussian RV it is µ.\n• E[c] = c for a constant c.\n• We can calculate the expected value of functions of RVs as\nE[g(X)] =\nZ ∞\n−∞\ng(x)fX(x)dx.\n• The expectation is linear, i.e.,\nE[g(X) + ch(X)] = E[g(X)] + cE[h(X)].\nThe variance of a RV measures its variation about the mean. It is evaluated as\nVAR[X] =\nZ ∞\n−∞\n(x −µX)2fX(x)dx.\nFurthermore, we denote the standard deviation as\nσX := STD[X] =\np\nVAR[X].\n76\nFor a uniform RV\nVAR[X] =\nZ b\na\n\u0012\nx −b + a\n2\n\u00132\n1\nb −adx = (b −a)2\n12\n.\nFor a Gaussian RV, we ﬁrst use the property of the pdf to write\nZ ∞\n−∞\ne−1\n2( x−µ\nσ )\n2\ndx =\n√\n2πσ.\nTaking a derivative with respect to σ on both sides lead to\nZ ∞\n−∞\ne−1\n2( x−µ\nσ )\n2\n(x −µ)2σ−3dx =\n√\n2π\nwhich after a bit of algebra gives us\nVAR[X] =\nZ ∞\n−∞\n(x −µ)2\n1\n√\n2πσe−1\n2( x−µ\nσ )\n2\ndx = σ2.\n7.2.5\nPair of RVs\nIn probabilistic ML we deal with multiple random variables. For example, the input, the output\nand the weights might all be RVs. Thus we need to extend concepts from a single RV to a vector\nof RVs. We do this in this section by ﬁrst considering a pair of RVs. Most of the concepts deﬁned\nfor a pair of RVs carry forward to a vector of RVs.\nA pair of RVs is a mapping from the measure space, with event class E , of the form\nX : E →R2,\nwhere the mapping can be discrete or continuous. We will sometimes use the notation X = (X, Y ).\nFor example, we can spin the spinner twice and measure ψ1 ∈(0, 2π], ψ2 ∈(0, 2π]. In this case,\nwe can deﬁne the two RVs\nX(ψ1) = ψ1\n2π,\nY (ψ2) = ψ2\n2π.\nEvents for X are sets in R2. To compute probability of events, we need to deﬁne the joint\ncdf FXY : R2 →R, where\nFXY (x, y) = P[X ≤x, Y ≤y] = P[ξ ∈S : X(ξ) ≤x, Y (ξ) ≤y].\nAnalogous to single RVs\n• Joint cdfs are non-increasing functions of x, y. In other words, for x ≥x′ and y ≥y′\nFXY (x, y) ≥FXY (x′, y′).\n• limx→−∞F(x, y) = 0, limy→−∞F(x, y) = 0, limx,y→∞F(x, y) = 1.\n• We can calculate\nP[x1 < X ≤x2, y1 < Y ≤y2] = FXY (x2, y2) + FXY (x1, y1) −FXY (x1, y2) −FXY (x2, y1).\nFor X, Y jointly continuous, we can deﬁne the joint pdf as\nfXY (x, y) = ∂2FXY (x, y)\n∂x∂y\nwhich enjoys the following properties\n77\n• fXY (x, y) = 0 as x →±∞or y →±∞.\n• FXY (x, y) =\nR x\n−∞\nR y\n−∞fXY (r, s)drds.\n•\nR ∞\n−∞\nR ∞\n−∞fXY (r, s)drds = 1.\n• P[x1 < X ≤x2, y1 < Y ≤y2] =\nR x2\nx1\nR y2\ny1 fXY (x, y)dxdy.\n• P[X ∈B] =\nR R\nB fXY (x, y)dxdy.\nLet us look at some important joint random variables :\n1. Joint uniform RV: for some region (a, b] × (c, d], the joint pdf is given by\nfXY (x, y) =\n(\n1\n(b−a)(d−c)\nif (x, y) ∈(a, b] × (c, d]\n0\notherwise\n.\n(7.5)\n2. Joint Gaussian RV: the joint pdf is given by\nfXY (x, y) =\n1\np\n(2π)2det(Σ)\nexp\n\u0014\n−1\n2(x −µ)⊤Σ−1(x −µ)\n\u0015\nwhere x = (x, y), µ = (µx, µy) is the mean, and Σ is called the covariance matrix\nΣ =\n\u0014 σ2\nx\nρσxσy\nρσxσy\nσ2\ny\n\u0015\n.\nThe covariance matrix is symmetric and positive deﬁnite.\nWe can deﬁne the marginal PDF of the RV X, which is the pdf of X assuming Y attains\nall possible values\nfX(x) =\nZ ∞\n−∞\nfXY (x, y)dy.\nSimilarly, the marginal of Y is\nfY (y) =\nZ ∞\n−∞\nfXY (x, y)dx.\nThe RVs X and Y are said to be independent if fXY (x, y) = fX(x)fY (y).\nQuestion 7.2.1. Show that the joint uniform RVs with joint pdf (7.5) are independent.\nConsider the function g(X), which can be scalar-, vector-, or tensor-valued, then its expected\nvalue is given by\nE[g(X)] =\nZ ∞\n−∞\nZ ∞\n−∞\ng(x)fXY (x, y)dxdy\nas long as the integral is deﬁned. For instance:\n• For g(X) = X, we have E[g(X)] =\nR ∞\n−∞\nR ∞\n−∞xfXY (x, y)dxdy.\n• For g(X) = X, we have a vector valued expectation E[g(X)] = [E[X], E[Y ]].\n• For g(X) = X + Y , we have E[g(X)] = E[X] + E[Y ].\n78\nThe covariance of X is given by\nCOV[X] = E[(X −E[X]) ⊗(X −E[X])]\nwhere\nCOV[X]11 = E[(X −E[X])2] = VAR[X]\nCOV[X]22 = E[(Y −E[Y ])2] = VAR[Y ]\nCOV[X]12 = COV[X]21 = E[(X −E[X])(Y −E[Y ])].\nX and Y are said to be uncorrelated if COV[X]12 = 0. Furthermore, COV[X]12 = 0 for\nindependent RVs. Caution: COV[X]12 = 0 does not imply the RVs are independent!\nFinally, we are interested in looking at the pdf of Y when we know X = ˆx. A good guess\nwould be fXY (ˆx, y). However, this need not be a pdf as it need not integrate to unity over y.\nThis leads us to the conditional pdf of Y when we know X = ˆx,\nfY |X(y|ˆx) =\nfXY (ˆx, y)\nR ∞\n−∞fXY (ˆx, y)dy = fXY (ˆx, y)\nfX(ˆx)\n.\nSimilarly, we can write the conditional pdf of X given Y = ˆy as\nfX|Y (x|ˆy) = fXY (x, ˆy)\nfY (ˆy)\n.\nWe note that the extension of a regression problem to probabilistic framework leads us to\ndetermining the conditional distribution of the output given an instance of the input. This will\nbe discussed in Section 7.4.\n7.3\nUnsupervised probabilistic deep learning algorithms\nWe begin with a vector of RVs X with NX components with a pdf given by fX. Let’s assume\nthat we are given a dataset of samples {xi} sampled from the density fX, which we denote by\nxi ∼fX. For instance, these samples could correspond to RGB images of cars, with a resolution\nof 512 × 512. Note that this would mean that the samples would lie in a space with dimension\nNX = 512 × 512 × 3, which is quite large! We can treat each pixel of the images as a RV,\ntaking values given by the pixel intensities (across all 3 channels). Thus, these images can be\nseen as samples of a NX-dimensional RV with some unknown density fX. Also, because of the\ninherent structure of the objects (i.e. the cars) in these images, the various components of the\nmultidimensional RV can be expected to be highly correlated, leading to a non-trivial form of\nfX. This correlation also implies that it might be possible to reduce the dimension of X from\nNX to a smaller number and thus make the representation simpler.\nWe are interested in using the given ﬁnite set of samples {xi} to learning the density fX of\nthe data, and generating new samples from the learned distribution. Such methods are known as\ngenerative algorithms. Although a number of generative algorithms are available, we focus on a\nspeciﬁc type of deep learning algorithm known as Generative Adversarial Networks, or GANs for\nshort.\n7.3.1\nGANs\nGANs were ﬁrst proposed by Goodfellow et al. [6] in 2014. Since then, many variants of GANs\nhave been proposed which diﬀer based on the network architecture and the objective function\n79\nused to train the GAN. We begin by describing the abstract problem setup followed by the\narchitecture and training procedure of a GAN.\nConsider the dataset S = {xi ∈ΩX ⊂RNX : 1 ≤i ≤Ntrain}. We assume the samples are\nrealizations of some RV X with density fX, i.e., xi ∼fX. We want to train a GAN to learn fX\nand generate new samples from it.\nA GAN typically comprises two sub-networks, a generator and a discriminator (or critic).\nThe generator is a network of the form\ng(.; θg) : ΩZ →ΩX,\ng : z 7→x\n(7.6)\nwhere θg are the trainable parameters and z ∈ΩZ ⊂RNZ is the realization of a RV Z following\na simple distribution, such as an uncorrelated multivariate Gaussian with density\nfZ(z) =\n1\np\n(2π)2det(Σ)\nexp\n\u0014\n−1\n2(z −µ)⊤Σ−1(z −µ)\n\u0015\nwith µ = 0,\nΣ = I.\nTypically, NZ ≪NX with Z known as the latent variable of the GAN. The architecture of the\ngenerator will depend on the size/shape of x. If x is a vector, then g can be an MLP with\ninput dimension NZ and output dimension NX. If x is an image, say of shape H × W × 3, then\nthe generator architecture will have a few fully connected layers, followed by a reshape into a\ncoarse image with many channels, which is pushed through a number of transpose convolution\nchannels that gradually increase the spatial resolution and compress the number of channels to\nﬁnally scale up to the shape H × W × 3. This is also known as a decoder architecture, similar\nto the upward branch of a U-Net (see Figure 5.9.) In either case, for a ﬁxed θg, the generator\ng transforms the RV Z to another RV, Xg = g(Z; θg) with density fg\nX, which corresponds to\nthe latent density fZ pushed-forward by g. We want to choose θg such that fg\nX is close to the\nunknown target distribution fX.\nThe critic network is of the form\nd(.; θd) : ΩX →R\n(7.7)\nwith the trainable parameters θd. Once again, the critic architecture will depend on the shape of\nx. If x is a vector then d can be an MLP with input dimension NX and output dimension 1.\nIf x is an image, then the critic architecture will have a few convolution layers, followed be a\nﬂattening layer and a number of fully connected layers. This is similar to the CNN architecture\nshown in Figure 5.7 but with a scalar output and without an output function.\nFigure 7.3: Schematic of a GAN\nThe schematic of the GAN along with the inputs and outputs of the sub-networks is shown\nin Figure 7.3. The generator and critic play adversarial roles. The critic is trained to distinguish\n80\nbetween true samples coming from fX and fake samples generated by g with the density fg\nX. The\ngenerator on the other hand is trained to fool the critic by trying to generate realistic samples,\ni.e., samples similar to those sampled from fX.\nWe deﬁne the objective function describing a Wasserstein GAN (WGAN) [2], which has\nbetter robustness and convergence properties compared to the original GAN. The objective\nfunction is given by\nΠ(θg, θd) =\n1\nNtrain\nNtrain\nX\ni=1\nd(xi; θd)\n|\n{z\n}\ncritic value on real samples\n−\n1\nNtrain\nNtrain\nX\ni=1\nd(g(zi; θg); θd)\n|\n{z\n}\ncritic value on fake samples\n(7.8)\nwhere xi ∈S are samples from the true target distribution fX, while zi ∼fZ are passed through\ng to generate the fake samples. To distinguish between true and fake samples, the critic attains\nlarge positive values when evaluated on real samples and large negative values on fake generated\nsamples. Thus, critic is trained to maximize objective function. In other words, we want to solve\nthe problem\nθ∗\nd(θg) = arg max\nθd\nΠ(θg, θd) for any θg.\n(7.9)\nNote that the optimal parameters of the critic will depend on θg. Now to fool the critic, the\ngenerator g tries to minimize the objective function,\nθ∗\ng = arg min\nθg\nΠ(θg, θ∗\nd).\n(7.10)\nThus, training the WGAN corresponds to solving a minmax optimization problem. We note that\nthe critic and the generator are working in an adversarial manner. That is, while the former is\ntrying to maximize the objective function, the latter is trying to minimize it. Hence the name\ngenerative adversarial network.\nIn practice, we need to add a stabilizing term to the critic loss. So the critic is trained to\nmaximize\nΠc(θg, θd) = Π(θg, θd) −λ\n¯N\n¯\nN\nX\ni=1\n\u0012\r\r\r\r\n∂d\n∂ˆx(ˆxi; θd)\n\r\r\r\r −1\n\u00132\n(7.11)\nwhere ˆxi = αxi + (1 −α)g(zi; θg) and α is sampled from a uniform RV in (0, 1). The additional\nterm in (7.11) is known as a gradient penalty term and is used to constraining the (norm of)\ngradient of the critic d with respect to its input to be close to 1, and thus be 1-Lipschitz function.\nFor further details on this term, we direct the interested readers to [7].\nThe iterative Algorithm 1 is used train g and d simultaneously, which is also called alternating\nsteepest descent, where ηd and ηg are the learning rates for the critic and the generator, respectively.\nNote that we take K > 1 optimization steps for the critic followed by a single optimization step\nfor the generator. This is because we want to solve the inner maximization problem ﬁrst so that\nthe critic is able to distinguish between real and fake samples. Although taking a very large K\nwould lead to a more accurate solve of the minmax problem, it would also make the training\nalgorithm computationally intractable for moderately sized networks. Thus, K is typically chosen\nbetween 4 to 6 in practice.\nThe minmax problem is a hard optimization problem to solve, and convergence is usually\nreached after training for many epochs. Alternatively, the critic optimization steps can be done\nover mini-batches of the training data, with many mini-batches taken per epoch, leading to a\nsimilar number of optimization steps for a relatively small number of epochs. As the iterations\ngo on, d becomes better at detecting fake samples and g becomes better at creating samples that\ncan fool the critic.\n81\nAlgorithm 1: Algorithm to train a GAN\nInput: θ0\nd, θ0\ng, K, N_epochs, ηd, ηg\nfor n = 1, ..., N_epochs do\nˆθd ←θ(n−1)\nd\nfor k = 1, ..., K do\nMaximization update:\nˆθd ←ˆθd + ηd\n∂Πc\n∂θd\n(θ(n−1)\ng\n, ˆθd)\nend\nθ(n)\nd\n←ˆθd\nMinimization update:\nθn\ng ←θ(n−1)\ng\n−ηg\n∂Π\n∂θg\n(θ(n−1)\ng\n, θ(n)\nd )\nend\nUnder the assumption of inﬁnite capacity (Nθg, Nθd →∞), inﬁnite data (Ntrain →∞) and a\nperfect optimizer, we can prove that the generated distribution fg\nX converges weakly to the\ntarget distribution fX [2]. This is equivalent to saying\nEZ[ℓ(g(Z; θ∗\ng))] −→EX[ℓ(X)],\n(7.12)\nfor every continuous, bounded function ℓon ΩX, i.e., ℓ∈Cb(ΩX). Once the GAN is trained, we\ncan use the optimized g to generate new samples from fg\nX ≈fX by ﬁrst sampling z ∼fZ, and\nthen passing it through the generator to get the sample x = g(z; θ∗\ng). Furthermore, due to the\nweak convergence described above, the statistics (mean, variance, etc) of the generated samples\nwill convergence to the true statistics associated with fX.\nRemark 7.3.1. We make a few important remarks here:\n1. Once the GAN is trained, we typically only retain the generator and don’t need the critic.\nThe primary role of training the critic is to obtain a suitable g that can generate realistic\nsamples.\n2. The reason the term \"Wasserstein\" appears in the name WGAN is because one can show\nthat solving the minmax problem is equivalent to minimizing the Wasserstein-1 distance\nbetween fg\nX and fX [2, 28]. The Wasserstein-1 distance is a popular metric used to measure\ndiscrepancies between two probability measures.\n3. Since the dimension NZ of the latent variable is typically much smaller than the dimension\nNX of samples in ΩX, the trained generator also provides a low dimensional representation\nof high-dimensional data, which can be very useful in several downstream tasks [21, 22].\n7.4\nSupervised probabilistic deep learning algorithms\nRecall the deterministic problem where given the labelled/pairwise dataset\nS = {(xi, yi) : xi ∈ΩX ⊂RNX, y ∈ΩY ⊂RNY }Ntrain\ni=1\n(7.13)\n82\nwe want to ﬁnd y for a new x not appearing in S. We have seen in the previous chapters how\nneural networks can be used to solve such a regression (or classiﬁcation) problem.\nNow let us consider the probabilistic version of this problem. We assume that x and y are\nmodelled using RVs X and Y , respectively. Further, let the paired samples in (7.13) be drawn\nfrom the unknown joint distribution fXY . Then given a realization X = ˆx, we wish to use S to\ndetermine the conditional distribution fY |X(y|ˆx) and generate samples from it.\nThere are several popular approaches to solve this probabilistic problem, such as Bayesian\nneural networks, variational inference, dropouts or deep Boltzman machines. But we will focus\non an extension of GANs which also addresses these type of problems.\n7.4.1\nConditional GANs\nConditional GANs were ﬁrst proposed in [17] to learn conditional distributions. We will discuss\na special variant of these models known as conditional Wasserstein GANS (cWGANs) which\nwere developed in [1], and used to solve a number of physics-based (inverse) problems in [25].\nFigure 7.4: Schematic of a conditional GAN\nThe schematic of a conditional GAN is depicted in Figure 7.4. The generator is a network of\nthe form\ng(.; θg) : ΩZ × ΩX →ΩY ,\ng : (z, x) 7→y\n(7.14)\nwhere z ∼fZ is the latent variable. Note that unlike a GAN, the generator in a conditional\nGAN also takes as input x. For a given value of X = ˆx, sampling z ∼fZ will generate many\nsamples of y from some induced conditional distribution fg\nY |X(y|ˆx). The goal is to prescribe the\nparameters θg such that fg\nY |X(y|ˆx) approximates the true conditional fY |X(y|ˆx) for (almost)\nevery value of ˆx.\nThe critic is a network of the form\nd(.; θd) : ΩX × ΩY →R\n(7.15)\nwhich is trained to distinguish between paired samples (x, y) generated from the true joint\ndistribution fXY and the fake pairs (x, ˆy) where ˆy is generated by g given (real) x.\n83\nThe objective function for a cWGAN is given by\nΠ(θg, θd) =\n1\nNtrain\nNtrain\nX\ni=1\nd(xi, yi; θd)\n|\n{z\n}\ncritic value on real pairs\n−\n1\nNtrain\nNtrain\nX\ni=1\nd(xi, g(zi, xi; θg); θd)\n|\n{z\n}\ncritic value on fake pairs\n.\n(7.16)\nAs earlier, the critic is trained to maximize the objective function (given by (7.9)) while the\ngenerator is trained to minimize it (given by (7.10)). Further, a stabilizating gradient penalty\nterm needs to be included when optimizing the critic (see [25]). The generator and critic are\ntrained using the alternating steepest descent algorithm described for GANs.\nUnder the assumption of inﬁnite capacity (Nθg, Nθd →∞), inﬁnite data (Ntrain →∞) and\na perfect optimizer, we can prove [1] that the generated conditional distribution fg\nY |X(y|ˆx)\nconverges in a weak sense to the target condition distribution fY |X(y|ˆx) (on average) for a given\nX = ˆx.\n84\nBibliography\n[1] J. Adler and O. Öktem, Deep bayesian inversion. https://arxiv.org/abs/1811.05910,\n2018.\n[2] M. Arjovsky, S. Chintala, and L. Bottou, Wasserstein generative adversarial net-\nworks, in Proceedings of the 34th International Conference on Machine Learning, D. Precup\nand Y. W. Teh, eds., vol. 70 of Proceedings of Machine Learning Research, International\nConvention Centre, Sydney, Australia, 06–11 Aug 2017, PMLR, pp. 214–223.\n[3] R. Bischof and M. Kraus, Multi-objective loss balancing for physics-informed deep\nlearning. http://rgdoi.net/10.13140/RG.2.2.20057.24169, 2021.\n[4] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud, Neural ordinary\ndiﬀerential equations. https://arxiv.org/abs/1806.07366, 2018.\n[5] T. Chen and H. Chen, Universal approximation to nonlinear operators by neural net-\nworks with arbitrary activation functions and its application to dynamical systems, IEEE\nTransactions on Neural Networks, 6 (1995), pp. 911–917.\n[6] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,\nA. Courville, and Y. Bengio, Generative adversarial nets, in Advances in neural\ninformation processing systems, 2014, pp. 2672–2680.\n[7] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville,\nImproved training of wasserstein gans, in Advances in neural information processing systems,\n2017, pp. 5767–5777.\n[8] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition,\nin 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016,\npp. 770–778.\n[9] P. Kidger and T. Lyons, Universal Approximation with Deep Narrow Networks, in\nProceedings of Thirty Third Conference on Learning Theory, J. Abernethy and S. Agarwal,\neds., vol. 125 of Proceedings of Machine Learning Research, PMLR, 09–12 Jul 2020, pp. 2306–\n2327.\n[10] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization. https://arxiv.\norg/abs/1412.6980v9, 2017.\n[11] I. Lagaris, A. Likas, and D. Papageorgiou, Neural-network methods for boundary\nvalue problems with irregular boundaries, IEEE Transactions on Neural Networks, 11 (2000),\npp. 1041–1049.\n85\n[12] S. Lanthaler, S. Mishra, and G. E. Karniadakis, Error estimates for DeepONets:\na deep learning framework in inﬁnite dimensions, Transactions of Mathematics and Its\nApplications, 6 (2022).\n[13] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart,\nand A. Anandkumar, Fourier neural operator for parametric partial diﬀerential equations.\nhttps://arxiv.org/abs/2010.08895, 2020.\n[14] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis, Learning nonlinear\noperators via deeponet based on the universal approximation theorem of operators, Nature\nMachine Intelligence, 3 (2021), pp. 218–229.\n[15] A. L. Maas, A. Y. Hannun, A. Y. Ng, et al., Rectiﬁer nonlinearities improve neural\nnetwork acoustic models, in Proc. ICML, vol. 30, 2013.\n[16] L. McClenny and U. Braga-Neto, Self-adaptive physics-informed neural networks using\na soft attention mechanism. https://arxiv.org/abs/2009.04544, 2020.\n[17] M. Mirza and S. Osindero, Conditional generative adversarial nets. https://arxiv.\norg/abs/1411.1784, 2014.\n[18] S. Mishra and R. Molinaro, Estimates on the generalization error of physics-informed\nneural networks for approximating PDEs, IMA Journal of Numerical Analysis, (2022).\n[19] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro, Robust stochastic approximation\napproach to stochastic programming, SIAM Journal on Optimization, 19 (2009), pp. 1574–\n1609.\n[20] D. Patel, D. Ray, M. R. A. Abdelmalik, T. J. R. Hughes, and A. A. Oberai,\nVariationally mimetic operator networks. https://arxiv.org/abs/2209.12871, 2022.\n[21] D. V. Patel and A. A. Oberai, Gan-based priors for quantifying uncertainty in supervised\nlearning, SIAM/ASA Journal on Uncertainty Quantiﬁcation, 9 (2021), pp. 1314–1343.\n[22] D. V. Patel, D. Ray, and A. A. Oberai, Solution of physics-based bayesian inverse prob-\nlems with deep generative priors, Computer Methods in Applied Mechanics and Engineering,\n400 (2022), p. 115428.\n[23] A. Pinkus, Approximation theory of the mlp model in neural networks, Acta Numerica, 8\n(1999), pp. 143–195.\n[24] M. Raissi, P. Perdikaris, and G. Karniadakis, Physics-informed neural networks: A\ndeep learning framework for solving forward and inverse problems involving nonlinear partial\ndiﬀerential equations, Journal of Computational Physics, 378 (2019), pp. 686–707.\n[25] D. Ray, H. Ramaswamy, D. V. Patel, and A. A. Oberai, The eﬃcacy and gen-\neralizability of conditional gans for posterior inference in physics-based inverse problems.\nhttps://arxiv.org/abs/2202.07773, 2022.\n[26] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomed-\nical image segmentation, in Medical Image Computing and Computer-Assisted Intervention –\nMICCAI 2015, N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, eds., Cham, 2015,\nSpringer International Publishing, pp. 234–241.\n86\n[27] V. Sitzmann, J. N. P. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein,\nImplicit neural representations with periodic activation functions. https://arxiv.org/abs/\n2006.09661, 2020.\n[28] C. Villani, Optimal Transport: Old and New, Grundlehren der mathematischen Wis-\nsenschaften, Springer Berlin Heidelberg, 2008.\n[29] S. Wang, Y. Teng, and P. Perdikaris, Understanding and mitigating gradient ﬂow\npathologies in physics-informed neural networks, SIAM Journal on Scientiﬁc Computing, 43\n(2021), pp. A3055–A3081.\n[30] S. Wang, H. Wang, and P. Perdikaris, Learning the solution operator of parametric\npartial diﬀerential equations with physics-informed deeponets, Science Advances, 7 (2021).\n[31] L. Wu, C. Ma, and W. E, How sgd selects the global minima in over-parameterized learning:\nA dynamical stability perspective, in Advances in Neural Information Processing Systems,\nS. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, eds.,\nvol. 31, Curran Associates, Inc., 2018.\n[32] M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, Denseaspp for semantic segmentation\nin street scenes, in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2018, pp. 3684–3692.\n[33] D. Yarotsky and A. Zhevnerchuk, The phase diagram of approximation rates for deep\nneural networks. https://arxiv.org/abs/1906.09477, 2019.\n87\n",
  "categories": [
    "cs.LG",
    "math-ph",
    "math.MP",
    "68T07"
  ],
  "published": "2023-01-03",
  "updated": "2023-01-03"
}