{
  "id": "http://arxiv.org/abs/2301.01379v1",
  "title": "A Succinct Summary of Reinforcement Learning",
  "authors": [
    "Sanjeevan Ahilan"
  ],
  "abstract": "This document is a concise summary of many key results in single-agent\nreinforcement learning (RL). The intended audience are those who already have\nsome familiarity with RL and are looking to review, reference and/or remind\nthemselves of important ideas in the field.",
  "text": "arXiv:2301.01379v1  [cs.AI]  3 Jan 2023\nA Succinct Summary of Reinforcement Learning\nSanjeevan Ahilan∗\nAbstract\nThis document is a concise summary of many key results in single-\nagent reinforcement learning (RL). The intended audience are those who\nalready have some familiarity with RL and are looking to review, reference\nand/or remind themselves of important ideas in the ﬁeld.\nContents\n1\nAcknowledgements\n2\n2\nFundamentals\n2\n2.1\nThe RL paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2.2\nAgent and environment\n. . . . . . . . . . . . . . . . . . . . . . .\n2\n2.3\nObservability . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.4\nMarkov processes and Markov reward processes . . . . . . . . . .\n3\n2.5\nMarkov decision processes . . . . . . . . . . . . . . . . . . . . . .\n3\n2.6\nPolicies, values and models\n. . . . . . . . . . . . . . . . . . . . .\n3\n2.7\nDynamic programming . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nModel-free approaches\n6\n3.1\nPrediction\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.2\nControl with action-value functions . . . . . . . . . . . . . . . . .\n8\n3.3\nValue function approximation . . . . . . . . . . . . . . . . . . . .\n9\n3.4\nPolicy gradient methods . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.5\nBaselines\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.6\nCompatible function approximation . . . . . . . . . . . . . . . . .\n11\n3.7\nDeterministic policy gradients . . . . . . . . . . . . . . . . . . . .\n12\n4\nModel-based Approaches\n12\n4.1\nModel Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n4.2\nCombining model-free and model-based approaches . . . . . . . .\n13\n5\nLatent variables and partial observability\n14\n5.1\nLatent variable models . . . . . . . . . . . . . . . . . . . . . . . .\n14\n5.2\nPartially observable Markov decision processes\n. . . . . . . . . .\n14\n6\nDeep reinforcement learning\n15\n6.1\nExperience replay . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n6.2\nTarget networks . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n∗sanjeevanahilan@gmail.com. Much of this work was done at the Gatsby Unit, UCL.\n1\n1\nAcknowledgements\nI would like to thank Peter Dayan, David Silver, Chris Watkins and ChatGPT\nfor helpful feedback. Much of this work was drawn from David Silver’s UCL\ncourse1 and Sutton and Barto’s textbook (Sutton and Barto, 2018) and formed\nthe introductory chapter of my PhD thesis (Ahilan, 2021).\n2\nFundamentals\n2.1\nThe RL paradigm\nThe ﬁeld of reinforcement learning (RL) (Sutton and Barto, 2018) concerns it-\nself with the computational principles underlying goal-directed learning through\ninteraction. Although primarily seen as a ﬁeld of machine learning, it has a rich\nhistory spanning multiple ﬁelds. In psychology it can be used to model classi-\ncal (Pavlovian) and operant (instrumental) conditioning. In neuroscience it has\nbeen used to model the dopamine system of the brain (Schultz et al., 1997). In\neconomics, it relates to ﬁelds such as bounded rationality, and in engineering\nit has extensive overlap with the ﬁeld of optimal control (Bellman, 1957). In\nmathematics, investigation has continued under the guise of operations research.\nThe plethora of perspectives ensures that RL continues to be an exciting and\nextraordinarily interdisciplinary ﬁeld.\n2.2\nAgent and environment\nRL problems typically draw a separation between the agent and the environ-\nment. The agent receives observation ot and scalar reward rt from the environ-\nment and emits action at, where t indicates the time step. The environment\nreceives action at from the agent and then emits a reward rt+1 and an ob-\nservation ot+1. The cycle then begins again with the agent emitting its next\naction.\nHow the environment responds to the agent’s action is determined by the\nenvironment state st, which is updated at every time step. The conditional\ndistribution for the next environment state depends only on the present state\nand action and therefore satisﬁes the Markov property:\nP(st+1|st, at) = P(st+1|s1, . . . , st, a1, . . . , at)\n(1)\nThe environment state is in general private from the agent, which only re-\nceives observations and rewards. The conditional distribution for the next ob-\nservation given the current observation is not in general Markov, and so it may\nbe beneﬁcial for an agent to construct its own notion of state sα\nt , which it uses\nto determine its next action. This can be deﬁned as sα\nt = f(ht), where ht is the\nhistory of the agent’s sequence of observations, actions and rewards:\nht = a1, o1, r1, . . . , at, ot, rt\n(2)\n1https://www.davidsilver.uk/teaching/\n2\n2.3\nObservability\nA special case exists when the observation received by the agent ot is identical\nto the environment state st (such that there is no need to distinguish between\nthe two). This is the assumption underlying the formalism of Markov decision\nprocesses covered in the next section. An environment is partially observable\nif the agent cannot observe the full environment state, meaning that the con-\nditional distribution for its next observation given its current observation does\nnot satisfy the Markov property. This assumption underlies the formalism of a\npartially observable Markov decision process which we describe in Section 5.2.\n2.4\nMarkov processes and Markov reward processes\nA Markov process (or Markov chain) is a sequence of random states with the\nMarkov property. It is deﬁned in terms of the tuple ⟨S, P⟩where S is a ﬁnite\nset of states and P : S × S →[0, 1] is the state transition probability kernel.\nA Markov Reward Process (MRP) ⟨S, P, r, γ⟩extends the Markov process\nby including a reward function r : S × S →R for each state transition and a\ndiscount factor γ. The immediate expected reward in a given state is deﬁned\nas: r(s) = P\ns′ P(s, s′)r(s, s′).\nThe discount factor γ ∈[0, 1] is used to determine the present value of future\nrewards. Conventionally, a reward received k steps into the future is of worth\nγk times what it would be worth if received immediately. As we will shortly\nsee, the cumulative sum of discounted rewards is a quantity RL agents often\nseek to maximise, and so γ < 1 ensures that this sum is bounded (assuming r\nis bounded).\n2.5\nMarkov decision processes\nSingle-agent RL can be formalised in terms of Markov decision processes (MDPs).\nThe idea of an MDP is to capture the key components available to the learning\nagent; the agent’s sensation of the state of its environment, the actions it takes\nwhich can aﬀect the state, and the rewards associated with states and actions.\nAn MDP extends the formalism of an MRP to include a ﬁnite set of actions on\nwhich both P and r depend. Discrete-time, inﬁnite-horizon MDPs are described\nin terms of the 5-tuple ⟨S, A, P, r, γ⟩where S is the set of states, A is the\nset of actions, P : S × A × S →[0, 1] is the state transition probability kernel,\nr : S×A×S →R is the immediate reward function and γ ∈[0, 1) is the discount\nfactor. The expected immediate reward for a given state and action is deﬁned\nas r(s, a) = P\ns′ P(s, a, s′)r(s, a, s′), which we use for convenience subsequently.\n2.6\nPolicies, values and models\nCommon components of a reinforcement learning agent are a policy, value func-\ntion and a model. The policy π : S ×A →[0, 1] is the agent’s behaviour function\nwhich denotes the probability of taking action a in state s. Agents may also act\naccording to a deterministic policy µ : S →A. We will assume that policies are\nstochastic unless otherwise noted.\n3\nGiven an MDP and a policy π, the observed state sequence is a Markov\nprocess ⟨S, Pπ⟩.\nPπ(s, s′) =\nX\na∈A\nπ(s, a)P(s, a, s′)\n(3)\nSimilarly, the state and reward sequence is a MRP ⟨S, Pπ, rπ, γ⟩in which:\nrπ(s) =\nX\na∈A\nπ(s, a)r(s, a)\n(4)\nStarting from any particular state s at time step t = 0, the value function\nvπ(s) is a prediction of the expected discounted future reward given that the\nagent starts in state s and follows policy π:\nvπ(s) = Eπ\n\" ∞\nX\nt=0\nγtrt+1|s0 = s\n#\n(5)\nwhere rt+1 = r(st, at, st+1)\nwhich is the solution of an associated Bellman expectation equation:\nvπ(s) =\nX\na∈A\nπ(s, a)\n\"\nr(s, a) + γ\nX\ns′∈S\nP(s, a, s′)vπ(s′)\n#\n(6)\nIn matrix form the Bellman expectation equation can be expressed in terms\nof the induced MRP:\nvπ = rπ + γPπvπ = (I −γPπ)−1rπ\n(7)\nwhere vπ ∈R|S| and rπ ∈R|S| are the vector of values and expected imme-\ndiate rewards respectively for each state under policy π. We can also deﬁne a\nBellman expectation backup operator:\nT π(v) = rπ + γPπv\n(8)\nwhich has a ﬁxed point of vπ.\nAn action-value for a policy π can also be deﬁned, which is the expected dis-\ncounted future reward for executing action a and subsequently following policy\nπ.\nqπ(s, a) = r(s, a) + γ\nX\ns′∈S\nP(s, a, s′)vπ(s′)\n= r(s, a) + γ\nX\ns′∈S\nP(s, a, s′)\nX\na′∈A\nπ(s′, a′)qπ(s′, a′)\n(9)\nThe process of estimating vπ or qπ is known as policy evaluation. Policies\ncan be evaluated without directly knowing or estimating a model, using instead\nthe directly sampled experience of the environment, an approach which is known\nas ‘model-free’. However a ‘model-based’ approach is also possible in which a\nmodel is used to predict what the environment will do next. A key component\nof a model is an estimate of P(s, a, s′), the probability of the next state given\nthe current state and action. Another is an estimate of r(s, a), the expected\nimmediate reward.\n4\nPolicy evaluation enables a value function to be learned for a given policy.\nHowever, we often wish to learn the best possible policy. The value function for\nthis is known as the optimal value function and corresponds to the maximum\nvalue function over all policies:\nv∗(s) = max\nπ\nvπ(s)\n(10)\nThe deﬁnition of the optimal action-value function (which evaluates the im-\nmediate action a in state s) is similarly:\nq∗(s, a) = max\nπ\nqπ(s, a)\n(11)\nA partial ordering over policies can be deﬁned according to:\nπ ≥π′ if vπ(s) ≥vπ′(s), ∀s\n(12)\nFor any MDP there exists an optimal policy π∗that is better than or equal\nto all other policies. All optimal policies achieve the optimal value function and\noptimal action-value function and there is always a deterministic optimal policy\nfor any MDP. The latter is achieved by selecting:\na = arg max\na∈A\nq∗(s, a)\n(13)\nIf there are many possible actions which satisfy this, any of these may be\nchosen to constitute an optimal policy (of which there may be many).\nThe\noptimal value and state-value functions satisfy Bellman optimality equations:\nv∗(s) = max\na∈A q∗(s, a)\nv∗(s) = max\na∈A\nh\nr(s, a) + γ\nX\ns′∈S\nP(s, a, s′)v∗(s′)\ni\nq∗(s, a) = r(s, a) + γ\nX\ns′∈S\nP(s, a, s′)max\na′\nq∗(s′, a′)\n(14)\nThe Bellman optimality equation is non-linear with no closed form solution\n(in general). Solving it therefore requires iterative solution methods.\n2.7\nDynamic programming\nDynamic programming (DP) (Bertsekas et al., 1995) refers to a collection of\nalgorithms that can be used to compute optimal policies given a perfect model\nof the environment as an MDP. In general, DP solves complex problems by\nbreaking them down into subproblems and then combining the solutions. It is\nparticularly useful for overlapping subproblems, the solutions to which reoccur\nmany times when solving the overall problem, making it more computationally\neﬃcient to cache and reuse them.\nWhen applied to MDPs, the recursive decomposition of DP corresponds to\nthe Bellman equation and the cached solution to the value function. DP assumes\nthat the MDP is fully known and therefore does not address the full RL problem\nbut instead addresses the problem of planning. By planning, the prediction\nproblem can be addressed by ﬁnding the value function vπ of a given policy\n5\nπ. This can be evaluated by iterative application of the Bellman Expectation\nBackup (Equation 8).\nThis leads to convergence to a unique ﬁxed point vπ, which can be shown\nusing the contraction mapping theorem (also known as the Banach ﬁxed-point\ntheorem) (Banach, 1922). When a Bellman expectation backup operator T π\nis applied to two value functions u and v over states, we ﬁnd that it is a γ-\ncontraction:\n||T π(u) −T π(v)||∞= ||(rπ + γPπu) −(rπ + γPπv)||∞\n= ||γPπ(u −v)||∞\n≤||γPπ1||u −v||∞||∞\n≤γ||u −v||∞\n(15)\nwhere 1 is a vector of ones and the inﬁnity norm of a vector a is denoted\n||a||∞and is deﬁned as the maximum value of its components. This contraction\nensures that both u and v converge to the unique ﬁxed point of T π which is vπ.\nFor control, DP can be used to ﬁnd the optimal value function v∗and in turn\nthe optimal policy π∗. One possibility is policy iteration in which the current\npolicy π is ﬁrst evaluated as described and then subsequently improved to π′\nsuch that:\nπ′(s) = arg max\na∈A\nqπ(s, a)\n(16)\nThis improves the value from any state s over one step:\nqπ(s, π′(s)) = max\na∈A qπ(s, a) ≥\nX\na∈A\nπ(s, a)qπ(s, a) = vπ(s)\n(17)\nIt can be shown that this improves the value function such that that vπ′(s) ≥\nvπ(s) (Silver, 2015). This process is then repeated, with improvements ending\nwhen the Bellman optimality equation (14) has been satisﬁed and convergence\nto π∗achieved. A generalisation of policy iteration is also possible in which,\ninstead of waiting for policy evaluation to converge, only n steps of evaluation\nare taken before policy improvement occurs and the process is repeated. If n = 1\nthis is known as value iteration, as the policy is no longer explicit (being a direct\nconsequence of the value function). Like policy iteration, value iteration is also\nguaranteed to converge to the optimal value function and policy. This can be\ndemonstrated using the contraction mapping theorem.\n3\nModel-free approaches\n3.1\nPrediction\nAs has been outlined, dynamic programming can be used to solve known MDPs\nenabling optimal value functions and policies to be found. However, in many\ncases the MDP is not directly known - instead an agent taking actions in the\nMDP must learn directly from its experiences, as it transitions from state to\nstate and receives rewards accordingly. One approach, known as ‘model-free’,\nseeks to solve MDPs without learning transitions or rewards. For prediction, a\n6\nkey quantity to estimate in this setting is the expected discounted future reward.\nA sampled estimate of this, starting from state st, is known as the return:\nRt = rt+1 + γrt+2 + γ2rt+3 + ... =\n∞\nX\nk=0\nγkrt+k+1\n(18)\nwhich depends on the actions sampled from the policy, and states from\ntransitions.\nMonte-Carlo (MC) methods seek to estimate this directly using complete\nepisodes of experience. Introducing a learning rate αt, the agent’s value function\ncan therefore be updated according to2:\nv(st) ←v(st) + αt\n\u0002\nRt −v(st)\n\u0003\n(19)\nThe value function updated in this way will converge to a solution with min-\nimum mean-square error (best ﬁt to the observed returns), assuming a suitable\nsequential decrease in the learning rate.\nTemporal-diﬀerence (TD) learning methods learn from incomplete episodes\nby bootstrapping. For example, if learning occurs after a single step, this is\nknown as TD(0), which has the following update:\nv(st) ←v(st) + αt\n\u0002\nrt+1 + γv(st+1) −v(st)\n\u0003\n(20)\nwhere rt+1 + γv(st+1) is known as the target. This approximates the full-width\nBellman expectation backup (Equation 8) in which every successor state and\naction is considered, with experiences instead being sampled. TD(0) will con-\nverge to the solution of the maximum likelihood Markov model which best ﬁts\nthe data (again assuming a suitable sequential decrease in the learning rate).\nThis solution may be diﬀerent from the minimum mean-square error solution of\nMC methods, which do not assume the Markov property.\nUnlike MC methods, TD methods introduce bias into the estimated return\nas the currently estimated value function may be diﬀerent from the true value\nfunction. However, they generally have reduced variance relative to MC meth-\nods, as in MC the estimated return depends on a potentially long sequence of\nrandom actions, transitions and rewards.\nThe distinction between MC and TD methods can be blurred by considering\nmulti-step TD methods (rather than only TD(0)), in which rewards are sampled\nfor a number of steps before the value function is used to compute an estimate\nof future rewards. The n-step return is deﬁned as:\nR(n)\nt\n= rt+1 + γrt+2 + ... + γn−1rt+n + γnv(st+n)\n(21)\nAs n →∞it tends towards the unbiased MC return. An algorithm may\nseek to ﬁnd a good bias-variance tradeoﬀby estimating a weighted combination\nof n-step returns; one popular method to do this is known as TD(λ):\nRλ\nt = (1 −λ)\n∞\nX\nn=1\nλn−1R(n)\nt\n(22)\nwhere λ ∈[0, 1].\n2assuming a table-based representation rather than use of a function approximator\n7\n3.2\nControl with action-value functions\nModel free control concerns itself with optimising rather than evaluating the\nRL objective. Policies may be evaluated according to various objectives. In\nthe case of continuing environments, the objective can be the average value or\nthe average reward per time-step. We focus instead on episodic environments,\nassuming an initial distribution over starting states p0(s) : S →[0, 1]. The\nobjective is thus:\nJ(π) = Eπ\n\" ∞\nX\nt=0\nγtrt+1|p0(s)\n#\n(23)\nNote that if the domain of the starting state distribution is only over a single\nstarting state, the objective is simply the value function (Equation 5) in that\nstarting state. This objective can equivalently be expressed as:\nJ(π) = Es∼ρπ,a∼π[r(s, a)]\n(24)\nwhere:\nρπ(s) :=\nX\ns′\n∞\nX\nt=0\nγtp(st = s|s′, π)p0(s′)\n(25)\nis the improper discounted state distribution induced by policy π starting from\nan initial state distribution p0(s′). In Section 3.4 we describe policy gradient\nmethods which seek to optimise this objective directly.\nHowever, we ﬁrst consider model-free approaches which rely on an action-\nvalue function q(s, a) to achieve control (a value function v(s) alone is insuﬃcient\nfor model-free control).\nThe optimal action-value function q∗(s, a) must be\nlearned, with MC and TD methods both viable.\nOnce it has been learned,\nan optimal policy may be achieved by selecting the best action in each state\n(Equation 13).\nHowever, unlike dynamic programming, full-width backups are not used and\nso if actions are selected greedily (meaning those with highest action-values are\nalways chosen) then certain states and actions may never be correctly evalu-\nated. Model-free RL methods must therefore allow for enough exploration dur-\ning learning before ultimately exploiting this learning to achieve near-optimal\ncumulative reward.\nOne simple approach, known as ǫ-greedy is to take a random action with\nprobability ǫ but otherwise act greedily according to the current estimate of\nthe action-value function. The value of ǫ can be decreased with the number of\nepisodes. This can satisfy a condition known as greedy in the limit of inﬁnite\nexploration in which all state-action pairs are explored inﬁnitely many times\nand the policy converges to the greedy policy.\nOne popular algorithm for model-free control is known as Q-learning (Watkins and Dayan,\n1992), which seeks to learn the optimal action-value function whilst using a pol-\nicy which also takes exploratory actions (such as epsilon greedy). This learning\nis termed oﬀ-policy as the policy used to sample experience is diﬀerent from the\npolicy being learned (the optimal policy). The resulting update is:\nq(st, at) ←q(st, at) + α\n\u0002\nrt+1 + γmax\na′∈A q(st+1, a′) −q(st, at)\n\u0003\n(26)\n8\nAn alternative to oﬀ-policy Q-learning is on-policy SARSA (Rummery and Niranjan,\n1994). This uses the sampled sampled state st, action at, reward rt+1, next state\nst+1, and next action at+1 for updates3:\nq(st, at) ←q(st, at) + α(rt+1 + γq(st+1, at+1) −q(st, at))\n(27)\n3.3\nValue function approximation\nSo far we have assumed a tabular representation of states and actions such\nthat each state is separately updated. However, in practice we would like value\nfunctions and policies to generalise to new states and actions, and so it is ben-\neﬁcial to use function approximators such as deep neural networks. A common\napproach is to approximate the value function or action-value function:\nvw(s) = ˆv(s; w) ≈vπ(s)\nqw(s, a) = ˆq(s, a; w) ≈qπ(s, a)\n(28)\nwhere w are the parameters we wish to learn.\nIf we start by assuming we\nknow the true value function vπ, we can deﬁne a mean square error between the\napproximate value function and the true function:\nL(w) = Eπ[(vπ(s) −vw(s))2]\n(29)\nGiven a distribution of states s ∼p(s)4, we can minimise this iteratively\nusing stochastic gradient descent:\nw ←w + α(vπ(st) −vw(st))∇wvw(st)\n(30)\nIn reality we can only use a better estimate of vπ provided by the sampled\nreward(s). For example, if we use the TD(0) target the update is:\nw ←w + α(rt+1 + γvw(st+1) −vw(st))∇wvw(st)\n(31)\nUpdates like this are known as ‘semi-gradient’ as the gradient of the value\nfunction used to deﬁne the target is ignored.\nIf we use a linear function approximator vw(s) = x(s)T w (where features\nx(s) and w are vectors), then we ﬁnd:\nw ←w + α(rt+1 + γvw(st+1) −vw(st))x(st)\n(32)\nindicating that the linear weights are updated in proportion to the activity\nof their corresponding features. Non-linear function approximators can also be\nused, but typically have weaker convergence guarantees than linear function\napproximators. Nevertheless, due to their ﬂexibility such approximators have\nenabled impressive performance in a number of challenging domains, such as\nAtari games (Mnih et al., 2015) and Go (Silver et al., 2016).\n3and also gives SARSA its name\n4we later discuss a method for sampling states\n9\n3.4\nPolicy gradient methods\nParameterised stochastic policies πθ may be improved using the policy gradient\ntheorem (Sutton et al., 2000). This can be derived for any of the common RL\nobjectives. To demonstrate a derivation of this result we use a starting state\nobjective J(θ) = vπθ(s0) with a single starting state s0:\n∇θJ(θ) = ∇θvπ(s0)\n= ∇θ\nX\na\nπ(s0, a)qπ(s0, a)\n=\nX\na\n∇θπ(s0, a)qπ(s0, a) + π(s0, a)∇θqπ(s0, a)\n=\nX\na\n∇θπ(s0, a)qπ(s0, a) + π(s0, a)∇θ\nh\nr(s0, a) +\nX\ns′\nγP(s0, a, s′)vπ(s′)\ni\n=\nX\na\n∇θπ(s0, a)qπ(s0, a) + π(s0, a)\nX\ns′\nγP(s0, a, s′)∇θvπ(s′)\n(33)\nWe note that we could continue to unroll ∇θvπ(s′) on the R.H.S in the same\nway as we have already done. Considering now transitions from starting state\ns0 to arbitrary state s we therefore ﬁnd:\n∇θvπ(s0) =\nX\ns\n∞\nX\nt=0\nγtp(st = s|s0, π)\nX\na\n∇θπ(s, a)qπ(s, a)\n(34)\nwhere P∞\nt=0 γtp(st = s|s0, π) is the discounted state distribution ρπ(s) from\na ﬁxed starting state s0 (Equation 25). This derivation holds even when there\nis a distribution over starting states, and gives us the policy gradient theorem:\n∇θJ(θ) =\nX\ns\nρπ(s)\nX\na\n∇θπ(s, a)qπ(s, a)\n(35)\nUsing the likelihood ratio trick:\n∇θπ(s, a) = π(s, a)∇θπ(s, a)\nπ(s, a)\n= π(s, a)∇θ log π(s, a)\n(36)\nthis can be equivalently expressed as:\n∇θJ(θ) =\nX\ns\nρπ(s)\nX\na\nπ(s, a)qπ(s, a)∇θ log π(s, a)\n= Eπ[qπ(s, a)∇θ log π(s, a)]\n(37)\nThe policy gradient theorem result enables model-free learning as gradi-\nents need only be determined for the policy rather than for properties of the\nenvironment. There are a variety of approaches for determining qπ. If qπ is ap-\nproximated using the sample return (Equation 18), this leads to the algorithm\nknown as REINFORCE (Williams, 1992):\nθ ←θ + αRt∇θ log π(st, at)\n(38)\n10\nAs there is no bootstrapping here, this is also known as MC policy gradient.\nAn alternative approach is to separately approximate qπ with a ‘critic’ qw giving\nrise to what are commonly known as ‘actor-critic’ methods. These introduce two\nsets of parameter updates; the critic parameters w are updated to approximate\nqπ, and the policy (actor) parameters θ are updated according to the policy\ngradient as indicated by the critic. The critic itself can be updated according\nto the TD error. An example of this approach is SARSA actor-critic:\nw ←w + α1(rt+1 + γqw(st+1, at+1) −qw(st, at))∇wqw(st, at)\nθ ←θ + α2qw(st, at)∇θ log π(st, at)\n(39)\nwhere diﬀerent learning rates α1 and α2 may be used for the actor and the\ncritic.\n3.5\nBaselines\nWhether we use REINFORCE or an actor-critic based approach to policy gradi-\nents, it is possible to reduce the variance further by the introduction of baselines.\nIf this baseline depends only on the state s, then we ﬁnd it introduces no bias:\nX\ns\nρπ(s)\nX\na\n∇θπ(s, a)b(s) =\nX\ns\nρπ(s)b(s)∇θ\nX\na\nπ(s, a)\n=\nX\ns\nρπ(s)b(s)∇θ1\n= 0\n(40)\nA natural choice for the state-dependent baseline is the value function:\n∇θJ(θ) = Eπ[(qπ(s, a) −vπ(s))∇θ log π(s, a)]\n= Eπ[Aπ(s, a)∇θ log π(s, a)]\n(41)\nwhere Aπ is known as the advantage, which may in some algorithms be approx-\nimated directly (rather than approximating both qπ and vπ).\n3.6\nCompatible function approximation\nIn the general case, our choice to approximate qπ with qw introduces bias such\nthat there are no guarantees of convergence to a local optimum. However, in the\nspecial case of a compatible function approximator we can introduce no bias and\ntake steps in the direction of the true policy gradient. This becomes possible\nwhen the critic’s function approximator reaches a minimum in the mean-squared\nerror:\n0 = Eπ[∇w(qπ(s, a) −qw(s, a))2]\n= Eπ[(qπ(s, a) −qw(s, a))∇wqw(s, a)]\n(42)\nIf we choose qw(s, a) such that ∇wqw(s, a) = ∇θ log π(s, a) we ﬁnd:\nEπ[qπ(s, a)∇θ log π(s, a)] = Eπ[qw(s, a)∇θ log π(s, a)]\n(43)\n11\nwhere the L.H.S is equal to the true policy gradient and so our function ap-\nproximation has introduced no bias. For example, if the policy is a Boltzmann\npolicy with a linear combination of features, of the form:\nπ(s, a) =\neθT φ(s,a)\nP\na′ eθT φ(s,a′)\n(44)\nthen a compatible value function must be linear in the same features as the\npolicy except normalised to zero mean for each state using a subtractive baseline\n(Sutton et al., 2000).\nqw(s, a) = wT [φ(s, a) −\nX\na′\nφ(s, a′)π(s, a′)]\n(45)\n3.7\nDeterministic policy gradients\nRather than have a policy specify a probability for certain actions in certain\nstates we can instead have it simply be a function mapping states to actions\nµθ : S →A and, in the case of continuous actions, seek to ﬁnd the gradient\nof the objective with respect to the policy parameters. An example of an al-\ngorithm which uses such an approach is Deterministic Policy Gradients (DPG)\n(Silver et al., 2014).\nThe DPG algorithm builds on the deterministic policy\ngradient theorem:\n∇θJ(θ) = Es∼ρµ[∇θµθ(s)∇aqµ(s, a)|a=µθ(s)].\n(46)\nwhere the parameters of the policy are adjusted in an oﬀ-policy fashion using\nan exploratory behavioural policy (which is a noisy version of the deterministic\npolicy). In practice qµ is approximated by the critic qw, which is diﬀerentiable\nin the action and updated using Q-learning:\nδt = rt+1 + γqw(st+1, µθ(st+1)) −qw(st, at)\nw ←w + α1δt∇wqw(st, at)\nThe parameters of the policy are then updated according to:\nθ ←θ + α2∇θµθ(st)∇aqw(st, at)|a=µθ(st)\n(47)\n4\nModel-based Approaches\nIn model-free RL agents learn to take actions directly from experiences, without\never modelling transitions in the environment or reward functions, whereas in\nmodel-based RL the agent attempts to learn these. The key beneﬁt is that if\nthe agent can perfectly predict the environment ‘in its head’, then it no longer\nneeds to interact directly with the environment in order to learn an optimal\npolicy.\n4.1\nModel Learning\nRecall that MDPs are deﬁned in terms of the 5-tuple ⟨S, A, P, r, γ⟩. Although\nmodels can be predictions about anything, a natural starting point is to ap-\nproximate the state transition function Pη ≈P and immediate reward function\n12\nrη ≈r. We can then use dynamic programming to learn the optimal policy for\nan approximate MDP ⟨S, A, Pη, rη, γ⟩, the performance of which may be worse\nthan for the true MDP.\nGiven a ﬁxed set of experiences, a model can be learned using supervised\nmethods. For predicting immediate expected scalar rewards, this is a regression\nproblem whereas for predicting the distribution over next states this a density\nestimation problem. Given the simplicity of this framing, a range of function\napproximators may be employed, including neural networks and Gaussian pro-\ncesses.\n4.2\nCombining model-free and model-based approaches\nOnce a model is learned it can be used for planning. However, in many situ-\nations it is computationally infeasible to do the full-width backups of dynamic\nprogramming as the state space is too large. Instead, experiences can be sam-\npled from the model and used as data by a model-free algorithm.\nA well known architecture which combines model-based and model-free RL is\nthe Dyna architecture (Sutton, 1991). Dyna treats samples of simulated and real\nexperience similarly, using both to learn a value function. Simulated experience\nis generated by the model which is itself learned from real experience. In Dyna,\nmodel-free based updates depend on the state the agent is currently in, whereas\nfor the model-based component starting states can be sampled randomly and\nthen rolled forwards using the model to update the value function using e.g.\nTD learning.\nOne potential disadvantage of Dyna is that it does not preferentially treat\nthe state the agent is currently in.\nIn many cases, such as deciding on the\nnext move in chess, it is useful to start all rollouts from the current state (the\nboard position) when choosing the next move. This is known as forward search,\nwhere a search tree is built with the current state as the root. Forward based\nsearch often uses sample based rollouts rather than full-width ones so as to be\ncomputationally tractable and this is known as simulation-based search.\nAn eﬀective algorithm for simulation-based search is Monte-Carlo Tree search\n(Coulom, 2007). It uses the MC return to estimate the action-value function for\nall nodes in the search tree using the current policy. It then improves the pol-\nicy, for example by being ǫ-greedy with respect to the new action-value function\n(or more commonly handling exploration-exploitation using Upper Conﬁdence\nTrees, see Kocsis and Szepesv´ari (2006) for a more detailed discussion). MC\nTree Search is equivalent to MC control applied to simulated experience and\ntherefore is guaranteed to converge on the optimal search tree. Instead of using\nMC control for search it is also possible to use TD-based control, which will\nincrease bias but reduce variance.\nModel-based RL is a highly active area of research. Recent advances include\nMuZero (Schrittwieser et al., 2020), which extends model-based predictions to\nvalue functions and policies, and Dreamer which plans using latent variable\nmodels (Hafner et al., 2019).\n13\n5\nLatent variables and partial observability\n5.1\nLatent variable models\nHidden or ‘latent’ variables correspond to variables which are not directly ob-\nserved but nevertheless inﬂuence observed variables and thus may be inferred\nfrom observation. In reinforcement learning, it can be beneﬁcial for agents to\ninfer latent variables as these often provide a simpler and more parsimonious\ndescription of the world, enabling better predictions of future states and thus\nmore eﬀective control.\nLatent variable models are common in the ﬁeld of unsupervised learning.\nGiven data p(x) we may describe a probability distribution over x according to:\np(x; θx|z, θz) =\nZ\ndz p(x|z; θx|z)p(z; θz)\n(48)\nwhere θx|z parameterises the conditional distribution x|z and θz parame-\nterises the distribution over z.\nKey aims in unsupervised learning include capturing high-dimensional cor-\nrelations with fewer parameters (as in probabilistic principal components analy-\nsis), generating samples from a data distribution, describing an underlying gen-\nerative process z which describes causes of x, and ﬂexibly modelling complex\ndistributions even when the underlying components are simple (e.g. belonging\nto an exponential family).\n5.2\nPartially observable Markov decision processes\nA partially observable Markov decision process (POMDP) (Kaelbling et al.,\n1998) is a generalisation of an MDP in which the agent cannot directly ob-\nserve the true state of the system, the dynamics of which is determined by an\nMDP. Formally, a POMDP is a 7-tuple ⟨S, A, P, r, O, Ω, γ⟩where S is the set\nof states, A is the set of actions, P : S × A × S →[0, 1] is the state transition\nprobability kernel, r : S × A × S →R is the reward function, O is the set of\nobservations, Ω: S × A × O →[0, 1] is the observation probability kernel and\nγ ∈[0, 1) is the discount factor. As with MDPs, agents in POMDPs seek to\nlearn a policy π(sα\nt ) which maximises some notion of cumulative reward, com-\nmonly Eπ[P∞\nt=0 γtrt+1]. This policy depends on the agent’s representation of\nstate sα\nt = f(ht), which is a function of its history.\nOne approach to solving POMDPs is by maintaining a belief state over the\nlatent environment state - transitions for which satisfy the Markov property.\nMaintaining a belief over states only requires knowledge of the previous belief\nstate, the action taken and the current observation. Beliefs may then be updated\naccording to:\nb′(s′) = ηΩ(o′|s′, a)\nX\ns∈S\nP(s′|s, a)b(s)\n(49)\nwhere η = 1/ P\ns′ Ω(o′|s′, a) P\ns∈S P(s′|s, a)b(s) is a normalising constant.\nA Markovian belief state allows a POMDP to be formulated as an MDP\nwhere every belief is a state. However, in practice, maintaining belief states in\nPOMDPs will be computationally intractable for any reasonably sized problem.\nIn order to address this, approximate solutions may be used.\nAlternatively,\n14\nagents learning using function approximators which condition on the past can\nconstruct their own state representations, which may in turn enable relevant\naspects of the state to be approximately Markov.\n6\nDeep reinforcement learning\nThe policies and value functions used in reinforcement learning can be learned\nusing artiﬁcial neural network function approximators. When such networks\nhave many layers they are conventionally denoted as ‘deep’, and are typically\ntrained on large amounts of data using stochastic gradient descent (LeCun et al.,\n2015). The application of deep networks in model-free reinforcement learning\ngarnered extensive attention when they were successfully used to learn a variety\nof Atari games from scratch (Mnih et al., 2013). For the particular problem\nof learning from pixels a convolutional neural network architecture was used\n(LeCun et al., 1998), which are highly eﬀective at extracting useful features\nfrom images. They have been extensively used on supervised image classiﬁcation\ntasks due to their ability to scale to large and complex datasets (LeCun et al.,\n2015).\nA deep analysis of deep reinforcement learning (DRL) is beyond the scope\nof this summary. However we review two key techniques used to overcome the\ntechnical challenge of stabilising training.\n6.1\nExperience replay\nAs an agent interacts with its environment it receives experiences that can be\nused for learning. However, rather than using those experiences immediately, it\nis possible to store such experience in a ‘replay buﬀer’ and sample them at a later\npoint in time for learning. The beneﬁts of such an approach were introduced by\nMnih et al. (2013) for their ‘deep Q-learning’ algorithm. At each timestep, this\nmethod stores experiences et = (st, at, rt+1, st+1) in a replay buﬀer over many\nepisodes. After suﬃcient experience has been collected, Q-learning updates are\nthen applied to randomly sampled experiences from the buﬀer. This breaks the\ncorrelation between samples, reducing the variance of updates and the potential\nto overﬁt to recent experience. Further improvements to the method can be\nmade by prioritised (as opposed to random) sampling of experiences according to\ntheir importance, determined using the temporal-diﬀerence error (Schaul et al.,\n2015).\n6.2\nTarget networks\nWhen using temporal diﬀerence learning with deep function approximators a\ncommon challenge is stability of learning. A source of instability arises when\nthe same function approximator is used to evaluate both the value of the current\nstate and the value of the target state for the temporal diﬀerence update. After\nsuch updates, the approximated value of both current and target state change\n(unlike tabular methods), which can lead to a runaway target. To address this,\ndeep RL algorithms often make use of a separate target network that remains\nstable even whilst the standard network is updated. As it is not desirable for\nthe target network to diverge too far from the standard network’s improved\n15\npredictions, at ﬁxed intervals the parameters of the standard network can be\ncopied to the target network. Alternatively, this transition is made more slowly\nusing Polyak averaging:\nφtarg ←ρφtarg + (1 −ρ)φ\n(50)\nwhere φ are the parameters of the standard network and ρ is a hyperparameter\ntypically close to 1.\n16\nReferences\nSanjeevan Ahilan. Structures for Sophisticated Behaviour: Feudal Hierarchies\nand World Models. PhD thesis, UCL (University College London), 2021.\nStefan Banach. Sur les op´erations dans les ensembles abstraits et leur application\naux ´equations int´egrales. Fund. math, 3(1):133–181, 1922.\nRichard Bellman. A markovian decision process. Journal of mathematics and\nmechanics, pages 679–684, 1957.\nDimitri P Bertsekas, Dimitri P Bertsekas, Dimitri P Bertsekas, and Dimitri P\nBertsekas. Dynamic programming and optimal control, volume 1. Athena\nscientiﬁc Belmont, MA, 1995.\nR´emi Coulom. Eﬃcient selectivity and backup operators in monte-carlo tree\nsearch. In International conference on computers and games, pages 72–83.\nSpringer, 2007.\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi.\nDream to control: Learning behaviors by latent imagination. arXiv preprint\narXiv:1912.01603, 2019.\nLeslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning\nand acting in partially observable stochastic domains. Artiﬁcial intelligence,\n101(1-2):99–134, 1998.\nLevente Kocsis and Csaba Szepesv´ari. Bandit based monte-carlo planning. In\nEuropean conference on machine learning, pages 282–293. Springer, 2006.\nYann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-based\nlearning applied to document recognition. Proceedings of the IEEE, 86(11):\n2278–2324, 1998.\nYann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. nature, 521\n(7553):436–444, 2015.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis\nAntonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep\nreinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Ve-\nness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidje-\nland, Georg Ostrovski, et al. Human-level control through deep reinforcement\nlearning. Nature, 518(7540):529, 2015.\nGavin A Rummery and Mahesan Niranjan. On-line Q-learning using connec-\ntionist systems, volume 37. University of Cambridge, Department of Engi-\nneering Cambridge, UK, 1994.\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver.\nPrioritized\nexperience replay. arXiv preprint arXiv:1511.05952, 2015.\n17\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,\nLaurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hass-\nabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning\nwith a learned model. Nature, 588(7839):604–609, 2020.\nWolfram Schultz, Peter Dayan, and P Read Montague. A neural substrate of\nprediction and reward. Science, 275(5306):1593–1599, 1997.\nDavid Silver. Lecture 3: Planning by dynamic programming. Google DeepMind,\n2015.\nDavid Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and\nMartin Riedmiller. Deterministic policy gradient algorithms. In ICML, 2014.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George\nVan Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneer-\nshelvam, Marc Lanctot, et al. Mastering the game of go with deep neural\nnetworks and tree search. nature, 529(7587):484–489, 2016.\nRichard S Sutton. Dyna, an integrated architecture for learning, planning, and\nreacting. ACM Sigart Bulletin, 2(4):160–163, 1991.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduc-\ntion. 2018.\nRichard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour.\nPolicy gradient methods for reinforcement learning with function approxima-\ntion. In Advances in neural information processing systems, pages 1057–1063,\n2000.\nChristopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8\n(3-4):279–292, 1992.\nRonald J Williams. Simple statistical gradient-following algorithms for connec-\ntionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992.\n18\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2023-01-03",
  "updated": "2023-01-03"
}