{
  "id": "http://arxiv.org/abs/2301.03933v1",
  "title": "Hint assisted reinforcement learning: an application in radio astronomy",
  "authors": [
    "Sarod Yatawatta"
  ],
  "abstract": "Model based reinforcement learning has proven to be more sample efficient\nthan model free methods. On the other hand, the construction of a dynamics\nmodel in model based reinforcement learning has increased complexity. Data\nprocessing tasks in radio astronomy are such situations where the original\nproblem which is being solved by reinforcement learning itself is the creation\nof a model. Fortunately, many methods based on heuristics or signal processing\ndo exist to perform the same tasks and we can leverage them to propose the best\naction to take, or in other words, to provide a `hint'. We propose to use\n`hints' generated by the environment as an aid to the reinforcement learning\nprocess mitigating the complexity of model construction. We modify the soft\nactor critic algorithm to use hints and use the alternating direction method of\nmultipliers algorithm with inequality constraints to train the agent. Results\nin several environments show that we get the increased sample efficiency by\nusing hints as compared to model free methods.",
  "text": "HINT ASSISTED REINFORCEMENT LEARNING: AN APPLICATION\nIN RADIO ASTRONOMY\nA PREPRINT\nSarod Yatawatta\nASTRON, The Netherlands Institute for Radio Astronomy,\nDwingeloo, The Netherlands\nyatawatta@astron.nl\nJanuary 11, 2023\nABSTRACT\nModel based reinforcement learning has proven to be more sample efﬁcient than model free meth-\nods. On the other hand, the construction of a dynamics model in model based reinforcement learning\nhas increased complexity. Data processing tasks in radio astronomy are such situations where the\noriginal problem which is being solved by reinforcement learning itself is the creation of a model.\nFortunately, many methods based on heuristics or signal processing do exist to perform the same\ntasks and we can leverage them to propose the best action to take, or in other words, to provide a\n‘hint’. We propose to use ‘hints’ generated by the environment as an aid to the reinforcement learn-\ning process mitigating the complexity of model construction. We modify the soft actor critic algo-\nrithm to use hints and use the alternating direction method of multipliers algorithm with inequality\nconstraints to train the agent. Results in several environments show that we get the increased sample\nefﬁciency by using hints as compared to model free methods.\n1\nIntroduction\nDeep reinforcement learning (RL) is moving beyond its breakthrough mainstream applications (Levine et al., 2015;\nMnih et al., 2015; Silver et al., 2016) into diverse and fringe disciplines, for example into radio astronomy (Yatawatta\nand Avruch, 2021). Many of these applications of RL use model free methods for training the RL agent, where\nthe agent interacts with the environment via the reward for each action being taken. Model based reinforcement\nlearning (Clavera et al., 2020, 2018; Janner et al., 2019; Wang et al., 2019) builds an additional internal model of the\nenvironment to predict next states given the current state and action being taken. Therefore model based RL is able to\nmimic the environment thus requiring fewer interactions with the environment. Hence model based RL is generally\nmore efﬁcient in terms of the data needed for learning.\nModern radio astronomy is heavily data intensive, and new instruments are being built that will generate petabytes\nof stored data at terabits per second rates. We give a brief and simpliﬁed introduction to radio interferometry here:\nCelestial signals are sampled by an array of sensors on Earth and sent to a correlator. Signals from each sensor is\ncorrelated with the signals from other sensors to form correlations of interferometric pairs. Finally, an image of the\ncelestial sky is formed by performing a Fourier transform on the sphere using the correlated data. The transformation\nof the raw data produced by the sensor array to an image of the celestial sky requires many precise operations on the\ndata. Machine learning has been applied to almost every operation in this whole transformation, for example in data\ninspection (Mesarcik et al., 2020), in interference mitigation (Vafaei Sadr et al., 2020) and in image formation (Wu\net al., 2022). Calibration is another major operation performed on the data that builds a model for the systematic errors\naffecting the data, enabling correction for such errors. Reinforcement learning has already been applied to ﬁne tune\ncalibration (Yatawatta and Avruch, 2021).\nIn this paper, we focus on improving the data efﬁciency in training an RL agent to perform tasks in ﬁne tuning radio\ninterferometric calibration. Calibration itself is a task that builds a model and the direct application of model based RL\nto learn this task is infeasible. This is mainly due to the complexity of creating two models as well as ill-conditioning.\narXiv:2301.03933v1  [astro-ph.IM]  10 Jan 2023\nHint assisted reinforcement learning: an application in radio astronomy\nA PREPRINT\nAs an alternative, we modify the agent to use a hint for the possible action to take given the state information. This\nhint can be generated by any means, for example using signal processing or by heuristics or even by a human expert.\nThis hint does not have to be perfect as well. We can use any metric to measure the distance between the action\nproduced by the agent and the hint and we keep this distance below a threshold. We modify the soft actor-critic (SAC)\nalgorithm (Haarnoja et al., 2018a,b) with this concept of using hints and that boils down to policy optimization under\nan inequality constraint. We use the alternating direction method of multipliers (ADMM) (Boyd et al., 2011) algorithm\nwith inequality constraints (Giesen and Laue, 2019) to train the actor.\nRelated work\n• The use of RL in data processing tasks such as calibration is not uncommon. For example, (Ichnowski\net al., 2021) use RL to accelerate the solving of quadratic programs by ﬁne tuning hyperparameters. Both\n(Yatawatta and Avruch, 2021) and (Ichnowski et al., 2021) are using model free RL and therefore would\nbeneﬁt from model based RL and using hints as proposed by this work.\n• The use of ADMM in policy optimization has been done before, for example (Mordatch and Todorov, 2014)\nuse ADMM in robot trajectory optimization. Similarly, (Levine et al., 2015) use Bregman divergence based\nADMM for learning visuomotor policies. Both these methods have used equality constraints and our work\nuses inequality constraints with ADMM (Giesen and Laue, 2019) enabling ﬂexible constraints, especially\nwhen the hints are not entirely accurate.\n• Model based RL combined with a model free learner is used by (Nagabandi et al., 2017) in a model predictive\ncontrol setting. The model free learner is initialized by a dynamics model. The update of the dynamics model\nand the model free learner is done alternatively. Our work is an improvement of (Nagabandi et al., 2017),\nespecially when the dynamics model has high complexity, by replacing the dynamics model by hints and by\nusing inequality constraints to allow for some inaccuracies in the provided hints. In other words, our method\nis simpler than the method of (Nagabandi et al., 2017) provided that there is a way to generate hints that are\naccurate enough.\n• Constrained optimization has been used in RL, e.g., (Farahmand et al., 2008; Yang et al., 2021; Zhou et al.,\n2022), mainly in safety critical applications or to limit a use of a resource such as fuel. Such work apply\npenalties to the reward or constrain the critic during training. In contrast, our work applies constraints on the\npolicy or the actor.\nThe rest of the paper is organized as follows: In section 2, we describe the SAC algorithm using hints. In section 3,\nwe provide an overview of radio interferometric calibration and how RL is being used in calibration. Next, in section\n4, we provide results based on several environments and ﬁnally, we draw our conclusions in section 5.\nNotation: Lower case bold letters refer to column vectors (e.g. s). Upper case bold letters refer to matrices (e.g.\nC). The matrix inverse, transpose, Hermitian transpose, and conjugation are referred to as (.)−1, (.)T , (.)H, (.)⋆,\nrespectively. The matrix Kronecker product is given by ⊗. The vectorized representation of a matrix is given by\nvec(.). The identity matrix of size N × N is given by IN. Estimated parameters are denoted by a hat, c\n(.). All\nlogarithms are to the base e, unless stated otherwise.\n2\nSoft actor critic with hints\nIn this section, we describe the SAC algorithm (Haarnoja et al., 2018b) that has been modiﬁed to use hints. Similar\nmodiﬁcations can be done to other RL algorithms such as TD3 (Fujimoto et al., 2018). At step index t, st ∈S\nis the state vector and at ∈A is the action being taken, which leads to the next state st+1 giving us the reward\nr(st, at) ∈R. The critic is represented by Qθ(st, at) parameterized by θ and the policy is given by πφ(at|st) which\nis parameterized by φ. For off policy learning, the past experience is collected in the replay buffer D that collects\ntuples (st, at, r(st, at), st+1). An ofﬂine (or target) critic is parameterized by θ that is updated at delayed intervals\nwith weight τ.\nThe value function is given by\nVθ(st) = Ea∼πφ\n\u0002\nQθ(st, at) −α log πφ(at|st)\n\u0003\n(1)\nand the critic is updated by minimizing the loss\nJQ(θ) = E(st,at)∼D\n(2)\n\u00141\n2\n\u0000Qθ(st, at) −\n\u0000r(st, at) + γEst+1∼p\n\u0002\nVθ(st+1)\n\u0003\u0001\u00012\n\u0015\n2\nHint assisted reinforcement learning: an application in radio astronomy\nA PREPRINT\nwhere p is the state transition probability from st to st+1 due to at.\nWe introduce the hint ht ∈A as follows. We introduce a constraint c(at, ht) < δ where c(·, ·) can be any metric,\nsuch as the mean squared error (MSE) or Kullback Liebler divergence (KLD). The distance measured by c(·, ·) is kept\nbelow the threshold δ ∈R+. The hint can be made stochastic as well, for example by providing both the mean and\nthe covariance for ht but in this work we only consider deterministic hints.\nFor the policy update, we need to incorporate the aforementioned constraint. Following (Giesen and Laue, 2019), we\nbuild\ng(at, ht)\n△=\n\u0000[c(at, ht) −δ]+\n\u00012\n(3)\nwhere [x]+ = x if x > 0 and otherwise [x]+ = 0. The augmented Lagrangian is given by\nLπ(φ)\n(4)\n= Est∼D,at∼πφ [α log (πφ(at|st)) −Qθ(st, at)]\n+ρ\n2Eat∼πφ\n\u0002\ng(at, ht)2\u0003\n+ µEat∼πφ [g(at, ht)]\nwhere ρ ∈R+ is the regularization parameter and µ ∈R is the Lagrange multiplier. Using (4), we can apply ADMM\n(Giesen and Laue, 2019) for policy update and at each iteration, we update the Lagrange multiplier as\nµ ←µ + ρEat∼πφ [g(at, ht)] .\n(5)\nThe complete SAC with hints is given in algorithm 1. Similar to (Haarnoja et al., 2018b), the practical implementation\nAlgorithm 1 Soft actor critic with hints\nRequire: θ1,θ2,φ, C: cadence of Lagrange multiplier update, ρ: regularization factor, α: temperature, γ: discount\nfactor, τ: weight update factor, λ: learning rate\n1: Initialize θi ←θi for i ∈[1, 2], µ ←0, D ←{take random steps to ﬁll D tuples}\n2: for each iteration do\n3:\nfor each environment step do\n4:\nat ∼πφ(at|st)\n5:\nGet st+1 and r(st, at) from the environment\n6:\nD ←D ∪(st, at, r(st, at), st+1)\n7:\nend for\n8:\nfor each learning step do\n9:\nθi ←θi −λ∇θJQ(θ) for i ∈[1, 2]\n10:\nφ ←φ −λ∇φLπ(φ)\n11:\nθi ←τθi + (1 −τ)θi for i ∈[1, 2]\n12:\nif learning step is a multiple of C then\n13:\nµ ←µ + ρEat∼πφ [g(at, ht)]\n14:\nend if\n15:\nend for\n16: end for\nof algorithm 1 use two target critic networks and use the minimum Q-value of the two for evaluating (1).\n3\nRadio interferometric calibration\nIn this section, we give a brief overview of radio interferometric calibration and the problem pertaining to calibration\nthat is being solved by RL. In Fig. 1, we show a simple schematic of a pair of receivers on Earth collecting data from\nthe sky and correlating that data, forming an interferometer.\nGiven a pair of receivers p and q on Earth, the output at the correlator can be given as (Hamaker et al., 1996)\nVpq =\nK\nX\ni=0\nJpiCpqiJH\nqi + Npq\n(6)\nwhere Vpq (∈C2×2) is a 2 × 2 matrix of complex numbers that are the observed data. This data are assumed to be\ncomposed of a signal corresponding to a source in the sky being observed (target) and K signals of outlier sources that\n3\nHint assisted reinforcement learning: an application in radio astronomy\nA PREPRINT\nBeam\nReceiver\nCorrelator\nAtmosphere\nVpq\nTarget\nOutlier\nOutlier\nBeam\nJp\nJq\nReceiver\nFigure 1: A simple interferometer observing a target direction while there are two outlier sources appearing as well.\nact as interference. The signal from each source in the sky at the receiver is corrupted by systematic errors (varying\nboth in time and in frequency) that are represented as Jpi,Jqi (∈C2×2) in (6). The uncorrupted signal of each source\nis given by Cpqi (∈C2×2) and this is fairly stable and can be pre-computed. Additionally, we also have a contribution\nfrom noise, which is modeled by Npq (∈C2×2).\nGiven N receivers, we can form N(N −1)/2 interferometers and by observing over a long time period and a large\nbandwidth, we can increase the number of collected data points. Calibration in a nutshell is ﬁnding Jpi,Jqi in (6),\ngiven Vpq and Cpqi for all p,q and i. There are specialized software already to perform calibration, see for example\n(Yatawatta et al., 2012). Our interest in using RL is to select the directions to calibrate, in other words, given K\npossible directions in (6) i ∈[1, K], select the subset of i (say I) to get the best output data quality with minimum\ncomputational cost spent in calibration. Note that the target direction corresponds to i = 0 and is always included in\nthe calibration. We should also mention here that calibration is computationally demanding, and it has to be performed\nfor each data point of many data points covering a large time and frequency interval, accumulating into thousands of\nseparate calibrations.\nWe use an approach based on the Akaike information criterion (AIC) (Akaike, 1974) to generate the hint by selecting\nthe best possible subset of i ∈[1, K] (or I) to use in calibration. We rewrite (6) in vector form as\nvpq =\nK\nX\ni=0\nspqi + npq\n(7)\nwhere spqi = (J⋆\nqi ⊗Jpi)vec(Cpqi), vpq = vec(Vpq), and npq = vec(Npq). We can stack up vpq in (7) for all\npossible p, q and for all time and frequency ranges within which a single solution for calibration is obtained. Let us\ncall this y. We can do the same for the right hand side of (7) for each direction i to get si and also for the noise to get\nn. Let us rewrite (7) after stacking as\ny = s0 +\nX\ni∈I\nsi + n\n(8)\nwhere we have s0 for the model of the target direction and the remaining directions are taken from the set I. For this\nsetup, after calibration, we ﬁnd the residual signal as\nr = y −bs0 −\nX\ni∈I\nbsi\n(9)\nwhere bsi is the model constructed by calibration (so not necessarily the true model). Using (9), we ﬁnd the AIC as\nAICI =\n\u0012Nσr\nσy\n\u00132\n+ N|I|\n(10)\nwhere σy and σr are the standard deviations of y and r in (9), respectively. The cardinality of I is given by |I|.\n4\nHint assisted reinforcement learning: an application in radio astronomy\nA PREPRINT\nWith K directions, we have 2K possible conﬁgurations for I. By evaluating (10) for each of them, it is possible\nto select I that minimizes (10), which is what is done in current practice. There are several shortcomings of this\napproach. First, 2K is a large number, especially when we need to perform a computationally expensive calibration for\neach one of them (in real-time if needed). Secondly, we ignore the underlying global dependencies between different\nobservations (i.e., when the target is in different directions in the sky). The celestial sky is very stable in terms of\nsource directions and their intensities. However, because we evaluate (10) for each observation individually, there is\nno way of incorporating this stable behavior with respect to multiple observations into one. This is the motivation for\nusing RL for the determination of I given any observation. We intend to cut down the number of calibration runs\nneeded to evaluate (10) from 2K to a lower value around K.\nLet us explain the mapping between j ∈[0, 2K) and a vector in RK before we explain the generation of the hint. In\nTable 1, we show the relation between index j, the canonical vector ej ∈RK and the set I for K = 3. The hint for\nany given observation is generated as follows:\n1. We evaluate the AIC for all j ∈[0, 2K) using (10) and the mapping in Table 1. Let us call the AIC for the\nj-th index as AICj.\n2. We transform the AIC into the range [0, 1] as\nνj =\nexp (−AICj/100)\nP2K−1\nj′=0 exp (−AICj′/100)\n.\n(11)\n3. We generate the hint h as\nh =\n2K−1\nX\nj=0\nνjej.\n(12)\nFinally, the indices in h (+1 for consistency) that have values > 0.5 are selected to be part of I and be part of\ncalibration. Consequently, we design the actor in the SAC algorithm to produce an action in [0, 1]K, or in other words,\nthe predict the probability of each i being selected to I. In section 4, we will discuss the RL agent in detail.\nTable 1: Mapping between 2K and K for K = 3\nj\nej\nI\n0\n[0 0 0]T\n{ }\n1\n[0 0 1]T\n{1}\n2\n[0 1 0]T\n{2}\n3\n[0 1 1]T\n{1, 2}\n...\n...\n...\n7\n[1 1 1]T\n{1, 2, 3}\n4\nResults\nWe present results based on several environments in this section. The focus is to test the sample efﬁciency of hint\nassisted RL (expected to be similar to model based RL in sample efﬁciency) compared with model free RL. The\nhyperparameters used in all experiments are given in Table 2.\n4.1\nBipedal walker\nWe consider the BipedalWalker-v3 and BipedalWalkerHardcore-v3 environments provided by openai.gym. In\nall experiments with this environment, we use the standard two layer network model for the actor and the critic. We\ntrain the agent for one realization of the BipedalWalker-v3 until it converges and we save the models for the actor\nand the critic to provide hints in all experiments that follow. In Fig. 2, we show the performance of agents over 10\nruns with different random seed initialization. Note that the hints are provided by a separate agent initialized with the\nsaved model that is reused in all runs.\nWe see in Fig. 2 that by using hints, we can learn faster compared to having no hints. In the next experiment,\nwe still keep the saved model to provide hints (learnt from BipedalWalker-v3) and train the agent to solve the\n5\nHint assisted reinforcement learning: an application in radio astronomy\nA PREPRINT\nTable 2: Hyperparameters used in all experiments\nHyperparameter\nBipedal walker\nElastic net\nCalibration\nlearning rate λ\n3e-4\n1e-3\n3e-4\nbatch size\n256\n64\n256\nτ\n0.005\n0.005\n0.005\nα\n0.036\n0.03\n0.04\nγ\n0.99\n0.99\n0.99\nreward scale\n×5 if > 0\n×2\n×10 if > 0\nρ\n0.001\n0.01\n1.0\nconstraint c(·, ·) < δ\nMSE\nMSE\nKLD\nδ = 0.5\nδ = 0.1\nδ = 0.01\nC\n10\n10\n10\nFigure 2: The performance of SAC in the BipedalWalker-v3 environment. The agent assisted by hints learns faster.\nBipedalWalkerHardcore-v3 environment. Note that with this setting, the hints provided are to a general extent\ninaccurate. To accommodate this inaccuracy, we tune the values of ρ and δ in Algorithm 1 as shown in Table 2. In Fig.\n3, we show the results in BipedalWalkerHardcore-v3 environment averaged over 4 runs with different random seed.\nWe also show the result for an agent who is initialized by the saved model that was used to generate the hints. Obviously\nthis means that the networks used for both experiments BipedalWalker-v3 and BipedalWalkerHardcore-v3 are\nthe same. Using hints shows a clear improvement as opposed to using no hints. However, compared to initialization\nof the model with the saved model, using hints needs more iterations to reach the highest expected reward.\n4.2\nElastic net regression\nGiven the observation x (∈RM) and the linear model x = Aθ with design matrix A (∈RM×M), elastic net regression\n(Zou and Hastie, 2005) ﬁnds a solution for θ as\nbθ = arg min\nθ\n\u0000∥x −Aθ∥2 + ρ2∥θ∥2 + ρ1∥θ∥1\n\u0001\n.\n(13)\nModel free RL has been used to determine the best values for ρ1 and ρ2 by Yatawatta and Avruch (2021). In this\nexperiment, we compare the model free RL performance with the performance using hints to determine the best\nvalues for ρ1 and ρ2. We use grid search to generate a hint for ρ1 and ρ2, albeit on a coarse grid.\nThe state vector is a concatenation of vec(A) and the eigenvalues (1 + λ(H)) where\nH = A1\n2\n\u0000AT A + (ρ2 + ρ1δ(∥θ∥))I\n\u0001−1 \u0000−2AT \u0001\n.\n(14)\nThe reward is evaluated as\n∥x∥\n∥x−Aθ∥+ min(1+λ(H))\nmax(1+λ(H)).\n6\nHint assisted reinforcement learning: an application in radio astronomy\nA PREPRINT\nFigure 3: The performance of SAC in the BipedalWalkerHardcore-v3 environment. The agent assisted by hints\nlearns faster, but not as fast as an agent that is initialized with the model that is being used to generate the hints.\nIn Fig. 4 we show the performance of SAC in solving the elastic net environment (20 different realizations) for\na problem size of M = 20. The number of steps per episode is limited to 10. Using hints shows only a slight\nimprovement initially but both methods reach the solution almost at the same rate. The lack of a major difference in\nFig. 4 we attribute to the simplicity of the problem.\nFigure 4: The performance of SAC in solving the elastic net regression hyperparameter selection.\n4.3\nRadio interferometric calibration\nWe use a specialized simulator to simulate observations of the LOFAR radio telescope (van Haarlem et al., 2013) in\ntraining the RL agent. We consider K = 5 well known sources as outliers in the sky (namely Cassiopeia A, Cygnus\nA, Taurus A, Virgo A and Hercules A). We vary the target direction, observing frequency, observing epoch, receiver\nnoise and systematic errors in each simulation. We also make sure each observation is valid, i.e., the target remains\nabove the horizon.\nThe state is a concatenation of the inﬂuence map (Yatawatta and Avruch, 2021) of each calibration (an image of\n128×128 pixels), the angular separation of each K sources from the target and some metadata including the azimuth\nand elevation of each source, observing frequency and N. The action is the K probabilities of each source being\nselected into I (the output of the actor is in [−1, 1]K which is transformed into [0, 1]K). The reward is calculated\nusing the negative AIC given in (10). We ﬁnd the AIC when I = { }, i.e., when only the target is selected for\ncalibration and subtract it from the reward. We also scale the reward to have a standard deviation of about 1.\nIn Fig. 5, we show the reward obtained by SAC agent with and without hints. The number of steps per each episode\n(an episode is one simulated observation) is kept at 7. As seen in Fig. 5, using hints enable us to achieve a slightly\n7\nHint assisted reinforcement learning: an application in radio astronomy\nA PREPRINT\nhigher reward. We also have seen that the actor is prone to get stuck in local minima, producing the same action for\nall observations. This is signiﬁcantly reduced by using hints. Due to the same reason, we have used large value for ρ\nand a small value for δ as given in Table 2.\nFigure 5: The performance of SAC in solving selection of I in calibration.\nIn Fig. 6 we show the rewards obtained by agents that are trained both with and without using hints. The training used\nabout 4000 episodes. In addition, we also show in Fig. 6 the reward obtained by just using the hint as the action. In\neach randomly generated episode, the agents take 7 steps and the action giving the maximum reward is taken to be\nthe solution. We order the episodes by the sorted reward obtained by the agent trained without hints, for clarity. It is\nclear from Fig. 6 that in most episodes, the agent trained using hints perform better than (or equal to) the agent trained\nwithout hints. The converse is happening in only a handful of episodes. It is also noteworthy to see that just using the\nhint as the action gives mixed results, being both better and worse than the trained agents.\nFigure 6: Performance evaluation of the trained agents (after about 4000 training episodes). The evaluation episodes\nare ordered according to the reward gained by the agent trained without using hints. The agent trained with hints gives\na better or equal reward in almost all episodes. Using the hint itself as the action gives mixed results, some being better\nand some being worse.\n5\nConclusions\nWhen an alternative method exists to generate actions to take in any given state, we have modiﬁed the SAC algorithm\nto use such actions as hints in learning the policy. Results based on several environments show that it is beneﬁcial to\n8\nHint assisted reinforcement learning: an application in radio astronomy\nA PREPRINT\nuse hints, even when the hints are not entirely accurate. Further investigations can be done in using stochastic hints as\nopposed to deterministic hints as done in this work. The source code for the software is available online 1.\nAcknowledgements\nWe thank the anonymous reviewers for the careful review and helpful comments.\nReferences\nAkaike, H. (1974). A New Look at the Statistical Model Identiﬁcation. IEEE Transactions on Automatic Control,\n19:716–723.\nBoyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J. (2011). Distributed optimization and statistical learning via\nthe alternating direction method of multipliers. Foundations and Trends® in Machine Learning, 3(1):1–122.\nClavera, I., Fu, V., and Abbeel, P. (2020). Model-Augmented Actor-Critic: Backpropagating through Paths. arXiv\ne-prints, page arXiv:2005.08068.\nClavera, I., Rothfuss, J., Schulman, J., Fujita, Y., Asfour, T., and Abbeel, P. (2018). Model-Based Reinforcement\nLearning via Meta-Policy Optimization. arXiv e-prints, page arXiv:1809.05214.\nFarahmand, A., Ghavamzadeh, M., Mannor, S., and Szepesv´ari, C. (2008). Regularized policy iteration. Advances in\nNeural Information Processing Systems, 21.\nFujimoto, S., van Hoof, H., and Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods.\narXiv e-prints, page arXiv:1802.09477.\nGiesen, J. and Laue, S. (2019). Combining ADMM and the augmented Lagrangian method for efﬁciently handling\nmany constraints. In Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence,\nIJCAI-19, pages 4525–4531. International Joint Conferences on Artiﬁcial Intelligence Organization.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018a). Soft Actor-Critic: Off-Policy Maximum Entropy Deep\nReinforcement Learning with a Stochastic Actor. arXiv e-prints, page arXiv:1801.01290.\nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., and\nLevine, S. (2018b). Soft Actor-Critic Algorithms and Applications. arXiv e-prints, page arXiv:1812.05905.\nHamaker, J. P., Bregman, J. D., and Sault, R. J. (1996). Understanding radio polarimetry. I. Mathematical foundations.\nAstronomy and Astrophysics Supp., 117:137–147.\nIchnowski, J., Jain, P., Stellato, B., Banjac, G., Luo, M., Borrelli, F., Gonzalez, J. E., Stoica, I., and Goldberg, K.\n(2021). Accelerating Quadratic Optimization with Reinforcement Learning. arXiv e-prints, page arXiv:2107.10847.\nJanner, M., Fu, J., Zhang, M., and Levine, S. (2019). When to trust your model: Model-based policy optimization.\nAdvances in Neural Information Processing Systems, 32.\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. (2015). End-to-End Training of Deep Visuomotor Policies. arXiv\ne-prints, page arXiv:1504.00702.\nMesarcik, M., Boonstra, A.-J., Meijer, C., Jansen, W., Ranguelova, E., and van Nieuwpoort, R. V. (2020). Deep learn-\ning assisted data inspection for radio astronomy. Monthly Notices of the Royal Astronomical Society, 496(2):1517–\n1529.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fid-\njeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wier-\nstra, D., Legg, S., and Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature,\n518(7540):529–533.\nMordatch, I. and Todorov, E. (2014). Combining the beneﬁts of function approximation and trajectory optimization.\nIn Robotics: Science and Systems, volume 4.\nNagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. (2017). Neural Network Dynamics for Model-Based Deep\nReinforcement Learning with Model-Free Fine-Tuning. arXiv e-prints, page arXiv:1708.02596.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,\nLeach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. (2016). Mastering the game of go with deep neural\nnetworks and tree search. Nature, 529(7587):484–489.\n1https://github.com/SarodYatawatta/hintRL\n9\nHint assisted reinforcement learning: an application in radio astronomy\nA PREPRINT\nVafaei Sadr, A., Bassett, B. A., Oozeer, N., Fantaye, Y., and Finlay, C. (2020). Deep learning improves identiﬁcation\nof Radio Frequency Interference. Monthly Notices of the Royal Astronomical Society, 499(1):379–390.\nvan Haarlem, M. P., Wise, M. W., Gunst, A. W., et al. (2013). LOFAR: The LOw-Frequency ARray. Astronomy and\nAstrophysics, 556:A2.\nWang, T., Bao, X., Clavera, I., Hoang, J., Wen, Y., Langlois, E., Zhang, S., Zhang, G., Abbeel, P., and Ba, J. (2019).\nBenchmarking Model-Based Reinforcement Learning. arXiv e-prints, page arXiv:1907.02057.\nWu, B., Liu, C., Eckart, B., and Kautz, J. (2022). Neural interferometry: Image reconstruction from astronomical\ninterferometers using transformer-conditioned neural ﬁelds.\nProceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 36(3):2685–2693.\nYang, Q., Sim˜ao, T. D., Tindemans, S. H., and Spaan, M. T. (2021). WCSAC: Worst-case soft actor critic for safety-\nconstrained reinforcement learning. In AAAI, pages 10639–10646.\nYatawatta, S. and Avruch, I. M. (2021). Deep reinforcement learning for smart calibration of radio telescopes. Monthly\nNotices of the Royal Astronomical Society, 505(2):2141–2150.\nYatawatta, S., Kazemi, S., and Zaroubi, S. (2012). GPU accelerated nonlinear optimization in radio interferometric\ncalibration. in Proceedings of Innovative Parallel Computing (InPar’12).\nZhou, X., Zhang, X., Zhao, H., Xiong, J., and Wei, J. (2022). Constrained soft actor-critic for energy-aware trajectory\ndesign in UAV-aided IoT networks. IEEE Wireless Communications Letters, 11(7):1414–1418.\nZou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical\nSociety: Series B (Statistical Methodology), 67(2):301–320.\n10\n",
  "categories": [
    "astro-ph.IM",
    "cs.LG"
  ],
  "published": "2023-01-10",
  "updated": "2023-01-10"
}