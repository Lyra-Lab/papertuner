{
  "id": "http://arxiv.org/abs/2301.06340v2",
  "title": "Deep Learning Models to Study Sentence Comprehension in the Human Brain",
  "authors": [
    "Sophie Arana",
    "Jacques Pesnot Lerousseau",
    "Peter Hagoort"
  ],
  "abstract": "Recent artificial neural networks that process natural language achieve\nunprecedented performance in tasks requiring sentence-level understanding. As\nsuch, they could be interesting models of the integration of linguistic\ninformation in the human brain. We review works that compare these artificial\nlanguage models with human brain activity and we assess the extent to which\nthis approach has improved our understanding of the neural processes involved\nin natural language comprehension. Two main results emerge. First, the neural\nrepresentation of word meaning aligns with the context-dependent, dense word\nvectors used by the artificial neural networks. Second, the processing\nhierarchy that emerges within artificial neural networks broadly matches the\nbrain, but is surprisingly inconsistent across studies. We discuss current\nchallenges in establishing artificial neural networks as process models of\nnatural language comprehension. We suggest exploiting the highly structured\nrepresentational geometry of artificial neural networks when mapping\nrepresentations to brain data.",
  "text": " \n1 \nDeep Learning Models to Study Sentence Comprehension in the \nHuman Brain \n \nSophie Arana✝,1,2,3,*, Jacques Pesnot Lerousseau✝,1 and Peter Hagoort2,3 \n \n1 Department of Experimental Psychology, University of Oxford, OX2 6HG, United \nKingdom \n2 Max Planck Institute for Psycholinguistics, 6525 XD Nijmegen, The Netherlands \n3 Donders Institute for Cognition, Brain and Behaviour, Radboud University, 6525 AJ \nNijmegen, The Netherlands \n* Correspondence: arana.sophie@gmail.com  \n✝ Co-first authorship \n \n \n \nDisclosure of interest: The authors declare no competing interests. \nAcknowledgements: We thank Christopher Summerfield, Micha Heilbron and Jessica A. F. \nThompson for their precious comments.  \nFunding sources: Work supported by Fondation Pour l’Audition (J.P.L.) \nAuthor contributions: Conceptualization S.A., J.P.L. and P.H.; Supervision P.H.; Writing – \noriginal draft S.A. and J.P.L.; Writing – review & editing S.A., J.P.L. and P.H. \n \n \n2 \nAbstract \n \nRecent artificial neural networks that process natural language achieve unprecedented \nperformance in tasks requiring sentence-level understanding. As such, they could be \ninteresting models of the integration of linguistic information in the human brain. We review \nworks that compare these artificial language models with human brain activity and we assess \nthe extent to which this approach has improved our understanding of the neural processes \ninvolved in natural language comprehension. Two main results emerge. First, the neural \nrepresentation of word meaning aligns with the context-dependent, dense word vectors used \nby the artificial neural networks. Second, the processing hierarchy that emerges within artificial \nneural networks broadly matches the brain, but is surprisingly inconsistent across studies. We \ndiscuss current challenges in establishing artificial neural networks as process models of \nnatural language comprehension. We suggest exploiting the highly structured representational \ngeometry of artificial neural networks when mapping representations to brain data. \n \n \n \n \n \n \n3 \nI. \nNew analytic tools have enabled the study of brain activity \nduring sentence comprehension.  \n \nRecent advances in natural language processing, i.e. the automatic analysis of natural \nlanguage by computer algorithms, have greatly impacted the study of brain activity during \nsentence comprehension. Two main innovations are at the core of these successes: \nrepresenting words as low-dimensional dense vectors and learning such representations with \nartificial neural networks trained on large text corpora (Goldberg, 2016).  \n \na. Learning word vectors with artificial neural networks \n \nIn order to build natural language processing algorithms, it is necessary to represent \nlanguage units, such as words, as numbers. This operation is called embedding. To illustrate \nthis point, let’s take a toy language composed of five words: King, Queen, Man, Woman, Men. \nA naive embedding would be to convert each word as a one-hot vector, where each word is \nits own dimension: King is the first dimension [1, 0, 0, 0, 0], Queen is the second dimension \n[0, 1, 0, 0, 0], Man is the third dimension [0, 0, 1, 0, 0], and so on. Such an embedding has \ntwo disadvantages. First, it is memory inefficient, as the vector size increases quickly with the \nsize of the vocabulary: in our example, n dimensions encode at most n words. Second, there \nis no similarity structure between the words as each word is completely independent from the \nothers, which is a problem for generalisation purposes, such as learning a new word and \nquickly inferring its meaning based on similar words in the vocabulary.  \n \nA better implementation is to use distributed encoding (Harris, 1954; Hinton, 1986; \nRumelhart, Hinton, & Williams, 1986), i.e. to construct the representation of each word as a \nset of multiple features. Usually distributed encoding is thought to represent semantic features \nrather than visual, orthographic, auditory or phonological properties of words, although in \nprinciple any feature could be represented through a distributed code. In our toy example, one \ncan represent each word as a 3-dimensional vector with “gender” as the first dimension (0: \nmale, 1: female), “number” as the second dimension (0: singular; 1: plural) and “regal” as the \nthird dimension: King is [0, 0, 1], Queen is [1, 0, 1], Male is [0, 0, 0], Woman is [1, 0, 0] and \nMen is [0, 1, 0]. Such a distributed representation has multiple advantages. First, it is memory \nefficient: in our example, n dimensions encode at most 2n words. Its dimensionality is thus \nmuch lower than the naive embedding one, hence the term “low-dimensional”. In practice, the \nnumber of dimensions in recent models ranges from about 50 to a few hundred. Second, it \nallows performing semantically meaningful operations on vectors (Mikolov, Chen, Corrado, & \nDean, 2013): the operation King + Woman = [0, 0, 1] + [1, 0, 0] = [1, 0, 1] = Queen. Third, it \nfacilitates generalisation by reusing the same features to encode new words and thus infer \ntheir similarity to the other words: in our toy language, the new word Kings would be encoded \nas [0, 1, 1], which happens to be close to King ([0, 0, 1]) and far from Woman ([1, 0, 0]). On a \nside note, this inference will only be meaningful if the initial distributed representation is \nmeaningful. For example here, “gender” as an input variable has been imposed by hand.  \n \n \nMoving from representing each word as a unique dimension to representing them \ninstead as dense vectors has been described as “perhaps the biggest conceptual jump” in \nnatural language processing (Goldberg, 2016). Nowadays, nearly all tools developed in \nnatural language processing use such distributed vector representations of individual words. \n \n4 \nRecent embeddings typically consist of several hundreds of dimensions, for example 640 in \n(Mikolov et al., 2013) or 512 in (Vaswani, Shazeer, & Parmar, 2017). Contrary to our toy \nexample, the dimensions of these embeddings are not easily interpretable as they do not \ncorrespond to salient features, such as gender.  \n \nLearning word embedding by training artificial neural networks on large text corpora.  \n \n \nFinding an optimal low-dimensional word embedding is a difficult problem, especially \nwhen the vectors have several hundreds of dimensions. The current approach in natural \nlanguage processing is to use large text corpora to learn the appropriate embeddings. This \nrelies on the distributional hypothesis: the fact that semantic features that distinguish the \nmeanings of words are reflected in the statistics of their use within large text corpora (Firth, \n1957; Mikolov et al., 2013; Mitchell et al., 2008), i.e. that words that have a close semantic \nproximity tend to appear within similar contexts. This means that the embeddings do not use \nthe statistics of the world nor the co-occurrence of words and real-world situations directly but \nonly through the statistics internal to the text corpora. Learning these statistics is typically done \nby artificial neural networks and deep learning techniques (LeCun, Bengio, & Hinton, 2015). \nThese artificial neural networks are composed of artificial neurons that realise simple  \noperations on inputs. The artificial neurons are organised in layers. In recurrent architectures, \nthe output of each layer is fed back as input to the same layer at the next time step. In serial \narchitectures, such as transformers, the output of the artificial neurons of one layer are the \ninput to the artificial neurons of the next layer. Note that “layer” does not refer to the six cortical \nlayers of the brain but rather to the stage of an artificial neuron in this series of pools of artificial \nneurons. Two main artificial neural network architectures dominate the field: recurrent neural \nnetworks and transformers.  \n \n \nFirst, recurrent neural networks have been extensively used in natural language \nprocessing. They process input sequences one element at a time, while maintaining a \nseparate state vector that contains the information about previous context. The most used \narchitecture is the long-short term memory unit (LSTM) (Mikolov, Karafiát, Burget, Černocký, \n& Khudanpur, 2010; Sundermeyer, Schlüter, & Ney, 2012). In such architectures, the input \nlayer is a one-hot encoding vector that corresponds to the word that is presented. The next \nlayers are composed of LSTM units. Each LSTM unit is itself composed of three gates that \ncontrol which information of the hidden state vector is going to be forgotten, maintained and \nintegrated to the input (Hochreiter & Schmidhuber, 1997). This allows for flexible gating of \ninformation to either maintain it over a long period of time or quickly forget it when necessary. \nOver training, the network learns to build and maintain the best possible state vector, i.e. the \nvector that captures the most information about the sentence. Depending on the task it has \nbeen trained on, the network can output next word predictions, next sentence selections and \nsentence topic predictions (Ghosh et al., 2016).  \n \n \nSecond, the transformer architecture (Vaswani et al., 2017) is currently the state of the \nart for natural language processing. Contrary to recurrent neural networks that process \nsequences one element at a time, transformers process whole sequences in parallel. In the \ninput layer, each word of the sentence is fed to the network as a one-hot vector. Each word is \nthen independently embedded by a first embedding layer. This produces a context-free \nembedding of the words. A positional encoding vector is then added to the context-free \nembedding of each word so that the model has information about the word order. Then, \n \n5 \ntransformers consist of a series of embedding layers, where the embedding of each word is \nmixed with the embeddings of the surrounding words to produce a context-dependent \nembedding. As a result, the first embedding layer is a context-free embedding of each word \nwhile the last embedding layer provides a context-dependent embedding of each word. The \ncentral component of each embedding layer is the “attention head” that controls the amount \nof mixing between word embeddings and the precise locus of this mixing through a “self-\nattention” mechanism. It should be noted that “attention” only refers vaguely to the usual notion \nof attention in psychology and the reader should treat them as two different concepts. More \nspecifically, an attention head proceeds in three steps. (1) First, the attention head computes \nthree projections of each word, called the “query”, “key” and “value” vectors. This projection is \nlearned by the model during training. (2) Second, for each word, the attention head computes \na similarity score between the “query” vector of the word and the “key” vectors of all words, by \ntaking the dot product of each vector pair. This gives a value that indicates to what extent the \n“key” of each word matches the “query” of each word. (3) Third, the attention head computes \na weighted average of the “value” vectors for each word, where the weight is equal to the \nsimilarity score computed above. This weighted average is the new embedding of each word, \nready to be passed to the next layer for another round of mixing.  \n \nTo illustrate further, let’s take the example sentence “I put my cash in the bank”. The \ninterest of this example is the polysemic word “bank”, which can refer to a financial institution \nor to land alongside a river. To disambiguate its meaning, an attention head might produce a \n“query” vector for “bank” that has high values for features like “institution-related”, “money-\nrelated”, “river-related”, “lake-related”. (in plain text, the attention head asks: does anyone in \nthe sentence know if we are talking about money-related stuff or about river-related stuff?). \nThe “key” vector of “cash” will probably have a high similarity score with the “query” vector \nbecause it has a high  “money-related” feature value (in plain text, the word “cash” responds: \nyes, I have information to know that we are talking about money-related stuff and not river-\nrelated stuff!). The attention head will finally mix the embedding of the word “bank” with the \nembedding of the word “cash”. As a result, the “river-related” feature disappears and the \n“money-related” feature is reinforced in the context-dependent embedding of the word “bank”. \nTo sum up, one can think of an attention head as a series of computations that project the \nwords in a subspace, measure the similarity between each word in that subspace, and mix \nthe representation of the words according to this similarity measure. In practice, the features \nof the word vector are not built-in but rather learned through training, and contrary to our \nexample they are usually not easily determined in any interpretable way. Further, transformers \nare composed of multiple attention heads in each layer and also involve further mixing \nstrategies. Transformers are usually trained to predict a masked word in a sentence or a \nparagraph. The most common transformer models that have been used in neuroscience are \nBERT (Devlin, Chang, Lee, & Toutanova, 2018), GPT-2 (Radford, Wu, Child, Luan, & Amodei, \n2019) and GPT-3 (Brown, Mann, & Ryder, 2020).   \n \n \n \nLearning word embeddings on large text corpora by means of artificial neural networks, \nhas produced excellent results in a variety of natural language processing tasks, such as \nquestion answering, translation or text summarisation. Further, this approach has provided \nnew models of language processing for cognitive neuroscientists interested in how the brain \nprocesses language. Indeed, these models have two advantages. First, they are usually \ntrained end-to-end with almost no a priori knowledge on how language works. This is an \nadvantage because they do not depend on linguistic theories and thus provide more objective \n \n6 \nmeasures on language. Further, this releases additional assumptions concerning the fact that \nhumans are born with in-built knowledge of the language structure. Second, they can process \nnatural language, thus providing tools to analyse human behaviour and brain activity during \nnaturalistic tasks. As a consequence, a multitude of methods to compare the brain with these \nlanguage models have been developed.  \n \nb. Comparing brain activity with artificial neural networks \n \n \n \nThe most common paradigm involves presenting the same words or sentences to \nhuman participants and to the models. Three approaches have been used to then compare \nhuman behaviour and brain activity with the models: directly correlating brain activity with the \nmodel’s activity, comparing behaviour and brain activity with metrics derived from the model’s \noutputs, and comparing the geometry of the representations extracted from the brain activity \nand the model’s activity. Each approach involves different assumptions and can lead to \ndifferent conclusions concerning the link between the brain activity and the models.  \n \nComparison of brain activity with the model's activity.  \n \n \nThe first approach is to directly compare the patterns of brain activity during the \npresentation of words to the word embeddings of the model. The assumption is that if the brain \nis using the same embedding strategy as the model, then one should be able to build a \nmapping between the brain activity and the word embeddings. Usually, a further assumption \nis that this mapping is linear and can thus be approximated by a linear regression.  \n \n \nThe linear regression approach was employed for the first time by Mitchell and \ncollaborators in 2008 (Mitchell et al., 2008). The authors measured brain activity during the \npresentation of single words using fMRI. The activity in each voxel was then decomposed as \na weighted sum of the corresponding word embedding. The mapping between the word \nembedding and each voxel was learned by a linear regression and was used to predict brain \nresponses to words that were not used during training. Geometrically, a linear regression can \nbe thought of as a hyperplane. In this context, the hyperplane is a tilted flat surface in the \nspace whose dimensions correspond to the embedding space dimensions and a dimension \ncorresponding to the brain response. The brain response associated with a given word can be \ndescribed by a projection of the word in the embedding space on a line in the tilted direction \nof the hyperplane. Therefore, the success of the linear regression in one voxel implies that the \nactivity in this voxel is correlated to the projection of the words onto a line in the embedding \nspace. This linear mapping approach can be equally applied at the sentence level by replacing \nthe word embedding with an embeddings vector for the integrated sentence meaning. Using \nsentence-level embeddings, recent works have extended this approach to other regression \ntechniques, such as linear ridge regression (Caucheteux & King, 2022), other models, such \nas GPT-2 (Goldstein, Dabush, et al., 2022), and other brain recording modalities, such as \nMEG (Wehbe, Vaswani, Knight, & Mitchell, 2014) or ECoG (Goldstein, Zada, et al., 2022). \nThese works are reviewed in detail in the next part.  \n \nInferring identity between brain and models. \n \nNot every comparison method is well suited to establish correspondence between a \nmodel and the brain. A single correlation between one model and the brain data, as is typically \n \n7 \ndone using the regression approach, provides limited information. Indeed, the level of \nexplanation at which one can infer identity based on model fits is difficult, because similar \nfunctions do not automatically imply similar realisations (Guest & Martin, 2021; Jonas & \nKording, 2017). A cautious approach is usually to refrain from inferences concerning \nexplanation at the lowest levels, like those about detailed implementation. Further, inferences \nare most meaningful when based on a contrastive approach, comparing model fits between \nmultiple models that differ in one aspect. \n \nOverfitting. \n \nAny work that compares large artificial neural networks and brain data using regression \ntechniques faces the issue of overfitting. This is partly because the number of trials used as \ntraining data for the regression is usually much smaller than the number of parameters to train. \nIn these conditions, even a bad model could learn the idiosyncratic characteristics of the \ntraining set and artificially fit the training data. To reduce this issue, standard methodological \ntools are used. First, regularisation methods, such as ridge regression (Tikhonov, 1963) or \nlasso regression (Tibshirani, 1996), penalise the regression for large parameters. This biases \nthe regression towards sparse sets of parameters, i.e. sets with a low number of non-zero \nparameters. Such sets are less prone to overfitting. Second, cross-validation reduces \noverfitting by using independent datasets for training and testing the models fit to the brain. \n \nComparison of brain activity with metrics derived from the model.  \n \n \nThe second approach is to compare brain activity with metrics derived from the model's \noutputs. The main assumption is that if the brain and the models share similar computations, \nthey will produce similar outputs. This does not assume that the brain and the model rely on \nsimilar representations, but rather that they produce the same outputs or that they converge \ntoward the same result. The most common method is to compare the surprise of the model \nand the brain activity. Surprise is a quantity developed in the field of information theory \n(Shannon, 1948) to measure the degree of unexpectedness of a stimulus. It has a precise \nmathematical formulation: -log2P(w) where P(w) is the probability of occurrence of the word w. \nIt nonetheless corresponds to the intuitive notion of surprise, i.e. it is low when a word is highly \nexpected by the model and high when a word is highly unexpected by the model. A further \nusual assumption is that the brain activity scales with the surprise, such that highly surprising \nwords elicit large brain activity while highly unsurprising words elicit small brain activity (Mars \net al., 2008).  \nInformation-based surprise has been used in a recent paper by Heilbron and \ncollaborators (Heilbron, Armeni, Schoffelen, Hagoort, & de Lange, 2020). The authors \ncompute the surprise of a language model, GPT-2, relative to different linguistic levels, namely \nsyntactic, phonemic and semantic. Syntactic surprise refers to the extent to which the model \nexpects the syntactic category of each word. Similarly, phonemic and semantic surprise \ncorrespond to the extent to which the model expects the phonemes and the semantic features \nrespectively. The authors then correlate the brain activity measured by EEG and MEG with \nthe surprise computed from the model with respect to the different linguistic levels to reveal \nthe location and time windows during which the brain processes each linguistic level. Other \nrecent works have used this method with other brain imaging, such as fMRI (Brennan, Dyer, \nKuncoro, & Hale, 2020; Schmitt et al., 2021a), and with other output measures, such as \nentropy (Donhauser & Baillet, 2020).  \n \n8 \nAnother related form of surprisal that has been shown to modulate the brain's \nresponses is Bayesian surprise., i.e. how much a model’s belief changes based on an \nincoming word (Itti & Baldi, 2009). In the context of sentence comprehension, this has been \ndemonstrated for example by Rabovsky et al's sentence gestalt model, a recurrent neural \nnetwork architecture trained on sentence comprehension. The word-induced update in the \nhidden unit activations of the network came to reflect the update of a probabilistic sentence \nrepresentation. The magnitude of this update was then shown to predict amplitude \nmodulations in neural activity across tasks (Rabovsky, Hansen, & McClelland, 2018). To our \nknowledge the Bayesian surprisal metric has not yet been used in conjunction with large-scale \nlanguage models as a comparison for human brain data (but see M. Kumar et al., 2022 for \npredicting human narrative segmentation).  \n \nComparison of the geometry of the representations.   \n \nThe third approach is to compare the geometry of the representations of the words \ncomputed from the model and from the brain activity. Unlike the two previous approaches, this \nmethod does not necessarily assume a linear relationship between the brain and the model. \nGeometry is usually inferred through representational similarity analysis (Diedrichsen & \nKriegeskorte, 2017; Kriegeskorte, Mur, & Bandettini, 2008), which consists in computing the \nmatrix of dissimilarities between activity patterns elicited by each word. A matrix of \ndissimilarities is obtained for the brain activity and for the model. The test then consists in \ncomparing both matrices, usually by calculating rank-based correlations.  \n \nThis approach has been used to compare the evolution of the semantic representation \nduring the sentence presentation in both a BERT model and brain activity recorded by EEG \nand MEG (Lyu, Tyler, Fang, & Marslen-Wilson, 2021). Overall, it has been less used to study \nsentence comprehension compared to the two previous ones. This is surprising given that, at \nfirst glance, the estimation of the representational geometries is less computationally \ndemanding and requires less assumptions about the distribution of the data than the linear \nencoding approach (Diedrichsen & Kriegeskorte, 2017). One reason for the lack of research \nusing representational similarity analysis is the difficulty to build a reliable dissimilarity matrix \nin the context of natural language comprehension. In natural language, most of the words are \nnot repeated over the course of a text. This means that most words correspond to unique \ntrials, leading to very large and noisy dissimilarity matrices. Nonetheless, it should be noted \nthat, in practice, linear encoding models and representational similarity analysis leads to \nsimilar results (Thirion, Pedregosa, & Eickenberg, 2015).  \n \n \n \n \n9 \n \n \nFigure 1. Overview of the modelling and multivariate analytic tools to study brain activity during speech comprehension. \nBrain activity is recorded during the presentation of single words or sentences. Artificial neural networks trained on large text \ncorpora, such as LSTM or transformer networks, are used as models of natural language processing: their activity is also recorded \nduring the presentation of the same single words or sentences. Three approaches have been used to then compare brain activity \nwith the models. 1. Directly correlate brain activity with the model’s activity, for example with linear (ridge) regression. 2. Compare \nbehaviour and brain activity with metrics derived from the model’s outputs, such as surprise values. 3. Compare the geometry of \nthe representations extracted from the brain activity and the model’s activity by comparing the representational dissimilarity \nmatrices (RDM).  \n \n \nMapping detailed model embeddings onto coarse brain measures. \n \nArtificial neural networks usually learn separate embeddings for each word in the \nsentence rather than one fixed size sentence embedding. This causes a challenge whenever \na mapping onto a modality with coarser resolution is attempted. For example, the \nhemodynamic response recorded in fMRI data unfolds over several seconds and will inevitably \nspan multiple words given naturalistic stimulus presentation. There are multiple ways to \ncircumvent this issue. \n  \nWhen mapping language models onto fMRI data, most researchers have relied on \naveraging or concatenating word embeddings to match the resolution of the neural data \n(Anderson et al., 2021; Toneva, Mitchell, & Wehbe, 2020, 2022a; Wehbe, Vaswani, et al., \n2014). In recent transformer models, contextualised embeddings aggregate information on \npreceding word meaning, which makes it possible to simply rely on individual embeddings \nfrom the final hidden layer, such as the sentence final word (Schrimpf et al., 2021). Another \napproach specific to pretrained BERT transformers is to use an additional token embedding \nas an aggregate sentence embedding. BERT transformer models currently available are \ncommonly trained to infer whether one sentence is a plausible continuation of another. This \nsentence classification task requires a pair of sentences as input to the model. To indicate the \nseparation of a potential second sentence in the input, BERT requires each input sentence to \nbegin with the meaningless special token [CLS]. Just like every word, the embedding at the \nspecial token [CLS] position is getting increasingly mixed with the embeddings of the \nsubsequent words in higher-level layers of the model. As a result, the embedding at the [CLS] \nposition reflects an aggregate of the embedding of the words that constitutes the sentence. \nBoth the [CLS] token’s embedding or an aggregate measure of all word embeddings through \n \n10 \nmax-pooling have been used to model human data on sentence-level classification tasks such \nas judging semantic textual similarity or sentiment analysis (Devlin et al., 2018; Reimers & \nGurevych, 2019). However, token-specific embeddings often do not reach maximal \nperformance without further fine-tuning (Devlin et al., 2018; Reimers & Gurevych, 2019). \nOverall, the choice of how to integrate sentence embeddings is rarely explicitly motivated and \nonly few papers have directly compared how different approaches affect a model’s fit to brain \ndata.  \n \n \n11 \nII. \nThe brain represents words as low-dimensional dense vectors \nin a context-dependent fashion.  \n \n \nThe first contribution of the combined development of models of natural language \nprocessing and methods to compare the brain activity with these models is a precise \ninvestigation of the representational code of the words' meaning during sentence \ncomprehension.   \n \na. Word meaning representation as low-dimensional dense vectors \n \n \nHypotheses about the neural representation of words can be placed on a spectrum. \nOn one side, the word representations could be localised, with each word being encoded by \na specific population of neurons and no overlap between different words. On the other side, \nthe representation of the words could be distributed, with all words being encoded in the same \npopulation of neurons but with different patterns for each word. This second hypothesis has \nreceived a lot of attention, notably because of the success of the distributed representation in \nthe field of natural language processing. As a consequence, a lot of work has been done to \ncompare the dense vector representation used in natural language processing to the brain \nactivity.  \n \nAs previously cited, the work of Mitchell and collaborators in 2008 (Mitchell et al., 2008) \nwas the first to test the distributed representation hypothesis in the human brain. In this study, \nthe authors predicted fMRI activity using a linear combination of the vector representation of \nthe presented words. A strong implication of a successful linear mapping is that if the weights \nof the linear regression are diverse enough across all voxels and if the number of voxels is \nlarge enough, the brain activity potentially spans the entire embedding space of the model. \nThis would imply that the brain represented words in a similar fashion as the model, i.e. as \nlow-dimensional dense vectors distributed in space. Such a representation would allow for \ninterpolation, which is incompatible with a naive embedding, with one feature for every word. \nHowever, note that this implication is only probable and not guaranteed. Indeed, if the words \nin the embedding space are linearly separable from one another, a similar approach would \nsucceed even if the brain used the naive embedding, with one feature for each word. \nNonetheless, this scenario is improbable, because there are many more words than \ndimensions in the embedding space and thus many more configurations where most words \nare not linearly separable than configurations where they are.  \n \nFollowing this proof of concept, this result has been replicated and extended in later \nworks.  First, this result has been replicated using complete sentences rather than single \nwords (Anderson et al., 2019; Huth, de Heer, Griffiths, Theunissen, & Gallant, 2016; Millet & \nKing, 2021; Millet et al., 2022; Pereira et al., 2018; Wehbe, Murphy, et al., 2014). Second, the \ndistributed representation has been shown to be at least partly stable across individuals, as \nthe spatial localisation of the linear mapping between the word embedding and the fMRI data \nappears to be somewhat overlapping across individuals (Huth et al., 2016). Third, it has been \nshown that the distributed representation of the semantic information is encoded in different \ncortical areas and is independent from non-semantic features of the words. During visual \nstimulation, the encoding is independent from the visual appearance of the words (Wehbe, \nMurphy, et al., 2014). Similarly, during auditory stimulation, the encoding is independent from \n \n12 \nthe spectral and articulatory features of words (de Heer, Huth, Griffiths, Gallant, & Theunissen, \n2017). The encoding is also independent from the syntactic role of a word in the sentence \n(Wehbe, Murphy, et al., 2014). Indeed, multiple cortical regions represent semantic \ninformation about the words irrespective of their grammatical role, such as grammatical \nsubject or object (Anderson et al., 2019). Finally, the encoding of semantic information has \nbeen proposed to be partly modality independent, as the geometry of representation of visually \npresented words was correlated to those of auditorily presented words in the left pars \ntriangularis (Liuzzi et al., 2017). Recently, Popham and collaborators have shown that the \nsemantic representations of images and speech are aligned at the border of the visual cortex \n(Popham et al., 2021).  \n \n \nMost of these studies used linear regression to compare fMRI single voxel activity to \nthe vector representation of the words. However, this minimally proves that at least one \ndirection in the embedding space is encoded in the brain. One could argue that the brain may \nrepresent the language model’s embedding space only partly but not in its entirety. In the most \nextreme scenario, one could even argue that the voxels are actually all responding to the same \nsingle direction in the embedding space, and thus not at all making use of a dense low-\ndimensional representation of the words. In this case, an encoding model would have good \nperformances at predicting brain activity from the word embeddings but a decoding model \nwould have poor performances at predicting the word embeddings from brain activity. To \naddress this criticism, Goldstein and collaborators have successfully shown that information \nin the word embedding can be used to predict brain activity but also that brain activity can be \nused to predict the word embeddings (Goldstein, Dabush, et al., 2022; Goldstein, Zada, et al., \n2022). The linear mapping between the brain and the neural network performed relatively well \nin both directions, with a cross-validated correlation score of 0.15 for the encoding and a \nprediction score of 0.70 for the decoding in the IFG. The good performances in the two \ndirections with the same linear model suggest that the brain representations are not restricted \nto a small subset of the embedding space used by the natural language processing models. \nIndeed, if the geometric relationships between words were only very partly overlapping \nbetween the brain and the neural networks, such good cross-validated performances would \nbe unlikely. Another approach put forward to overcome the pitfalls of a unidirectional \ncorrelation measure when comparing models and brains is the “direct interface” test, which \nconsists of evaluating model performance after directly using the mapped brain activity in \nplace of a single layer activation in the neural network. This approach additionally ensures that \nany similarity in encoding between brain and networks are functionally relevant and not just \ndriven by spurious variance in the signal. It has been used in the context of object recognition \nin vision (Sexton & Love, 2022), but never in language comprehension. \n \nb. Word meaning representation in context \n \n \nThe previous part focused on models that use context-free embeddings, i.e. fixed word \nvectors that do not change depending on the surrounding context. However, context helps to \ndisambiguate the meaning of words and to assign them the correct semantic features. The \npolysemic word ”bank” refers to a financial institution in the context A: “I put my cash in the \nbank”, but it refers to a landscape in context B: “I walked by the river bank”. Consequently, the \nword vector representation of the word “bank” should not be the same in the two contexts. The \nsemantic features of “bank” in context A should be related to money and finance, while the \nsemantic features of “bank” in context B should be related to river and walking. Even for words \n \n13 \nwith unambiguous word sense, context often directs attention to a subset of semantic features \nof a word's meaning. For example, in a phrase like \"throwing a banana\", features relating to \nthe colour or taste of a banana are less important than its shape or weight. \n \n \n \nThe first study using contextualised word embedding was conducted by Wehbe and \ncollaborators (2014). The authors used a recurrent neural architecture to generate two vectors \nfor each word of the sentence, one representation of the context and one contextualised \nembedding of the word. Based on these two vectors they predicted MEG activity before, during \nand after the presentation of each word using a linear regression. Their results show that \nbefore the presentation of a word in a sentence context, the context representation predicts \nMEG activity. Shortly after the presentation of the word, the contextualised word \nrepresentation predicts MEG activity. Finally, after the presentation of the word, the updated \nrepresentation of the context predicts MEG activity.  This can be interpreted as evidence that \nthe brain represents the context as a latent variable, and integrates the meaning of the word \nto this context representation to produce a context-dependent embedding (Wehbe, Vaswani, \net al., 2014).  \n \n \nFurther, it has been shown and replicated multiple times that contextualised \nembeddings are better predictors of brain activity than context-free ones in large parts of the \nlanguage network. This effect has been shown in EEG and MEG recordings during the \npresentation of sentences with a verb-object noun relationship (Lyu et al., 2019) and in fMRI \nand MEG recordings during the presentation of narratives (Caucheteux & King, 2022; Toneva \n& Wehbe, 2019). In a recent article, Schrimpf and collaborators compared a large number of \nmodels, including models that take context into account or not, to brain data recorded during \nthe presentation of narratives. Contextual models were systematically associated with better \nfit to the brain data compared to context-free models, for different datasets and different \nrecording modalities, including fMRI, MEG and ECoG (Schrimpf et al., 2021). One surprising \nresult is that despite the apparent similarity between the brain and LSTM neural networks in \nterms of stimulus presentation (serial presentation) and computations (recurrent neural \narchitectures), transformers are better predictors of brain activity. More generally, the \nperformance of the neural networks in language tasks is a key predictor of their ability to match \nbrain activity (Caucheteux & King, 2022; Schrimpf et al., 2021). Predictability of brain activity \nhas been interpreted by some as suggesting a “convergence” in processes between models \nand the brain. This inference has also been criticised, however, because measures of \npredictive power are often averaged across inputs that vary in a large number of feature \ndimensions, therefore limiting its explanatory power  (Bowers et al., 2022). \n \nWhat explains the better fit of the contextualised representation? The advantage of \ncontextualised representation could be explained by contextual information improving word \nembeddings, for example because polysemic words are disambiguated. It could also be \nexplained by the fact that the contextualised embedding contains information about previous \nwords which could be used to account for variance in brain activity related to those previous \nwords. When systematically varying the amount of context that is used by the recurrent \nlanguage model, the better fit of the contextualised representation to the brain was reported \nto be due to both of those factors simultaneously (Jain & Huth, 2018). Also, words presented \nwith more context were associated with stronger fMRI activity and better encoding (Deniz, \nTseng, Wehbe, & Gallant, 2021).  \n \n \n14 \n \nFinally, several artificial neural networks have been shown to capture sentence \nmeaning beyond a simple \"sum of its parts\". As models of brain activity, they outperform \nalternative models that rely on simpler operations such as point-wise averages or \nconcatenation of context-free word embeddings, especially when distinguishing neural \nrepresentations evoked by closely related sentences (Anderson et al., 2021; Sun, Wang, \nZhang, & Zong, 2019). Moreover, the contextualised embeddings of recent language models \nseem to capture semantic information beyond individual word meaning, emerging through \ncoactivation with a combination of words, such as activating aspects of the verb \"reading\" \nwhen hearing the phrase \"the girl began the book\" or “eating” for the phrase “the goat finished \nthe book” (Toneva, Mitchell, & Wehbe, 2022b). \n \n \n \n15 \nIII. \nThe processing hierarchy within artificial and neural language \nmodels \n \n \nIt appears from the literature summarised in the previous part that words are encoded \nin distributed neural activation patterns that are well captured by artificial language models. \nHowever, the mere mapping between natural language processing models and neural \nrepresentations is not considered the end goal of this line of research. Through such a \nmapping, the hierarchical architecture of artificial neural networks can reveal a parallel \nfunctional hierarchy in the brain, similar to what has been demonstrated for the domain of \nvision (Cichy, Khosla, Pantazis, Torralba, & Oliva, 2016; Eickenberg, Gramfort, Varoquaux, & \nThirion, 2017; Güçlü & van Gerven, 2015). \n \na. Contrastive \napproach \nto \nlocalise \nbrain \nnetworks \ninvolved \nin \ncontextualisation processes \n \nFrom decades of neuroimaging research using mainly univariate analysis methods the \nlanguage community has collected a wealth of data and formulated detailed hypotheses about \nthe functional involvement of brain regions during sentence processing. We know that the left \nposterior and middle temporal cortex activates early after word onset and in a modality-\nindependent manner (Arana, Marquand, Hultén, Hagoort, & Schoffelen, 2020) with its \nactivation being strongly time-locked (Hultén, Schoffelen, Uddén, Lam, & Hagoort, 2019). This \npattern suggests a largely lexicalized, bottom-up process and hence this area is thought to \nencode lexical representations including lexicalized syntactic representations in posterior \nparts (Matchin, Brodbeck, Hammerly, & Lau, 2019; Nelson et al., 2017). More anterior regions \nof the cortex, including the anterior temporal lobe as well as the inferior frontal gyrus, are \nusually activated only subsequently but sustain activation for longer (Arana et al., 2020); \n(Brodbeck, Presacco, & Simon, 2018). Moreover, the inferior frontal gyrus has been shown to \nengage in functional coupling with the temporal cortex, likely enabling integration of multiple \nwords over longer time scales (Baggio & Hagoort, 2011; Hultén et al., 2019; Schoffelen et al., \n2017). This network of frontal and posterior temporal regions is also consistently activated \nduring tasks requiring semantic control (Solomon & Thompson-Schill, 2020), with damage to \nboth regions leading to similarly impaired semantic access in a variety of tasks including \nconceptual combination (Jackson, 2021; Jefferies & Lambon Ralph, 2006; Jefferies, 2013). \nThe anterior temporal lobe (ATL) has been suggested to be sensitive to both syntactic and \nsemantic features of a sentence (Rogalsky & Hickok, 2009) and to play a role in conceptual \ncombination (Pylkkänen, 2019; Zhang & Pylkkänen, 2015). In studies contrasting lists, \nphrases and sentences, graded effects in ATL have been reported in response to increasingly \nlarger contextual units (Matchin, Brodbeck, et al., 2019). At the same time, evidence from \nneuropsychological disorders and simulation studies suggest a causal role for ATL already at \nthe single-word level, specifically for tasks requiring semantic memory such as picture naming \n(Lambon Ralph, Jefferies, Patterson, & Rogers, 2017; Shimotake et al., 2015). ATL activation \nis thus unlikely to be restricted to combinatorial processing. Attempts to reconcile the \ndiscrepant findings regarding ATL function have pointed out a common sensitivity to \nconceptual specificity during both single and multi word processing (Westerlund & Pylkkänen, \n2014; Zhang & Pylkkänen, 2015). Finally, the temporal parietal junction, including angular \ngyrus, increases in activation at later time points in the sentence (Matchin, Brodbeck, et al., \n2019). Although the functional role of angular gyrus in sentence processing is still debated, \n \n16 \nthere is now accumulating evidence for its involvement in higher-level event-related \nprocessing (Binder & Desai, 2011; Branzi, Pobric, Jung, & Lambon Ralph, 2021; Leonardelli \n& Fairhall, 2022; Matchin, Liao, Gaston, & Lau, 2019). \n \nThe univariate results summarised above are based on either contrastive designs such \nas comparing lists and sentences, minimal word pairs with compositional and non \ncompositional meaning or on regressors that quantify structural complexity according to \nlinguistic parsing models. In contrast, deep neural networks can model hierarchical processing \nstages based on naturalistic language data without the need to rely on engineered task \ndesigns. Their hierarchical architecture is well-suited for capturing the gradient of \ncontextualization seen in the brain. Exploring the hierarchical gradients that emerge in artificial \nlanguage models may hence provide complementary insights into the computational hierarchy \nof the brain. \n \nb. A gradient of contextualisation  \n \nAs detailed in part II, context-dependent representations of the words have been \nshown to better model human brain data than context-free word embeddings. Moreover, \nstudies using the linear mapping approach in a spatially resolved manner have shown that \ndifferent timescales of contextualization may be associated with neural activity in distinct brain \nareas. This approach involves computing the voxel-wise fit for each voxel individually or group \nof voxels and comparing the fit across several language models that encode varying degrees \nof context. While multiple studies report a spatially varying gradient of contextualization in the \nbrain (Antonello, Turek, & Vo, 2021; Jain & Huth, 2018; Qian, Qiu, & Huang, 2016; Schmitt et \nal., 2021b; Toneva & Wehbe, 2019), they vary in terms of the exact mapping of this gradient \nonto brain areas. While some find highly contextualised representations to be restricted to \nanterior temporal lobe and posterior temporal lobe (Toneva et al., 2022a), other findings \nsuggest an inferior-superior gradient spanning temporal and inferior parietal cortex, \nemphasising angular gyrus and precuneus as processing information at longer contextual \nwindows (Antonello et al., 2021; Schmitt et al., 2021b). Again others suggest a combination of \nall of the above (Caucheteux & King, 2022).  \n \nEven though the deep learning models have the means to apply continuous measures \nof contextual richness, their conclusions with respect to linguistic function remain rather coarse \nin comparison to the collective insights stemming from univariate studies. The main distinction \nrelies on a binary split into brain areas preferring short or no context and brain areas preferring \nlong contexts. It remains to be explored whether the observed differences in neural \nrepresentations are sufficiently explained by this binary categorisation into non-contextual and \ncontextual meaning representations or whether we can identify intermediate stages of context-\ndependence. For example, can we identify meaningful definitions of distinct ‘chunks’ of \ncontext, other than the amount of characters that can distinguish sentences, paragraphs and \nnarratives?  \n \nOn top of a rather coarse localisation, the internal discrepancy in the findings makes it \nhard to reconcile with the previous literature. Namely, studies differ on the precise brain areas \nthat they report, spanning all four temporal, parietal, frontal and occipital lobes. Several factors \ncan explain the discrepancies in the reported brain localisation: \n \n \n17 \nFirst, the studies use different neuroimaging methods that have different profiles of \nspatial and temporal resolution. For example, MEG has millisecond temporal resolution, which \nis excellent to differentiate the neural response to consecutive words, but it has a relatively \npoor spatial resolution, which makes it inappropriate to differentiate activity in neighbouring \nneural populations. On the contrary, fMRI has a poor temporal resolution but millimetre spatial \nresolution, which makes it less appropriate to differentiate activity of words presented close in \ntime, as previously stated.  \n \nSecond, the fact that successful encoding of brain activity has been observed in \ndifferent brain areas could indicate that the neural code for word meaning is partly redundant \nor at least correlated across brain areas. In a neural network, the activity across successive \nlayers is correlated because the layers receive input from one another. Similarly, correlations \nbetween brain areas could emerge from the functional connectivity between regions, the \nactivity of one area being the input of another, or from the fact that two areas receive the  \nsame inputs but perform different computations.  \n \nFinally, the discrepancies could emerge from the variety of artificial neural networks \nthat have been used to study brain activity. These models vary in architecture, training data, \nand objective function. It is thus unclear to what extent these different models produce different \nword embeddings, and whether these differences are important when fitting brain activity. On \nthe one hand, the brain could represent a common part of the information shared across all \nmodels. On the other hand, some models might be better than others and this might depend \non the brain areas. For example, it has been shown that the geometry of word meaning \nrepresentations is not the same if it is computed from behavioural ratings of semantic features \nor from statistical information about its linguistic contexts. This difference is reflected in the \nbrain regions that encode each representation (Wang et al., 2018). To answer these \nquestions, a recent paper suggests that the variety of word embeddings used by the different \nneural networks actually span a meta-embedding space, whose main axes correspond to how \nmuch and how contextual information is taken into account (Antonello et al., 2021). This \nsuggests that a key difference between neural networks is how contextual information is \nintegrated in the word embeddings. This further suggests that contextual information might be \na key feature to explain brain activity in different brain areas. \n \nc. Partition of linguistic representations along the gradient \n \nA linguistically motivated processing hierarchy assumes increasingly abstract \nrepresentations of language input, spanning phonemes, words, phrases and full narratives. \nSuch a hierarchy emphasises the qualitative differences between what type of information is \nbeing integrated and is orthogonal to the previously discussed effect of context. In fact, context \ncan not only modulate representations at all linguistic levels but also lead to interactions \nbetween levels. For example, the probability of a word occurring given its context also shapes \npredictions about upcoming phonemes (Heilbron et al., 2020). Different levels of abstraction, \nas defined by linguistic theory, have been shown to be characterised by dissociable neural \nsignatures. To reveal such feature-specific neural markers, a language model’s predictions \ncan be used as tools to approximate a given feature's expectedness (Donhauser & Baillet, \n2020; Heilbron et al., 2020). \n \nA more direct approach to probe the information encoded in hidden representations of \n \n18 \nartificial neural networks has been taken in the field of natural language processing. NLP \nresearchers have applied a suite of tools, such as the analysis of a model’s predictions to \nspecific, carefully controlled sentences, linear decoders or probes, representational similarity \nanalysis and model ablation. These approaches provide evidence that artificial neural \nnetworks trained on language tasks develop a rich set of both semantic and syntactic \nknowledge (Rogers, Kovaleva, & Rumshisky, 2020; Wattenberg, Viegas, & Coenen, 2019), \nincluding hierarchical syntactic representations (Manning, Clark, Hewitt, Khandelwal, & Levy, \n2020), subject-predicate agreement (Gulordava, Aina, & Boleda, 2018), syntactic and \nsemantic roles, as well as semantic relations. \n \nMost recent model architectures consist of multiple layers of units, with transformations \nbeing implemented in the connections between layers as well as in additional attention heads. \nIf a hierarchical structure of representations should emerge from these models it should \ntherefore necessarily be constrained by their intrinsic architecture. On aggregate measures, \ni.e. across multiple sentences, it appears that different types of linguistic knowledge can be \npartly localised to specific network layers or groups thereof. Specifically, information regarding \nsyntactic structure seems to interact with degree of context along a hierarchy of layers, such \nthat deeper models are able to decode deeper parsing tree layers (Blevins, Levy, & \nZettlemoyer, 2018) and more high-level combinatorial information, such as coreference \n(Tenney, Das, & Pavlick, 2019). However, on a sentence-by-sentence basis, it can be noted \nthat any hierarchical order of layers is dynamically adjusted within transformer models. For \nexample, higher-level representations, such as semantic roles, may emerge earlier in the \nhierarchy if they are needed to disambiguate information at lower levels, such as  part-of-\nspeech (Tenney et al., 2019). This flexibility in how layers can temporarily reorganise the \nprocessing hierarchy essentially replaces the need for feed-back connections and is possible \ndue to transformer models selecting relevant context through self-attention, taking into account \nboth previous as well as following words. \n \nSeveral groups have explored whether representations emerging at different layers of \nthe language models vary in their capacity to map onto brain data. One emerging pattern \nacross studies is that middle layers map better onto brain activity as compared to early or late \noutput layers (Anderson et al., 2021; Caucheteux & King, 2022; Kell, Yamins, Shook, Norman-\nHaignere, & McDermott, 2018; Schrimpf et al., 2021; Thompson, Bengio, Formisano, & \nSchönwiesner, 2021; Toneva & Wehbe, 2019; Wehbe, Vaswani, et al., 2014). This is \ninteresting, given the machine learning results reviewed above, which suggest that middle and \nlate layers encode the most abstract linguistic information. Since these layers are closest to \nthe model output layer, they preserve the information that is most useful for the downstream \nlinguistic task the model is trained on (Rogers et al., 2020). This could also suggest that \nhumans and artificial neural networks systematically differ in the last layers because their \ncomputational goal is different. Indeed, on one side, artificial neural networks predict the \nupcoming word using only information present in the text. On the other side, humans \nsupposedly infer meaning by further integrating other sources of information coming from the \nexternal world through perception and action. It should be noted, however, that differences in \nencoding performance across layers are generally small. For example, Anderson and \ncolleagues report only a small drop from 76% to 70% rank performance score (50% signalling \nchance performance) in predicting brain activity when using different layers of BERT \n(Anderson et al., 2021).  \n \n \n19 \nd. Changes in representational geometry along the gradient \n \n Representations emerging in artificial neural networks are highly structured. While \nsome of this structure is driven by the architectural configuration of layers as discussed above, \nsome structure also emerges through learning and is due to task demands both within layer-\nspecific embeddings as well as within additional attention heads. The latter is not always \nexplicitly taken into account when using those structured representations to model brain data. \n \nOne meaningful low-dimensional vector representation of a set of stimuli is one in \nwhich those stimuli that belong to a common abstract category will cluster together in the \nembedding space. Such clusters, also referred to as manifolds, usually span only a subset of \nthe full embedding space, i.e. they are low-dimensional. Neural manifolds take on different \nshapes and dimensionalities in different language models, but some degree of clustering is \nobserved across all (Cai, Huang, Bian, & Church, 2020). For example, in the context of \nsupervised learning of visual classification tasks, artificial neural networks optimise the shape, \ndimensionality and distance of manifolds such that task-relevant abstract classes they \nrepresent become linearly separable to the best degree possible (Cohen, Chung, Lee, & \nSompolinsky, 2020). Similarly, a gradual compression of word manifolds can also be observed \nacross layers in trained artificial neural networks that process natural language. Increased \nlinear separability across layers has been reported for abstract classes like part-of-speech, \nentities and dependency depth, though, notably, these increases were restricted to items with \nambiguous word sense (Mamou et al., 2020). The concept of manifolds has great potential \nbecause it describes geometric objects that can be studied with precise mathematical tools. \nCharacterising the manifold geometry has inspired a range of metrics that quantify \ncomputationally relevant changes to neural representations. Data-driven approaches such as \nclustering and dimensionality reduction (e.g. decomposition by non-negative matrix \nfactorisation or PCA) are already being used to gain deeper insights into the brain’s functional \ntopology (Hamilton, Edwards, & Chang, 2018; Schoffelen et al., 2017). Leveraging the concept \nof linear separability of manifolds, mean field capacity analysis has been developed to quantify \nsubtle characteristics of the representational manifold such as its radius, dimensionality and \ncentre-correlation (Cohen et al., 2020).  \n \nFurthermore, a separation between semantic features and relational encoding has \nbeen observed in trained language models. For example, within the low-dimensional context \nembeddings, semantic and syntactic information begin to occupy orthogonal subspaces \n(Wattenberg et al., 2019). In transformer models, the introduction of the attention heads and \na separate position encoding for each word further reinforces such a separation. This allows \nthe model to learn flexible projections between relational information and semantic features. \nIndeed, in fully trained models, at least a subset of the attention heads seem to encode highly \nfocused dependency relations such as determiner-noun or object-verb relations (A. Clark, \n2013; K. Clark, Khandelwal, Levy, & Manning, 2019; Manning et al., 2020; Wattenberg et al., \n2019). Because information encoded across attention heads is more localised (a single \ndependency relation is usually represented by just one attention head) using attention heads \nas feature models can reveal a more fine-grained functional mapping across brain areas (S. \nKumar et al., 2022). The computational principles that underlie this factorisation of knowledge \nin transformer models have been linked to those in models of hippocampal functioning \n(Whittington, Warren, & Behrens, 2021) which learn a spatial code that is separate from the \nrepresentations of object identity in the environment. A similar point about separating relational \n \n20 \ncodes from feature codes has been made regarding the division of labour of ventral and dorsal \nstreams in scene perception. Specifically, low-dimensional manifolds representing relations \nbetween objects in a scene might serve the purpose of enabling transfer of abstract relational \ninformation (Summerfield, Luyckx, & Sheahan, 2020). \n \nClose inspection of the representational geometry within artificial neural networks \nallows us to extract more specific hypotheses about the underlying computational principles \n(Jazayeri & Ostojic, 2021). Having established that sentence embeddings captured in current \nlanguage models broadly map onto neural representations in the brain, we can now constrain \nthis mapping by explicitly taking into account the representational geometry evident in those \nlanguage models. Different stimulus materials might drive clustering of neural representations \nin slightly different ways, possibly leading to the discrepancies in identifying higher-level “high-\ncontext” brain areas. The reported redundancy in the neural code might be a consequence of \ndifferent brain areas encoding semantic information that corresponds to distinct subspaces of \nthe model embedding vector. \n \n \n \n21 \nIV. \nFuture directions towards interpretable process models of \nsentence comprehension in the human brain.  \n \n \nThe development of artificial neural networks to process natural language has two \nadvantages. First, they are trained end-to-end and thus provide objective measures that do \nnot depend on a priori assumptions concerning how language is structured and how humans \nare born with in-built knowledge of this structure. Second, they can process natural language \nand allow the analysis of brain data in the context of natural language comprehension. \nHowever, it remains unclear to what extent these models are to be taken as process models \nof the brain, i.e. to what extent they align on an algorithmic level to the brain and how well the \ncomputations that they realise are similar to the computations that the brain realises (Barsalou, \n2017). We identify three challenges that need to be solved to make these artificial neural \nnetworks good candidates as true process models of natural language comprehension in the \nbrain.  \n \na. Improving interpretability \n \nArtificial neural networks are sometimes depicted as “black box models”, because it is \nhard to describe how they work at an algorithmic level. However, some modelling approaches \nare harder to interpret than others. For example, word embeddings that directly rely on corpus \nco-occurrence statistics allow a direct interpretation of the coefficients of the regression, \nbecause each dimension transparently represents the co-occurrence with one other word in \nthe vocabulary (Huth et al., 2016; Popham et al., 2021). For example, a “river”-specific voxel \nwould have a large weight for “river” and a low weight for all other dimensions. On the contrary, \nlearned embeddings such as GPT-2 have arbitrary dimensions that are not directly \ninterpretable. Having a more transparent mapping between the algorithmic level and the input \nstimulus features does not guarantee high interpretability. In fact, it has been argued that due \nto the high-dimensional parameter space of complex neural systems the emerging solutions \nlikely rely on very complex nonlinear interactions between multiple features which might not \nneatly fit onto human-interpretable dimensions (Hasson, Nastase, & Goldstein, 2020). \nNonetheless, having a transparent mapping from network representations to stimulus features \nseems to be a prerequisite for identifying and mapping the emerging algorithmic properties \nonto our existing cognitive theories as well as brain networks. \n \nOn the methods side, some approaches result in more or less interpretable results. \nComparing brain activity with the model’s using linear regression is the most direct test but it \nyields little insight about how the brain works if the model itself is uninterpretable. Alternatively, \nusing metrics derived from the model’s output, such as surprise, can be more interpretable. \nFor example, Heilbron and collaborators computed interpretable metrics from the GPT-2 \nactivations, namely the phonemic, syntactic and semantic surprise associated with each word \n(Heilbron et al., 2020). The linear regression of these metrics on brain activity yields \ninterpretable results, e.g. here the fact that the brain is processing phonemic, syntactic and \nsemantic aspects in a hierarchical way, at different times and in different locations.  \n \nAnother way to improve the interpretability of the results is to analyse the model’s \nactivity prior to using it to fit brain activity. This involves having a better qualitative \nunderstanding of the different clusters of artificial neurons that compose the model. For \n \n22 \nexample, Caucheteux and collaborators (Caucheteux & Gramfort, 2021) clustered and \nanalysed the GPT-2 activations to disentangle syntactic from semantic composition. They \nwere then able to localise using fMRI the brain regions selectively associated with syntactic \nand semantic composition, beyond the mere representation of the individual syntactic or \nsemantic features of each word. We believe that the field should rely on this general approach \nmore heavily and could benefit from the tools that have been developed in the machine \nlearning community to probe representations within artificial neural networks, such as linear \ndecoders, representational similarity analysis and representational geometry analysis. \nIdentifying the subcomponents that compose the artificial neural networks and their specific \nrole will improve the interpretability and the insight gained from comparing representations \nbetween models and the brain. Previously, we discussed the effect of shallow versus deep \ncontext in artificial network representations. Beyond quantifying the number of words in the \ncontext, we are currently lacking a more qualitative description of what sort of information \nmight co-vary with context depth. For example, an intuitive distinction is the difference between \nadjacent and nonadjacent dependency relations. Can we identify further clusters apparent in \nartificial neural network representations that are modulated by contextual depth but not \nroutinely taken into account in current process models? \n \nFinally, a detailed algorithmic understanding might only be useful if a mapping onto the \nbrain can be established later on. Therefore, we do not want to argue that algorithmic \ninterpretability should be the sole focus going forward. The complementary objective of \nimproving the model-to-brain mapping will also be required. Some attempts at optimising a \nneural network for its similarity to the brain have reported increased task performance in both \nobject recognition and NLP tasks (Kubilius et al., 2019; Toneva & Wehbe, 2019).  \n \nb. Controlling for the relative contribution of the model and the linear \nregression to fit of the brain activity  \n \nThe activity of any large language model typically contains a lot of information about \nthe stimuli themselves. As the brain is responding to those stimuli, the models might fit the \nbrain simply because the stimuli themselves are helpful to predict brain activity. Indeed, there \nare two transformations playing a role in the fit: the non-linear encoding of the stimuli by the \nmodel and the (linear) regression between the model and the brain. A large part of the \npredictive power of these models could actually be due to the linear regression between the \nmodels and the brain activity. Indeed, with a sufficient number of parameters, the linear \nregression can learn the average brain activity of each trial. One way of measuring the relative \ncontribution of the model and the linear regression is to use artificial neural networks with \nrandom weights. This equalises the number of parameters fed to the regression and the \ninformation about the stimuli while removing what has been learned by the model during \ntraining. Several studies have run this control on fMRI data recorded during language \nprocessing (Kell et al., 2018; Millet & King, 2021; Millet et al., 2022; Schrimpf et al., 2021). \nThey confirm that trained models are better than untrained ones to explain brain activity. This \nproves that the predictive power of these models is at least partly related to the way they \nactually process the stimuli. However, there are two major caveats. First, the added value of \ntrained models over untrained ones is usually relatively small, ranging from less than 1% to \n53% in (Schrimpf et al., 2021). Second, it has been reported that random models with the \nsame architecture but a different number of layers can yield to large differences in predictive \npower, from 0.2 to 0.6 between different versions of GPT (Schrimpf et al., 2021). This calls for \n \n23 \ncaution when interpreting the results because it suggests that at least part of the predictive \npower of these models is explained by the linear regression and not necessary because they \nare good process models of the human brain.  \n \nc. Situation model \n \nOne last issue is how to place language comprehension in an integrated \ncomprehension system. Indeed, the artificial neural networks used as models of language \ncomprehension in the brain focus exclusively on language-internal tasks, e.g. next-word \nprediction tasks. These models never observe the situation in which the language is produced, \nand have no knowledge of the locutor, much less of any covert intentions. This is different \nfrom how humans process language.  \n \nFirst, it is known that the situation model plays an important role in language \ncomprehension. For example, it is almost always the case that language happens in a \ncommunicative context, where several streams of information in addition to the speech signal \nitself have to be taken into account. For example, pragmatic knowledge has to interface with \nthe semantic information provided by the words. The idea of separating pure semantics from \npragmatics has been heavily criticised, for example by Jackendoff (Jackendoff, 2003). Another \nexample are visual cues such as gestures that accompany speech production. Indeed, it is \nknown that gestures provide semantic information that is integrated with the speech \ninformation to form a coherent representation (Kelly, Ozyürek, & Maris, 2010; Vigliocco, \nPerniss, & Vinson, 2014; Willems, Ozyürek, & Hagoort, 2007). More generally speaking, the \ncurrent artificial networks only exploit the statistics of the texts, whereas another critical source \nof information is the statistics of the world. Indeed, sentence production and comprehension \nco-occurs with multiple sources of information, that includes all kinds of sensory information, \nsuch as non-verbal, visual and auditory percepts, so that word meaning is actually grounded \nin the external world. Such factors have not been taken into account so far by the artificial \nneural network modelling approach. However, it should be noted that there are in principle no \nlimitations to do so. Indeed, gestures can be fed as additional input features to the artificial \nneural networks. As such, they would provide contextual information that could be used by the \nartificial neural networks to produce better contextualised embeddings of the words.  \n \nSecond, another issue concerns the computational goal of the artificial neural networks \nthat process natural language, i.e. their objective function. Language is used by humans to \nconvey information about the world whereas the sole goal of the artificial neural networks is to \npredict masked words using the surrounding words. When presented with a descriptive text \nor speech, a human listener constructs a representation of the described situation (Zwaan & \nRadvansky, 1998). This representation helps support comprehension and helps drawing \ninferences about the situation. Take the example introduced by McClelland and collaborators \n(2020): “John spread jam on some bread. The knife had been dipped in poison.”. A human \nlistener might infer from this sentence that the jam was spread with a knife, that poison has \nbeen transferred to the bread, and that if John eats it, he may die (McClelland, Hill, Rudolph, \nBaldridge, & Schütze, 2020). This inference is based on a rich representation which \nincorporates prior world knowledge. It is unclear how to incorporate such knowledge in artificial \nneural networks. At the moment, these networks are mostly trained on masked word prediction \ntasks and are not forced to draw inferences on the external world. Recent attempts include \nincorporating a long-term memory integrated representational system (McClelland et al., \n \n24 \n2020), or training language models to recognize or even generate images (Ramesh, Pavlov, \nGoh, & Gray, 2021). It should nonetheless be noted that the word embedding produced by the \nartificial neural networks actually aligns to common human knowledge, such as information \nabout object features like size or shape (Grand, Blank, Pereira, & Fedorenko, 2022).  \n \n \n \n \n25 \nConclusion \n \nArtificial neural networks are promising tools to study brain activity during sentence \ncomprehension. They generate explicit instantiations of low-dimensional vector-based \nrepresentations of words. Once a model is sufficiently trained on a language corpus, these \nrepresentations can be extracted automatically and quickly from large natural language \nstimulus datasets. They therefore facilitate the analysis of neuroimaging data recorded during \nnatural language comprehension of sentences and narratives. Their success in predicting \nbrain data suggests that these representations can indeed be useful models of sentence-level \nmeaning representations. Furthermore, the representations generated by the most recent \nlanguage models are modulated by features relevant to sentence processing, such as context \ndepth. \n \nNonetheless, the insights into the functional hierarchy of the brain network for language \nthat  deep learning models provide are currently very coarse, which makes it difficult to \ncomplement current hypotheses about cognitive process models. The main limitations are \ntheir interpretability and their ability to be good process models of the brain. In particular, most \nmodels are hard to interpret, they consist of word embedding with arbitrary dimensions that \ndo not map onto simple concepts or simple syntactic roles. Furthermore, most models are \ntrained on next-word prediction tasks, without any reference to the external world and without \na communicative function. It is also currently unclear to what extent the predictive power is \nexplained by the regression only and not so much by the ability of the model to capture brain \nprocesses.  \n \nIn order to make the most out of these models, we suggest that future directions could \ninvolve: (1) more constrained tests when directly comparing models activation and brain \nactivity using regression methods, (2) better descriptions of the model's internal \nrepresentations, i.e. what information is clustered and what information is factorised. One good \nstarting point would be to clarify which aspects of a sentence’s meaning is modulated by \ncontext, beyond a binary dichotomy between single words and words in context. Here, \nlanguage researchers can borrow techniques from the machine learning literature on deep \nlearning models, where multiple analysis techniques have been explored to provide insight \ninto the inner workings of recent deep language models. \n \n \n \n \n26 \nReferences \n \nAnderson, A. J., Kiela, D., Binder, J. R., Fernandino, L., Humphries, C. J., Conant, L. L., \nRaizada, R. D. S., et al. (2021). Deep Artificial Neural Networks Reveal a Distributed \nCortical Network Encoding Propositional Sentence-Level Meaning. The Journal of \nNeuroscience, 41(18), 4100–4119. \nAnderson, A. J., Lalor, E. C., Lin, F., Binder, J. R., Fernandino, L., Humphries, C. J., Conant, \nL. L., et al. (2019). Multiple regions of a cortical network commonly encode the meaning \nof words in multiple grammatical positions of read sentences. Cerebral Cortex, 29(6), \n2396–2411. \nAntonello, R., Turek, J. S., & Vo, V. (2021). Low-Dimensional Structure in the Space of \nLanguage Representations is Reflected in Brain Responses. Advances in Neural \nInformation Processing Systems, 34. \nArana, S., Marquand, A., Hultén, A., Hagoort, P., & Schoffelen, J.-M. (2020). Sensory \nModality-Independent Activation of the Brain Network for Language. The Journal of \nNeuroscience, 40(14), 2914–2924. \nBaggio, G., & Hagoort, P. (2011). The balance between memory and unification in semantics: \nA dynamic account of the N400. Language and cognitive processes, 26(9), 1338–1367. \nBarsalou, L. W. (2017). What does semantic tiling of the cortex tell us about semantics? \nNeuropsychologia, 105, 18–38. \nBinder, J. R., & Desai, R. H. (2011). The neurobiology of semantic memory. Trends in \nCognitive Sciences, 15(11), 527–536. \nBlevins, T., Levy, O., & Zettlemoyer, L. (2018). Deep rnns encode soft hierarchical syntax. \nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics \n(Volume 2: Short Papers) (pp. 14–19). Presented at the Proceedings of the 56th Annual \nMeeting of the Association for Computational Linguistics (Volume 2: Short Papers), \nStroudsburg, PA, USA: Association for Computational Linguistics. \nBowers, J. S., Malhotra, G., Dujmović, M., Montero, M. L., Tsvetkov, C., Biscione, V., Puebla, \nG., et al. (2022). Deep Problems with Neural Network Models of Human Vision. \nBehavioral and Brain Sciences, 1–74. \nBranzi, F. M., Pobric, G., Jung, J., & Lambon Ralph, M. A. (2021). The Left Angular Gyrus Is \nCausally Involved in Context-dependent Integration and Associative Encoding during \nNarrative Reading. Journal of Cognitive Neuroscience, 1–14. \nBrennan, J. R., Dyer, C., Kuncoro, A., & Hale, J. T. (2020). Localizing syntactic predictions \nusing recurrent neural network grammars. Neuropsychologia, 146, 107479. \nBrodbeck, C., Presacco, A., & Simon, J. Z. (2018). Neural source dynamics of brain responses \nto continuous stimuli: Speech processing from acoustics to comprehension. Neuroimage, \n172, 162–174. \n \n27 \nBrown, T., Mann, B., & Ryder, N. (2020). Language models are few-shot learners. Advances \nin neural information processing systems, 33, 1877–1901. \nCai, X., Huang, J., Bian, Y., & Church, K. (2020). Isotropy in the contextual embedding space: \nClusters and manifolds. International Conference on Learning Representations. \nCaucheteux, C., & Gramfort, A. (2021). Disentangling syntax and semantics in the brain with \ndeep networks. Machine learning : proceedings of the International Conference. \nInternational Conference on Machine Learning. \nCaucheteux, C., & King, J.-R. (2022). Brains and algorithms partially converge in natural \nlanguage processing. Communications Biology, 5(1), 134. \nCichy, R. M., Khosla, A., Pantazis, D., Torralba, A., & Oliva, A. (2016). Comparison of deep \nneural networks to spatio-temporal cortical dynamics of human visual object recognition \nreveals hierarchical correspondence. Scientific Reports, 6, 27755. \nClark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive \nscience. Behavioral and Brain Sciences, 36(3), 181–204. \nClark, K., Khandelwal, U., Levy, O., & Manning, C. D. (2019). What does BERT look at? an \nanalysis of bert’s attention. Proceedings of the 2019 ACL Workshop BlackboxNLP: \nAnalyzing and Interpreting Neural Networks for NLP (pp. 276–286). Stroudsburg, PA, \nUSA: Association for Computational Linguistics. \nCohen, U., Chung, S., Lee, D. D., & Sompolinsky, H. (2020). Separability and geometry of \nobject manifolds in deep neural networks. Nature Communications, 11(1), 746. \nDeniz, F., Tseng, C., Wehbe, L., & Gallant, J. L. (2021). Semantic representations during \nlanguage comprehension are affected by context. BioRxiv. \nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep \nBidirectional Transformers for Language Understanding. arXiv. \nDiedrichsen, J., & Kriegeskorte, N. (2017). Representational models: A common framework \nfor understanding encoding, pattern-component, and representational-similarity analysis. \nPLoS Computational Biology, 13(4), e1005508. \nDonhauser, P. W., & Baillet, S. (2020). Two distinct neural timescales for predictive speech \nprocessing. Neuron, 105(2), 385-393.e9. \nEickenberg, M., Gramfort, A., Varoquaux, G., & Thirion, B. (2017). Seeing it all: Convolutional \nnetwork layers map the function of the human visual system. Neuroimage, 152, 184–194. \nFirth, J. R. (1957). A synopsis of linguistic theory, 1930-1955. Studies in linguistic analysis. \nGhosh, S., Vinyals, O., Strope, B., Roy, S., Dean, T., & Heck, L. (2016). Contextual LSTM \n(CLSTM) models for Large scale NLP tasks. arXiv. \nGoldberg, Y. (2016). A primer on neural network models for natural language processing. \nJournal of Artificial Intelligence Research, 57, 345–420. \nGoldstein, A., Dabush, A., Aubrey, B., Schain, M., Nastase, S. A., Zada, Z., Ham, E., et al. \n \n28 \n(2022). Brain embeddings with shared geometry to artificial contextual embeddings, as a \ncode for representing language in the human brain. BioRxiv. \nGoldstein, A., Zada, Z., Buchnik, E., Schain, M., Price, A., Aubrey, B., Nastase, S. A., et al. \n(2022). Shared computational principles for language processing in humans and deep \nlanguage models. Nature Neuroscience, 25(3), 369–380. \nGrand, G., Blank, I. A., Pereira, F., & Fedorenko, E. (2022). Semantic projection recovers rich \nhuman knowledge of multiple object features from word embeddings. Nature Human \nBehaviour. \nGüçlü, U., & van Gerven, M. A. J. (2015). Deep Neural Networks Reveal a Gradient in the \nComplexity of Neural Representations across the Ventral Stream. The Journal of \nNeuroscience, 35(27), 10005–10014. \nGuest, O., & Martin, A. E. (2021). On logical inference over brains, behaviour, and artificial \nneural networks. \nGulordava, K., Aina, L., & Boleda, G. (2018). How to represent a word and predict it, too: \nImproving tied architectures for language modelling. Proceedings of the 2018 Conference \non Empirical Methods in Natural Language Processing (pp. 2936–2941). Presented at \nthe Proceedings of the 2018 Conference on Empirical Methods in Natural Language \nProcessing, Stroudsburg, PA, USA: Association for Computational Linguistics. \nHamilton, L. S., Edwards, E., & Chang, E. F. (2018). A spatial map of onset and sustained \nresponses to speech in the human superior temporal gyrus. Current Biology, 28(12), \n1860-1871.e4. \nHarris, Z. S. (1954). Distributional Structure. WORD, 10(2–3), 146–162. \nHasson, U., Nastase, S. A., & Goldstein, A. (2020). Direct fit to nature: an evolutionary \nperspective on biological and artificial neural networks. Neuron, 105(3), 416–434. \nde Heer, W. A., Huth, A. G., Griffiths, T. L., Gallant, J. L., & Theunissen, F. E. (2017). The \nhierarchical cortical organization of human speech processing. The Journal of \nNeuroscience, 37(27), 6539–6557. \nHeilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P., & de Lange, F. P. (2020). A hierarchy \nof linguistic predictions during natural language comprehension. BioRxiv. \nHinton, G. E. (1986). Learning distributed representations of concepts. Proceedings of the \neighth annual conference of the cognitive science society, 1, 12. \nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), \n1735–1780. \nHultén, A., Schoffelen, J.-M., Uddén, J., Lam, N. H. L., & Hagoort, P. (2019). How the brain \nmakes sense beyond the processing of single words - An MEG study. Neuroimage, 186, \n586–594. \nHuth, A. G., de Heer, W. A., Griffiths, T. L., Theunissen, F. E., & Gallant, J. L. (2016). Natural \nspeech reveals the semantic maps that tile human cerebral cortex. Nature, 532(7600), \n \n29 \n453–458. \nItti, L., & Baldi, P. (2009). Bayesian surprise attracts human attention. Vision Research, 49(10), \n1295–1306. \nJackendoff, R. (2003). Précis of Foundations of language: brain, meaning, grammar, \nevolution. Behavioral and Brain Sciences, 26(6), 651–65; discussion 666. \nJackson, R. L. (2021). The neural correlates of semantic control revisited. Neuroimage, 224, \n117444. \nJain, S., & Huth, A. (2018). Incorporating Context into Language Encoding Models for fMRI. \nBioRxiv, 6629–6638. \nJazayeri, M., & Ostojic, S. (2021). Interpreting neural computations by examining intrinsic and \nembedding dimensionality of neural activity. Current Opinion in Neurobiology, 70, 113–\n120. \nJefferies, E., & Lambon Ralph, M. A. (2006). Semantic impairment in stroke aphasia versus \nsemantic dementia: a case-series comparison. Brain: A Journal of Neurology, 129(Pt 8), \n2132–2147. \nJefferies, E. (2013). The neural basis of semantic cognition: converging evidence from \nneuropsychology, neuroimaging and TMS. Cortex, 49(3), 611–625. \nJonas, E., & Kording, K. P. (2017). Could a neuroscientist understand a microprocessor? \nPLoS Computational Biology, 13(1), e1005268. \nKell, A. J. E., Yamins, D. L. K., Shook, E. N., Norman-Haignere, S. V., & McDermott, J. H. \n(2018). A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts \nBrain Responses, and Reveals a Cortical Processing Hierarchy. Neuron, 98(3), 630-\n644.e16. \nKelly, S. D., Ozyürek, A., & Maris, E. (2010). Two sides of the same coin: speech and gesture \nmutually interact to enhance comprehension. Psychological Science, 21(2), 260–267. \nKriegeskorte, N., Mur, M., & Bandettini, P. (2008). Representational similarity analysis - \nconnecting the branches of systems neuroscience. Frontiers in Systems Neuroscience, \n2, 4. \nKubilius, J., Schrimpf, M., Kar, K., Rajalingham, R., Hong, H., Majaj, N., Issa, E., et al. (2019). \nBrain-Like Object Recognition with High-Performing Shallow Recurrent ANNs. Advances \nin Neural Information Processing Systems. \nKumar, M., Goldstein, A., Michelmann, S., Zacks, J. M., Hasson, U., & Norman, K. A. (2022). \nBayesian surprise predicts human event segmentation in story listening. \nKumar, S., Sumers, T. R., Yamakoshi, T., Goldstein, A., Hasson, U., Norman, K. A., Griffiths, \nT. L., et al. (2022). Reconstructing the cascade of language processing in the brain using \nthe internal computations of a transformer-based language model. BioRxiv. \nLambon Ralph, M. A., Jefferies, E., Patterson, K., & Rogers, T. T. (2017). The neural and \n \n30 \ncomputational bases of semantic cognition. Nature Reviews. Neuroscience, 18(1), 42–\n55. \nLeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436–444. \nLeonardelli, E., & Fairhall, S. L. (2022). Similarity-based fMRI-MEG fusion reveals hierarchical \norganisation within the brain’s semantic system. Neuroimage, 259, 119405. \nLiuzzi, A. G., Bruffaerts, R., Peeters, R., Adamczuk, K., Keuleers, E., De Deyne, S., Storms, \nG., et al. (2017). Cross-modal representation of spoken and written word meaning in left \npars triangularis. Neuroimage, 150, 292–307. \nLyu, B., Choi, H. S., Marslen-Wilson, W. D., Clarke, A., Randall, B., & Tyler, L. K. (2019). \nNeural dynamics of semantic composition. Proceedings of the National Academy of \nSciences of the United States of America, 116(42), 21318–21327. \nLyu, B., Tyler, L. K., Fang, Y., & Marslen-Wilson, W. D. (2021). Humans, machines, and \nlanguage: A deep alignment in underlying computational styles? BioRxiv. \nMamou, J., Le, H., Del Rio, M., Stephenson, C., Tang, H., Kim, Y., & Chung, S. (2020). \nEmergence of Separable Manifolds in Deep Language Representations. arXiv. \nManning, C. D., Clark, K., Hewitt, J., Khandelwal, U., & Levy, O. (2020). Emergent linguistic \nstructure in artificial neural networks trained by self-supervision. Proceedings of the \nNational Academy of Sciences of the United States of America, 117(48), 30046–30054. \nMars, R. B., Debener, S., Gladwin, T. E., Harrison, L. M., Haggard, P., Rothwell, J. C., & \nBestmann, S. (2008). Trial-by-trial fluctuations in the event-related electroencephalogram \nreflect dynamic changes in the degree of surprise. The Journal of Neuroscience, 28(47), \n12539–12545. \nMatchin, W., Brodbeck, C., Hammerly, C., & Lau, E. (2019). The temporal dynamics of \nstructure and content in sentence comprehension: Evidence from fMRI-constrained MEG. \nHuman Brain Mapping, 40(2), 663–678. \nMatchin, W., Liao, C.-H., Gaston, P., & Lau, E. (2019). Same words, different structures: An \nfMRI investigation of argument relations and the angular gyrus. Neuropsychologia, 125, \n116–128. \nMcClelland, J. L., Hill, F., Rudolph, M., Baldridge, J., & Schütze, H. (2020). Placing language \nin an integrated understanding system: Next steps toward human-level performance in \nneural language models. Proceedings of the National Academy of Sciences of the United \nStates of America, 117(42), 25966–25974. \nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word \nRepresentations in Vector Space. arXiv. \nMikolov, T., Karafiát, M., Burget, L., Černocký, J., & Khudanpur, S. (2010). Recurrent neural \nnetwork based language model. Interspeech 2010 (pp. 1045–1048). Presented at the \nInterspeech 2010, ISCA: ISCA. \nMillet, J., Caucheteux, C., Orhan, P., Boubenec, Y., Gramfort, A., Dunbar, E., Pallier, C., et al. \n \n31 \n(2022). Toward a realistic model of speech processing in the brain with self-supervised \nlearning. arXiv. \nMillet, J., & King, J.-R. (2021). Inductive biases, pretraining and fine-tuning jointly account for \nbrain responses to speech. \nMitchell, T. M., Shinkareva, S. V., Carlson, A., Chang, K.-M., Malave, V. L., Mason, R. A., & \nJust, M. A. (2008). Predicting human brain activity associated with the meanings of nouns. \nScience, 320(5880), 1191–1195. \nNelson, M. J., El Karoui, I., Giber, K., Yang, X., Cohen, L., Koopman, H., Cash, S. S., et al. \n(2017). Neurophysiological dynamics of phrase-structure building during sentence \nprocessing. Proceedings of the National Academy of Sciences of the United States of \nAmerica, 114(18), E3669–E3678. \nPereira, F., Lou, B., Pritchett, B., Ritter, S., Gershman, S. J., Kanwisher, N., Botvinick, M., et \nal. (2018). Toward a universal decoder of linguistic meaning from brain activation. Nature \nCommunications, 9(1), 963. \nPopham, S. F., Huth, A. G., Bilenko, N. Y., Deniz, F., Gao, J. S., Nunez-Elizalde, A. O., & \nGallant, J. L. (2021). Visual and linguistic semantic representations are aligned at the \nborder of human visual cortex. Nature Neuroscience, 24(11), 1628–1636. \nPylkkänen, L. (2019). The neural basis of combinatory syntax and semantics. Science, \n366(6461), 62–66. \nQian, P., Qiu, X., & Huang, X. (2016). Bridging LSTM Architecture and the Neural Dynamics \nduring Reading. arXiv. \nRabovsky, M., Hansen, S. S., & McClelland, J. L. (2018). Modelling the N400 brain potential \nas change in a probabilistic representation of meaning. Nature Human Behaviour, 2(9), \n693–705. \nRadford, A., Wu, J., Child, R., Luan, D., & Amodei, D. (2019). Language models are \nunsupervised multitask learners. OpenAI blog. \nRamesh, A., Pavlov, M., Goh, G., & Gray, S. (2021). Zero-shot text-to-image generation. \nMachine learning : proceedings of the International Conference. International Conference \non Machine Learning, 8821–8831. \nReimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese \nBERT-Networks. Proceedings of the 2019 Conference on Empirical Methods in Natural \nLanguage Processing and the 9th International Joint Conference on Natural Language \nProcessing (EMNLP-IJCNLP) (pp. 3973–3983). Presented at the Proceedings of the \n2019 Conference on Empirical Methods in Natural Language Processing and the 9th \nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), \nStroudsburg, PA, USA: Association for Computational Linguistics. \nRogalsky, C., & Hickok, G. (2009). Selective attention to semantic and syntactic features \nmodulates sentence processing networks in anterior temporal cortex. Cerebral Cortex, \n19(4), 786–796. \n \n32 \nRogers, A., Kovaleva, O., & Rumshisky, A. (2020). A primer in bertology: what we know about \nhow BERT works. Transactions of the Association for Computational Linguistics, 8, 842–\n866. \nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-\npropagating errors. Nature, 323(6088), 533–536. \nSchmitt, L.-M., Erb, J., Tune, S., Rysop, A. U., Hartwigsen, G., & Obleser, J. (2021a). \nPredicting speech from a cortical hierarchy of event-based time scales. Science \nAdvances, 7(49), eabi6070. \nSchmitt, L.-M., Erb, J., Tune, S., Rysop, A. U., Hartwigsen, G., & Obleser, J. (2021b). \nPredicting speech from a cortical hierarchy of event-based time scales. Science \nAdvances, 7(49), eabi6070. \nSchoffelen, J. M., Hultén, A., Lam, N., Marquand, A. F., Uddén, J., & Hagoort, P. (2017). \nFrequency-specific directed interactions in the human brain network for language. \nProceedings of the National Academy of Sciences of the United States of America, \n114(30), 8083–8088. \nSchrimpf, M., Blank, I. A., Tuckute, G., Kauf, C., Hosseini, E. A., Kanwisher, N., Tenenbaum, \nJ. B., et al. (2021). The neural architecture of language: Integrative modeling converges \non predictive processing. Proceedings of the National Academy of Sciences of the United \nStates of America, 118(45). \nSexton, N. J., & Love, B. C. (2022). Reassessing hierarchical correspondences between brain \nand deep networks through direct interface. Sci. Adv., 8(28). \nShannon, C. E. (1948). A mathematical theory of communication. Bell System Technical \nJournal, 27(3), 379–423. \nShimotake, A., Matsumoto, R., Ueno, T., Kunieda, T., Saito, S., Hoffman, P., Kikuchi, T., et al. \n(2015). Direct exploration of the role of the ventral anterior temporal lobe in semantic \nmemory: cortical stimulation and local field potential evidence from subdural grid \nelectrodes. Cerebral Cortex, 25(10), 3802–3817. \nSolomon, S. H., & Thompson-Schill, S. L. (2020). Feature uncertainty predicts behavioral and \nneural responses to combined concepts. The Journal of Neuroscience, 40(25), 4900–\n4912. \nSummerfield, C., Luyckx, F., & Sheahan, H. (2020). Structure learning and the posterior \nparietal cortex. Progress in Neurobiology, 184, 101717. \nSun, J., Wang, S., Zhang, J., & Zong, C. (2019). Towards Sentence-Level Brain Decoding \nwith Distributed Representations. Proceedings of the AAAI Conference on Artificial \nIntelligence, 33, 7047–7054. \nSundermeyer, M., Schlüter, R., & Ney, H. (2012). LSTM neural networks for language \nmodeling. Interspeech 2012 (pp. 194–197). Presented at the Interspeech 2012, ISCA: \nISCA. \nTenney, I., Das, D., & Pavlick, E. (2019). BERT rediscovers the classical NLP pipeline. \n \n33 \nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics \n(pp. 4593–4601). Presented at the Proceedings of the 57th Annual Meeting of the \nAssociation for Computational Linguistics, Stroudsburg, PA, USA: Association for \nComputational Linguistics. \nThirion, B., Pedregosa, F., & Eickenberg, M. (2015). Correlations of correlations are not \nreliable statistics: implications for multivariate pattern analysis. ICML Workshop on …. \nThompson, J. A., Bengio, Y., Formisano, E., & Schönwiesner, M. (2021). Training neural \nnetworks to recognize speech increased their correspondence to the human auditory \npathway but did not yield a shared hierarchy of acoustic features. BioRxiv. \nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal \nStatistical Society: Series B (Methodological), 58(1), 267–288. \nTikhonov, A. N. (1963). On the solution of ill-posed problems and the method of regularization. \nDoklady akademii nauk. \nToneva, M., Mitchell, T. M., & Wehbe, L. (2020). The meaning that emerges from combining \nwords is robustly localizable in space but not in time. BioRxiv. \nToneva, M., Mitchell, T. M., & Wehbe, L. (2022a). Combining computational controls with \nnatural text reveals aspects of meaning composition. Nature Computational Science, \n2(11), 745–757. \nToneva, M., Mitchell, T. M., & Wehbe, L. (2022b). Combining computational controls with \nnatural text reveals aspects of meaning composition. Nature Computational Science, \n2(11), 745–757. \nToneva, M., & Wehbe, L. (2019). Interpreting and improving natural-language processing (in \nmachines) with natural language-processing (in the brain). Advances in neural \ninformation processing systems, 32. \nVaswani, A., Shazeer, N., & Parmar, N. (2017). Attention is all you need. Advances in neural \ninformation processing systems. \nVigliocco, G., Perniss, P., & Vinson, D. (2014). Language as a multimodal phenomenon: \nimplications for language learning, processing and evolution. Philosophical Transactions \nof the Royal Society of London. Series B, Biological Sciences, 369(1651), 20130292. \nWang, X., Wu, W., Ling, Z., Xu, Y., Fang, Y., Wang, X., Binder, J. R., et al. (2018). \nOrganizational principles of abstract words in the human brain. Cerebral Cortex, 28(12), \n4305–4318. \nWattenberg, M., Viegas, F. B., & Coenen, A. (2019). Visualizing and measuring the geometry \nof BERT. Advances in neural information processing systems, 32. \nWehbe, L., Murphy, B., Talukdar, P., Fyshe, A., Ramdas, A., & Mitchell, T. (2014). \nSimultaneously uncovering the patterns of brain regions involved in different story reading \nsubprocesses. Plos One, 9(11), e112575. \nWehbe, L., Vaswani, A., Knight, K., & Mitchell, T. (2014). Aligning context-based statistical \n \n34 \nmodels of language with brain activity during reading. Proceedings of the 2014 \nConference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 233–\n243). Presented at the Proceedings of the 2014 Conference on Empirical Methods in \nNatural Language Processing (EMNLP), Stroudsburg, PA, USA: Association for \nComputational Linguistics. \nWesterlund, M., & Pylkkänen, L. (2014). The role of the left anterior temporal lobe in semantic \ncomposition vs. semantic memory. Neuropsychologia, 57, 59–70. \nWhittington, J. C. R., Warren, J., & Behrens, T. E. J. (2021). Relating transformers to models \nand neural representations of the hippocampal formation. arXiv. \nWillems, R. M., Ozyürek, A., & Hagoort, P. (2007). When language meets action: the neural \nintegration of gesture and speech. Cerebral Cortex, 17(10), 2322–2333. \nZhang, L., & Pylkkänen, L. (2015). The interplay of composition and concept specificity in the \nleft anterior temporal lobe: an MEG study. Neuroimage, 111, 228–240. \nZwaan, R. A., & Radvansky, G. A. (1998). Situation models in language comprehension and \nmemory. Psychological Bulletin, 123(2), 162–185. \n",
  "categories": [
    "cs.CL",
    "q-bio.NC"
  ],
  "published": "2023-01-16",
  "updated": "2023-02-16"
}