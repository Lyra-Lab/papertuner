{
  "id": "http://arxiv.org/abs/2301.06527v1",
  "title": "XNLI 2.0: Improving XNLI dataset and performance on Cross Lingual Understanding (XLU)",
  "authors": [
    "Ankit Kumar Upadhyay",
    "Harsit Kumar Upadhya"
  ],
  "abstract": "Natural Language Processing systems are heavily dependent on the availability\nof annotated data to train practical models. Primarily, models are trained on\nEnglish datasets. In recent times, significant advances have been made in\nmultilingual understanding due to the steeply increasing necessity of working\nin different languages. One of the points that stands out is that since there\nare now so many pre-trained multilingual models, we can utilize them for\ncross-lingual understanding tasks. Using cross-lingual understanding and\nNatural Language Inference, it is possible to train models whose applications\nextend beyond the training language. We can leverage the power of machine\ntranslation to skip the tiresome part of translating datasets from one language\nto another. In this work, we focus on improving the original XNLI dataset by\nre-translating the MNLI dataset in all of the 14 different languages present in\nXNLI, including the test and dev sets of XNLI using Google Translate. We also\nperform experiments by training models in all 15 languages and analyzing their\nperformance on the task of natural language inference. We then expand our\nboundary to investigate if we could improve performance in low-resource\nlanguages such as Swahili and Urdu by training models in languages other than\nEnglish.",
  "text": "XNLI 2.0: Improving XNLI dataset and\nperformance on Cross Lingual Understanding\n(XLU)\nAnkit Kumar Upadhyay\nDept. of Computer Science and Engineering\nJSS Academy of Technical Education\nBengaluru, India\nankitkupadhyay@jssateb.ac.in\nHarsit Kumar Upadhya\nDept. of Information Science and Engineering\nJSS Academy of Technical Education\nBengaluru, India\n1js19is039@jssateb.ac.in\nAbstract—Natural Language Processing systems are heavily\ndependent on the availability of annotated data to train practical\nmodels. Primarily, models are trained on English datasets. In\nrecent times, significant advances have been made in multilingual\nunderstanding due to the steeply increasing necessity of working\nin different languages. One of the points that stands out is that\nsince there are now so many pre-trained multilingual models,\nwe can utilize them for cross-lingual understanding tasks. Using\ncross-lingual understanding and Natural Language Inference, it\nis possible to train models whose applications extend beyond\nthe training language. We can leverage the power of machine\ntranslation to skip the tiresome part of translating datasets from\none language to another. In this work, we focus on improving\nthe original XNLI dataset by re-translating the MNLI dataset\nin all of the 14 different languages present in XNLI, including\nthe test and dev sets of XNLI using Google Translate. We also\nperform experiments by training models in all 15 languages and\nanalyzing their performance on the task of natural language\ninference. We then expand our boundary to investigate if we\ncould improve performance in low-resource languages such as\nSwahili and Urdu by training models in languages other than\nEnglish.\nIndex\nTerms—Cross-lingual\nNatural\nLanguage\nInference\n(XNLI), Natural Language Understanding (NLU), transfer learn-\ning, Natural Language Processing, Multi-Genre Natural Lan-\nguage Inference (MNLI), low-resource languages, machine-\ntranslation\nI. INTRODUCTION\nNatural Language Processing systems typically rely on\nannotated data in one language on which it is trained and\nperforms the task in that language only. As the world is\nglobalized and the products developed in one country are being\nshipped across boundaries, it has become essential that our\nlanguage system not only understands the language on which\nit is trained but also other languages of the user that interacts\nwith the product. Often, it is not possible to annotate data in\nall possible languages.\nThrough Cross-Lingual Language Understanding (XLU), it\nis now possible to transfer learning from one natural language\nto another, especially when we do not have much data avail-\nable for the target language. This paper focuses on Natural\nLanguage Inference (NLI), in which we are given a pair of\nsentences named premise and hypothesis. The system has to\npredict if the hypothesis entails the premise, contradicts it or is\nneutral. We make use of the Cross-lingual Natural Language\nInference Corpus (XNLI) that consists of a crowdsourced\ncollection of 5000 test and 2500 development pairs in English,\nFrench, Spanish, German, Greek, Bulgarian, Russian, Turkish,\nArabic, Vietnamese, Thai, Chinese, Hindi, Swahili, and Urdu.\nThe XNLI corpus consists of a total of 112, 500 annotated\npairs.\nWhile going through the original machine translations of\nMNLI to the Hindi language, we observed a lot of discrep-\nancies. These wrong translations could severely impact the\nperformance of a model. Cloud translation tools of present\ntimes are significantly more advanced than a few years before.\nAs a result, it eliminates the need for proficiency in multiple\nlanguages to translate the dataset correctly. We have used\nGoogle Translate to freshly translate the MNLI dataset to\n14 different language training datasets. Our paper specifically\ntalks about the drawbacks and deficiencies of the current XNLI\ncorpus and the impact that better translation could have on\nthe model performance. We will be utilizing the new XNLI\ndataset, called XNLI 2.0 for reporting accuracy metrics.\nXNLI corpus is focused on test and development example\npairs, and it is used for the evaluation of cross-lingual sentence\nunderstanding, where models are trained on one of the lan-\nguages and then tested in other languages. Most of the models\nuntil now have been trained only in English and evaluated in\nother languages. However, in this paper, we will try to focus on\nperforming cross-lingual transfer learning by training separate\nmodels on each of the 15 languages and then comparing the\naccuracy to determine which language model performs better\nduring evaluation on the test set. For this experiment, we\nmachine-translate the MNLI dataset to 14 different languages\nin the XNLI dataset. We treat these machine-translated datasets\nas separate training sets for the particular language.\nCross-lingual language understanding has been of utmost\nimportance also because of its use in understanding low-\nresource languages. Languages like Urdu and Swahili are\nclassified as low-resource because there are few available or\nFig. 1. Sample discrepancies in the original translations of the MNLI dataset.\nannotated datasets in these languages, making it difficult to\ntrain a model for performing tasks in these languages. In\nthis paper, we also perform multiple experiments to determine\nwhich high-resource languages could better help learn these\nlow-resource languages in the XNLI corpus.\nThe paper is organized as follows: We will do a literature\nreview of works relating to the XNLI corpus in cross-lingual\nlearning. In section 3, we try to present the steps undertaken in\nconcluding the mistranslations in the XNLI corpus and build-\ning up the newly translated train, test, and dev sets. Section 4\nfocuses on describing metrics and doing a comparative study,\nreporting training details, analysis, and results. Finally, in the\nlast section, we present concluding remarks and ways in which\nit can impact or enhance future works.\nII. RELATED WORK\nXNLI As we know, most natural language processing\nsystems rely on annotated or labelled data. This data is usually\nin a single language, so the system can only perform the task in\nthat language. However, in practice, systems need to be able\nto handle inputs in many languages. It is difficult because\nannotating data in all languages a system might encounter\nis nearly impossible. In the paper (Conneau et al., 2018),\nthe authors introduce a new benchmark for evaluating natural\nlanguage processing systems. The benchmark, Cross-lingual\nNatural Language Inference corpus, or XNLI, consists of 7500\nhuman-annotated development and test examples in the format\nof NLI three-way classification: premise, hypothesis, and label.\nIt was then translated by employing professional translators\nin 14 other languages, French, Spanish, German, Greek, Bul-\ngarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese,\nHindi, Swahili, and Urdu. These languages comprise several\nlanguage families and contain two low-resource languages,\nSwahili and Urdu.\nUnsupervised Multilingual Word Embeddings Words\nfrom many languages are represented by multilingual word\nembeddings (MWEs) in a single distributional vector space.\nThe acquisition of multilingual embeddings using unsuper-\nvised MWE (UMWE) methods does not require cross-lingual\nsupervision, which is a considerable improvement over con-\nventional supervised methods and creates a wealth of new\nopportunities for low-resource languages. However, prior work\nfor learning UMWEs only uses a number of unsupervised\nbilingual word embeddings (UBWEs) that have been trained\nindividually to produce multilingual embeddings. The inter-\ndependencies that exist between numerous languages are not\ntaken advantage of by these methods. A completely unsuper-\nvised approach is suggested for learning MWEs1 that directly\ntakes advantage of the relationships between all language pairs\nto remedy the aforementioned shortcoming. In experiments on\ncross-lingual word similarity and multilingual word transla-\ntion, the model significantly outperforms earlier methods and\neven bests supervised methods developed using cross-lingual\nresources (Chen and Cardie et al., 2018).\nGLUE Technology for natural language understanding\n(NLU) must be able to handle language in a way that is not\nrestricted to a specific task or dataset if it is to be useful.\nThe General Language Understanding Evaluation (GLUE)\nbenchmark (Wang et al., 2019) is established in order to\nachieve this goal. GLUE is created to reward and encourage\nmodels that share generic language knowledge across tasks\nby providing tasks with little to no training data. A collection\nTABLE I\nACCURACY OF MODELS FINE-TUNED ON EACH OF THE 15 LANGUAGES AND TESTED ON ORIGINAL XNLI TEST SET. TRAINING SET IS SAME FOR EACH\nLANGUAGE MODEL I.E., NEWLY TRANSLATED MNLI DATASET IN RESPECTIVE LANGUAGES.\nModels/Training Language\nar\nbg\nzh\nen\nfr\nde\nel\nhi\nru\nes\nsw\nth\ntr\nur\nvi\nAverage\nConneau 2020 XLM-R Base\n73.8\n79.6\n76.7\n85.8\n79.7\n78.7\n77.5\n72.4\n78.1\n80.7\n66.5\n74.6\n74.2\n68.3\n76.5\n76.21\nEnglish\n71.1\n76.3\n72.2\n83.7\n76.8\n75.3\n74.9\n68.6\n74.5\n77.5\n65\n70.6\n71.7\n65\n73.4\n73.11\nHindi\n72.6\n77.9\n75.3\n81.4\n77.4\n76.9\n76.3\n73.2\n76.1\n78\n66.6\n73.3\n73.7\n69.6\n75.8\n74.94\nFrench\n71.9\n77.2\n74\n82.7\n78.6\n77.2\n76.1\n70.6\n75.7\n78.6\n65.8\n71.5\n72.7\n67\n75\n74.31\nSpanish\n72.9\n78.3\n74.7\n82.7\n77.9\n77\n76.3\n70.8\n76.4\n79.9\n65.7\n72.1\n72.7\n68.1\n75.7\n74.75\nUrdu\n72.5\n78.2\n74.7\n81.3\n77.7\n76.7\n76.4\n72.4\n76\n77.9\n66.6\n74\n74.5\n69.8\n75.9\n74.97\nChinese\n72.6\n77.3\n76.7\n82.3\n77.5\n76.8\n76.1\n71.6\n75.4\n78.4\n66.5\n72.9\n73.7\n67.8\n76.4\n74.8\nGerman\n73\n78.5\n74.6\n83.2\n78.3\n78.7\n76.8\n71.2\n76.4\n79.2\n66.5\n73\n73.8\n66.9\n75.6\n75.05\nGreek\n73.8\n78.6\n74.5\n82.5\n77.9\n77.7\n77.8\n71.7\n76.2\n79.3\n66.6\n73\n73.1\n67.8\n75.8\n75.09\nBulgarian\n73.1\n79.5\n74.9\n82.9\n78.3\n78.1\n76.7\n71.7\n77.5\n78.9\n66.5\n72.2\n73.3\n68\n75.8\n75.16\nThai\n72.9\n78.1\n75.2\n82.1\n78.1\n77.2\n76.9\n71.4\n76.2\n78.8\n67.4\n76\n73.8\n68.8\n76.5\n75.29\nRussian\n73\n78.4\n75.1\n82.2\n78\n77.1\n77.6\n71.1\n76.1\n78.9\n67.1\n72.2\n73.1\n67.9\n76.3\n74.94\nVietnamese\n72.5\n77.4\n75.1\n81.8\n77.2\n76.2\n75.4\n70.9\n75.2\n78.4\n66.5\n72.1\n72.8\n67.7\n77.5\n74.45\nTurkish\n72.4\n77.5\n74.8\n81.9\n77.3\n76.3\n76\n72\n75.8\n78.6\n66.6\n72.5\n75\n68.4\n75.6\n74.71\nArabic\n74.5\n77.8\n74.9\n81.6\n77.6\n76.9\n76.5\n71.8\n76.5\n78.3\n66.2\n73.2\n72.9\n68.5\n76.2\n74.89\nSwahili\n72\n77.1\n74.2\n80.1\n75.9\n75.7\n75.3\n71\n74.8\n77.4\n70.8\n72.5\n72.5\n67.6\n75.4\n74.15\nTABLE II\nACCURACY OF MODELS FINE-TUNED ON EACH OF THE 15 LANGUAGES AND TESTED ON XNLI 2.0 TEST SET. TRAINING SET IS SAME FOR EACH\nLANGUAGE MODEL I.E., NEWLY TRANSLATED MNLI DATASET IN RESPECTIVE LANGUAGES.\nModels/Training Language\nar\nbg\nzh\nen\nfr\nde\nel\nhi\nru\nes\nsw\nth\ntr\nur\nvi\nAverage\nConneau 2020 XLM-R Base\n73.8\n79.6\n76.7\n85.8\n79.7\n78.7\n77.5\n72.4\n78.1\n80.7\n66.5\n74.6\n74.2\n68.3\n76.5\n76.21\nEnglish\n74.4\n79\n76.8\n83.7\n78.7\n78.3\n76.6\n75.1\n78.9\n79.8\n68.7\n73.8\n74.8\n73.7\n77.2\n76.63\nHindi\n75.6\n79.5\n77.6\n81.4\n79.7\n78.4\n78.1\n79.5\n79.9\n79.4\n69.7\n76.1\n75.9\n76.4\n79\n77.74\nFrench\n74.5\n79.5\n77.3\n82.7\n80.7\n79.1\n78.1\n75.8\n79.5\n79.9\n69.7\n74.4\n75.5\n75.3\n78.4\n77.36\nSpanish\n75.6\n80.1\n78\n82.7\n80.4\n79.3\n78\n76.5\n80.2\n81.5\n69.9\n74.6\n76\n75.8\n78.5\n77.81\nUrdu\n74.8\n79.9\n77.8\n81.3\n79.5\n78.1\n78\n77.7\n79.3\n79.9\n70.2\n76.8\n76.4\n78.4\n78.6\n77.78\nChinese\n75.2\n79.8\n80.8\n82.3\n79.5\n78.9\n77.4\n77\n80\n79.9\n69.8\n75.7\n76.5\n76.1\n78.9\n77.85\nGerman\n76.6\n80.2\n77.6\n83.2\n80.3\n81\n77.8\n76.7\n80.3\n80.9\n70.4\n75.1\n76.7\n76.5\n79\n78.15\nGreek\n76.3\n80.5\n77.9\n82.5\n79.7\n79.2\n79.5\n76.7\n79.9\n80.9\n70.8\n75.6\n75.3\n75.7\n78.7\n77.95\nBulgarian\n75.6\n81.2\n78.4\n82.9\n80.5\n79.9\n78.1\n76.9\n80.9\n80.8\n69.7\n75.3\n76.1\n75.9\n79.2\n78.09\nThai\n75.9\n79.5\n78.6\n82.1\n79.5\n79.3\n78.2\n75.6\n79.5\n80.9\n70.7\n78.4\n75.7\n74.6\n79.4\n77.86\nRussian\n75.9\n79.4\n77.5\n82.2\n79.7\n78.7\n79.1\n76.2\n79.7\n80.3\n70.1\n75.7\n75.2\n75.1\n78.5\n77.55\nVietnamese\n75.6\n79.4\n78.2\n81.8\n79.1\n78.2\n77.4\n76.1\n78.6\n79.5\n69.5\n75.4\n75.4\n75.6\n80.1\n77.33\nTurkish\n75.2\n78.8\n77.9\n81.9\n79.3\n77.8\n78\n77.4\n79.6\n79.7\n69.5\n75.8\n79.1\n76.7\n78.2\n77.66\nArabic\n78.4\n79.5\n77.9\n81.6\n79.2\n78.9\n77.8\n76.3\n79.5\n79.8\n69.8\n75.5\n76.3\n75.8\n79.1\n77.69\nSwahili\n75.1\n78.5\n76.8\n80.1\n78.4\n77.7\n77.2\n74.9\n78.5\n78.4\n75.9\n74.8\n75.2\n75\n78.3\n76.99\nof specially created diagnostic tests is also part of GLUE,\nallowing for thorough linguistic analysis of models. Evaluation\nof baselines based on existing transfer learning methodologies\nreveals that multi-task training on all tasks outperforms train-\ning a separate model for each task. The best model’s poor\nabsolute performance, on the other hand, highlights the need\nfor more advanced generic NLU systems.\nXLM-R It is a transformer-based multilingual masked\nlanguage model trained on 2.5 TB of newly created clean\nCommonCrawl data in 100 languages (Conneau et al., 2020).\nIt was used to obtain state-of-the-art performance on cross-\nlingual classification, sequence labelling and question answer-\ning. Models that are pretrained on many languages can be\neffective in cross-lingual understanding tasks. These models\nare called multilingual masked language models (MLM). For\nCross-lingual understanding, the XLM-R model outperforms\nthe XLM-100 and mBERT models by 10.2% and 14.6%\naverage accuracy. On the Swahili and Urdu low-resource lan-\nguages, XLM-R outperforms XLM-100 by 15.7% and 11.4%,\nand mBERT by 23.5% and 15.8%. XLM-R outperforms the\nUnicoder and XLM models by a 5.5% and 5.8% average\naccuracy, respectively.\nCross-Lingual Retrieval Augmented Prompt for Low-\nResource Languages Recent empirical cross-lingual transfer\nexperiments have demonstrated the robust multilinguality of\nMultilingual Pretrained Language Models (MPLMs). In this\nresearch (Nie et al., 2022), the Prompts Augmented by Re-\ntrieval Crosslingually (PARC) pipeline is presented to enhance\nthe context with semantically related sentences retrieved from\na high-resource language (HRL) as prompts, hence improving\nthe zero-shot performance on low-resource languages (LRLs).\nWith multilingual parallel test sets across 10 LRLs covering 6\nlanguage families, PARC increases the zero-shot performance\non three downstream tasks (binary sentiment classification,\ntopic categorization, and natural language inference) in both\nunlabeled settings (+5.1%) and labelled settings (+16.3%).\nFig. 2.\nHorizontal axis denotes the languages on which model is trained and the vertical axis denotes different accuracies for the model tested on all 15\nlanguages in the test set. Different color labels are used to denoted models tested in the respective languages.\nAdditionally, PARC-labeled surpasses the baseline for fine-\ntuning by 3.7%. The similarity between the high- and low-\nresource languages and cross-lingual transfer performance are\nrevealed to be significantly positively correlated.\nIII. DATASETS\nThe MNLI dataset has been machine-translated into 14 dif-\nferent languages to produce synthetic training data in these lan-\nguages. It is done to compute and analyze various benchmarks\nsuch as TRANSLATE-TRAIN and TRANSLATE-TEST (Con-\nneau et al., 2020). While going through the dataset translation\nin the Hindi language, there has been a variety of discrepancy\nfound. A few examples have been recorded in Fig. 1. We\ncan see in example 1 where the original Hindi translation\nportrays a different meaning than what is being conveyed\nby the English counterpart. In contrast, if we look at the\nnew Hindi translation, the sentence is appropriately translated.\nExample 2 and example 4 have no fundamental structure\nand meaning in the original Hindi translation, and the new\nHindi translation gives the real meaning. In examples 5, 6\nand 7, the premise/hypothesis in German, French and Swahili\nis recorded along with their corresponding original and new\ntranslations. We could find significant differences in each of\nthe new translations compared to the original ones. Similarly,\nthroughout 3,92,000 examples in the training set, there are\nexcessive occurrences of mistranslations, which in turn lead\nto inaccurate benchmarks.\nAs we look deeper into every machine-translated train set,\nwe find that the current cloud translation systems, such as\nGoogle Translate, perform at a level far better than that of three\nyears ago. In releases since 2019, the average BLEU scores of\nGoogle Translate API over 100+ languages have improved by\n5 pts and 7 pts on low-resource languages. We translated the\nEnglish train set into all 14 languages using Google Translate\nand curated a separate dataset for each language. We also\nexplored the quality of translation in the XNLI test and dev\nsets. Human translators have done these translations, but we\nstill found considerable mistranslations. The new translations\nperformed on Google Translate provide a much more accurate\ninterpretation of their English counterparts, and therefore, a\ncompletely new dataset has been curated for each of the\n15 languages in XNLI for test and dev sets. We call this\nnew dataset XNLI 2.0, which consists of machine-translated\ntrain sets and test and dev sets on which evaluations will be\nperformed.\nIV. EXPERIMENTATION AND RESULTS\nA. Training details\nAs already stated, this paper aims to do a comparative study\nof how the changes in dataset translations bring about changes\nin several benchmarks. We aim to show that (i) languages other\nthan English could also be a better choice for natural language\ninference (NLI) and (ii) try to find languages that give better\nperformance in understanding low-resource languages such as\nSwahili and Urdu. We primarily use XLM-RoBERTa (base),\na model pre-trained on 2.5TB of filtered CommonCrawl data\nconsisting of 100 languages (Conneau et al. 2020). Because of\nlimited resources, it was not possible to reproduce the model\nfrom the original paper (Conneau et al., 2020), and therefore\nwe used our training parameters.\nFor cross-lingual understanding, we first fine-tune XLM-\nRoBERTa(base) model on the MNLI dataset for three epochs.\nWe use the Adam optimizer and the XLM-R tokenizer to\nencode the input dataset. 80% of the dataset is used for\nFig. 3. Performance of models trained in 15 different languages on Swahili test set of XNLI 2.0.\nFig. 4. Performance of models trained in 15 different languages on Urdu test set of XNLI 2.0.\ntraining while the other 20% is used for validation. After the\nmodel is trained, we first test it on the original test set of 14\nlanguages and tabulate the accuracy score in Table I. Then,\nwe test the model on the new test dataset of XNLI 2.0 and\nstudy how the changes in the test dataset produce changes\nin accuracy. The recorded accuracy on the original XNLI test\ndataset is approximately 73% for the model trained in English.\nIn contrast, on our XNLI 2.0 test set, we obtain around a 3%\nincrease in accuracy, i.e., ∼76%.\nSimilarly, we trained separate models on machine-translated\ndatasets of all other 14 languages and tested it on both XNLI\nand XNLI 2.0 datasets. We tabulate the accuracy score and\npresent it in Table I and Table II.\nB. Analysis and Results\nWe have observed in all works related to XNLI that there is\na heavy dependence on the English language dataset for train-\ning the model and performing cross-lingual transfer learning.\nThis, in turn, limits our capability to not even test models\ntrained in languages other than English. While English is a\nhigh-resource language, so are German, French, and Spanish.\nComparing the performance of our model on XNLI and\nXNLI 2.0 in Table I and Table II, we observe a difference\nin average accuracy in the range of 2.5%- 3% for all 15\nlanguages. Because of fresh translations of the training dataset,\nthere has been a significant improvement in the accuracy\nof the model. If we also analyze the model’s performance\nin every language on the new test set, we can see that the\nmodel trained in German has an average accuracy of 78.15%.\nIn comparison, the model trained in English has an average\naccuracy of 76.63%, which depicts that the model trained\nin German outperforms that trained in English in the cross-\nlingual natural language inference task.\nFig. 2 shows the performance of 15 different models (trained\non each of the 15 languages) on every language in the XNLI\n2.0 test set. To perform several tasks on models trained in\nthese languages, we need to look at the performance of models\non every language in test set and find the one that gives a\nbetter accuracy. This will facilitate in choosing a high-resource\nlanguage in which a model could be trained and that gives a\nbetter performance in XLU. We can see in Fig. 2 that there\nare several high resource languages that give competent cross-\nlingual performance as compared to the same language in\nwhich the model is trained, such as German, Bulgarian and\nSpanish.\nTable II gives an interesting viewpoint on languages that\nperform the best with regard to low-resource languages. In the\ncase of Swahili, three languages perform the best and have\nalmost similar accuracy. Thai (70.7%), Greek (70.8%), and\nGerman (70.4%) outperform English (68.7%), as can be seen\nin Fig. 3. In the case of Urdu, Turkish (76.7%) and German\n(76.5%) performs the best and outperform English (73.7%),\nas demonstrated in Fig. 4. One commonality worth noticing is\nhow German (high-resource) language could be leveraged for\nperforming tasks in low-resource languages instead of relying\non English-only models.\nCONCLUSION\nThis paper demonstrates that our new machine-translated\nXNLI corpus (XNLI 2.0) performs around 3% better on cross-\nlingual natural language inference than the original machine-\ntranslated dataset (XNLI).\nAlthough it was not possible for us to replicate the original\nmodel (Conneau et al., 2020), we still outperformed the\noriginal model using our XNLI 2.0 dataset, which establishes\nthe fact that our XNLI 2.0 dataset should significantly exceed\nthe original dataset performance if used in the original model.\nWe have also shown that there are many languages other\nthan English, such as German, Greek, and Bulgarian on which\nmodels could be trained that perform well on cross-lingual\nlanguage understanding tasks. Finally, we have shown how\nthe high-resource languages perform better cross-lingual un-\nderstanding of low-resource languages on the XNLI 2.0 dataset\nas compared to the original XNLI dataset which demonstrates\nthat the re-translation of original XNLI dataset to XNLI 2.0\ndataset has significantly improved the performance.\nOur work in this paper establishes the possibility of cross-\nlingual transfer learning playing a significant role in several\nother tasks such as question-answering, text-summarization,\nand named-entity recognition, in which training language\ndoesn’t have to be restricted only to English, but several other\nhigh-resource languages could be put into use. Performing\nseveral natural language understanding tasks, especially in\nlow-resource languages, will foster technological inclusion.\nFuture works could include expanding the scope of the training\ndataset by including examples in several other low-resource\nlanguages such as Bengali, Kannada, Tamil, etc.\nCODE AND DATA\nThe new dataset, i.e., XNLI 2.0, is available on the Hugging\nFace hub.\nACKNOWLEDGEMENTS\nWe are very thankful to Dr. Naidila Sadashiv and Dr. Naveen\nN C for their invaluable guidance and mentoring throughout\nthe research.\nREFERENCES\n[1] Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams,\nSamuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI:\nEvaluating cross-lingual sentence representations. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing,\npages 2475–2485, Brussels, Belgium. Association for Computational\nLinguistics.\n[2] Alexis Conneau and Guillaume Lample. 2019. Cross-lingual language\nmodel pretraining. In Advances in Neural Information Processing Sys-\ntems 32, pages 7059–7069.\n[3] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaud-\nhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott,\nLuke Zettle- ´ moyer, and Veselin Stoyanov. 2020. Unsupervised cross-\nlingual representation learning at scale. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pages\n8440– 8451. Association for Computational Linguistics.\n[4] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher\nD. Manning. 2015. A large annotated corpus for learning natural\nlanguage inference. In EMNLP.\n[5] ˇZeljko Agic and Natalie Schluter. 2018. Baselines and ´ test data for\ncross-lingual inference. LREC.\n[6] Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc Barrault, and\nAntoine Bordes. 2017. Supervised learning of universal sentence rep-\nresentations from natural language inference data. In EMNLP, pages\n670–680.\n[7] Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-\nlingual transferability of monolingual representations. In Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguis-\ntics, pages 4623–4637. Association for Computational Linguistics.\n[8] Lo¨ıc Barrault, Ondˇrej Bojar, Marta R. Costa-jussa,‘ Christian Fed-\nermann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck,\nPhilipp Koehn, Shervin Malmasi, Christof Monz, Mathias Muller, ¨\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the\n2019 Conference on Machine Translation (WMT19). In Proceedings\nof the Fourth Conference on Machine Translation (Volume 2: Shared\nTask Papers, Day 1), pages 1–61, Florence, Italy. Association for\nComputational Linguistics.\n[9] Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan\nFirat, and Melvin Johnson. 2020. XTREME: A massively multilingual\nmultitask benchmark for evaluating cross-lingual generalization. arXiv\npreprint arXiv:2003.11080.\n[10] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n2019. RoBERTa: A robustly optimized BERT pretraining approach.\narXiv preprint arXiv:1907.11692.\n[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\nAttention is all you need. In Advances in Neural Information Processing\nSystems 30, pages 5998–6008.\n[12] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019.\nExploring the limits of transfer learning with a unified text-to-text\ntransformer. arXiv preprint arXiv:1910.10683.\n[13] Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R Bowman. 2019. Glue: A multi-task benchmark and\nanalysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461.\n[14] Translation\nArtifacts\nin\nCross-lingual\nTransfer\nLearning\n(https://aclanthology.org/2020.emnlp-main.618) (Artetxe et al., EMNLP\n2020).\n[15] Xilun Chen and Claire Cardie. 2018. Unsupervised Multilingual Word\nEmbeddings. In Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages 261–270, Brussels,\nBelgium. Association for Computational Linguistics.\n[16] Ercong Nie, Sheng Liang, Helmut Schmid, Hinrich Sch¨utze. 2022.\nCross-Lingual Retrieval Augmented Prompt for Low-Resource Lan-\nguages. arXiv:2212.09651v1.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2023-01-16",
  "updated": "2023-01-16"
}