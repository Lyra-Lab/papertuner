{
  "id": "http://arxiv.org/abs/2301.07487v1",
  "title": "Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness",
  "authors": [
    "Ezgi Korkmaz"
  ],
  "abstract": "Learning from raw high dimensional data via interaction with a given\nenvironment has been effectively achieved through the utilization of deep\nneural networks. Yet the observed degradation in policy performance caused by\nimperceptible worst-case policy dependent translations along high sensitivity\ndirections (i.e. adversarial perturbations) raises concerns on the robustness\nof deep reinforcement learning policies. In our paper, we show that these high\nsensitivity directions do not lie only along particular worst-case directions,\nbut rather are more abundant in the deep neural policy landscape and can be\nfound via more natural means in a black-box setting. Furthermore, we show that\nvanilla training techniques intriguingly result in learning more robust\npolicies compared to the policies learnt via the state-of-the-art adversarial\ntraining techniques. We believe our work lays out intriguing properties of the\ndeep reinforcement learning policy manifold and our results can help to build\nrobust and generalizable deep reinforcement learning policies.",
  "text": "Adversarial Robust Deep Reinforcement Learning Requires Redeﬁning\nRobustness\nEzgi Korkmaz\nAbstract\nLearning from raw high dimensional data via interaction with\na given environment has been effectively achieved through\nthe utilization of deep neural networks. Yet the observed\ndegradation in policy performance caused by imperceptible\nworst-case policy dependent translations along high sensitiv-\nity directions (i.e. adversarial perturbations) raises concerns\non the robustness of deep reinforcement learning policies. In\nour paper, we show that these high sensitivity directions do\nnot lie only along particular worst-case directions, but rather\nare more abundant in the deep neural policy landscape and\ncan be found via more natural means in a black-box set-\nting. Furthermore, we show that vanilla training techniques\nintriguingly result in learning more robust policies compared\nto the policies learnt via the state-of-the-art adversarial train-\ning techniques. We believe our work lays out intriguing prop-\nerties of the deep reinforcement learning policy manifold and\nour results can help to build robust and generalizable deep\nreinforcement learning policies.\n1\nIntroduction\nFollowing the initial work of Mnih et al. (2015), the use\nof deep neural networks as function approximators in rein-\nforcement learning has led to a dramatic increase in the ca-\npabilities of reinforcement learning policies (Schulman et al.\n2017; Vinyals et al. 2019; Schrittwieser et al. 2020). In par-\nticular, these developments allow for the direct learning of\nstrong policies from raw, high-dimensional inputs (i.e. vi-\nsual observations). With the successes of these new methods\ncome new challenges regarding the robustness and general-\nization capabilities of deep reinforcement learning agents.\nInitially, Szegedy et al. (2014) showed that speciﬁcally\ncrafted imperceptible perturbations can lead to misclassiﬁ-\ncation in image classiﬁcation. After this initial work a new\nresearch area emerged to investigate the abilities of deep\nneural networks against speciﬁcally crafted adversarial ex-\namples. While various works studied many different ways to\ncompute these examples (Carlini and Wagner 2017; Madry\net al. 2018; Goodfellow, Shelens, and Szegedy 2015; Ku-\nrakin, Goodfellow, and Bengio 2016), several works focused\non studying ways to increase the robustness against such\nspeciﬁcally crafted perturbations, based on training with the\nexistence of such perturbations (Madry et al. 2018; Tram`er\net al. 2018; Goodfellow, Shelens, and Szegedy 2015; Xie\nand Yuille 2020).\nAs image classiﬁcation suffered from this vulnerability\ntowards worst-case distributional shift in the input, a series\nof work conducted in deep reinforcement learning showed\nthat deep neural policies are also susceptible to speciﬁcally\ncrafted imperceptible perturbations (Huang et al. 2017; Kos\nand Song 2017; Pattanaik et al. 2018; Yen-Chen et al. 2017;\nKorkmaz 2020; Sun et al. 2020; Korkmaz 2021b). While one\nline of work put effort on exploring these vulnerabilities in\ndeep neural policies, another line in parallel focused making\nthem robust and reliable via adversarial training (Pinto et al.\n2017; Mandlekar et al. 2017; Gleave et al. 2020).\nWhile adversarial perturbations and adversarial training\nprovide a notion of robustness for trained deep neural poli-\ncies, in this paper we approach the resilience problem of\ndeep reinforcement learning from a wider perspective, and\npropose to investigate the deep neural policy manifold along\nhigh-sensitivity directions. Along this line we essentially\nseek answers for the following questions:\n• How can we probe the deep neural policy decision\nboundary with policy-independent high-sensitivity direc-\ntions innate to the MDP within the perceptual similarity\nbound?\n• Is it possible to affect the state-of-the-art deep rein-\nforcement learning policy performance trained in high-\ndimensional state representation MDPs with policy-\nindependent high-sensitivity directions intrinsic to the\nMDP?\n• What are the effects of state-of-the-art certiﬁed adver-\nsarial training on the robustness of the policy com-\npared to straightforward vanilla training when policy-\nindependent high-sensitivity directions are present?\nThus, to be able answer these questions, in this work we fo-\ncus on the notion of robustness for deep reinforcement learn-\ning policies and make the following contributions:\n• We probe the deep reinforcement learning manifold\nvia policy dependent and policy-independent high-\nsensitivity directions innate to the MDP.\n• We run multiple experiments in the Arcade Learning\nEnvironment (ALE) in various games with high di-\nmensional state representation and provide the relation-\nship between the perceptual similarities to base states\nunder policy dependent and policy-independent high-\nsensitivity directions.\n• We compare policy-independent high-sensitivity direc-\ntions with the state-of-the-art adversarial directions\nbased on ℓp-norm changes, and we show that policy-\nindependent high-sensitivity directions intrinsic to the\nMDP are competitive in degrading the performance of\nthe deep reinforcement learning policies with lower per-\nceptual similarity distance. Thus, the results of this\ncontradistinction of adversarial directions and policy-\nindependent high-sensitivity directions intrinsic to the\nMDP evidently demonstrates the abundance of high-\nsensitivity directions in the deep reinforcement learning\npolicy manifold.\n• Finally, we inspect state-of-the-art adversarial training\nunder changes intrinsic to the MDP, and demonstrate that\nthe adversarially trained models become more vulnera-\nble to several different types of policy-independent high-\nsensitivity directions compared to vanilla trained models.\n2\nBackground and Related Work\n2.1\nPreliminaries\nIn this paper we consider Markov Decision Processes\n(MDPs) given by a tuple (S, A, T , r, γ, si). The reinforce-\nment learning agent interacts with the MDP by observing\nstates s ∈S, taking actions a ∈A and receiving rewards\nr(s, a, s′). Here si represents the initial state of the agent,\nand γ ∈(0, 1] represents the discount factor. The probabil-\nity of transitioning to state s′ when the agent takes action a\nin state s is determined by the Markovian transition kernel\nT : S × A × S →R. The reward received by the agent\nwhen taking action a in state s is given by the reward func-\ntion r : S × A × S →R. The goal of the agent is to learn\na policy π : S × A →R which takes an action a in state s\nthat maximizes the expected cumulative discounted reward\nPT −1\nt=0 γtr(st, at, st+1) that the agent receives via interact-\ning with the environment.\n˜π = arg max\nπ\nX\nt\nEst,at∼Pπ[r(st, at, st+1)]\n(1)\nwhere Pπ represents the occupancy distribution of the tra-\njectory followed by the policy π(at|st). Hence, this goal can\nbe achieved via learning the state-action value function via\niterative Bellman update\nQ(s, a) = Eπ[\nT −1\nX\nt=0\nγtr(st, at, st+1)|si = s, ai = a]\nassigning a value to each state-action pair. In high dimen-\nsional state representation MDPs the state-action values are\nestimated via function approximators.\nθt+1 = θt + α(Qtarget\nt\n−Q(st, at; θt))∇θtQ(st, at; θt)\nwhere Qtarget\nt\nis r(st, at, st+1) + γ maxa Q(st+1, a; θt).\n2.2\nComputing Adversarial Directions\nSzegedy et al. (2014) proposed to minimize the distance\nbetween the base image and adversarially produced im-\nage to create adversarial directions. The authors used box-\nconstrained L-BFGS to solve this optimization problem.\nGoodfellow, Shelens, and Szegedy (2015) introduced the\nfast gradient method (FGM),\nxadv = x + ϵ ·\n∇xJ(x, y)\n||∇xJ(x, y)||p\n,\n(2)\nfor crafting adversarial examples in image classiﬁcation by\ntaking the gradient of the cost function J(x, y) used to train\nthe neural network in the direction of the input, where x is\nthe input, y is the output label, and J(x, y) is the cost func-\ntion. Carlini and Wagner (2017) introduced targeted attacks\nin the image classiﬁcation domain based on distance mini-\nmization between the adversarial image and the base image\nwhile targeting a particular label. Thus, in deep reinforce-\nment learning the Carlini and Wagner (2017) formulation\nwill ﬁnd the minimum distance to a nearby state in an ϵ-ball\nDϵ,p(s) such that,\nmin\nˆs∈Dϵ,p(s) ∥ˆs −s∥p\nsubject to arg max\na\nQ(s, a) ̸= arg max\na\nQ(ˆs, a)\nwhere s ∈S represents the base state, ˆs ∈Dϵ,p(s) repre-\nsents the state when it is moved along the adversarial direc-\ntions. This formulation attempts to minimize the distance to\nthe base state, constrained to states leading to sub-optimal\nactions as determined by the Q-network. Note that the Car-\nlini & Wagner formulation has quite recently been used to\ndemonstrate that the state-of-the-art adversarial trained poli-\ncies share similar, and even in some cases identical, adver-\nsarial directions with the vanilla trained deep reinforcement\nlearning policies (Korkmaz 2022a). In contrast to adversar-\nial attacks, in our proposed threat model we will not need\nany information on the cost function used to train the net-\nwork, the Q-network of the trained agent, or access to the\nvisited states themselves.\n2.3\nAdversarial Approach in Deep Reinforcement\nLearning\nThe ﬁrst adversarial attacks on deep reinforcement learning\nintroduced by Huang et al. (2017) and Kos and Song (2017)\nadapted FGSM from image classiﬁcation to the deep rein-\nforcement learning setting. Subsequently, Pinto et al. (2017)\nand Gleave et al. (2020) focused on modeling the interaction\nbetween the adversary and the agent as a zero-sum Markov\ngame, while Yen-Chen et al. (2017); Sun et al. (2020) fo-\ncused on strategically timing when (i.e. in which state) to at-\ntack an agent using perturbations computed with the Carlini\n& Wagner adversarial formulation. Orthogonal to this line\nof research some studies demonstrated that deep reinforce-\nment learning policies learn adversarial directions from un-\nderlying MDPs that are shared across states, across MDPs\nand across algorithms (Korkmaz 2022a). While proposing\nnovel techniques to uncover non-robust features, some re-\ncent studies demonstrated the persistent existence of the\nnon-robust features in state-of-the-art adversarial training\nmethods1 (Korkmaz 2021b).\n1See (Korkmaz 2021a) for inaccuracy and inconsistency of the\nstate-action value function learnt by adversarially trained policies.\nFor more on robustness problems in inverse deep reinforcement\nlearning see (Korkmaz 2022b,c)\n2.4\nPerceptual Similarity Distance\nInternal activations of networks trained for high-level tasks\ncorrespond to human perceptual judgements across different\nnetwork architectures (Krizhevsky, Sutskever, and E. Hinton\n2012; Simonyan and Zisserman 2015; Iandola et al. 2016)\nwithout calibration (Zhang et al. 2018). More importantly,\nit is possible to measure the perceptual similarity distance\nbetween two images with LPIPS matching human percep-\ntion. Thus, in our experiments we measure the distance of\nmoving along the high sensitivity directions from the base\nstates with LPIPS. In particular, Psimilarity(s, ˆs) returns the\ndistance between s and ˆs based on network activations, and\nresults in an effective approximation of human perception.\nIn more detail, the LPIPS metric is given by measuring the\nℓ2-distance between a normalized version of the activations\nof the neural network at several internal layers. For each\nlayer l let Wl be the width, Hl the height, and Cl the num-\nber of channels. Further, let yl ∈RWl×Hl×Cl denote the\nvector of activations in convolutional layer l. To compute\nthe perceptual similarity distance between two states s and\nˆs, ﬁrst calculate the channel-normalized internal activations\nˆyl\ns, ˆyl\nˆs ∈RWl×Hl×Cl (corresponding to s and ˆs respectively)\nfor L internal layers, and scale each channel in ˆyl\ns and ˆyl\nˆs by\nthe same, ﬁxed weight vector wl ∈RCl. The last step is then\nto compute the perceptual similarity distance by ﬁrst averag-\ning the ℓ2-distance between the scaled activations over the\nspatial dimensions, and then summing over the L layers.\n3\nMoving Through the Deep Neural Policy\nManifold via High-Sensitivity Directions\nTo investigate the deep neural policy manifold we will probe\nthe deep reinforcement learning decision boundary via both\nadversarial directions and directions innate to the state repre-\nsentations. While the adversarial directions are speciﬁcally\noptimized high-sensitivity directions in the deep neural pol-\nicy landscape (i.e. worst-case distributional shift) within an\nimperceptibility bound as described in Section 2.3, the nat-\nural directions represents intrinsic semantic changes in the\nstate representations within the imperceptibility distance.\nDeﬁnition 3.1. Let π be a policy in an MDP M and let\nS be the set of states in M. Let ϵ, δ > 0. An (ϵ, δ)-high-\nsensitivity direction function for π is a function ξ(s, π) tak-\ning values in S such that Psimilarity(s, s + ξ(s, π)) ≤ϵ for all\ns ∈S, and\nEat∼π(st+ξ(st,π),·)\n\"T −1\nX\nt=0\nγtr(st, at, st+1)\n#\n< δ · Eat∼π(st,·)\n\"T −1\nX\nt=0\nγtr(st, at, st+1)\n#\n.\nIntuitively, ξ(s, π) is a high-sensitivity direction function\nif translating by ξ(s, π) in state s causes a signiﬁcant drop\nin expected cumulative rewards when executing the policy\nπ. Note that the function ξ(s, π) in Deﬁnition 3.1 takes the\npolicy π as input, and so is able to use information about\nthe behavior of π in state s in order to compute the direction\nAlgorithm 1: Probing Neural Manifold with High-sensitivity\nDirections within Perceptual Similarity\nInput: Policy π(s, a), high-sensitivity direction function\nξ(s, π), internal activations in convolutional layer yl ∈\nRWl×Hl×Cl, parameter ϵ, δ > 0.\nfor t = 0 to T do\nat = arg maxa′∈A(s) π(st + ξ(st, π), a′)\nSample st+1 ∼T (st, at, ·)\nPsimilarity(s, s + ξ(s, π)) =\nP\nl\n1\nHlWl\nP\nh,w∥wl ⊙(ˆyl\nshw −ˆyl\n(s+ξ(s,π))hw)∥2\n2\nPS+ = Psimilarity(st, st + ξ(st, π))\nR+ = r(st, at)\nend for\nReturn: Total reward R and average perceptual similarity\nPS\nT .\nξ. We next introduce a restricted version of Deﬁnition 3.1\nwhere the function is not allowed to use any information\nabout π.\nDeﬁnition 3.2. Let S be the set of states for an MDP M,\nlet π ∈Π be a set of policies in M, and let ξ : S →S\nbe a function on S. Let ϵ, δ > 0. ξ(s) is a ﬁxed (ϵ, δ)-high-\nsensitivity direction function if the function φ(s, π) = ξ(s)\nis an (ϵ, δ)-high-sensitivity direction function for all π ∈Π.\nTo probe the deep reinforcement learning policy land-\nscape we will utilize policy dependent worst-case high-\nsensitivity directions (i.e. adversarial perturbations) as de-\nscribed in Deﬁnition 3.1 and policy-independent directions\ninnate to the MDP as described in Deﬁnition 3.2. This prob-\ning methodology intrinsically juxtaposes adversarial direc-\ntions and policy-independent directions with respect to their\nperceptual similarity distance (see Section 2.4) to the base\nstates and their degree of impact on the policy performance.\nMore importantly, we question the imperceptibility of ℓp-\nnorm bounded adversarial directions in terms of perceptual\nsimilarity distance, and compare this imperceptibility notion\nto the policy-independent high-sensitivity directions intrin-\nsic to the MDP. The fact that policy-independent high sen-\nsitivity directions innate to the MDP can achieve ultimately\nsimilar or higher drop in the expected cumulative rewards\nwithin the perceptual similarity distance brings the line of\nresearch focusing on adversarial directions into question.\nMore importantly, the fact that policies trained to resist these\nadversarial directions and claimed to be ”certiﬁed” robust\nare essentially less robust than simple vanilla trained deep\nreinforcement learning policies as demonstrated in Section 4\nbrings the intrinsic trade-off made during training into ques-\ntion.\nWhile it is possible to interpret the outcomes of contrast-\ning worst-case policy dependent high sensitivity directions\n(i.e. adversarial) and policy-independent high-sensitivity di-\nrections as crucially surprising in terms of the security per-\nspective2, our goal is to provide an exact fundamental trade-\noff made by employing both adversarial attacks and training\n2In terms of the security perspective the research conducted\nin the worst-case high-sensitivity directions in deep reinforcement\nTable 1: Impacts on the policy performance, perceptual similarity distances Psimilarity to the base states, and raw scores for\nCarlini and Wagner (2017) formulation and policy-independent high-sensitivity directions innate to the environment. We report\nall of the results with the standard error of the mean.\nALE MDPs\nBankHeist\nJamesBond\nPong\nRiverraid\nTimePilot\nC&W Impact\n0.982±0.009\n0.451±0.231\n0.995±0.014\n0.928±0.030\n0.567 ±0.159\nB&C Impact\n0.966± 0.030\n0.913 ±0.047\n1.0±0.009\n0.951 ±0.016\n0.663±0.239\nBlurred Observations Impact\n0.979±0.009\n0.635±0.200\n1.0±0.000\n0.946±0.015\n0.589±0.150\nRotation Impact\n0.997±0.004\n0.635±0.189\n0.99±0.015\n0.942±0.042\n0.581±0.158\nShifting Impact\n0.985 ±0.005\n0.865±0.140\n1.0±0.00\n0.935 ±0.023\n0.623±0.199\nDCT Artifacts Impact\n0.980 ±0.013\n0.884 ±0.128\n0.962±0.032\n0.803 ±0.051\n0.578 ±0.271\nPT Impact\n0.998±0.003\n0.865±0.087\n0.996±0.009\n0.968±0.006\n0.624±0.198\nC&W Psimilarity\n0.0657±0.0073\n0.2622±0.0312\n0.6134±0.0271\n0.2714±0.0285\n0.1336± 0.0231\nB&C Psimilarity\n0.0307±0.0039\n0.011± 0.0003\n0.2190± 0.0046\n0.2147±0.0212\n0.1045± 0.0031\nBlurred Observations Psimilarity\n0.1672±0.0192\n0.0707±0.0074\n0.0351±0.0072\n0.1442±0.0107\n0.2014±0.0645\nRotation Psimilarity\n0.0520±0.0070\n0.0275±0.0016\n0.1020±0.0115\n0.0422± 0.0033\n0.1020±0.0115\nShifting Psimilarity\n0.0492±0.0046\n0.0650±0.0092\n0.2455±0.0432\n0.0945±0.0032\n0.1167±0.0121\nDCT Artifacts Psimilarity\n0.0240±0.0037\n0.1325±0.0301\n0.2506±0.0559\n0.2250±0.0202\n0.1592±0.0369\nPT Psimilarity\n0.0398±0.0067\n0.012±0.0007\n0.0140±0.0018\n0.0422±0.0016\n0.0440±0.0050\nC&W Raw Scores\n15.0±2.549\n285.0±25.495\n-20.8±0.189\n1168.0± 140.696\n4090.0±347.979\nB&C Raw Scores\n17.0±1.651\n45.0±6.846\n-21.0±0.000\n744.0±76.957\n3180.0±711.027\nBlurred Observations Raw Scores\n18.0±3.405\n190.0±33.015\n-21.0±0.000\n820.0±72.013\n3880.0±329.484\nRotation Raw Scores\n2.0±1.264\n190.0± 27.203\n-20.6±0.209\n873.0±201.866\n3150.0±482.959\nShifting Raw Scores\n13.0±1.449\n70.0±20.248\n-21.0±0.000\n988.0± 89.057\n3560.0± 437.538\nDCT Artifacts Raw Scores\n17.0±3.478\n60.0±18.439\n-19.4±0.428\n2589.0±389.679\n3980.0±593.936\nPT Raw Scores\n1.0±0.948\n75.0±12.649\n-20.9±0.126\n486.0±29.127\n3550.0±435.028\nB&C [α, β]\n[1.2,40]\n[0.9,20]\n[1.7,40]\n[2.4,-275]\n[2.4,-260]\nBlurring Kernel Size\n5\n3\n3\n5\n5\nRotation Degree\n1.4\n1.6\n3\n1.8\n5\nShifting [ti, tj]\n[1,1]\n[0,1]\n[2,1]\n[1,2]\n[2,2]\nPT Norm\n1\n1\n3\n2\n3\ntechniques. The fact that worst-case directions are heavily\ninvestigated in deep reinforcement learning research without\nclear cost and trade-off of these design choices essentially\nmight create bias on inﬂuencing future research directions.\nTo probe the deep neural policy manifold via policy-\nindependent high sensitivity directions we focus on intrin-\nsic changes that are as simple as possible in the high di-\nmensional state representation MDPs. We categorize these\nchanges with respect to their frequency spectrum and below\nwe explain precisely how these high sensitivity directions\nare computed.\nLow Frequency Policy-Independent High-Sensitivity\nDirections: For the low frequency investigation we utilized\nbrightness and contrast change in the state representations.\nWe have kept moving along high-sensitivity direction as\nsimple as possible as a linear transformation of the base\nstate,\nˆs(i, j) = s(i, j) · α + β,\n(3)\nwhere s(i, j) is the ijth pixel of state s, and α and β are the\nlinear brightness parameters. The perspective transform of\nstate representations includes a mapping between four dif-\nlearning relies heavily on a strong adversary assumption. In par-\nticular, this assumption refers to an adversary that has access to\nthe policy’s perception system, training details of the policy (e.g.\nalgorithm, neural network architecture, training dataset), ability to\nalter observations in real time, simultaneous modiﬁcations to the\nobservation system of the policy with computationally demanding\nadversarial formulations as described in Section 2.2 and in Section\n2.3\nferent source and destination pixels given that\nˆs(i, j) = s\n \nΓ11si + Γ12sj + Γ13\nΓ31si + Γ32sj + Γ33\n, Γ21si + Γ22sj + Γ23\nΓ31si + Γ32sj + Γ33\n!\nδk\n\n\nsdstk\ni\nsdstk\nj\n1\n\n= Γ ·\n\n\nssrck\ni\nssrck\nj\n1\n\n.\n(4)\nThe norm of a perspective transformation is deﬁned as the\nmaximum distance that one of the corners of the square\nmoves under this mapping. Note that the perspective trans-\nformation has effects on both high and low frequencies as\nalso portrayed in Section 5.\nHigh Frequency Policy-Independent High-Sensitivity\nDirections: On the high frequency side we included com-\npression artifacts caused by the discrete cosine transform\nresulting in the loss of high frequency components, also\nreferred to as ringing and blocking artifacts. Another high\nsensitivity direction considered on the high frequency side\nof the spectrum is blurring3. In particular, median blurring\nwhich is a nonlinear noise removal technique that replaces\nthe base pixel value with the median pixel value of its neigh-\nbouring pixels. In this category kernel size k refers to the fact\nthat the median is computed over a k×k neighborhood of the\n3Note that in the blurring category one might use several dif-\nferent type of blurring techniques as Gaussian blurring, zoom blur-\nring, defocus blur. Yet all these different types of techniques occupy\nthe same frequency band in the Fourier domain.\nBase State\nShift\nPT\nBlur\nDCT\nB&C\nFigure\n1:\nBase\nframe\nand\npolicy-independent\nhigh-\nsensitivity directions. Columns: base frame, shifting, per-\nspective transformation, blurring, discrete cosine transform\nartifacts, brightness and contrast. Up: JamesBond. Down:\nBankHeist. The results for the rest of the MDPs in consider-\nation are reported in the full version of the paper.\nbase pixel. One of the most fundamental geometric transfor-\nmations leading to high frequency changes rotates the state\nobservation around the centering pixel with corresponding\nrotation angle reported as degrees. Lastly, on the geometric\ntransformations, shifting is included, which moves the input\nin the x or y direction with as few pixels moved as possible.\nThis is denoted with [ti, tj] as the distance shifted, where ti\nis in the direction of x and tj is in the direction of y.\nFigure 1 demonstrates the visual interpretation of mov-\ning along these policy-independent high-sensitivity direc-\ntions innate to the environment described above. While mov-\ning along these policy-independent directions is visually im-\nperceptible, we also report exact perceptual similarity dis-\ntances to the base states computed by Algorithm 1 in Table\n1. In more detail, Table 1 shows the raw scores, correspond-\ning performance drops, perceptual similarities to the base\nstates and corresponding hyperparameters for the policy\ndependent (i.e. adversarial) and policy-independent high-\nsensitivity sensitivity directions. Hence, the results in Table\n1 demonstrate that the policy-independent high-sensitivity\ndirections cause similar or higher degradation in the policy\nperformance within similar perceptual similarity distance.\nTo compute the results in Table 1, Algorithm 1 described\nin Section 3 is utilized.\n4\nMoving Along High-Sensitivity Directions\nin the Adversarially Trained Neural\nManifold\nIn this section we investigate state-of-the-art adversarially\ntrained deep reinforcement learning policies with policy-\nindependent high-sensitivity directions described in Section\n3. In particular, we test State Adversarial Double Deep Q-\nNetwork, a state-of-the-art algorithm (Huan et al. 2020). In\nthis paper the authors propose using what they call a state-\nadversarial MDP to model adversarial attacks in deep rein-\nforcement learning. Based on this model they develop meth-\nods to regularize Double Deep Q-Network policies to be\ncertiﬁed robust to adversarial attacks. In more detail, letting\nB(s) be the ℓp-norm ball of radius ϵ, this regularization is\nachieved by adding,\nR(θ) = max{ max\nˆs∈B(s)\nmax\na̸=arg maxa′ Q(s,a′) Qθ(ˆs, a)\n−Qθ(ˆs, arg max\na′\nQ(s, a′), −c}.\nto the temporal difference loss used in standard DQN. In\nparticular, for a sample of the form (s, a, r, s′) the loss is\nL(θ) = LH\n\u0010\nr + γ max\na′ Qtarget(s′, a′) −Qθ(s, a)\n\u0011\n+ R(θ)\nwhere LH is the Huber loss. Furthermore, we also test the\nmost recent adversarial training technique RADIAL. In par-\nticular, the RADIAL method utilizes interval bound prop-\nagation (IBP) to compute upper and lower bounds on the\nQ-function under perturbations of norm ϵ. In particular, let-\nting Qupper(s, a, ϵ) and Qlower(s, a, ϵ) be the respective upper\nand lower bounds on the Q-function when the state s is per-\nturbed by ℓp-norm at most ϵ. For a given state s and action\na, the RADIAL method utilizes the action-value difference\nand the overlap given by\nQdiff(s, ˆa) = max(0, Q(s, ˆa) −Q(s, a)).\nThe overlap is deﬁned by\nOV(s, ˆa, ϵ) = max(0,Qupper(s, ˆa, ϵ)\n−Qlower(s, a, ϵ) + 1\n2Qdiff(s, ˆa)).\nThe adversarial loss used in RADIAL is then given by the\nexpectation over a minibatch of transitions\nLadv(θ, ϵ) = Es,a,s′\n\"X\nˆa∈A\nOV(s, ˆa, ϵ) · Qdiff(s, ˆa)\n#\n.\nDuring training the adversarial loss Ladv(θ, ϵ) is added to the\nstandard temporal difference loss. Note that both of these\nadversarial training algorithms SA-DDQN and RADIAL\nappeared in NeurIPS 2020 as a spotlight presentation and\nNeurIPS 2021 consecutively. Thus, it is of great and critical\nimportance in the lines of AI-safety and in terms of affect-\ning overall research progress and effort to outline both the\nlimitations and the actual robustness capabilities of these al-\ngorithms.\nTable 2 reports the impact values of the policy-\nindependent high-sensitivity directions introduced to the\nvanilla trained deep reinforcement learning policies and\nthe state-of-the-art adversarially trained deep reinforcement\nlearning policies for both SA-DDQN and RADIAL. Note\nthat the hyperparameters for Table 2 are identical to the hy-\nperparameters in Table 1 for consistency. Thus, the results\nin Table 2 are not speciﬁcally optimized to affect adversarial\ntraining. However, Figure 2 reports the effect of varying the\namount of movement along policy-independent non-robust\ndirections, where α stands for contrast, β stands for bright-\nness, and κ for the level of artifacts caused by the discrete\ncosine transform. Intriguingly, as these parameters for high-\nsensitivity directions are varied Figure 2 demonstrates that\nsimple vanilla trained deep reinforcement learning policies\nare more robust compared to state-of-the-art adversarially\nTable 2: The effects of moving along policy-independent high-sensitivity directions in state-of-the-art adversarially trained\n(SA-DDQN and RADIAL) and vanilla trained deep reinforcement learning policy manifolds.\nEnvironment\nBankHeist\nPong\nTraining Method\nSA-DDQN\nRADIAL\nVanilla Trained\nSA-DDQN\nRADIAL\nVanilla Trained\nB&C (I)\n0.881±0.010\n0.959±0.002\n0.971±0.030\n1.0±0.000\n1.0±0.000\n0.996±0.009\nDiscrete Cosine Transform Artifacts (I)\n0.960±0.0014\n1.0±0.000\n0.984±0.013\n1.0±0.000\n1.0±0.000\n0.962±0.032\nPerspective Transform (I)\n1.0±0.000\n1.0±0.000\n1.0±0.003\n0.992±0.0034\n1.0±0.000\n0.996±0.009\nBlurred Observations (I)\n0.003±0.002\n0.985±0.003\n0.983±0.009\n0.805±0.123\n0.901±0.021\n1.0±0.000\nRotation (I)\n1.0±0.000\n0.992±0.000\n1.0±0.004\n1.0±0.000\n1.0±0.000\n0.99±0.015\nShifting (I)\n1.0±0.000\n1.0±0.000\n0.989±0.005\n1.0±0.000\n1.0±0.000\n1.0±0.000\nFigure 2: The performance drop results when moved along\npolicy-independent high-sensitivity directions of the state-\nof-the-art adversarially trained deep reinforcement learn-\ning policy manifold and vanilla trained deep reinforcement\nlearning policy manifold with varying the degrees of discrete\ncosine transform artifacts, brightness, rotation, and contrast.\ntrained ones. For instance, modifying brightness with β in\nthe range 3.1 to 20.0 causes impact close to 1.0 (i.e. total\ncollapse of the policy) for the adversarially trained policy,\nbut has negligible impact on the vanilla trained policy.\nThe results in Figure 2 demonstrate that, across a wide\nrange of parameters, adversarially trained neural policies\nare less robust to natural directions innate to the MDP than\nvanilla trained policies. This occurs despite the fact that the\ncentral purpose of adversarial training is to increase robust-\nness to imperceptible perturbations, where imperceptibility\nis measured by ℓp-norm. Our results indicate that an increase\nin robustness to ℓp-norm bounded perturbations can come\nat the cost of a loss in robustness to other natural types of\nimperceptible high-sensitivity directions. These results call\ninto question the use of adversarial training for the creation\nof robust deep reinforcement learning policies, and in par-\nticular the use of ℓp-norm bounds as a metric of impercepti-\nbility.\nThe fact that adversarial training fails to provide robust-\nness has manifold implications. In particular, from the secu-\nrity point of view the effort put into making robust and reli-\nable policies has been misdirected, resulting in policies that\nare in fact less robust than simple vanilla training. From the\nalignment perspective, while adversarial training is built to\ntarget and make policies safe against adversarial directions,\nit actually caused these policies to be misaligned with human\nperception. In terms of foundational understanding of the\npolicies that are being built, our paper brings the term “ro-\nBase\nC&W\nB&C\nBlur\nRotate\nShift\nPT\nDCT\nFigure 3: Up: Fs(u, v) for BankHeist. Down: Fs(u, v) for\nRiverraid. Columns: base state observation, the Carlini &\nWagner formulation, brightness and contrast, blurred obser-\nvations, rotation, shifting, perspective transformation, dis-\ncrete cosine transform artifacts.\nbustness” into question. The decrease in resilience to overall\ndistributional shift that “certiﬁed robust” adversarial training\nmethods encounter demonstrates the need for further inves-\ntigation into how robustness should be deﬁned.\n5\nThe Frequency Spectrum of the\nHigh-sensitivity Directions\nIn this section we provide frequency analysis of the policy\ndependent worst-case high-sensitivity directions and policy-\nindependent high-sensitivity directions intrinsic to the high\ndimensional state representation MDP. The purpose of this\nanalysis is to provide quantitative evidence that policy-\nindependent high-sensitivity directions cover a broader por-\ntion of the spectrum; thus, provide a broader perspective\non robustness than policy dependent adversarial directions\nalone. In particular, the results in Figure 4 and 3 demon-\nstrates how each direction has distinctly different effects\nin the Fourier spectrum, both policy dependent and policy-\nindependent. In more detail, the frequency spectrum is\nFs(u, v) = 1\nIJ\nI−1\nX\ni=0\nJ−1\nX\nj=0\nˆs(i, j)e−j2π(ui/I+vj/J)\n(5)\nwhere ˆs = (s + ξ(s, π)). Furthermore, we quantify these\neffects by measuring, for each type of high-sensitivity di-\nrection, the change in total Fourier energy at each spatial\nfrequency level.\nE(f) =\nX\nu,v\nmax{u,v}=f\n|Fs(u, v)|2\n(6)\nIn Figure 3 we show the Fourier spectrum of the base state\ns and the states moved towards high sensitivity directions\nfrom the base states ˆs with both policy-independent adver-\nsarial directions (Carlini and Wagner 2017), and the high-\nsensitivity directions intrinsic to the MDP. In these spec-\ntrums the magnitude of the spatial frequencies increases by\nFigure 4: Total energy E(f) spectrum with various perturba-\ntions: worst-case directions (C&W), discrete cosine trans-\nform artifacts, perspective transformation, brightness and\ncontrast, shifting, rotation in RiverRaid.\nmoving outward from the center, and the center of the image\nrepresents the Fourier basis function where spatial frequen-\ncies are zero. To investigate which type of high-sensitivity\ndirections occupy which band in the Fourier domain we\ncompute total energy E(f) for all basis functions whose\nmaximum spatial frequency is f. Hence, Figure 4 shows the\npower spectral density of the base state compared to states\nthat diverge from base states along the high-sensitivity direc-\ntion computed via Algorithm 1 for both policy-independent\nhigh-sensitivity directions and policy dependent adversarial\ndirections Carlini and Wagner (2017).\nAside from outlining our methodology, Section 5 serves\nthe purpose of explaining results obtained in Section 4.\nIn particular, training techniques (e.g. adversarial training)\nsolely focusing on building robustness towards high spatial\nfrequency corruptions become more vulnerable towards cor-\nruptions in a different band of the spectrum. Figure 4 demon-\nstrates that each policy-independent high-sensitivity direc-\ntion occupies a different particular band in the frequency\ndomain. In more detail, while the policy dependent adver-\nsarial directions increase higher frequencies, the artifacts\ncaused by discrete cosine transform decreases the magnitude\nof the high frequency band. Along this line both the linear\ntransformation described in 3 and the geometric transforma-\ntion described in 4 decreases the magnitude of the low fre-\nquency band. The fact that Figure 4 demonstrates that high-\nsensitivity directions indeed capture a broader set of direc-\ntions in the frequency domain assists in providing a wider\nnotion of robustness compared to solely relying on worst-\ncase distributional shifts.\n6\nExperimental Details\nIn our experiments the vanilla trained deep neural policies\nare trained with Deep Q-Network with Double Q-learning\nproposed by (Hasselt, Guez, and Silver 2016) with pri-\noritized experience replay (Schaul et al. 2016), and the\nadversarially trained deep neural policies are trained via\nthe theoretically justiﬁed State-Adversarial MDP modelled\nState-Adversarial Double Deep Q-Network (SA-DDQN),\nand with RADIAL (see Section 4) with prioritized experi-\nence replay (Schaul et al. 2016) with the OpenAI Gym wrap-\nper version (Brockman et al. 2016) of the Arcade Learning\nEnvironment (Bellemare et al. 2013). Note that all of the\nexperiments are conducted in policies trained with high di-\nmensional state representations. To be able to compare be-\ntween different algorithms and different games the perfor-\nmance degradation of the deep reinforcement learning pol-\nicy is deﬁned as the normalized impact of an adversary on\nthe agent:\nI = Scoreclean −Scoreadv\nScoreclean −Scoreﬁxed\nmin\n.\n(7)\nScoreﬁxed\nmin is a ﬁxed minimum score for a game, Scoreadv and\nScoreclean are the scores of the agent with and without any\nmodiﬁcation to the agent’s observations system respectively.\nAll of the results reported in the paper are from 10 inde-\npendent runs. In all of our tables and ﬁgures we include the\nmeans and the standard error of the mean values. More re-\nsults on the issues discussed in Section 5 are provided in\nthe full version of the paper with additional high-sensitivity\nanalysis of policy gradient techniques, visualizations of the\nbase states and moving along the high-sensitivity directions\nintrinsic to the MDP.\n7\nConclusion\nIn this paper we focused on probing the deep neural pol-\nicy decision boundary via both policy dependent speciﬁcally\noptimized worst-case high-sensitivity directions and policy-\nindependent high-sensitivity directions innate to the high di-\nmensional state representation MDPs. We compared these\nworst-case adversarial directions computed via the-state-of-\nthe art techniques with policy-independent ingrained direc-\ntions in the Arcade Learning Environment (ALE). We ques-\ntioned the imperceptibility notion of the ℓp-norm bounded\nadversarial directions, and demonstrated that the states with\nminimal ingrained high-sensitivity directions are more per-\nceptually similar to the base states compared to adversarial\ndirections. Furthermore, we demonstrated that the fact that\nthe policy-independent high-sensitivity directions achieve\nhigher impact on policy performance with lower perceptual\nsimilarity distance without having access to the policy train-\ning details, real time access to the policy’s memory and per-\nception system, and computationally demanding adversarial\nformulations to compute simultaneous perturbations is evi-\ndence that high-sensitivity directions are naturally abundant\nin the deep reinforcement learning policy manifold. Most\nimportantly, we show that state-of-the-art methods proposed\nto solve robustness problems in deep reinforcement learn-\ning are more fragile compared to vanilla trained deep neural\npolicies. We argued for the signiﬁcance of the interpretations\nof robustness in terms of the bias it creates in future research\ndirections. Further, while we highlighted the importance of\ninvestigating the robustness of trained deep neural policies\nin a more diverse spectrum, we believe our study can pro-\nvide a basis for understanding intriguing properties of the\ndeep reinforcement learning decision boundary and can be\ninstrumental in building more robust and generalizable deep\nneural policies.\nReferences\nBellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.\n2013. The arcade learning environment: An evaluation plat-\nform for general agents. Journal of Artiﬁcial Intelligence\nResearch., 253–279.\nBrockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.;\nSchulman, J.; Tang, J.; and Zaremba, W. 2016. Openai gym.\narXiv:1606.01540.\nCarlini, N.; and Wagner, D. 2017. Towards Evaluating the\nrobustness of neural networks. In 2017 IEEE Symposium on\nSecurity and Privacy (SP), 39–57.\nGleave, A.; Dennis, M.; Wild, C.; Neel, K.; Levine, S.; and\nRussell, S. 2020. Adversarial Policies: Attacking Deep Re-\ninforcement Learning. International Conference on Learn-\ning Representations ICLR.\nGoodfellow, I.; Shelens, J.; and Szegedy, C. 2015. Explaning\nand Harnessing Adversarial Examples. International Con-\nference on Learning Representations.\nHasselt, H. v.; Guez, A.; and Silver, D. 2016. Deep Rein-\nforcement Learning with Double Q-Learning. Association\nfor the Advancement of Artiﬁcial Intelligence (AAAI).\nHuan, Z.; Chen, H.; Xiao, C.; Li, B.; Liu, M.; Boning, D. S.;\nand Hseh, C. 2020. Robust Deep Renforcement Learning\nagainst Adversaral Perturbations on State Observations. In\nLarochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and\nLin, H., eds., Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nHuang, S.; Papernot, N.; Goodfellow, Y., Ian an Duan; and\nAbbeel, P. 2017. Adversarial Attacks on Neural Network\nPolicies. Workshop Track of the 5th International Confer-\nence on Learning Representations.\nIandola, F. N.; Han, S.; Moskewicz, M. W.; Ashraf, K.;\nJ. Dally, W.; and Keutzer, K. 2016. SqueezeNet: AlexNet-\nlevel accuracy with 50x fewer parameters and¡ 0.5 MB\nmodel size. arXiv preprint arXiv:1602.07360.\nKorkmaz, E. 2020. Nesterov Momentum Adversarial Per-\nturbations in the Deep Reinforcement Learning Domain. In-\nternational Conference on Machine Learning, ICML 2020,\nInductive Biases, Invariances and Generalization in Rein-\nforcement Learning Workshop.\nKorkmaz, E. 2021a. Inaccuracy of State-Action Value Func-\ntion for Non-Optimal Actions in Adversarially Trained Deep\nNeural Policies. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR)\nWorkshops, 2323–2327.\nKorkmaz, E. 2021b. Investigating Vulnerabilities of Deep\nNeural Policies. Conference on Uncertainty in Artiﬁcial In-\ntelligence (UAI).\nKorkmaz, E. 2022a.\nDeep Reinforcement Learning Poli-\ncies Learn Shared Adversarial Features Across MDPs. AAAI\nConference on Artiﬁcial Intelligence.\nKorkmaz, E. 2022b. The Robustness of Inverse Reinforce-\nment Learning.\nKorkmaz, E. 2022c. Spectral Robustness Analysis of Deep\nImitation Learning.\nKos, J.; and Song, D. 2017. Delving Into Adversarial At-\ntacks on Deep Policies. International Conference on Learn-\ning Representations.\nKrizhevsky, A.; Sutskever, I.; and E. Hinton, G. 2012. Im-\nagenet classiﬁcation with deep convolutional neural net-\nworks. Advances in neural information processing systems.\nKurakin, A.; Goodfellow, I.; and Bengio, S. 2016.\nAd-\nversarial examples in the physical world.\narXiv preprint\narXiv:1607.02533.\nMadry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and\nVladu, A. 2018.\nTowards Deep Learning Models Resis-\ntant to Adversarial Attacks.\nIn 6th International Confer-\nence on Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference Track Pro-\nceedings. OpenReview.net.\nMandlekar, A.; Zhu, Y.; Garg, A.; Fei-Fei, L.; and Savarese,\nS. 2017. Adversarially Robust Policy Learning: Active Con-\nstruction of Physically-Plausible Perturbations. In Proceed-\nings of the IEEE/RSJ International Conference on Intelli-\ngent Robots and Systems (IROS), 3932–3939.\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-\nness, J.; Bellemare, a. G.; Graves, A.; Riedmiller, M.; Fidje-\nland, A.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.;\nAntonoglou; King, H.; Kumaran, D.; Wierstra, D.; Legg, S.;\nand Hassabis, D. 2015. Human-level control through deep\nreinforcement learning. Nature, 518: 529–533.\nPattanaik, A.; Tang, Z.; Liu, S.; and Gautham, B. 2018.\nRobust Deep Reinforcement Learning with Adversarial At-\ntacks.\nIn Proceedings of the 17th International Confer-\nence on Autonomous Agents and MultiAgent Systems, 2040–\n2042.\nPinto, L.; Davidson, J.; Sukthankar, R.; and Gupta, A. 2017.\nRobust Adversarial Reinforcement Learning. International\nConference on Learning Representations ICLR.\nSchaul, T.; Quan, J.; Antonogloua, I.; and Silver, D. 2016.\nPrioritized Experience Replay. International Conference on\nLearning Representations (ICLR).\nSchrittwieser, J.; Antonoglou, I.; Hubert, T.; Simonyan, K.;\nSifre, L.; Schmitt, S.; Guez, A.; Lockhart, E.; Hassabis, D.;\nGraepel, T.; Lillicrap, T. P.; and Silver, D. 2020. Mastering\nAtari, Go, chess and shogi by planning with a learned model.\nNature, 588(7839): 604–609.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv:1707.06347v2 [cs.LG].\nSimonyan, K.; and Zisserman, A. 2015. Very deep convolu-\ntional networks for large-scale image recognition. Interna-\ntional Conference on Learning Representations, ICLR.\nSun, J.; Zhang, T.; Xiafei, L.; Ma, X.; Zheng, Y.; Chen, K.;\nand Liu, Y. 2020. Stealthy and efﬁcient advrsarial attacks\naganst deep reinforcement learning. Association for the Ad-\nvancement of Artiﬁcial Intelligence (AAAI).\nSzegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan,\nD.; Goodfellow, I.; and Fergus, R. 2014. Intriguing proper-\nties of neural networks. In Proceedings of the International\nConference on Learning Representations (ICLR).\nTram`er, F.; Kurakin, A.; Papernot, N.; Goodfellow, I. J.;\nBoneh, D.; and McDaniel, P. D. 2018. Ensemble Adver-\nsarial Training: Attacks and Defenses. In 6th International\nConference on Learning Representations, ICLR 2018, Van-\ncouver, BC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings. OpenReview.net.\nVinyals, O.; Babuschkin, I.; Czarnecki, W. M.; Mathieu, M.;\nDudzik, A.; Chung, J.; Choi, D. H.; Powell, R.; Ewalds, T.;\nGeorgiev, P.; Oh, J.; Horgan, D.; Kroiss, M.; Danihelka, I.;\nHuang, A.; Sifre, L.; Cai, T.; Agapiou, J. P.; Jaderberg, M.;\nVezhnevets, A. S.; Leblond, R.; Pohlen, T.; Dalibard, V.;\nBudden, D.; Sulsky, Y.; Molloy, J.; Paine, T. L.; G¨ulc¸ehre,\nC¸ .; Wang, Z.; Pfaff, T.; Wu, Y.; Ring, R.; Yogatama, D.;\nW¨unsch, D.; McKinney, K.; Smith, O.; Schaul, T.; Lillicrap,\nT. P.; Kavukcuoglu, K.; Hassabis, D.; Apps, C.; and Silver,\nD. 2019. Grandmaster level in StarCraft II using multi-agent\nreinforcement learning. Nature, 575(7782): 350–354.\nXie, C.; and Yuille, A. L. 2020.\nIntriguing Properties of\nAdversarial Training at Scale.\nIn 8th International Con-\nference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\nYen-Chen, L.; Zhag-Wei, H.; Lao, Y.-H.; Shih, M.-L.; ing\nYu Lu; and Sun, M. 2017. Tactics of Advrsarial Attack on\nDeep Reinforcement Learning Agnts. Proceedings of the\nTwenty-Sixth International Joint Conference on Artiﬁcial In-\ntelligence, 3756–3762.\nZhang, R.; Isola, P.; Efros, A.; Shechtman, E.; and Wang, O.\n2018. The Unreasonable Effectiveness of Deep Features as\na Perceptual Metric. Conference on Computer Vision and\nPattern Recognition (CVPR).\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CR",
    "stat.ML"
  ],
  "published": "2023-01-17",
  "updated": "2023-01-17"
}