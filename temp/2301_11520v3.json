{
  "id": "http://arxiv.org/abs/2301.11520v3",
  "title": "SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning",
  "authors": [
    "Dongseok Shim",
    "Seungjae Lee",
    "H. Jin Kim"
  ],
  "abstract": "As previous representations for reinforcement learning cannot effectively\nincorporate a human-intuitive understanding of the 3D environment, they usually\nsuffer from sub-optimal performances. In this paper, we present Semantic-aware\nNeural Radiance Fields for Reinforcement Learning (SNeRL), which jointly\noptimizes semantic-aware neural radiance fields (NeRF) with a convolutional\nencoder to learn 3D-aware neural implicit representation from multi-view\nimages. We introduce 3D semantic and distilled feature fields in parallel to\nthe RGB radiance fields in NeRF to learn semantic and object-centric\nrepresentation for reinforcement learning. SNeRL outperforms not only previous\npixel-based representations but also recent 3D-aware representations both in\nmodel-free and model-based reinforcement learning.",
  "text": "SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\nDongseok Shim * 1 Seungjae Lee * 2 3 H. Jin Kim 1 2 3\nAbstract\nAs previous representations for reinforcement\nlearning cannot effectively incorporate a human-\nintuitive understanding of the 3D environment,\nthey usually suffer from sub-optimal perfor-\nmances. In this paper, we present Semantic-aware\nNeural Radiance Fields for Reinforcement Learn-\ning (SNeRL), which jointly optimizes semantic-\naware neural radiance fields (NeRF) with a convo-\nlutional encoder to learn 3D-aware neural implicit\nrepresentation from multi-view images. We in-\ntroduce 3D semantic and distilled feature fields\nin parallel to the RGB radiance fields in NeRF\nto learn semantic and object-centric representa-\ntion for reinforcement learning. SNeRL outper-\nforms not only previous pixel-based representa-\ntions but also recent 3D-aware representations\nboth in model-free and model-based reinforce-\nment learning.\n1. Introduction\nDeveloping agents that can achieve complex control tasks\ndirectly from image inputs has been a long-standing prob-\nlem in reinforcement learning (RL). Previous works over\nthe past few years have made notable progress in the data\nefficiency of learning visual control problems. The most\nchallenging part of solving visual control tasks is obtain-\ning the low-dimensional latent representations from high-\ndimensional observations. To this end, they pre-train the\nencoder in various ways such as unsupervised representa-\ntion learning via image reconstruction using offline datasets\n(Finn et al., 2016; Kulkarni et al., 2019; Islam et al., 2022),\ncontrastive learning (Zhan et al., 2022), reconstructing task\ninformation (Yang & Nachum, 2021; Yamada et al., 2022),\nand training multi-view consistency (Dwibedi et al., 2018).\n*Equal contribution; Order was determined by coin flip\n1Interdisciplinary Program in AI, Seoul National University\n2Aerospace Engineering, Seoul National University 3ASRI, AIIS,\nSeoul National University.\nCorrespondence to: H. Jin Kim\n<hjinkim@snu.ac.kr>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\nNeRF\nEncoder\nAction\nReward\nRGB field\nFeature field\nRL\nagent\nùíµ\nSemantic field\nMulti-view Image\nFigure 1.\nSemantic-aware NeRF for reinforcement learning.\nWe present SNeRL, a reinforcement learning framework that\nlearns 3D-aware representation with a convolutional encoder and\nsemantic-aware NeRF decoder. The latent vectors from the en-\ncoder are propagated to the policy network to generate an action\nfor RL agents.\nOther approaches utilize joint learning of auxiliary unsuper-\nvised tasks (Laskin et al., 2020b; Schwarzer et al., 2020),\nand data-augmented reinforcement learning (Laskin et al.,\n2020a; Yarats et al., 2021).\nWhile a number of works have been proposed to improve\nthe data efficiency in visual control problems, the majority\nof the encoders trained from those methods have limited\ncapability in obtaining 3D structural information and lack\nequivariance to 3D transformations. Such limitations come\nfrom ignoring 3D structural information and learning visual\nrepresentation from a single-view observation.\nRecently, there have been attempts to consider 3D informa-\ntion of the environment in robot control and manipulation\n(Li et al., 2022; Driess et al., 2022) by learning implicit\nspatial representation via neural radiance fields (NeRF)\n(Mildenhall et al., 2020). They map pixel-level multi-view\nobservations of a scene to a latent vector through an autoen-\ncoder structure, where the NeRF decoder provides 3D struc-\ntured neural scene representation via RGB self-supervision\nfor each view.\nEven though the aforementioned pioneers achieved better\nperformance compared to the previous RL algorithms with\na single-view observation, they still did not take full advan-\ntage of 3D-aware representation learning. It is because those\n1\narXiv:2301.11520v3  [cs.LG]  31 May 2023\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\nmethods only exploit RGB supervision to train NeRF, which\nmakes it difficult for the encoder to learn object-centric\nor semantic representation for RL downstream tasks. Al-\nthough NeRF-RL (Driess et al., 2022) proposes composi-\ntional NeRF to mitigate such limitations, the RL agents in\nNeRF-RL require object-individual masks during training\nand deployment to utilize semantic representations, which\nis quite unrealistic.\nIn this work, we propose Semantic-aware Neural Radiance\nFields for Reinforcement Learning (SNeRL) which learns\nboth 3D-aware semantic and geometric representation for\nRL agents in a 3-dimensional environment. First of all,\nour proposed method learns 3D-aware semantic represen-\ntation by predicting 3D semantic fields with ground truth\nlabels. As a result, SNeRL enables downstream visual con-\ntrol tasks without object-individual masks and addresses\nthe limitation of the prior work (Driess et al., 2022). Also,\nto capture further fine-grained features that could not be\nfully expressed in semantic fields and to take advantage of\ndata-driven approaches, we employ an off-the-shelf feature\ndescriptor (Caron et al., 2021) as a teacher network and\nlearn to predict feature fields via a distillation method such\nas Kobayashi et al. (2022).\nWe also introduce a multi-view adaptation of recent self-\npredictive representation learning (Chen & He, 2021) as an\nauxiliary task which further improves the performance of\nSNeRL. In the proposed auxiliary task, SNeRL computes\nthe target representation by utilizing the observations from\ndifferent camera views in the same timestep to learn spatially\nconsistent representation.\nOur proposed SNeRL outperforms not only the previ-\nous single-view representation learning algorithms for RL\n(Laskin et al., 2020b; Yarats et al., 2021) but also the state-of-\nthe-art method with multi-view observations (Driess et al.,\n2022) in four different visual control tasks.\nTo sum up, our contribution can be summarized as follows:\n‚Ä¢ We present SNeRL, a framework that utilizes NeRF\nwith semantic and distilled feature fields to learn 3D-\naware semantic representation for reinforcement learn-\ning.\n‚Ä¢ We validate the effectiveness of SNeRL both with\nmodel-free and model-based methods. To the best of\nour knowledge, SNeRL is the first work that leverages\nsemantic-aware representations without object masks\nin RL downstream tasks. Also, this is the first study to\nutilize 3D-aware representations to model-based RL.\n‚Ä¢ The proposed SNeRL outperforms the previous single\nand multi-view image-based RL algorithms in four dif-\nferent 3D environments from Meta-world. In addition,\nauxiliary self-predictive representation learning with\nmulti-view observations proposed for spatially consis-\ntent representation can enable further improvements.\n2. Related Work\n2.1. 3D Scene Representation Learning\nTo learn 3D-aware representation from a single view image,\nthe previous methods exploit standard convolutional autoen-\ncoder architecture conditioned by the camera poses, which\ngenerates scenes from arbitrary views with either determinis-\ntic (Tatarchenko et al., 2016; Worrall et al., 2017) or stochas-\ntic (Eslami et al., 2018) latent vectors. Recently, neural radi-\nance fields (NeRF) have achieved an exceptional progress in\nunderstanding 3D scenes and synthesizing novel views. Fol-\nlowing, some approaches propose latent-conditioned NeRF\n(Martin-Brualla et al., 2021; Yu et al., 2021; Wang et al.,\n2021), but the major objective of the aforementioned meth-\nods is improving the quality of synthesized images rather\nthan extracting time-variant latent vectors with 3D dynamic\nscene understanding from multi-view inputs. In this paper,\nwe leverage the autoencoder with convolutional encoder and\nNeRF-style decoder (Li et al., 2022; Driess et al., 2022) so\nthat the encoder can extract 3D-aware representation from\nmulti-view inputs for RL downstream tasks.\n2.2. Representation Learning for RL\nThe RL frameworks with image inputs typically have an\nencoder, which maps high-dimensional observations to a\nlow-dimensional latent vector. RL agent is trained over\nthe latent state space to maximize its objective functions,\ne.g., the total discounted reward for each episode. While\na number of works have made significant advancements, it\nstill remains a challenging open problem.\nTo address the sample inefficiency of image-based RL, prior\nworks adopt various data-augmentation techniques (Laskin\net al., 2020a; Yarats et al., 2021), contrastive learning with\ndata augmentation (Laskin et al., 2020b; Schwarzer et al.,\n2020; Stooke et al., 2021; Liu & Abbeel, 2021; Zhan et al.,\n2022), representation learning from image reconstruction\n(Islam et al., 2022; Kulkarni et al., 2019), or task informa-\ntion reconstruction (Yang & Nachum, 2021; Yamada et al.,\n2022). Other approaches propose to capture the relations\nbetween multi-view data (Dwibedi et al., 2018; Kinose et al.,\n2022; Sermanet et al., 2018) or keypoints (Manuelli et al.,\n2020). There are also some approaches leveraging transi-\ntion sequence data (Hansen et al., 2020; You et al., 2022),\nor pre-training with offline image-based RL (Wang et al.,\n2022). Unfortunately, these works have limited capability in\nlearning 3D-structural information and could not obtain an\nintuitive understanding of the 3D environments that humans\nhave because of the 2D bias that 2D convolutional neural\nnetworks have.\n2\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\nIn recent, there have been attempts to learn the 3D struc-\nture of the real world (Li et al., 2022; Driess et al., 2022).\nLi et al. (2022) firstly proposes autoencoder with convolu-\ntional encoder and NeRF (Mildenhall et al., 2020) decoder\nto control the visuomotor with learned dynamics model\nand model-predictive control (MPC). Following, NeRF-RL\n(Driess et al., 2022) extends the prior study and firstly in-\ntroduces NeRF-based architecture to the general model-free\nRL framework. However, they could not learn semantic\nfeatures due to the limited RGB supervision with na¬®ƒ±ve\nNeRF. To learn object-centric representation only with RGB\nsupervision, NeRF-RL presents compositional NeRF with\nobject-individual masks, but requiring masks during the\ndeployment of RL agents seems to be a strong assumption.\nIn this paper, we propose SNeRL which learns both geo-\nmetric and semantic information with RGB, semantic, and\ndistilled feature supervision for RL downstream tasks with-\nout any object masks during the inference phase.\n3. Preliminaries\n3.1. Neural Radiance Fields\nThe concept of neural radiance fields (NeRF) (Mildenhall\net al., 2020) is to represent the 3D scene with learnable\nand continuous volumetric fields fŒ∏. Specifically, at any\n3D world coordinate x ‚ààR3 and unit viewing direction\nd ‚ààR3, fŒ∏ estimates the differntiable volume density œÉ\nand RGB color c: fŒ∏(x, d) = (œÉ, c). Let the camera ray of\nthe pixel in the camera coordinate be r = o + td, where o\nindicates the camera origin. The corresponding pixel value\nfrom an arbitrary view can be rendered through volumetric\nradiance fields as:\nC(r) =\nZ tf\ntn\nT(t)œÉ(t)c(t)dt\n(1)\nwhere T(t) = exp(‚àí\nR t\ntn œÉ(s)ds) and tn and tf indicate\npre-defined lower and upper bound of the depth respectively.\nThen, fŒ∏, which is usually formulated with MLP, is opti-\nmized through pixel-wise RGB supervision from multiple\nviews as:\nL =\nX\ni,j\n|| ÀÜC(ri,j) ‚àíC(ri,j)||2\n2,\n(2)\nwhere ri,j indicates ray j from images of ith view. ÀÜC and\nC represents the rendered volumetric fields into 2D image\nand ground truth pixel value respectively.\n3.2. Reinforcement Learning\nWe consider a finite-horizon Markov Decision Process\n(MDP) M = (O, A, T , R, Œ≥), where O denotes the high-\ndimensional observation space (image pixels), A the action\nspace, T (o‚Ä≤|o, a) the transition dynamics (o, o‚Ä≤ ‚ààO, a ‚àà\nA), R : O √ó A ‚ÜíR the reward function, and Œ≥ ‚àà[0, 1)\nthe discount factor. Following the general idea of learn-\ning RL downstream tasks with pre-trained scene repre-\nsentations, we consider an encoder ‚Ñ¶: O ‚ÜíZ that\nmaps and high-dimensional observation o ‚ààO to a low-\ndimensional latent state z ‚ààZ on which an RL agent\noperates. To learn how to succeed in downstream tasks,\nthe RL policy œÄŒ∏(a ‚ààA|z = ‚Ñ¶(o)) maximizes the total\ndiscounted reward PH‚àí1\nt=0\n= Œ≥tR(ot, at) of trajectories\nœÑi = (z0, o0, ..., zH, oH)i.\n4. Method\nIn this section, we demonstrate the details of SNeRL which\nconsists of a multi-view convolutional image encoder and\na latent-conditioned NeRF decoder to learn the 3D-aware\nrepresentation. Compared to the previous method (Driess\net al., 2022) which also proposes NeRF supervision for RL,\nSNeRL is capable of extracting object-centric or semantic\nrepresentation without any object-individual masks during\ndeployment. The pre-trained image encoder is exploited as a\nfeature extractor for downstream RL tasks, and the overview\nof SNeRL framework is depicted in Figure 2.\n4.1. Multi-view Encoder\nSimilar to Li et al. (2022), we adopt the multi-view en-\ncoder ‚Ñ¶which fuses the observations from multiple cam-\nera views together to learn a single latent vector z for\nRL tasks. The encoder takes the pixel-level observations\noi ‚ààRH√óW √ó3, and the corresponding camera projection\nmatrices Ki ‚ààR3√ó4 captured from V different camera view\nas inputs, i.e., i = 1 ¬∑ ¬∑ ¬∑ V . To generate z ‚ààZ from the in-\nputs, a convolutional network ECNN first extracts viewpoint-\ninvariant features from each image. The features from dif-\nferent camera views are channel-wise concatenated with\ntheir corresponding (flattened) camera projection matrices\nto reflect the viewpoint information to the following feature\nvectors. Then, the concatenated vectors are passed through\nMLP layers, gMLP, to produce mid-level viewpoint-aware\nencodings. Lastly, the feature encodings from different cam-\nera views are averaged to generate a single encoding, and\nthe averaged feature encoding is projected to the latent space\nZ with the latent encoder hMLP as follows:\nz = ‚Ñ¶(o1:V , K1:V )\n= hMLP( 1\nV\nV\nX\ni=1\ngMLP(ECNN(oi), Ki))\n(3)\n4.2. Semantic-aware NeRF Decoder\nTo inject 3D structural information into the latent vector\nz, we leverage a latent-conditioned NeRF architecture (Yu\n3\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\nœÉ\nx\nd\nz\nfŒ∏\nyv\nÀúyv‚Ä≤\nCosine\nSimilarity Loss\nEnc\nEnc\nEnc\nPrediction\nhead\nSemantic\nsupervision\nRGB\nsupervision\nFeature\nsupervision\nStage 1. Pre-train multi-view encoder with semantic-aware NeRF\nEnc\nEnc\nEnc\nEnvironment\nBuffer\nShared Encoder\nActor\nCritic\nOff-the-shelf RL agent\nStage 2. Train downstream off-the-shelf RL agent\nShared Encoder\nLatent\nembedding\nNeural\nrendering\ns|z, x\nf|z, x\nc|z, x, d\nFigure 2. SNeRL Overview. SNeRL consists of two stages, which are pre-training NeRF-based autoencoder and fine-tuning to the\ndownstream RL tasks, respectively. With observations from three different camera views, an encoder produces a single latent vector z, and\na decoder with neural rendering function fŒ∏ takes the position x, viewing direction d in the 3D coordinates and z as inputs to synthesize\nthree different fields in the arbitrary views. An auxiliary multi-view self-prediction loss is applied to enable view-invariant representation.\nThen, the encoder and the decoder are jointly optimized in a supervised manner with an offline dataset. The pre-trained encoder is utilized\nas a feature extractor to train the policy with off-the-shelf RL algorithms.\net al., 2021; Martin-Brualla et al., 2021; Wang et al., 2021)\nfor the decoder. The difference between previous latent-\nconditioned NeRF and our proposed SNeRL is that the\nneural rendering function fŒ∏ from SNeRL not only synthe-\nsizes novel views with RGB pixel value c but also with the\nsemantic label s (Zhi et al., 2021; Fu et al., 2022; Kundu\net al., 2022) and high-dimensional distilled features f from\nthe large-scale teacher network (Kobayashi et al., 2022) as\nfollows:\nc = fŒ∏(z, x, d), s, f = fŒ∏(z, x)\n(4)\nBy estimating three different radiance fields (semantic, fea-\nture, and RGB), the latent vector z is jointly optimized to\nlearn the geometric and semantic representations of the 3D\nenvironment. Unlike RGB value c which is dependent on\nboth the position x and the viewing direction d, we formu-\nlate the semantic label and distilled feature to be invariant\nto the viewing direction d because the inherent properties\nof the scene or the object do not change according to the\ndirection of the camera ray.\nAs SNeRL predicts three different fields, RGB, semantic,\nand distilled feature, by adding field-wise branches, they\nshare the neural rendering function fŒ∏ until estimating the\ndensity œÉ. It indicates that three radiance fields have the\nsame accumulated transmittance T(t) at depth t ‚àà[tn, tf]\nalong the ray r = o + td as\nT(t) = exp(‚àí\nZ t\ntn\nœÉ(r(s))ds).\n(5)\nFor rendering the RGB field, we follow the same training\nframework as general latent-conditioned NeRF (Yu et al.,\n2021; Martin-Brualla et al., 2021; Wang et al., 2021), which\noptimizes the neural rendering function fŒ∏ via pixel-wise\nRGB supervision. RGB supervision enables the encoder\nto extract geometric features from the observed environ-\nment by learning the RGB and density distribution in the\n3-dimensional space. The rendered pixel value ÀÜC(r) can be\ncalculated as\nÀÜC(r) =\nZ tf\ntn\nT(t)œÉ(r)c(r, d)dt,\n(6)\nand the loss function for RGB field, LRGB, can be formu-\nlated with simple L2 loss between the rendered ÀÜC(r) and\nthe ground truth pixel colors C(r),\nLRGB =\nX\ni,j\n|| ÀÜC(ri,j) ‚àíC(ri,j)||2\n2,\n(7)\nwhere ri,j indicates the camera ray j from the observation\ni, oi.\nUnfortunately, optimizing the encoder only with only an\nRGB reconstruction is difficult to capture the semantic or\nobject-centric properties of the 3D scene, which are crucial\nfor downstream RL tasks. Therefore, we extend NeRF-\nbased decoder by appending additional branches before\ninjecting the viewing direction d into the rendering function,\nfŒ∏ for semantic segmentation. The rendered semantic labels\n4\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\nÀÜS(r) can be calculated as\nÀÜS(r) =\nZ tf\ntn\nT(t)œÉ(r)s(r)dt\n(8)\nand the loss function for semantic field, Lseg, can be formu-\nlated with the standard cross entropy loss,\nLsem = ‚àí\nX\ni,j\nL\nX\nl=1\nSl(ri,j)log ÀÜSl(ri,j),\n(9)\nwhere ÀÜSl and Sl denote the probability of the ray j in ob-\nservation i belonging to the class l and its corresponding\nground-truth semantic labels, respectively.\nTo capture further fine-grained features that could not be\nfully expressed in semantic fields, SNeRL also synthesizes\ndistilled feature fields (Kobayashi et al. (2022)) that predict\nthe output of a pre-trained feature descriptor in a knowledge-\ndistillation manner (Hinton et al.). It is well known from\nprior literature (Caron et al., 2021) that Vision Transformer\n(ViT) (Dosovitskiy et al., 2020) trained in a self-supervised\nmanner, e.g. DINO (Caron et al., 2021), can work as an\nexcellent feature descriptor which explicitly represents the\nscene layouts such as object boundaries. Since the output\nof the ViT feature descriptor contains high-dimensional\ninformation with different values in all pixels depending\non the geometric relationship or semantic meaning, the pre-\ntrained ViT becomes a good feature descriptor with another\nadvantage from the semantic label.\nTherefore, we take advantage of such benefits to the NeRF-\nbased decoder so that the latent vector z learns high-level\ninformation by distilling the knowledge from ViT teacher\nnetwork which cannot be learned via ground-truth semantic\nsupervision. The distilled feature fields can be rendered as\nÀÜF(r) =\nZ tf\ntn\nT(t)œÉ(r)f(r)dt.\n(10)\nThe loss function for distilled feature field, Lfeat, is for-\nmulated by penalizing the difference between the rendered\nfeatures ÀÜF(r) and the outputs of ViT feature descriptor\nF(o, r) as\nLfeat =\nX\ni,j\n|| ÀÜF(ri,j) ‚àíF(oi, ri,j)||1.\n(11)\nFinally, the total loss function L for jointly optimizing the\nmulti-view encoder and NeRF-based decoder can be for-\nmulated as the linear combination of aforementioned losses\nas:\nL = LRGB + ŒªsemLsem + ŒªfeatLfeat\n(12)\nwhere Œªsem and Œªfeat are set to 0.004 and 0.04, respectively,\nto balance the losses (Zhi et al., 2021; Kobayashi et al.,\n2022). After training, the multi-view encoder ‚Ñ¶is exploited\nas a 3D structural and semantic feature extractor for any\noff-the-shelf downstream RL algorithms.\n4.3. Multi-view Self Predictive Representation\nWe additionally enforce the multi-view self-predictive loss\nto the latent vector z to ensure that the encoder learns the\nviewpoint-invariant representation with observations from\nthe same scene. The randomly sampled observations from\ntwo different camera pose, o1 and o2, are processed by\nthe convolutional feature extractor, ECNN, and the weights\nof the feature extractor are shared between two inputs. A\nfeature from one view, z1, is mapped with a prediction\nnetwork, hpred, to match it to the feature from the other\nview, z2. We formulate the self-predictive loss function D\nwith negative cosine similarity as follows:\nD(p1, z2) = ‚àí\np1\n||p1||2\n¬∑\nz2\n||z2||2\n,\n(13)\nwhere p1 and z2 indicate two output vectors, p1\n‚âú\nhpred(ECNN(o1)) and z2 ‚âúECNN(o2), respectively. We\nassume that z2 is constant and the encoder ECNN only re-\nceives the gradient from p1 following Chen & He (2021).\nThe symmetrized auxiliary representation loss function can\nbe formulated as follows:\nLaux = 1\n2D(p1, z2) + 1\n2D(p2, z1).\n(14)\n5. Experiments\nIn this section, we demonstrate several experiments on the\n3-dimensional environments to explore the effectiveness\nof SNeRL compared to existing state-of-the-art RL algo-\nrithms both in model-free and model-based settings. We\nfix the downstream RL algorithms and adopt Soft Actor-\nCritic (Haarnoja et al., 2018) in the model-free setting and\nDreamer (Hafner et al., 2019) in the model-based setting for\nSNeRL and all the baselines for a fair evaluation.\nEnvironments.\nWe evaluate SNeRL on four visual control\nenvironments based on the MuJoCo (Todorov et al., 2012),\nincluding some complex control tasks that require clever use\nof interactions between the objects to obtain high rewards.\nAll the tasks are performed by a simulated Sawyer robot\nwhich has a single arm and gripper in hand (4-DoF). The\naction space of the Sawyer robot consists of the position\n(x,y,z) of the end-effector and gripper control (open/close).\nThe agent takes 128x128 images from three different camera\nviews as pixel-level inputs and receives dense rewards from\nthe environment provided by Meta-world (Yu et al., 2020).\n‚Ä¢ Window-open-v2 : This environment involves the\nSawyer robot opening a sliding window with a handle.\nThe initial state of the robotic hand is [0, 0.4, 0.2] and\nthe robot receives rewards for pushing the handle and\nopening a window located in [-0.1, 0.785, 0.16].\n5\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\nEpisode Return (x1e3)\nEpisode Return (x1e3)\nEpisode Return (x1e3)\nEpisode Return (x1e3)\nTimestep (√ó 1e5)\nTimestep (√ó 1e5)\nTimestep (√ó 1e5)\nTimestep (√ó 1e5)\nHammer-v2\nWindow-open-v2\nDrawer-open-v2\nSoccer-v2\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n0\n2\n4\n6\n8\n10\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n0\n2\n4\n6\n8\n10\n1.0\n2.0\n3.0\n4.0\n0\n1\n2\n3\n4\n1.0\n2.0\n3.0\n4.0\n0\n1\n2\n3\n4\n5\n6\n7\n8\nFigure 3. Episode returns of the evaluation results. Shading indicates a standard deviation across 4 seeds. The curves are not visible in\nthe Hammer-v2 environment as they overlap each other. Note that SNeRL in this figure is obtained without auxiliary loss in section 4.3\n(multi-view self-predictive presentation), which could enable further improvements in some environments.\n‚Ä¢ Hammer-v2 : The Sawyer robot is supposed to grasp\nthe handle of the hammer, which is generated in a\nrandom position, and hit the head of the nail to drive\nit. The initial state of the robotic hand is generated\nrandomly in {(x, y, z)| ‚àí0.5 ‚â§x ‚â§0.5, 0.4 ‚â§y ‚â§\n1, 0.05 ‚â§z ‚â§0.5}. The robot receives rewards for\npicking up the hammer and inserting the nail into a\npiece of wood.\n‚Ä¢ Drawer-open-v2 : The Sawyer robot is supposed to\nopen a drawer by holding the handle of the drawer and\npulling it. The initial state of the robotic hand is the\nsame as Hammer-v2. The robot receives rewards for\nopening a drawer.\n‚Ä¢ Soccer-v2 : In this task, the Sawyer robot tries to score\nby pushing a soccer ball that is generated in a random\nposition. The initial state of the robotic hand is the\nsame as Hammer-v2. The robot receives rewards for\ntouching the soccer ball and putting it into the net.\nWe refer to Meta-world (Yu et al., 2020) for more details\nincluding the reward function and the range of the random\npositions.\nBaselines.\nWe compare SNeRL to several state-of-the-art\nvisual RL methods and a 3D-aware RL method, which are\nbriefly described below. DrQ-v2 (Yarats et al., 2021) is an\nimproved version of DrQ (Yarats et al., 2020), which solves\nvisual control tasks with data augmentation and scheduled\nexploration noise. CURL (Laskin et al., 2020b) trains RL\nagents with an auxiliary contrastive loss which ensures that\nthe embeddings for data-augmented versions of observa-\ntions match. CURL-multiview is a multi-view adaptation\nof CURL, which utilize 3 different camera views and has\na CNN encoder with the same structure as that of SNeRL.\nCNN-AE uses a standard CNN autoencoder (instead of\nNeRF decoder) to pre-train an encoder using the reconstruc-\ntion loss proposed in Finn et al. (2016). NeRF-RL (Driess\net al., 2022) pre-trains an autoencoder with convolutional\nencoder and na¬®ƒ±ve NeRF-style decoder, without semantic\nand feature supervision.\nWe note that learning downstream RL tasks in CNN-AE and\nNeRF-RL shares the identical method as SNeRL, and they\nuse the same offline dataset collected by random actions and\nthe policies provided by Meta-world (half-and-half mixed).\nWe refer the reader to Appendix B.2 for the experiments on\nother datasets. Also, all the multi-view methods (CURL-\nmultiview, CNN-AE, NeRF-RL, SNeRL) receive the same\nobservations and do not receive per-object masks from the\nenvironment. For the rest of the baselines which operate\non a single view, we choose a single camera position from\n6\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\n(A) Semantic and Distilled Feature Fields\n(B) Multi-view Self Predictive Representation\n(C) SNeRL for Model-based RL\nEpisode Return (x1e3)\nTimestep (√ó 1e5)\nTimestep (√ó 1e5)\nEpisode Return (x1e3)\nWindow-open-v2\nSoccer-v2\nOurs only feature fields\nOurs only semantic fields\nOurs (SNeRL)\nTimestep (√ó 1e5)\nHammer-v2\nTimestep (√ó 1e5)\nOurs + self-predictive loss\nOurs (SNeRL)\nTimestep (√ó 1e5)\nTimestep (√ó 1e5)\nWindow-open-v2\nDrawer-open-v2\nOurs (SNeRL) + Dreamer\nNeRF-RL + Dreamer\nCNN-AE + Dreamer\nWindow-open-v2\nFigure 4. Ablation study. (a): SNeRL with both semantic and feature supervision shows higher performance than the case where only one\nof the two is applied. Also, in relatively simple environments, using only one of the two could be enough to improve the performance\nof the prior work. (b): Additional multi-view self-predictive loss can further improve SNeRL in some environments. (c): Learned\nrepresentations via SNeRL can also be adopted in model-based RL.\nwhich the states of each object can be observed clearly.\n5.1. Experiment Result\nFigure 3 shows the episode returns of SNeRL and baselines\nin 4 different visual control tasks. Thanks to the learned\nobject-centric representation via semantic and distilled fea-\nture supervision, SNeRL consistently outperforms state-\nof-the-art visual RL methods and the prior 3D-aware RL\nmethod (NeRF-RL) in terms of data efficiency and perfor-\nmance.\nSpecifically, the contrastive baselines (CURL, CURL-\nmultiview) and DrQ-v2 could not achieve high returns in\nthe difficult environments (soccer and hammer) even though\nsome of them succeed in the relatively easy environment\n(window). The results also show that pre-training CNN via\nna¬®ƒ±ve reconstruction loss (CNN-AE) with offline data could\nnot succeed in the environments at all. These imply that\nextracting not only the 3D-aware geometric but also the\nobject-centric and semantic information from multi-view\nobservations is critical for RL performances.\nInterestingly, we observe that pre-training a NeRF-based\nautoencoder only with RGB supervision (NeRF-RL) is not\nsufficient to learn the features for RL downstream tasks,\nand it can not outperform multi-view adaptation of the vi-\nsual RL method with contrastive loss (CURL-multiview).\nThese are contrary to the results reported in the prior work\n(Driess et al., 2022), which we analyze as follows: the en-\nvironments we adopted in this work are more challenging\ncompared to those of Driess et al. (2022), which consist of\nsimple-shaped objects with primary colors. Therefore, it\nis relatively difficult to obtain semantic information simply\nusing RGB supervision. Thus, leveraging a semantic-aware\nNeRF decoder is required to extract the features for better\nperformances in RL downstream tasks in the practical use\nof 3D-aware RL, which is consistent with our analysis.\n5.2. Ablation Study\nSemantic and Distilled Feature Fields.\nTo validate how\neach semantic-aware radiance field leveraged in SNeRL (se-\nmantic and distilled feature fields) contributes to the down-\nstream RL performances, we evaluate its two ablated vari-\nants without semantic and feature supervision, respectively.\nAs shown in Figure 4(a), SNeRL, which takes advantage\nof both semantic and feature supervision from ground-truth\nlabels and a ViT-based feature descriptor, achieves the best\nperformance compared to all the ablated models. We ob-\nserve that the performance gap between SNeRL and the\nablated models depends on the environment, as semantic\nlabels are sufficient to learn semantic information in a rel-\n7\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\nview 1\nview 2\nview 3\nview 1\nview 2\nview 3\nNeRF-RL\nSNeRL\nview 1\nview 2\nview 3\nview 1\nview 2\nview 3\nview 1\nview 2\nview 3\nview 1\nview 2\nview 3\nFigure 5. Qualitative results on the image reconstruction in 3 different camera views via neural rendering. The synthesized images from\nSNeRL achieve better fidelity compared to NeRF-RL in several environments.\natively simple environment (window), but it requires both\nsupervision in a complex environment (soccer).\nMulti-view Representation Learning.\nWe also introduce\nauxiliary representation learning which is suitable for multi-\nview observations with self-predictive loss. To demonstrate\nits effectiveness on downstream RL tasks, we evaluate two\ndifferent models, which are SNeRL with and without multi-\nview self-predictive representation learning. By enforcing\nthe latent vector to be invariant to the viewpoint of the obser-\nvation, we report that the proposed representation learning\nimproves the RL agent‚Äôs performance in some environments\nas shown in Figure 4(b).\n5.3. Image Reconstruction via Neural Rendering\nEven though we only leverage the convolutional encoder\nfor downstream RL tasks, we compare the image render-\ning performance of NeRF-based decoders from SNeRL and\nNeRF-RL to explore the relationship between the synthe-\nsized image quality and RL performances. As NeRF origi-\nnally aims to synthesize images of arbitrary camera views\nfrom a static scene, NeRF-RL, which also trains the volu-\nmetric field with a sole RGB supervision, cannot reconstruct\ndynamic objects, e.g. the robot arm, in input images without\nsemantic information. On the other hand, SNeRL, which\nutilizes semantic labels and feature outputs from the ViT\nteacher network as additional supervision signals, not only\nachieves better RL performance but also well represents the\ndynamic scene and produces high-fidelity rendering outputs\nas shown in Figure 5.\n5.4. SNeRL for Model-based RL\nIn this section, we evaluate whether the learned representa-\ntion via SNeRL can also be adopted in off-the-shelf model-\nbased reinforcement learning algorithms, which train a\nworld model to characterize the environment and conduct\nplanning over the learned model. We adopt Dreamer (Hafner\net al., 2019) as a downstream model-based RL agent and\nreplace the encoder of the representation model with our\npre-trained encoder. Refer to Appendix A for additional\nimplementation details and an architectural overview.\nOur results are shown in Figure 4(c). We observe that learn-\ning model-based RL with the pre-trained encoder of SNeRL\noutperforms the pre-trained weights of the prior 3D-aware\nRL method (NeRF-RL) and na¬®ƒ±ve CNN autoencoder. This\nempirical evidence is consistent with the case of model-free\nRL in section 5.1, indicating that the proposed method al-\nlows the encoder to learn representations that are important\nfor general off-the-shelf RL agents.\n6. Conclusion\nIn this paper, we present SNeRL, a semantic-aware radi-\nance field for RL, that outperforms existing representation\nlearning methods for RL algorithms across four different\n3-dimensional environments. SNeRL leverages semantic\nand distilled feature supervision with latent condition NeRF\nautoencoders as well as RGB supervision to enable image\nencoders to express 3D-aware geometric and semantic rep-\nresentation on downstream RL tasks. We also propose a\nmulti-view self-predictive loss as an auxiliary representation\nlearning to force latent vectors to be viewpoint invariant. Fi-\nnally, we verify that SNeRL is effective in both model-free\nand model-based RL algorithms.\nLimitations.\nDespite these improvements, SNeRL inher-\nits the limitations of the prior 3D-aware RL method. First of\nall, SNeRL requires multi-view offline data, and collecting\nan offline dataset covering the state space in some complex\ncontrol tasks might be challenging. Also, our method uses a\nNeRF decoder that consumes more computational budget\nthan CNN, so there might be limitations in extending our\nmethod to an online setup which trains the encoder concur-\nrently with RL agents.\n8\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\n7. Acknowledgement\nThis research was supported by Institute of Information &\ncommunications Technology Planning & Evaluation (IITP)\ngrant funded by the Korea government(MSIT) [NO.2021-\n0-01343, Artificial Intelligence Graduate School Program\n(Seoul National University)].\nAlso, this research was\nsupported by Unmanned Vehicles Core Technology Re-\nsearch and Development Program through the National\nResearch Foundation of Korea(NRF) and Unmanned Ve-\nhicle Advanced Research Center(UVARC) funded by the\nMinistry of Science and ICT, the Republic of Korea(NRF-\n2020M3C1C1A01086411). Seungjae Lee would like to\nacknowledge financial support from Hyundai Motor Chung\nMong-Koo Foundation.\nReferences\nCaron, M., Touvron, H., Misra, I., J¬¥egou, H., Mairal, J.,\nBojanowski, P., and Joulin, A. Emerging properties in\nself-supervised vision transformers. In Proceedings of\nthe IEEE/CVF International Conference on Computer\nVision, pp. 9650‚Äì9660, 2021.\nChen, X. and He, K. Exploring simple siamese represen-\ntation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp.\n15750‚Äì15758, 2021.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In\nInternational Conference on Learning Representations,\n2020.\nDriess, D., Schubert, I., Florence, P., Li, Y., and Toussaint,\nM. Reinforcement learning with neural radiance fields.\narXiv preprint arXiv:2206.01634, 2022.\nDwibedi, D., Tompson, J., Lynch, C., and Sermanet, P.\nLearning actionable representations from visual obser-\nvations. In 2018 IEEE/RSJ international conference on\nintelligent robots and systems (IROS), pp. 1577‚Äì1584.\nIEEE, 2018.\nEslami, S. A., Jimenez Rezende, D., Besse, F., Viola, F.,\nMorcos, A. S., Garnelo, M., Ruderman, A., Rusu, A. A.,\nDanihelka, I., Gregor, K., et al. Neural scene represen-\ntation and rendering. Science, 360(6394):1204‚Äì1210,\n2018.\nFinn, C., Tan, X. Y., Duan, Y., Darrell, T., Levine, S., and\nAbbeel, P. Deep spatial autoencoders for visuomotor\nlearning. In 2016 IEEE International Conference on\nRobotics and Automation (ICRA), pp. 512‚Äì519. IEEE,\n2016.\nFu, X., Zhang, S., Chen, T., Lu, Y., Zhu, L., Zhou, X.,\nGeiger, A., and Liao, Y. Panoptic nerf: 3d-to-2d label\ntransfer for panoptic urban scene segmentation. arXiv\npreprint arXiv:2203.15224, 2022.\nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha,\nS., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P.,\net al. Soft actor-critic algorithms and applications. arXiv\npreprint arXiv:1812.05905, 2018.\nHafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to\ncontrol: Learning behaviors by latent imagination. arXiv\npreprint arXiv:1912.01603, 2019.\nHansen, N., Jangir, R., Sun, Y., Aleny`a, G., Abbeel, P.,\nEfros, A. A., Pinto, L., and Wang, X. Self-supervised\npolicy adaptation during deployment.\narXiv preprint\narXiv:2007.04309, 2020.\nHinton, G., Vinyals, O., Dean, J., et al. Distilling the knowl-\nedge in a neural network.\nIslam, R., Zang, H., Goyal, A., Lamb, A., Kawaguchi, K.,\nLi, X., Laroche, R., Bengio, Y., and Combes, R. T. D.\nDiscrete factorial representations as an abstraction for\ngoal conditioned reinforcement learning. arXiv preprint\narXiv:2211.00247, 2022.\nKinose, A., Okada, M., Okumura, R., and Taniguchi, T.\nMulti-view dreaming: Multi-view world model with con-\ntrastive learning. arXiv preprint arXiv:2203.11024, 2022.\nKobayashi, S., Matsumoto, E., and Sitzmann, V. Decom-\nposing nerf for editing via feature field distillation. arXiv\npreprint arXiv:2205.15585, 2022.\nKulkarni, T. D., Gupta, A., Ionescu, C., Borgeaud, S.,\nReynolds, M., Zisserman, A., and Mnih, V. Unsupervised\nlearning of object keypoints for perception and control.\nAdvances in neural information processing systems, 32,\n2019.\nKundu, A., Genova, K., Yin, X., Fathi, A., Pantofaru,\nC., Guibas, L. J., Tagliasacchi, A., Dellaert, F., and\nFunkhouser, T. Panoptic neural fields: A semantic object-\naware neural scene representation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 12871‚Äì12881, 2022.\nLaskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and\nSrinivas, A. Reinforcement learning with augmented data.\nAdvances in neural information processing systems, 33:\n19884‚Äì19895, 2020a.\nLaskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive\nunsupervised representations for reinforcement learning.\nIn International Conference on Machine Learning, pp.\n5639‚Äì5650. PMLR, 2020b.\n9\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\nLi, Y., Li, S., Sitzmann, V., Agrawal, P., and Torralba, A.\n3d neural scene representations for visuomotor control.\nIn Conference on Robot Learning, pp. 112‚Äì123. PMLR,\n2022.\nLiu, H. and Abbeel, P. Behavior from the void: Unsuper-\nvised active pre-training. Advances in Neural Information\nProcessing Systems, 34:18459‚Äì18473, 2021.\nManuelli, L., Li, Y., Florence, P., and Tedrake, R. Key-\npoints into the future: Self-supervised correspondence\nin model-based reinforcement learning. arXiv preprint\narXiv:2009.05085, 2020.\nMartin-Brualla, R., Radwan, N., Sajjadi, M. S., Barron,\nJ. T., Dosovitskiy, A., and Duckworth, D. Nerf in the\nwild: Neural radiance fields for unconstrained photo col-\nlections. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 7210‚Äì\n7219, 2021.\nMildenhall, B., Srinivasan, P., Tancik, M., Barron, J., Ra-\nmamoorthi, R., and Ng, R. Nerf: Representing scenes as\nneural radiance fields for view synthesis. In European\nconference on computer vision, 2020.\nSchwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville,\nA., and Bachman, P. Data-efficient reinforcement learn-\ning with self-predictive representations. arXiv preprint\narXiv:2007.05929, 2020.\nSermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E.,\nSchaal, S., Levine, S., and Brain, G. Time-contrastive\nnetworks: Self-supervised learning from video. In 2018\nIEEE international conference on robotics and automa-\ntion (ICRA), pp. 1134‚Äì1141. IEEE, 2018.\nStooke, A., Lee, K., Abbeel, P., and Laskin, M. Decoupling\nrepresentation learning from reinforcement learning. In\nInternational Conference on Machine Learning, pp. 9870‚Äì\n9879. PMLR, 2021.\nTatarchenko, M., Dosovitskiy, A., and Brox, T. Multi-view\n3d models from single images with a convolutional net-\nwork. In European Conference on Computer Vision, pp.\n322‚Äì337. Springer, 2016.\nTodorov, E., Erez, T., and Tassa, Y. Mujoco: A physics\nengine for model-based control. In 2012 IEEE/RSJ inter-\nnational conference on intelligent robots and systems, pp.\n5026‚Äì5033. IEEE, 2012.\nWang, C., Luo, X., Ross, K., and Li, D. Vrl3: A data-driven\nframework for visual deep reinforcement learning. arXiv\npreprint arXiv:2202.10324, 2022.\nWang, Q., Wang, Z., Genova, K., Srinivasan, P. P., Zhou,\nH., Barron, J. T., Martin-Brualla, R., Snavely, N., and\nFunkhouser, T. Ibrnet: Learning multi-view image-based\nrendering. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 4690‚Äì\n4699, 2021.\nWorrall, D. E., Garbin, S. J., Turmukhambetov, D., and\nBrostow, G. J. Interpretable transformations with encoder-\ndecoder networks. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision, pp. 5726‚Äì5735,\n2017.\nYamada, J., Pertsch, K., Gunjal, A., and Lim, J. J.\nTask-induced representation learning.\narXiv preprint\narXiv:2204.11827, 2022.\nYang, M. and Nachum, O. Representation matters: of-\nfline pretraining for sequential decision making. In In-\nternational Conference on Machine Learning, pp. 11784‚Äì\n11794. PMLR, 2021.\nYarats, D., Kostrikov, I., and Fergus, R. Image augmentation\nis all you need: Regularizing deep reinforcement learning\nfrom pixels. In International Conference on Learning\nRepresentations, 2020.\nYarats, D., Fergus, R., Lazaric, A., and Pinto, L. Mastering\nvisual continuous control: Improved data-augmented re-\ninforcement learning. arXiv preprint arXiv:2107.09645,\n2021.\nYou, B., Arenz, O., Chen, Y., and Peters, J. Integrating con-\ntrastive learning with dynamic models for reinforcement\nlearning from images. Neurocomputing, 476:102‚Äì114,\n2022.\nYu, A., Ye, V., Tancik, M., and Kanazawa, A. pixelnerf:\nNeural radiance fields from one or few images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pp. 4578‚Äì4587, 2021.\nYu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn,\nC., and Levine, S. Meta-world: A benchmark and evalua-\ntion for multi-task and meta reinforcement learning. In\nConference on robot learning, pp. 1094‚Äì1100. PMLR,\n2020.\nZhan, A., Zhao, R., Pinto, L., Abbeel, P., and Laskin,\nM. Learning visual robotic control efficiently with con-\ntrastive pre-training and data augmentation.\nIn 2022\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), pp. 4040‚Äì4047. IEEE, 2022.\nZhi, S., Laidlow, T., Leutenegger, S., and Davison, A. J.\nIn-place scene labelling and understanding with implicit\nscene representation. 2021.\n10\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\nA. Algorithms\nA.1. Model-free RL : Soft Actor-Critic\nIn this project, we adopt Soft Actor-Critic algorithm (Haarnoja et al., 2018) (SAC) in all the experiments of the model-free\ndownstream RL. SAC optimizes stochastic policies to maximize both the expected trajectory returns and the expected\nentropy of the actions. Although SAC shows promising performances on a range of control tasks over continuous action\nspaces including many benchmark tasks, it fails or suffers from data inefficiency in some visual control tasks. To train actor\nnetwork œÄœï and critic networks QŒ∏1, QŒ∏2 SAC algorithm minimizes the following objective functions\nJœÄ(œï) = Est‚àºD\n\u0002\nEat‚àºœÄœï[Œ± log(œÄœï(at|st)) ‚àímin\ni=1,2 QŒ∏i(st, at)]\n\u0003\n(15)\nJQ(Œ∏i) = Est,at,st+1,r‚àºD,at+1‚àºœÄœï(st+1)\n\u00141\n2(QŒ∏i(st, at) ‚àí(r + Œ≥T ))2\n\u0015\n,\n(16)\nwhere D denotes the replay buffer, Œ± the temperature hyperparameter, and Œ≥ the discount factor. The target value T in Eq.\n16 is\nT = min\ni=1,2 QÀÜŒ∏i(st+1, at+1) ‚àíŒ± log œÄœï(at+1|st+1).\n(17)\nSAC also utilizes target networks QÀÜŒ∏1, QÀÜŒ∏2 which are obtained as an Exponentially Moving Average (EMA) of the Q\nnetworks (QŒ∏1, QŒ∏2) for better learning stability, and gradient-based temperature tuning to determine the relative importance\nof the entropy,\nJ(Œ±) = Eat‚àºœÄœï(st)[‚àíŒ± log œÄœï(at|st) ‚àíŒ± ¬ØH].\n(18)\nA.2. Model-based RL : Dreamer\nTo evaluate whether SNeRL can also be adopted in model-based RL algorithms, we adopt Dreamer (Hafner et al., 2019).\nDreamer learns the world model which consists of the following components:\npŒ∏(st|st‚àí1, at‚àí1, ot)\nqŒ∏(ot|st)\nqŒ∏(rt|st)\nqŒ∏(st|st‚àí1, at‚àí1).\n(19)\nThese components are jointly optimized to increase the variational lower bound, which includes the following terms:\nJ t\nO = ln q(ot|st)\nJ t\nR = ln q(rt|st)\nJ t\nD = ‚àíŒ≤KL\n\u0000p(st|st‚àí1, at‚àí1, ot)||q(st|st‚àí1, at‚àí1)\n\u0001\n.\n(20)\nWe replace the convolutional encoder of Dreamer with our feature extractor ‚Ñ¶(shared encoder of SNeRL) to design an\nRL with a dynamics model over the latent space with pre-trained mapping. To learn the action and value models, Dreamer\noptimizes the value model vœà and the action model qœï using the objectives\nmax\nœï\nE\n\u0012 t+H\nX\nœÑ=t\nVŒª(sœÑ)\n\u0013\nmax\nœà\nEqŒ∏,qœï\n\u0012 t+H\nX\nœÑ=t\n1\n2\n\f\f\f\n\f\f\fvœà(sœÑ) ‚àíVŒª(sœÑ)\n\f\f\f\n\f\f\f\n2\u0013\n,\n(21)\nwhere H denotes the horizon, and VŒª the exponentially-weight value estimation. Refer to Dreamer (Hafner et al., 2019) for\ndetails.\n11\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\nA.3. Pseudo-code\nAlgorithm 1 Stage 1. pre-train multi-view encoder with SNeRL\n1: Input: encoder ‚Ñ¶, off-the-shelf feature descriptor ÀÜF, offline dataset D\n2: for iteration=1,2,...,N do\n3:\nfor sample minibatch d from D do\n4:\nz ‚Üê‚Ñ¶(o1:V , K1:V )\n5:\nL ‚ÜêLRGB + ŒªsemLsem + ŒªfeatLfeat\n6:\nupdate the parameters of ‚Ñ¶to minimize L\n7:\nend for\n8: end for\nAlgorithm 2 Stage 2. Downstream Model-free RL (SAC)\n1: Input: total training episodes N, Env, environment horizon H, actor network œÄœï, critic networks QŒ∏i=1,2, target critic\nnetworks QÀÜŒ∏i=1,2, temperature Œ±, replay buffer B, pre-trained encoder ‚Ñ¶.\n2: for iteration=1,2,...,N do\n3:\not=0 ‚ÜêEnv.reset()\n4:\nfor t=0,1,...,H-1 do\n5:\nat ‚ÜêœÄœï(¬∑|‚Ñ¶(ot))\n6:\nrt, ot+1 ‚ÜêEnv.step(at)\n7:\nB ‚ÜêB ‚à™{(ot, at, rt, ot+1)}\n8:\nend for\n9:\nfor each gradient step do\n10:\nŒ∏i ‚ÜêŒ∏i ‚àíŒªQ‚àáŒ∏iJQ(Œ∏i)\n11:\nœï ‚Üêœï ‚àíŒªœÄ‚àáœïJœÄ(œï)\n12:\nŒ± ‚ÜêŒ± ‚àíŒª‚àáŒ±J(Œ±)\n13:\nif update target critic networks then\n14:\nÀÜŒ∏i ‚ÜêœÑŒ∏i + (1 ‚àíœÑ)ÀÜŒ∏i\n15:\nend if\n16:\nend for\n17: end for\nAlgorithm 3 Stage 2. Downstream Model-based RL (Dreamer)\n1: Input: total training episodes N, update step C, Env, environment horizon T, imagination horizon H, Neural network\nparameters Œ∏, œï, œà, replay buffer B, pre-trained encoder ‚Ñ¶.\n2: for iteration=1,2,...,N do\n3:\nfor c=1,...,C do\n4:\nsample data sequence {(at, ot, rt)}t=0,...,H‚àí1 ‚àºB\n5:\ncompute model states st ‚àºpŒ∏(st|‚Ñ¶(ot‚àí1), at‚àí1) and reward qŒ∏(rt|‚Ñ¶(ot))\n6:\nupdate Œ∏ using representation learning\n7:\nimaging trajectories and compute value estimates VŒª(sœÑ)\n8:\nœï ‚Üêœï + Œ±‚àáœï\nPt+H\nœÑ=t VŒª(sœÑ)\n9:\nœà ‚Üêœà + Œ±‚àáœà\nPt+H\nœÑ=t\n1\n2||vœà(sœÑ) ‚àíVŒª(sœÑ)||2\n10:\nend for\n11:\not=0 ‚ÜêEnv.reset()\n12:\nfor t=0,1,...,T ‚àí1 do\n13:\nat ‚Üêqœï(¬∑|‚Ñ¶(ot))\n14:\nrt, ot+1 ‚ÜêEnv.step(at)\n15:\nB ‚ÜêB ‚à™{(ot, at, rt, ot+1)}\n16:\nend for\n17: end for\n12\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\nB. Training & Experiments Details\nB.1. Encoder Architecture\nWe design the encoder architecture similar to Laskin et al. (2020b), which consists of multiple convolutional layers and\nReLU activation, but modify it to be applicable in the multi-view observation inputs. The same encoder is also adopted in\nthe actor and critic to embed the pixel-level (multi-view) observations. We demonstrate the details of the convolutional\nencoder with PyTorch-like pseudo-code as below.\nAlgorithm 4 Multi-view Encoder Pseudocode, PyTorch-like\ndef encoder(x1, x2, x3, K1, K2, K3, z_dim):\n\"\"\"\nMulti-view ConvNet encoder\nargs:\nB = batch_size, C = channels,\nH, W =spatial_dims\nx1, x2, x3: images from 3 different camera views\nx1, x2, x3 shape: [B, C, H, W]\nK1, K2, K3: camera poses from 3 different camera views\nK1, K2, K3 shape: [B, 4, 4]\nz_dim = latent dimension\n\"\"\"\nx = x / 255.\n# c: channels, f: filters\n# k: kernel, s: stride\nz1 = Conv2d(c=x1.shape[1], f=32, k=3, s=2)(x1)\nz1 = ReLU(z1)\nz2 = Conv2d(c=x2.shape[1], f=32, k=3, s=2)(x2)\nz2 = ReLU(z2)\nz3 = Conv2d(c=x3.shape[1], f=32, k=3, s=2)(x3)\nz3 = ReLU(z3)\nfor _ in range(num_layers-1):\nz1 = Conv2d(c=32, f=32, k=3, s=1)(z1)\nz1 = ReLU(z1)\nz2 = Conv2d(c=32, f=32, k=3, s=1)(z2)\nz2 = ReLU(z2)\nz3 = Conv2d(c=32, f=32, k=3, s=1)(z3)\nz3 = ReLU(z3)\nz1 = flatten(z1)\nz2 = flatten(z2)\nz3 = flatten(z3)\nz1 = concat([z1, K1.view(B,16)], dim=1)\nz2 = concat([z2, K1.view(B,16)], dim=1)\nz3 = concat([z3, K1.view(B,16)], dim=1)\nz1 = Linear(z1.shape[1], z_dim)(z1)\nz2 = Linear(z2.shape[1], z_dim)(z2)\nz3 = Linear(z3.shape[1], z_dim)(z3)\nz = concat([z1, z2, z3], dim=1).mean(dim=1)\nz = Linear(z.shape[1], z_dim)(z)\nz = LayerNorm(z)\nz = tanh(z)\nreturn z\n13\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\nB.2. Datasets\nThe offline datasets for SNeRL and baselines consist of 14400 scenes. Each scene consists of three image observations\ntaken from different camera views. The observations from each camera view are represented in Figure 1 (Window-open-v2)\nand Figure 5 (Soccer-v2, Hammer-v2, Drawer-open-v2). To collect the dataset, we utilized random actions and the policies\nprovided by Meta-world (half-and-half mixed).\nTo observe how the performance of the proposed method varies with the quality of the dataset, we further trained the SNeRL\nencoder with a dataset collected by a single expert demo and random actions. Only 120 scenes of the total scenes (14400,\n120/14400‚âÉ1%) were obtained from the path of the expert demo, and the remaining 14280 scenes were obtained by taking\nrandom actions from one moment of the path of the expert demo. As shown in Figure 6, we observe that the quality of the\ndataset slightly affects the learning stability, but there is no dramatic performance degradation. The results show that there\nwould be no significant degradation in the performance of the SNeRL if the dataset adequately covers the state space, even\nif the policy that collects the offline dataset is suboptimal.\nEpisode Return (x1e3)\nWindow-open-v2\nHammer-v2\nDrawer-open-v2\nTimestep (√ó 1e5)\nTimestep (√ó 1e5)\nTimestep (√ó 1e5)\nOurs (SNeRL) with 50% expert dataset : expert policy + random action (half and half mixed)\nOurs (SNeRL) with  1% expert dataset : single expert demo + random action \nFigure 6. The performance of SNeRL with different quality of offline datasets.\nB.3. Computational Resources\nStage 1 (pre-training encoder) in our experiments has been performed using a single NVIDIA RTX A6000 and AMD Ryzen\n2950X, and stage 2 (RL downstream tasks) has been performed using an NVIDIA RTX A5000 and AMD Ryzen 2950X.\nTraining the SNeRL encoder takes 4-5 days and learning model-based RL and model-free RL takes 1-2 days.\nB.4. Hyperparameters\nTable 1. Hyperparameters for pre-training multi-view encoder\nSNeRL\nConvolution layers\n4\nNumber of filters\n32\nNon-linearity\nReLU\nMLP layers for NeRF\n8\nHidden units (MLP)\n256\nNumber of different views\n3\nNeRF learning rate\n5e-4\nNumber of rays per gradient step\n1024\nNumber of samples per ray\n64\n14\nSNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning\nTable 2. Hyperparameters for SAC (for SNeRL and baselines)\nSAC\nhidden layer\n(1024, 1024)\nframe stack\n2\nreplay buffer size\n100000\ninitial random steps\n1000\nbatch size\n128\nactor learning rate\n1e-3\ncritic learning rate\n1e-3\nŒ± learning rate\n1e-4\nŒ≤ for Adam optimizer (actor, critic)\n0.9\neps for Adam optimizer (Œ±)\n1e-08\nŒ≤ for Adam optimizer (Œ±)\n0.5\neps for Adam optimizer (Œ±)\n1e-08\ncritic target update interval\n2\nactor network update interval\n2\nactor log std min, max\n-10, 2\ninit temperature\n0.1\nœÑ for EMA\n0.01\ndiscount factor Œ≥\n0.99\nTable 3. Hyperparameters for Dreamer (for SNeRL and baselines)\nDreamer\nembedding size\n63\nhidden / belief size\n128\nstate size\n30\naction noise\n0.3\nbatch size\n32\nworld model learning rate\n1e-3\nactor learning rate\n5e-5\nvalue network learning rate\n5e-5\ndiscount factor Œ≥\n0.99\nreplay buffer size\n100000\nplanning horizon\n15\neps for Adam optimizer\n1e-07\n15\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV",
    "cs.RO"
  ],
  "published": "2023-01-27",
  "updated": "2023-05-31"
}