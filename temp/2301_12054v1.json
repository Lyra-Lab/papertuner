{
  "id": "http://arxiv.org/abs/2301.12054v1",
  "title": "Adversarial Learning Networks: Source-free Unsupervised Domain Incremental Learning",
  "authors": [
    "Abhinit Kumar Ambastha",
    "Leong Tze Yun"
  ],
  "abstract": "This work presents an approach for incrementally updating deep neural network\n(DNN) models in a non-stationary environment. DNN models are sensitive to\nchanges in input data distribution, which limits their application to problem\nsettings with stationary input datasets. In a non-stationary environment,\nupdating a DNN model requires parameter re-training or model fine-tuning. We\npropose an unsupervised source-free method to update DNN classification models.\nThe contributions of this work are two-fold. First, we use trainable Gaussian\nprototypes to generate representative samples for future iterations; second,\nusing unsupervised domain adaptation, we incrementally adapt the existing model\nusing unlabelled data. Unlike existing methods, our approach can update a DNN\nmodel incrementally for non-stationary source and target tasks without storing\npast training data. We evaluated our work on incremental sentiment prediction\nand incremental disease prediction applications and compared our approach to\nstate-of-the-art continual learning, domain adaptation, and ensemble learning\nmethods. Our results show that our approach achieved improved performance\ncompared to existing incremental learning methods. We observe minimal\nforgetting of past knowledge over many iterations, which can help us develop\nunsupervised self-learning systems.",
  "text": "Adversarial Learning Networks: Source-free Unsupervised Domain\nIncremental Learning\nAbhinit Kumar Ambastha\nLeong Tze Yun\nNational University of Singapore\nNational University of Singapore\nAbstract\nThis work presents an approach for incremen-\ntally updating deep neural network (DNN) mod-\nels in a non-stationary environment. DNN mod-\nels are sensitive to changes in input data distri-\nbution, which limits their application to prob-\nlem settings with stationary input datasets.\nIn\na non-stationary environment, updating a DNN\nmodel requires parameter re-training or model\nﬁne-tuning. We propose an unsupervised source-\nfree method to update DNN classiﬁcation mod-\nels. The contributions of this work are two-fold.\nFirst, we use trainable Gaussian prototypes to\ngenerate representative samples for future itera-\ntions; second, using unsupervised domain adap-\ntation, we incrementally adapt the existing model\nusing unlabelled data.\nUnlike existing meth-\nods, our approach can update a DNN model in-\ncrementally for non-stationary source and target\ntasks without storing past training data. We eval-\nuated our work on incremental sentiment predic-\ntion and incremental disease prediction applica-\ntions and compared our approach to state-of-the-\nart continual learning, domain adaptation, and\nensemble learning methods.\nOur results show\nthat our approach achieved improved perfor-\nmance compared to existing incremental learning\nmethods. We observe minimal forgetting of past\nknowledge over many iterations, which can help\nus develop unsupervised self-learning systems.\n1\nIntroduction\nDeep neural networks (DNN) have shown exceptional per-\nformance in several practical classiﬁcation problems by\napproximating mapping functions between input data and\noutput classes. However, they fail to generalize well in\nnon-stationary environments where the labeling function\nchanges continually due to shifts in the feature distribu-\ntion of the input data (McGaughey et al., 2016; Chan et al.,\n2020; Ioffe and Szegedy, 2015; Bickel et al., 2009). This is\nreferred to as domain incremental learning problem setting\nand limits the applicability of DNNs to stationary domains\nwith sizeable labeled training datasets. In order to mini-\nmize model degradation in this problem setting, we need to\nretrain the model every time we have an updated dataset.\nThis process needs us to store previously observed train-\ning data and continuously label new incoming data (Hoff-\nman et al., 2012; Srivastava et al., 2021; Wulfmeier et al.,\n2018).\nContinual learning methods and domain adapta-\ntion methods have been proposed in recent literature to ad-\ndress the problem of updating DNN models by adding addi-\ntional network parameters to learn new mapping functions\nor store representative samples from past training data (Li\nand Hoiem, 2017; Hoffman et al., 2014; Zenke et al., 2017;\nSrivastava et al., 2021). Even though they minimize the\nforgetting of previously learned knowledge, they require\nfuture increments to be labeled and fail to address the issue\nof continuously annotating incoming data. Unsupervised\ndomain adaptation methods overcome this limitation of la-\nbeling new data (Ganin et al., 2016; Zhao et al., 2019, 2018;\nChuramani et al., 2021). However, they still require storing\npast labeled training data.\nThis work presents a source-free unsupervised domain in-\ncremental learning approach – adversarial learning net-\nwork (ALeN). The main contribution of this work is that it\nenables incrementally updating a DNN model using unla-\nbelled data without storing past training data. Our work is\nmotivated by adversarial domain adaptation (Ganin et al.,\n2016) and class incremental domain adaptation (Kundu\net al., 2020). Ganin et al. (2016) presented an approach\nto carry out unsupervised adversarial domain adaptation at\na ﬁxed time-point which assumes stationary source and tar-\nget tasks.\nOur work is an incremental extension of the\napproach and allows adapting a base model for a non-\nstationary target task. Our problem formulation helps po-\nsition the incremental learning problem setting as a contin-\nual domain adaptation problem by assuming the incremen-\ntal input as a single non-stationary dataset. Kundu et al.\n(2020) present a source-free approach for class incremental\nlearning (learning additional classes from target data with-\nout storing source domain data). Their approach uses do-\nmain adaptation to retain source model knowledge but does\nnot address incremental learning. We extend their approach\narXiv:2301.12054v1  [cs.LG]  28 Jan 2023\nAdversarial Learning Networks: Source-free Unsupervised Domain Incremental Learning\nto non-stationary input distributions. We use prototype net-\nworks to generate past training data representatives; this\nalleviates the need for storing past training samples and re-\nduces the memory complexity of our approach.\nWe depict our method in Figure 1. Our method is divided in\ntwo stages – base classiﬁer learning and incremental learn-\ning stage. We train a base classiﬁer for a supervised source\ntask in the ﬁrst stage. We learn the cluster prototypes from\nthe feature space of this base classiﬁer. We update the clus-\nter prototypes using ﬁne-tuning for a new source dataset\nevery time. In the second stage, we use unsupervised do-\nmain adaptation to adapt the base classiﬁer for a target task.\nWe update the target task model using domain adaptation\nevery time for incremental target dataset updates.\n2\nRELATED WORK\nExisting methods in the literature achieve incremental up-\ndates for DNN models by either learning a domain invariant\nfeature mapping space or extending the model parameter\nspace. In this section, we brieﬂy study existing works for\ndomain incremental learning.\nUnsupervised domain adaptation: Ganin et al. (2016)\npropose an adversarial domain adaptation approach to min-\nimize domain discrepancy. They provide an adversarial un-\nsupervised domain adaptation approach (DANN) to learn\na common model for labeled source domain and unla-\nbeled target domain datasets. Adversarial domain adap-\ntation methods learn a domain in-variant feature space.\nZhao et al. (2018) further extend this approach as multi-\nsource domain adaptation (MSDA) to learn a more gener-\nalized base source model and ensure that the domain dis-\ncrepancy with the incoming target data would be lower.\nDiscrepancy metrics are sensitive to feature representation\n(Hoffman et al., 2012; Blitzer et al., 2008; Tzeng et al.,\n2017) and require hand-crafting source and target features.\nAlthough these methods update a model using unlabelled\ndata, they are not incremental and require representative\nmemory. Kim et al. (2021) and Yang et al. (2021) present\ndomain adaptation methods and do not address the prob-\nlem setting with source and target domain data available\nincrementally. Kim et al. (2021) use class-conﬁdent target\ndomain samples to pseudo-label target domain data using a\npre-trained source model. This can lead to negative train-\ning. We address negative training in our work using out-\nof-distribution samples and derive class prototypes from\nsource-domain feature space. The proposed method can-\nnot accommodate baseline source model updates. The ap-\nproach requires re-training from scratch if the source do-\nmain distribution changes. Our approach can incrementally\nupdate the model using labeled source and unlabeled target\ndomain data.\nContinual learning:\nIn contrast to domain adaptation\nmethods, continual learning methods consider data from\ndifferent domains sampled from a single non-stationary\ndistribution.\nHoffman et al. (2014) provide a continual\nadaptation (CMA) approach which learns a low dimen-\nsional embedding subspace for incoming non-stationary\ntarget data. The authors update a parametric kernel as the\ntarget domain distribution evolves1. Also, the method as-\nsumes the sources are closely related to the target task. We\nrelax this assumption in our method. Finally, they use a\ngeodesic kernel to map target instances to the source do-\nmain. A limitation of this approach is that it does not ad-\ndress the possibility that an evolving source domain dis-\ntribution requires re-training the source model using stored\nsource domain data. Also, using a kernel-based approach is\ncomputationally intensive if the target dataset size is large,\nwhich limits the scalability of the approach.\nCastro et al. (2018) propose a DNN-based continual learn-\ning approach to address the abovementioned limitations.\nThe authors trained a joint feature extractor using stored\nlabeled source and target domain data. This partially re-\nsolves the resource constraints in an incremental learning\nproblem setting, but the requirement of labeled data makes\nit unsuitable for unsupervised domain incremental learn-\ning. Li and Hoiem (2017) provide a continual learning ap-\nproach that expands the network parametric space to learn\nnew classes and feature spaces using distillation loss. The\nmethod achieves low catastrophic forgetting but at the cost\nof using labeled source and target data.\nEnsemble learning: ensemble learning approaches have\nbeen proposed in the literature to learn an incremental\nmodel using boosting and bagging approaches (Muhlbaier\net al., 2008; Polikar et al., 2001; Muhlbaier et al., 2004;\nPolikar et al., 2010; Ditzler et al., 2010). Elwell and Po-\nlikar (2011) propose a boosting-based incremental learn-\ning algorithm that uses model weighting to adapt for non-\nstationary target data.\nThe algorithm allocates higher\nweight to classiﬁers capable of identifying previously un-\nseen instances while penalizing the other classiﬁers in the\nensemble. The approach requires labeled source and target\ndomain data like previous works.\nA signiﬁcant limitation of boosting-based incremental\nlearning algorithms is the time complexity of train-\ning. Weights for all previous data sample errors are re-\ncalculated in every iteration, which makes the approach\nunsuitable for large datasets and frequent incremental up-\ndates.\n3\nOUR APPROACH\nThis section provides the details of ALeN (our proposed\napproach). Our method has two key components – a knowl-\n1Kernel-based methods deﬁne a multi-kernel feature transfor-\nmation function but are limited to pre-deﬁned kernels due to sym-\nmetry and positive-deﬁnite constraints\nAbhinit Kumar Ambastha, Leong Tze Yun\nFigure 1: Generic architecture for the proposed unsupervised domain incremental learning method\nFigure 2: ALeN architecture: Architecture of the proposed\ndomain incremental learning approach.\nedge representation mechanism for past knowledge replay\nand an incremental learning algorithm to learn an updated\ntarget task model. Figure 2 shows the architecture of the\nproposed approach. The base model network includes two\nparts – a feature extractor and a classiﬁer. The feature ex-\ntractor network learns a latent feature space from the input\ndata which minimizes the classiﬁcation loss. The classiﬁer\nnetwork maps the latent feature space to the label space.\nBy learning a generative model for the latent features ex-\ntracted from source domain data, we can generate repre-\nsentatives of the training data for the base model. We learn\na Gaussian estimate for each class-wise source domain fea-\nture distribution. Clustering methods use class-wise feature\nmoments to deﬁne membership criteria for a given cluster.\nWe estimate class-wise posterior distributions of the input\ndata given a feature mapping network (Snell et al., 2017).\nWe use the Gaussian prototypes as guides to generate class-\nwise data representatives.\nDeﬁnition 1. Gaussian prototypes: The Gaussian proto-\ntypes for a given class c are deﬁned as a multi-variate Gaus-\nsian prior for each class c in the latent space U. It is given\nby Pc\ns = N(µc\ns, Σc\ns), where µc\ns and Σc\ns denote the mean\nand covariance of features fs(xs), where xs ∈[c].\n3.1\nForesighted source training\nThis section describes the foresighted learning stage. This\nstage aims to identify tight class-wise clusters in feature\nposterior distribution using Gaussian estimation.\nAlgo-\nrithm 1 describes the process of foresighted training and\nlearning the source domain prototypes.\nWe denote the feature extractor function as fs and the clas-\nsiﬁer function as gs, which maps the feature extractor out-\nput to a |Cs + 1|-class label space (where Cs is the source\ntask label set size). The latent space is denoted by U. We\nminimize cross-entropy loss (lce) to learn gs.\nlce =\nE\n(xs,ys)∼D(t)\ns\nlce(gs · fs(xs), ys)\n(1)\nMinimizing lce alone has been shown to bias the base\nmodel to source domain characteristics (Kundu et al.,\n2020). Cross-entropy loss ensures discriminative decision\nboundaries in the latent feature space but leads to over-\nconﬁdent predictions. In order to generate representative\nsamples for source distribution for future iterations, we\nminimize category bias by penalizing over-conﬁdent pre-\ndiction. We achieve this by identifying out-of-distribution\nsamples (OOD samples). We reduce the number of neg-\native samples (low-conﬁdence source domain samples) by\ntraining the classiﬁer to identify a |Cs + 1|-class for OOD\nsamples. The negative samples are identiﬁed based on the\nlearned class-wise prototypes uc\ns ∼Pc\ns, c ∈Cs using a\nk −σ conﬁdence interval (line 22-29) (we identiﬁed k = 3\nusing sensitivity analysis).\nWe use a class separability objective Ls1 to enforce the\nclass-wise features to attain higher afﬁnity to the class-wise\nAdversarial Learning Networks: Source-free Unsupervised Domain Incremental Learning\nfeature extractor (fs, ft)\nClassiﬁer (g, gs)\nDiscriminator (d)\nResNet-50 (till avg. pool layer)\n2048\nInput\n256\n-\nInput\n256\n-\nFC\n1024\nELU\nFC\n64\nELU\nFC\n64\nELU\nBN\n-\nFC\n|Cs + 1|\n-\nFC\n2\n-\nFC\n256\nELU\nFC\n256\nELU\nBN\n-\nTable 1: Network architecture for ALeN. Cs is the number of classes.\nprototypes\nLs1 :\nE\n(xs,ys)∼D(t)\ns\n−log\n\n\nexp(Pys\ns (f (t)\ns (us)))\nP\nc∈Cs exp(Pcs(f (t)\ns (us)))\n\n\n(2)\nTo update the prototypes at the end of the training, we use\na class separability approach to learn a stationary source\ndistribution space (U −space) and store the guides for\nthe upcoming incremental phase. These guides are used to\nsample pseudo-source domain instances in the incremental\nlearning stage and apply pseudo labels to them.\nLs = Ls1 + Ls2\n(3)\nLs2 :\nE\n(xs,ys)∼D(t)\ns\nlce(gs · fs(xs), ys)\n+\nE\n(un,yn)∼¯\nD(t)\ns\nlce(gs(un), yn)\n(4)\n3.2\nDomain incremental update algorithm\nIn this section, we describe the domain incremental update\nstage of the proposed method. We use the learned proto-\ntypes and the unlabelled target domain data to incremen-\ntally updated base classiﬁer for the target task. Algorithm 2\npresents the proposed algorithm.\nThe incremental learning network comprises three sub-\nmodules – feature extractor, classiﬁer, and domain discrim-\ninator.\nThe feature extractor and classiﬁer use the pre-\ntrained base model parameters (line 2) and have the same\narchitecture as the base model. In order to retain source\ndomain knowledge during this stage, we sample the class-\nwise prototypes to obtain labelled source domain represen-\ntatives (line 11 - 11).\nWe carry out adversarial domain\nadaptation to learn a target model using unlabeled target\ndomain data (Ganin et al., 2016; Tzeng et al., 2017) and\nthe sampled source domain representatives. The feature\nextractor learns a common latent feature mapping for both\nthe source and target domain. Line 12-16 shows the target\ndomain sampling and feature extraction.\nWe use a domain discriminator network d(t) to estimate\nthe H−distance between the source and target domains.\nThe target domain features and source domain samples are\npassed to the domain discriminator. The true labels for the\ndomain discriminator are provided incorrectly to increase\nthe domain confusion loss. A gradient reversal layer be-\ntween the domain discriminator and the target feature ex-\ntractor is used to adversarially train the feature extractor\n(line18 and line 19). We use cross-entropy loss to update\nthe base classiﬁer (using only source domain representa-\ntives) to ensure the retention of previously learned decision\nboundaries (line 17). We update the feature extractor and\ndomain discriminator until convergence.\n4\nEXPERIMENTS AND RESULTS\nSetup: The experiments were written in Python, and Py-\nTorch was used to implement the neural network architec-\nture, gradient propagation, and the training loop. SpaCy\nwas used to implement the NLP functions and preprocess-\ning steps. We used a workstation with a Tesla V100 graph-\nics card with 32GB GPU memory and 256GB RAM.\n4.1\nIncremental Amazon products review prediction\nWe illustrate the performance of our proposed method us-\ning a sentiment classiﬁcation task for Amazon product re-\nviews (He and McAuley, 2016; McAuley et al., 2015). The\ntask is to predict a given product rating based on an incom-\ning user review of the product. The rating is an integer in\nthe range [1, 5]. We use the Amazon reviews dataset to con-\nstruct an incremental learning problem for predicting prod-\nuct ratings for a category (target task) for which we only\nhave access to unlabelled data. The remaining product re-\nview datasets are used as sources available to the training\nalgorithm sequentially in a randomized order. In total, the\ndataset contains labeled review data from 25 categories (we\nignore three categories with insufﬁcient data). We extract\n5000 samples from each category.\nThe relative positioning of word vectors encodes the se-\nmantic meaning of a sentence. Hence, even though two cor-\npora may contain words from the same vocabulary, their se-\nmantic meanings can differ. A sentiment prediction model\naims to infer the sentiment of a phrase by analyzing the\njoint distribution of the set of tokens in the sentence. Due\nto the extensive vocabulary size, estimating this joint dis-\nAbhinit Kumar Ambastha, Leong Tze Yun\nAlgorithm 1 Foresighted baseline model training\n1: Require: Source samples Ds, model parameters\nθfs, θgs, batch size of the source samples\nNsrc and negative samples Nneg\n2: repeat\n3:\nObtain a mini-batch of source samples Ss\n=\n{xs, ys) ∼Ds}\n4:\nθfs ←θfs + δθfs,Ss\n5: until reached the end of 1 epoch\n6: for c ∈Cs do\n7:\nDc\ns ←{(xs, ys) : (xs, ys) ∈Ds, ys = c}\n8:\nµc\ns ←mean(xs,ys)∈Dcs(fs(xs))\n9:\nΣc\ns ←cov(xs,ys)∈Dcs(fs(xs))\n10:\nPc\ns ←N(µc\ns, Σc\ns)\n11: end for\n12: µs ←mean(xs,ys)∈Ds(fs(xs))\n13: Σs ←cov(xs,ys)∈Ds(fs(xs))\n14: Ps ←N(µs, Σs)\n15: Loss ←[Ls1, Ls2]\n16: Opt ←[Adam{fs}, Adam{fs,gs}]\n17: Φ ←[{θfs}, {θfs,gs}]\n18: iter ←0\n19: repeat\n20:\niter ←iter +1\n21:\ncur ←iter mod 2\n22:\nSs ←{(xs, ys) ∼Ds}\n23:\nSn ←IdentifyNegativeSamples(Ps, {Pc\ns : c ∈\nCs}, Nneg)\n24:\nfor (xs, ys) ∈Ss do\n25:\nys ←gs ◦fs(xs)\n26:\nend for\n27:\nfor (un, yn) ∈Sn do\n28:\nyn ←gs(un)\n29:\nend for\n30:\nΦ[cur] ←Φ[cur] + Opt[cur](−δΦ[cur],Loss[cur])\n31:\nif reached the end of an epoch then\n32:\nRecalculate Gaussian prototypes following\nlines 6-14\n33:\nend if\n34: until Convergence\ntribution for a given corpus can be infeasible. Hence, it\nis desirable to be able to adapt an existing model trained\non a large labeled corpus to a smaller or unlabelled corpus\ndrawn from the same vocabulary. In this experiment, we\naim to incrementally learn a model for an unlabelled cor-\npus given multiple sequentially available labeled corpora\n(drawn from the same vocabulary).\nWe evaluated our method on the sentiment classiﬁcation\nAlgorithm 2 Domain Incremental Learning\n1: Require: Target samples Dt, Gaussian Prototypes\nPc\ns, model parameters\nθf (t)\ns , θg(t)\ns , θf (t)\nt , θg(t)\nt , θd(t), Training\nsample size N\n2: Initialize: θf (t)\nt\n←θf (t)\ns , θg(t)\nt\n←θg(t)\ns\n3: Loss ←[Lc, Ld]\n4: Opt ←[Adam{f (t)\nt\n}, Adam{d(t),f (t)\nt\n}]\n5: iter ←0\n6: repeat\n7:\niter ←iter+1\n8:\nfor uc\ns ∼Pc\ns do\n9:\nˆy ←gt(uc\ns)\n10:\nLc ←Lc + lce(ys, c)\n11:\nend for\n12:\nfor uc\ns ∼Pc\ns, xt ∈Dt do\n13:\nv ←ft(xt)\n14:\nˆyd ←d([uc\ns, v])\n15:\nLd ←Ld + lce( ˆyd, [0, 1])\n16:\nend for\n17:\nθf (t)\nt\n←θf (t)\nt\n+ Adam{f (t)\nt\n}(−∇1\nN\nP Lc)\n18:\nθd(t) ←θd(t) + Adam{d(t),f (t)\nt\n}(−∇1\nN\nP Ld)\n19:\nθf (t)\nt\n←θf (t)\nt\n−Adam{d(t),f (t)\nt\n}(−∇1\nN\nP Ld)\n20: until Convergence\ntask on the Amazon reviews dataset (McAuley et al., 2015;\nHe and McAuley, 2016), which consists of global vec-\ntors (GloVe) for product text reviews. We extracted tokens\nby removing duplicate and incomplete rows and removing\nstop words, punctuation, and alphanumeric words. We con-\nverted the resultant tokens to GloVe vectors (Pennington\net al., 2014) of shape (1, 300) and computed a mean vector\nfor the review. We processed a maximum of 5000 reviews\nfor every product. We used the review rating as the label.\nWe compare the ﬁnal predictive accuracy after 24 iterations\nin an incremental setting for categories of the amazon re-\nviews dataset. We set each category as the target dataset\nand the rest of the datasets. The ﬁrst iteration (t + 0) is the\nbaseline model trained using the ﬁrst source dataset picked\nrandomly and the selected target dataset. We split the target\ndataset into training and test data using stratiﬁed sampling\nto ensure robust testing. We remove the labels for the train-\ning target data.\nThe results (Figure 3) clearly show that modeling the task\nas an incremental learning task leads to better target task\nperformance for most categories. We observe that com-\npared methods perform better or equally good for digital\nmusic, groceries, and electronics categories, which we ob-\nserved was due to near normal distribution of the label set,\nwith the majority of labels being 2 or 3.\nAdversarial Learning Networks: Source-free Unsupervised Domain Incremental Learning\n(a) Electronics reviews target\n(b) Grocery reviews target\n(c) Movies reviews target\n(d) Books reviews target\nFigure 3: Comparison of related unsupervised incremental learning methods with the proposed method (ALeN). (a-d)\nshow results for ﬁve increments for Electronics, Grocery, Movies, and Books review sentiment prediction target domains,\nrespectively\n4.2\nIncremental learning for Alzheimer’s disease\nprediction\nAlzheimer’s disease has been the focus of many disease\nprediction works due to the availability of labeled data from\nthe Alzheimer’s Disease Neuroimaging Initiative (ADNI)\ndatabase (adni.loni.usc.edu).\nThe dataset was collected\nfrom the US and Canadian populations. This dataset has\nbeen the source for multiple studies and disease predic-\ntion models (Dimitriadis et al., 2018; Parisot et al., 2018;\nMueller et al., 2005; Hansson et al., 2018; Eskildsen et al.,\n2013). Models trained on the ADNI dataset have poor per-\nformance when applied to real-world data from a popula-\ntion other than US and Canada. We overcome this bottle-\nneck by formulating an incremental learning scenario, with\navailable labeled datasets as sources and the unlabelled tar-\nget population dataset as the target task.\nIn the following experiments, we use brain MRI imag-\ning data obtained from ADNI and the Australian Imaging,\nBiomarker & Lifestyle Flagship Study of Ageing (AIBL)\ndatasets. ADNI and AIBL are multi-site longitudinal stud-\nies aimed at understanding the progression of Alzheimer’s\ndisease. The datasets consist of clinical, imaging, genetic,\nand biomarker data. We are interested in predictive mod-\nels for the early detection of dementia; hence we select the\nscreening stage MRI images for training.\nAlzheimer’s disease progression symptoms include atro-\nphied brain regions, especially the Hippocampus and Tem-\nporal regions. We used voxel-based morphometry (VBM)\n(Ashburner and Friston, 2000) to extract regional volumes\nfor different regions of interest in the brain. We extract\n112 regions of interest from the preprocessed gray mat-\nter images using the automated anatomical labeling atlas\n(AAL-MNI152 template) to create volumetric datasets for\nindividual Regions of interest (ROIs).\nWe use ten dataset replicates for this experiment. We train\nthe models with ten random parameter initializations. We\nuse an 80/10/10 split for training, testing, and valida-\ntion. We present the results in Figure 4, showing that our\nproposed approach (ALeN) can incrementally update the\nmodel with minimal forgetting for previously observed do-\nmains.\nTable 2 shows a comparison of the average accuracy\nand forgetting of the existing methods and the proposed\nmethod. We observe that the unsupervised continual learn-\nAbhinit Kumar Ambastha, Leong Tze Yun\n(a) Hippocampal left (HL) target domain\n(b) Hippocampal right (HR) target domain\n(c) Temporal left (TL) target domain\n(d) Temporal right (TR) target domain\n(e) Occipital left (OL) target domain\n(f) Occipital right (OR) target domain\nFigure 4: Comparison of related unsupervised incremental learning methods with the proposed method (ALeN). (a-f) show\nthe overall accuracy for past domains for all iterations for different target domains.\ning methods (CMA-GFK and CMA-SA) were able to learn\nan incremental hypothesis but failed to achieve a high pre-\ndiction accuracy. The methods use a domain alignment ap-\nproach which assumes a linear mapping between a low-\ndimension source and target embedding. Embeddings gen-\nerated using feature reduction methods such as principal\ncomponent analysis (PCA) might not be able to learn very\nefﬁcient principal components if the features do not have\nmuch variance. Also, we observe that the support vector\nmachine (SVM) classiﬁer has a low accuracy on the task,\nwhich reduces the overall accuracy of the approach.\nThe adversarial domain adaptation methods (MDAN and\nB-DANN) select the best feature mapping for the source\ndomain. These methods’ target domain accuracy is high,\nbut they fail to learn from the source domain data incre-\nmentally. When we apply this feature mapping function to\nthe target domain, we realize that if the model has a low\nerror on the source domain, it stops updating the feature\nextractor for future source domains.\nC-DANN is a variant of the adversarial domain adaptation\nnetwork in which we provide a representational memory.\nThis memory is used to store past data samples but has a\nlimited size. This emulates an incremental learning prob-\nlem setting where we store past samples. We observe that\nAdversarial Learning Networks: Source-free Unsupervised Domain Incremental Learning\nADDA\nCMA-GFK (SVM)\nCMA-SA (SVM)\nMDAN\nTarget\nAvg. acc\nFgt(%)\nAvg. acc\nFgt(%)\nAvg. acc\nFgt(%)\nAvg. acc\nFgt(%)\nHippocampal L\n76.51%\n-0.83%\n77.89%\n3.48%\n74.09%\n4.34%\n78.79%\n0.17%\nHippocampal R\n77.41%\n0.00%\n78.86%\n1.76%\n71.22%\n0.10%\n80.65%\n0.00%\nTemporal L\n78.10%\n-4.66%\n76.83%\n2.89%\n67.11%\n3.19%\n81.28%\n-3.14%\nTemporal R\n78.74%\n0.00%\n78.28%\n1.31%\n71.73%\n5.22%\n79.90%\n0.00%\nOccipital L\n69.29%\n0.00%\n68.77%\n1.21%\n59.81%\n2.40%\n70.10%\n0.00%\nOccipital R\n77.14%\n-1.19%\n74.62%\n-0.95%\n70.10%\n9.65%\n81.32%\n-2.98%\nB-DANN\nC-DANN\nFT\nALeN\nTarget\nAvg. acc\nFgt(%)\nAvg. acc\nFgt(%)\nAvg. acc\nFgt(%)\nAvg. acc\nFgt(%)\nHippocampal L\n79.21%\n1.42%\n85.92%\n-0.73%\n66.71%\n0.00%\n87.88%\n-2.90%\nHippocampal R\n82.81%\n-0.91%\n88.73%\n-7.48%\n69.67%\n-0.42%\n89.46%\n0.70%\nTemporal L\n80.94%\n-1.72%\n86.96%\n-0.13%\n64.87%\n-0.42%\n90.79%\n-0.17%\nTemporal R\n78.76%\n3.74%\n87.39%\n-8.36%\n67.32%\n-0.78%\n86.73%\n-1.25%\nOccipital L\n72.32%\n4.82%\n80.02%\n-0.44%\n65.19%\n-0.35%\n74.74%\n-3.39%\nOccipital R\n78.37%\n2.01%\n79.52%\n-7.12%\n74.67%\n0.00%\n80.58%\n-5.11%\nTable 2: A comparison of our method (ALeN) with unsupervised adversarial domain adaptation method (ADDA), unsuper-\nvised continual learning methods (CMA-GFK, CMA-SA), unsupervised multi-task learning methods (MDAN, B-DANN,\nC-DANN) and ﬁne-tuning (FT) using only labeled source data. Avg. acc (%) is the average overall accuracy and Fgt(%) is\nthe forgetting at the end of all iterations. We average the results over 100 runs.\nthe method could minimize forgetting using the stored sam-\nples. However, due to the limited size of the memory, for-\ngetting occurs as the number of increments increases.\n4.3\nAblation study and sensitivity analysis\nWe present a sensitivity analysis and an ablation study for\nthe proposed method. We illustrate the effects of the anal-\nysis on the automated disease diagnosis task for predict-\ning Alzheimer’s disease using a non-stationary source do-\nmain (simulated using regions Hippocampus Right, Tem-\nporal left and right, and occipital left and right) and non-\nstationary target domain data (sampled from Hippocampus\nLeft).\nEffectiveness of Gaussian prototypes and OOD sam-\nples: We analyzed the sensitivity of the hyper-parameter\nk used for carrying out negative sample identiﬁcation in\nthe foresighted learning algorithm (algorithm 1). We use\na 3-σ conﬁdence interval to pseudo-label the |Cs + 1|th-\nclass OOD samples following the empirical rule for Gaus-\nsian distributions. In order to verify that our Gaussian esti-\nmates are accurate, we empirically tested the efﬁciency of\nthe assumed conﬁdence interval. We observed that 3-σ pro-\nvided the maximum predictive accuracy and best captured\nthe source distribution characteristics. By setting k to 5, we\ncan set the minimum number of negative samples, which\ncan be considered an ablation of OOD samples. We ob-\nserve the average accuracy for the target task to be 77.20%\ncompared to 89.46% when using a 3-σ conﬁdence interval.\nEffect of balancing source and target unlabelled data:\nWe used a balanced source (Nsrc) and target (Nneg) do-\nmain dataset to train our baseline model. We test the ro-\nbustness of our model to imbalanced data by varying the\nNsrc/Nneg ratio by ±0.5.\nEffect of class separation loss: We carry out the abla-\ntion study by removing the class separation loss. We learn\nthe post-increment accuracy of the target domain classiﬁer\nwithout applying the class separation loss (Ls1). We ob-\nserve that the average prediction accuracy without the loss\nminimization was 70.39% compared to 89.46% using the\nclass separation loss.\n5\nCONCLUSION\nIn this work, we present ALeN, an approach for unsuper-\nvised domain incremental learning. We present an empir-\nical analysis of our approach by applying it domain incre-\nmental learning tasks in sentiment prediction and disease\nclassiﬁcation applications. We compare our approach to\nexisting state-of-the-art methods to show that the method\nachieves promising results all our experiments. We present\na structured ablation study to observe the impact of dif-\nferent components of our proposed algorithm. Our work\nAbhinit Kumar Ambastha, Leong Tze Yun\nshows that we can incrementally update DNN classiﬁcation\nmodels using unlabelled data without storing past training\ndata.\nOne of the limitations of this method is that it assumes the\nsource and target data to belong to the same global fea-\nture space, which might not be the case in several appli-\ncations. We identify this problem setting as feature incre-\nmental learning, and aim to address it in our future works.\nAlso, due to limited size of the parametric space of the\nDNN models (assumed in our work), we observe that our\nmethod is prone to overﬁtting after multiple increments. In\nthis paper, we have not analysed this issue and aim to do\nso in our future works. Some works in the literature over-\ncome this limitation by adding additional model parameters\nas new domains are incrementally added (Li and Hoiem,\n2017; Polikar et al., 2010).\nReferences\nJ. Ashburner and K. J. Friston. Voxel-based morphome-\ntry—the methods. Neuroimage, 11(6):805–821, 2000.\nS. Bickel, M. Br¨uckner, and T. Scheffer. Discriminative\nlearning under covariate shift. Journal of Machine Learn-\ning Research, 10(9), 2009.\nJ. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and\nJ. Wortman. Learning bounds for domain adaptation. In\nAdvances in neural information processing systems, pages\n129–136, 2008.\nF. M. Castro, M. J. Mar´ın-Jim´enez, N. Guil, C. Schmid,\nand K. Alahari. End-to-end incremental learning. In Pro-\nceedings of the European conference on computer vision\n(ECCV), pages 233–248, 2018.\nA. Chan, A. Alaa, Z. Qian, and M. Van Der Schaar. Unla-\nbelled data improves Bayesian uncertainty calibration un-\nder covariate shift. In H. D. III and A. Singh, editors,\nProceedings of the 37th International Conference on Ma-\nchine Learning, volume 119 of Proceedings of Machine\nLearning Research, pages 1392–1402. PMLR, 13–18 Jul\n2020. URL https://proceedings.mlr.press/\nv119/chan20a.html.\nN. Churamani, O. Kara, and H. Gunes.\nDomain-\nincremental continual learning for mitigating bias in fa-\ncial expression and action unit recognition. arXiv preprint\narXiv:2103.08637, 2021.\nS. I. Dimitriadis, D. Liparas, M. N. Tsolaki, A. D. N.\nInitiative, et al.\nRandom forest feature selection, fu-\nsion and ensemble strategy: Combining multiple morpho-\nlogical mri measures to discriminate among healhy el-\nderly, mci, cmci and alzheimer’s disease patients: From\nthe alzheimer’s disease neuroimaging initiative (adni)\ndatabase. Journal of neuroscience methods, 302:14–23,\n2018.\nG. Ditzler, M. D. Muhlbaier, and R. Polikar. Incremental\nlearning of new classes in unbalanced datasets: Learn++.\nudnc. In International Workshop on Multiple Classiﬁer\nSystems, pages 33–42. Springer, 2010.\nR. Elwell and R. Polikar. Incremental learning of concept\ndrift in nonstationary environments. IEEE Transactions\non Neural Networks, 22(10):1517–1531, 2011.\nS. F. Eskildsen, P. Coup´e, D. Garc´ıa-Lorenzo, V. Fonov,\nJ. C. Pruessner, D. L. Collins, A. D. N. Initiative, et al.\nPrediction of alzheimer’s disease in subjects with mild\ncognitive impairment from the adni cohort using patterns\nof cortical thinning. Neuroimage, 65:511–521, 2013.\nY.\nGanin,\nE.\nUstinova,\nH.\nAjakan,\nP.\nGermain,\nH. Larochelle, F. Laviolette, M. Marchand, and V. Lem-\npitsky. Domain-adversarial training of neural networks.\nThe Journal of Machine Learning Research, 17(1):2096–\n2030, 2016.\nO. Hansson, J. Seibyl, E. Stomrud, H. Zetterberg, J. Q.\nTrojanowski, T. Bittner, V. Lifke, V. Corradini, U. Eichen-\nlaub, R. Batrla, et al. Csf biomarkers of alzheimer’s dis-\nease concord with amyloid-β pet and predict clinical pro-\ngression: a study of fully automated immunoassays in\nbioﬁnder and adni cohorts. Alzheimer’s & Dementia, 14\n(11):1470–1481, 2018.\nR. He and J. McAuley. Ups and downs: Modeling the\nvisual evolution of fashion trends with one-class collabo-\nrative ﬁltering. In proceedings of the 25th international\nconference on world wide web, pages 507–517, 2016.\nJ. Hoffman, B. Kulis, T. Darrell, and K. Saenko. Discov-\nering latent domains for multisource domain adaptation.\nIn European Conference on Computer Vision, pages 702–\n715. Springer, 2012.\nJ. Hoffman, T. Darrell, and K. Saenko. Continuous man-\nifold based adaptation for evolving visual domains.\nIn\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 867–874, 2014.\nS. Ioffe and C. Szegedy. Batch normalization: Accelerat-\ning deep network training by reducing internal covariate\nshift. arXiv preprint arXiv:1502.03167, 2015.\nY. Kim, D. Cho, K. Han, P. Panda, and S. Hong. Do-\nmain adaptation without source data. IEEE Transactions\non Artiﬁcial Intelligence, 2(6):508–518, 2021.\nJ. N. Kundu, R. M. Venkatesh, N. Venkat, A. Revanur,\nand R. V. Babu.\nClass-incremental domain adaptation.\nIn Computer Vision–ECCV 2020: 16th European Con-\nference, Glasgow, UK, August 23–28, 2020, Proceedings,\nPart XIII 16, pages 53–69. Springer, 2020.\nAdversarial Learning Networks: Source-free Unsupervised Domain Incremental Learning\nZ. Li and D. Hoiem. Learning without forgetting. IEEE\ntransactions on pattern analysis and machine intelli-\ngence, 40(12):2935–2947, 2017.\nJ. McAuley, C. Targett, Q. Shi, and A. Van Den Hengel.\nImage-based recommendations on styles and substitutes.\nIn Proceedings of the 38th international ACM SIGIR con-\nference on research and development in information re-\ntrieval, pages 43–52, 2015.\nG. McGaughey,\nW. P. Walters,\nand B. Goldman.\nUnderstanding covariate shift in model performance.\nF1000Research, 5, 2016.\nS. G. Mueller, M. W. Weiner, L. J. Thal, R. C. Petersen,\nC. R. Jack, W. Jagust, J. Q. Trojanowski, A. W. Toga, and\nL. Beckett. Ways toward an early diagnosis in alzheimer’s\ndisease: the alzheimer’s disease neuroimaging initiative\n(adni). Alzheimer’s & Dementia, 1(1):55–66, 2005.\nM. Muhlbaier, A. Topalis, and R. Polikar. Learn++. mt:\nA new approach to incremental learning. In International\nWorkshop on Multiple Classiﬁer Systems, pages 52–61.\nSpringer, 2004.\nM. D. Muhlbaier, A. Topalis, and R. Polikar. Learn ++.\nnc: Combining ensemble of classiﬁers with dynamically\nweighted consult-and-vote for efﬁcient incremental learn-\ning of new classes. IEEE transactions on neural networks,\n20(1):152–168, 2008.\nS. Parisot, S. I. Ktena, E. Ferrante, M. Lee, R. Guerrero,\nB. Glocker, and D. Rueckert. Disease prediction using\ngraph convolutional networks: application to autism spec-\ntrum disorder and alzheimer’s disease.\nMedical image\nanalysis, 48:117–130, 2018.\nJ. Pennington, R. Socher, and C. D. Manning.\nGlove:\nGlobal vectors for word representation. In Proceedings\nof the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP), pages 1532–1543, 2014.\nR. Polikar, L. Upda, S. S. Upda, and V. Honavar. Learn++:\nAn incremental learning algorithm for supervised neural\nnetworks. IEEE transactions on systems, man, and cyber-\nnetics, part C (applications and reviews), 31(4):497–508,\n2001.\nR. Polikar, J. DePasquale, H. S. Mohammed, G. Brown,\nand L. I. Kuncheva. Learn++. mf: A random subspace\napproach for the missing feature problem. Pattern Recog-\nnition, 43(11):3817–3832, 2010.\nJ. Snell, K. Swersky, and R. S. Zemel.\nPrototyp-\nical networks for few-shot learning.\narXiv preprint\narXiv:1703.05175, 2017.\nS. Srivastava, M. Yaqub, K. Nandakumar, Z. Ge, and\nD. Mahapatra.\nContinual domain incremental learning\nfor chest x-ray classiﬁcation in low-resource clinical set-\ntings. In Domain Adaptation and Representation Transfer,\nand Affordable Healthcare and AI for Resource Diverse\nGlobal Health, pages 226–238. Springer, 2021.\nE. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adver-\nsarial discriminative domain adaptation. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 7167–7176, 2017.\nM. Wulfmeier, A. Bewley, and I. Posner. Incremental ad-\nversarial domain adaptation for continually changing en-\nvironments. In 2018 IEEE International conference on\nrobotics and automation (ICRA), pages 1–9. IEEE, 2018.\nS. Yang, Y. Wang, J. Van De Weijer, L. Herranz, and\nS. Jui.\nGeneralized source-free domain adaptation.\nIn\nProceedings of the IEEE/CVF International Conference\non Computer Vision, pages 8978–8987, 2021.\nF. Zenke, B. Poole, and S. Ganguli. Continual learning\nthrough synaptic intelligence.\nProceedings of machine\nlearning research, 70:3987, 2017.\nH. Zhao, S. Zhang, G. Wu, J. M. Moura, J. P. Costeira,\nand G. J. Gordon. Adversarial multiple source domain\nadaptation. In Advances in neural information processing\nsystems, pages 8559–8570, 2018.\nS. Zhao, G. Wang, S. Zhang, Y. Gu, Y. Li, Z. Song, P. Xu,\nR. Hu, H. Chai, and K. Keutzer.\nMulti-source distill-\ning domain adaptation. arXiv preprint arXiv:1911.11554,\n2019.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-01-28",
  "updated": "2023-01-28"
}