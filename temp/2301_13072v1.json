{
  "id": "http://arxiv.org/abs/2301.13072v1",
  "title": "Guided Deep Reinforcement Learning for Articulated Swimming Robots",
  "authors": [
    "Jiaheng Hu",
    "Tony Dear"
  ],
  "abstract": "Deep reinforcement learning has recently been applied to a variety of\nrobotics applications, but learning locomotion for robots with unconventional\nconfigurations is still limited. Prior work has shown that, despite the simple\nmodeling of articulated swimmer robots, such systems struggle to find effective\ngaits using reinforcement learning due to the heterogeneity of the search\nspace. In this work, we leverage insight from geometric models of these robots\nin order to focus on promising regions of the space and guide the learning\nprocess. We demonstrate that our augmented learning technique is able to\nproduce gaits for different learning goals for swimmer robots in both low and\nhigh Reynolds number fluids.",
  "text": "Guided Deep Reinforcement Learning for Articulated Swimming Robots\nJiaheng Hu1 and Tony Dear1\nAbstract— Deep reinforcement learning has recently been\napplied to a variety of robotics applications, but learning\nlocomotion for robots with unconventional conﬁgurations is still\nlimited. Prior work has shown that, despite the simple modeling\nof articulated swimmer robots, such systems struggle to ﬁnd\neffective gaits using reinforcement learning due to the hetero-\ngeneity of the search space. In this work, we leverage insight\nfrom geometric models of these robots in order to focus on\npromising regions of the space and guide the learning process.\nWe demonstrate that our augmented learning technique is able\nto produce gaits for different learning goals for swimmer robots\nin both low and high Reynolds number ﬂuids.\nI. INTRODUCTION\nArticulated swimming robots have attracted much interest\nfrom researchers due to their effective locomotive capabilities\nas well as the richness of their geometric structure. The basis\nof their locomotion arises from the interaction between ac-\ntuation of their joints and the surrounding ﬂuid environment.\nSuch interactions depend highly on the nature of the ﬂuid,\nbut previous work has shown that in the cases of extremely\nlow or extremely high Reynolds number ﬂuids, a kinematic\nsystem can be approximated, leading to great insights into\ntrajectory planning [1].\nEven for these idealized systems, however, it is still\ndifﬁcult to derive optimal trajectories analytically. These\ndifﬁculties are compounded when dealing with robots with\nmore complex morphologies or higher-dimensional joint\nspaces. Deep reinforcement learning (RL) has recently shown\npromise to be an effective search strategy, as algorithms\nhave developed to make techniques feasible on physical\nsystems. However, the heterogeneity of the search space and\nthe sparsity of the corresponding reward functions introduce\nadditional challenges for motion planning with RL.\nIn this paper, we exploit the geometric structure of three-\nlink swimmer systems in low and high Reynolds number\nﬂuids to restrict the search space of our reinforcement\nlearning algorithm and learn effective locomoting gaits from\na blank slate. We show that this approach is able to speed\nup training time, as the robot is less likely to be trapped into\nexecuting suboptimal gaits. At the same time, we show that\nthe RL method is still ﬂexible enough to be optimized for\ndifferent objectives, such as energy and speed.\nTo the best of our knowledge, this is the ﬁrst attempt to\nconﬁne RL policy search by utilizing the geometry of the\nsystem at hand. This is also one of the ﬁrst attempts to the\nlocomotion problem of articulated swimmers using model-\nfree deep reinforcement learning.\n1Computer Science Department, Columbia University, New York, NY\n10027, USA {jh3916, tbd2115}@columbia.edu\nFig. 1: A swimming snake robot comprised of three artic-\nulated slender bodies. The coordinates (x, y, θ) denote the\nSE(2) inertial conﬁguration of the proximal link, which also\nhas velocities (ξx, ξy, ξθ) relative to a body-ﬁxed frame. The\nrelative angles of the joints are denoted α = (α1, α2).\nII. PRIOR WORK\nA. Geometric Structure\nIn recent decades, techniques and methods from geometric\nmechanics have been a popular way to model and control\nmechanical systems. A key idea is that of symmetries in a\nsystem’s conﬁguration space, which allow for the reduction\nof the equations of motion to a simpler form. This has been\naddressed for general mechanical systems by [2], as well\nas nonholonomic systems by [3]. For locomoting systems,\ngeometric reduction is often leveraged in tandem with a\ndecomposition of the conﬁguration variables into actuated\nshape variables and unactuated position variables. If such\na splitting is possible, then the conﬁguration space often\ntakes on a ﬁber bundle structure, whereby a mapping called\nthe connection relates trajectories between each subspace.\nAnalysis of the connection can then give us intuition into\nways to perform motion planning and control of the system,\nas detailed by [4] and [5]. This mathematical structure also\nlends itself to visualization and design tools, detailed by [6].\nMuch of the progress in the geometric mechanics of loco-\nmotion is predicated on the assumption that the symmetries\nof a system coincide exactly with the position degrees of\nfreedom. Robots that can be modeled with nonholonomic\nconstraints are examples in which these symmetries occur.\nNonholonomic wheeled snake robots have received consid-\nerable attention from researchers such as [7] and [8] treating\nthem as kinematic systems, so named because constraints\nthat eliminate the need to consider second-order dynamics\nwhen modeling its locomotion. This allows for the treatment\nof the system’s locomotion, and subsequent motion planning,\nas a result of geometric phase (see [9], [10], [11], [12]).\nGeometric methods have also examined systems locomot-\ning in ﬂuids. As with terrestrial systems, such a description\narXiv:2301.13072v1  [cs.RO]  30 Jan 2023\nis most useful if the position degrees of freedom correspond\nto system symmetries and the rest to internal shape. For\nsingle bodies, motion may be achieved through temporal\ndeformation of the body’s shape. For articulated swimmers\nlike the three-link robot shown in Fig. 1, deformation occurs\nnaturally when joints are moved relative to each other (see\n[1], [13], [14]), analogous to the terrestrial version of the\nsystem.\nArticulated swimmer robots belong to a family of gen-\neral snake-like robots, which are characterized by a large\nnumber of degrees of freedom and locomotion patterns\nthat exhibit cyclic motions through coordination of their\njoints. Therefore, snake-like robots are usually controlled\nthrough kinematics-based methods [15], [16]. These meth-\nods, however, often rely on hand-tuning a number of different\nparameters, which can be costly as well as inﬂexible in new\nenvironments.\nB. Gait Optimization and Reinforcement Learning\nThe problem of gait optimization has been approached\nthrough a variety of traditional optimization methods, such\nas evolutionary algorithms [17], gradient-based methods [18]\nand Bayesian optimization [19]. However, these methods\noften suffer from local optima, and while the resulting gaits\nappear effective in locomoting the robots, they are often\nstill quite inefﬁcient when compared to the natural motion\nachieved by animals.\nReinforcement learning is a data-driven method that\nsearches for a reward-maximizing policy under a given\nenvironment. As an algorithm based on trial-and-error, it\nhas the advantage of not requiring a speciﬁc model of the\nenvironment or expert knowledge of the problem. With re-\ncent advancements in deep neural network and reinforcement\nlearning algorithms, reinforcement learning has become a\nuseful tool for solving robot control tasks such as walker’s lo-\ncomotion [20], dexterous manipulation [21], and autonomous\ndriving [22].\nThere have been a few attempts to solve the problem of\ngait optimization through reinforcement learning. Bing et\nal. [23] used PPO to train a forward-locomotion controller\nfor a wheeled snake robot and were able to generate gaits\nthat out-perform those derived from Bayesian optimization\nand grid search. Sharma and Kitani [24] proposed phase-\nDDPG, where they explicitly trained a cyclic policies for a\nwalker robot by oscillating the weight of the policy network\nwith the phase of the robot. These methods were able to\ngenerate fairly natural gaits on certain robots, but often failed\nto converge to global optima as the robot environment grew\nmore complex. For example, none of the methods were able\nto solve the swimmer environment [25].\nIII. MODEL AND METHODS\nA. Swimmer Model\nAs shown in Fig. 1, our swimmer robot consists of three\nrigid links, each of length R, which can rotate relative to\none another. Its conﬁguration is deﬁned by q ∈Q = G × B,\nwhere g = (x, y, θ)T ∈G = SE(2) speciﬁes the position\nand orientation of the ﬁrst link in an inertial frame; we\nmeasure a link’s position at the center of the link. The\njoint angles α = (α1, α2)T ∈B specify the links’ relative\norientation. We can view Q as a principal ﬁber bundle,\nin which trajectories in the shape or base space B lift to\ntrajectories in the group G (see [11]).\n1) Low Reynolds Number: Following the treatment of [1],\nwe assume that the swimmer is comprised of three slender\nbodies and suspended in a planar ﬂuid. In the low Reynolds\nnumber case, viscous drag forces dominate inertial forces.\nThis allows us to approximate the drag forces as linear\nfunctions of the system’s body and shape velocities ξ and\n˙α; we also assume that net forces acting on the system are\nzero for all time due to damping out by drag forces. We can\nthen derive a Pfafﬁan constraint on the swimming system’s\nvelocities as\nF =\n\n\nFx\nFy\nFθ\n\n=\n\n\n0\n0\n0\n\n= ω1(α)ξ + ω2(α) ˙α,\n(1)\nwhere ω1 ∈R3×3 and ω2 ∈R3×2. The variables ξ =\n(ξx, ξy, ξθ)T give us the body velocity of the system, as\nshown in Fig. 1. In SE(2), the mapping that takes body\nvelocities to inertial velocities is given by ˙g = TeLgξ, where\nTeLg =\n\n\ncos θ\n−sin θ\n0\nsin θ\ncos θ\n0\n0\n0\n1\n\n.\n(2)\nThe full forms of these components can be found in [1].\nThe general approach would be to ﬁrst compute local drag\nforces on each link, and then combine them to ﬁnd the total\nforce components for each of the body frame directions. In\naddition to the system link length R, the kinematics also\nutilize the drag constant of the ﬂuid, characterized by k.\nSince the number of independent constraints is equal to\nthe dimension of the group, these equations are sufﬁcient\nto derive a kinematic connection for the system ([8]). In\nother words, the constraint equations fully describe the ﬁrst-\norder dynamics of the group variables in terms of the shape\nvariables only. Thus, Eq. (3) can be rearranged to show this\nexplicitly as the kinematic reconstruction equation:\nξ = −A(α) ˙α = −ω−1\n1 ω2 ˙α.\n(3)\nA(α) is called the local connection form, a mapping that\ndepends only on the shape variables, in this case α1 and α2.\n2) High Reynolds Number: The high Reynolds number\ncase is opposite from the low Reynolds number environment\nin that inertial forces dominate viscous forces. Despite the\nentirely different swimming conditions, the model of the\nswimmer robot can once again be approximated as kine-\nmatic. A Lagrangian for the robot can be expressed in terms\nof its kinetic energy, as there is no means of storing energy\nor application of external forces:\nL = 1\n2\n\u0000ξ\n˙α\n\u0001\nM(α)\n\u0012ξ\n˙α\n\u0013\n.\n(4)\nThe mass matrix M is a function of the system conﬁguration\nα, and it can be decomposed into blocks containing the\nsystem’s local connection [8]:\nM(α) =\n\u0012\nI(α)\nI(α)A(α)\n(I(α)A(α))T\nm(α)\n\u0013\n.\nTo derive the mass matrix M, we recognize that the\nLagrangian of the three-link system is equal to the sum of the\nLagrangians Li of each of the individual links. Each link has\nan associated inertia tensor Ii dependent on the shape that\nwe use to model it. In addition, each link has an added mass\nMi, which arises due to the inertia of a displaced ﬂuid as a\nbody moves through it; like the inertia tensor, Mi is solely\na function of the body geometry. [1] gives an example of the\nadded mass tensor for an elliptical body. The total effective\ninertia of a single link is then Ii + Mi, which gives us a\nLagrangian of the form\nL =\n3\nX\ni=1\nLi =\n3\nX\ni=1\n1\n2ξT\ni (Ii + Mi)ξi\n(5)\nOnce the total Lagrangian is written down, it can be\nrearranged into the form of Eq. (4), from which the local\nconnection A(α) can then be extracted.\n3) Connection Visualization: The structure of the connec-\ntion form in Eq. (3) can be visualized in order to understand\nthe response of ξ to input trajectories without regard to time,\naccording to [6]. We can ﬁrst integrate each row of Eq. (3)\nover time to obtain a measure of displacement corresponding\nto the body frame directions. In the world frame, this measure\nprovides the exact rotational displacement, i.e., ˙θ = ξθ for\nthe third row, and an approximation of the translational\ncomponent for the ﬁrst two rows. If our input trajectories\nare periodic, we can transform this “body velocity integral”\ninto one over the trajectory ψ : [0, T] →B in the joint\nspace, since the kinematics are independent of input pacing.\nStokes’ theorem can then be applied to perform a second\ntransformation into an area integral over β, the region of the\njoint space enclosed by ψ:\n−\nZ T\n0\nA(α(τ)) ˙α(τ) dτ = −\nZ\nψ\nA(α) dα = −\nZ\nβ\ndA(α).\n(6)\nThe integrand in the rightmost integral is the exterior deriva-\ntive of A, computed as the curl of A in two dimensions.\nFor example, the connection exterior derivative of Eq. (3)\nhas three components, one for each row i given by\ndAi(α) = ∂Ai,2\n∂α1\n−∂Ai,1\n∂α2\n,\n(7)\nwhere Ai,j is the element corresponding to the ith row and\njth column of A.\nThe magnitudes of the body-x component (ﬁrst row) of\nthe connection exterior derivative of each swimmer over the\nα1-α2 joint space, for a ﬁxed set of sample parameters,\nare depicted in Fig. 2. The area integral over an enclosed\nregion is the geometric phase, a measure of the expected\ndisplacement in the corresponding direction. In particular, a\nFig. 2: Visualizations of the body-x components of the\nlocal connection’s exterior derivative for the low and high\nReynolds swimmers, respectively. Periodic trajectories can\nbe represented as closed curves on these surfaces, and the\nrobot’s associated displacement corresponds to the enclosed\nvolume.\ntrajectory that advances in a counter-clockwise direction over\ntime in joint space will yield positive displacement, since that\ncorresponds to a positive area integral; negative displacement\nis achieved with a clockwise trajectory.\nFor both swimmers, we see that a high value of the\nbody velocity integral, and thus a high displacement per\ngait cycle, is generally achieved by executing gaits that\nencircle a zero contour of these exterior derivative surfaces.\nHowever, the optimal parameters of these gaits differ for the\ntwo swimmers, with a larger range for the low Reynolds\ncase and a smaller range for the high Reynolds case. In\naddition, the means of ﬁnding a gait is not obvious when\nthe joint angles are restricted to be smaller than the zero\ncontour. Finally, while we do not show them here we may\nalso be concerned with the y and θ components as well.\nAnalytically optimizing gaits is thus equivalent to solving\na multi-objective constrained optimization problem over a\ncontinuous space, a task that becomes exponentially more\ndifﬁcult with increasing system complexity.\nB. Baseline-Guided Policy Search (BGPS)\nBased on the geometric models of the robots, we pro-\npose an augmented reinforcement learning algorithm called\nBaseline-Guided Policy Search (BGPS), in which we restrict\nthe policy search space of the learning algorithm by utilizing\na baseline policy approximated from the geometric structure.\n1) Robot Environment Setup: In this work, we focus on\nlocomotion for three-link swimmer robots; the study of more\ncomplex robots will the subject of future work. The state of\nthe robot at time t is st = (α1, α2, θ, t) , which contains\nboth the joint angles and orientation of the swimmer. The\naction taken by the robot at time t is at = ( ˙α1, ˙α2), the\nvelocities of the two joints. We investigate two choices of\nreward functions, which corresponding to two tasks with\ndifferent optimization goals.\nThe ﬁrst task is to optimize the total distance the robot\ntravels in a pre-determined direction in a given amount of\ntime. The reward is therefore very straightforward: after the\nrobot makes a transition (st, at, st+1), the value of the reward\nfunction Rt is set to be\nRt = xt+1 −xt.\n(8)\nThe second task is to simultaneously maximize the dis-\ntance travelled and minimize the energy spent. We use a\nkinetic energy metric and deﬁne the reward function as\nRt = xt+1 −xt −β∥˙α∥,\n(9)\nwhere β is a coefﬁcient that controls the weight of the energy\npenalty.\n2) Proximal Policy Optimization: A number of reinforce-\nment learning algorithms have been shown to be effective\nfor different physical systems, although the comparison of\ntheir various performances is not the focus of this paper. For\nthis work, we choose the proximal policy optimization (PPO)\nalgorithm by Schulman et al. [26], in which an agent seeks\nto optimize the surrogate objective within the trust region\nby clipping the probability ratio. PPO has been shown to\noutperform other online policy gradient methods, with the\nadvantage of being easy to implement.\n3) Baseline from Geometric Structure: The key idea of\nthis work is that we can exploit what we know about\nthe system structure, e.g., as shown in Fig. 2, to help\nrestrict the search space in which reinforcement learning\noperates. Speciﬁcally, the exterior derivative plots suggest\nthat the optimal gaits for moving forward can be roughly\napproximated as single-frequency sinusoidal functions whose\njoint-space loops overlay the blue ridges and whose phases\nare large enough to encircle the widths of the same. Note\nthat the actual optimal policies have no such restriction, e.g.\nas single-frequency sinusoidal functions. This is particularly\nthe case if we have joint limits that prevent the joint angles\nfrom extending all the way out to the zero contour at the ends\nof the ridges. However, such an approximation is sufﬁcient\nfor formulating a baseline policy from which RL techniques\ncan then improve upon with a large number of degrees of\nfreedom.\n4) RL Policy from Baseline: Once we obtain a baseline\npolicy πbase(s) through the method described above, we then\nuse reinforcement learning to search for a separate policy\nFig. 3: The training curve of different action ranges for opti-\nmizing the travelled distance for the low Reynolds swimmer.\nRed: 0.1, orange: 0.2, cyan: 0.3, blue: 0.6.\nπRL(s). Our eventual policy is then\nπfinal(s) = πbase(s) + πRL(s)\n(10)\nThe most important reason for using a baseline is that we can\nnow control the size of the policy search space by reducing\nthe action range of our RL-learned policy, |πRL|. By doing\nso, we limit the policy search to be within the vicinity of our\nbaseline policy, thus guiding the policy search. A properly\nsmall action range can shape the policy search space to be\nnear convex, allowing gradient-based methods like RL to be\nparticularly suitable.\n5) Action Range from Geometric Structure: Given an\nenvironment step length t, the amount of deviation δ that\nthe robot is allowed from the baseline policy, and an action\nrange α, we can relate these quantities as δ = αt. Thus, for\neach action cycle of length T, the maximum deviation per\ncycle is δtotal = αT = Nδ, where N is the number of steps\nper cycle.\nThe choice of action range α is another parameter whose\nvalue can be informed by the system’s geometric structure.\nα can be interpreted as the maximum amount that we would\nallow the policy to “stray” away from the baseline. Since\nthe baseline is just an approximation for the optimal policy,\nα needs to be sufﬁciently large to allow exploration of the\npolicy space to occur. However, the exterior derivative plots\ncan also give us an upper bound on the action range, as there\nis a ﬁnite distance away from our chosen baseline at which\nthe effectiveness of an action would start to drop.\nIV. LEARNING AND RESULTS\nWe implement BGPS with different action ranges, and\ncompare the performances directly with PPO and phase-\nDDPG [24]. Our results show that our algorithm generally\noutperforms the other methods, and that a smaller action\nrange is able to boost the performance of the learned policy,\nconﬁrming the importance of conﬁning the policy search\nspace.\nA. Parameters\nWe implemented both a low and high Reynolds three-link\nswimmer for our simulations. We used a link length of 0.3\nBFG\nPPO\nPhase-DDPG\nBGPS (0.6)\nBGPS (0.3)\nBGPS (0.2)\nBGPS (0.15)\nBGPS (0.1)\nDistance\n111.05\n31.79\n1.14\n32.08\n39.39\n117.6\n133.3\n130.8\nEnergy\n75.08\n28.58\n0.08\n21.63\n15.61\n29.88\n37.58\n85.22\nBFG\nPPO\nPhase-DDPG\nBGPS (0.6)\nBGPS (0.3)\nBGPS (0.2)\nBGPS (0.15)\nBGPS (0.1)\nDistance\n94.71\n19.73\n13.27\n122.8\n116.5\n141.8\n126.2\n121.4\nEnergy\n58.75\n13.20\n9.62\n9.04\n9.47\n72.87\n77.31\n76.47\nTABLE I: The average reward of the learned policy for the low Reynolds swimmer (top) and high Reynolds swimmer\n(bottom). BFG refers to the baseline policy that we observed from the robots’ geometric structures (no learning). PPO and\nphase-DDPG are the main algorithms to which we compared results. BGPS refers to Baseline-Guided Policy Search (our\nmethod), with results provided for several choices of action range for different trials.\nm for the low Reynolds case, a nod toward the prevalence of\nmicro-swimmers in this category. For the high Reynolds case,\nwe set the ﬂuid density to ρ = 1 kg/m3, and treat the links as\nellipses with semi-major axis a = 4 m and semi-minor axis\nb = 1 m. The exterior derivative plots of the swimmers in\nFig. 2 were obtained using the same parameter values. Our\nenvironment step time was set to 0.04 s per step. For both\nthe low and high Reynolds swimmer, we run separate trials\nfor optimizing the speed with and without energy concern.\nWe set β to 0.1 for the task of optimizing for energy usage.\nB. Network Architecture\nWe followed the settings outlined in Schulman et al. [26]\nfor implementing PPO. Our policy network, which maps\nfrom observation to action, consists of two hidden layers\nof size 64 and a linear output layer at the end. Rectiﬁed\nLinear units (ReLU) were used as the activation function for\nevery layer except the output layer. Our value network has\nthe same architecture as our policy network, except mapping\nfrom (observation, action) to value space. No parameter is\nshared between the two networks.\nC. Training Settings\nWe run our experiments on a a computer with an i7-8650U\nCPU running at 1.90Ghz and an Nvidia GTX 1070 GPU. For\neach given algorithms and settings, we run for 2.5 million\ntime steps. For each single trial, our algorithm takes about 3\nhours to run.\nD. Results\nTable\nI shows the results of different algorithms for\nlearning locomotive gaits for each swimmer. The ”Distance”\nrow refers to the task of maximizing the distance traveled\nper time in a given direction (the x axis), and the ”Energy”\nrow refers to the task of locomoting the robot forward while\nsimultaneously minimizing the energy spent.\nBFG refers to “baseline from geometry,” which is the\nbaseline gait we estimated by looking at the geometric model\nof the robot. For both swimmers, we set a baseline of\n0.6cos(t) for each joint, with a phase difference of 1 rad\nbetween them. Baseline-Guided Policy Search (BGPS) is\nour method, and the accompanying number on each column\nheader marks the action range for that trial. Both PPO and\nPhase-DDPG are learning from scratch without utilizing the\ngeometric model, and both of them perform extremely poorly\nFig. 4: Joint angle (top) and workspace (bottom) trajectories\nof the low Reynolds swimmer from the best learning trian\n(BGPS 0.15). The joint angle trajectories are similar to but\nimprove upon the baseline derived from geometry.\ncomparing to the other methods shown. In particular, they are\nunable to learn a gait that performs even close to the baseline\ngait derived from simple inspection.\nBGPS also performs poorly when the action range is too\nlarge, but beats all other baselines as the action range is\nreduced. Fig. 3 shows the training curve of optimizing the\ndistance for the low Reynolds swimmer. We can clearly see\nfrom the plot that training tends to converge to a higher\nreward when the action range is between 0.1 and 0.2, but\nfails to converge when between 0.3 and 0.6. This shows that\na smaller action range within the appropriate region is the key\nto our algorithm’s success at locomoting the swimmer. For\nboth the low and high Reynolds swimmers, our algorithm\nproduced the best result for both the task of optimizing\ndistance and of minimizing energy spent, among all the\nmethods we tested.\nThe joint angle and workspace trajectories of the low\nReynolds swimmer learned from the best trial (BGPS 0.15)\nare shown in Fig. 4. As expected, the joint angle trajectories\nare not entirely too different from the baseline that we\nwrote down from inspection of geometry. However, subtle\ndifferences, such as the varying of the relative phases and\namplitudes of the two joints over time, suggest the existence\nof higher-frequency components that were not at all obvi-\nous from simple inspection. The accompanying workspace\ntrajectory maximizes the distance reward compared to the\nother learning trials, as shown in the ﬁrst row of Table I.\nV. CONCLUSIONS AND FUTURE WORK\nWe have leveraged traditional motion planning techniques\nfrom geometric mechanics to make deep reinforcement\nlearning feasible for training articulated swimming robots.\nSuch systems exhibit challenges, such as a policy search\nspace with many local optima, that have previously made it\ndifﬁcult for common DRL approaches. Our approach, which\ncombines intuition with learning, is able to produce superior\nresults for different robot models and different environments.\nThe fact that our algorithm is able to work across different\ntasks and robots suggests that this method may easily be\ngeneralized. Other robots with similar kinematics or even\ndynamics can beneﬁt from initialization with an informed\nbaseline. Since the baseline need not be exact, this also opens\npresents an opportunity for work with higher-dimensional\nsystems for which pure optimization is very difﬁcult. Visu-\nalization of geometry would not be necessary to determine\nthe exact form of optimal gaits.\nThe task of selecting a proper action range is still under\ninvestigation. In this work we had the ability to compare\ndifferent values of this parameter and found the best one\nfor the given robot and environment, and the interpretation\nof this parameter will certainly vary for other systems. Real\nsystems would not have the luxury of trying different values\nuntil ﬁnding the one that works best. Thus, a direct line of\nfuture work would be to determine whether the action range\ncan also be guided by system geometry.\nREFERENCES\n[1] R. L. Hatton and H. Choset, “Geometric swimming at low and high\nreynolds numbers,” IEEE Transactions on Robotics, vol. 29, no. 3, pp.\n615–624, 2013.\n[2] J. E. Marsden and T. S. Ratiu, Introduction to Mechanics and Symme-\ntry: A Basic Exposition of Classical Mechanical Systems.\nSpringer\nScience & Business Media, 2013, vol. 17.\n[3] A. M. Bloch, P. S. Krishnaprasad, J. E. Marsden, and R. M. Murray,\n“Nonholonomic mechanical systems with symmetry,” Archive for\nRational Mechanics and Analysis, vol. 136, no. 1, pp. 21–99, 1996.\n[4] S. D. Kelly and R. M. Murray, “The geometry and control of\ndissipative systems,” in Proceedings of the 35th IEEE Conference on\nDecision and Control, vol. 1.\nIEEE, 1996, pp. 981–986.\n[5] J. P. Ostrowski and J. W. Burdick, “The geometric mechanics of\nundulatory robotic locomotion,” The International Journal of Robotics\nResearch, vol. 17, no. 7, pp. 683–701, 1998.\n[6] R. L. Hatton and H. Choset, “Geometric motion planning: The local\nconnection, stokes’ theorem, and the importance of coordinate choice,”\nThe International Journal of Robotics Research, vol. 30, no. 8, pp.\n988–1014, 2011.\n[7] J. Ostrowski, “Computing reduced equations for robotic systems\nwith constraints and symmetries,” Robotics and Automation, IEEE\nTransactions on, vol. 15, no. 1, pp. 111–123, Feb 1999.\n[8] E. A. Shammas, H. Choset, and A. A. Rizzi, “Geometric motion plan-\nning analysis for two classes of underactuated mechanical systems,”\nThe International Journal of Robotics Research, vol. 26, no. 10, pp.\n1043–1073, 2007.\n[9] R. M. Murray and S. S. Sastry, “Nonholonomic motion planning:\nSteering using sinusoids,” IEEE Transactions on Automatic Control,\nvol. 38, no. 5, pp. 700–716, 1993.\n[10] R. Mukherjee and D. P. Anderson, “Nonholonomic motion planning\nusing stoke’s theorem,” in Robotics and Automation, 1993. Proceed-\nings., 1993 IEEE International Conference on. IEEE, 1993, pp. 802–\n809.\n[11] S. D. Kelly and R. M. Murray, “Geometric phases and robotic\nlocomotion,” Journal of Robotic Systems, vol. 12, no. 6, pp. 417–431,\n1995.\n[12] F. Bullo and K. M. Lynch, “Kinematic controllability for decou-\npled trajectory planning in underactuated mechanical systems,” IEEE\nTransactions on Robotics and Automation, vol. 17, no. 4, pp. 402–412,\n2001.\n[13] J. B. Melli, C. W. Rowley, and D. S. Rufat, “Motion planning for an\narticulated body in a perfect planar ﬂuid,” SIAM Journal on applied\ndynamical systems, vol. 5, no. 4, pp. 650–669, 2006.\n[14] L. Burton, R. L. Hatton, H. Choset, and A. Hosoi, “Two-link swim-\nming using buoyant orientation,” Physics of Fluids, vol. 22, no. 9, p.\n091703, 2010.\n[15] Z. Zuo, Z. Wang, B. Li, and S. Ma, “Serpentine locomotion of a\nsnake-like robot in water environment,” 03 2009, pp. 25 – 30.\n[16] M. Tesch, K. Lipkin, I. Brown, R. Hatton, A. Peck, J. Rembisz,\nand H. Choset, “Parameterized and scripted gaits for modular snake\nrobots,” Advanced Robotics, vol. 23, no. 9, pp. 1131–1158, 2009.\n[17] S. Chernova and M. Veloso, “An evolutionary approach to gait learning\nfor four-legged robots,” in 2004 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566),\nvol. 3, Sep. 2004, pp. 2562–2567 vol.3.\n[18] N. Kohl and P. Stone, “Machine learning for fast quadrupedal loco-\nmotion,” in AAAI, 2004.\n[19] R. Calandra, N. Gopalan, A. Seyfarth, J. Peters, and M. Deisenroth,\n“Bayesian gait optimization for bipedal locomotion,” 02 2014, pp.\n274–290.\n[20] X. B. Peng, G. Berseth, K. Yin, and M. van de Panne, “Deeploco:\nDynamic locomotion skills using hierarchical deep reinforcement\nlearning,” ACM Transactions on Graphics (Proc. SIGGRAPH 2017),\nvol. 36, no. 4, 2017.\n[21] A. Rajeswaran, V. Kumar, A. Gupta, J. Schulman, E. Todorov,\nand S. Levine, “Learning complex dexterous manipulation with\ndeep\nreinforcement\nlearning\nand\ndemonstrations,”\nCoRR,\nvol.\nabs/1709.10087, 2017. [Online]. Available: http://arxiv.org/abs/1709.\n10087\n[22] P. Long, T. Fan, X. Liao, W. Liu, H. Zhang, and J. Pan, “Towards\noptimally decentralized multi-robot collision avoidance via deep rein-\nforcement learning,” 2017.\n[23] Z. Bing, C. Lemke, Z. Jiang, K. Huang, and A. Knoll, “Energy-\nefﬁcient slithering gait exploration for a snake-like robot based on\nreinforcement learning,” 2019.\n[24] A. Sharma and K. M. Kitani, “Phase-parametric policies for reinforce-\nment learning in cyclic environments,” in AAAI, 2018.\n[25] R. Coulom, “Reinforcement learning using neural networks, with\napplications to motor control,” 2002.\n[26] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal\npolicy\noptimization\nalgorithms,”\narXiv\npreprint\narXiv:1707.06347, 2017.\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2023-01-30",
  "updated": "2023-01-30"
}