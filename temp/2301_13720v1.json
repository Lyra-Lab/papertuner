{
  "id": "http://arxiv.org/abs/2301.13720v1",
  "title": "Zero-shot cross-lingual transfer language selection using linguistic similarity",
  "authors": [
    "Juuso Eronen",
    "Michal Ptaszynski",
    "Fumito Masui"
  ],
  "abstract": "We study the selection of transfer languages for different Natural Language\nProcessing tasks, specifically sentiment analysis, named entity recognition and\ndependency parsing. In order to select an optimal transfer language, we propose\nto utilize different linguistic similarity metrics to measure the distance\nbetween languages and make the choice of transfer language based on this\ninformation instead of relying on intuition. We demonstrate that linguistic\nsimilarity correlates with cross-lingual transfer performance for all of the\nproposed tasks. We also show that there is a statistically significant\ndifference in choosing the optimal language as the transfer source instead of\nEnglish. This allows us to select a more suitable transfer language which can\nbe used to better leverage knowledge from high-resource languages in order to\nimprove the performance of language applications lacking data. For the study,\nwe used datasets from eight different languages from three language families.",
  "text": "Zero-Shot Cross-Lingual Transfer Language Selection Using \nLinguistic Similarity \nJuuso Eronen**, Michal Ptaszynski* and Fumito Masui* \n*Kitami Institute of Technology, 165, Koencho, Kitami, 090-0015, Hokkaido, Japan \n  \n  \nARTICLE INFO \nABSTRACT \nKeywords: \nWe study the selection of transfer languages for different Natural Language Processing tasks, \nMultilingual Natural Language Pro- \nspecifically sentiment analysis, named entity recognition and dependency parsing. In order to \ncessing \nselect an optimal transfer language, we propose to utilize different linguistic similarity metrics \nZero-Shot Learning \nto measure the distance between languages and make the choice of transfer language based on this \nTransfer Learning \ninformation instead of relying on intuition. We demonstrate that linguistic similarity correlates \nLinguistics \nwith cross-lingual transfer performance for all of the proposed tasks. We also show that there \nLanguage similarity \nis a Statistically significant difference in choosing the optimal language as the transfer source \ninstead of English. This allows us to select a more suitable transfer language which can be used to \nbetter leverage knowledge from high-resource languages in order to improve the performance of \nlanguage applications lacking data. For the study, we used datasets from eight different languages \nfrom three language families. \n1. Introduction \nAs with any other supervised learning problem, the tasks in Natural Language Processing (NLP) require sufficiently \nlarge labeled datasets. What sets NLP apart from other fields is the presence of multiple languages the datasets can \nappear in. This means that in order to successfully train models for all of the world’s 7,100 languages [1], one would \nneed to annotate a dataset for each language. This is a very difficult and costly task [2, 3] and has led to a small number \nof high-resource languages to dominate the field [4, 5, 6]. This imbalance in the distribution of resources among \nlanguages calls for the need to develop technologies that would make model development for low-resource languages \nrealistically feasible and efficient. \nIn order to address this problem, cross-lingual transfer been proposed as a solution. This means leveraging labeled \ndata from high-resource languages in order to improve the performance on lower-resource languages [7, 8, 9, 10]. \nParticularly, the popularity of cross-lingual zero-shot learning, or training on one task/language and testing on a \ndifferent task/language completely unknown to the model, has increased greatly in the recent years. Zero-shot learning \nhas gained in popularity because it does not require any labeled data in the target language for training [11, 12]. \nMoreover, zero-shot cross-lingual transfer utilizes large pre-trained multilingual transformer models like Multilingual \nBERT [13] or XLM-RoBERTa [14]. These models are fine-tuned with training data in a language called the source \nlanguage (usually a high-resource language) and then used to predict entries from other languages than that used in \ntraining, often with satisfying results [11, 15]. \nThe choice of transfer language is usually done by intuition [16] or simply defaults to English, as is the case \nwith popular multilingual benchmarks like XTREME [12] and XGLUE [17], even though there is no actual evidence \nbacking up these choices. Furthermore, in a survey of 157 cross-lingual learning papers by Pikuliak et al. [18] they \nfound out that English is used in 149 of those papers, followed by German with 82 papers in total. There has also \nbeen some attempts in developing a more systematic transfer language selection method [19]. However, this method \nrequires training of a ranking model, limiting its use to the tasks and datasets used for training, making it unusable \noff-the-shelf for other applications. \nChoosing the optimal language for cross-lingual transfer remains widely an understudied problem. Usually, the \nsuitable source language candidate is decided experimentally or by pure intuition by the individual (researcher, or ML \npractitioner) based on their own theoretical knowledge and experience in the field. One option to select the source \nlanguage would be by taking a look at languages that are from the same language group as the target language [20]. \n  \n*Corresponding author \nb4 eronen. juuso@gmail .com (J. Eronen) \nORCID(s): 0000-0001-9841-3652 (J. Eronen) \n  \nEronen et al.: Preprint submitted to Elsevier \nPage \n1 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nHowever, this does not guarantee that that the linguistic features shared between the two languages would be similar \n[21]. \nIn order to contribute to the further understanding and solving this problem, we propose a method for choosing the \nsource language for cross-lingual transfer. We show that there is a correlation between linguistic similarity and model \nperformance, allowing us to select the best transfer language by comparing the source and target languages using \ndifferent linguistic similarity measures. We also show that multilingual transformer models can be used to obtain good \nperformance on the target language in a zero-shot learning setting. To select the optimal source language for transfer, \nwe propose to quantify the features of languages to compute a metric that can be used in comparing the closeness of \nlanguages using their linguistic properties. \nThere are some existing metrics that use linguistic features in order to measure the linguistic distance between \nlanguages [22, 23, 24]. However, as these metrics simply take a handful or only a single linguistic feature into account, \nwe propose a new linguistic similarity metric, which contains almost two hundred different features, based on the World \nAtlas of Language Structures (WALS) [25]. This allows us to not simply rely only on a single or a handful of features, \nbut to have a more robust metric by better quantifying all aspects of the languages. \nIn this research we concentrated on three different Natural Language Processing tasks, namely, sentiment analysis, \nnamed entity recognition and dependency parsing. We used datasets from eight different languages, namely English, \nGerman, Danish, Polish, Croatian, Russian, Japanese and Korean. The languages were chosen as they have relatively \nhigh quality datasets available. Also, the languages represent different language families (English, German, Danish - \nGermanic; Polish, Russian, Croatian - Slavic; Japanese, Korean - Koreano-Japonic language family). This also gives \nus the opportunity to study the efficacy of cross-lingual transfer learning between and within language family groups. \nIn previous research [26] we showed that cross-lingual transfer performance correlates with the linguistic similarity \nof the prediction target language and the source language used for fine-tuning the models. Our hypothesis is that this is \ntrue for also other NLP tasks. In the experiments, we used multilingual transformer models, namely Multilingual BERT \nand XLM-RoBERTa, which were fine-tuned by using each of the languages as source and target. We calculated the \nlinguistic similarity between all of our proposed languages using four different linguistic similarity metrics, EzGlot, \neLinguistics, a quantified model based on WALS and averaged lang2vec. To demonstrate the effectiveness of our \nmethod, we then measured the correlation between the zero-shot cross-lingual transfer performance and linguistic \nsimilarity. \nThe paper outline is as follows. In Section 2 we go through all areas of previous research that are addressed in \nthis paper. In Section 3 we describe all the tasks and datasets applied to this research and present their differences and \nfeatures. In Section 4 we describe the applied multilingual transformer models and the linguistic similarity metrics used \nin this research. In Section 5 we describe our experiment workflow and go through all the results from the conducted \nexperiments. In Section 6 we discuss the results in general and bring out the most interesting findings in relation to the \nresearch goals. \n1.1. Contributions of This Study \nThe goal of this research is to develop a method for cross-lingual transfer language selection. Most often, the choice \nof a transfer source is made purely by relying on the practitioner’s own judgement, using their accumulated experience \non the field and theoretical knowledge or simply choosing a language from the same language family as the target \n[20]. The current methods have many problems as they are prone to bias from the practitioner and also completely \nunoptimized. In fact, one could even say that there is no systematic method usable off-the-shelf that could be used to \ndetermine, which languages should be considered as the cross-lingual transfer source. \nWe propose to investigate the possibility that different linguistic similarity metrics could be utilized when trying to \nfind possible source language candidates for cross-lingual transfer also for other tasks than abusive language detection. \nWe hypothesize that linguistic similarity correlates with cross-lingual transfer efficacy, meaning that by using more \nsimilar languages, we would be able to achieve higher model performance. \nThis research is was conducted in order to confirm the findings of our previous research [26] also with other Natural \nLanguage Processing tasks. We improved the calculation process of the linguistic similarity metric quantified from the \nWorld Atlas of Language Structures. This was done by selecting all of the features that would have a defined value for \nboth languages in all possible language pairs instead of having to be shared between all of the languages, increasing \nthe robustness of the metric. Also, we applied a linguistic similarity metric based on lang2vec by Littell et al. [27]. \nThe main contributions of this work are as follows: \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 2 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \ne We confirm the transfer language selection method based on linguistic similarity with multiple NLP tasks. \ne We demonstrate the efficacy of two multidomain linguistic similarity metrics: improved quantified WALS and \naveraged lang2vec. \ne We show that there is a significant difference in choosing an optimal transfer source language over English. \nIn practice, we propose to fine-tune cross-lingual pretrained transformer models, specifically mBERT and XLM-R, \non three different Natural Language Processing tasks (sentiment analysis, named entity recognition and dependency \nparsing) using each of our proposed languages (English, German, Danish, Polish, Russian, Japanese, Korean) and then \nperform zero-shot prediction on the rest of the languages of the proposed set. We calculated the linguistic similarity \nbetween all of our proposed languages using four different linguistic similarity metrics, EzGlot, eLinguistics, quantified \nWorld Atlas of Language Structures and an averaged lang2vec proximity vectors. We then calculated the correlation \nbetween the zero-shot cross-lingual transfer performance and linguistic similarity to show the effectiveness of our \nmethod. A block-diagram of the system is shown in Figure 1. \nSource language \nTarget language \ndata \nLinguistic \ndata \n| \nsimilarity \ng — \nee \nOutput \nFine-tuning \n= Fine-tuned model \nPrediction \n  \n  \n  \n  \n  \nPre-trained \nmultilingual \ntransformer model \nFigure 1: Block diagram of the proposed system \n2. Previous Research \n2.1. Measuring Linguistic Similarity \nAlready in 2006, the relation between the difficulty of language learning and the similarity of languages in general \nwas discussed in a book by Ringbom [28]. The Finnish language scene was presented as an example in order to \ndemonstrate the importance of cross-linguistic similarity in foreign language learning [29]. In short, he showed that \nFinnish-speaking Finns have a harder time learning English than Swedish-speaking Finns. The reason behind this being \nthe closer relation between Swedish and English languages, giving an advantage to Swedish speakers when it comes \nto transferring the existing linguistic knowledge. \nCottorell et al. [30] showed that not every language is equally difficult to model. It was also shown by them that there \nis a correlation between the morphological richness of a language and the performance of the model. This means that \nthe more complex the language is, the more difficult it becomes to model. This is hinting that more simple languages \nmight not work so well when used as cross-lingual transfer sources for languages of higher complexity. This also implies \nthat the direct relatedness (for example, language family) of languages should not be the only criteria in deciding the \ncross-lingual transfer source language as other features of the languages should also be thoroughly considered in order \nto find the most optimal transfer language. \nThere has been some research in attempting to quantify a linguistic similarity metric from different linguistic \nfeatures. However, these metrics mostly commonly rely only on one or just a few different linguistic features. For \nexample, by comparing the consonants contained in a predefined set of words while taking into account the order \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 3 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nin which these consonants appear in the words, one can calculate a genetic proximity score between two languages. \nThis is implemented as the eLinguistics [23] similarity metric. The metric makes it possible to get information about \nthe direct relatedness of the compared languages. However, once the used languages start to become more and more \ndistant, accidental similarities in consonants are introduced and there is a significant increase in the error rate. This is \nalso acknowledged by the authors. Even though the metric is easy to calculate, it completely ignores all other kinds of \nlinguistic features, for example, semantic, syntactic, or morphological. \nAnother method to calculate a similarity metric is to take a look at the vocabularies of two languages and concentrate \non their similarity. EzGlot [24] uses lexical similarity as its basis for computation. The metric uses lexical similarity \nbetween the two compared languages while at the same time taking into account the amount of words the two languages \nare sharing with other languages. This allows for the calculation of similarity between the two languages in relation to \nthe similarity with every other language. \nAggarwal et al. [22] proposed a linguistic similarity metric that utilizes multiple aspects of languages. Their metric, \ncalled STL, is based on Semantic, Terminological (lexical) and Linguistic (syntactic) similarity of languages. The \nmethod outperformed previous similarity metrics that concentrated only on one of the previously mentioned aspects \n(31, 32]. They noticed that the terminological measures showed a much higher contribution when compared to the \nother two features. However, in order to use the metric, the structure of the used vocabulary dataset needs to be in the \nform of a complex ontology. Due to this fact and because of the dataset only consisting of German, French, Italian, \nDutch, Spanish and English, and due to the dataset used by the authors being no longer available, it was not feasible \nto use the metric as a part of this research. \nThe lang2vec developed by Littell et al. [27] is a database that represents languages as typological, phylogenetic, \nand geographical vectors, which are derived from a number of different linguistic resources, for example, WALS \n[25], PHOIBLE [33], Ethnologue [1], and Glottolog [34]. Each of these utilize multiple different features, making \nthem more robust than the EzGlot or eLinguistics metrics. The lang2vec is a fully-fledged library that can be used to \nquery for different linguistic features and to get pre-computed distances between languages, based on some typological \ninformation. \nThe World Atlas of Language Structures (WALS) project [25] consists of a database that catalogs phonological, \nword semantic and grammatical knowledge for 2,662 languages with almost two hundred different linguistic features \nfrom multiple domains. Using a linguistic similarity measure quantified from the WALS database into would allow \na more robust method to measure similarity and would aid capturing all aspects of the languages instead of relying \nonly on a single or a handful of linguistic features. Concentrating purely on using WALS to create a similarity metric \nwould also preserve homogeneity and allow a more explainable and controllable implementation. In previous research \n[26] we proposed a novel linguistic similarity metric quantified from the WALS database. This metric proved to be \nmore robust compared to the other metrics, at least for the applied abusive language detection task, as it was based on \nmultiple kinds of linguistic features. \n2.2. Transfer Language Selection \nSelecting the optimal language for cross-lingual transfer remains mostly an unanswered question. Most of the time, \nthe decision of which language to use as the transfer source comes up to the practitioner’s consideration. This is usually \ndone experimentally or by intuition [20, 35, 16] or by simply relying on English [36, 37, 38]. For example, in order to \nget a more successful transfer, Cottorell and Heigold [20] focused on using languages belonging to the same language \nfamily as the cross-lingual transfer target. However, even though the languages are part of the same language family, \ntwo languages could be very distant for example when looking at the complexity of grammar, which means that it does \nnot guarantee them sharing the same linguistic features [21]. \nA common way for choosing the transfer language is to simply default to English. The reason being that it is the \nde-facto highest resource language available for most NLP tasks [4]. This is also the case with popular multilingual \nbenchmarks like XTREME [12] and XGLUE [17]. Although, recently benchmarks like XTREME-R [39] have started \nto include cross-lingual training sets. Furthermore, in a survey of 157 cross-lingual learning papers by Pikuliak et al. \n[18] they found out that English was used in 149 papers, followed by German with 82 papers. Additionally, it has been \nshown that other languages than English, for example, German and Russian tend to work better as transfer sources \n[40]. \nDuong et al. [41] found out that choosing the transfer language based on language family is not optimal for many \nlanguages. For example, their experiments showed that the best source language for both Finnish and German is \nCzech, even though being from a different language family than the targets. They concluded that apparently, the best \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 4 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nsource language for cross-lingual transfer is not predictable from language family information. Instead, they proposed \ntwo methods for transfer language selection. The first being based on the Jensen-Shannon divergence between the \ndistributions of parts-of-speech n-grams on a pair of languages. The second method was based on the word-order \ninformation feature in WALS. Both of these methods showed improvements over choosing English or a language from \nthe same family as the target. They also experimented with using multiple source languages, which further improved \nthe performance. \nIt has been shown [42, 43, 44] that transferring from many high-resource languages at the same time can yield \nhigher results compared to selecting only a single language as the transfer source. However, these methods do not \nconsider the actual relation between the source and the target languages and the amount of contribution of each of the \nlanguages to the total score. Also, Nooralahzadeh et al. [45] discovered that certain morphosyntactic features shared \nbetween languages tend to give a boost to cross-lingual transfer performance. \nLin et al. [19] developed a ranking method for possible transfer language candidates using the lang2vec metrics \n[27] together with dataset dependent features like word overlap and type-token ratio. they discovered that using both \nthe dataset independent linguistic features and database dependent features to train the ranking model yields the best \nresults. However, as their method requires training of the ranking model, it is dependent on the tasks and datasets used \nfor training and is not usable out of the box for other applications. \nIn another study [38] it was shown that the transfer performance with English as the source correlates with the \nlinguistic similarity metrics of lang2vec [27], meaning that target languages more similar to English yielded higher \nscores. They found out that similarity of syntactic structures especially play an important role in selecting the source \nlanguage for tasks like parts-of-speech tagging (POS), named entity recognition (NER) and dependency parsing (DEP). \nThey also discovered that the fine-tuning corpus size of the target language also makes a difference considering the \ncross-lingual transfer performance, especially for higher level tasks like question answering. However, their research \nconcentrated only on using English as the source language and the capabilities of other languages as the transfer source \nwere left completely unexplored. \nMartinez et al. [46] found out that differences in language morphology in cross-lingual transfer generally lead to \na higher loss than when transferring between languages with the same morphological typology. Furthermore, they \nshowed that parts-of-speech tagging tends to be more sensitive towards changes in morphological typology compared \nto sentiment analysis, which seems to be more sensitive to variables related to the fine-tuning data and the transfer \nperformance being generally harder to predict. \nIn their research, Gaikwad et al. [10] discovered that there could be a relation between cross-lingual transfer \nperformance and language similarity. They classified entries in the Marathi language using multiple languages, \nspecifically Bengali, Greek, English, Turkish and Hindi as cross-lingual transfer sources. Their results showed that \nthe closest language of these to Marathi, Hindi, also had the highest performance. This hints that a solution to the \nproblem of cross-lingual transfer language selection could be found with the aid of linguistic similarity. \nIn our previous research [26], we showed that there is a correlation between language similarity metrics and cross- \nlingual transfer efficiency, at least for offensive language identification. This allows for choosing of an ideal transfer \nlanguage by using different metrics to compare the similarity languages without having to rely on one’s intuition. We \nalso showed that choosing a transfer language, for example, only by looking at the language family is not always the \nbest option. \n3. Tasks \nIn this research, we concentrate on three different NLP tasks. Sentiment analysis as a document classification \ntask. Named entity recognition as a token classification task. And lastly, dependency parsing for understanding the \nimportance of syntax and grammar in cross-lingual transfer. \nWe hypothesize that the zero-shot cross-lingual transfer performance correlates with the linguistic similarity of the \nsource and target languages. In order to confirm our hypothesis, we used datasets from eight different languages, namely \nEnglish, German, Danish, Polish, Russian, Croatian, Japanese and Korean for all of the tasks. We chose these languages \nas they had high quality datasets compared to other options and because the languages represent three different language \nfamilies (English, German, Danish - Germanic; Polish, Russian, Croatian - Slavic; Japanese, Korean - Koreano-Japonic \nlanguage family). This also gives us the opportunity to study the efficacy of cross-lingual transfer learning between \nand within different language family groups. \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 5 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \n3.1. Sentiment Analysis \nIn the field of NLP, sentiment analysis is one of the most active research areas [47]. The recent research in sentiment \nanalysis, as with many other NLP tasks, has mainly focused on using deep neural networks and pretrained language \nmodels [48, 49, 50, 51, 52]. The popularization of multilingual transformer models has made it possible to utilize \ncross-lingual transfer in order to train models for low-resource languages. \nRasooli et al. [53] used a set of 16 languages from different language families, namely Indo-European, Turkic, \nAfro-Asiatic, Uralic, and Sino-Tibetan, to learn a sentiment analysis model. Their experiments showed that for most \ntarget languages the best result can be obtained by leveraging from multiple source languages at the same time. Also, \ndatasets of a similar genre and domain tended to yield higher results when compared to out-of-domain and dissimilar \ngenres. \nPelicon et al. [54] used zero-shot cross-lingual transfer to classify Croatian news articles with an mBERT model \nfine-tuned using Slovene data with good results. In addition, Kumar et al. [55] used XLM-R and performed cross- \nlingual transfer from English to Hindi. Their model compared favorably to the used benchmarks and gives an effective \nsolution to the analysis of sentiments in a resource-poor scenario. \nThe majority of the sentiment analysis datasets used in this research consists of product reviews, as we attempted \nto keep the domain the same throughout the languages. However, for some languages, we were unable to find such \ndata, most notably Croatian, which consists of news articles. We also had to adjust the labels of some of the datasets so \nthat they would match among all of the languages. Training and evaluation splits were retained from original datasets \nif possible, otherwise datasets were split to 80% training and 20% evaluation. \nFor this research we used the Multilingual Amazon Reviews Corpus [56], which covers English, Japanese and \nGerman. The dataset contains over 200,000 reviews for each language collected between 2015 and 2019. The reviews \nare labeled from one to five stars. However, as the other datasets used in this research used a two-point scale (positive, \nnegative), we adjusted the labels accordingly (positive: 5 and 4 stars, negative: 2 and \n1 stars). \nFor Danish, we used a dataset containing almost 45,000 reviews crawled from Trustpilot by Alessandro Gianfelici! . \nFor Polish, we used the PolEmo 2.0 corpus [57]. This dataset contains over 8,000 reviews from the domains of medicine, \nhotels, products and school. For both of these datasets, we also had to adjust the labels of this dataset to a two-point \nscale similarly to the Amazon Reviews dataset. \nThe Russian dataset used in this research was a product review dataset by Smetanin et al. [58]. The dataset consists \nof 90,000 automatically labeled reviews on the topic \"Women’s Clothes and Accessories\", split evenly among three \nclasses (positive, neutral, negative). The Croatian dataset is the same used by Pelicon et al. [54], containing around \n2,000 news articles. The articles were collected from 24sata, one of the leading Croatian media companies. The \nannotations were done by 6 people using a five-level Likert scale. The annotations were later adjusted to a three-point \nscale by the authors. For the purpose of our experiments, in case of both datasets, we left out the neutral reviews in \norder to binarize the labels. \nThe Korean dataset used in this research was Naver sentiment movie corpus v1.0*. The dataset consists of Naver \nMovie reviews, with 100,000 positive and negative samples. The reviews were originally rated from one to ten, but the \ncreators binarized the dataset prior to publishing. \n3.2. Named Entity Recognition \nThe research on Named Entity Recognition (NER) has also shifted towards using Deep Neural Networks and most \nrecently, pretrained transformer models [59, 60, 61]. Cross-lingual transfer has also been applied to NER in multiple \nresearch. Fritzler et al. [62] used a metric-learning method to at the time outperform a state-of-the-art recurrent neural \nnetwork method and showed to be capable in both few-shot and zero-shot settings. Moon. et al. [63] used multilingual \nBERT to fine-tune a NER model in multiple languages and showed it to be more effective than a model fine-tuned \nonly on a single language. This demonstrates that the model can leverage knowledge from other languages in order to \nimprove its performance on one. \nHvingelby et al. [64] presented a Danish NLP resource based on the Danish Universal Dependencies treebank and \nshowed that transferring from other Germanic languages, especially from English and Norwegian, to Danish can yield \ngood results when using mBERT. However, using other Germanic languages in addition to Danish did not give any \nbetter results compared to fine-tuning only with Danish in their case. \n  \n‘https ://github.com/AlessandroGianfelici/danish_reviews_dataset \n*nttps: //github.com/e9t/nsmc \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 6 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nEntity projection [65, 66] has been used to generate pseudo-labeled datasets for low-resource NER datasets with \nthe help of parallel corpora. However, it has been shown by Weber and Steedman [67] that entity projection can \nbe outperformed by cross-lingual transfer and XLM-RoBERTa. The reason behind this could be explained by the \ndiscovery by Lauscher et al. [38], who showed that transfer performance with English as the source correlates with the \nsimilarity of the languages when dealing with a NER task. \nIn this study, we used the WikiANN [68] multilingual NER dataset also used by XTREME benchmark [12] for all \nof the proposed languages. WikiANN consists of Wikipedia articles annotated with LOC (location), PER (person), and \nORG (organisation) NER tags. We used the version by Rahimi et al. [69], which has a balanced train, development, \nand test splits and supports 176 of the 282 languages from the original WikiANN corpus. \n3.3. Dependency Parsing \nCross-lingual transfer in dependency parsing (DEP) has been studied for some time before the advent of \nmultilingual transformer models [70, 71, 72, 73]. These studies mainly used deep neural network-based methods on \nparallel corpora. The research by Duong et al. [41] discussed earlier in Section 2.2 was also conducted on a dependency \nparsing task. Instead of using parallel corpora, their research was built around syntactic cross-lingual word embeddings \n[74] trained over POS contexts to emphasize syntax. \nMultilingual transformer models have also seen success in the dependency parsing task [15, 75, 76]. Most notably, \nin their study, Lauscher et al. [38] discovered that structural and syntactic similarities between languages are the most \ndetermining factor when it comes to the success of cross-lingual transfer for lower-level tasks like POS-tagging and \nDEP. \nThe dataset used for all of the proposed languages in this study was the Universal Dependencies v2 [77], a widely \nused resource in NLP as well as in linguistic research. The dataset was also used in the XTREME [12] benchmark \nand in the research by Lauscher et al. [38] described earlier. Universal Dependencies is a framework for a consistent \nannotation of grammar, including parts-of-speech, morphological features, and syntactic dependencies across a total \nof more than 100 languages. \n4. Methods \n4.1. Models \nFor the experiments we used two pre-trained multilingual transformer models. The experiments were carried out \nin a zero-shot cross-lingual setting [78], meaning that the fine-tuning is done using only data from another language \nthan the target language. \nMultilingual BERT (mBERT) [13] is the multilingual version of BERT, which stands for Bidirectional Encoder \nRepresentations from Transformers. It is based on an attention mechanism called the Transformer [79] that learns \ncontextual relations between words (or sub-words) in text. One of the features transformer models introduced is the \ncapability to read text input in both directions at once, instead of being able to only read it sequentially from left-to- \nright or right-to-left. Taking advantage of this bidirectional capability, BERT is pre-trained on two NLP tasks, Masked \nLanguage Modeling and Next Sentence Prediction. The objective of Masked Language Modeling is to mask a word in \na sentence and have the algorithm predict based on the word’s context what word has been hidden. In Next Sentence \nPrediction, the algorithm takes two masked sentences and needs to predict if they have a sequential connection or not. \nAlthough mBERT has not been trained using any cross-lingual data, it has showed cross-lingual capabilities and had \ngood results in many cross-lingual tasks [80]. This also includes various zero-shot transfer tasks. Multilingual BERT \nhas even been shown to outperform the usage of various cross-lingual embeddings [15]. This ability to generalize \ncould come from having word pieces used in all languages, for example, numbers, URLs, etc, mapped to a shared \nspace. This in turn forces the co-occurring pieces to also be mapped to a shared space, thus spreading the effect to \nother word pieces, until different languages are close in a shared space [11]. \nXLM-RoBERTa (XLM-R) [14] is a multi-lingual transformer model, also trained with the Masked Language \nModel objective. XLM-R is trained on around a total of 2.5tb of CommonCrawl data in one hundred different languages. \nThe model is trained in the same way as the monolingual RoBERTa [81]. This means, that the only objective in its \npre-training is Masked Language Modeling. The model is not trained on the Next Sentence Prediction task like BERT \nor using the parallel Translation Language Model objective of XLM. \nXLM-R has been shown to outperform both mBERT and XLM on a many cross-lingual benchmarks, including \nzero-shot cross-lingual transfer tasks [82]. It has also been shown to perform well on low-resource languages. A \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 7 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nTable 1 \neLinguistics metric between all applied languages \n  \nDanish \nEnglish \nGerman \nCroatian \nPolish \nRussian \nJapanese \nKorean \n  \nDanish \n0.00 \n20.60 \n38.20 \n66.20 \n68.20 \n66.20 \n95.20 \n97.20 \nEnglish \n20.60 \n0.00 \n30.80 \n60.30 \n66.90 \n60.30 \n88.30 \n90.00 \nGerman \n38.20 \n30.80 \n0.00 \n64.50 \n68.10 \n64.50 \n87.40 \n95.50 \nCroatian \n66.20 \n60.30 \n64.50 \n0.00 \n10.70 \n5.60 \n90.70 \n87.20 \nPolish \n68.20 \n66.90 \n68.10 \n10.70 \n0.00 \n5.10 \n93.30 \n89.50 \nRussian \n66.20 \n60.30 \n64.50 \n5.60 \n5.10 \n0.00 \n93.30 \n89.50 \nJapanese \n95.20 \n88.30 \n87.40 \n90.70 \n93.30 \n93.30 \n0.00 \n88.00 \nKorean \n97.20 \n90.00 \n95.50 \n87.20 \n89.50 \n89.50 \n88.00 \n0.00 \n  \nnotable feature of XLM-R is that it can also match the performance of to state-of-the-art monolingual models, \nwhich demonstrates that it is possible to create multilingual models without sacrificing per-language performance \nin a monolingual setting [14], most likely thanks to the sheer amount of data used in the pre-training. \n4.2. Linguistic Similarity Metrics \nTo be able to calculate the correlation between cross-lingual zero-shot transfer performance and language similarity \nfor the proposed tasks, we needed \na way to quantify the aspects of all of the languages in our proposed set, \nspecifically, a language similarity metric. We utilized four language similarity measures, eLinguistics [23], EzGlot \n[24], the multidomain metric we quantified from the linguistic features presented in WALS [25] and averaged genetic, \ngeographic, syntactic, inventory, phonological and featural metrics from lang2vec [27]. We propose that linguistic \nsimilarity metrics could be utilized when trying to find optimal source language candidates for cross-lingual transfer. \nWe hypothesize that linguistic similarity correlates with cross-lingual transfer efficacy, meaning that by using more \nsimilar languages, we would be able to achieve higher model performance. \neLinguistics [23] works by calculating a genetic proximity value for a pair of languages based on the use of phonetic \nconsonants. The score is calculated by taking a predefined word set and comparing the consonants contained in these \nwords. The method also takes into account the order of the consonants. This way, it is possible to get information \nregarding the closeness of the phonetics of the pair of languages set for comparison. The assessment of the relationship \nof the consonants is based on the research done by Brown et al [83]. \nEven though completely disregarding semantic, morphological, and syntactic similarity and being very simple in \nformulation, the similarity values produced by the method seemed to be in line with our expectations and the two \nmultidomain metrics (WALS, lang2vec) used in this research. However, as the distance between the two compared \nlanguages increased, the method seemed to become increasingly more prone to errors. This is due to the surging \namount of accidental similarities in consonants. The similarity measure can be accessed from a web service*. The \nsimilarity values between our proposed languages are shown in Table 1. \nEzGlot [24] \nis based on the similarity of vocabularies, or lexical similarity, of the two compared languages. \nEzGlot’s similarity metric is computed by taking the lexical similarity between the two compared languages, while \nin addition taking into account the number of words the pair of languages also have in common every other language. \nThis makes it possible to compute a similarity measure for a pair of languages in relation to their closeness with \nevery other language. Also, due to including the calculation of the number of words the languages share with all other \nlanguages, the similarity measure becomes asymmetric between every pair of languages. This also supports studies \nstating that mutual language intelligibility is being considered asymmetric as well [84, 21]. \nA pre-computed language similarity matrix and the formula for its computation can be found on the EzGlot \nsimilarity metric project’s web page*. However, the usability of the metric is hindered by the high amount of missing \nvalues in the similarity matrix. For example taking a look at Japanese, which is one of the languages utilized in \nour experiments, over half of the values are missing for our proposed languages. Also, the authors of the similarity \nmeasure do not give away their data source. This means that we are unable to say anything regarding the quality of the \ncomputations. This also makes it more difficult to fill in the missing values to the similarity matrix. We extracted the \n  \n3nttp ://www.elinguistics.net/Compare_Languages. aspx \n‘https: //www.ezglot.com/most-similar-languages .php \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 8 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nTable 2 \nEzGlot metric between all of the proposed languages \n  \nDanish \nEnglish \nGerman \nCroatian \nPolish \nRussian \nJapanese \nKorean \n  \nDanish \n100 \n9 \n17 \nN/A \n13 \nN/A \nN/A \n9 \nEnglish \n6 \n100 \n28 \n6 \n19 \n14 \n7 \n26 \nGerman \n6 \n15 \n100 \n4 \n8 \n4 \nN/A \n5 \nCroatian \nN/A \n4 \n5 \n100 \n14 \n9 \nN/A \n5 \nPolish \n6 \n12 \n9 \n14 \n100 \n15 \nN/A \n5 \nRussian \nN/A \n11 \n7 \n11 \n19 \n100 \nN/A \n11 \nJapanese \nN/A \n2 \nN/A \nN/A \nN/A \nN/A \n100 \n8 \nKorean \n1 \n5 \n2 \n1 \n1 \n3 \n4 \n100 \n  \nTable 3 \nAveraged lang2vec metric between all of the proposed languages \n  \nDanish \nEnglish \nGerman \nCroatian \nPolish \nRussian \nJapanese \nKorean \n  \nDanish \n0.000 \n0.511 \n0.487 \n0.550 \n0.565 \n0.597 \n0.694 \n0.691 \nEnglish \n0.511 \n0.000 \n0.352 \n0.578 \n0.486 \n0.488 \n0.635 \n0.578 \nGerman \n0.487 \n0.352 \n0.000 \n0.550 \n0.470 \n0.471 \n0.594 \n0.579 \nCroatian \n0.550 \n0.578 \n0.550 \n0.000 \n0.513 \n0.505 \n0.709 \n0.699 \nPolish \n0.565 \n0.486 \n0.470 \n0.513 \n~=0.000 \n0.344 \n0.624 \n0.619 \nRussian \n0.597 \n0.488 \n0.471 \n0.505 \n0.344 \n0.000 \n0.589 \n0.585 \nJapanese \n0.694 \n0.635 \n0.594 \n0.709 \n0.624 \n0.589 \n0.000 \n0.518 \nKorean \n0.691 \n0.578 \n0.579 \n0.699 \n0.619 \n0.585 \n0.518 \n0.000 \n  \nsimilarity values from the EzGlot’s similarity matrix for the proposed languages. These values are presented in Table \n2. \nAveraged lang2vec is calculated from genetic, geographic, syntactic, inventory, phonological and featural metrics \nof lang2vec. Lang2vec [27] is a database that provides vector identifications of languages based on different linguistic \nfeatures based on various linguistic resources like WALS [25], PHOIBLE [33], Ethnologue [1], and Glottolog [34]. \nThe lang2vec is a fully-fledged library that can be used to query for different linguistic features and to get pre-computed \ngenetic, geographic, syntactic, inventory, phonological and featural distances between languages. In order to use \nlang2vec as a multidomain linguistic similarity metric, we used an average value of these six categories. \nThe method is based on multiple types of linguistic features, making it naturally more robust than EzGlot or \neLinguistics similarity metrics, which only rely on a single kind of linguistic feature each. The method also uses a \nlarger amount of data compared to the previously described metric based on WALS. Additionally, lang2vec deals \nwith the missing values in linguistic resources by predicting them [85]. However, due to being based on multiple \nsources, the heterogeneous nature of the method brings up many questions. For example, there might be incoherence \nas we do not know how features are selected from different sources and how they are weighted. Also, the features are \none-hot encoded which causes a complete loss of ordinality between feature values. Additionally, using geographical \ninformation as one of the vectors seems questionable as it was shown to be unreliable when predicting similarity of \nlanguages [86]. The averaged distance matrix for lang2vec is shown in Table 3. \nQuantified World Atlas of Language Structures is a similarity metric developed by us in previous research \n[26]. It is based on The World Atlas of Language Structures (WALS) [25], which is a massive language database \nthat records phonological, word semantic and grammatical information for a total of 2,662 languages from over 200 \ndifferent language families. There are 192 different linguistic features in the database currently (May 2022). However, \nmany of the linguistic features are missing for of the available languages. For example, one of the most extensively \ndocumented language, English, has about 150 features documented in the database. This amount rapidly decreases for \nlanguages studied less. Taking Danish as an example, it only 58 features documented>. Considering every language \nand all of the features, this adds up to over 58,000 data points in total in the WALS database. This means the whole \n  \n>Some even less studied languages have an even smaller number of features documented, e.g. Chuj language, spoken in Guatemala, has only \n29, while the Indonesian Kutai language has only a single feature documented. \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 9 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nTable 4 \nQuantified WALS metric between all of the proposed languages \n  \nDanish \nEnglish \nGerman \nCroatian \nPolish \nRussian \nJapanese \nKorean \n  \nDanish \n0.000 \n0.109 \n0.140 \n0.167 \n~=0.197 \n0.155 \n0.236 \n0.202 \nEnglish \n0.109 \n0.000 \n0.136 \n0.179 \n0.164 \n0.141 \n0.252 \n0.209 \nGerman \n0.140 \n0.136 \n0.000 \n0.221 \n0.196 \n0.140 \n0.248 \n0.225 \nCroatian \n0.167 \n0.179 \n0.221 \n0.000 \n0.160 \n0.080 \n0.272 \n0.229 \nPolish \n0.197 \n0.164 \n0.196 \n0.160 \n0.000 \n0.097 \n0.249 \n0.210 \nRussian \n0.155 \n0.141 \n0.140 \n0.080 \n0.097 \n0.000 \n0.225 \n0.196 \nJapanese \n0.236 \n0.252 \n0.248 \n0.272 \n0.249 \n0.225 \n0.000 \n0.108 \nKorean \n0.202 \n0.209 \n0.225 \n0.229 \n0.210 \n0.196 \n0.108 \n0.000 \n  \ndatabase is only approximately 12% populated, meaning a vast majority of the information is missing. Also many \nmajor and widely studied languages are missing many features. For example, 25% of all of the features are missing for \nEnglish. These missing values and the sparsity of the data is the main point of concern when quantifying the WALS \ndatabase into a linguistic similarity metric as using lesser known and not so widely studied languages means having \nless common features among them. \nIn previous research, we quantified a novel linguistic similarity metric from the WALS database based on the \nfeatures all of the proposed languages shared. One of the problems of the metric was that as the amount of languages \nincreased, the amount of features shared with them decreased due to missing values in the database. This time, we \nimproved the calculation process and increased the robustness of the metric. The improved version attempts to counter \nthe issue caused by the diminishing feature count. This was done by selecting all of the features that would have \na defined value for both languages in all possible language pairs instead of having to be shared between all of the \nlanguages. The language pairs were formed from our proposed languages (English, German, Danish, Polish, Russian, \nCroatian, Japanese and Korean). Otherwise, the process remained the same. In short, we converted the possible \nfeature values into numeric and calculated an average euclidean distance between all language pairs. This resulted \nin a symmetric distance metric. The goal was to create a multidomain similarity metric that would also be coherent \nand try to preserve the ordinality of the feature values. The finished distance matrix is shown in Table 4. \nLastly, our plan was to take a look at the STL similarity measure [22], which is based on multiple linguistic \nfeatures. The measure puts together three different aspects of language by using Semantic, Terminological (lexical) \nand Linguistic (syntactic) similarity to form a single metric. According to the authors, the STL metric outperformed \nmany previous measures that were relying only on one of the previously mentioned feature types [31, 32]. However, in \norder to be able to use the metric, the vocabulary dataset must be structured in the form of an ontology, which restricts \nthe metric’s use. Due to this fact and because of the lack of available languages for the used dataset, it was not possible \nfor us to utilize the metric in this research. \n5. Experiments \n5.1. Setup \nWe fine-tuned both of the models (mBERT, XLM-R) with all of the proposed languages (English, German, Danish, \nPolish, Russian, Croatian, Japanese and Korean) for all of the tasks. Fine-tuning refers to the training of the parameters \nof a pre-trained language model (like BERT) with task-specific labeled data. This produced 16 fine-tuned models for \neach task, which sums up to a total of 48 fine-tuned models (two transformer models, eight languages, three tasks). \nAfter fine-tuning, we evaluated the models with test datasets from all of the previously mentioned languages to compute \nthe cross-lingual zero-shot transfer scores. We did not use a train-dev-test, but only train-test scenario for evaluation, \nbecause the test dataset has nothing to do with the training dataset in a zero-shot task. We also do not aim at optimizing \nfor each dataset, or creating a product, but rather study general properties. We evaluated the models with a macro \nFl-score for sentiment analysis and NER, and Label Attachment Score (LAS) for the dependency parsing task. \nAfter finishing the evaluations for all of the fine-tuned models, we took a look at the correlation between the \nzero-shot cross-lingual transfer scores and linguistic similarity. This was done by using the four previously introduced \nlinguistic similarity metrics (eLinguistics, EzGlot, WALS and lang2vec). We computed Pearson’s and Spearman’s \ncorrelations between the models’ cross-lingual zero-shot transfer scores and the language similarity measures. The \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 10 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \n  \n  \nTable 5 \nTasks, models and linguistic similarity metrics used in the experiments \nTasks \nModels \nLinguistic Similarity Metrics \nSentiment Analysis \nmBERT \nEzGlot \nNamed Entity Recognition \nXLM-RoBERTa _ eLinguistics \nDependency Parsing \nAveraged lang2vec \nQuantified WALS \n  \n  \n  \n  \n  \n  \nTable 6 \nSentiment analysis: F1-scores for mBERT \nTARGET \nDanish \nEnglish \nGerman \nCroatian \nPolish \nRussian \nJapanese \n(Korean \nDanish \n0.976 \n0.875 \n0.951 \n0.941 \n0.934 \n0.876 \n0.800 \n0.881 \nEnglish \n0.942 \n0.935 \n0.935 \n0.921 \n0.921 \n0.849 \n0.645 \n0.838 \nGerman \n0.901 \n0.816 \n0.971 \n0.908 \n0.889 \n0.828 \n0.711 \n0.741 \nSOURCE \nCroatian \n0.952 \n0.883 \n0.948 \n0.973 \n0.940 \n0.863 \n0.802 \n0.881 \nPolish \n0.952 \n0.876 \n0.948 \n0.948 \n0.967 \n0.861 \n0.771 \n0.878 \nRussian \n0.949 \n0.862 \n0.939 \n0.938 \n0.933 \n0.957 \n0.774 \n0.867 \nJapanese \n0.908 \n0.799 \n0.903 \n0.894 \n0.870 \n0.807 \n0.914 \n0.869 \nKorean \n0.940 \n0.848 \n0.935 \n0.930 \n0.909 \n0.850 \n0.815 \n0.957 \nTable 7 \nSentiment analysis: F1-scores for XLM-R \nTARGET \nDanish \nEnglish \nGerman \nCroatian \nPolish \nRussian \nJapanese \nKorean \nDanish \n0.972 \n0.857 \n0.932 \n0.934 \n0.926 \n0.869 \n0.749 \n0.846 \nEnglish \n0.939 \n0.925 \n0.920 \n0.922 \n0.916 \n0.844 \n0.705 \n0.816 \nGerman \n0.882 \n0.791 \n0.966 \n0.890 \n0.871 \n0.816 \n0.671 \n0.711 \nSOURCE \nCroatian \n0.945 \n0.859 \n0.925 \n0.969 \n0.929 \n0.876 \n0.763 \n0.835 \nPolish \n0.946 \n0.853 \n0.929 \n0.939 \n0.960 \n0.865 \n0.632 \n0.834 \nRussian \n0.918 \n0.792 \n0.895 \n0.913 \n0.901 \n0.953 \n0.726 \n0.832 \nJapanese \n0.888 \n0.793 \n0.880 \n0.871 \n0.851 \n0.773 \n0.905 \n0.832 \nKorean \n0.921 \n0.804 \n0.903 \n0.911 \n0.880 \n0.824 \n0.690 \n0.953 \n  \ntasks, models and linguistic similarity metrics used in the experiments are listed in Table 5. The models were fine-tuned \nby using PyTorch and the Huggingface Transformers library [87]. The hardware used was an Nvidia GTX 1080Ti GPU. \n5.2. Results \nBoth of the multilingual transformer models (mnBERT, XLM-R) were fine-tuned with all of the proposed languages \nfor each task (sentiment analysis, NER, DEP) we introduced earlier. The models were fine-tuned using only the training \ndataset from a single language before the evaluation step. The model evaluation scores are presented in Tables 6 and \n7 for sentiment analysis, Tables 8 and 9 for NER and Tables 10 and 11 for DEP. \nLooking at the results, we can clearly say that XLM-R outperformed mBERT in \nall of the tasks. The only exception \nto this was the sentiment analysis task, where mBERT slightly outperformed XLM-R. It can be noted from the results \nthat the highest transfer scores usually belong to the languages in the same language family as the source language \n(English, German, Danish - Germanic; Croatian, Polish, Russian - Slavic; Japanese, Korean - Koreano-Japonic). Also, \nmost of the time there is a clear difference in the scores when evaluating with the same language as the source compared \nto zero-shot cross-lingual transfer. The exceptions to this are the sentiment analysis task for both models and the NER \ntask for XLM-R. \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 11 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \n  \n  \n  \n  \n  \n  \n  \n  \nTable 8 \nNER: Fi-scores for mBERT \nTARGET \nDanish \nEnglish \nGerman \nCroatian \nPolish \nRussian \nJapanese \nKorean \nDanish \n0.957 \n0.813 \n0.801 \n0.480 \n0.763 \n0.791 \n0.675 \n0.640 \nEnglish \n0.778 \n0.930 \n0.827 \n0.744 \n0.852 \n0.729 \n0.773 \n0.652 \nGerman \n0.770 \n0.866 \n0.936 \n0.751 \n0.879 \n0.805 \n0.766 \n0.648 \nSOURCE \nCroatian \n0.667 \n0.710 \n0.748 \n0.876 \n0.727 \n0.770 \n0.707 \n0.622 \nPolish \n0.691 \n0.676 \n0.702 \n0.695 \n0.956 \n0.648 \n0.754 \n0.625 \nRussian \n0.759 \n0.825 \n0.764 \n0.761 \n0.867 \n0.946 \n0.759 \n0.564 \nJapanese \n0.754 \n0.827 \n0.761 \n0.659 \n0.693 \n0.743 \n0.926 \n0.673 \nKorean \n0.602 \n0.694 \n0.668 \n0.670 \n0.675 \n0.705 \n0.700 \n0.867 \nTable 9 \nNER: Fi-scores for XLM-R \nTARGET \nDanish \nEnglish \nGerman \nCroatian \nPolish \nRussian \nJapanese \n(Korean \nDanish \n0.975 \n0.868 \n0.876 \n0.723 \n0.958 \n0.910 \n0.873 \n0.755 \nEnglish \n0.955 \n0.941 \n0.941 \n0.820 \n0.961 \n0.934 \n0.920 \n0.781 \nGerman \n0.960 \n0.926 \n0.948 \n0.846 \n0.975 \n0.930 \n0.918 \n0.765 \nSOURCE = _ Croatian \n0.920 \n0.842 \n0.872 \n0.911 \n0.919 \n0.889 \n0.834 \n0.723 \nPolish \n0.949 \n0.885 \n0.897 \n0.877 \n0.981 \n0.897 \n0.891 \n0.740 \nRussian \n0.923 \n0.908 \n0.915 \n0.611 \n0.951 \n0.951 \n0.880 \n0.737 \nJapanese \n0.955 \n0.909 \n0.920 \n0.847 \n0.961 \n0.926 \n0.936 \n0.789 \nKorean \n0.827 \n0.752 \n0.799 \n0.491 \n0.751 \n0.820 \n0.840 \n0.900 \nTable 10 \nDEP: LAS-scores for mBERT \nTARGET \nDanish \nEnglish \nGerman \nCroatian \nPolish \nRussian \nJapanese \nKorean \nDanish \n0.860 \n0.545 \n0.631 \n0.619 \n0.556 \n0.647 \n0.092 \n0.026 \nEnglish \n0.652 \n0.891 \n0.670 \n0.624 \n0.570 \n0.653 \n0.165 \n0.021 \nGerman \n0.635 \n0.603 \n0.842 \n0.672 \n0.613 \n0.733 \n0.130 \n0.062 \nSOURCE = Croatian \n0.581 \n0.607 \n0.633 \n0.893 \n0.645 \n0.778 \n0.124 \n0.030 \nPolish \n0.520 \n0.518 \n0.577 \n0.676 \n0.924 \n0.760 \n0.112 \n0.023 \nRussian \n0.594 \n0.604 \n0.643 \n0.730 \n0.666 \n0.878 \n0.131 \n0.020 \nJapanese \n0.132 \n0.148 \n0.163 \n0.114 \n0.117 \n0.126 \n0.926 \n0.033 \nKorean \n0.058 \n0.065 \n0.060 \n0.035 \n0.045 \n0.054 \n0.035 \n0.293 \n  \nIn dependency parsing, XLM-R slightly outperformed mBERT as expected. However, in the sentiment analysis \ntask mBERT scored slightly higher than XLM-R overall, with both models scoring high across all language pairs. \nSome language pairs even achieving zero-shot cross-lingual transfer F-score of over 0.95. In this task, there seems not \nto be a clear pattern what kind of language pairs tend to yield higher performance. For example, Slavic languages seem \nto work better as sources for Danish compared to German languages in the case of both models. The scores are also \nsimilarly high across the board for the NER task with XLM-R, with the model being able to achieve very high scores \nwith zero-shot transfer. The performance difference between mBERT and XLM-R is also more noticeable in the case \nof NER. \nAs can be seen from Table 12, both Japanese and Korean worked decently well as cross-lingual transfer sources \nfor both sentiment analysis and NER tasks, even though being very different from the other languages used in the \nexperiments as they are the only non Indo-European languages. However, in the case of DEP their performance \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 12 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \n  \n  \n  \n  \n  \n  \nTable 11 \nDEP: LAS-scores for XLM-R \nTARGET \nDanish \nEnglish \nGerman \nCroatian \nPolish \nRussian \nJapanese \nKorean \nDanish \n0.888 \n0.679 \n0.725 \n0.706 \n0.672 \n0.715 \n0.095 \n0.366 \nEnglish \n0.733 \n0.911 \n0.728 \n0.720 \n0.700 \n0.716 \n0.112 \n0.364 \nGerman \n0.712 \n0.681 \n0.854 \n0.751 \n0.732 \n0.784 \n0.066 \n0.405 \nSOURCE = Croatian \n0.639 \n0.668 \n0.702 \n0.910 \n0.798 \n0.818 \n0.069 \n0.375 \nPolish \n0.614 \n0.603 \n0.676 \n0.780 \n0.945 \n0.804 \n0.049 \n0.384 \nRussian \n0.642 \n0.645 \n0.722 \n0.801 \n0.796 \n0.890 \n0.101 \n0.378 \nJapanese \n0.118 \n0.132 \n0.172 \n0.098 \n0.122 \n0.106 \n0.937 \n0.317 \nKorean \n0.326 \n0.292 \n0.381 \n0.298 \n0.325 \n0.304 \n0.183 \n0.877 \nTable 12 \nAverage scores for each source language on each task \nmBERT \nXLM-R \nSentiment \nNER’ \nDEP __ \nSentiment \nNER \nDEP \nDanish \n0.904 \n0.740 \n0.497 \n0.886 \n0.867 \n0.606 \nEnglish \n0.873 \n0.786 \n0.531 \n0.874 \n0.907 \n0.623 \nGerman \n0.845 \n0.803 \n0.536 \n0.825 \n0.909 \n0.623 \nCroatian \n0.905 \n0.728 \n0.536 \n0.888 \n0.864 \n0.622 \nPolish \n0.900 \n0.719 \n0.514 \n0.870 \n0.890 \n0.607 \nRussian \n0.902 \n0.781 \n0.533 \n0.866 \n0.860 \n0.622 \nJapanese \n0.871 \n0.754 \n0.220 \n0.849 \n0.905 \n0.250 \nKorean \n0.898 \n0.698 \n0.081 \n0.861 \n0.773 \n0.373 \n  \nis extremely low. Except for this case with DEP, all of the proposed languages seem to be quite equal as cross- \nlingual transfer sources in general. Interestingly, German, Croatian and Russian seem to perform slightly better overall \ncompared to the other languages, especially with mBERT. A similar phenomenon was also experienced by Turc et al. \n[40] and in our previous research [26]. \n5.3. Effect of Linguistic Similarity \nWe calculated the correlation between the zero-shot cross-lingual transfer results of the two models and each of the \nfour proposed linguistic similarity metrics (EzGlot, eLinguistics, WALS and lang2vec) in all proposed NLP tasks using \nboth Pearson’s and Spearman’s correlation coefficients (p-value). We were forced to ignore some of the language pairs \nwhen calculating the correlations with the EzGlot metric as some of the similarity values were missing. The correlation \nanalysis results are shown in Table 13 for sentiment analysis, Table 14 for NER, Table 15 for DEP. \nLooking at the results, one can say that there is mostly a strong correlation between lang2vec, WALS and \neLinguistics metrics and the cross-lingual zero-shot transfer score, and a strong-moderate correlation between the \nEzGlot metric and the transfer scores for NER and DEP. In the case of sentiment analysis, the correlation is noticeably \nlower with XLM-R, staying at a moderate level with all of the linguistic similarity metrics. The correlation is strongest \nwith the dependency parsing task with XLM-R, with the highest absolute Spearman’s correlation being 0.897 with \neLinguistics metric. Also, the results show p-value < 0.05 for all of the tasks, models and metrics, indicating statistical \nsignificance. \nFor both of the models, the correlation for lang2vec, WALS and eLinguistics metrics are generally higher than \nEzGlot, except in the case of sentiment analysis, where EzGlot’s correlation is slightly higher than WALS and \neLinguistics using both Pearson’s and Spearman’s correlation coefficients. Also, the correlations were generally slightly \nstronger with mBERT in sentiment analysis, while XLM-R had higher correlations in both NER and DEP tasks. \nHowever, the results changed drastically for all tasks except dependency parsing, when we removed the anchor \npoints of same source-target language pairs (monolingual scenarios), leaving only the zero-shot transfer results. This \nwas necessary to do in order remove the bias brought by the monolingual data points, as the scores are higher and the \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 13 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \nTable 13 \nSentiment analysis: Pearson’s and Spearman's correlation coefficients for model F1 scores and linguistic similarity metrics \nPearson \nSpearman \nXLM-R \nmBERT \nXLM-R \nmBERT \np \np-value \np \np-value \np \np-value \np \np-value \nWALS \n-0.297 \n0.017 \n-0.645 \n0.000 = -0.331 \n0.008 \n-0.537 \n0.000 \nEzGlot \n0.389 \n0.005 \n0.729 \n0.000 \n0.533 \n0.000 \n0.586 \n0.000 \neLinguistics \n= -0.355 \n0.004 \n-0.648 \n0.000 \n=-0.413 \n0.001 \n-0.652 \n0.000 \nlang2vec \n-0.418 \n0.001 \n-0.746 \n0.000 \n-0.482 \n0.000 \n-0.623 \n0.000 \nTable 14 \nNER: Pearson's and Spearman's correlation coefficients for model F1 scores and linguistic similarity metrics \nPearson \nSpearman \nXLM-R \nmBERT \nXLM-R \nmBERT \np \np-value \np \np-value \np \np-value \np \np-value \nWALS \n-0.514 \n0.000 \n-0.500 \n0.000 \n= -0.510 \n0.000 \n-0.486 \n0.000 \nEzGlot \n0.494 \n0.000 \n0.427 \n0.002 \n0.464 \n0.001 \n0.401 \n0.004 \neLinguistics \n -0.580 \n0.000 \n-0.517 \n0.000 \n-0.608 \n0.000 \n-0.553 \n0.000 \nlang2vec \n-0.465 \n0.000 \n-0.432 \n0.000 \n= \n-0.504 \n0.000 \n-0.461 \n0.000 \nTable 15 \nDEP: Pearson’s and Spearman's correlation coefficients for model LAS scores and linguistic similarity metrics \nPearson \nSpearman \nXLM-R \nmBERT \nXLM-R \nmBERT \np \np-value \np \np-value \np \np-value \np \np-value \nWALS \n-0.781 \n0.000 \n-0.718 \n0.000 \n-0.844 \n0.000 \n-0.693 \n0.000 \nEzGlot \n0.588 \n0.000 \n0.516 \n0.000 \n0.694 \n0.000 \n0.561 \n0.000 \neLinguistics \n-0.845 \n0.000 \n-0.840 \n0.000 \n=-0.897 \n0.000 \n-0.867 \n0.000 \nlang2vec \n-0.702 \n0.000 \n-0.679 \n0.000 \n= -0.848 \n0.000 \n-0.754 \n0.000 \n  \nlanguages would also be the most similar (same). The results after removing the same source-target language pairs are \nshown in Table 16 for sentiment analysis, Table 17 for NER, Table 18 for DEP. \nFirst of all, for eLinguistics and WALS similarity metrics, the correlations generally dropped from strong to \nmoderate for the NER task while EzGlot’s correlation fell close to zero and also lost all statistical significance. The \ncorrelation of lang2vec also plummeted and mostly lost statistical significance, but remained higher than EzGlot’s. \nInterestingly, for sentiment analysis, the result looks completely opposite. The correlations of EzGlot and lang2vec \nonly fell slightly while WALS’ and eLinguistics’ correlation plummeted down and lost statistical significance for \nXLM-R. The correlations in the dependency parsing task only dropped slightly for all of the linguistic similarity \nmetrics. Also, after removing the same source-target language pairs, the strongest correlations are still found in DEP, \nfollowed by NER, while sentiment analysis still has the weakest correlations overall. However, as linguistic similarity \nstill correlates with cross-lingual transfer performance, we can get an improved model performance by using linguistic \nsimilarity for transfer language selection. \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 14 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nTable 16 \nSentiment analysis: Pearson’s and Spearman's correlation coefficients for model F1 scores and linguistic similarity metrics \nfor zero-shot only \n  \n  \n  \n  \nPearson \nSpearman \nXLM-R \nmBERT \nXLM-R \nmBERT \np \np-value \np \np-value \np \np-value \np \np-value \nWALS \n-0.111 \n0.415 \n-0.262 \n0.051 \n= -0.168 \n0.216 \n-0.315 \n0.018 \nEzGlot \n0.327 \n0.035 \n0.303 \n0.051 \n0.403 \n0.008 \n0.313 \n0.044 \neLinguistics \n -0.229 \n0.090 \n-0.392 \n0.003 \n-0.284 \n0.034 \n-0.487 \n0.000 \nlang2vec \n-0.380 \n0.004 \n-0.454 \n0.000 \n=-0.374 \n0.005 \n-0.440 \n0.001 \n  \nTable 17 \nNER: Pearson's and Spearman's correlation coefficients for model F1 scores and linguistic similarity metrics for zero-shot \nonly \n  \n  \n  \n  \nPearson \nSpearman \nXLM-R \nmBERT \nXLM-R \nmBERT \np \np-value \np \np-value \np \np-value \np \np-value \nWALS \n-0.336 \n0.011 \n-0.347 \n0.009 \n-0.316 \n0.017 \n-0.293 \n0.028 \nEzGlot \n0.173 \n0.274 \n0.120 \n0.448 \n0.170 \n0.282 \n0.095 \n0.549 \neLinguistics \n-0.453 \n0.000 \n-0.384 \n0.003 \n-0.458 \n0.000 \n-0.389 \n0.003 \nlang2vec \n-0.234 \n0.082 \n-0.214 \n0.113 \n-0.307 \n0.021 \n-0.256 \n0.057 \n  \nTable 18 \nDEP: Pearson's and Spearman's correlation coefficients for model LAS scores and linguistic similarity metrics for zero-shot \nonly \n  \n  \n  \n  \nPearson \nSpearman \nXLM-R \nmBERT \nXLM-R \nmBERT \np \np-value \np \np-value \np \np-value \np \np-value \nWALS \n-0.738 \n0.000 \n-0.661 \n0.000 \n-0.769 \n0.000 \n-0.588 \n0.000 \nEzGlot \n0.421 \n0.005 \n0.373 \n0.015 \n0.488 \n0.001 \n0.349 \n0.024 \neLinguistics \n-0.795 \n0.000 \n-0.822 \n0.000 \n-0.848 \n0.000 \n-0.842 \n0.000 \nlang2vec \n-0.714 \n0.000 \n-0.709 \n0.000 = -0.775 \n0.000 \n-0.676 \n0.000 \n  \n6. Discussion \n6.1. Transfer Language Performance \nXLM-R outperforming mBERT generally matches our expectations, as it also did so on a variety of benchmark \ntasks [12, 17]. The reason behind this most likely is the fact that XLM-R uses a vastly larger amount of data for \npretraining compared to mBERT. The performance difference between the two models is the most clear in the NER \ntask. \nAccording to the results, simply choosing English as the transfer source did not yield top results most of the time, \nsometimes even as a source language to other languages in the Germanic language group. For example, it had a lower \nthan average performance in sentiment analysis and an average performance in the other two tasks probably due to \nits simplicity when compared to both Danish and German. It was also slightly outperformed by Slavic languages in \nsome cases when used as a source for other Germanic languages. Another reason could be the influence of French \n[88, 89, 90], which might further distance it from the other Germanic languages. Also, the differences in morphology \ncould be a factor here. Danish and German probably work better with each other due to a great amount of historic \nmutual influence. \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 15 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nIn sentiment analysis, the models achieved slightly better scores with English and it was on-par with other Germanic \nlanguages. However, all of the Slavic languages still tended to work slightly better as transfer sources. These results \nshow that other languages should also be considered over English as the cross-lingual transfer source if available. For \nthe other two tasks, NER and DEP, English performed well and showed to be a good transfer source for the other two \nGermanic languages. \nIn most cases, using languages from the same language family as the source language yielded the highest cross- \nlingual transfer scores. This matches with the typical intuition-based selection process used to select source language for \ncross-lingual transfer. However, relying only on intuition and looking purely at language families when when selecting \nthe transfer language will lead to diminished results in some cases. \nOne example would be taking Polish as the target language for NER task. One could expect that in this case, the \nbest transfer languages would be Croatian and Russian, but looking at the results (Tables 8 and 9) German had a \nhigher cross-lingual transfer score even though it is from the Germanic language family, not Slavic. This could be, for \nexample, due to mutual influence of these two languages. The grammar of both Danish and English is relatively simple \ncompared to German, which could aid them in generalizing better with one another. Looking at the scores, it can be \nnoted that German is a good source for both Germanic and Slavic languages, which could mean that the historical \nmutual influence between the Germans and Slavs could be a factor here. Furthermore, German, in addition to having a \nhigher average performance on most tasks, tended to also work exceptionally well also as a source language for other \nSlavic languages, most likely because of the reasons discussed above. \nIn addition, Japanese and Korean did not achieve comparably better scores with one another, contrary to our \nexpectations, and were even slightly outperformed by the other languages approximately half of the time, even though \nbeing more similar with each other compared to any of the other proposed languages. Here the reason could be, for \nexample, the differences in the writing systems, as neither of these two languages use alphabets and their systems also \ngreatly differ from each other. \nAlso, both Russian and Croatian had a higher than average performance on most of the tasks. This was similar to \nour previous research [26] where Russian performed exceptionally well as a transfer source for offensive language \nidentification. However, unlike in our previous research, Russian did not perform noticeably well as the transfer \nlanguage source for Korean and Japanese. Thus the phenomenon experienced previously is most likely related to \nthe topic of offensive language identification itself or to the properties of these specific datasets. We will investigate \nthis in later research. Also, Japanese and Korean had a satisfying performance as source languages for the Germanic \nand Slavic languages in both sentiment analysis and NER tasks, even though Japanese and Korean are fundamentally \ndifferent from the languages of these two families, as they are the only non Indo-European languages in the proposed \nset. This demonstrates that multilingual transformer models are also able to leverage knowledge even from very distant \nlanguages. \n6.2. Analysis of Linguistic Similarity Metrics \nThe correlation between cross-lingual transfer performance and the similarity metrics were strong or moderate \nwith all of the proposed metrics, which would suggest that using even a single feature such as lexical information or \nby comparing phonetic consonants is still effective to some extent. \n6.2.1. EzGlot \nHowever, when considering only the zero-shot transfer results, EzGlot’s similarity metric’s correlation dropped \ndrastically and out of statistical significance in the NER task. This shows that it does not necessarily rely on lexical \nfeatures and that other linguistic features need to be considered when choosing the source language for NER. On \nthe other hand, the same happened with the lang2vec metric despite it being created using different features from \nmultiple domains. Also, the opposite happened in the sentiment analysis task, as both WALS and eLinguistics metrics’ \ncorrelation dropped drastically and out of statistical significance. This hints the importance of lexical similarity when \nchoosing the source language for sentiment analysis tasks. \n6.2.2. eLinguistics \nSurprisingly, even though using only a predefined set of phonetic consonants for its calculation, the correlation of \neLinguistics’ similarity metric was stronger in all tasks compared to the the correlation of the WALS metric, which we \nquantified from the WALS database using linguistic features from multiple domains. The correlation of eLinguistics \nwas also higher than the averaged lang2vec metric in both NER and DEP. The reason behind this could be that including \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 16 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nall of possible features between each language pair could have caused too many irrelevant features to be included. This \ncan cause a possible bias the metric calculation. \nHowever, the eLinguistics metric also has its weak points as it is based on only a single aspect of language, even \nthough its correlation being the strongest. One can see from Table \n1 that eLinguistics shows Japanese being very \ndistant from Korean, being at the same level as Polish and Russian, with Croatian being seemingly closer to Korean \nthan Japanese, which is not true due to the similarities in the vocabulary and grammar of Japanese and Korean. Taking \na look at Tables 4 and 3, it is clear that the WALS and lang2vec metrics are a lot more robust to this kind of errors. \nThe reason most likely is that instead of using only a single linguistic feature like the eLinguistics metric, the WALS \nand lang2vec metrics are based on a large amount of features spanning over multiple domains. \n6.2.3. Averaged lang2vec and Quantified WALS \nBoth lang2vec and WALS had a strong correlation with the DEP task. Although both metrics are based on a large \nnumber of linguistic features spanning over multiple domains, their correlations varied greatly with the other two \ntasks. Specifically, in NER, the correlation of lang2vec was noticeably lower. In sentiment analysis, the correlation of \nWALS plummeted while lang2vec stayed at a moderate level. The reason is probably in the calculation of the metrics. \nIn the quantified WALS metric, features are treated as continuous whereas lang2vec uses one-hot encoding. Also, in \nquantified WALS, every single feature has the same weight whereas in averaged lang2vec every category of features has \nthe same weight but might contain a different amount of features. Additionally, the features in lang2vec are collected \nfrom multiple sources, which increases the amount of data while possibly introducing incoherence. Lastly, the number \nof features used in the similarity calculations with the quantified WALS metric varies slightly between language pairs \ndue to missing values in the database. Lang2vec tries to counter this by using a model to predict the missing values, \nalthough this might introduce more errors in some cases. \nIn the future, we will take another glance at the WALS database, aiming for a better quantification by looking at \nthe importance of each feature group (syntactic, lexical, phonetic, etc.) and weighing accordingly while filtering out \nredundant features in order to develop an even more effective and comprehensive similarity metric. We will also take a \nlook at lang2vec, aiming to filter out redundant features and weigh the categories accordingly instead of simply taking \nan average in order to make it better suited for transfer language selection. After all, both WALS and lang2vec metrics \nare more robust thanks to being based on a large amount of features spanning over multiple domains instead of using \nonly a single linguistic feature like eLinguistics or EzGlot. \n6.3. Task-Specific Analysis \n6.3.1. Sentiment Analysis \nLooking at the f-scores of the sentiment analysis task, it is clear that the results are very high across the board and \nthe score differences between language groups are also very small, with sometimes languages from other language \ngroups than the target emerging as the best performers. This is the case for example with Danish, as Croatian achieved \nthe highest zero-shot transfer scores for both mBERT and XLM-R instead of another Germanic language. \nA trait only observed in this task was that lang2vec and EzGlot were the only metrics keeping a moderate correlation \nin the zero-shot setting. The fact that Ezglot’s correlation stayed moderate hints the importance of lexical features. One \ncould argue that the reason behind the overall high scores might be due to the task being too easy, as it simply required \nthe classification of the entries into positive and negative. This has also been shown in other research [91]. However, this \nalso shows that it could be possible to achieve at least close to state-of-the-art results with multilingual transformer \nmodels in a zero-shot cross-lingual setting. This raises questions about how to improve the cross-lingual models to \nbetter utilize cross-lingual transfer. In the future, it would be useful to further investigate the models’ behaviour in \nzero-shot setting. This could also be useful in the further development of measures to support low-resource languages. \n6.3.2. Named Entity Recognition \nFor mBERT, the zero-shot results of the NER task look clearly lower than with same language pairs and quite \neven across all of the proposed languages and the languages belonging to the same group having generally a slightly \nhigher score. However, the results of XLM-R closely resemble those of the sentiment analysis task as the results are \nconsiderably high across all language pairs. This further shows the potential these models have in relieving the issues \nwith low-resource languages. Also, there is a moderate correlation between the zero-shot transfer performance and \nlinguistic similarity for both WALS and eLinguistics metrics, and a weak/moderate correlation with lang2vec. \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 17 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \n6.3.3. Dependency Parsing \nThe zero-shot results in DEP also seem clearly lower than with same language pairs and somewhat even across the \nboard. The languages belonging to the same language family also generally have a slightly higher score. As the task \nrequires the understanding of syntax and grammar and the scores are still reasonably high overall, the results could also \nsupport studies claiming that cross-lingual transformer models are able to learn grammar without explicit information \n[92]. \nOn the other hand, Japanese and Korean had very poor performance as source languages while also being very \ndifficult target languages in the DEP task, unlike in the sentiment analysis and NER tasks. This could mean that the \nmodel is unable to generalize to the syntax and grammar of Indo-European languages with Japonic-Koreanic languages \nand vice versa. The reason might be due to the differences in writing systems. In the DEP task, both XLM-R and \nmBERT keep a strong correlation between the zero-shot transfer performance and linguistic similarity with WALS, \neLinguistics and lang2vec metrics and a moderate correlation with EzGlot in a zero-shot setting. \n6.4. Impact \nAs there is a correlation with linguistic similarity and cross-lingual transfer performance for all of the tasks, \nincluding the abusive language identification task used in our previous research [26], it is possible to use linguistic \nsimilarity for transfer language selection, at least for these tasks. However, the correlation varied greatly from task \nto task, which means there is a lot of room for improvement in developing an optimal similarity metric for transfer \nlanguage selection. \nIn order to confirm the efficacy of choosing another language over English as the cross-lingual transfer source, we \nperformed a z-test between the results of using English as transfer source and using the language with the highest score. \nThe test showed Z = —3.18 < —1.96 and p = 0.001 < 0.05 meaning that there is a significant difference between \nusing English and the optimal language as the cross-lingual transfer source. \nBased on these results, as there is a significant difference between using English and the optimal language as the \ncross-lingual transfer source, it is better to look for high-resource languages that have proper data available and are as \nclose as possible to the target language based on a similarity metric instead of making a decision based on intuition \nor simply relying on English. This allows one to make a more informed and effective decision and makes model \ndevelopment more efficient. \n6.5. Future Research \nIn the future, we are planning to analyze, what kind of linguistic features are the most important from the \npoint of view of cross-lingual transfer. A solution could be grouping the features presented in WALS into syntactic, \nlexical, phonetic, etc., and calculating, which feature group has the strongest correlation with the cross-lingual transfer \nperformance. We could then re-quantify the WALS database using this information in order to develop an even more \neffective and comprehensive similarity metric. It would also be beneficial if the WALS project received more attention \nand the feature matrix became more densely populated. Also, instead of taking an average of lang2vec’s categories, \nthey should be weighed by importance. \nAs shown by the DEP task, the models might be able to learn syntax and grammar without any explicit information. \nThis could mean that adding explicit syntactic and grammatical information to the pre-training process of the models \nmight also improve their performance. We will take a look at this in the future. Also, as the models achieved zero-shot \ntransfer scores rivaling those of the monolingual settings, especially in sentiment analysis, it would be useful to perform \nan in-depth investigation about the models’ behaviour in a zero-shot transfer learning setting to possibly find insights \non how to improve their transfer learning capabilities. \n7. Conclusions \nIn this research we studied cross-lingual transfer language selection for zero-shot learning using three different \nNLP tasks, namely, sentiment analysis, NER, and dependency parsing. We showed the effectiveness of cross-lingual \nzero-shot transfer learning with a total of eight languages from three language families. In this way, existing data from \nhigher-resource languages may be used to improve the performance of languages that lack sufficient data. \nWe found a strong correlation between the similarity of the used languages and cross-lingual transfer performance. \nThe transfer performance declines when the distance between the languages increases. This allows for the selection \nof a more suitable transfer language by assessing linguistic similarities rather than simply depending on intuition \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 18 of 23\nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nor defaulting to English. As our experiments have demonstrated, there is a significant difference in choosing the \noptimal transfer language over defaulting to English. As there is a correlation between linguistic similarity and transfer \nperformance and a significant difference between using English and the optimal language as the cross-lingual transfer \nsource, one should instead choose the source language based on a linguistic similarity measure. Our experiments also \ndemonstrated that lexical information alone is insufficient to determine the optimal transfer languages at least for the \ntasks of NER and DEP. \nit is better to look for high-resource languages that have proper data available and are as close as possible to the target \nlanguage based on a similarity metric instead of making a decision based on intuition or simply relying on English. \nThis allows one to make a more informed and effective decision and makes model development more efficient. \nThe results showed that the proposed method for cross-lingual transfer language selection could also be useful as a \ngeneral method for other Natural Language Processing tasks, at least based on these tasks and our previous research. We \nalso showed that it is possible to achieve good performance on the target language in a zero-shot cross-lingual transfer \nsetting with multiple NLP tasks. This helps in developing better systems, especially when dealing with low-resource \nlanguages. \nWe improved a novel linguistic similarity metric consisting of various linguistic features by using the WALS \ndatabase. Our proposed method did not show the strongest correlation with the transfer performance, but it still showed \npotential as a metric that could be useful for the selection process, especially if given a more refined or inclusive feature \nset. In the future, we will reassess the importances of the linguistic features used in the similarity metric calculation in \norder to have a more refined feature set, aiming to create an even more effective and comprehensive linguistic similarity \nmetric. \nLastly, even though the overall high scores in the sentiment analysis task might be caused by the task being too \neasy, it also shows that it could be possible to achieve results close to those of a monolingual fine-tuning in a zero-shot \ncross-lingual transfer setting. This means it could be useful to thoroughly investigate the models’ behaviour in zero-shot \nsetting in order to find insights to improving their transfer capabilities. Also, as the DEP task demonstrated that the \nmodels might have a capability to understand grammar, adding explicit syntactic and grammatical information to the \nmodels’ pre-training could also increase performance. \nCRediT authorship contribution statement \nJuuso Eronen: Conceptualization, Methodology, Software, Writing - Original Draft, Writing - Review & Editing. \nMichal Ptaszynski: Conceptualization, Methodology, Supervision, Data Curation. Fumito Masui: Conceptualization, \nMethodology, Supervision, Data Curation. \nReferences \n[1] \nDavid M. Eberhard, Gary F. Simons, and Charles D. Fennig, editors. Ethnologue: Languages of the World. \nSIL International, Dallas, TX, \nUSA, twenty-fifth edition, 2022. \n[2] \nSean P. Engelson and Ido Dagan. Minimizing manual annotation cost in supervised training from corpora. In Proceedings of the 34th Annual \nMeeting on Association for Computational Linguistics, ACL ’96, page 319-326, USA, 1996. Association for Computational Linguistics. \n[3] \nSandipan Dandapat, Priyanka Biswas, Monojit Choudhury, and Kalika Bali. Complex linguistic annotation—no easy way out! a case from \nbangla and hindi pos labeling tasks. In Proceedings of the Third Linguistic Annotation Workshop (LAW III), pages 10-18, 2009. \n[4] \nJulia Hirschberg and Christopher D. Manning. Advances in natural language processing. Science, 349(6245):261-266, 2015. \n[5] \nEdoardo Maria Ponti, Helen O’Horan, Yevgeni Berzak, Ivan Vuli¢, Roi Reichart, Thierry Poibeau, Ekaterina Shutova, and Anna Korhonen. \nModeling language variation and universals: A survey on typological linguistics for natural language processing. Computational Linguistics, \n45(3):559-601, September 2019. \n[6] \nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and fate of linguistic diversity and inclusion in \nthe NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282-6293, Online, July \n2020. Association for Computational Linguistics. \n[7] \nLong Duong, Trevor Cohn, Steven Bird, and Paul Cook. \nLow resource dependency parsing: Cross-lingual parameter sharing in a neural \nnetwork parser. \nIn Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint \nconference on natural language processing (volume 2: short papers), pages 845-850, 2015. \n[8] \nRouzbeh Ghasemi, Seyed Arad Ashrafi Asli, and Saeedeh Momtazi. Deep persian sentiment analysis: Cross-lingual training for low-resource \nlanguages. Journal of Information Science, page 0165551520962781, 2020. \n[9] \nRaj Dabre, Chenhui Chu, and Anoop Kunchukuttan. \nA survey of multilingual neural machine translation. \nACM Comput. Surv., 53(5), \nSeptember 2020. \n[10] \nSaurabh Gaikwad, Tharindu Ranasinghe, Marcos Zampieri, and Christopher M. Homan. Cross-lingual offensive language identification for \nlow resource languages: The case of marathi, 2021. \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 19 of 23\n{11] \n{12] \n(13) \n{14] \n{15} \n[16] \n[17] \n[18] \n{19} \n[20] \n[21] \n[22] \n[23] \n[24] \n[25] \n[26] \n[27] \n[28] \n[29] \n[30] \n[31] \n[32] \n[33] \n[34] \n[35] \n[36] \n[37] \n[38] \nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nTelmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual bert? In ACL, 2019. \nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. Xtreme: A massively multilingual multi-task \nbenchmark for evaluating cross-lingual generalisation. In International Conference on Machine Learning, pages 4411-4421. PMLR, 2020. \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. \nBert: Pre-training of deep bidirectional transformers for language \nunderstanding, 2018. \nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, \nLuke Zettlemoyer, and Veselin Stoyanov. \nUnsupervised cross-lingual representation learning at scale. \nIn Proceedings of the 58th Annual \nMeeting of the Association for Computational Linguistics, pages 8440-8451, Online, 2020. Association for Computational Linguistics. \nShijie Wu and Mark Dredze. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT. In Proceedings of the 2019 Conference \non Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP- \nIJCNLP), pages 833-844, Hong Kong, China, November 2019. Association for Computational Linguistics. \nEdward P. Stabler and Edward L. Keenan. Structural similarity within and among languages. Theoretical Computer Science, 293(2):345-363, \n2003. Algebraic Methods in Language Processing. \nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, et al. Xglue: \nA new benchmark datasetfor cross-lingual pre-training, understanding and generation. In Proceedings of the 2020 Conference on Empirical \nMethods in Natural Language Processing (EMNLP), pages 6008-6018, 2020. \nMatt Pikuliak, Marian Simko, and Maria Bielikova. Cross-lingual learning for text processing: A survey. Expert Systems with Applications, \n165:113765, 2021. \nYu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junxian He, Zhisong Zhang, Xuezhe \nMa, Antonios Anastasopoulos, Patrick Littell, and Graham Neubig. Choosing transfer languages for cross-lingual learning. \nIn Proceedings \nof the 57th Annual Meeting of the Association for Computational Linguistics, pages 3125-3135, Florence, Italy, July 2019. Association for \nComputational Linguistics. \nRyan Cotterell and Georg Heigold. Cross-lingual character-level neural morphological tagging. \nIn Proceedings of the 2017 Conference on \nEmpirical Methods in Natural Language Processing, pages 748-759, Copenhagen, Denmark, September 2017. Association for Computational \nLinguistics. \nCharlotte Gooskens, Vincent J. van Heuven, Jelena Golubovi¢é, Anja Schiippert, Femke Swarte, and Stefanie Voigt. \nMutual intelligibility \nbetween closely related languages in europe. International Journal of Multilingualism, 15(2):169-193, 2018. \nNitish Aggarwal, Tobias Wunner, Mihael Aréan, Paul Buitelaar, and Sean O’Riain. A similarity measure based on semantic, terminological \nand linguistic information. In Proceedings of the 6th International Workshop on Ontology Matching, 01 2011. \nVincent Beaufils and Johannes Tomin. Stochastic approach to worldwide language classification: the signals and the noise towards long-range \nexploration, Oct 2020. \nLazar Kovacevic, Vladimir Bradic, Gerard de Melo, Sinisa Zdravkovic, and Olga Ryzhova. Ezglot. https: //www.ezglot.com/, 2021. \nMatthew S. Dryer and Martin Haspelmath, editors. WALS Online. Max Planck Institute for Evolutionary Anthropology, Leipzig, 2013. \nJuuso Eronen, Michal Ptaszynski, Fumito Masui, Masaki Arata, Gniewosz Leliwa, and Michal Wroczynski. Transfer language selection for \nzero-shot cross-lingual abusive language detection. Information Processing and Management, 59(4):102981, 2022. \nPatrick Littell, David R. Mortensen, Ke Lin, Katherine Kairis, Carlisle Turner, and Lori Levin. URIEL and lang2vec: Representing languages \nas typological, geographical, and phylogenetic vectors. In Proceedings of the 15th Conference of the European Chapter of the Association for \nComputational Linguistics: Volume 2, Short Papers, pages 8-14, Valencia, Spain, April 2017. Association for Computational Linguistics. \nHakan Ringbom. Cross-linguistic Similarity in Foreign Language Learning. Multilingual Matters, 2006. \nRobert Bley-Vroman. The evolving context of the fundamental difference hypothesis. Studies in Second Language Acquisition, 31(2):175—198, \n2009. \nRyan Cotterell, Sabrina J. Mielke, Jason Eisner, and Brian Roark. Are all languages equally hard to language-model? In Proceedings of the \n2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume \n2 (Short Papers), pages 536-541, New Orleans, Louisiana, 2018. Association for Computational Linguistics. \nPalakorn Achananuparp, Xiaohua Hu, and Xiajiong Shen. The evaluation of sentence similarity measures. In Proceedings of the International \nConference on Data Warehousing and Knowledge Discovery, volume 5182, pages 305-316, 09 2008. \nAminul Islam and Diana Inkpen. Semantic text similarity using corpus-based word similarity and string similarity. ACM Trans. Knowl. Discov. \nData, 2(2), 2008. \nSteven Moran, Daniel McCloy, and Richard Wright, editors. PHOIBLE Online. Max Planck Institute for Evolutionary Anthropology, Leipzig, \n2014. \nHarald Hammarstrém, Robert Forkel, Martin Haspelmath, and Sebastian Bank. glottolog/glottolog: Glottolog database 4.5, December 2021. \nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, \nGreg Corrado, et al. Google’s multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association \nfor Computational Linguistics, 5:339-351, 2017. \nYang Chen and Alan Ritter. Model selection for cross-lingual transfer. In Proceedings of the 2021 Conference on Empirical Methods in Natural \nLanguage Processing, pages 5675-5687, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational \nLinguistics. \nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. \nTyDi \nQA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages. Transactions of the Association for \nComputational Linguistics, 8:454-470, 07 2020. \nAnne Lauscher, Vinit Ravishankar, Ivan Vuli¢, and Goran Glava8. From zero to hero: On the limitations of zero-shot language transfer with \nmultilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages \n4483-4499, Online, November 2020. Association for Computational Linguistics. \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 20 of 23\n[39] \n[40] \n[41] \n[42] \n[43] \n[44] \n[45] \n[46] \n[47] \n[48] \n[49] \n[50] \n[51] \n[52] \n[53] \n[54] \n[55] \n[56] \n[57] \n[58] \n[59] \n[60] \n[61] \n[62] \n[63] \n[64] \nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nSebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, \nand Melvin Johnson. XTREME-R: Towards more challenging and nuanced multilingual evaluation. In Proceedings of the 2021 Conference \non Empirical Methods in Natural Language Processing, pages 10215-10245, Online and Punta Cana, Dominican Republic, November 2021. \nAssociation for Computational Linguistics. \nTulia Turc, Kenton Lee, Jacob Eisenstein, Ming-Wei Chang, and Kristina Toutanova. \nRevisiting the primacy of english in zero-shot cross- \nlingual transfer, 2021. \nLong Duong, Trevor Cohn, Steven Bird, and Paul Cook. Cross-lingual transfer for unsupervised dependency parsing without parallel data. In \nProceedings of the Nineteenth Conference on Computational Natural Language Learning, pages 113-122, 2015. \nXilun Chen, Ahmed Hassan Awadallah, Hany Hassan, Wei Wang, and Claire Cardie. \nMulti-source cross-lingual model transfer: Learning \nwhat to share. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3098-3112, Florence, Italy, \nJuly 2019. Association for Computational Linguistics. \nQuynh Do and Judith Gaspers. \nCross-lingual transfer learning with data selection for large-scale spoken language understanding. \nIn \nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference \non Natural Language Processing (EMNLP-IJCNLP), pages 1455-1460, Hong Kong, China, 2019. Association for Computational Linguistics. \nNiels van der Heijden, Helen Yannakoudakis, Pushkar Mishra, and Ekaterina Shutova. Multilingual and cross-lingual document classification: \nA meta-learning approach. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: \nMain Volume, pages 1966-1976, Online, April 2021. Association for Computational Linguistics. \nFarhad Nooralahzadeh, Giannis Bekoulis, Johannes Bjerva, and Isabelle Augenstein. Zero-shot cross-lingual transfer with meta learning. In \nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4547-4562, Online, November \n2020. Association for Computational Linguistics. \nAntonio Martinez-Garcia, Toni Badia, and Jeremy Barnes. \nEvaluating morphological typology in zero-shot cross-lingual transfer. \nIn \nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on \nNatural Language Processing (Volume 1: Long Papers), pages 3136-3153, Online, August 2021. Association for Computational Linguistics. \nBing Liu. Sentiment analysis and opinion mining. Synthesis lectures on human language technologies, 5(1):1—-167, 2012. \nKoyel Chakraborty, Siddhartha Bhattacharyya, and Rajib Bag. A survey of sentiment analysis from social media data. IEEE Transactions on \nComputational Social Systems, 7(2):450-464, 2020. \nAshima Yadav and Dinesh Kumar Vishwakarma. \nSentiment analysis using deep learning architectures: A review. \nArtificial Intelligence \nReview, 53(6):4335-4385, aug 2020. \nHu Xu, Bing Liu, Lei Shu, and Philip Yu. BERT post-training for review reading comprehension and aspect-based sentiment analysis. \nIn \nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language \nTechnologies, Volume 1 (Long and Short Papers), pages 2324-2335, Minneapolis, Minnesota, June 2019. Association for Computational \nLinguistics. \nAnindya Sarkar, Sujeeth Reddy, and Raghu Sesha Iyengar. Zero-shot multilingual sentiment analysis using hierarchical attentive network and \nbert. \nIn Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval, NLPIR 2019, \npage 49-56, New York, NY, USA, 2019. Association for Computing Machinery. \nMarouane Birjali, Mohammed Kasri, and Abderrahim Beni-Hssane. A comprehensive survey on sentiment analysis: Approaches, challenges \nand trends. Knowledge-Based Systems, 226:107134, 2021. \nMohammad Sadegh Rasooli, Noura Farra, Axinia Radeva, Tao Yu, and Kathleen McKeown. \nCross-lingual sentiment transfer with limited \nresources. Machine Translation, 32(1):143-165, 2018. \nAndraz Pelicon, Marko Pranjié, Dragana Miljkovi¢, Blaz Skrlj, and Senja Pollak. \nZero-shot learning for cross-lingual news sentiment \nclassification. Applied Sciences, 10(17), 2020. \nAkshi Kumar and Victor Hugo C. Albuquerque. Sentiment analysis using xlm-r transformer and zero-shot transfer learning on resource-poor \nindian language. ACM Trans. Asian Low-Resour. Lang. Inf. Process., 20(5), jun 2021. \nPhillip Keung, Yichao Lu, Gyérgy Szarvas, and Noah A Smith. The multilingual amazon reviews corpus. arXiv preprint arXiv:2010.02573, \n2020. \nJan Kocon, Piotr Mitkowski, and Monika Zasko-Zieliriska. Multi-level sentiment analysis of PolEmo 2.0: Extended corpus of multi-domain \nconsumer reviews. \nIn Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 980-991, Hong \nKong, China, November 2019. Association for Computational Linguistics. \nSergey Smetanin and Michail Komarov. Sentiment analysis of product reviews in russian using convolutional neural networks. In 2019 IEEE \n21st Conference on Business Informatics (CBI), volume 01, pages 482-486, July 2019. \nVikas Yadav and Steven Bethard. \nA survey on recent advances in named entity recognition from deep learning models. \narXiv preprint \narXiv:1910.11470, 2019. \nJing Li, Aixin Sun, Jianglei Han, and Chenliang Li. A survey on deep learning for named entity recognition. JEEE Transactions on Knowledge \nand Data Engineering, 34(1):50-70, 2022. \nSajid Ali, Khalid Masood, Anas Riaz, and Amna Saud. \nNamed entity recognition using deep learning: A review. \nIn 2022 International \nConference on Business Analytics for Technology and Security (ICBATS), pages 1-7. IEEE, 2022. \nAlexander Fritzler, Varvara Logacheva, and Maksim Kretov. Few-shot classification in named entity recognition task. In Proceedings of the \n34th ACM/SIGAPP Symposium on Applied Computing, SAC ’19, page 993-1000, New York, NY, USA, 2019. Association for Computing \nMachinery. \nTaesun Moon, Parul Awasthy, Jian Ni, and Radu Florian. Towards lingua franca named entity recognition with bert, 2019. \nRasmus Hvingelby, Amalie Brogaard Pauli, Maria Barrett, Christina Rosted, Lasse Malm Lidegaard, and Anders Sggaard. DaNE: A named \nentity resource for Danish. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 4597-4604, Marseille, France, \nMay 2020. European Language Resources Association. \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 21 of 23\n[65] \n[66] \n[67] \n[68] \n[69] \n[70] \n[71] \n[72] \n[73] \n[74] \n[75] \n[76] \n[77] \n[78] \n[79] \n[80] \n[81] \n[82] \n[83] \n[84] \n[85] \nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nAlankar Jain, Bhargavi Paranjape, and Zachary C. Lipton. Entity projection via machine translation for cross-lingual NER. In Proceedings of \nthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language \nProcessing (EMNLP-IJCNLP), pages 1083-1092, Hong Kong, China, November 2019. Association for Computational Linguistics. \nBing Li, Yujie He, and Wenjin Xu. Cross-lingual named entity recognition using parallel corpus: A new approach using xlm-roberta alignment. \narXiv preprint arXiv:2101.11112, 2021. \nSabine Weber and Mark Steedman. \nZero-shot cross-lingual transfer is a hard baseline to beat in German fine-grained entity typing. \nIn \nProceedings of the Second Workshop on Insights from Negative Results in NLP, pages 42-48, Online and Punta Cana, Dominican Republic, \nNovember 2021. Association for Computational Linguistics. \nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. \nCross-lingual name tagging and linking for 282 \nlanguages. \nIn Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages \n1946-1958, Vancouver, Canada, July 2017. Association for Computational Linguistics. \nAfshin Rahimi, Yuan Li, and Trevor Cohn. \nMassively multilingual transfer for NER. \nIn Proceedings of the 57th Annual Meeting of the \nAssociation for Computational Linguistics, pages 151-164, Florence, Italy, July 2019. Association for Computational Linguistics. \nMin Xiao and Yuhong Guo. Distributed word representation learning for cross-lingual dependency parsing. In Proceedings of the Eighteenth \nConference on Computational Natural Language Learning, pages 119-129, Ann Arbor, Michigan, June 2014. Association for Computational \nLinguistics. \nJérg Tiedemann. \nCross-lingual dependency parsing with universal dependencies and predicted pos labels. \nIn Proceedings of the Third \nInternational Conference on Dependency Linguistics (Depling 2015), pages 340-349, 2015. \nJiang Guo, Wanxiang Che, David Yarowsky, Haifeng Wang, and Ting Liu. \nCross-lingual dependency parsing based on distributed \nrepresentations. \nIn Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International \nJoint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1234-1244, Beijing, China, July 2015. Association for \nComputational Linguistics. \nOphélie Lacroix, Lauriane Aufrant, Guillaume Wisniewski, and Frangois Yvon. Frustratingly easy cross-lingual transfer for transition-based \ndependency parsing. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: \nHuman Language Technologies, pages 1058-1063, 2016. \nMohit Bansal. Dependency link embeddings: Continuous representations of syntactic substructures. \nIn Proceedings of the Ist Workshop \non Vector Space Modeling for Natural Language Processing, pages 102-108, Denver, Colorado, June 2015. Association for Computational \nLinguistics. \nDan Kondratyuk and Milan Straka. 75 languages, 1 model: Parsing Universal Dependencies universally. In Proceedings of the 2019 Conference \non Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP- \nIJCNLP), pages 2779-2795, Hong Kong, China, November 2019. Association for Computational Linguistics. \nMatej Uléar and Marko Robnik-Sikonja. Finest bert and crosloengual bert. In International Conference on Text, Speech, and Dialogue, pages \n104-111. Springer, 2020. \nJoakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Haji¢, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis \nTyers, and Daniel Zeman. Universal Dependencies v2: An evergrowing multilingual treebank collection. In Proceedings of the 12th Language \nResources and Evaluation Conference, pages 4034-4043, Marseille, France, May 2020. European Language Resources Association. \nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, \nAntoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen \nCreel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin \nEthayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori \nHashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, \nPratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith \nKuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, \nAli Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, \nBen Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung \nPark, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, \nJack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, \nArmin W. Thomas, Florian Tramér, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, \nJiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On \nthe opportunities and risks of foundation models, 2021. \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention \nis all you need. Advances in neural information processing systems, 30, 2017. \nKarthikeyan K, Zihan Wang, Stephen Mayhew, and Dan Roth. Cross-lingual ability of multilingual bert: An empirical study. In International \nConference on Learning Representations, 2020. \nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. \nRoberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. \nAlexis Conneau and Guillaume Lample. \nCross-lingual language model pretraining. Advances in Neural Information Processing Systems, \n32:7059-7069, 2019. \nCecil Brown, Eric Holman, and Sgren Wichmann. Sound correspondences in the world’s languages. Language, 89:4—29, 03 2013. \nCharlotte Gooskens. \nThe contribution of linguistic factors to the intelligibility of closely related languages. \nJournal of Multilingual and \nmulticultural development, 28(6):445—467, 2007. \nChaitanya Malaviya, Graham Neubig, and Patrick Littell. \nLearning language representations for typology prediction. \nIn Proceedings of \nthe 2017 Conference on Empirical Methods in Natural Language Processing, pages 2529-2535, Copenhagen, Denmark, September 2017. \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 22 of 23\n[86] \n[87] \n[88] \n[89] \n[90] \n[91] \n[92] \nZero-Shot Cross-Lingual Transfer Language Selection Using Linguistic Similarity \nAssociation for Computational Linguistics. \nBenedikt Szmrecsanyi. Geography is overrated. Dialectological and folk dialectological concepts of space, pages 215-231, 2012. \nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, \nMorgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain \nGugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings \nof the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online, October 2020. \nAssociation for Computational Linguistics. \nLeon Kellner. Historical outlines of English syntax. Macmillan, 1892. \nChristiane Dalton-Puffer. The French influence on Middle English morphology: A corpus-based study on derivation, volume 20. Walter de \nGruyter, 2011. \nPhilip Durkin. Borrowed words: A history of loanwords in English. Oxford University Press, 2014. \nJuuso Eronen, Michal Ptaszynski, Fumito Masui, Aleksander Smywiriski-Pohl, Gniewosz Leliwa, and Michal Wroczynski. Improving classifier \ntraining efficiency for automatic cyberbullying detection with feature density. Information Processing & Management, 58(5):102616, 2021. \nGanesh Jawahar, Benoit Sagot, and Djamé Seddah. \nWhat does BERT learn about the structure of language? \nIn Proceedings of the 57th \nAnnual Meeting of the Association for Computational Linguistics, pages 3651-3657, Florence, Italy, July 2019. Association for Computational \nLinguistics. \n  \nEronen et al.: Preprint submitted to Elsevier \nPage 23 of 23\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-01-31",
  "updated": "2023-01-31"
}