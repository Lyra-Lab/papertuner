{
  "id": "http://arxiv.org/abs/2301.13868v1",
  "title": "PADL: Language-Directed Physics-Based Character Control",
  "authors": [
    "Jordan Juravsky",
    "Yunrong Guo",
    "Sanja Fidler",
    "Xue Bin Peng"
  ],
  "abstract": "Developing systems that can synthesize natural and life-like motions for\nsimulated characters has long been a focus for computer animation. But in order\nfor these systems to be useful for downstream applications, they need not only\nproduce high-quality motions, but must also provide an accessible and versatile\ninterface through which users can direct a character's behaviors. Natural\nlanguage provides a simple-to-use and expressive medium for specifying a user's\nintent. Recent breakthroughs in natural language processing (NLP) have\ndemonstrated effective use of language-based interfaces for applications such\nas image generation and program synthesis. In this work, we present PADL, which\nleverages recent innovations in NLP in order to take steps towards developing\nlanguage-directed controllers for physics-based character animation. PADL\nallows users to issue natural language commands for specifying both high-level\ntasks and low-level skills that a character should perform. We present an\nadversarial imitation learning approach for training policies to map high-level\nlanguage commands to low-level controls that enable a character to perform the\ndesired task and skill specified by a user's commands. Furthermore, we propose\na multi-task aggregation method that leverages a language-based multiple-choice\nquestion-answering approach to determine high-level task objectives from\nlanguage commands. We show that our framework can be applied to effectively\ndirect a simulated humanoid character to perform a diverse array of complex\nmotor skills.",
  "text": "PADL: Language-Directed Physics-Based Character Control\nJordan Juravsky\nNVIDIA\nUniversity of Waterloo\nCanada\njjuravsky@nvidia.com\nYunrong Guo\nNVIDIA\nCanada\nkellyg@nvidia.com\nSanja Fidler\nNVIDIA\nUniversity of Toronto\nCanada\nsfidler@nvidia.com\nXue Bin Peng\nNVIDIA\nSimon Fraser University\nCanada\njapeng@nvidia.com\n(a) Skill command: \"jump and swing sword down\".\n(b) Skill command: \"shield charge forward\".\nFigure 1: Our framework allows users to direct the behaviors of physically simulated characters using natural language com-\nmands. Left: Humanoid character performing a jump attack. Right: Character knocking over a target object by performing a\nshield charge.\nABSTRACT\nDeveloping systems that can synthesize natural and life-like mo-\ntions for simulated characters has long been a focus for computer an-\nimation. But in order for these systems to be useful for downstream\napplications, they need not only produce high-quality motions, but\nmust also provide an accessible and versatile interface through\nwhich users can direct a character’s behaviors. Natural language\nprovides a simple-to-use and expressive medium for specifying a\nuser’s intent. Recent breakthroughs in natural language processing\n(NLP) have demonstrated effective use of language-based interfaces\nfor applications such as image generation and program synthesis.\nIn this work, we present PADL, which leverages recent innovations\nin NLP in order to take steps towards developing language-directed\ncontrollers for physics-based character animation. PADL allows\nusers to issue natural language commands for specifying both high-\nlevel tasks and low-level skills that a character should perform. We\npresent an adversarial imitation learning approach for training poli-\ncies to map high-level language commands to low-level controls\nthat enable a character to perform the desired task and skill specified\nby a user’s commands. Furthermore, we propose a multi-task ag-\ngregation method that leverages a language-based multiple-choice\nquestion-answering approach to determine high-level task objec-\ntives from language commands. We show that our framework can\nbe applied to effectively direct a simulated humanoid character to\nperform a diverse array of complex motor skills.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSA ’22 Conference Papers, December 6–9, 2022, Daegu, Republic of Korea\n© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9470-3/22/12...$15.00\nhttps://doi.org/10.1145/3550469.3555391\nCCS CONCEPTS\n• Computing methodologies →Procedural animation; Adver-\nsarial learning.\nKEYWORDS\ncharacter animation, language commands, reinforcement learning,\nadversarial imitation learning\nACM Reference Format:\nJordan Juravsky, Yunrong Guo, Sanja Fidler, and Xue Bin Peng. 2022. PADL:\nLanguage-Directed Physics-Based Character Control. In SIGGRAPH Asia\n2022 Conference Papers (SA ’22 Conference Papers), December 6–9, 2022, Daegu,\nRepublic of Korea. ACM, New York, NY, USA, 12 pages. https://doi.org/10.\n1145/3550469.3555391\n1\nINTRODUCTION\nDeveloping physically simulated characters that are capable of pro-\nducing complex and life-like behaviors has been one of the central\nchallenges in computer animation. Efforts in this domain has led to\nsystems that can produce high-quality motions for a wide range of\nskills [Clegg et al. 2018; de Lasa et al. 2010; Hodgins et al. 1995; Lee\net al. 2010a; Liu and Hodgins 2018; Liu et al. 2016; Mordatch et al.\n2012; Peng et al. 2018a; Tan et al. 2014; Wang et al. 2009]. However,\nin order for these systems to be useful for downstream applications,\nthe control models need not only produce high quality motions,\nbut also provide users with an accessible and versatile interface\nthrough which to direct a character’s behaviors. This interface is\ncommonly instantiated through compact control abstractions, such\nas joystick controls or target way points. These control abstractions\nallow users to easily direct a character’s behavior via high-level\ncommands, but they can greatly restrict the variety and granular-\nity of the behaviors that a user can actively control. Alternatively,\nmotion tracking models can provide a versatile interface that en-\nables fine-grain control over a character’s movements by directly\nspecifying target motion trajectories. However, authoring motion\ntrajectories can be a labour-intensive process, requiring significant\ndomain expertise or specialized equipment (e.g. motion capture).\narXiv:2301.13868v1  [cs.LG]  31 Jan 2023\nSA ’22 Conference Papers, December 6–9, 2022, Daegu, Republic of Korea\nJordan Juravsky, Yunrong Guo, Sanja Fidler, and Xue Bin Peng\nAn ideal animation system should provide an accessible interface\nthat allows users to easily specify desired behaviors for a character,\nwhile also being sufficiently versatile to enable control over a rich\ncorpus of skills. Natural language offers a promising medium that is\nboth accessible and versatile. The recent development of large and\nexpressive language models has provided powerful tools for inte-\ngrating natural language interfaces for a wide range of downstream\napplications [Brown et al. 2020; Devlin et al. 2018; Radford et al.\n2021], such as generating functional code and realistic images from\nnatural language descriptions [Chen et al. 2021; Ramesh et al. 2022;\nTan et al. 2018]. In this work, we aim to leverage these techniques\nfrom NLP to take steps towards developing a language-directed\nsystem for physics-based character animation.\nThe central contribution of this work is a system for language-\ndirected physics-based character animation, which enables users\nto direct the behaviors of a physically simulated character using\nnatural language commands. Given a dataset of motion clips and\ncaptions, which describe the behaviors depicted in each clip, our\nsystem trains control policies to map from high-level language\ncommands to low-level motor commands that enable a character\nto reproduce the corresponding skills. We present an adversarial\nimitation learning approach that allows a policy to reproduce a\ndiverse array of skills, while also learning to ground each skill in\nlanguage commands. Our policies can also be trained to perform\nadditional auxiliary tasks. We present a language-based multi-task\naggregation model, which selects between a collection of task-\nspecific policies according to a given command, thereby allowing\nusers to easily direct a character to perform various high-level\ntasks via natural language. We present one of the first systems\nthat can effectively leverage language commands to direct full-\nbody physically simulated character to perform a diverse array\nof complex motor skills. The code for this work is available at\nhttps://github.com/nv-tlabs/PADL.\n2\nRELATED WORK\nSynthesizing natural and intelligent behaviors for simulated charac-\nters has been a core subject of interest in computer animation, with\na large body of work focused on building kinematic and physics-\nbased control models that can generate life-like motions [Clegg\net al. 2018; da Silva et al. 2008; Hodgins et al. 1995; Holden et al.\n2016; Lee et al. 2010a; Liu and Hodgins 2018; Tan et al. 2014; Wang\net al. 2009, 2012]. While a great deal of emphasis has been placed\non motion quality, considerably less attention has been devoted on\nthe directability of the resulting models at run-time. Directability is\noften incorporated into these models via control abstractions that\nallow users to direct a character’s behaviors through high-level com-\nmands. These abstractions tend to introduce a trade-off between\naccessibility and versatility. Simple control abstractions, such as joy-\nstick commands or target waypoints, [Agrawal and van de Panne\n2016; Coros et al. 2009; Holden et al. 2017; Lee et al. 2021b,a, 2010b;\nLing et al. 2020; Peng et al. 2018a, 2022, 2021; Starke et al. 2019;\nTreuille et al. 2007; Zhang et al. 2020], provide an accessible in-\nterface that can be easily adopted by users. But these abstractions\ncan also limit the versatility of the behaviors that can be actively\ncontrolled by a user. Alternatively, general motion tracking models\ncan provide a versatile interface, which allows for fine-grain control\nover a character’s movements through target motion trajectories\n[Bergamin et al. 2019; Park et al. 2019; Pollard et al. 2002; Wang\net al. 2020; Won et al. 2020; Yamane et al. 2010]. These target tra-\njectories specify desired poses for the character to reach at every\ntimesteps, which in principle can direct the character to perform\nany feasible motion. However, this versatility often comes at the\ncost of accessibility, since authoring target motion trajectories can\nbe as tedious and labour intensive as manual keyframe animation.\nMotion capture can be a more expeditious approach for generating\ntarget trajectories for motion-tracking models [Peng et al. 2018b;\nWang et al. 2020; Yu et al. 2021; Yuan et al. 2021], but tends to require\nspecialized equipment and may limit the reproducible behaviors to\nthose that can be physically performed by the user. In this work,\nwe aim to leverage natural language to develop an accessible and\nversatile control interface for physics-based character animation.\nNatural Language Processing: Language models trained on in-\ncreasingly large datasets have been shown to develop powerful\nrepresentations for text data [Devlin et al. 2018; Liu et al. 2019;\nRaffel et al. 2019], which can be used for a wide range of down-\nstream applications. One such example is text-guided synthesis,\nwhere a user’s prompt, expressed in natural language, can be used\nto direct models to produce different types of content. Large au-\ntoregressive models are able to generate coherent text completions\ngiven a user’s starter prompt [Brown et al. 2020]. These models\nlead to the popularization of “prompt engineering\", where the aim\nis to construct optimal prompt templates that elicit the desired be-\nhaviors from a language model. Such prompt-based systems, often\ncombined with filtering or other post-processing techniques, have\nbeen successfully used to solve grade-school math problems and\ncompetitive programming challenges [Cobbe et al. 2021; Li et al.\n2022]. Text-guided synthesis can also be applied across different\nmodalities. Here, the language model does not directly generate\nthe desired content, instead it provides a semantically meaningful\nencoding for a user’s language prompt, which can then be used\nby a separately trained decoder to generate content in a different\nmodality. Nichol et al. [2021] and Ramesh et al. [2022] successfully\nused this approach to generate photo-realistic images from natu-\nral language, leveraging the text encoder from CLIP [Radford et al.\n2021]. In this work, we aim to leverage powerful language models to\ndevelop language-directed controllers for physics-based character\nanimation.\nLanguage-Directed Animation: Synthesizing motion from lan-\nguage is one of the core challenges of audio-driven facial animation,\nwhere the goal is to generate facial motions for a given utterance.\nThese models typically take advantage of the temporal correspon-\ndence between units of speech (phonemes) and facial articulations\n(visemes) in order to synthesize plausible facial animations for a\nparticular utterance [Brand 1999; Deena and Galata 2009; Hong\net al. 2002; Karras et al. 2017; Pelachaud et al. 1996]. A similar tem-\nporal correspondence can also be leveraged to generate full-body\ngestures from speech [Ahuja and Morency 2019; Alexanderson et al.\n2020; Levine et al. 2009]. While these techniques can be highly ef-\nfective for generating realistic motions from speech, they are not\ndirectly applicable in more general settings where there is no clear\ntemporal correspondence between language and motion. For ex-\nample, a high-level command such as “knock over the red block”\nPADL: Language-Directed Physics-Based Character Control\nSA ’22 Conference Papers, December 6–9, 2022, Daegu, Republic of Korea\nimplicitly encodes a sequence of skills that a character should per-\nform. Sequence-to-sequence models have been proposed to map\nhigh-level language descriptions to motion trajectories [Lin et al.\n2018; Plappert et al. 2017]. Ahuja and Morency [2019] and Tevet\net al. [2022] proposed autoencoder frameworks that learns a joint\nembedding of language and motion, which can be used to gen-\nerate full-body motions from language descriptions. While these\ntechniques have demonstrated promising results, they have been\nprimarily focused on developing kinematic motion models. In this\nwork, we aim to develop a language-directed model for physics-\nbased character animation, which maps high-level language com-\nmands to low-level controls that enable a character to perform the\ndesired behaviors.\n3\nBACKGROUND\nOur characters are trained using a goal-conditioned reinforcement\nlearning framework, where an agent interacts with an environment\naccording to a control policy 𝜋in order to fulfill a given goal g ∈\nG, drawn from a goal distribution g ∼𝑝(g). At each time step\n𝑡, the agent observes the state of the environment s𝑡∈S, and\nresponds by applying an action a𝑡∈A, sampled from the policy\na𝑡∼𝜋(a𝑡|s𝑡, g). After applying the action a𝑡, the environment\ntransitions to a new state s𝑡+1, and the agent receives a scalar\nreward 𝑟𝑡= 𝑟(s𝑡, a𝑡, s𝑡+1, g) that reflects the desirability of the state\ntransition for the given goal g. The agent’s objective is to learn\npolicy 𝜋that maximizes its expected discounted return 𝐽(𝜋),\n𝐽(𝜋) = E𝑝(g)E𝑝(𝜏|𝜋,g)\n\"𝑇−1\n∑︁\n𝑡=0\n𝛾𝑡𝑟𝑡\n#\n,\n(1)\nwhere 𝑝(𝜏|𝜋, g) = 𝑝(s0) Î𝑇−1\n𝑡=0 𝑝(s𝑡+1|s𝑡, a𝑡)𝜋(a𝑡|s𝑡, g) denotes the\nlikelihood of a trajectory 𝜏= (s0, a0, s1, ..., s𝑇) under a policy 𝜋\ngiven a goal g,𝑝(s0) is the initial state distribution, and𝑝(s𝑡+1|s𝑡, s𝑎)\nrepresents the transition dynamics of the environment. 𝑇is the\ntime horizon of a trajectory, and 𝛾∈[0, 1] is a discount factor.\n4\nOVERVIEW\nIn this paper we introduce Physics-based Animation Directed with\nLanguage (PADL; pronounced “paddle”), a system for developing\nlanguage-directed control models for physics-based character an-\nimation. Our framework allows users to control the motion of a\ncharacter by specifying a task to complete, as well as a specific\nskill to use while completing that task. Tasks represent high-level\nobjectives that the agent must accomplish, such as navigating to\na target location or interacting with a specific object. In addition\nto specifying what task an agent must accomplish, it is important\nfor users to be able to control how the task is accomplished. For\nexample, given the task of navigating to a target location, an agent\ncan walk, run, or jump to the target. In our system, the desired\ntask and skill for the character are specified separately via natural\nlanguage in the form of a task command and a skill command.\nOur framework consists of three stages, and a schematic overview\nof the system is available in Figure 2. First, in the Skill Embedding\nstage, a reference motion dataset M = {(m𝑖,𝑐𝑖)}, containing mo-\ntion clips m𝑖annotated with natural language captions 𝑐𝑖, is used to\nlearn a shared embedding space Z of motions and text. Each motion\nclip 𝑚𝑖= {ˆq𝑖\n𝑡} is represented by a sequence of poses ˆq𝑖\n𝑡. A motion\nFigure 2: The PADL framework consists of three stages. 1)\nIn the Skill Embedding stage, a dataset of motion clips and\ncorresponding text captions are used to learn a joint em-\nbedding of motions and captions. 2) In the Policy Training\nstage, the learned skill embedding is used to train a collec-\ntion of policies to perform various tasks, while imitating be-\nhaviors in the dataset. 3) Finally, in the Multi-Task Aggrega-\ntion stage, policies trained for different tasks are combined\ninto a multi-task controller that can be directed to perform\ndifferent tasks and skills via language commands.\nencoder 𝑧𝑖𝑚= Enc𝑚(m𝑖) and language encoder 𝑧𝑖\n𝑙= Enc𝑙(𝑐𝑖) are\ntrained to map each motion and caption pair to similar embeddings\n𝑧𝑖𝑚≈𝑧𝑖\n𝑙. Next, in the Policy Training stage, this embedding is used\nto train a collection of reinforcement learning policies, where each\npolicy 𝜋𝑖(a𝑡|s𝑡, g, z) is trained to perform a particular task using\nvarious skills z ∈Z from the embedding. Once trained, the policy\ncan then be directed to execute a particular skill by conditioning\n𝜋on the embedding of a given language command 𝑧𝑙= Enc𝑙(𝑐).\nFinally, in the Multi-Task Aggregation stage, the different policies\nare integrated into a multi-task controller that can be directed using\nlanguage commands to perform a specific task using a desired skill.\n5\nSKILL EMBEDDING\nIn the Skill Embedding stage, our objective is to construct an em-\nbedding space that aligns motions with their corresponding natural\nlanguage descriptions. To do this, we follow a similar procedure as\nMotionCLIP [Tevet et al. 2022], where a transformer autoencoder is\nSA ’22 Conference Papers, December 6–9, 2022, Daegu, Republic of Korea\nJordan Juravsky, Yunrong Guo, Sanja Fidler, and Xue Bin Peng\ntrained to encode motion sequences into a latent representation that\n“aligns” with the language embedding from a pre-trained CLIP text\nencoder [Radford et al. 2021]. Given a motion clip ˆm = (ˆq1, ..., ˆq𝑛)\nand its caption 𝑐, a motion encoder z = Enc𝑚( ˆm) maps the motion\nto an embedding z. The embedding is normalized to lie on a unit\nsphere ||z|| = 1. Following Tevet et al. [2022], Enc𝑚(m) is mod-\neled by a bidirectional transformer [Devlin et al. 2018]. A motion\ndecoder is jointly trained with the encoder to produce a reconstruc-\ntion sequence m = (q1, ..., q𝑛) to recover ˆm from z. The decoder is\nalso modelled as a birectional transformer m = Dec(z, U), which\ndecodes all frames of in parallel using a learned constant query\nsequence U = (u1, ..., u𝑛), similar to the final layer of Carion et al.\n[2020]. The autoencoder is trained with the loss:\nLauto = Lrecon + 0.1Lalign.\n(2)\nThe reconstruction loss Lrecon measures the error between the\nreconstructed sequence and original motion:\nLrecon = 1\n𝑛\n𝑛\n∑︁\n𝑡=1\n||ˆq𝑡−Dec (Enc𝑚( ˆm) , U) ||2\n2.\n(3)\nThe alignment loss Lalign measures the cosine distance between a\nmotion embedding and the language embedding:\nLalign = 1 −𝑑cos (Enc𝑚( ˆm) , Enc𝑙(𝑐)) .\n(4)\nThe language encoder Enc𝑙(m) is modeled using a pre-trained\nCLIP text encoder with an added head of two fully-connected layers,\nwhere only this output head is fine-tuned according to Eq. 4. To help\navoid overfitting, for every minibatch of motion sequences sampled\nfrom the dataset we also extract a random subsequence from each\nmotion and add these slices to the batch that the model is trained\non. These subsequences only contribute to the reconstruction loss.\n6\nPOLICY TRAINING\nOnce we have a joint embedding of motions and captions, we will\nnext use the embedding to train control policies that enable a physi-\ncally simulated character to perform various high-level tasks while\nusing skills specified by language commands. At each timestep 𝑡,\nthe policy 𝜋(a𝑡|s𝑡, g, z) receives as input the state of the character\ns𝑡, a task-specific goal g, and a skill latent z. The goal g specifies\nhigh-level task objectives that the character should achieve, such as\nmoving to a target location or facing a desired direction. The skill\nlatent z specifies the skill that the character should use to achieve\nthe desired goal, such as walking vs running to a target location.\nThe latents are generated by encoding motion clips z = Enc𝑚(m)\nsampled from the dataset M. In order to train a policy to perform\na given task using a desired skill, we utilize a reward function\nconsisting of two components:\n𝑟𝑡= 𝑟skill\n𝑡\n+ 𝜆task𝑟task\n𝑡\n,\n(5)\nwhere 𝑟skill\n𝑡\nis a skill-reward, and 𝑟task\n𝑡\nis a task-reward with coeffi-\ncient 𝜆task.\n6.1\nSkill Objective\nTo train the policy to perform the skill specified by a particular z𝑖,\nwe enforce that the policy’s distribution of state transitions (s, s′)\nmatches that of the corresponding motion clip m𝑖. To accomplish\nthis, we train an adversarial discriminator 𝐷(s, s′, z) on the joint\ndistribution of state transitions and skill encodings [Ho and Er-\nmon 2016; Merel et al. 2017; Peng et al. 2021]. The discriminator\nis trained to predict if a given state transition (s, s′) is from the\nmotion clip corresponding to z, or if the transition is from the sim-\nulated character or from a different motion clip in the dataset. The\ndiscriminator is trained by minimizing the following loss:\nL𝐷= E𝑝M (m)\n\u0014\n−E𝑝m(s,s′)\n\u0002\nlog(𝐷(s, s′, z))\n\u0003\n(6)\n−𝑤𝐷E𝑝𝜋(s,s′|z)\n\u0002\nlog(1 −𝐷(s, s′, z))\n\u0003\n(7)\n−(1 −𝑤𝐷) E𝑝M\\m(s,s′)\n\u0002\nlog(1 −𝐷(s, s′, z))\n\u0003\n(8)\n+ 𝑤gp E𝑝m(s,s′)\n\u0014\f\f\f\n\f\f\f∇𝜙𝐷(𝜙, z)\n\f\f\f𝜙=(s,s′)\n\f\f\f\n\f\f\f\n2\u0015 \u0015\n.\n(9)\n𝑝M (m) represents the likelihood of sampling a motion clip m from\na dataset M, and z = Enc𝑚(m) is the encoding of the motion\nclip. 𝑝m(s, s′) denotes the likelihood of observing a state transition\nfrom a given motion clip, and 𝑝𝜋(s, s′|z) is the likelihood of ob-\nserving a state transition from the policy 𝜋when conditioned on z.\n𝑝M\\m(s, s′) represents the likelihood of observing a state transi-\ntion by sampling random transitions from other motion clips in the\ndataset, excluding m, and 𝑤𝐷is a manually specified coefficient.\nThe final term in the loss is a gradient penalty with coefficient\n𝑤gp [Peng et al. 2021], which improves stability of the adversarial\ntraining process. The skill-reward is then given by:\n𝑟skill\n𝑡\n= −log (1 −𝐷(s𝑡, s𝑡+1, z)) .\n(10)\nTo direct the policy with a skill command 𝑐skill after it has been\ntrained, the model is provided with the encoding z = Enc𝑙(𝑐skill).\nBy conditioning the discriminator on both state transitions and\nlatents, our method explicitly encourages the policy to imitate\nevery motion clip in the dataset, which can greatly reduce mode\ncollapse. We elaborate on this benefit and compare our approach\nto related adversarial RL frameworks in Appendix D.\n7\nMULTI-TASK AGGREGATION\nEach policy from the Policy Training stage is capable of performing\na variety of skills, but each is only able to perform a single high-level\ntask involving a single target object. We show that these individual\npolicies can be aggregated into a more flexible composite policy,\nwhich allows users to direct the character to perform a variety\nof different tasks in an environment containing multiple objects.\nHowever, in our experiments, we found that attempting to use\nthe procedure in Section 6 to train a single multi-task policy to\nperform all tasks leads to poor performance. Effectively training\nmulti-task policies remains a challenging and open problem in RL,\nand prior systems have often taken a divide-and-conquer approach\nfor multi-task RL [Ghosh et al. 2018; Ruder 2017; Rusu et al. 2015].\nTo create a more flexible multi-task, multi-object controller, we\naggregate a collection of single-task policies together. At each\ntimestep, the user’s current task command is used to generate\nprompts that are fed to a multiple-choice question-answering (QA)\nmodel. The QA model identifies which task and environment object\nare being referenced by the user. The single-task controller for the\nPADL: Language-Directed Physics-Based Character Control\nSA ’22 Conference Papers, December 6–9, 2022, Daegu, Republic of Korea\nidentified task is then set as the active policy controlling the char-\nacter, and the state of the identified object is passed to the selected\npolicy. An overview of this procedure is provided with pseudocode\nin Algorithm 1 in the Appendix. Note that since the character is\nbeing driven by a single policy from Section 6 at every timestep, the\naggregated controller can only follow one high-level task involving\na single object at a time. However, with this controller the user can\ndynamically control which task and object are focused on using\nnatural language.\n7.1\nMultiple Choice Question Answering\nAn overview of the language-based selection model is shown in\nFigure 3. The multiple-choice QA model is constructed using a\npre-trained BERT model fine-tuned on the SWAG dataset [Zellers\net al. 2018]. Each multiple-choice question is formulated as an ini-\ntial prompt sentence (Sentence A) alongside 𝑛candidate follow-up\nsentences (Sentence B) [Devlin et al. 2018]. The model then outputs\nscores for 𝑛distinct sequences, where sequence 𝑖is the concatena-\ntion of the prompt sentence with the 𝑖-th candidate sentence. The\nobject corresponding to the candidate sentence with the highest\nscore is selected as the target object for the policy. A similar process\nis used to identify the task from the user’s command.\nFor each task command provided by the user, the model is pro-\nvided with two separate multiple-choice questions to identify the\nrelevant task and object, respectively. The first question identifies\nthe task, where each multiple choice option corresponds to a trained\npolicy. The inputs to the QA model follow a story-like format in\norder to mimic the elements of the SWAG dataset that the model\nwas fine-tuned on. For example, if the task command is “knock over\nthe blue tower”, the candidate sequence for the strike policy is:\n• \"Bob wants to knock over the blue tower. This should be easy\nfor him since he possesses the ability to knock over a specified\nobject.\"\nSimilarly, the candidate sequence for the location policy is given\nby:\n• \"Bob wants to knock over the blue tower. This should be easy\nfor him since he possesses the ability to navigate to a specified\ndestination.\"\nThe multiple-choice QA model will then predict which sequence of\nsentences are most likely. Similarly, in the multiple-choice question\nto extract the target object, each object is given a multiple choice\noption describing the object’s appearance. The candidate sequence\nfor the green block is given by:\n• \"Bob wants to knock over the blue tower. He starts by turning\nhis attention to the green object nearby.\"\n8\nEXPERIMENTAL SETUP\nWe evaluate the effectiveness of our framework by training language-\ndirected control policies for a 3D simulated humanoid character.\nThe character is equipped with a sword and shield, similar to the\none used by Peng et al. [2022], with 37 degrees-of-freedom, and\nsimilar state and action representations. The dataset contains a\ntotal of 131 individual clips, for a total of approximately 9 minutes\nof motion data. Each clip is manually labeled with 1-4 captions that\ndescribe the behavior of the character within a particular clip, for a\nFigure 3: Overview of the language-based selection model\nused to select a target object based on the user’s task com-\nmand. The task command is used to generate a collection of\ncandidate sentences, each corresponding to a particular ob-\nject in the environment. A multiple-choice QA model is then\nused to predict the most likely candidate sentence, based on\nthe task command. The model’s prediction is used to iden-\ntify the target object the user referenced.\ntotal of 265 captions in the entire dataset. Fig. 4 illustrates examples\nof motion clips in the dataset along with their respective captions.\n8.1\nTasks\nIn addition to training policies to imitate skills from the dataset, each\npolicy is also trained to perform an additional high-level task. Here,\nwe provide an overview of the various tasks, and more detailed\ndescriptions are available in Appendix B.\n(1) Facing: First, we have a simple facing task, where the objec-\ntive is for the character to turn and face a target direction\nd∗, encoded as a 2D vector on the horizontal plane. The goal\ninput g𝑡= ˜d∗\n𝑡for the policy records the goal direction in the\ncharacter’s local coordinate frame.\n(2) Location: Next, we have a target location task, where the\nobjective is for the character to navigate to a target loca-\ntion x∗. The goal g𝑡= ˜x∗\n𝑡records the target location in the\ncharacter’s local coordinate frame ˜x∗\n𝑡.\n(3) Strike: Finally, we have a strike task, where the objective\nis for the character to knock over a target object. The goal\ng𝑡= ( ˜x∗\n𝑡, ˜¤x∗\n𝑡, ˜𝑞∗\n𝑡, ˜¤𝑞∗\n𝑡) records the target object’s position ˜x∗\n𝑡,\nrotation ˜𝑞∗\n𝑡, linear velocity ˜¤x∗\n𝑡, and angular velocity ˜¤𝑞∗\n𝑡. All\nfeatures are expressed in the character’s local frame.\n8.2\nTraining\nAll physics simulations are performed using Isaac Gym, a massively\nparallel GPU-based physics simulator [Makoviychuk et al. 2021].\nThe simulation is performed at a frequency of 120Hz, while the\npolicies operate at a frequency of 30Hz. 4096 environments are\nsimulated in parallel on a single A100 GPU. A 128D latent space\nis used for the skill embedding. The policy, value function, and\ndiscriminator are modeled using separate multi-layer perceptrons\nSA ’22 Conference Papers, December 6–9, 2022, Daegu, Republic of Korea\nJordan Juravsky, Yunrong Guo, Sanja Fidler, and Xue Bin Peng\n(a) \"sprint forwards while swinging arms\".\n(b) \"left shield bash\", \"shield bash left\", \"shield bash to the left while standing still\".\n(c) \"slash right\", \"right swing\", \"swing sword to the right\", \"stand still and slash to the right\".\n(d) task: Location. skill: \"sprint forward while swinging arms\".\n(e) task: Strike. skill: \"shield bash to the right\".\nFigure 4: (a) – (c): Reference motion clips (left side) and their corresponding captions, along with motions produced by a\nsimulated character when directed to perform the reference skills through language commands (right side). More reference\nmotions and policy trajectories are shown in Fig. 7 in the Appendix. (d) – (e): Trained policies completing tasks with different\nskills.\nwith ReLU units and hidden layers containing [1024, 1024, 512]\nunits. Each policy is trained using proximal policy optimization\nwith about 7 billion samples [Schulman et al. 2017], corresponding\nto approximately 7 years of simulated time, which requires about\n2.5 days of real-world time. Selecting a weight 𝜆task for the task\nreward that effectively balances the task and skill reward can be\nchallenging, and may require task-specific tuning. We therefore\napply an adaptive method to dynamically adjust 𝜆task based on a\ntarget task-reward value [Mentzer et al. 2021]. More details are\navailable in Appendix B.4.\n9\nRESULTS\nWe first train policies without auxiliary tasks to evaluate the model’s\nability to reproduce skills from a motion dataset. Examples of the\npolicy’s behaviors when given various skill commands are avail-\nable in Fig. 4. The policy is able to follow a variety of language\ncommands, ranging from locomotion skills, such as walking and\nrunning, to more athletic behaviors, such as sword swings and\nshield bashes. Since the language encoder is built on a large CLIP\nmodel [Radford et al. 2021], it exhibits some robustness to new\ncommands, which were not in the dataset. For example, the model\ncorrectly performs a casual walking motion when prompted with:\n“take a leisurely stroll”, even though no captions in the dataset con-\ntained “leisurely” or phrased walking as “taking a walk”. However,\ndue to the relatively small amount of captions used to train the\nencoder, the model can still produce incorrect behaviors for some\nnew commands. The character successfully performs a right slash\nwhen given the prompt: “right slash”. However, “right slash with\nsword” leads the character to perform a left slash.\nIn addition to learning skills from a motion dataset, our poli-\ncies can also be trained to perform additional high-level tasks, as\noutlined in Section 8.1. Examples of the tasks are available in Fig-\nure 4. Separate policies are trained for each task, which can then\nbe integrated into a single multi-task controller that activates the\nappropriate policy given a task command. We demonstrate the effec-\ntiveness of the multi-task controller in an environment containing\nmultiple objects that the character can interact with. The user can\nissue a task command for specifying the target object and the de-\nsired task that the character should perform. Our multiple-choice\nquestion-answering framework is able to consistently identify the\ncorrect task and target object from a user’s commands. For exam-\nple, given the command: “knock over the blue block”. the selection\nmodel correctly identifies the policy for the Strike task, and selects\nthe blue block as the target. The selection model can also parse\nmore unusual commands, such as \"mosey on down to the maroon\nsaloon\", which correctly identifies the Location task and selects the\nred block. Despite the generalization capabilities of large language\nmodels, some commands can still lead to incorrect behaviors. More\nexamples of task commands and the resulting behaviors from the\nmodel are available in Appendix C.\n9.1\nDataset Coverage\nTo determine the impact of learning a skill embedding that aligns\nmotions and text, we evaluate our model’s ability to reproduce\nPADL: Language-Directed Physics-Based Character Control\nSA ’22 Conference Papers, December 6–9, 2022, Daegu, Republic of Korea\nFigure 5: Comparing dataset coverage when different skill\nencodings are used during the Policy Training stage.\n“Learned Skill Embeddings” use the 128D embedding from\nthe learned motion encoder detailed in Section 5. We com-\npare against baselines where policies are trained directly us-\ning the 512D CLIP text encodings of the dataset captions and\nwhere these encodings are reduced to 128D using PCA.\nvarious motions in the dataset when given the respective com-\nmands. We conduct this evaluation using a thresholded cover-\nage metric. Given a sequence of states specified by a motion clip\nˆm = (ˆs0, ˆs2, ..., ˆs𝑛), a policy trajectory 𝜏= (s0, s2, ..., s𝑘) for a skill\nencoding z = Enc𝑙(𝑐) (where 𝑐is a caption for ˆm), and a threshold\nparameter 𝜖> 0, we define the coverage to be:\ncoverage(𝜏, ˆm,𝑐,𝜖) = 1\n𝑛\n𝑛\n∑︁\n𝑖=0\nI\n\u0012\u0012\nmin\n𝑗∈{0,...,𝑘} ||ˆs𝑖−s𝑗||2\n\u0013\n≤𝜖\n\u0013\n(11)\nThis metric determines the fraction of the states in a motion clip\nthat are sufficiently close to a state in the policy’s trajectory. In our\nexperiments we collect 300 timesteps (10 seconds) per trajectory.\nInstead of selecting a fixed threshold 𝜖, we apply Equation 11 with\ndifferent values of 𝜖between [0, 3] to produce a coverage curve.\nFigure 5 compares the performance of the PADL model with\nbaseline models that directly use the CLIP encoding of a caption\nas input to the policy. Coverage statistics are averaged across all\nthe captions for each motion clip in the dataset, and then averaged\nacross all motion clips. The raw CLIP encoding is 512D, while our\nlearned skill embedding is 128D. We include an additional baseline\nmodel, which uses PCA to reduce the dimensionality of the CLIP\nencoding to 128D. Our learned embedding is able to better repro-\nduce behaviors in the dataset. Directly using the CLIP encoding as\ninput to the policy tends to result in lower quality motions, and has\na higher tendency of performing incorrect behaviors when directed\nwith language commands.\n9.2\nSkill Interpolation\nIn addition to enabling language control, the learned skill embed-\nding also leads to semantically meaningful interpolations between\ndifferent skills. Given two skill commands 𝑐1 and 𝑐2, we encode\neach caption into the corresponding latents z1 and z2 using the lan-\nguage encoder. We then interpolate between the two latents using\nspherical interpolation, and condition the policy on the interpolated\nlatent to produce a trajectory. For example, given two commands:\n“walk forward” and “sprint forward while swinging arms”, interpo-\nlating between the two latents leads to locomotion behaviors that\nFigure 6: Interpolating skills in the latent space leads to se-\nmantically meaningful intermediate behaviors, such as trav-\neling with different walking heights and speeds.\ntravel at different speeds. Figure 6 records the average velocity\nof the character when the policy is conditioned on different in-\nterpolated latents. Similarly, interpolating between “walk forward”\nand “crouching walk forward” leads to gaits with different walk-\ning heights. However, not all pairs of commands lead to intuitive\nintermediate behaviors.\n10\nCONCLUSIONS\nIn this work we presented PADL, a framework for learning language-\ndirected controllers for physics-based character animation. Lan-\nguage is used to specify both high-level tasks that a character should\nperform and low-level skills that the character should use to ac-\ncomplish the tasks. While our models are able to imitate a diverse\narray of skills from motion data, the models remain limited in the\nvariety of high-level tasks that they can perform. We are interested\nin exploring more scalable approaches to modelling character inter-\nactions with the environment, replacing the finite a priori collection\nof tasks with a more general strategy that allows the user to specify\narbitrary environment interactions with natural language. We are\nadditionally interested in scaling PADL to much larger labelled\nmotion capture datasets [Punnakkal et al. 2021], which may lead\nto agents and language encoders that can model a greater diver-\nsity of skills while being more robust to paraphrasing and capable\nof generalizing to new commands. In particular, we expect the\nlanguage encoder from the Skill Embedding stage to improve sig-\nnificantly with more text data. We are excited for further advances\nin language-guided physics-based character animation and hope\nthat our work contributes towards the development of powerful,\nhigh-quality animation tools with broadly accessible, versatile, and\neasy-to-use interfaces.\nSA ’22 Conference Papers, December 6–9, 2022, Daegu, Republic of Korea\nJordan Juravsky, Yunrong Guo, Sanja Fidler, and Xue Bin Peng\nACKNOWLEDGMENTS\nWe would like to thank Reallusion1 for providing motion capture\nreference data for this project. Additionally, we would like to thank\nthe anonymous reviews for their feedback, and Steve Masseroni and\nMargaret Albrecht for their help in producing the supplementary\nvideo.\nREFERENCES\nShailen Agrawal and Michiel van de Panne. 2016. Task-based Locomotion. ACM\nTransactions on Graphics (Proc. SIGGRAPH 2016) 35, 4 (2016).\nC. Ahuja and L. Morency. 2019. Language2Pose: Natural Language Grounded Pose\nForecasting. In 2019 International Conference on 3D Vision (3DV). IEEE Computer\nSociety, Los Alamitos, CA, USA, 719–728. https://doi.org/10.1109/3DV.2019.00084\nSimon Alexanderson, Gustav Eje Henter, Taras Kucherenko, and Jonas Beskow. 2020.\nStyle-Controllable Speech-Driven Gesture Synthesis Using Normalising Flows.\nComputer Graphics Forum (2020). https://doi.org/10.1111/cgf.13946\nKevin Bergamin, Simon Clavet, Daniel Holden, and James Richard Forbes. 2019.\nDReCon: Data-Driven Responsive Control of Physics-Based Characters. ACM\nTrans. Graph. 38, 6, Article 206 (Nov. 2019), 11 pages.\nhttps://doi.org/10.1145/\n3355089.3356536\nMatthew Brand. 1999. Voice Puppetry. In Proceedings of the 26th Annual Conference on\nComputer Graphics and Interactive Techniques (SIGGRAPH ’99). ACM Press/Addison-\nWesley Publishing Co., USA, 21–28. https://doi.org/10.1145/311535.311537\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language Models are Few-Shot Learners. CoRR abs/2005.14165\n(2020). arXiv:2005.14165 https://arxiv.org/abs/2005.14165\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\nand Sergey Zagoruyko. 2020. End-to-End Object Detection with Transformers.\nhttps://doi.org/10.48550/ARXIV.2005.12872\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de\nOliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail\nPavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol,\nAlex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu\nJain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei,\nSam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large\nLanguage Models Trained on Code. CoRR abs/2107.03374 (2021). arXiv:2107.03374\nhttps://arxiv.org/abs/2107.03374\nAlexander Clegg, Wenhao Yu, Jie Tan, C. Karen Liu, and Greg Turk. 2018. Learning to\nDress: Synthesizing Human Dressing Motion via Deep Reinforcement Learning.\nACM Trans. Graph. 37, 6, Article 179 (dec 2018), 10 pages. https://doi.org/10.1145/\n3272127.3275048\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christo-\npher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word\nProblems. https://doi.org/10.48550/ARXIV.2110.14168\nStelian Coros, Philippe Beaudoin, and Michiel van de Panne. 2009. Robust Task-based\nControl Policies for Physics-based Characters. ACM Trans. Graph. (Proc. SIGGRAPH\nAsia) 28, 5 (2009), Article 170.\nMarco da Silva, Yeuhi Abe, and Jovan Popović. 2008. Simulation of Human Motion\nData using Short-Horizon Model-Predictive Control. Computer Graphics Forum 27\n(2008).\nMartin de Lasa, Igor Mordatch, and Aaron Hertzmann. 2010. Feature-Based Locomotion\nControllers. ACM Transactions on Graphics 29, 3 (2010).\nSalil Deena and Aphrodite Galata. 2009. Speech-Driven Facial Animation Using a\nShared Gaussian Process Latent Variable Model. In Proceedings of the 5th Interna-\ntional Symposium on Advances in Visual Computing: Part I (Las Vegas, Nevada)\n(ISVC ’09). Springer-Verlag, Berlin, Heidelberg, 89–100. https://doi.org/10.1007/978-\n3-642-10331-5_9\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nhttps://doi.org/10.48550/ARXIV.1810.04805\n1https://actorcore.reallusion.com/\nDibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, and Sergey Levine.\n2018. Divide-and-Conquer Reinforcement Learning. In International Conference on\nLearning Representations. https://openreview.net/forum?id=rJwelMbR-\nF. Sebastin Grassia. 1998. Practical Parameterization of Rotations Using the Exponential\nMap. J. Graph. Tools 3, 3 (March 1998), 29–48. https://doi.org/10.1080/10867651.\n1998.10487493\nJonathan Ho and Stefano Ermon. 2016. Generative Adversarial Imitation Learning. In\nAdvances in Neural Information Processing Systems, D. Lee, M. Sugiyama, U. Luxburg,\nI. Guyon, and R. Garnett (Eds.), Vol. 29. Curran Associates, Inc. https://proceedings.\nneurips.cc/paper/2016/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf\nJessica K. Hodgins, Wayne L. Wooten, David C. Brogan, and James F. O’Brien. 1995.\nAnimating Human Athletics. In Proceedings of the 22nd Annual Conference on\nComputer Graphics and Interactive Techniques (SIGGRAPH ’95). Association for\nComputing Machinery, New York, NY, USA, 71–78. https://doi.org/10.1145/218380.\n218414\nDaniel Holden, Taku Komura, and Jun Saito. 2017. Phase-Functioned Neural Networks\nfor Character Control. ACM Trans. Graph. 36, 4, Article 42 (jul 2017), 13 pages.\nhttps://doi.org/10.1145/3072959.3073663\nDaniel Holden, Jun Saito, and Taku Komura. 2016. A Deep Learning Framework for\nCharacter Motion Synthesis and Editing. ACM Trans. Graph. 35, 4, Article 138 (jul\n2016), 11 pages. https://doi.org/10.1145/2897824.2925975\nPengyu Hong, Zhen Wen, and T.S. Huang. 2002. Real-time speech-driven face anima-\ntion with expressions using neural networks. IEEE Transactions on Neural Networks\n13, 4 (2002), 916–927. https://doi.org/10.1109/TNN.2002.1021892\nTero Karras, Timo Aila, Samuli Laine, Antti Herva, and Jaakko Lehtinen. 2017. Audio-\nDriven Facial Animation by Joint End-to-End Learning of Pose and Emotion. ACM\nTrans. Graph. 36, 4, Article 94 (jul 2017), 12 pages. https://doi.org/10.1145/3072959.\n3073658\nKyungho Lee, Sehee Min, Sunmin Lee, and Jehee Lee. 2021b. Learning Time-Critical\nResponses for Interactive Character Control. ACM Trans. Graph. 40, 4, Article 147\n(jul 2021), 11 pages. https://doi.org/10.1145/3450626.3459826\nSeyoung Lee, Sunmin Lee, Yongwoo Lee, and Jehee Lee. 2021a. Learning a family of\nmotor skills from a single motion clip. ACM Trans. Graph. 40, 4, Article 93 (2021).\nYoonsang Lee, Sungeun Kim, and Jehee Lee. 2010a. Data-Driven Biped Control. ACM\nTrans. Graph. 29, 4, Article 129 (July 2010), 8 pages. https://doi.org/10.1145/1778765.\n1781155\nYongjoon Lee, Kevin Wampler, Gilbert Bernstein, Jovan Popović, and Zoran Popović.\n2010b. Motion Fields for Interactive Character Locomotion. ACM Trans. Graph. 29,\n6, Article 138 (dec 2010), 8 pages. https://doi.org/10.1145/1882261.1866160\nS. Levine, C. Theobalt, and V. Koltun. 2009. Real-Time Prosody-Driven Synthesis\nof Body Language. ACM Transactions on Graphics 28 (12 2009), 1–10.\nhttps:\n//doi.org/10.1145/1618452.1618518\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi\nLeblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas\nHubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen,\nPo-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy,\nDaniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas,\nKoray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-Level Code Generation\nwith AlphaCode. https://doi.org/10.48550/ARXIV.2203.07814\nAngela S. Lin, Lemeng Wu, Rodolfo Corona, Kevin Tai, Qixing Huang, and Raymond J.\nMooney. 2018. Generating Animated Videos of Human Activities from Natural\nLanguage Descriptions. In Proceedings of the Visually Grounded Interaction and\nLanguage Workshop at NeurIPS 2018. http://www.cs.utexas.edu/users/ai-labpub-\nview.php?PubID=127730\nHung Yu Ling, Fabio Zinno, George Cheng, and Michiel van de Panne. 2020. Character\nControllers Using Motion VAEs. ACM Trans. Graph. 39, 4 (2020).\nLibin Liu and Jessica Hodgins. August 2018. Learning Basketball Dribbling Skills Using\nTrajectory Optimization and Deep Reinforcement Learning. ACM Transactions on\nGraphics 37, 4 (August 2018).\nLibin Liu, Michiel van de Panne, and KangKang Yin. 2016. Guided Learning of Control\nGraphs for Physics-Based Characters. ACM Transactions on Graphics 35, 3 (2016).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly\nOptimized BERT Pretraining Approach. https://doi.org/10.48550/ARXIV.1907.11692\nViktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey,\nMiles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and\nGavriel State. 2021. Isaac Gym: High Performance GPU-Based Physics Simulation\nFor Robot Learning. CoRR abs/2108.10470 (2021). arXiv:2108.10470 https://arxiv.\norg/abs/2108.10470\nFabian Mentzer, Eirikur Agustsson, Johannes Ballé, David Minnen, Nick Johnston, and\nGeorge Toderici. 2021. Neural Video Compression using GANs for Detail Synthesis\nand Propagation. https://doi.org/10.48550/ARXIV.2107.12038\nJosh Merel, Yuval Tassa, Dhruva TB, Sriram Srinivasan, Jay Lemmon, Ziyu Wang,\nGreg Wayne, and Nicolas Heess. 2017. Learning human behaviors from motion\ncapture by adversarial imitation. CoRR abs/1707.02201 (2017). arXiv:1707.02201\nhttp://arxiv.org/abs/1707.02201\nIgor Mordatch, Emanuel Todorov, and Zoran Popović. 2012. Discovery of Complex\nBehaviors through Contact-Invariant Optimization. ACM Trans. Graph. 31, 4, Article\nPADL: Language-Directed Physics-Based Character Control\nSA ’22 Conference Papers, December 6–9, 2022, Daegu, Republic of Korea\n43 (jul 2012), 8 pages. https://doi.org/10.1145/2185520.2185539\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcGrew, Ilya Sutskever, and Mark Chen. 2021. GLIDE: Towards Photorealistic\nImage Generation and Editing with Text-Guided Diffusion Models.\nhttps://doi.\norg/10.48550/ARXIV.2112.10741\nSoohwan Park, Hoseok Ryu, Seyoung Lee, Sunmin Lee, and Jehee Lee. 2019. Learning\nPredict-and-Simulate Policies from Unorganized Human Motion Data. ACM Trans.\nGraph. 38, 6, Article 205 (Nov. 2019), 11 pages.\nhttps://doi.org/10.1145/3355089.\n3356501\nCatherine Pelachaud, Norman Badler, and Mark Steedman. 1996. Generating Facial\nExpressions for Speech. Cognitive Science 20 (03 1996), 1–46. https://doi.org/10.\n1016/S0364-0213(99)80001-9\nXue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. 2018a. Deep-\nMimic: Example-guided Deep Reinforcement Learning of Physics-based Charac-\nter Skills. ACM Trans. Graph. 37, 4, Article 143 (July 2018), 14 pages.\nhttps:\n//doi.org/10.1145/3197517.3201311\nXue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. 2022. ASE:\nLarge-scale Reusable Adversarial Skill Embeddings for Physically Simulated Char-\nacters. ACM Trans. Graph. 41, 4, Article 94 (July 2022).\nXue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Abbeel, and Sergey Levine.\n2018b. SFV: Reinforcement Learning of Physical Skills from Videos. ACM Trans.\nGraph. 37, 6, Article 178 (Nov. 2018), 14 pages.\nXue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. 2021. AMP:\nAdversarial Motion Priors for Stylized Physics-Based Character Control. ACM Trans.\nGraph. 40, 4, Article 1 (July 2021), 15 pages. https://doi.org/10.1145/3450626.3459670\nMatthias Plappert, Christian Mandery, and Tamim Asfour. 2017. Learning a bidirec-\ntional mapping between human whole-body motion and natural language using\ndeep recurrent neural networks. CoRR abs/1705.06400 (2017). arXiv:1705.06400\nhttp://arxiv.org/abs/1705.06400\nNancy Pollard, Jessica Hodgins, Marcia Riley, and Christopher Atkeson. 2002. Adapting\nHuman Motion for the Control of a Humanoid Robot. 2 (04 2002). https://doi.org/\n10.1109/ROBOT.2002.1014737\nAbhinanda R. Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-\nRamirez, and Michael J. Black. 2021. BABEL: Bodies, Action and Behavior with\nEnglish Labels. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern\nRecognition (CVPR). 722–731.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From\nNatural Language Supervision. CoRR abs/2103.00020 (2021). arXiv:2103.00020\nhttps://arxiv.org/abs/2103.00020\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the Limits of Transfer\nLearning with a Unified Text-to-Text Transformer. https://doi.org/10.48550/ARXIV.\n1910.10683\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\nHierarchical Text-Conditional Image Generation with CLIP Latents. https://doi.\norg/10.48550/ARXIV.2204.06125\nSebastian Ruder. 2017. An Overview of Multi-Task Learning in Deep Neural Networks.\nCoRR abs/1706.05098 (2017). arXiv:1706.05098 http://arxiv.org/abs/1706.05098\nAndrei A. Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins,\nJames Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and\nRaia Hadsell. 2015. Policy Distillation. https://doi.org/10.48550/ARXIV.1511.06295\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. CoRR abs/1707.06347 (2017).\narXiv:1707.06347 http://arxiv.org/abs/1707.06347\nSebastian Starke, He Zhang, Taku Komura, and Jun Saito. 2019. Neural State Machine\nfor Character-Scene Interactions. ACM Trans. Graph. 38, 6, Article 209 (nov 2019),\n14 pages. https://doi.org/10.1145/3355089.3356505\nFuwen Tan, Song Feng, and Vicente Ordonez. 2018. Text2Scene: Generating Abstract\nScenes from Textual Descriptions. CoRR abs/1809.01110 (2018). arXiv:1809.01110\nhttp://arxiv.org/abs/1809.01110\nJie Tan, Yuting Gu, C. Karen Liu, and Greg Turk. 2014. Learning Bicycle Stunts. ACM\nTrans. Graph. 33, 4, Article 50 (July 2014), 12 pages. https://doi.org/10.1145/2601097.\n2601121\nGuy Tevet, Brian Gordon, Amir Hertz, Amit H. Bermano, and Daniel Cohen-Or. 2022.\nMotionCLIP: Exposing Human Motion Generation to CLIP Space. https://doi.org/\n10.48550/ARXIV.2203.08063\nAdrien Treuille, Yongjoon Lee, and Zoran Popović. 2007. Near-Optimal Character\nAnimation with Continuous Control. In ACM SIGGRAPH 2007 Papers (San Diego,\nCalifornia) (SIGGRAPH ’07). Association for Computing Machinery, New York, NY,\nUSA, 7–es. https://doi.org/10.1145/1275808.1276386\nJack M. Wang, David J. Fleet, and Aaron Hertzmann. 2009. Optimizing Walking\nControllers. In ACM SIGGRAPH Asia 2009 Papers (Yokohama, Japan) (SIGGRAPH\nAsia ’09). Association for Computing Machinery, New York, NY, USA, Article 168,\n8 pages. https://doi.org/10.1145/1661412.1618514\nJack M. Wang, Samuel R. Hamner, Scott L. Delp, and Vladlen Koltun. 2012. Optimizing\nLocomotion Controllers Using Biologically-Based Actuators and Objectives. ACM\nTrans. Graph. 31, 4, Article 25 (jul 2012), 11 pages. https://doi.org/10.1145/2185520.\n2185521\nTingwu Wang, Yunrong Guo, Maria Shugrina, and Sanja Fidler. 2020. UniCon: Universal\nNeural Controller For Physics-based Character Motion. arXiv:2011.15119 [cs.GR]\nJungdam Won, Deepak Gopinath, and Jessica Hodgins. 2020. A Scalable Approach to\nControl Diverse Behaviors for Physically Simulated Characters. ACM Trans. Graph.\n39, 4, Article 33 (jul 2020), 12 pages. https://doi.org/10.1145/3386569.3392381\nKatsu Yamane, Stuart O. Anderson, and Jessica K. Hodgins. 2010. Controlling humanoid\nrobots with human motion data: Experimental validation. In 2010 10th IEEE-RAS\nInternational Conference on Humanoid Robots. 504–510. https://doi.org/10.1109/\nICHR.2010.5686312\nRi Yu, Hwangpil Park, and Jehee Lee. 2021. Human Dynamics from Monocular Video\nwith Dynamic Camera Movements. ACM Trans. Graph. 40, 6, Article 208 (dec 2021),\n14 pages. https://doi.org/10.1145/3478513.3480504\nY. Yuan, S. Wei, T. Simon, K. Kitani, and J. Saragih. 2021. SimPoE: Simulated Character\nControl for 3D Human Pose Estimation. In 2021 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR). IEEE Computer Society, Los Alamitos, CA,\nUSA, 7155–7165. https://doi.org/10.1109/CVPR46437.2021.00708\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A Large-\nScale Adversarial Dataset for Grounded Commonsense Inference. https://doi.org/\n10.48550/ARXIV.1808.05326\nYunbo Zhang, Wenhao Yu, C. Karen Liu, Charlie Kemp, and Greg Turk. 2020. Learning\nto Manipulate Amorphous Materials. ACM Trans. Graph. 39, 6, Article 189 (nov\n2020), 11 pages. https://doi.org/10.1145/3414685.3417868\nSA ’22 Conference Papers, December 6–9, 2022, Daegu, Republic of Korea\nJordan Juravsky, Yunrong Guo, Sanja Fidler, and Xue Bin Peng\nA\nSTATE AND ACTION REPRESENTATION\nWe evaluate the effectiveness of our framework by training language-\ndirected control policies for a 3D simulated humanoid character.\nThe character is equipped with a sword and shield, similar to the\none used by Peng et al. [2022], with a total of 37 degrees-of-freedom.\nThe character’s state s𝑡is represented by a collection of features that\ndescribes the configuration of the character’s body. The features\ninclude:\n• Height of the root from the ground.\n• Rotation of the root in the character’s local coordinate frame.\n• Local rotation of each joint.\n• Local velocity of each joint.\n• Positions of the hands, feet, sword and shield in the charac-\nter’s local coordinate frame.\nThe root is designated to be the pelvis. The character’s local\ncoordinate frame is defined with the origin located at the character’s\npelvis, and the x-axis aligned along the root link’s facing direction,\nwith the y-axis aligned with the global up vector. The rotation of\neach joint is encoded using two 3D vectors, which represent the\ntangent and normal of the link’s local coordinate frame expressed\nin the link’s parent coordinate frame [Peng et al. 2021]. Each action\na𝑡specifies target rotations for PD controllers positioned at each\njoint. Following Peng et al. [2021], the target rotations for 3D joints\nare specified using a 3D exponential map Grassia [1998].\nB\nTASK DETAILS\nB.1\nFacing Task\nThe facing task reward is given by:\n𝑟task\n𝑡\n= min \u0000d𝑡· d∗\n𝑡, 0.5\u0001\n(12)\nwhere d𝑡is the agent’s facing direction. We threshold the reward,\nwhich creates an optimal “cone\" where the task reward is saturated,\nallowing the agent to deviate slightly from the target heading in\norder to better imitate skills.\nB.2\nLocation Task\nThe location task reward is calculated according to:\n𝑟task\n𝑡\n=\n(\n0.2𝑟pos\n𝑡\n+ 0.8𝑟vel\n𝑡\n||x∗−x||2 > 𝛿pos\n0.8\n||x∗−x||2 ≤𝛿pos\n(13)\nwhere x denotes the position of the character’s root, and 𝑟pos\n𝑡\nen-\ncourages the character to be close to the target:\n𝑟pos\n𝑡\n= exp\n\u0010\n−0.25||x∗−x||2\n2\n\u0011\n,\n(14)\n𝑟vel\n𝑡\nencourages the character to move towards the target. This\nvelocity reward incentivizes the agent to travel speed of at least\n𝛿vel = 0.5 m/s in the direction of the target, and not travel in any\nother direction:\n𝑟vel\n𝑡\n= exp\n\u0010\n−0.25\n\u0010\nmax(𝛿vel −𝑣proj\n𝑡\n, 0) + 0.1𝑣perp\n𝑡\n\u0011\u0011\n(15)\nwhere\n𝑣proj\n𝑡\n= ||projx∗(v𝑡)||2\n(16)\n𝑣perp\n𝑡\n= ||perpx∗(v𝑡)||2\n(17)\ndefine the agent’s velocity in the direction of and tangent to the\ntarget, respectively. We saturate the task reward when the agent\ngets within 𝛿pos = 2m of the target, and terminate the episode\nwhen the block is knocked over to disincentivize the agent simply\nrunning into the block.\nB.3\nStrike Task\nFinally, we have a strike task, where the objective is for the character\nto knock over a target object. The goal g𝑡= ( ˜x∗\n𝑡, ˜¤x∗\n𝑡, ˜𝑞∗\n𝑡, ˜¤𝑞∗\n𝑡) records\nthe target object’s position ˜x∗\n𝑡, rotation ˜𝑞∗\n𝑡, linear velocity ˜¤x∗\n𝑡, and\nangular velocity ˜¤𝑞∗\n𝑡. All features are expressed in the character’s\nlocal coordinate frame. The task-reward is then given by:\n𝑟task\n𝑡\n=\n(\n0.2𝑟pos\n𝑡\n+ 0.8𝑟vel\n𝑡\n+ 0.8𝑟knock\n𝑡\n𝑢∗\n𝑡· 𝑢up ≥0.3\n1.4\n𝑢∗\n𝑡· 𝑢up < 0.3\n(18)\nwhere the knock reward incentivizes the agent to knock over the\nblock:\n𝑟knock\n𝑡\n= 1 −𝑢∗\n𝑡· 𝑢up.\n(19)\nHere, 𝑢up is the global up vector, and 𝑢∗\n𝑡is target object’s local\nup vector expressed in the global coordinate frame. The position\nreward 𝑟pos\n𝑡\nand velocity reward 𝑟vel\n𝑡\nare the same as those used for\nthe location task. The task reward saturates when the block has\nbeen sufficiently tipped over.\nB.4\nAdaptive Task Weight Schedule\nSelecting a weight 𝜆task for the task reward that effectively balances\nthe task and skill reward can be challenging, and can require task-\nspecific tuning. Setting 𝜆task too low can lead to policies that only\nlearn to imitate skills without any regard for the task. Similarly,\nwhen 𝜆task is too high, the policy can learn to perform the task\nusing unnatural behaviors, entirely ignoring the skill command.\nTherefore, instead of using a constant task weight or manually\nconstructing an annealing schedule for 𝜆task, we use a proportional\ncontroller to dynamically adjust 𝜆task over the course of the training\nprocess, in a similar manner as Mentzer et al. [2021]. The controller\nis parameterized by a target task reward ˆ𝑟tar, as well as by a con-\ntroller gain𝑘𝑝and a small positive constant 𝜖for numerical stability.\nAt epoch 𝑖, we calculate the mean task reward 𝑟task\n𝑖\nacross the ex-\nperience buffer. We then update the task weight 𝜆task\n𝑖\naccording to\nthe error between 𝑟task\n𝑖\nand ˆ𝑟tar in log-space:\n𝜆task\n𝑖+1 = exp\n\u0010\nlog\n\u0010\n𝜆task\n𝑖\n\u0011\n+ 𝑘𝑝\n\u0010\nlog\n\u0010\nˆ𝑟tar + 𝜖\n\u0011\n−log\n\u0010\n𝑟task\n𝑖\n+ 𝜖\n\u0011\u0011\u0011\n(20)\nThe task weight is initialized to be 𝜆task\n0\n= 3, and 𝜆task\n𝑖\nis clamped to\nthe range [0.5, 3]. For the location task we set a target task reward\nweight of 0.15, while for the strike task we set a target reward of\n0.3. For the facing task we found the controller to be unnecessary\nand used a constant 𝜆task = 1.\nC\nMULTIPLE-CHOICE MODEL EXAMPLE\nOUTPUTS\nIn Table 1, we provide examples of task commands and the cor-\nresponding object and policy that the multiple-choice QA model\nPADL: Language-Directed Physics-Based Character Control\nSA ’22 Conference Papers, December 6–9, 2022, Daegu, Republic of Korea\n(a) \"forward walk\", \"walk forward while swaying arms\".\n(b) \"sprint forwards while swinging arms\".\n(c) \"kick\", \"kick with right leg\", \"right kick into step forward\", \"right leg kick\".\n(d) \"left shield bash\", \"shield bash left\", \"shield bash to the left while standing still\".\n(e) \"slash right\", \"right swing\", \"swing sword to the right\", \"stand still and slash to the right\".\nFigure 7: Reference motion clips (left side) and their corresponding captions, along with motions produced by a simulated\ncharacter when directed to perform the reference skills through language commands (right side).\nidentified. We observe that the QA model is able to correctly identify\nthe user’s intent even when provided with exotic task commands\nsuch as “destroy the green guy” or “mosey on down to the maroon\nsaloon”. We also provide several examples where the QA model\nincorrectly identifies the task and/or object. For example, the model\npredicts that the task command “go to the blue target” references\nthe strike task instead of the location task, while “go to the blue\nblock” and “go to the blue tower” are correctly identified as the\nlocation task. The QA model is also occasionally sensitive to para-\nphrasing, such as when it correctly identifies the task in “navigate\nto the lime rectangular prism\" but not in “navigate toward the lime\nrectangular prism\".\nD\nCOMPARING PADL TO OTHER\nADVERSARIAL RL FRAMEWORKS\nWhen training PADL agents (detailed in Section 6), the skill ob-\njective explicitly rewards agents for being able to imitate every\nmotion clip in the dataset, using a discriminator trained on the joint\ndistribution of state transitions and skill embeddings. We find that\nthe use of a joint discriminator helps to mitigate mode collapse\nduring PADL training when compared to other work in adversarial\nreinforcement learning that uses discriminators trained only on\nthe marginal distribution of state transitions. Here we specifically\ncompare our method to two related adversarial RL frameworks,\nAMP [Peng et al. 2021] and ASE [Peng et al. 2022].\nD.1\nComparison to AMP\nAMP, like PADL, trains agents using a combination of task and\nskill rewards. However, since AMP’s skill reward uses a marginal\ndiscriminator, mode collapse can occur, where agents focus on\nimitating a specific subset of skills in the reference motion data\nwhile completing the high-level task. PADL’s use of a joint discrim-\ninator in the skill reward, where policies are explicitly trained to\naccomplish the high-level task using different reference skills, can\nimproves a policy’s coverage of the dataset. Moreover, PADL agents,\nunlike AMP agents, are conditioned on a latent variable encoding\nthe skill to be used. This allows a user to control in real-time which\nSA ’22 Conference Papers, December 6–9, 2022, Daegu, Republic of Korea\nJordan Juravsky, Yunrong Guo, Sanja Fidler, and Xue Bin Peng\nTable 1: Example task commands and the corresponding object and task identified by the multiple-choice QA model.\nTask Command\nIdentified Object\nIdentified Task\n\"knock over the blue block\"\n\"the blue object nearby.\" ✓\n\"knock over a specified object.\" ✓\n\"knock over the green block\"\n\"the green object nearby.\" ✓\n\"knock over a specified object.\" ✓\n\"go to the red block\"\n\"the red object nearby.\" ✓\n\"navigate to a specified destination.\"✓\n\"go to the orange block\"\n\"the orange object nearby.\"✓\n\"navigate to a specified destination.\"✓\n\"face the purple block\"\n\"the purple object nearby.\" ✓\n\"orient himself to face a specified heading.\"✓\n\"knock over the purple target\"\n\"the purple object nearby.\" ✓\n\"knock over a specified object.\" ✓\n\"turn towards the blue target\"\n\"the blue object nearby.\" ✓\n\"orient himself to face a specified heading.\"✓\n\"turn towards the orange target\"\n\"the orange object nearby.\" ✓\n\"orient himself to face a specified heading.\"✓\n\"face the orange target\"\n\"the orange object nearby.\" ✓\n\"orient himself to face a specified heading.\"✓\n\"face the purple target\"\n\"the purple object nearby.\" ✓\n\"orient himself to face a specified heading.\"✓\n\"go to the blue target\"\n\"the blue object nearby.\" ✓\n\"knock over a specified object.\" ✗\n\"topple the red tower\"\n\"the red object nearby.\"✓\n\"knock over a specified object.\" ✓\n\"face the orange obelisk\"\n\"the orange object nearby.\" ✓\n\"orient himself to face a specified heading.\" ✓\n\"navigate to the lime rectangular prism\"\n\"the green object nearby.\" ✓\n\"navigate to a specified destination.\" ✓\n\"navigate toward the lime rectangular prism\"\n\"the green object nearby.\" ✓\n\"orient himself to face a specified heading.\" ✗\n\"look at the stop sign\"\n\"the red object nearby.\" ✓\n\"orient himself to face a specified heading.\" ✓\n\"watch the sunset\"\n\"the red object nearby.\" ✓\n\"orient himself to face a specified heading.\" ✓\n\"knock over the cobalt block\"\n\"the red object nearby.\" ✗\n\"knock over a specified object.\" ✓\n\"get close to the violet marker\"\n\"the purple object nearby.\"✓\n\"orient himself to face a specified heading.\" ✗\n\"destroy the green guy\"\n\"the green object nearby.\" ✓\n\"knock over a specified object.\" ✓\n\"mosey on down to the maroon saloon\"\n\"the red object nearby.\"✓\n\"navigate to a specified destination.\" ✓\nskills a trained agent uses to accomplish a task, which is crucial for\neffective language control.\nD.2\nComparison to ASE\nBoth ASE low-level controllers and PADL controllers are condi-\ntioned on skill latents, allowing the skill the agent uses to be dynam-\nically controlled. During ASE training, latents are drawn randomly\nfrom a prior distribution (e.g. the unit sphere); the policy learns a\nmeaningful representation on this latent space throughout train-\ning using a marginal discriminator combined with an encoder that\npromotes high mutual information between a latent and its cor-\nresponding policy trajectory. This approach too can lead to mode\ncollapse, with only a subset of skills from the reference dataset\nbeing represented in the latent space. PADL mitigates this type\nof mode collapse by assigning a distinct motion latent to every\nmotion clip in the reference dataset (these latents are learned in\nthe Skill Embedding stage), guaranteeing that every motion clip is\nrepresented in the latent space.\nIn one of our early experiments developing language-controlled\nanimation systems, we attached a language head on top of an ASE\nlow-level controller. We created a dataset of (latent, caption) pairs by\nsampling latents from the unit sphere, recording trajectories from a\npre-trained controller checkpoint with those latents, and annotating\nthe trajectories with natural language. We then trained a small MLP\nto reverse the annotation process and map the BERT embeddings of\na trajectory’s caption to the corresponding latent that produced the\ntrajectory. This approach allowed for a policy’s skill to be controlled\nwith language, but is annotation inefficient, since each dataset of\n(latent, caption) pairs is only applicable for a specific checkpoint’s\nlearned latent space. A different ASE checkpoint (which possesses a\nALGORITHM 1: Multi-Task Aggregation\nagentState ←agent state;\nwhile not done do\nskillLatent = Encl(getSkillCommand());\npolicyIdx, objectIdx = QAModel(getTaskCommand());\npolicy = policies[policyIdx];\ntargetObjectState = objects[objectIdx];\naction = policy(agentState, skillLatent, targetObjectState);\nagentState = env.step(action);\nend\ndifferent learned latent space) requires the collection of an entirely\nnew dataset of annotations. Moreover, due to mode-collapse, more\ncomplicated skills in the dataset were often not represented in the\npolicy’s latent space.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL",
    "cs.GR"
  ],
  "published": "2023-01-31",
  "updated": "2023-01-31"
}