{
  "id": "http://arxiv.org/abs/2302.00150v1",
  "title": "Multi-Grade Deep Learning",
  "authors": [
    "Yuesheng Xu"
  ],
  "abstract": "The current deep learning model is of a single-grade, that is, it learns a\ndeep neural network by solving a single nonconvex optimization problem. When\nthe layer number of the neural network is large, it is computationally\nchallenging to carry out such a task efficiently. Inspired by the human\neducation process which arranges learning in grades, we propose a multi-grade\nlearning model: We successively solve a number of optimization problems of\nsmall sizes, which are organized in grades, to learn a shallow neural network\nfor each grade. Specifically, the current grade is to learn the leftover from\nthe previous grade. In each of the grades, we learn a shallow neural network\nstacked on the top of the neural network, learned in the previous grades, which\nremains unchanged in training of the current and future grades. By dividing the\ntask of learning a deep neural network into learning several shallow neural\nnetworks, one can alleviate the severity of the nonconvexity of the original\noptimization problem of a large size. When all grades of the learning are\ncompleted, the final neural network learned is a stair-shape neural network,\nwhich is the superposition of networks learned from all grades. Such a model\nenables us to learn a deep neural network much more effectively and\nefficiently. Moreover, multi-grade learning naturally leads to adaptive\nlearning. We prove that in the context of function approximation if the neural\nnetwork generated by a new grade is nontrivial, the optimal error of the grade\nis strictly reduced from the optimal error of the previous grade. Furthermore,\nwe provide several proof-of-concept numerical examples which demonstrate that\nthe proposed multi-grade model outperforms significantly the traditional\nsingle-grade model and is much more robust than the traditional model.",
  "text": "Multi-Grade Deep Learning\nYuesheng Xu∗\nAbstract\nDeep learning requires solving a nonconvex optimization problem of a large size to learn a\ndeep neural network. The current deep learning model is of a single-grade, that is, it learns\na deep neural network by solving a single nonconvex optimization problem. When the layer\nnumber of the neural network is large, it is computationally challenging to carry out such a\ntask eﬃciently. The complexity of the task comes from learning all weight matrices and bias\nvectors from one single nonconvex optimization problem of a large size. Inspired by the human\neducation process which arranges learning in grades, we propose a multi-grade learning model:\nInstead of solving one single optimization problem of a large size, we successively solve a number\nof optimization problems of small sizes, which are organized in grades, to learn a shallow neural\nnetwork for each grade. Speciﬁcally, the current grade is to learn the leftover from the previous\ngrade.\nIn each of the grades, we learn a shallow neural network stacked on the top of the\nneural network, learned in the previous grades, which remains unchanged in training of the\ncurrent and future grades. By dividing the task of learning a deep neural network into learning\nseveral shallow neural networks, one can alleviate the severity of the nonconvexity of the original\noptimization problem of a large size. When all grades of the learning are completed, the ﬁnal\nneural network learned is a stair-shape neural network, which is the superposition of networks\nlearned from all grades. Such a model enables us to learn a deep neural network much more\neﬀectively and eﬃciently. Moreover, multi-grade learning naturally leads to adaptive learning.\nWe prove that in the context of function approximation if the neural network generated by a\nnew grade is nontrivial, the optimal error of the grade is strictly reduced from the optimal error\nof the previous grade. Furthermore, we provide several proof-of-concept numerical examples\nwhich demonstrate that the proposed multi-grade model outperforms signiﬁcantly the traditional\nsingle-grade model and is much more robust than the traditional model.\nKeywords: multi-grade deep learning, deep neural network, adaptive learning\n1\nIntroduction\nThe immense success of deep learning (LeCun et al., 2015; Goodfellow et al., 2016) has been widely\nrecognized. Its great impact to science and technology has been witnessed (H¨aggstr¨om et al., 2019;\nKrizhevsky et al., 2012; Shen et al., 2017; Torlai et al., 2018) and will continue to deepen. From\na mathematical perspective, such successes are mainly due to the powerful expressiveness of deep\nneural networks in representing a function (Daubechies et al., 2022; Shen et al., 2021). The deep\nneural networks are learned by solving optimization problems which determine their parameters\n(weight matrices and bias vectors) that deﬁne them with activation functions. The optimization\nproblems are highly nonconvex and have large numbers of parameters. Solving such optimization\nproblems is challenging. Due to having large numbers of parameters, ﬁnding a global minimizer is\ndiﬃcult. By employing the stochastic gradient descent method (Bottou 1998; Bottou et al., 2012;\n∗Department of Mathematics and Statistics, Old Dominion University, Norfolk, VA 23529, USA. E-mail address:\ny1xu@odu.edu.\n1\narXiv:2302.00150v1  [cs.LG]  1 Feb 2023\nKingma and Ba, 2015) to solve the optimization problems, most likely only local minimizers may\nbe found, since gradient-based optimization starting from random initialization appears to often\nget stuck in poor solutions (Bengio et al., 2007). Moreover, convergence of the iteration is very\nslow. This has been recognized as a major computational obstacle of deep learning.\nThe current model of deep learning uses a single optimization problem to train all parameters\nneeded for the desired deep neural network. We will refer it to as the single-grade model. When\nsolving a single-grade learning model, the more layers the neural network possesses, the severer\nits nonconvexity is, and thus, more diﬃculty we would encounter when trying to solve it. The\nsingle-grade deep learning model is like to teach a middle school student to learn Calculus, who\nhas no required preliminary knowledge such as elementary algebra, trigonometry or college algebra.\nEven if it is not an impossible mission, it is extremely diﬃcult.\nThe goal of this study is to address this challenge. Inspired by human learning process, we\npropose multi-grade deep learning models in which deep neural networks are learned grade-by-grade\nfrom shallow to depth, with redundancy. Instead of solving one single optimization problem with\na big number of parameters, we will solve several optimization problems, each with a signiﬁcantly\nsmaller number of parameters which determine a shallow neural network. After several grades of\nlearning, we build a deep neural network with a structure diﬀerent from the one learned by the\nsingle-grade learning but with a comparable approximation accuracy (or even better). The neural\nnetwork learned from the proposed multi-grade learning model is the superposition of stairs-shape\nneural networks. Since often it is easier to learn several shallow neural networks than a deep one,\nsuch models enable us to learn deep neural networks much more eﬀectively and eﬃciently, avoiding\nsolving one single optimization problem with a large number of parameters.\nThe proposed multi-grade deep learning model imitates the human learning process. In our\nmodern society, human education is often organized in ﬁve large stages, which include elementary\nschool, middle school, high school, college and graduate school. Each of these stages of schooling\nis further divided into diﬀerent grades. For example, elementary school is normally divided into\nﬁve grades and its curriculums are designed according to the grades, with substantial amount of\nredundancy, proceeding from the surface to the inner essence, from an easy level to a sophisticated\nlevel. The knowledge that students had learned in a current grade would serve as a basis to learn\nnew knowledge in the next grade. At the end of each grade, there are examinations to ensure\nthat students do learn what are required to know. The human learning experience shows that the\nmulti-grade learning is eﬀective and eﬃcient.\nAt the technical level, the development of multi-grade learning models is inﬂuenced by the\nmultilevel augmentation method for solving nonlinear system of a large size which results from\ndiscretization of Hammerstein equations, a class of nonlinear integral equations, (Chen et al.,\n2009), see also (Chen et al., 2015).\nSolving a Hammerstein equation by projecting it onto a\npiecewise polynomial subspace that has a multiscale structure including higher levels of resolutions\nboils down to solving a discrete nonlinear system of a large size. The more levels of resolutions\nare included in the subspace, the higher the accuracy of the approximate solution has. However,\nsolving a nonlinear system of a large size is computational expansive. With the help of the multiscale\nanalysis (Chen et al., 1999; Daubechies, 1992; Micchelli and Xu, 1994), we reformulated the process\nof solving the nonlinear system of a large size into two major steps: First, we solve a nonlinear\nsystem which result from projecting the Hammerstein equation onto a subspace that contains only\nlower resolution levels. We then compensate the error by solving a linear system corresponding to\nthe subspace that contains high resolution levels, and this two-step process is repeated. With this\nreformulation, we avoid solving a nonlinear system of a large size, and instead, we solve a nonlinear\nsystem of a small size several times with subsidizing by solving linear systems of large sizes. It\nwas proved in (Chen et al., 2009) that this method generates an approximate solution having the\n2\nsame accuracy order as solving the original nonlinear system of a large size, but with signiﬁcantly\nless computational costs. Inspired by such an idea, we propose in this paper to reformulate the\noptimization problem for learning the entire neural network as many optimization problems for\nlearning parts of a deep neural network. These optimization problems are arranged in grades in the\nway that learning of the current grade is to abstract information from the remainder of learning\nof the previous grade. In each of the grades, we solve an optimization problem of a signiﬁcantly\nsmaller size, which is much easier to solve than the original entire problem. The present grade\nis to learn from the leftover of the previous grade. In fact, according to (Wu et al., 2005; Wu\nand Xu, 2002) under certain conditions, online gradient methods for training a class of nonlinear\nfeedforward shallow neural networks has guaranteed deterministic convergence. The multi-grade\nlearning model takes this advantage by training a number of shallow neural networks which is a\nsubstantially easier task, avoiding training a deep neural network which is a much diﬃcult task.\nA deep neural network learned by a multi-grade learning model diﬀers signiﬁcantly from deep\nneural networks learned by the existing learning models. In a multi-grade learning model, each\ngrade updates the knowledge gained from learning of the previous grades and the total knowledge\nlearned up to the current grade is the accumulation of the knowledge learned in all previous grades\nplus the update in the current grade.\nMathematically, the deep neural network learned up to\nthe current grade is the superposition of all the neural networks learned in all grades so far, (see,\nFigure 3.1 for an illustration). Such a neural network has great redundancy which enhances the\nexpressiveness in presenting data/functions.\nThe multi-grade learning is suitable for adaptive computation. It naturally allows us to add\na new shallow neural network to the neural network learned from the previous grades without\nchanging it. In particular, the multi-grade learning model is natural for learning a solution of an\noperator equation such as a partial diﬀerential equation and an integral equation, especially when\nits solution has certain singularity, since it has a innate posterior error.\nWe organize this paper in seven sections. In section 2, we review the single-grade learning\nmodel.\nSection 3 is devoted to the development of multi-grade models.\nWe then establish in\nsection 4 theoretical results that justify the proposed model. In section 5, we discuss crucial issues\nrelated to the implementation of the proposed multi-grade learning models. In section 6, we provide\nseveral proof-of-concept numerical examples. Finally, we make conclusive remarks in section 7.\n2\nDeep Neural Networks: Single-Grade Learning\nIn this section, we recall the deﬁnition of the standard single-grade deep learning model, and discuss\nits computational challenges.\nMany real world problems require to learn a function from given data. Let s, t ∈N. Suppose\nthat we are given m pairs of points (xi, yi), i ∈Nm := {1, 2, . . . , m}, where xi ∈Rs and yi ∈Rt.\nWe wish to learn from this set of data a function f : Rs →Rt, which represents intrinsic information\nembedded in the data. Deep neural networks furnish us with an excellent representation of the\nfunction due to their gifted expressiveness as a result of their special structure.\nWe ﬁrst recall the deﬁnition of a deep neural network.\nA deep neural network of n layers\ncomposed of n −1 hidden layers and one output layer is constructed with n weight matrices Wk\nand bias vectors bk, k ∈Nn, through a pre-selected activation function σ : R →R. Speciﬁcally,\nwe suppose that d ∈N, and for a vector x := [x1, x2, . . . , xd]⊤∈Rd, we deﬁne the vector-valued\nfunction by the activation function\nσ(x) := [σ(x1), . . . , σ(xd)]⊤.\n(2.1)\n3\nAs in (Xu and Zhang, 2021), for n vector-valued functions fk, k ∈Nn, such that the range of fk is\ncontained in the domain of fk+1, k ∈Nn−1, the consecutive composition of fk, k ∈Nn, is denoted\nby\nn\nK\nk=1\nfk := fn ◦fn−1 ◦· · · ◦f2 ◦f1,\n(2.2)\nwhose domain is that of f1. For Wi ∈Rmi×mi−1, with m0 := s, mn := t and bi ∈Rmi, i ∈Nn, a\ndeep neural network is a function deﬁned by\nNn(x) :=\n \nWn\nn−1\nK\ni=1\nσ(Wi · +bi) + bn\n!\n(x),\nx ∈Rs.\n(2.3)\nThe n-th layer is the output layer. Clearly, Nn : Rs →Rt is a vector-valued function. From (2.3)\nand the deﬁnition (2.1), we have the recursion\nN1(x) := σ(W1x + b1)\n(2.4)\nand\nNk+1(x) = σ(Wk+1Nk(x) + bk+1),\nx ∈Rs,\nfor all k ∈Nn−1.\n(2.5)\nNote that when k := n −1, σ in (2.5) is viewed as the identity map.\nWe now return to learning a function from a given data set. For m pairs of given data points\n(xi, yi), i ∈Nm, one can learn a function\nNn(x) := Nn({W∗\nj, b∗\nj}n\nj=1; x),\nx ∈Rs\n(2.6)\nby solving the parameters {W∗\nj, b∗\nj}n\nj=1 with W∗\nj ∈Rmj×mj−1 and b∗\nj ∈Rmj from the minimization\nproblem\nmin\n( m\nX\nk=1\n∥Nn({Wj, bj}n\nj=1; xk) −yk∥2\nℓ2 : Wj ∈Rmj×mj−1, bj ∈Rmj, j ∈Nn\n)\n,\n(2.7)\nwhere ∥· ∥ℓ2 denotes the Euclidean vector norm of Rt. When we actually solve problem (2.7) we\nmay need to add an appropriate regularization term if overﬁtting occurs. We postpone this issue\nuntil later so that we can concentrate on crucial conceptual issues.\nLearning problem (2.7) has a continuous counterpart in the context of function approximation.\nFor a vector-valued function g := [g1, . . . , gt]⊤: D ⊆Rs →Rt, we deﬁne\n∥g∥:=\n\n\nt\nX\nj=1\n∥gj∥2\n2\n\n\n1\n2\n,\nwhere ∥gj∥2 :=\n\u0012Z\nD\n|gj(x)|2dx\n\u0013 1\n2\n.\nBy L2(D, Rt) we denote the Hilbert space of the functions g : D →Rt with ∥g∥< ∞.\nThe\ninner-product of the space L2(D, Rt) is deﬁned by\n⟨f, g⟩:=\nt\nX\nj=1\nZ\nD\nfj(x)gj(x)dx,\nfor f, g ∈L2(D, Rt).\nBelow, we describe a continuous version of learning problem (2.7). Given a function f ∈L2(D, Rt),\nwe wish to learn a deep neural network Nn in the form of (2.6) by solving {W∗\nj, b∗\nj}n\nj=1 from the\ncontinuous minimization problem\nmin\n\b\n∥f(·) −Nn({Wj, bj}n\nj=1; ·)∥2 : Wj ∈Rmj×mj−1, bj ∈Rmj, j ∈Nn\n\t\n.\n(2.8)\n4\nThe function Nn({Wj, bj}n\nj=1; ·) is called a best approximation from the set Ωn of deep neural\nnetworks having the form (2.3) to f. Note that the set Ωn is a nonconvex closed subset of L2(D, Rt).\nLearning a deep neural network from either discrete data or a continuous function boils down to\nﬁnding the optimal weight matrices and bias vectors by solving minimization problem (2.7) or (2.8).\nThe single-grade learning model learns the parameters of all layers together in one single grade. It\nis like to ask a college student to learn Linear Algebra without any backgrounds in High School\nAlgebra or College Algebra, and learn all of these courses all together. Or image how diﬃcult\nwill be for a college freshman who has no College Algebra or Trigonometry knowledge to learn\nCalculus. Computational challenges in solving these minimization problems come from their severe\nnonconvexity, which is the result of learning all parameters at once, besides their ill-posedness.\nDue to their severe nonconvexity, often their global minimizers cannot be found. Even using the\nmainstream method the stochastic gradient descent (Bottou 1998; Bottou and Bousquet, 2012) in\nsolving minimization problems of this type, it is time consuming. Training a deep neural network\nin high dimensions requires even much more computing time.\nAfter a deep neural network is learned, if we realize that its accuracy is not satisfactory, then\nwe need to train a new deep neural network with more layers. In this case, we have to start it over\nto train the new one in the single-grade learning model. This is not computationally eﬃcient. One\nshould have a model which allows updating the learned neural network to form a new one without\ntraining a complete new neural network. The multi-grade learning model to be proposed in the\nnext section will address these issues.\n3\nMulti-Grade Learning Models\nMotivated by the human education system, we introduce in this section multi-grade learning models\nto learn deep neural networks.\nWe ﬁrst consider learning a neural network that approximates f ∈L2(D, Rt). Instead of learning\nit by the single-grade model (2.8), we organize the “curriculum” in l grades and allow the system\nto learn it in a multi-grade manner. Speciﬁcally, we choose kj ∈N, j ∈Nl so that n−1 = Pl\nj=1 kj.\nFor each kj, we choose a set of matrix widths {mk : k = 0, 1, . . . , kj}, which may be diﬀerent for\ndiﬀerent kj, and mkj = t. For simplicity, our notation does not indicate the dependence of the\nmatrix widths mk on kj.\nWe ﬁrst describe learning in grade 1. The goal of the ﬁrst grade learning is to learn the neural\nnetwork Nk1 that takes the form of (2.3) with n := k1. To this end, we deﬁne the error function of\ngrade 1 by\ne1({Wj, bj}k1\nj=1; x) := f(x) −Nk1({Wj, bj}k1\nj=1; x),\nx ∈Rs,\n(3.1)\nwhere {Wj, bj}k1\nj=1 are parameters to be learned. We ﬁnd the parameters {W∗\n1,j, b∗\n1,j}k1\nj=1 from the\noptimization problem\nmin{∥e1({Wj, bj}k1\nj=1; ·)∥2 : Wj ∈Rmj×mj−1, bj ∈Rmj, j ∈Nk1},\n(3.2)\nwith m0 := s. Once the optimal parameters {W∗\n1,j, b∗\n1,j}k1\nj=1 are learned, we let\nf1(x) = N ∗\nk1(x) := Nk1({W∗\nj, b∗\nj}k1\nj=1; x),\nx ∈Rs,\nwhich is the “knowledge” about f that we have learned in grade 1 and we deﬁne the optimal error\nof grade 1 by setting\ne∗\n1(x) := f(x) −f1(x),\nfor x ∈Rs.\n5\nUsually, ∥e∗\n1∥is not small and thus the learning will continue.\nIn grade 2, we will learn a shallow neural network on the top of N ∗\nk1 from e∗\n1, which is the\nleftover from learning of grade 1. Speciﬁcally, we deﬁne the error function of grade 2 by\ne2({Wj, bj}k2\nj=1; x) := e∗\n1(x) −(Nk2({Wj, bj}k2\nj=1; ·) ◦N ∗\nk1)(x),\nx ∈Rs,\n(3.3)\nwhere N ∗\nk1 has been learned in grade 1 and its parameters will be ﬁxed in learning of the future\ngrades. The shallow neural network Nk2, which has the form (2.3) with n := k2, will be learned in\ngrade 2. We ﬁnd {W∗\n2,j, b∗\n2,j}k1\nj=1 from the optimization problem\nmin{∥e2({Wj, bj}k2\nj=1; ·)∥2 : Wj ∈Rmj×mj−1, bj ∈Rmj, j ∈Nk2},\n(3.4)\nwith m0 := t and mk2 := t, and we let\nf2(x) = (N ∗\nk2 ◦N ∗\nk1)(x) := (Nk2({W∗\n2,j, b∗\n2,j}k1\nj=1; ·) ◦N ∗\nk1)(x),\nx ∈Rs.\nNote that f2 is the newly learned neural network N ∗\nk2 stacked on the top of the neural network N ∗\nk1\nlearned in the previous grade. The optimal error of grade 2 is deﬁned by\ne∗\n2(x) := e∗\n1(x) −f2(x),\nfor x ∈Rs.\nSuppose that the neural networks N ∗\nki of grades i, for i ∈N, have been learned and the optimal\nerror of grade i is given by e∗\ni . We deﬁne the error function of grade i + 1 by\nei+1({Wj, bj}ki+1\nj=1 ; x) := e∗\ni (x) −(Nki+1({Wj, bj}ki+1\nj=1 ; ·) ◦N ∗\nki ◦· · · ◦N ∗\nk1)(x),\nx ∈Rs,\nwhere Nki+1 is a neural network to be learned in grade i+1 and it has the form (2.3) with n := ki+1.\nWe then ﬁnd {W∗\ni+1,j, b∗\ni+1,j}ki+1\nj=1 from the optimization problem\nmin{∥ei+1({Wj, bj}ki+1\nj=1 ; ·)∥2 : Wj ∈Rmj×mj−1, bj ∈Rmj, j ∈Nki+1},\n(3.5)\nwith m0 := t and mki+1 := t, and we let\nfi+1(x) := (N ∗\nki+1 ◦N ∗\nki ◦· · · ◦N ∗\nk1)(x),\nx ∈Rs,\nwhere N ∗\nki+1 := Nki+1({W∗\ni+1,j, b∗\ni+1,j}ki+1\nj=1 ; ·). Clearly, fi+1 is a best approximation from the set\nΩi+1 := {Nki+1({Wj, bj}ki+1\nj=1 ; ·) ◦N ∗\nki ◦· · · ◦N ∗\nk1 : Wj ∈Rmj×mj−1, bj ∈Rmj, j ∈Nki+1}\n(3.6)\nto e∗\ni . Again, fi+1 is the newly learned neural network N ∗\nki+1 stacked on the top of the neural\nnetwork N ∗\nki ◦· · · ◦N ∗\nk1 learned in the previous grades. The optimal error of grade i + 1 is deﬁned\nby\ne∗\ni+1(x) := e∗\ni (x) −fi+1(x),\nfor x ∈Rs.\nFinally, the l grade learning model generates the neural network\nfl :=\nl\nX\ni=1\nfi.\n(3.7)\nUnlike the neural network Nn learned by (2.8), the neural network fl deﬁned by (3.7) learned by the\nl-grade model is the accumulation of the knowledge learned from all the grades. In each grade, the\n6\nsystem learns based on the knowledge gained from learning of the previous grades. Mathematically,\nthe neural network fl is the superposition of all l networks fi, i ∈Nl, learned in l grades, and each\nfi is a shallow network learned in grade i composed with the shallow networks learned from the\nprevious grades. In general, the neural network fl has a stairs-shape. We illustrate a three grade\nlearning model in Figure 3.1. Note that the neural network that results from a three grade learning\nmodel is the sum of three networks: the network (left) learned from grade 1 plus the network\n(center) learned from grade 2, which is stacked on the top of the network learned in grade 1, and\nplus the network (right) learned from grade 3, which is stacked on the top of the network learned\nin grade 2. The parts, bounded by dash lines, of the networks of grades 2 and 3 are the exact copy\nof the networks learned from the previous grades. They keep unchanged in training of a new grade.\nFigure 3.1: Illustration of a three-grade learning model: The stairs-shape network - the superposi-\ntion of three networks.\nFrom the structure of the neural network, we ﬁnd it suitable for adaptive learning since when\nthe norm of the optimal error of the current grade is not within a given tolerance we add a new\ngrade without changing the neural networks learned in the previous grades. In passing, we would\nlike to point out that the neural network learned from the multi-grade learning model has the ﬂavor\nof the adaptive mode decomposition originally introduced in (Huang et al., 1998), see also (Chen\net al., 2006; Xu et al., 2006).\nLearning a function from m pairs of discrete data points {(xi, yi)}m\nj=1 may be conducted in the\nsame way, with the modiﬁcation speciﬁed below. First of all, the error functions are now deﬁned\nby\ne1({Wj, bj}k1\nj=1; xk) := yk −Nk1({Wj, bj}k1\nj=1; xk),\nk ∈Nm,\nthe optimal error of grade 1 by\ne∗\n1(xk) := e1({W∗\nj, b∗\nj}k1\nj=1; xk),\nk ∈Nm,\nand for i = 2, 3, . . . , l,\nei({Wj, bj}ki\nj=1; xk) := e∗\ni−1(xk) −(Nki({Wj, bj}k1\nj=1; ·) ◦N ∗\nki−1 ◦· · · ◦N ∗\nk1)(xk),\nk ∈Nm,\n7\nand the optimal error of grade i by\ne∗\ni (xk) := ei({W∗\nj, b∗\nj}ki\nj=1; xk),\nk ∈Nm.\nThe L2-norm square in the minimization problems (3.2), (3.4) and (3.5) is replaced by the discrete\nforms\n∥e1({Wj, bj}k1\nj=1; ·)∥2\nm :=\nm\nX\nk=1\n∥yk −Nk1({Wj, bj}k1\nj=1; xk)∥2\nℓ2\n(3.8)\nand for i = 2, 3, . . . , l,\n∥ei({Wj, bj}ki\nj=1; ·)∥2\nm :=\nm\nX\nk=1\n∥e∗\ni−1(xk) −(Nki({Wj, bj}k1\nj=1; ·) ◦N ∗\nki−1 ◦· · · ◦N ∗\nk1)(xk)∥2\nℓ2.\n(3.9)\nEverything else remains the same as the function approximation case.\nLearning a solution of an operator equation, such as initial/boundary value problems of lin-\near/nonlinear diﬀerential equations, integral equations and functional equations, can be carried out\nin the same way. Note that learning a solution of nonlinear diﬀerential equations by the single-\ngrade learning model was recently considered by (Raissi 2018; Xu and Zeng, 2023). Let G denote\na nonlinear operator and we consider the operator equation\nG(f) = 0,\n(3.10)\nwhere f : Rs →Rt is a solution to be learned. For an initial value problem or a boundary value\nproblem of a diﬀerential equation, the operator G includes the initial value conditions or/and the\nboundary value conditions.\nWe will learn a solution f of equation (3.10) by the l-grade learning model. We deﬁne the error\nfunction of grade 1 by\nei({Wj, bj}k1\nj=1; ·) := G(Nk1({Wj, bj}k1\nj=1; ·)).\nWe then ﬁnd {W∗\n1,j, b∗\n1,j}k1\nj=1 by solving\nmin\n( m\nX\nk=1\n∥ei({Wj, bj}k1\nj=1; xk))∥2\nℓ2 : Wj ∈Rmj×mj−1, bj ∈Rmj, j ∈Nk1\n)\n,\n(3.11)\nwith m0 := s and mk1 := t, and obtain the knowledge of grade 1 for the solution of equation (3.10),\nwhich has the form\nf1 := N ∗\nk1 = Nk1({W∗\n1,j, b∗\n1,j}k1\nj=1; ·).\n(3.12)\nFor i = 2, 3, . . . , l, we successively deﬁne the error function of grade i by\nei({Wj, bj}ki\nj=1; x) := G\n\n\ni−1\nX\nµ=1\nfµ(x) + (Nki({Wj, bj}ki\nj=1; ·) ◦fi−1 ◦· · · ◦f1)(x)\n\n\nand ﬁnd {W∗\ni,j, b∗\ni,j}ki\nj=1 by solving the minimization problems\nmin\n( m\nX\nk=1\n\r\r\rei({Wj, bj}ki\nj=1; xk)\n\r\r\r\n2\nℓ2 : Wj ∈Rmj×mj−1, bj ∈Rmj, j ∈Nki\n)\n,\n(3.13)\n8\nwith m0 := t and mki := t. We let\nfi := Nki({W∗\ni,j, b∗\ni,j}ki\nj=1; ·) ◦fi−1 ◦· · · ◦f1,\ni = 2, 3, . . . l.\n(3.14)\nWe deﬁne the optimal error of grade i by e∗\ni := ei({W∗\nj, b∗\nj}ki\nj=1; x). The learning stops if ∥e∗\ni ∥ℓ2 is\nwithin a given tolerant error bound or we reach the maximal grade number l. When the l-grade\nlearning process is completed, we obtain the neural network\nfl :=\nl\nX\ni=1\nfi,\n(3.15)\nwhich serves as an approximate solution of the operator equation (3.10).\nThe proposed multi-grade learning is particularly suitable for adaptive solutions of operator\nequations. In this case, we have a nature posterior error ∥e∗\ni ∥ℓ2 to control the stop of learning.\nMoreover, the multi-grade learning model allows us to add a new term fi+1 to fi if ∥e∗\ni ∥ℓ2 is not\nwithin the given tolerance, without starting over to learn the entire neural network.\nThe proposed multi-grade model works for other norms. As an example, we consider classiﬁca-\ntion using the categorical cross entropy loss function (Brownlee 2019). Given data {(xk, yk)}m\nk=1 ⊂\nRs × Nt, where xk are observations and yk are labels, we wish to ﬁnd a classiﬁer C : Rs →Nt such\nthat it correctly classiﬁes the training data pairs and as well as new data points not in the given\ndata set. For a given true label y := [y1, . . . , yt]⊤∈Rr and its prediction ˆy := [ˆy1, . . . , ˆyt]⊤, we\ndeﬁne the individual categorical cross entropy by\nL(y, ˆy) :=\nt\nX\nk=1\nyk ln(ˆyk).\n(3.16)\nThe standard deep learning model for classiﬁcation is to ﬁnd W∗\nj ∈Rmj×mj−1, b∗\nj ∈Rmj, j ∈Nn\nwith m0 := s and mn := t such that\nmin\n(\n−1\nm\nm\nX\nk=1\nL(yk, Nn({Wj, bj}n\nj=1; xk)) : Wj ∈Rmj×mj−1, bj ∈Rmj, j ∈Nn\n)\n,\n(3.17)\nwhere L is deﬁned by (3.16). When the dimension t of the vector-valued prediction function is large,\nsolving the nonconvex optimization problem (3.17) with a large number n of layers is a challenging\ntask if it is not impossible. The proposed multi-grade model can overcome this computational\nchallenge since instead of solving one nonconvex optimization problem of a large size for one deep\nneural network, we solve nonconvex optimization problems of small sizes for several shallow neural\nnetworks.\nWe now describe the l-grade learning model for the classiﬁcation problem.\nWe ﬁrst learn\nW∗\n1,j ∈Rmj×mj−1, b∗\n1,j ∈Rmj, j ∈Nk1 with m0 := s and mk1 := t such that\nmin\n(\n−1\nm\nm\nX\nk=1\nL(yk, Nk1({Wj, bj}k1\nj=1; xk)) : Wj ∈Rmj×mj−1, bj ∈Rmj, j ∈Nk1\n)\n,\n(3.18)\nand deﬁne prediction f1 of grade 1 in the same way as in (3.12). For i = 2, 3, . . . , l, we deﬁne\npossible predictions of grade i by\n˜yi({Wj, bj}ki\nj=1; xk) :=\ni−1\nX\nµ=1\nfµ(xk) + (Nki({Wj, bj}ki\nj=1; ·) ◦fi−1 ◦· · · ◦f1)(xk).\n9\nWe then successively learn W∗\ni,j ∈Rmj×mj−1, b∗\ni,j ∈Rmj, j ∈Nki with m0 = mki := t such that\nmin\n(\n−1\nm\nm\nX\nk=1\nL(yk, ˜yi({Wj, bj}ki\nj=1; xk)) : Wj ∈Rmj×mj−1, bj ∈Rmj, j ∈Nk1\n)\n,\n(3.19)\nand deﬁne prediction fi of grade i by setting\nfi(x) := (Nki({W∗\ni,j, b∗\ni,j}ki\nj=1; ·) ◦fi−1 ◦· · · ◦f1)(x),\nx ∈Rs.\nWhen all l grades of learning are completed, we obtain our prediction fl which has the same form\nas in (3.15).\n4\nAnalysis of the Multi-Grade Learning Model\nWe analyze in this section the proposed multi-grade learning model. The main result is the optimal\nerror of a multi-grade learning model decreases as the number of grades increases. To this end,\nwe ﬁrst present several useful observations regarding a best approximation from a nonconvex set,\nwhich are interesting in their own right.\nLet H denote a Hilbert space and Ωa closed set (not necessarily convex) in H. We wish to\nunderstand a best approximation from Ωto an f ∈H. We ﬁrst present a necessary condition and a\nsuﬃcient condition for an element to be a best approximation. To this end, we deﬁne a star-shape\nset centered at a given g0 ∈Ωby\nΩ(g0) := {g ∈Ω: λg + (1 −λ)g0 ∈Ωfor all λ ∈(0, 1)}.\n(4.1)\nClearly, a point g of Ωis in Ω(g0) if and only if the line segment connecting the two points g0 and\ng is completely located in Ω.\nTheorem 4.1 Suppose that H is a Hilbert space and Ωis a closed set in H. Let f ∈H \\ Ω.\n(i) If g0 ∈Ωis a best approximation from Ωto f, then for all g ∈Ω(g0),\n⟨f −g0, g −g0⟩≤0.\n(4.2)\n(ii) If g0 ∈Ωand (4.2) holds for all g ∈Ω, then g0 is a best approximation from Ωto f.\nProof: (i) Assume that (4.2) does not hold for all g ∈Ω(g0). Then, there exists some g ∈Ω(g0)\nsuch that\n⟨f −g0, g −g0⟩> 0.\nSince g ∈Ω(g0), by the deﬁnition of Ω(g0), we observe that\ngλ := λg + (1 −λ)g0 ∈Ω,\nfor all λ ∈(0, 1).\nHence, we have that\nf −gλ = f −g0 −λ(g −g0).\nThis, with a direct computation, leads to\n∥f −gλ∥2 = ∥f −g0∥2 −λ(2 ⟨f −g0, g −g0⟩−λ∥g −g0∥2).\n10\nUpon choosing λ to satisfy\n0 < λ < min\n\u001a\n1, 2 ⟨f −g0, g −g0⟩\n∥g −g0∥2\n\u001b\n,\nwe obtain that ∥f −gλ∥< ∥f −g0∥. That is, g0 is not a best approximation from Ωto f, a\ncontradiction. The contradiction implies that (4.2) must hold for all g ∈Ω(g0).\n(ii) By hypothesis, for all g ∈Ω, we observe that\n∥f −g0∥2 = ⟨f −g0, f −g⟩+ ⟨f −g0, g −g0⟩\n≤⟨f −g0, f −g⟩\n≤∥f −g0∥∥f −g∥.\nThis implies that\n∥f −g0∥≤∥f −g∥,\nfor all g ∈Ω.\nThat is, g0 is a best approximation from Ωto f.\n2\nNote that in general, Ω(g0) ̸= Ω, and thus, the necessary condition stated in Theorem 4.1 is\nnot the same as the suﬃcient condition. However, under some condition, they can be the same.\nThe following special result is a corollary of Theorem 4.1.\nCorollary 4.2 Suppose that H is a Hilbert space and Ωis a closed set in H. Let f ∈H \\ Ω. If\ng0 ∈Ωsuch that Ω(g0) = Ω, then g0 ∈Ωis a best approximation from Ωto f if and only if\n⟨f −g0, g −g0⟩≤0,\nfor all g ∈Ω(g0).\n(4.3)\nProof: When Ω(g0) = Ω, the two conditions in Items (i) and (ii) of Theorem 4.1 are identical,\nwhich gives a characterization of a best approximation in this special case.\n2\nIn particular, when Ωis a convex set, for any g0 ∈Ω, we have that Ω= Ω(g0). Thus, Corollary\n4.2 reduces to the well-known characterization of a best approximation from a convex set, see, for\nexample, (Deutsch 2001). For this reason, Theorem 4.1 is a natural extension of the characterization\nof a best approximation from a convex set to that from a nonconvex set. In fact, Theorem 4.1 leads\nto a characterization of a best approximation from a nonconvex set, which we present next. For a\ngiven f ∈H and a ﬁxed g0 ∈Ω, we let r0 := ∥f −g0∥and deﬁne the open ball\nB(f, r0) := {g ∈H : ∥f −g∥< r0}.\nMoreover, for a given g0 ∈Ω, we let\n˜Ω(g0) := Ω\\ Ω(g0)\nand\nd(f, ˜Ω(g0)) := inf{∥f −g∥: g ∈˜Ω(g0)}.\nTheorem 4.3 Suppose that H is a Hilbert space and Ωis a closed set in H. Let f ∈H \\ Ωand\ng0 ∈Ω. The following statements are equivalent:\n(i) The element g0 is a best approximation from Ωto f.\n(ii) There holds the equation\nB(f, r0) ∩Ω= ∅.\n(4.4)\n11\n(iii) The element g0 satisﬁes the conditions\n⟨f −g0, g −g0⟩≤0,\nfor all g ∈Ω(g0)\n(4.5)\nand\n∥f −g0∥≤d(f, ˜Ω(g0)).\n(4.6)\nProof: We ﬁrst show that Items (i) and (ii) are equivalent. Suppose that equation (4.4) holds. Let\ng ∈Ωbe arbitrary. Because of (4.4), we ﬁnd that g /∈B(f, r0). Hence, we have that ∥f −g∥≥r0.\nEquivalently, we ﬁnd that\n∥f −g0∥≤∥f −g∥,\nfor all g ∈Ω.\n(4.7)\nThat is, g0 is a best approximation from Ωto f. Conversely, suppose that g0 is a best approximation\nfrom Ωto f. Then, inequality (4.7) holds. This implies that for all g ∈Ω, g /∈B(f, r0). That is,\nequation (4.4) holds.\nWe next show that Items (i) and (iii) are equivalent. Suppose that g0 ∈Ωis a best approx-\nimation from Ωto f. By Item (i) of Theorem 4.1, we have that (4.5). Condition (4.6) follows\nfrom the deﬁnition of g0 being a best approximation from Ωto f and the fact that ˜Ω(g0) ⊆Ω.\nConversely, suppose that g0 ∈Ωsatisﬁes both (4.5) and (4.6). By Item (ii) of Theorem 4.1, g0 is\na best approximation from Ω(g0) to f. That is, we have that\n∥f −g0∥≤∥f −g∥,\nfor all g ∈Ω(g0).\nThis together with condition (4.6) implies that\n∥f −g0∥≤∥f −g∥,\nfor all g ∈Ω:= Ω(g0) ∪˜Ω(g0).\nIn other words, g0 is a best approximation from Ωto f.\n2\nFigure 4.2 illustrates the characterization of a best approximation from a nonconvex set to an\nelement in a Hilbert space. As shown in the ﬁgure, Ωis a nonconvex set of H, f is an element in H,\nnot in Ω, and g0 ∈Ωis a best approximation from Ωto f, the shaded domain is the set Ω(g0), and\n˜Ω(g0) is the rest in Ωnot in Ω(g0). Intuitively, the set Ω(g0) consists of all points in Ωwhich one\ncan “see” from the point g0. Clearly, g0 is a best approximation from Ω(g0) to f and the distance\nof f to ˜Ω(g0) is larger than r0 := ∥f −g0∥. Therefore, g0 is a best approximation from Ωto f.\nFigure 4.2: Projection to a Nonconvex Set\n12\nWe now return to the multi-grade learning model. In the next theorem, we represent the original\nfunction f in terms of the “knowledge” (neural networks) learned in all grades and show that the\nnorms of the optimal errors are nonincreasing.\nTheorem 4.4 Let f ∈L2(D, Rt). The following statements hold:\n(i) For all l ∈N,\nf(x) =\nl\nX\ni=1\n(N ∗\nki ◦N ∗\nki−1 ◦· · · ◦N ∗\nk1)(x) + e∗\nl (x),\nx ∈Rs.\n(4.8)\n(ii) For all i ∈N,\n∥e∗\ni ∥2 ≥∥fi+1∥2 + ∥e∗\ni+1∥2.\n(4.9)\n(iii) For all i ∈N,\n∥e∗\ni+1∥≤∥e∗\ni ∥,\n(4.10)\nand the sequence ∥e∗\ni ∥, i ∈N, has a nonnegative limit.\n(iv) For each i ∈N, either fi+1 = 0 or\n∥e∗\ni+1∥< ∥e∗\ni ∥.\n(4.11)\nProof: (i) By induction on l, we can express the original function f as\nf =\nl\nX\ni=1\nfi + e∗\nl .\n(4.12)\nBy the l-grade learning model, we ﬁnd for each i ∈Nl that\nfi(x) := (N ∗\nki ◦N ∗\nki−1 ◦· · · ◦N ∗\nk1)(x),\nx ∈Rs.\nUpon substituting the equation above into the right-hand-side of equation (4.12), we obtain formula\n(4.8).\n(ii) Note that for all i ∈N, e∗\ni = fi+1 + e∗\ni+1. Hence, we have that\n∥e∗\ni ∥2 =\n\nfi+1 + e∗\ni+1, fi+1 + e∗\ni+1\n\u000b\n.\nExpanding the inner-product on the right-hand-side of the equation above yields that\n∥e∗\ni ∥2 = ∥fi+1∥2 + ∥e∗\ni+1∥2 + 2\n\nfi+1, e∗\ni+1\n\u000b\n.\n(4.13)\nWe next apply Theorem 4.1 with H := L2(D, Rt), f := e∗\ni , g0 := fi+1 and g := 0. According to the\ndeﬁnition (3.6) of Ωi+1, we see that\n0 ∈Ωi+1 and λ0 + (1 −λ)fi+1 ∈Ωi+1,\nfor all λ ∈(0, 1).\nHence, by the deﬁnition (4.1) of set Ωi+1(fi+1) with Ω:= Ωi+1 and g0 := fi+1, we conclude that\n0 ∈Ωi+1(fi+1). By Item (i) of Theorem 4.1, we observe that\n\nfi+1, e∗\ni+1\n\u000b\n= ⟨fi+1, e∗\ni −fi+1⟩≥0.\nThis together with (4.13) establishes Item (ii).\n13\n(iii) Inequality (4.10) follows directly from Item (ii). The existence of the limit of the sequence\n∥e∗\ni ∥, i ∈N, is a direct consequence of its nonincreasingness (4.10) and nonnegativity.\n(iv) If fi+1 ̸= 0, then ∥fi+1∥> 0. By inequality (4.9), we obtain (4.11).\n2\nWe observe from (4.8) that the network fl learned from the l-grade model has much redundancy:\nN ∗\nki appears in the network l −i + 1 times, for i ∈Nl. The redundancy increases signiﬁcantly the\nexpressiveness of the neural network.\nFor multi-grade learning of discrete data, we have results similar to those of Theorem 4.4.\nTheorem 4.5 (i) There holds for l ∈N that\nyk =\nl\nX\ni=1\n(N ∗\nki ◦N ∗\nki−1 ◦· · · ◦N ∗\nk1)(xk) + e∗\nl (xk),\nk ∈Nm.\n(4.14)\n(ii) For all i ∈N,\n∥e∗\ni ∥2\nm ≥∥fi+1∥2\nm + ∥e∗\ni+1∥2\nm.\n(4.15)\n(iii) For all i ∈N,\n∥e∗\ni+1∥m ≤∥e∗\ni ∥m,\n(4.16)\nand the sequence ∥e∗\ni ∥m, i ∈N, has a nonnegative limit.\n(iv) For each i ∈N, either fi+1 = 0 or\n∥e∗\ni+1∥m < ∥e∗\ni ∥m.\n(4.17)\nItem (i) of Theorem 4.5 indicates that for an l-grade learning model, the neural network\nPl\ni=1 N ∗\nki ◦N ∗\nki−1 ◦· · · ◦N ∗\nk1 “interpolates” the data points (xk, yk), for k ∈Nm, up to the er-\nror e∗\nl (xk).\nTheorems 4.4 and 4.5 ensure that every grade of a multi-grade model reduces the global ap-\nproximation error if the network learned from the grade is nontrivial.\n5\nImplementation\nWe discuss in this section several crucial issues in implementing the multi-grade learning model\nproposed in the previous sections. These issues include regularization to overcome over-ﬁtting,\nremoval of output layers for each grade, design of the grades, and the posterior error for adaptive\ncomputation.\nSolving the optimization problems involved in the multi-grade learning models described earlier\nmay suﬀer from over-ﬁtting as solving the one involved in the single-grade learning model does\n(Rice et al., 2020). When over-ﬁtting occurs, regularization may be necessary. One may use the ℓ2\nregularization. We prefer employing the ℓ1 regularization, because it can alleviate over-ﬁtting and\nat the same time promote the sparsity (Liu et al., 2022). Speciﬁcally, we add the vector ℓ1-norm\nof the weight matrix to the minimization problems for our multi-grade learning models. We ﬁrst\nrecall the deﬁnition of the vector ℓ1-norm of a matrix. Given a matrix A := [aij] ∈Rt×s, its vector\nℓ1-norm is deﬁned by\n∥A∥1 :=\nt\nX\ni=1\ns\nX\nj=1\n|aij|,\n14\nthe sum of the absolute values of all entries of the matrix. Note that the vector ℓ1 norm, diﬀerent\nfrom the standard ℓ1 matrix norm, see (Horn and Johnson, 2012), is the ℓ1-norm of a matrix when\nit is treated as a vector in Rts.\nWe next take the minimization problem (3.13) as an example to illustrate how a regularization\nterm is added. Instead of solving minimization problem (3.13), we ﬁnd {W∗\ni,j, b∗\ni,j}ki\nj=1 by solving\nthe following regularization problem\nmin\n\n\n\nm\nX\nk=1\n∥ei({Wj, bj}ki\nj=1; xk)∥2\nℓ2 +\nki\nX\nj=1\nλj∥Wj∥1 : Wj ∈Rmj×mj−1, bj ∈Rmj, j ∈Nki\n\n\n,\n(5.1)\nwhere λj > 0 are the regularization parameters.\nA similar regularization approach was used\nin (Xu and Zeng, 2022) in training deep neural networks. Parameter choice strategies for the ℓ1\nregularization proposed in (Liu et al., 2022) is suitable for the case when the ﬁdelity term is convex.\nOne needs to extend the strategies to treat the regularization problem (5.1) when the ﬁdelity term\nis nonconvex.\nIn the multi-grade learning models described in section 3, each grade has an output layer for\nthe purpose of deﬁning the error function for the grade. We recommend that after the shallow\nneural network of the grade is learned, its output layer be removed before entering the next grade\nof learning. For example, when deﬁning the error function e2({Wj, bj}k2\nj=1; x) of grade 2, instead\nof using (3.3), we employ the modiﬁed error function\n˜e2({Wj, bj}k2\nj=1; x) := e∗\n1(x) −(Nk2({Wj, bj}k2\nj=1; ·) ◦˜\nN ∗\nk1)(x),\nx ∈Rs,\nwhere ˜\nN ∗\nk1 is the shallow neural network of grade 1 after removing the output layer and the column\nsize of Nk2({Wj, bj}k2\nj=1; ·) is changed accordingly. We make the same modiﬁcation for the error\nfunctions of grades i > 2. In this way, the learning will be more eﬃcient.\nProper deﬁning the “grades” is vital to the success of a multi-grade learning model. It depends\non speciﬁc applications. In general, we should take the following factors into consideration: In each\nof the grades, we prefer to training a shallow neural network because it is more eﬃcient to learn\na shallow neural network than a deep one. However, the more grades we have, the more overhead\nwe pay. The overhead includes output layers of the grades. Hence, we need to balance the depth\nof the shallow neural networks and the overhead.\nIn the ﬁrst a few grades, it is not necessary to learn networks of these grades with high accuracy\nsince the leftover will be learned in future grades. If one tries to learn them with high accuracy in\nthe ﬁrst a few grades, one may end up with spending too much times on these grades.\nOne of the advantages of the multi-grade learning is that it is suitable for adaptive computation.\nThe multi-grade learning model naturally allows us to continuously add new grades without starting\nover which a single-grade learning model would do. This requires the availability of a posterior\nerror. For most of learning problems, the error function for each grade can serve as an innate\nposterior error.\nFinally, we comment that one can use more than one activation functions to build a neural\nnetwork. Numerical examples presented in section 6 show the advantages of using diﬀerent activa-\ntion functions in one neural network. Diﬀerent activation functions serve diﬀerent purposes. Using\ndiﬀerent activation functions will not change the theoretical results of Theorems 4.4 and 4.5.\n6\nNumerical Examples\nWe present in this section three proof-of-concept numerical examples to demonstrate the robustness\nof the proposed multi-grade learning model in comparison to the classical single-grade learning\n15\nmodel. We will learn three functions, which are either oscillatory or singular. For each of the\nfunctions, we consider both noise free and noisy cases.\nAll the experiments reported in this section are performed with Python on an Intel Core i7\nCPU with 1.80GHz, 16 Gb RAM and 12 Gb NVIDIA GeForce MX150 GPU.\nBelow, we describe our experiment data for a given function f.\nTraining data: {(xn, yn)}N\nn=1 ⊂[a, b]×R, where N := 5, 000, xn’s are equally spaced on [a, b], and\ngiven xn, the corresponding yn is computed by yn = f(xn) + en. For the noise free cases, en = 0\nand for the noisy cases, en’s are independent and identically distributed Gaussian random variables\nwith mean 0 and standard deviation 0.05.\nTesting data: {(x′\nn, y′\nn)}N′\nn=1 ⊂[a, b] × R, where N′ := 1, 000, x′\nn’s are equally spaced on [a, b] and\ngiven x′\nn, the corresponding y′\nn is computed by y′\nn = f(x′\nn).\nValidation data: Due to the high frequency of the target function, we prefer to have as much\ntraining data as we can. Therefore, instead of leaving out some training data as validation data,\nwe randomly copy 20% of the training data and add Gaussian noise with mean 0 and standard\ndeviation 0.01 to the corresponding y values. The validation data remains the same for both the\nmulti-grade model and the classical single-grade model.\nGiven predictions ˆyn for yn, the mean squared error for training is deﬁned by\nmse (train) := 1\nN\nN\nX\nn=1\n(ˆyn −yn)2\n(6.1)\nand the relative squared error for training is deﬁned by\nrse (train) :=\nPN\nn=1(ˆyn −yn)2\nPN\nn=1 y2n\n.\n(6.2)\nLikewise, suppose that ˆyn is an approximation for y′\nn and we deﬁne the mean squared error and\nthe relative squared error on the testing data respectively by\nmse (test) := 1\nN′\nN′\nX\nn=1\n(ˆyn −y′\nn)2\n(6.3)\nand\nrse (test) :=\nPN′\nn=1(ˆyn −y′\nn)2\nPN′\nn=1(y′n)2\n.\n(6.4)\nFor the three examples, both the noise free and noisy cases, each grade of the multi-grade model\nis trained with the Adam optimizer and with batch size 32 and a learning rate decay 1.0 × 10−2.\nFor the single-grade learning model, we use three diﬀerent epochs in training for diﬀerent examples,\naiming at obtaining the best outcome for each individual example. The epochs will be speciﬁed in\neach example. The training time reported in this section is measured in seconds.\nExample 1: We consider approximating the oscillatory function\nf(x) = sin(100x), x ∈[a, b] := [0, 1].\nFor this example, we use the following three grade model:\nGrade 1 : [1] →[256] →[256] →[1],\nGrade 2 : [1] →[256]F →[256]F →[128] →[128] →[64] →[1],\nGrade 3 : [1] →[256]F →[256]F →[128]F →[128]F →[64]F →[64] →[32] →[32] →[1].\n16\nHere, [n] indicates a fully connected layer with n neurons, and [n]F indicates [n] with all parameters\nﬁxed during training. We choose the activation function for hidden layers in the ﬁrst grade to be\nthe sin function and that for the remaining hidden layers to be the ReLU function. No activation\nfunction is applied to the last layer of each grade.\nFigure 6.3: Example 1 (noise free): The training, testing and validation data.\nThe corresponding classic single-grade architecture is given by\n[1] →[256] →[256] →[128] →[128] →[64] →[64] →[32] →[32] →[1],\nin which the activation function for the ﬁrst two hidden layers is the sin function and that for the\nremaining hidden layers is the ReLU function, the same as in the multi-grade model. No activation\nis applied to the last layer. The classical single-grade network is trained with Adam: learning rate\n0.01 with decay 1.0 × 10−2 and batch size 32 for 550 epochs.\nTable I: Example 1 (noise free): Training time and accuracy for each grade.\ngrade\nlearning rate\nepochs\ntraining time\nmse (train)\nrse (train)\n1\n0.1\n5\n4.4092\n1.7961 × 10−2\n3.5771 × 10−2\n2\n0.01\n20\n14.5209\n1.1362 × 10−4\n2.2629 × 10−4\n3\n0.01\n40\n30.0601\n2.0203 × 10−5\n4.0237 × 10−5\nThe training, testing and the validation data for the noise free case of Example 1 are plotted in\nFigure 6.3. Numerical results for this case are listed in Tables I and II. The corresponding ﬁgures\nare plotted in Figures 6.4 and 6.5.\nTable II: Example 1 (noise free): multi-grade vs single-grade.\nmethod\ntraining time\nmse (train)\nrse (train)\nmse (test)\nrse (test)\nmulti-grade\n48.9902\n2.0203 × 10−5\n4.0237 × 10−5\n2.1450 × 10−5\n4.2746 × 10−5\nsingle-grade\n485.7447\n3.1399 × 10−1\n6.2534 × 10−1\n3.1374 × 10−1\n6.2522 × 10−1\nThe training, testing and the validation data for the noisy case of Example 1 are plotted in\nFigure 6.6. Numerical results for this case are listed in Tables III and IV, and the corresponding\nﬁgures are plotted in Figures 6.7 and 6.8.\nExample 2: We consider approximating the oscillatory function with a variable magnitude\nf(x) := xsin(100x),\nx ∈[a, b] := [0, 1].\n17\nFigure 6.4: Example 1 (noise free): Loss curves during training: (a)-(c) multi-grade; (d) single-\ngrade.\nFigure 6.5: Example 1 (noise free) - Predictions on the testing set: (a) after each grade of multi-\ngrade; (b) multi-grade; (c) single-grade.\n18\nFigure 6.6: Example 1 (noisy case): The training, testing and the validation data.\nFigure 6.7: Example 1 (noisy case): Loss curves during training. (a)-(c) multi-grade; (d) single-\ngrade.\n19\nFigure 6.8: Example 1 (noisy case): Predictions on the testing set. (a) after each grade of multi-\ngrade; (b) multi-grade; (c) single-grade.\nFigure 6.9: Example 2 (noise free): The training, testing and validation data.\n20\nTable III: Example 1 (noisy case): Training time and accuracy for each grade.\ngrade\nlearning rate\nepochs\ntraining time\nmse (train)\nrse (train)\n1\n0.1\n5\n4.3172\n4.3381 × 10−2\n8.6013 × 10−2\n2\n0.01\n20\n13.9507\n2.5290 × 10−3\n5.0143 × 10−2\n3\n0.01\n40\n29.8275\n2.4058 × 10−3\n4.7701 × 10−2\nTable IV: Example 1 (noisy case): multi-grade vs single-grade.\nmethod\ntraining time\nmse (train)\nrse (train)\nmse (test)\nrse (test)\nmulti-grade\n48.0953\n2.4058 × 10−3\n4.7701 × 10−3\n1.1476 × 10−4\n2.2869 × 10−4\nsingle-grade\n474.8579\n3.1994 × 10−1\n6.3436 × 10−1\n3.1720 × 10−1\n6.3213 × 10−1\nFor this example, we use a three grade model:\nGrade 1 : [1] →[256] →[256] →[1],\nGrade 2 : [1] →[256]F →[256]F →[128] →[64] →[64] →[1],\nGrade 3 : [1] →[256]F →[256]F →[128]F →[64]F →[64]F →[64] →[32] →[32] →[1].\nThe activation function for hidden layers in the ﬁrst grade is the sin function and the activation\nfunction for the rest of hidden layers is the ReLU function. No activation function is applied to the\nlast layer of each grade. The corresponding classic architecture is given by\n[1] →[256] →[256] →[128] →[64] →[64] →[64] →[32] →[32] →[1],\nin which the activation function for the ﬁrst two hidden layers is the sin function and the activation\nfor the rest of hidden layers is the ReLU function. No activation is applied to the last layer. The\nsingle-grade network is trained with Adam: learning rate 0.01 with decay 1.0×10−2 and batch size\n32 for 800 epochs.\nFor the noise free case of Example 2, the training, testing and the validation data are plotted\nin Figure 6.9. Numerical results for this case are listed in Tables V and VI, and the corresponding\nﬁgures are plotted in Figures 6.10 and 6.11.\nTable V: Example 2 (noise free): Training time and accuracy for each grade.\ngrade\nlearning rate\nepochs\ntraining time\nmse (train)\nrse (train)\n1\n0.1\n5\n4.7529\n3.1325 × 10−2\n1.8554 × 10−1\n2\n0.01\n20\n15.1325\n1.1796 × 10−5\n6.9869 × 10−5\n3\n0.01\n40\n39.5186\n3.8516 × 10−6\n2.2813 × 10−5\nFor the noisy case of Example 2, the training, testing and the validation data are plotted in\nFigure 6.12. Numerical results for this case are listed in Tables VII and VIII, and the corresponding\nﬁgures are plotted in Figures 6.13 and 6.14.\nExample 3: In this example, we consider a non-diﬀerentiable function\nf(x) := (x + 1)(f4 ◦f3 ◦f2 ◦f1)(x), x ∈[a, b] := [−1, 1],\n(6.5)\nwhere (f ◦g)(x) means the composition f(g(x)) assuming the image of g lies in the domain of f,\n21\nFigure 6.10: Example 2 (noise free) - Loss curves during training: (a)-(c) multi-grade; (d) single-\ngrade.\nFigure 6.11: Example 2 (noise free) - Predictions on the testing set: (a) after each grade of multi-\ngrade; (b) multi-grade; (c) single-grade.\n22\nFigure 6.12: Example 2 (noisy case): The training, testing and validation data.\nFigure 6.13: Example 2 - Loss curves during training: (a)-(c) multi-grade; (d) single-grade.\n23\nTable VI: Example 2 (noise free): multi-grade vs single-grade.\nmethod\ntraining time\nmse (train)\nrse (train)\nmse (test)\nrse (test)\nmulti-grade\n59.4039\n3.8516 × 10−6\n2.2813 × 10−5\n3.9175 × 10−6\n2.3210 × 10−5\nsingle-grade\n734.3698\n7.7438 × 10−3\n4.5867 × 10−2\n7.7376 × 10−3\n4.5842 × 10−2\nTable VII: Example 2 (noisy case): Training time and accuracy for each grade.\ngrade\nlearning rate\nepochs\ntraining time\nmse (train)\nrse (train)\n1\n0.1\n5\n2.9456\n9.4412 × 10−3\n5.5099 × 10−2\n2\n0.01\n20\n10.7950\n2.5191 × 10−3\n1.4701 × 10−2\n3\n0.01\n40\n24.0130\n2.4562 × 10−3\n1.4334 × 10−2\nand fk’s are given by\nf1(x) := |cos(π(x −0.3)) −0.7|,\nf2(x) := |cos(2π(x −0.5)) −0.5|,\nf3(x) := −|x −1.3| + 1.3,\nf4(x) := −|x −0.9| + 0.9.\nFor this example, we use the following three grades:\nGrade 1 : [1] →[128] →[128] →[1],\nGrade 2 : [1] →[128]F →[128]F →[64] →[64] →[64] →[1],\nGrade 4 : [1] →[128]F →[128]F →[64]F →[64]F →[64]F →[32] →[32] →[32] →[1].\nThe activation function for hidden layers in the ﬁrst grade is the sin function and the activation\nfunction for the rest of hidden layers is the ReLU function. No activation function is applied to the\nlast layer of each grade. The corresponding classic architecture is given by\n[1] →[128] →[128] →[64] →[64] →[64] →[32] →[32] →[32] →[1],\nin which the activation function for the ﬁrst two hidden layers is the sin function and the activation\nfor the rest of hidden layers is the ReLU function. No activation is applied to the last layer. The\nsingle-grade network is trained with Adam: learning rate 0.01 with decay 1.0×10−2 and batch size\n32 for 500 epochs.\nFor the noise free case of Example 3, the training, testing and the validation data are plotted\nin Figure 6.15. Numerical results for this case are listed in Tables IX and X. The corresponding\nﬁgures are plotted in Figures 6.16 and 6.17.\nThe training, testing and the validation data for the noisy case of Example 3 are plotted in\nFigure 6.18. Numerical results for this case are listed in Tables XI and XII, and the corresponding\nﬁgures are plotted in Figures 6.19 and 6.20.\nWe observe from the numerical results presented in this section that for all examples (for both\nnoise free and noisy cases) the multi-grade model outperforms signiﬁcantly the single-grade model\nin terms of training time, training accuracy and prediction accuracy in all counts.\nMoreover,\nmulti-grade models are much easier to train than their corresponding single-grade models.\n7\nConclusive Remarks\nTo address computational challenges in learning deep neural networks, we have proposed multi-\ngrade learning models, inspired by the human education process which arranges learning in grades.\n24\nFigure 6.14: Example 2 - Predictions on the testing set: (a) after each grade of multi-grade; (b)\nmulti-grade; (c) single-grade.\nFigure 6.15: Example 3 (noise free): The training, testing and the validation data.\n25\nFigure 6.16: Example 3 (noise free) - Loss curves during training: (a)-(c) multi-grade; (d) single-\ngrade.\nFigure 6.17: Example 3 (noise free) - Predictions on the testing set: (a) after each grade of multi-\ngrade; (b) multi-grade; (c) single-gade.\n26\nFigure 6.18: Example 3 (noisy case): The training, testing and the validation data.\nFigure 6.19: Example 3 (noisy case) - Loss curves during training: (a)-(c) multi-grade; (d) single-\ngrade.\n27\nTable VIII: Example 2 (noisy case): multi-grade vs single-grade.\nmethod\ntraining time\nmse (train)\nrse (train)\nmse (test)\nrse (test)\nmulti-grade\n37.7535\n2.4562 × 10−3\n1.4334 × 10−2\n9.8123 × 10−5\n5.8133 × 10−4\nsingle-grade\n684.0808\n1.3066 × 10−2\n7.6253 × 10−2\n1.0483 × 10−2\n6.2108 × 10−2\nTable IX: Example 3 (noise free): Training time and accuracy for each grade.\ngrade\nlearning rate\nepochs\ntraining time\nmse (train)\nrse (train)\n1\n0.1\n5\n3.9165\n2.9048 × 10−3\n6.9275 × 10−3\n2\n0.01\n10\n6.7180\n8.8397 × 10−6\n2.1081 × 10−5\n3\n0.01\n40\n26.5470\n2.8498 × 10−6\n6.7963 × 10−6\nUnlike the deep neural network learned from the standard single-grade model, the neural network\nlearned from a multi-grade model is the superposition of the neural networks, in a stair-shape, each\nof which is learned from one grade of the learning. We have proved that for function approximation\nif the neural network generated by a new grade is nontrivial, the optimal error of the grade is\nstrictly reduced from the optimal error of the previous grade. Three proof-of-concept numerical\nexamples presented in the paper demonstrate that the proposed the multi-grade deep learning model\noutperforms signiﬁcantly the single-grade deep learning model in all aspects, including training\ntime, training accuracy and prediction accuracy. Our computational experiences show that the\nmulti-grade model is much easier to train than the corresponding single-grade model. We conclude\nthat the proposed multi-grade model is much more robust than the traditional single-grade model.\nAcknowledgement: The author is indebted to graduate student Mr. Lei Huang for his assistance\nin preparation of the numerical examples presented in section 6 and to Dr.\nTingting Wu for\nhelpful discussion of issues related to approximation of the oscillatory function. The author of this\npaper is supported in part by US National Science Foundation under grants DMS-1912958 and\nDMS-2208386, and by the US National Institutes of Health under grant R21CA263876.\nReferences\nY. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, Greedy layer-wise training of deep\nnetworks, in “Advances in Neural Information Processing Systems,” 2007, pp. 153–160.\nL. Bottou,\nOnline algorithms and stochastic approximations,\nin “Online Learning\nand Neural Networks”, Cambridge University Press, 1998. ISBN 978-0-521-65263-6.\nL. Bottou and O. Bousquet, The tradeoﬀs of large scale learning, in “Optimization\nfor Machine Learning”, S. Sra, S. Nowozin, S. J. Wright, (eds.), MIT Press, Cambridge,\n2012, pp. 351–368.\nJ. Brownlee, Probability for machine learning:\nDiscover how to harness uncertainty\nwith Python, Machine Learning Mastery, 2019.\nQ. Chen, N. Huang, S. Riemenschneider and Y. Xu, A B-spline approach for empiri-\ncal mode decompositions, Advances in Computational Mathematics 24 (1-4) (2006), 171-195\n28\nTable X: Example 3 (noise free): multi-grade vs single-grade.\nmethod\ntraining time\nmse (train)\nrse (train)\nmse (test)\nrse (test)\nmulti-grade\n37.1816\n2.8498 × 10−6\n6.7963 × 10−6\n3.0007 × 10−6\n7.1605 × 10−6\nsingle-grade\n377.6011\n1.3259 × 10−2\n3.1620 × 10−2\n1.3243 × 10−2\n3.1602 × 10−2\nTable XI: Example 3 (noisy case): Training time and accuracy for each grade.\ngrade\nlearning rate\nepochs\ntraining time\nmse (train)\nrse (train)\n1\n0.1\n5\n4.0153\n2.8773 × 10−2\n6.8177 × 10−2\n2\n0.01\n10\n6.8856\n2.6177 × 10−3\n6.2027 × 10−3\n3\n0.01\n25\n16.3114\n2.5108 × 10−3\n5.9495 × 10−3\nZ. Chen, C. A. Micchelli and Y. Xu, A construction of interpolating wavelets on in-\nvariant sets, Mathematics of Computation 68 (228) (1999), 1569-1587.\nZ. Chen, C. A. Micchelli and Y. Xu, Multiscale Methods for Fredholm Integral Equa-\ntions, Cambridge University Press, New York, 2015.\nZ. Chen, B. Wu and Y. Xu, Fast multilevel augmentation methods for solving Ham-\nmerstein equations, SIAM Journal on Numerical Analysis 47 (3) (2009), 2321-2346.\nI. Daubechies, Ten Lectures on Wavelets, SIAM, Philadelphia, 1992.\nI. Daubechies, R. DeVore, S. Foucart, B. Hanin, and G. Petrova, Nonlinear approxi-\nmation and (deep) ReLU networks, Constructive Approximation 55 (2022), 127–172.\nF. Deutsch, Best Approximation in Inner Product Spaces, Springer-Verlag, New York,\n2001.\nI. Goodfellow, Y. Bengio, and A. Courville, Deep Learning, MIT Press, Cambridge,\n2016.\nI. H¨aggstr¨om, C. R. Schmidtlein, G. Campanella, T. J. Fuchs, DeepPET: A deep en-\ncoder–decoder network for directly solving the PET image reconstruction inverse problem,\nMedical Image Analysis 54 (2019) 253-262.\nR. A. Horn and C. R. Johnson,\nMatrix Analysis, 2nd Edition, Cambridge Univer-\nsity Press, New York, 2012.\nN. E. Huang, Z. Shen, S. R. Long, M. C. Wu, H. H. Shih, Q. Zheng, N.-C. Yen, C.\nC. Tung, H. H. Liu, The empirical mode decomposition and the Hilbert spectrum for\nnonlinear and non-stationary time series analysis, Proc. Roy. Soc. London A 454 (1998),\n903-995.\nD. P. Kingma and J. L. Ba, ADAM: A method for stochastic optimization, Pub-\nlished as a conference paper at ICLR 2015.\n29\nTable XII: Example 3 (noisy case): multi-grade vs single-grade.\nmethod\ntraining time\nmse (train)\nrse (train)\nmse (test)\nrse (test)\nmulti-grade\n27.2122\n2.5108 × 10−3\n5.9495 × 10−3\n1.5818 × 10−4\n3.7746 × 10−4\nsingle-grade\n406.6714\n1.8728 × 10−2\n4.4376 × 10−2\n1.6499 × 10−2\n3.9372 × 10−2\nFigure 6.20: Example 3 (noisy case) - Predictions on the testing set: (a) after each grade of multi-\ngrade; (b) multi-grade; (c) single-grade.\nA. Krizhevsky, I. Sutskever, and G. Hinton, ImageNet classiﬁcation with deep convolutional\nneural networks, Neural Information Processing Systems (NIPS), Lake Tahoe, Nevada, 2012.\nY. LeCun, Y. Bengio, and G. Hinton, Deep learning, Nature 521 (2015), no. 7553,\n436–444, 2015.\nQ. Liu, R. Wang, Y. Xu and M. Yan, Parameter choices for sparse regularization\nwith the ℓ1 norm, Inverse Problems 39 (2023) 025004 (34pp)\nC. A. Micchelli and Y. Xu, Using the matrix reﬁnement equation for the construc-\ntion of wavelets on invariant sets, Applied and Computational Harmonic Analysis 1 (4)\n(1994), 391-401.\nM. Raissi, Deep hidden physics models:\nDeep learning of nonlinear partial diﬀeren-\ntial equations, Journal of Machine Learning Research 19 (2018) 1-24.\nL. Rice, E. Wong, and J. Z. Kolter,\nOverﬁtting in adversarially robust deep learn-\n30\ning, in “Proceedings of the 37 th International Conference on Machine Learning,” Vienna,\nAustria, PMLR 119, 2020.\nD. Shen, G. Wu, and H. Suk, Deep learning in medical image analysis, Annu Rev\nBiomed Eng. 19 (2017) 221-248.\nZ. Shen, H. Yang, and S. Zhang, Deep network with approximation error being recip-\nrocal of width to power of square root of depth, Neural Comput. 33 (2021), no. 4, 1005–1036.\nG. Torlai, G. Mazzola, J. Carrasquilla, M. Troyer, R. Melko, G. Carleo, Neural-network\nquantum state tomography, Nature Physics 14 (5) (2018), 447–450.\nW. Wu, G. Feng, Z. Li, and Y. Xu, Deterministic convergence of an online gradient\nmethod for BP neural networks, IEEE Transactions on Neural Networks 16 (3) (2005),\n533-540.\nW. Wu and Y. Xu, Deterministic convergence of an online gradient method for neu-\nral networks, Journal of Computational and Applied Mathematics 144 (1-2) (2002), 335-347.\nY. Xu, B. Liu, J. Liu, and S. Riemenschneider, Two-dimensional empirical mode de-\ncomposition by ﬁnite elements, Proceedings of the Royal Society A: Mathematical, Physical\nand Engineering Sciences 462 (2006), 3081-3096.\nY. Xu and T. Zeng,\nSparse deep neural network for nonlinear partial diﬀerential\nequations, Numer. Math. Theor. Meth. Appl. 16 (2023), 58-78.\nY. Xu and H. Zhang, Convergence of deep ReLU networks, arXiv preprint arXiv:2107.12530.\nY. Xu and H. Zhang,\nConvergence of deep convolutional neural networks,\nNeural\nNetworks, 153 (2022), 553–563.\n31\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-02-01",
  "updated": "2023-02-01"
}