{
  "id": "http://arxiv.org/abs/2302.00842v3",
  "title": "Effective Random Test Generation for Deep Learning Compilers",
  "authors": [
    "Luyao Ren",
    "ZiHeng Wang",
    "Yingfei Xiong",
    "Li Zhang",
    "Guoyue Jiang",
    "Tao Xie"
  ],
  "abstract": "Deep learning compilers help address the difficulties of deploying deep\nlearning models on diverse types of hardware. Testing deep learning compilers\nis highly crucial, because they are impacting countless AI applications that\nuse them for model optimization and deployment. To test deep learning\ncompilers, random testing, the testing method popularly used for compiler\ntesting practices, faces the challenge of generating semantically valid test\ninputs, i.e., deep learning models that satisfy the semantic model\nspecifications (in short as semantic specifications). To tackle this challenge,\nin this paper, we propose a novel approach named Isra, including a\ndomain-specific constraint solver that resolves the constraints from the\nsemantic specifications without backtracking. We implement and apply our\napproach to three popular real-world deep learning compilers including TVM,\nGlow, and a commercial compiler named SophGo. The evaluation results show that\nIsra is more effective than the state-of-the-art approaches and the baseline\napproaches on constructing valid test inputs for compiler-bug detection, and\nIsra successfully finds 24 previously unknown bugs in released versions of the\nthree compilers. These results indicate Isra's effectiveness and practical\nvalue.",
  "text": "Effective Random Test Generation for Deep\nLearning Compilers\nLuyao Ren, Ziheng Wang, Yingfei Xiong\nPeking University\nLi Zhang, Guoyue Jiang\nSophgo Technologies Ltd\nTao Xie\nPeking University\nAbstract—Deep learning compilers help address the difficulties\nof deploying deep learning models on diverse types of hardware.\nTesting deep learning compilers is highly crucial, because they\nare impacting countless AI applications that use them for model\noptimization and deployment. To test deep learning compilers,\nrandom testing, the testing method popularly used for compiler\ntesting practices, faces the challenge of generating semantically\nvalid test inputs, i.e., deep learning models that satisfy the seman-\ntic model specifications (in short as semantic specifications). To\ntackle this challenge, in this paper, we propose a novel approach\nnamed Isra, including a domain-specific constraint solver that\nresolves the constraints from the semantic specifications without\nbacktracking. We implement and apply our approach to three\npopular real-world deep learning compilers including TVM,\nGlow, and a commercial compiler named SophGo. The evaluation\nresults show that Isra is more effective than the state-of-the-art\napproaches and the baseline approaches on constructing valid\ntest inputs for compiler-bug detection, and Isra successfully finds\n24 previously unknown bugs in released versions of the three\ncompilers. These results indicate Isra’s effectiveness and practical\nvalue.\nIndex Terms—random testing, test generation, deep learning\ncompilers, compiler testing, constraint solving\nI. INTRODUCTION\nIn recent years, deep learning has been widely used in\nsoftware systems from various domains, such as autonomous\ndriving, e-commerce, and smart cities. Given that deep learn-\ning models become increasingly large and complicated, there\nare emerging needs to deploy versatile deep learning models\non different types of hardware such as GPU, FPGA, and\nTPU [18]. To reduce the burden of optimizing deep learning\nmodels and to address the difficulty of model deployment on\nhardware, deep learning compilers have been developed, such\nas TVM [4], Glow [35], and XLA [20]. These deep learning\ncompilers have been widely used for the optimization and\ndeployment of deep learning models, especially those with\ncritical performance requirements.\nTesting a deep learning compiler is vital for two main rea-\nsons. First, if a deep learning compiler used by AI applications\ncontains bugs, the deployed AI applications can exhibit serious\nfailing behaviors. For example, a critical bug of TVM’s SPIRV\ncodegen led to incorrect results for a TVM-optimized model’s\noutput, which affected all users who use TVM for their\nmodel deployment on the Nvidia Vulkan backend1. Second,\nthe highly sophisticated compilation process in a deep learning\ncompiler is error-prone. According to a very recent study [36],\n1https://github.com/apache/tvm/pull/8102\nduring a time period of 15 months, there are 845 bug-fixing\npull requests on the TVM project, including 318 bugs in total.\nA deep learning model, as the input of a deep learning com-\npiler, needs to satisfy semantic specifications in two aspects;\notherwise, it will be rejected by the deep learning compilers\nat an early stage before invoking the actual core functionality\nof the compilers. First, a deep learning model is a neural\nnetwork arranged as a directed and acyclic graph. Second,\nthe model also needs to satisfy certain constraints, which are\nrequired by the operations in the model. For example, within\na model, a MatMul operation (denoting matrix multiplication)\nwith two input matrices (2-D tensors) requires that the number\nof columns in the first matrix is equal to the number of rows\nin the second matrix.\nTo test core compiler parts (achievable by only valid inputs),\none can indeed adopt random testing (the most popularly used\ntechnology for compiler testing practices [3]) in a straight-\nforward way: generating possible inputs and filtering them\nby checking against the semantic specifications2, also called\nas declarative-style random test generation [2], [10], [13];\nhowever, this style suffers from two main issues. First, random\ntesting in the declarative style has a fairly low probability\nof satisfying the semantic specifications, especially for com-\nplicated operations (as shown in our experimental results in\nSection IV), wasting testing the budget on generating and\nchecking many invalid inputs. Second, valid inputs generated\nby this random testing strategy tend to be simple, as complex\ninputs have an even lower probability of satisfying the seman-\ntic specifications, whereas it is highly critical to also generate\ncomplicated models in order to achieve various testing goals.\nTo better conduct random testing on a deep learning com-\npiler, we take the semantic specifications of a deep learning\nmodel as logic constraints; in this way, test generation is\nequivalent to finding solutions to the constraints. However,\nwe face a challenge due to complex constraints, i.e., those\nrelated to both the graph structure of the model and operations\nwithin the model. Furthermore, within the constraints, the\ninvolved first-order/second-order logic (as shown in Section II)\nis undecidable in general [39] and causes existing solvers not\nto be able to encode, or perform efficiently [9], [15].\nTo address the preceding challenge, we propose a novel\napproach named Isra based on the following insight: the\n2Checking against the semantic specifications can be in the form of a\nboolean checker such as repOK [2], etc.\narXiv:2302.00842v3  [cs.SE]  26 Dec 2024\nconstraints on a deep learning model have certain properties,\nallowing us to iteratively resolve and simplify the constraints\nto effectively find solutions, by following a proper instantiation\norder. We design two strategies in the core part of Isra, a novel\ndomain-specific constraint solver. Our solver conducts instan-\ntiation with an order for gradually resolving and simplifying\nconstraints. Based on the consistency among the constraints,\nIsra, with our domain-specific constraint solver, is able to find\nsemantically valid inputs without backtracking, while ensuring\nboth soundness (the generated inputs are semantically valid)\nand completeness (no loss for the probability of generating\nany valid model).\nTo evaluate Isra, we implement it and empirically compare\nit with\nfive baselines: (1) the aforementioned approach of\nrandom test generation, named as declarative-style generation,\n(2) test generation based on the idea of feedback-guided test\ngeneration [27], named Randoop-like generation, (3) a state-\nof-the-art tool named Muffin [16] implementing a generation-\nbased approach for deep learning models, and (4) a mutation-\nbased approach, TVMFuzz [36], toward testing deep learning\ncompilers , (5) a state-of-the-art generation-based approach\nnamed NNSmith [21] toward testing deep learning compilers.\nOur evaluation results show that our Isra approach substan-\ntially outperforms the baselines on the metrics of generated\nvalid test inputs under the same setting, for demonstrating our\napproach’s effectiveness. Furthermore, to investigate the bug\ndetection capability, when used to test the same benchmark\n(TVM, Glow, and SophGo), Isra detects 33 unique bugs in\ntotal (with 18 on TVM, 4 on Glow, and 11 on SophGo),\nperforming better or as well than the baselines.\nIn addition, among the bugs found by Isra, there are 24\npreviously unknown bugs. After these previously unknown\nbugs were reported to compiler developers, 19 were confirmed\nand 16 were already fixed upon our bug reporting so far.\nThe positive feedback from the developers also shows Isra’s\nhigh value in practice. The source code, experimental results,\nand bug reports are publicly available at https://github.com/\nisraProj/isra.\nIn summary, this paper makes the following contributions:\n• An effective test generation approach named Isra for\ntesting deep learning compilers, based on instantiation-\nbased constraint solving, working in a backtrack-free way,\nwith the guarantee of soundness and completeness.\n• A domain-specific constraint solver toward finding so-\nlutions to the constraints specified from the semantic\nspecifications of a deep learning model, with two novel\nstrategies for resolving complex constraints.\n• Implementation and evaluation of our approach for show-\ning its high effectiveness and high practical value, includ-\ning outperforming state-of-the-art approaches on cover-\nage metrics, achieving comparable and complementary\nresults on the bug detection, and successfully finding 33\nunique bugs in three popular real-world deep learning\ncompilers.\nDeep Learning Compilers\nComputation Graph\nExecution\nTest Oracles\n Information\nTest Program\nGenerator\nFig. 1: The Pipeline of Testing Deep Learning Compilers\nII. BACKGROUND AND OVERVIEW\nFigure 1 shows our overall pipeline of random testing for\ndeep learning compilers. In the stage of test generation, we use\nour test program generator to generate random computation\ngraphs that are semantically valid. For the test oracle, we use\nboth differential testing and the crash status of the compiler\nunder test [3]. Before formally describing our approach in\ndetail (as shown in Section\nIII), we first take an overview\nof background with specific examples, then illustrate our\napproach in a nutshell.\nA. Computational Graph for Deep Learning\nA deep learning model, as the input of deep learning\ncompiler, can be regarded as a computation graph, where an\nedge represents a tensor (an N-D array), denoting the flow of\ndata between nodes, and a node represents (1) an operation\n(denotes a mathematical function) whose inputs are the in-\ncoming edge(s) and outputs are the outcoming edge(s), or (2)\na tensor placeholder where we need to represent creation of a\nuser-defined variable such as the input tensor of computational\ngraph. The computation graph can be executed by feeding the\nspecific data into tensor placeholders. The formal definitions\nof the computation graph are shown in Section III.\nAs an example, Figure 2 shows a deep learning model with\ntwo operations. The first operation is Add. It takes two tensors\np and q as its input and outputs a tensor r as their sum. The\nsecond operation is Concat. It accompanies an attribute axis\n(denoting the dimension of the axis to concatenate on) and\ntakes two tensors r and s as its input, and outputs a tensor\nt as their concatenated results. The edge in the computation\ngraph represents the dataflow that gets transferred between the\nnodes. For example, r, as the output of Add operation, could\nbe transferred to the input of Concat operation.\nA computation graph is directed and acyclic, specifying the\norder of computation. In the example, you need to compute\nAdd first in order to compute Concat because the output of\nAdd (i.e., tensor r) flows to the input of Concat. Except for\nthe acyclicity of the graph, for each operation, the number\nof incoming edges should be aligned with the number of\ninput arguments defined by corresponding mathematical func-\ntion that operation denotes. For example, Concat requires\ntwo or more input arguments, so the number of incoming\nedges should be more than or equal to two. We called those\nsemantic specification of computation graphs as graph-level\nconstraints.\nBesides graph-level constraints, each operations in the com-\nputation graph holds its internal semantic specification, speci-\nFig. 2: An Example of a Computation Graph.\nfied from the definition of mathematical function that operation\ndenotes, which we called operation-level constraints. In\nour example, as the input of Add operation, tensor p and\nq should have the same shape. Similarly, as for Concat\noperation, tensor r and s must have the same shape, except\nfor the dimension of the axis to concatenate on (defined\nby an operation’s attribute axis). Particularly, by explicitly\ndenoting the structure of tensors, operation-level constraints\nof the computation graph in Figure 2 could be specified as\nfollows (dima denotes the dimension size of tensor a, and\na[i] denotes the length of the i-th dimension of tensor a) :\ndimp = dimq = dimr\n(1)\n∀i ∈[1, 2, ..., dimp], p[i] = q[i] = r[i]\n(2)\ndimr = dims = dimt\n(3)\n∀i ∈[1, 2, ..., dimr] ∧i ̸= axis, r[i] = s[i] = t[i]\n(4)\nt[axis] = r[axis] + s[axis]\n(5)\n1 ≤axis ≤dimr\n(6)\nB. Challenges\nThe complicated semantic specifications of the computation\ngraph, which consist of both the graph-level constraints and\noperation-level constraints, result in the sparsity of seman-\ntically valid inputs (compared to syntactically valid inputs).\nThus, random test generation suffers the issues on effec-\ntiveness. Specifically, the approach that randomly generates\npossible computation graphs, and then filters invalid ones\nby checking against the semantic specification, holds fairly\nlow possibility to satisfy the semantic specification. As our\nprevious example, for two tensors of the input of an Add\noperation, assume that the range of the tensor’s dimension\nsize is O(D) and the length of each dimension is O(L), the\ngeneration holds O(L−D) possibility of producing valid ones\ndue to Equation 2. The possibility diminishes with larger deep\nlearning model, wider range, or more complex specifications.\nIn order to better conduct random testing on deep learning\ncompilers, instead of taking semantic specification as a black-\nbox checker, we explicitly specify the semantic specifications\nof computation graph as constraints, i.e., take semantic spec-\nification as an whitebox, and test generation is equivalent to\nfind solutions that satisfies those constraints.\nHowever, existing practices of constraint solving are limited\ndue to our complex constraints which are expressed in first-\norder/second-order logic instead of propositional logic due to\nfollowing reasons: (1) the acyclicity of computation graph;\n(2) the existence of quantifiers such as ∀i ∈[1, 2, ..., dimp]\nin Equation 2; (3) the existence of unknown functions such\nas r[axis] in Equation 5 (it is called unknown function\nbecause we actually need to construct a function that maps\nto the length of each dimension of a tensor that we may not\nknow the dimension size). Compared with propositional logic\nthat is decidable, solving first-order/second-order logic (with\nquantifiers and unknown functions) is challenging because\ntheoretically the first-order/second-order logic is undecidable\nin general [39] and also, in practice, quantifiers and unknown\nfunctions cause existing solvers unable to encode, or perform\ninefficiently [15], [34], [9].\nC. Instantiation-based Constraint Solving\nInstantiation\n[1], [19], [30], [32], [33] is a widely used\ntechnique for solving constraint satisfaction problem (CSP)\nsuch as satisfiability modulo theories (SMT). By assigning the\nvalues to the variables in the constraints, we could get an\ninstantiation of the constraints.\nAn instantiation-based solver starts from an empty instanti-\nation and extends instantiation (mostly in an iterative way) to\nfind solutions. An instantiation could be extended by assigning\nthe values to the variables that are not assigned in the instan-\ntiation before. In the meanwhile, by replacing the variables\nwith their assigned specific values in the constraints, also,\nwith the help of solver, it is possible to simplify constraints\nand reduce the domain of unassigned variables (it is called\nas constraint propagation [1]). For example, for the constraint\nS ⊂T (S and T are set variables), if we first instantiate T\nas {1, 2}, then it is easy to simplify the constraints by solving\nconstraints and simultaneously deducing the domain of S, i.e.,\nS ⊂T ⇒S ⊂{1, 2} ⇒S ∈{∅, {1}, {2}}.\nWith instantiation, the elimination technique has a chance\nto be applied for the simplification on constraints which are\noriginally encoded in first-order logic (or higher-order logic).\nInspired by quantifier elimination [6], [15], if we already\ndetermine the domains of quantifiers or unknown functions\nin constraints according to the instantiation, we could then\nsimplify the constraint by rewriting the constraints to an\nquantifier-free and unknown-function-free form. For example,\nassume S is a set variable, for the constraint ∀x ∈S, P(x), if\nwe already instantiate the domain of the universal quantifier\nx as S : {1, 2, 3}, then we could eliminate the quantifier\nby rewriting the constraints as follows: ∀x ∈S, P(x) =⇒\nP(1) ∧P(2) ∧P(3).\nWe call an instantiation consistent if it could be extended\nto a solution, otherwise it is inconsistent. For example, in\nthe constraint S ⊂T, if we first instantiate T as ∅, then\nthe instantiation is inconsistent. Generally, instantiation-based\nsolvers may backtrack to try other instantiations if it finds\ninstantiations are inconsistent. Backtracking decreases the\nsolver’s efficiency on finding solutions.\nD. Isra in a Nutshell\nTo effectively generate semantically valid computation\ngraphs, we propose an effective random test generation ap-\nproach, named Isra, including a domain-specific constraint\nsolver with two strategies: graph-level and operation-level\nconstraint resolving, based on our key idea that constraints are\nable to be simplified with a well-designed instantiation order.\nNext, we introduce a running example to briefly illustrate how\nIsra works.\n1) Graph-level constraint resolving: Our generation fol-\nlows a topological order of operations in computation graph.\nWe say node a precedes node b, or b succeeds a, if there exists\na path, where a appears earlier than b in the path. Each time\nwe generate a node, this node does not precedes any existing\nnode. For example, as the graph shown in Figure 2, followed\nby a topological order, i.e., operation Add then operation\nConcat, our approach instantiates operations one by one. For\neach operations, we first instantiate its type and the number\nof incoming edges.\nIn this way, our approach resolves the constraints by parti-\ntioning them into several subparts. Each subpart corresponds to\na single operation and its related edges. Furthermore, because\nthe output of operations could be determined only by the input\nand attributes, we rewrite the constraints by substituting the\noutput as a function of the input and attributes. In our example,\nthe constraints are as follows, where Specop(V ) is defined as\na set of constraints on V that specifies from the specification\nof an operation with type op, fop denotes the mathematical\nfunction of the operation with type op:\nS1 : SpecAdd(p, q) ∧(r = fAdd(p, q))\n(7)\nS2 : SpecConcat(axis, r, s) ∧(t = fConcat(axis, r, s)) (8)\nAfter resolving constraints with instantiating operations\nin the graph by their topological order, our goal turns to\ninstantiate each single operation by solving constraints related\nto the operation (in our example, they are SpecAdd(p, q) and\nSpecConcat(axis, r, s)), as shown in the next part.\n2) Operation-level constraint resolving: The instantiation\nof a new operation includes assigning the value to its op-\neration type, attributes, input (incoming edge(s)) (output of\nthe operation is excluded as explained before). As a concrete\nexample, assume we already instantiate a computation graph\nwith a single Add operation (with p and q as its input as shown\nin the red part of Figure 3), now we extend the instantiation\nby appending a new operation into the computation graph.\nAssume we instantiate type of the new operation as Concat,\nand the number of incoming edges of the new operation as two\nin our example. We now set symbol variables for the input\nand attributes of the operation. In the example of Concat, we\ndenote variable ax for the attribute axis, and variable x and\ny as two incoming edges (note, ax, x and y are just symbol\nvariables, which will be assigned with values later, such as\nassigning r to x).\nFor each incoming edge, i.e., each tensor in the input,\ninstead of instantiating the whole tensor, we focus on instan-\ntiating the structure of the tensor first due to that the semantic\nspecifications are only related to tensor structure. For an edge\nx, we setup a set of symbol variables to substitute x in the\nconstraints, including dx (denoting the dimension size of x,\ni.e., dimx) and xi (denoting each dimension’s length of x, i.e.,\nx[i]). In this way, the constraints could be further specified as\nfollows:\nV\nk∈{1,2,3} Ck(ax, dx, dy, xi, yi)\n(9)\nC1 : dx = dy\n(10)\nC2 : ∀i ∈[1, 2, ..., dx] ∧(i ̸= ax), xi = yi\n(11)\nC3 : 1 ≤ax ≤dx\n(12)\nTo sample a random solution to the above constraints, our\napproach instantiates variables in a well-designed order: first\nare variables related to x; then attribute axis; finally variables\nrelated to y. In our example, the order is as follows: dx; xi;\naxis; dy; yi. Note, in the order, variables related to the same\ntensor are ordered together in a group, also, within the group,\ninstantiation of the dimension size (e.g., dx) is ahead of the\nlength of each dimension (e.g., xi). The detailed illustration\nand explanation of the order are shown in Section III-C.\nFollowed by this order, we are able to simplify the con-\nstraints to quantifier-free and unknown-function-free by the\nelimination technique mentioned in Section II-C; also control-\nlably choose ways for instantiating unassigned tensors, i.e.,\ninstantiating the tensor as an instantiated one such as r for x;\nor as the output from a tensor placeholder such as s for y.\nDetails are shown in Section III.\nWith above simplification, the constraints belong to propa-\ngation logic which is decidable. Thus, we are able to conduct\nconstraint solving by constraint propagation [1] to find solu-\ntions. In addition, the constraint propagation will not produce\nan empty domain (which causes the instantiation inconsistent)\ndue to the good property as explained in next part, resulting\nin the overall process being backtrack-free.\n3) Properties of Isra: Based on the graph theory and\nthe theory of constraint satisfaction problem (CSP) [1], our\ninstantiation-based solver holds some good properties due to\ncharacteristics of constraints on deep learning models. We\ndraw main conclusions here, formal definitions and detailed\nexplanations are shown in Section III.\nThe first property is called graph-level directional consis-\ntency. For any semantically valid computation graph with a\ntopological order on operations as (O1, O2, ..., Oi), we can\nconsistently extend the instantiation to include Oi+1 (with\ntopological order as (O1, O2, ..., Oi, Oi+1)) as long as ensur-\ning satisfaction of constraints related to Oi+1.\nThe second property is called operation-level global con-\nsistency. For constraints related to each single operation, after\ndetermining operation’s type and the number of tensors in the\ninput, by taking attributes as a variable and each tensor in the\ninput of the operation as a variable, this CSP, which consists\nof constraints on the operation level, is globally consistent:\nFig. 3: A running example of Isra\nany consistent instantiation of a subset of the variables can be\nextended to a consistent instantiation of all of the variables\nwithout backtracking [7].\nIII. DETAILED APPROACH DESCRIPTION\nA. Notations and Definitions\n1) Concepts in the Computation Graph: A tensor t is a\ngeneralized vector, like N-D array (N is a positive integer).\nThe structure of tensor t is defined as a set Strt, denoting\nthe structural information of the tensor. Strt includes (1) a\nnumerical value dimt that denotes t’s dimension size and (2)\na variable-length array that denotes each dimension’s length\nof tensor t (i.e., t[i] as the i-st dimension’s length).\nAn operation n is defined as a function with tensor(s) as\ninput and output. It has some parameters: we denote Opn as its\ntype (Opn ∈AllOps, AllOps is a unverisal set which contains\nall types of operations), Attrn as a set which contains at-\ntributes of the operation (such as strides in Conv operation, and\naxis in SoftMax operation). Also, Inputn = {in1, in2, ...} is\na set which contains one or more tensors as the input of n,\nand Outputn = {out1, out2, ...} as the output of n. Specially,\nAttrn contains a special attribute Indegreen as the number\nof tensors in Inputn, i.e., Indegreen = |Inputn|.\nA tensor placeholder tp is simply a variable, denoting a\ntensor to which the specific data will be assigned later. A\ntensor placeholder that denotes tensor p could be created with\nmerely Strp, without need of specific data.\nA computation graph G is defined as an ordered triple\n(VG, EG, ψG), where the set VG denotes the nodes, the set EG\ndenotes the edges. An element in VG is either an operation or a\ntensor placeholder. An element in EG is a tensor. ψG is called\nan incidence function which maps an edge into a pair of nodes\n(i.e., a mapping from E(G) →V (G) × V (G)), denoting the\nstructure of the graph.\n2) Constraint Satisfaction Problem: A constraint C is a\nlimitation placed on the values of variables. Consider a finite\nsequence of variables S := {x1, x2, ..., xk}, with respective\ndomains D(x1), . . ., D(xk) associated with them. So each\nvariable xi ranges over the domain D(xi). By a constraint C\non S we mean a subset of D(x1) × D(x2)... × D(xk).\nAn instantiation Q is defined as a set of tuples, written\nas ⟨x1 = v1, x2 = v2, ..., xm = vm⟩, denoting that a specific\nvalue vi that has been assigned to the variable xi.\nA constraint satisfaction problem (CSP) P : (V, D, C),\nwhere V is a set of variables, D is the set of domains of\nvalues for each variable, C is a set of constraints. A solution\nto a problem P is an instantiation Q, containing the assignment\nof all variables in V , which satisfies all of constraints in SC.\nNamely, an instantiation Q is a solution of P : (V, D, C) only\nif it satisfies with all of constraints in C.\n3) Local Consistency and Global Consistency: Let X =\n(X1, X2, ..., Xn) be a set of variables in a CSP, and let X\n′ =\n(X\n′\n1, X\n′\n2, ..., X\n′\nm) be a subset of variables from X. A partial\ninstantiation of variables\nD\nX\n′\n1 = x1, X\n′\n2 = x2, ..., X\n′\nm = xm\nE\nis locally consistent if it satisfies all the constraints in X\n′. A\nglobally consistent CSP is one in which any locally consistent\npartial instantiation of variables can be extended to a consistent\nfull instantiation. Globally consistent CSPs have the property\nthat a solution can be found without backtracking [7].\nB. Graph-level constraint resolving\nBased on the acyclic trait of the computation graph, for any\ncomputation graph, there always exists a topological order of\noperations, i.e., for every two operation x and y in computation\ngraph, if there is a tensor that is both the output tensor of x\nand the input of y, then operation x comes before operation\ny in the ordering.\nOur approach works as a top-down way to incrementally\ninstantiate a computation graph by iteratively instantiate a new\noperation and appending it into the computation graph, as\nshown in Figure 3. Specifically, we follow the topological\norder of operations to generate them in the computation\ngraph. When to generate a new operation x, we need to\ninstantiate OPx, Attrx and Inputx, with ensuring the sat-\nisfaction SpecOPx(Attrx, Inputx) (the same with the defi-\nnition in Section II-D). We leave the instantiation of edges\nin Outputx later (when we instantiate another operation y\nwith an edge from x to y) because the value of Outputx\ncould be determined only by OPx, Attrx and Inputx, i.e.,\nOutputx = fOPx(Attrx, Inputx).\nAfter finishing the instantiation for current the operation\n(with its attributes and incoming edges), we iterate the same\nprocess for instantiating the next operation if the number of\noperations in the computation graph has not exceeded to the\nparameter we set (named numopG as the number of operations\nin generated computation graph).\n1) Property of graph-level directional consistency: Taking\neach operation as a variable (a set variable Vx, containing\nOpx, Inputx, Attrx), constraints of the computation graph\ncould be rewritten as a binary CSP, consisting of two kinds\nof constraints: first are unary constraints on each variable,\nand second are binary constraints between two variables (i.e.,\nthe output of an operation equals to the input of another\noperation). According to the definition, an instantiation Q is\nlocally consistent on Vx if Q satisfies all the constraints in Vx.\nBecause our instantiation follows the topological order of\na computation graph, with only instantiating edges which are\nfrom previous nodes (variables that are ahead of Vx in the\norder) to current variable Vx, the instantiation on Vx will not\naffect local consistency of previous variables (based on the\nproperty of topological order). Thus, for each x, as long as\nwe are able to instantiate Q with its local consistency on Vx\nevery time, then we could include Vx in the end of topological\norder, and instantiation is still consistent. Also, because we\ninstantiate edges with the direction followed by the topological\norder, the overall graph is loop-free. Thus, following with the\ntopological order, ensuring local consistency on each operation\nleads to bracktrack-free instantiation on the graph level.\nC. Operation-level constraint resolving\nTo instantiate a new operation x and append it in the existing\ncomputation graph, we need to instantiate (1) OPx, (2) Attrx,\n(3) Inputx (i.e., incoming edges). We take the instantiation\nof these items into following steps.\nWe first determine the OPx by random sampling from\nAllOps. Thereafter, the constraints can be specified. To\nmodel constraints as a CSP as Px(V, D, C), we define a\nvariable set Vx which contains all the numerical items from\nAttrx and Inputx, i.e., Vx\n=\nAttrx ∪S\nt∈Inputx Strt,\nand also, specify the semantic specification of the operation\nSpecOPx(Attrx, Inputx) as a set of constraints C on the\nvariables. A specific example is shown in Equation 10-12.\nTo solve the above CSP, based on constraint propagation [1],\nour approach works as follows: iteratively extending the in-\nstantiation by picking an unassigned variable and assigning\nthe value to the variable from its domains; after each turn’s\ninstantiation on a variable, with the help of the solver, our ap-\nproach will conduct constraint propagation among unassigned\nvariables throughout the constraints to reduce the domain of\nunassigned variables.\nDuring the above process, there are three main issues as\nfollows.\nFirst, the existence of quantifiers and unknown functions\nin the constraints introduces the difficulty of conducting con-\nstraint propagation. For example, in Equation 11: C2 : ∀i ∈\n[1, 2, ..., dx] ∧(i ̸= axis), xi = yi, without instantiating the\nvalue of axis, we do not know whether C2 implies x1 = y1.\nSecond, how to instantiates ψG (the structure of the graph)\nduring the solving process. A straightforward way (in a\n‘generate-then-match’ style) works as follows: first instantiate\nall symbol variables in Inputx, and then matches each tensor\nin Inputx with instantiated tensors (i.e., outcoming edges of\ninstantiated nodes) by comparing their structures. However,\nbecause the domain of the tensors under instantiation is large,\nthe probability of exactly matching instantiated tensors is\nfairly small. For example, in the example of Section II, to\ninstantiate tensor x and y (i.e., incoming edges of Concat),\nthe number of possible solutions is exploded, which leads to\nfairly low probability of the equivalence between Strx/Stry\nand structures of instantiated tensors (such as Strp). Thus,\nthis straightforward way will lead to the result that generated\ncomputation graph tends to be simple and scattered.\nThird, constraint propagation might produce an empty do-\nmain, causing backtracking of the solver (i.e., inconsistence of\nthe instantiation), which affects the effectiveness of constraint\nsolving. For example, for the constraint (x = y) ∧(x =\nz) ∧(y ̸= z), if the instantiation is ⟨x = 1⟩, after constraint\npropagation, y holds an empty domain.\nTo address those issues, we tailor a well order that success-\nfully (1) simplifies the constraints to be quantifier-free and\nunknown-function-free, and (2) enables to controllably select\nchoices for instantiating unassigned tensors; with the guaran-\ntee that our propagation-based instantiation is backtrack-free\n(keeps consistency during the whole process).\nIn the following parts, we will first describe the order, and\nthen explain our reasons for that, finally, illustrate the property\nof operation-level constraint resolving in our approach.\n1) The order of the instantiation: For an operation x,\nour order contains several groups, arranged as follows:\nG1, G2, G3, ..., G|Indegreex|+2. First group consists of one\nvariable: G1 = {Indegreex}. Second group is the variables\nrelated to the first tensor in the input (first incoming edge\nof this operation), i.e., G2 = Strin1(in1 ∈Inputx). Third\ngroup is the attributes of operations (except for Indegreex),\ni.e., G3 = Attrx \\ {Indegreex}. For the rest, each group is\nvariables related to the next tensor in the input (next incoming\nedge of this operation), i.e., Gi+2 = Strini(ini ∈Inputx). In\nthe groups related to each tensor (assume the tensor is t), the\nvariable that denotes the dimension size of the tensor is ahead\nof the variables that denote the length of each dimension of\nthe tensor, i.e., dt (denotes dimt) is ahead of ti (denotes t[i]).\n2) Reason I: Quantifier and unknown function elimination:\nSpecifically, there are two forms which cause the existing\nconstraint solving or sampling approaches [5], [9] hard to\nhandle. First is the quantifier in the constraints, which hold the\nforms as ∀i ∈f(x), C(i), where f is a function returns a set,\nx is a variable, C(i) is a constraint whose form is dependent\non the value of i. Second is the unknown function. Constraints\nmay contain terms such as tf(x), where f(x) is a function that\nreturns an integer number and tf(x) is the variable that denotes\nthe t[f(x)] (f(x)-st dimension’s length of tensor t).\nOur instantiation could eliminate the quantifiers and un-\nknown functions in constraints with the following reason. As\nquantifiers in form of ∀i ∈f(x), C(i), for all of variable\nv whose existence in C(i) depends on the domain of f(x),\nwe call that v depend on x. As constraints with unknown\nfunction such as tf(x), we call ti depends on x. For constraints\non deep learning models, the above dependencies among\nconstraints are loop-free. Our instantiation order is actually a\ntopological order satisfying those dependencies, i.e., ensuring\nthat for any dependencies that x depends on y, y is ahead\nof x in the order. Thus, followed by the instantiation order,\nwith satisfying the precondition of eliminating quantifiers and\nunknown functions by instantiation, we could simplify the\nconstraints to a decidable propositional logic for constraint\npropagation.\n3) Reason II: On-demand instantiation: The reason why\nwe put the variables related to the same tensor in a group, i.e.,\ninstantiate the tensor(s) one by one, is due to the consideration\nof instantiating ψG.\nWe design an on-demand policy for instantiating the tensor,\nwith consideration of the instantiation for ψG in the mean-\nwhile. For each tensor t in the input of x (t ∈Inputx), our on-\ndemand policy instantiates Strt with two choices: (1) reusing\nthe structure Strs of an existing tensor s as long as they are\nconsistent with the instantiation, in other words, we set t’s\nstructure the same as an instantiated tensor s, which is from the\noutput of an instantiated node ex, i.e., instantiating t = s with\nψG(t) = (ex, x) (if there are more than one satisfied tensor, we\nwill randomly pick one of them; if no such tensor, we choose\nthe second way); (2) creating a new tensor placeholder as a\nnode n, and set its output as t, in other words, instantiating a\ntensor t with t ∈Outputn as well as ψG(t) = (n, x).\nWe select the choice of instantiating tensors according to a\nBernoulli distribution, as a common way to produce random\nboolean decisions. Any other distributions are also allowed.\nThe distribution is controlled by a parameter picking rate.\nThe higher picking rate is, the higher chance that our\napproach would select the first choice (i.e., reusing an existing\ntensor). To favor generating more complicated computation\ngraphs, we set picking rate relatively high in practice. We\nwill further explain the effect of this parameter in Section IV.\n4) Property of operation-level global consistency: We il-\nlustrate that our propagation-based instantiation will not lead\nto inconsistence due to the property we called operation-\nlevel global consistency. For the constraints related to each\nsingle operation such as x, after determining the type (OPx)\nand the number of tensors in the input (Indegreex), we\ncould take attributes as a variable and each tensor in the\ninput of the operation as a variable. With good properties of\nspecification on deep learning operations, this CSP is globally\nconsistent [7]. Thus, any order for instantiation, including our\ndelicately-designed instantiation order in our approach, will\nnot lead to inconsistence.\nD. Properties of Isra\nOverall, Isra in our approach is backtrack-free, sound and\ncomplete. Based on the property of graph-level directional\nconsistency and opertional-level global consistency, we are\nable to instantiate without backtracking. And also, soundness\nis guaranteed because the instantiation is always consistent,\nleading to the satisfaction of final solutions. Completeness is\ndue to that we do not lose probability of generation of any\ninstantiation during the whole process, in other words, any\ninstantiation that satisfies the constraints has the possibility to\nbe generated.\nIV. EVALUATION\nTo evaluate the effectiveness of our approach, we compare\nour approach with five baselines, including three state-of-\nthe-art approaches Muffin [16], TVMFuzz [36], and NN-\nSmith [21]. Also, we evaluate them on three popular real-world\ndeep learning compilers to investigate their bug detection\ncapability. We construct the computation graph based on the\nONNX [25] standard. Our implementation is on Python 3,\nsupporting generation of 65 operations in total [31]. We ad-\ndress the following three research questions with an evaluation\nconducted on Ubuntu 20.04 of a laptop computer with Intel®\nCore™i5-1135G7 CPU @ 2.40GHz and memory of 8GB.\nMore details are included on our project website [31].\nRQ1: Is Isra effective for generating test inputs for testing\ndeep learning compilers?\nRQ2: How effective and practical are the generated tests in\nrevealing bugs in popular real-world deep learning compilers?\nRQ3: Does our approach outperform state-of-art approaches\nin terms of testing deep learning compilers in terms of\ncoverage and bug detection capability?\nA. Compared Work\nTo assess the effectiveness of Isra, we first design and\nimplement two baselines as the representative of another\ntwo types of test generation techniques, named DeclGen and\nRandoop-Gen. In addition, we also compare Isra with three\nstate-of-the-art approaches that can be applied for testing deep\nlearning compilers. More specifically, we include the following\nrepresentative techniques in our evaluation:\n1) DeclGen: Declarative-style generation constructs deep\nlearning models only based on the syntax grammar, in short\nas DeclGen. When determining the shape of tensors, it just\nrandomly generates from all choices. After the construction\nof the input, i.e., a whole computation graph, this approach\ndirectly feeds input into the compiler under test, and relies on\nthe compiler’s running to check whether the model is satisfied\nwith its semantic specifications.\n2) Randoop-like generation: Inspired by feedback-directed\nrandom test generation [27], this approach conducts random\ngeneration for operation construction, i.e., randomly constructs\na new operation to append it into the model, and checks\nwhether the model satisfies the semantic specifications or not.\nThis generation way can avoid generating invalid models at\nearly stages, leading to the improvement of overall effective-\nness, while the generation for a single operation to satisfy its\nsemantic specifications is still ineffective.\n3) Muffin: Muffin [16] is a state-of-the-art generation-based\ntesting approach, proposed originally to test deep learning\nlibraries such as Keras [37], generating models based on two\nmodel structure templates (chain structure with skips and cell-\nbased structure) with deep learning library APIs. To satisfy\nwith tensor structural constraints, Muffin hardcodes additional\nReshaping layers to reshape the input/output tensors for the\nconnection between layers. Though Muffin is not designed\nfor testing deep learning compilers, we can retrofit it to barely\nachieve the goal by converting the models constructed by high-\nlevel APIs into computation graphs with existing tools [38].\n4) TVMFuzz: TVMFuzz [36] is a fuzzer which specifically\ntargets testing TVM [4]. It randomly generates and mutates\nTensor-level IR(TIR) expressions (the intermediary language\nof TVM) for fuzzing test, mainly towards the type-related bug\ndetection.\n5) NNSmith: NNSmith [21] is a generation-based approach\nfor testing deep learning compilers, as a parallel research\nwork with ours. NNSmith constructs the computation graphs\nbased on the specifications of deep learning models: it tries\npossible choices for types, attributes, and input/output of\noperations by iteratively adding constraints from specifications\nand leveraging existing constraint solver Z3 [5] to check the\nsatisfaction; if constraints are not satisfied, it will backtrack\nand try different choices to pick.\nB. Study Subjects and Settings\nWe choose TVM , Glow (two popular open-source com-\npilers), and SophGo (a state-of-practice commercial compiler)\nas our study subjects. For TVM and Glow, we download their\nofficial released versions from GitHub: TVM as v0.7 (commit\nid 728b829), Glow3 (commit id a2036bd). For SophGo, we at-\ntain its latest released version from the company that develops\nit.\nFor test oracles, we use two types of oracles: (1) runtime\nfailure (including error/crash behaviors) of compilers, i.e.,\nwhen a computation graph causes the exception of compilers\n(excluding invalid inputs that violate the specifications); (2)\ndifferential testing by feeding the same random inputs and\ncomparing the outputs of compiled models from different\ncompilers. In differential testing, we set the relative error\nas 10% (we set this value relatively large in order to avoid\nmany false positives) for automatically comparing results from\ndifferent compilers under the same input.\nFor Isra, we set the upper bound on the tensor dimension\nas 5, and the upper bound on the length of each dimension\nas 5 (which is aligned with default settings of Muffin). For\npicking rate, we set it with 0.97 based on our sensitivity\nexperiments, as shown in Table II. Except for above parameters\nwhich keep the same among all of experiments, we set the\nlower bound and upper bound of operation number in the\ngraphs (named lb and ub) according to the experiments, the\nnumopG is uniform sampling between lb and ub.\nFor Muffin, we obey its default settings. For alignment with\ncomparsions on coverage metrics, we convert the Keras models\ngenerated by Muffin to ONNX format with TF2ONNX [38].\nBecause Keras API is more higher level than ONNX, the\nconverting leads to the growth of model size. For fairness,\nin the experiment on coverage measurement, for every model\nMuffin generated, we adopt Isra to generate a model with the\nnumber of operations same as Muffin, named Isra*. Also, we\nensure that both approaches have a same operation set (of size\n36, which is the number of overlapped operations).\nFor TVMFuzz, we obey its default parameters. Due to the\ndifference on the type of generated inputs (Isra generates\ncomputation graphs, while TVMFuzz generates programs in\nTIR, an IR for the TVM compiler), the metrics that we design\nfor the work in this paper are not applicable for TVMFuzz. So\nwe are unable to measure our coverage metrics on TVMFuzz.\nTVMFuzz is compiler-dependent, so we are unable to test\nTVMFuzz on compilers except for TVM.\nFor NNSmith, we obey its default parameters. For fairness,\nin the experiment on coverage measurement, we align the\nsettings of Isra with those from NNSmith, named Isra**. For\n3Glow does not have release version on GitHub. Thus we directly download\nits latest version.\nevery model NNSmith generated, Isra** generates a model\nwith the same number of operations. Also, we ensure that\nboth approaches have a same operation set (of size 40, which\nis the number of overlapped operations).\nFor the experiment on coverage measurement, we run each\napproach to generate a fixed number of models, as 10000.\nFor the parameters of operation number in the graphs, we\nset lb as 1 and ub as 200 for Isra as well as DeclGen and\nRandoop-Gen in the experiment. To eliminate randomness, We\nrun our approach and baselines separately for five times and\ncalculate the average number. According to our results of the\nexperiments, the coverage metrics of the three approaches are\nall saturated or converged, so it is a reasonable setting for our\nevaluations.\nFor investigating bug detection capability, aligned with the\nsetting in\n[16], for each method, we generate 300 compu-\ntation graphs for testing compilers. Due to the difference of\nsupported operations for each compiler, we run the generators\nwith filtering the generated graphs that contain unsupported\noperations until the graph number reaches to 300. To reduce\nmanual work on deduplication, we set the generated model\nrelatively small with lb as 1 and ub as 10.\nTo save manual effort for checking duplicated bugs, we\nautomatically check and filter the bugs with the same error\nmessages and stack traces as produced before. For the rest of\nthe bugs, two authors manually check their root causes through\nerror messages, stack traces, and generated inputs to further\neliminate duplicated bugs and false positives.\nC. Metrics\nIn order to evaluate the effectiveness and practicability of\ndifferent approaches, we investigate on their bug-detection\ncapability, by counting the number of distinct bugs they detect\nwithin a fixed number of test inputs when used to test real-\nworld deep learning compilers.\nFurthermore, to better measure various aspects of generated\ninputs, we inherit the coverage criteria from previous research\nwork [23] and expand upon them to measure the diversity\nof computation graphs, including types, tensor shapes, and\nattributes of operations as well as connections between op-\nerations. The design of these coverage criteria is motivated by\nthe fact that (1) existing traditional code coverage criteria are\nineffective in deep learning testing [23], and neuron coverage\ncriteria [29] are also invalid in our evaluation because compiler\ntesting involves different input models, and (2) type and shape\nproblems are major root causes of deep learning compiler\nbugs as demonstrated in a recent study [36], also, (3) a lot\nof high-level optimizations in deep learning compilers, which\nare error-prone due to continuous and frequent development\nand modifications [36], [8], could only be triggered by dif-\nferent types of specific connections between operations in the\ncomputation graphs [24].\nSpecifically, we design 11 types of metrics for measuring\ncoverage among input space. The metrics are of two major\ncategories: graph-level metrics and operation-level metrics.\nFor operation-level metrics, we mainly follow the work by\n[23]. For graph-level metrics, we design them with analogy of\nconcepts in structural code coverage and combinatorial testing.\nBesides the preceding metrics, we also count the frequency of\noccurrences for operations and calculate the distribution of\noperations.\n1) Graph-level metrics: Graph-level metrics are designed\nfor measuring various aspects of a single generated model.\nLet a test set be the set of models generated by an approach.\nGiven a test set I, which contains nI graphs, the graph-level\nmetrics are defined as follows.\nNumber of Operations (NOO) of a graph g is defined as\nthe total number of operations in g. Number of Operation\nTypes (NOT) of a graph g is defined as the number of different\ntypes of operations in g. Number of Operation Pairs (NOP)\nof a graph g is defined as the number of different edges in g\nthat connect two operations together. Number of Operation\nTriples (NTR) of a graph g is defined as the number of\ndifferent pairs of adjacent edges that connect three operations\ntogether in g. Number of Shapes and Attributes (NSA) of\na graph g is defined as the total number of different shapes\nof input tensors and attributes (except for its input dgrees)\nfor each operation in graph g. These graph-level metrics\nfor test set I are defined as the average of each of them\namong all the graphs in I: GLM(I) =\nΣgGLMg(g)\nnI\n, where\nGLM ∈{NOO, NOT, NOP, NTR, NSA}.\n2) Operation-level metrics: Operation-level metrics are de-\nsigned for measuring various aspects of operations among the\nwhole test set. An operation corpus is a set of operations\nwith their attributes including the operation name and the\npossible number of different input degrees. Given an operation\ncorpus C containing nC different operations and a test set\nI, we first calculate the metric on each type of operator o\nin I, denoted as XXCop(o), then we have the operation-\nlevel metric of I as the average of the operation-level metric\non different operators, i.e., XXC(I) = ΣoXXCop(o)\nnC\n, where\nXXC ∈{OTC, IDC, ODC, SEC, DEC, SAC}. We sim-\nply explain the meanings of these six metrics as below, and\ndetailed definitions of these operation-level metrics are shown\nin our project website [31].\nOperation Type Coverage (OTC), Input Degree Cov-\nerage (IDC), Output Degree Coverage (ODC) show the\ndiversity of operations types, and the diversity of the input and\noutput degrees of them in the test set I respectively. Single\nEdge Coverage (SEC) shows the diversity of edges between\nthe operations in I. Double Edge Coverage (DEC) shows\nthe diversity of pairs of edges that are adjacent, which is\nactually the coverage of different triples of operations that are\nconnected in a graph in the test set. Shapes and Attributes\nCoverage (SAC) indicates the diversity of attributes of the\noperations (except for input degrees) and their input tensor\nshapes in the test set.\nD. RQ1: Evaluation Results of Generated Inputs\n1) Operation-level metrics: The result of experiment on\ncoverage measurement is shown in Table I. Firstly, with\nalignment on the number of generated inputs, we can find\nTABLE I: Results on Graph-level and Operation-level Metrics\nIsra\nDeclGen\nRandoop-\nGen\nIsra*\nMuffin\nIsra**\nNNSmith\ntime/s\n320.1713\n9011.9172\n4721.8233\n20.1031\n25847.6804\n111.8615\n880.7737\nOTC\n100%\n97.536%\n98.46%\n100%\n100%\n100%\n100%\nIDC\n92.95%\n90.178%\n89.966%\n91.85%\n88.52%\n89.54%\n88.71%\nODC\n11.848\n4.928\n10.616\n8.75\n4.22\n6.925\n6.225\nSEC\n98.27%\n67.804%\n88.08%\n98.15%\n35.49%\n99.38%\n78.50%\nDEC\n90.208%\n2.126%\n45.324%\n57.7%\n4.95%\n84.30%\n28.70%\nSPC\n3001.938\n227.018\n1509.356\n1192.22\n556.44\n2822.9\n1641.725\nNOO\n100.8766\n2.8783\n100.1231\n10.4236\n10.4236\n16.3999\n16.3999\nNOT\n45.237\n2.76\n33.0021\n8.6589\n5.3289\n13.1333\n10.5113\nNOP\n103.7621\n1.4856\n98.6460\n7.8243\n6.399\n14.5851\n10.4880\nNTR\n102.9130\n0.6042\n105.5774\n4.6481\n6.0766\n13.065\n7.6740\nNSA\n26.6252\n1.5533\n10.0211\n5.9253\n11.3604\n10.8192\n10.1459\nthat Isra outperforms the baselines greatly with respect to the\namount of time, showing the efficiency of our approach.\nFor operation-level metrics, we find that Isra is able to cover\nall kinds of operations that we have chosen and all kinds of\ninput degrees of each type of operation. Compared with the\ntwo baselines, Isra has higher coverage on all of operation-\nlevel metrics, especially for SEC, DEC, and SAC.\n2) Graph-level metrics: We find that the NOO (Number\nof Operations) of our approach and Randoop-Gen are closer\nto the average of the lower bound and upper bound of the\noperation numbers that we set (consistent with our uniform\nsampling for the number of operations), while DeclGen’s\nNOO remains at a significantly lower level. The reason is\nthat DeclGen holds less probability to satisfy the semantic\nspecifications, which leads to generating rather simple models\nand bad performance on the graph-level metrics. Randoop-\nGen’s NOP and NTR are comparable with our approach,\nhowever, the operation types (NOT) of Isra are more diverse,\nmaking it outperform Randoop-Gen at coverage of operation\npairs (SEC) and triples (DEC). The results of graph-level\nmetrics indicate that Isra are capable of generating diverse,\nlarge and complex models.\n3)\nDistribution of Operation Frequency: As shown in\nFigure 4, Figure 5, and Figure 6, Isra is able to generate\ndifferent operations in a relatively uniform distribution, which\nis consistent with our uniform sampling for picking the type\nof operation. As a contrast, all of baselines fail to generate\na sufficient number of operations with relatively complicated\nsemantic specifications such as Conv and Gemm. It shows\nthat\nall of baselines have a limitation that the diversity\nof models generated by them is weak. This is because the\nconstraints of some operations are relatively complicated and\nless possible to satisfy. Those complex operations are less\npossible to be chosen in the valid output by the filter of\nthe iterative checking process. Besides, among models gen-\nerated by Muffin, reshape operation appears most frequently\nbecause Muffin forces the insertions of reshape operation\nbefore many operations to ensure semantic specifications.\n4) picking rate: To evaluate the effect of picking rate\nparameter, we also compare the results of Isra with the setting\nof different picking rates. The settings are the same as the\nFig. 4: Distribution of Operation Frequency of Isra , DeclGen,\nand Randoop-Gen (captioning only parts of operations in x-\naxis for a clear presentation, numbers at y-axis are normalized\nwith the total number).\nTABLE II: Results of Sensitivity Experiment of picking rate\nPicking-\nRate\ntime/s\nNOP\nNTR\nNSA\n0.5\n215.29\n51.07\n23.73\n62.86\n0.8\n260.47\n82.91\n63.99\n42.05\n0.9\n269.98\n94.34\n83.30\n33.84\n0.95\n270.01\n100.17\n94.19\n29.54\n0.97\n267.24\n102.56\n98.77\n27.77\n0.99\n266.36\n104.94\n103.57\n25.93\nexperiment on coverage measurement, except that we set the\noperation number in the graphs as a fixed number 100. The\nresult is shown in Table II. If the picking rate is relatively\nhigh, the operations are more likely to be matched with\nexisting tensors, leading to NOP, NTR going high. In the\nmeanwhile, the shape of newly created tensors is more likely\nto be equal to the shape of previous tensors, leading to the\nresult that NSA goes down as the picking rate increases.\nWe finally choose its value as 0.97, as a trade-off.\nE. RQ2: Evalution Results on Bug-Detection Capability\nWe investigate the effectiveness of Isra and baselines in\nterms of distinct bugs detected in the same version of compiler\nFig. 5: Distribution of Operation Frequency of Isra* and\nMuffin.\nFig. 6: Distribution of Operation Frequency of Isra** and\nNNSmith.\nTABLE III: # unique bugs detected on different compilers.\nIsra DeclGen\nRandoop-\nGen\nMuffin TVMFuzz NNSmith4\nTVM\n18\n8\n14\n12\n5\n21\nGlow\n4\n2\n2\n2\n-\n4\nSophGo 11\n3\n3\n9\n-\n7\nTotal\n33\n13\n19\n23\n5\n32\nFig. 7: The overlaps of detected bugs among dif-\nferent approaches on testing three compilers (red\nfor TVM, green for Glow, blue for SophGo).\nsubjects. After analysis of deduplication, Isra detects 33 unique\nbugs in total, as shown in Table III,\nperforming better or as\nwell than baselines on all of three benchmarks. The capability\non bug detection shows the high effectiveness of Isra.\nTo\nfurther evaluate bug-detection capability of Isra, we conduct a\nstudy on the bugs Isra detected.\n1) Bug Study: Among all of unique bugs detected by Isra,\nwe categorize them into two types based on the test oracles de-\ntecting them: (1) error bug (29 of 33) : the given input triggers\nan error, crash or unexpected exception during compilation;\n(2) inconsistency bug (4 of 33): the compiler executes the\ngiven input and terminates normally but the compiled model\nproduces different outputs according to differential testing.\nAfter we report found bugs to the corresponding community,\ncompiler developers give responses for most of them, all with\npositive feedback. Among all of bugs found by Isra, there are\n24 previously unknown bugs. Until now, a majority of our de-\ntected bugs (27 of 33) have already been fixed by developers,\nwith 16 bugs directly owing to our bug reporting/pull requests,\nbenefiting all the compiler users and their applications. One of\nTVM core developers replies with the following message for\nbugs reported by us5: “These two errors that you generated\nwere excellent real bugs with the importer and were very easy\n4The number of NNSmith’s detected unique bugs on TVM is different from\nthe results in the NNSmith’s paper [21] due to the different settings on the\ncompiler versions and testing amount.\n5https://github.com/apache/tvm/pull/7208#issuecomment-754406762\nto understand and replicate with your post. If they’re being\nauto-generated they look excellent!” The feedback from real-\nworld developers is also strong evidence showing that Isra is\npractical for testing real-world deep learning compilers and\nable to detect critical bugs in practice.\nWe list 24 previously unknown bugs (as well as bug reports)\ndetected by Isra in Table IV, including the stage of root causes,\nthe time it takes to have the bug confirmed and fixed, and the\nGitHub issue ID 6 for reference. The details of confirmed/fixed\nbugs are as follows.\nTVM-1 is a bug in the compiler backend, caused by a\npass named DynamicToStatic which should not be defined\nat optimization level 3. It will lead the internal error when\nthe deep learning model contains operations such as MatMul,\nDropout. After our reporting, developers reordered passes\nin the backend and lowered DynamicToStatic optimization\nlevel to fix it 7.\nTVM-2 is a bug in the compiler frontend. The TVM\ndeveloper has explained the cause of the bug and fixed it8:\n“It’s due to a bad assumption made in PRelu conversion about\nthe input layout......Our current PRelu converter assumes that\n6For\nreference,\nthe\nURLs\nof\nTVM’s\nand\nGlow’s\nbug\nreports\nare\nhttps://github.com/apache/tvm/issues/#IssueID,\nand\nhttps://github.com/pytorch/glow/issues/#IssueID. The URLs for SophGo’s\nbug reports are internal and not publicly accessible.\n7https://github.com/apache/tvm/pull/7213\n8https://github.com/apache/tvm/issues/7202#issuecomment-754372403\nTABLE IV: Previously unknown bugs detected by Isra.\nID\nType\nLocation Confirmed\nFixed\nIssue ID\nTVM-1\nerror\nbackend\n<1 day\n10 days\n7200\nTVM-2\nerror\nfrontend\n<1 day\n<1 day\n7202\nTVM-3\nerror\nfrontend\n<1 day\n6 months\n7241\nTVM-4\nerror\nfrontend\n<1 day\n<1 day\n7244\nTVM-5\nerror\nbackend\n7 months\n7 months\n7262\nTVM-6\nerror\nfrontend\n42 days\n42 days\n7263\nTVM-7\nerror\nfrontend\n<2 days\n10 months\n8889\nTVM-8\nerror\nbackend\n<2 days\n10 months\n8890\nTVM-9\ninconsistency backend\nN/A\n10 months\n7270\nTVM-10\ninconsistency unknown\nN/A\nN/A\n12734\nTVM-11\nerror\nunknown\nN/A\nN/A\n12735\nGlow-1\nerror\nbackend\n13 days\nN/A\n5995\nGlow-2\nerror\nunknown\nN/A\nN/A\n5991\nSophGo-1\nerror\nfrontend\n<3 days\nN/A\n*\nSophGo-2\nerror\nbackend\n<3 days\n<7 days\n*\nSophGo-3\nerror\nbackend\n<3 days\n<7 days\n*\nSophGo-4\nerror\nfrontend\n<3 days\n<7 days\n*\nSophGo-5\nerror\nbackend\n<3 days\n<7 days\n*\nSophGo-6\nerror\nbackend\n<3 days\n<7 days\n*\nSophGo-7 inconsistency backend\n<3 days\n<7 days\n*\nSophGo-8\nerror\nfrontend\n<3 days\n<7 days\n*\nSophGo-9\nerror\nfrontend\n<3 days\n<7 days\n*\nSophGo-10\nerror\nfrontend\n<3 days\nN/A\n*\nSophGo-11\nerror\nbackend\n<3 days\n<7 days\n*\nincoming data is in NCHW format and that the slope will have\nC total elements. Neither of these are actual requirements for\nONNX PReLu.”\nTVM-3 and TVM-4 are bugs in the compiler frontend.\nSome of parameters in LpPool and LeakyRelu operation in\nONNX standard allow default value but it was not supported\nin TVM.\nTVM-5 is a bug in the compiler backend, which catches\nan edge case of the compiler’s type checker as explained by\na TVM developer9.\nTVM-6 and TVM-7 are bugs in the compiler frontend. The\nformer is due to that one of input tensors in Gemm operation\ncan be optional by the standard but the TVM doesn’t allow that\nbehavior. The latter is due to the fact that “Split operation is\nnot dynamic inputs” as explained by the compiler developer.\nTVM-8 is a bug in the compiler backend due to incon-\nsistent specifications between different versions on Squeeze\noperation, causing erroneous implementation in the compiler.\nTVM-9 is a bug in the compiler backend. The erroneous\nbackend implementation causes the inconsistent outputs on a\nmodel containing Flatten and ReduceL1 operation. Devel-\nopers have not confirmed this bug, but it has been fixed in the\nnext released version of the compiler.\nGlow-1 is a bug in the compiler backend due to that\nReduceSum operation in the input model contains multi axis\ninputs and attributes.\nSophGo-1 is a bug in the compiler frontend. The compiler\ndoes not support that the weight tensor of Conv operation\nis an input tensor. Developers do not fix this bug due to\nthe reason that they suppose users take this parameter as a\nconstant because this parameter usually does not change after\ndeployment.\n9https://github.com/apache/tvm/issues/7262#issuecomment-911968074\nSophGo-2 is a bug in the compiler backend. If the inputs\nof Mean operation are the same tensor, then the calculation of\nmean operation will throw exception due to the naming error.\nSophGo-3 is a bug in the compiler backend. ReduceProd\noperation will register a buffer, but buffer size has not been\nassigned which leads to the incorrect parameters in calculation\nand further causes the final result wrong.\nSophGo-4 is a bug in the compiler frontend due to erro-\nneous parsing for Pooling operation.\nSophGo-5 is a bug in the compiler backend due to incorrect\nimplementation on Split operation.\nSophGo-6 is a bug in the compiler backend. The compiler\nassumes the second input of PRelu holds a specific shape\nwhich is not consistent with the specification.\nSophGo-7 is a bug in the compiler backend. The compiler\nbackend incorrectly covers the input data for the calculation\non Cos operation, causing the wrong results of model’s output.\nSophGo-8, SophGo-9, and SophGo-10 are bugs in the\ncompiler frontend due to erroneous parsing for Unsqueeze\noperation, Split operation and Gemm operation. ONNX stan-\ndard of some operations has changed after version 13. The\ncompiler only implements old version and is not compatible\nwith latest standard. Developers do not fix SophGo-10 due to\nthe same reason as SophGo-1.\nSophGo-11 is a bug in the compiler backend due to\nincorrect implementation on Resize operation.\n2) False positives: Though our test generation approach is\nsound (i.e., the models generated by our approach ensures the\nvalidity), false positives may be introduced by floating point\nerrors [42], [41]. For differential testing, one of test oracles\nwe used, when it comes to checking output equivalence, false\nalarms may be produced due to floating point errors/inaccura-\ncies. To reduce false alarms, we use a high error tolerance in\ncomparison of models’ outputs (the relative error is set to 10%\nas mentioned in Section IV-B), which is similar to a parallel\nwork [21]. In our evaluation, Isra did not find false positives\ncaused by the floating-point errors.\nF. RQ3: Comparison with State-of-the-art Approaches\nWe compare Isra with three state-of-the-art approaches (i.e.,\nMuffin [16], TVMFuzz [36] and NNSmith [21]) in terms of\ncoverage and bug detection capability.\nFor the comparison on coverage, as result shown in Table I,\n(1) under the same settings, Isra* outperforms Muffin on most\nof coverage metrics except for only NTR and NSA, and\n(2) under the same settings, Isra** consistently outperforms\nNNSmith on all of coverage metrics. The reasons why Muffin\nachieves higher NTR and NSA than Isra* are as follows.\nFor NTR, this is mainly because Muffin contains a cells\nmode, which will favor generating dense graph structure which\ncontains many triples, but its DEC is still significantly low\ncompared with Isra*, due to the fewer types of operations\nin per graph (NOT). Muffin’s higher coverage on NSA\nis because Muffin inserts Reshape layers between adjacent\nlayers for patching tensor structural constraints in a hardcode\nway, leads to change tensor shape frequently. However, Muffin\nis still significantly lower than Isra on SAC, which is the\ncoverage of NSA among the whole test set.\nFor the comparison on bug detection capability, as shown\nin Table III, Isra detects more bugs than Muffin on three\ncompilers, with 1.5x, 2x, 1.22x respectively, in total 33 versus\n23. Also, Isra significantly outperforms TVMFuzz on detecting\nbugs on TVM with 18 versus 5. Isra achieves comparable re-\nsults compared to NNSmith (33 versus 32) on three compilers.\nIn addition, we also investigate the overlaps of detected bugs\namong Isra, Muffin, TVMFuzz, and NNSmith as shown in\nFigure 7. There are overlapping bugs among them as well as\ndistinct non-overlapping bugs, indicating that these approaches\nare complementary. There is no overlapping bug between\nTVMFuzz and other approaches, primarily because TVMFuzz\nfuzzes on low-level Relay IR instead of computational graphs.\nOverall, as shown in evaluation results, compared to state-\nof-the-art approaches under the same settings, Isra outper-\nforms those approaches on various coverage metrics, and\nalso achieves comparable and complementary results on bug\ndetection. The results indicates that (1) Isra is more effective\nand more efficient than existing approaches for test generation,\n(2) Isra is effective and complementary to existing approaches\nfor detecting bugs in real-world compilers.\nV. RELATED WORK\nRandom Testing and Test Generation. Random testing [26]\nsimply constructs test inputs in a random manner. For example,\nRandoop [27] and EvoSuite [11] aim to generate JUnit tests\nfor classes by incrementally combining sequences of method\ncalls. Besides many aspects of advantages, random testing\nstill faces problems including inefficiency, imprecision, and\nlack of available information to guide test generation. To test\ndeep learning compilers, our work conducts random testing\nby enhancing the effectiveness of test generation. Another test\ngeneration way is bounded-exhaustive testing. For example,\nUDITA [12] uses bounded-exhaustive testing to enumerate the\npaths through the generator with various optimizations. For\ndeep learning models, the space of computation graph and the\nshape of tensors in it can be super large, and the valid space\nis very sparse; thus, it is intolerable to enumerate all kinds of\nthe inputs by searching.\nGrammar-based Fuzzing. Fuzzing is a common approach\nfor randomly generating inputs to test software. It may gener-\nate inputs from scratch, or do mutation on a series of valid seed\ninputs. Without any knowledge of the input of the software\nunder test, generating valid inputs randomly is ineffective,\nespecially for the software such as compilers whose inputs are\nhighly-structured. To improve it, grammar-based fuzzing [14],\n[17] is proposed, which relies on a grammar specification\nto generate structured inputs, usually in context-free forms.\nDeep learning models with semantic specifications fail to be\nrepresented as a context-free grammar. Recently Padhye et\nal. [28] propose Zest, which is based on coverage-guided\nfuzzing, targeting at producing semantically valid inputs.\nHowever, Zest still requires developers to manually design a\ngenerator that can construct syntactically valid test programs.\nDifferent implementations for the generator could highly affect\nthe effectiveness of test generation, especially for languages\nwith complicated specifications such as deep learning models.\nTesting Deep Learning Toolkits. Deep learning toolkits\ninclude deep learning libraries (frameworks) and deep learning\ncompilers. The differences between them lie in their primary\nfunctions and interfaces provided to users, which result in\ndivergent emphases in designing corresponding testing ap-\nproaches.\nDeep learning libraries, such as Keras and TensorFlow, are\nprimarily used for simplifying the implementation of deep\nlearning models, they provide high-level APIs and abstrac-\ntions of pre-defined layers/models as well as optimizers, loss\nfunctions and other utilities that allow users to easily define\nand train deep learning models. To test deep learning libraries,\nLEMON [40] and Muffin [16] focus on generating parameters\nand call sequences of high-level APIs.\nDeep learning compilers, such as TVM, are designed to\ntransform deep learning models into efficient low-level code\nfor deployment and execution on different hardware devices,\nand they focus on optimizing the computational graph of the\nmodels to improve execution efficiency. To test deep learning\ncompilers, one direction is to directly generate inputs of\ndeep learning compilers, i.e., computation graphs, including\nresearch work GraphFuzzer [23] and MT-DLComp [41]; an-\nother direction is to fuzz low-level intermediate representation\nof the compiler (e.g., TVM’s compiler-specific intermediate\nrepresentation), including research work TVMFuzz [36] and\nTzer [22]. Our approach, Isra, as well as its two parallel works\nNNSmith [21] and HirGen [24] belong to the former, i.e. test\ngeneration of computation graphs.\nTo generate test inputs for deep learning toolkits, the va-\nlidity of test inputs is a critical challenge: invalid test inputs\nwill largely diminish effectiveness and efficiency of testing.\nTo address it, different techniques are proposed by existing\nresearch work. For example, Muffin [16], GraphFuzzer [23]\nand HirGen [24] are all restricted to certain types of op-\nerations and connections for generating computation graphs,\nwhich will bias the generated graphs. Specifically, Muffin [16]\nensures the semantic specification by inserting the reshape\nlayers between adjacent layers in origin models. Unfortunately,\ndoing so biases the generated computation graphs to include\nmany Reshape operations as shown in our evaluation. Graph-\nFuzzer [23] and HirGen [24] try to adjust mismatched tensor\nshapes through slicing and padding. They will also bias the\ngenerated computation graphs to include many Slice and\nPadding operations.\nNNSmith [21], as a parallel work with Isra, addresses the\nvalidity challenge by leveraging the existing constraint solver\nZ3 [5]. However, it leads to a further problem which is lack\nof diversity due to that existing constraint solvers tend to\npick boundary values for constraints. To relieve this problem,\nNNSmith tries to iteratively add extra constraints (named\n“attribute binning” [21]). When extra constraints produce an\nunsatisfiable one, NNSmith will randomly drop some of the\nconstraints and retry, until it succeeds. It results in following\ndisadvantages: (1) extra overhead for constraint solving due to\nthe iteratively retrying; (2) a biased distribution of operations\nand parameters in the generated models, the models tend to\ncontain more “simple” operations such as Add and Sub, and\nless “complicated” operations such as Conv and Gemm (as\nseen in both of the results in NNSmith’s paper [21] and our\nevaluation).\nCompared with related work, our test generation approach\novercomes the validity challenge by the domain-specific con-\nstraint solver proposed in this paper. It offers several advan-\ntages as follows:\n• The computation graphs generated by our approach\nare more diverse because our domain-specific constraint\nsolver can sample diverse operations/parameters without\nbias, as evidenced in our evaluation.\n• Our approach is more efficient due to lower computa-\ntional costs compared to other solutions such as repeat-\nedly calling the external constraint solver (as NNSmith\ndid), as evidenced in our evaluation.\n• Our approach is more scalable because our domain-\nspecific constraint solver is lightweight and backtrack-\nfree, without inherent limitations on the type and the\nsize of generated computation graphs, which is potentially\nbeneficial for other scenarios such as generating extreme\ntest cases for stress testing.\nBesides test generation for valid computation graphs, exist-\ning research work also proposes other techniques to enhance\nthe effectiveness of deep learning compiler testing, which\nare orthometric to our approach. NNSmith [21] conducts\nvalue searching for improving numeric validity with gradients.\nHirGen [24] proposes “disruptive generation” to generate\ncomputation graphs containing obvious breaks of specifica-\ntions for detecting incorrect exception handling bugs. In addi-\ntion, mutation-based approaches such as TVMFuzz [36] and\nTzer [22], conduct a series of heuristic-based mutation rules on\nseed inputs (i.e., existing models) at the compiler’s low-level\nintermediate representation. Our approach is complementary\nto these techniques.\nVI. CONCLUSION\nIn this paper, to construct diverse and semantically valid\ncomputation graphs for testing deep learning compilers, we\nproposed a new approach named Isra, including a novel\ndomain-specific solver for effectively resolving constraints on\ncomputation graphs. We have implemented and evaluated our\napproach against\nfive baselines, and also applied Isra to\ntest three real-world deep learning compilers. The evalua-\ntion results show that\n(1) Isra outperforms the baselines\nincluding two state-of-the-art approaches (Muffin [16] and\nNNSmith [21]) on coverage metrics, demonstrating Isra’s\neffectiveness in generating diverse computation graphs; (2)\nIsra performs better or as well than state-of-the-art approaches\non bug detection, the result of Isra is also complementary to\nthose from existing approaches; (3) Isra detects 24 previously\nunknown bugs in the released versions of the three compilers,\ndemonstrating its high value in practice.\nVII. ACKNOWLEDGMENTS\nThis work was partially supported by National Natural\nScience Foundation of China under Grant No. 62161146003,\nand the Tencent Foundation/XPLORER PRIZE.\nWe would like to thank Haiyue Ma, Zhengkai Wu, Chenxi\nLi for their help with improving the presentation of this work.\nREFERENCES\n[1] Krzysztof R. Apt. Principles of constraint programming. Cambridge\nUniversity Press, 2003.\n[2] Chandrasekhar Boyapati, Sarfraz Khurshid, and Darko Marinov. Korat:\nAutomated testing based on java predicates. ISSTA ’02, page 123–133,\nNew York, NY, USA, 2002. Association for Computing Machinery.\n[3] Junjie Chen, Jibesh Patra, Michael Pradel, Yingfei Xiong, Hongyu\nZhang, Dan Hao, and Lu Zhang. A survey of compiler testing. ACM\nComput. Surv., 53(1):4:1–4:36, 2020.\n[4] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q.\nYan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis\nCeze, Carlos Guestrin, and Arvind Krishnamurthy. TVM: an automated\nend-to-end optimizing compiler for deep learning.\nIn Andrea C.\nArpaci-Dusseau and Geoff Voelker, editors, 13th USENIX Symposium on\nOperating Systems Design and Implementation, OSDI 2018, Carlsbad,\nCA, USA, October 8-10, 2018, pages 578–594. USENIX Association,\n2018.\n[5] Leonardo Mendonc¸a de Moura and Nikolaj Bjørner. Z3: an efficient\nSMT solver. In C. R. Ramakrishnan and Jakob Rehof, editors, Tools\nand Algorithms for the Construction and Analysis of Systems, 14th\nInternational Conference, TACAS 2008, Held as Part of the Joint\nEuropean Conferences on Theory and Practice of Software, ETAPS\n2008, Budapest, Hungary, March 29-April 6, 2008. Proceedings, volume\n4963 of Lecture Notes in Computer Science, pages 337–340. Springer,\n2008.\n[6] Leonardo Mendonc¸a de Moura and Nikolaj S. Bjørner.\nEfficient e-\nmatching for SMT solvers.\nIn Frank Pfenning, editor, Automated\nDeduction - CADE-21, 21st International Conference on Automated\nDeduction, Bremen, Germany, July 17-20, 2007, Proceedings, volume\n4603 of Lecture Notes in Computer Science, pages 183–198. Springer,\n2007.\n[7] Rina Dechter. From local to global consistency. Artificial intelligence,\n55(1):87–107, 1992.\n[8] Xiaoting Du, Zheng Zheng, Lei Ma, and Jianjun Zhao. An empirical\nstudy on common bugs in deep learning compilers. In 2021 IEEE 32nd\nInternational Symposium on Software Reliability Engineering (ISSRE),\npages 184–195. IEEE, 2021.\n[9] Rafael Dutra, Jonathan Bachrach, and Koushik Sen.\nSmtsampler:\nefficient stimulus generation from complex SMT constraints.\nIn Iris\nBahar, editor, Proceedings of the International Conference on Computer-\nAided Design, ICCAD 2018, San Diego, CA, USA, November 05-08,\n2018, page 30. ACM, 2018.\n[10] Bassem Elkarablieh, Darko Marinov, and Sarfraz Khurshid. Efficient\nsolving of structural constraints.\nIn Barbara G. Ryder and Andreas\nZeller, editors, Proceedings of the ACM/SIGSOFT International Sympo-\nsium on Software Testing and Analysis, ISSTA 2008, Seattle, WA, USA,\nJuly 20-24, 2008, pages 39–50. ACM, 2008.\n[11] Gordon Fraser and Andrea Arcuri.\nEvosuite: automatic test suite\ngeneration for object-oriented software. In Proceedings of the 19th ACM\nSIGSOFT symposium and the 13th European conference on Foundations\nof software engineering, pages 416–419, 2011.\n[12] Milos Gligoric, Tihomir Gvero, Vilas Jagannath, Sarfraz Khurshid,\nViktor Kuncak, and Darko Marinov. Test generation through program-\nming in udita.\nIn Proceedings of the 32nd ACM/IEEE International\nConference on Software Engineering-Volume 1, pages 225–234, 2010.\n[13] Milos Gligoric, Tihomir Gvero, Steven Lauterburg, Darko Marinov,\nand Sarfraz Khurshid. Optimizing generation of object graphs in java\npathfinder.\nIn Second International Conference on Software Testing\nVerification and Validation, ICST 2009, Denver, Colorado, USA, April\n1-4, 2009, pages 51–60. IEEE Computer Society, 2009.\n[14] Patrice Godefroid, Adam Kiezun, and Michael Y Levin. Grammar-based\nwhitebox fuzzing. In Proceedings of the 29th ACM SIGPLAN conference\non programming language design and implementation, pages 206–215,\n2008.\n[15] Erich Gr¨adel, Phokion G Kolaitis, Leonid Libkin, Maarten Marx, Joel\nSpencer, Moshe Y Vardi, Yde Venema, Scott Weinstein, et al. Finite\nModel Theory and its applications. Springer, 2007.\n[16] Jiazhen Gu, Xuchuan Luo, Yangfan Zhou, and Xin Wang.\nMuffin:\nTesting deep learning libraries via neural architecture fuzzing. In 44th\nIEEE/ACM 44th International Conference on Software Engineering,\nICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022, pages 1418–1430.\nACM, 2022.\n[17] Christian Holler, Kim Herzig, and Andreas Zeller. Fuzzing with code\nfragments. In Tadayoshi Kohno, editor, Proceedings of the 21th USENIX\nSecurity Symposium, Bellevue, WA, USA, August 8-10, 2012, pages 445–\n458. USENIX Association, 2012.\n[18] Norman P. Jouppi, Cliff Young, Nishant Patil, David A. Patterson,\nGaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan\nBoden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao,\nChris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean,\nBen Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William\nGulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu,\nRobert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski,\nAlexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch,\nNaveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le,\nChris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean,\nAdriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi\nNarayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick,\nNarayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir\nSalek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snel-\nham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory\nThorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard\nWalter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-datacenter\nperformance analysis of a tensor processing unit. In Proceedings of the\n44th Annual International Symposium on Computer Architecture, ISCA\n2017, Toronto, ON, Canada, June 24-28, 2017, pages 1–12. ACM, 2017.\n[19] Vipin Kumar. Algorithms for constraint-satisfaction problems: A survey.\nAI magazine, 13(1):32–32, 1992.\n[20] Chris Leary and Todd Wang.\nXla - tensorflow, compiled, 2017.\nAccessed: 2020-04-22.\n[21] Jiawei Liu, Jinkun Lin, Fabian Ruffy, Cheng Tan, Jinyang Li, Aurojit\nPanda, and Lingming Zhang. Nnsmith: Generating diverse and valid test\ncases for deep learning compilers. In Tor M. Aamodt, Natalie D. Enright\nJerger, and Michael M. Swift, editors, Proceedings of the 28th ACM\nInternational Conference on Architectural Support for Programming\nLanguages and Operating Systems, Volume 2, ASPLOS 2023, Vancouver,\nBC, Canada, March 25-29, 2023, pages 530–543. ACM, 2023.\n[22] Jiawei Liu, Yuxiang Wei, Sen Yang, Yinlin Deng, and Lingming Zhang.\nCoverage-guided tensor compiler fuzzing with joint ir-pass mutation.\nProc. ACM Program. Lang., 6(OOPSLA1):1–26, 2022.\n[23] Weisi Luo, Dong Chai, Xiaoyue Run, Jiang Wang, Chunrong Fang, and\nZhenyu Chen.\nGraph-based fuzz testing for deep learning inference\nengines.\nIn 43rd IEEE/ACM International Conference on Software\nEngineering, ICSE 2021, Madrid, Spain, 22-30 May 2021, pages 288–\n299. IEEE, 2021.\n[24] Haoyang Ma, Qingchao Shen, Yongqiang Tian, Junjie Chen, and Shing-\nChi Cheung. Fuzzing deep learning compilers with hirgen. In Ren´e Just\nand Gordon Fraser, editors, Proceedings of the 32nd ACM SIGSOFT\nInternational Symposium on Software Testing and Analysis, ISSTA 2023,\nSeattle, WA, USA, July 17-21, 2023, pages 248–260. ACM, 2023.\n[25] Microsoft. Onnx github repository, 2017. Accessed: 2020-04-16.\n[26] Alessandro Orso and Gregg Rothermel.\nSoftware testing: a research\ntravelogue (2000–2014). In Future of Software Engineering Proceed-\nings, pages 117–132. 2014.\n[27] Carlos Pacheco, Shuvendu K Lahiri, Michael D Ernst, and Thomas\nBall. Feedback-directed random test generation. In 29th International\nConference on Software Engineering (ICSE’07), pages 75–84. IEEE,\n2007.\n[28] Rohan Padhye, Caroline Lemieux, Koushik Sen, Mike Papadakis, and\nYves Le Traon.\nSemantic fuzzing with zest.\nIn Proceedings of the\n28th ACM SIGSOFT International Symposium on Software Testing and\nAnalysis, pages 329–340, 2019.\n[29] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore:\nAutomated whitebox testing of deep learning systems. In Proceedings of\nthe 26th Symposium on Operating Systems Principles, Shanghai, China,\nOctober 28-31, 2017, pages 1–18. ACM, 2017.\n[30] Patrick Prosser. Hybrid algorithms for the constraint satisfaction prob-\nlem. Computational intelligence, 9(3):268–299, 1993.\n[31] Luyao Ren. Isra project repository, https://github.com/israproj/isra, 2022.\n[32] Andrew Reynolds, Cesare Tinelli, and Leonardo De Moura. Finding\nconflicting instances of quantified formulas in smt.\nIn 2014 Formal\nMethods in Computer-Aided Design (FMCAD), pages 195–202. IEEE,\n2014.\n[33] Andrew Reynolds, Cesare Tinelli, Amit Goel, Sava Krsti´c, Morgan\nDeters, and Clark Barrett. Quantifier instantiation techniques for finite\nmodel finding in smt.\nIn International Conference on Automated\nDeduction, pages 377–391. Springer, 2013.\n[34] Andrew Reynolds, Cesare Tinelli, Amit Goel, Sava Krstic, Morgan\nDeters, and Clark W. Barrett.\nQuantifier instantiation techniques for\nfinite model finding in SMT. In Maria Paola Bonacina, editor, Automated\nDeduction - CADE-24 - 24th International Conference on Automated\nDeduction, Lake Placid, NY, USA, June 9-14, 2013. Proceedings, volume\n7898 of Lecture Notes in Computer Science, pages 377–391. Springer,\n2013.\n[35] Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Summer Deng, Roman\nDzhabarov, James Hegeman, Roman Levenstein, Bert Maher, Nadathur\nSatish, Jakob Olesen, Jongsoo Park, Artem Rakhov, and Misha Smelyan-\nskiy. Glow: Graph lowering compiler techniques for neural networks.\nCoRR, abs/1805.00907, 2018.\n[36] Qingchao Shen, Haoyang Ma, Junjie Chen, Yongqiang Tian, Shing-Chi\nCheung, and Xiang Chen.\nA comprehensive study of deep learning\ncompiler bugs.\nIn Diomidis Spinellis, Georgios Gousios, Marsha\nChechik, and Massimiliano Di Penta, editors, ESEC/FSE ’21: 29th ACM\nJoint European Software Engineering Conference and Symposium on the\nFoundations of Software Engineering, Athens, Greece, August 23-28,\n2021, pages 968–980. ACM, 2021.\n[37] Keras Team. Keras project repository, 2022.\n[38] ONNX Team. Tensorflow-onnx project repository, 2022.\n[39] Alan Mathison Turing et al. On computable numbers, with an application\nto the entscheidungsproblem. J. of Math, 58(345-363):5, 1936.\n[40] Zan Wang, Ming Yan, Junjie Chen, Shuang Liu, and Dongdi Zhang.\nDeep learning library testing via effective model generation. In Prem De-\nvanbu, Myra B. Cohen, and Thomas Zimmermann, editors, ESEC/FSE\n’20: 28th ACM Joint European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering, Virtual Event,\nUSA, November 8-13, 2020, pages 788–799. ACM, 2020.\n[41] Dongwei Xiao, Zhibo Liu, Yuanyuan Yuan, Qi Pang, and Shuai Wang.\nMetamorphic testing of deep learning compilers.\nProc. ACM Meas.\nAnal. Comput. Syst., 6(1):15:1–15:28, 2022.\n[42] Daming Zou, Ran Wang, Yingfei Xiong, Lu Zhang, Zhendong Su, and\nHong Mei. A genetic algorithm for detecting significant floating-point\ninaccuracies. In 2015 IEEE/ACM 37th IEEE International Conference\non Software Engineering, volume 1, pages 529–539, 2015.\n",
  "categories": [
    "cs.SE"
  ],
  "published": "2023-02-02",
  "updated": "2024-12-26"
}