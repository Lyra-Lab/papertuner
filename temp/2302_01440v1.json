{
  "id": "http://arxiv.org/abs/2302.01440v1",
  "title": "Generalized Uncertainty of Deep Neural Networks: Taxonomy and Applications",
  "authors": [
    "Chengyu Dong"
  ],
  "abstract": "Deep neural networks have seen enormous success in various real-world\napplications. Beyond their predictions as point estimates, increasing attention\nhas been focused on quantifying the uncertainty of their predictions. In this\nreview, we show that the uncertainty of deep neural networks is not only\nimportant in a sense of interpretability and transparency, but also crucial in\nfurther advancing their performance, particularly in learning systems seeking\nrobustness and efficiency. We will generalize the definition of the uncertainty\nof deep neural networks to any number or vector that is associated with an\ninput or an input-label pair, and catalog existing methods on ``mining'' such\nuncertainty from a deep model. We will include those methods from the classic\nfield of uncertainty quantification as well as those methods that are specific\nto deep neural networks. We then show a wide spectrum of applications of such\ngeneralized uncertainty in realistic learning tasks including robust learning\nsuch as noisy learning, adversarially robust learning; data-efficient learning\nsuch as semi-supervised and weakly-supervised learning; and model-efficient\nlearning such as model compression and knowledge distillation.",
  "text": "arXiv:2302.01440v1  [cs.LG]  2 Feb 2023\nGeneralized Uncertainty of Deep Neural Networks:\nTaxonomy and Applications\nChengyu Dong\nUniversity of California, San Diego\ncdong@eng.ucsd.edu\nAbstract\nDeep neural networks have seen enormous success in various real-world applica-\ntions. Beyond their predictions as point estimates, increasing attention has been\nfocused on quantifying the uncertainty of their predictions. In this review, we\nshow that the uncertainty of deep neural networks is not only important in a sense\nof interpretability and transparency, but also crucial in further advancing their per-\nformance, particularly in learning systems seeking robustness and efﬁciency. We\nwill generalize the deﬁnition of the uncertainty of deep neural networks to any\nnumber or vector that is associated with an input or an input-label pair, and cata-\nlog existing methods on “mining” such uncertainty from a deep model. We will\ninclude those methods from the classic ﬁeld of uncertainty quantiﬁcation as well\nas those methods that are speciﬁc to deep neural networks. We then show a wide\nspectrum of applications of such generalized uncertainty in realistic learning tasks\nincluding robust learning such as noisy learning, adversarially robust learning;\ndata-efﬁcient learning such as semi-supervised and weakly-supervised learning;\nand model-efﬁcient learning such as model compression and knowledge distilla-\ntion.\n1\nIntroduction\nDespite the vast success of deep neural networks, their decision process is hard to interpret and is\nknown as a black box. In real-world applications, it is necessary that a decision system is not only ac-\ncurate but also trustworthy, in the sense that it must know when it is likely to make errors (Guo et al.,\n2017). The interpretability and transparency of the decision process of deep neural networks have\nthus gained increasing attention, around which the pivot is often a reliable uncertainty measure that\nusers can judge and manage the decisions. A variety of uncertainty measures and strategies to im-\nprove them have been developed so far.\nIn this review, we show that the uncertainty estimates of deep neural networks are important not\nonly in improving their trustworthiness, but also in further advancing their performance, particularly\nin terms of robustness and efﬁciency. We will ﬁrst review the possible uncertainty estimates we can\nleverage for deep neural networks. These include the classic deﬁnition of uncertainty, for example,\nthe maximum probability or entropy of the predictive distribution, as well as strategies to improve\nit such as a diverse form of ensemble techniques. These ensemble techniques are mostly unique to\ndeep neural networks, which either utilize the speciﬁc design in the network architecture such as\nMC-dropout (Gal & Ghahramani, 2016), or utilize the distinct optimization process of deep neural\nnetworks such as Snapshot ensemble (Huang et al., 2017b).\nWe will further investigate the uncertainty estimates that are deﬁned beyond the predictive distribu-\ntion. For this, we generalize the deﬁnition of uncertainty from an estimate associated with a model\nprediction to any number or vector associated with a data example, where the label can either be\nprovided or missing. We see that under such a deﬁnition, multiple intriguing properties of deep\nneural networks can be leveraged to deﬁne an uncertainty estimate. These include measures based\non inference dynamics of deep neural networks such as prediction depth (Baldock et al., 2021), and\nmeasures based on training dynamics of deep neural networks such as learning order (Arpit et al.,\n2017; Hacohen et al., 2020).\nFinally, we discuss the potential applications of these uncertainty estimates in various learning prob-\nlems, particularly when robustness and efﬁciency are of major interest. We show that in realistic\ndatasets where the labels are expensive to obtain or the label noise is pervasive, reliable uncer-\ntainty estimates can be utilized to improve the performance greatly. We also show that uncertainty\nestimates can be utilized to enhance the performance of deep neural networks under adversarial at-\ntacks (Goodfellow et al., 2015). We then move to learning problems where the computation cost is\nprohibitive. We show that reliable uncertainty estimates can improve both the training and inference\nefﬁciency of deep neural networks, when utilized in advanced efﬁcient learning techniques such as\nknowledge distillation (Hinton et al., 2015) and adaptive inference time (Graves, 2016).\nThis review will be structured as follows. In Section 2, we will introduce the necessary background\nsuch as the design, training and inference of deep neural networks, as well as the generalized def-\ninition of uncertainty. In Section 3, we will review various existing uncertainty estimates under\nsuch a deﬁnition, along with their problems and the strategies to improve them. In Section 4, we\ndemonstrate how we can use the uncertainty estimates of deep neural networks to advance their per-\nformance in robust and efﬁcient learning. Finally, Section 5 concludes our review and illuminates\nthe potential opportunities in this direction.\n2\nPreliminaries\n2.1\nDeep neural networks\nWe consider a supervised learning setting where a deep neural network fθ is deﬁned as a function\nmapping from input domain X to output domain Y, namely fθ : X →Y. A deep neural network\nusually consists of multiple feed-forward layers, where each layer l is parameterized by a set of\nweights θl. We denote the function mapping of the deep neural network up to layer L as fθ1:L.\nWith a slight abuse of notation, we denote the function mapping of all layers, or the entire deep\nneural network as fθ. Note that it is not necessarily the case that fθ1:L(x) ∈Y, namely the output\nof an intermediate layer can have higher or lower dimensionality than the output domain. During\ninference, given any unseen example input-label pair (x, y), where x ∈X and y ∈Y, a deep neural\nnetwork accepts x and produces a prediction fθ(x), where we expect fθ(x) = y.\ny = fθ(x)\nTraining a deep neural network usually requires a training sample, namely a set of input-label pairs\nD = {(xi, yi)}N\ni=1. The most commonly used method to train a deep neural network is empirical\nrisk minimization. We ﬁrst deﬁne a distance function in the output domain l : Y ×Y →R, which we\ntypically refer as the loss function. It is desired that such a distance function sufﬁces l(y1, y2) ≥0\nfor any y1, y2 ∈Y and l(y, y) = 0 for any y ∈Y. Empirical risk minimization can be deﬁned as\nθ∗= arg min\nθ\nX\n(x,y)∈D\nl(fθ(x), y),\n(1)\nnamely we seek an optimal set of model weights that minimize the distance between the network\noutputs and the labels for all training examples.\nTo solve such a minimization problem, the typically used optimization method is gradient descent\nwith multiple updates. For a total of M updates, we repeatedly calculate the gradient of each weight\nwith respect to the minimization objective and update the weights in the opposite direction of the\ngradient. In its simplest form, the update rule can be expressed as\nθt+1 = θt −α∇θ\nX\n(x,y)∈D\nl(fθt(x), y),\n(2)\nwhere α is a scalar and is typically referred to as the learning rate. Here we have denoted the network\nweights after t updates as θt.\n2\n2.2\nDeﬁnition of the generalized uncertainty\nWe deﬁne the uncertainty of a deep neural network as a function that maps a data example to any\nreal number or vector, namely cθ : X × Y →RK, where cθ means that such a function mapping\nis deﬁned by the network fθ. Note that sometimes such a function mapping can take the input only,\nwith the label missing, in which case we denote it as cθ : X →RK, with a slight abuse of notation.\nWe note that such a deﬁnition can cover multiple classical deﬁnitions of uncertainty. For example,\nin a multi-class classiﬁcation setting where each input is associated with a label selected from a set\nof K classes, a deep neural network’s conﬁdence of its prediction is simply deﬁned as the maximum\nprobability mass in its output vector, namely cθ(x) = maxk fθ(x)[k], where fθ(x)[k] denotes the\nk-th entry of the prediction vector fθ(x). Here we have in fact interpreted the prediction vector in\na probabilistic sense. Let 1k=k′ ∈Y ⊆RK denotes the vector where only the k′-th entry is 1 and\nothers are 0, we then have Ω= {1k=1, 1k=2, · · · , 1k=K} deﬁned as the sample space, and fθ(x)[k′]\ndenotes the probability measure of the singleton set {1k=k′}. One may recognize that this is the\nstandard deﬁnition of a categorical distribution.\nNote that in order to use the prediction vector as a valid probability measure, it is desired that\nthe prediction vector lies in a (K −1)-dimensional simplex, namely fθ(x) ∈∆K−1 := {y ∈\n[0, 1]K | ∥y∥1 = 1}. We will review more typical deﬁnitions of the uncertainty of deep neural\nnetworks in later sections.\n3\nUncertainty Estimation of Deep Neural Networks\nStandard deep neural networks are usually deterministic inference systems, namely the prediction on\na given input will not vary for multiple forward passes. It is possible to inject inherent randomness\ninto a deep model’s inference process by specifying a prior distribution over its weights θ. By the\nBayesian theorem, the uncertainty of a prediction can then be captured by the posterior distribution\nof the prediction conditioned on the model weights. Such a deep neural network variation is called\na Bayesian Neural Network (BNN) (Denker et al., 1987; Tishby et al., 1989; Buntine & Weigend,\n1991). It is rather obvious that the inference of such neural networks would be computationally\nexpensive, as multiple samplings of the model weights are required for one single inference. There\nhave been a variety of attempts to make BNNs computationally tractable, such as Laplace approxi-\nmation (Bridle et al., 2011) and Markov chain Monte Carlo (MCMC) methods (Neal, 1995).\nIn this review, we will focus only on the uncertainty of standard deterministic deep neural networks,\nwhile skipping recent works on BNNs. The reasons are two folds. First, in practice, BNNs are\nstill difﬁcult to implement and computationally slow (Lakshminarayanan et al., 2017). Second and\nmore importantly, in this review, we are not seeking the role of uncertainty in the transparency of the\nmodel decision process, but rather in aiding the model training. For example, a typical application\nof uncertainty is to select high-quality unlabeled data that is most relevant to the current task from a\nlarge corpus. We will discuss more of such applications in the following sections.\n3.1\nClassic uncertainty\nIn Section 2.2, we have mentioned the classic way to interpret the predictive vector of a deep neural\nnetwork as a probability distribution. In practice, to create a valid probabilistic vector, we often\nneed to apply a non-linear activation function on top of the network outputs. Let the original output\nvector of a network as z ∈RK, and σ : RK →[0, 1]K as a non-linear function. We will then\nmake fθ(x) = σ(z) for an input x, where z is often referred as the logits. For example, in a binary\nclassiﬁcation task, we will use the sigmoid function, namely\nσ(z) =\n1\n1 + exp(−z),\nand in a multi-class classiﬁcation task, we will use the softmax function, namely\nσ(z)[k] =\nexp(z[k])\nPK\nk=1 exp(z[k])\n.\nA valid probabilistic vector can be directly interpreted as the uncertainty of the model prediction.\nOne can also transform such an uncertainty vector into a number, which will be particularly handy\n3\nin obtaining the ranking of a set of data examples based on an uncertainty measure. Various trans-\nformations are used in the literature, typically including maximum probability, entropy, and margin.\n• Maximum probability: cθ(x) = maxk fθ(x)[k],\n• Entropy: cθ(x) = −fθ(x) · log fθ(x),\n• Margin: cθ(x) = maxk fθ(x)[k] −maxk̸=k′ fθ(x)[k], where k′ = arg maxk fθ(x)[k].\n3.2\nEvaluation of classic uncertainty\nThe desired properties of an uncertainty estimate may vary signiﬁcantly across the tasks of interest,\nsometimes even orthogonally. Consequently, a variety of uncertainty evaluation metrics exists in the\nliterature. Here we roughly group them into three types based on the property desired.\nRecovering the true distribution.\nAssuming the data examples are sampled from a joint distri-\nbution PX,Y (x, y) = PX(x)PY |X(y|x) deﬁned over X × Y. Given any input x, it is desired that\nthe predictive uncertainty, namely the probabilistic vector fθ(x) produced by a deep neural network,\ncan recover the true conditional distribution PY |X(y|x). Note that such recovery is a stricter con-\ncern compared to accuracy (Lakshminarayanan et al., 2017). A network’s prediction can be very\naccurate, yet signiﬁcantly deviate from the true conditional distribution. A typical example is that\nthe network always outputs the one-hot label on an input x.\nTo evaluate the quality of the recovery, one can use commonly seen loss functions such as nega-\ntive log-likelihood (NLL) loss and mean squared error (MSE). Interestingly, it is known in meteo-\nrology that these two loss functions have good properties 1, or they are known as proper scoring\nrules (Gneiting & Raftery, 2007). A proper scoring rule is one loss function where l(p, q) ≥l(q, q)\nwith equality if and only if p = q. This means that minimizing the distance between the predictive\nuncertainty and one-hot labels can in fact recover the true conditional distribution. To see that for\nNLL, we can apply the Gibbs inequality, namely\n−E(x,y)∼P (x,y)1y·log fθ(x) = −Ex∼P (x)P(y|x)·log fθ(x) ≥−Ex∼P (x)P(y|x)·log P(y|x), (3)\nwhere the equality establishes if and only if fθ(x) = P(y|x). To see that for MSE, we can decom-\npose the MSE and ﬁnd that\n∥1y −fθ(x)∥2\n2 = ∥1y −P(y|x)∥2\n2 + ∥P(y|x) −fθ(x)∥2\n2 ≥∥1y −P(y|x)∥2\n2,\n(4)\nwhere the equality establishes if and only if fθ(x) = P(y|x). Therefore, in practice when there are\nonly one-hot labels available, we can still use NLL or MSE to quantify the quality of the uncertainty\nin terms of its recovery to the true conditional distribution.\nCalibration.\nLoosely speaking, calibration of deep neural networks implicates the network un-\ncertainty should reﬂect the probability that it makes errors (Guo et al., 2017). Here we are only\ninterested in the network’s argmax prediction, which we denote as yθ = arg maxk fθ(x)[k] for\nsimplicity. We are also only interested in the uncertainty as a real number between 0 and 1, which\nreﬂects the probability of the pointwise prediction yθ being correct. Usually, this is simply the maxi-\nmum probability, namely cθ(x) = maxk fθ(x)[k]. Formally, calibration then desires that (Guo et al.,\n2017)\nP(yθ = y | cθ(x) = p) = p, ∀p ∈[0, 1].\n(5)\nHere the left denotes the accuracy of the network prediction on all examples where the network\nreports uncertainty of p, and the right is the value of the uncertainty. Therefore, calibration means\nthat the uncertainty of a prediction should genuinely match the probability of correctness of such\nprediction. It may help the understanding of calibration if we interpret the probability P as the limit\nof the frequency. Therefore, an alternative way to formalize calibration is (Kuleshov et al., 2018),\nlim\nN→∞\nPN\ni=1 1(yθ = yi) · 1(cθ(xi) = p)\nPN\ni=1 1(cθ(xi) = p)\n= p,\n(6)\nwhere 1(·) is the indicator function.\n1MSE is also known as the Brier score in meteorology\n4\nA legacy issue is that, calibration of deep neural networks is in fact a weaker requirement of\nthe model uncertainty compared to the deﬁnition of calibration in standard statistical terminol-\nogy (Zadrozny & Elkan, 2001), which can be denoted as\nP(y ∈·|fθ(x) = p) = p, ∀p ∈∆K−1,\n(7)\nnamely for any predictive distribution of the network, the label should distribute exactly as that\npredictive distribution. Note that, although a true model producing the true conditional distribution\nis certainly calibrated, a (statistically) calibrated model does not necessarily have to recover the\ntrue conditional distribution and a model that is not close to the true model can nevertheless be\ncalibrated (Vaicenavicius et al., 2019).\nTo measure the uncertainty quality in terms of calibration, one can use the difference in expectation\nbetween the accuracy and the uncertainty, namely\nEp [|P(yθ = y | cθ(x) = p) −p|] .\n(8)\nIn practice, one can report the Expected Calibration Error (ECE) (Naeini et al., 2015), which approx-\nimates the expectation by binning. Speciﬁcally, the predictions on all test examples are partitions\ninto several equally-spaced bins, where predictions with similar uncertainty will be assigned into the\nsame bin. We can then approximate P(yθ = y | cθ(x) = p) in Equation (8) by the faction of correct\npredictions in each bin, and approximate p in Equation (8) by the average uncertainty in this bin.\nWhen the calibration of all-class predictions instead of the argmax prediction is of interest, one can\nalso use the Static Calibration Error (SCE) (Nixon et al., 2019) to quantify the uncertainty quality.\nAll-class calibration measures are often more effective in assessing the calibration error (Nixon et al.,\n2019). Other variation includes the adaptive Expected Calibration Error (aECE) (Nixon et al., 2019),\nwhich partitions the predictions into several bins with an equal number of predictions in each bin.\nThis can be more robust to the number of bins (Patel et al., 2021), since the uncertainty distribution\nis often far from a uniform one (Guo et al., 2017), and the number of bins is critical in evaluating\nthe calibration genuinely (Kumar et al., 2019).\nOrdinal ranking.\nIn many practical settings, the absolute distance between model uncertainty and\nthe probability of correctness is often not necessary. Instead, the ranking of a set of predictions based\non the uncertainty measure is more important. The major objective here is to distinguish correct\nfrom incorrect predictions. Therefore it is desired that correct predictions have higher conﬁdence\nestimates than incorrect predictions. Formally, a prefect ordinal ranking means that (Moon et al.,\n2020)\ncθ(xi) ≤cθ(xj) ⇐⇒P(yθ,i = yi|xi) ≤P(yθ,j = yj|xj).\n(9)\nTo measure the uncertainty quality in terms of ordinal ranking, one can specify an uncertainty\nthreshold, such that predictions with uncertainty above this threshold are regarded as correct pre-\ndictions. However, this often rises the problem of a trade-off between false negatives and false\npositives (Hendrycks & Gimpel, 2017). For a threshold-free evaluation, one can use metrics such as\nthe Area Under the Receiver Operating Characteristic curve (AUROC) (Davis & Goadrich, 2006),\nthe Area Under the Precision-Recall curve (AURC) (Manning & Schütze, 2002) and the Area under\nthe Risk-Coverage curve (AURC) (Geifman et al., 2019). The underlying idea of all these measures\nis to aggregate the accuracy under all possible thresholding of the set of predictions.\nOrdinal ranking has a wide variety of applications in practice. For example, uncertainty with ac-\ncurate ranking can effectively identify those examples that come from a distribution substantially\ndifferent from the distribution that the model is trained on. Such examples are known as the out-of-\ndistribution (OOD) examples. Ordinal ranking is also important in active learning (Settles, 2009),\nwhere the goal is to build a model knowing which examples should be labeled to improve its\nperformance. Quality uncertainty ranking can thus greatly reduce human labeling efforts. Ordi-\nnal ranking is also crucial in selective classiﬁcation (Geifman & El-Yaniv, 2017) or failure predic-\ntion (Hendrycks & Gimpel, 2017; Hecker et al., 2018), where the goal is to reject some predictions\nin test time that are likely to be incorrect. The rejected examples can be passed on to backup infer-\nence systems or humans, such that the overall prediction accuracy can be greatly improved. We will\ndiscuss more applications in later sections and frequently revisit the ranking measure of uncertainty.\n3.3\nProblems and improvement of classic uncertainty\nIt is well-known that the classic uncertainty of deep neural networks may have major drawbacks.\nIt may be poorly calibrated (Guo et al., 2017), yields inconsistent ranking (Corbière et al., 2019),\n5\nand is vulnerable to perturbations such as adversarial attack (Szegedy et al., 2014; Goodfellow et al.,\n2015) and dataset shifts (Hendrycks & Gimpel, 2017; Ovadia et al., 2019).\nTo overcome these drawbacks of classic uncertainty, a variety of methods have been proposed, which\ncan be roughly divided into two veins. The ﬁrst vein is called post-processing, namely the uncer-\ntainty is rectiﬁed after a model is trained. In this case, a validation set is often needed. The second\nvein can be broadly referred to as regularization, namely the training process is modiﬁed to take into\nconsideration. not only the accuracy but also the uncertainty.\nPost-processing.\nThe very ﬁrst and also the simplest post-processing method for deep neural\nnetworks is probably temperature scaling (Guo et al., 2017). The idea is to insert a hyperparameter\ncalled temperature in the softmax function of a trained model and ﬁne-tune it on a validation set.\nThe softmax function now becomes\nσ(z; T )[k] =\nexp(z[k]/T )\nPK\nk=1 exp(z[k]/T )\n.\n(10)\nWhen T is larger, the new predictive distribution will become softer (higher entropy), which may\nalleviate the typical problem of deep neural network’s uncertainty, namely being over-conﬁdent\nand inappropriately close to the one-hot label. Temperature scaling has been shown to be quite\neffective in improving the uncertainty, particularly in terms of calibration, outperforming more so-\nphisticated calibration methods such as histogram binning (Zadrozny & Elkan, 2001), Isotonic re-\ngression (Zadrozny & Elkan, 2002) and Platt scaling (Platt, 1999). On top of its simplicity and\neffectiveness, another reason for the popularity of temperature scaling is probably that temperature\nscaling will preserve the model’s accuracy albeit improving the uncertainty, because scaling the log-\nits by a scalar will not change the argmax of the predictive vector. Note that despite its success in\ncalibration, temperature scaling cannot improve the ordinal ranking of the model uncertainty. The\nreason is similar as the logit scaling is universal to all examples.\nMany variations of temperature scaling have been proposed to further enhance its effectiveness. For\nexample, one can perform the temperature scaling for each class in a one-vs-all manner, despite sac-\nriﬁcing the accuracy since the argmax label is no longer preserved (Kull et al., 2019). Multiple tem-\nperature scaling methods can be ensembled to achieve better uncertainty quality (Zhang et al., 2020).\nOne can also combine temperature scaling with other calibration methods such as histogram binning\nto get a theoretical guarantee on the calibration error (Kumar et al., 2019). Liang et al. (2018) ob-\nserved that adding small adversarial perturbation to the input after temperature scaling can further\nimprove the ranking of the uncertainty and thus better separate in- and out-of-distribution exam-\nples. When there are data from multiple domains available, one can learn and predict the most\nproper temperature when encountering an unseen example that is likely shifted from existing distri-\nbutions (Yu et al., 2022).\nRegularization.\nThe standard deep neural network training protocol may already have some\nspeciﬁc design that favors the learning of uncertainty. For example, as also mentioned before, the\ntypically used loss functions such as NLL and MSE are in fact proper scoring rules, which can\nrecover the true conditional distribution when minimized (Lakshminarayanan et al., 2017). The un-\ncertainty learned by standard network training may be particularly effective in terms of ranking, and\ncan serve as a strong baseline for detecting misclassiﬁed and OOD examples (Hendrycks & Gimpel,\n2017). Several simple regularization methods commonly used for improving performance have been\nshown to effectively improve the model uncertainty as well. These include early stopping or more\nadvanced instance-wise early stopping (Geifman et al., 2019), label smoothing (Müller et al., 2019),\nfocal loss (Mukhoti et al., 2020), dropout (Srivastava et al., 2014) and data augmentation methods\nsuch as mixup (Thulasidasan et al., 2019) and Augmix (Hendrycks et al., 2020).\nλ ∼Beta(α, α)\nMore advanced methods for improving performance are also demonstrated to improve un-\ncertainty.\nAdversarial training (Goodfellow et al., 2015; Kurakin et al., 2017; Madry et al.,\n2018),\noriginally designed to improve the robustness of deep neural networks against\nadversarial examples (Goodfellow et al., 2015),\nhave been shown to improve the uncer-\ntainty (Lakshminarayanan et al., 2017). Knowledge distillation can also be viewed as a regular-\nization method with the aid of auxiliary models and has been shown to improve uncertainty in\na diverse form, such as self-distillation (Kim et al., 2021), dropout distillation (Bulò et al., 2016;\n6\nGurau et al., 2018) and ensemble distillation (Mariet et al., 2020). Recent years have seen a boom\nin pre-training, which trains the model on a large corpus without any labels, or in a so-called self-\nsupervised scheme. Pre-training can beneﬁt the learning of “universal representations” that transfers\nto multiple domains (Rebufﬁet al., 2017), and can be viewed as a better initialization of a deep neu-\nral network. Therefore, it is not surprising that pre-training can improve the model uncertainty\nand robustness, even when the second-stage learning (or ﬁne-tuning) is happening on a sufﬁciently\nlarge dataset where pre-training fails to improve the performance signiﬁcantly compared to directly\ntraining on it (Hendrycks et al., 2019).\nThere are also a variety of regularization methods speciﬁcally designed for uncertainty learning, such\nas penalizing low-entropy predictive distribution in the training objective (Pereyra et al., 2017), inter-\npolating the predictive distribution and the one-hot label using uncertainty score (Devries & Taylor,\n2018), incorporating OOD examples into training and enforcing the model to produce low-\nconﬁdence predictions on them (Lee et al., 2018), variance-weighted variation of label smooth-\ning (Seo et al., 2019). Moon et al. (2020) shows that penalizing the ranking difference between\nthe uncertainty yielded by predictions at one training step and the moving-averaged predictions at\nmultiple training steps can improve uncertainty learning, particularly effective for ordinal ranking.\nMaddox et al. (2019) shows that the mean and variance of network weights across multiple training\nsteps can serve as a good prior for their uncertainty distributions, thus building an efﬁcient approx-\nimation for BNNs. One can also utilize an alternative network or alternative network modules to\nspecialize in uncertainty learning while leaving the original network intact (Corbière et al., 2019;\nGeifman & El-Yaniv, 2019).\nNote that these regularization methods are not necessarily orthogonal to each other, and may even\nhurt the uncertainty or accuracy when combined together. For example, it is well-known that label\nsmoothing and knowledge distillation are not compatible with each other (Müller et al., 2019). And\nensembling (see Section 3.4) combined with data augmentation methods such as mixup can also\nhurt the model calibration (Wen et al., 2021).\n3.4\nEnsemble uncertainty\nIn this section, we speciﬁcally focus on those methods that achieve better uncertainty quality by\naggregating multiple classic uncertainty estimates. In essence, any randomness or perturbation in\nthe input, sampling, weights, and optimization of deep neural networks during training or inference\ncan be utilized to generate multiple uncertainty estimates for aggregation (Renda et al., 2019). We\nthus roughly partition the diverse ensembling methods into three families based on the origins of\nsuch randomness or perturbation.\nModel ensemble.\nProbably the most well-known uncertainty ensemble method is Monte-Carlo\nDropout (MC-dropout) (Gal & Ghahramani, 2016). The idea is to utilize dropout to randomize\nthe inference process of a network and average multiple stochastic predictions of one example to\ngenerate a better uncertainty estimate. Despite its simplicity, MC-dropout is shown to be akin to the\nGaussian process and thus can be an efﬁcient approximation of BNNs (Gal & Ghahramani, 2016),\nand has been widely used in practice. Bachman et al. (2014) generalized such an idea using any\nnetwork modules to randomize the inference process.\nAnother\nwell-known\nensemble\nmethod\nis\ntypically\nreferred\nto\nas\ndeep\nensem-\nble (Lakshminarayanan et al., 2017).\nHere the randomness originates from the initialization\nof the deep neural networks, which is typically sampled from Gaussian distributions. One can\nthus train multiple deep neural networks on the same training set, and average their predictive\ndistributions to obtain a better uncertainty estimate. It is possible to further promote the diversity\nof deep ensemble by combining the randomness in the data sampling process, e.g., bagging\nand boosting (Livieris et al., 2021).\nBut in general cases, such a combination may not neces-\nsarily improve the uncertainty estimates and sometimes hurts the performance (Lee et al., 2015;\nLakshminarayanan et al., 2017). Deep ensemble is widely used in practice, but may suffer from\nboth increased training cost and inference cost. To reduce the inference cost, one can distill the\nknowledge of the ensemble into a single and small network (Hinton et al., 2015; Mariet et al.,\n2020; Nam et al., 2021). To reduce the training cost, instead of training multiple networks, one\ncan also only train multiple modules and share other modules. For example, Kim et al. (2018)\nshows ensemble multiple attention modules trained with different attention masks can improve\nthe uncertainty for image retrieval. Wen et al. (2020) generalizes such an idea by composing each\n7\nweight matrix in a network based on the Hadamard product between a shared matrix among all\nensemble members and a low-cost matrix unique to each member.\nInput ensemble.\nWe have already mentioned the idea of using bagging or boosting to promote di-\nversity in ensemble. This on its own can in fact be viewed as input ensemble, namely the randomness\noriginates from either the sampling of the inputs, which can be utilized in training, or perturbation\nand data augmentation of the inputs, which can be utilized in both training and inference. For\nexample, during training, Nanni et al. (2019) utilizes data augmentation to build an ensemble for\nbioimage classiﬁcation while Guo & Gould (2015) utilizes data augmentation for deep ensemble in\nobject detection.\nDuring inference, aggregating by data augmentation is even more widely used, particularly in med-\nical image processing (Wang et al., 2018, 2019). Such a technique is also commonly referred to as\ntest-time augmentation (Ayhan & Berens, 2018). Empirical analyses found that test-time augmen-\ntation is quite sensitive to the data augmentation being used (Shanmugam et al., 2020). Methods\nare thus proposed to learn appropriate data augmentation, for example, that customizes for each test\ninput individually (Kim et al., 2020).\nOptimization ensemble.\nThe randomness of the optimization of a deep neural network can also\nbe utilized to generate an ensemble. Snapshot ensemble (Huang et al., 2017b) utilizes the fact that\nthe weight landscape of a deep neural network is non-convex and the weights may traverse multiple\nlocal minima during optimization. They thus propose repeatedly decreasing and increasing the\nlearning rate to let the optimization converge multiple times. Each converged network checkpoint\ncan then serve as a member of the ensemble. Yang & Wang (2020) adapts such an idea to adaptive\nlearning rate schedulers. Fast Geometric Ensembling (FGE) (Garipov et al., 2018) and Stochastic\nWeight Averaging (SWA) (Izmailov et al., 2018) share a similar idea with snapshot ensemble, albeit\nthe speciﬁc strategy to sample checkpoints along the training trajectory may differ. Hyperparameter\nensemble (Wenzel et al., 2020) is a general method for optimization ensemble which uses AutoML\nto search for multiple hyperparameters for ensembling.\n3.5\n“Deep” uncertainty\nWe now move from the classic deﬁnition of uncertainty, namely predictive distribution and its vari-\nants, to a more generalized deﬁnition of uncertainty, especially those that are unique to the training\nor inference process of deep neural networks. We roughly partition these uncertainty measures into\ntwo groups, namely those based on inference dynamics, and those based on training dynamics.\nInference dynamics.\nDuring the inference of deep neural networks, typically only the outputs\nare desired. However, it is possible to mine other forms of measures associated with the network\ninference process as uncertainty. For example, Oberdiek et al. (2018) and Lee & AlRegib (2020)\nmeasure the uncertainty of a prediction using gradient information. When predicting on a single\nexample, the gradients of the network weights with respect to the loss are calculated, where the\nlabel is simply the predicted class. The uncertainty can then be deﬁned as the norm of the gradients.\nWhen the training set or some training examples are available during inference, one can measure\nthe uncertainty by quantifying the “similarity” between the test example and the training exam-\nples (Raghu et al., 2019; Ramalho & Corbalan, 2020). In essence, such methods build a density\nestimation in the input space and thus can reject those test inputs that are likely to be off-distribution.\nvan Amersfoort et al. (2020) adapts this idea by maintaining only a set of representative training ex-\namples (or their feature vectors) called centroids. The uncertainty is then quantiﬁed as the distance\nbetween the test input and the centroid that is closest to it. Note that such uncertainty measures\ndispense with the need to predict the label. Alternatively, Jiang et al. (2018) deﬁnes the uncertainty\nas the ratio between the distance from the test input to the closest centroid and the distance from\nthe test input to the centroid associated with the class predicted by the model, which can be more\nrobust. These methods may be inherently connected to few-shot learning methods such as Pro-\ntoNet (Snell et al., 2017) where the training set is also directly used to help the inference.\nFinally, there exist some intriguing behaviors of the inference process of deep neural networks that\ncan be leveraged to quantify the uncertainty. Baldock et al. (2021) observed that through the for-\nward propagation in a deep neural network with multiple layers, the prediction of some examples\nmay already be determined after only a few layers. Here the intermediate predictions are made by k-\nnearest neighbors classiﬁers on the hidden representations. They thus deﬁne an uncertainty measure\n8\ncalled Prediction Depth (PD). It is shown that the prediction depth may be closely correlated with\nthe margin of the ﬁnal predictive distribution and examples with large prediction depth may be more\ndifﬁcult. Second, it is known that the inference of deep neural network is vulnerable to adversarial\nperturbation. It is observed that the predictions on some examples are more resistant to adversarial\nperturbation. Therefore, one can deﬁne the smallest perturbation size required to change the model’s\nprediction as an uncertainty measure (Carlini et al., 2019). Such a metric, typically referred as mini-\nmum adversarial perturbation (Carlini & Wagner, 2017) or adversarial input margin (Baldock et al.,\n2021), may also be closely correlated with the uncertainty deﬁned on the predictive distribution.\nTraining dynamics.\nDeep neural networks may exhibit even more intriguing behaviors during\ntraining. It is well-known that deep neural networks can perfectly ﬁt even pure noise (Zhang et al.,\n2017), which raises the question of whether deep networks simply “memorize” the training set even\non real datasets. However, through a careful investigation of the predictions on individual train-\ning examples, Arpit et al. (2017) ﬁnds that data examples are not learned at the same pace during\ntraining. Those real data examples are learned ﬁrst, while those random data, either with random\ninput or random labels, are learned late. Further, they also found that simple data examples are\nlearned earlier than those difﬁcult data examples. These observations demonstrate that deep neural\nnetworks are not simply memorizing data since they appear to be aware of the content and semantics.\nHacohen et al. (2020) further shows such a learning order of training examples is consistent across\ndifferent random initializations of a network and different model architectures. Even more intrigu-\ningly, they ﬁnd that such a consistent learning order is not observed for non-parametric classiﬁers\nsuch as AdaBoost. They also observed that when trained on synthetic datasets where the images\nare different rotation or colorization of Gabor patches, such a consistent learning order disappears\nas well. They thus hypothesize that such an intriguing behavior may originate from the interplay\nbetween deep neural networks and natural datasets with recognizable patterns and semantics.\nToneva et al. (2019) observed that certain training examples are frequently forgotten during training,\nwhich means that they can be ﬁrst predicted correctly, then incorrectly. The frequency of such for-\ngotten events is shared across different neural architectures. When removing those least forgettable\nexamples from training, model performance can be largely maintained Nevertheless, Dong et al.\n(2021a) shows an opposite trend exists in adversarial training, where those most forgettable exam-\nples 2 may be removed without degrading performance, and sometimes even improving it. The\nvariance of the gradient of a data example during training is also shown to be strongly correlated\nwith its difﬁculty (Agarwal & Hooker, 2022).\n4\nUtilize Uncertainty for Better Performance\nStarting from this section, we demonstrate that the uncertainty of deep neural networks can be\nutilized to improve the robustness and efﬁciency of learning systems in a variety of realistic applica-\ntions.\n4.1\nUncertainty for robust learning\nWe ﬁrst focus on the application of uncertainty in robust learning. Here we will skip the detection\nof OOD examples as it is often a standard application of model uncertainty. Instead, we will discuss\nhow to utilize uncertainty in learning with noisy labels and adversarially robust learning.\nLearning with noisy labels.\nOne straightforward idea for learning with noisy labels is to identify\nthose labels in the training data that are likely to be incorrect. This naturally calls the necessity of un-\ncertainty estimates, which is often referred to as Conﬁdence Learning (CL) (Northcutt et al., 2021)\nin noisy learning regime. Classic uncertainty can, of course, be utilized to identify noisy labels, but\nmay be inferior since sufﬁciently trained deep neural networks can memorize the labels and thus\nbe over-conﬁdent. To combat this, one can simply perform early stopping to obtain a better uncer-\ntainty (Liu et al., 2020a). This implicitly leverages the training dynamics of deep neural networks,\nnamely noisy data will be learned late during training. By further combining the observation of\nprediction depth, namely noisy data will be learned in later layers of a deep neural network, one can\nearly stop different parts of the network at different training checkpoints. In speciﬁc, early layers\nwill be trained ﬁrst and later layers will be progressively trained with few epochs, while the early\n2Here a correct prediction is deﬁned under adversarial perturbation.\n9\nlayers will be frozen (Bai et al., 2021). Xia et al. (2021) further exploits this idea by dividing all\nnetwork weights into those that are important for ﬁtting clean labels and those that are important for\nﬁtting noisy labels based on gradient norm. The former weights are updated regularly while the lat-\nter are simply penalized with weight decay. This can be viewed as early stopping the training of the\nnetwork in a parameter-wise manner. Han et al. (2020) also considers suppressing the learning on\ndata examples that are likely to have noisy labels during training to avoid memorizing noisy labels.\nInstead of seeking better output uncertainty estimates, several methods directly utilize the uncer-\ntainty yielded by training dynamics of deep neural networks as a metric to identify noisy labels.\nFor example, Anonymous (2023) proposes using Time-Consistency Prediction (TCP) to select clean\ndata, which is simply the stability of the prediction on an example throughout training. Slightly\ndifferently, Pleiss et al. (2020) proposes to use the averaged margin of an example through training\nto select clean data.\nOn top of detecting noisy labels, uncertainty estimates are also important in more advanced and\nsophisticated noisy learning methods. For example, Co-teaching (Han et al., 2018) trains two net-\nworks simultaneously where one network is trained on the potentially clean labels selected by its\npeer network in a mini-batch. A reasonably good uncertainty estimate for noisy labels is necessary\nhere. DivideMix (Li et al., 2020) further develops this idea by using one network to divide the train-\ning set into a clean partition and a noisy partition. The peer network is trained on the clean partition,\nalong with the noisy partition without labels in a semi-supervised manner. DivideMix has achieved\nstate-of-the-art on multiple noisy learning benchmarks.\nAdversarially robust learning.\nAdversarial training (Goodfellow et al., 2015; Madry et al., 2018)\nis so far the most effective way to enhance the robustness of deep neural networks against adversar-\nial examples. However, the robust accuracy achieved on small benchmark datasets such as CIFAR-\n10 (Krizhevsky, 2009) or CIFAR-100 (Krizhevsky, 2009) is still unsatisfactory, not to mention larger\ndatasets such as ImageNet (Russakovsky et al., 2014), especially against those strong adversarial\nattacks such as AutoAttack (Croce & Hein, 2020). Surprisingly, recent studies found that robust-\nness achieved by adversarial training can be greatly boosted if the deep neural networks are trained\non additional data, which is either selected from large unlabeled data corpus (Uesato et al., 2019;\nCarmon et al., 2019) or simply generated by generative models (Sehwag et al., 2021; Gowal et al.,\n2021), thus requiring minimal human effort. Because those additional data examples may be far\naway from the original data distribution, selecting high-quality additional data is demonstrated to\nbe crucial in this process (Uesato et al., 2019; Gowal et al., 2020). The typical metric used to select\nadditional data is the classic uncertainty score yielded by a classiﬁer trained on the in-distribution\nclean data. Dong et al. (2021a) suggests that it may be better to use training stability, namely the\nfrequency of predictions on an example being correct throughout training. However, how to build\nan uncertainty metric to select high-quality additional data remains an open and important problem.\nSome intriguing problems in adversarially robust learning may also require learning good uncer-\ntainty estimates. For example, robust overﬁtting (Rice et al., 2020) is a well-known problem of\nadversarial training, which refers to the phenomenon that during adversarial training, the robust ac-\ncuracy on the test set will unexpectedly start to decrease after a certain number of training steps.\nSuch overﬁtting occurs consistently across different datasets, training settings, adversary settings,\nand neural architectures. However, when conducting standard training on the same dataset, such an\noverﬁtting phenomenon is not observed. Recent work shows that robust overﬁtting may originate\nfrom the label noise implicitly exists in adversarial training, which is induced by the mismatch be-\ntween the true conditional distribution and the label distribution of the adversarial examples used\nfor training (Dong et al., 2021b). Therefore, a straightforward solution to robust overﬁtting is to\nrecover the true conditional distribution using uncertainty estimates generated by a model, and is\ndemonstrated to signiﬁcantly alleviate robust overﬁtting (Chen et al., 2021).\n4.2\nUncertainty for efﬁcient learning\nIn this section, we show that the uncertainty of deep neural networks can be utilized to improve\nefﬁciency of deep neural networks. Here we focus on two types of efﬁciency concerns: data efﬁ-\nciency, namely to reduce the human effort on data annotation in training deep neural networks, and\nmodel efﬁciency, namely to reduce the computation cost of the training and inference of deep neural\nnetworks.\n10\n4.2.1\nData efﬁciency\nSemi-supervised learning.\nSemi-supervised learning tackles the challenge in real-world appli-\ncations where only a limited number of labeled data is available, while the vast majority of data is\nunlabeled. A natural solution to semi-supervised learning is incorporating the labels predicted by\na classiﬁer on unlabeled data into training, which are typically referred to as pseudo-labels (Lee,\n2013). Such a process can be repeatedly conducted when a better classiﬁer is trained after more\npseudo-labeled data is available, which is known as bootstrapping or self-training (McClosky et al.,\n2006).\nHowever, since the pseudo-labels predicted by a classiﬁer are very likely to be incorrect, it is\ncrucial to deﬁne a reliable uncertainty measure to select those correct pseudo-labels. Classic un-\ncertainty can be utilized here but may inevitably suffer from over-conﬁdence (Guo et al., 2017)\nand memorization Zhang et al. (2017). The recent development of self-training has seen an in-\ncreasing trend of “uncertainty-aware” methods. For example, instead of using classic uncertainty,\nMukherjee & Awadallah (2020) uses MC-dropout to select pseudo-labels for self-training. They\nalso design a loss function that incorporates the uncertainty on the correctness of pseudo-labels into\ntraining, where those selected pseudo-labels that are more likely to be correct will be more focused\non. Rizve et al. (2021) further shows that self-training using MC-dropout as an uncertainty measure\nalong with other careful designs can compete with much more sophisticated semi-supervised learn-\ning methods such as consistency regularization (Laine & Aila, 2016; Tarvainen & Valpola, 2017)\nand mixed methods (Berthelot et al., 2019), albeit enjoying the advantage that no data augmentation\nis explicitly required. Zhou et al. (2020) uses the consistency of the model prediction through train-\ning as an uncertainty measure in self-training. The hypothesis here is those model predictions that\nare largely invariant during training are more likely to be correct. This again is reminiscent of the\nintriguing training dynamics of deep neural networks as mentioned before.\nWeakly-supervised learning.\nWeakly-supervised learning is even more challenging than semi-\nsupervised learning in that the limited number of labels are not absolutely correct. In most cases,\nthey are generated by a set of pre-deﬁned rules or provided by annotators without domain exper-\ntise, which are referred to as weak supervision. Under such circumstances, a reliable uncertainty\nestimate becomes more important in not only selecting pseudo-labels predicted by classiﬁers on\nthe unlabeled set, but also selecting those labels given by weak supervision that are likely to be\ncorrect. Mekala et al. (2022) provides a comprehensive analysis of uncertainty estimates in weakly-\nsupervised text classiﬁcation. They found that among multiple uncertainty measures including clas-\nsic uncertainty, prediction stability and MC-dropout, the learning order performs the best for select-\ning pseudo-labels given by weak supervision. As mentioned before, the learning order here refers\nto the consistent order of learning real and noisy data by deep neural networks during training. Be-\ncause incorrect pseudo-labels are learned late by the model, the epoch at which the model prediction\nmatches the pseudo-label can be an effective metric to distinguish correct and incorrect labels.\n4.2.2\nModel efﬁciency\nModel compression and Knowledge distillation.\nTo excel in realistic tasks, deep neural networks\noften have to be excessively large in capacity. This brings signiﬁcant computation burden in both\ntraining deep neural networks and using deep neural networks for predictions. How to compress\na large deep neural network while maintaining its performance is thus gaining increasing attention.\nOne well-known method to compress deep neural networks is knowledge distillation (Hinton et al.,\n2015), namely a small network called student is trained using the predictions provided by a large\nnetwork called teacher, rather than using the original labels in the training set. Despite its success,\nwhy teacher predictions can help student learning has always been a mystery. A recent ﬁnding, that\nthe student predictions on the training or test set can often disagree with the teacher predictions on\na large number of examples (Stanton et al., 2021), further mystiﬁes knowledge distillation.\nToward better understanding and improving knowledge distillation, a signiﬁcant effort in theoretical\nanalyses has been made in recent works. In speciﬁc, Menon et al. (2021) shows that the predictive\ndistribution provided by the teacher can improve the generalization of students because the predic-\ntive distribution is a better approximation to the true conditional distribution than one-hot labels,\nwhere true conditional distribution as supervision can reduce the variance. Dao et al. (2021) shows\nthat the distance between teacher predictions and the true conditional distribution can directly bound\nthe student accuracy. Based on this understanding, to improve knowledge distillation we essentially\n11\nrequire the teacher to learn better uncertainty estimates in terms of its recovery of the true condi-\ntional distribution. Recent work has thus proposed to directly optimize the teacher to learn the true\nconditional distribution, which is dubbed as student-oriented teacher training and has achieved better\nstudent performance Dong et al. (2022).\nAdaptive inference time.\nDeep neural networks often consist of a great many layers. The infer-\nence process can thus be computationally prohibitive as the calculation of each layer has to be made\nsequentially, one after another. An idea to reduce the inference cost is to early stop the inference pro-\ncess of some examples when the outputs of the intermediate layers are already informative enough\nto make predictions, which is known as adaptive inference time in sequence processing and classi-\nﬁcation (Graves, 2016). Such a strategy leverages the inference dynamics of deep neural networks\nwhere easy examples may have a small prediction depth.\nTo ensure an accurate early stopping, a reliable uncertainty estimate is important here to determine\nwhen the early predictions are likely to be correct. The entropy of the predictive distribution at early\nclassiﬁcation heads is widely used in adaptive inference methods across applications in computer\nvision and natural language processing, albeit the speciﬁc designs of the network architecture or\ntraining strategies differ. BranchyNet (Teerapittayanon et al., 2016) inserts multiple early-exit clas-\nsiﬁcation heads between intermediate layers and trains the network to minimize the weighted sum of\nthe loss functions deﬁned at all classiﬁcation heads. Multi-Scale DenseNet (MSDNet) (Huang et al.,\n2017a) reﬁnes this idea by altering the network architecture such that feature representations at mul-\ntiple scales can be utilized jointly to determine an early exit. In sequence classiﬁcation where trans-\nformers dominate, adaptive inference time can be built into the network architecture (Dehghani et al.,\n2018; Xin et al., 2020). FastBert (Liu et al., 2020b) improves the early-exit accuracy by introducing\nself-distillation on intermediate classiﬁcation heads. Schwartz et al. (2020) improves the uncertainty\nestimate for early exits by using temperature scaling to calibrate the predictive distribution and\nachieves a better speed-accuracy trade-off.\n5\nConclusion\nIn this survey, we review a wide spectrum of uncertainty measures we can deﬁne for deep neural\nnetworks. These include the classic deﬁnitions of uncertainty such as those based on the predictive\ndistribution, and those deﬁnitions of uncertainty that are closely connected to the training and infer-\nence dynamics of deep neural networks. We show that these uncertainty measures can be leveraged\nin realistic applications across computer vision and natural language processing, to improve the ro-\nbustness and efﬁciency of learning systems. We believe there are more scenarios where reliable\nuncertainty estimates are crucial to the performance. We also believe the increasingly popular use\ncases of uncertainty estimates beyond interpretability and transparency will in turn facilitate more\nopportunities in uncertainty learning of deep neural networks.\nReferences\nAgarwal, C. and Hooker, S.\nEstimating example difﬁculty using variance of gradients.\n2022\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10358–10368,\n2022.\nAnonymous. A time-consistency curriculum for learning from instance-dependent noisy labels. In\nOpenreview, 2023.\nArpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M. S., Maharaj, T., Fischer,\nA., Courville, A. C., Bengio, Y., and Lacoste-Julien, S. A closer look at memorization in deep\nnetworks. ArXiv, abs/1706.05394, 2017.\nAyhan, M. S. and Berens, P. Test-time data augmentation for estimation of heteroscedastic aleatoric\nuncertainty in deep neural networks. 2018.\nBachman, P., Alsharif, O., and Precup, D. Learning with pseudo-ensembles. ArXiv, abs/1412.4864,\n2014.\nBai, Y.-L., Yang, E., Han, B., Yang, Y., Li, J., Mao, Y., Niu, G., and Liu, T. Understanding and im-\nproving early stopping for learning with noisy labels. In Neural Information Processing Systems,\n2021.\n12\nBaldock, R. J. N., Maennel, H., and Neyshabur, B. Deep learning through the lens of example\ndifﬁculty. In Neural Information Processing Systems, 2021.\nBerthelot, D., Carlini, N., Goodfellow, I. J., Papernot, N., Oliver, A., and Raffel, C. Mixmatch: A\nholistic approach to semi-supervised learning. ArXiv, abs/1905.02249, 2019.\nBridle, J. S., Cheeseman, P. C., Fels, S. S., Gull, S. F., Herz, A. V. M., Hopﬁeld, J. J., Kerns, D.,\nKnutsen, A., Koerner, D., Lewicki, M. S., Loredo, T. J., Luttrell, S. P., Meir, R., Miller, K.,\nMitchell, M., Neal, R. M., Nowlan, S. J., Robinson, D. E., Rose, K., Sibisi, S., Skilling, J., and\nSompolinsky, H. Bayesian methods for adaptive models. 2011.\nBulò, S. R., Porzi, L., and Kontschieder, P. Dropout distillation. In International Conference on\nMachine Learning, 2016.\nBuntine, W. L. and Weigend, A. S. Bayesian back-propagation. Complex Syst., 5, 1991.\nCarlini, N. and Wagner, D. A. Towards evaluating the robustness of neural networks. 2017 IEEE\nSymposium on Security and Privacy (SP), pp. 39–57, 2017.\nCarlini, N., Erlingsson, Ú., and Papernot, N. Distribution density, tails, and outliers in machine\nlearning: Metrics and applications. ArXiv, abs/1910.13427, 2019.\nCarmon, Y., Raghunathan, A., Schmidt, L., Liang, P., and Duchi, J. C. Unlabeled data improves\nadversarial robustness. ArXiv, abs/1905.13736, 2019.\nChen, T., Zhang, Z. A., Liu, S., Chang, S., and Wang, Z. Robust overﬁtting may be mitigated by\nproperly learned smoothening. In International Conference on Learning Representations, 2021.\nCorbière, C., Thome, N., Bar-Hen, A., Cord, M., and Pérez, P. Addressing failure prediction by\nlearning model conﬁdence. ArXiv, abs/1910.04851, 2019.\nCroce, F. and Hein, M. Reliable evaluation of adversarial robustness with an ensemble of diverse\nparameter-free attacks. ArXiv, abs/2003.01690, 2020.\nDao, T., Kamath, G. M., Syrgkanis, V., and Mackey, L. W. Knowledge distillation as semiparametric\ninference. ArXiv, abs/2104.09732, 2021.\nDavis, J. and Goadrich, M. H. The relationship between precision-recall and roc curves. Proceedings\nof the 23rd international conference on Machine learning, 2006.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. Universal transformers. ArXiv,\nabs/1807.03819, 2018.\nDenker, J. S., Schwartz, D. B., Wittner, B. S., Solla, S. A., Howard, R. E., Jackel, L. D., and Hopﬁeld,\nJ. J. Large automatic learning, rule extraction, and generalization. Complex Syst., 1, 1987.\nDevries, T. and Taylor, G. W. Learning conﬁdence for out-of-distribution detection in neural net-\nworks. ArXiv, abs/1802.04865, 2018.\nDong, C., Liu, L., and Shang, J. Data quality matters for adversarial training: An empirical study.\n2021a.\nDong, C., Liu, L., and Shang, J. Label noise in adversarial training: A novel perspective to study\nrobust overﬁtting. 2021b.\nDong, C., Liu, L., and Shang, J. Soteacher: A student-oriented teacher network training framework\nfor knowledge distillation. ArXiv, abs/2206.06661, 2022.\nGal, Y. and Ghahramani, Z. Dropout as a bayesian approximation: Representing model uncertainty\nin deep learning. ArXiv, abs/1506.02142, 2016.\nGaripov, T., Izmailov, P., Podoprikhin, D., Vetrov, D. P., and Wilson, A. G. Loss surfaces, mode\nconnectivity, and fast ensembling of dnns. ArXiv, abs/1802.10026, 2018.\nGeifman, Y. and El-Yaniv, R. Selective classiﬁcation for deep neural networks. In NIPS, 2017.\n13\nGeifman, Y. and El-Yaniv, R. Selectivenet: A deep neural network with an integrated reject option.\nIn International Conference on Machine Learning, 2019.\nGeifman, Y., Uziel, G., and El-Yaniv, R. Bias-reduced uncertainty estimation for deep neural classi-\nﬁers. In International Conference on Learning Representations, 2019.\nGneiting, T. and Raftery, A. E. Strictly proper scoring rules, prediction, and estimation. Journal of\nthe American Statistical Association, 102:359 – 378, 2007.\nGoodfellow, I. J., Shlens, J., and Szegedy, C.\nExplaining and harnessing adversarial examples.\nCoRR, abs/1412.6572, 2015.\nGowal, S., Qin, C., Uesato, J., Mann, T. A., and Kohli, P. Uncovering the limits of adversarial\ntraining against norm-bounded adversarial examples. ArXiv, abs/2010.03593, 2020.\nGowal, S., Rebufﬁ, S.-A., Wiles, O., Stimberg, F., Calian, D. A., and Mann, T. Improving robustness\nusing generated data. In Neural Information Processing Systems, 2021.\nGraves, A. Adaptive computation time for recurrent neural networks. ArXiv, abs/1603.08983, 2016.\nGuo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. ArXiv,\nabs/1706.04599, 2017.\nGuo, J. and Gould, S. Deep cnn ensemble with data augmentation for object detection. ArXiv,\nabs/1506.07224, 2015.\nGurau, C., Bewley, A., and Posner, I. Dropout distillation for efﬁciently estimating model conﬁ-\ndence. ArXiv, abs/1809.10562, 2018.\nHacohen, G., Choshen, L., and Weinshall, D. Let’s agree to agree: Neural networks share classiﬁca-\ntion order on real datasets. In International Conference on Machine Learning, 2020.\nHan, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I. W.-H., and Sugiyama, M. Co-teaching:\nRobust training of deep neural networks with extremely noisy labels. In Neural Information\nProcessing Systems, 2018.\nHan, B., Niu, G., Yu, X., Yao, Q., Xu, M., Tsang, I. W.-H., and Sugiyama, M. Sigua: Forgetting may\nmake learning with noisy labels more robust. In International Conference on Machine Learning,\n2020.\nHecker, S., Dai, D., and Gool, L. V. Failure prediction for autonomous driving. 2018 IEEE Intelli-\ngent Vehicles Symposium (IV), pp. 1792–1799, 2018.\nHendrycks, D. and Gimpel, K. A baseline for detecting misclassiﬁed and out-of-distribution exam-\nples in neural networks. ArXiv, abs/1610.02136, 2017.\nHendrycks, D., Lee, K., and Mazeika, M. Using pre-training can improve model robustness and\nuncertainty. ArXiv, abs/1901.09960, 2019.\nHendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer, J., and Lakshminarayanan, B. Augmix:\nA simple data processing method to improve robustness and uncertainty. ArXiv, abs/1912.02781,\n2020.\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. ArXiv,\nabs/1503.02531, 2015.\nHuang, G., Chen, D., Li, T., Wu, F., van der Maaten, L., and Weinberger, K. Q. Multi-scale dense\nnetworks for resource efﬁcient image classiﬁcation. In International Conference on Learning\nRepresentations, 2017a.\nHuang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., and Weinberger, K. Q. Snapshot ensembles:\nTrain 1, get m for free. ArXiv, abs/1704.00109, 2017b.\nIzmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D. P., and Wilson, A. G. Averaging weights leads\nto wider optima and better generalization. ArXiv, abs/1803.05407, 2018.\n14\nJiang, H., Kim, B., and Gupta, M. R. To trust or not to trust a classiﬁer. In Neural Information\nProcessing Systems, 2018.\nKim, I., Kim, Y., and Kim, S. Learning loss for test-time augmentation. ArXiv, abs/2010.11422,\n2020.\nKim, K., Ji, B., Yoon, D., and Hwang, S. Self-knowledge distillation with progressive reﬁnement of\ntargets. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 6547–6556,\n2021.\nKim, W., Goyal, B., Chawla, K., Lee, J., and Kwon, K. Attention-based ensemble for deep metric\nlearning. In European Conference on Computer Vision, 2018.\nKrizhevsky, A. Learning multiple layers of features from tiny images. 2009.\nKuleshov, V., Fenner, N., and Ermon, S. Accurate uncertainties for deep learning using calibrated\nregression. ArXiv, abs/1807.00263, 2018.\nKull, M., Perelló-Nieto, M., Kängsepp, M., de Menezes e Silva Filho, T., Song, H., and Flach, P. A.\nBeyond temperature scaling: Obtaining well-calibrated multiclass probabilities with dirichlet cal-\nibration. In Neural Information Processing Systems, 2019.\nKumar, A., Liang, P., and Ma, T. Veriﬁed uncertainty calibration. ArXiv, abs/1909.10155, 2019.\nKurakin, A., Goodfellow, I. J., and Bengio, S.\nAdversarial machine learning at scale.\nArXiv,\nabs/1611.01236, 2017.\nLaine, S. and Aila, T. Temporal ensembling for semi-supervised learning. ArXiv, abs/1610.02242,\n2016.\nLakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty\nestimation using deep ensembles. In NIPS, 2017.\nLee, D.-H. Pseudo-label : The simple and efﬁcient semi-supervised learning method for deep neural\nnetworks. 2013.\nLee, J. and AlRegib, G. Gradients as a measure of uncertainty in neural networks. 2020 IEEE\nInternational Conference on Image Processing (ICIP), pp. 2416–2420, 2020.\nLee, K., Lee, H., Lee, K., and Shin, J. Training conﬁdence-calibrated classiﬁers for detecting out-\nof-distribution samples. ArXiv, abs/1711.09325, 2018.\nLee, S., Purushwalkam, S., Cogswell, M., Crandall, D. J., and Batra, D. Why m heads are better\nthan one: Training a diverse ensemble of deep networks. ArXiv, abs/1511.06314, 2015.\nLi, J., Socher, R., and Hoi, S. C. H. Dividemix: Learning with noisy labels as semi-supervised\nlearning. ArXiv, abs/2002.07394, 2020.\nLiang, S., Li, Y., and Srikant, R. Enhancing the reliability of out-of-distribution image detection in\nneural networks. arXiv: Learning, 2018.\nLiu, S., Niles-Weed, J., Razavian, N., and Fernandez-Granda, C.\nEarly-learning regularization\nprevents memorization of noisy labels. ArXiv, abs/2007.00151, 2020a.\nLiu, W., Zhou, P., Zhao, Z., Wang, Z., Deng, H., and Ju, Q. Fastbert: a self-distilling bert with\nadaptive inference time. In Annual Meeting of the Association for Computational Linguistics,\n2020b.\nLivieris, I. E., Iliadis, L. S., and Pintelas, P. B. On ensemble techniques of weight-constrained neural\nnetworks. Evolving Systems, 12:155–167, 2021.\nMaddox, W. J., Garipov, T., Izmailov, P., Vetrov, D. P., and Wilson, A. G. A simple baseline for\nbayesian uncertainty in deep learning. In Neural Information Processing Systems, 2019.\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models\nresistant to adversarial attacks. ArXiv, abs/1706.06083, 2018.\n15\nManning, C. D. and Schütze, H. Foundations of statistical natural language processing. In SGMD,\n2002.\nMariet, Z. E., Jenatton, R., Wenzel, F., and Tran, D. Distilling ensembles improves uncertainty\nestimates. 2020.\nMcClosky, D., Charniak, E., and Johnson, M. Effective self-training for parsing. In North American\nChapter of the Association for Computational Linguistics, 2006.\nMekala, D., Dong, C., and Shang, J. Lops: Learning order inspired pseudo-label selection for\nweakly supervised text classiﬁcation. ArXiv, abs/2205.12528, 2022.\nMenon, A. K., Rawat, A. S., Reddi, S. J., Kim, S., and Kumar, S. A statistical perspective on\ndistillation. In International Conference on Machine Learning, 2021.\nMoon, J., Kim, J., Shin, Y., and Hwang, S. Conﬁdence-aware learning for deep neural networks. In\nInternational Conference on Machine Learning, 2020.\nMukherjee, S. and Awadallah, A. H. Uncertainty-aware self-training for few-shot text classiﬁcation.\nIn Neural Information Processing Systems, 2020.\nMukhoti, J., Kulharia, V., Sanyal, A., Golodetz, S., Torr, P. H. S., and Dokania, P. K. Calibrating\ndeep neural networks using focal loss. ArXiv, abs/2002.09437, 2020.\nMüller, R., Kornblith, S., and Hinton, G. E. When does label smoothing help? In Neural Information\nProcessing Systems, 2019.\nNaeini, M. P., Cooper, G. F., and Hauskrecht, M. Obtaining well calibrated probabilities using\nbayesian binning. Proceedings of the ... AAAI Conference on Artiﬁcial Intelligence. AAAI Confer-\nence on Artiﬁcial Intelligence, 2015:2901–2907, 2015.\nNam, G. C., Yoon, J., Lee, Y., and Lee, J. Diversity matters when learning from ensembles. ArXiv,\nabs/2110.14149, 2021.\nNanni, L., Brahnam, S., and Maguolo, G. Data augmentation for building an ensemble of convolu-\ntional neural networks. Innovation in Medicine and Healthcare Systems, and Multimedia, 2019.\nNeal, R. M. Bayesian learning for neural networks. 1995.\nNixon, J., Dusenberry, M. W., Zhang, L., Jerfel, G., and Tran, D. Measuring calibration in deep\nlearning. ArXiv, abs/1904.01685, 2019.\nNorthcutt, C. G., Jiang, L., and Chuang, I. L. Conﬁdent learning: Estimating uncertainty in dataset\nlabels. J. Artif. Intell. Res., 70:1373–1411, 2021.\nOberdiek, P., Rottmann, M., and Gottschalk, H. Classiﬁcation uncertainty of deep neural networks\nbased on gradient information. In ANNPR, 2018.\nOvadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon, J. V., Lakshminarayanan,\nB., and Snoek, J. Can you trust your model’s uncertainty? evaluating predictive uncertainty under\ndataset shift. ArXiv, abs/1906.02530, 2019.\nPatel, K., Beluch, W. H., Zhang, D., Pfeiffer, M., and Yang, B.\nOn-manifold adversarial data\naugmentation improves uncertainty calibration. 2020 25th International Conference on Pattern\nRecognition (ICPR), pp. 8029–8036, 2021.\nPereyra, G., Tucker, G., Chorowski, J., Kaiser, L., and Hinton, G. E. Regularizing neural networks\nby penalizing conﬁdent output distributions. ArXiv, abs/1701.06548, 2017.\nPlatt, J. Probabilistic outputs for support vector machines and comparisons to regularized likelihood\nmethods. 1999.\nPleiss, G., Zhang, T., Elenberg, E. R., and Weinberger, K. Q. Identifying mislabeled data using the\narea under the margin ranking. ArXiv, abs/2001.10528, 2020.\n16\nRaghu, M., Blumer, K., Sayres, R., Obermeyer, Z., Kleinberg, R. D., Mullainathan, S., and Klein-\nberg, J. M. Direct uncertainty prediction for medical second opinions. In International Conference\non Machine Learning, 2019.\nRamalho, T. and Corbalan, M. Density estimation in representation space to predict model uncer-\ntainty. ArXiv, abs/1908.07235, 2020.\nRebufﬁ, S.-A., Bilen, H., and Vedaldi, A. Learning multiple visual domains with residual adapters.\nIn NIPS, 2017.\nRenda, A., Barsacchi, M., Bechini, A., and Marcelloni, F. Comparing ensemble strategies for deep\nlearning: An application to facial expression recognition. Expert Syst. Appl., 136:1–11, 2019.\nRice, L., Wong, E., and Kolter, J. Z. Overﬁtting in adversarially robust deep learning. In Interna-\ntional Conference on Machine Learning, 2020.\nRizve, M. N., Duarte, K., Rawat, Y. S., and Shah, M. In defense of pseudo-labeling: An uncertainty-\naware pseudo-label selection framework for semi-supervised learning. ArXiv, abs/2101.06329,\n2021.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla,\nA., Bernstein, M. S., Berg, A. C., and Fei-Fei, L. Imagenet large scale visual recognition challenge.\nInternational Journal of Computer Vision, 115:211–252, 2014.\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. The right tool for\nthe job: Matching model and instance complexities. In Annual Meeting of the Association for\nComputational Linguistics, 2020.\nSehwag, V., Mahloujifar, S., Handina, T., Dai, S., Xiang, C., Chiang, M., and Mittal, P. Robust\nlearning meets generative models: Can proxy distributions improve adversarial robustness? In\nInternational Conference on Learning Representations, 2021.\nSeo, S., Seo, P. H., and Han, B. Learning for single-shot conﬁdence calibration in deep neural\nnetworks through stochastic inferences. 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 9022–9030, 2019.\nSettles, B. Active learning literature survey. 2009.\nShanmugam, D., Blalock, D. W., Balakrishnan, G., and Guttag, J. V. When and why test-time\naugmentation works. ArXiv, abs/2011.11156, 2020.\nSnell, J., Swersky, K., and Zemel, R. S.\nPrototypical networks for few-shot learning.\nArXiv,\nabs/1703.05175, 2017.\nSrivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: a simple\nway to prevent neural networks from overﬁtting. J. Mach. Learn. Res., 15:1929–1958, 2014.\nStanton, S., Izmailov, P., Kirichenko, P., Alemi, A. A., and Wilson, A. G. Does knowledge distilla-\ntion really work? In Neural Information Processing Systems, 2021.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., and Fergus, R.\nIntriguing properties of neural networks. CoRR, abs/1312.6199, 2014.\nTarvainen, A. and Valpola, H. Mean teachers are better role models: Weight-averaged consistency\ntargets improve semi-supervised deep learning results. In NIPS, 2017.\nTeerapittayanon, S., McDanel, B., and Kung, H. T. Branchynet: Fast inference via early exiting\nfrom deep neural networks. 2016 23rd International Conference on Pattern Recognition (ICPR),\npp. 2464–2469, 2016.\nThulasidasan, S., Chennupati, G., Bilmes, J. A., Bhattacharya, T., and Michalak, S. E. On mixup\ntraining: Improved calibration and predictive uncertainty for deep neural networks. In Neural\nInformation Processing Systems, 2019.\n17\nTishby, N., Levin, E., and Solla, S. A. Consistent inference of probabilities in layered networks:\npredictions and generalizations. International 1989 Joint Conference on Neural Networks, pp.\n403–409 vol.2, 1989.\nToneva, M., Sordoni, A., des Combes, R. T., Trischler, A., Bengio, Y., and Gordon, G. J. An em-\npirical study of example forgetting during deep neural network learning. ArXiv, abs/1812.05159,\n2019.\nUesato, J., Alayrac, J.-B., Huang, P.-S., Stanforth, R., Fawzi, A., and Kohli, P. Are labels required\nfor improving adversarial robustness? ArXiv, abs/1905.13725, 2019.\nVaicenavicius, J., Widmann, D., Andersson, C. R., Lindsten, F., Roll, J., and Schön, T. B. Evaluating\nmodel calibration in classiﬁcation. In International Conference on Artiﬁcial Intelligence and\nStatistics, 2019.\nvan Amersfoort, J. R., Smith, L., Teh, Y. W., and Gal, Y. Simple and scalable epistemic uncertainty\nestimation using a single deep deterministic neural network.\nIn International Conference on\nMachine Learning, 2020.\nWang, G., Li, W., Ourselin, S., and Vercauteren, T. K. M. Automatic brain tumor segmentation\nusing convolutional neural networks with test-time augmentation. ArXiv, abs/1810.07884, 2018.\nWang, G., Li, W., Aertsen, M., Deprest, J. A., Ourselin, S., and Vercauteren, T. K. M. Aleatoric\nuncertainty estimation with test-time augmentation for medical image segmentation with convo-\nlutional neural networks. Neurocomputing, 335:34 – 45, 2019.\nWen, Y., Tran, D., and Ba, J. Batchensemble: An alternative approach to efﬁcient ensemble and\nlifelong learning. ArXiv, abs/2002.06715, 2020.\nWen, Y., Jerfel, G., Muller, R., Dusenberry, M. W., Snoek, J., Lakshminarayanan, B., and Tran, D.\nCombining ensembles and data augmentation can harm your calibration. ArXiv, abs/2010.09875,\n2021.\nWenzel, F., Snoek, J., Tran, D., and Jenatton, R. Hyperparameter ensembles for robustness and\nuncertainty quantiﬁcation. ArXiv, abs/2006.13570, 2020.\nXia, X., Liu, T., Han, B., Gong, C., Wang, N., Ge, Z., and Chang, Y. Robust early-learning: Hinder-\ning the memorization of noisy labels. In International Conference on Learning Representations,\n2021.\nXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. J. Deebert: Dynamic early exiting for accelerating bert\ninference. In Annual Meeting of the Association for Computational Linguistics, 2020.\nYang, J. and Wang, F. Auto-ensemble: An adaptive learning rate scheduling based deep learning\nmodel ensembling. IEEE Access, 8:217499–217509, 2020.\nYu, Y., Bates, S., Ma, Y.-A., and Jordan, M. I. Robust calibration with multi-domain temperature\nscaling. ArXiv, abs/2206.02757, 2022.\nZadrozny, B. and Elkan, C. P. Obtaining calibrated probability estimates from decision trees and\nnaive bayesian classiﬁers. In International Conference on Machine Learning, 2001.\nZadrozny, B. and Elkan, C. P. Transforming classiﬁer scores into accurate multiclass probability\nestimates. Proceedings of the eighth ACM SIGKDD international conference on Knowledge dis-\ncovery and data mining, 2002.\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires\nrethinking generalization. ArXiv, abs/1611.03530, 2017.\nZhang, J., Kailkhura, B., and Han, T. Y.-J. Mix-n-match: Ensemble and compositional methods for\nuncertainty calibration in deep learning. In International Conference on Machine Learning, 2020.\nZhou, T., Wang, S., and Bilmes, J. A. Time-consistent self-supervision for semi-supervised learning.\nIn ICML, 2020.\n18\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-02-02",
  "updated": "2023-02-02"
}