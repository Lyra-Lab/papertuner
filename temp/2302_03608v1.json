{
  "id": "http://arxiv.org/abs/2302.03608v1",
  "title": "Online Reinforcement Learning with Uncertain Episode Lengths",
  "authors": [
    "Debmalya Mandal",
    "Goran Radanovic",
    "Jiarui Gan",
    "Adish Singla",
    "Rupak Majumdar"
  ],
  "abstract": "Existing episodic reinforcement algorithms assume that the length of an\nepisode is fixed across time and known a priori. In this paper, we consider a\ngeneral framework of episodic reinforcement learning when the length of each\nepisode is drawn from a distribution. We first establish that this problem is\nequivalent to online reinforcement learning with general discounting where the\nlearner is trying to optimize the expected discounted sum of rewards over an\ninfinite horizon, but where the discounting function is not necessarily\ngeometric. We show that minimizing regret with this new general discounting is\nequivalent to minimizing regret with uncertain episode lengths. We then design\na reinforcement learning algorithm that minimizes regret with general\ndiscounting but acts for the setting with uncertain episode lengths. We\ninstantiate our general bound for different types of discounting, including\ngeometric and polynomial discounting. We also show that we can obtain similar\nregret bounds even when the uncertainty over the episode lengths is unknown, by\nestimating the unknown distribution over time. Finally, we compare our learning\nalgorithms with existing value-iteration based episodic RL algorithms in a\ngrid-world environment.",
  "text": "Online Reinforcement Learning with Uncertain Episode Lengths\nDebmalya Mandal\nMax Planck Institute for Software Systems\ndmandal@mpi-sws.org\nGoran Radanovi´c\nMax Planck Institute for Software Systems\ngradanovic@mpi-sws.org\nJiarui Gan\nUniversity of Oxford\njiarui.gan@cs.ox.ac.uk\nAdish Singla\nMax Planck Institute for Software Systems\nadishs@mpi-sws.org\nRupak Majumdar\nMax Planck Institute for Software Systems\nrupak@mpi-sws.org\nFebruary 8, 2023\nAbstract\nExisting episodic reinforcement algorithms assume that the length of an episode is ﬁxed across time and known\na priori. In this paper, we consider a general framework of episodic reinforcement learning when the length of\neach episode is drawn from a distribution. We ﬁrst establish that this problem is equivalent to online reinforcement\nlearning with general discounting where the learner is trying to optimize the expected discounted sum of rewards\nover an inﬁnite horizon, but where the discounting function is not necessarily geometric. We show that minimizing\nregret with this new general discounting is equivalent to minimizing regret with uncertain episode lengths. We then\ndesign a reinforcement learning algorithm that minimizes regret with general discounting but acts for the setting with\nuncertain episode lengths. We instantiate our general bound for different types of discounting, including geometric\nand polynomial discounting. We also show that we can obtain similar regret bounds even when the uncertainty over\nthe episode lengths is unknown, by estimating the unknown distribution over time. Finally, we compare our learning\nalgorithms with existing value-iteration based episodic RL algorithms on a grid-world environment.\n1\nIntroduction\nWe consider the problem of episodic reinforcement learning, where a learning agent interacts with the environment\nover a number of episodes [SB18]. The framework of episodic reinforcement learning usually considers two types of\nepisode lengths: either each episode has a ﬁxed and invariant length H, or each episode may have a varying length\ncontrolled by the learner. The ﬁxed-length assumption is relevant for recommender systems [Agg+16] where the plat-\nform interacts with a user for a ﬁxed number of rounds. Variable length episodes arise naturally in robotics [KBP13],\nwhere each episode is associated with a learning agent completing a task, and so the length of the episode is en-\ntirely controlled by the learner. Fixed horizon lengths make the design of learning algorithms easier, and is the usual\nassumption in most papers on theoretical reinforcement learning [AOM17; Jin+18].\nIn this paper, we take a different perspective on episodic reinforcement learning and assume that the length of each\nepisode is drawn from a distribution. This situation often arises in online platforms where the length of an episode\n(i.e., the duration of a visit by a user) is not ﬁxed a priori, but follows a predictable distribution [OSB14]. Additionally,\nvarious econometric and psychological evidence suggest that humans learn by maintaining a risk/hazard distribution\nover the future [Soz98], which can be interpreted as a distribution over the horizon length. Despite a large and growing\n1\narXiv:2302.03608v1  [cs.LG]  7 Feb 2023\nliterature on episodic reinforcement learning, except for [Fed+19], uncertain epsiodic lengths or settings with general\nsurvival rates of agents have not been studied before.\nOur Contributions: In this paper, we describe reinforcement learning algorithms for general distributions over\nepisode lengths. Our main contribution is a general learning algorithm which can be adapted to a given distribution\nover episode lengths to obtain sub-linear regret over time. In particular, our contributions are the following.\n• We ﬁrst establish an equivalence between maximization of expected total reward with uncertain episode lengths\nand maximization of expected (general) discounted sum of rewards over an inﬁnite horizon. In particular, we\nshow that minimization of regret is equivalent in these two environments.\n• Next we design a learning algorithm for the setting with arbitrary distribution over the episode lengths. Our\nalgorithm generalizes the value-iteration based learning algorithm of Azar et al. [AOM17] by carefully choosing\nan effective horizon length and then updating the backward induction step based on the distribution over episode\nlengths. In order to analyze its regret, we use the equivalence result above, and bound its regret for a setting\nwith general discounting.\n• We instantiate our general regret bound for different types of discounting (or equivalently episode distribu-\ntions), including geometric and polynomial discounting, and obtain sub-linear regret bounds. For geometric\ndiscounting with parameter γ, we bound regret by eO(\n√\nSAT/(1 −γ)1.5) which matches the recently estab-\nlished minimax optimal regret for the non-episodic setting [HZG21]. For the polynomial discounting of the\nform h−p we upper bound regret by eO(\n√\nSAT\n1\n2−1/p ).\n• Finally, we show that we can obtain similar regret bounds even when the uncertainty over the episode lengths is\nunknown, by estimating the unknown distribution over time. In fact, for geometric discounting, we recover the\nsame regret bound (i.e. eO(\n√\nSAT/(1 −γ)1.5) up to logarithmic factors, and for the polynomial discounting we\nobtain a regret bound of eO(\n√\nSAT\np\n1+2p ), which asymptotically matches the previous regret bound.\nOur results require novel and non-trivial generalizations of episodic learning algorithms and straightforward exten-\nsions to existing algorithms do not work. Indeed, a naive approach would be to use the expected episode length as the\nﬁxed horizon length H. However, this fails with heavy-tailed distributions which often appear in practice. Alternately,\nwe could compute an upper bound on the episode length so that with high probability the lengths of all the T episodes\nare within this bound. Such an upper bound can be computed with the knowledge of distribution over episode lengths\nand using standard concentration inequalities. However, these upper bounds become loose either with a large number\nof episodes or for heavy-tailed distributions.\n1.1\nRelated Work\nEpisodic Reinforcement Learning: Our work is closely related to the UCB-VI algorithm of Azar et al. [AOM17],\nwhich achieves O(\n√\nHSAT) regret for episodic RL with ﬁxed horizon length H. The main difference between our\nalgorithm and UCB-VI is that we use a different equation for backward-induction where future payoffs are discounted\nby a factor of γ(h + 1)/γ(h) at step h, where γ is a general discount function. Beyond [AOM17], several papers\nhave considered different versions of episodic RL including changing transition function [Jin+18; JL20], and function\napproximation [Jin+20; WSY20; YW20].\nGeneral Discounting: Our work is also closely related to reinforcement learning with general discounting. Even\nthough geometric discounting is the most-studied discounting because of its theoretical properties [Ber12], there is\na wealth of evidence suggesting that humans use general discounting and time-inconsistent decision making [Ain75;\nMaz85; GM04]. In general, optimizing discounted sum of rewards with respect to a general discounting might be\ndifﬁcult as we are not guaranteed to have a stationary optimal policy. Fedus et al. [Fed+19] study RL with hyperbolic\ndiscounting and learn many Q-values each with a different (geometric) discounting. Our model is more general, and\nour algorithm is based on a modiﬁed value iteration. We also obtain theoretical bounds on regret in our general setting.\nFinally, Pitis [Pit19] introduced more general state, action based discounting but that is out of scope of this paper.\nStochastic Shortest Path: Our work is related to the stochastic shortest path (SSP), introduced by Bertsekas et\nal. [BT91]. In SSP, the goal of the learner is to reach a designated state in an MDP, and minimize the expected total\n2\ncost of the policy before reaching that goal. Recently, there has been a surge of interest in deriving online learning\nalgorithms for SSP [Ros+20; Coh+21; Tar+21]. Our setting differs from SSP in two ways. First, the horizon length\nis effectively controlled by the learner in SSP, once she has a good approximation of the model. But in our setting,\nthe horizon length is drawn from a distribution at the start of an episode by the nature, and is unknown to the learner\nduring that episode. Second, when the model is known in SSP, different policies induce different distributions over the\nhorizon length. Therefore, in contrast to our setting, minimizing regret in SSP is not the same as minimizing regret\nunder general discounting.\nOther Related Work: Note that uncertainty over episode lengths can also be interpreted as hazardous MDP [HM72],\nwhere hazard rate is deﬁned to be the negative rate of change of log-survival time. Sozou [Soz98] showed that different\nprior belief over hazard rates imply different types of discounting. We actually show equivalence between general dis-\ncounting and uncertain episode lengths, even in terms of regret bounds. Finally, this setting is captured by the partially\nobservable Markov decision processes [KLC98], where one can make the uncertain parameters hidden and/or partially\nobservable.\n2\nModel\nWe consider the problem of episodic reinforcement learning with uncertain episode length. An agent interacts with an\nMDP M = (S, A, r, P, PH), where PH denotes the probability distribution over the episode length. We assume that the\nrewards are bounded between 0 and 1. The agent interacts with the environment for T episodes as follows.\n• At episode k ∈[T], the starting state xk,1 is chosen arbitrarily and the length of the episode Hk ∼PH(·). 1\n• For h ∈[Hk], let the state visited be xk,h and the action taken be ak,h. Then, the next state xk,h+1 ∼\nP(·|xk,h, ak,h).\nThe agent interacts with the MDP M for T episodes and the goal is to maximize the expected undiscounted sum\nof rewards. Given a sequence of k episode lengths {Hk}k∈[T ] the expected cumulative reward of an agent’s policy\nπ = {πk}k∈[T ] is given as\nRew\n\u0000π; {Hk}k∈[T ]\n\u0001\n=\nT\nX\nk=1\nE\n\" Hk\nX\nh=1\nr(xk,h, ak,h)\n#\nSince each Hk is a random variable drawn from the distribution PH(·), we are interested in expected reward with\nrespect to distribution PH.\nE\n\u0002\nRew\n\u0000π; {Hk}k∈[T ]\n\u0001\u0003\n= E\n\" T\nX\nk=1\n∞\nX\nHk=1\nPH(Hk)\nHk\nX\nh=1\nr(xk,h, ak,h)\n#\n= Eπ\n\" T\nX\nk=1\n∞\nX\nh=1\nPH(H ≥h)r(xt,h, at,h)\n#\n(1)\nAs is standard in the literature on online learning, we will consider the problem of minimizing regret instead of\nmaximizing the reward. Given an episode length Hk and starting state xk,1 let π⋆\nk be the policy that maximizes the ex-\npected sum of rewards over Hk steps i.e. π⋆\nk ∈argmaxπEπ\nhPHk\nh=1 r(xk,h, ak,h)|xk,1\ni\n. We will write V πk(xk,1; Hk)\nto write the (undiscounted) value function of a policy πk over Hk steps starting from state xk,1. Then π⋆\nk is also deﬁned\nas π⋆\nk ∈argmaxπV π(xk,1; Hk). We will also write V ⋆(xk,1; Hk) to denote the corresponding value of the optimal\nvalue function. Now we can deﬁne the regret over T steps as follows.\nDeﬁnition 1. The regret of a learning algorithm π = {πk}k∈[T ] over T steps with episode lengths {Hk}k∈[T ] is\nReg (π; {Hk}) =\nX\nk∈[T ]\nV ⋆(xk,1; Hk) −V πk(xk,1; Hk)\n(2)\n1The parameter Hk is unknown to the learner during episode k.\n3\nNote that the regret as deﬁned in eq. (2) is actually a random variable as the episode lengths are also randomly\ngenerated from the distribution PH(·). So we will be interested in bounding the expected regret. Let V ⋆(xk,1) be\nthe expected value of V ⋆(xk,1; Hk) i.e. V ⋆(xk,1) = P\nℓV ⋆(xk,1; ℓ)PH(ℓ). Then the expected regret of a learning\nalgorithm is given as\nReg(π; PH(·)) =\nX\nk∈[T ]\nV ⋆(xk,1) −EHk [V πk(xk,1; Hk)]\n2.1\nAn Equivalent Model of General Discounting\nWe ﬁrst establish that the problem of minimizing regret in our setting is equivalent to minimizing regret in a different\nenvironment, where the goal is to minimize discounted reward over an inﬁnite horizon with a general notion of dis-\ncounting. By setting γ(h) = PH(H ≥h), the expected reward in eq. (1) becomes a sum of T expected rewards under\nthe general discounting function {γ(h)}∞\nh=1.\nE\n\u0002\nRew(π; {Hk}k∈[T ])\n\u0003\n=\nT\nX\nt=1\nE\n\" ∞\nX\nh=1\nγ(h)r(xt,h, at,h)|xk,1\n#\nTherefore, we consider the equivalent setting where the agent is interacting with the MDP M = (S, A, r, P, γ) where\nγ = {γ(h)}∞\nh=1 is a general discounting factor. We will require the following two properties from the discounting\nfactors:\n1. γ(1) = 1,\n2. P∞\nh=1 γ(h) ≤M for some universal constant M > 0.\nThe ﬁrst assumption is without loss of generality as we can normalize all the discount factors without affecting the\nmaximization problem. The second assumption guarantees that the optimal policy is well-deﬁned. Note that this\nassumption rules out hyperbolic discounting γ(h) =\n1\n1+h, but does allow discount factors of the form γ(h) = h−p for\nany p > 1. Finally, note that our original reformulation of γ(h) = PH(H ≥h) trivially satisﬁes the ﬁrst assumption.\nThe second assumption essentially ensures that the horizon length has a ﬁnite mean. We will also write Γ(h) to deﬁne\nthe sum of the tail part of the series starting at h i.e.\nΓ(h) =\nX\nj≥h\nγ(j)\n(3)\nIn this new environment, the learner solves the following episodic reinforcement learning problem over T episodes.\nEnvironment: General Discounting\n1. The starting state xk,1 is chosen arbitrarily.\n2. The agent maximizes E [P∞\nh=1 γ(h)r(xk,h, ak,h)|xk,1] over an inﬁnite horizon.\nNotice that even though the new environment is episodic, the length of each episode is inﬁnite. So this environment\nis not realistic, and we are only introducing this hypothetical environment to design our algorithm and analyze its\nperformance.\nSuppose that we are given a learning algorithm π = {πk}k∈[T ]. We allow the possibility that πk is a non-stationary\npolicy as each πk is used to maximizing a discounted sum of rewards with respect to a general discounting factor and\nin general the optimal policy need not be stationary. A non-stationary policy πk is a collection of policies {πk,h}∞\nh=1\nwhere πk,h : (S × A)h−1 × S →∆(A). Given a non-stationary policy πk at episode k, we deﬁne the state-action Q\nfunction and the value function as\nQπk(x, a; γ) = E\n\" ∞\nX\nh=1\nγ(h)r(xk,h, ak,h)|xk,1 = x, ak,1 = a\n#\nV πk(x; γ) = E\n\" ∞\nX\nh=1\nγ(h)r(xk,h, ak,h)|xk,1 = x\n#\n4\nHere ak,h ∼πk,h(xk,1, ak,1, . . . , xk,h−1, ak,h−1, xk,h). In this environment, we again measure the regret as the sum\nof sub-optimality gaps over the T episodes.\nDeﬁnition 2. Let the optimal value function be deﬁned as V ⋆(x; γ) = supπ V π(x; γ). Then we deﬁne regret for a\nlearning algorithm π = {πk}k∈[T ] as\nReg(π, γ) =\nT\nX\nk=1\nV ⋆(xk,1; γ) −V πk(xk,1; γ)\n(4)\nOur next result shows that it is sufﬁcient to minimize regret with respect to the new environment of episodic\nreinforcement learning. In fact, if any algorithm has regret R(T) with respect to the new benchmark, then it has regret\nat most R(T) with respect to the original environment with uncertain episode lengths.\nLemma 1. For any learning algorithm π = {πk}k∈[T ] we have the following guarantee:\nReg(π; PH(·)) ≤Reg(π; γ).\nWe also show that a converse of lemma 1 holds with additional restrictions on the discount factor γ.\nLemma 2. Suppose the discount factor γ is non-increasing. Then there exists a distribution PH(·) over the episode\nlengths so that\nReg(π; γ) ≤Reg(π; PH(·)).\nBecause of lemma 1, it is sufﬁcient to bound a learning algorithm’s regret for the environment with inﬁnite horizon\nand general discounting. Therefore, we now focus on designing a learning algorithm that acts in an episodic setting\nwith uncertain episode lengths, but analyze its regret in the inﬁnite horizon setting with general discounting.\n3\nAlgorithm: Regret Minimization under General Discounting\nWe now introduce our main algorithm. Given a non-stationary policy πk, we deﬁne the state-action function and value\nfunction at step h as follows.\nQπk\nh (x, a) = E\n\n\n∞\nX\nj=1\nγ(j)r(xk,h+j−1, ak,h+j−1) | Hh−1, xk,h = x, ak,h = a\n\n\nV πk\nh (x) = E\n\n\n∞\nX\nj=1\nγ(j)r(xk,h+j−1, ak,h+j−1) | Hh−1, xk,h = x\n\n\nwhere Hh−1 = (xk,1, ak,1, . . . , ak,h−1) and ak,h+j ∼πk,h+j(Hh+j−1, xk,h+j). Note that, both the state-action Q-\nfunction and the value function depend on the history Hh−1. Moreover, conditioned on the history, we are evaluating\nthe total discounted reward as if the policy {πk,h+j}j≥0 was used from the beginning. We ﬁrst establish some relations\nregarding the above state-action and value functions. We drop the episode index k for ease of exposition. Given a\nnon-stationary policy π = {πh}h≥1 let\nQπ\nh(x, a) = r(x, a) + γ(2) · E\n\n\n∞\nX\nj=1\nγ(j + 1)\nγ(2)\nr(xh+j, ah+j)|Hh−1, xh = x, ah = a\n\n\n= r(x, a) + γ(2)Exh+1∼P(·|x,a)\n\nE\n\n\n∞\nX\nj=1\nγ(j + 1)\nγ(2)\nr(xh+j+1, ah+j+1)|Hh, xh+1\n\n\n\n\n= r(x, a) + γ(2)Exh+1∼P(·|x,a)\n\u0002\nV π\nh+1(xh+1; γ2)\n\u0003\n5\nALGORITHM 1: UCB-VI Generalized\nInput: Discount factor {γ(h)}∞\nh=1, parameter ∆\nH ←∅.\nfor h = 1, . . . , N(∆) do\nSet Q1,h(x, a) ←P∞\nj=1 γh(j) =\n1\nγ(h)\nP∞\nj=1 γ(j + h −1) for all x ∈S and a ∈A.\nfor t = 1, . . . , T do\nUpdate-Q-values(H, γ, ∆).\nReceive state xt,1.\nfor h = 1, . . . do\nif h ≤N(∆) then\nTake action at,h = argmaxaQt,h(xt,h, a)\nUpdate H = H ∪(xt,h, at,h, xt,h+1)\nif xt,h+1 is a terminal state then\nContinue to the next episode.\nTake an arbitrary action.\nwhere in the last line we write γ2 to denote the discount factor γ2(j) =\nγ(j+1)\nγ(2)\nand V π\nh+1(xh+1; γ2) is the value\nfunction at time-step h with respect to the new discount factor γ2. By a similar argument one can write the action-\nvalue function with respect to the discount factor γ2 as the following expression.\nQπ\nh(x, a; γ2)\n= r(x, a) + γ2(2)Exh+1∼P(·|x,a)\n\u0002\nV π\nh+1(xh+1; γ2)\n\u0003\n= r(x, a) + γ(3)\nγ(2)Exh+1∼P(·|x,a)\n\u0002\nV π\nh+1(xh+1; γ3)\n\u0003\nwhere the discount factor γ3 is given as γ3(j) = γ(j+2)\nγ(3) . In general, we have the following relation.\nQπ\nh(x, a; γk) = r(x, a) + γ(k + 1)\nγ(k)\nExh+1∼P(·|x,a)\n\u0002\nV π\nh+1(xh+1; γk+1)\n\u0003\n(5)\nwhere the discount factor γk is deﬁned as γk(j) = γ(j+k−1)\nγ(k)\nfor j = 1, 2, . . .. Notice that when γ is a geometric\ndiscounting, we only need equation.\nQπ\nh(x, a) = r(x, a) + γExh+1∼P(·|x,a)\n\u0002\nV π\nh+1(xh+1)\n\u0003\n(6)\nDescription of the Learning Algorithm\n: The sequence of recurrence relations eq. (5) motivates our main algorithm\n(1). Our algorithm is based on the upper conﬁdence value iteration algorithm (UCBVI [AOM17]). In an episodic\nreinforcement learning setting with ﬁxed horizon length H, UCBVI uses backward induction to update the Q-values\nat the end of each episode, and takes greedy action according to the Q-table.\nHowever, in our setting, there is no ﬁxed horizon length and the Q-values are related through an inﬁnite se-\nquence of recurrence relations. So, algorithm 1 considers a truncated version of the sequence of recurrence rela-\ntions eq. (5). In particular, given an input discount factor {γ(h)}∞\nh=1\n2 and a parameter ∆, algorithm 1 ﬁrst deter-\nmines N(∆) as a measure of effective length of the horizon. In particular, we set N(∆) to be an index so that\nΓ(N(∆)) = P\nj≥N(∆) γ(j) ≤∆. Note that, such an index N(∆) always exists as we assumed that the total sum\nof the discounting factors converges. Then algorithm 1 maintains an estimate of the Q value for all possible discount\nfactors up to N(∆) i.e. γk for k = 1, . . . , N(∆).\nThe details of the update procedure is provided in the appendix. In the update procedure, we ﬁrst set the (N(∆) +\n1)-th Q-value to be ∆/γ(N(∆) + 1) which is always an upper bound on the Q-value with discount factor γN(∆)+1\n2Recall that γ(h) = PH(H ≥h).\n6\nbecause of the way algorithm 1 sets the value N(∆). Then starting from level N(∆), we update the Q-values through\nbackward induction and eq. (5).\nNote that our algorithm needs to maintain N(∆) action-value tables. We will later show that in order to obtain sub-\nlinear regret we need to choose ∆based on the particular discount factor. In particular, for the geometric discount factor\nγ(h) = γh−1 we need to choose N(∆) =\nlog T\nlog(1/γ). On the other hand, discounting factor of the form γ(h) = 1/hp\nrequires N(∆) = O\n\u0000T 1/(2p−1)\u0001\n.\n4\nAnalysis\nThe next theorem provides an upper bound on the regret Reg(π; γ). In order to state the theorem, we need a new\nnotation. Let the function t : N →R be deﬁned as\nt(h) =\n(\n1\nif h = 1\nγ(h)\nγ(1)\nQh\nj=2\n\u0010\n1 +\nγ(j)\njβΓ(j)\n\u0011\no.w.\nNote that the function t is parameterized by the parameter β and depends on the discount factor γ(·).\nTheorem 1 (Informal). With probability at least 1 −δ, Algorithm 1 has the following regret.\nReg(π; γ) ≤\n∆T\nγ(N(∆) + 1)t(N(∆) + 1) +\nmax\nh∈[N(∆)] t(h)Γ(h + 1)\nγ(h)\neO\n\u0010p\nSATN(∆)\n\u0011\nTheorem 1 states a generic bound that holds for any discount factor. The main terms in the bound are O\n\u0010p\nSATN(∆)\n\u0011\n,\n∆T, and several factors dependent on the discount factor γ. We now instantiate the bound for different discount factors\nby choosing appropriate value of ∆and the parameter β.\nCorollary 2. Consider the discount factor γ(h) = h−p. For p ≥2 and T ≥O(S3A) we have\nReg(T) ≤eO\n\u0010\nS1/2A1/2T\n1\n2−1/p\n\u0011\nand for 1 < p < 2 and T ≥O\n\u0010\n(S3A)\n2p−1\np−1\n\u0011\nwe have\nReg(T) ≤eO\n\u0010\n(p −1)−\np\np−1 S1/2A1/2T\n1\n2−1/p\n\u0011\nWe prove corollary 2 by substituting β = p −1 and ∆= O\n\u0010\nT −p−1\n2p−1\n\u0011\n. Note that this result suggests that as\np increases to inﬁnity, the regret bound converges to O(\n√\nT). This also suggests that for exponentially decaying\ndiscounting factor, our algorithm should have exactly O(\n√\nT) regret. We verify this claim next.\nCorollary 3. Consider the discount factor γ(h) = γh−1 for γ ∈[0, 1) and suppose T ≥\nS3A\n(1−γ)4 . Then algorithm 1\nhas regret at most\nReg(T) ≤eO\n\u0010√\nSAT/(1 −γ)1.5\u0011\nHere we substitute β = 3/2 and ∆= T −1/(1 −γ). Our regret bound for the geometric discounting matches the\nminimax optimal regret bound of the non-episodic setting of [HZG21].\nProof Sketch of Theorem 1\n: We now give an overview of the main steps of the proof. Although the proof is based\nupon the proof of the UCB-VI algorithm [AOM17], there are several differences.\n• Let V ⋆\nh (·) be the optimal value function under discounting factor γh(·) i.e. V ⋆\nh (x) = supπ V π(x; γh). We\nﬁrst show that the estimates Vk,h maintained by Algorithm 1 upper bound the optimal value functions i.e.\nVk,h(x) ≥V ⋆\nh (x) for any k, h ∈[N(∆)].\n7\nALGORITHM 2: Estimating Unknown Discount Factor\nInput: Horizon Length H⋆= N(∆).\nSet block length B =\n√\nT log T log(log(T)/δ).\nSet ˆγ0 to be an arbitrary discount factor.\nfor j = 0, 1, . . . , log(T/B) −1 do\nif j > 0 then\nˆγj(h) = 1 −ˆFH(h −1) forall h.\nˆ∆j = P\nh≥H⋆+1 ˆγj(h).\nRun algorithm 1 for 2jB episodes with inputs ˆγj and ˆ∆j.\n/* update empirical distribution function\n*/\nˆFH(h) =\n1\n2jB\nP2jB\nt=0 1 {Ht ≤h}.\n• Let e∆k,h = Vk,h −V πk\nh . Then regret can be bounded as\nReg(π; γ) =\nT\nX\nk=1\nV ⋆(xk,1) −V πk\n1 (xk,1) ≤\nT\nX\nk=1\nVk,1(xk,1) −V πk\n1 (xk,1) ≤\nT\nX\nk=1\ne∆k,1(xk,1)\n• Let eδk,h = e∆k,h(xk,h). Then, the main part of the proof of theorem 1 is establishing the following recurrent\nrelation.\neδk,h ≤γ(h + 1)\nγ(h)\n\u0012\n1 +\nγ(h + 1)\n(h + 1)βΓ(h + 1)\n\u0013\neδk,h+1 +\n√\n2Lεk,h + ek,h + bk,h + εk,h + fk,h\nHere εk,h and εk,h are Martingale difference sequences and bk,h, ek,h, fk,h are either the bonus term or behave\nsimilarly as the bonus term.\n• We complete the proof by summing the recurrence relation above over all the episodes and from h = 1 to\nN(∆). Although [AOM17] established a similar recurrence relation, there are two major differences. First the\nmultiplicative factor in front of eδk,h+1 is changing with time-step h and is not a constant. This is because the\nbackward induction step uses eq. (5) in our setting. Second, after expanding the recurrence relation from h = 1\nto N(∆) the ﬁnal term is no longer zero and an extra O(∆T) term shows up in the regret bound.\n5\nEstimating the Discount Function\nIn this section we consider the situation when the discount function γ(h) = PH(H ≥h) is not unknown. We start\nwith the assumption that the optimal value of N(∆) (say H⋆) is known. The next lemma bounds the regret achieved\nby running an algorithm with N(∆) = H⋆with the true discounting γ and an estimate of the discounting ˆγ. Our\nalgorithm partitions the entire sequence of T episodes into blocks of lengths B, 2B, 22B, . . . , 2sB for s = log(T/B)−\n1. At the end of each block the algorithm recomputes an estimate of γ. Recall that we deﬁned γ(h) = Pr(H ≥h).\nSince every episode we get one sample from the distribution of H (the random length of the current episode) we can\nuse the empirical distribution function of horizon length to obtain ˆγ. At the end of block B, the algorithm computes\nˆγB, and runs algorithm 1 with this estimate and ˆ∆B = ˆΓB(H⋆+ 1) = P\nh≥H⋆+1 ˆγB(h) for the block B + 1.\nTheorem 4 (Informal). When run with horizon length H⋆, algorithm 2 has the following regret bound with probability\nat least 1 −δ\nReg(π; γ) ≤min\nL∈[T ]\n\u0010\nTΓ(L + 1) + 2L log(T)\n√\nT)\n\u0011\n+ Γ(H⋆)T\n+ max\nh∈[H⋆]\nt(h)\nγ(h)g(h)Γ(h + 1)O(T −1/4)\nΓ(h + 1)\neO\n\u0010√\nSATH⋆\n\u0011\nwhere g(h) = exp\nn\nO\n\u0010Ph\nk=2\nT −1/4\nγ(k)+kβΓ(k)\n\u0011o\n.\n8\n(a) Geometric, γ = 0.9\n(b) Geometric, γ = 0.95\n(c) Geometric, γ = 0.975\n(d) Polynomial, p = 1.4\n(e) Polynomial, p = 1.6\n(f) Polynomial, p = 2.0\n(g) Quasi-Hyperbolic, β = 0.7\n(h) Quasi-Hyperbolic, β = 0.8\n(i) Quasi-Hyperbolic, β = 0.9\nFigure 1: Comparison of our algorithm with different variants of UCB-VI on the Taxi environment [Die00]. The regret\nis measured over 100 episodes, and the length of each episode is drawn independently from a given distribution. Each\nplot shows average regret and standard error from 10 trials.\nOur proof relies on bounding the estimation error of ˆγ and ˆΓ. We can use the classical DKW inequality [DKW56]\nto bound the maximum deviation between empirical CDF (bPH(·)) and true CDF (PH). Through a union bound over the\nlog(T) blocks, this immediately provides a bound between ∥ˆγj −γj∥∞for all j ∈[log(T/B)]. However, we also\nneed to bound the distance between ˆΓj(·) and Γ(·) for all j (deﬁned in (3)). A naive application of DKW inequality\nresults in an additive bound between ˆΓj(h) and Γ(h) that grows at a rate of h. This is insufﬁcient for our case to get\na sublinear regret bound. However, we show that we can use the multiclass fundamental theorem [SB14] to derive an\nerror bound that grows at a rate of √log h and this is sufﬁcient for our proof.\nThe main challenge in the proof of theorem 4 is controlling the growth of the term t(h)/γ(h). Notice that this term\nis a product of h terms of the form 1 +\nγ(k)\nkβΓ(k), so any error in estimating γ could blow up the product by a factor of\nh. We could show that the regret is multiplied by an additional function g(h) which is parameterized by β. We next\ninstantiate theorem 4 for different discount factors and show that we can obtain regret bounds similar to corollary 2,\nand 3 up to logarithmic factors.\n9\nCorollary 5. Consider the discount factor γ(h) = h−p for p ≥2. Then the regret of algorithm 2 is\nReg(T) ≤\n\n\n\neO\n\u0010√\nSAT\np+1\n2p\n\u0011\nif T ≥O\n\u0000(S3/2A1/2)p\u0001\neO\n\u0010\nS2AT\n1\n2−1/p\n\u0011\nif T ≤O\n\u0000(S3/2A1/2)p\u0001\nCorollary 6. Consider the geometric discount factor γ(h) = γh−1 for γ ∈[0, 1) and suppose\nT\nlog3 T ≥\nS3A\n(1−γ)4 . Then\nalgorithm 2 has regret at most eO\n\u0010√\nSAT/(1 −γ)1.5\u0011\n.\nFor the polynomial discounting we get a regret of the order of T (p+1)/2p which is worse than the regret bound of\ntheorem 1 by a factor of T 1/2p. However, the difference goes to zero as p increases and approaches the same limit\nof eO(\n√\nT). On the other hand, for geometric discounting we recover the same regret as corollary 3. Interestingly,\nHe et al. [HZG21] obtained a similar bound on regret for the non-episodic setting where the learner maximizes her\nlong-term geometrically distributed reward.\nUnknown N(∆): Note that algorithm 2 takes as input the optimal value of N(∆) or H⋆. However, this problem\ncan be handled through a direct application of model selection algorithms in online learning [Cut+21]. Let Reg(H⋆)\nbe the regret when algorithm 2 is run with true H⋆. We now instantiate algorithm 2 for different choices of H⋆and\nperform model selection over them. In particular, we can consider H⋆= 2, 22, . . . , 2O(log T ) as it is sufﬁcient to\nconsider H⋆= O(T). Moreover, given true H⋆there exists eH ≤2H⋆for which the regret is increased by at most a\nconstant. This step requires bounding t(H⋆)\nγ(H⋆)/ t( f\nH)\nγ( e\nH) and is constant for the discounting factors considered in the paper.\nWe now apply algorithm 1 from [Cut+21] to the collection of O(log T) models and obtain a regret bound of at most\nO\n\u0010√log TReg( eH)\n\u0011\n= eO(Reg(H⋆)).\n6\nExperiments\nWe evaluated the performance of our algorithm on the Taxi environment, a 5×5 grid-world environment introduced by\n[Die00]. The details of this environment is provided in the appendix, since the exact details are not too important for\nunderstanding the experimental results. We considered 100 episodes and each episode length was generated uniformly\nat random from the following distributions. 3\n1. Geometric discounting γ(h) = γh−1.\n2. Polynomial discounting γ(h) = h−p.\n3. Quasi-Hyperbolic discounting γ(h) = β1{h>1}γh−1\nFigure 1 shows some representative parameters for three different types of discounting. For the geometric discounting,\nwe show γ = 0.9, 0.95 and 0.975. For the polynomial discounting we generated the horizon lengths from a polynomial\nwith p ∈{1.4, 1.6, 2.0} and added an offset of 20. Finally, for the Quasi-hyperbolic discounting, we ﬁxed γ at 0.95\nand considered three values of β: 0.7, 0.8, and 0.9.\nWe compared our algorithm (1) with two variants of UCB-VI [AOM17] – (a) UCB-VI-Hoeffding computes\nbonus terms using Chernoff-Hoeffding inequality, and (b) UCB-VI-Bernstein computes bonus terms using Bernstein-\nFreedman inequality. It is known that when the horizon length is ﬁxed and known, UCB-VI-Bernstein achieves\nminimax optimal regret bounds. We implemented two versions of UCB-VI with three different assumed horizon\nlengths.\nFigure 1 shows that, for several situations, our algorithm strict improves in regret compared to all the other vari-\nants of UCB-VI. These include Geometric discounting (γ = 0.95 and 0.975) and Quasi-Hyperbolic discounting (all\npossible choices of β). For the other scenarios (e.g. polynomial discounting), our algorithm performs as well as the\nbest version UCB-VI. Figure 1 also highlights the importance of choosing not only the right horizon length but also\nthe correct update equation in backward induction. Consider for example, ﬁgure 1b for the geometric discounting\n3Here γ(h) refers to probability that the episode lengths exceeds h i.e. γ(h) = Pr(H ≥h).\n10\nwith γ = 0.95. Here the expected horizon length is\n1\n1−γ = 20. However, different UCB-VI variants (horizon lengths\n10, 20, 30 and Bernstein and Hoeffding variants) perform worse. Our algorithm beneﬁts by choosing the right effective\nhorizon length, and also the correct update equation (6).\n7\nConclusion\nIn this paper, we have designed reinforcement learning algorithms when the episode lengths are uncertain and drawn\nfrom a ﬁxed distribution. Our general learning algorithm (1) and result (theorem 1) can be instantiated for different\ntypes of distributions to obtain sub-linear regret bounds. Some interesting directions of future work include extension\nof our algorithm to function approximation [Jin+20], changing probability transition function [Jin+18], etc. We are\nalso interested in other models of episode lengths. For example, one can consider a setting where the lengths are\nadversarially generated but there is a limit on the total amount of change. This is similar to the notion of variation\nbudget [BGZ14] considered in the literature on non-stationary multi-armed bandits.\nAcknowledgements\nGoran Radanovi´c acknowledges that his research was, in part, funded by the Deutsche Forschungsgemeinschaft (DFG,\nGerman Research Foundation) – project number 467367360.\nReferences\n[Agg+16]\nCharu C Aggarwal et al. Recommender systems. Vol. 1. Springer, 2016 (cit. on p. 1).\n[Ain75]\nGeorge Ainslie. “Specious reward: a behavioral theory of impulsiveness and impulse control.” In: Psy-\nchological bulletin 82.4 (1975), p. 463 (cit. on p. 2).\n[AOM17]\nMohammad Gheshlaghi Azar, Ian Osband, and R´emi Munos. “Minimax regret bounds for reinforcement\nlearning”. In: International Conference on Machine Learning. PMLR. 2017, pp. 263–272 (cit. on pp. 1,\n2, 6–8, 10, 17, 18).\n[Ber12]\nDimitri Bertsekas. Dynamic programming and optimal control: Volume I. Vol. 1. Athena scientiﬁc, 2012\n(cit. on p. 2).\n[BGZ14]\nOmar Besbes, Yonatan Gur, and Assaf Zeevi. “Stochastic multi-armed-bandit problem with non-stationary\nrewards”. In: Advances in neural information processing systems 27 (2014) (cit. on p. 11).\n[Bro+16]\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Woj-\nciech Zaremba. “Openai gym”. In: arXiv preprint arXiv:1606.01540 (2016) (cit. on p. 14).\n[BT91]\nDimitri P Bertsekas and John N Tsitsiklis. “An analysis of stochastic shortest path problems”. In: Mathe-\nmatics of Operations Research 16.3 (1991), pp. 580–595 (cit. on p. 2).\n[Coh+21]\nAlon Cohen, Yonathan Efroni, Yishay Mansour, and Aviv Rosenberg. “Minimax regret for stochastic\nshortest path”. In: Advances in Neural Information Processing Systems 34 (2021), pp. 28350–28361 (cit.\non p. 3).\n[Cut+21]\nAshok Cutkosky, Christoph Dann, Abhimanyu Das, Claudio Gentile, Aldo Pacchiano, and Manish Puro-\nhit. “Dynamic balancing for model selection in bandits and rl”. In: International Conference on Machine\nLearning. PMLR. 2021, pp. 2276–2285 (cit. on p. 10).\n[Die00]\nThomas G Dietterich. “Hierarchical reinforcement learning with the MAXQ value function decomposi-\ntion”. In: Journal of artiﬁcial intelligence research 13 (2000), pp. 227–303 (cit. on pp. 9, 10, 14).\n[DKW56]\nAryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz. “Asymptotic minimax character of the sample dis-\ntribution function and of the classical multinomial estimator”. In: The Annals of Mathematical Statistics\n(1956), pp. 642–669 (cit. on pp. 9, 23).\n11\n[Fed+19]\nWilliam Fedus, Carles Gelada, Yoshua Bengio, Marc G Bellemare, and Hugo Larochelle. “Hyperbolic\ndiscounting and learning over multiple horizons”. In: arXiv preprint arXiv:1902.06865 (2019) (cit. on\np. 2).\n[GM04]\nLeonard Green and Joel Myerson. “A discounting framework for choice with delayed and probabilistic\nrewards.” In: Psychological bulletin 130.5 (2004), p. 769 (cit. on p. 2).\n[HM72]\nRonald A Howard and James E Matheson. “Risk-sensitive Markov decision processes”. In: Management\nscience 18.7 (1972), pp. 356–369 (cit. on p. 3).\n[HZG21]\nJiafan He, Dongruo Zhou, and Quanquan Gu. “Nearly minimax optimal reinforcement learning for dis-\ncounted MDPs”. In: Advances in Neural Information Processing Systems 34 (2021), pp. 22288–22300\n(cit. on pp. 2, 7, 10).\n[Jin+18]\nChi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. “Is Q-learning provably efﬁcient?”\nIn: Advances in neural information processing systems 31 (2018) (cit. on pp. 1, 2, 11).\n[Jin+20]\nChi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. “Provably efﬁcient reinforcement learning\nwith linear function approximation”. In: Conference on Learning Theory. PMLR. 2020, pp. 2137–2143\n(cit. on pp. 2, 11).\n[JL20]\nTiancheng Jin and Haipeng Luo. “Simultaneously learning stochastic and adversarial episodic mdps with\nknown transition”. In: Advances in neural information processing systems 33 (2020), pp. 16557–16566\n(cit. on p. 2).\n[KBP13]\nJens Kober, J Andrew Bagnell, and Jan Peters. “Reinforcement learning in robotics: A survey”. In: The\nInternational Journal of Robotics Research 32.11 (2013), pp. 1238–1274 (cit. on p. 1).\n[KLC98]\nLeslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. “Planning and acting in partially\nobservable stochastic domains”. In: Artiﬁcial intelligence 101.1-2 (1998), pp. 99–134 (cit. on p. 3).\n[Mas90]\nPascal Massart. “The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality”. In: The annals of\nProbability (1990), pp. 1269–1283 (cit. on p. 23).\n[Maz85]\nJames E Mazur. “Probability and delay of reinforcement as factors in discrete-trial choice”. In: Journal\nof the Experimental Analysis of Behavior 43.3 (1985), pp. 341–351 (cit. on p. 2).\n[Nat89]\nBalas K Natarajan. “On learning sets and functions”. In: Machine Learning 4.1 (1989), pp. 67–97 (cit. on\np. 29).\n[OSB14]\nDaniel FO Onah, Jane Sinclair, and Russell Boyatt. “Dropout rates of massive open online courses: be-\nhavioural patterns”. In: EDULEARN14 proceedings 1 (2014), pp. 5825–5834 (cit. on p. 1).\n[Pit19]\nSilviu Pitis. “Rethinking the discount factor in reinforcement learning: A decision theoretic approach”.\nIn: Proceedings of the AAAI Conference on Artiﬁcial Intelligence. Vol. 33. 2019, pp. 7949–7956 (cit. on\np. 2).\n[Ros+20]\nAviv Rosenberg, Alon Cohen, Yishay Mansour, and Haim Kaplan. “Near-optimal regret bounds for\nstochastic shortest path”. In: International Conference on Machine Learning. PMLR. 2020, pp. 8210–\n8219 (cit. on p. 3).\n[SB14]\nShai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms.\nCambridge university press, 2014 (cit. on pp. 9, 29).\n[SB18]\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018 (cit. on\np. 1).\n[Soz98]\nPeter D Sozou. “On hyperbolic discounting and uncertain hazard rates”. In: Proceedings of the Royal\nSociety of London. Series B: Biological Sciences 265.1409 (1998), pp. 2015–2020 (cit. on pp. 1, 3).\n[Tar+21]\nJean Tarbouriech, Runlong Zhou, Simon S Du, Matteo Pirotta, Michal Valko, and Alessandro Lazaric.\n“Stochastic shortest path: Minimax, parameter-free and towards horizon-free regret”. In: Advances in\nNeural Information Processing Systems 34 (2021), pp. 6843–6855 (cit. on p. 3).\n12\n[WSY20]\nRuosong Wang, Russ R Salakhutdinov, and Lin Yang. “Reinforcement learning with general value func-\ntion approximation: Provably efﬁcient approach via bounded eluder dimension”. In: Advances in Neural\nInformation Processing Systems 33 (2020), pp. 6123–6135 (cit. on p. 2).\n[YW20]\nLin Yang and Mengdi Wang. “Reinforcement learning in feature space: Matrix bandit, kernels, and regret\nbound”. In: International Conference on Machine Learning. PMLR. 2020, pp. 10746–10756 (cit. on p. 2).\n13\nA\nPseudocode of Update-Q-Values\nALGORITHM 3: Update-Q-values\nInput: Discount factor {γ(h)}∞\nh=1, parameter ∆, episode k, and dataset H.\nNk(x, a, y) = P\n(x′,a′,y′)∈H 1 {x′ = x, a′ = a, y′ = y} for all (x, a, y) ∈S × A × S.\nNk(x, a) = P\ny Nk(x, a, y) for all (x, a) ∈S × A.\nSet ˆP(y|x, a) = Nk(x,a,y)\nNk(x,a) for all (x, a) s.t. Nk(x, a) > 0. Otherwise, ˆP(y|x, a) = 1\nS\n/* Update all the Q values\n*/\nQk,N(∆)+1(x, a) ←\n∆\nγ(N(∆)+1) for all x ∈S and a ∈A.\nVk,N(∆)+1(x, a) ←\n∆\nγ(N(∆)+1) for all x ∈S.\nfor h = N(∆), . . . , 1 do\n/* Define [ ˆPkVk,h+1](s, a) = Ex∼ˆ\nPk(·|s,a)\n\u0002\nVk,h+1(x)\n\u0003\nand let Γ(h) = P\nj≥h γ(j).\n*/\nUCBk,h(s, a) = 3Γ(h + 1)\nγ(h)\nln(SATN(∆)/δ)\np\nNk(s, a)\nQk,h(s, a) = min {Qk−1,h(s, a), r(s, a)\n+ γ(h + 1)\nγ(h)\n[ ˆPkVk,h+1](s, a) + UCBk,h(s, a)\n\u001b\nVk,h(s, a) = max\na∈A Qk,h(s, a)\nB\nAdditional Experimental Details\nIn the Taxi environment [Die00] environment, there are four randomly chosen designated locations indicated by Red,\nGreen, Blue, and Yellow. The driver (learner) starts at a random square and the passenger is at a random location. The\ntaxi drives to the passenger’s location, picks up the passenger (from one of the four locations), drives to the passenger’s\ndestination (another one of the four speciﬁed locations), and then drops off the passenger. There are six actions and\nthe observations include the location of the taxi and the starting point and the destination of the passenger. Each step\nincurs a cost of 1. There is a reward of 20 for successfully delivering a passenger, and a reward of −10 for executing\npickup and drop-off actions illegally. We used the Open-AI implementation Taxi-v3 [Bro+16] to interact with the\nenvironment.\nAll experiments were conducted on a computer cluster with machines equipped with2Intel XeonE5-2667 v2 CPUs\nwith 3.3GHz (16 cores) and 50 GB RAM.\nC\nEquivalence between Reg(π; γ) and Reg(π; PH(·))\nC.1\nProof of Lemma 1\nProof. Consider any learning algorithm π = {πk}k∈[T ].\nEHk [V πk(xk,1; Hk)] =\nX\nℓ\nPH(ℓ)E [V πk(xk,1; ℓ)] = E\n\"X\nℓ\nPH(ℓ)\nℓ\nX\nh=1\nr(xk,h, ak,h)\n#\n= E\n\" ∞\nX\nh=1\nPH(Hk ≥h)r(xk,h, ak,h)\n#\n= E\n\" ∞\nX\nh=1\nγ(h)r(xk,h, ak,h)\n#\n= V πk(xk,1; γ)\n14\nTherefore, we have the following result.\nT\nX\nk=1\nEHk [V πk(xk,1; Hk)] =\nT\nX\nk=1\nV πk(xk,1; γ)\n(7)\nWe now upper bound the quantity P\nℓV ⋆(xk,1; ℓ)PH(ℓ). Let π⋆\nk,ℓbe the policy that maximizes the ℓ-step value function\ni.e. V ⋆(xk,1; ℓ) = V π⋆\nk,ℓ(xk,1; ℓ). Let us also deﬁne a new policy π∞as follows.\n1. At step h, draw an index j ∈[h, ∞) according to the distribution\nPH(·)\nPH(H≥h).\n2. Take action π⋆\nk,j(xk,1, ak,1, . . . , xk,h−1, ak,h−1, xk,h).\nThen the inﬁnite-horizon discounted sum of rewards under the policy π∞is given as\nEπ∞\n\" ∞\nX\nh=1\nγ(h)r(xk,h, ak,h)\n#\n= E\n\" ∞\nX\nh=1\nPH(H ≥h)Eπ∞[r(xk,h, ak,h)|Hh]\n#\n= E\n\n\n∞\nX\nh=1\nPH(H ≥h)\n∞\nX\nj=h\nPH(j)\nPH(H ≥h)Eak,h∼π⋆\nk,j(Hh) [r(xk,h, ak,h)|Hh]\n\n\n= E\n\n\n∞\nX\nh=1\n∞\nX\nj=h\nPH(j)Eak,h∼π⋆\nk,j(Hh) [r(xk,h, ak,h)|Hh]\n\n\n= E\n\n\n∞\nX\nj=1\nPH(j)\nj\nX\nh=1\nEak,h∼π⋆\nk,j(Hh) [r(xk,h, ak,h)|Hh]\n\n\n= E\n\n\n∞\nX\nj=1\nPH(j)Eπ⋆\nk,j\n\" j\nX\nh=1\nr(xk,h, ak,h)\n#\n=\nX\nj\nV ⋆(xk,1; ℓ)PH(j)\nSince π∞is one candidate policy for maximizing inﬁnite horizon sum of discounted rewards, its value function is\nbounded by the optimal value function.\nT\nX\nk=1\nX\nℓ\nPH(ℓ)V ⋆(xk,1; ℓ) =\nT\nX\nk=1\nV π∞(xk,1; γ) ≤\nT\nX\nk=1\nV ⋆(xk,1; γ)\n(8)\nCombining equations 7 and 8 we get that the regret under the new general discounting environment upper bounds the\nregret under the original environment i.e. Reg(π; PH(·)) ≤Reg(π; γ).\nC.2\nProof of Lemma 2\nProof. Given a discount factor γ, we deﬁne the following distribution over episode lengths.\nPH(H = h) = γ(h) −γ(h + 1)\nThis deﬁnition implies that Pr(H ≥h) = γ(h) and under this condition lemma 1 already shows that PT\nk=1 EHk [V πk(xk,1; Hk)] =\nPT\nk=1 V πk(xk,1; γ). Therefore, it remains to upper bound the quantity PT\nk=1 V ⋆(xk,1; γ). Let π⋆= (π⋆\n1, π⋆\n2, . . .) be\nthe policy that maximizes long-term discounted sum of rewards with respect to the discount factor γ. Note that one\n15\ncan also play the policy π⋆for the ﬁnite horizon setting.\nX\nℓ\nPH(ℓ)V ⋆(xk,1; ℓ) ≥\nX\nℓ\nPH(ℓ)V π⋆(xk,1; ℓ) =\nX\nℓ\nPH(ℓ)Eπ⋆\n\n\nℓ\nX\nj=1\nr(xk,j, ak,j)\n\n\n= Eπ⋆\n\n\n∞\nX\nj=1\nX\nℓ≥j\n(γ(ℓ) −γ(ℓ+ 1))r(xk,j, ak,j)\n\n\n= Eπ⋆\n\n\n∞\nX\nj=1\nγ(j)r(xk,j, ak,j)\n\n= V ⋆(xk,1; γ)\nSince the optimal value under the new environment (episode distribution PH(·)) upper bounds the optimal value under\nthe discount factor γ, the regret under PH(·) can only get worse.\nD\nProof of theorem 1 and Related Corollaries\nWe ﬁrst show that the Q values at each time-step upper bounds the optimal Q values.\nLemma 3. With probability at least 1 −δ, for all k ∈[T], h ∈[N(∆)], x ∈S, a ∈A, we have\nQk,h(x, a) ≥Q⋆\nh(x, a), Vk,h(x) ≥V ⋆\nh (x)\nProof. We prove this statement using induction. First note that we always have Qk,N(∆)+1(x, a) ≥Q⋆\nN(∆)+1(x, a)\njust from the initialization.Suppose the given claim is true for all k′ ≤k −1 and also Qk,h′(x, a) ≥Q⋆\nk(x, a) for\nall h′ ≥h + 1. We now wish to show that Qk,h(x, a) ≥Q⋆\nk(x, a). We can assume that Qk,h(x, a) ̸= Qk−1,h(x, a)\notherwise the claim is true just from the induction hypothesis. Then we have,\nQk,h(x, a) −Q⋆\nh(x, a) = γ(h + 1)\nγ(h)\n\u0010\n[ ˆPkVk,h+1](x, a) −[PV ⋆\nh+1](x, a)\n\u0011\n+ UCBk,h(x, a)\n≥γ(h + 1)\nγ(h)\n\u0010\n[( ˆPk −P)V ⋆\nh+1](x, a)\n\u0011\n+ UCBk,h(x, a)\nThe last inequality follows because from the induction hypothesis Vk,h+1(y) ≥V ⋆\nh+1(y). We now bound Bernstein’s\ninequality and union bound to bound the ﬁrst term. Using the fact that V ⋆\nh+1(·) is uniformly bounded by Γ(h +\n1)/γ(h + 1), we get that the following bound holds with probability at least 1 −δ for any (x, a) ∈S × A, t ∈[T],\nand h ∈[N(∆)].\n\f\f\f[( ˆPk −P)V ⋆\nh ](x, a)\n\f\f\f ≤\ns\n2V⋆\nh(x, a) ln(SATN(∆)/δ)\nNk(x, a)\n+ 2Γ(h) ln(SATN(∆)/δ)\n3γ(h)Nk(x, a)\nWe now bound the variance term V⋆\nh+1(x, a) by (Γ(h + 1)/γ(h + 1))2 and substitute the above bound to obtain the\nfollowing bound on the different between Q values.\nQk,h(x, a) −Q⋆\nh(x, a) ≥−Γ(h + 1)\nγ(h)\n s\n2 ln(SATN(∆)/δ)\nNk(x, a)\n+ 2 ln(SATN(∆)/δ)\n3Nk(x, a)\n!\n+ UCBk,h(x, a)\nNow it follows from the deﬁnition of UCBk,h(x, a) that the ﬁnal term is non-negative.\n16\nD.1\nProof of theorem 1\nWe ﬁrst provide a formal statement of theorem 1.\nTheorem 7. With probability at least 1 −δ, Algorithm 1 has the following regret.\nReg(π; γ) ≤\n∆T\nγ(N(∆) + 1)t(N(∆) + 1) +\nmax\nh∈[N(∆)] t(h)γ(h + 1)\nγ(h)\nO\n\u0010p\nTN(∆) log(TN(∆)/δ)\n\u0011\n+\nmax\nh∈[N(∆)] t(h)Γ(h + 1)\nγ(h)\nO\n\u0010p\nSATN(∆)\n\u0011\n+\nmax\nh∈[N(∆)] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1)\neO\n\u0000S2A(N(∆))β\u0001\nProof. At episode k, time h the algorithm takes the best action according to Qk,h(·, ·). Therefore, πk,h(xk,h) =\nak,h ∈argmaxaQk,h(xk,h, a). Moreover, V πk(xk,h) = Qπk\nh (xk,h, πk,h(xk,h)) = Qπk(xk,h, ak,h).\ne∆k,h(xk,h) = Vk,h(xk,h) −V πk\nh (xk,h) = Qk,h(xk,h, ak,h) −Qπk\nh (xk,h, ak,h)\n≤γ(h + 1)\nγ(h)\n[ ˆPkVk,h+1 −PV πk\nh+1](xk,h, ak,h) + UCBk,h(xk,h, ak,h)\nNow we proceed similarly as the proof of theorem 1 from [AOM17] and derive a recurrence relation for eδk,h =\ne∆k,h(xk,h).\neδk,h ≤γ(h + 1)\nγ(h)\n[( ˆPk −P)(Vh,k+1 −V ⋆\nh+1)](xk,h, ak,h)\n+ γ(h + 1)\nγ(h)\n[P(Vk,h+1 −V πk\nh+1)](xk,h, ak,h)\n+ γ(h + 1)\nγ(h)\n[( ˆPk −P)V ⋆\nh+1](xk,h, ak,h)\n+ UCBk,h(xk,h, ak,h)\n= ck,h + γ(h + 1)\nγ(h)\n[P e∆k,h+1](xk,h, ak,h) + ek,h + bk,h\nwhere in the last line we used the following notations.\nck,h = γ(h + 1)\nγ(h)\n[( ˆPk −P)(Vk,h+1 −V ⋆\nh+1)](xk,h, ak,h)\n(9)\nek,h = γ(h + 1)\nγ(h)\n[( ˆPk −P)V ⋆\nh+1](xk,h, ak,h)\n(10)\nbk,h = UCBk,h(xk,h, ak,h)\n(11)\nFinally, writing εk,h as the following error term\nεk,h = γ(h + 1)\nγ(h)\n[P e∆k,h+1](xk,h, ak,h) −γ(h + 1)\nγ(h)\ne∆k,h+1(xk,h+1)\n(12)\nwe get the following recurrence relation for eδk,h.\neδk,h ≤γ(h + 1)\nγ(h)\neδk,h+1 + ck,h + ek,h + bk,h + εk,h\n(13)\nLet L = 2 ln(SATN(∆)/δ). Then we can apply Bernstein’s inequality (lemma 4) to conclude that with probability\nat least 1 −δ the following bound holds for each j ∈[N(∆)] and k ∈[T].\nek,j ≤γ(j + 1)\nγ(j)\n\n\ns\nLVary∼P(·|xk,j,ak,j)V ⋆\nj+1(y)\nNk(xk,j, ak,j)\n+ L maxy V ⋆\nj+1(y)\n3Nk(xk,j, ak,j)\n\n\n17\nNow instantiating the above bound for j = h and using the fact that V ⋆\nh+1(y) is bounded by Γ(h)/γ(h) we get the\nfollowing bound on ek,h.\nek,h ≤Γ(h + 1)\nγ(h)\n s\nL\nNk(xk,h, ak,h) +\nL\n3Nk(xk,h, ak,h)\n!\nNow we bound the term ck,h deﬁned in eq. (9).\nck,h = γ(h + 1)\nγ(h)\nh\n( ˆPk −P)(Vh,k+1 −V ⋆\nh+1)\ni\n(xk,h, ak,h)\n= γ(h + 1)\nγ(h)\nX\ny\n\u0010\nˆPk(y|xk,h, ak,h) −P(y|xk,h, ak,h)\n\u0011\n(Vk,h+1(y) −V ⋆\nh+1(y))\nWe now substitute two bounds. First we can use Bernstein inequality to bound the difference between estimated\nprobability and actual probability as\n\f\f\f ˆPk(y|xk,h, ak,h) −P(y|xk,h, ak,h)\n\f\f\f ≤\ns\n2P(y|xk,h, ak,h)(1 −P(y|xk,h, ak,h))L\nNk(xk,h, ak,h)\n+\n2L\n3Nk(xk,h, ak,h)\nSecond we bound Vk,h+1(y) −V ⋆\nh+1(y) ≤Vk,h+1(y) −V πk\nh+1(y) ≤e∆k,h+1(y). This gives us the following bound on\nck,h.\nck,h ≤γ(h + 1)\nγ(h)\nX\ny\ns\n2P(y|xk,h, ak,h)L\nNk(xk,h, ak,h)\ne∆k,h+1(y) + γ(h + 1)\nγ(h)\nX\ny\n2L\n3Nk(xk,h, ak,h)\ne∆k,h+1(y)\n≤γ(h + 1)\nγ(h)\n√\n2L\nX\ny\ns\nP(y|xk,h, ak,h)\nNk(xk,h, ak,h)\ne∆k,h+1(y)\n|\n{z\n}\n:=T1\n+\n2SLΓ(h + 1)\n3γ(h)Nk(xk,h, ak,h)\nWe now bound the term labelled T1. Let\n[y]k,h =\n(\ny : P(y|xk,h, ak,h)Nk(xk,h, ak,h) ≥2(h + 1)2βL\n\u0012Γ(h + 1)\nγ(h + 1)\n\u00132)\n.\n(14)\nHere we choose a threshold that is different than [AOM17]. In fact, we require the additional poly(h) factor to ensure\nconvergence of the ﬁnal recurrence relation. Moreover, the threshold is parameterized by the parameter β, and can be\nchosen based on the particular discount factor.\nT1 =\nX\ny∈[y]k,h\ns\nP(y|xk,h, ak,h)\nNk(xk,h, ak,h)\ne∆k,h+1(y) +\nX\ny /∈[y]k,h\ns\nP(y|xk,h, ak,h)\nNk(xk,h, ak,h)\ne∆k,h+1(y)\nThe ﬁrst term can be bounded as\nX\ny∈[y]k,h\ns\nP(y|xk,h, ak,h)\nNk(xk,h, ak,h)\ne∆k,h+1(y) = ˆεk,h +\ns\nP(xk,h+1|xk,h, ak,h)\nNk(xk,h, ak,h)\ne∆k,h+1(xk,h+1)1 {xk,h+1 ∈[y]k,h}\n≤ˆεk,h +\nγ(h + 1)\n√\n2L(h + 1)βΓ(h + 1)\neδk,h+1\n18\nwhere ˆεk,h is an error term deﬁned as follows.\nˆεk,h =\nX\ny∈[y]k,h\ns\nP(y|xk,h, ak,h)\nNk(xk,h, ak,h)\ne∆k,h+1(y)\n−\ns\nP(xk,h+1|xk,h, ak,h)\nNk(xk,h, ak,h)\ne∆k,h+1(xk,h+1)1 {xk,h+1 ∈[y]k,h}\n(15)\nIt can be checked that ˆεk,h is actually a Martingale difference sequence. The second term of T1 can be bounded as\nX\ny /∈[y]k,h\ns\nP(y|xk,h, ak,h)\nNk(xk,h, ak,h)\ne∆k,h+1(y) ≤\nX\ny /∈[y]k,h\np\nP(y|xk,h, ak,h)Nk(xk,h, ak,h)\nNk(xk,h, ak,h)\ne∆k,h+1(y)\n≤\n√\n2L(h + 1)βS\nNk(xk,h, ak,h)\n\u0012Γ(h + 1)\nγ(h + 1)\n\u00132\nSubstituting the bound on T1 in the upper bound on ck,h we get the following bound.\nck,h ≤γ(h + 1)\nγ(h)\n√\n2L\n \nˆεk,h +\nγ(h + 1)\n√\n2L(h + 1)βΓ(h + 1)\neδk,h+1 +\n√\n2L(h + 1)βS\nNk(xk,h, ak,h)\n\u0012Γ(h + 1)\nγ(h + 1)\n\u00132!\n+\n2SLΓ(h + 1)\n3γ(h)Nk(xk,h, ak,h)\n≤γ(h + 1)\nγ(h)\n√\n2Lˆεk,h +\n(γ(h + 1))2\nγ(h)Γ(h + 1)(h + 1)β eδk,h+1 + (h + 1)β(Γ(h + 1))2\nγ(h)γ(h + 1)\n2LS\nNk(xk,h, ak,h)\n|\n{z\n}\n:=fk,h\nSubstituting the previous upper bound on ch,k in eq. (13) and writing εk,h = γ(h + 1)/γ(h)ˆεk,h we get the ﬁnal\nrecurrence relation for eδk,h.\neδk,h ≤γ(h + 1)\nγ(h)\n\u0012\n1 +\nγ(h + 1)\n(h + 1)βΓ(h + 1)\n\u0013\neδk,h+1 +\n√\n2Lεk,h + ek,h + bk,h + εk,h + fk,h\n(16)\nThen expanding the recurrence relation eq. (16) from h = 1 to h = N(∆) we get the following equation.\neδk,1 ≤t(N(∆) + 1)eδk,N(∆)+1 +\nN(∆)\nX\nh=1\nt(h)\n\u0010√\n2Lεk,h + ek,h + bk,h + εk,h + fk,h\n\u0011\n(17)\nWe now use the fact that eδk,N(∆)+1 ≤∆/γ(N(∆) + 1) and obtain the following bound on eδk,1.\neδk,1 ≤\n∆\nγ(N(∆) + 1)t(N(∆) + 1) +\nN(∆)\nX\nh=1\nt(h)\n\u0010√\n2Lεk,h + ek,h + bk,h + εk,h + fk,h\n\u0011\nSumming over the T episodes the ﬁnal bound on regret is given as.\nReg(T) ≤\nT\nX\nk=1\neδk,1 ≤\n∆T\nγ(N(∆) + 1)t(N(∆) + 1) +\nT\nX\nk=1\nN(∆)\nX\nh=1\nt(h)\n\u0010√\n2Lεk,h + ek,h + bk,h + εk,h + fk,h\n\u0011\n(18)\nWe now bound the second term. First consider the exploration bonus term bk,h.\nT\nX\nk=1\nN(∆)\nX\nh=1\nt(h)bk,h =\nT\nX\nk=1\nN(∆)\nX\nh=1\nt(h)Γ(h + 1)\nγ(h)\nL\np\nNk(xk,h, ak,h)\n≤L\nmax\nh∈[N(∆)] t(h)Γ(h + 1)\nγ(h)\nX\nk,h\n1\np\nNk(xk,h, ak,h)\n19\nUsing a simple counting argument we can bound the last summation as follows. Here we use the fact that only the ﬁrst\nN(∆) steps of each episode are used to update the Nk(·, ·) counters.\nT\nX\nk=1\nN(∆)\nX\nh=1\n1\np\nNk(xk,h, ak,h)\n=\nX\nx,a\nNT (x,a)\nX\nn=1\n1\n√n ≤2\nX\nx,a\np\nNk(x, a)\n≤2\n√\nSA\nsX\nx,a\nNT (x, a) = 2\np\nSATN(∆)\nThis result gives us the following bound on the sum of bk,hterms.\nT\nX\nk=1\nN(∆)\nX\nh=1\nt(h)bk,h = L\nmax\nh∈[N(∆)] t(h)Γ(h + 1)\nγ(h)\nO\n\u0010p\nSATN(∆)\n\u0011\n(19)\nBy a similar argument we can prove the following bounds on the ek,h and fk,h terms.\nT\nX\nk=1\nN(∆)\nX\nh=1\nt(h)ek,h = L\nmax\nh∈[N(∆)] t(h)Γ(h + 1)\nγ(h)\nO\n\u0010p\nSATN(∆)\n\u0011\n(20)\nT\nX\nk=1\nN(∆)\nX\nh=1\nt(h)fk,h ≤L\nmax\nh∈[N(∆)] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1)O\n\u0000S2A(N(∆))β log(TN(∆))\n\u0001\n(21)\nWe now consider the martingale differences term εk,h and εk,h. If we write Fk,h to denote the sigma-algebra\ngenerated by the actions, and states until step h of episode k, then we have E[εk,h|Fk,h] = 0. Moreover, each εk,h is\nbouned by O\n\u0010\nΓ(h+1)\nγ(h)\n\u0011\n. So we can deﬁne a new set of martingales eεk,h = ϵk,h\nγ(h)\nΓ(h+1) so that |eεk,h| ≤1. Now, we\ncan apply the Azuma-Hoeffding inequality to get the following result with probability at least 1 −δ.\nT\nX\nk=1\nN(∆)\nX\nh=1\nt(h)εk,h ≤\nmax\nh∈[N(∆)] t(h)Γ(h + 1)\nγ(h)\nT\nX\nk=1\nN(∆)\nX\nh=1\n|eεk,h|\n≤\nmax\nh∈[N(∆)] t(h)Γ(h + 1)\nγ(h)\nO\n\u0010p\nTN(∆) log(TN(∆)/δ)\n\u0011\n(22)\nNow recall that we deﬁned εk,h = γ(h+1)\nγ(h) ˆεk,h where ˆεk,h was deﬁned in equation 15. This gives us the following\nexpression for εk,h.\nεk,h = γ(h + 1)\nγ(h)\nX\ny∈[y]k,h\ns\nP(y|xk,h, ak,h)\nNk(xk,h, ak,h)\ne∆k,h+1(y)\n−γ(h + 1)\nγ(h)\ns\nP(xk,h+1|xk,h, ak,h)\nNk(xk,h, ak,h)\ne∆k,h+1(xk,h+1)1 {xk,h+1 ∈[y]k,h}\nIt can be easily veriﬁed that E[εk,h|Fk,h] = 0. We now establish an upper bound on εk,h. Consider the ﬁrst term in\nthe deﬁnition of εk,h.\nγ(h + 1)\nγ(h)\nX\ny∈[y]k,h\ns\nP(y|xk,h, ak,h)\nNk(xk,h, ak,h)\ne∆k,h+1(y)\n= γ(h + 1)\nγ(h)\nX\ny∈[y]k,h\nP(y|xk,h, ak,h)\np\nP(y|xk,h, ak,h)Nk(xk,h, ak,h)\ne∆k,h+1(y)\n≤\n1\n√\n2L(h + 1)β\nγ(h + 1)\nγ(h)\nγ(h + 1)\nΓ(h + 1)\nΓ(h + 1)\nγ(h + 1) ≤\n1\n2\n√\nL\nγ(h + 1)\nγ(h)\n20\nThe last inequality uses the deﬁnition of [y]k,h (eq. (14)).The second term of εk,h can be bounded similarly. Therefore,\nagain using Azuma-Hoeffding inequality, we get the following result with probability at least 1 −δ.\nT\nX\nk=1\nN(∆)\nX\nh=1\n√\n2Lt(h)εk,h ≤\nmax\nh∈[N(∆)] t(h)γ(h + 1)\nγ(h)\nO\n\u0010p\nTN(∆) log(TN(∆)/δ)\n\u0011\n(23)\nSubstituting equations 19, 20, 21, 22, 23 in equation 18 we get the ﬁnal bound on regret.\nLemma 4 (Bernstein’s Inequality). Let Z1, . . . , Zn be i.i.d. random variables with values bounded by H and let\nδ > 0. Then with probability at least 1 −δ we have\nEZ1 −1\nn\nn\nX\ni=1\nZi ≤\nr\n2Var(Z1) ln(2/δ)\nn\n+ 2H ln(2/δ)\n3n\nD.2\nProof of Corollary 2\nProof. Consider the discount factor γ(h) = h−p for p ≥2. Recall that we deﬁned Γ(h) = P\nj≥h γ(j). We will use\nthe following bound on Γ(h) ∈\nh\nh−p+1\np−1 , h−p \u0010\n1 +\nh\np−1\n\u0011i\n. We substitute β = p −1 and bound various γ-dependent\nconstants appearing in the regret bound.\nt(N(∆) + 1)\nγ(N(∆) + 1) =\nN(∆)+1\nY\nj=2\n\u0012\n1 +\nγ(j)\njβΓ(j)\n\u0013\n≤\nN(∆)+1\nY\nj=2\n\u0012\n1 + p −1\nj1+β\n\u0013\n≤e(p−1) P\nj j−p ≤e\nmax\nh∈[N(∆)] t(h)Γ(h + 1)\nγ(h)\n=\nmax\nh∈[N(∆)] Γ(h + 1)\nh\nY\nj=2\n\u0012\n1 +\nγ(j)\njβΓ(j)\n\u0013\n≤e\nmax\nh∈[N(∆)] h−p\n\u0012\n1 +\nh\np −1\n\u0013\n= e\nThe last inequality follows because the ﬁnal term is a decreasing function of h for p > 1.\nmax\nh∈[N(∆)] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1) =\nmax\nh∈[N(∆)]\n(Γ(h + 1))2\nγ(h + 1)\nh\nY\nj=2\n\u0012\n1 +\nγ(j)\njβΓ(j)\n\u0013\n≤e\nmax\nh∈[N(∆)](h + 1)−p\n\u0012\n1 + h + 1\np −1\n\u00132\n≤e\nHere we use p ≥2 to conclude that the term inside max is non-increasing. We now bound the term N(∆). Recall that\nN(∆) is the value of h such that Γ(h) is bounded from above by ∆. Using the lower bound of h−p+1/(p −1) we get\nN(∆) should be at least (∆(p −1))−1/(p−1).Substituting the value of N(∆) and the bound on the three constants in\nthe expression for regret, we get the following bound on regret.\nReg(T) ≤e∆T + e (∆(p −1))−\n1\n2(p−1)\nh\neO(\n√\nSAT) + eO(\n√\nT)\ni\n+ e (∆(p −1))−1/2\np−1 eO(S2A)\nFor T ≥O(S3A) then the last term is dominated by the other terms in the expression above. Finally, substituting\n∆= T −p−1\n2p−1 (p −1)−\n1\n2p−1 we get the following bound on regret.\nReg(T) ≤(p −1)−\n1\n2p−1 S1/2A1/2T\np\n2p−1 = O\n\u0010\nS1/2A1/2T\np\n2p−1\n\u0011\nThe last equality uses the fact that p−O(1/p) = O(1) for p ≥2.\n21\n1 < p < 2: We now substitute β = p −1. Proceeding similarly as earlier, we can establish the following inequali-\nties.\nt(N(∆) + 1)\nγ(N(∆) + 1) ≤e(p−1)/β = e\nmax\nh∈[N(∆)] t(h)Γ(h + 1)\nγ(h)\n≤e2\nmax\nh∈[N(∆)] h−p\n\u0012\n1 +\nh\np −1\n\u0013\n≤e\np\np −1\nThe third term is different for the case of 1 < p < 2.\nmax\nh∈[N(∆)] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1) ≤e\nmax\nh∈[N(∆)](h + 1)−p\n\u0012\n1 + h + 1\np −1\n\u00132\n≤e\nmax\nh∈[N(∆)](h + 1)−p\n\u0012\n1 + 2h + 1\np −1 + (h + 1)2\n(p −1)2\n\u0013\n≤e\n\u0012\np\n2(p −1) + (1 + N(∆))2−p\n(p −1)2\n\u0013\n≤\n2e\np −1(N(∆))2−p\nWe now substitute the inequalities above and use N(∆) = (∆(p −1))−1/(p−1).\nReg(T) ≤e∆T + e\np\np −1\n√\nSAT(∆(p −1))−1/2(p−1)\n+\n2e\np −1(∆(p −1))−1/(p−1)S2A log(T) + e\np\nT log T(∆(p −1))−1/2(p−1)\nWe substitute ∆= T −p−1\n2p−1 to get the following bound on regret.\nReg(T) ≤eT p/(2p−1) + e\np\np −1(p −1)−1/2(p−1)T p/(2p−1)√\nSA\n+\n2e\np −1(p −1)−1/(p−1)T 1/(2p−1)S2A log T + e(p −1)−1/2(p−1)T p/(2p−1)p\nlog T\nIf T ≥O\n\u0000(S3A)(2p−1)/2(p−1)\u0001\nthen the ﬁrst term dominates the third term and we get the following bound on regret.\nReg(T) ≤4e\np\np −1T\np\n2p−1 p\nSA log T(p −1)−1/(p−1)\nD.3\nProof of Corollary 3\nProof. For discount factor γ(h) = γh−1, it is easy to check that Γ(h) = γh−1/(1 −γ). We will substitute β = 3/2\nand use the following inequality.\nh\nY\nj=2\n\u0012\n1 +\nγ(j)\njβΓ(j)\n\u0013\n=\nh\nY\nj=2\n\u0012\n1 + 1 −γ\nj3/2\n\u0013\n= exp\n\n(1 −γ)\nh\nX\nj=2\n1\nj3/2\n\n≤e1−γ\nThis gives the following bounds on the three constants.\nt(N(∆) + 1)\nγ(N(∆) + 1) =\nN(∆)+1\nY\nj=2\n\u0012\n1 +\nγ(j)\njβΓ(j)\n\u0013\n≤e1−γ\nmax\nh∈[N(∆)] t(h)Γ(h + 1)\nγ(h)\n=\nmax\nh∈[N(∆)] Γ(h + 1)\nh\nY\nj=2\n\u0012\n1 +\nγ(j)\njβΓ(j)\n\u0013\n≤\nmax\nh∈[N(∆)]\nγh\n1 −γ e1−γ ≤\nγ\n1 −γ e1−γ\n22\nNow let us consider the second constant.\nmax\nh∈[N(∆)] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1) =\nmax\nh∈[N(∆)]\n(Γ(h + 1))2\nγ(h + 1)\nh\nY\nj=2\n\u0012\n1 +\nγ(j)\njβΓ(j)\n\u0013\n≤\nmax\nh∈[N(∆)]\nγh\n(1 −γ)2 e1−γ ≤\nγ\n(1 −γ)2 e1−γ\nSince Γ(h) = γh/(1 −γ), it is easy to show that N(∆) = log(1/∆(1−γ))\nlog(1/γ)\n. Substituting the bounds on the constants\nand the bound on N(∆) we get the following bound on the regret.\nReg(T) ≤e1−γ∆T + e1−γ\n1 −γ\ns\nlog(1/∆(1 −γ))\nlog(1/γ)\n\u0010\neO(\n√\nSAT) + eO(\n√\nT)\n\u0011\n+\ne1−γ\n(1 −γ)2\n\u0012log(1/∆(1 −γ))\nlog(1/γ)\n\u0013β\neO(S2A)\nIf T ≥\n1\n(1−γ)2 (N(∆))2 eO(S3A) then the last term in the expression is dominated by the other terms. We further use\nthe inequality log(1/γ) ≥1 −γ to simplify the regret expression.\nReg(T) ≤e1−γ∆T +\n1\n(1 −γ)1.5\np\nlog(1/∆(1 −γ)) eO(\n√\nSAT)\nto get a regret bound of eO\n\u0010 √\nSAT\n(1−γ)1.5\n\u0011\n. Notice that we need to satisfy the following bound on T.\nT ≥\n1\n(1 −γ)2\n\u0012log(1/∆(1 −γ))\nlog(1/γ)\n\u00132\neO(S3A)\nFor ∆= T −1/(1 −γ) the above bound is equivalent to the condition T ≥eO\n\u0000S3A/(1 −γ)4\u0001\nE\nProof of theorem 4 and Related Corollaries\nWe ﬁrst provide a formal statement of theorem 4.\nTheorem 8. When run with horizon length H⋆, algorithm 2 has the following regret bound with probability at least\n1 −δ\nReg(π; γ) ≤min\nL∈[T ]\n\u0010\nTΓ(L + 1) + 2L log(T)\n√\nT)\n\u0011\n+ Γ(H⋆+ 1)T + eO\n\u0010√\nT\n\u0011\n+ log(T) t(H⋆+ 1)\nγ(H⋆+ 1)g(H⋆+ 1) + max\nh∈[H⋆]\nt(h)\nγ(h)g(h)Γ(h + 1)\n\u0012\n1 + O(T −1/4)\nΓ(h + 1)\n\u0013\neO\n\u0010√\nSATH⋆\n\u0011\n+ log2(T) log(TH⋆) max\nh∈[H⋆] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1)g(h)\n\u0012\n1 + O(T −1/4)\nΓ(h + 1)\n\u00132\nO\n\u0000S2A(H⋆)β\u0001\n+ log5/2(T) max\nh∈[H⋆]\nt(h)\nγ(h)γ(h + 1)g(h)\n\u0012\n1 + O(T −1/4)\nγ(h + 1)\n\u0013\nO\n\u0010p\nTH⋆log(TH⋆/δ)\n\u0011\nwhere g(h) = exp\nn\nO\n\u0010Ph\nk=2\nT −1/4\nγ(k)+kβΓ(k)\n\u0011o\n.\nProof. Since each ˆγj is the empirical distribution function, by the Dvoretzky–Kiefer–Wolfowitz inequality [DKW56;\nMas90] and a union bound over the log(T/B) blocks the following result holds.\nPr\n \n∀j sup\nh\n|ˆγj(h) −γ(h)| >\nr\nlog (2 log(T/B)/δ)\n2jB\n!\n≤δ\n23\nFor the remainder of the proof we will assume ∥ˆγj −γ∥∞≤εj for εj =\nq\nlog(2 log(T/B)/δ)\n2jB\n. Now consider any\nj > 1. Since algorithm 1 is run for 2jB episodes with estimate ˆγj we have\nX\nk∈Bj\nV ⋆(xk,1; bγj) −V πk(xk,1; bγj) ≤R(H⋆, ˆ∆j, 2jB, bγj)\nHere we write Bj to denote the episodes in block j and R(H, ∆, T, bγj) is the regret bound derived in theorem 1 with\nN(∆) = H and discount factor bγj. We establish a lower bound on V ⋆(xk,1; bγj).\nV ⋆(xk,1; γ) = Eπ⋆(γ)\n\" ∞\nX\nh=1\nγ(h)r(xk,h, ak,h)\n#\n≤Eπ⋆(γ)\n\" L\nX\nh=1\nγ(h)r(xk,h, ak,h)\n#\n+ Γ(L + 1)\n≤Eπ⋆(γ)\n\" L\nX\nh=1\nˆγj(h)r(xk,h, ak,h)\n#\n+ εjL + Γ(L + 1)\n≤V ⋆(xk,1; bγj) + εjL + Γ(L + 1)\nThe ﬁrst inequality uses the fact that rewards are bounded by 1 and Γ(L+1) = P\nh≥L+1 γ(h). The second inequality\nuses maxh |ˆγj(h) −γ(h)| ≤εj. This gives us the following lower bound.\nV ⋆(xk,1; bγj) ≥V ⋆(xk,1; γ) −εjL −Γ(L + 1)\n(24)\nWe now derive an upper bound on V πk(xk,1; bγj).\nV πk(xk,1; γ) = Eπk\n\" ∞\nX\nh=1\nγ(h)r(xk,h, ak,h)\n#\n≥Eπk\n\" L\nX\nh=1\nγ(h)r(xk,h, ak,h)\n#\n−Γ(L + 1)\n≥Eπk\n\" L\nX\nh=1\nˆγ(h)r(xk,h, ak,h)\n#\n−εjL −Γ(L + 1)\n= V πk(xk,1; bγj) −Eπk\n\"\n∞\nX\nh=L+1\nˆγ(h)r(xk,h, ak,h)\n#\n−εjL −Γ(L + 1)\n≥V πk(xk,1; bγj) −ˆΓj(L + 1) −εjL −Γ(L + 1)\n(25)\nWe now use lemma 5 with D = T and a union bound over the log(T/B) blocks to get the following bound for all\nj ∈[log(T/B)] and h ∈[T].\n\f\f\fˆΓj(h) −Γ(h)\n\f\f\f ≤\ns\nlog T + log(log(T/B)/δ)\n|Bj|\n≤\n\u0010\n1 +\np\nlog T\n\u0011\nεj\nSubstituting the above bound in equation 25 and rearranging we get the following bound.\nV πk(xk,1; bγ) ≤V πk(xk,1; γ) + εj\n\u0010\nL + 1 +\np\nlog T\n\u0011\n(26)\n24\nUsing equations 24 and 26 we can bound the regret of algorithm 2 during the episodes of block Bj.\nX\nk∈Bj\nV ⋆(xk,1; γ) −V πk(xk,1; γ)\n≤\nX\nk∈Bj\nV ⋆(xk,1; bγ) −V πk(xk,1; bγ) + Γ(L + 1) + εj\n\u0010\n2L + 1 +\np\nlog T\n\u0011\n≤R(H⋆, ˆ∆j, 2jB, bγj) + |Bj| Γ(L + 1) + |Bj| εj\n\u0010\n2L + 1 +\np\nlog T\n\u0011\nWe use the fact that εj =\nq\nlog(2 log(T/B)/δ\n|Bj|\nand sum over all j = 1, . . . , log(T/B).\nT\nX\nt=1\nV ⋆(xt,1; γ) −V πt(xt,1; γ) ≤\nlog(T/B)\nX\nj=1\nR(H⋆, ˆ∆j, 2jB, bγj)\n+\nX\nj\n|Bj| Γ(L + 1) +\nlog(T/B)\nX\nj=1\np\nlog(2 log(T/B)/δ)O\nq\n|Bj|\n\u0010\n2L + 1 +\np\nlog T\n\u0011\nNow we use P\nj |Bj| = T and P\nj\np\n|Bj| ≤\np\nlog(T/B)\nqP\nj |Bj| = O\n\u0010p\nT log(T/B)\n\u0011\n, and B =\n√\nT log T log(log(T)/δ)\nto get the following bound on regret.\nT\nX\nt=1\nV ⋆(xt,1; γ) −V πt(xt,1; γ) ≤\nlog(T/B)\nX\nj=1\nR(H⋆, ˆ∆j, 2jB, bγj)\n+ min\nL∈[T ]\n\u0010\nTΓ(L + 1) + 2L log(T)\n√\nT)\n\u0011\nWe now bound the ﬁrst term by substituting the deﬁnition of of R(H⋆, ˆ∆j, |Bj| , bγj).\nlog(T/B)\nX\nj=1\nR(H⋆, ˆ∆j, |Bj| , bγj) ≤\nlog(T/B)\nX\nj=1\nˆ∆j |Bj|\nˆγj(H⋆+ 1)\nˆtj(H⋆+ 1)\n+\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nˆtj(h)\nˆΓj(h + 1)\nˆγj(h)\nO\n\u0012q\nSA |Bj| H⋆\n\u0013\n+\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nˆtj(h) (ˆΓj(h + 1))2\nˆγj(h)ˆγj(h + 1)O\n\u0010\nS2AH⋆β log(|Bj| H⋆)\n\u0011\n+\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nˆtj(h) ˆγj(h + 1)\nˆγj(h)\nO\n\u0012q\n|Bj| H⋆log(|Bj| H⋆/δ)\n\u0013\n(27)\nWe now bound the four terms separately. We will frequently use the following two identities.\nlog(T/B)\nX\nj=1\nεj ≤\n∞\nX\nj=1\nε1\n(\n√\n2)j−1 = O\n r\nlog(log(T/B)/δ)\nB\n!\n(28)\n25\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nˆtj(h)\nˆγj(h) =\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nh\nY\nk=2\n \n1 +\nˆγj(k)\nkβˆΓj(k)\n!\n≤\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nh\nY\nk=2\n\u0012\n1 +\nγ(k)\nkβΓ(k) + 2εj\n√log T\nkβΓ(k)\n\u0013\n[By lemma 6]\n=\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nt(h)\nγ(h)\nh\nY\nk=2\n\u0012\n1 +\n2εj\n√log T\nγ(k) + kβΓ(k)\n\u0013\n≤log(T/B) max\nh∈[H⋆]\nt(h)\nγ(h)\nlog(T/B)\nX\nj=1\nh\nY\nk=2\n\u0012\n1 +\n2εj\n√log T\nγ(k) + kβΓ(k)\n\u0013\n≤log(T/B) max\nh∈[H⋆]\nt(h)\nγ(h)\nlog(T/B)\nX\nj=1\nexp\n( h\nX\nk=2\n2εj\n√log T\nγ(k) + kβΓ(k)\n)\n≤log(T/B) max\nh∈[H⋆]\nt(h)\nγ(h) exp\n\n\n\nlog(T/B)\nX\nj=1\nh\nX\nk=2\n2εj\n√log T\nγ(k) + kβΓ(k)\n\n\n\n≤log(T/B) max\nh∈[H⋆]\nt(h)\nγ(h) exp\n(\nO\n r\nlog(log(T/B)/δ)\nB\nh\nX\nk=2\n2√log T\nγ(k) + kβΓ(k)\n!)\n[By eq. 28]\n≤log(T) max\nh∈[H⋆]\nt(h)\nγ(h) exp\n(\nO\n h\nX\nk=2\nT −1/4\nγ(k) + kβΓ(k)\n!)\n[As B =\n√\nT log T log(log(T)/δ)]\nNow writing g(h) = exp\nn\nO\n\u0010Ph\nk=2\nT −1/4\nγ(k)+kβΓ(k)\n\u0011o\nwe get the following inequality.\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nˆtj(h)\nˆγj(h) ≤log(T) max\nh∈[H⋆]\nt(h)\nγ(h)g(h)\n(29)\nlog(T/B)\nX\nj=1\nˆ∆j |Bj|\nˆγj(H⋆+ 1)\nˆtj(H⋆+ 1) =\nlog(T/B)\nX\nj=1\nˆΓj(H⋆+ 1) |Bj|\nˆγj(H⋆+ 1)\nˆtj(H⋆+ 1)\n≤\nlog(T/B)\nX\nj=1\n\u0010\n∆⋆+ 2εj\np\nlog T\n\u0011\n|Bj|\nˆtj(H⋆+ 1)\nˆγj(H⋆+ 1)\n[Since ∆⋆= Γ(H⋆+ 1), and using lemma 5]\n≤\nlog(T/B)\nX\nj=1\n\u0010\n∆⋆+ 2εj\np\nlog T\n\u0011\n|Bj|\nlog(T/B)\nX\nj=1\nˆtj(H⋆+ 1)\nˆγj(H⋆+ 1)\n[Since\nX\ni\naibi ≤\nX\ni\nai\nX\ni\nbi for ai, bi ≥0.]\n26\nThe ﬁrst summation can be bounded as follows.\nlog(T/B)\nX\nj=1\n\u0010\n∆⋆+ 2εj\np\nlog T\n\u0011\n|Bj| = ∆⋆T + 2\np\nlog T\nX\nj\nεj |Bj|\n= ∆⋆T + O\n\u0010p\nlog T\np\nlog(log(T/B)/δ)\n\u0011 X\nj\np\nBj\n≤∆⋆T + O\n\u0010√\nT log T log log T\n\u0011\nThe second summation can be bounded by log(T) t(H⋆+1)\nγ(H⋆+1)g(H⋆+ 1) by following the same steps used to derive\neq. (29). This gives us the following bound on the ﬁrst term in eq. (27).\nlog(T/B)\nX\nj=1\nˆ∆j |Bj|\nˆγj(H⋆+ 1)\nˆtj(H⋆+ 1) ≤∆⋆T + O\n\u0010√\nT log T log log T\n\u0011\n+ log(T) t(H⋆+ 1)\nγ(H⋆+ 1)g(H⋆+ 1)\n(30)\nNow we bound the second term in eq. (27).\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nˆtj(h)\nˆΓj(h + 1)\nˆγj(h)\n≤log(T)\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nt(h)\nγ(h)g(h)\n\u0010\nΓ(h + 1) + 2εj\np\nlog T\n\u0011\n[By inequality 29 and lemma 5]\n≤log(T) log(T/B) max\nh∈[H⋆]\nt(h)\nγ(h)g(h)Γ(h + 1)\n\n1 + 2√log T\nΓ(h + 1)\nlog(T/B)\nX\nj=1\nεj\n\n\n≤log(T) log(T/B) max\nh∈[H⋆]\nt(h)\nγ(h)g(h)Γ(h + 1)O\n \n1 + 2√log T\nΓ(h + 1)\nr\nlog(log(T/B)/δ)\nB\n!\n≤log2(T) max\nh∈[H⋆]\nt(h)\nγ(h)g(h)Γ(h + 1)O\n\u0012\n1 +\nT −1/4\nΓ(h + 1)\n\u0013\n[As B =\n√\nT log T log(log(T/B)/δ)]\nNow we have the following bound on the second term in eq. (27).\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nˆtj(h)\nˆΓj(h + 1)\nˆγj(h)\nO\n\u0012q\nSA |Bj| H⋆\n\u0013\n≤\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nˆtj(h)\nˆΓj(h + 1)\nˆγj(h)\nlog(T/B)\nX\nj=1\nO\n\u0012q\nSA |Bj| H⋆\n\u0013\n≤log2(T) max\nh∈[H⋆]\nt(h)\nγ(h)Γ(h + 1)g(h)O\n\u0012\n1 +\nT −1/4\nΓ(h + 1)\n\u0013 p\nlog(T/B)\nv\nu\nu\nt\nlog(T/B)\nX\nj=1\nSA |Bj| H⋆\n≤log5/2(T) max\nh∈[H⋆]\nt(h)\nγ(h)g(h)Γ(h + 1)\n\u0012\n1 + O(T −1/4)\nΓ(h + 1)\n\u0013\nO\n\u0010√\nSATH⋆\n\u0011\n(31)\n27\nFollowing similar steps, we can also bound the fourth term in equation 27.\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nˆtj(h) ˆγj(h + 1)\nˆγj(h)\nO\n\u0012q\n|Bj| H⋆log(|Bj| H⋆/δ)\n\u0013\n≤log5/2(T) max\nh∈[H⋆]\nt(h)\nγ(h)γ(h + 1)g(h)\n\u0012\n1 + O(T −1/4)\nγ(h + 1)\n\u0013\nO\n\u0010p\nTH⋆log(TH⋆/δ)\n\u0011\n(32)\nWe now consider the third term in equation 27.\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nˆ\ntj(h) (ˆΓj(h + 1))2\nˆγj(h)ˆγj(h + 1)\n≤log(T)\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nt(h)\nγ(h)g(h)\n\u0012Γ(h + 1)\nγ(h + 1) + 2εj\n√log T\nγ(h + 1)\n\u0013 \u0010\nΓ(h + 1) + 2εj\np\nlog T\n\u0011\n[By inequality 29, and lemma 5, and 7]\n≤log(T) max\nh∈[H⋆] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1)g(h)\nlog(T/B)\nX\nj=1\n\u0012\n1 + 2εj\n√log T\nΓ(h + 1)\n\u0013 \u0012\n1 + 2εj\n√log T\nΓ(h + 1)\n\u0013\n≤log(T) log(T/B) max\nh∈[H⋆] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1)g(h)\n\n1 +\nX\nj\n2εj\n√log T\nΓ(h + 1)\n\n\n2\n≤log(T) log(T/B) max\nh∈[H⋆] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1)g(h)\n \n1 + 2√log T\nΓ(h + 1)\nr\nlog(log(T/B)/δ)\nB\n!2\n≤log2(T) max\nh∈[H⋆] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1)g(h)\n\u0012\n1 + O(T −1/4)\nΓ(h + 1)\n\u00132\nThis gives the following bound on the third term.\nlog(T/B)\nX\nj=1\nmax\nh∈[H⋆]\nˆ\ntj(h) (ˆΓj(h + 1))2\nˆγj(h)ˆγj(h + 1)O\n\u0000S2A(H⋆)β log(|Bj| H⋆)\n\u0001\n≤log2(T) log(TH⋆) max\nh∈[H⋆] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1)g(h)\n\u0012\n1 + O(T −1/4)\nΓ(h + 1)\n\u00132\nO\n\u0000S2A(H⋆)β\u0001\n(33)\nSubstituting bounds 30, 31, 33, and 32 in eq. 27 we get the following bound.\nlog(T/B)\nX\nj=1\nR(H⋆, ˆ∆j, |Bj| , bγj) ≤∆⋆T + O\n\u0010√\nT log T log log T\n\u0011\n+ log(T) t(H⋆+ 1)\nγ(H⋆+ 1)g(H⋆+ 1)\n+ log5/2(T) max\nh∈[H⋆]\nt(h)\nγ(h)g(h)Γ(h + 1)\n\u0012\n1 + O(T −1/4)\nΓ(h + 1)\n\u0013\nO\n\u0010√\nSATH⋆\n\u0011\n+ log2(T) log(TH⋆) max\nh∈[H⋆] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1)g(h)\n\u0012\n1 + O(T −1/4)\nΓ(h + 1)\n\u00132\nO\n\u0000S2A(H⋆)β\u0001\n+ log5/2(T) max\nh∈[H⋆]\nt(h)\nγ(h)γ(h + 1)g(h)\n\u0012\n1 + O(T −1/4)\nγ(h + 1)\n\u0013\nO\n\u0010p\nTH⋆log(TH⋆/δ)\n\u0011\n28\nLemma 5. Let H1, . . . , Hn be iid according to a distribution D. For h ∈{1, . . . , D} let ˆΓn(h) = P\nu≥h ˆγn(h). Then\nwith probability at least 1 −δ the following holds.\nmax\nh∈{1,...,D}\n\f\f\fˆΓn(h) −Γ(h)\n\f\f\f ≤\nr\nlog D + log(1/δ)\nn\nProof. We will prove this statement by ﬁrst constructing a class of functions, and proving uniform convergence over\nthis class. In particular the class consists of a ﬁnite number of multi-valued functions, and we will use the bounded\nNatarajan dimension [Nat89] of this class to derive uniform convergence guarantees.\nLet hv : N →N be deﬁned by fv(x) = max{0, x−v}. Consider the class of functions H = {fv : v ∈{1, 2, . . . , D}}.\nLet H1, . . . , Hn be n random variables drawn i.i.d. from some distribution D. Then for any h ∈{1, . . . , D} we have\n1\nn\nn\nX\ni=1\nfh(Hi) = 1\nn\nn\nX\ni=1\nmax{Hi −h, 0} = 1\nn\nn\nX\ni=1\n∞\nX\nu=h\n1 {Hi > u}\n= 1\nn\nn\nX\ni=1\n∞\nX\nu=h\n(1 −1 {Hi ≤h}) =\n∞\nX\nu=h\n1\nn\nn\nX\ni=1\n(1 −1 {Hi ≤u})\n=\n∞\nX\nu=h\n\u0010\n1 −ˆFn(u)\n\u0011\n=\n∞\nX\nu=h\nˆγn(u) = ˆΓn(h)\nSimilarly one can show that EH∼D[fh(H)] = Γ(h). Then by the multiclass fundamental theorem [SB14] the follow-\ning result holds as long as n ≥Ndim(H) log(D)+log(1/δ)\nε2\n.\nPr\n \nmax\nh∈H\n\f\f\f\f\f\n1\nn\nn\nX\ni=1\nfh(Hi) −E[fh(H)]\n\f\f\f\f\f > ε\n!\n≤δ\nThis also implies that Pr\n\u0010\nmaxh∈[D]\n\f\f\fˆΓn(h) −Γ(h)\n\f\f\f > ε\n\u0011\n≤δ. We now bound the term Ndim(H) which is the\nNatarajan dimension of the class of functions H. In fact, we prove that Ndim(H) = 1.\nConsider a set {a, b} ⊆[D] shattered by H. We will assume that a < b. This implies there exist two function f0\nand f1 such that f0(x) ̸= f1(x) for x ∈{a, b}. Moreover, for every B ⊆{a, b} there exists a function h ∈H such\nthat h(x) = f0(x) for all x ∈B and h(x) = f1(x) for all x ∈B \\ {a, b}.\nWe will use the following property of the class H. Any function fv ∈H is characterized by a vector of the\nfollowing form (0, 0, . . . , 0, 1, 2, . . . , B −v) where the i-th entry is fv(i). Since all functions take value 0 at point 1,\na cannot be zero. Otherwise, either f0(a) or f1(a) must be non-zero, and there doesn’t exist a function that takes that\nnon-zero value at a. This contradicts the fact that the set {a, b} is shattered by H.\nTherefore, suppose a ≥2. First observe that f0(b)−f0(a) ≤b−a, as there does not exist any function f ∈H that\njumps by more than b −a as input changes from a to b. By the same argument we must have f1(b) −f1(a) ≤b −a.\nMoreover, as the set is shattered by H there must exist a function f ∈H such that f(a) = f0(a) and f(b) = f1(b).\nThis implies f1(b) −f0(a) ≤b −1. Similarly we have f0(b) −f1(a) ≤b −a.\nWe now consider two cases. First, f0(a) = 0. Then f1(a) ≥1, and moreover f1(b) = f1(a) + b −a ≥\nb −a + 1 as any function taking non-zero value at a must increase by 1 every step. However, this is a contradiction,\nas f1(b) −f0(a) ≥b −a + 1.\nFor the second case, suppose f0(a) ≥1. In that case, f1(a) = f0(a) + b −a. We suppose f1(a) ≥f0(a) + 1.\nThis is without loss of generality as the case f1(a) ≤f0(a) −1 can be covered by exchanging the roles of f0 and\nf1. Since f1(a) ≥1, we must have f1(b) = f1(a) + b −a. But this leads to a contradiction as f1(b) −f0(a) =\nf1(a) −f0(a) + b −a ≥b −a + 1. Therefore, the set {a, b} cannot be shattered by the function class H.\nLemma 6. Suppose |ˆγj(h) −γ(h)| ≤εj and\n\f\f\fˆΓj(h) −Γ(h)\n\f\f\f ≤εj(1 + √log T) then we have\n\f\f\f\f\f\nˆγj(h)\nˆΓj(h)\n−γ(h)\nΓ(h)\n\f\f\f\f\f ≤εj(2 + √log T)\nΓ(h)\n29\nProof.\n\f\f\f\f\f\nˆγj(h)\nˆΓj(h)\n−γ(h)\nΓ(h)\n\f\f\f\f\f =\n\f\f\fˆγj(h)Γ(h) −γ(h)ˆΓj(h)\n\f\f\f\nΓ(h)ˆΓj(h)\n≤\nˆγj(h)\n\f\f\fΓ(h) −ˆΓj(h)\n\f\f\f + ˆΓj(h) |ˆγj(h) −γ(h)|\nΓ(h)ˆΓj(h)\n≤ˆγj(h)εj(1 + √log T)\nΓ(h)ˆΓj(h)\n+\nεj\nΓ(h) ≤εj(2 + √log T)\nΓ(h)\nThe last line uses ˆγj(h) ≤ˆΓj(h).\nLemma 7. Suppose |ˆγj(h) −γ(h)| ≤εj and\n\f\f\fˆΓj(h) −Γ(h)\n\f\f\f ≤εj(1 + √log T) then we have\n\f\f\f\f\f\nˆΓj(h)\nˆγj(h) −Γ(h)\nγ(h)\n\f\f\f\f\f ≤εj(1 + √log T)\nγ(h)\nProof.\n\f\f\f\f\f\nˆΓj(h)\nˆγj(h) −Γ(h)\nγ(h)\n\f\f\f\f\f =\n\f\f\fˆΓj(h)γ(h) −Γ(h)ˆγj(h)\n\f\f\f\nγ(h)ˆγj(h)\n≤\nˆΓj(h) |γ(h) −ˆγj(h)| + ˆγj(h)\n\f\f\fˆΓj(h) −Γ(h)\n\f\f\f\nγ(h)ˆγj(h)\n≤\nˆΓj(h)\nˆγj(h)\nεj\nγ(h) + εj(1 + √log T)\nγ(h)\nThis gives us the following bound.\nˆΓj(h)\nˆγj(h) −Γ(h)\nγ(h) ≤\nˆΓj(h)\nˆγj(h)\nεj\nγ(h) + εj(1 + √log T)\nγ(h)\nRearranging we get the following inequality.\nˆΓj(h)\nˆγj(h) ≤\nΓ(h)\nγ(h) + εj\n+ εj(1 + √log T)\nγ(h) + εj\n≤Γ(h)\nγ(h) + εj(1 + √log T)\nγ(h)\n30\nE.1\nProof of Corollary 6\nProof. For the geometric discount factor we have γ(k) = γk−1 and Γ(k) = γk−1/(1 −γ). We substitute β = 3/2\nand get the following bound on g(h).\ng(h) = exp\n(\nO\n h\nX\nk=2\nT −1/4\nγ(k) + kβΓ(k)\n!)\n= exp\n(\nO\n h\nX\nk=2\n1\nγk−1\nT −1/4\n1 + k3/2/(1 −γ)\n!)\n≤exp\n(\nO\n \nT −1/4\nγh−1\nh\nX\nk=2\n1\n1 + k3/2/(1 −γ)\n!)\n≤exp\n(\nO\n \nT −1/4\nγh−1\nZ h\nk=2\n(1 −γ)dk\nk3/2\n!)\n≤exp\n\u001a\nO\n\u0012T −1/4(1 −γ)h−1/2\nγh−1\n\u0013\u001b\nIn corollary 3 we showed that t(h)\nγ(h) ≤e1−γ for any h. We also substitute H⋆=\nlog T\n2 log(1/γ). This gives us the\nfollowing bounds.\nmax\nh∈[H⋆]\nt(h)\nγ(h)g(h)Γ(h + 1)\n\u0012\n1 + O(T −1/4)\nΓ(h + 1)\n\u0013\n≤e1−γ max\nh∈[H⋆] exp\n\u001a\nO\n\u0012T −1/4(1 −γ)h−1/2\nγh−1\n\u0013\u001b \u0012 γh\n1 −γ + O(T −1/4)\n\u0013\n≤2γe1−γ\n1 −γ\nmax\nh∈[H⋆] exp\n\u001a\nO\n\u0012T −1/4(1 −γ)h−1/2\nγh−1\n\u0013\u001b\n(34)\nNow we observe that the function f(h) = h−1/2\nγh−1 is a decreasing function of h for 2h log h > log(1/γ). Otherwise\nf is increasing in h. This implies maxh∈[H⋆]\nh−1/2\nγh−1 ≤max\n\u001a\n1, T 1/4\nq\n2 log(1/γ)\nlog T\n\u001b\n≤T 1/4\nq\n2 log(1/γ)\nlog T\nas long as\nT ≥\nlog2 T\n4(1−γ)2 . Substituting this bound in eq. (34) we get\nmax\nh∈[H⋆]\nt(h)\nγ(h)g(h)Γ(h + 1)\n\u0012\n1 + O(T −1/4)\nΓ(h + 1)\n\u0013\n≤2γe1−γ\n1 −γ exp\n(\nO\n \nγ(1 −γ)\ns\nlog(1/γ)\nlog T\n!)\n≤2γe1−γ\n1 −γ exp {O (√γ(1 −γ))} = O\n\u0012γe1−γ\n1 −γ\n\u0013\n(35)\nBy a similar argument we can prove the following bounds.\nmax\nh∈[H⋆]\nt(h)\nγ(h)g(h)γ(h + 1)\n\u0012\n1 + O(T −1/4)\nγ(h + 1)\n\u0013\n≤O\n\u0012γe1−γ\n1 −γ\n\u0013\n(36)\nmax\nh∈[H⋆] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1)g(h)\n\u0012\n1 + O(T −1/4)\nΓ(h + 1)\n\u00132\n≤max\nh∈[H⋆] e1−γ\n\u0012 γh/2\n1 −γ + γh/2O(T −1/4)\n\u00132\ng(h) = O\n\u0012 γe1−γ\n(1 −γ)2\n\u0013\n(37)\n31\nFinally, we bound the remaining term in the regret expression from theorem 4: minL∈[T ] TΓ(L + 1) + 2L log(T)\n√\nT.\nIn particular we substitute L =\nlog T\n2 log(1/γ) to get the following bound on this term.\n√\nT\n1 −γ +\n√\nT log2 T\nlog(1/γ) = O\n √\nT log2 T\n1 −γ\n!\n(38)\nNote that this choice of L requires L ≤T which is satisﬁed as long as T ≥O(1/(1 −γ)2). Substituting bounds 38,\n37, 36, and 35 in the regret expression from theorem 4 we get the following bound on regret.\nReg(π; γ) ≤O\n √\nT log2 T\n1 −γ\n!\n+ O\n\u0010√\nT log T log log T\n\u0011\n+ log5/2(T)O\n \ne1−γ\n1 −γ\ns\nSAT\nlog T\nlog(1/γ)\n!\n+ log2(T) log\n\u0012 T log T\nlog(1/γ)\n\u0013\nO\n \ne1−γ\n(1 −γ)2 S2A\n\u0012\nlog T\nlog(1/γ)\n\u00133/2!\n≤O\n √\nT log2 T\n1 −γ\n!\n+ O\n √\nSAT log3 T\n(1 −γ)3/2\n!\n+ O\n \nS2A log9/2 T\n(1 −γ)7/2\n!\n≤O\n √\nSAT log3 T\n(1 −γ)3/2\n!\n[If T/ log3 T ≥S3A/(1 −γ)4]\nE.2\nProof of Corollary 5\nProof. For the polynomial discount factor we have γ(k) = k−p and Γ(k) ∈\nh\nk−p+1\np−1 , k−p \u0010\n1 +\nk\np−1\n\u0011i\n. We substitute\nβ = p −1 and get the following bound on g(h).\ng(h) = exp\n(\nO\n h\nX\nk=2\nT −1/4\nγ(k) + kβΓ(k)\n!)\n= exp\n(\nO\n h\nX\nk=2\nT −1/4\nk−p + 1/(p −1)\n!)\n≤exp\n(\nO\n \nT −1/4\nh\nX\nk=2\nkp\n1 + kp/(p −1)\n!)\n≤exp\n\u001a\nO\n\u0012\nT −1/4\nhp+1\n1 + hp/(p −1)\n\u0013\u001b\nThe last inequality follows because the function kp/(1 + kp/(p −1)) is an increasing function of k. In corollary 2 we\nshowed that t(h)\nγ(h) ≤e for any h and p > 1. We also substitute H⋆= T 1/2p. This gives us the following bounds.\nmax\nh∈[H⋆]\nt(h)\nγ(h)g(h)Γ(h + 1)\n\u0012\n1 + O(T −1/4)\nΓ(h + 1)\n\u0013\n≤e max\nh∈[H⋆] exp\n\u001a\nO\n\u0012\nT −1/4\nhp+1\n1 + hp/(p −1)\n\u0013\u001b \u0012\n(h + 1)−p\n\u0012\n1 + h + 1\np −1\n\u0013\n+ O(T −1/4)\n\u0013\n≤e\n\u0012\n1 + 2−(p−1)\np −1\n\u0013\nmax\nh∈[H⋆] exp\n\u001a\nO\n\u0012\nT −1/4\nhp+1\n1 + hp/(p −1)\n\u0013\u001b\nFor p ≥2, hp+1/(1 + hp/(p −1)) ≤2h. This gives the following bound on the term above.\nmax\nh∈[H⋆]\nt(h)\nγ(h)g(h)Γ(h + 1)\n\u0012\n1 + O(T −1/4)\nΓ(h + 1)\n\u0013\n≤2e exp\nn\nO(T −1/4 + T 1/2p)\no\n≤2e\n(39)\nBy a similar argument we can prove the following bounds.\nmax\nh∈[H⋆]\nt(h)\nγ(h)g(h)γ(h + 1)\n\u0012\n1 + O(T −1/4)\nγ(h + 1)\n\u0013\n≤2e\n(40)\n32\nmax\nh∈[H⋆] t(h) (Γ(h + 1))2\nγ(h)γ(h + 1)g(h)\n\u0012\n1 + O(T −1/4)\nΓ(h + 1)\n\u00132\n≤max\nh∈[H⋆] e(p −1)(h + 1)−p\n\u0012\n1 + h + 1\np −1\n\u00132\nmax\nh∈[H⋆] g(h)\n≤2e\n[As p ≥2]\n(41)\nFinally, we bound the remaining term in the regret expression from theorem 4: minL∈[T ] TΓ(L + 1) + 2L log(T)\n√\nT.\nWe substitute L = T 1/2p and get the following bound on the ﬁnal term.\n2T 1/2p log T + T · T −1/2\n\u0012\n1 + T 1/2p\np −1\n\u0013\n= O\n\u0010\nT\n1+p\n2p\n\u0011\n(42)\nSubstituting bounds 39, 40, 41, and 42 in the regret expression from theorem 4 we get the following bound on regret.\nReg(π; γ) ≤T\np+1\n2p + T\n\u0012\nT −1/2 + T −(1−p)/2p\np −1\n\u0013\n+ O\n\u0010√\nT log T\n\u0011\n+ log5/2(T)O\n\u0010p\nSAT (2p+1)/2p\n\u0011\n+ log3(T) · O\n\u0010\nS2AT (p−1)/2p\u0011\n+ log5/2(T) · O\n\u0010p\nT (2p+1)/2p log T\n\u0011\nIf T > (S3/2A1/2)p then the ﬁrst and the fourth term dominates and regret is at most O(\n√\nSAT (p+1)/2p). On the\nother hand, if T < (S3/2A1/2)p then the ﬁfth term dominates and regret is at most eO\n\u0000S2AT (p−1)/2p\u0001\n.\n33\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-02-07",
  "updated": "2023-02-07"
}