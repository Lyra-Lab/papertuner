{
  "id": "http://arxiv.org/abs/2302.03836v1",
  "title": "Topological Deep Learning: A Review of an Emerging Paradigm",
  "authors": [
    "Ali Zia",
    "Abdelwahed Khamis",
    "James Nichols",
    "Zeeshan Hayder",
    "Vivien Rolland",
    "Lars Petersson"
  ],
  "abstract": "Topological data analysis (TDA) provides insight into data shape. The\nsummaries obtained by these methods are principled global descriptions of\nmulti-dimensional data whilst exhibiting stable properties such as robustness\nto deformation and noise. Such properties are desirable in deep learning\npipelines but they are typically obtained using non-TDA strategies. This is\npartly caused by the difficulty of combining TDA constructs (e.g. barcode and\npersistence diagrams) with current deep learning algorithms. Fortunately, we\nare now witnessing a growth of deep learning applications embracing\ntopologically-guided components. In this survey, we review the nascent field of\ntopological deep learning by first revisiting the core concepts of TDA. We then\nexplore how the use of TDA techniques has evolved over time to support deep\nlearning frameworks, and how they can be integrated into different aspects of\ndeep learning. Furthermore, we touch on TDA usage for analyzing existing deep\nmodels; deep topological analytics. Finally, we discuss the challenges and\nfuture prospects of topological deep learning.",
  "text": "Topological Deep Learning: A Review of an Emerging Paradigm\nAli Zia1,2 , Abdelwahed Khamis2 , James Nichols1 , Zeeshan Hayder2 , Vivien Rolland2 and Lars\nPetersson2\n1College of Science, Australian National University, Australia\n2CSIRO, Australia\nAbstract\nTopological data analysis (TDA) provides insight\ninto data shape.\nThe summaries obtained by\nthese methods are principled global descriptions\nof multi-dimensional data whilst exhibiting stable\nproperties such as robustness to deformation and\nnoise. Such properties are desirable in deep learn-\ning pipelines but they are typically obtained using\nnon-TDA strategies. This is partly caused by the\ndifﬁculty of combining TDA constructs (e.g. bar-\ncode and persistence diagrams) with current deep\nlearning algorithms. Fortunately, we are now wit-\nnessing a growth of deep learning applications em-\nbracing topologically-guided components. In this\nsurvey, we review the nascent ﬁeld of topologi-\ncal deep learning by ﬁrst revisiting core concepts\nof TDA. We then explore how the use of TDA\ntechniques has evolved over time to support deep\nlearning frameworks, and how they can be inte-\ngrated into different aspects of deep learning. Fur-\nthermore, we touch on TDA usage for analyz-\ning existing deep models; deep topological analyt-\nics. Finally, we discuss the challenges and future\nprospects of topological deep learning.\n1\nIntroduction\nTopological data analysis (TDA) is a relatively recent amal-\ngam of theory and algorithms that aim to obtain a geometric\nand topological understanding of data from real world appli-\ncations. The approach to data employed in TDA fundamen-\ntally differs from that in statistical learning. Rather than ﬁnd-\ning summary statistics, estimators, ﬁtting approximate distri-\nbutions, clustering, or training neural nets, TDA instead seeks\nto understand the properties of the geometric object, often a\nmanifold, on which the data resides. This reﬂects the com-\nmon intuition that data tends to lie on, or close to, a lower\ndimensional manifold that is embedded in high dimensional\nfeature space. In this article, we sometimes refer to this as the\ndata manifold.\nThe main goal of TDA is to infer information about the\nglobal structure of the data manifold, such as its connectiv-\nity and the presence of multi-dimensional holes. An impor-\ntant property of the topological information obtained is its\ninvariance to continuous deformation and scaling. This prop-\nerty also lends itself to robustness against perturbation and\nnoise. Another beneﬁt is the versatility of the TDA methods,\nowed mostly to the abstract origins of algebraic topology. The\nmethods are applicable to a wide variety of data types and\nobjects. This includes point cloud data in Euclidean spaces,\ncategorical data, or the analysis of images and functions. Due\nto these aspects, the absence of parameters to tune, and the\nfundamental mathematical nature of the TDA approach, it is\nintriguing to include it in deep neural networks.\nThere has been much recent activity in co-opting topo-\nlogical approaches in deep learning, however, there remain\nconsiderable open questions as to what the leading approach\nshould be, due to many computational and theoretical con-\ncerns. The TDA methods discussed in this paper form but a\nsmall part of the ever-expanding interface between topolog-\nical data analysis and machine learning. We did our best to\nchoose work that has a historical and linear connection with\ndeep learning approaches, to improve understandability.\nThis survey provides the broader machine learning com-\nmunity with a convenient starting point to explore how TDA\nhas been integrated with deep learning. To the best of our\nknowledge, this is the ﬁrst work that comprehensively covers\ntopological deep learning and organizes the research works in\nthis ﬁeld in a uniﬁed taxonomy (Section 3).\nWe start in Section 2 by introducing the key theoretical\nconcepts of TDA and their representations for learning. In\nSection 3 we explain how topological approaches can ﬁt into\ndifferent deep learning constructs, such as learnable features,\nfeature transformations, and loss functions. In Section 3.4 we\nshed the light on a promising use of TDA to understand and\ndissect trained deep models, called deep topological analyt-\nics.\nWe continue in Section 4.1 with a discussion of the known\nchallenges of TDA and its adaptation to deep neural net-\nworks. We further discuss future directions and adjacent ap-\nplications of topological deep learning, and we present some\ncurrent libraries. Finally, we make some concluding remarks\nin Section 5.\nNotations: We write X ∈Rn×d to denote the data set,\nwhere n is the number of samples and d the number of fea-\ntures or dimensions.\nWe write M to denote the underly-\ning data manifold, which for the purposes of this survey is\na locally Euclidean space embedded in Rd.We write BD and\narXiv:2302.03836v1  [cs.LG]  8 Feb 2023\ntopological summaries\nDeep Topological \nAnalytics\ndeep \nfeatures\ninput\noutput\nfixed-length vector\nlearnable\nembedding\nTopological Embedding\nTopological Representation\ntopological layer \nalignment\nTopological Loss\noutput layer\ntopological\n loss\nOn-training Integration\ntopological analysis\nPost-training \ninterpretability\nresilience & stability\nhomological representation\nversatility (a frameowk adaptable to all data types)\ntopological advantages \ntopological signature \nFigure 1: Topological Deep Learning introduces TDA methods to deep models leading to topological neural architectures that can potentially\naddress deep learning limitations. This is done by plugging topological components for (a) learning features Embedding (Section 3.1), (b)\nenhancing the learned Representations (Section 3.2), and/or (c) regularizing the model using a topological Loss (Section 3.3). Beyond that,\n(d) TDA can be used post-training to reveal insights of trained models (interpretability) (Section 3.4).\nPD as abbreviations of barcode diagram and persistence dia-\ngram.\n2\nOverview of TDA\nAn object’s topology is broadly deﬁned as the characteris-\ntics that remain invariant under continuous deformation, as if\nthe object was made of soft rubber. How many connected\ncomponents the object contains, the holes or voids it con-\ntains, and how the object loops back on itself are a few ex-\namples of topological properties. In a sense, topological in-\nformation can be considered qualitative. For example, if we\ndemonstrate that data points lie on two totally disconnected\nsub-manifolds, then we know that the data comes from two\nvery distinct sources, or that the underlying system has two\ndistinct states.\nA central concept is that of homology, which is a powerful\ntool to characterize the topological features of a space. Ho-\nmology is an abstract concept. It is difﬁcult to work with,\nand its general deﬁnition is outside the scope of this paper or\neven most of the TDA literature [Carlsson, 2009]. In essence,\nthe k-th homology (where 0 ≤k ≤d) is a group that char-\nacterizes the set of k-dimensional holes (or voids) in a topo-\nlogical space. A 1-dimensional hole can be traced around\nwith a 1-dimensional loop (like a loop of string), whereas a\n2-dimensional hole is a void, for example, the void within a\nhollow sphere in 3 dimensions. These k-dimensional holes\nare counted by the Betti numbers. The k-th Betti number is\ndeﬁned as the rank of the k-th homology, which in general\ncan be quite difﬁcult to compute. Fortunately, there are some\nspaces for which the Betti numbers are relatively straightfor-\nward to compute.\n2.1\nSimplicial complexes and persistent homology\nThe k-th homology is much more convenient to work with\nwhen we restrict ourselves to simplicial complexes, which are\nstructures built upon discrete sets. This is the natural domain\nfor data-driven and machine learning applications.\nA simplex can be considered a generalization of a trian-\ngle or tetrahedron, it is the simplest polytope of any given\ndimension. A simplex in zero dimensions is a point, in one\ndimension is a line segment, in two dimensions is a triangle,\nin three is a tetrahedron, and so on. We use k-simplex to re-\nfer to a simplex of dimension k. Note that any simplex is\ncomposed of faces which are themselves simplices of lower\ndimension. A simplicial complex K is a collection of sim-\nplices with two properties: each face of a simplex in K must\nalso be in K, and the intersection of any two simplices of K\nis either empty or a face of both of them.\nConsider each point in our data set X to be a vertex (a 0-\nsimplex). We can deﬁne a set of 1-simplices as connections\nbetween pairs of vertices, 2-simplices between collections of\nthree vertices, and so on. Thus we build a simplicial com-\nplex K that gives some sense of “connectivity” between data\npoints. It can be thought of as a hyper-graph on X. Note that\nK is not necessarily unique on X.\nHomological information is much easier obtained for a\nsimplicial complex, and in particular, the k-th Betti num-\nber can be obtained through tractable linear algebra.The Betti\nnumbers in this setting are closely related to Euler charac-\nteristic, which gives the relationship between the numbers of\nvertices, edges, and faces in a polyhedron.\nThe goal now is to construct simplicial complexes on X\nthat reﬂect the underlying topology of M. This is done by\nvarying scale, typically a radius r > 0. The ˇCech complex\nand the Vietoris-Rips complex are two typical constructions\n[Chazal and Michel, 2021]. A ˇCech complex Cr(X) includes\na k-simplex on (k + 1) vertices of X if the collection of balls\nof radius r centered on each vertex has a non-empty inter-\nsection. The Vietoris-Rips (or simply Rips) complex Vr(X)\nincludes a k-simplex on any set of (k + 1) vertices that all\nhave a pairwise distance less than r of each other [Zomoro-\ndian, 2010]. These two constructions of simplicial complexes\ncan yield very different results on the same data set with the\nsame r.\nPersistent homology is obtained through a ﬁltration F,\nwhich is a growing sequence of sub-complexes: K1 ⊆K2 ⊆\n. . . ⊆Kn = K. Two commonly used examples of ﬁltration\nare the sets of simplicial complexes, Cr(X) or Vr(X), that\nare obtained with increasing radius r. As we vary r, these\nconstructs will naturally reﬂect different aspects of the topol-\nogy of M. There is monotone inclusion of these simplicial\ncomplexes with increasing r, i.e. for two radii r ≤r′ we have\nthat Cr(X) ⊆Cr′(X) and Vr(X) ⊆Vr′(X).\nThe key idea is to track changes in topological features as\nthey appear and disappear over the ﬁltration. We may see\nnew loops created, separate components connected, or holes\nﬁlled in as we increase r. We record the lifetime of these fea-\ntures with respect to r, that is the appearance (at bi for birth)\nand disappearance (at di for death) of a particular topological\nfeature.\n2.2\nRepresentations of persistent homology\nThe set of birth and death coordinates obtained from the ﬁl-\ntration forms the backbone of persistent homology. The two\nmost popular representations of this information are barcode\ndiagrams and persistence diagrams [Carlsson, 2009]. The\nmulti-set of intervals (bi, di) form the barcode diagram (BD),\nthe name coming from the visual representation of the set of\nintervals as stacked line segments. In the persistence diagram\n(PD) the lifetime of each feature is represented by a point in\nR2 with coordinates (bi, di). A ﬁltration may have several\ncopies of the same birth and death interval, which is repre-\nsented in the PD by giving the point (bi, di) an integer val-\nued multiplicity. It is important to note that the BD and PD\ncontain equivalent information and one can deﬁne a bijection\nbetween the two. From here onwards we use the term PD to\nrefer to either construct unless BD is explicitly referred to.\nA data set’s PD contains a wealth of topological informa-\ntion. Features that have a long persistence interval (di −bi)\nare considered to be likely to reﬂect the true topological fea-\ntures of the underlying manifold M. These features are rep-\nresented in the PD by points far away from the diagonal. A\nshort persistence interval describes a feature that is possibly\ngenerated from noise or is otherwise insigniﬁcant. Features\nwith short persistence will be represented by points close to\nthe diagonal line in the PD. Hence, points in the PD further\nfrom the diagonal are considered more informative.\nComparing the PDs of two objects is a way to assess their\ntopological similarity. In the next section, we discuss various\nmethods to represent them in manners suitable for machine\nlearning and computation.\n2.3\nHomological feature vectorizations\nMost machine learning methods assume that the input data re-\nsides in Rd or more generally some Hilbert space H. Hence\nthey cannot be directly applied to datasets comprised of PDs,\nand the multi-set information contained in the PD needs to\nbe represented in some vector format. This process is called\nvectorization, which requires the deﬁnition of a continuous\nmap f : PD →H. There is a plethora of different methods\nin the literature and there are some subtle consequences that\ncome with different choices of vectorization techniques [Ali\net al., 2022]. It is important to note that these vectorization\nmethods can be thought of as handcrafted feature engineer-\ning, rather than feature learning. In this section, we discuss\nvarious strategies that have evolved over time.\nA simple approach for representing PDs is using their sta-\ntistical properties such as the sum, mean, variance, maxi-\nmum, minimum, etc [Ali et al., 2022]. The total Betti number\nof a certain ﬁltration can also be used as a summary represen-\ntation [Cang et al., 2015]. These approaches yield a univari-\nate output and lose information, however can still be useful.\nAnother approach is to vectorize BDs using histogram-like\nmethods [Cang and Wei, 2017]. The basic concept is to dis-\ncretize the BD along the ﬁltration axis, creating equal sized\nbins in which we count the number of persistent intervals. Al-\nternatively, tropical coordinates deﬁned on the space of BDs\nare a useful and stable algebraic representation [Kaliˇsnik,\n2018].\nYet a different approach is to construct various forms of\npersistence functions from PDs. These functions are read-\nily vectorized themselves, however, it is also convenient to\nwork with them directly for many tasks [Bubenik, 2020;\nAdams et al., 2017]. Example of these persistent functions\nincludes persistence landscape [Bubenik, 2020] , persistence\nBetti number [Edelsbrunner et al., 2002], persistence Betti\nfunction [Xia et al., 2017], persistence surfaces and persis-\ntence images [Adams et al., 2017], etc.\nA useful feature representation technique called persis-\ntence codebooks [Zieli´nski et al., 2020] uses bag-of-words\nquantization techniques to group data points into a ﬁxed sized\nvector. Chevyrev et. al. [Chevyrev et al., 2020] proposed per-\nsistence paths, which is a feature map for barcodes.\nRepresentation can vary from simple to complex struc-\ntures. To get better structural representations there is scope\nto investigate new methods of vectorization which can ben-\neﬁt topological learning models. Note however that when a\nlarge feature vector is used to represent PDs, the curse of di-\nmensionality comes into play. In this case, variable selection,\nregularization approaches, or dropout methods should be con-\nsidered [Pun et al., 2022].\nIn addition, it is important to consider the comparison of\ndifferent PDs. To this end various metrics have been pro-\nposed, such as bottleneck distance [Mileyko et al., 2011],\nand adaptations of Gromov-Hausdorff and Wasserstein met-\nric [Bubenik et al., 2017]. Many other metrics have been\nconsidered in the literature as well. A central consideration\nis the stability of vectorizations and metrics. We discuss this\nfurther in Section 3.3.\nAs discussed, vectorization methods can be used in in-\nput space, however, kernel-based models are another impor-\ntant way to combine PD information with machine learn-\ning models [Kwitt et al., 2015].\nSince metrics can be\nmodiﬁed into kernels, various approaches have been pro-\nposed to induce kernel function from PD information [Pun\net al., 2022] and into traditional machine learning approaches\nlike PCA and SVM. Topological-based kernel methods have\nbeen used successfully in various ways [Zhu et al., 2016;\nKwitt et al., 2015], however techniques based on kernel meth-\nods suffer from scalability issues [Pun et al., 2022], as train-\ning typically scales poorly with the sample number (e.g.,\nroughly cubic in the case of kernel-SVMs). We do not dis-\ncuss topological kernel methods any further in this paper.\nMany of the aforementioned methods have advantageous\nstability properties with respect to standard metrics in TDA\nlike the Wasserstein or Bottleneck distances. However, they\nall have the same drawback: the mapping of topological rep-\nresentation that is compatible with existing learning tech-\nniques is pre-deﬁned. Therefore, it is ﬁxed and agnostic to\nany speciﬁc learning task, which makes it suboptimal. The\nphenomenal success of deep neural networks has shown that\nlearning representations (i.e. feature learning) is a preferable\napproach.\n3\nTopological Deep Learning (TDL)\nTopological representations that incorporate structural infor-\nmation hold great promise for topological deep learning mod-\nels [Hofer et al., 2017]. Combining these cues with deep\nlearning approaches has inherent beneﬁts in various applica-\ntions. On the ﬂip side, deep learning approaches can be use-\nful in overcoming some common hurdles faced by TDA ap-\nproaches in estimating robust topological features. The incor-\nporation of topological concepts into deep learning has only\nrecently been investigated and the following are general ben-\neﬁts:\n• Global features from input data can be efﬁciently and ro-\nbustly extracted that would otherwise be inaccessible via\ntraditional feature maps.\n• TDA is versatile and adaptable, meaning that we are not\nlimited to speciﬁc problems and types of data (such as im-\nages, sensor measurements, time series, graphs, etc.).\n• TDA is noise-resistant in several different problems, in-\ncluding the classiﬁcation of 3D surface meshes, the recog-\nnition of 2D object shapes, the manifold of natural image\npatches, analyzing activity patterns of the visual cortex, and\nclustering [Pun et al., 2022; Ali et al., 2022].\n• TDA can be applied to arbitrary data structures even with-\nout any prepossessing, with the right ﬁltrations.\n• A new trend is emerging that allows efﬁcient back-\npropagation through persistent homology components.\nThis is a long-standing challenge in TDA (further discussed\nin Sec. 3.3), but now topological layers are becoming com-\npatible with deep learning and end-to-end training schemes.\nWe reiterate that though the beneﬁts of using TDA (more\nspeciﬁcally persistent homology) and deep learning together\nhave demonstrated success, there are still some theoretical\nand computational challenges in the application of TDA to\ndata. We discuss these issues at length in Section 4.1.\nIn the rest of this section, we investigate TDA for deep\nlearning from lenses of different magniﬁcations and perspec-\ntives as shown in Figure 1.\nIn particular, we explore the\nuse of persistent homology in various different ways. The\ndiscussion in Section 3.1-3.3 is focused on the on-training\nintegration of TDA. That is, building topological neural ar-\nchitectures. However, a holistic view should also consider\nTDA’s contribution to post-training (deep topological analyt-\nics). This uses TDA to study the ‘shape’ of a trained model.\nThus, we review works that studied deep model complexity\nand interpretability using TDA in Section 3.4.\n3.1\nLearning Topological Features Embedding\nIn this section, we extend the discussion of ﬁxed vectorization\nmethods (Section 2.3 ) by introducing deep learnable vector-\nization (i.e. embedding). A key advantage here is the pos-\nsibility of leveraging the deep model to simultaneously learn\nthe vectorization of data and the representation of the target\ntask. For example, we may parameterize the vectorization\nof persistence diagrams PD to embedding vector V ∈Rd\nby neural layers fw where w denotes the trainable parame-\nters. Guided by the task loss, we can efﬁciently learn map-\nping fw : PDx →Vx and automatically answer the question\nof “which family of vectorizations should best work for the\ngiven task”.\nHandling PDs by neural networks is the focus of many\ndeep topological embedding works.\nGenerally, PDs deep\nvectorization layers should be continuous and permutation\ninvariant with respect to the input. The latter requirement\nis motivated by the set nature of the persistence diagram.\nHofer et al. [Hofer et al., 2017] introduced the ﬁrst learn-\nable deep vectorization of PDs. It adopts a permutation in-\nvariant transformation by evaluating the PD’s points against\nGuassian(s) whose mean and variance are learned during the\ntraining. Since permutation invariance was explored in other\ndeep learning problems (e.g. Deep Set [Zaheer et al., 2017]\nfor points cloud), some vectorization techniques for PD were\nborrowed from them.\nFor example, PersLay [Carri`ere et\nal., 2019] builds on DeepSets for embedding extended PDs\nencoding graphs and uses it for graph classiﬁcation.\nRe-\ncently, transformers were used for PDs embedding.\nPers-\nformer [Reinauer et al., 2021] architecture showed superi-\nority in synthetic and graph tasks while having some inter-\npretability features. Note that transformers without positional\nencoding can be made as expressive as Deep Sets. Thus, the\npermutation invariance requirement can be maintained.\nBeyond PDs, deep embedding was explored for other\ntopological signatures.\nFor example, PLLay [Kim et al.,\n2020] provides a layer for embedding persistence landscapes.\nPLLay claim to robustness to extreme topological distortion\nis backed by a tight stability bound that’s independent of the\ninput complexity.\nTopological embedding transforms the topological input\nwith a complex structure into a vector representation compat-\nible with deep models. As discussed in this section, the pro-\ncess uses a custom topological input layer for embedding. In\nthe next section, we explore topological components that en-\nhance deep learning representation and usually have the ﬂex-\nibility to be plugged anywhere in the network.\n3.2\nIntegration of Topological Representations\nRepresentation learning is the process of learning features\nfrom data that can be used to improve the accuracy of the\nmodel.\nDeep learning excels in this regard thanks to its\npowerful feature learning, but having a good representation\ngoes further than achieving good performance on a target task\n[Bengio et al., 2013]. For example, TDA’s stability can make\ndeep representation resilient to input perturbation [de Surrel\net al., 2022]. Below we review two categories of deep topo-\nlogical representations.\nConstrained Representations: One approach is to train\ndeep neural networks to learn representations that preserve\npersistent homology of the input data. Again, TDA’s versatil-\nity ensures the feasibility of this as the topological signature\ncan be computed for both the input and the internal represen-\ntation. For example, Topological Autoencoders [Moor et al.,\n2020] does the alignment through a loss minimizing the di-\nvergence between input and latent representation topologies\n(both captured by PDs).\nAugmented Representations: Another approach for topo-\nlogical representation is augmenting the deep features with\ntopological signatures. Persistence Enhanced Graph Network\n(PEGN) [Zhao et al., 2020] developed graph spatial convolu-\ntion that builds on persistence homology. Normally, convolu-\ntion ﬁlters are made adaptive to local graph structures by us-\ning node degree information. In contrast, PEGN weights the\nmessage passing (between nodes) by neighborhood informa-\ntion captured by persistence images. Moreover, Graph Filter-\nation Learning (GFL) [Hofer et al., 2020] adapts the readout\noperation (a graph pooling-like operation) in Graph Neural\nNetwork (GNN) to be topologically aware. BDs are com-\nputed for the graph nodes feature and vectorized. Interest-\ningly, the ﬁltration function is learned end-to-end. Topolog-\nical Graph Layer (TOGL) [Horn et al., 2022] extends GFL’s\nidea and learns multiple ﬁltrations of a graph (rather than one)\nin an end-to-end manner.\nUnlike the embedding layers (e.g. PersLay [Carri`ere et al.,\n2019]) that expect a pre-speciﬁed input type (e.g. PDs), the\ntopological representation layers discussed in this section en-\njoy more ﬂexibility regarding the input and placement in the\nnetwork. This comes with the attached cost of requiring care-\nful design choices and guarantees on the layer characteristics\n(e.g. consistency of gradients in [Hofer et al., 2020]).\n3.3\nTopological Loss\nThe most common approach for leveraging topology in deep\nlearning is incorporating a topological penalty in the loss.\nThe popularity of the approach stems from the fact that\nLoss-based integration is straightforward and doesn’t require\nchanging the architecture or adding additional layers. The\nonly caveat is that the loss should differentiable and easy to\ncompute. As iterated previously the capability of topological\nfeatures in capturing the complex structure of the data means\ndeep learning can learn robust representations guided by the\ntopological loss. Thus, the representations are likely invari-\nant w.r.t typical transformations present in real-world datasets\nsuch as noise, and outliers. An example of this is a common\npersistence loss [Hu et al., 2019], which minimizes the differ-\nence between a predicted persistence diagram PDX and the\ntrue diagram PDY :\nLtopological = d(PDX, PDY )\n(1)\nThis has been used either as a standalone loss or as a reg-\nularizer (i.e. augmenting another loss) [Hu et al., 2019] in\napplications such as semantic segmentation [Hu et al., 2019],\ngenerative modeling [Wang et al., 2020].\nAs discussed in 3.1, PDs do not lend themselves to vector\nrepresentations in Euclidean space. Moreover, the PD is not\ndifferentiable (a key requirement for using backpropagation).\nOne strategy to resolve this is leveraging a divergence or met-\nric that can handle PDs. The p-Wasserstein1 distance and the\nbottleneck distance are popular choices:\ndp,q(PDX, PDY ) =\nh\ninf\nπ∈Π(PDX,PDY )\nX\nt∈PDX\n∥t −π(t)∥p\nq\ni 1\np\n(2)\nd∞(PDX, PDY ) =\ninf\nπ∈Π(PDX,PDY ) sup\nt∈PDX\n∥t −π(t)∥∞\n(3)\nwhere t is a point corresponding to a (bi, di) ∈R2 that is in\nPDX, and where Π(PDX, PDY ) denotes a the set of bijec-\ntion between PDX and PDY , and ∥.∥q is the ℓq Euclidean\nnorm. It can be seen that bottleneck distance is the largest\ndistance between any pair of corresponding points across all\nbijections that preserve the partial ordering of the points (i.e.\nwe cannot match a point with a birth time greater than another\npoint’s death time). This ensures that the topological features\nto be matched are comparable.\nThe initial popularity of bottleneck distance is perhaps fu-\neled by a stability theorem [Cohen-Steiner et al., 2005] for\nPDs of continuous functions. According to this theorem, bot-\ntleneck distance is controlled by L∞distance, that is\nd∞(PDf1, PDf2) ≤C∥f1 −f2∥∞\n(4)\nform some constant C. In effect, this means that the diagrams\nare stable with respect to small perturbations of the underly-\ning data. A similar stability result exists for the p-Wasserstein\ndistance. These are the foundation of the stability guarantees\nby recent deep learning works such as the stability of Heat\nKernel Signature in graphs [Carri`ere et al., 2019] and sta-\nbility of mini-batch-based diagram distances in Topological\nAutoencoders [Moor et al., 2020].\n1The “Wasserstein” distance in TDA literature is slightly differ-\nent from the common Wasserstein (i.e. Kantrovich optimal mass\ntransport [Peyr´e et al., 2019]) metric. The ﬁrst seeks a determinis-\ntic bijection that best aligns the diagrams (hard assignment) and the\nmass can be freely added to or removed from the diagonal. The latter\nis based on probabilistic coupling (soft assignment). This also has\nimplications for the kind of algorithms that can be used to estimate\nthe distance.\nAmong the limitations of (2) and (3) is the high computa-\ntional budget needed by these distances when the number of\npoints is large. As the distance requires point-wise matching,\nthe computational complexity is O(n3) for n points [Anirudh\net al., 2016]. Also, in many applications [Wang et al., 2020;\nChen et al., 2019], we aim to learn a model fw that aligns a\npredicted diagram PDP with a target (i.e. ground truth) dia-\ngram PDT by gradually moving PDP points towards PDT .\nThis is typically achieved by pushing w in the negative direc-\ntion of ∇wLtopological and, obviously, assumes that the loss is\ndifferentiable w.r.t. the diagram. While the Wasserstein dis-\ntance satisﬁes this requirement in general, it can have some\ninstability issues [Solomon et al., 2021]. Below, we select a\nfew representative papers using topological losses in various\napplications and show how they handle these issues.\nIn generative modeling, TopoGAN [Wang et al., 2020]\nuses a slightly modiﬁed 1-Wassertsein distance to align the\ndiagrams of generated and real images in medical image ap-\nplications. The loss ignores the death time and focuses only\non the birth time of the diagram features. Framed in this way,\nthe loss becomes similar to the Sliced Wasserstein [Peyr´e et\nal., 2019] which can be computed efﬁciently and is still dif-\nferentiable. A similar loss was used by [Hu et al., 2019] for\nsegmentation to encourage the deep model to produce out-\nput whose topology is close to the ground truth. The cross-\nentropy loss is augmented with the 2-Wasserstein loss be-\ntween persistence diagrams. To alleviate the computational\nburden, the method performs the calculation on a single small\nimage patch (part of the image) at a time. In [Clough et al.,\n2022] the authors rely on Betti numbers for semi-supervised\nimage segmentation. A notable advantage here is the out-\nput of a network trained on a small set of labeled images can\nstill capture the actual Betti numbers correctly. This gives us\nthe opportunity to train the model initially on a small labeled\ndataset guided by the Betti numbers loss Lβ. Then, the model\nis ﬁne-tuned using large unlabeled dataset and guided by a\nloss (that incorporates Lβ). Since Betti numbers estimation\nis robust for the unlabeled data, Lβ will regularize the second\nstage of training (ﬁne-tuning). In classiﬁcation, [Chen et al.,\n2019] uses a topological regularizer. To speed up the com-\nputation it focuses on the zero homological dimension where\nthe persistence computations are particularly faster.\n3.4\nDeep Topological Analytics\nThe complementary value of TDA goes beyond on-training\nintegration and constructing topological neural architectures.\nIn fact, leveraging TDA methods post-training can be even\nmore insightful and powerful.\nCurrently, researchers use\nTDA to address deep learning transparency [Liu et al., 2020],\nstudying model complexity [Rieck et al., 2019] and even\ntracking down answers for seemingly mysterious aspects of\ndeep learning e.g. why deep networks outperform shallow\nones [Naitzat et al., 2020]. These efforts are centered around\nanalyzing deep models using TDA approaches. Hence, we\ncall it deep topological analytics. Due to space limitations,\nwe explore only two aspects of it below.\nQuantifying structural complexity:\n[Watanabe and Ya-\nmana, 2021] treats the neural networks as a weighted graph\nG(V, E) where V and E denote the network neurons and the\nrelevance scores (computed from weights); respectively. By\ncomputing persistence features (e.g. Betti numbers) across\nﬁltration, we can gain insight into the network complexity.\nFor example, the increase in the Betti number (the occurrence\nof a cycle between a set of neurons) can reﬂect the complex-\nity of knowledge in the deep neural networks. In [Rieck et\nal., 2019] the authors follow the same line and further de-\nvelop training optimization strategies (e.g. early stopping)\ninformed by homological features.\nVisual exploration of models: another use of TDA here is\nproviding a post-hoc explanation and/or visual exploration of\nthe internal functioning of deep models. For example, topo-\nlogical information provides insight into the overall structure\nof high-dimensional functions. The authors in [Liu et al.,\n2020] use this to offer a scalable visual exploration tool for\ndata-driven (and black box) models. This is an important re-\nsearch problem whose key challenge is doing it in an intu-\nitive way. They also use topological splines to visualize the\nhigh dimensional error landscape of the models. Similarly,\nTopoAct [Rathore et al., 2021] offers insightful information\non neural network learned representations and provides a vi-\nsual exploration tool to study topological summaries of acti-\nvation vectors.\n4\nDiscussion\nTDA is a steadily developing and promising area, with suc-\ncesses in a wide variety of applications. However, there are\nopen questions in applying TDA with deep neural networks.\nIn this section, we highlight several open challenges for fu-\nture research of deep TDA in both practical and theoretical\naspects and paint a speculative picture by outlining what per-\nsistent homology holds for the future. We also note some\nopen-source implementations for researchers to get started.\n4.1\nChallenges\nDespite the success of TDA and its use in deep learning we\ndescribe a few notable challenges here.\nComputational cost: Many aspects of calculating persis-\ntent homology are computational intractable. The construc-\ntion of the ˇCech complex for a given r is known to be an\nNP-hard task. Computing Betti numbers for a given simpli-\ncial complex are also infesable to compute for very large scale\ncomplexes. Costs of calculating TDA information adds to the\nalready computationally expensive deep learning routines.\nLack of universal framework for vectorization: There\nis no universally accepted framework for incorporating topo-\nlogical information into deep learning. This is a theoretical\nmatter as well as a computational one, for example there is a\nlack of strong theory encoding persistence diagrams as vec-\ntors, as discussed earlier. There have been a variety of ad-\nhoc solutions of varying merit, recently cataloged in [Ali et\nal., 2022]. Alternatively, vectorization methods have been\nchosen as part of learning strategies [Hofer et al., 2017;\nMoor et al., 2020].\nStatistical guarantees: Through this article we have not\ndiscussed the statistical aspects of persistence due to ﬁnite\nsampling. For example, there is no guarantee that the PD\nderived from X reﬂects the true homology of M. The frame-\nwork for understanding the statistical robustness of persis-\ntence information is evolving.\nSome simple strategies for\nveriﬁcation such as sub-sampling and cross-validation have\nbeen used in the literature [Chazal and Michel, 2021]. There\nis scope to further understand issues such as the minimum\nnumber of data points required to guarantee robust PDs. Fur-\nthermore, persistence is not well understood from a proba-\nbilistic point of view, e.g. the distribution of persistence from\na distribution of shapes.\nHigh-dimensional learning challenge: There is no under-\nlying theoretical framework for what topological features to\nexpect with high-dimensional data. While abstract topolog-\nical spaces can be enormously complex in high dimensions,\nwe do not know whether to expect data to behave similarly.\nMoreover high dimensional homological features are unatain-\nable due to computational cost, and in any case sensitivity of\nPDs to sampling or noise is not well understood in high di-\nmension. This makes learning the underlying topology of the\ndata for use in deep neural networks challenging.\nThe need for a good backpropagation strategy: The dif-\nferentiability of PDs or other homological quantities is not\nguaranteed or necessarily well understood. This makes back-\npropagation in deep neural networks that incorporate topolog-\nical signatures extremely challenging or only feasible under\nspecial conditions [Moor et al., 2020].\nCapturing multi-variate persistence:\nIn some cases,\nmultiple concurrent ﬁltrations are needed to fully capture the\ntopology of the data manifold, especially for data in higher\ndimensions. This leads to multi-variate persistence, where\nthe birth and death of topological features occur in multiple\ndimensions. This notion of persistence does not have a com-\nplete discrete invariant, unlike the one-dimensional BD that\nwe’ve discussed so far. For the practical use of multi-variate\npersistence in deep learning, we would need new theoretical\nframeworks and better computational methods.\n4.2\nSuccesses and Future Directions:\nDeep TDA has demonstrated potential in a variety of chal-\nlenging settings. The invariance of PH information to contin-\nuous deformation means TDA applies well to settings where\nobjects should have consistent shapes but may be transformed\nin some way. TDA also performs well at bridging the gap be-\ntween structural information and prior knowledge. If we have\nprior knowledge of the topology of a class of objects, then\nPDs are an effective tool for the classiﬁcation and compari-\nson of data against this class, even in the presence of noise\nor limited data. This robustness incorporates well into deep\nlearning.\nTDA can produce good results in small datasets, this is es-\npecially useful for medical imaging applications where ex-\npense and privacy concerns limit data acquisition [Byrne et\nal., 2021]. TDA has also been used in other settings with\nlimited or noisy data such as power forecasting [Senekane et\nal., 2021], segmenting aerial photography [Mosinska et al.,\n2018] and astronomy [Murugan and Robertson, 2019].\nIn some applications, topological information may be more\nsigniﬁcant than statistical (pixel-wise) information. For ex-\nample, in [Vukicevic et al., 2017] detecting holes between\nheart chambers is more important than inferring the thickness\nof septal walls. For these types of applications, a loss func-\ntion combining topological and statistical information can be\nadjusted in favor of topology, when training a network.\nAs PH encapsulates global structure, developing topologi-\ncal loss functions could suppress small false positives or false\nnegatives, particularly for computer vision tasks. For exam-\nple, in image segmentation, morphological operations or con-\nditional random ﬁeld-based techniques are used to remove lo-\ncal errors, but they do not possess knowledge of global topol-\nogy. The beneﬁt of PH-based loss is that the correct global\ntopology can be propagated, with local label smoothness.\nIt would be interesting to explore sophisticated deep learn-\ning architectures that learn mappings between high dimen-\nsional data and their corresponding PDs or other topological\nrepresentations, furthering [de Surrel et al., 2022]. Moreover,\ndeep learning may yet yield new kinds of topological repre-\nsentation other than PDs, with robustness to different data de-\nformations. PH could have further applications in multi-class\nopen-set problems (where data may have unknown classes).\nIf the topology among classes is relatively consistent, then the\nobject labels of unknown classes can be better predicted.\n4.3\nImplementations\nThere are a number of open-source implementations of TDA\navailable to practitioners. Here we present two libraries that\nhave interfaces with deep learning architectures.\nGUDHI is an open-source library2 that implements rel-\nevant geometric data structures and TDA algorithms, and\nit can be integrated into the TensorFlow framework.\nPer-\nsLay [Carri`ere et al., 2019] and RipsLayer are implemen-\ntations using GUDHI that learn persistence representations\nfrom complexes and PDs. They can handle automatic differ-\nentiation and are readily integrated in deep learning architec-\ntures.\nGiotto-deep3 is an open-source extension of the Giotto-\nTDA library. It aims to provide seamless integration between\nTDA and deep learning on top of PyTorch. To use topol-\nogy for both pre-processing data (using a variety of available\nmethods) and using it within neural networks, the developers\naim to provide several off-the-shelf architectures. One such\nexample is that of Persformer [Reinauer et al., 2021].\n5\nConclusion\nThe recent growth in TDA, and the established efﬁcacy of\ndeep learning, has meant that integration of these techniques\nhas been inevitable. There is no universal paradigm for com-\nbining TDA and deep learning.This article surveyed numer-\nous ways in which these frameworks have beneﬁted each\nother. We began with an overview of the key TDA concepts.\nFollowing this we reviewed TDA in deep learning from a va-\nriety of perspectives. We described numerous challenges and\nopportunities that remain in this ﬁeld, as well as some ob-\nserved success.\n2https://gudhi.inria.fr/\n3https://github.com/giotto-ai/giotto-deep\nReferences\n[Adams et al., 2017] Henry\nAdams,\nTegan\nEmerson,\nMichael Kirby, Rachel Neville, Chris Peterson, Patrick\nShipman, Sofya Chepushtanova, Eric Hanson, Francis\nMotta, and Lori Ziegelmeier. Persistence images: A stable\nvector representation of persistent homology.\nJ. Mach.\nLearn. Res., 18(1):218–252, jan 2017.\n[Ali et al., 2022] Dashti\nAli,\nAras\nAsaad,\nMaria-Jose\nJimenez, Vidit Nanda, Eduardo Paluzo-Hidalgo, and\nManuel Soriano-Trigueros.\nA survey of vectorization\nmethods in topological data analysis, December 2022.\n[Anirudh et al., 2016] Rushil Anirudh, Vinay Venkataraman,\nKarthikeyan Natesan Ramamurthy, and Pavan Turaga. A\nriemannian framework for statistical analysis of topologi-\ncal persistence diagrams. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition work-\nshops, pages 68–76, 2016.\n[Bengio et al., 2013] Yoshua Bengio, Aaron Courville, and\nPascal Vincent.\nRepresentation learning: A review and\nnew perspectives. IEEE transactions on pattern analysis\nand machine intelligence, 35(8):1798–1828, 2013.\n[Bubenik et al., 2017] Peter Bubenik, Vin de Silva, and\nJonathan A. Scott. Interleaving and gromov-hausdorff dis-\ntance. arXiv: Category Theory, 2017.\n[Bubenik, 2020] Peter Bubenik. The persistence landscape\nand some of its properties. In Topological Data Analysis,\npages 97–117. Springer International Publishing, 2020.\n[Byrne et al., 2021] Nick Byrne, James R. Clough, Giovanni\nMontana, and Andrew P. King. A persistent homology-\nbased topological loss function for multi-class CNN seg-\nmentation of cardiac MRI. In Statistical Atlases and Com-\nputational Models of the Heart., pages 3–13. Springer In-\nternational Publishing, 2021.\n[Cang and Wei, 2017] Zixuan\nCang\nand\nGuo-Wei\nWei.\nTopologyNet:\nTopology based deep convolutional and\nmulti-task neural networks for biomolecular property pre-\ndictions. PLOS Computational Biology, 13(7), jul 2017.\n[Cang et al., 2015] Zixuan Cang, Lin Mu, Kedi Wu, Kristo-\npher Opron, Kelin Xia, and Guo-Wei Wei. A topologi-\ncal approach for protein classiﬁcation. Molecular Based\nMathematical Biology, 3(1):null, 2015.\n[Carlsson, 2009] Gunnar Carlsson. Topology and data. Bul-\nletin of the American Mathematical Society, 46(2):255–\n308, January 2009.\n[Carri`ere et al., 2019] Mathieu Carri`ere, Fr´ed´eric Chazal,\nYuichi Ike, Th´eo Lacombe, Martin Royer, and Yuhei\nUmeda. Perslay: A neural network layer for persistence\ndiagrams and new graph topological signatures. In Inter-\nnational Conference on Artiﬁcial Intelligence and Statis-\ntics, 2019.\n[Chazal and Michel, 2021] Fr´ed´eric Chazal and Bertrand\nMichel. An introduction to topological data analysis: fun-\ndamental and practical aspects for data scientists. Fron-\ntiers in artiﬁcial intelligence, 4, 2021.\n[Chen et al., 2019] Chao Chen, Xiuyan Ni, Qinxun Bai, and\nYusu Wang. A topological regularizer for classiﬁers via\npersistent homology. In The 22nd International Confer-\nence on Artiﬁcial Intelligence and Statistics, pages 2573–\n2582. PMLR, 2019.\n[Chevyrev et al., 2020] Ilya Chevyrev, Vidit Nanda, and\nHarald Oberhauser. Persistence paths and signature fea-\ntures in topological data analysis.\nIEEE Transactions\non Pattern Analysis and Machine Intelligence, 42(1):192–\n202, jan 2020.\n[Clough et al., 2022] James R. Clough, Nicholas Byrne,\nIlkay Oksuz, Veronika A. Zimmer, Julia A. Schnabel, and\nAndrew P. King.\nA topological loss function for deep-\nlearning based image segmentation using persistent ho-\nmology. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 44(12):8766–8778, dec 2022.\n[Cohen-Steiner et al., 2005] David Cohen-Steiner, Herbert\nEdelsbrunner, and John Harer. Stability of persistence di-\nagrams. In Proceedings of the twenty-ﬁrst annual sympo-\nsium on Computational geometry, pages 263–271, 2005.\n[de Surrel et al., 2022] Thibault de Surrel, Felix Hensel,\nMathieu Carri`ere, Th´eo Lacombe, Yuichi Ike, Hiroaki\nKurihara, Marc Glisse, and Fr´ed´eric Chazal. Ripsnet: a\ngeneral architecture for fast and robust estimation of the\npersistent homology of point clouds. In Topological, Al-\ngebraic and Geometric Learning Workshops 2022, pages\n96–106. PMLR, 2022.\n[Edelsbrunner et al., 2002] Edelsbrunner,\nLetscher,\nand\nZomorodian. Topological persistence and simpliﬁcation.\nDiscrete & Computational Geometry, 28(4):511–533,\nNovember 2002.\n[Hofer et al., 2017] Christoph Hofer, Roland Kwitt, Marc\nNiethammer, and Andreas Uhl. Deep learning with topo-\nlogical signatures. Advances in neural information pro-\ncessing systems, 30, 2017.\n[Hofer et al., 2020] Christoph Hofer, Florian Graf, Bastian\nRieck, Marc Niethammer, and Roland Kwitt. Graph ﬁltra-\ntion learning. In Hal Daum´e III and Aarti Singh, editors,\nProceedings of the 37th International Conference on Ma-\nchine Learning, volume 119 of Proceedings of Machine\nLearning Research, pages 4314–4323. PMLR, 13–18 Jul\n2020.\n[Horn et al., 2022] Max Horn, Edward De Brouwer, Michael\nMoor, Yves Moreau, Bastian Rieck, and Karsten Borg-\nwardt.\nTopological graph neural networks.\nIn Interna-\ntional Conference on Learning Representations, 2022.\n[Hu et al., 2019] Xiaoling Hu, Fuxin Li, Dimitris Samaras,\nand Chao Chen.\nTopology-preserving deep image seg-\nmentation. Advances in neural information processing sys-\ntems, 32, 2019.\n[Kaliˇsnik, 2018] Sara Kaliˇsnik. Tropical coordinates on the\nspace of persistence barcodes. Foundations of Computa-\ntional Mathematics, 19(1):101–129, jan 2018.\n[Kim et al., 2020] K. Kim, J. Kim, M. Zaheer, J. Kim,\nF. Chazal, and L. andWasserman. Pllay: Efﬁcient topolog-\nical layer based on persistent landscapes. In In Advances in\nNeural Information Processing Systems (NeurIPS 2020),\n2020.\n[Kwitt et al., 2015] Roland Kwitt, Stefan Huber, Marc Ni-\nethammer, Weili Lin, and Ulrich Bauer. Statistical topo-\nlogical data analysis - a kernel perspective. In C. Cortes,\nN. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, edi-\ntors, Advances in Neural Information Processing Systems,\nvolume 28. Curran Associates, Inc., 2015.\n[Liu et al., 2020] Shusen Liu, Jim Gaffney, Luc Peterson,\nPeter B. Robinson, Harsh Bhatia, Valerio Pascucci,\nBrian K. Spears, Peer-Timo Bremer, Di Wang, Dan\nMaljovec, Rushil Anirudh, Jayaraman J. Thiagarajan,\nSam Ade Jacobs, Brian C. Van Essen, David Hysom, and\nJae-Seung Yeom. Scalable topological data analysis and\nvisualization for evaluating data-driven models in scien-\ntiﬁc applications. IEEE Transactions on Visualization and\nComputer Graphics, 26(1):291–300, jan 2020.\n[Mileyko et al., 2011] Yuriy Mileyko, Sayan Mukherjee, and\nJohn Harer. Probability measures on the space of persis-\ntence diagrams.\nInverse Problems, 27(12):124007, nov\n2011.\n[Moor et al., 2020] Michael Moor,\nMax Horn,\nBastian\nRieck, and Karsten Borgwardt. Topological autoencoders.\nIn International conference on machine learning, pages\n7045–7054. PMLR, 2020.\n[Mosinska et al., 2018] Agata Mosinska, Pablo Marquez-\nNeila, Mateusz Kozinski, and Pascal Fua.\nBeyond the\npixel-wise loss for topology-aware delineation. In 2018\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition. IEEE, jun 2018.\n[Murugan and Robertson, 2019] Jeff Murugan and Duncan\nRobertson. An introduction to topological data analysis\nfor physicists: From lgm to frbs, 2019.\n[Naitzat et al., 2020] Gregory Naitzat, Andrey Zhitnikov,\nand Lek-Heng Lim. Topology of deep neural networks.\nThe Journal of Machine Learning Research, 21(1):7503–\n7542, 2020.\n[Peyr´e et al., 2019] Gabriel Peyr´e, Marco Cuturi, et al. Com-\nputational optimal transport: With applications to data sci-\nence.\nFoundations and Trends® in Machine Learning,\n11(5-6):355–607, 2019.\n[Pun et al., 2022] Chi Seng Pun, Si Xian Lee, and Kelin\nXia. Persistent-homology-based machine learning: a sur-\nvey and a comparative study. Artiﬁcial Intelligence Re-\nview, 55(7):5169–5213, feb 2022.\n[Rathore et al., 2021] Archit Rathore,\nNithin Chalapathi,\nSourabh Palande, and Bei Wang. TopoAct: Visually ex-\nploring the shape of activations in deep learning. Com-\nputer Graphics Forum, 40(1):382–397, jan 2021.\n[Reinauer et al., 2021] Raphael Reinauer, Matteo Caorsi,\nand Nicolas Berkouk. Persformer: A transformer archi-\ntecture for topological machine learning, 2021.\n[Rieck et al., 2019] Bastian Rieck, Matteo Togninalli, Chris-\ntian Bock, Michael Moor, Max Horn, Thomas Gumbsch,\nand Karsten Borgwardt. Neural persistence: A complexity\nmeasure for deep neural networks using algebraic topol-\nogy. In International Conference on Learning Represen-\ntations (ICLR), 2019.\n[Senekane et al., 2021] Makhamisa Senekane, Naleli Jubert\nMatjelo, and Benedict Molibeli Taele. Improving short-\nterm output power forecasting using topological data anal-\nysis and machine learning.\nIn 2021 International Con-\nference on Electrical, Computer and Energy Technologies\n(ICECET). IEEE, dec 2021.\n[Solomon et al., 2021] Yitzchak Solomon, Alexander Wag-\nner, and Paul Bendich. A fast and robust method for global\ntopological functional optimization. In International Con-\nference on Artiﬁcial Intelligence and Statistics, pages 109–\n117. PMLR, 2021.\n[Vukicevic et al., 2017] Marija\nVukicevic,\nBobak\nMosadegh,\nJames K. Min,\nand Stephen H. Little.\nCardiac 3d printing and its future directions.\nJACC:\nCardiovascular Imaging, 10(2):171–184, feb 2017.\n[Wang et al., 2020] Fan Wang, Huidong Liu, Dimitris Sama-\nras, and Chao Chen.\nTopogan: A topology-aware gen-\nerative adversarial network. In European Conference on\nComputer Vision, pages 118–136. Springer, 2020.\n[Watanabe and Yamana, 2021] Satoru Watanabe and Hayato\nYamana.\nTopological measurement of deep neural net-\nworks using persistent homology. Annals of Mathematics\nand Artiﬁcial Intelligence, 90(1):75–92, jul 2021.\n[Xia et al., 2017] Kelin Xia, Zhiming Li, and Lin Mu. Multi-\nscale persistent functions for biomolecular structure char-\nacterization. Bulletin of Mathematical Biology, 80(1):1–\n31, nov 2017.\n[Zaheer et al., 2017] Manzil Zaheer, Satwik Kottur, Siamak\nRavanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,\nand Alexander J Smola. Deep sets. In Advances in Neu-\nral Information Processing Systems 30, pages 3391–3401,\n2017.\n[Zhao et al., 2020] Qi Zhao, Ze Ye, Chao Chen, and Yusu\nWang. Persistence enhanced graph neural network. In Sil-\nvia Chiappa and Roberto Calandra, editors, Proceedings\nof the Twenty Third International Conference on Artiﬁcial\nIntelligence and Statistics, volume 108 of Proceedings of\nMachine Learning Research, pages 2896–2906. PMLR,\n26–28 Aug 2020.\n[Zhu et al., 2016] Xiaojin Zhu, A. Vartanian, Mani Bansal,\nDuy Nguyen, and Luke Brandl. Stochastic multiresolution\npersistent homology kernel. In International Joint Confer-\nence on Artiﬁcial Intelligence, 2016.\n[Zieli´nski et al., 2020] Bartosz Zieli´nski, Michał Lipi´nski,\nMateusz Juda, Matthias Zeppelzauer, and Paweł Dłotko.\nPersistence codebooks for topological data analysis. Arti-\nﬁcial Intelligence Review, 54(3):1969–2009, sep 2020.\n[Zomorodian, 2010] Afra Zomorodian.\nFast construction\nof the vietoris-rips complex.\nComputers and Graphics,\n34(3):263–271, jun 2010.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-02-08",
  "updated": "2023-02-08"
}