{
  "id": "http://arxiv.org/abs/2302.03896v3",
  "title": "EvoText: Enhancing Natural Language Generation Models via Self-Escalation Learning for Up-to-Date Knowledge and Improved Performance",
  "authors": [
    "Zhengqing Yuan",
    "Huiwen Xue",
    "Chao Zhang",
    "Yongming Liu"
  ],
  "abstract": "In recent years, pretrained models have been widely used in various fields,\nincluding natural language understanding, computer vision, and natural language\ngeneration. However, the performance of these language generation models is\nhighly dependent on the model size and the dataset size. While larger models\nexcel in some aspects, they cannot learn up-to-date knowledge and are\nrelatively difficult to relearn. In this paper, we introduce EvoText, a novel\ntraining method that enhances the performance of any natural language\ngeneration model without requiring additional datasets during the entire\ntraining process (although a prior dataset is necessary for pretraining).\nEvoText employs two models: $G$, a text generation model, and $D$, a model that\ncan determine whether the data generated by $G$ is legitimate. Initially, the\nfine-tuned $D$ model serves as the knowledge base. The text generated by $G$ is\nthen input to $D$ to determine whether it is legitimate. Finally, $G$ is\nfine-tuned based on $D$'s output. EvoText enables the model to learn up-to-date\nknowledge through a self-escalation process that builds on a priori knowledge.\nWhen EvoText needs to learn something new, it simply fine-tunes the $D$ model.\nOur approach applies to autoregressive language modeling for all Transformer\nclasses. With EvoText, eight models achieved stable improvements in seven\nnatural language processing tasks without any changes to the model structure.",
  "text": "\u0001\u0002\u0003\u0001\u0004\u0005\u0006\u0007\b\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\nCitation: Yuan, Z.; Xue, H.; Zhang,\nC.; Liu, Y. EvoText: Enhancing\nNatural Language Generation\nModels via Self-Escalation Learning\nfor Up-to-Date Knowledge and\nImproved Performance. Appl. Sci.\n2023, 13, 4758. https://doi.org/\n10.3390/app13084758\nAcademic Editors: Ahmed Rafea and\nJulian Szymanski\nReceived: 10 March 2023\nRevised: 3 April 2023\nAccepted: 7 April 2023\nPublished: 10 April 2023\nCopyright:\n© 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed\nunder\nthe\nterms\nand\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\napplied  \nsciences\nArticle\nEvoText: Enhancing Natural Language Generation Models via\nSelf-Escalation Learning for Up-to-Date Knowledge and\nImproved Performance\nZhengqing Yuan 1,*\n, Huiwen Xue 2,†, Chao Zhang 1,† and Yongming Liu 1,*,†\n1\nSchool of Artiﬁcial Intelligence, Anhui Polytechnic University, Wuhu 241009, China;\n2\nSchool of Optoelectronic Science and Engineering, Soochow University, Suzhou 215031, China;\n*\nCorrespondence: zhengqingyuan@ieee.org (Z.Y.); liuyongming1015@163.com (Y.L.)\n†\nThese authors contributed equally to this work.\nAbstract: In recent years, pretrained models have been widely used in various ﬁelds, including natural\nlanguage understanding, computer vision, and natural language generation. However, the performance\nof these language generation models is highly dependent on the model size and the dataset size. While\nlarger models excel in some aspects, they cannot learn up-to-date knowledge and are relatively difﬁcult\nto relearn. In this paper, we introduce EvoText, a novel training method that enhances the performance of\nany natural language generation model without requiring additional datasets during the entire training\nprocess (although a prior dataset is necessary for pretraining). EvoText employs two models: G, a text\ngeneration model, and D, a model that can determine whether the data generated by G is legitimate.\nInitially, the ﬁne-tuned D model serves as the knowledge base. The text generated by G is then input to D\nto determine whether it is legitimate. Finally, G is ﬁne-tuned based on D’s output. EvoText enables the\nmodel to learn up-to-date knowledge through a self-escalation process that builds on a priori knowledge.\nWhen EvoText needs to learn something new, it simply ﬁne-tunes the D model. Our approach applies to\nautoregressive language modeling for all Transformer classes. With EvoText, eight models achieved stable\nimprovements in seven natural language processing tasks without any changes to the model structure.\nKeywords: training system; ﬁne-tuning; BERT; GPT\n1. Introduction\nPre-training models have shown great promise in natural language processing, with the\nTransformer model [1] proposing an encoder–decoder architecture based solely on the self-\nattention mechanism, enabling the construction of large-scale models that can be pretrained\non vast amounts of data. Language models [2–4] can be broadly categorized into two types:\nautoregressive language modeling and autoencoder language modeling. autoregressive lan-\nguage models, such as ELMO [5], GPT [6], and T5 [7], predict the next possible word based\non the preceding context, making them well-suited for generative tasks. On the other hand,\nautoencoder language models, such as BERT [8] and RoBERTa [9], predict intermediate words\nbased on context and are better suited for natural language understanding tasks.\nIn recent years, generative models, including VAE [10], GAN [11], and DDPM [12], have\nmade signiﬁcant progress in computer vision. However, natural language generation presents\nunique challenges due to its discrete and temporal nature. To address these challenges, a\ncommon approach is to use unsupervised training of large language models, such as GPT-\n3 [13], which has 175 billion parameters. Despite their potential, training these models can\nbe challenging due to their size, and further training once deployed can be difﬁcult. As a\nAppl. Sci. 2023, 13, 4758. https://doi.org/10.3390/app13084758\nhttps://www.mdpi.com/journal/applsci\narXiv:2302.03896v3  [cs.CL]  13 Apr 2023\nAppl. Sci. 2023, 13, 4758\n2 of 22\nresult, a zero-shot approach is often adopted, which does not require ﬁne-tuning the model\nfor speciﬁc downstream tasks. However, this approach has limitations; large language models\nmay not perform as well as smaller models with ﬁne-tuning for certain tasks that rely heavily\non supervised learning. It is also akin to using a computer that has not been upgraded for an\nextended period, and its obsolescence is only a matter of time. Therefore, there is an urgent need\nfor novel approaches that can balance the beneﬁts and limitations of large language models to\nimprove their performance and longevity. While reinforcement learning from human feedback\n(RLHF) [14] is one such approach that holds promise, it is still limited by the availability and\nquality of human feedback data, and it may not always generalize well to other tasks.\nThis paper introduces a novel training process for pretrained models, which we call\nEvoText. The proposed EvoText method can continuously learn from new data without\nsuffering from the limitations of unsupervised learning or requiring additional datasets for\nﬁne-tuning. Speciﬁcally, we merge the input text and the generated text and then use a\nnatural language understanding model to label the data for supervised ﬁne-tuning of the\ngenerative model. Simply retraining the smaller discriminator model is required to enable the\ngenerative model to acquire up-to-date knowledge. To address the issues of natural language\nunderstanding errors and overﬁtting, we adopt a small learning rate and epoch size during\nﬁne-tuning. Unlike GAN models, we do not modify the parameters of the natural language\nunderstanding model during training, and only ﬁne-tune it when necessary to incorporate new\nknowledge. The contributions of this work are as follows:\n•\nEvoText partially mitigates the problem of low-quality samples generated by the genera-\ntive model.\n•\nEvoText improves the model’s performance without additional data during the training\nprocess. (Note that while additional datasets are used in the system for warm-up and\nlearning up-to-date data, as illustrated in Figure 1, only the data generated by the generator\nare used in the crucial training process.)\n•\nEvoText enables continuous and sustainable improvement in seven natural language pro-\ncessing tasks, including natural language understanding and natural language generation,\nwithout altering the model’s structure.\n•\nThe proposed method achieves results comparable to those of large generative networks,\neven with relatively limited computational resources.\n•\nWe will make the source code for EvoText publicly available on GitHub (https://github.\ncom/DLYuanGod/Auto-learning (accessed on 9 April 2023).\nAppl. Sci. 2023, 13, 4758\n3 of 22\nFigure 1. EvoText full process.\nThe novelty of this paper lies in the proposed method for enhancing the performance of\nnatural language generation models without the need for additional datasets during training.\nWith this approach, EvoText allows the generative model to acquire up-to-date knowledge with\nminimal additional cost. This is achieved by updating the knowledge base through retraining\nthe smaller discriminator model, as opposed to retraining the entire model with additional\ndata.\n2. Background\nIn this section, we brieﬂy overview the language model, Transformer, the GPT series of\nmodels, the BERT series of models, and the GAN model.\n2.1. Language Modeling\nIn this subsection, we briefly describe the implementation of the two language modelings.\n2.1.1. Autoregressive Language Modeling\nAutoregressive language modeling [15,16] is a type of language modeling used in natural\nlanguage processing. It involves predicting the next token in a sequence based on the previous\ntokens. Given a sequence of N tokens, (t1, t2, . . . , tN), the probability of a token tk is modeled\nby calculating the conditional probability of tk given all preceding tokens, (t1, t2, . . . , tk−1),\nusing the following formula:\nP(t1, t2, . . . , tN) =\nN\n∏\nk=1\nP(tk | t1, t2, . . . , tk−1)\n(1)\nThis Formula (1) calculates the joint probability of all the tokens in the sequence. The\nproduct of all the conditional probabilities is taken from the ﬁrst token to the last token. This\nmeans that to predict the probability of a token, we need to know the probability of all preceding\ntokens. In the case of the backward model, the token after tk needs to be calculated.\n2.1.2. Autoencoder Language Modeling\nAutoencoder language modeling (ALM) [17,18] is a type of language modeling that\ninvolves predicting a target token in a sequence of tokens, while considering the probabilities of\nAppl. Sci. 2023, 13, 4758\n4 of 22\nall preceding and following tokens. This can be represented mathematically as predicting tk in\nthe sequence (t1, t2, ..., tk, ..., tN), where the probabilities of t1:k−1 and tk+1:N are also calculated.\nALM is a powerful modeling method that can be applied to natural language under-\nstanding tasks. By considering the entire sentence when predicting a target token, ALM can\ncapture the semantic and syntactic relationships between words in the sentence. This makes\nit particularly useful for tasks such as language generation, summarization, and machine\ntranslation.\nThe concept of ALM is closely related to the masked language model (MLM), which is\nused as a pretraining strategy in the popular BERT model. In MLM, a percentage of the tokens\nin a sequence are randomly masked out, and the model is trained to predict the missing tokens\nwhile considering the context of the surrounding tokens. This approach is similar to ALM, as it\nalso involves predicting a target token while considering the context of the surrounding tokens.\n2.2. Transformer\nThe Transformer model [3,19–23] is modeled and applied to natural language processing\ntasks using only self-attentive mechanisms.\n2.2.1. Transformer Encoder\nTransformer Encoder takes a sequence of tokens as input, which are ﬁrst processed through\na word embedding and positional embedding layer. The resulting vector dimension is called\ndmodel.\nNext, the Transformer Encoder uses a self-attentive mechanism to compute the output\ntokens. This mechanism involves creating three copies of the input token, which are referred to\nas Q, K, and V. Each of these copies is used in the attention calculation, which computes the\nweights between all pairs of tokens in the input sequence. The attention calculation formula is\nshown below:\nAttention(Q, K, V) = so f tmax\n\u0012 QKT\n√dk\n\u0013\nV\nHere, dk represents the vector dimension processed by each attention head. The value of\ndk is equal to dmodel divided by the number of attention heads used in the multi-head attention\nmechanism.\nFinally, the values calculated for each attention head are concatenated and passed through\na multi-layer perceptron (MLP) layer to produce the ﬁnal output tokens. Overall, the self-\nattentive mechanism used in the Transformer Encoder allows the model to capture complex\nrelationships between tokens in the input sequence and produce high-quality representations\nfor natural language processing tasks.\n2.2.2. Transformer Decoder\nThe decoder module is calculated in the same way as the encoder module, except that\nmasking is added to the multi-headed attention mechanism to mask tokens that have not yet\nbeen generated.\n2.3. GPT Series of Models\n2.3.1. GPT\nThe Generative Pre-training Transformer (GPT) [6] was introduced by Radford et al. in\n2018 as an improvement on the Transformer model, which had been mainly used for natural\nlanguage understanding tasks. GPT was the ﬁrst model to apply a pretrained Transformer\nmodel to natural language processing.\nAppl. Sci. 2023, 13, 4758\n5 of 22\nGPT uses a multi-layer Transformer Decoder for the language model, which consists of 12\nblocks of Transformer Decoders [24] with up to 117 million parameters. This approach allows\nGPT to generate high-quality natural language text by predicting the next word in a sequence\nof words.\nOverall, GPT has been shown to achieve impressive results on a range of natural language\nprocessing tasks, such as text classiﬁcation, language translation, and text generation. Its success\nhas led to the development of larger and more powerful Transformer-based models, such as\nGPT-2 and GPT-3, which continue to push the boundaries of natural language processing.\n2.3.2. GPT-2 and GPT-3\nGPT-2 [25] and GPT-3 [13] represent advanced versions of the original Generative Pre-\ntraining Transformer (GPT) model. They employ 48 and 96-layer Transformer Decoder stacks,\nrespectively, with a signiﬁcantly larger number of parameters: 1.5 billion and 175 billion.\nIn addition to their larger model sizes, both GPT-2 and GPT-3 are trained on larger and\nmore diverse datasets, enabling them to capture more complex and nuanced patterns in natural\nlanguage. As a result, these models have achieved state-of-the-art performance on a range of\nnatural language processing tasks, including language translation, question answering, and\ntext generation.\nThe success of GPT-2 and GPT-3 highlights the tremendous potential of pretrained Trans-\nformer models in natural language processing research. With continued advances in this ﬁeld,\nwe can expect even more powerful language models in the future.\n2.4. BERT Series of Models\nBERT [8,26], RoBERTa [9], ALBERT [27], XLNET [28], TinyBERT [29], and ELECTRA [30]\nare all state-of-the-art natural language understanding models that employ the Transformer\nEncoder layer. While there are slight differences in their training approaches, they all share the\nsame underlying architecture.\nThe architecture of the Transformer Encoder layer is particularly well-suited for natural\nlanguage understanding tasks, as it allows the model to capture long-range dependencies\nbetween words and phrases in the text. By leveraging self-attention mechanisms, these models\ncan dynamically weigh the importance of different words in the input sequence, enabling them\nto extract more nuanced and complex representations of language.\n2.5. GAN\nThe main idea of generative adversarial networks (GAN) [11,31,32] is to build two models,\na generator (G) model and a discriminator (D) model. During training, the G model tries to\nimprove its manufacturing process to create realistic outputs that can fool the D model, while\nthe D model tries to accurately distinguish between real and generated data like a police ofﬁcer\ninspecting forgeries. However, achieving a balance between the two models is essential, since\nan imbalance can lead to one model not converging. Speciﬁcally, the G model needs to create\nrealistic outputs that can fool the D model, while the D model needs to accurately distinguish\nbetween real and generated data. Despite the challenges, GANs have shown great potential in\ngenerating high-quality and diverse samples in a range of applications, such as image synthesis,\ntext generation, and music composition.\n2.6. RLHF\nReinforcement-learning-based training methods have shown state-of-the-art success in\nrecent years in natural language processing tasks. One of the most advanced training methods\nis the reinforcement learning with hybrid feedback (RLHF) approach. This method combines\nthe advantages of both human feedback and self-supervised learning, enabling the model\nAppl. Sci. 2023, 13, 4758\n6 of 22\nto learn from its own mistakes while also beneﬁting from human expertise. RLHF has been\nsuccessfully applied in various tasks, including machine translation, text generation, and\nsummarization.\nThis is due to the absence of human feedback in the approach proposed in this article. So\nwe used the feedback provided by ChatGPT [33].\n3. EvoText\nIn this section, the training process of EvoText is introduced, and the theoretical represen-\ntation and algorithmic implementation are given.\n3.1. Priori Learning of Discriminator\nGiven an a priori dataset Data = {(x1, y1), (x2, y2), · · · , (xN, yN)}, which could be related\nto tasks, such as grammar judgment or semantic rationalization, we aim to ﬁne-tune the\ndiscriminator model using the following objective function:\nmin\nθ EData\n\"\nN\n∑\nn=1\nL(Dθ(xn), yn)\n#\n(2)\nHere, Dθ represents the pretrained natural language understanding model, and L repre-\nsents the loss function.\n3.2. Pre-Warm-Up Training for Generator\nWe need to freeze all Transformer blocks in the pretrained natural language generative\nmodel G and add new linear and softmax layers on top. We then train the newly added layers\nusing the prior dataset through the following equation:\nmin\nϑ\nEData\n\"\nN\n∑\nn=1\nL\n\u0010\nFϑ(Gblocks\nΘ\n(xn)), yn\n\u0011#\n(3)\nwhere Gblocks\nΘ\nrefers to all the frozen Transformer blocks of the pretrained model G with\nparameters Θ, and Fϑ represents the newly added linear layer and its trainable parameters. The\nobjective is to minimize the expected value of the loss function L on the prior dataset, where yn\nis the ground truth label for input example xn.\n3.3. Training Dataset\nGive a new token Zin\nK = {z1, z2, · · · , zk} representing the input generative model G, such\nas “Yesterday”, the token is fed into the G model to obtain Zout\nK\n= {z1, z2, · · · , zk+n}, e.g.,\n“Yesterday, a man named Jack said he saw an alien, the skies near New Orleans”. It can express as\nZout\nK\n= GΘ(Zin\nK ). The generated token Zout\nK\nis fed into the discriminator model D to obtain the\nlabel YK = Dθ(Zout\nK ) for each token. Suppose we need to construct M samples, and ﬁnally, we\nobtain the posterior data as Z =\n\b\u0000Zout\n1 , Y1\n\u0001\n,\n\u0000Zout\n2 , Y2\n\u0001\n, · · · ,\n\u0000Zout\nM , YM\n\u0001\t\n.\n3.4. Supervised Fine-Tuning for Generators\nSupervised training has a greater impact on the model than unsupervised training, even\nwhen using the same dataset size. To ﬁne-tune the parameters of all Transformer blocks in\nthe generator Gblocks\nΘ\n, we enable the gradient of all generator parameters. The optimization\nobjective is deﬁned as follows:\nAppl. Sci. 2023, 13, 4758\n7 of 22\nmin\nΦ E(Zout,Y)∼Z\n\"\nM\n∑\nn=1\nL\n\u0010\nFϑ(Gblocks\nΘ\n\u0000Zout\nn )\n\u0001\n, Yn\n\u0011#\n(4)\nwhere Φ denotes the parameters of the model Gblocks\nΘ\n, and ϑ represents the parameters of the\nlinear layer.\n3.5. Semi-Supervised Fine-Tuning for Generators\nAssuming that grammatical sentences are labeled as YK = 1 and ungrammatical sen-\ntences are labeled as YK = 0, we extract all tokens with YK = 1 in the dataset Z as Z f it =\n{z1, z2, · · · , zm}. The generator model G is again unsupervised pretrained using corpus\nZ f it. The generator model G is then unsupervised pretrained using the corpus Z f it, where\nzk = (u1, u2, · · · , un) represents an unsupervised token. To accomplish this, we use autore-\ngressive language modeling to maximize the following likelihood function:\nL1\n\u0010\nZ f it\u0011\n= ∑\ni\nlogG(ui | ui−k, . . . , ui−1; Θ)\n(5)\nwhere k is the size of the context window.\n3.6. Self-Escalation\nYou can adjust the discriminator model D to make it perform better, and EvoText is even\nsuperior. You can also continue to pretrain the discriminator model D with new knowledge\nto bring it up-to-date. Certain words of text in the Z dataset are randomly masked, and the\nmasked words are predicted using the D model, which is then input to G for supervised\nﬁne-tuning. As illustrated in Figure 2, the proposed method consists of several steps. First,\nthe up-to-date data is retrained to update the discriminator. Second, the generator is given a\ncommand containing a speciﬁc year (e.g., “In 2022”) to generate the data. Third, 15% of the\nwords in the generated data is masked. Fourth, the discriminator performs word completion\non the masked words. Finally, the completed data are subjected to a supervised ﬁne-tuning of\nthe generator, which is labeled one (by default, all statements are grammatically correct after\nthe discriminator’s completion).\nFigure 2. The process of learning up-to-date knowledge. The probability of being masked in this case is\n15%, which is the same proportion as in the pretraining of BERT.\nAppl. Sci. 2023, 13, 4758\n8 of 22\n3.7. Algorithm Implementation\nAs shown in Algorithms 1 and 2, this is the full process of EvoText.\nAlgorithm 1 Priori Learning.\nRequire: Training samples Data = {(x, y)}, ﬁne-tuning step Nep, learning rate τ1 for D, and\nlearning rate τ2 for G.\n1: Initialize θ and ϑ\n2: for t = 1 · · · Nep do\n3:\nfor minibatch B ⊂Data do\n4:\ng1 ←EB\nh\n∑N\nn=1 ▽θL(Dθ(xn), yn)\ni\n5:\nθ ←θ −τ1g1\n6:\ng2 ←EB\nh\n∑N\nn=1 ▽ϑL\n\u0010\nFϑ(Gblocks\nΘ\n(xn)), yn\n\u0011i\n7:\nϑ ←ϑ −τ2g2\n8:\nend for\n9: end for\nAlgorithm 2 Training Process.\nRequire: Input token Zin\nM = {ZK} = (zk), sequence maximum length l, termination character\nα, minibatch size M, Supervised Fine-tuning learning rate τ3, Semi-supervised Fine-tuning\nlearning rate τ4.\n1: Initialize Θ and Φ.\n2: for b = 1 · · · M do\n3:\nfor t = 1 · · · l do\n4:\nzk+t ←GΘ(Zb)\n5:\nif zk+t = α then\n6:\nbreak\n7:\nend if\n8:\nK ←K + 1\n9:\nZb ←(zk−t, zk)\n10:\nend for\n11: end for\n12: Y ←Dθ(Zin\nM)\n13: Zout\nM =\nn\n(Zin\nM, Y)\no\n14: if Supervised Fine-tuning then\n15:\nThe cross-entropy loss function L.\n16:\ng ←EB\nh\n∑M\nn=1 ▽ΦL\n\u0010\nFϑ(Gblocks\nΘ\n\u0000Zout\nn )\n\u0001\n, Yn\n\u0011i\n17:\nΦ ←Φ −τ3g\n18: end if\n19: if Semi-upervised Fine-tuning then\n20:\nUnsupervised loss function L1.\n21:\nfor n = 1 . . . m do\n22:\nif Yn = 1 then\n23:\ng ←EB\n\u0002▽ΘL1\n\u0000GΘ\n\u0000Zout\nn\n\u0001\n, Zout\nn\n\u0001\u0003\n24:\nend if\n25:\nΘ ←Θ −τ4g\n26:\nend for\n27: end if\nAppl. Sci. 2023, 13, 4758\n9 of 22\n4. Experimental Setup\nIn this section, we describe our experimental setup to demonstrate the performance of our\nalgorithm.\n4.1. Experimental Environment\nWe utilize a server conﬁguration consisting of a 120-core Xeon(R) Platinum 8358P CPU @\n2.60 GHz and 8 NVIDIA A100 (80 GB) GPUs. In order to ensure optimal efﬁciency, we release\nthe GPU when the model is not being trained after deploying it in a real-world scenario.\n4.2. Experimental Model\nTo demonstrate the performance of EvoText, we adopted the GAN approach and se-\nlected 4 natural language understanding models and 8 natural language generation models.\n(These models are part of the PyTorch-Transformers library: https://github.com/huggingface/\npytorch-transformers (accessed on 17 February 2023)).\n4.2.1. BERT\nThe BERT model is widely recognized as one of the most outstanding models in recent\nyears, having topped the GLUE tasks list when it was ﬁrst released [34]. For this experiment,\nwe exclusively utilize the large-cased version of BERT as the discriminator model. We apply\nthe EvoText method to the ﬁne-tuning of this model. Notably, the BERT large−cased model boasts\n16 layers of Transformer encoders, 24 self-attentive heads, and 330 million parameters, while\nthe BERTbase−cased model has 12 layers of Transformer encoders, 12 self-attentive heads, and\n104 million parameters.\n4.2.2. RoBERTa\nThe RoBERTa model is an improved version of the BERT model that requires longer\ntraining time, a larger batch size, and more training data. Unlike BERT, RoBERTa uses dy-\nnamic masking and text encoding, moving away from BERT’s NSP task. It modiﬁes the key\nhyperparameters in BERT based on BERT’s language masking strategy, resulting in better gen-\neralization to downstream tasks. Despite these modiﬁcations, the overall number of parameters\nin RoBERTa is consistent with BERT.\n4.2.3. GPT-2\nThe primary contribution of GPT-2 is its exploration of the performance of larger-scale\nmodels in ZERO-SHOT scenarios, where no ﬁne-tuning is used. With only pretraining, hints,\nand predictions, GPT-2 achieved state-of-the-art results in 8 out of 9 tasks. Additionally, it is\nan exceptional model for natural language generation. The GPT-2small model and BERTmedium\nhave 24 and 12 layers of Transformer decoders, 24 and 12 self-attentive heads, and 335M and\n124M parameters, respectively. Moreover, GPT-2 also offers larger models, such as GPT-2large\nwith 774M parameters and GPT-2xl with 1.5B parameters.\n4.2.4. GPT-Neo\nThe GPT-Neo [35,36] 1.3B is a Transformer model trained on the Pile using cross-entropy\nloss. As an autoregressive language model, it learns to predict the next token in a given sequence\nof English text, thereby capturing internal representations of English. These representations\ncan then be used to extract features that are useful for downstream tasks. Although language\nmodels have many applications beyond this, there are still many unknowns in this area of\nresearch.\nAppl. Sci. 2023, 13, 4758\n10 of 22\n4.2.5. OPT\nThe OPT model [37] is primarily pretrained using a causal language model (CLM) target,\nwhich belongs to the family of GPT-3 models. The pretrained model can be used to evaluate\nprompts and generate text for downstream tasks. Additionally, the model can be ﬁne-tuned on\ndownstream tasks using CLM instances. The experiments in this paper use models with 125M\nand 350M parameters.\n4.2.6. Transformer-XL\nThe Transformer-XL model [38] introduces two innovations to the Vanilla Transformer, a\nrecurrence mechanism and relative positional coding. An additional advantage of Transformer-\nXL over Vanilla Transformer is that it can be used for word-level and character-level language\nmodeling. It achieves state-of-the-art language modeling results on several different datasets\nand combines a circularity mechanism with an attention mechanism that allows the model to\nlearn long-term dependencies.\n4.2.7. Language Models with Pre-trained Word Embeddings and without Pre-Trained\nWord Embeddings\nIn addition to attention-based models, pretrained word embedding models such as\nWord2Vec [39] or Glove [40] can also yield good results when incorporated into the word\nembedding layer. Similarly, scratch-trained word embedding layers can be effective for speciﬁc\ntasks, such as hate detection or text toxicity detection [41–46]. In this paper, we evaluate and\ncompare various pretrained and scratch-trained word embedding models as discriminators to\nassess their impact on the overall training system.\n4.3. Dataset\nIn the subsequent experiments, we chose an a priori dataset for discriminators.\n4.3.1. CoLA\nThe CoLA (corpus of linguistic acceptability) [47] consists of 10,657 sentences from 23\nlinguistic publications, professionally annotated for acceptability (grammaticality) by their\noriginal authors. The public version presented here contains 9594 sentences from the training\nand development sets, excluding 1063 sentences from the retention test set. The goal is to\nclassify the sentences into either acceptable or unacceptable categories based on their gram-\nmaticality. Due to its carefully curated and annotated nature, CoLA is a valuable resource for\nevaluating the performance of various NLP models and techniques in the domain of language\nunderstanding.\n4.3.2. LAMBADA\nThe source of the corpus constructed by LAMBADA [48] is unpublished anthologies. The\nrationale is to minimize the inﬂuence of generic knowledge on the answers, i.e., it is difﬁcult\nfor the model to derive answers from generic knowledge. It consists of 5325 novels and 465\nmillion words. LAMBADA has been widely used for language generation tasks and language\nunderstanding tasks, such as language modeling and text comprehension, where the goal is to\npredict the next word in a given sentence based on the preceding context.\n4.3.3. CBT\nThe children’s book test (CBT) aims to directly measure the extent to which language\nmodels exploit the wider language environment. The CBT is built from books that are freely\naccessible. The CBT has been widely used for evaluating the performance of various NLP\nmodels and techniques in the domain of language understanding and generation.\nAppl. Sci. 2023, 13, 4758\n11 of 22\n4.3.4. WikiText\nThe WikiText [49] dataset is a large-scale language modeling dataset that is widely used\nin natural language processing research. It is created by extracting articles from the English\nWikipedia and is available in three versions: WikiText-2, WikiText-103, and WikiText-500k.\nThe dataset includes articles covering a wide range of topics, providing a diverse range of\ntext for training and evaluation. The WikiText dataset has been used in various language\nmodeling tasks, including next word prediction, text generation, and text classiﬁcation. It is a\nvaluable resource for training and evaluating natural language processing models, and its use\nhas contributed signiﬁcantly to the development of language modeling research.\n4.3.5. PTB\nThe PTB(penn treebank dataset) [50] contains 42,000, 3000, and 3000 English sentences\nfor the training set, validation set, and test set. “<sos>” is the start signal of each sentence,\nand “<eos>” is the end signal of each sentence. The dataset is annotated with part-of-speech\ntags, constituency parse trees, and semantic roles, providing rich linguistic annotations for\nvarious natural language processing tasks. The PTB has been used in a wide range of natural\nlanguage processing tasks, including language modeling, part-of-speech tagging, named entity\nrecognition, parsing, and machine translation.\n4.3.6. enwiki8 and text8\nThe text8 (the dataset is available for download at https://huggingface.co/datasets/\nenwik8 (accessed on 17 February 2023)) comes from enwiki8 (the dataset is available for down-\nload at http://mattmahoney.NET/dc/text8.zip (accessed on 17 February 2023)), which was\nﬁrst used to conduct text compression. Simply put, enwiki8 is the ﬁrst 100,000,000 characters\npicked up from Wikipedia; and text8 is the result of removing all kinds of strange symbols\nand non-English characters from these characters, then converting uppercase characters into\nlowercase characters and transforming numbers into the corresponding English words. This\ndataset aims to learn distributed representations of words that capture their semantic and\nsyntactic relationships, and it has been used in various natural language processing tasks,\nincluding language modeling, text generation, and word embeddings.\n4.3.7. 1BM\nThe 1BW (one billion word) [51] dataset is a large English language corpus used for\npretraining language models. It contains one billion words and is freely available for research\npurposes. This benchmark dataset is widely used for evaluating the performance of statistical\nlanguage models and is composed of various genres and topics, including news, technology,\nand novels. It was proposed by the Google Brain team and is considered a standard for\nmeasuring progress in the ﬁeld of natural language processing. The 1BW dataset has been\nused for pretraining language models to improve their performance on downstream NLP tasks,\nsuch as text classiﬁcation, sentiment analysis, and language generation.\n4.4. Model Evaluation Indicators\nWe use PPL (perplexity), ACC (accuracy), and BPC (bits-per-character) as performance\nmetrics for our experiments. PPL measures the average number of choices available to the\nmodel when predicting the next word in a sentence and is calculated using the following\nformula:\nPPL(S) =\nm\ns\n1\np(w1, w2, . . . , wm) =\nm\ns\nm\n∏\ni=1\n1\np(wi | w1, . . . , wi−1)\n(6)\nAppl. Sci. 2023, 13, 4758\n12 of 22\nwhere S is the sentence being evaluated, m is the length of the sentence, and p(wi | w1, . . . , wi−1)\nis the probability of the i-th word given the preceding words in the sentence. A lower PPL\nvalue indicates better model performance.\nACC measures the percentage of correct judgments out of all judgment cases and is\ncalculated using the following formula:\nACC =\nTP + TN\nTP + TN + FP + FN\n(7)\nwhere TP (true positive) is the number of cases correctly judged as positive, TN (true negative)\nis the number of cases correctly judged as negative, FP (false positive) is the number of cases\nincorrectly judged as positive, and FN (false negative) is the number of cases incorrectly judged\nas negative.\nIn the this work, accuracy refers to the percentage of correctly predicted tokens in the test\ndataset. In other words, it measures how often the model predicted the correct next word given\nthe previous words in the sentence. This metric is commonly used to evaluate the performance\nof language models.\nBPC measures the number of bits required on average to encode each character in the text\nand is calculated using the following formula:\nBPC = −1\nm\nm\n∑\ni=1\nlog2 p(wi | w1, . . . , wi−1)\n(8)\nwhere m is the length of the text, and log2 p(wi | w1, . . . , wi−1) is the number of bits required\nto encode the i-th character given the preceding characters in the text. A lower BPC value\nindicates better model performance.\nSpeciﬁcally, BPC measures the number of bits needed to encode each character in the text.\nLower BPC values indicate better compression, which in turn indicates that the model has\nlearned to better capture the patterns and structure of the text.\n5. Experimental Procedure\nThis section shows the parameters that need to be tuned in the actual training and the\ncomparison with other models.\n5.1. Data Preprocessing\nIn this paper, we preprocessed the data using common techniques such as regular ex-\npression substitution and expanding English abbreviations. Table 1 shows the details of the\npreprocessing steps.\nAppl. Sci. 2023, 13, 4758\n13 of 22\nTable 1. Example of data preprocessing for the training set in downstream tasks.\nOperation\nText\nOriginal Text\nour friends won’t buy this analysis, let alone the next one we propose.\none more pseudo generalization and i ’m giving up .\none more pseudo generalization or i ’m giving up .\ni ’ll ﬁx you a drink .\nwe ’re dancing the night away .\nMy little #ObsessedWith MyDog @ Cafe Solstice Capitol Hill\nMore #tinyepic things #tinyepicwestern , this one is crazy @user I may be one of your. . .\nLast night @ Omnia Night Club At Caesars Palace\nfriendship at its ﬁnest. ....#pixar #toystory #buzz #woody #friends #friendship #bff. . .\nI L VE working for a cause! Yesterday’s balloon decor for SNN 11th Annual Back 2 School Health . . .\nAfter preprocessing\nour friends will not buy this analysis let alone the next one we propose\none more pseudo generalization and i am giving up\none more pseudo generalization or i am giving up\ni will ﬁx you a drink\nwe are dancing the night away\nMy little ObsessedWithMyDog Cafe Solstice Capitol Hill\nMore tinyepic things tinyepicwestern this one is crazy user I may be one of your\nLast night Omnia Night Club At Caesars Palace\nfriendship at its ﬁnest pixar toystory buzz woody friends friendship bff\nI L VE working for a cause Yesterdays balloon decor for SNN 11th Annual Back 2 School Health\n5.2. Fine-Tuning of Discriminators For Priori Datasets\nIn this paper, we employed BERTlarge, BERTbase, RoBERTalarge, or RoBERTabase as the\ndiscriminator model. Since these models are pretrained, they need to be ﬁne-tuned to achieve\noptimal results in downstream tasks. During the ﬁne-tuning process, it is recommended to use\na lower learning rate and a smaller number of epochs to update the model. This is because\nusing large learning rates and epochs may cause the model to fail to converge or overﬁt, which\ncan negatively impact the model’s performance in this task.\nResults As illustrated in Figure 3, we observed that the large model performed better on\nthe CoLA task, with RoBERTa exhibiting the lowest loss rate. During the pretraining process,\nwe set the maximum length of the tokenizer to 45, enabled padding, and used a minibatch size\nof 512. We ﬁne-tuned the model using several of the most commonly used parameter settings.\nAs summarized in Table 2, we achieved the best results with a learning rate of 3 × 10−5 and 10\nepochs. Following the ﬁne-tuning process, the RoBERTalarge model demonstrated the ability to\nmake judgments about grammatical plausibility.\nTable 2. Setting different parameters allows the RoBERTalarge model to ﬁne-tune the loss rate of the\nvalidation set in the CoLA task.\nLearning Rate\n2 × 10−5\n3 × 10−5\n4 × 10−5\nepoch\n5\n0.378204\n0.392302\n0.425639\n10\n0.360829\n0.359063\n0.370926\n15\n0.363742\n0.362149\n0.360642\nAppl. Sci. 2023, 13, 4758\n14 of 22\nFigure 3. BERTlarge, BERTbase, RoBERTalarge, and RoBERTabase model ﬁne-tuned loss rate in CoLA dataset.\n5.3. Prewarm-Up Training of Generator\nDuring pretraining of GPT-2medium, we followed the same data preprocessing steps as\nbefore, with one exception: GPT’s tokenizer does not auto-pad sentences to the maximum\nlength. Therefore, we used the special token “<|endoftext|>” to pad sentences that were not\nlong enough. Unlike the input to the BERTlarge model, the generator model used in GPT-\n2medium is an autoregressive language model that requires mask attention to model the data. As\nsuch, we needed to provide masks in the input to the GPT-2medium model to ensure optimal\nperformance.\nAs shown in Table 3, the best performance was achieved with a learning rate of 1 × 10−2\nand 10 epochs. During training, we only updated the parameters of the last linear layer, which\nallowed the model to be easily ﬁne-tuned for supervised tasks and prevented extensive updates\nto the linear layer parameters during ﬁne-tuning.\nTable 3. With all Transformer block parameters of GPT-2 frozen, only the linear layer is trained to validate\nthe results of the set on the CoLA task.\nModel Input\nLoss ↓\nlr =\n5 × 10−3\nlr =\n5 × 10−3\nlr =\n5 × 10−3\nlr =\n1 × 10−2\nlr =\n1 × 10−2\nlr =\n1 × 10−2\nlr =\n2 × 10−2\nlr =\n2 × 10−2\nlr =\n2 × 10−2\nEpoch = 5\nEpoch = 10\nEpoch = 15\nEpoch = 5\nEpoch = 10\nEpoch = 15\nEpoch = 5\nEpoch = 10\nEpoch = 15\nToken\n2.3275\n2.3275\n2.3275\n2.3275\n2.3275\n2.3275\n2.3275\n2.3275\n2.3275\nToken+mask attention\n0.6910\n0.6825\n0.6722\n0.6810\n0.6721\n0.6730\n0.6754\n0.6772\n0.6772\n5.4. Training Process\nAs illustrated in Figure 4, our framework involves training the discriminator model\nBERTlarge and the generator model GPT-2medium in a training loop. Firstly, we identify common\ntext word-initial words and use GPT-2medium to complete the sentences. Subsequently, the\ncompleted sentences are fed into the ﬁne-tuned BERTlarge model to evaluate their grammatical\nplausibility. This evaluation result is then utilized to conduct supervised ﬁne-tuning of the GPT-\n2medium model. To avoid the discriminator model’s errors signiﬁcantly affecting the generator\nmodel, we adopt a minimal learning rate and train the generator model only one round.\nAppl. Sci. 2023, 13, 4758\n15 of 22\nFigure 4. Training process of discriminator (BERTlarge) and generator (GPT-2medium) in EvoText. The black\nline indicates the forward propagation process, and the blue line indicates the reverse ﬁne-tuning process\nafter acquiring the label.\nTable 4 presents some examples of text generated by GPT-2medium. Subsequently, we fed\nthis data to a discriminator model to assess their syntactic plausibility. The discriminator\nmodel’s output value of 1 or 0 indicates whether the modiﬁed sentence is grammatically valid\nor invalid, respectively. Next, we used these labeled data to perform supervised ﬁne-tuning\nof the GPT-2medium model. Figure 5 shows that we used a learning rate of 1 × 10−4 and a\nminibatch size of 64 for ﬁne-tuning.\nTable 4. We did not ﬁne-tune the text generated by the previous GPT-2medium model. Then, it is input\nto the discriminator for judgment. The red marker denotes the data generated by the generator, while\na D Output of 0 indicates that the statement is not grammatically correct, and 1 indicates that it is\ngrammatically correct.\nG Input\nG Output\nD Ouput\nThat\nThat doesn’ot have any signiﬁcance, right?\n0\nIt\nIt was a beautiful night of sunshine with some gorgeous light falling.\n1\nHe\nHe said: \"\"[W]ith this being an attack of our religion I do feel the time will not have arrived.\"\"\n0\nWe\nWe have already started the implementation phase and will keep the project in mind throughout.\n1\nI\nI ’ve done all those jobs.\n1\nAppl. Sci. 2023, 13, 4758\n16 of 22\nFigure 5. Supervised ﬁne-tuning of the training loss of the GPT-2medium model.\n5.5. Semi-Supervised Fine-Tuning Generator Model\nBased on the discriminator output presented in Table 4, the sentences labeled with a value\nof 1 are fed back into the generator model for further pretraining. This process aims to enhance\nthe generator’s ability to produce grammatically correct text.\nTable 5 illustrates the evaluation of 10 tasks using 4 different natural language understand-\ning models. The results indicate that RoBERTalarge outperforms the other models. Thus, we\nselected RoBERTalarge as the discriminator for subsequent experiments. As demonstrated in\nTable 6, utilizing pretrained word embeddings as a replacement for the model’s word embed-\nding layer or reinitializing the parameters to train from scratch resulted in inferior performance\ncompared to the original RoBERTalarge model.\nTable 5. After 156 minibatch-sized EvoText sessions, we evaluated the performance of various natural\nlanguage understanding models (D) and the same language generation model (GPTmedium) on 7 natural\nlanguage processing tasks ZERO-SHOT. Each model was tested ﬁve times on each task, and the results\nwere averaged.\nModel\nLAMBADA\nCBT\nWikiText\nPTB\nenwiki8\ntext8\n1BW\nPPL ↓\nACC ↑\nACC ↑(CN)\nACC ↑(NE)\nPPL ↓(WikiText2)\nPPL ↓(WikiText103)\nPPL ↓\nBPC ↓\nBPC ↓\nPPL ↓\nBaseline\nGPT-2medium\n15.60\n55.48\n92.35\n87.10\n22.76\n26.37\n47.33\n1.01\n1.06\n55.72\nOurs\nGPT-2medium(G)+\nRoBERTalarge(D)\n14.21\n57.20\n92.85\n87.00\n21.94\n25.12\n45.93\n1.00\n1.06\n55.01\nRoBERTabase(D)\n14.91\n56.21\n92.84\n87.15\n22.40\n25.49\n45.99\n1.01\n1.07\n55.82\nBERTlarge(D)\n14.28\n57.14\n92.83\n87.32\n21.97\n25.12\n45.94\n1.00\n1.06\n55.03\nBERTbase(D)\n14.92\n56.17\n92.81\n87.02\n21.44\n25.32\n45.98\n1.01\n1.07\n55.80\nAppl. Sci. 2023, 13, 4758\n17 of 22\nTable 6. After 156 minibatch-sized EvoText sessions, we evaluated the performance of various natural\nlanguage understanding models (D) with or without pretrained word embeddings and the same language\ngeneration model (GPTmedium) on 7 natural language processing tasks using ZERO-SHOT. Each model\nwas tested ﬁve times on each task, and the results were averaged.\nModel\nLAMBADA\nCBT\nWikiText\nPTB\nenwiki8\ntext8\n1BW\nPPL ↓\nACC ↑\nACC ↑(CN)\nACC ↑(NE)\nPPL ↓(WikiText2)\nPPL ↓(WikiText103)\nPPL ↓\nBPC ↓\nBPC ↓\nPPL ↓\nBaseline\nGPT-2medium\n15.60\n55.48\n92.35\n87.10\n22.76\n26.37\n47.33\n1.01\n1.06\n55.72\nOurs\nGPT-2medium(G)+\nRoBERTalarge(D)\n14.21\n57.20\n92.85\n87.00\n21.94\n25.12\n45.93\n1.00\n1.06\n55.01\nRoBERTalarge+\nWord2Vec\n15.19\n57.13\n92.85\n87.03\n23.01\n27.72\n46.91\n1.03\n1.06\n55.61\nRoBERTalarge+\nGlove\n15.29\n57.03\n92.79\n87.13\n23.11\n27.22\n46.93\n1.04\n1.06\n55.10\nRoBERTalarge+\nScratch-trained\n15.01\n56.13\n92.79\n87.03\n21.81\n26.42\n46.01\n1.04\n1.06\n55.10\n5.6. Experimental Results\nAfter a training process consisting of 156 minibatch iterations, which represents the\naverage of the LAMBADA, CBT, WikiText, PTB, enwiki8, text8, and 1BW dataset sizes, we\nevaluated the performance of six natural language generation models on various datasets,\nincluding LAMBADA, CBT, WikiText, PTB, enwiki8, text8, and 1BW.\nTable 7 shows that EvoText can steadily improve the performance of eight natural language\ngeneration models. Notably, the training process with just 156 steps can produce better results\nwithout altering the model architecture or the initial pretraining method. It is surprising\nto see that EvoText improves the performance of the GPTsmall model to surpass that of the\nOPT125M model. These results indicate that EvoText can signiﬁcantly enhance the performance\nof the model without requiring extensive modiﬁcations. Our approach demonstrates favorable\nperformance compared to the current state-of-the-art RLHF in terms of Chatgpt feedback on\nnearly every task. Based on the results presented in Table 8, it can be observed that the EvoText\ntraining approach is highly effective in rectifying a signiﬁcant portion of the grammatical errors\nproduced by the model, which is an impressive outcome.\nAppl. Sci. 2023, 13, 4758\n18 of 22\nTable 7.\nZERO-SHOT performance of the model on 7 natural language processing tasks after 156\nminibatch-sized EvoText sessions. Each model was run ﬁve times on each task and averaged.\nModel\nLAMBADA\nCBT\nWikiText\nPTB\nenwiki8\ntext8\n1BW\nPPL ↓\nACC ↑\nACC ↑(CN)\nACC ↑(NE)\nPPL ↓(WikiText2)\nPPL ↓(WikiText103)\nPPL ↓\nBPC ↓\nBPC ↓\nPPL ↓\nBaseline\nGPT-2small\n35.13\n45.99\n87.65\n83.4\n29.41\n37.50\n65.85\n1.16\n1,17\n75.20\nGPT-2medium\n15.60\n55.48\n92.35\n87.1\n22.76\n26.37\n47.33\n1.01\n1.06\n55.72\nGPT-2large\n10.87\n60.12\n93.45\n88.0\n19.93\n22.05\n40.31\n0.97\n1.02\n44.575\nGPT-2xl\n8.63\n63.24\n93.30\n89.05\n18.34\n17.48\n35.76\n0.93\n0.98\n42.16\nGPT-Neo1.3B\n7.50\n57.2\n-\n-\n13.10\n-\n-\n-\n-\nOPT125M\n32.93\n47.2\n88.24\n86.81\n28.14\n38.23\n60.15\n1.17\n1,16\n72.89\nOPT350M\n13.29\n56.99\n92.25\n87.31\n20.99\n25.01\n46.23\n1.00\n1.06\n50.72\nTransformer-XL\n-\n-\n-\n-\n-\n-\n54.5\n0.99\n1.08\n-\nRLHF by Chatgpt\nGPT-2small\n30.43\n45.79\n88.36\n84.44\n29.20\n37.48\n64.91\n1.16\n1,16\n74.99\nGPT-2medium\n15.29\n55.01\n92.41\n87.80\n22.66\n26.28\n47.30\n1.01\n1.06\n55.69\nGPT-2large\n10.88\n60.13\n93.39\n88.09\n19.23\n22.05\n39.91\n0.97\n1.02\n44.50\nGPT-2xl\n8.57\n63.24\n93.87\n89.07\n18.21\n17.47\n35.75\n0.93\n0.98\n42.11\nGPT-Neo1.3B\n7.52\n57.18\n-\n-\n15.37\n13.00\n-\n-\n-\n-\nOPT125M\n30.71\n47.29\n88.17\n86.99\n28.12\n37.03\n60.24\n1.16\n1,16\n70.99\nOPT350M\n13.01\n57.09\n92.55\n87.43\n20.81\n25.01\n46.28\n1.00\n1.06\n50.54\nTransformer-XL\n-\n-\n-\n-\n-\n-\n54.25\n0.99\n1.08\n-\nOurs(D is\nRoBERTalarge)\nEvoText GPT-2small\n28.42\n46.29\n89.23\n84.48\n28.31\n35.72\n62.48\n1.14\n1,16\n73.82\nEvoText GPT-2medium\n14.21\n57.20\n92.85\n87.0\n21.94\n25.12\n45.93\n1.00\n1.06\n55.01\nEvoText GPT-2large\n10.57\n60.14\n93.80\n88.32\n19.93\n22.06\n38.20\n0.96\n1.02\n44.41\nEvoText GPT-2xl\n8.09\n63.23\n93.90\n89.15\n17.92\n17.50\n34.91\n0.92\n0.98\n42.09\nEvoText GPT-Neo1.3B\n7.41\n57.19\n-\n-\n15.29\n12.81\n-\n-\n-\n-\nEvoText OPT125M\n27.51\n50.09\n91.01\n85.24\n28.00\n35.89\n55.99\n1.14\n1,15\n70.21\nEvoText OPT350M\n12.10\n57.41\n92.69\n88.91\n21.99\n25.00\n46.33\n1.00\n1.06\n50.02\nEvoText\nTransformer-XL\n-\n-\n-\n-\n-\n-\n53.21\n0.99\n1.07\n-\nTable 8. We entered “Once upon a time” into Baseline’s GPT-2xl and EvoText GPT-2xl, respectively, for\ncomparison.\nGeneration by Baseline GPT-2xl\nOnce upon a time, girl name is Lisa.\nLisa is like to go on walk in park, but yesterday, she goes on walk and she lost.\nShe asks help from man which he see, but man doesn’t speak English.\nShe feels very scared and doesn’t know how to come back to her house.\nSuddenly, she saw a police car and she run to them.\nPolice helped her and she come back to her house safely.\nGeneration by EvoText GPT-2xl\nOnce upon a time, there was a girl named Lisa.\nLisa enjoyed going for walks in the park, but yesterday, she got lost while on a walk.\nShe asked for help from a man she saw, but he didn’t speak English.\nShe felt very scared and didn’t know how to get back home.\nSuddenly, she saw a police car and ran towards them.\nThe police ofﬁcers helped her and she was able to return home safely.\n5.7. Up-to-Data Knowledge Update\nWe collected abstracts of preprints published on arXiv from June to September 2022 as the\nmost up-to-date knowledge dataset. We partitioned the dataset into a training dataset, valida-\ntion dataset, and testing dataset with a split ratio of 7:1:2. In the conventional methodology, the\ngenerator model is directly subjected to ﬁne-tuning. In contrast, our methodology as shown in\nSection 3.6 entails solely ﬁne-tuning the discriminator model, followed by EvoText training. To\nensure equitable experimental outcomes, we employ the identical epoch across all trials.\nTable 9 demonstrates that the up-to-date knowledge update generator model, imple-\nmented with EvoText’s approach, outperforms retraining the generator model while maintain-\ning its performance on the ZERO-SHOT task. However, the scalability of EvoText on larger\nAppl. Sci. 2023, 13, 4758\n19 of 22\ndatasets and more complex natural language processing tasks is not discussed in this paper.\nNevertheless, based on the results presented in Table 9, it can be observed that the model not\nonly acquires new knowledge but also avoids catastrophic forgetting of the original knowledge,\nwhich is promising for future research on scalability.\nTable 9. We evaluated the performance of both models on the arXiv dataset by retraining the discriminator\nat a learning rate of 1 × 10−4 and the generator at a learning rate of 5 × 10−5.\nModel\nPPL↓(arXiv)\nPPL↓(LAMBADA) (ZERO-SHOT)\nBaseline\nGPT-2small\n37.14 (ZERO-SHOT)\n35.13\nOPT125M\n36.80 (ZERO-SHOT)\n32.93\nTraditonal\nGPT-2small\n20.42\n28.10\nOPT125M\n20.13\n28.01\nOurs\nEvoText GPT-2small\n19.81\n28.12\nEvoText OPT125M\n18.94\n28.00\n5.8. Ablation Experiments\nTo investigate the effect of each module in EvoText, we performed ablation experiments\nby removing them one by one. These modules include the ﬁne-tuning discriminator model, the\nprewarm-up generator, and the supervised and semi-supervised ﬁne-tuning generator models.\nBased on Table 10, it can be observed that each module of EvoText has an impact on the\nﬁnal results. Removing any of these modules may cause some negative impact on the overall\nperformance. It is worth noting that the supervised learning module is deemed necessary in\nour approach.\nTable 10. We evaluated the impact of removing a particular module of EvoText GPT-2media on ZERO-\nSHOT performance in seven tasks.\nModel\nLAMBADA\nCBT\nWikiText\nPTB\nenwiki8\ntext8\n1BW\nPPL ↓\nACC ↑\nACC ↑(CN)\nACC ↑(NE)\nPPL ↓(WikiText2)\nPPL ↓(WikiText103)\nPPL ↓\nBPC ↓\nBPC ↓\nPPL ↓\nInit\nFull module EvoText\n14.21\n57.20\n92.85\n87.0\n21.94\n25.12\n45.93\n1.00\n1.06\n55.01\nAblation\nExperiments\nRemove D pretraining\n14.80\n55.94\n92.95\n86.53\n22.46\n27.54\n46.99\n1.01\n1.06\n55.72\nRemove G\nPre-warm-up Training\n14.34\n57.03\n92.90\n86.93\n22.12\n25.54\n45.99\n1.00\n1.06\n55.11\nRemove supervised\nﬁne-tuning\n15.12\n55.62\n92.89\n87.23\n22.70\n27.64\n47.90\n1.01\n1.06\n56.11\nRemove\nsemi-supervised\nﬁne-tuning\n14.42\n57.04\n92.90\n86.23\n22.08\n25.69\n46.70\n1.00\n1.06\n55.88\n6. Conclusions and Future Work\nIn this article, we introduced EvoText, a training process for two pretrained models\naimed at addressing the challenges of insufﬁcient sample data and computational resources,\nallowing models to continue learning after deployment. Through ﬁne-tuning discriminators\nand prewarm-up training generators, we achieved better model performance with just 156\ntraining steps, signiﬁcantly improving performance without requiring additional training data.\nThis approach steadily improves the performance of natural language understanding and\ngeneration tasks without changing the model structure, with the potential for even greater\nperformance improvements over time. EvoText is an effective and scalable training model\nthat holds great promise for low-resource NLP tasks. Our extensive experiments demonstrate\nAppl. Sci. 2023, 13, 4758\n20 of 22\nthe potential for improving pretrained model performance and highlight the importance of\nsupervised learning.\nFuture research directions may include exploring the potential for EvoText in other NLP\ntasks and applications, investigating the impact of different discriminator and generator\narchitectures on model performance, and further exploring the potential for continued learning\nafter deployment in other settings. Additionally, our study highlights the importance of\nsupervised learning in NLP and suggests that future research should continue to focus on\ndeveloping effective training processes for pretrained models in low-resource settings.\nAuthor Contributions: Conceptualization, methodology, formal analysis, and software, Z.Y.; veriﬁcation,\nZ.Y., C.Z. and H.X.; writing—original draft preparation, Z.Y., C.Z. and H.X.; writing—review and editing,\nY.L.; visualization, C.Z. and H.X.; supervision, Y.L.; funding acquisition, Y.L. All authors have read and\nagreed to the published version of the manuscript.\nFunding: The authors gratefully acknowledge the support of the AIMTEEL 202201 Open Fund for\nIntelligent Mining Technology and Equipment Engineering Laboratory in Anhui Province and the Anhui\nProvincial Department of Education Scientiﬁc Research Key Project (Grant No. 2022AH050995). The\nﬁnancial assistance provided by these projects was instrumental in carrying out the research presented\nin this paper. We would like to thank all the members of the laboratory for their valuable support and\nassistance. Without their help, this research would not have been possible. Finally, we would like to\nexpress our gratitude to the Anhui Polytechnic University for providing the necessary facilities and\nresources for this study.\nInstitutional Review Board Statement: The study did not require ethical approval.\nInformed Consent Statement: Not applicable.\nData Availability Statement: All data generated by the generators in this article are placed in: https:\n//github.com/DLYuanGod/Auto-learning/blob/main/Gen.csv (access on 10 April 2023).\nConﬂicts of Interest: The authors declare no conﬂict of interest.\nReferences\n1.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, L.; Polosukhin, I. Attention is all you need. In\nAdvances in Neural Information Processing Systems; Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S.,\nGarnett, R., Eds.; Curran Associates, Inc.: Red Hook, NY, USA; 2017; Volume 30. Available online: https://proceedings.neurips.cc/\npaper/2017/ﬁle/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf (accessed on 17 February 2023).\n2.\nHuang, S.; Renals, S. Hierarchical bayesian language models for conversational speech recognition. IEEE Trans. Audio Speech Lang.\nProcess. 2020, 18, 1941–1954.\n3.\nWang, C.; Dai, S.; Wang, Y.; Yang, F.; Qiu, M.; Chen, K.; Zhou, W.; Huang, J. Arobert: An asr robust pre-trained language model for\nspoken language understanding. IEEE/ACM Trans. Audio Speech Lang. Process. 2022, 30, 1207–1218.\n4.\nYu, F.-H.; Chen, K.-Y.; Lu, K.-H. Non-autoregressive asr modeling using pre-trained language models for chinese speech recognition.\nIEEE/ACM Trans. Audio Speech Lang. Process. 2022, 30, 1474–1482.\n5.\nPeters, M.E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; Zettlemoyer, L. Deep contextualized word representations.\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), New Orleans, LA, USA, 1–6 June 2018; pp. 2227–2237. Available online: https:\n//aclanthology.org/N18-1202 (accessed on 17 February 2023).\n6.\nRadford, A.; Narasimhan, K. Improving Language Understanding by Generative Pre-Training. J. Mach. Learn. Res. 2018, 23, 1–18.\n7.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; Liu, P.J. Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer. J. Mach. Learn. Res. 2020, 21, 1–67. Available online: http://jmlr.org/papers/v21/20-074.html\n(accessed on 17 February 2023).\n8.\nDevlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding.\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), Minneapolis, MN, USA, 2–7 June 2019; pp. 4171–4186. Available online:\nhttps://aclanthology.org/N19-1423 (accessed on 17 February 2023).\nAppl. Sci. 2023, 13, 4758\n21 of 22\n9.\nZhuang, L.; Wayne, L.; Ya, S.; Jun, Z. A robustly optimized BERT pre-training approach with post-training. In Proceedings of the 20th\nChinese National Conference on Computational Linguistics, Huhhot, China, 13–15 August 2021; pp. 1218–1227. Available online:\nhttps://aclanthology.org/2021.ccl-1.108 (accessed on 17 February 2023).\n10.\nKipf, T.N.; Welling, M. Variational graph auto-encoders. In Proceedings of the NIPS Workshop on Bayesian Deep Learning, Barcelona,\nSpain, 9–10 December 2016.\n11.\nGoodfellow, I.J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; Bengio, Y. Generative training nets. In\nProceedings of the 27th International Conference on Neural Information Processing Systems, Montreal, QC, Canada, 8–13 December\n2014; Volume 2; pp. 2672–2680.\n12.\nHo, J.; Jain, A.; Abbeel, P. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems; Larochelle,\nH., Ranzato, M., Hadsell, R., Balcan, M., Lin, H., Eds.; Curran Associates, Inc.: Red Hook, NY, USA, 2020; Volume 33, pp. 6840–6851.\nAvailable online: https://proceedings.neurips.cc/paper/2020/ﬁle/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf (accessed on 17\nFebruary 2023).\n13.\nBrown, T.B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. Language\nModels are Few-Shot Learners. In Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS), Virtual,\n6–12 December 2020.\n14.\nChristiano, P.F.; Shah, Z.; Mordatch, I.; Schneider, J.; Blackwell, T.; Tobin, J.; Abbeel, P.; Zaremba, W. Deep reinforcement learning from\nhuman preferences. In Advances in Neural Information Processing Systems; Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus,\nR., Vishwanathan, S., and Garnett, R., Eds.; Curran Associates, Inc.: Red Hook, NY, USA, 2017; Volume 30, pp. 4302–4310.\n15.\nShannon, M.; Zen, H.; Byrne, W. Autoregressive models for statistical parametric speech synthesis. IEEE Trans. Audio Speech Lang.\nProcess 2013, 21, 587–597.\n16.\nNiedz, M.; Ciołek, M.; Cisowski, K. Elimination of impulsive disturbances from stereo audio recordings using vector autoregressive\nmodeling and variable-order kalman ﬁltering. IEEE/ACM Trans. Audio Speech Lang. Process. 2015, 23, 970–981.\n17.\nDeng, F.; Bao, C.; Kleijn, W.B. Sparse hidden markov models for speech enhancement in non-stationary noise environments. IEEE/ACM\nTrans. Audio Speech Lang. Process. 2015, 23, 1973–1987.\n18.\nÖzdemir, O.; Kerzel, M.; Weber, C.; Lee, J.H.; Wermter, S. Language model-based paired variational autoencoders for robotic language\nlearning. IEEE Trans. Cogn. Dev. Syst. 2022. https://doi.org/10.1109/TCDS.2022.3204452 (accessed on 17 February 2023).\n19.\nZhang, Z.; Wu, Y.; Zhou, J.; Duan, S.; Zhao, H.; Wang, R. Sg-net: Syntax guided transformer for language representation. IEEE Trans.\nPattern Anal. Mach. Intell. 2022, 44, 3285–3299.\n20.\nChoudhary, T.; Goyal, V.; Bansal, A. Wtasr: Wavelet transformer for automatic speech recognition of indian languages. Big Data Min.\nAnal. 2023, 6, 85–91.\n21.\nAloysius, N.; Nedungadi, G.M.P. Incorporating relative position information in transformer-based sign language recognition and\ntranslation. IEEE Access 2021, 9, 145929–145942.\n22.\nLi, Z.; Li, Z.; Zhang, J.; Feng, Y.; Zhou, J. Bridging text and video: A universal multimodal transformer for audio-visual scene-aware\ndialog. IEEE/ACM Trans. Audio, Speech, Lang. Process. 2021, 29, 2476–2483.\n23.\nQi, Q.; Lin, L.; Zhang, R.; Xue, C. Medt: Using multimodal encoding-decoding network as in transformer for multimodal sentiment\nanalysis. IEEE Access 2022, 10, 28750–28759.\n24.\nLiu, P.J.; Saleh, M.; Pot, E.; Goodrich, B.; Sepassi, R.; Kaiser, L.; Shazeer, N. Generating wikipedia by summarizing long sequences. Int.\nConf. Learn. Represent. 2018. Available online: https://openreview.net/forum?id=Hyg0vbWC- (accessed on 17 February 2023).\n25.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I. Language models are unsupervised multitask learners. OpenAI Blog\n2019, 1, 9.\n26.\nBai, Y.; Yi, J.; Tao, J.; Tian, Z.; Wen, Z.; Zhang, S. Fast end-to-end speech recognition via non-autoregressive models and cross-modal\nknowledge transferring from bert. IEEE/ACM Trans. Audio Speech Lang. Process. 2021, 29, 1897–1911.\n27.\nLan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; Soricut, R. Albert: A lite bert for self-supervised learning of language representa-\ntions. Int. Conf. Learn. Represent. 2020. Available online: https://openreview.net/forum?id=H1eA7AEtvS (accessed on 17 February 2023).\n28.\nYang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R.; Le, Q.V. XLNet: Generalized Autoregressive Pretraining for Language Under-\nstanding; Curran Associates Inc.: Red Hook, NY, USA, 2019.\n29.\nJiao, X.; Yin, Y.; Shang, L.; Jiang, X.; Chen, X.; Li, L.; Wang, F.; Liu, Q. TinyBERT: Distilling BERT for natural language understanding. In\nFindings of the Association for Computational Linguistics: EMNLP 2020; Association for Computational Linguistics: Stroudsburg, PA,\nUSA, 2020; pp. 4163–4174. Available online: https://aclanthology.org/2020.ﬁndings-emnlp.372 (accessed on 17 February 2023).\n30.\nClark, K.; Luong, M.-T.; Le, Q.V.; Manning, C.D. Electra: Pre-training text encoders as discriminators rather than generators. Int. Conf.\nLearn. Represent. 2020. Available online: https://openreview.net/forum?id=r1xMH1BtvB (accessed on 17 February 2023).\n31.\nSalimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V.; Radford, A.; Chen, X. Improved techniques for training gans. In Proceedings of\nthe 30th International Conference on Neural Information Processing Systems, Barcelona, Spain, 6–12 December 2016; pp. 2234–2242.\n32.\nIsola, P.; Zhu, J.-Y.; Zhou, T.; Efros, A.A. Image-to-image translation with conditional training networks. In Proceedings of the 2017\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21–26 July 2017; pp. 5967–5976.\nAppl. Sci. 2023, 13, 4758\n22 of 22\n33.\nSchulman, J.; Zoph, B.; Kim, C.; Hilton, J.; Menick, J.; Weng, J.; Uribe, J.F.C.; Fedus, L.; Metz, L.; Pokorny, M.; et al. ChatGPT: Optimizing\nLanguage Models for Dialogue. OpenAI, 30 November 2022. Available online: https://openai.com/blog/chatgpt (accessed on 10\nApril 2023).\n34.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; Bowman, S. GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks\nfor NLP, Brussels, Belgium, 1–2 November 2018; pp. 353–355. Available online: https://aclanthology.org/W18-5446 (accessed on 10\nApril 2023).\n35.\nBlack, S.; Leo, G.; Wang, P.; Leahy, C.; Biderman, S. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorﬂow.\n2021. Available online: https://www.semanticscholar.org/paper/GPT-Neo%3A-Large-Scale-Autoregressive-Language-with-Black-\nGao/7e5008713c404445dd8786753526f1a45b93de12 (accessed on 10 April 2023).\n36.\nGao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.; Foster, C.; Phang, J.; He, H.; Thite, A.; Nabeshima, N.; et al. The pile: An 800gb\ndataset of diverse text for language modeling. arXiv 2020, arXiv:2101.00027.\n37.\nIyer, S.; Lin, X.V.; Pasunuru, R.; Mihaylov, T.; Simig, D.; Yu, P.; Shuster, K.; Wang, T.; Liu, Q.; Koura, P.S.; et al. Opt-iml: Scaling language\nmodel instruction meta learning through the lens of generalization. arXiv 2022, arXiv:2212.12017.\n38.\nDai, Z.; Yang, Z.; Yang, Y.; Carbonell, J.; Le, Q.V.; Salakhutdinov, R. Transformer-xl: Attentive Language Models Beyond a Fixed-Length\nContext. 2019. Available online: https://arxiv.org/abs/1901.02860 (accessed on 10 April 2023).\n39.\nMikolov, T.; Chen, K.; Corrado, G.; Dean, J. Efﬁcient Estimation of Word Representations in Vector Space. In Proceedings of the Interna-\ntional Conference on Learning Representations (ICLR), Scottsdale, AZ, USA, 2–4 May 2013.\n40.\nPennington, J.; Socher, R.; Manning, C.D. GloVe: Global Vectors for Word Representation. In Proceedings of the Conference on Empiri-\ncal Methods in Natural Language Processing (EMNLP), Doha, Qatar, 25–29 October 2014.\n41.\nSubba, B.; Kumari, S. A heterogeneous stacking ensemble based sentiment analysis framework using multiple word embeddings.\nComput. Intell. 2022, 38, 530–559.\n42.\nRodriguez, P.L.; Spirling, A. Word embeddings: What works, what doesn’t, and how to tell the difference for applied research. J.\nPolitics 2022, 84, 101–115.\n43.\nSiino, M.; Di Nuovo, E.; Tinnirello, I.; La Cascia, M. Fake News Spreaders Detection: Sometimes Attention Is Not All You Need. Infor-\nmation 2022, 13, 426.\n44.\nSaleh, H.; Alhothali, A.; Moria, K. Detection of Hate Speech using BERT and Hate Speech Word Embedding with Deep Model. arXiv\n2021, arXiv:2111.01515.\n45.\nMarco, S.; Elisa, D.N.; Ilenia, T.; Marco, L.C. Detection of hate speech spreaders using convolutional neural networks. In Proceedings\nof Conference and Labs of the Evaluation Forum (CLEF), CEUR Workshop Proceedings, Bucharest, Romania, 21–24 September 2021;\npp. 2126–2136.\n46.\nIncitti, F.; Urli, F.; Snidaro, L. Beyond word embeddings: A survey. Inf. Fusion 2023, 89, 418–436.\n47.\nWarstadt, A.; Singh, A.; Bowman, S.R. Neural Network Acceptability Judgments. Trans. Assoc. Comput. Linguist. 2019, 7, 625–641.\nAvailable online: https://aclanthology.org/Q19-1040 (accessed on 10 April 2023).\n48.\nPaperno, D.; Kruszewski, G.; Lazaridou, A.; Pham, N.Q.; Bernardi, R.; Pezzelle, S.; Baroni, M.; Boleda, G.; Fernandez, R. The LAM-\nBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers), Berlin, Germany, 7–12 August 2016; pp. 1525–1534. Available online:\nhttp://www.aclweb.org/anthology/P16-1144 (accessed on 10 April 2023).\n49.\nMerity, S.; Xiong, C.; Bradbury, J.; Socher, R. Pointer sentinel mixture models. In Proceedings of the International Conference on\nLearning Representations (ICLR), Toulon, France, 24–26 April 2017. Available online: https://openreview.net/forum?id=Byj72udxe\n(accessed on 10 April 2023).\n50.\nMarcus, M.P.; Santorini, B.; Marcinkiewicz, M.A. Building a large annotated corpus of English: The Penn Treebank. Comput. Linguist.\n1993, 19, 313–330. Available online: https://www.aclweb.org/anthology/J93-2004 (accessed on 10 April 2023).\n51.\nChelba, C.; Mikolov, T.; Schuster, M.; Ge, Q.; Brants, T.; Koehn, P.; Robinson, T. One billion word benchmark for measuring progress in\nstatistical language modeling. In Proceedings of the 2013 International Conference on Computational Linguistics, Potsdam, Germany,\n19–22 March 2013; pp. 2634–2643.\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s)\nand contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or\nproperty resulting from any ideas, methods, instructions or products referred to in the content.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-02-08",
  "updated": "2023-04-13"
}