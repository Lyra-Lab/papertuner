{
  "id": "http://arxiv.org/abs/2302.03927v1",
  "title": "On the Applicability of Language Models to Block-Based Programs",
  "authors": [
    "Elisabeth Griebl",
    "Benedikt Fein",
    "Florian Obermüller",
    "Gordon Fraser",
    "René Just"
  ],
  "abstract": "Block-based programming languages like Scratch are increasingly popular for\nprogramming education and end-user programming. Recent program analyses build\non the insight that source code can be modelled using techniques from natural\nlanguage processing. Many of the regularities of source code that support this\napproach are due to the syntactic overhead imposed by textual programming\nlanguages. This syntactic overhead, however, is precisely what block-based\nlanguages remove in order to simplify programming. Consequently, it is unclear\nhow well this modelling approach performs on block-based programming languages.\nIn this paper, we investigate the applicability of language models for the\npopular block-based programming language Scratch. We model Scratch programs\nusing n-gram models, the most essential type of language model, and\ntransformers, a popular deep learning model. Evaluation on the example tasks of\ncode completion and bug finding confirm that blocks inhibit predictability, but\nthe use of language models is nevertheless feasible. Our findings serve as\nfoundation for improving tooling and analyses for block-based languages.",
  "text": "On the Applicability of Language Models\nto Block-Based Programs\nElisabeth Griebl∗, Benedikt Fein∗, Florian Oberm¨uller∗, Gordon Fraser∗, Ren´e Just†\n∗University of Passau, Passau, Germany\n†University of Washington, Seattle, USA\n{elisabeth.griebl, benedikt.fein, ﬂorian.obermueller, gordon.fraser}@uni-passau.de, rjust@cs.washington.edu\nAbstract—Block-based programming languages like SCRATCH\nare increasingly popular for programming education and end-\nuser programming. Recent program analyses build on the insight\nthat source code can be modelled using techniques from natural\nlanguage processing. Many of the regularities of source code that\nsupport this approach are due to the syntactic overhead imposed\nby textual programming languages. This syntactic overhead,\nhowever, is precisely what block-based languages remove in order\nto simplify programming. Consequently, it is unclear how well\nthis modelling approach performs on block-based programming\nlanguages. In this paper, we investigate the applicability of lan-\nguage models for the popular block-based programming language\nSCRATCH. We model SCRATCH programs using n-gram models,\nthe most essential type of language model, and transformers, a\npopular deep learning model. Evaluation on the example tasks\nof code completion and bug ﬁnding conﬁrm that blocks inhibit\npredictability, but the use of language models is nevertheless\nfeasible. Our ﬁndings serve as foundation for improving tooling\nand analyses for block-based languages.\nIndex Terms—Block-Based Programs, Scratch, Natural Lan-\nguage Model, Code Completion, Bugram\nI. INTRODUCTION\nBlock-based programming languages are becoming increas-\ningly popular for education [1] as well industrial applications\nrequiring end-user programming [2]–[4]. The distinguishing\nfeature of these programming languages is that they reduce the\nsyntactic overhead that is common for text-based languages,\nand instead represent programming constructs using graphical\nblocks which can only be combined in syntactically valid ways.\nFigure 1a shows a JAVA function that prints “Hello world!”\n10 times; the same functionality can be implemented in the\npopular block-based language SCRATCH [5] with only three\nblocks (Fig. 1b). Programming is usually further simpliﬁed\nby explicitly listing all available blocks in the user interface,\nsuch that programmers neither need to memorize syntax nor\navailable commands and APIs (recognition over recall).\nJust like for programs written in text-based programming\nlanguages, there is a need to apply program analysis also to\nblock-based programs: Learners may beneﬁt from automatically\ngenerated hints and feedback, and programmers may beneﬁt\nfrom code completion or bug detection. A popular approach\nto implement such analyses is to treat source code like natural\nlanguage, and thus beneﬁt from the recent proliferation of\nresearch on natural language processing (NLP) methods. At\nthe core of these methods lies the concept of language models,\nwhich capture the probability distributions over sequences of\npublic static void main(\nString[] args) {\nfor (int i = 0; i < 10; i++) {\nSystem.out.println(\n\"Hello World!\");\n}\n}\n(a) Java function.\n(b) SCRATCH code.\npublic →static →void →main →( →String →[ →] →\nargs →) →{ →for →( →int →i →= →0 →; →i →<\n→10 →; →i →++ →) →{ →System →. →out →. →\nprintln →( →¨ →Hello World! →¨ →) →; →} →}\n(c) Java token sequence.\nWhen\nclicked →\nrepeat\ntimes\n→\n10 →\nsay\n→\nHello World!\n(d) SCRATCH token sequence.\nFigure 1: Example code in text- and block-based format.\nwords. As source code has been observed to exhibit regularities\nthat make it amenable to natural language processing [6],\nthe same models can also be used to capture probability\ndistributions for source code. These models can, for example,\npredict common code sequences for code completion [7], or\nidentify unusual code sequences for bug detection [8], [9].\nLanguage models are constructed by interpreting the source\ncode as sequences of tokens, i.e., individual words or symbols\nseparated by whitespace or deliminator characters. The JAVA\nprogram in Fig. 1a would thus be interpreted as the sequence\nof 39 tokens shown in Fig. 1c. The SCRATCH version of the\nsame program (Fig. 1b) results in a simple stream of only\nﬁve tokens (Fig. 1d). On the one hand, this difference can be\ninterpreted as strong reafﬁrmation of just how much block-\nbased programming reduces the cognitive overhead compared to\ntext-based programming languages [10]. On the other hand, it is\nunclear how this simpliﬁcation changes the resulting language\nmodels. Even when removing “syntactic” tokens [11], the\nremaining tokens in the JAVA example are intuitively at a lower\nlevel of abstraction than the tokens of the SCRATCH program,\nwhich contains less technical details such as modiﬁers or types.\nConsequently, it is unclear how suitable language models are\nfor program analysis tasks on block-based programs.\nIn order to investigate whether block-based programs can be\nmodelled and analyzed using language models, we empirically\narXiv:2302.03927v1  [cs.PL]  8 Feb 2023\ninvestigate models based on programs written in the SCRATCH\nprogramming language [5], which is the most popular block-\nbased programming language and aims at young learners. There\nis a thriving community of millions of users who share their\nprograms, thus providing large amounts of code, allowing us\nto perform an extrinsic evaluation of the language models. In\ndetail, the contributions of this paper are as follows:\n• We describe and implement the process of creating n-gram\nmodels from SCRATCH programs. While there are various\nalternative neural models, n-gram models have been shown\nto perform well for many tasks, and a sound understanding\nrequires interpretable models.\n• We evaluate the suitability of n-gram models for the\ncommon tasks of code completion, i.e., the prediction\nof which block will be used next in a token stream, using\na dataset of publicly shared SCRATCH projects.\n• We evaluate the ability of n-gram models to identify erro-\nneous solutions for SCRATCH programming assignments.\n• We investigate whether transformers, a popular deep\nlearning model, can improve the performance of the\ncompletion task compared to n-gram models.\nOur experiments conﬁrm that block-based programs differ\nfundamentally from text-based programs in a way that nega-\ntively affects their predictability. However, there nevertheless\nare elements of syntax and repetitiveness that make blocks\nsufﬁciently predictable to enable the use of natural language\nmodels for block-based programming languages.\nII. BACKGROUND\nBlock-based programming languages have recently received\nincreased attention for teaching programming concepts to\nnovices [1] as well as for industrial applications requiring\nend-user programming [2]–[4]. In this paper, we focus on the\npopular educational programming language SCRATCH [5].\nA. The Scratch Programming Language\nSCRATCH [5] is a block based programming language for\nyoung learners. SCRATCH programs control the behavior of\nsprites in an environment (stage); each sprite can implement\nmultiple scripts. Figure 1b exempliﬁes such a script: Scripts\nstart with event handlers (e.g.,\nWhen\nclicked ) followed by blocks that\nare executed after the event occurred. To support recognition\nover recall blocks are color coded based on categories: control\nstructures are orange like the\nblock in Fig. 1b, blocks\naffecting the visual appearance of sprites are purple (e.g.,\nsay Hello World! ), etc. Blocks are further divided into different shapes,\nsuch as stackable blocks (statements) and round or diamond-\nshaped reporter blocks that ﬁt into holes in other blocks\n(expressions). Blocks may have free text spaces for numbers\nand strings like in\nsay Hello World! , and drop-down menus to select\npre-set options. SCRATCH enables a remix culture [10] where\nusers share their programs, and others clone and enhance them.\nEven though the block shapes prevent syntactical errors,\nbuilding programs can nevertheless be challenging: learn-\ners may struggle to implement functionality or may have\nmisconceptions [12]–[15], and programmers may miss the\nconvenience and support of modern programming environments.\nAs a consequence, various analysis tools have been proposed,\nmainly implementing traditional program analyses such as\nlinting (e.g., [16], [17]) or automated testing (e.g., [18], [19]).\nHowever, analysis tools using NLP methods are, to the best of\nour knowledge, not available yet.\nB. N-gram Models\nProbabilistic language models are used to assign probabilities\nto sequences of tokens in a given language. N-gram language\nmodels are based on the Markov assumption, which states\nthat the probability of a sentence s can be estimated based on\na chain of probabilities for all its tokens w1 . . . wn to occur.\nN-gram models further simplify this idea and assume that each\nword actually depends only on its n −1 preceding words, that\nis, on its local context. Given n = 3 and s = ⟨w1w2w3w4⟩, an\nn-gram model thus estimates the probability of s as follows:\nP(s) ≈P(w1) × P(w2|w1) × P(w3|w1w2) × P(w4|w2w3)\nIn contrast to regular Markov chains, the probability of w4 is\nestimated considering only the context w2w3, not w1w2w3. The\nfactors of the product are conditional probabilities estimated\nusing the count of occurrences of tokens in the training data.\nFor example, the probability P(c|ab) with local context ab has\na probability of 1, if only c follows ab in the training data.\nA probability of 1 is rather unrealistic in practice: It is\nmore likely that the training data did not contain all possible\ntokens that may follow the local context. Therefore, smoothing\nalgorithms shift the raw probabilities based only on the counts\nof the n-grams, so that other n-grams that are not part of the\ntraining data are also assigned probabilities > 0. This way,\nthe model is not “inﬁnitely surprised” by n-grams other than\nthose present in the training data set. A popular smoothing\nmechanism is modiﬁed Kneser-Ney smoothing [20], which has\nbeen reported to perform best in natural language contexts [21]\nand was also used in prior work on programming languages [6].\nC. Deep Learning Models: Transformer\nWhile n-gram models are valued for their simplicity and\nease of interpretation, research has recently shifted towards\nneural approaches. Vaswani et al. [22] proposed the transformer\narchitecture to enable capturing long range information during\nautomated natural language translation. The transformer design\nmakes use of the encoder-decoder architecture [22]: In the\nencoding part the model learns weight matrices for different\nword relations that encode how strong a word-encoding is\ninﬂuenced by every other word within the sequence [22].\nDuring decoding the next generated token is inﬂuenced not\nonly by the previously generated output, but also by the weight\nmatrices over the whole input sequence [22]. Transformers\nallow for self-supervised learning, e.g., by masking random\ntokens in input sequences and training the transformer to predict\nthe missing words [23]. These models tend to require a more\ncomputationally expensive training process compared to n-gram\nmodels, yet it has been shown that a transformer trained on a\nlarge dataset (e.g., BERT [23], CodeBERT [24]) can be used\nwithout or with only little ﬁne-tuning to assist with tasks in\nthe domain of source code processing [25]–[28].\nD. Program Analysis with Language Models\nHindle et al. introduced the “naturalness hypothesis” based\non which they proposed to use n-gram models to model source\ncode [6]. As an initial application, they presented a simple\ncode completion based on the n-gram model, which suggests\na ranking of the most likely tokens based on the local context\nof the completion. That is, one maximizes the probability of\nthe complete sequence by choosing the most probable n-gram\nbased on the given local context.\nThe idea of beneﬁting from concepts of NLP and combining\nit with other techniques has been taken up successfully in\nother work, for example, in the area of code completion [7],\n[29]. However, further investigations of the naturalness hy-\npothesis have also shown that large parts of the naturalness\nof code are due to syntactic elements such as parentheses\nor semicolons [11]. Even though source code is less natural\nthan previously thought, regularities can still be found in the\nsource code even after removing certain syntactic elements. In\nparticular, despite the limitations of the naturalness hypothesis,\nn-gram models have been determined to be capable of repre-\nsenting source code very well, often better than deep neural\nnetworks [30], when appropriately conﬁgured.\nIf source code is regular, then irregularities in the source\ncode are suspicious: Ray et al. demonstrated that buggy code\nhas a higher entropy than correct source code [9]. This insight\nenables the application of n-gram models to identify bugs in\nsource code. In particular, Wang et al. introduced BUGRAM,\nan automated approach for ﬁnding bugs based on n-gram\nmodels [8]. Given a speciﬁc project, BUGRAM trains an n-gram\nmodel on the source code, calculates probabilities for all\nsequences in the source code, and reports sequences with\nlow probability as potential bugs.\nDue to their speciﬁc structure, transformers open up further\npossibilities for code analysis. They can be used to jointly\nlearn from code and natural language by training the model on\nsequences containing both tokenized code and its documenta-\ntion to enable code search using natural language descriptions\nor generating documentation [24]. Those pre-trained models\ncan then also be applied to code-only tasks like identifying\nbuggy code [31] and generating potential ﬁxes for it [32].\nIII. LANGUAGE MODELS FOR SCRATCH\nSCRATCH differs from text-based programming languages,\nto which language models have been previously applied. Thus,\nwe ﬁrst need to deﬁne how to tokenize SCRATCH programs.\nWe then describe how n-gram models are generated and how\nthey are applied for code completion and bug ﬁnding. We\nfurther describe how we obtain the transformer, and how it\ncan be used for code completion.\nA. Tokenizing SCRATCH Programs\nTokenizing text-based programming languages is straight-\nforward, e.g., by directly lexing the source code. It is less\nobvious, however, how to tokenize SCRATCH programs: a\nSCRATCH program consists of a ZIP-ﬁle containing resources\n(images, sounds) as well as a text ﬁle in JavaScript Object\nNotation (JSON) format describing the code. The JSON ﬁle\ndescribes a program in terms of the targets (i.e., stage and\nsprites), and each target consists of its name, variables, lists,\nmessages, sounds, costumes, scripts, procedures (i.e., custom\nblocks), and blocks. The blocks are listed in an arbitrary order\n(e.g., the order in which they were inserted in the program),\nand each block consists of a unique identiﬁer as well as the\nidentiﬁers of the parent and successor blocks, as well as any\nparameter blocks. The block identiﬁers and their parent/child\nrelations are used in the SCRATCH virtual machine to create a\nsyntax-tree-like representation. Although the JSON format is\nspeciﬁc to SCRATCH, other block-based languages represent\nprograms similarly; for example, SNAP! encodes blocks in\nXML [33]. In order to tokenize a program, we ﬁrst use the\nparser provided by the LITTERBOX [16] analysis framework\nand create the abstract syntax tree for that program. We then\ntraverse the syntax tree in preorder, adding each traversed\nnode that represents a concrete block to the token stream. The\nresulting sequence of tokens is illustrated in Fig. 1c.\nTo reduce the vocabulary size and avoid out-of-vocabulary\nissues [34], we treat literals as follows [35]: First, we do not\ninclude string and number literals. On one hand, predicting\nliterals is very difﬁcult; on the other hand, text and numbers\nentered by users are usually very dependent on the use case.\nSecond, similar to prior work [36], [37], we generalize the\noccurrence of concrete variables to the occurrence of the generic\nvariable block var and calls to self deﬁned blocks as procedures\ncall\ncall\n. Since most programs actually deﬁne only a few\nvariables and procedures, this is on the one hand potentially\nnot a very large loss of information, but on the other hand\nsimpliﬁes generalization across project boundaries.\nEven though SCRATCH treats the drop-down menus that\nsome blocks include as individual blocks in its internal\nrepresentation, we do not include these as tokens as they are\ninseparable and are tailored to the speciﬁc block and use case.\nThus, overall we only include statement and expression blocks\nas tokens, which results in a vocabulary size of 137 blocks. As\nusual in NLP, we introduce structural blocks for the beginning\nand ending of scripts or sprites. Consequently, the remaining\ntoken sequence of the script in Fig. 1b looks as follows:\nBegin Script →\nWhen\nclicked →\nrepeat\ntimes\n→\nsay\n→End Script\nB. Code Completion using N-gram Models\nN-gram models are the most fundamental type of language\nmodels. They work with relatively small amounts of training\ndata compared to more modern deep learning approaches, and\nhave been successfully applied to various software engineering\ntasks (cf. Section II-D), and are therefore implemented as a\nbaseline for the use of language models in our work. The ﬁrst\ntask on which we apply the n-gram model is code completion.\nThe idea is to provide code completion for the next block\n(a) Incomplete script during\ndevelopment.\n(b) Completed script using an uncom-\nmon condition in the repeat block.\nFigure 2: Example of a SCRATCH script.\nwhen interpreting the preceding code linearly (i.e., in the order\nof the token sequence). Fig. 2a shows a simple SCRATCH\nprogram during development. The existing code tokens would\nbe interpreted in the following order:\nBegin Script →\nWhen\nclicked →\nrepeat until\n→\n=\n→End Script\nFor a 3-gram model,\nand\n=\nrepresent the\nlocal context to be completed by the model. Thus, in this\ncase the code completion shall suggest blocks that are usually\nused for equality comparisons in while-loops, such as\nvar ,\ncostume number\n, or\nround\n. To this end, we build a “general”\nn-gram model using modiﬁed Kneser-Ney smoothing on a\nlarge set of SCRATCH programs as described in Section III-A.\nA special case for code completion in SCRATCH exists in\nthe case of procedure deﬁnition blocks: Analogous to method\nheaders in JAVA, these blocks build the header of a newly\ncreated custom block. Since procedure deﬁnitions are not added\nusing drag and drop like other blocks, but using a dedicated\ndialogue to set the name and select possible parameters, we\nexclude procedure deﬁnitions from predictions. Another special\ncase is the behavior of code completion when the model predicts\nthe end of a block sequence based on the End Script blocks\nthat were observed during training. In this case, instead of\nthis prediction, which has no value for a user, the completion\nreturns a prediction for a new ﬁrst block, i.e., the completion\nfor the context at the beginning of a script.\nTo the best of our knowledge, this is the ﬁrst implementation\nof code completion for block-based programming languages.\nTherefore, we implemented a simple code completion based\npurely on the probabilities of the n-gram model, which allows\nus to evaluate how well the language model itself represents\nthe language, and provides a baseline for further research.\nC. Bug Finding using N-gram Models\nLanguage models can reﬂect that buggy code is less regular\nthan non-buggy code [9]. For example, Fig. 2b shows a\nSCRATCH script including a very unlikely condition in the\nloop block\n: Comparing the audio volume\nvolume to the\nx position of a sprite\nx position is very unlikely to be a meaningful\ncomparison. Ideally, a bug ﬁnding approach would be capable\nof capturing these and other irregularities in the source code.\nThis principle has been applied in the BUGRAM approach [8]\n(cf. Section II-D). As SCRATCH projects are much smaller\nthan JAVA or PYTHON projects, and likely contain less\nrepetitive API usages, BUGRAM is not directly applicable.\nHowever, a common scenario in an educational context is that\nstudents implement a task for which there are one or more\nmodel solutions. Thus, we train an n-gram model on model\nsolutions or known good student solutions, and then assess the\nprobabilities of all sequences in the student solutions, reporting\nthose with a particularly low probability. Since this model is not\ntrained on the entire code base (i.e., all student solutions), unlike\nin the BUGRAM approach, the use of a smoothing algorithm\nis necessary, for which we again use modiﬁed Kneser-Ney\nsmoothing. Additionally, we skip any preprocessing applied by\nBUGRAM, such as including the whole path in method names\nor skipping tokens with a particularly low count as proposed\nby Wang et al. [8] when parsing the source code, due to the\nsimple structure of SCRATCH. Thus, we structurally use the\nsame model for code completion and the bug ﬁnding task.\nD. Code Completion using a Transformer\nWhile n-gram models are simple, light weight, and easily\ninterpretable, transformers are more contemporary and often\nyield better results [25], [38]. However, they are limited to\ntasks where very large amounts of training data are available,\nwhich in our study includes only the code completion task.\nUsing the script tokenization (Section III-A) we generate\none token sequence [t0, . . . , tn] per sprite by concatenating\nthe token sequences from all procedure deﬁnitions followed\nby scripts. Since users can place their blocks freely on the\ncanvas, we tokenize the scripts in the order which they\nappear in the project ﬁle. This usually represents the order\nin which the user created them. For training we used the\nRoBERTa [39] implementation provided in the PYTHON\ntransformers library [40]. As identiﬁers are removed from\nthe token sequence, the model does not have to handle out-\nof-vocabulary scenarios and the vocabulary is reduced to 137\ndifferent tokens. Therefore, a simple word-level tokenizer that\nassigns a numeric identiﬁer for each token is used.\nThe model is trained using a masked language model [23]\nwith the default RoBERTa approach of randomly masking\ntokens [39]. In this approach tokens are randomly replaced by\na placeholder [MASK] for which the original token then has\nto be predicted based on the surrounding context:\nBegin →\nWhen\nclicked →[MASK] →\nsay\n→End\nWhen using the trained model for code completion, a\nsequence of up to the last m −1 tokens of the existing code\nis extracted and the additional [MASK] token appended. The\nmodel then predicts a probability for each token to replace\nthis mask, so that a top-x selection of suggestions can be\npresented to the user. Analogous to the completion with n-gram\nmodels, procedure deﬁnitions are excluded as described in\nSection III-B, and predictions for the end of a script are replaced\nby suggestions for a new script.\nIV. EVALUATION\nAs a baseline to gain an understanding of the applicability\nof language models in the context of block-based programming\nlanguages, we experimentally examine n-gram models on\nSCRATCH programs from two opposite angles: First, we\nconsider how well n-gram models capture the regularities of\nSCRATCH programs by looking at the highest probabilities\nencoded in the model, using the task of code completion.\nSecond, we consider how well n-gram models detect deviations\nfrom common patterns by looking at the lowest probabilities\nencoded in the model, using the task of bug ﬁnding. Finally,\nwe investigate whether predictions can be improved using deep\nlearning models. This leads to the following research questions:\n• RQ1, Completion: How well does code completion based\non n-gram models perform on SCRATCH source code?\n• RQ2, Bug Finding: How well does bug ﬁnding based\non n-gram models perform on SCRATCH source code?\n• RQ3, Model Comparison: Do transformer-based deep\nlearning models improve over n-gram models?\nA. RQ1: Code Completion with N-gram Models\n1) Experimental Setup: To create a general model for the\npurpose of code completion, we trained an n-gram model on\n100 000 randomly selected SCRATCH programs. Between May\n2021 and February 2022 we retrieved the 10 000 most recently\npublicly shared SCRATCH programs each day using the REST\nAPI of the SCRATCH website, resulting in a total 2.7 million\nprojects. From these, we ﬁltered projects with less than 10\nblocks, as these very often represent projects in which children\nfocused on arts and drawing, e.g., drawing a background and\narranging sprites on it, rather than coding. Furthermore, we\nexcluded remixes (i.e., copied and modiﬁed programs). From\nthe resulting 1.1 million projects, we then randomly sampled\n110 000, which we split into a training set of 100 000 projects,\nand an evaluation set of 10 000 projects. Comparing the number\nof blocks between projects in the training and evaluation sets\nshows that there is no signiﬁcant difference (p = 0.191 using\na Mann-Whitney U test [41]), thus conﬁrming that the two\ndatasets are drawn from the same overall distribution.\nTo choose a suitable value for the sequence length n, we\ntrained the model for n = {1, 2, 3, 4}. We used 4 as the\nupper bound for two reasons: First, we observed only marginal\nimprovements for larger n, which is in line with ﬁndings in\nprior work [6], [30]. Second, the models require substantially\nmore memory and computation time as n increases. This is\nparticularly relevant in the bug-ﬁnding use case, where a custom\nmodel is trained, e.g., for a given set of programs, in order to\nidentify unusual sequences. In the context of ﬁnding bugs in\nprogramming assignments, which tend to use relatively small\nand simple SCRATCH programs, training a complex model\nmay not be worthwhile. We use the same 100 000 randomly\nselected SCRATCH programs for each n-gram model.\nThe projects from the evaluation data set are broken down\ninto sets of local contexts, each consisting of n−1 blocks. Every\ncontext is given to the completion engine that is asked to predict\nTable I: Accuracy of code completion in top x suggestions for\ndifferent values of n.\nTop 1\nTop 2\nTop 3\nTop 5\nTop 10\n1-gram\n6.24 %\n12.27 %\n17.17 %\n25.88 %\n44.38 %\n2-gram\n23.22 %\n34.56 %\n42.54 %\n54.03 %\n69.44 %\n3-gram\n31.41 %\n43.87 %\n52.08 %\n62.82 %\n76.07 %\n4-gram\n36.31 %\n49.05 %\n56.77 %\n66.76 %\n78.35 %\ntransformer\n33.83 %\n43.78 %\n49.91 %\n57.79 %\n69.32 %\nthe next block for this context. The completion engine returns\nthe top x = {1, 2, 3, 5, 10} blocks ranked by their probability,\nand we evaluate the code-completion suggestions in terms of\ntop-x accuracy, i.e., the ratio of suggestions that contained\nthe actual block in the original program. We consider top-x\naccuracy for varying x and n, and we evaluate the inﬂuence\nof block frequency, category, and shape on top-x accuracy.\n2) Threats to Validity: Threats to external validity arise\nas results may not generalize to projects outside our dataset.\nWe conﬁrmed that in terms of size the sample is a valid\nrepresentation of publicly shared projects; however, unﬁnished,\nunshared programs might have other properties. To avoid\nskewing results with very similar code we used only original\nprojects and excluded remixes. Threats to internal validity\nmay arise from our implementation: Although we tested\nand validated all code thoroughly, our implementation may\nconfound the studied measurements and relationships. For\nexample, rare aspects of the SCRATCH program representation\nnot encountered during testing may be misrepresented. Threats\nto construct validity may arise from our choice of top-x\naccuracy as metric rather than precision or recall. This choice\nis based on the use case: We assume that each suggestion in\nthe top-x is equally useful. Indeed a deployed code-completion\nengine can suppress low-conﬁdence predictions, and a user\ndoes not have to accept incorrect suggestions.\n3) Results: Table I shows the top-x accuracy of code\ncompletion for n = {1, 2, 3, 4}. Top-x accuracy is deﬁned\nas the sum of all true positive predictions per block divided\nby the total number of predictions. A prediction is considered\na true positive if the set of top-x suggestions contains the\nactual block to be predicted. By deﬁnition, top-x accuracy\nmonotonically increases with increasing x. We also observe\nthat it increases with increasing n. Since 4-grams perform best,\nwe use 4-grams for the rest of RQ1. Furthermore, we use top-3\naccuracy for subsequent evaluations as 3 is a reasonable number\nfor suggestions in the SCRATCH user interface (e.g., in the\n“backpack” of code snippets) satisfying the design philosophy\nof SCRATCH to keep the cognitive load low [10].\nThe overall best predicted block is\nWhen\nclicked with a top-3\naccuracy of 95.52 %. With an occurrence rate of 6.03 % this is\nthe second most frequent block overall in the evaluation data.\nConsequently, it is likely that occurrence has an inﬂuence on the\nperformance of the prediction. We note that the top blocks in\nother categories show a substantially lower top-3 accuracy, thus\nthere appears to be an inﬂuence also of the category. Finally, we\nobserve that generally oval and diamond shaped blocks all have\nTable II: Completion accuracy by category for n=4, x=3.\nGroup\nOccurrences\nAccuracy\nAcc. Transformer\nsound\n2.5 %\n32.9 %\n44.7 %\npen\n0.9 %\n34.0 %\n25.1 %\nmyblocks\n0.9 %\n41.3 %\n60.6 %\nevent\n15.3 %\n48.0 %\n30.4 %\nmotion\n12.2 %\n51.5 %\n48.9 %\nlooks\n20.7 %\n53.9 %\n64.2 %\ncontrol\n21.2 %\n58.8 %\n31.0 %\noperator\n9.6 %\n66.3 %\n53.2 %\ndata\n11.4 %\n68.0 %\n66.3 %\nsensing\n5.3 %\n73.6 %\n75.9 %\nTable III: Completion accuracy by shape for n=4, x=3.\nShape\nOccurrences\nAccuracy\nAcc. Transformer\nend\n1.3 %\n29.9 %\n58.9 %\nhat\n13.7 %\n47.6 %\n16.9 %\nstack\n52.0 %\n51.8 %\n56.3 %\nc\n12.2 %\n59.9 %\n14.0 %\noval\n11.5 %\n74.9 %\n67.6 %\ndiamond\n9.4 %\n75.2 %\n70.7 %\nparticularly high accuracy values, suggesting that the shape\nof blocks also contributes to the prediction. In order to better\nunderstand what determines the overall prediction performance,\nwe therefore investigate the inﬂuence of these three aspects:\n(1) the frequency with which blocks occur in practice; (2) the\ncategory the blocks belong to (e.g., motion, looks, . . . ); and (3)\nthe shape of the blocks (e.g., regular stackable blocks, event\nhandler blocks, . . . ).\nFigure 3a summarizes these three aspects and their inﬂuence\non the top-3 accuracy for n = 4: The plot is split into facets\nbased on the 10 different block categories in which the blocks\nare sorted in the SCRATCH user interface. For each category,\nblocks are plotted based on the number of occurrences in the\ntraining data (x-axis) and the resulting top-3 accuracy (y-axis).\nData points are color-coded based on their shape. In particular,\nhat blocks represent event handlers, stack blocks are regular\nstatements, oval blocks represent reporters returning numerical\nor textual data, diamond shaped blocks represent Boolean\nvalues, c-shaped blocks are control structures such as loops\nand if-conditions, and stop blocks are statement blocks that\ncannot have successors. The “myblocks” category only contains\none block because we generalize identiﬁers (see Section III-B),\nand only predict calls to these self-created blocks, not their\ncreation. Consequently, data points for procedure deﬁnition\nblocks and their possible parameters are not included. To\nbetter understand the inﬂuence of block frequency, category,\nand shape on accuracy, we used multiple linear regression to\nmodel this relationship. We include occurrence as a continuous\nvariable and category and shape as categorical variables.\nSince we are interested in whether the accuracy for particular\ncategories and/or shapes, independently of occurrence, differs\nsigniﬁcantly from the average accuracy, we used deviation\ncoding—comparing each level to the grand mean. Figure 3b\nshows the results of the regression analysis.\nTable IV: Top 3 predictions for the example in Fig. 2a.\nBlock\nConﬁdence\nvar\n80.31 %\ncostume number\n5.67 %\nanswer\n5.44 %\nFigure 3a suggests differences between categories, which are\nsummarized in Table II: Blocks of the sensing, data, operator,\nand control categories are predicted with a substantially higher\naccuracy than, for example, blocks from the sound or pen\ncategories. One of the reasons for this lies in differences in the\nfrequency of occurrence. For example, blocks of the pen or\nsound categories appear much less frequently than, for example,\nblocks from the motion or looks categories. The importance\nof occurrence can also be observed within categories, not\njust across categories. The ﬁtted lines in Fig. 3a very clearly\ndemonstrate that across all categories and shapes, the number of\noccurrences has a positive inﬂuence on the accuracy: The more\nfrequently a block occurs in practice, the higher its probability\nof being predicted correctly. This is, for example, conﬁrmed by\nthe high occurrence and accuracy of the best predicted block\nof the event category (cf. Table II), i.e.,\nWhen\nclicked . Figure 3b\nconﬁrms that the occurrence has a signiﬁcant effect on the\naccuracy of the prediction, although the small coefﬁcient of\nthe regression model indicates the inﬂuence is small.\nHowever, not all differences between the categories can be\nexplained through the numbers of occurrences. For example,\nthe categories sensing and operators can be predicted relatively\nwell (Table II), even though blocks in these categories occur less\nfrequently overall compared to, e.g., motion blocks. Figure 3b\nconﬁrms some inﬂuence of the category; in particular, the\nmotion category has a signiﬁcant inﬂuence on the prediction\naccuracy. However, we observe that the well-predictable sensing\nand operator categories differ from others in an important\nproperty: they all contain a particularly large proportion of\ndiamond-shaped and oval-shaped blocks, which represent\nexpressions rather than statements (100 % expression blocks in\noperator, 83 % in sensing). Pen and sound blocks, for example,\nconsist of almost only regular stack blocks. Figure 3a generally\ndemonstrates with the different colors and ﬁtted lines that stack\nblocks appear to be more difﬁcult to predict than, for example,\noval blocks (cf. categories data, looks, motion, sensing) or\ndiamond-shaped blocks (cf. sensing). Table III supports this\nimpression using the top-3 accuracy values grouped by the\nshape of the blocks. Figure 3b conﬁrms that oval and diamond\nshapes have a signiﬁcant positive inﬂuence on the prediction,\nwhereas hat blocks have a signiﬁcant negative inﬂuence.\nWhenever there is a block in the local context that is usually\nfollowed by an expression block, this signiﬁcantly limits the\nactual choice of matching blocks. The code example from\nFig. 2a illustrates this phenomenon:\nhas room only for\ndiamond-shaped expression blocks. Round expression blocks\nare intended to go into the round placeholders of the\n=\nblock, which reduces the successor blocks. The top blocks\npen\nsensing\nsound\noperator\nmyblock\ncontrol\ndata\nevent\nlooks\nmotion\n0k\n1k\n2k\n0k\n10k\n20k\n0k\n10k\n20k\n0k\n10k\n20k\n0k\n10k\n20k\n0k\n20k\n40k\n0k\n20k\n40k\n0k\n20k\n40k\n0k\n20k\n40k\n0k\n10k\n20k\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nOccurrences\nAccuracy\nShape\nc\ndiamond\nend\nhat\noval\nstack\n(a) Top-3 accuracy vs. occurrence of blocks, grouped by category and shape of blocks. (Note the differently\nscaled x-axes.)\nVariable\nCoefﬁcient\np\nIntercept\n2.94 × 10−1\n< 0.001\nOccurrences\n1.49 × 10−5\n< 0.001\nCategory\ncontrol\n−1.05 × 10−1\n0.216\ndata\n9.81 × 10−3\n0.834\nevent\n−4.62 × 10−2\n0.634\nlooks\n5.69 × 10−3\n0.897\nmotion\n1.08 × 10−1\n0.019\npen\n5.72 × 10−2\n0.342\nsensing\n−2.68 × 10−2\n0.622\nsound\n−9.49 × 10−2\n0.108\noperator\n1.28 × 10−2\n0.831\nmyblock\n7.96 × 10−2\n0.612\nShape\nc\n−2.34 × 10−2\n0.797\ndiamond\n1.55 × 10−1\n0.034\nend\n3.62 × 10−2\n0.763\nhat\n−2.86 × 10−1\n0.003\noval\n1.92 × 10−1\n0.002\nstack\n−7.35 × 10−2\n0.146\n(b) General linear model.\nFigure 3: Inﬂuence of block occurrence, category, and shape on code-completion accuracy.\nask\nand wait\nif\nanswer =\nthen\nask\nand wait →\nif\nthen\n→\n=\n→\nanswer\n(a) Code for processing user input (left) and corresponding 4-gram (right).\nThe probability of this 4-gram is 97.56 %.\nBegin Script →\nWhen backdrop switches to backdrop1\n→\nhide\n→End Script\n(b) Hiding a sprite on change of the backdrop as an isolated script with no\nother blocks following. The 4-gram has a probability of 95.48 %.\nFigure 4: Common language idioms as captured by the model.\nsuggested by the model for this scenario are shown in Table IV.\nThe model is 80.58 % conﬁdent that a\nvar should be inserted\ninto the\n=\nblock.\ncostume number\nand\nanswer have a\ncomparably low probability of 5.69 % and 5.29 % compared to\nthe variable block. While\ncostume number\nis a sensible suggestion\nfor this scenario, the\nanswer block would only be usable if\npreceded by a\nask\nand wait block to which the answer block could\nrefer to (cf. Fig. 4a for a usage example), but in Fig. 2a\nthere is no such block. Nevertheless, all 3 suggestions do\nhave in common that their shape makes them a syntactically\ncorrect building block. This effect is comparable to the prior\nobservation that syntactic elements contribute substantially\nto the predictability of source code [11]. Accordingly, for\ncategories like pen, sound, or motion, which do not contain\nmuch syntactical constraints, completion performs worse.\nFinally, some regularity can be attributed to recurring idioms\nin common SCRATCH code. Considering n-grams to which the\nmodel assigns very high probabilities and excluding known\npatterns such as starting with a\nWhen\nclicked block, we see that the\nmodel has learned some idioms that go beyond purely syntactic\nregularities. Similar to traditional programming languages, there\nare certain token sequences that repeat across the boundaries\nof speciﬁc programs. For example, the 4-gram in Fig. 4a\nhas a probability of 97.56 %, and describes requesting a user\ninput and reacting based on a comparison of this. While this\nsequence occurs much less frequently than programs beginning\nwith\nWhen\nclicked (7 465 vs. 499 146 occurrences), the model is\nvery conﬁdent that\nanswer follows as last block for this local\ncontext. This can only be partially explained by occurrence,\ncategory, and shape: Although\nanswer is an oval block and thus\nﬁts syntactically well, there are numerous other oval blocks in\nSCRATCH. However, the total probability for all other blocks\nof all shapes to follow this local context is less than 2.5 %.\nOther examples of idioms we observed include isolating\nfunctionality in very short scripts (c.f. Fig. 4b) and repetitions\nof the same blocks (e.g., inserting elements into a list). These\nidioms show a connection of the blocks on a semantic level\nand therefore indicate further repetitive, predictable structures\napart from pure syntactic connections, i.e., shapes, and block\nfrequency, i.e., occurrences. These idioms are comparable to\nthose discovered in traditional programming languages.\nSummary (RQ1, Completion): The best model (4-gram)\nachieves a top-3 accuracy of 56.77 %. Prediction quality is\ninﬂuenced by block frequency, shape, and category, but we\nalso found recurring idioms inﬂuencing regularities.\nB. RQ2: Bug Finding with N-gram Models\nFor RQ2 we use the n-gram model in the inverse way\ncompared to RQ1: Rather than predicting the most likely blocks,\nwe are interested in the least likely sequences of blocks.\nIn contrast to the evaluation in the BUGRAM paper [8], we\nassume that for the small student solutions we can identify\nall actual errors in the programs using tests. Thus, in our\npaper, a test suite acts as ground truth to evaluate whether\nthe most unlikely sequences actually contain erroneous code.\nIn the original paper, the authors looked for undetected bugs\nand refactoring possibilities in very large projects. Instead of\ncomparing the code with a ground truth like existing tests, it\nwas manually examined for improvement possibilities.\n1) Experimental Setup: The application scenario of the\nbug ﬁnding task is a programming assignment given in an\neducational context. We use the dataset provided with the\nreplication package of the WHISKER [19] paper on testing\nSCRATCH programs, consisting of 41 student solutions and\none model solution of a Fruit Catching game (c.f. Fig. 5a). The\nobjective of the game is to catch as many apples and bananas\nas possible in 30 seconds by moving a bowl at the bottom of\nthe screen. For bananas that touch the ground, the player loses\npoints. The game is lost if an apple drops on the ground.\nWe trained a 3-gram model with modiﬁed Kneser-Ney\nsmoothing on the model solution and one student solution\nwhich was deemed almost correct using automated tests [19].\nWe use n = 3 based on prior results of Wang et al., who\nfound that 3-gram models perform best in ﬁnding bugs and\nrefactoring opportunities [8].\nUsing this model, we determined the probabilities for all\noccurring sequences for the 41 student programs (that is,\nincluding the best student solution). Intuitively, sequences with\nlower probability assigned by the model are more likely to\ncontain bugs. When extracting sequences we exclude “loose”\ncode, i.e., blocks and scripts not connected to an event handler\nwhich are never executed. Since the ideal sequence length\nfor this analysis has not previously been investigated in our\ncontext, we performed the evaluation for sequence lengths\nfrom 3 to 6. Longer sequence lengths are unlikely to be useful\nfor the generally small SCRATCH programs, as the sequences\notherwise would frequently exceed script boundaries.\nTo investigate whether low probability sequences indicate\nbugs, we randomly selected 10 of the 41 student solutions\nfor manual validation, considering only those with at least 10\nsequences, as they are otherwise unlikely to fully implement\nany aspects of functionality. For each of these 10 programs,\nwe manually classiﬁed the 10 sequences with the lowest\nprobabilities, for each of the sequence lengths in the range\nof 3 to 6. Two authors independently evaluated whether the\ncorresponding sequences contained bugs or not. As ground\ntruth for the existence of bugs we use the extensive WHISKER\ntest suite provided by Stahlbauer et al. [19] that fully covers\nthe program behavior. We consider a sequence to contain a\nbug exactly if it causes the failure or skipping of one or more\ntest cases. Thus for each sequence, we determined (1) whether\nthe sequence contains at least one bug, and (2) for each failing\ntest whether the sequence contributes to the failure. In the case\nof disagreement of the two independent classiﬁcations, these\nindividual cases were discussed again until a consensus was\nreached. Thus overall, 400 sequences were manually classiﬁed.\nTable V: Percentage of found bugs for each sequence length.\nSequence length\n3\n4\n5\n6\nBugs found (%)\n57.58\n62.88\n64.39\n56.06\nTable VI: For sequences of length 4, the bottom 10 most\nunlikely sequences and 10 random sequences, the Precision@10\nfor each sequence to contain at least one bug, as well as the\nproportion of found bugs in total, and the total number of bugs\nin the program.\nPrecision@10\n% Bugs Found\nBugs\nBottom\nRandom\nBottom\nRandom\nTotal\nK6 S01\n90.0\n40.0\n96.3\n37.04\n27.0\nK6 S12\n60.0\n90.0\n80.8\n26.9\n26.0\nK6 S15\n60.0\n30.0\n62.5\n31.3\n16.0\nK6 S18\n60.0\n50.0\n50.0\n50.0\n6.0\nK6 S31\n30.0\n10.0\n75.0\n12.5\n8.0\nK7 S03\n60.0\n40.0\n27.3\n45.5\n11.0\nK7 S10\n20.0\n0.0\n28.6\n0.0\n7.0\nK7 S14\n30.0\n20.0\n60.0\n20.0\n5.0\nK7 S17\n10.0\n10.0\n50.0\n50.0\n2.0\nK7 S24\n70.0\n50.0\n33.33\n20.8\n24.0\nAverage\n49.0\n34.0\n62.88\n28.79\n13.2\np\n0.073\n0.003\nˆ\nA12\n0.69\n0.84\nAs a baseline, we further selected and classiﬁed 10 random\nsequences per program using the best sequence length deter-\nmined by the classiﬁcation of low probability sequences, which\nallows us to determine if 10 most unlikely sequences are more\nlikely bugs than random sequences.\n2) Threats to Validity: Threats to external validity arise as\nour experiments are based on one task and student solutions\nfrom two school classes, so the results may not generalize to\nother tasks or classes. To avoid threats to internal validity, we\nrandomized the selection of projects to avoid bias. As manual\nclassiﬁcation may be inﬂuenced by subjective interpretation,\neach sequence was independently classiﬁed by two authors of\nthis paper to minimize the inﬂuence on the results (inter-rater\nreliability: 88.8 %). Furthermore, the same authors classiﬁed\nall sequences to ensure that the results are consistent and\ncomparable to one another. To ensure construct validity of\nour evaluation, we rely on accepted measures for bug ﬁnding,\nconsidering the number of buggy sequences as well as the\nnumber of unique bugs.\n3) Results: Table V lists the overall percentage of bugs found\n(b = #bugs found/#bugs in total) for different sequence\nlengths. Sequences of length 5 ﬁnd the most bugs overall,\nclosely followed by sequences of length 4; sequences of length\n6 identify the fewest bugs. The minor difference between\nsequences of length 4 and 5 originates only from a single\nprogram, for which sequence length 5 ﬁnds signiﬁcantly more\nbugs (K7 S03: 27.27 % vs. 90.91 %). In all other programs,\nthe results of sequence length 4 are equal or even better.\nSince sequences of length 4 narrow down the source of the\nproblem/bug better than sequences of length 5, all further\nresults are based on a sequence length of 4 tokens.\n(a) The Fruit Catching game.\n(b) Student solution K7 S10 for Banana sprite.\nColored blocks are examples for reported\nsequences that cause the failure of tests.\nFigure 5: Example for Fruit Catching game with buggy student\nsolution.\nTable VI lists the results per program based on the least\nlikely and random sequences. Precision@10 is given in terms\nof the number of sequences in the bottom 10 containing at least\none bug, i.e., the probability of a sequence in the bottom 10\nto contain an actual bug. The table also shows the proportion\nof all bugs found in each program. Since the programs are\nrelatively small, there is frequently more than one bug per\nsequence; for reference, the table also lists the total number\nof bugs, which corresponds to the number of failed tests.\nIn terms of the percentage of buggy sequences, on average\nhalf the sequences among the least likely ones contain at least\none bug, in contrast to only 34 % of the randomly selected\nsequences. The difference is not signiﬁcant at α = 0.05\n(p = 0.073), but note that, since the sequences are randomly se-\nlected, there is some overlap with the 10 least likely sequences:\non average 16 % of the randomly selected sequences are also\namong the bottom 10. However, randomly selected sequences\nof length 4 are only capable of ﬁnding 29 % of bugs in total,\nand they ﬁnd no more than 50 % of the bugs in any program. In\ncontrast, the least likely sequences ﬁnd an average of 63 % of\nthe bugs in total. The improvement of the least likely sequences\nover random sequences is statistically signiﬁcant, with a large\nVargha-Delaney effect size of ˆA12 = 0.84 and p = 0.003 using\nthe Mann-Whitney U test. This conﬁrms that indeed the n-gram\nmodel captures the expected structure of the solution.\nFor example, Fig. 5b shows student solution K7 S10 of\nthe Banana sprite. The colored parts of the source code are\ntwo sequences of length 3 that are among the 10 least likely\naccording to the model. For both examples, the code actually\nleads to failing test cases: For the ﬁrst sequence, the solution\nviolates the task speciﬁcation that the Banana should start one\nsecond after the ﬂag was clicked. The program, however, starts\nimmediately with the for-loop that moves the Banana. For the\nsecond code sequence, the Banana is supposed to disappear\nfor one second after the point deduction as a time penalty, but\nthe buggy implementation causes the Banana to start dropping\nfrom the top again immediately.\nWe generally observed that blocks not included in the training\ndata have a strong inﬂuence on the probability of a sequence. In\nparticular, not only are such sequences assigned particularly low\nprobabilities, but the same unusual block tends to inﬂuence the\nprobability of several surrounding sequences. As a consequence,\nthe least likely 10 sequences may in the worst case cover only a\nfraction of the program. For example, solution K6 S01 violates\nthe task speciﬁcation by starting the game (i.e., each script\nin each sprite) with pressing the up arrow key instead of the\ngreen ﬂag. The model has not seen this block in the training\ndata. As a result, 9 of 10 sequences reported least likely by the\nmodel, contain the unknown\nWhen up arrow\nkey pressed block. We noticed\nthe same behavior with uncommon, yet correct, blocks as well.\nThe original BUGRAM paper suggests ﬁltering rare tokens [8].\nDue to the much more limited vocabulary in SCRATCH we\ndecided not to implement such an approach; however, the\nexperimental results suggest that this could be a useful addition\nwhen implementing this approach in practice, although a\nchallenge for this will be to not discard too large parts of\nthe rather small student solutions, which would also cause\npotential bugs to go undetected.\nProgram K7 S17 is the overall best student solution, which\nwas included in the training data of the model. The fact that\none of the two bugs in this program was identiﬁed (Table VI)\nshows that, similar to the BUGRAM [8] approach, errors can\neven be found in programs which served as training data.\nSummary (RQ2, Bug Finding): For 9 out of 10 programs,\nthe number of bugs found using the model is greater than or\nequal to that of randomly selected sequences; this improvement\nis statistically signiﬁcant.\nC. RQ3, Comparison: Code Completion with Transformer\n1) Experimental Setup: We trained the RoBERTa model [39]\non a dataset obtained using the same procedure as described in\nSection IV-A1, but by sampling 500 000 programs with 517 431\nsprites. To limit sequences to the maximum length m that can\nbe processed by the model, we split them into subsequences:\nThe ﬁrst generated sequence for a program with n tokens is\n[t0, . . . , tmin(n,m)]. Then the ﬁrst script not fully included in\nthis sequence is taken as the starting point for the next one.\nTo embed the script into maximally possible context, tokens\npreceding and following this script are added symmetrically\nto ﬁll the sequence up to a length of m. This is repeated until\nall tokens have been included in at least one subsequence. By\nnot splitting sequences at script-level, it is also possible to\npredict when a script should ﬁnish and a new one should be\nstarted. Applying the sequence splitting resulted in 4 493 833\nsequences for training. The same 10 000 programs as for RQ1\nwere used for evaluation.\nWe used the default RoBERTa hyperparameters as starting\npoint for further tuning [39]. As the language to be modeled\nis smaller, and programs also tend to be small, the tuning\ndecreased the maximum sequence length to 256, the number\nof hidden layers (12 to 2), their sizes (hidden size 256,\nintermediate size 512), and number of attention heads (12 to 4).\nThe other parameters remained unchanged.\n2) Threats to Validity: The same threats to validity apply as\ndescribed in Section IV-A2. An additional threat to construct\nvalidity arises from the splitting into smaller subsequences. To\nmitigate this risk, we compared the results when splitting the\nsequence into one padded sub-sequence per script, and using\nnon-overlapping chunks of tokens of the model’s sequence\nlength. We chose the strategy described in Section III-D as\nthe others did not improve the prediction accuracy. Our results\nachieved using the RoBERTa model [39] might not generalize\nto alternative transformer-based models. This matches our aim\nof not maximizing any particular metric of model performance,\nbut instead providing a baseline and evidence that investigating\ndifferent models as part of future work is warranted.\n3) Results: Table I shows the top-x accuracy of code\ncompletion. The accuracy for the ﬁrst three suggested tokens\nis similar to the one for the 3-gram model, i.e., the second\nbest n-gram model according to the results for RQ1.\nTables II and III give a more detailed insight which types of\nblocks can be predicted accurately. The transformer outperforms\nthe 4-gram model (i.e., the best n-gram model) for predictions\nof end blocks ( delete this clone and\nstop\n) and performs similarly on\nregular statements (stack), Boolean expressions (diamond), and\nplaceholders (oval). For script starts (hat) and branching blocks\n(c) the accuracy is worse compared to the n-gram model.\nThe discrepancy between being able to predict ends of\nscripts and new starts is noteworthy. Scripts in SCRATCH\ndo not have to use an “end” block as terminal statement.\nTherefore, in most cases no clear indicator exists when a new\nscript should start. Within the “hat” group of blocks the ones\nwith the best accuracy are\nwhen backdrop switches to\n(35.3 %),\nwhen key\nis pressed\n(25.3 %), and\nwhen I receive\n(22.5 %). In those cases it is likely that\ncorresponding statements to change the background or send\nmessages are placed near the end of previous scripts, which\ncan then be interpreted as hints that new scripts should start.\nNote that the “end” blocks are not necessarily placed at the end\nof a sprite sequence. Instead, the scripts within the sequence\nare saved in the same order as they were created by the user.\nHence, the model cannot use the number of tokens following\nthe “end” block to learn if a script should end.\nFor branching blocks (c-blocks) the transformer has substan-\ntially lower accuracy (14.0 %) compared to the n-gram model\n(59.5 %). This may be caused by the bidirectional attention\nthat is applied during training: Using the surrounding context\nfrom both sides of the masked c-block, the model learns that\nthey are in most cases, except for forever-loops, followed by\na Boolean condition token. As this information is missing\nduring the code completion task, there is not enough context\nto reliably predict the correct token. However, modifying the\nattention mechanism to only allow unidirectional attention on\ntokens preceding the masked one resulted in worse accuracy.\nOverall, the prediction accuracy of the transformer is worse\nthan the best n-gram model. Transformers can handle large\nvocabularies in the order of tens of thousands of different\nwords [23], but this advantage is of little use as the tokenized\nSCRATCH language only has 137 words. Additionally, the use-\nfulness of long range information to the next token prediction is\nnot clear. The scripts are ordered in the sequence in which the\nuser inserted the ﬁrst block contained in it while creating the\nprogram and do not depend on each other in the program ﬂow\nexcept for passed messages in between. For example, in the\ncontext within the same sprite some scripts might handle user\ninputs and the resulting movement while others are triggered\non interactions with other sprites to play sounds or change the\nlook. Therefore, we expect that the next token mostly depends\non the short-range local context and the unrelated blocks of\nthe other scripts act as distracting factor.\nSummary (RQ3, Model Comparison): The transformer\nmodel performs comparable to the 3-gram model and thereby\nworse than our best n-gram model (n = 4). We conjecture that\nthis is caused by the small vocabulary and the importance of\nshort-range over long-ranged information for code completion.\nV. DISCUSSION\nRecent program analyses frequently build on the “natural\nhypothesis” [6], which assumes that programming languages\nhave similar regularities as natural language, and source code is\ntherefore amenable to natural language processing techniques. It\nhas been shown that a certain degree of this observed regularity\nin source code is due to syntactic overhead in text-based\nprogramming languages. Speciﬁcally, Rahman et al. showed\nlower regularities when ﬁltering common syntax tokens such\nas delimiters or nesting tokens [11]. Our investigation is related\nin that block-based programming languages explicitly avoid\nsuch syntax tokens, thus also making the language potentially\nless repetitive and predictable.\nCompared to JAVA, the code completion for SCRATCH\nappears to perform slightly worse. For example, in a related\nstudy [29] based on a 3-gram model for JAVA, the top-1\naccuracy was almost 20 % higher than the corresponding\ntop-1 accuracy for the 4-gram model in SCRATCH. This study\nmodeled the source code as a stream of lexical tokens including\nidentiﬁers, keywords, or symbols, speciﬁed by the programming\nlanguage [29]. Further, this study evaluated a completion\ntask, using local context such as <if, (, node> and\ncompletion suggestions such as “!= null”, “== null”, and\n“.isRoot()” [29]. The task design is comparable to that of\nour completion task. Accordingly, we assume that the results are\ncomparable to the extent possible across programming language\nboundaries. A difference in prediction accuracy of almost 20 %\ntherefore suggests that there are substantial differences either\nin the properties of block-based vs. text-based programming\nlanguages or in the characteristics of their programs.\n_updateBubble\n_getBubbleState\n_formatBubbleText uid\n_renderBubble\ngetCustomState\nsimple\nsetCustomState\n_onTargetWillExit\nupdateTextSkin\naddListener\ncreateDrawable\ncreateTextSkin\nupdateDrawableSkinId\n_positionBubble\nsay\nHello World\nthink\nHello World!\nFigure 6: Two different SCRATCH blocks vs. their shared\nimplementation and call-tree in JAVASCRIPT.\nBased on recent trends in NLP and software engineering, one\nmight expect deep learning models to make a difference here,\nbut our results suggest this is not the case (cf. Section IV-C3).\nIn a similar experiment on JAVA code by Ciniselli et al. the\ntransformer model achieved a better prediction accuracy than\ntheir best n-gram model [25]. This suggests that there is a\ndifference in how the model is able to use long-range informa-\ntion in block-based vs. text-based programming languages. To\nmitigate this, it may be possible to integrate more information\ninto a deep learning model [38], [42]. For example, using\nthe ﬂat sequences as the input to the transformer removes all\nstructural information from the code. Guo et al. [42] adapted\nthe attention mechanism of their GraphCodeBERT model to\nfocus on related code elements determined by the data ﬂow\ngraph. Similarly, for SCRATCH information about the relation\nof scripts (e.g., passed messages, changed sprite attributes)\ncould be extracted [16] to help the transformer focus on scripts\nrelated to the one that should be completed and ignore other\nones. This could improve the capture of long-range information\nin a long sequence containing several short scripts.\nAs part of RQ1 and RQ3 (Sections IV-A and IV-C), we\ndiscussed some aspects of block-based programs and their\ninﬂuence on predictability. Besides these aspects, there is\nanother important difference between traditional, text-based pro-\ngramming languages and block-based programming languages\nthat likely has an impact on repetitiveness and predictability:\nthe level of abstraction at which source code is deﬁned. Figure 6\nshows the actual JAVASCRIPT code that is executed by a\nsay\nor a\nthink\nblock, as well as the corresponding call tree of\nthe corresponding callback functions in the SCRATCH virtual\nmachine code. On the one hand, the example shows how\nabstract SCRATCH code is compared to traditional programming\nlanguages. Not only is syntactic overhead removed, but to\nsome extent the code is also streamlined. On the other hand,\nthis causes repetitions in the JAVASCRIPT code and thus\nmay be interpreted as introducing code regularities: in the\nJAVASCRIPT implementation both the\nsay\nand\nthink\nblocks\ncreate, format, and render a speech bubble, and the call to\n_updateBubble is essentially the same for both blocks.\nHowever, in the SCRATCH code, the two blocks are distinct\nwithout any repetitiveness. The JAVASCRIPT code is thus more\nrepetitive and predictable.\nVI. CONCLUSIONS\nA recent trend in software engineering research is to\napply language models and NLP techniques to text-based\nprogramming languages for a multitude of different tasks. A\nniche of programming languages excluded from this trend\nso far is represented by block-based languages, which differ\nfrom text-based languages by some of the very properties that\nmake NLP techniques applicable to source code. In order to\nshed light on the applicability of language models to block-\nbased programs, we empirically studied n-gram and transformer\nmodels for SCRATCH. Although our results demonstrate that\nblock-based languages are more challenging to predict, they\nnevertheless demonstrate that the approach is viable.\nPrior work on code completion suggests various ways in\nwhich our baseline model could potentially be improved: For\nexample, it is conceivable that other models, such as statistical\ngraph models [7], [11] or neural models (e.g., [43], [44]),\ncould improve performance. Further ﬁltering of the vocabulary,\ne.g., to ﬁlter rare blocks [8], might lead to performance\nimprovements [34], [35]. The performance could also be\nimproved by taking additional context into account [45];\nfor example, for SCRATCH the context could be provided\nby the sprite or stage being edited. Similarly, it might be\npossible to build models for different types of programs; for\nexample, games might differ fundamentally from animation or\nart projects in SCRATCH, thus leading to different models.\nBesides the performance of the models, an important question\nfor future work concerns the application of these models. For\nexample, unlike text-based programming there is no text-cursor\nat which to display code completion, thus creating a usability\nchallenge. Nevertheless, the inclusion of a code completion\nsystem into the SCRATCH user interface should be feasible,\nsince the blocks are already presented grouped by their type.\nA modiﬁcation of the interface could show the recommended\nblocks as such a group. However, the educational application\ndomain may also suggest the need for custom models that\ntake the education level into account; for example, code\ncompletion should not recommend blocks that require concepts\na learner is not yet aware of, which could be determined using\ncomputational thinking metrics for SCRATCH [46]. Furthermore,\nbeyond our initial bug ﬁnding task, we envision many possible\napplications of language models in the educational domain.\nTo support replication and future work, the source code of\nthe tokenizer and the language models, the datasets, analysis\nscripts, and the raw results can be found at\nhttps://doi.org/10.6084/m9.ﬁgshare.19382588.v1\nACKNOWLEDGEMENTS\nThis work is supported by BMWK (50RM2100B, “ANUKI”),\nBayerische Forschungsstiftung (AZ-1520-21, “DeepCode”),\nand DFG (FR2955/4-1, “STUNT”).\nREFERENCES\n[1] M. M. McGill and A. Decker, “Tools, languages, and environments\nused in primary and secondary computing education,” in Proceedings of\nthe 2020 ACM Conference on Innovation and Technology in Computer\nScience Education, ser. ITiCSE ’20.\nNew York, NY, USA: Association\nfor Computing Machinery, 2020, p. 103–109. [Online]. Available:\nhttps://doi.org/10.1145/3341525.3387365\n[2] D. Weintrop, D. C. Shepherd, P. Francis, and D. Franklin, “Blockly goes\nto work: Block-based programming for industrial robots,” in 2017 IEEE\nBlocks and Beyond Workshop (B&B).\nIEEE, 2017, pp. 29–36.\n[3] N. Ritschel, V. Kovalenko, R. Holmes, R. Garcia, and D. C. Shepherd,\n“Comparing block-based programming models for two-armed robots,”\nIEEE Transactions on Software Engineering, 2020.\n[4] C. Mayr-Dorn, M. Winterer, C. Salomon, D. Hohensinger, and\nR. Ramler, “Considerations for using block-based languages for\nindustrial robot programming - a case study,” in 3rd IEEE/ACM\nInternational Workshop on Robotics Software Engineering, RoSE@ICSE\n2021, Madrid, Spain, June 2, 2021.\nIEEE, 2021, pp. 5–12. [Online].\nAvailable: https://doi.org/10.1109/RoSE52553.2021.00008\n[5] J. Maloney, M. Resnick, N. Rusk, B. Silverman, and E. Eastmond,\n“The Scratch programming language and environment,” ACM Trans.\nComput. Educ., vol. 10, no. 4, pp. 16:1–16:15, 2010. [Online]. Available:\nhttps://doi.org/10.1145/1868358.1868363\n[6] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. T. Devanbu, “On the\nnaturalness of software,” in 34th International Conference on Software\nEngineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland, M. Glinz,\nG. C. Murphy, and M. Pezz`e, Eds.\nIEEE Computer Society, 2012, pp.\n837–847. [Online]. Available: https://doi.org/10.1109/ICSE.2012.6227135\n[7] V. Raychev, M. T. Vechev, and E. Yahav, “Code completion\nwith statistical language models,” in ACM SIGPLAN Conference\non Programming Language Design and Implementation, PLDI ’14,\nEdinburgh, United Kingdom - June 09 - 11, 2014, M. F. P. O’Boyle\nand K. Pingali, Eds.\nACM, 2014, pp. 419–428. [Online]. Available:\nhttps://doi.org/10.1145/2594291.2594321\n[8] S. Wang, D. Chollak, D. Movshovitz-Attias, and L. Tan, “Bugram:\nbug detection with n-gram language models,” in Proceedings of the\n31st IEEE/ACM International Conference on Automated Software\nEngineering, ASE 2016, Singapore, September 3-7, 2016, D. Lo, S. Apel,\nand S. Khurshid, Eds.\nACM, 2016, pp. 708–719. [Online]. Available:\nhttps://doi.org/10.1145/2970276.2970341\n[9] B. Ray, V. Hellendoorn, S. Godhane, Z. Tu, A. Bacchelli, and P. De-\nvanbu, “On the ‘naturalness’ of buggy code,” in 2016 IEEE/ACM 38th\nInternational Conference on Software Engineering (ICSE).\nIEEE, 2016,\npp. 428–439.\n[10] D. Bau, J. Gray, C. Kelleher, J. Sheldon, and F. A. Turbak, “Learnable\nprogramming: blocks and beyond,” Commun. ACM, vol. 60, no. 6, pp.\n72–80, 2017. [Online]. Available: https://doi.org/10.1145/3015455\n[11] M. Rahman, D. Palani, and P. C. Rigby, “Natural software revisited,” in\n2019 IEEE/ACM 41st International Conference on Software Engineering\n(ICSE), 2019, pp. 37–48.\n[12] F. Hermans, K. T. Stolee, and D. Hoepelman, “Smells in block-based\nprogramming languages,” in 2016 IEEE Symposium on Visual Languages\nand Human-Centric Computing (VL/HCC).\nIEEE, 2016, pp. 68–72.\n[Online]. Available: http://ieeexplore.ieee.org/document/7739666/\n[13] F. Hermans and E. Aivaloglou, “Do code smells hamper novice\nprogramming? A controlled experiment on Scratch programs,” in 2016\nIEEE 24th International Conference on Program Comprehension (ICPC),\nMay 2016, pp. 1–10.\n[14] P. Techapalokul and E. Tilevich, “Understanding recurring quality\nproblems and their impact on code sharing in block-based software,”\nin 2017 IEEE Symposium on Visual Languages and Human-Centric\nComputing (VL/HCC).\nIEEE, 2017, pp. 43–51. [Online]. Available:\nhttp://ieeexplore.ieee.org/document/8103449/\n[15] C. Fr¨adrich, F. Oberm¨uller, N. K¨orber, U. Heuer, and G. Fraser,\n“Common bugs in Scratch programs,” in Proceedings of the 2020\nACM Conference on Innovation and Technology in Computer Science\nEducation, ser. ITiCSE ’20.\nNew York, NY, USA: Association\nfor Computing Machinery, 2020, p. 89–95. [Online]. Available:\nhttps://doi.org/10.1145/3341525.3387389\n[16] G. Fraser, U. Heuer, N. K¨orber, F. Oberm¨uller, and E. Wasmeier,\n“LitterBox: A linter for Scratch programs,” in 2021 IEEE/ACM 43rd\nInternational Conference on Software Engineering: Software Engineering\nEducation and Training (ICSE-SEET), 2021, pp. 183–188.\n[17] P. Techapalokul and E. Tilevich, “Quality Hound — an online code smell\nanalyzer for Scratch programs,” in 2017 IEEE Symposium on Visual\nLanguages and Human-Centric Computing (VL/HCC), Oct 2017, pp.\n337–338.\n[18] D. E. Johnson, “ITCH: Individual testing of computer homework\nfor Scratch assignments,” in Proceedings of the 47th ACM Technical\nSymposium on Computing Science Education. ACM, 2016, pp. 223–227.\n[19] A. Stahlbauer, M. Kreis, and G. Fraser, “Testing Scratch programs\nautomatically,” in Proceedings of the 2019 27th ACM Joint Meeting\non European Software Engineering Conference and Symposium on the\nFoundations of Software Engineering, 2019, pp. 165–175.\n[20] S. F. Chen and J. Goodman, “An empirical study of smoothing techniques\nfor language modeling,” Comput. Speech Lang., vol. 13, no. 4, pp. 359–\n393, 1999. [Online]. Available: https://doi.org/10.1006/csla.1999.0128\n[21] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and\nT. Robinson, “One billion word benchmark for measuring progress in\nstatistical language modeling,” in INTERSPEECH 2014, 15th Annual\nConference of the International Speech Communication Association,\nSingapore, September 14-18, 2014, H. Li, H. M. Meng, B. Ma, E. Chng,\nand L. Xie, Eds.\nISCA, 2014, pp. 2635–2639. [Online]. Available:\nhttp://www.isca-speech.org/archive/interspeech 2014/i14 2635.html\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems, I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds.,\nvol. 30.\nCurran Associates, Inc., 2017.\n[23] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers).\nMinneapolis,\nMinnesota: Association for Computational Linguistics, Jun. 2019, pp.\n4171–4186. [Online]. Available: https://aclanthology.org/N19-1423\n[24] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou,\nB. Qin, T. Liu, D. Jiang, and M. Zhou, “CodeBERT: A pre-trained\nmodel for programming and natural languages,” in Findings of the\nAssociation for Computational Linguistics: EMNLP 2020.\nOnline:\nAssociation for Computational Linguistics, Nov. 2020, pp. 1536–1547.\n[Online]. Available: https://aclanthology.org/2020.ﬁndings-emnlp.139\n[25] M. Ciniselli, N. Cooper, L. Pascarella, D. Poshyvanyk, M. Di Penta,\nand G. Bavota, “An empirical study on the usage of BERT models for\ncode completion,” in 2021 IEEE/ACM 18th International Conference on\nMining Software Repositories (MSR).\nIEEE, May 2021, pp. 108–119.\n[Online]. Available: https://doi.org/10.1109/msr52588.2021.00024\n[26] N. Chirkova and S. Troshin, “Empirical study of transformers for\nsource code,” in ESEC/FSE ’21: 29th ACM Joint European Software\nEngineering Conference and Symposium on the Foundations of\nSoftware Engineering, Athens, Greece, August 23-28, 2021, D. Spinellis,\nG. Gousios, M. Chechik, and M. D. Penta, Eds.\nACM, 2021, pp.\n703–715. [Online]. Available: https://doi.org/10.1145/3468264.3468611\n[27] R. Degiovanni and M. Papadakis, “µbert: Mutation testing using pre-\ntrained language models,” in 2022 IEEE International Conference on\nSoftware Testing, Veriﬁcation and Validation Workshops (ICSTW), 2022,\npp. 160–169.\n[28] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, “Intellicode\ncompose: code generation using transformer,” in ESEC/FSE ’20:\n28th ACM Joint European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering, Virtual\nEvent, USA, November 8-13, 2020, P. Devanbu, M. B. Cohen, and\nT. Zimmermann, Eds.\nACM, 2020, pp. 1433–1443. [Online]. Available:\nhttps://doi.org/10.1145/3368089.3417058\n[29] T. T. Nguyen, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen, “A\nstatistical semantic language model for source code,” in Joint Meeting of\nthe European Software Engineering Conference and the ACM SIGSOFT\nSymposium on the Foundations of Software Engineering, ESEC/FSE’13,\nSaint Petersburg, Russian Federation, August 18-26, 2013, B. Meyer,\nL. Baresi, and M. Mezini, Eds.\nACM, 2013, pp. 532–542. [Online].\nAvailable: https://doi.org/10.1145/2491411.2491458\n[30] V. J. Hellendoorn and P. Devanbu, “Are deep neural networks the best\nchoice for modeling source code?” in Proceedings of the 2017 11th Joint\nMeeting on Foundations of Software Engineering, 2017, pp. 763–773.\n[31] C. Pan, M. Lu, and B. Xu, “An empirical study on software defect\nprediction using CodeBERT model,” Applied Sciences, vol. 11, no. 11,\n2021. [Online]. Available: https://www.mdpi.com/2076-3417/11/11/4793\n[32] E. Mashhadi and H. Hemmati, “Applying CodeBERT for automated pro-\ngram repair of Java simple bugs,” in 2021 IEEE/ACM 18th International\nConference on Mining Software Repositories (MSR), 2021, pp. 505–509.\n[33] B. Harvey, D. D. Garcia, T. Barnes, N. Titterton, D. Armendariz,\nL. Segars, E. Lemon, S. Morris, and J. Paley, “Snap!(build your own\nblocks),” in Proceeding of the 44th ACM technical symposium on\nComputer science education, 2013, pp. 759–759.\n[34] R. Karampatsis, H. Babii, R. Robbes, C. Sutton, and A. Janes, “Big\ncode != big vocabulary: open-vocabulary models for source code,” in\nICSE ’20: 42nd International Conference on Software Engineering,\nSeoul, South Korea, 27 June - 19 July, 2020, G. Rothermel and\nD. Bae, Eds.\nACM, 2020, pp. 1073–1085. [Online]. Available:\nhttps://doi.org/10.1145/3377811.3380342\n[35] H. Babii, A. Janes, and R. Robbes, “Modeling vocabulary for big code\nmachine learning,” arXiv preprint arXiv:1904.01873, 2019.\n[36] U. Z. Ahmed, P. Kumar, A. Karkare, P. Kar, and S. Gulwani, “Compilation\nerror repair: for the student programs, from the student programs,”\nin Proceedings of the 40th International Conference on Software\nEngineering: Software Engineering Education and Training, 2018, pp.\n78–87.\n[37] S. Xu, Y. Yao, F. Xu, T. Gu, H. Tong, and J. Lu, “Commit message\ngeneration for source code changes,” in IJCAI, 2019.\n[38] S. Kim, J. Zhao, Y. Tian, and S. Chandra, “Code prediction by feeding\ntrees to transformers,” in 2021 IEEE/ACM 43rd International Conference\non Software Engineering (ICSE), 2021, pp. 150–162.\n[39] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V. Stoyanov, “RoBERTa: A robustly optimized bert\npretraining approach,” 2019.\n[40] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen,\nC. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame,\nQ. Lhoest, and A. Rush, “Transformers: State-of-the-art natural language\nprocessing,” in Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System Demonstrations.\nAssociation for Computational Linguistics, Oct. 2020, pp. 38–45.\n[Online]. Available: https://doi.org/10.18653/v1/2020.emnlp-demos.6\n[41] H. B. Mann and D. R. Whitney, “On a test of whether one of two\nrandom variables is stochastically larger than the other,” The annals of\nmathematical statistics, pp. 50–60, 1947.\n[42] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,\nA. Svyatkovskiy, S. Fu, M. Tufano, S. K. Deng, C. Clement, D. Drain,\nN. Sundaresan, J. Yin, D. Jiang, and M. Zhou, “GraphCodeBERT:\nPre-training code representations with data ﬂow,” 2020. [Online].\nAvailable: https://arxiv.org/abs/2009.08366\n[43] J. Li, Y. Wang, M. R. Lyu, and I. King, “Code completion with neural\nattention and pointer networks,” in Proceedings of the 27th International\nJoint Conference on Artiﬁcial Intelligence, ser. IJCAI’18.\nAAAI Press,\n2018, p. 4159–25.\n[44] F. Liu, G. Li, Y. Zhao, and Z. Jin, “Multi-task learning based pre-\ntrained language model for code completion,” in Proceedings of the 35th\nIEEE/ACM International Conference on Automated Software Engineering,\n2020, pp. 473–485.\n[45] M. Asaduzzaman, C. K. Roy, K. A. Schneider, and D. Hou, “Context-\nsensitive code completion tool for better API usability,” in 2014 IEEE\nInternational Conference on Software Maintenance and Evolution. IEEE,\n2014, pp. 621–624.\n[46] J. Moreno-Le´on, G. Robles, and M. Rom´an-Gonz´alez, “Dr. Scratch:\nAutomatic analysis of scratch projects to assess and foster computational\nthinking,” RED. Revista de Educaci´on a Distancia, no. 46, pp. 1–23,\n2015.\n",
  "categories": [
    "cs.PL",
    "cs.SE",
    "68-04",
    "D.2.5; D.2.3"
  ],
  "published": "2023-02-08",
  "updated": "2023-02-08"
}