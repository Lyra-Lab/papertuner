{
  "id": "http://arxiv.org/abs/2302.03954v1",
  "title": "Temporal Video-Language Alignment Network for Reward Shaping in Reinforcement Learning",
  "authors": [
    "Ziyuan Cao",
    "Reshma Anugundanahalli Ramachandra",
    "Kelin Yu"
  ],
  "abstract": "Designing appropriate reward functions for Reinforcement Learning (RL)\napproaches has been a significant problem, especially for complex environments\nsuch as Atari games. Utilizing natural language instructions to provide\nintermediate rewards to RL agents in a process known as reward shaping can help\nthe agent in reaching the goal state faster. In this work, we propose a natural\nlanguage-based reward shaping approach that maps trajectories from the\nMontezuma's Revenge game environment to corresponding natural language\ninstructions using an extension of the LanguagE-Action Reward Network (LEARN)\nframework. These trajectory-language mappings are further used to generate\nintermediate rewards which are integrated into reward functions that can be\nutilized to learn an optimal policy for any standard RL algorithms. For a set\nof 15 tasks from Atari's Montezuma's Revenge game, the Ext-LEARN approach leads\nto the successful completion of tasks more often on average than the reward\nshaping approach that uses the LEARN framework and performs even better than\nthe reward shaping framework without natural language-based rewards.",
  "text": " \n \n \n \nTemporal Video-Language Alignment Network  \nfor Reward Shaping in Reinforcement Learning \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nKeywords—Reinforcement Learning, Natural Language, Reward Shaping, Markov Decision Process, Language-aided Reinforcement \nAbstract— Designing appropriate reward functions for Reinforcement Learning (RL) approaches has been a significant problem, \nespecially for complex environments such as Atari games. Utilizing natural language instructions to provide intermediate rewards to \nRL agents in a process known as reward shaping can help the agent in reaching the goal state faster. In this work, we propose a \nnatural language-based reward shaping approach that maps trajectories from the Montezuma’s Revenge game environment to \ncorresponding natural language instructions using an extension of the LanguagE-Action Reward Network (LEARN) framework.\nThese trajectory-language mappings are further used to generate intermediate rewards which are integrated into reward functions \nthat can be utilized to learn an optimal policy for any standard RL algorithms. For a set of 15 tasks from Atari’s Montezuma’s \nRevenge game, the Ext-LEARN approach leads to the successful completion of tasks more often on average than the reward shaping \napproach that uses the LEARN framework and performs even better than the reward shaping framework without natural language-\nbased rewards\nZiyuan Cao\nReshma Anugundanahalli\nRamachandra\nKelin Yu\ncoliny@gatech.edu\n. \nLearning \nI. INTRODUCTION \nReinforcement Learning (RL) has been used extensively in games with promising results in terms of performance. RL \nalgorithms utilize the concepts of rewards (positive or negative reinforcements) to help agents in learning to optimize their \nbehaviors in an environment with which they interact. Subsequently, RL has been successfully implemented in complex \nenvironments such as Atari games [1]. However, defining reward functions has been the most significant challenge in RL, \nespecially while scaling RL algorithms for real-world applications with large state spaces. Rewards must be defined accurately \nand efficiently to clearly represent the tasks to be completed by the agent. There have been several attempts to combat the \nproblem of defining reward functions, one of which is to define sparse rewards such as providing positive rewards for reaching \nthe goal state while the agent receives no rewards if it fails to reach the goal state [2]. This could be an extreme solution that \nwould lead to slow and difficult learning. On the other hand, rewards could be specified in a dense manner such as the running \nscore in an Atari game [3]. Although such rewards are easier to learn from since the agent can randomly take series of actions \nto easily find rewards, it would be difficult and time-consuming to specify rewards in such detail. Thus, specifying intermediate \nrewards designed to help the agent in reaching the goal state could be a solution to defining rewards. This process is known as \nreward shaping [4].  \n \n \n \nFig. 1 An RL agent in the Montezuma’s Revenge Atari game environment meant to follow the blue trajectory in order to reach the golden key. \nFig. 1 shows an instance of an RL agent interacting with the complex Atari game environment, Montezuma’s Revenge. If \nthe goal is to obtain the golden key, the intermediate steps include moving to the left, jumping over the skull, and moving \ntowards the key. If the agent is only rewarded once it reaches the goal state, as is the case while defining sparse rewards, then \nthe agent wastes a lot of time exploring the environment while it learns to reach the goal state. Thus, providing intermediate \nrewards that advance the agent towards the goal state or reward shaping could help in saving exploration time and reduce the \nnumber of agent’s interactions with the environment. Various approaches have been used for rewards shaping in RL [5][6]. \nHowever, translating human inputs to real-valued rewards could be challenging due to cognitive bias and requirement of \nprogramming experts who can accurately map instructions to actions in the trajectories. Designing or accurately specifying \nintermediate rewards is another problem that is quite difficult to solve for people not well-versed with the gaming environment \nor strategies.  \n \nNatural language has been shown to be successful in communicating with agents powered by reinforcement learning and \nimitation learning [7]. Utilizing natural language for reward shaping helps in designing dense rewards. For tasks as seen in the \nexample in Fig. 1, providing instructions in natural language in order to design intermediate rewards is seen to improve the \nperformance of RL agents [8]. When the agent is given instructions such as “Jump over the skull while going to the left”, \nlearning is faster, and the agent completes the task 60% more often on average when compared to learning without using natural \nlanguage-based reward shaping. Since non-experts can also provide natural language instructions, specifying rewards or \ndescribing the task environment becomes more straight-forwards and novel skills can be taught to RL agents in an easier and \nconvenient manner. However, this approach has several drawbacks. \n \n \n \nApproaches that exploit natural language instructions for reward shaping train a language reward model that predicts how \na piece of trajectory matches with a language command using a collected dataset of <instruction, trajectory> pairs. However, \nthis approach requires a separate supervised training phase for grounding the language commands in the environment in which \nthe agent will be deployed. For example, the instruction “Jump over the snake” needs grounding of object “snake” to the \ncorresponding pixels in the image of the current state and “jump” must be mapped to the appropriate action in the action space. \nCollection of thousands of <instruction, trajectory> pairs is needed. This makes the design hard to scale in terms of generalizing \nto other environments. Another drawback is that an oversimplified encoding strategy of trajectories, which ignores temporal \nand state information, is utilized as the input to the language reward model. A better encoder of trajectories may provide more \nprecise reward shaping. Thus, this work proposes improvements to these previous natural language-based reward shaping \ntechniques in order to resolve these limitations and improve natural-language-instructions-to-reward mapping and, in turn, the \nperformance of the RL agent. \n \nThe proposed approach for natural language-based reward shaping first obtains the word embeddings of the natural \nlanguage instructions corresponding to the trajectories taken from the Atari Grand Challenge dataset [9]. Bidirectional Encoder \nRepresentations from Transformers (BERT) [10] is utilized to obtain the word embeddings since it captures the underlying \ncontext of the natural language instructions, especially in cases where the given instructions are with respect to time or space. \nFurther, the proposed approach considers the entire trajectory as complete action sequences while previous approaches \naggregated the sequence of past actions into an action-frequency vector. Additionally, an alignment model is also proposed to \nmap the entire trajectory frames to the corresponding language descriptions. This helps in comparing and validating the \ntrajectory and corresponding language descriptions generated before using them to design the reward function.  \n \nThe main contributions of this work include: \ni. \nGeneration of natural language instructions using BERT to obtain word embeddings of the instructions. \nii. \nImplementation of an alignment model that maps the entire trajectory to the corresponding language description \nin order to discriminate between matched and unmatched trajectory-language pairs. \n \nThe rest of this paper is divided as follows: Section II includes a brief summary of the relevant related works, Section III \ndetails the proposed language-based reward shaping model and the language-aided RL model. Section IV details the \nexperimental setup, Section V presents the results while Sections VII and VIII discuss the results and conclusions of the work \nrespectively. \n \nII. RELATED WORKS \n \nThe intersection of natural language with RL algorithms has been explored in multiple ways. On one hand, popular Natural \nLanguage Processing (NLP) tasks such as question-answering and text generation have been solved using RL [11][12]. In such \ntasks, for example, with the task of summarization, training based on supervised learning suffers from the inherent assumption \nthat the ground truth is available for every training step otherwise known as exposure bias. Thus, combining RL’s global \nsequence prediction training with the standard word prediction helps improve the performance on NLP tasks [13]. With respect \nto learning an optimal policy using RL algorithms, reward functions must be defined accurately. For environments that provide \ndense reward signals such as the game scores in Atari game environments, an agent policy can be trained such that it outperforms \nhuman players for most of the games [14]. However, in the environment considered in this work, games like Montezuma’s \nRevenge provide sparse reward signals, making it difficult to learn an optimal policy. Environments with sparse reward signals \nalso demand more data and resources [15]. Thus, reward shaping is used to augment the reward function with inputs from \nhuman observers in order to increase the number of rewards which, in turn, speeds up the policy learning process [16]. Human \ninputs can be given in terms of task demonstrations [17], just good or bad binary type of inputs as in TAMER [18] or through \nnatural language [19][20]. The problem with the first two types of human feedback is the fact that it requires expert programmers \nwho can map optimal demonstrations and also know the right action to take at every point in the game. Utilizing natural language, \non the other hand, can help even non-experts provide feedback. \n \nUsing natural language-based instructions in learning RL policies has been explored quite extensively in the past. From \nadventure games based on language [21] to tasks involving robots avoiding obstacles or navigation [19][20], natural language \nhas been exploited for providing instructions. [22] proposes a language-action reward network which implements a reward \nfunction by grounding language commands in the environment. Another approach included data obtained from human teachers \ndescribing in detail the tasks that they were performing [23]. The descriptions provided by the teachers were mapped to a reward \nfunction which was used to train the RL agent to learn policies. [24] is another similar approach, more in-line with our work, \nwhere natural language narration was used for reward shaping for tasks in the Montezuma’s Revenge game environment. The \nresults showed that the approach outperformed methods without reward shaping.  \n \nSeveral works have explored the problem of how RL agents learn to follow natural language instructions. [25] combines \nthe state information with the natural language description of the goal to learn policies. Reward shaping is performed based on \nthe agent’s distance from reference trajectories and from the goal. Another straightforward approach to using natural language \nfor reward shaping can be seen in [26], which describes rules using natural language to decide whether to choose an action. \n[27][28] also perform language-to-reward mapping by designing rewards based on if the agent arrives at the right location or \nnot by following the instruction provided. These approaches still face the problem of efficiently defining reward functions since \nit requires an expert programmer who can make decisions about how language instructions are mapped to the environment.  \n \nHowever, all these approaches are difficult to scale to more complex environments since scaling the simple rule-based \nfeatures would require a lot of engineering for different instances of the tasks. Describing instructions using natural language \nand mapping them directly to reward functions, as done in our approach, could be more convenient and scalable. The proposed \napproach solves a language conditional RL task utilizing NLP methods for assistive learning, thus combining the two ways in \nwhich RL and NLP have been combined previously [7].  \n \nApproaches such as [29][30] have also explored specifying subgoals using natural language instructions in order to improve \nhow the RL agent follows the instruction specified using natural language. Our approach, on the other hand, focuses on \nimproving RL algorithms using natural language instructions. [31] more recently proposes a language-conditioned offline \nreward learning approach that learns language-conditioned rewards effectively from annotations obtained through robot \ninteractions with the environment. To solve the problem of grounding language in the robot’s observation space, datasets \nobtained offline are leveraged to learn whether the language instruction is completed when there is a change in the state for \noffline multi-task RL. However, language-conditioned instruction-following RL agents in such approaches utilize action spaces \nwhich are simple and parameterized. Once the instructions are provided, they are fixed over the entire execution of the agent \nwith little opportunity provided for subsequent interactions by the instructor.  \n \nPrevious works have also explored utilizing features from the instruction-reward mappings in action-value functions [32]. \nMost similar to our work is the LEARN model introduced in [8].  LEARN or LanguagE-Action Reward Network trains a neural \nnetwork to take as input a trajectory-instruction pair and produce as output the prediction probabilities of how well the language \ninstruction describes the actions taken in the trajectory. These probabilities are used to design the reward function that is used \nto train the RL agent. Possible drawbacks with such an approach include temporal ordering, state-based rewards and multi-step \ninstructions. LEARN only considers past actions while forming its action-frequency vector instead of considering the complete \naction sequence as is done in our approach. For example, for an instruction such as “Jump over the skull while going to the \nleft”, intermediate language-based rewards are designed by considering trajectories with higher frequencies associated with the \n“jump” and “left” actions. The drawback of such an approach is that it completely ignores temporal and state information. The \ninstruction “Jump over the skull while going to the left” provides a significant amount of important information as to where the \nagent should jump towards and when should it make the jump. Jumping left is different from jumping towards left while \navoiding the skull. Such an approach would not utilize the state information provided by instructions such as “Go towards the \nkey” or “Avoid the pit”. Evidently, the reward functions defined using the language instructions could be modelled as a function \nof both the past actions and states to improve the learning. Thus, our approach utilizes a contextual language-reward mapping \nthat considers information provided by the entire instruction rather than taking actions based on only high-frequency action \nwords.  \n \nIII. PROPOSED APPROACH \nA. Dataset and Preprocessing \nThe RL agent was trained on the dataset obtained from [8]. This data consists of 20 trajectories from human gameplays of \nthe Montezuma’s Revenge game obtained from the Atari Grand Challenge dataset [9]. 2,708 three-second-long frames were \nextracted from these trajectories to generate 6,870 language descriptions using Amazon Mechanical Turk (see supplementary \nmaterial for examples of language annotations). These language annotations are instructions for the agent to follow in order to \nreach the goal state from the start state.  \n \nFor the model to understand the natural language of annotations for each of those trajectories, BERT was used to obtain \nword embeddings of the language instructions. A transformer model such as BERT is used instead of pre-trained embeddings \nor RNN with GloVe [33] to obtain the word embeddings for these natural language instructions. BERT is able to capture the \nunderlying context of the natural language instructions, especially in cases where the given instructions are with respect to time \nor space. Thus, the proposed approach considers the entire trajectory as complete action sequences while previous approaches \naggregated the sequence of past actions into an action-frequency vector, thereby losing temporal and state information.  \n \nB. Model \nWe consider an augmented version of a Markov Decision Process (MDP) in RL, defined by 𝑀𝐷𝑃′: ⟨𝑆, 𝐴,𝑇, 𝑅, γ, l⟩ where 𝑆 \ndenotes set of states, 𝐴 is the set of actions, 𝑇: 𝑆× 𝐴× 𝑆→[0,1] is the transition probabilities of going from current state 𝑠𝑡∈\n𝑆 to state 𝑠𝑡+1 ∈𝑆 by taking action 𝑎𝑡∈𝐴. The reward function 𝑅: 𝑆× 𝐴→ℝ maps current state-action (𝑠𝑡,𝑎𝑡) pair to \ncorresponding real-valued rewards, 𝛾 is the discount factor. The extension to this MDP is provided by including the natural \nlanguage instruction 𝑙. RL uses this 𝑀𝐷𝑃′ in order to learn an optimal policy π∗ that maximizes the expected sum of rewards. \nThis process of learning an optimal policy is two-fold involving the Ext-LEARN framework and using the language instructions \nfor reward shaping in RL.  \n \nThe extended version of LanguagE-Action Reward Network (LEARN) uses the entire trajectory τ and the corresponding \nlanguage instruction 𝑙 obtained from data preprocessing (Sub-section A) i.e., the pair (𝜏, 𝑙) as input to train a neural network to \npredict if the language accurately describes the actions taken in that particular trajectory. To train the Ext-LEARN model, we \nneed both positive and negative examples of trajectory-language pairs. In a positive example, the language correctly describes \nthe trajectory. In a negative example, the language does not describe the trajectory. The positive examples come directly from \nthe dataset manually collected in [8]. To sample negative examples, we randomly sample a language instruction which describes \na different section of the same trajectory. This results in 12K pairs consisting of the same number of positive and negative \nexamples. \n  \n \nFig. 2 Overview of proposed approach. \n \nA diagram of the proposed Ext-LEARN model is shown in Fig 2.  Ext-LEARN model takes a trajectory-language pair as \nthe input. The trajectory is represented by a sequence of frames of the UI of the Montezuma’s Revenge game. As provided in \nthe dataset collected in [8], each trajectory consists of 150 frames. For each trajectory, we extracted 15 evenly spaced frames \nfrom the 150 frames. These extracted frames are fed into a pretrained image encoder to generate frame feature vectors. The \nlanguage instruction is fed into a pretrained language encoder to generate language feature vectors. Later, both sets of feature \nvectors go through a Multi-Layer Perceptron (MLP) to match the feature sizes. Both the sets of transformed feature vectors are \nfed into a separate transformer encoder. Average pooling is applied on both sets of vectors, generating two fixed-size vectors \nrepresenting the trajectory and the language command respectively. To predict whether they match or not, the two vectors are \nconcatenated and fed into another MLP which in the end produces a scalar. Intuitively, through minimizing the cross-entropy \nloss, the two transformer encoders are encouraged to learn how the frames and the language command align to each other. Ir \nall the training in our experiments, the parameters of the image encoder and the language encoder are kept frozen.  \n \nThe output of the Ext-LEARN framework which is the prediction probabilities are utilized to design intermediate rewards. \nThe reward function is modelled as a function of both the past states and actions. \n \nIV. EXPERIMENTAL EVALUATION \nSimilar to [8], we conducted experiments on the Atari game Montezuma’s Revenge. In this game, the agent needs to \nnavigate around different rooms with several obstacles such as ladders, ropes, doors, enemy objects, etc. With many objects \nand interactions, this game serves as a good environment to train an RL agent. Results from [8] serve as the baseline to compare \nthe performance of our model.  \n \nOur model is trained and tested with the dataset obtained from [8]. Level 1 of Montezuma’s revenge consists of 24 rooms. \n14 are used for training, and 10 for validation and testing. Although objects remain the same across training and test/validation \ndatasets, each room consists of only a subset of the objects arranged in various layouts. The training dataset is created with \n160,000 (frame, language) pairs from the training set, and a validation dataset with 40,000 pairs from the validation set. A set \nof 15 tasks which involve the agent going from a particular start state to a fixed goal state across multiple rooms are considered. \nThe agent interacts with various objects from a fixed set of objects during this period and for each task, on reaching the goal \nstate, the agent receives a +1 reward from the environment and zero otherwise.  \n \nExperiments \nIn this experiment, we compare our new framework with two previous frameworks:  \n1. Ext-only: No language-based reward is used. Only a standard MDP with the original environment reward is \nconsidered.  \n2. Ext+Lang: The environment reward for successful completion of the task is combined with the language-based \nreward provided at each step. \n3. New approach: The language reward from the proposed alignment model is used along with the environment \nreward.  \n \nThe following metric are used to evaluate the performance of the model: \n1.  AUC: From each training, the number of timesteps is plotted against the number of successful episodes. The area \nunder the curve (AUC) denotes how quickly the agent learns, and is used as the metric to compare two policy training \nruns. \n2. Final Policy: To compare the final learned policy with ExtOnly, Ext+Lang and the new approach, policy evaluation is \nperformed at the end of 500,000 training steps. For each policy training run, we use the learned policy for an additional \n10,000 timesteps without updating it, and record the number of successful episodes. \n \nV. RESULTS \nFrom the 10 policy learning test sets, we performed 10 policy learning runs for each set and each description. Fig. 3 below \nshows that our new policy is slightly faster than the Ext-learn and much faster than the Ext algorithm. It means that combining \nframes and languages together with complete action sequences is useful. The graphs show that the average number of successful \nepisodes is around 1500 for the Ext-Lang algorithm, and this value is around 1760 for our algorithm. Also, our framework can \nreach 1500 successful episodes at around 420,000 time stamp which shows that it is around 15% faster than the Ext-learn \nalgorithm. \n \nAnalysis of every test set was also performed. The correlation rate (variance) of each dataset is compared with the results \nin [8]. Fig. 4 and Fig. 5 shows the performance of the ExtOnly, Ext-Lang and proposed approach for task 4 and task 6. The \nproposed approach has a slightly higher success rate than the other baseline algorithms. Our algorithm has a small variance for \nsimple tasks like task 4 and a large variance for more complex tasks such as task 6. Thus, our algorithm learns much faster than \nExt-Lang and ExtOnly with average number of successful episodes about 1750 after 500,000 timesteps, while Ext+Lang is only \nat 1500 at that timestamp, which amounts to a 15% speed-up. Alternately, after 500,000 timesteps, ExtOnly is at around 800 \naverage successful episodes, thereby giving a 90% relative improvement. \n \n \n \nFig. 3: Comparisons of the reward functions. (left) Solid lines denote the mean successful episodes averaged over all 15 tasks which reaches around 1500 for \nExt-Lang and about 850 for ExtOnly. The shaded regions represent 95% confidence intervals. (right) The mean successful episodes averaged over all 15 \ntasks for the proposed model reaches 1760, outperforming ExtOnly and Ext-Lang. \n \n \n \n \nFig. 4: Comparisons of the reward functions for task 4. (left) Solid lines denote the number of successful episodes for task 4 which reaches almost 12,00 for \nExt-Lang and about 6000 for ExtOnly. The shaded regions represent 95% confidence intervals. (right) The number of successful episodes for task 4 for the \nproposed model reaches 14,000, outperforming ExtOnly and Ext-Lang. \n \n \n \n \nFig. 5: Comparisons of the reward functions for a more complicated task 6. (left) Solid lines denote the number of successful episodes for task 6 which \nreaches almost 90 for Ext-Lang. The shaded region represents 95% confidence intervals. (right) The number of successful episodes for task 6 for the \nproposed model reaches 120, outperforming ExtOnly and Ext-Lang. \n \nVI. \nDISCUSSION \nWhile utilizing natural language instructions for reward shaping has its benefits, there are some drawbacks to natural \nlanguage-based rewards in RL. Firstly, grounding objects or actions within the environment using natural language proves to \nbe challenging. For example, the instruction “Jump over the snake” needs grounding of object “snake” to the corresponding \npixels in the image of the current state and “jump” must be mapped to the appropriate action in the action space. Additionally, \nnatural language instructions could be ambiguous. “Jump over the snake” does not clearly specify the direction in which the \nagent should jump, leftward or rightward. Natural language also could be incomplete. The agent might have to move first in a \ncertain direction before jumping over the snake. Synonyms of a word could be used to specify the same action (“move left” or \n“go left”).  \n \n𝑀𝐷𝑃′ considered includes only one-step language instruction 𝑙, while real-world scenarios usually involve multi-step \ninstructions. A possible way to extend this work to handle multi-step instructions could be to determine if the language \ninstruction is actually complete using another neural network-based or heuristic-based model. Then the prediction could be \nconcatenated with the proposed model such that the model transitions to the new state only if the current instruction is completed. \n \nThe alignment model achieves an accuracy of 67% accuracy, whereas the LEARN model reached around 82%. This \ndifference could be attributed to the separate supervised training process performed for grounding natural language commands \nin the environment in which the agent is deployed. Thus, the Ext-Learn model could be improved to provide better descriptions \nof the game trajectories which could lead to faster learning. \n \nVII. CONCLUSIONS  \nA natural language-based reward shaping model, which is an extension of the LEARN model, is proposed. Entire trajectories \nfrom the dataset are mapped to the corresponding language instructions representations obtained through BERT. This (trajectory, \ninstruction) pair is fed into the Ext-LEARN model to predict if the instruction accurately describes the actions taken in the \ntrajectory. This prediction probability is used to generate intermediate rewards in the language-aided RL model. The \nexperiments performed show that using natural language-based rewards improves the policy learned and the time taken for the \ntraining process. Further, using a contextual word-embedding model such as BERT to generate vector representations of the \nnatural language instructions and modelling the reward function based on complete action sequences improves the performance \nof the RL agent as compared to using GloVe embeddings with reward functions based only on past actions aggregated into \naction-frequency vectors.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nREFERENCES\n[1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D. and Riedmiller, M., 2013. Playing atari with deep reinforcement \nlearning. arXiv preprint arXiv:1312.5602.\n[2] Vecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess, N., Rothörl, T., Lampe, T. and Riedmiller, M., 2017. Leveraging demonstrations \nfor deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817.\n[3] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G. and Petersen, \nS., 2015. Human-level control through deep reinforcement learning. nature, 518(7540), pp.529-533.\n[4] Ng, A.Y., Harada, D. and Russell, S., 1999, June. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml (Vol. \n99, pp. 278-287).\n[5] Hu, Y., Wang, W., Jia, H., Wang, Y., Chen, Y., Hao, J., Wu, F. and Fan, C., 2020. Learning to utilize shaping rewards: A new approach of reward \nshaping. Advances in Neural Information Processing Systems, 33, pp.15931-15941.\n[6] Grzes, M. and Kudenko, D., 2008, September. Plan-based reward shaping for reinforcement learning. In 2008 4th International IEEE Conference Intelligent \nSystems (Vol. 2, pp. 10-22). IEEE.\n[7] Luketina, J., Nardelli, N., Farquhar, G., Foerster, J., Andreas, J., Grefenstette, E., Whiteson, S. and Rocktäschel, T., 2019. A survey of reinforcement \nlearning informed by natural language. arXiv preprint arXiv:1906.03926.\n[8] Goyal, P., Niekum, S. and Mooney, R.J., 2019. Using natural language for reward shaping in reinforcement learning. arXiv preprint arXiv:1903.02020.\n[9] Kurin, V., Nowozin, S., Hofmann, K., Beyer, L. and Leibe, B., 2017. The atari grand challenge dataset. arXiv preprint arXiv:1705.10998.\n[10] Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint \narXiv:1810.04805.\n[11] Xiong, C., Zhong, V. and Socher, R., 2017. Dcn+: Mixed objective and deep residual coattention for question answering. arXiv preprint arXiv:1711.00106.\n[12] Li, J., Monroe, W., Ritter, A., Galley, M., Gao, J. and Jurafsky, D., 2016. Deep reinforcement learning for dialogue generation. arXiv preprint \narXiv:1606.01541.\n[13] Paulus, R., Xiong, C. and Socher, R., 2017. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304.\n[14] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G. and Petersen, \nS., 2015. Human-level control through deep reinforcement learning. nature, 518(7540), pp.529-533.\n[15] Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M. and \nDieleman, S., 2016. Mastering the game of Go with deep neural networks and tree search. nature, 529(7587), pp.484-489.\n[16] Dorigo, M. and Colombetti, M., 1998. Robot shaping: an experiment in behavior engineering. MIT press.\n[17] Argall, B.D., Chernova, S., Veloso, M. and Browning, B., 2009. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5), \npp.469-483.\n[18] Warnell, G., Waytowich, N., Lawhern, V. and Stone, P., 2018, April. Deep tamer: Interactive agent shaping in high-dimensional state spaces. \nIn Proceedings of the AAAI conference on artificial intelligence (Vol. 32, No. 1).\n[19] Shah, P., Fiser, M., Faust, A., Kew, J.C. and Hakkani-Tur, D., 2018. Follownet: Robot navigation by following natural language directions with deep \nreinforcement learning. arXiv preprint arXiv:1805.06150.\n[20] Blukis, V., Brukhim, N., Bennett, A., Knepper, R.A. and Artzi, Y., 2018. Following high-level navigation instructions on a simulated quadcopter with \nimitation learning. arXiv preprint arXiv:1806.00047.\n[21] He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L. and Ostendorf, M., 2015. Deep reinforcement learning with a natural language action space. arXiv preprint \narXiv:1511.04636.\n[22] Fu, J., Korattikara, A., Levine, S. and Guadarrama, S., 2019. From language to goals: Inverse reinforcement learning for vision-based instruction \nfollowing. arXiv preprint arXiv:1902.07742.\n[23] Tung, H.Y., Harley, A.W., Huang, L.K. and Fragkiadaki, K., 2018. Reward learning from narrated demonstrations. In Proceedings of the IEEE Conference \non Computer Vision and Pattern Recognition (pp. 7004-7013).\n[24] Kaplan, R., Sauer, C. and Sosa, A., 2017. Beating atari with natural language guided reinforcement learning. arXiv preprint arXiv:1704.05539.\n[25] Misra, D., Langford, J. and Artzi, Y., 2017. Mapping instructions and visual observations to actions with reinforcement learning. arXiv preprint \narXiv:1704.08795.\n[26] Kuhlmann, G., Stone, P., Mooney, R. and Shavlik, J., 2004, July. Guiding a reinforcement learner with natural language advice: Initial results in RoboCup \nsoccer. In The AAAI-2004 workshop on supervisory control of learning and adaptive systems.\n[27] Branavan, S.R.K., Silver, D. and Barzilay, R., 2012. Learning to win by reading manuals in a monte-carlo framework. Journal of Artificial Intelligence \nResearch, 43, pp.661-704.\n[28] Bahdanau, Dzmitry, Felix Hill, Jan Leike, Edward Hughes, Arian Hosseini, Pushmeet Kohli, and Edward Grefenstette. \"Learning to understand goal \nspecifications by modelling reward.\" arXiv preprint arXiv:1806.01946 (2018).\n[29] Kaplan, R., Sauer, C. and Sosa, A., 2017. Beating atari with natural language guided reinforcement learning. arXiv preprint arXiv:1704.05539.\n[30] Branavan, S.R.K., Kushman, N., Lei, T. and Barzilay, R., 2012. Learning high-level planning from text. The Association for Computational Linguistics.\n[31] Nair, S., Mitchell, E., Chen, K., Savarese, S. and Finn, C., 2022, January. Learning language-conditioned robot behavior from offline data and crowd-\nsourced annotation. In Conference on Robot Learning (pp. 1303-1315). PMLR.\n[32] Wang, X., Huang, Q., Celikyilmaz, A., Gao, J., Shen, D., Wang, Y.F., Wang, W.Y. and Zhang, L., 2019. Reinforced cross-modal matching and self-\nsupervised imitation learning for vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. \n6629-6638).\n[33] Pennington, J., Socher, R. and Manning, C.D., 2014, October. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on \nempirical methods in natural language processing (EMNLP) (pp. 1532-1543).\n \n \n \n \n \n \n \n \nA. Natural Language Instructions\nTable 1 provides some examples of natural language instructions collected from Amazon Mechanical Turk. These \ninstructions do include variations in terms of vocabulary and length. There are spelling errors and semantically incomplete \nsentences. The model is still able to extract relevant information from such instructions. \nTable 1: Natural Language Instructions corresponding the trajectories in the frames of the dataset.\nSl. No.\nLanguage Instructions\n \n \n1\nwalk to the bars\n \n \n2\nclimb up of the laddar and go the left\n \n \n3\ngo to upstaird\n \n \n4\nRUN STRIGHT TOWRADS LADDER\n \n \n5\ntwo jumb while going left  and take someone,then go left and \nright\n \n \n \n \n \n \n. \n \n \n \n \n \n \nAPPENDIX\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2023-02-08",
  "updated": "2023-02-08"
}