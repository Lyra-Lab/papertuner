{
  "id": "http://arxiv.org/abs/2302.04793v1",
  "title": "AI-based Question Answering Assistance for Analyzing Natural-language Requirements",
  "authors": [
    "Saad Ezzini",
    "Sallam Abualhaija",
    "Chetan Arora",
    "Mehrdad Sabetzadeh"
  ],
  "abstract": "By virtue of being prevalently written in natural language (NL), requirements\nare prone to various defects, e.g., inconsistency and incompleteness. As such,\nrequirements are frequently subject to quality assurance processes. These\nprocesses, when carried out entirely manually, are tedious and may further\noverlook important quality issues due to time and budget pressures. In this\npaper, we propose QAssist -- a question-answering (QA) approach that provides\nautomated assistance to stakeholders, including requirements engineers, during\nthe analysis of NL requirements. Posing a question and getting an instant\nanswer is beneficial in various quality-assurance scenarios, e.g.,\nincompleteness detection. Answering requirements-related questions\nautomatically is challenging since the scope of the search for answers can go\nbeyond the given requirements specification. To that end, QAssist provides\nsupport for mining external domain-knowledge resources. Our work is one of the\nfirst initiatives to bring together QA and external domain knowledge for\naddressing requirements engineering challenges. We evaluate QAssist on a\ndataset covering three application domains and containing a total of 387\nquestion-answer pairs. We experiment with state-of-the-art QA methods, based\nprimarily on recent large-scale language models. In our empirical study,\nQAssist localizes the answer to a question to three passages within the\nrequirements specification and within the external domain-knowledge resource\nwith an average recall of 90.1% and 96.5%, respectively. QAssist extracts the\nactual answer to the posed question with an average accuracy of 84.2%.\n  Keywords: Natural-language Requirements, Question Answering (QA), Language\nModels, Natural Language Processing (NLP), Natural Language Generation (NLG),\nBERT, T5.",
  "text": "AI-based Question Answering Assistance for\nAnalyzing Natural-language Requirements\nSaad Ezzini∗, Sallam Abualhaija∗, Chetan Arora‡§, Mehrdad Sabetzadeh†\n∗SnT Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg\n‡Deakin University, Geelong, Australia\n§Monash University, Victoria, Australia\n†School of Electrical Engineering and Computer Science, University of Ottawa, Canada\nEmail: {saad.ezzini, sallam.abualhaija}@uni.lu, chetan.arora@monash.edu, m.sabetzadeh@uottawa.ca\nAbstract—By virtue of being prevalently written in natural\nlanguage (NL), requirements are prone to various defects, e.g.,\ninconsistency and incompleteness. As such, requirements are\nfrequently subject to quality assurance processes. These pro-\ncesses, when carried out entirely manually, are tedious and\nmay further overlook important quality issues due to time and\nbudget pressures. In this paper, we propose QAssist – a question-\nanswering (QA) approach that provides automated assistance\nto stakeholders, including requirements engineers, during the\nanalysis of NL requirements. Posing a question and getting an\ninstant answer is beneﬁcial in various quality-assurance sce-\nnarios, e.g., incompleteness detection. Answering requirements-\nrelated questions automatically is challenging since the scope of\nthe search for answers can go beyond the given requirements\nspeciﬁcation. To that end, QAssist provides support for mining\nexternal domain-knowledge resources. Our work is one of the ﬁrst\ninitiatives to bring together QA and external domain knowledge\nfor addressing requirements engineering challenges. We evaluate\nQAssist on a dataset covering three application domains and\ncontaining a total of 387 question-answer pairs. We experiment\nwith state-of-the-art QA methods, based primarily on recent\nlarge-scale language models. In our empirical study, QAssist\nlocalizes the answer to a question to three passages within\nthe requirements speciﬁcation and within the external domain-\nknowledge resource with an average recall of 90.1% and 96.5%,\nrespectively. QAssist extracts the actual answer to the posed\nquestion with an average accuracy of 84.2%.\nIndex Terms—Natural-language Requirements, Question An-\nswering (QA), Language Models, Natural Language Processing\n(NLP), Natural Language Generation (NLG), BERT, T5.\nI. INTRODUCTION\nA software requirements speciﬁcation (SRS) is a pivotal ar-\ntifact in Requirements Engineering (RE). An SRS lays out the\ndesired characteristics, functions, and qualities of a proposed\nsystem [1]. SRSs are frequently analyzed by requirements\nengineers as well as by other stakeholders to ensure the\nquality of the requirements [2]. To enable the creation of\na shared understanding among stakeholders from different\nbackgrounds, e.g., product managers, domain experts, and\ndevelopers, requirements are most often written in natural lan-\nguage (NL) [3]. Despite its numerous advantages, NL is highly\nprone to issues such as ambiguity [4], [5], incompleteness [6],\n[7] and inconsistency [8]. Manually screening for such issues\nin a lengthy SRS with tens or hundreds of pages is time-\nconsuming, since such screening requires domain knowledge\nfor accurately interpreting the requirements. Evoking domain\nknowledge is not always quick or easy for humans.\nQuestion answering (QA), i.e., mechanisms to instantly ob-\ntain answers to questions posed in NL [9], would be useful as a\nway to make requirements quality assurance more efﬁcient. To\nillustrate, consider the example requirements in Fig. 1. These\nrequirements originate from an SRS in the aerospace domain.\nTo facilitate illustration, the requirements in the ﬁgure are\npreﬁxed with identiﬁers. For simplicity, we further assume\nthat each requirement in our example is one text passage.\nIn practice, a passage, as we elaborate in Section III, can\nbe made up of multiple consecutive sentences, potentially\ncovering multiple requirements.\nWhile analyzing requirement DR-13 in Fig. 1, the developer\nimplementing the computations related to the “wet mass” of\na spacecraft might come up with question Q1, also shown in\nFig. 1. Q1 could be prompted by the developer having doubts\nabout the concept of “wet mass” or them wanting to verify\ntheir interpretation. A challenge here is that the answer to a\nposed question may be absent from the SRS. This happens\nto be the case for Q1. Since the presence of a requirements\nglossary cannot be taken for granted either [10], to answer\nQ1, one may need to consult external domain resources. These\nresources could be other SRSs from the same domain, or when\nsuch SRSs are non-existent or sparse, a domain-speciﬁc corpus\nextracted from a general source such as Wikipedia. On the\nright side of Fig. 1, we show excerpts of a domain-speciﬁc\ncorpus automatically extracted from Wikipedia using an exist-\ning open-source corpus extractor [11]. As seen from the ﬁgure,\njust like the SRS being examined, the corpus is made up of\npassages. These passages may nonetheless be dispersed across\nmultiple documents in the corpus. An answer to Q1 can be\nfound in the extracted corpus. This answer can guide analysts\ntoward making the SRS more complete by providing additional\ninformation in the SRS about the concept of “wet mass”.\nIn Fig. 1, we provide two further questions, Q2 and Q3, that\ncan tip analysts to potential quality problems in our example\nSRS. Automated QA will ﬁnd DR-27 and more speciﬁcally\nthe highlighted segment in that requirement to be an answer\nto Q2. Upon examining this answer and not ﬁnding the exact\nfrequency of wet-mass computations, the analysts will likely\nconclude that the SRS is incomplete. For a ﬁnal example,\narXiv:2302.04793v1  [cs.SE]  9 Feb 2023\nDomain-\nspecific \nCorpus\nSoftware \nRequirements \nSpecification \n(SRS)\nQ3 What is the function of navigation camera system?\nQ1 What is the wet mass of a spacecraft? \nHow often shall the wet mass be computed?\nQ2\n[DR-13] The wet mass of the spacecraft shall not exceed \n3004 kg.\nText passages extracted from requirements specifications\n[DR-27] The wet mass of the spacecraft shall be \ncomputed at a regular basis.\n[SCIR-20] The navigation camera system shall be used \nonly for the detection of the comet nucleus.\n[MISS-29] The spacecraft shall use a navigation camera \nsystem for asteroid detection, approach navigation and \nclose fly-by tracking of the asteroid.\n[SCIR-19] The spacecraft shall carry a science camera \nsystem as part of the scientific payload and a separate \nnavigation camera system.\nIncomplete requirement?\nInconsistent requirements?\nText passages extracted from a domain-specific corpus\nIn aerospace engineering, mass ratio is a measure of \nthe efficiency of a rocket. \nIt describes how much more massive the vehicle is with \npropellant than without; that is, the ratio of the rocket's \nwet mass (vehicle plus contents plus propellant) to its \ndry mass (vehicle plus contents).\nA navigation system is a computing system that aids in \nnavigation. Navigation systems may be entirely on board \nthe vehicle or vessel that the system is controlling (for \nexample, on the ship's bridge) or located elsewhere, \nmaking use of radio or other signal transmission to control \nthe vehicle or vessel.\nNavigation is a field of study that focuses on the process \nof monitoring and controlling the movement of a craft or \nvehicle from one place to another. The field of navigation \nincludes four general categories: land navigation, marine \nnavigation, aeronautic navigation, and space navigation.\nDomain Knowledge\n…\n…\n…\nFig. 1: Example – from left to right: passages from SRS; posed questions; passages from external domain-speciﬁc corpus.\nconsider Q3. In response to this question, QA identiﬁes several\nlikely answers both in the SRS as well as in the extracted\ncorpus. Among the answers are segments from requirements\nSCIR-20 and MISS-29. Reviewing these two requirements\nside by side (rather than encountering them potentially many\npages apart in the SRS) provides the analysts with a much\nbetter chance of noticing the inconsistency between what the\ntwo requirements expect of the “navigation camera system”.\nThe answers from the domain-speciﬁc corpus and the passages\nwhere these answers are located provide additional useful\ninformation for the review process.\nIn this paper, we propose QAssist – standing for Question\nAnswering Assistance for Improved Requirements Analysis.\nQAssist builds on open-domain QA, which is the task of\nﬁnding in a collection of documents the answer to a given\nquestion [12]. QAssist takes as input a question posed in\nNL and returns as output a list of text passages that likely\ncontain the answer to the question. QAssist further demarcates\na possible answer (text segment) within each retrieved passage.\nGiven questions such as Q1, Q2 and Q3 in Fig. 1, we are\ninterested in two sets of text passages: one obtained from the\nSRS under analysis (left side of Fig. 1) and the other obtained\nby mining a domain-speciﬁc knowledge resource (right side\nof Fig. 1). These passages and the answers found within them\nprovide a focused view, helping analysts better understand the\nrequirements and more effectively pinpoint quality problems.\nContributions. Our contributions are as follows:\n(1) We devise QAssist, an AI-based QA approach aimed at\nproviding assistance with requirements analysis. Given a ques-\ntion posed in NL about the requirements in an SRS, QAssist\nemploys Natural Language Processing (NLP) to retrieve two\nlists of relevant text passages: one from the SRS and one from\na domain-speciﬁc corpus. In each passage, the likely answer\nto the posed question is highlighted. When a domain-speciﬁc\ncorpus does not exist, QAssist automatically builds one, using\nthe phrases appearing in the given SRS as seed terms. Our\nimplementation of QAssist is publicly available [13].\n(2) We develop in a semi-automatic manner a QA dataset\ntailored to NL requirements. We name this dataset REQuestA\n– standing for Requirements Engineering Question-Answering\ndataset. REQuestA has been built by two third-party human\nanalysts over six SRSs spanning three application domains.\nOverall, REQuestA contains 387 question-answer pairs. Of\nthese, 214 are manually deﬁned; the remaining 173 are gen-\nerated automatically and then subjected to manual validation.\nWe make the REQuestA dataset publicly available [13].\n(3) We empirically evaluate QAssist on the REQuestA\ndataset. Our results indicate that QAssist retrieves with an\naccuracy of 100% from a domain-speciﬁc corpus the document\nthat is most relevant to a given question. Furthermore, QAssist\nlocalizes the answer to a question to three passages within\nthe requirements speciﬁcation and within the corpus with\nan average recall of 90.1% and 96.5%, respectively. QAssist\ndemarcates the actual answer to a question with an average\naccuracy of 84.2%.\nSigniﬁcance. We believe our work is signiﬁcant for the RE\nand NLP communities, as we discuss next. In RE, auto-\nmated QA has been investigated only to a limited extent and\nmostly in the context of traceability [14]–[18]. Traceability QA\nprimarily targets the relationship between different artifacts,\ne.g., requirements, design diagrams and source code. More\nrecently, QA has been studied for improving the understanding\nof compliance requirements [19]. For RE, the signiﬁcance\nof our work is two-fold. First, our QA solution is, to our\nknowledge, the ﬁrst to empirically investigate the application\nof modern QA technologies over industrial requirements.\nThrough a seamless process of posing questions and getting\ninstant answers, our approach enables analysts to explore po-\ntential quality issues, e.g., incompleteness and inconsistency.\nSecond, alongside our solution, we build and publicly release\na considerably sized QA dataset covering six SRSs from\nthree application domains. This dataset focuses on clariﬁcation\nquestions posed over SRSs and is the ﬁrst dataset of its kind.\nQA is widely studied in the NLP community [20], where,\nas we elaborate in Section VII, many automated solutions and\ndatasets have been proposed and evaluated. The most well-\nknown QA datasets in the NLP literature are derived from\nWikipedia, e.g., SQUAD [21], TriviaQA [22] and NQ [23],\nto name few. There are also examples of domain-speciﬁc\ndatasets, e.g., in the medical [24]–[26] and railway [27] do-\nmains. From an NLP standpoint, our work is signiﬁcant in that\nit is capable of looking beyond a single source for identifying\nanswers to a posed question. The NLP literature concentrates\nmainly on the situation where the answer to a question resides\nin an a-priori-known source document (or text passage). Our\nwork departs from this position by bringing in a secondary\nsource of knowledge, namely a domain-speciﬁc corpus, to\ncomplement the primary source (in our case, an SRS), while\nmaintaining the distinction between the two sources. Using a\nsecondary source is necessitated by our application context:\nSRSs are typically highly technical with a potentially large\namount of tacit (unstated) domain knowledge underpinning\nthem. By provisioning for, and when necessary, automatically\nconstructing a domain-speciﬁc corpus, our approach increases\nthe chance that analysts will obtain satisfactory answers to\ntheir requirements-related questions.\nStructure.\nSection II presents background. Section III de-\nscribes our QA approach. Section IV reports on our empirical\nevaluation. Section V compares with broad-based search en-\ngines.\nSection VI explores threats to validity. Section VII\ndiscusses related work. Section VIII concludes the paper.\nII. BACKGROUND\nThis section describes the background for our QA approach.\nOpen-domain QA. Our proposed approach targets the open-\ndomain QA task (deﬁned in Section I). Modern open-domain\nQA solutions work in two stages, combining information re-\ntrieval (IR) with machine reading comprehension (MRC) [28].\nIR is applied ﬁrst to narrow the search space by ﬁnding the\nrelevant text passages that likely contain the answer to a\nquestion [29]. Subsequently, MRC models extract the likely\nanswer to the question from the text passages retrieved [21].\nAn IR-based method is referred to as a RETRIEVER since\nit retrieves relevant text, while an MRC-based model is\nreferred to as a READER since it reads the text to ﬁnd\nthe answer [12]. State-of-the-art QA techniques rely heavily\non language models (LMs) such as BERT [30] as an en-\nabling technology [31]. Below, we introduce IR and MRC\nalongside the LMs that we consider and experiment with in\nthe development of our approach.\nInformation Retrieval (IR). Given a query and a collection\nof documents, IR methods are designed to rank the documents\naccording to their relevance to the query [32]. Traditional\nmethods in IR include term frequency - inverse document\nfrequency (TF-IDF) and Okapi Best Matching (BM25). TF-\nIDF assigns a composite weight to each term occurring in\nthe document depending on its occurrence frequency in the\ndocument relative to its frequency in the entire document\ncollection [33]. These weights are used to transform a text\nsequence into a mathematical vector. Following this, both the\nquery and the documents are represented as vectors, with\nthe query being treated as a (short) document. Relevance is\ncomputed using similarity metrics, e.g., cosine similarity [32].\nSimilarity metrics quantify the similarity between the query\nand a document while normalizing the difference in vector\nlength; vectors for documents are signiﬁcantly longer than\nthose for queries. Unlike TF-IDF which is a binary model\nrelying on the presence of question terms in the document\ncollection, BM25 is a probabilistic model that improves the\nTF-IDF weights using relevance feedback [34].\nIn the context of QA, IR-based methods assess relevance\nof documents as well as text passages within individual docu-\nments. In the latter case, each passage is regarded as a single\ndocument during vectorization. Despite being relatively old,\nBM25 and to a lesser extent TF-IDF are still widely applied\nin text retrieval tasks due to their simple implementation and\nrobust behavior [35]. In addition to traditional methods, dense\nand reranking methods have recently been introduced in the\nQA literature [35]–[38]. Leveraging language models, dense\nmethods compute relevance based on the text representations\nin the dense vector space, whereas reranking methods combine\nthe rankings of two different IR-based methods.\nMachine Reading Comprehension (MRC). MRC models\nare speciﬁcally used to extract the likely answer to a given\nquestion from a text passage [21]. MRC is often solved using\npre-trained language models (e.g., BERT), introduced next.\nThese models typically limit the length of the text passage to\nbe less than or equal to 512 tokens [30], [39].\nLanguage Models (LMs). Large-scale neural LMs have\nrapidly dominated the state-of-the-art in NLP [40]. LMs are\npre-trained on large bodies of text in order to learn contextual\ninformation, regularities of language, and syntactic and se-\nmantic relations between words. This learned knowledge can\nthen be used by ﬁne-tuning LMs to solve downstream NLP\ntasks [41], e.g., QA [42]. Below, we brieﬂy discuss the LMs\nthat we consider and experiment with in this paper.\nBidirectional Encoder Representations from Transformers\n(BERT) [30] is pre-trained on the BooksCorpus and English\nWikipedia with two training objectives, namely masked lan-\nguage modeling (MLM) and next sentence prediction (NSP).\nIn MLM, a fraction of the tokens in the pre-training text are\nrandomly masked. The model is trained to predict the original\nvocabulary of these masked tokens based on the surrounding\ncontext. For example, BERT should predict the masked token\n“briefed” in the phrase “[MASK] reporters on”. In NSP, the\nmodel is trained to predict whether two text segments are\nconsecutive in the original text. BERT learns contextualized\nrepresentations of words by utilizing the Transformer archi-\ntecture [43] and attention mechanisms that allow the model to\nattend to different information from different representations.\nFor example, the model re-weights the embeddings of “bank”\nand “river” in the sentence “I walked along the banks of the\nriver” to highlight the meaning of “bank” in this context.\nEfﬁciently Learning an Encoder that Classiﬁes Token Re-\nplacements Accurately (ELECTRA) [44] improves the contex-\ntual representations learned by BERT by replacing the MLM\ntraining objective with a token replacement objective, i.e.,\nrandomly replacing some tokens instead of masking them.\nA Lite BERT (ALBERT) [45], A Distilled Version of BERT\n(DistilBERT) [46], MiniLM [47] and the Robustly optimized\nBERT pre-training approach (RoBERTa) [48] are other variants\nthat optimize the size and computational cost of BERT using\nmethods such as knowledge distillation [49] – a technique that\ntransfers knowledge from a large unwieldy model to generate\na smaller model with less parameters yet similar performance.\nThe text-to-text transfer transformer (T5) model [50] is\nanother interesting and popular LM. T5 is pre-trained on the\nColossal Clean Crawled Corpus (C4) which was also released\nalongside the model. C4 consists of hundreds of gigabytes of\nclean text that is crawled from the Web. Compared to BERT-\nstyle models, T5 uses a text-to-text framework that enables\naddressing a wider spectrum of NLP downstream tasks as long\nas they can be formulated as a text-to-text problem.\nIII. APPROACH\nIn this section, we describe our approach and also establish\nthe notation that we use throughout the rest of the paper. Fig. 2\nshows an overview of our approach. QAssist takes as input a\nquestion (q) posed in NL by the user and an SRS. In step 1,\nQAssist retrieves the most relevant document (d) to q from\nan external domain-speciﬁc corpus (D). In step 2, QAssist\ngenerates a list of text passages (T ) by splitting the input SRS\nand d. QAssist then ﬁnds in step 3 the top-k text passages\n(R ⊂T ) that are most relevant to q. In step 4, QAssist\nextracts a likely answer from each text passage retrieved in\nstep 3. QAssist ﬁnally returns as output the relevant text\npassages from step 3 alongside the answers extracted in step 4.\nAs explained in Section II, the pipeline for an open-domain\nQA system like QAssist is made up of two phases: (i) IR-\nbased (spanning steps 1 – 3) and (ii) MRC-based (step 4). In\nphase (i), we apply two RETRIEVERS, one for retrieving d ∈D\nin step 1 (document retriever – for short RETRIEVERD) and\nanother for ﬁnding R in step 3 (passage retriever – for short\nRETRIEVERT ). Next, we elaborate each step of QAssist.\nA. Step 1: Document Retrieval\nAs a prerequisite for applying RETRIEVERD in this step,\na corpus D should be available. When D is absent, it can\nbe automatically generated using existing corpus-extraction\nmethods [5], [11], [51]–[54]. QAssist’s ability to incorporate\nan external corpus of knowledge into the QA process is im-\nportant as a way to enrich the output with domain knowledge.\nIn this step, RETRIEVERD mines D to ﬁnd a document d that\nis most relevant to q. In particular, RETRIEVERD computes\nﬁrst the relevance between q and each document in D, and\nthen ranks these documents according to relevance scores.\nFrom the resulting ranked list, QAssist selects as the result of\nstep 1 the most relevant document (d ∈D). Note that, while\nunnecessary for our purposes in this paper, the number of most\nrelevant documents to retrieve from D can be conﬁgured to a\nvalue c > 1. In that case, the output d from step 1 would be\nthe sequential combination of the top-c retrieved documents.\nB. Step 2: Splitting\nThis step takes two documents as input: the SRS under\nanalysis as well as the most relevant corpus document d\nretrieved in step 1. QAssist automatically generates two lists\nTS and TD of text passages by splitting the given SRS and d,\nrespectively. To do so, we employ a simple NLP pipeline\nthat consists of tokenization and sentence splitting, breaking\nthe input text (SRS and d) into tokens and sentences. Using\nthe annotations from this NLP pipeline, we iterate over each\ndocument to identify the text passages.\nRecall from Section II that LM-based READERS (which we\napply in step 4) typically limit passage length to 512 tokens.\nAccordingly, we deﬁne a text passage as a paragraph, unless\nthe paragraph is too long (i.e., has more than 512 tokens)\nand thus cannot be processed by LMs in its entirety. Long\nparagraphs are split with one sentence of overlap to preserve\ncontext. Concretely, we apply the following procedure to split\nlong paragraphs into coherent passages.\nAssume that a given paragraph has a sequence of n sen-\ntences, s1, . . . , sn. We put consecutive sentences s1, . . . , si\ninto one passage, such that the length of the resulting passage\nis less than or equal to 512 tokens. In the next iteration, we\nstart at si, i.e., the last sentence of the previous passage.\nTo create the next passage, we take consecutive sentences\nsi, . . . , sj subject to the 512-token length constraint. This\nprocess is repeated until all the sentences in the paragraph\nhave been covered. The rationale for a one-sentence overlap\nbetween adjacent passages from the same paragraph is to help\nmaintain ﬂow continuity in the passages.\nThe output from step 2 (TS and TD) is passed to step 3.\nC. Step 3: Passage Retrieval\nIn this step, we apply RETRIEVERT to ﬁnd the k most rele-\nvant text passages to q from each TS and TD. We denote the set\nof resulting passages by RS ⊂TS and RD ⊂TD, respectively.\nIn a similar manner to step 1, RETRIEVERT computes and\nassigns relevance scores to each text passage in TS and TD.\nThe passages in each TS and TD are sorted in descending order\nof relevance and the top-k passages are picked. In Section IV,\nwe empirically assess the implications of the value of k for\npractice. RS and RD constitute the input to step 4.\nD. Step 4: Answer Extraction\nIn the last step of QAssist, we apply a READER to extract\na likely answer to q from each text passage in RS and\nRD. The likely answers are highlighted in and presented\ntogether with RS and RD as the output of QAssist. Which\nREADER technology yields the best results is a question that\nwe investigate empirically in Section IV.\nIV. EMPIRICAL EVALUATION\nIn this section, we empirically evaluate QAssist.\nA. Research Questions (RQs)\nOur evaluation addresses the following RQs:\nRQ1: Which\nRETRIEVER has the highest accuracy in\nﬁnding text that is most relevant to a given question? Recall\nfrom Section III that QAssist employs RETRIEVERD in step 1\n(i.e., Document Retrieval) and RETRIEVERT in step 3 (i.e.,\nSRS\nSplitting\nPassage Retrieval\nAnswer Extraction\nDocument \nRetrieval\nQuestion\nReader\nDocument\nRetriever \nRelevant Passages \nContaining  Answers\n2\n1\n3\n4\nIR-based Component\nDomain-\nspeciﬁc \nCorpus\nMRC-based Component\nUser\nRelevant \nDocument\nText Passages\nRelevant  \nPassages\nPassage \nRetriever\nFig. 2: Overview of our approach (QAssist).\nPassage Retrieval). RETRIEVERD takes as input a collection\nof documents and returns as output the most relevant document\nd ∈D. RETRIEVERT takes as input a list of text passages and\nreturn as output the top-k passages relevant to a given question.\nFor each RETRIEVER, we investigate in RQ1 four alternatives\nfrom the IR literature as outlined in Section IV-D. RQ1\nidentiﬁes the most accurate alternative for each RETRIEVER.\nRQ2: Which READER produces the most accurate results for\nextracting the likely answer to a given question? QAssist uses\nin step 4 (i.e., Answer Extraction) a READER for extracting\na likely answer to a given question from each relevant text\npassage retrieved by the passage retrievers in step 3. Multiple\nalternative READERS can be applied here as we explain in\nSection IV-D. RQ2 investigates these alternatives and identiﬁes\nthe most accurate one.\nRQ3: Does QAssist run in practical time? RQ3 analyzes\nQAssist’s execution time. To be applicable in practice, QAssist\nneeds to be able to answer questions in practical time.\nB. Implementation\nWe implement QAssist using Python 3.7.13 and Jupyter\nNotebooks [55]. Speciﬁcally, we implement the NLP pipeline\n(including the tokenizer and sentence splitter) using the\nTransformers 3.0.1 library [56]. We implement the tradi-\ntional IR methods and TF-IDF vectorization using Scikit-\nlearn 1.0.2 [57], and implement BM25 using the BM25\n0.2.2 library [58]. The language models that we experiment\nwith include the IR-based models DistilBERT-base-tas-b and\nMiniLM-L-12-v2 from BeIR [59] and the MRC-based models\nALBERT-large v1.0, BERT-large-uncased, DistilBERT-base-\ncased, ELECTRA-base, MiniLM-uncased and RoBERTa-base\nfrom HuggingFace [60]. For corpus extraction from Wikipedia,\nwe use the Wikipedia 1.4.0 library [61]. For question gener-\nation, discussed in Section IV-C, we use NLTK 3.2.5 [62] to\npreprocess text from SRSs and corpus documents. We then\napply T5-base-question-generator and BERT-base-cased-qa-\nevaluator for automatically generating and assessing question-\nanswer pairs. Both of these models are from HuggingFace.\nC. Data Collection Procedure\nTo evaluate QAssist, we collected six SRSs from three\napplication domains, namely aerospace, defence, and secu-\nrity. Our data collection resulted in a QA dataset named\nREQuestA (RE Question-Answering dataset). To reduce the\ncost and effort required for the construction of this dataset,\nabout half of the question-answer pairs in REQuestA were\ngenerated automatically using text generation models [50] and\nthen validated by human analysts. The remaining half were\ndeﬁned manually. In this section, we discuss the desiderata for\nREQuestA, the automatic QA generation method, the process\nfor manual deﬁnition of question-answer pairs, and ﬁnally the\ndetails of the resulting dataset.\nDesiderata.\nWe identify the following desiderata for RE-\nQuestA in view of the analytical goals we would like to\nsupport, as discussed in Section I.\n(1) Focus on content-based questions. REQuestA is populated\nwith clariﬁcation questions over SRSs. REQuestA thereby\ndoes not contain questions that are not directly related to the\nSRS content, for instance, questions related to change impact\nanalysis or project management, an example of which would\nbe “How many requirements are not implemented in Phase-\n1 of the project?”. Questions of this nature are legitimate in\nRE [18], but are outside the scope of our current work.\n(2) Inclusion of external sources of knowledge. Motivated\nby covering the domain knowledge that is often left tacit\nin SRSs, we would like REQuestA to include relevant text\npassages not only from SRSs but also from external sources\nof knowledge. The inclusion of external knowledge sources\nenables us to more conclusively evaluate the effectiveness of\nQA by considering requirements-related questions that would\ngo unanswered based on the contents of a given SRS alone.\nQA Auto-generation. Despite the availability of QA datasets,\nnone of them are directly applicable in our work, as ex-\nplained in Section I. Building a ground truth for QA requires\nconsiderable manual effort for proposing both questions and\nanswers. This prompted us to consider question generation\n(QG) [63], [64] as an aid during dataset construction. QG\nenables automated derivation of a large number of questions\nand answers from a given knowledge source; these questions\nand answers can subsequently be subjected to manual valida-\ntion for correctness. Such validation generally takes less time\nand cognitive effort from humans than deriving questions and\nanswers from scratch.\nAn entry in REQuestA is a text passage and a question-\nanswer pair associated with that passage. An answer in our\nwork is a short text span in a sentence. The questions and\nanswers in REQuestA are derived from two different sources:\nSRS\nDomain Analysis\nSplitting\nQuestion-answer \nPair Generation\nPreprocessing \nb\na\nc\nd\nWikipedia \nArticles\nText \nPassages\nPreprocessed \nSRS\nQG Model\n<q,a> & Text \nPassages\nFig. 3: Overview of our question generation method (used\nexclusively for building our dataset, REQuestA).\nthe input SRS and a domain-speciﬁc corpus created automat-\nically around the content of the input SRS. Fig. 3 shows an\noverview of our method for automatically generating questions\nand answers. Given an SRS as input, our method returns a list\nof question-answer pairs in four steps, elaborated next.\n(a) Preprocessing: In this step, we preprocess the input SRS by\napplying an NLP pipeline. The goal of this step is to identify a\nset of concepts which are used in the next step to analyze the\ndomain of the input SRS. To ﬁnd these concepts, we applied\nREGICE [10] – an existing tool for extracting glossary terms\nfrom NL requirements.\n(b) Domain Analysis: We build in this step a minimal domain-\nspeciﬁc corpus. To do so, we ﬁrst group the SRSs from the\nsame domain and then use the concepts extracted from these\nSRSs in step (a). Speciﬁcally, we compute for each concept a\nTF-IDF score, adapted to work over phrases (e.g., “navigation\ncamera”) rather than only individual terms (e.g., “camera”).\nNext, we attempt to increase the speciﬁcity of the concepts\nby removing any generic concepts (e.g., “camera”) appearing\nin WordNet [65] – a generic lexical database for English. We\nthen sort the concepts in descending order of TF-IDF scores\nand select the top-50 concepts, referring to these concepts\nas keywords. Inspired by recent work on the generation of\ndomain-speciﬁc corpora for requirements analysis tasks [5],\nwe use each keyword to query Wikipedia and ﬁnd a matching\narticle, i.e., an article whose title overlaps with a given key-\nword. Finally, we randomly select from the matching articles\na subset to use in the next step.\n(c) Splitting: In this step, we use the same method presented\nin Section III to automatically split the SRS and Wikipedia\narticles into a set of text passages.\n(d) Question-answer Pair Generation: In this step, we use a\nQG model based on the T5 language model (introduced in\nSection II). We give as input a text passage to the QG model.\nThe model ﬁrst extracts a random answer from the passage\nand then automatically generates a corresponding question. For\nexample, for passage DR-13 in Fig. 1, the QG model could\nﬁrst pick “3004 kg”, and then generate the following question:\n“What shall the wet mass of the spacecraft not exceed?”. The\noutput of the QG model includes the text passage and a set\nof automatically generated question-answer pairs. Each such\npair will be denoted ⟨q, a⟩hereafter. Note that multiple pairs\ncan be generated from the same text passage. To reduce the\nmanual effort needed for validating the questions and answers,\nwe apply a QA evaluator that is based on BERT. The evaluator\ntakes as input a pair ⟨q, a⟩and returns as output a value\nrepresenting its prediction about whether the pair is valid.\nWe sort the auto-generated pairs according to the resulting\nscores from the evaluator, and then select the top 5% of the\n⟨q, a⟩pairs automatically generated from each SRS and the\nWikipedia articles in the respective corpus.\nConstruction of REQuestA. The construction of REQuestA\ninvolved two third-party (non-author) human analysts. The ﬁrst\nanalyst has a Master’s degree in multilingualism. The second\nanalyst has a computer science background with a Master’s\ndegree in quality assurance. Both analysts had prior experience\nwith software requirements and had previously contributed to\nannotation tasks involving SRSs. Before starting their work,\nthe analysts participated in a half-day training session on\nquestion answering in RE where they additionally received\ninstructions about the desiderata for REQuestA.\nWe shared with the analysts the original SRSs, the randomly\nselected Wikipedia articles (created during the domain analysis\nstep in Fig. 3), and the list of automatically generated ⟨q, a⟩\npairs for each SRS. The analysts were asked to handle each\n⟨q, a⟩pair as follows. Each question q was labeled as valid\nindicating that q was correct as-is, rephrased indicating that\nq was semantically correct but required structural improve-\nment to become valid, or invalid indicating that q did not\nmake sense. Similarly, each answer a was labeled as correct,\ncorrected, or invalid with similar indications to the ones\nmentioned above for q. Additionally, a could be labeled as\nnot in context indicating that the question cannot be answered\nfrom the given text passage. In this case, we consider the\nanswers as invalid. We further asked the analysts to manually\ndeﬁne question-answer pairs on each text passage during the\nvalidation process. We discuss quality considerations for our\ndataset later in this section.\nTo construct the REQuestA dataset, we ﬁltered out any pair\nwhere either q or a was invalid. For the remaining pairs, we\nused the rephrased q and corrected a according to the revisions\nsuggested by the human analysts. In total, we automatically\ngenerated 204 ⟨q, a⟩pairs; 111 from the SRSs and 93 from\nthe Wikipedia articles. From these, we ﬁltered 31 pairs due to\ninvalid questions or answers, leaving 173 pairs in the dataset\n(86 from the SRSs and 87 from the Wikipedia articles). We\nfurther included in REQuestA question-answer pairs that the\nanalysts had deﬁned manually during the validation process\nalongside the respective text passages. In total, the analysts\nmanually deﬁned 214 pairs (103 from the SRSs and 111 from\nthe Wikipedia articles). Overall, REQuestA contains 387 pairs.\nTable I provides summary statistics for REQuestA. Speciﬁ-\ncally, the table lists the number of auto-generated ⟨q, a⟩pairs\n(auto) as well as the number of pairs manually deﬁned by\nthe analysts (man). The table further shows |TD| indicating\nthe average number of text passages in the Wikipedia articles\n(noting that there are multiple articles in each corpus), and\nTABLE I: Summary Statistics for the REQuestA Dataset.\n⟨q, a⟩\n⟨q, a⟩\nDomain\n|TD|\nauto\nman\nSRS\n|TS|\nauto\nman\nAerospace\n42\n45\n53\n#1\n24\n8\n18\n#2\n107\n37\n40\nDefence\n94\n38\n50\n#3\n11\n5\n4\n#4\n71\n19\n26\nSecurity\n23\n4\n8\n#5\n18\n15\n13\n#6\n4\n2\n2\nTotal\n159\n87\n111\n-\n235\n86\n103\n|TS| indicating the number of text passages in each SRS.\nQuality of REQuestA. As a quality measure, the two analysts\nreviewed an overlapping subset amounting to 10% of the\nauto-generated ⟨q, a⟩pairs. We counted an agreement when\nthe analysts selected the same label for a given question or\nanswer (i.e., valid or invalid), noting that valid includes both\nrephrased and corrected. On this subset, the analysts were in\nfull agreement (i.e., no disagreements) on the labels for the\nquestions and answers.\nTo further ensure the quality of the dataset, we analyzed\nall the automatically generated questions and answers against\nthe corrections provided by the human analysts. Out of the\n173 valid questions, the analysts collectively rephrased 24\nquestions (representing ≈14% of the auto-generated questions)\nand corrected 46 answers (representing ≈26% of the auto-\nextracted answers). Out of the 46 corrected answers, 26\nwere expanded by the analysts to include missing tokens,\ne.g., the auto-extracted answer “software code” was corrected\nto “implemented software code”. To increase the quality of\nour dataset, we included in REQuestA the corrected answers\nand not the auto-extracted ones. Following best practices\nin the natural-language generation literature and machine\ntranslation [66], we apply BLEU for lexical similarity and\nBERTScore for semantic similarity. Given two questions, q1\nand q2, BLEU measures the overlapping tokens between q1\nand q2. The score is then normalized by the total number of the\ntokens in q1 and q2. BERTScore measures semantic similarity\nbetween q1 and q2 based on contextual word embeddings.\nThe resulting scores are BLEU=0.54 and BERTScore=0.95.\nThese values indicate that the auto-generated questions and\nthe rephrased ones are semantically very similar albeit using\ndifferent structures. These scores indicate that our QG method\nsuccessfully produces semantically correct questions, while\nalso implying that the analysts frequently chose to make\nstructural improvements for better readability.\nSince no training or ﬁne-tuning is performed in our ap-\nproach, we use REQuestA in its entirety for empirically eval-\nuating the available QA technologies. To facilitate replication\nand future research, REQuestA is made publicly available [13].\nD. Evaluation Procedure\nTo answer our RQs, we conduct the following experiments.\nSee Section II for background.\nEXPI. This experiment answers RQ1. We evaluate in EXPI\nfour alternative RETRIEVERS, including the traditional RE-\nTRIEVERS TF-IDF and BM25, DistilBERT dense RETRIEVER,\nand a reranking RETRIEVER that pairs BM25 with MiniLM\ncross encoder. We identify in EXPI the most accurate RE-\nTRIEVER applied in step 1 of our approach (Fig. 2) for\nretrieving the most relevant external document from a domain-\nspeciﬁc corpus. We further identify the most accurate RE-\nTRIEVER in step 3 for retrieving from the input SRS and\nthe most relevant external document the top-k relevant text\npassages for a given question. We compare the performance\nof the alternative RETRIEVERS using two evaluation metrics\ncommonly used in the IR literature [29]. The ﬁrst metric is\nrecall@k (R@k) and assesses whether the document (or text\npassage) containing the correct answer to a given question\n(q) is in the ranked list of the top-k documents (or passages)\nproduced by the RETRIEVER. The second metric, normalized\ndiscounted cumulative gain@k (nDCG@k), is similar to R@k,\nexcept that it accounts not only for the mere presence of the\nrelevant document (or passage) but also for its rank.\nWe note that we are interested only in the most relevant\ndocument (top-1) retrieved by the document RETRIEVER in\nstep 1 of our approach. In this case, ranking is not relevant\nand the above two metrics produce the same result; we thus\nreport only R@1 for the document RETRIEVER. To run EXPI,\nusing an existing open-source tool [11], we generate domain-\nspeciﬁc corpora covering the aerospace, defence, and security\ndomains and corresponding to the SRSs in our study.\nEXPII. This experiment answers RQ2. To extract the answer\nto a given question in step 4 of our approach (Fig. 2), we\nexperiment with the following alternative READERS: ALBERT,\nBERT, DistilBERT, ELECTRA, MiniLM, and RoBERTa. We\ncompare the performance of the READERS using Accuracy\n(A), computed as the number of questions correctly answered\nby the READER divided by the total number of questions.\nTo decide whether an answer is correct, we compare the\nextracted answer by the READERS against the answer provided\nby the analysts in our dataset (REQuestA). We evaluate an\nextracted answer for correctness in three different modes. Let\naGT denote the ground-truth answer to a question. In exact\nmatching mode, the extracted answer fully matches aGT . In\npartial matching mode, the extracted answer partially matches\n(i.e., overlaps with) aGT . In semantic matching mode, the\nextracted answer has a cosine semantic similarity with aGT\nthat is greater than a predeﬁned threshold. In our work, we\napply a threshold of 0.5 [67]. The ﬁrst two modes evaluate\ncorrectness at a lexical level, whereas the last mode measures\ncorrectness based on meaning.\nIn addition to reporting accuracy, we also report F1\nmeasure – another commonly-reported lexical metric in the\nQA literature [68]. F1 is the harmonic mean computed as\n2 ∗P ∗R/(P + R), where P is the precision and R is the\nrecall. We deﬁne P as the number of overlapping tokens\nbetween the extracted answer and aGT divided by the total\nnumber of tokens in the extracted answer. We deﬁne R as the\nTABLE II: R@1 of Document RETRIEVER (RQ1).\nDomain\n|D|†\nTF-IDF\nBM25\nDense\nReranking\nAerospace\n1158\n100\n100\n99.0\n100\nDefence\n781\n100\n100\n98.9\n100\nSecurity\n50\n100\n100\n91.7\n100\n† |D| is the number of articles in the corpus (D) of Wikipedia articles.\nnumber of overlapping tokens between the extracted answer\nand aGT divided by the total number of tokens in aGT . We\nreport in EXPII overall F1-score averages for all questions.\nEXPIII.\nThis experiment answers RQ3. We report the\nexecution of our approach with the most accurate models from\nthe previous experiments. EXPIII is conducted on the Google\nColaboratory cloud using the free plan with the following\nspeciﬁcations: Intel(R) Xeon(R) CPU@2.20GHz, Tesla T4\nGPU, and 13GB RAM.\nE. Answers to the RQs\nRQ1. Which RETRIEVER has the highest accuracy in ﬁnding\ntext that is most relevant to a given question? RQ1 identiﬁes\nthe best-performing (i) document RETRIEVER and (ii) passage\nRETRIEVER to be applied in steps 1 and 3 of QAssist,\nrespectively. Tables II and III show the results of EXPI.\nIn Table II, traditional RETRIEVERS (TF-IDF and BM25)\nare clearly able to ﬁnd the most relevant documents across\nall domains, thus achieving a perfect R@1. In comparison,\nour dense RETRIEVER (DistilBERT) has an average R@1 of\n96.5%, which is slightly worse than the traditional RETRIEV-\nERS. The reranking RETRIEVER achieves a perfect R@1 as\nwell since it partially uses the results of BM25. In view of\nthese results, we select BM25 as the RETRIEVER to use for\nstep 1 of our approach, since BM25 is computationally more\nefﬁcient than the reranking RETRIEVER. Compared to TF-\nIDF, BM25 is more robust [69] and widely-applied in the QA\nliterature [35].\nIn Table III, we show the results for retrieving the most\nrelevant k text passages for k = 1, 3, 5, 10. The upper part\nof the table provides the average results for our collection of\nsix SRSs. The lower part of the table shows the results for\nretrieving passages from the most relevant external document.\nWe recall from Section III that TS denotes the set of passages\nwithin a given SRS and TD denotes the passages in the most\nrelevant external document from the corpus. In our dataset, an\nSRS has on average about 40 passages, whereas an external\ndocument has on average 53 passages. Here, recall measures\nthe presence of the relevant passage in the retrieved passages,\nwhereas nDCG measures whether the relevant passage has a\nhigher rank among the retrieved passages. In our analysis, we\nfocus on recall, noting that rank does not play as signiﬁcant\na role for small values of k (≤3) where our discussion of\nrecall, below, leads us to.\nWe observe from Table III that the reranking RETRIEVER\noutperforms the alternatives in the two metrics and for all k\nvalues, except for the security domain as we elaborate later. We\nnaturally see improvement in recall with higher values of k.\nTABLE III: Accuracy of Passage RETRIEVER (RQ1).\nTop-1\nTop-3\nTop-5\nTop-10\nFrom TS\nR\nnDCG\nR\nnDCG\nR\nnDCG\nR\nnDCG\n(1)\n60.3\n60.3\n78.6\n70.9\n84.3\n73.3\n89.8\n75.1\n(2)\n62.8\n62.8\n78.5\n71.9\n85.4\n74.6\n92.4\n76.9\n(3)\n52.9\n52.9\n81.2\n70.3\n86.5\n72.5\n88.5\n73.2\n(4)\n78.9\n78.9\n90.1\n85.8\n92.2\n86.6\n92.4\n86.7\nFrom TD\nR\nnDCG\nR\nnDCG\nR\nnDCG\nR\nnDCG\nAerospace\n(1)\n43.6\n43.6\n64.7\n56.0\n68.6\n57.5\n91.5\n94.9\n(2)\n50.5\n50.5\n83.0\n70.1\n91.7\n73.6\n95.0\n74.7\n(3)\n66.7\n66.7\n87.3\n79.5\n90.6\n80.8\n91.3\n81.0\n(4)\n75.1\n75.1\n95.0\n87.3\n95.0\n87.3\n95.0\n87.3\nDefence\n(1)\n41.9\n41.9\n62.1\n54.1\n66.7\n55.9\n86.3\n62.2\n(2)\n38.0\n38.0\n81.2\n64.1\n89.3\n67.3\n94.6\n69.1\n(3)\n77.2\n77.2\n89.8\n84.7\n91.2\n85.2\n92.5\n85.6\n(4)\n76.0\n76.0\n94.6\n87.6\n94.6\n87.6\n94.6\n87.6\nSecurity\n(1)\n33.4\n33.4\n70.0\n53.8\n70.0\n53.8\n80.0\n57.4\n(2)\n43.3\n43.3\n70.0\n59.3\n100\n71.3\n100\n71.3\n(3)\n63.3\n63.3\n100\n85.2\n100\n85.2\n100\n85.2\n(4)\n80.0\n80.0\n100\n92.6\n100\n92.6\n100\n92.6\n(1) TF-IDF, (2) BM25, (3) Dense, and (4) Reranking.\nConcretely, the reranking RETRIEVER achieves for retrieving\npassages from the SRSs an average recall of 78.9%, 90.1%,\n92.2%, and 92.4% at k = 1, k = 3, k = 5, and k = 10,\nrespectively. The same RETRIEVER achieves for retrieving\npassages from the external document an average recall of\n77.0% at k = 1, and 96.5% at k = 3, k = 5, and k = 10.\nSelecting the best value of k has practical implications.\nWhile higher k values yield better recall, they entail additional\neffort for reviewing the results of QAssist. For instance,\nselecting k = 10 yields the best overall results, which implies\nthat a stakeholder has more relevant context at their disposal\nfor understanding and interpreting the requirements. However,\nthis comes at the cost of more time and effort needed to browse\nthrough the retrieved text passages. We deem k = 3 as a\nreasonable compromise in our context, since the gain in recall\nat k = 5 (in comparison to k = 3) is merely ≈2 percentage\npoints; selecting k = 5 would imply browsing through two\nadditional passages per question. That said, k can be left as a\nuser-conﬁgurable parameter, to be adjusted according to needs\nand the time budget available.\nThe results show that the dense RETRIEVER, DistilBERT,\nperforms on par with the reranking RETRIEVER for the se-\ncurity domain. In our collection, the domain-speciﬁc corpus\ngenerated for security is the smallest among the corpora as\nit is generated from two SRSs, one of which is very small\n(SRS #6). Furthermore, the number of passages analyzed\nin this domain is 23, compared to the aerospace and de-\nfence with an average of 42 and 94 passages, respectively.\nThis observation suggests that the dense RETRIEVER is more\neffective when there is a fewer number of passages. The\nperformance of the reranking RETRIEVER is in general better\nthan that of the dense RETRIEVER for k = 3. Consequently,\nwe select the reranking RETRIEVER as the best-performing\nalternative for step 3 of our approach.\nThe answer to RQ1 is that BM25 is the best document\nRETRIEVER with a perfect recall, and the reranking RE-\nTRIEVER is the best passage RETRIEVER with an average\nrecall@3 of 90.1% and 96.5% for SRSs and external\n(corpus) documents, respectively.\nRQ2. Which READER produces the most accurate results for\nextracting the likely answer to a given question? Table IV\nshows the results of EXPII, comparing the accuracy of the\nREADERS for extracting the answer to a given question. Note\nthat in RQ1, we focused on retrieving passages, whereas\nin RQ2, we are interested in determining which READER\nidentiﬁes the most accurate text span containing the answer\nwithin the passages already found.\nThe table shows that the most accurate READER varies de-\npending on which matching mode we choose. Considering the\nexact matching mode, RoBERTa is the most accurate READER,\nfollowed by ALBERT, with an average accuracy of 24.6% and\n24.3%, respectively. This ﬁnding is corroborated by the F1\nmeasure. Nevertheless, both READERS are outperformed by\nDistilBERT in the partial matching mode which achieves the\nbest average accuracy of 86.4%.\nNoting their lexical nature, the exact and partial matching\nmodes as well as the F1 measure have the drawback that they\nfocus on whether the extracted answer is literally the same as\nthe one in the ground truth rather than providing equivalent\ninformation [70]. For example, consider question Q1 in Fig. 1.\nThe answer extracted for this question from the ﬁrst passage\nof the domain-speciﬁc corpus (right side of the ﬁgure) could\nbe the following: “how much more massive the vehicle is\nwith propellant than without”. This answer does not have a\nlexical overlap with the highlighted answer (shaded green in\nthe ﬁgure), despite considerable similarity in meaning. For\nsuch cases, lexical metrics would evaluate the extracted answer\nas incorrect. To better assess the performance of the READERS\nin our context, where users may be seeking all closely relevant\ninformation, we further report results for the semantic match-\ning mode. Using the semantic matching mode would lead\nus to the same conclusion as that offered by exact matching\nand F1. That is, ALBERT and RoBERTa have the highest\naverage accuracy of 84.2% and 84.0%, respectively. Despite\nthe similar behavior of the two models, ALBERT considerably\noutperforms RoBERTa in partial matching mode with an\naverage percentage points of ≈19%. We thus select ALBERT\nas the best-performing READER for answer extraction.\nSince Wikipedia has been used for pre-training BERT\nand many variants thereof, and considering that part of our\nquestion-answer pairs originate from Wikipedia, we show that\nanswer extraction in our approach is still accurate for content\nthat originates from sources different from Wikipedia. Recall\nfrom Table I that REQuestA contains a total of 189 (= 86\n+ 103) question-answer pairs from SRSs and another 198 (=\n87 + 111) pairs from Wikipedia articles. The 189 question-\nanswer pairs from the SRSs are independent from Wikipedia.\nThe performance of BERT-based models over these pairs is a\nTABLE IV: READER Accuracy Results (RQ2); table further\nshows loading time for READERS (a consideration for RQ3).\nModel\nAccuracy\nF1\nTime\nExact\nPartial\nSemantic\nALBERT\n24.3\n79.1\n84.2\n64.6\n193.2\nqS\n31.7\n78.0\n86.4\n67.6\nqD\n17.2\n80.2\n82.1\n61.7\nBERT\n21.4\n70.6\n82.9\n63.1\n32.2\nqS\n28.3\n70.4\n83.8\n66.4\nqD\n14.8\n70.8\n82.1\n59.9\nDistilBERT\n23.0\n86.4\n75.9\n61.0\n5.8\nqS\n32.7\n86.4\n77.3\n67.5\nqD\n13.7\n86.5\n74.6\n54.8\nELECTRA\n21.1\n81.3\n81.0\n60.1\n19.1\nqS\n31.2\n82.1\n80.6\n65.0\nqD\n11.5\n80.5\n81.4\n55.4\nMiniLM\n23.3\n73.3\n82.4\n63.4\n5.0\nqS\n32.4\n73.6\n82.6\n66.1\nqD\n14.6\n73.0\n82.2\n60.9\nRoBERTa\n24.6\n60.2\n84.0\n65.2\n11.6\nqS\n32.8\n61.0\n84.3\n70.1\nqD\n16.8\n59.4\n83.7\n60.5\nThe table reports performance results for all question-answer pairs as\nwell as for SRS-based (qS) and domain-based (qD) pairs separately.\nrepresentative indicator for non-Wikipedia content.\nIn Table IV, we further provide a breakdown of the READER\nresults based on the origin of the question-answer pairs. We\ndenote SRS-based questions as qS and domain-based questions\n(which, in our case study, are sourced from Wikipedia) as\nqD. The table shows that all models achieve on-par or better\naccuracy over qS compared to qD. Based on the breakdown\nin Table IV, we conclude that the exposure of BERT-based\nmodels to Wikipedia during pre-training is unlikely to have\ninﬂuenced our performance results.\nThe answer to RQ2 is that considering both lexical and\nsemantic measures, ALBERT provides the best overall\ntrade-off for answer extraction with an average accuracy of\n≈24% in the exact matching mode, ≈79% in the partial\nmatching mode, and ≈84% in the semantic matching mode.\nRQ3. Does QAssist run in practical time? To answer RQ3,\nwe discuss the execution time of our approach based on the\nconclusions from RQ1 and RQ2 and the setup described under\nEXPIII in Section IV-D. Based on RQ1, we select BM25 as\nthe document RETRIEVER and the reranking method as the\npassage RETRIEVER. For answer extraction, based on RQ2,\nwe select ALBERT as the READER. With these choices, we\nreport the execution time for each step of QAssist (Fig. 2).\nRetrieving the most relevant document from the corpora cre-\nated for the aerospace, defence, and security domains (step 1)\nrequires 2.06, 1.37, and 0.08 seconds, respectively. The time\nrequired in step 2 for splitting a document into tokens and\nsentences is comparatively negligible. For retrieving relevant\npassages in step 3, we note that the six SRSs in our study\nvary in size from small (SRS#6 with 32 requirements) to large\n(SRS#2 with 1041 requirements). Similarly, the Wikipedia\narticles (making up the domain-speciﬁc corpora) from which\nwe retrieve passages vary in size, as shown previously in\nTable I. For our dataset, the time required for retrieving\npassages from an SRS is 2.27 seconds for the smallest SRS and\n6.43 seconds for the largest. For corpus articles, the average\ntime for passage retrieval is 2.62 seconds. Extracting answers\nfrom passages, i.e., step 4, takes an average of 1.1 seconds.\nIn addition to the above-reported execution times, there is\na one-time loading overhead for the READER, as shown in\nthe last column of Table IV. For ALBERT (best READER\nfrom RQ2), this overhead is ≈3.2 minutes. We deem this\noverhead acceptable considering that, once the READER has\nbeen loaded, the user can ask as many questions as desired.\nExcluding the overhead for loading the READER, answering\nan individual question, when averaged across all questions in\nour dataset, takes 10.36 seconds. We believe this execution\ntime is reasonable for most practical situations. Moreover, the\nexecution time can be improved if one has access to more\npowerful computing resources than ours (Google Colab’s free\nplan, as noted in Section IV-D).\nWhen run on Google Colab’s free plan, our approach\ntakes an average of 10.36 seconds to answer an individual\nquestion. In addition, one has to provision for a one-time\noverhead of 3.2 minutes to load the required language\nmodel (ALBERT). We ﬁnd this level of performance practi-\ncal for question answering over requirements. Performance\ncan be further improved with more powerful computational\nresources for language models.\nV. COMPARISON WITH BROAD-BASED SEARCH ENGINES\nAn intuitive way for QA during the analysis of an SRS\nwould be to pose the questions to a (broad-based) search en-\ngine such as Google. In the context of our work, search engines\nare generally not very effective for two main reasons. First,\nanswers to domain-speciﬁc questions can reside in company-\nspeciﬁc documents which are unlikely to be accessible to\nsearch engines. Our approach, in contrast, gives analysts\nthe possibility to plug company-speciﬁc documents into the\nQA system. Second, the lack of domain-speciﬁcity in search\nengines can easily result in misleading answers. For example,\nan online search for “rocket mass” instead of “wet mass” to\nanswer Q1 in Fig. 1 would point the analyst to the design\nof a rocket mass heater1, which is not relevant to the space\ndomain. Unlike search engines, our approach is scoped to the\noriginal SRS and any external knowledge resources selected\nby the user. As such, questions are implicitly disambiguated\nas long as the external knowledge resources are domain-\nspeciﬁc. To further illustrate, consider the question “What is\nNEAT?”. Posing this question online would lead to irrelevant\n1https://en.wikipedia.org/wiki/Rocket mass heater\nresults due to the ambiguous abbreviation, whereas posing the\nsame question to our approach would retrieve the deﬁnition of\n“Near-Earth Asteroid Tracking” – inline with the SRS content.\nTo empirically assess the success rate of search engines in\nour problem context, we posed to Google from our dataset a\ntotal of 50 verbatim questions. Of these, 20 questions were\nSRS-based and 30 were domain-based. The authors inde-\npendently investigated whether the top-3 retrieved documents\nby Google contained the correct answer as per our ground\ntruth. Out of the 50 questions, we found that 16 questions\nwere answered correctly by Google, leading to a success\nrate of 32%. From the 16 correctly answered questions, 14\nwere domain-based. We note that the domain-based questions\nin our dataset, REQuestA, originate from Wikipedia articles,\nwhich search engines have access to and can crawl. The out-\ncome would most likely have been different had the external\nknowledge resource not been public. Therefore, in addition to\nthe need for explicit disambiguation as discussed above, the\nsuccess rate of search engines is likely to be affected by the\npublic accessibility of the documents that should be considered\nduring QA. In conclusion, we believe that search engines are\ncurrently not the best alternative for QA over specialized and\nproprietary material – a situation that is common in RE.\nVI. THREATS TO VALIDITY\nThe validity concerns most pertinent to our evaluation are\ninternal and external validity.\nInternal Validity. The main concern regarding internal validity\nis dataset bias. To mitigate bias, the authors ensured that they\nwere not involved in dataset construction; this task was done\nexclusively by third parties (non-authors) who had no exposure\nto our technical solution.\nExternal Validity. Our evaluation is based on a dataset contain-\ning six industrial SRSs and spanning three different application\ndomains. The results we obtained across these SRSs and do-\nmains combined with the comparatively large size of our QA\ndataset provide conﬁdence about the generalizability of our\nempirical ﬁndings. Additional experimentation is nevertheless\nimportant to further mitigate external-validity threats.\nVII. RELATED WORK\nIn this section, we position our work in the existing literature\non QA as studied by the RE and NLP communities.\nQA in RE. There has been only limited research where QA\nis applied for addressing RE problems. Existing works focus\non requirements traceability [15]–[17], identifying compliance\nrequirements [19], [71], and extracting information from on-\nline forums [72]. These techniques are mostly IR-based, with\nthe exception of [19], which, like our approach, uses machine\nreading comprehension (MRC). Our approach differs from\n[19] both in its purpose and also in how it employs MRC.\nFirst, whereas [19] focuses on QA over legal provisions (e.g.,\nprivacy regulations), our approach deals with QA over SRSs.\nSecond, [19] is limited in that it applies MRC to a-priori-\nspeciﬁed documents only. Our approach can, in contrast, mine\ndomain-related content from Wikipedia in an attempt to make\ntacit domain knowledge explicit and thereby handle questions\nthat would go unanswered if the scope of search for answers\nwas limited to the SRS under analysis only.\nIn terms of QA datasets, not many such datasets are avail-\nable in RE. Abualhaija et al.’s dataset of 107 question-and-\nanswer pairs [19] is built over legal documents. In contrast,\nour dataset, REQuestA, is built over SRSs. To our knowledge,\nREQestA is the ﬁrst dataset of its kind, providing a total of\n387 question-and-answer pairs on industrial requirements.\nMalviya et al. [18] investigate questions that requirements\nengineers typically ask throughout the development process.\nThey collect through a survey with industry practitioners a set\nof 159 questions, grouped into nine different categories such\nas project management and quality assessment. Malviya et al.’s\nquestions are broad and can crosscut several artifacts in\nthe development life cycle. Our work focuses speciﬁcally\non clariﬁcation questions asked over SRSs and associated\ndomain-knowledge resources; our objective here is developing\nautomated QA technologies that can answer such questions.\nQA in NLP. QA tasks in the NLP literature include question\nclassiﬁcation, answer extraction, and question generation [73]–\n[76]. Answer extraction is considered to be the main QA\ntask in NLP [77]. Recent advances in QA answer extraction\ninclude ﬁne-tuning large scale language models such as BERT,\nRoBERTa, and ALBERT [78]–[81]. Inspired by the NLP liter-\nature, we apply in our work the QA models reported in a recent\nQA benchmark [35]. Several existing QA datasets curated\nfrom generic text are publicly available. These datasets include\nSQuAD [21], GLUE [82], and TriviaQA [22]. There are also\nsome domain-speciﬁc datasets, e.g., for the medical [25] and\nrailway [27] domains. For the same reasons mentioned earlier\nwhen discussing related work in RE, none of the available\ndatasets in NLP are suitable for our needs in this paper.\nLanguage models have been employed for various text\ngeneration tasks [64], including question generation (QG) [50],\n[83]. QG models have enabled researchers in many ﬁelds to\nautomatically generate their own synthetic QA datasets [84]–\n[87]. Our dataset was partially generated using QG. To our\nknowledge, QG has not been attempted in RE before.\nOur work is distinguished from QA research in NLP in that\nwe provide an end-to-end solution. Our approach covers all\nQA steps starting from posing a question down to providing\nthe most relevant passages and potential answers. Foundational\nresearch in NLP often focuses on individual QA steps, e.g.,\nIR-based text retrieval or MRC-based answer extraction. Our\nwork does not contribute to the foundations for QA. Nev-\nertheless, our motivating use case (QA over requirements),\nour combination of NLP technologies, the ﬂexibility to build\ndomain-speciﬁc corpora and consult them during QA, and our\nextensive empirical evaluation of QA in an RE context are, to\nthe best of our knowledge, new.\nVIII. CONCLUSION\nIn this paper, we proposed QAssist – an AI-based question-\nanswering (QA) system to support the analysis of natural-\nlanguage requirements. Given a question, QAssist retrieves\nrelevant text passages from both the requirements document\nbeing analyzed as well as an external source of domain\nknowledge. QAssist further highlights the likely answer to\nthe question in each retrieved text passage. The ﬂexibility to\nincorporate an external knowledge source into the QA process\nenables QAssist to answer otherwise unanswerable questions\nrelated to the tacit domain information assumed by the re-\nquirements. When a domain-knowledge resource is absent,\nQAssist automatically builds one by mining Wikipedia articles,\nusing the terminology in the requirements being analyzed to\nguide the mining process. To evaluate QAssist, we created\nthrough third-party annotators a QA dataset, named REQuestA.\nBoth QAssist and REQuestA are publicly available [13]. Our\nempirical results indicate that QAssist localizes the answer to\na posed question to three passages within the requirements\ndocument and within the external domain-knowledge resource\nwith an average recall of 90.1% and 96.5%, respectively.\nNarrowing the scope to these passages, QAssist has an average\naccuracy of 84.2% in pinpointing the actual answer.\nIn future work, we would like to conduct user studies to\nbetter understand how practitioners would interact with re-\nquirements documents when equipped with a QA tool. Another\nfuture direction is to experiment with emerging QA methods\nin NLP that are capable of producing a “no answer” outcome\nwhen a question is not answerable with sufﬁcient accuracy.\nAcknowledgements. This work was funded by Luxem-\nbourg’s National Research Fund (FNR) under the grant\nBRIDGES18/IS/12632261 and NSERC of Canada under the\nDiscovery and Discovery Accelerator programs. We are grate-\nful to the research and development team at QRA Corp. for\nvaluable insights and assistance.\nREFERENCES\n[1] A. van Lamsweerde, Requirements Engineering: From System Goals to\nUML Models to Software Speciﬁcations, 1st ed.\nWiley, 2009.\n[2] K. Pohl, Requirements Engineering, 1st ed.\nSpringer, 2010.\n[3] L. Zhao, W. Alhoshan, A. Ferrari, K. J. Letsholo, M. A. Ajagbe, E.-\nV. Chioasca, and R. T. Batista-Navarro, “Natural language processing\n(nlp) for requirements engineering: A systematic mapping study,” arXiv\npreprint arXiv:2004.01099, 2020.\n[4] A. Ferrari and A. Esuli, “An NLP approach for cross-domain ambiguity\ndetection in requirements engineering,” Automated Software Engineer-\ning, vol. 26, no. 3, 2019.\n[5] S. Ezzini, S. Abualhaija, C. Arora, M. Sabetzadeh, and L. C. Briand,\n“Using domain-speciﬁc corpora for improved handling of ambiguity in\nrequirements,” in 2021 IEEE/ACM 43rd International Conference on\nSoftware Engineering, 2021.\n[6] F. Dalpiaz, I. Schalk, and G. Lucassen, “Pinpointing ambiguity and\nincompleteness in requirements engineering via information visualiza-\ntion and NLP,” in Proceedings of the 24th Working Conference on\nRequirements Engineering: Foundation for Software Quality, 2018.\n[7] C. Arora, M. Sabetzadeh, and L. C. Briand, “An empirical study on\nthe potential usefulness of domain models for completeness checking\nof requirements,” Empirical Software Engineering, vol. 24, no. 4, pp.\n2509–2539, 2019.\n[8] I. Hadar, A. Zamansky, and D. M. Berry, “The inconsistency between\ntheory and practice in managing inconsistency in requirements engineer-\ning,” Empirical Software Engineering, vol. 24, no. 6, pp. 3972–4005,\n2019.\n[9] D. Jurafsky and J. H. Martin, Speech and Language Processing, 3rd ed.,\n2020, https://web.stanford.edu/∼jurafsky/slp3/(visited 2021-06-04).\n[10] C. Arora, M. Sabetzadeh, L. Briand, and F. Zimmer, “Automated extrac-\ntion and clustering of requirements glossary terms,” IEEE Transactions\non Software Engineering, vol. 43, no. 10, 2017.\n[11] S. Ezzini, S. Abualhaija, and M. Sabetzadeh, “Wikidominer: Wikipedia\ndomain-speciﬁc miner,” in Proceedings of the 17th joint meeting of the\nEuropean Software Engineering Conference and the ACM SIGSOFT\nSymposium on the Foundations of Software Engineering, 2022.\n[12] F. Zhu, W. Lei, C. Wang, J. Zheng, S. Poria, and T.-S. Chua, “Re-\ntrieving and reading: A comprehensive survey on open-domain question\nanswering,” arXiv preprint arXiv:2101.00774, 2021.\n[13] “Replication package,” 2022. [Online]. Available: https://gitlab.uni.lu/\nsezzini/QAssist/\n[14] J. I. Maletic and M. L. Collard, “Tql: A query language to support\ntraceability,” in 2009 ICSE workshop on traceability in emerging forms\nof software engineering.\nIEEE, 2009, pp. 16–20.\n[15] P. M¨ader and J. Cleland-Huang, “A visual language for modeling and\nexecuting traceability queries,” Software & Systems Modeling, vol. 12,\nno. 3, pp. 537–553, 2013.\n[16] P. Pruski, S. Lohar, W. Goss, A. Rasin, and J. Cleland-Huang, “Tiqi:\nanswering unstructured natural language trace queries,” Requirements\nEngineering, vol. 20, no. 3, pp. 215–232, 2015.\n[17] J. Lin, Y. Liu, J. Guo, J. Cleland-Huang, W. Goss, W. Liu, S. Lohar,\nN. Monaikul, and A. Rasin, “Tiqi: A natural language interface for\nquerying software project data,” in 2017 32nd IEEE/ACM International\nConference on Automated Software Engineering. IEEE, 2017, pp. 973–\n977.\n[18] S. Malviya, M. Vierhauser, J. Cleland-Huang, and S. Ghaisas, “What\nquestions do requirements engineers ask?” in 2017 IEEE 25th Interna-\ntional Requirements Engineering Conference. IEEE, 2017, pp. 100–109.\n[19] S. Abualhaija, C. Arora, A. Sleimi, and L. Briand, “Automated question\nanswering for improved understanding of compliance requirements: A\nmulti-document study,” in In Proceedings of the 30th IEEE International\nRequirements Engineering Conference, Melbourne, Australia 15-19 Au-\ngust 2022, 2022.\n[20] M. A. C. Soares and F. S. Parreiras, “A literature review on question\nanswering techniques, paradigms and systems,” Journal of King Saud\nUniversity-Computer and Information Sciences, vol. 32, no. 6, pp. 635–\n646, 2020.\n[21] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\nquestions\nfor\nmachine\ncomprehension\nof\ntext,”\narXiv\npreprint\narXiv:1606.05250, 2016.\n[22] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, “Triviaqa: A large\nscale distantly supervised challenge dataset for reading comprehension,”\narXiv preprint arXiv:1705.03551, 2017.\n[23] T. Kwiatkowski, J. Palomaki, O. Redﬁeld, M. Collins, A. Parikh,\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., “Natural\nquestions: a benchmark for question answering research,” Transactions\nof the Association for Computational Linguistics, vol. 7, pp. 453–466,\n2019.\n[24] A. Pampari, P. Raghavan, J. Liang, and J. Peng, “emrqa: A large corpus\nfor question answering on electronic medical records,” in Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language\nProcessing, 2018, pp. 2357–2368.\n[25] J. He, M. Fu, and M. Tu, “Applying deep matching networks to chinese\nmedical question answering: a study and a dataset,” BMC medical\ninformatics and decision making, vol. 19, no. 2, pp. 91–100, 2019.\n[26] Y. Tian, W. Ma, F. Xia, and Y. Song, “Chimed: A chinese medical corpus\nfor question answering,” in Proceedings of the 18th BioNLP Workshop\nand Shared Task, 2019, pp. 250–260.\n[27] Z. Hu, “Research and implementation of railway technical speciﬁcation\nquestion answering system based on deep learning,” in 2020 IEEE\n5th Information Technology and Mechatronics Engineering Conference\n(ITOEC), 2020, pp. 5–9.\n[28] D. Chen, A. Fisch, J. Weston, and A. Bordes, “Reading Wikipedia\nto answer open-domain questions,” in Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics.\nAssociation\nfor Computational Linguistics, 2017, pp. 1870–1879.\n[29] M. McGill and G. Salton, Introduction to Modern Information Retrieval.\nMcGraw-Hill, 1983.\n[30] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” 2018.\n[31] S. Liu, X. Zhang, S. Zhang, H. Wang, and W. Zhang, “Neural machine\nreading comprehension: Methods and trends,” Applied Sciences, vol. 9,\nno. 18, p. 3698, 2019.\n[32] C. Manning, P. Raghavan, and H. Schutze, Introduction to Information\nRetrieval, 1st ed.\nCambridge University Press, 2008.\n[33] K. S. Jones, “A statistical interpretation of term speciﬁcity and its\napplication in retrieval,” Journal of documentation, 1972.\n[34] S. Robertson and H. Zaragoza, The probabilistic relevance framework:\nBM25 and beyond.\nNow Publishers Inc, 2009.\n[35] N. Thakur, N. Reimers, A. R¨uckl´e, A. Srivastava, and I. Gurevych, “Beir:\nA heterogenous benchmark for zero-shot evaluation of information\nretrieval models,” arXiv preprint arXiv:2104.08663, 2021.\n[36] R. Nogueira and K. Cho, “Passage re-ranking with bert,” arXiv preprint\narXiv:1901.04085, 2019.\n[37] K. Wang, N. Thakur, N. Reimers, and I. Gurevych, “Gpl: Generative\npseudo labeling for unsupervised domain adaptation of dense retrieval,”\narXiv preprint arXiv:2112.07577, 2021.\n[38] S. Zhuang and G. Zuccon, “Dealing with typos for bert-based passage\nretrieval and ranking,” in Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, 2021, pp. 2836–\n2842.\n[39] D. Chen and W.-t. Yih, “Open-domain question answering,” in Proceed-\nings of the 58th Annual Meeting of the Association for Computational\nLinguistics: Tutorial Abstracts.\nOnline: Association for Computational\nLinguistics, 2020, pp. 34–37.\n[40] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\nH. K¨uttler, M. Lewis, W.-t. Yih, T. Rockt¨aschel et al., “Retrieval-\naugmented generation for knowledge-intensive nlp tasks,” Advances in\nNeural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\n[41] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Trans-\nactions on knowledge and data engineering, vol. 22, no. 10, pp. 1345–\n1359, 2009.\n[42] F. Petroni, T. Rockt¨aschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller,\nand S. Riedel, “Language models as knowledge bases?” arXiv preprint\narXiv:1909.01066, 2019.\n[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” arXiv preprint\narXiv:1706.03762, 2017.\n[44] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra: Pre-\ntraining text encoders as discriminators rather than generators,” arXiv\npreprint arXiv:2003.10555, 2020.\n[45] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n“Albert: A lite bert for self-supervised learning of language representa-\ntions,” arXiv preprint arXiv:1909.11942, 2019.\n[46] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled\nversion of bert: smaller, faster, cheaper and lighter,” arXiv preprint\narXiv:1910.01108, 2019.\n[47] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou, “Minilm:\nDeep self-attention distillation for task-agnostic compression of pre-\ntrained transformers,” Advances in Neural Information Processing Sys-\ntems, vol. 33, pp. 5776–5788, 2020.\n[48] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert\npretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\n[49] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: A\nsurvey,” International Journal of Computer Vision, vol. 129, no. 6, pp.\n1789–1819, 2021.\n[50] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer,” 2019.\n[51] D. Milne, O. Medelyan, and I. Witten, “Mining domain-speciﬁc\nthesauri from wikipedia: A case study,” in Proceedings of the 5th\nIEEE/WIC/ACM International Conference on Web Intelligence (WI 2006\nMain Conference Proceedings)(WI’06), 2006.\n[52] G. Cui, Q. Lu, W. Li, and Y. Chen, “Corpus exploitation from Wikipedia\nfor ontology construction,” in Proceedings of the Sixth International\nConference on Language Resources and Evaluation (LREC’08).\nMar-\nrakech, Morocco: European Language Resources Association (ELRA),\nMay 2008.\n[53] A. Ferrari, G. O. Spagnolo, and S. Gnesi, “Pure: A dataset of public re-\nquirements documents,” in 2017 IEEE 25th International Requirements\nEngineering Conference, 2017.\n[54] K. Saxena, T. Singh, A. Patil, S. Sunkle, and V. Kulkarni, “Leveraging\nWikipedia navigational templates for curating domain-speciﬁc fuzzy\nconceptual bases,” in Proceedings of the Second Workshop on Data\nScience with Human in the Loop: Language Advances.\nOnline:\nAssociation for Computational Linguistics, Jun. 2021, pp. 1–7.\n[55] T. Kluyver, B. Ragan-Kelley, F. P´erez, B. Granger, M. Bussonnier,\nJ. Frederic, K. Kelley, J. Hamrick, J. Grout, S. Corlay, P. Ivanov,\nD. Avila, S. Abdalla, and C. Willing, “Jupyter notebooks – a publishing\nformat for reproducible computational workﬂows,” in Positioning and\nPower in Academic Publishing: Players, Agents and Agendas, 2016.\n[56] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi,\nP. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer,\nP. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger,\nM. Drame, Q. Lhoest, and A. M. Rush, “Transformers: State-of-the-art\nnatural language processing,” in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing: System Demon-\nstrations.\nAssociation for Computational Linguistics, 2020.\n[57] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,\n“Scikit-learn: Machine learning in Python,” Journal of Machine Learn-\ning Research, vol. 12, pp. 2825–2830, 2011.\n[58] B. Dorian, J. Sarthak, N. V´ıt, and nlp4whp, “dorianbrown/rank bm25,”\n2022. [Online]. Available: https://doi.org/10.5281/zenodo.6106156\n[59] N. Thakur, N. Reimers, A. R¨uckl´e, A. Srivastava, and I. Gurevych,\n“BEIR: A heterogeneous benchmark for zero-shot evaluation of informa-\ntion retrieval models,” in Thirty-ﬁfth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track (Round 2), 2021.\n[60] “Hugging face,” 2022. [Online]. Available: https://huggingface.co/\n[61] J. Goldsmith, “The wikipedia libray,” 2022. [Online]. Available:\nhttps://pypi.org/project/wikipedia/\n[62] E. Loper and S. Bird, “Nltk: The natural language toolkit,” in Proceed-\nings of the ACL-02 Workshop on Effective Tools and Methodologies for\nTeaching Natural Language Processing and Computational Linguistics,\n2002.\n[63] X. Du and C. Cardie, “Identifying where to focus in reading comprehen-\nsion for neural question generation,” in Proceedings of the 2017 Confer-\nence on Empirical Methods in Natural Language Processing.\nCopen-\nhagen, Denmark: Association for Computational Linguistics, 2017, pp.\n2067–2073.\n[64] L. Pan, W. Lei, T.-S. Chua, and M.-Y. Kan, “Recent advances in neural\nquestion generation,” arXiv preprint arXiv:1905.08949, 2019.\n[65] G. Miller, “WordNet: A lexical database for English,” Communications\nof the ACM, vol. 38, no. 11, 1995.\n[66] M. Hanna and O. Bojar, “A ﬁne-grained analysis of BERTScore,” in\nProceedings of the Sixth Conference on Machine Translation.\nOnline:\nAssociation for Computational Linguistics, 2021, pp. 507–517.\n[67] D. Ramage, A. N. Rafferty, and C. D. Manning, “Random walks for text\nsemantic similarity,” in Proceedings of the 2009 workshop on graph-\nbased methods for natural language processing (TextGraphs-4), 2009,\npp. 23–31.\n[68] B. B. Cambazoglu, M. Sanderson, F. Scholer, and B. Croft, “A review of\npublic datasets in question answering research,” in ACM SIGIR Forum,\nvol. 54, no. 2.\nACM New York, NY, USA, 2021, pp. 1–23.\n[69] J. S. Whissell and C. L. Clarke, “Improving document clustering using\nokapi bm25 feature weighting,” Information retrieval, vol. 14, no. 5, pp.\n466–487, 2011.\n[70] J. Risch, T. M¨oller, J. Gutsch, and M. Pietsch, “Semantic answer\nsimilarity for evaluating question answering models,” arXiv preprint\narXiv:2108.06130, 2021.\n[71] A. Sleimi, M. Ceci, N. Sannier, M. Sabetzadeh, L. Briand, and J. Dann,\n“A query system for extracting requirements-related information from\nlegal texts,” in 27th IEEE International Requirements Engineering\nConference.\nIEEE, 2019.\n[72] G. M. Kanchev, P. K. Murukannaiah, A. K. Chopra, and P. Sawyer, “Ca-\nnary: an interactive and query-based approach to extract requirements\nfrom online forums,” in 2017 IEEE 25th International Requirements\nEngineering Conference.\nIEEE, 2017, pp. 470–471.\n[73] T. Hao, X. Li, Y. He, F. L. Wang, and Y. Qu, “Recent progress\nin leveraging deep learning methods for question answering,” Neural\nComputing and Applications, pp. 1–19, 2022.\n[74] A. A. Yusuf, F. Chong, and M. Xianling, “An analysis of graph con-\nvolutional networks and recent datasets for visual question answering,”\nArtiﬁcial Intelligence Review, pp. 1–24, 2022.\n[75] H. Jin, Y. Luo, C. Gao, X. Tang, and P. Yuan, “Comqa: Question\nanswering over knowledge base via semantic matching,” IEEE Access,\nvol. 7, pp. 75 235–75 246, 2019.\n[76] D. Diefenbach, A. Both, K. Singh, and P. Maret, “Towards a question\nanswering system over the semantic web,” Semantic Web, vol. 11, no. 3,\npp. 421–439, 2020.\n[77] B. Ojokoh and E. Adebisi, “A review of question answering systems,”\nJournal of Web Engineering, vol. 17, no. 8, pp. 717–758, 2018.\n[78] L. Jing, C. Gulcehre, J. Peurifoy, Y. Shen, M. Tegmark, M. Soljacic, and\nY. Bengio, “Gated orthogonal recurrent units: On learning to forget,”\nNeural computation, vol. 31, no. 4, pp. 765–783, 2019.\n[79] A. Wulamu, Z. Sun, Y. Xie, C. Xu, and A. Yang, “An improved end-to-\nend memory network for qa tasks,” CMC-COMPUTERS MATERIALS\n& CONTINUA, vol. 60, no. 3, pp. 1283–1295, 2019.\n[80] Q. Ren, X. Cheng, and S. Su, “Multi-task learning with generative\nadversarial training for multi-passage machine reading comprehension,”\nin Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 34,\nno. 05, 2020, pp. 8705–8712.\n[81] T. Parshakova, F. Rameau, A. Serdega, I. S. Kweon, and D.-S. Kim, “La-\ntent question interpretation through variational adaptation,” IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, vol. 27,\nno. 11, pp. 1713–1724, 2019.\n[82] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n“Glue: A multi-task benchmark and analysis platform for natural lan-\nguage understanding,” arXiv preprint arXiv:1804.07461, 2018.\n[83] V. Kumar, Y. Hua, G. Ramakrishnan, G. Qi, L. Gao, and Y.-F. Li,\n“Difﬁculty-controllable multi-hop question generation from knowledge\ngraphs,” in International Semantic Web Conference.\nSpringer, 2019,\npp. 382–398.\n[84] N. F. Liu, T. Lee, R. Jia, and P. Liang, “Can small and synthetic\nbenchmarks drive modeling innovation? a retrospective study of question\nanswering modeling approaches,” arXiv preprint arXiv:2102.01065,\n2021.\n[85] M. Bartolo, T. Thrush, R. Jia, S. Riedel, P. Stenetorp, and D. Kiela, “Im-\nproving question answering model robustness with synthetic adversarial\ndata generation,” arXiv preprint arXiv:2104.08678, 2021.\n[86] A. D. Lelkes, V. Q. Tran, and C. Yu, “Quiz-style question generation\nfor news stories,” in Proceedings of the Web Conference 2021, 2021,\npp. 2501–2511.\n[87] S. Gupta, A. Agarwal, M. Gaur, K. Roy, V. Narayanan, P. Kumaraguru,\nand A. Sheth, “Learning to automate follow-up question generation using\nprocess knowledge for depression triage on reddit posts,” arXiv preprint\narXiv:2205.13884, 2022.\n",
  "categories": [
    "cs.SE"
  ],
  "published": "2023-02-09",
  "updated": "2023-02-09"
}