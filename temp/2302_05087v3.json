{
  "id": "http://arxiv.org/abs/2302.05087v3",
  "title": "Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models",
  "authors": [
    "Yang Liu",
    "Dingkang Yang",
    "Yan Wang",
    "Jing Liu",
    "Jun Liu",
    "Azzedine Boukerche",
    "Peng Sun",
    "Liang Song"
  ],
  "abstract": "Video Anomaly Detection (VAD) serves as a pivotal technology in the\nintelligent surveillance systems, enabling the temporal or spatial\nidentification of anomalous events within videos. While existing reviews\npredominantly concentrate on conventional unsupervised methods, they often\noverlook the emergence of weakly-supervised and fully-unsupervised approaches.\nTo address this gap, this survey extends the conventional scope of VAD beyond\nunsupervised methods, encompassing a broader spectrum termed Generalized Video\nAnomaly Event Detection (GVAED). By skillfully incorporating recent\nadvancements rooted in diverse assumptions and learning frameworks, this survey\nintroduces an intuitive taxonomy that seamlessly navigates through\nunsupervised, weakly-supervised, supervised and fully-unsupervised VAD\nmethodologies, elucidating the distinctions and interconnections within these\nresearch trajectories. In addition, this survey facilitates prospective\nresearchers by assembling a compilation of research resources, including public\ndatasets, available codebases, programming tools, and pertinent literature.\nFurthermore, this survey quantitatively assesses model performance, delves into\nresearch challenges and directions, and outlines potential avenues for future\nexploration.",
  "text": "Generalized Video Anomaly Event Detection: Systematic Taxonomy and\nComparison of Deep Models\nYANG LIU∗, Fudan University, China and University of Toronto, Canada\nDINGKANG YANG and YAN WANG, Fudan University, China\nJING LIU, Fudan University, China, University of British Columbia, Canada, and Duke Kunshan University, China\nJUN LIU, Singapore University of Technology and Design, Singapore\nAZZEDINE BOUKERCHE, University of Ottawa, Canada\nPENG SUN†, Duke Kunshan University, China\nLIANG SONG†, Fudan University, China\nVideo Anomaly Detection (VAD) serves as a pivotal technology in the intelligent surveillance systems, enabling the temporal or spatial\nidentification of anomalous events within videos. While existing reviews predominantly concentrate on conventional unsupervised\nmethods, they often overlook the emergence of weakly-supervised and fully-unsupervised approaches. To address this gap, this survey\nextends the conventional scope of VAD beyond unsupervised methods, encompassing a broader spectrum termed Generalized Video\nAnomaly Event Detection (GVAED). By skillfully incorporating recent advancements rooted in diverse assumptions and learning\nframeworks, this survey introduces an intuitive taxonomy that seamlessly navigates through unsupervised, weakly-supervised,\nsupervised and fully-unsupervised VAD methodologies, elucidating the distinctions and interconnections within these research\ntrajectories. In addition, this survey facilitates prospective researchers by assembling a compilation of research resources, including\npublic datasets, available codebases, programming tools, and pertinent literature. Furthermore, this survey quantitatively assesses\nmodel performance, delves into research challenges and directions, and outlines potential avenues for future exploration.\nCCS Concepts: • General and reference →Surveys and overviews; • Applied computing →Surveillance mechanisms; •\nInformation systems →Data streaming.\nAdditional Key Words and Phrases: Anomaly detection, video understanding, deep learning, intelligent surveillance system\nACM Reference Format:\nYang Liu, Dingkang Yang, Yan Wang, Jing Liu, Jun Liu, Azzedine Boukerche, Peng Sun, and Liang Song. 2024. Generalized Video\nAnomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models. 1, 1 (February 2024), 36 pages. https://doi.org/\nXXXXXXX.XXXXXXX\n∗This paper was revised by Yang Liu during his FDU-UofT Joint Ph.D. Training Program at the University of Toronto, Canada.\n† Prof. Peng Sun and Prof. Liang Song are the co-corresponding authors of this paper.\nAuthors’ addresses: Yang Liu, yang_liu20@fudan.edu.cn, Fudan University, Shanghai, 200433, China and University of Toronto, Toronto, M5S 1A1,\nOntario, Canada; Dingkang Yang, dkyang20@fudan.edu.cn; Yan Wang, yanwang19@fudan.edu.cn, Fudan University, Shanghai, 200433, China; Jing Liu,\njingliu19@fudan.edu.cn, Fudan University, Shanghai, 200433, China and University of British Columbia, Vancouver, V6T 1Z4, British Columbia, Canada and\nDuke Kunshan University, Suzhou, 215316, Jiangsu, China; Jun Liu, jun_liu@sutd.edu.sg, Singapore University of Technology and Design, Singapore, 487372,\nSingapore; Azzedine Boukerche, aboukerc@uOttawa.ca, University of Ottawa, Ottawa, K1N 6N5, Ontario, Canada; Peng Sun, peng.sun568@duke.edu,\nDuke Kunshan University, Suzhou, 215316, Jiangsu, China; Liang Song, songl@fudan.edu.cn, Fudan University, Shanghai, 200433, China.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2024 Association for Computing Machinery.\nManuscript submitted to ACM\nManuscript submitted to ACM\n1\narXiv:2302.05087v3  [cs.CV]  1 Feb 2024\n2\nYang Liu et al.\n1\nINTRODUCTION\nSurveillance cameras can sense environmental spatial-temporal information without contact and have been the primary\ndata collection tool for public services such as security protection [28], crime warning [158], and traffic management\n[101]. However, with the rapid development of smart cities and digital society, the number of surveillance cameras\nis growing explosively, making the ensuing video analysis a significant challenge. Traditional manual inspection is\ntime-consuming and laborious and may cause missing detections due to human visual fatigue [100], hardly coping with\nthe vast scale video stream. As the core technology of intelligent surveillance systems, Video Anomaly Detection (VAD)\naims to automatically analyze video patterns and locate abnormal events. Due to its potential application in unmanned\nfactories, self-driving vehicles, and secure communities, VAD has received wide attention from academia and industry.\nVAD in a narrow sense refers specifically to the unsupervised research paradigm that uses only normal videos\nto learn a normality model, abbreviated as UVAD. Such methods share the same assumption as the long-established\nAnomaly Detection (AD) tasks in non-visual data (e.g., time series [7] and graphs [117]) and images [61]. They assume\nthe normality model learned on normal samples cannot represent anomalous samples. Typically, UVAD consists of two\nphases, normality learning and downstream anomaly detection [50, 92, 101, 197]. UAVD shares a similar modeling process\nwith other AD tasks without predefining and collecting anomalies, following the open-world principle. In the real world,\nanomalies are diverse and rare, so they cannot be defined and fully collected in advance. Therefore, UVAD was favored\nby early researchers and was once considered the prevailing VAD paradigm. However, the definition of anomaly is\nidealistic, ignoring that normal events are diverse. It is also unrealistic to collect all possible regular events for modeling.\nIn addition, the learned UVAD model has difficulty maintaining a reasonable balance between representation and\ngeneralization power, either due to the insufficient representational that false-alarms unseen normal events as anomalies\nor the excessive generalization power that effectively reconstructs anomalous events. Numerous experiments [218] have\nshown that UVAD is valid for only simple scenarios. The model performance on complex datasets [92] is much inferior\nto that of simple single-scene videos [84, 107], which limits the application of VAD technicals in realistic scenarios.\nIn contrast, Weakly-supervised Abnormal Event Detection (WAED) departs from the ambiguous setting that all are\nanomalous except normal with a clearer definition for the anomaly that is more consistent with human consciousness\n(e.g., traffic accidents, robbery, stealing, and shooting). [158]. Given its potential for immediate references in real-\nlife applications such as traffic management platforms and violence warning systems, WAED has become another\nmainstream VAD paradigm since 2018 [39, 98, 164]. Generally, WAED models directly output anomaly scores by\ncomparing the spatial-temporal features of normal and abnormal events through Multiple Instance Learning (MIL).\nThe previous study [39] proved that WAED could understand the essential difference between normal and abnormal.\nTherefore, its results are more reliable than that of UVAD. Unfortunately, WAED does not follow the basic assumptions\nof AD tasks, which is more like a binary classification under unique settings (e.g., data imbalance and positive samples\ncontaining multiple subcategories). Therefore, existing reviews [12, 136, 139] mainly focus on UVAD and consider\nWAED as a marginal research pathway, lacking the systematic organization for WAED datasets and methods.\nIn recent times, certain researchers [129, 207] have introduced fully-unsupervised methods, which eliminate the\nneed for labels and eliminate any prerequisites on the training data. Given the deeply ingrained association of UVAD\nwith modeling using solely normal data, we retain the nomenclature UVAD for these techniques, rather than referring\nto them as semi-supervised VAD. Conversely, the emerging paradigm of absolute unsupervised setting is denoted as\nFVAD, contributing to terminology conceptual clarity in this survey and future research.\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n3\nGVAED\nUVAD\nSVAD\nWAED\nFVAD\nFrame-level\nPatch-level\nObject-level\nUnimodal\nMultimodal\nSingle-stream Models\nMulti-stream Models\nDatasets\nMetrics\nCodes\nLiterature\nChallenges\nTrends\nSupervision\nInput\nStructure\nTaxonomy of GVAED\nClassification Models\nMIL Ranking Models\nClassification models\nRegression models\nFrame-level label-based\nUnlabeled video-based\nResources\nDiscussions\nFig. 1. Taxonomy of Generalized Video Anomaly Event Detection (GVAED). We provide a hierarchical taxonomy that organizes\nexisting deep GVAED models by supervised signals, model inputs, and network structure into a systematic framework, including\nUnsupervised Video Anomaly Detection (UVAD), Weakly-supervised Abnormal Event Detection (WAED), Fully-unsupervised VAD\n(FVAD) and Supervised VAD (SVAD). Besides, we collate benchmark datasets, evaluation metrics, available codes, and literature to a\npublic GitHub repository1. Finally, we analyze the research challenges and possible trends.\nIn summary, this survey focuses on anomaly event detection in surveillance videos, integrating deep VAD methods\nbased on different assumptions, learning paradigms, and supervision into a systematic taxonomy: Generalized Video\nAnomaly Event Detection (GVAED), as shown in Fig. 1. We compare the differences and performance among different\nmethods, sorting out the recent advances in GVAED. In addition, we collate available research resources, such as\ndatasets, metrics, codes, and literature, into a public GitHub repository1. Moreover, we analyze the research challenges\nand future trends, which can guide further research and promote the development and applications of GVAED.\n1.1\nLiterature Statistics\nWe count the publications and citations of academic papers on the topic of Video Anomaly Detection and Abnormal\nEvent Detection in the past 12 years through reference databases (e.g., ACM Digital Library, IEEE explore, ScienceDirect,\nand SpringerLink) and search engines. The results are shown in Fig. 2, where the bar and line graph indicate the number\nof publications and citations, respectively. The lines in Fig. 2(a) and Fig. 2(b) show a steadily increasing trend, indicating\nthat GVAED has received wide attention. Therefore, a systematic taxonomy and comparison of GVAED methods are\nnecessary to guide further development. As mentioned above, current works focus on unsupervised methods that use\nonly regular videos to train models to represent normality. Thus, the development of UVAD is limited by representation\nmeans. Until 2016, UVAD utilized handicraft features, such as Local Binary Patterns (LBP) [56, 123, 216], Histogram Of\nGradients (HOG) [50, 116, 148], and Space-Time Interest Point (STIP) [34]. The performance is poor and relies on a priori\nknowledge. As a result, VAD developed slowly. Fortunately, after 2016, with the development of deep learning, especially\nthe application of Convolutional Neural Networks (CNNs) in image processing [20, 74, 193] and video understanding\n[194–196], VAD has ushered in new development opportunities. The research progress increased significantly, as\nshown in Fig 2(a). Deep CNNs can extract the video patterns end-to-end, freeing VAD research from complex a priori\n1 https://github.com/fudanyliu/GVAED.git\nManuscript submitted to ACM\n4\nYang Liu et al.\n（a）Video anomaly detection topic\n（b）Abnormal event detection topic\nFig. 2. Publication and citation statistics on the topic of (a) Video Anomaly Detection and (b) Abnormal Event Detection.\nTable 1. Analysis and Comparison of Related Reviews.\nYear\nRef.\nMain Focus\nResearch Pathways𝑎\nTopics𝑎,𝑏\nUVAD\nWAED\nSVAD\nFVAD\nLW\nOD\nCS\nOE\n2018\n[67]\nUnsupervised and semi-supervised methods.\n!\n%\n%\n%\n!\n%\n%\n%\n2019\n[106]\nWeakly-supervised VAD methods and applications.\n%\n!\n%\n%\n%\n%\n%\n%\n2020\n[139]\nUnsupervised single-scene video anomaly detection.\n!\n%\n!\n%\n!\n%\n!\n%\n2021\n[125]\nDeep learning driven unsupervised VAD methods.\n!\n%\n%\n%\n!\n%\n%\n%\n2021\n[144]\nUnsupervised crowd anomaly detection methods.\n!\n%\n%\n%\n%\n%\n%\n%\n2021\n[150]\nTraffic scene video anomaly detection.\n!\n%\n%\n%\n%\n%\n%\n%\n2022\n[136]\nUnsupervised video anomaly detection.\n!\n%\n%\n%\n%\n!\n%\n%\n2022\n[12]\nOne&two-class classification-based methods.\n!\n!\n!\n%\n!\n!\n%\n%\n2023\nOurs\nGVAED taxonomy, challenges and trends.\n!\n!\n!\n!\n!\n!\n!\n!\na: %means no systematic analysis, while !is vice versa. b: LW=lightweight, OD=online detection, CS=cross-scene, and OE=online evolution.\nknowledge construction. In addition, compared with manual features [34, 56, 148], deep representations can capture\nmulti-scale spatial semantic features and extract long-range temporal contextual features, which are more efficient in\nlearning video normality. On the one hand, the large amount of video generated by the surveillance cameras provides\nsufficient training data for deep GVAED models. On the other hand, the iteratively updated Graphics Processing Units\n(GPUs) make it possible to train large-scale models. As a result, VAD has developed rapidly in recent years and started\nto move from academic research to commercial applications. Similarly, Fig 2(b) reflects the research enthusiasm and\ndevelopment potential of abnormal event detection. To accelerate the application of GVAED in terminal devices and\ninspire future researchers, this survey organizes various GVAED models into a unified framework. Additionally, we\ncollect commonly used datasets, publicly available codes, and classic literature for further research.\n1.2\nRelated Reviews\nIn the past four years, several reviews [7, 12, 26, 64, 67, 106, 122, 125, 128, 136, 139, 144, 150, 154] have covered GVAED\nworks and generated various classification systems. We analyze the methodologies covered in recent reviews and the\nresearch topics related to real-world deployment, as shown in Table 1. The mainstream reviews [67, 139] still consider\nVAD as a narrow unsupervised task, lacking attention to WAED with excellent application value and FVAD methods\nusing unlimited training data. In addition, they are biased against Supervised Video Anomaly Detection (SVAD), arguing\nthat data labeling makes SVAD challenging to develop. However, the game engines [38, 152] and automatic annotations\n[42, 49] make it possible to obtain anomalous events and fine-grained labels. In addition, the existing review suffers\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n5\nfrom three major weaknesses: (1) [139] and [150] attempted to link existing works to specific scenes, missing the cross-\nscene challenges in real-world. Specifically, [139] pointed out that existing works were trained and tested on videos of\nthe same scene, so they only reviewed single-single methods, leaving out the latest cross-scene VAD research. [150]\nfocuses on the traffic VAD methods, innovatively analyzing the applicability of existing works in traffic scenes. However,\nweakly-supervised methods for crime and violence detection fail to be included in [150]. (2) Due to timeliness, earlier\nreviews [26, 125, 144, 150] were unable to cover the latest research and were outdated for predicting research trends.\nRecent surveys [12, 136] lack discussion of the interaction of GVAED with new techniques such as causal machine\nlearning [88, 103], domain adaptation [44, 183], and online evolutive learning [76, 77, 156], which are expected to be\nthe future directions of GVAED and essential to model deployment. (3) Although the latest review [12] in 2022 has\nstarted to incorporate WAED and SVAD, it still treats them as a marginal exploration, lacking a systematic organization\nof the datasets, literature, and trends.\n1.3\nContribution Summary\nGVAED will usher in new development opportunities with the rapid growth of deep learning technicals and surveillance\nvideos. To clarify the development of GVAED and inspire future research, this survey integrates UVAD, WAED, FVAD,\nand SVAD into a unified framework from an application perspective. The main contributions of this survey are\nsummarized in the following four aspects:\n(1) To the best of our knowledge, it is the first comprehensive survey that extends video anomaly detection from\nnarrow unsupervised methods to generalized video anomaly event detection. We analyze the various research\nroutes and clearly state the lineage and trends of deep GVAED models to help advance the field.\n(2) We organize various GVAED methods with different assumptions and learning frameworks from an application\nperspective, providing an intuitive taxonomy based on supervision signals, input data, and network structures.\n(3) This survey collects accessible datasets, literature, and codes and makes them publicly available. Moreover, we\nanalyze the potential applications of other deep learning techniques and structures in GVAED tasks.\n(4) We examine the research challenges and trends within GVAED in the context of the development of deep learning\ntechniques, which is anticipated to provide valuable guidance for upcoming researchers and engineers.\nThe remainder of this survey is organized as follows. Section 2 provides an overview of the basics and research\nbackground of GVAED, including the definition of anomalies, basic assumptions, main evaluation metrics, and benchmark\ndatasets. Sections 3-5 introduce the unsupervised, weakly-supervised, supervised and fully-unsupervised GVAED\nmethods, respectively. We analyze the extant methods’ general ideas and specific implementations and compare their\nstrengths and weaknesses. Further, we quantitatively compare the performance in Section 7. Section 8 analyzes the\nchallenges and research outlook on the development of GVAED. Section 9 concludes this survey.\n2\nFOUNDATIONS OF GVAED\n2.1\nDefinition of the Anomaly\nUVAD follows the assumption of general AD tasks [128] and considers all events that have not occurred in the training\nset as abnormal. In other words, the training set of the UVAD dataset contains only normal events, while videos in the\ntest set that differ from the training set are considered anomalies. Thus, certain normal events in the subjective human\nManuscript submitted to ACM\n6\nYang Liu et al.\n(a) UVAD\n(b) WAED\n(c) SVAD\n(d) FUVAD\nNormal frame\nAbnormal frame\nw/ frame-level label\nw/o frame-level label\nVideo-level label\nFig. 3. Illustration of training data. (a) UVAD trains the model using only normal data, with the hidden implication that all video-\nlevel and frame-level labels are 0. (b) WAED models use positive and negative samples and require frame-level labels, where 𝑌= 0\nindicates normal video and 𝑌= 1 indicates an anomaly. (c) SVAD is trained using a fine-grained frame-level labeling supervised\nmodel, where the semantics of the frame-level labels expose the video-level labels. (d) FVAD attempts to learn the anomaly detector\nfrom under-processed data with training data containing both normal and anomalous samples and without any level of labeling.\nconsciousness may also be labeled anomalies. For instance, in the UCSD Pedestrian datasets [84], riding a bicycle on\nthe college campus is labeled abnormal simply because the training set fails to contain such events. This seemingly odd\ndefinition is dictated by the diversity and rarity of real-world anomalies. Collecting a sufficient number of anomalous\nevents with a full range of categories is nearly impossible. In response, researchers have taken the alternative route of\ncollecting enough normal videos to train models to describe the boundary of normal patterns and treat events that fall\noutside the boundary as anomalies. Unfortunately, it is also costly to collect all possible normal events for training. In\naddition, abnormal and normal frames share most of the appearance and motion information, making their patterns\noverlap. Therefore, letting the model find a discriminative pattern boundary without seeing abnormal events is infeasible.\nIn contrast, WAED takes a more intuitive definition of anomalies. Events that are subjectively perceived as abnormal\nby humans are considered anomalies, such as thefts and traffic accidents [158]. The training set for WAED tasks contains\nboth normal and abnormal events and provides easily accessible video-level labels to supervise the model. Compared\nwith fine-grained frame-level labels, video-level labels only tell the model whether a video contains abnormal events\nwithout revealing the exact location of the abnormalities, avoiding the costly frame-by-frame labeling and providing\nmore reliable supervision. In contrast, the discrete frame-level annotations (0=normal, 1=abnormal) in SVAD ignore the\ntransition continuity from normal to abnormal events. WAED needs to predefine abnormal events so that it can only\ndistinguish specified abnormal events.\n2.2\nProblem Formation\nIn UVAD, the training data contains only normal events, as shown in Fig. 3(a). Such methods aim to describe the\nboundaries of normal spatial-temporal patterns with a proxy task and consider the test samples whose patterns fall\noutside the learned boundaries as anomalies [104]. Fig. 4 shows the two-stage anomaly detection framework in UVAD.\nThe deep network trained by performing the proxy task in the training phase is directly applied as a normality model\nfor anomaly detection in the testing phase. The performance of the proxy task is a credential to calculate the anomaly\nscore. Formulaically, the process of UVAD is as follows:\n𝑒= 𝑑(𝑓(𝒙test;𝜃), 𝒙test)\n(1)\nwhere 𝜃denotes the learnable parameters of the deep model 𝑓, designed to characterize the prototype of normal events.\n𝑑denotes the deviation between the test sample 𝑥test and the well-trained 𝑓, which is usually a quantifiable distance,\nsuch as the Mean Square Error (MSE) of the prediction result, the 𝐿2 distance in the feature space and the difference\nof the distribution [112, 131, 219]. Noting that the normality model is obtained by optimizing the proxy task. This\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n7\nData \nPre-processing\nNormality \nLearning\nWell-trained \nModel\nData \nPre-processing\nScore \nCalculation\nAnomaly Score\nTraning Phase: Normality Learning\nTesting Phase: Anomaly Detection\nFig. 4. Illustration of the two-stage UVAD framework. Anomaly detection is performed in the test phase as a downstream task of\nproxy task-based normality learning. The example video frames are from the CUHK Avenue [107] dataset.\nprocess is independent of the downstream anomaly detection, so the performance of the proxy task cannot directly\ndetermine the anomaly detection performance. In addition, for the reconstruction-based [46, 50] and prediction-based\n[92, 101] methods, the final anomaly score is usually a relative value in the range [0, 1]. A higher score indicates a\nlarger deviation. Generally, these methods convert the absolute deviation 𝑒into a relative anomaly score by performing\nmaximum-minimum normalization. They not only explicitly require all training data to be normal but also include the\nhidden assumption that the test videos must include anomalous events. In other words, any test video will yield high\nscore intervals, which indicates that such methods are offline and may produce false alarms for normal videos.\nWAED methods [39, 98, 164] always follow the MIL ranking framework. Fig. 3(b) shows the training data composition\nfor WAED, where both normal and anomalous events need to be pre-collected and labeled. Video-level labels are easy\nto obtain and often more accurate than the fine-grained frame-level labels for SVAD shown in Fig. 3(c). In a concrete\nimplementation, WAED treats the video as a bag containing several instances, as illustrated in Fig. 5. The normal video\nV𝑛forms a negative bag B𝑛, while the abnormal video V𝑎a positive bag B𝑎. Based on MIL, WVEAD aims to train a\nregression model 𝑟(·) to assign scores to instances, with the basic goal that the maximum score of B𝑎is higher than\nthat of B𝑛. Thus, the WAED methods do not rely on an additional self-supervised proxy task but compute anomaly\nscores directly. The objective function 𝑂(B𝑎, B𝑛) is as follows:\n𝑂(B𝑎, B𝑛) = min max\n\u0012\n0, 1 −max\n𝑖∈B𝑎\n𝑟\n\u0010\nV𝑖\n𝑎\n\u0011\n+ max\n𝑖∈B𝑛\n𝑟\n\u0010\nV𝑖\n𝑛\n\u0011\u0013\n+ 𝜆1\n𝐶𝑠𝑚𝑜𝑜𝑡ℎ\nz                            }|                            {\n𝑛−1\n∑︁\n𝑖\n\u0010\n𝑟\n\u0010\nV𝑖\n𝑎\n\u0011\n−𝑟\n\u0010\nV𝑖+1\n𝑎\n\u0011\u00112\n+𝜆2\n𝐶𝑠𝑝𝑎𝑟𝑠𝑖𝑡𝑦\nz      }|      {\n𝑛\n∑︁\n𝑖\n𝑟\n\u0010\nV𝑖\n𝑎\n\u0011\n(2)\nIn addition to the additional anomaly curve smoothness constraint 𝐶𝑠𝑚𝑜𝑜𝑡ℎand sparsity constraint 𝐶𝑠𝑝𝑎𝑟𝑠𝑖𝑡𝑦, the core\nof 𝑂(B𝑎, B𝑛) is to train a ranking model capable of distinguishing the spatial-temporal patterns between B𝑎and\nB𝑛. Subsequent WAED works [39, 98, 102, 164, 172, 223] have followed the idea of MIL ranking and made effective\nimprovements regarding feature extraction [223], label denoising [220], and the objective function [98]. However, as\nshown in Fig. 5, the MIL regression module takes the extracted feature representations as input, so the performance of\nWAED methods partially depends on the pre-trained feature extractor, making the calculation costly.\nThe training data of FVAD contains both normal and abnormal, and none of the data labels are available for model\ntraining, as shown in Fig. 3(d). One class of FVAD methods follows a similar workflow to that of UVAD, i.e., learning\nthe normality model directly from the original data. Although the training data contains anomalous events, the low\nManuscript submitted to ACM\n8\nYang Liu et al.\n…\nAnomaly Video\nNormal Video\nPretrained \nFeature Extractor\nMIL \nRegression\nAnomaly Score\n0.4, 0.8, 0.7, …, 0.1\nMIL Ranking Loss With Constraints \n…\nTest Video\n…\n0.1, 0.2, 0.2, …, 0.1\n0.2, 0.4, 0.7, …, 0.3\nDataflow for Test\nNegative Instance\nDataflow for Anomaly\nDataflow for Normal\nObjective Function\nPositive Instance\nTest Instance\nFig. 5. Structure of the MIL ranking model [158]. The anomalous video V𝑎and the normal video V𝑛are first sliced into several equal-\nsize instances. The positive bag B𝑎contains at least one positive instance, while the negative bag B𝑛contains only normal instances.\nIn the test phase, the well-trained MIL regression model output the anomaly scores of instances in the test video V𝑡directly.\nfrequency of anomalies limits their impact on model optimization. As a result, the model learned on many normal\nvideos and a small number of abnormal frames is still only effective in representing normal events and generates large\nerrors for anomalous events. Another class of methods tries to discover anomalies through the mutual collaboration of\nthe representation learner and anomaly detector. Generally, the learning process of FVAD can be formulated as follows:\nF = arg min\nΘ\n∑︁\n𝐼∈I\nL𝑓𝑜𝑐( ˆ𝑦= 𝜙(𝑚= 𝜑(𝑥= 𝑓(𝐼))),𝑙)\n(3)\nwhere the aim is to learn an anomaly detector F via a deep neural network which consists of a backbone network\n𝑓(·; Θ𝑏) : R𝐻×𝑊×3 ↦→R𝐷𝑏that transforms an input video frame 𝐼to feature 𝒙, an anomaly representation learner\n𝜑(·; Θ𝑎) : R𝐷𝑏↦→R𝐷𝑛that converts 𝑥to an anomaly specific representation 𝑚, and an anomaly score regression\nlayer 𝜙(·; Θ𝑠) : R𝐷𝑠↦→R that learns to predict 𝑚to an anomaly score 𝑦. The overall parameters Θ = {Θ𝑏, Θ𝑎, Θ𝑠}\nare optimized by the focal loss. Research on fully-unsupervised methods is still in its infancy, and they exploit the\nimbalance of samples and the significant difference of anomalies in the GVAED task.\n2.3\nBenchmark Datasets\nIn Table 2, we show and compare the statistical results and properties of the frequently used GVAED datasets. Several\ndatasets [1, 92, 158, 186] have been proposed with different annotated signals to match new research requirements\nafter 2018, which reflects the trend of GVAED from unsupervised to weakly-unsupervised [158], from unimodal to\nmultimodal [186] and from simple to complex real-world scenarios [92] at the data level.\n2.3.1\nSubway Entrance & Exit. As an earlier dataset, Subway [2] includes two independent sub-datasets, Entrance and\nExit, which record the subway entrance and exit scenes, respectively. The anomalous events include people who skip\nthe subway entrance to evade tickets, cleaners who behave differently from regular entry and exit, and people who\ntravel in the wrong direction. Due to the cursory nature of the labeling work and the lack of clarity in the definition of\nanomalous events, most existing works have refrained from using this dataset for model evaluation. Therefore, we do\nnot provide quantitative performance comparison results on this dataset but only briefly describe its characteristics to\nreflect the lineage of GVAED datasets development.\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n9\nTable 2. Representative GVAED Datasets. Italicized ones indicate WAED datasets, and underlined one is multimodal dataset.\nYear\nDataset\n#Videos\n#Frames\n#Scenes\n#Anomalies\nTotal\nTraining\nTesting\nTotal\nTraining\nTesting\nNormal\nAbnormal\n2008\nSubway Entrance1\n-\n-\n-\n144,250\n76,543\n67,797\n132,138\n12,112\n1\n51\n2008\nSubway Exit2\n-\n-\n-\n64,901\n22,500\n42,401\n60,410\n4,491\n1\n14\n2011\nUMN2†\n-\n-\n-\n7,741\n-\n-\n6,165\n1,576\n3\n11\n2013\nUCSD Ped13\n70\n34\n36\n14,000\n6,800\n7,200\n9,995\n4,005\n1\n61\n2013\nUCSD Ped24\n28\n16\n12\n4,560\n2,550\n2,010\n2,924\n1,636\n1\n21\n2013\nCUHK Avenue4\n37\n16\n21\n30,652\n15,328\n15,324\n26,832\n3,820\n1\n77\n2018\nShanghaiTech5\n-\n-\n-\n317,398\n274,515\n42,883\n300,308\n17,090\n13\n158\n2018\nUCF-Crime6\n1,900\n1,610\n290\n13,741,393\n12,631,211\n1,110,182\n-\n-\n-\n950\n2019\nShanghaiTech Weakly7\n437\n330\n107\n-\n-\n-\n-\n-\n-\n-\n2020\nStreet Scene8\n81\n46\n35\n203,257\n56,847\n146,410\n159,341\n43,916\n205\n17\n2020\nXD-Violance9\n4,754\n-\n-\n-\n-\n-\n-\n-\n-\n-\n2022\nUBnormal10 ‡\n543\n268\n211\n236,902\n116,087\n92,640\n147,887\n89,015\n29\n660\n† Following previous works, we set the frame rate to 15 fps. ‡ The UBnormal dataset is supervised and includes a validation set with 64 videos.\n2.3.2\nUMN. The UMN [28] is also an early GVAED dataset containing 11 short videos captured from three different\nscenes: grassland, indoor hall, and park. The scenes are set by the researcher rather than naturally filmed to detect\nabnormal crowd behavior in indoor and outdoor scenes, i.e., the crowd suddenly shifts from normal interaction to\nevacuation and flees abruptly to simulate fear. The anomalies are artificially conceived and played out, ignoring the\ndiversity and rarity of anomalies in the real-world. Similar to the Subway [2] dataset, UMN has been abandoned by\nrecent researchers due to the lack of spatial annotation.\n2.3.3\nUCSD Pedestrian. UCSD Ped1 & Ped2 [84] are the most widely used UVAD datasets collected from university\ncampuses with simple but realistic scenarios. They reflect the value of GVAED in public security applications. Specifically,\nthe Ped1 dataset is captured by a camera with a viewpoint perpendicular to the road, so the moving object’s size changes\nwith its spatial position. In contrast, the Ped2 dataset used a camera whose viewpoint is parallel to the direction of the\nroad, which is simpler than Ped1. Pedestrian walking is defined as normal, while behaviors and objects different from it\nare considered abnormal, such as biking, skateboarding, and driving. Since the scene is classical and anomalous events\nare easy to understand, UCSD Pedestrian is widely used by existing works, and the frame-level AUC has been as high\nas 99%, reflecting the saturation of model performance. Therefore, the dataset in simple scenes has become a constraint\nfor GVAED development. The large-scale and cross-scene datasets have become an inevitable trend.\n2.3.4\nCUHK Avenue. Similar to UCSD Pedestrian, the CUHK Avenue [107] dataset is also collected from the university\ncampus, and both focus on anomalous events that occur on the road outside of expectations. The difference is that most of\nthe 47 anomalous events in CUHK Avenue are simulated by the data collector, including appearance anomalies (e.g., bags\nplaced on the grass) and motion anomalies, such as throwing and wrong direction. CUHK Avenue provides both frame-\nlevel and pixel-level spatial annotations. In addition, its large data scale makes it one of the mainstream UVAD datasets.\n2.3.5\nShanghaiTech. The UCSD Pedestrian [84] and CUHK Avenue [107] datasets only consider anomalous events in\na single scene, while the real world usually faces the challenge of spatial-temporal pattern shifts across scenes. For\nthis reason, the team from ShanghaiTech University proposed the ShanghaiTech [92] dataset containing 13 scenes,\n1 https://vision.eecs.yorku.ca/research/anomalous-behaviour-data/sets/\n2 http://mha.cs.umn.edu/proj_events.shtml#crowd\n3 http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm\n4 http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html\n5 https://svip-lab.github.io/dataset/campus_dataset.html\n6 https://webpages.charlotte.edu/cchen62/dataset.html\n7 https://github.com/jx-zhong-for-academic-purpose/GCN-Anomaly-Detection/\n8 https://www.merl.com/demos/video-anomaly-detection\n9 https://roc-ng.github.io/XD-Violence/\n10 https://github.com/lilygeorgescu/UBnormal\nManuscript submitted to ACM\n10\nYang Liu et al.\nNormal\nShooting\nRiot\nExplosion\nCar accident\nFig. 6. Examples of XD-Violence dataset [186]. XD-Violence is a multimodal dataset for violence detection, including video and audio.\nWe show video frames here. The anomalous events are not all from the real-world but also include movie and game footage, etc.\nproviding the largest UVAD benchmark. Abnormal behaviors are defined as all collected behaviors that distinguish\nthem from normal walking, such as riding a bicycle, crossing a road, and jumping forward. Unfortunately, although\nthe collectors pointed out the shortcomings of the existing dataset with a single scenario, their proposed FFP [92] was\nnot explicitly designed to address the cross-scene challenges but rather to treat it as a whole without differentiating\nbetween scenarios. For the WAED setting, researchers [220] proposed to move some anomalous videos from the test\nset to the training set and provided video-level labels for each training video, introducing the ShanghaiTech Weakly\ndataset, which has become the mainstream WAED benchmark. A compelling phenomenon is that the performance of\nWAED methods on ShanghaiTech Weakly (frame-level AUC is typically > 85% and has reached up to 95%) is generally\nhigher than that of UVAD methods on the ShanghaiTech (frame-level AUC is typically between 70 ∼80%), providing\nevidence for the applicability of WAED in complex scenarios over UVAD.\n2.3.6\nUCF-Crime. UCF-Crime [158] is the first WAED dataset, presented together with the original MIL ranking\nframework. UCF-Crime consists of 1900 unedited real-world surveillance videos collected from the Internet. The\nabnormal events contain 850 anomalies of human concern in 13 categories: Abuse, Arrest, Arson, Assault, Burglary,\nExplosion, Fighting, Road Accidents, Robbery, Shooting, Shoplifting, Stealing, and Vandalism. Unlike the UVAD dataset\nabove, its training set contains anomalous videos and provides a video-level label for each video, where 0 indicates\nnormal, and 1 indicates anomalous. The anomalous events in the WAED dataset are predefined and are usually associated\nwith specific scenarios, such as car accidents in urban traffic, shoplifting, and shootings in neighborhoods. Therefore,\nWAED can provide more credible results for real scenarios with better application potential.\n2.3.7\nXD-Violence. As the first audio-video dataset, XD-Violence [186] expands anomaly event detection from single-\nmodal video understanding to multimodal signal processing, facilitating the coexistence of GVAED and multimedia\ncommunities. XD-Violence focuses on violent behaviors, such as abuse, explosion, car accident, struggle, shootings,\nand riots, as shown in Fig. 6. Due to the rarity of violent behaviors and the high difficulty of capturing violence, the\noriginal videos include some movie clips in addition to real-world surveillance videos. XD-Violence provides a new way\nto think about the GVAED by extending the data modality from single videos to sound, text, and others.\n2.3.8\nUBnormal. Inspired by the computer vision community benefiting from synthetic data, Acsintoae et al. [1]\npropose the first GVAED benchmark with virtual scenes, named UBnormal. Notably, utilizing a data engine to synthesize\ndata under predetermined instructions rather than collecting real-world data makes pixel-level labeling possible.\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n11\n(a) Metrics System\n/\nQualitative\nAccuracy\nCost\nTPR, FNR, FPR, TNR,\nAUROC, PROC, DR, EER\nPR Curve, AUC Curve,\nAnomaly Score Curve; \nPrediction Error Map\nParameter Size,\nInferrence Speed, FLOPs\nQuantitative\nTP\nFN\nFP\nTN\n1\n0\nNormal (0)\nGT Label\nPredicted Label\nTP\nFN\nTN\n(b) Confusion Matrix\nAbnormal (1)\nFig. 7. Illustration of GVAED performance evaluation system. We show the (a) metrics system and (b) confusion matrix.\nTherefore, UBnormal is supervised. UBnormal is built to address the problem that WAED ignores the open-set nature of\nanomalies that prevents the model from correctly corresponding to new anomalies. The test set contains anomalous\nevents not present in the training set. Moreover, it provides a validation set for model tuning for the first time.\n2.4\nPerformance Evaluation\nExisting GVAED methods evaluate model performance in terms of detection accuracy and operational cost. The former\nconcerns the ability to discriminate anomalous events while the latter aims to measure the deployment potential on\nresource-limited devices. According to the scale of detected anomalies, the detection accuracy criteria are divided into\nthree levels: Temporal-Detection-Oriented (TDO), Object-Detection-Oriented (ODO), and Spatial-Localization-Oriented\n(SLO). Specifically, TDO criteria require the model to determine anomalous events’ starting and ending temporal\nposition without spatial localization of abnormal pixels. In contrast, ODO criteria include object-level, region-level,\nand track-level, focusing on specific anomaly objects or trajectories. SLO criteria encourage pixel-level localization of\nanomalous events. As for operational cost criteria, the commonly used metric include parameter size, inference speed,\nand the number of FLOating Point operations (FLOPs) on the same platform, as shown in Fig. 7(a).\nWe can evaluate the model performance quantitatively by comparing the predicted results with the ground truth\nlabels. It is worth noting that the predicted labels of some GVAED models (e.g., prediction-based UVAD and WAED) are\ncontinuous values in the range of [0, 1]. In contrast, the true labels are discrete 0 or 1, so a threshold value must first be\nselected when calculating the performance metrics. Samples with abnormal scores below the threshold are considered\nnormal, and vice versa. In this way, we obtain the confusion matrix shown in Fig. 7(b), where TP, FN, FP, and TN denote\nthe number of abnormal samples correctly detected, abnormal samples mistakenly detected as normal, normal samples\nmistakenly detected as abnormal, and normal samples correctly detected, respectively. The True-Positive-Rate (TPR),\nFalse-Positive-Rate (FPR), True-Negative-Rate (TNR), and False-Negative-Rate (FNR) are defined as follows:\nTPR =\nTP\nTP+FN; FPR =\nFP\nFP+TN; TNR =\nTN\nFP+TN; FNR =\nFN\nTP+FN\n(4)\nwhich are used to calculate the Area Under the Receiver Operating Characteristic (AUROC) and Average Precision\n(AP).\nAUROC: The horizontal and vertical coordinates of the Receiver Operating Characteristic (ROC) curve are the\nFPR and TPR, and the curve is obtained by calculating the FPR and TPR under multiple sets of thresholds. The area\nof the region enclosed by the ROC curve and the horizontal axis is often used to evaluate binary classification tasks,\ndenoted as AUROC. The value of AUROC is within the range of [0, 1], and higher values indicate better performance.\nAUROC can visualize the generalization performance of the GVAED model and help to select the best alarm threshold.\nManuscript submitted to ACM\n12\nYang Liu et al.\nIn addition, the Equal Error Rate (EER), i.e., the proportion of incorrectly classified frames when TPR and FNR are equal,\nis also used to measure the performance of anomaly detection models.\nAP: Due to the highly unbalanced nature of positive and negative samples in GVAED tasks, i.e., the TN is usually\nlarger than the TP, researchers think that the area under the Precision-Recall (PR) curve is more suitable for evaluating\nGVAED models, denoted as AP. The horizontal coordinates of the PR curve are the Recall (i.e., the TPR in Eq. 4), while\nthe vertical coordinate represents the Precision, defined as Precision =\nTP\nTP+FP. A point on the PR curve corresponds\nto the Precision and Recall values at a certain threshold. Currently, AP has become the main metric for multimodal\nGVAED models [181, 186, 187] and is widely used to evaluate the performance on the XD-Violence dataset [186].\n3\nUNSUPERVISED VIDEO ANOMALY DETECTION\nExisting reviews [125, 139] usually classify UVAD methods into distance-based [29, 30, 148], probability-based\n[6, 23, 146, 149], and reconstruction-based [46, 50, 101] according to the deviations calculation means. Early traditional\nmethods relied on manual features such as foreground masks [148], histogram of flow [30], motion magnitude [148], HOG\n[29], dense trajectories [173], and STIP [34], which relied on human a priori knowledge and had poor representational\npower. With the rise of deep learning in computer vision tasks [151, 209, 217], recent approaches preferred to extracting\nfeatures representations in an end-to-end framework with deep Auto-Encoder (AE) [24, 50, 97, 101, 131], Generative\nAdversarial Network (GAN) [9, 17, 59, 60, 75, 126, 214], and Vision Transformer (ViT) [40, 70, 204].\nThis section is dedicated to providing a systematic overview of UVAD methods driven by deep learning techniques [21,\n22, 32, 95, 103]. It’s worth noting that the traditional taxonomy, as outlined in previous studies [125, 139], predominantly\nfocuses on manual feature-based methods, which are limited in elucidating the evolving landscape of deep UVAD\nmodels. Deep CNNs exhibit a remarkable capacity for modeling the spatiotemporal intricacies within video sequences\nand generating profound representations of various sensory domains, contingent upon the nature of the input data.\nConsequently, we categorize the work found in existing literature into three principal groups, contingent upon the\nnature of the data employed: 1) Frame-level methods always utilize entire RGB and optical flow frames as input and\nendeavor to capture a holistic understanding of normality within the video data. Such approaches consider the entirety\nof the frame, aiming to comprehend the global context. 2) Patch-level methods recognize the repetitive spatial-\ntemporal information present in video sequences, so they extract features solely from designated regions of interest.\nThey intentionally disregard redundant data from repetitive regions and interactions among regional information that\ndo not warrant particular attention. This strategy offers distinct advantages in terms of computational efficiency and\ninference speed. 3) Object-level methods, emerging in recent years with the development of target detection models,\nshift the focus towards detecting foreground objects and scrutinizing the behavior of specific objects within the video\ncontext. Object-level methods consider the relationship between objects and their backgrounds, showcasing impressive\nperformance in the task of identifying anomalous events within complex scenes. Based on the aforementioned analysis,\nthis section classifies UVAD methods into three distinct categories: frame-level, patch-level, and object-level. This\ncategorization is aligned with a hierarchical \"input-structure\" taxonomy shown in Fig. 1. This taxonomy serves as a\nguiding framework for organizing and understanding the landscape of UVAD.\n3.1\nFrame-level Methods\nDeep CNNs can directly extract abstract features from videos and learn task-specific deep representations. Frame-level\nmethods use complete RGB frames, sequences, or optical flows as input to model the normality of normal events in a\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n13\nself-supervised learning manner. Existing methods can be classified into two categories according to model structure:\nsingle-stream and multi-stream. The former does not distinguish spatial and temporal information. They usually take the\noriginal RGB videos as input and learn the spatial-temporal patterns by reconstructing the input sequence or predicting\nthe next frame. Existing single-stream work focuses on designing more efficient network structures. They introduce\nmore powerful representational learners such as 3D convolution [219] and U-net [92]. In contrast, multi-stream networks\ntypically treat appearance and motion as different dimensions of information and attempt to learn spatial and temporal\nnormality using different agent tasks or network architectures. In addition to spatial-temporal separation modeling,\nexisting dual-stream works explored spatial-temporal coherence [97] and consistency [8] to perform anomaly detection.\n3.1.1\nSingle-Stream Models. Single-stream models typically use a single generative model to describe the spatial-\ntemporal patterns of normal events by performing a proxy task and preserving the normality in learnable parameters.\nFor example, the Predictive Convolutional Long Short-Term Memory (PC-LSTM) [121] used a conforming ConvLSTM\nnetwork to model the evolution of video sequences. Hasan et al. [50] constructed a fully convolutional Feed-Forward Auto-\nEncoder (FF-AE) with manual features as input, which can learn task-specific representations in an end-to-end manner.\nLiu et al. [92] proposed a Future Frame Prediction (FFP) method that used a GAN-based video prediction framework\nto learn the normality. Its extension, FFPN [112], further specified the design principles of predictive UVAD networks.\nSingh and Pankajakshan [155] also used a predictive task to detect anomalies, proposing conformal structures based on\n2D & 3D convolution and convLSTM to characterize spatial-temporal patterns more efficiently.\nTo address the detail loss in frame generation, Li et al. [85] proposed a Spatial-Temporal U-net network (STU-net) that\ncombined the advantages of U-net in representing spatial information with the ability of convLSTM to model temporal\nvariations for moving objects. [221] proposed a sparse coding-based neural network called AnomalyNet, which used\nthree neural networks to integrate the advantages of feature learning, sparse representation, and dictionary learning.\nIn [124], the authors proposed an Incremental Spatial-Temporal Learner (ISTL) to explore the nature of anomalous\nbehavior over time. ISTL used active learning with fuzzy aggregation to continuously update and distinguish between\nnew anomalous and normal events evolving. The anoPCN in [198] unified the reconstruction and prediction methods\ninto a deep predictive coding network by introducing an error refinement module to reconstruct the prediction errors\nand refining the coarse predictions generated by the predictive coding module.\nTo lessen the deep model’s ability to generalize anomalous samples, memory-augmented Auto-Encoder (memAE)\n[46] embedded an external memory network between the encoder and decoder to record the prototypical patterns of\nnormal events. Further, Park et al. [131] introduced an attention-based memory addressing mechanism and proposed to\nupdate the memory pool during the testing phase to ensure that the network can better represent normal events.\nLuo et al. [113] proposed a sparse coding-inspired neural network model, namely Temporally-coherent Sparse Coding\n(TSC). It used a sequential iterative soft thresholding algorithm to optimize the sparse coefficients. [31] introduces\nresidual connection [53] into the auto-encoder to eliminate the gradient disappearance problem during normality\nlearning. Experiments have shown that ResNet brong 3%, 2% and 5% frame-level AUC gains for the proposed Residual\nSpatial-Temporal Auto-Encoder (R-STAE) on CUHK Avenue [107], LV [72] and UCSD Ped2 [84] datasets, respectively.\nThe DD-GAN in [35] introduced an additional motion discriminator to GAN. The dual discriminators structure\nencouraged the generator to generate more realistic frames with motion continuity. Yu et al. [202] also used GAN to\nmodel normality. The proposed Adversarial Event Prediction (AEP) network performed adversarial learning on past\nand future events to explore the correlation. Similarly, Zhao et al. [218] explored spatial-temporal correlations by GAN\nand used a spatial-temporal LSTM to extract appearance and motion information within a unified unit.\nManuscript submitted to ACM\n14\nYang Liu et al.\nTable 3. Frame-level Multi-stream UVAD Methods.\nYear\nMethod\nBackbone\nAnalysis\n2017\nAMDN [191]\nAE\nPros: Learning appearance and motion patterns separately.\nCons: Determining boundaries by OC-SVM with limited capability.\n2017\nSTAE [219]\nAE\nPros: Using 3D CNN to learn the spatial-temporal patterns.\nCons: Dual decoders causing huge computational costs.\n2019\nAMC [127]\nGAN\nPros: Learning the correspondence between appearance and motion.\nCons: Limited performance on the complex datasets.\n2019\nGANs [142]\nGAN\nPros: Training two GANs to learn temporal and spatial distribution.\nCons: Unstable traning process and high training cost.\n2020\nCDD-AE [14]\nAE\nPros: Using two auto-encoders to learn spatial and temporal patterns.\nCons: No special consideration in the design of the encoder structure.\n2020\nOGNet [205]\nGAN\nPros: Using generators and discriminators to learn normality.\nCons: Adversarial learning making the training process unstable.\n2021\nAMMC-net [8]\nAE\nPros: Exploring the consistency of appearance and motion.\nCons: Lack of analysis to the flow-frame generation task.\n2021\nDSTAE [82]\nAE, ConvLSTM\nPros: Using two auto-encoders to perform different tasks.\nCons: High computional cost and relying on optical flow network.\n2022\nAMAE [97]\nAE\nPros: Using two encoders and three decoders to learn features.\nCons: High training cost and relying on optical flow network.\n2022\nSTM-AE [101]\nAE, GAN\nPros: Using two memory-enhanced auto-encoders to learn normality.\nCons: Unstable traning process and high computional costs.\n[16] proposed a Bidirectional Prediction (Bi-Pre) framework that used forward and backward prediction sub-networks\nto reason about normal frames. In the test phase, only part significant regions are used to calculate the anomaly score,\nallowing the model to focus on the foreground. Wang et al. [178] used multi-path convGRU to perform frame prediction.\nThe proposed ROADMAP model included three non-local modules to handle different scales of objects.\n3.1.2\nMulti-Stream Models. . The multiplicity of multi-stream models is reflected in the multiple sources of the input\ndata and the multiple tasks corresponding to multiple outputs. Considering that video anomaly may manifest as outliers\nin appearance or motion, an intuitive idea is to use multi-stream networks to model spatial and temporal normality\nseparately [13, 14, 82, 175, 188]. In addition, learning associations between appearance and motion, such as consistency[8],\ncoherence [97, 101], and correspondence [127], is another effective GVAED solution. Events without such associations\nare discriminated against as anomalies. The multi-stream model has achieved significant success in recent years due to\nthe high matching of its design motivation with the GVAED task. The multi-stream methods are summarized in Table 3.\nMotivated by the remarkable success of 3D CNN in video understanding tasks, Zhao et al. [219] proposed a 3D\nconvolutional-based Spatial-Temporal Auto-Encoder (STAE) to model normality by simultaneously performing recon-\nstruction and prediction tasks. STAE included two decoders, which outputted reconstructed and predicted frames, respec-\ntively. In contrast, Appearance and Motion DeepNet (AMDN) [191] used two stacked denoising auto-encoders to encode\nRGB frames and optical flow separately. Similarly, Chang et al. [14] also used two auto-encoders to capture spatial and tem-\nporal information, respectively. One learned the appearance by reconstructing the last frame, while the other outputted\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n15\nRGB differences to simulate the generation of optical flow. Deep K-means clustering was used to force the extracted fea-\nture compact and detect anomalies. DSTAE [82] introduced convLSTM to a two-stream auto-encoder to better model the\ntemporal variations. The reconstruction errors of the two encoders are weighted and used to calculate anomaly scores.\nIn addition to spatial-temporal separation, Nguyen and Meunier [127] proposed to learn the correspondence between\nappearance and motion. To this end, they proposed an AE with two decoders, one for reconstructing input frames\nand the other for predicting optical flow. Cai et al. [8] proposed an Appearance-Motion Memory Consistency network\n(AMMC-net), which aimed to capture the spatial-temporal consistency in high-level feature space.\nLiu et al. [97] proposed an Appearance-Motion united Auto-Encoder (AMAE) framework using two independent auto-\nencoders to perform denoising and optical flow generation tasks separately. Moreover, they utilized an additional decoder\nto fuse spatial-temporal features and predict future frames to model spatial-temporal normality. STM-AE [101] and AMP-\nNet [99] introduced the memory into the dual-stream auto-encoder to record prototype appearance and motion patterns.\nAdversarial learning was used to explore the connection between spatial and temporal information of regular events.\nAside from the above anomaly detection means such as reconstruction error [82, 97, 101, 219], clustering [13, 14]\nand one-class classification [191], researchers attempted to utilize the discriminator of GAN to directly output results.\nFor instance, Ravanbakhsh et al. [142] used GAN to learn the normal distribution and detect anomalies directly by\ndiscriminators. The authors use a cross-channel approach to prevent the discriminator from learning mundane constant\nfunctions. OGNet [205] shifted the discriminator from discriminating real or generated frames to distinguishing good\nor poor reconstructions. The well-trained discriminator can find subtle distortions in the reconstruction results and\ndetect non-obvious anomalies.\nThe patch-level methods [96, 118, 145, 147] takes the video patch (spatial-temporal cube) as input. Compared with\nframe-level methods that consider anomalies roughly, i.e., anomalies are reflected in spatial or temporal dimensions\nbeyond expectation, patch-level methods consider finding anomalies from specific spatial-temporal regions rather than\nanalyzing the whole sequence. Patch formation can be divided into three categories: scale equipartition [25, 78, 96, 118,\n147, 185, 222], information equipartition [73], and foreground object extraction [177]. Specifically, scale equipartition\nis the simplest. The video sequence is equipartitioned into several spatial-temporal cubes of uniform size along the\nspatial and temporal dimensions. The subsequent modeling process is similar to frame-level methods. The information\nequipartition strategy considers that image blocks of the same size do not contain the same information. Regions close\nto the camera contain less information per unit area than those far away. Before representation, all cubes will be first\nresized to the same size. The foreground object extraction focuses on modeling regions with information variation to\navoid the learning cost and disruption of the background. After the sequences are equated into same-scale cubes, those\ncontaining only background will be eliminated.\nRoshtkhari and Levine [145] densely sampled video sequences at different spatial and temporal scales and used a\nprobabilistic framework to model the spatial-temporal composition of the video volumes. The STCNN [222] treated\nUVAD as a binary classification task. It first extracted patches’ appearance and motion information and outputted the\ndiscriminative results with an FCN. It first equated the video sequence into patches of 3 × 3 × 7 and retained only\nthe part of the region containing moving pixels to ensure the robustness of the model to local noise and improve\nthe detection accuracy. Deep-Cascade [147] employed a cascaded autoencoder to represent video patches. It used a\nlightweight network to select local patches of interest and then applied a complex 3D convolutional network to detect\nanomalies. The lightweight network can filter simple normal patches to reduce computational costs and save processing\ntime. S2-VAE [177] first detected the foreground and retained only the cell containing the object as input. And then,\na shadow generative network was used to fit the data distribution. The output was fed to another deep generative\nManuscript submitted to ACM\n16\nYang Liu et al.\nTable 4. Object-level UVAD Methods.\nYear\nMethod\nDetector\nDecision Logic\nContributions\n2017\nLDGK [54]\nFast\nR-\nCNN\nAnomaly score of\nthe detected object\nproposal\nIntegrating a generic CNN and environment-related\nanomaly detector to detect video anomalies and record\nthe cause of the anomalies.\n2018\nDCF [68]\nYOLO\nClassification\nExtracting foreground objects by object detection models\nand Kalman filtering and discriminating anomalies by\npose and motion classification.\n2019\nOC-AE [62]\nSSD\nOne-versus-rest bi-\nnary classification\nProposing an object-centric convolutional autoencoder\nto encode motion and appearance and discriminating\nanomalies using a one-versus-rest classifier.\n2021\nBackground-\nAgnostic\n[44]\nSSD-FPN,\nYOLOv3\nBinary classification\nUsing a set of autoencoders to extract the appearance and\nmotion features of foreground objects and then using a\nset of binary classifiers to detect anomalies.\n2021\nMulti-task\n[43]\nYOLOv3\nBinary classification\nTraining a 3D convolutional neural network to generate\ndiscriminative representation by performing multiple self-\nsupervised learning tasks.\n2021\nOAD [37]\nYOLOv3\nClustering\nAn online VAD method with asymptotic bounds on the\nfalse alarm rate, providing a procedure for selecting a\nproper decision threshold.\n2021\nHF2-VAD\n[105]\nCascade\nR-CNN\nPrediction error\nA hybrid framework that seamlessly integrates sequence\nreconstruction and frame prediction to handle video\nanomaly detection.\n2020\nVEC [201]\nCascade\nR-CNN\nCube construction\nerror\nProposing a video event completion framework to exploit\nadvanced semantic and temporal contextual information\nfor video anomaly detection.\n2022\nBiP [15]\nCascade\nR-CNN\nAppearance and mo-\ntion error\nProposing a bi-directional architecture with three consis-\ntency constraints to regularize the prediction task from\nthe pixel, cross pattern, and temporal levels.\n2022\nHSNBM [5]\nCascade\nR-CNN\nFrame and object\nprediction error\nDesigning a hierarchical scene normative binding model-\ning framework to detect global and local anomalies.\nnetwork to model normality. Wu et al. [185] proposed a deep one-class neural network (DeepOC). Specifically, DeepOC\nused stacked auto-encoders to generate low-dimensional features for frame and optical flow patches and simultaneously\ntrained the OC classifier to make these representations more compact.\nSpatial-Temporal Cascade Auto-Encoder (ST-CaAE) [78] first used an adversarial autoencoder to identify anomalous\nvideos and excluded simple regular patches. The retained patches were fed to a convolutional autoencoder, which\ndiscriminated anomalies based on reconstruction errors. Liu et al. [96] proposed an Attention augmented Spatial-\nTemporal Auto-Encoder (AST-AE) that equated frames in spatial dimensions into 8 × 8 parts and models spatial and\ntemporal information using CNN and LSTM, respectively. In the downstream anomaly detection stage, AST-AE only\nretained significant regions with large prediction errors to calculate the anomaly score.\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n17\n3.2\nObject-level Methods\nThe emergence of high-performance object detection models [45, 143, 192] provides a new idea for GVAED, i.e., using\na pre-trained object detector to extract the objects of interest from the video sequence before normality learning.\nCompared with the frame-level and patch-level methods, the object-level methods [44, 93, 160, 200] enable the model\nto ignore redundant background information and focus on modeling the behavioral interactions of foreground objects.\nIn addition to outperforming object-free methods in terms of performance, object-level methods are also considered\nfeasible to investigate scene-adaptive GVAED models. Existing studies [43, 44] show that object-level methods perform\nsignificantly better than other methods on multi-scene datasets such as ShanghaiTech [92]. Table 4 compares the object\ndetectors, decision logic, and main contributions of existing object-level methods.\nRyota et al. [54] attempted to describe anomalous events in a human-understandable form by detecting and analyzing\nthe classes, behaviors, and attributes of specific objects. The proposed LDGK model first used multi-task learning to\nobtain anomaly-related semantic information and then inserted an anomaly detector to analyze scene-independent\nfeatures to detect anomalies. The DCF [68] used a pose classifier and an LSTM network to model the spatial and motion\ninformation of the detected objects, respectively. [62] formalizes UVAD as a one-versus-rest binary classification task.\nThe proposed OC-AE first encoded the motion and appearance of selected objects and then clustered the training\nsamples into normal clusters. An object is considered anomalous in the inference stage if the one-versus-rest classifier’s\nhighest classification score is negative. Its extension, the Background-Agnostic framework [44], introduced instance\nsegmentation, allowing the model to focus only on the primary object. In addition, the authors used pseudo-anomaly\nexamples to perform adversarial learning to improve the appearance and motion auto-encoders.\nTo make full use of the contextual information, Yu et al. [201] proposed a Video Event Completion (VEC) method that\nused appearance and motion as cues to locate regions of interest. VEC recovered the original video events by solving\nvisual completion tests to capture high-level semantics and inferring deleted patches. Georgescu et al. [43] designed\nseveral self-supervised learning tasks, including discrimination of forward/backward moving objects, discrimination\nof objects in continuous/intermittent frames, and reconstruction of object-specific appearance. In the testing phase,\nanomalous objects would lead to large prediction discrepancies.\nDoshi and Yilmaz [37] proposed an Online Anomaly Detection (OAD) scheme that used detected object information\nsuch as location, category, and size as input to a clustering model to detect anomalous events. HF2-VAD [105] seamlessly\nintegrated frames reconstruction and prediction. It used memory to record the normal pattern of optical flow recon-\nstruction and captured the correlation between RGB frames and optical flow using a conditional variation auto-encoder.\nChen et al. [15] proposed a Bidirectional Prediction (BiP) architecture with three consistency constraints. Specifically,\nprediction consistency considered the symmetry of motion and appearance in forward and backward prediction.\nAssociation consistency considered the correlation between frames and optical flow, and temporal consistency was\nused to ensure that BiP can generate temporally consistent frames.\nIn summary, object-level methods, employing well-trained object detection/segmentation models to isolate significant\nforeground targets from video frames and developing scene-independent anomaly detection models through analysis of\ntarget-specific attributes, offer notable advantages over frame-level and patch-level approaches in real-world cross-scene\ndatasets. However, these methods face challenges in capturing the interaction between scenes and backgrounds, leading\nto performance degradation in handling scene-specific anomalous events, such as a person walking on a motorway.\nAddressing this limitation, Liu et al. [93] explored the semantic interaction between prototypical features of foreground\ntargets and the background scene using memory networks. Alternatively, instance segmentation proves more effective\nManuscript submitted to ACM\n18\nYang Liu et al.\nin modeling target-scene interactions. For instance, the Hierarchical Scene Normality-Binding Modeling (HSNBM)\nframework [5] attempted to dissect global and local scenes, which introduced a scene object-binding frame prediction\nmodule to capture the relationship between foreground and background through scene segmentation. Looking ahead,\nobject-level methods with object detection or instance segmentation will play a crucial role in discovering anomalous\nevents in real-world highly dynamic environments, such as autonomous driving and intelligent industries.\n4\nWEAKLY-SUPERVISED ABNORMAL EVENT DETECTION\nUsing weakly semantic video-level labels to supervise the model was first proposed by Sultani et al. [158] in 2018,\nlaying the foundation for WAED based on multiple instance learning [114, 132, 211]. The synchronously released UCF-\ncrime dataset collected 13 classes of real-world criminal behaviors and provided video-level labels for training sets.\nFollowing researchers [164, 220] made UVAD datasets meet WAED requirements by moving some anomalous test videos\nto the training set, introducing various WAED benchmarks such as the reorganized UCSD Ped2 [98] and ShanghaiTech\nWeakly [79, 220]. In 2020, the XD-Violence [186] dataset extended GVAED to multimodal signal processing.\nThis section is dedicated to providing an in-depth exploration of existing WAED models, with a focus on their\ncategorization into unimodal and multimodal approaches based on the input data modalities. This taxonomy is\ninstrumental in guiding the development of GVAED methods, fostering the transition from video processing to a\nbroader multimodal understanding communities. Unimodal models [39, 79, 102, 158, 164, 208, 220], similar to UVAD\ntechniques, primarily rely on RGB frames as input data. However, they distinguish themselves by directly computing\nthe anomaly score. These models center their efforts on analyzing successive RGB frames to detect anomalies within\nthe videos. In contrast, multimodal models [19, 181, 186, 187, 203] aim to leverage diverse data sources, including\nvideo, audio, text, and optical flow, to extract effective anomaly-related clues. These methods harness the power of\nmultiple modalities to enhance the overall understanding of anomalies, making them more robust and versatile in\ncapturing complex abnormal events. This categorization scheme not only clarifies the distinctions between unimodal\nand multimodal WAED models but also sets the stage for the evolution of GVAED techniques that integrate various\ndata modalities, paving the way for a more comprehensive approach to anomaly detection and event understanding.\n4.1\nUnimodal Models\nThe unimodal WAED model typically slices the unedited video into several fixed-size clips. They consider each clip as an\ninstance, and all clips from a video form a bag with the same video-level label. And then, pre-trained feature extractors,\nsuch as Convolutional 3D (C3D) [165], Temporal Segment Networks (TSN) [176], and Inflated 3D (I3D) [11], is used to\nextract the spatial-temporal features of the examples. Generally, the scoring module takes deep representations as input\nand calculates the anomaly score for each instance with the supervision of video-level labels.\nThe MIL ranking framework [158] introduced multiple instance learning to GVAED for the first time, using a 3-\nlayer Fully Connected Network (FCN) to predict high anomaly scores for anomalous clips and introducing sparsity and\nsmooth constraints to avoid drastic fluctuations in the score curve. Zhu and Newsam [223] considered motion as the\nkey to WAED performance. To this end, they proposed a temporal augmented network to learn motion-aware features\nand used attention blocks [89] to incorporate temporal context into a MIL ranking model. Snehashis et al. [119] used a\ndual-stream CNN to extract spatial and temporal features separately and fed the fused features as spatial-temporal\nrepresentations into an FCN to perform anomaly classification. The authors compared the performance of different\ndeep CNN architectures (e.g., ResNet- 50 [53], Inception V3 [161], and VGG-16 [157]) for feature extraction.\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n19\nZhong et al. [220] treated WAED as a supervised learning task under noisy labels, arguing that the supervised\naction recognition models can perform anomaly detection after the label noise is removed. In response, they designed\na graph convolutional network to correct the labels. [172] proposed Anomaly Regression Network (ARNet) to learn\ndiscriminative features WAED. Specifically, ARNet used dynamic multiple-instance learning loss and center loss to\nenlarge the inter-class distance instances and reduce the intra-class distance of regular instances, respectively.\nWaseem et al. [80] proposed a two-stage WAED framework that first used an echo state network to obtain spatially and\ntemporally aware features. And then, they used a 3D convolutional network to extract spatial-temporal features and fuse\nthem with the features from the first stage as the input to a binary classifier. Tian et al. proposed Robust Temporal Feature\nMagnitude (RTFM) learning by training a feature volume learning function to identify positive examples efficiently. In\naddition, RTFM utilized self-attention to capture both long and short-time correlations. Muhammad et al. [208] proposed\na self-reasoning framework that uses binary clustering to generate pseudo-labels to supervise the MIL regression models.\nThe CLustering Assisted Weakly Supervised (CLAWS) learning with normalcy sppression in [206] proposed a random\nbatch-based training strategy to reduce the correlation between batches. In addition, the authors introduced a loss\nbased on clustering distance to optimize the network to weaken the effect of label noise. Kamoona et al. [66] proposed\na Deep Temporal Encoding-Decoding (DTED) to capture the temporal evolution of videos over time. They treated\ninstances of the same bag as sequential visual data rather than as independent individuals. In addition, DTED uses joint\nloss to optimize to maximize the average distance between normal and abnormal videos.\nThe Weakly-supervised Temporal Relationship (WSTR) learning framework [212] enhanced the model’s discrimina-\ntive power by exploring the temporal relationships between clips. The proposed transformer-enabled encoder converts\nthe task-irrelevant representations into task-specific features by mining the semantic correlations and positional rela-\ntionships between video clips. Weakly Supervised Anomaly Localization (WSAL) [115] performed anomaly detection\nby fusing temporal and spatial contexts and proposed a higher-order context encoding model to measure temporal\ndynamic changes. In addition, the authors collected a dataset called TAD for traffic anomaly detection.\nFeng et al. [39] proposed a Multi-Instance Self-Training (MIST) framework consisting of a multi-instance pseudo\nlabel generator and a self-guided attention-enhancing feature encoder for generating more reliable fragment-level\npseudo labels and extracting task-specific representations, respectively. Liu et al. [98] proposed a Self-guiding Multi-\ninstance Ranking (SMR) framework that used a clustering module to generate pseudo labels to aid the training of\nsupervised multi-instance regression models to explore task-relevant feature representations. The authors compared the\nperformance of different recurrent neural networks in exploring temporal correlation. Spatial-Temporal Attention (STA)\n[102] explored the connection between example local representations and global spatial-temporal features through a\nrecurrent cross-attention operation and used mutual cosine loss to encourage the enhanced features to be task specific.\n4.2\nMultimodal Models\nThe emergence of TV shows and streaming media has broadened the application scope of GVAED technicals, transi-\ntioning them from traditional offline surveillance video analysis to online video stream detection. Unlike surveillance\nvideos, which typically consist of only RGB images, most online video content, including vlogs, live streams, and talk\nshows, incorporates multiple modalities such as language, speech, and subtitle text. Extracting anomaly-related cues\nfrom these diverse data modalities exceeds the capabilities of current unimodal methods.\nReal-world data are heterogeneous, and effectively exploiting the complementary nature of multimodal data is\nthe key to developing robust and efficient GVAED models. Due to the limitation of datasets, most existing works\n[186, 187, 203] focused on video and audio information fusion to detect violent behaviors from surveillance videos.\nManuscript submitted to ACM\n20\nYang Liu et al.\nTable 5. Multimodal WAED Models.\nYear\nMethod\nInput Modality\nContributions\n2020\nHL-Net\n[186]\nVideo + Audio\nCollecting the XD-Violence violence detection datasets and proposing a\nthree-branch neural network model for multimodal anomaly detection.\n2021\nFVAI [130]\nVideo + Audio\nProposing a pooling-based feature fusion strategy to fuse video and\naudio information to obtain more discriminative feature representations.\n2022\nSC [133]\nVideo + Audio\nProposing an audio-visual scene classification dataset containing 5\nclasses of anomalous events and a deep classification model.\n2022\nMACIL-SD\n[203]\nVideo + Audio\nProposing a modality-aware contrastive instance learning with a self-\ndistillation strategy to address the modality heterogeneity challenges.\n2022\nACF [182]\nVideo + Audio\nProposing a two-stage multimodal information fusion method for vio-\nlence detection that first refines video-level labels into clip-level labels.\n2022\nMSAF [181]\nVideo + Au-\ndio, Video +\nOptical flow\nProposing multimodal labels refinement to refine video-level ground\ntruth into pseudo-clip-level labels and implicitly align multimodal infor-\nmation with multimodal supervise-attention fusion network.\n2022\nMD [153]\nVideo + Audio\n+ Flow\nUsing mutual distillation to transfer information and proposing a multi-\nmodal fusion network to fuse video, audio, and flow features.\n2022\nHL-Net+\n[187]\nVideo + Audio\nIntroducing coarse-grained violent frame and fine-grained violence de-\ntection tasks and proposing audio-visual violence detection network.\n2022\nAGAN\n[134]\nVideo + Audio\nUsing cross-modal interaction to enhance video and audio and comput-\ning high-confidence violence scores using temporal convolution.\nMoreover, inspired by the frame-level multi-stream UVAD models [82, 97], recent work [181] considered RGB frames\nand optical flow as different modalities. We display the modalities and principles of existing multimodal GVAED models\n[130, 133, 134, 153, 181, 182, 186, 187, 203] in Table 5.\nWu et al. [186] released the first multimodal GVAED dataset and proposed a three-branch network called HL-\nNet for multimodal violence detection. Specifically, the similarity branch used a similarity prior to capture long-\nrange correlations. In contrast, the proximity branch used proximity prior to capture local location relationships, and\nthe scoring branch dynamically captured the proximity of predicted scores. Experimental results demonstrated the\nmultimodal data’s positive impact on GVAED. The following MACIL-SD in [203] utilized a lightweight dual-stream\nnetwork to overcome the heterogeneity challenge. It used self-distillation to transfer unimodal visual knowledge to\naudio-visual models to narrow the semantic gap between multimodal features.\nResearchers [130, 134] attempted to explore more effective feature extraction and multimodal information fusion\nstrategies. For example, Pang et al. [130] proposed to use a bilinear pooling mechanism to fuse visual and audio\ninformation and encourage the model to learn from each other to obtain a more effective representation. Audio-Guided\nAttention Network (AGAN) [134] first used a deep neural network to extract video and audio features and then enhanced\nthe features in the temporal dimension using a cross-modal perceptual local arousal network.\nWei et al. [182] proposed a two-stage multimodal information fusion method, which first refines video-level hard\nlabels into clip-level soft labels and then uses an attention module for multimodal information fusion. Their extension\nwork, Multimodal Supervised Attentional Augmentation Fusion (MSAF) [181], used attention fusion to align information\nand achieved implicit alignment of multimodal data.\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n21\nInitial Anomaly\nDetection\nPseudo Normal Samples\nFeature  \nExtractor \nFCN-based \nScorer\nAnomaly Scores\nPseudo Anomalous Samples\nFeature  \nExtractor \nGenerator\nDiscriminator\nPsedo-labels from Generator\nPsedo-labels from Discriminator\nLoss\nLoss\nUnabeled Videos\nIterative Learning\nIterative optimization\n(a)\n(b)\nFig. 8. Workflow of two representative FVAD methods: (a) SDOR [129] and (b) GCL [207]. Given the unlabeled videos, the SDOR first\ndivided them into pseudo-normal and anomalous sets by initial anomaly detection. GCL introduces cross-supervision to train the\ngenerator G and discriminator D to learn anomaly detectors. The pseudo-labels from G and D are used to compute each other’s losses.\nShang et al. [153] observed that existing models are limited by small datasets and proposed mutual distillation to\ntransfer information from large-scale datasets to small datasets. They proposed a multimodal attention fusion strategy\nto fuse RGB images, audio, and flow to obtain a more discriminative representation. [133] introduced an audio-visual\nscene classification task and released a multimodal dataset. The authors try different deep networks and fusion strategies\nto explore the most effective classification model.\n5\nSUPERVISED VIDEO ANOMALY DETECTION\nSupervised video anomaly detection requires frame-level or pixel-level labels to supervise models to distinguish\nbetween normal and anomalies. Therefore, it is often considered a classification task rather than a mainstream GVAED\nscheme. On the one hand, collecting fine-grained labeled anomalous samples is time-consuming. On the other hand,\nthe anomalous behavior occurs gradually, and the degree of anomaly is a relative value, while manual labeling can only\nprovide discrete 0/1 labels, which cannot adequately describe the severity and temporal continuity of video anomalies.\nExisting SVAD methods usually consider VAD a binary classification task under data imbalance conditions. However,\ngame engines can simulate various types of anomalous events and provide frame-level and pixel-level personalized\nannotations. With the penetration of synthetic datasets in vision tasks, supervised training of GVAED models with\nvirtual anomalies is expected to become possible. Researchers need to focus on the domain adaptation problem posed by\nsynthetic datasets, i.e., how to cope with the covariate shifts between synthetic data and the real-world surveillance video\nand the ensuing performance degradation. Moreover, although the training set contains partially labeled anomalies,\nSVAD models still need to consider how to reasonably generalize the anomalies to detect unseen anomalous events in\nreal-world scenarios. SVAD is an open-set recognition task rather than a supervised binary classification.\n6\nFULLY-UNSUPERVISED VIDEO ANOMALY DETECTION\nFully-unsupervised Video Anomaly Detection (FVAD) does not limit the composition of the training data and requires\nno data annotation. In other words, FVAD tries to learn an anomaly detector from the random raw data, which is a\nnewly emerged technical route in recent years.\nIonescu et al. [167] introduced the unmasking technique to computer vision tasks, proposing an FVAD framework\nthat requires no training sequences. They iteratively trained a binary classifier to distinguish two consecutive video\nsequences and simultaneously removed the most discriminative features at each step. Inspired by [167], Liu et al. [94]\ntried to establish the connection between heuristic unmasking and multiple classifiers two sample tests to improve its\nManuscript submitted to ACM\n22\nYang Liu et al.\ntesting capability. In this regard, they proposed a history sampling method to increase the testing power as well as to\nimprove the GVAED performance. Li et al. [83] first used a distribution clustering framework to identify the possible\nanomalous samples in the training data, and then used the clustered subset of normal data to train the auto-encoder.\nAn encoder that can describe normality was obtained by repeating normal subset selection and representation learning.\nThe recent representative FVAD works are Self-trained Deep Ordinal regression (SDOR) [129] and Generative\nCooperative Learning (GCL) [207], which attempted to learn anomaly scorers from unlabeled videos in an end-to-end\nmanner, as shown in Fig. 8(a) and 8(b). Specifically, SDOR [129] first determined the initial pseudo-normal and abnormal\nsets and then computed the abnormal scores using pre-trained ResNet-50 and FCN. The representation module and\nthe scorer were optimized iteratively in a self-training manner. Moreover, Lin et al. [88] looked at the pseudo label\ngeneration process in SDOR from a causal inference perspective and proposed a causal graph to analyze confounding\neffects and eliminate the impact of noisy pseudo labels. In addition, their proposed CIL model improved significantly by\nperforming counterfactual inference to capture long-range temporal dependencies.\nIn contrast, GCL [207] attempted to exploit the low-frequency nature of anomalous events. It included a generator G\nand a discriminator D, which were supervised by each other in a cooperative rather. The generator primarily generated\nrepresentations for normal events. While for anomaly events, the generator used negative learning techniques to distort\nthe anomaly representation and generated pseudo-labels to train D. The discriminator estimated the probability of\nanomalies and created pseudo labels to improve G. The scarcity and infrequent occurrence of anomalies provide valuable\ninsights into the development of FVAD. Hu et al. [55] leveraged the rarity of anomalies, operating under the assumption\nthat the small number of anomalous samples in the training set has a limited impact on the normality of the model\nlearning process. Inspired by the Masked Auto-Encoder (MAE) [51], their proposed TMAE learned representations\nusing a visual transformer performing a complementary task. Notably, MAE [51] applied masks primarily to 2D images,\nwhereas video anomalies are closely linked to temporal information. To address this challenge, TMAE first identified\nvideo foregrounds and constructed temporal cubes to serve as masked objects, ensuring a more comprehensive approach\nto anomaly detection in video data.\n7\nPERFORMANCE COMPARISON\nWe collect the performance of existing works on publicly available datasets [84, 92, 107, 158, 220] to quantitatively\ncompare the superiority and present the GVAED development progress. Table 6 presents the frame-level AUC and EER\nof the early UVAD models on UCSD Ped1 & Ped2 [84], and CUHK Avenue [107] datasets and the frame-level AUC on\nthe ShanghaiTech [92] dataset. Since the recent UVAD [14, 46, 97, 101] and FVAD [88, 129, 207] only report frame-level\nAUC as the main evaluation metric, we have collated these methods separately in Table 7. The ShanghaiTech dataset\nwas proposed in 2018 with the FFP [92] model, so methods before this time were usually tested without this dataset.\nWith the advantage of its data size and quality, ShanghaiTech has become the most widely used UVAD benchmark. An\ninteresting phenomenon is that the object-level methods outperform other frame-level and patch-level models on the\ncross-scene ShanghaiTech dataset. For example, the frame-level AUC of the Multi-task [43] model is as high as 90.2%,\nwhich is 12.1% higher than the state-of-the-art frame-level methods [69]. It shows that for cross-scene GVAED tasks,\nusing an object detector to separate the foreground object of interest from the scene can effectively avoid interference\nof the background. In addition, the multi-stream model learns normality in both temporal and spatial dimensions and\ngenerally outperforms the single-stream model. The usage frequency shows that UCSD Ped2 [84], CUHK Avenue [107],\nand ShanghaiTech [92] have become the prevailing benchmarks for UVAD evaluation. Future work should consider\ntesting and comparing the proposed methods on these three datasets.\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n23\nTable 6. EER and AUC Comparison of Early Unsupervised Methods on Benchmark Datasets.\nYear\nMethod\nPed1 AUC\nPed1 EER\nPed2 AUC\nPed2 EER\nAvenue AUC\nAvenue EER\nShanghaiTech AUC\n2015\nDRAM [190]\n92.1\n16.0\n90.8\n17.0\n-\n-\n-\n2015\nSTVP [3]\n93.9\n12.9\n94.6\n10.6\n-\n-\n-\n2016\nCMAC [215]\n85.0\n-\n90.0\n-\n-\n-\n-\n2016\nFF-AE [50]\n81.0\n27.9\n90.0\n21.7\n70.2\n25.1\n60.9\n2017\nDEM [41]\n92.5\n15.1\n-\n-\n-\n-\n-\n2017\nCFS [73]\n82.0\n21.1\n84.0\n19.2\n-\n-\n-\n2017\nWTA-AE [166]\n91.9\n15.9\n92.8\n11.2\n82.1\n24.2\n-\n2017\nEBM [171]\n70.3\n35.4\n86.4\n16.5\n78.8\n27.2\n-\n2017\nCPE [168]\n78.2\n24.0\n80.7\n19.0\n-\n-\n-\n2017\nLDGK [54]\n-\n-\n92.2\n13.9\n-\n-\n-\n2017\nsRNN [110]\n-\n-\n92.2\n-\n81.7\n-\n68.0\n2017\nGANS [141]\n97.4\n8.0\n93.5\n14.0\n-\n-\n-\n2017\nOGNG [159]\n93.8\n-\n94.0\n-\n-\n-\n-\n2018\nFFP [92]\n83.1\n-\n95.4\n-\n85.1\n-\n72.8\n2018\nPP-CNN [140]\n95.7\n8.0\n88.4\n18.0\n-\n-\n-\n2019\nFAED [108]\n93.8\n14.0\n95.0\n-\n-\n-\n-\n2019\nNNC [63]\n-\n-\n-\n-\n88.9\n-\n-\n2019\nOC-AE [62]\n-\n-\n97.8\n-\n90.4\n-\n84.9\n2019\nAMC [127]\n-\n-\n96.2\n-\n86.9\n-\n-\n2019\nMLR [170]\n82.3\n23.5\n99.2\n2.5\n71.5\n36.4\n-\n2019\nmemAE [46]\n-\n-\n94.1\n-\n83.3\n-\n71.2\n2019\nMLEP [91]\n-\n-\n-\n-\n92.8\n-\n76.8\n2019\nBMAN [71]\n-\n-\n96.6\n-\n90.0\n-\n76.2\n2020\nStreet Scene [137]\n77.3\n25.9\n88.3\n18.9\n72.0\n33.0\n-\n2020\nIPR [162]\n82.6\n-\n96.2\n-\n83.7\n-\n73.0\n2020\nDFSN [138]\n86.0\n23.3\n94.0\n14.1\n87.2\n18.8\n-\nTable 7. AUC Comparison of Recent Unsupervised and Fully-unsupervised (Marked in Italics) Methods on Benchmark Datasets.\nYear\nMethod\nPed2\nAvenue\nShanghaiTech\nYear\nMethod\nPed2\nAvenue\nShanghaiTech\n2020\nMNAD-R [131]\n90.2\n82.8\n69.8\n2020\nMNAD-P [131]\n97.0\n88.5\n70.5\n2020\nDD-GAN [35]\n95.6\n84.9\n73.7\n2020\nSDOR [129]\n83.2\n-\n-\n2020\nASSAD [36]\n97.8\n86.4\n71.6\n2020\nFSSA [109]\n96.2\n85.8\n77.9\n2020\nVEC [201]\n97.3\n89.6\n74.8\n2020\nMultispace[60]\n95.4\n86.8\n73.6\n2020\nCDD-AE [14]\n96.5\n86.0\n73.3\n2021\nCDD-AE+ [13]\n96.7\n87.1\n73.7\n2021\nMulti-task (object level) [43]\n99.8\n91.9\n89.3\n2021\nMulti-task (frame level) [43]\n92.4\n86.9\n83.5\n2021\nMulti-task (late fusion) [43]\n99.8\n92.8\n90.2\n2021\nHF2AVD [105]\n99.3\n91.1\n76.2\n2021\nAST-AE [96]\n96.6\n85.2\n68.8\n2021\nROADMAP[178]\n96.3\n88.3\n76.6\n2021\nCT-D2GAN[40]\n97.2\n85.9\n77.7\n2022\nAMAE [97]\n97.4\n88.2\n73.6\n2022\nSTM-AE [101]\n98.1\n89.8\n73.8\n2022\nBiP [15]\n97.4\n86.7\n73.6\n2022\nAR-AE [69]\n98.3\n90.3\n78.1\n2022\nTAC-Net[60]\n98.1\n88.8\n77.2\n2022\nSTC-Net [218]\n96.7\n87.8\n73.1\n2022\nHSNBM [5]\n95.2\n91.6\n76.5\n2022\nCIL(ResNet50)+DCFD [88]\n97.9\n85.9\n-\n2022\nCIL(ResNet50)+DCFD+CTCE [88]\n99.4\n87.3\n-\n2022\nCIL(I3D-RGB)+DCFD+CTCE [88]\n98.7\n90.3\n-\n2022\nGCL𝑃𝑇(RESNEXT) [207]\n-\n-\n78.93\nTable 8 presents the performance of WAED methods on the UCF-Cirme [158] and ShanghaiTech Weaky [220] datasets.\nAs mentioned previously, WAED models usually rely on pre-trained feature extractors [11, 165, 176] to obtain feature\nrepresentations. Commonly used features include C3D𝑅𝐺𝐵, I3D𝑅𝐺𝐵, and I3D𝑅𝐺𝐵+𝑂𝑝𝑡𝑖𝑐𝑎𝑙𝑓𝑙𝑜𝑤. The performance gap of\nthe same model using different features show that the effectiveness of the WAED model is related to the pre-trained\nfeature extractors, with the I3D outperforming the simple 3 × 3 × 3 convolution-based C3D network due to the separate\nconsideration of temporal information variation. Future WAED work should test the performance of the proposed model\non current commonly used features or provide the performance of existing works on emerging features to demonstrate\nthat the performance gain comes from the model design rather than benefiting from a more robust feature extraction\nnetwork. In addition to detection performance, other metrics are processing speed and deployment cost. GVAED typically\nemploys the Average Inference Speed (AIS) as a visual metric to gauge the overhead cost of the model. Figures reported\nin existing literature often lack direct comparability due to variations in experimental environments and computational\nplatforms. Moreover, Recent advancements in GVAED research, such as object-level methods and weakly-supervised\nManuscript submitted to ACM\n24\nYang Liu et al.\nTable 8. Quantitative Performance Comparison of Weakly-supervised Methods on Public Datasets.\nMethod\nFeature\nUCF-Crime AUC\nUCF-Crime FAR\nShanghaiTech AUC\nShanghaiTech FAR\nMIR [158]\nC3D𝑅𝐺𝐵\n75.40\n1.90\n86.30\n0.15\nTCN [213]\nC3D𝑅𝐺𝐵\n78.70\n-\n82.50\n0.10\nZhong [220]\nC3D𝑅𝐺𝐵\n80.67\n3.30\n76.44\n-\nARNet [172]\nC3D𝑅𝐺𝐵\n-\n-\n85.01\n0.57\nI3D𝑅𝐺𝐵\n-\n-\n85.38\n0.27\nI3D𝑅𝐺𝐵+𝑂𝑝𝑡𝑖𝑐𝑎𝑙𝐹𝑙𝑜𝑤\n-\n-\n91.24\n0.10\nMIST [39]\nC3D𝑅𝐺𝐵\n81.40\n2.19\n93.13\n1.71\nI3D𝑅𝐺𝐵\n82.30\n0.13\n94.83\n0.05\nRTFM [164]\nC3D𝑅𝐺𝐵\n83.28\n-\n91.51\n-\nI3D𝑅𝐺𝐵\n84.30\n-\n97.21\n-\nSMR [98]\nI3D𝑅𝐺𝐵+𝑂𝑝𝑡𝑖𝑐𝑎𝑙𝐹𝑙𝑜𝑤\n81.70\n-\n-\n-\nDTED [66]\nC3D𝑅𝐺𝐵\n79.49\n0.50\n87.42\n-\nschemes, typically involve intricate data preprocessing and calls to pre-trained models, such as foreground object\ndetection, optical flow estimation, and spatial-temporal feature extraction with well-trained 3D convolutional networks.\nIt remains unclear whether the computational cost and processing time associated with these aspects are factored\ninto the overhead cost of the proposed model. Consequently, reporting inference speed and comparing computational\ncost are not widespread practice in GVAED research. The few papers providing such results often lack a detailed\ndescription of the experimental setup. Nevertheless, we diligently collected the AIS of existing works to offer an intuitive\ndemonstration of the trend toward lighter-weight GVAED research. Acknowledging the impact of image resolution on\nmodel inference speed, we follow [139] to summarize these data while simultaneously documenting the dataset used\nfor model testing. The results are publicly accessible in our GitHub repository1 and will be continuously updated.\n8\nCHALLENGES AND TRENDS\n8.1\nResearch Challenges\n8.1.1\nMock anomalies vs. Real anomalies: How to bridge domain offsets between mock and real anomalies? GVAED aims\nto automatically detect anomalous events in the living environment to provide a safe space for humans. However, the\ndifficulty of collecting anomalies makes most of the existing datasets formulate abnormal events by human simulation,\nsuch as the UMN [28], CUHK Avenue [107], and ShanghaiTech [92]. The mock anomalies are simpler, and their spatial-\ntemporal patterns differ significantly from normal events, resulting in well-trained models difficult to detect complex\nanomalies. In addition, the set of limited categories of anomalous events conflicts with the diverse nature of real\nanomalies. As a result, models learned on such datasets perform poorly in real-world scenarios. Therefore, collecting\ndatasets containing various real anomalies and designing models to bridge the gap between mock and real anomalies is\nan essential challenge for GVAED development.\n8.1.2\nSingle-scene vs. Multi-scenes: How to develop cross-scenario GVAED models for the real world? Mainstream\nunsupervised datasets [84, 107] and UVAD methods [14, 92, 131] only consider single-scene videos, while the real world\nalways contains multiple scenes, which constitutes another challenge for UVAD methods. Although the UMN [28] and\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n25\nShanghaiTech [92] datasets include multiple scenes, the anomalous events of the former are all crowd dispersal, while\nthe 13 scenes of the latter are similar. Therefore, most UVAD methods [92, 101] do not consider the scene differences\nbut learn normality directly from the original video as in other single-scene datasets [84, 107]. Recent researchers\n[44, 62] believe that object-level methods are a feasible way to learn scene-invariant normality by extracting specific\nobjects from the scenes and then analyzing the spatial-temporal patterns of objects and backgrounds separately. The\nmulti-scene problem is inescapable for model deployment as it is almost impossible to train a scene-specific model\nfor each terminal device. Developing cross-scene GVAED models using domain adaptation/generalization techniques\n[90, 183, 189] to learn scene-invariant normality is a definite challenge.\n8.1.3\nReal data vs. Synthetic data: How to develop large fine-grained GVAED models using synthetic data? Due to the\nrarity and diversity of anomalies, collecting and labeling anomalous events is time-consuming and laborious. Therefore,\nresearchers [1] have considered using game engines [38, 152] to synthesize anomaly data. We remain optimistic about\nthis attempt and believe it may lead to new research opportunities for GVAED. While anomaly detection tasks suffer\nfrom a lack of data and missing labels. Synthetic data can generate various anomalous samples and provide precise\nframe-level or even pixel-level annotations, making it possible to develop SVAD models and save data preparation costs\nfor large-scale GVAED model training. However, a concomitant challenge is that covariate shifts between synthetic and\nreal data may make the trained GVAED models not work in real scenes.\n8.1.4\nUnimodal vs. Multimodal: How to effectively fuse multimodal data to mine anomaly-related clues? Researchers\n[186, 187] are aware of the positive impact of multimodal data (e.g., audio) for GVAED. However, existing works are\nstuck on the lack of datasets and the validity of model structures. XD-Violence [186] is the only mainstream multimodal\nGVAED dataset, but it only contains video and audio, and much data is collected from movies and games rather than the\nreal world. With the popularity of IoT, using various sensors to collect environmental information (e.g., temperature,\nbrightness, and humidity) can assist cameras in detecting abnormal events. However, mining useful clues from valid\ndata and developing efficient multimodal GVAED models need further research, such as task-specific feature extraction\nfrom heterogeneous data, semantic alignment of different modalities, anomaly-related multimodal information fusion,\namd domain offset bridging in emerging cross-modal GVAED research.\n8.1.5\nSingle-view vs. Multi-view: How to integrate complementary information from multi-view data? In places such\nas traffic intersections and parks, the same area is usually covered by multiple camera views, deriving another task:\nanomalous event detection in multi-view videos. Multi-view data can provide more comprehensive environmental\nawareness data, which is wildly used for re-identification [87, 199], tracking [163] and gaze estimation [86]. However,\nexisting datasets [28, 84, 92, 107] are all single-view, making multi-view GVAED research still a gap. The simplest idea\nis to combine data from all views to train the same model and determine anomalies through a voting or winner-take-all\nstrategy. However, such approaches are training-costly and do not consider the differences and complementarities\nbetween multi-view data. Therefore, multi-view GVAED remains to be investigated.\n8.1.6\nOffline vs. Online Detection: How to develop light-weight end-to-end online GVAED models? The deployable\nIntelligent video surveillance systems (IVSS) need to process continuously generated video streams 24/7 online and\nrespond to anomalous events in real-time so that noteworthy clips can be saved in time to reduce storage and transmission\ncosts. Unfortunately, existing GVAED models are designed for public datasets rather than real-time video streams,\nprimarily pursuing detection performance while avoiding the online detection challenges. For example, the dominant\nprediction-based methods [92, 97, 101, 131] in UVAD route can only give the prediction error of the current input\nManuscript submitted to ACM\n26\nYang Liu et al.\nin a single-step execution, while the Informative anomaly score needs are obtained after performing the maximum-\nminimum normalization over the prediction error of all frames. Although the model can directly determine the current\nframe as an anomaly with a pre-set error threshold, existing attempts show that the manually selected threshold is\nunreliable. WAED [39, 98, 115, 158, 164] can directly output anomaly scores for segments. However, the input to the\nscoring module is a discriminative spatial-temporal representation rather than the original video. The representations\nusually rely on pre-trained 3D convolution-based feature extractors [11, 165, 176]. The time cost is unacceptable for\nresource-limited terminal devices [65]. Therefore, developing online detection models is the primary challenge for\nGVAED deployment, determining its application potential in IVSS and streaming media platforms.\n8.2\nDevelopment Trends\n8.2.1\nData level: Toward real-world GVAED model development for multi-view cross-scene multimodal data. From single-\nscene [84, 107] to multi-scene [92, 137], from real-word vides [28] to synthetic data [1], and from unimodal [158] to\nmultimodal [186], GVAED datasets are moving towards large-scale and realistic scenarios. We see this as a positive\ntrend that will continue with the growth of online video platforms and tools. On the one hand, real-world scenarios\nand anomalies are diverse, so efficient models for real-world applications need to be trained on large-scale datasets\nthat contain various anomalous events. On the other hand, the Internet has made it possible to collect multi-scene and\nmulti-view videos, including sufficient rare anomalous behaviors such as violence and crime. Furthermore, multimodal\nand synthetic data will be increasingly important in GVAED research. The XD-Violence [186] dataset has demonstrated\nthe positive impact of multimodal data on GVAED. In the future, with streaming media (e.g., TikTok, Netflix, and\nHulu) and online video sites (e.g., YouTube, iQIYI, and Youku), more modal data can be collected. Besides, virtual game\nengines (e.g., Airsim [152] and Carla [38]) can synthesize rare anomalous events and provide fine-grained annotations\non demand.The connection of GVAED with other tasks (e.g., multimodal analysis [181] and few-shot learning [109])\nwill tend to be close, with the latter inspiring the design of GVAED models under specific data conditions.\n8.2.2\nRepresentation level: Adaptive transfer of emerging representation learning and feature extraction means. Deep\nlearning has enabled spatial-temporal representations to be derived directly from the raw videos in an end-to-end\nmanner without a human prior [179]. The earlier deep GVAED models benefit from CNNs and pursue complex deep\nnetworks to extract more abstract features. For example, the UVAD models attempt to introduce dual-stream networks to\nlearn spatial and temporal representations [14, 97, 101], and use 3D convolutional networks to model temporal features\n[46, 219]. From C3D [165] to I3D [11], the WAED models [39, 98, 164] benefit from more powerful pre-trained feature\nextractors and achieves general performance gains on existing datasets [158, 220]. We observe that the representation\nmeans of WAED will become increasingly sophisticated. New visual representation learning models such as Transformer\n[4, 57, 81, 169] will drive WAED development. In contrast, UVAD does not pursue abstract representations. Overly\npowerful deep networks may lead to missing anomalous events as normal due to overgeneralization [46, 131]. Future\nresearchers should consider using clever representation strategies (e.g., causal representation learning [103]) to balance\nthe model’s powerful representation of normal events and the limited generalization of abnormal events [99]. Powerful\ngenerative models such as graph learning [58, 111, 120] and diffusion models [27] are expected to provide more effective\nnormality learning tools for UVAD. In addition, researchers should consider introducing emerging techniques (e.g.,\ndomain adaptation [44, 174] and knowledge distillation [180]) to develop GVAED models for learning scene-invariant\nrepresentation from multi-scene and multi-view videos.\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n27\n8.2.3\nDeployment level: Lightweight easy-to-deploy model development for resource-constrained end devices. Model\ndeployment is an inevitable trend for GVAED development. As mentioned above, the multi-scene and the diversity of\nanomalies in real-world videos pose new challenges for model design and training, such as online detection, lightweight\nmodels, and high view robustness. On the one hand, the computational resources of terminal devices are limited.\nMost deep GVAED methods are overly pursuing performance at the expense of running costs. On the other hand,\nexisting models are trained offline, which cannot perform real-time detection. Model compression [33] and knowledge\ndistillation [47] can drive the development of lightweight GVAED models. Online evolutive learning [76, 77], dge-cloud\ncollaboration, and integrated sensing and control technologies [135] enable models to dynamically optimize learnable\nparameters in complex environments such as modern industry [99] and intelligent transportation systems [194].\n8.2.4\nMethodology level: High-efficiency & robust GVAED development by integrating different research pathways. This\nsurvey compares the four main GVAED technical routes: UVAD, WAED, SVAD, and FVAD. UVAD has been regarded as\nthe mainstream solution, although WAED gradually dominates in recent years. However, the trend of UVAD is unclear\ndue to its performance saturation on limited datasets [84]. In addition, the setting of anomalies in UVAD datasets\nmakes UVAD models challenging to work in complex scenes. Self-supervised visual representation technicals (e.g.,\ncontrast learning [18, 48, 52, 60] and deep clustering [10, 210]) may provide new ideas for UVAD. In contrast, WAED\nhas been widely noticed as a research hotspot due to its excellent performance in crime detection [158]. In addition,\nthe multimodal video anomaly detection tasks also follow WAED routes. SVAD is once abandoned due to the lack of\nlabels and anomalies. However, it may face new research opportunities with the emergence of synthetic datasets [1].\nIn contrast, FVAD can learn directly from raw video data without the cost of training data filtering and annotations,\nmaking it a hot research topic. The various routes are not completely independent, and existing works [100, 184] have\nstarted to combine the assumptions of different methods to develop more efficient GVAED models.\n9\nCONCLUSION\nThis survey is the first to integrate the deep learning-driven technical routes based on different assumptions and\nlearning frameworks into a unified generalized video anomaly event detection framework. We provide a hierarchical\nGVAED taxonomy that systematically organizes the existing literature by supervision, input data, and network structure,\nfocusing on the recent advances such as weakly-supervised, fully-unsupervised, and multimodal methods. To provide a\ncomprehensive survey of the extant work, we collect benchmark datasets and available codes, sort out the development\nlines of various methods, and perform performance comparisons and strengths analysis. This survey helps clarify\nthe connections among deep GVAED routes and advance community development. In addition, we analyze research\nchallenges and future trends in the context of deep learning technology development and possible problems faced by\nGAED model deployment, which can serve as a guide for future researchers and engineers.\nACKNOWLEDGMENTS\nThis work is supported in part by the China Mobile Research Fund of the Chinese Ministry of Education under Grant No.\nKEH2310029, the National Natural Science Foundation of China under Grant No. 62250410368, and the Specific Research\nFund of the Innovation Platform for Academicians of Hainan Province under Grant No. YSPTZX202314. Additional\nsupport is acknowledged from the Shanghai Key Research Laboratory of NSAI and the Joint Laboratory on Networked\nAI Edge Computing Fudan University-Changan. The work of Yang Liu was financially supported in part by the China\nManuscript submitted to ACM\n28\nYang Liu et al.\nScholarship Council (File No. 202306100221). The authors extend their appreciation to the anonymous reviewers for their\nvaluable comments and suggestions, as well as to the authors of the reviewed papers for their contributions to the field.\nREFERENCES\n[1] Andra Acsintoae, Andrei Florescu, Mariana-Iuliana Georgescu, Tudor Mare, Paul Sumedrea, Radu Tudor Ionescu, Fahad Shahbaz Khan, and\nMubarak Shah. 2022. UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 20143–20153.\n[2] Amit Adam, Ehud Rivlin, Ilan Shimshoni, and Daviv Reinitz. 2008. Robust real-time unusual event detection using multiple fixed-location monitors.\nIEEE transactions on pattern analysis and machine intelligence 30, 3 (2008), 555–560.\n[3] Borislav Antić and BjörnT28 Ommer. 2015. Spatio-temporal Video Parsing for Abnormality Detection. arXiv preprint arXiv:1502.06235 (2015).\n[4] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and Cordelia Schmid. 2021. Vivit: A video vision transformer. In\nProceedings of the IEEE/CVF International Conference on Computer Vision. 6836–6846.\n[5] Qianyue Bao, Fang Liu, Yang Liu, Licheng Jiao, Xu Liu, and Lingling Li. 2022. Hierarchical scene normality-binding modeling for anomaly detection\nin surveillance videos. In Proceedings of the 30th ACM International Conference on Multimedia. 6103–6112.\n[6] Yannick Benezeth, P-M Jodoin, Venkatesh Saligrama, and Christophe Rosenberger. 2009. Abnormal events detection based on spatio-temporal\nco-occurences. In 2009 IEEE conference on computer vision and pattern recognition. IEEE, 2458–2465.\n[7] Ane Blázquez-García, Angel Conde, Usue Mori, and Jose A Lozano. 2021. A review on outlier/anomaly detection in time series data. ACM\nComputing Surveys (CSUR) 54, 3 (2021), 1–33.\n[8] Ruichu Cai, Hao Zhang, Wen Liu, Shenghua Gao, and Zhifeng Hao. 2021. Appearance-motion memory consistency network for video anomaly\ndetection. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 938–946.\n[9] Yiheng Cai, Jiaqi Liu, Yajun Guo, Shaobin Hu, and Shinan Lang. 2021. Video anomaly detection with multi-scale feature and temporal information\nfusion. Neurocomputing 423 (2021), 264–273.\n[10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. 2020. Unsupervised learning of visual features by\ncontrasting cluster assignments. Advances in Neural Information Processing Systems 33 (2020), 9912–9924.\n[11] Joao Carreira and Andrew Zisserman. 2017. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. 6299–6308.\n[12] S Chandrakala, K Deepak, and G Revathy. 2022. Anomaly detection in surveillance videos: a thematic taxonomy of deep models, review and\nperformance analysis. Artificial Intelligence Review (2022), 1–50.\n[13] Yunpeng Chang, Zhigang Tu, Wei Xie, Bin Luo, Shifu Zhang, Haigang Sui, and Junsong Yuan. 2021. Video anomaly detection with spatio-temporal\ndissociation. Pattern Recognition 122 (2021), 108213.\n[14] Yunpeng Chang, Zhigang Tu, Wei Xie, and Junsong Yuan. 2020. Clustering driven deep autoencoder for video anomaly detection. In European\nConference on Computer Vision. Springer, 329–345.\n[15] Chengwei Chen, Yuan Xie, Shaohui Lin, Angela Yao, Guannan Jiang, Wei Zhang, Yanyun Qu, Ruizhi Qiao, Bo Ren, and Lizhuang Ma. 2022.\nComprehensive Regularization in a Bi-directional Predictive Network for Video Anomaly Detection. In Proceedings of the American association for\nartificial intelligence. 1–9.\n[16] Dongyue Chen, Pengtao Wang, Lingyi Yue, Yuxin Zhang, and Tong Jia. 2020. Anomaly detection in surveillance video based on bidirectional\nprediction. Image and Vision Computing 98 (2020), 103915.\n[17] Dongyue Chen, Lingyi Yue, Xingya Chang, Ming Xu, and Tong Jia. 2021. NM-GAN: Noise-modulated generative adversarial network for video\nanomaly detection. Pattern Recognition 116 (2021), 107969.\n[18] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations.\nIn International conference on machine learning. PMLR, 1597–1607.\n[19] Weiling Chen, Keng Teck Ma, Zi Jian Yew, Minhoe Hur, and David Aik-Aun Khoo. 2023. TEVAD: Improved video anomaly detection with captions.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5548–5558.\n[20] Zhaoyu Chen, Bo Li, Jianghe Xu, Shuang Wu, Shouhong Ding, and Wenqiang Zhang. 2022. Towards Practical Certifiable Patch Defense with\nVision Transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 15148–15158.\n[21] Kai Cheng, Yang Liu, and Xinhua Zeng. 2023. Learning Graph Enhanced Spatial-Temporal Coherence for Video Anomaly Detection. IEEE Signal\nProcessing Letters 30 (2023), 314–318.\n[22] Kai Cheng, Xinhua Zeng, Yang Liu, Mengyang Zhao, Chengxin Pang, and Xing Hu. 2023. Spatial-Temporal Graph Convolutional Network Boosted\nFlow-Frame Prediction For Video Anomaly Detection. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 1–5.\n[23] Kai-Wen Cheng, Yie-Tarng Chen, and Wen-Hsien Fang. 2015. Video anomaly detection and localization using hierarchical feature representation\nand Gaussian process regression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2909–2917.\n[24] Yong Shean Chong and Yong Haur Tay. 2017. Abnormal event detection in videos using spatiotemporal autoencoder. In International symposium\non neural networks. Springer, 189–196.\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n29\n[25] Peter Christiansen, Lars N Nielsen, Kim A Steen, Rasmus N Jørgensen, and Henrik Karstoft. 2016. DeepAnomaly: Combining background subtraction\nand deep learning for detecting obstacles and anomalies in an agricultural field. Sensors 16, 11 (2016), 1904.\n[26] Andrew A Cook, Göksel Mısırlı, and Zhong Fan. 2019. Anomaly detection for IoT time-series data: A survey. IEEE Internet of Things Journal 7, 7\n(2019), 6481–6494.\n[27] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. 2022. Diffusion models in vision: A survey. arXiv preprint\narXiv:2209.04747 (2022).\n[28] Xinyi Cui, Qingshan Liu, Mingchen Gao, and Dimitris N Metaxas. 2011. Abnormal detection using interaction energy potentials. In CVPR 2011.\nIEEE, 3161–3167.\n[29] Navneet Dalal and Bill Triggs. 2005. Histograms of oriented gradients for human detection. In 2005 IEEE computer society conference on computer\nvision and pattern recognition (CVPR’05), Vol. 1. Ieee, 886–893.\n[30] Navneet Dalal, Bill Triggs, and Cordelia Schmid. 2006. Human detection using oriented histograms of flow and appearance. In European conference\non computer vision. Springer, 428–441.\n[31] K Deepak, S Chandrakala, and C Krishna Mohan. 2021. Residual spatiotemporal autoencoder for unsupervised video anomaly detection. Signal,\nImage and Video Processing 15, 1 (2021), 215–222.\n[32] Hanqiu Deng, Zhaoxiang Zhang, Shihao Zou, and Xingyu Li. 2023. Bi-Directional Frame Interpolation for Unsupervised Video Anomaly Detection.\nIn Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2634–2643.\n[33] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. 2020. Model compression and hardware acceleration for neural networks: A comprehensive\nsurvey. Proc. IEEE 108, 4 (2020), 485–532.\n[34] Piotr Dollár, Vincent Rabaud, Garrison Cottrell, and Serge Belongie. 2005. Behavior recognition via sparse spatio-temporal features. In 2005 IEEE\ninternational workshop on visual surveillance and performance evaluation of tracking and surveillance. IEEE, 65–72.\n[35] Fei Dong, Yu Zhang, and Xiushan Nie. 2020. Dual discriminator generative adversarial network for video anomaly detection. IEEE Access 8 (2020),\n88170–88176.\n[36] Keval Doshi and Yasin Yilmaz. 2020. Any-shot sequential anomaly detection in surveillance videos. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops. 934–935.\n[37] Keval Doshi and Yasin Yilmaz. 2021. Online anomaly detection in surveillance videos with asymptotic bound on false alarm rate. Pattern Recognition\n114 (2021), 107865.\n[38] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. 2017. CARLA: An open urban driving simulator. In\nConference on robot learning. PMLR, 1–16.\n[39] Jia-Chang Feng, Fa-Ting Hong, and Wei-Shi Zheng. 2021. Mist: Multiple instance self-training framework for video anomaly detection. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition. 14009–14018.\n[40] Xinyang Feng, Dongjin Song, Yuncong Chen, Zhengzhang Chen, Jingchao Ni, and Haifeng Chen. 2021. Convolutional transformer based dual\ndiscriminator generative adversarial networks for video anomaly detection. In Proceedings of the 29th ACM International Conference on Multimedia.\n5546–5554.\n[41] Yachuang Feng, Yuan Yuan, and Xiaoqiang Lu. 2017. Learning deep event models for crowd anomaly detection. Neurocomputing 219 (2017), 548–556.\n[42] Félix Fuentes-Hurtado, Abdolrahim Kadkhodamohammadi, Evangello Flouty, Santiago Barbarisi, Imanol Luengo, and Danail Stoyanov. 2019.\nEasyLabels: weak labels for scene segmentation in laparoscopic videos. International journal of computer assisted radiology and surgery 14, 7 (2019),\n1247–1257.\n[43] Mariana-Iuliana Georgescu, Antonio Barbalau, Radu Tudor Ionescu, Fahad Shahbaz Khan, Marius Popescu, and Mubarak Shah. 2021. Anomaly\ndetection in video via self-supervised and multi-task learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.\n12742–12752.\n[44] Mariana Iuliana Georgescu, Radu Tudor Ionescu, Fahad Shahbaz Khan, Marius Popescu, and Mubarak Shah. 2021. A background-agnostic framework\nwith adversarial training for abnormal event detection in video. IEEE transactions on pattern analysis and machine intelligence 44, 9 (2021), 4505–4523.\n[45] Ross Girshick. 2015. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision. 1440–1448.\n[46] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. 2019. Memorizing\nnormality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In Proceedings of the IEEE/CVF International\nConference on Computer Vision. 1705–1714.\n[47] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey. International Journal of Computer\nVision 129, 6 (2021), 1789–1819.\n[48] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires,\nZhaohan Guo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural\ninformation processing systems 33 (2020), 21271–21284.\n[49] H Haberfehlner, AI Buizer, KL Stolk, SS van de Ven, I Aleo, LA Bonouvrié, J Harlaar, and MM van der Krogt. 2020. Automatic video tracking using\ndeep learning in dyskinetic cerebral palsy. Gait Posture 81 (2020), 132–133.\n[50] Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K Roy-Chowdhury, and Larry S Davis. 2016. Learning temporal regularity in video\nsequences. In Proceedings of the IEEE conference on computer vision and pattern recognition. 733–742.\nManuscript submitted to ACM\n30\nYang Liu et al.\n[51] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2022. Masked autoencoders are scalable vision learners. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16000–16009.\n[52] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition. 9729–9738.\n[53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference\non computer vision and pattern recognition. 770–778.\n[54] Ryota Hinami, Tao Mei, and Shin’ichi Satoh. 2017. Joint detection and recounting of abnormal events by learning deep generic knowledge. In\nProceedings of the IEEE international conference on computer vision. 3619–3627.\n[55] Jingtao Hu, Guang Yu, Siqi Wang, En Zhu, Zhiping Cai, and Xinzhong Zhu. 2022. Detecting Anomalous Events from Unlabeled Videos via Temporal\nMasked Auto-Encoding. In 2022 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 1–6.\n[56] Xing Hu, Yingping Huang, Xiumin Gao, Lingkun Luo, and Qianqian Duan. 2018. Squirrel-cage local binary pattern and its application in video\nanomaly detection. IEEE Transactions on Information Forensics and Security 14, 4 (2018), 1007–1022.\n[57] Chao Huang, Chengliang Liu, Jie Wen, Lian Wu, Yong Xu, Qiuping Jiang, and Yaowei Wang. 2022. Weakly Supervised Video Anomaly Detection\nvia Self-Guided Temporal Discriminative Transformer. IEEE Transactions on Cybernetics (2022).\n[58] Chao Huang, Yabo Liu, Zheng Zhang, Chengliang Liu, Jie Wen, Yong Xu, and Yaowei Wang. 2022. Hierarchical Graph Embedded Pose Regularity\nLearning via Spatio-Temporal Transformer for Abnormal Behavior Detection. In Proceedings of the 30th ACM International Conference on Multimedia.\n307–315.\n[59] Chao Huang, Jie Wen, Yong Xu, Qiuping Jiang, Jian Yang, Yaowei Wang, and David Zhang. 2022. Self-supervised attentive generative adversarial\nnetworks for video anomaly detection. IEEE transactions on neural networks and learning systems (2022).\n[60] Chao Huang, Zhihao Wu, Jie Wen, Yong Xu, Qiuping Jiang, and Yaowei Wang. 2021. Abnormal event detection using deep contrastive learning for\nintelligent video surveillance system. IEEE Transactions on Industrial Informatics 18, 8 (2021), 5171–5179.\n[61] Chao Huang, Zehua Yang, Jie Wen, Yong Xu, Qiuping Jiang, Jian Yang, and Yaowei Wang. 2021. Self-supervision-augmented deep autoencoder for\nunsupervised visual anomaly detection. IEEE Transactions on Cybernetics 52, 12 (2021), 13834–13847.\n[62] Radu Tudor Ionescu, Fahad Shahbaz Khan, Mariana-Iuliana Georgescu, and Ling Shao. 2019. Object-centric auto-encoders and dummy anomalies\nfor abnormal event detection in video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7842–7851.\n[63] Radu Tudor Ionescu, Sorina Smeureanu, Marius Popescu, and Bogdan Alexe. 2019. Detecting abnormal events in video using narrowed normality\nclusters. In 2019 IEEE winter conference on applications of computer vision (WACV). IEEE, 1951–1960.\n[64] Sabah Abdulazeez Jebur, Khalid A Hussein, Haider Kadhim Hoomod, Laith Alzubaidi, and José Santamaría. 2022. Review on Deep Learning\nApproaches for Anomaly Event Detection in Video Surveillance. Electronics 12, 1 (2022), 29.\n[65] Bobo Ju, Yang Liu, Liang Song, Guixiang Gan, Zengwen Li, and Linhua Jiang. 2023. A High-Reliability Edge-side Mobile Terminal Shared Computing\nArchitecture Based on Task Triple-stage Full-cycle Monitoring. IEEE Internet of Things Journal (2023).\n[66] Ammar Mansoor Kamoona, Amirali Khodadadian Gostar, Alireza Bab-Hadiashar, and Reza Hoseinnezhad. 2023. Multiple instance-based video\nanomaly detection using deep temporal encoding–decoding. Expert Systems with Applications 214 (2023), 119079.\n[67] B Ravi Kiran, Dilip Mathew Thomas, and Ranjith Parakkal. 2018. An overview of deep learning based methods for unsupervised and semi-\nsupervised anomaly detection in videos. Journal of Imaging 4, 2 (2018), 36.\n[68] Kwang-Eun Ko and Kwee-Bo Sim. 2018. Deep convolutional framework for abnormal behavior detection in a smart surveillance system. Engineering\nApplications of Artificial Intelligence 67 (2018), 226–234.\n[69] Viet-Tuan Le and Yong-Guk Kim. 2023. Attention-based residual autoencoder for video anomaly detection. Applied Intelligence 53, 3 (2023), 3240–\n3254.\n[70] Jooyeon Lee, Woo-Jeoung Nam, and Seong-Whan Lee. 2022. Multi-Contextual Predictions with Vision Transformer for Video Anomaly Detection.\nIn 2022 26th International Conference on Pattern Recognition (ICPR). IEEE, 1012–1018.\n[71] Sangmin Lee, Hak Gu Kim, and Yong Man Ro. 2019. BMAN: bidirectional multi-scale aggregation networks for abnormal event detection. IEEE\nTransactions on Image Processing 29 (2019), 2395–2408.\n[72] Roberto Leyva, Victor Sanchez, and Chang-Tsun Li. 2017. The LV dataset: A realistic surveillance video dataset for abnormal event detection. In\n2017 5th international workshop on biometrics and forensics (IWBF). IEEE, 1–6.\n[73] Roberto Leyva, Victor Sanchez, and Chang-Tsun Li. 2017. Video anomaly detection with compact feature sets for online performance. IEEE\nTransactions on Image Processing 26, 7 (2017), 3463–3478.\n[74] Di Li, Yang Liu, and Liang Song. 2022. Adaptive Weighted Losses with Distribution Approximation for Efficient Consistency-based Semi-supervised\nLearning. IEEE Transactions on Circuits and Systems for Video Technology 32, 11 (2022), 7832–7842.\n[75] Daoheng Li, Xiushan Nie, Xiaofeng Li, Yu Zhang, and Yilong Yin. 2022. Context-related video anomaly detection via generative adversarial\nnetwork. Pattern Recognition Letters 156 (2022), 183–189.\n[76] Di Li and Liang Song. 2022. Multi-Agent Multi-View Collaborative Perception Based on Semi-Supervised Online Evolutive Learning. Sensors 22, 18\n(2022), 6893.\n[77] Di Li, Xiaoguang Zhu, and Liang Song. 2022. Mutual match for semi-supervised online evolutive learning. Applied Intelligence (2022), 1–15.\n[78] Nanjun Li, Faliang Chang, and Chunsheng Liu. 2020. Spatial-temporal cascade autoencoder for video anomaly detection in crowded scenes. IEEE\nTransactions on Multimedia 23 (2020), 203–215.\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n31\n[79] Nannan Li, Jia-Xing Zhong, Xiujun Shu, and Huiwen Guo. 2022. Weakly-supervised anomaly detection in video surveillance via graph convolutional\nlabel noise cleaning. Neurocomputing 481 (2022), 154–167.\n[80] Nannan Li, Jia-Xing Zhong, Xiujun Shu, and Huiwen Guo. 2022. Weakly-supervised anomaly detection in video surveillance via graph convolutional\nlabel noise cleaning. Neurocomputing 481 (2022), 154–167.\n[81] Shuo Li, Fang Liu, and Licheng Jiao. 2022. Self-training multi-sequence learning with Transformer for weakly supervised video anomaly detection.\nProceedings of the AAAI, Virtual 24 (2022).\n[82] Tong Li, Xinyue Chen, Fushun Zhu, Zhengyu Zhang, and Hua Yan. 2021. Two-stream deep spatial-temporal auto-encoder for surveillance video\nabnormal event detection. Neurocomputing 439 (2021), 256–270.\n[83] Tangqing Li, Zheng Wang, Siying Liu, and Wen-Yan Lin. 2021. Deep unsupervised anomaly detection. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision. 3636–3645.\n[84] Weixin Li, Vijay Mahadevan, and Nuno Vasconcelos. 2013. Anomaly detection and localization in crowded scenes. IEEE transactions on pattern\nanalysis and machine intelligence 36, 1 (2013), 18–32.\n[85] Yuanyuan Li, Yiheng Cai, Jiaqi Liu, Shinan Lang, and Xinfeng Zhang. 2019. Spatio-Temporal Unity Networking for Video Anomaly Detection. IEEE\nAccess 7 (2019), 172425–172432. https://doi.org/10.1109/ACCESS.2019.2954540\n[86] Dongze Lian, Lina Hu, Weixin Luo, Yanyu Xu, Lixin Duan, Jingyi Yu, and Shenghua Gao. 2018. Multiview multitask gaze estimation with deep\nconvolutional neural networks. IEEE transactions on neural networks and learning systems 30, 10 (2018), 3010–3023.\n[87] Weipeng Lin, Yidong Li, Xiaoliang Yang, Peixi Peng, and Junliang Xing. 2019. Multi-view learning for vehicle re-identification. In 2019 IEEE\ninternational conference on multimedia and expo (ICME). IEEE, 832–837.\n[88] Xiangru Lin, Yuyang Chen, Guanbin Li, and Yizhou Yu. 2022. A Causal Inference Look at Unsupervised Video Anomaly Detection. In Thirty-Sixth\nAAAI Conference on Artificial Intelligence. 1620–1629.\n[89] Jing Liu, Yang Liu, Di Li, Hanqi Wang, Xiaohong Huang, and Liang Song. 2023. DSDCLA: Driving style detection via hybrid CNN-LSTM with\nmulti-level attention fusion. Applied Intelligence (2023), 1–18.\n[90] Jing Liu, Yang Liu, Wei Zhu, Xiaoguang Zhu, and Liang Song. 2023. Distributional and spatial-temporal robust representation learning for\ntransportation activity recognition. Pattern Recognition 140 (2023), 109568.\n[91] Wen Liu, Weixin Luo, Zhengxin Li, Peilin Zhao, Shenghua Gao, et al. 2019. Margin Learning Embedded Prediction for Video Anomaly Detection\nwith A Few Anomalies.. In IJCAI. 3023–3030.\n[92] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. 2018. Future frame prediction for anomaly detection–a new baseline. In Proceedings of the\nIEEE conference on computer vision and pattern recognition. 6536–6545.\n[93] Yang Liu, Zhengliang Guo, Jing Liu, Chengfang Li, and Liang Song. 2023. Osin: Object-centric scene inference network for unsupervised video\nanomaly detection. IEEE Signal Processing Letters 30 (2023), 359–363.\n[94] Yusha Liu, Chun-Liang Li, and Barnabás Póczos. 2018. Classifier Two Sample Test for Video Anomaly Detections.. In BMVC. 71.\n[95] Yang Liu, Di Li, Wei Zhu, Dingkang Yang, Jing Liu, and Liang Song. 2023. MSN-net: Multi-Scale Normality Network for Video Anomaly Detection.\nIn ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 1–5.\n[96] Yang Liu, Shuang Li, Jing Liu, Hao Yang, Mengyang Zhao, Xinhua Zeng, Wei Ni, and Liang Song. 2021. Learning Attention Augmented Spatial-\ntemporal Normality for Video Anomaly Detection. In 2021 3rd International Symposium on Smart and Healthy Cities (ISHC). IEEE, 137–144.\n[97] Yang Liu, Jing Liu, Jieyu Lin, Mengyang Zhao, and Liang Song. 2022. Appearance-Motion United Auto-Encoder Framework for Video Anomaly\nDetection. IEEE Transactions on Circuits and Systems II: Express Briefs 69, 5 (2022), 2498–2502.\n[98] Yang Liu, Jing Liu, Wei Ni, and Liang Song. 2022. Abnormal Event Detection with Self-guiding Multi-instance Ranking Framework. In 2022\nInternational Joint Conference on Neural Networks (IJCNN). IEEE, 01–07.\n[99] Yang Liu, Jing Liu, Kun Yang, Bobo Ju, Siao Liu, Yuzheng Wang, Dingkang Yang, Peng Sun, and Liang Song. 2023. AMP-Net: Appearance-Motion\nPrototype Network Assisted Automatic Video Anomaly Detection System. IEEE Transactions on Industrial Informatics (2023), 1–13.\n[100] Yang Liu, Jing Liu, Mengyang Zhao, Shuang Li, and Liang Song. 2022. Collaborative Normality Learning Framework for Weakly Supervised Video\nAnomaly Detection. IEEE Transactions on Circuits and Systems II: Express Briefs 69, 5 (2022), 2508–2512.\n[101] Yang Liu, Jing Liu, Mengyang Zhao, Dingkang Yang, Xiaoguang Zhu, and Liang Song. 2022. Learning Appearance-Motion Normality for Video\nAnomaly Detection. In 2022 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 1–6.\n[102] Yang Liu, Jing Liu, Xiaoguang Zhu, Donglai Wei, Xiaohong Huang, and Liang Song. 2022. Learning Task-Specific Representation for Video Anomaly\nDetection with Spatial-Temporal Attention. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2190–2194.\n[103] Yang Liu, Zhaoyang Xia, Mengyang Zhao, Donglai Wei, Yuzheng Wang, Siao Liu, Bobo Ju, Gaoyun Fang, Jing Liu, and Liang Song. 2023. Learning\nCausality-Inspired Representation Consistency for Video Anomaly Detection. In Proceedings of the 31st ACM International Conference on Multimedia.\n203–212.\n[104] Yang Liu, Dingkang Yang, Gaoyun Fang, Yuzheng Wang, Donglai Wei, Mengyang Zhao, Kai Cheng, Jing Liu, and Liang Song. 2023. Stochastic\nvideo normality network for abnormal event detection in surveillance videos. Knowledge-Based Systems (2023), 110986.\n[105] Zhian Liu, Yongwei Nie, Chengjiang Long, Qing Zhang, and Guiqing Li. 2021. A hybrid video anomaly detection framework via memory-augmented\nflow reconstruction and flow-guided frame prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 13588–13597.\nManuscript submitted to ACM\n32\nYang Liu et al.\n[106] Vina Lomte, Satish Singh, Siddharth Patil, Siddheshwar Patil, and Durgesh Pahurkar. 2019. A Survey on Real World Anomaly Detection in Live\nVideo Surveillance Techniques. International Journal of Research in Engineering, Science and Management 2, 2 (2019), 2581–5792.\n[107] Cewu Lu, Jianping Shi, and Jiaya Jia. 2013. Abnormal event detection at 150 fps in matlab. In Proceedings of the IEEE international conference on\ncomputer vision. 2720–2727.\n[108] Cewu Lu, Jianping Shi, Weiming Wang, and Jiaya Jia. 2019. Fast abnormal event detection. International Journal of Computer Vision 127, 8 (2019),\n993–1011.\n[109] Yiwei Lu, Frank Yu, Mahesh Kumar Krishna Reddy, and Yang Wang. 2020. Few-shot scene-adaptive anomaly detection. In European Conference on\nComputer Vision. Springer, 125–141.\n[110] Weixin Luo, Wen Liu, and Shenghua Gao. 2017. A revisit of sparse coding based anomaly detection in stacked rnn framework. In Proceedings of the\nIEEE international conference on computer vision. 341–349.\n[111] Weixin Luo, Wen Liu, and Shenghua Gao. 2021. Normal graph: Spatial temporal graph convolutional networks based prediction network for\nskeleton based video anomaly detection. Neurocomputing 444 (2021), 332–337.\n[112] Weixin Luo, Wen Liu, Dongze Lian, and Shenghua Gao. 2021. Future frame prediction network for video anomaly detection. IEEE Transactions on\nPattern Analysis and Machine Intelligence (2021).\n[113] Weixin Luo, Wen Liu, Dongze Lian, Jinhui Tang, Lixin Duan, Xi Peng, and Shenghua Gao. 2019. Video anomaly detection with sparse coding\ninspired deep neural networks. IEEE transactions on pattern analysis and machine intelligence 43, 3 (2019), 1070–1084.\n[114] Hui Lv, Zhongqi Yue, Qianru Sun, Bin Luo, Zhen Cui, and Hanwang Zhang. 2023. Unbiased Multiple Instance Learning for Weakly Supervised\nVideo Anomaly Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8022–8031.\n[115] Hui Lv, Chuanwei Zhou, Zhen Cui, Chunyan Xu, Yong Li, and Jian Yang. 2021. Localizing anomalies from weakly-labeled videos. IEEE transactions\non image processing 30 (2021), 4505–4515.\n[116] Ke Ma, Michael Doescher, and Christopher Bodden. 2015. Anomaly detection in crowded scenes using dense trajectories. University of Wisconsin-\nMadison (2015).\n[117] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and Leman Akoglu. 2021. A comprehensive survey on graph\nanomaly detection with deep learning. IEEE Transactions on Knowledge and Data Engineering (2021).\n[118] Vijay Mahadevan, Weixin Li, Viral Bhalodia, and Nuno Vasconcelos. 2010. Anomaly detection in crowded scenes. In 2010 IEEE computer society\nconference on computer vision and pattern recognition. IEEE, 1975–1981.\n[119] Snehashis Majhi, Ratnakar Dash, and Pankaj Kumar Sa. 2020. Two-Stream CNN architecture for anomalous event detection in real world scenarios.\nIn International Conference on Computer Vision and Image Processing. Springer, 343–353.\n[120] Amir Markovitz, Gilad Sharir, Itamar Friedman, Lihi Zelnik-Manor, and Shai Avidan. 2020. Graph embedded pose clustering for anomaly detection.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10539–10547.\n[121] Jefferson Ryan Medel and Andreas Savakis. 2016. Anomaly detection in video using predictive convolutional long short-term memory networks.\narXiv preprint arXiv:1612.00390 (2016).\n[122] Harshadkumar S Modi, Dr Parikh, and A Dhaval. 2022. A Survey on Crowd Anomaly Detection. International Journal of Computing and Digital\nSystems 12, 1 (2022), 1081–1096.\n[123] Ruwan Nawarathna, JungHwan Oh, Jayantha Muthukudage, Wallapak Tavanapong, Johnny Wong, Piet C De Groen, and Shou Jiang Tang. 2014.\nAbnormal image detection in endoscopy videos using a filter bank and local binary patterns. Neurocomputing 144 (2014), 70–91.\n[124] Rashmika Nawaratne, Damminda Alahakoon, Daswin De Silva, and Xinghuo Yu. 2019. Spatiotemporal anomaly detection using deep learning for\nreal-time video surveillance. IEEE Transactions on Industrial Informatics 16, 1 (2019), 393–402.\n[125] Rashmiranjan Nayak, Umesh Chandra Pati, and Santos Kumar Das. 2021. A comprehensive review on deep learning-based methods for video\nanomaly detection. Image and Vision Computing 106 (2021), 104078.\n[126] Khac-Tuan Nguyen, Dat-Thanh Dinh, Minh N Do, and Minh-Triet Tran. 2020. Anomaly detection in traffic surveillance videos with gan-based\nfuture frame prediction. In Proceedings of the 2020 International Conference on Multimedia Retrieval. 457–463.\n[127] Trong-Nguyen Nguyen and Jean Meunier. 2019. Anomaly detection in video sequence with appearance-motion correspondence. In Proceedings of\nthe IEEE/CVF international conference on computer vision. 1273–1283.\n[128] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. 2021. Deep learning for anomaly detection: A review. ACM Computing\nSurveys (CSUR) 54, 2 (2021), 1–38.\n[129] Guansong Pang, Cheng Yan, Chunhua Shen, Anton van den Hengel, and Xiao Bai. 2020. Self-trained deep ordinal regression for end-to-end video\nanomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 12173–12182.\n[130] Wen-Feng Pang, Qian-Hua He, Yong-jian Hu, and Yan-Xiong Li. 2021. Violence detection in videos based on fusing visual and audio information.\nIn ICASSP 2021-2021 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2260–2264.\n[131] Hyunjong Park, Jongyoun Noh, and Bumsub Ham. 2020. Learning memory-guided normality for anomaly detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 14372–14381.\n[132] Seongheon Park, Hanjae Kim, Minsu Kim, Dahye Kim, and Kwanghoon Sohn. 2023. Normality Guided Multiple Instance Learning for Weakly\nSupervised Video Anomaly Detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2665–2674.\n[133] Lam Pham, Dat Ngo, Tho Nguyen, Phu Nguyen, Truong Hoang, and Alexander Schindler. 2022. An audio-visual dataset and deep learning\nframeworks for crowded scene classification. In Proceedings of the 19th International Conference on Content-based Multimedia Indexing. 23–28.\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n33\n[134] Yujiang Pu and Xiaoyu Wu. 2022. Audio-Guided Attention Network for Weakly Supervised Violence Detection. In 2022 2nd International Conference\non Consumer Electronics and Computer Engineering (ICCECE). IEEE, 219–223.\n[135] Lang Qian, Peng Sun, Jing Liu, Azzedine Boukerche, and Liang Song. 2023. A Novel Bidirectional Optimization Framework for Intelligent Agents\nCapable of Online Evolution. arXiv preprint (2023).\n[136] Rohit Raja, Prakash Chandra Sharma, Md Rashid Mahmood, and Dinesh Kumar Saini. 2022. Analysis of anomaly detection in surveillance video:\nrecent trends and future vision. Multimedia Tools and Applications (2022), 1–17.\n[137] Bharathkumar Ramachandra and Michael Jones. 2020. Street Scene: A new dataset and evaluation protocol for video anomaly detection. In\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2569–2578.\n[138] Bharathkumar Ramachandra, Michael Jones, and Ranga Vatsavai. 2020. Learning a distance function with a Siamese network to localize anomalies\nin videos. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2598–2607.\n[139] Bharathkumar Ramachandra, Michael J Jones, and Ranga Raju Vatsavai. 2020. A survey of single-scene video anomaly detection. IEEE transactions\non pattern analysis and machine intelligence 44, 5 (2020), 2293–2312.\n[140] Mahdyar Ravanbakhsh, Moin Nabi, Hossein Mousavi, Enver Sangineto, and Nicu Sebe. 2018. Plug-and-play cnn for crowd motion analysis: An\napplication in abnormal event detection. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 1689–1698.\n[141] Mahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lucio Marcenaro, Carlo Regazzoni, and Nicu Sebe. 2017. Abnormal event detection in videos\nusing generative adversarial nets. In 2017 IEEE international conference on image processing (ICIP). IEEE, 1577–1581.\n[142] Mahdyar Ravanbakhsh, Enver Sangineto, Moin Nabi, and Nicu Sebe. 2019. Training adversarial discriminators for cross-channel abnormal event\ndetection in crowds. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 1896–1904.\n[143] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You only look once: Unified, real-time object detection. In Proceedings of the\nIEEE conference on computer vision and pattern recognition. 779–788.\n[144] Khosro Rezaee, Sara Mohammad Rezakhani, Mohammad R Khosravi, and Mohammad Kazem Moghimi. 2021. A survey on deep learning-based\nreal-time crowd anomaly detection for secure distributed video surveillance. Personal and Ubiquitous Computing (2021), 1–17.\n[145] Mehrsan Javan Roshtkhari and Martin D Levine. 2013. An on-line, real-time learning method for detecting anomalies in videos using spatio-\ntemporal compositions. Computer vision and image understanding 117, 10 (2013), 1436–1452.\n[146] Mohammad Sabokrou, Mahmood Fathy, Mojtaba Hoseini, and Reinhard Klette. 2015. Real-time anomaly detection and localization in crowded\nscenes. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 56–62.\n[147] Mohammad Sabokrou, Mohsen Fayyaz, Mahmood Fathy, and Reinhard Klette. 2017. Deep-cascade: Cascading 3d deep neural networks for fast\nanomaly detection and localization in crowded scenes. IEEE Transactions on Image Processing 26, 4 (2017), 1992–2004.\n[148] Venkatesh Saligrama and Zhu Chen. 2012. Video anomaly detection based on local statistical aggregates. In 2012 IEEE Conference on computer\nvision and pattern recognition. IEEE, 2112–2119.\n[149] Venkatesh Saligrama, Janusz Konrad, and Pierre-Marc Jodoin. 2010. Video anomaly identification. IEEE Signal Processing Magazine 27, 5 (2010), 18–33.\n[150] Kelathodi Kumaran Santhosh, Debi Prosad Dogra, and Partha Pratim Roy. 2020. Anomaly detection in road traffic using visual surveillance: A\nsurvey. ACM Computing Surveys (CSUR) 53, 6 (2020), 1–26.\n[151] Sam Sattarzadeh, Mahesh Sudhakar, and Konstantinos N Plataniotis. 2021. SVEA: A Small-scale Benchmark for Validating the Usability of Post-hoc\nExplainable AI Solutions in Image and Signal Recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 4158–4167.\n[152] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. 2018. Airsim: High-fidelity visual and physical simulation for autonomous vehicles.\nIn Field and service robotics. Springer, 621–635.\n[153] Yimeng Shang, Xiaoyu Wu, and Rui Liu. 2022. Multimodal Violent Video Recognition Based on Mutual Distillation. In Chinese Conference on\nPattern Recognition and Computer Vision (PRCV). Springer, 623–637.\n[154] Md Sharif, Lei Jiao, Christian W Omlin, et al. 2022. Deep Crowd Anomaly Detection: State-of-the-Art, Challenges, and Future Research Directions.\narXiv preprint arXiv:2210.13927 (2022).\n[155] Prakhar Singh and Vinod Pankajakshan. 2018. A Deep Learning Based Technique for Anomaly Detection in Surveillance Videos. In 2018 Twenty\nFourth National Conference on Communications (NCC). 1–6. https://doi.org/10.1109/NCC.2018.8599969\n[156] Liang Song, Xing Hu, Guanhua Zhang, Petros Spachos, Konstantinos N Plataniotis, and Hequan Wu. 2022. Networking systems of ai: on the\nconvergence of computing and communications. IEEE Internet of Things Journal 9, 20 (2022), 20352–20381.\n[157] Rupesh K Srivastava, Klaus Greff, and Jürgen Schmidhuber. 2015. Training very deep networks. Advances in neural information processing systems\n28 (2015).\n[158] Waqas Sultani, Chen Chen, and Mubarak Shah. 2018. Real-world anomaly detection in surveillance videos. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition. 6479–6488.\n[159] Qianru Sun, Hong Liu, and Tatsuya Harada. 2017. Online growing neural gas for anomaly detection in changing surveillance scenes. Pattern\nRecognition 64 (2017), 187–201.\n[160] Shengyang Sun and Xiaojin Gong. 2023. Hierarchical Semantic Contrast for Scene-aware Video Anomaly Detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 22846–22856.\n[161] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer\nvision. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2818–2826.\nManuscript submitted to ACM\n34\nYang Liu et al.\n[162] Yao Tang, Lin Zhao, Shanshan Zhang, Chen Gong, Guangyu Li, and Jian Yang. 2020. Integrating prediction and reconstruction for anomaly\ndetection. Pattern Recognition Letters 129 (2020), 123–130.\n[163] Zheng Tang, Renshu Gu, and Jenq-Neng Hwang. 2018. Joint multi-view people tracking and pose estimation for 3D scene reconstruction. In 2018\nIEEE International Conference on Multimedia and Expo (ICME). IEEE, 1–6.\n[164] Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh, Johan W Verjans, and Gustavo Carneiro. 2021. Weakly-supervised video anomaly\ndetection with robust temporal feature magnitude learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 4975–4986.\n[165] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. 2015. Learning spatiotemporal features with 3d convolutional\nnetworks. In Proceedings of the IEEE international conference on computer vision. 4489–4497.\n[166] Hanh TM Tran and David Hogg. 2017. Anomaly detection using a convolutional winner-take-all autoencoder. In Proceedings of the British Machine\nVision Conference 2017. British Machine Vision Association.\n[167] Radu Tudor Ionescu, Sorina Smeureanu, Bogdan Alexe, and Marius Popescu. 2017. Unmasking the abnormal events in video. In Proceedings of the\nIEEE international conference on computer vision. 2895–2903.\n[168] Francesco Turchini, Lorenzo Seidenari, and Alberto Del Bimbo. 2017. Convex polytope ensembles for spatio-temporal anomaly detection. In\nInternational Conference on Image Analysis and Processing. Springer, 174–184.\n[169] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nall you need. Advances in neural information processing systems 30 (2017).\n[170] Hung Vu, Tu Dinh Nguyen, Trung Le, Wei Luo, and Dinh Phung. 2019. Robust anomaly detection in videos using multilevel representations. In\nProceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 5216–5223.\n[171] Hung Vu, Dinh Phung, Tu Dinh Nguyen, Anthony Trevors, and Svetha Venkatesh. 2017. Energy-based models for video anomaly detection. arXiv\npreprint arXiv:1708.05211 (2017).\n[172] Boyang Wan, Yuming Fang, Xue Xia, and Jiajie Mei. 2020. Weakly supervised video anomaly detection via center-guided discriminative learning.\nIn 2020 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 1–6.\n[173] Heng Wang, Alexander Kläser, Cordelia Schmid, and Cheng-Lin Liu. 2013. Dense trajectories and motion boundary descriptors for action\nrecognition. International journal of computer vision 103, 1 (2013), 60–79.\n[174] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. 2022. Generalizing to\nunseen domains: A survey on domain generalization. IEEE Transactions on Knowledge and Data Engineering (2022).\n[175] Le Wang, Junwen Tian, Sanping Zhou, Haoyue Shi, and Gang Hua. 2023. Memory-augmented appearance-motion network for video anomaly\ndetection. Pattern Recognition 138 (2023), 109335.\n[176] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. 2016. Temporal segment networks: Towards good\npractices for deep action recognition. In European conference on computer vision. Springer, 20–36.\n[177] Tian Wang, Meina Qiao, Zhiwei Lin, Ce Li, Hichem Snoussi, Zhe Liu, and Chang Choi. 2018. Generative neural networks for anomaly detection in\ncrowded scenes. IEEE Transactions on Information Forensics and Security 14, 5 (2018), 1390–1399.\n[178] Xuanzhao Wang, Zhengping Che, Bo Jiang, Ning Xiao, Ke Yang, Jian Tang, Jieping Ye, Jingyu Wang, and Qi Qi. 2021. Robust unsupervised video\nanomaly detection by multipath frame prediction. IEEE transactions on neural networks and learning systems (2021).\n[179] Yuzheng Wang, Zhaoyu Chen, Dingkang Yang, Yang Liu, Siao Liu, Wenqiang Zhang, and Lizhe Qi. 2023. Adversarial contrastive distillation with\nadaptive denoising. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 1–5.\n[180] Yuzheng Wang, Zhaoyu Chen, Jie Zhang, Dingkang Yang, Zuhao Ge, Yang Liu, Siao Liu, Yunquan Sun, Wenqiang Zhang, and Lizhe Qi. 2023.\nSampling to Distill: Knowledge Transfer from Open-World Data. arXiv preprint arXiv:2307.16601 (2023).\n[181] Donglai Wei, Yang Liu, Xiaoguang Zhu, Jing Liu, and Xinhua Zeng. 2022. MSAF: Multimodal Supervise-Attention Enhanced Fusion for Video\nAnomaly Detection. IEEE Signal Processing Letters 29 (2022), 2178–2182.\n[182] Dong-Lai Wei, Chen-Geng Liu, Yang Liu, Jing Liu, Xiao-Guang Zhu, and Xin-Hua Zeng. 2022. Look, Listen and Pay More Attention: Fusing Multi-\nModal Information for Video Violence Detection. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 1980–1984.\n[183] Garrett Wilson and Diane J Cook. 2020. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology\n(TIST) 11, 5 (2020), 1–46.\n[184] Jhih-Ciang Wu, He-Yen Hsieh, Ding-Jie Chen, Chiou-Shann Fuh, and Tyng-Luh Liu. 2022. Self-supervised Sparse Representation for Video Anomaly\nDetection. In European Conference on Computer Vision. Springer, 729–745.\n[185] Peng Wu, Jing Liu, and Fang Shen. 2019. A deep one-class neural network for anomalous event detection in complex scenes. IEEE transactions on\nneural networks and learning systems 31, 7 (2019), 2609–2622.\n[186] Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, and Zhiwei Yang. 2020. Not only look, but also listen: Learning multimodal\nviolence detection under weak supervision. In European conference on computer vision. Springer, 322–339.\n[187] Peng Wu, Xiaotao Liu, and Jing Liu. 2022. Weakly supervised audio-visual violence detection. IEEE Transactions on Multimedia (2022).\n[188] Peihao Wu, Wenqian Wang, Faliang Chang, Chunsheng Liu, and Bin Wang. 2023. DSS-Net: Dynamic Self-Supervised Network for Video Anomaly\nDetection. IEEE Transactions on Multimedia (2023).\n[189] Xiaowei Xiang, Yang Liu, Gaoyun Fang, Jing Liu, and Mengyang Zhao. 2023. Two-Stage Alignments Framework for Unsupervised Domain\nAdaptation on Time Series Data. IEEE Signal Processing Letters (2023).\nManuscript submitted to ACM\nGeneralized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models\n35\n[190] Dan Xu, Elisa Ricci, Yan Yan, Jingkuan Song, and Nicu Sebe. 2015. Learning deep representations of appearance and motion for anomalous event\ndetection. arXiv preprint arXiv:1510.01553 (2015).\n[191] Dan Xu, Yan Yan, Elisa Ricci, and Nicu Sebe. 2017. Detecting anomalous events in videos by learning deep representations of appearance and\nmotion. Computer Vision and Image Understanding 156 (2017), 117–127.\n[192] Hang Xu, Lewei Yao, Wei Zhang, Xiaodan Liang, and Zhenguo Li. 2019. Auto-fpn: Automatic network architecture adaptation for object detection\nbeyond classification. In Proceedings of the IEEE/CVF international conference on computer vision. 6649–6658.\n[193] Dingkang Yang, Shuai Huang, Shunli Wang, Yang Liu, Peng Zhai, Liuzhen Su, Mingcheng Li, and Lihua Zhang. 2022. Emotion Recognition for\nMultiple Context Awareness. In European Conference on Computer Vision. Springer, 144–162.\n[194] Dingkang Yang, Shuai Huang, Zhi Xu, Zhenpeng Li, Shunli Wang, Mingcheng Li, Yuzheng Wang, Yang Liu, Kun Yang, Zhaoyu Chen, et al. 2023.\nAIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception. arXiv preprint arXiv:2307.13933 (2023).\n[195] Dingkang Yang, Yang Liu, Can Huang, Mingcheng Li, Xiao Zhao, Yuzheng Wang, Kun Yang, Yan Wang, Peng Zhai, and Lihua Zhang. 2023. Target\nand source modality co-reinforcement for emotion understanding from asynchronous multimodal sequences. Knowledge-Based Systems 265 (2023),\n110370.\n[196] Kun Yang, Dingkang Yang, Jingyu Zhang, Mingcheng Li, Yang Liu, Jing Liu, Hanqi Wang, Peng Sun, and Liang Song. 2023. Spatio-Temporal\nDomain Awareness for Multi-Agent Collaborative Perception. arXiv preprint arXiv:2307.13929 (2023).\n[197] Zhiwei Yang, Jing Liu, Zhaoyang Wu, Peng Wu, and Xiaotao Liu. 2023. Video Event Restoration Based on Keyframes for Video Anomaly Detection.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14592–14601.\n[198] Muchao Ye, Xiaojiang Peng, Weihao Gan, Wei Wu, and Yu Qiao. 2019. Anopcn: Video anomaly detection via deep predictive coding network. In\nProceedings of the 27th ACM International Conference on Multimedia. 1805–1813.\n[199] Qingze Yin, Guodong Ding, Shaogang Gong, Zhenmin Tang, et al. 2021. Multi-view label prediction for unsupervised learning person re-\nidentification. IEEE Signal Processing Letters 28 (2021), 1390–1394.\n[200] Guang Yu, Siqi Wang, Zhiping Cai, Xinwang Liu, En Zhu, and Jianping Yin. 2023. Video Anomaly Detection via Visual Cloze Tests. IEEE Transactions\non Information Forensics and Security (2023).\n[201] Guang Yu, Siqi Wang, Zhiping Cai, En Zhu, Chuanfu Xu, Jianping Yin, and Marius Kloft. 2020. Cloze test helps: Effective video anomaly detection\nvia learning to complete video events. In Proceedings of the 28th ACM International Conference on Multimedia. 583–591.\n[202] Jongmin Yu, Younkwan Lee, Kin Choong Yow, Moongu Jeon, and Witold Pedrycz. 2021. Abnormal event detection and localization via adversarial\nevent prediction. IEEE Transactions on Neural Networks and Learning Systems (2021).\n[203] Jiashuo Yu, Jinyu Liu, Ying Cheng, Rui Feng, and Yuejie Zhang. 2022. Modality-Aware Contrastive Instance Learning with Self-Distillation for\nWeakly-Supervised Audio-Visual Violence Detection. In Proceedings of the 30th ACM International Conference on Multimedia. 6278–6287.\n[204] Hongchun Yuan, Zhenyu Cai, Hui Zhou, Yue Wang, and Xiangzhi Chen. 2021. TransAnomaly: Video Anomaly Detection Using Video Vision\nTransformer. IEEE Access 9 (2021), 123977–123986.\n[205] Muhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid, and Seung-Ik Lee. 2020. Old is gold: Redefining the adversarially learned one-class\nclassifier training paradigm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14183–14193.\n[206] Muhammad Zaigham Zaheer, Arif Mahmood, Marcella Astrid, and Seung-Ik Lee. 2020. Claws: Clustering assisted weakly supervised learning with\nnormalcy suppression for anomalous event detection. In European Conference on Computer Vision. Springer, 358–376.\n[207] M Zaigham Zaheer, Arif Mahmood, M Haris Khan, Mattia Segu, Fisher Yu, and Seung-Ik Lee. 2022. Generative Cooperative Learning for\nUnsupervised Video Anomaly Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14744–14754.\n[208] Muhammad Zaigham Zaheer, Arif Mahmood, Hochul Shin, and Seung-Ik Lee. 2020. A self-reasoning framework for anomaly detection using\nvideo-level labels. IEEE Signal Processing Letters 27 (2020), 1705–1709.\n[209] Cheng Zhan, Han Hu, Zhi Wang, Rongfei Fan, and Dusit Niyato. 2019. Unmanned aircraft system aided adaptive video streaming: A joint\noptimization approach. IEEE Transactions on Multimedia 22, 3 (2019), 795–807.\n[210] Xiaohang Zhan, Jiahao Xie, Ziwei Liu, Yew-Soon Ong, and Chen Change Loy. 2020. Online deep clustering for unsupervised representation\nlearning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 6688–6697.\n[211] Chen Zhang, Guorong Li, Yuankai Qi, Shuhui Wang, Laiyun Qing, Qingming Huang, and Ming-Hsuan Yang. 2023. Exploiting Completeness and\nUncertainty of Pseudo Labels for Weakly Supervised Video Anomaly Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. 16271–16280.\n[212] Dasheng Zhang, Chao Huang, Chengliang Liu, and Yong Xu. 2022. Weakly Supervised Video Anomaly Detection via Transformer-Enabled\nTemporal Relation Learning. IEEE Signal Processing Letters (2022).\n[213] Jiangong Zhang, Laiyun Qing, and Jun Miao. 2019. Temporal convolutional network with complementary inner bag loss for weakly supervised\nanomaly detection. In 2019 IEEE International Conference on Image Processing (ICIP). IEEE, 4030–4034.\n[214] Qianqian Zhang, Guorui Feng, and Hanzhou Wu. 2022. Surveillance video anomaly detection via non-local U-Net frame prediction. Multimedia\nTools and Applications (2022), 1–16.\n[215] Ying Zhang, Huchuan Lu, Lihe Zhang, and Xiang Ruan. 2016. Combining motion and appearance cues for anomaly detection. Pattern Recognition\n51 (2016), 443–452.\n[216] Zhenzhen Zhang, Jianjun Hou, Qinglong Ma, and Zhaohong Li. 2015. Efficient video frame insertion and deletion detection based on inconsistency\nof correlations between local binary pattern coded frames. Security and Communication networks 8, 2 (2015), 311–320.\nManuscript submitted to ACM\n36\nYang Liu et al.\n[217] Zhe Zhang, Shiyao Ma, Zhaohui Yang, Zehui Xiong, Jiawen Kang, Yi Wu, Kejia Zhang, and Dusit Niyato. 2022. Robust semi-supervised federated\nlearning for images automatic recognition in internet of drones. IEEE Internet of Things Journal (2022).\n[218] Mengyang Zhao, Yang Liu, Jing Liu, and Xinhua Zeng. 2022. Exploiting Spatial-temporal Correlations for Video Anomaly Detection. In 2022 26th\nInternational Conference on Pattern Recognition (ICPR). IEEE, 1727–1733.\n[219] Yiru Zhao, Bing Deng, Chen Shen, Yao Liu, Hongtao Lu, and Xian-Sheng Hua. 2017. Spatio-temporal autoencoder for video anomaly detection. In\nProceedings of the 25th ACM international conference on Multimedia. 1933–1941.\n[220] Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H Li, and Ge Li. 2019. Graph convolutional label noise cleaner: Train a plug-and-play\naction classifier for anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1237–1246.\n[221] Joey Tianyi Zhou, Jiawei Du, Hongyuan Zhu, Xi Peng, Yong Liu, and Rick Siow Mong Goh. 2019. Anomalynet: An anomaly detection network for\nvideo surveillance. IEEE Transactions on Information Forensics and Security 14, 10 (2019), 2537–2550.\n[222] Shifu Zhou, Wei Shen, Dan Zeng, Mei Fang, Yuanwang Wei, and Zhijiang Zhang. 2016. Spatial–temporal convolutional neural networks for\nanomaly detection and localization in crowded scenes. Signal Processing: Image Communication 47 (2016), 358–368.\n[223] Yi Zhu and Shawn Newsam. 2019. Motion-aware feature for improved video anomaly detection. In The British Machine Vision Conference. 1–12.\nManuscript submitted to ACM\n",
  "categories": [
    "cs.CV",
    "cs.MM"
  ],
  "published": "2023-02-10",
  "updated": "2024-02-01"
}