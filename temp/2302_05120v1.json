{
  "id": "http://arxiv.org/abs/2302.05120v1",
  "title": "Step by Step Loss Goes Very Far: Multi-Step Quantization for Adversarial Text Attacks",
  "authors": [
    "Piotr Gaiński",
    "Klaudia Bałazy"
  ],
  "abstract": "We propose a novel gradient-based attack against transformer-based language\nmodels that searches for an adversarial example in a continuous space of token\nprobabilities. Our algorithm mitigates the gap between adversarial loss for\ncontinuous and discrete text representations by performing multi-step\nquantization in a quantization-compensation loop. Experiments show that our\nmethod significantly outperforms other approaches on various natural language\nprocessing (NLP) tasks.",
  "text": "Step by Step Loss Goes Very Far:\nMulti-Step Quantization for Adversarial Text Attacks\nPiotr Gai´nski\nJagiellonian University\nArdigen\npiotr.gainski@doctoral.uj.edu.pl\nKlaudia Bałazy\nJagiellonian University\nklaudia.balazy@doctoral.uj.edu.pl\nAbstract\nWe propose a novel gradient-based attack\nagainst transformer-based language models\nthat searches for an adversarial example in a\ncontinuous space of token probabilities. Our\nalgorithm mitigates the gap between adversar-\nial loss for continuous and discrete text repre-\nsentations by performing multi-step quantiza-\ntion in a quantization-compensation loop. Ex-\nperiments show that our method signiﬁcantly\noutperforms other approaches on various natu-\nral language processing (NLP) tasks.\n1\nIntroduction\nDeep neural networks achieve impressive results,\nbut their vulnerability to adversarial attacks causes\nmajor security threats and is a concern when inter-\npreting or explaining model predictions.\nIn computer vision, the most successful at-\ntack methods use gradient-based optimization tech-\nniques (Carlini and Wagner, 2017; Madry et al.,\n2018). They minimize adversarial loss function\nthat encourages the prediction error and impercep-\ntibility of a generated example.\nDevelopment of optimization-based attacks in\nNLP is much more challenging due to the discrete\nnature of text. Recent methods (Guo et al., 2021;\nYuan et al., 2021) overcome this limitation by per-\nforming a gradient descent in the continuous space\nof token representations and eventually quantizing\nthem into discrete text.\nA quantization of a token can signiﬁcantly\nchange its embedding and cause an undesired\nchange of the loss value, degrading the adver-\nsarial example. To our knowledge, all existing\noptimization-based NLP attacks quantize all to-\nkens in a text at once, which creates a considerable\ngap between adversarial loss for continuous and\ndiscrete text representations.\nIn this paper, we propose MANGO1 (Multi-\nstep quANtization Gradient-based adversarial Op-\n1Code available at github.com/gmum/MANGO.\ntimizer): a novel optimization-based attack against\nTransformer (Vaswani et al., 2017) language mod-\nels that mitigates the aforementioned gap by per-\nforming multi-step quantization in a quantization-\ncompensation loop. MANGO quantizes continuous\ntoken representations one by one and reoptimizes\nthe adversarial example after each quantization to\ncompensate undesired degradation of adversarial\nloss value. The construction of MANGO intro-\nduces interesting problems that are addressed in\nSection 3. MANGO achieves superior performance\nin various NLP tasks, outperforming recent white-\nbox (optimization-based) and black-box attacks.\n2\nRelated Work\nAdversarial attacks can be roughly divided into two\ncategories: white-box attacks that have access to\nthe internal model’s states (e.g. gradient) and more\ncommon black-box attacks that only know outputs\nof the model. In our paper, we focus on a white-box\nversion of our MANGO attack. In Appendix D, we\ndevelop a version of MANGO that can be used in\nthe loosened black-box setting.\nBlack-Box Methods\nMost black-box NLP at-\ntacks deﬁne a space of character or word replace-\nments and heuristically search it for an adversarial\nexample (Yoo et al., 2020). The search space is lim-\nited with semantic ad hoc constraints (e.g. limiting\nedit distance or restricting possible replacements to\nsynonyms) to preserve the attack’s imperceptibility.\nSuch constraints disallow some speciﬁc perturba-\ntions (e.g. replacing a word with its antagonist even\nif the semantics is preserved in the context of other\nperturbations) and tend to generate semantically\nincorrect examples (Morris et al., 2020a).\nWhite-Box Methods\nMany white-box methods\nuse gradients to guide a heuristic search in a space\nof text perturbations (Ebrahimi et al., 2018; Cheng\net al., 2019; Xu and Du, 2020). Recent methods\ntake a step further and perform gradient descent\narXiv:2302.05120v1  [cs.CL]  10 Feb 2023\noptimization. They aim to ﬁnd an example that\nminimizes the adversarial loss function, which en-\ncourages the prediction error and the imperceptibil-\nity of the attack. Because the similarity and ﬂuency\nof an example are controlled by a powerful external\nmodel used in the loss, optimization-based methods\ndo not require hand-crafted semantic constraints,\nmaking them more ﬂexible than black-box ones.\nAdapting gradient descent in NLP attacks is a\nchallenging problem due to the discrete nature of\nthe optimized text. Yuan et al. (2021) overcome\nthis issue by performing optimization in the con-\ntinuous space of token embeddings and replacing\neach token with a possibly new token, which em-\nbedding is the closest to the optimized one. An\nalternative approach is the GBDA method (Guo\net al., 2021) that optimizes a continuous distribu-\ntion of stochastic one-hot vectors and repeatedly\nsamples adversarial examples from the optimized\ndistribution until it fools the attacked model.\nQuantization\nBoth methods mentioned above\nquantize all continuous representations of tokens to\na text at once. Quantization of a single token may\nsigniﬁcantly change its embedding and cause an\nundesirable change of adversarial loss value. When\nquantizing all tokens at once, the changes accu-\nmulate to a considerable gap between adversarial\nloss for continuous and discrete text representations\n(see Section 6). Our MANGO mitigates this gap.\n3\nMANGO\nThis section describes our MANGO method. Un-\nlike other optimization-based methods that quan-\ntize all token representations at once, MANGO con-\nstitutes an entirely new algorithm that quantizes a\ntoken and compensates for the resulting change in\nan adversarial loss value in a step-by-step manner.\nThe construction of MANGO introduces interest-\ning problems that are addressed in the Optimiza-\ntion, Vector Selection and Candidates Selection\nparagraphs and are further evaluated in Section 5.\nContinuous Token Representation\nThe ﬁrst\nlearnable layer of Transformer takes as input a se-\nquence of tokens x = (t1, ..., tn), where ti ∈2|V |\nhas a single non-zero binary value at index k indi-\ncating that it represents the k-th token in vocabulary\nV .\nSimilarly to Guo et al. (2021), we relax the in-\nput sequence x and replace one-hot encodings ti\nwith probability vectors πi. Because the ﬁrst learn-\nable Transformer layer is a simple linear layer, it\ncan take probability vectors as input without any\nmodiﬁcation.\nA probability vector πi constitutes probability\ndistribution over tokens from V .\nIn the embedding layer, the Transformer embeds\nprobability vectors with the function e:\ne(πi) =\n|V |\nX\nj=1\n(πi)jEj,\n(1)\nwhere Ej is the embedding vector of the j-th token.\nIf πi is quantized, meaning it is a one-hot vector\nrepresenting some token k, function e simply looks\nup the k-th embedding: e(πi) = Ek. In MANGO,\nπi is a probabilistic vector, and its embedding e(πi)\nis a mixture of embeddings of all tokens weighted\nby their probabilities πi. We parameterize πi with\nlogits Θi and a standard softmax function σ, so that\nπi = σ(Θi) and x = σ(Θ) for Θ = (Θ1, ..., Θn).\nLoss function\nLet m : X →R|Y | be a classiﬁer\nthat outputs logit vectors and properly predicts a la-\nbel y ∈Y for some datapoint x ∈X, meaning that\narg maxk m(x)k = y. An adversarial example is a\nsample x′ ∈X that is imperceptible (according to\nspeciﬁed criteria) from x but changes the output of\nthe model. In an optimization-based setting, search-\ning for an adversarial example is usually deﬁned as\na minimization of an adversarial loss function.\nFollowing Guo et al. (2021), we compose our\nadversarial loss L as a combination of margin loss\nlm, ﬂuency loss lf, and similarity loss ls:\nL(x′) = lm(m, x′, y)+λflf(g, x′)+λsls(g, x′, x),\n(2)\nwhere λf and λs are the coefﬁcients used to balance\nthe losses and g is a reference model.\nMargin loss lm encourages model m to missclas-\nsify x′ by a margin κ:\nlm(m, x′, y) = max(m(x′)y−max\nk̸=y m(x′)k+κ, 0).\nFluency loss lf promotes x′ with a high probabil-\nity of being generated by a causal language model\ng that predicts the next token distribution:\nlf(g, x′) = −\nn\nX\ni=1\n|V |\nX\nj=1\n(πi)jg(π1, ..., πi−1)j.\nSimilarity loss ls is based on BERTScore (Zhang\net al., 2020) and captures the semantic similar-\nity between x and x′ using contextualized embed-\ndings of tokens φg(x) = (v1, ..., vn) and φg(x′) =\n(v′\n1, ..., v′\nn) produced by the reference model g :\nls(g, x′, x) = −\nn\nX\ni=1\nwi max\nj\nvT\ni v′\nj,\nwhere wi is the inverse frequency of token ti.\nQuantization-Compensation\nLoop\nMANGO\nalgorithm searches for a x′ that minimizes L, quan-\ntizing and compensating it step by step. Algo-\nrithm 1 introduces the idea of MANGO.\nIn the ﬁrst line, the parameters Θ of x′ are ini-\ntialized, so that Θ′\nij = C · (xi)j for some constant\nC. Each loop starts with optimization of x′ with\nrespect to L. Then vector selection is performed\nto select π′\ni from x′ which will be quantized in the\ncurrent step. Given π′\ni, MANGO performs candi-\ndates selection and selects m the most promising\ntokens c1, ..., cm to which x′\ni can be quantized. In\nthe 6th line, each candidate cj is evaluated by com-\nputing L for a sequence x′ with vector π′\ni quantized\nto cj. Finally, π′\ni is quantized to the best cj chosen\nfrom the previous step. Quantized π′\ni will no longer\nbe updated during optimization. MANGO repeats\nlines 2-7 until all vectors in x′ are quantized.\nAlgorithm 1: MANGO\nData: adversarial loss L (eq. 2)\nResult: sentence x′ that minimizes L\n1 initialize x′ = (π′\n1, ..., π′\nn)\n2 while x′ is not fully quantized do\n3\noptimization: optimize parameters of\nx′\n4\nvector selection: select probabilistic\nvector π′\ni from x′ for quantization\n5\ncandidates selection: select m tokens\ncandidates from π′\ni\n6\nevaluate these m candidates with loss L\n7\nquantize π′\ni to best evaluated token\nOptimization\nWe optimize x′ with the Adam op-\ntimizer (Kingma and Ba, 2014) which is reset after\neach quantization (see Section 5). This allows x′\nto rapidly change its trajectory to compensate for\nthe degradation of L. The initial number of opti-\nmization steps is S, but it decreases by a factor of\n2 in each loop to reduce computational costs.\nVector Selection\nIn line 4th, we choose vector\nπ′\ni with the highest entropy (see Section 5), because\nits quantization will introduce the most signiﬁcant\nchange to x′ and is likely to increase the loss value\nthe most. Intuitively, we want such degrading quan-\ntizations to occur early in the algorithm, because\nthe more vectors are not quantized yet, the larger\ncapacity x′ has to compensate for degradation by\nﬁnding another local minimum of L.\nCandidates Selection\nIn this phase, we select m\ntokens that can be used to quantize the probability\nvector π′\ni with possibly a small degradation of L.\nQuantization of π′\ni with token k is a step qk =\n(−(π′\ni)1, −(π′\ni)2, ..., 1 −(π′\ni)k, ..., −(π′\ni)n) in the\nπ′\ni space. As π′\ni is likely to be in the proximity of\nits local minimum with respect to L, we want the\nstep qk to have (1) the lowest norm ∥qk∥possible\nand (2) follow the direction of the local (minus)\ngradient. We use this intuition in the formulation\nof the token score sk, which is a weighted mean of\nthe probability (π′\ni)k and the direction score dk:\nsk = λprob(π′\ni)k + (1 −λprob)dk.\n(3)\nNote that (π′\ni)k is inversely proportional to ∥qk∥.\nWe deﬁne dk as cosine similarity between qk and\nthe local (minus) gradient (see Section 5):\ndk =\nqk\n\u0010\n−∇π′\niL(x′)\n\u0011T\n∥qk∥· ∥∇π′\niL(x′)∥\n(4)\nWe then select m tokens with the highest scores sk.\n4\nExperiments\nIn this section, we evaluate MANGO on various\nNLP tasks and compare it to recent NLP attacks.\nBaselines\nWe compare our method with the lat-\nest white-box GBDA attack (Guo et al., 2021), as\nwell as recent black-box attacks implemented in\nTextAttack (Morris et al., 2020b): BERT-Attack (Li\net al., 2020), BAE (Garg and Ramakrishnan, 2020)\nand TextFooler (Jin et al., 2020). To emphasize the\nimportance of multi-step quantization, we evaluate\nthe Naive version of MANGO that performs quan-\ntization in one step. MANGO, Naive and GBDA\nattacks use identical loss. All hyperparameters are\nlisted in appendix A.\nTasks\nWe attack BERT models from TextAttack\nﬁne-tuned on three text classiﬁcation tasks: AG\nNews (Zhang et al., 2015), Yelp Reviews (Zhang\net al., 2015), IMDB (Maas et al., 2011), and MNLI\ntask for natural language inference, (Williams et al.,\n2018). In MNLI p., an attack is allowed to modify\nTask\nMethod\nAdv.\nAdv. prob.\nUSE sim.\nBERTScore\n∆perp.\n∆gram.\n# queries\nAG News\n(99.6)\nTextFooler\n16.2\n43.7 ± 26.0\n0.81 ± 0.13\n0.83 ± 0.10\n373 ± 548\n0.26 ± 0.69\n334 ± 224\nBert-Attack\n20.1\n45.7 ± 27.7\n0.83 ± 0.11\n0.86 ± 0.09\n86 ± 133\n0.06 ± 0.49\n620 ± 472\nBAE\n12.6\n41.1 ± 24.1\n0.78 ± 0.16\n0.84 ± 0.11\n157 ± 289\n0.07 ± 0.53\n424 ± 353\nnaive\n43.7\n44.5 ± 43.1\n0.82 ± 0.10\n0.87 ± 0.06\n67 ± 141\n0.13 ± 0.62\n102 ± 6\nGBDA\n12.9\n13.7 ± 29.4\n0.72 ± 0.13\n0.80 ± 0.09\n241 ± 382\n0.17 ± 0.72\n1098 ± 69\nMANGO\n2.7\n3.2 ± 15.3\n0.78 ± 0.10\n0.83 ± 0.06\n30 ± 108\n0.10 ± 0.63\n496 ± 125\nIMDB\n(98.2)\nTextFooler\n0.6\n34.1 ± 16.9\n0.94 ± 0.08\n0.93 ± 0.07\n108 ± 214\n01.03 ± 1.81\n761 ± 1 000\nBert-Attack\n0.6\n28.0 ± 18.6\n0.96 ± 0.07\n0.96 ± 0.05\n19 ± 38\n0.05 ± 0.65\n900 ± 922\nBAE\n0.2\n29.3 ± 18.3\n0.95 ± 0.08\n0.95 ± 0.06\n27 ± 59\n0.10 ± 0.76\n651 ± 665\nnaive\n30.5\n31.1 ± 42.6\n0.86 ± 0.09\n0.83 ± 0.10\n288 ± 346\n1.56 ± 2.75\n100 ± 13\nGBDA\n6.3\n7.0 ± 21.3\n0.83 ± 0.11\n0.79 ± 0.08\n294 ± 271\n1.44 ± 2.22\n1082 ± 146\nMANGO\n0.3\n0.7 ± 5.7\n0.88 ± 0.07\n0.83 ± 0.08\n59 ± 73\n0.99 ± 2.15\n1647 ± 746\nYelp\n(99.9)\nTextFooler\n4.5\n31.7 ± 22.6\n0.92 ± 0.10\n0.93 ± 0.06\n90 ± 192\n0.50 ± 01.06\n495 ± 526\nBert-Attack\n1.9\n28.3 ± 19.1\n0.93 ± 0.09\n0.94 ± 0.06\n16 ± 38\n0.00 ± 0.55\n665 ± 713\nBAE\n2.8\n30.5 ± 21.1\n0.92 ± 0.11\n0.93 ± 0.06\n29 ± 130\n0.06 ± 0.60\n501 ± 525\nnaive\n35.1\n35.8 ± 45.4\n0.82 ± 0.13\n0.84 ± 0.09\n25 ± 84\n0.75 ± 1.93\n102 ± 3\nGBDA\n4.5\n4.9 ± 18.3\n0.79 ± 0.12\n0.81 ± 0.06\n5 ± 42\n0.37 ± 1.59\n1101 ± 35\nMANGO\n8.5\n8.9 ± 27.4\n0.82 ± 0.12\n0.80 ± 0.07\n-30 ± 38\n0.34 ± 1.72\n1128 ± 718\nMNLI premise\n(94.7)\nTextFooler\n94.7\n-\n-\n-\n-\n-\n-\nBert-Attack\n3.9\n34.3 ± 23.5\n0.93 ± 0.08\n0.96 ± 0.04\n30 ± 58\n0.02 ± 0.26\n146 ± 148\nBAE\n5.0\n34.3 ± 23.5\n0.92 ± 0.09\n0.95 ± 0.04\n42 ± 107\n0.01 ± 0.26\n112 ± 108\nnaive\n31.6\n33.9 ± 24.0\n0.91 ± 0.07\n0.94 ± 0.04\n64 ± 116\n-0.01 ± 0.50\n97 ± 23\nGBDA\n5.9\n30.3 ± 21.9\n0.80 ± 0.12\n0.87 ± 0.07\n301 ± 446\n0.09 ± 0.67\n1044 ± 247\nMANGO\n2.4\n31.6 ± 23.3\n0.88 ± 0.08\n0.91 ± 0.05\n73 ± 123\n0.05 ± 0.60\n326 ± 125\nMNLI hyp.\n(94.7)\nTextFooler\n6.5\n35.5 ± 24.2\n0.94 ± 0.07\n0.95 ± 0.04\n77 ± 139\n0.13 ± 0.39\n77 ± 44\nBert-Attack\n2.6\n34.3 ± 24.3\n1.00 ± 0.01\n0.97 ± 0.03\n1 ± 0\n0.00 ± 0.06\n95 ± 62\nBAE\n3.5\n34.8 ± 24.4\n0.95 ± 0.06\n0.97 ± 0.03\n29 ± 57\n0.03 ± 0.25\n74 ± 39\nnaive\n8.4\n32.1 ± 22.7\n0.89 ± 0.08\n0.93 ± 0.04\n115 ± 209\n0.07 ± 0.36\n97 ± 23\nGBDA\n0.6\n27.4 ± 21.4\n0.81 ± 0.12\n0.89 ± 0.06\n220 ± 454\n0.09 ± 0.42\n1044 ± 247\nMANGO\n0.3\n30.0 ± 22.4\n0.89 ± 0.09\n0.93 ± 0.04\n85 ± 155\n0.06 ± 0.38\n258 ± 68\nTable 1: Results for black-box and white-box methods. We report: the initial training accuracy of BERT model\n(under Task); training accuracy under attack (Adv.); probability of ground-truth label prediction under attack (Adv.\nprob.); similarity between the original and perturbed text computed with USE (Cer et al., 2018) (USE sim.) and\nwith F1 BERTScore (BERTScore); percent change in perplexity computed with GPT-2 (Radford et al., 2019) (∆\nperpl.); increase in the number of grammar errors (∆gram.) obtained with LanguageTool (github.com/jxmorris12/\nlanguage_tool_python); average number of queries to a victim model (# queries). We omit results for TextFooler on\nMNLI p., as it has not generated any adversarial example. We also report standard deviation for each result, except\nadversarial accuracy as it is simply the percent of successful attacks. Our MANGO method achieves superior\nresults on most tasks while maintaining high semantic similarity and grammar ﬂuency. The best results for Adv.\nare bold.\nonly the premise, and in MNLI h., only the hypoth-\nesis. For each task, we randomly select 1000 attack\ntargets from the training set. We use a training set\nas it provides more challenging targets and is more\nrelevant to Adversarial Training (Bai et al., 2021).\nResults\nResults can be found in Table 1. Our\nMANGO substantially reduces the training accu-\nracy of the BERT model in all tasks, while main-\ntaining a high level of semantic similarity to the\noriginal input. The attacks of MANGO are difﬁ-\ncult (low Adv. prob., which indicates that model\nmisclassiﬁes an example by a large margin), ﬂuent\n(low ∆perp.) and do not ﬂaw the grammatical\ncorrectness (low ∆gram.).\nIn almost all settings, MANGO outperforms\nother attacks in terms of training accuracy, which\nwe believe to be the fairest metric for comparing\noptimization-based methods with black-box ones\ndue to inherent design biases (see Appendix B).\nMANGO surpasses the recent state-of-the-art\noptimization-based GBDA attack in terms of most\nconsidered metrics: in terms of Adv. acc. and\nBERTScore on 4/5 tasks and in terms of USE sim.,\n∆perpl. and ∆gram. on 5/5 tasks.\nMoreover, MANGO achieves considerably bet-\nter results than its Naive version, emphasizing the\nimportance of multi-step quantization.\nQualitative Results\nWe provide qualitative anal-\nysis of a few adversarial examples generated by\nBAE, GBDA, and MANGO in Appendix C.\n5\nAblation Study\nIn this section, we evaluate three solutions from\nSection 3 that improve the core idea of multi-step\nquantization:\n1. selection of probability vector to quantization\nby maximal entropy (instead of minimal en-\ntropy, which seems more natural choice),\n2. scoring token candidates by weighted mean of\ntoken probability and gradient direction score\n(eq. 4),\n3. resetting optimizer after every quantization.\nFigure 1 compares different MANGO settings.\nWe may observe that selection of probability vec-\ntor for quantization by maximal entropy (\"max en-\ntropy\") is better than selection by minimal entropy\n(\"min entropy\"). Resetting the optimizer after ev-\nery quantization enhances the performance for both\n\"max entropy\" and \"min entropy\" settings. Finally,\nwe see that MANGO beneﬁts from using both to-\nken’s probability and gradient direction to score\ntoken candidates.\nFigure 1:\nFinal adversarial losses for different\nMANGO setting.\n\"max entropy + optimizer resets\"\nstands for a version of MANGO that selects probability\nvector for quantization by maximal entropy and resets\noptimizer after every quantization. Rest of the names\nfollow the same pattern. We also present the inﬂuence\nof the coefﬁcient λprob used in token candidates scor-\ning function (eq. 4). Loss values are averaged over 10\nsamples from IMDB dataset.\n6\nVisualization of Quantization Gap\nTo visualize the quantization gap between adversar-\nial loss for continuous and discrete text representa-\ntions, we compared adversarial losses of MANGO,\nGBDA and a Naive version of MANGO that does\nnot use multi-step quantization. The comparison\ncan be found in Figure 2. We observe that the Naive\nmethod converges to the lowest value loss in the\noptimization phase, but the value explodes after\nquantization. The GBDA method, which samples\nprobability vectors that resemble discrete one-hot\nvectors using Gumbel-softmax (Jang et al., 2017),\nreaches a higher minimum, but its quantization gap\nis much smaller than that of Naive method. Finally,\nin the case of MANGO, we observe sudden peaks\nand slow declines of loss values that correspond\nto the quantization-compensation loop, in which\nthe quantization of single tokens is followed by the\ncompensation of the quantization gap. After opti-\nmization, MANGO continues to quantize tokens\nstep by step further decreasing the loss. MANGO\nobtains a signiﬁcantly lower ﬁnal adversarial loss\nthan GBDA and Naive, avoiding the quantization\ngap.\nFigure 2: Adversarial loss for epochs 50-200 of op-\ntimization for Naive, GBDA and MANGO methods.\nThe vertical dashed line shows the end of optimization.\nNaive and GBDA methods immediately quantize the\ntokens, while MANGO do it step by step. The right-\nmost points shows the ﬁnal adversarial loss value. We\nobserve that after optimization, MANGO continues to\nquantize tokens step by step and eventually reaches the\nbest adversarial loss value. Loss values are averaged\nover 9 samples from IMDB dataset.\n7\nConclusion\nWe developed MANGO, a novel optimization-\nbased attack against Transformer models that\nmitigates the gap between adversarial loss for\ncontinuous and discrete text representations us-\ning a quantization-compensation loop. MANGO\nachieves superior results on various NLP tasks,\noutperforming recent black-box and optimization-\nbased attacks.\nLimitations\nOne limitation is that the number of queries of\nMANGO to the attacked model depends on the\nlength of the input sequence. Therefore, MANGO\nmay suffer a long attack time on datasets with long\nsequences (like IMDB or Yelp).\nMoreover, MANGO is restricted only to token\nreplacement. The inability to insert or remove to-\nkens can lead to reduced attack performance.\nThe most important limitation is the white-box\nnature of MANGO that excludes it from applica-\ntions when the internal model’s states cannot be\nknown. To partially circumvent this limitation, we\npropose Gray MANGO - a version of MANGO\nthat can be used in the loosened black-box setting,\nwhich we call gray-box setting (see appendix D).\nAcknowledgements\nThe work of Klaudia Bałazy was carried out within\nthe research project \"Bio-inspired artiﬁcial neural\nnetwork\" (grant no. POIR.04.04.00-00-14DE/18-\n00) within the Team-Net program of the Founda-\ntion for Polish Science co-ﬁnanced by the European\nUnion under the European Regional Development\nFund. Piotr Gai´nski and Klaudia Bałazy are afﬁl-\niated with Doctoral School of Exact and Natural\nSciences at the Jagiellonian University.\nReferences\nTao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian\nWang. 2021. Recent advances in adversarial train-\ning for adversarial robustness. In Proceedings of the\nThirtieth International Joint Conference on Artiﬁcial\nIntelligence, IJCAI 2021, Virtual Event / Montreal,\nCanada, 19-27 August 2021, pages 4312–4321. ij-\ncai.org.\nNathaniel Berger, Stefan Riezler, Sebastian Ebert, and\nArtem Sokolov. 2021.\nDon’t search for a search\nmethod - simple heuristics sufﬁce for adversarial\ntext attacks. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2021, Virtual Event / Punta Cana,\nDominican Republic, 7-11 November, 2021, pages\n8216–8224. Association for Computational Linguis-\ntics.\nNicholas Carlini and David A. Wagner. 2017. Towards\nevaluating the robustness of neural networks.\nIn\n2017 IEEE Symposium on Security and Privacy, SP\n2017, San Jose, CA, USA, May 22-26, 2017, pages\n39–57. IEEE Computer Society.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Con-\nstant, Mario Guajardo-Cespedes, Steve Yuan, Chris\nTar, Yun-Hsuan Sung, Brian Strope, and Ray\nKurzweil. 2018. Universal sentence encoder. CoRR,\nabs/1803.11175.\nPin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi,\nand Cho-Jui Hsieh. 2017. ZOO: zeroth order opti-\nmization based black-box attacks to deep neural net-\nworks without training substitute models.\nIn Pro-\nceedings of the 10th ACM Workshop on Artiﬁcial In-\ntelligence and Security, AISec@CCS 2017, Dallas,\nTX, USA, November 3, 2017, pages 15–26. ACM.\nXiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue\nLin, Mingyi Hong, and David D. Cox. 2019. Zo-\nadamm: Zeroth-order adaptive momentum method\nfor black-box optimization. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 7202–7213.\nYong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.\nRobust neural machine translation with doubly ad-\nversarial inputs. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4324–4333, Florence, Italy. Associa-\ntion for Computational Linguistics.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing\nDou. 2018. Hotﬂip: White-box adversarial exam-\nples for text classiﬁcation.\nIn Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2018, Melbourne, Aus-\ntralia, July 15-20, 2018, Volume 2: Short Papers,\npages 31–36. Association for Computational Lin-\nguistics.\nSiddhant Garg and Goutham Ramakrishnan. 2020.\nBAE: BERT-based adversarial examples for text\nclassiﬁcation. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 6174–6181, Online. As-\nsociation for Computational Linguistics.\nChuan Guo, Alexandre Sablayrolles, Hervé Jégou, and\nDouwe Kiela. 2021.\nGradient-based adversarial\nattacks against text transformers.\nIn Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 5747–5757. Association for\nComputational Linguistics.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Categor-\nical reparameterization with gumbel-softmax. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2020. Is bert really robust? a strong base-\nline for natural language attack on text classiﬁcation\nand entailment.\nProceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, 34(05):8018–8025.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,\nand Xipeng Qiu. 2020. BERT-ATTACK: Adversar-\nial attack against BERT using BERT. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n6193–6202, Online. Association for Computational\nLinguistics.\nSijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan\nZhang, Alfred O. Hero III, and Pramod K. Varsh-\nney. 2020. A primer on zeroth-order optimization in\nsignal processing and machine learning: Principals,\nrecent advances, and applications. IEEE Signal Pro-\ncess. Mag., 37(5):43–54.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y. Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn The 49th Annual Meeting of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Proceedings of the Conference, 19-24 June,\n2011, Portland, Oregon, USA, pages 142–150. The\nAssociation for Computer Linguistics.\nAleksander Madry,\nAleksandar Makelov,\nLudwig\nSchmidt, Dimitris Tsipras, and Adrian Vladu. 2018.\nTowards deep learning models resistant to adver-\nsarial attacks. In 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings. OpenReview.net.\nJohn Morris, Eli Liﬂand, Jack Lanchantin, Yangfeng\nJi, and Yanjun Qi. 2020a.\nReevaluating adversar-\nial examples in natural language. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 3829–3839, Online. Association for\nComputational Linguistics.\nJohn X. Morris, Eli Liﬂand, Jin Yong Yoo, Jake\nGrigsby, Di Jin, and Yanjun Qi. 2020b. Textattack:\nA framework for adversarial attacks, data augmenta-\ntion, and adversarial training in NLP. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demon-\nstrations, EMNLP 2020 - Demos, Online, November\n16-20, 2020, pages 119–126. Association for Com-\nputational Linguistics.\nYurii E. Nesterov and Vladimir G. Spokoiny. 2017.\nRandom gradient-free minimization of convex func-\ntions. Found. Comput. Math., 17(2):527–566.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014.\nGlove: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing, EMNLP 2014, October 25-29, 2014,\nDoha, Qatar, A meeting of SIGDAT, a Special Inter-\nest Group of the ACL, pages 1532–1543. ACL.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nYining Wang, Simon S. Du, Sivaraman Balakrishnan,\nand Aarti Singh. 2018. Stochastic zeroth-order op-\ntimization in high dimensions.\nIn International\nConference on Artiﬁcial Intelligence and Statistics,\nAISTATS 2018, 9-11 April 2018, Playa Blanca,\nLanzarote, Canary Islands, Spain, volume 84 of\nProceedings of Machine Learning Research, pages\n1356–1365. PMLR.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018.\nA broad-coverage challenge corpus\nfor sentence understanding through inference.\nIn\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages\n1112–1122. Association for Computational Linguis-\ntics.\nJincheng Xu and Qingfeng Du. 2020.\nTexttricker:\nLoss-based and gradient-based adversarial attacks\non text classiﬁcation models. Engineering Applica-\ntions of Artiﬁcial Intelligence, 92:103641.\nJin Yong Yoo, John X. Morris, Eli Liﬂand, and Yan-\njun Qi. 2020.\nSearching for a search method:\nBenchmarking search algorithms for generating\nNLP adversarial examples.\nIn Proceedings of\nthe Third BlackboxNLP Workshop on Analyzing\nand Interpreting Neural Networks for NLP, Black-\nboxNLP@EMNLP 2020, Online, November 2020,\npages 323–332. Association for Computational Lin-\nguistics.\nLifan Yuan, Yichi Zhang, Yangyi Chen, and Wei Wei.\n2021.\nBridge the gap between cv and nlp!\na\ngradient-based textual adversarial attack framework.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with BERT.\nIn 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in Neural Information Pro-\ncessing Systems 28: Annual Conference on Neural\nInformation Processing Systems 2015, December 7-\n12, 2015, Montreal, Quebec, Canada, pages 649–\n657.\nA\nHyperparameters\nMANGO\nTo save computational resources dur-\ning candidates selection, we use the dynamic num-\nber of candidates m. We rescale the candidate\nscores sk to [0, 1] and take at most M = 5 can-\ndidates whose scores differ from the best score at\nmost by a threshold T = 0.5: sk ≥maxj sj −T.\nWe use λprob = 0.5 in Equation (4).\nWhite-Box\nAttacks\nMANGO,\nNaive\nand\nGBDA methods use the loss function Equation (2)\nwith the same parameters λs = 20, λf = 1,\nκ = 5 (taken from Guo et al. (2021)) for all\ntasks, except Yelp, where they use λs = 10. As\na reference model g, we used the GPT-2 model\ndownloaded from the ofﬁcial GBDA repository.\nWe set C = 10 for initialization of the adversarial\nsample parameters. The number of optimization\nepochs S = 100 for all models and the batch size\nin GBDA was set to 10.\nBlack-Box Attacks\nWe take TextFooler, BertAt-\ntack, and BAE implementations from TextAttack\n(Morris et al., 2020b) along with their original pa-\nrameters. For fair comparison, we set the USE\nsimilarity threshold to the lowest value (0.2) used\nalong these methods. Following the GBDA paper,\nwe slightly modify the BertAttack method to miti-\ngate its problem with subtokens and extremely long\ntime of attack.\nB\nComparison Fairness\nWhen comparing the results of optimization-based\n(MANGO, GBDA, Naive MANGO) and black-\nbox methods (TextFooler, Bert-Attack, BAE), we\nshould note that black-box methods stop perturb-\ning text as soon as they fool the model, while\noptimization-based attacks minimize adversarial\nloss (that encourage them to fool the model by\nsome margin) for some ﬁxed number of steps.\nThe former improves similarity metrics (USE sim.,\nBERTScore) and the latter highly decreases the\nmodel’s prediction on ground-truth labels (Adv.\nprob.), increasing the difﬁculty of generated sam-\nple. Therefore, we believe that training accuracy\nunder attack (Adv.) is the fairest metric to make a\ndirect comparison between optimization-based and\nclassic black-box methods.\nC\nAttack Examples\nTo draw some insights into MANGO performance,\nwe compared examples generated by BAE, GBDA\nand MANGO. We chose all the sentences from AG\nNews and MNLI hypothesis that were successfully\nperturbed by the three considered methods and on\nwhich the methods obtained USE cosine similar-\nity score greater than 0.9. We then sampled two\nsentences from AG News and two from MNLI hy-\npothesis tasks. To avoid cherry-picking, we ﬁxed\na seed and sampled only once. Examples can be\nfound in table 2 and in table 3. We are careful in\ndrawing any conclusion from the qualitative results,\nhowever, there seems to be a trend consistent with\nthe result from table 1 and our observations from\nappendix B: BAE perturbs less words than GBDA\nand MANGO, but also achieves lower conﬁdence\nof the mislassiﬁed label.\nD\nGray MANGO\nTo circumvent the white-box nature of MANGO\nattack, we additionally develop Gray MANGO: a\nversion of MANGO that can be used in the loos-\nened black-box setting, which we call gray-box\nsetting.\nGray-Box Setting\nGray MANGO is not strictly\na black-box attack, as it requires the attacked model\nto take probability vectors and needs access to to-\nken vocabulary V . Transformer-based models sat-\nisfy these assumptions: they usually share the same\nV and their embedding function e can be used for\nboth one-hot and probability vectors. However, to\navoid misconception, we call this loosened black-\nbox setting a grey-box setting.\nZeroth-Order Optimization\nGray MANGO is\nbased on Zeroth-Order Optimization (ZOO) (Nes-\nterov and Spokoiny, 2017). The idea of ZOO is to\napproximate the gradient using only zeroth order\nloss values. In computer vision, Chen et al. (2017)\ndeveloped a ZOO-based attack that signiﬁcantly\noutperforms other black-box attacks. We believe\nthat this success can be transferred to the NLP do-\nmain. Berger et al. (2021) have proposed an NLP\nattack that uses a discrete version of ZOO, but the\nresults were unsatisfactory. Our Gray MANGO\nmethod is the ﬁrst to successfully adapt the contin-\nuous version of ZOO in NLP attacks.\nFormulation\nThe main modiﬁcation with re-\nspect to MANGO is the use of the zeroth-order\ngradient approximation of the gradient ∇Θ′L(x′)\nMethod\nPrediction\nSentence\nAG News - Example no 1.\nOriginal\nworld (100%)\nair india trial witness said motivated by revenge ( reuters ) reuters\n- a desire for revenge motivated a prosecution witness to tell the\nair india bombing trial he had been asked to carry an mysterious\nsuitcase on to an airliner, defense lawyers charged on wednesday.\nBAE\nsci/tech (61%)\nair india trial witness said motivated by revenge ( reuters ) website\n- a desire for revenge motivated a prosecution witness to tell the air\nindia company s he had been asked to carry an mysterious suitcase\non to an account, defense lawyers charged on wednesday.\nGBDA\nbusiness (99%)\nair india trial witness said motivated by revenge - today\ninvestigative reuters reporting a desire for revenge motivated crim-\ninal prosecution witnesses to tell the air canada strike trial he had\nbeen asked to carry an mysterious suitcase on to an airliner, de-\nfense lawyers charged on tuesday.\nMANGO\nbusiness (100%)\nair indies trial witness said motivated by revenge ( reuters ) time\n- a desire for revenge motivated a prosecution witness to tell the\nair america arson trial he had been asked to carry a mysterious\nsuitcase on to an airliner, defense lawyers charged on monday.\nAG News - Example no 2.\nOriginal\nbusiness (91%)\nbrazil passes bankruptcy reform brazilian congress gives the green\nlight to a long awaited overhaul of bankruptcy laws, which it hopes\nwill reduce business and credit costs.\nBAE\nsci/tech (95%)\nbrazil passes bankruptcy reform brazilian congress gives the green\nlight to a long awaited overhaul of copyright laws, which it hopes\nwill reduce business and credit costs.\nGBDA\nworld (95%)\nbrazil passes bankruptcy reform brazilian congress gives the green\nlight to a long awaited overhaul of privacy laws, which it aims will\nreduce tourism and population impacts.\nMANGO\nworld (99%)\nbrazil passes golf reform brazilian congress gives the green light\nto a long awaited overhaul of elections laws, which it hopes will\nreduce spending and maintenance costs.\nTable 2: Attack examples sampled from AG News dataset.\n(Liu et al., 2020):\ne∇Θ′L(x′) = 1\nK\nK\nX\ni=1\nL(σ(Θ′ + µui)) −L(x′)\nµ\nui,\nwhere ui is a noise sampled from the normal distri-\nbution, µ is the scale factor and σ(Θ′ + µui) is x′\nwith noise µui added to its parameters Θ′.\nAs e∇Θ′L(x′) is unstable, we set λprob = 1 and\nuse AMSGrad variant of Adam (Chen et al., 2019)\nwithout reset after every quantization. To reduce\nthe high dimensionality of x′, which is an issue in\nZOO (Wang et al., 2018), we disallow replacement\nof the original token with tokens that have a co-\nsine similarity of GloVe (Pennington et al., 2014)\nembedding lower than 0.\nHyperparameters\nWe use almost the same pa-\nrameters as for MANGO (see appendix A), but with\nλprob = 1, S = 140 and λs = 80. To save compu-\ntational resources, we set S = 100 for the IMDB\nand Yelp datasets. Based on small grid search, we\nset the noise scaling parameter µ = 0.1.\nMethod\nPrediction\nSentence\nMNLI hypothesis - Example no 1.\nOriginal\ncontraditcion (96%)\npremise: the houses are built to a long - standing design and are\nﬁlled with embroidery, lace, and crochet work.\nhypothesis: there is no embroidery in the houses.\nBAE\nneutral (45%)\nhypothesis: there is no ﬁre in the houses.\nGBDA\nneutral (100%)\nhypothesis: there is liturgical embroidery in the houses.\nMANGO\nneutral (99%)\nhypothesis: there is no erosion in the ruins.\nMNLI hypothesis - Example no 2.\nOriginal\ncontradiction (100%)\npremise: whether the service emerges as an adaptation from pri-\nmary care or as an innovation from the ed is less important than\nwhether it can be evaluated to the satisfaction of those who make\nkey decisions about whether it becomes part of standard practice.\nhypothesis: key decision makers are not important to decided\nthings.\nBAE\nneutral (96%)\nhypothesis: consensus decision makers are not important to ﬁrst\nthings.\nGBDA\nneutral (98%)\nhypothesis: key decision makers are noted fairchild – emery\nassociates.\nMANGO\nneutral (99%)\nhypothesis: older ahlers are also important in this regard.\nTable 3: Attack examples sampled from MNLI hypothesis task.\nResults\nWe evaluated the Gray MANGO method\nand compared it to vanilla MANGO. Results can\nbe found in table 4.\nGray MANGO, which is the ﬁrst method to in-\ncorporate continuous ZOO in NLP attack, performs\ncompetitively with other black-box attacks in terms\nof training accuracy reduction, but struggles to keep\nadversarial examples similar to original texts. We\nbelieve that the performance of Gray MANGO may\nbe greatly elevated by a more thorough design of\nZOO components (Liu et al., 2020). This may be\nan interesting topic for future research.\nTask\nMethod\nAdv.\nAdv. prob.\nUSE sim.\nBERTScore\n∆perp.\n∆gram.\n# queries\nAG News\n(99.6)\nTextFooler\n16.2\n43.7 ± 26.0\n0.81 ± 0.13\n0.83 ± 0.10\n373 ± 548\n0.26 ± 0.69\n334 ± 224\nBert-Attack\n20.1\n45.7 ± 27.7\n0.83 ± 0.11\n0.86 ± 0.09\n86 ± 133\n0.06 ± 0.49\n620 ± 472\nBAE\n12.6\n41.1 ± 24.1\n0.78 ± 0.16\n0.84 ± 0.11\n157 ± 289\n0.07 ± 0.53\n424 ± 353\nG-MANGO\n9.7\n11.0 ± 25.8\n0.57 ± 0.23\n0.67 ± 0.14\n16k ± 47k\n-0.03 ± 0.61\n3728 ± 244\nMANGO\n2.7\n3.2 ± 15.3\n0.78 ± 0.10\n0.83 ± 0.06\n30 ± 108\n0.10 ± 0.63\n496 ± 125\nIMDB\n(98.2)\nTextFooler\n0.6\n34.1 ± 16.9\n0.94 ± 0.08\n0.93 ± 0.07\n108 ± 214\n01.03 ± 1.81\n761 ± 1 000\nBert-Attack\n0.6\n28.0 ± 18.6\n0.96 ± 0.07\n0.96 ± 0.05\n19 ± 38\n0.05 ± 0.65\n900 ± 922\nBAE\n0.2\n29.3 ± 18.3\n0.95 ± 0.08\n0.95 ± 0.06\n27 ± 59\n0.10 ± 0.76\n651 ± 665\nG-MANGO\n8.6\n10.8 ± 24.3\n0.65 ± 0.21\n0.66 ± 0.14\n16k ± 38k\n0.19 ± 1.97\n3142 ± 669\nMANGO\n0.3\n0.7 ± 5.7\n0.88 ± 0.07\n0.83 ± 0.08\n59 ± 73\n0.99 ± 2.15\n1647 ± 746\nYelp\n(99.9)\nTextFooler\n4.5\n31.7 ± 22.6\n0.92 ± 0.10\n0.93 ± 0.06\n90 ± 192\n0.50 ± 01.06\n495 ± 526\nBert-Attack\n1.9\n28.3 ± 19.1\n0.93 ± 0.09\n0.94 ± 0.06\n16 ± 38\n0.00 ± 0.55\n665 ± 713\nBAE\n2.8\n30.5 ± 21.1\n0.92 ± 0.11\n0.93 ± 0.06\n29 ± 130\n0.06 ± 0.60\n501 ± 525\nG-MANGO\n15.7\n16.4 ± 32.1\n0.62 ± 0.27\n0.69 ± 0.15\n14k ± 36k\n-0.01 ± 1.68\n2803 ± 516\nMANGO\n8.5\n8.9 ± 27.4\n0.82 ± 0.12\n0.80 ± 0.07\n-30 ± 38\n0.34 ± 1.72\n1128 ± 718\nMNLI p.\n(94.7)\nTextFooler\n94.7\n-\n-\n-\n-\n-\n-\nBert-Attack\n3.9\n34.3 ± 23.5\n0.93 ± 0.08\n0.96 ± 0.04\n30 ± 58\n0.02 ± 0.26\n146 ± 148\nBAE\n5.0\n34.3 ± 23.5\n0.92 ± 0.09\n0.95 ± 0.04\n42 ± 107\n0.01 ± 0.26\n112 ± 108\nG-MANGO\n35.1\n33.4 ± 23.0\n0.77 ± 0.18\n0.84 ± 0.10\n5876 ± 19k\n-0.06 ± 0.64\n3158 ± 761\nMANGO\n2.4\n31.6 ± 23.3\n0.88 ± 0.08\n0.91 ± 0.05\n73 ± 123\n0.05 ± 0.60\n326 ± 125\nMNLI h.\n(94.7)\nTextFooler\n6.5\n35.5 ± 24.2\n0.94 ± 0.07\n0.95 ± 0.04\n77 ± 139\n0.13 ± 0.39\n77 ± 44\nBert-Attack\n2.6\n34.3 ± 24.3\n1.00 ± 0.01\n0.97 ± 0.03\n1 ± 0\n0.00 ± 0.06\n95 ± 62\nBAE\n3.5\n34.8 ± 24.4\n0.95 ± 0.06\n0.97 ± 0.03\n29 ± 57\n0.03 ± 0.25\n74 ± 39\nG-MANGO\n9.1\n30.8 ± 22.4\n0.83 ± 0.13\n0.89 ± 0.07\n1402 ± 3272\n0.04 ± 0.35\n3387 ± 807\nMANGO\n0.3\n30.0 ± 22.4\n0.89 ± 0.09\n0.93 ± 0.04\n85 ± 155\n0.06 ± 0.38\n258 ± 68\nTable 4:\nComparison of Gray MANGO with black-box methods and vanilla MANGO. We report: the initial\ntraining accuracy of BERT model (under Task); training accuracy under attack (Adv.); probability of ground-truth\nlabel prediction under attack (Adv. prob.); similarity between the original and perturbed text computed with USE\n(Cer et al., 2018) (USE sim.) and with F1 BERTScore (BERTScore); percent change in perplexity computed\nwith GPT-2 (Radford et al., 2019) (∆perpl.); increase in the number of grammar errors (∆gram.) obtained\nwith LanguageTool (github.com/jxmorris12/language_tool_python); average number of queries to a victim model\n(# queries). We omit results for TextFooler on MNLI p., as it has not generated any adversarial example. We\nalso report standard deviation for each result, except adversarial accuracy as it is simply the percent of successful\nattacks. The best results for Adv. are bold.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-02-10",
  "updated": "2023-02-10"
}