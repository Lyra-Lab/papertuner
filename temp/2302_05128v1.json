{
  "id": "http://arxiv.org/abs/2302.05128v1",
  "title": "Translating Natural Language to Planning Goals with Large-Language Models",
  "authors": [
    "Yaqi Xie",
    "Chen Yu",
    "Tongyao Zhu",
    "Jinbin Bai",
    "Ze Gong",
    "Harold Soh"
  ],
  "abstract": "Recent large language models (LLMs) have demonstrated remarkable performance\non a variety of natural language processing (NLP) tasks, leading to intense\nexcitement about their applicability across various domains. Unfortunately,\nrecent work has also shown that LLMs are unable to perform accurate reasoning\nnor solve planning problems, which may limit their usefulness for\nrobotics-related tasks. In this work, our central question is whether LLMs are\nable to translate goals specified in natural language to a structured planning\nlanguage. If so, LLM can act as a natural interface between the planner and\nhuman users; the translated goal can be handed to domain-independent AI\nplanners that are very effective at planning. Our empirical results on GPT 3.5\nvariants show that LLMs are much better suited towards translation rather than\nplanning. We find that LLMs are able to leverage commonsense knowledge and\nreasoning to furnish missing details from under-specified goals (as is often\nthe case in natural language). However, our experiments also reveal that LLMs\ncan fail to generate goals in tasks that involve numerical or physical (e.g.,\nspatial) reasoning, and that LLMs are sensitive to the prompts used. As such,\nthese models are promising for translation to structured planning languages,\nbut care should be taken in their use.",
  "text": "Translating Natural Language to Planning Goals\nwith Large-Language Models\nYaqi Xie1,\nChen Yu1, Tongyao Zhu1,\nJinbin Bai1,\nZe Gong1, Harold Soh1,2\n1Dept. of Computer Science, School of Computing\n2Smart Systems Institute\nNational University of Singapore\n{yaqixie, yuchen21, tongyao.zhu, jinbin, zegong, harold}@comp.nus.edu.sg\nAbstract—Recent large language models (LLMs) have demon-\nstrated remarkable performance on a variety of natural language\nprocessing (NLP) tasks, leading to intense excitement about their\napplicability across various domains. Unfortunately, recent work\nhas also shown that LLMs are unable to perform accurate\nreasoning nor solve planning problems, which may limit their\nusefulness for robotics-related tasks. In this work, our central\nquestion is whether LLMs are able to translate goals speciﬁed\nin natural language to a structured planning language. If so,\nLLM can act as a natural interface between the planner and\nhuman users; the translated goal can be handed to domain-\nindependent AI planners that are very effective at planning. Our\nempirical results on GPT 3.5 variants show that LLMs are much\nbetter suited towards translation rather than planning. We ﬁnd\nthat LLMs are able to leverage commonsense knowledge and\nreasoning to furnish missing details from under-speciﬁed goals (as\nis often the case in natural language). However, our experiments\nalso reveal that LLMs can fail to generate goals in tasks that\ninvolve numerical or physical (e.g., spatial) reasoning, and that\nLLMs are sensitive to the prompts used. As such, these models\nare promising for translation to structured planning languages,\nbut care should be taken in their use.\nI. INTRODUCTION\nThe world is currently abuzz with enthusiasm over re-\ncent transformer-based models that are trained on very large\namounts of text data. Despite being trained in a purely self-\nsupervised fashion, these large language models (LLMs) have\nshown tremendous versatility in the types of tasks they can\nperform. Within robotics, recent works have examined their\nviability for planning and to provide high-level semantic\nknowledge [27, 24, 29, 2, 18, 4]. Despite their promise, recent\nevidence strongly suggests that LLMs are, on their own, poor\nplanners [34, 8, 21]. This is perhaps unsurprising given that\nthese models are trained for text completion, rather than to\nreason over speciﬁed domains.\nIn this work, we consider the task of translation (Fig. 1).\nInstead of asking an LLM to plan (which is perhaps a dubious\nenterprise), we simply ask it to extract a planning goal from\ngiven a natural language instruction. Given the effectiveness\nof LLMs on natural language tasks, this translation task may\nappear as a natural ﬁt and if a goal can indeed be extracted,\nwe can leverage highly-effective classical planners to derive a\nplan. This modular approach leverages the strengths of each\ncomponent: the LLM provides linguistic competence, while\nthe planner provides structured reasoning.\nHowever, can LLMs actually translate goals from natural\nlanguage to PDDL? Translation is a non-trivial task. In the\ncase of translation between human languages, we are keenly\naware that direct literal translation is often sub-optimal. Proper\ntranslation requires one to leverage contextual information\nand prior knowledge. When translating to PDDL, there are\nadditional constraints; PDDL is a formal language where small\nsyntax errors can cause failures to parse and subsequently plan.\nMoreover, goal translation needs to be compatible with the\ngiven PDDL domain and problem information.\nHere, we contribute an empirical study aimed at answering\nthe above question. Speciﬁcally, we use GPT-3.5 to translate\nEnglish instructions to PDDL goals. We present results from\ntwo domains, Blocksworld [31], which enables complex spec-\niﬁcations, and ALFRED [28], which is based on real-world\nhousehold environments. Our results indicate that GPT-3.5 can\nbe an effective translator. It is able to generate achievable goals\nthat are consistent with the natural language instruction and the\nprovided PDDL domain/problem speciﬁcations. In particular,\nwe ﬁnd that goals can be obtained even with ambiguous\ncommands or those that require general commonsense knowl-\nedge. For example, when asked to generate a goal from the\ninstruction “Set a table for two”, the LLM is able to furnish a\ncomplete goal speciﬁcation comprising correct tableware (e.g.,\ntwo cups, plates, and utensils).\nBased on the above, one tempting conclusion might be that\nwe can simply “plug-in” GPT-3.5 as a natural language inter-\nface to planners. However, the reality is less straightforward.\nOur experiments also show that LLMs can behave in counter-\nintuitive ways; they perform poorly on simple counting and\nspatial inference during translation. Example failure cases\ninclude simple commands like “Put the pen into the box\nthat has three books”. Through a subtask analysis approach,\nwe attempt to pinpoint how these models are failing — our\nanalysis suggests that the LLM fails to properly understand\npredicate semantics in the Blocksworld domain, nor appre-\nciates hierarchical relationships between objects in ALFRED.\nWe believe these results will be useful for those of us designing\nrobot systems that leverage LLMs as they highlight potential\npitfalls when using these black-box models. In summary, we\nshow that LLMs can be remarkably potent translators due to\narXiv:2302.05128v1  [cs.CL]  10 Feb 2023\nAlfred\nA. Domains\nLLM\nPrompt\n(deﬁne (domain put_task)\n (:requirements :typing)\n (:types\n    object\n    receptacle\n    …)\n (:predicates\n    (objectAtLocation ?o - object ?r - receptacle)\n    (inReceptacle ?o - object ?r - receptacle)\n    …)\n(:action GotoLocation\n    …))\nQ: (deﬁne (problem 12345)\n    (:domain put_task)\n    (:objects\n        Apple1 - object\n        CounterTop1 - receptacle\n        Fridge1 - receptable\n        ...)\n     (:init\n        (objectAtLocation Apple1 CounterTop1)\n        ...))\n    Put an apple in the fridge.\n    Write the goal speciﬁcation in PDDL:\nA: (:goal \n        (inReceptacle Apple1 Fridge1))\nQ: (deﬁne (problem 54321)\n    (:domain put_task)\n    (:objects\n        Laptop1 - object\n        Desk1 - receptacle\n        ...)\n     (:init\n        (objectAtLocation Laptop1 Sofa1)\n        ...))\n    Put a laptop in the drawer.\n    Write the goal speciﬁcation in PDDL:\nA: \n(:goal \n        (inReceptacle Laptop1 Drawer1))\nDomain\nPDDL\nOne-shot\nProblem\nPDDL \nTest\nProblem\nPDDL \nOne-shot\nLanguage\nInstruction\nOne-shot\nGoal PDDL\nTest\nLanguage\nInstruction\nBlocksworld\nB. n-Shot Prompting\nFig. 1.\nThis paper investigates LLM-based translation of natural language\ninto PDDL goals. (A) Our experiments use two domains: Blocksworld and\nAlfred. (B) Using n-shot prompts (n = 1 in this example), we ﬁnd that\nLLMs can effectively translate English goals to PDDL when the language\ninstruction is sufﬁciently speciﬁc. However, the LLM fails to consistently\nproduce accurate goals in tasks that require formal or physical reasoning on\nthe speciﬁed planning domain/problem.\ntheir linguistic competence (and certainly, are more effective\nat translation than at planning), but at the same time, more\nresearch is needed to successfully apply them to general goal\ntranslation.\nII. PRELIMINARIES AND RELATED WORK\nIn this section, we provide a brief overview of background\nmaterial and prior work related to our goal translation. Here,\nwe focus on enabling robots to comprehend human utterances;\nthere is signiﬁcant work in enabling robots to generate ut-\nterances (e.g.,\n[5]), which we will not delve into here. We\nbegin with a description of Large Language Models (LLMs),\nfollowed by a short review of PDDL and machine translation\nto structured languages.\nA. Large Language Models\nLarge Language Models (LLMs) are self-supervised non-\nparametric models that are trained on a very large corpus\nof text data. They typically have a considerable number of\nparameters, ranging from hundreds of millions [9] to billions\n[3]. Examples of LLMs include Google’s PaLM [7] and\nLaMDA [33], and OpenAI’s GPT models [3]. LLMs are\ntypically trained to perform language completion, e.g., to\npredict the likelihood of the next tokens conditioned upon\nsome text. Nevertheless, by providing suitable prompts, LLMs\ncan be made to perform surprisingly well on many NLP\ntasks [3, 9, 26]. In particular, when given a few task examples\n(called a few-shot prompt), LLMs can quickly be quickly\nadapted for speciﬁc use-cases [16, 19, 17, 2]. Our experiments\nin this work are on GPT-3.5 variants with zero-shot prompts (a\nquery but no examples) and n-shot prompts (where n examples\nare provided along with the query).\nB. Planning Domain Deﬁnition Language (PDDL)\nThe Planning Domain Deﬁnition Language (PDDL) [11]\nis a family of STRIPS-style languages that deﬁne a planning\nproblem. In the following, we give a brief overview of PDDL\nand refer interested readers to comprehensive guides [13]. A\nPDDL planning problem is split into two parts [1]:\n• A domain that deﬁnes the “universal” aspects of a prob-\nlem, i.e., the elements that are present in all speciﬁc\nproblem instances such as object types, predicates, and\navailable actions. An action speciﬁes a change to the state\nof the world and is typically structured into three parts:\nits parameters, preconditions, and effects.\n• A problem which describes the exact instance or situation\nthat we aim to solve. It speciﬁes the objects that exist in\nthe world and an initial state. A key element of interest\nin this work is the goal. The goal is a set of logical\nexpressions of predicates that should be satisﬁed.\nState-of-the-art AI planners such as Fast Downward [15] take\nas input a PDDL domain and problem, and output a plan that\nresults in goal satisfaction.\nC. Machine Translation from Natural Language to Structured\nLanguages\nGoal translation is a speciﬁc form of machine translation to\nstructured languages. Direct linguistic parsing and mechanical\nsubstitution often fail to produce useful goal translations —\nnatural language goals can be stated in various ways and may\nbe incomplete, requiring common sense reasoning and world\nknowledge to furnish missing details. For example, translating\n“make a tomato sandwich” into a PDDL goal requires a robot\nto know the composition of a tomato sandwich (that the tomato\nis sliced and between two slices of bread). Moreover, the\ntranslator has to be linguistically competent; it has to generate\na goal that is syntactically correct and consistent with the\ndomain speciﬁcation.\nThere is signiﬁcant prior work in structured programming\ncode generation using natural language and recent work ap-\nplying LLMs towards programming languages [6]. Within the\ncontext of natural language understanding for robot planning\nand decision-making, early works applied classical NLP tools\nand predeﬁned rules, e.g., for extracting objects and actions\nto generate a PDDL domain [32]. Later works used non-\nparametric ﬂexible models such as deep neural networks for\nvarious tasks involving the extraction of PDDL models/plans\nfrom natural language [32, 22, 10, 30] and to translate natural\nlanguage commands to LTL goal speciﬁcations [14, 25].\nRecent work has applied LLMs for translation to linear\ntemporal logic (LTL) formulae [12, 20]. In this work, we\nfocus on translation to PDDL. Very recent work has shown\nthat LLMs are unable to reliably generate plans in PDDL\ndomains [23, 29].\nOur work is related to very recent work by Collins et al. [8],\nwhich showed that a “parse-and-solve” model — using the\nLLM to translate a natural language goal into PDDL that can\nbe processed by a planner — was more effective than direct\nLLM planning. This parse-and-solve approach is intuitive\nand appealing in that it separates goal understanding from\nplanning. PDDL goals are interpretable and it is possible to\nperform constraint checking and veriﬁcation of the goal before\npassing it to a planner. While initial results are promising [8],\nit remains unclear how effective LLMs are at translation.\nIn our study, we examine this question in a comprehensive\nmanner using various tasks (e.g., partially-speciﬁed goals with\nambiguity) in two domains.\nIII. PROBLEM STATEMENT: GOAL TRANSLATION WITH\nLARGE-LANGUAGE MODELS\nIn this work, we investigate the potential of state-of-the-\nart large-language models (LLMs) — speciﬁcally, variants of\nGPT-3.5 — for n-shot translation. Given a query prompt q, we\nare interested in obtaining a goal state g speciﬁed in planning\nlanguage (PDDL). We assume the goal g can be sampled from\na posterior distribution,\ng ∼p(g|q)\n(1)\nwhere the query q = (qm, qe, qn) comprises a description of\nthe domain qm (written in PDDL) and the desired goal state\nin natural language qn. q also contains qe which comprises\nn examples of correct translations; this form of prompting is\nhighly effective for inducing desired outputs from LLMs and\nhas been used in prior work on LLM planning and translation\n(e.g., [29]). An example prompt used in our work is shown in\nFig. 1.\nIt is generally challenging to precisely pinpoint the exact\nreason for observed performance since state-of-the-art LLMs\nare large transformer-based models. Moreover, these models\nremain closed-sourced with limited API access. Prior works\nhave tended to focus on success rates that illustrate under\nwhat circumstances (or what tasks) these LLMs can plan\nor translate [8]. We will follow suit and also report success\nrates as a primary metric. However, success rates only give\nus a glimpse into the capabilities and limitations of LLMs.\nIt remains an open question as to what underlies a model’s\nspeciﬁc performance on a task (or lack thereof).\nA. Task Types\nTo better understand under what circumstances LLMs can\neffectively translate, we design tasks that require different\nunderlying capabilities. These tasks range from simple fully\nspeciﬁed tasks (where the goal conﬁguration is speciﬁed\nunambiguously) to partially-speciﬁed instructions where in-\nference or commonsense reasoning is required. We delay\nthe precise description of our task types to Sec. IV on the\nexperimental setup.\nB. Translation Subtasks\nWe will attempt to understand the limitations of the LLM\nvia an analytical approach that breaks apart the translation\ntask and the conditional probability distribution in Eqn. (1). In\nparticular, our translation task can be cognitively decomposed\ninto three key steps:\n1) Domain Understanding: the LLM has to parse and\ncomprehend the PDDL domain speciﬁcation;\n2) Goal Inference: the LLM has to infer the intended\ngoal given the natural language utterance and domain\nknowledge;\n3) PDDL Goal Speciﬁcation: the LLM has to output the\ninferred goal in the correct PDDL syntax.\nIn a similar vein, we can factorize Eqn. 1 into the following:\np(g|q) =\nZZ\np(g|z, m, q)p(z|m, q)p(m|q)dmdz\n(2)\nwhere m represents domain information that can be in-\nferred from the query q, i.e., the objects, actions, and pred-\nicates within the PDDL domain speciﬁcation. The distribu-\ntion p(z|m, q) represents the posterior over a “latent” goal\nz obtained via inference over the domain m and natural\nlanguage goal within the query. This latent goal z needs\nto be transcripted into PDDL goal g. While the LLM may\nnot explicitly represent the distributions in (2) above, the\nfactorization applies to any conditional distribution (1). This\nfactorization, along with the decomposition of the transla-\ntion task, above provides a starting point for our analytical\napproach. Potentially, we can investigate possible causes by\nchanging parts of Eqn. (2) via careful modiﬁcations to the\nconditioning prompt q. Speciﬁcally, to investigate:\n1) Domain Understanding: we focus on p(m|q) by keeping\nthe PDDL domain speciﬁcation qm the same, but chang-\ning qn to directly ask about elements in the domain (e.g.,\nthe objects and predicates);\n2) Goal Inference: potentially, the model has correctly\ninferred p(z|m, q) but fails to generate the goal in PDDL.\nTo better distinguish these situations, we can ask the\nmodel to generate z in different languages (e.g., natural\nlanguage or Python). If the model is able to output a valid\ngoal in a different syntax, it suggests that the model has\ncorrectly inferred z;\n3) PDDL Goal Speciﬁcation: likewise, we can examine\nif the model is unable to correctly output the goal\nspeciﬁcation in PDDL p(g|z, m, q) if the goal inference\nis correct (the model is able to generate z in a different\nlanguage) but the PDDL output is wrong.\nNote that care should be taken in interpreting the outcomes\nof such prompt modiﬁcations; we may inadvertently change\ndifferent factors simultaneously. In addition, we are always\nreliant on the syntax output distribution and thus, cannot\ncompletely isolate the individual subtasks. Nevertheless, we\nassume that by restricting our changes to only certain parts of\nthe prompt, we are limiting the changes to the distributions.\nThen, by correlating the results of the above tests to the nature\nof the translation task, we can better understand the failure\npoints in the model. We will further discuss the application of\nthis analysis in Section V-B.\nIV. EXPERIMENTAL SETUP\nThis section describes our experimental setup, including the\ndomains, tasks, and evaluation methodology.\nA. Domains\nBlocksworld is a classical planning domain, widely used\nfor education and research purposes due to its simplicity in\nspeciﬁcation and complexity in optimal planning. Blocksworld\nproblems describe vertical spatial relations between objects on\na table. A goal state speciﬁes objects in a certain order. In our\nsetup, the objects are all colored blocks, similar to Valmeekam\net al. [34]. For most tasks, every block has a unique color,\ne.g., red_block. However, there are certain tasks where\nmultiple blocks have the same color and are distinguished\nby an additional index sufﬁx in the problem deﬁnition, e.g.,\nred_block_1.\nALFRED-L is based on Action Learning From Realistic\nEnvironments and Directives (ALFRED) [28], a household do-\nmain for learning to map instructions in natural language and\negocentric vision to action sequences. We chose ALFRED as it\nis consistent with our goal of translating from natural language\nand is representative of possible real-world applications. In\nour setup, we use three basic classes of objects in ALFRED,\nnamely, object, receptacle, and agent. An object is\na base class for a movable item. A receptacle may contain\nobjects, and an agent is able to execute actions on objects.\nA receptacle_object is a subclass of receptacle\nand object, and denotes a movable object that can contain\nanother object. We reduced the original domain and problem\nspeciﬁcation so that it would ﬁt within the LLM’s query\nwindow length by removing unused predicates and combining\ninReceptacle and inReceptacleObject. With these\nclasses, we construct different scenes with randomly initialised\nobjects. For each scene, we also initialise one agent to allow\nthe goal state to contain predicates involving an agent. Our\nscenes include:\n1) Kitchen: containing foods (e.g., Apple, Potato),\nutensils (e.g., Knife, Fork), and receptacles (e.g.,\nSinkBasin and Fridge).\n2) Living\nroom: containing items (e.g., FloorLamp,\nLaptop and RemoteControl) and receptacles (e.g.,\nSofa and Drawer).\n3) Bedroom: containing items like AlarmClock, Book,\nand CD, and receptacles like Drawer and Bed.\nB. Task Design\nA typical conﬁguration in a Blocksworld problem consists\nof one or more stacks, where each stack is a sequence of\nblocks placed one on top of the other. In ALFRED-L, the tasks\ncomprise interacting with objects (e.g., moving, slicing) to\narrive at a goal conﬁguration (e.g., a sandwich). By describing\nthe desired conﬁguration with different levels of speciﬁcity or\nconstraints, we can derive problems tasks that require one or\nmore of the following skills:\n1) Linguistic competence: Able to parse the text correctly.\nThe model has to recognize entities that are referred to\nin the text and understand their relations that are encoded\nwith verbs and prepositions (and predicates in PDDL).\n2) Object association: Able to associate objects in PDDL\nproblems with entities in the natural language text. In\ncases where blocks may have the same color, the model\nneeds to ground an ambiguous reference like “a red\nblock” to a concrete PDDL symbol like red_block_1.\nLikewise, in ALFRED-L, the model would like to ground\nan item like “book” to Book1.\n3) Numerical reasoning: Able to count objects and perform\nsimple arithmetic reasoning. For example, the model\nneeds to build a stack with a certain amount of blocks,\nor distribute blocks evenly in every stack.\n4) Physical reasoning: Able to understand physical law and\nrelationships between objects. For example, the model\nneeds to infer the spatial relations from a sequence\nrepresentation of blocks in a stack. In ALFRED-L, the\nmodel has to infer whether one object contains another.\n5) World Knowledge: Able to infer valid conﬁgurations\nfrom instructions that involve semantically rich object\nproperties, especially those that are not explicitly speci-\nﬁed in the domain. In ALFRED-L, world knowledge in-\ncludes commonsense relationships between objects (e.g.,\nan apple is a fruit; ice cream is usually kept in the fridge)\nTABLE I\nTASK DESCRIPTIONS. LEGEND: LC (LINGUISTIC COMPETENCE), OA (OBJECT ASSOCIATION), NR (NUMERICAL REASONING), PR (PHYSICAL\nREASONING, INCLUDING SPATIAL), WK (WORLD KNOWLEDGE)\nPrimary Skill(s) Tested\nDomain\nTask Name\nTask Description\n# Instances\nLC\nOA\nNR\nPR\nWK\nExample\nBlocksWorld\nExplicitStacks\nStack\nconﬁgurations\nspeciﬁed with no object\nambiguity\n300\n✓\nBuild two stacks. In the ﬁrst stack,\nthe black block is on the table, the\ngreen block is on top of the black\nblock, the violet block is on top of the\ngreen block, and there is nothing on\nthe violet block. In the second stack,\n...\nExplicitStacks-II\nStack\nconﬁgurations\nspeciﬁed with no object\nambiguity\n(alternative\nordering speciﬁcation)\n600\n✓\n✓\nBuild four stacks. In the ﬁrst stack,\nthere are the gold block, and the\nred block from bottom to top. In the\nsecond stack, ...\nBlockAmbiguity\nStack\nconﬁgurations\nspeciﬁed\nwith\nobject\nambiguity\n(multiple\nblocks\nwith\nthe\nsame\ncolor)\n200\n✓\n✓\nCreate two stacks. In the ﬁrst stack,\na red block is on top of a red block,\nand the yellow block is on top of a\nred block....\nNBlocks\nOne stack with N blocks\n300\n✓\n✓\n✓\nCreate a stack that contains two\nblocks.\nKStacks\nK stacks with the same\nnumber of blocks\n300\n✓\n✓\n✓\nUsing all blocks speciﬁed in the\nproblem, create exactly two stacks\nthat are of the same height.\nPrimeStack\nOne stack with a prime\nnumber of blocks\n200\n✓\n✓\n✓\n✓\nMake a stack with a prime number\nof blocks.\nKStacksColor\nK\nstacks\nwhere\neach\nstack\ncomprises blocks\nwith the same color\n200\n✓\n✓\n✓\n✓\nUsing all blocks speciﬁed in the\nproblem, build K stacks where each\nstack comprises blocks with the same\ncolor.\nALFRED-L\nExplicitInstruct\nFully-speciﬁed\ninstruction\n100\n✓\nPut a sliced apple on the plate.\nMoveSynonym\nMove an object to a re-\nceptacle replaced with its\nsynonym.\n100\n✓\n✓\nPut Pencil1 on a couch. (Context:\nName Sofa1 is used in the objects\ndeclaration.)\nMoveNextTo\nMove an object next to\nanother object.\n100\n✓\n✓\n✓\nPut Book5 next to Book4. Do not\nmove Book4.\nMoveToCount2\nMove\nan\nobject\nto\na\nreceptacle containing 2\nother objects.\n100\n✓\n✓\n✓\n✓\nMove KeyChain1 to the box with two\nbooks\nMoveToCount3\nMove\nan\nobject\nto\na\nreceptacle containing 3\nother objects.\n100\n✓\n✓\n✓\n✓\nMove KeyChain1 to the box with\nthree books\nMoveToMore\nMove an object to the\nreceptacle with more ob-\njects.\n100\n✓\n✓\n✓\n✓\nMove KeyChain1 to the box with\nmore books\nMoveNested\nMove\nan\nobject\nto\na\n”nested” object\n100\n✓\n✓\n✓\n✓\nPut KeyChain1 on the sofa with\nBook1. Do not put it in box. (Ex-\nample Context: Book1 is in Box1,\nwhich is on Sofa1.)\nMoveNested2\nMove object to a two-\nlayer nested object\n100\n✓\n✓\n✓\n✓\nPut KeyChain1 on the sofa with\nBook2. Do not put it in box. (Ex-\nample Context: Book2 is in Box3,\nwhich is in Box4. Box4 is on Sofa2.)\nCutFruits\nCut some fruits and put\nthem on the plate.\n20\n✓\n✓\n✓\nCut some fruits and put them on the\nplate.\nPrepareMeal\nPrepare a meal.\n20\n✓\n✓\n✓\n✓\n✓\nPrepare a meal.\nIceCream\nPut the ice-cream where\nit belongs.\n20\n✓\n✓\n✓\nPut the ice-cream where it belongs.\nSetTable2\nSet the table for two per-\nsons.\n20\n✓\n✓\n✓\n✓\n✓\nSet the table for two persons.\nCleanKitchen\nClean up the kitchen.\n20\n✓\n✓\n✓\n✓\nClean up the kitchen.\nThe tasks we crafted are shown in Table IV-B, along\nwith the skills that the tasks were designed to test. The\nnumber of instances for each task varied but in general,\nfor Blocksworld tasks, we tested problems with 4,8 and 12\nblocks (100 instances with differing conﬁgurations each). The\nExplicitStacks-II task has a large number of tasks as we also\nvaried the ordering speciﬁcation (“bottom to top” and “top\nto bottom”). The ALFRED-L tasks comprised 100 instances\nfor each task (with different goal objects and initial states),\nexcept the tasks with the broadest instructions (i.e., CutFruits,\nPrepareMeal, IceCream, SetTable2, CleanKitchen). For almost\nall the tasks, we used one-shot prompting (n = 1) to limit\nthe prompt length. We also tested zero-shot prompts but the\nperformance was poorer in those cases (see Supplementary\nMaterial).\nC. LLM Models\nWe used variants of GPT-3.5 for our experiments, specif-\nically code-davinci-002 for all Blocksworld tasks and\ntext-davinci-003 for ALFRED-L. These two models\nwere chosen because they achieved the best performance in\ninitial translation experiments.\nD. Evaluation Methods\nTranslation Success Rate. As stated in Sec. III, we use\nsuccess rates as a metric for evaluating the translation per-\nformance of the LLM. For both domains, we had strict and\nloose criteria for success, similar to prior work [29]. The\nstrict criteria required that the LLM produce a goal that\nwas acceptable to a planner and did not needlessly change\npredicates. In Blocksworld, a goal stack was correct under the\nstrict criteria if (i) the block order was correct, (ii) the bottom-\nmost block is stated as being on table and (iii) the top-most\nblock is stated as clear (having nothing on top of it). The loose\nmetric captures instances where the planner did not satisfy\neither (ii) or (iii), i.e., the ordering was correct but it failed\nto consistently apply the clear and ontable predicates.\nFor ALFRED-L tasks, we manually designed rules to check\nwhether the generated goal satisﬁes each task instruction, e.g.,\nfor SetTable2, that two sets of tableware were placed on the\ntable. The loose criteria in this case ensure that corresponding\npredicates must be true and the strict criteria additionally\ncheck that no unnecessary predicates are speciﬁed (e.g., for\nSetTable2, that no items are in the sink).\nFor Blocksworld, we designed additional checks that indi-\ncate what was wrong with the goal stack:\n• A Domain/Syntax error check evaluates if the goal\ncomplies with the PDDL context. The parser reports an\nerror if the goal has a wrong syntax or includes an object\nthat doesn’t exist in the world.\n• A Physical error check tests if the predicates form\na stack that is physically possible to construct (if the\ndomain/syntax check is passed). Typical errors include\nhaving more than one block on top of the same block,\nor that a block sits above and below another block at the\nsame time.\nSubtask Scores. In addition to the metrics above, we con-\nducted tests on the LLMs ability to perform domain un-\nderstanding and goal inference. In Blocksworld, our domain\nunderstanding test comprised ﬁve basic queries:\n• Object extraction: “List all the objects that appear in\nthe PDDL problem deﬁnition”.\n• Color-based Object extraction: the query is similar to\nabove except we add a constraint “that has the color\n[color]”.\n• On Predicate: “Determine whether the [object A] is on\nthe [object B] in the initial state. Answer with yes or no”.\n• Clear Predicate: “Determine whether the [object] is on\nthe table in the initial state. Answer with yes or no.”\n• OnTable Predicate: “Determine whether there is nothing\non the top of the [object] in the initial state. Answer with\nyes or no”.\nThe score is an average of how many of the above questions\nthe LLM answered correctly. To test goal inference ability, we\ntasked the LLM to generate the stacks as Python lists. This was\ninspired by the observation in preliminary trials that on some\ntasks, the LLM would successfully output Python lists even\nthough the PDDL goal was incorrect. We acknowledge this is\nan imperfect test but is one of the few available to us given the\nconstraints of working with black-box LLMs. In ALFRED-\nL, the domain and goal inference questions were tailored\nfor each task (see Supplementary Material). For example, for\nMoveToCount2, the relevant queries were:\n• Domain understanding: “Which box has two [target\ntypes] in the initial state?”\n• Goal Inference: “Move [moved object] to the box with\ntwo [target types]. Which object should we move the\n[moved object] into?”\nV. RESULTS AND DISCUSSION\nIn the following, we summarize our main ﬁndings, starting\nwith an analysis of the translation success rates on the different\ntasks, followed by a discussion of the LLM’s performance\non the subtasks described in Sec. III. In general, the LLM\nhad excellent performance on tasks that has unambiguously\nspeciﬁed goals, but the performance was mixed on tasks\nwith goals that were only partially-speciﬁed 1. Nevertheless,\nperformance was well above chance and in all of the tasks,\npreliminary tests showed that LLM failed to plan (planning\nsuccess rates were 0% for most tasks, similar to ﬁndings in\nprior work [34]).\nA. Success Rate Analysis\nThe success rates in Table II show that the LLM can\ndirectly translate explicitly and unambiguously speciﬁed\ngoals in natural language to PDDL, with some caveats\ndiscussed below. In ExplicitStacks and ExplicitInstruct —\nwhere the natural language instructions precisely state the\n1We also tested a baseline translator similar to Steinert et al. [32] which\nuses traditional NLP techniques, but this method was far poorer than the LLM\nin initial experiments.\nTABLE II\nSUCCESS RATES ON THE GOAL TRANSLATION TASKS\nSuccess Rate (%)\nDomain\nTask Name\nLoose\nStrict\nBlocksWorld\nExplicitStacks\n99.67\n98.67\nExplicitStacks-II\n58.00\n52.17\nBlockAmbiguity\n18.00\n14.50\nNBlocks\n62.00\n57.33\nKStacks\n55.00\n55.00\nPrimeStack\n87.00\n87.00\nKStacksColor\n19.00\n19.00\nAlfred\nExplicitInstruct\n100.00\n100.00\nMoveSynonym\n100.00\n100.00\nMoveNextTo\n99.00\n99.00\nMoveToCount2\n96.00\n96.00\nMoveToCount3\n74.00\n74.00\nMoveToMore\n86.00\n86.00\nMoveNested\n59.00\n59.00\nMoveNested2\n53.00\n53.00\nCutFruits\n100.00\n100.00\nPrepareMeal\n90.00\n60.00\nIceCream\n80.00\n75.00\nSetTable2\n100.00\n100.00\nCleanKitchen\n90.00\n35.00\nTABLE III\nSUCCESS RATES DEPENDING ON THE SIZE OF THE PROMPT EXAMPLE\nOne-Shot Example\nTarget Goal\n4 blocks\n8 blocks\n12 blocks\n4 blocks\n67%\n75%\n100%\n8 blocks\n52%\n77%\n100%\n12 blocks\n32%\n80%\n99%\ndesired conﬁguration —- goal correctness was close to 100%.\nFor ExplicitStacks, goal translation performance was sustained\neven when the number of blocks in the domain and desired\nnumber of stacks were varied, suggesting that the LLM is able\nto generalize across initial domain conﬁgurations. Likewise,\nfor ExplicitInstruct in the ALFRED-L domain, the LLM\nachieved perfect goal translation across multiple objects in the\ndifferent scenes.\nHowever, our results also show that LLM is sensitive to the\nnatural language goal prompt. For example, ExplicitStacks-\nII also unambiguously speciﬁes the stack conﬁgurations, e.g.,\n“... In the ﬁrst stack, there are the magenta block, the red block,\nand the yellow block, from bottom to top. In the second stack,\n...” with the difference being that the blocks are stated from\n“bottom to top” or “top to bottom”. However, performance\nfor this task was only ≈50%. Further analysis showed that\nthe model ordered the blocks incorrectly when the natural\nlanguage instruction was stated from “top to bottom”, indi-\ncating an ordering bias in the PDDL generation. A different,\nbut related, phenomenon was observed on the MoveNextTo\ntask in the ALFRED-L domain. Here, the LLMs were largely\ncapable of repositioning one object adjacent to another but\nif the constraint “do not move [Object B]” was dropped, the\nLLMs speciﬁed a goal that relocates both objects to the one\nlocation. Although this is technically correct, it is unexpected\nand may violate the intentions of a user. The above results\nindicate that the LLM was able to linguistically reason about\nthe goal statement (together with the speciﬁed PDDL domain),\nbut also illustrate biases in the translation process.\nThe LLM’s performance was mixed on the tasks that involve\npartial speciﬁcation of the goal. As previously mentioned,\nthese tasks require either inference or commonsense knowl-\nedge to solve. In general, we found that the LLM was able\nto apply commonsense reasoning to “ﬁll the gaps” in the\nnatural language goal speciﬁcation and demonstrated a\ndegree of elementary numerical/physical reasoning. This is\nmost apparent in the ALFRED-L tasks in Tbl. II, the LLM was\nable to identify object types (e.g., fruits), and synonyms (e.g.,\ncouch and sofa, timepiece and watch), apply elementary spatial\nreasoning (next to) and was aware of common relationships\n(e.g., that ice cream is generally kept in a fridge). It was\nalso able to translate the ambiguous goals statements in the\nSetTable2 and CleanKitchen tasks to a good degree.\nHowever, the LLM displayed poorer competence on tasks\nthat involve nontrivial numerical reasoning and physical\nworld inference. Our results support prior ﬁndings [34, 8, 21]\nthat LLMs have difﬁculty with reasoning-related tasks. We\nfound the ALFRED-L tasks that involve counting objects\n(MoveCount2, MoveCount3) and spatial relationship reason-\ning (MoveNested, MoveNested2) were more challenging for\nthe LLM. Errors increased as the task became more difﬁcult\n(from two to three objects in the receptacle and from one-level\nto two-level nesting). Similarly, in Blocksworld, performance\nfor NBlocks and KStacks, which involved numerical reason-\ning, was ≈50 −60%. Here, the LLM tended to blindly copy\nthe prompt example (e.g., if the prompt example included a\ngoal with 2 stacks, the LLM would output a PDDL goal for\n2 stacks even though the natural language goal speciﬁed 4\nstacks). In general, we found the LLM required a sufﬁciently\ngood example to generalize well. Table III shows that the\nnumber of blocks used on the one-shot example signiﬁcantly\naffects the performance on target tasks and larger examples\ngenerally led to better performance.\nSomewhat surprisingly, the lowest scores in Blocksworld\nare associated with BlockAmbiguity and KStacksColor; these\ntwo problems require the LLM to associate objects based\non their color and we had apriori expected the LLM to be\ncapable of such associations and perform well on this task.\nIn BlockAmbiguity, in most cases, the blocks speciﬁed in the\ngoal were correct but with an incorrect ordering, with errors\noccurring near the ontable predicate. For KStacksColor, the\nLLM tended to generate goal stacks that were physically valid,\nbut either (i) one of the stacks would not be of a single color, or\n(ii) built the wrong number of stacks. Likewise, performance\non the MoveSynonym task was subpar.\nB. Subtask Analysis\nOur subtask analysis results are summarized in Tbl. IV.\nHere, we conducted separate tests for domain understanding\nand goal inference, and compare the scores for each subtask\nwhen the overall translation task was successful versus when\nit failed. By examining the change in the performance on\neach subtask, we aim to piece together if errors were mainly\nTABLE IV\nPERFORMANCE ON SUBTASKS (HIGHER SCORES INDICATE BETTER PERFORMANCE)\nBaseline Score\nChange in Score\n(Successes Only)\n(Failures Only)\nDomain\nTask Name\nDomain Und.\nGoal Inference\nDomain Und.\nGoal Inference\nBlocksWorld\nExplicitStacks\n80.00\n100.00\n0.00\n0.00\nExplicitStacks-II\n73.33\n95.53\n1.48\n-13.30\nBlockAmbiguity\n68.89\n29.63\n-5.11\n-28.47\nNBlocks\n67.69\n96.51\n-4.07\n-2.76\nKStacks\n63.64\n95.15\n1.92\n-81.82\nPrimeStack\n56.92\n76.44\n2.05\n-22.59\nKStacksColor\n80.00\n11.86\n-23.14\n13.68\nAlfred\nExplicitInstruct\n100.00\n100.00\n-\n-\nMoveSynonym\n90.00\n96.00\n-\n-\nMoveNextTo\n100.00\n100.00\n0.00\n-100.00\nMoveToCount2\n95.83\n89.58\n4.17\n-89.58\nMoveToCount3\n90.54\n83.78\n-36.69\n-72.24\nMoveToMore\n97.67\n93.02\n-4.81\n-64.45\nMoveNested\n98.31\n89.83\n-8.07\n-72.76\nMoveNested2\n84.91\n86.79\n-38.10\n-67.64\nCutFruits\n85.00\n85.00\n-\n-\nPrepareMeal\n100.00\n83.33\n0.00\n-33.33\nIceCream\n100.00\n93.75\n0.00\n-18.75\nSetTable2\n100.00\n100.00\n-\n-\nCleanKitchen\n94.44\n94.44\n-44.44\n5.56\ncaused by the (i) model failing to properly parse/understand\nthe domain, (ii) errors in goal inference, and/or (iii) inability to\nproperly generate PDDL. Performance on either/both domain\nunderstanding and goal inference subtasks generally fell when\ntranslation failed, which supports our structured analysis and\nindicates that our tests are reasonable indications of the LLM’s\nunderlying capabilities.\nWe noted that the Blocksworld domain understanding scores\nwere relatively low compared to ALFRED-L, even when the\ntranslation was successful. Further analysis showed that the\nLLM was able to perform object extraction from the PDDL\n(with lower performance when extracting based on color), but\nfailed to answer predicate-related queries; the success rate\non these queries was ≈40-60%. This suggests that the\nLLM did not understand the physical world semantics of\nthe predicates. Instead, it leveraged on its linguistic abilities\nto construct goal stacks using the prompt example. This\npotentially explains (i) why the example prompt was key to\nperformance in the Blocksworld domain and (ii) the nature of\nthe errors in the BlockAmbiguity and KStacksColor domains2.\nFor ALFRED-L, for certain tasks (MoveToCount2, Move-\nToMore, MoveToNested), the model still retained similar\ndomain understanding scores in the failure condition, but\nachieved poorer goal inference scores. This suggests the LLM\nwas able to parse the PDDL ﬁle to obtain relevant informa-\ntion, but was unable to integrate the information to arrive\nat a suitable goal. For example, on the MoveToMore task,\nthe LLM was able to infer which box had more items (e.g.,\nbooks), but failed to make the seemingly simple inference\n2Interestingly, for these two problems, we noted that Python list generation\nwas substantially poorer than PDDL generation. For KStacksColor, this was\npartially due to overﬁtting on the example prompt.\nto place the target item in that box. On MoveToCount3 and\nMoveToNested2, the model’s domain understanding score fell;\nit was no longer able to consistently identify which item\ncontained 3 objects or was located within another object.\nAs expected, goal inference scores also fell. Our analysis\nindicates that the LLM’s ability to fundamentally reason about\nthe domain is limited (in this case, numerical and hierarchical\nreasoning), leading to errors in translation.\nC. Discussion: Other Findings\nThe above results show that LLMs have signiﬁcant linguistic\ncapabilities and as such, are sometimes able to mask their\nlimitations (e.g., when translating in the BlocksWorld domain).\nWe see that the overall performance of LLM is impressive on\ncommon tasks on ALFRED, presumably due to the associa-\ntions learnt from the huge amount of text data it has consumed.\nWe also noted that the LLM tended to presume that the\ngiven instructions were valid. For example, when asked to\n“Put two oranges on the table” and no oranges present in\nthe PDDL speciﬁcation, the LLMs would simply create two\norange instances (e.g., Orange_1 and Orange_2) along\nwith the goal speciﬁcation. When asked to “Put a sliced plate\non the table”, the model added a predicate that the plate\nshould be sliced, again suggesting linguistic competence but a\nlack of reasoning. Interestingly, when asked directly whether\na plate can be sliced, the LLMs stated “No”, which coincides\nwith our common-sense knowledge. The LLM appears to be\nable to infer the validity of some instructions, but they do\nnot perform this validation actively. When we added “ask a\nquestion if it is not achievable.”, the LLM would respond\ncorrectly that oranges were not present but not that the plate\nwas not sliceable. In short, responses were inconsistent and\nvaried depending on the objects in question.\nVI. CONCLUSION\nSummary. In this paper, we presented an empirical study into\nthe effectiveness of LLMs, speciﬁcally GPT-3.5 variants, for\nthe task of natural language goal translation to PDDL. Our key\nﬁnding was that LLMs have potent linguistic processing ability\n— formal linguistic competence [21] — and can leverage\ncommonsense knowledge (e.g., associations between objects)\nto achieve impressive translation performance, especially on\nthe common household tasks in ALFRED-L. However, we\nalso ﬁnd that the models lack fundamental formal and physical\nworld reasoning capabilities that are necessary for translation.\nThis can be problematic for robotics tasks which involve\ntasks that are less common (e.g., in specialized factories or\nsettings involving assistive care) and not well-represented in\nthe training corpus. Indeed, our work suggests that there is\nroom to improve the LLMs, potentially to integrate reasoning\neven at the translation stage.\nLimitations and Future Work. Our study can be improved\nin several ways. First, previous work has noted that LLMs\ncan be sensitive to the prompts used. We also found this to be\ntrue for goal translation. An exhaustive experiment involving\nall possible prompts was intractable, so we relied on previous\nﬁndings [8, 29] and initial trials to design reasonable prompts.\nHowever, it is possible that other prompt structures may give\nbetter performance. Next, our subtask analysis was necessarily\nlimited given only API access to the models. The subtask tests\ncannot unambiguously conﬁrm if an LLM succeeds on a given\nsubtask as performance is always contingent upon the LLM\nbeing able to output a text answer. We are unable to always\npinpoint errors in the transcription of latent information to\ntext and future work can examine latent encodings to derive\nstronger conclusions. Nevertheless, we hope our results and\nanalysis shed light on the potential of applying LLMs to goal\ntranslation.\nACKNOWLEDGEMENTS\nThis research is supported by the National Research Foun-\ndation, Singapore under its Medium Sized Center for Ad-\nvanced Robotics Technology Innovation.\nREFERENCES\n[1] Planning.wiki - the ai planning & pddl wiki. URL https:\n//planning.wiki/.\n[2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alexander\nHerzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian\nIchter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano,\nKyle Jeffrey, Sally Jesmonth, Nikhil J. Joshi, Ryan Julian,\nDmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada,\nPeter Pastor, Jornell Quiambao, Kanishka Rao, Jarek\nRettinghouse, Diego Reyes, Pierre Sermanet, Nicolas\nSievers, Clayton Tan, Alexander Toshev, Vincent Van-\nhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and\nMengyuan Yan. Do as I can, not as I say: Grounding\nlanguage in robotic affordances. CoRR, abs/2204.01691,\n2022.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al. Language models are few-shot learners. Advances\nin neural information processing systems, 33:1877–1901,\n2020.\n[4] Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao,\nKeerthana Gopalakrishnan, Michael S. Ryoo, Austin\nStone, and Daniel Kappler. Open-vocabulary queryable\nscene representations for real world planning. In arXiv\npreprint arXiv:2209.09874, 2022.\n[5] Kaiqi Chen, Jeffrey Fong, and Harold Soh.\nMirror:\nDifferentiable deep social projection for assistive human-\nrobot communication. In Proceedings of Robotics: Sci-\nence and Systems, June 2022.\n[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman,\net al. Evaluating large language models trained on code.\narXiv preprint arXiv:2107.03374, 2021.\n[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022.\n[8] Katherine M. Collins, Catherine Wong, Jiahai Feng,\nMegan Wei, and Joshua B. Tenenbaum. Structured, ﬂex-\nible, and robust: benchmarking and improving large lan-\nguage models towards more human-like behavior in out-\nof-distribution reasoning tasks. CoRR, abs/2205.05718,\n2022.\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding.\narXiv\npreprint arXiv:1810.04805, 2018.\n[10] Wenfeng Feng, Hankz Hankui Zhuo, and Subbarao\nKambhampati.\nExtracting action sequences from texts\nbased on deep reinforcement learning.\narXiv preprint\narXiv:1803.02632, 2018.\n[11] Maria Fox and Derek Long. Pddl2. 1: An extension to\npddl for expressing temporal planning domains. Journal\nof artiﬁcial intelligence research, 20:61–124, 2003.\n[12] Francesco Fuggitti and Tathagata Chakraborti.\nNl2ltl–\na python package for converting natural language (nl)\ninstructions to linear temporal logic (ltl) formulas.\nIn\nAAAI Conference on Artiﬁcial Intelligence, 2023.\n[13] Hector Geffner and Blai Bonet. A concise introduction\nto models and methods for automated planning. Morgan\n& Claypool Publishers, 2013.\n[14] Nakul Gopalan, Dilip Arumugam, Lawson LS Wong, and\nStefanie Tellex. Sequence-to-sequence language ground-\ning of non-markovian task speciﬁcations. In Robotics:\nScience and Systems, volume 2018, 2018.\n[15] Malte Helmert.\nThe fast downward planning system.\nJournal of Artiﬁcial Intelligence Research, 26:191–246,\n2006.\n[16] Jie Huang and Kevin Chen-Chuan Chang.\nTowards\nreasoning in large language models: A survey.\narXiv\npreprint arXiv:2212.10403, 2022.\n[17] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. Language models as zero-shot planners:\nExtracting actionable knowledge for embodied agents.\narXiv preprint arXiv:2201.07207, 2022.\n[18] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tomp-\nson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet,\nTomas Jackson, Noah Brown, Linda Luu, Sergey Levine,\nKarol Hausman, and brian ichter.\nInner monologue:\nEmbodied reasoning through planning with language\nmodels. In 6th Annual Conference on Robot Learning,\n2022.\n[19] Shuang Li, Xavier Puig, Yilun Du, Clinton Wang, Ekin\nAkyurek, Antonio Torralba, Jacob Andreas, and Igor\nMordatch.\nPre-trained language models for interac-\ntive decision-making. arXiv preprint arXiv:2202.01771,\n2022.\n[20] Jason Xinyu Liu, Ziyi Yang, Benjamin Schornstein,\nSam Liang, Ifrah Idrees, Stefanie Tellex, and Ankit\nShah. Lang2ltl: Translating natural language commands\nto temporal speciﬁcation with large language models. In\nWorkshop on Language and Robotics at CoRL 2022.\n[21] Kyle Mahowald, Anna A. Ivanova, Idan A. Blank,\nNancy Kanwisher, Joshua B. Tenenbaum, and Evelina\nFedorenko.\nDissociating language and thought in\nlarge language models: a cognitive perspective. CoRR,\nabs/2301.06627, 2023.\n[22] Shivam Miglani and Neil Yorke-Smith. Nltopddl: One-\nshot learning of pddl models from natural language\nprocess manuals. In ICAPS’20 Workshop on Knowledge\nEngineering for Planning and Scheduling (KEPS’20).\nICAPS, 2020.\n[23] Alberto Olmo, Sarath Sreedharan, and Subbarao Kamb-\nhampati. Gpt3-to-plan: Extracting plans from text using\ngpt-3. arXiv preprint arXiv:2106.07131, 2021.\n[24] Vishal Pallagani, Bharath Muppasani, Keerthiram Mu-\nrugesan, Francesca Rossi, Lior Horesh, Biplav Srivas-\ntava, Francesco Fabiano, and Andrea Loreggia. Plans-\nformer: Generating symbolic plans using transformers.\nCoRR, abs/2212.08681, 2022.\n[25] Roma Patel, Ellie Pavlick, and Stefanie Tellex. Ground-\ning language to non-markovian tasks with no supervision\nof task speciﬁcations. In Robotics: Science and Systems,\n2020.\n[26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learn-\ning with a uniﬁed text-to-text transformer. The Journal\nof Machine Learning Research, 21(1):5485–5551, 2020.\n[27] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah\nIdrees, David Paulius, and Stefanie Tellex. Planning with\nlarge language models via corrective re-prompting. In\nNeurIPS 2022 Foundation Models for Decision Making\nWorkshop, 2022. URL https://openreview.net/forum?id=\ncMDMRBe1TKs.\n[28] Mohit\nShridhar,\nJesse\nThomason,\nDaniel\nGordon,\nYonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke\nZettlemoyer, and Dieter Fox.\nALFRED: A Bench-\nmark for Interpreting Grounded Instructions for Everyday\nTasks. In The IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2020.\n[29] Tom Silver, Varun Hariprasad, Reece S Shuttleworth,\nNishanth Kumar, Tom´as Lozano-P´erez, and Leslie Pack\nKaelbling.\nPDDL planning with pretrained large lan-\nguage models. In NeurIPS 2022 Foundation Models for\nDecision Making Workshop, 2022.\n[30] Nisha Simon and Christian Muise. A natural language\nmodel for generating pddl.\n[31] John Slaney and Sylvie Thi´ebaux. Blocks world revis-\nited. Artiﬁcial Intelligence, 125(1):119–153, 2001.\n[32] Maur´ıcio Steinert and Felipe Rech Meneguzzi. Planning\ndomain generation from natural language step-by-step in-\nstructions. In 2020 Workshop on Knowledge Engineering\nfor Planning and Scheduling (KEPS@ ICAPS), 2020,\nFranc¸a., 2020.\n[33] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia\nJin, Taylor Bos, Leslie Baker, Yu Du, et al.\nLamda:\nLanguage models for dialog applications. arXiv preprint\narXiv:2201.08239, 2022.\n[34] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan,\nand Subbarao Kambhampati.\nLarge language models\nstill can’t plan (a benchmark for LLMs on planning and\nreasoning about change). In NeurIPS 2022 Foundation\nModels for Decision Making Workshop, 2022.\nAPPENDIX\nIn this section, we provide supplementary material regarding\nour benchmark problems, evaluation, and prompts for error\nanalysis. We also provide additional results (breakdown ac-\ncording to number of blocks for the Blocksworld domain) and\nzero-shot results on the ALFRED-L tasks. Code is available\nat https://github.com/clear-nus/gpt-pddl/.\nA. Blocksworld benchmark generation\nThis subsection details how test cases are generated for all\ntasks on the Blocksworld domain.\nProblem Generation Using the PDDL problem generator\nfrom [31], we ﬁrst generate PDDL problems with different\nnumbers of objects. There are three options of the number\nof objects: 4, 8, and 12. Each option has 100 corresponding\nproblems generated. Since these problems use alphabet letters\nas object symbols, they serve as templates for generating\nconcrete test problems suited for speciﬁc tasks. Additional\nproblems are generated as templates for n-shot examples.\nFor each task, the objects in the problem templates are\nﬁrst randomly replaced by the actual blocks. Then, a text\ninstruction is sampled for each problem. The original goal\ngenerated along with a problem template is removed and\ndoes not serve as the ground truth since it cannot satisfy our\ntask requirement most of the time, but it does contribute to\nthe instruction sampling in tasks like ExplicitStacks so as to\nsimplify the pipeline.\nInstruction Generation The instructions are constructed with\nnatural language templates. We brieﬂy describe how instruc-\ntions are sampled in each task below.\n(a) ExplicitStacks We ﬁrst construct a complete block con-\nﬁguration that satisﬁes the original goal in problem tem-\nplate, then describe the conﬁguration stack by stack. For\neach stack, we ﬁrst describe it precisely with predicates,\nthen use language templates to translate this formula form\ninto text. The colors don’t repeat.\n(b) ExplicitStacks-II The sampling process is similar to\nExplicitStacks, except that each stack is speciﬁed with a\nsequence of blocks and an indication of the direction, that\nis, whether the blocks are speciﬁed from bottom to top or\nfrom top to bottom. Note that we generate one benchmark\nfor each direction, therefore this task has twice as many\ntest cases as most of the other tasks. The colors don’t\nrepeat.\n(c) BlockAmbiguity The sampling process is similar to Ex-\nplicitStacks, but for each stack the ontable and clear\npredicates are not translated, and colors are repeatable.\n(d) NBlocks We ﬁrst construct a complete block conﬁgura-\ntion that satisﬁes the original goal in problem template,\nthen uniformly choose one stack of blocks out of this\nconﬁguration, and use the number of blocks in this stack\nas the parameter N in the instruction. The colors don’t\nrepeat.\n(e) KStacks The parameter K is sampled uniformly from\nnon-trivial divisors of the total number of objects deﬁned\nin problem. The colors don’t repeat.\n(f) PrimeStack The instruction always asks to build a stack\nof blocks with a prime number of blocks. The colors don’t\nrepeat.\n(g) KStacksColor The parameter K is sampled uniformly\nfrom a predeﬁned range, ensuring that the solution exists.\nThe colors can repeat.\nWe also introduce variations on the verb to make the\ninstructions more diverse. For all phrases like “create a stack”,\nthe verb “create” is replaced by a synonym like “make”.\nIt’s worth mentioning that since the domain of Blocksworld\nis short enough, it is treated as part of the query in every\nexample rather than a preﬁx of the whole prompt. This is not\nthe case in ALFRED-L, since repeating its domain deﬁnition\nwould easily exceed the token limit. All tasks generate bench-\nmarks from problems of either all 3 object number options\n(4, 8, 12) or 2 of them (8, 12). There are 100 test cases per\nobject number option per task. See Tbl. V for the detailed\nconﬁguration.\nB. Blocksworld Evaluation\nAll Blocksworld tasks have a common evaluation pipeline\nthat involves the following four stages:\n1) Problem\nparsing ﬁlls the generated goal into the\nprompted incomplete PDDL problem, and use a PDDL\ndedicated parser to parse the problem text along with\nthe Blocksworld domain ﬁle. Then, the goal formula is\nextracted from the output of parser. This stage performs\nthe domain/syntax error check. If the goal is syntactically\nincorrect, or an unseen predicate or object is used in the\ngoal, or an predicate has erroneous arguments, the parser\nwill report a domain/syntax error.\n2) Graph construction converts the goal formula into a\ndirected graph. With all predicates in the goal specifying\nspatial relations between blocks, this stage attempts to\nbuild a directed graph that represents whether a block\nis on another block by existence of a directed edge be-\ntween the two corresponding nodes. table and air are\nintroduced as two special nodes for encoding ontable\nand clear predicates. The graph represents at least one\nphysically valid conﬁguration of blocks, which means\nthat no two edges are starting from or pointing to the\nsame node (except that the starting node is table or\nthe ending node is air), and there shall exist neither\ncycles nor loops. This stage performs the physical error\ncheck, and any error in the graph construction process is\nreported as a physical error.\n3) Stack construction converts the graph into a set of chains\nof blocks that are accepted as stacks. This stage is also\nwhere the loose metric and the strict metric differ. If\na chain of blocks in the graph satisﬁes the criteria of\nthe metric, it is accepted as a valid stack and considered\nin the last stage. Unaccepted blocks are simply ignored.\nThis stage does not need error checking. The difference\nbetween the two metrics is:\n• Loose metric: accepts a chain of blocks as a stack as\nlong as its bottom block has the ontable property\nOR its top block has the clear property. Thus if a\nblock is not mentioned, it is ignored in tje evaluation.\n• Strict metric: accepts a chain of blocks as a stack as\nlong as its bottom block has the ontable property\nAND its top block has the clear property.\n4) Constraint Validation evaluates the accepted stacks\nagainst the natural language instruction. Evaluation at\nthis stage varies depending on the tasks. If the stacks do\nnot satisfy the instruction, it is reported as a constraint\nviolation.\nC. ALFRED-L Evaluation\nIn the following, we provide a speciﬁcation of the losse and\nstrict criteria applied for each of the ALFRED-L tasks.\n(a) ExplicitInstruct\n• Loose: Predicted goal speciﬁcation includes all predi-\ncates in the ground truth goal speciﬁcation.\n• Strict: Predicted goal speciﬁcation matches the ground\ntruth goal speciﬁcation exactly.\n(b) MoveSynonym\n• Loose: If the synonym is a receptacle: whether the\nreceptacle receives the moved object. If the synonym\nis a movable object: whether the object is moved to\nthe correct position.\n• Strict: Predicted goal speciﬁcation has no agent inside.\n(c) MoveNextTo\n• Loose: Predicted goal speciﬁcation includes the two\nobjects at the same location.\n• Strict: Predicted goal speciﬁcation includes the two\nobjects at the same location. The target object is not\nmoved. There is no agent in the goal state.\n(d) MoveToCount2\n• Loose: Predicted goal speciﬁcation includes the object\nin the correct box.\n• Strict: Predicted goal speciﬁcation includes the object\nin the correct box. There is no agent in the goal state.\n(e) MoveToCount3\n• Loose: Predicted goal speciﬁcation includes the object\nin the correct box.\n• Strict: Predicted goal speciﬁcation includes the object\nin the correct box. There is no agent in the goal state.\n(f) MoveToMore\n• Loose: Predicted goal speciﬁcation includes the object\nin the correct box.\n• Strict: Predicted goal speciﬁcation includes the object\nin the correct box. There is no agent in the goal state.\n(g) MoveNested\n• Loose: Predicted goal speciﬁcation includes the object\nin the correct box.\n• Strict: Predicted goal speciﬁcation includes the object\nin the correct box. There is no agent in the goal state.\nThere are no redundant predicates in the goal state.\n(h) MoveNested2\n• Loose: Predicted goal speciﬁcation includes the object\nin the correct box.\n• Strict: Predicted goal speciﬁcation includes the object\nin the correct box. There is no agent in the goal state.\nThere is no redundant predicates in the goal state.\n(i) CutFruits\n• Loose: Some fruits are cut and put on the plate.\n• Strict: Only fruits are cut and put on the plate.\n(j) PrepareMeal\n• Loose: Some food is served on the table.\n• Strict: Only food is served on the table.\n(k) IceCream\n• Loose: Ice cream is put in the fridge.\n• Strict: Only ice cream is put in the fridge, with no\nchange to other predicates.\n(l) SetTable2\n• Loose: The table is set for two persons.\n• Strict: The table is set for two persons with no change\nto other predicates.\n(m) CleanKitchen\n• Loose: Nothing on the ﬂoor and no violation of the\ncan-contain rule.\n• Strict: Nothing in the sink. Nothing in the microwave\nor oven.\nD. BlocksWorld Error Analysis\nDomain Understanding. For the domain understanding anal-\nysis, the ﬁve queries are applied in the context of all test\ncases. The prompts are exactly the same as those used for goal\ntranslation, except that the instruction describing the goal state\nis replaced with one of the ﬁve queries (note that instructions\nand answers in the examples are kept unchanged). We brieﬂy\ndescribe the query-answer pair construction processes below:\n• Object Extraction: there is no randomness in the query.\nThe answer should be all objects listed in the problem\ndeﬁnition. The model is asked to split the output objects\nwith spaces.\n• Color-based Object Extraction: one color in query is\nuniformly sampled from all colors of blocks that appear\nin the problem. The answer should be those blocks with\nthis certain color. The model is asked to split the output\nobjects with spaces.\n• On Predicate: one fact using the on predicate is uni-\nformly sampled from all facts listed in the initial state.\nThen, with 50% chance, one of its arguments is replaced\nby another random block. The query asks about the\ncorrectness of the fact. The ground truth answer depends\non whether the argument replacement is executed.\n• OnTable Predicate: ﬁrst, we extract all blocks on the\ntable (modiﬁed by ontable predicate) in the initial\nstate. Then, the object in query is sampled from these\nTABLE V\nSUCCESS RATES AND ERROR REPORT ON THE BLOCKSWORLD GOAL TRANSLATION TASKS\nSuccess Rates (%)\nDetailed Error Rates (%)\nTask Name\nNumber of Objects\nLoose\nStrict\nDomain/Syntax Error\nPhysical Error\nConstraint Violation\nExplicitStacks\n4\n100.00\n100.00\n0.00\n0.00\n0.00\n8\n100.00\n100.00\n0.00\n0.00\n0.00\n12\n99.00\n96.00\n0.00\n1.00\n3.00\nAverage\n99.67\n98.67\nExplicitStacks-II (bottom to top)\n4\n55.00\n48.00\n0.00\n13.00\n39.00\n8\n98.00\n77.00\n0.00\n1.00\n22.00\n12\n99.00\n99.00\n0.00\n1.00\n0.00\nAverage\n84.00\n74.67\nExplicitStacks-II (top to bottom)\n4\n19.00\n17.00\n0.00\n17.00\n66.00\n8\n35.00\n30.00\n0.00\n3.00\n67.00\n12\n42.00\n42.00\n0.00\n0.00\n58.00\nAverage\n32.00\n29.67\nNBlocks\n4\n72.00\n62.00\n0.00\n2.00\n36.00\n8\n50.00\n46.00\n0.00\n5.00\n49.00\n12\n64.00\n64.00\n0.00\n0.00\n36.00\nAverage\n62.00\n57.33\nKStacks\n4\n94.00\n94.00\n0.00\n5.00\n1.00\n8\n51.00\n51.00\n1.00\n2.00\n46.00\n12\n20.00\n20.00\n0.00\n12.00\n68.00\nAverage\n55.00\n55.00\nPrimeStack\n8\n97.00\n97.00\n0.00\n0.00\n3.00\n12\n77.00\n77.00\n0.00\n0.00\n23.00\nAverage\n87.00\n87.00\nBlockAmbiguity\n8\n26.00\n23.00\n2.00\n17.00\n58.00\n12\n10.00\n6.00\n0.00\n25.00\n69.00\nAverage\n18.00\n14.50\nKStacksColor\n8\n7.00\n7.00\n1.00\n16.00\n76.00\n12\n31.00\n31.00\n0.00\n4.00\n65.00\nAverage\n19.00\n19.00\nblocks. With 50% chance, we sample the object in query\nfrom blocks that are not on the table instead.\n• Clear Predicate: Same as the process for ontable\npredicate, except that we look for the clear predicates\nin the initial state.\nSince running these ﬁve queries over all test cases would\nbe very costly, we sample 7 out of every 100 test cases for\nthis analysis, except for benchmarks where successes/failures\nare rare.\nGoal Inference. For the goal inference analysis, we use the\nsame domain deﬁnition, example problems and test problems\nas those in the goal translation task, except that all goal\nspeciﬁcations (both in the examples and in the test queries)\nare represented by hierarchical Python lists instead of PDDL\nformulas. We use list of lists of PDDL object symbols to\nspecify all the stacks in a satisfying conﬁguration. The instruc-\ntions are also rephrased accordingly. Despite the ambiguity in\ndirection of block sequences under this particular form, we\nexpect the model to discriminate the bias underneath when\nrequired (note that many tasks actually accept interpretations\nin both directions), provided that answers in the examples are\nalso replaced by corresponding lists.\nE. ALFRED-L Error Analysis\nALFRED-L errors were analyzed via direct questions re-\ngarding the domain and/or goal.\n(a) ExplicitInstruct\n• Domain understanding: Where is the {object}?\n• Goal inference: Write the goal speciﬁcation in natural\nlanguage.\n(b) MoveSynonym. There are two cases, as the synonym can\nbe either a receptacle or an object. If it is an object:\n• Domain understanding: Is there a {synonym} in the\ninitial state?\n• Goal inference: Put a {synonym} on Desk1. Which\nobject should we put on Desk1?\nIf it is a receptacle:\n• Domain understanding: Is there a {synonym} in the\ninitial state?\n• Goal inference: Put the {random object} on/in a\n{synonym}. Where should we put {random object}?\n(c) MoveNextTo\n• Domain understanding: Where is {target object} in the\ninitial state?\n• Goal\ninference:\nPut\n{moved object}\nnext\nto\nTABLE VI\nERROR ANALYSIS ON TRANSLATION SUBTASKS ACROSS TASKS (ZERO SHOT).\nError Rate (%) under Successes\nError Rate (%) under Failure\nDomain\nTask Name\nNum. Failures\nDomain Understanding\nGoal Inference\nDomain Understanding\nGoal Inference\nALFRED-L\nMoveSynonym\n2/100\n48.98\n1.02\n100.00\n0.00\nMoveNextTo\n4/100\n0.00\n5.21\n0.00\n75.00\nMoveToCount2\n22/100\n0.00\n2.56\n0.00\n52.17\nMoveToCount3\n53/100\n17.02\n8.51\n16.98\n86.79\nMoveToMore\n27/100\n1.37\n2.74\n11.11\n59.26\nMoveNested\n42/100\n5.17\n8.62\n4.76\n80.95\nMoveNested2\n57/100\n11.63\n11.63\n31.58\n85.96\nTABLE VII\nSUCCESS RATES ON THE GOAL TRANSLATION TASKS (ZERO SHOT)\nSuccess Rate (%)\nDomain\nTask Name\nLoose\nStrict\nALFRED-L\nMoveSynonym\n98.00\n90.00\nMoveNextTo\n96.00\n95.00\nMoveToCount2\n78.00\n72.00\nMoveToCount3\n47.00\n43.00\nMoveToMore\n73.00\n69.00\nMoveNested\n58.00\n55.00\nMoveNested2\n43.00\n39.00\n{target object}.\nDo\nnot\nmove\n{target object}.\nWhere should we put {moved object}?\n(d) MoveToCount2\n• Domain\nunderstanding:\nWhich\nbox\nhas\ntwo\n{target type}s in the initial state?\n• Goal inference: Move {moved object} to the box with\ntwo {target type}s. Which object should we move\n{moved object} into?\n(e) MoveToCount3\n• Domain\nunderstanding:\nWhich\nbox\nhas\nthree\n{target type}s in the initial state?\n• Goal inference: Move {moved object} to the box with\nthree {target type}s. Which object should we move\n{moved object} into?\n(f) MoveToMore\n• Domain\nunderstanding:\nWhich\nbox\nhas\nmore\n{target type}s in the initial state?\n• Goal inference: Move {moved object} to the box with\nmore {target type}s. Which object should we move\n{moved object} into?\n(g) MoveNested\n• Domain understanding: Which sofa is {target object}\nin?\n• Goal inference: Put the {moved object} on the sofa\nwith {target object}. Do not put it in box. Which\nobject should we put {moved object} on?\n(h) MoveNested2\n• Domain understanding: Which sofa is {target object}\nin?\n• Goal inference: Put the {moved object} on the sofa\nwith {target object}. Do not put it in box. Which\nobject should we put {moved object} on?\n(i) CutFruits\n• Domain understanding: What are the fruits?\n• Goal inference: Write the goal speciﬁcation in natural\nlanguage.\n(j) PrepareMeal\n• Domain understanding: What does “prepare a meal”\nmean? What are the relevant objects for preparing a\nmeal?\n• Goal inference: Write the goal speciﬁcation in natural\nlanguage.\n(k) IceCream\n• Domain understanding: Where does the ice cream\nbelong?\n• Goal inference: Write the goal speciﬁcation in natural\nlanguage.\n(l) SetTable2\n• Domain understanding: What does “set the table”\nmean? What are the relevant objects for setting the\ntable?\n• Goal inference: Write the goal speciﬁcation in natural\nlanguage.\n(m) CleanKitchen\n• Domain understanding: What does “clean up the\nkitchen” mean? What are the objects relevant objects\nfor cleaning up the kitchen? Where to put them?\n• Goal inference: Write the goal speciﬁcation in natural\nlanguage.\nF.\nZero-shot performance for ALFRED-L partially speciﬁed\ntasks\nThe zero-shot performance for ALFRED-L partially speci-\nﬁed tasks is listed in Table VII. An error analysis is shown in\nTable VI. In general, performance was lower in the zero-shot\nsetting than in the one-shot setting.\nG.\nFine-grained Performance Report on Blocksworld Tasks\nTbl. V lists detailed success rates and error rates of trans-\nlation results on all Blocksworld tasks. Results of each task\nare split by the number of objects appearing in the problem\nand evaluated separately. For the ExplicitStacks-II task, the\nbenchmark is split into two parts based on the direction of\nthe block sequence in the instruction (whether in each stack\nTABLE VIII\nEXPLICITSTACKS-II (TOP TO BOTTOM):\nEFFECT OF THE EXAMPLE PREDICATE ORDER\nSuccess Rates (%)\nPredicate Order\nNumber of Objects\nLoose\nStrict\nOnTable ﬁrst\n4\n19.00\n17.00\n8\n35.00\n30.00\n12\n42.00\n42.00\nAverage\n32.00\n29.67\nClear ﬁrst\n4\n62.00\n48.00\n8\n98.00\n88.00\n12\n98.00\n98.00\nAverage\n86.00\n78.00\nthe ﬁrt block mentioned is the one at bottom or the one on\nthe top). The constraint violation rates are based on the strict\nmetric. In general, the model makes few PDDL domain-related\nerrors (i.e., PDDL syntax check related to use of correct syntax\nand existence of predicates and objects), which reﬂects that it\nunderstands the syntax of PDDL and knows the objects it can\nuse for building a stack. In contrast, the model fails to avoid\nphysical errors, especially in tasks that involve speciﬁcation\nof multiple stacks. This coincides with the results from the\ndomain understanding subtask tests (in Sec. V) where the\nmodel achieved high performance in object extraction tests\nbut performs much worse in predicate understanding tests.\nH. ExplicitStacks-II: Sensitivity to Sequential Patterns\nIn the ExplicitStacks-II task on Blocksworld, there is a\nsigniﬁcant performance drop when switching the order of\nblock speciﬁcation in stacks from the bottom-to-top pattern to\nthe top-to-bottom pattern. One hypothesis for this phenomenon\nis that the PDDL goal in the example always speciﬁes ﬁrst the\nblock at the bottom of a stack with an ontable predicate,\nthen a sequence of on predicates, and last the block at the\ntop with a clear predicate, which matches a bottom-to-\ntop pattern. Hence, the model captures the linguistic relation\nbetween the PDDL goal and text instruction more easily when\nthey have the same pattern. To validate this hypothesis, we pro-\nvide a different example for the benchmark with top-to-bottom\npattern. This example uses exactly the same PDDL problem\nas the original one, but reverses the order of predicates in\nspeciﬁcation of every stack when deﬁning the ground truth\nPDDL goal. This example matches a top-to-bottom pattern. As\na result, the performance is improved by a signiﬁcant margin\nas shown in Tbl. VIII. This implies that the model is sensitive\nto the sequential patterns in n-shot examples.\nI. KStacks: Do More Examples Guarantee Generalization?\nIn the KStacks task on Blocksworld, one can see that the\nperformance decreases as the number of objects increases.\nConsidering that the number of stacks requested in the in-\nstruction is uniformly sampled from the non-trivial divisors\nof total number of objects, an increase in objects leads to\nmore diverse instructions, which makes it more difﬁcult for\nthe model to choose the right length of block sequences. In\nTABLE IX\nKSTACKS: EFFECT OF THE NUMBER OF EXAMPLES\n# of Stacks(K)\nSuccess Rates(%)\nPrompt\n# of Objects\nExample\nTest\nLoose\nStrict\n1-shot\n4\n2\n2\n94.00\n94.00\n8\n4\n2, 4\n51.00\n51.00\n12\n4\n2, 3, 4, 6\n20.00\n20.00\nAverage\n55.00\n55.00\n2-shot\n4\n2, 2\n2\n100.00\n100.00\n8\n2, 4\n2, 4\n51.00\n51.00\n12\n2, 4\n2, 3, 4, 6\n21.00\n21.00\nAverage\n57.33\n57.33\nfact, a closer observation on the generated goals shows that\nthe model is copying the goal pattern (including the number\nof stacks and the height of each stack) in 90-95% cases where\nnumber of objects is 8 and in 50-60% cases where number\nof objects is 12. Potentially, the model may perform better if\nmore examples are provided to help it generalize. However,\nthis turns out not be the case. Even provided with a second\nexample where the instruction requests a different number of\nstacks, there is no signiﬁcant improvement, as shown in Tbl.\nIX. The table lists all the values of K (the number of stacks to\nbuild) used in both n-shot (n = 1, 2) examples and test cases.\nOur results show that even when all possible answer patterns\nare provided (as in the 8-object cases), the model still failed\nto reliably translate instructions. Possibly, the n-shot prompt\ndesign may need to be improved for translation.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2023-02-10",
  "updated": "2023-02-10"
}