{
  "id": "http://arxiv.org/abs/2302.06370v1",
  "title": "Review of Deep Reinforcement Learning for Autonomous Driving",
  "authors": [
    "B. Udugama"
  ],
  "abstract": "Since deep neural networks' resurgence, reinforcement learning has gradually\nstrengthened and surpassed humans in many conventional games. However, it is\nnot easy to copy these accomplishments to autonomous driving because state\nspaces are immensely complicated in the real world and action spaces are\ncontinuous and fine control is necessary. Besides, autonomous driving systems\nmust also maintain their functionality regardless of the environment's\ncomplexity. The deep reinforcement learning domain (DRL) has become a robust\nlearning framework to handle complex policies in high dimensional surroundings\nwith deep representation learning. This research outlines deep, reinforcement\nlearning algorithms (DRL). It presents a nomenclature of autonomous driving in\nwhich DRL techniques have been used, thus discussing important computational\nissues in evaluating autonomous driving agents in the real environment.\nInstead, it involves similar but not standard RL techniques, adjoining fields\nsuch as emulation of actions, modelling imitation, inverse reinforcement\nlearning. The simulators' role in training agents is addressed, as are the\nmethods for validating, checking and robustness of existing RL solutions.",
  "text": "M00734040 Bavantha Udugama  \n \n \nAbstract—Since \ndeep \nneural \nnetworks' \nresurgence, \nreinforcement learning has gradually strengthened and surpassed \nhumans in many conventional games. However, it is not easy to \ncopy these accomplishments to autonomous driving because state \nspaces are immensely complicated in the real world and action \nspaces are continuous and fine control is necessary. Besides, \nautonomous \ndriving \nsystems \nmust \nalso \nmaintain \ntheir \nfunctionality regardless of the environment's complexity. The \ndeep reinforcement learning domain (DRL) has become a robust \nlearning framework to handle complex policies in high \ndimensional surroundings with deep representation learning. This \nresearch outlines deep, reinforcement learning algorithms (DRL). \nIt presents a nomenclature of autonomous driving in which DRL \ntechniques \nhave \nbeen \nused, \nthus \ndiscussing \nimportant \ncomputational issues in evaluating autonomous driving agents in \nthe real environment. Instead, it involves similar but not standard \nRL techniques, adjoining fields such as emulation of actions, \nmodelling imitation, inverse reinforcement learning. The \nsimulators' role in training agents is addressed, as are the methods \nfor validating, checking and robustness of existing RL solutions. \n \nIndex Terms— Autonomous driving, Deep Reinforcement \nlearning, Controller learning, Motion planning, Trajectory \noptimization \nI. INTRODUCTION \nOR a decade, the autonomous car has been in the news and \ncontinues to dominate auto headlines. Researchers, robotics \norganizations and the automotive industry have been fascinated \nby an autonomous vehicle. Human driving is accident-prone. \nThe failure of humans to obtain smarter spontaneous driving \ndecisions triggers road collisions, asset loss and fatalities[1]. \nThe autonomous vehicle offers us the capability to restore an \nerror-prone human driver by offering reassurance and \nprotection. Driverless systems consist of various functions at \nthe perception level that has now attained high accuracy due to \ndeep learning architectures. In addition to perception, DRL \nautonomous driving technologies have addressed several \nchallenges in which conventional supervised learning \ntechniques are no longer valid. First, as the estimation of the \nagent's behaviour alters upcoming sensor information gained \nfrom the context where the autonomous agent works, the role \nof optimum driving speed in a metropolitan setting, for \nexample, adjusts. Second, the regulatory factors such as the \ntime of collision, the longitudinal deviation w.r.t to the \noptimum route of the autonomous system reflect both the \ndynamics of the agent and environmental ambiguity[2]. Of this \nkind, challenges will entail the concept of the stochastic cost \nfunction to be maximized. This describes a higher feature space \nprovided with  \n \nSpecific settings wherein the agent & ecosystem has been \nstudied, which is significant. In these kinds of situations, \nresearchers attempt to overcome a systematic decision-making \nframework formulated under the classic Reinforcement \nLearning (RL) conditions, where the system is expected to \nobserve and perceive the ecosystem and, therefore, behave \nadequately at every moment. Optimum behaviour is attributed \nto the policy[3].  \nThe principles of reinforcement learning, the classification of \ntasks where RL is a probable approach, particularly in cruising \nstrategy, \npredictive \ncognition, \ntrajectory \nand \nnavigation planning, and low-level control system architecture, \nare discussed in this survey. This analysis also reflects on RL's \nnumerous engagements in the context of autonomous driving \n(AD). Ultimately, discuss deploying modern RL techniques \nlike imitation learning and deep Q learning by showing the \nmain constraints and consequences[1]. \n \nThe main aspects of this review: \n• \nSelf-contained RL overview for the automotive sector. \n• \nComprehensive literature overview about the use of \nRL for various automated driving assignments. \n• \nAnalysis of the main problems and prospects for RL \napplying \nto \nautomated \nvehicles \nin \nthe \nreal \nenvironment. \nII. CONSTITUENTS OF AUTONOMOUS DRIVING SYSTEM \nFig. 1. contains the specific parts of an AD unit's Motion \nplanning, showing the flow from the route planning to the self-\ncontrol actuation. The sensor design involves several sets of \nsensors, radars and LIDARs in a typical autonomous driving \nvehicle and a GPS-GNSS system for accurate positioning and \ninertial measurement units that offer 3D localization to the \ndevice[15]. \nThe purpose of the perception component is to produce an \nintermediary level description of the system's status that is then \nused by a policymaking technique that will effectively establish \nthe operational policy. This primary condition would consist of \nlane placement, drivable region, symbolic location such as \npedestrians and vehicles, state of others, and traffic lights. \nPerception problems spread to the remainder of the \ncommunication chain[6]. Robust technology realization is \nessential for safety; therefore, redundant options improve \nconfidence in detection. This is accomplished by combining \nmultiple vision tasks, including semantic segmentation, motion \nReview of Deep Reinforcement Learning for \nAutonomous Driving \nB. Udugama, Middlesex University, M00734040 \nF\nM00734040 Bavantha Udugama  \n \nestimation, estimation of depth, identification of soiling, which \nis typically easily unified directly through a multi-task design. \nA. Understand the Surrounding \nThe abstract mid-level representation of the perception state \nfrom the perception module is mapped by this main module on \nthe higher-level intervention or even decision-making module. \nAbstractly, \nthis \nspecific \nportion \ngroups \nthree \ntasks: \nComprehension of the scene, decision and even preparation. As \nseen in figure one module, it is assembled on top of the \nalgorithmic localization or detection tasks to establish a higher-\nlevel understanding of the scene. It attempts to vigorously \nsimplify situations by fusing heterogeneous sensor capital as \nthe information becomes even more abstract[4]. \nThis merger material offers a broad and condensed context \nfor the components of the decision. Fusion provides a sceptical \nsensor image of the eco system and models the sensor noise and \neven uncertainties of identification across many modalities such \nas LIDAR, radar, video, ultrasound. This essentially involves \nweighting the projections by using a process based on values. \nB. Localization and Mapping \nOne of the crucial foundations of autonomous driving is \nvisualization. When an area is surveyed, it is easy to find the \nvehicle's actual location on the map. The first coherent AD \npresentations relied largely on localization to pre-mapped areas. \nConventional mapping techniques are improved by semantic \nobject recognition for coherent disambiguation because of the \nextent of the query. In particular, localized high-definition maps \ncan be seen as a precedent for object detection. \nC. Route Planning and policy \nIn the AD pipeline, route preparation is a key factor. This \nmodule is required to create motion-level controls that \nmanoeuvre the car, providing a route-level plan from HD maps \nor GPS based maps. \nD. Controlling the Autonomous system \nA controller determines the speed, steering angle and \ndecelerating behaviour expected by a pre-established map such \nas Google maps over each point in the road or appropriate \ndriving recording of the same values at each waypoint. Path \nfollowing, by contrast, includes a terrestrial model of the \nautomobile's dynamics viewing the waypoints over a given \nduration in sequence[7]. \nIII. RL FOR AUTONOMOUS DRIVING TASKS \nAD tasks where RL could be implemented include \noptimization of controllers, scheduling of paths and \noptimization of trajectories, movement planning and dynamic \npath planning, expansion of high-level driving policies for \ncomplex navigation tasks, outcome-based policy learning for \nexpressways, crossings, mergers and splits, reward learning \nwith converse reinforcement learning from intelligence expert \ndata. Then briefly study the state space, action space and \nrewards mechanisms in these ecosystems before exploring the \nDRL frameworks for AD tasks[6]. \nImplementing adequate state spaces, action spaces, and \nrewards mechanisms is essential in order to effectively apply \nDRL to automated driving assignments. Frequently utilized \nstate-space characteristics for automated driving include: ego-\nvehicle location, heading, and velocity, as well as other \nconstraints in the ego-vehicle sensor view spectrum [5]. This is \nfurther improved by lane details like lane number, route \ncontour, ego-vehicle context and projected trajectory, \nlongitudinal data such as time to collision, and ultimately \nscenario relevant data such as traffic regulations and locations \nof the signal( see Fig. 2.). \n \nIV. REINFORCEMENT LEARNING – MODELING \nA. Modelling the Autonomous system \nA key aspect of the learning experience is modelling the ego-\nvehicle movement as it poses the tradeoff question between \nmodel accuracy and computational capital. Since RL strategies \nuse a large number of episodes to evaluate optimum strategy, \nthe environmental phase time, which strongly depends on the \nvehicle dynamics model's assessment time, has a profound \neffect on training time. More complex models with a larger \nnumber of parameters and complicated tyre models must be \nchosen from the simplest kinematic model to more advanced \ndynamics models[15].  \nSpecial simulators are also used to model traffic and \nsurrounding vehicles. Using cellular automation models, some \nauthors build their environments. Some use MOBIL, which is a \ngeneral model (minimizing lane shift-induced overall braking) \nto extract discretionary and obligatory lane change laws for a \nbroad class of car-following models; the Intelligent Driving \nModel (IDM), a single-lane continuous microscopic model[3]. \nB. Simulation \nTo gain complete control over the model, some writers build \nself-made environments, although there are commercial and \nopen-source environments that can include this functionality. \nFig. 1. Layers of motion planning for AD systems[5] \nFig. 2. deep reinforcement learning based autonomous driving[4] \nM00734040 Bavantha Udugama  \n \nAny of them used in recent research into motion planning with \nRL are briefly described in Table 1. \nC. Actions Space \nThe choice of action configuration depends highly on the \nvehicle model and task configured for the reinforcement \nlearning problem. Although it is possible to see two key layers \nof control: one is the basic control of the car by regulating de \naccelerating and accelerating orders, and the other operates on \nthe behavioural layer and determines strategic level decisions, \nsuch as lane shift, lane management, Accurate reference point \nsetting, etc. The agent gives an order at this stage to low-level \ncontrollers who determine the real trajectory. Just a few papers \ndeal with the layer of motion planning, where the mission \nspecifies the endpoints [11]. In comparison, few papers deviate \nfrom constraints on vehicle motion and produce behaviour by \nmoving onto a grid, such as in classic microscopic models of \ncellular automatics[3]. \nD. Rewarding Functions \nThe agent attempts to fulfil a mission during preparation, \nnormally containing of more than one move. An episode is \ncalled this mission. An episode ends until one of the following \ncriteria is met: \n• \nThe agent executes the role efficiently. \n• \nA previously specified stage is reached by the \nepisode \n• \nA terminating status enhances. \nThe first two cases are insignificant and rely on the real \nproblem's nature. Terminal cases are usually circumstances in \nwhich the agent enters a position from which it is difficult to \nperform the actual mission, or the agent commits an intolerable \nerror. Vehicle motion preparation agents normally use \ntermination circumstances, such as accident or exiting the track \nor lane with other members or barriers, since the episode \neventually concludes with these two. There are lighter ways, \nwith examples of having too high a tangent angle to the track or \nreaching too close to other people, where the episode terminates \nwith failure before the crash occurs. These \"before crash\" \nterminations accelerate the training by taking the loss details \nforward in time, while caution is required in their design[15]. \nThe first significant factor is the pacing of the incentive, \nwhere the builder of the reinforcement learning approach has to \nselect a combination of both the pros and cons of the following \nstrategies: \n• \nRewarding and discounting it back, which could result \nin a slower learning process while reducing the \npolicy's human-driven shaping. \n• \nNaturally, the discount often occurs in this approach, \nproviding immediate reward at each stage by \nmeasuring \nthe \ncurrent \nsituation, \nresulting \nin \nconsiderably faster learning, but the choice of the \nimmediate reward strongly impacts the developed \nstrategy, which often escapes the strategy. \n• \nIn predefined times or travel distances[6], or where a \npositive or poor decision takes place, an intermediate \noption might be to offer an incentive. \nE. Observation Space \nThe room for perception explains the universe to the agent. \nIt needs to have adequate information to choose the required \naction, so it includes - based on the mission - the following \nknowledge: \n1) Vehicle State Observation \nThe most widely used and often the easiest observation for \nthe ego vehicle consists of the unceasing variables of (|e|, v, \nƟe) representing the lateral direction from the centre-line of \nthe lane, vehicle speed, and yaw angle correspondingly for \nlane holding, navigation, easy racing, overtaking, or \nmanoeuvring activities. (see Fig. 3). \n \nFig.3. Basic vehicle state model[1] \n2) Environment Observation \nHaving knowledge about the vehicle world and representing \nit to the learning agent reflects a high degree of diversity in \nthe literature. It is possible to observe different degrees of \nsensor abstractions: \n• \nPerception level, where camera images, lidar or \nradar data are transferred to the agent \n• \nThe intermediate stage, where idealized knowledge \nabout sensors is provided \n• \nGround truth stage, where all information that is \nmeasurable and not detectable is given. \nThe structure of the sensor model also affects the Deep RL \nagent's neural network structure since image-like or array-\nlike inputs infer 2D or 1D CNN structures, whereas a single \ndense network results in a simple collection of scalar \ninformation. There are examples of combining these two \nM00734040 Bavantha Udugama  \n \nkinds of inputs. The network thus has to have two distinct \ntypes of input layers[3]. \nV. EVENT-BASED CLASSIFICATION OF THE APPROACHES \nWhile machine learning may be assumed to provide an \noverall end-to-end approach to autonomous driving, the review \nof recent literature indicates that research on Reinforcement \nLearning may provide answers to some sub-tasks of this \nproblem. The articles can be structured around these problems \nin recent years, where a well-dedicated condition or case is \nselected and investigated whether it can be overcome by a self-\nlearning agent[5]. \nA. Following a car \nThe simplest challenge in this survey is to follow vehicles, \nwhere the question is articulated as follows: There are two \nsimulation participants, a leading vehicle and the following \nvehicle, each retaining their side positions in a lane, and the \nfollowing vehicle changes its longitudinal velocity to ensure a \nsafe subsequent distance. The space out of observation consists \nof the tuple (v, dv, ds), representing agent velocity, lead \nvelocity difference, and distance of headway[4]. \nB. Lane following \nLane-keeping or following the trajectory is still a basic \ncontrol task, but this concern focuses on lateral control, as \nopposed to car follow-up. There are two distinct approaches to \nthe observation room in these studies: One is the lateral \ndirection and angle of the vehicle in the road, \"ground truth,\" \nwhile the second is a front camera view. Naturally, the agents \nuse external simulators, TORCS, and GAZEBO/ROS in these \ninstances for image-based control. The gap from the centerline \nof the lane is almost often regarded by incentive programmes \nas an instant reward. It is important to remember that these \nagents barely consider the dynamics of the vehicle and, oddly, \ndo not rely on collective longitudinal regulation[15]. \nC. Ramp Merging \nThe ramp merge dilemma deals with the highway on-ramp \nsituation, where the ego vehicle has to locate the necessary \ndistance to get on the highway between two vehicles. In the \nsimplest method, the longitudinal regulation where the agent \napproaches this position is available for learning, as can be seen \nin—other papers, such as using complete power of steering and \nacceleration. The linear acceleration of the car accelerates and \ndecelerates in the acts, and the ego vehicle keeps its lane when \nperforming these actions. The \"lane change left\" and \"lane \nchange right\" behaviour indicate lateral motion[2]. \nD. Driving in Stream Of Traffic \nIn recent articles, the most complex situation discussed is \nwhere the autonomous agent is driving in traffic. Naturally, the \ntopology of the network, the quantity and behaviour of the \nadjacent vehicles, the operation of traffic laws, and many other \nfeatures also make this role scalable. In the previous pages, such \nas lane-keeping, or car trailing, sub-tasks of this scenario have \nbeen examined[8]. \nVI. CONCLUSIONS \nIn real-world autonomous driving systems, reinforcement \nlearning is still an active and emerging field. While a few \ncommercial implementations are successful, relatively little \nliterature or large-scale public databases are available. \nTherefore, we were inspired to formalize and coordinate RL \nautonomous driving implementations. Interacting agents are \ninterested in autonomous driving situations that need \nTABLE III \nPROS AND CONS OF USING SIMULATION FOR DRL \nPros \nCons \nCosts \nUnder-modeling \nEnsured Safety \nDynamics \nTraining speed \nObservations/ Sensor models \nSimple dynamics can be handled \nMultiagent behaviours \n \n \nTABLE II \nAUTONOMOUS DRIVING TASKS WITH REUQIRED DRL OT LEARN POLICY BEHAVIOUR \nAD Task  \nDRL METHOD & DESCRIPTION  \nIMPROVEMENTS & TRADEOFFS \nLane Keep \nA DRL method for discrete actions (DQN) and continuous \nactions (DDAC) using the TORCS simulator is proposed by the \nauthors[2]. Authors[3] are studying discrete and continuous \npolicies to follow the lane and optimise average velocity using \nDQNs and Deep Deterministic Actor Critic (DDAC). \n1. This research concludes that the use of continuous behaviour creates \nsmoother pathways, although it leads to more restrictive termination \nconditions on the negative side and longer convergence time to \nunderstand. 2. For quicker integration & improved efficiency, \neliminating memory replay in DQNs helps. The one hot action space \nencoding resulted in sudden power of steering. Although the continuing \nstrategy of DDAC helps smooth the acts and delivers improved results. \nLane Change \nAuthors[15] are using Q-learning to learn a no-operation \nguideline for egovehicle, lane shift to left/right, \naccelerate/decelerate. \nCompared to conventional approaches, this approach is more stable, \nand consists of identifying fixed way points, velocity profiles, and \ndirection curvature to be taken by the ego car. \nRamp Merging \nAuthors[5] suggest recurrent architectures, namely LSTMs, to \nmodel long-term dependencies for the merger into a highway \nramp of ego automobiles. \nTo execute the merging more robustly, historical experience of state \nknowledge is used. \nIntersections \nTo negotiate intersection, authors use DQN to test the Q-value \nfor state-action pairs[15],, \nAuthor-defined Creep-Go behaviour allow the vehicle to more securely \nnavigate intersections with small spaces and visibility. \nMotion Planning \nAn improved A¤ algorithm is proposed by the authors[88] to \nlearn a heuristic function using deep neural networks over \nimage-based obstacle map input. \nSmooth vehicle control behaviour and increased performance compared \nto multi-step DQNN \nFig.4. Ramp merge: (a) simulated scenario and (b) real-world location[3] \nM00734040 Bavantha Udugama  \n \nnegotiation and complex decision making that fits RL. \nHowever, in order to provide advanced ideas that we address \nin-depth, there are more problems to be overcome. Detailed \ntheoretical reinforcement learning is discussed in this work. \nLatest advances in the area have demonstrated that numerous \ndeep reinforcement learning methods can be successfully used \nfor various stages of motion planning problems for autonomous \nvehicles, but several questions remain unanswered. The key \nbenefit of these approaches is that unstructured data such as raw \nor slightly pre-processed radar or camera-based image \ninformation can be treated. \nThe comparatively low computational criteria of the trained \nnetwork are one of the key advantages of using deep neural \nnetworks trained by a reinforcement learning agent in motion \nplanning. While this method requires a large number of trials in \nthe learning phase to obtain adequate knowledge, as stated \nbefore, for basic problems of convex optimization, the \nmechanism converges easily. However, the preparation can \nrapidly hit millions of measures with complicated situations, \nmeaning only one setup of hyperparameters or incentive \nhypothesis can last hours or even days. \nSince complex reinforcement learning tasks involve ongoing \niteration on the design of the environment, network \nconfiguration, incentive scheme, or even the algorithm itself, it \nis a time-consuming activity to design such a method. The \nmeasurement time depends heavily on the delegated computing \nresources and the required outcome interpretation and \ninference. On this basis, it is not shocking that most articles \nnowadays deal with small subtasks of motion planning, and the \nmost complicated situations can not be found in the literature, \nsuch as travelling in urban traffic. RL itself, like many \nheuristics, has a tradeoff between efficiency and the need for \ncapital.  \n \nThe principal purpose of reinforcement learning is to \nstatistically optimize the long-term incentive. Nevertheless, the \nmain priority is the avoidance of injuries for vehicle control \nactivities. Although the use of behaviour that triggers \nsignificant negative rewards does not inherently eliminate RL, \nother strategies must control the hazards. In several ways, the \nliterature discusses protection and threats, for which[4] offers \nan exemplary overview. In this field, two principal directions \ncan be separated. The approaches using the optimization criteria \nare included in one group of solutions, while the other group \nincludes algorithms that change the discovery process. One has \nsome choices for adjusting optimization parameters as well. \nThe worst-case criterion is the first. Addressing the worst-case \nsituations solves the concerns created by the uncertainty \nresulting from the stochastic instability of the system and the \nparameter uncertainties. The risk-sensitive criteria are the \nsecond choice. In this circumstance, a scalar parameter, a so-\ncalled risk susceptibility parameter, is applied to the loss \nfunction to control the degree of risk. Finally, it is possible to \nuse a restricted Markov decision process (MDP), where the \ndefault MDP tuple is expanded with a constraint set that must \nbe satisfied by the policy function. \nContrary to the classic exploration approach, changing the \nexploration phase is an alternative, which means that the agent \nknows something from scratch. That also leads to disastrous \nsituations with vehicle control applications. In comparison, \nfully unintended discovery techniques spend a lot of time \ninvestigating the meaningless areas of the underlying state \nspace, which is particularly important in broad and continuous \nstate spaces. Two key directions are available. Through \napplying external intelligence, one guides the discovery \nprocess, while the other uses risk assessment. Through \ndemonstrating the fascinating or dangerous sections of state \nspace, the demonstrator may also lead the exploration online. \nAnd, ultimately, a supervisory control system will follow \nchallenging constraints. \nOverall, a dynamically changing field is the principle of \nstable RL. The subject's importance is unquestionable from the \npoint of view of vehicle regulation, not only for safety but also \nfor the reduction of the state and the room for intervention. The \npreference of troublesome, so-called corner cases from a large \nrange of unrelated conditions is one of the major problems with \npreparation and validation. \n \nIn general, three paths to narrowing the gap exist: \n• \nIdentification of the system, aiming to adapt the \nsimulation to reality. \n• \nDomain adaptation helps to learn a well model from a \nsource distribution of data on a separate target data \ndistribution. \n• \nRandomization of the domain targeted learning in a \nvery randomized environment that covers the target \nand makes the agent resilient. \n \nThe tradeoff between the completely modelled system and \nfeasibility was discussed, so identifying the system is not \ndefined here. One aims to locate the transition strategy between \nthe virtual and the actual representations during Domain \nadaptation. As an example, this transition can be solved by a \nsemantically segmented image for image sequences taken from \na front-facing camera. In [2], the two realms meet at the \nsegmented stage in the centre, while in [1], the authors attempt \nto build \"realistic\" training images using generative adversarial \nnetworks (GAN) [7]. \nOverall, many problems in this area remain to be addressed, \nsuch as environmental detail and sensor simulation, \ncomputational specifications, transferability to actual systems, \nrobustness, and agent validation. Due to these concerns, it can \nbe claimed that reinforcement learning is not an adequate \nmethod for automotive motion planning. However, when \ncombined with other approaches, it can be very useful in \nsolving complex optimization challenges. \nREFERENCES \n[1] A.-E. Sallab, M. Abdou, E. Perot, and S. Yogamani, \"Deep \nreinforcement learning framework for autonomous driving,\" \nElectronic Imaging, vol. 2017, no. 19, pp. 70–76, 2017. \n[2] V. Talpaert., I. Sobh., B. R. Kiran., P. Mannion., S. Yogamani., A. \nEl-Sallab., and P. Perez., \"Exploring applications of deep \nreinforcement learning for real-world autonomous driving systems,\" \nin Proceedings of the 14th International Joint Conference on \nComputer Vision, Imaging and Computer Graphics Theory and \nApplications - Volume 5 VISAPP: VISAPP,, INSTICC. SciTePress, \n2019, pp. 564–572. \n[3] K. El Madawi, H. Rashed, A. El Sallab, O. Nasr, H. Kamel, and S. \nYogamani, \"Rgb and lidar fusion based 3d semantic segmentation \nfor autonomous driving,\" in 2019 IEEE Intelligent Transportation \nSystems Conference (ITSC). IEEE, 2019, pp. 7–12.  \nM00734040 Bavantha Udugama  \n \n[4] M. Siam, S. Elkerdawy, M. Jagersand, and S. Yogamani, “Deep \nsemantic segmentation for automated driving: Taxonomy, roadmap \nand challenges,\" in 2017 IEEE 20th international conference on \nintelligent transportation systems (ITSC). IEEE, 2017, pp. 1–8. \n[5] S. Kuutti, R. Bowden, Y. Jin, P. Barber, and S. Fallah, \"A survey of \ndeep learning applications to autonomous vehicle control,\" IEEE \nTransactions on Intelligent Transportation Systems, 2020. \n[6] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, and K. Fujimura, \n\"Navigating occluded intersections with autonomous vehicles using \ndeep reinforcement learning,\" in 2018 IEEE International \nConference on Robotics and Automation (ICRA). IEEE, 2018, pp. \n2034–2039. \n[7] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. \nGoyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang et al., \"End to \nend learning for self-driving cars,\" in NIPS 2016 Deep Learning \nSymposium, 2016. \n[8] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, \n\"CARLA: An open urban driving simulator,\" in Proceedings of the \n1st Annual Conference on Robot Learning, 2017, pp. 1–16. \n[9] B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom, \nand A. Sumner, \"Torcs, the open racing car simulator,\" Software \navailable at http://torcs. sourceforge. net, vol. 4, 2000. \n[10] S. Shah, D. Dey, C. Lovett, and A. Kapoor, \"Airsim: High-fidelity \nvisual and physical simulation for autonomous vehicles,\" in Field \nand Service Robotics. Springer, 2018, pp. 621–635. \n[11] N. Koenig and A. Howard, \"Design and use paradigms for gazebo, \nan open-source multi-robot simulator,\" in 2004 International \nConference on Intelligent Robots and Systems (IROS), vol. 3. IEEE, \n2004, pp. 2149–2154. \n[12] ] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P. \nFlötteröd, R. Hilbrich, L. Lücken, J. Rummel, P. Wagner, and E. \nWießner, “Microscopic traffic simulation using sumo,” in The 21st \nIEEE International Conference on Intelligent Transportation \nSystems. IEEE, 2018. 12 \n[13] C. Quiter and M. Ernst, “deepdrive/deepdrive: 2.0,” Mar. 2018. \n[Online]. Available: https://doi.org/10.5281/zenodo.1248998 \n[14] Nvidia, \n“Drive \nConstellation \nnow \navailable,” \nhttps://blogs.nvidia.com/blog/2019/03/18/drive-constellation-now-\navailable/, 2019, [accessed 24-Feb-2021]. \n[15] P. Wang, C.-Y. Chan, and A. de La Fortelle, \"A reinforcement \nlearning based approach for automated lane change maneuvers,\" in \n2018 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2018, pp. \n1379–1384. \n \n \n \n \n \n \n",
  "categories": [
    "cs.RO"
  ],
  "published": "2023-02-13",
  "updated": "2023-02-13"
}