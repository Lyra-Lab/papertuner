{
  "id": "http://arxiv.org/abs/2302.07503v1",
  "title": "Excess risk bound for deep learning under weak dependence",
  "authors": [
    "William Kengne"
  ],
  "abstract": "This paper considers deep neural networks for learning weakly dependent\nprocesses in a general framework that includes, for instance, regression\nestimation, time series prediction, time series classification. The $\\psi$-weak\ndependence structure considered is quite large and covers other conditions such\nas mixing, association,$\\ldots$ Firstly, the approximation of smooth functions\nby deep neural networks with a broad class of activation functions is\nconsidered. We derive the required depth, width and sparsity of a deep neural\nnetwork to approximate any H\\\"{o}lder smooth function, defined on any compact\nset $\\mx$. Secondly, we establish a bound of the excess risk for the learning\nof weakly dependent observations by deep neural networks. When the target\nfunction is sufficiently smooth, this bound is close to the usual\n$\\mathcal{O}(n^{-1/2})$.",
  "text": "arXiv:2302.07503v1  [stat.ML]  15 Feb 2023\nExcess risk bound for deep learning under weak\ndependence\nFebruary 16, 2023\nWilliam KENGNE 1\nTHEMA, CY Cergy Paris Universit´e, 33 Boulevard du Port, 95011 Cergy-Pontoise Cedex, France\nE-mail: william.kengne@cyu.fr\nAbstract: This paper considers deep neural networks for learning weakly dependent processes in a general\nframework that includes, for instance, regression estimation, time series prediction, time series classiﬁca-\ntion. The ψ-weak dependence structure considered is quite large and covers other conditions such as mixing,\nassociation,. . . Firstly, the approximation of smooth functions by deep neural networks with a broad class of\nactivation functions is considered. We derive the required depth, width and sparsity of a deep neural network\nto approximate any H¨older smooth function, deﬁned on any compact set X. Secondly, we establish a bound of\nthe excess risk for the learning of weakly dependent observations by deep neural networks. When the target\nfunction is suﬃciently smooth, this bound is close to the usual O(n−1/2).\nKeywords: Deep neural networks, weak dependence, function approximation, excess risk, ERM principle.\n1\nIntroduction\nWe focus on the supervised learning framework and consider the training sample Dn = {Z1 = (X1, Y1), · · · , Zn =\n(Xn, Yn)} which is a trajectory of a stationary and ergodic process {Zt = (Xt, Yt), t ∈Z}, taking values in\nZ = X × Y, where X is the input space and Y the output space. In the sequel, we assume that X ⊂Rdx and\nY ⊂Rdy, with dx, dy ∈N. Denote by F(X, Y) the set of measurable functions from X to Y. Consider a loss\nfunction ℓ: Rdy × Y →[0, ∞) and for a predictor h ∈F(X, Y), deﬁne the risk,\nR(h) = EZ0\n\u0002ℓ\u0000h(X0), Y0\n\u0001\u0003.\n(1.1)\nFor a learner h ∈F(X, Y), a prediction of Yt is bYt = h(Xt) for all t ∈Z. The ”best” predictor h∗∈F(X, Y),\nwhen it exists, is the one that achieves the smallest risk, that is,\nR(h∗) =\ninf\nh∈F(X ,Y)R(h).\n(1.2)\n1Developed within the ANR BREAKRISK: ANR-17-CE26-0001-01, the MME-DII center of excellence (ANR-11-LABEX-0023-\n01) and the CY Initiative of Excellence (grant ”Investissements d’Avenir” ANR-16-IDEX-0008), Project ”EcoDep” PSI-AAP2020-\n0000000013\n1\n2\nExcess risk bound for deep learning under weak dependence\nWe would like to construct a learner h ∈F(X, Y) such that, for any t ∈Z, h(Xt) is average ”close” to Yt;\nthat is, a learner with the smallest risk. But in general, this risk cannot be minimized in practice, because the\ndistribution of (X0, Y0) is unknown. Consider the empirical risk, deﬁned for all h ∈F(X, Y) by,\nbRn(h) = 1\nn\nn\nX\ni=1\nℓ\u0000h(Xi), Yi\n\u0001.\n(1.3)\nThus, the aim is to build from the training sample Dn, a learner bhn, that minimizes the empirical risk. In the\nsequel, we set ℓ(h, z) = ℓ\u0000h(x), y\u0001 for all z = (x, y) ∈X × Y and h ∈F(X, Y).\nWe focus on the class of deep neural networks (DNN) predictors, with a general activation function σ :\nR →R. Recall that, a neural network architecture (L, p) stands for a positive integer L called the number of\nhidden layers or depth and a width vector p = (p0, p1, · · · , pL+1) ∈NL+2. Therefore, a DNN with network\narchitecture (L, p) is any function of the form,\nh : Rp0 →RpL+1, x 7→h(x) = AL+1 ◦σL ◦AL ◦σL−1 ◦AL−1 ◦· · · ◦σ1 ◦A1(x),\n(1.4)\nwhere for any ℓ= 1, · · · , L + 1, Aℓ: Rpℓ−1 →Rpℓis an aﬃne linear application deﬁned by Aℓ(x) := Wℓx + bℓ\nfor given pℓ−1 × pℓweight matrix Wℓ, a shift vector bℓ∈Rpℓand σℓ: Rpℓ→Rpℓis an element-wise nonlinear\nactivation map deﬁned as σℓ(z) = (σ(z1), · · · , σ(zpℓ))T for all z = (z1, · · · , zpℓ)T ∈Rpℓ, and T denotes the\ntranspose. In the setting considered here, p0 = dx (input dimension) and pL+1 = dy (output dimension). For\na DNN f of the form (1.4), the vector of its parameters is denoted by θ(h), that is,\nθ(h) =\n\u0000vec(W1)T , bT\n1 , · · · , vec(WL+1)T , bT\nL+1\n\u0001\n,\nwhere vec(W) denotes the vector obtained by concatenating the column vectors of the matrix W. In the\nsequel, we deal with an activation function σ and denote by Hσ,dx,dy the set of DNNs with dx dimensional\ninput and dy dimensional output. For any h ∈Hσ,dx,dy with a neural network architecture (L, p), depth(h)\ndenotes its depth and width(h) denotes its width, that is, depth(h) = L and width(h) = max\n1≤ℓ≤Lpℓ. For any\nL, N, S, B, F > 0, consider the sets,\nHσ,dx,dy(L, N) =\n\b\nh ∈Hσ,dx,dy, depth(h) ≤L, width(h) ≤N\n\t\n,\nHσ,dx,dy(L, N, S, B) = \bh ∈Hσ,dx,dy(L, N), |θ(h)|0 ≤S, ∥θ(h)∥≤B\t,\n(1.5)\nHσ,dx,dy(L, N, S, B, F) = \bh1X , h ∈Hσ,dx,dy(L, N, S, B), ∥h∥∞,X ≤F\t,\n(1.6)\nwhere for any x = (x1, · · · , xd)T ∈Rd, |x|0 = Pd\ni=1 1(xi ̸= 0), ∥x∥= max\n1≤i≤d|xi| and ∥h∥∞,X = supx∈X ∥h(x)∥\n(see also the Subsection 2.1 bellow).\nThere are many works in the literature on the approximation of smooth functions by DNNs. We refer\nto [25], [23], [20], [18], [21], [22] and the references therein for an overview. The existing results are mainly\nobtained, either for a restricted class of activation function (for example, ReLU), or for a speciﬁc case of input\nspace (for example, X = [0, 1]dx). The ﬁrst contribution of this paper:\n(i) Establish approximation results by DNNs with a quite large class of activation function, of smooth\nfunctions deﬁned on any compact input space X ⊂Rdx.\nW. Kengne\n3\nThe second issue of this work is the learning of the process {Zt = (Xt, Yt), t ∈Z} by the class of DNNs\nHσ,dx,dy(L, N, S, B, F) with L, N, S, B, F > 0. There is a large literature that addresses such question with\nindependent and identically distributed (i.i.d.) observations. See, among others papers, [1], [18], [22], [13],\n[19]. For some theoretical results on dependent observations, see for instance, [14], [15], [17]. These works\nare for mixing observations and/or for the regression problem. The recent work of [12] considers a general\nframework of supervised learning for weakly dependent processes, where the weak dependence structure covers\nmixing conditions. But, the convergence of the excess risk is not addressed. The second contribution of this\npaper:\n(ii) Derive the convergence rate of the excess risk for the learning of ψ-weakly dependent processes by DNNs.\nThis rate is close to the usual O(n−1/2) when the target function is suﬃciently smooth.\nThe rest of the paper is structured as follows. Section 2 sets some notations and assumptions. Section 3\nfocuses on H¨older smooth functions approximation by DNNs. Section 4 considers the excess risk and provides\nits convergence rate, whereas Section 5 is devoted to the proofs of the main results.\n2\nNotations and assumptions\n2.1\nSome notations\nThroughout the sequel, the following notations will be used, with d ∈N, and where E1, E2 are subsets of\nseparable Banach spaces equipped with norms ∥· ∥E1 and ∥· ∥E2 respectively.\n• N0 = N ∪{0}.\n• For all x ∈R, [x] denotes the integer part of x, ⌈x⌉denotes the smallest integer ≥x and ⌊x⌋denotes the\nlargest integer ≤x.\n• ∥x∥= max\n1≤i≤d|xi|, |x|0 = Pd\ni=1 1(xi ̸= 0) for all x = (x1, · · · , xd)T ∈Rd.\n• ∥x∥= max\n1≤i≤p\nPq\nj=1 |xi,j| for any matrix x = (xi,j) ∈Mp,q(R); where Mp,q(R) denotes the set of matrices\nof dimension p × q with coeﬃcients in R.\n• For any function h : E1 →E2 and U ⊆E1,\n∥h∥∞= sup\nx∈E1\n∥h(x)∥E2, ∥h∥∞,U = sup\nx∈U\n∥h(x)∥E2 and\nLipα(h) :=\nsup\nx1,x2∈E1, x1̸=x2\n∥h(x1) −h(x2)∥E2\n∥x1 −x2∥α\nE1\nfor any α ∈[0, 1].\n• For any K > 0 and α ∈[0, 1], Λα,K(E1, E2) (simply Λα,K(E1) when E2 ⊆R) denotes the set of functions\nh : Eu\n1 →E2 for some u ∈N, such that ∥h∥∞< ∞and Lipα(h) ≤K. When α = 1, we set Lip1(h) =\nLip(h) and Λ1(E1) = Λ1,1(E1, R).\n• F(E1, E2) denotes the set of measurable functions from E1 to E2.\n• For any h ∈F(E1, E2) and ǫ > 0, B(h, ǫ) denotes the ball of radius ǫ of F(E1, E2) centered at h, that\nis, B(h, ǫ) = \bf ∈F(E1, E2), ∥f −h∥∞≤ǫ\t.\n4\nExcess risk bound for deep learning under weak dependence\n• For any H ⊂F(E1, E2), the ǫ-covering number N(H, ǫ) of H is the minimal number of balls of radius ǫ\nneeded to cover H; that is,\nN(H, ǫ) = inf\nn\nm ≥1 : ∃h1, · · · , hm ∈H such that H ⊂\nm\n[\ni=1\nB(hi, ǫ)\no\n.\n• For any U ⊂E1, U denotes the closure of U.\n• For any function g : R →R, g′ and g′′ denote the ﬁrst and second order derivatives of g. Also, for all\nx ∈R, we set g′(x+) := limǫ↓0\n\u0000g(x + ǫ) −g(x)\u0001/ǫ and g′(x−) := limǫ↓0\n\u0000g(x −ǫ) −g(x)\u0001/ǫ.\n• For any bounded set U ⊂E1, ∥U∥= supx∈U ∥x∥.\n• For all x = (x1, · · · , xd) ∈Rd and β = (β1, · · · , βd) ∈Nd\n0, xβ = xβ1\n1 · . . . · xβd\nd\nand β! = β1! · . . . · βd!.\n2.2\nSome assumptions and weak dependence\nLet us set the following assumptions on the input space X and the loss function ℓ.\n(A1): X ⊂Rdx is a compact set.\nUnder (A1), we set\nR := max(1, 4∥X∥).\n(2.1)\n(A2): There exists Kℓ> 0 such that, ℓ∈Λ1,Kℓ(Rdy × Y).\nFor all L, N, S, B, F > 0, we set\nML,N,S,B,F := max\n\u0010\nsup\nh∈Hσ,dx,dy\nsup\nz∈Z\nℓ(h, z), 1\n\u0011\nand GL,N,S,B,F :=\nsup\nh1,h2∈Hσ,dx,dy ,h1̸=h2\nsup\nz∈Z\n|ℓ(h1, z) −ℓ(h2, z)|\n∥h1 −h2∥∞\n,\n(2.2)\nwith Hσ,dx,dy = Hσ,dx,dy(L, N, S, B, F). When it is clear in the context and no confusion can arise, we use\nM, G for ML,N,S,B,F and GL,N,S,B,F respectively. Under the assumption (A1) and if Y is bounded, we have,\nf\nMF :=\nsup\n(y1,y2)∈B\u0000max(F,∥Y∥)\u0001 ℓ(y1, y2) < ∞,\n(2.3)\nwhere B\u0000 max(F, ∥Y∥)\u0001 ⊂Rdy × Rdy is the ball of radius max(F, ∥Y∥), centered at 0. Let z0 = (x0, y0) ﬁxed.\nIn addition to (A2), we have for all z = (x, y) ∈Z and h ∈Hσ,dx,dy,\nℓ(h, z) ≤ℓ(h, z0) + |ℓ(h, z) −ℓ(h, z0)| ≤ℓ(h(x0), y0) + |ℓ(h(x), y) −ℓ(h(x0), y0)|\n≤f\nMF + Kℓ(∥x −x0∥+ ∥y −y0∥) ≤f\nMF + 2Kℓ(∥X∥+ ∥Y∥).\n(2.4)\nIn this case, we get,\nML,N,S,B,F ≤max\n\u0000f\nMF + 2Kℓ(∥X∥+ ∥Y∥), 1\n\u0001\n< ∞,\n(2.5)\nand this bound does not depend on L, N, S, B. Also, under (A2), we get for all h1, h2 ∈Hσ,dx,dy with h1 ̸= h2\nand z = (x, y) ∈Z such that h1(x) ̸= h2(x),\n|ℓ(h1, z) −ℓ(h2, z)|\n∥h1 −h2∥∞\n= |ℓ(h1(x), y) −ℓ(h2(x), y)|\n∥(h1(x), y) −(h2(x), y)∥× ∥(h1(x), y) −(h2(x), y)∥\n∥h1 −h2∥∞\n≤Kℓ.\n(2.6)\nW. Kengne\n5\nHence, GL,N,S,B,F ≤Kℓand this bound does not depend on L, N, S, B, F.\nLet us give the deﬁnition of the weak dependence in a general context, see [8] and [4]. Let E be a separable\nBanach space.\nDeﬁnition 2.1 An E-valued process (Zt)t∈Z is said to be (Λ1(E), ψ, ǫ)-weakly dependent if there exists a\nfunction ψ : [0, ∞)2 × N2 →[0, ∞) and a sequence ǫ = (ǫ(r))r∈N decreasing to zero at inﬁnity such that, for\nany g1, g2 ∈Λ1(E) with g1 : Eu →R, g2 : Ev →R (u, v ∈N) and for any u-tuple (s1, · · · , su) and any v-tuple\n(t1, · · · , tv) with s1 ≤· · · ≤su ≤su + r ≤t1 ≤· · · ≤tv, the following inequality is fulﬁlled:\n|Cov (g1(Zs1, · · · , Zsu), g2(Zt1, · · · , Ztv))| ≤ψ (Lip(g1), Lip(g2), u, v) ǫ(r).\nThe following choices of ψ (see also [4]) are well known examples.\n• ψ (Lip(g1), Lip(g2), u, v) = vLip(g2): the θ-weak dependence, then denote ǫ(r) = θ(r);\n• ψ (Lip(g1), Lip(g2), u, v) = uLip(g1) + vLip(g2): the η-weak dependence, then denote ǫ(r) = η(r);\n• ψ (Lip(g1), Lip(g2), u, v) = uvLip(g1) · Lip(g2): the κ-weak dependence, then denote ǫ(r) = κ(r);\n• ψ (Lip(g1), Lip(g2), u, v) = uLip(g1)+vLip(g2)+uvLip(g1)·Lip(g2): the λ-weak dependence, then denote\nǫ(r) = λ(r).\nLet us set now the weak dependence assumption.\n(A3): The process {Zt = (Xt, Yt), t ∈Z} is stationary ergodic and (Λ1(Z), ψ, ǫ)-weakly dependent with\nǫr = O(r−γ) for some γ > 3.\nNumerous classical models satisfy (A3); for example, ARMAX, TARX, GARCH-X, ARMAX-GARCH, APARCH-\nX ([10]), multivariate INGARCH (with exponential family conditional distribution), see for instance [7], [5].\nOther examples such as APARCH-X(δ, ∞), ARX(∞)-ARCH(∞) introduced by [6] also satisfy (A3), see [7].\n3\nH¨older smooth functions approximation by DNNs\nLet U ⊆Rdx. For any β = (β1, · · · , βdx)T ∈Ndx and x = (x1, · · · , xdx)T ∈U, we set\n|β| =\ndx\nX\ni=i\nβi and ∂β =\n∂|β|\n∂xβ1\n1 · · · ∂xβdx\ndx\n.\nFor any s > 0, the H¨older space Cs(U) is a set of vector valued functions h : U →Rdy such that, for any\nβ ∈Ndx with |β| ≤[s], ∥∂βh∥∞< ∞and for any β ∈Ndx with |β| = [s], Lips−[s](∂βh) < ∞. This space is\nequipped with the norm\n∥h∥Cs(U) =\nX\n0≤|β|≤[s]\n∥∂βh∥∞+\nX\n|β|=[s]\nLips−[s](∂βh).\nFor s > 0, U ⊆Rdx, Cs(U) with the norm ∥· ∥Cs(U) is a Banach space (see for example [24], [9]). For any\ns > 0, U ⊆Rdx and K > 0, set\nCs,K(U) =\n\b\nh ∈Cs(U), ∥h∥Cs(U) ≤K\n\t\n.\nLet us consider the following deﬁnition, see also [18], [19].\n6\nExcess risk bound for deep learning under weak dependence\nDeﬁnition 3.1 Let a function g : R →R.\n1. g is continuous piecewise linear (or ”piecewise linear” for notational simplicity) if it is continuous and\nthere exists K (K ∈N) break points a1, · · · , aK ∈R with a1 ≤a2 ≤· · · ≤aK such that, for any\nk = 1, · · · , K, g′(ak−) ̸= g′(ak+) and g is linear on (−∞, a1], [a1, a2], · · · [aK, ∞).\n2. g is locally quadratic if there exits an interval (a, b) on which g is three times continuously diﬀerentiable\nwith bounded derivatives and there exists t ∈(a, b) such that g′(t) ̸= 0 and g′′(t) ̸= 0.\nThe classical activation functions σ(z) = max(z, 0) (ReLU) is piecewise linear and σ(z) = 1/(1+e−z) (sigmoid)\nis locally quadratic (see [18]). Consider the following assumption on the activation function σ.\n(A4): There exists Kσ > 0 such that σ ∈Λ1,Kσ(R). Moreover, σ is either piecewise linear or locally quadratic\nand ﬁxes a segment I ⊆[0, 1].\nRecall that, σ ﬁxes the segment I if σ(z) = z for all z ∈I. Here are some examples of activation functions\nsatisfying (A4).\n• ReLU: σ(z) = max(z, 0).\n• Leaky ReLU: σ(z) = max(z, az), for a ∈(0, 1).\n• Exponential linear unit (ELU) [3]: σ(z) = a(ez −1)1(z ≤0) + z1(z > 0), for a > 0.\n• Inverse square root linear unit (ISRLU) [2]: σ(z) =\nz\n√\n1+az21(z ≤0) + z1(z > 0), for a > 0.\n• SignReLu [16]: σ(z) = a\nz\n1−z1(z < 0) + z1(z ≥0), for a ∈(0, 1).\nThe following theorem provides an approximation of any H¨older smooth function by DNNs with a large\nclass of activation functions.\nTheorem 3.2 Assume that (A1) and (A4) hold. Let s, K > 0 and h : X →Rdy with h ∈Cs,K(X). There\nexist L0, N0, S0, B0 > 0 depending only on dx, dy, s, K, R (deﬁned at (2.1)) and σ such that, for all ǫ > 0, there\nexists a neural network hǫ ∈Hσ,dx,dy(L0 log+(1/ǫ), N0ǫ−dx/s, S0ǫ−dx/s log+(1/ǫ), B0ǫ−4(dx/s+1) satisfying,\n∥h −hǫ∥∞,X ≤ǫ,\n(3.1)\nwhere log+ x = max(1, log x) for all x > 0.\nTheorem 3.2 is an extension of Theorem 1 of [18] to vector valued functions deﬁned on any compact subset\nof Rdx. This theorem also extends Theorem 1 of [21] to deep neural networks with a broad class of activation\nfunctions.\n4\nExcess risk bound\nConsider the learning problem presented in the introduction, based on the observations Dn = {Z1 = (X1, Y1), · · · , Zn =\n(Xn, Yn)} which is a trajectory of a stationary and ergodic process {Zt = (Xt, Yt), t ∈Z}, taking values in\nW. Kengne\n7\nZ = X ×Y, with Y ⊂R, that is, dy = 1. We focus on the class of DNNs Hσ,dx,1(Ln, Nn, Sn, Bn, Fn) (deﬁned at\n(1.6)), for some Ln, Nn, Sn, Bn, Fn > 0. Set for the sequel, Hσ,n := Hσ,dx,1(Ln, Nn, Sn, Bn, Fn). Also, denote,\nMn := MLn,Nn,Sn,Bn,Fn, Gn := GLn,Nn,Sn,Bn,Fn and f\nMn := MFn,\n(4.1)\nwhere MLn,Nn,Sn,Bn,Fn, GLn,Nn,Sn,Bn,Fn and MFn are deﬁned in (2.2) and (2.3). The neural network obtained\nfrom the empirical risk minimization (ERM) algorithm is given by,\nbhn = argmin\nh∈Hσ,n\nbRn(h),\n(4.2)\nwhere bRn is deﬁned at (1.3). The excess risk is given by R(bhn)−infh∈F(X ,Y) R(h), where R is deﬁned at (1.1).\nThis excess risk has the following well known decomposition,\nR(bhn) −\ninf\nh∈F(X ,Y)R(h) = R(bhn) −\ninf\nh∈Hσ,nR(h)\n|\n{z\n}\nEstimation error\n+\ninf\nh∈Hσ,nR(h) −\ninf\nh∈F(X ,Y)R(h)\n|\n{z\n}\nApproximation error\n.\n(4.3)\nIn the sequel, we consider the target neural network hHσ,n ∈Hσ,n, assumed to exist and satisﬁes,\nR(hHσ,n) =\ninf\nh∈Hσ,nR(h).\n(4.4)\nA major concern in statistical learning theory is to solve the problem of the best trade-oﬀbetween the\nestimation and approximation errors.\nThis is done by taking into account the complexity of the class of\nhypothesis functions, which must not be too large, nor too small.\nIn this section, we focus on the convergence rate of the excess risk, and set an additional assumption.\n(A5): There exists a predictor h∗∈F(X, Y), satisfying R(h∗) = infh∈F(X ,Y) R(h) and h∗∈Cs,K(X) for some\ns, K > 0.\nThe following theorem provides a bound of the excess risk.\nTheorem 4.1 Assume that (A1)-(A5) hold. Let η, ν ∈(0, 1) and α > 2 + dx/s. Then, there exist universal\nconstants L0, N0, S0, B0 > 0 and n0 ≥1, depending on Kℓ, Cσ, dx, η, ν, α, s, L0, N0, S0, B0, such that, for any\nn ≥n0, Fn > 0, if for the DNN’s class\nHσ,n = Hσ,dx,1\n\u0010\n(L0/α) log n, N0ndx/(sα), (S0/α)ndx/(sα) log n, n4(dx/s+1)/α, Fn\n\u0011\n,\nMn < ∞, then, we have with probability at least 1 −η,\nR(bhn) −\ninf\nh∈F(X ,Y)R(h) ≤2Mn + Kℓ\nn1/α\n+\n \nlog(2C1 log n/η)\nCn,2\n!1/2\n(4.5)\nfor some constant C1 > 0, where bhn is deﬁned in (4.2), Mn is given in (4.1) and (2.2), and Cn,2 is given in\n(5.21).\nRemark 4.2 If Y is bounded, and by choosing a constant sequence Fn = F > 0, then, from (2.5), (4.1) and\n(4.5), we get,\nR(bhn) −\ninf\nh∈F(X ,Y)R(h) ≤2 max\n\u0000f\nMF + 2Kℓ(∥X∥+ ∥Y∥), 1\n\u0001\n+ Kℓ\nn1/α\n+\n \nlog(2C1 log n/η)\nCn,2\n!1/2\n.\n(4.6)\n8\nExcess risk bound for deep learning under weak dependence\nWhich shows that, the convergence rate of the excess risk is less than O(n−1/α) for all α > 2 + dx/s. This\nrate is close to O(n−1/2) when s ≫dx. Note that, the rate obtained in [18] is close to O(1/n) when s ≫dx.\nBut, these ﬁndings are in speciﬁc cases of regression, classiﬁcation, independent observations and with an\ninput space X = [0, 1]dx. The results obtained here are in a general setting and for a large class of dependent\nprocesses.\n5\nProofs of the main results\n5.1\nProof of Theorem 3.2\nFirst of all, remark the parallelization property for two networks of the same depth: for any d1, d2 ∈N and\nL, N1, N2, S1, S2, B1, B2 > 0,\nh1 ∈Hσ,dx,d1(L, N1, S1, B1), h2 ∈Hσ,dx,d2(L, N2, S2, B2) ⇒(h1, h2) ∈Hσ,dx,d1+d2(L, N1+N2, S1+S2, B1+B2).\nThus, it suﬃces to prove the theorem with dy = 1. To do this, we split the proof into two parts, as in the\nproof of Theorem 1 in [21]. Let s, K, ǫ > 0 and h ∈Cs,K(X). Without loss of generality, we assume that ǫ < 1\nand that, σ ﬁxes the segment I = [1/4, 3/4].\n(1) Assume that X ⊂[1/4, 3/4]dx.\n(i) Case of piecewise linear activation functions.\nLet ρ be the ReLU activation function. From Theorem 1 in [21], for any m ≥1, M ≥max{5dx, (s + 1)dx, (K +\n1)edx}, there exists a network eh1 ∈Hρ,dx,1(L1, N1, S1, 1) with L1 = 9 + (m + 5)(1 + ⌈log2 max(dx, s)⌉),\nN1 = 6(dx + ⌈s⌉)M and S1 ≤142(dx + s + 1)3+dxM(m + 6), such that,\n∥eh1 −h∥∞,X ≤(2KRs + 1)(1 + d2\nx + s2)6dxM2−m + K(9R)sM −s/dx.\n(5.1)\nFrom Lemma A1 of [18], there exists a network, with the activation function σ, that produces the same output\nof this ReLU neural network; that is, there exists eh2 ∈Hσ,dx,1(L2, N2, S2, B2) satisfying,\n∥eh1 −eh2∥∞,X = 0,\nwith L2 = L1, N2 = 2N1, S2 = 4S1 + 2L1N1 + 1 and for some B2 > 0 depending on σ. Therefore, we get from\n(5.1)\n∥h −eh2∥∞,X ≤(2KRs + 1)(1 + d2\nx + s2)6dxM2−m + K(9R)sM −s/dx.\n(5.2)\nBy taking M = max\nn\n5dx, (s + 1)dx, (K + 1)edx, (9R)dx(2K)dx/sǫ−dx/so\nand m = max\nn\nlog2\n\u0010\n2(2KRs + 1)(1 +\nd2\nx + s2)6dxMǫ−1\u0011\n, 1\no\nin (5.2), we get\n∥h −eh2∥∞,X ≤ǫ.\n(5.3)\nFor these choices of M and m, we have, M ≤5dx + (s + 1)dx + (K + 1)edx + 9dx(2K)dx/sǫ−dx/s and m ≤\n1 + log2\n\u0010\n2(K + 1)(1 + d2\nx + s2)6dxMǫ−1\u0011\n. Hence, one can easily see that:\nL2 = L1 = 9 + (m + 5)(1 + ⌈log2 max(dx, s)⌉) ≤L0 log(1/ǫ), N2 = 2N1 = 12(dx + ⌈s⌉)M ≤N0ǫ−dx/s,\nS2 = 4S1 + 2L1N1 + 1 ≤568(dx + s + 1)3+dxM(m + 6) + 2L0\n\u0000 log(1/ǫ)\u0001N0ǫ−dx/s + 1 ≤S0ǫ−dx/s log(1/ǫ),\nW. Kengne\n9\nand B2 ≤B0ǫ−4(dx/s+1) for some L0, N0, S0, B0 > 0 depending on dx, s, K, R and σ. Thus, the result holds\nfor piecewise linear activation functions and when X ⊂[1/4, 3/4]dx.\n(ii) Case of locally quadratic activation functions.\nLet M ∈N. Consider the grid points with length 1/M inside the dx-dimensional unit hypercube [0, 1]dx,\nGdx,M :=\nn 1\nM(m1, · · · , mdx), mj ∈{0, 1, · · ·M}, j = 1, · · · , dx\no\n.\n(5.4)\nFor all z ∈X ∩Gdx,M, denote by P s\nz,Mh(·) the ⌊s⌋-th order Taylor polynomial of h around zzz, that is\nP s\nz h(x) =\nX\nβ∈Ndx\n0 ,|β|≤s\n(∂f)(z)(x −z)β\nβ!\n.\n(5.5)\nWhen z ∈[0, 1]dx \\X, set z∗=\nargmin\nez∈argminu∈X∩Gdx,M(∥z−u∥)\n∥ez∥. One can easily check that, zzz∗exists and is unique.\nTherefore, set\nP s\nz h(x) = P s\nz∗h(x).\n(5.6)\nDeﬁne for all x = (x1, . . . , xdx)′, z = (z1, . . . , zdx)′ ∈Gdx,M,\nP sh(x) =\nX\nz∈Gdx,M\nP s\nz h(x)\ndx\nY\nj=1\n\u00001 −M|xj −zj|\u0001\n+.\n(5.7)\nFor M ≥4, we get from the proof of Theorem 1 in [21],\n∥P sh −h∥∞,X ≤K3sM−s.\n(5.8)\nAccording to the proof of Theorem 1 in [18], for some L3, N3, S3, B3 > 0, one can ﬁnd a neural network,\neh3 ∈Hσ,dx,1\n\u0010\nL3 log(1/ǫ), N3ǫ−dx/s, S3ǫ−dx/s log(1/ǫ), B3ǫ−4(dx/s+1)\u0011\nsuch that,\n∥P sh −eh3∥∞,X ≤C1ǫ for some C1 > 0.\n(5.9)\nBy taking M = [ǫ−1/s], the result follows from (5.8) and (5.9).\n(2) The general case of any compact set X.\nAs in the proof of Theorem 1 in [21], deﬁne the aﬃne map,\nT : Rdx →Rdx, x 7→x/R + (1/2, . . ., 1/2)T,\n(5.10)\nwhere R is deﬁned in (2.1). By setting X ′ := T (X), one can see that X ′ ⊆[1/4, 3/4]dx. Also, one can easily\nverify that, since h ∈Cs,K(X), then, g := h(T −1) ∈Cs,RsK(X ′). Therefore, let us apply the result of the ﬁrst\npart to g ∈Cs,RsK(X ′). So, for some L0, N0, S0, B0 > 0 depending only on dx, s, K and R, there exists a neural\nnetwork eg ∈Hσ,dx,1(L0 log+(1/ǫ), N0ǫ−dx/s, S0ǫ−dx/s log+(1/ǫ), B0ǫ−4(dx/s+1)) satisfying,\n∥g −eg∥∞,X ′ ≤ǫ.\n(5.11)\nNow, consider the neural network,\n10\nExcess risk bound for deep learning under weak dependence\neh := eg ◦σ0 ◦T, where σ0(z1 . . . , zdx) =\n\u0000σ(z1), . . . , σ(zdx)\n\u0001\nfor all (z1 . . . , zdx) ∈Rdx.\n(5.12)\nSince T is an aﬃne transformation, the neural network eh is obtained from eg by adding one hidden layer, which\nalso adds 2dx non-zero parameters. That is,\neh ∈Hσ,dx,1\n\u0010\n(L0 + 1) log+(1/ǫ), N0ǫ−dx/s, (S0 + 2dx)ǫ−dx/s log+(1/ǫ), max(B0, 1)ǫ−4(dx/s+1)\u0011\n.\n(5.13)\nAlso, T (X) = X ′ ⊆[1/4, 3/4] and, since this set is ﬁxed by σ, we get from (5.11),\n∥h −eh∥∞,X = ∥g ◦T −eg ◦σ0 ◦T ∥∞,X ≤∥g −eg∥∞,X ′ ≤ǫ.\n(5.14)\nThus, the result holds from (5.13) and (5.14). This completes the proof of the theorem.\n■\n5.2\nProof of Theorem 4.1\nLet Ln, Nn, Sn, Bn, Fn > 0 and consider the DNN’s class Hσ,n = Hσ,dx,1(Ln, Nn, Sn, Bn, Fn) ⊂F(X, Y).\nUnder (4.4), (A5) and the decomposition (4.3), we get,\nR(bhn) −\ninf\nh∈F(X ,Y)R(h) = R(bhn) −R(h∗) =\n\u0010\nR(bhn) −R(hHσ,n)\n\u0011\n+\n\u0010\nR(hHσ,n) −R(h∗)\n\u0011\n.\n(5.15)\nLet η, ν ∈(0, 1) and α > 2 + dx/s. Let us deal ﬁrst with the approximation error. Observe that, from the\nLipschitz property of ℓ, it holds for all h ∈Hσ,n, that,\nR(h) −R(h∗) = EZ0\n\u0002ℓ\u0000h(X0), Y0\n\u0001\u0003 −EZ0\n\u0002ℓ\u0000h∗(X0), Y0\n\u0001\u0003 ≤KℓEX0|h(X0) −h∗(X0)|.\nHence, we get,\nR(hHσ,n) −R(h∗) =\ninf\nh∈Hσ,nR(h) −R(h∗) =\ninf\nh∈Hσ,n\n\u0010\nR(h) −R(h∗)\n\u0011\n≤Kℓ\ninf\nh∈Hσ,n\n\u0010\nEX0\n\f\fh(X0) −h∗(X0)\n\f\f\n\u0011\n(5.16)\nSince h∗∈Cs,K(X), for ǫ = n−1/α in Theorem 3.2, one can ﬁnd positive constants L0, N0, S0, B0 such that,\nwith\nLn = L0\nα log n, Nn = N0ndx/(sα), Sn = S0\nα ndx/(sα) log n and Bn = B0n4(dx/s+1)/α\n(5.17)\nthere is a neural network hn ∈Hσ,n = Hσ,dx,1(Ln, Nn, Sn, Bn, Fn) satisfying,\n∥hn −h∗∥∞,X ≤\n1\nn1/α .\nTherefore, in addition to (5.16), it holds that,\nR(hHσ,n) −R(h∗) ≤\nKℓ\nn1/α .\n(5.18)\nLet us consider now the estimation error over the class Hσ,n = Hσ,dx,1(Ln, Nn, Sn, Bn, Fn) where Ln, Nn, Sn, Bn\nare given in (5.17). Under (A1)-(A3), one can ﬁnd a constant Cn,1 > 0 (see [11]) such that, for all h ∈Hσ,n,\nE\nh\u0010\nn\nX\ni=1\n\u0010\nℓ(h(Xi), Yi) −E[ℓ(h(X0), Y0)]\n\u0011\u00112i\n≤Cn,1n.\n(5.19)\nW. Kengne\n11\nWith the assumption α > 2 + dx/s, one can easily see that the condition (4.7) of Theorem 4.3 in [12] holds.\nFrom this theorem, there exists a constant C1 > 0 and en0 depending on Kℓ, Cσ, dx, η, ν, α, s such that, for any\nn ≥en0 and with probability at least 1 −η, the estimation error satisﬁes,\nR(bhn) −R(hHσ,n) ≤2Mn\nn1/α +\n \nlog(2C1 log n/η)\nCn,2\n!1/2\n,\n(5.20)\nwhere,\nCn,2 =\nn2\nnCn,1 + log n nν−1/4(2Mn)ν/Cn,1\n,\n(5.21)\nwith Cn,1 deﬁned in (5.19). Thus, the result follows from (5.15), (5.18) and (5.20). This completes the proof\nof the theorem.\n■\nReferences\n[1] Bauer, B., and Kohler, M. On deep learning as a remedy for the curse of dimensionality in nonpara-\nmetric regression. The Annals of Statistics 47, 4 (2019), 2261 – 2285.\n[2] Carlile, B., Delamarter, G., Kinney, P., Marti, A., and Whitney, B. Improving deep learning\nby inverse square root linear units (isrlus). arXiv preprint arXiv:1710.09967 (2017).\n[3] Clevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast and accurate deep network learning\nby exponential linear units (elus). arXiv preprint arXiv:1511.07289 (2015).\n[4] Dedecker, J., Doukhan, P., Lang, G., Le´on, J. R., Louhichi, S., and Prieur, C.\nWeak\ndependence: With Examples and Applications. Lecture Notes in Statistics 190, Springer-Verlag, New\nYork, 2007.\n[5] Diop, M. L., and Kengne, W. A general procedure for change-point detection in multivariate time\nseries. TEST (2022), 1–33.\n[6] Diop, M. L., and Kengne, W.\nInference and model selection in general causal time series with\nexogenous covariates. Electronic Journal of Statistics 16, 1 (2022), 116–157.\n[7] Diop, M. L., and Kengne, W. Statistical learning for ψ-weakly dependent processes. arXiv preprint\narXiv:2210.00088 (2022).\n[8] Doukhan, P., and Louhichi, S.\nA new weak dependence condition and applications to moment\ninequalities. Stochastic processes and their applications 84, 2 (1999), 313–342.\n[9] Ern, A., and Guermond, J.-L. Theory and practice of ﬁnite elements, vol. 159. Springer, 2004.\n[10] Francq, C., et al. Qml inference for volatility models with covariates. Econometric Theory 35, 1\n(2019), 37–72.\n[11] Hwang, E., and Shin, D. W. A study on moment inequalities under a weak dependence. Journal of\nthe Korean Statistical Society 42, 1 (2013), 133–141.\n12\nExcess risk bound for deep learning under weak dependence\n[12] Kengne, W., and Modou, W.\nDeep learning for ψ-weakly dependent processes.\narXiv preprint\narXiv:2302.00333 (2023).\n[13] Kim, Y., Ohn, I., and Kim, D. Fast convergence rates of deep neural networks for classiﬁcation. Neural\nNetworks 138 (2021), 179–197.\n[14] Kohler, M., and Krzyzak, A. On the rate of convergence of a deep recurrent neural network estimate\nin a regression problem with dependent data. arXiv preprint arXiv:2011.00328 (2020).\n[15] Kurisu, D., Fukami, R., and Koike, Y. Adaptive deep learning for nonparametric time series regres-\nsion. arXiv preprint arXiv:2207.02546 (2022).\n[16] Lin, G., and Shen, W. Research on convolutional neural network based on improved relu piecewise\nactivation function. Procedia computer science 131 (2018), 977–984.\n[17] Ma, M., and Safikhani, A. Theoretical analysis of deep neural networks for temporally dependent\nobservations. arXiv preprint arXiv:2210.11530 (2022).\n[18] Ohn, I., and Kim, Y. Smooth function approximation by deep neural networks with general activation\nfunctions. Entropy 21, 7 (2019), 627.\n[19] Ohn, I., and Kim, Y. Nonconvex sparse regularization for deep neural networks and its optimality.\nNeural Computation 34, 2 (2022), 476–517.\n[20] Petersen, P., and Voigtlaender, F. Optimal approximation of piecewise smooth functions using\ndeep relu neural networks. Neural Networks 108 (2018), 296–330.\n[21] Schmidt-Hieber, J.\nDeep relu network approximation of functions on a manifold.\narXiv preprint\narXiv:1908.00695 (2019).\n[22] Schmidt-Hieber, J. Nonparametric regression using deep neural networks with relu activation function.\nThe Annals of Statistics (2020), 1875 – 1897.\n[23] Suzuki, T. Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal\nrate and curse of dimensionality. arXiv preprint arXiv:1810.08033 (2018).\n[24] Triebel, H. Theory of function spaces II, vol. 84 of Monogr. Math., Basel. Basel: Birkh¨auser Verlag,\n1992.\n[25] Yarotsky, D. Error bounds for approximations with deep relu networks. Neural Networks 94 (2017),\n103–114.\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2023-02-15",
  "updated": "2023-02-15"
}