{
  "id": "http://arxiv.org/abs/2302.08969v1",
  "title": "Deep Reinforcement Learning for mmWave Initial Beam Alignment",
  "authors": [
    "Daniel Tandler",
    "Sebastian Dörner",
    "Marc Gauger",
    "Stephan ten Brink"
  ],
  "abstract": "We investigate the applicability of deep reinforcement learning algorithms to\nthe adaptive initial access beam alignment problem for mmWave communications\nusing the state-of-the-art proximal policy optimization algorithm as an\nexample. In comparison to recent unsupervised learning based approaches\ndeveloped to tackle this problem, deep reinforcement learning has the potential\nto address a new and wider range of applications, since, in principle, no\n(differentiable) model of the channel and/or the whole system is required for\ntraining, and only agent-environment interactions are necessary to learn an\nalgorithm (be it online or using a recorded dataset). We show that, although\nthe chosen off-the-shelf deep reinforcement learning agent fails to perform\nwell when trained on realistic problem sizes, introducing action space shaping\nin the form of beamforming modules vastly improves the performance, without\nsacrificing much generalizability. Using this add-on, the agent is able to\ndeliver competitive performance to various state-of-the-art methods on\nsimulated environments, even under realistic problem sizes. This demonstrates\nthat through well-directed modification, deep reinforcement learning may have a\nchance to compete with other approaches in this area, opening up many\nstraightforward extensions to other/similar scenarios.",
  "text": "Deep Reinforcement Learning\nfor mmWave Initial Beam Alignment\nDaniel Tandler, Sebastian D¨orner, Marc Gauger and Stephan ten Brink\nInstitute of Telecommunications, University of Stuttgart, Pfaffenwaldring 47, 70659 Stuttgart, Germany\n{tandler,doerner,gauger,tenbrink}@inue.uni-stuttgart.de\nAbstract—We investigate the applicability of deep reinforce-\nment learning algorithms to the adaptive initial access beam\nalignment problem for mmWave communications using the\nstate-of-the-art proximal policy optimization algorithm as an\nexample. In comparison to recent unsupervised learning based\napproaches developed to tackle this problem, deep reinforcement\nlearning has the potential to address a new and wider range\nof applications, since, in principle, no (differentiable) model of\nthe channel and/or the whole system is required for training,\nand only agent-environment interactions are necessary to learn\nan algorithm (be it online or using a recorded dataset). We\nshow that, although the chosen off-the-shelf deep reinforcement\nlearning agent fails to perform well when trained on realistic\nproblem sizes, introducing action space shaping in the form of\nbeamforming modules vastly improves the performance, without\nsacriﬁcing much generalizability. Using this add-on, the agent is\nable to deliver competitive performance to various state-of-the-art\nmethods on simulated environments, even under realistic problem\nsizes. This demonstrates that through well-directed modiﬁcation,\ndeep reinforcement learning may have a chance to compete with\nother approaches in this area, opening up many straightforward\nextensions to other/similar scenarios.\nI. INTRODUCTION\nMmWave communication systems are usually equipped\nwith large antenna arrays to enable highly directional trans-\nmissions using beamforming techniques to overcome the large\npathloss at these frequencies. This large number of antennas\npaired with the usage of hybrid or analog beamforming to limit\ncost and power consumption raises the need of new initial\naccess (IA) strategies for mmmWave systems, often referred\nto as beam training or beam alignment (BA). Most of the\nexisting work done on BA relies on using beam codebooks,\ni.e. employing a predeﬁned set of beampatterns, together\nwith methods like compressed sensing, Bayesian approaches\nand machine learning. Recently, it was demonstrated that not\nusing predeﬁned codebooks together with unsupervised deep\nlearning may offer performance improvements compared to\ncodebook-based approaches [1].\nThere has been a great amount of work concerning the\nmmWave BA problem using codebooks of predeﬁned beam-\npatterns and mostly classical methods, like in [2], where a\nhierarchical codebook was designed and used in conjunction\nwith compressed sensing methods to perform channel state\ninformation (CSI) estimation followed by precoder/combiner\ndesign using this estimate. In [3] the power-based bisection\nsearch applied in [2] is enhanced to a noisy generalized\nh = PL\nl=1 αla(φl)\nUE\nRFC\nwt = f(at)\nDRL\nAgent\nNRX\nyt ∈C\nat ∈RNA\nwt ∈CNRX\nCombiner\nwt ∈CNRX\nBS\n∈CNRX\n...\n+\n+\nnNRX\nn1\nFig. 1: System model of the DRL-based BA algorithm. Note how at\neach timestep t, the DRL agent receives one complex-valued symbol\nyt from the receiver and emits one real valued action vector at.\nform by making use of the Bayes posterior. The work in [4]\nintroduced AgileLink, a fast mmWave BA algorithm using\nhashing of random direction to determine the best beam\nalignment in a logarithmic number of measurements. Other\napproaches like [5] and [6] reformulate the mmWave BA\nproblem into a multi armed-bandit problem and solve it using\nmethods developed for this setting. Another line of research\nuses deep reinforcement learning (DRL) to address various\nrelated problems in the area of mmWave communications,\nlike [7] where a special DRL structure is proposed to perform\nµBS selection and angle-based BA. [8] proposes a method\nto learn and adapt specialized beam codebooks using DRL\nwithout requiring any explicit channel knowledge. The works\nconcerning the IA BA problem often have in common that\nthey choose the beams out of predeﬁned codebooks in some\nway or the other. In [1] it was shown that dense neural\nnetwork (DNN)-based codebook-free BA algorithms may be\nable to outperform their codebook based counterparts. Finally,\n[9] shows that recurrent neural networks (RNNs) together\nwith end-to-end learning are able to outperform state-of-the-art\nBA methods in the codebook free (adaptive) beam alignment\nproblem. [9] can be regarded as being the most relevant for\nthis work as both share the same setup, optimization goals and\ntrain the same RNN-based agent. Yet, our approach differs in\nthe way the RNN agent is trained: While [9] trains the agent\nin a end-to-end fashion requiring a speciﬁc channel model,\narXiv:2302.08969v1  [cs.IT]  17 Feb 2023\nin this work the agent is trained using the DRL framework,\nwhere the environment is treated as a black box. This means\nthat our DRL-based approach could in theory also be trained\ndirectly in deployed systems with various unknown hardware\nimpairments. Fig. 1 depicts the system under consideration.\nThe main contributions of this work are as follows:\n• We demonstrate the difﬁculties that arise when applying\noff-the-shelf state-of-the-art DRL algorithms to the prob-\nlem of adaptive IA BA with the example of the proximal\npolicy optimization (PPO) [10] algorithm\n• We propose a modiﬁcation of the DRL algorithm by\nintroducing beamforming modules to address these dif-\nﬁculties, which can in principle be applied to all other\ncontinuous-valued action space DRL algorithms\n• We show that by using this modiﬁcation, the PPO algo-\nrithm performs well even under realistic system sizes\nWe want to emphasize that our objective is not to outperform\nthe model-based end-to-end trained state-of-the-art methods\nof [1] and [9] (which would be quite unrealistic since we\nessentially try to approximate the training gradient of these\nmethods with the DRL framework), but to show that the\nDRL approach can also deliver comparably good performance\nwithout the explicit need for any knowledge or model of the\nmmWave channel during training.\nThis paper is organized as follows: Section II describes the\nsystem model and gives some information about the basics of\nDRL. In Section III, we propose a way to reformulate the BA\nproblem into a partially observable Markov decision process\n(POMDP) and discuss possible mappings of the action space\nof the DRL agent to the space of combining vectors. Following\nthis, Section IV gives some numerical results and compares\nthe performance of the DRL approach to various baselines.\nFinally, Section V concludes the paper.\nII. PRELIMINARIES\nIn this paper, we consider the problem of one-sided beam\nalignment in mmWave communications, i.e., the initial (up-\nlink) communication between a basestation (BS) equipped\nwith an uniform linar array (ULA) consisting of NRX antennas\nand a single user equipment (UE) with a single antenna\nsending a constant signal omnidirectionally. Note that in this\nsetup, no initial knowledge about the channel between BS and\nUE is assumed. In the rest of this paper, we assume that the\nBS is only equipped with a single radio frequency (RF) chain,\ni.e., analog beamforming and that the transmission channel\nbetween UE and BS is static, i.e. constant during each run of\nthe BA algorithm. Also, the BS has T −1 discrete time steps\nto probe the channel between itself and the UE with analog\nprobing beams wt (0 ≤t < T) to gain as much information\nabout the channel as possible, after which the BS has to use\nthis gained information to output the estimated best combining\nvector maximizing the beamforming gain ∥wHh∥2\n2. As shown\nin [9] this problem can be regarded as an active sensing prob-\nlem in which the design of the t−th probing beam depends\non all previous timesteps, i.e., wt = G(w0≤t′≤t−1, y0≤t′≤t−1)\nwith received symbols yt and nonlinear map G. The rest of\nthis section gives a detailed description of the system model\nand the speciﬁc setting considered.\nA. System Model\nSince the mmWave channel can be regarded as being\nspatially sparse, we assume the widely used geometric channel\nmodel [11] for the mmWave channel description. As we\nconsider one sided BA, the channel can be described with\na single vector, i.e.,\nh =\nL−1\nX\nl=0\nαla(φl) ∈CNRX\n(1)\nfor a channel with L paths, complex path gain αl ∼CN(0, 1)\nand angle-of-arrival (AoA) φl ∼U[−60◦, 60◦] of path l. a(φ)\ndescribes the array response of the ULA for angle φ and is\ngiven by\na(φ) =\n1\n√\nN\n(1, ejπ sin(φ), . . . , ejπ(N−1) sin(φ)) ∈CN\n(2)\nwith N antennas, and under the assumption of an element\nspacing of λ\n2 . As it is assumed that the BS uses only one RF\nchain and the UE sends a constant stream of pilot symbols with\nnormalized power PTX = 1, the whole system at timestep t\ncan be described by\nyt = wH\nt h + wH\nt nt ∈C\n(3)\nwith yt being the complex-valued received symbol at the\nBS, wt being the NRX dimensional complex-valued analog\ncombining vector and noise vector nt ∼CN(0, σ2\nnINRX), all\nfor timestep t. The per-antenna signal-to-noise-ratio (SNR)\nis deﬁned as SNR =\n1\nσ2n . The whole system model is also\ndepicted in Fig. 1.\nB. Deep Reinforcement Learning\nIn this section, we provide a short overview over the\nbasics of DRL. The reinforcement learning (RL) framework\nconsist of an abstract agent (sequentially) interacting with its\nenvironment to fulﬁll a certain task. Formally, this process\ncan be modelled using a Markov decision process (MDP) M\nwhich can be expressed by the 4−tuple (S, A, T, R), where S\ndenotes the set of all possible environment states, A is the set\nof all possible actions the agent can take, T the transition\nprobability function of the environment and R the reward\nprobability function. In this formalism, at time step t, the\nagent observes the environment state s ∈S, executes the\naction a ∈A, after which the environment emits the scalar\nreward r ∼R(r|s, a), and transitions to a new environment\nstate s′ ∼T(s′|s, a) according to transition function T, and\nthe step counter increases by 1. POMDPs, can be seen as a\ngeneralization of MDPs to scenarios where the agent does not\nreceive the true environment state s′ after executing action\na, but only an (possibly noisy) observation o of said state,\no ∼O(o|s′, a) with conditional observation probability O.\nNote that other quantities of POMDPs behave exactly like they\ndo in MDPs. Due to only receiving partial information about\nits environment in the form of observations o, the agent is\nforced to act under uncertainty over its environment, which\ncan be represented using so called belief states. The goal\nof an RL algorithm in an MDP is now to ﬁnd a policy\nπ(a|s) (i.e., a potentially probabilistic mapping from states to\nactions) which maximizes the expected long-term cumulative\ndiscounted reward, deﬁned as\nJ(π) = Es0,π,T\n\" ∞\nX\nt=0\nγtr(st, at)\n#\n(4)\nwith initial state distribution s0 ∼p0, when following policy\nπ and with discount factor γ ∈[0, 1] which determines how\nimportant short term rewards are versus long term rewards.\nOut of the vast family of different DRL algorithms, the PPO\nalgorithm was chosen for this work, a state-of-the art model-\nfree on-policy algorithm. It belongs to the class of policy\ngradient algorithms which directly try to optimize the policy\nby estimating the gradient of the reward with respect to the\npolicy parameters. Since this algorithm has the on-policy\nproperty, it needs to interact continuously with its environment\nin order to improve its policy. The reason for choosing this\nalgorithm lies in its demonstrated robustness against hyper-\nparameter variations while still offering good performance.\nAlso, as it was shown that DRL algorithms combined with\na memory network in the form of an long short-term memory\n(LSTM) network may be able to outperform their memory-\nless counterparts in POMDPs [12], the chosen PPO algorithm\nwas modiﬁed in a similar way as described in [12]. For the\nimplementation in this paper, we chose gated recurrent unit\n(GRU) cells over LSTM cells for the memory network due to\ntheir lower complexity and ease of implementation.\nIII. APPLYING DRL TO MMWAVE BEAM ALIGNMENT\nThis section goes into more depth on how to combine the\naforementioned RL framework with the problem of BA. The\nﬁrst task is to reformulate the BA problem as a MDP or\nPOMDP in order to enable the application of RL algorithms.\nLuckily, the problem formulation offers itself very naturally\nto be expressed in the form of a POMDP. While there are\nmultiple ways to deﬁne said POMDP, one direct way is:\n• S: The set of possible channel parameters, constant over\none run of the BA algorithm\n• A: To be speciﬁed\n• T: T(s′|s, a) =\n\u001a\n1,\ns′ = s\n0,\nelse\n.\n• R: R(r|s, a) =\n(\n∥wH\nT −1h∥2\n2\n∥h∥2\n2\n,\nt = T −1\n0,\nelse\n.\n• Ω: The set of possible (noisy) observations y: Ω⊆C\n• O: The set of conditional observation probabilities: y ∼\nO(y|s, a, n)\n• γ: Set to 1, i.e., an undiscounted POMDP\nNote that the states of this POMDP are implicitly given with h,\nand the actions a with w. Also, in order to use standard neural\nnetwork (NN) frameworks, the complex valued observations\nNeural Network:\nat = f(θ, ot, ht−1)\nDRL Agent\nTransmit Symbol:\nsample nt\nyt = wH\nt h + wH\nt nt\nProcess Action:\nwt = f(at)\nCalculate Reward and Observation:\nrt = ∥wH\nt h∥2\n2\n∥h∥2\n2\nif t = T −1, 0 else\not+1 = yt\nEnvironment\naction\nat\nobservation\not\nobservation\not+1\nreward\nrt\nwt\nwt, yt\nht−1\nFig. 2: Agent-environment interaction for the partially observable\nMarkov decision problem formulation of the beam alignment task.\ny are converted to real numbers by interpreting them as 2\ndimensional real valued vectors.\nThe agent-environment interaction for this POMDP is also\nshown in Fig. 2. Note, that the reward function results in a\nreward signal which can be regarded as being sparse and,\nthus, renders the task more difﬁcult to learn for a RL agent\n[13]. Improving this reward function using reward shaping\nby introducing a potential-based shaping function [14] (e.g.\nthe information of the agent about the CSI) for example may\nfurther improve performance and is left for future research.\nNow, the last thing open to deﬁne is the agent’s action\nspace A and the mapping of its actions at to the respective\ncombining vectors wt, i.e., determining a possibly highly\nnonlinear function\nwt = f(at) ∈CNRX\n(5)\nand f : A →Wfeasible with A ⊆RNA, action space dimension\nNA and Wfeasible ⊆CNRX (also depicted in Fig. 1). As the\ncomplex-valued combining vectors wt often have to obey\ncertain constraints (unit norm and/or unit modulus constraint in\nfully analogue beamforming, for example), ﬁnding the optimal\nmap f from the real valued action space of the agent to the\ncomplex space of feasible combining vectors Wfeasible is not\nan easy task. Since the action space directly inﬂuences the\ndifﬁculty and complexity of the learning task for the DRL\nagent, ideally one wants to keep the action space and its\nstructure as simple as possible. On the other hand, there is\na certain subset of (largely unknown) required combining\nvectors Wrequired ⊆Wfeasible which are necessary for any BA\nalgorithm to perform well, and f needs to cover/parameterize\n(like highly directional beams to achieve high beamforming\ngain). This greatly increases the difﬁculty of designing f as one\nessentially needs to strike a balance of a simple structure of\nA while ensuring good coverage of Wrequired. In the following,\ntwo different implementations for f under the constraint that\n||w||2 = 1 are proposed and described in more detail. Note\nthat for many cases, both implementations can be easily\nextended to consider more severe and realistic constraints on\nw.\nA. Direct Map\nThis map describes the direct mapping of the actions of\nthe agent to the combining vectors. Here, the actions at of\nthe agent at each timestep t, are assumed to be real-valued\n2NRX-dimensional vectors, i.e., at ∈R2NRX and can be seen as\nthe unscaled, real-valued representation of the corresponding\nnormalized combining vector wt. Therefore, these actions are\nthen mapped to the complex domain via\nwunscaled,t = (a[0:NRX],t + ja[NRX:2NRX],t) ∈CNRX\n(6)\nfollowed by normalization, i.e.,\nwt =\nwunscaled,t\n∥wunscaled,t∥2\n(7)\nwith the notation x[0:R] representing the R-dimensional vector\nconsisting of the the ﬁrst R elements of x.\nThe advantage of this mapping lies in its ability to reach all\npossible combining vectors, i.e., it covers all of Wfeasible. This\nhowever comes at the cost of a highly complex structure of\nA together with a linear scaling of NA by the number of\nantenna elements NRX. As shown in the results section, these\ndrawbacks seem to pose a challenge too great to overcome for\nthe DRL agent, causing it to fail to learn when the problem\nsizes reach practically relevant dimensions.\nB. Beamforming Map\nThe second map (in the following referred to as ”beamform-\ning map” or ”beamforming module”) tries to address the prob-\nlems of the direct map in the following way: The foundation\nis the observation that BA algorithms employing hierarchical\ncodebooks show good performance [2] [3], i.e., covering much\nof Wrequired, while the beams of these codebooks are well\ndeﬁned by their angular direction αbeam ∈[−π\n2 , π\n2 ] and angular\nbeam width 2 · βbeam with βbeam ∈(0, π\n2 ], i.e., by only two\nreal valued parameters. Thus, the idea is to generate these\nbeampatterns (speciﬁed by αbeam and βbeam) “on the ﬂy”, i.e.,\nat each timestep t of the BA algorithm, the agent inputs\na speciﬁc αbeam,t and βbeam,t into the beamforming module\nwhich then returns the beampattern (wt) speciﬁed by these\ninputs, thereby drastically reducing NA from 2NRX to 2.\nAs the generation algorithms for the respective codebooks\nhowever require extensive and lengthy numerical optimization\nfor each beampattern, the continuation of this idea is to skip\nthis online optimization process by using a pretrained DNN\ninstead, resulting in a parameterized mapping with weights Θ,\ni.e., f is now a parameterized function fΘ and\nw = fΘ(αbeam, βbeam)\n(8)\nThis DNN representing fΘ is a simple feedforward NN with\n2 input units and 2NRX output units. In the following, it is\nimplicitly assumed that the 2NRX dimensional real valued out-\nput of the DNN is mapped to normalized complex combining\nvectors as in III-A and that this step is part of the beamforming\nmodule. From a high-level perspective, training of this DNN\ncan be done by inputting random input pairs (αbeam, βbeam) into\nthe DNN and then adjusting its weights such that its produced\nbeampatterns match the speciﬁcation of its input as close as\npossible, e.g. by maximizing the received power inside the\nangular range [αbeam −βbeam, αbeam +βbeam] and minimizing it\noutside. Now, a more detailed description follows.\nA random input pair (αbeam, βbeam), αbeam ∼U[−π\n2 , π\n2 ] and\nβbeam ∼(0, βmax] with βmax = min( π\n2 , | π\n2 −|αbeam||) is\ndrawn uniformly. Then, the following is done: First, the\nintervals Uinside = [−π, π]∪[2(αbeam−βbeam), 2(αbeam+βbeam)]\nand Uoutside = [−π, π]\\Uinside are determined. After this, K\nrandom angles are uniformly sampled from Uinside and Uoutside\nrespectively (resulting in the sets Sinside and Soutside), i.e.,\nsampling uniformly in the Ψ - space [15]. Then, (αbeam, βbeam)\nis fed into the DNN and the combining vector w is obtained\nfrom its output. Following this, the weights of the DNN are\noptimized via stochastic gradient descent (averaged over B\ninput samples) to minimize the following objective:\nJ(Θ) = −Einside[g]+Eoutside[g]+Varoutside(g)+ϵ·Varinside(g)\n(9)\nwith g = ∥wHaΨ(θ)∥2 (aΨ(φ) denotes a modiﬁed (2) where\nthe π sin(φ)-terms are replaced with φ) and with the subscripts\ninside and outside indicating that the expectation is taken over\nall angles θ from Sinside and Soutside respectively. The variance\nterms in (9) are used as regularizers to generate more smooth\nbeampatters with ϵ ≥0 being a hyperparameter to control the\nripple inside the beampattern’s angular coverage range. Fig 3\n(bottom) shows the resulting beampatterns when generating a\nset of 8 beams similar to a simple equal beam-width codebook\nwith 8 entries, using a trained DNN beamforming module,\nassuming NRX = 32, batch size = 1000, K = 1000, ϵ = 1,\n5000 parameter updates and using a 3 Layer DNN with 128\nunits in the ﬁrst two layers (25K parameters in total). Plotted\nin Fig. 3 is the reference gain, deﬁned as [15]:\nG(θ, cq) = ∥cH\nq a(θ)∥2\n2\n(10)\nover all possible AoA θ ∈[−π\n2 , π\n2 ] and where each codeword\ncq represents a speciﬁc combining vector wq (note that here\nthe index q refers to the index in the codeword and not a\ntimestep). For example, c4 in Fig. 3 (bottom) was generated\nby the beamforming module by inputting αbeam = −11.25 and\nβbeam = 22.5. For comparison, the top of the ﬁgure displays\nbeampatterns from a codebook with 8 entries generated with\na classical numerical optimization approach described in [15]\nwith algorithm parameters N = 3, M = 12, L = 300. While\nthe beampatterns of the beamforming map are not as sharp as\nthe ones generated using the algorithm of [15], they can be\ngenerated ”on-the-ﬂy” for arbitrary input combinations since\nmuch of the complexity of beampattern generation is ofﬂoaded\nto the training phase of the DNN. Also, the beamforming\nmap is inherently differentiable due to its NN-based structure,\nmeaning one can exchange the feedforward-NN used in the\napproach of [9] with this pretrained beamforming map and\nfreeze its parameters in order to gain insights about the\n−50\n0\n50\n0\n0.1\n0.2\nAngle θ [◦]\nReference gain G(θ, cq)\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\n−50\n0\n50\n0\n0.1\n0.2\nAngle θ [◦]\nReference gain G(θ, cq)\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nFig. 3: Top: Equal beam width codebook with 8 entries generated by\nthe algorithm described in [15], as reference. Bottom: Beampatterns\nwith the same properties generated using the proposed beamforming\nmap.\nTABLE I: Hyperparameters for architecture and training of the DRL\nagent\nParameter\nValue\nUnits per Layer\n128\nGRU Layers\n2\nFF Layers\n2\nEntropy Coefﬁcient\n0.001\nClip Coefﬁcient\n0.2\nγ\n1\nParameter\nValue\nMax. grad norm\n0.5\nTraining SNR\n20 dB\nBatch size\n2000\nOptimizer\nADAM\nLearning rate\n3 · 10−4\nValue Coefﬁcient\n0.5\nNumber of workers\n2000\nperformance limitations induced by the beamforming module.\nFurther, one can extend this idea in a straightforward fashion to\nother schemes, i.e., generating beams with multiple directions,\nand to other and different hardware restrictions.\nIV. NUMERICAL RESULTS\nIn this section we present numerical results for the DRL\nagent using the two different beamforming maps previously\nintroduced (in the following denoted as DRLDM and DRLBF).\nIn all experiments, the DRL agent was trained over the full-\nlength rollouts of the BA algorithm, i.e., T timesteps, using\nback-propagation through time (BPTT). Table I lists the most\nimportant hyperparameters used for our experiments. Also,\nthe number of parameters of all chosen NN-based approaches\nwere selected to be roughly equal for easier comparison. We\ncompare our results to the following ﬁve baselines:\n1) Maximum ratio combining (MRC)\nwith perfect CSI\n(MRCCSI): Under the assumption of full CSI knowledge\nat the receiver, this can be seen as the optimal scheme\nand thus represents an upper bound on the achievable\nbeamforming gain.\n2) MRC with estimated CSI (MRCOMP)1: Here, the CSI for\nthe MRC scheme is estimated using compressed sensing\n1The implementation of this approach was taken from the github repository\nof [9].\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6 ·105\n100\n101\nnum parameter updates\nBeamforming gain\nDRLDM NRX = 2\nDRLDM NRX = 6\nDRLDM NRX = 14\nRNNUNSUP[9] NRX = 14\nMRCCSI NRX = 14\nDRLBF (best) NRX = 32\nDRLBF (avg) NRX = 32\nFig. 4: Beamforming gain during training of the DRLDM agent using\nthe direct map for different values of NRX. Also plotted is the training\ntrajectory of a DRLBF agent with NRX = 32 antennas, exhibiting a\nmuch more favorable training curve than the DRLDM approaches even\nthough it uses more antenna elements. (avg) denotes the average taken\nover 5 identical agents trained with different random seeds and (best)\ndenotes the best performing one out of these 5 agents.\n(orthogonal matching pursuit (OMP)).\n3) RNN based unsupervised [9] (RNNUNSUP): This ap-\nproach trains a RNN in a unsupervised end-to-end\nfashion to maximize the beamforming gain at the last\ntimestep T −1.\n4) DNN non-adaptive unsupervised [16] (DNNUNSUP): In\nthis scheme, a DNN is trained to maximize the beam-\nforming gain at the last timestep T −1 after observing\nT −1 pilots obtained by sensing the channel using a\nsequence of ﬁxed, random sensing vectors.\n5) Exhaustive search based on a codebook generated as in\n[15].\nNote, that both RNNUNSUP and DNNUNSUP are trained in an\nunsupervised end-to-end fashion by maximizing the beam-\nforming gain, requiring an explicit channel model for opti-\nmization during training, which our approach does not need.\nFirst, the DRL approach using the direct map (DRLDM) is\ninvestigated. For this, a separate DRLDM agent is trained\nfor each different value of NRX, starting from 2 up to 14\nwith the running time T of the algorithm kept constant to\nT = 5, L = 1, with a total of 100K parameter updates.\nThe results can be seen in Fig. 4: Whereas the agent more or\nless successfully manages to learn a policy for the 2 antenna\ncase, even moderately increasing the amount of antennas\nquickly causes the learning to fail. Also note the loss of\nconvergence during training which may be mitigable through\nearly stopping or further hyperparameter tuning. Although\nsome light hyperparameter tuning for the DRL agent was\nattempted without much success, making deﬁnite statements\nabout the performance of DRL agents in general is not an easy\ntask, as also outlined in [17]. However, given how quickly the\nperformance of the agent declined when increasing NRX, the\nchances that one obtains good and working hyperparameters\n−10\n0\n10\n20\n100\n101\nSNR [dB]\nBeamforming gain\nRNNUNSUP[9]\nDNNUNSUP[16]\nMRCCSI\nMRCOMP\nExhaustive\nDRLBF (best)\nDRLBF (avg)\nFig. 5: Beamforming gain vs. SNR of the DRLBF agent and various\nother methods for T = 5, NRX = 32. Note that as expected, the\nDRLBF agent fails to match the performance of its unsupervised\ncounterparts, but it manages to outperform the MRCOMP in the high\nSNR regime.\nfor the direct map for realistic problem sizes seem very slim.\nNext, a new experiment is performed in which the perfor-\nmance of the DRL agent using the beamforming map (DRLBF)\nis more closely examined and compared to other approaches.\nFor this, a trained 3-layer (25K parameters) DNN beamform-\ning map is used and all hyperparameters of the NNs are kept\nthe same as in the previous experiment, T = 5 and NRX = 32.\nThe results are shown in Fig. 5. First one can observe that\nusing this beamforming map, the DRL agent is able to learn a\nBA policy for NRX = 32, which is quite remarkable in itself\ngiven the previous results using the direct map (the ﬁrst 150K\nparameter updates during training are also shown in Fig. 4).\nAlthough the DRLBF agent fails to reach the performance of\nits unsupervised end-to-end counterparts and only manages\nto slightly outperform the MRCOMP approach in the high\nSNR regime, with the help of better beamforming modules,\nmore hyperparameter optimization and other advanced DRL\ntechniques, further possible performance enhancements to the\nDRLBF agent should be possible. We also want to note that\naddressing the sample efﬁciency of the DRL agent was not the\nfocus of the present study and is left for future investigations.\nV. CONCLUSION\nThis work demonstrates the possibility of tackling the\nproblem of adaptive initial access beam alignment in mmWave\ncommunications using a DRL framework and the state-of-the-\nart PPO algorithm. DRL based methods have a theoretical\nadvantage over current deep unsupervised end-to-end methods\ndesigned for this problem in that they do not require any form\nof environment model and can learn just from interactions with\nthe environment. It is shown that while the direct application of\nthe off-the-shelf PPO algorithm fails on relevant problem sizes,\nusing a specially designed beamforming module enables it to\nscale and deliver competitive performance, even under realistic\nproblem sizes. Due to its DNN-based implementation, the pro-\nposed beamforming module can be easily trained and adapted\nto different antenna conﬁgurations and hardware constraints,\nproviding a large amount of ﬂexibility. There are multiple\npossible future directions: one can for example study more\nand possibly better beamforming modules as they have been\nshown to have a great impact on performance, or, investigate\nthe performance when using a frame-stack method in favour\nof a memory network for the DRL agent.\nREFERENCES\n[1] F. Sohrabi, Z. Chen, and W. Yu, “Deep Active Learning Approach to\nAdaptive Beamforming for mmWave Initial Alignment,” IEEE Journal\non Selected Areas in Communications, vol. 39, no. 8, pp. 2347–2360,\n2021.\n[2] A. Alkhateeb, O. El Ayach, G. Leus, and R. W. Heath, “Channel Es-\ntimation and Hybrid Precoding for Millimeter Wave Cellular Systems,”\nIEEE Journal of Selected Topics in Signal Processing, vol. 8, no. 5, pp.\n831–846, 2014.\n[3] S.-E. Chiu, N. Ronquillo, and T. Javidi, “Active Learning and CSI\nAcquisition for mmWave Initial Alignment,” IEEE Journal on Selected\nAreas in Communications, vol. 37, no. 11, pp. 2474–2489, 2019.\n[4] H. Hassanieh, O. Abari, M. Rodriguez, M. Abdelghany, D. Katabi,\nand P. Indyk, “Fast Millimeter Wave Beam Alignment,” in Proceedings\nof the 2018 Conference of the ACM Special Interest Group on Data\nCommunication, ser. SIGCOMM ’18. New York, NY, USA: Association\nfor Computing Machinery, 2018, p. 432–445.\n[5] M. Hashemi, A. Sabharwal, C. Emre Koksal, and N. B. Shroff, “Efﬁ-\ncient Beam Alignment in Millimeter Wave Systems Using Contextual\nBandits,” in IEEE INFOCOM 2018 - IEEE Conference on Computer\nCommunications, 2018, pp. 2393–2401.\n[6] W. Wu, N. Cheng, N. Zhang, P. Yang, W. Zhuang, and X. Shen,\n“Fast mmwave Beam Alignment via Correlated Bandit Learning,” IEEE\nTransactions on Wireless Communications, vol. 18, no. 12, pp. 5894–\n5908, 2019.\n[7] V. Raj, N. Nayak, and S. Kalyani, “Deep Reinforcement Learning\nBased Blind mmWave MIMO Beam Alignment,” IEEE Transactions\non Wireless Communications, vol. 21, no. 10, pp. 8772–8785, 2022.\n[8] Y. Zhang, M. Alrabeiah, and A. Alkhateeb, “reinforcement learning of\nbeam codebooks in millimeter wave and terahertz mimo systems,” IEEE\nTransactions on Communications.\n[9] F. Sohrabi, T. Jiang, W. Cui, and W. Yu, “Active Sensing for Commu-\nnications by Learning,” IEEE Journal on Selected Areas in Communi-\ncations, vol. 40, no. 6, pp. 1780–1794, 2022.\n[10] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal policy optimization algorithms,” 2017.\n[11] O. E. Ayach, S. Rajagopal, S. Abu-Surra, Z. Pi, and R. W. Heath,\n“Spatially sparse precoding in millimeter wave mimo systems,” IEEE\nTransactions on Wireless Communications, vol. 13, no. 3, pp. 1499–\n1513, 2014.\n[12] L. Meng, R. Gorbet, and D. Kuli´c, “Memory-based deep reinforcement\nlearning for pomdps,” in 2021 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS).\nIEEE Press, 2021, p.\n5619–5626.\n[13] J. Hare, “Dealing with Sparse Rewards in Reinforcement Learning,”\n2019.\n[14] A. Y. Ng, D. Harada, and S. J. Russell, “Policy invariance under\nreward transformations: Theory and application to reward shaping,”\nin Proceedings of the Sixteenth International Conference on Machine\nLearning, ser. ICML ’99. San Francisco, CA, USA: Morgan Kaufmann\nPublishers Inc., 1999, p. 278–287.\n[15] J. Song, J. Choi, and D. J. Love, “Codebook design for hybrid\nbeamforming in millimeter wave systems,” in 2015 IEEE International\nConference on Communications (ICC), 2015, pp. 1298–1303.\n[16] K. M. Attiah, F. Sohrabi, and W. Yu, “Deep Learning Approach\nto Channel Sensing and Hybrid Precoding for TDD Massive MIMO\nSystems,” in 2020 IEEE Globecom Workshops (GC Worksohps, 2020,\npp. 1–6.\n[17] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger,\n“Deep reinforcement learning that matters,” Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, vol. 32, 09 2017.\n",
  "categories": [
    "cs.IT",
    "cs.LG",
    "math.IT"
  ],
  "published": "2023-02-17",
  "updated": "2023-02-17"
}