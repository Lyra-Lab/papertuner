{
  "id": "http://arxiv.org/abs/2302.10825v1",
  "title": "Curiosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning",
  "authors": [
    "Jiong Li",
    "Pratik Gajane"
  ],
  "abstract": "Sparsity of rewards while applying a deep reinforcement learning method\nnegatively affects its sample-efficiency. A viable solution to deal with the\nsparsity of rewards is to learn via intrinsic motivation which advocates for\nadding an intrinsic reward to the reward function to encourage the agent to\nexplore the environment and expand the sample space. Though intrinsic\nmotivation methods are widely used to improve data-efficient learning in the\nreinforcement learning model, they also suffer from the so-called detachment\nproblem. In this article, we discuss the limitations of intrinsic curiosity\nmodule in sparse-reward multi-agent reinforcement learning and propose a method\ncalled I-Go-Explore that combines the intrinsic curiosity module with the\nGo-Explore framework to alleviate the detachment problem.",
  "text": "Curiosity-driven Exploration in Sparse-reward Multi-agent Reinforcement\nLearning\nJiong Li 1 Pratik Gajane 1\nAbstract\nSparsity of rewards while applying a deep rein-\nforcement learning method negatively affects its\nsample-efﬁciency. A viable solution to deal with\nthe sparsity of rewards is to learn via intrinsic mo-\ntivation which advocates for adding an intrinsic re-\nward to the reward function to encourage the agent\nto explore the environment and expand the sample\nspace. Though intrinsic motivation methods are\nwidely used to improve data-efﬁcient learning in\nthe reinforcement learning model, they also suf-\nfer from the so-called detachment problem. In\nthis article, we discuss the limitations of intrinsic\ncuriosity module in sparse-reward multi-agent re-\ninforcement learning and propose a method called\nI-Go-Explore that combines the intrinsic curios-\nity module with the Go-Explore framework to\nalleviate the detachment problem.\n1. Introduction\nIn a Reinforcement Learning (RL) task, an agent learns\nfrom the feedback given by the environment. In contrast\nto other paradigms of machine learning, an advantage of\nusing reinforcement learning is the ability to solve complex\nlearning tasks without expert domain knowledge. Particu-\nlarly, in multi-agent reinforcement learning, the agent often\noperates in a complex environment. For instance, Li et al.\n(2020) consider the problem of learning an optimal courier\ndispatching policy in a multi-agent dynamic environment.\nA reinforcement learning problem is typically modeled us-\ning a Markov decision process (MDP) (Kaelbling et al.,\n1996; Sutton & Barto, 2018). An MDP is deﬁned by a tuple\nthat is formed by the state set, the action set, the transition\nfunction, and the reward function. The state set and the\naction set are typically assumed to be known to the agent\nwhile the transition function and the reward function are\nunknown to the agent. When an agent takes a particular\naction in a particular state, it receives a reward given by the\n1Eindhoven\nUniversity\nof\nTechnology.\nCorrespon-\ndence to:\nJiong Li <j.li11@student.tue.nl>, Pratik Gajane\n<p.gajane@tue.nl>.\nreward function and the environment transitions to a state\ngiven by the transition function. In a reinforcement learning\nproblem, the learning goal is formalized as the outcome of\nmaximizing the obtained cumulative reward. In order to\nmaximize the cumulative reward, a reinforcement learning\nagent faces a signiﬁcant challenge known in the literature as\nthe exploration-exploitation dilemma. An agent may choose\nactions tried in the past and found to be rewarding (i.e. ex-\nploitation) or it may choose unexplored actions to see if they\nare more rewarding (i.e. exploration). Without sufﬁcient\nexploration, an agent might not be able to ﬁnd the optimal\nsolution to the reinforcement learning problem. On the other\nhand, excessive exploration does not contribute to the goal\nof maximizing the cumulative reward and hence it repre-\nsents sample-inefﬁciency. Thus, the ability to efﬁciently\nexplore the environment remains key to sample-efﬁcient\nreinforcement learning.\nExploration in reinforcement learning is often driven via\nrewards and hence the density of rewards in the environment\nsigniﬁcantly inﬂuences the efﬁciency and thus sustainability\nof the model. The efﬁciency is presented by the training\ntime of the model and the execution time to achieve cer-\ntain goals. Sustainability measures the feasible range of the\nmodel. Sparsity in the neural networks might improve the\nlearning efﬁciency, however, a sparse-reward environment\nseverely reduces the efﬁciency and sustainability of most\nreinforcement learning algorithms. Consequently, the spar-\nsity of rewards diminishes the applicability of reinforcement\nlearning to real-life problems as shown by Wu et al. (2020).\nA feasible solution to this problem is to improve the explo-\nration efﬁciency to get as much reward as possible. For\nexample, a solution known as reward shaping uses some\nsimulated positive rewards in the environment to encourage\nmore exploration of the actual rewards from the interaction\nwith the environment. However, reward shaping is sensi-\ntive to the reward density in a sparse-reward environment.\nAnother possible solution is to apply intrinsic motivation\ntechniques during the learning process. Intrinsic motivation\nrewards the agent for exploring new states via an intrinsic\nreward. As explained in Andres et al. (2022), there are four\napproaches to achieve intrinsic motivation – 1) count-based,\n2) prediction-error, 3) random network distillation, and\n4) rewarding impact-driven exploration. These methods are\nable to improve exploration in most cases, though they also\narXiv:2302.10825v1  [cs.AI]  21 Feb 2023\nCuriosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning\nhave some drawbacks, such as derailment and detachment.\nDerailment describes a situation that the agent ﬁnds it hard\nto get back to the frontier exploration in the next episode\nsince the intrinsic motivation rewards the seldom visited\nstates. When the intrinsic rewards run out during the explo-\nration, the agent ﬁnishes the learning in current episode and\ngoes back to the starting state for the next episode. However,\nthe agent is expected to go further from the end position\nin the previous episode which is not attracted to the agent\nsince some states are already visited in the last episode.\nThis situation is known as detachment. The detachment\nand derailment issues are specially discussed in intrinsic\nmotivation methods Ecoffet et al. (2021). Especially for the\nprediction-error approach, one of the popular implementa-\ntions is via intrinsic curiosity module (ICM). This is due\nto the ability of ICM to deal with complex environments\nand domains (Yang et al., 2019; Ladosz et al., 2022; Andres\net al., 2022). Accordingly, we use ICM as a baseline for our\nproposed solution.\nThis article focuses on improving the prediction-error in-\ntrinsic motivation method in the multi-agent reinforcement\nlearning task to improve its sample-efﬁciency in sparse-\nreward environments. In this work, we consider multi-agent\ngame theoretic environment so as to simulate a complicated\nspares-reward environment which is close to reality. We\nfocus on a mixed task which is not fully cooperative or not\nfully competitive. In the considered hybrid multi-agent en-\nvironment, the proposed method can show the impact on ex-\nploration in cooperation as well as competition. Each agent\ncan only observe parts of the environment and this partial\nobservability increases the complexity of the problem. In\nthis article, we choose multi-agent deep deterministic policy\ngradient (MADDPG) as the learning algorithm with ICM.\nSince MADDPG allows each agent to apply different reward\nfunctions and supports the centralized training-decentralized\nexecution framework, it is an efﬁcient learning algorithm in\nthis multi-agent environment. We use ICM which provides\nthe intrinsic reward function for each agent and the MAD-\nDPG algorithm allows the agent to get the external reward\nfrom the interaction with the environment.\nThe main contributions of this work are as follows :\n1. We propose a novel solution to improve the training\ntime and to ﬁnd the optimal solution efﬁciently. Our\nsolution also indicates a potential method to learn efﬁ-\nciently without using larger more complicated models.\n2. We analyze the drawbacks of some existing solutions\nin sparse-reward environment and compare the per-\nformance with the proposed method in a multi-agent\nenvironment.\n3. Based on the provided experiment results, we provide\nsome future directions to further improve the proposed\nmethod paving the way for improving the efﬁciency of\nthe reinforcement learning methods.\n4. Our work shows a way to deal with the sparsity and\nimprove data-efﬁcient learning in a sparse deep re-\ninforcement learning environment by expanding the\nreplay experience for an existing method.\n1.1. Related Work\nThe sparse-reward environment is commonly encountered\nin real-world applications, especially in multi-agent rein-\nforcement learning problems. Using intrinsic motivation\nshows a signiﬁcant improvement in improving the sam-\nple efﬁciency in sparse-reward environments, for instance\ntraining a grandmaster in StarCraft (Vinyals et al., 2019),\ntraining the defensive escort team (Sheikh & B¨ol¨oni, 2020)\netc. However, as Delos Reyes et al. (2022) demonstrated,\nexploration using ICM suffers from problems like detach-\nment and inadequate exploration quality when used with\nlimited observations.\nBurda et al. (2018) also show that random network distilla-\ntion (RND), another intrinsic motivated method, is sufﬁcient\nto motivate the agent to explore efﬁciently in a short-term\ndecision process, though it has a lower performance in a\nlong-term goal. For instance, Burda et al. (2018) consider\nMontezuma’s Revenge. Montezuma’s Revenge is an atari\ngame in which the agent tries to collect useful items to es-\ncape with some actions, like jumping, running, sliding down\npoles, and climbing chains and ladders. However, the agent\nmight get stuck if it focuses on short-term rewards (like\ncollecting the items or ﬁghting with enemies) but moves\naway from the exit door. RND can help the agent to get\nthe key, however, it does not realize that the key should be\nsaved in a long-term strategy due to the limitation of the\nkey. Another solution is proposed in Hester et al. (2018)\nwhich applied the pre-training with demonstration data to\nexpand the replay buffer. The improved experiment results\nin the same environment, Montezuma’s Revenge, indicate\nthat expanding the replay buffer might alleviate the detach-\nment and derailment issue. However, collecting the domain\nknowledge and generating demonstration data requires extra\nknowledge of each experiment environment which might be\nunavailable or not conveniently attainable.\nGo-Explore (Ecoffet et al., 2021) is another method which\naims to increase the efﬁciency of exploration. However,\nGo-Explore is more fragile in the multi-agent environment\ndue to its complexity. In this article, we propose to combine\nthe features of Go-Explore and the ICM framework with the\naim of solving detachment and derailment issues.\nCuriosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning\n2. Problem Formulation\nWe design a decentralized partially observable environment\ninvolving multiple agents to simulate the real-world envi-\nronment. The naive ICM method and the improved ICM\nmethod are implemented on a multi-agent deep determin-\nistic policy gradient (MADDPG) algorithm. To quantify\nthe improvement, we design a complicated sparse-reward\nenvironment in which a few speciﬁc states result in positive\nrewards. By comparing the total reward of our proposed\nmethod, I-Go-Explore, with the total reward of the baseline\nnaive ICM method, we showcase the performance improve-\nment of the proposed method.\nWe use the formulation of a partially observable Markov\ngame to represent a multi-agent Markov game. A partially\nobservable Markov game is described with the following\nquantities :\n• n agents.\n• The public state space S.\n• The action space for each agent, denoted as A1, ...An.\n• The observation space for each agent, denoted as\nOi, ..., On.\n• The stochastic policy for each agent as πi : Oi ×Ai →\n[0, 1]. The next state is produced by the current state\nand joint action from each agent i.e., T : S × Ai ×\n... × An →S.\n• The reward function for each agent is ri : S × Ai →\nR, then the agent i arrives at a private observed state\noi : S →Oi and the private observation space is\nO = o1, ..., on.\n• The deterministic policies for each agent as µOi →Ai\nand the deterministic policy space is µ = µ1, ..., µn.\n• The policy parameters for each agent are θi and the\npolicy parameters’ space is Θ = θ1, ..., θn. And the\npolicy is represented by its parameters: πi →θi.\n• The action-state value function for each agent is cal-\nculated using the private observation space and the\njoint actions during the centralized training i.e., Qµ\ni :\nO, a1, ..., an where a1 ∈Ai, ...an ∈An. Each Qµ\ni is\nindependent.\n3. Methodology\nIn this section, we describe the methodology used in our\narticle.\n3.1. Multi-Agent Deep Deterministic Policy Gradient\n(MADDPG)\nThe MADDPG algorithm performs centralized training and\ndecentralized execution to accelerate the training process\nusing an actor-critic algorithm (Konda & Tsitsiklis, 1999).\nSince this algorithm is extended from Deep Determinis-\ntic Policy Gradient (DDPG), the parameters of the policy\nare optimized by the gradient descent method. As for the\nactor-critic framework, the actor network achieves the de-\ncentralized execution which independently optimizes the\npolicy only with self-observed information. The actor net-\nwork for each agent applies the policy gradient to maximize\nthe expected reward signals and the optimization process of\nthe policy is achieved by the gradient of the expected reward\nsignals for each agent as:\n∇θiJ (θi)\n= EO,a D[∇aiQµ\ni (O, a1, ..., an)∇θiµi(oi)|ai=µθi(oi)]\nwhere Qµ\ni (O, a1, ..., an) is the decentralized action-value\nfunction for agent i, and D is the experience replace buffer\nwhich contains the samples of trajectory at each episode.\nAs for the critic network for each agent, it achieves central-\nized training where it gathers all the information from each\nagent and evaluates the quality of the policy from the actor\nnetwork. The loss function used in the critic network is:\nL(θi) = EO,a1,...,an,r1,...,rn,O′[Qµ\ni (O, a1, ..., an) −y)2]\ny = ri + γQµ′\ni (O′, a1, ..., an, r1, ..., rn)\nwhere y is the true reward signal from the next state and\nµ′ is the target policy. However, in the general sum game,\neach agent will make a trade-off between the maximization\nof individual reward and the maximization of team reward.\nWith this framework, centralized training and decentral-\nized execution are implemented for the multi-agent Markov\ngame. The reward function is designed based on the intrin-\nsic motivation method to solve the sparse-reward issue. The\nstructure of this framework is shown in Figure 1 and further\ndetails are discussed below. The ﬁgure only illustrates the\nstructure for a single agent and the other agents have the\nsame structure.\n3.2. Intrinsic Curiosity Module\nThe ICM method (Pathak et al., 2017) motivates the agent to\nexplore in multi-agent environment by giving an additional\nreward signal as an intrinsic reward. The intrinsic reward\nis the prediction error which is the difference between the\npredicted next state and the actual next state. The more\ndisparate the prediction and the reality, the higher is the\nreturned intrinsic reward. For the decentralized execution,\neach agent has a private intrinsic reward module. ICM\nis composed of three parts: encoder, forward model, and\ninverse model.\nCuriosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning\nFigure 1. MADDPG structure and intrinsic reward function for\nagent 1. The critic network calculates the total reward signal for\nthe chosen action and the total reward is the sum of the external\nreward and the intrinsic reward.\nBelow we explain the intrinsic curiosity model for agent i.\nThe encoder abstracts the feature from the input state which\nis denoted as φi(oi), where oi is the private observation\nfor agent i. Let f be a function representing the forward\nmodel which generates a prediction of the next state with\nthe current state feature and the action taken as input i.e.,\no′i\nt+1 = f i(oi\nt, ai\nt),\n(1)\nwhere o′i\nt+1 is the prediction of the next observation of agent\ni at time t. The loss function used is the prediction error\nLi = 1\n2||o′i\nt+1 −oi\nt+1||2\n2.\n(2)\nThe individual intrinsic reward is calculated using the fol-\nlowing function g which quantiﬁes the novelty:\nriri\nt\n= g(o′i\nt+1, oi\nt+1),\n(3)\nwhere riri\nt\nis the intrinsic reward at time t for agent i. The\nlast part in the intrinsic curiosity module is the inverse model\nwhich uses the feature of current observation and the feature\nof next observation to predict a possible action at the current\nstate. Then the inverse prediction error is used to train the\ninverse model and the encoder. The complete details of the\nICM for agent i are shown in Figure 1.\nReward Function in MADDPG\nThe reward function is\ndivided into two parts – external reward signal and intrinsic\nreward signal. For agent i at time t, the reward can be\ndenoted as:\nri\nt = riri\nt\n+ reri\nt\n,\n(4)\nwhere ri\nt is the total reward, riri\nt\nis the intrinsic reward and\nreri\nt\nis the external reward. The intrinsic reward is generated\nusing the ICM method. The external reward is decided by\nthe environment. Below we consider a function h which\nquantiﬁes the relationship between group A and group B\nwhere the agents in group A are the opponents of the agents\nin group B. Similar to Zhou et al. (2022), the external\nreward signal is composed of the relative external reward\nand the base external reward. The relative external reward\nis calculated as:\nreri\nt\n=\nM\nX\ni=1\nN\nX\nj=1\nh(Ai, Bj),\n(5)\nwhere group A contains M agents and group B contains N\nagents. Furthermore, let δ be the base reward signal. Then,\nthe sum of the two base reward signals of two groups is\nshown as:\nreri\nt\n=\n(\nreri\nt\n+ δ\nif agent belongs to group A\nreri\nt\n−δ\nif agent belongs to group B.\n(6)\nThis reward function is used with the MADDPG method.\n3.3. I-Go-Explore\nGo-Explore (Ecoffet et al., 2021) starts the exploration from\nthe achieved state which alleviates the problems of detach-\nment and derailment for intrinsic motivation methods. The\nGo-Explore method has two phases: exploration and robus-\ntify. In the exploration phase, it ﬁrst selects a state from\nthe archive, simulates the chosen state, explores from the\nsimulated state, and ﬁnally updates the exploration result\nto the archive. The archive is the visited state’s space and\nthe selection is based on the probability distribution or a\nheuristic method. As mentioned in Ecoffet et al. (2021),\ndetachment might happen when the intrinsic reward guides\nthe agent to the frontier of the exploration and it loses in-\nterest in going back to the exploration state. Therefore, we\npropose a solution to add an exploration phase at the end\nof each episode to give additional motivation so that the\nexperience in the replay buffer is expanded. This explo-\nration is decentralized and each agent explores with ﬁxed\nsteps. As for the simulation step, the trajectory, the action\nsequence, and the reward signal also need to be replayed\nfrom the storage. The newly visited states will be updated\nto the achievement of agent i. The existing state will be\nupdated with fewer trajectory steps in the achievement. In\nour experiments, we show that the performance and training\nefﬁciency of reinforcement learning can be improved by our\nproposed solution.\n4. Experimental Results\nThis article is extended based on the success of the ICM\nmethod in multi-agent reinforcement learning sparse reward\nCuriosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning\nFigure 2. The large black circles represent the landmark, the red\ncircles are the predators and the green circle is the prey.\nenvironment and the success of Go-Explore in a single-\nagent sparse reward environment. The initial multi-agent\nreinforcement learning (MARL) environment implements\nMulti-agent Particle Environment (MPE) which is proposed\nin Lowe et al. (2017). This environment was created for\nMADDPG method and it contains several scenarios to test\nan algorithm’s performance in a cooperative task 1.\nIn the given experiments, the base model applies MADDPG\nas the learning algorithm so that each agent has an individual\nreward function. Based on MADDPG framework, we test\nthe performance of our proposed solution I-Go-Explore\nagainst the baseline of ICM.\nIn the MADDPG framework, each agent has its own actor-\ncritic network and target actor-critic network which contains\ntwo-layer ReLU MLPs with 64 cells in each layer. The\nactor-critic network updates the parameters in each episode\nand the target actor-critic network applies the soft update\nfunction to optimize the parameters in a short time period.\nUsually, the learning rate of the critic network is slightly\nhigher than the actor network’s since the aim of the critic\nnetwork is to evaluate the actor network. In the ICM method,\nthe encoder has two ReLU MLPs with 64 cells in each\nlayer, and the forward function and inverse both have one\nReLU MLP with 64 cells in each layer. In the experiment,\nwe applied the same environment as Lowe et al. (2017).\nTherefore, the basic parameters setup are following Lowe\net al. (2017), except updating frequency for target network\nsince the aim of this paper focuses on the performances\nof intrinsic motivation instead of optimizing MADDPG\nframework. The target network is updated every 10 steps\n1The code used for the experiments will be made available via\na Github repository on acceptance of this submission.\nduring the training to alleviate the side effects of the partial\nobservation environment.\nWe conduct three sets of experiments to show that our pro-\nposed solution provides a promising improvement in the\nmulti-agent environment. There are three predators, one\nprey and two landmarks (obstacles) in this chasing environ-\nment. The prey moves faster than the predator. The prey\nis punished for being caught or moving out of the vision\nborder which is -1.0 to +1.0. The prey is easy to be caught\nby moving out of the border. The prey gets a punishment\nof -10 for each occurrence of the above two situations. The\npredators get rewards by catching the prey with +10 for\neach catch and they are not punished for moving out of the\nvision border. An illustration of this environment is shown\nin Figure 2.\nEach agent has an individual policy and a reward system to\nmaximize the individual reward. A more optimal solution\nis for the predators to learn to cooperate to chase the prey\nwhich indicates the maximization of the total reward. The\nprey-predator is a two-dimensional environment and the\nstate is presented by the positions of agents and landmarks,\nand the action is formed by ﬁve discrete moving directions.\nHowever, each agent has an observation limitation that only\nallows the agent to observe the partial environment within a\ncircular area of radius 0.5.\nThe aim of the ﬁrst experiment is to observe the perfor-\nmances of both methods in terms of the reward of prey\nand predators and also the time-consumption. For the sec-\nond experiment, we aim to compare the long-term training\nresult with the short-term training result to analyze if I-Go-\nExplore indicates a good effect on the long-term task. The\nthird experiment focuses on the I-Go-Explore method which\ntests the performances with different exploration lengths to\nobserve the impact of exploration length. The evaluation\nsetting during the training for the three experiments is the\nsame. We evaluate the rewards of each agent for every 5\nepisodes since the environment is stochastic and it would be\nmore reasonable to evaluate average rewards for a batch of\ntraining episodes. There are training phases and test phases\nfor the experiments. We trained the model according to the\nexperiment settings and applied the trained model to the\ncorresponding test environment.\n4.1. Experiment 1\nThe aim of the ﬁrst experiment is to observe the perfor-\nmances of both methods in terms of the reward of prey\nand predators and also the time-consumption. We trained\nthe model for ICM and I-Go-Explore separately for 500\nepisodes with 100 steps in each episode. Then, we test the\nrewards of ICM model and the I-Go-Explore model in the\nprey-predator environment (different setting from the train-\ning) in 100 episodes with 100 steps in each episode. With\nCuriosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning\nFigure 3. The ﬁgure compares the reward of prey and predators\nfor 500 episodes with 100 steps in ICM method and I-Go-Explore\nmethod.\nthe same parameter setups for the training process in ICM\nand I-Go-Explore, I-Go-Explore has higher average rewards\nduring the test as shown in Figure 3. We took 50 steps (half\nthe length of the episode) for the exploration phase in I-Go-\nExplore method since the exploration information might\nhelp the sampling during the target network training and we\ndo not want the impact of the exploration to be stronger than\nthe actual training experience.\nNevertheless, from the results shown in Figure 3, some\noutliers catch our attention, like the episode around 55 and\nthe episode around 65. Recall that the prey is punished for\ngetting caught while, predators are rewarded for catching the\nprey. These outliers indicate that the prey is easily caught by\nthe predators in those episodes and policies for the predators\nare better. To conﬁrm our conjecture, we checked the images\nin this evaluation period and each image records the moves\nof each agent during the episode. Observing the image\nwith the large negative prey reward shows that the prey\nalways runs out of the borders to escape the predators and\npredators successfully move out of the borders to catch the\nprey. In the I-Go-Explore method, the prey and predators\ngot 0 rewards. One possible explanation is that the prey\nhas a larger velocity and it learns to escape the predators\nand keeps itself within the border. On the other hand, the\npredators are too slow to catch the prey even though they\nknow how to move toward the prey.\nThe average rewards for prey and predators under each\nmethod are shown in Table 1. From the average rewards, we\nobserve that prey can avoid being caught while keeping itself\nwithin the borders in I-Go-Explore method. We conjecture\nthat it beneﬁted from the exploration so that the MADDPG\nmodel gave better guidance during the test.\nagent\nICM\nI-Go-Explore\nprey\n-786.62\n-268.83\npredator (each)\n16.76\n8.88\nTable 1. The average rewards for 500 episodes in two methods are\nshown in here.\nFigure 4. The ﬁgure is the intrinsic reward for each agent. Agent 4\nis the prey and the rest are the predators.\nWe trained the model on Intel Core i5-8265 cpu with 8GB\nRAM and the go-explore ICM method took less training\ntime (15805 seconds) than the ICM method (18005 sec-\nonds).\n4.2. Experiment 2\nTo compare the impact of I-Go-Explore on long-term tasks\nfrom experiment 1, the training step in experiment 2 is 20\nsteps for each episode. Similar to the exploration performed\nin experiment 1, the exploration phase takes half the episode\nlength which is 10 steps for go-explore ICM method. The\ntotal reward in two methods for 100 episodes with 20 steps is\nplotted in Figure 4. From the results, there is little difference\nbetween the two models in the test environment. Indeed,\nin the shorter steps of training, ICM method showed better\nperformance in prey agent which is the opposite of longer\nsteps of training. The average rewards of the short-term\ntraining and the long-term training for two methods are\nshown in Table 2. Also, there is little difference between\nthe computation time of the two methods.\n4.3. Experiment 3\nIn the last experiment, we compared the I-Go-Explore\nmethod with different exploration settings in 100 episodes of\n100 steps each. The short-exploration model is trained with\nCuriosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning\nagent\nICM\n(long)\nI-Go\n-Explore\n(long)\nICM\n(short)\nI-Go\n-Explore\n(short)\nprey\n-786.62\n-268.83\n-15.66\n-22.96\npredator\n16.76\n8.88\n10.70\n7.77\nTable 2. This table compares the difference of the rewards between\ntwo methods in long-term training and short-term training.\nFigure 5. The red line represents the total reward value for preda-\ntors and the green represents the total reward value for prey\n10 steps in the exploration phase and the long-exploration\nmodel is trained with 50 steps in the exploration phase. The\nframework parameter settings are the same for both the\nmodels except for the exploration length. In Figure 5, we\ncompared ICM, I-Go-Explore with 50 steps exploration, and\nI-Go-Explore with 10 steps exploration. From the experi-\nmental results, for prey agent, go-explore ICM model with\na shorter exploration phase has a better test performance as\nshown in Table 3. One possible explanation is that too much\narbitrary exploration might have a side effect on the policy\nin this environment.\nagent\nICM\nI-Go-Explore\nICM(long)\nI-Go-Explore\nICM(short)\nprey\n-786.62\n-268.83\n-143.10\npredator\n16.76\n8.88\n14.14\nTable 3. This table compares the I-Go-Explore method trained with\ndifferent exploration lengths and the shorter exploration length gets\na better result in the experiment.\n5. Concluding Remarks and Future Work\nIn this article, we showed that the proposed method I-Go-\nExplore shows promising results for the task of improving\nthe performance of the existing reinforcement learning meth-\nods in a data-sparse environment. We also showed how the\nI-Go-Explore method uses sparsity to improve the model’s\nefﬁciency. This article indicates a promising research direc-\ntion to improve the model efﬁciency with sparsity.\nOur experimental results showcase the better performance of\nI-Go-Explore compared to the baseline of the ICM method.\nInteresting directions for future work include the following.\n• For learning algorithm : ICM method has been imple-\nmented with some other algorithms, like COMA (De-\nlos Reyes et al., 2022; Yang et al., 2021), PPO (Zhang\net al., 2022). We could implement I-Go-Explore with\nsuch different learning algorithms.\n• To show the robustness of the I-Go-Explore method,\nwe could test more partially observable multi-agent\nenvironments.\nReferences\nAndres, A., Villar-Rodriguez, E., and Del Ser, J. An eval-\nuation study of intrinsic motivation techniques applied\nto reinforcement learning over hard exploration environ-\nments. arXiv preprint arXiv:2205.11184, 2022.\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. Ex-\nploration by random network distillation. arXiv preprint\narXiv:1810.12894, 2018.\nDelos Reyes, R., Son, K., Jung, J., Kang, W. J., and Yi,\nY. Curiosity-driven multi-agent exploration with mixed\nobjectives. arXiv e-prints, pp. arXiv–2210, 2022.\nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., and\nClune, J. First return, then explore. Nature, 590(7847):\n580–586, 2021.\nHester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T.,\nPiot, B., Horgan, D., Quan, J., Sendonaris, A., Osband, I.,\net al. Deep q-learning from demonstrations. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence,\nvolume 32, 2018.\nKaelbling, L. P., Littman, M. L., and Moore, A. W. Re-\ninforcement learning: A survey. Journal of artiﬁcial\nintelligence research, 4:237–285, 1996.\nKonda, V. and Tsitsiklis, J. Actor-critic algorithms. In\nSolla, S., Leen, T., and M¨uller, K. (eds.), Advances in\nNeural Information Processing Systems, volume 12. MIT\nPress,\n1999.\nURL\nhttps://proceedings.\nCuriosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning\nneurips.cc/paper/1999/file/\n6449f44a102fde848669bdd9eb6b76fa-Paper.\npdf.\nLadosz, P., Weng, L., Kim, M., and Oh, H. Exploration\nin deep reinforcement learning: A survey. Information\nFusion, 2022.\nLi, Y., Zheng, Y., and Yang, Q. Cooperative multi-agent\nreinforcement learning in express system. In Proceedings\nof the 29th ACM International Conference on Information\n& Knowledge Management, pp. 805–814, 2020.\nLowe, R., Wu, Y. I., Tamar, A., Harb, J., Pieter Abbeel,\nO., and Mordatch, I. Multi-agent actor-critic for mixed\ncooperative-competitive environments. Advances in neu-\nral information processing systems, 30, 2017.\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T.\nCuriosity-driven exploration by self-supervised predic-\ntion. In International conference on machine learning,\npp. 2778–2787. PMLR, 2017.\nSheikh, H. U. and B¨ol¨oni, L. Multi-agent reinforcement\nlearning for problems with combined individual and team\nreward. In 2020 International Joint Conference on Neural\nNetworks (IJCNN), pp. 1–8. IEEE, 2020.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction. MIT press, 2018.\nVinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,\nDudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,\nT., Georgiev, P., et al. Grandmaster level in starcraft ii\nusing multi-agent reinforcement learning. Nature, 575\n(7782):350–354, 2019.\nWu, T., Zhou, P., Wang, B., Li, A., Tang, X., Xu, Z., Chen,\nK., and Ding, X. Joint trafﬁc control and multi-channel\nreassignment for core backbone network in sdn-iot: A\nmulti-agent deep reinforcement learning approach. IEEE\nTransactions on Network Science and Engineering, 8(1):\n231–245, 2020.\nYang, H., Shi, D., Zhao, C., Xie, G., and Yang, S. Ciexplore:\nCuriosity and inﬂuence-based exploration in multi-agent\ncooperative scenarios with sparse rewards. In Proceed-\nings of the 30th ACM International Conference on In-\nformation & Knowledge Management, pp. 2321–2330,\n2021.\nYang, H.-K., Chiang, P.-H., Hong, M.-F., and Lee, C.-Y.\nFlow-based intrinsic curiosity module. arXiv preprint\narXiv:1905.10071, 2019.\nZhang, J., Zhang, Z., Han, S., and L¨u, S. Proximal policy\noptimization via enhanced exploration efﬁciency. Infor-\nmation Sciences, 609:750–765, 2022.\nZhou, X., Zhou, S., Mou, X., and He, Y. Multirobot collab-\norative pursuit target robot by improved maddpg. Com-\nputational Intelligence and Neuroscience, 2022, 2022.\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2023-02-21",
  "updated": "2023-02-21"
}