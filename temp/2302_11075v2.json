{
  "id": "http://arxiv.org/abs/2302.11075v2",
  "title": "Deep Active Learning in the Presence of Label Noise: A Survey",
  "authors": [
    "Moseli Mots'oehli",
    "Kyungim Baek"
  ],
  "abstract": "Deep active learning has emerged as a powerful tool for training deep\nlearning models within a predefined labeling budget. These models have achieved\nperformances comparable to those trained in an offline setting. However, deep\nactive learning faces substantial issues when dealing with classification\ndatasets containing noisy labels. In this literature review, we discuss the\ncurrent state of deep active learning in the presence of label noise,\nhighlighting unique approaches, their strengths, and weaknesses. With the\nrecent success of vision transformers in image classification tasks, we provide\na brief overview and consider how the transformer layers and attention\nmechanisms can be used to enhance diversity, importance, and uncertainty-based\nselection in queries sent to an oracle for labeling. We further propose\nexploring contrastive learning methods to derive good image representations\nthat can aid in selecting high-value samples for labeling in an active learning\nsetting. We also highlight the need for creating unified benchmarks and\nstandardized datasets for deep active learning in the presence of label noise\nfor image classification to promote the reproducibility of research. The review\nconcludes by suggesting avenues for future research in this area.",
  "text": "DEEP ACTIVE LEARNING IN THE PRESENCE OF LABEL NOISE: A\nSURVEY\nMoseli Mots’oehli\nDepartment of Information and Computer Science\nUniversity of Hawai’i at Manoa\nHonolulu, HI 96822\nmoselim@hawaii.edu\nKyungim Baek\nDepartment of Information and Computer Science\nUniversity of Hawai’i at Manoa\nHonolulu, HI 96822\nkyungim@hawaii.edu\nSeptember 21, 2023\nABSTRACT\nDeep active learning has emerged as a powerful tool for training deep learning models within a\npredefined labeling budget. These models have achieved performances comparable to those trained\nin an offline setting. However, deep active learning faces substantial issues when dealing with\nclassification datasets containing noisy labels. In this literature review, we discuss the current state of\ndeep active learning in the presence of label noise, highlighting unique approaches, their strengths,\nand weaknesses. With the recent success of vision transformers in image classification tasks, we\nprovide a brief overview and consider how the transformer layers and attention mechanisms can\nbe used to enhance diversity, importance, and uncertainty-based selection in queries sent to an\noracle for labeling. We further propose exploring contrastive learning methods to derive good image\nrepresentations that can aid in selecting high-value samples for labeling in an active learning setting.\nWe also highlight the need for creating unified benchmarks and standardized datasets for deep active\nlearning in the presence of label noise for image classification to promote the reproducibility of\nresearch. The review concludes by suggesting avenues for future research in this area.\nKeywords Active Learning · Label Noise · Image Classification · Vision Transformer\n1\nIntroduction\nMachine learning algorithms are a sub-class of artificial intelligence that learns from data to perform a pre-defined\ntask such as classification, regression, or clustering. Of the numerous algorithms for machine learning, artificial\nneural networks, deep neural networks in particular have done exceptionally well in tasks involving complex data\nrepresentations such as images, text, and sound. The main reason for this is that if you have a large enough dataset,\nyou can build more extensive and complex models with little to no risk of over-fitting. While this works in theory, the\npractical applications have major drawbacks such as the need for labeled training examples that come at a high cost due\nto the time needed to label the data, the high cost of labor in very specialized fields, or the cost of running simulations\nthat would produce the ground truth dataset. The solution comes in the form of deep active learning (DAL) algorithms,\nwhich strive to let the learning algorithm iteratively pick data examples to be labeled from a larger unlabelled dataset, in\nsuch a manner that results in: (1) A smaller labeled training set, (2) A dataset that is representative of the underlying\ndata distribution leading to a near-optimal learner, (3) A data labeling skim that does not exceed the labeling budget.\nWhile this works well for most use cases, real-world dataset labeling has inherent label noise due to a variety of factors\nsuch as redundant observations being labeled differently, the best human expert classification performance being low,\nor the use of auto-labeling software such as Mechanical Turk. This has adverse effects on these DAL algorithms’\nperformance, and most existing DAL literature focuses on noise-free settings. We explore existing literature around\nthe problem of using DAL algorithms in the existence of label noise. We are particularly interested in the image\nclassification domain using different deep representation learning frameworks such as convolutional neural networks\n(CNNs) and vision transformer networks.\narXiv:2302.11075v2  [cs.LG]  19 Sep 2023\nActive Learning With Label Noise\nA SURVEY\nIn Section 2, we briefly discuss deep learning and the architectures used in image classification. Section 3, presents\nthe main ideas behind active learning as well as the issues that arise when datasets contain noisy labels. In Section 4,\nwe detail commonly used datasets for active learning on image classification tasks as well as the evaluation metrics.\nSection 5 is a detailed analysis of the literature on active learning with label noise in image classification tasks. We\nconclude by exploring possible directions for future research in DAL on vision tasks under label noise.\n2\nDeep Learning\nDeep Learning (DL) refers to the use of artificial neural networks (ANNs) with multiple hidden layers Ivakhnenko and\nLapa [1965], to approximate known or unknown functions. The multi-layered neural network was built on top of the\nperceptron Rosenblatt [1958] introduced in 1958. Over the years, different domain-specific DL architectures have been\ndeveloped to enhance the quality of the learned representations from the different data modalities. Early research focused\non improving optimization, custom layers and connections, activation functions, loss functions, and hyper-parameter\ntuning techniques for the multi-layer perceptron as a way to improve performance on different data modalities. For\ntabular data, tree-based ensemble learning algorithms such as Random forest Breiman [2001], XGBoost Chen and\nGuestrin [2016], and CatBoost Prokhorenkova et al. [2018] are preferred over DL for their superior performance and\nresource efficiency. A non-exhaustive selection of interesting neural network adaptations to tabular data includes Schäfl\net al. [2022], Roman et al. [2022], Popov et al. [2020], Arik and Pfister [2021], Baohua et al. [2019]. In the natural\nlanguage processing domain, earlier work involved learning word and sentence representation using shallow neural\nnetworks in an unsupervised setting Pennington et al. [2014], Novák et al. [2020], Grave et al. [2018]. Until the wide\nadoption of attention-based transformer language models Vaswani et al. [2017], See et al. [2017], word and sentence\nlevel embeddings are fed to a DL model with recurrent connections such as a Long-Short-Term-Memory(LSTM)\nnetwork Hochreiter and Schmidhuber [1997] to achieve state-of-the-art results on down-stream text classification,\nsentence completion, named entity recognition or summarization tasks. For non-temporal visual tasks such as image\nclassification, object detection, segmentation, and pose estimation Artacho and Savakis [2020], CNN-based architectures\nwith specialized output layers and a lot of training data are still the most widely adopted approach. With each of these\ncomplex tasks, there are different challenges in the data annotation process that introduce varying levels of label noise.\nWhile the DL methods discussed in this section have been applied to other supervised learning vision tasks such as\ndetection and segmentation, we focus on approaches for image classification in this section. We give a brief overview\nof CNNs that are responsible for a large share of progress in vision-based tasks. We then highlight the use of more\ncomplex CNNs for image classification and finally explore the literature on state-of-the-art spatial attention-based\nmodels (Vision Transformers) in the context of image classification Kolesnikov et al. [2021].\n2.1\nConvolutional Neural Networks\nConvolutional neural networks were introduced by Yann Lecun and Yashua Bengio as an improvement to human-based\nfeature extraction in training multi-layer neural networks on spatial data Lecun et al. [1998]. The key deficiencies with\ntraining fully connected feed-forward neural networks (FFNN) using back-propagation for computer vision tasks are\nefficiency and transformation (rotation, translation) invariance. Handling high-dimensional image data with standard\ninput neurons is non-trivial and inefficient. Given that low-resolution image datasets are normally 28 × 28, the initial\ninput layer using an FFNN would contain 784 neurons. If the subsequent hidden layer had as little as 100 neurons, the\n2-layer fully connected network immediately has more than 78400 weights and bias terms connecting the two layers.\nThe weights of the network are stored in high-dimensional matrices, and the flow of information in the forward and\nbackward pass is performed using matrix operations. The number of input neurons and hidden layer depth required for\naccurate approximation of complex image-to-class mappings on high-resolution images using FFNNs is large.\nCNNs are especially good at handling image data for three main reasons; firstly the convolution operation uses a sliding\nfilter to identify and highlight the presence of local relations between pixels that represent important features. By so\ndoing, we capture features expressing lines, edges, and corners implicitly. Secondly, in CNNs, the input image is not\nflattened into an array as is the case in using FFNN. This means the relative positions of pixels in a grid format are\npreserved and so we do not lose information through rearranging the pixels. Finally, in CNNs, filters have their own\nweights, but the same filter is used to slide over the image, and this means the features learned are invariant to the\npositions of patches on the image as the convolution operation learns local relationships between pixels. In addition,\nthis way CNNs are able to use fewer weights than would be the case if we were to consider the absolute positions of\npixels on the image and capture positional encoding. Figure 1 depicts a simple CNN with convolution, pooling, and\nfully connected layers with non-linearity activation functions for image classification.\nMore advanced CNN architectures have been introduced, mostly similar in that they have multiple convolution and\npooling blocks (earlier layers capture low-level features, and deeper layers capture higher-level features). The most\n2\nActive Learning With Label Noise\nA SURVEY\nFigure 1: [Source: Standard CNN]. CNN for classifying an image into one of the categories: bird, sunset, dog, cat, and\nmore common objects.\nFigure 2: Resnet50 architecture with convolutional blocks of different filter sizes and max pooling. Residual connections\nacting as memory cells arch above the blocks passing initial information all the way to the final layer He et al. [2016].\nnotable of these are GoogLeNet Szegedy et al. [2015], VGG Simonyan and Zisserman [2015], ResNet He et al.\n[2016], DenseNet Huang et al. [2017], and EfficientNet M. Tan [2019]. For example, ResNet, as shown in Figure 2,\ndemonstrated the idea of residual connections (also called skip connections). The residual connections pass one layer’s\ninputs directly to the next convolution block to provide lower-level context to the subsequent layer hence combating\nvanishing gradients in very deep networks. DenseNet on the other hand has a dense building block in that all the\nlayers in a block have direct connections with each other, allowing for a more effective reuse of features in the network.\nAlso, by having all layers connected, a regularization effect is created so that the network does not learn redundant\nrepresentations, hence combating over-fitting.\nThe different layers are connected by non-linear activation functions such as the popular ReLU and Elu Nair and Hinton\n[2010], Clevert et al. [2016]. CNNs have been the dominant approach to computer vision benchmarks for a large part of\nthe last decade mainly due to their ability to extract meaningful spatial features from images. The main catalysts in\nascending order of importance for this were the availability of large labeled training datasets, advances in computing\nhardware, and a reduction in the computational cost of training such Deep Neural Networks (DNNs). Post ImageNet Jia\net al. [2009], CNN-based models trained on very large labeled datasets have been used in the feature extraction and\npre-training step of most fine-tuned state-of-the-art approaches in different vision tasks.\n2.2\nVision Transformers\nBefore full transformer models in the language domain, the best LSTM models use a low dimensional vector repre-\nsentation to pass information from an encoder network to a decoder network, while using an attention mechanism.\nAttention in this setting is used to learn what parts of an input sequence are most important in predicting different parts\nof the output. In the original paper \"Attention is all you need\" Vaswani et al. [2017], Vaswani et al. demonstrate that\nlong temporal dependencies can be learned without the need for recurrence. The three fundamental components in a\ntransformer network are a positional encoding of words, attention, and self-attention mechanisms. Positional encoding\nof both input and output tokens is achieved by assigning integer values to tokens/words based on their relative position\nin the input and output sequences. Unlike LSTMs, the work of learning word progression and relationships between\ninput and output words is done implicitly by the network instead of designing networks with explicit bias in the form\nof recurrent cells and sequential processing. Self-attention makes it possible to learn good representations for most\nlanguages given a sufficiently large collection of text in a semi-supervised manner by masking tokens and letting the\n3\nActive Learning With Label Noise\nA SURVEY\nFigure 3: Vision transformer architecture showing an input image split into 14 by 14 patches, and linearly projected to\nthe standard transformer input space. The far right side of the image shows the components of the standard transformer\nblock with multi-head attention Kolesnikov et al. [2021].\nnetwork learn what the missing word is in any given input sequence. The learned representations are then used on\na downstream task with fewer labeled data. Because transformers do not process input tokens in sequence, they are\nperfect for parallel GPU training.\nLike most great innovations, the fundamental ideas of the transformer have been incorporated into CNNs Wortsman\net al. [2022], Zihang et al. [2021], Srinivas1 et al. [2021], and in some cases completely replacing CNNs Touvron et al.\n[2020], Liu et al. [2021], Chen et al. [2022] to produce state-of-the-art results in various computer vision benchmarks.\nIn Kolesnikov et al. [2021], Kolesnikov et al. present the earliest vision transformer(ViT) to surpass state-of-the-art\nCNNs on most image classification benchmarks. They show that in the large dataset regime, ViTs achieve higher\nclassification accuracy, are more computationally efficient, and show no signs of saturation compared to CNNs such as\nResNet and EfficientNet on increasingly larger datasets. The main difference between the natural language processing\n(NLP) transformers and the vision transformers is in how the input is encoded. With vision transformers, they take 14\nby 14 patches from an image, flatten them, and apply a linear projection onto a higher dimensional space equal to that\nof the original input space of the NLP transformer. The spatial proximity relations of patches are implicitly left to the\ntransformer to learn in the following way: They add a trainable 1D positional encoding vector to each patch’s linear\nprojection. The positional representations are organized in the order of the patches starting from the top left corner to\nthe bottom right corner of the image as depicted in Figure 3. Beyond input encoding, the rest of the ViT architecture is\nsimilar to that of the language transformer for classification tasks.\nThe paper shows interestingly that, through the attention mechanism the transformer layers are able to learn the same\nlow-to-high level features with increasing depth as is the case with deep CNNs. Other notable implementations of\nViTs for image classification without label noise include Yu et al. [2022], Chen et al. [2022], Liu et al. [2021]. ViTs\nare included in this review and further discussed in Section 6 as we perceive them to be a very important area for\nfuture research. This is because they are progressively becoming the dominant multi-modal approach and yet very\nlittle work has been done in applying them to DAL and learning with label noise for image classification. These\nmodels are designed in a modular fashion to easily be able to learn both language and image representations for image\ncaptioning, classification, scene-text understanding, and visual question answering Yu et al. [2022]. It is particularly\ninteresting since the authors present a joint contrastive loss (image-to-text and text-to-image), image classification loss,\nand image-to-language captioning loss, allowing for efficient training of a single network for multiple tasks, and the\nability to transfer the learned representations to a different downstream task and dataset.\nIn the next section the active learning (AL) framework for machine learning is described, including key approaches for\ntraining deep learning models on a labeling budget in the case of clean labels, and finally, the scene is set for label noise\nand the literature addressing DL on noisy labels.\n4\nActive Learning With Label Noise\nA SURVEY\nFigure 4: The five main components to the standard Active Learning Framework. Each of these components may vary\ndepending on the complexity of the data to be learned and the available resources. Most work in active learning has\nfocused on the development of query selection algorithms that lead to highly informative and diverse data samples for\nlabeling by the oracle.\n3\nActive learning\nIn most supervised machine learning use cases, there is an initial data collection and labeling cost, in both money\nand time. In some domains and tasks, datasets are inherently difficult to label for a variety of reasons, meaning more\ntime is needed even by an expert human annotator to assign a label to each sample. In other cases the cost of hiring\nexpert annotators is high, such as is the case in medical imaging Górriz et al. [2017], Konyushkova et al. [2017], or\nthe cost of producing the samples is high, such as is the case in experimental physics where observations come from\nvery expensive telescopes or particle accelerators. This presents a challenge to the real-world use of machine learning\nsystems, especially as unlabeled dataset sizes increase. Active learning is a machine learning paradigm, as depicted in\nFigure 4, that seeks to address this problem by letting learning algorithms iteratively select a subset Lm of size m, from\na larger unlabelled dataset U n of size n : m ≤n, to be labeled by an oracle O for training. The active learning mantra\ncan be stated as follows: Train a machine learning model on a significantly smaller labeled dataset, with little to no drop\nin test performance, all the while staying within a pre-determined labeling budget B.\nDeep active learning algorithms (DAL), while overlapping, can broadly be grouped into pool-based methods, density-\nbased methods, and data expansion methods. Pool-based methods select samples for labeling from an unlabeled pool,\nbased on either the uncertainty of the currently trained model on samples U n, the diversity of samples in the labeled set\nLm used to train the current model or a combination of both Lewis and Gale [1994], McCallum and Nigam [1998],\nC.Shui et al. [2020]. Pool-based methods are simple in their formation and implementation but can be computationally\nexpensive for large datasets of high dimensional data such as images. Since pool-based methods largely rely on metrics\nevaluated on the entire unlabeled dataset to select new candidates, this is not ideal for applications that require low\nlatency. Density-based methods seek to capture key characteristics of the underlying data distribution. This is done by\nselecting a core-set of samples for labeling that are sufficiently representative of the entire dataset, and leads to good\ngeneralization Sener and Savarese [2018], Phillips and Tai [2018], Phillips [2016]. More recent literature blends pool\nand density-based methods to take advantage of each approach’s benefits. These methods thus lead to efficient and robust\nmodels trained on core-sets containing diverse samples that maximize the margins between object classes Har-Peled\net al. [2007], Geifman and El-Yaniv [2017]. Some methods in this approach use the hidden layer representations\nfrom training a self-supervised task on the image data, instead of the raw pixels. These include pre-training on image\norientation, random (90, 180, 270, 360)◦rotation classification, or self-supervised contrastive learning, where the target\nis an arbitrary patch of adjacent pixels in the image Chen et al. [2020], Du et al. [2021], Wang et al. [2021a]. Data\nexpansion methods seek to expand the training dataset, by generating reasonably realistic synthetic data samples for\n5\nActive Learning With Label Noise\nA SURVEY\neach target class to enhance the learning algorithm’s performance on the real test dataset Chen et al. [2020]. Since\ntheir introduction, Generative Adversarial Networks (GANs) and their variations Goodfellow et al. [2014], Gonog\nand Zhou [2019], Sinha et al. [2019a] were the go-to method for generating synthetic data. However, the training of\nGANs is unstable, the samples tend to be unrealistic, and it is hard to evaluate these samples for quality Barnett [2018],\nMescheder et al. [2018].\n3.1\nLabel Noise\nlabel noise refers to the scenario in which data labels are corrupted, with or without intention, so that we do not have\n100% confidence in their correctness. Label noise is different from feature noise which is normally used to refer to\nadding gaussian noise to feature values. Label noise impacts learning algorithms more adversely than feature noise\ndoes, and is harder to deal with Chicheng and Kamalika [2015], Wei et al. [2022], Algan and Ulusoy [2021], Cordeiro\nand Carneiro [2020]. Label noise is inherent in the data collection and processing life-cycle. Most real-world datasets\nare subjected to a number of label noise sources based on how the data is collected, curated, and stored. Label noise\nin practice broadly stems from (1) incorrect crowd-sourced labels where the annotators are non-experts such as is the\ncase with Worker [2022], and Amazon [2022], (2) incorrect expert annotations due to the complexity of the data, as is\ncommon in medical fields Górriz et al. [2017], (3) labeling errors introduced by automatic labeling by web crawling\nsoftware and other AI labeling systems such as Scale.ai [2022], (4) noise introduced by multiple experts or non-experts\nlabeling the same sample differently.\nLearning noisy labels is especially hard due to the fact that cost functions are generally significantly less complex\nthan feature extraction layers. Label noise can be grouped, and is mostly treated based on what is known about the\nnoise-generating distribution Nagarajan et al. [2013]. Some datasets contain label noise from a known and quantifiable\ngenerative distribution, while in other cases, too little or nothing is known about the noise transition matrix to model.\nLabel noise can be class-independent or class-dependent. Class-independent label noise is the easiest to generate. The\ngenerative process can be summarized in this manner: for each sample, the class label is replaced with a random class\nlabel, with a fixed probability 1/N where N is the number of classes Patrinin et al. [2017]. Class-dependent label noise\nis normally a result of expert human annotation. It results from pairs of very closely related or indistinguishable classes\nbeing occasionally mislabeled Han et al. [2018a]. For example, the true large-sized cat is occasionally labeled as a small\ndog, and visa versa. Common methods for training DL models include first filtering out samples with a high probability\nof being noisy and iteratively training on a dataset with trusted labels until a threshold is reached. The filtering process\nin most literature involves training two different neural networks with a custom loss, and monitoring samples on which\nthey disagree on predictions. This method works well since it has been shown that the networks train on stronger signals\nfirst, which is the case in a dataset with predominantly clean labels. Representative methods in this approach, trained\nin a non-active learning manner include Decoupling Malach and Shalev-Shwartz [2017] and Co-teaching Han et al.\n[2018b]. The main implementation difference between the two approaches is in how the two networks’ weights are\nupdated. Decoupling updates each network’s weights based on its prediction error when the networks have a prediction\ndisagreement. Co-teaching on the other hand, cross-updates the weights with the error signal from the other network.\nUnlike Decoupling, Co-teaching addresses noisy labels explicitly by enabling the networks to peek into each other’s\nhidden state, simultaneously reducing the risk of each network over-fitting the noisy input.\nWe have introduced deep learning, active learning, and learning with label noise. For further reading, we suggest the\nsurvey papers Algan and Ulusoy [2021], Ren et al. [2020] on image classification with noisy labels, and DAL on clean\nlabels respectively. DAL methods on noisy labels are presented in Section 5.\n4\nEvaluation Datasets and Metrics\nIn this section, we introduce datasets and evaluation metrics commonly used for active learning and learning with\nlabel noise. The State-of-the-art DAL methods on zero-label noise datasets are Górriz et al. [2017], Konyushkova et al.\n[2017], Sener and Savarese [2018], Phillips and Tai [2018], Phillips [2016]. Datasets and their meta-data are provided\nthat pertain to AL and DL on label noise. We conclude the section by exploring the evaluation metrics for DAL for\nnoisy data on common bench-marking datasets.\n4.1\nDatasets\nAs stated previously, the meteoric rise of DL algorithms was largely due to the availability of large labeled training\ndatasets. In image classification, the most notable datasets include MNIST LeCun et al. [2010], ImageNet Jia et al.\n[2009], CIFAR-100 Krizhevsky et al. [2009], CALTECH-101 Fei-Fei et al. [2004], SVHN Netzer et al. [2011], and MS\nCOCO Lin et al. [2014]. Public evaluation datasets facilitate a centralized evaluation of algorithms on a pre-defined task.\n6\nActive Learning With Label Noise\nA SURVEY\nThese datasets can be downloaded from their websites or the different DL frameworks such as PyTorch, TensorFlow,\nJax, and Theano. The best-performing models and their results on these datasets are normally hosted on a public\nleaderboard for the dataset. When evaluating datasets for DAL on image classification tasks, the standard practice is\nto use the same datasets as in full dataset image classification, but we monitor performance gain after a pre-defined\nnumber of labeled examples. While in practice this may not be the case that all labels are available upfront as is the case\nin using fully labeled datasets, the training cycle of DAL algorithms applied on these complete datasets substantially\nmimics the process of obtaining labels from an oracle for a live stream of unlabeled data within a budget.\nThe datasets vary widely in size, the number of classes, and the complexity inherent in telling the classes apart. Of all\nthe commonly used AL datasets, MNIST is the least complex, with only 10 classes of hand-written digits in single\nchannel 28 × 28 images. CALTECH-101, ImageNet, and CIFAR-100 are higher-resolution image datasets. These\ndatasets contain more classes than MNIST, some of which are harder to tell apart. In passive learning, the model sees\nall available training samples per class, but in the DAL setting, depending on the query algorithm and scarcity of a class,\nthe algorithms may never see more than a third of the samples of certain under-sampled classes. This leads to poor\nvalidation performance.\nLarge-sized datasets with high-resolution images also pose a computational problem in DAL algorithms that select\ndiverse samples based on a distance measure to all other unlabeled images. This can be extremely costly to compute in\nboth time and hardware resource requirements. For these reasons, the reported performance of DAL algorithms on these\ndatasets is lower than that of non-active learning algorithms since researchers have a low incentive to test complex DAL\nalgorithms on large datasets. Table 1 contains a non-exhaustive list of commonly used image classification datasets for\nactive learning and learning with label noise.\nDataset\nYear\n# Samples\nClasses\nDAL Papers\nlabel noise Papers\nImageNet\n2012\n1,431,167\n1000\nYi et al. [2022]\nHataya and Nakayama\n[2018]\nSVHN\n2011\n660,000\n10\nGupta et al. [2020], Wang\net al. [2021b], Sener and\nSavarese [2018]\nGupta et al. [2020], Xia\net al. [2020]\nMNIST\n2010\n70,000\n10\nLi et al. [2022], Hauß-\nmann et al. [2019], Gupta\net al. [2020]\nYounesian et al. [2020],\nGupta et al. [2020]\nCIFAR(10,100)\n2009\n60,000\n(10,100)\nDu et al. [2021], Youne-\nsian et al. [2020], Wei\net al. [2022], Li et al.\n[2022], Shui et al. [2020],\nHaußmann et al. [2019],\nGupta\net\nal.\n[2020],\nYounesian et al. [2021]\nYounesian et al. [2020],\nHan\net\nal.\n[2018a],\nHataya and Nakayama\n[2018],\nGupta\net\nal.\n[2020], Younesian et al.\n[2021]\nCaltech101\n2004\n9,000\n101\nLi et al. [2022], Yi et al.\n[2022]\n-\nTable 1: Image classification datasets commonly used for deep active learning and the training of DL with noisy labels\nImageNet and SVHN, being the larger of these datasets, are not well suited for DAL because training a single DL\nmodel on a large dataset is computationally expensive, and takes a lot of time. The computation complexity is worse in\nthe case of DAL algorithms due to the iterative nature of the process. Retraining a large model over and over on the\nImageNet or SVHN datasets is time-consuming. This is reflected in the literature by the reluctance of authors to use\nthese large datasets for DAL classification, in favor of relatively small datasets such as CALTECH-101 and CIFAR-100.\nDeveloping and training algorithms for handling noisy labels follows one of two paths: using datasets with noisy labels\nintroduced by one or more of the noise sources listed in Section 3.1, or noise-free datasets to which measurable label\nnoise is injected by perturbing existing trusted labels. In the existing literature, the same datasets (MNIST, CIFAR-100,\nCALTECH-100) commonly used for image classification are used for noisy label classification, with a pre-determined\nprobability of swapping each label. This probability is also called the noise rate, and the higher it is, the more corrupted\nthe dataset becomes after noise injection. In literature, it is common to inject 30% −60% random symmetric label\nnoise before training, while keeping a test set that is free of label noise.\nDatasets such as ImageNet with a large number of classes (1000) tend to also not be favored for the purpose of evaluating\nDAL methods that address learning under label noise. The reason here is that, with more class labels, the likelihood of\nclass-dependent labeling errors at the time the dataset was created is higher. The kind of deliberate noise injected into\n7\nActive Learning With Label Noise\nA SURVEY\ndatasets for noisy label learning is class-independent and is measurable as opposed to class-dependent noise that may\nbe inherent in the data collection and annotation process. Class-dependent label noise makes training DL models harder,\nand there is lower confidence in the correctness of the test set labels used for evaluation.\n4.2\nEvaluation Metrics\nThe general evaluation methodologies for DAL algorithms on noisy labels are the same as those used for standard\ndatasets for image classification. Top-1 accuracy is the most commonly used metric. Not much consideration is given to\nthe underlying class distribution in most existing work and so it would be of interest to explore how class imbalances\naffect DAL algorithms in the presence of label noise. Since DAL is also concerned about performance under a budget,\nit would make sense to measure budget efficiency, a measure commonly not well documented in the literature.\nGiven that active learning involves training a model a couple of times for every batch of labels received, it becomes\nobvious that the computational cost of DAL algorithms should be a big consideration. In Yoo et al. [2017], Yoo et al.\npropose the use of a small network for performing query selection so that the retraining and labeling cycles run faster.\nOnce the labeling budget is exhausted, a larger and more powerful network is then trained using the obtained labels.\nWhile this approach can be very useful in scenarios where time is of the essence, it has a big drawback. The weakness\nis that using a weaker learner for sample selection could lead to lower sample diversity since a weaker learner does not\nperform a very good job of understanding the boundaries between classes in the feature space.\nIn Sinha et al. [2019b], Signha et al. demonstrate the use of transfer learning for fast extraction of useful representations\nin a DAL setting. They show that using large pre-trained models and only fine-tuning the DAL task achieves good\nresults with considerably fewer labeled examples. This means that, given the same budget, their approach has a higher\nlabel efficiency than a model trained from scratch. This also means for a given target performance, they require less\ncomputational resources and time to fit the target, by leveraging good pre-trained model weights. The work of Settles\nSettles [2009] contains a comprehensive survey of the computational cost of active learning algorithms. The main\nfindings in this work are that the cost is influenced largely by the dataset size in terms of both the number of samples\nand the size of each sample. Settles also states the complexity of the query selection algorithm as well as the number\nof samples per batch are big factors in the total computational cost of DAL. In the case of DAL with label noise, it is\ncritical that we have a good understanding of the underlying label noise so that the test set remains clean. While this\nworks in developing DAL algorithms on well-known datasets, it remains unclear how the test set integrity is guaranteed\nin practice. If the correctness of the test labels cannot be guaranteed, evaluation methods such as top-1 accuracy,\nprecision, and recall do not offer any reliable measure for the network’s generalization performance.\nIn this section, we explored datasets and evaluation metrics commonly used in comparing DAL algorithms, in particular\nunder the setting of label noise. The next section is the main focus of this work. We explore methods leveraging the\nversatility of deep neural networks in the active learning framework where labeling budget is an important metric, and\nwe are faced with a noisy label challenge.\n5\nDeep Active Learning Algorithms for Noisy Labels\nIn this section, we focus on the main contribution of this manuscript: exploring literature on DAL algorithms used\nfor image classification in the presence of label noise. It is worth stating that while literature is rich in theoretical\napproaches for handling label noise in the offline setting, very little has been done for active learning algorithms.\nMethods that address label noise by modeling the underlying generative distribution and filtering noisy label examples\nfrom the training set to achieve better performance are few and in between. We foresee a lot of work going into this\nwork and look forward to understanding how iterative processes best approximate a noisy label distribution. We are also\ninterested in understanding how low sample numbers affect label noise distributions. These ideas remain unexplored in\nliterature.\nThe methods in this section are predominantly independent of the noise distribution and seek noise-robust active training\nby using customized model architectures, loss functions, or training procedures. In Gupta et al. [2020], Gupta et al.\npropose the use of standard sample diversity and importance query policies, supplemented by the model’s confidence\nscores on samples. They argue that DNNs are normally uncertain about the decision boundaries between classes\nvery early in training. Training with label noise exacerbates this problem since temporary and imaginary boundaries\ncould form based on mislabeled samples, and through diversity sampling, get propagated into important query batches\nthat influence model uncertainty and sample importance. The authors use the BALD score, introduced in Gal et al.\n[2017] (not to be confused with the paper Cao and Tsang [2021]) as an importance score to ensure the information\ncontent of samples for labeling per batch is optimal and a good representation of the entire dataset. The inclusion of\n8\nActive Learning With Label Noise\nA SURVEY\nmodel uncertainty in the sample selection query ensures labeled batches will include samples the current model is very\nuncertain about, and these, assuming satisfactory oracle label accuracy, improve the entire DAL cycle under label noise.\nThe inclusion of highly uncertain samples is only one-half of the novelty of their approach to robustify learning under a\nnoisy oracle. They include a denoising layer to their network. The denoising layer is explained in the following manner:\nThe softmax output of the original classifier is fed to the denoising layer, and the model is trained on the denoising\nlayer, which represents a non-zero probability of predicting a particular label given the true label. This final denoising\nlayer’s weights, unlike normal final softmax outputs, are constrained to ensure noisy labels have little impact during\ntraining. During testing, the penultimate layer’s output is used instead of the denoising layer for prediction. In this\nway, the denoising layer serves as a rigorous teacher to the student, becoming a noise-robust model used in testing and\ndeployment.\nGupta et al. show that their method, in the noise-free setting achieves similar performance to the common baseline\nDAL methods, such as the original BALD Cao and Tsang [2021], core-set, entropy-based selection, and random sample\nselection on the MNIST, CIFAR10, and SVHN datasets. We attribute these results purely to the addition of model\nuncertainty to diversity and information gain in selecting samples. We argue the denoising layer as described in the\npaper would serve no purpose if the oracle provides only clean labels and so the results in the noise-free setting would\nbe better stated as: “no performance loss or gain\" from adding the denoising layer in the noise-free setting. At 10% and\n30% label noise, their method outperforms the reported standard benchmarks on all of the three datasets, speaking to\nthe effectiveness of their denoising mechanism. While these are good results, they fail to demonstrate how the approach\ncompares to similar state-of-the-art DAL methods tailored for noisy labels, and how adding the same denoising layer to\nDAL ResNets trained under entropy only or random sample selection compares to their approach. The paper also lacks\ndetails on the training setup that is important for reproducibility, such as the hardware used, the exact deep learning\narchitecture, whether pre-trained weights are used or not, and the number of training epochs.\nSimilar to Gupta et al. [2020], in Younesian et al. [2020], Younesian et al. introduce a DAL framework (DuoLab) for\ntraining on noisy labels using weak and strong oracles. CIFAR10 and CIFAR100 are used in training and testing a\nCNN, with 30% and 60% label noise. They adopt similar criteria to Gupta et al. [2020] for query selection, namely\nusing information gain and uncertainty. The weak and strong oracles refer to the innate differences in human labelers’\ngeneralization abilities and labeling quality. It is assumed that weak oracles are cheaper and more likely to produce\nincorrect labels than strong oracles, and so this approach’s novelty is particularly more interesting in the real-world\nsetting where there is always a need to reduce labeling fees paid to the oracles. However, the use of two oracles seems\nto not affect the overall performance of the DAL classifier in their work, but rather gives budget subsidies.\nWhen it comes to dealing with noisy labels, instead of robustifying their model, Younesian et al. approach the problem\nby filtering out samples suspected to have noisy labels. Their DAL approach starts with an initially labeled dataset\nused to train the classifier, but they deviate from the conventional use of a random batch of samples to perform this\ninitial training. Their overall approach hinges on a key and possibly flawed assumption that there exists a small and\nclean batch of training examples that can be used to initially train the model. While in practice we can optimistically\nassume it is possible to push physical data and labeling boundaries so this initial clean set is available, the paper lacks\nthe minimum theoretical guarantees in the case we are unable to say with “100%\" certainty that specific labels from the\noracle are correct. If we were to initially assume some labels are “100%\" correct, the paper does not make it clear as to\nhow the correctness of such labels is verified given that the oracles have a noise rate of up to 60%. Once the model is\ntrained on the initial clean samples, the model predicts the classes of all samples in the unlabeled pool, and the model’s\nconfidence score on the top 2 classes is used in deciding whether a sample is noisy or not. They measure the difference\nin top-2 class probabilities for each sample, and declare the top-k samples with the lowest margin as potentially noisy\nand so not fit for training the network.\nIt becomes obvious that this approach will lead to a number of false positives and false negatives, and are likely to affect\nthe overall performance of the model in classes that are hard to tell apart. Another issue is that a model trained on a\nvery small subset of a large dataset can display a high top-1 class prediction confidence while being totally wrong if it\nhas seen very little to no examples of a class in training. They combat this by reusing the noisy labels once all the clean\nexamples are exhausted. The noisy samples are clustered based on the trained model’s penultimate representation of\neach noisy sample. The samples within each cluster are then ranked based on their informativeness, and the top K\nsamples per cluster are picked and used to further train the model. The training batches on these samples are not random.\nThey are ordered so that the most informative samples are in the first training batch and the least informative are in\nthe last batch. It is unclear how this process circumvents the need for actual ground truth labels, and how this further\ntraining on noisy labels does not negatively affect the performance of a model first trained on clean labels. Reported test\naccuracy results show that DuoLab outperforms noise-resilient baselines on both CIFAR 10 and 100 with 30% label\nnoise.\n9\nActive Learning With Label Noise\nA SURVEY\nIn Younesian et al. [2021], Younesian et al. propose QActor, an approach that follows the same idea as their earlier work\nin Younesian et al. [2020]: Identifying and reusing possibly noisy labels. Over and above this, the paper introduces\na noise-aware informative measure, and they demonstrate for the first time how dynamic allocation of the labeling\nbudget per query leads to better performance as compared to the convention of equal distribution of the budget across\nthe number of query cycles. In this paper, it is assumed that a small set of clean initial training data, as well as a clean\ntest data, are readily available. The DAL algorithm decides which samples are to be sent to the oracle for labeling.\nThey also leave it to the DAL algorithms to decide how many of the samples fit the selection criteria and are labeled\nper iteration. On each iteration, a batch of highly informative samples as measured by entropy is sent to the oracle for\nlabeling, and then the labels are compared to the current model’s predictions. Samples, where there is a disagreement,\nare sent to a suspicious data collection. These samples are later ranked on informativeness, and resent to the oracle for\nrelabeling, with the previously assigned label kept in mind. The authors argue that relabeling samples that the model is\nuncertain about and are potentially incorrectly labeled by the oracle leads to the highest gain in model performance\nsince these are likely samples from classes that are easy to mix up.\nIn Huang et al. [2016], Huang et al. approach DAL under oracle noise in a way that is different from most of the work\ndiscussed thus far. The complexity analysis performed in their work proves DAL is possible and viable with oracle\nepiphany. They explain that empirical evidence has shown that oracles are likely to delay providing labels on samples\nthey are unsure about until more related examples are presented to the oracle, at which point they have an epiphany, and\nare able to provide a more confident label to such examples. Having had the experience of hand-labeling 1000 images\nfrom a large fisheries dataset, we agree with the authors that oracle abstention and epiphany are realistic considerations.\nAn interesting idea mentioned in the paper is adding one more possible class label for a classification problem with N\nclasses so that the oracle has N + 1 possible classes to choose from. This class label can be one of: \"I don’t know\" or\n\"unsure\". Adding this choice relieves the oracle of the urge to guess between two or three most likely label assignments\nfor a hard sample image, hence increasing the overall confidence in the correctness of the class labels that are actually\nprovided. This is true since in a perfect world the oracle has to be the source of absolute truth, and so we are not\ninterested in cases where the oracle guesses correctly. An oracle guessing would be useful if they are allowed to provide\na measure of how confident they are of their guess.\nIn their approach, they use Markov chains to model when epiphany occurs for a certain class, that was previously hard\nto label to the oracle. The two main assumptions made in this work are: 1) The oracle is honest, and 100% accurate on\nthe samples he/she decides to label anything other than “unsure\". 2) Given the oracle is honest, all samples avoided will\nhave correct labels once epiphany occurs. For both assumptions, the authors further assume once an epiphany occurs,\nno drastic changes in the oracle’s assignment of labels will occur. While these assumptions sound reasonable, it is\nworth stating that in the real world, a time-constrained oracle optimizing for earnings is unlikely to avoid assigning\nlabels to samples they are unsure about. This is especially true if they learn earlier on that samples they label \"unsure\"\nwill always return, requiring more of their time. The assumption that the oracle is 100% accurate on examples they\nlabel is also very unrealistic and does not factor in the wide-ranging spectrum of human capabilities in labeling. It\nwould be more useful if the authors stated how 100% oracle label accuracy would be guaranteed or tested post-epiphany.\nComparing this approach to the works Younesian et al. [2021, 2020] of Younesian et al. a spectrum of computer-human\ninvolvement in label noise filtration can be drawn. At one end Huang et al. use the oracle as a sole decider of label\nuncertainty, and in a more hybrid setting, Younesian et al. use the currently trained classifier’s confidence score together\nwith the oracle’s label as a filter for potentially noisy labels. The model-only approach to noise filtration is using the\nconfidence score margin of the top-2 predicted classes as used in Gupta et al. [2020]. In both Younesian et al. and\nHuang et al.’s approaches, the “uncertain\" samples are sent back for relabeling.\nIn Amin et al. [2021], Amin et al. present the dual-purpose learning framework. While they do not explicitly focus\non nor address label noise, their combined DAL, and abstention learning approaches provide valuable insight into the\nintricacies of DAL and abstention, which are important components that are not discussed as rigorously in Huang et al.\n[2016]. The two bodies of work also differ in that Amin et al. investigate the generalization bounds of the DAL and\nabstention setting and find interestingly that even with an unlimited labeling budget and no labeling noise, the upper\nbound on the observable generalization loss that exists in the passive learning case can not be guaranteed while oracle\nabstention is at play. The authors however do not provide empirical comparisons of their method to state-of-the-art\nDAL algorithms, nor do they detail how their unique dual method would impact performance when applied to such\nhighly-performant algorithms. The work of Yan et al. Yan et al. [2016] is a more general approach to the work of\nAmin et al. since they consider DAL under imperfect labelers, and allow for abstention. Yan et al. go on to show\nthat under strict assumptions on the dimensionality of the decision boundary, abstention, and noise rates close to the\ndecision boundaries, their method generalizes the lower bounds on algorithms such as Amin et al. [2021]. A significant\ncontribution of their work is in demonstrating that their algorithm need not be aware of either the label noise rate or\nabstention rate. With restrictions on how label noise and the rate of abstention are distributed around the decision\nboundaries, their algorithm performs significantly better than prior methods.\n10\nActive Learning With Label Noise\nA SURVEY\n6\nConclusion and Future Research Directions\nTo summarize the literature: (1) A lot of work has been done on training DL models on noisy labels in an offline\nsetting. (2) The literature on DAL methods is also very rich in the case of no-label noise. (3) While this is an area of\nresearch that has very practical applications and financial impact, there is very little work done at the intersection of\nDAL and label noise. It is worth noting that the power of ViTs has not been explored in DAL as much as it has been\nin fully labeled image classification datasets, and so we see huge potential in improving DAL by leveraging unique\ncharacteristics of ViTs in image classification tasks. The exploration of the transformer layers and how attention can be\nused in understanding diversity, importance, and uncertainty is especially intriguing to us. In Section 3, works using\nself-supervised pre-training are presented that attain good lower-dimensional representations of the images. These have\nbeen shown to lead to a good core set of samples for labeling in an active learning framework. It is of high importance\nthat contrastive learning methods are explored further as methods for deriving good image representations that can then\nbe used in improving the diversity, importance, and uncertainty-based selection in queries sent to an oracle for labeling.\nIn terms of DAL for noisy labels, the works of Younesian et al, Huang et al. and Gupta et al. described in Section 5\nrepresent methods and ideas with substantial impact in DAL. In all these approaches, filtration of noise is well addressed,\nbut with a lot of questionable assumptions. Establishing unified DAL and label noise benchmarks and datasets would\nadd a lot of value and ensure future methods conform to realistic assumptions about the training process and the oracle.\nIn all methods addressing this problem, only CNNs are used and the performance is only compared to baseline DAL\nmethods as opposed to a more convincing comparison to state-of-the-art DAL methods. We would like to be able to\nperform the same analysis on DAL methods for label noise using ViTs against the best DAL methods. This is especially\nimportant since ViTs have been the most dominant architectural choice for image classification in recent times. It is also\ncritical that substantial effort is put into understanding how key assumptions made in DAL methods for noisy labels\naffect results, and establish convergence and complexity guarantees mathematically. This means that it is not adequate\nto only understand DAL on noisy labels through experimentation, but also some effort needs to be put into establishing\nconvergence guarantees at a theoretical level.\nIn all existing domain literature, to the best of our knowledge, the oracle gets no feedback. It would be very interesting\nto explore how a cyclical feedback loop between the oracle and the model improves both of them. Intuitively we\nhypothesize it would help to have tips generated by the model in an unsupervised manner accessible to the oracle in\ncases of difficult samples. A step closer to the ideal interaction is allowing for abstention and epiphany. One way to do\nthis is through contrastive learning. We hypothesize that, since contrastive learning models, through rotation or patch\nprediction for example, can be trained with no actual problem domain labels and thus noise-free, the representations\nlearned from this step can be used in proposing similar samples to an oracle before they can abstain from labeling a\nsample. In this manner, the oracle may reach epiphany much earlier than they would in the frameworks listed in the\nliterature. We would also consider answering the question of whether a real human annotator with a known or unknown\nnoise rate can improve, and by how much in this setting.\nSince reproducibility and robustness are key factors in the ongoing development of DL and AL, we are interested in\nperforming extensive training of out-of-the-box DL models in the active learning framework in the presence of label\nnoise. This will not only ensure researchers know what to expect out of different models without any hyperparameter\ntuning but also set up benchmarks that are specific to DAL with label noise. We are interested in covering as many\nimage classification datasets as possible, using multiple CNN and ViT-based models, different AL algorithms, different\nnoise handling algorithms, as well as different loss functions. This is vital to the field as it encourages clear and concise\nstatements about the mode and training conditions, and ensures future methods are to be compared on a level playing\nfield.\nLastly, a key consideration is in computation. Most work in the area does not explicitly state and discuss the\ncomputational complexity of the methods. In a world ever so gravitating towards lowering carbon footprint, it is\nimportant we are able to assess DAL methods on noisy labels not only on their accuracy but also on their computational\nrequirement. This is an interesting avenue of research if one considers how recent work on training large language\nmodels has shown there are considerable trade-offs and gains to be made in the computational operations required,\nmodel performance, architectural choices as well as representation size. Some of these have gone in the face of\nestablished results Xiaoqi et al. [2020], Clark et al. [2020].\nReferences\nG.V. Ivakhnenko and V.M Lapa. The group method of data handling. Automation and remote control, 26(6):895–902,\n1965.\nF. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. In\nPsychological Review., volume 65, pages 386–408, 1958.\n11\nActive Learning With Label Noise\nA SURVEY\nL. Breiman. Random forests. Machine Learning Journal, 45:5–32, 2001. doi:10.1023/A:1010933404324.\nT. Chen and C. Guestrin.\nXgboost:\nA scalable tree boosting system.\nIn Proceedings of the 22nd ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785–794, 08 2016.\ndoi:10.1145/2939672.2939785.\nL. Prokhorenkova, G. Gleb, A. Vorobev, A.V. Dorogush, and A. Gulin. Catboost: unbiased boosting with categorical\nfeatures. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Proceedings\nof the 32nd International Conference on Neural Information Processing Systems, volume 31, page 6639–6649.\nCurran Associates, Inc., 2018.\nB. Schäfl, L. Gruber, A. Bitto-Nemling, and S. Hochreiter. Hopular: Modern hopfield networks for tabular data. ArXiv,\nabs/2206.00664, 2022.\nL. Roman, C. Valeriia, S. Avi, B. Arpit, C. Bruss, G. Tom, W. Andrew, and G. Micah. Transfer learning with deep\ntabular models. 06 2022. doi:10.48550/arXiv.2206.15306.\nS. Popov, S. Morozov, and A. Babenko. Neural oblivious decision ensembles for deep learning on tabular data. ArXiv,\nabs/1909.06312, 2020.\nS.O. Arik and T. Pfister. Tabnet: Attentive interpretable tabular learning. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 35, pages 6679–6687, 2021. doi:10.1609/aaai.v35i8.16826.\nS. Baohua, Y. Lin, Z. Wenhan, L. Michael, D. Patrick, Y. Charles, and D. Jason. Supertml: Two-dimensional word\nembedding for the precognition on structured tabular data. In 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition Workshops, pages 2973–2981, 06 2019. doi:10.1109/CVPRW.2019.00360.\nJ. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word representation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, 10 2014.\ndoi:10.3115/v1/D14-1162. URL http://www.aclweb.org/anthology/D14-1162.\nA. Novák, L. Laki, and B. Novák. CBOW-tag: a modified CBOW algorithm for generating embedding models from\nannotated corpora. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4798–4801.\nEuropean Language Resources Association, 09 2020. ISBN 979-10-95546-34-4. URL https://aclanthology.\norg/2020.lrec-1.590.\nE. Grave, P. Bojanowski, P. Gupta, A. Joulin, and T. Mikolov. Learning word vectors for 157 languages. In Proceedings\nof the International Conference on Language Resources and Evaluation (LREC 2018), 2018.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you\nneed. In Advances in Neural Information Processing Systems, volume 30, 2017. URL https://arxiv.org/pdf/\n1706.03762.pdf.\nA. See, P. Liu, and C. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of\nthe 55th Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1073–1083, 2017.\nS. Hochreiter and J. Schmidhuber. Long short-term memory. In Neural Computation, volume 9, pages 1735–1780,\n1997.\nB. Artacho and A.E. Savakis. Unipose: Unified human pose estimation in single images and videos. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pages 7033–7042, 2020.\nA. Kolesnikov, A. Dosovitskiy, D. Weissenborn, G. Heigold, J. Uszkoreit, L. Beyer, M. Minderer, M. Dehghani,\nN. Houlsby, S. Gelly, T. Unterthiner, and X. Zhai. An image is worth 16x16 words: Transformers for image\nrecognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, 2021. URL\nhttps://openreview.net/forum?id=YicbFdNTTy.\nY. Lecun, L. Bottou, Y. Bengio., and P. Haffner. Gradient-based learning applied to document recognition. Proceedings\nof the IEEE, 86(11):2278–2324, 09 1998. ISSN 0018-9219. doi:10.1109/5.726791.\nC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going\ndeeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2015.\nK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. International\nConference on Learning Representations, abs/1409.1556, 2015.\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 770–778, 2016.\nG. Huang, Z. Liu, L. Van Der Maaten, and K. Weinberger. Densely connected convolutional networks. In 2017 IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR), pages 2261–2269, 2017. doi:10.1109/CVPR.2017.243.\n12\nActive Learning With Label Noise\nA SURVEY\nQ.V. Le M. Tan. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference\non machine learning, pages 6105–6114. PMLR, 2019.\nV. Nair and G.E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th\nInternational Conference on International Conference on Machine Learning, ICML’10, page 807–814, Madison, WI,\nUSA, 2010. Omnipress. ISBN 9781605589077.\nD. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by exponential linear units(elus).\nIn Y. Bengio and Y. LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016,\nConference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.07289.\nD. Jia, D. Wei, S. Richard, L. Li-Jia, L. Kai, and L. Fei-Fei.\nImagenet:\na large-scale hierarchical im-\nage database.\nIn IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, 06 2009.\ndoi:10.1109/CVPR.2009.5206848.\nM. Wortsman, G. Ilharco, S.Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A.S Morcos, H. Namkoong, A. Farhadi, Y. Carmon,\nS. Kornblith, and L. Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy\nwithout increasing inference time. In International Conference on Machine Learning, pages 23965–23998. PMLR,\n2022.\nD. Zihang, L. Hanxiao, L. Quoc, and T. Mingxing. Coatnet: Marrying convolution and attention for all data sizes. In\n35th Conference on Neural Information Processing Systems, 06 2021.\nA. Srinivas1, T. Lin, N. Parmar, J. Shlens, P. Abbeel, and A. Vaswani. Bottleneck transformers for visual recognition.\nIn 2021 Conference on Computer Vision and Pattern Recognition, 2021.\nHerve Touvron, Joao Caballero, Matthieu Guillaumin, and Hervé Jégou. Going deeper with transformers: A study of\ndeep multi-head attention models for image classification. In International Conference on Computer Vision (ICCV),\n2020.\nH. Liu, G. Cheng, W. Lin, J. Yang, J. Yang, H. Zhang, Z. Zhang, and W. Wu. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In International Conference on Computer Vision (ICCV), 2021.\nX. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa,\nL. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue, A. Thapliyal, J. Bradbury,\nW. Kuo, M. Seyedhosseini, C. Jia, B. Ayan, C. Riquelme, A. Steiner, A. Angelova, X. Zhai, N. Houlsby, and\nR. Soricut. Pali: A jointly-scaled multilingual language-image model. In Arxiv, 2022.\nJ. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive captioners are image-text\nfoundation models. Transactions on Machine Learning Research, abs/2205.01917, 2022.\nM. Górriz, A. Carlier, E. Faure, and X. Giró i Nieto. Cost-effective active learning for melanoma segmentation. ArXiv,\nabs/1711.09168, 2017.\nK. Konyushkova, R. Sznitman, and P. Fua. Learning active learning from data. In NIPS, 2017.\nD. Lewis and W. Gale. A sequential algorithm for training text classifiers. In SIGIR ’94, pages 3–12, London, 1994.\nSpringer London.\nA. McCallum and K. Nigam. Employing em and pool-based active learning for text classification. In International\nConference of Machine Learning, 1998.\nC.Shui, F.Zhou, C.Gagn’e, and B.Wang. Deep active learning: Unified and principled method for query and training. In\nInternational Conference on Artificial Intelligence and Statistics, 2020.\nO. Sener and S. Savarese. Active learning for convolutional neural networks: A core-set approach. International\nConference on Learning Representations (Poster), 2018. URL http://dblp.uni-trier.de/db/conf/iclr/\niclr2018.html#SenerS18.\nJ. Phillips and W.M. Tai. Near-optimal coresets of kernel density estimates. In 34th International Symposium\non Computational Geometry, SoCG 2018, June 11-14, 2018, Budapest, Hungary, volume 99 of LIPIcs, pages\n66:1–66:13. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2018. doi:10.4230/LIPIcs.SoCG.2018.66. URL\nhttps://doi.org/10.4230/LIPIcs.SoCG.2018.66.\nJ. Phillips. Coresets and sketches. CoRR, abs/1601.00617, 2016. URL http://arxiv.org/abs/1601.00617.\nS. Har-Peled, D. Roth, and D. Zimak. Maximum margin coresets for active and noise tolerant learning. In International\nJoint Conferences on Artificial Intelligence, 2007.\nY. Geifman and R. El-Yaniv. Deep active learning over the long tail. ArXiv, abs/1711.00941, 2017.\n13\nActive Learning With Label Noise\nA SURVEY\nT. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual representations.\nIn Proceedings of the 37th International Conference on Machine Learning, ICML’20, pages 1597–1607. JMLR.org,\n2020.\nP. Du, S. Zhao, H. Chen, S. Chai, H. Chen, and C. Li. Contrastive coding for active learning under class distri-\nbution mismatch. In IEEE/CVF International Conference on Computer Vision (ICCV), pages 8907–8916, 2021.\ndoi:10.1109/ICCV48922.2021.00880.\nC. Wang, A. Singla, and Y. Chen. Teaching an active learner with contrastive examples. In Advances in Neural\nInformation Processing Systems, 2021a.\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative\nadversarial networks. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors,\nAdvances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https:\n//proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.\nL. Gonog and Y. Zhou. A review: Generative adversarial networks. In The 14th IEEE Conference on Industrial\nElectronics and Applications (ICIEA), pages 505–510, 2019. doi:10.1109/ICIEA.2019.8833686.\nS. Sinha, S. Ebrahimi, and T. Darrell. Variational adversarial active learning. In IEEE/CVF International Conference\non Computer Vision (ICCV), pages 5971–5980, Los Alamitos, CA, USA, 03 2019a. IEEE Computer Society.\ndoi:10.1109/ICCV.2019.00607. URL https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00607.\nS. Barnett. Convergence problems with generative adversarial networks (gans). ArXiv, abs/1806.11382, 2018.\nL. Mescheder, A. Geiger, and S. Nowozin. Which training methods for gans do actually converge? In The 35th\nInternational Conference on Machine Learning, 2018.\nZ. Chicheng and C. Kamalika.\nActive learning from weak and strong labelers.\nIn C. Cortes, N. Lawrence,\nD. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, vol-\nume 28. Curran Associates, Inc., 2015.\nURL https://proceedings.neurips.cc/paper/2015/file/\neba0dc302bcd9a273f8bbb72be3a687b-Paper.pdf.\nJ. Wei, Z. Zhu, H. Cheng, T. Liu, G. Niu, and Y. Liu. Learning with noisy labels revisited: A study using real-world\nhuman annotations. 10th International Conference on Learning Representations, 2022.\nG. Algan and I. Ulusoy. Image classification with deep learning in the presence of noisy labels: A survey. ArXiv,\nabs/1912.05170, 2021.\nF. Cordeiro and G. Carneiro. A survey on deep learning with noisy labels: How to train your model when you cannot\ntrust on the annotations? In The 33rd SIBGRAPI Conference on Graphics, Patterns and Images, pages 9–16, 11\n2020. doi:10.1109/SIBGRAPI51738.2020.00010.\nClick Worker. Clickworker, 2022. URL https://www.clickworker.com.\nAmazon. Amazon mechanical turk, 2022. URL https://www.mturk.com/.\nScale.ai. Scale ai, 2022. URL https://scale.com/.\nN. Nagarajan, D. Inderjit, R. Pradeep, and T. Ambuj. Learning with noisy labels. In Advances in Neural Information\nProcessing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/\npaper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf.\nG. Patrinin, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A\nloss correction approach. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages\n2233–2241, 07 2017. doi:10.1109/CVPR.2017.240.\nB. Han, J. Yao, G. Niu, M. Zhou, I. Tsang, Y. Zhang, and M. Sugiyama. Masking: A new perspective of noisy\nsupervision. In Advances in Neural Information Processing Systems, 05 2018a.\nE. Malach and S. Shalev-Shwartz. Decoupling \"when to update\" from \"how to update\". In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems, NIPS’17, page 961–971, Red Hook, NY, USA,\n2017. Curran Associates Inc. ISBN 9781510860964.\nB. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, and M. Sugiyama. Co-teaching: Robust training of deep neural\nnetworks with extremely noisy labels. In Advances in Neural Information Processing Systems, 2018b.\nP. Ren, Y. Xiao, X. Chang, P. Huang, Z. Li, X. Chen, and X. Wang. A survey of deep active learning. ACM Computing\nSurveys (CSUR), 54:1 – 40, 2020.\nY. LeCun, C. Cortes, and C.J. Burges.\nMnist handwritten digit database.\nATT Labs [Online]. Available:\nhttp://yann.lecun.com/exdb/mnist, 2, 2010.\n14\nActive Learning With Label Noise\nA SURVEY\nA. Krizhevsky, V. Nair, and G. Hinton. Cifar-100 (canadian institute for advanced research). 2009. URL http:\n//www.cs.toronto.edu/~kriz/cifar.html.\nL. Fei-Fei, M. Andreetto, M. Ranzato, and P. Perona. Learning generative visual models from few training examples:\nAn incremental bayesian approach tested on 101 object categories, 2004.\nY. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Ng. Reading digits in natural images with unsupervised\nfeature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.\nT. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, P. Doll’a r, and C.L. Zitnick.\nMicrosoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. URL http://arxiv.org/abs/1405.\n0312.\nJ.S Yi, M. Seo, J. Park, and D. Choi. Using self-supervised pretext tasks for active learning. In European Conference\non Computer Vision, 2022.\nR. Hataya and H. Nakayama. Investigating cnns’ learning representation under label noise. In International Conference\non Learning Representations, 2018.\nG. Gupta, A.K. Sahu, and W. Lin. Noisy batch active learning with deterministic annealing. In arXiv: Learning, 2020.\nT. Wang, X. Li, P. Yang, G. Hu, X. Zeng, S. Huang, C. Xu, and M. Xu. Boosting active learning via improving test\nperformance. In AAAI Conference on Artificial Intelligence, 2021b.\nX. Xia, T. Liu, B. Han, N. Wang, M. Gong, H. Liu, G. Niu, D. Tao, and M. Sugiyama. Part-dependent label noise:\nTowards instance-dependent label noise. In Proceedings of the 34th International Conference on Neural Information\nProcessing Systems, NIPS’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\nX. Li, P. Yang, T. Wang, X. Zhan, M. Xu, D. Dou, and C. Xu. Deep active learning with noise stability. In International\nConference on Learning Representations, 05 2022. doi:10.48550/arXiv.2205.13340.\nM. Haußmann, F. Hamprecht, and M. Kandemir. Deep active learning with adaptive acquisition. In Proceedings of the\n28th International Joint Conference on Artificial Intelligence, IJCAI’19, page 2470–2476. AAAI Press, 2019. ISBN\n9780999241141.\nT. Younesian, D.H. Epema, and L. Chen. Active learning for noisy data streams using weak and strong labelers. ArXiv,\nabs/2010.14149, 2020.\nC. Shui, F. Zhou, C. Gagn’e, and B. Wang. Deep active learning: Unified and principled method for query and training.\nIn AISTATS, 2020.\nT. Younesian, Z. Zhao, A. Ghiassi, R. Birke, and L. Chen. Qactor: Active learning on noisy labels. In Vineeth N.\nBalasubramanian and Ivor Tsang, editors, Proceedings of The 13th Asian Conference on Machine Learning, volume\n157 of Proceedings of Machine Learning Research, pages 548–563. PMLR, 17–19 Nov 2021. URL https:\n//proceedings.mlr.press/v157/younesian21a.html.\nS. Yoo, H. Kim, I. Kim, M. Kim, and S. Hwang. Active learning for deep neural networks. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70, pages 2070–2079. JMLR. org, 2017.\nK. Sinha, Y. Zhang, P. Doshi, and I. Dhillon. Efficient active learning for deep neural networks. In Proceedings of the\n36th International Conference on Machine Learning, pages 3560–3568, 2019b.\nB. Settles. Active learning literature survey. In ECML PKDD Workshop on Active Learning and Experimental Design,\npages 11–46, 2009.\nY. Gal, R. Islam, and Z. Ghahramani. Deep bayesian active learning with image data. In International Conference of\nMachine Learning, volume abs/1703.02910, pages 1183–1192, 2017.\nX. Cao and I. Tsang. Bayesian active learning by disagreements: A geometric perspective. ArXiv, abs/2105.02543,\n2021.\nT. Huang, L. Lihong, A. Vartanian, S. Amershi, and X.J. Zhu. Active learning with oracle epiphany. In D. Lee,\nM. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Sys-\ntems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/\n299fb2142d7de959380f91c01c3a293c-Paper.pdf.\nK. Amin, G. DeSalvo, and A. Rostamizadeh. Learning with labeling induced abstentions. In M. Ranzato, A. Beygelzimer,\nY. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems,\nvolume 34, pages 12576–12586. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/\npaper/2021/file/689041c2baed0f6d91050495d632d6e0-Paper.pdf.\n15\nActive Learning With Label Noise\nA SURVEY\nS. Yan, K. Chaudhuri, and T. Javidi. Active learning from imperfect labelers. In Proceedings of the 30th International\nConference on Neural Information Processing Systems, NIPS’16, page 2136–2144, Red Hook, NY, USA, 2016.\nCurran Associates Inc. ISBN 9781510838819.\nJ. Xiaoqi, Y. Yichun, S. Lifeng, J. Xin, C. Xiao, L. Linlin, W. Fang, and L. Qun. TinyBERT: Distilling BERT for\nnatural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020\", pages\n4163–4174, Online, 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.\nfindings-emnlp.372.\nK. Clark, M. Luong, Q.V. Le, and C.D. Manning. Electra: Pre-training text encoders as discriminators rather than\ngenerators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=r1xMH1BtvB.\nA. Eitan, E. Smolyansky, I. Harpaz, and S. Perets. Connected papers, 2019. URL https://www.connectedpapers.\ncom/.\nAppendix\n.1\nLiterature Networks\nBelow we present a visual depiction of the literature most related to this review focusing on deep active learning on\nnoisy labels for image classification. We present the views as images of network graphs where the nodes represent\npapers and the edges represent the similarity between articles. The larger the node, the more influential the article is to\nrelated articles, and the thicker the connection between any two papers, the more closely related the articles are. The\nConnected Papers visualization tool Eitan et al. [2019] was used to create these graphs.\n16\nActive Learning With Label Noise\nA SURVEY\nFigure 5: Deep active learning with noisy labels literature closely related to Younesian et al. Younesian et al. [2021]\n17\nActive Learning With Label Noise\nA SURVEY\nFigure 6: Deep active learning with noisy labels papers related to the work of Huang et al. Huang et al. [2016]\n18\nActive Learning With Label Noise\nA SURVEY\nFigure 7: Deep active learning with noisy labels papers related to the work of Yan et al. Yan et al. [2016]\n19\nActive Learning With Label Noise\nA SURVEY\nFigure 8: Active learning work closely related to the survey manuscript by Settles Settles [2009]\n20\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-02-22",
  "updated": "2023-09-19"
}