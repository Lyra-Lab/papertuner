{
  "id": "http://arxiv.org/abs/2302.11622v1",
  "title": "Unsupervised 3D Object Learning through Neuron Activity aware Plasticity",
  "authors": [
    "Beomseok Kang",
    "Biswadeep Chakraborty",
    "Saibal Mukhopadhyay"
  ],
  "abstract": "We present an unsupervised deep learning model for 3D object classification.\nConventional Hebbian learning, a well-known unsupervised model, suffers from\nloss of local features leading to reduced performance for tasks with complex\ngeometric objects. We present a deep network with a novel Neuron Activity Aware\n(NeAW) Hebbian learning rule that dynamically switches the neurons to be\ngoverned by Hebbian learning or anti-Hebbian learning, depending on its\nactivity. We analytically show that NeAW Hebbian learning relieves the bias in\nneuron activity, allowing more neurons to attend to the representation of the\n3D objects. Empirical results show that the NeAW Hebbian learning outperforms\nother variants of Hebbian learning and shows higher accuracy over fully\nsupervised models when training data is limited.",
  "text": "Published as a conference paper at ICLR 2023\nUNSUPERVISED 3D OBJECT LEARNING\nTHROUGH\nNEURON ACTIVITY AWARE PLASTICITY\nBeomseok Kang, Biswadeep Chakraborty & Saibal Mukhopadhyay\nSchool of Electrical and Computer Engineering\nGeorgia Institute of Technology, Atlanta, GA 30332, USA\n{beomseok, biswadeep, smukhopadhyay6}@gatech.edu\nABSTRACT\nWe present an unsupervised deep learning model for 3D object classiﬁcation.\nConventional Hebbian learning, a well-known unsupervised model, suffers from\nloss of local features leading to reduced performance for tasks with complex ge-\nometric objects. We present a deep network with a novel Neuron Activity Aware\n(NeAW) Hebbian learning rule that dynamically switches the neurons to be gov-\nerned by Hebbian learning or anti-Hebbian learning, depending on its activity. We\nanalytically show that NeAW Hebbian learning relieves the bias in neuron activity,\nallowing more neurons to attend to the representation of the 3D objects. Empir-\nical results show that the NeAW Hebbian learning outperforms other variants of\nHebbian learning and shows higher accuracy over fully supervised models when\ntraining data is limited.\n1\nINTRODUCTION\nSupervised deep networks for recognizing objects from 3D point clouds have demonstrated high\naccuracy but generally suffer from poor performance when labeled training data is limited (Wu\net al., 2015; Qi et al., 2017a;b; Wang et al., 2019; Maturana & Scherer, 2015). On the other hand,\nself-supervised or unsupervised models can be trained without labeled data hence improving the\nperformance in data efﬁcient scenarios. Self-supervised learning methods have been studied for 3D\nobject recognition mostly in an autoencoder setting, which necessarily reconstructs input to learn\nthe representation (Achlioptas et al., 2018; Girdhar et al., 2016). Unsupervised learning has also\nbeen applied to pre-process the input for an encoder but still largely relying on supervised learning\n(Li et al., 2018). Conventionally, self-organizing maps and growing neural gas have been used as\nfully unsupervised learning for 3D objects while they aim to reconstruct the surface of the objects\n(do Rˆego et al., 2007; Mole & Ara´ujo, 2010). A fully unsupervised deep network for 3D object\nclassiﬁcation has rarely been studied.\nUnsupervised Hebbian learning is known to offer attractive advantages such as data efﬁciency, noise\nrobustness, and adaptability for various applications (Najarro & Risi, 2020; Kang et al., 2022; Mi-\nconi et al., 2018; Zhou et al., 2022). The basic Hebbian and anti-Hebbian learning refer to that\nsynaptic weight is strengthened and weakened, respectively, when pre- and post-synaptic neurons are\nsimultaneously activated (Hebb, 2005). Many past efforts have developed variants of Hebb’s rule.\nExamples include Oja’s rule and Grossberg’s rule (Oja, 1982; Grossberg, 1976) for object recog-\nnition (Amato et al., 2019; Miconi, 2021), ABCD rule (Soltoggio et al., 2007) for meta-learning\nand reinforcement tasks (Najarro & Risi, 2020), and another variant for hetero-associative memory\n(Limbacher & Legenstein, 2020). However, Hebbian learning is often vulnerable to the loss of local\nfeatures (Miconi, 2021; Bahroun et al., 2017; Bahroun & Soltoggio, 2017; Amato et al., 2019). This\nis a major challenge for applying Hebbian rules for tasks with more complex geometric objects, such\nas object recognition from 3D point clouds.\nIn this paper, we present an unsupervised deep learning model for 3D object recognition that uses\na novel neuron activity-aware plasticity-based Hebbian learning to mitigate the vanishing of local\nfeatures, thereby improving the performance of 3D object classiﬁcation. We observe that, in net-\nworks trained with plain Hebbian learning, only a few neurons always activate irrespective of the\nobject class. In other words, spatial features of 3D objects are represented by the activation of only a\n1\narXiv:2302.11622v1  [cs.AI]  22 Feb 2023\nPublished as a conference paper at ICLR 2023\nfew speciﬁc neurons, which degrades task performance. We develop a hybrid Hebbian learning rule,\nreferred to as the Neuron Activity Aware (NeAW) Hebbian, that relieves the biased activity. The\nkey concept of NeAW Hebbian is to dynamically convert the learning rule of synapses associated\nwith an output neuron from Hebbian to anti-Hebbian or vice versa depending on the activity of the\noutput neuron. The reduction of bias allows a different subset of neurons to activate for different\nobject classes, which increases class-to-class dissimilarity in the latent space. Our deep learning\nmodel uses a feature extraction module trained by NeAW Hebbian learning, and a classiﬁer module\nis trained by supervised learning. The feature extractor designed as a multi-layer perceptron (MLP)\ntransforms the positional vector of sampled points on 3D objects into high-dimensional space (Qi\net al., 2017a;b). The experimental results evaluated on ModelNet10 and ModelNet40 (Wu et al.,\n2015) show that the proposed NeAW Hebbian learning outperforms the prior Hebbian rules for\nefﬁcient unsupervised 3D deep learning tasks. This paper makes the following key contributions:\n• We present a deep learning model for 3D object recognition with the NeAW Hebbian learn-\ning rule that dynamically controls Hebbian and anti-Hebbian learning to relax the biased\nactivity of neurons. The NeAW Hebbian learning efﬁciently transforms spatial features of\nvarious classes of 3D objects into a high-dimensional space deﬁned by neuron activities.\n• We analytically prove that the NeAW Hebbian learning relieves the biased activity of output\nneurons if the input is under a given geometric condition, while solely applying Hebbian\nor anti-Hebbian learning does not guarantee the relaxation of the skewed activity.\n• We analytically prove that purely Hebbian learning and anti-Hebbian learning on the biased\nneuron activity leads to a poor subspace representation with few principal components,\nthereby limiting the performance in the classiﬁcation tasks.\n• We empirically demonstrate that the NeAW Hebbian learning rule outperforms the exist-\ning variants of Hebbian learning rules in the 3D object recognition task. We also show\nthat NeAW Hebbian learning achieves higher accuracy than end-to-end supervised learn-\ning when training data is limited (data-efﬁcient learning).\n2\nRELATED WORK AND BACKGROUND\nDeep Learning Models for 3D Object Recognition\nSupervised 3D convolutional neural network\n(CNN) models have been developed to process a volumetric representation of 3D objects (Maturana\n& Scherer, 2015; Wu et al., 2015) but high sparsity in volumetric input increases computation cost,\nlimiting applications to low-resolution point clouds. Multi-view CNN renders 3D objects into im-\nages at different views and processes these images using 2D CNN (Su et al., 2015). VGG and\nResNet-based models show good performance when the training images are well-engineered under\nproper multi-views (Su et al., 2018). However, 2D CNN-based approaches are difﬁcult to scale\nto complex 3D tasks (Qi et al., 2017a). Recently low-complexity point-based models have been\ndesigned to process a point cloud where input is (x, y, z) coordinates of points (Qi et al., 2017a).\nSelf-supervised learning models have also been proposed based on autoencoder and generative ad-\nversarial networks (Wu et al., 2016; Sharma et al., 2016). Both models accept a voxel representation\nof 3D objects and learn the latent representation of the objects by reconstructing the voxel. The\nlearned representation is used as the input to an additional classiﬁer. Our unsupervised learning\ndoes not use labels, but unlike existing self-supervised models, our approach does not reconstruct\nthe objects.\nHebbian Learning Models\nThe variants of the Hebbian learning rule are given as:\nw(t + 1) =\n\n\n\nw(t) + ηyx\nHebb’s rule\nw(t) + ηy(x −yw(t))\nOja’s rule\nw(t) + ηy(x −w(t))\nGrossberg’s rule\n(1)\nwhere x, y, and w are the input, output, and weight, η is the learning rate. Hebb’s rule is the\nbasic form of Hebbian learning where weights are updated if both the input and output neuron\nﬁre (Hebb, 2005). This linear association can be interpreted as biologically plausible principal\ncomponent analysis (PCA) if the data samples are assumed to be zero-mean (Weingessel & Hornik,\n2000). However, the plain Hebb’s rule is often vulnerable to the divergence of weight vectors as\n2\nPublished as a conference paper at ICLR 2023\nFigure 1: 3D object classiﬁcation model.\nthere is no explicit upper bound (Porr & W¨org¨otter, 2007). Oja’s rule has an additional constraint\nterm on Hebb’s rule to normalize the weight vectors (Oja, 1982). It is derived by dividing the weight\nupdate in plain Hebb’s rule by its norm, assuming that the input and output are linearly associated.\nGrossberg’s rule is another variant where a constraint term enables the weight to be converged on the\ninput; hence the weight is more aligned with the frequently observed data (Grossberg, 1976). Recent\nworks have used Grossberg’s rule for image feature extraction (Miconi, 2021; Amato et al., 2019),\nWe can also modify Grossberg’s rule to be identical to Kohonen’s self-organizing map (SOM) by\napplying Winner-Take-All (WTA) to output neurons and can be used for learning two-dimensional or\nthree-dimensional spatial features (Kohonen, 2012; Li et al., 2018). Another variant of Grossberg’s\nrule with the notion of neuron activity is recently studied for learning 2D point sets (Kang et al.,\n2022). In this paper, we mainly compare NeAW learning with Hebb’s rule, Grossberg’s rule, and\nOja’s rule and demonstrate the proposed rule is superior to them in the 3D object classiﬁcation task.\n3\nPROPOSED APPROACH\n3.1\n3D OBJECT CLASSIFICATION MODEL\nWe process (x, y, z) coordinates of points in a point cloud by a shared three-layer MLP with com-\npetition layers. NeAW learning is applied on the MLP (i.e. encoder), and the other layers (i.e.\nclassiﬁer) are trained by supervised learning. The output vector for a point is written by:\nxli+1 = WTA(ReLU(Wli,T xli))\n(2)\nwhere xli, Wli, and xli+1 are the input, weight, and output vector at layer li, respectively. Winner-\ntake-all (WTA) is applied on the output of each layer so that the single output neuron with the highest\nsimilarity (i.e. the lowest ||xli −Wli\n:,j|| for xli+1\nj\n) has a non-zero value while others are forced to\nbe zero (Coates et al., 2011; Hu et al., 2014; Miconi, 2021). Figure 1 shows the architecture of our\nmodel. There are two types of inhibition for the last output neurons in the encoder. Each WTA\nlayer makes lateral inhibition, as shown in the ﬁgure, and cross-point inhibition is applied to the last\noutput neurons after the lateral inhibition. Then, the model aggregates the transformed vector by\nsumming the elements of the remaining neurons across the columns. The cross-inhibition and the\nsummation can be interpreted as a MaxPooling layer that directly ﬁnds the maximum value of each\ncolumn. The proposed model consequently arrives at a PointNet-style model which combines the\nMLP and MaxPooling layer in the encoder design. The aggregated activation of the last neurons,\nthe global feature of objects, is processed by FC layers with ReLU and Softmax activation in the\nclassiﬁer. LayerNorm layers are included between the FC layers. We observe the WTA modules\nplay an important role in the representation learning. The proposed learning in the model without\nthe WTA modules is discussed in Appendix E.\n3.2\nMOTIVATIONAL STUDY\nThe main motivation of the paper is to demonstrate the unsupervised learning rule that can balance\nthe neuron activity and its theoretical analysis in the 3D object classiﬁcation task. Figure 2 shows\n3\nPublished as a conference paper at ICLR 2023\nFigure 2: The average neuron activity of each output neuron for ModelNet10 with (a) NeAW learn-\ning with only positive weight update (Hebbian-like), (b) NeAW learning with only negative weight\nupdate (anti-Hebbian-like), and (c) NeAW learning.\nthe average neuron activity in the last layer of the encoder with different learning rules. Note, the\ndetails of the ﬁgure are explained again in Section 4. We measure how frequently each output neu-\nron activates across different points, so-called neuron activity, to quantify whether the transformed\nvectors are distinguishable in high-dimensional space or merged to a simple global feature. We ob-\nserve that having only positive or negative weight update in the NeAW Hebbian learning lead to few\nspeciﬁc neurons in the last layer to frequently activate regardless of objects. Given input vectors are\nprojected onto weight vectors, the biased activation of output neurons indicates the small number\nof principal weight vectors in high-dimensional space. In other words, multiple input points are\nassociated with few weight vectors, which signiﬁcantly decreases the variance of context vectors\nwith different labels and consequently leads to classiﬁcation failure. Hence, the skewed activity’s\nrelaxation is considered the key problem to solve in this paper.\n3.3\nNEURON ACTIVITY AWARE HEBBIAN LEARNING\nDeﬁnition 1. Neuron activity p(j) of the j-th output neuron is the number of the activation divided\nby the number of entire points. The neuron activity is 1 if the neuron activates for all the points in a\npoint cloud, and 0 if the neuron never activates.\nDeﬁnition 2. Activation boundary is the plane with the same Euclidean distance from two weight\nvectors that determine which neuron of the weight vector to activate.\nOur learning rule is given by:\nW:,j(t + 1) = W:,j(t) + f(p∗−p(j)) η\nN\nN−1\nX\ni=0\n1i ∈arg min\nk\n∥xk−W:,j(t)∥2(xi −W:,j(t))\n(3)\nW:,j(t + 1) = W:,j(t) + f(p∗−p(j)) η\nN (xk −W:,j(t))\n=\n\u001aW:,j(t) + a η\nN (xk −W:,j(t))\nif p∗> p(j)\nW:,j(t) −b η\nN (xk −W:,j(t))\nif p∗< p(j)\n(4)\nwhere W:,j(t) is the weight vector of the j-th output neuron at t, xi is the i-th input vector in a point\ncloud, p∗is the optimal neuron activity. As WTA layers in the proposed model search the closest\nweight vectors from the input vectors, the learning rule is motivated by Grossberg’s rule and the\nprior work (Kang et al., 2022). The optimal neuron activity is given by 1/d where d is the number of\noutput neurons (Kang et al., 2022). The indicator function is considered another WTA. Each weight\nvector searches the closest input vector with regard to Euclidean distance and is moved closer or\nfurther to the input vector depending on the range of p(j). It describes the conversion between\nHebbian and anti-Hebbian learning using the indicator function f, indicating Hebbian learning for\nlow-activity neurons (p∗> p(j)) and anti-Hebbian learning for high-activity neurons (p∗< p(j)).\nNote, the weight is not updated if the neuron activity is optimal. The learning rule is simpliﬁed to\n(4) if the k-th input vector is closest to the weight vector. We introduce a constant a and b in the\nequation to vary the importance of Hebbian and anti-Hebbian learning. In addition to the NeAW\n4\nPublished as a conference paper at ICLR 2023\nFigure 3: Geometry of input and weight vectors. (a) the initial geometry of the vectors and the\nchanged geometry of the vectors after (b) NeAW-H learning, (c) NeAW-aH learning, and (d) NeAW\nlearning.\nHebbian learning rule, we deﬁne NeAW-H learning (a=1 and b=-1) and NeAW-aH learning (a=-1\nand b=1) by making the learning rule only has either positive or negative (i.e. non-hybrid) weight\nupdate regardless of the activity. Note, NeAW Hebbian learning does not refer NeAW-H learning.\nWe observe that balancing the neuron activity is also an important function in the brain (Keck et al.,\n2017; Turrigiano & Nelson, 2004). The biological plausibility of the proposed learning rule is\ndiscussed in Appendix G.\nAn important question is how the hybridization of the learning rules depending on the neuron activity\neffectively relaxes the biased neuron activity. In a simpliﬁed scenario, we can consider two weight\nvectors and an input vector with the assumption that the corresponding neuron of each weight vector\nis highly active or non-active. Also, a and b are assumed to be 1. The problem is whether NeAW\nHebbian learning shifts the activation boundary so that the other neuron is activated after learning.\nTo help the intuitive understanding of how NeAW Hebbian learning changes the activation boundary,\nthe geometry of the three vectors is described in Figure 3. In Figure 3(a), the activation boundary\nas a dotted line shows that input xi is left to the boundary plane, and the j-th neuron is assumed\nto be highly active while the j′-th neuron is not active before learning. We expect that the learning\nproperly shifts the activation boundary to activate the j′-th neuron and relieve the biased activity.\nThe geometry of the vectors after NeAW-H, NeAW-aH, and NeAW Hebbian learning are compared\nin Figure 3(b), (c), and (d), respectively. The activity-agnostic rules move both the weight vectors\nto be closer to or further from the activation boundary, implying the activation boundary cannot be\neffectively shifted to either left or right space. It is desirable that both the weight vectors are also\nshifted in the same direction. In this light, NeAW Hebbian learning offers the appropriate shift of\nthe activation boundary and consequently switches the winner neuron from the highly active j-th\nneuron to the non-active j\n′-th neuron, as shown in Figure 3(d).\nBased on this idea, we develop formal theorems and corollaries to mathematically prove that NeAW\nHebbian learning changes the winner neuron while the others cannot:\nTheorem 1. Let two weight vectors W:,j and W:,j′ with the biased activity and an input vector xi\nsatisfy ∥xi −W:,j(t)∥2 < ∥xi −W:,j′ (t)∥2 at t. Then, NeAW Hebbian learning changes the sign\nof the inequality to ∥xi −W:,j(t + 1)∥2 > ∥xi −W:,j′(t + 1)∥2 at t + 1 if the vectors are on the\ngeometric condition (1 + η\nN )∥xi −W:,j(t)∥2 > (1 −η\nN )∥xi −W:,j′(t)∥2.\nProof Sketch. (Formal proof in Appendix A) Due to the biased activity, each weight vector is updated\nby Hebbian learning or anti-Hebbian learning. Then, we have two update rules, W:,j(t + 1) =\nW:,j(t)−η\nN (xi −W:,j(t)) for the high-active j-th neuron and W:,j′(t+1) = W:,j′(t)+ η\nN (xi −\nW:,j′(t)) for the low-active j\n′-th neuron. From the assumption, we have ∥xi −W:,j(t)∥2 <\n∥xi −W:,j′ (t)∥2, which indicates that the winner neuron is the j-th neuron at t. Using the two\nupdates rule to substitute the weight vectors from t to t + 1, the inequality at t + 1 is derived.\nThe sign in the inequality is ﬂipped if the Euclidean distances satisfy (1 + η\nN )∥xi −W:,j(t)∥2 >\n(1 −η\nN )∥xi −W:,j′(t)∥2.\n5\nPublished as a conference paper at ICLR 2023\nCorollary 1.1. Let two weight vectors W:,j and W:,j′ with the biased activity and an input vector\nxi satisfy ∥xi −W:,j(t)∥2 < ∥xi −W:,j′ (t)∥2 at t. Then, Hebbian learning (NeAW-H) preserves\nthe sign of the inequality to ∥xi −W:,j(t + 1)∥2 < ∥xi −W:,j′(t + 1)∥2 at t + 1.\nCorollary 1.2. Let two weight vectors W:,j and W:,j′ with the biased activity and an input vector\nxi satisfy ∥xi −W:,j(t)∥2 < ∥xi −W:,j′(t)∥2 at t. Then, anti-Hebbian learning (NeAW-aH)\npreserves the sign of the inequality to ∥xi −W:,j(t + 1)∥2 < ∥xi −W:,j′(t + 1)∥2 at t + 1.\nProof for corollaries is in Appendix A.\nThe prior study has shown that Grossberg’s rule combined with WTA does k-means clustering (Hu\net al., 2014). While NeAW Hebbian learning does not aim to minimize the distance between the in-\nput and weight required for the clustering, we can interpret the activated neurons as cluster centroids.\nThe more output neurons the model activates with the balanced activity, the smaller spatial features\nare responsible for each neuron if they create separable clusters. In other words, more activated\nneurons are preferred to avoid the vanishing of local features, which can be achieved by maximizing\nthe variance of the neuron activity. The variance at layer li is written by:\nVar(yli) = 1\nN\nN−1\nX\nk=0\n(yli\nk −µyli) · (yli −µyli)\n= 1\nN\nN−1\nX\nk=0\nyli\nk · yli\nk −1\nN 2\nN−1\nX\nk,w=0\nyli\nk · yli\nw = 1 −1\nN 2\nN−1\nX\nk,w=0\nyli\nk · yli\nw\n(5)\nwhere yli is a binary vector whose element is either 0 or 1. It is obtained from (2) by applying a step\nfunction on it only to consider whether the neuron is activated or not. Also, yli\nk · yli\nk is always 1 as\nthe binary vector has a single non-zero element due to WTA. The proposed NeAW Hebbian learning\neventually aims to increase the variance of the activated neurons so that each neuron is responsible\nfor the different geometric parts of 3D objects by properly rotating the weight vectors through the\nneuron activity aware plasticity, or more formally:\nTheorem 2. For a network of N neurons in the biased activity with weight vectors W and an input\nvector x, the variance of the distribution of activated neurons follows Var(NeAW Hebbian) ≥\nVar(Hebbian)orVar(anti −Hebbian) which means NeAW Hebbian learning provides the greatest\nvariance of the neuron activity compared to solely using Hebbian or anti-Hebbian learning.\nProof Sketch. (Formal proof in Appendix A) First, we note that Hebbian (H) and anti-Hebbian (aH)\nnetworks optimize the information capacity by orthonormalizing the principal subspace (Plumbley,\n1993). To prove that the neuronal activations for the hybrid learning rule have a greater variance\nthan the standard H and aH learning rules, we ﬁrst assume that a network with a greater number\nof neuronal activations can encode a large number of principal components in the subspace. We\nconsider that the local H and aH learning rules recover a principal subspace from the data using a\nprincipled cost function (Pehlevan et al., 2015). Hence, we rewrite the problem as:\nspanmin(LNeAW ) ≥spanmin(LH) or spanmin(LaH)\n(6)\nwhere LH and LaH denote, the subspace learned using the Hebbian and anti-Hebbian learning rules,\nrespectively. Let us consider the case where we have N neurons learned using H or aH learning\nrules. Without loss of generality, we can assume that the ﬁrst n (n < N) neurons are trained\nusing the learning method L and the metric space of the neuron vectors and the weight vectors to\nbe represented as W. For the n + 1-th neuron, the neuron can be trained using either the same\nlearning rule L or the complementary learning rule ¯L. Thus, we study the differential gain from\nusing one learning rule over the other for the n + 1-th neuron. A learning rule extracts the principal\ncomponents of the input space by projecting the input space into the orthogonal subspace, and a\nneural network with a larger number of principal components can learn a better representation of\nthe input space. Again, H and aH learning rules lead to different principal components (Rubner &\nTavan, 1989; Carlson, 1990). Hence, we show that using NeAW Hebbian learning approach leads\nto an increase in the number of the principal components learned, which leads to a greater span of\nthe learned subspace for the hybrid learning model compared to the homogeneous H or aH learning\nmethod, which, in turn, entails a higher variance for the neuronal activity.\n6\nPublished as a conference paper at ICLR 2023\nFigure 4: t-SNE for ModelNet10 in (a) Hebb’s rule, (b) Grossberg’s rule, (c) Oja’ rule, and (d)\nNeAW learning.\nFigure 5: Effect of training with regard to the variance of neuron activity and test accuracy. (a) the\nvariance of neuron activity with the different learning rules in ModelNet10 and (b) ModelNet40 and\n(c) the test accuracy at different epochs for ModelNet10 and (d) ModelNet40.\n4\nEXPERIMENTAL RESULT\nExperimental setting\nThe proposed model is evaluated on ModelNet10 and ModelNet40 datasets,\nwhich include 10-class and 40-class 3D CAD objects for 3D deep learning (Wu et al., 2015). We\nrandomly sample 1024 points on the surface of the objects. The encoder is trained by unsupervised\nlearning with unlabeled datasets while the classiﬁer is trained by supervised learning with object\nlabels. Note, the weights of the encoder are not changed during training the classiﬁer. For all the\nunsupervised learning rules, we set training epoch 50, learning rate 1e-2, and train batch 4; however,\nlearning rate is proportionally increased when the amount of training data is limited. For example,\nlearning rate is 1e-1 for 10% training data and 4e-2 for 25% training data. For the classiﬁer, we use\ntraining epoch 100, learning rate 1e-3, and train batch 32 for both ModelNet10 and ModelNet40.\nWe report average instance accuracy for test datasets.\nBalancing of Neuron Activity\nFigure 2 shows the neuron activity of each output neuron for the\nactivity-agnostic and activity-aware learning in the same model. x-axis in the ﬁgure is the index of\noutput neurons, and y-axis is the normalized number of the neuron activation for ModelNet10 test\ndataset. The activity is ﬁrst summed over the all test samples and then normalized by the entire\nactivities. In NeAW-H and NeAW-aH learning cases, the activity is dominated by few neurons,\nimplying only few neurons are used to represent the objects. On the other hand, NeAW learning\nclearly displays the more distributed activity than activity-agnostic cases. In addition, the activity\nlevel increases for non-active neurons and decreases for high-active neurons, enabling more output\nneurons contribute to the representation. We also study how the neuron activity differs depending\non the object classes. Figure 10 and 11 in Appendix C show the average neuron activity for the 10\nobject classes in NeAW learning and NeAW-H and NeAW-aH learning. It displays that the neurons\nin NeAW learning object-dependently activate, thereby, increasing the dissimilarity between the\nobjects in the latent space. However, the neurons in the NeAW-H and NeAW-aH learning often\nactivate regardless of the object classes. We study the correlation between the object-dependent\nactivation and better representation learning in Appendix B. Also, the causal link between the biased\nactivity and poor learning is studied in detail.\n7\nPublished as a conference paper at ICLR 2023\nTable 1: Comparison with other unsupervised learning rules.\nLearning Rule\nModelNet-10\nModelNet-40\nVariance\nAccuracy (%)\nVariance\nAccuracy (%)\nHebb’s rule\n0.9663\n72.22 ± 0.46\n0.9391\n41.52 ± 1.21\nGrossberg’s rule\n0.8714\n74.45 ± 0.56\n0.8693\n55.96 ± 0.39\nOja’s rule\n0.9182\n80.40 ± 0.03\n0.8953\n61.22 ± 3.82\nOurs - NeAW-H\n0.9165\n74.19 ± 0.21\n0.9251\n49.83 ± 4.72\nOurs - NeAW-aH\n0.8624\n64.63 ± 0.60\n0.8511\n35.65 ± 0.80\nOurs - NeAW\n0.9793\n88.22 ± 0.62\n0.9741\n76.20 ± 0.30\nEvolution of Neuron Activity during Unsupervised Learning\nWe study the evolution of neuron\nactivity and test accuracy as a function of training epochs of the unsupervised encoder. In all the\ncases, the unsupervised encoder is frozen after training with Hebbian learning for certain number\nof epochs, followed by full supervised training of the classiﬁer. Figure 5 shows the variance of\nneuron activity during training of the model. We observe that variance is quickly saturated to a\nhigh value in NeAW Hebbian learning. In comparison, the variance increases slowly and to a lower\nvalue with NeAW-H learning and NeAW-aH learning, respectively. Figure 5(c) and (d) represent\nthe test accuracy for ModelNet10 and ModelNet40 at different training epochs with NeAW Hebbian\nlearning. We observe the variance and accuracy converge in 50 training epochs.\nComparison with other Hebbian learning\nFigure 4 show t-SNE plots for the Hebb’s rule, Gross-\nberg’s rule, Oja’s rule, and NeAW learning. It qualitatively indicates that the NeAW learning is su-\nperior to the other Hebbian learning rules as the context vectors are well clustered with their labels.\nWe also study the neuron activity distribution shown in Figure 2 for the other rules. The neuron ac-\ntivity of the NeAW learning is also well balanced than the other rules (see Figure 9 in Appendix C.)\nThe Grossberg’s rule and Oja’s rule still display the skewed neuron activity dominated by the few\nneurons while the Hebb’s rule achieves the relatively balanced activity. However, we observe that\nthe Hebb’s rule has many neurons that activate regardless of the object, indicating the less object-\ndependent activation than the Grossberg’s rule and Oja’s rule as shown in Figure 12 in Appendix C.\nTable 1 compares the variance of the neuron activity and test accuracy with the plain Hebb’s rule,\nGrossberg’s rule, Oja’s rule, and NeAW Hebbian learning. The test accuracy is evaluated on the\ntrained models with 5 different random seeds, and the mean and variance of the accuracy are listed\nin the table. We observe the NeAW Hebbian learning shows the highest variance of the neuron\nactivity and improved test accuracy in both the datasets. In particular, the accuracy gap increases\nin ModelNet40, implying the potential pitfalls of the other Hebbian learning rules on complex 3D\nobject datasets. Note, though the plain Hebb’s rule show the higher variance of the neuron activity,\nwhich indicates the balanced activity, than the Oja’s rule, the test accuracy is lower. This is due to\nthe unbounded norm of the weight vectors. In other words, instead of learning the points that distin-\nguish objects with different labels, commonly appeared points in the training samples continuously\nstrengthens the certain weights (see Figure 20 in Appendix F.) It can increase the point-to-point\nvariation but sacriﬁce local features.\nComparison with Existing Supervised Learning Models\nTable 2 compares the proposed model\nwith prior models for 3D object recognition. First, our model shows similar accuracy to other\nself-supervised model (Wu et al., 2016; Sharma et al., 2016). Accuracy difference with 3D-GAN\nincreases in ModelNet40 but 3D-GAN needs to reconstruct a (30, 30, 30) voxel to learn the represen-\ntation resulting in high computational cost for high-resolution 3D objects. Our model shows similar\naccuracy to other fully supervised voxel-based approaches while using 6.2× fewer parameters (Wu\net al., 2015). The accuracy difference with other large point-based approaches is more pronounced\nin ModelNet40 but they use 1.8× to 4.3× more parameters than our model. We also compare un-\nsupervised version and a fully supervised version (i.e., the encoder is trained with backpropagation\ninstead of Hebbian learning) of our model. The proposed unsupervised version shows a marginally\nlower accuracy from the supervised version. In addition, we observe that the unsupervised learning\npreserves the accuracy well in smaller models than the supervised learning. Figure 16 in Appendix\nE shows that 0.09MB unsupervised model achieves 72.5% for ModelNet40 while the supervised\n8\nPublished as a conference paper at ICLR 2023\nTable 2: Overall accuracy (%) and model size comparison with supervised models.\nModel\nLearning\nModelNet10\nModelNet40\nSize (MB)\n3DShapeNet (Wu et al., 2015)\nSupervised\n83.5\n77.0\n19.2\nPointNet (Qi et al., 2017a)\nSupervised\n-\n89.2\n13.2\nPointNet++ (Qi et al., 2017b)\nSupervised\n-\n91.9\n5.6\nDGCNN (Wang et al., 2019)\nSupervised\n-\n92.9\n6.9\nVConv-DAE (Sharma et al., 2016)\nSelf-supervised\n80.5\n75.5\n-\n3D-GAN (Wu et al., 2016)\nSelf-supervised\n91.0\n83.3\n-\nOurs - Backpropagation\nSelf-supervised\n92.2\n79.5\n4.1\nOurs - Backpropagation\nSupervised\n91.1\n77.2\n3.1\nOurs - NeAW Hebbian\nUnsupervised\n88.2\n76.2\n3.1\nFigure 6: Comparison with other learning rules in limited training data. (a) the accuracy of end-\nto-end supervised learning (backpropagation) and NeAW Hebbian learning for ModelNet10 and (c)\nModelNet40, (b) and the accuracy of other unsupervised learning for ModelNet10 and (d) Model-\nNet40.\nlearning in the same model shows 59.9%. Our model also includes self-supervised learning while\nhaving more parameters and computational costs (Appendix E).\nLearning with Less Training Data.\nWe study the performance of NeAW Hebbian as the amount\nof the training samples decreases (Figure 6). We randomly drop samples from each class based on\nthe percentage amount of the labeled data. For the other Hebbian learning rules, NeAW Hebbian\nlearning always shows higher accuracy in both ModelNet10 and ModelNet40. As shown in Table 2,\nNeAW Hebbian learning achieves marginally lower accuracy to fully supervised model (denoted as\nBackpropagation) when the all data is used for training. However, in all other cases with less than\n25% labeled data, NeAW Hebbian learning shows higher performance than Backpropagation. Our\nmodel also shows better performance than alternative Hebbian rules at reduced training data.\n5\nCONCLUSION\nWe present the neuron activity aware (NeAW) plasticity in Hebbian learning that combines Hebbian\nand anti-Hebbian learning to relieve the biased neuron activity. Multi theorems and corollaries are\ndeveloped to understand how NeAW Hebbian learning relieves the biased neuron activity and the\ncorrelation between the balanced activity and better representation learning in the classiﬁcation task.\nExperimental results demonstrate that NeAW Hebbian learning achieves higher accuracy than other\nHebbian learning rules and supervised learning with limited training data. We believe the paper\nprovides a theoretical understanding of how neuron activity can be used for modulating the learning\nrule and its application to unsupervised 3D learning.\n9\nPublished as a conference paper at ICLR 2023\nACKNOWLEDGMENTS\nThis material is based on work sponsored by the Ofﬁce of Naval Research under Grant Number\nN00014-20-1-2432. The views and conclusions contained in this document are those of the authors\nand should not be interpreted as representing the ofﬁcial policies, either expressed or implied, of the\nOfﬁce of Naval Research or the U.S. Government.\nREFERENCES\nPanos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representa-\ntions and generative models for 3d point clouds. In International conference on machine learning,\npp. 40–49. PMLR, 2018.\nGiuseppe Amato, Fabio Carrara, Fabrizio Falchi, Claudio Gennaro, and Gabriele Lagani. Hebbian\nlearning meets deep convolutional neural networks. In International Conference on Image Anal-\nysis and Processing, pp. 324–334. Springer, 2019.\nYanis Bahroun and Andrea Soltoggio. Online representation learning with single and multi-layer\nhebbian networks for image classiﬁcation. In International Conference on Artiﬁcial Neural Net-\nworks, pp. 354–363. Springer, 2017.\nYanis Bahroun, Eug´enie Hunsicker, and Andrea Soltoggio. Building efﬁcient deep hebbian networks\nfor image classiﬁcation tasks. In International conference on artiﬁcial neural networks, pp. 364–\n372. Springer, 2017.\nArthur Carlson. Anti-hebbian learning in a non-linear neural network. Biological cybernetics, 64\n(2):171–176, 1990.\nAdam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised\nfeature learning. In Proceedings of the fourteenth international conference on artiﬁcial intelli-\ngence and statistics, pp. 215–223. JMLR Workshop and Conference Proceedings, 2011.\nRenata LME do Rˆego, Aluizio FR Araujo, and Fernando B de Lima Neto. Growing self-organizing\nmaps for surface reconstruction from unstructured point clouds. In 2007 International Joint Con-\nference on Neural Networks, pp. 1900–1905. IEEE, 2007.\nRohit Girdhar, David F Fouhey, Mikel Rodriguez, and Abhinav Gupta. Learning a predictable and\ngenerative vector representation for objects. In European Conference on Computer Vision, pp.\n484–499. Springer, 2016.\nStephen Grossberg. Adaptive pattern classiﬁcation and universal recoding: I. parallel development\nand coding of neural feature detectors. Biological cybernetics, 23(3):121–134, 1976.\nDonald Olding Hebb. The organization of behavior: A neuropsychological theory. Psychology\nPress, 2005.\nXiaolin Hu, Jianwei Zhang, Peng Qi, and Bo Zhang. Modeling response properties of v2 neurons\nusing a hierarchical k-means model. Neurocomputing, 134:198–205, 2014.\nBeomseok Kang, Harshit Kumar, Saurabh Dash, and Saibal Mukhopadhyay. Unsupervised hebbian\nlearning on point sets in starcraft ii. arXiv preprint arXiv:2207.12323, 2022.\nTara Keck, Taro Toyoizumi, Lu Chen, Brent Doiron, Daniel E Feldman, Kevin Fox, Wulfram Ger-\nstner, Philip G Haydon, Mark H¨ubener, Hey-Kyoung Lee, et al. Integrating hebbian and homeo-\nstatic plasticity: the current state of the ﬁeld and future research directions. Philosophical Trans-\nactions of the Royal Society B: Biological Sciences, 372(1715):20160158, 2017.\nTrupti M Kodinariya and Prashant R Makwana. Review on determining number of cluster in k-\nmeans clustering. International Journal, 1(6):90–95, 2013.\nTeuvo Kohonen. Self-organization and associative memory, volume 8. Springer Science & Business\nMedia, 2012.\n10\nPublished as a conference paper at ICLR 2023\nJiaxin Li, Ben M Chen, and Gim Hee Lee. So-net: Self-organizing network for point cloud analysis.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9397–\n9406, 2018.\nThomas Limbacher and Robert Legenstein. H-mem: Harnessing synaptic plasticity with hebbian\nmemory networks. Advances in Neural Information Processing Systems, 33:21627–21637, 2020.\nDaniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time\nobject recognition. In 2015 IEEE/RSJ international conference on intelligent robots and systems\n(IROS), pp. 922–928. IEEE, 2015.\nThomas Miconi.\nMulti-layer hebbian networks with modern deep learning frameworks.\narXiv\npreprint arXiv:2107.01729, 2021.\nThomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic neural\nnetworks with backpropagation. In International Conference on Machine Learning, pp. 3559–\n3568. PMLR, 2018.\nVilson Luiz Dalle Mole and Aluizio Fausto Ribeiro Ara´ujo. Growing self-organizing surface map:\nLearning a surface topology from a point cloud. Neural computation, 22(3):689–729, 2010.\nElias Najarro and Sebastian Risi. Meta-learning through hebbian plasticity in random networks.\nAdvances in Neural Information Processing Systems, 33:20719–20731, 2020.\nErkki Oja. Simpliﬁed neuron model as a principal component analyzer. Journal of mathematical\nbiology, 15(3):267–273, 1982.\nCengiz Pehlevan, Tao Hu, and Dmitri B Chklovskii. A hebbian/anti-hebbian neural network for\nlinear subspace learning: A derivation from multidimensional scaling of streaming data. Neural\ncomputation, 27(7):1461–1495, 2015.\nMark D Plumbley. A hebbian/anti-hebbian network which optimizes information capacity by or-\nthonormalizing the principal subspace. In 1993 Third International Conference on Artiﬁcial Neu-\nral Networks, pp. 86–90. IET, 1993.\nBernd Porr and Florentin W¨org¨otter. Learning with “relevance”: using a third factor to stabilize\nhebbian learning. Neural computation, 19(10):2694–2719, 2007.\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets\nfor 3d classiﬁcation and segmentation. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 652–660, 2017a.\nCharles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical fea-\nture learning on point sets in a metric space. Advances in neural information processing systems,\n30, 2017b.\nJeanne Rubner and Paul Tavan. A self-organizing network for principal-component analysis. EPL\n(Europhysics Letters), 10(7):693, 1989.\nNicol Schraudolph and Terrence J Sejnowski. Competitive anti-hebbian learning of invariants. Ad-\nvances in Neural Information Processing Systems, 4, 1991.\nAbhishek Sharma, Oliver Grau, and Mario Fritz. Vconv-dae: Deep volumetric shape learning with-\nout object labels. In European conference on computer vision, pp. 236–250. Springer, 2016.\nAndrea Soltoggio, Peter Durr, Claudio Mattiussi, and Dario Floreano. Evolving neuromodulatory\ntopologies for reinforcement learning-like problems. In 2007 IEEE Congress on evolutionary\ncomputation, pp. 2471–2478. IEEE, 2007.\nHang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convo-\nlutional neural networks for 3d shape recognition.\nIn Proceedings of the IEEE international\nconference on computer vision, pp. 945–953, 2015.\n11\nPublished as a conference paper at ICLR 2023\nJong-Chyi Su, Matheus Gadelha, Rui Wang, and Subhransu Maji. A deeper look at 3d shape clas-\nsiﬁers. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pp.\n0–0, 2018.\nGina G Turrigiano and Sacha B Nelson. Homeostatic plasticity in the developing nervous system.\nNature reviews neuroscience, 5(2):97–107, 2004.\nYue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon.\nDynamic graph cnn for learning on point clouds. Acm Transactions On Graphics (tog), 38(5):\n1–12, 2019.\nSatosi Watanabe and Nikhil Pakvasa. Subspace method of pattern recognition. In Proc. 1st. IJCPR,\npp. 25–32, 1973.\nAndreas Weingessel and Kurt Hornik. Local pca algorithms. IEEE Transactions on Neural Net-\nworks, 11(6):1242–1250, 2000.\nJiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a proba-\nbilistic latent space of object shapes via 3d generative-adversarial modeling. Advances in neural\ninformation processing systems, 29, 2016.\nZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong\nXiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 1912–1920, 2015.\nZhenyue Zhang and Yuqing Xia. Minimal sample subspace learning: Theory and algorithms. J.\nMach. Learn. Res., 20:143–1, 2019.\nYanpeng Zhou, Maosen Wang, Manas Gupta, Arulmurugan Ambikapathi, Ponnuthurai Nagaratnam\nSuganthan, and Savitha Ramasamy. Investigating robustness of biological vs. backprop based\nlearning. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 3533–3537. IEEE, 2022.\n12\nPublished as a conference paper at ICLR 2023\nA\nPROOF OF THEOREM\nTheorem 1. Let two weight vectors W:,j and W:,j′ with the biased activity and an input vector xi\nsatisfy ∥xi −W:,j(t)∥2 < ∥xi −W:,j′ (t)∥2 at t. Then, NeAW Hebbian learning changes the sign\nof the inequality to ∥xi −W:,j(t + 1)∥2 > ∥xi −W:,j′(t + 1)∥2 at t + 1 if the vectors are on the\ngeometric condition (1 + η\nN )∥xi −W:,j(t)∥2 > (1 −η\nN )∥xi −W:,j′(t)∥2.\nProof. Let’s say there are an input vector xi, corresponding winner is the j-th output neuron (i.e.\nj = arg min\nk\n∥xi −W:,k(t)∥), and the other loser is the j\n′-th output neuron. We are interested in\nhow the hybrid update changes the activation boundary. Our learning rule is given by:\nW:,j(t + 1) = W:,j(t) + f(p∗−p(j)) η\nN\nN−1\nX\ni=0\n1i ∈arg min\nk\n∥xk−W:,j(t)∥2(xi −W:,j(t))\n(7)\nGiven there is a single input vector, the learning rule can be simpliﬁed by:\nW:,j(t + 1) = W:,j(t) + f(p∗−p(j)) η\nN (xi −W:,j(t))\n(8)\nAs we are interested in the hybrid update relieves the biased activity such as too high activity or zero\nactivity, the j-th and j\n′-th neuron are assumed to have higher p(j) and lower p(j\n′) than p∗. Then,\nwith an assumption that a and b are 1 to simplify the case, the equation is again written as:\nW:,j(t + 1) = W:,j(t) −η\nN (xi −W:,j(t))\n(9)\nW:,j′(t + 1) = W:,j′(t) + η\nN (xi −W:,j′(t))\n(10)\nOur goal is to show that the learning rules change the activation boundary. At t before learning, the\nEuclidean distance for the j-th neuron is lower than the j\n′-th neuron, or more formally:\n∥xi −W:,j(t)∥2\n2 < ∥xi −W:,j′(t)∥2\n2\n(11)\n∥xi −W:,j(t)∥2\n2 −∥xi −W:,j′ (t)∥2\n2 < 0\n(12)\n(W:,j(t) + W:,j′(t) −2xi) · (W:,j(t) −W:,j′(t)) < 0\n(13)\nAt t + 1 after learning, the left-hand side of (13) is written as:\n(W:,j(t + 1) + W:,j′(t + 1) −2xi) · (W:,j(t + 1) −W:,j′(t + 1))\n(14)\nThe hybrid update changes the winner neuron if it is positive. Using (9) and (10), (14) can be\nre-written as:\n{W:,j(t) + W:,j′(t) −2xi + η\nN (W:,j(t) −W:,j′ (t))}·\n{W:,j(t) −W:,j′(t) −η\nN (2xi −W:,j(t) −W:,j′ (t))}\n(15)\n4η\nN ∥xi −W:,j(t)∥2\n2 + (1 −η\nN )2(W:,j(t) + W:,j′(t) −2xi) · (W:,j(t) −W:,j′(t))\n(16)\n13\nPublished as a conference paper at ICLR 2023\nSubstituting the second term in (16) by the left-hand side of (12),\n(1 + η\nN )2∥xi −W:,j(t)∥2\n2 −(1 −η\nN )2∥xi −W:,j′(t)∥2\n2\n(17)\nEquation (14) is positive if\n∥xi −W:,j′(t)∥2\n2\n∥xi −W:,j(t)∥2\n2\n< (1 + η\nN )2\n(1 −η\nN )2\n(18)\n■\nCorollary 1.1. Let two weight vectors W:,j and W:,j′ with the biased activity and an input vector\nxi satisfy ∥xi −W:,j(t)∥2 < ∥xi −W:,j′(t)∥2 at t. Then, Hebbian learning (NeAW-H) preserves\nthe sign of the inequality to ∥xi −W:,j(t + 1)∥2 < ∥xi −W:,j′(t + 1)∥2 at t + 1.\nProof. Similar to the proof of Theorem 1, we need to show the equation below is positive if Hebbian\nlearning changes the winner neuron:\n(W:,j(t + 1) + W:,j′(t + 1) −2xi) · (W:,j(t + 1) −W:,j′(t + 1))\n(19)\nWe have Hebbian learning rules for both the j-th and j\n′-th neuron as:\nW:,j(t + 1) = W:,j(t) + η\nN (xi −W:,j(t))\n(20)\nW:,j′(t + 1) = W:,j′(t) + η\nN (xi −W:,j′(t))\n(21)\nUsing the learning rules to re-write (19),\n{W:,j(t) + W:,j′(t) −2xi + η\nN (2xi −W:,j(t) −W:,j′(t))}·\n{W:,j(t) −W:,j′(t) −η\nN (W:,j(t) −W:,j′(t))}\n(22)\n(1 −η\nN )2(W:,j(t) + W:,j′(t) −2xi) · (W:,j(t) −W:,j′(t))\n(23)\nAs we assume\n(W:,j(t) + W:,j′(t) −2xi) · (W:,j(t) −W:,j′(t)) < 0\n(24)\nEquation (19) is always negative. Thus Hebbian learning cannot change the winner neuron.\n■\nCorollary 1.2. Let two weight vectors W:,j and W:,j′ with the biased activity and an input vector\nxi satisfy ∥xi −W:,j(t)∥2 < ∥xi −W:,j′(t)∥2 at t. Then, anti-Hebbian learning (NeAW-aH)\npreserves the sign of the inequality to ∥xi −W:,j(t + 1)∥2 < ∥xi −W:,j′(t + 1)∥2 at t + 1.\nProof. Similar to the proof of Theorem 1, we need to show the equation below is positive if anti-\nHebbian learning changes the winner neuron:\n(W:,j(t + 1) + W:,j′(t + 1) −2xi) · (W:,j(t + 1) −W:,j′(t + 1))\n(25)\nWe have anti-Hebbian learning rules for both the j-th and j\n′-th neuron as:\n14\nPublished as a conference paper at ICLR 2023\nW:,j(t + 1) = W:,j(t) −η\nN (xi −W:,j(t))\n(26)\nW:,j′(t + 1) = W:,j′(t) −η\nN (xi −W:,j′(t))\n(27)\nUsing the learning rules to re-write (25),\n{W:,j(t) + W:,j′(t) −2xi + η\nN (2xi −W:,j(t) −W:,j′(t))}·\n{W:,j(t) −W:,j′(t) −η\nN (W:,j(t) −W:,j′(t))}\n(28)\n(1 −η\nN )2(W:,j(t) + W:,j′(t) −2xi) · (W:,j(t) −W:,j′(t))\n(29)\nAs we assume\n(W:,j(t) + W:,j′(t) −2xi) · (W:,j(t) −W:,j′(t)) < 0\n(30)\nEquation (25) is always negative. Thus anti-Hebbian learning cannot change the winner neuron.\n■\nTheorem 2. For a network of N neurons in the biased activity with weight vectors W and an in-\nput vector x, the variance of the distribution of activated neurons follows Var(NeAW Hebbian) ≥\nVar(Hebbian)orVar(anti −Hebbian) which indicates NeAW Hebbian learning provides the great-\nest variance of the neuron activity compared to solely using Hebbian or anti-Hebbian learning.\nProof. First, we note the results shown by Plumbley (1993), that Hebbian/anti-Hebbian networks\noptimize the information capacity by orthonormalizing the principal subspace. In order to prove that\nthe neuronal activations for the hybrid learning rule have a greater variance than the normal Hebbian\nor anti-Hebbian learning rules, we ﬁrst assume that a network with a greater number of neuronal\nactivations can encode a greater number of principal components in the subspace. We consider that\nthe local Hebbian and anti-Hebbian learning rules recover a principal subspace from the data using\na principled cost function as shown by Pehlevan et al. (2015).\nThe subspace method of classiﬁcation is pattern recognition method where the primary model for\na class is a linear subspace of the Euclidean pattern space (Watanabe & Pakvasa, 1973). Let ui ∈\nRn, i = 1 . . . m, m < n be a set of m linearly independent vectors that spans the subspace L as:\nP = P (u1, . . . , um) =\n(\nx | x =\nm\nX\ni=1\nαiui\n)\nwhere αi is some scalar. That is, essentially, the problem can be re-written as:\nVar(NeAW Hebbian) ≥Var(Hebbian) or Var(anti −Hebbian)\n⇒spanmin(LNeAW Hebbian) ≥spanmin(LHebbian) or spanmin(Lanti−Hebbian)\n(31)\nwhere spanmin is the minimum spanning subspace, i.e., the subspace does not have a smaller sub-\nspace spanned by a subset of the samples. For a detailed explanation of minimal sample subspace\nrefer to the works of Zhang & Xia (2019).\nAs shown before, the learning rules for the Hebbian and anti-Hebbian cases are given, respectively\nas:\nWHebb\n:,j (t + 1) = WHebb\n:,j (t) + η\nN (xi −W:,j(t))\nWanti-Hebb\n:,j\n(t + 1) = Wanti-Hebb\n:,j\n(t) −η\nN (xi −W:,j(t))\n(32)\n15\nPublished as a conference paper at ICLR 2023\nNow, let us consider the case where we have N neurons which are learned using Hebbian or anti-\nHebbian learning rules. We aim to bifurcate the sets of trained neurons into Hebbian and anti-\nHebbian classes. Without loss of generality, we can assume that the ﬁrst n (n < N) neurons are\ntrained using the a learning method L ∈{LH = Hebbian, LaH = anti-Hebbian}. Mathematically,\nwe do a binary partition of the metric space of the neurons and the weights. If we consider the\nmetric space of the neuron vectors and the weight vectors to be represented as W, we deﬁne the\npartition from X →Y, Z where X denotes the space of all the vectors, Y denotes the space of\nvectors where all the neurons have already learnt (which is interpreted as a coloring of the vector)\nusing learning algorithm L. Z denotes the space of vectors where none of the neurons have been\nassigned a learning rule, i.e., they can learn using either LH or LaH. We denote the span of the\nsubspace learned using the learning method L is denoted as span(xL\n1 , . . . , xL\nn).\nNow, let us consider the n+1-th neuron. There might be two cases that might arise: (i) the neuron is\ntrained using the same learning rule L with which the previous n neurons are trained. (ii) the neuron\nis trained using the complementary learning rule ¯L. Thus, we study the differential gain by using\none learning rule over the other for the n + 1-th neuron.\nWe know that a learning rule basically extracts the principal components of the input space by\nprojecting the input space into the orthogonal subspace. Also, we know that a neural network with\na richer subspace, i.e., a subspace with a larger number of orthogonal principal components, can\nlearn a better representation of the input space than a subspace with a fewer number of orthogonal\nprincipal components.\nAgain, let us consider the subspaces learned by the Hebbian LH and the anti-Hebbian LaH learning\nrules. From previous works (Rubner & Tavan, 1989; Carlson, 1990; Schraudolph & Sejnowski,\n1991) we know that anti-Hebbian and Hebbian learning rules lead to different principal components.\nAs described by Schraudolph & Sejnowski (1991) while the built-in temporal smoothness constraint\nenables Hebbian neurons to learn the invariance classes, anti-Hebbian synapses have been used\nfor lateral decorrelation of feature detectors and removal of temporal variations from the input.\nA set of Hebbian feature detectors whose weight vectors span the hyperplane would characterize\nthe associated class of stimuli. The anti-Hebbian learning algorithm, however, provides a more\nefﬁcient representation when the dimensionality of the hyperplane is more than half of the input\nspace. This is because, in that case, we require fewer normal vectors than spanning vectors for\nunique characterization. Since anti-Hebbian neurons remove the variance within a stimulus class,\nthey present a different output representation to subsequent layers than the Hebbian neurons. Thus,\nsince the linear subspaces learned by the two learning rules are distinct, we can say that:\nspan(xL\n1 , . . . , xL\nn ∪xL\nn+1) ⊆span(xL\n1 , . . . , xL\nn ∪x\n¯\nL\nn+1)\n⇒span(Hebbian) or span(anti-Hebbian) ⊆span(NeAW Hebbian)\n(33)\nThus, we see that using a hybrid Hebbian learning rule increases the span of the subspace learned,\ni.e., increases the variance of the neuron activations.\n■\nCorollary 2.1. Hebbian or anti-Hebbian learning on the biased neuron activity leads to a poor\nsubspace representation than NeAW Hebbian learning\nProof. As we showed before in Theorem 2 in (33), we see that the span of the subspace learned\nby the NeAW Hebbian learning algorithm is greater than the span of the homogeneous Hebbian\nor anti-Hebbian learning method. Let us label each neuron as ’H’ or ’aH’ for Hebbian or anti-\nHebbian learning methods depending on the learning algorithm it selects. Now, let us denote the\ndistribution of the nodes marked ’H’ and the distribution of the nodes marked ’aH’ in the vector\nspace of the neuron weights. We notice that since the total number of neurons is ﬁxed, the total\nprobability mass of these two distributions is also ﬁxed. Thus, if there are a lot more ’H’ neurons, the\nprobability mass of the ’aH’ neurons is lower - i.e., for the set of neurons ni ∈N, P[P\ni∈H∼N ni] >\nP[P\ni∈aH∼N ni]. Now, considering the full set of principal components that the entire network can\nlearn using an inﬁnite number of neurons. As we keep decreasing the number of neurons, we keep\nlearning a subset of this ideal set of principal components. Since the sum of probability masses\nof the Hebbian and anti-Hebbian learning algorithms are ﬁxed, if the distribution is heavily biased\n16\nPublished as a conference paper at ICLR 2023\ntowards just Hebbian learning, then the principal axes that could be learned from the anti-Hebbian\nlearning are not present. Thus, if we represent the principal axes of the learning rule L as aL,\nthen spanmin(P aX) ⊆spanmin(P aNeAW), where X represents either Hebbian or anti-Hebbian\nlearning neurons. Hence, a biased learning algorithm leads to a poorer selection of the principal\naxes, which leads to worse performance.\n■\nB\nCAUSAL RELATION BETWEEN NEURON ACTIVITY AND REPRESENTATION\nLEARNING\nFigure 7: (a) Dissimilarity matrix for ModelNet10 after NeAW learning, (b) L2 norm change of\nthe dissimilarity matrix by deactivating a neuron in NeAW leanring, and (c) dissimilarity matrix\nafter learning using Hebb’s rule. The x-axis in (b) indicates the variance of the deactivated neuron’s\nactivity for different object classes, and the y-axis is the L2 norm change of the dissimilarity matrix\nby the deactivation.\nCausal Relation between the Biased Activity and Poor Learning\nHebbian-like unsupervised\nlearning rules with Winner-Take-All (WTA) can be interpreted as a clustering module. We focus\non the factors that affect the quality of clustering to build a causal connection between the neuron\nactivity and the performance. It is well-known that (1) the number of cluster centroids and (2) their\nseparability play a signiﬁcant role in determining the quality of clustering (Kodinariya & Makwana,\n2013). If there are too few cluster centroids, irrelevant points will be associated with the same\ncluster centroid. Likewise, if the cluster centroids are not separable, i.e., multiple centroids are\npositioned at the same location, the effective number of clusters will be still low. In the end, we\nneed the enough number of well separable cluster centroids that behave as principal components for\ndifferent local features in the object. In Corollary 2.1 (Appendix A), we have proved that Hebbian\nand anti-Hebbian learning leads to biased neuron activity and hence, fewer principal components\nthan NeAW learning. Fewer principal components indicate fewer separable cluster centroids which\nleads to poor performance. Hence, using Corollary 2.1 we can state that a biased activity in the\nHebbian and anti-Hebbian learning causes a poor performance. That is, the biased activity problem\nshould be resolved to improve the performance. However, it does not necessarily indicate that the\nbalanced activity leads to a better performance.\nCorrelation between the Balanced Activity and Better Clustering\nThe causal link between bi-\nased activity and poor performance suggests that reducing activity bias may improve performance.\nHowever, it does not necessarily indicate that the only balancing activity causes a better perfor-\nmance. In particular, we observe that the “balanced activity with object-dependent activation” is\ncorrelated with the clustering quality. The idea behind is that the neurons that activate regardless of\nthe object classes (no object dependency) will not capture the features to design the classiﬁcation\nboundary. In this light, we design an additional experiment to quantify the change of the clustering\nquality depending on how dependent neurons are to different object classes. We quantify the quality\nusing the dissimilarity matrix. The dissimilarity matrix for ModelNet10 is given by 10x10 matrix,\nwhere each element is the dissimilarity between the encoded vectors of the corresponding two object\n17\nPublished as a conference paper at ICLR 2023\nclasses (see Figure 7(a).) We choose cosine similarity to calculate the dissimilarity. Let xA and xB\nfor the encoded vector for A and B objects. Then, the dissimilarity in the matrix (D) is given by\nD[A][B] = 1 −\nxA·xB\n|xA||xB|. We calculate the change of the L2 norm of the dissimilarity matrix by\ndeactivating neurons one by one. The experiment results demonstrate that deactivating the neuron\nthat object-dependently activates reduces the dissimilarity between the objects, indicating a poor\nclustering quality. In contrast, deactivating the neuron that activates regardless of the object labels\nincreases the dissimilarity, leading to a better clustering quality (see Figure 7(b).) Note, the number\nof activating neurons during the experiment are same as we deactivate a single neuron at a time.\nFigure 7(c) shows that the dissimilarity matrix in Hebb’s rule, and all the matrix elements in NeAW\nare higher than Hebb’s rule. It indicates that the object classes are easily distinguishable in NeAW.\nAs many neurons activate regardless of the objects in Hebb’s rule, the inner product between the en-\ncoded vectors increases, and the cosine-based dissimilarity decreases leading to the poor clustering\nquality. That is, the encoded vectors with less common activation will have the higher dissimilarity,\nwhich indicates the better clustering quality. In the end, the higher dissimilarity is preferred; and we\nconsider the L2 norm of the dissimilarity matrix to compare the cluster quality between the learning\nrules. The L2 norm for Hebb’s rule, Grossberg’s rule, Oja’s rule, and NeAW learning are 0.630,\n1.049, 1.313, and 1.482, respectively. Note, the accuracy of them for ModelNet10 are 72.22%,\n74.45%, 80.40%, and 88.22%, respectively.\nIn summary, the biased activity causes a poor learning in the proposed model while the balanced\nactivity is not a cause of the better performance. The additional experiment demonstrates that the\nclustering quality varies depending on both the activity balance and the neuron’s object dependency.\nC\nNEURON ACTIVITY DISTRIBUTION IN OTHER LEARNING RULE\nFigure 8: The average neuron activity of each output neuron for ModelNet10 (a) before and (b) after\nend-to-end supervised learning.\nFigure 9: The average neuron activity of each output neuron for ModelNet10 with (a) Hebb’s rule,\n(b) Grossberg’s rule, and (c) Oja’s rule.\n18\nPublished as a conference paper at ICLR 2023\nWe study the neuron activity distribution of the supervised model and the other Hebbian learning\nrules. The neuron activity here is the average number of the activation calculated by the same way\nin Figure 2.\nNeuron Activity in Supervised Learning\nNote, our end-to-end supervised model does not have\nthe WTA modules as they are not differentiable while the other Hebbian learning rules are trained\nwith the WTA modules. We observe that the neuron activity of the supervised model is evenly\ndistributed before training as shown in Figure 8(a). This will be related to the random initialization\nof the weights, which randomly activate the neuron through ReLU function. Figure 8(b) displays\nthat the activity is less balanced after training in the supervised model though there is no highly\nskewed activity observed in the Hebbian and anti-Hebbian learning.\nFigure 10: Neuron activity for different object class in NeAW learning. 180 neurons are displayed\namong total 1024 output neurons as the others do not activate for all the test samples.\nFigure 11: Neuron activity for different object class in (a) NeAW-H learning and (b) NeAW-aH\nlearning. 29 and 18 neurons for Hebbian and anti-Hebbian learning, respectively, are displayed\namong total 1024 output neurons as the others do not activate for all the test samples.\nNeuron Activity in other Hebbian Learning\nWe perform the experiment for the other Hebbian\nlearning rules such as Hebb’s rule, Grossberg’s rule, and Oja’s rule. Figure 9 shows the average\nneuron activity is also biased in Grossberg’s rule and Oja’s rule, which is similar with NeAW-H\nand NeAW-aH learning in Figure 2. Hebb’s rule has the relatively balanced activity than the others.\nHowever, it does not lead to the better accruacy as summarized in Table 1. We qualitatively observe\nthat all the learning rules have the less balanced activity than the NeAW learning. It parallels with\nthe variance of the neuron activity in Table 1.\nObject-wise Neuron Activity in Hebbian Learning Rules\nFigure 2, 8, and 9 show the average\nneuron activity for all the test samples. However, the average neuron activity for the different object\nclass is not presented. First, Figure 10 and 11 display the average neuron activity in NeAW learning\nand NeAW-H and NeAW-aH learning, respectively. Each column in the ﬁgure represents a neuron\nindex, and row indicates an object label. The value of elements is normalized, where the maximum\n19\nPublished as a conference paper at ICLR 2023\nFigure 12: The neuron activity for different object class in (a) Hebb’s rule, (b) Grossberg’s rule, and\n(c) Oja’s rule.\nvalue is 1 (red) indicating the neuron activates for all the object samples while the minimum value is\n0 (white) meaning the neuron never activates for this object. The ﬁgures represent that the number of\nactivating neurons is higher in the NeAW learning than the activity-agnostic NeAW-H and NeAW-\naH learning, and more importantly, the neurons selectively activate depending on the object labels.\nSimilarly, Figure 12 shows the average activation in Hebb’s rule, Grossberg’s rule, and Oja’s rule. As\nwe view the neuron activation in WTA modules as the cluster centroid, it means Hebb’s rule creates\nmany cluster centroids that group all the objects regardless of their labels, thereby, decreasing the\ndissimilarity between the objects in the latent space. In other words, it is hard to distinguish the rows\n(i.e. encoded vectors) in Figure 12(a). This is the reason why Hebb’s rule gives a poor performance\nwhile having the balanced activity (Figure 9). Though Grossberg’s rule and Oja’s rule in Figure\n12(b) and (c) display sparse columns and more object-dependent activation, the number of activating\nneurons is small (∼35). In this case, they create separable cluster centroids but the number of them\nare too few.\nD\nIMPACT OF HYPERPARAMETERS IN NEAW LEARNING\nWe evaluate the impact of hyperparameters a and b in (3) with the variance of the neuron activity.\nFigure 13(a) and (c) show that the higher a and b results in a faster increase of the variance during\ntraining. This can be also understood through the geometric condition given in Theorem 1:\n∥xi −W:,j′(t)∥2\n2\n∥xi −W:,j(t)∥2\n2\n< (1 + η\nN )2\n(1 −η\nN )2\n(34)\nwhich describes that NeAW Hebbian learning relieves the biased activity only if the ratio of the\nEuclidean distance are lower than the ratio of (1 + η\nN ) and (1 −η\nN ). Given a and b can be regarded\nas different learning rates η, higher a and higher b will increase the numerator and decrease the\ndenominator, respectively. It results in the geometric condition for relieving the biased activity to\nbecome weaker, and consequently, such pair of a and b quickly increases the variance. However, in\nFigure 13(b) and (d), we observe the maximum accuracy in different a and b values are saturated\nto the similar level, which implies that the ﬁnal accuracy with the NeAW Hebbian learning is not\nsensitive to a and b.\na=0 or b=0 cases indicate that the either Hebbian or anti-Hebbian learning is used rather than the\nhybridization. In the very low rates such as 0, 0.01, 0.05, and 0.1 for a and b, the saturate accuracy\nand variance is still at the similar level. However, it is important to note that the variance of the\nneuron activity slowly increases in the lower rate as shown in Figure 13(a) and (c), indicating the\n20\nPublished as a conference paper at ICLR 2023\nFigure 13: Effect of a and b for ModelNet40. (a) the variance of the neuron activity with ﬁxed a and\nvariable b, (b) the accuracy with the variable b values, (c) the variance of the neuron activity with\nvariable a and ﬁxed b, and (d) the accuracy with the variable a values.\nFigure 14: Geometry of input and weight vectors with different a and b values. (a) the initial\ngeometry of the vectors and the changed geometry of the vectors after (b) NeAW learning with a=1\nand b=0 and (c) NeAW learning with a=0 and b=1.\nslow relaxation of the biased activity. It can be intuitively understood by the vector schematic in\nFigure 14. Though the NeAW learning with either a=0 or b=0 will not guarantee the fast relaxation\nof the biased activity, the direction of the weight update can be still properly designed. Figure 15\nshows the test accuracy for ModelNet40 with the models which encoder is only trained by 5 epochs.\nWe observe that having higher a and b generally achieves higher accuracy than having very low a\nand b values.\nE\nPERFORMANCE ANALYSIS IN VARIOUS MODEL ARCHITECTURE\nRelation between Model Complexity and Performance\nWe explore the smaller and larger model\narchitectures with the supervised and unsupervised learning to understand how the performance\nchanges depending on the model complexity. We design few simpler models with single-layer en-\ncoder and classiﬁer. Figure 16 shows how the test accuracy for ModelNet40 changes in the various\nmodels using end-to-end backpropagation and NeAW Hebbian learning. We observe the accuracy\nof the simpler supervised models clearly drops to 46.6%∼61.9% while the unsupervised models\nperform better showing 56.1%∼72.5% accuracy. The performance difference is particularly large at\nthe model which size is 0.09MB: the unsupervised model achieves 71.7% and the supervised model\n21\nPublished as a conference paper at ICLR 2023\nFigure 15: (a) The accuracy for ModelNet40 at the encoder training epoch 5 with the variable a\nvalues and (b) the variable b values.\nFigure 16: Test accuracy for ModelNet40 in smaller and larger models using supervised (Backprop-\nagation) and NeAW Hebbian learning.\nshows 59.9%. For the larger models, we increase the dimension of the last layer in the encoder.\nHowever, the supervised models show slightly higher accuracy than the baseline model while the\nperformance of the unsupervised models saturate.\nFigure 17: Self-supervised model architecture. A reconstruction decoder with deconvolution and\nupsample layers is added on the proposed supervised and unsupervised model architecture.\nSelf-supervised Model and its Performance\nWe design a self-supervised learning model based\non the proposed model in Table 2. Figure 17 shows the schematic of the model architecture. The\nself-supervised learning cannot be directly applied on the same architecture and requires another\nautoencoder-based decoder to reconstruct the input point cloud. Hence, a simple decoder with three\ndeconvolution and upsample layers is added in the proposed architecture. The self-supervised model\nis ﬁrst trained with the encoder and reconstruction decoder to learn the latent representation. The\nmain difference in training the self-supervised model is a loss function. We use Chamfer distance\nloss to compare the distance between ground truth and reconstructed point cloud. After the training,\nthe parameters of the encoder are ﬁxed, and the classiﬁer decoder is trained by supervised learn-\n22\nPublished as a conference paper at ICLR 2023\ning. We observe the self-supervised model achieves the similar accuracy in both ModelNet10 and\nModelNet40 datasets with the supervised and unsupervised models. The accuracy is 92.2% for\nModelNet10 and 79.5% for ModelNet40. However, we would like to note the computational cost\nof the reconstruction decoder. While the encoder and classiﬁer used in the inference are same with\nthe supervised and unsupervised models (3.1MB), the number of parameters in the simple recon-\nstruction decoder is 263,539 (1.0MB, total 4.1MB). Also, during the training, the activation size per\nsample increases from 808 to 12,104 due to the reconstruction. In other words, the computational\ncost for the inference is same, but the overhead of training is larger in the self-supervised learning.\nFigure 18: Test accuracy for (a) ModelNet10 and (b) ModelNet40 in NeAW Hebbian learning with\nand without WTA modules.\nFigure 19: (a) Examples of point MNIST datasets and (b) test accuracy for point MNIST in super-\nvised learning (Backpropagation) and NeAW Hebbian learning.\nModel Performance without WTA modules\nWe would like to note that the learning rule in Equa-\ntion (1) includes the indicator function. It represents that the weight update is based on the closest\ninput point that is another WTA operation to couple with the WTA modules. Hence, the proposed\nlearning rule is designed along with the WTA modules, which is an important contribution in the\npaper. Figure 18 shows the test accuracy comparison between the model with and without the WTA\nmodules in both ModelNet10 and ModelNet40. The model without the WTA modules achieves\n71.8% for ModelNet10 and 25.5% for ModelNet40 while having the WTA modules shows 88.2%\nfor ModelNet10 and 76.2% for ModelNet40. The results show that the model performance sig-\nniﬁcantly drops without the WTA modules. Thus, this ablation study demonstrate that the NeAW\nlearning should accompany the WTA modules.\nModel Performance in Point MNIST\nWe train and evaluate our supervised and unsupervised\nmodels on a simple 2D classiﬁcation task using point MNIST datasets. The model architecture is\nsame with the other experiments. Each data samples include the point cloud representation of the\nMNIST images where the coordinates of the dark pixels are included. Figure 19 shows that the\nend-to-end supervised learning achieves 96.3%, and the NeAW learning shows the 94.8%. As the\ndatasets are relatively simple than ModelNet10 and ModelNet40, both the learning methods shows\nthe similarly high accuracy. However, we observe that Hebb’s rule, Grossberg’s rule, and Oja’s rule\nachieve 58.7%, 55.2%, and 69.0% test accuracy, respectively.\n23\nPublished as a conference paper at ICLR 2023\nFigure 20: Visualization of weight vectors. (top) the weight vectors in the last layer trained by\nNeAW Hebbian learning and (bottom) plain Hebb’s rule. Dark elements refer the low values while\nbright elements are with the high values.\nF\nVISUALIZATION OF WEIGHT VECTOR\nFigure 20 shows the weight vectors of the last layer in the encoder trained by different unsupervised\nlearning rules. While plain Hebb’s rule in Table 1 achieves the higher variance of the neuron activity\nthan the other learning rules, the variance between context vectors of different objects is low. It\nindicates that the trained model uses more neurons to represent objects, but it fails to activate the\ndifferent subset of neurons for different objects. Given plain Hebb’s rule repeatedly strengthens the\nweight of frequently activated pre- and post- synaptic neurons, the weight vectors will mostly learn\nthe commonly appeared position vectors of objects, which degrade the sample-to-sample variation.\nFigure 20 describes the sparse weight matrix trained by Hebb’s rule showing that the only few\nelements are with high values.\nG\nBIOLOGICAL PLAUSIBILITY OF NEAW LEARNING\nAs the key feature of NeAW Hebbian learning is to alleviate the biased activity issue, the question\nis whether our brains have similar functions. We view the NeAW Hebbian learning is analogous\nto Homeostatic plasticity in a biological brain (Turrigiano & Nelson, 2004). It explains the exci-\ntatory and inhibitory neurons are dynamically controlled to relieve the biased activity and help the\nproper brain function such as memory (Keck et al., 2017). It is still an active research area, but one\nmechanism in such neurons is to strengthen the inhibitory neurons onto excitatory neurons to reduce\nthe activity, or vice versa. Our NeAW learning rule, which dynamically utilizes Hebbian learning\n(excitation) and anti-Hebbian learning (inhibition) as a function of the neuron activity, is similar\nwith such neuron dynamics driven by the homeostatic plasticity in the brain. Also, NeAW Heb-\nbian learning is still locally deﬁned unsupervised rule, which is an important feature of biologically\nplausible learning. However, according to Dale’s principle, the mature neuron releases the single\ntype of neurotransmitters indicating the neuron can be either excitatory or inhibitory, while NeAW\nHebbian learning assumes the neuron can be both the excitatory and inhibitory but as a function of\nthe activity.\n24\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2023-02-22",
  "updated": "2023-02-22"
}