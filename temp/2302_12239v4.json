{
  "id": "http://arxiv.org/abs/2302.12239v4",
  "title": "What makes a language easy to deep-learn? Deep neural networks and humans similarly benefit from compositional structure",
  "authors": [
    "Lukas Galke",
    "Yoav Ram",
    "Limor Raviv"
  ],
  "abstract": "Deep neural networks drive the success of natural language processing. A\nfundamental property of language is its compositional structure, allowing\nhumans to systematically produce forms for new meanings. For humans, languages\nwith more compositional and transparent structures are typically easier to\nlearn than those with opaque and irregular structures. However, this\nlearnability advantage has not yet been shown for deep neural networks,\nlimiting their use as models for human language learning. Here, we directly\ntest how neural networks compare to humans in learning and generalizing\ndifferent languages that vary in their degree of compositional structure. We\nevaluate the memorization and generalization capabilities of a large language\nmodel and recurrent neural networks, and show that both deep neural networks\nexhibit a learnability advantage for more structured linguistic input: neural\nnetworks exposed to more compositional languages show more systematic\ngeneralization, greater agreement between different agents, and greater\nsimilarity to human learners.",
  "text": "What makes a language easy to deep-learn?\nDeep neural networks and humans similarly\nbenefit from compositional structure\nLukas Galke∗\nYoav Ram†\nLimor Raviv‡\nOctober 11, 2024\nDeep neural networks drive the success of natural language processing.\nA fundamental property of language is its compositional structure, allowing\nhumans to systematically produce forms for new meanings.\nFor humans,\nlanguages with more compositional and transparent structures are typically\neasier to learn than those with opaque and irregular structures. However,\nthis learnability advantage has not yet been shown for deep neural networks,\nlimiting their use as models for human language learning. Here, we directly\ntest how neural networks compare to humans in learning and generalizing\ndifferent languages that vary in their degree of compositional structure. We\nevaluate the memorization and generalization capabilities of a large language\nmodel and recurrent neural networks, and show that both deep neural net-\nworks exhibit a learnability advantage for more structured linguistic input:\nneural networks exposed to more compositional languages show more system-\natic generalization, greater agreement between different agents, and greater\nsimilarity to human learners.\nCompositionality, i.e., whether the meaning of a compound expression can be de-\nrived solely from the meaning of its constituent parts, has been studied for decades\nby both computer scientists and linguists [1, 2, 3, 4, 5]. In particular, languages differ\n∗Dept. of Mathematics and Computer Science, University of Southern Denmark, Odense, Denmark\nLEADS group, Max Planck Institute for Psycholinguistics, Nijmegen, Netherlands\nemail: galke@imada.sdu.dk\n†School of Zoology, Faculty of Life Sciences, Tel Aviv University, Tel Aviv, Israel\nSagol School of Neuroscience, Tel Aviv University, Tel Aviv, Israel\n‡LEADS group, Max Planck Institute for Psycholinguistics, Nijmegen, Netherlands\ncSCAN, University of Glasgow, Glasgow, UK\n1\narXiv:2302.12239v4  [cs.CL]  10 Oct 2024\nin how they map meanings into morphosyntactic structures [6, 7] and cross-linguistic\nstudies find substantial differences in the degree of structural complexity across lan-\nguages [8, 9, 10, 11, 12, 13, 14]. These differences can stem from multiple and often con-\nfounded aspects of linguistic structure including the degree of compositionality [7], which\ncan be quantified by correlating differences in meaning with differences in form [15]. For\nexample, the English term “white horse” is compositional since its meaning can be di-\nrectly inferred given knowledge about its constituents “white” and “horse”. In contrast,\nconsider the equivalent German term “Schimmel”, whose meaning cannot be derived\nfrom “weiß” (white) and “Pferd” (horse). Crucially, compositionality directly affects\nour ability to make systematic generalizations in a given language and thus shapes its\nimmense expressive power – which also explains its high relevance in machine learn-\ning [2, 16, 17, 18, 19, 20, 21, 22, 23, 1, 24].\nImportantly, cross-linguistic differences in compositional structure were suggested to\nimpact human language learning and generalization in the real world [25, 26, 27] as\nwell as in lab experiments [28, 29, 30, 31, 32], with more compositional linguistic struc-\ntures typically being easier to learn for adult learners. In a large-scale artificial lan-\nguage learning study with adult human participants, the acquisition of a broad yet\ntightly controlled range of comparable languages with different degrees of composi-\ntional structure was tested [28].\nResults showed that more compositional languages\nwere learned faster, better, and more consistently by the adult learners, and that learn-\ning more structured languages also promoted better generalizations and more robust\nconvergence on labels for new, unfamiliar meanings. This is likely because more sys-\ntematic and compositional linguistic input allow learners to derive a set of generative\nrules rather than rote memorizing individual forms, and then enables learners to use\nthese rules to produce an infinite number of utterances after exposure to just a finite\nset [33, 34, 35, 32, 36]. This learnability and generalization advantage for more structured\nlinguistic input has far-reaching implications for broader theories on language evolution\nin our species (and potentially other learning systems): A large body of computational\nmodels and experimental work with human participants show that more systematic and\ncompositional structures emerge during cross-generational transmission and communi-\ncation precisely because such structures are learned better, while still allowing for high\nexpressivity [33, 34, 35, 32, 37, 30, 38, 39, 40]. Hence, popular theories of language\nevolution attribute the emergence of systematic and compositional structure in natural\nlanguages to such learnability pressures [32, 41], suggesting a causal role not only in lan-\nguage learning, but also in shaping the way human languages are structured. To what\nextent this advantage of linguistic structure carries over to artificial learning systems is\ncurrently poorly understood – which is the aim of the current study.\nDespite an increasing body of work that reports striking similarities between humans\nand large language models [42, 43, 44, 45, 46, 47, 48], and despite large language models\nbeing incredibly proficient at using language and generalizing to new tasks with little to\nno new training data [49, 50, 51, 52], research on emergent communication suggests that\ndeep neural networks (the class of models that underlies large language models) show\nno correlation between the degree of compositional structure in the emergent language\nand the generalization capabilities of the networks.\nIn other words, unlike humans,\n2\nartificial neural networks do not seem to benefit from more compositional structure\nwhen they are made to develop their own communication protocol, at least without\ndedicated intervention [53, 54, 55, 56] (but see [57]). Thus, this finding raises the question\nof whether systematic and compositional linguistic structure is helpful at all for deep\nneural networks, and to what extent compositionality affects the memorization and\ngeneralization abilities of deep neural networks learning a new language.\nThe mismatch with humans can potentially be explained by differences in model de-\nsign and experimental procedure [58]. For instance, deep neural networks typically have\nimmense model capacity due to overparametrization [59, 60, 61, 62, 63, 64], which means\nthey could easily memorize all individual forms without the need to identify composi-\ntional patterns [23, 58]. A competing hypothesis is that neural networks do benefit from\ncompositional structure in the data given that this structure is reflected in the statistical\npatterns of the data which impacts the optimization of the model parameters [65, 66].\nSpecifically, in a language with a higher degree of compositionality, the individual units\nof meaning are reused in different contexts and thus appear more often in the training\ndata, such that these recurring units of meaning and their contextualization patterns are\nlearned better because of the repeated presentation throughout training (cf. [67, 24]).\nHere, we explore this precise relationship between compositional structure and gener-\nalization with deep neural networks. The central question we aim to answer is: Do deep\nneural network models exhibit the same learning and generalization advantage when\ntrained on more structured linguistic input as human adults? Specifically, we investi-\ngate whether the advantage of compositionality in language learning and language use\ncarries over to artificial learning systems, while considering GPT-3.5 as a pre-trained\nlarge language model and a custom model architecture based on recurrent neural net-\nworks (RNNs) trained from scratch. Our work contributes to the understanding of deep\nneural networks and large language models, sheds new light on the similarity between\nhumans and machines, and, consequently, opens up future directions of simulating the\nvery emergence of language and linguistic structure with deep neural network agents.\nTo allow for direct comparisons between humans and machines, we carefully follow\nthe experimental procedure and measures of a recent large-scale preregistered language\nlearning study with adult participants [28]. We consider 10 input languages, each of\nwhich has emerged independently and spontaneously through a group communication\nexperiment with adult human participants [68]. The languages describe four different\nnovel shapes moving on the screen in a different direction (0-360 degree), and vary\nin their degree of compositional structure: ranging from fully idiosyncratic languages\nwith entirely different labels for two related meaning (e.g., ’kuim’ and ’goom’ for the\nsame shape moving into a different direction) to highly structured languages, which\nre-use parts of the descriptive label (e.g., referring to the two scenes as ’fest-ii’ and\n’fest-ui’). See Figure 1. Neural networks were then trained on the exact same stimuli\npresented to humans and in the same order, using the same learning tasks, providing\nthe same feedback during learning blocks, and evaluated with the same memorization\nand generalization tests. Figure 1 shows the recurrent neural network architecture and\nsummarizes the experimental procedure: Full details of the experimental setup, custom\n3\n  \n  \n  \nStructure \nScore\n→\n  \ntup-oo\ntup-oi\nguas-oi\nwas-ui\nfest-ui\nfest-ii\nLang2\n0.09\nLang1\n0.07\nLang3\n0.25\nLang4\n0.35\nLang6\n0.59\nLang5\n0.58\nLang8\n0.78\nLang7\n0.69\nLang9\n0.83\nLang10\n0.85\npnuun-a\npnuun-ko\nkoemet\nwoes-ik\nmomet-uk\nmomet-fe\nmoof\nwuit\npofs\nsuith\nkuim\ngoom\nLow Structure\nMedium Structure\nHigh Structure\nNo systematic mapping between the \nlabels in the language and the scenes’ \nshape or direction.\nHigh to fully consistent systematic mapping of \npart-labels with respect to shape, but no clear \nstructure for direction.\nHigh to fully consistent systematic \nmapping with respect to both shape and \ndirection.\n…\n…\nFigure 1: Overview of input languages (Top), the experimental procedure (Bottom Cen-\nter) along with exemplary input data from one language (Bottom Left), and the\nmodel architecture (Bottom Right). Low-structured input languages show no\nsigns of systematicity or compositionality, whereas high-structured languages\nare systematic and compositional with respect to both attributes: shape and\nangle. For each language, we train the model for multiple rounds of exposure,\nguessing, production. After each round, we conduct a memorization test to\nevaluate productions for previously seen items and a generalization test evalu-\nating the productions for new items. Graphical elements in the upper part of\nthis figure are re-used and adapted with permission from Raviv et al. [68].\n4\nTable 1: Glossary of Metrics\nMetric\nDescription\nProduction Similarity\nOne minus length-normalized edit distance\nSemantic Difference\nSum of the difference in shape (1 if different and 0 otherwise)\nand the absolute difference in angles (divided by 180)\nStructure Score\nPearson correlation between (a) pairwise semantic distances\nand (b) pairwise normalized Levenshtein distances, where\n(a) and (b) are calculated on all pairs of items in the original\ninput language\nGeneralization Score\nPearson correlation between (a) pairwise semantic difference\nand (b) pairwise length-normalized edit distance, where (a)\nand (b) are calculated on all pairs between productions for\nmemorized items and productions for generalized items\nConvergence Score\nAverage of all values for item-level production similarity for\nthe same items between different learners trained on the\nsame language\nHuman Label Similarity\nItem-level production similarity to (other) human learners,\naveraged across different human learners\nTrue Label Similarity\nItem-level production similarity to input language\nrecurrent neural neural network models, and how we employed large language models\nare provided in the Methods section.\nResults\nTo preview our results, we find a consistent advantage of more systematic and composi-\ntional linguistic structure for learning and generalization, closely reflecting adult human\nparticipants. The generalization behavior of both large language models (pre-trained on\nother languages) and recurrent neural networks (trained from scratch) was far more sys-\ntematic and transparent when the input languages were more compositional. Moreover,\nrecurrent neural network agents displayed a higher agreement with other agents as well\nas with humans when the input was more compositional, leading to converging trans-\nparent generalizations for new unseen input. A glossary of evaluation metrics can be\nfound in Table 1. More detailed descriptions of the metrics are provided in the Methods\nsection.\n5\n0.25\n0.50\n0.75\nStruct. Score\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nGeneralization Score\n(A) Humans\n0.25\n0.50\n0.75\nStruct. Score\n(B) GPT-3.5\n0.25\n0.50\n0.75\nStruct. Score\n(C) RNNs\nFigure 2: Final Generalization Score achieved by humans (A), GPT-3.5 (B), and re-\ncurrent neural networks (C) for each of the input languages. The x-axis shows\nthe structure score of the input languages. Each point corresponds to the gen-\neralization score calculated for the entire input language. This score reflects\nthe degree to which learners systematically generalized new labels relative to\nthe labels they learned. For example, generalization score would be high if\nlearners successfully recombines previously used parts, e.g., combining ’muif’\nfor the shape and ’i’ for the direction into ’muif-i’. Error regions of the re-\ngression lines are 95% confidence intervals estimated via bootstrapping. More\nstructure in the input language leads to more systematic generalization for all\nthree learning systems.\nMore compositional structure leads to higher similarity to humans and more\nsystematic generalization of large language models\nWe first test whether large language models benefit from compositional structure when\nlearning a new language. Such language models are pre-trained to predict left-out words\nin web-scale corpora of text data, leaving them with high competence in at least one\nlanguage, similar to adult human participants. Specifically, we employ the large language\nmodel GPT-3.5 (version text-davinci-003) which is capable of in-context learning, i. e.,\nhaving the model tackle a new task only based on a few examples in the prompt [49, 69].\nWe make use of this property to evaluate the model in learning the new languages.\nFor each input language, we insert the form-meaning pairs in the prompt of the large\nlanguage model, followed by a single meaning for which the label needs to be completed.\nWe repeat this procedure multiple times to have the language model produce labels\nfor the memorization test (known meanings) as well as the generalization test (new\nmeanings).\nIn the generalization test, there is no true label in the input language. To capture the\ndegree to which new labels conform to the labels of the input language (i.e., to what\n6\nTable 2: Generalization examples from neural network and human learners, showing la-\nbels generated for unseen scenes. The column GPT-3.5 corresponds to comple-\ntions generated by the GPT-3.5 model text-davinci-003 via in-context learning,\nwhere the training data is provided in context. The examples cover the differ-\nently structured input languages from low to high.\nStruct.\nShape\nAngle\nHuman\nRNN\nGPT-3.5\nlow\n2\n360\nkokoke\nseefe\ntik-tik\n4\n45\nwoti\nkite\nhihi\n3\n150\nptiu\nmimi\nhihi\nmid-low\n3\n225\nwangsuus\nwangsoe\nwangsuus\n4\n225\ngntsoe\ngntuu\ngntsii\n1\n135\nsketsi\ngesh\ngeshts\nmid\n3\n60\npowi\npowu-u-u\npowee\n4\n330\nfuottoa\nfuotio\nfuottu-u-u\n1\n30\nfewo-o-o-o\nfewen\nfewee\nmid-high\n1\n30\nfas-a\nfas-a\nfas-a\n3\n360\nmuif-i\nmuif-a\nmuif-i\n1\n225\nfas-huif\nfas-huif\nfas-huif\nhigh\n4\n60\nsmut-tkk\nsmut-tk\nsmut-ttk\n2\n360\nnif-k\nnif-kks\nnif-k\n1\n315\nwef-ks\nwef-kks\nwef-kks\nextent the generalization is systematic), we correlate the pairwise label difference and\nthe pairwise semantic difference between the labels generated for new scenes and the\nlabels generated by the same agent for known scenes [28].\nStrikingly, the results reveal that a higher degree of compositional structure in the\ninput language leads to generalizations that are more systematic (see Figure 2B), closely\nreflecting the pattern of adult human learners (Figure 2A). Table 2 shows examples of\nthe final productions of humans and large language models during generalization (more\nexamples are provided in Tab. 6 and 7 in the SI).\nIn addition, we evaluate the production similarity as character-level length-normalized\nedit distance between the generated labels and labels produced by human participants\nduring generalization. The results show that, given more structured linguistic input,\nGPT-3.5 also yields productions that are more similar to the productions of human\nparticipants, calculated as the average similarity between GPT-3.5’s production and all\nhuman productions for the same scene in the same language (Figure 3B). Analogously,\nFigure 3A shows the similarity of humans to other human learners during generalization.\nWe then conduct an error analysis to understand better whether the memorization\nerrors are similarly affected by the degree of compositional structure. We analyze the\ncases where the learning system fails to memorize the correct label perfectly and calculate\n7\n0.25\n0.50\n0.75\nStruct. Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nHuman Label Similarity\n(A) Humans\n0.25\n0.50\n0.75\nStruct. Score\n(B) GPT-3.5\n0.25\n0.50\n0.75\nStruct. Score\n(C) RNNs\nFigure 3: Final Similarity to Humans during Generalization Final production\nsimilarity with (other) human participants during generalization achieved by\nhumans (A), GPT-3.5 (B) and recurrent neural networks (C) for each of the\ninput languages. The x-axis shows the structure score of the input languages.\nEach point corresponds to the production similarity score (calculated as length-\nnormalized edit distance) between humans’ productions and models’ produc-\ntions for every item in the language. For example, a recurrent neural network\nthat produced ’muif-a’ for shape 3 moving in direction 360 degrees would\nhave a high production similarity to the majority of human participants who\nproduced ’muif-i’. Error regions of the regression lines show 95% confidence\nintervals estimated via bootstrapping. More structure in the input language\nleads to more similarity to human participants for both RNNs and GPT-3.5.\nthe production similarity (1 minus length-normalized edit distance). Again, the results\nshow the same pattern for adult human participants and large language models (see\nFigure 4A and B): When there is more structure in the input language, the non-perfectly\nmemorized productions are more similar to the correct labels.\nMore compositional structure leads to higher similarity to humans and more\nsystematic generalization with recurrent neural networks\nIn addition to large language models, we test a custom neural network architecture\ntrained from random initialization, which allows us to conduct a close analysis of the\nlearning trajectory. Our custom model architecture is designed to simulate the expo-\nsure, guessing, and production that human participants were exposed to (see Figure 1).\nThe architecture is inspired by image-captioning approaches [70], the emergent commu-\nnication literature [71], and in particular, our recent review paper [58] which suggested\nhaving shared model parameters between generation and processing of a label.\nOur\n8\n0.25\n0.50\n0.75\nStruct. Score\n0.0\n0.2\n0.4\n0.6\n0.8\nTrue Label Similarity\n(A) Humans\n0.25\n0.50\n0.75\nStruct. Score\n(B) GPT-3.5\n0.25\n0.50\n0.75\nStruct. Score\n(C) RNNs\nFigure 4: Memorization Error Analysis for human participants (A), GPT-3.5 (B),\nand recurrent neural networks (C). The error rates are 33.30% for humans,\n7.39% for GPT-3.5 via in-context learning, and 13.87% for RNNs after 100\nepochs of training. The x-axis shows the structure score of the input language.\nEach point corresponds to the production similarity score (calculated as length-\nnormalized edit distance) between an erroneously memorized label for a given\nitem and the correct corresponding label as it appears in the input language.\nFor example, ’wangsus’ has a higher similarity with ’wangsuus’ than ’gempt’.\nError bands of the regression lines show 95% confidence intervals estimated\nvia bootstrapping. More structure leads to erroneously memorized examples\nbeing more similar to the ground truth of the input language.\nmodel consists of two components: a generative component that facilitates the produc-\ntion of a descriptive sequence of symbols (here, a label) for a scene, while a contrastive\ncomponent shapes the latent space and enables the models to carry out guessing tasks\nduring learning (i.e., given a label, pick the correct scene from a set of distractors). Each\ncomponent has a sequential recurrent neural network module to carry out the generation\nand processing of a sequence, respectively, for which we use the well-known long short-\nterm memory [72]. The symbol embedding that maps each symbol of the sequence into\na continuous vector is shared between the generative and the contrastive component.\nMoreover, the two components share the same encoder module that transforms an input\nscene into a latent representation, which then serve as the initial state of the generative\ncomponent. Production tasks are modeled by a generative objective: Based on this ini-\ntial state, the model generates a label, character by character. This generated label is\ncompared to the target input language by character-level cross-entropy. Guessing tasks\nare modeled by a contrastive objective [73], which aligns the latent representation of\ninput scenes and corresponding labels and facilitates selecting the correct scene from a\n9\nFigure 5: Learning trajectory of Recurrent Neural Networks’ Memorization\nand Generalization Performance. More structured languages lead to bet-\nter and faster reproduction of the input language (A), to better generalization\non unknown scenes (C), better agreement with human participants during\nmemorization (B) and generalization (D), and higher convergence between\nnetworks (E). (A): Production similarity between labels generated by neural\nagents and labels of the input language. (B): Production similarity between\nlabels generated by neural agents and labels generated by human participants.\n(C): Generalization score of labels generated by neural agents for new scenes\nthat were not part of the training data. (D): Production similarity between\nlabels generated by neural agents and labels generated by human participants\nfor unseen scenes.\n(E): Convergence score measures the similarity between\nlabels generated for unseen scenes by different neural agents. Stars mark the\nround at which neural agents first exceed the final performance of human par-\nticipants. Input languages are grouped into 5 bins. Each line is the average\nof 200 neural agents with a different random initialization. A star marks the\nepoch at which the RNN agents exceed human performance. Results are cut\noff for visualization at epoch 60, full results in SI.\n10\nset of distractors. As the encoder is shared, the contrastive objective shapes the space\nof initial states of the production model.\nIn total, we trained 1,000 neural network agents with different random seeds (100 for\neach of the ten input languages) and calculated the following measures after each train-\ning round: the similarity between networks’ productions and the input language; the\nsimilarity between networks’ productions and the human learners’ productions during\nmemorization and generalization; the generalization score capturing the degree of sys-\ntematicity; and a convergence score capturing the agreement between different agents.\nWe evaluated these measures after each of 100 rounds.\nThe results are shown in Figure 5. Extended results can be found in Figs. 6–11 in the\nSI. In the following, we present the results for the learning trajectory organized along the\ntwo types of tests: memorization and generalization, before presenting the final results\nof RNNs.\nMemorization Trajectory\nHow well did neural agents memorize the input languages?\nAnd how similar were their generated labels to those produced by human learners dur-\ning generalization? This is measured by production similarity [28], which captures the\nsimilarity between the original label and the produced label by calculating the average\nnormalized edit distance between two labels for the same scene. We use this measure in\ntwo ways: once to compare the generated labels to the true label of the input language\nand once to compare the machine-generated label to the human-generated label for the\nsame scene.\nSimilarity to Input Languages during Memorization\nWith sufficient training rounds,\nall languages can be learned by all neural network agents, reaching a production sim-\nilarity of at least 0.8 (out of 1) by round 60 (Figure 5A). Structured languages are\nlearned significantly better (LME 1; β = 0.045, SD = 0.001, z = 62.865, p < 0.001),\ni. e., they show a higher similarity with the input language. However, this advantage\ntends to diminish over training rounds (LME 1; β = −0.005, SD < 0.001, z = −54.978,\np < 0.001).\nSimilarity to Humans during Memorization\nWe measure the similarity to humans dur-\ning memorization (i. e., comparing productions of both learning systems after complet-\ning the training rounds) and the memorization test data of the neural network agents\nafter each training round (Figure 5B). More compositional input languages led to a\nsignificantly greater similarity with human learners (LME 2; β = 0.097, SD = 0.001,\nz = 81.429, p < 0.001). This effect became even stronger over rounds (LME 2; β = 0.022,\nSD = 0.000, z = 208.708, p < 0.001).\nGeneralization Trajectory\nWe evaluate the productions of neural agents when they\ngeneralize, i. e., produce labels for new scenes that were not part of the training data.\nWe test the productions regarding three aspects: the degree of systematicity, the sim-\nilarity to humans, and the generalization convergence between different agents.\nAs\n11\nwith large language models, we evaluate the generalization score. More structured lan-\nguages consistently led to significantly higher generalization scores (Figure 5C) (LME 3;\nβ = 0.088, SD = 0.001, z = 148.901, p < 0.001), and this effect became stronger with\ntime (β = 0.046, SD < 0.001, z = 703.483, p < 0.001).\nSimilarity to Humans during Generalization\nWe measure the similarity between the\nproductions of neural network agents and humans for new scenes (Figure 5D), i. e., during\ngeneralization. Examples are shown in Table 2. More structure in the input language led\nto a significantly higher similarity between humans and neural agents (LME 5; β = 0.132,\nSD = 0.002, z = 70.280, p < 0.001), which became stronger over rounds (β = 0.046,\nSD < 0.001, z = 344.287, p < 0.001).\nConvergence between Neural Agents during Generalization\nMore structured lan-\nguages lead to better agreement between networks (LME 4; β = 0.043, SD = 0.001,\nz = 49.027, p < 0.001), such that, for more structured languages, different neural agents\nlearning the same input language produced more similar labels for new scenes (Fig-\nure 5E). This effect became stronger over rounds (β = 0.009, SD < 0.001, z = 121.740,\np < 0.001).\nFinal Results of RNNs\nTo compare our custom recurrent neural network agents with\nlarge language models and with humans, we visualize the relationship between composi-\ntional structure of the input language and final generalization performance in Figure 2C.\nAll three learning systems (Humans, RNNs, and GPT-3.5) show the same trend: more\ncompositionality in the input language leads to more systematic generalization.\nMoreover, we calculate the average similarity to generalizations of human participants\non the same language and item.\nComparing the productions during generalization,\nthe results show that a higher degree of structure in the input language leads to more\nsimilarity with humans (see Figure 3C). This pattern of compositional structure leading\nto more human-like generalizations is present in both RNNs’ and GPT-3.5’s generated\nlabels – as well as when comparing humans to other humans (see Figure 3).\nLastly, we visualize the results of the memorization error analysis for recurrent neural\nnetworks alongside humans and GPT-3.5 in Figure 4.\nThe pattern is the same for\nall three different learning systems, be it artificial or biological: more compositional\nstructure leads to errors that are more similar to the true label.\nDiscussion\nOur results show that deep neural networks benefit from more structured linguistic input\nas humans do and that neural networks’ performance becomes increasingly more human-\nlike when trained on more structured languages. This structure bias can be found in\nnetworks’ learning trajectories and even more so in the networks’ generalization behavior,\nmimicking previous findings with humans. Although all languages can eventually be\n(almost) perfectly learned, we show that more structured languages are learned better\n12\nand more similarly to human productions. Deep neural networks and humans produce\nnearly identical labels when trained on high-structured languages but not when trained\non low-structured languages. Moreover, networks that learn more structured languages\nare significantly better at systematic generalization to new, unseen items, and crucially,\ntheir generalizations are significantly more consistent and more human-like. This means\nthat highly systematic grammars allow for better generalization and facilitate greater\nalignment between different neural agents and between neural agents and humans. We\nhave replicated these results with small recurrent neural networks and with transformer-\nbased large language models, showing that, together with humans, all three learning\nsystems show the same bias in systematic generalization and memorization errors. Thus,\nour findings strengthen the idea that language models are useful for studying human\ncognitive mechanisms, complementing the increasing evidence of similarity in language\nlearning between humans and machines [42, 43, 44, 45, 46, 47, 48].\nSpecifically, we find very similar effects of structure on generalization and on the\nsimilarity to humans across all three learning systems. While we find a different slope\nfor humans and RNNs in the memorization error analysis (likely due to RNNs being\nless impacted by memorization difficulty given sufficient training), the general trend is\nconsistent: for both humans and artificial agents, exposure to more structured languages\nleads to production errors that are nevertheless more similar to the correct labels (i.e.,\ntheir errors are less “wrong”).\nWe assume that the reason for the increased similarity between machines and hu-\nmans is that the ways to generalize are more transparent in high-structured languages,\nwhile there are none or less transparent generalization patterns available in low- and\nmedium-structured languages. This leads both humans and neural networks to a higher\nproduction variation in lower structured languages, as different options on how to gener-\nalize are equally likely. This point is well supported by results from humans, who indeed\nshow increased convergence between participants when learning higher structured lan-\nguages [28] Our results thereby demonstrate that what is more transparent for humans\nis also more transparent for deep neural networks.\nAnalyzing the learning trajectory of recurrent neural networks, we find that languages\nwith mid and mid-low structures often show an advantage in both memorization and\ngeneralization during the early stages of learning. This may be due to the fact that these\nmid-structured languages trade off full expressiveness with more simplicity (see Tab. 3 in\nthe SI). For example, one of the mid-structured languages includes a marker for “moving\non the diagonal”, but does not distinguish the direction of the movement (e.g., center to\nnorth-east vs. center to south-west). As a result, the same label is used for two distinct\nmeanings, which is easier to learn in the first place (less variation), but not sufficient to\nfully differentiate between items and thus harming systematic generalization.\nAs for implications, our findings first and foremost support the idea that languages’\nunderlying grammatical structure can be learned directly from (grounded) linguistic\ninput alone [74, 75, 76, 41, 35]. To ensure that the advantage of more structured linguistic\ninput does not stem from the fact that the learning system was already proficient in a\ndifferent language – i. e., as are pre-trained language models and adult humans – we\nalso also considered models trained from random initialization. Therefore, our results\n13\npredict that children would also benefit from more systematic compositional structure\nin the same way adults do – a prediction we are currently testing (preregistration: [77]).\nOur findings have further implications for machine learning, where systematic gener-\nalization beyond the training distribution (out-of-domain) is of high interest [17, 19, 78,\n20, 21]. Systematic in-domain generalization, as studied here, is a critical prerequisite for\nsystematic out-of-domain generalization. Specifically, we show that seeding a learning\nsystem with well-structured inputs can improve their ability to systematically general-\nize to combinations that were not observed during training. Even though our study is\nbased on artificial languages, our findings directly pertain to the natural language pro-\ncessing of real-world languages. To confirm this prediction, we re-analyzed data from\nWu et al. [14], who used the Wug Test [79] to test language models’ ability to predict\ndifferent forms of unfamiliar words in a wide range of natural languages. Indeed, we\nfind that the Wug Test accuracy negatively correlates with the degree of irregularity of\nthe language (Spearman’s ρ = −0.96, p < 10−15; Kendall’s τ = −0.86, p < 10−14). This\nstrong negative correlation suggests that natural languages with fewer irregularities, i. e.,\nmore consistently structured natural languages, are indeed easier to learn for machines.\nCrucially, there is a positive correlation between the degree of linguistic structure and\npopulation size [12, 80, 81, 68], with low-resource languages (i. e., languages spoken by\nsmaller communities for which there is only very little training data available) typically\nhaving less structured languages. Since our study predicts that such languages are harder\nto learn for deep neural networks, this results in a double whammy for developing natural\nlanguage processing systems for small communities’ languages – exacerbating challenges\nof low-resource language modeling [82]. Interestingly, the benefit of structured input\ncould also explain the importance of highly-structured programming languages in the\ndata mix for training large language models [83].\nFinally, our results are of high relevance to the field of emergent communication.\nEmergent communication strives to simulate the evolution of language with multi-agent\nreinforcement learning [84, 85, 56, 71, 86, 87, 53, 88, 89]. However, as argued in the\nintroduction, certain linguistic phenomena of natural language appear to be hard to\nreplicate in multi-agent reinforcement learning [55, 90, 54, 58], which had raised the\nquestion whether compositionality is helpful for neural networks at all. We hypothesized\nthat these mismatches are caused by the lack of cognitive constraints [58] eradicating the\nlearnability pressure underlying human language evolution [37]. Our findings support\nthe importance of a learnability pressure for compositional languages to emerge. By\nconfirming a result previously found in humans [28] in deep neural networks, we take\nthe first steps to bring emergent communication closer to the field of language evolution,\nsupporting simulations of language emergence with neural networks.\nAn interesting direction for future research is to investigate potential differences in\nthe amount of training that a neural network needs compared to humans.\nThrough\nanchoring our experiments in human data, we were able to directly identify the point\nduring training at which recurrent neural networks equalize with human participants.\nHowever, the location of this point depends on various factors such as the amount of\ndata, the number of parameters that are optimized, and the number of optimization\nsteps – which makes it challenging to predict this point in advance. While we have\n14\nidentified this point through analyzing the learning trajectory, our analysis does not\ndepend on it, as all measures including the similarity between humans and machines are\ncalculated based on productions taken at the end of training.\nFurthermore, we have chosen to work with an input representation that we deemed\neasiest to process for each type of learning system. Since the particular way in which\nagents represent the visual world was not the object of the current study, our rationale\nhere was to provide each learning system with a representation that is easiest or most\nnatural to process. Human participants would have likely had a harder time finding\npatterns in attribute-value vectors, consisting of 6 numbers, than in short video clips\nwith moving objects. In contrast, operating on raw pixels is expected to introduce more\ndifficulty for machine learning models in terms of disentangling representations [85].\nFuture work could examine whether neural nets segment visual stimuli in a similar way\nas humans in grounded language learning.\nIn conclusion, our findings shed light on the relationship between language and language-\nlearning systems, showing that linguistic structure is crucial for language learnability and\nsystematic generalization in neural networks. Our results suggest that more structured\nlanguages are easier to learn, regardless of the learning system: a human, a recurrent\nneural network, or a large language model. Thus, generalization capabilities are heavily\ninfluenced by compositional structure, with both biological and artificial learning systems\nbenefitting from more structured input by facilitating more systematic and transparent\ngeneralizations.\nIn future work, we will analyze how this learnability bias for more\nstructure affects neural networks engaged in collaborative communication games, and\ntest how this kind of systematic structure arises in the first place in emergent commu-\nnication simulations. Moreover, our findings give a clear prediction that children would\nbenefit from more structure in the linguistic input, which we will test by conducting a\nlearnability study with children.\nMethods\nInput Languages\nThe input languages with different degrees of compositional structure\ncome from a previous communication study in which groups of interacting participants\ntook turns producing and guessing labels for different dynamic scenes, creating new\nartificial languages over time [68]. Ten of the final languages created by these groups then\nserved as input languages for a follow-up study on language learnability with humans [28].\nFor our experiments, we used the same ten input languages. These input languages\nare considered the ground truth.\nEach of the ten input languages contains a set of\n23 label-scene mappings. Each scene comprises one of four different shapes moving in\ndifferent directions between 0 and 360 degrees. The languages vary in their degree of\ncompositional structure, with structure scores ranging from 0.09 to 0.85.\nTopographic Similarity to Quantify Compositional Structure\nCrucially, the ten input\nlanguages have different degrees of structure, ranging from languages with no structure\nto languages with consistent, systematic grammar. Each language has a structure score\n15\nrepresented by topographic similarity [15], quantifying the degree to which similar la-\nbels describe similar meanings. The topographic similarity is measured as the Pearson\ncorrelation between all labels’ pairwise length-normalized edit distances and their corre-\nsponding pairwise semantic differences. The semantic difference between two scenes is\ncalculated as the sum of the difference in shape and the difference in angles [28]. The\ndifference in shape is zero if the two scenes contain the same shape, and one otherwise.\nThe difference in angles is calculated as the absolute difference divided by 180. The to-\npographic similarity of a language is then calculated as the pairwise correlation between\nall semantic differences and all normalized edit distances. For a complete list of input\nlanguages and their structure scores, see Table 3 in the SI.\nHuman Learning Data\nAside from the input languages, we use reference data from 100\nhuman participants learning these input languages [28]. The participants were different\nfrom those who created the languages. A hundred participants, ten per input language,\nengaged in repeated learning blocks consisting of passive exposure (in which the target\nlabel-meaning mappings were presented on the screen one by one), guessing trials (in\nwhich participants needed to pick the right scene from a set of possible distractors), and\nproduction trials (in which participants needed to generate a descriptive label for a target\nscene based on what they had learned). During training, humans received feedback on\ntheir performance.\nLarge Language Models\nFor the large language models, we supplied the full training\ndata of the respective input language to GPT-3.5: 23 lines consisting of shape-angle pairs\nin a textual format, and the corresponding target label. These 23 lines were followed\nby a single line that only contained shape and angle but no word. GPT-3.5 was made\nto predict the most likely word as completion, for which it could take into account the\n23 triples presented in the prompt. In the memorization task, the target word appears\nearlier in the prompt, which means that the perfect solution would be to simply copy\nthis word. In the generalization task, we gave GPT-3.5 a combination of shape and\nangle not present in the training data (and not in the prompt). The model generated\nthe most likely descriptive word for the new shape-angle pair.\nWe had to make certain technical choices when using GPT-3.5. First, we chose a\nconsistent input representation (Javascript Object Notation). We do not insert a task\ndescription to avoid potential bias. Instead we purely rely on next-token prediction.\nSecond, we set the sampling temperature to zero, which controls the randomness of the\ngeneration, such that we obtain deterministic generations. Third, we do not impose\nany restrictions on the characters that can be generated but rely on its ability to detect\nthis pattern from the training data. Fourth, we do not feed back GPT-3.5’s previous\nproductions into the prompt. Lastly, GPT-3.5’s tokenization procedure (how text is\nsplit into subword tokens) could have been problematic for applying it to our artificial\nlanguages.\nHowever, we found that GPT-3.5 still reaches high memorization perfor-\nmance, which suggests that tokenization is not a problem. We have confirmed that the\n16\nwords of the artificial languages are tokenized as expected with the OpenAI’s Tokenizer\n(https://platform.openai.com/tokenizer): falling back to one token per character.\nCustom Recurrent Neural Network Architecture\nOur custom model architecture (see\nFigure 1, right) is based on two components: a generative component and a contrastive\ncomponent. The generative component is conditioned on the input scene and generates\na label letter by letter. The contrastive component ensures that the matching scenes and\nlabels are close in the representation space and non-matching pairs are apart from each\nother. For processing the sequence of letters, each component uses a recurrent neural\nnetwork, for which we use the well-known long short-term memory (LSTM) [72]. In the\nfollowing, we describe the input representation before we describe the two components\nand their interactions.\nScenes were shown to human participants as short videos [28]. For the recurrent neural\nnetworks, we use a simplified representation of the scenes. The rationale for choosing this\ninput representation over images is that both humans and models receive the respective\neasiest possible input type to process, allowing for a fair comparison [91, 92]. For the\nrecurrent neural networks, we employ a one-hot encoding of the shape concatenated with\na sine and a cosine transformation of the angle. The sine–cosine transformation promotes\na similar treatment of angles that are close to each other, while each unique angle\ncan be distinguished. For example, shape 2 (between 1 and 4) moving at a 90-degree\nangle is converted to a vector (0, 1, 0, 0, 1, 0), shape 3 with 45 degrees is converted to\n(0, 0, 1, 0, 0.71, 0.71), and shape 4 with 135 degrees is converted to (0, 0, 0, 1, 0.71, −0.71).\nWe refer to the resulting 6-dimensional vector representation of the input as a scene x.\nBy using this input representation, we focus on the ability of systematic generalization\nin language learning rather than the ability to learn disentangled representations. If the\nneural networks were trained on pixel input instead, the task would be more challenging\nas neural networks would need to learn disentangled representations on the fly [85].\nWithin the generative component, the input scene x is first encoded to a latent repre-\nsentation h by Encoder, a feedforward network (we use a multilayer perceptron with\none hidden layer), such that we obtain a latent representation h = Encoder(x). This\nlatent representation h is then used as the initial state of the recurrent neural network\nWriter.\nThe Writer sequentially produces a sequence of letters, i. e., a label, as\noutput. This Writer consists of three modules: an input embedding for previously\nproduced characters, an LSTM cell, and an output layer that produces the next letter.\nFor the contrastive component, we use another recurrent module Reader that reads a\nlabel m sequentially (i. e., letter by letter) while updating its state. As for the Writer,\nwe again use an LSTM. A fully-connected layer transforms the final state into a latent\nrepresentation z, such that z = Reader(m), where m is the input label. The reading\ncomponent is used for contrastive learning, i. e., they are trained so that the hidden\nrepresentation of the label z matches the representation of the corresponding scene\nh = Encoder(x), which is used as the initial hidden state of the generative Writer\nmodule.\n17\nTo ensure that the contrastive training procedure affects the generative component,\nwe couple the two components: First, the embedding (i. e., the mapping between the\nagent’s alphabet and the first latent representation) parameters are shared between the\ninput layer of Reader, the input layer of Writer, and the output layer of Writer.\nSecond, the same encoder module is used in both the generative and the contrastive\ncomponents (see Figure 1).\nThe output dimension of Encoder, the hidden state sizes of Reader and Writer,\nand the embedding size are all set to 50. A sensitivity analysis of the hidden size on\nthe dependent variables of interest is provided in Figs. 18–20 in the SI. Similarly to\nNakkiran et al. [59], larger hidden sizes led to a faster increase in memorization and\ngeneralization.\nTraining Procedure\nWe train the recurrent neural networks for multiple training rounds\nas in the experiments with human participants [28]. Each training round consists of three\nblocks: exposure, guessing, and production block, described in detail in the following.\nAs typical in neural network training, we train the network with backpropagation and\nstochastic gradient descent, where the gradient is estimated based on a small number\nof examples (minibatches) [93, 94]. The batch size, which also determines the number\nof distractors, is set to 5, reflecting human short-term memory constraints [95]. Only\nin the guessing block, we set the batch size of 1 and use the same distractors as in the\nexperiments with human participants, instead of other exemplars from the same batch.\nIn the exposure block, human participants were exposed to scenes with the correspond-\ning target labels. Therefore, we train the deep learning models using a loss function with\ntwo terms: a generative and a contrastive loss term. The generative loss, Lgen, is a token-\nwise cross-entropy with the ground-truth label of the original language. The contrastive\nloss, Lcon, promotes similar latent representations of scenes and labels that correspond\nto each other and contrasts representations that do not. Specifically, we use the nor-\nmalized temperature-scaled cross-entropy loss (NTXent) [73]. We use other scenes in\nthe same batch as distractors for the contrastive loss term. The final loss function is\nL = Lgen + αconLcon. The factor αcon determines the relative weight of the loss terms.\nFor the main experiment, we use αcon = 0.1. A sensitivity analysis using other values\nfor αcon is provided in Figs. 21-23 of the SI.\nIn the guessing block, we use the same loss function as in the exposure block. The\ncontrastive loss term Lcon mirrors the task in which human participants had to select\nthe correct scene against the distractors given a label. The generative loss term Lgen is\nused so that the model does not “forget” how to generate [96]. Notably, the guessing\ntask itself could be also carried out by having the models generate a descriptive label for\neach scene and then select the closest one to the given label in terms of edit distance.\nHowever, we opted for optimizing shared parameters through a contrastive loss to ensure\nthat the guessing task would also have an effect on the production task (and vice-versa).\nIn more detail, the latent representation z = Encoder(x) of the scene x should be\nclosest to the latent representation z′ = Reader(m) of the corresponding label m.\nThe difference from exposure training is that in the guessing block, we use the identical\n18\ndistractors used in experiments with humans, whereas, in the exposure block, we use the\nother scenes from the same batch. The trajectory of guessing accuracy during training\nis shown in Fig. 12 in the SI.\nIn the production block, a scene was presented to human participants, who had to\nproduce a label. We again use the same generative loss as in the previous block, Lgen, to\nmodel the production block. In the production block, however, we omit the contrastive\nloss term and train only on generation. Thus, the loss function for the production block\nis L = Lgen.\nThe parameters are randomly initialized by He initialization [97], the default initial-\nization method in PyTorch [98]. We employ the widely used Adam optimizer [99] to\ncarry out the optimization of the loss function with the default learning rate of 10−3.\nAs common in machine learning, we have to make certain decisions about the neural\nnetwork architecture design, optimization procedure, and hyperparameters. All these\ndecisions may impact the results.\nHowever, we have varied relevant hyperparameter\nsettings and found that the results are robust and do not dependent on specific settings\nof the hyperparameters (see Figs. 24 and 25 in the SI).\nMeasures\nProduction similarity measures the overlap between two sets of labels. It\nis computed as one minus the normalized edit distance between pairs of labels. For\nour analysis, we use production similarity once to quantify the similarity between the\ngenerated labels and the ground truth of the input languages, and once to quantify the\nsimilarity of labels generated by neural network agents with labels produced by human\nlearners. For example, a recurrent neural network that produced ’muif-a’ for shape 3\nmoving in direction 360 degrees would have a high production similarity to the majority\nof human participants who produced ’muif-i’.\nThe generalization score measures the degree of systematicity during the generalization\ntest [28]. We take two sets of scenes: a training set, on which the agents were trained,\nand a test set, on which the agents were not trained. We then do the following for each\nagent. First, we take two sets of labels: one previously generated for each training scene\nby the agent and another that we let the agent generate for each test scene. Second,\nthe difference between train and test scenes is measured by pairwise semantic difference.\nSemantic difference is calculated as topographic similarity. Third, the difference between\ntrain and test labels is measured by pairwise normalized edit distance.\nFinally, we\ncompute the Pearson correlation between these two differences across all scenes. Then,\nwe take the average correlation coefficient across all agents as the generalization score.\nThe convergence score measures the similarity in the generalization test between agents\nthat learned the same language. We take the test set on which the agents have not been\ntrained and let each agent produce a label for each scene. We compute the pairwise\nnormalized edit distance between all generated labels per scene so that if we have n test\nscenes and k agents, we compute n · k(k−1)\n2\ndistances. We then compute the average\ndistance across both scenes and labels and take one minus the average distance as the\nconvergence score. Therefore, if all agents produce the same label for each test scene,\n19\nwe would get a convergence score of 1. Conversely, if each agent produced a different\nlabel for the same scene, the convergence score would be zero\nStatistical Analyses\nWe trained 100 differently-initialized neural network models over\n100 rounds for each of the ten input languages. The testing in each round consisted of\n23 memorization and 13 generalization examples. This makes a total of 2.3M memo-\nrization and 1.3M generalization test results subject to statistical analyses. Significance\nwas tested using linear mixed-effects models, as implemented in the Python package\nstatsmodels [100], for production similarity (LME 1), generalization score (LME 3),\ngeneralization convergence (LME 4), as well as production similarity to humans in\nmemorization (LME 2) and generalization (LME 5). We use the structure score and\nthe logarithmized round number in all measures as a fixed effect. The number of rounds\nwas logarithmized following scaling laws of neural language models [60]. Both the struc-\nture score and the logarithmized round number were centered and scaled We consider\ntwo random effects: the random seed for initialization (which also determines the input\nlanguage) and the specific scene. For LME 5, scaling the log-transformed round number\nto unit variance hindered convergence, so the log rounds were only centered. The full\nresults of the statistical models are provided in Tab. 4 in the SI, with partial regression\nplots shown in Figs. 13–17. In Tab. 5 in the SI, we provide an additional analysis of\nproduction similarity to ground truth at rounds 10, 40, 70, and 100.\nCode Availability\nThe code for reproducing our experiments is available on GitHub https://github.com/\nlgalke/easy2deeplearn.\nReferences\n[1] Jacob Andreas. Measuring compositionality in representation learning. In Proc.\nof ICLR. OpenReview.net, 2019.\n[2] Brenden M. Lake and Marco Baroni.\nHuman-like systematic generalization\nthrough a meta-learning neural network. Nature, 623(7985):115–121, November\n2023. ISSN 1476-4687.\n[3] Zolt´an Gendler Szab´o. Compositionality. In The Stanford Encyclopedia of Philos-\nophy. Metaphysics Research Lab, Stanford University, Fall 2022 edition, 2022.\n[4] Jerry A Fodor and Ernest Lepore. The compositionality papers. Oxford University\nPress, 2002.\n[5] Theo M.V. Janssen. Frege, Contextuality and Compositionality. Journal of Logic,\nLanguage and Information, 10(1):115–136, March 2001.\nISSN 1572-9583.\ndoi:\n10.1023/A:1026542332224.\n20\n[6] Matthew S. Dryer and Martin Haspelmath. The World Atlas of Language Struc-\ntures Online. Leipzig: Max Planck Institute for Evolutionary Anthropology, 2013.\n[7] Nicholas Evans and Stephen C. Levinson.\nThe myth of language universals:\nLanguage diversity and its importance for cognitive science.\nBehavioral and\nBrain Sciences, 32(5):429–448, 2009. ISSN 1469-1825, 0140-525X. doi: 10.1017/\nS0140525X0999094X.\n[8] Farrell Ackerman and Robert Malouf. Morphological organization: the low condi-\ntional entropy conjecture. Language, 89(3):429–464, 2013. ISSN 0097-8507.\n[9] Christian Bentz and Aleksandrs Berdicevskis. Learning pressures reduce morpho-\nlogical complexity: Linking corpus, computational and experimental evidence. In\nProceedings of the Workshop on Computational Linguistics for Linguistic Complex-\nity (CL4LC), pages 222–232, Osaka, Japan, 2016. The COLING 2016 Organizing\nCommittee.\n[10] Kees Hengeveld and Sterre Leufkens.\nTransparent and non-transparent lan-\nguages. Folia Linguistica, 52(1):139–175, 2018. ISSN 0165-4004. doi: 10.1515/\nflin-2018-0003.\n[11] Molly Lewis and Michael C Frank. Linguistic niches emerge from pressures at\nmultiple timescales. In CogSci, 2016.\n[12] Gary Lupyan and Rick Dale. Language structure is partly determined by social\nstructure. PloS one, 5(1), 2010.\n[13] Stewart M. McCauley and Morten H. Christiansen. Language learning as language\nuse: A cross-linguistic model of child language development. Psychological review,\n126(1):1, 2019.\n[14] Shijie Wu, Ryan Cotterell, and Timothy O’Donnell. Morphological irregularity\ncorrelates with frequency.\nIn Proc. of ACL, pages 5117–5126, Florence, Italy,\n2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1505.\n[15] Henry Brighton and Simon Kirby. Understanding linguistic evolution by visualizing\nthe emergence of topographic mappings. Artif. Life, 12(2):229–242, 2006.\n[16] Ekin Akyurek and Jacob Andreas. LexSym: Compositionality as lexical symmetry.\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 639–657, Toronto, Canada, July 2023. Association for\nComputational Linguistics. doi: 10.18653/v1/2023.acl-long.38.\n[17] Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar,\nTiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Ara-\nbella Sinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar Batsuren, Kaiser\n21\nSun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, Ryan Cot-\nterell, and Zhijing Jin. State-of-the-art generalisation research in NLP: A taxon-\nomy and review. ArXiv preprint, abs/2210.03050, 2023.\n[18] Zhenlin Xu, Marc Niethammer, and Colin A Raffel. Compositional generaliza-\ntion in unsupervised compositional representation learning: A study on disentan-\nglement and emergent language. In Advances in Neural Information Processing\nSystems 35, pages 25074–25087, 2022.\n[19] Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality\ndecomposed: how do neural networks generalise? Journal of Artificial Intelligence\nResearch, 67:757–795, 2020.\n[20] Brenden M. Lake and Marco Baroni. Generalization without systematicity: On\nthe compositional skills of sequence-to-sequence recurrent networks. In Proc. of\nICML, pages 2879–2888. PMLR, 2018.\n[21] Najoung Kim and Tal Linzen. COGS: A compositional generalization challenge\nbased on semantic interpretation.\nIn Proc. of EMNLP, pages 9087–9105, On-\nline, 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.\nemnlp-main.731.\n[22] Marco Baroni. Linguistic generalization and compositionality in modern artificial\nneural networks. Philosophical Transactions of the Royal Society B, 375(1791):\n20190307, 2020.\n[23] Cinjon Resnick, Abhinav Gupta, Jakob N. Foerster, Andrew M. Dai, and\nKyunghyun Cho. Capacity, bandwidth, and compositionality in emergent language\nlearning. In AAMAS, pages 1125–1133. International Foundation for Autonomous\nAgents and Multiagent Systems, 2020.\n[24] Tom´as Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.\nDistributed representations of words and phrases and their compositionality. In\nAdvances in Neural Information Processing Systems 26, pages 3111–3119, 2013.\n[25] Robert M. DeKeyser. What Makes Learning Second-Language Grammar Difficult?\nA Review of Issues. Language Learning, 55(S1):1–25, 2005. ISSN 1467-9922. doi:\n10.1111/j.0023-8333.2005.00294.x.\n[26] Vera Kempe and Patricia J. Brooks. Second language learning of complex inflec-\ntional systems. Language Learning, 58(4):703–746, 2008. doi: 10.1111/j.1467-9922.\n2008.00477.x.\n[27] Vera Kempe and Brian MacWhinney. The acquisition of case marking by adult\nlearners of Russian and German. Studies in second language acquisition, 20(4):\n543–587, 1998.\n22\n[28] Limor Raviv, Marianne de Heer Kloots, and Antje Meyer. What makes a language\neasy to learn? a preregistered study on how systematic structure and community\nsize affect language learnability. Cognition, 210, 2021. doi: https://doi.org/10.\n1016/j.cognition.2021.104620.\n[29] Simon Kirby and Monica Tamariz.\nCumulative cultural evolution, population\nstructure, and the origin of combinatoriality in human language. Philosophical\nTransactions of the Royal Society B: Biological Sciences, 2021. ISSN 0962-8436.\n[30] Limor Raviv and Inbal Arnon. Systematicity, but not compositionality: Examining\nthe emergence of linguistic structure in children and adults using iterated learning.\nCognition, 181:160–173, 2018. ISSN 0010-0277. doi: 10.1016/j.cognition.2018.08.\n011.\n[31] Hannah Cornish, Rick Dale, Simon Kirby, and Morten H. Christiansen.\nSe-\nquence Memory Constraints Give Rise to Language-Like Structure through It-\nerated Learning.\nPLOS ONE, 12(1):e0168532, 2017.\nISSN 1932-6203.\ndoi:\n10.1371/journal.pone.0168532.\n[32] Simon Kirby, Hannah Cornish, and Kenny Smith. Cumulative cultural evolution\nin the laboratory: An experimental approach to the origins of structure in human\nlanguage. Proceedings of the National Academy of Sciences, 105(31):10681–10686,\n2008.\n[33] Simon Kirby. Learning, bottlenecks and the evolution of recursive syntax, 2002.\n[34] Simon Kirby, Kenny Smith, and Henry Brighton. From UG to universals: Lin-\nguistic adaptation through iterated learning. Studies in Language, 28(3):587–607,\n2004.\n[35] Willem H. Zuidema. How the poverty of the stimulus solves the poverty of the\nstimulus. In Advances in Neural Information Processing Systems 15, pages 43–50.\nMIT Press, 2002.\n[36] Monica Tamariz and Simon Kirby. The cultural evolution of language. Current\nOpinion in Psychology, 8:37–43, 2016. ISSN 2352250X. doi: 10.1016/j.copsyc.\n2015.09.003.\n[37] Simon Kirby, Monica Tamariz, Hannah Cornish, and Kenny Smith. Compression\nand communication in the cultural evolution of linguistic structure. Cognition,\n141:87–102, 2015.\n[38] Yasamin Motamedi, Marieke Schouwstra, Kenny Smith, Jennifer Culbertson, and\nSimon Kirby. Evolving artificial sign languages in the lab: From improvised gesture\nto systematic sign. Cognition, 192:103964, 2019. ISSN 0010-0277. doi: 10.1016/j.\ncognition.2019.05.001.\n23\n[39] Yasamin Motamedi, Kenny Smith, Marieke Schouwstra, Jennifer Culbertson, and\nSimon Kirby. The emergence of systematic argument distinctions in artificial sign\nlanguages. Journal of Language Evolution, 6(2):77–98, 2021.\n[40] Jon W. Carr, Kenny Smith, Jennifer Culbertson, and Simon Kirby. Simplicity and\ninformativeness in semantic category systems. Cognition, 202:104289, 2020. ISSN\n0010-0277. doi: 10.1016/j.cognition.2020.104289.\n[41] Michael Tomasello. Constructing a language: A usage-based theory of language\nacquisition. Harvard university press, 2005.\n[42] Belinda Z. Li, Maxwell Nye, and Jacob Andreas. Implicit representations of mean-\ning in neural language models. In Proc. of ACL, pages 1813–1827. Association for\nComputational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.143.\n[43] Roma Patel and Ellie Pavlick. Mapping language models to grounded conceptual\nspaces. In Proc. of ICLR. OpenReview.net, 2022.\n[44] Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Vi´egas, Hanspeter Pfister,\nand Martin Wattenberg. Emergent world representations: Exploring a sequence\nmodel trained on a synthetic task. In Proc. of ICLR. OpenReview.net, 2023.\n[45] Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick,\nand Anders Søgaard. Can language models encode perceptual structure without\ngrounding? a case study in color. In Proceedings of the 25th Conference on Com-\nputational Natural Language Learning, pages 109–132, Online, 2021. Association\nfor Computational Linguistics. doi: 10.18653/v1/2021.conll-1.9.\n[46] Shashank Srikant, Ben Lipkin, Anna A Ivanova, Evelina Fedorenko, and Una-May\nO’Reilly. Convergent representations of computer programs in human and artificial\nneural networks. In Advances in Neural Information Processing Systems 35, 2022.\n[47] Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A. Hos-\nseini, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. The neural\narchitecture of language: Integrative modeling converges on predictive processing.\nProceedings of the National Academy of Sciences, 118(45):e2105646118, 2021.\n[48] Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Antonia Creswell,\nDharshan Kumaran, James L. McClelland, and Felix Hill. Language models show\nhuman-like content effects on reasoning. ArXiv preprint, abs/2207.07051, 2022.\n[49] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-\njamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\n24\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems 33, 2020.\n[50] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fe-\ndus. Emergent abilities of large language models. ArXiv preprint, abs/2206.07682,\n2022.\n[51] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora,\nSydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Ni-\nladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya\nDemszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon,\nJohn Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lau-\nren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tat-\nsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle\nHsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri,\nSiddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei\nKoh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the op-\nportunities and risks of foundation models. ArXiv preprint, abs/2108.07258, 2021.\n[52] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al. Language models are unsupervised multitask learners. OpenAI\nblog, 1(8):9, 2019.\n[53] Mathieu Rita, Corentin Tallec, Paul Michel, Jean-Bastien Grill, Olivier Pietquin,\nEmmanuel Dupoux, and Florian Strub. Emergent communication: Generalization\nand overfitting in lewis games.\nIn Advances in Neural Information Processing\nSystems 35, 2022.\n[54] Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux,\nand Marco Baroni. Compositionality and generalization in emergent languages.\nIn Proc. of ACL, pages 4427–4442, Online, 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.acl-main.407.\n[55] Satwik Kottur, Jos´e Moura, Stefan Lee, and Dhruv Batra. Natural language does\nnot emerge ‘naturally’ in multi-agent dialog. In Proc. of EMNLP, pages 2962–\n2967, Copenhagen, Denmark, 2017. Association for Computational Linguistics.\ndoi: 10.18653/v1/D17-1321.\n[56] Fushan Li and Michael Bowling. Ease-of-teaching and language structure from\nemergent communication. In Advances in Neural Information Processing Systems\n32, pages 15825–15835, 2019.\n25\n[57] Henry Conklin and Kenny Smith. Compositionality with variation reliably emerges\nin neural networks. In The Eleventh International Conference on Learning Repre-\nsentations, 2023. URL https://openreview.net/forum?id=-Yzz6vlX7V-.\n[58] Lukas Galke, Yoav Ram, and Limor Raviv. Emergent communication for under-\nstanding human language evolution: What’s missing? In Emergent Communica-\ntion Workshop at ICLR 2022, 2022.\n[59] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and\nIlya Sutskever. Deep double descent: Where bigger models and more data hurt.\nIn Proc. of ICLR. OpenReview.net, 2020.\n[60] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling\nLaws for Neural Language Models. ArXiv preprint, abs/2001.08361, 2020.\n[61] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern\nmachine-learning practice and the classical bias–variance trade-off. Proceedings of\nthe National Academy of Sciences, 116(32):15849–15854, 2019.\n[62] Sanjeev Arora, Nadav Cohen, and Elad Hazan.\nOn the optimization of deep\nnetworks: Implicit acceleration by overparameterization. In Jennifer Dy and An-\ndreas Krause, editors, Proceedings of the 35th International Conference on Ma-\nchine Learning, volume 80, pages 244–253. PMLR, 10–15 Jul 2018.\n[63] David JC MacKay. Information theory, inference and learning algorithms. Cam-\nbridge university press, 2003.\n[64] George Cybenko. Approximation by superpositions of a sigmoidal function. Math.\nControl. Signals Syst., 2(4):303–314, 1989. doi: 10.1007/BF02551274.\n[65] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian\nTram`er, and Chiyuan Zhang. Quantifying memorization across neural language\nmodels. CoRR, abs/2202.07646, 2022.\n[66] Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan.\nMemorization without overfitting: Analyzing the training dynamics of large lan-\nguage models. In Advances in Neural Information Processing Systems 35, pages\n38274–38290, 2022.\n[67] Zellig S. Harris. Distributional Structure. WORD, 10(2-3):146–162, 1954. ISSN\n0043-7956, 2373-5112. doi: 10.1080/00437956.1954.11659520.\n[68] Limor Raviv, Antje Meyer, and Shiri Lev-Ari. Larger communities create more\nsystematic languages.\nProceedings of the Royal Society B, 286(1907):20191262,\n2019.\n26\n[69] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,\nPeter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\nTraining lan-\nguage models to follow instructions with human feedback. In Advances in Neural\nInformation Processing Systems 35, pages 27730–27744, 2022.\n[70] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and\ntell: A neural image caption generator. In IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages\n3156–3164. IEEE Computer Society, 2015. doi: 10.1109/CVPR.2015.7298935.\n[71] Angeliki Lazaridou and Marco Baroni. Emergent multi-agent communication in\nthe deep learning era, 2020.\n[72] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Com-\nputation, 9(8):1735–1780, 1997. doi: 10.1162/neco.1997.9.8.1735.\n[73] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A\nsimple framework for contrastive learning of visual representations. In Proc. of\nICML, volume 119, pages 1597–1607. PMLR, 2020.\n[74] Wai Keen Vong, Wentao Wang, A. Emin Orhan, and Brenden M. Lake. Grounded\nlanguage acquisition through the eyes and ears of a single child.\nScience, 383\n(6682):504–511, February 2024. doi: 10.1126/science.adi1374.\n[75] Steven Piantadosi. Modern language models refute Chomsky’s approach to lan-\nguage, 2023.\n[76] Steven T. Piantadosi and Evelina Fedorenko. Infinitely productive language can\narise from chance under communicative pressure. Journal of Language Evolution,\n2(2):141–147, 2017. ISSN 2058-4571. doi: 10.1093/jole/lzw013.\n[77] Imme Lammertink, Mary Bazioni, Marianne de Heer Kloots, and Limor Raviv.\nLearnability effects in Children: are more structured languages easier to learn?,\nJuly 2022. URL https://osf.io/w89ju.\n[78] Andor Diera, Abdelhalim Dahou, Lukas Galke, Fabian Karl, Florian Sihler, and\nAnsgar Scherp. GenCodeSearchNet: A benchmark test suite for evaluating gener-\nalization in programming language understanding. In Proceedings of the 1st Gen-\nBench Workshop on (Benchmarking) Generalisation in NLP, pages 12–24, Singa-\npore, December 2023. Association for Computational Linguistics. doi: 10.18653/\nv1/2023.genbench-1.2. URL https://aclanthology.org/2023.genbench-1.2.\n[79] Jean Berko. The Child’s Learning of English Morphology. WORD, 14(2-3):150–\n177, 1958. ISSN 0043-7956, 2373-5112. doi: 10.1080/00437956.1958.11659661.\n27\n[80] Irit Meir, Assaf Israel, Wendy Sandler, Carol A. Padden, and Mark Aronoff. The\ninfluence of community on language structure: evidence from two young sign lan-\nguages. Linguistic Variation, 12(2):247–291, 2012.\n[81] Christian Bentz, Annemarie Verkerk, Douwe Kiela, Felix Hill, and Paula Buttery.\nAdaptive Communication: Languages with More Non-Native Speakers Tend to\nHave Fewer Word Forms. PLOS ONE, 10(6), 2015. ISSN 1932-6203. doi: 10.\n1371/journal.pone.0128254.\n[82] Alham Fikri Aji, Genta Indra Winata, Fajri Koto, Samuel Cahyawijaya, Ade Ro-\nmadhony, Rahmad Mahendra, Kemal Kurniawan, David Moeljadi, Radityo Eko\nPrasojo, Timothy Baldwin, Jey Han Lau, and Sebastian Ruder.\nOne country,\n700+ languages: NLP challenges for underrepresented languages and dialects in\nIndonesia. In Proceedings of the 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pages 7226–7249, Dublin, Ire-\nland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/\n2022.acl-long.500. URL https://aclanthology.org/2022.acl-long.500.\n[83] Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr\nLocatelli, Marzieh Fadaee, Ahmet ¨Ust¨un, and Sara Hooker. To code, or not to\ncode? exploring impact of code in pre-training, 2024. URL https://arxiv.org/\nabs/2408.10914.\n[84] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent co-\noperation and the emergence of (natural) language. In Proc. of ICLR. OpenRe-\nview.net, 2017.\n[85] Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. Emer-\ngence of linguistic communication from referential games with symbolic and pixel\ninput. In Proc. of ICLR. OpenReview.net, 2018.\n[86] Yi Ren, Shangmin Guo, Matthieu Labeau, Shay B. Cohen, and Simon Kirby.\nCompositional languages emerge in a neural iterated learning model. In Proc. of\nICLR. OpenReview.net, 2020.\n[87] Jesse Mu and Noah D. Goodman. Emergent communication of generalizations. In\nAdvances in Neural Information Processing Systems 34, pages 17994–18007, 2021.\n[88] Mathieu Rita, Florian Strub, Jean-Bastien Grill, Olivier Pietquin, and Emmanuel\nDupoux. On the role of population heterogeneity in emergent communication. In\nProc. of ICLR. OpenReview.net, 2022.\n[89] Rahma Chaabouni, Florian Strub, Florent Altch´e, Eugene Tarassov, Corentin Tal-\nlec, Elnaz Davoodi, Kory Wallace Mathewson, Olivier Tieleman, Angeliki Lazari-\ndou, and Bilal Piot. Emergent communication at scale. In Proc. of ICLR. Open-\nReview.net, 2022.\n28\n[90] Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, and Marco Baroni.\nAnti-efficient encoding in emergent communication. In Advances in Neural Infor-\nmation Processing Systems 32, pages 6290–6300, 2019.\n[91] Chaz Firestone.\nPerformance vs. competence in human–machine comparisons.\nProceedings of the National Academy of Sciences, 117(43):26562–26571, 2020.\n[92] Philippe G. Schyns, Lukas Snoek, and Christoph Daube. Degrees of algorithmic\nequivalence between the brain and its DNN models. Trends in Cognitive Sciences,\n26(12):1090–1102, December 2022. ISSN 1364-6613. doi: 10.1016/j.tics.2022.09.\n003.\n[93] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning rep-\nresentations by back-propagating errors. Nature, 323(6088):533–536, 1986. ISSN\n1476-4687.\n[94] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press,\n2016. http://www.deeplearningbook.org.\n[95] Nelson Cowan. The magical number 4 in short-term memory: A reconsideration\nof mental storage capacity. Behavioral and brain sciences, 24(1):87–114, 2001.\n[96] Roger Ratcliff. Connectionist models of recognition memory: Constraints imposed\nby learning and forgetting functions. Psychological Review, 97(2):285–308, 1990.\nISSN 1939-1471. doi: 10.1037/0033-295X.97.2.285.\n[97] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into\nrectifiers: Surpassing human-level performance on imagenet classification. In Proc\nof ICCV, pages 1026–1034. IEEE Computer Society, 2015. doi: 10.1109/ICCV.\n2015.123.\n[98] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-\ngory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Al-\nban Desmaison, Andreas K¨opf, Edward Yang, Zachary DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and\nSoumith Chintala. Pytorch: An imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Processing Systems 32, pages 8024–\n8035, 2019.\n[99] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\nIn Proc. of ICLR, 2015.\n[100] Skipper Seabold and Josef Perktold.\nstatsmodels: Econometric and statistical\nmodeling with python. In 9th Python in Science Conference, 2010.\n29\nAcknowledgments\nWe thank Dota Tianai Dong, Koen de Reus, Yosef Prat, Tal Simon, Willem Zuidema,\nTessa Verhoef, Mitja Nikolaus, Marieke Woensdregt, and Adam Kohan for their com-\nments and discussions. We thank Shinje Wu and Marianne de Heer Kloots for sharing\ntheir data.\n30\nA. Supplementary Information\nDetails of the input languages\nTable 3.\nExtended Results\nProduction Similarity to Ground Truth during Memorization\nFigure 6\nFigure 6: Production Similarity to the ground truth of the input language as a function of\nround number. Color indicates the degree of structure (darker means higher).\nStars indicate where neural network agents exceed human performance.\nProduction Similarity to Humans during Memorization\nFigure 7\nSystematicity during Generalization\nFigure 8\n31\nInput Language\nStructure Score\nAmbiguity %\nStructure Bin\nS1\n0.09\n0\n1\nB1\n0.07\n0\n1\nS2\n0.25\n0.35\n2\nB2\n0.35\n0.09\n2\nS3\n0.59\n0.13\n3\nB3\n0.58\n0.17\n3\nS4\n0.79\n0\n4\nB4\n0.69\n0\n4\nS5\n0.84\n0\n5\nB5\n0.85\n0\n5\nTable 3: Structure scores of the input languages\nFigure 7: Production similarity to humans learning the same input language during\nmemorization. Color indicates the degree of structure (darker means higher).\n32\nFigure 8: Generalization score as a function of round number.\nColor represents the\ndegree of structure. Stars indicate where neural network agents exceed human\nperformance.\n33\nFigure 9: Convergence score as a function of round number. Color indicates the degree\nof structure (darker means higher). Stars indicate where neural network agents\nexceed human performance.\nConvergence Score\nFigure 9\nProduction Similarity to Humans during Generalization\nFigure 10\nBinary Accuracy with respect to Ground Truth during Memorization\nFigure 11\nGuessing Accuracy\nFigure 12\nStatistical Analyses of the Results\nTable 4 shows the results of the statistical models LME 1, LME 2, LME 3, LME 4,\nLME 5, and LME 6.\n34\nFigure 10: Production Similarity to humans during generalization. Color indicates the\ndegree of structure (darker means higher).\nLME 1: Production Similarity during Memorization\nThe dependent variable is the production similarity to the ground truth in the memo-\nrization test rounds rounds. Figure 13 shows the partial regression plots.\nLME 2: Production Similarity to Humans during Memorization\nThe dependent variable is the production similarity to human participants in the mem-\norization test rounds. Figure 14 shows the partial regression plots.\nLME 3: Systematicity during Generalization\nThe dependent variable is the generalization score in the generalization test rounds.\nFigure 15 shows the partial regression plots.\nLME 4: Convergence Score during Generalization\nThe dependent variable is the convergence score in the generalization test rounds. Fig-\nure 16 shows the partial regression plots.\n35\nFigure 11: Binary Accuracy with respect to ground truth of input languages as a func-\ntion of round number. To compute binary accuracy, we compare the labels\nproduced by the neural agents with the ground truth label of the input lan-\nguage and each label receives a score of one if it is exactly the same as the\nground truth and zero otherwise. These boolean scores are then averaged\nto obtain binary accuracy. Color indicates the degree of structure (darker\nmeans higher). Stars indicate where neural network agents exceed human\nperformance.\nLME 5: Production Similarity to Humans during Generalization\nThe dependent variable is the production similarity to human participants in the gener-\nalization test rounds. Figure 17 shows the partial regression plots.\nLME 6: Production Similarity to Ground Truth at Specific Rounds\nTable 5 shows the results of the statistical models for the production similarity to ground\ntruth during memorization at specific rounds: 10, 40, 70, and 100.\nSensitivity to Hyperparameters\nWe found the training robust to most experimental configurations, such as learning rate,\nnumber of layers, and whether the parameters are shared between reader and writer\nmodels. However, one particular hyperparameter that affects the model capacity has\n36\nTable 4: Linear Mixed-Effects Regression Results.\nLME 1: Production similarity to\nground truth during memorization. LME 2: Production similarity to Humans\nduring Generalization. LME 3: Generalization Score. All tests are two-sided.\nLME 1: Production Similarity to Ground Truth\nCoef.\nStd.Err.\nz\nP> |z|\n[0.025\n0.975]\nIntercept\n0.797\n0.001\n1110.436\n0.000\n0.796\n0.798\nscale(StructureScore)\n0.045\n0.001\n62.865\n0.000\n0.044\n0.047\nscale(np.log(Round))\n0.199\n0.000\n2060.282\n0.000\n0.199\n0.199\nscale(StructureScore):scale(np.log(Round))\n-0.005\n0.000\n-54.978\n0.000\n-0.005\n-0.005\nLME 2: Prod. Sim. to Humans during Memorization\nCoef.\nStd.Err.\nz\nP> |z|\n[0.025\n0.975]\nIntercept\n0.701\n0.001\n517.888\n0.000\n0.698\n0.704\nscale(StructureScore)\n0.097\n0.001\n71.429\n0.000\n0.094\n0.099\nscale(np.log(Round))\n0.156\n0.000\n1504.189\n0.000\n0.155\n0.156\nscale(StructureScore):scale(np.log(Round))\n0.022\n0.000\n208.708\n0.000\n0.021\n0.022\nLME 3: Generalization Score\nCoef.\nStd.Err.\nz\nP> |z|\n[0.025\n0.975]\nIntercept\n0.468\n0.001\n790.838\n0.000\n0.467\n0.469\nscale(StructureScore)\n0.088\n0.001\n148.901\n0.000\n0.087\n0.089\nscale(np.log(Round))\n0.084\n0.000\n1281.568\n0.000\n0.084\n0.084\nscale(StructureScore):scale(np.log(Round))\n0.046\n0.000\n703.483\n0.000\n0.046\n0.046\nLME 4: Convergence Score\nCoef.\nStd.Err.\nz\nP> |z|\n[0.025\n0.975]\nIntercept\n0.792\n0.001\n900.121\n0.000\n0.790\n0.794\nscale(StructureScore)\n0.043\n0.001\n49.027\n0.000\n0.041\n0.045\nscale(np.log(Round))\n0.094\n0.000\n1220.090\n0.000\n0.094\n0.094\nscale(StructureScore):scale(np.log(Round))\n0.009\n0.000\n121.740\n0.000\n0.009\n0.010\nLME 5: Prod. Sim. to Humans during Generalization\nCoef.\nStd.Err.\nz\nP> |z|\n[0.025\n0.975]\nIntercept\n0.529\n0.002\n280.903\n0.000\n0.525\n0.533\nscale(StructureScore)\n0.132\n0.002\n70.280\n0.000\n0.129\n0.136\ncenter(np.log(Round))\n0.101\n0.000\n749.746\n0.000\n0.101\n0.101\nscale(StructureScore):center(np.log(Round))\n0.046\n0.000\n344.287\n0.000\n0.046\n0.047\n37\nFigure 12: Guessing Accuracy: selecting the right scene among distractors within the\ncontrastive training objective during training. Guessing accuracy was calcu-\nlated during training with active dropout since it was part of the training\nphase.\nTable 5: Linear Mixed-Effects Regression Results: Production Similarity to Humans dur-\ning Memorization at a Fixed Round Number. All tests are two-sided.\nRound 10\nCoef.\nStd.Err.\nz\nP> |z|\n[0.025\n0.975]\nIntercept\n0.459\n0.000\n1009.015\n0.000\n0.458\n0.459\nscale(StructureScore)\n0.024\n0.001\n16.470\n0.000\n0.021\n0.027\nRound 40\nCoef.\nStd.Err.\nz\nP> |z|\n[0.025\n0.975]\nIntercept\n0.830\n0.000\n2370.225\n0.000\n0.829\n0.831\nscale(StructureScore)\n0.094\n0.001\n78.439\n0.000\n0.091\n0.096\nRound 70\nCoef.\nStd.Err.\nz\nP> |z|\n[0.025\n0.975]\nIntercept\n0.936\n0.000\n19666.003\n0.000\n0.936\n0.936\nscale(StructureScore)\n0.021\n0.001\n23.322\n0.000\n0.020\n0.023\nRound 100\nCoef.\nStd.Err.\nz\nP> |z|\n[0.025\n0.975]\nIntercept\n0.965\n0.000\n31871.611\n0.000\n0.965\n0.965\nscale(StructureScore)\n0.005\n0.001\n7.725\n0.000\n0.004\n0.007\n38\nFigure 13: Partial regression plots of LME 1: Production Similarity to ground truth\nduring memorization\n39\nFigure 14: Partial regression plots of LME 1: Production Similarity to humans during\nMemorization\n40\nFigure 15: Partial regression plots of LME 3: Generalization Systematicity\n41\nFigure 16: Partial regression plots of LME 4: Convergence Score (during Generlization)\n42\nFigure 17: Partial regression plots of LME 5: Production similarity to humans during\ngeneralization\n43\nFigure 18: Average production similarity to ground truth during memorization across\ninput languages as a function of the size of the neural networks’ hidden layers.\na substantial effect on the learning speed: the size of the hidden layers, for which we\nprovide a sensitivity analysis in the following. Subsequently, we also provide a sensitivity\nanalysis of the scaling factor α for the contrastive loss term Lcon.\nSensitivity to Hidden Layer Size\nWe vary the hidden size and plot the average scores over the 10 input languages. Fig-\nure 18 shows the production similarity during memorization. Figure 19 shows the pro-\nduction similarity between neural agents and human learners during testing. Figure 20\nshows the generalization score of neural agents.\nSensitivity to the Scaling Factor for the Contrastive Loss Term\nWe experiment with different scaling factors α for the contrastive loss term. Figure 21\nshows the results for production similarity with ground truth during the memorization\ntest. Here, we report the average across all input languages with different degrees of\nstructuredness. Figure 22 shows the results for production similarity with human learners\nduring the generalization test. Again, a scaling factor of 0.1 leads to the best results in\nterms of learning speed. However, there is little difference to using a scaling factor of\n0.2. Figure 23 shows the results for generalization score (scaled to [0, 1]). We observe\nthat the scaling factor of 0.1 has advantages in learning speed. Starting at step 1,300,\nthe generalization score increases faster with scaling factor 0.1 than with other scaling\nfactors.\n44\nFigure 19: Average production similarity to humans across input languages as a function\nof the size of the neural networks’ hidden layers.\nFigure 20: Average generalization systematicity across input languages as a function of\nthe size of the neural networks’ hidden layers\n45\nFigure 21: Average production similarity to ground truth across input languages during\nmemorization as a function of the scaling factor for the contrastive loss term.\n46\nFigure 22: Average production similarity with human learners across input languages\nduring generalization as a function of the scaling factor for the contrastive\nloss term.\nFigure 23: Average generalization score across input languages as a function of the scal-\ning factor for the contrastive loss term.\n47\nStructure effect does not depend on specific hyperparameter choices\nFigure 24 and Figure 25 show the relationship between the degree of compositional\nstructure and the generalizations core with respect to the hidden size and the scaling\nfactor for the contrastive loss.\nFigure 24: Relationship between compositional structure and generalization score with\ndifferent values for hidden size controlling the model capacity\n48\nFigure 25: Relationship between compositional structure and generalization score with\ndifferent values for contrastive loss weight controlling the influence of the\ncontrastive objective used for guessing tasks\nExample Data\nWe provide example data from the memorization test in Table 6 and from the general-\nization test in Table 2. The sample is stratified with respect to the by-producer average\nproduction similarity to human participants. For each percentile out of 0, 25, 50, 75,\nand 100, we randomly sample 8 items. All samples are taken at the end of training (after\nepoch 100).\n49\nTable 6: Example Data from the Memorization Test.\nSim. to Humans\nProducer\nLang.\nShape\nAngle\nTrue Label\nParticipant Label\nRNN Label\n0.247826\n4082\nS1\n4\n225\nwufk\nmoof\nwufk\n0.247826\n4082\nS1\n2\n330\nweft\npuif\nweft\n0.247826\n4082\nS1\n1\n240\nsmoogg\nhuift\nsmoogg\n0.247826\n4082\nS1\n4\n300\nsuith\nkeft\nsuith\n0.247826\n4082\nS1\n3\n135\nspuut\nweft\nspuut\n0.247826\n4082\nS1\n1\n60\nkoof\npuif\nkoof\n0.247826\n4082\nS1\n4\n120\nwafk\nsgeft\nwafk\n0.247826\n4082\nS1\n3\n270\nhuipt\nkoom\nhuipt\n0.700932\n1093\nB2\n4\n270\ngnt\ngnt\ngnt\n0.700932\n1093\nB2\n4\n135\ngnts\ngntsi\ngnts\n0.700932\n1093\nB2\n2\n330\ngmps\ngmps\ngmps\n0.700932\n1093\nB2\n2\n120\nwangsus\nwangsus\nwangsu\n0.700932\n1093\nB2\n2\n45\ngempt\nwangsus\ngempt\n0.700932\n1093\nB2\n4\n330\ngntsi\ngnit\ngntsi\n0.700932\n1093\nB2\n2\n300\nwangsu\ngmps\nwangsu\n0.700932\n1093\nB2\n1\n30\nsket\nsket\nsket\n0.883282\n3090\nS5\n2\n180\nnif-a\nnif-a\nnif-as\n0.883282\n3090\nS5\n1\n90\nwef-t\nwef-t\nwef-t\n0.883282\n3090\nS5\n1\n150\nwef-aat\nwef-att\nwef-aat\n0.883282\n3090\nS5\n1\n240\nwef-ssa\nwef-aas\nwef-ssa\n0.883282\n3090\nS5\n4\n210\nsmut-aas\nsmut-aas\nsmut-aas\n0.883282\n3090\nS5\n3\n180\npti-a\npti-a\npti-a\n0.883282\n3090\nS5\n3\n315\npti-kks\npti-ssk\npti-kks\n0.883282\n3090\nS5\n2\n225\nnif-as\nnif-as\nnif-as\n0.956522\n1020\nB1\n2\n330\nwak-ta\nwak-ta\nwak-ta\n0.956522\n1020\nB1\n4\n225\ngtgt\ngtgt\ngtgt\n0.956522\n1020\nB1\n4\n120\nftft\nftft\nftft\n0.956522\n1020\nB1\n2\n150\nhehi\nhehi\nhehi\n0.956522\n1020\nB1\n1\n210\nha-ia\nha-ia\nha-ia\n0.956522\n1020\nB1\n3\n45\nfiti\nfiti\nfiti\n0.956522\n1020\nB1\n4\n60\nkite\nfik\nkite\n0.956522\n1020\nB1\n4\n360\npepepe\npepepe\npepepe\n1.000000\n1008\nB4\n4\n225\nfak-huif\nfak-huif\nfak-huif\n1.000000\n1008\nB4\n3\n45\nmuif-a\nmuif-a\nmuif-a\n1.000000\n1008\nB4\n4\n120\nfak-e\nfak-e\nfak-e\n1.000000\n1008\nB4\n1\n315\nfas-pok\nfas-pok\nfas-pok\n1.000000\n1008\nB4\n3\n135\nmuif-e\nmuif-e\nmuif-e\n1.000000\n1008\nB4\n2\n135\npok-e\npok-e\npok-e\n1.000000\n1008\nB4\n4\n300\nfak-pok\nfak-pok\nfak-pok\n1.000000\n1008\nB4\n2\n210\npok-huif\npok-huif\npok-huif\n50\nTable 7: Example Data from the Generalization Test.\nSim. to Humans\nProducer\nLang.\nShape\nAngle\nParticipant Label\nRNN Label\n0.092308\n1048\nB1\n2\n360\nkokoke\nseefe\n0.092308\n1048\nB1\n1\n225\npo-ti\nha-ia\n0.092308\n1048\nB1\n3\n90\nghio\nmimi\n0.092308\n1048\nB1\n4\n240\nkhio\ngtgt\n0.092308\n1048\nB1\n3\n150\nptpt\nmimi\n0.092308\n1048\nB1\n2\n300\nko-toe\nwak-ta\n0.092308\n1048\nB1\n4\n210\nka-ia\ngtgt\n0.092308\n1048\nB1\n2\n225\nhaia\npooti\n0.443223\n8064\nB2\n3\n150\nwangsi\nwangsuu\n0.443223\n8064\nB2\n4\n225\ngntsoe\ngntuu\n0.443223\n8064\nB2\n1\n135\nsketsi\ngesh\n0.443223\n8064\nB2\n4\n360\ngnt\nskek\n0.443223\n8064\nB2\n4\n60\ngmpsi\nskek\n0.443223\n8064\nB2\n2\n270\nwng\nwangsuu\n0.443223\n8064\nB2\n2\n360\nwang\ngempt\n0.443223\n8064\nB2\n3\n225\nwangsuus\nwangsoe\n0.590884\n1083\nS3\n4\n60\nfuottee\nfuoto-o-o-o\n0.590884\n1083\nS3\n4\n150\nfuottoo\nfuottii\n0.590884\n1083\nS3\n1\n30\nfewo-o-o-o\nfewen\n0.590884\n1083\nS3\n3\n60\npowi\npowu-u-u\n0.590884\n1083\nS3\n1\n225\nfewo-o-o-o\nfewo-o-o\n0.590884\n1083\nS3\n3\n225\npowee\npowwoo\n0.590884\n1083\nS3\n2\n360\nasken\nasko-o-o\n0.590884\n1083\nS3\n4\n330\nfuottoa\nfuotio\n0.772497\n2058\nB4\n2\n360\npok-i\npok\n0.772497\n2058\nB4\n4\n330\nfak-pok\nfas-i\n0.772497\n2058\nB4\n4\n60\nfak-a\nfak-e\n0.772497\n2058\nB4\n1\n30\nfas-a\nfas-a\n0.772497\n2058\nB4\n2\n300\npok\npok-u\n0.772497\n2058\nB4\n4\n90\nfak-u\nfak-e\n0.772497\n2058\nB4\n4\n150\nfak-w-w-e\nfak-e\n0.772497\n2058\nB4\n3\n225\nmuif-huif\nmuif-huif\n0.923443\n5016\nB4\n4\n90\nfak-iii\nfak-e\n0.923443\n5016\nB4\n1\n120\nfas-e\nfas-e\n0.923443\n5016\nB4\n2\n300\npok\npok\n0.923443\n5016\nB4\n3\n60\nmuif-a\nmuif-a\n0.923443\n5016\nB4\n3\n360\nmuif-i\nmuif-a\n0.923443\n5016\nB4\n4\n150\nfak-e\nfak-e\n0.923443\n5016\nB4\n3\n225\nmuif-huif\nmuif-huif\n0.923443\n5016\nB4\n4\n150\nfak-e\nfak-e\n51\n",
  "categories": [
    "cs.CL",
    "I.2.7"
  ],
  "published": "2023-02-23",
  "updated": "2024-10-10"
}