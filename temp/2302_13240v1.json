{
  "id": "http://arxiv.org/abs/2302.13240v1",
  "title": "Q-Cogni: An Integrated Causal Reinforcement Learning Framework",
  "authors": [
    "Cris Cunha",
    "Wei Liu",
    "Tim French",
    "Ajmal Mian"
  ],
  "abstract": "We present Q-Cogni, an algorithmically integrated causal reinforcement\nlearning framework that redesigns Q-Learning with an autonomous causal\nstructure discovery method to improve the learning process with causal\ninference. Q-Cogni achieves optimal learning with a pre-learned structural\ncausal model of the environment that can be queried during the learning process\nto infer cause-and-effect relationships embedded in a state-action space. We\nleverage on the sample efficient techniques of reinforcement learning, enable\nreasoning about a broader set of policies and bring higher degrees of\ninterpretability to decisions made by the reinforcement learning agent. We\napply Q-Cogni on the Vehicle Routing Problem (VRP) and compare against\nstate-of-the-art reinforcement learning algorithms. We report results that\ndemonstrate better policies, improved learning efficiency and superior\ninterpretability of the agent's decision making. We also compare this approach\nwith traditional shortest-path search algorithms and demonstrate the benefits\nof our causal reinforcement learning framework to high dimensional problems.\nFinally, we apply Q-Cogni to derive optimal routing decisions for taxis in New\nYork City using the Taxi & Limousine Commission trip record data and compare\nwith shortest-path search, reporting results that show 85% of the cases with an\nequal or better policy derived from Q-Cogni in a real-world domain.",
  "text": "Q-Cogni: An Integrated Causal Reinforcement Learning Framework\nCristiano da Costa Cunha1 , Wei Liu1 , Tim French1 , Ajmal Mian1\n1Department of Computer Science, University of Western Australia\ncris.dacostacunha@research.uwa.edu.au, {wei.liu, tim.french, ajmal.mian}@uwa.edu.au\nAbstract\nWe present Q-Cogni, an algorithmically integrated\ncausal reinforcement learning framework that re-\ndesigns Q-Learning with an autonomous causal\nstructure discovery method to improve the learn-\ning process with causal inference.\nQ-Cogni\nachieves optimal learning with a pre-learned struc-\ntural causal model of the environment that can be\nqueried during the learning process to infer cause-\nand-effect relationships embedded in a state-action\nspace. We leverage on the sample efﬁcient tech-\nniques of reinforcement learning, enable reasoning\nabout a broader set of policies and bring higher de-\ngrees of interpretability to decisions made by the\nreinforcement learning agent. We apply Q-Cogni\non the Vehicle Routing Problem (VRP) and com-\npare against state-of-the-art reinforcement learning\nalgorithms. We report results that demonstrate bet-\nter policies, improved learning efﬁciency and su-\nperior interpretability of the agent’s decision mak-\ning. We also compare this approach with traditional\nshortest-path search algorithms and demonstrate\nthe beneﬁts of our causal reinforcement learning\nframework to high dimensional problems. Finally,\nwe apply Q-Cogni to derive optimal routing deci-\nsions for taxis in New York City using the Taxi &\nLimousine Commission trip record data and com-\npare with shortest-path search, reporting results\nthat show 85% of the cases with an equal or bet-\nter policy derived from Q-Cogni in a real-world do-\nmain.\n1\nIntroduction\nEvidence suggests that the human brain operates as a dual\nsystem. One system learns to repeat actions that lead to a re-\nward, analogous to a model-free agent in reinforcement learn-\ning, the other learns a model of the environment which is used\nto plan actions, analogous to a model-based agent. These\nsystems coexist in both cooperation and competition which\nallows the human brain to negotiate a balance between cog-\nnitively cheap but inaccurate model-free algorithms and rela-\ntively precise but expensive model-based algorithms [Gersh-\nman, 2017].\nDespite the beneﬁts causal inference can bring to au-\ntonomous learning agents, the degree of integration in arti-\nﬁcial intelligence research are limited. This limitation be-\ncomes a risk, in particular that data-driven models are often\nused to infer causal effects. Solely relying on data that is\nnever bias-free, eventually leads to untrustworthy decisions\nand sub-optimal interventions [Prosperi et al., 2020].\nIn this paper, we present Q-Cogni a framework that in-\ntegrates autonomous causal structure discovery and causal\ninference into a model-free reinforcement learning method.\nThere are several emergent methods for integrating causal-\nity with reinforcement learning such as reward correction\n[Buesing et al., 2018], meta-reinforcement learning [Das-\ngupta et al., 2019], latent causal-transition models [Gasse\net al., 2021], schema networks [Kansky et al., 2017] and\nexplainable agents [Madumal et al., 2020].\nHowever, no\nmethod presents an approach that embeds causal reasoning,\nfrom an autonomously derived causal structure of the en-\nvironment, during the learning process of a reinforcement\nlearning agent to guide the generation of an optimal policy.\nThus, our approach is able to target improvements in policy\nquality, learning efﬁciency and interpretability concurrently.\nQ-Cogni samples data from an environment to discover\nthe causal structure describing the relationship between state\ntransitions, actions and rewards. This causal structure is then\nused to construct a Bayesian Network which is used during\na redesigned Q-Learning process where the agent interacts\nwith the environment guided by the probability of achieving\nthe goal and receiving rewards in a probabilistic manner. The\ncausal structure integrated with the learning procedure deliv-\ners higher sample efﬁciency as it causally manages the trade-\noff between exploration and exploitation, is able to derive a\nbroader set of policies as rewards are much less sparse and\nprovides interpretability of the agent’s decision making in the\nform of conditional probabilities related to each state transi-\ntion for a given set of actions.\nWe validate our approach on the Vehicle Routing Problem\n(VRP) [Toth and Vigo, 2002]. We start by comparing op-\ntimal learning metrics against state-of-the-art reinforcement\nlearning algorithms PPO [Schulman et al., 2017] and DDQN\n[Van Hasselt et al., 2016], using the Taxi-v3 environment\nfrom OpenAI Gym [Brockman et al., 2016]. We also com-\npare the advantages and disadvantages of Q-Cogni against the\nshortest-path search algorithms Djikstra’s [Dijkstra, 1959]\narXiv:2302.13240v1  [cs.LG]  26 Feb 2023\nand A* [Hart et al., 1968] with a particular focus in under-\nstanding applicability and scalability. Finally, we run exper-\niments in a real-world scale problem, using the New York\nCity TLC trip record data, which contains all taxi movements\nin New York City from 2013 to date [Taxi and Comission,\n2022], to validate Q-Cogni’s capabilities to autonomously\nroute taxis for a given pickup and drop-off.\nOur contributions with Q-Cogni are three-fold.\nFirstly,\nQ-Cogni is the ﬁrst fully integrated, explainable, domain-\nagnostic and hybrid model-based and model-free reinforce-\nment learning method that introduces autonomous causal\nstructure discovery to derive an efﬁcient model of the envi-\nronment and uses that causal structure within the learning\nprocess. Secondly, we redesigned the Q-Learning algorithm\nto use causal inference in the action selection process and a\nprobabilistic Q-function during training in order to optimise\npolicy learning. Finally, through extensive experiments, we\ndemonstrate Q-Cogni’s superior capability in achieving bet-\nter policies, improved learning efﬁciency and interpretability\nas well as near-linear scalability to higher dimension prob-\nlems in a real-world navigation context.\n2\nBackground\nThe focus of this work lies at the uniﬁcation of causal in-\nference and reinforcement learning.\nThis is an emerging\nﬁeld that aims to overcome challenges in reinforcement learn-\ning such as 1) the lack of ability to identify or react to\nnovel circumstances agents have not been programmed for\n[Darwiche, 2018; Chen and Liu, 2018], 2) low levels of\ninterpretability that erodes user’s trust and does not pro-\nmote ethical and unbiased systems [Ribeiro et al., 2016;\nMarcus, 2018] and 3) the lack of understanding of cause-and-\neffect relationships [Pearl, 2010].\nOur approach builds upon a wealth of previous contribu-\ntions to these areas, which we brieﬂy cover below.\nCausal Structure Discovery. Revealing causal information\nby analysing observational data, i.e. “causal structure discov-\nery”, has been a signiﬁcant area of recent research to over-\ncome the challenges with time, resources and costs by de-\nsigning and running experiments [Kuang et al., 2020].\nMost of the work associated with integrating causal struc-\nture discovery and reinforcement learning have been focused\non using reinforcement learning to discover cause-and-effect\nrelationships in environments which agents interact with to\nlearn [Zhu et al., 2019; Wang et al., 2021; Huang et al., 2020;\nAmirinezhad et al., 2022; Sauter et al., 2022]. To our knowl-\nedge, a small amount of work has explored the reverse appli-\ncation, such as schema networks [Kansky et al., 2017], coun-\nterfactual learning [Lu et al., 2020] and causal MDPs [Lu et\nal., 2022].\nWe build upon this work and redesign the way in which\nstructure causal models (SCMs) are used. In the related work\nthey are typically used to augment input data with what-if sce-\nnarios a-priori to the agent learning process. In our approach,\nthe SCM is embedded as part of a redesigned Q-Learning al-\ngorithm and only used during the learning process. Our ap-\nproach also enables learning a broader set of policies since\nwhat-if scenarios are estimated for each state-action pair dur-\ning the learning process. This not only improves policy opti-\nmality but also provides a superior sample efﬁciency as it al-\nlows for “shortcutting” the exploration step during the learn-\ning process.\nCausal Inference. Recent work has demonstrated the bene-\nﬁts of integrating causal inference in reinforcement learning.\nIn Seitzer, Sch¨olkopf, and Martius [2021] the authors\ndemonstrate improvement in policy quality by deriving a\nmeasure that captures the causal inﬂuence of actions on the\nenvironment in a robotics control environment and devise a\npractical method to integrate in the exploration and learning\nof reinforcement learning agents.\nIn Yang et al. [2022] the authors propose an augmented\nDQN algorithm which receives interference labels during\ntraining as an intervention into the environment and embed\na latent state into its model, creating resilience by learning to\nhandle abnormal event (e.g. frozen screens in Atari games).\nIn Gasse et al. [2021] the authors derive a framework to use\na structural causal model as a Partially Observable Markov\nDecision Process (POMDP) in model-based reinforcement\nlearning.\nLeveraging upon these concepts, in our approach we ex-\npand further the structural causal model and ﬁt it with a\nBayesian Network. This enables our redesigned Q-Learning\nprocedure to receive rewards as a function of the probability\nof achieving a goal for a given state-action pair, signiﬁcantly\nimproving the sample efﬁciency of the agent, as in each step\nthe agent is able to concurrently derive dense rewards for sev-\neral state transitions regulated by the causal structure. To our\nknowledge this is an integration perspective not yet explored.\nModel-Free Reinforcement Learning. Centre to the rein-\nforcement learning paradigm is the learning agent, which is\nthe “actor” that learns the optimal sequence of actions for a\ngiven task, i.e. the optimal policy. As this policy is not known\na priori, the aim is to develop an agent capable of learning it\nby interacting with the environment [Kaelbling et al., 1996],\nan approach known as model-free reinforcement learning.\nModel-free reinforcement learning relies on algorithms\nthat sample from experience and estimate a utility func-\ntion such as SARSA, Q-Learning and Actor-Critic methods\n[Arulkumaran et al., 2017]. Recent advances in deep learn-\ning have promoted growth of model-free methods[C¸ alıs¸ır and\nPehlivano˘glu, 2019]. However, whilst model-free reinforce-\nment learning is a promising area to enable human-level artiﬁ-\ncial intelligence, it comes with its own limitations. These are\nthe applicability restricted to a narrow set of assumptions (e.g.\na Markov Decision Process) that is not necessarily reﬂective\nof the dynamics of the real-world environment [John, 2020];\nlower performance when evaluating off-policy decisions (i.e.\npolicies different than those contained in the underlying data\nused by the agent) [Bannon et al., 2020] and perpetual partial\nobservability since sensory data provide imperfect informa-\ntion about the environment and hidden variables are often the\nones causally related to rewards [Gershman, 2017]. These\nlimitations are a disadvantage of model-free reinforcement\nlearning which can be overcome with explicit models of the\nenvironment, i.e. model-based reinforcement learning. How-\never, whilst the model-based approach would enhance sam-\nFigure 1: Q-Cogni causal reinforcement learning framework with its\nmodules highlighted.\nple efﬁciency, it also would come at a cost of increased com-\nputational complexity as many more samples are required to\nderive an accurate model of the environment [Polydoros and\nNalpantidis, 2017].\nIn our work we use causal structure discovery to simul-\ntaneously provide model-free reinforcement learning agents\nwith the ability of dealing with imperfect environments (e.g.,\nlatent variables) and maintain sample efﬁciency of a model-\nbased approach. A hybrid approach not extensively explored\nto our knowledge.\n3\nQ-Cogni\nWe present Q-Cogni, a framework that integrates autonomous\ncausal structure discovery, causal inference and reinforce-\nment learning. Figure 1 illustrates the modules and interfaces\nwhich we further detail below.\n3.1\nAutonomous Causal Structure Discovery\nThe ﬁrst module in Q-Cogni is designed to autonomously dis-\ncover the causal structure contained in an environment. It\nstarts by applying a random walk in the environment while\nstoring the state, actions and rewards. The number of steps\nrequired to visit every state in the environment with a random\nwalk is proportional to the harmonic number, approximating\nthe natural logarithm function which grows without limit, al-\nbeit slowly, demonstrating the efﬁciency of the process. This\nsampled dataset contains all the information necessary to de-\nscribe the full state-action space and its associated transitions.\nA further beneﬁt of our approach is that this step only needs\nto be performed once regardless of the environment conﬁgu-\nration.\nWe use the NOTEARS algorithm [Zheng et al., 2018] in the\nQ-Cogni framework, providing an efﬁcient method to derive\nthe causal structure encoded in the dataset sampled from the\nenvironment.\nThe resulting structure learned is then encoded as a DAG\nG with nodes v ∈G, state variables x, actions a and edges e\n∈G which represent the state transition probabilities. With\na maximum likelihood estimation procedure, the discovered\nstructure is then ﬁtted with the dataset sample generated to\nestimate the conditional probability distributions of the graph\nand encode it as a Bayesian Network.\nWhilst this module focuses on autonomous causal struc-\nture learning, Q-Cogni provides the ﬂexibility to receive hu-\nman inputs in the form of tabu nodes and edges, i.e. con-\nstraints in which a human expert can introduce in the causal\nstructure discovery procedure. This capability allows integra-\ntion between domain knowledge with a data-driven approach\nproviding a superior model in comparison to using either in\nisolation.\n3.2\nCausal Inference\nWe leverage upon the causal structure model discovered and\nthe Bayesian Network of the environment estimated to pro-\nvide Q-Cogni’s reinforcement learning module with causal\ninference capabilities.\nThe causal inference sub-module uses the causal DAG\nG(V, E) and receives from the Q-Cogni agent a single state\ns ∈S containing each state variable x ∈s with values sam-\npled from the environment M, the actions list A containing\neach action a ∈A and the ﬁrst-priority sub-goal o to be\nsolved. The marginals in each node v ∈V ∩s are updated\nwith the state variable values x ∀x ∈s. The procedure de-\nscribed in Algorithm 1 selects the best action a∗and calcu-\nlates the associated P(o = True|x, A) where a∗∈A. This is\nanalogous to a probabilistic reward estimation r for the given\n(s, a∗) pair.\nThis module enables the gains in learning efﬁciency by the\nagent as it shortcuts the reinforcement learning exploration\nprocedure through the structural prior knowledge of the con-\nditional probability distributions of (s, a) pairs encoded in the\nDAG. It also provides explicit interpretability capabilities to\nthe Q-Cogni reinforcement learning module by being able to\nestimate P(o = True|x, A).\n3.3\nModiﬁed Q-Learning\nThe modiﬁed Q-Learning module uses a hybrid learning pro-\ncedure that uses Q-Cogni’s causal inference module and a ϵ-\ndecay exploration strategy. In addition, one central idea in\nQ-Cogni is to use the inherent and known structure of the re-\ninforcement learning sub-goals for a given task to reduce the\nproblem dimensionality. Whilst this is not a strict limitation\nto the approach, when available a-priori it gives a signiﬁcant\nadvantage in computational efﬁciency for the learning pro-\ncedure as Q-Cogni shrinks the state −action space to the\nAlgorithm 1 Q-Cogni causal inference routine\nProcedure: INFER-MAX-PROB(G, s, A, o)\nInput: G(V ): causal structure DAG as a function of nodes\nv ∈V where V = {s, A, O} with a ﬁtted\nBayesianNetwork containing P(v| parents of v) ∀v ∈V ,\ns: a single state s ∈S containing state variables x ∈s, A:\nactions list with each action a ∈A where a values\n∈{True, False} , o: node representing goal to be solved\nwhere o ∈O\nOutput: a∗: action a ∈A where a = True ∧A \\ a = False\nfor max p, p: P(o = True|x, A) where a∗∈A\n1: Let p = 0, a = False ∀a ∈A, a∗= Ø\n2: for each v ∈V ∩s do\n3:\n* Update each node v representing a state variable\nx ∈s with its value\n4:\nv ←x\n5: end for\n6: for each a ∈A do\n7:\n* Calculate the probability of g = True for each\naction a ∈A when a = True\n8:\na ←True\n9:\na−←False ∀a−∈A \\ a\n10:\nif p < P(o = True|V ∩s, A) then\n11:\np ←P(o = True|V ∩s, A)\n12:\na∗←a\n13:\nend if\n14: end for\n15: return a∗, p\nsubset that matters for a given sub-goal. Q-Cogni’s learning\nprocedure can receive a prioritised list O of ordered reinforce-\nment learning sub-goals o and uses that information to choose\nwhen to “explore vs. infer”. If such a goal sequence is not\nknown a-priori the beneﬁts from our approach still hold, al-\nbeit a lower sample efﬁciency but still superior to a traditional\nagent that would require balancing exploration vs. exploita-\ntion.\nTo achieve that, for the prioritised sub-goal o, Q-Cogni as-\nsesses if max P(o = True|x, A) takes place when a∗∈A\nis a parent node ∈V of the sub-goal node o. In this case,\nthe agent selects a∗and directly applies into the environment\nto obtain the reward r adjusted by P(o = True|x, A) dur-\ning the value function update procedure, a step taken to avoid\nreward sparsity and improve learning performannce. The Q-\ntable stores this result, providing a more robust estimation of\nvalue without having to perform wide exploration in the en-\nvironment in contrast to unadjusted rewards. Otherwise, the\nQ-Cogni agent will perform the ϵ-decay exploration proce-\ndure. Algorithm 2 describes the modiﬁed Q-Learning routine\nin Q-Cogni.\nThis routine enables optimised learning. Policies are im-\nproved by the upfront knowledge acquired with the causal\nstructure module and the representation of the state transition\noutcomes. Unnecessary exploration of state-action pairs that\ndo not improve the probability of achieving the learning goal\nare eliminated, thus improving learning efﬁciency.\nAlgorithm 2 Q-Cogni modiﬁed Q-Learning\nProcedure: Q-COGNI-LEARN(G, M, A, O)\nInput: G(V ): causal structure DAG as a function of nodes\nv ∈V and V = {s, A, O}, M: environment containing a list\nS of states s ∈S where s = {si . . . st}, A: actions\ncontaining list of actions a ∈A, O: sequence of goal nodes\noj ∈O ∈V and j ∈[1, . . . , ngoals] in a priority order\nParameters: N: number of episodes, α: learning rate, γ:\ndiscount rate, ϵ: initial threshold for exploration, ϵmin:\nminimum ϵ, δ: decay rate for ϵ\nOutput: Q-table with Q(s, a) pairs estimating the optimal\npolicy π*\n1: Initialise Q(s, a) arbitrarily;\n2: for each episode n ∈[1, . . . , N] do\n3:\nInitialise state s = si from environment M;\n4:\nLet j = 1, a = ∅\n5:\nwhile s ̸= st do\n6:\na∗, p = INFER-MAX-PROB(G, s, A, oj)\n7:\nif a∗∈parents of oj then\n8:\na ←a∗\n9:\nj ←j + 1\n10:\nelse\n11:\nµ ←u ∼U(0, 1)\n12:\nif µ < ϵ then\n13:\na ←RANDOM(A)\n14:\nelse\n15:\na ←max Q(s, .)\n16:\nend if\n17:\nϵ ←max(ϵmin, ϵ ∗δ)\n18:\nend if\n19:\n* Apply action a in environment M with state s,\nobserve reward r and next state s′\n20:\nQ(s, a) ←\nQ(s, a) + α · (r · p + γ · maxa Q(s′, a) −Q(s, a))\n21:\ns ←s′\n22:\nend while\n23: end for\n24: return Q(s, a), π*\n4\nApproach Validation\nTo validate our approach, we start with the Vehicle Routing\nProblem (VRP). Here, we formally deﬁne the VRP problem\nand brieﬂy discuss traditional solutions on which we build\nours upon.\nVRP. In our work, inspired by the emergence of self-driving\ncars, we use a variant of the VRP, where goods need to be\npicked up from a certain location and dropped off at their\ndestination. The pick-up and drop-off must be done by the\nsame vehicle, which is why the pick-up location and drop-off\nlocation must be included in the same route [Braekers et al.,\n2016].\nThis variant is known as the VRP with Pickup and Deliv-\nery, a NP-hard problem extensively studied by the operations\nresearch community given its importance to the logistics in-\ndustry. The objective is to ﬁnd the least-cost tour, i.e. the\nshortest route, to fulﬁll the pickup and drop-off requirements\n[Ballesteros Silva and Escobar Zuluaga, 2016].\nFigure 2: Taxi-v3 environment showing a sequence of actions de-\nrived by a Q-Learning agent.\nShortest-Path Search Methods. The shortest-path search\nproblem is one of the most fundamental problems in com-\nbinatorial optimisation. As a minimum, to solve most com-\nbinatorial optimisation problems either shortest-path search\ncomputations are called as part of the solving procedure or\nconcepts from the framework are used [Gallo and Pallottino,\n1986]. Similarly, it is natural to solve the VRP with Pickup\nand Delivery with shortest-path search methods.\nDespite successful shortest-path search algorithms such as\nDjikstra’s and A*, VRP as a NP-hard problem can be very\nchallenging for these methods. Exact algorithms like Djik-\nstra’s can be computationally intractable depending on the\nscale of the problem [Drori et al., 2020]; approximate algo-\nrithms like A* provide only worst-case guarantees and are\nnot scalable [Williamson and Shmoys, 2011].\nReinforce-\nment learning is an appealing direction to such problems as\nit provides a generalisable, sample efﬁcient and heuristic-\nfree method to overcome the characteristic computational in-\ntractability of NP-hard problems.\n5\nExperimental Results\nWe start with the Taxi-v3 environment from OpenAI Gym\n[Brockman et al., 2016], a software implementation of an in-\nstance of the VRP with Pickup and Delivery. The environ-\nment was ﬁrst introduced by Dietterich [2000] to illustrate\nchallenges in hierarchical reinforcement learning.\nFigure 2 illustrates the Taxi-v3 environment with the exam-\nple of a solution using the Q-Learning algorithm. The 5×5\ngrid has four possible initial locations for the passenger and\ndestination indicated by R(ed), G(reen), Y(ellow), and B(lue).\nThe objective is to pick up the passenger at one location and\ndrop them off in another. The agent receive a reward of +20\npoints for a successful drop-off, and receives a reward of -1\nfor every movement. There are 404 reachable discrete states\nand six possible actions by the agent (move west, move east,\nmove north, move south, pickup and deliver).\nAll experiments were performed on a p4d.24xlarge GPU\nenabled AWS EC2 instance.\n5.1\nOptimal Learning\nWe trained Q-Cogni, Q-Learning, DDQN and PPO algo-\nrithms for 1,000 episodes in the Taxi-v3 environment. DDQN\nand PPO were implemented using the Rlib python package\n[Liang et al., 2018]. Hyperparameters for DDQN and PPO\nwere tuned using the BayesOptSearch module, using 100 tri-\nals over 1,000 episodes each.\nResults and Discussion.\nFigure 3 illustrates the autonomously discovered structure for\nthe Taxiv3 environment using Q-Cogni, after 500,000 sam-\nFigure 3: Discovered causal structure for the Taxiv3 environment.\nSub-goal nodes are highlighted in red.\nples collected with a random walk. We used the implemen-\ntation of the NOTEARS algorithm in the CausalNex python\npackage [Beaumont et al., 2021] to construct the causal struc-\nture model and ﬁt the conditional probability distributions\nthrough a Bayesian Network. The relationships discovered\nare quite intuitive demonstrating the high performance of the\nmethod. For example, for the node passenger in taxi to be\nTrue the nodes taxi on passenger location and pickup action\nmust be True.\nThe only domain related inputs given to the algorithm were\nconstraints such as the sub-goal node 1 pax in taxi cannot\nbe a child node of the sub-goal 2 node drop-off and location\nnodes must be a parent node. In addition, the list of ordered\nsub-goals was provided to Q-Cogni’s reinforcement learning\nmodule as [pax in taxi, drop-off].\nFigure 4 shows the results achieved over the 1,000 training\nepisodes. We observe that all methods present similar pol-\nicy performance (total reward per episode towards the end of\ntraining). However, Q-Cogni achieves superior stability and\nlearning efﬁciency in comparison to all other methods, as it is\nable to use the causal structure model and its causal inference\ncapability to accelerate the action selection process when in-\nteracting with the environment. In addition, Figure 5 demon-\nstrates the interpretability capabilities of Q-Cogni. At each\nstep, a probability of the best action to be taken is provided\nallowing for better diagnostics, tuning and most importantly\nassessment of possible biases built into autonomous agents\nsuch as Q-Cogni.\n5.2\nComparison to Shortest-Path Search Methods\nWe analyse the characteristics of our approach against\nshortest-path search methods to highlight the advantages and\ndisadvantages of Q-Cogni. We perform experiments in which\nexpand the Taxi-v3 environment into larger state sizes, repre-\nsented by a grid of n×m rows and columns. We then compare\nthe time taken to achieve an optimal tour against Djikstra’s al-\ngorithm and A∗using a Manhattan distance heuristic.\nResults and Discussion.\nWe report our comparison analysis across dimensionality,\nprior knowledge requirements, transportability between con-\nFigure 4: Total reward vs. number of episodes for comparative rein-\nforcement learning methods.\nFigure 5: Q-Cogni interpretability. Sequence of decisions in a ran-\ndomly selected episode post training.\nﬁgurations and interpretability.\nScalability. Q-Cogni excels at large networks. Fig 6 shows\nthe average time taken to identify the optimal tour for vary-\ning grid sizes representing a different scale of the Taxi-v3 en-\nvironment. We performed experiments for grid sizes from\n8×8, to 512×512 and implemented best-ﬁt curves to extrap-\nolate the increase in problem dimension.\nWe can observe that Q-Cogni takes orders of magnitude\nlonger to identify the optimal tour for a low number of nodes.\nAs the number of nodes increases Q-Cogni is much more ef-\nﬁcient. This is a product of the sample efﬁciency delivered\nwithin the Q-Cogni framework where the causal component\nenables “shortcuting” of exploration requirements, thus re-\nducing the need to proportionally increase the observations\nrequired by the agent.\nA-priori knowledge requirement. Q-Cogni require no\nprior knowledge of the map. Shortest-path methods require\nprior knowledge of the graph structure to be effectively ap-\nplied. For example, in our taxi problem, both Djikstra’s and\nA* require the map upfront.\nQ-Cogni requires the causal\nstructure encoded as a graph, but does not require the map\nitself. This is a signiﬁcant advantage to enable application in\nthe real-world as a-priori knowledge can be limited for navi-\ngation applications.\nTransferability.\nIf the conﬁguration within the map\nchanges (e.g. the initial passenger position), Q-Cogni would\nnot need to be retrained. The same agent trained in a con-\nﬁguration of a map can be deployed to another conﬁgura-\ntion seamlessly. On the other hand, if conﬁguration changes\ntake place, we would require to rerun the shortest-path search\nalgorithms. Therefore, Q-Cogni has a signiﬁcant advantage\nfor dynamic settings, a common characteristic of real-world\nFigure 6: Time taken (s) vs. number of nodes for Taxi-v3 modi-\nﬁed environment. Full lines are the experiments performed for each\nalgorithm and dashed lines the extrapolation performed. Log scale.\nFigure 7: Pickup and drop-off points of yellow cabs on New York\nCity 15th October 2022. Highlighted area is the selected for Q-\nCogni experiments. Built with kepler-gl (https://kepler.gl/).\nproblems.\nInterpretability. Shortest-path search methods are limited\nin interpretability of decisions made by the algorithm to de-\nrive the optimal tour. The causes in a particular edge is pre-\nferred over another are not explicitly described as part of their\noutput. Q-Cogni not only is able to provide a full history of\nreasons in which each decision was made but also the causes\nand associated probabilities of alternative outcomes at each\nstep. This is another signiﬁcant advantage on the applicabil-\nity of Q-Cogni for real-world problems in which there is an\ninterface between humans and the agent.\n5.3\nReal-World Application: Routing New York\nCity Taxis\nWe use the New York City Taxi & Limousine Commission\ntrip record data, which contains all taxi movements in New\nYork City from 2013 to date [Taxi and Comission, 2022], to\nvalidate the applicability of Q-Cogni in a real-world context.\nFigure 7 shows all pickup and drop-off locations of yellow\ncabs on the 15th of October 2022. We see the highest density\nof taxi trips being in Manhattan, represented on the left hand\nside of Figure 7. However, we choose the neighborhoods be-\ntween Long Island City and Astoria, represented in the high-\nlighted area of Figure 7 as they have a more challenging street\nconﬁguration than Manhattan.\nWe used the OSMNX library [Boeing, 2017] to convert the\nstreet map into a graph representation where intersections are\nnodes and edges are streets. We created a custom OpenAI\n[Brockman et al., 2016] gym environment to enable ﬁtting\nof the Bayesian Network and training of Q-Cogni. The re-\nFigure 8: Graph representation of the Astoria region in New York\nCity used to train Q-Cogni.\nsulting graph is shown in Figure 8 containing 666 nodes and\n1712 edges resulting in a state-action space of size 443,556,\na problem 103 larger than our Taxi-v3 environment.\nWe use the causal structure derived in Figure 3 and per-\nform a random walk with 1,000,000 steps in the custom built\nenvironment to ﬁt a Bayesian Network to the causal model. It\nis important to appreciate here the transferability and domain\nagnostic characteristic of our approach, where we leverage on\nthe structure previously discovered for the same task but in a\ncomplete different map.\nWe train Q-Cogni once for 100,000 episodes and evalu-\nate the trained agent against several trips contained in the\noriginal dataset without retraining each time the trip conﬁg-\nuration changes in the real-world dataset. This is a signiﬁ-\ncant beneﬁt of the approach when comparing to shortest-path\nsearch methods. We also compare Q-Cogni results against\nQ-Learning to observe the effects of the causal model against\npolicy quality and compare against Dijkstra’s algorithm to ob-\nserve the effects of policy efﬁciency.\nFirst, Figure 9 shows the optimal routes generated by Q-\nLearning and Q-Cogni after 100,000 training episodes for a\nselected route. We can see that Q-Cogni signiﬁcantly im-\nproves the policy generation reaching a near-optimal shortest-\npath result post training whereas Q-Learning fails to detect\nthe right pickup point and performs multiple loops to get to\nthe destination. Across the 615 trips evaluated, Q-Learning\nwas able to generate only 12% of routes in which had the\nsame travel distance as Q-Cogni generated routes, with the\nremainder being longer. These results show the beneﬁts of\nthe causal module of Q-Cogni towards optimal learning.\nIn addition, Figure 10 shows a sample comparison of opti-\nmal routes generated with Dijkstra’s algorithm and Q-Cogni.\nWe observe on the left picture that Q-Cogni generates a more\nefﬁcient route (in red) than Dijkstra’s (in blue) measured as\nthe total distance travelled. On the right picture Q-Cogni gen-\nerates a signiﬁcantly different route which is slightly worse,\nalbeit close in terms of distance. Overall, across the 615 trips\nevaluated we report 28% where Q-Cogni generated a shorter\nroute than Dijkstra’s, 57% were the same and 15% worse.\nThese results show the applicability of Q-Cogni for a real-\nworld case with demonstrated (i) transferability - the causal\nnetwork obtained in the Taxi-v3 environment can be used for\nthe same task; (ii) no prior knowledge required - Q-Cogni\ndoes not need to have access to the global map; and (iii) ex-\nFigure 9: Optimal routes generated by Q-Learning (left) and Q-\nCogni (right) after 100,000 episodes. Origin and destination high-\nlighted with the white and green circles respectively.\nFigure 10: Q-Cogni (red) generated routes vs. Dijkstra’s (blue) for\n2 random trips evaluated on the 15th October 2022. Origin and des-\ntination highlighted with the white and green circles respectively\nplainability and expandability - where the Bayesian Network\ncan be expanded to incorporate other causal relations such as\ntrafﬁc and weather. The application of Q-Cogni in this real-\nworld dataset demonstrate a promising framework to bring\ntogether causal inference and reinforcement learning to solve\nrelevant and challenging problems.\n6\nConclusion\nWe have presented Q-Cogni, a novel causal reinforcement\nlearning framework that redesigns Q-Learning with an au-\ntonomous causal structure discovery method and causal in-\nference as a hybrid model-based and model-free approach.\nWe have implemented a framework that leverages upon a\ndata-driven causal structure model discovered autonomously\n(but ﬂexible enough to accommodate domain knowledge\nbased inputs) and redesigned the Q-Learning algorithm to\napply causal inference during the learning process in a re-\ninforcement learning setting.\nOur approach exploits the causal structural knowledge con-\ntained in a reinforcement learning environment to shortcut\nexploration requirements of a state space by the agent to de-\nrive a more robust policy with less training requirements as\nit increases the sample efﬁciency of the learning process.\nTogether, these techniques are shown to achieve a superior\npolicy, substantially improve learning efﬁciency, provide su-\nperior interpretability, efﬁciently scale with higher problem\ndimensions and more generalisable to varying problem con-\nﬁgurations. While these beneﬁts have been illustrated in the\ncontext of one speciﬁc application – the VRP problem in the\nnavigation domain – it can be applied to any reinforcement\nlearning problem that contains an environment with a implicit\nrepresentation of the causal relationships between state vari-\nables and some level of prior knowledge of the environment\ndynamics.\nWe believe that the integration of causality and reinforce-\nment learning will continue to be an attractive area towards\nhuman level intelligence for autonomous learning agents.\nOne promising avenue of research is to broaden the integrated\napproach to continuous state-action spaces such as control en-\nvironments, a current focus of our research.\nReferences\n[Amirinezhad et al., 2022] Amir\nAmirinezhad,\nSaber\nSalehkaleybar, and Matin Hashemi.\nActive learning\nof causal structures with deep reinforcement learning.\nNeural Networks, 2022.\n[Arulkumaran et al., 2017] Kai Arulkumaran, Marc Peter\nDeisenroth, Miles Brundage, and Anil Anthony Bharath.\nA brief survey of deep reinforcement learning.\narXiv\npreprint arXiv:1708.05866, 2017.\n[Ballesteros Silva and Escobar Zuluaga, 2016] Pedro Pablo\nBallesteros Silva and Antonio Escobar Zuluaga. Review\nof state of the art vehicle routing problem with pickup and\ndelivery (vrppd). Ingenier´ıa y Desarrollo, 34(2):463–482,\n2016.\n[Bannon et al., 2020] James Bannon, Brad Windsor, Wenbo\nSong, and Tao Li.\nCausality and batch reinforcement\nlearning: Complementary approaches to planning in un-\nknown domains. arXiv preprint arXiv:2006.02579, 2020.\n[Beaumont et al., 2021] Paul Beaumont, Ben Horsburgh,\nPhilip Pilgerstorfer, Angel Droth, Richard Oentaryo,\nSteven Ler, Hiep Nguyen, Gabriel Azevedo Ferreira, Zain\nPatel, and Wesley Leong. CausalNex, 10 2021.\n[Boeing, 2017] Geoff Boeing. Osmnx: New methods for ac-\nquiring, constructing, analyzing, and visualizing complex\nstreet networks. Computers, Environment and Urban Sys-\ntems, 65:126–139, 2017.\n[Braekers et al., 2016] Kris Braekers, Katrien Ramaekers,\nand Inneke Van Nieuwenhuyse. The vehicle routing prob-\nlem: State of the art classiﬁcation and review. Computers\n& Industrial Engineering, 99:300–313, 2016.\n[Brockman et al., 2016] Greg Brockman,\nVicki Cheung,\nLudwig Pettersson, Jonas Schneider, John Schulman, Jie\nTang, and Wojciech Zaremba. Openai gym. arXiv preprint\narXiv:1606.01540, 2016.\n[Buesing et al., 2018] Lars Buesing, Theophane Weber, Yori\nZwols, Sebastien Racaniere, Arthur Guez, Jean-Baptiste\nLespiau, and Nicolas Heess.\nWoulda, coulda, shoulda:\nCounterfactually-guided policy search.\narXiv preprint\narXiv:1811.06272, 2018.\n[C¸ alıs¸ır and Pehlivano˘glu, 2019] Sinan\nC¸ alıs¸ır\nand\nMeltem Kurt Pehlivano˘glu.\nModel-free reinforce-\nment learning algorithms: A survey. In 2019 27th Signal\nProcessing and Communications Applications Conference\n(SIU), pages 1–4. IEEE, 2019.\n[Chen and Liu, 2018] Zhiyuan Chen and Bing Liu. Lifelong\nmachine learning. Synthesis Lectures on Artiﬁcial Intelli-\ngence and Machine Learning, 12(3):1–207, 2018.\n[Darwiche, 2018] Adnan Darwiche.\nHuman-level intelli-\ngence or animal-like abilities?\nCommunications of the\nACM, 61(10):56–67, 2018.\n[Dasgupta et al., 2019] Ishita Dasgupta, Jane Wang, Silvia\nChiappa, Jovana Mitrovic, Pedro Ortega, David Raposo,\nEdward Hughes, Peter Battaglia, Matthew Botvinick,\nand Zeb Kurth-Nelson.\nCausal reasoning from meta-\nreinforcement learning. arXiv preprint arXiv:1901.08162,\n2019.\n[Dietterich, 2000] Thomas G Dietterich.\nHierarchical re-\ninforcement learning with the maxq value function de-\ncomposition.\nJournal of artiﬁcial intelligence research,\n13:227–303, 2000.\n[Dijkstra, 1959] Edsger W Dijkstra.\nA note on two prob-\nlems in connexion with graphs. Numerische mathematik,\n1(1):269–271, 1959.\n[Drori et al., 2020] Iddo Drori, Anant Kharkar, William R\nSickinger, Brandon Kates, Qiang Ma, Suwen Ge, Eden\nDolev,\nBrenda Dietrich,\nDavid P Williamson,\nand\nMadeleine Udell.\nLearning to solve combinatorial op-\ntimization problems on real-world graphs in linear time.\nIn 2020 19th IEEE International Conference on Machine\nLearning and Applications (ICMLA), pages 19–24. IEEE,\n2020.\n[Gallo and Pallottino, 1986] Giorgio Gallo and Stefano Pal-\nlottino. Shortest path methods: A unifying approach. In\nNetﬂow at Pisa, pages 38–64. Springer, 1986.\n[Gasse et al., 2021] Maxime Gasse, Damien Grasset, Guil-\nlaume Gaudron, and Pierre-Yves Oudeyer. Causal rein-\nforcement learning using observational and interventional\ndata. arXiv preprint arXiv:2106.14421, 2021.\n[Gershman, 2017] Samuel Gershman. Reinforcement learn-\ning and causal models. The Oxford handbook of causal\nreasoning, page 295, 2017.\n[Hart et al., 1968] Peter Hart, Nils Nilsson, and Bertram\nRaphael. A formal basis for the heuristic determination\nof minimum cost paths. IEEE Transactions on Systems\nScience and Cybernetics, 4(2):100–107, 1968.\n[Huang et al., 2020] Xiaoshui Huang, Fujin Zhu, Lois Hol-\nloway, and Ali Haidar. Causal discovery from incomplete\ndata using an encoder and reinforcement learning. arXiv\npreprint arXiv:2006.05554, 2020.\n[John, 2020] St John.\nCausal reinforcement learning: A\nprimer, Dec 2020.\n[Kaelbling et al., 1996] Leslie Pack Kaelbling, Michael L\nLittman, and Andrew W Moore.\nReinforcement learn-\ning: A survey. Journal of artiﬁcial intelligence research,\n4:237–285, 1996.\n[Kansky et al., 2017] Ken Kansky, Tom Silver, David A\nM´ely,\nMohamed\nEldawy,\nMiguel\nL´azaro-Gredilla,\nXinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott\nPhoenix, and Dileep George.\nSchema networks: Zero-\nshot transfer with a generative causal model of intuitive\nphysics. In International conference on machine learning,\npages 1809–1818. PMLR, 2017.\n[Kuang et al., 2020] Kun Kuang, Lian Li, Zhi Geng, Lei Xu,\nKun Zhang, Beishui Liao, Huaxin Huang, Peng Ding,\nWang Miao, and Zhichao Jiang. Causal inference. En-\ngineering, 6(3):253–263, 2020.\n[Liang et al., 2018] Eric Liang, Richard Liaw, Robert Nishi-\nhara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph\nGonzalez, Michael Jordan, and Ion Stoica.\nRllib: Ab-\nstractions for distributed reinforcement learning. In Inter-\nnational Conference on Machine Learning, pages 3053–\n3062. PMLR, 2018.\n[Lu et al., 2020] Chaochao Lu, Biwei Huang, Ke Wang,\nJos´e Miguel Hern´andez-Lobato, Kun Zhang, and Bernhard\nSch¨olkopf.\nSample-efﬁcient reinforcement learning via\ncounterfactual-based data augmentation.\narXiv preprint\narXiv:2012.09092, 2020.\n[Lu et al., 2022] Yangyi Lu, Amirhossein Meisami, and Am-\nbuj Tewari.\nEfﬁcient reinforcement learning with prior\ncausal knowledge. In Conference on Causal Learning and\nReasoning, pages 526–541. PMLR, 2022.\n[Madumal et al., 2020] Prashan Madumal, Tim Miller, Liz\nSonenberg, and Frank Vetere. Explainable reinforcement\nlearning through a causal lens. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 34, pages\n2493–2500, 2020.\n[Marcus, 2018] Gary Marcus. Deep learning: A critical ap-\npraisal. arXiv preprint arXiv:1801.00631, 2018.\n[Pearl, 2010] Judea Pearl. Causal inference. Causality: ob-\njectives and assessment, pages 39–58, 2010.\n[Polydoros and Nalpantidis, 2017] Athanasios S Polydoros\nand Lazaros Nalpantidis.\nSurvey of model-based rein-\nforcement learning: Applications on robotics. Journal of\nIntelligent & Robotic Systems, 86(2):153–173, 2017.\n[Prosperi et al., 2020] Mattia Prosperi, Yi Guo, Matt Sper-\nrin, James S Koopman, Jae S Min, Xing He, Shannan\nRich, Mo Wang, Iain E Buchan, and Jiang Bian. Causal in-\nference and counterfactual prediction in machine learning\nfor actionable healthcare.\nNature Machine Intelligence,\n2(7):369–375, 2020.\n[Ribeiro et al., 2016] Marco Tulio Ribeiro, Sameer Singh,\nand Carlos Guestrin. ” why should i trust you?” explain-\ning the predictions of any classiﬁer. In Proceedings of the\n22nd ACM SIGKDD international conference on knowl-\nedge discovery and data mining, pages 1135–1144, 2016.\n[Sauter et al., 2022] Andreas Sauter, Erman Acar, and Vin-\ncent Franc¸ois-Lavet.\nA meta-reinforcement learn-\ning algorithm for causal discovery.\narXiv preprint\narXiv:2207.08457, 2022.\n[Schulman et al., 2017] John Schulman, Filip Wolski, Pra-\nfulla Dhariwal, Alec Radford, and Oleg Klimov.\nProx-\nimal policy optimization algorithms.\narXiv preprint\narXiv:1707.06347, 2017.\n[Seitzer et al., 2021] Maximilian\nSeitzer,\nBernhard\nSch¨olkopf, and Georg Martius.\nCausal inﬂuence de-\ntection for improving efﬁciency in reinforcement learning.\nIn M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang,\nand J. Wortman Vaughan, editors, Advances in Neural\nInformation Processing Systems,\nvolume 34,\npages\n22905–22918. Curran Associates, Inc., 2021.\n[Taxi and Comission, 2022] New York City Taxi and Limou-\nsine Comission. NYC TLC, 11 2022.\n[Toth and Vigo, 2002] Paolo Toth and Daniele Vigo.\nAn\noverview of vehicle routing problems. The vehicle rout-\ning problem, pages 1–26, 2002.\n[Van Hasselt et al., 2016] Hado Van Hasselt, Arthur Guez,\nand David Silver. Deep reinforcement learning with dou-\nble q-learning. In Proceedings of the AAAI conference on\nartiﬁcial intelligence, volume 30, 2016.\n[Wang et al., 2021] Xiaoqiang Wang, Yali Du, Shengyu\nZhu, Liangjun Ke, Zhitang Chen, Jianye Hao, and Jun\nWang.\nOrdering-based causal discovery with reinforce-\nment learning. arXiv preprint arXiv:2105.06631, 2021.\n[Williamson and Shmoys, 2011] David P Williamson and\nDavid B Shmoys.\nThe design of approximation algo-\nrithms. Cambridge university press, 2011.\n[Yang et al., 2022] Chao-Han Huck Yang, I-Te Danny Hung,\nYi Ouyang, and Pin-Yu Chen.\nTraining a resilient q-\nnetwork against observational interference. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence, vol-\nume 36, pages 8814–8822, 2022.\n[Zheng et al., 2018] Xun Zheng, Bryon Aragam, Pradeep\nRavikumar, and Eric P. Xing. Dags with no tears: continu-\nous optimization for structure learning. In Proceedings of\nthe 32nd International Conference on Neural Information\nProcessing Systems (NIPS’18), pages 9492–9503, 2018.\n[Zhu et al., 2019] Shengyu Zhu, Ignavier Ng, and Zhitang\nChen.\nCausal discovery with reinforcement learning.\narXiv preprint arXiv:1906.04477, 2019.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "I.2.8; I.2.9; I.2.1"
  ],
  "published": "2023-02-26",
  "updated": "2023-02-26"
}