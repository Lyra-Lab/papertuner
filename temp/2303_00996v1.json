{
  "id": "http://arxiv.org/abs/2303.00996v1",
  "title": "Unsupervised Meta-Learning via Few-shot Pseudo-supervised Contrastive Learning",
  "authors": [
    "Huiwon Jang",
    "Hankook Lee",
    "Jinwoo Shin"
  ],
  "abstract": "Unsupervised meta-learning aims to learn generalizable knowledge across a\ndistribution of tasks constructed from unlabeled data. Here, the main challenge\nis how to construct diverse tasks for meta-learning without label information;\nrecent works have proposed to create, e.g., pseudo-labeling via pretrained\nrepresentations or creating synthetic samples via generative models. However,\nsuch a task construction strategy is fundamentally limited due to heavy\nreliance on the immutable pseudo-labels during meta-learning and the quality of\nthe representations or the generated samples. To overcome the limitations, we\npropose a simple yet effective unsupervised meta-learning framework, coined\nPseudo-supervised Contrast (PsCo), for few-shot classification. We are inspired\nby the recent self-supervised learning literature; PsCo utilizes a momentum\nnetwork and a queue of previous batches to improve pseudo-labeling and\nconstruct diverse tasks in a progressive manner. Our extensive experiments\ndemonstrate that PsCo outperforms existing unsupervised meta-learning methods\nunder various in-domain and cross-domain few-shot classification benchmarks. We\nalso validate that PsCo is easily scalable to a large-scale benchmark, while\nrecent prior-art meta-schemes are not.",
  "text": "Published as a conference paper at ICLR 2023\nUNSUPERVISED META-LEARNING VIA FEW-SHOT\nPSEUDO-SUPERVISED CONTRASTIVE LEARNING\nHuiwon JangA∗Hankook LeeB∗† Jinwoo ShinA\nAKorea Advanced Institute of Science and Technology (KAIST)\nBLG AI Research\n{huiwoen0516, jinwoos}@kaist.ac.kr\nhankook.lee@lgresearch.ai\nABSTRACT\nUnsupervised meta-learning aims to learn generalizable knowledge across a dis-\ntribution of tasks constructed from unlabeled data. Here, the main challenge is\nhow to construct diverse tasks for meta-learning without label information; recent\nworks have proposed to create, e.g., pseudo-labeling via pretrained representations\nor creating synthetic samples via generative models. However, such a task con-\nstruction strategy is fundamentally limited due to heavy reliance on the immutable\npseudo-labels during meta-learning and the quality of the representations or the\ngenerated samples. To overcome the limitations, we propose a simple yet effec-\ntive unsupervised meta-learning framework, coined Pseudo-supervised Contrast\n(PsCo), for few-shot classiﬁcation. We are inspired by the recent self-supervised\nlearning literature; PsCo utilizes a momentum network and a queue of previous\nbatches to improve pseudo-labeling and construct diverse tasks in a progressive\nmanner. Our extensive experiments demonstrate that PsCo outperforms existing\nunsupervised meta-learning methods under various in-domain and cross-domain\nfew-shot classiﬁcation benchmarks. We also validate that PsCo is easily scalable\nto a large-scale benchmark, while recent prior-art meta-schemes are not.\n1\nINTRODUCTION\nLearning to learn (Thrun & Pratt, 1998), also known as meta-learning, aims to learn general knowl-\nedge about how to solve unseen, yet relevant tasks from prior experiences solving diverse tasks. In\nrecent years, the concept of meta-learning has found various applications, e.g., few-shot classiﬁca-\ntion (Snell et al., 2017; Finn et al., 2017), reinforcement learning (Duan et al., 2017; Houthooft et al.,\n2018; Alet et al., 2020), hyperparameter optimization (Franceschi et al., 2018), and so on. Among\nthem, few-shot classiﬁcation is arguably the most popular one, whose goal is to learn some knowl-\nedge to classify test samples of unseen classes during (meta-)training with few labeled samples. The\ncommon approach is to construct a distribution of few-shot classiﬁcation (i.e., N-way K-shot) tasks\nand optimize a model to generalize across tasks (sampled from the distribution) so that it can rapidly\nadapt to new tasks. This approach has shown remarkable performance in various few-shot classi-\nﬁcation tasks but suffers from limited scalability as the task construction phase typically requires a\nlarge number of human-annotated labels.\nTo mitigate the issue, there have been several recent attempts to apply meta-learning to unlabeled\ndata, i.e., unsupervised meta-learning (UML) (Hsu et al., 2019; Khodadadeh et al., 2019; 2021; Lee\net al., 2021; Kong et al., 2021). To perform meta-learning without labels, the authors have sug-\ngested various ways to construct synthetic tasks. For example, pioneering works (Hsu et al., 2019;\nKhodadadeh et al., 2019) assigned pseudo-labels via data augmentations or clustering based on pre-\ntrained representations. In contrast, recent approaches (Khodadadeh et al., 2021; Lee et al., 2021;\nKong et al., 2021) utilized generative models to generate synthetic (in-class) samples or learn un-\nknown labels via categorical latent variables. They have achieved moderate performance in few-shot\nlearning benchmarks, but are fundamentally limited as: (a) the pseudo-labeling strategies are ﬁxed\nduring meta-learning and impossible to correct mislabeled samples; (b) the generative approaches\nheavily rely on the quality of generated samples and are cumbersome to scale into large-scale setups.\n∗Equal contributions\n†Work done at KAIST\n1\narXiv:2303.00996v1  [cs.LG]  2 Mar 2023\nPublished as a conference paper at ICLR 2023\nBackbone \nProjector \nPredictor \nEMA \nStop-grad \nPsCo \nN-way K-shot\nSupport Samples \nQueue \nMatching \nFigure 1: An overview of the proposed Pseudo-supervised Contrast (PsCo). PsCo constructs an N-\nway K-shot few-shot classiﬁcation task using the current mini-batch {xi} and the queue of previous\nmini-batches; and then, it learns the task via contrastive learning. Here, A is a label assignment ma-\ntrix found by the Sinkhorn-Knopp algorithm (Cuturi, 2013), A is a pre-deﬁned augmentation distri-\nbution, f is a backbone feature extractor, g and h are projection and prediction MLPs, respectively,\nand φ is an exponential moving average (EMA) of the model parameter θ.\nTo overcome the limitations of the existing UML approaches, in this paper, we ask whether one\ncan (a) progressively improve a pseudo-labeling strategy during meta-learning, and (b) construct\nmore diverse tasks without generative models. We draw inspiration from recent advances in self-\nsupervised learning literature (He et al., 2020; Khosla et al., 2020), which has shown remarkable\nsuccess in representation learning without labeled data. In particular, we utilize (a) a momentum\nnetwork to improve pseudo-labeling progressively via temporal ensemble; and (b) a momentum\nqueue to construct diverse tasks using previous mini-batches in an online manner.\nFormally, we propose Pseudo-supervised Contrast (PsCo), a novel and effective unsupervised meta-\nlearning framework, for few-shot classiﬁcation. Our key idea is to construct few-shot classiﬁcation\ntasks using the current and previous mini-batches based on the momentum network and the mo-\nmentum queue. Speciﬁcally, given a random mini-batch of N unlabeled samples, we treat them\nas N queries (i.e., test samples) of different N labels, and then select K shots (i.e., training sam-\nples) for each label from the queue of previous mini-batches based on representations extracted by\nthe momentum network. To further improve the selection procedure, we utilize top-K sampling\nafter applying a matching algorithm, Sinkhorn-Knopp (Cuturi, 2013). Finally, we optimize our\nmodel via supervised contrastive learning (Khosla et al., 2020) for solving the N-way K-shot task.\nRemark that our few-shot task construction relies on not only the current mini-batch but also the\nmomentum network and the queue of previous mini-batches. Therefore, our task construction (i.e.,\npseudo-labeling) strategy (a) is progressively improved during meta-learning with the momentum\nnetwork, and (b) constructs diverse tasks since the shots can be selected from the entire dataset. Our\nframework is illustrated in Figure 1.\nThroughout extensive experiments, we demonstrate the effectiveness of the proposed framework,\nPsCo, under various few-shot classiﬁcation benchmarks. First, PsCo achieves state-of-the-art per-\nformance under both Omniglot (Lake et al., 2011) and miniImageNet (Ravi & Larochelle, 2017)\nfew-shot benchmarks; its performance is even competitive with supervised meta-learning methods.\nNext, PsCo also shows superiority under cross-domain few-shot learning scenarios. Finally, we\ndemonstrate that PsCo is scalable to a large-scale benchmark, ImageNet (Deng et al., 2009).\nWe summarize our contributions as follows:\n• We propose PsCo, an effective unsupervised meta-learning (UML) framework for few-shot clas-\nsiﬁcation, which constructs diverse few-shot pseudo-tasks without labels utilizing the momen-\ntum network and the queue of previous batches in a progressive manner.\n• We achieve state-of-the-art performance on few-shot classiﬁcation benchmarks, Omniglot (Lake\net al., 2011) and miniImageNet (Ravi & Larochelle, 2017). For example, PsCo outperforms the\nprior art of UML, Meta-SVEBM (Kong et al., 2021), by 5% accuracy gain (58.03→63.26), for\n5-way 5-shot tasks of miniImageNet (see Table 1).\n2\nPublished as a conference paper at ICLR 2023\n• We show that PsCo achieves comparable performance with supervised meta-learning methods\nin various few-shot classiﬁcation benchmarks. For example, PsCo achieves 44.01% accuracy for\n5-way 5-shot tasks of an unseen domain, Cars (Krause et al., 2013), while supervised MAML\n(Finn et al., 2017) does 41.17% (see Table 2).\n• We validate PsCo is also applicable to a large-scale dataset: e.g., we improve PsCo by 5.78%\naccuracy gain (47.67→53.45) for 5-way 5-shot tasks of Cars using large-scale unlabeled data,\nImageNet (Deng et al., 2009) (see Table 3).\n2\nPRELIMINARIES\n2.1\nPROBLEM STATEMENT: UNSUPERVISED FEW-SHOT LEARNING\nThe problem of interest in this paper is unsupervised few-shot learning, one of the popular unsu-\npervised meta-learning applications. This aims to learn generalizable knowledge without human\nannotations for quickly adapting to unseen but relevant few-shot tasks. Following the meta-learning\nliterature, we refer to the learning phase as meta-training and the adaptation phase as meta-test.\nFormally, we are only able to utilize an unlabeled dataset Dmeta train := {xi} during meta-training\nour model. At the meta-test phase, we transfer the model to new few-shot tasks {Ti} ∼Dmeta test\nwhere each task Ti aims to classify query samples {xq} among N labels using support (i.e., training)\nsamples S = {(xs, ys)}NK\ns=1. We here assume the task Ti consists of K support samples for each\nlabel y ∈{1, . . . , N}, which is referred to as N-way K-shot classiﬁcation. Note that Dmeta train\nand Dmeta test can come from the same domain (i.e., the standard in-domain setting) or different\ndomains (i.e., cross-domain) as suggested by Chen et al. (2019).\n2.2\nCONTRASTIVE LEARNING\nContrastive learning (Oord et al., 2018; Chen et al., 2020a; He et al., 2020; Khosla et al., 2020) aims\nto learn meaningful representations by maximizing the similarity between similar (i.e., positive)\nsamples, and minimizing the similarity between dissimilar (i.e., negative) samples on the repre-\nsentation space. We ﬁrst describe a general form of contrastive learning objectives based on the\ntemperature-normalized cross entropy (Chen et al., 2020a; He et al., 2020) and its variant for multi-\nple positives (Khosla et al., 2020) as follows:\nLContrast({qi}N\ni=1, {kj}M\nj=1, A; τ) := −1\nN\nN\nX\ni=1\n1\nP\nj Ai,j\nM\nX\nj=1\nAi,j log\nexp(q⊤\ni kj/τ)\nPM\nk=1 exp(q⊤\ni kk/τ)\n,\n(1)\nwhere {qi} and {kj} are ℓ2-normalized query and key representations, respectively, A ∈{0, 1}NM\nrepresents whether qi and kj are positive (Ai,j = 1) or negative (Ai,j = 0), and τ is a hyperparam-\neter for temperature scaling.\nBased on the recent observations in the self-supervised learning literature, we also describe a general\nscheme to construct the query and key representations using data augmentations and a momentum\nnetwork. Formally, given a random mini-batch {xi}, the representations can be obtained as follows:\nqi = Normalize(hθ ◦gθ ◦fθ(ti,1(xi))),\nki = Normalize(gφ ◦fφ(ti,2(xi))),\n(2)\nwhere Normalize(·) is ℓ2 normalization, ti,1 ∼A1 and ti,2 ∼A2 are random data augmentations,\nf is a backbone feature extractor like ResNet (He et al., 2016), g and h are projection and predic-\ntion MLPs,1 respectively, and φ is an exponential moving average (i.e., momentum) of the model\nparameter θ.2 Since a large number of negative samples plays a crucial role in contrastive learning,\none can re-use the key representations of previous mini-batches by maintaining a queue (He et al.,\n2020).\nNote that the above forms (1) and (2) can be formulated as various contrastive learning frameworks.\nFor example, SimCLR (Chen et al., 2020a) is a special case of no momentum φ and no predictor h.\nIn addition, self-supervised contrastive learning methods (Chen et al., 2020a; He et al., 2020) often\nassume that ki is only the positive key of qi, i.e., Ai,j = 1 if and only if i = j, while supervised\ncontrastive learning (Khosla et al., 2020) directly uses labels for A.\n1The prediction MLPs have been utilized in the recent SSL literature (Grill et al., 2020; Chen et al., 2021).\n2φ is updated by φ ←mφ + (1 −m)θ for each training iteration where m is a momentum hyperparameter.\n3\nPublished as a conference paper at ICLR 2023\nAlgorithm 1 Pseudo-supervised Contrast (PsCo): PyTorch-like Pseudocode\n# f, g, h: backbone, projector, and predictor\n# {f,g}_ema: momentum backbone, and projector\n# queue: momentum queue (Mxd)\n# mm: matrix multiplication, mul: element-wise multiplication\ndef PsCo(x):\n# x: a mini-batch of N samples\nx1, x2 = aug1(x), aug2(x)\n# two augmented views of x\nq = h(g(f(x1)))\n# (Nxd) N query representations\nz = g_ema(f_ema(x2))\n# (Nxd) N query momentum representations\nsim = mm(z, queue.T)\n# (NxM) similarity matrix\nA_tilde = sinkhorn(sim)\n# (NxM) soft pseudo-label assignment matrix\ns, A = select_topK(queue, A_tilde) # (NKxd) s: support momentum representations\n# (NxNK) A: pseudo-label assignment matrix\nlogits = mm(q, s.T) / temperature\nloss = logits.logsumexp(dim=1) - mul(logits, A).sum(dim=1) / K\nreturn loss.mean()\n3\nMETHOD: PSEUDO-SUPERVISED CONTRASTIVE META-LEARNING\nIn this section, we introduce Pseudo-supervised Contrast (PsCo), a novel and effective framework\nfor unsupervised few-shot learning. Our key idea is to construct few-shot classiﬁcation pseudo-\ntasks using the current and previous mini-batches with the momentum network and the momentum\nqueue. We then employ supervised contrastive learning (Khosla et al., 2020) for learning the pseudo-\ntasks. The detailed implementations of our task construction, meta-training objective, and meta-test\nscheme for unsupervised few-shot learning are described in Section 3.1, 3.2, and 3.3, respectively.\nOur framework is illustrated in Figure 1 and its pseudo-code is provided in Algorithm 1. Note that\nwe use the same notations described in Section 2 for consistency.\n3.1\nONLINE PSEUDO-TASK CONSTRUCTION\nWe here describe how to construct a few-shot pseudo-task using unlabeled data Dmeta train = {xi}.\nTo this end, we maintain a queue of previous mini-batches. Then, we treat the previous and current\nmini-batch samples as training (i.e., shots) and test (i.e., queries) samples for our few-shot pseudo-\ntask. Formally, let B := {xi}N\ni=1 be the current mini-batch randomly sampled from Dmeta train, and\nQ := {˜xj}M\nj=1 be the queue of previous mini-batch samples. Now, we treat B = {xi}N\ni=1 as queries\nof N different pseudo-labels and ﬁnd K (appropriate) shots for each pseudo-label from the queue\nQ. Remark that this approach to utilize the previous mini-batches encourages us to construct more\ndiverse tasks.\nTo ﬁnd the shots efﬁciently, we utilize the momentum network and the momentum queue described\nin Section 2.2. For the current mini-batch samples, we compute the momentum query represen-\ntations with data augmentations ti,2 ∼A2, i.e., zi := Normalize(gφ ◦fφ(ti,2(xi))). Following\nHe et al. (2020), we store only the momentum representations of the previous mini-batch samples\ninstead of raw data in the queue Qz, i.e., Qz := {˜zj}M\nj=1. Remark that the use of the momentum\nnetwork is not only for efﬁciency but also for improving our task construction strategy because the\nmomentum network is consistent and progressively improved during training. Following He et al.\n(2020), we randomly initialize the queue Qz at the beginning of training.\nNow, the remaining question is as follows: How to ﬁnd K appropriate shots from the queue Q for\neach pseudo-label using the momentum representations? Before introducing our algorithm, we ﬁrst\ndiscuss two requirements for constructing semantically meaningful few-shot tasks: (i) shots and\nqueries of the same label should be semantically similar, and (ii) all shots should be different. Based\non these requirements, we formulate our assignment problem as follows:\nmax\n˜A∈{0,1}N×M\nN\nX\ni=1\nM\nX\nj=1\n˜Aij · z⊤\ni ˜zj\nsuch that\nX\nj\n˜Aij = K,\nX\ni\n˜Aij ≤1.\n(3)\nObtaining the exact optimal solution to the above assignment problem for each training iteration\nmight be too expensive for our purpose (Ramshaw & Tarjan, 2012). Instead, we use an approximate\nalgorithm: we ﬁrst apply a fast version (Cuturi, 2013) of the Sinkhorn-Knopp algorithm to solve the\n4\nPublished as a conference paper at ICLR 2023\nfollowing problem:\nmax\n˜A∈[0,1]N×M\nN\nX\ni=1\nM\nX\nj=1\n˜Aij · z⊤\ni ˜zj + ϵH( ˜A)\nsuch that\nX\nj\n˜Aij = 1/N,\nX\ni\n˜Aij = 1/M,\n(4)\nwhich is an entropy-regularized optimal transport problem (Cuturi, 2013). Its optimal solution ˜A∗\ncan be obtained efﬁciently and can be considered as a soft assignment matrix between the current\nmini-batch {zi}N\ni=1 and the queue Qz = {˜zj}M\nj=1. Hence, we select top-K elements for each row\nof the assignment matrix ˜A∗and ﬁnally construct an N-way K-shot pseudo-task consisting of (a)\nquery samples B = {xi}N\ni=1, (b) the support representations Sz := {˜zs}NK\ns=1, and (c) the pseudo-\nlabel assignment matrix A ∈{0, 1}N×NK. Note that Figure 1 shows an example of a 5-way 2-shot\ntask. We empirically observe that our task construction strategy satisﬁes the above requirements (i)\nand (ii) (see Section 4.3).\n3.2\nMETA-TRAINING: SUPERVISED CONTRASTIVE LEARNING WITH PSEUDO TASKS\nWe now describe our meta-learning objective LPsCo for learning our few-shot pseudo-tasks. We here\nuse our model θ to obtain query representations: qi := Normalize(hθ ◦gθ ◦fθ(ti,1(xi))) where\nti,1 ∼A1 is a random data augmentation for each i. Then, our objective LPsCo is deﬁned as follows:\nLPsCo := LContrast({qi}N\ni=1, Sz, A; τPsCo),\n(5)\nwhere Sz := {˜zs}NK\ns=1 is the support representations and A ∈{0, 1}N×NK is the pseudo-label\nassignment matrix, which are constructed by our task construction strategy described in Section 3.1.\nSince our framework PsCo uses the same architectural components as a self-supervised learning\nframework, MoCo (He et al., 2020), the MoCo objective LMoCo can be incorporated into our PsCo\nwithout additional computation costs. Note that the MoCo objective can be written as follows:\nLMoCo := LContrast({qi}N\ni=1, {zi}N\ni=1 ∪Qz, AMoCo; τMoCo),\n(6)\nwhere (AMoCo)i,j = 1 if and only if i = j, and zi := Normalize(gφ ◦fφ(ti,2(xi))) as described in\nSection 3.1. We optimize our model θ via all the objectives, i.e., Ltotal := LPsCo + LMoCo. Remark\nagain that φ is updated by exponential moving average (EMA), i.e., φ ←mφ + (1 −m)θ.\nWeak augmentation for momentum representations. To successfully ﬁnd the pseudo-label as-\nsignment matrix A, we apply weak augmentations for the momentum representations (i.e., A2 is\nweaker than A1) as Zheng et al. (2021) did. This reduces the noise in the representations and con-\nsequently enhances the performance of our PsCo as A becomes more accurate (see Section 4.3).\n3.3\nMETA-TEST\nAt the meta-test stage, we have an N-way K-shot task T consisting of query samples {xq} and\nsupport samples S = {(xs, ys)}NK\ns=1.3 We here discard the momentum network φ and use only the\nonline network θ. To predict labels, we ﬁrst compute the query representation qq := Normalize(hθ ◦\ngθ◦fθ(xq)) and the support representations zs := Normalize (gθ ◦fθ(xs))). Then we predict a label\nby the following classiﬁcation rule: ˆy := arg maxy q⊤\nq cy where cy := Normalize(P\ns 1ys=y ·zs) is\nthe prototype vector. This is inspired by our LPsCo, which can be interpreted as minimizing distance\nfrom the mean (i.e., prototype) of the shot representations.4\nFurther adaptation for cross-domain few-shot classiﬁcation. Under cross-domain few-shot clas-\nsiﬁcation scenarios, the model θ should further adapt to the meta-test domain due to the dissimilarity\nfrom meta-training. We here suggest an efﬁcient adaptation scheme using only a few labeled sam-\nples. Our idea is to consider the support samples as queries. To be speciﬁc, we compute the query\nrepresentation qs := Normalize(hθ◦gθ◦fθ(xs)) for each support sample xs, and construct the label\nassignment matrix A′ as A′\ns,s′ = 1 if and only if ys = ys′. Then we simply optimize only gθ and\nhθ via contrastive learning, i.e., LContrast({qs}, {zs}, A′; τPsCo), for few iterations. We empirically\nobserve that this adaptation scheme is effective under cross-domain settings (see Section 4.3).\n3Note that N and K for meta-training and meta-test could be different. We use a large N (e.g., N = 256)\nduring meta-training to fully utilize computational resources like standard deep learning, and a small N (e.g.,\nN = 5) during meta-test following the meta-learning literature.\n4LPsCo = −1\nN\nP\ni\n1\nτPsCo q⊤\ni\n\u0010\n1\nK\nP\nj Ai,jzj\n\u0011\n+ term not depending on A.\n5\nPublished as a conference paper at ICLR 2023\nTable 1: Few-shot classiﬁcation accuracy (%) on Omniglot and miniImageNet benchmarks. We\nreport the average accuracy over 2000 few-shot tasks for PsCo and self-supervised learning methods.\nOther reported numbers borrow from Khodadadeh et al. (2021); Kong et al. (2021). Bold entries\nindicate the best for each task conﬁguration, among unsupervised and self-supervised methods.\nOmniglot (way, shot)\nminiImageNet (way, shot)\nMethod\n(5,1)\n(5,5)\n(20,1)\n(20,5)\n(5,1)\n(5,5)\n(5,20)\n(5,50)\nTraining from Scratch\n52.50\n74.78\n24.91\n47.62\n27.59\n38.48\n51.53\n59.63\nUnsupervised meta-learning\nCACTUs-MAML\n68.84\n87.78\n48.09\n73.36\n39.90\n53.97\n63.84\n69.64\nCACTUs-ProtoNets\n68.12\n83.58\n47.75\n66.27\n39.18\n53.36\n61.54\n63.55\nUMTRA\n83.80\n95.43\n74.25\n92.12\n39.93\n50.73\n61.11\n67.15\nLASIUM-MAML\n83.26\n95.29\n-\n-\n40.19\n54.56\n65.17\n69.13\nLASIUM-ProtoNets\n80.15\n91.10\n-\n-\n40.05\n52.53\n61.09\n64.89\nMeta-GMVAE\n94.92\n97.09\n82.21\n90.61\n42.82\n55.73\n63.14\n68.26\nMeta-SVEBM\n91.85\n97.21\n79.66\n92.21\n43.38\n58.03\n67.07\n72.28\nPsCo (Ours)\n96.37\n99.13\n89.64\n97.07\n46.70\n63.26\n72.22\n73.50\nSelf-supervised learning\nSimCLR\n92.13\n97.06\n80.95\n91.60\n43.35\n52.50\n61.83\n64.85\nMoCo v2\n92.66\n97.38\n82.13\n92.35\n41.92\n50.94\n60.23\n63.45\nSwAV\n93.13\n97.32\n82.63\n92.12\n43.24\n52.41\n61.36\n64.52\nSupervised meta-learning\nMAML\n94.46\n98.83\n84.60\n96.29\n46.81\n62.13\n71.03\n75.54\nProtoNets\n98.35\n99.58\n95.31\n98.81\n46.56\n62.29\n70.05\n72.04\n4\nEXPERIMENTS\nIn this section, we demonstrate the effectiveness of the proposed framework under standard few-\nshot learning benchmarks (Section 4.1) and cross-domain few-shot learning benchmarks (Section\n4.2). We provide ablation studies regarding PsCo in Section 4.3. Following Lee et al. (2021), we\nmainly use Conv4 and Conv5 architectures for Omniglot (Lake et al., 2011) and miniImageNet (Ravi\n& Larochelle, 2017), respectively, for the backbone feature extractor fθ. For the number of shots\nduring meta-learning, we use K = 1 for Omniglot and K = 4 for miniImageNet (see Table 6 for the\nsensitivity of K). Other details are fully described in Appendix A. We omit the conﬁdence intervals\nin this section for clarity, and the full results with them are provided in Appendix F.\n4.1\nSTANDARD FEW-SHOT BENCHMARKS\nSetup. We here evaluate PsCo on the standard few-shot benchmarks of unsupervised meta-learning:\nOmniglot (Lake et al., 2011) and miniImageNet (Ravi & Larochelle, 2017). We compare PsCo’s\nperformance with unsupervised meta-learning methods (Hsu et al., 2019; Khodadadeh et al., 2019;\n2021; Lee et al., 2021; Kong et al., 2021), self-supervised learning methods (Chen et al., 2020a;b;\nCaron et al., 2020), and supervised meta-learning methods (Finn et al., 2017; Snell et al., 2017) on\nthe benchmarks. The details of the benchmarks and the baselines are described in Appendix D.\nFew-shot classiﬁcation results. Table 1 shows the results of the few-shot classiﬁcation with various\n(way, shot) tasks of Omniglot and miniImageNet. PsCo achieves state-of-the-art performance on\nboth Omniglot and miniImageNet benchmarks under the unsupervised setting. For example, we\nobtain 5% accuracy gain (67.07 →72.22) on miniImageNet 5-way 20-shot tasks. Moreover, the\nperformance is even competitive with supervised meta-learning methods, ProtoNets (Snell et al.,\n2017), and MAML (Finn et al., 2017) as well.\n4.2\nCROSS-DOMAIN FEW-SHOT BENCHMARKS\nSetup. We evaluate PsCo on cross-domain few-shot classiﬁcation benchmarks following Oh et al.\n(2022). To be speciﬁc, we use (a) benchmark of large-similarity with ImageNet: CUB (Wah et al.,\n2011), Cars (Krause et al., 2013), Places (Zhou et al., 2018), and Plantae (Horn et al., 2018); (b)\nbenchmarks of small-similarity with ImageNet: CropDiseases (Mohanty et al., 2016), EuroSAT\n(Helber et al., 2019), ISIC (Codella et al., 2018), and ChestX (Wang et al., 2017). As baselines, we\n6\nPublished as a conference paper at ICLR 2023\nTable 2: Few-shot classiﬁcation accuracy (%) on cross-domain few-shot classiﬁcation benchmarks.\nWe transfer Conv5 trained on miniImageNet to each benchmark. We report the average accuracy\nover 2000 few-shot tasks for all methods, except Meta-SVEBM as it is evaluated over 200 tasks\ndue to the long evaluation time. Bold entries indicate the best for each task conﬁguration, among\nunsupervised and self-supervised methods.\n(a) Cross-domain few-shot benchmarks similar to miniImageNet.\nCUB\nCars\nPlaces\nPlantae\nMethod\n(5, 5)\n(5, 20)\n(5, 5)\n(5, 20)\n(5, 5)\n(5, 20)\n(5, 5)\n(5, 20)\nUnsupervised meta-learning\nMeta-GMVAE\n47.48\n54.08\n31.39\n38.36\n57.70\n65.08\n38.27\n45.02\nMeta-SVEBM\n45.50\n54.61\n34.27\n46.23\n51.27\n61.09\n38.12\n46.22\nPsCo (Ours)\n57.38\n68.58\n44.01\n57.50\n63.60\n73.95\n52.72\n64.53\nSelf-supervised learning\nSimCLR\n52.11\n61.89\n37.40\n50.05\n60.10\n69.93\n43.42\n54.92\nMoCo v2\n53.23\n62.81\n38.65\n51.77\n59.09\n69.08\n43.97\n55.45\nSwAV\n51.58\n61.38\n36.85\n50.03\n59.57\n69.70\n42.68\n54.03\nSupervised meta-learning\nMAML\n56.57\n64.17\n41.17\n48.82\n60.05\n67.54\n47.33\n54.86\nProtoNets\n56.74\n65.03\n38.98\n47.98\n59.39\n67.77\n45.89\n54.29\n(b) Cross-domain few-shot benchmarks dissimilar to miniImageNet.\nCropDiseases\nEuroSAT\nISIC\nChestX\nMethod\n(5, 5)\n(5, 20)\n(5, 5)\n(5, 20)\n(5, 5)\n(5, 20)\n(5, 5)\n(5, 20)\nUnsupervised meta-learning\nMeta-GMVAE\n73.56\n81.22\n73.83\n80.11\n33.48\n39.48\n23.23\n26.26\nMeta-SVEBM\n71.82\n83.13\n70.83\n80.21\n38.85\n48.43\n26.26\n28.91\nPsCo (Ours)\n88.24\n94.95\n81.08\n87.65\n44.00\n54.59\n24.78\n27.69\nSelf-supervised learning\nSimCLR\n79.90\n88.73\n79.14\n85.05\n42.83\n51.35\n25.14\n29.21\nMoCo v2\n80.96\n89.85\n79.94\n86.16\n43.43\n52.14\n25.24\n29.19\nSwAV\n80.15\n89.24\n79.31\n85.62\n43.21\n51.99\n24.99\n28.57\nSupervised meta-learning\nMAML\n77.76\n83.24\n71.48\n76.70\n47.34\n55.09\n22.61\n24.25\nProtoNets\n76.01\n83.64\n64.91\n70.88\n40.62\n48.38\n23.15\n25.72\ntest the previous state-of-the-art unsupervised meta-learning (Lee et al., 2021; Kong et al., 2021),\nself-supervised learning (Chen et al., 2020a;b; Caron et al., 2020), and supervised meta-learning\n(Finn et al., 2017; Snell et al., 2017). We here use our adaptation scheme (Section 3.3) with 50\niterations. The details of the benchmarks and implementations are described in Appendix E.\nSmall-scale cross-domain few-shot classiﬁcation results. We here evaluate various Conv5 models\nmeta-trained on miniImageNet as used in Section 4.1. Table 2 shows that PsCo outperforms all the\nbaselines across all the benchmarks, except ChestX, which is too different from the distribution of\nminiImageNet (Oh et al., 2022). Somewhat interestingly, PsCo competitive with supervised learning\nunder these benchmarks, e.g., PsCo achieves 88% accuracy on CropDiseases 5-way 5-shot tasks,\nwhereas MAML gets 77%. This implies that our unsupervised method, PsCo, generalizes on more\ndiverse tasks than supervised learning, which is specialized to in-domain tasks.\nLarge-scale cross-domain few-shot classiﬁcation results. We also validate that our meta-learning\nframework is applicable to the large-scale benchmark, ImageNet (Deng et al., 2009). Remark that\nthe recent unsupervised meta-learning methods (Lee et al., 2021; Kong et al., 2021; Khodadadeh\net al., 2021) rely on generative models, so they are not easily applicable to such a large-scale bench-\nmark. For example, we observe that PsCo is 2.7 times faster than the best baseline, Meta-SVEBM\n(Kong et al., 2021), even though Meta-SVEBM uses low-dimensional representations instead of full\nimages during training. Hence, we compare PsCo with (a) self-supervised methods, MoCo v2 (Chen\net al., 2020b) and BYOL (Grill et al., 2020), and (b) the publicly-available supervised learning base-\nline. We here use the ResNet-50 (He et al., 2016) architecture. The training details are described in\nAppendix E.4 and we also provide ResNet-18 results in Appendix F.\n7\nPublished as a conference paper at ICLR 2023\nTable 3: 5-way 5-shot classiﬁcation accuracy (%) on cross-domain few-shot benchmarks. We trans-\nfer ImageNet-trained ResNet-50 models to each benchmark. We report the average accuracy over\n600 few-shot tasks.\nMethod\nCUB\nCars\nPlaces\nPlantae\nCropDiseases\nEuroSAT\nISIC\nChestX\nMoCo v2\n64.16\n47.67\n81.39\n61.36\n82.89\n76.96\n38.26\n24.28\n+PsCo (Ours)\n76.63\n53.45\n83.87\n69.17\n89.85\n83.99\n41.64\n23.60\nBYOL\n67.45\n45.74\n75.43\n56.86\n80.82\n77.70\n37.27\n24.15\n+PsCo (Ours)\n82.13\n56.19\n83.80\n71.14\n92.92\n85.33\n42.90\n26.05\nSupervised\n89.13\n75.15\n84.41\n72.91\n90.96\n85.64\n43.34\n25.35\nTable 4: Component ablation studies on Omniglot.\nMomentum\nPredictor\nSinkhorn\nTop-K sampling\nLMoCo\n(5, 1)\n(5, 5)\n(20, 1)\n(20, 5)\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n96.37\n99.13\n89.64\n97.07\n\u0017\n\u0013\n\u0013\n\u0013\n\u0013\n90.32\n96.78\n76.17\n90.41\n\u0013\n\u0017\n\u0013\n\u0013\n\u0013\n90.21\n96.86\n76.15\n90.53\n\u0013\n\u0013\n\u0017\n\u0013\n\u0013\n95.81\n98.94\n88.25\n96.57\n\u0013\n\u0013\n\u0013\n\u0017\n\u0013\n94.95\n98.81\n86.32\n96.05\n\u0013\n\u0013\n\u0013\n\u0013\n\u0017\n93.16\n97.40\n81.03\n91.33\nTable 3 shows that (i) PsCo consistently improves both MoCo and BYOL under this setup (e.g.,\n67% →82% in CUB), and (ii) PsCo beneﬁts from the large-scale dataset as we obtain a huge amount\nof performance gain on the benchmarks of large-similarity with ImageNet: CUB, Cars, Places, and\nPlantae. Consequently, we achieve comparable performance with the supervised learning baseline,\nexcept Cars, which shows that our PsCo is applicable to large-scale unlabeled datasets.\n4.3\nABLATION STUDY\nComponent analysis. In Table 4, we demonstrate the necessity of each component in PsCo by\nremoving the components one by one: momentum encoder φ, prediction head h, Sinkhorn-Knopp\nalgorithm, top-K sampling for sampling support samples, and the MoCo objective, LMoCo (6). We\nfound that the momentum network φ and the prediction head h are critical architectural components\nin our framework like recent self-supervised learning frameworks (Grill et al., 2020; Chen et al.,\n2021). In addition, Table 4 shows that training with only our objective, LPsCo (5), achieves mean-\ningful performance, but incorporating it into MoCo is more beneﬁcial. To further validate that our\ntask construction is progressively improved during meta-learning, we evaluate whether a query and\na corresponding support sample have the same true label. Figure 2a shows that our task construction\nis progressively improved, i.e., the task requirement (i) described in Section 3.1 satisﬁes.\nTable 4 also veriﬁes the contribution of the Sinkhorn-Knopp algorithm and Top-K sampling for the\nperformance of PsCo. We further analyze the effect of the Sinkhorn-Knopp algorithm by measuring\nthe overlap ratio of selected supports between different pseudo-labels. As shown in Figure 2b, there\nare almost zero overlaps when using the Sinkhorn-Knopp algorithm, which means the constructed\ntask is a valid few-shot task, satisfying the task requirement (ii) described in Section 3.1.\nAdaptation effect on cross-domain. To validate the effect of our adaptation scheme (Section 3.3),\nwe evaluate the few-shot classiﬁcation accuracy during the adaptation process on miniImageNet\n(i.e., in-domain) and CropDiseases (i.e., cross-domain) benchmarks. As shown in Figure 2d, we\nfound that the adaptation scheme is more useful in cross-domain benchmarks than in-domain ones.\nBased on these results, we apply the scheme to only the cross-domain scenarios. We also found\nthat our adaptation does not cause over-ﬁtting since we only optimize the projection and prediction\nheads gθ and hθ. The results for the adaptation effect on the whole benchmarks are represented in\nAppendix C.\nAugmentations. We here conﬁrm that weak augmentation for the momentum network (i.e., A2) is\nmore effective than strong augmentation unlike other self-supervised learning literature (Chen et al.,\n2020a; He et al., 2020). We denote the standard augmentation consisting of both geometric and color\ntransformations by Strong, and a weaker augmentation consisting of only geometric transformations\nas Weak (see details in Appendix A). As shown in Table 5, utilizing the weak augmentation for A2\nis much more beneﬁcial since it helps to ﬁnd an accurate pseudo-label assignment matrix A.\n8\nPublished as a conference paper at ICLR 2023\n0\n100\n200\n300\n400\ntrain epochs\n30\n40\n50\n60\n70\naccuracy (%)\ncorrectly sampled shots\nOurs\nMoCo v2\n(a) Pseudo-label quality\n0\n100\n200\n300\n400\ntrain epochs\n0\n2\n4\n6\noverlap ratio (%)\noverlap between sampled shots\nw/ sinkhorn\nw/out sinkhorn\n(b) Shot overlap ratio\n0\n20\n40\n60\n80\n100\nadaptation iterations\n55\n60\n65\n70\n75\naccuracy (%)\nminiImageNet \n miniImageNet\n5shot\n20shot\n(c) In-domain adaptation\n0\n20\n40\n60\n80\n100\nadaptation iterations\n80\n85\n90\n95\n100\naccuracy (%)\nminiImageNet \n CropDiseases\n5shot\n20shot\n(d) Cross-domain adaptation\nFigure 2: (a) Pseudo-label quality, measuring the agreement between pseudo-labels and true la-\nbels, (b) Shot overlap ratio, measuring whether the shots for each pseudo-label are disjoint, during\nmeta-training. (c,d) Performance while adaptation on in-domain (miniImageNet) and cross-domain\n(CropDiseases) benchmarks, respectively. We obtain these results from 100 random batches.\nTable 5: The ablation study with varying augmentation\nchoices for A1 and A2 on miniImageNet.\nA1\nA2\n(5, 1)\n(5, 5)\n(5, 20)\n(5, 50)\nStrong\nStrong\n44.54\n60.04\n68.61\n71.20\nStrong\nWeak\n46.70\n63.26\n72.22\n73.50\nWeak\nStrong\n43.56\n58.77\n67.21\n69.46\nWeak\nWeak\n40.68\n55.05\n63.32\n65.82\nTable 6: The ablation study with vary-\ning K on miniImageNet.\nK\n(5, 1)\n(5, 5)\n(5, 20)\n(5, 50)\n1\n45.88\n61.84\n70.25\n72.76\n4\n46.70\n63.26\n72.22\n73.50\n16\n46.31\n62.76\n70.91\n73.43\n64\n46.60\n62.50\n70.82\n73.22\nTraining K. We also look at the effect of the training K, i.e. number of shots sampled online.\nWe conduct the experiment with K ∈{1, 4, 16, 64}. We observe that PsCo performs consistently\nwell regardless of the choice of K as shown in Table 6. The proper K is suggested to obtain the\nbest-performing models, e.g., K = 4 for miniImageNet and K = 1 for Omniglot are the best.\n5\nRELATED WORKS\nUnsupervised meta-learning. Unsupervised meta-learning (Hsu et al., 2019; Khodadadeh et al.,\n2019; Lee et al., 2021; Kong et al., 2021; Khodadadeh et al., 2021) links meta-learning and unsu-\npervised learning by constructing synthetic tasks and extracting the meaningful information from\nunlabeled data. For example, CACTUs (Hsu et al., 2019) cluster the data on the pretrained repre-\nsentations at the beginning of meta-learning to assign pseudo-labels. Instead of pseudo-labeling,\nUMTRA (Khodadadeh et al., 2019) and LASIUM (Khodadadeh et al., 2021) generate synthetic\nsamples using data augmentations or pretrained generative networks like BigBiGAN (Donahue &\nSimonyan, 2019). Meta-GMVAE (Lee et al., 2021) and Meta-SVEBM (Kong et al., 2021) represent\nunknown labels via categorical latent variables using variational autoencoders (Kingma & Welling,\n2014) and energy-based models (Teh et al., 2003), respectively. In this paper, we suggest a novel\nonline pseudo-labeling strategy to construct diverse tasks without help from any pretrained network\nor generative model. As a result, our method is easily applicable to large-scale datasets.\nSelf-supervised learning. Self-supervised learning (SSL) (Doersch et al., 2015) has shown remark-\nable success for unsupervised representation learning across various domains, including vision (He\net al., 2020; Chen et al., 2020a), speech (Oord et al., 2018), and reinforcement learning (Laskin\net al., 2020). Among SSL objectives, contrastive learning (Oord et al., 2018; Chen et al., 2020a; He\net al., 2020) is arguably most popular for learning meaningful representations. In addition, recent\nadvances have been made with the development of various architectural components: e.g., Siamese\nnetworks (Doersch et al., 2015), momentum networks (He et al., 2020), and asymmetric architec-\ntures (Grill et al., 2020; Chen & He, 2021). In this paper, we utilize the SSL components to construct\ndiverse few-shot tasks in an unsupervised manner.\n6\nCONCLUSION\nAlthough unsupervised meta-learning (UML) and self-supervised learning (SSL) share the same\npurpose of learning generalizable knowledge to unseen tasks by utilizing unlabeled data, there still\nexists a gap between UML and SSL literature. In this paper, we bridge the gap as we tailor various\nSSL components to UML, especially for few-shot classiﬁcation, and we achieve superior perfor-\nmance under various few-shot classiﬁcation scenarios. We believe our research could bring many\nfuture research directions in both the UML and SSL communities.\n9\nPublished as a conference paper at ICLR 2023\nETHICS STATEMENT\nUnsupervised learning, especially self-supervised learning, often requires a large number of training\nsamples, a huge model, and a high computational cost for training the model on large-scale data to\nobtain meaningful representations because of the absence of human annotations. Furthermore, ﬁne-\ntuning the model for solving a new task is also time-consuming and memory-inefﬁcient. Hence, it\ncould raise environmental issues such as carbon generation, which could bring an abnormal climate\nand accelerate global warming. In that sense, meta-learning should be considered as a solution since\nits purpose is to learn generalizable knowledge that can be quickly adapted to unseen tasks. In\nparticular, unsupervised meta-learning, which beneﬁts from both meta-learning and unsupervised\nlearning, would be an important research direction. We believe that our work could be a useful step\ntoward learning easily-generalizable knowledge from unlabeled data.\nREPRODUCIBILITY STATEMENT\nWe provide all the details to reproduce our experimental results in Appendix A, D, and E. The code\nis available at https://github.com/alinlab/PsCo. In our experiments, we mainly use\nNVIDIA GTX3090 GPUs.\nACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING\nThis work was mainly supported by Institute of Information & communications Technology Plan-\nning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Arti-\nﬁcial Intelligence Graduate School Program (KAIST); No.2022-0-00713, Meta-learning applicable\nto real-world problems; No.2022-0-00959, Few-shot Learning of Causal Inference in Vision and\nLanguage for Decision Making).\nREFERENCES\nFerran Alet, Martin F. Schneider, Tomas Lozano-Perez, and Leslie Pack Kaelbling. Meta-learning\ncuriosity algorithms.\nIn International Conference on Learning Representations, 2020.\nURL\nhttps://openreview.net/forum?id=BygdyxHFDS.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. In Advances in Neu-\nral Information Processing Systems 33: Annual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\nTing Chen, Simon Kornblith, Mohammad Norouzi 0002, and Geoffrey E. Hinton. A simple frame-\nwork for contrastive learning of visual representations. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of\nProceedings of Machine Learning Research, pp. 1597–1607. PMLR, 2020a.\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer\nlook at few-shot classiﬁcation. In 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750–15758, 2021.\nXinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. CoRR, abs/2003.04297, 2020b.\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\ntransformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n9640–9649, 2021.\nNoel C. F. Codella, David Gutman, M. Emre Celebi, Brian Helba, Michael A. Marchetti, Stephen W.\nDusza, Aadi Kalloo, Konstantinos Liopyris, Nabin K. Mishra, Harald Kittler, and Allan Halpern.\n10\nPublished as a conference paper at ICLR 2023\nSkin lesion analysis toward melanoma detection: A challenge at the 2017 international sympo-\nsium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic).\nIn 15th IEEE International Symposium on Biomedical Imaging, ISBI 2018, Washington, DC,\nUSA, April 4-7, 2018, pp. 168–172. IEEE, 2018.\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in\nNeural Information Processing Systems 26: 27th Annual Conference on Neural Information Pro-\ncessing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada,\nUnited States, pp. 2292–2300, 2013.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li 0002. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pp. 248–255. IEEE,\n2009.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by\ncontext prediction. In Proceedings of the IEEE international conference on computer vision, pp.\n1422–1430, 2015.\nJeff Donahue and Karen Simonyan. Large scale adversarial representation learning. In Advances in\nneural information processing systems, 2019.\nYan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast\nreinforcement learning via slow reinforcement learning, 2017. URL https://openreview.\nnet/forum?id=HkLXCE9lx.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. In International conference on machine learning, pp. 1126–1135. PMLR, 2017.\nLuca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel\nprogramming for hyperparameter optimization and meta-learning. In International Conference\non Machine Learning, pp. 1568–1577. PMLR, 2018.\nJean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\net al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural\ninformation processing systems, 33:21271–21284, 2020.\nYunhui Guo, Noel Codella, Leonid Karlinsky, James V. Codella, John R. Smith, Kate Saenko, Ta-\njana Rosing, and Rog´erio Feris. A broader study of cross-domain few-shot learning. In Computer\nVision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceed-\nings, Part XXVII, volume 12372 of Lecture Notes in Computer Science, pp. 124–141. Springer,\n2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for\nunsupervised visual representation learning. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 9726–9735. IEEE,\n2020.\nPatrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset\nand deep learning benchmark for land use and land cover classiﬁcation. IEEE J Sel. Topics in\nAppl. Earth Observ. and Remote Sensing, 12(7):2217–2226, 2019.\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alexander Shepard, Hartwig\nAdam, Pietro Perona, and Serge J. Belongie. The inaturalist species classiﬁcation and detection\ndataset. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt\nLake City, UT, USA, June 18-22, 2018, pp. 8769–8778. IEEE Computer Society, 2018.\n11\nPublished as a conference paper at ICLR 2023\nRein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI Jonathan Ho, and\nPieter Abbeel. Evolved policy gradients. In Advances in Neural Information Processing Systems,\n2018.\nKyle Hsu, Sergey Levine, and Chelsea Finn.\nUnsupervised learning via meta-learning.\nIn 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019. OpenReview.net, 2019.\nSiavash Khodadadeh, Ladislau B¨ol¨oni, and Mubarak Shah. Unsupervised meta-learning for few-shot\nimage classiﬁcation. In Advances in Neural Information Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,\nVancouver, BC, Canada, pp. 10132–10142, 2019.\nSiavash Khodadadeh, Sharare Zehtabian, Saeed Vahidian, Weijia Wang, Bill Lin, and Ladislau\nB¨olo¨oi. Unsupervised meta-learning through latent-space interpolation in generative models. In\n9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net, 2021.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In Advances in Neu-\nral Information Processing Systems 33: Annual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,\n2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\nDiederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International\nConference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,\nConference Track Proceedings, 2014.\nDeqian Kong, Bo Pang, and Ying Nian Wu. Unsupervised meta-learning via latent space energy-\nbased model of symbol vector coupling. In Fifth Workshop on Meta-Learning at the Conference\non Neural Information Processing Systems, 2021.\nJonathan Krause, Michael Stark 0003, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-\ngrained categorization. In 2013 IEEE International Conference on Computer Vision Workshops,\nICCV Workshops 2013, Sydney, Australia, December 1-8, 2013, pp. 554–561. IEEE Computer\nSociety, 2013.\nBrenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning\nof simple visual concepts. In Proceedings of the 33th Annual Meeting of the Cognitive Science So-\nciety, CogSci 2011, Boston, Massachusetts, USA, July 20-23, 2011. cognitivesciencesociety.org,\n2011.\nMichael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representa-\ntions for reinforcement learning. In International Conference on Machine Learning, 2020.\nDong-Bok Lee, Dongchan Min, Seanie Lee, and Sung Ju Hwang. Meta-gmvae: Mixture of gaussian\nvae for unsupervised meta-learning. In 9th International Conference on Learning Representa-\ntions, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\nSharada P Mohanty, David P Hughes, and Marcel Salath´e. Using deep learning for image-based\nplant disease detection. Frontiers in plant science, 7:1419, 2016.\nJaehoon Oh, Sungnyun Kim, Namgyu Ho, Jin-Hwa Kim, Hwanjun Song, and Se-Young Yun. Un-\nderstanding cross-domain few-shot learning: An experimental study. CoRR, abs/2202.01339,\n2022.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\n12\nPublished as a conference paper at ICLR 2023\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Edward\nYang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep\nlearning library. In Advances in Neural Information Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver,\nBC, Canada, pp. 8024–8035, 2019.\nLyle Ramshaw and Robert E Tarjan. On minimum-cost assignments in unbalanced bipartite graphs.\nHP Labs, Palo Alto, CA, USA, Tech. Rep. HPL-2012-40R1, 20, 2012.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In 5th Interna-\ntional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net, 2017.\nJake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning.\nIn Advances in Neural Information Processing Systems 30: Annual Conference on Neural Infor-\nmation Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 4080–4090,\n2017.\nYee Whye Teh, Max Welling, Simon Osindero, and Geoffrey E. Hinton. Energy-based models\nfor sparse overcomplete representations. Journal of Machine Learning Research, 4:1235–1260,\n2003.\nSebastian Thrun and Lorien Pratt. Learning to learn. Springer, 1998.\nHung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, and Ming-Hsuan Yang 0001. Cross-domain few-\nshot classiﬁcation via learned feature-wise transformation. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net, 2020.\nC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-ucsd birds 200. Technical\nReport CNS-TR-2011-001, California Institute of Technology, 2011.\nXiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M. Sum-\nmers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised\nclassiﬁcation and localization of common thorax diseases. In 2017 IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp.\n3462–3471. IEEE Computer Society, 2017.\nMingkai Zheng, Shan You, Fei Wang 0032, Chen Qian 0006, Changshui Zhang, Xiaogang Wang\n0001, and Chang Xu 0002. Ressl: Relational self-supervised learning with weak augmentation.\nIn Advances in Neural Information Processing Systems 34: Annual Conference on Neural Infor-\nmation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 2543–2555,\n2021.\nBolei Zhou, `Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba 0001. Places: A\n10 million image database for scene recognition. IEEE Trans. Pattern Anal. Mach. Intell., 40(6):\n1452–1464, 2018.\n13\nPublished as a conference paper at ICLR 2023\nA\nIMPLEMENTATION DETAILS\nWe train our models via stochastic gradient descent (SGD) with a batch size of N = 256 for 400\nepochs. Following Chen et al. (2020b); Chen & He (2021), we use an initial learning rate of 0.03\nwith the cosine learning schedule, τMoCo = 0.2, and a weight decay of 5×10−4. We use a queue size\nof M = 16384 since Omniglot (Lake et al., 2011) and miniImageNet (Ravi & Larochelle, 2017)\nhas roughly 100k meta-training samples. Following Lee et al. (2021), we use Conv4 and Conv5 for\nOmniglot and miniImageNet, respectively, for the backbone feature extractor fθ. We describe the\ndetailed architectures in Table 7. For projection and prediction MLPs, gθ and hθ, we use 2-layer\nMLPs with a hidden size of 2048 and an output dimension of 128. For the hyperparameters of PsCo,\nwe use τPsCo = 1 and a momentum parameter of m = 0.99 (see Appendix B for the hyperparameter\nsensitivity). For the number of shots during meta-learning, we use K = 1 for Omniglot and K = 4\nfor miniImageNet (see Table 6 for the sensitivity of K). We use the last-epoch model for evaluation\nwithout any guidance from the meta-validation dataset.\nTable 7: Pytorch-like architecture descriptions for standard few-shot benchmarks\nBackbone\nLayer descriptions\nOutput shape\nConv4\n[Conv2d(3×3, 64 filter), BatchNorm2d, ReLU, MaxPool2d(2×2)] ×4\n64 × 2 × 2\nConv5\n[Conv2d(3×3, 64 filter), BatchNorm2d, ReLU, MaxPool2d(2×2)] ×5\n64 × 2 × 2\nAugmentations. We describe the augmentations for Omniglot and miniImagenet in Table 8. For\nOmniglot, because it is difﬁcult to apply many augmentations to gray-scale images, we use the same\nrule for weak and strong augmentations. For miniImageNet, we use only geometric transformations\nfor the weak augmentation following Zheng et al. (2021).\nTable 8: Pytorch-like augmentation descriptions for Omniglot and miniImageNet\nDataset\nAugmentation\nDescriptions\nOmniglot\nStrong, Weak\nRandomResizeCrop(28, scale=(0.2, 1))\nRandomHorizontalFlip()\nminiImagenet\nStrong\nRandomResizedCrop(84, scale=(0.2, 1))\nRandomApply([ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.1)\nRandomGrayScale(p=0.2)\nRandomHorizontalFlip()\nWeak\nRandomResizedCrop(84, scale=(0.2, 1))\nRandomHorizontalFlip()\nTraining procedures. To ensure the performance of PsCo and self-supervised learning models, we\nuse three independently-trained models with random seeds and report the average performance of\nthem.\nB\nANALYSIS ON HYPERPARAMETER SENSITIVITY\nFor the small-scale experiments, we use a momentum of m = 0.99 and a temperature of τPsCo = 1.\nWe here provide more ablation experiments with varying the hyperparameters m and τPsCo. Table 9\nand 10 show the sensitivity of hyperparameters on the miniImageNet dataset. We observe that PsCo\nachieves good performance even for non-optimal hyperparameters.\nTable 9: Sensitivity of momentum m on mini-\nImageNet (way, shot).\nm\n(5, 1)\n(5, 5)\n(5, 20)\n(5, 50)\n0.9\n46.49\n62.18\n70.21\n72.77\n0.99\n46.70\n63.26\n72.22\n73.50\n0.999\n45.96\n61.53\n69.66\n72.04\nTable 10: Sensitivity of temperature τPsCo on\nminiImageNet (way, shot).\nτPsCo\n(5, 1)\n(5, 5)\n(5, 20)\n(5, 50)\n0.2\n46.43\n62.29\n70.04\n72.22\n0.5\n46.32\n62.63\n70.50\n73.15\n1.0\n46.70\n63.26\n72.22\n73.50\n14\nPublished as a conference paper at ICLR 2023\nC\nEFFECT OF ADAPTATION\nWe measure the performance with and without our adaptation scheme on various domains using\nminiImageNet-pretrained PsCo. Table 11 shows that our adaptation scheme enhances the way to\nadapt to each domain. In particular, the adaptation scheme is highly suggested for cross-domain\nfew-shot classiﬁcation scenarios.\nTable 11: Before and after adaptation of PsCo in few-shot classiﬁcation.\nAdaptation\nminiImageNet\nCUB\nCars\nPlaces\nPlantae\nCropDiseases\nEuroSAT\nISIC\nChestX\n5-way 5-shot\n\u0017\n63.26\n55.15\n42.27\n62.98\n48.31\n79.75\n74.73\n41.18\n24.54\n\u0013\n63.30\n57.38\n44.01\n63.60\n52.72\n88.24\n81.08\n44.00\n24.78\n5-way 20-shot\n\u0017\n72.22\n62.35\n51.02\n70.85\n55.91\n84.72\n78.96\n48.53\n27.60\n\u0013\n73.00\n68.58\n57.50\n73.95\n64.53\n94.95\n87.65\n54.59\n27.69\nD\nSETUP FOR STANDARD FEW-SHOT BENCHMARKS\nWe here describe details of benchmarks and baselines in Section D.1 and D.2, respectively, for the\nstandard few-shot classiﬁcation experiments (Section 4.1).\nD.1\nDATASETS\nOmniglot (Lake et al., 2011) is a 28 × 28 gray-scale dataset of 1623 characters with 20 samples\neach. We follow the setup of unsupervised meta-learning approaches (Hsu et al., 2019). We split the\ndataset into 120, 100, and 323 classes for meta-training, meta-validation, and meta-test respectively.\nIn addition, the 0, 90, 180, and 270 degrees rotated views for each class become the different cate-\ngories. Thus, we have a total of 6492, 400, and 1292 classes for meta-training, meta-validation, and\nmeta-test respectively.\nMiniImageNet (Ravi & Larochelle, 2017) is an 84 × 84 resized subset of ILSVRC-2012 (Deng\net al., 2009) with 600 samples each. We split the dataset into 64, 16, and 20 classes for meta-\ntraining, meta-validation, and meta-test respectively as introduced in Ravi & Larochelle (2017).\nD.2\nBASELINES\nWe compare our performance with unsupervised meta-learning, self-supervised learning, and su-\npervised meta-learning methods. To be speciﬁc, (a) for the unsupervised meta-learning, we use\nCACTUs (Hsu et al., 2019) of the best options (ACAI clustering for Omniglot and DeepCluster for\nminiImageNet), UMTRA (Khodadadeh et al., 2019), LASIUM (Laskin et al., 2020) of the best op-\ntions (LASIUM-RO-GAN for Omniglot and LASIUM-N-GAN for miniImageNet), Meta-GMVAE\n(Lee et al., 2021), Meta-SVEBM (Kong et al., 2021); (b) for the self-supervised learning methods,\nwe use SimCLR (Chen et al., 2020a), MoCo v2 (Chen et al., 2020b), and SwAV (Caron et al., 2020);\n(c) for the supervised meta-learning, we use the results of MAML (Finn et al., 2017) and ProtoNets\n(Snell et al., 2017) reported in (Hsu et al., 2019).\nFor training self-supervised learning methods in our experimental setups, we use the same architec-\nture and hyperparameters. For the hyperparameter of temperature scaling, we use the value provided\nin each paper: τSimCLR = 0.5 for SimCLR, τMoCo = 0.2 for MoCo v2, and τSwAV = 0.1 for SwAV. For\nevaluation, we use K-Nearest Neightobrs (K-NN) for self-supervised learning methods since their\nclassiﬁcation rules are not speciﬁed.\n15\nPublished as a conference paper at ICLR 2023\nE\nSETUP FOR CROSS-DOMAIN FEW-SHOT BENCHMARKS\nWe now describe the setup for cross-domain few-shot benchmarks, including detailed information on\ndatasets, baseline experiments, implementational details, and the setup for large-scale experiments.\nE.1\nDATASETS\nFor the cross-domain few-shot benchmarks, we use eight different datasets. We describe the dataset\ninformation in Table 12. We use the dataset split described in Tseng et al. (2020) for the benchmark\nof high-similarity and we use the dataset split described in Guo et al. (2020) for the benchmark of\nlow-similarity. Because we do not perform the meta-training procedure using the datasets of cross-\ndomain benchmarks, we only utilize the meta-test splits on these datasets. We use the 84×84 resized\nsamples for evaluation on small-scale experiments.\nTable 12: Information of datasets for cross-domain few-shot benchmarks.\nImageNet similarity\nDatset\n# of classes\n# of samples\nHigh\nCUB (Wah et al., 2011)\n200\n11,788\nCars (Krause et al., 2013)\n196\n16,185\nPlaces (Zhou et al., 2018)\n365\n1,800,000\nPlantae (Horn et al., 2018)\n5089\n675,170\nLow\nCropDiseases (Mohanty et al., 2016)\n38\n43,456\nEuroSAT (Helber et al., 2019)\n10\n27,000\nISIC (Codella et al., 2018)\n7\n10,015\nChestX (Wang et al., 2017)\n7\n25,848\nE.2\nBASELINES\nWe compare our performance with (a) previous in-domain state-of-the-art methods of unsupervised\nmeta-learning, self-supervised learning models, and supervised meta-learning models.\nUnsupervised meta-learning models. We use previous in-domain state-of-the-art methods of un-\nsupervised meta-learning models, Meta-GMVAE(Lee et al., 2021) and Meta-SVEBM (Kong et al.,\n2021). We use the miniImageNet pretrained parameters that the paper provided, and follow the\nmeta-test procedure of each model to evaluate the performance.\nSelf-supervised learning models. We use SimCLR (Chen et al., 2020a), MoCo v2 (Chen et al.,\n2020b), and SwAV (Caron et al., 2020) of miniImageNet pretrained parameters as our baselines.\nBecause self-supervised learning models are pretrained on miniImageNet, we additionally ﬁne-tune\nthe models with a linear classiﬁer to let the models adapt to each domain. Following the setting\nprovided in Guo et al. (2020); Oh et al. (2022), we detach the head of the models gθ and attach the\nlinear classiﬁer cψ to the model. We freeze the base network fθ while ﬁne-tuning and only cψ is\nlearned. We ﬁne-tune the models via SGD with an initial learning rate of 0.01, a momentum of 0.9,\nweight decay of 0.001, and a batch size of N = 4 for 100 epochs.\nSupervised meta-learning models. We use MAML (Finn et al., 2017) and ProtoNets (Snell et al.,\n2017) of Conv5 architectures of miniImageNet pretrained. Following the procedure of Snell et al.\n(2017), we train the models via Adam (Kingma & Ba, 2015) with a learning rate of 0.001 and cut\nthe learning rate in half for every training of 2000 episodes. We train them for 60K episodes and use\nthe model of the best validation accuracy. We train them through a 5-way 5-shot, and the rest of the\nhyperparameters are referenced in their respective papers. We observe that their performances are\nsimilar to the performance described in Table 1.\nE.3\nEVALUATION DETAILS\nTo evaluate our method, we apply our adaptation scheme. Following Section 3.3, we freeze the base\nnetwork fθ. We train only projection head gθ and prediction head hθ via SGD with an initial learning\nrate of 0.01, a momentum of 0.9, and weight decay of 0.001 as self-supervised learning models are\nﬁne-tuned. We only apply 50 iterations of our adaptation scheme when reporting performance.\n16\nPublished as a conference paper at ICLR 2023\nE.4\nLARGE-SCALE SETUP\nHere, we describe the setup for large-scale experiments. For evaluating, we use the same protocol\nwith the small-scale experiments, except the scale of images is 224 × 224.\nAugmentations. For large-scale experiments, we use 224 × 224-scaled data. Thus, we use similar\nyet slightly different augmentation schemes with small-scale experiments. Following the strong\naugmentation used in Chen et al. (2020b;a), we additionally apply GaussianBlur as a random\naugmentation. We use the same conﬁguration for weak augmentation. For evaluation, we resize the\nimages into 256 × 256 and then apply the CenterCrop to make 224 × 224 images by following\nGuo et al. (2020).\nImageNet pretraining. We pretrain MoCo v2 (Chen et al., 2020b), BYOL (Grill et al., 2020),\nand our PsCo of ResNet-18/50 (He et al., 2016) via SGD with a batch size of N = 256 for 200\nepochs. Following (Chen et al., 2020b; Chen & He, 2021), we use an initial learning rate of 0.03\nwith the cosine learning schedule, τMoCo = 0.2 and a weight decay of 0.0001. We use a queue size\nof M = 65536 and momentum of m = 0.999. For the parameters of PsCo, we use τPsCo = 0.2 and\nK = 16 as the queue is 4 times bigger. For supervised pretraining, we use the the model checkpoint\nofﬁcially provided by torchvision (Paszke et al., 2019).\n17\nPublished as a conference paper at ICLR 2023\nF\nEXPERIMENTAL RESULTS WITH 95% CONFIDENCE INTERVAL\nWe here provide the experimental results of Table 1, 2, and 3 with 95% conﬁdence intervals in Table\n13, 14, and 15, respectively.\nTable 13: Few-shot classiﬁcation accuracy (%) on Omniglot and miniImageNet with a 95% conﬁ-\ndence interval over 2000 few-shot tasks.\nOmniglot (way, shot)\nminiImageNet (way, shot)\nMethod\n(5, 1)\n(5, 5)\n(20, 1)\n(20, 5)\n(5, 1)\n(5, 5)\n(5, 20)\n(5, 50)\nSimCLR\n92.13±0.30\n97.06±0.13\n80.95±0.21\n91.60±0.12\n43.35±0.42\n52.50±0.39\n61.83±0.35\n64.85±0.32\nMoCo v2\n92.66±0.28\n97.38±0.12\n82,13±0.21\n92.34±0.11\n41.92±0.41\n50.94±0.38\n60.23±0.35\n63.45±0.33\nSwAV\n93.13±0.27\n97.32±0.13\n82.63±0.21\n92.12±0.12\n43.24±0.42\n52.41±0.39\n61.36±0.35\n64.52±0.33\nPsCo (ours)\n96.37±0.20\n99.13±0.07\n89.60±0.17\n97.07±0.07\n46.70±0.42\n63.26±0.37\n72.22±0.32\n73.50±0.29\nTable 14: Few-shot classiﬁcation accuracy (%) on cross-domain few-shot classiﬁcation benchmarks\nof Conv5 pretrained on miniImageNet with a 95% conﬁdence interval over 2000 few-shot tasks.\n(a) Cross-domain few-shot benchmarks similar to miniImageNet.\nCUB\nCars\nPlaces\nPlantae\nMethod\n(5, 5)\n(5, 20)\n(5, 5)\n(5, 20)\n(5, 5)\n(5, 20)\n(5, 5)\n(5, 20)\nMeta-GMVAE\n47.48±0.47\n54.08±0.45\n31.39±0.34\n38.36±0.35\n57.70±0.47\n65.08±0.38\n38.27±0.40\n45.02±0.37\nMeta-SVEBM\n45.50±0.83\n54.61±0.91\n34.27±0.79\n46.23±0.87\n51.27±0.82\n61.09±0.85\n38.12±0.86\n46.22±0.85\nSimCLR\n52.11±0.45\n61.89±0.45\n37.40±0.35\n50.05±0.39\n60.10±0.40\n69.93±0.35\n43.42±0.37\n54.92±0.36\nMoCo v2\n53.23±0.45\n62.81±0.45\n38.65±0.35\n51.77±0.39\n59.09±0.40\n69.08±0.36\n43.97±0.37\n55.45±0.36\nSwAV\n51.58±0.45\n61.38±0.46\n36.85±0.33\n50.03±0.38\n59.57±0.40\n69.70±0.36\n42.68±0.37\n54.03±0.36\nPsCo (ours)\n57.38±0.44\n68.58±0.41\n44.01±0.39\n57.50±0.40\n63.60±0.41\n73.95±0.36\n52.72±0.39\n64.53±0.36\nMAML\n56.57±0.43\n64.17±0.40\n41.17±0.40\n48.82±0.40\n60.05±0.42\n67.54±0.37\n47.33±0.41\n54.86±0.38\nProtoNets\n56.74±0.43\n65.03±0.41\n38.98±0.37\n47.98±0.38\n59.39±0.40\n67.77±0.36\n45.89±0.40\n54.29±0.38\n(b) Cross-domain few-shot benchmarks dissimilar to miniImageNet.\nCropDiseases\nEuroSAT\nISIC\nChestX\nMethod\n(5, 5)\n(5, 20)\n(5, 5)\n(5, 20)\n(5, 5)\n(5, 20)\n(5, 5)\n(5, 20)\nMeta-GMVAE\n73.56±0.53\n81.22±0.39\n73.83±0.42\n80.11±0.35\n33.48±0.30\n39.48±0.28\n23.23±0.23\n26.26±0.24\nMeta-SVEBM\n71.82±1.03\n83.13±0.78\n70.83±0.83\n80.21±0.73\n38.85±0.76\n48.43±0.81\n26.26±0.65\n28.91±0.69\nSimCLR\n79.90±0.39\n88.73±0.28\n79.14±0.38\n85.05±0.32\n42.83±0.29\n51.35±0.27\n25.14±0.23\n29.21±0.24\nMoCo v2\n80.96±0.37\n89.85±0.27\n79.94±0.37\n86.16±0.31\n43.43±0.30\n52.14±0.27\n25.24±0.23\n29.19±0.24\nSwAV\n80.15±0.39\n89.24±0.28\n79.31±0.39\n85.62±0.31\n43.21±0.30\n51.99±0.27\n24.99±0.23\n28.57±0.24\nPsCo (ours)\n88.24±0.31\n94.95±0.18\n81.08±0.35\n87.65±0.28\n44.00±0.30\n54.59±0.29\n24.78±0.23\n27.69±0.23\nMAML\n77.76±0.39\n83.24±0.34\n71.48±0.38\n76.70±0.33\n47.34±0.37\n55.09±0.34\n22.61±0.22\n24.25±0.22\nProtoNets\n76.01±0.40\n83.64±0.33\n64.91±0.38\n70.88±0.33\n40.62±0.31\n48.38±0.29\n23.15±0.22\n25.72±0.23\nTable 15: Few-shot classiﬁcation accuracy (%) on cross-domain few-shot classiﬁcation benchmarks\nof pretrained ResNet-18/50 on ImageNet with a 95% conﬁdence interval (5-way 5-shot).\nMethods\nCUB\nCars\nPlaces\nPlantae\nCropDiseases\nEuroSAT\nISIC\nChestX\nResNet-18 pretrained\nMoCo v2\n61.88±0.96\n46.42±0.73\n79.11±0.68\n56.24±0.72\n81.48±0.74\n75.98±0.73\n38.21±0.53\n24.34±0.36\n+PsCo (Ours)\n70.08±0.87\n50.73±0.76\n79.74±0.64\n61.55±0.76\n87.91±0.57\n79.92±0.64\n40.61±0.52\n25.03±0.42\nResNet-50 pretrained\nMoCo v2\n64.16±0.91\n47.67±0.75\n81.39±0.64\n61.36±0.79\n82.89±0.77\n76.96±0.68\n38.26±0.56\n24.28±0.39\n+PsCo (Ours)\n76.63±0.84\n53.45±0.76\n83.87±0.58\n69.17±0.70\n89.85±0.78\n83.99±0.52\n41.64±0.55\n23.60±0.36\nBYOL\n67.45±0.88\n45.74±0.76\n75.43±0.79\n56.86±0.84\n80.82±0.86\n77.70±0.71\n37.27±0.56\n24.15±0.36\n+PsCo (Ours)\n82.13±0.70\n56.19±0.76\n83.80±0.62\n71.14±0.71\n92.92±0.44\n85.33±0.54\n42.90±0.55\n26.05±0.46\nSupervised\n89.13±0.55\n75.15±0.75\n84.41±0.61\n72.91±0.73\n90.96±0.48\n85.64±0.52\n43.34±0.57\n25.35±0.41\n18\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV"
  ],
  "published": "2023-03-02",
  "updated": "2023-03-02"
}