{
  "id": "http://arxiv.org/abs/2303.01566v1",
  "title": "On the Provable Advantage of Unsupervised Pretraining",
  "authors": [
    "Jiawei Ge",
    "Shange Tang",
    "Jianqing Fan",
    "Chi Jin"
  ],
  "abstract": "Unsupervised pretraining, which learns a useful representation using a large\namount of unlabeled data to facilitate the learning of downstream tasks, is a\ncritical component of modern large-scale machine learning systems. Despite its\ntremendous empirical success, the rigorous theoretical understanding of why\nunsupervised pretraining generally helps remains rather limited -- most\nexisting results are restricted to particular methods or approaches for\nunsupervised pretraining with specialized structural assumptions. This paper\nstudies a generic framework, where the unsupervised representation learning\ntask is specified by an abstract class of latent variable models $\\Phi$ and the\ndownstream task is specified by a class of prediction functions $\\Psi$. We\nconsider a natural approach of using Maximum Likelihood Estimation (MLE) for\nunsupervised pretraining and Empirical Risk Minimization (ERM) for learning\ndownstream tasks. We prove that, under a mild ''informative'' condition, our\nalgorithm achieves an excess risk of\n$\\tilde{\\mathcal{O}}(\\sqrt{\\mathcal{C}_\\Phi/m} + \\sqrt{\\mathcal{C}_\\Psi/n})$\nfor downstream tasks, where $\\mathcal{C}_\\Phi, \\mathcal{C}_\\Psi$ are complexity\nmeasures of function classes $\\Phi, \\Psi$, and $m, n$ are the number of\nunlabeled and labeled data respectively. Comparing to the baseline of\n$\\tilde{\\mathcal{O}}(\\sqrt{\\mathcal{C}_{\\Phi \\circ \\Psi}/n})$ achieved by\nperforming supervised learning using only the labeled data, our result\nrigorously shows the benefit of unsupervised pretraining when $m \\gg n$ and\n$\\mathcal{C}_{\\Phi\\circ \\Psi} > \\mathcal{C}_\\Psi$. This paper further shows\nthat our generic framework covers a wide range of approaches for unsupervised\npretraining, including factor models, Gaussian mixture models, and contrastive\nlearning.",
  "text": "arXiv:2303.01566v1  [stat.ML]  2 Mar 2023\nOn the Provable Advantage of Unsupervised Pretraining\nJiawei Ge∗†\nShange Tang ∗†\nJianqing Fan†\nChi Jin‡\nAbstract\nUnsupervised pretraining, which learns a useful representation using a large amount of un-\nlabeled data to facilitate the learning of downstream tasks, is a critical component of modern\nlarge-scale machine learning systems.\nDespite its tremendous empirical success, the rigor-\nous theoretical understanding of why unsupervised pretraining generally helps remains rather\nlimited—most existing results are restricted to particular methods or approaches for unsu-\npervised pretraining with specialized structural assumptions.\nThis paper studies a generic\nframework, where the unsupervised representation learning task is speciﬁed by an abstract\nclass of latent variable models Φ and the downstream task is speciﬁed by a class of prediction\nfunctions Ψ. We consider a natural approach of using Maximum Likelihood Estimation (MLE)\nfor unsupervised pretraining and Empirical Risk Minimization (ERM) for learning downstream\ntasks. We prove that, under a mild “informative” condition, our algorithm achieves an excess\nrisk of ˜O(\np\nCΦ/m +\np\nCΨ/n) for downstream tasks, where CΦ, CΨ are complexity measures of\nfunction classes Φ, Ψ, and m, n are the number of unlabeled and labeled data respectively.\nComparing to the baseline of ˜O(\np\nCΦ◦Ψ/n) achieved by performing supervised learning using\nonly the labeled data, our result rigorously shows the beneﬁt of unsupervised pretraining when\nm ≫n and CΦ◦Ψ > CΨ. This paper further shows that our generic framework covers a wide\nrange of approaches for unsupervised pretraining, including factor models, Gaussian mixture\nmodels, and contrastive learning.\n1\nIntroduction\nUnsupervised pretraining aims to eﬃciently use a large amount of unlabeled data to learn a useful\nrepresentation that facilitates the learning of downstream tasks. This technique has been widely\nused in modern machine learning systems including computer vision (Caron et al., 2019; Dai et al.,\n2021), natural language processing (Radford et al., 2018; Devlin et al., 2018; Song et al., 2019) and\nspeech processing (Schneider et al., 2019; Baevski et al., 2020). Despite its tremendous empirical\nsuccess, it remains elusive why pretrained representations, which are learned without the informa-\ntion of downstream tasks, often help to learn the downstream tasks.\nThere have been several recent eﬀorts trying to understand various approaches of unsuper-\nvised pretraining from theoretical perspectives, including language models Saunshi et al. (2020);\nWei et al. (2021), contrastive learning Arora et al. (2019); Tosh et al. (2021b,a); HaoChen et al.\n(2021); Saunshi et al. (2022), and reconstruction-based self-supervised learning Lee et al. (2021).\n∗equal contribution\n†Department of Operations Research and Financial Engineering, Princeton University; {jg5300,shangetang,\njqfan}@princeton.edu\n‡Department of Electrical and Computer Engineering, Princeton University; chij@princeton.edu\n1\nWhile this line of works justiﬁes the use of unsupervised pretraining in the corresponding regimes,\nmany of them do not prove the advantage of unsupervised learning, in terms of sample complexity,\neven when compared to the naive baseline of performing supervised learning purely using the labeled\ndata. Furthermore, these results only apply to particular approaches of unsupervised pretraining\nconsidered in their papers, and crucially rely on the specialized structural assumptions, which do\nnot generalize beyond the settings they studied. Thus, we raise the following question:\nCan we develop a generic framework which provably explains the advantage of\nunsupervised pretraining?\nThis paper answers the above question positively.\nWe consider the generic setup where the data x and its label y are connected by an unobserved\nrepresentation z. Concretely, we assume (x, z) is sampled from a latent variable model φ∗in an\nabstract class Φ, and the distribution of label y conditioned on representation z is drawn from\ndistributions ψ∗in class Ψ. We considers a natural approach of using Maximum Likelihood Esti-\nmation (MLE) for unsupervised pretraining, which approximately learns the latent variable model\nφ∗using m unlabeled data. We then use the results of representation learning and Empirical Risk\nMinimization (ERM) to learn the downstream predictor ψ∗using n labeled data. Investigating this\ngeneric setup allows us to bypass the limitation of prior works that are restrictied to the speciﬁc\napproaches for unsupervised pretraining.\nWe prove that, under a mild “informative” condition (Assumption 3.2), our algorithm achieves\na excess risk of ˜O(\np\nCΦ/m +\np\nCΨ/n) for downstream tasks, where CΦ, CΨ are complexity measures\nof function classes Φ, Ψ, and m, n are the number of unlabeled and labeled data respectively.\nComparing to the baseline of ˜O(\np\nCΦ◦Ψ/n) achieved by performing supervised learning using only\nthe labeled data, our result rigorously shows the beneﬁt of unsupervised pretraining when we have\nabundant unlabeled data m ≫n and when the complexity of composite class CΦ◦Ψ is much greater\nthan the complexity of downstream task alone CΨ.\nFinally, this paper proves that our generic framework (including the “informative” condition)\ncaptures a wide range of setups for unsupervised pretraining, including (1) factor models with linear\nregression as downstream tasks; (2) Gaussian mixture models with classiﬁcation as downstream\ntasks; and (3) Contrastive learning with linear regression as downstream tasks.\n1.1\nRelated work\nApplications and methods for unsupervised pretraining.\nUnsupervised pretraining has\nachieved tremendous success in image recognition (Caron et al., 2019), objective detection (Dai et al.,\n2021), natural language processing (Devlin et al., 2018; Radford et al., 2018; Song et al., 2019)\nand speech recognition (Schneider et al., 2019; Baevski et al., 2020). Two most widely-used pre-\ntraining approaches are (1) feature-based approaches (Brown et al., 1992; Mikolov et al., 2013;\nMelamud et al., 2016; Peter et al., 2018), which pretrains a model to extract representations and\ndirectly uses the pretrained representations as inputs for the downstream tasks; (2) ﬁne-tuning\nbased approaches, (see, e.g., Devlin et al., 2018), which ﬁne-tunes all the model parameters in the\nneighborhood of pretrained representations based on downstream tasks. Erhan et al. (2010) pro-\nvides the ﬁrst empirical understanding on the role of pretraining. They argue that pretraining\nserves as a form of regularization that eﬀectively guides the learning of downstream tasks.\nA majority of settings where pretraining is used fall into the category of semi-supervised learn-\ning (see, e.g., Zhu, 2005), where a large amount of unlabeled data and a small amount of labeled\ndata are observed during the training process.\nSemi-supervised learning methods aim to build\n2\na better predictor by eﬃciently utilizing the unlabeled data. Some traditional methods include:\ngenerative models (e.g. Ratsaby & Venkatesh, 1995), low-density separation (Joachims et al., 1999;\nLawrence & Jordan, 2004; Szummer & Jaakkola, 2002), and graph-based methods (Belkin et al.,\n2006).\nWhile most works in this line propose new methods and show favorable empirical per-\nformance, they do not provide rigorous theoretical understanding on the beneﬁt of unsupervised\npretraining.\nTheoretical understanding of unsupervised pretraining.\nRecent years witness a surge of\ntheoretical results that provide explanations for various unsupervised pretraining methods that ex-\ntract representations from unlabeled data. For example, (Saunshi et al., 2020; Wei et al., 2021) con-\nsiders pretraining vector embeddings in the language models, while (Arora et al., 2019; Tosh et al.,\n2021b,a; HaoChen et al., 2021; Saunshi et al., 2022; Lee et al., 2021) consider several Self-Supervised\nLearning (SSL) approaches for pretraining. In terms of results, Wei et al. (2021) shows that lin-\near predictor on the top of pretrained languange model can recover their ground truth model;\nArora et al. (2019); Saunshi et al. (2020); Tosh et al. (2021b,a); Saunshi et al. (2022) show that\nthe prediction loss of downstream task can be bounded by the loss of unsupervised pretraining\ntasks. These two lines of results do not prove the sample complexity advantage of unsupervised\nlearning when compared to the baseline of performing supervised learning purely using the labeled\ndata.\nThe most related results are Lee et al. (2021); HaoChen et al. (2021), which explicitly show\nthe sample complexity advantage of certain unsupervised pretraining methods. However, Lee et al.\n(2021) focuses on reconstruction-based SSL, and critically relies on a conditional independency\nassumption on the feature and its reconstruction conditioned on the label; HaoChen et al. (2021)\nconsiders contrastive learning, and their results relies on deterministic feature map and the spectral\nconditions of the normalized adjacency matrix. Both results only apply to the speciﬁc setups and\napproaches of unsupervised pretraining in their papers, which do not apply to other setups in\ngeneral (for instance, the three examples in Section 4, 5, 6). On the contrary, this paper develops a\ngeneric framework for unsupervised pretraining using only abstract function classes, which applies\nto a wide range of setups.\nOther approaches for representation learning.\nThere is another line of recent theoreti-\ncal works that learn representation via multitask learning. Baxter (2000) provides generalization\nbounds for multitask transfer learning assuming a generative model and a shared representation\namong tasks. Maurer et al. (2016) theoretically analyses a general method for learning represen-\ntations from multitasks and illustrates their method in a linear feature setting. Tripuraneni et al.\n(2021); Du et al. (2020) provide sample eﬃcient algorithms that solve the problem of multitask lin-\near regression. Tripuraneni et al. (2020) further considers generic nonlinear feature representations\nand shows sample complexity guarantees for diverse training tasks. Their results diﬀer from our\nwork because they learn representations by supervised learning using labeled data of other tasks,\nwhile our work learns representations by unsupervised learning using unlabeled data.\n2\nProblem Setup\nNotation.\nWe denote by P(x) and p(x) the cumulative distribution function and the probability\ndensity function deﬁned on x ∈X, respectively. We deﬁne [n] = {1, 2, . . ., n}. The cardinality of\n3\nset A is denoted by |A|. Let ∥· ∥2 be the ℓ2 norm of a vector or the spectral norm of a matrix. We\ndenote by ∥· ∥F the Frobenius norm of a matrix. For a matrix M ∈Rm×n, we denote by σmin(M)\nand σmax(M) the smallest singular value and the largest singular value of M, respectively. For two\nprobability distributions P1 and P2, we denote the Total Variation (TV) distance and the Hellinger\ndistance between these two distributions by dTV(P1, P2) and H(P1, P2), respectively.\nWe denote by x ∈X and y ∈Y the input data and the objective of the downstream tasks,\nrespectively. Our goal is to predict y using x. We assume that x is connected to y through an\nunobserved latent variable z ∈Z (which is also considered as a representation of x). Given the\nlatent variable z, the data x and the objective y are independent of each other. To incorporate a large\nclass of real-world applications, such as contrastive learning, we consider the setup where learning\ncan possibly have access to some side information s ∈S. We assume that (x, s, z) ∼Pφ∗(x, s, z)\nand y|z ∼Pψ∗(y|z), where Pφ∗and Pψ∗are distributions indexed by φ∗∈Φ and ψ∗∈Ψ. It then\nholds that\nPφ∗,ψ∗(x, y) =\nZ\nPφ∗(x, z)Pψ∗(y|z) dz,\nwhich implies the probability distribution of (x, y) depends on both φ∗and ψ∗. Our setting includes\nthe special case where y = f ∗(z) + ε. Function f ∗∈F is the ground truth function and ε ∼\nN(0, σ2) is a Gaussian noise independent of z.\nIn this case, the conditional random variable\ny|z ∼N(f ∗(z), σ2), whose probability distribution Pf ∗(y|z) is parameterized by f ∗∈F.\nLet ℓ(·, ·) be a loss function. For any pair (φ, ψ) ∈Φ × Ψ, the optimal predictor gφ,ψ is deﬁned\nas follows,\ngφ,ψ ←arg min\ng\nEPφ,ψ\n\u0002\nℓ\n\u0000g(x), y\n\u0001\u0003\n,\n(1)\nwhere the minimum is taken on all the possible functions and EPφ,ψ := E(x,y)∼Pφ,ψ(x,y).\nOur\nprediction function class is therefore given by\nGΦ,Ψ :=\n\b\ngφ,ψ\n\f\fφ ∈Φ, ψ ∈Ψ\n\t\n.\nIn particular, if ℓ(·, ·) is the squared loss function, then the optimal predictor has a closed form\nsolution gφ,ψ(x) = EPφ,ψ[y|x] and the prediction function class GΦ,Ψ = {EPφ,ψ[y|x] | φ ∈Φ, ψ ∈Ψ}.\nGiven an estimator pair (ˆφ, ˆψ), we deﬁne the excess risk with respect to loss ℓ(·, ·) as\nErrorℓ(ˆφ, ˆψ) := EPφ∗,ψ∗\n\u0002\nℓ\n\u0000g ˆφ, ˆψ(x), y\n\u0001\u0003\n−EPφ∗,ψ∗\n\u0002\nℓ\n\u0000gφ∗,ψ∗(x), y\n\u0001\u0003\n,\n(2)\nwhere φ∗and ψ∗are the ground truth parameters. By the deﬁnition of gφ∗,ψ∗, we have Error(ˆφ, ˆψ) ≥\n0. We aim to learn an estimator pair (ˆφ, ˆψ) from data that achieves smallest order of the excess\nrisk.\nWe consider the setting where the latent variable z cannot be observed. Speciﬁcally, we are\ngiven many unlabeled data and its corresponding side information {xi, si}m\ni=1 that are sampled i.i.d\nfrom an unknown distribution Pφ∗(x, s) and only a few labeled data {xj, yj}n\nj=1 that are sampled\ni.i.d from an unknown distribution Pφ∗,ψ∗(x, y). Here we assume that the labeled data {xj, yj}n\nj=1\nis independent with the unlabeled data {xi, si}m\ni=1 with understanding m ≫n.\n4\nAlgorithm 1 Two-Phase MLE+ERM\n1: Input: {xi, si}m\ni=1, {xj, yj}n\nj=1\n2: Use unlabeled data and its corresponding side information {xi, si}m\ni=1 to learn ˆφ via MLE:\nˆφ ←arg max\nφ∈Φ\nm\nX\ni=1\nlog pφ(xi, si).\n(3)\n3: Fix ˆφ and use labeled data {xj, yj}n\nj=1 to learn ˆψ via ERM:\nˆψ ←arg min\nψ∈Ψ\nn\nX\nj=1\nℓ\n\u0000g ˆφ,ψ(xj), yj\n\u0001\n.\n(4)\n4: Output: ˆφ and ˆψ.\nLearning algorithm.\nWe consider a natural learning algorithm consisting of two phases (Algo-\nrithm 1). In the unsupervised pretraining phase, we use MLE to estimate φ∗based on the unlabeled\ndata {xi, si}m\ni=1. In the downstream tasks learning phase, we use ERM to estimate ψ∗based on\npretrained ˆφ and the labeled data {xj, yj}n\nj=1. See algorithm 1 for details.\nWe remark that another natural learning algorithm in our setting is to use a two-phase MLE.\nTo be speciﬁc, in the unsupervised pretraining phase, we use MLE to estimate φ∗based on the\nunlabeled data {xi, si}m\ni=1 as (3). In the downstream tasks learning phase, we again use MLE to\nestimate ψ∗based on pretrained ˆφ and the labeled data {xj, yj}n\nj=1. However, we can show that\nthis two-phase MLE scheme fails in the worst case. See Appendix E for the details.\nComplexity measures.\nSample complexity guarantee for Algorithm 1 will be phrased in terms\nof three complexity measurements, i.e., bracketing number, covering number and the Rademacher\ncomplexity, which are deﬁned as follows.\nWe denote by PX (Φ) a set of parameterized density\nfunctions pφ(x) deﬁned on x ∈X\nPX (Φ) := {pφ(x) | φ ∈Φ},\nwhere φ ∈Φ is the parameter.\nDeﬁnition 2.1 (ǫ-Bracket and Bracketing Number). Let ǫ > 0. Under ∥· ∥1 distance, a set of\nfunctions N[ ](PX (Φ), ǫ) is an ǫ-bracket of PX (Φ) if for any pφ(x) ∈PX (Φ), there exists a function\n¯pφ(x) ∈N[ ](PX (Φ), ǫ) such that the following two properties hold:\n• ¯pφ(x) ≥pφ(x), ∀x ∈X\n• ∥¯pφ(x) −pφ(x)∥1 = R |¯pφ(x) −pφ(x)| dx ≤ǫ\nNote that ¯pφ(x) need not to belong to PX (Φ). The bracketing number N[ ](PX (Φ), ǫ) is the cardi-\nnality of the smallest ǫ-bracket needed to cover PX (Φ). The entropy is deﬁned as the logarithm of\nthe bracketing number.\nTo measure the complexity of a function class, we consider the covering number and the\nRademacher complexity deﬁned as follows.\n5\nDeﬁnition 2.2 (ǫ-Cover and Covering Number). Let F be a function class and (F, ∥·∥) be a metric\nspace. For each ǫ > 0, a set of functions N(F, ǫ, ∥· ∥) is called an ǫ-cover of F if for any f ∈F,\nthere exists a function g ∈N(F, ǫ, ∥· ∥) such that ∥f −g∥≤ǫ. The covering number N(F, ǫ, ∥· ∥)\nis deﬁned as the cardinality of the smallest ǫ-cover needed to cover F.\nDeﬁnition 2.3 (Rademacher Complexity). Suppose that x1, . . . , xn are sampled i.i.d from a prob-\nability distribution D deﬁned on a set X. Let G be a class of functions mapping from X to R. The\nempirical Rademacher complexity of G is deﬁned as follows,\nˆRn(G) := E{σi}n\ni=1∼Unif{±1}\n\u0014\nsup\ng∈G\n2\nn\nn\nX\ni=1\nσig(xi)\n\u0015\n,\nwhere {σi}n\ni=1 are independent random variables drawn from the Rademacher distribution and the\nexpectation is taken over the randomness of {σi}n\ni=1. The Rademacher complexity of G is deﬁned\nas\nRn(G) := E{xi}n\ni=1∼D[ ˆRn(G)].\n3\nMain Results\nIn this section, we ﬁrst introduce a mild “informative” condition for unsupervised pretraining. We\nshow this “informative” condition is necessary for pretraining to beneﬁt downstream tasks. We\nthen provide our main results—statistical guarantees for unsupervised pretraining and downstream\ntasks for Algorithm 1. Finally, in Section 3.1, we generalize our results to a more technical but\nweaker version of the “informative” condition, which turns out to be useful in capturing our third\nexample of contrastive learning (Section 6).\nInformative pretraining tasks.\nWe ﬁrst note that under our generic setup, unsupervised pre-\ntraining may not beneﬁt downstream tasks at all in the worst case if no further conditions are\nassumed.\nProposition 3.1. There exist classes (Φ, Ψ) as in Section 2 such that, regardless of unsupervised\npretraining algorithms used, pretraining using unlabeled data provides no additional information\ntowards learning predictor gφ∗,ψ∗.\nConsider the latent variable model z = Ax, where x ∼N(0, Id), A ∈Φ is the parameter of the\nmodel. Then, no matter how many unlabeled {xi} we have, we can gain no information of A from\nthe data! In this case, unsupervised pretraining is not beneﬁcial for any downstream task.\nTherefore, it’s crucial to give an assumption that guarantees our unsupervised pretraining is\ninformative.\nAs a thought experiment, suppose that in the pretraining step, we ﬁnd an exact\ndensity estimator ˆφ for the marginal distribution of x, s , i.e., p ˆφ(x, s) = pφ∗(x, s) holds for every\nx, s. We should expect that this estimator also fully reveals the relationship between x and z,\ni.e., p ˆφ(x, z) = pφ∗(x, z) holds for every x, z. Unfortunately, this condition does not hold in most\npractical setups and is often too strong. As an example, consider Gaussian mixture models, where\nz ∈[K] is the cluster that data point x ∈Rd belongs to. Then in this case, it is impossible for\nus to ensure p ˆφ(x, z) = pφ∗(x, z), since a permutation of z makes no diﬀerence in the marginal\ndistribution of x. However, notice that in many circumstances, a permutation of the class label will\n6\nnot aﬀect the downstream task learning. In these cases, a permutation of the clusters is allowed.\nMotivated by this observation, we introduce the following informative assumption which allows\ncertain “transformation” induced by the downstream task:\nAssumption 3.2 (κ−1-informative condition). We assume that the model class Φ is κ−1-informative\nwith respect to a transformation group TΦ. That is, for any φ ∈Φ, there exists T1 ∈TΦ such that\ndTV\n\u0000PT1◦φ(x, z), Pφ∗(x, z)\n\u0001\n≤κ · dTV\n\u0000Pφ(x, s), Pφ∗(x, s)\n\u0001\n.\n(5)\nHere φ∗is the ground truth parameter. Furthermore, we assume that TΦ is induced by transfor-\nmation group TΨ on Ψ, i.e., for any T1 ∈TΦ, there exists T2 ∈TΨ such that for any (φ, ψ) ∈Φ × Ψ,\nPφ,ψ(x, y) = PT1◦φ,T2◦ψ(x, y).\n(6)\nUnder Assumption 3.2, if the pretrained ˆφ accurately estimates the marginal distribution of x, s\nup to high accuracy, then it also reveals the correct relation between x and representation z up to\nsome transformation TΦ which is allowed by the downstream task, which makes it possible to learn\nthe downstream task using less labeled data.\nProposition 3.1 shows that the informative condition is necessary for pretraining to bring advan-\ntage since the counter example in the proposition is precisely 0-informative. We will also show this\ninformative condition is rich enough to capture a wide range of unsupervised pretraining methods in\nSection 4, 5, 6, including factor models, Gaussian mixture models, and contrastive learning models.\nGuarantees for unsupervised pretraining.\nRecall that PX ×S(Φ) := {pφ(x, s) | φ ∈Φ}. We\nhave the following guarantee for the MLE step (line 2) of Algorithm 1.\nTheorem 3.3. Let ˆφ be the maximizer deﬁned in (3). Then, with probability at least 1 −δ, we\nhave\ndTV\n\u0000P ˆφ(x, s), Pφ∗(x, s)\n\u0001\n≤3\ns\n1\nm log N[ ](PX ×S(Φ), 1\nm)\nδ\n,\nwhere N[ ] is the bracketing number as in Deﬁnition 2.1.\nTheorem 3.3 claims that the TV error in estimating the joint distribution of (x, s) decreases as\nO(CΦ/m) where m is the number of unlabeled data, and CΦ = log N[ ](PX ×S(Φ), 1/m) measures\nthe complexity of learning the latent variable models Φ. This result mostly follows from standard\nanalysis of MLE (Van de Geer, 2000). We include the proof in Appendix A.1 for completeness. If\nthe model is κ−1-informative, Theorem 3.3 further implies that with probability at least 1 −δ,\nmin\nψ EPφ∗,ψ∗\n\u0002\nℓ\n\u0000g ˆφ,ψ(x), y\n\u0001\u0003\n−EPφ∗,ψ∗\n\u0002\nℓ\n\u0000gφ∗,ψ∗(x), y\n\u0001\u0003\n≤12κL\nr\n1\nm log N[ ](PX ×S(Φ), 1/m)\nδ\n.\nSee Lemma A.2 for the details.\nThis inequality claims that if we learn a perfect downstream\npredictor using the estimated representation ˆφ, excess risk is small.\n7\nGuarantees for downstream task learning.\nIn practice, we can only learn an approximate\ndownstream predictor using a small amount of labeled data. We upper bound the excess risk of\nAlgorithm 1 as follows.\nTheorem 3.4. Let ˆφ and ˆψ be the outputs of Algorithm 1. Suppose that the loss function ℓ:\nY × Y →R is L-bounded and our model is κ−1-informative. Then, with probability at least 1 −δ,\nthe excess risk of Algorithm 1 is bounded as:\nErrorℓ(ˆφ, ˆψ) ≤2 max\nφ∈Φ Rn(ℓ◦Gφ,Ψ) + 12κL ·\nr\n1\nm log 2N[ ](PX ×S(Φ), 1/m)\nδ\n+ 2L ·\nr\n2\nn log 4\nδ .\nHere Rn(·) denotes the Rademacher complexity, and\nℓ◦Gφ,Ψ :=\n\b\nℓ\n\u0000gφ,ψ(x), y\n\u0001\n: X × Y →[−L, L]\n\f\fψ ∈Ψ\n\t\n.\nNote that the Rademacher complexity of a function class can be bounded by its metric entropy.\nWe then have the following corollary.\nCorollary 3.5. Under the same preconditions as Theorem 3.4, we have:\nErrorℓ(ˆφ, ˆψ) ≤˜c max\nφ∈Φ L\nr\nlog N(ℓ◦Gφ,Ψ, L/√n, ∥· ∥∞)\nn\n+ 2L\nr\n2\nn log 4\nδ\n+ 12κL\nr\n1\nm log 2N[ ](PX ×S(Φ), 1/m)\nδ\n,\nwhere ˜c is an absolute constant, N(F, δ, ∥· ∥∞) is the δ−covering number of function class F with\nrespect to the metric ∥· ∥∞.\nBy Corollary 3.5, the excess risk of our Algorithm 1 is approximately ˜O(\np\nCΦ/m +\np\nCΨ/n),\nwhere CΦ and CΨ are roughly the log bracketing number of class Φ and the log covering number\nof Ψ. Note that excess risk for the baseline algorithm that learns downstream task using only\nlabeled data is ˜O(\np\nCΦ◦Ψ/n), where CΦ◦Ψ is the log covering number of composite function class\nΦ ◦Ψ. In many practical scenarios such as training a linear predictor on top of a pretrained deep\nneural networks, the complexity CΦ◦Ψ is much larger than CΨ. We also often have signiﬁcantly\nmore unlabeled data than labeled data (m ≫n). In these scenarios, our result rigorously shows\nthe signiﬁcant advantage of unsupervised pretraining compared to the baseline algorithm which\ndirectly performs supervised learning without using unlabeled data.\n3.1\nGuarantees for weakly informative models\nWe introduce a relaxed version of Assumption 3.2, which allows us to capture a richer class of\nexamples.\nAssumption 3.6 (κ−1-weakly-informative condition). We assume model (Φ, Ψ) is κ−1-weakly-\ninformative, that is, for any φ ∈Φ, there exists ψ ∈Ψ such that\ndTV\n\u0000Pφ,ψ(x, y), Pφ∗,ψ∗(x, y)\n\u0001\n≤κ · H\n\u0000Pφ(x, s), Pφ∗(x, s)\n\u0001\n.\n(7)\nHere we denote by φ∗, ψ∗the ground truth parameters.\n8\nAssumption 3.6 relaxes Assumption 3.2 by making two modiﬁcations: (i) replace the LHS of\n(5) by the TV distance between the joint distribution of (x, y); (ii) replace the TV distance on the\nRHS by the Hellinger distance. See more on the relation of two assumptions in Appendix A.4.1.\nIn fact, Assumption 3.6 is suﬃcient for us to achieve the same theoretical guarantee as that in\nTheorem 3.4.\nTheorem 3.7. Theorem 3.4 still holds under the κ−1-weakly-informative assumptions.\nThe proof of Theorem 3.7 requires a stronger version of MLE guarantee than Theorem 3.3,\nwhich guarantees the closeness in terms of Hellinger distance. We leave the details in Appendix\nA.4.\n4\nPretraining via Factor Models\nHigh-dimensional data is very common in modern statistics and machine learning, and we often\nsuﬀer from the curse of dimensionality when directly analyzing data in high-dimensional spaces.\nTo tackle the problem, we usually assume that high-dimensional data has some low-dimensional\nstructures. One of the widely studied models in this setting is the factor model, which models the\nhigh-dimensional measurements by low-rank plus sparse structures in data matrices to decorrelate\nthe covariates. Learning this latent structure falls into the framework of unsupervised statistical\nlearning. In this section, we instantiate our theoretical framework using the factor model with\nlinear regression as a downstream task. We rigorously show how unsupervised pretraining can help\nreduce sample complexity in this case.\nModel Setup.\nFactor model (see, e.g., Lawley & Maxwell, 1971; Bai & Ng, 2002; Forni et al.,\n2005; Fan et al., 2021) is widely used in ﬁnance, computational biology, and sociology, where the\nhigh-dimensional measurements are strongly correlated. For the latent variable model, we consider\nthe factor model with standard Gaussian components, which is deﬁned as follows.\nDeﬁnition 4.1 (Factor Model). Suppose that we have d-dimensional random vector x, whose\ndependence is driven by r factors z. The factor model assumes\nx = B∗z + µ,\nwhere B∗is a d × r factor loading matrix. Here µ ∼N(0, Id) is the idiosyncratic component that is\nuncorrelated with the common factor z ∼N(0, Ir). We assume that the ground truth parameters\nB∗∈B, where B := {B ∈Rd×r | ∥B∥2 ≤D} for some D > 0.\nFor the downstream task, we assume that the latent factors z inﬂuence on the response y in a\nsimilar manner as on x and hence consider the following linear regression problem\ny = β∗T z + ν,\nwhere ν ∼N(0, ε2) is a Gaussian noise that is uncorrelated with the factor z and the idiosyncratic\ncomponent µ. We assume that the ground truth parameters β∗∈C, where C := {β ∈Rr | ∥β∥2 ≤\nD} for some D > 0. The latent variable model (i.e., Φ) and the the prediction class (i.e.,Ψ) are then\nrepresented by B and C, respectively. In the sequel, we consider the case where no side information is\navailable, i.e., we only have access to i.i.d unlabeled data {xi}m\ni=1 and i.i.d labeled data {xj, yj}n\nj=1.\n9\nFor regression models, it is natural to consider the squared loss function ℓ(x, y) := (y −x)2.\nThen, the optimal predictor gB,β under the distribution PB,β has the following closed form solution,\ngB,β(x) = EPB,β[y | x] = βT BT (BBT + Id)−1x.\nAnd the excess risk is now deﬁned as\nErrorℓ( ˆB, ˆβ) := EPB∗,β∗\n\u0002\u0000y −g ˆ\nB, ˆβ(x)\n\u00012\u0003\n−EPB∗,β∗\n\u0002\u0000y −gB∗,β∗(x)\n\u00012\u0003\n.\nInformative condition.\nWe ﬁrst show that Assumption 3.2 holds for the factor model with\nlinear regression as downstream tasks. The idea of the factor model is to learn a low-dimensional\nrepresentation z, where a rotation over z is allowed since in the downstream task, we can also rotate\nβ to adapt to the rotated z.\nLemma 4.2. Factor model with linear regression as downstream tasks is κ−1-informative, where\nκ = c1(σ∗\nmax + 1)4\n(σ∗\nmin)3\n.\nHere c1 is some absolute constants, σ∗\nmax and σ∗\nmin are the largest and smallest singular value of\nB∗, respectively.\nTheoretical results.\nRecall that in Theorem 3.4, we assume a L-bounded loss function to guar-\nantee the performance of Algorithm 1. Thus, instead of directly applying Algorithm 1 to the squared\nloss function, we consider Algorithm 1 with truncated squared loss, i.e.,\n˜ℓ(x, y) := (y −x)2 · 1{(y−x)2≤L} + L · 1{(y−x)2>L}.\n(8)\nHere L is a carefully chosen truncation level. To be more speciﬁc, in the ﬁrst phase, we still use\nMLE to learn an estimator ˆB as that in line 2 of Algorithm 1. In the second phase, we apply ERM\nto the truncated squared loss to learn an estimator ˆβ, i.e.,\nˆβ ←arg min\nβ∈C\nn\nX\nj=1\n˜ℓ\n\u0000g ˆ\nB,β(xj), yj\n\u0001\n.\nWe then have the following theoretical guarantee.\nTheorem 4.3. We consider Algorithm 1 with truncated squared loss (8) with L = (D2 + 1)3 log n.\nLet ˆB, ˆβ be the outputs of Algorithm 1. Then, for factor models with linear regression as downstream\ntasks, with probability at least 1 −δ, the excess risk can be bounded as follows,\nErrorℓ( ˆB, ˆβ) ≤˜O\n\u0012\nκL\nr\ndr\nm + L\nr r\nn\n\u0013\n,\nwhere D is deﬁned in the sets B and C, and κ is speciﬁed in Lemma 4.2. Here ˜O(·) omits absolute\nconstants and the polylogarithmic factors in m, d, r, D, 1/δ.\n10\nNotice that the rate we obtain in Theorem 4.3 is not optimal for this speciﬁc task: by the nature\nof squared loss, if we consider a direct d−dimensional linear regression (from x to y) with n data, we\ncan usually achieve the fast rate, where excess risk decreases as ˜O(d/n). To ﬁll this gap, we consider\nAlgorithm 1 with Φ = Rd×r and Ψ = Rr and denote D := max{∥B∗∥2, ∥β∗∥2}. Following a more\nreﬁned analysis other than using a uniform concentration technique (which is suitable for general\nproblems but not optimal in this speciﬁc task), we achieve the following theoretical guarantee with\na sharper risk rate:\nTheorem 4.4 (Fast rate). Let ˆB, ˆβ be the outputs of Algorithm 1. Then, if m ≳(D2+1)2d log(1/δ),\nn ≳(D2 + 1)2r log(1/δ), for factor models with linear regression as downstream tasks, with proba-\nbility at least 1 −δ, the excess risk can be bounded as follows,\nErrorℓ( ˆB, ˆβ) ≤O\n\u0012\n(D2 + 1)6(D4 + σ∗−4\nmin )d log(1/δ)\nm\n+ (D2 + 1)2 r log(4/δ)\nn\n\u0013\n.\nHere O(·) omits some absolute constants.\nTheorem 4.4 shows the beneﬁt of unsupervised pretraining in the following sense. Assuming D\nand σ∗\nmin are both constants. The price paid for learning the loading matrix is ˜O(d/m), which is\nsmall when m is very large. Notice that, since x is a d-dimensional vector, the usual linear regression\nwill have a risk of ˜O(d/n). In the risk bound provided by Theorem 4.4, the risk related to n scales\nas ˜O(r/n). Usually, the factor is assumed to be low-dimensional compared with the input (d ≫r).\nThen when m ≫n, the risk bound ˜O(d/m + r/n) is much better than ˜O(d/n).\n5\nPretraining via Gaussian Mixture Models\nIn this section, we show how pretraining using Gaussian Mixture Models (GMMs) can beneﬁt the\ndownstream classiﬁcation tasks, under our theoretical framework.\nModel setup.\nFor the latent variable model, we consider a d-dimensional GMM with K compo-\nnents and equal weights. To be speciﬁc, the latent variable z that represents the cluster is sampled\nuniformly from [K]. In each cluster, the data is sampled from a standard Gaussian distribution,\ni.e., x|z = i ∼N(u∗\ni , Id) for any i ∈[K]. It then holds that\nx ∼\nK\nX\ni=1\n1\nK N(u∗\ni , Id).\nWe denote by U the parameter space with each element consisting of K centers (d-dimensional\nvectors).\nWe assume that the set of parameters U satisﬁes the normalization condition—there exists\nD > 0 such that for any u = {ui}K\ni=1 ∈U, we have ∥ui∥2 ≤D√d log K, ∀i ∈[K]. We further\nassume the ground-truth centers {u∗\ni }K\ni=1 ∈U satisfy the following separation condition.\nAssumption 5.1 (Separation condition). The true parameters {u∗\ni }K\ni=1 ∈U satisﬁes\n∥u∗\ni −u∗\nj∥2 ≥100\np\nd log K, ∀i ̸= j.\n11\nFor the downstream task, we consider the binary classiﬁcation problems with label y ∈{0, 1}.\nWe denote by Ψ the set of 2K classiﬁers such that for each ψ ∈Ψ, and any i ∈[K], we have either\nPψ(y = 1|z = i) = 1 −ε or Pψ(y = 0|z = i) = 1 −ε, where ε represents the noise. Then, the latent\nvariable model and the prediction class are represented by U and Ψ, respectively. In the sequel, we\nconsider the case where no side information is available, i.e., we only have access to i.i.d unlabeled\ndata {xi}m\ni=1 and i.i.d labeled data {xj, yj}n\nj=1. For classiﬁcation problems, it is natural to consider\nthe 0 −1 loss function ℓ(x, y) := 1{x̸=y} which is bounded by 1.\nInformative condition.\nWe prove that Assumption 3.2 for the above model.\nWe have the\nfollowing guarantee.\nLemma 5.2. Let ˜U = {u ∈U | dTV(pu(x), pu∗(x)) ≤1/(4K)}. Under Assumption 5.1, GMMs with\nparameters in ˜U is O(1)-informative with respect to the transformation group induced by downstream\nclassiﬁcation tasks.\nTheoretical results\nWe have the following theoretical guarantee.\nTheorem 5.3. Let ˆu, ˆψ be the outputs of Algorithm 1. Suppose that Assumption 5.1 holds and\nm = ˜Ω(dK3). Then, for the Gaussian mixture model with classiﬁcation as downstream tasks, with\nprobability at least 1 −δ, the excess risk can be bounded as follows,\nErrorℓ(ˆu, ˆψ) ≤˜O\n\u0012r\ndK\nm +\nr\nK\nn\n\u0013\n,\nHere ˜O(·) omits some constants and the polylogarithmic factors in m, d, K, D, 1/δ.\nTheorem 5.3 shows the power of unsupervised pretraining under this setting in the following\nsense: Note that the number of parameters of a GMM is dK, therefore if we directly do classiﬁcation\nwithout unsupervised pretraining, the risk will scale as ˜O(\np\ndK/n). When d is large and m ≫n,\nwe achieve a better risk bound than supervised learning that only uses the labeled data.\n6\nPretraining via Contrastive Learning\nFor human beings, when given many pictures of diﬀerent animals, we are able to infer which\npictures show the same animals even if we do not have any prior knowledge about the animals. In\nthis process, we inadvertently learn a representation for each picture that can be used to capture\nthe similarity between diﬀerent pictures. Contrastive learning mimics the way human learns. To\nbe more speciﬁc, based on positive and negative pairs, contrastive learning learns to embed data\ninto some space where similar sample pairs stay close to each other and dissimilar ones are far\napart. In this section, we show how pretraining (learning the embedding function) can beneﬁt the\ndownstream linear regression tasks under our theoretical framework.\nModel setup.\nIn the setting of contrastive learning, we assume that x and x′ are sampled in-\ndependently from the same distribution P(x). The similarity between x and x′ is captured by a\n12\nrepresentation function fθ∗: X →Rr in the following sense,\nP(t = 1 | x, x′) =\n1\n1 + e−fθ∗(x)T fθ∗(x′) ,\nP(t = −1 | x, x′) =\n1\n1 + efθ∗(x)T fθ∗(x′) .\nHere t is a random variable that labels the similarity between x and x′. If the data pair (x, x′) is\nsimilar, then t tends to be 1. If the data pair (x, x′) is not similar (negative samples), then t tends\nto be −1. We assume (x, x′, t) ∼Pfθ∗(x, x′, t). Here, (x′, t) can be viewed as side information.\nThe latent variable z is deﬁned as z := fθ∗(x) + µ, where µ ∼N(0, Ir) is a Gaussian noise that is\nuncorrelated with x. We denote (x, z) ∼Pfθ∗(x, z).\nFor the downstream task, we consider the following linear regression problem\ny = β∗T z + ν,\nwhere ν ∼N(0, 1) is a Gaussian noise. We assume that the true parameters θ∗∈Θ and β∗∈B,\nwhich satisfy a standard normalization assumption, i.e., ∥fθ(x)∥2 ≤1 for any θ ∈Θ and x ∈X\nand ∥β∥2 ≤D for any β ∈B. We have access to i.i.d unlabeled data {xi, x′\ni, ti}m\ni=1 and i.i.d labeled\ndata {xj, yj}n\nj=1. Here (x′\ni, ti) is the side information corresponding to xi.\nIn the sequel, we consider the squared loss function ℓ(x, y) := (y −x)2. We use the same form\nof truncated squared loss as in (8).\nWeakly informative condition.\nWe ﬁrst prove that the above model satisﬁes Assumption 3.6:\nLemma 6.1. Contrastive learning with linear regression as downstream tasks is κ−1-weakly-informative,\nwhere\nκ = c3 ·\ns\n1\nσmin(E[fθ∗(x)fθ∗(x)T ]).\nHere c3 is an absolute constant.\nTheoretical results.\nWe deﬁne a set of density functions PX ×S(Fθ) := {pfθ(x, x′, t) | θ ∈Θ}.\nWe then have the following theoretical guarantee.\nTheorem 6.2. We consider Algorithm 1 with truncated squared loss (8) where L = 36(D2+1) log n.\nLet ˆθ, ˆβ be the outputs of Algorithm 1. Then, for contrastive learning with linear regression as\ndownstream tasks, with probability at least 1 −δ, the excess risk can be bounded as follows,\nErrorℓ(ˆθ, ˆβ) ≤˜O\n\u0012\nκL\ns\nlog N[ ]\n\u0000PX ×S(Fθ), 1/m2\u0001\nm\n+ L\nr\n1\nn\n\u0013\n,\nwhere L = 36(D2 + 1) log n and κ is speciﬁed in Lemma 6.1. Here ˜O(·) omits some constants and\nthe polylogarithmic factors in 1/δ.\nNote that the excess risk of directly training with labeled data strongly depends on the com-\nplexity of the function class Fθ. In the case that m ≫n, the excess risk of Theorem 6.2 scales as\n˜O(\np\n1/n), which beats the pure supervised learning if the complexity of Fθ is quite large. Thus,\nthe utility of unsupervised pretraining is revealed for contrastive learning.\n13\n7\nConclusions\nThis paper proposes a generic theoretic framework for explaining the statistical beneﬁts of unsu-\npervised pretraining. We study the natural scheme of using MLE for unsupervised pretraining and\nERM for downstream task learning. We identify a natural “informative” condition, under which\nour algorithm achieves an excess risk bound that signiﬁcantly improves over the baseline achieved\nby purely supervised learning in the typical practical regimes. We further instantiate our theoretical\nframework with three concrete approaches for unsupervised pretraining and provide corresponding\nguarantees.\n14\nReferences\nAgarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W. Flambe: Structural complexity and\nrepresentation learning of low rank mdps. Advances in neural information processing systems,\n33:20095–20107, 2020.\nArora, S., Khandeparkar, H., Khodak, M., Plevrakis, O., and Saunshi, N. A theoretical analysis of\ncontrastive unsupervised representation learning. arXiv preprint arXiv:1902.09229, 2019.\nBaevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations.\nAdvances in Neural Information Processing Systems, 33:\n12449–12460, 2020.\nBai, J. and Ng, S. Determining the number of factors in approximate factor models. Econometrica,\n70(1):191–221, 2002.\nBaxter, J.\nA model of inductive bias learning.\nJournal of Artiﬁcial Intelligence Research, 12:\n149–198, mar 2000. doi: 10.1613/jair.731. URL https://doi.org/10.1613%2Fjair.731.\nBelkin, M., Niyogi, P., and Sindhwani, V. Manifold regularization: A geometric framework for\nlearning from labeled and unlabeled examples.\nJournal of machine learning research, 7(11),\n2006.\nBrown, P. F., Della Pietra, V. J., Desouza, P. V., Lai, J. C., and Mercer, R. L. Class-based n-gram\nmodels of natural language. Computational linguistics, 18(4):467–480, 1992.\nCaron, M., Bojanowski, P., Mairal, J., and Joulin, A. Unsupervised pre-training of image features\non non-curated data. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 2959–2968, 2019.\nChen, Y., Chi, Y., Fan, J., and Ma, C. 2021.\nDai, Z., Cai, B., Lin, Y., and Chen, J. Up-detr: Unsupervised pre-training for object detection\nwith transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 1601–1610, 2021.\nDavis, C. and Kahan, W. M. The rotation of eigenvectors by a perturbation. iii. SIAM Journal on\nNumerical Analysis, 7(1):1–46, 1970.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nDevroye, L., Mehrabian, A., and Reddad, T. The total variation distance between high-dimensional\ngaussians. arXiv preprint arXiv:1810.08693, 2018.\nDu, S. S., Hu, W., Kakade, S. M., Lee, J. D., and Lei, Q. Few-shot learning via learning the\nrepresentation, provably. arXiv preprint arXiv:2002.09434, 2020.\nErhan, D., Courville, A., Bengio, Y., and Vincent, P. Why does unsupervised pre-training help\ndeep learning? In Proceedings of the thirteenth international conference on artiﬁcial intelligence\nand statistics, pp. 201–208. JMLR Workshop and Conference Proceedings, 2010.\n15\nFan, J., Wang, K., Zhong, Y., and Zhu, Z. Robust high dimensional factor models with appli-\ncations to statistical machine learning. Statistical science: a review journal of the Institute of\nMathematical Statistics, 36(2):303, 2021.\nForni, M., Hallin, M., Lippi, M., and Reichlin, L. The generalized dynamic factor model: one-sided\nestimation and forecasting. Journal of the American statistical association, 100(471):830–840,\n2005.\nHaoChen, J. Z., Wei, C., Gaidon, A., and Ma, T. Provable guarantees for self-supervised deep\nlearning with spectral contrastive loss. Advances in Neural Information Processing Systems, 34:\n5000–5011, 2021.\nJin, C., Netrapalli, P., Ge, R., Kakade, S. M., and Jordan, M. I. A short note on concentration\ninequalities for random vectors with subgaussian norm. arXiv preprint arXiv:1902.03736, 2019.\nJoachims, T. et al. Transductive inference for text classiﬁcation using support vector machines. In\nIcml, volume 99, pp. 200–209, 1999.\nLawley, D. N. and Maxwell, A. E. Factor analysis as a statistical method. 1971.\nLawrence, N. and Jordan, M. Semi-supervised learning via gaussian processes. Advances in neural\ninformation processing systems, 17, 2004.\nLedoux, M. and Talagrand, M. Probability in Banach Spaces: isoperimetry and processes. Springer\nScience & Business Media, 2013.\nLee, J. D., Lei, Q., Saunshi, N., and Zhuo, J. Predicting what you already know helps: Provable\nself-supervised learning. Advances in Neural Information Processing Systems, 34:309–323, 2021.\nLiu, Q., Chung, A., Szepesvari, C., and Jin, C. When is partially observable reinforcement learn-\ning not scary?\nIn Loh, P.-L. and Raginsky, M. (eds.), Proceedings of Thirty Fifth Conference\non Learning Theory, volume 178 of Proceedings of Machine Learning Research, pp. 5175–5220.\nPMLR, 02–05 Jul 2022.\nMa, C., Wang, K., Chi, Y., and Chen, Y. Implicit regularization in nonconvex statistical estimation:\nGradient descent converges linearly for phase retrieval and matrix completion. In International\nConference on Machine Learning, pp. 3345–3354. PMLR, 2018.\nMarshall, A. W., Olkin, I., and Arnold, B. C. Inequalities: Theory of Majorization and its Appli-\ncations, volume 143. Springer, second edition, 2011. doi: 10.1007/978-0-387-68276-1.\nMaurer,\nA.,\nPontil,\nM.,\nand Romera-Paredes,\nB.\nThe beneﬁt\nof multitask represen-\ntation\nlearning.\nJournal\nof\nMachine\nLearning\nResearch,\n17(81):1–32,\n2016.\nURL\nhttp://jmlr.org/papers/v17/15-242.html.\nMelamud, O., Goldberger, J., and Dagan, I. context2vec: Learning generic context embedding\nwith bidirectional lstm. In Proceedings of the 20th SIGNLL conference on computational natural\nlanguage learning, pp. 51–61, 2016.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. Distributed representations of\nwords and phrases and their compositionality. Advances in neural information processing systems,\n26, 2013.\n16\nPeter, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep\ncontextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding\nby generative pre-training. 2018.\nRatsaby, J. and Venkatesh, S. S. Learning from a mixture of labeled and unlabeled examples with\nparametric side information. In Proceedings of the eighth annual conference on Computational\nlearning theory, pp. 412–417, 1995.\nSaunshi, N., Malladi, S., and Arora, S. A mathematical exploration of why language models help\nsolve downstream tasks. arXiv preprint arXiv:2010.03648, 2020.\nSaunshi, N., Ash, J., Goel, S., Misra, D., Zhang, C., Arora, S., Kakade, S., and Krishnamurthy,\nA. Understanding contrastive learning requires incorporating inductive biases. arXiv preprint\narXiv:2202.14037, 2022.\nSchmitt, B. A. Perturbation bounds for matrix square roots and pythagorean sums. Linear algebra\nand its applications, 174:215–227, 1992.\nSchneider, S., Baevski, A., Collobert, R., and Auli, M. wav2vec: Unsupervised pre-training for\nspeech recognition. arXiv preprint arXiv:1904.05862, 2019.\nSong, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y. Mass: Masked sequence to sequence pre-training\nfor language generation. arXiv preprint arXiv:1905.02450, 2019.\nSzummer, M. and Jaakkola, T. Information regularization with partially labeled data. Advances\nin Neural Information processing systems, 15, 2002.\nTosh, C., Krishnamurthy, A., and Hsu, D. Contrastive learning, multi-view redundancy, and linear\nmodels. In Algorithmic Learning Theory, pp. 1179–1206. PMLR, 2021a.\nTosh, C., Krishnamurthy, A., and Hsu, D. Contrastive estimation reveals topic posterior information\nto linear models. J. Mach. Learn. Res., 22:281–1, 2021b.\nTripuraneni, N., Jordan, M., and Jin, C. On the theory of transfer learning: The importance of\ntask diversity. Advances in Neural Information Processing Systems, 33:7852–7862, 2020.\nTripuraneni, N., Jin, C., and Jordan, M.\nProvable meta-learning of linear representations.\nIn\nInternational Conference on Machine Learning, pp. 10434–10443. PMLR, 2021.\nVan de Geer, S. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.\nVershynin, R.\nHigh-dimensional probability: An introduction with applications in data science,\nvolume 47. Cambridge university press, 2018.\nWainwright, M. J. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge\nUniversity Press, 2019.\nWei, C., Xie, S. M., and Ma, T. Why do pretrained language models help in downstream tasks?\nan analysis of head and prompt tuning. Advances in Neural Information Processing Systems, 34:\n16158–16170, 2021.\n17\nZhang, T. From ε-entropy to KL-entropy: Analysis of minimum information complexity density\nestimation. The Annals of Statistics, 34(5), oct 2006.\nZhu, X. J. Semi-supervised learning literature survey. 2005.\n18\nA\nProofs for Section 3\nIn Section A.1, we prove Theorem 3.3, which gives a TV distance guarantee for the MLE step\nin Algorithm 1. Our proof is inspired by Van de Geer (2000); Zhang (2006), and largely follows\nAgarwal et al. (2020); Liu et al. (2022). In Section A.2, we prove Theorem 3.4 that guarantees the\nperformance of Algorithm 1 by upper bounding the excess risk. The proof relies on the fact that the\nlabeled data {xj, yj}n\nj=1 are independent of the unlabeled data {xi, si}m\ni=1. In Section A.3, we prove\nCorollary 3.5 based on the analysis of Gaussian complexity. In Section A.4, we prove Theorem 3.7\nby ﬁrst showing that the MLE step in Algorithm 1 actually guarantees an upper bound on the\nHellinger distance, which is stronger than the TV distance guarantee mentioned in Theorem 3.3.\nA.1\nProofs for Theorem 3.3\nIn the sequel, we prove Theorem 3.3.\nProof of Theorem 3.3. For notation simplicity, we denote x := (x, s). Recall that we deﬁne PX ×S(Φ) :=\n{pφ(x, s) | φ ∈Φ}. Let N[ ](PX ×S(Φ), ǫ) be the smallest ǫ-bracket of PX ×S(Φ). We have |N[ ](PX ×S(Φ), ǫ)| =\nN[ ](PX ×S(Φ), ǫ), where N[ ](PX ×S(Φ), ǫ) is the bracketing number of PX ×S(Φ).\nBy Markov\ninequality and Boole’s inequality, it holds with probability at least 1 −δ that for all ¯pφ(x) ∈\nN[ ](PX ×S(Φ), ǫ)\n1\n2\nm\nX\ni=1\nlog ¯pφ(xi)\npφ∗(xi) ≤log E\nh\ne\n1\n2\nPm\ni=1 log\n¯\npφ(xi)\npφ∗(xi) i\n+ log N[ ](PX ×S(Φ), ǫ)\nδ\n.\n(9)\nNote that ˆφ is the maximizer of the likelihood function, i.e.\nˆφ ←arg max\nφ∈Φ\nm\nX\ni=1\nlog pφ(xi),\nwhich implies\n1\n2\nm\nX\ni=1\nlog\n¯p ˆφ(xi)\npφ∗(xi) ≥0.\n(10)\nThen we have with probability at least 1 −δ that\n0 ≤log E\nh\ne\n1\n2\nPm\ni=1 log\n¯\np ˆ\nφ(xi)\npφ∗(xi) i\n+ log N[ ](PX ×S(Φ), ǫ)\nδ\n,\n= m log E\n\u0014s\n¯p ˆφ(x)\npφ∗(x)\n\u0015\n+ log N[ ](PX ×S(Φ), ǫ)\nδ\n,\n= m log\nZ q\n¯p ˆφ(x)pφ∗(x) dx + log N[ ](PX ×S(Φ), ǫ)\nδ\n,\n≤m\n\u0012 Z q\n¯p ˆφ(x)pφ∗(x) dx −1\n\u0013\n+ log N[ ](PX ×S(Φ), ǫ)\nδ\n,\n(11)\n19\nwhere the last inequality follows from the fact that log x ≤x −1. By rearranging the terms, we\nhave\n1 −\nZ q\n¯p ˆφ(x)pφ∗(x) dx ≤1\nm log N[ ](PX ×S(Φ), ǫ)\nδ\n.\n(12)\nBy the deﬁnition of bracket, we obtain\nZ\n¯p ˆφ(x)dx =\nZ\n(¯p ˆφ(x) −p ˆφ(x))dx +\nZ\np ˆφ(x)dx ≤ǫ + 1,\nwhich implies\nZ \u0010q\n¯p ˆφ(x) −\nq\npφ∗(x)\n\u00112\ndx ≤2\n\u0012\n1 −\nZ q\n¯p ˆφ(x)pφ∗(x)dx\n\u0013\n+ ǫ\n(13)\nand\nZ \u0010q\n¯p ˆφ(x) +\nq\npφ∗(x)\n\u00112\ndx ≤2\nZ\n¯p ˆφ(x) + pφ∗(x) dx ≤2ǫ + 4.\n(14)\nCombining (12) and (13), we show that\nZ \u0010q\n¯p ˆφ(x) −\nq\npφ∗(x)\n\u00112\ndx ≤2\nm log N[ ](PX ×S(Φ), ǫ)\nδ\n+ ǫ.\n(15)\nBy Cauchy-Schwarz inequality, it then holds that\n\u0012 Z\n|¯p ˆφ(x) −pφ∗(x)| dx\n\u00132\n≤\nZ \u0010q\n¯p ˆφ(x) +\nq\npφ∗(x)\n\u00112\ndx ·\nZ \u0010q\n¯p ˆφ(x) −\nq\npφ∗(x)\n\u00112\ndx,\n≤(2ǫ + 4) ·\n\u0012 2\nm log N[ ](PX ×S(Φ), ǫ)\nδ\n+ ǫ\n\u0013\n,\n(16)\nwhere the last inequality follows from (14) and (15). Note that\n\u0012 Z\n|p ˆφ(x) −pφ∗(x)| dx\n\u00132\n−\n\u0012 Z\n|¯p ˆφ(x) −pφ∗(x)| dx\n\u00132\n=\n\u0012 Z\n|p ˆφ(x) −pφ∗(x)| + |¯p ˆφ(x) −pφ∗(x)| dx\n\u0013\n·\n\u0012 Z\n|p ˆφ(x) −pφ∗(x)| −|¯p ˆφ(x) −pφ∗(x)| dx\n\u0013\n≤\n\u0012 Z\n|p ˆφ(x) −pφ∗(x)| + |¯p ˆφ(x) −pφ∗(x)| dx\n\u0013\n·\nZ\n|p ˆφ(x) −¯p ˆφ(x)| dx\n≤(ǫ + 4) · ǫ.\n(17)\nAdding (16) and (17) together, we have\n\u0012 Z\n|p ˆφ(x) −pφ∗(x)| dx\n\u00132\n≤(2ǫ + 4) ·\n\u0012 2\nm log N[ ](PX ×S(Φ), ǫ)\nδ\n+ ǫ\n\u0013\n+ (ǫ + 4) · ǫ,\n(18)\n20\nwhich implies\ndTV\n\u0000P ˆφ(x), Pφ∗(x)\n\u0001\n= 1\n2\nZ\n|p ˆφ(x) −pφ∗(x)| dx\n≤1\n2\ns\n(2ǫ + 4) ·\n\u0012 2\nm log N[ ](PX ×S(Φ), ǫ)\nδ\n+ ǫ\n\u0013\n+ (ǫ + 4) · ǫ.\n(19)\nSetting ǫ = 1/m, we have with probability at least 1 −δ that\ndTV\n\u0000P ˆφ(x), Pφ∗(x)\n\u0001\n≤1\n2\ns\u0012 2\nm + 4\n\u0013\n·\n\u0012 2\nm log N[ ](PX ×S(Φ), 1/m)\nδ\n+ 1\nm\n\u0013\n+\n\u0012 1\nm + 4\n\u0013\n· 1\nm\n≤3 ·\nr\n1\nm log N[ ](PX ×S(Φ), 1/m)\nδ\n.\n(20)\nThus, we prove Theorem 3.3.\nA.2\nProofs for Theorem 3.4\nBefore proving the theorem, we ﬁrst present some useful results that will be used in the proof\nof Theorem 3.4. Lemma A.1 upper bounds the diﬀerence between empirical loss and population\nloss by an application of bounded diﬀerence inequality and a standard symmetrization argument.\nLemma A.2 relates excess risks with the total variation distance between probability distributions.\nFor notation simplicity, we denote E(x,y)∼Pφ,ψ(x,y) by Eφ,ψ in the following. We further denote by\nE the expectation taken over the ground truth parameter, i.e., E := E(x,y)∼Pφ∗,ψ∗(x,y).\nLemma A.1. Suppose that ℓ(·, ·) is a L-bounded loss function. For any given φ ∈Φ, with probability\nat least 1 −δ,\nsup\nψ∈Ψ\n\f\f\f\fE[ℓ(gφ,ψ(x), y)] −1\nn\nn\nX\nj=1\nℓ(gφ,ψ(xj), yj)\n\f\f\f\f ≤Rn(ℓ◦Gφ,Ψ) + L\nr\n2 log(2/δ)\nn\n,\n(21)\nwhere Rn(ℓ◦Gφ,Ψ) is the Rademacher complexity of the function class ℓ◦Gφ,Ψ deﬁned in Theorem\n3.4.\nProof of Lemma A.1. First notice that, when a pair (xj, yj) changes, since ℓis L-bounded, the\nrandom variable\nsup\nψ∈Ψ\n\u0012\nE[ℓ(gφ,ψ(x), y)] −1\nn\nn\nX\nj=1\nℓ(gφ,ψ(xj), yj)\n\u0013\n(22)\ncan change by no more than 2L/n. McDiarmid’s inequality implies that with probability at least\n1 −δ/2,\nsup\nψ∈Ψ\n\u0012\nE[ℓ(gφ,ψ(x), y)] −1\nn\nn\nX\nj=1\nℓ(gφ,ψ(xj), yj)\n\u0013\n≤E\n\u0014\nsup\nψ∈Ψ\n\u0012\nE[ℓ(gφ,ψ(x), y)] −1\nn\nn\nX\nj=1\nℓ(gφ,ψ(xj), yj)\n\u0013\u0015\n+ L\nr\n2 log(2/δ)\nn\n.\n(23)\n21\nLet {x′\nj, y′\nj}n\nj=1 be independent copies of {xj, yj}n\nj=1 and {σj}n\nj=1 be i.i.d. Rademacher random\nvariables. Using the standard symmetrization technique, we have\nE\n\u0014\nsup\nψ∈Ψ\n\u0012\nE[ℓ(gφ,ψ(x), y)] −1\nn\nn\nX\nj=1\nℓ(gφ,ψ(xj), yj)\n\u0013\u0015\n= E\n\u0014\nsup\nψ∈Ψ\nE\n\u0014 1\nn\nn\nX\nj=1\nℓ(gφ,ψ(x′\nj), y′\nj) −1\nn\nn\nX\nj=1\nℓ(gφ,ψ(xj), yj)\n\f\f\f\f{xj, yj}n\nj=1\n\u0015\u0015\n≤E\n\u0014\nsup\nψ∈Ψ\n\u0012 1\nn\nn\nX\nj=1\nℓ(gφ,ψ(x′\nj), y′\nj) −1\nn\nn\nX\nj=1\nℓ(gφ,ψ(xj), yj)\n\u0013\u0015\n≤E\n\u0014\nsup\nψ∈Ψ\n1\nn\nn\nX\nj=1\nσj\n\u0012\nℓ(gφ,ψ(x′\nj), y′\nj) −ℓ(gφ,ψ(xj), yj)\n\u0013\u0015\n≤2E\n\u0014\nsup\nψ∈Ψ\n1\nn\nn\nX\nj=1\nσjℓ(gφ,ψ(xj), yj)\n\u0015\n= Rn(ℓ◦Gφ,Ψ).\n(24)\nTherefore, with probability at least 1 −δ/2,\nsup\nψ∈Ψ\n\u0012\nE[ℓ(gφ,ψ(x), y)] −1\nn\nn\nX\nj=1\nℓ(gφ,ψ(xj), yj)\n\u0013\n≤Rn(ℓ◦Gφ,Ψ) + L\nr\n2 log(2/δ)\nn\n(25)\nSimilarly, with probability at least 1 −δ/2,\nsup\nψ∈Ψ\n\u0012 1\nn\nn\nX\nj=1\nℓ(gφ,ψ(xj), yj) −E[ℓ(gφ,ψ(x), y)]\n\u0013\n≤Rn(ℓ◦Gφ,Ψ) + L\nr\n2 log(2/δ)\nn\n(26)\nCombine these together, we prove Lemma A.1.\nLemma A.2. Suppose that ℓ(·, ·) is a L-bounded loss function. Then, it holds for any φ ∈Φ, ψ ∈Ψ\nthat\nE[ℓ(gφ,ψ(x), y)] −E[ℓ(gφ∗,ψ∗(x), y)] ≤4L · dTV(Pφ,ψ(x, y), Pφ∗,ψ∗(x, y)).\n(27)\nProof of Lemma A.2.\nEφ∗,ψ∗[ℓ(gφ,ψ(x), y)] −Eφ∗,ψ∗[ℓ(gφ∗,ψ∗(x), y)]\n= Eφ∗,ψ∗[ℓ(gφ,ψ(x), y)] −Eφ,ψ[ℓ(gφ,ψ(x), y)]\n+ Eφ,ψ[ℓ(gφ,ψ(x), y)] −Eφ,ψ[ℓ(gφ∗,ψ∗(x), y)]\n+ Eφ,ψ[ℓ(gφ∗,ψ∗(x), y)] −Eφ∗,ψ∗[ℓ(gφ∗,ψ∗(x), y)].\n(28)\nFirst notice that, by deﬁnition of gφ,ψ,\nEφ,ψ[ℓ(gφ,ψ(x), y)] −Eφ,ψ[ℓ(gφ∗,ψ∗(x), y)] ≤0.\n(29)\n22\nFor the other two terms, based on the fact that ℓis L-bounded, we have\n|Eφ∗,ψ∗[ℓ(gφ,ψ(x), y)] −Eφ,ψ[ℓ(gφ,ψ(x), y)]|\n=\n\f\f\f\f\nZ\nℓ(gφ,ψ(x), y)pφ∗,ψ∗(x, y)dxdy −\nZ\nℓ(gφ,ψ(x), y)pφ,ψ(x, y)dxdy\n\f\f\f\f\n=\n\f\f\f\f\nZ\nℓ(gφ,ψ(x), y)(pφ∗,ψ∗(x, y) −pφ,ψ(x, y))dxdy\n\f\f\f\f\n≤\nZ\n|ℓ(gφ,ψ(x), y)||(pφ∗,ψ∗(x, y) −pφ,ψ(x, y))|dxdy\n≤\nZ\nL|(pφ∗,ψ∗(x, y) −pφ,ψ(x, y))|dxdy\n= 2L · dTV(Pφ,ψ(x, y), Pφ∗,ψ∗(x, y)).\n(30)\nSimilarly, it holds that\n|Eφ∗,ψ∗[ℓ(gφ∗,ψ∗(x), y)] −Eφ,ψ[ℓ(gφ∗,ψ∗(x), y)]| ≤2L · dTV(Pφ,ψ(x, y), Pφ∗,ψ∗(x, y)).\n(31)\nCombining (28), (29), (30) and (31), we obtain\nEφ∗,ψ∗[ℓ(gφ,ψ(x), y)] −Eφ∗,ψ∗[ℓ(gφ∗,ψ∗(x), y)] ≤4L · dTV(Pφ,ψ(x, y), Pφ∗,ψ∗(x, y)).\n(32)\nWith Lemma A.1 and Lemma A.2, we are able to state our proofs for Theorem 3.4 in the\nfollowing. The main idea of the proof is decomposing the risk. And a key observation is that the\nlabeled data {xj, yj}n\nj=1 are independent of the pretrained ˆφ, which is learned from the unlabeled\ndata {xi}m\ni=1.\nProof of Theorem 3.4. Let\n˜ψ := arg min\nψ∈Ψ\ndTV(Pˆφ,ψ(x, y), Pφ∗,ψ∗(x, y)).\n(33)\nAnd for any φ ∈Φ, ψ ∈Ψ, we deﬁne\n∆φ,ψ := Eφ∗,ψ∗[ℓ(gφ,ψ(x), y)] −1\nn\nn\nX\nj=1\nℓ(gφ,ψ(xj), yj).\n(34)\n23\nRecall that the excess risk is deﬁned in (2). It then holds that\nErrorℓ(ˆφ, ˆψ) = Eφ∗,ψ∗[ℓ(g ˆφ, ˆ\nψ(x), y)] −Eφ∗,ψ∗[ℓ(gφ∗,ψ∗(x), y)]\n= Eφ∗,ψ∗[ℓ(g ˆφ, ˆ\nψ(x), y)] −1\nn\nn\nX\nj=1\nℓ(g ˆφ, ˆ\nψ(xj), yj)\n+ 1\nn\nn\nX\nj=1\nℓ(g ˆφ, ˆψ(xj), yj) −1\nn\nn\nX\nj=1\nℓ(g ˆφ, ˜ψ(xj), yj)\n(≤0, by ERM in Algorithm 1)\n+ 1\nn\nn\nX\nj=1\nℓ(g ˆφ, ˜ψ(xj), yj) −Eφ∗,ψ∗[ℓ(g ˆφ, ˜ψ(x), y)]\n+ Eφ∗,ψ∗[ℓ(g ˆφ, ˜ψ(x), y)] −Eφ∗,ψ∗[ℓ(gφ∗,ψ∗(x), y)]\n≤∆ˆφ, ˆ\nψ −∆ˆφ, ˜ψ + Eφ∗,ψ∗[ℓ(g ˆφ, ˜ψ(x), y)] −Eφ∗,ψ∗[ℓ(gφ∗,ψ∗(x), y)].\n(35)\nBy lemma A.2, we have\nEφ∗,ψ∗[ℓ(g ˆφ, ˜ψ(x), y)] −Eφ∗,ψ∗[ℓ(gφ∗,ψ∗(x), y)]\n≤4L · dTV(Pˆφ, ˜\nψ(x, y), Pφ∗,ψ∗(x, y))\n= 4L · min\nψ∈Ψ dTV(Pˆφ,ψ(x, y), Pφ∗,ψ∗(x, y))\n(by deﬁnition of ˜ψ)\n≤4κL · dTV(Pˆφ(x, s), Pφ∗(x, s)).\n(36)\nThe last line holds, since by Assumption 3.2, for any ˆφ ∈Φ, we choose T1 that satisﬁes (5) and T2\nthat satisﬁes (6). Let ψ = T −1\n2\n◦ψ∗. It then holds that\nmin\nψ∈Ψ dTV(Pˆφ,ψ(x, y), Pφ∗,ψ∗(x, y)) ≤dTV\n\u0000P ˆφ,ψ(x, y), Pφ∗,ψ∗(x, y)\n\u0001\n= dTV\n\u0000PT1◦ˆφ,ψ∗(x, y), Pφ∗,ψ∗(x, y)\n\u0001\n≤dTV\n\u0000PT1◦ˆφ(x, z), Pφ∗(x, z)\n\u0001\n≤κ · dTV\n\u0000P ˆφ(x, s), Pφ∗(x, s)\n\u0001\n.\n(37)\nCombining (35) and (36), we have\nErrorℓ(ˆφ, ˆψ) ≤∆ˆφ, ˆψ −∆ˆφ, ˜ψ + 4κL · dTV(Pˆφ(x, s), Pφ∗(x, s)).\n(38)\nWe deﬁne the following events\nD :=\n(\ndTV(Pˆφ(x, s), Pφ∗(x, s)) ≤3\nr\n1\nm log 2N(PX ×S(Φ), 1/m)\nδ\n)\n(39)\nand\nR :=\n(\nsup\nψ∈Ψ\n|∆ˆφ,ψ| ≤Rn(ℓ◦Gˆφ,Ψ) + L\nr\n2 log(4/δ)\nn\n)\n.\n(40)\n24\nIt holds that\nP(D ∩R) = E[1D∩R] = E[E[1D1R|ˆφ]] = E[1DE[1R|ˆφ]] = E[1DP(R|ˆφ)],\n(41)\nwhere the third equation follows from the fact that D is ˆφ-measurable. Note that {xj, yj}n\nj=1 is\nindependent of ˆφ. By Lemma A.1, for any given ˆφ, with probability at least 1 −δ/2,\nsup\nψ∈Ψ\n|∆ˆφ,ψ| ≤Rn(ℓ◦Gˆφ,Ψ) + L\nr\n2 log(4/δ)\nn\n,\n(42)\ni.e.,\nP(R|ˆφ) ≥1 −δ/2.\n(43)\nBy Lemma 3.3, with probability at least 1 −δ/2, the output of the ﬁrst step of our algorithm ˆφ,\nsatisﬁes\ndTV(Pˆφ(x, s), Pφ∗(x, s)) ≤3\nr\n1\nm log 2N(PX ×S(Φ), 1/m)\nδ\n(44)\ni.e.,\nP(D) ≥1 −δ/2.\n(45)\nBy (41), (43) and (45), we have\nP(D ∩R) ≥(1 −δ/2)2 ≥1 −δ.\n(46)\nThen, under event D ∩R, by our decomposition (38), we have\nErrorℓ(ˆφ, ˆψ) ≤∆ˆφ, ˆψ −∆ˆφ, ˜\nψ + 4κL · dTV(Pˆφ(x, s), Pφ∗(x, s))\n≤2 sup\nψ∈Ψ\n|∆ˆφ,ψ| + 4κL · dTV(Pˆφ(x, s), Pφ∗(x, s))\n≤2Rn(ℓ◦Gˆφ,Ψ) + 2L\nr\n2 log(4/δ)\nn\n+ 12κL\nr\n1\nm log 2N(PX ×S(Φ), 1/m)\nδ\n≤2 max\nφ∈Φ Rn(ℓ◦Gφ,Ψ) + 2L\nr\n2 log(4/δ)\nn\n+ 12κL\nr\n1\nm log 2N(PX ×S(Φ), 1/m)\nδ\n.\n(47)\nThus, we prove Theorem 3.4.\nA.3\nProofs for Corollary 3.5\nIn the following, we give the proof of Corollary 3.5, which is based on the analysis of Gaussian\ncomplexity.\n25\nProof. By Theorem 3.4, we have\nErrorℓ(ˆφ, ˆψ) ≤2 max\nφ∈Φ Rn(ℓ◦Gφ,Ψ) + 2L ·\nr\n2\nn log 4\nδ + 12κL ·\nr\n1\nm log 2N[ ](PX ×S(Φ), 1/m)\nδ\n. (48)\nTherefore, it remains to bound the Rademacher complexity term. By Ledoux & Talagrand (2013),\nthe Rademacher complexity is upper bounded by the Gaussian complexity, i.e.,\nRn(F) ≤c · Gn(F) = c · E ˆ\nGn(F),\n(49)\nwhere c is some absolute constants. Here Gn(F) is the Gaussian complexity, and it’s empirical\nversion is deﬁned as\nˆ\nGn(F) := Egi\n\u0014\nsup\nf∈F\n\f\f\f\f\n2\nn\nn\nX\ni=1\ngif(xi)\n\f\f\f\f\n\f\f\f\f x1, · · · , xn\n\u0015\n(50)\nwhere g1, · · · , gn are i.i.d. N(0, 1) random variables. By (5.36) in Wainwright (2019), we have\nˆ\nGn(ℓ◦Gφ,Ψ) ≤\n1\n√n · min\nδ∈[0,L]\n\u001a\nδ√n + 2L\nq\nlog N(ℓ◦Gφ,Ψ, δ, ∥· ∥∞)\n\u001b\n≤\n1\n√n\n\u0012\nL + 2L\nq\nlog N(ℓ◦Gφ,Ψ, L/√n, ∥· ∥∞)\n\u0013\n(Take δ = L/√n)\n≤3L\nr\nlog N(ℓ◦Gφ,Ψ, L/√n, ∥· ∥∞)\nn\n.\n(51)\nCombining (49) and (51), we obtain\nRn(ℓ◦Gφ,Ψ) ≤3cL\nr\nlog N(ℓ◦Gφ,Ψ, L/√n, ∥· ∥∞)\nn\n.\n(52)\nBy (48) and (52), we ﬁnish the proof.\nA.4\nProofs for Theorem 3.7\nIn this section, we ﬁrst show the relation of Assumption 3.2 and Assumption 3.6. We then show\nthat the MLE step in line 2 of Algorithm 1 guarantees an upper bound on the Hellinger distance\nH(P ˆφ(x, s), Pφ∗(x, s)). Then, using the same techniques as that in the proof of Theorem 3.4, we\nprove Theorem 3.7.\nA.4.1\nRelation of Assumption 3.2 and Assumption 3.6\nAssumption 3.6 is actually a relaxation of Assumption 3.2. To see this, by Assumption 3.2, for any\nφ ∈Φ, we choose T1 that satisﬁes (5) and T2 that satisﬁes (6). Let ψ = T −1\n2\n◦ψ∗. It then holds\nthat\ndTV\n\u0000Pφ,ψ(x, y), Pφ∗,ψ∗(x, y)\n\u0001\n= dTV\n\u0000PT1◦φ,ψ∗(x, y), Pφ∗,ψ∗(x, y)\n\u0001\n≤dTV\n\u0000PT1◦φ(x, z), Pφ∗(x, z)\n\u0001\n≤κ · dTV\n\u0000Pφ(x, s), Pφ∗(x, s)\n\u0001\n.\n26\nNote that the TV distance can be upper bounded by the Hellinger distance. Thus, Assumption 3.2\ndirectly implies Assumption 3.6.\nA.4.2\nHellinger Distance Guarantee\nSuppose that ˆφ is the output of the MLE step in Algorithm 1, which satisﬁes\nˆφ ←arg max\nφ∈Φ\nm\nX\ni=1\nlog pφ(xi, si).\n(53)\nWe have the following theoretical guarantee on the Hellinger distance between P ˆφ(x, s) and Pφ∗(x, s).\nLemma A.3. Let ˆφ be the output of Algorithm 1. It then holds that with probability at least 1 −δ\nthat\nH\n\u0000P ˆφ(x, s), Pφ∗(x, s)\n\u0001\n≤\ns\n2\nm log N[ ]\n\u0000PX ×S(Φ), 1/m2\u0001\nδ\n,\n(54)\nwhere we denote PX ×S(Φ) := {pφ(x, s) | φ ∈Φ}.\nProof of Lemma A.3. For notation simplicity, we denote x := (x, s). Let ǫ > 0. Similar to the\nproof of Theorem 3.3, we obtain with probability at least 1 −δ\n1 −\nZ q\n¯p ˆφ(x)pφ∗(x) dx ≤1\nm log N[ ]\n\u0000PX ×S(Φ), ǫ\n\u0001\nδ\n.\n(55)\nHere ¯p ˆφ(x) ∈N[ ](PX ×S(Φ), ǫ) that satisﬁes ¯p ˆφ(x) ≥pφ∗(x) for any x and\nZ\n¯p ˆφ(x) −pφ∗(x) dx ≤ǫ.\n(56)\nNote that\n1 −\nZ q\np ˆφ(x)pφ∗(x) dx −\n\u0012\n1 −\nZ q\n¯p ˆφ(x)pφ∗(x) dx\n\u0013\n=\nZ \u0010q\n¯p ˆφ(x) −\nq\np ˆφ(x)\n\u0011q\npφ∗(x) dx\n≤\nsZ \u0010q\n¯p ˆφ(x) −\nq\np ˆφ(x)\n\u00112\ndx\n=\nsZ\n¯p ˆφ(x) + p ˆφ(x) −2\nq\n¯p ˆφ(x)p ˆφ(x) dx\n≤\nsZ\n¯p ˆφ(x) −p ˆφ(x) dx\n≤√ǫ.\n(57)\n27\nHere the ﬁrst inequality follows from Cauchy-Schwarz inequality and the second follows from the\nfact that\nq\n¯p ˆφ(x)p ˆφ(x) ≥p ˆφ(x). By (55) and (57), we have\n1 −\nZ q\np ˆφ(x)pφ∗(x) dx ≤√ǫ + 1\nm log N[ ]\n\u0000PX ×S(Φ), ǫ\n\u0001\nδ\n,\n(58)\nwhich implies that\nH2\u0000P ˆφ(x), Pφ∗(x)\n\u0001\n= 1 −\nZ q\np ˆφ(x)pφ∗(x) dx ≤√ǫ + 1\nm log N[ ]\n\u0000PX ×S(Φ), ǫ\n\u0001\nδ\n.\n(59)\nSet ǫ = 1/m2. We have\nH2\u0000P ˆφ(x, s), Pφ∗(x, s)\n\u0001\n≤2\nm log N[ ]\n\u0000PX ×S(Φ), 1/m2\u0001\nδ\n.\n(60)\nA.4.3\nProof of Theorem 3.7\nWith Lemma A.3 in hand, we are ready to prove Theorem 3.7.\nProof of Theorem 3.7. Let ˆφ be the output of the MLE step in Algorithm 1. And for any φ ∈\nΦ, ψ ∈Ψ, we deﬁne\n∆φ,ψ := Eφ∗,ψ∗[ℓ(gφ,ψ(x), y)] −1\nn\nn\nX\nj=1\nℓ(gφ,ψ(xj), yj).\n(61)\nFollowing the same arguments as that in the proof of Theorem 3.4, we have with probability at\nleast 1 −δ,\nH\n\u0000P ˆφ(x, s), Pφ∗(x, s)\n\u0001\n≤\nr\n2\nm log 2N(PX ×S(Φ), 1/m2)\nδ\n(62)\nand\nsup\nψ∈Ψ\n|∆ˆφ,ψ| ≤Rn(ℓ◦Gˆφ,Ψ) + L\nr\n2 log(4/δ)\nn\n.\n(63)\nMoreover, as mentioned in (35), we have\nErrorℓ(ˆφ, ˆψ) ≤∆ˆφ, ˆ\nψ −∆ˆφ, ˜ψ + Eφ∗,ψ∗[ℓ(g ˆφ, ˜\nψ(x), y)] −Eφ∗,ψ∗[ℓ(gφ∗,ψ∗(x), y)]\n≤2Rn(ℓ◦Gˆφ,Ψ) + 2L\nr\n2 log(4/δ)\nn\n+ Eφ∗,ψ∗[ℓ(gφ∗,ψ∗(x), y)] −Eφ∗,ψ∗[ℓ(gφ∗,ψ∗(x), y)],\n(64)\n28\nwhere ˜ψ := arg minψ∈Ψ dTV(P ˆφ,ψ(x, y), Pφ∗,ψ∗(x, y)) and the second inequality follows from (63).\nBy lemma A.2, we have\nEφ∗,ψ∗[ℓ(g ˆφ, ˜\nψ(x), y)] −Eφ∗,ψ∗[ℓ(gφ∗,ψ∗(x), y)]\n≤4L · dTV(P ˆφ, ˜\nψ(x, y), Pφ∗,ψ∗(x, y))\n= 4L · min\nψ∈Ψ dTV(P ˆφ,ψ(x, y), Pφ∗,ψ∗(x, y))\n(by deﬁnition of ˜ψ)\n≤1) 4κL · H(P ˆφ(x, s), Pφ∗(x, s))\n≤2) 4κL\nr\n2\nm log 2N(PX ×S(Φ), 1/m2)\nδ\n,\n(65)\nwhere 1) follows from Assumption 3.6 and 2) follows from (62). Combining (64) and (65), we have\nErrorℓ(ˆφ, ˆψ) ≤2Rn(ℓ◦Gˆφ,Ψ) + 2L\nr\n2 log(4/δ)\nn\n+ 4κL\nr\n2\nm log 2N(PX ×S(Φ), 1/m2)\nδ\n≤2 max\nφ∈Φ Rn(ℓ◦Gφ,Ψ) + 2L\nr\n2 log(4/δ)\nn\n+ 4κL\nr\n2\nm log 2N(PX ×S(Φ), 1/m2)\nδ\n.\n(66)\nB\nProofs for Section 4\nIn Section B.1, by analysing the total variation distance between two high-dimensional Gaussians\nand applying the Davis-Kahan theorem, we show that factor model with linear regression as down-\nstream tasks has κ-transferability (Lemma 4.2), where κ depends on the largest and smallest singular\nvalue of the ground truth parameter B∗. In Section B.2 and Section B.3, we prove two lemmas\nthat will be used in the proof of Theorem 4.3. To be speciﬁc, in Section B.2, we upper bound\nthe bracketing number of the set P(B) by using ǫ-discretization (Lemma B.5). In Section B.3, we\nprove Lemma B.6, which will be used to upper bound the Rademacher complexity of the function\nclass ℓ◦GB,C. In Section B.4, we prove Theorem 4.3. Finally, in Section B.5, we provide a reﬁned\nanalysis for proving Theorem 4.4.\nB.1\nProofs for Lemma 4.2\nFirst of all, we present some useful lemmas that will be used in the proof of Lemma 4.2. Given two\nhigh-dimensional Gaussians, we can bound their total variation distance as follows.\nLemma B.1 (Theorem 1.2 and Proposition 2.1 in Devroye et al. (2018)). Suppose that d > 1. Let\nµ1 ̸= µ2 ∈Rd. Then, we have\n1\n200 ≤dTV\n\u0000N(µ1, Id), N(µ2, Id)\n\u0001\nmin{1, ∥µ1 −µ2∥2}\n≤1.\nLemma B.2 (Theorem 1.1 in Devroye et al. (2018)). Suppose that d > 1. Let µ ∈Rd and Σ1 ̸= Σ2\nbe positive deﬁnite d × d matrices. Then, we have\n1\n100 ≤\ndTV\n\u0000N(µ, Σ1), N(µ, Σ2)\n\u0001\nmin{1, ∥Σ−1/2\n1\nΣ2Σ−1/2\n1\n−Id∥F}\n≤3\n2.\n29\nRecall that we deﬁne B := {B ∈Rd×r | ∥B∥2 ≤D}.\nLet B ∈B and B∗be the ground\ntruth parameter.\nWe denote by σ∗\nmax and σ∗\nmin the largest and smallest singular value of B∗,\nrespectively. Moreover, we denote the singular value decomposition of B and B∗by B = UΣV\nand B∗= U ∗Σ∗V ∗, respectively. Here Σ, Σ∗∈Rr×r are diagonal matrices and U, U ∗∈Rd×r,\nV, V ∗∈Rr×d are matrices with orthogonal columns. Let\nM := BBT = UΛU T,\nM ∗:= B∗B∗T = U ∗Λ∗U ∗T ,\n(67)\nwhere Λ := ΣΣT and Λ∗:= Σ∗Σ∗T . We deﬁne\nO := arg min\nO∈Or×r ∥UO −U ∗∥F.\n(68)\nThen, we have the following lemmas.\nLemma B.3. For M, M ∗deﬁned in (67) and O deﬁned in (68), there exists some absolute constants\nc > 1 such that\n∥UO −U ∗∥F ≤\nc\n(σ∗\nmin)2 ∥M −M ∗∥F.\nHere σ∗\nmin is the smallest singular value of the true parameter B∗.\nProof. An application of Davis-Kahan Theorem (Davis & Kahan, 1970).\nLemma B.4. For M, M ∗deﬁned in (67) and O deﬁned in (68), there exists some absolute constants\nc such that\n∥Λ1/2O −OΛ∗1/2∥F ≤4c(σ∗\nmax)2\n(σ∗\nmin)3 ∥M −M ∗∥F.\nHere σ∗\nmin is the smallest singular value of the true parameter B∗.\nProof of Lemma B.4. Our proof is inspired by Ma et al. (2018). By Lemma 2.1 in Schmitt (1992),\nwe have\n∥Λ1/2O −OΛ∗1/2∥F ≤\n1\np\nσmin(M ∗)\n∥OT ΛO −Λ∗∥F =\n1\nσ∗\nmin\n∥OT ΛO −Λ∗∥F.\n(69)\nNote that Λ = U T MU and Λ∗= U ∗T M ∗U ∗. Thus, we have\n∥OT ΛO −Λ∗∥F = ∥OT U T MUO −U ∗T M ∗U ∗∥F\n≤∥OT U T MUO −OT U T M ∗UO∥F + ∥OT U T M ∗UO −U ∗T M ∗UO∥F\n+ ∥U ∗T M ∗UO −U ∗T M ∗U ∗∥F\n≤∥M −M ∗∥F + 2∥M ∗∥2∥UO −U ∗∥F\n≤∥M −M ∗∥F + 2c\n\u0012σ∗\nmax\nσ∗\nmin\n\u00132\n∥M −M ∗∥F\n≤4c\n\u0012σ∗\nmax\nσ∗\nmin\n\u00132\n∥M −M ∗∥F,\n(70)\n30\nwhere the third inequality follows from Lemma B.3. Combing (69) and (70), we have\n∥Λ1/2O −OΛ∗1/2∥F ≤4c(σ∗\nmax)2\n(σ∗\nmin)3 ∥M −M ∗∥F.\nNow we are ready to prove Lemma 4.2.\nProof of Lemma 4.2. Let Or×r := {O ∈Rr×r | OOT = OT O = Ir}. First of all, we show that for\nany (B, β, O) ∈B × C × O, it holds that PB,β(x, y) = PBO,OT β(x, y). This can be easily seen by\nthe following observation,\nPBO,OT β ∼N\n\u0012\n0,\n\u0014\nBO(BO)T\nBOOT β\nβT OOT BT\n(OT β)T OT β\n\u0015 \u0013\n= N\n\u0012\n0,\n\u0014\nBBT\nBβ\nβT BT\nβT β\n\u0015 \u0013\n∼PB,β.\nBy Lemma B.3, it holds for some constant c > 1 that\n∥UO −U ∗∥F ≤\nc\n(σ∗\nmin)2 ∥BBT −B∗B∗T ∥F.\n(71)\nBy Lemma B.4, it holds for some constant c > 1 that\n∥ΣO −OΣ∗∥F ≤4c(σ∗\nmax)2\n(σ∗\nmin)3 ∥BBT −B∗B∗T ∥F.\n(72)\nLet ˆO := V −1OV ∗∈Or×r. By (71) and (72), we have\n∥B ˆO −B∗∥F = ∥UΣOV ∗−U ∗Σ∗V ∗∥F\n≤∥UΣO −U ∗Σ∗∥F\n≤∥UΣO −UOΣ∗∥F + ∥UOΣ∗−U ∗Σ∗∥F\n≤∥ΣO −OΣ∗∥F + ∥UO −U ∗∥F∥Σ∗∥2\n≤c ·\n\u00124(σ∗\nmax)2\n(σ∗\nmin)3 +\nσ∗\nmax\n(σ∗\nmin)2\n\u0013\n· ∥BBT −B∗B∗T ∥F\n≤5c(σ∗\nmax)2\n(σ∗\nmin)3\n· ∥BBT −B∗B∗T ∥F.\n(73)\nNote that\ndTV\n\u0000PB ˆ\nO(x, z), PB∗(x, z)\n\u0001\n=\nZ\n|pB ˆ\nO(x | z) −pB∗(x | z)|p(z) dxdz\n=\nZ\ndTV\n\u0000N(B ˆOz, Id), N(B∗z, Id)\n\u0001\np(z) dz\n≤\nZ\nmin{1, ∥B ˆOz −B∗z∥2}p(z) dz\n≤min\n\b\n1, E[∥B ˆOz −B∗z∥2]\n\t\n,\n(74)\n31\nwhere the ﬁrst inequality follows from Lemma B.1. We can show that\nE[∥B ˆOz −B∗z∥2] ≤\n\u0010\nE\n\u0002\n∥B ˆOz −B∗z∥2\n2\n\u0003\u00111/2\n=\n\u0010\nE\n\u0002\nzT (B ˆO −B∗)T (B ˆO −B∗)z\n\u0003\u00111/2\n=\n\u0010\nE\n\u0002\nTr\n\u0000(B ˆO −B∗)T (B ˆO −B∗)zzT\u0001\u0003\u00111/2\n=\n\u0010\nTr\n\u0000(B ˆO −B∗)T (B ˆO −B∗)\n\u0001\u00111/2\n= ∥B ˆO −B∗∥F.\n(75)\nBy (73), (74) and (75), it holds that\ndTV\n\u0000PB ˆ\nO(x, z), PB∗(x, z)\n\u0001\n≤min\n\b\n1, ∥B ˆO −B∗∥F\n\t\n≤min\n\u001a\n1, 5c(σ∗\nmax)2\n(σ∗\nmin)3\n· ∥BBT −B∗B∗T ∥F\n\u001b\n≤5c(σ∗\nmax)2\n(σ∗\nmin)3\n·\n\u0000(σ∗\nmax)2 + 1\n\u0001\n· min\n\u001a\n1, ∥BBT −B∗B∗T ∥F\n(σ∗max)2 + 1\n\u001b\n,\n(76)\nwhere the last inequality follows from c > 1 and\n(σ∗\nmax)2 + 1\nσ∗\nmin\n≥2σ∗\nmax\nσ∗\nmin\n> 1.\nBy Lemma B.2, we have\ndTV(pB(x), pB∗(x))\n≥\n1\n100 min\n\b\n1, ∥(B∗B∗T + Id)−1/2(BBT −B∗B∗T )(B∗B∗T + Id)−1/2∥F\n\t\n.\n(77)\nNote that\n∥(B∗B∗T + Id)−1/2(BBT −B∗B∗T )(B∗B∗T + Id)−1/2∥F\n≥∥BBT −B∗B∗T ∥F\n∥B∗B∗T + Id∥2\n≥∥BBT −B∗B∗T ∥F\n(σ∗max)2 + 1\n.\n(78)\nThus, by (77) and (78), it holds that\ndTV(pB(x), pB∗(x)) ≥\n1\n100 min\n\u001a\n1, ∥BBT −B∗B∗T ∥F\n(σ∗max)2 + 1\n\u001b\n(79)\nFinally, by (76) and (79), we have\ndTV\n\u0000PB ˆ\nO(x, z), PB∗(x, z)\n\u0001\n≤500c(σ∗\nmax)2\u0000(σ∗\nmax)2 + 1\n\u0001\n(σ∗\nmin)3\n· dTV(pB(x), pB∗(x))\n≤500c(σ∗\nmax + 1)4\n(σ∗\nmin)3\n· dTV(pB(x), pB∗(x)).\n32\nB.2\nBracketing Number\nBy an application of ǫ-discretization technique, we upper bound the bracketing number of P(B) as\nfollows.\nLemma B.5. Let PX (B) := {N(0, BBT + Id) | B ∈B}, where B = {B ∈Rd×r | ∥B∥2 ≤D} for\nsome D > 0. Then the entropy can be bounded as follows,\nlog N[ ](PX (B), 1/m) ≤4dr log\n\u000024mdr(D2 + 1)\n\u0001\n.\nProof of Lemma B.5. We consider a set of Gaussian distribution\nPX (B) :=\n\u001a\npΣ(x) =\n1\np\n(2π)d|Σ|\ne−1\n2 xT Σ−1x\n\f\f\f\f Σ = BBT + Id, B ∈B\n\u001b\n,\nwhere B = {B ∈Rd×r | ∥B∥2 ≤D}. Note that\nλmax(Σ−1) =\n\u0000λmin(Σ)\n\u0001−1 = 1, λmin(Σ−1) =\n\u0000λmax(Σ)\n\u0001−1 ≥\n1\nD2 + 1.\n(80)\nHere we denote by λmax(Σ−1) and λmin(Σ−1) the largest eigenvalue and the smallest eigenvalue of\nΣ−1, respectively. Our goal is to ﬁnd a 1/m-bracket N[ ](PX (B), 1/m) of PX (B). In other words,\nfor any pΣ(x) ∈PX (B), we need to deﬁne ¯pΣ(x) ∈N[ ](PX (B), 1/m) such that\n• ¯pΣ(x) ≥pΣ(x), ∀x ∈Rd\n•\nR\n|¯pΣ(x) −pΣ(x)| dx ≤1/m.\nNote that rank(BBT ) = r < d and Σ = BBT + Id. Thus, the eigendecomposition of Σ−1 has the\nfollowing form\nΣ−1 = V\n\n\nλ1\n...\nλr\n1\n...\n1\n\n\nV T = U\n\n\nλ1 −1\n...\nλr −1\n\nU T + Id,\n(81)\nwhere V V T = V T V = Id and U ∈Rd×r is the ﬁrst r columns of V . For notation simplicity, we\ndenote\nΛ :=\n\n\nλ1 −1\n...\nλr −1\n\n.\nThus, we have Σ−1 = UΛU T +Id. For some ﬁxed 0 < ǫ ≤(D2+1)−1/2 (which we will choose later),\nif λi ∈[kǫ, (k+1)ǫ) for some k ∈Z, we deﬁne ¯λi := (k−1)ǫ. Note that λi ≥λmin(Σ−1) ≥(D2+1)−1.\nThus, it holds that k ≥2 and ¯λi = (k −1)ǫ ≥ǫ > 0. Moreover, we have ǫ ≤λi −¯λi ≤2ǫ. We deﬁne\n¯Λ :=\n\n\n¯λ1 −1\n...\n¯λr −1\n\n.\n33\nFor the matrix U = (ui,j) ∈Rd×r, if ui,j ∈[\nkǫ\n3\n√\ndr, (k+1)ǫ\n3\n√\ndr ) for some k ∈Z, we deﬁne ¯ui,j :=\nkǫ\n3\n√\ndr\nand ¯U := (¯ui,j) ∈Rd×r. It then holds that\n∥U −¯U∥2 ≤∥U −¯U∥F =\nsX\ni,j\n|ui,j −¯ui,j|2 ≤\n√\ndr ·\nǫ\n3\n√\ndr\n= ǫ\n3.\n(82)\nWe deﬁne\nΣ−1 := ¯U ¯Λ ¯U T + Id.\n(83)\nNote that (D2 + 1)−1 ≤λi ≤1 and |ui,j| ≤1. Thus, we totally have\n\u00121 −(D2 + 1)−1\nǫ\n\u0013r\n·\n\u00126\n√\ndr\nǫ\n\u0013dr\n=\n\u0012\nD2\n(D2 + 1)ǫ\n\u0013r\n·\n\u00126\n√\ndr\nǫ\n\u0013dr\n(84)\nmany ¯Σ−1. Note that for any ∥x∥2 = 1, we have\nxT (Σ−1 −Σ−1)x = xT (U T ΛU −¯U ¯Λ ¯U T )x\n= xT U T (Λ −¯Λ)Ux + xT (U −¯U)T ¯Λ(U + ¯U)x\n≥λmin(Λ −¯Λ) −∥(U −¯U)T ¯Λ(U + ¯U)∥2\n≥λmin(Λ −¯Λ) −∥U −¯U∥2 · ∥¯Λ(U + ¯U)∥2\n≥ǫ −3\n\u0012\n2ǫ +\nD2\nD2 + 1\n\u0013\n∥U −¯U∥2\n≥ǫ −3\n\u0012\n2ǫ +\nD2\nD2 + 1\n\u0013\n· ǫ\n3 ≥0,\nwhere the third inequality follows from\n∥¯Λ(U + ¯U)∥2 ≤∥¯Λ∥2∥U + ¯U∥2 ≤\n\u0012\n2ǫ + 1 −\n1\nD2 + 1\n\u0013\n·\n\u0012\n2 + ǫ\n3\n\u0013\n≤3\n\u0012\n2ǫ +\nD2\nD2 + 1\n\u0013\n.\nand the last inequality follows from our assumption ǫ ≤(D2 + 1)−1/2. Thus, for any x ∈Rd, it\nholds that\nxT (Σ−1 −Σ−1)x ≥0.\n(85)\nWe consider ¯pΣ(x) of the following form\n¯pΣ(x) = c\ns\n|Σ−1|\n(2π)d e−1\n2 xT Σ−1x.\nBy (85), we have: ¯pΣ(x) ≥pΣ(x) holds for any x ∈Rd if and only if\nc ≥\ns\n|Σ−1|\n|Σ−1|\n=\ns\nλ1 . . . λr\n¯λ1 . . . ¯λr\n.\n34\nNote that\nλi\n¯λi\n≤(k + 1)ǫ\n(k −1)ǫ = 1 +\n2\nk −1 ≤1 + 4\nk ≤1 + 4(D2 + 1)ǫ,\nwhere the second inequality follows from k ≥2 and the last inequality follows from kǫ ≥(D2+σ2)−1.\nWe then obtain that\ns\nλ1 . . . λr\n¯λ1 . . . ¯λr\n≤\n\u00001 + 4(D2 + 1)ǫ\n\u0001r/2.\nLet c = (1 + 4(D2 + 1)ǫ)r/2. It then holds that\nc ≥\ns\nλ1 . . . λr\n¯λ1 . . . ¯λr\n,\nwhich implies ¯pΣ(x) ≥pΣ(x) holds for any x ∈Rd. Note that\nZ\n|¯pΣ(x) −pΣ(x)| dx = c −1 = (1 + 4(D2 + 1)ǫ)r/2 −1 ≤4(D2 + 1)ǫr,\nwhere the last inequality follow from (1 + x)r/2 ≤1 + rx for x ≤r−1. Let\nǫ =\n1\n4(D2 + 1)mr .\n(86)\nWe have\nZ\n|¯pΣ(x) −pΣ(x)| dx ≤4(D2 + 1)ǫr = 1\nm.\nBy (84) and (86), we show that\nN[ ](PX (B), 1/m) ≤(4rmD2)r ·\n\u000024rm(D2 + 1)\n√\ndr\n\u0001dr,\nwhich implies\nlog N[ ](PX (B), 1/m) ≤4dr log\n\u000024mdr(D2 + 1)\n\u0001\n.\nB.3\nRademacher Complexity\nNote that for ﬁxed B the prediction function class\nGB,C :=\n\b\ngB,β(x) = βT BT (BBT + σ2Id)−1x\n\f\f β ∈C\n\t\nbelongs to a linear hypothesis class. For a linear hypothesis class H, we can bound its empirical\nRademacher complexity as follows.\n35\nLemma B.6. For a linear hypothesis class H = {hβ(x) = βT x | β ∈Rr, ∥β∥2 ≤D}, where x ∈Rr\nand ∥x∥2 ≤X, the empirical Rademacher complexity can be bounded as follows,\nˆRn(H) ≤2DX\n√n .\nProof of Lemma B.6. Note that\nˆRn(H) = 2\nnEσi\n\u0014\nsup\n∥β∥2≤D\nn\nX\ni=1\nσi · βT xi\n\u0015\n= 2\nnEσi\n\u0014\nsup\n∥β∥2≤D\nβT\n\u0012\nn\nX\ni=1\nσixi\n\u0013\u0015\n≤2\nnEσi\n\u0014\nsup\n∥β∥2≤D\n∥β∥2\n\r\r\r\r\nn\nX\ni=1\nσixi\n\r\r\r\r\n2\n\u0015\n≤2D\nn Eσi\n\u0014sX\ni,j\nσiσjxT\ni xj\n\u0015\n.\nBy Jensen’s inequality, we then have\nˆRn(H) ≤2D\nn Eσi\n\u0014sX\ni,j\nσiσjxT\ni xj\n\u0015\n≤2D\nn\nv\nu\nu\ntEσi\n\u0014 X\ni,j\nσiσjxT\ni xj\n\u0015\n= 2D\nn\nv\nu\nu\nt\nn\nX\ni=1\n∥xi∥2 ≤2DX\n√n .\nB.4\nProofs for Theorem 4.3\nIn this section, we verify the utility of Algorithm 1 by proving Theorem 4.3.\nRecall that the\ntruncated squared loss is deﬁned as\n˜ℓ(x, y) := (y −x)2I{(y−x)2≤L} + L · I{(y−x)2>L},\n(87)\nwhich is L−bounded and 2\n√\nL−Lipschitz w.r.t. the ﬁrst argument. Before proving Theorem 4.3,\nwe need to state some core lemmas. Recall the deﬁnition of gB,β(x):\ngB,β(x) := arg min\ng\nEB,β[ℓ(g(x), y)].\n(88)\nSince ℓis the squared loss, it’s obvious that\ngB,β(x) := arg min\ng\nEB,β[ℓ(g(x), y)] = EPB,β(x,y)[y | x] = βT BT (BBT + Id)−1x.\n(89)\nThe next lemma shows that the optimal predictor under the squared loss ℓand the truncated\nsquared loss ˜ℓstays the same.\nLemma B.7. We denote by ˜gB,β the optimal predictor under truncated squared loss, i.e.,\n˜gB,β ←arg min\ng\nEB,β[˜ℓ(g(x), y)].\n(90)\nIt then holds that\n˜gB,β(x) = EPB,β(x,y)[y | x] = gB,β(x).\n(91)\n36\nProof of Lemma B.7. Notice that, the distribution (under parameter B, β) of y given x is a Gaussian\ndistribution with mean µ = EPB,β(x,y)[y | x] and variance v2 (which is of no importance). We deﬁne\nfunction f as\nf(a) := EB,β[˜ℓ(a, y) | x]\n=\nZ a+\n√\nL\na−\n√\nL\n(y −a)2\n1\nv\n√\n2πe−(y−µ)2\n2v2\ndy +\nZ +∞\na+\n√\nL\nL\n1\nv\n√\n2π e−(y−µ)2\n2v2\ndy\n+\nZ a−\n√\nL\n−∞\nL\n1\nv\n√\n2π\ne−(y−µ)2\n2v2\ndy.\n(92)\nThen, it holds that\nf ′(a) =\nL\nv\n√\n2π e−(a−µ+\n√\nL)2\n2v2\n−\nL\nv\n√\n2πe−(a−µ−\n√\nL)2\n2v2\n+\nZ a+\n√\nL\na−\n√\nL\n2(a −y)\n1\nv\n√\n2π e−(y−µ)2\n2v2\ndy\n−\nL\nv\n√\n2πe−(a−µ+\n√\nL)2\n2v2\n+\nL\nv\n√\n2π e−(a−µ−\n√\nL)2\n2v2\n=\nZ a+\n√\nL\na−\n√\nL\n2(a −y)\n1\nv\n√\n2π e−(y−µ)2\n2v2\ndy\n=\nZ a\na−\n√\nL\n2(a −y)\n1\nv\n√\n2π e−(y−µ)2\n2v2\ndy +\nZ a+\n√\nL\na\n2(a −y)\n1\nv\n√\n2πe−(y−µ)2\n2v2\ndy\n=\nZ √\nL\n0\n2z\n1\nv\n√\n2πe−(a−z−µ)2\n2v2\ndz −\nZ √\nL\n0\n2z\n1\nv\n√\n2π e−(a+z−µ)2\n2v2\ndz\n=\nZ √\nL\n0\n2z\nv\n√\n2π (e−(a−z−µ)2\n2v2\n−e−(a+z−µ)2\n2v2\n)dz.\n(93)\nNotice that for z ∈[0,\n√\nL],\ne−(a−z−µ)2\n2v2\n−e−(a+z−µ)2\n2v2\n> 0 when a > µ,\n(94)\ne−(a−z−µ)2\n2v2\n−e−(a+z−µ)2\n2v2\n< 0 when a < µ.\n(95)\nTherefore, we have f ′(a) < 0 when a < µ, f ′(a) > 0 when a > µ, which implies that a = µ is the\nunique minimizer of f(a), i.e.,\n˜gB,β(x) = EPB,β(x,y)[y | x] = gB,β(x).\n(96)\nThe following lemma shows that the truncation has no signiﬁcant inﬂuence on the excess risk.\nLemma B.8. There exist c2 = (D2 + 1)3, such that\nErrorℓ( ˆB, ˆβ) ≤EB∗,β∗[˜ℓ(g ˆ\nB, ˆβ(x), y)] −EB∗,β∗[˜ℓ(gB∗,β∗(x), y)] +\nr\n2Lc2\nπ\ne−L\n2c2 .\n(97)\n37\nProof of Lemma B.8.\nErrorℓ( ˆB, ˆβ) = EB∗,β∗[ℓ(g ˆ\nB, ˆβ(x), y)] −EB∗,β∗[ℓ(gB∗,β∗(x), y)]\n= EB∗,β∗[ℓ(g ˆ\nB, ˆβ(x), y)] −EB∗,β∗[˜ℓ(g ˆ\nB, ˆβ(x), y)]\n+ EB∗,β∗[˜ℓ(g ˆ\nB, ˆβ(x), y)] −EB∗,β∗[˜ℓ(gB∗,β∗(x), y)]\n+ EB∗,β∗[˜ℓ(gB∗,β∗(x), y)] −EB∗,β∗[ℓ(gB∗,β∗(x), y)]\n(≤0 since ˜ℓ≤ℓ)\n≤sup\nB,β\n{EB∗,β∗[ℓ(gB,β(x), y)] −EB∗,β∗[˜ℓ(gB,β(x), y)]}\n+ EB∗,β∗[˜ℓ(g ˆ\nB, ˆβ(x), y)] −EB∗,β∗[˜ℓ(gB∗,β∗(x), y)]\n(98)\nFor the ﬁrst term, we have\nsup\nB,β\n{EB∗,β∗[ℓ(gB,β(x), y)] −EB∗,β∗[˜ℓ(gB,β(x), y)]}\n= sup\nB,β\n{EB∗,β∗((gB,β(x) −y)2 −L)1{(gB,β(x)−y)2≥L}}.\n(99)\nNotice that\ngB,β(x) −y = βT BT (BBT + Id)−1x −y ∼N(0, λ2),\n(100)\nwhere\nλ2 = V arB∗,β∗[gB,β(x) −y]\n= EB∗,β∗(βT BT (BBT + Id)−1x −y)2\n= ǫ2 + βT BT (BBT + Id)−1(B∗B∗T + Id)(BBT + Id)−1Bβ\n+ β∗T β∗−2βT BT (BBT + Id)−1B∗β∗\n≤ǫ2 + β∗T β∗+ ∥(BBT + Id)−1∥2\n2 · ∥B∗B∗T + Id∥2 · ∥Bβ∥2\n2\n+ 2∥(BBT + Id)−1∥2 · ∥B∗β∗∥2 · ∥Bβ∥2\n≤ǫ2 + β∗T β∗+ D4∥B∗B∗T + Id∥2 + 2D2∥B∗β∗∥2\n≤1 + D2 + D4(D2 + 1) + 2D4\n≤c2.\n(101)\n38\nTherefore\nsup\nB,β\n{EB∗,β∗((gB,β(x) −y)2 −L)1{(gB,β(x)−y)2≥L}}\n= sup\nλ\n2\nZ +∞\n√\nL\n1\nλ\n√\n2π (x2 −L)e−x2\n2λ2 dx\n= 2 sup\nλ\n\u001a\n−\nλ\n√\n2π xe−x2\n2λ2\n\f\f\f\f\n+∞\n√\nL\n+ (λ2 −L)\nZ +∞\n√\nL\n1\nλ\n√\n2π e−x2\n2λ2 dx\n\u001b\n= 2 sup\nλ\n\u001ar\nL\n2πλe−\nL\n2λ2 + (λ2 −L)\nZ +∞\n√\nL\n1\nλ\n√\n2π e−x2\n2λ2 dx\n\u001b\n≤2 sup\nλ\n\u001ar\nL\n2πλe−\nL\n2λ2\n\u001b\n(since L ≥c2 ≥λ2)\n=\nr\n2Lc2\nπ\ne−\nL\n2c2 .\n(102)\nThe last equation holds since λe−\nL\n2λ2 monotone increases w.r.t. λ, and λ ≤√c1. Combining (98),\n(99) and (102), we ﬁnish the proof.\nNow we are ready to prove Theorem 4.3.\nProof of Theorem 4.3. Note that ˜l is L−bounded. By Lemma B.7, we can apply Theorem 3.4 to ˜l,\nwhich gives\nEB∗,β∗[˜ℓ(g ˆ\nB, ˆβ(x), y)] −EB∗,β∗[˜ℓ(gB∗,β∗(x), y)]\n≤2 max\nB∈B Rn\n\u0010\n˜ℓ◦GB,C\n\u0011\n+ L ·\nr\n2\nn log 4\nδ + 12κL ·\nr\n1\nm log 2N[ ](PX (B), 1/m)\nδ\n.\n(103)\nHere κ = c1(σ∗\nmax + 1)4/σ∗3\nmin is the transferability deﬁned in Lemma 4.2.\nBy Lemma B.5, we have\nlog N[ ](P(B), 1/m) ≤4dr log(24mdr(D2 + 1)).\n(104)\nSince ˜l is 2\n√\nL−Lipschitz w.r.t. the ﬁrst argument, the contraction principle (Theoerem 4.12 in\nLedoux & Talagrand (2013)) gives\nRn\n\u0010\n˜ℓ◦GB,C\n\u0011\n≤2\n√\nLRn (GB,C) .\n(105)\n39\nTherefore it remains to bound Rn (GB,C) . By Lemma B.6, for ﬁxed B,\nRn (GB,C) = E{xj}n\nj=1E{σj}n\nj=1[sup\nβ\n2\nn\nn\nX\nj=1\nσjgB,β(xj)]\n= E{xj}n\nj=1E{σj}n\nj=1[sup\nβ\n2\nn\nn\nX\nj=1\nσjβT BT (BBT + Id)−1xj]\n≤E{xj}n\nj=1[ 2D\n√n sup\nj\n∥BT (BBT + Id)−1xj∥2]\n(By Lemma B.6, since ∥β∥2 ≤D)\n= 2D\n√nE{xj}n\nj=1[sup\nj\n∥BT (BBT + Id)−1xj∥2].\n(106)\nNote that xj ∼N(0, B∗B∗T + Id). Therefore BT (BBT + Id)−1xj ∼N(0, Σ), where\nΣ := BT (BBT + Id)−1(B∗B∗T + Id)(BBT + Id)−1B.\n(107)\nThus, we have\nΣ−1\n2 BT (BBT + Id)−1xj ∼N(0, Ir).\n(108)\nLet uj := Σ−1\n2 BT (BBT + Id)−1xj, then\nE{xj}n\nj=1[sup\nj\n∥BT (BBT + Id)−1xj∥2]\n= E{xj}n\nj=1[sup\nj\n∥Σ\n1\n2 uj∥2]\n≤E{xj}n\nj=1[sup\nj\n∥Σ\n1\n2 ∥2∥uj∥2]\n≤sup ∥Σ\n1\n2 ∥2E{xj}n\nj=1[sup\nj\n∥uj∥2].\n(109)\nBy the Theorem 3.1.1 in Vershynin (2018), ∥uj∥−√r is c4−subGaussian for some absolute constant\nc4. Therefore, for any t > 0,\neE[t supj ∥uj∥2] ≤E[et supj ∥uj∥2]\n(by Jensen’s inequality)\n≤\nn\nX\nj=1\nE[et∥uj∥2]\n=\nn\nX\nj=1\nE[et∥uj∥2−√r]et√r\n≤\nn\nX\nj=1\ne\nt2\n2 c4et√r\n= net√r+ t2\n2 c4.\n(110)\nTaking log on both sides, we have\nE[sup\nj\n∥uj∥2] ≤log n\nt\n+ √r + t\n2c4,\n(111)\n40\nwhich holds for any t > 0. Take t =\nq\n2 log n\nc4\n, we get\nE[sup\nj\n∥uj∥2] ≤\np\n2c4 log n + √r.\n(112)\nNote that\n∥Σ∥2 = ∥BT (BBT + Id)−1(B∗B∗T + Id)(BBT + Id)−1B∥2\n≤∥B∥2\n2 · ∥(BBT + Id)−1∥2\n2 · ∥B∗B∗T + Id∥\n≤(D2 + 1)2,\n(113)\ni.e., sup ∥Σ\n1\n2 ∥2 ≤(D2 + 1). Combining (106), (109), (112) and (113), we have\nRn (Gφ,Ψ) ≤2D\n√nE{xj}n\nj=1[sup\nj\n∥BT (BBT + Id)−1xj∥2]\n≤2D\n√n sup ∥Σ\n1\n2 ∥2E{xj}n\nj=1[sup\nj\n∥uj∥2]\n≤2D\n√n(D2 + 1)(\np\n2c4 log n + √r),\n(114)\nwhich implies\nmax\nφ∈Φ Rn\n\u0010\n˜ℓ◦Gφ,Ψ\n\u0011\n≤2\n√\nL max\nφinΦ Rn (Gφ,Ψ) ≤2\n√\nL 2D\n√n(D2 + 1)(\np\n2c4 log n + √r)\n(115)\nWe are now ready to bound the excess risk. By Lemma B.8, we have\nErrorℓ( ˆB, ˆβ) ≤EB∗,β∗[˜ℓ(g ˆ\nB, ˆβ(x), y)] −EB∗,β∗[˜ℓ(gB∗,β∗(x), y)] +\nr\n2Lc2\nπ\ne−L\n2c2\n≤2 max\nφ∈Φ Rn\n\u0010\n˜ℓ◦Gφ,Ψ\n\u0011\n+ L ·\nr\n2\nn log 4\nδ\n+ 12κL ·\nr\n1\nm log 2N[ ](PX (B), 1/m)\nδ\n+\nr\n2Lc2\nπ\ne−L\n2c2\n≤4\n√\nL 2D\n√n(D2 + 1)(\np\n2c4 log n + √r) + L ·\nr\n2\nn log 4\nδ\n+ 12κL\nr\n1\nm(4dr log(24mdr(D2 + 1)) + log(2/δ)) +\nr\n2Lc2\nπ\ne−\nL\n2c2 ,\n(116)\nwhere the second inequality follows from (103) and the last inequality follows from (104), (115).\nHere c4 is an absolute constant. Note that c2 = (D2 + 1)3 and L = c2 log n. Thus, we have\nErrorℓ( ˆB, ˆβ) ≤8\n√\n2c4L\nr\n1\nn + 8L\nr r\nn + L ·\nr\n2\nn log 4\nδ\n+ 12κL\nr\n1\nm(4dr log(24mdr(D2 + 1)) + log(2/δ)) + L\nr\n2\nπn\n≤˜O\n\u0012\nκL\nr\ndr\nm + L\nr r\nn\n\u0013\n,\n(117)\n41\nwhere L = (D2 + 1)3 log n and κ = c1(σ∗\nmax + 1)4/σ∗3\nmin for some absolute constants c1.\nB.5\nProofs for Theorem 4.4\nIn this section, we provide a reﬁned analysis for proving Theorem 4.4. First notice that we can\nrewrite our model (without z) as\ny = β∗T C∗x + w,\n(118)\nwhere β∗∈Rr×1, C∗= B∗T (B∗B∗T +Id)−1 ∈Rr×d, x ∼N(0, B∗B∗T +Id), w ∼N(0, ǫ2 +∥β∗∥2\n2 −\nβ∗T B∗T (B∗B∗T + Id)−1B∗β∗). Here w and x are independent. Therefore we can write our data as\nY = XC∗T β∗+ W,\n(119)\nwhere Y = (y1, · · · , yn)T ∈Rn×1, X = (x1, · · · , xn)T ∈Rn×d, W = (w1, · · · , wn)T ∈Rn×1.\nIn the ﬁrst step (MLE), we obtain an estimator ˆB and the corresponding estimator ˆC =\nˆBT ( ˆB ˆBT + Id)−1. Then our estimator ˆβ for the second step (ERM) is given by\nˆβ = arg min\nβ\n∥Y −X ˆCT β∥2\n2\n= ((X ˆCT )T (X ˆCT ))−1(X ˆCT )T Y\n= ( ˆCXT X ˆCT)−1 ˆCXT Y.\n(120)\nThen our risk is given by\nErrorℓ( ˆB, ˆβ) = EPB∗,β∗(x,y)\n\u0002\u0000y −g ˆ\nB, ˆβ(x)\n\u00012\u0003\n−EPB∗,β∗(x,y)\n\u0002\u0000y −gB∗,β∗(x)\n\u00012\u0003\n= E[(β∗T C∗x + w −ˆβT ˆBT ( ˆB ˆBT + Id)−1x)2] −E[w2]\n= E[(β∗T C∗x −ˆβT ˆCx)2]\n= (β∗T C∗−ˆβT ˆC)(B∗B∗T + Id)(β∗T C∗−ˆβT ˆC)T\n≤∥B∗B∗T + Id∥2∥ˆCT ˆβ −C∗T β∗∥2\n2\n(121)\nOur goal is to bound ∥ˆCT ˆβ −C∗T β∗∥2\n2. Consider the SVD of C∗T and ˆCT , i.e., C∗T = U ∗Λ∗V ∗T ,\nˆCT = ˆU ˆΛˆV T . Then, we have\nˆCT ˆβ −C∗T β∗\n= ˆCT ( ˆCXTX ˆCT )−1 ˆCXTY −C∗T β∗\n= ˆCT ( ˆCXTX ˆCT )−1 ˆCXT(XC∗T β∗+ W) −C∗T β∗\n= ( ˆCT ( ˆCXTX ˆCT )−1 ˆCXT XC∗T −C∗T )β∗+ ˆCT ( ˆCXT X ˆCT)−1 ˆCXT W\n= ( ˆU( ˆU T XTX ˆU)−1 ˆU T XTXU ∗−U ∗)Λ∗V ∗T β∗+ ˆU( ˆU T XT X ˆU)−1 ˆU T XT W.\n(122)\nTherefore\n∥ˆCT ˆβ −C∗T β∗∥2\n2 ≤2∥( ˆU( ˆU T XT X ˆU)−1 ˆU T XT XU ∗−U ∗)∥2\n2∥Λ∗∥2\n2∥β∗∥2\n2\n+ 2∥ˆU( ˆU T XTX ˆU)−1 ˆU T XT W∥2\n2\n(123)\nWe give two lemmas for bounding the related terms. The ﬁrst lemma considers the bias term:\n42\nLemma B.9. Let Σ := B∗B∗T + Id. If n ≳∥Σ∥2r log(1/δ), then with probability at least 1 −δ,\n∥( ˆU( ˆU T XTX ˆU)−1 ˆU T XTXU ∗−U ∗)∥2\n2 ≤O(∥Σ∥2∆2),\n(124)\nwhere ∆= dist( ˆU, U ∗) := ∥ˆU ˆU T −U ∗U ∗T ∥.\nThe second lemma considers the variance term:\nLemma B.10. Let Σ := B∗B∗T + Id. If n ≳∥Σ∥2r log(1/δ), then with probability at least 1 −δ,\n∥ˆU( ˆU T XT X ˆU)−1 ˆU T XT W∥2\n2 ≤O\n\u0012σ2r log(4/δ)\nn\n\u0013\n,\n(125)\nwhere σ2 := E(w2) = ǫ2 + ∥β∗∥2\n2 −β∗T B∗T (B∗B∗T + Id)−1B∗β∗is the variance of w.\nUsing this two lemmas together with the decomposition (123), we have\n∥ˆCT ˆβ −C∗T β∗∥2\n2 ≤O\n\u0012\n∥β∗∥2∥Λ∗∥2∥Σ∥2∆2 + σ2r log(4/δ)\nn\n\u0013\n.\n(126)\nNow it remains to control ∆, which is related to the estimation error of the ﬁrst step (MLE). The\nfollowing lemma gives an upper bound for ∆.\nLemma B.11. If m ≳∥Σ∥2d log(1/δ), then with probability at least 1 −δ,\n∆2 ≤O\n\u0012\n∥Σ∥2 d log(1/δ)\nm\nλ−2\nr (C∗T C∗)\n\u0013\n,\n(127)\nwhere λr(C∗T C∗) is the r-th (smallest) nonzero eigenvalue of C∗T C∗.\nProof for Theorem 4.4. By Lemma B.9, B.10, B.11, we have\nErrorℓ( ˆB, ˆβ) ≤∥Σ∥∥ˆCT ˆβ −C∗T β∗∥2\n2\n≤O(∥β∗∥2∥Λ∗∥2∥Σ∥3∆2 + ∥Σ∥σ2r log(4/δ)\nn\n).\n≤O(∥β∗∥2∥Λ∗∥2∥Σ∥5λ−2\nr (C∗T C∗)d log(1/δ)\nm\n+ ∥Σ∥σ2r log(4/δ)\nn\n).\n(128)\nUsing the assumptions that ∥β∗∥≤D and ∥B∗∥≤D, we can bound these terms by D and\nquantities related to ground truth. First notice that Σ have eigenvalues σ∗2\n1 + 1 ≥σ∗2\n2 + 1 ≥· · · ≥\nσ∗2\nr + 1 ≥1 = · · · = 1, where σ∗\ni are singular values of B∗, therefore ∥Σ∥≤D2 + 1. Also, since\nC∗T C∗= (B∗B∗T + Id)−1B∗B∗T (B∗B∗T + Id)−1\n= (B∗B∗T + Id)−1 −(B∗B∗T + Id)−2\n= Σ−1 −Σ−2,\n(129)\nwe know that C∗T C∗has r nonzero eigenvalues {(σ∗\ni +σ∗−1\ni\n)−2}r\ni=1. Therefore ∥Λ∗∥2 = ∥C∗T C∗∥≤\n1/4,\nλ−2\nr (C∗T C∗) ≤max((σ∗\n1 + σ∗−1\n1\n)4, (σ∗\nr + σ∗−1\nr\n)4)\n≤O(D4 + σ∗−4\nr\n).\n(130)\n43\nFor σ2, we have\nσ2 = ǫ2 + ∥β∗∥2\n2 −β∗T B∗T (B∗B∗T + Id)−1B∗β∗\n≤1 + ∥β∗∥2∥Ir −B∗T (B∗B∗T + Id)−1B∗∥\n≤1 + D2.\n(131)\nCombine all this bounds, we have\nErrorℓ( ˆB, ˆβ) ≤O(∥β∗∥2∥Λ∗∥2∥Σ∥5λ−2\nr (C∗T C∗)d log(1/δ)\nm\n+ ∥Σ∥σ2r log(4/δ)\nn\n).\n≤O((D2 + 1)6(D4 + σ∗−4\nmin)d log(1/δ)\nm\n+ (D2 + 1)2 r log(4/δ)\nn\n).\n(132)\nIn the sequel, we give the proofs of Lemma B.9, B.10 and B.11. We ﬁrst prove some additional\ntechnical lemmas. The following lemma, which is a simple corollary of Tripuraneni et al. (2021)\nLemma 20, shows the concentration property of empirical covariance matrix.\nLemma B.12. Let Σ ∈Rd be a positive deﬁnite matrix. Let {xi}n\ni=1 be d−dimensional Gaussian\nrandom vectors i.i.d. sample from N(0, Σ), X = (x1, · · · , xn)T ∈Rn×d. Then for any A, B ∈Rd×r,\nwe have with probability at least 1 −δ\n∥AT (XT X\nn\n)B −AT ΣB∥2 ≤O(∥A∥∥B∥∥Σ∥(\nr r\nn + r\nn +\nr\nlog(1/δ)\nn\n+ log(1/δ)\nn\n).\n(133)\nProof. We write the SVD of A and B: A = U1Λ1V T\n1 , B = U2Λ2V T\n2 , where U1, U2 ∈Rd×r,\nΛ1, Λ2, V1, V2 ∈Rr×r. Then\n∥AT (XT X\nn\n)B −AT ΣB∥2 = ∥V1Λ1U T\n1 (XTX\nn\n)U2Λ2V T\n2 −V1Λ1U T\n1 ΣU2Λ2V T\n2 ∥2\n≤∥V1Λ1∥∥U T\n1 (XTX\nn\n)U2 −U T\n1 ΣU2∥∥Λ2V T\n2 ∥\n≤∥A∥∥B∥∥U T\n1 (XT X\nn\n)U2 −U T\n1 ΣU2∥.\n(134)\nNow since U1, U2 ∈Rd×r are projection matrices, we can apply Tripuraneni et al. (2021) Lemma\n20, therefore\n∥U T\n1 (XT X\nn\n)U2 −U T\n1 ΣU2∥≤O(∥Σ∥(\nr r\nn + r\nn +\nr\nlog(1/δ)\nn\n+ log(1/δ)\nn\n))\n(135)\nwhich gives what we want.\nThe following lemma is a basic matrix perturbation result (see Tripuraneni et al. (2021) Lemma\n25).\nLemma B.13. Let A be a positive deﬁnite matrix and E another matrix which satisﬁes ∥EA−1∥≤\n1\n4, then F := (A + E)−1 −A−1 satisﬁes ∥F∥≤4\n3∥A−1∥∥EA−1∥.\n44\nWith these two technical lemmas, we are able to prove Lemma B.9, B.10.\nProof of Lemma B.9. We consider ˆU ∈Rd×r and ˆU T\n⊥∈Rd×(d−r) be orthonormal projection\nmatrices spanning orthogonal subspaces which are rank r and rank d −r respectively, so that\nrange( ˆU) ⊕range( ˆU⊥) = Rd. Then ∆= dist( ˆU, U ∗) = ∥ˆU T\n⊥U ∗∥2 (see Chen et al. (2021) Lemma\n2.5). Notice that Id = ˆU ˆU T + ˆU⊥ˆU T\n⊥, we have\nˆU( ˆU T XT X ˆU)−1 ˆU T XT XU ∗−U ∗\n= ˆU( ˆU T XTX ˆU)−1 ˆU T XTX( ˆU ˆU T + ˆU⊥ˆU T\n⊥)U ∗−U ∗\n= ˆU( ˆU T XTX ˆU)−1 ˆU T XTX ˆU ˆU T U ∗+ ˆU( ˆU T XT X ˆU)−1 ˆU T XT X ˆU⊥ˆU T\n⊥U ∗−U ∗\n= ˆU( ˆU T XTX ˆU)−1 ˆU T XTX ˆU⊥ˆU T\n⊥U ∗+ ˆU ˆU T U ∗−U ∗\n= ˆU( ˆU T XTX ˆU)−1 ˆU T XTX ˆU⊥ˆU T\n⊥U ∗−ˆU⊥ˆU T\n⊥U ∗\n(136)\nTherefore\n∥ˆU( ˆU T XTX ˆU)−1 ˆU T XTXU ∗−U ∗∥2\n2 ≤2∥ˆU( ˆU T XT X ˆU)−1 ˆU T XTX ˆU⊥ˆU T\n⊥U ∗∥2\n2 + 2∥ˆU⊥ˆU T\n⊥U ∗∥2\n2.\n(137)\nFor the second term,\n∥ˆU⊥ˆU T\n⊥U ∗∥2\n2 ≤∥ˆU⊥|2∥ˆU T\n⊥U ∗∥2 ≤∆2.\n(138)\nFor the ﬁrst term,\n∥ˆU( ˆU T XT X ˆU)−1 ˆU T XT X ˆU⊥ˆU T\n⊥U ∗∥\n= ∥ˆU( ˆU T XTX\nn\nˆU)−1 ˆU T XTX\nn\nˆU⊥ˆU T\n⊥U ∗∥\n= ∥ˆU(( ˆU T Σ ˆU)−1 + F)( ˆU T Σ ˆU⊥ˆU T\n⊥U ∗+ E1)∥\n≤∥( ˆU T Σ ˆU)−1( ˆU T Σ ˆU⊥ˆU T\n⊥U ∗)∥+ ∥( ˆU T Σ ˆU)−1E1∥+ ∥F ˆU TΣ ˆU⊥ˆU T\n⊥U ∗∥+ ∥FE1∥,\n(139)\nwhere E1 = ˆU T XT X\nn\nˆU⊥ˆU T\n⊥U ∗−ˆU T Σ ˆU⊥ˆU T\n⊥U ∗, F = ( ˆU T XT X\nn\nˆU)−1−( ˆU T Σ ˆU)−1. In order to bound\n∥F∥, let E = ˆU T XT X\nn\nˆU −ˆU T Σ ˆU, then by Lemma B.12, with probability at least 1 −δ,\n∥E∥≤O(∥Σ∥(\nr r\nn + r\nn +\nr\nlog(1/δ)\nn\n+ log(1/δ)\nn\n)).\n(140)\nTherefore, since λmin(Σ) = 1,\n∥E( ˆU T Σ ˆU)−1∥≤∥E∥∥( ˆU TΣ ˆU)−1∥\n≤∥E∥λmin(Σ)−1\n≤O(∥Σ∥(\nr r\nn + r\nn +\nr\nlog(1/δ)\nn\n+ log(1/δ)\nn\n))\n(141)\n45\nNotice that n ≳∥Σ∥2r log(1/δ) implies p r\nn + r\nn +\nq\nlog(1/δ)\nn\n+ log(1/δ)\nn\n≲∥Σ∥−1. Thus, we show\nthat when n is large enough, we have ∥E( ˆU T Σ ˆU)−1∥≤1\n4. Therefore we can apply Lemma B.13,\nwhich gives\n∥F∥≤4\n3∥E( ˆU TΣ ˆU)−1∥∥( ˆU TΣ ˆU)−1∥\n≤4\n3 × 1\n4∥( ˆU T Σ ˆU)−1∥\n≤1\n3.\n(142)\nAs for ∥E1∥, directly applying Lemma B.12, using n ≳∥Σ∥2r log(1/δ), we get\n∥E1∥≤O(∥Σ∥∥ˆU⊥ˆU T\n⊥U ∗∥(\nr r\nn + r\nn +\nr\nlog(1/δ)\nn\n+ log(1/δ)\nn\n))\n≤O(∥Σ∥∆∥Σ∥−1)\n≤O(∆)\n(143)\nCombining (139),(142)and(143), we have\n∥ˆU( ˆU T XT X ˆU)−1 ˆU T XT X ˆU⊥ˆU T\n⊥U ∗∥\n≤∥( ˆU T Σ ˆU)−1( ˆU T Σ ˆU⊥ˆU T\n⊥U ∗)∥+ ∥( ˆU TΣ ˆU)−1E1∥+ ∥F ˆU T Σ ˆU⊥ˆU T\n⊥U ∗∥+ ∥FE1∥\n≤∥( ˆU T Σ ˆU)−1∥∥( ˆU T Σ ˆU⊥ˆU T\n⊥U ∗)∥+ ∥( ˆU TΣ ˆU)−1∥∥E1∥+ ∥F∥∥ˆU TΣ ˆU⊥ˆU T\n⊥U ∗∥+ ∥F∥∥E1∥\n≤λmin(Σ)−1∥Σ∥∥ˆU T\n⊥U ∗∥+ λmin(Σ)−1∥E1∥+ ∥F∥∥Σ∥∥ˆU T\n⊥U ∗∥+ ∥F∥∥E1∥\n≤λmin(Σ)−1∥Σ∥∆+ λmin(Σ)−1O(λmin(Σ)∆) + 1\n3λmin(Σ)−1∥Σ∥∆+ 1\n3λmin(Σ)−1O(λmin(Σ)∆)\n≤O(∥Σ∥∆)\n(144)\nFinally, combining (137),(138) and (144), we get\n∥( ˆU( ˆU T XTX ˆU)−1 ˆU T XTXU ∗−U ∗)∥2\n2 ≤O(∥Σ∥2∆2),\n(145)\nwith probability at least 1 −δ, which is what we want.\nProof of Lemma B.10.\n∥ˆU( ˆU T XTX ˆU)−1 ˆU T XTW∥2\n2 ≤∥( ˆU TXT X ˆU)−1 ˆU T XTW∥2\n2\n= (( ˆU T XT X ˆU)−1 ˆU T XT W)T (( ˆU T XT X ˆU)−1 ˆU T XT W)\n= W T ( 1\nn\nX ˆU\n√n ( ˆU T XT X\nn\nˆU)−2 ˆU T XT\n√n\n)W.\n(146)\nLet A = 1\nn\nX ˆU\n√n ( ˆU T XT X\nn\nˆU)−2 ˆUT XT\n√n , W = σV , then V ∼N(0, In). By Hanson-Wright inequality\n(see Vershynin (2018) Theorem 6.2.1),\nP(|V T AV −E[V T AV ]| ≥t) ≤2 exp(−c min(\nt2\n∥A∥2\nF\n,\nt\n∥A∥2\n)).\n(147)\n46\nHence with probability at least 1 −δ,\nV T AV ≤E[V T AV ] + O(∥A∥F\nr\nlog 2\nδ ) + O(∥A∥2 log 2\nδ ).\n(148)\nNotice that E[V T AV ] = Tr(A), therefore it remains to bound Tr(A), ∥A∥F and ∥A∥2. If we deﬁne\nB = X ˆU\n√n ∈Rn×r, then A = 1\nnB(BT B)−2BT . Therefore\nTr(A) = Tr( 1\nnB(BT B)−2BT )\n= 1\nnTr((BT B)−2BT B)\n= 1\nnTr((BT B)−1)\n≤r\nn∥(BT B)−1∥2\n(149)\nLet the SVD of B be B = PMQT , where P ∈Rn×r, M, Q ∈Rr×r, then\n∥A∥2 = 1\nn∥B(BT B)−2BT ∥2\n= 1\nn∥PMQT(QM 2QT )−2QMP T∥2\n= 1\nn∥PM −2P T ∥2\n≤1\nn∥M −2∥2\n= 1\nn∥(BT B)−1∥2\n(150)\nAlso notice that A is rank r, therefore ∥A∥F ≤√r∥A∥2. Thus it remains to bound ∥(BT B)−1∥2 =\n∥( ˆU T XT X\nn\nˆU)−1∥2. Let F = ( ˆU T XT X\nn\nˆU)−1 −( ˆU T Σ ˆU)−1. Recall (142), which states that with\nprobability at least 1 −δ, we have ∥F∥≤1\n3λmin(Σ)−1. Therefore\n∥( ˆU T XTX\nn\nˆU)−1∥= ∥( ˆU T Σ ˆU)−1 + F∥\n≤∥( ˆU T Σ ˆU)−1∥+ ∥F∥\n≤O(λmin(Σ)−1).\n(151)\nThus ∥A∥≤O( 1\nnλmin(Σ)−1), ∥A∥F ≤O(\n√r\nn λmin(Σ)−1), Tr(A) ≤O( r\nnλmin(Σ)−1). Therefore\nwith probability at least 1 −2δ,\nV T AV ≤E[V T AV ] + O(∥A∥F\nr\nlog 2\nδ ) + O(∥A∥2 log 2\nδ )\n≤O( r\nnλmin(Σ)−1) + O(\n√r\nn λmin(Σ)−1\nr\nlog 2\nδ ) + O( 1\nnλmin(Σ)−1 log 2\nδ )\n≤O( r\nnλmin(Σ)−1 log 2\nδ )\n= O( r\nn log 2\nδ ).\n(152)\n47\nThe last line holds since λmin(Σ) = 1. Recall\n∥ˆU( ˆU T XT X ˆU)−1 ˆU T XT W∥2\n2 = W T AW = σ2V T AV,\n(153)\ncombining this with the above bound for V T AV yields our desired result.\nFinally we prove Lemma B.11 in the following.\nProof of Lemma B.11. In the ﬁrst step, we have m unlabeled data {xi}m\ni=1 i.i.d.\nsample from\nN(0, Σ). Let ˆΣ =\n1\nm\nPm\ni=1 xixT\ni be the empirical covariance matrix. Then by Lemma B.12, with\nprobability at least 1 −δ,\n∥Σ −ˆΣ∥≤O(∥Σ∥(\nr\nd\nm + d\nm +\nr\nlog(1/δ)\nm\n+ log(1/δ)\nm\n))\n(154)\nWe claim that\n∥ˆB ˆBT −(ˆΣ −Id)∥2 ≤∥ˆΣ −Σ∥,\n(155)\nand the proof of this claim will be at the end of this section. With the claim,\n∥ˆB ˆBT −B∗B∗T ∥= ∥ˆB ˆBT −(ˆΣ −Id) + (ˆΣ −Id) −(Σ −Id)∥\n≤∥ˆB ˆBT −(ˆΣ −Id)∥+ ∥Σ −ˆΣ∥\n≤2∥Σ −ˆΣ∥.\n(156)\nNotice that\nC∗T C∗= (B∗B∗T + Id)−1B∗B∗T (B∗B∗T + Id)−1\n= (B∗B∗T + Id)−1 −(B∗B∗T + Id)−2\n(157)\nSimilarly\nˆCT ˆC = ( ˆB ˆBT + Id)−1 −( ˆB ˆBT + Id)−2.\n(158)\nLet E2 = ( ˆB ˆBT + Id) −(B∗B∗T + Id), F2 = ( ˆB ˆBT + Id)−1 −(B∗B∗T + Id)−1. Then\n∥E2∥≤2∥Σ −ˆΣ∥≤O(∥Σ∥(\nr\nd\nm + d\nm +\nr\nlog(1/δ)\nm\n+ log(1/δ)\nm\n)).\n(159)\nTherefore when m ≳∥Σ∥2d log(1/δ), ∥E2∥≤O(∥Σ∥\nq\nd log(1/δ)\nm\n), ∥E2Σ−1∥≤∥E2∥∥Σ−1∥≤1/4.\nThen we can apply Lemma B.13, which gives\n∥F2∥≤4\n3∥Σ−1∥∥E2Σ−1∥\n≤4\n3∥Σ−1∥2∥E2∥\n≤O(λ−2\nmin(Σ)∥Σ∥\nr\nd log(1/δ)\nm\n)\n= O(∥Σ∥\nr\nd log(1/δ)\nm\n).\n(160)\n48\nThe last line holds since λmin(Σ) = 1. Thus\n∥C∗T C∗−ˆCT ˆC∥= ∥(Σ−1 + F2) −(Σ−1 + F2)2 −(Σ−1 −Σ−2)∥\n= ∥F2 −Σ−1F2 −F2Σ−1 −F 2\n2 ∥\n≤∥F2∥+ 2∥Σ−1∥∥F2∥+ ∥F2∥2\n≤O(∥Σ∥\nr\nd log(1/δ)\nm\n).\n(161)\nTherefore by Davis-Kahan theorem,\n∆= dist(U ∗, ˆU) ≤O(λ−1\nr (C∗T C∗)∥C∗T C∗−ˆCT ˆC∥).\n(162)\nCombining the above three inequalities, we have\n∆2 ≤O(∥Σ∥2 d log(1/δ)\nm\nλ−2\nr (C∗T C∗)).\n(163)\nFinally we will need to prove the claim (155). Notice that the MLE estimator ˆB is given by\nˆB = arg max\nB∈Rd×r\nm\nX\ni=1\npB(xi)\n= arg max\nB∈Rd×r (−log det(BBT + Id) −Tr(ˆΣ(BBT + Id)−1))\n= arg min\nB∈Rd×r (log det(BBT + Id) + Tr(ˆΣ(BBT + Id)−1))\n(164)\nLet ˆΣ = ˆU ˆΛ ˆU T and (BBT + Id) = UΛU T , where ˆU and U are orthogonal matrices, ˆΛ =\ndiag(ˆλ1, · · · , ˆλd), Λ = diag(λ1, · · · , λd) and ˆλ1 ≥. . . ≥ˆλd, λ1 ≥. . . ≥λd. Since rank(BBT ) ≤r,\nwe have λr+1 = . . . λd = 1. By Ruhe’s trace inequality (see P341 of Marshall et al. (2011)), we\nhave\nTr(ˆΣ(BBT + Id)−1)) ≥\nd\nX\nj=1\nλ−1\nj ˆλj,\n(165)\nand the equality holds only when the two matrices have simultaneous ordered spectral decomposi-\ntion, i.e., U = ˆU. Therefore\nmin\nB∈Rd×r(log det(BBT + Id) + Tr(ˆΣ(BBT + Id)−1))\n=\nmin\n{λj}d\nj=1\nd\nX\nj=1\n(log λj + λ−1\nj ˆλj)\nsubject to λ1 ≥· · · ≥λr ≥λr+1 = · · · = λd = 1\n(166)\nand the minimum is achieved when λj = ˆλj, for j = 1, · · · , r.\nTherefore the MLE estimator\nˆB satisﬁes ( ˆB ˆBT + Id) = ˆUΛ ˆU T where Λ = diag(ˆλ1, · · · , ˆλr, 1, · · · , 1). Thus, we have ˆB ˆBT =\n49\nˆU(Λ −Id) ˆU T , which implies\n∥ˆB ˆBT −(ˆΣ −Id)∥2\n= ∥ˆU(Λ −Id) ˆU T −ˆU(ˆΛ −Id) ˆU T ∥\n≤∥Λ −ˆΛ∥\n=\nmax\nj=r+1,··· ,d |ˆλj −1|\n≤\nmax\nj=1,··· ,d |ˆλj −λj(Σ)|\n≤∥ˆΣ −Σ∥.\n(167)\nHere the last inequality follows from Weyl’s Theorem. Thus, we prove claim (155).\nC\nProofs for Section 5\nIn Section C.1, we show that GMM with classiﬁcation as downstream tasks has c2-transferability\nfor some absolute constants c2 (Lemma 5.2). In Section C.2 and Section C.3, we prove two lemmas\nthat will be used in the proof of Theorem 5.3. To be speciﬁc, in Section C.2, we upper bound the\nbracketing number of the set P(U) by using ǫ-discretization (Lemma C.5). In Section C.3, we prove\nLemma C.6, which will be used to upper bound the Rademacher complexity of the function class\nℓ◦Gu,Ψ. Finally, in Section C.4, we prove Theorem 5.3.\nC.1\nProofs for Lemma 5.2\nBefore going to the proof of this theorem, we ﬁrst state some basic deﬁnitions and useful lemmas.\nWe deﬁne the balls of radius 8√d log K around each u∗\ni and ui as\nΩ∗\ni :=\nn\nx ∈Rd | ∥x −u∗\ni ∥≤8\np\nd log K\no\n(168)\nΩi :=\nn\nx ∈Rd | ∥x −ui∥≤8\np\nd log K\no\n(169)\nWe denote the p.d.f of N (ui, Id) and N (u∗\ni , Id) by Pi and P ∗\ni respectively.\nLemma C.1. If\ndTV (pu(x), pu∗(x)) ≤\n1\n4K ,\n(170)\nthen there exists a permutation of u such that ∥u∗\ni −ui∥≤16√d log K holds for every 1 ≤i ≤K.\nBefore proving Lemma C.1, we ﬁrst state a useful result of Gaussian norm concentration.\nLemma C.2. Let X ∼N (0, Id), then\nP(∥X∥≥t) ≤2 exp(−t2\n16d).\n(171)\n50\nProof. This is a simple application of Jin et al. (2019) Lemma 1.3. Notice that X is 1-subGaussian,\ntherefore taking σ =\n√\nd in Jin et al. (2019) Lemma 1.3 yields what we want.\nProof of Lemma C.1. We prove by contradiction. If the statement is not true, since the separation\nsatisﬁes 100√d log K ≥2·16√d log K, there must exist a u∗\ni (W.L.O.G., denote it by u∗\n1), such that\n∥u∗\n1 −uj∥> 16√d log K for any 1 ≤j ≤K. Then\n2dTV (pu(x), pu∗(x)) =\nZ\nRd\n\f\f\f\f\n1\nK\nK\nX\ni=1\nP ∗\ni −1\nK\nK\nX\ni=1\nPi\n\f\f\f\fdx\n≥\nZ\nΩ∗\n1\n\f\f\f\f\n1\nK\nK\nX\ni=1\nP ∗\ni −1\nK\nK\nX\ni=1\nPi\n\f\f\f\fdx\n≥\nZ\nΩ∗\n1\n1\nK\nK\nX\ni=1\nP ∗\ni dx −\nZ\nΩ∗\n1\n1\nK\nK\nX\ni=1\nPidx\n≥\nZ\nΩ∗\n1\n1\nK P ∗\n1 dx −1\nK\nK\nX\ni=1\nZ\nΩ∗\n1\nPidx\n= 1\nK P(N (u∗\n1, Id) ∈Ω∗\n1) −1\nK\nK\nX\ni=1\nP(N (ui, Id) ∈Ω∗\n1)\n(172)\nSince ∥u∗\n1 −ui∥> 16√d log K, therefore Ω∗\n1 ∩Ωi = ∅, which implies (by Lemma C.2)\nP(N (ui, Id) ∈Ω∗\n1) ≤P(N (ui, Id) ∈ΩC\ni ) ≤2 exp(−(8√d log K)2\n16d\n) = 2e−4 log K\n(173)\nAlso, by Lemma C.2,\nP(N (u∗\n1, Id) ∈Ω∗\n1) ≥1 −2 exp(−(8√d log K)2\n16d\n) = 1 −2e−4 log K\n(174)\nTherefore,\n2dTV (pu(x), pu∗(x)) ≥1\nK P(N (u∗\n1, Id) ∈Ω∗\n1) −1\nK\nK\nX\ni=1\nP(N (ui, Id) ∈Ω∗\n1)\n≥1\nK (1 −2e−4 log K) −1\nK\nK\nX\ni=1\n2e−4 log K\n= 1\nK −(2 + 2\nK )e−4 log K\n≥1\nK −3e−4 log K\n= 1\nK −3( 1\nK )4\n=\n1\n2K\n(175)\nwhich is a contradiction.\n51\nWe then state the core lemmas of proving Lemma 5.2.\nLemma C.3. If for any i, ∥ui −u∗\ni ∥≤16√d log K, then for Ω∗\n1 (corresponding results hold for\neach Ω∗\ni ),\nZ\nΩ∗\n1\n|P ∗\n1 −P1|dx ≥c1 min {∥u∗\n1 −u1∥, 1} ,\n(176)\nwhere c1 =\n1\n200.\nLemma C.4. If for any i, ∥ui −u∗\ni ∥≤16√d log K, then for Ω∗\n1 (corresponding results hold for\neach Ω∗\ni ), then for every j ̸= 1,\nZ\nΩ∗\n1\n|P ∗\nj −Pj|dx ≤c2\nK min\n\b\n∥u∗\nj −uj∥, 1\n\t\n,\n(177)\nwhere c2 = 2688\n\u00001\n2\n\u000169 .\nWith these lemmas, we are now able to prove Lemma 5.2.\nProof of Lemma 5.2. By Lemma C.1, there exists a permutation of u such that ∥u∗\ni −ui∥≤\n16√d log K holds for every 1 ≤i ≤K. Therefore Lemma C.3, C.4 can be applied. Notice that\nZ\nΩ∗\n1\n|pu(x) −pu∗(x)|dx =\nZ\nΩ∗\n1\n\f\f\f\f\n1\nK\nK\nX\ni=1\nP ∗\ni −1\nK\nK\nX\ni=1\nPi\n\f\f\f\fdx\n≥\nZ\nΩ∗\n1\n\f\f\f\f\n1\nK P ∗\n1 −1\nK Pi\n\f\f\f\fdx −\nZ\nΩ∗\n1\n\f\f\f\f\n1\nK\nK\nX\ni=2\nP ∗\ni −1\nK\nK\nX\ni=2\nPi\n\f\f\f\fdx\n≥1\nK\nZ\nΩ∗\n1\n|P ∗\n1 −Pi|dx −1\nK\nK\nX\ni=2\nZ\nΩ∗\n1\n|P ∗\ni −Pi|dx\n≥c1\nK min {∥u∗\n1 −u1∥, 1} −c2\nK2\nK\nX\ni=2\nmin {∥u∗\ni −ui∥, 1} ,\n(178)\nwhere the last line comes from Lemma C.3, C.4.\nSum up all the equations above for corresponding 1 ≤i ≤K, since {Ω∗\ni }K\ni=1 are disjoint, we\n52\nhave\ndTV (pu(x), pu∗(x)) = 1\n2\nZ\nRd |pu(x) −pu∗(x)|dx\n≥1\n2\nK\nX\ni=1\nZ\nΩ∗\ni\n|pu(x) −pu∗(x)|dx\n≥1\n2\n\u0012c1\nK −(K −1)c2\nK2\n\u0013 K\nX\ni=1\nmin {∥u∗\ni −ui∥, 1}\n≥1\n2 (c1 −c2) · 1\nK\nK\nX\ni=1\nmin {∥u∗\ni −ui∥, 1}\n= 1\n2\n \n1\n200 −2688\n\u00121\n2\n\u001369!\n· 1\nK\nK\nX\ni=1\nmin {∥u∗\ni −ui∥, 1}\n≥\n1\n500 · 1\nK\nK\nX\ni=1\nmin {∥u∗\ni −ui∥, 1} .\n(179)\nIn the end, we refer to Lemma B.1, which states that\ndTV(N(u∗\ni , Id), N(ui, Id)) ≤min(∥u∗\ni −ui∥, 1).\n(180)\nTake σ(u) = {ui}K\ni=1,\ndTV\n\u0000pσ(u)(x, z), pu∗(x, z)\n\u0001\n=\nK\nX\ni=1\nP(z = i)dTV(N(u∗\ni , Id), N(ui, Id))\n≤\nK\nX\ni=1\n1\nK min(∥u∗\ni −ui∥, 1)\n≤500dTV (pu(x), pu∗(x)) .\n(181)\nFinally we state the proof of Lemma C.3 and C.4.\nProof of Lemma C.3. W.L.O.G.,let u∗\n1 = 0, ∆:= ∥u1∥≤16√d log K, and u1 = (−∆, 0, 0, · · · , 0).\nThe densities are given by\nP ∗\n1 (x) = (\n1\n√\n2π)de−1\n2 ∥x∥2\n(182)\nP1(x) = (\n1\n√\n2π\n)de−1\n2 ∥x−u1∥2\n(183)\nWe consider an area S ⊂Ω∗\n1:\nS :=\n\u001a\nx = (x1, · · · , xd)\n\f\f\f\fx ∈Ω∗\n1, x1 ≥1\n10\n\u001b\n(184)\n53\nThen for any x ∈S, ∥x∥2 ≤∥x −u1∥2, which implies P ∗\n1 (x) ≥P1(x). Therefore\nZ\nΩ∗\n1\n|P ∗\n1 −P1|dx ≥\nZ\nS\n|P ∗\n1 −P1|dx\n=\nZ\nS\n(\n1\n√\n2π)d \u0010\ne−1\n2 ∥x∥2 −e−1\n2 ∥x−u1∥2\u0011\ndx\n=\nZ\nS\n(\n1\n√\n2π)de−1\n2 ∥x∥2 \u0010\n1 −e\n1\n2 ∥x∥2−1\n2 ∥x−u1∥2\u0011\ndx\n≥min\nx∈S\n\u0010\n1 −e\n1\n2 ∥x∥2−1\n2 ∥x−u1∥2\u0011 Z\nS\n(\n1\n√\n2π )de−1\n2 ∥x∥2dx\n= min\nx∈S\n\u0010\n1 −e\n1\n2 ∥x∥2−1\n2 ∥x−u1∥2\u0011\nP(N (0, Id) ∈S)\n(185)\nFor minx∈S\n\u0010\n1 −e\n1\n2 ∥x∥2−1\n2 ∥x−u1∥2\u0011\n, notice that for any x = (x1, · · · , xd) ∈S,\n1\n2∥x∥2 −1\n2∥x −u1∥2 = −x1∆−1\n2∆2 ≤−1\n10∆\n(186)\nThus\nmin\nx∈S\n\u0010\n1 −e\n1\n2 ∥x∥2−1\n2 ∥x−u1∥2\u0011\n≥1 −e−1\n10 ∆\n(187)\nTake c3 =\n1\n20. We claim that\nmin\nx∈S\n\u0010\n1 −e\n1\n2 ∥x∥2−1\n2 ∥x−u1∥2\u0011\n≥c3 min{∆, 1}.\n(188)\nIn fact, when 0 ≤∆≤1,\nmin\nx∈S\n\u0010\n1 −e\n1\n2 ∥x∥2−1\n2 ∥x−u1∥2\u0011\n≥1 −e−1\n10 ∆≥1\n20∆.\n(189)\nThe last inequality holds, since if we let f(x) = e−1\n10 x + 1\n20x −1, Then f(0) = 0,\nf ′(x) = −1\n10e−1\n10 x + 1\n20 ≤0\n(190)\nfor any x ∈[0, 10 log 2]. Thus for any ∆∈[0, 1],\ne−1\n10 ∆+ 1\n20∆−1 = f(∆) ≤f(0) = 0.\n(191)\nWhen 1 ≤∆≤16√d log K,\nmin\nx∈S\n\u0010\n1 −e\n1\n2 ∥x∥2−1\n2 ∥x−u1∥2\u0011\n≥1 −e−1\n10 ∆≥1 −e−1\n10 ≥1\n20 · 1\n(192)\nTherefore we have shown that\nmin\nx∈S\n\u0010\n1 −e\n1\n2 ∥x∥2−1\n2 ∥x−u1∥2\u0011\n≥c3 min{∆, 1}.\n(193)\n54\nwhere c3 =\n1\n20.\nAs for P(N (0, Id) ∈S), take\nS′ :=\n\u001a\nx = (x1, · · · , xd)\n\f\f\f\f2\np\nd log 2 ≥x1 ≥1\n10, x2\n2 + · · · + x2\nd ≤60d log K\n\u001b\n.\n(194)\nThen S′ ⊂S. Therefore\nP(N (0, Id) ∈S) ≥P(N (0, Id) ∈S′)\n= P(2\np\nd log 2 ≥x1 ≥1\n10, x2\n2 + · · · + x2\nd ≤60d log K, x ∼N (0, Id))\n= P\n\u0012\n2\np\nd log 2 ≥N(0, 1) ≥1\n10\n\u0013\nP\n\u0010\n∥N(0, Id−1)∥2 ≤60d log K\n\u0011\n≥P\n\u0012\n2\np\nlog 2 ≥N(0, 1) ≥1\n10\n\u0013\nP\n\u0010\n∥N(0, Id−1)∥2 ≤60(d −1) log 2\n\u0011\n> P\n\u0012\n2\np\nlog 2 ≥N(0, 1) ≥1\n10\n\u0013\n· (1 −2e−2)\n(by Lemma C.2)\n> 1\n4 · (1 −2e−2)\n> 1\n10\n(195)\nCombine all these results, we have\nZ\nΩ∗\n1\n|P ∗\n1 −P1|dx ≥min\nx∈S\n\u0010\n1 −e\n1\n2 ∥x∥2−1\n2 ∥x−u1∥2\u0011\nP(N (0, Id) ∈S)\n≥c3 min{∆, 1} · 1\n10\n=\n1\n200 min{∥u∗\n1 −u1∥, 1}\n(196)\nProof of Lemma C.4. For any i ̸= 1,\nZ\nΩ∗\n1\n|P ∗\ni −Pi|dx =\nZ\nΩ∗\n1\n(\n1\n√\n2π\n)d|e−1\n2 ∥x−u∗\ni ∥2 −e−1\n2 ∥x−ui∥2|dx.\n(197)\nNotice that if we denote a(x) := ∥x −u∗\ni ∥, δ(x) := ∥x −u∗\ni ∥−∥x −ui∥,∆:= ∥ui −u∗\ni ∥, then\n|δ(x)| ≤∆≤16√d log K, and for any x ∈Ω∗\n1, a(x) ≥92√d log K (due to separation condition).\n55\nTherefore\nmax\nx∈Ω∗\n1\n\f\f\f\fe−1\n2 ∥x−u∗\ni ∥2 −e−1\n2 ∥x−ui∥2\f\f\f\f\n= max\nx∈Ω∗\n1\n\f\f\f\fe−1\n2 a(x)2 −e−1\n2 (a(x)−δ(x))2\f\f\f\f\n≤max\n(\f\f\f\fe−1\n2 a(x)2 −e−1\n2 (a(x)−δ(x))2\f\f\f\f\n\f\f\f\f\fa(x) ≥92\np\nd log K, |δ(x)| ≤∆\n)\n≤\nmax\na≥92√d log K{max(|e−a2\n2 −e−(a−∆)2\n2\n|, |e−a2\n2 −e−(a+∆)2\n2\n|)}\n=\nmax\na≥92√d log K{max(e−(a−∆)2\n2\n−e−a2\n2 , e−a2\n2 −e−(a+∆)2\n2\n)}\n≤max(\nmax\na≥92√d log K e−(a−∆)2\n2\n−e−a2\n2 ,\nmax\na≥92√d log K e−a2\n2 −e−(a+∆)2\n2\n))\n≤max(\nmax\na≥76√d log K e−a2\n2 −e−(a+∆)2\n2\n,\nmax\na≥92√d log K e−a2\n2 −e−(a+∆)2\n2\n)).\n(198)\nThe last inequality holds since ∆≤16√d log K. For ﬁxed ∆, let f(a) = e−a2\n2 −e−(a+∆)2\n2\n. Then\nf ′(a) = −ae−a2\n2 + (a + ∆)e−(a+∆)2\n2\n(199)\nWe ﬁrst show that f ′(a) ≤0, for any a ≥76√d log K. Notice that\nf ′(a) = −ae−a2\n2 + (a + ∆)e−(a+∆)2\n2\n≤0\n⇐⇒(a + ∆)e−(a+∆)2\n2\n≤ae−a2\n2\n⇐⇒1 + ∆\na ≤ea∆+ 1\n2 ∆2\n(200)\nThe last statement is true because\nea∆+ 1\n2 ∆2 ≥1 + a∆+ 1\n2∆2 ≥1 + ∆\na\n(201)\nwhen a ≥76√d log K > 1.\nSince f ′(a) ≤0 for any a ≥76√d log K, we have\nf(a) ≤f(76\np\nd log K)\n= exp(−1\n2(76\np\nd log K)2) −exp(−1\n2(76\np\nd log K + ∆)2)\n= e−1\n2 (76√d log K)2(1 −e−76√d log K∆−1\n2 ∆2)\n≤e−1\n2 (76√d log K)2(76\np\nd log K∆+ 1\n2∆2)\n≤e−1\n2 (76√d log K)2 · 84\np\nd log K∆\n(since ∆≤16\np\nd log K).\n(202)\n56\nWhich shows\nmax\na≥76√d log K e−a2\n2 −e−(a+∆)2\n2\n≤e−1\n2 (76√d log K)2 · 84\np\nd log K∆\n(203)\nSimilarly\nmax\na≥92√d log K e−a2\n2 −e−(a+∆)2\n2\n≤e−1\n2 (92√d log K)2 · 100\np\nd log K∆\n(204)\nTherefore\nmax\nx∈Ω∗\n1\n\f\f\f\fe−1\n2 ∥x−u∗\ni ∥2 −e−1\n2 ∥x−ui∥2\f\f\f\f ≤e−1\n2 (76√d log K)2 · 84\np\nd log K∆≤c4 min {∥u∗\ni −ui∥, 1}\n(205)\nwhere c4 = e−1\n2 (76√d log K)2 · 1344d log K (Since ∆≤16√d log K min {∆, 1}). Notice that\nc4 = e−1\n2 (76√d log K)2 · 1344d log K\n≤e−1\n2 (76√d log K)2 · 1344kdK\n≤e−1\n2 (76√d log K)2 · 1344k2d\n= 1344e−2886d log K\n≤1344e−1\n2 (70√d log K)2\n(206)\nW.L.O.G., let u∗\n1 = 0, and deﬁne u′ = (50√d log K, 0, · · · , 0), then\nZ\nΩ∗\n1\n|P ∗\ni −Pi|dx =\nZ\nΩ∗\n1\n(\n1\n√\n2π)d|e−1\n2 ∥x−u∗\ni ∥2 −e−1\n2 ∥x−ui∥2|dx\n≤\nZ\nΩ∗\n1\n(\n1\n√\n2π\n)d max\nx∈Ω∗\n1\n|e−1\n2 ∥x−u∗\ni ∥2 −e−1\n2 ∥x−ui∥2|dx\n≤\nZ\nΩ∗\n1\n(\n1\n√\n2π\n)d1344e−1\n2 (70√d log K)2 min {∥u∗\ni −ui∥, 1} dx\n= min {∥u∗\ni −ui∥, 1}\nZ\nΩ∗\n1\n(\n1\n√\n2π )d1344e−1\n2 (70√d log K)2dx\n≤1344 min{∥u∗\ni −ui∥, 1}\nZ\nΩ∗\n1\n(\n1\n√\n2π )de−1\n2 ∥x−u′∥2dx\n≤1344 min{∥u∗\ni −ui∥, 1} P(N (u′, Id) ∈Ω∗\n1)\n≤1344 min{∥u∗\ni −ui∥, 1} P(∥N (u′, Id) −u′∥≥34\np\nd log K)\n≤1344 min{∥u∗\ni −ui∥, 1} · 2 exp(−(34√d log K)2\n16d\n)\n(by Lemma C.2)\n≤1344 min{∥u∗\ni −ui∥, 1} · 2 exp(−70 log K)\n= 2688 min{∥u∗\ni −ui∥, 1}\n\u0012 1\nK\n\u001370\n≤2688\n\u00121\n2\n\u001369 \u0012 1\nK\n\u0013\nmin {∥u∗\ni −ui∥, 1}\n(207)\n57\nC.2\nBracketing Number\nWe upper bound the bracketing number of PX (U) as follows.\nLemma C.5. Let\nPX (U) :=\n\u001a K\nX\ni=1\n1\nK N(ui, Id)\n\f\f\f\f u = {ui}K\ni=1 ∈U\n\u001b\n.\nWe assume there exists D > 0 such that for any u = {ui}K\ni=1 ∈U, it holds that\n∥ui∥2 ≤D\np\nd log K, ∀i ∈[K].\nThen the entropy can be bounded as follows,\nlog N\n\u0000PX (U), 1/m\n\u0001\n≤2dK log(6mdKD).\nProof of Lemma C.5. First of all, we consider a set of standard Gaussian distribution\nPX (A) :=\n\u001a\npa(x) =\n1\n√\n2π\ne−\n∥x−a∥2\n2\n2\n\f\f\f\f a ∈A\n\u001b\n,\nwhere A = {a ∈Rd | ∥a∥2 ≤D√d log K}. Our goal is to ﬁnd a 1/m-bracket N[ ](PX (A), 1/m) of\nPX (A). In other words, for any pa(x) ∈PX (A), we need to deﬁne ¯pa(x) ∈N[ ](PX (A), 1/m) such\nthat\n• ¯pa(x) ≥pa(x), ∀x ∈Rd\n•\nR\n|¯pa(x) −pa(x)| dx ≤1/m.\nWe consider ¯pa(x) of the form\n¯pa(x) =\n1\n√\n2π e−\nc1∥x−¯a∥2\n2\n2\n+c2.\nWe then specify ¯a ∈Rd, c1 ∈R and c2 ∈R. Let a = (a1, . . . , ad) and ǫ > 0 be a parameter that\nwill be chosen later. If ai ∈[kǫ, (k + 1)ǫ) for some k ∈Z, we deﬁne ¯ai := kǫ and ¯a := (¯a1, . . . , ¯ad),\nwhich implies\n∥a −¯a∥2\n2 ≤dǫ2.\n(208)\nNote that ¯pa(x) ≥pa(x) holds for any x ∈Rd if and only if\n(c1 −1)\n\r\r\r\rx + a −c1¯a\nc1 −1\n\r\r\r\r\n2\n2\n+\nc1\n1 −c1\n∥a −¯a∥2\n2 ≤2c2, ∀x ∈Rd.\nLet c1 = 1 −ǫ. Then, we have ¯pa(x) ≥pa(x) if and only if\n−ǫ\n\r\r\r\rx + a −c1¯a\nc1 −1\n\r\r\r\r\n2\n2\n+ 1 −ǫ\nǫ\n∥a −¯a∥2\n2 ≤2c2, ∀x ∈Rd.\n58\nNote that\n−ǫ\n\r\r\r\rx + a −c1¯a\nc1 −1\n\r\r\r\r\n2\n2\n+ 1 −ǫ\nǫ\n∥a −¯a∥2\n2 ≤1 −ǫ\nǫ\n∥a −¯a∥2\n2 ≤d(1 −ǫ)ǫ,\nwhere the last inequality follows from (208).\nThus, by choosing c2 = d(1 −ǫ)ǫ/2, we obtain\n¯pa(x) ≥pa(x) for any x ∈Rd. Note that\nZ\n|¯pa(x) −pa(x)| dx =\n1\n√c1\n· ec2 −1 = e\nd(1−ǫ)ǫ\n2\n√1 −ǫ −1 ≤\n\u00001 + d(1 −ǫ)ǫ\n\u0001\n· (1 + ǫ) −1 ≤(1 + 2d)ǫ.\nHere the ﬁrst inequality follows from the fact that ex ≤1+2x and\n1\n√1−x ≤1+x for any 0 < x < 1/2.\nLet (1 + 2d)ǫ = m−1. It then holds that\nZ\n|¯pa(x) −pa(x)| dx ≤(1 + 2d)ǫ = 1\nm.\nRecall that for any a ∈A, it holds that ∥a∥2 ≤D√d log K. Thus, we have\nN[ ](PX (A), 1/m) ≤\n\u00122D√d log K\nǫ\n\u0013d\n=\n\u0010\n2mD(1 + 2d)\np\nd log K\n\u0011d\n.\nThen, we consider a set of Gaussian mixture model\nPX (U) :=\n\u001a K\nX\ni=1\n1\nK N(ui, Id)\n\f\f\f\f u = {ui}K\ni=1 ∈U\n\u001b\n,\nwhere U = {{ui}K\ni=1 | ∥ui∥2 ≤D√d log K, ∀i ∈[K]}. Our goal is to ﬁnd a 1/m-bracket N(PX (U), 1/m)\nof PX (U). For any pu(x) ∈PX (U), it holds that\npu(x) =\nK\nX\ni=1\n1\nK pui(x),\nwhere pui(x) ∈PX (A). Note that for any i ∈[K], there exists ¯pui(x) ∈N[ ](PX (A), 1/m), such\nthat\n• ¯pui(x) ≥pui(x), ∀x ∈Rd\n•\nR\n|¯pui(x) −pui(x)| dx ≤1/m.\nWe deﬁne\n¯pu(x) =\nK\nX\ni=1\n1\nK ¯pui(x).\nIt then holds that\n¯pu(x) =\nK\nX\ni=1\n1\nK ¯pui(x) ≥\nK\nX\ni=1\n1\nK pui(x) = pu(x), ∀x ∈Rd\n59\nand\nZ\n|¯pu(x) −pu(x)| dx ≤\nK\nX\ni=1\n1\nK\nZ\n|¯pui(x) −pui(x)| dx ≤\nK\nX\ni=1\n1\nmK = 1\nm.\nThus, we obtain that\nN[ ](PX (U), 1/m) ≤\n\u0010\nN[ ](PX (A), 1/m)\n\u0011K\n≤\n\u0010\n2mD(1 + 2d)\np\nd log K\n\u0011dK\n,\nwhich implies that\nlog N[ ](PX (U), 1/m) ≤dK log\n\u0010\n2mD(1 + 2d)\np\nd log K\n\u0011\n≤2dK log(6mdKD).\nC.3\nRademacher Complexity\nGiven labeled data {xj, yj}n\nj=1 and the pretrained ˆu, the function class\n\b\n(1gˆu,ψ(x1)̸=y1, . . . , 1gˆu,ψ(xn)̸=yn)\n\f\f ψ ∈Ψ\n\t\nis a ﬁnite function class, whose Rademacher complexity can be bounded by the following lemma.\nLemma C.6. Let A = {a1, . . . , aN} be a ﬁnite set of vectors in Rn.\nThen, the Rademacher\ncomplexity can be bounded as follows,\nRn(A) ≤max\na∈A ∥a∥2 · 2√2 log N\nn\n.\nProof. Note that for any λ > 0\nRn(A) = E\n\u0014\nsup\na∈A\n2\nn\nn\nX\ni=1\nσiai\n\u0015\n≤1\nλ log E\nh\nesupa∈A\n2λ\nn\nPn\ni=1 σiaii\n≤1\nλ log\nX\na∈A\nE\nh\ne\n2λ\nn\nPn\ni=1 σiaii\n= 1\nλ log\nX\na∈A\nn\nY\ni=1\nE\nh\ne\n2λ\nn σiaii\n,\n(209)\nwhere the ﬁrst inequality follows from Jensen’s inequality. Recall that σi is a Rademacher random\nvariable. Thus, we have\nE\nh\ne\n2λ\nn σiaii\n= 1\n2e\n2λ\nn ai + 1\n2e−2λ\nn ai ≤e\n2λ2a2\ni\nn2 ,\n(210)\nwhere the last inequality follows from the fact that (ex + e−x)/2 ≤ex2/2. By (209) and (210), we\nhave\nRn(A) ≤1\nλ log\nX\na∈A\ne\n2λ2∥a∥2\nn2\n≤1\nλ log |A|e\n2λ2\nn2 ·maxa∈A ∥a∥2 = 1\nλ log N + 2λ\nn2 · max\na∈A ∥a∥2.\n(211)\nLet λ =\np\nn log N/2 maxa∈A ∥a∥2. We obtain that\nRn(A) ≤max\na∈A ∥a∥· 2√2 log N\nn\n.\n60\nC.4\nProofs for Theorem 5.3\nIn the sequel, we prove Theorem 5.3.\nProof. Let Φ = U and Ψ be the set of 2K classiﬁcations. Recall that the loss function is deﬁned as\nℓ(x, y) = 1{x̸=y}, which is upper bound by 1. Let m = ˜Ω(dK3). By Theorem 3.3 and Lemma C.5,\nit holds that\ndTV\n\u0000P ˆφ(x), Pφ∗(x)\n\u0001\n≲\nr\n1\nm log N[ ](PX (Φ), 1/m)\nδ\n≲\nr\ndK\nm log mdKD\nδ\n≲1\nK .\nThen, by Lemma 5.2, Assumption 3.2 holds for Gaussian mixture models. By Theorem 3.4, with\nprobability at least 1 −δ, we have the following excess risk bound,\nErrorℓ(ˆφ, ˆψ) ≤2 max\nφ∈Φ Rn(ℓ◦Gφ,Ψ) +\nr\n2\nn log 4\nδ + 12κ ·\nr\n1\nm log 2N(PX(Φ), 1/m)\nδ\n,\nwhere κ = c2 is some absolute constants that represents the transferability of the model. By Lemma\nC.5, we further have\nErrorℓ(ˆφ, ˆψ) ≤2 max\nφ∈Φ Rn(ℓ◦Gφ,Ψ) +\nr\n2\nn log 4\nδ + 12κ ·\nr\n2dK\nm\nlog 12mdKD\nδ\n.\n(212)\nFor any φ ∈Φ, we have\nRn(ℓ◦Gφ,Ψ) = E\n\u0014\nsup\nψ∈Ψ\n1\nn\nn\nX\ni=1\nσi1{gφ,ψ(xi)̸=yi}\n\u0015\n.\n(213)\nNote that |Ψ| = 2K. By Lemma C.6, it holds for any φ ∈Φ that\nRn(ℓ◦Gφ,Ψ) ≤√n · 2\np\n2 log 2K\nn\n= 2\nr\n2K log 2\nn\n.\n(214)\nBy (212) and (214), we have\nErrorℓ(ˆφ, ˆψ) ≤4\nr\n2K log 2\nn\n+\nr\n2\nn log 4\nδ + 12κ ·\nr\n2dK\nm\nlog 12mdKD\nδ\n= O\n\u0012s\nK log 1\nδ\nn\n+ κ\ns\ndK log mdKD\nδ\nm\n\u0013\n= ˜O\n\u0012r\nK\nn + κ\nr\ndK\nm\n\u0013\n,\nwhere κ = c2 is some absolute constants that represents the transferability of the model.\nThus, we prove Theorem 5.3.\nD\nProofs for Section 6\nIn Section D.1, we show that contrastive learning with linear regression as downstream tasks is\nκ−1-weakly-informative by proving Lemma 6.1. In Section D.2, we prove Theorem 6.2.\n61\nD.1\nProofs for Lemma 6.1\nRecall that in the setting of contrastive learning, we assume that x and x′ are sampled independently\nfrom the same distribution P(x). And we assume the label t that captures the similarity between\nx and x′ satisﬁes\nP(t = 1 | x, x′) =\n1\n1 + e−fθ∗(x)T fθ∗(x′) ,\nP(t = −1 | x, x′) =\n1\n1 + efθ∗(x)T fθ∗(x′) .\nLemma 6.1 directly follows from the following lemma.\nLemma D.1. There exists O ∈Rr×r, OT O = OOT = Ir such that\ndTV\n\u0000POfθ(x, z), Pfθ∗(x, z)\n\u0001\n≤c ·\ns\n1\nσmin(E[fθ∗(x)fθ∗(x)T ]) · H\n\u0000Pfθ(x, x′, t), Pfθ∗(x, x′, t)\n\u0001\n.\nHere c is some absolute constants.\nWe ﬁrst prove the following lemma, which is the core of the proof of Lemma D.1.\nLemma D.2. Suppose that E[fθ(x)fθ∗(x)T ] = E[fθ∗(x)fθ(x)T ] are positive semi-deﬁnite matrices.\nThen we have\nE\n\u0002\u0000fθ(x)T fθ(x′) −fθ∗(x)T fθ∗(x′)\n\u00012\u0003\n≥(2\n√\n2 −2)σmin\n\u0000E[fθ∗(x)fθ∗(x)T ]\n\u0001\n· E[∥fθ∗(x) −fθ(x)∥2\n2].\nProof of Lemma D.2. For notation simplicity, we denote ∆(x) := fθ∗(x)−fθ(x). It then holds that\nE\n\u0002\u0000fθ(x)T fθ(x′) −fθ∗(x)T fθ∗(x′)\n\u00012\u0003\n= E\n\u0002\u0000fθ∗(x)T ∆(x′) + ∆(x)T fθ∗(x′) −∆(x)T ∆(x′)\n\u00012\u0003\n= E\n\u0002\u0000∆(x)T ∆(x′)\n\u00012 −2\n√\n2∆(x)T ∆(x′)fθ∗(x′)T ∆(x) + 2fθ∗(x)T ∆(x′)fθ∗(x′)T ∆(x)\n\u0003\n+ (4 −2\n√\n2)E[fθ(x′)T ∆(x)∆(x)T fθ∗(x′)] + (2\n√\n2 −2)E[fθ∗(x′)T ∆(x)∆(x)T fθ∗(x′)].\n(215)\nFor the ﬁrst term of (215), we have\nE\n\u0002\u0000∆(x)T ∆(x′)\n\u00012 −2\n√\n2∆(x)T ∆(x′)fθ∗(x′)T ∆(x) + 2fθ∗(x)T ∆(x′)fθ∗(x′)T ∆(x)\n\u0003\n= Tr\n\u0010\nE[∆(x′)∆(x′)T ∆(x)∆(x)T −2\n√\n2∆(x′)fθ∗(x′)T ∆(x)∆(x)T + 2∆(x′)fθ∗(x′)T ∆(x)fθ∗(x)T ]\n\u0011\n= Tr\n\u0010\u0000E[∆(x)∆(x)T ]\n\u00012 −2\n√\n2E[∆(x)fθ∗(x)T ] · E[∆(x)∆(x)T ] + 2\n\u0000E[∆(x)fθ∗(x)T ]\n\u00012\u0011\n= Tr\n\u0010\u0000E[∆(x)∆(x)T ] −\n√\n2E[∆(x)fθ∗(x)T ]\n\u00012\u0011\n,\n(216)\nwhere the second equation follows from our assumption that x, x′ are i.i.d. Note that E[fθ(x)fθ∗(x)T ] =\nE[fθ∗(x)fθ(x)T ]. Thus, we obtain\n\u0010\nE[∆(x)∆(x)T ] −\n√\n2E[∆(x)fθ∗(x)T ]\n\u0011T\n= E[∆(x)∆(x)T ] −\n√\n2E[fθ∗(x)∆(x)T ]\n= E[∆(x)∆(x)T ] −\n√\n2E[∆(x)fθ∗(x)T ],\n(217)\n62\nwhich implies that E[∆(x)∆(x)T ] −\n√\n2E[∆(x)fθ∗(x)T ] is symmetric. It then holds that\nE\n\u0002\u0000∆(x)T ∆(x′)\n\u00012 −2\n√\n2∆(x)T ∆(x′)fθ∗(x′)∆(x) + 2fθ∗(x)T ∆(x′)fθ∗(x′)T ∆(x)\n\u0003\n= Tr\n\u0010\u0000E[∆(x)∆(x)T ] −\n√\n2E[∆(x)fθ∗(x)T ]\n\u00012\u0011\n≥0.\n(218)\nFor the second term of (215), we have\nE[fθ(x′)T ∆(x)∆(x)T fθ∗(x′)] = Tr\n\u0010\nE[fθ∗(x′)fθ(x′)T ] · E[∆(x)∆(x)T ]\n\u0011\n≥0,\n(219)\nwhere the inequality follows from the fact E[fθ∗(x′)fθ(x′)T ] ≽0 and E[∆(x)∆(x)T ] ≽0.\nFor the third term of (215), we have\nE[fθ∗(x′)T ∆(x)∆(x)T fθ∗(x′)] = Tr\n\u0010\nE[fθ∗(x)fθ∗(x)T ] · E[∆(x)∆(x)T ]\n\u0011\n≥σmin\n\u0000E[fθ∗(x)fθ∗(x)T ]\n\u0001\nTr\n\u0010\nE[∆(x)∆(x)T ]\n\u0011\n= σmin\n\u0000E[fθ∗(x)fθ∗(x)T ]\n\u0001\nE[∥∆(x)∥2\n2].\n(220)\nCombining (215), (218), (219) and (220), we have\nE\n\u0002\u0000fθ(x)T fθ(x′) −fθ∗(x)T fθ∗(x′)\n\u00012\u0003\n≥(2\n√\n2 −2)σmin\n\u0000E[fθ∗(x)fθ∗(x)T ]\n\u0001\nE[∥∆(x)∥2\n2]\n(221)\nWith Lemma D.2, we prove Lemma D.1 in the following.\nProof of Lemma D.1. We consider the singular value decomposition (SVD) of E[fθ(x)fθ∗(x)T ] =\nU1Σ1V T\n1\nand E[fθ∗(x)fθ(x)T ] = (E[fθ(x)fθ∗(x)T ])T = V1Σ1U T\n1 . We deﬁne O := V1U T\n1 ∈Rr×r,\nwhich satisﬁes OT O = OOT = Ir. It then holds that\nE[Ofθ(x)fθ∗(x)T ] = E\n\u0002\nfθ∗(x)\n\u0000Ofθ(x)\n\u0001T \u0003\n= V1Σ1V T\n1 ,\n(222)\nwhich are positive semi-deﬁnite matrices. By Lemma D.2, we have\nE\n\u0002\u0000fθ(x)T fθ(x′) −fθ∗(x)T fθ∗(x′)\n\u00012\u0003\n≥(2\n√\n2 −2)σmin\n\u0000E[fθ∗(x)fθ∗(x)T ]\n\u0001\n· E[∥fθ∗(x) −Ofθ(x)∥2\n2].\n(223)\nFor Hellinger distance, we have\n2H2\u0000Pfθ(x, x′, t), Pfθ∗(x, x′, t)\n\u0001\n=\nZ \u0010q\npfθ(x, x′, t) −\nq\npfθ∗(x, x′, t)\n\u00112\ndtdxdx′\n=\nZ \u0010q\npfθ(t = 1 | x, x′) −\nq\npfθ∗(t = 1 | x, x′)\n\u00112\np(x, x′) dxdx′\n+\nZ \u0010q\npfθ(t = 0 | x, x′) −\nq\npfθ∗(t = 0 | x, x′)\n\u00112\np(x, x′) dxdx′\n(224)\n63\nFor the ﬁrst term of (224), we have\nZ \u0010q\npfθ(t = 1 | x, x′) −\nq\npfθ∗(t = 1 | x, x′)\n\u00112\np(x, x′) dxdx′\n=\nZ \u0010q\nh\n\u0000fθ(x)T fθ(x′)\n\u0001\n−\nq\nh\n\u0000fθ∗(x)T fθ∗(x′)\n\u00112\np(x, x′) dxdx′,\n(225)\nwhere\nh(a) :=\n1\n1 + e−a .\n(226)\nBy Cauchy-Schwartz inequality, we have |fθ(x)T fθ(x′)| ≤∥fθ(x)∥2∥fθ(x′)∥2 ≤1. Note that for any\na, b ∈[−1, 1], we have\n\u0010p\nh(a) −\np\nh(b)\n\u00112\n=\n\u0000h(a) −h(b)\n\u00012\n\u0010p\nh(a) +\np\nh(b)\n\u00112 ≥1\n4\n\u0000h(a) −h(b)\n\u00012 = 1\n4h′(ξ)2(a −b)2 ≥\n1\n2 + e + e−1 (a −b)2.\n(227)\nThus, it holds that\nZ \u0010q\npfθ(t = 1 | x, x′) −\nq\npfθ∗(t = 1 | x, x′)\n\u00112\np(x, x′) dxdx′\n≥\n1\n2 + e + e−1\nZ \u0000fθ(x)T fθ(x′) −fθ∗(x)T fθ∗(x′)\n\u00012p(x, x′) dxdx′\n=\n1\n2 + e + e−1 E\n\u0002\u0000fθ(x)T fθ(x′) −fθ∗(x)T fθ∗(x′)\n\u00012\u0003\n.\n(228)\nSimilarly, For the second term of (224), we have\nZ \u0010q\npfθ(t = 0 | x, x′) −\nq\npfθ∗(t = 0 | x, x′)\n\u00112\np(x, x′) dxdx′\n≥\n1\n2 + e + e−1 E\n\u0002\u0000fθ(x)T fθ(x′) −fθ∗(x)T fθ∗(x′)\n\u00012\u0003\n(229)\nCombining (224), (228) and (229), we have\nH2\u0000Pfθ(x, x′, t), Pfθ∗(x, x′, t)\n\u0001\n≥\n1\n2 + e + e−1 E\n\u0002\u0000fθ(x)T fθ(x′) −fθ∗(x)T fθ∗(x′)\n\u00012\u0003\n.\n(230)\nWe choose O ∈Rr×r that satisﬁes (223). For the TV distance, we have\ndTV\n\u0000POfθ(x, z), Pfθ∗(x, z)\n\u0001\n= 1\n2\nZ\n|pOfθ(z | x) −pfθ∗(z | x)|p(x) dx.\n(231)\n64\nNote that z | x ∼N(fθ(x), Ir). By Lemma B.1, we have\ndTV\n\u0000POfθ(x, z), Pfθ∗(x, z)\n\u0001\n= 1\n2\nZ\n|pOfθ(z | x) −pfθ∗(z | x)|p(x) dx\n≤1\n2\nZ\nmin{1, ∥Ofθ(x) −fθ∗(x)∥2}p(x) dx\n≤1\n2 min\n\u001a\n1,\nZ\n∥Ofθ(x) −fθ∗(x)∥2p(x) dx\n\u001b\n= 1\n2 min\n\b\n1, E[∥Ofθ(x) −fθ∗(x)∥2]\n\t\n.\n(232)\nCombining (223), (230) and (232), we show that\ndTV\n\u0000POfθ(x, z), Pfθ∗(x, z)\n\u0001\n≤1\n2E[∥Ofθ(x) −fθ∗(x)∥2]\n≤1\n2\nq\nE[∥Ofθ(x) −fθ∗(x)∥2\n2]\n≤1\n2\ns\n1\n(2\n√\n2 −2)σmin\n\u0000E[fθ∗(x)fθ∗(x)T ]\n\u0001E\n\u0002\u0000fθ(x)T fθ(x′) −fθ∗(x)T fθ∗(x′)\n\u00012\u0003\n≤1\n2\ns\n2 + e + e−1\n(2\n√\n2 −2)σmin\n\u0000E[fθ∗(x)fθ∗(x)T ]\n\u0001H\n\u0000Pfθ(x, x′, t), Pfθ∗(x, x′, t)\n\u0001\n.\n(233)\nThus, we prove Lemma D.1.\nLemma D.1 directly implies Lemma 6.1.\nProof of Lemma 6.1. For any θ ∈Θ, we choose O ∈Rr×r that satisﬁes Lemma D.1. It then holds\nthat\ndTV\n\u0000Pfθ,OT β∗(x, y), Pfθ∗,β∗(x, y)\n\u0001\n= dTV\n\u0000POfθ,β∗(x, y), Pfθ∗,β∗(x, y)\n\u0001\n≤dTV\n\u0000POfθ(x, z), Pfθ∗(x, z)\n\u0001\n≤c ·\ns\n1\nσmin(E[fθ∗(x)fθ∗(x)T ]) · H\n\u0000Pfθ(x, x′, t), Pfθ∗(x, x′, t)\n\u0001\n.\nThus, we prove that the model is κ−1-weakly-informative, where\nκ = c ·\ns\n1\nσmin(E[fθ∗(x)fθ∗(x)T ]).\n(234)\nHere c is some absolute constants.\n65\nD.2\nProofs for Theorem 6.2\nIn this section, we prove Theorem 6.2. Suppose that ˆθ, ˆβ are the outputs of Algorithm 1. Let ℓbe\nthe squared loss and ˜ℓbe its truncation with truncation level L. The optimal predictor deﬁned in\n(1) has the following closed form solution\ngθ,β(x) = Eθ,β[y | x] = βT fθ(x).\n(235)\nWe have the following guarantees.\nLemma D.3. Let the truncation level L = 36(D2 + 1) log n. It then holds that\nsup\nθ,β\n\b\nEθ∗,β∗\u0002\nℓ\n\u0000gθ,β(x), y\n\u0001\u0003\n−Eθ∗,β∗\u0002˜ℓ\n\u0000gθ,β(x), y\n\u0001\u0003\t\n≤\nr\n18(D2 + 1) log n\nπn\n.\n(236)\nProof of Lemma D.3. Note that\n\u0000gθ,β(x) −y\n\u0001\f\fx =\n\u0000βT fθ(x) −y\n\u0001\f\fx ∼N\n\u0000βT fθ(x) −β∗T fθ∗(x), 1\n\u0001\n(237)\nWe denote by c(x) := βT fθ(x) −β∗T fθ∗(x). It holds that |c(x)| ≤2D. Thus, it holds for any θ, β\nthat\nEθ∗,β∗\u0002\nℓ\n\u0000gθ,β(x), y\n\u0001\n−˜ℓ\n\u0000gθ,β(x), y\n\u0001 \f\f x\n\u0003\n= Eθ∗,β∗\nh\u0010\u0000gθ,β(x) −y\n\u00012 −L\n\u0011\n1{(gθ,β(x)−y)2>L}\n\f\f x\ni\n=\nZ +∞\n√\nL\n(u2 −L) ·\n1\n√\n2π e−\n\u0000u−c(x)\n\u00012\n2\ndu\n=\nZ +∞\n√\nL−c(x)\n\u0000(u + c(x))2 −L\n\u0001\n·\n1\n√\n2π e−u2\n2 du\n=\n√\nL + c(x)\n√\n2π\ne−\n\u0000√\nL−c(x)\n\u00012\n2\n+ 1 + c(x)2 −L\n√\n2π\nZ +∞\n√\nL−c(x)\ne−u2\n2 du\n≤\n√\nL + c(x)\n√\n2π\ne−\n\u0000√\nL−c(x)\n\u00012\n2\n(L ≥4D2 + 1 ≥c(x)2 + 1)\n≤2(\n√\nL −c(x))\n√\n2π\ne−\n\u0000√\nL−c(x)\n\u00012\n2\n(L ≥36D2 ≥(3c(x))2)\n≤2(\n√\nL −2D)\n√\n2π\ne−\n\u0000√\nL−2D\n\u00012\n2\n≤\n√\nL\n√\n2π e−L\n8\n(\n√\nL −2D ≥\n√\nL\n2 )\n(238)\n66\nAs a result, we show that\nsup\nθ,β\n\b\nEθ∗,β∗\u0002\nℓ\n\u0000gθ,β(x), y\n\u0001\u0003\n−Eθ∗,β∗\u0002˜ℓ\n\u0000gθ,β(x), y\n\u0001\u0003\t\n≤Eθ∗,β∗\nh\nsup\nθ,β\nEθ∗,β∗\u0002\nℓ\n\u0000gθ,β(x), y\n\u0001\n−˜ℓ\n\u0000gθ,β(x), y\n\u0001 \f\f x\n\u0003i\n≤\n√\nL\n√\n2π e−L\n8\n≤\nr\n18(D2 + 1) log n\nπn\n.\n(L = 36(D2 + 1) log n)\n(239)\nLemma D.4. Suppose that ˆθ, ˆβ are the outputs of Algorithm 1. Let ˜ℓbe the truncated squared loss\nwith truncation level L. Then there exists an absolute constant c such that with probability at least\n1 −δ that\nEθ∗,β∗\u0002˜ℓ\n\u0000gˆθ, ˆβ(x), y\n\u0001\u0003\n−Eθ∗,β∗\u0002˜ℓ\n\u0000gθ∗,β∗(x), y\n\u0001\u0003\n≤cκL ·\ns\n1\nm log N[ ]\n\u0000PX ×S(Fθ), 1/m2\u0001\nδ\n+ cL\nr\nlog 1/δ\nn\n+ c\n√\nL sup\nθ∈Θ\nRn(Gθ,B),\n(240)\nwhere\nκ = c3\ns\n1\nσmin\n\u0000E[fθ∗(x)fθ∗(x)T ]\n\u0001\nfor some absolute constants c3. Here Rn(Gθ,B) is the Rademacher complexity deﬁned as\nRn(Gθ,B) = E\n\u0014\nsup\nβ∈B\n2\nn\nn\nX\ni=1\nσigθ,β(xi)\n\u0015\n,\n(241)\nwhere σi are Rademacher random variables.\nProof of Lemma D.4. With Lemma B.7 and Lemma 6.1 in hand, Lemma D.4 follows directly from\nTheorem 3.7 and the fact that ˜ℓis 2\n√\nL-Lipschitz.\nWith Lemma D.3 and Lemma D.4 in hand, we are now ready to prove Theorem 6.2.\nProof of Theorem 6.2. Note that\nErrorℓ(ˆθ, ˆβ) = Eθ∗,β∗\u0002\nℓ\n\u0000gˆθ, ˆβ(x), y\n\u0001\u0003\n−Eθ∗,β∗\u0002\nℓ\n\u0000gθ∗,β∗(x), y\n\u0001\u0003\n= Eθ∗,β∗\u0002\nℓ\n\u0000gˆθ, ˆβ(x), y\n\u0001\u0003\n−Eθ∗,β∗\u0002˜ℓ\n\u0000gˆθ, ˆβ(x), y\n\u0001\u0003\n+ Eθ∗,β∗\u0002˜ℓ\n\u0000gˆθ, ˆβ(x), y\n\u0001\u0003\n−Eθ∗,β∗\u0002˜ℓ\n\u0000gθ∗,β∗(x), y\n\u0001\u0003\n+ Eθ∗,β∗\u0002˜ℓ\n\u0000gθ∗,β∗(x), y\n\u0001\u0003\n−Eθ∗,β∗\u0002\nℓ\n\u0000gθ∗,β∗(x), y\n\u0001\u0003\n≤sup\nθ,β\n\b\nEθ∗,β∗\u0002\nℓ\n\u0000gθ,β(x), y\n\u0001\u0003\n−Eθ∗,β∗\u0002˜ℓ\n\u0000gθ,β(x), y\n\u0001\u0003\t\n+ Eθ∗,β∗\u0002˜ℓ\n\u0000gˆθ, ˆβ(x), y\n\u0001\u0003\n−Eθ∗,β∗\u0002˜ℓ\n\u0000gθ∗,β∗(x), y\n\u0001\u0003\n.\n(242)\n67\nLet the truncation level be L = 36(D2 + 1) log n. By Lemma D.3 and Lemma D.4, we have\nError(ˆθ, ˆβ)\n≤cκL ·\ns\n1\nm log N[ ]\n\u0000PX ×S(Fθ), 1/m2\u0001\nδ\n+ cL\nr\nlog 1/δ\nn\n+ c\n√\nL sup\nθ∈Θ\nRn(Gθ,B)\n+\nr\n18(D2 + 1) log n\nπn\n.\n(243)\nFor the Rademacher complexity, we have\nRn(Gθ,B) = E\n\u0014\nsup\nβ∈B\n2\nn\nn\nX\ni=1\nσigθ,β(xi)\n\u0015\n= E\n\u0014\nsup\nβ∈B\n2\nn\nn\nX\ni=1\nσiβT fθ(xi)\n\u0015\n≤2D\n√n,\n(244)\nwhere the last inequality follows from Lemma B.6. Combining (243) and (244), we have\nError(ˆθ, ˆβ)\n≤cκL ·\ns\n1\nm log N[ ]\n\u0000PX ×S(Fθ), 1/m2\u0001\nδ\n+ cL\nr\nlog 1/δ\nn\n+ 2cD\nr\nL\nn\n+\nr\n18(D2 + 1) log n\nπn\n= ˜O\n\u0012\nκL\ns\nlog N[ ]\n\u0000PX ×S(Fθ), 1/m2\u0001\nm\n+ L\nr\n1\nn\n\u0013\n,\n(245)\nwhere L = 36(D2 + 1) log n and\nκ = c3\ns\n1\nσmin\n\u0000E[fθ∗(x)fθ∗(x)T ]\n\u0001\nfor some absolute constants c3.\nE\nFailure of Two-Phase MLE\nFor simplicity, in the sequel, we consider the case where no side information is available, i.e., we\nhave access to unlabeled data {xi}m\ni=1 and labeled data {xj, yj}n\nj=1. Another natural scheme is to\nuse a two-phase MLE (Algorithm 2). To be speciﬁc, in the ﬁrst phase, we use MLE to estimate φ∗\nbased on the unlabeled data {xi}m\ni=1. In the second phase, we use MLE again to estimate ψ∗based\non pretrained ˆφ and the labeled data {xj, yj}n\nj=1.\n68\nAlgorithm 2 Two-phase MLE\n1: Input: {xi}m\ni=1, {(xj, yj)}n\nj=1\n2: Use unlabeled data {xi}m\ni=1 to learn ˆφ via MLE:\nˆφ ←arg max\nφ∈Φ\nm\nX\ni=1\nlog pφ(xi).\n3: Fix ˆφ and use labeled data {(xj, yj)}n\nj=1 to learn ˆψ via MLE:\nˆψ ←arg max\nψ∈Ψ\nn\nX\nj=1\nlog p ˆφ,ψ(xj, yj).\n4: Output: ˆφ and ˆψ.\nNote that the two-phase MLE does not directly associate the learning process with the loss\nfunction. Thus, the only way to evaluate the excess risk is to study the total variation distance\nbetween P ˆφ, ˆ\nψ(x, y) and Pφ∗,ψ∗(x, y). In the pretraining phase, MLE guarantees that the estimator\nP ˆφ is close to Pφ∗in the sense of total variation distance (Theorem 3.3). However, it’s still possible\nthat for some x, P ˆφ(x) = 0 while Pφ∗(x) ̸= 0. This phenomenon may result in log p ˆφ,ψ∗(xj, yj) =\n−∞for some labeled data in the learning of downstream tasks, which will dramatically inﬂuence\nthe behaviour of MLE for estimating ψ∗and ﬁnally lead to the failure of the second phase. Inspired\nby this idea, we give the following theorem.\nTheorem E.1. There exists Φ, Ψ, φ∗∈Φ, ψ∗∈Ψ, such that for any constant c > 0, there exists\nm, n ≥c such that with probability at least 1\n2(1 −e−1)e−1, we have\ndTV\n\u0000P ˆφ, ˆψ(x, y), Pφ∗,ψ∗(x, y)\n\u0001\n≥1\n8,\nwhere ˆφ and ˆψ are the outputs of Algorithm 2.\nProof of Theorem E.1. We construct the counter example as follows. Let (x, y, z) ∈N+ ×N+ ×N+.\nWe assume that the true parameter (φ∗, ψ∗) = (φ1, ψ1), which satisﬁes\nPφ1(x = k, z = k) = 1\n2k\n∀k ∈N+,\nPφ1(x = m, z = n) = 0 ∀m ̸= n,\nPψ1(y = k|z = k) = 1, ∀k ∈N+.\nFor i ≥2, we deﬁne Pφi as follows,\nPφi(x = 1, z = 1) = 1\n2 + 1\n2i ,\nPφi(x = k, z = k) = 1\n2k ∀k /∈{1, i}\nPφi(x = m, z = n) = 0 ∀m ̸= n or m = n = i.\nWe deﬁne Pψ2 as follows, for any k ∈N+,\nPψ2(y = 1|z = k) = 1\n4,\nPψ2(y = 2|z = k) = 1\n2\nPψ2(y = j|z = k) = 1\n2j ∀j /∈{1, 2}.\n69\nWe denote Φ := {φi | i ∈N+} and Ψ := {ψ1, ψ2}. In the sequel, we show that Algorithm 2 fails on\nthis case. Recall that we denote by {xi}m\ni=1 and {xj, yj}n\nj=1 the unlabeled data and labeled data,\nrespectively. We have the following observations:\n• We deﬁne i := min{k ̸= 1 | k /∈{xi}m\ni=1}. If we have 1 ∈{xi}m\ni=1, then the maximizer of\nlikelihood function ˆφ satisﬁes ˆφ = φi.\n• Suppose that ˆφ = φi for some i ̸= 1 and i ∈{yj}n\nj=1. We then have ˆψ = ψ2.\nWe deﬁne the event E := {∃i ̸= 1, such that ˆφ = φi and i ∈{yj}n\nj=1}. Under event E, we have\nˆφ = φi for some i ̸= 1 and ˆψ = ψ2, which implies\ndTV\n\u0000P ˆφ, ˆψ(x, y), Pφ∗,ψ∗(x, y)\n\u0001\n= 1\n2\nZ Z\n|pφi,ψ2(x, y) −pφ1,ψ1(x, y)| dxdy\n≥1\n2\nZ \f\f\f\f\nZ\npφi,ψ2(x, y) −pφ1,ψ1(x, y) dx\n\f\f\f\f dy\n= 1\n2\nZ\n|pφi,ψ2(y) −pφ1,ψ1(y)| dy\n≥1\n2|Pφi,ψ2(y = 2) −Pφ1,ψ1(y = 2)| = 1\n8\n(246)\nIn the following, we only need to lower bound the probability of event E. Note that\nP(E) = P\n\u0000∪∞\ni=2\n\bˆφ = φ, i ∈{yj}n\nj=1\n\t\u0001\n=\n∞\nX\ni=2\nP\n\u0000ˆφ = φi, i ∈{yj}n\nj=1\n\u0001\n=\n∞\nX\ni=2\nP(ˆφ = φi) · P\n\u0000i ∈{yj}n\nj=1\n\u0001\n=\n∞\nX\ni=2\n\u0012\n1 −\n\u0012\n1 −1\n2i\n\u0013n\u0013\n· P(ˆφ = φi).\n(247)\nThus, it holds for any L ≥2 that\nP(E) ≥\nL\nX\ni=2\n\u0012\n1 −\n\u0012\n1 −1\n2i\n\u0013n\u0013\n· P(ˆφ = φi)\n≥\n\u0012\n1 −\n\u0012\n1 −1\n2L\n\u0013n\u0013\n· P\n\u0000∃2 ≤i ≤L, ˆφ = φi\n\u0001\n.\n(248)\n70\nNote that\nP\n\u0000∃2 ≤i ≤L, ˆφ = φi\n\u0001\n= P\n\u0010\b\n1 ∈{xi}m\ni=1\n\t\n∩\n\b\n∃2 ≤i ≤L, i /∈{xi}m\ni=1\n\t\u0011\n≥P\n\u0010\b\n1 ∈{xi}m\ni=1\n\t\n∩\n\b\nL /∈{xi}m\ni=1\n\t\u0011\n≥P\n\u00001 ∈{xi}m\ni=1\n\u0001\n+ P\n\u0000L /∈{xi}m\ni=1\n\u0001\n−1\n= P\n\u0000L /∈{xi}m\ni=1\n\u0001\n−P\n\u00001 /∈{xi}m\ni=1\n\u0001\n=\n\u0012\n1 −1\n2L\n\u0013m\n−1\n2m .\n(249)\nCombining (248) and (249), we have for any L ≥2\nP(E) ≥\n\u0012\n1 −\n\u0012\n1 −1\n2L\n\u0013n\u0013\n·\n\u0012\u0012\n1 −1\n2L\n\u0013m\n−1\n2m\n\u0013\n.\n(250)\nSetting 2L = m = n, we obtain that\nP(E) ≥\n\u0012\n1 −\n\u0012\n1 −1\nm\n\u0013m\u0013\n·\n\u0012\u0012\n1 −1\nm\n\u0013m\n−1\n2m\n\u0013\n→(1 −e−1) · e−1, as m →∞.\n(251)\nThus, for any c > 0, there exists m, n ≥c such that\nP(E) ≥1\n2(1 −e−1) · e−1.\n71\n",
  "categories": [
    "stat.ML",
    "cs.LG",
    "math.ST",
    "stat.TH"
  ],
  "published": "2023-03-02",
  "updated": "2023-03-02"
}