{
  "id": "http://arxiv.org/abs/2303.04456v1",
  "title": "RM-Depth: Unsupervised Learning of Recurrent Monocular Depth in Dynamic Scenes",
  "authors": [
    "Tak-Wai Hui"
  ],
  "abstract": "Unsupervised methods have showed promising results on monocular depth\nestimation. However, the training data must be captured in scenes without\nmoving objects. To push the envelope of accuracy, recent methods tend to\nincrease their model parameters. In this paper, an unsupervised learning\nframework is proposed to jointly predict monocular depth and complete 3D motion\nincluding the motions of moving objects and camera. (1) Recurrent modulation\nunits are used to adaptively and iteratively fuse encoder and decoder features.\nThis not only improves the single-image depth inference but also does not\noverspend model parameters. (2) Instead of using a single set of filters for\nupsampling, multiple sets of filters are devised for the residual upsampling.\nThis facilitates the learning of edge-preserving filters and leads to the\nimproved performance. (3) A warping-based network is used to estimate a motion\nfield of moving objects without using semantic priors. This breaks down the\nrequirement of scene rigidity and allows to use general videos for the\nunsupervised learning. The motion field is further regularized by an\noutlier-aware training loss. Despite the depth model just uses a single image\nin test time and 2.97M parameters, it achieves state-of-the-art results on the\nKITTI and Cityscapes benchmarks.",
  "text": "RM-Depth: Unsupervised Learning of Recurrent Monocular Depth\nin Dynamic Scenes*\nTak-Wai Hui\nH-1 Research\neetwhui@gmail.com\nAbstract\nUnsupervised methods have showed promising results on\nmonocular depth estimation. However, the training data\nmust be captured in scenes without moving objects. To push\nthe envelope of accuracy, recent methods tend to increase\ntheir model parameters.\nIn this paper, an unsupervised\nlearning framework is proposed to jointly predict monoc-\nular depth and complete 3D motion including the motions\nof moving objects and camera. (1) Recurrent modulation\nunits are used to adaptively and iteratively fuse encoder\nand decoder features. This not only improves the single-\nimage depth inference but also does not overspend model\nparameters.\n(2) Instead of using a single set of ﬁlters\nfor upsampling, multiple sets of ﬁlters are devised for the\nresidual upsampling. This facilitates the learning of edge-\npreserving ﬁlters and leads to the improved performance.\n(3) A warping-based network is used to estimate a motion\nﬁeld of moving objects without using semantic priors. This\nbreaks down the requirement of scene rigidity and allows to\nuse general videos for the unsupervised learning. The mo-\ntion ﬁeld is further regularized by an outlier-aware training\nloss. Despite the depth model just uses a single image in\ntest time and 2.97M parameters, it achieves state-of-the-art\nresults on the KITTI and Cityscapes benchmarks.\n1. Introduction\nVisual perception is an important ability for human to\nunderstand and perceive the world. As a consequence, re-\nsearch work on scene geometry has attracted a lot of atten-\ntion over several decades. This promotes the deployment of\ntechnology to numerous applications such as autonomous\nvehicle, interactive robot, virtual and augmented reality, and\nmore. The problem of scene geometry generally involves\nestimating depth, camera motion1, and optical ﬂow from an\n*This research work is not for commercial use unless a prior arrange-\nment has been made with the author.\n1The words, ego-motion, camera motion and pose, are interchangeably\nused throughout the paper.\nimage sequence. The above computer vision tasks are often\nrecovered together since they are coupled through geomet-\nric constraints [36,45]\nUnlike depth from triangulation, single-image depth es-\ntimation is inherently ill-posed because there are multiple\npossible 3D points along each light ray towards the camera\ncenter. Convolutional neural networks have demonstrated\nthe ability to exploit the relationship between a captured\nimage and the corresponding scene depth [9,26]. Recently,\nunsupervised methods [13,14,36,45,48] have achieved ap-\npealing performance than the early supervised counterparts.\nTheir successes primarily rely on the use of the classical\ntechnique, structure from motion. Given at least two im-\nages, a novel view generated from an image will be con-\nsistent with another image in the pair if depth and camera\nmotion are correctly estimated. However, this strictly re-\nquires scene rigidity, i.e. the training data must be captured\nin scenes without moving objects other than the moving\ncamera itself. To get rid of this requirement, stereo image\nsequences [13] and masking out dynamic objects [36, 48]\nare commonly adopted.\nRecent works tend to devise a\nmulti-image approach [43], a large amount of model param-\neters [16], and semantic priors [7] for improving the depth\naccuracy.\nIn this paper, an unsupervised learning framework of re-\ncurrent monocular depth, dubbed RM-Depth, is proposed\nto jointly predict depth, camera motion, and motion ﬁeld of\nmoving objects without requiring static scenes in the train-\ning data. RM-Depth requires neither a large number of pa-\nrameters nor prior semantic information. Particularly, im-\nage pairs are used in training while only a single image is\nused for depth inference at test time. The contributions of\nthis work are summarized as follows:\n1. Recurrent modulation unit (RMU) – Fusion of fea-\nture maps across encoder and decoder often appears in\ndepth estimation [14,48]. In the proposed method, the\ndecoder consists of RMUs. The fusion is iteratively re-\nﬁned by adaptive modulating the encoder features us-\ning the hidden state of RMU. This in turn improves the\n1\narXiv:2303.04456v1  [cs.CV]  8 Mar 2023\nperformance of single-image depth inference.\n2. Residual upsampling – Conventionally, feature maps\nare upsampled using a single set of ﬁlters [39, 46]. In\nthis work, multiple sets of ﬁlters are proposed such that\neach set of them is speciﬁcally trained for upsampling\nsome of the spectral components. This effectively im-\nproves upsampling along edges.\n3. Motion ﬁeld of moving objects – Besides camera mo-\ntion, a 3D motion ﬁeld of moving objects is recovered\nin a coarse-to-ﬁne framework through a warping ap-\nproach. This breaks down the scene rigidity assump-\ntion and allows to use general videos for the unsu-\npervised learning. The unsupervised learning of mo-\ntion ﬁeld is further improved by introducing an outlier-\naware regularization loss.\nWith the above innovations, RM-Depth achieves state-\nof-the-art results on the KITTI and Cityscapes benchmarks.\nThe depth model only requires 2.97M parameters, while\nit achieves 4.8 and 44 times reduction in model size com-\nparing to the popular Monodepth2 [14] and PackNet [16],\nrespectively.\nThe project page of this paper is available\nat https://github.com/twhui/RM-Depth.\n2. Related Work\n2.1. Unsupervised Joint Learning of Depth and Ego-\nmotion\nDepth from a Single Image. A pioneer work from Zhou et\nal. [48] proposes an unsupervised learning framework for\nestimating depth and ego-motion. Based on [48], Godard et\nal. [14] introduce the per-pixel minimum reprojection loss,\nauto-masking of stationary pixels, and full-scale estimation\nloss for improving the unsupervised training. Mahjourian et\nal. [32] and Bian et al. [2] explore the consistencies of 3D\npoint clouds and depth maps across consecutive frames, re-\nspectively. Wang et al. [7] devise to use direct visual odom-\netry for pose estimation without requiring additional pose\nnetwork. Recently, Guizilini et al. [16] utilize 3D convolu-\ntions for packing and unpacking feature maps. Johnston et\nal. [23] propose to estimate depth map using self-attention\nand disparity volume. Poggi et al. [35] impose depth un-\ncertainty during unsupervised training.\nUnlike the prior\nworks, RM-Depth introduces recurrent modulation units\nand residual upsampling in the depth model. The proposed\ncomponents lead to the improved performance while the\ndepth model just requires a very small number of param-\neters (2.97M).\nThe previous works recover rigid ﬂow2 through the pro-\njection of estimated scene depth, and hence moving objects\n2A component of optical ﬂow that is solely due to camera motion with-\nout considering moving objects in the scene.\nin the scene cannot be taken into account. To recover full\nﬂow, Yin et al. [45] propose to use a network cascade to esti-\nmate the residual ﬂow accounting for moving objects. Ran-\njan et al. [36] propose a framework that facilitates the co-\nordinated trainings of depth, ego-motion, and optical ﬂow.\nTheir method reasons about segmenting a scene into static\nand moving regions. Chen et al. [4] use a separated ﬂow\nnetwork and introduce an online optimization scheme. Dif-\nferent from the prior works, scene rigidity is not required\nin the training of RM-Depth. The motion network of RM-\nDepth recovers both camera and object motions. Therefore,\nfull ﬂow can be used for the unsupervised training. This in\nturn improves the performance on depth estimation.\nDepth from Multiple Images. Wang et al. [42] exploit\nthe temporal correlation across consecutive frames by us-\ning convolutional long short-term memory (LSTM). De-\nspite a 10-frame sequence is used, it just performs on par\nwith Monodepth2 [14]. Li et al. [29] utilize the encoded\nfeature resulting from a self-contained optical ﬂow network\nas the input to each LSTM. However, their model requires\n15 LSTM modules for a proper depth inference.\nLi et\nal. [28] propose a self-supervised online meta-learning that\nuses LSTM to aggregate spatial-temporal information in\nthe past. Watson et al. [43] propose a cost volume based\napproach to fuse temporal information. Unlike LSTM or\nGRU [5], the proposed recurrent modulation unit (RMU)\nuses features from a single static image as the input but not\nfeatures from a time varying image sequence.\n2.2. Unsupervised Joint Learning of Depth, Egomo-\ntion, and Object Motion\nVideo data is often captured in scenes involving dynamic\nobjects. Therefore, the assumption of scene rigidity is vio-\nlated. Most of the prior works rely on additional segmen-\ntation labels to assist the unsupervised learning of object\nmotion. With semantic prior, Casser et al. [3] estimate the\n3D motion of each dynamic object using a network similar\nto the one used for ego-motion. Gordon et al. [15] propose\na network for estimating the motion ﬁeld of moving objects.\nA pre-computed segmentation mask that pinpoints the loca-\ntions of moving objects imposes regularization of the mo-\ntion ﬁeld. Li et al. [28] eliminate the use of semantic priors\nin [15] by introducing a sparsity loss. Gao et al. propose\nattentional CNN blocks to disentangle camera and object\nmotion without semantic priors [10], but their experimen-\ntal results are limited to the KITTI dataset. Lee et al. [27]\npropose an instance-aware photometric and geometric con-\nsistency loss that imposes self-supervisory signals for static\nand moving object regions. RM-Depth estimates the motion\nﬁeld of moving objects without using semantic priors. A\nwarping-based network is proposed for the motion ﬁeld es-\ntimation. An outlier-aware training loss is further exploited\nfor regularizing the motion ﬁeld. Using the proposed inno-\n2\nFigure 1. An overview of the unsupervised learning framework. For brevity, a 3-level design is shown. Given an image sequence\n{I1, I2, I3}, deﬁne It=2 as the target image and the rest {Is=1,3} as the source images. Depth map and motion ﬁeld are estimated in\na coarse-to-ﬁne framework. For the motion network, {Is} are warped towards It in accordance to the image projection computed by\nEq. (2) using motion ﬁeld Tobj , camera pose (Rcam, tcam), and scene depth Dt. For the depth network, encoder and decoder features are\nadaptively and iteratively fused by RMUs. More details of the depth and motion networks are presented in Secs. 3.2 and 3.4, respectively.\nvations, RM-Depth outperforms the prior works.\n2.3. Unsupervised Learning of Depth Using Stereo\nTraining Data\nThe scene rigidity requirement limits unsupervised\nmethods to use monocular data without involving dynamic\nobjects in scenes.\nSince the left and right images in a\nstereo rig are captured simultaneously, stereo data provides\nan alternative option for the unsupervised training. Garg et\nal. [11] propose to use the photometric difference between\nimages in each stereo pair for governing the learning of\nmonocular depth estimation. Godard et al. [13] explore the\nconsistency between the disparities produced relative to the\nleft and right images. Zhan et al. [47] devise the temporal\nand spatial clues in stereo image sequences for improving\nthe unsupervised training. Yang et al. [44] aligns the illu-\nmination of the training images and model the photometric\nuncertainties of pixels on the input images.\n3. Depth from a Single Image\nUnsupervised learning of single-image depth estimation\nis often achieved by training two networks together [14,48].\nThe primary (depth) network takes an image as the in-\nput and gradually predicts scene depth (up to a scale fac-\ntor) with increasing spatial resolutions.\nThe secondary\n(pose) network estimates camera motion for each image\npair. Source frames in a given image sequence are warped\ntowards the target frame by projecting the computed 3D\npoint cloud to the target frame. The difference between\nthe target and each synthesized frame is used as the driv-\ning force for the unsupervised training. In this work, an\nunsupervised learning framework RM-Depth is proposed\nfor joint learning of depth, ego-motion, and object mo-\ntion in general scenes. An overview of the learning frame-\nwork is shown in Fig. 1. In more details, the depth net-\nwork utilizes Recurrent Modulation Units (RMU) to adap-\ntively and iteratively combine encoder and decoder features\n(Sec. 3.2). Residual upsampling (Sec. 3.3) is used to facil-\nitate the learning of edge-aware ﬁlters. Furthermore, a 3D\nmotion ﬁeld of moving objects (Sec. 3.4) is recovered. As\nit will show later (Sec. 4), the proposed innovations lead to\nthe improved depth accuracy despite not using any segmen-\ntation labels.\n3.1. Preliminaries\nPerspective Projection. Denote O as the camera coordi-\nnate system associated with image I and Ω⊂R2 as the\nimage domain. Suppose D : Ω→R is the depth map. A\npoint x ∈Ωon I is the image projection from a 3D point\np ∈R3. Once D(x) (i.e. z-coordinate of p) is given, p can\nbe recovered by back-projection of x as follows:\np = D(x)K−1 \u0000x\n1\u0001⊺,\n(1)\nwhere K denotes a 3 × 3 camera intrinsic matrix.\nNovel View Synthesis.\nSuppose an image sequence\n{I1, I2, ..., IN} is given. In the following, subscripts t and\ns will be used to denote variables that are deﬁned in the tar-\nget and source views, respectively. Let’s consider one of\nthe frames It being the target view and the rest being the\nsource views Is(1 ≤N, s ̸= t). The transformation from\nOt to Os is governed by a 3 × 3 rotation matrix R and a 3D\ntranslation vector t. Using Eq. (1), the image projection of\npt onto Is is given by:\n\u0000xs\n1\u0001⊺∼= K\n\u0010\nRDt(xt)K−1 \u0000xt\n1\u0001⊺+ t\n\u0011\n,\n(2)\nwhere “∼=” denotes equality up to a positive scale factor and\nDt is the depth map at the target view. Is is warped towards\nIt to form a novel view Is→t in accordance with the visual\ndisplacement (i.e. optical ﬂow) xs −xt.\n3\nFigure 2. The network architectures of different depth models: (a)\nConventional method [14,48] and (b) RMU-based model. For the\nease of representation, a 3-level design is illustrated.\n3.2. Recurrent Depth Network\nTop-down approach [14,48] often adopts U-Net architec-\nture [37] for depth inference. Fig. 2a provides an overview\nof the network architecture. The upsampled decoder feature\nx from the previous level is fused with the corresponding\nencoder feature F through a concatenation followed by a\nconvolution layer. The feature fusion can be represented as\nfollows:\nh = θ(conv([x, F])),\n(3)\nwhere “θ” and “conv” represent an activation function and a\nconvolution layer, respectively. Since the convolution ker-\nnels are ﬁxed, the fusion cannot be adapted for different\ninputs. This limits the performance of depth inference.\nIt is desired to make the feature fusion to be adaptive.\nIntuitively, the decoder feature can be augmented with a\nmodulated encoder feature. To this end, the encoder fea-\nture is adaptively transformed according to the current hid-\nden state of the decoder. This is equivalent to change the\nfeed-forward behavior of the encoder despite using the same\ninput. Besides, recurrent CNN has been shown useful in\nimproving network performance [24]. Taking these inspi-\nrations, Recurrent Modulation Unit (RMU) is devised for\ndynamic and iterative feature fusion in the depth network.\nFig. 2b provides an overview of the proposed network. This\ndesign leads to the improved depth accuracy (Sec.4). In the\nfollowing, when the operations are presented in a pyramid\nlevel, the same operations are applicable to other levels.\nRecurrent Modulation Unit (RMU). There are two com-\nponents inside a RMU, namely modulation and update.\nFig. 3 shows the details. At iteration step k, the encoder\nfeature F is adaptively modulated according to the previous\nFigure 3. The technical details of a RMU. At iteration k, the en-\ncoder feature F is modulated to F ′\nk. The new hidden state hk is a\nweighted average between F ′\nk and the previous hidden state hk−1\naccording to the element-wise adaptive scalar zk.\nfused feature hk−1 (i.e. the hidden state at iteration k −1)\nthrough an afﬁne transformation3 consisting of weight and\nbias terms (wk, bk) as follows (modulation phase):\nwk, bk = convs([hk−1, F]),\n(4a)\nF′\nk = tanh(conv(wk ⊙F + bk)),\n(4b)\nwhere “convs” and “⊙” denote convolutions and the\nHadamard product, respectively. Eq (4a). can be re-written\nto a residual form as conv(conv(hk−1) + conv(F)). Since\nF is ﬁxed, the second term can be pre-computed to reduce\nthe computational complexity. The hidden state hk−1 is\ncombined with the modulated encoder feature F ′\nk for the\nfeature fusion according to an element-wise adaptive scalar\nzk as follows (update phase):\nzk = σ(conv([hk−1, F′\nk])),\n(5a)\nhk = (1 −zk) ⊙hk−1 + zk ⊙F′\nk,\n(5b)\nwhere “σ” denotes a sigmoid function. Particularly, the con-\nventional feature fusion in Eq. (3) is static while the pro-\nposed feature fusion is both dynamic and iterative.\nComparing to GRU [5], RMU uses features from a sin-\ngle static image as the input but not features from a time-\nvarying image sequence. GRU uses an extra memory state\nthat depends on the input at the current time for the update.\nAs a whole, GRU uses two sigmoid gates while RMU uses\none sigmoid gate.\nHidden State Initialization. Instead of initializing the ﬁrst\nhidden state h0 with zero, F resulting from the top level of\nthe encoder is converted to h0 as follows:\nh0 = tanh(convs(F)).\n(6)\nDepth Inference. Depth map Dt is inferred from the last\n3There could be other choices for the modulation function, afﬁne trans-\nformation is selected because of its low computational complexity.\n4\nhidden state. To prevent numerical issues during backprop-\nagation, Dt is bounded by [Dmin, Dmax] as follows:\nˆDt = σ (convs (hk)) ,\n(7a)\nDt = Dmin(1 −ˆDt) + Dmax ˆDt.\n(7b)\n3.3. Residual Upsampling\nUpsampling decoder feature is required when passing\nfrom a low-resolution to a high-resolution level in top-down\napproach [14,48]. A feature map x is upsampled to x′ by a\nupsampling function f (such as deconvolution [46] or sub-\npixel convolution [39]). The process can be represented by\nx′ = θ (f(x; W)) ,\n(8)\nwhere “θ” denotes an activation function.\nSince a fea-\nturemap like a colour image consists of different spectral\ncomponents, a single ﬁlter W is not universal enough to per-\nform well on all regions. It is desired to use different up-\nsampling ﬁlters on different regions (ﬂat region: averaging\nﬁlter, edge region: high-pass ﬁlter). To this end, a generic\nupsampling layer that uses multiple ﬁlters Wi is proposed as\nfollows:\nx′ = θ\n\u0012X\ni\nfi(x; Wi)\n\u0013\n.\n(9)\nParticularly, each upsampling operator fi is band-limited\nto some spectral components. The individual upsampled\nfeature maps are summed before applying the activation.\nTo compromise between accuracy and speed, RM-Depth is\nlimited to use two kinds of upsampling operators, namely\nlow-frequency fl and high-frequency fh ones, as follows:\nx′ = θ\n\u0000fl(conv1×1(x)) + fh(x; Wh)\n\u0001\n,\n(10)\nwhere a 1×1 convolution is used to squeeze x for match-\ning the channel dimension of fh(·). A bilinear upsampling\nis chosen as fl. Besides the 1×1 convolution, there is no\nadditional increase in model parameters or computational\noverhead in comparison to Eq. (8).\n3.4. Object Motion\nUnsupervised learning of depth relies on novel view syn-\nthesis as presented in Sec. 3.1. Prior works tend to jointly\nrecover depth and camera motion but leaving out motions of\nmoving objects [14,16]. As a result, the visual displacement\nthat is computed by Eq. (2) is just a component of full ﬂow\n(so-called rigid ﬂow) inferred by the camera motion. The\nnovel view is not correctly synthesized and in turn affects\nthe unsupervised training. Artifacts often exist in moving\nobjects when the object motion is not taken into consider-\nation (see Fig. 6 in Sec. 4.2). To resolve this issue, both\nthe camera and object motions are necessarily recovered.\nSince it is rare to have objects spinning on their owns with\nFigure 4. The architecture of the proposed motion network. The\nencoder is shared by the pose and object motion decoders. Object\nmotion ﬁeld Tobj is reﬁned in a multi-scale framework by feed-\nbackwarding the previous estimate to the encoder through novel\nview synthesis (see Sec. 3.1).\nlarge magnitudes in street-view scenes, it can be assumed\nthat the rotational motion of moving objects is nearly zero.\nAn overview of the proposed motion network is shown in\nFig. 4. More details are presented below.\nWarping-Based Motion Field Inference.\nMotions of\nmoving objects are estimated in form of a motion ﬁeld\nTobj : Ω→R3 in a coarse-to-ﬁne framework as shown in\nFig. 4. The motion ﬁeld Tobj is combined with camera mo-\ntion tcam to form a complete motion ﬁeld. Source images\n{Is} are warped towards the target image It in accordance\nwith the full ﬂow ufull = xs −xt, where xs is computed\nby Eq. (2). For the initialization, {Is} are warped towards\nIt in accordance with the rigid ﬂow by setting Tobj = 0.\nThe warped source images {Is→t} together with the target\nimage It are fed into the motion encoder to generate a new\nset of multi-scale encoder features {F(It, Is→t)}. The en-\ncoder features are more aligned to It since {Is} have been\nwarped towards It. This in turn makes the generation of mo-\ntion ﬁeld easier as inspired by the feature warping proposed\nin LiteFlowNet series [20–22]. The object motion decoder\nreﬁnes the previous estimate Tobj,l+1 by augmenting with\nthe encoder feature at the same scale as follows:\nTobj,l = convs([T ↑2\nobj,l+1, Fl(It, Is→t)]) + T ↑2\nobj,l+1, (11)\nwhere “convs” represents several convolution layers and\n(·)↑2 denotes an upsampling operator by a factor of 2. Par-\nticularly, the encoder features are warped for the motion re-\nﬁnement. This is different from prior works [15,28] that use\nﬁxed encoder features.\nOutlier-Aware Regularization Loss. Motion ﬁeld is gen-\nerally sparse since moving objects do not fully occupy a\n5\nscene, i.e. Tobj(x) = 0 when an image position x is not\naffected by non-rigid motion. This observation can impose\na constraint on the unsupervised training and in turn im-\nproves the depth accuracy. A motion mask M is constructed\nby comparing full ﬂow ufull (computed by Eq. (2) using\ndepth, camera and object motions) against rigid ﬂow urig\n(using only depth and camera motion). If there are no mov-\ning objects in the scene other than the moving camera itself,\nthen ufull = urig. Otherwise, ufull ̸= urig. This mo-\ntivation allows us to segment image locations affected by\nnon-rigid motions using the following condition:\nM(x) = [||ufull −urig||2 < α],\n(12)\nwhere [·] is the Iverson bracket. A thresholding approach is\nused to suppress outliers by setting α = 0.5. When an im-\nage position x is affected by non-rigid motions, M(x) = 0.\nOtherwise, M(x) = 1. With the motion mask, an outlier-\naware regularization loss Lreg on the motion ﬁeld is pro-\nposed as follows:\nLreg(Tobj) =\nX\nx∈Ω\ng(M · Tobj),\n(13)\nwhere g(·) is chosen to be the sparsity function [28] as it\nencourages more sparsity than L1 norm. Lreg helps the\nmotion network to properly learn Tobj by suppressing the\ngrowth of undesired object motion in rigid regions.\n4. Experiments\n4.1. Implementation Details\nNetwork Architecture. The overviews of depth and mo-\ntion networks can be referred to Figs. 2 and 4, respectively.\nA modiﬁed 6-level ResNet18 [17] that contains an addi-\ntional convolution layer at the bottom level and excludes the\nclassiﬁcation head is adopted as the encoders. Particularly,\nthe top two levels are not used in the depth encoder. For\nthe motion network, the pose decoder is adopted from [14].\nThe object motion decoder4 uses 9 and 2 RMUs in level 4\nand the remained levels, respectively. RMUs are not shared\nacross different levels in order to maximize ﬁlter diversity\nfor different scales.\nTraining Details.\nThe whole system is implemented in\nTensorFlow [1].\nSame augmentations are performed on\nthe training data as [14], namely 50% horizontal ﬂips, ran-\ndom brightness, contrast, saturation, and hue jitter. Fol-\nlowing [48], the length of each image sequence is ﬁxed to\n3 frames. The central frame is treated as the target view.\nThe depth and motion networks are jointly trained using\nAdam [25] with a batch size varying from 16 to 40 on mul-\ntiple GPUs. To address the stationary pixels and the oc-\n4The ﬁrst bottom level of the object motion decoder is modiﬁed com-\npared with the CVPR version [18]\nclusion problem, the auto-masking and the per-pixel min-\nimum reprojection loss [14] are adopted. Depth map and\nmotion ﬁeld are regularized by an edge-aware smoothness\nloss [14] while the proposed outlier-aware regularization\nloss is further imposed on the object motion ﬁeld. The self-\nsupervision [40] is also adopted but no cropping is applied.\nSome parts of RM-Depth require pre-training5. After that,\nthe overall network is trained for 20 epochs. A learning rate\nof 1e-4 for the ﬁrst 10 epochs and reduce the learning rate\nto 1e-5 for the remained epochs. All the encoders have been\npre-trained on ImageNet [38].\nDataset. The system is trained and validated on KITTI [12]\nand Cityscapes [6]. The image resolution is set to 640×192.\nFor KITTI, the data split of Eigen et al. [9] that excludes all\nthe evaluation frames is used as the training set. For an eval-\nuation, static frames are excluded so that it is comparable to\nZhou et al. [48]. For Cityscapes, the standard training split\nis used and no static frames are neglected. The cropping\nscheme “A” deﬁned in [43] is used for the evaluation.\n4.2. Results\nRM-Depth is compared against prior state-of-the-art\nmethods such that they are also trained on monocular im-\nage sequences and perform single-image depth inference\nwithout using online reﬁnement unless otherwise speciﬁed.\nDepth map is capped to 80m [13] and is normalized using\nmedian scaling [48].\nOther experimental results (related\nto generalization on unseen dataset, visual odometry, and\nmore) are available in the supplementary material [19].\nDepth (KITTI). As shown in the upper half of Table 1,\nRM-Depth outperforms the compared methods. Examples\nof estimations are provided in Fig. 5. It can be observed\nthat RM-Depth is superior in recovering thin structures and\nmoving objects than GeoNet [45]. Monodepth2 [14] can-\nnot correctly predict depth values on objects with reﬂective\nsurface (the on-road train in the ﬁrst example and the white\ncar in the third example) while RM-Depth is free of such\ndefects. PackNet [16] and RM-Depth recover depth maps\nwith sharp discontinuities. However the moving car in the\nsecond example is not correctly estimated by PackNet.\nDepth (Cityscapes). This dataset is more challenging as\nit involves more moving objects than KITTI. Only a few\nworks report the evaluation results on Cityscapes. The bot-\ntom half of Table 1 summarizes the results. Despite RM-\nDepth does not use segmentation labels, it outperforms the\nprior works. Visual comparison is provided in Fig. 6. When\nobject motion is neglected, holes (i.e. depth values tend to\nthe maximum) often appear on moving objects.\nObject Motion and Segmentation. The protocol as [36]\nis followed and the motion segmentation is evaluated on\nthe KITTI 2015 dataset [33]. The results are summarized\n5This is different from the CVPR version [18].\n6\nTable 1. Monocular depth results on the KITTI dataset (K) by the testing split of Eigen et al. [8] and the testing split of Cityscapes dataset\n(CS). Models that require explicit semantic data are highlighted. The best in each category is in bold and the second best is underlined.\nMethod\nSemantics\nTraining\nTesting\nError (lower is better)\nAccuracy (higher is better)\ndatset\ndatset\nAbsRel\nSqRel\nRMS\nRMSlog\nδ < 1.25\nδ < 1.252\nδ < 1.253\nZhou et al. [48]\nK\nK\n0.208\n1.768\n6.856\n0.283\n0.678\n0.885\n0.957\nGeoNet [45]\nK\nK\n0.164\n1.303\n6.090\n0.247\n0.765\n0.919\n0.968\nMahjourian et al. [32]\nK\nK\n0.163\n1.240\n6.220\n0.250\n0.762\n0.916\n0.968\nGeoNet [45]\nK\nK\n0.155\n1.296\n5.857\n0.233\n0.793\n0.931\n0.973\nDDVO [7]\nK\nK\n0.151\n1.257\n5.583\n0.228\n0.810\n0.936\n0.974\nLi et al. [29]\nK\nK\n0.150\n1.127\n5.564\n0.229\n0.823\n0.936\n0.974\nDF-Net [49]\nK\nK\n0.150\n1.124\n5.507\n0.223\n0.806\n0.933\n0.973\nPilzer et al. [34]\nK\nK\n0.142\n1.231\n5.785\n0.239\n0.795\n0.924\n0.968\nEPC++ [30]\nK\nK\n0.141\n1.029\n5.350\n0.216\n0.816\n0.941\n0.976\nStruct2Depth [3]\n•\nK\nK\n0.141\n1.026\n5.291\n0.215\n0.816\n0.945\n0.979\nCC [36]\nK\nK\n0.140\n1.070\n5.326\n0.217\n0.826\n0.941\n0.975\nBian et al. [2]\nK\nK\n0.137\n1.089\n5.439\n0.217\n0.830\n0.942\n0.975\nGLNet [4]\nK\nK\n0.135\n1.070\n5.230\n0.210\n0.841\n0.948\n0.980\nLi et al. [28]\n•\nK\nK\n0.130\n0.950\n5.138\n0.209\n0.843\n0.948\n0.978\nGordon et al. [15]\n•\nK\nK\n0.128\n0.959\n5.230\n0.212\n0.845\n0.947\n0.976\nDistilled Semantics [41]\n•\nK\nK\n0.126\n0.835\n4.937\n0.199\n0.844\n0.953\n0.982\nMonodepth2 [14]\nK\nK\n0.115\n0.882\n4.701\n0.190\n0.879\n0.961\n0.982\nPackNet [16]\nK\nK\n0.111\n0.785\n4.601\n0.189\n0.878\n0.960\n0.982\nPackNet [16] (with velocity weak supervision)\nK\nK\n0.111\n0.829\n4.788\n0.199\n0.864\n0.954\n0.980\nJohnston et al. [23]\nK\nK\n0.111\n0.941\n4.817\n0.189\n0.885\n0.961\n0.981\nMonodepth2-Boot+Self [35]\nK\nK\n0.111\n0.826\n4.667\n0.184\n0.880\n0.961\n0.983\nMonodepth2-Boot+Log [35]\nK\nK\n0.117\n0.900\n4.838\n0.192\n0.873\n0.958\n0.981\nLee et al. [27]\n•\nK\nK\n0.112\n0.777\n4.772\n0.191\n0.872\n0.959\n0.982\nGao et al. [10]\nK\nK\n0.112\n0.866\n4.693\n0.189\n0.881\n0.961\n0.981\nRM-Depth\nK\nK\n0.107\n0.687\n4.476\n0.181\n0.883\n0.964\n0.984\nZhou et al. [48]\nCS + K\nK\n0.198\n1.836\n6.565\n0.275\n0.718\n0.901\n0.960\nMahjourian et al. [32]\nCS + K\nK\n0.159\n1.231\n5.912\n0.243\n0.784\n0.923\n0.970\nGeoNet [45]\nCS + K\nK\n0.153\n1.328\n5.737\n0.232\n0.802\n0.934\n0.972\nDDVO [7]\nCS + K\nK\n0.148\n1.187\n5.496\n0.226\n0.812\n0.938\n0.975\nDF-Net [49]\nCS + K\nK\n0.146\n1.182\n5.215\n0.213\n0.818\n0.943\n0.978\nPackNet [16]\nCS + K\nK\n0.108\n0.727\n4.426\n0.184\n0.885\n0.963\n0.984\nPackNet [16] (with velocity weak supervision)\nCS + K\nK\n0.108\n0.803\n4.642\n0.195\n0.875\n0.958\n0.980\nRM-Depth\nCS + K\nK\n0.105\n0.675\n4.368\n0.178\n0.889\n0.965\n0.984\nStruct2Depth [3]\n•\nCS\nCS\n0.145\n1.737\n7.280\n0.205\n0.813\n0.942\n0.976\nGLNet [4] (with online reﬁnement)\nCS\nCS\n0.129\n1.044\n5.361\n0.212\n0.843\n0.938\n0.976\nGordon et al. [15]\n•\nCS\nCS\n0.127\n1.330\n6.960\n0.195\n0.830\n0.947\n0.981\nLi et al. [28]\nCS\nCS\n0.119\n1.290\n6.980\n0.190\n0.846\n0.952\n0.982\nLee et al. [27]\n•\nCS\nCS\n0.111\n1.158\n6.437\n0.182\n0.868\n0.961\n0.983\nRM-Depth\nCS\nCS\n0.090\n0.825\n5.503\n0.143\n0.913\n0.980\n0.993\nRGB image\nGeoNet [45]\nMonodepth2 [14]\nPackNet [16]\nRM-Depth\nFigure 5. Examples of depth map predictions on KITTI.\nin Table 2. RM-Depth outperforms the compared methods\nincluding Distilled Semantics [41] while RM-Depth nei-\nther uses semantic labels for training nor semantic network.\nFig. 7 show examples of motion ﬁeld and segmentation pre-\ndictions.\nOptical Flow. It is computed by Eq. (2) using depth, cam-\nera and object motions. As provided in Table 3, AEE is im-\nproved when object motion is considered. The performance\nis reasonable since no stand-alone optical ﬂow network is\nconstructed. Examples of optical ﬂow are shown in Fig. 7.\nModel Size and Runtime. As shown in Fig. 8, RM-Depth\njust requires 2.97M parameters for the depth model while it\noutperforms the prior works even for those with semantics.\nRM-Depth runs at 40FPS for a single depth prediction on a\n7\nRGB image\nwithout object motion\nStruct2Depth [3]\nGordon et al. [15]\nRM-Depth\nFigure 6. Examples of depth map predictions on Cityscapes.\nRGB image\nDepth\nMotion ﬁeld\nSegmentation mask\nOptical ﬂow\nFigure 7. Examples of depth, object motion ﬁeld, segmentation mask, and optical ﬂow predictions on the KITTI 2015 dataset.\nTable 2. Motion segmentation results on the KITTI 2015 dataset.\nModel\nSemantics\nIntersection over Union (IoU)\nOverall\nStatic car\nMoving car\nEPC++ [30]\n50.00\n-\n-\nCC [36]\n•\n56.94\n55.77\n58.11\nDS [41]\n•\n62.66\n58.42\n66.89\nDS (semantic network) [41]\n•\n63.98\n64.16\n63.79\nRM-Depth\n72.12\n71.87\n72.37\nTable 3. Optical ﬂow results in terms of average end-point error\non the KITTI 2015 dataset.\nModel\nExplicit\nAll\nF1\nﬂow network\nDistilled Semantics (ego-motion) [41]\n13.50\n51.22%\nDistilled Semantics [41]\n•\n11.61\n25.78%\nGeoNet (DirFlowNetS) [45]\n•\n12.21\n-\nGeoNet [45]\n•\n10.81\n-\nGLNet [4]\n8.35\n-\nRM-Depth (w/o warping)\n10.45\n43.95%\nRM-Depth\n8.16\n31.47%\nmachine equipped with a GeForce GTX 1080.\n4.3. Ablation Study\nThe contributions of the proposed components are stud-\nied by evaluating different variants of RM-Depth. Since\nmoving objects are limited on KITTI, the proposed com-\nponents that are related to object motion are evaluated on\nCityscapes. All the results are evaluated on their testing\nsplits and are capped at 80m per standard practice.\nRMU and Residual Upsampling. As shown in Table 4, the\nfull model outperforms the baseline by a large margin. The\n0\n20\n40\n60\n80\n100\n120\n140\n0.11\n0.12\n0.13\n0.14 Struct2Depth [3]\nCC [36]\nBian et al. [2]\nGordon et al. [15]\nDistilled Semantics [41]\nLite-HR-Depth [31]\nMonodepth2 [14]\nLee et al. [27]\nHR-Depth [31]\nPackNet [16]\nRM-Depth\nNumber of model parameters (M)\nError (AbsRel)\nFigure 8. Error of depth models on KITTI against the number of\nmodel parameters. Red dots denote models requiring semantics.\nproposed components are effective in improving the depth\naccuracy. By removing either the residual upsampling or\nRMU, the depth error is increased. Thanks to the residual\nupsampling, depth edges are less dispersed comparing to\nthe results using conventional upsampling [39] as demon-\nstrated in Fig. 9. A RMU consists of modulation and update\nparts. When the modulation part is removed, the depth er-\nror is increased. This indicates that the depth improvement\nis largely beneﬁted by the modulation since it adaptively\nmodiﬁes the feed-forward behavior of the encoder.\nObject Motion. The full model performs the best among\n8\nTable 4. Ablation study of RM-Depth on KITTI.\nModel\nError\nAbsRel\nSqRel\nRMS\nRMSlog\nfull\n0.1081\n0.7100\n4.5138\n0.1831\nw/o residual upsampling\n0.1097\n0.7313\n4.5269\n0.1839\nw/o RMU\n0.1167\n0.8186\n4.7100\n0.1895\nw/o modulation\n0.1165\n0.7546\n4.6623\n0.1910\nbaseline (w/o my contributions)\n0.1187\n0.8382\n4.7894\n0.1927\nRGB image\nConventional [39]\nResidual upsampling\nFigure 9. Depth map predictions using different upsamplings.\nTable 5. Ablation study of RM-Depth on Cityscapes.\nModel\nError\nAbsRel\nSqRel\nRMS\nRMSlog\nfull\n0.0903\n0.8248\n5.5027\n0.1430\nw/o warping\n0.0933\n0.9248\n5.6283\n0.1461\nw/o outlier-aware regularization\n0.0995\n0.9986\n5.8281\n0.1545\nusing sparsity loss as [28]\n0.1066\n1.1073\n6.0965\n0.1642\nw/o object motion estimation\n0.1174\n1.1195\n6.4542\n0.1729\nbaseline (w/o my contributions)\n0.1335\n1.8784\n6.9748\n0.1912\nall the variants as summarized in Table 5. The proposed\ncomponents are effective in improving depth accuracy on\nnon-rigid scenes. When warping is disabled, the source im-\nages are not warped towards the target image. There is a\nlarge “visual gap” between images in the pair, and hence\nthe depth error is increased. By disabling the outlier-aware\nregularization, the depth accuracy is deteriorated. Compar-\ning to the variant using the sparsity loss [28], the full model\nperforms much better. When object motion estimation and\nother proposed components are disabled, it has been expe-\nrienced that the training becomes diverged after 6 epochs.\nHoles often appear on moving objects as revealed in Fig. 6.\nNumber of RMUs. Compromising accuracy and computa-\ntional complexity, at most 2 RMUs are assigned for levels 2\n– 3. As summarized in Table 6, depth accuracy and runtime\nincrease with the number of RMUs.\nTable 6. Ablation study of the number of RMUs on KITTI.\nNumber of RMUs\nError\nRuntime\nAbsRel\nSqRel\nRMS\nRMSlog\n[ms]\n3 (L4: 1, L3: 1, L2: 1)\n0.1161\n0.7713\n4.6799\n0.1906\n14.99\n6 (L4: 2, L3: 2, L2: 2)\n0.1135\n0.7490\n4.6128\n0.1877\n20.40\n8 (L4: 4, L3: 2, L2: 2)\n0.1098\n0.7251\n4.5535\n0.1845\n22.07\n13 (L4: 9, L3: 2, L2: 2)\n0.1081\n0.7100\n4.5138\n0.1831\n24.78\n5. Conclusion\nRM-Depth, an unsupervised learning framework, is pro-\nposed for single-image depth estimation. Complete motion\nthat includes camera and object motions is used to assist the\nunsupervised learning. This breaks down the scene rigidity\nrequirement. The depth network utilizes recurrent modu-\nlation units for dynamic and iterative feature fusion. The\nuse of residual upsampling enables speciﬁc upsampling of\ndifferent spectral components. For the motion network, a\nwarping-based approach has been devised to recover ob-\nject motion. An outlier-aware regularization loss has also\nbeen exploited. With the proposed innovations, the depth\nnetwork achieves promising results while it only requires\n2.97M model parameters.\nReferences\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C.\nCitro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-\nmawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,\nR. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,\nR. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J.\nShlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Van-\nhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. Warden, M.\nWattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow:\nLarge-scale machine learning on heterogeneous distributed\nsystems, 2015. 6\n[2] J.-W. Bian, Z. Li, N.Wang, H. Zhan, C. Shen, M.-M. Cheng,\nand I. Reid. Unsupervised scale-consistent depth and ego-\nmotion learning from monocular video. In NeurIPS, pages\n35–45, 2019. 2, 7, 8\n[3] V. Casser, S. Pirk, R. Mahjourian, and A. Angelova. Depth\nprediction without the sensors: Leveraging structure for un-\nsupervised learning from monocular videos. In AAAI, pages\n8001–8008, 2019. 2, 7, 8\n[4] Y. Chen, C. Schmid, and C. Sminchisescu. Self-supervised\nlearning with geometric constraints in monocular video con-\nnecting ﬂow, depth, and camera. In ICCV, pages 7063–7072,\n2019. 2, 7, 8\n[5] K. Cho, B. V. Merri¨enboer, D. Bahdanau, and Y. Bengio.\nOn the properties of neural machine translation: Encoder-\ndecoder approaches. In SSST-8, pages 103–111, 2014. 2,\n4\n[6] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,\nR. Benenson, U. Franke, S. Roth, and B. Schiele.\nThe\ncityscapes dataset for semantic urban scene understanding.\nIn CVPR, pages 3213–3223, 2016. 6\n9\n[7] C.Wang, J. M. Buenaposada, R. Zhu, and S. Lucey. Learn-\ning depth from monocular videos using direct methods. In\nCVPR, pages 2022–2030, 2018. 1, 2, 7\n[8] D. Eigen and R. Fergu. Predicting depth, surface normals and\nsemantic labels with a common multi-scale convolutional ar-\nchitecture. In ICCV, pages 2650–2658, 2015. 7\n[9] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction\nfrom a single image using a multi-scale deep network. In\nNeurIPS, pages 2366–2374, 2014. 1, 6\n[10] F. Gao, J. Yu, H. Shen, Y. Wang, and H. Yang.\nAtten-\ntional separation-and-aggregation network for selfsupervised\ndepth-pose learning in dynamic scenes.\nIn CoRL, pages\n2195–2205, 2020. 2, 7\n[11] R. Garg, V. Kumar BG, and I. Reid. Unsupervised CNN for\nsingle view depth estimation: Geometry to the rescue. In\nECCV, pages 740–756, 2016. 3\n[12] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets\nrobotics: The KITTI dataset. IJRR, 32(1):1231–1237, 2013.\n6\n[13] C. Godard, O. M. Aodha, and G. J. Brostow. Unsupervised\nmonocular depth estimation with left-right consistency. In\nCVPR, pages 270–279, 2017. 1, 3, 6\n[14] C. Godard, O. M. Aodha, M. Firman, and G. Brostow. Dig-\nging into self-supervised monocular depth estimation.\nIn\nICCV, pages 3828–3838, 2019. 1, 2, 3, 4, 5, 6, 7, 8\n[15] A. Gordon, H. Li, R. Jonschkowski, and A. Angelova. Depth\nfrom videos in the wild: Unsupervised monocular depth\nlearning from unknown cameras.\nIn ICCV, pages 8977–\n8986, 2019. 2, 5, 7, 8\n[16] V. Guizilini, R. Ambrus, S. Pillai, A. Raventos, and A.\nGaidon. 3d packing for self-supervised monocular depth es-\ntimation. In CVPR, pages 2485–2494, 2020. 1, 2, 5, 6, 7,\n8\n[17] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in\ndeep residual networks. In ECCV, pages 630–645, 2016. 6\n[18] T.-W. Hui. RM-Depth: Unsupervised Learning of Recur-\nrent Monocular Depth in Dynamic Scenes. In CVPR, pages\n1675–1684, 2022. 6\n[19] T.-W. Hui. Supplementary material for RM-Depth: Unsu-\npervised learning of recurrent monocular depth in dynamic\nscenes, 2022. 6\n[20] T.-W. Hui and C. C. Loy. LiteFlowNet3: Resolving corre-\nspondence ambiguity for more accurate optical ﬂow estima-\ntion. In ECCV, pages 169–184, 2020. 5\n[21] T.-W. Hui, X. Tang, and C. C. Loy.\nLiteFlowNet:\nA\nlightweight convolutional neural network for optical ﬂow es-\ntimation. In CVPR, pages 8981–8989, 2018. 5\n[22] T.-W. Hui, X. Tang, and C. C. Loy.\nA lightweight opti-\ncal ﬂow CNN – Revisiting data ﬁdelity and regularization.\nTPAMI, 43(8):2555–2569, 2021. 5\n[23] A. Johnston and G. Carneiro.\nSelf-supervised monocular\ntrained depth estimation using self-attention and discrete dis-\nparity volume. In CVPR, pages 4756–4765, 2020. 2, 7\n[24] J. Kim, J. K. Lee, and K. M. Lee. Deeply-recursive convolu-\ntional network for image super-resolution. In CVPR, pages\n1637–1645, 2016. 4\n[25] D. P. Kingma and J. Bar. Adam: A method for stochastic\noptimization. In ICLR, 2015. 6\n[26] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N.\nNavab.\nDeeper depth prediction with fully convolutional\nresidual networks. In 3DV, pages 239–248, 2016. 1\n[27] S. Lee, S. Im, S. Lin, and I. S. Kweon. Learning monoc-\nular depth in dynamic scenes via instance-aware projection\nconsistency. In AAAI, pages 1863–1872, 2021. 2, 7, 8\n[28] H. Li, A. Gordon, H. Zhao, V. Casser, and A. Angelova. Un-\nsupervised monocular depth learning in dynamic scenes. In\nCoRL, pages 1908–1917, 2020. 2, 5, 6, 7, 9\n[29] S. Li, F. Xue, X. Wang, Z. Yan, and H. Zha. Sequential ad-\nversarial learning for self-supervised deep visual odometry.\nIn ICCV, pages 2851–2860, 2019. 2, 7\n[30] C. Luo, Z. Yang, P. Wang, Y. Wang, W. Xu, R. Nevatia,\nand A. Yuille. Every pixel counts ++: Joint learning of ge-\nometry and motion with 3d holistic understanding. TPAMI,\n42(10):2624–2641, 2020. 7, 8\n[31] X. Lyu, L. Liu, M. Wang, X. Kong, L. Liu, Y. Liu, X. Chen,\nand Y. Yuan.\nHR-Depth: High resolution self-supervised\nmonocular depth estimation.\nIn AAAI, pages 2294–2301,\n2021. 8\n[32] R. Mahjourian, M. Wicke, and A. Angelovn. Unsupervised\nlearning of depth and ego-motion from monocular video us-\ning 3D geometric constraints. In CVPR, pages 5667–5675,\n2018. 2, 7\n[33] M. Menze and A. Geiger. Object scene ﬂow for autonomous\nvehicles. In CVPR, pages 3061–3070, 2015. 6\n[34] A. Pilzer, S. Lathuili`ere, N. Sebe, and E. Ricci. Reﬁne and\ndistill: Exploiting cycle-inconsistency and knowledge dis-\ntillation for unsupervised monocular depth estimation.\nIn\nCVPR, pages 9768–9777, 2019. 7\n[35] M. Poggi, F. Aleotti, F. Tosi, and S. Mattoccia. On the un-\ncertainty of self-supervised monocular depth estimation. In\nCVPR, pages 3227–3237, 2020. 2, 7\n[36] A. Ranjan, V. Jampani, L. Balles, K. Kim, D. Sun, J. Wulff,\nand M. J. Black. Competitive Collaboration: Joint unsuper-\nvised learning of depth, camera motion, optical ﬂow and mo-\ntion segmentation. In CVPR, pages 12240–12249, 2019. 1,\n2, 6, 7, 8\n[37] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-\ntional networks for biomedical image segmentation. In MIC-\nCAI, pages 234–241, 2015. 4\n[38] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S.\nMa, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C.\nBerg, and F.-F. Li. ImageNet large scale visual recognition\nchallenge. IJCV, 115:211–252, 2015. 6\n[39] W. Shi, J. Caballero, F. Husz´ar, J. Totz, A. P. Aitken, R.\nBishop, D. Rueckert, and Z. Wang.\nReal-time single im-\nage and video super-resolution using an efﬁcient sub-pixel\nconvolutional neural network. In CVPR, pages 1874–1883,\n2016. 2, 5, 8, 9\n[40] A. Stone, D. Maurer, A. Ayvaci, A. Angelova, and R. Jon-\nschkowski.\nSMURF: Self-teaching multi-frame unsuper-\nvised RAFT with full-image warping. In CVPR, pages 3887–\n3896, 2021. 6\n10\n[41] F. Tosi, F. Aleotti, P. Z. Ramirez, M. Poggia, S. Salti, L. D.\nStefano, and S. Mattoccia. Distilled semantics for compre-\nhensive scene understanding from videos. In CVPR, pages\n4654–4665, 2020. 7, 8\n[42] Y. Wang, P. Wang, Z. Yang, C. Luo, Y. Yang, and W. Xu.\nUnOS: Uniﬁed unsupervised optical-ﬂow and stereo-depth\nestimation by watching videos. In CVPR, pages 8071–8081,\n2019. 2\n[43] J. Watson, O. M. Aodha, V. Prisacariu, G. Brostow, and M.\nFirman. The temporal opportunist: Self-supervised multi-\nframe monocular depth. In CVPR, pages 1164–1174, 2021.\n1, 2, 6\n[44] N. Yang, L. Stumberg, R. Wang, and D. Cremers. D3VO:\nDeep depth, deep pose and deep uncertainty for monocular\nvisual odometry. In CVPR, pages 1281–1292, 2020. 3\n[45] Z. Yin and J. Shi. GeoNet: Unsupervised learning of dense\ndepth, optical ﬂow and camera pose. In CVPR, pages 1983–\n1992, 2018. 1, 2, 6, 7, 8\n[46] M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive decon-\nvolutional networks for mid and high level feature learning.\nIn ICCV, pages 2018–2025, 2011. 2, 5\n[47] H. Zhan, R. Garg, C. S.Weerasekera, K. Li, H. Agarwal, and\nI. Rei. Unsupervised learning of monocular depth estimation\nand visual odometry with deep feature reconstruction.\nIn\nCVPR, pages 340–349, 2018. 3\n[48] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsu-\npervised learning of depth and ego-motion from video. In\nCVPR, pages 1851–1858, 2017. 1, 2, 3, 4, 5, 6, 7\n[49] Y. Zou, Z. Luo, and J.-B. Huang. DF-Net: Unsupervised\njoint learning of depth and ﬂow using cross-task consistency.\nIn ECCV, pages 38–55, 2018. 7\n11\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2023-03-08",
  "updated": "2023-03-08"
}