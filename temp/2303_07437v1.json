{
  "id": "http://arxiv.org/abs/2303.07437v1",
  "title": "Unsupervised Representation Learning in Partially Observable Atari Games",
  "authors": [
    "Li Meng",
    "Morten Goodwin",
    "Anis Yazidi",
    "Paal Engelstad"
  ],
  "abstract": "State representation learning aims to capture latent factors of an\nenvironment. Contrastive methods have performed better than generative models\nin previous state representation learning research. Although some researchers\nrealize the connections between masked image modeling and contrastive\nrepresentation learning, the effort is focused on using masks as an\naugmentation technique to represent the latent generative factors better.\nPartially observable environments in reinforcement learning have not yet been\ncarefully studied using unsupervised state representation learning methods.\n  In this article, we create an unsupervised state representation learning\nscheme for partially observable states. We conducted our experiment on a\nprevious Atari 2600 framework designed to evaluate representation learning\nmodels. A contrastive method called Spatiotemporal DeepInfomax (ST-DIM) has\nshown state-of-the-art performance on this benchmark but remains inferior to\nits supervised counterpart. Our approach improves ST-DIM when the environment\nis not fully observable and achieves higher F1 scores and accuracy scores than\nthe supervised learning counterpart. The mean accuracy score averaged over\ncategories of our approach is ~66%, compared to ~38% of supervised learning.\nThe mean F1 score is ~64% to ~33%.",
  "text": "Unsupervised State Representation Learning in\nPartially Observable Atari Games\nLi Meng1[0000−0002−8867−9104], Morten Goodwin2,3[0000−0001−6331−702X], Anis\nYazidi3[0000−0001−7591−1659], and Paal Engelstad1[0009−0000−8371−927X]\n1 University of Oslo, Norway,\n2 University of Agder, Norway\n3 Oslo Metropolitan University, Norway\nAbstract. State representation learning aims to capture latent factors\nof an environment. Contrastive methods have performed better than\ngenerative models in previous state representation learning research. Al-\nthough some researchers realize the connections between masked image\nmodeling and contrastive representation learning, the eﬀort is focused on\nusing masks as an augmentation technique to represent the latent gener-\native factors better. Partially observable environments in reinforcement\nlearning have not yet been carefully studied using unsupervised state\nrepresentation learning methods.\nIn this article, we create an unsupervised state representation learning\nscheme for partially observable states. We conducted our experiment\non a previous Atari 2600 framework designed to evaluate representation\nlearning models. A contrastive method called Spatiotemporal DeepInfo-\nmax (ST-DIM) has shown state-of-the-art performance on this bench-\nmark but remains inferior to its supervised counterpart. Our approach\nimproves ST-DIM when the environment is not fully observable and\nachieves higher F1 scores and accuracy scores than the supervised learn-\ning counterpart. The mean accuracy score averaged over categories of\nour approach is ∼66 %, compared to ∼38 % of supervised learning.\nThe mean F1 score is ∼64 % to ∼33 %. The code can be found on\nhttps://github.com/mengli11235/MST_DIM.\nKeywords: State representation Learning · Contrastive learning.\n1\nIntroduction\nDeep representation learning is a machine learning (ML) type that focuses on\nlearning useful data representations. These representations can be learned using\ndeep neural networks (NNs) and transferred to a variety of downstream computer\nvision (CV), and natural language processing (NLP) tasks [7, 15]. Deep repre-\nsentation learning includes autoencoders [14], generative models [9], contrastive\nmethods [11,20] and transformer models [6].\nState representation learning (SRL) [1, 13, 18] is a particular ﬁeld of repre-\nsentation learning where the state observations are commonly seen in the rein-\nforcement learning (RL) setup. Agents can interact with the environment, which\narXiv:2303.07437v1  [cs.LG]  13 Mar 2023\n2\nL. Meng et al.\nitself changes accordingly throughout interactions. RL is a well-established ML\nﬁeld that solves the Markov decision process (MDP) [21]. Traditional RL meth-\nods such as Q-learning [26] have evolved through adopting NNs [19]. Moreover,\nconvolutional neural networks (CNNs) are deployed in environments with image\ninputs, and RL agents can learn from raw pixels.\nPartially observable Markov decision processes (PODMPs) [3,23] are MDPs\nwhere an agent can only observe a limited part of the environment, not the full\nstate. Recently, there have been some developments in decoupling SRL from\nRL [10, 16, 17, 22]. More improvements by using SRL have been reported in\npartially observable environments than in fully observable ones. However, it is\nnot clear how the POMDP state is captured and preserved by representations.\nThis paper designs an unsupervised representation learning scheme for par-\ntially observable environments. This method extends ST-DIM [1] and introduces\nan unsupervised pretraining setting suitable to partially observable Atari Games.\nDiﬀerent pretraining hyper-parameter choices are also discussed in our ablation\nstudy.\nOur contribution is summarized as follows: (1) We propose MST-DIM, a\ncontrastive method suitable to pretrain data collected in a partially observable\nenvironment. (2) We test our method on the SRL benchmark using 20 Atari\n2600 games and compare the results with the ST-DIM and supervised methods.\n(3) Should the percentages of the observable parts of states be the same in\npretraining and in probing? Extensive evaluations are conducted to examine\nwhat is needed for SRL in order to let the model accurately predict the ground\ntruth labels.\n2\nRelated Work\nSelf-supervised Learning Self-supervised learning learns useful representa-\ntions from unlabeled data, which can be used in various downstream tasks. The\nmethodology has played an important role in NLP [6] and CV ﬁelds. Contrastive\nPredictive Coding (CPC) [20] learns predictive representations by capturing the\ninformation that is maximally useful to predict future (spatial or temporal) sam-\nples. SimCLR [4] provides a simple yet eﬀective framework for contrastive learn-\ning. Momentum Contrast (MoCo) keeps dynamic dictionaries for contrastive\nlearning. Self-supervised Vision Transformers [5] study the usage of ViTs ( [8])\non Momentum Contrast (MoCo) [11].\nContrastive Representations for RL Contrastive Unsupervised Representa-\ntions for RL (CURL) [16] is an RL pipeline that extracts high-level features using\nan auxiliary contrastive loss, which can be combined with on-policy or oﬀ-policy\nRL algorithms. Masked Contrastive Representation Learning (M-CURL) [27]\nimproves the data eﬃciency in CURL by considering the correlation among\nconsecutive inputs and using masks to help the transformer module learn to re-\nconstruct the features of the ground truth. The loss is deﬁned as a sum of the RL\nloss and masked contrastive loss, and the transformer module is discarded during\nUnsupervised State Representation Learning in Games\n3\ninference. M-CURL reportedly outperforms CURL on 21 out of 26 environments\nfrom Atari 2600 Games.\nRepresentation Learning in POMDPs Predictions of Bootstrapped Latents\n(PBL) [10] is an RL algorithm designed for the multitask setting. PBL is trained\nby predicting future latent observations from partial histories and the current\nstates from latent observations. Histories in POMDPs are typically compressed\ninto a current agent state using NNs [23]. Agents trained with PBL achieve signif-\nicantly higher human normalized scores than baseline methods in the partially\nobservable DMLab 30 environments, but the gap is at most minimal in fully\nobservable environments. Augmented Temporal Contrast (ATC) [22] associates\ntemporally close pairs of observations and also shows its usefulness in POMDPs.\n3\nMethod\nST-DIM [1] is a method that captures the latent generative factors through max-\nimizing mutual information lower-bound estimate over consecutive observations\nxt and xt+1 given a set of cross-episode observations χ = {x1, x2, ..., xn}, origi-\nnated from agents interacting with RL environments. It uses infoNCE [20] that\nmaximizes Eq. 1 as the mutual information estimator between patches as Deep\nInfoMax (DIM) [12] does.\nINCE({(xi, yi)}N\ni=1) =\nN\nX\ni=1\nlog\nexp f(xi, yi)\nPN\nj=1 exp f(xi, yj)\n(1)\nFor any i, (xi, yi) is called positive examples from the joint distribution p(x, y)\nand (xi, yj) from the product of marginals p(x)p(y) is called negative examples\nfor any i ̸= j. Meanwhile, f(x, y) is a score function, i.e., a bilinear layer.\nST-DIM utilizes both the global-local (Eq. 2) and local-local objective (Eq.\n3). An illustration of the global-local contrastive task is also shown in Figure 1.\nThe diﬀerence between the local-local and global-local tasks is that an additional\nMLP is used to extract the global features.\nLGL =\nM\nX\nm=1\nN\nX\nn=1\n−log\nexp gm,n(xt, xt+1)\nP\nxt∗∈Xnext exp gm,n(xt, xt∗)\n(2)\nLLL =\nM\nX\nm=1\nN\nX\nn=1\n−log\nexp fm,n(xt, xt+1)\nP\nxt∗∈Xnext exp fm,n(xt, xt∗)\n(3)\nHere, M and N are the height and width, gm,n(xt, xt+1) = φ(xt)T Wgφm,n(xt+1)\nand φ(m, n) is the local feature vector produced by convolutional layers in the\nrepresentation encoder φ at the location (m, n). On the other hand, fm,n(xt, xt+1) =\nφm,n(xt)T Wlφm,n(xt+1). Observations xt and xt+1 are temporally adjacent, whereas\nxt∗is a randomly sampled observation from the minibatch.\nIn order to ﬁt ST-DIM into partially observable environments, we propose\nMST-DIM and deﬁne random masks kt for each consecutive pair (xt, xt+1) that\nis drawn from a binomial distribution K. Therefore, Eq. 2 and Eq. 3 are modiﬁed\nas Eq. 4 and Eq. 5:\n4\nL. Meng et al.\nxt\nAnchor\nxt+1\nPositive\nxt*\nNegative\nConv Layers\nPositive Local Feature\nConv Layers\nNegative Local Feature\nConv Layers\nGlobal Feature\nMLP\nBilinear Layer\nBilinear Layer\nDiscriminator\nFig. 1. An illustration of the global-local contrastive task in ST-DIM. For the local-\nlocal contrastive task, we discard the MLP and use the local feature of the anchor.\nLMGL =\nM\nX\nm=1\nN\nX\nn=1\n−log\nexp gm,n(xtkt, xt+1)\nP\nxt∗∈Xnext exp gm,n(xtkt, xt∗)\n(4)\nLMLL =\nM\nX\nm=1\nN\nX\nn=1\n−log\nexp fm,n(xtkt, xt+1)\nP\nxt∗∈Xnext exp fm,n(xtkt, xt∗)\n(5)\nMeanwhile, we can tune the probability of masking in K for both the pre-\ntraining and probing. For example, a masking ratio of 0.4 means that 40 % of\nthe full observation is not visible to the agent.\n4\nExperimental Details\nOur experiment is conducted among 20 Atari games of Arcade Learning Envi-\nronment (ALE). Performances are evaluated by probe accuracy and F1 scores\nfor each game. Because ALE does not directly provide ground truth infor-\nmation, ST-DIM has been conducted on the newly designed Atari Annotated\nRAM Interface (AtariARI) [1] that exposes the state variables from the source\ncode [24] in 22 games. State variables are categorized as agent localization (Agent\nLoc.), small object localization (Small Loc.), other localization (Other Loc.),\nscore/clock/lives/display, and miscellaneous (Misc.). Detailed descriptions of\nstates for each game across categories can be found in the original paper [1].\nWe also summarize probe accuracy and F1 scores across those state categories\nfor each game in our experiment. Not all categories are available for each game,\nand we only include results from applicable ones.\nDue to practical implementation issues, we exclude Berzerk, Riverraid and\nYars Revenge and include Battle Zone in our experiment, making a total number\nof 20 games. Trajectories collected by random agents are used in our experiment,\nas ST-DIM [1] suggested it can be a better choice than using trajectories from\nPPO agents.\nFor pretraining, we use diﬀerent partially observable setups. To verify if the\nmasking ratio in pretraining can be diﬀerent from the probing, ﬁve types of\npretraining images have been considered in our experiment, as illustrated by\nUnsupervised State Representation Learning in Games\n5\nTable 1. Parameter Choices\nHyper-parameter\nValue\nImage Size\n160 × 210\nBatch Size\n64\nLearning Rate\n3e-4\nEntropy Threshold\n0.6\nPretraining Steps\n80000\nProbe Training Steps\n35000\nProbe Testing Steps\n10000\n(a) Original Image\n(b) 20% Masked\n(c) 40% Masked\n(d) 60% Masked\n(e) 80% Masked\nFig. 2. The original image is masked by diﬀerent percentages of random noise.\nFigure 2. The images can be original, 20% masked, 40% masked, 60% masked,\nor 80% masked.\nWe follow the same probing protocol as ST-DIM and focus on the explicitness,\ni.e., how well the latent generative factors can be recovered. This is done by\ntraining a linear classiﬁer that predicts the state variables using the learned\nrepresentations. We keep the hyper-parameters the same as ST-DIM to make\nour experiment comparable. A short list of hyper-parameters is shown in Table\n1. Setting the entropy threshold removes large objects that have low entropy from\nthe labels. The encoder architecture is the same as in ST-DIM, as illustrated by\nFigure 3.\n5\nResults\nIn this section, we demonstrate the performance of unsupervised representation\nlearning in the partially observable reinforcement learning environment. Table 2\nshows the F1 scores for each game and Table 3 shows the accuracy. \"Observable\"\nrepresents the setting where the probing is implemented with full observations.\n\"Non-Observable\" is the ST-DIM setting where the pretraining is with full ob-\nservations, but the probing are with partial observations. \"Supervised\" is the\nsetting where no pretraining is included but the model is trained and tested\nonly in probing using the supervised manner. \"Ratio\" indicates the masking ra-\ntio in probing, e.g., ratio 0.2 equals that 20% part of each observation in probing\nis not visible. By default, the masking ratio in probing is set to 0.4.\nIt is obvious that the model is capable of predicting the state variables the\nmost in fully observable environments, as the observations in probing are not\nmasked by noise. The rest results are all from models trained and tested with\nmasked observations for probing. The performance deteriorates considerably if\nthe model still uses ST-DIM and pretrains on full observations. Supervised train-\n6\nL. Meng et al.\nFig. 3. Architecture of the encoder.\ning also exhibits the same characteristic and obtains similar results with ST-DIM.\nOn the other hand, MST-DIM achieves signiﬁcantly higher accuracy and F1\nscores by taking advantage of masked pretraining. Pretraining with a diﬀerent\nmasking ratio has also shown improvements over ST-DIM. Using masking ratios\n0.2 and 0.6 in pretraining yields slightly inferior results than the default masking\nratio of 0.4. Surprisingly, using a masking ratio of 0.8 still improves the ST-DIM\ndespite most of the images being masked in this setting.\nThe mean accuracy score (0.66) and F1 score (0.64) of MST-DIM are slightly\nworse than the accuracy score (0.71) and F1 score (0.7) under the fully observable\nsetup. However, these results considerably exceed those of supervised learning\n(0.38 and 0.33) and ST-DIM (0.38 and 0.34) under the same partially observable\nsetup. For the other three masking ratios, a ratio of 0.6 achieves the highest\naccuracy (0.63) and F1 scores (0.61), a ratio of 0.2 achieves slightly lower scores\n(0.61 and 0.59), and a ratio of 0.8 obtains the worst accuracy and F1 scores (0.53\nand 0.5).\nMeanwhile, Table 4 and Table 5 show the results for each category averaged\nover games. It is clear that MST-DIM still performs the best category-wise in\npartially observable environments, and achieves scores that is slightly worse than\nST-DIM that is probed under fully observable environments. A diﬀerent masking\nratio in pretraining still enhances the model’s capability across games in probing\ntasks, and even a masking ratio (0.8) that is remote from the probing masking\nratio (0.4) can facilitate achieving better scores for each ground truth category.\n6\nDiscussion\nIt was found that there was a sizable gap between the performances of ST-DIM\nand supervised training by [1]. However, their diﬀerence is trivial under our par-\ntially observable setting. Meanwhile, MST-DIM has demonstrated better per-\nformance than both of them. The reason might be that ST-DIM and supervised\nmethods do not possess better initializations, and yield similarly deteriorated\nresults in partially observable environments.\nOn the other hand, Randomly initialized CNNs can perform reasonably well\nin probing tasks. Their scores are only slightly lower than those of generative\nUnsupervised State Representation Learning in Games\n7\nTable 2. Probe F1 scores of each game averaged across categories\nGames\nObservable Non-observable Supervised Pretrain Ratio 0.2 Ratio 0.6 Ratio 0.8\nAsteroids\n0.46\n0.39\n0.39\n0.45\n0.45\n0.44\n0.44\nBattle Zone\n0.5\n0.29\n0.28\n0.45\n0.39\n0.41\n0.38\nBowling\n0.96\n0.29\n0.29\n0.9\n0.72\n0.85\n0.63\nBoxing\n0.59\n0.11\n0.1\n0.53\n0.38\n0.43\n0.2\nBreakout\n0.87\n0.37\n0.37\n0.85\n0.83\n0.85\n0.29\nDemon Attack\n0.66\n0.46\n0.45\n0.64\n0.6\n0.63\n0.57\nFreeway\n0.81\n0.03\n0.03\n0.27\n0.27\n0.1\n0.05\nFrostbite\n0.72\n0.33\n0.34\n0.7\n0.65\n0.66\n0.58\nHero\n0.92\n0.57\n0.58\n0.9\n0.87\n0.88\n0.84\nMs Pacman\n0.7\n0.35\n0.36\n0.69\n0.64\n0.68\n0.62\nMontezuma Revenge\n0.77\n0.54\n0.53\n0.75\n0.74\n0.74\n0.71\nPitfall\n0.68\n0.24\n0.25\n0.62\n0.53\n0.6\n0.54\nPong\n0.81\n0.13\n0.13\n0.71\n0.69\n0.65\n0.42\nPrivate Eye\n0.88\n0.5\n0.49\n0.84\n0.81\n0.82\n0.68\nQbert\n0.72\n0.47\n0.47\n0.71\n0.69\n0.71\n0.69\nSeaquest\n0.64\n0.38\n0.37\n0.62\n0.59\n0.61\n0.54\nSpace Invaders\n0.56\n0.45\n0.44\n0.56\n0.55\n0.57\n0.56\nTennis\n0.6\n0.13\n0.13\n0.48\n0.34\n0.4\n0.23\nVenture\n0.55\n0.39\n0.4\n0.54\n0.53\n0.53\n0.52\nVideo Pinball\n0.62\n0.29\n0.3\n0.62\n0.57\n0.63\n0.6\nMean\n0.7\n0.34\n0.33\n0.64\n0.59\n0.61\n0.5\nmethods in [1], because random CNNs are considered a strong prior in Atari\ngames and can capture the inductive bias [2,25].\nDiﬀerent masking ratios in pretraining have shown to be eﬀective, even for a\nlarge masking ratio that generates visually invisible images. Although the closer\nthe pretraining masking ratio is, the more accurate the probing prediction can\nbe, the masking ratio in pretraining is not required to be the same. The results\nindicate that unsupervised pretraining can learn reliable latent generative factors\nfrom a diﬀerent data distribution. Unlike in fully observable environments, this\nis strong evidence that contrastive methods can play a key role in strengthening\nmodel capabilities in partially observable environments for downstream tasks.\nThe results of small object localization in Table 4 and 5 are highlighted in\nST-DIM because generative methods typically do not penalize enough for not\nmodeling the pixels making up small objects. The local-local contrastive task\nin ST-DIM is specialized in capturing local representation. In our experiment,\nit is clear that the performances of ST-DIM and the supervised method have\ndropped signiﬁcantly in partially observable experiments because small objects\nwith few pixels can be easily masked out completely. On the other hand, MST-\nDIM overcomes this problem by masked pretraining and is close to achieving the\nsame level of performance (0.47 to 0.53 for F1 scores and 0.5 to 0.55 in accuracy\nscores). However, MST-DIM has suﬀered from pretraining with a masking ratio\nof 0.8, which masks most of the image.\nIn some games, such as boxing, easy-to-learn features might saturate the ob-\njective and let contrastive methods fail. For example, contrastive methods other\nthan ST-DIM fail to model features besides the clock in boxing [1]. This is also\na problem in partially observable environments and causes ST-DIM, supervised\nmethod, and MST-DIM with a masking ratio of 0.8 to perform worse. On the\n8\nL. Meng et al.\nTable 3. Probe accuracy scores of each game averaged across categories\nGames\nObservable Non-observable Supervised Pretrain Ratio 0.2 Ratio 0.6 Ratio 0.8\nAsteroids\n0.5\n0.46\n0.46\n0.5\n0.5\n0.5\n0.49\nBattle Zone\n0.52\n0.36\n0.35\n0.48\n0.44\n0.45\n0.44\nBowling\n0.96\n0.36\n0.36\n0.91\n0.73\n0.85\n0.66\nBoxing\n0.59\n0.14\n0.13\n0.54\n0.39\n0.44\n0.22\nBreakout\n0.88\n0.41\n0.41\n0.86\n0.84\n0.86\n0.37\nDemon Attack\n0.66\n0.47\n0.47\n0.65\n0.6\n0.64\n0.58\nFreeway\n0.81\n0.06\n0.06\n0.3\n0.3\n0.15\n0.09\nFrostbite\n0.73\n0.38\n0.38\n0.7\n0.66\n0.67\n0.59\nHero\n0.92\n0.59\n0.59\n0.9\n0.88\n0.88\n0.84\nMs Pacman\n0.71\n0.4\n0.4\n0.7\n0.66\n0.69\n0.65\nMontezuma Revenge\n0.77\n0.55\n0.54\n0.76\n0.74\n0.74\n0.72\nPitfall\n0.69\n0.3\n0.3\n0.64\n0.55\n0.62\n0.57\nPong\n0.82\n0.21\n0.21\n0.73\n0.7\n0.67\n0.47\nPrivate Eye\n0.88\n0.52\n0.51\n0.84\n0.81\n0.83\n0.68\nQbert\n0.73\n0.51\n0.51\n0.72\n0.7\n0.71\n0.69\nSeaquest\n0.66\n0.46\n0.45\n0.63\n0.61\n0.63\n0.57\nSpace Invaders\n0.57\n0.48\n0.47\n0.59\n0.57\n0.59\n0.58\nTennis\n0.61\n0.22\n0.22\n0.51\n0.39\n0.44\n0.29\nVenture\n0.56\n0.41\n0.42\n0.55\n0.54\n0.54\n0.53\nVideo Pinball\n0.63\n0.31\n0.32\n0.62\n0.57\n0.63\n0.61\nMean\n0.71\n0.38\n0.38\n0.66\n0.61\n0.63\n0.53\nTable 4. Probe F1 scores of diﬀerent ground truth categories averaged across all games\nCategories\nObservable Non-observable Supervised Pretrain Ratio 0.2 Ratio 0.6 Ratio 0.8\nAgent Loc.\n0.58\n0.26\n0.26\n0.52\n0.48\n0.5\n0.41\nMisc.\n0.73\n0.48\n0.48\n0.72\n0.68\n0.7\n0.61\nOther Loc.\n0.64\n0.34\n0.34\n0.59\n0.56\n0.54\n0.47\nScore/Clock/Lives/Display\n0.9\n0.42\n0.42\n0.86\n0.77\n0.83\n0.7\nSmall Loc.\n0.53\n0.21\n0.21\n0.47\n0.42\n0.44\n0.28\nother hand, MST-DIM with a masking ratio close to 0.4 exhibits robustness as\nST-DIM in fully observable environments.\nFor the study of diﬀerent masking ratios, we observe that a masking ratio\nthat is slightly higher than one of the underlying observations obtains the best\naccuracy and F1 scores when the original ratio is not available. Thus, it suggests\nthat a higher masking ratio facilitates unsupervised representation learning in\npartially observable environments. If we were to choose between decreasing or\nincreasing the ratio with the same amount in pretraining, increasing the number\ncould be a reasonable choice.\n7\nConclusion\nWe propose MST-DIM in this paper to deal with partially observable environ-\nments through pretraining. MST-DIM is a contrastive method based on an esti-\nmate of mutual information bound and uses masking in unsupervised pretraining\nto ensure the agent can learn reliable latent generative factors. Experiments are\nconducted using a benchmark of unsupervised learning on the annotated inter-\nface of Atari 2600 games. MST-DIM shows the beneﬁt of using unsupervised\nrepresentation learning in partially observable environments by achieving higher\naccuracy and F1 scores than ST-DIM and supervised learning.\nUnsupervised State Representation Learning in Games\n9\nTable 5. Probe accuracy scores of diﬀerent ground truth categories\nCategories\nObservable Non-observable Supervised Pretrain Ratio 0.2 Ratio 0.6 Ratio 0.8\nAgent Loc.\n0.59\n0.32\n0.32\n0.54\n0.5\n0.52\n0.44\nMisc.\n0.74\n0.52\n0.52\n0.73\n0.69\n0.71\n0.64\nOther Loc.\n0.65\n0.38\n0.38\n0.59\n0.58\n0.55\n0.49\nScore/Clock/Lives/Display\n0.9\n0.44\n0.44\n0.87\n0.78\n0.84\n0.71\nSmall Loc.\n0.55\n0.29\n0.29\n0.5\n0.46\n0.47\n0.34\nFor future work, it would be interesting to directly apply MST-DIM to RL\nenvironments and evaluate its performance using RL baselines. Designing an\nauxiliary contrastive loss in RL is typical, but the implementation details can\nvary among diﬀerent research. Exploiting the weight initialization of pretrained\nrepresentation encoders that resembles more to CV probing tasks can also be an\nintriguing topic.\nRecognizing small objects can be a challenging task in partially observable\nenvironments. ST-DIM deploys the local-local contrastive task to reliably learn\nthe representations of small objects. However, it still suﬀers from information loss\nand obtains worse results in partially observable environments. MST-DIM deals\nwith this issue and achieves scores close to ST-DIM under the fully observable\nsetup. It still remains to be studied how to recognize small objects where the\nenvironment is almost invisible, and information loss is severe.\nAcknowledgements This work was performed on the [ML node] resource,\nowned by the University of Oslo, and operated by the Department for Research\nComputing at USIT, the University of Oslo IT-department. http://www.hpc.uio.no/\nReferences\n1. Anand, A., Racah, E., Ozair, S., Bengio, Y., Côté, M.A., Hjelm, R.D.: Unsupervised\nstate representation learning in atari. Advances in neural information processing\nsystems 32 (2019)\n2. Burda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T., Efros, A.A.: Large-\nscale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355 (2018)\n3. Cassandra, A.R., Kaelbling, L.P., Littman, M.L.: Acting optimally in partially\nobservable stochastic domains. In: Aaai. vol. 94, pp. 1023–1028 (1994)\n4. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-\ntrastive learning of visual representations. In: International conference on machine\nlearning. pp. 1597–1607. PMLR (2020)\n5. Chen, X., Xie, S., He, K.: An empirical study of training self-supervised vision\ntransformers. In: Proceedings of the IEEE/CVF International Conference on Com-\nputer Vision. pp. 9640–9649 (2021)\n6. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n7. Doersch, C., Zisserman, A.: Multi-task self-supervised visual learning. In: Pro-\nceedings of the IEEE international conference on computer vision. pp. 2051–2060\n(2017)\n10\nL. Meng et al.\n8. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n9. Gregor, K., Danihelka, I., Graves, A., Rezende, D., Wierstra, D.: Draw: A recur-\nrent neural network for image generation. In: International conference on machine\nlearning. pp. 1462–1471. PMLR (2015)\n10. Guo, Z.D., Pires, B.A., Piot, B., Grill, J.B., Altché, F., Munos, R., Azar, M.G.:\nBootstrap latent-predictive representations for multitask reinforcement learning.\nIn: International Conference on Machine Learning. pp. 3875–3886. PMLR (2020)\n11. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised\nvisual representation learning. In: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition. pp. 9729–9738 (2020)\n12. Hjelm, R.D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P.,\nTrischler, A., Bengio, Y.: Learning deep representations by mutual information\nestimation and maximization. arXiv preprint arXiv:1808.06670 (2018)\n13. Jonschkowski, R., Brock, O.: Learning state representations with robotic priors.\nAutonomous Robots 39(3), 407–428 (2015)\n14. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114 (2013)\n15. Kolesnikov, A., Zhai, X., Beyer, L.: Revisiting self-supervised visual representation\nlearning. In: Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition. pp. 1920–1929 (2019)\n16. Laskin, M., Srinivas, A., Abbeel, P.: Curl: Contrastive unsupervised representations\nfor reinforcement learning. In: International Conference on Machine Learning. pp.\n5639–5650. PMLR (2020)\n17. Lee, K.H., Fischer, I., Liu, A., Guo, Y., Lee, H., Canny, J., Guadarrama, S.: Pre-\ndictive information accelerates learning in rl. Advances in Neural Information Pro-\ncessing Systems 33, 11890–11901 (2020)\n18. Lesort, T., Díaz-Rodríguez, N., Goudou, J.F., Filliat, D.: State representation\nlearning for control: An overview. Neural Networks 108, 379–392 (2018)\n19. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,\nRiedmiller, M.: Playing atari with deep reinforcement learning (2013)\n20. Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748 (2018)\n21. Puterman, M.L.: Markov decision processes. Handbooks in operations research and\nmanagement science 2, 331–434 (1990)\n22. Stooke, A., Lee, K., Abbeel, P., Laskin, M.: Decoupling representation learning\nfrom reinforcement learning. In: International Conference on Machine Learning.\npp. 9870–9879. PMLR (2021)\n23. Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduction. MIT press\n(2018)\n24. Taylor, L.N., Whalen, Z.: Playing the past: History and nostalgia in video games.\nJSTOR (2008)\n25. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Deep image prior. In: Proceedings of the\nIEEE conference on computer vision and pattern recognition. pp. 9446–9454 (2018)\n26. Watkins, C.J., Dayan, P.: Q-learning. Machine learning 8(3-4), 279–292 (1992)\n27. Zhu, J., Xia, Y., Wu, L., Deng, J., Zhou, W., Qin, T., Liu, T.Y., Li, H.: Masked\ncontrastive representation learning for reinforcement learning. IEEE Transactions\non Pattern Analysis and Machine Intelligence (2022)\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-03-13",
  "updated": "2023-03-13"
}