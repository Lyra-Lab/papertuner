{
  "id": "http://arxiv.org/abs/2303.08809v2",
  "title": "Cascading and Direct Approaches to Unsupervised Constituency Parsing on Spoken Sentences",
  "authors": [
    "Yuan Tseng",
    "Cheng-I Lai",
    "Hung-yi Lee"
  ],
  "abstract": "Past work on unsupervised parsing is constrained to written form. In this\npaper, we present the first study on unsupervised spoken constituency parsing\ngiven unlabeled spoken sentences and unpaired textual data. The goal is to\ndetermine the spoken sentences' hierarchical syntactic structure in the form of\nconstituency parse trees, such that each node is a span of audio that\ncorresponds to a constituent. We compare two approaches: (1) cascading an\nunsupervised automatic speech recognition (ASR) model and an unsupervised\nparser to obtain parse trees on ASR transcripts, and (2) direct training an\nunsupervised parser on continuous word-level speech representations. This is\ndone by first splitting utterances into sequences of word-level segments, and\naggregating self-supervised speech representations within segments to obtain\nsegment embeddings. We find that separately training a parser on the unpaired\ntext and directly applying it on ASR transcripts for inference produces better\nresults for unsupervised parsing. Additionally, our results suggest that\naccurate segmentation alone may be sufficient to parse spoken sentences\naccurately. Finally, we show the direct approach may learn head-directionality\ncorrectly for both head-initial and head-final languages without any explicit\ninductive bias.",
  "text": "CASCADING AND DIRECT APPROACHES TO UNSUPERVISED\nCONSTITUENCY PARSING ON SPOKEN SENTENCES\nYuan Tseng1\nCheng-I Jeff Lai2\nHung-yi Lee1\n1 National Taiwan University\n2 MIT CSAIL\nr11942082@ntu.edu.tw\nABSTRACT\nPast work on unsupervised parsing is constrained to written form.\nIn this paper, we present the ﬁrst study on unsupervised spoken\nconstituency parsing given unlabeled spoken sentences and un-\npaired textual data. The goal is to determine the spoken sentences’\nhierarchical syntactic structure in the form of constituency parse\ntrees, such that each node is a span of audio that corresponds\nto a constituent. We compare two approaches: (1) cascading an\nunsupervised automatic speech recognition (ASR) model and an\nunsupervised parser to obtain parse trees on ASR transcripts, and\n(2) direct training an unsupervised parser on continuous word-level\nspeech representations. This is done by ﬁrst splitting utterances into\nsequences of word-level segments, and aggregating self-supervised\nspeech representations within segments to obtain segment embed-\ndings. We ﬁnd that separately training a parser on the unpaired text\nand directly applying it on ASR transcripts for inference produces\nbetter results for unsupervised parsing.\nAdditionally, our results\nsuggest that accurate segmentation alone may be sufﬁcient to parse\nspoken sentences accurately. Finally, we show the direct approach\nmay learn head-directionality correctly for both head-initial and\nhead-ﬁnal languages without any explicit inductive bias.\nIndex Terms— Unsupervised constituency parsing, unsuper-\nvised word segmentation, self-supervised speech representations\n1. INTRODUCTION\nUnsupervised constituency parsing is a long-standing research chal-\nlenge in natural language processing [1, 2, 3, 4] that aims to auto-\nmatically determine the syntactic constituent structure of sentences\nwithout access to any training labels. It sheds light on how children\nare able to learn high-level linguistic information, such as syntax and\ngrammar, without expert supervision.\nAdditionally, constituency\nparse trees have also been shown to improve and provide greater in-\nterpretability to a variety of downstream tasks such as semantic role\nlabeling [5], word representation learning [6], and speech synthesis\n[7, 8, 9].\nTo the best of our knowledge, syntactic parsing on speech was\ndone exclusively in a supervised manner, using paired text tran-\nscripts and syntactic labels for training. However, these approaches\ncannot be applied to low-resource languages without paired data.\nThis motivates us to explore a more realistic setting where only raw\nspeech and limited unpaired textual data are available.\nWith no speech-text pairs or tree-text pairs, the ﬁrst approach\nto unsupervised spoken constituency parsing is to cascade unsuper-\nvised ASR with an unsupervised parser. We compare training the\nparser on limited unpaired text and ASR transcripts and ﬁnd that\ntraining on ASR transcripts does not help the model parse ASR tran-\nscripts of the same domain.\nWe also propose a framework to directly parse spoken input\nwithout any intermediate textual form. First, we split an utterance\ninto word-level segments, and transform each segment into a con-\ntinuous embedding. Then we directly use this sequence of segment\nembeddings as input for our unsupervised parser. We refer to this as\nthe direct approach.\nContributions.\n(1) We perform the ﬁrst investigation of unsu-\npervised constituency parsing on spoken sentences using only raw\nspeech and unpaired text. (2) We demonstrate that for parsing ASR\ntranscripts, training on limited unpaired text is still better than train-\ning on ASR transcripts, and we quantify the effects of ASR errors\non unsupervised parsing. (3) We propose a framework to directly\nparse continuous speech without intermediate lexical unit discovery.\n(4) We show that our direct approach may induce parse trees with\nthe correct branching direction for different spoken languages.\n2. RELATED WORK\n2.1. Unsupervised Constituency Parsing\nPrevious studies in unsupervised constituency parsing focus on ob-\ntaining constituency tree structures from large unannotated text cor-\npora, usually by encouraging neural models to follow syntactic struc-\nture [4, 10, 11], or parameterizing linguistic models with neural net-\nworks [12, 13, 14].\nA recent line of work in visually-grounded grammar induction\nleverages paired images to improve unsupervised constituency pars-\ning [15, 16, 17]. We consider AV-NSL [18] in particular to be most\nrelevant to our work, as they extend this approach to audio-visual\nlearning and attempt to learn constituency parse trees from raw\nspeech and image pairs. Unlike AV-NSL, our work does not rely on\npaired visual grounding data.\n2.2. Parsing Speech with Supervision\nPast works on syntactic parsing of speech address topics such as dis-\nﬂuency detection [19, 20], or incorporating prosodic cues [21, 22].\nHowever, most previous works require oracle transcripts, which is\nan unrealistic setting. Yoshikawa et al. [23] shows that it is possible\nto build a supervised dependency parser that jointly detect disﬂu-\nencies and ASR errors, and Pupier et al. [24] build an end-to-end\nsupervised dependency parser for French that jointly predicts tran-\nscription and dependency tree from raw speech signals. Both works\nshow an improvement over cascading baseline systems.\nAdditionally, prosodic features are shown to be closely related to\nsyntax [25, 26] and beneﬁcial for both constituency parsing [27, 28]\nand dependency parsing [29], even under the presence of ASR errors\n[30]. These works motivate us to explore ways of using speech fea-\ntures to improve unsupervised constituency parsing of spoken data.\narXiv:2303.08809v2  [cs.CL]  9 May 2023\nFig. 1.\nDiagram of our proposed direct approach to unsupervised spoken constituency parsing, using only raw speech and unpaired text.\nTextual transcripts of the input sentence are only shown for illustrative purposes.\n2.3. Unsupervised Spoken Language Modeling\nUnsupervised spoken language modeling [31] aims to learn a spo-\nken language model that simultaneously learns different levels of\nlinguistic structure from raw speech signals with little or no textual\ndata. The ZeroSpeech Challenge 2021 [32, 33] proposes to evaluate\nsuch a model at the acoustic, lexical, syntactic, and semantic lev-\nels. They ﬁnd that while current spoken language models excel at\nthe acoustic and lexical levels, higher levels of linguistic structure\nare much more difﬁcult to model. They only require their models to\nbe able to determine how grammatical a sentence is, while our work\naims to solve the more challenging problem of producing the exact\nconstituency structure of a sentence.\n3. METHOD\n3.1. Background\nConstituency parsing is usually formulated under the binary setting\nin order to reduce computation complexity. [34] This setting entails\nthat for a sentence with n words {x1, x2, ..., xn}, each constituent\nspanning xi:j is composed of two constituents spanning xi:k and\nxk+1:j for some k such that i ≤k < j. Our unsupervised parser\nfollows the chart-based Deep Inside-Outside Recursive Autoencoder\n(DIORA) framework proposed by [12] to produce binary parse trees\nwithout using any syntactically labeled data.\n3.1.1. Chart-based Constituency Parsing\nChart-based parsers ﬁnd the optimal tree out of all valid binary parse\ntrees by ﬁlling the upper-triangular portion of an n × n chart with\na score si,j for each cell. For 1 ≤i < j ≤n, the score in cell\n(i, j) represents how likely the span xi:j is a constituent. The CKY\ndynamic programming algorithm [35, 36] is then used to determine\nthe parse tree with the highest total score.\n3.1.2. Unsupervised Parser Architecture: DIORA\nDIORA consists of an encoder and a decoder, and operates similarly\nto masked language models. The framework recursively encodes all\nbut one of the words from the input sentence as a vector, and opti-\nmizes that vector to reconstruct the missing word. The core assump-\ntion is that the most efﬁcient weights to produce such an encoding\ncan be used as scores for chart-based constituency parsing. DIORA\ninitially represents the input sentence with pretrained ELMo charac-\nter embeddings, but subsequent work [17] shows that the framework\ncan also be used with randomly initialized word embeddings.\n3.2. Cascading Parsing with Unsupervised ASR\nA straightforward approach to unsupervised spoken constituency\nparsing is to obtain word-level transcripts from unsupervised ASR,\nthen represent each word with a randomly initialized vector. An\nunsupervised parser can then produce parse trees using these vector\nsequences as input.\nOur unsupervised ASR system adopts the wav2vec-U frame-\nwork [37]. wav2vec-U ﬁrst phonemizes unpaired text data, then\nperforms a series of preprocessing steps on unlabeled speech to pro-\nduce higher-level features with length similar to phoneme sequences.\nThey use adversarial training to train a model to predict phoneme\nsequences from speech features. A weighted ﬁnite-state transducer\n(WFST) trained on the unpaired text data is then used to decode the\noutput into words. Further improvements are be obtained through\nHidden Markov Model (HMM) self-training. The phoneme output\nof the HMM achieves a lower phone error rate and can be decoded\ninto more accurate word-level transcripts.\n3.3. Direct Parsing on Speech Segments\nThe direct parsing approach extends the DIORA framework by train-\ning on continuous word-level speech embeddings instead of ELMo\nembeddings. Using continuous segment representations allows our\nparser to beneﬁt from continuous information in speech, in compar-\nison to the discretization approach recently proposed in spoken lan-\nguage modeling [31]. This design choice is supported by AV-NSL\n[18], which ﬁnds that continuous segment representations outper-\nforms discrete representations for audio-visual parsing.\nFrom each spoken utterance, we prepare (1) frame-level fea-\ntures, and (2) word-level segments. Frame-level features can ex-\ntracted from a pretrained self-supervised speech model such as\nXLSR-53 [37], and word-level segments can be determined with\nunsupervised word segmentation models [38, 39]. Mirroring AV-\nNSL, we represent each segment with a continuous embedding\nparameterized by a simple weighted average of frame-level features.\nWeights are determined by a learnable two-layer MLP that is jointly\noptimized with the parser. This sequence of word-level segment\nembeddings is then directly used as input for our parser, then jointly\noptimized with the reconstruction loss proposed in DIORA [12].\n4. EXPERIMENTS\n4.1. Datasets, Preprocessing, and Hyperparameters\nExperiments are mainly conducted on the SpokenCOCO dataset\n[40], a 742h English read-speech dataset produced by 2.3k speakers\nreading the captions in MSCOCO [41]. Each image in MSCOCO\ncorresponds to 5 captions on average. Following [16], we use the\nspoken captions of the 83k/5k/5k image split for training, validation,\nand testing respectively. The textual captions of the remaining 31k\nimages are used as unpaired text data for unsupervised ASR. We\nfocus on the more practical setting of unsupervised spoken con-\nstituency parsing using speech and unpaired text data only, hence\nwe do not utilize the image data.\nAdditional experiments in Korean are done on the Zeroth-\nKorean corpus1, which contains 51.6hrs of audio spoken by 105\nspeakers for training, and 1.6hrs by 10 different speakers for testing.\nWe use the utterances of 10 speakers in the original training set for\nvalidation.\nWe note that due to a lack of labelled speech data, we are limited\nto experimenting on high-resource languages. Following [15, 16],\nground-truth parse trees are obtained from the outputs of an off-\nthe-shelf parser [42] on the normalized text captions. Punctuation\nis removed from the trees, and we run forced alignment using the\nMontreal Forced Aligner [43] to obtain oracle word boundaries.\nFor all experiments, we use the same hyperparameters as the\nrandomly initialized DIORA experiment in [17], with a batch size\nof 32 and learning rate of 5e −3. We perform unsupervised model\nselection with the reconstruction loss of DIORA on the validation\nset. Our cascading and direct systems are trained for 10 epochs and\n2000 batches respectively, as we found our direct systems to con-\nverge much faster. Further details are available in our training code2.\n4.2. Evaluation\nUnsupervised constituency parsing on text is typically evaluated\nwith F1 score of constituents, where a match is only counted if a\npredicted constituent and a oracle constituent consist of the exact\nsame words. However, erroneous word segmentation or ASR may\nintroduce mismatch in the number of word-level leaves between\nmodel predictions and ground truth parse trees.\nTherefore, we\nmatch the constituents ﬁrst by calculating an alignment between our\nword-level segments and oracle text, similar to SParseval [44].\nWe use forced alignment to determine the spans of audio that\ncorrespond to each word in the oracle sentence. We then compute the\noptimal one-to-one mapping that maximizes total span overlap be-\ntween oracle segmentation and our proposed word segmentations3.\n1https://github.com/goodatlas/zeroth\n2https://github.com/roger-tseng/speech-parsing\n3This mapping is determined via bipartite weight mapping, where the\nnodes are speech segments and the weights are given by the overlap dura-\ntion across nodes.\nThis allows us to ﬁrst match nodes between predicted and ground\ntruth parse trees, and calculate an F1 score that jointly considers\nsegmentation and parsing performance. We include whole sentence\nspans in our evaluation, in order to compare to AV-NSL [18]. For all\nexperiments, we evaluate fully unsupervised parsing with this pro-\nposed F1 score, and report the average and standard deviation of\ncorpus-level F1 of the best model before convergence over ﬁve dif-\nferent random seeds.\n4.3. Results of Cascading Systems\nWe train two unsupervised ASR models to observe how varying ac-\ncuracy of ASR may affect parsing performance. The two models are\ntrained with and without self-training following the original setup of\nwav2vec-U. They are denoted as ASR-ST and ASR respectively. We\nuse a 100-hour subset of speech from the training set, and 150k un-\npaired text sentences as our training data. Word-level transcriptions\nfor the entire SpokenCOCO dataset are decoded from the phoneme\noutput sequences of the ASR models. Word error rate of training set\ntranscripts is 13.15% and 28.25% for AST-ST and ASR.\nThe training set transcripts are then used to train our parser. We\nfollow [16] and use the 10,000 most commonly occurring words in\ntheir respective training set transcripts as the vocabulary set for the\nparser. Since we assume the availability of unpaired text data in\nthe cascade scenario, we also consider training a parser from the\nunpaired text data alone.\nTraining split\nTraining\nTesting\nF1\n(A)\nEntire train set\noracle\noracle\n57.15 ± 2.09\n(B)\nUnpaired text\noracle\nASR-ST\n44.08 ± 1.64\n(C)\nEntire train set\nASR-ST\nASR-ST\n40.53 ± 1.65\n(D)\nUnpaired text\noracle\nASR\n34.97 ± 1.32\n(E)\nEntire train set\nASR\nASR\n31.01 ± 1.17\nTable 1. F1 score of our casacading systems. The leftmost column\nlists whether training data comes from the unpaired text split or the\ntraining split. We also list the F1 score obtained by training and\ntesting our parser on oracle transcripts in row (A) as a topline.\nEffect of ASR errors on parsing. When comparing results across\ndifferent blocks, we can see that parsing accuracy is heavily de-\ngraded when ASR errors are present. Additionally, one might expect\nthat training a parser on ASR transcripts would allow it to better han-\ndle text with ASR errors during inference. However, by comparing\nrows (B)/(C) and rows (D)/(E), we see that training our parser on\nunpaired oracle text is consistently better than training on ASR tran-\nscripts. We note that this occurs in spite of the training set being\nnearly 3x larger than the unpaired text set. We hypothesize that this\nis partially caused by the decoding process of wav2vec-U. Uncom-\nmon words rarely get decoded by the WFST language model. As a\nresult, the ASR transcripts contain fewer types of words compared to\nthe original vocabulary, and parser trained on imperfect transcripts\ndeal with more out-of-vocabulary words. The training set transcripts\nfrom the AST-ST model only use 8.2k words out of the original 16k\nwords present in the ground truth captions.\n4.4. Results of Direct Systems\nFor direct systems, only speech features and word boundaries are\nrequired. We extract frame-level speech features from the 14th layer\nof XLSR-53 [45], a publicly available wav2vec 2.0 model pretrained\non 53 languages. For word boundaries, we naively split all utterances\ninto 0.5-second segments, to encompass approximately one word in\neach segment4.\nSince this method of segmentation is very inaccurate, the pars-\ning results are similarly poor (Table 2 row (D)). However, when pro-\nvided with ground truth segmentation during testing (Table 2 row\n(C)), the parser trained on ﬁxed length segments is able to achieve\na performance similar to the parser trained on ground truth. This\nsuggests that our direct system is limited by segmentation accuracy\nduring inference.\n4.5. A Hybrid Approach: Segmenting speech with word bound-\naries determined by unsupervised ASR\nApproach\nSegmentation\nF1\nTraining\nTesting\n(A)\nAV-NSL\nground truth\nground truth\n55.51\n(B)\nOurs\nground truth\nground truth\n57.11 ± 0.00\n(C)\nDirect\nevery 0.5 sec.\nground truth\n57.10 ± 0.01\n(D)\nDirect\nevery 0.5 sec.\nevery 0.5 sec.\n3.88 ± 0.00\n(E)\nHybrid\nAST-ST\nAST-ST\n40.44 ± 1.72\n(F)\nHybrid\nASR\nASR\n28.49 ± 0.57\nTable 2. F1 score of direct and hybrid systems. We include F1\nscores obtained by training and testing using oracle segmentation in\nrows (A) and (B) as toplines.\nWe compare our systems with AV-NSL under oracle segmentation\nsettings in rows (A) and (B). We ﬁnd that our parser outperforms\nAV-NSL despite not using any visual grounding information. This\nsuggests that the DIORA framework may be better suited for unsu-\npervised spoken constituency parsing.\nWe experimented with a speech-only unsupervised word seg-\nmentation method [46], but found it to be suboptimal. Therefore,\nwe consider a hybrid approach that uses forced alignment to obtain\nword boundaries from unsupervised ASR transcripts. We ﬁnd that\nwhen word boundaries are sufﬁciently accurate, using word bound-\naries alone can achieve similar accuracy to cascading systems, as\nshown in Table 1 row (C) and Table 2 row (E). This implies that ac-\ncurate word segmentation is necessary for unsupervised constituency\nparsing from speech, which aligns with the ﬁndings in AV-NSL.\n4.6. On the Inductive Bias and Trivial Tree Structure\nDue to the head-initial property of English [47], constituency parse\ntrees tend to be right-branching, especially if punctuation is re-\nmoved. On the other hand, for head-ﬁnal languages such as Japanese\nand Korean, trees are left-branching instead.\nIn our direct and hybrid systems, we observe that our models\ntend to converge to producing right-branching trees on Spoken-\n4As a reference, average word length in SpokenCOCO is about 0.4 sec-\nonds, see Appendix of [40].\nCOCO5. It is worthwhile to note that our framework does not\napply any inductive bias that encourages the model to favor right-\nbranching trees; hence, it is non-trivial for such a phenomenon to\nemerge. We hypothesize that our systems learn a language’s branch-\ning direction from continuous spoken input without supervision.\nWe empirically verify this claim by conducting experiments on\nKorean, a primarily left-branching language. Over 5 runs with dif-\nferent random seeds, 3 runs converge towards producing some left-\nbranching structures (Fig. 2), supporting our hypothesis.\nEnglish\nKorean\nRule-based\nLeft branching\n24.68\n27.15\nRight branching\n57.11\n7.60\nSpeech only\n0.5 sec. segmentation\n57.10 ± 0.01\n18.53 ± 8.99\nTable 3.\nResults of direct systems trained on English (right-\nbranching) and Korean (left-branching), respectively, using the same\nsetting as Table 2 row (C).\n(a) Generated parse tree\n(b) Oracle parse tree\nFig. 2. A sample pair of oracle and generated Korean parse trees.\nOnly textual transcripts are shown for ease of visualization.\n5. CONCLUSION\nThe work investigates cascading and direct approaches to perform\nconstituency parsing on speech input, while only requiring raw\nspeech and unpaired data. For cascading systems, we empirically\nshow that parsers trained on ASR transcripts do not parse ASR tran-\nscripts better than parsers trained on unpaired text. For direct and\nhybrid systems, our results suggest that using segmentation alone\nmay be sufﬁcient to produce unsupervised parse trees.\nFor future work, we expect to extend our system to end-to-end\ntraining to jointly optimize word segmentation and parsing. Addi-\ntionally, we also plan to investigate whether unsupervised spoken\nconstituency parsing can be improve other speech processing tasks\nunder low-resource scenarios, such as text-to-speech, spoken ques-\ntion answering, or spoken content retrieval.\nAcknowledgement. We thank the helpful discussions with Freda\nShi, Shang-Wen Li, Ali Elkahky, and Abdelrahman Mohamed. We\nalso thank the National Center for High-performance Computing\n(NCHC) of National Applied Research Laboratories (NARLabs) in\nTaiwan for providing computational and storage resources.\n5We note that right-branching is a difﬁcult baseline even for parsers\ntrained on oracle text. As shown in Table 1 row (A) and [17], unsupervised\ntext parsers only marginally improve on right-branching trees.\n6. REFERENCES\n[1] G. Carroll et al., “Two experiments on learning probabilistic\ndependency grammars from corpora,” Tech. Rep., USA, 1992.\n[2] D. Klein et al., “Corpus-based induction of syntactic structure:\nModels of dependency and constituency,” in ACL, 2004.\n[3] R. Bod, “An all-subtrees approach to unsupervised parsing,”\nin COLING-ACL, 2006.\n[4] Y. Shen et al., “Neural language modeling by jointly learning\nsyntax and lexicon,” in ICLR, 2018.\n[5] E. Strubell et al., “Linguistically-informed self-attention for\nsemantic role labeling,” in EMNLP, 2018.\n[6] A. Kuncoro et al., “Syntactic structure distillation pretraining\nfor bidirectional encoders,” TACL, 2020.\n[7] H. Guo et al., “Exploiting Syntactic Features in a Parsed Tree\nto Improve End-to-End TTS,” in Interspeech, 2019.\n[8] S. Tyagi et al.,\n“Dynamic Prosody Generation for Speech\nSynthesis Using Linguistics-Driven Acoustic Embedding Se-\nlection,” in Interspeech, 2020.\n[9] C. Song et al.,\n“Syntactic representation learning for neu-\nral network based tts with syntactic parse tree traversal,” in\nICASSP, 2021.\n[10] Y. Shen et al., “Ordered neurons: Integrating tree structures\ninto recurrent neural networks,” in ICLR, 2019.\n[11] Y. Kim et al., “Unsupervised recurrent neural network gram-\nmars,” in NAACL-HLT, 2019.\n[12] A. Drozdov et al., “Unsupervised latent tree induction with\ndeep inside-outside recursive auto-encoders,” in NAACL-HLT,\n2019.\n[13] H. Zhu et al.,\n“The return of lexical dependencies: Neural\nlexicalized PCFGs,” TACL, 2020.\n[14] S. Yang et al.,\n“Neural bi-lexicalized PCFG induction,”\nin\nACL-IJCNLP, 2021.\n[15] H. Shi et al., “Visually grounded neural syntax acquisition,” in\nACL, 2019.\n[16] Y. Zhao et al.,\n“Visually grounded compound pcfgs,”\nin\nEMNLP, 2020.\n[17] B. Wan et al., “Unsupervised vision-language grammar induc-\ntion with shared structure modeling,” in ICLR, 2021.\n[18] C. I. Lai et al.,\n“Textless phrase structure induction from\nvisually-grounded speech,” 2023.\n[19] E. Charniak and M. Johnson, “Edit detection and parsing for\ntranscribed speech,” in NAACL, 2001.\n[20] M. Honnibal et al., “Joint incremental disﬂuency detection and\ndependency parsing,” TACL, 2014.\n[21] J. G. Kahn et al., “Effective use of prosody in parsing conver-\nsational speech,” in HLT/EMNLP, 2005.\n[22] M. Dreyer et al., “Exploiting prosody for PCFGs with latent\nannotations,” in Interspeech 2007, 2007.\n[23] M. Yoshikawa et al., “Joint transition-based dependency pars-\ning and disﬂuency detection for automatic speech recognition\ntexts,” in EMNLP, 2016.\n[24] A. Pupier et al., “End-to-End Dependency Parsing of Spoken\nFrench,” in Interspeech, 2022.\n[25] F. Grosjean et al., “The patterns of silence: Performance struc-\ntures in sentence production,” Cognitive Psychology, 1979.\n[26] P. Price et al., “The use of prosody in syntactic disambigua-\ntion,” in Workshop on Speech and Natural Language, 1991.\n[27] Z. Huang et al., “Appropriately handled prosodic breaks help\nPCFG parsing,” in NAACL-HLT, 2010.\n[28] T. Tran et al., “Parsing speech: a neural approach to integrating\nlexical and acoustic-prosodic information,” in NAACL-HLT,\n2018.\n[29] H. Ghaly et al., “Using prosody to improve dependency pars-\ning,” in International Conference on Speech Prosody, 2020.\n[30] T. Tran et al., “Assessing the Use of Prosody in Constituency\nParsing of Imperfect Transcripts,” in Interspeech, 2021.\n[31] K. Lakhotia et al., “On generative spoken language modeling\nfrom raw audio,” TACL, 2021.\n[32] T. A. Nguyen et al.,\n“The zero resource speech benchmark\n2021: Metrics and baselines for unsupervised spoken language\nmodeling,” in NeurIPS SAS Workshop, 2020.\n[33] E. Dunbar et al., “The zero resource speech challenge 2021:\nSpoken language modelling,” arXiv, 2021.\n[34] D. Jurafsky et al., Speech and Language Processing (2nd Edi-\ntion), Prentice-Hall, Inc., USA, 2009.\n[35] T. Kasami, “An efﬁcient recognition and syntax-analysis algo-\nrithm for context-free languages,” Coordinated Science Labo-\nratory Report, 1966.\n[36] D. H Younger, “Recognition and parsing of context-free lan-\nguages in time n3,” Information and control, 1967.\n[37] A. Baevski et al.,\n“Unsupervised speech recognition,”\nNeurIPS, 2021.\n[38] S. Bhati et al.,\n“Segmental Contrastive Predictive Coding\nfor Unsupervised Word Segmentation,” in Proc. Interspeech,\n2021.\n[39] H. Kamper,\n“Word segmentation on discovered phone\nunits with dynamic programming and self-supervised scoring,”\narXiv preprint arXiv:2202.11929, 2022.\n[40] W. N. Hsu et al., “Text-free image-to-speech synthesis using\nlearned segmental units,” in ACL-ICJNLP, 2021.\n[41] T. Y. Lin et al., “Microsoft coco: Common objects in context,”\nin ECCV, 2014.\n[42] N. Kitaev et al., “Constituency parsing with a self-attentive\nencoder,” in ACL, 2018.\n[43] M. McAuliffe et al.,\n“Montreal Forced Aligner: Trainable\nText-Speech Alignment Using Kaldi,” in Interspeech, 2017.\n[44] B. Roark et al.,\n“Sparseval: Evaluation metrics for parsing\nspeech,” in LREC, 2006.\n[45] A. Conneau et al., “Unsupervised Cross-Lingual Representa-\ntion Learning for Speech Recognition,” in Interspeech, 2021.\n[46] T. Fuchs et al., “Unsupervised Word Segmentation using K\nNearest Neighbors,” in Interspeech, 2022.\n[47] M. C. Baker, The atoms of language: The mind’s hidden rules\nof grammar, Basic books, 2008.\n",
  "categories": [
    "cs.CL",
    "eess.AS"
  ],
  "published": "2023-03-15",
  "updated": "2023-05-09"
}