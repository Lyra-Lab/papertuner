{
  "id": "http://arxiv.org/abs/2303.09013v1",
  "title": "Self-Inspection Method of Unmanned Aerial Vehicles in Power Plants Using Deep Q-Network Reinforcement Learning",
  "authors": [
    "Haoran Guan"
  ],
  "abstract": "For the purpose of inspecting power plants, autonomous robots can be built\nusing reinforcement learning techniques. The method replicates the environment\nand employs a simple reinforcement learning (RL) algorithm. This strategy might\nbe applied in several sectors, including the electricity generation sector. A\npre-trained model with perception, planning, and action is suggested by the\nresearch. To address optimization problems, such as the Unmanned Aerial Vehicle\n(UAV) navigation problem, Deep Q-network (DQN), a reinforcement learning-based\nframework that Deepmind launched in 2015, incorporates both deep learning and\nQ-learning. To overcome problems with current procedures, the research proposes\na power plant inspection system incorporating UAV autonomous navigation and DQN\nreinforcement learning. These training processes set reward functions with\nreference to states and consider both internal and external effect factors,\nwhich distinguishes them from other reinforcement learning training techniques\nnow in use. The key components of the reinforcement learning segment of the\ntechnique, for instance, introduce states such as the simulation of a wind\nfield, the battery charge level of an unmanned aerial vehicle, the height the\nUAV reached, etc. The trained model makes it more likely that the inspection\nstrategy will be applied in practice by enabling the UAV to move around on its\nown in difficult environments. The average score of the model converges to\n9,000. The trained model allowed the UAV to make the fewest number of rotations\nnecessary to go to the target point.",
  "text": "Self-Inspection Method of Unmanned Aerial \nVehicles in Power Plants Using Deep Q-Network \nReinforcement Learning \nA Deep-Q network application for autonomous path planning \n \nHaoran Guan \nDepartment of Mechanical and Industrial Engineering \nUniversity of Toronto \nToronto, Canada \nhaoran.guan@mail.utoronto.ca \n \n* Corresponding author: haoran.guan@mail.utoronto.ca\n \n \nAbstract â€“ For the purpose of inspecting power plants, \nautonomous robots can be built using reinforcement learning \ntechniques. The method replicates the environment and \nemploys a simple reinforcement learning (RL) algorithm. This \nstrategy might be applied in several sectors, including the \nelectricity generation sector. A pre-trained model with \nperception, planning, and action is suggested by the research. \nTo address optimization problems, such as the Unmanned \nAerial Vehicle (UAV) navigation problem, Deep Q-network \n(DQN), a reinforcement learning-based framework that \nDeepmind launched in 2015, incorporates both deep learning \nand Q-learning. To overcome problems with current \nprocedures, the research proposes a power plant inspection \nsystem incorporating UAV autonomous navigation and DQN \nreinforcement learning. These training processes set reward \nfunctions with reference to states and consider both internal and \nexternal effect factors, which distinguishes them from other \nreinforcement learning training techniques now in use. The key \ncomponents of the reinforcement learning segment of the \ntechnique, for instance, introduce states such as the simulation \nof a wind field, the battery charge level of an unmanned aerial \nvehicle, the height the UAV reached, etc. The trained model \nmakes it more likely that the inspection strategy will be applied \nin practice by enabling the UAV to move around on its own in \ndifficult environments. The average score of the model \nconverges to 9,000. The trained model allowed the UAV to make \nthe fewest number of rotations necessary to go to the target \npoint. \nI. INTRODUCTION \nThis study will show an innovative application for \nunmanned aerial vehicle (UAV) autonomous navigation \nemploying a reinforcement learning (RL) trained model in a \nsimulated power plant environment, with realistic element \nsettings such as battery charge level, wind field, etc. under the \nDeep-Q Network (DQN) framework. The trained model \nincreases the likelihood that the inspection approach will be \nused in practice by allowing the UAV to travel independently \nin challenging settings. \nPower plant inspection is an occupation with excellent \npotential for automation through artificial intelligence. The \nexorbitant expense of putting up cameras and the demand for \nongoing maintenance has led to an increase in the use of UAV \nfor self-inspection. [1] Additionally, it is perilous to have \ntechnicians in the area (circulating) because mishaps still \nhappen in manufacturing facilities, requiring specialized \ntraining to be swift and exact. Motion planning, object \nrecognition, and path planning are necessary for this \ntraditional UAV navigation. [2] Traditional navigation \ntechniques like artificial potential fields [3] and SAR system-\nbased UAV navigation [4] have challenges when it comes to \nsimulating the environment. These challenges can be solved \nby using RL algorithms. Additionally, traditional navigation \nmethods are less flexible and cost more to apply with sensors. \nReinforcement learning, a subfield of machine learning, \nhas garnered a lot of interest in both academia and business. \nThe reinforcement learning strategy, could also be less \ncomputationally demanding than the traditional path planning \nmethod. In a way, a pre-trained model with perception, \nplanning, and action will enable the UAV to quickly discover \nthe optimal route to the target.  \nNumerous studies on reinforcement learning algorithms \nhave been conducted in various disciplines, including UAV \napplications. [5] [6] Jonathan M. Aitken et al. [7] suggest a \nway of using autonomous robots to inspect water and sewer \npipe networks with localization and mapping for the \nreplacement of manual inspection. Xiaoping Jia et al. [8] \npropose an application approach of intelligent control \ntechnology in the inspection robot in a thermal power plant. \nHuy X. Pham et al. [9] conducted a simulation and \nimplementation of the UAV system using reinforcement \nlearning.  \nDeep Q-network [10], a reinforcement learning-based \nframework introduced by Deepmind in 2015, combines both \ndeep learning and Q-learning and is a potential approach to \ntackle optimization issues, such as the UAV navigation \nproblem. The model needs to employ a Q function, which \nassesses the predicted discounted benefit for choosing a \ncertain action at a given state, rather than a Q table in the \nproposed UAV performance scenario. \nThe research suggests a power plant inspection system \nwith UAV autonomous navigation using DQN reinforcement \nlearning to address the issues with existing practices. These \ntraining procedures differ from existing reinforcement \nlearning training methods in that they set reward functions \nwith reference to states and consider both internal and exterior \neffect elements. For instance, the essential parts of the \nreinforcement learning section of the approach introduce \nstates like the simulation of a wind field, the charge level of \nan unmanned aerial vehicle's battery, the height the UAV \nreached, etc. The trained model increases the likelihood that \nthe inspection approach will be used in practice by allowing \nthe UAV to travel independently in challenging settings. \nII. METHOD \nA. UAV Self-Inspection Model \nThe navigation of the UAV during the inspection is the \nprimary subject of this paper. The power plant, which contains \nbuildings and wires, is where the inspection is conducted. \n[Figure 1] The model is used to depict the power plant as well \nas the internal and exterior environmental conditions, \nincluding the amount of wind and charge level, among others. \nIn order to update statuses and rewards, the UAV itself and \nthe environment it is in both contribute to perception. [Figure \n2] \nFigure 1.   UAV navigation in power plant environment with \nreinforcement learning model \nFigure 2.   Perception model of the UAV \nB. Key Elements of Deep-Q Network Reinforcement \nLearning \nMarkov decision processes may be specifically described \nas reinforcement learning (MDPs). They have four \ncomponents: a policy, a reward signal, an environment, and \na utility function. This makes it a strong contender for \nhandling complicated problems and capturing real-world \ncircumstances. This element will focus on the utility function \nwhich is the Q function detail.  \nHowever, traditional reinforcement learning calls for the \nagents to produce prior knowledge for the new state and \nadopt the proper representations of the environment based on \nhigh-dimensional input. Its usefulness is limited to the low-\ndimensional space where the characteristics may be used to \ntheir full potential. Deep reinforcement learning has \ndeveloped into a high performing tool for solving \ncomplicated issues, and DQN, which incorporates deep \nneural networks into reinforcement learning, was created to \nfill in these gaps. [10] Moreover, DQN is a model-free, value-\nbased, off-policy method whenever using the established \nstrategies and updating the Q value. State space and action \nspaces are limited sizes to decrease computational power. Q \n(s, a) will be represented by a function, not a table. Next is \nthe pseudocode for the DQN method that demonstrates \ndetails. [Table 1] \nTABLE I.  DEEP Q-LEARNING NETWORK  \nAlgorithm 1: Deep Q-learning WITH EXPERIENCE REPLAY \nInitialize replay memory ğ·  to capacity ğ‘  \nInitialize action-value function ğ‘„  with random weights ğœƒ  \nInitialize target action-value function â„š with weights ğœƒÌ‡ = ğœƒ \nfor episode 1, M do initialize sequence ğ‘ 1 = {ğ‘¥1} and preprocessed \nsequence  ğœ‘1 = ğœ‘(ğ‘ 1) \n for t = 1. T do \n \nBoolean step | if with probability ğœ€ select a random \naction ğ‘ğ‘¡ \n \n \nOtherwise, select \n ğ‘ğ‘¡= ğ‘ğ‘Ÿğ‘” ğ‘šğ‘ğ‘¥ğ‘ğ‘„(ğœ‘(ğ‘ ğ‘¡),ğ‘; ğœƒ) \n \n \nExecute ğ‘ğ‘¡ in emulator and observe reward ğ‘Ÿğ‘¡ and \nimage ğ‘¥ğ‘¡+1 \n \nLet  ğ‘ ğ‘¡+1 = ğ‘ ğ‘¡ , ğ‘ğ‘¡ ,ğ‘¥ğ‘¡+1 and preprocess ğœ‘ğ‘¡+1 = ğœ‘(ğ‘ ğ‘¡+1) \n \nStore experience ğœ‘ğ‘¡ , ğ‘ğ‘¡ , ğ‘Ÿğ‘¡ , ğœ‘ğ‘¡+1 in D \n \nSample \nrandom \nminibatch \nof \nexperiences \nğœ‘ğ‘— , ğ‘ğ‘— , ğ‘Ÿğ‘— ,ğœ‘ğ‘—+1 from D \n \nBoolean Step | if episode terminates at step ğ‘—+ 1, ğ‘¦ğ‘—=\nğ‘Ÿğ‘— \n \n \nOtherwise, perform calculation ğ‘¦ğ‘—=\nğ‘Ÿğ‘—+ ğ›¾ğ‘šğ‘ğ‘¥ğ‘ ,â„š(ğœ‘ğ‘—+1, ğ‘â€²; ğœƒÌ‡) \n \nPerform a gradient descent step on (ğ‘¦ğ‘—= ğ‘„(ğœ‘ğ‘— ,ğ‘ğ‘—; ğœƒ))2 \nwith respect to weights ğœƒ  \n \nReset â„š= ğ‘„  \n End for t loop  \nEnd for episode loop \n \nThe UAV agent is trained using the curriculum learning \napproach in the three-dimensional space of x100 y100 z22, \nand the gradient training of the agent is done using preset \ncourses of varying degrees of difficulty so that the agent can \ngain decision-making expertise more quickly. It is required \n \nto randomly select behaviors to assess the environment since \nearly training lacks decision-making experience. The random \ntesting time in this study is set to 4000, and agent actions are \nchosen using the greedy approach. The period's greedy \nprobability gradually drops from 1 to 0.01. 4000 episodes \nlater the likelihood of being greedy is still 0.01.  \nC. Environment \nThe environment is created using Matplotlib and Axes 3D \nlibrary functions in Python. This major body construction \nconsists of three steps that produce impediments to path \nplanning. There are three levels of considerations in play in \nthis section. The first is the size of the planning space; it is \nnot feasible to design a highly decentralized space due to both \npractical and computational reasons. A cubic area is defined \nas having a length of 100km, a height of 100km, and a width \nof 22km to condense the modeling space. The second step is \nto establish a wind field with a wind speed that is initially set \nat 30 km/h. This modeling can be further complicated instead \nof setting a fixed number, specifying a function that causes \nthe wind speed to change over time and increase with height. \nThe last step in defining the construction pieces is to specify \ntheir render color, size, and center coordinates. [Figure 3] \nFigure 3.   Simulation space (unit: km) \n \nD. Reward \nThe drone's total reward may be computed by adding the \nclimb reward and target reward. However, some additional \nreward and punishments are provided at various termination \nstages, such as terminating at the goal point with a collision \nor having the battery run out without a collision, etc. These \ntermination stages are considered with each respective \nreward modification. Here, avoiding collisions is of utmost \nimportance to reach the targeted spot. As a result, the total \nreward will be raised or lowered to a clear stage depending \non the reward adjustment for achieving the goal location and \na collision incidence. However, as none of these events will \nhave a significant impact on the ultimate termination stage, \nsituations like going over the maximum number of steps or \nrunning out of battery are given a tiny reward modification. \n[Table 2] \nTABLE II.  REWARD CALCULATION ALGORITHM \n \nAlgorithm 2: Reward calculation in different end stages \nInitialize clime reward ğ‘Ÿğ‘ğ‘™ğ‘–ğ‘šğ‘= ğ‘¤ğ‘Ã— (|ğ‘ ğ‘’ğ‘™ğ‘“. ğ‘§âˆ’ğ‘ ğ‘’ğ‘™ğ‘“. ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡[2]|) \nInitialize target reward ğ‘Ÿğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ ğ‘¤ğ‘–ğ‘¡â„ ğ‘Ÿğ‘’ğ‘ ğ‘ğ‘’ğ‘ğ‘¡ ğ‘¡ğ‘œ ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ \nTotal reward ğ‘Ÿğ‘¡ğ‘œğ‘¡ğ‘ğ‘™= ğ‘Ÿğ‘ğ‘™ğ‘–ğ‘šğ‘ + ğ‘Ÿğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ \nBoolean step | if the UAV did not get to the target NOR crash NOR \nexceed the maximum steps it can take in the space \n \nCalculate total reward \n \n \nif there is a crash and collision, return ğ‘Ÿğ‘¡ğ‘œğ‘¡ğ‘ğ‘™âˆ’500 \n \n \nif the UAV gets to the target position, return ğ‘Ÿğ‘¡ğ‘œğ‘¡ğ‘ğ‘™+\n500 \n \n \nif the UAV exceeds the maximum steps it can take \nin space, return ğ‘Ÿğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ âˆ’ 30 \n \n \nIf the battery run out, return ğ‘Ÿğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ âˆ’ 30 \n \nE. States and Actions \nThe state is defined by initializing the various state \nparameters of the drone and updating with calculation of its \nmotion change. States play a significant role in determining \nthe reward. One of the reward conditions, for instance, must \nspecify that if the UAV reaches the intended place. The \ncoordinates position and target coordinates must be recorded \nin the states and the UAV coordinate position must be updated \nconcurrently with the motion.  \nThe procedure updates the state by calculating the motion \nchange from the action value. The change value might be \ncomputed by using the update algorithm's functions. Another \npenalty may be applied if the UAV does not always move, and \nthe change values are equal to zero. Other states, such as the \nUAV's energy consumption, the distance to obstacles, the \nsteps that the UAV has made, etc., may be updated using the \namended UAV coordinates. \nThe indicators for fundamental energy consumption, \ncurrent energy use, and spent energy are all included in the \nstate. For calculating rewards, other states like range detected, \nsurrounding barriers, crash probability, distance steps taken, \ngoal distance, etc. are also collected. \nPrior to utilizing actions to update the states, action \nfunctions ğ‘ğ‘¡ are probabilistically chosen at random ğœ€ using \nthe greedy strategy. After training, that greedy approach drops \nfrom 1 and converges to 0.01. This is done to address the issue \nthat the model cannot choose the best course of action when \nthe overall scores have very significant negative values. \nIII. RESULTS \nAfter 40,000 episodes of training on top of a pretrained \nmodel, the sum of episodes trained are 173,399 episodes. The \nmodel reaches a steady average score around 9,000 proved by \nthe average score plot in the last 5000 episodes of training. \n[Figure 3] The training ends at 40000 since the average score \nhas been declining for the previous two thousand training \nsessions rather than increasing. [Figure 4] \nWith the aid of a trained model and a randomly generated \nenvironment, the UAV successfully took a direct path to the \ntarget location with the fewest number of turns feasible. \nFigure 4.   Average score plot for the last 5000 episodes \nAfter 360 episodes of training, the graph's score \nconsiderably increased and converged. The average score plot \ndisplays a smooth curve with an upward trend near 10,000. \n[Figure 5] \nFigure 5.   Average score plot for the first 360 episodes \nA comparison was made using the pre-trained model and \nthe model that is been trained after 173,399 episodes. [Figure \n6] and [Figure 7] As can be seen in the graph, the pre-trained \nmodel still causes the UAV to crash into the building and fly \nthrough to the desired location, but the trained model performs \nsignificantly better with time.  \nFigure 6.   A path navigated using the pre-trained model in simulated \nspace \n \nFigure 7.   A path navigated using the converged model in simulated space \nIV. DISCUSSION \nThe discussion will address the reasons behind why, \noccasionally, the score plummets to a severely negative \nnumber at the closing of the training sessions. [Figure 8] The \naverage score alone cannot serve as an accurate indicator of \nachievement. It is merely a metric for determining whether to \ncontinue training the model. The risk of a building crash \ncannot be tolerated by UAV navigation since it would result \nin a significant financial loss.  \nFigure 8.   Score graph for the last 5000 episodes \nThe fact that the target generation positions might \noccasionally be quite near to the building and the drone has no \nchoice but to crash into the building to reach the target place \nis one possible source of mistakes. However, both will lead to \nsevere punishments and rewarding outcomes, which is how \nthings balance out. Increasing the penalties for crashing and \nmaking it the major goal of navigation, with reaching the \ntarget point as the second primary goal, is one possible \nsolution to address this cause of inaccuracy.  \nThe strong wind from below may trigger another source \nof inaccuracy by pushing the UAV into the building. However, \nbecause training takes a very long time, turning off the wings \nfield and then waiting to see how the score will be during the \ntraining period is not the best strategy to adopt. After the \nmodel has been created for at least 1,000 episodes, it is \nfeasible to test this strategy by disabling the wind field in real-\ntime flight navigation instances. \nV. CONCLUSION \nThe strategy described in the study uses the DQN \nreinforcement learning method to teach a UAV to learn to \nnavigate to the intended position randomly generated in a \nmodel of a power plant that includes a wind field and \nconsiders energy usage. The simulation of the outcome \ndemonstrates that this method can offer a template for a UAV \nto navigate to the intended place while maintaining \nawareness of itself and its surroundings. The training score \nplot which converges at 10,000 provides evidence about the \nexperiment's methodology. Additionally, this article makes \nsome suggestions for prospective upgrades to real-world \ndeployment. For instance, provide a feasible verification \nmechanism to explain for outlying scores or develop a wind \nfield function to match the external fluid effect seen in the \nactual world to prevent mistakes. \nREFERENCES \n[1] \nYu, C.; Qu, B.; Zhu, Y.; Ji, Y.; Zhao, H.; Xing, Z. Design of the \nTransmission Line Inspection System Based on UAV. In Proceedings \n[2] \nH. Huang, Y. Yang, H. Wang, Z. Ding, H. Sari and F. Adachi, \"Deep \nReinforcement Learning for UAV Navigation Through Massive \nMIMO Technique,\" in IEEE Transactions on Vehicular Technology, \nvol. 69, no. 1, pp. 1117-1121, Jan. 2020, doi: \n10.1109/TVT.2019.2952549. \n[3] \nI. K. Nikolos, K. P. Valavanis, N. C. Tsourveloudis and A. N. \nKostaras, \"Evolutionary algorithm based offline/online path planner \nfor UAV navigation,\" in IEEE Transactions on Systems, Man, and \nCybernetics, Part B (Cybernetics), vol. 33, no. 6, pp. 898-912, Dec. \n2003, doi: 10.1109/TSMCB.2002.804370. \n[4] \nD. Nitti, F. Bovenga, M. Chiaradia, M. Greco, and G. Pinelli, \nâ€œFeasibility of Using Synthetic Aperture Radar to Aid  UAV \nNavigation,â€ Sensors, vol. 15, no. 8, pp. 18334â€“18359, Jul. 2015, doi: \n10.3390/s150818334. \n[5] \nX. Han, J. Wang, J. Xue, and Q. Zhang, \"Intelligent Decision-Making \nfor 3-Dimensional Dynamic Obstacle Avoidance of UAV Based on \nDeep Reinforcement Learning,\" 2019 11th International Conference \non Wireless Communications and Signal Processing (WCSP), 2019, \npp. 1-6, doi: 10.1109/WCSP.2019.8928110. \n[6] \nH.M.La,R.S.Lim,W.Sheng,andJ.Chen,â€œCooperativeflocking and \nlearning in multi-robot systems for predator avoidance,â€ in Cyber \nTechnology in Automation, Control and Intelligent Systems \n(CYBER), 2013 IEEE 3rd Annual International Conference on. IEEE, \n2013, pp. 337â€“342. \n[7] \nJ. M. Aitken et al., \"Simultaneous Localization and Mapping for \nInspection Robots in Water and Sewer Pipe Networks: A Review,\" in \nIEEE Access, vol. 9, pp. 140173-140198, 2021, doi: \n10.1109/ACCESS.2021.3115981. \n[8] \nXiaoping J., Wenjie Y., Haichao L., Shanqiu J. and Yude Zhang. \nâ€œApplication of Environment-perception Intelligent Control \nTechnology in the Inspection Robot of Coal Conveyance Corridor in \nThermal Power Plant,â€, in IOP Conf. Ser.: Earth Environ. Sci. 772 \n012056, 2021 \n[9] \nPham, H., La, H., Feil-Seifer, D., & Nguyen, L. â€œAutonomous UAV \nNavigation Using Reinforcement Learning,â€ in arxiv , Janurary 12, \n2018, doi:1801.05086 \n[10] Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control \nthrough deep reinforcement learning. Nature 518, 529â€“533 (2015). \ndoi: 10.1038/nature14236\n \n",
  "categories": [
    "cs.RO",
    "cs.LG"
  ],
  "published": "2023-03-16",
  "updated": "2023-03-16"
}