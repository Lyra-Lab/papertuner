{
  "id": "http://arxiv.org/abs/2303.09536v1",
  "title": "Deep Metric Learning for Unsupervised Remote Sensing Change Detection",
  "authors": [
    "Wele Gedara Chaminda Bandara",
    "Vishal M. Patel"
  ],
  "abstract": "Remote Sensing Change Detection (RS-CD) aims to detect relevant changes from\nMulti-Temporal Remote Sensing Images (MT-RSIs), which aids in various RS\napplications such as land cover, land use, human development analysis, and\ndisaster response. The performance of existing RS-CD methods is attributed to\ntraining on large annotated datasets. Furthermore, most of these models are\nless transferable in the sense that the trained model often performs very\npoorly when there is a domain gap between training and test datasets. This\npaper proposes an unsupervised CD method based on deep metric learning that can\ndeal with both of these issues. Given an MT-RSI, the proposed method generates\ncorresponding change probability map by iteratively optimizing an unsupervised\nCD loss without training it on a large dataset. Our unsupervised CD method\nconsists of two interconnected deep networks, namely Deep-Change Probability\nGenerator (D-CPG) and Deep-Feature Extractor (D-FE). The D-CPG is designed to\npredict change and no change probability maps for a given MT-RSI, while D-FE is\nused to extract deep features of MT-RSI that will be further used in the\nproposed unsupervised CD loss. We use transfer learning capability to\ninitialize the parameters of D-FE. We iteratively optimize the parameters of\nD-CPG and D-FE for a given MT-RSI by minimizing the proposed unsupervised\n``similarity-dissimilarity loss''. This loss is motivated by the principle of\nmetric learning where we simultaneously maximize the distance between change\npair-wise pixels while minimizing the distance between no-change pair-wise\npixels in bi-temporal image domain and their deep feature domain. The\nexperiments conducted on three CD datasets show that our unsupervised CD method\nachieves significant improvements over the state-of-the-art supervised and\nunsupervised CD methods. Code available at https://github.com/wgcban/Metric-CD",
  "text": "IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n1\nDeep Metric Learning for Unsupervised Remote\nSensing Change Detection\nWele Gedara Chaminda Bandara, Student Member, IEEE, and Vishal M. Patel, Senior Member, IEEE\nAbstract—Remote Sensing Change Detection (RS-CD) aims to\ndetect relevant changes from Multi-Temporal Remote Sensing\nImages (MT-RSIs), which aids in various RS applications such\nas land cover, land use, human development analysis, and\ndisaster response. The performance of existing RS-CD methods is\nattributed to training on large annotated datasets. Furthermore,\nmost of these models are less transferable in the sense that\nthe trained model often performs very poorly when there is\na domain gap between training and test datasets. This paper\nproposes an unsupervised CD method based on deep metric\nlearning that can deal with both of these issues. Given an\nMT-RSI, the proposed method generates corresponding change\nprobability map by iteratively optimizing an unsupervised CD\nloss without training it on a large dataset. Our unsupervised CD\nmethod consists of two interconnected deep networks, namely\nDeep-Change Probability Generator (D-CPG) and Deep-Feature\nExtractor (D-FE). The D-CPG is designed to predict change\nand no change probability maps for a given MT-RSI, while\nD-FE is used to extract deep features of MT-RSI that will be\nfurther used in the proposed unsupervised CD loss. We use\ntransfer learning capability to initialize the parameters of D-\nFE. We iteratively optimize the parameters of D-CPG and D-\nFE for a given MT-RSI by minimizing the proposed unsuper-\nvised “similarity-dissimilarity loss”. This loss is motivated by the\nprinciple of metric learning where we simultaneously maximize\nthe distance between change pair-wise pixels while minimizing\nthe distance between no-change pair-wise pixels in bi-temporal\nimage domain and their deep feature domain. The experiments\nconducted on three CD datasets show that our unsupervised CD\nmethod achieves significant improvements over the state-of-the-\nart supervised and unsupervised CD methods. Code available at\nhttps://github.com/wgcban/Metric-CD\nI. INTRODUCTION\nChange detection (CD) aims to detect relevant changes from\nmulti-temporal remote sensing images (MT-RSIs) captured\nover the same geographic area at distinct times. In recent years,\nCD has gained great attention in the research community due\nto its numerous applications in urban planning, environmental\nmonitoring, disaster assessment, agriculture, forestry, etc. [10],\n[47]. Most of these practical applications typically need to\nquickly obtain the relevant change information from large\nMT-RSIs. In the context of CD, the irrelevant changes such\nas vegetation color variations, building shadows, atmospheric\nvariations, lighting changes, and cloud covering limit the\naccuracy of change maps.\nMost CD techniques proposed in the literature are based\non supervised training where a model (i.e., classical or deep\nlearning based model) is trained with a large number of\nMT-RSIs and corresponding change labels (see Fig. 1-(a)).\nThe authors are with the Department of Electrical and Computer En-\ngineering, Johns Hopkins University, Baltimore, MD, 21218 USA e-mail:\n{wbandar1, vpatel36}@jhu.edu.\nFig. 1: How our deep metric learning-based CD method\ndiffers from the state-of-the-art CD methods. (a) Supervised\nCD: In this case, MT-RSIs and the corresponding change\nmasks are available for training the CD model [11], [32],\n[53]. (b) Self-supervised CD: In this case, change masks are\nnot available during training, but a large unlabeled MT-RSI\ndataset is utilized to synthesize changes [9], [39]. (c) Unsu-\npervised CD: These methods start with a sub-optimal pre-\ntrained Feature Extractor (FE) that is trained for some other\ntask to obtain deep features, and then perform Change Vector\nAnalysis (CVA) followed by thresholding to obtain change\nmaps [21], [41]. (d) Deep Metric Learning for Unsupervised\nCD (ours): We iteratively optimize the parameters of deep\nchange probability generator (D-CPG) to obtain change and\nno-change probability maps for a given MT-RSI according to\nthe proposed unsupervised “similarity-dissimilarity” loss for\nCD. The “similarity-dissimilarity” loss is motivated by the\npopular principle of metric learning where we maximize the\ndistance between change pair-wise pixels and minimize the\ndistance between no-change pairwise pixels in bi-temporal\nimage domain and deep feature domain.\nHowever, in MT problems, it is expensive and often impossible\nto obtain a large number of annotated training samples for\nmodeling change and no change classes [2], [4], [11], [32],\n[53]. This limited access to labeled training data has motivated\nthe researchers to explore self-supervised and unsupervised\nCD methods.\nThe recently proposed self-supervised methods are often\nbased on either self-supervised pre-training [2], [9] or con-\ntrastive learning [39] (see Fig. 1-(b)). In self-supervised pre-\ntraining a model is trained by utilizing the change labels\nextracted directly from the MT-RSIs themselves. In other\nwords, instead of training a model for the CD task, self-\nsupervised pre-training optimizes a model for a pretext task\narXiv:2303.09536v1  [cs.CV]  16 Mar 2023\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n2\nin which the labels are extracted directly from the MT-RSIs\nby alternating relative positions of patches. By doing that,\nthe network is pre-trained to recognize the changes in MT-\nRSIs with no annotated change masks. In contrastive self-\nsupervised CD, a pseudo-Siamese network is trained to regress\nthe output between its two branches, which is pre-trained\nin a contrastive way on a large unlabeled MT-RSI dataset.\nHowever, both of these self-supervised CD methods require\na large unlabeled MT-RSI dataset for optimizing the model\nparameters. In addition, the performance of the trained model\nheavily depends on how realistic the changes they make while\ntraining the model.\nIn contrast to self-supervised CD methods, unsupervised\nCD methods [21], [41] do not often depend on a large MT-\nRSI dataset for training. They extract feature representations\nof a given MT-RSI from a Feature Extractor (FE) which is\ninitialized from a network pre-trained for some other task\n(like classification, segmentation, etc.) via transfer learning\n(see Fig. 1-(c)). Next, Change Vector Analysis (CVA) [42]\nor Deep Feature Difference (D-FD) [55] is performed on the\nextracted deep features to identify the change pixels. However,\nthe existing deep unsupervised CD methods are sensitive to\npseudo unwanted changes, because the feature extractor is not\ntrained to produce deep features which are invariant to color\nand seasonal transformations that usually appear in MT-RSIs.\nIn order to address the aforementioned issues of state-of-\nthe-art (SOTA) CD methods, we propose a novel unsupervised\ndeep metric learning-based strategy for CD (see Fig. 1 - (d)).\nOur CD method consists of two networks, namely Deep-\nChange Probability Generator (D-CPG) and Deep-Feature\nExtractor (D-FE). The parameters of D-CPG are randomly\ninitialized while the parameters of D-FE are initialized via\ntransfer learning. The weights of both D-CPG and D-FE\nare iteratively optimized for a given MT-RSI according to\nthe proposed similarity-dissimilarity loss which is motivated\nby the principal of metric learning where we maximize the\nsimilarity between no-change pair-wise pixels while mini-\nmizing the similarity between change pair-wise pixels in bi-\ntemporal image domain and their respective deep feature\ndomain as shown in Fig. 2. Furthermore, to force the deep\nfeatures of the given MT-RSI from the D-FE to be invariant to\nlocal colorimetric variations, we also define a self-supervised\nauxiliary task in which we force the feature representations\nof a given MT-RSI to be consistent under different random\ncolor transformations through the auxiliary context consistency\nloss. In our experiments, we show the significance of the\nproposed CD method by comparing it with SOTA supervised,\nself-supervised, and unsupervised CD methods.\nII. RELATED WORKS\na) Classical CD methods.: Although there are many\nclassical methods available for CD from RSIs, most of them\nare based on the Difference Image (DI). A DI can be obtained\nby image differentiation [49], image ratio [48], Principal\nComponent Analysis (PCA) [25], or Change Vector Analysis\n(CVA) [27]. Once the DI is calculated, the binary change map\nis obtained by classifying the DI into two classes, namely\nFig. 2: We start from random weights for D-CPG θ0\nD-CPG,\nand iteratively update the weights in order to minimize the\nproposed LCD. At the t-th iteration, weights θt\nD-CPG are used\nto generate the change probability maps Pt\nc = fθD-CPG(Id). Pt\nc\nis used to calculate LCD. The gradient of the loss LCD with\nrespect to the weights θ is then computed and used to update\nthe parameters.\nchange class and no change class. This can be achieved\nby thresholding the DI with an appropriate threshold [49],\nusing a clustering algorithm (i.e., K-Means [8], [18], met-\nric learning [18], etc.), or a supervised classification [52].\nHowever, the main drawback of these classical approaches is\nthat the resulting change map contains many pseudo-random\nchanges due to isolated radiometric changes, vegetation color\nvariations, and color changes due to seasonal effects [56].\nAlthough supervised classification is the best way to avoid\nsuch random changes, it requires expertly annotated pixel-\nlevel labels, limiting the applicability of many conventional\napproaches.\nb) Deep Learning-based CD methods.: Recently, a va-\nriety of methods based on Convolutional Neural Networks\n(ConvNets) have also been proposed for CD from MT-RSIs\ndue to their excellent ability to learn low-level and high-level\nfeatures automatically during training [3], [22], [44]. Among\nthese, most of them are fully supervised methods. They often\nrequire more training samples and more training time to learn\nthe deep features of RSIs [15]. Early Fusion (EF) [53], and\nSiamese networks [11] are the widely adopted architectures\nfor supervised CD. However, collecting a large number of\ntraining samples that require learning deep features is difficult\nto achieve in many CD applications, especially with high-\nprecision and pixel-level annotations [22]. In recent years, the\nlimited access to labeled change information has increased\nthe researcher’s attention to self-supervised and unsupervised\nCD. Self-supervised pre-training (SSP-CD) [26], Generative\nAdversarial Networks [39] (GAN-CD), and contrastive loss-\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n3\nbased pseudo-Siamese networks (CPS-CD) [9] are the widely\nused self-supervised CD methods. Even though self-supervised\nlearning encourages the network to learn meaningful features,\nit requires a large unlabeled dataset that may not be available\nfor some CD applications. Most unsupervised CD methods\nare based on transfer learning [36], where they extract deep\nfeatures of MT-RSIs by initializing network weights that are\npre-trained for some other task (i.e., segmentation or classi-\nfication) on real or RS images. Examples of unsupervised-\nCD include Deep - Feature Difference (FeatureDiff) [41] and\nDeep - Change Vector Analysis (D-CVA) [21]. Once the\ndeep features are obtained, FeatureDiff and D-CVA obtain the\nchange map by computing the feature difference or performing\nCVA followed by thresholding, respectively.\nIII. PROPOSED METHOD\nA. Problem formulation\nConsider two co-registered multi-spectral remote sensing\nimages I1 and I2 with b bands, captured over the same\ngeographical area at distinct time instances t1 and t2, respec-\ntively. Usually I1 and I2 are referred to as the pre-change\nimage and post-change image, respectively and {I1, I2} is\nreferred to as the multi-temporal image. Our objective is to\ndetect the changes from I1 and I2 in an unsupervised way;\ni.e., we classify the set of all pixels (Ω) into change (Ωc)\nand no-change (Ωnc) pixels. If we compute the dissimilarity\nbetween each pixel of I1 and I2 individually, we often end up\nobtaining either not-meaningful changes from a context-based\nperspective or changes which are minute to be considered as a\nchange of interest because of inherent radiometric dissimilar-\nities between pre-change and post-change images. Such small\nisolated radiometric changes are therefore not considered as a\nchange.\nB. Overview of the proposed solution\nThe proposed unsupervised framework addresses the afore-\nmentioned issues of change detection. As depicted in Fig.\n3, we first perform polynomial color correction on multi-\ntemporal image {I1, I2} to obtain pre-processed multi-\ntemporal image {eI1,eI2}. Next, we compute the difference\nimage Id and process it through a Deep - Change Probability\nGenerator (D-CPG) to generate the change probability map\nPc. The weights of D-CPG are optimized by minimizing\nthe proposed similarity-dissimilarity loss in an unsupervised\nmanner. However, minimizing the similarity-dissimilarity loss\nin image-domain results in detecting unwanted changes due\nto local radiometric shifts of pre-change and post-change\nimages. To make our change detection framework robust to\nsuch isolated radiometric changes, we also utilize similarity-\ndissimilarity loss on multi-level feature representations of\n{eI1,eI2}, obtained from a Deep Feature Extractor (D-FE) that\nis pre-trained for some other task. In order to constrain features\nfrom D-FE to be robust to isolated color shifts, we apply data\naugmentations on {eI1,eI2} and constrain their deep-features to\nbe consistent with deep-features of {eI1,eI2} via the context-\nconsistency loss. We iteratively minimize the overall loss\nfunction LCD for unsupervised CD to generate optimal change\nprobability map P∗\nc for a given multi-temporal image {I1, I2}\nas shown in fig. 2.\nC. Proposed unsupervised CD framework\na) Image preprocessing and colorimetric mapping.: We\nadopt Polynomial Color Correction (PCC) [20] to reduce col-\norimetric changes due to different sensor properties, seasonal\nvariations, and lighting conditions. It has been experimen-\ntally shown that PCC can significantly reduce colorimetric\nerrors [1], [17]. Using PCC, we map the pre-change image\ncolor space to the post-change image color space using a\npolynomial mapping function M, usually found by Least\nSquares Regression (LSR) [17]. To do that, we first take Nc\ncolor samples (pixels) from each pre-change and post-change\nimage by downsampling the original images by a factor of\nβ. We denote the downsampled versions of pre-change and\npost-change images as i1 and i2, respectively. The reason\nfor applying LSR on downsampled image pixels is to more\naccurately bring out the color properties of the images than\nthe actual changes present in multi-temporal image. Also, this\nwill significantly reduce the pre-processing time especially\nwhen the satellite images have a very high spatial resolution.\nConsequently, we compute the polynomial mapping matrix M\nthat maps values of ρ(i1) to the colors of the post-change\nimage i2, where ρ(·) is a polynomial kernel function that\nmaps the spectral vectors of the satellite image to a higher\nd-dimensional space. See the supplementary document for an\nassessment of the different kernel functions. The mapping\nmatrix M is found by least squares regression and has a closed\nform solution of the form:\n  \\mathbf  {M} = (\\rho ( \\mathbf {i}_1)^T\\rho (\\mathbf {i}_1))^{-1} \\rho (\\mathbf {i}_1)^T \\mathbf {i}_2. \n(1)\nOnce M is computed, we obtain our final color transformed\nversion of pre-change and post-change images as follows:\n  \\ w idetil d e { \\ma t hbf {I}}_1 = \\mathbf {M} \\rho (\\mathbf {I}_1)\\text { , and } \\widetilde {\\mathbf {I}}_2 = \\mathbf {I}_2. \n(2)\nb) Difference Image Id.: We compute the dissimilarity\nmeasure Id between pre-change (eI1) and post-change (eI2)\nimages by utilizing the Mahalanobis distance [12] as follows:\n  \\\nmathbf  {I}\n_d \n=  \n\\l\neft  \\| \n\\w\nide\nt\nild e  {\\\nm\nathbf {I}}_1 - \\widetilde {\\mathbf {I}}_2\\right \\|_{\\mathbf {M}} = \\sqrt {\\left (\\widetilde {\\mathbf {I}}_1-\\widetilde {\\mathbf {I}}_2\\right )^T \\mathbf {S}^{-1} \\left (\\widetilde {\\mathbf {I}}_2-\\widetilde {\\mathbf {I}}_2\\right )}, \n(3)\nwhere S is the covariance matrix of size b × b, and T denotes\nthe matrix transpose. We use the Mahalanobis distance since\nit computes the dissimilarity measure based on the distribution\npattern of data points through the covariance matrix S, unlike\nL1 or L2 distances. The conventional CD methods use the\ndifference image Id and threshold it with an appropriate\ngeneric rule to obtain the binary change map [38]. However,\nthe latter case is sensitive to noise and local illumination\nvariations, and does not consider local consistency properties\nand object-based perspective of the change mask.\nc) Deep-Change Probability Generator (D-CPG).: To\nenforce the local consistency property of the change mask,\nwe go one step beyond the conventional CD methods where\nwe take the difference image Id as input and process it\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n4\nDeep - Change Probability Generator\nMulti-level Deep Feature Extractor\nPolynomial\nColor\nCorrection\nDissimilarity\nMeasure\nRandom Data Augmentations\nRandom intensity, hue, saturation\njittering, and random noise \nMulti-level Deep Features\nUnsupervised\nLoss  for CD\nConv.\nConv.\nConv.\nConv.\nConv.\nConv.\nConv.\nConv.\nPre-trained VGG16\n+\n+\nPredicted Change\nProbability Map\nConv.\nRes. Block (1)\nRes. Block (2)\nRes. Block (.)\nConv.\nRes. Block (N)\nSoftmax\ninput MT-RSI\n0\n1\nScale\nScale\n-th iteration of optimization\nMinimizing \n\niteratively\n0\n1\nFig. 3: The proposed unsupervised deep metric learning framework for CD in multi-temporal remote sensing images.\nthrough a Deep - Change Probability Generator (D-CPG)\nnetwork. The D-CPG estimates the probability of change for\na given pixel location (i, j) −Pc(i, j) by considering a local\nneighborhood N(i, j) around (i, j) in the difference image Id.\nMathematically, we can defined this process as:\n  \\ mathbf {P}_c = f_{\\theta _{\\text {D-CPG}}} (\\mathbf {I}_d), \n(4)\nwhere fθD-CPG(·) is the parametric representation of D-CPG.\nThe size of the local-neighborhood N(i, j) is usually defined\nby the size of the receptive field [29] of D-CPG, and can\nbe appropriately controlled by changing the network depth.\nWe conduct experiments with different ConvNet architectures\nfor our D-CPG like hourglass network [33], U-Net [40] with\nskip connections, and Residual Networks (ResNet) [19] with\ndifferent depth (i.e., 4, 8, 16, 32, and 64 residual blocks).\nThe parameters of the D-CPG are initialized randomly and\noptimized over each iteration for a given MT-RSI during\ntesting (see Fig. 2) by minimizing the proposed unsupervised\nloss LCD function which will be introduced in the following\nsection.\nD. Proposed unsupervised loss function for CD\nThe proposed unsupervised loss function for CD LCD con-\nsists of three terms, namely image-domain and feature-domain\nsimilarity-dissimilarity loss (Limg and Lfeat) and context con-\nsistency loss (Lctx) as detailed below.\na) Image-domain loss for CD: Limg.: We propose an\nunsupervised loss function for CD, motivated by the most\npopular principle in metric learning: minimize the distance\nbetween similar datasets and maximize the distance between\ndissimilar datasets [37], [51], [54]. We adopt this notion to\nbuild our unsupervised loss for CD where we try to maximize\nthe distance between change pairwise pixels while minimizing\nthe distance between no-change pairwise pixels. We weight\neI1 and eI2 according to the change probability Pc and no\nchange probability Pnc (= 1 −Pc) to identify change and\nno change pixels, and then use the Mahalanobis distance to\ncalculate the change loss Limg-c(eI1,eI2, Pc) and no-change loss\nLimg-nc(eI1,eI2, Pnc) as follows:\n  \\begin {aligned} \\label {image_loss} \\mathcal {L}_{\\text {img}} &= \\alpha _{\\text {img}} \\mathcal {L}_{\\text {img-c}}(\\widetilde {\\mathbf {I}}_1, \\widetilde {\\mathbf {I}}_2, \\mathbf {P}_{\\text {c}}) + \\mathcal {L}_{\\text {img-nc}}(\\widetilde {\\mathbf {I}}_1, \\widetilde {\\mathbf {I}}_2, \\mathbf {P}_{\\text {nc}})\\\\ &= - \\alpha _{\\text {img}} \\left \\| \\mathbf {P}_{\\text {c}} (\\widetilde {\\mathbf {I}}_1 - \\widetilde {\\mathbf {I}}_2)\\right \\|_{\\mathbf {M}} + \\left \\| \\mathbf {P}_{\\text {nc}} (\\widetilde {\\mathbf {I}}_1 - \\widetilde {\\mathbf {I}}_2)\\right \\|_{\\mathbf {M}}, \\end {aligned} g\n(5)\nwhere we refer Limg to as image-domain change-nochange loss\nfor CD, and αimg is the regularization parameter for balancing\nthe trade-off between image-domain change and no-change\nloss terms. By default we set αimg to 1.0. When Limg is\nminimized, the no change loss Limg-nc will lead to minimize\nthe distance between no-change pairwise pixels, and change\nloss Limg-c will lead to maximize the distance between change\npairwise pixels. Since Limg is differentiable, it back-propagates\nthrough the D-CPG and adjusts its parameters to generate\nbetter change map until it converges.\nb) Feature-domain loss for CD: Lfeat.:\nTo impose\ncontext-based perspective of change maps in optimization,\nwe adopt transfer learning [36] capability for CD. Transfer\nlearning enables us to extract deep-features from a multi-layer\ndeep feature extractor (D-FE) that is pre-trained for some other\ntask. Such deep feature representations can capture spatial\ncontext of an image effectively. Therefore, to improve context-\nbased perspective of predicted change map, we extend our\nchange-nochange loss function from image-domain to feature-\ndomain by utilizing the multi-layer feature representations of\nmulti-temporal image {I1, I2} as follows:\n  \\la b\ne\nl\n {e\nq\n:f\ne at_loss} \\\nsc ri\npt size \\b e gin {aligne\nd}  \\\nma thcal {L\n}\n_\n{\n\\\ntex\nt\n {f\neat}\n} &= \\sum _{\nl = 1}\n^{\nL} \n\\ l\neft ( \\alpha \n_ { \\t\nex\nt {\nf\n}\n}^{l} \\mathcal {L}_{\\text {feat-c}}(\\mathbf {F}_1^{l}, \\mathbf {F}_2^{l}, d^l(\\mathbf {P}_{\\text {c}})) + \\mathcal {L}_{\\text {feat-nc}}(\\mathbf {F}_1^{l}, \\mathbf {F}_2^{l}, d^l(\\mathbf {P}_{\\text {nc}})) \\right ) \\\\ &= \\sum _{l=1}^{L} \\left ( -\\alpha _{\\text {feat}}^{l} \\left \\| d^l(\\mathbf {P}_{\\text {c}}) (\\mathbf {F}_1^{l} - \\mathbf {F}_2^{l})\\right \\|_{\\mathbf {M}} + \\left \\| d^l(\\mathbf {P}_{\\text {nc}}) (\\mathbf {F}_1^{l} - \\mathbf {F}_2^{l})\\right \\|_{\\mathbf {M}} \\right ), \\end {aligned} \n(6)\nwhere l denotes the l-th layer of L-layered D-FE, αl\nfeat is the\nregularization parameter for the l-th layer that is set to 1.0,\nLfeat-c(·) is the feature-domain change loss, Lfeat-nc(·) is the\nfeature-domain no-change loss, dl(·) is the nearest-neighbor\ndown-sampling of {Pc, Pnc} to the spatial size of Fl\n1 or Fl\n2,\nrespectively.\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n5\nc) Context consistency loss for CD: Lctx.: Although deep\nfeatures can provide contextual information for an image to\na great extent, they are still sensitive to different illumina-\ntion conditions, different image sensors, and noise. To make\nour estimated change map Pc to be further robust to such\nunwanted changes, we apply random augmentations on the\nmulti-temporal image {I1, I2} at each iteration of testing,\nprocess them though the deep feature extractor, and then\nconstrain these features to be consistent with their original\nfeature representations. In particular, we apply random color\naugmentations such as random brightness, contrast, saturation\nand hue jittering, as well as random noise augmentation [45].\nWe denote the augmented versions of pre-change and post-\nchange images at t-th iteration of optimization as [ˆI1]t and\n[ˆI2]t, and their respective features from the l-th layer of D-\nFE as [ˆFl\n1]t and [ˆFl\n2]t, respectively. We then impose context\nconsistency between the original feature representations and\nthe augmented feature representations by context consistency\nloss Lctx as follows:\n  \\m a\nt\nh\ncal\n {L}_{\n\\ t ext \n{ct\nx}}\n =\n \\sum\n _ {l=1\n}^{\nL} \n\\\nl\neft ( \\left \\| \\mathbf {F}_1^{l} - [\\hat {\\mathbf {F}}^l_{1}]_t \\right \\|_1 + \\left \\| \\mathbf {F}_2^{l} - [\\hat {\\mathbf {F}}^l_{2}]_t \\right \\|_1 \\right ), \\label {eq:ctx_loss} \n(7)\nwhere ∥·∥1 denotes the L1 norm.\nd) Sparsity penalty: Lsparse.: A trivial solution exists\nfor the objective (6) where, Pc = 1. To avoid such degenerate\nsolutions, we introduce a sparsity penalty Lsparse in the form\nof 1/sin(·) that discourages D-CPG predicting all-one (or all\nzero); specifically,\n  \\math c al \n{\nL }\n_{\ns\np\nars\ne\n}\n = \n\\sin \\l\ne\nf\nt \n( \\frac {\\pi }{hw} \\sum _{i=1}^{h} \\sum _{j=1}^{w} P_c(i,j)\\right )^{-1}, \n(8)\nwhere (h, w) is the (height, width) of the change probability\nmap Pc.\nThe overall unsupervised loss function for CD LCD that we\nuse to optimize D-CPG and D-FE is defined as follows:\n  \\ l abel {overall_loss} \\mathcal {L}_{\\text {CD}} = \\mathcal {L}_{\\text {img}} + \\mathcal {L}_{\\text {feat}} + \\mathcal {L}_{\\text {ctx}} + \\mathcal {L}_{sparse}. g\n(9)\nBy minimizing LCD using an optimizer such as Adam [23]\nover the parameters of D-CPG (θD-CPG) and D-FE (θD-FE), we\nobtain optimum change probability map P∗\nc for a given bi-\ntemporal image {I1, I2}, starting from the difference image\nId. Hence, the empirical information available to the change\ndetection process is the bi-temporal image {I1, I2} and pre-\ntrained weights of D-FE θ0\nDFE from transfer learning, and hence\nthe CD process is completely unsupervised. This approach is\nschematically depicted in Fig. 2 and Fig. 3.\nIV. EXPERIMENTAL SETUP\na) Datasets.: We evaluate the performance of our pro-\nposed CD framework on three widely used and publically\navailable CD datasets, namely Onera Satellite Change Detec-\ntion Dataset (abbreviated as OCSD) [7], SZTAKI AirChange\nBenchmark set (abbreviated as SZTAKI) [5], [6], and Quick-\nBird dataset [55]. The OSCD dataset consists of 24 pairs of\n13-band (b = 13) multispectral images taken from the Sentinel-\n2 satellites [16], captured all over the world (such as Brazil,\nthe US, Europe, the Middle East, and Asia) between 2015\nand 2018. Note that we present results on the testing set of\nthe OSCD dataset. In addition, the SZTAKI dataset contains\n13 aerial image pairs of size 952×640 and the corresponding\nbinary change masks. The bi-temporal images are taken with\na 23-years of time difference. We use all 13 image pairs of\nthe SZTAKI dataset for testing. Furthermore, the QuickBird\ndataset consists of a three-band bi-temporal image pair of\nsize 1154 × 740 which was captured in 2009 and 2014. In\nthe QuickBird dataset, seasonal changes are very prominent -\nmaking it difficult to filter out relevant changes from unwanted\nones compared to the other two datasets.\nb) Implementation details and reproducibility.: We im-\nplemented our unsupervised CD framework in Pytorch with\nan NVIDIA Quadro RTX 8000 GPU. As we discussed in\nSec. III, our CD framework does not involve a training phase,\nunlike supervised or self-supervised CD approaches. Instead,\nwe iteratively optimize the parameters of the network for\neach MT-RSI separately. After a fixed number of iterations\n(set to 80), we save the resulting change probability map\nfrom the network (note that we do not save the network\nparameters), and obtain a binary change map by thresholding it\nwith an appropriate probability (set to 0.5, but can manually\nvary for better performance). We initialize multi-level D-FE\nfrom VGG-16 [46] pre-trained weights on ImageNet [24].\nAdditional experiments on how CD performance varies with\ndifferent initialization methods can be found in the supple-\nmentary document. For all the experiments, we used Adam\noptimizer with a learning rate of 1e-5. Implementation code\nwill be made publicly available after the review process.\nc) Performance metrics and visualization of change\nmaps.: We adopt the overall accuracy (OA), user accuracy\n(UA) - often called as precision, recall rate (Recall), F-1\nmeasure (F1), and Area Under the Receiver Operating Charac-\nteristic (ROC) curve (AUC) to quantitatively compare the CD\nperformance of different methods. Among these, AUC is the\nbest way to summarize a model’s ability to distinguish between\nchange and no change regions. A higher value of AUC\nindicates a good CD model. For the qualitative comparisons,\nwe highlight each pixel in the predicted change map with\ndifferent colors to denote true positives (TP), true negatives\n( TN ), false positives (FP), and false negatives (FN). Thus,\na change map with more green and white pixels, and fewer\nred and blue pixels indicates a good CD performance.\nV. RESULTS AND DISCUSSION\na) Quantitative results.: Table I presents average quan-\ntitative results corresponding to different CD methods. From\nthese results, we can make the following key observations: 1.\nThe proposed CD method outperforms the existing unsuper-\nvised approaches (classical and deep) by a significant margin,\nespecially in OA, F1 and AUC metrics. 2. It even outperforms\nrecent deep supervised approaches, which are pre-trained in an\nunsupervised manner on a large RS (SeCo- (P) [32]) dataset\nand then fine-tuned on the OSCD training set in a supervised\nmanner. The latter observation proves the point we made in the\nintroduction, where the supervised CD approaches are weak\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n6\nTABLE I: The average quantitative CD results on the OSCD dataset [7], the SZTAKI AirChange dataset [5], [6], and the\nQuickBird dataset [55]. Higher values of OA, UA, Recall, F1, and AUC indicate good change detection performance. Color\nconvention: Best, 2-nd Best, 3-rd Best.\nMethod\nOSCD Dataset [7]\nSZTAKI AirChange Dataset [5], [6]\nQuickBird Dataset [55]\nOA\nUA\nRecall\nF1\nAUC\nOA\nUA\nRecall\nF1\nAUC\nOA\nUA\nRecall\nF1\nAUC\nUnsupervised CD methods:\nCVA [43]\n0.830\n0.164\n0.457\n0.192\n0.751\n0.768\n0.118\n0.545\n0.179\n0.708\n0.858\n0.415\n0.699\n0.521\n0.865\nDPCA [13]\n0.929\n0.238\n0.238\n0.189\n0.823\n0.861\n0.167\n0.377\n0.191\n0.704\n0.893\n0.512\n0.508\n0.510\n0.807\nImageDiff [31]\n0.799\n0.148\n0.483\n0.181\n0.752\n0.769\n0.121\n0.546\n0.182\n0.696\n0.857\n0.411\n0.695\n0.516\n0.861\nImageRatio [28]\n0.810\n0.113\n0.402\n0.149\n0.697\n0.863\n0.067\n0.136\n0.042\n0.604\n0.829\n0.352\n0.667\n0.461\n0.807\nImageRegr [30]\n0.897\n0.196\n0.413\n0.222\n0.821\n0.747\n0.134\n0.615\n0.198\n0.743\n0.870\n0.437\n0.647\n0.522\n0.849\nIRMAD [35]\n0.900\n0.155\n0.472\n0.204\n0.864\n0.822\n0.162\n0.507\n0.223\n0.722\n0.843\n0.394\n0.789\n0.526\n0.885\nMAD [34]\n0.877\n0.143\n0.534\n0.191\n0.857\n0.770\n0.121\n0.594\n0.189\n0.714\n0.815\n0.348\n0.782\n0.482\n0.871\nPCDA [14]\n0.855\n0.192\n0.456\n0.210\n0.805\n0.758\n0.119\n0.598\n0.183\n0.737\n0.862\n0.425\n0.728\n0.537\n0.889\nDeepCVA [42]\n0.916\n0.216\n0.350\n0.245\n0.835\n0.859\n0.259\n0.277\n0.268\n0.847\n0.863\n0.432\n0.544\n0.482\n0.891\nSupervised CD methods (supervised on the OSCD training set and tested on the test set of the respective dataset):\nUNet [50]\n0.946\n0.466\n0.176\n0.186\n0.776\n0.936\n0.187\n0.098\n0.088\n0.718\n0.844\n0.344\n0.355\n0.349\n0.839\nSeCo-\n(Rand) [32]\n0.946\n0.477\n0.150\n0.180\n0.812\n0.907\n0.234\n0.306\n0.210\n0.720\n0.880\n0.506\n0.488\n0.497\n0.777\nSeCo-(Pre) [32]\n0.928\n0.412\n0.182\n0.195\n0.747\n0.860\n0.198\n0.363\n0.199\n0.691\n0.905\n0.654\n0.588\n0.619\n0.867\nDeep Metric Learning CD:\nOurs\n0.958\n0.383\n0.480\n0.325\n0.937\n0.948\n0.325\n0.256\n0.286\n0.913\n0.908\n0.486\n0.697\n0.573\n0.928\n0\n0.2\n0.4\n0.6\n0.8\n1\nFalse Positive Rate (FPR)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTrue Positive Rate (TPR)\nCVA\nDPCA\nImageDiff\nImageRatio\nImageRegr\nIRMAD\nMAD\nPCDA\nDeepCVA\nUNet (Random Init., Sup.:OSCD)\nSeCo(Pre:ImageNet, Sup.:OSCD)\nSeCo(Pre:Sentinel-2, Sup.:OSCD)\nDeep Distance Metric Learning (Ours)\n0\n0.2\n0.4\n0.6\n0.8\n1\nFalse Positive Rate (FPR)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTrue Positive Rate (TPR)\nCVA\nDPCA\nImageDiff\nImageRatio\nImageRegr\nIRMAD\nMAD\nPCDA\nDeepCVA\nUNet (Random Init., Sup.:OSCD)\nSeCo(Pre:ImageNet, Sup.:OSCD)\nSeCo(Pre:Sentinel-2, Sup.:OSCD)\nDeep Metric Learning (Ours)\n0\n0.2\n0.4\n0.6\n0.8\n1\nFalse Positive Rate (FPR)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTrue Positive Rate (TPR)\nCVA\nDPCA\nImageDiff\nImageRatio\nImageRegr\nIRMAD\nMAD\nPCDA\nDeepCVA\nUNet (Random Init., Sup.:OSCD)\nSeCo(Pre:ImageNet, Sup.:OSCD)\nSeCo(Pre:Sentinel-2, Sup.:OSCD)\nDeep Metric Learning (Ours)\nFig. 4: Average ROC curves corresponding to different CD methods on (a) OSCD [7], (b) SZTAKI [5], [6], and (c) QuickBird\ndatasets [55].\nat generalizing under domain shifts, which is common in the\ndatasets we consider here. Since the proposed CD approach\naddresses this problem by optimizing the network parameters\nfor a given MT-RSI separately to obtain the change probability\nmap, it shows consistently higher CD performance in OA, F1,\nand AUC metrics than the SOTA approaches. Furthermore,\nwe visualize the ROC curves corresponding to different CD\nmethods in Fig. 4. From these graphs, one can clearly see the\nsignificance of the proposed CD approach compared to SOTA\nCD methods.\nb) Qualitative results.: We present qualitative visual-\nizations of the predicted change maps from different CD\nalgorithms on the OSCD, SZTAKI, and QuickBird datasets\nin Fig. 5, Fig. 6, and Fig. 7, respectively. We can see that\nthe proposed CD can capture most of the relevant changes\nthat are difficult to capture by the SOTA CD methods (see\nthe highlighted regions by bounding boxes) while minimiz-\ning false positives and false negative predictions. Please see\nsupplementary document for additional qualitative results.\nVI. ABLATION STUDY\na) Effect of PCC.: Fig. 8 shows an example of how\nPCC minimizes the global and local color changes in MT-\nRSIs. After PCC, the pre-change image’s (I1) color space is\nmapped to the post-change image’s (I2) color space - making\nthe difference image (Id) to be less sensitive to colorimetric\nchanges. Please see supplementary document for additional\nexamples.\nTABLE II: Caption\nNetwork\nAUC\nU-Net\n0.847\nHourglass\n0.852\nResNet (N=4)\n0.890\nResNet (N=8)\n0.908\nResNet (N=16)\n0.922\nResNet (N=32)\n0.937\nResNet (N=64)\n0.925\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n7\nFig. 5: Qualitative results on lasvegas image in the OSCD dataset. (a) I1. (b) I2. (c) Reference change mask. (d)\nImageDiff [31]. (e) ImageRatio [28]. (f) ImageRegr [30]. (g) CVA [43]. (h) DPCA [13]. (i) MAD [34]. (j) IRMAD [35].\n(k) PCDA [14]. (l) Deep-CVA [42]. (m) SeCo [32]: Self-supervised pre-trained on Sentinel-2 [16] and supervised on the\nOSCD [7] training set. (n) Deep Metric Learning CD (Ours).\nFig. 6: Qualitative results on the archive image in the SZTAKI dataset [5], [6]. (a) I1. (b) I2. (c) Reference change mask.\n(d) ImageDiff [31]. (e) ImageRatio [28]. (f) ImageRegr [30]. (g) CVA [43]. (h) DPCA [13]. (i) MAD [34]. (j) IRMAD [35].\n(k) PCDA [14]. (l) Deep-CVA [42]. (m) SeCo [32]: Self-supervised pre-trained on Sentinel-2 [16] and supervised on the\nOSCD [7] training set. (n) Deep Metric Learning CD (Ours).\nb) Experiments with different architectures for D-CPG.:\nWe experimented with different network architectures for D-\nCPG, and the results are summarized in Fig. 9 and Tab. II.\nWe observe a higher AUC when we cascade 32 residual blocks\n(denoted as ResNet (N = 32)) compared to Hourglass, U-Net,\nand other variations of ResNet. Furthermore, we generally ob-\nserve faster convergence when we utilize ResNet architecture\nthan U-Net and Hourglass networks. Therefore, for all of our\nexperiments, we utilized ResNet with N = 32.\nc) Convergence characteristic.: Fig. 10 shows an exam-\nple of how the proposed approach converges to the optimal\nchange probability map P∗\nc for a given bi-temporal image pair.\nd) Contribution from different loss terms.:\nTable III\nsummarizes the contribution from each loss term - Limg,\nLfeat, and Lctx on the CD performance. We can observe that\ncomputing similarity-dissimilarity loss in the feature domain\n(Lfeat) improves the CD performance in OA, F1, and AUC\nmetrics significantly over the image domain loss (Limg). Fur-\nthermore, imposing context-consistency constraint on deep\nfeatures (Lctx) on top of Lfeat results in a significant boost\nin the CD performance. Moreover, combining the three loss\nTABLE III: Contribution from each loss term in LCD on the\nCD performance for l ∈{1, 2}(averaging over the test-set of\nOSCD [7]).\nLimg\nLfeat\nLctx\nOA\nF1\nAUC\n✓\n✗\n✗\n0.911\n0.222\n0.821\n✗\n✓\n✗\n0.936\n0.296\n0.879\n✗\n✓\n✓\n0.953\n0.319\n0.926\n✓\n✓\n✗\n0.940\n0.303\n0.892\n✓\n✓\n✓\n0.958\n0.325\n0.937\nterms - Limg, Lfeat, and Lctx with appropriate regularization\nconstants further improves the CD performance.\ne) Performance with different scales of features from D-\nFE.: According to the results summarized in Tab. IV, the best\nCD performance is observed when we utilize deep features\nfrom the first two hierarchical scales of VGG-16 (i.e., l=1\nand 2). Utilizing additional scales of deep features result\nin degradation of the CD performance - so for all of our\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n8\nFig. 7: Qualitative results on the QuickBird dataset [55]. (a) I1. (b) I2. (c) Reference change mask. (d) ImageDiff [31]. (e)\nImageRatio [28]. (f) ImageRegr [30]. (g) CVA [43]. (h) DPCA [13]. (i) MAD [34]. (j) IRMAD [35]. (k) PCDA [14]. (l)\nDeep-CVA [42]. (m) SeCo [32]: Self-supervised pre-trained on Sentinel-2 [16] and supervised on the OSCD [7] training set.\n(n) Deep Metric Learning CD (Ours).\nFig. 8: The effect of PCC on rio image pair in the OSCD [7].\nFig. 9: Average ROC curves for different architectures of D-\nCPG on OSCD [7].\nFig. 10: How our CD method converges to the optimal\nchange map P∗\nc during testing on the beirut image pair\nin OSCD [7].\nTABLE IV: Effect of utilizing different scales of deep features\nfrom D-FE on the CD performance (averaging over test-set of\nOSCD [7]).\nLoss function configuration\nOA\nF1\nAUC\nBaseline: Limg\n0.911\n0.222\n0.821\nLimg+Ll\nfeat + Ll\nctx; l ∈{1}\n0.936\n0.298\n0.855\nLimg+Ll\nfeat + Ll\nctx; l ∈{1, 2}\n0.958\n0.325\n0.937\nLimg+Ll\nfeat + Ll\nctx; l ∈{1, 2, 3}\n0.943\n0.315\n0.929\nLimg+Ll\nfeat + Ll\nctx; l ∈{1, 2, 3, 4}\n0.932\n0.307\n0.914\nexperiments we set l = 2 in Lfeat (Eqn. 6) and Lctx (Eqn.\n7).\nVII. CONCLUSION\nSociety is becoming increasingly aware of human activities\non the global climate. There is overwhelming evidence that\nthese activities have short- and long-term effects on almost\nevery aspect of our lives. Using global climate measurements\nand simulations, it is now possible to observe changes on a\nglobal scale, such as sea level rise or changes in the Gulf\nStream. On the other hand, accurate predictions of local\nchanges are much more difficult to obtain. Common examples\ninclude land use for agriculture, deforestation, flooding, forest\nfires, growth of urban areas, and transportation infrastructure.\nTherefore, it is extremely important to monitor these local\nchanges as these are the factors that ultimately exacerbate the\nglobal climate crisis.\nREFERENCES\n[1] Deep white-balance editing. In: Proceedings of the IEEE/CVF Confer-\nence on computer vision and pattern recognition. pp. 1397–1406 (2020)\n3\n[2] Bandara, W.G.C., Nair, N.G., Patel, V.M.: Ddpm-cd: Remote sensing\nchange detection using denoising diffusion probabilistic models (2022).\nhttps://doi.org/10.48550/ARXIV.2206.11892, https://arxiv.org/abs/2206.\n11892 1\n[3] Bandara, W.G.C., Patel, V.M.: Revisiting consistency regularization for\nsemi-supervised change detection in remote sensing images (2022).\nhttps://doi.org/10.48550/ARXIV.2204.08454, https://arxiv.org/abs/2204.\n08454 2\n[4] Bandara, W.G.C., Patel, V.M.: A transformer-based siamese network\nfor change detection. In: IGARSS 2022 - 2022 IEEE International\nGeoscience and Remote Sensing Symposium. pp. 207–210 (2022).\nhttps://doi.org/10.1109/IGARSS46834.2022.9883686 1\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n9\n[5] Benedek, C., Szir´anyi, T.: A mixed markov model for change detection\nin aerial photos with large time differences. In: 2008 19th International\nConference on Pattern Recognition. pp. 1–4. IEEE (2008) 5, 6, 7\n[6] Benedek, C., Szir´anyi, T.: Change detection in optical aerial images by\na multilayer conditional mixed markov model. IEEE Transactions on\nGeoscience and Remote Sensing 47(10), 3416–3430 (2009) 5, 6, 7\n[7] Caye Daudt, R., Le Saux, B., Boulch, A., Gousseau, Y.: Urban change\ndetection for multispectral earth observation using convolutional neu-\nral networks. In: IEEE International Geoscience and Remote Sensing\nSymposium (IGARSS) (July 2018) 5, 6, 7, 8\n[8] Celik, T.: Unsupervised change detection in satellite images using\nprincipal component analysis and k-means clustering. IEEE Geoscience\nand Remote Sensing Letters 6(4), 772–776 (2009) 2\n[9] Chen, Y., Bruzzone, L.: Self-supervised change detection in multi-view\nremote sensing images. arXiv preprint arXiv:2103.05969 (2021) 1, 3\n[10] Coppin, P., Jonckheere, I., Nackaerts, K., Muys, B., Lambin, E.: Review\narticledigital change detection methods in ecosystem monitoring: a\nreview. International journal of remote sensing 25(9), 1565–1596 (2004)\n1\n[11] Daudt, R.C., Le Saux, B., Boulch, A.: Fully convolutional siamese net-\nworks for change detection. In: 2018 25th IEEE International Conference\non Image Processing (ICIP). pp. 4063–4067. IEEE (2018) 1, 2\n[12] De Maesschalck, R., Jouan-Rimbaud, D., Massart, D.L.: The maha-\nlanobis distance. Chemometrics and intelligent laboratory systems 50(1),\n1–18 (2000) 3\n[13] Deng, J., Wang, K., Deng, Y., Qi, G.: Pca-based land-use change\ndetection and analysis using multitemporal and multisensor satellite data.\nInternational Journal of Remote Sensing 29(16), 4823–4838 (2008) 6,\n7, 8\n[14] Dharani, M., Sreenivasulu, G.: Land use and land cover change detection\nby using principal component analysis and morphological operations\nin remote sensing applications. International Journal of Computers and\nApplications 43(5), 462–471 (2021) 6, 7, 8\n[15] Di Pilato, A., Taggio, N., Pompili, A., Iacobellis, M., Di Florio,\nA., Passarelli, D., Samarelli, S.: Deep learning approaches to earth\nobservation change detection. Remote Sensing 13(20),\n4083 (2021)\n2\n[16] Drusch, M., Del Bello, U., Carlier, S., Colin, O., Fernandez, V., Gascon,\nF., Hoersch, B., Isola, C., Laberinti, P., Martimort, P., et al.: Sentinel-\n2: Esa’s optical high-resolution mission for gmes operational services.\nRemote sensing of Environment 120, 25–36 (2012) 5, 7, 8\n[17] Finlayson, G.D., Mackiewicz, M., Hurlbert, A.: Color correction using\nroot-polynomial regression. IEEE Transactions on Image Processing\n24(5), 1460–1470 (2015) 3\n[18] Hartigan, J.A., Wong, M.A.: Algorithm as 136: A k-means clustering\nalgorithm. Journal of the royal statistical society. series c (applied\nstatistics) 28(1), 100–108 (1979) 2\n[19] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image\nrecognition. In: Proceedings of the IEEE conference on computer vision\nand pattern recognition. pp. 770–778 (2016) 4\n[20] Hong, G., Luo, M.R., Rhodes, P.A.: A study of digital camera colori-\nmetric characterization based on polynomial modeling. Color Research\n& Application: Endorsed by Inter-Society Color Council, The Colour\nGroup (Great Britain), Canadian Society for Color, Color Science\nAssociation of Japan, Dutch Society for the Study of Color, The Swedish\nColour Centre Foundation, Colour Society of Australia, Centre Franc¸ais\nde la Couleur 26(1), 76–84 (2001) 3\n[21] de Jong, K.L., Bosman, A.S.: Unsupervised change detection in satellite\nimages using convolutional neural networks. In: 2019 International Joint\nConference on Neural Networks (IJCNN). pp. 1–8. IEEE (2019) 1, 2,\n3\n[22] Khelifi, L., Mignotte, M.: Deep learning for change detection in remote\nsensing images: Comprehensive review and meta-analysis. IEEE Access\n8, 126385–126400 (2020) 2\n[23] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980 (2014) 5\n[24] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with\ndeep convolutional neural networks. Advances in neural information\nprocessing systems 25, 1097–1105 (2012) 5\n[25] Kuncheva, L.I., Faithfull, W.J.: Pca feature extraction for change detec-\ntion in multidimensional unlabeled data. IEEE transactions on neural\nnetworks and learning systems 25(1), 69–80 (2013) 2\n[26] Leenstra, M., Marcos, D., Bovolo, F., Tuia, D.: Self-supervised pre-\ntraining enhances change detection in sentinel-2 imagery. arXiv preprint\narXiv:2101.08122 (2021) 2\n[27] Lu, D., Mausel, P., Brondizio, E., Moran, E.: Change detection tech-\nniques. International journal of remote sensing 25(12), 2365–2401\n(2004) 2\n[28] Lu, D., Mausel, P., Brondizio, E., Moran, E.: Change detection tech-\nniques. International journal of remote sensing 25(12), 2365–2401\n(2004) 6, 7, 8\n[29] Luo, W., Li, Y., Urtasun, R., Zemel, R.: Understanding the effective\nreceptive field in deep convolutional neural networks. In: Proceedings\nof the 30th International Conference on Neural Information Processing\nSystems. pp. 4905–4913 (2016) 4\n[30] Luppino, L.T., Bianchi, F.M., Moser, G., Anfinsen, S.N.: Unsupervised\nimage regression for heterogeneous change detection. IEEE Transac-\ntions on Geoscience and Remote Sensing 57(12), 9960–9975 (2019).\nhttps://doi.org/10.1109/TGRS.2019.2930348 6, 7, 8\n[31] MAHMOUDZADEH, H.: Digital change detection using remotely\nsensed data for monitoring green space destruction in tabriz (2007) 6,\n7, 8\n[32] Ma˜nas, O., Lacoste, A., Giro-i Nieto, X., Vazquez, D., Rodriguez, P.:\nSeasonal contrast: Unsupervised pre-training from uncurated remote\nsensing data. arXiv preprint arXiv:2103.16607 (2021) 1, 5, 6, 7, 8\n[33] Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human\npose estimation. In: European conference on computer vision. pp. 483–\n499. Springer (2016) 4\n[34] Nielsen, A.A., Conradsen, K., Simpson, J.J.: Multivariate alteration de-\ntection (mad) and maf postprocessing in multispectral, bitemporal image\ndata: New approaches to change detection studies. Remote Sensing of\nEnvironment 64(1), 1–19 (1998) 6, 7, 8\n[35] Nielsen, A.A.: The regularized iteratively reweighted mad method for\nchange detection in multi-and hyperspectral data. IEEE Transactions on\nImage processing 16(2), 463–478 (2007) 6, 7, 8\n[36] Pan, S.J., Yang, Q.: A survey on transfer learning. IEEE Transactions\non knowledge and data engineering 22(10), 1345–1359 (2009) 3, 4\n[37] Qian, Q., Tang, J., Li, H., Zhu, S., Jin, R.: Large-scale distance metric\nlearning with uncertainty. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. pp. 8542–8550 (2018) 4\n[38] Radke, R.J., Andra, S., Al-Kofahi, O., Roysam, B.: Image change\ndetection algorithms: a systematic survey. IEEE transactions on image\nprocessing 14(3), 294–307 (2005) 3\n[39] Ren, C., Wang, X., Gao, J., Zhou, X., Chen, H.: Unsupervised change\ndetection in satellite images with generative adversarial network. IEEE\nTransactions on Geoscience and Remote Sensing (2020) 1, 2\n[40] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks\nfor biomedical image segmentation. In: International Conference on\nMedical image computing and computer-assisted intervention. pp. 234–\n241. Springer (2015) 4\n[41] Saha, S., Bovolo, F., Bruzzone, L.: Unsupervised deep change vector\nanalysis for multiple-change detection in vhr images. IEEE Transac-\ntions on Geoscience and Remote Sensing 57(6), 3677–3693 (2019).\nhttps://doi.org/10.1109/TGRS.2018.2886643 1, 2, 3\n[42] Saha, S., Bovolo, F., Bruzzone, L.: Unsupervised deep change vector\nanalysis for multiple-change detection in vhr images. IEEE Transac-\ntions on Geoscience and Remote Sensing 57(6), 3677–3693 (2019).\nhttps://doi.org/10.1109/TGRS.2018.2886643 2, 6, 7, 8\n[43] Saha, S., Bovolo, F., Bruzzone, L.: Unsupervised deep change vector\nanalysis for multiple-change detection in vhr images. IEEE Transactions\non Geoscience and Remote Sensing 57(6), 3677–3693 (2019) 6, 7, 8\n[44] Shi, W., Zhang, M., Zhang, R., Chen, S., Zhan, Z.: Change detection\nbased on artificial intelligence: State-of-the-art and challenges. Remote\nSensing 12(10), 1688 (2020) 2\n[45] Shorten, C., Khoshgoftaar, T.M.: A survey on image data augmentation\nfor deep learning. Journal of Big Data 6(1), 1–48 (2019) 5\n[46] Simonyan, K., Zisserman, A.: Very deep convolutional networks for\nlarge-scale image recognition. arXiv preprint arXiv:1409.1556 (2014) 5\n[47] Singh, A.: Review article digital change detection techniques using\nremotely-sensed data. International journal of remote sensing 10(6),\n989–1003 (1989) 1\n[48] Skifstad, K., Jain, R.: Illumination independent change detection for\nreal world image sequences. Computer vision, graphics, and image\nprocessing 46(3), 387–399 (1989) 2\n[49] St-Charles, P.L., Bilodeau, G.A., Bergevin, R.: Subsense: A universal\nchange detection method with local adaptive sensitivity. IEEE Transac-\ntions on Image Processing 24(1), 359–373 (2014) 2\n[50] Sun, S., Mu, L., Wang, L., Liu, P.: L-unet: An lstm network for remote\nsensing image change detection. IEEE Geoscience and Remote Sensing\nLetters (2020) 6\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n10\n[51] Tang, H., Chu, S., Hasegawa-Johnson, M., Huang, T.: Partially su-\npervised speaker clustering. IEEE transactions on pattern analysis and\nmachine intelligence 34(5), 959–971 (2011) 4\n[52] Walter, V.: Object-based classification of remote sensing data for change\ndetection. ISPRS Journal of photogrammetry and remote sensing 58(3-\n4), 225–238 (2004) 2\n[53] Wiratama, W., Sim, D.: Fusion network for change detection of high-\nresolution panchromatic imagery. Applied Sciences 9(7), 1441 (2019)\n1, 2\n[54] Xing, E., Jordan, M., Russell, S.J., Ng, A.: Distance metric learning\nwith application to clustering with side-information. Advances in neural\ninformation processing systems 15, 521–528 (2002) 4\n[55] Zhang,\nM.,\nShi,\nW.:\nA\nfeature\ndifference\nconvolutional\nneu-\nral\nnetwork-based\nchange\ndetection\nmethod.\nIEEE\nTransactions\non Geoscience and Remote Sensing 58(10), 7232–7246 (2020).\nhttps://doi.org/10.1109/TGRS.2020.2981051 2, 5, 6, 8\n[56] Zhao, W., Mou, L., Chen, J., Bo, Y., Emery, W.J.: Incorporating metric\nlearning and adversarial network for seasonal invariant change detection.\nIEEE Transactions on Geoscience and Remote Sensing 58(4), 2720–\n2731 (2020). https://doi.org/10.1109/TGRS.2019.2953879 2\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "eess.IV"
  ],
  "published": "2023-03-16",
  "updated": "2023-03-16"
}