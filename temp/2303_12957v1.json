{
  "id": "http://arxiv.org/abs/2303.12957v1",
  "title": "Reinforcement Learning with Exogenous States and Rewards",
  "authors": [
    "George Trimponias",
    "Thomas G. Dietterich"
  ],
  "abstract": "Exogenous state variables and rewards can slow reinforcement learning by\ninjecting uncontrolled variation into the reward signal. This paper formalizes\nexogenous state variables and rewards and shows that if the reward function\ndecomposes additively into endogenous and exogenous components, the MDP can be\ndecomposed into an exogenous Markov Reward Process (based on the exogenous\nreward) and an endogenous Markov Decision Process (optimizing the endogenous\nreward). Any optimal policy for the endogenous MDP is also an optimal policy\nfor the original MDP, but because the endogenous reward typically has reduced\nvariance, the endogenous MDP is easier to solve. We study settings where the\ndecomposition of the state space into exogenous and endogenous state spaces is\nnot given but must be discovered. The paper introduces and proves correctness\nof algorithms for discovering the exogenous and endogenous subspaces of the\nstate space when they are mixed through linear combination. These algorithms\ncan be applied during reinforcement learning to discover the exogenous space,\nremove the exogenous reward, and focus reinforcement learning on the endogenous\nMDP. Experiments on a variety of challenging synthetic MDPs show that these\nmethods, applied online, discover large exogenous state spaces and produce\nsubstantial speedups in reinforcement learning.",
  "text": "REINFORCEMENT LEARNING WITH EXOGENOUS STATES AND\nREWARDS\nA PREPRINT\nGeorge Trimponias‚àó\nAmazon\nLuxembourg\ntrimpog@amazon.lu\nThomas G. Dietterich‚àó\nCollaborative Robotics and Intelligent Systems (CoRIS) Institute\nOregon State University, Corvallis, OR 97331 USA\ntgd@cs.orst.edu\nMarch 24, 2023\nABSTRACT\nExogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled\nvariation into the reward signal. This paper formalizes exogenous state variables and rewards and\nshows that if the reward function decomposes additively into endogenous and exogenous components,\nthe MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous\nreward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any\noptimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because\nthe endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We\nstudy settings where the decomposition of the state space into exogenous and endogenous state spaces\nis not given but must be discovered. The paper introduces and proves correctness of algorithms for\ndiscovering the exogenous and endogenous subspaces of the state space when they are mixed through\nlinear combination. These algorithms can be applied during reinforcement learning to discover the\nexogenous space, remove the exogenous reward, and focus reinforcement learning on the endogenous\nMDP. Experiments on a variety of challenging synthetic MDPs show that these methods, applied\nonline, discover large exogenous state spaces and produce substantial speedups in reinforcement\nlearning.\nKeywords: Reinforcement learning, exogenous state variables, Markov Decision Processes, Markov\nReward Processes, causal discovery\n1\nIntroduction\nIn many practical settings, the actions of an agent have only a limited effect on the environment. For example, in a\nwireless cellular network, the cell tower base stations have many parameters that must be dynamically controlled to\noptimize network performance. We can formulate this as a Markov Decision Process (MDP) in which the reward\nfunction is the negative of the number of users who are suffering from low bandwidth. However, this reward is heavily\ninÔ¨Çuenced by exogenous factors such as the number, location, and behavior of the cellular network customers. Customer\ndemand varies stochastically as a function of exogenous factors (news, sporting events, trafÔ¨Åc accidents). In addition,\natmospheric conditions can affect the capacity of each wireless channel. This high degree of stochasticity can confuse\nreinforcement learning algorithms, because during exploration, the beneÔ¨Åt of trying action a in state s is hard to\ndetermine because the change in reward is obscured by the exogenous components of the reward. Many trials are\nrequired to average away these exogenous components so that the effect of the action can be measured. For temporal\ndifference algorithms, such as Q Learning, the learning rate needs to be very small. For policy gradient algorithms, the\nnumber of Monte Carlo trials required to estimate the gradient grows very large (or equivalently, the step size must be\nvery small). In this paper, we analyze this setting and develop algorithms for automatically detecting and removing the\neffects of exogenous state variables. This accelerates reinforcement learning (RL).\n‚àóEqual contributions\narXiv:2303.12957v1  [cs.LG]  22 Mar 2023\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nThe Ô¨Årst part of the paper deÔ¨Ånes exogenous state variables based on a causal foundation. It then introduces Exogenous\nState MDPs, which capture a broad class of MDPs with exogenous states. We characterize the space of all Exogenous\nState MDPs in terms of constraints on the structure of the corresponding two-time step dynamic Bayesian network. The\npaper analyzes the properties of exogenous subspaces of the state space and proves that every Exogenous State MDP\nhas a unique maximal exogenous subspace that contains all other exogenous subspaces. The paper then shows that,\nunder the assumption that the reward function decomposes additively into exogenous and endogenous components, the\nBellman equation for the original MDP decomposes into two equations: one for an exogenous Markov reward process\n(Exo-MRP) and the other for an endogenous MDP (Endo-MDP). Importantly, every optimal policy for the Endo-MDP\nis an optimal policy for the full MDP.\nIn the second part of the paper, we identify one condition‚Äîbased on the covariance between the endogenous and\nexogenous returns‚Äîunder which solving the Endo-MDP is faster (in sample complexity) than solving the full MDP. To\ndo this, we derive dynamic programming updates for the covariance between the H-horizon returns of the Exo-MRP\nand the Endo-MDP, which may be of independent interest.\nThe third part of the paper formulates the problem of discovering the Exo/Endo Decomposition assuming we have access\nto a sufÔ¨Åciently large and representative sample of ‚ü®s, a, r, s‚Ä≤‚ü©tuples recorded from MDP trajectories. The problem is\nformalized as a constrained optimization problem where the objective is to Ô¨Ånd an exogenous subspace that minimizes\nthe variance of the endogenous reward subject to conditional mutual information constraints that enforce the Exogenous\nState MDP structure. We introduce and study several variations of this constrained optimization problem. For the\ncase of linear subspaces, the mutual information constraints can be replaced by constraints on a quantity called the\nconditional correlation coefÔ¨Åcient (CCC), and this yields practical algorithms. This section concludes with a discussion\nof the conditions that the MDP and the exploration policy should satisfy such that the sample of ‚ü®s, a, r, s‚Ä≤‚ü©tuples is\nsufÔ¨Åcient for identifying violations of the conditional mutual information constraints and ensure that the variables in the\ndiscovered exogenous subspace are causally exogenous.\nThe fourth part of the paper introduces two algorithms for solving the conditional correlation formulation and studies\ntheir soundness and correctness. One algorithm solves a series of global optimization problems, and the other is a\nstepwise algorithm that identiÔ¨Åes one exogenous dimension at a time. We prove the correctness of the global algorithm,\nand we develop some insights into when the stepwise algorithm works well.\nFinally, the Ô¨Åfth part of the paper articulates a set of research questions and executes experiments to answer those\nquestions. The main Ô¨Ånding is that the Exo/Endo decomposition algorithms can discover large exogenous subspaces\nthat yield substantial speedups in reinforcement learning for the high-performing PPO actor-critic algorithm. The\nexperiments also study the sensitivity of our algorithms to their hyperparameters and suggest a practical strategy for\nsetting them. We also explore the behavior of the algorithms on MDPs with nonlinear rewards, nonlinear dynamics, and\ndiscrete states. Connections to previous work are interleaved throughout the paper.\n2\nMDPs with Exogenous States and Rewards\nWe study discrete time stationary MDPs with stochastic rewards and stochastic transitions [Puterman, 1994, Sutton\nand Barto, 1998]; the state and action spaces may be either discrete or continuous. Notation: state space S, action\nspace A, reward distribution R: S √ó A 7‚ÜíP(R) (where P(R) is the space of probability distributions over the real\nnumbers), transition function P : S √ó A 7‚ÜíP(S) (where P(S) is the space of probability distributions over S), starting\nstate distribution P0 ‚ààP(S), and discount factor Œ≥ ‚àà(0, 1]. We assume that for all (s, a) ‚ààS √ó A, R(s, a) has\nexpected value m(s, a) and Ô¨Ånite variance œÉ2(s, a). We denote random variables by capital letters (S, A, etc.) and their\ncorresponding values by lower case letters (s, a, etc.). Table 1 summarizes the notation employed in this paper.\nLet the state space S take the form S = √ód\ni=1Si, where Si deÔ¨Ånes the domain of the ith state variable. In our problems\nthe domain of each variable is either the real numbers or a Ô¨Ånite, discrete set of values. Each state s ‚ààS can then be\nwritten as a d-tuple of the values of these state variables s = (s1, . . . , sd), with si ‚ààSi. We refer to si as the value of\nthe ith state variable. We denote by St = √ód\ni=iSt,i the random vector for the state at time t. Similarly, At is the random\nvariable for the action at time t, and Rt is the random variable for the reward at time t. In some formulas, instead of\nindexing by time, we will use ‚Äúprime‚Äù notation. For example, S and S‚Ä≤ denote the current and next states, respectively\n(and analogously for A and A‚Ä≤, R and R‚Ä≤). When it is clear from the context, we will also refer to Si as the i-th state\nvariable. In the terminology of Koller and Friedman [2009], Si is a ‚Äútemplate variable‚Äù that refers to the family of\nrandom variables that correspond to Si at all time steps: {S1,i, S2,i, . . . , SH,i}.\nWe are interested in problems where the set of state variables S can be decomposed into endogenous and exogenous\nsets E and X. In the simplest case, this can be accomplished by variable selection. Following the notation of Efroni\net al. [2022a], deÔ¨Åne an index set I as a subset of [d] = {1, . . . , d} and Ic = [d] \\ I as its complement. The variable\n2\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nSymbol\nMeaning\nendo, exo\nendogenous, exogenous\nCMI\nConditional Mutual Information\nCCC\nConditional Correlation CoefÔ¨Åcient\nS, E, X, A\nState, Endo State, Exo State, and Action Spaces\nS/S‚Ä≤\nRandom Vector for Current/Next State\ns, a, r\nRealizations of the State, Action, and Reward\nSi, Si, si\nThe ith component of S, S, s\nS, S‚Ä≤, A, R\nObservational data for state, next state, action, reward\nP(B)\nSpace of probability distributions over space B\nS = (E, X)\nDecomposition of S into endo and exo sets E and X\n‚äÜ, ‚äÇ\nSubset of, Strict (proper) subset of\n[d]\nThe set {1, . . . , d}\nI, Ic\nIndex set (‚äÜ[d]), Complement of I (= [d] \\ I)\nS[I], S[I], s[I]\n(Si)i‚ààI, (Si)i‚ààI, (si)i‚ààI\nI(A; B | C)\nCMI of random vectors A and B given C\nA ‚ä•‚ä•B | C\nConditional independence of A and B given C\nR, Rd\nSet of real numbers, Real d-dimensional vector space\nRm√ón\nThe vector space of m √ó n matrices over R\nS\nW= [E, X]\nLinear decomposition of S into endo/exo parts E and X via W\nA + B, A ‚äïB\nSum (Direct Sum) of vector spaces A and B\nA ‚äëB, A ‚äèB\nA is a vector subspace (proper subspace) of vector space B\nA‚ä•\nOrthogonal complement of subspace A of vector space S\ndim(A)\nDimension of vector subspace A\nIn, 0m√ón\nIdentity matrix of size n, Matrix of zeros of size m √ó n\nWexo\nMatrix that deÔ¨Ånes the linear exogenous subspace\ntr(A), A‚ä§, det(A)\nTrace of matrix A, Transpose of A, Determinant of A\n‚à•u‚à•2\nEuclidean norm of vector u\nŒ£AA, Œ£AB\nCovariance matrix of A, Cross-covariance matrix of A, B\nN(¬µ, œÉ2)\nGaussian distribution with mean ¬µ and variance œÉ2\nTable 1: Symbols and Abbreviations.\nselection formulation aims to discover an index set I so that the state vector S = √ód\ni=1Si can be decomposed into two\ndisjoint sets of state variables X = √ói‚ààISi = S[I] and E = √ói‚ààIcSi = S[Ic]. We will also denote the corresponding\nexo and endo state spaces as X = √ói‚ààISi = S[I] and E = √ói‚ààIcSi = S[Ic].\nIn many problems, the given state variables do not neatly separate into endogenous and exogenous subsets. Instead, we\nmust discover a mapping Œæ : U 7‚ÜíV such that the Ô¨Årst dexo dimensions of Œæ(S) provide the exogenous state variables,\nand the remaining d ‚àídexo dimensions give the endogenous state variables. In this paper, we will be interested in the\ncase where Œæ is a full-rank linear transformation. In general, we will argue that Œæ should a diffeomorphism so that no\ninformation in the original space S is lost in the new space Œæ(S).\n2.1\nExogenous State Variables\nThe notion of exogeneity is fundamentally causal: a variable is exogenous if it is impossible for our actions to affect its\nvalue. We formalize this in terms of Pearl‚Äôs do-calculus.\nDeÔ¨Ånition 1 (Causally-Exogenous Variables). A set of state variables X = S[I] is causally exogenous for MDP M\nwith causal graph G if and only if for all times t < H, graph G encodes the conditional independence\nP(Xt+1, . . . , XH | Xt, do(At = at)) = P(Xt+1, . . . , XH | Xt)\n‚àÄat ‚ààA.\n(1)\nCausal exogeneity is a qualitative property that depends only on the structure of the causal graph G and not on the\nparameters of the probability distributions associated with each node.\nConsider any state decomposition of S based on an index set I ‚äÜ[d], so that X = S[I] and E = S[Ic]. Assuming the\nMDP is stationary, its causal graph can be described by the 2-timestep Dynamic Bayesian Network shown in Figure 1,\nwhich includes all possible edges between X, E, and actions A at the current time step and X‚Ä≤ and E‚Ä≤ in the next\n3\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nùê¥\nùëã\nùúã\nùê∏\nùëã‚Ä≤\nùê∏‚Ä≤\nFigure 1: A fully-connected dynamic Bayesian network for state decomposition S = (E, X) (reward function not\nshown).\ntime step. We have included diachronic edges (from one time step to the next) and synchronic edges (within a single\ntime step). At most one of the two synchronic edges connecting X‚Ä≤ and E‚Ä≤ can be present in an MDP. In this diagram,\nE, X, E‚Ä≤, and X‚Ä≤ denote sets of state variables, and if there is an edge between two nodes U ‚ÜíV , it means there exists\nat least one edge between one variable in U and one variable in V .\nWe now derive a condition on the structure of the DBN causal graph that is sufÔ¨Åcient to ensure that the variables in X\nare exogenous.\nTheorem 1 (Full Exogenous DBN). Any DBN with a causal graph matching the structure of Figure 2 and any DBN\ngraph obtained by deleting edges from this structure, when unrolled for H time steps, yields an MDP for which X is\ncausally exogenous. We call the state decomposition S = (E, X) a full exo/endo decomposition with exogenous set X.\nProof. Figure 3 shows the DBN causal graph unrolled over the H-step horizon. To obtain the result, we will apply Rule\n3 of the do-calculus to remove the do(At) action. We will treat A0, . . . , At‚àí1, At+1, . . . , AH‚àí1 as random variables\ndistributed according to (possibly stochastic) policy œÄ. We must show that the result holds for all possible policies.\nRule 3 states that for any causal graph G\nP(F | do(G), do(H), J) = P(F | do(G), J), if F ‚ä•‚ä•H | G ‚à™J in ÀúG,\nwhere ÀúG is the graph obtained by Ô¨Årst deleting all edges pointing into G and then deleting all arrows pointing into H, if\nH is not an ancestor of J.\nConsider the query P(Xt+1, . . . , XH\n| Xt, do(At)) and apply Rule 3 with the following bindings:\nF\n‚Üê\nXt+1, . . . , XH, G ‚Üê‚àÖ, H ‚ÜêAt, and J ‚ÜêXt; in that case, ÀúG will be the same as G, except that the incom-\ning edges to At from Xt and Et are removed.\nNow apply the d-separation procedure to ÀúG by forming the graph of all variables mentioned in the query\nP(Xt+1, . . . , XH | Xt) and their ancestors. This ancestral graph contains only the singleton node At and the\nMarkov chain P(X0) ¬∑ P(X1 | X0) ¬∑ ¬∑ ¬∑ P(Xt+1 | Xt) ¬∑ ¬∑ ¬∑ P(XH | XH‚àí1). The fact that AT is disconnected from the\nMarkov chain establishes that Xt+1, . . . , XH ‚ä•‚ä•At | Xt. Applying Rule 3 gives the result. Note that the policy œÄ\nplays no role in the derivation. Hence, the result applies for any possible policy.\nDeleting edges from the DBN graph cannot introduce new ancestors, so X remains exogenous under edge deletion.\nùê¥\nùëã\nùúã\nùê∏\nùëã‚Ä≤\nùê∏‚Ä≤\nFigure 2: Restricted dynamic Bayesian network sufÔ¨Åcient to establish that X is exogenous. We call this the ‚Äúfull setting‚Äù\nbecause it includes the synchronic edge from X‚Ä≤ to E‚Ä≤.\n4\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nùê¥ùë°‚àí1\nùê¥ùë°\nùê¥ùë°+1\nùê∏ùë°‚àí1\nùê∏ùë°\nùê∏ùë°+1\nùëãùë°‚àí1\nùëãùë°\nùëãùë°+1\nùúã\nùúã\nùúã\n‚Ä¶\n‚Ä¶\nùê¥0\nùê∏0\nùëã0\nùúã\nùê¥ùêª‚àí1\nùê∏ùêª\nùëãùêª\nùúã\nFigure 3: Unrolled state transition diagram for the full exo DBN.\nNext, we examine some properties of exo/endo decompositions.\nWe Ô¨Årst note that it is possible for an MDP with exogenous state variables to accept multiple exo/endo decompositions.\nIn Figure 4, the decompositions (X1 = {S2}, E1 = {S1, S3}) and (X2 = {S1, S2}, E3 = {S3}) are both valid\ndecompositions, since they match the full DBN template of Figure 2. This shows that the set E in an exo/endo\ndecomposition (E, X) may contain additional exogenous state variables not in X.\nTheorem 2 (Union of Exo/Endo Decompositions). Assume an MDP accepts two full exo/endo decompositions (E1, X1)\nand (E2, X2). DeÔ¨Åne the union of the two decompositions as the state decomposition (E, X) with X = X1 ‚à™X2 and\nE = E1 ‚à©E2. Then (E, X) is a full exo/endo decomposition with exo state set X.\nProof. Because X1 is exogenous, there are no edges from any of the remaining state variables E1 and no edges from\nthe action A to any variable in X1. Similarly, there are no edges from nodes in E2 or A to nodes in X2. Hence, the\nunion X = X1 ‚à™X2 also has no incoming edges from E1, E2, or A, and therefore has no incoming edges from E.\nThis shows that the decomposition (E, X) satisÔ¨Åes the conditions of Theorem 1.\nOn the other hand, not every subset of exogenous state variables can yield a valid exo/endo decomposition.\nTheorem 3 (Exogenous Subsets). Let (X, E) be a full exo/endo decomposition and X1 and X2 be non-empty, disjoint\nproper subsets of X, X1 ‚à©X2 = ‚àÖ, such that their disjoint union gives back X: X1 ‚à™X2 = X. Then (X1, X2 ‚à™E) is\nnot necessarily a valid full exo/endo decomposition.\nProof. By example. In Figure 4, the decomposition X3 = {S1}, E3 = {S2, S3} is not a valid full exo/endo decompo-\nsition due to the link from E‚Ä≤\n3 to X‚Ä≤\n3 (speciÔ¨Åcally the link S‚Ä≤\n2 ‚ÜíS‚Ä≤\n1).\nFor an MDP with exogenous state variables, we will be interested in the exo/endo decomposition where the exo set X\nis as large as possible. This is formalized in the next deÔ¨Ånition:\nDeÔ¨Ånition 2 (Maximal Exo/Endo Decomposition). Given an MDP, the full exo/endo decomposition (E, X) is maximal\nif there is no other full exo/endo decomposition ( ÀúE, ÀúX) with | ÀúX| > |X|. We denote the maximal decomposition as\n(Em, Xm) and call Xm the maximal exo set.\nCorollary 1 (Uniqueness of Maximal Exo/Endo Decomposition). The maximal\nexo/endo decomposition of any MDP is unique.\nùê¥\nùëÜ2\nùëÜ3\nùëÜ2\n‚Ä≤\nùëÜ3\n‚Ä≤\nùëÜ1\nùëÜ1\n‚Ä≤\nFigure 4: State transition diagram of MDP with 3 state variables.\n5\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nùê¥\nùëã\nùúã\nùê∏\nùëã‚Ä≤\nùê∏‚Ä≤\nFigure 5: The Diachronic Exogenous State MDP.\nProof. By contradiction. Suppose that there are two distinct maximal decompositions (E1, X1) and (E2, X2). By\nTheorem 2, their union would be a full exo/endo decomposition with an exo state set of higher cardinality. This\ncontradicts the assumption that (E1, X1) and (E2, X2) were maximal.\nCorollary 2 (Containment for Maximal Exo/Endo Decomposition). For the maximal exo/endo decomposition\n(Em, Xm), it holds that Xm ‚äáX, where X is the exo set of any full exo/endo decomposition (E, X).\nProof. By contradiction. Suppose there exists a decomposition (E, X) so that X Ã∏‚äÜXm. By Theorem 2, we could take\nthe union of (E, X) and (Em, Xm) to get a new full exo/endo decomposition with exo set X ‚à™Xm ‚äÉX of higher\ncardinality than X.\nIn our work, we speciÔ¨Åcally focus on MDPs with exogenous state variables that match the full exogenous DBN.\nDeÔ¨Ånition 3 (Exogenous State MDP). Any MDP whose structure matches Figure 2 or any subset of its edges is called\nan exogenous state MDP with exo set X.\nWe will analyze two types of exogenous state MDPs: The full setting shown in Figure 2 and the diachronic setting\nwhere the edge X‚Ä≤ ‚ÜíE‚Ä≤ is removed, as shown in Figure 5. Note that in the full setting, the MDP dynamics can be\nfactored as follows\nP(E‚Ä≤, X‚Ä≤ | E, X, A) = P(X‚Ä≤ | X) ¬∑ P(E‚Ä≤ | E, X, A, X‚Ä≤).\n(2)\nSimilarly, in the diachronic setting, the MDP dynamics can be factored as\nP(E‚Ä≤, X‚Ä≤ | E, X, A) = P(X‚Ä≤ | X) ¬∑ P(E‚Ä≤ | E, X, A).\n(3)\nIt is useful to draw a distinction between state variables that are causally exogenous and state variables that are\nstatistically exogenous.\nDeÔ¨Ånition 4 (Statistically-Exogenous Variables). A set of state variables X = S[I] is statistically exogenous in MDP\nM if the transition probability distribution P(E‚Ä≤, X‚Ä≤|E, X, A) can be factored according to either (2) or (3).\nIn reinforcement learning, we typically start with an MDP for which we do not know anything about the causal graph\nbeyond the fact that it deÔ¨Ånes an MDP. However, by collecting data and Ô¨Åtting a model of the transition probabilities,\nwe may infer that the MDP dynamics satisfy (2) or (3). In such cases, we can conclude that X is statistically exogenous\nbut we cannot infer that X is causally exogenous. In Section 3.4, we will discuss additional assumptions required to\nconclude that a set of state variables that are statistically exogenous are also causally exogenous. In Appendix A, we\nprovide a structural characterization of exogenous state variables and MDPs with exogenous state variables, and show\nthat under certain conditions the maximal exo set contains all exogenous state variables, with the exception of some\nedge cases.\n2.2\nAdditive Reward Decomposition\nIn this paper, we identify and analyze a case where reinforcement learning can be accelerated even when the exogenous\nvariables are all relevant to the policy, the dynamics, and the reward. This case arises when the reward function can be\ndecomposed additively into two functions, Rexo, which only depends on X, and Rend, which can depend on both X\nand E.\nDeÔ¨Ånition 5 (Additively Decomposable Exogenous State MDP). An Additively\nDecomposable Exogenous State MDP is an Exogenous State MDP whose reward function can be decomposed into the\nsum of two terms\nR(x, e, a) = Rexo(x) + Rend(x, e, a),\n6\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nwhere Rexo :X 7‚ÜíP(R) is the exogenous reward function and Rend :E √ó X √ó A 7‚ÜíP(R) is the endogenous reward\nfunction. If the reward function is deÔ¨Åned as a distribution over S √ó A √ó S, we instead consider the decomposition\nR(x, e, a, x‚Ä≤, e‚Ä≤) = Rexo(x, x‚Ä≤) + Rend(x, e, a, x‚Ä≤, e‚Ä≤).\nLet mexo(x) and œÉ2\nexo(x) < ‚àûbe the mean and variance of the exogenous reward distribution in state x. Similarly, let\nmend(e, x, a) and œÉ2\nend(e, x, a) < ‚àûbe the mean and variance of the endogenous reward distribution for state-action\npair (e, x, a).\nTheorem 4. For any Additively Decomposable Exogenous State MDP with exo set X, the H-step Ô¨Ånite-horizon Bellman\noptimality equation can be decomposed into two separate equations, one for a Markov Reward Process involving only\nX and Rexo and the other for an MDP (the endo-MDP) involving only Rend\nV (e, x; h) = Vexo(x; h) + Vend(e, x; h)\n(4)\nVexo(x; h) = mexo(x) + Œ≥Ex‚Ä≤‚àºP (x‚Ä≤|x)[Vexo(x‚Ä≤; h ‚àí1)]\n(5)\nVend(e, x; h) = max\na\nmend(e, x, a) + Ex‚Ä≤‚àºP (x‚Ä≤|x);e‚Ä≤‚àºP (e‚Ä≤|e,x,a)[Vend(e‚Ä≤, x‚Ä≤; h ‚àí1)].\n(6)\nProof. We consider the diachronic setting; the full setting can be shown similarly by replacing P(e‚Ä≤ | x, e, a) by\nP(e‚Ä≤ | x, x‚Ä≤, e, a).\nProof by induction on the horizon H. Note that the expectations could be either sums (if S is discrete) or integrals (if S\nis continuous).\nBase case: H = 1; we take one action and terminate.\nV (e, x; 1) = mexo(x) + max\na\nmend(x, a).\nThe base case is established by setting Vexo(x; 1) = mexo(x) and Vend(e, x; 1) = maxa mend(e, x, a).\nRecursive case: H = h.\nV (e, x; h) = mexo(x) + max\na {mend(e, x, a) +\nEx‚Ä≤‚àºP (x‚Ä≤|x);e‚Ä≤‚àºP (e‚Ä≤|e,x,a)[Vexo(x‚Ä≤; h ‚àí1) + Vend(e‚Ä≤, x‚Ä≤; h ‚àí1)]}.\nDistribute the expectation over the sum in brackets and simplify. We obtain\nV (e, x; h) = mexo(x) + Œ≥Ex‚Ä≤‚àºP (x‚Ä≤|x)[Vexo(x‚Ä≤; h ‚àí1)] +\nmax\na {mend(e, x, a) + Œ≥Ex‚Ä≤‚àºP (x‚Ä≤|x);e‚Ä≤‚àºP (e‚Ä≤|e,x,a)[Vend(e‚Ä≤, x‚Ä≤; h ‚àí1)]}.\nThe result is established by setting\nVexo(x; h) = mexo(x) + Œ≥Ex‚Ä≤‚àºP (x‚Ä≤|x)[Vexo(x‚Ä≤; h ‚àí1)]\nVend(e, x; h) = max\na {mend(e, x, a) + Œ≥Ex‚Ä≤‚àºP (x‚Ä≤|x);e‚Ä≤‚àºP (e‚Ä≤|e,x,a)[Vend(e‚Ä≤, x‚Ä≤; h ‚àí1)]}.\nCorollary 3. Any optimal policy for the endo-MDP of Equation (6) is an optimal policy for the full exogenous state\nMDP.\nProof. Because Vexo(s; H) does not depend on the policy, the optimal policy can be computed simply by solving the\nendo-MDP.\nWe will refer to Equations (4), (5) and (6) as the Exo/Endo Decomposition of the full MDP. In the remainder of this\npaper, unless stated otherwise, we will work with the full setting because it is more general.\nBray (2019) proves a result similar to Theorem 4. He also identiÔ¨Åes conditions under which value iteration and\npolicy iteration for a fully-speciÔ¨Åed Endo-MDP can be accelerated by computing the eigenvector decomposition of the\nendogenous transition matrix. While such techniques are useful for MDP planning with a known transition matrix,\nwe do not know how to exploit them in reinforcement learning where the MDP is unknown. In other related work,\nMcGregor et al. (2017) show how to remove known exogenous state variables in order to accelerate the Model Free\nMonte Carlo algorithm [Fonteneau et al., 2012]. Their experiments obtain substantial improvements in policy evaluation\nand reinforcement learning.\n7\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n2.3\nVariance Analysis of the Exo/Endo Decomposition\nSuppose we are given the decomposition of the state space into exogenous and endogenous sets. Under what conditions\nwould reinforcement learning on the endogenous MDP be more efÔ¨Åcient than on the original MDP? To explore this\nquestion, let us consider the problem of estimating the value of a Ô¨Åxed policy œÄ in a given start state s0 via Monte Carlo\ntrials of length H. We will compare the sample complexity of estimating V œÄ(s0; H) on the full MDP to the sample\ncomplexity of estimating V œÄ\nend(s0; H) on the Endogenous MDP. Of course most RL algorithms must do more than\nsimply estimate V œÄ(s0; H) for Ô¨Åxed œÄ, but the difÔ¨Åculty of estimating V œÄ(s0; H) is closely related to the difÔ¨Åculty of\nÔ¨Åtting a value function approximator or estimating the gradient in a policy gradient method.\nDeÔ¨Åne BœÄ(s0; H) to be a random variable for the H-step cumulative discounted return of starting in state s0 and\nchoosing actions according to œÄ for H steps. The expected value of BœÄ(s0; H) is the value function V œÄ(s0; H). To\ncompute a Monte Carlo estimate of V œÄ(s0; H), we will generate N realizations b1, . . . , bN of BœÄ(s0; H) by executing\nN H-step trajectories in the MDP, each time starting in s0.\nTheorem 5. For any œµ > 0 and any 0 < Œ¥ < 1, let ÀÜV œÄ(s0; H) = N ‚àí1 PN\ni=0 bi be the Monte Carlo estimate of the\nexpected H-step return of policy œÄ starting in state s0. If\nN ‚â•Var[BœÄ(s0; H)]\nŒ¥œµ2\nthen\nP[| ÀÜV œÄ(s0; H) ‚àíV œÄ(s0; H)| > œµ] ‚â§Œ¥.\nProof. This is a simple application of the Chebychev Inequality,\nP(|X ‚àíE[X]| > œµ) ‚â§Var[X]\nœµ2\n,\nwith X = ÀÜV œÄ(s0; H). The variance of the mean of N iid random variables is the variance of any single variable divided\nby N. Hence Var[ ÀÜV œÄ(s0; H)] = Var[BœÄ(s0; H)]/N. To obtain the result, plug this into the Chebychev inequality,\nset the rhs equal to Œ¥, and solve for N.\nNow let us consider the Exo/Endo decomposition of the MDP. Let BœÄ\nexo(s0; H) denote the H-step return of the\nexogenous MRP and BœÄ\nend(s0; H) denote the return of the endogenous MDP. Then BœÄ\nx(s0; H) + BœÄ\ne (s0; H) is a\nrandom variable denoting the cumulative H-horizon discounted return of the original, full MDP. Let Var[BœÄ(s0; H))]\nbe the variance of BœÄ(s0; H) and Cov[BœÄ\nexo(s0; H), BœÄ\nend(s0; H)] be the covariance between the exogenous and\nendogenous returns.\nTheorem 6. The Chebychev upper bound on the number of Monte Carlo trials required to estimate\nV œÄ(s0; H) using the endogenous MDP will be reduced compared to the full MDP iff Var[BœÄ\nexo(s0; H)] >\n‚àí2 Cov[BœÄ\nexo(s0; H), BœÄ\nend(s0; H)].\nProof. From Theorem 5, we know that the sample size bound using the endogenous MDP will be less than the required\nsample size using the full MDP when Var[BœÄ\nend(s0; H)] < Var[BœÄ\nexo(s0; H) + BœÄ\nend(s0; H)]. The variance of the\nsum of two random variables is\nVar[BœÄ\nexo(s0; H) + BœÄ\nend(s0; H)] = Var[BœÄ\nexo(s0; H)] + Var[BœÄ\nend(s0; H)] +\n2Cov[BœÄ\nexo(s0; H), BœÄ\nend(s0; H)].\nHence,\nVar[BœÄ\nend(s0; H)] < Var[BœÄ\nexo(s0; H) + BœÄ\nend(s0; H)]\nif and only if\nVar[BœÄ\nexo(s0; H)] > ‚àí2Cov[BœÄ\nexo(s0; H), BœÄ\nend(s0; H)].\nTo evaluate this covariance condition, we need to compute the variance and covariance of the H-step returns. In\nAppendix C, we derive dynamic programming formulas for these quantities. The dynamic programs allow us to check\nthe covariance condition of Theorem 6 in every state, including the start state s0, so that we can decide whether to solve\nthe original MDP or the endo-MDP. Some special cases are easy to verify. For example, if the mean exogenous reward\nmexo(s) = 0, for all states, then the covariance condition reduces to œÉ2\nexo(s) > 0.\n8\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nAlgorithm 1 Practical RL with the Exo/Endo Decomposition\n1: Inputs: Decomposition steps L, Exogenous reward update steps M, Policy update steps K, Total steps N\n2: Datasets: RL training tuples D, Exogenous reward examples Dexo\n3: //Phase 1\n4: Initialize policy and/or value networks randomly\n5: for i = 1 to L do\n6:\nRun RL to collect a new transition ‚ü®si, ai, ri, s‚Ä≤\ni‚ü©and add it to D\n7:\nif i mod K == 0 then\n8:\nUpdate the policy using last K observations with observed rewards ri\n9:\nend if\n10: end for\n11: Run a state decomposition algorithm (see Section 4) on D to compute the exogenous state mapping Œæexo\n12: Let Dexo{(Œæexo(si), ri)}L\ni=1 be the exogenous reward training set\n13: Fit the exogenous reward function ÀÜmexo to Dexo\n14: Update D by replacing each tuple ‚ü®si, ai, ri, s‚Ä≤\ni‚ü©by ‚ü®si, ai, ri ‚àíÀÜmexo(Œæexo(si)), s‚Ä≤\ni‚ü©\n15: //Phase 2\n16: for i = L + 1 to N do\n17:\nRun RL to collect a new transition ‚ü®si, ai, ri, s‚Ä≤\ni‚ü©\n18:\nAdd (Œæexo(si), ri) to Dexo\n19:\nAdd ‚ü®si, ai, ri ‚àíÀÜmexo(Œæexo(si)), s‚Ä≤\ni‚ü©to D\n20:\nif i mod K == 0 then\n21:\nUpdate the policy using last K observations in D\n22:\nend if\n23:\nif i mod M == 0 then\n24:\nUpdate the estimate of the exogenous reward function ÀÜmexo with the last M observations in Dexo\n25:\nend if\n26: end for\n3\nDecomposing an Exogenous State MDP: Optimization Formulations\nWe now turn our attention to discovering the exogenous variables (or exogenous subspace) of an MDP from data\ncollected during exploration. Our overall strategy is shown in Algorithm 1. We assume we are applying an online\nreinforcement learning algorithm, such as PPO or Q-learning. As it interacts with the environment, it collects\n‚ü®st, at, rt, st+1‚ü©experience tuples into a dataset D. After L tuples have been collected, we apply an exogenous space\ndiscovery algorithm (see Section 4) to Ô¨Ånd a function Œæexo, so that xt = Œæexo(st) computes the exogenous state xt\nfrom state st. By applying Œæexo to each st of the experience tuples, we assemble a supervised training set Dexo of the\nform {(xt, rt)}. We then solve a regression problem to predict as much of the reward rt from xt as possible. The\nresulting Ô¨Åtted function ÀÜmexo is our estimate of the mean of the exogenous reward function. Because of the additive\ndecomposition, we can therefore estimate the endogenous rewards as ÀÜrend,t := rt ‚àíÀÜmexo(st). We then convert the\nset of experience tuples into modiÔ¨Åed tuples ‚ü®st, at, ÀÜrend,t, st+1‚ü©by replacing the original reward rt values with our\nestimate of the endogenous reward and resume running the online reinforcement learning algorithm. Depending on the\nalgorithm, we may need to re-initialize the data structures using the modiÔ¨Åed experience tuples. In any case, as the\nalgorithm collects additional full experience tuples ‚ü®st, at, rt, st+1‚ü©, each is converted to an experience tuple for the\nendo-MDP by replacing rt by ÀÜrend,t. In our experiments, we Ô¨Ånd that there is some beneÔ¨Åt to repeating the reward\nregression at regular intervals, so we also add (Œæexo(si), ri) to Dexo at each time step. However, we do not observe\nsimilar beneÔ¨Åts from rerunning the exogenous space discovery algorithm, especially when the state transition function\nis linear, so we only execute it once.\nThe heart of our approach is the exogenous state discovery problem. This is easiest to discuss for the case where we\nare given a Ô¨Åxed set of state variables and our goal is to determine which state variables are exogenous and which\nare endogenous. Within this variable selection setting, we formulate the discovery problem as one of Ô¨Ånding a set of\nvariables that minimizes the residuals of the Ô¨Åtted exogenous reward function ÀÜmexo subject to the constraint that the\nvariables are exogenous. We formulate the exogeneity constraint in terms of conditional mutual information for both\nthe full and the diachronic DBN structures. Then we present two variations of the optimization formulation. The Ô¨Årst\nvariation solves the discovery problem jointly by selecting the exogenous features that minimize the residuals of the\nexogenous reward regression. The second variation takes a hierarchical approach in which we Ô¨Årst select the maximal\nexogenous subspace and then perform the reward regression. We will show that this two-phase algorithm Ô¨Ånds the same\n9\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nsolution as the joint optimization. This is the approach adopted in Algorithm 1, because it allows us to repeat the reward\nregressions without recomputing the exogenous subspace.\nAfter presenting these ideas in the context of exogenous variable selection, we then consider the most general formulation\nin which the exo-endo decomposition is deÔ¨Åned by a diffeomorphism that maps from a continuous state space to a\nspace that separates the exogenous and endogenous subspaces.\nFinally, we study a practical special case in which the exogenous and endogenous spaces are deÔ¨Åned by a linear mapping.\nIn this setting, we show how to approximate the mutual information constraints by constraints on the conditional\ncorrelation coefÔ¨Åcients. In Section 4, we will then present two algorithms for solving the linear formulation.\n3.1\nVariable Selection Formulation\nThe variable selection formulation aims to discover an index set I with the following two properties:\n‚Ä¢ I decomposes the set of state variables S = √ód\ni=1Si into two disjoint sets X = √ói‚ààISi and E = √ói‚ààIcSi\nthat satisfy the structure of Figure 2 or Figure 5.\n‚Ä¢ The squared error of the exogenous reward regression, P\nt[ ÀÜmexo(xt) ‚àírt]2 is minimized, where ÀÜmexo\nregresses rt onto X = √ói‚ààISi.\nHow can we state the Ô¨Årst property in a form suitable for constrained optimization? We can express the required\nstructure in terms of two conditional mutual information (CMI) constraints. The full setting can be enforced by the\nconstraint\nI(X‚Ä≤; [E, A] | X) = 0,\n(7)\nwhere X = S[I] and E = S[IC]. This says that if we know the value of X, then the combination of the endogenous\nstate variables and the action carries no additional information about X‚Ä≤, the value of the exogenous state in the next\ntime step. Note that this does not constrain the synchronic link X‚Ä≤ ‚ÜíE‚Ä≤.\nTo remove the synchronic link and enforce the diachronic structure, we can strengthen the conditional mutual information\nconstraint as follows\nI(X‚Ä≤; [E, A, E‚Ä≤] | X) = 0\n(8)\nThis says that given the exogenous state X, X‚Ä≤ carries no information about E, A, or E‚Ä≤.\nNote that our discovery algorithms will be evaluating these conditional mutual information constraints on the Ô¨Ånite\nsample of tuples collected from exploring the MDP. Below, we will discuss the conditions that must be satisÔ¨Åed by the\nMDP and the exploration policy to ensure the soundness of these conditional mutual information computations.\n3.1.1\nCoupled Formulation\nWe can now formalize the exogenous space discovery problem for the full setting as follows\nI‚àó, ÀÜm‚àó\nexo =\narg min\nI‚äÜ[d], ÀÜmexo:X7‚ÜíR\nE[( ÀÜmexo(X) ‚àíR)2]\nsubject to I(X‚Ä≤; [E, A] | X) = 0, X = S[I], E = S[Ic].\n(9)\nFor the diachronic setting, we must use the stronger CMI constraint of Equation (8) instead.\n3.1.2\nHierarchical Formulation\nFormulation (9) is coupled‚Äîthat is, it simultaneously involves the exo/endo state decomposition and the exogenous\nreward regression. The following gives an alternative hierarchical formulation that decouples the optimization of the\nstate representation from the reward regression\nÀÜm‚àó\nexo =\narg min\nÀÜmexo:X ‚àó7‚ÜíR\nE[( ÀÜmexo(X) ‚àíR)2], where X‚àó= S[I‚àó] and\n(10)\nI‚àó= arg max\nI‚äÜ[d]\n|I|\n(11)\nsubject to I(X‚Ä≤; [E, A] | X) = 0, X = S[I], E = S[Ic].\n(12)\nFormulation (10)-(12) breaks the coupled problem into two subproblems: Subproblem (11)-(12) computes the maximal\nexo/endo decomposition, while subproblem (10) computes the best exogenous reward estimator corresponding to the\n10\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\npreviously-computed exo/endo decomposition. The formulation is hierarchical, since we Ô¨Årst compute the exo/endo\nstate representation and subsequently Ô¨Åt the exogenous reward function. Our ultimate goal is still to solve the coupled\nformulation; the hierarchical formulation serves as a computationally simpler proxy. How do the two formulations\nrelate? Let Icoupled be the solution to the coupled formulation and Ihier be the solution to the hierarchical formulation.\nThe containment property of Corollary 2 implies that Icoupled ‚äÜIhier, since the maximal exo set of (11)-(12) contains\nthe exo set of any other exo/endo decomposition, including the endo/exo decomposition of (9). It is possible that\nIcoupled ‚äÇIhier if there are exogenous variables that carry no information about the exogenous reward function Rexo.\nThe hierarchical formulation will include those variables in the reward regression, and this may increase the variance\nof the estimated ÀÜmexo, because the regression will need to Ô¨Åt zero-valued weights for these irrelevant variables. The\nvariance can be addressed in the usual way by employing L1 regularization when Ô¨Åtting ÀÜmexo. Hence in practice (and\nin the limit of inÔ¨Ånite data), the two formulations will produce the same solution.\n3.2\nContinuous Formulation\nIn the continuous formulation, we assume that the state space S ‚äÜRd is an open subset U ‚äÜRd. Each s ‚ààS is thus\na d-dimensional real-valued vector. The discovery problem is to Ô¨Ånd a mapping Œæ : U 7‚ÜíV where V ‚äÜRd is also\nan open set such that the endogenous and exogenous state spaces can be readily extracted. Without loss of generality,\nwe stipulate that the Ô¨Årst dexo components in Œæ(S) deÔ¨Åne the exogenous state, while the remaining dend = d ‚àídexo\ncomponents deÔ¨Åne the endogenous state. Hence, I = [dexo], so that X = Œæ(S)[I] and E = Œæ(S)[Ic].\nWhat conditions must the mapping Œæ satisfy? It is desirable that the mapping preserve probability densities so that the\nresulting exo/endo MDP is equivalent to the original MDP. A natural choice is the class of diffeomorphisms [Tu, 2010].\nA diffeomorphism from an open subset U ‚äÜRd to an open subset V ‚äÜRd is deÔ¨Åned as a bijective map Œæ : U 7‚ÜíV so\nthat both Œæ and its inverse Œæ‚àí1 are continuously differentiable. We denote the class of such diffeomorphisms from U to\nany possible open set V as Xd\nU.\nDiffeomorphisms have several important properties. From an information theoretic point of view, they do not lose\nany information about the original random variables. In particular, by using the change of variables formula, we can\nanalytically write the probability density function of the transformation Œæ using the probability density of S and the\nJacobian of the transformation. Assuming the density of S is pS(s‚Ä≤ | s, a), the density pZ(z‚Ä≤ | z, a) for the transformed\nvariables Z = f(S) is given by\npZ(z‚Ä≤ | z, a) = pS(f ‚àí1(z‚Ä≤) | f ‚àí1(z), a) ¬∑ | det(Jf ‚àí1(z‚Ä≤))|,\nwhere Jf ‚àí1(z‚Ä≤) is the Jacobian matrix of the inverse transformation f ‚àí1 and det(¬∑) is the determinant. In particular,\nR\nU pS(s‚Ä≤ | s, a)ds‚Ä≤ =\nR\nV pZ(z‚Ä≤ | z, a)dz‚Ä≤ = 1. Hence, the original MDP deÔ¨Åned on S can be transformed into an\nequivalent MDP deÔ¨Åned on the transformation Œæ(S). An implication of the above is that mutual information between\ntwo random vectors is preserved under diffeomorphisms [Kraskov et al., 2004]. This result can be extended to the\nconditional mutual information. As a result, independence and conditional independence are also preserved under\ndiffeomorphisms, as they correspond to the special case of zero mutual or conditional mutual information.\nWith these assumptions, the coupled formulation takes the following form\nŒæ‚àó, ÀÜm‚àó\nexo, d‚àó\nexo =\narg min\nŒæ‚ààXd\nS, ÀÜmexo:X7‚ÜíR,dexo‚àà{0,...,d}\nE[( ÀÜmexo(X) ‚àíR)2]\nsubject to I(X‚Ä≤; [E, A] | X) = 0,\nwhere I = [dexo], X = Œæ(S)[I], and E = Œæ(S)[Ic].\n(13)\nThis formulation jointly optimizes the diffeomorphic state transformation and the exogenous reward function, so that\nthe latter best Ô¨Åts the observed reward. Notice that the optimal dexo is unknown, so the formulation for the optimal\ndecomposition must consider all possible values for dexo.\nSimilarly, we can deÔ¨Åne the hierarchical formulation\nÀÜm‚àó\nexo = arg min\nÀÜmexo:X ‚àó7‚ÜíR\nE[( ÀÜmexo(X‚àó) ‚àíR)2], where X ‚àó= S[I‚àó], I‚àó= [d‚àó\nexo], and\n(14)\nŒæ‚àó, d‚àó\nexo =\narg max\nŒæ‚ààXd\nS,dexo‚àà{0,...,d}\ndexo\n(15)\nsubject to I(X‚Ä≤; [E, A] | X) = 0,\n(16)\nwhere I = [dexo], X = Œæ(S)[I], and E = Œæ(S)[Ic].\n(17)\nEquations (15)-(17) Ô¨Årst compute the diffeomorphic state transformation so that the corresponding exo/endo decompo-\nsition is valid and has the highest possible number of exogenous state variables. Equation (14) subsequently estimates\nthe exogenous reward function.\n11\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nAs before, we can replace the CMI constraint in these formulations with the stronger version from Equation (8) to\nenforce the diachronic DBN structure.\nThe diffeomorphic formulation provides a general framework for continuous state variables. However, diffeomorphic\ntransformations do not usually accept a simple parameterization. They can in principle be represented by invertible\nneural networks [Ardizzone et al., 2019, Behrmann et al., 2021] or normalizing Ô¨Çows [Rezende and Mohamed, 2015,\nKobyzev et al., 2021]. Particular care must be taken to ensure that the corresponding mappings are not just bijective\nbut also continuously differentiable with a continuously differentiable inverse. This can be very restrictive in practice.\nRecent work by Koenen et al. [2021] relaxes the deÔ¨Ånition of a diffeomorphism to allow for mappings that are bijective\nand continuously differentiable almost everywhere, with a set of critical points of Lebesgue measure 0. The new\nclass of transformations, named Lebesgue-diffeomorphisms or L-diffeomorphisms, is consistent with several types of\nnormalizing Ô¨Çows and can thus replace the more rigid class of diffeomorphisms.\n3.3\nLinear Formulation\nWe now introduce a tractable formulation for the general class of continuous diffeomorphisms from Section 3.2.\nConcretely, we consider the general linear group GL(d, R) of invertible linear transformations in vector space Rd.\nInstead of directly deÔ¨Åning the full state transformation Œæ, we start by deÔ¨Åning a linear mapping Œæexo from the full state\nspace to the exogenous state space. We then deÔ¨Åne the endogenous state space as its orthogonal complement. Let Œæexo\nbe speciÔ¨Åed by a matrix Wexo ‚ààRd√ódexo with 0 ‚â§dexo ‚â§d, where dexo is the dimension of subspace X. Given a\nstate s, its projected exogenous state is W ‚ä§\nexo ¬∑ s. The endogenous subspace is the orthogonal complement of X of\ndimension dend = d ‚àídexo, written E = X ‚ä•and deÔ¨Åned by some matrix Wend. The endogenous state e contains the\ncomponents of s in subspace E. In the linear setting, E and X are vector subspaces of vector space S, and we can write\nS = E ‚äïX, with dim(S) = dim(E) + dim(X). We will use the notation\nS\nW= [E, X],\nto denote the state-space decomposition deÔ¨Åned by W = (Wexo, Wend). The matrix W is invertible with rank d, and\nhence, deÔ¨Ånes a linear diffeomorphism.\nLet P ‚ààP(S) denote a probability distribution over the state space. We can map this distribution into a distribution ÀúP\nin the decomposed state space through the change of variables formula:\nÀúP(X = x, E = e) =\n1\n| det(W)|P(S = W ‚àí1[x, e]).\n(18)\nThe marginal distributions ÀúP(X) and ÀúP(E) can be computed by marginalizing in the original space. For example,\nÀúP(X = x) =\n1\n| det(W)|\nZ\ne\nP(S = W ‚àí1[x, e])de.\nIn the remainder of the paper, we will simply write P(X) and P(E) for the probability distributions in the transformed\nspace.\nFor computational convenience, we impose the requirement that the columns of Wexo consist of orthonormal vectors,\nso that W ‚ä§\nexo ¬∑ Wexo = Idexo. Linear decomposition theory [Friedberg et al., 2002] then tells us that the exogenous state\ncan be represented in the original d-dimensional basis as x = Œæexo(s) = Wexo ¬∑ W ‚ä§\nexo ¬∑ s, and the endogenous state\ne = Œæend(s) = s ‚àíx = s ‚àíWexo ¬∑ W ‚ä§\nexo ¬∑ s. Under this approach, Œæexo(s) + Œæend(s) = x + e = s. Every state s ‚ààS\ncan be written uniquely in this way (i.e., this is a direct sum) for a given exogenous projection matrix Wexo. Of course,\nlike Wexo, the endogenous matrix Wend can also be expressed using orthonormal components.\nBecause diffeomorphisms preserve mutual information [Kraskov et al., 2004], the conditional mutual information\nconstraint holds in the mapped state space\nI(Wexo ¬∑ S‚Ä≤; [Wend ¬∑ S, A] | Wexo ¬∑ S) = 0\nif and only if it holds in the original space,\nI(X‚Ä≤; [E, A] | X) = 0.\nNote that it is possible for two different matrices Wexo and W ‚Ä≤\nexo to satisfy the conditional mutual information constraint\nif they are related by an invertible matrix U ‚ààRdexo√ódexo as Wexo = W ‚Ä≤\nexo ¬∑ U (and similarly for Wend). Hence, when\nwe solve the coupled (Equation (13)) or hierarchical (Equations (14)-(17)) formulations, the exogenous subspace is only\n12\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nidentiÔ¨Åed up to multiplication by an invertible matrix U. This can make it difÔ¨Åcult to interpret the discovered subspace.\nOne suggestion is to put Mexo into block-diagonal canonical form, so that to the extent possible, the coordinates of the\nmapped space are the same as in the original space.\nTo develop the linear formulations of the discovery problem, let us consider the database D of {(si, ai, ri, s‚Ä≤\ni)}N\ni=1\nsample transitions collected in Algorithm 1. We start by centering si and s‚Ä≤\ni by subtracting off the mean of the observed\nstates. Let S ‚ààRN√ód, A ‚ààRN√ók, R ‚ààRN√ó1, S‚Ä≤ ‚ààRN√ód be the matrices containing the observations of si, ai, ri,\nand s‚Ä≤\ni, respectively. These are samples from the random variables S, A, R, S‚Ä≤, and we will estimate the expectations\nrequired in the optimization formulations (expected squared reward regression error and CMI values) from these\nsamples. Given observation matrices S, S‚Ä≤, we can write the corresponding exogenous and endogenous states as\nX = S ¬∑ Wexo, X‚Ä≤ = S‚Ä≤ ¬∑ Wexo, E = S ‚àíS ¬∑ Wexo ¬∑ W ‚ä§\nexo, and E‚Ä≤ = S‚Ä≤ ‚àíS‚Ä≤ ¬∑ Wexo ¬∑ W ‚ä§\nexo.\nThe exogenous reward regression problem takes a particularly simple form if we adopt linear regression. Let ÀÜX =\nS ¬∑ Wexo be the matrix of estimated exogenous state vectors, and let w‚àó\nR be the Ô¨Åtted coefÔ¨Åcients of the linear regression.\nThis coefÔ¨Åcient vector can be computed as the solution of the usual least squares problem\nw‚àó\nR = arg min\nwR‚ààRdexo\n‚à•ÀÜX ¬∑ wR ‚àíR‚à•2\n2 = arg min\nwR‚ààRdexo\n{( ÀÜX ¬∑ wR ‚àíR)‚ä§¬∑ ( ÀÜX ¬∑ wR ‚àíR)} = ( ÀÜX‚ä§ÀÜX)‚àí1 ÀÜX‚ä§R.\n(19)\nThis gives us the optimization objective for the linear formulation. Now let us consider how to express the conditional\nmutual information constraints.\nEstimating mutual information (and conditional mutual information) has been studied extensively in machine learning.\nRecent work exploits variational bounds [Donsker and Varadhan, 1983, Nguyen et al., 2007, Nowozin et al., 2016,\nBarber and Agakov, 2003, Blei et al., 2017] to enable differentiable end-to-end estimation of mutual information with\ndeep nets [Belghazi et al., 2018, Poole et al., 2019, Alemi et al., 2018, Hjelm et al., 2019, van den Oord et al., 2018].\nDespite their promise, mutual information estimation by maximizing variational lower bounds is challenging due\nto inherent statistical limitations [McAllester and Stratos, 2020]. Alternative approaches for estimating the mutual\ninformation include k-nearest neighbors [Kraskov et al., 2004], ensemble estimation [Moon et al., 2017], jackknife\nestimation [Zeng et al., 2018], kernel density estimation [Kandasamy et al., 2015, Han et al., 2020], and Gaussian copula\nmethods [Singh and P¬¥oczos, 2017]. All of these require substantial computation, and some of them also require delicate\nhyperparameter tuning. Extending them to estimate conditional mutual information raises additional challenges.\nWe have chosen instead to replace conditional mutual information with a quantity we call the conditional correlation\ncoefÔ¨Åcient (CCC). To motivate the CCC, assume that variables X, Y, Z are distributed according to a multivariate\nGaussian distribution. In this case, it is known [Baba et al., 2004] that X and Y are conditionally independent given Z,\nif and only if\nŒ£XY ‚àíŒ£Y ZŒ£‚àí1\nZZŒ£XZ = 0,\nwhere Œ£AA is the covariance matrix of A and Œ£AB is the cross-covariance matrix of A and B. We can normalize the\nabove expression to obtain the normalized cross-covariance matrix\nV (X, Y, Z) = Œ£‚àí1/2\nXX (Œ£XY ‚àíŒ£XZŒ£‚àí1\nZZŒ£ZY )Œ£‚àí1/2\nY Y\n= 0.\n(20)\nIt is not hard to see that Equation (20) holds if and only if\ntr(V ‚ä§(X, Y, Z) ¬∑ V (X, Y, Z)) = 0,\nwhere tr(¬∑) is the trace function. We call the quantity tr(V ‚ä§(X, Y, Z) ¬∑ V (X, Y, Z)) the conditional correlation\ncoefÔ¨Åcient (CCC), and we denote it by CCC(X, Y | Z).2 Because (20) involves matrix inversion, we apply Tikhonov\nregularization [Tikhonov and Arsenin, 1977] to all inverse matrices with a small positive constant Œª > 0 for numerical\nstability. For instance, Œ£‚àí1/2\nXX becomes (Œ£XX + Œª ¬∑ In)‚àí1/2, where n is the size of random vector X.\nOf course, the equivalence of zero CCC and conditional independence does not necessarily hold outside of the\nmultivariate Gaussian distribution. A more general approach, which inspired our CCC method, would be to employ\nkernel measures of conditional independence [Fukumizu et al., 2004, 2008]. These deÔ¨Åne normalized cross-covariance\noperators on reproducing kernel Hilbert spaces (RKHS) and establish that conditional independence holds if and only\nif the Hilbert-Schmidt norm of the operator is 0. Unfortunately, mapping our data into an RKHS would make the\nexogenous space discovery problem hard to optimize in terms of the underlying projection matrix Wexo. Instead, we\ndirectly optimize over the linearly projected data. Our experiments in Section 5 demonstrate that, despite being much\nsimpler than kernel measures of conditional independence, our method can still provide useful results in MDPs with\nnonlinear dynamics.\n2In Dietterich et al. [2018], we referred to this quantity as the Partial Correlation CoefÔ¨Åcient (PCC), but this was an error. While\nthe PCC can be used to determine conditional independence for Gaussians, it is a different quantity.\n13\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nWe can now express the coupled formulation of the exogenous subspace discovery problem for the full setting as follows\nmin\n0‚â§dexo‚â§d,Wexo‚ààRd√ódexo,wR‚ààRdexo ‚à•S ¬∑ Wexo ¬∑ wR ‚àíR‚à•2\n2\nsubject to W ‚ä§\nexoWexo = Idexo\nCCC(S‚Ä≤Wexo; [S ‚àíSWexoW ‚ä§\nexo, A] | SWexo) < œµ.\n(21)\nNote that we have relaxed the CCC constraint slightly to allow non-zero values less than a small constant œµ.\nTo obtain a linear coupled formulation for the diachronic case, let\nY (Wexo) = [S ‚àíSWexoW ‚ä§\nexo, S‚Ä≤ ‚àíS‚Ä≤WexoW ‚ä§\nexo, A].\nThis creates a stacked matrix corresponding to [E, E‚Ä≤, A]. We can then write the CCC constraint for the diachronic\ncase as\nCCC(S‚Ä≤Wexo; Y (Wexo) | SWexo) = 0.\nThis is the CCC equivalent of the diachronic CMI constraint of Equation (8).\nThe linear hierarchical formulation for the full case can be written as\nmin\nwR‚ààRd‚àóexo ‚à•S ¬∑ W ‚àó\nexo ¬∑ wR ‚àíR‚à•2\n2\nwhere\nd‚àó\nexo, W ‚àó\nexo =\narg max\ndexo‚àà{0,...,d},Wexo‚ààRd√ódexo\ndexo\nsubject to W ‚ä§\nexoWexo = Idexo\nCCC(S‚Ä≤Wexo; [S ‚àíSWexoW ‚ä§\nexo, A] | SWexo) < œµ.\n(22)\nAlthough we have written the outer objective in terms of linear regression, this is not essential. Once the optimal linear\nprojection matrix W ‚àó\nexo ‚ààRd√ódexo has been determined and the reward regression dataset Dexo has been constructed,\nany form of regression‚Äîincluding nonlinear neural network regression‚Äîcan be employed.\n3.4\nConditions Establishing Sound Inference\nWhat conditions must hold so that the coupled or hierarchical optimization discovery formulations, when applied\nto the collected data D, Ô¨Ånd valid exo/endo decompositions? In this section, we address this question for ‚Äútabular‚Äù\nMDPs‚Äîthat is, MDPs with Ô¨Ånite, discrete state and action spaces. To Ô¨Ånd valid exo/endo decompositions, we need\nto ensure that the factorizations of Equations (2) or (3) hold in all states of the MDP. This means that our exploration\npolicy needs to visit all states, and it needs to execute all possible actions in each state to verify that the action does not\naffect (either directly or indirectly) the exogenous variables. We formalize this as follows.\nConsider an idealized version of Algorithm 1 that collects the tuple dataset D by executing a Ô¨Åxed exploration policy\nœÄx for a large number of steps. We will require that œÄx is fully randomized, according to the following deÔ¨Ånition:\nDeÔ¨Ånition 6 (Fully Randomized Policy). An exploration policy œÄx is fully randomized for a tabular MDP with action\nspace A if œÄx assigns non-zero probability to every possible action a ‚ààA in every state s ‚ààS.\nWe will also require that the structure of the MDP is such that a fully-randomized policy will visit every state s ‚ààS\ninÔ¨Ånitely often.\nDeÔ¨Ånition 7 (Admissible MDP). A tabular MDP is admissible if any fully-randomized policy will visit every state in\nthe MDP inÔ¨Ånitely often.\nExamples of admissible MDPs include episodic MDPs and ergodic MDPs. An episodic MDP begins each episode in a\nÔ¨Åxed start state s0 and executes a policy until a terminal state is reached. Then it resets to the starting state. It must\nsatisfy the requirement that all policies will reach a terminal state in a Ô¨Ånite number of steps. The simplest episodic\nMDP always terminates after a Ô¨Åxed number of steps H, which is called the horizon time of the MDP. Note that if an\nepisodic MDP contains states that are not reachable from the start state s0 by any policy, then these must be deleted\nfrom the MDP in order to satisfy the deÔ¨Ånition of admissibility.\nAn ergodic MDP has the property that for all policies, every state is reachable from every other state in a Ô¨Ånite number\nof steps, and the time between successive visits to any given state is aperiodic.\n14\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nTheorem 7 (Soundness of Empirical Conditional Mutual Information). Let M be an admissible MDP, and let\nD be a set of ‚ü®s, a, r, s‚Ä≤‚ü©tuples collected by executing fully-randomized policy œÄx for n steps and recording the\nstate, action, reward, and result state at each step. Let (X, E) be a proposed exo/endo decomposition of S. Let\nÀÜP(E, X, A, E‚Ä≤, X‚Ä≤) be the maximum-likelihood estimate of the joint distribution of the decomposed (S, A, S‚Ä≤) triples,\nand let ÀÜI(X‚Ä≤; E, A, E‚Ä≤|X) be the corresponding estimate of the conditional mutual information for the diachronic form.\nIf limn‚Üí‚àûÀÜI(X‚Ä≤; E, A, E‚Ä≤|X) = 0 then (X, E) is a valid exo/endo decomposition of S.\nProof. From the structure of an MDP, we know that the joint distribution ÀÜP(E, A, E‚Ä≤X‚Ä≤) can be factored as\nÀÜP(E, X, A, E‚Ä≤X‚Ä≤) = ÀÜP(E, X) ÀÜP(A|E, X) ÀÜP(E‚Ä≤, X‚Ä≤|E, X, A).\nThe Ô¨Årst term is the empirical probability of visiting state S = (E, X), the second term is the empirical estimate of œÄx,\nand the third term is the estimated transition dynamics of the MDP. The empirical conditional mutual information can\nbe written as\nÀÜI(X‚Ä≤; E, A, E‚Ä≤|X) =\nX\nE,X,A,E‚Ä≤,X‚Ä≤\nÀÜP(E, X, A, E‚Ä≤, X‚Ä≤)\n\"\nlog\nÀÜP(X‚Ä≤, E, A, E‚Ä≤|X)\nÀÜP(X‚Ä≤|X) ÀÜP(E, A, E‚Ä≤|X)\n#\n.\nIf ÀÜI(X‚Ä≤; E, A, E‚Ä≤|X) = 0, then it is easy to show that the fraction inside the log must be equal to 1 for all\n(E, X, E‚Ä≤, X‚Ä≤, A) for which ÀÜP(E, X, E‚Ä≤, X‚Ä≤, A) > 0. As n gets large, this will hold for all possible state transi-\ntions. Hence,\nÀÜP(X‚Ä≤, E, A, E‚Ä≤|X) = ÀÜP(X‚Ä≤|X) ÀÜP(E, A, E‚Ä≤|X).\n(23)\nFrom the structure of the MDP, we know the left-hand side of (23) can be rewritten as\nÀÜP(X‚Ä≤, E, A, E‚Ä≤|X) = ÀÜP(X‚Ä≤, E‚Ä≤|E, X, A) ÀÜP(A|E, X) ÀÜP(E|X).\nBy applying the chain rule of conditional probability, we can rewrite the right-hand side of (23) as\nÀÜP(X‚Ä≤|X) ÀÜP(E, A, E‚Ä≤|X) = ÀÜP(X‚Ä≤|X) ÀÜP(E‚Ä≤|E, X, A) ÀÜP(A|E, X) ÀÜP(E|X).\nSubstituting these into (23) gives\nÀÜP(X‚Ä≤, E‚Ä≤|E, X, A) ÀÜP(A|E, X) ÀÜP(E|X) = ÀÜP(X‚Ä≤|X) ÀÜP(E‚Ä≤|E, X, A) ÀÜP(A|E, X) ÀÜP(E|X).\nBecause the MDP is admissible and œÄx is fully randomized, then for n sufÔ¨Åciently large,\nÀÜP(A|E, X) > 0\n‚àÄE, X, A\nÀÜP(E|X) > 0\n‚àÄE, X.\nHence, we can cancel them from both sides of the equation to obtain\nÀÜP(X‚Ä≤, E‚Ä≤|E, X, A) = ÀÜP(X‚Ä≤|X) ÀÜP(E‚Ä≤|E, X, A).\nAs n ‚Üí‚àû, all estimates will converge to their true values, and we will obtain\nP(X‚Ä≤, E‚Ä≤|E, X, A) = P(X‚Ä≤|X)P(E‚Ä≤|E, X, A),\n(24)\nwhich is the factorization for the diachronic Exogenous State MDP. This proves that (X, E) is a valid exo/endo\ndecomposition of the diachronic form.\nBecause we know the exploration policy œÄx, we could replace ÀÜP(A|E, X) by œÄx(A|E, X) throughout this derivation.\nNote also that P(E‚Ä≤, X‚Ä≤|E, X, A) will be zero for states S‚Ä≤ = (E‚Ä≤, X‚Ä≤) that cannot be directly reached from (E, X) by\nany action A. This is not a problem, because the conditional mutual information formula is only concerned with the\nterms where P(E, X, A, E‚Ä≤, X‚Ä≤) > 0.\nAn analogous theorem and proof can easily be provided for Exogenous State MDPs of the full form.\nFor purposes of computing a valid exo/endo decomposition that respects the full or diachronic factorization, this theorem\nsufÔ¨Åces. But it only shows that the variables in X are statistically exogenous. We might also wish to prove that they are\ncausally exogenous according to DeÔ¨Ånition 1. To apply this deÔ¨Ånition, we need a causal graph. To infer the structure of\na causal graph from observational data, we can make the following faithfulness assumption [Spirtes et al., 2000].\nAssumption 1 (Faithfulness). The set of conditional independencies exhibited by the observed distribution P(S‚Ä≤|S, A)\nis exactly the set of conditional independencies entailed by the causal graph of the MDP.\n15\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nCorollary 4. If the conditions of Theorem 7 hold, the underlying structural causal model has the structure of a\nstationary MDP, and the causal model satisÔ¨Åes the faithfulness assumption, then the set X satisÔ¨Åes the causal deÔ¨Ånition\nof exogeneity in DeÔ¨Ånition 1.\nProof. From the assumptions (faithfulness, MDP structure, and Theorem 7), we know that every probabilistic de-\npendency (and independency) implied by the structure of the causal graph will be exhibited by the observed data\nD.\nConsider the diachronic setting Ô¨Årst. Assume that any of the links E ‚ÜíX, A ‚ÜíX or E‚Ä≤ ‚ÜíX were present in the\ncausal graph G. In that case, variables E, A, E‚Ä≤ are not d-separated from X‚Ä≤ given X. But then faithfulness would\nimply that X‚Ä≤ Ã∏‚ä•‚ä•E, A, E‚Ä≤ | X, which is a contradiction. Hence the causal graph has the form of Figure 5.\nFor the full setting, we can similarly show the links A ‚ÜíX and E ‚ÜíX must be missing. What about link E‚Ä≤ ‚ÜíX‚Ä≤?\nAssume that such a link indeed exists in G. Consider any state variable E‚Ä≤\ni in set E‚Ä≤, so that the link E‚Ä≤\ni ‚ÜíX‚Ä≤ is\npresent. If either link A ‚ÜíE‚Ä≤\ni or E ‚ÜíE‚Ä≤\ni is present, then there is a directed path from A (or E) to X‚Ä≤ through E‚Ä≤\ni.\nBut in that case variables A (or E) and X‚Ä≤ are not d-separated given X, so the faithfulness assumption implies that\nX‚Ä≤ Ã∏‚ä•‚ä•A | X (or X‚Ä≤ Ã∏‚ä•‚ä•E | X), which is a contradiction. What if there is no link from A or E to E‚Ä≤\ni? In that case, we\ncan immediately see that both X and E‚Ä≤\ni are causally exogenous. Hence the causal graph has the form of Figure 2.\nIn summary, if the conditional mutual information I(X‚Ä≤; E, A, E‚Ä≤|X) is zero, then the factorization in (24) matches the\nstructure of the causal graph G, and the set X satisÔ¨Åes the causal deÔ¨Ånition of exogeneity.\nNote that if the faithfulness assumption does not hold, then the set X may contain variables that coincidentally satisfy\nthe factorization of (24) but are not causally exogenous. However, if X is a maximal set of exogenous variables, then\nwe know that it contains all of the causally exogenous variables (and possibly others).\nIn our experiments, we initialize our chosen reinforcement learning algorithm, PPO, to implement a fully-randomized\npolicy. However, as PPO is an on-policy RL algorithm, the policy gradually departs from full randomization, and we\nlose any guarantee that all states will be visited and all actions exercised. Consequently, the resulting exogenous sets X\nmay not be valid.\n4\nAlgorithms for Decomposing an MDP into Exogenous and Endogenous Components\nThis section introduces two practical algorithms for addressing the exogenous subspace discovery problem. The Ô¨Årst\nalgorithm, GRDS (Global Rank Descending Scheme), is based on the linear hierarchical formulation of Equation (22).\nIt initializes dexo to d and decreases dexo one dimension at a time until it can Ô¨Ånd a Wexo matrix whose CCC is near\nzero. We refer to it as a ‚Äúglobal‚Äù scheme, because it must solve a series of global manifold optimization problems. The\nsecond algorithm, SRAS (Stepwise Rank-Ascending Scheme), starts with dexo := 0 and constructs the Wexo matrix by\nadding one column at a time as long as it can keep CCC near zero. SRAS only needs to solve one-dimensional manifold\noptimization problems, so it has the potential to be faster.\n4.1\nGRDS: Global Rank Descending Scheme\nAlgorithm 2 gives the pseudo-code for the global rank descending scheme, GRDS. GRDS solves the inner objective\n(Equation (22)) by iterating from dexo := d down to zero. Instead of treating the CCC < œµ condition as a constraint,\nwe put CCC into the objective and minimize it (line 6). If the optimization Ô¨Ånds a Wexo with CCC < œµ, we know that\nthis gives the maximum value, d‚àó\nexo. Hence, we can halt and return Wexo as the solution.\nOne might hope that we could use a more efÔ¨Åcient search procedure, such as binary search, to Ô¨Ånd d‚àó\nexo. Unfortunately,\nbecause not all subsets of the maximal exogenous subspace are valid decompositions (Theorem 3), it is possible for an\nexogenous subset with ÀÜd < d‚àó\nexo to violate the CCC < œµ constraint.\nThe orthonormality constraint in the minimization (line 6) forces the weight matrix Wexo to lie on a Stiefel manifold\n[Stiefel, 1935]. Hence, line 6 seeks to minimize a function on a manifold, a problem to which we can apply familiar\ntools for Euclidean spaces such as gradient descent, steepest descent and conjugate gradient. Several optimization\nalgorithms exist for optimizing on Stiefel manifolds [Jiang and Dai, 2015, Absil et al., 2007, Edelman et al., 1999].\nManifold optimization on a Stiefel manifold has previously been considered by Bach and Jordan [2003] in the context\nof Independent Component Analysis, but in their case the linearly projected data are subsequently mapped to a\nReproducing Kernel Hilbert Space (RKHS).\n16\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nAlgorithm 2 GRDS: Global Rank-Descending Scheme\n1: Inputs: A database of transitions {(si, ai, s‚Ä≤\ni)}N\ni=1 provided as matrices S, A, S‚Ä≤\n2: Output: The exogenous state projection matrix W ‚àó\nexo\n3: for dexo = d down to 1 do\n4:\nSet\nY (Wexo) ‚Üê[S ‚àíSWexoW ‚ä§\nexo, S‚Ä≤ ‚àíS‚Ä≤WexoW ‚ä§\nexo, A] for the diachronic setting\n5:\nor\nY (Wexo) ‚Üê[S ‚àíSWexoW ‚ä§\nexo, A] for the full setting\n6:\nSolve the following optimization problem\nW ‚àó\nexo :=\narg min\nWexo‚ààRd√ódexo\nCCC(S‚Ä≤Wexo; Y (Wexo) | SWexo)\nsubject to W ‚ä§\nexoWexo = Idexo\n7:\nSet\nCCC ‚ÜêCCC(S‚Ä≤W ‚àó\nexo; Y (W ‚àó\nexo) | SW ‚àó\nexo)\n8:\nif CCC < œµ then\n9:\nreturn W ‚àó\nexo\n10:\nend if\n11: end for\n12: return null projection 0\n4.2\nAnalysis of the Global Rank-Descending Scheme\nIn this section, we study the properties of the global rank-descending scheme. We assume that we Ô¨Åt the exo reward\nfunction using linear regression (19), since this simpliÔ¨Åes our analysis. Directly analyzing Algorithm 2 is hard, because\nit (i) uses the CCC objective as a proxy for conditional independence, (ii) involves estimation errors due to having a\nÔ¨Ånite number of samples, and (iii) involves approximations in the optimization (both from numerical errors and from\nthe œµ threshold). To side-step these challenges, we consider an oracle variant of our setting, where we have access to\nthe true joint distribution P(S, A, S‚Ä≤) and perfect algorithms for solving all optimization problems (including the exo\nreward linear regression). Access to P(S, A, S‚Ä≤) is equivalent to having an inÔ¨Ånite training sample collected by visiting\nall states and executing all actions so that estimation errors vanish when computing the conditional mutual information\nand the expected value of the residual error in (19).\nUnder this oracle setting, we prove that the global rank-descending scheme returns the unique exogenous subspace of\nmaximum rank. In practice, if we have a sufÔ¨Åciently representative sample of ‚ü®s, a, s‚Ä≤, r‚ü©tuples and the CCC captures\nconditional independence reasonably well, we can hope that our methods will still give useful results.\nAlgorithm 3 shows the oracle version of GRDS. It is identical to GRDS except that the optimization step of minimizing\nthe CCC is replaced by the following feasibility problem:\nFind Wexo ‚ààRd√ódexo such that:\nW ‚ä§\nexoWexo = Idexo\nI(S‚Ä≤Wexo; [S ‚àíSWexoW ‚ä§\nexo, A] | SWexo) = 0.\n(25)\nTheorem 8. The Oracle-GRDS algorithm returns a matrix Wexo such that\n(a) the subspace X deÔ¨Åned by Wexo and the subspace E deÔ¨Åned as the orthogonal complement of X form a valid\nexo/endo decomposition of the full form;\n(b) the subspace X has maximal dimension over all valid exo/endo decompositions; and\n(c) the subspace X is unique and contains all other exogenous subspaces Àú\nX that could form valid exo/endo decomposi-\ntions.\nProof. To prove property (a), Ô¨Årst note that we can deÔ¨Åne the joint distribution P(X = WexoW ‚ä§\nexos, E = s ‚àí\nWexoW ‚ä§\nexos) = P(S = s), because each column of Wexo is a unit vector and they are orthogonal. Because Wexo is a\nfeasible solution to the manifold optimization Problem 25 and the conditional mutual information is zero, we know\nfrom Theorem 7 that P(S‚Ä≤|S, A) factors as P(X‚Ä≤|X)P(E‚Ä≤|X, E, A, X‚Ä≤). Hence, it is a valid exo/endo decomposition\naccording to Theorem 1.\n17\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nAlgorithm 3 Oracle-GRDS for the Full Setting\n1: Input: Joint distribution P(S, A, S‚Ä≤) corresponding a fully-randomized policy controlling an admissible exogenous\nstate MDP with maximal exogenous subspace X deÔ¨Åned by W ‚àó\nexo\n2: Output: Matrix ÀÜW ‚àó\nexo\n3: for dexo = d down to 1 do\n4:\nSolve the following system of equations for Wexo ‚ààRd,dexo:\nW ‚ä§\nexoWexo = Idexo\nI(S‚Ä≤Wexo; [S ‚àíSWexoW ‚ä§\nexo, A] | SWexo) = 0.\n5:\nif the above system is feasible with solution Wexo of rank dexo then\n6:\nÀÜW ‚àó\nexo := Wexo\n7:\nreturn ÀÜW ‚àó\nexo\n8:\nend if\n9: end for\n10: return null matrix 0\nProperty (b) follows from the fact that dexo is the largest value that yields a feasible solution to Problem 25.\nTo establish property (c), we need to prove three lemmas, which are the vector space versions of Theorem 2, Corollary 1,\nand Corollary 2.\nLemma 1 (Union of Exo/Endo Decompositions). Let [X1, E1] and [X2, E2] be two full exo/endo decompositions of an\nMDP M with state space S, where X1 = {W ‚ä§\n1 s : s ‚ààS} and X2 = {W ‚ä§\n2 s : s ‚ààS} and where W ‚ä§\n1 W1 = Id1√ód1\nand W ‚ä§\n2 W2 = Id2√ód2, 1 ‚â§d1, d2 ‚â§d. Let X = X1 + X2 be the subspace formed by the sum of subspaces X1 and X2,\nand let E be its complement. It then holds that the state decomposition S = [E, X] with E ‚ààE and X ‚ààX is a valid\nfull exo/endo decomposition of S.\nProof. We wish to follow the same reasoning as in Theorem 2. To do this, we need to compute the linear subspaces\nequivalent to X = X1 + X2 and E = E1 ‚à©E2. Consider the following subspaces expressed in the original d-dimensional\ncoordinate system:\nX = X1 ‚à©X2 = {W2W ‚ä§\n2 W1W ‚ä§\n1 s : s ‚ààRd}\n(26)\nÀÜ\nX1 = X1 ‚à©E2 = {s ‚ààX1 : W2W ‚ä§\n2 s = 0}\n(27)\nÀÜ\nX2 = X2 ‚à©E1 = {s ‚ààX2 : W1W ‚ä§\n1 s = 0}\n(28)\nX = X1 + X2 = X ‚äïÀÜ\nX1 ‚äïÀÜ\nX2\n(29)\nE = X ‚ä•= {s ‚ààRd : W1W ‚ä§\n1 s = 0 ‚àßW2W ‚ä§\n2 s = 0}\n(30)\nEquation (26) deÔ¨Ånes the intersection of the two subspaces X1 and X2, because W1W ‚ä§\n1 is a projection matrix that\nprojects s into X1, and W2W ‚ä§\n2 is a projection matrix that projects W1W ‚ä§\n1 s into X2. In Equation (27), we deÔ¨Åne ÀÜ\nX1 as\nthe set of points in X1 that are not in X2, because W2W ‚ä§\n2 maps them to zero. Hence, they are also not in the intersection\nX. This implies that X1 = X ‚äïÀÜ\nX1. Note that because E2 is deÔ¨Åned as the points that W2W ‚ä§\n2 maps to zero, ÀÜ\nX1 can\nalso be expressed as X1 ‚à©E2. Similarly, ÀÜ\nX2 = X2 ‚à©E1, and X2 = X ‚äïÀÜ\nX2. We can therefore express X1 + X2 as the\ndirect sum X ‚äïÀÜ\nX1 ‚äïÀÜ\nX2 (Equation (29)). Equation (30) deÔ¨Ånes E as the set of points that are simultaneously in the\northogonal complements of both X1 and X2. Either ÀÜ\nX1 or ÀÜ\nX2 may of course be the zero vector spaces, if one exogenous\nsubspace is contained in the other.\nNow we can deÔ¨Åne random variables that permit us to apply the proof of Theorem 2. DeÔ¨Åne random variables for the\ninput spaces: X1 ‚ààX1, E1 ‚ààE1, X2 ‚ààX2, for the (X, E) decomposition. Similarly, deÔ¨Åne random variables for the\noutput spaces: X = (X, ÀÜX1, ÀÜX2), where X ‚ààX, ÀÜX1 ‚ààÀÜ\nX1, and ÀÜX2 ‚ààÀÜ\nX2. Let S ‚ààS.\nBecause (X1, E1) is a valid decomposition, there can be no edges from E1 or A to any variable in X1. Similarly,\nbecause (X2, E2) is a valid decomposition, there can be no edges from E2 or A to any variable in X2. Consequently,\nthere can be no edges from E and A to any subspace in X. This demonstrates that (X, E) is a valid full exo/endo\ndecomposition of the state space.\nLemma 2 (Unique Maximal Subspace). The maximal exo vector subspace Xmax deÔ¨Åned by Wexo,max is unique.\n18\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nAlgorithm 4 Stepwise Rank Ascending Scheme: SRAS\n1: Inputs: A database of transitions {(si, ai, ri, s‚Ä≤\ni)}N\ni=1\n2: Output: The exogenous state projection matrix Wexo\n3: Initialize Wexo ‚Üê[ ], Wtemp ‚Üê[ ], Cx ‚Üê[ ], k ‚Üê0\n4: repeat\n5:\nN ‚Üêorthonormal basis for the null space of Cx\n6:\nSolve the following optimization problem\nÀÜw :=\narg min\nw‚ààR(d‚àík)√ó1CCC(S‚Ä≤[Wtemp, N ‚ä§w]; A | S[Wtemp, N ‚ä§w])\nsubject to w‚ä§w = 1\n7:\nwk+1 ‚ÜêN ‚ä§ÀÜw\n8:\nCx ‚ÜêCx ‚à™{wk+1}\n9:\nCCCsim ‚ÜêCCC(S‚Ä≤[Wtemp, wk+1]; A | S[Wtemp, wk+1])\n10:\nif CCCsim < œµ then\n11:\nWtemp ‚ÜêWtemp ‚à™{wk+1}\n12:\nE ‚ÜêS ‚àíSWtempW ‚ä§\ntemp\n13:\nCCCfull ‚ÜêCCC(S‚Ä≤Wtemp; [E, A] | SWtemp)\n14:\nif CCCfull < œµ then\n15:\nWexo ‚ÜêWtemp\n16:\nend if\n17:\nend if\n18:\nk ‚Üêk + 1\n19: until k = d\n20: return Wexo\nProof. By contradiction. If there were 2 distinct maximal subspaces, then Lemma 1 would allow us to combine them to\nget an even larger exogenous vector subspace. This is a contradiction.\nLemma 3. Let Wexo,max deÔ¨Åne the maximal exogenous vector subspace Xmax, and let Wexo deÔ¨Åne any other\nexogenous vector subspace X. Then X ‚äëXmax.\nProof. By contradiction. If there were an exogenous subspace X not contained within Xmax, then by Lemma 1\nwe could combine the 2 exo/endo decompositions to get an even larger exogenous subspace. This contradicts the\nassumption that Xmax is maximal.\nThese three lemmas establish property (c) and complete the proof of Theorem 8.\nThis concludes our analysis of the Oracle-GRDS for the full setting (Algorithm 3). The analysis can trivially be\nextended to the diachronic setting.\nHow well does this analysis carry over to the non-oracle GRDS algorithm? GRDS departs from the oracle version\nin three ways. First, GRDS employs the CCC in place of conditional mutual information (CMI). This may assign\nnon-zero CCC values to Wexo matrices that actually have zero CMI. This will cause GRDS to under-estimate dexo, the\ndimensionality of the exogenous state space. Second, GRDS only requires CCC to be less than a parameter œµ. If œµ is\nlarge, then GRDS may stop too soon and over-estimate dexo. Hence, by introducing œµ, GRDS is able to compensate\nsomewhat for the failures of CCC. Third, the database of transitions is not inÔ¨Ånite, so the value of CCC that GRDS\ncomputes may be too high or too low. This in turn may cause dexo to be too small or too large. In our experiments, we\nwill compare the estimated dexo to our understanding of its true value.\n4.3\nStepwise Algorithm SRAS\nThe global scheme computes the entire Wexo matrix at once. In this section, we introduce an alternative stepwise\nalgorithm, the Stepwise Rank Ascending Scheme (SRAS, see Algorithm 4), which constructs the matrix Wexo\nincrementally by solving a sequence of small manifold optimization problems.\n19\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nSRAS maintains the current partial solution Wexo, a temporary matrix Wtemp that may be extended to update Wexo, a\nset of all candidate column vectors generated so far, Cx, and an orthonormal basis N for the null space of Cx. (N is a\nmatrix with a number of columns equal to the dimension of the null space of Cx.) The set of candidate column vectors\nCx contains all of the column vectors in Wexo and possibly some additional vectors that were rejected for violating the\nfull CCC constraint, as we will discuss below.\nSuppose we have already found the Ô¨Årst k columns of Wexo = [w1, w2, . . . , wk]. To ensure that the new column wk+1\nis orthogonal to all k previous vectors, we restrict wk+1 to lie in the space deÔ¨Åned by N by requiring it to have the form\nwk+1 = N ‚ä§w. This ensures that it is orthogonal to all columns of Wexo and to any additional vectors in Cx.\nIn Line 6, we compute a new candidate vector ÀÜw by solving a simpliÔ¨Åed CCC minimization problem on the (d ‚àík) √ó 1-\ndimensional Stiefel manifold. Recall that the full objective I(X‚Ä≤; [E, A] | X) = 0 seeks to enforce the conditional\nindependence X‚Ä≤ ‚ä•‚ä•E, A | X. This requires us to know X and E, whereas at this point in the algorithm, we only know\na portion of X, and we therefore do not know E at all. We circumvent this problem by using the simpliÔ¨Åed objective\nI(X‚Ä≤\nk; A | X1, . . . , Xk) (approximated via the CCC). This objective ensures that A has no effect on the exogenous\nvariables X‚Ä≤ in the next time step, which eliminates the edge A ‚ÜíX‚Ä≤, but it does not protect against the possibility\nthat A causes a change in some chain of endogenous variables that affect X in some subsequent time step. Hence, the\nsimpliÔ¨Åed objective is a necessary but not sufÔ¨Åcient condition for X to be a valid exogenous subspace. See Appendix B\nfor a detailed discussion of this point.\nLines 7 and 8 compute the new candidate vector wk+1 by mapping ÀÜw into the null space deÔ¨Åned by N and then adding\nit to Cx. In Line 9, we compute CCCsim, the value of the simpliÔ¨Åed objective (which is the same as the value that\nminimized the objective in Line 6). In Line 10, we check whether this is less than œµ. If not, we increment k and loop\nback to Line 5 and Ô¨Ånd another ÀÜw vector. But if CCCsim < œµ, then in Lines 11-13, we compute the corresponding E\nmatrix and compute CCCfull, the CCC of the full objective. In Line 14, we check whether CCCfull < œµ. If so, then\nwe have a valid new column to add to Wexo. If not, we increment k and loop back to Line 5.\nRecall that not all subsets of the maximal exogenous subspace are themselves valid exogenous subspaces that satisfy\nthe full CCC constraint (Theorem 3). Hence, it is important that SRAS does not terminate when adding a candidate\nvector to Wexo causes the full constraint to be violated. Note, however, that every subset of the maximal exogenous\nsubspace must satisfy the simpliÔ¨Åed objective, because otherwise, the action variable is directly affecting one of the\nexogenous state variables.\nTo allow SRAS to continue making progress when the full constraint is violated, the algorithm maintains the matrix\nWtemp. This matrix contains all of the candidates that have satisÔ¨Åed the simpliÔ¨Åed objective. If a subsequent candidate\nwk+1 allows Wtemp to satisfy the full constraint, then we set Wexo to Wtemp and continue. The algorithm terminates\nwhen k = d.\nThe primary advantage of SRAS compared to GRDS is that the CCC minimization problems have dimension (d‚àík)√ó1.\nHowever, GRDS can halt as soon as it Ô¨Ånds a Wexo that satisÔ¨Åes the full CCC objective, whereas SRAS must solve\nall d problems. We can introduce heuristics to terminate d early. For example, we can monitor the residual variance\n‚à•ÀÜX ¬∑ wR ‚àíR‚à•2\n2 of the reward regression. This decreases monotonically as columns are added to Wexo, and when\nthose decreases become very small, we can terminate SRAS. This can make SRAS very efÔ¨Åcient when the exogenous\nsubspace has low rank dexo relative to the rank d of the full state space. In such cases, GRDS must solve d ‚àídexo + 1\nlarge manifold optimization problems of dimension at least d √ó dexo, whereas SRAS must only solve dexo problems of\ndimension (d ‚àík) √ó 1.\nWhat can we say about the correctness of SRAS? First, in an oracle version of SRAS (where the MDP was admissible,\nthe data were collected using a fully-randomized policy, and CMI was computed instead of CCC), the Wexo matrix\nreturned by SRAS would deÔ¨Åne a valid exo/endo decomposition. This is because it would satisfy the full CMI constraint.\nHowever, it would not necessarily deÔ¨Åne the maximal exogenous subspace, because the w vectors found using the\nsimpliÔ¨Åed objective and stored in Wtemp might not be a subset of a satisfying Wexo matrix. Of course, because the\nactual SRAS algorithm introduces the CCC approximation and only requires the CCC to be less than œµ, we do not have\nany guarantee that the Wexo matrix returned by SRAS deÔ¨Ånes a valid exogenous subspace. We now turn to experimental\ntests of the algorithms to see whether they produce useful results despite their several approximations.\n5\nExperimental Study\nWe conducted a series of experiments to understand the behavior of our algorithms. In addition to GRDS and SRAS,\nwe deÔ¨Åned a third algorithm, SimpliÔ¨Åed-GRDS that applies the simpliÔ¨Åed objective CCC(S‚Ä≤WExo; A|SWexo) in Line\n6 of Algorithm 2 but then still uses the full objective in Line 7. Like SRAS, SimpliÔ¨Åed-GRDS will always return a valid\nWexo, but it may not be maximal.\n20\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nIn this section, we present experiments to address the following research questions:\nRQ1: Do our methods speed up reinforcement learning in terms of sample complexity? In terms of total CPU time?\nRQ2: Do our methods discover the correct maximal exogenous subspaces?\nRQ3: What is the best approach to reward regression? Single linear, repeated linear, or online neural network\nregression?\nRQ4: How do the algorithms behave when the MDPs are changed to have different properties: (a) rewards are\nnonlinear, (b) transition dynamics are nonlinear, (c) action space is combinatorial, and (d) states and actions\nare discrete?\nRQ5: How do the algorithms behave when the covariance condition is violated?\nRQ6: What are the risks and beneÔ¨Åts of using the simpliÔ¨Åed objective in SRAS and SimpliÔ¨Åed-GRDS?\nRQ7: How should hyperparameters be set? How many training tuples should be collected before performing\nexogenous subspace discovery? When should reward regression begin and on what schedule?\n5.1\nExperimental Details\nWe compare Ô¨Åve methods:\n‚Ä¢ Baseline: Reinforcement learning applied to the full reward (sum of exogenous and endogenous components)\n‚Ä¢ GRDS: The Global Rank Descending Scheme\n‚Ä¢ SimpliÔ¨Åed-GRDS: GRDS using the simpliÔ¨Åed objective\n‚Ä¢ SRAS: The Stepwise Rank Ascending Scheme\n‚Ä¢ Endo Reward Oracle: Reinforcement learning applied with the oracle endogenous reward.\nAs the reinforcement learning algorithm, we employ the PPO implementation from stable-baselines3 [RafÔ¨Ån et al., 2021]\nin PyTorch [Paszke et al., 2019], and we model the MDPs in the OpenAI Gym framework [Brockman et al., 2016]. We\nuse the default PPO hyperparameters in stable-baselines3, which include a clip range of 0.2, a value function coefÔ¨Åcient\nof 0.5, an entropy coefÔ¨Åcient of 0, and a generalized advantage estimation (GAE) parameter of 0.95. The policy and\nvalue networks have two hidden layers of 64 tanh units each. For PPO optimization, we employ the default Adam\noptimizer [Kingma and Ba, 2015] in PyTorch with default hyperparameters Œ≤1 = 0.9, Œ≤2 = 0.999, eps = 1 √ó 10‚àí5\nand a default learning rate of lrP P O = 0.0003. The batch size for updating the policy and value networks is 64\nsamples. The discount factor in the MDPs is set to Œ≥ = 0.99. The default number of steps between each policy update\nis K = 1536. The number of steps L after which we compute the exo/endo decomposition and the total number of\ntraining steps in the experiment N vary per experiment, but their default values are L = 3000 and N = 6000. We\nsummarize all hyperparameters in Table 2.\nIn each experimental run, we maintain two instances of the Gym environment. Training is carried out in the primary\ninstance. After every policy update, we copy the policy to the second instance and evaluate the performance of the\npolicy (without learning) for 1000 steps. To reduce measurement variance, these evaluations always start with the same\nrandom seed.\nTo solve the manifold optimization problems, we apply the solvers implemented in the Pymanopt package [Townsend\net al., 2016]. We use the Steepest Descent solver with line search with the default Pymanopt hyperparameters. For the\nCCC constraint, œµ is set to 0.05. The Tikhonov regularizer inside the CCC is set to Œª = 0.01. These hyperparameters\nwere determined empirically via random search [Bergstra and Bengio, 2012].\nTo implement neural network reward regression, we use a separate neural network in sklearn [Buitinck et al., 2013] with\n2 hidden layers of 50 and 25 units, respectively, and ReLU activations. We train with Adam using the default Adam\ncoefÔ¨Åcients. The learning rate is set by default to lrregr = 0.0003 and the L2 regularization to 3 √ó 10‚àí5. The batch size\nis set to 256. During Phase 1, we train the net with the L collected samples until convergence (or a maximum number\nof 125 epochs). During Phase 2, we perform online learning by updating the neural net every M = 256 training steps\nwith a single pass over the last 256 samples. Linear regression is computed using the standard least squares matrix\nsolution without regularization.\nWe chose to use the default library hyperparameters for PPO, Adam optimization, and manifold optimization to enable\na fair comparison of the different methods. Furthermore, for online reinforcement learning algorithms, it is not feasible\nto perform extensive hyperparameter searches because of the cost (and risk) of interacting in the real world. Algorithms\n21\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nUsed for\nDescription\nSymbol\nDefault Value\nFixed\nPPO\nclipping parameter\n-\n0.2\nYes\nvalue function coefÔ¨Åcient\n-\n0.5\nYes\nentropy coefÔ¨Åcient\n-\n0\nYes\nGAE parameter\n-\n0.95\nYes\nPolicy & Value Nets\nnumber of layers\n-\n2\nYes\nunits per layer\n-\n64, 64\nYes\nactivation function\n-\ntanh\nYes\nPPO Optimization\nwith Adam\nAdam learning rate\nlrP P O\n0.0003\nYes\nAdam coefÔ¨Åcients\nŒ≤1, Œ≤2, eps\n0.9, 0.99, 1e-5\nYes\nbatch size\n-\n64\nYes\nL2 regularization\n-\n0\nYes\nReinforcement\nLearning\ndiscount factor\nŒ≥\n0.99\nYes\npolicy update steps\nK\n1536\nYes\ntotal training steps\nN\n60000\nNo\nsteps for decomposition\nL\n3000\nNo\nsteps for exo regression\nM\n256\nYes\nevaluation steps\n-\n1000\nYes\nManifold\nOptimization\nCCC threshold\nœµ\n0.05\nYes\nTikhonov regularizer\nŒª\n0.01\nYes\nExo regression Net\nnumber of layers\n-\n2\nYes\nunits per layer\n-\n50, 25\nYes\nactivation function\n-\nrelu\nYes\nExo Regression\nOptimization\nwith Adam\nAdam learning rate\nlrregr\n0.0003\nNo\nAdam coefÔ¨Åcients\nŒ≤1, Œ≤2, eps\n0.9, 0.99, 1e-8\nYes\nbatch size\n-\n256\nYes\nL2 regularization\n-\n0.00003\nYes\nTable 2: Hyperparameters for High-D setting.\nthat only perform well after extensive hyperparameter search are not usable in practice. Hence, we wanted to minimize\nhyperparameter tuning.\nIn all our MDPs, we use the default values in Table 2 for all hyperparameters except for the number of decomposition\nsteps L and the number of training steps L. The former is the most critical hyperparameter; it is discussed in detail in\nSection 5.6. The latter is set to a number that is high enough for our methods to converge or be near the limit. Finally, in\na few settings we found it beneÔ¨Åcial to use a regression learning rate of 0.0006 instead of 0.0003 for better convergence.\nFor each setting, we report the values of the hyperparameters that are different from their default values. We run all\nexperiments on a c5.4xlarge EC2 machine on AWS3.\n5.2\nPerformance Comparison on High-Dimensional Linear Dynamical MDPs\nTo address RQ1 and RQ2, we deÔ¨Åne a set of high-dimensional MDPs with linear dynamics. Each MDP, by design, has\nm endogenous and n exogenous variables, so that et ‚ààRm and xt ‚ààRn. There is a single action variable at that takes\n10 discrete values (‚àí1, ‚àí0.777, ‚àí0.555, . . . , 0, . . . , 0.555, 0.777, +1). The policy chooses one of these values at each\ntime step. The exo and endo transition functions are\nxt+1 = Mexo ¬∑ xt + Œµexo\net+1 = Mend ¬∑\n\u0014\net\nxt\n\u0015\n+ Ma ¬∑ at + Œµend,\nwhere Mexo ‚ààRn√ón is the transition function for the exogenous MRP; Mend ‚ààRm√óm is the transition function for\nthe endogenous MDP involving et and xt; Ma ‚ààRm is the coefÔ¨Åcient for the action at, and it is set to a vector of\nones; Œµexo ‚ààRn is the exogenous noise, whose elements are distributed according to N(0, 0.09); and Œµend ‚ààRm is\nthe endogenous noise, whose elements are distributed according to N(0, 0.04). The observed state vector st ‚ààRm+n\nis a linear mixture of the hidden exogenous and endogenous states deÔ¨Åned as\nst = M ¬∑\n\u0014\net\nxt\n\u0015\n,\n3https://aws.amazon.com/ec2/instance-types/c5/.\n22\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nwhere M ‚ààR(m+n)√ó(m+n). The elements in Mexo, Mend, and M are generated according to N(0, 1) and then each\nrow of each matrix is normalized to sum to 0.99 for stability4. The elements of the initial endo and exo states are\nrandomly initialized from a uniform distribution over [0, 1].\nIn our Ô¨Årst set of experiments, the reward at time t is\nRt = Rexo,t + Rend,t,\nwhere Rexo,t = ‚àí3 ¬∑ avg(xt) is the exogenous reward and Rend,t = e‚àí|avg(et)‚àí1| is the endogenous reward, and avg(¬∑)\ndenotes the average over a vector‚Äôs elements. For this class of MDPs, the optimal policy seeks to drive the average of et\nto 1.\nWe experiment in total with 6 MDPs with different choices for the numbers n and m of the exogenous and endogenous\nvariables, respectively: (i) 5-D state with m = 2 endo variables and n = 3 exo variables; (ii) 10-D state with m = 5\nand n = 5; (iii) 20-D state with m = 10 and n = 10; (iv) 30-D state with m = 15 and n = 15; (iv) 45-D state with\nm = 22 and n = 23; and (vi) 50-D state with m = 25 endo variables and n = 25 exo variables. For the 50-D setting,\nwe use N = 100000 training steps and L = 10000 decomposition steps due to its higher dimensionality. Similarly,\nwe use N = 80000 and L = 5000 for the 45-D MDP, and L = 5000 for the 30-D MDP. On the other hand, we set\nN = 50000 and L = 2000 for the 5-D MDP. For each MDP, we run 20 replications with different random seeds and\nreport the average results and standard deviations.\n5.2.1\nRL Performance\nTo address RQ1, we report the performance for each of the 6 MDPs over 20 replications in Figure 6. In all 6 MDPs, all\nof our methods far-outperform the baseline. Indeed, in the 5-D and 10-D MDPs, the baseline does not show any sign of\nlearning, and in the larger problems, the baseline‚Äôs performance has attained roughly half of the performance of our\nmethods after 65 policy updates. A simple linear extrapolation of the baseline learning curve for the 50-D problem\nsuggests that it will require 132 policy updates to attain the performance of the other methods. This is more than 3\ntimes as long as the 40 updates our methods require. Hence, in terms of sample complexity, our methods are much\nmore efÔ¨Åcient than the baseline method.\nOn these MDPs, the SimpliÔ¨Åed-GRDS and SRAS methods are able to match the performance of the Endo Reward\nOracle, which is given the correct endogenous reward from the very start. GRDS performs very well on all MDPs\nexcept for the 5-D one, where it is still able to outperform the baseline.\nRQ1 also asks whether our methods are superior in terms of CPU time. Table 3 reports the CPU time required by\nthe various methods. The Baseline and Endo Reward Oracle consume identical amounts of time, so the table only\nlists the Baseline CPU time. We observe that even the slowest of our methods (SRAS on the 50-D problem) requires\nonly 45% more time than the Baseline. However, if we again extrapolate the baseline to 132 policy updates, where its\nperformance would match our methods, that would require 2997 seconds of CPU time, which is 49% more than SRAS.\nHence, even if there is zero cost to collecting training samples in the real world, our methods are still faster.\n5.2.2\nRank of Discovered Exo Subspace\nRQ2 asks whether our methods Ô¨Ånd the correct maximal exogenous subspaces. Our experiments revealed a surprise.\nAlthough we constructed the MDPs with the goal of creating an n-dimensional exogenous space and an m-dimensional\nendogeous space, our methods usually discover exogenous spaces with n + m ‚àí1 dimensions. Upon further analysis,\nwe realized that because the action variable is 1-dimensional, it can only affect a 1-dimensional subspace of the\nn + m-dimensional state space. Consequently, the true maximal exogenous subspace has dimension n + m ‚àí1. The\nresults in Table 3 show that the SimpliÔ¨Åed-GRDS always Ô¨Ånds an exogenous subspace of the correct dimension. The\nexogenous space computed by SRAS is sometimes slightly smaller on the smaller MDPs, and the space computed by\nGRDS is sometimes slightly smaller on the larger MDPs. We believe the failures of SRAS are due to the approximations\nthat we discussed in Section 4.3. We suspect the failures of GRDS reÔ¨Çect failures of the manifold optimization to Ô¨Ånd\nthe optimum in high-dimensional problems.\nThe fact that the exogenous space has dimension n + m ‚àí1 explains the relative amount of CPU time consumed by the\ndifferent algorithms. SRAS is often the slowest, because it must solve n + m optimization problems whereas GRDS\nand SimpliÔ¨Åed-GRDS must only solve two (large) manifold optimization problems before terminating.\n4Notice that all matrices Mexo, Mend, and M in our synthetic linear MDPs are stochastic. Future work could explore more\ngeneral classes of MDPs.\n23\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n5\n10\n15\n20\n25\n30\n# of Policy Updates\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 50000\nDecomposition steps = 2000\nPolicy update steps = 1536\nExo variables = 3\nEndo variables = 2\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(a) 5-D MDP (m = 2, n = 3).\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(b) 10-D MDP (m = 5, n = 5).\n0\n10\n20\n30\n40\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 10\nEndo variables = 10\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(c) 20-D MDP (m = 10, n = 10).\n0\n10\n20\n30\n40\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 5000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(d) 30-D MDP (m = 15, n = 15).\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 8000\nPolicy update steps = 1536\nExo variables = 23\nEndo variables = 22\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(e) 45-D MDP (m = 22, n = 23).\n0\n10\n20\n30\n40\n50\n60\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 100000\nDecomposition steps = 10000\nPolicy update steps = 1536\nExo variables = 25\nEndo variables = 25\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(f) 50-D MDP (m = 25, n = 25).\nFigure 6: Comparison of various methods in high-D linear MDPs.\n5.2.3\nComparison of Methods for Exo Reward Regression\nWe performed a second set of experiments to investigate how the type and conÔ¨Åguration of exo reward regression affects\nthe performance of our methods. We compare three reward regression conÔ¨Ågurations. The Ô¨Årst conÔ¨Åguration is Single\nLinear Regression, which Ô¨Åts a linear model for the exo reward and performs regression only once at the end of Phase\n1 of Algorithm 1. The second conÔ¨Åguration is Repeated Linear Regression. Like Single Linear Regression, it Ô¨Åts a\nlinear model at the end of Phase 1. In addition, it re-Ô¨Åts the model in Phase 2 every 1,000 collected samples using all\ntransition data so far. Note that this is different from online Algorithm 1, which updates the exogenous reward function\nin Phase 2 every M observations using only the last M observations in Dexo. The goal was to understand whether\nregular regression with all observed transition data can perform better than a single linear regression at the end of Phase\n1. The third conÔ¨Åguration is Online Neural Net Regression which Ô¨Åts a neural network to the exo reward data. At the\n24\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nTotal State\nExo State\nEndo State\nMethod\nExo Subspace\nTotal Time\nDecomposition\nVariables\nVariables\nVariables\nRank\n(secs)\nTime (secs)\n5\n3\n2\nBaseline\n-\n245.9¬±14.7\n-\nGRDS\n4.0¬±4.0\n294.9¬±113.0\n6.3¬±4.6\nSimpliÔ¨Åed-GRDS\n4.0¬±0.0\n297.0¬±115.4\n9.6¬±5.0\nSRAS\n3.95¬±0.22\n297.1¬±113.3\n14.0¬±14.0\n10\n5\n5\nBaseline\n-\n345.3¬±10.48\n-\nGRDS\n8.65¬±0.48\n413.5¬±148.0\n22.6¬±18.0\nSimpliÔ¨Åed-GRDS\n9.0¬±0.0\n413.7¬±144.4\n13.1¬±9.1\nSRAS\n8.75¬±0.54\n434.3¬±157.6\n34.2¬±46.7\n20\n10\n10\nBaseline\n-\n450.2¬±34.8\n-\nGRDS\n18.1¬±1.18\n525.3¬±167.9\n41.8¬±47.8\nSimpliÔ¨Åed-GRDS\n19.0¬±0.0\n513.6¬±141.3\n8.6¬±4.4\nSRAS\n18.4¬±0.66\n562.4¬±188.5\n86.0¬±72.3\n30\n15\n15\nBaseline\n-\n509.9¬±43.0\n-\nGRDS\n28.25¬±1.22\n597.5¬±185.6\n56.9¬±84.1\nSimpliÔ¨Åed-GRDS\n29.0¬±0.0\n584.3¬±136.9\n12.5¬±6.0\nSRAS\n27.9¬±1.58\n688.9¬±276.5\n177.5¬±234.9\n45\n23\n22\nBaseline\n-\n895.4¬±159.3\n-\nGRDS\n43.4¬±0.86\n1041.7¬±287.6\n84.9¬±154.2\nSimpliÔ¨Åed-GRDS\n44.0¬±0.0\n1006.3¬±207.8\n13.3¬±8.4\nSRAS\n44.0¬±0.0\n1323.4¬±716.1\n605.1¬±538.7\n50\n25\n25\nBaseline\n-\n1472.4¬±126.0\n-\nGRDS\n48.45¬±0.86\n1659.6¬±282.7\n81.7¬±105.8\nSimpliÔ¨Åed-GRDS\n49.0¬±0.0\n1634.5¬±239.1\n15.7¬±9.2\nSRAS\n49.0¬±0.0\n2009.2¬±840.3\n667.8¬±616.9\nTable 3: Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition\ntime for the linear MDPs.\nend of Phase 1, we perform reward regression until convergence. During Phase 2, we then perform a single epoch every\n256 steps.\nWe plot RL performance over the 20 replications in Figure 6 on two MDPs: (i) the 10-D state MDP (Figures 7a-7b)\nand (ii) the 50-D state MDP (Figures 7c-7d). We compare the three regression methods applied to GRDS and SRAS.\nThe results show that Online Neural Net Regression generally outperforms Single and Repeated Linear Regression,\neven though the exo reward function Rexo,t is a linear function of the exo state. We speculate that this is because it is\ncontinually incorporating new data, which in turn may allow PPO to make more progress. Repeated Linear Regression\nalso incorporates new data, but at a slower rate. Furthermore, when applied to SRAS, it becomes unstable and exhibits\nhigh variance. We speculate that this may be because it is only Ô¨Åtting to the most recent 1000 data points. Future work\nmight consider training on all accumulated data and imposing strong regularization to improve stability.\nBased on the superior performance of online neural network regression, we adopt it as the default reward regression\nmethod in the remainder of our experiments.\n5.3\nExploring ModiÔ¨Åcations of the MDPs\nTo address RQ4, we now study the performance of our methods when they are applied to MDPs that depart in various\nways from the linear dynamical MDPs studied thus far:\n(a) Rewards are nonlinear functions of the state,\n(b) Transition dynamics are nonlinear,\n(c) The action space is combinatorial, and\n(d) The states and actions are discrete.\n5.3.1\nNonlinear Exogenous Reward Functions\nBecause we have adopted online neural network reward regression, we expect that our methods should be able to\nÔ¨Åt nonlinear exogenous reward functions. We consider the high-D linear setting of Section 5.2 with m = n = 15.\nWe perform exo/endo decomposition after L = 5000 steps and train for a total of N = 80000 steps. We found it\n25\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 20\nAlgorithm\nBaseline\nSingle Linear Regression\nRepeated Linear Regression\nOnline Neural Net Regression\nEndo Reward Oracle\n(a) GRDS (m = 5, n = 5).\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 20\nAlgorithm\nBaseline PPO\nSingle Linear Regression\nRepeated Linear Regression\nOnline Neural Net Regression\nEndo Reward Oracle\n(b) SRAS (m = 5, n = 5).\n0\n10\n20\n30\n40\n50\n60\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 100000\nDecomposition steps = 10000\nPolicy update steps = 1536\nExo variables = 25\nEndo variables = 25\nReplications = 20\nAlgorithm\nBaseline\nSingle Linear Regression\nRepeated Linear Regression\nOnline Neural Net Regression\nEndo Reward Oracle\n(c) GRDS (m = 25, n = 25).\n0\n10\n20\n30\n40\n50\n60\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 100000\nDecomposition steps = 10000\nPolicy update steps = 1536\nExo variables = 25\nEndo variables = 25\nReplications = 20\nAlgorithm\nBaseline PPO\nSingle Linear Regression\nRepeated Linear Regression\nOnline Neural Net Regression\nEndo Reward Oracle\n(d) SRAS (m = 25, n = 25).\nFigure 7: Impact of the type of exo reward regression on RL performance.\nbeneÔ¨Åcial to use a learning rate for the exogenous reward regression of 0.0006 instead of the default 0.0003; the higher\nlearning rate can help the exogenous reward neural net to adapt faster. We perform 20 replications with different seeds.\nFurthermore, we replace the linear exogenous reward function Rexo,t by the following four choices:\n‚Ä¢ R1\nexo,t = clip(6 ¬∑ (avg(xt) + 1\n3 ¬∑ avg(x2\nt) ‚àí2\n15 ¬∑ avg(x3\nt)), ‚àí5.0, 5.0), a 3rd degree polynomial.\n‚Ä¢ R2\nexo,t = ‚àí3 ¬∑ e‚àí|avg(xt)|1.5, a function of avg(xt) with a single mode.\n‚Ä¢ R3\nexo,t = ‚àí3 ¬∑ (e‚àí|avg(xt+1.5)|2 ‚àíe‚àí|avg(xt‚àí1.5)|2), a function of avg(xt) with two modes.\n‚Ä¢ R4\nexo,t = ‚àí3 ¬∑ (e‚àí|avg(xt+1)|2 + 3\n2 ¬∑ e‚àí|avg(xt‚àí1.5)|2 ‚àí5\n3e‚àí|avg(xt)|2), a function of avg(xt) with three modes.\nFigures 8a-8d plot the results.\nWe generally observe that all methods match the Endo Reward Oracle‚Äôs performance and outperform the baseline by a\nlarge margin. This conÔ¨Årms that the nonlinear reward regression is able to Ô¨Åt these nonlinear reward functions. As we\nhave observed before, the RL performance of SRAS is a bit unstable, perhaps because it is not always able to detect the\nmaximal exogenous subspace. The SimpliÔ¨Åed-GRDS method also shows a tiny bit of instability.\nTable 4 reports the average rank of the discovered exogenous subspaces. The true exogenous space has rank 29, but the\nexogenous reward only depends on 15 of those dimensions. The SimpliÔ¨Åed-GRDS method is most consistently able to\nÔ¨Ånd the true rank, whereas SRAS and GRDS struggle to capture that last dimension.\n5.3.2\nNonlinear State Transition Dynamics\nSo far, we have considered MDPs with linear state transitions for the endogenous and exogenous states. It is a natural\nquestion whether our algorithms can handle more general MDPs. In this section, we provide experimental results on a\nmore general class of nonlinear MDPs. Even though we lack a rigorous theoretical understanding, our results hint at the\npotential of the CCC objective to discover useful exo/endo state decompositions even when the dynamics are nonlinear.\n26\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 5000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nRegression net learning rate = 0.0006\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(a) R1\nexo,t.\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 5000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nRegression net learning rate = 0.0006\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(b) R2\nexo,t.\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 5000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nRegression net learning rate = 0.0006\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(c) R3\nexo,t.\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 5000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nRegression net learning rate = 0.0006\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(d) R4\nexo,t.\nFigure 8: RL performance for MDPs with nonlinear exo reward functions.\nExo Reward\nMethod\nExo Subspace\nTotal Time\nDecomposition\nFunction\nRank\n(secs)\nTime (secs)\nR1\nexo,t\nBaseline\n-\n1520.0¬±153.0\n-\nGRDS\n28.05¬±1.02\n1874.6¬±357.3\n155.8¬±122.1\nSimpliÔ¨Åed-GRDS\n29.0¬±0.0\n1808.6¬±278.7\n40.5¬±11.3\nSRAS\n27.95¬±1.56\n1914.4¬±510.3\n251.8¬±249.5\nR2\nexo,t\nBaseline\n-\n1510.9¬±136.2\n-\nGRDS\n28.3¬±0.9\n1860.4¬±359.9\n150.3¬±155.1\nSimpliÔ¨Åed-GRDS\n29.0¬±0.0\n1797.8¬±264.4\n31.9¬±7.7\nSRAS\n27.95¬±1.56\n1925.4¬±516.3\n297.7¬±244.1\nR3\nexo,t\nBaseline\n-\n1518.1¬±146.4\n-\nGRDS\n28.0¬±1.0\n1879.6¬±340.0\n150.3¬±118.1\nSimpliÔ¨Åed-GRDS\n29.0¬±0.0\n1818.0¬±262.2\n28.6¬±8.5\nSRAS\n27.95¬±1.56\n1932.4¬±505.4\n272.9¬±265.5\nR4\nexo,t\nBaseline\n-\n1513.8¬±103.7\n-\nGRDS\n28.35¬±0.79\n1849.8¬±366.1\n123.2¬±111.5\nSimpliÔ¨Åed-GRDS\n28.35¬±0.79\n1812.0¬±268.9\n37.1¬±11.3\nSRAS\n29.0¬±0.0\n1926.7¬±560.5\n307.4¬±320.7\nTable 4: Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition\ntime for the MDPs with nonlinear exo rewards.\nIn the experiments in this section, we introduce nonlinear dynamics, but we still conÔ¨Ågure the exogenous and endogenous\nstate spaces so that they are linear projections of the full state space. We study the following three MDPs, which are\ndeÔ¨Åned according to the recipe in Section 5.2 with the following modiÔ¨Åcations:\n27\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(a) M1.\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(b) M2.\n0\n10\n20\n30\n40\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(c) M3.\n0\n10\n20\n30\n40\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(d) M3 (œµ = 0.1).\nFigure 9: RL performance for MDPs with nonlinear state transitions.\n‚Ä¢ M1 is a 10-D MDP with m = n = 5 and a single action variable. The exo and endo state transitions are\nxt+1 = clip(Mexo ¬∑ xt + 1\n3 ¬∑ Nexo ¬∑ x2\nt ‚àí2\n15 ¬∑ Kexo ¬∑ x3\nt, ‚àí4, 4) + Œµexo\net+1 = Mend ¬∑\n\u0014\net\nxt\n\u0015\n+ Ma ¬∑ at + Œµend,\nwhere Mexo, me, Ma and Œµexo, Œµend are deÔ¨Åned as in Section 5.2. Furthermore, the two matrices Nexo ‚ààRn,n\nand Kexo ‚ààRn,n are generated following the same procedure as Mexo. M1 has nonlinear exogenous\ndynamics but linear endogenous dynamics.\n‚Ä¢ M2 is exactly the same as M1 except that the endogenous transition function now has a nonlinear dependence\non the action:\net+1 = Mend ¬∑\n\u0014\net\nxt\n\u0015\n+ Ma ¬∑ at + Na ¬∑ a2\nt + Œµend.\nThe entries in Na ‚ààRm are sampled from the uniform distribution over [0.5, 1.5).\n‚Ä¢ M3 is a 10-D MDP with m = n = 5 and a single action variable. The exogenous and endogenous state\ntransitions functions are\nxt+1 = clip(5 ¬∑ sign(xt) ¬∑\np\n|xt| ‚àísin(xt), ‚àí2, 2) + Œµexo\net+1 = Mend ¬∑\n\u0014\net\nxt\n\u0015\n+ sin(3 ¬∑ at) + Œµend.\nThe entries in the noise vectors Œµexo and Œµend are sampled from N(0, 0.16) and N(0, 0.09), respectively. Like\nM2, M3‚Äôs exo and endo transition functions are both nonlinear.\nFigures 9a-9c plot the RL performance over 15 replications with different seeds and Table 5 reports the ranks of the\ndiscovered exogenous subspaces. Consider Ô¨Årst MDPs M1 and M2. The SimpliÔ¨Åed-GRDS and SRAS algorithms\n28\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nMDP\nExo/Endo State\nMethod\nExo Subspace\nTotal Time\nDecomposition\nVariables\nRank\n(secs)\nTime (secs)\nM1\n5/5\nBaseline\n-\n665.3¬±39.9\n-\nGRDS\n9.0¬±0.0\n890.4¬±140.6\n55.0¬±34.0\nSimpliÔ¨Åed-GRDS\n9.0¬±0.0\n906.7¬±154.6\n72.2¬±19.8\nSRAS\n9.0¬±0.0\n951.2¬±253.2\n172.8¬±73.2\nM2\n5/5\nBaseline\n-\n717.0¬±30.3\n-\nGRDS\n9.0¬±0.0\n959.5¬±137.1\n61.9¬±41.2\nSimpliÔ¨Åed-GRDS\n9.0¬±0.0\n981.2¬±138.0\n85.0¬±18.1\nSRAS\n9.0¬±0.0\n1019.8¬±271.9\n187.4¬±72.6\nM3\n(œµ = 0.05)\n5/5\nBaseline\n-\n768.2¬±50.1\n-\nGRDS\n7.67¬±1.44\n1033.0¬±241.2\n125.8¬±126.7\nSimpliÔ¨Åed-GRDS\n3.60¬±3.28\n988.1¬±160.4\n38.7¬±19.0\nSRAS\n3.47¬±3.36\n1081.3¬±370.7\n168.8¬±70.7\nM3\n(œµ = 0.1)\n5/5\nBaseline\n-\n768.2¬±50.1\n-\nGRDS\n7.67¬±1.44\n1033.0¬±241.2\n125.8¬±126.7\nSimpliÔ¨Åed-GRDS\n9.0¬±0.0\n918.3¬±129.7\n17.7¬±4.6\nSRAS\n9.0¬±0.0\n1023.3¬±341.5\n230.4¬±69.7\nTable 5: Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition\ntime for the MDPs with nonlinear state transitions.\nperform very well. They converge quickly, and they are able to match the Endo Reward Oracle‚Äôs performance. In\nboth settings, the Baseline converges to a suboptimal value and suffers from high variance. Most shocklingly, GRDS\nperforms catastrophically and exhibits very high variance even though it discovers an exogenous subspace of the correct\nrank.\nNow consider M3 in Figure 9c. On this problem, SimpliÔ¨Åed-GRDS and SRAS perform poorly (although still better\nthan the baseline), while GRDS performs much better. However, none of the methods is able to match the Endo Reward\nOracle. Table 5 reveals that all of the methods, but particularly SimpliÔ¨Åed-GRDS and SRAS, are failing to discover\nthe correct exogenous subspace. This suggests that the CCC computation when applied to the simpliÔ¨Åed objective is\nnot Ô¨Ånding good solutions. To evaluate this possibility, we reran the M3 experiment with a larger value of œµ = 0.1\ninstead of its default value of 0.05. With this change, the results improve dramatically for SimpliÔ¨Åed-GRDS and SRAS,\nand they are able to match the performance of the Endo Reward Oracle. However, the performance of GRDS does\nnot improve, which suggests that it is not able to Ô¨Ånd good solutions to the large manifold optimization problems that\nit is solving. This could be because the CCC objective is confused by the nonlinear dynamics or it could be that the\noptimization is trapped in local minima.\n5.3.3\nCombinatorial Action Spaces\nA limitation of our experiments so far has been that the action space is one-dimensional. We have seen that this implies\nthat the maximal exogenous subspace has dimension n + m ‚àí1 rather than n as originally intended. In this section, we\ndescribe experiments where we introduce higher-dimensional action spaces. For example, with a 5-dimensional action\nspace, the policy must now select one of 10 values for each of the 5 action variables. In effect, the MDP now has 105\nprimitive actions.\nTo design MDPs with high-dimensional action spaces, we modify the general linear setting of Section 5.2, so that the\naction at ‚ààRl is an l-D vector, and the matrix Ma multiplying at is in Rm,l. As in the single-action setting, each action\nvariable takes 10 possible values evenly-spaced in [‚àí1, 1]. We consider 6 MDPs with a variety of different structures,\nwhich may in principle appear in real applications:\n‚Ä¢ 10-D MDP with m = n = 5 and l = 5. The action matrix Ma is dense, meaning that all its entries are nonzero.\nWe sample the entries in Ma from the uniform distribution over [0, 1) and subsequently normalize each row of\nMa to sum to 0.99 for stability. We apply the decomposition algorithms after L = 6000 steps and train for a\ntotal of N = 200000 steps.\n‚Ä¢ 20-D MDP with m = n = 10 and l = 10. The action matrix Ma is dense, and generated as above. We set\nL = 10000 and N = 200000.\n‚Ä¢ 30-D MDP with m = n = 15 and l = 8. The action matrix Ma is partial dense, meaning that only l = 8\nout of the m = 15 endogenous states are controlled, but these 8 states are controlled by all actions. Ma\nis generated as above, except that the rows corresponding to non-controlled endo variables are 0. We set\nL = 15000 and N = 200000.\n29\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nAction\nExo/Endo\nMethod\nExo Subspace\nTotal Time\nDecomposition\nVariables\nState Variables\nRank\n(secs)\nTime (secs)\n5 (dense)\n5/5\nBaseline\n-\n3576.8¬±293.2\n-\nGRDS\n7.47¬±0.50\n4835.2¬±960.6\n69.8¬±39.1\nSimpliÔ¨Åed-GRDS\n7.47¬±0.50\n4864.5¬±979.3\n148.1¬±70.2\nSRAS\n7.27¬±0.44\n4914.6¬±1126.0\n402.8¬±134.0\n10 (dense)\n10/10\nBaseline\n-\n6162.7¬±478.0\n-\nGRDS\n15.6¬±0.49\n7745.5¬±1299.4\n396.5¬±228.8\nSimpliÔ¨Åed-GRDS\n16.0¬±0.0\n7731.9¬±1292.3\n387.0¬±145.0\nSRAS\n16.0¬±0.0\n8064.7¬±2006.7\n1546.7¬±483.1\n8 (partial\ndense)\n15/15\nBaseline\n-\n8734.0¬±878.1\n-\nGRDS\n27.0¬±0.0\n10400.0¬±1599.6\n397.0¬±244.3\nSimpliÔ¨Åed-GRDS\n27.0¬±0.0\n10150.5¬±1452.2\n556.7¬±121.8\nSRAS\n26.93¬±0.25\n11448.9¬±4052.9\n3702.0¬±1730.1\n8 (partial\ndisjoint)\n15/15\nBaseline\n-\n8388.9¬±706.1\n-\nGRDS\n22.0¬±0.0\n10399.5¬±2200.5\n1136.4¬±433.9\nSimpliÔ¨Åed-GRDS\n22.0¬±0.0\n10362.2¬±1687.5\n1520.7¬±304.4\nSRAS\n22.0¬±0.0\n10389.7¬±1877.7\n1224.7¬±506.5\n10 (partial\ndisjoint\nsparse)\n15/20\nBaseline\n-\n12088.9¬±973.6\n-\nGRDS\n23.2¬±0.4\n16126.3¬±1096.9\n2799.9¬±473.5\nSimpliÔ¨Åed-GRDS\n5.47¬±1.89\n18627.7¬±3103.6\n7019.8¬±1427.1\nSRAS\n6.2¬±1.51\n13984.1¬±6876.4\n1976.3¬±1504.9\n20 (partial\ndisjoint\nsparse)\n30/40\nBaseline\n-\n35134.1¬±2877.3\n-\nGRDS\n50.0¬±0.0\n43345.1¬±3470.3\n10256.2¬±2719.9\nSimpliÔ¨Åed-GRDS\n50.0¬±0.0\n44187.4¬±3672.7\n10670.1¬±3015.7\nSRAS\n49.0¬±0.0\n44350.5¬±3540.4\n11691.1¬±2925.2\nTable 6: Mean and standard deviation of the rank of the discovered exo subspace, total execution time, and decomposition\ntime for MDPs with multiple action variables.\n‚Ä¢ 30-D MDP with m = n = 15 and l = 8. The action matrix Ma is partial disjoint, meaning that only l = 8 out\nof the m = 15 endo states are controlled but each of these 8 states is controlled by a distinct action variable.\nWe sample the l = 8 nonzero entries of Ma from the uniform distribution over [0.5, 1.5). We set L = 15000\nand N = 200000.\n‚Ä¢ 35-D MDP with m = 20, n = 15 and l = 10. The action matrix Ma is partial disjoint, i.e., only 10 endogenous\nstate variables are directly controlled through the 10 actions (each by a distinct action variable) whereas the\nremaining ones are controlled indirectly through the other endogenous states; furthermore, the endo transition\nmatrix Me is sparse with sparsity (fraction of nonzeros) 14.3%. SpeciÔ¨Åcally, Me is generated as in Section 5.2\nexcept that only a small part of the matrix (equal to 14.3%) is initialized to nonzero values. We set L = 20000\nand N = 200000.\n‚Ä¢ 70-D MDP with m = 40, n = 30 and l = 20. The action matrix Ma is partial disjoint,i.e., only 20 endogenous\nstates are directly controlled through the 20 actions whereas the remaining ones are controlled indirectly\nthrough the other endogenous states. The endo transition matrix Me is sparse with sparsity 14.3%. We set\nL = 35000 and N = 300000.\nFigures 10a-10f plot RL performance for these 6 MDPs. We run 15 replications with different seeds and a reward\nregression learning rate of 0.0006. A Ô¨Årst observation is that in all 6 settings the baseline struggles and shows very slow\nimprovement over time time (e.g., Figure 10b). Its performance appears to decay on the smallest of these MDPs (Figure\n10a). The Endo Reward Oracle performs visibly better than the Baseline and is able to attain higher rewards. However,\nit exhibits high variance and it improves very slowly (e.g., Figure 10b).\nSurprisingly, the SimpliÔ¨Åed-GRDS and SRAS methods substantially outperform the Endo Reward Oracle. It appears\nthat our methods are able to discover additional exogenous dimensions that reduce the variance of the endogenous\nreward below the level of the reward oracle. Table 6 conÔ¨Årms that, with the exception of the 35-dimensional sparse,\npartial disjoint MDP, the algorithms are all discovering exogenous spaces of the expected size. For example, in the\nlargest MDP, which has 70 dimensions and a 20-dimensional action space, GRDS and SimpliÔ¨Åed-GRDS both discover\na 50-dimensional exogenous space. The algorithms are challenged by the fourth MDP (n=15, m = 20, l = 10). On this\nMDP, GRDS Ô¨Ånds a 23.2-dimensional exo space on average, but SimpliÔ¨Åed-GRDS and SRAS only Ô¨Ånd exo spaces\nwith an average dimension of 5.47 and 6.2, respectively. They also exhibit high variation in the number of discovered\ndimensions. Despite these failures, SimpliÔ¨Åed-GRDS and SRAS perform very well on all six of these MDPs. GRDS\nstruggles on the dense MDPs, but does quite well on the sparse and partial disjoint MDPs.\n30\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n20\n40\n60\n80\n100\n120\n# of Policy Updates\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 200000\nDecomposition steps = 6000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nAction variables = 5\nRegression net learning rate = 0.0006\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(a) Dense 10-D MDP.\n0\n20\n40\n60\n80\n100\n120\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 200000\nDecomposition steps = 10000\nPolicy update steps = 1536\nExo variables = 10\nEndo variables = 10\nAction variables = 10\nRegression net learning rate = 0.0006\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(b) Dense 20-D MDP.\n0\n20\n40\n60\n80\n100\n120\n# of Policy Updates\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nTrue Endo Reward\nTotal training steps = 200000\nDecomposition steps = 15000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nAction variables = 8\nRegression net learning rate = 0.0006\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(c) Partial dense 30-D MDP.\n0\n20\n40\n60\n80\n100\n120\n# of Policy Updates\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 200000\nDecomposition steps = 15000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nAction variables = 8\nRegression net learning rate = 0.0006\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(d) Partial disjoint 30-D MDP.\n0\n20\n40\n60\n80\n100\n120\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 200000\nDecomposition steps = 20000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 20\nAction variables = 10\nRegression net learning rate = 0.0006\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(e) Partial disjoint sparse 35-D MDP.\n0\n50\n100\n150\n200\n# of Policy Updates\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 300000\nDecomposition steps = 35000\nPolicy update steps = 1536\nExo variables = 30\nEndo variables = 40\nAction variables = 20\nRegression net learning rate = 0.0006\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(f) Partial disjoint sparse 70-D MDP.\nFigure 10: RL performance for MDPs with multiple action variables.\n5.3.4\nDiscrete MDPs\nThe Ô¨Ånal variation in MDP structure that we studied was to create an MDP with discrete states. Figure 11 shows a\nsimple routing problem deÔ¨Åned on a road network. There are 9 endogenous states corresponding to the nodes of the\nnetwork. This MDP is episodic; each episode starts in the starting node, v0, and ends in the terminal node v8. Each\nedge in the network has a corresponding traversal cost, and the goal of the agent is to reach the terminal node while\nminimizing the total cost. There are 4 exogenous state variables; each of them is independent of the others and evolves\nas xt+1,i = 0.9 ¬∑ xt,i + Œµexo, where Œµexo is distributed according to N(0, 1) and i ‚àà{1, 2, 3, 4}. These exogenous state\nvariables are intended to model global phenomena such as amount of automobile trafÔ¨Åc, fog, snow, and pedestrian\ntrafÔ¨Åc. These quantities evolve independently of the navigation decisions of the agent, but they modify the cost of\ntraversing the edges.\n31\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nThe reward function is the sum of two terms:\nrt = ‚àícost(st ‚Üíst+1) ‚àí\n4\nX\ni=1\nxt,i.\nThe Ô¨Årst term is the endogenous reward Rend,t and the second term is the exogenous reward Rexo,t.\nThe actions at each node consist in choosing one of the outbound edges to traverse. We restrict the set of actions to\nmove only rightward (i.e., toward states with higher subscripts). For instance, there are three available actions at node\nv0 corresponding to the three outgoing edges, but only a single action at node v4. The cost of traversing an edge is\nshown by the edge weights in Figure 11. The MDP is deterministic: a given action (i.e., edge selection) at a given node\nalways results in the same transition. The observed state consists of the 1-hot encoding for the 9 endo states plus the\n4-D continuous exo state variables.\nWe apply episodic RL with PPO. Since the MDP is small, we modify some of the hyperparameters as follows. We\nset the total training steps to N = 10000 and the policy update steps to K = 128. We perform decomposition after\nL = 300 episodes instead of the default value of 3,000 steps. The PPO batch size is set to 32. Every time we update the\npolicy, we execute 300 evaluation episodes (in a separate instance of the MDP environment). For reward regression,\nwe compute a Single Linear Regression rather than Online Neural Net Regression (see Section 5.2.3). We perform 15\nreplications, each with a different random seed.\nFigure 12a plots RL performance. Note that our methods perform on par with the Endo Reward Oracle and exhibit\nvery little variance, with the exception of SRAS. On the other hand, the Baseline makes very slow progress. Table\n7 reports the rank of the discovered exogenous subspace as well as the total time and decomposition time. GRDS\nand SimpliÔ¨Åed-GRDS discover large exogenous subspaces of rank 11 and 10, respectively. The computed exogenous\nsubspace includes the 4 exogenous state variables we deÔ¨Åned as well as linear combinations of the endogenous states\nthat do not depend or have only weak dependence on the action. In contrast, SRAS discovers an exo subspace of very\nlow rank, which explains its suboptimal performance.\nThe strong performance of our methods might be due to deterministic dynamics of the problem. To test this, we created\na stochastic version of the road network MDP. The transition probabilities are speciÔ¨Åed as follows:\n‚Ä¢ Taking action 0 / 1 / 2 at node v0 leads to nodes v1, v2, v4 with probabilities (0.5, 0.3, 0.2) /(0.3, 0.5, 0.2) /\n(0.3, 0.2, 0.5), respectively.\n‚Ä¢ Taking action 0 / 1 at node v1 leads to nodes v4, v5 with probabilities (0.6, 0.4) / (0.5, 0.5), respectively.\n‚Ä¢ Taking action 0 / 1 at node v2 leads to nodes v3, v4 with probabilities (0.5, 0.5) / (0.3, 0.7), respectively.\n‚Ä¢ Taking action 0 / 1 at node v3 leads to nodes v6, v7 with probabilities (0.7, 0.3) / (0.4, 0.6), respectively.\n‚Ä¢ Taking action 0 / 1 / 2 at node v4 leads to nodes v5, v6, v8 with probabilities (0.6, 0.2, 0.2) /(0, 1, 0) /\n(0.3, 0.2, 0.5), respectively.\n‚Ä¢ There is only one action (action 0) available at nodes v5, v6, v7, and this leads with probability 1 to nodes\nv8, v7, v8, respectively.\n‚Ä¢ Node v8 remains a terminal node.\nWe employ the same hyperparameters as in the deterministic setting, except that we set the total training steps\nN := 20000 and the number of decomposition episodes L := 600.\nFigure 12b plots the RL performance over 15 trials (each with a separate random seed). We observe that SimpliÔ¨Åed-\nGRDS and GRDS perform only slightly worse than the Endo Reward Oracle, while SRAS exhibits slightly lower average\nperformance and higher variance. The Baseline improves very slowly and lags far behind the other methods. Due to the\nstochastic transitions, all methods (and particularly the Baseline) exhibit larger variance than in the deterministic MDP\nin Figure 12a.\nùë£0\nùë£1\nùë£2\nùë£3\nùë£4\nùë£5\nùë£6\nùë£7\nùë£8\n2\n5\n3\n2\n2\n3\n2\n3\n3\n8\n2\n4\n4\n4\n2\nFigure 11: Graph for discrete MDP.\n32\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n20\n40\n60\n80\n# of Policy Updates\n11.5\n11.0\n10.5\n10.0\n9.5\n9.0\nTrue Endo Reward\nTotal training steps = 10000\nDecomposition episodes = 300\nPolicy update steps = 128\nExo variables = 4\nEndo variables = 9\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(a) Deterministic MDP.\n0\n25\n50\n75\n100\n125\n150\n# of Policy Updates\n11.5\n11.4\n11.3\n11.2\n11.1\n11.0\nTrue Endo Reward\nTotal training steps = 20000\nDecomposition episodes = 600\nPolicy update steps = 128\nExo variables = 4\nEndo variables = 9\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(b) Non-deterministic MDP.\nFigure 12: RL performance for discrete MDPs.\nMDP\nExo/Endo State\nMethod\nExo Subspace\nTotal Time\nDecomposition\nVariables\nRank\n(secs)\nTime (secs)\nDeterministic\n4/9\nBaseline\n-\n225.6¬±16.7\n-\nGRDS\n9.47¬±0.50\n271.4¬±55.8\n68.4¬±26.9\nSimpliÔ¨Åed-GRDS\n11.0¬±0.0\n229.3¬±18.0\n10.2¬±1.9\nSRAS\n2.13¬±1.63\n272.9¬±39.2\n72.3¬±13.5\nNon-deterministic\n4/9\nBaseline\n-\n419.9¬±10.8\n-\nGRDS\n9.66¬±0.70\n503.6¬±102.2\n109.1¬±42.7\nSimpliÔ¨Åed-GRDS\n11.0¬±0.0\n446.6¬±21.5\n15.6¬±2.1\nSRAS\n2.8¬±1.38\n488.0¬±56.0\n82.7¬±9.7\nTable 7: Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition\ntime for the discrete MDPs.\nRegarding the rank of the computed subspace, we notice from Table 7 that the ranks of the discovered exo subspaces\nare the same as for the deterministic MDP. This demonstrates the robustness of the algorithms. SimpliÔ¨Åed-GRDS Ô¨Ånds\nthe largest exogenous space with GRDS close behind. SRAS Ô¨Ånds much smaller spaces, which partially explains its\nslightly poorer performance. It is important to remember that not all dimensions of the discovered exogenous subspaces\nmay be relevant to reward regression. SRAS may only be discovering 2.8 dimensions, on average, but these are enough\nto give it a huge performance advantage over the Baseline.\nThe total CPU cost and decomposition cost are higher than the deterministic setting, which reÔ¨Çects the added cost of\nrunning online reward regression for twice as many steps.\nIn this problem, the rank-descending methods worked better than SRAS, but even SRAS gave excellent results, and the\ntotal CPU time required is not signiÔ¨Åcantly higher than the Baseline.\n5.3.5\nRQ4 Summary\nThe experiments demonstrate that our methods are able to handle nonlinear rewards, combinatorial action spaces, and\ndiscrete state spaces without modiÔ¨Åcation. When we introduced nonlinear transition dynamics, however, all of our\nmethods performed poorly on at least one of the three MDPs tested. Excellent performance for SimpliÔ¨Åed-GRDS and\nSRAS was restored by increasing the value of œµ, but this did not improve GRDS very much. This is additional evidence\nthat the high dimensional manifold optimization problems that GRDS solves are difÔ¨Åcult, particularly when the CCC is\ncomputed in a nonlinear setting.\n5.4\nAnti-correlated Exo and Endo Rewards\nRecall in Section 2.3 we showed that if the endogenous and exogenous rewards are negatively correlated, the variance\nof the endogenous reward can be less than the variance of the full reward and there is no variance reduction beneÔ¨Åt\nfrom computing the exo/endo reward decomposition. SpeciÔ¨Åcally, Theorem 6 introduced the covariance condition\nthat the variance of the exogenous reward must be greater than twice the negative covariance between the exogenous\n33\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n5\n10\n15\n20\n# of Policy Updates\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nTrue Endo Reward\nTotal training steps = 30000\nDecomposition steps = 1000\nPolicy update steps = 1536\nExo variables = 1\nEndo variables = 1\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(a) 2-D MDP (n = 1).\n0\n5\n10\n15\n20\n25\n# of Policy Updates\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nTrue Endo Reward\nTotal training steps = 40000\nDecomposition steps = 2000\nPolicy update steps = 1536\nExo variables = 2\nEndo variables = 2\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(b) 4-D MDP (n = 2).\nFigure 13: RL performance for MDPs with anti-correlated exo and endo rewards.\nand endogenous rewards. RQ5 asks how our algorithms behave when this covariance condition is violated. Does RL\nperformance suffer?\nTo study this question, we deÔ¨Åne the following family of 2n-D MDPs with a nonlinear exogenous reward\nxt+1 = 0.9 ¬∑ Mexo ¬∑ xt + Œµexo\net+1 = 0.45 ¬∑ Mend ¬∑ et + 0.55 ¬∑ Mexo ¬∑ xt + Ma ¬∑ at + Œµend\nRend,t = e‚àí|avg(et)‚àí2.5|/3\nRexo,t = e‚àí|avg(xt)+2.5|/3.\nThe endo and exo states et and xt are both n-D. The matrices Mexo ‚ààRn√ón and Mend ‚ààRn√ón follow the recipe\ngiven in Section 5.2. Ma ‚ààRn is the coefÔ¨Åcient for the action at, and it is set to be a vector of ones. The elements of\nthe exogenous noise Œµexo ‚ààRn are distributed according to N(0, 0.16), while the elements of the endogenous noise\nŒµend ‚ààRn are distributed according to N(0, 0.04). The observed state vector st ‚ààR2n is a linear mixture of the\nhidden exogenous and endogenous states, as in Section 5.2.\nAn interesting property of this class of MDPs is that the product Mexo ¬∑ xt appears with a relatively high coefÔ¨Åcient\nin both xt+1 and et+1. Hence, et+1 and xt+1 will exhibit positive correlation. As a result, when avg(et) (and thus\navg(xt)) is high and close to 2.5, Rend,t will be high and Rexo,t will be low. Conversely, when avg(et) (and thus\navg(xt)) is low and close to ‚àí2.5, Rend,t will be low and Rexo,t will be high. This causes the endo and exo rewards to\nbe anti-correlated to the point that the covariance condition is violated.\nSo verify this, we measured the variance Var(Rexo,t) and the covariance ‚àí2 ¬∑ Cov(Rend,t, Rexo,t) for both MDPs\nunder 2 policies: a random policy and the optimal policy. For the 2-D MDP, we Ô¨Ånd that Var(Rexo,t) = 0.026 and ‚àí2 ¬∑\nCov(Rend,t, Rexo,t) = 0.04 under a random policy, and Var(Rexo,t) = 0.026 and ‚àí2 ¬∑ Cov(Rend,t, Rexo,t) = 0.037\nunder the optimal policy. For the 4-D MDP, we Ô¨Ånd that Var(Rexo,t) = 0.021 and ‚àí2 ¬∑ Cov(Rend,t, Rexo,t) = 0.033\nunder a random policy, and Var(Rexo,t) = 0.021 and ‚àí2 ¬∑ Cov(Rend,t, Rexo,t) = 0.040 under the optimal policy.\nHence, the covariance condition is violated in all cases.\nFigures 13a and 13b show the RL performance for the different methods when n = 1 and n = 2, respectively.The\nexo subspace ranks and time costs are summarized in Table 8. Since the MDPs are rather small, for the former we set\nL = 1000 for the number of steps prior to applying the state-space decomposition methods and train for N = 30000.\nFor the latter set L = 2000 and N = 40000. We perform 15 trials with different seeds.\nNotice how the Endo Reward Oracle performs worse than the Baseline; this is to be expected, because the Endo Reward\nOracle is forced to use the Endo reward, which has higher variance than the full reward. Our naive expectation was that\nour decomposition methods would exhibit the same behavior, since they seek to Ô¨Ånd the maximal exogenous subspace.\nHowever, what we observe is that all of our methods do much better than the Endo Reward Oracle and, in the case of\nn = 1, they exceed the Baseline as well.\nThe key to understanding what is happening is to focus on the reward regression. The loss function for the reward\nregression is precisely the residual variance of the predicted reward, and the residual reward is precisely our estimate\nof the endogenous reward. Consequently, reward regression will ignore any exogenous variables that would cause an\nincrease in the variance of the endogenous reward. In the case of n = 1, the reward regression has apparently found\n34\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nsome component of the exogenous reward that is not anti-correlated with the endogenous reward and exploited it to\nreduce the variance of the endogenous reward. Consequently our methods are able to learn faster than the Baseline.\nThe larger result is that our algorithms can never be affected by anti-correlated rewards. In the worst case, the reward\nregression will ignore the exogenous variables and simply predict the mean exogenous reward. This will have no effect\non the endo-MDP or the RL performance of our methods.\nExo State\nEndo State\nMethod\nExo Subspace\nTotal Time\nDecomposition\nVariables\nVariables\nRank\n(secs)\nTime (secs)\n1\n1\nBaseline\n-\n221.9¬±9.7\n-\nGRDS\n1.0¬±0.0\n276.0¬±38.3\n2.2¬±0.7\nSimpliÔ¨Åed-GRDS\n1.0¬±0.0\n276.9¬±36.8\n2.2¬±0.6\nSRAS\n1.0¬±0.0\n276.6¬±39.6\n2.3¬±0.8\n2\n2\nBaseline\n-\n308.4¬±9.4\n-\nGRDS\n2.4¬±0.8\n402.8¬±67.0\n22.5¬±25.2\nSimpliÔ¨Åed-GRDS\n3.0¬±0.0\n394.3¬±55.5\n6.5¬±0.8\nSRAS\n2.26¬±0.93\n403.8¬±80.6\n24.0¬±23.1\nTable 8: Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition\ntime for the MDPs with anti-correlated exo and endo rewards.\n5.5\nRisks and BeneÔ¨Åts of SimpliÔ¨Åed-GRDS\nOur experimental results suggest that the SimpliÔ¨Åed-GRDS method performs very well. RQ6 asks about the risks\nof the SimpliÔ¨Åed-GRDS method. From a mathematical perspective, when we minimize I(X‚Ä≤; A | X) (or the CCC\napproximation), we are only eliminating edges from A to X‚Ä≤, which are the direct effects of A. We are not excluding any\nindirect path by which A might affect E‚Ä≤, and E‚Ä≤ might then affect X in some future time step. In the linear dynamical\nMDPs that we have studied, such indirect effects do not arise, and all effects of A on X‚Ä≤ are visible immediately. Of\ncourse, because SimpliÔ¨Åed-GRDS veriÔ¨Åes the full constraint I(X‚Ä≤; [E, A] | X) < œµ, SimpliÔ¨Åed-GRDS is still sound,\nbut it may fail to Ô¨Ånd the maximal exogenous subspace in complex MDPs (as we saw in some of the modiÔ¨Åed MDPs).\nThis suggests that for general MDPs, we should choose GRDS rather than SimpliÔ¨Åed-GRDS. However, our experiments\nhave also shown that there are several cases where GRDS encounters difÔ¨Åculty minimizing the CCC proxy of the\nI(X‚Ä≤; [E, A] | X) objective. Hence, we believe additional research is needed to improve the manifold optimization\nprocess so that GRDS can overcome these challenging cases.\n5.6\nPractical Considerations\nIn this section, we switch our attention to practical aspects of our proposed methods. Our goal is to answer RQ7, which\nconcerns how to set the hyperparameters of our method.\n5.6.1\nImpact of Hyperparameters on Baseline\nFirst, we investigate the impact of hyperparameters on the Baseline. For this purpose, we consider the High-D linear\nsetting of Section 5.2. Recall that for this setting we used the default PPO and Adam hyperparameters in stable-\nbaselines3, summarized in Table 2. We now ask whether the performance of Baseline can be improved by using\ndifferent hyperparameters. If that were the case, then a more careful hyperparameter tuning could be an alternative to\nour proposed algorithms.\nWe consider the 5-D MDP with 3 exo and 2 end variables of Figure 6a, where the Baseline Ô¨Çuctuates between 0.4 and\n0.5 with an average of 0.45, visibly lower than all other methods. To understand whether this can be improved further,\nwe tune 3 critical hyperparameters of PPO optimization. We let the batch size take values in {16, 32, 64, 128, 256, 512}\n(Figure 14a), the learning rate take values in {5 √ó 10‚àí5, 1 √ó 10‚àí4, 5 √ó 10‚àí4, 1 √ó 10‚àí3, 5 √ó 10‚àí3, 1 √ó 10‚àí2} (Figure\n14b), and the number of steps per policy update K take values in {250, 500, 750, 1000, 2000, 3000} (Figure 14c). For\neach experiment, all hyperparameters except for the tuned one are set to the values in Figure 6a. For Figure 14c, we\nperform policy evaluation every 1536 steps, instead of after each policy update, to ensure that all curves have the same\nnumber of evaluation points. We use 10 independent replications with different seeds. Finally, we increase the number\nof training steps to N = 80000 to ensure that the Baseline has enough training budget to converge.\nThe results demonstrate that none of the parameter combinations can raise performance signiÔ¨Åcantly. Some values for\nthe number of policy update steps manage to slightly improve the average Baseline performance to 0.5 from 0.45, but\n35\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 3000\nPolicy update steps = 1536\nLearning rate = 0.0003\nExo variables = 3\nEndo variables = 2\nReplications = 10\nBaseline\nBatch size = 16\nBatch size = 32\nBatch size = 64\nBatch size = 128\nBatch size = 256\nBatch size = 512\n(a) Tuning batch size.\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.35\n0.40\n0.45\n0.50\n0.55\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 3000\nPolicy update steps = 1536\nBatch size = 64\nExo variables = 3\nEndo variables = 2\nReplications = 10\nBaseline\nLearning rate = 0.00005\nLearning rate = 0.0001\nLearning rate = 0.0005\nLearning rate = 0.001\nLearning rate = 0.005\nLearning rate = 0.01\n(b) Tuning learning rate.\n0\n10\n20\n30\n40\n50\n# of Evaluations\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 3000\nLearning rate = 0.0003\nBatch size = 64\nExo variables = 3\nEndo variables = 2\nReplications = 10\nBaseline\nPolicy update steps = 250\nPolicy update steps = 500\nPolicy update steps = 750\nPolicy update steps = 1000\nPolicy update steps = 2000\nPolicy update steps = 3000\n(c) Tuning policy update steps K.\nFigure 14: RL performance for Baseline of Figure 6a under various hyperparameters.\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 10\nSimplified-GRDS\nDecomposition steps = 250\nDecomposition steps = 500\nDecomposition steps = 750\nDecomposition steps = 1000\nDecomposition steps = 1500\nDecomposition steps = 2000\nDecomposition steps = 2500\nDecomposition steps = 3000\nDecomposition steps = 4000\nDecomposition steps = 5000\nDecomposition steps = 6000\nDecomposition steps = 7000\n(a) SimpliÔ¨Åed-GRDS.\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 10\nSRAS\nDecomposition steps = 250\nDecomposition steps = 500\nDecomposition steps = 750\nDecomposition steps = 1000\nDecomposition steps = 1500\nDecomposition steps = 2000\nDecomposition steps = 2500\nDecomposition steps = 3000\nDecomposition steps = 4000\nDecomposition steps = 5000\nDecomposition steps = 6000\nDecomposition steps = 7000\n(b) SRAS.\nFigure 15: RL performance for varying values of L, the number of steps prior to applying the decomposition algorithms,\nfor the 10D setting of Figure 6b.\nthey still suffer from signiÔ¨Åcant variance. This is in sharp contrast to our algorithms and the Endo Reward Oracle in\nFigure 6a, which all exhibit much lower variance. Given that this experiment only tunes one hyperparameter at a time,\nwe cannot exclude the possibility that there are combinations of hyperparameters that can achieve a higher and more\nstable performance for the Baseline. However, its low performance under the default hyperparameters and on a range of\nreasonable values provides evidence that its poor performance mainly stems from the stochasticity of the exogenous\nrewards and not badly-chosen hyperparameters.\n36\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n2\n4\n6\n8\n10\nIndex i\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\nAngle (radians)\nTotal training steps = 60000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 10\nAlgorithm\nSimplified-GRDS\nSRAS\nFigure 16: Angle between orthogonal complements of computed exo subspaces corresponding to Li and Li+1\ndecomposition steps, where the index i ranges from 0 to 10.\n5.6.2\nSensitivity Analysis\nRecall from Section 5.1 that the main hyperparameter we need to set for our proposed methods is the number of\nsteps L prior to applying the exogenous subspace discovery algorithms. This speciÔ¨Åes the number of ‚ü®s, a, r, s‚Ä≤‚ü©\ntuples that are collected for subspace discovery. In this section, we shed light on the impact of L on RL performance.\nIn this direction, we consider the 10-D setting with 5 exo and 5 endo variables of Figure 6b, where our methods\nand the Endo Reward Oracle converge to a total reward of 0.8 in N = 50000 steps. In contrast, the Baseline only\nattains a reward of around 0.3 on average. We perform sensitivity analysis by considering twelve possible values\nfor L: 250, 500, 750, 1000, 1500, 2000, 2500, 3000, 4000, 5000, 6000, 7000. We denote each value by the increasing\nsequence Li, i ‚àà{0, . . . , 11}, where L0 = 250 and L10 = 7000. We report the average RL performance for the\nSimpliÔ¨Åed-GRDS and SRAS methods in Figure 15(a,b), since they both perform very well. For both methods, when\nL0 = 250, performance is barely better than the Baseline. As we increase L, performance improves steadily and almost\nmatches the Endo Reward Oracle as soon as we reach 2000 decomposition steps. Increasing L beyond 2000 gives\nminor beneÔ¨Åt and delays the time at which PPO can take advantage of the improved reward function.\nThis suggests that the decomposition algorithms have converged after L = 2000. Can we verify this? The rank of the\ndiscovered exogenous space is not informative, because it is always 9-dimensional for all 12 values of L and across all\n10 replications. To get a Ô¨Åner-grained measure, we can take advantage of the fact that the complement of the discovered\n9-dimensional exogenous space is a 1-dimensional space. This means it can be represented by a direction vector, and\nwe can compare different solutions by computing the angles between these direction vectors.\nFigure 16 depicts the angle (in radians) between the orthogonal complements of the exogenous subspaces for each\nconsecutive pair of Li and Li+1 values. If the methods had converged, these angles would be zero. They are not zero,\nwhich shows that the exogenous subspaces are continuing to change as L increases. But the angles are all very small\n(less than 0.5 degrees in the largest case), so these changes are not large, and they are converging (with few exceptions)\nmonotonically toward zero. SimpliÔ¨Åed-GRDS exhibits smooth convergence, whereas SRAS shows higher variance and\na few bumps.\nThe results suggest that manifold optimization performs very well on this problem, even with relatively small numbers\nof samples. What is then the reason for the different performance levels in Figure 15? The answer lies in the exogenous\nreward regression. Recall that after L steps, we conclude Phase 1 by computing the decomposition and then Ô¨Åtting the\nexogenous reward neural net to the L collected observations. Even though different numbers of decomposition steps\nresult in almost identical exogenous subspaces, the subsequent exogenous reward regression can yield dramatically\ndifferent exo reward models. When the value of L is very low, we have only a limited number of samples for the exo\nreward regression, and these might not to cover the exo state subspace adequately. As a result, the learned exo reward\nmodel may overÔ¨Åt the observations and fail to generalize to other subspaces. The subsequent online neural network\nreward regression in Phase 2 only processes each new observation once, so learning the correct exo reward model can\ntake many steps, and cause PPO to learn slowly.\nTo conÔ¨Årm the above, we perform a second experiment. Unlike previous experiments, we decouple decomposition and\nexogenous reward regression. Decomposition still takes place after Li steps. But reward regression is now performed\n37\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nPolicy update steps = 1536\nRegression steps = 3000\nReplications = 10\nSimplified-GRDS\nDecomposition steps = 250\nDecomposition steps = 500\nDecomposition steps = 750\nDecomposition steps = 250,\n Single Regression\nDecomposition steps = 500,\n Single Regression\nDecomposition steps = 750,\n Single Regression\nDecomposition steps = 250,\n Repeated Regression\nDecomposition steps = 500,\n Repeated Regression\nDecomposition steps = 750,\n Repeated Regression\n(a) SimpliÔ¨Åed-GRDS\n0\n10\n20\n30\n40\n# of Policy Updates\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nPolicy update steps = 1536\nRegression steps = 3000\nReplications = 10\nSRAS\nDecomposition steps = 250\nDecomposition steps = 500\nDecomposition steps = 750\nDecomposition steps = 250,\n Single Regression\nDecomposition steps = 500,\n Single Regression\nDecomposition steps = 750,\n Single Regression\nDecomposition steps = 250,\n Repeated Regression\nDecomposition steps = 500,\n Repeated Regression\nDecomposition steps = 750,\n Repeated Regression\n(b) SRAS\nFigure 17: RL performance when computing the exo/endo decomposition at Li = 250, 500, and 750 steps. Reward\nregression starts at 3000 steps. Default: online neural network regression; Single Regression: single linear regression at\n3000 steps; Repeated Regression: linear regression every 256 steps.\nafter 3000 samples have been collected (3000 is the default value for L in the high-D experiments). After learning\nthe exogenous reward model with the 3000 samples, we proceed to Phase 2. We experiment with three options for\nexogenous reward regression in Phase 2: (i) standard online learning where we update the exogenous reward model\nevery M = 256 steps; (ii) a single linear regression after which the exogenous reward model is never updated; and (iii)\nrepeated linear regression where we Ô¨Åt a new exogenous reward model from scratch every M = 256 steps. We study\nthe three lowest values for Li (L0 = 250, L1 = 500 and L2 = 750), as these were the values in Figure 15 with the\nworst performance.\nFigure 17 plots the RL performance averaged over 10 independent trials. We make several observations. First, increasing\nthe number of steps for learning the initial exogenous reward model from Li to 3000 improves RL performance. With\nonline learning, we match the Endo Reward Oracle‚Äôs average performance of 0.8. This conÔ¨Årms our previous hypothesis\nthat the reason for the bad performance in Figure 15 was the poor initial exogenous reward model. With a single\nregression, RL performance improves (especially for L0 = 250 and L1 = 500), but it performs worse than online\nlearning while having greater variance. Interestingly, learning a new linear regression model every M = 256 steps\nperforms the best and slightly outperforms online neural network regression. A plausible reason for this is that online\nregression only performs a single pass over the data, so it may adapt to the changing state and reward distribution more\nslowly. On the other hand, repeated linear regression requires 10 times as much computation time as online regression\nfor the settings in Figures 17a and 17b.\n5.6.3\nPractical Guidelines For State Decomposition and Exo Reward Regression\nThis sensitivity analysis suggests the following procedure for computing state space decompositions and reward\nregressions. Start with a small number for L (e.g., 250) and compute the corresponding exo subspace after L steps.\nThen, every ‚àÜL steps (e.g., 250), recompute the exo subspace until the discovered exo subspace stops changing.\nThis can be detected when (i) the rank of the subspace does not change, and (ii) the largest principal angle between\nconsecutive subspaces is close to 0 [Bj¬®orck and Golub, 1973].\nAs soon as we have an initial exo subspace, we can Ô¨Åt the exogeneous reward model, and each time we recompute\nthe subspace, we can re-Ô¨Åt the model. These initial Ô¨Åts could be performed with linear regression or neural network\nregression. Once the exogenous subspace as converged, we can switch to online neural network regression, because the\nregression inputs will have stabilized.\nApplication constraints may suggest alternative procedures. If each step executed in the MDP is very expensive, then the\ncost of the multiple decomposition and reward regression computations is easy to justify. However, if MDP transitions\nare cheap, then we need to take a different approach. If many similar MDPs will need to be optimized, we can use\nthis full procedure on a few of them to determine the value of L at which the exo space converges. We then just use\nthat value to trigger exo/endo decomposition and perform neural network reward regression starting at step L and\ncontinuing online. If there is only one MDP to be solved, L could be selected using a simulation of the MDP. In all of\nour experiments, we have followed this procedure for setting L.\n38\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nA last question concerns the value for the CCC threshold œµ. In theory, we should use very low values to minimize the\nchance of discovering an invalid exo/endo state decomposition, but this could come at the cost of having to perform\nmore steps in GRDS and SRAS. We lack theoretical guidance for making this decision. In principle, one could start\nwith a somewhat large value for œµ (e.g., 0.1) and perform multiple runs (in simulation or on a sample of MDPs) with\nprogressively smaller values of œµ until either the discovered exo subspace converges or RL performance stabilizes.\n5.7\nDiscussion\nLet us review the research questions and our results.\nRQ1: Do our methods speed up reinforcement learning in terms of sample complexity? In terms of total CPU time?\nWe found that our methods yield dramatic speedups for reinforcement learning both in terms of the number of\nMDP transitions observed and in terms of clock time. The improvements are so dramatic that we were not able\nto run the Baseline method long enough for it to match the level of RL performance achieved by our methods,\nso we estimated this by linear extrapolation.\nRQ2: Do our methods discover the correct maximal exogenous subspaces? Our methods surprised us by Ô¨Ånding\nexogenous subspaces that were larger than we had naively expected. Analysis showed that these subspaces did\nindeed have maximal dimension. Our experiments measured the correctness of the subspaces by the resulting\nRL performance. In nearly all cases, our methods matched the performance of the Endo Reward Oracle that\nwas told the correct subspace for the endogenous reward. In the 2-D covariance condition experiment, our\nmethods did better than the Endo Reward Oracle.\nRQ3: What is the best approach to reward regression? Single linear, repeated linear, or online neural network\nregression? We found that online neural network regression worked best and was more stable, even in cases\nwhere the reward function was linear.\nRQ4: How do the algorithms behave when the MDPs are changed to have different properties: (a) rewards are\nnonlinear, (b) transition dynamics are nonlinear, (c) action space is combinatorial, and (d) states and actions\nare discrete? Our methods, combined with neural network regression, handled the nonlinear rewards well.\nWhen the transition dynamics are nonlinear, our methods sometimes struggled, but we found that they worked\nwell if we increased the œµ parameter to allow the CCC objective to be substantially nonzero. Our methods\nworked very well in combinatorial action spaces and in discrete MDPs.\nRQ5: How do the algorithms behave when the covariance condition is violated? When the covariance condition is\nviolated, our methods still give excellent results. This is because reward regression minimizes the variance\nof the endogenous reward, so it only Ô¨Åts that aspect of the exogenous reward that can be removed without\nincreasing the variance of the endogenous reward.\nRQ6: What are the risks and beneÔ¨Åts of using the simpliÔ¨Åed objective in SRAS and SimpliÔ¨Åed-GRDS? The\nSimpliÔ¨Åed-GRDS method gave the best overall RL performance. It provides a nice balance of computational\nefÔ¨Åciency and optimization performance while remaining sound.\nRQ7: How should hyperparameters be set? How many training tuples should be collected before performing\nexogenous subspace discovery? When should reward regression begin, and on what schedule? We found that\nthe default values of the hyperparameters worked well, and we developed a tuning procedure for determining\nthe training set size K for exogenous subspace discovery. It starts with K = 250 and repeatedly adds 250\ntuples until the exo space stabilized. Small values of K worked surprisingly well.\nThe beneÔ¨Åts of our algorithms are not necessarily limited to linear MDPs. We observed strong performance with\ngeneral nonlinear exo reward functions, and often on MDPs with nonlinear exo/endo transition dynamics but where\nthe state space could be linearly decomposed into the exo and endo subspaces. This showed that our CCC objective\ncan provide good results even in these nonlinear cases. An alternative to our CCC-based linear approach would be to\nenforce the exo/endo decomposition using cross-covariance operators on a Reproducing Kernel Hilbert Space (RKHS).\nHowever, RKHS computations can be computationally challenging and are difÔ¨Åcult to scale to high-dimensional spaces\n[Fukumizu et al., 2008].\n6\nRelated Work\nIn this section, we review prior work on reinforcement learning with exogenous information that is not covered\nelsewhere in the paper. Efroni et al. [2022b] introduce the Exogenous Block Markov Decision Process (EX-BMDP)\nsetting to model environments with exogenous noise. Under the assumption that the endogenous state transition is\nnear deterministic, they propose the Path Predictive Elimination (PPE) algorithm. PPE learns a form of multi-step\n39\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\ninverse dynamics [Paster et al., 2021]. It can recover the latent endogenous model while being both sample efÔ¨Åcient\nand computationally efÔ¨Åcient. PPE runs in a reward-free setting and does not make any assumptions about the reward\nfunction. In their follow-up work, Efroni et al. [2022a] introduce the ExoMDP setting, which is a Ô¨Ånite state-action\nvariant of the EX-BMPD where the state directly decomposes into a set of endogenous and exogenous factors, while\nthe reward only depends on the endogenous state. They propose ExoRL, an algorithm that learns a near-optimal policy\nfor the ExoMDP with sample complexity polynomial in the number of endogenous state variables and logarithmic in\nthe number of exogenous components. Because the reward does not depend on the exogenous state variables, those\nvariables are not only exogenous but irrelevant, and they can be ignored during reinforcement learning. In our work, in\ncontrast, the exogenous variables can still be important for the policy, so the RL algorithm must consider them as inputs\nwhen learning the policy.\nIn a different thread of work, Chitnis and Lozano-P¬¥erez [2020] address the problem of learning a compact model of\nan MDP with endogenous and exogenous components for the purpose of planning. They consider only reducing the\nexogenous part of the state, given that is the part that induces the most noise, and assume that the reward function\ndecomposes into a sum over the individual effects of each exogenous state variable. They introduce an algorithm based\non the mutual information among the exogenous state variables that generates a compact representation, and they\nprovide conditions for the optimality of their method. Our assumption of additive reward decomposition is weaker than\ntheir per-state-variable assumption.\nAnother line of work related to exogenous information concerns curiosity-driven exploration by Pathak et al. [2017],\nwhere the goal is to learn a reward to enable the agent to explore its environment better in the presence of very sparse\nextrinsic rewards. It falls under the well-studied class of methods with intrinsic rewards [Bellemare et al., 2016, Haber\net al., 2018, Houthooft et al., 2016, Oh et al., 2015, Ostrovski et al., 2017, Badia et al., 2020] This work employs a\nself-supervised inverse dynamics model, which encodes the states into representations that are trained to predict the\naction. As a result, the learned representations do not include environmental features that cannot inÔ¨Çuence or are not\ninÔ¨Çuenced by the agent‚Äôs actions. Based on these representations, the agent can learn an exploration strategy that\nremoves the impact of the uncontrollable aspects of the environment. Note that the inverse dynamics objective is\ngenerally not sufÔ¨Åcient for control and does not typically come with theoretical guarantees [Rakelly et al., 2021]. Finally,\nwe mention that reinforcement learning with exogenous information has also been studied from a more empirical\nstandpoint in real problems such as learning to drive [Chen et al., 2021].\nFinally, our work shares similarities with other work on modeling the controllable aspects in the environment [e.g.,\nChoi et al., 2019, Song et al., 2020, Burda et al., 2019, Bellemare et al., 2012, Corcoll et al., 2022, Thomas et al.,\n2018]. The main difference is that in exo/endo MDPs, we capture the controllable versus uncontrollable aspects via\nthe exo/endo factorization, where endo (resp., exo) states correspond to the controllable (resp. uncontrollable) aspects.\nCombining the two families of approaches could be an avenue for future research. We also note the work by Yang et al.\n[2022], which proposes the dichotomy of control for return-conditioned supervised learning. This is accomplished\nby conditioning the policy on a latent variable representation of the future and introducing two conditional mutual\ninformation constraints that remove any information from the latent variable that has to do with randomness in the\nenvironment. However, unlike ours, their work is not concerned with exogenous state variables and decompositions.\n7\nConcluding Remarks\nIn this paper, we proposed a causal theory of exogeneity in reinforcement learning and showed that in causal models\nsatisfying the full and diachronic two-step DBN structures, the exogenous variables are causally exogenous. We intro-\nduced exogenous-state MDPs with additively decomposable rewards and proved that such MDPs can be decomposed\ninto an exogenous Markov reward process and an endogenous MDP such that any optimal policy for the endogenous\nMDP is an optimal policy for the original MDP. We studied the properties of valid exo/endo decompositions and proved\nthat there is a maximal exogenous subspace that contains all other exogenous subspaces. We also showed that not all\nsubsets of the maximal exogenous subspace deÔ¨Åne valid exo/endo decompositions.\nGiven an exogenous-state MDP, we wish to discover a useful exogenous subspace of its state space. We explored several\noptimization formulations based on conditional mutual information for discovering the exogenous state variables and\nestimating the exogenous reward. We developed two practical algorithms, GRDS and SRAS, for the case when the\nexogenous space is a linear projection of the full state space. Our algorithms use the conditional correlation coefÔ¨Åcient\n(CCC) as a measure for conditional independence and rely on solving a manifold optimization problem. Under the\nassumption that the exploration policy visits all states and tries all actions inÔ¨Ånitely often, we showed that GRDS\ndiscovers the maximal exogenous subspace. Furthermore, under the assumption of faithfulness, the exogenous subspace\nwe discover is guaranteed to contain only causally-exogenous components. We also introduced SimpliÔ¨Åed-GRDS,\nwhich employs a simpliÔ¨Åed CCC objective and then checks the solution to see if it satisÔ¨Åes the full CCC objective.\n40\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nOur experiments on a variety of high-dimensional linear dynamical MDPs demonstrated that our algorithms, particularly\nSimpliÔ¨Åed-GRDS and SRAS, can greatly accelerate reinforcement learning. Additional experiments showed that\nthese methods can also give excellent performance on MDPs with nonlinear rewards, nonlinear transition dynamics,\ncombinatorial action spaces, and discrete state spaces. The methods are robust to hyperparameter settings and do not\nrequire delicate hyperparameter tuning.\nThere are several important directions for future research. First, we lack a deep theoretical understanding of the manifold\noptimization problem corresponding to the CCC. It would be helpful to shed light on its sample complexity and on its\nloss landscape, which may contain local minima. It is also critical to elucidate the conditions under which the CCC can\ncapture conditional independence, at least for linearly decomposable MDPs. Second, we lack a rigorous theory of the\nimpact of exogenous reward regression on RL convergence. Such knowledge could help us design better exogenous\nreward regression methodologies. Third, in our work we constrain the exogenous rewards to be additive. However, it is\npossible that in real settings exogenous rewards are non-additive (e.g., multiplicative), and our decomposition theorem\nwill not hold. Investigating non-additive decompositions would be valuable. Fourth, a more precise mathematical\nanalysis of the simpliÔ¨Åed objective that we employ in the SimpliÔ¨Åed-GRDS and SRAS algorithms might help explain\ntheir strong empirical performance. Fifth, it would be nice to study nonlinear exo/endo decompositions, especially\nin the presence of nonlinear transition dynamics. Can efÔ¨Åcient, general strategies be discovered? Finally, are there\nalternatives to the assumptions (admissible MDP, fully-randomized policy, and faithfulness) that we adopted to prove\ncausal exogeneity of the subspaces discovered by GRDS?\nAcknowledgments\nWe would like to acknowledge support for this project from the National Science Foundation (NSF grant IIS-9988642),\nthe Multidisciplinary Research Program of the Department of Defense (MURI N00014-00-1-0637), and a gift to the\nOregon State University Foundation from Huawei. We thank Yuvraj Sharma for porting our implementation to Python\nand assisting with some of the experiments.\nA\nAdditional Theory on Exogenous State Variables\nDeÔ¨Ånition 1 for causal exogeneity is inherently structural: it deÔ¨Ånes causal exogeneity as a property of the structure\nof the causal graph. In this appendix, we exploit this to structurally characterize causally exogenous state variables\n(Appendix A.1) as well as the causal graphs with causally exogenous state variables (Appendix A.2).\nA.1\nStructural Characterization of Causally-Exogenous State Variables\nWe begin by providing a general structural condition in terms of the unrolled DBN of an MDP that guarantees causal\nexogeneity. To this goal, we introduce the structural notion of action-disconnected states; we subsequently show that\naction-disconnected state variables are causally exogenous.\nDeÔ¨Ånition 8 (Action-Disconnected State Variables). A state variable C is action-disconnected, if the unrolled decision\ndiagram contains no directed chain of the form At ‚Üí¬∑ ¬∑ ¬∑ ‚ÜíCœÑ, ‚àÄt ‚àà{0, . . . , H ‚àí1}, ‚àÄœÑ ‚àà{t + 1, . . . , H}.\nTo visualize action-disconnected state variables, consider the 3-state MDP of Figure 18a and assume a horizon H = 2.\nThe unrolled diagram for that value of H is depicted in Figure 18b. State variables S1 and S2 are action-disconnected,\nwhereas S3 is not due to the presence of the directed chains A ‚ÜíS‚Ä≤\n3 ‚ÜíS‚Ä≤‚Ä≤\n3 ‚ÜíS‚Ä≤‚Ä≤‚Ä≤\n3 , A‚Ä≤ ‚ÜíS‚Ä≤‚Ä≤\n3 ‚ÜíS‚Ä≤‚Ä≤‚Ä≤\n3 , and A‚Ä≤‚Ä≤‚Ä≤ ‚ÜíS‚Ä≤‚Ä≤‚Ä≤\n3 .\nTheorem 9. If a state variable C is action-disconnected, then C is causally exogenous.\nProof. Let G be the graph corresponding to the unrolled decision diagram of the MDP from timestep 0 to timestep H.\nDeÔ¨Åne a trail as a loop-free and undirected (i.e., all edge directions are ignored) path between two nodes. We will make\nuse of the d-separation theory with trails [see Pearl, 1988, 2009]. Furthermore, recall Rule 3 of the do-calculus for any\ncausal graph G\nP(F | do(G), do(H), J) = P(F | do(G), J), if F ‚ä•‚ä•H | G ‚à™J in ÀúG,\nwhere ÀúG is the graph obtained by Ô¨Årst deleting all edges pointing into G and then deleting all arrows pointing into H\nfrom nodes that are not ancestors of J. According to DeÔ¨Ånition 1, in order to show that C must be exogenous, we must\nprove that\nP(Ct+1, . . . , CH | Ct, do(At)) = P(Ct+1, . . . , CH | Ct).\nWe will prove this statement by applying Rule 3 of the do-calculus above. For this purpose, we set F ‚ÜêCt+1, . . . , CH,\nG ‚Üê‚àÖ, H ‚ÜêAt, and J ‚ÜêCt. Given G is the empty set and At cannot have incoming edges from any ancestor of Ct,\n41\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nthe graph ÀúG will be identical to G except that the incoming edges to At are deleted. To show exogeneity, it then sufÔ¨Åces\nto show that\nCt+1, . . . , CH ‚ä•‚ä•At | Ct in ÀúG.\nIndeed, consider any trail P connecting node At to node Ct+œÑ in ÀúG. Since C is action-disconnected, we know that\nthere can be no directed chain from At to Ct+œÑ of the form At ‚Üí¬∑ ¬∑ ¬∑ ‚ÜíCt+œÑ. We argue that this then implies that\nP must necessarily contain a collider node Z of the form ‚ÜíZ ‚Üê. To see why, notice that the Ô¨Årst link in P has the\nform At ‚Üí¬∑ ¬∑ ¬∑ . If all subsequent links in P are of the form ¬∑ ¬∑ ¬∑ ‚Üí¬∑ ¬∑ ¬∑ , then P would be a directed chain, which is a\ncontradiction. Hence, the edge directionality at P must change at some node, which implies there must be some collider\nnode Z in P. The following two cases are then possible:\n1. If Z is not Ct or an ancestor of Ct, then the P is d-separated and thus does not violate the causal DeÔ¨Ånition 1.\n2. Otherwise, assume that Z is either Ct or an ancestor of Ct. By the structure of the causal graph, it is not\nhard to see that in order for this assumption to hold, P must contain a link Vt ‚ÜêUt‚àí1 from some state or\naction variable Ut‚àí1 at time step t ‚àí1 to some state variable Vt at time step t. Consider the sub-trail up to\nnode Ut‚àí1, which will have the form At ‚Üí¬∑ ¬∑ ¬∑ Vt ‚ÜêUt‚àí1. The sub-trail must contain a collider node ÀúZ;\nfurthermore, ÀúZ has time index t or higher, so it cannot be Ct or an ancestor of Ct. But this shows that trail P\nis also d-separated and, hence, does not violate the causal exogeneity DeÔ¨Ånition 1.\nAccording to Corollary 5, state variables S1 and S2 in the MDP of Figure 9 are causally exogenous. What about the\nreverse direction? We show that it is a natural implication of DeÔ¨Ånition 1 for causal exogeneity.\nTheorem 10. If state variable C is causally exogenous, then C must be action-disconnected in the corresponding\ncausal graph G.\nProof. Assume C is not action-disconnected. Then there must exist some directed chain At ‚Üí¬∑ ¬∑ ¬∑ ‚ÜíCt+œÑ connecting\nAt to some future state Ct+œÑ. This chain cannot go through Ct due to the structure of G. In that case, At and Ct+œÑ\nare not d-separated by Ct in graph ÀúG, where ÀúG is identical to G except that the incoming edges to At are deleted. As\na result, we cannot apply Rule 3 of the do-calculus to simplify expression P(CtœÑ | do(At), Ct) to P(CtœÑ | Ct). The\nproperty P(Ct+œÑ | do(At), Ct) = P(Ct+œÑ | Ct) cannot thus hold as a result of the DAG structure of G. Intuitively, the\nexistence of chain At ‚Üí¬∑ ¬∑ ¬∑ ‚ÜíCt+œÑ in ÀúG means that there is a path through which an intervention do(At) on At can\ninÔ¨Çuence Ct+œÑ, which implies that state variable C cannot be causally exogenous.\nIt is not hard to see that Theorems 9 and 10 also hold, when C is a set of causally exogenous state variables. Taken\ntogether, Theorems 9 and 10 imply the next result.\nCorollary 5. The set of causally exogenous state variables is identical to the set of action-disconnected variables.\nProof. Theorem 9 establishes the forward direction, while Theorem 10 establishes the reverse direction.\nThe previous result allows us to prove two basic properties of causally exogenous state variables.\nùê¥\nùëÜ2\nùëÜ3\nùëÜ2\n‚Ä≤\nùëÜ3\n‚Ä≤\nùëÜ1\nùëÜ1\n‚Ä≤\n(a) MDP.\nùê¥\nùëÜ2\nùëÜ3\nùëÜ2\n‚Ä≤\nùëÜ3\n‚Ä≤\nùëÜ1\nùëÜ1\n‚Ä≤\nùê¥‚Ä≤\nùëÜ2\n‚Ä≤‚Ä≤\nùëÜ3\n‚Ä≤‚Ä≤\nùëÜ1\n‚Ä≤‚Ä≤\nùê¥‚Ä≤‚Ä≤\nùëÜ2\n‚Ä≤‚Ä≤‚Ä≤\nùëÜ1\n‚Ä≤‚Ä≤‚Ä≤\nùëÜ3\n‚Ä≤‚Ä≤‚Ä≤\n(b) Unrolling the MDP for H = 3.\nFigure 18: An MDP with 3 state variables S1, S2, S3, where S1 and S2 are exogenous.\n42\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nùê¥\nùëã\nùê∏\nùëã‚Ä≤\nùê∏‚Ä≤\nFigure 19: State transition diagram for an MDP with all links (black and dashed red) present. Links to the action node\nA from E and X are missing for fully randomized policies.\nTheorem 11. Consider any MDP with causally exogenous state variables.\n1. Assume that the two sets of state variables X1 = S[I1] and X2 = S[I2] are causally exogenous. Then the\nunion set X = S[I1 ‚à™I2] is also causally exogenous.\n2. Assume X = S[I] is a set of causally exogenous state variables. Any subset ÀúX ‚äÜX is then also causally\nexogenous.\nProof. We trivially have from Corollary 5:\n1. Any state variable in X1 or X2 must be action-disconnected, so the union X1 ‚à™X2 can only contain action-\ndisconnected state variables.\n2. Any state variable in X must be action-disconnected, hence state variables in ÀúX ‚äÜX must be action-\ndisconnected.\nFinally, we remark that deÔ¨Åning exogeneity in causal terms as in DeÔ¨Ånition 1 is the most natural choice. To see why,\nconsider the MDP in Figure 18a. For this MDP, S1 and S2 are exogenous, since their evolution does not depend\non the action. Indeed, both S1 and S2 are exogenous by Corollary 5, since it is easy to verify that they are both\naction-disconnected in the unrolled diagram. In particular, S1 will satisfy the causal deÔ¨Ånition P(S‚Ä≤\n1 | S1, do(A)) =\nP(S‚Ä≤\n1 | S1). On the other hand, it may not be true that P(S‚Ä≤\n1 | S1, A) equals P(S‚Ä≤\n1 | S1). This is because of the trail\nA ‚ÜêS2 ‚ÜíS‚Ä≤\n2 ‚ÜíS‚Ä≤\n1, which could make S‚Ä≤\n1 conditionally dependent on A given S1 even though S‚Ä≤\n1 is not causally\ndependent on A.\nA.2\nCharacterization of Causally-Exogenous Causal Graphs with the Complete Exogenous Set\nTheorem 7 establishes conditions under which the discovered set X of a full or diachronic exo/endo decomposition\nis causally exogenous. In particular, under these conditions, the maximal exo set will contain causally exogenous\nstate variables only. A natural question then arises: does the maximal exo set contain all causally exogenous state\nvariables? Notice that the union property (e.g., in Corollary 1 or Lemma 1) only allows us to take the union of exo/endo\ndecompositions, not the union of arbitrary sets of exogenous state variables. So, it is not immediately obvious whether\nthe maximal exo set will contain all exo state variables.\nTo answer this question, we Ô¨Årst introduce the concept of the complete exogenous set, which we then use to characterize\nthe structure of all possible causal graphs with casually exogenous state variables.\nDeÔ¨Ånition 9 (Complete Exogenous Set). The complete exogenous set Xf of an MDP is deÔ¨Åned as the set of all causally\nexogenous state variables. Its complement Ef = Xc\nf is called the complete endogenous set.\nObviously, Ef can only contain state variables that are not causally exogenous. Our next result is central and\ncharacterizes all possible causal graphs with causally exogenous state variables.\nTheorem 12. Consider any stationary MDP with causally exogenous state variables. Its underlying causal graph can\nonly have two possible forms: (i) the full exo DBN of Figure 2 with X = Xf, or (ii) a special class of DAGs where all\ndirected paths in the unrolled DBN from A to state variables in Ef terminate at a timestep equal to the horizon H.\n43\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nùê¥\nùëÜ3\nùëÜ3\n‚Ä≤\nùëÜ2\nùëÜ2\n‚Ä≤\nùëÜ1\nùëÜ1\n‚Ä≤\n(a) MDP.\nùê¥\nùëÜ3\nùëÜ3\n‚Ä≤\nùëÜ2\nùëÜ2\n‚Ä≤\nùëÜ1\nùëÜ1\n‚Ä≤\nùëÜ3\n‚Ä≤‚Ä≤\nùëÜ2\n‚Ä≤‚Ä≤\nùëÜ1\n‚Ä≤‚Ä≤\nùê¥‚Ä≤\n(b) Unrolling the MDP for H = 2.\nFigure 20: An edge case for Theorem 12, where the MDP does not accept any full exo/endo decomposition, even for\nthe maximal exo state space.\nProof. Consider the complete exo set Xf. By Corollary 5, we know that Xf must be action-disconnected. Furthermore,\nstate variables in Ef must not be action-disconnected, or else they would be causally exogenous and thus belong to Xf;\nthe latter is impossible since Xf by deÔ¨Ånition contains all causally exogenous state variables. As a result, for each\nstate variable Ei in Ef, there must be at least one Et,i with t ‚àà{1, . . . , H} so that there is a directed path from some\nAœÑ, œÑ ‚àà{0, t ‚àí1} to Et,i. Let‚Äôs now consider the state partition (Ef, Xf) in causal graph G. The unrestricted state\ntransition diagram will contain all links (black and dashed red) in Figure 19. In order to show that (Ef, Xf) matches\nthe causal full exo DBN of Figure 2, we must show that it must be missing all the red dashed links. We separately\nconsider the 3 links:\n1. The link A ‚ÜíX‚Ä≤ must not be present because it would trivially give a directed path from At to Xt+1, which\nviolates the exogeneity of Xj according to Corollary 5.\n2. The link E‚Ä≤ ‚ÜíX‚Ä≤ must also not be present. Indeed, assume there is some link E‚Ä≤\ni ‚ÜíX‚Ä≤\nj. By the observation\nabove, there is a directed path from some AœÑ to Et,i. But then we can extend this path to get a directed path\nfrom AœÑ to Xt,j using the link E‚Ä≤\ni ‚ÜíX‚Ä≤\nj, which violates the exogeneity of Xj according to Corollary 5.\n3. We Ô¨Ånally discuss the link E ‚ÜíX‚Ä≤. Assume there is such a link Ei ‚ÜíX‚Ä≤\nj. Given Ei is not exogenous, there\nis a directed path from some Ak to Et,i. We can then extend that path to Xt+1,j to get a directed path from\nAk to Xt+1,j using the link Ei ‚ÜíX‚Ä≤\nj, which violates the exogeneity of Xj according to Corollary 5.\nThere is only one special edge case we need to consider for point (3). If there is a single directed path from A to Ei\nthat terminates at EH,i, then it is not possible to extend that path to XH+1,j, since we cannot go beyond time horizon\nH.\nFigure 20 depicts a graph where case (ii) of Theorem 12 applies. Consider the MDP in Figure 20a with 3 state variables\nand a horizon of H = 2. We depict the unrolling process over H in Figure 20b. Directed paths starting from action\nnodes are shown in blue color. Notice that S1 is the only action-disconnected state variable, and according to Corollary\n5, it will be the only causally exogenous state variable. Hence, the maximal exo set will simply be Xf = {S1} with\ncorresponding Ef = {S2, S3}. Notice, however, the link from S2 to S‚Ä≤\n1; since this link originates from Ef to X‚Ä≤\nf, the\ncorresponding state partition (Ef, Xf) does not abide by the full causally exogenous DBN of Figure 2.\nTheorem 12 has important implications in practice. Assume any admissible MDP with a fully randomized exploration\npolicy. Under faithfulness, any MDP with exogenous state variables will generally accept a full exo/endo (statistical)\ndecomposition S = (E, X), namely, the one where X is the full exo set Xf containing all exogenous state variables.\nHence, the full exo DBN is not just a convenient tool based on the 2-timestep dynamics, but a powerful abstraction that\ncan usually capture the maximal exo set Xm under certain conditions. However, edge cases exist according to Theorem\n12: for instance, in the MDP of Figure 20 there is no valid full exo/endo decomposition even though the maximal exo\nset is nonempty, under the standard conditions. This answers the question we asked at the beginning of Appendix A.2.\nB\nThe SimpliÔ¨Åed Objective\nOur experiments showed that the SimpliÔ¨Åed-GRDS and SRAS algorithms often give excellent performance. Both\nof these algorithms employ the simpliÔ¨Åed information theoretic objective I(X‚Ä≤; A | X) = 0 in place of either the\n44\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nùê¥\nùëÜ3\nùëÜ2\nùëÜ3\n‚Ä≤\nùëÜ2\n‚Ä≤\nùëÜ1\nùëÜ1\n‚Ä≤\nFigure 21: MDP with 3 endogenous state variables S1, S2, S3.\ndiachronic objective I(X‚Ä≤; A, E, E‚Ä≤ | X) = 0 or the full objective I(X‚Ä≤; A, E | X) = 0. In this appendix, we analyze\nthe properties of the simpliÔ¨Åed objective.\nIt is easy to Ô¨Ånd examples where the simpliÔ¨Åed objective fails. Consider the DBN in Figure 21 of an MDP with the\nstate variables S1, S2 and S3. The policy determining the action depends only on S2 and S3 but not on S1. All state\nvariables can be endogenous (S1 and S2 directly, and S3 indirectly through its dependence on S1). However, the MDP\nsatisÔ¨Åes the condition I(S‚Ä≤\n3; A | S3) = 0. Hence, we cannot use the simpliÔ¨Åed objective to safely conclude whether a\nstate variable is exogenous or not when analyzing the 2-timestep DBN.\nAs we would expect, the simpliÔ¨Åed setting does not necessarily lead to valid full or diachronic exo/endo decompositions.\nIn this direction, Ô¨Årst note that even when the simpliÔ¨Åed objective returns a set X of variables that are causally\nexogenous, X and its complement E = Xc may not correspond to a valid exo/endo decomposition (E, X). Consider\nfor example X = {S3} in Figure 22a, which satisÔ¨Åes I(X‚Ä≤; A | X) = 0 but X and the corresponding E = {S1, S2}\nare not a valid full exo/endo decomposition because S‚Ä≤\n3 depends on S‚Ä≤\n2. This is not the case when using the full\nor diachronic objectives. Furthermore, it is possible for a state variable X to be exogenous, even though it fails to\nsatisfy I(X‚Ä≤; A | X) = 0. Consider X = {S3} in Figure 22b. It may not satisfy I(S‚Ä≤\n3; A | S3) = 0 due to the trail\nA ‚ÜêS2 ‚ÜíS‚Ä≤\n3 from A to S‚Ä≤\n3.\nDespite the fact that the simpliÔ¨Åed objective I(X‚Ä≤; A | X) = 0 is not sufÔ¨Åcient to identify exogenous variables,\nit can be used as a simpler proxy for the full objective I(X‚Ä≤; [E, A] | X) = 0. Any set of state variables X that\nsatisÔ¨Åes the full objective must necessarily satisfy the simpliÔ¨Åed objective, since the latter has fewer constraints than\nthe former. Of course, the simpliÔ¨Åed objective may return an over-estimate of the set of exogenous state variables,\npossibly contaminated with endogenous components. For this reason, it is always important to check the decomposition\n(E = Xc, X) that satisÔ¨Åes the simpliÔ¨Åed objective against the full (or diachronic) objective.\nThe shortcomings of the simpliÔ¨Åed objective result from attempting to apply it within the framework of the 2-timestep\nDBN. If we unroll the DBN and consider the H-horizon MDP, then the simpliÔ¨Åed objective corresponds to directly\nchecking that all variables in X are disconnected from A and therefore X is causally exogenous. The next theorem\nresembles Theorem 7 for full or diachronic factorizations.\nùê¥\nùëÜ3\nùëÜ2\nùëÜ3\n‚Ä≤\nùëÜ2\n‚Ä≤\nùëÜ1\nùëÜ1\n‚Ä≤\n(a) I(S‚Ä≤\n3; A | S3) = 0.\nùê¥\nùëÜ3\nùëÜ2\nùëÜ3\n‚Ä≤\nùëÜ2\n‚Ä≤\nùëÜ1\nùëÜ1\n‚Ä≤\n(b) I(S‚Ä≤\n3; A | S3) Ã∏= 0.\nFigure 22: State transition diagrams for two MDPs. In both MDPs, S2 and S3 are exogenous.\n45\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nTheorem 13. Assume that an H-horizon MDP is admissible, data D has been collected by executing a fully-randomized\npolicy for n steps, and the faithfulness assumption holds for the causal graph G describing the MDP. If\nlim\nn‚Üí‚àû\nÀÜI(XœÑ; At | Xt) = 0, ‚àÄt ‚â§H ‚àí1, t + 1 ‚â§œÑ ‚â§H,\n(31)\nthen X is causally exogenous.\nProof. (sketch) Equation (31) is equivalent to the statement that P(XœÑ | Xt, At) = P(XœÑ | Xt) for all t and œÑ.\nUnder the faithfulness assumption, we can infer that there is no directed path from any At to any XœÑ. Hence, X is\naction-disconnected in the causal graph G, and it follows from Theorem 9 that X is causally exogenous.\nThe reverse direction is trivial. If X is causally exogenous and the faithfulness assumption holds, then the simpliÔ¨Åed\ncondition must hold between each At and every XœÑ. It follows that in the unrolled MDP, the simpliÔ¨Åed objective will\ndetect exactly the exogenous variables (under the assumptions of the theorem).\nTheorem 13 has the drawback that it must consider all long-range dependencies, and this requires estimating\nP(Xt, XœÑ, At) for all t ‚â§H ‚àí1 and all œÑ such that t + 1 ‚â§œÑ ‚â§H. This makes much less effective use of\nthe data set D. In practical applications, it might be fruitful to explore approximate variants of this theorem that\nmake use of a small number F of forward steps. In this case, we could enforce just F constraints per timestep, i.e.,\nI(Xt+k; At | Xt) = 0, ‚àÄ1 ‚â§k ‚â§F, where F is a small number.\nC\nDynamic Program for the Covariance Condition\nTheorem 14. The variance of the H-step return BœÄ(s; H) can be computed as the solution to the following dynamic\nprogram:\nV œÄ(s; 0) := 0;\nVar[BœÄ(s; 0)] := 0\n(32)\nV œÄ(s; h) := m(s, œÄ(s)) + Œ≥Es‚Ä≤‚àºP (s‚Ä≤|s,œÄ(s))[V œÄ(s‚Ä≤; h ‚àí1)]\n(33)\nVar[BœÄ(s; h)] := œÉ2(s, œÄ(s)) ‚àíV œÄ(s; h)2 +\n(34)\nŒ≥2Es‚Ä≤‚àºP (s‚Ä≤|s,œÄ(s))[Var[BœÄ(s‚Ä≤; h ‚àí1)]] +\nEs‚Ä≤‚àºP (s‚Ä≤|s,œÄ(s)[m(s, œÄ(s)) + Œ≥V œÄ(s‚Ä≤; h ‚àí1)]2\nEquations (33) and (34) apply for all h > 0.\nProof. Sobel (1982) analyzed the variance of inÔ¨Ånite horizon discounted MDPs with deterministic rewards. We modify\nhis proof to handle a Ô¨Åxed horizon and stochastic rewards. We proceed by induction on h. To simplify notation, we\nomit the dependence on œÄ.\nBase Case: H = 0. This is established by Equations (32).\nInductive Step: H = h. Write\nB(s; h) = R(s) + Œ≥B(s‚Ä≤; h ‚àí1),\nwhere the rhs involves the three random variables R(s), S‚Ä≤, and B(s‚Ä≤; h ‚àí1) (with samples s‚Ä≤ ‚àºS‚Ä≤). To obtain\nEquation (33), compute the expected value\nV (s; h) = EB(s;h)[B(s; h)] = Es‚Ä≤,R(s),B(s‚Ä≤;h‚àí1)[R(s) + Œ≥B(s‚Ä≤; h ‚àí1)],\nand take each expectation in turn. To obtain the formula for the variance, write the standard formula for the variance:\nVar[B(s; h)] = EB(s;h)[B(s; h)2] ‚àíEB(s;h)[B(s; h)]2.\nSubstitute R(s) + Œ≥B(s‚Ä≤; h ‚àí1) in the Ô¨Årst term and simplify the second term to obtain\nEs‚Ä≤,R(s),B(s‚Ä≤;h‚àí1)\n\u0002\n{R(s) + Œ≥B(s‚Ä≤; h ‚àí1)}2\u0003\n‚àíV (s; h)2.\nExpand the square in the Ô¨Årst term:\nEs‚Ä≤,R(s),B(s‚Ä≤;h‚àí1)[R(s)2 + 2R(s)Œ≥B(s‚Ä≤; h ‚àí1) + Œ≥2B(s‚Ä≤; h ‚àí1)2] ‚àíV (s; h)2.\nDistribute the two innermost expectations over the sum:\nEs‚Ä≤[ER(s)[R(s)2] + 2m(s)Œ≥V (s‚Ä≤; h ‚àí1) + Œ≥2EB(s‚Ä≤;h‚àí1)[B(s‚Ä≤; h ‚àí1)2]] ‚àíV (s; h)2.\n46\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nApply the deÔ¨Ånition of variance in reverse to terms 1 and 3 in brackets:\nEs‚Ä≤[Var[R(s)] + m(s)2 + 2m(s)Œ≥V (s‚Ä≤; h ‚àí1) + Œ≥2Var[B(s; h ‚àí1)] + Œ≥2V (s‚Ä≤; h ‚àí1)2] ‚àíV (s; h)2\nFactor the quadratic involving terms 1, 3, and 4:\nEs‚Ä≤[œÉ2(s) + [m(s) + Œ≥V (s‚Ä≤; h ‚àí1)]2 + Œ≥2Var[B(s‚Ä≤; h ‚àí1)]] ‚àíV (s; h)2.\nFinally, distribute the expectation with respect to s‚Ä≤ to obtain Equation (34).\nTheorem 15. The covariance between the exogenous H-step return BœÄ\nx(x; H) and the endogenous H-step return\nBœÄ\ne (e, x; H) can be computed via the following dynamic program:\nCov[BœÄ\nx(x; 0), BœÄ\ne (e, x; 0)] := 0\n(35)\nCov[BœÄ\nx(x; h), BœÄ\ne (e, x; h)] :=\nŒ≥2Ee‚Ä≤‚àºP (e‚Ä≤|e,x),x‚Ä≤‚àºP (x‚Ä≤|x)\n\b\nCov[BœÄ\nx(x‚Ä≤; h ‚àí1), BœÄ\ne (e‚Ä≤, x‚Ä≤; h ‚àí1)] +\n(36)\n[mx(x) + Œ≥Vx(x‚Ä≤; h ‚àí1)] √ó [me(e, x, œÄ(e, x)) + Œ≥V œÄ\ne (e‚Ä≤, x‚Ä≤; h ‚àí1)]\n\t\n‚àíV œÄ\nx (x; h)V œÄ\ne (e, x; h).\nEquation (36) applies for all h > 0.\nProof. By induction. The base case is established by Equation (35). For the inductive case, we begin with the formula\nfor non-centered covariance:\nCov[Bx(x; h), Be(e, x; h)] = EBx(x;h),Be(e,x;h)[Bx(x; h)Be(e, x; h)] ‚àíVx(x; h)Ve(e, x; h).\nReplace Bx(x; h) by Rx(x) + Œ≥Bx(x‚Ä≤; h ‚àí1) and Be(e, x; h) by Re(e, x) + Œ≥Be(e‚Ä≤, x‚Ä≤; h ‚àí1) and re-\nplace the expectations wrt Bx(x; h) and Be(e, x; h) by expectations wrt the six variables {e‚Ä≤\n‚àºE‚Ä≤, x‚Ä≤\n‚àº\nX‚Ä≤, Rx(x), Re(e, x), Bx(x‚Ä≤; h ‚àí1), Be(e‚Ä≤, x‚Ä≤; h ‚àí1)}. We will use the following abbreviations for these variables:\ns‚Ä≤ = {e‚Ä≤, x‚Ä≤}, r = {Re(x), Re(e, x)}, and B = {Bx(x‚Ä≤; h ‚àí1), Be(e‚Ä≤, x‚Ä≤; h ‚àí1)}.\nCov[Bx(x; h), Be(e, x; h)] = ‚àíVx(x; h)Ve(e, x; h) +\nEs‚Ä≤,r,B[(Rx(x) + Œ≥Bx(x‚Ä≤; h ‚àí1))(Re(e, x) + Œ≥Be(e‚Ä≤, x‚Ä≤; h ‚àí1))].\nWe will focus on the expectation term. Multiply out the two terms and distribute the expectations wrt r and B:\nEs‚Ä≤[mx(x)me(e, x) + Œ≥mx(x)Ve(e‚Ä≤, x‚Ä≤; h ‚àí1) + Œ≥me(e, x)Vx(x‚Ä≤; h ‚àí1) +\nŒ≥2EB[Bx(x‚Ä≤; h ‚àí1)Be(e‚Ä≤, x‚Ä≤; h ‚àí1)]].\nApply the non-centered covariance formula ‚Äúin reverse‚Äù to term 4.\nEs‚Ä≤[mx(x)me(e, x) + Œ≥mx(x)Ve(e‚Ä≤, x‚Ä≤; h ‚àí1) + Œ≥me(e, x)Vx(x‚Ä≤; h ‚àí1) +\nŒ≥2Vx(x‚Ä≤; h ‚àí1)Ve(e‚Ä≤, x‚Ä≤; h ‚àí1) + Œ≥2Cov[Bx(x‚Ä≤; h ‚àí1)Be(e‚Ä≤, x‚Ä≤; h ‚àí1)]].\nDistribute expectation with respect to s‚Ä≤:\nmx(x)me(e, x) + Œ≥mx(x)Es‚Ä≤[Ve(e‚Ä≤, x‚Ä≤; h ‚àí1)] + Œ≥me(e, x)Es‚Ä≤[Vx(x‚Ä≤; h ‚àí1)] +\nŒ≥2Es‚Ä≤[Vx(x‚Ä≤; h ‚àí1)]Es‚Ä≤[Ve(e‚Ä≤, x‚Ä≤; h ‚àí1)] + Œ≥2Cov[Bx(x‚Ä≤; h ‚àí1)Be(e‚Ä≤, x‚Ä≤; h ‚àí1)]]\nObtain Equation (36) by factoring the Ô¨Årst four terms, writing the expectations explicitly, and including the\n‚àíVx(x; h)Ve(e, x; h) term.\nTo gain some intuition for this theorem, examine the three terms on the right-hand side of Equation (36). The Ô¨Årst is the\n‚Äúrecursive‚Äù covariance for h ‚àí1. The second is the one-step non-centered covariance, which is the expected value of\nthe product of the backed-up values for V œÄ\ne and V œÄ\nx . The third term is the product of V œÄ\ne and V œÄ\nx for the current state,\nwhich re-centers the covariance.\n47\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nReferences\nP.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University Press,\nPrinceton, NJ, USA, 2007.\nAlexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A. Saurous, and Kevin Murphy. Fixing a broken ELBO.\nIn Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 159‚Äì168, 2018.\nLynton Ardizzone, Jakob Kruse, Carsten Rother, and Ullrich K¬®othe. Analyzing inverse problems with invertible neural\nnetworks. In International Conference on Learning Representations, 2019.\nKunihiro Baba, Ritei Shibata, and Masaaki Sibuya. Partial correlation and conditional correlation as measures of\nconditional independence. Australian & New Zealand Journal of Statistics, 46(4):657‚Äì664, 2004.\nFrancis R. Bach and Michael I. Jordan. Kernel independent component analysis. J. Mach. Learn. Res., 3:1‚Äì48, 2003.\nAdri`a Puigdom`enech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier\nTieleman, Martin Arjovsky, Alexander Pritzel, Andrew Bolt, and Charles Blundell. Never give up: Learning directed\nexploration strategies. In International Conference on Learning Representations, 2020.\nDavid Barber and Felix Agakov. The im algorithm: A variational approach to information maximization. In Proceedings\nof the 16th International Conference on Neural Information Processing Systems, page 201‚Äì208, 2003.\nJens Behrmann, Paul Vicol, Kuan-Chieh Wang, Roger Grosse, and Joern-Henrik Jacobsen. Understanding and\nmitigating exploding inverses in invertible neural networks. In Proceedings of The 24th International Conference on\nArtiÔ¨Åcial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 1792‚Äì1800.\nPMLR, 2021.\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and\nDevon Hjelm. Mutual information neural estimation. In Proceedings of the 35th International Conference on\nMachine Learning, volume 80, pages 531‚Äì540. PMLR, 2018.\nMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-\nbased exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, volume 29,\n2016.\nMarc G. Bellemare, Joel Veness, and Michael Bowling. Investigating contingency awareness using atari 2600 games.\nIn Proceedings of the Twenty-Sixth AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI‚Äô12, page 864‚Äì871, 2012.\nJames Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. J. Mach. Learn. Res., 13:\n281‚Äì305, 2012.\nÀöAke Bj¬®orck and Gene H. Golub. Numerical methods for computing angles between linear subspaces. Mathematics of\nComputation, 27(123):579‚Äì594, 1973.\nDavid M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians. Journal of the\nAmerican Statistical Association, 112(518):859‚Äì877, 2017.\nRobert Bray. Markov decision processes with exogenous variables. Management Science, 65(10):4598‚Äì4606, 2019.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba.\nOpenai gym, 2016.\nLars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter\nPrettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and\nGa¬®el Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In ECML\nPKDD Workshop: Languages for Data Mining and Machine Learning, pages 108‚Äì122, 2013.\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. In\nInternational Conference on Learning Representations, 2019.\nDian Chen, Vladlen Koltun, and Philipp Kr¬®ahenb¬®uhl. Learning to drive from a world on rails. In 2021 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages 15570‚Äì15579, 2021.\nRohan Chitnis and Tom¬¥as Lozano-P¬¥erez. Learning compact models for planning with exogenous processes. In\nProceedings of the Conference on Robot Learning, volume 100 of Proceedings of Machine Learning Research, pages\n813‚Äì822, 2020.\nJongwook Choi, Yijie Guo, Marcin Moczulski, Junhyuk Oh, Neal Wu, Mohammad Norouzi, and Honglak Lee.\nContingency-aware exploration in reinforcement learning. In International Conference on Learning Representations,\n2019.\n48\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nOriol Corcoll, Youssef Sherif Mansour Mohamed, and Raul Vicente. Did i do that? blame as a means to identify\ncontrolled effects in reinforcement learning. Transactions on Machine Learning Research, 2022.\nThomas Dietterich, George Trimponias, and Zhitang Chen. Discovering and removing exogenous state variables and\nrewards for reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning,\nvolume 80, pages 1262‚Äì1270. PMLR, 2018.\nM. D. Donsker and S. R.S. Varadhan. Asymptotic evaluation of certain markov process expectations for large time. iv.\nCommunications on Pure and Applied Mathematics, 36(2):183‚Äì212, 1983.\nAlan Edelman, Tom¬¥as A. Arias, and Steven T. Smith. The geometry of algorithms with orthogonality constraints. SIAM\nJ. Matrix Anal. Appl., 20(2):303‚Äì353, 1999.\nYonathan Efroni, Dylan J Foster, Dipendra Misra, Akshay Krishnamurthy, and John Langford. Sample-efÔ¨Åcient\nreinforcement learning in the presence of exogenous information. In Proceedings of Thirty Fifth Conference on\nLearning Theory, volume 178, pages 5062‚Äì5127, 2022a.\nYonathan Efroni, Dipendra Misra, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Provably Ô¨Åltering\nexogenous distractors using multistep inverse dynamics. In International Conference on Learning Representations,\n2022b.\nRaphael Fonteneau, Susan A Murphy, Louis Wehenkel, and Damien Ernst. Batch mode reinforcement learning\nbased on the synthesis of artiÔ¨Åcial trajectories. Annals of Operations Research, 208(1):383‚Äì416, 2012. doi:\n10.1007/s10479-012-1248-5.\nStephen H. Friedberg, Arnold J. Insel, and Lawrence E. Spence. Linear algebra. Pearson Education, 4th ed. edition,\n2002.\nKenji Fukumizu, Francis R Bach, and Michael I Jordan. Dimensionality reduction for supervised learning with\nreproducing kernel hilbert spaces. Journal of Machine Learning Research, 5(Jan):73‚Äì99, 2004.\nKenji Fukumizu, Arthur Gretton, Xiaohai Sun, and Bernhard Sch¬®olkopf. Kernel measures of conditional dependence.\nIn Advances in neural information processing systems, pages 489‚Äì496, 2008.\nNick Haber, Damian Mrowca, Stephanie Wang, Li F Fei-Fei, and Daniel L Yamins. Learning to play with intrinsically-\nmotivated, self-aware agents. In Advances in Neural Information Processing Systems, volume 31, 2018.\nYanjun Han, Jiantao Jiao, Tsachy Weissman, and Yihong Wu. Optimal rates of entropy estimation over Lipschitz balls.\nThe Annals of Statistics, 48(6):3228 ‚Äì 3250, 2020.\nR. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, and\nYoshua Bengio.\nLearning deep representations by mutual information estimation and maximization.\nIn 7th\nInternational Conference on Learning Representations, ICLR, 2019.\nRein Houthooft, Xi Chen, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational\ninformation maximizing exploration. In Advances in Neural Information Processing Systems, volume 29, 2016.\nBo Jiang and Yu-Hong Dai. A framework of constraint preserving update schemes for optimization on stiefel manifold.\nMath. Program., 153(2):535‚Äì575, 2015.\nKirthevasan Kandasamy, Akshay Krishnamurthy, Barnab¬®ys P¬¥oczos, Larry Wasserman, and James M. Robins. Non-\nparametric von mises estimators for entropies, divergences and mutual informations. In Proceedings of the 28th\nInternational Conference on Neural Information Processing Systems - Volume 1, page 397‚Äì405, 2015.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\nIvan Kobyzev, Simon J.D. Prince, and Marcus A. Brubaker. Normalizing Ô¨Çows: An introduction and review of current\nmethods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):3964‚Äì3979, 2021.\nNiklas Koenen, Marvin N. Wright, Peter Maass, and Jens Behrmann. Generalization of the change of variables formula\nwith applications to residual Ô¨Çows. In ICML Workshop on Invertible Neural Networks, Normalizing Flows, and\nExplicit Likelihood Models, 2021.\nDaphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. The MIT Press,\nCambridge, MA, 2009.\nAlexander Kraskov, Harald St¬®ogbauer, and Peter Grassberger. Estimating mutual information. Phys. Rev. E, 69, 2004.\nDavid McAllester and Karl Stratos. Formal limitations on the measurement of mutual information. In Proceedings\nof the Twenty Third International Conference on ArtiÔ¨Åcial Intelligence and Statistics, volume 108, pages 875‚Äì884.\nPMLR, 2020.\nSean McGregor, Rachel Houtman, Claire Montgomery, Ronald Metoyer, and Thomas G. Dietterich. Factoring\nExogenous State for Model-Free Monte Carlo. arXiv 1703.09390, 2017.\n49\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nKevin R. Moon, Kumar Sricharan, and Alfred O. Hero. Ensemble estimation of mutual information. In 2017 IEEE\nInternational Symposium on Information Theory (ISIT), pages 3030‚Äì3034, 2017.\nXuanLong Nguyen, Martin J Wainwright, and Michael Jordan. Estimating divergence functionals and the likelihood\nratio by penalized convex risk minimization. In Advances in Neural Information Processing Systems, volume 20,\n2007.\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. F-gan: Training generative neural samplers using variational\ndivergence minimization. In Proceedings of the 30th International Conference on Neural Information Processing\nSystems, page 271‚Äì279, 2016.\nJunhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional video prediction\nusing deep networks in atari games. In Proceedings of the 28th International Conference on Neural Information\nProcessing Systems - Volume 2, page 2863‚Äì2871, 2015.\nGeorg Ostrovski, Marc G. Bellemare, A¬®aron van den Oord, and R¬¥emi Munos. Count-based exploration with neural\ndensity models. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, page\n2721‚Äì2730, 2017.\nKeiran Paster, Sheila A. McIlraith, and Jimmy Ba. Planning from pixels using inverse dynamics models. In International\nConference on Learning Representations, 2021.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin\nRaison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch:\nAn imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems\n32, pages 8024‚Äì8035, 2019.\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised\nprediction. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, page 2778‚Äì2787,\n2017.\nJudea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann\nPublishers Inc., San Francisco, CA, USA, 1988.\nJudea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, USA, 2nd edition, 2009.\nBen Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual\ninformation. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 5171‚Äì5180.\nPMLR, 2019.\nMartin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons,\nInc., New York, NY, USA, 1st edition, 1994.\nAntonin RafÔ¨Ån, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-\nbaselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1‚Äì8,\n2021.\nKate Rakelly, Abhishek Gupta, Carlos Florensa, and Sergey Levine. Which mutual-information representation learning\nobjectives are sufÔ¨Åcient for control? In Advances in Neural Information Processing Systems, volume 34, pages\n26345‚Äì26357, 2021.\nDanilo Rezende and Shakir Mohamed. Variational inference with normalizing Ô¨Çows. In Proceedings of the 32nd\nInternational Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages\n1530‚Äì1538. PMLR, 2015.\nShashank Singh and Barnab¬¥as P¬¥oczos. Nonparanormal information estimation. In Proceedings of the 34th International\nConference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3210‚Äì3219,\n2017.\nMatthew J. Sobel. The Variance of Discounted Markov Decision Processes. Journal of Applied Probability, 19(4):\n394‚Äì802, 1982.\nYuhang Song, Jianyi Wang, Thomas Lukasiewicz, Zhenghua Xu, Shangtong Zhang, Andrzej Wojcicki, and Mai Xu.\nMega-reward: Achieving human-level play without extrinsic rewards. Proceedings of the AAAI Conference on\nArtiÔ¨Åcial Intelligence, 34(04):5826‚Äì5833, 2020.\nP. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press, 2nd edition, 2000.\nE. Stiefel. Richtungsfelder und fernparallelismus in n-dimensionalen mannigfaltigkeiten. Commentarii Mathematici\nHelvetici, 8(1):305‚Äì353, 1935.\n50\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nRichard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA,\n1st edition, 1998.\nValentin Thomas, Emmanuel Bengio, William Fedus, Jules Pondard, Philippe Beaudoin, Hugo Larochelle, Joelle\nPineau, Doina Precup, and Yoshua Bengio. Disentangling the independently controllable factors of variation by\ninteracting with the world. CoRR, abs/1802.09484, 2018.\nAndrey N. Tikhonov and Vasiliy Y. Arsenin. Solutions of ill-posed problems. V. H. Winston & Sons, Washington, D.C.:\nJohn Wiley & Sons, New York, 1977.\nJames Townsend, Niklas Koep, and Sebastian Weichwald. Pymanopt: A python toolbox for optimization on manifolds\nusing automatic differentiation. Journal of Machine Learning Research, 17(137):1‚Äì5, 2016.\nL.W. Tu. An Introduction to Manifolds. Universitext. Springer New York, 2010.\nA¬®aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR,\nabs/1807.03748, 2018. URL http://arxiv.org/abs/1807.03748.\nMengjiao Yang, Dale Schuurmans, Pieter Abbeel, and OÔ¨År Nachum. Dichotomy of Control: Separating What You Can\nControl from What You Cannot. arXiv, 2210.13435(v1):1‚Äì19, 2022. URL http://arxiv.org/abs/2210.13435.\nXianli Zeng, Yingcun Xia, and Howell Tong. Jackknife approach to the estimation of mutual information. Proceedings\nof the National Academy of Sciences, 115(40):9956‚Äì9961, 2018.\n51\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2023-03-22",
  "updated": "2023-03-22"
}