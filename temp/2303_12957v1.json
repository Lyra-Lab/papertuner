{
  "id": "http://arxiv.org/abs/2303.12957v1",
  "title": "Reinforcement Learning with Exogenous States and Rewards",
  "authors": [
    "George Trimponias",
    "Thomas G. Dietterich"
  ],
  "abstract": "Exogenous state variables and rewards can slow reinforcement learning by\ninjecting uncontrolled variation into the reward signal. This paper formalizes\nexogenous state variables and rewards and shows that if the reward function\ndecomposes additively into endogenous and exogenous components, the MDP can be\ndecomposed into an exogenous Markov Reward Process (based on the exogenous\nreward) and an endogenous Markov Decision Process (optimizing the endogenous\nreward). Any optimal policy for the endogenous MDP is also an optimal policy\nfor the original MDP, but because the endogenous reward typically has reduced\nvariance, the endogenous MDP is easier to solve. We study settings where the\ndecomposition of the state space into exogenous and endogenous state spaces is\nnot given but must be discovered. The paper introduces and proves correctness\nof algorithms for discovering the exogenous and endogenous subspaces of the\nstate space when they are mixed through linear combination. These algorithms\ncan be applied during reinforcement learning to discover the exogenous space,\nremove the exogenous reward, and focus reinforcement learning on the endogenous\nMDP. Experiments on a variety of challenging synthetic MDPs show that these\nmethods, applied online, discover large exogenous state spaces and produce\nsubstantial speedups in reinforcement learning.",
  "text": "REINFORCEMENT LEARNING WITH EXOGENOUS STATES AND\nREWARDS\nA PREPRINT\nGeorge Trimponias∗\nAmazon\nLuxembourg\ntrimpog@amazon.lu\nThomas G. Dietterich∗\nCollaborative Robotics and Intelligent Systems (CoRIS) Institute\nOregon State University, Corvallis, OR 97331 USA\ntgd@cs.orst.edu\nMarch 24, 2023\nABSTRACT\nExogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled\nvariation into the reward signal. This paper formalizes exogenous state variables and rewards and\nshows that if the reward function decomposes additively into endogenous and exogenous components,\nthe MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous\nreward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any\noptimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because\nthe endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We\nstudy settings where the decomposition of the state space into exogenous and endogenous state spaces\nis not given but must be discovered. The paper introduces and proves correctness of algorithms for\ndiscovering the exogenous and endogenous subspaces of the state space when they are mixed through\nlinear combination. These algorithms can be applied during reinforcement learning to discover the\nexogenous space, remove the exogenous reward, and focus reinforcement learning on the endogenous\nMDP. Experiments on a variety of challenging synthetic MDPs show that these methods, applied\nonline, discover large exogenous state spaces and produce substantial speedups in reinforcement\nlearning.\nKeywords: Reinforcement learning, exogenous state variables, Markov Decision Processes, Markov\nReward Processes, causal discovery\n1\nIntroduction\nIn many practical settings, the actions of an agent have only a limited effect on the environment. For example, in a\nwireless cellular network, the cell tower base stations have many parameters that must be dynamically controlled to\noptimize network performance. We can formulate this as a Markov Decision Process (MDP) in which the reward\nfunction is the negative of the number of users who are suffering from low bandwidth. However, this reward is heavily\ninﬂuenced by exogenous factors such as the number, location, and behavior of the cellular network customers. Customer\ndemand varies stochastically as a function of exogenous factors (news, sporting events, trafﬁc accidents). In addition,\natmospheric conditions can affect the capacity of each wireless channel. This high degree of stochasticity can confuse\nreinforcement learning algorithms, because during exploration, the beneﬁt of trying action a in state s is hard to\ndetermine because the change in reward is obscured by the exogenous components of the reward. Many trials are\nrequired to average away these exogenous components so that the effect of the action can be measured. For temporal\ndifference algorithms, such as Q Learning, the learning rate needs to be very small. For policy gradient algorithms, the\nnumber of Monte Carlo trials required to estimate the gradient grows very large (or equivalently, the step size must be\nvery small). In this paper, we analyze this setting and develop algorithms for automatically detecting and removing the\neffects of exogenous state variables. This accelerates reinforcement learning (RL).\n∗Equal contributions\narXiv:2303.12957v1  [cs.LG]  22 Mar 2023\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nThe ﬁrst part of the paper deﬁnes exogenous state variables based on a causal foundation. It then introduces Exogenous\nState MDPs, which capture a broad class of MDPs with exogenous states. We characterize the space of all Exogenous\nState MDPs in terms of constraints on the structure of the corresponding two-time step dynamic Bayesian network. The\npaper analyzes the properties of exogenous subspaces of the state space and proves that every Exogenous State MDP\nhas a unique maximal exogenous subspace that contains all other exogenous subspaces. The paper then shows that,\nunder the assumption that the reward function decomposes additively into exogenous and endogenous components, the\nBellman equation for the original MDP decomposes into two equations: one for an exogenous Markov reward process\n(Exo-MRP) and the other for an endogenous MDP (Endo-MDP). Importantly, every optimal policy for the Endo-MDP\nis an optimal policy for the full MDP.\nIn the second part of the paper, we identify one condition—based on the covariance between the endogenous and\nexogenous returns—under which solving the Endo-MDP is faster (in sample complexity) than solving the full MDP. To\ndo this, we derive dynamic programming updates for the covariance between the H-horizon returns of the Exo-MRP\nand the Endo-MDP, which may be of independent interest.\nThe third part of the paper formulates the problem of discovering the Exo/Endo Decomposition assuming we have access\nto a sufﬁciently large and representative sample of ⟨s, a, r, s′⟩tuples recorded from MDP trajectories. The problem is\nformalized as a constrained optimization problem where the objective is to ﬁnd an exogenous subspace that minimizes\nthe variance of the endogenous reward subject to conditional mutual information constraints that enforce the Exogenous\nState MDP structure. We introduce and study several variations of this constrained optimization problem. For the\ncase of linear subspaces, the mutual information constraints can be replaced by constraints on a quantity called the\nconditional correlation coefﬁcient (CCC), and this yields practical algorithms. This section concludes with a discussion\nof the conditions that the MDP and the exploration policy should satisfy such that the sample of ⟨s, a, r, s′⟩tuples is\nsufﬁcient for identifying violations of the conditional mutual information constraints and ensure that the variables in the\ndiscovered exogenous subspace are causally exogenous.\nThe fourth part of the paper introduces two algorithms for solving the conditional correlation formulation and studies\ntheir soundness and correctness. One algorithm solves a series of global optimization problems, and the other is a\nstepwise algorithm that identiﬁes one exogenous dimension at a time. We prove the correctness of the global algorithm,\nand we develop some insights into when the stepwise algorithm works well.\nFinally, the ﬁfth part of the paper articulates a set of research questions and executes experiments to answer those\nquestions. The main ﬁnding is that the Exo/Endo decomposition algorithms can discover large exogenous subspaces\nthat yield substantial speedups in reinforcement learning for the high-performing PPO actor-critic algorithm. The\nexperiments also study the sensitivity of our algorithms to their hyperparameters and suggest a practical strategy for\nsetting them. We also explore the behavior of the algorithms on MDPs with nonlinear rewards, nonlinear dynamics, and\ndiscrete states. Connections to previous work are interleaved throughout the paper.\n2\nMDPs with Exogenous States and Rewards\nWe study discrete time stationary MDPs with stochastic rewards and stochastic transitions [Puterman, 1994, Sutton\nand Barto, 1998]; the state and action spaces may be either discrete or continuous. Notation: state space S, action\nspace A, reward distribution R: S × A 7→P(R) (where P(R) is the space of probability distributions over the real\nnumbers), transition function P : S × A 7→P(S) (where P(S) is the space of probability distributions over S), starting\nstate distribution P0 ∈P(S), and discount factor γ ∈(0, 1]. We assume that for all (s, a) ∈S × A, R(s, a) has\nexpected value m(s, a) and ﬁnite variance σ2(s, a). We denote random variables by capital letters (S, A, etc.) and their\ncorresponding values by lower case letters (s, a, etc.). Table 1 summarizes the notation employed in this paper.\nLet the state space S take the form S = ×d\ni=1Si, where Si deﬁnes the domain of the ith state variable. In our problems\nthe domain of each variable is either the real numbers or a ﬁnite, discrete set of values. Each state s ∈S can then be\nwritten as a d-tuple of the values of these state variables s = (s1, . . . , sd), with si ∈Si. We refer to si as the value of\nthe ith state variable. We denote by St = ×d\ni=iSt,i the random vector for the state at time t. Similarly, At is the random\nvariable for the action at time t, and Rt is the random variable for the reward at time t. In some formulas, instead of\nindexing by time, we will use “prime” notation. For example, S and S′ denote the current and next states, respectively\n(and analogously for A and A′, R and R′). When it is clear from the context, we will also refer to Si as the i-th state\nvariable. In the terminology of Koller and Friedman [2009], Si is a “template variable” that refers to the family of\nrandom variables that correspond to Si at all time steps: {S1,i, S2,i, . . . , SH,i}.\nWe are interested in problems where the set of state variables S can be decomposed into endogenous and exogenous\nsets E and X. In the simplest case, this can be accomplished by variable selection. Following the notation of Efroni\net al. [2022a], deﬁne an index set I as a subset of [d] = {1, . . . , d} and Ic = [d] \\ I as its complement. The variable\n2\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nSymbol\nMeaning\nendo, exo\nendogenous, exogenous\nCMI\nConditional Mutual Information\nCCC\nConditional Correlation Coefﬁcient\nS, E, X, A\nState, Endo State, Exo State, and Action Spaces\nS/S′\nRandom Vector for Current/Next State\ns, a, r\nRealizations of the State, Action, and Reward\nSi, Si, si\nThe ith component of S, S, s\nS, S′, A, R\nObservational data for state, next state, action, reward\nP(B)\nSpace of probability distributions over space B\nS = (E, X)\nDecomposition of S into endo and exo sets E and X\n⊆, ⊂\nSubset of, Strict (proper) subset of\n[d]\nThe set {1, . . . , d}\nI, Ic\nIndex set (⊆[d]), Complement of I (= [d] \\ I)\nS[I], S[I], s[I]\n(Si)i∈I, (Si)i∈I, (si)i∈I\nI(A; B | C)\nCMI of random vectors A and B given C\nA ⊥⊥B | C\nConditional independence of A and B given C\nR, Rd\nSet of real numbers, Real d-dimensional vector space\nRm×n\nThe vector space of m × n matrices over R\nS\nW= [E, X]\nLinear decomposition of S into endo/exo parts E and X via W\nA + B, A ⊕B\nSum (Direct Sum) of vector spaces A and B\nA ⊑B, A ⊏B\nA is a vector subspace (proper subspace) of vector space B\nA⊥\nOrthogonal complement of subspace A of vector space S\ndim(A)\nDimension of vector subspace A\nIn, 0m×n\nIdentity matrix of size n, Matrix of zeros of size m × n\nWexo\nMatrix that deﬁnes the linear exogenous subspace\ntr(A), A⊤, det(A)\nTrace of matrix A, Transpose of A, Determinant of A\n∥u∥2\nEuclidean norm of vector u\nΣAA, ΣAB\nCovariance matrix of A, Cross-covariance matrix of A, B\nN(µ, σ2)\nGaussian distribution with mean µ and variance σ2\nTable 1: Symbols and Abbreviations.\nselection formulation aims to discover an index set I so that the state vector S = ×d\ni=1Si can be decomposed into two\ndisjoint sets of state variables X = ×i∈ISi = S[I] and E = ×i∈IcSi = S[Ic]. We will also denote the corresponding\nexo and endo state spaces as X = ×i∈ISi = S[I] and E = ×i∈IcSi = S[Ic].\nIn many problems, the given state variables do not neatly separate into endogenous and exogenous subsets. Instead, we\nmust discover a mapping ξ : U 7→V such that the ﬁrst dexo dimensions of ξ(S) provide the exogenous state variables,\nand the remaining d −dexo dimensions give the endogenous state variables. In this paper, we will be interested in the\ncase where ξ is a full-rank linear transformation. In general, we will argue that ξ should a diffeomorphism so that no\ninformation in the original space S is lost in the new space ξ(S).\n2.1\nExogenous State Variables\nThe notion of exogeneity is fundamentally causal: a variable is exogenous if it is impossible for our actions to affect its\nvalue. We formalize this in terms of Pearl’s do-calculus.\nDeﬁnition 1 (Causally-Exogenous Variables). A set of state variables X = S[I] is causally exogenous for MDP M\nwith causal graph G if and only if for all times t < H, graph G encodes the conditional independence\nP(Xt+1, . . . , XH | Xt, do(At = at)) = P(Xt+1, . . . , XH | Xt)\n∀at ∈A.\n(1)\nCausal exogeneity is a qualitative property that depends only on the structure of the causal graph G and not on the\nparameters of the probability distributions associated with each node.\nConsider any state decomposition of S based on an index set I ⊆[d], so that X = S[I] and E = S[Ic]. Assuming the\nMDP is stationary, its causal graph can be described by the 2-timestep Dynamic Bayesian Network shown in Figure 1,\nwhich includes all possible edges between X, E, and actions A at the current time step and X′ and E′ in the next\n3\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n𝐴\n𝑋\n𝜋\n𝐸\n𝑋′\n𝐸′\nFigure 1: A fully-connected dynamic Bayesian network for state decomposition S = (E, X) (reward function not\nshown).\ntime step. We have included diachronic edges (from one time step to the next) and synchronic edges (within a single\ntime step). At most one of the two synchronic edges connecting X′ and E′ can be present in an MDP. In this diagram,\nE, X, E′, and X′ denote sets of state variables, and if there is an edge between two nodes U →V , it means there exists\nat least one edge between one variable in U and one variable in V .\nWe now derive a condition on the structure of the DBN causal graph that is sufﬁcient to ensure that the variables in X\nare exogenous.\nTheorem 1 (Full Exogenous DBN). Any DBN with a causal graph matching the structure of Figure 2 and any DBN\ngraph obtained by deleting edges from this structure, when unrolled for H time steps, yields an MDP for which X is\ncausally exogenous. We call the state decomposition S = (E, X) a full exo/endo decomposition with exogenous set X.\nProof. Figure 3 shows the DBN causal graph unrolled over the H-step horizon. To obtain the result, we will apply Rule\n3 of the do-calculus to remove the do(At) action. We will treat A0, . . . , At−1, At+1, . . . , AH−1 as random variables\ndistributed according to (possibly stochastic) policy π. We must show that the result holds for all possible policies.\nRule 3 states that for any causal graph G\nP(F | do(G), do(H), J) = P(F | do(G), J), if F ⊥⊥H | G ∪J in ˜G,\nwhere ˜G is the graph obtained by ﬁrst deleting all edges pointing into G and then deleting all arrows pointing into H, if\nH is not an ancestor of J.\nConsider the query P(Xt+1, . . . , XH\n| Xt, do(At)) and apply Rule 3 with the following bindings:\nF\n←\nXt+1, . . . , XH, G ←∅, H ←At, and J ←Xt; in that case, ˜G will be the same as G, except that the incom-\ning edges to At from Xt and Et are removed.\nNow apply the d-separation procedure to ˜G by forming the graph of all variables mentioned in the query\nP(Xt+1, . . . , XH | Xt) and their ancestors. This ancestral graph contains only the singleton node At and the\nMarkov chain P(X0) · P(X1 | X0) · · · P(Xt+1 | Xt) · · · P(XH | XH−1). The fact that AT is disconnected from the\nMarkov chain establishes that Xt+1, . . . , XH ⊥⊥At | Xt. Applying Rule 3 gives the result. Note that the policy π\nplays no role in the derivation. Hence, the result applies for any possible policy.\nDeleting edges from the DBN graph cannot introduce new ancestors, so X remains exogenous under edge deletion.\n𝐴\n𝑋\n𝜋\n𝐸\n𝑋′\n𝐸′\nFigure 2: Restricted dynamic Bayesian network sufﬁcient to establish that X is exogenous. We call this the “full setting”\nbecause it includes the synchronic edge from X′ to E′.\n4\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n𝐴𝑡−1\n𝐴𝑡\n𝐴𝑡+1\n𝐸𝑡−1\n𝐸𝑡\n𝐸𝑡+1\n𝑋𝑡−1\n𝑋𝑡\n𝑋𝑡+1\n𝜋\n𝜋\n𝜋\n…\n…\n𝐴0\n𝐸0\n𝑋0\n𝜋\n𝐴𝐻−1\n𝐸𝐻\n𝑋𝐻\n𝜋\nFigure 3: Unrolled state transition diagram for the full exo DBN.\nNext, we examine some properties of exo/endo decompositions.\nWe ﬁrst note that it is possible for an MDP with exogenous state variables to accept multiple exo/endo decompositions.\nIn Figure 4, the decompositions (X1 = {S2}, E1 = {S1, S3}) and (X2 = {S1, S2}, E3 = {S3}) are both valid\ndecompositions, since they match the full DBN template of Figure 2. This shows that the set E in an exo/endo\ndecomposition (E, X) may contain additional exogenous state variables not in X.\nTheorem 2 (Union of Exo/Endo Decompositions). Assume an MDP accepts two full exo/endo decompositions (E1, X1)\nand (E2, X2). Deﬁne the union of the two decompositions as the state decomposition (E, X) with X = X1 ∪X2 and\nE = E1 ∩E2. Then (E, X) is a full exo/endo decomposition with exo state set X.\nProof. Because X1 is exogenous, there are no edges from any of the remaining state variables E1 and no edges from\nthe action A to any variable in X1. Similarly, there are no edges from nodes in E2 or A to nodes in X2. Hence, the\nunion X = X1 ∪X2 also has no incoming edges from E1, E2, or A, and therefore has no incoming edges from E.\nThis shows that the decomposition (E, X) satisﬁes the conditions of Theorem 1.\nOn the other hand, not every subset of exogenous state variables can yield a valid exo/endo decomposition.\nTheorem 3 (Exogenous Subsets). Let (X, E) be a full exo/endo decomposition and X1 and X2 be non-empty, disjoint\nproper subsets of X, X1 ∩X2 = ∅, such that their disjoint union gives back X: X1 ∪X2 = X. Then (X1, X2 ∪E) is\nnot necessarily a valid full exo/endo decomposition.\nProof. By example. In Figure 4, the decomposition X3 = {S1}, E3 = {S2, S3} is not a valid full exo/endo decompo-\nsition due to the link from E′\n3 to X′\n3 (speciﬁcally the link S′\n2 →S′\n1).\nFor an MDP with exogenous state variables, we will be interested in the exo/endo decomposition where the exo set X\nis as large as possible. This is formalized in the next deﬁnition:\nDeﬁnition 2 (Maximal Exo/Endo Decomposition). Given an MDP, the full exo/endo decomposition (E, X) is maximal\nif there is no other full exo/endo decomposition ( ˜E, ˜X) with | ˜X| > |X|. We denote the maximal decomposition as\n(Em, Xm) and call Xm the maximal exo set.\nCorollary 1 (Uniqueness of Maximal Exo/Endo Decomposition). The maximal\nexo/endo decomposition of any MDP is unique.\n𝐴\n𝑆2\n𝑆3\n𝑆2\n′\n𝑆3\n′\n𝑆1\n𝑆1\n′\nFigure 4: State transition diagram of MDP with 3 state variables.\n5\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n𝐴\n𝑋\n𝜋\n𝐸\n𝑋′\n𝐸′\nFigure 5: The Diachronic Exogenous State MDP.\nProof. By contradiction. Suppose that there are two distinct maximal decompositions (E1, X1) and (E2, X2). By\nTheorem 2, their union would be a full exo/endo decomposition with an exo state set of higher cardinality. This\ncontradicts the assumption that (E1, X1) and (E2, X2) were maximal.\nCorollary 2 (Containment for Maximal Exo/Endo Decomposition). For the maximal exo/endo decomposition\n(Em, Xm), it holds that Xm ⊇X, where X is the exo set of any full exo/endo decomposition (E, X).\nProof. By contradiction. Suppose there exists a decomposition (E, X) so that X ̸⊆Xm. By Theorem 2, we could take\nthe union of (E, X) and (Em, Xm) to get a new full exo/endo decomposition with exo set X ∪Xm ⊃X of higher\ncardinality than X.\nIn our work, we speciﬁcally focus on MDPs with exogenous state variables that match the full exogenous DBN.\nDeﬁnition 3 (Exogenous State MDP). Any MDP whose structure matches Figure 2 or any subset of its edges is called\nan exogenous state MDP with exo set X.\nWe will analyze two types of exogenous state MDPs: The full setting shown in Figure 2 and the diachronic setting\nwhere the edge X′ →E′ is removed, as shown in Figure 5. Note that in the full setting, the MDP dynamics can be\nfactored as follows\nP(E′, X′ | E, X, A) = P(X′ | X) · P(E′ | E, X, A, X′).\n(2)\nSimilarly, in the diachronic setting, the MDP dynamics can be factored as\nP(E′, X′ | E, X, A) = P(X′ | X) · P(E′ | E, X, A).\n(3)\nIt is useful to draw a distinction between state variables that are causally exogenous and state variables that are\nstatistically exogenous.\nDeﬁnition 4 (Statistically-Exogenous Variables). A set of state variables X = S[I] is statistically exogenous in MDP\nM if the transition probability distribution P(E′, X′|E, X, A) can be factored according to either (2) or (3).\nIn reinforcement learning, we typically start with an MDP for which we do not know anything about the causal graph\nbeyond the fact that it deﬁnes an MDP. However, by collecting data and ﬁtting a model of the transition probabilities,\nwe may infer that the MDP dynamics satisfy (2) or (3). In such cases, we can conclude that X is statistically exogenous\nbut we cannot infer that X is causally exogenous. In Section 3.4, we will discuss additional assumptions required to\nconclude that a set of state variables that are statistically exogenous are also causally exogenous. In Appendix A, we\nprovide a structural characterization of exogenous state variables and MDPs with exogenous state variables, and show\nthat under certain conditions the maximal exo set contains all exogenous state variables, with the exception of some\nedge cases.\n2.2\nAdditive Reward Decomposition\nIn this paper, we identify and analyze a case where reinforcement learning can be accelerated even when the exogenous\nvariables are all relevant to the policy, the dynamics, and the reward. This case arises when the reward function can be\ndecomposed additively into two functions, Rexo, which only depends on X, and Rend, which can depend on both X\nand E.\nDeﬁnition 5 (Additively Decomposable Exogenous State MDP). An Additively\nDecomposable Exogenous State MDP is an Exogenous State MDP whose reward function can be decomposed into the\nsum of two terms\nR(x, e, a) = Rexo(x) + Rend(x, e, a),\n6\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nwhere Rexo :X 7→P(R) is the exogenous reward function and Rend :E × X × A 7→P(R) is the endogenous reward\nfunction. If the reward function is deﬁned as a distribution over S × A × S, we instead consider the decomposition\nR(x, e, a, x′, e′) = Rexo(x, x′) + Rend(x, e, a, x′, e′).\nLet mexo(x) and σ2\nexo(x) < ∞be the mean and variance of the exogenous reward distribution in state x. Similarly, let\nmend(e, x, a) and σ2\nend(e, x, a) < ∞be the mean and variance of the endogenous reward distribution for state-action\npair (e, x, a).\nTheorem 4. For any Additively Decomposable Exogenous State MDP with exo set X, the H-step ﬁnite-horizon Bellman\noptimality equation can be decomposed into two separate equations, one for a Markov Reward Process involving only\nX and Rexo and the other for an MDP (the endo-MDP) involving only Rend\nV (e, x; h) = Vexo(x; h) + Vend(e, x; h)\n(4)\nVexo(x; h) = mexo(x) + γEx′∼P (x′|x)[Vexo(x′; h −1)]\n(5)\nVend(e, x; h) = max\na\nmend(e, x, a) + Ex′∼P (x′|x);e′∼P (e′|e,x,a)[Vend(e′, x′; h −1)].\n(6)\nProof. We consider the diachronic setting; the full setting can be shown similarly by replacing P(e′ | x, e, a) by\nP(e′ | x, x′, e, a).\nProof by induction on the horizon H. Note that the expectations could be either sums (if S is discrete) or integrals (if S\nis continuous).\nBase case: H = 1; we take one action and terminate.\nV (e, x; 1) = mexo(x) + max\na\nmend(x, a).\nThe base case is established by setting Vexo(x; 1) = mexo(x) and Vend(e, x; 1) = maxa mend(e, x, a).\nRecursive case: H = h.\nV (e, x; h) = mexo(x) + max\na {mend(e, x, a) +\nEx′∼P (x′|x);e′∼P (e′|e,x,a)[Vexo(x′; h −1) + Vend(e′, x′; h −1)]}.\nDistribute the expectation over the sum in brackets and simplify. We obtain\nV (e, x; h) = mexo(x) + γEx′∼P (x′|x)[Vexo(x′; h −1)] +\nmax\na {mend(e, x, a) + γEx′∼P (x′|x);e′∼P (e′|e,x,a)[Vend(e′, x′; h −1)]}.\nThe result is established by setting\nVexo(x; h) = mexo(x) + γEx′∼P (x′|x)[Vexo(x′; h −1)]\nVend(e, x; h) = max\na {mend(e, x, a) + γEx′∼P (x′|x);e′∼P (e′|e,x,a)[Vend(e′, x′; h −1)]}.\nCorollary 3. Any optimal policy for the endo-MDP of Equation (6) is an optimal policy for the full exogenous state\nMDP.\nProof. Because Vexo(s; H) does not depend on the policy, the optimal policy can be computed simply by solving the\nendo-MDP.\nWe will refer to Equations (4), (5) and (6) as the Exo/Endo Decomposition of the full MDP. In the remainder of this\npaper, unless stated otherwise, we will work with the full setting because it is more general.\nBray (2019) proves a result similar to Theorem 4. He also identiﬁes conditions under which value iteration and\npolicy iteration for a fully-speciﬁed Endo-MDP can be accelerated by computing the eigenvector decomposition of the\nendogenous transition matrix. While such techniques are useful for MDP planning with a known transition matrix,\nwe do not know how to exploit them in reinforcement learning where the MDP is unknown. In other related work,\nMcGregor et al. (2017) show how to remove known exogenous state variables in order to accelerate the Model Free\nMonte Carlo algorithm [Fonteneau et al., 2012]. Their experiments obtain substantial improvements in policy evaluation\nand reinforcement learning.\n7\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n2.3\nVariance Analysis of the Exo/Endo Decomposition\nSuppose we are given the decomposition of the state space into exogenous and endogenous sets. Under what conditions\nwould reinforcement learning on the endogenous MDP be more efﬁcient than on the original MDP? To explore this\nquestion, let us consider the problem of estimating the value of a ﬁxed policy π in a given start state s0 via Monte Carlo\ntrials of length H. We will compare the sample complexity of estimating V π(s0; H) on the full MDP to the sample\ncomplexity of estimating V π\nend(s0; H) on the Endogenous MDP. Of course most RL algorithms must do more than\nsimply estimate V π(s0; H) for ﬁxed π, but the difﬁculty of estimating V π(s0; H) is closely related to the difﬁculty of\nﬁtting a value function approximator or estimating the gradient in a policy gradient method.\nDeﬁne Bπ(s0; H) to be a random variable for the H-step cumulative discounted return of starting in state s0 and\nchoosing actions according to π for H steps. The expected value of Bπ(s0; H) is the value function V π(s0; H). To\ncompute a Monte Carlo estimate of V π(s0; H), we will generate N realizations b1, . . . , bN of Bπ(s0; H) by executing\nN H-step trajectories in the MDP, each time starting in s0.\nTheorem 5. For any ϵ > 0 and any 0 < δ < 1, let ˆV π(s0; H) = N −1 PN\ni=0 bi be the Monte Carlo estimate of the\nexpected H-step return of policy π starting in state s0. If\nN ≥Var[Bπ(s0; H)]\nδϵ2\nthen\nP[| ˆV π(s0; H) −V π(s0; H)| > ϵ] ≤δ.\nProof. This is a simple application of the Chebychev Inequality,\nP(|X −E[X]| > ϵ) ≤Var[X]\nϵ2\n,\nwith X = ˆV π(s0; H). The variance of the mean of N iid random variables is the variance of any single variable divided\nby N. Hence Var[ ˆV π(s0; H)] = Var[Bπ(s0; H)]/N. To obtain the result, plug this into the Chebychev inequality,\nset the rhs equal to δ, and solve for N.\nNow let us consider the Exo/Endo decomposition of the MDP. Let Bπ\nexo(s0; H) denote the H-step return of the\nexogenous MRP and Bπ\nend(s0; H) denote the return of the endogenous MDP. Then Bπ\nx(s0; H) + Bπ\ne (s0; H) is a\nrandom variable denoting the cumulative H-horizon discounted return of the original, full MDP. Let Var[Bπ(s0; H))]\nbe the variance of Bπ(s0; H) and Cov[Bπ\nexo(s0; H), Bπ\nend(s0; H)] be the covariance between the exogenous and\nendogenous returns.\nTheorem 6. The Chebychev upper bound on the number of Monte Carlo trials required to estimate\nV π(s0; H) using the endogenous MDP will be reduced compared to the full MDP iff Var[Bπ\nexo(s0; H)] >\n−2 Cov[Bπ\nexo(s0; H), Bπ\nend(s0; H)].\nProof. From Theorem 5, we know that the sample size bound using the endogenous MDP will be less than the required\nsample size using the full MDP when Var[Bπ\nend(s0; H)] < Var[Bπ\nexo(s0; H) + Bπ\nend(s0; H)]. The variance of the\nsum of two random variables is\nVar[Bπ\nexo(s0; H) + Bπ\nend(s0; H)] = Var[Bπ\nexo(s0; H)] + Var[Bπ\nend(s0; H)] +\n2Cov[Bπ\nexo(s0; H), Bπ\nend(s0; H)].\nHence,\nVar[Bπ\nend(s0; H)] < Var[Bπ\nexo(s0; H) + Bπ\nend(s0; H)]\nif and only if\nVar[Bπ\nexo(s0; H)] > −2Cov[Bπ\nexo(s0; H), Bπ\nend(s0; H)].\nTo evaluate this covariance condition, we need to compute the variance and covariance of the H-step returns. In\nAppendix C, we derive dynamic programming formulas for these quantities. The dynamic programs allow us to check\nthe covariance condition of Theorem 6 in every state, including the start state s0, so that we can decide whether to solve\nthe original MDP or the endo-MDP. Some special cases are easy to verify. For example, if the mean exogenous reward\nmexo(s) = 0, for all states, then the covariance condition reduces to σ2\nexo(s) > 0.\n8\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nAlgorithm 1 Practical RL with the Exo/Endo Decomposition\n1: Inputs: Decomposition steps L, Exogenous reward update steps M, Policy update steps K, Total steps N\n2: Datasets: RL training tuples D, Exogenous reward examples Dexo\n3: //Phase 1\n4: Initialize policy and/or value networks randomly\n5: for i = 1 to L do\n6:\nRun RL to collect a new transition ⟨si, ai, ri, s′\ni⟩and add it to D\n7:\nif i mod K == 0 then\n8:\nUpdate the policy using last K observations with observed rewards ri\n9:\nend if\n10: end for\n11: Run a state decomposition algorithm (see Section 4) on D to compute the exogenous state mapping ξexo\n12: Let Dexo{(ξexo(si), ri)}L\ni=1 be the exogenous reward training set\n13: Fit the exogenous reward function ˆmexo to Dexo\n14: Update D by replacing each tuple ⟨si, ai, ri, s′\ni⟩by ⟨si, ai, ri −ˆmexo(ξexo(si)), s′\ni⟩\n15: //Phase 2\n16: for i = L + 1 to N do\n17:\nRun RL to collect a new transition ⟨si, ai, ri, s′\ni⟩\n18:\nAdd (ξexo(si), ri) to Dexo\n19:\nAdd ⟨si, ai, ri −ˆmexo(ξexo(si)), s′\ni⟩to D\n20:\nif i mod K == 0 then\n21:\nUpdate the policy using last K observations in D\n22:\nend if\n23:\nif i mod M == 0 then\n24:\nUpdate the estimate of the exogenous reward function ˆmexo with the last M observations in Dexo\n25:\nend if\n26: end for\n3\nDecomposing an Exogenous State MDP: Optimization Formulations\nWe now turn our attention to discovering the exogenous variables (or exogenous subspace) of an MDP from data\ncollected during exploration. Our overall strategy is shown in Algorithm 1. We assume we are applying an online\nreinforcement learning algorithm, such as PPO or Q-learning. As it interacts with the environment, it collects\n⟨st, at, rt, st+1⟩experience tuples into a dataset D. After L tuples have been collected, we apply an exogenous space\ndiscovery algorithm (see Section 4) to ﬁnd a function ξexo, so that xt = ξexo(st) computes the exogenous state xt\nfrom state st. By applying ξexo to each st of the experience tuples, we assemble a supervised training set Dexo of the\nform {(xt, rt)}. We then solve a regression problem to predict as much of the reward rt from xt as possible. The\nresulting ﬁtted function ˆmexo is our estimate of the mean of the exogenous reward function. Because of the additive\ndecomposition, we can therefore estimate the endogenous rewards as ˆrend,t := rt −ˆmexo(st). We then convert the\nset of experience tuples into modiﬁed tuples ⟨st, at, ˆrend,t, st+1⟩by replacing the original reward rt values with our\nestimate of the endogenous reward and resume running the online reinforcement learning algorithm. Depending on the\nalgorithm, we may need to re-initialize the data structures using the modiﬁed experience tuples. In any case, as the\nalgorithm collects additional full experience tuples ⟨st, at, rt, st+1⟩, each is converted to an experience tuple for the\nendo-MDP by replacing rt by ˆrend,t. In our experiments, we ﬁnd that there is some beneﬁt to repeating the reward\nregression at regular intervals, so we also add (ξexo(si), ri) to Dexo at each time step. However, we do not observe\nsimilar beneﬁts from rerunning the exogenous space discovery algorithm, especially when the state transition function\nis linear, so we only execute it once.\nThe heart of our approach is the exogenous state discovery problem. This is easiest to discuss for the case where we\nare given a ﬁxed set of state variables and our goal is to determine which state variables are exogenous and which\nare endogenous. Within this variable selection setting, we formulate the discovery problem as one of ﬁnding a set of\nvariables that minimizes the residuals of the ﬁtted exogenous reward function ˆmexo subject to the constraint that the\nvariables are exogenous. We formulate the exogeneity constraint in terms of conditional mutual information for both\nthe full and the diachronic DBN structures. Then we present two variations of the optimization formulation. The ﬁrst\nvariation solves the discovery problem jointly by selecting the exogenous features that minimize the residuals of the\nexogenous reward regression. The second variation takes a hierarchical approach in which we ﬁrst select the maximal\nexogenous subspace and then perform the reward regression. We will show that this two-phase algorithm ﬁnds the same\n9\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nsolution as the joint optimization. This is the approach adopted in Algorithm 1, because it allows us to repeat the reward\nregressions without recomputing the exogenous subspace.\nAfter presenting these ideas in the context of exogenous variable selection, we then consider the most general formulation\nin which the exo-endo decomposition is deﬁned by a diffeomorphism that maps from a continuous state space to a\nspace that separates the exogenous and endogenous subspaces.\nFinally, we study a practical special case in which the exogenous and endogenous spaces are deﬁned by a linear mapping.\nIn this setting, we show how to approximate the mutual information constraints by constraints on the conditional\ncorrelation coefﬁcients. In Section 4, we will then present two algorithms for solving the linear formulation.\n3.1\nVariable Selection Formulation\nThe variable selection formulation aims to discover an index set I with the following two properties:\n• I decomposes the set of state variables S = ×d\ni=1Si into two disjoint sets X = ×i∈ISi and E = ×i∈IcSi\nthat satisfy the structure of Figure 2 or Figure 5.\n• The squared error of the exogenous reward regression, P\nt[ ˆmexo(xt) −rt]2 is minimized, where ˆmexo\nregresses rt onto X = ×i∈ISi.\nHow can we state the ﬁrst property in a form suitable for constrained optimization? We can express the required\nstructure in terms of two conditional mutual information (CMI) constraints. The full setting can be enforced by the\nconstraint\nI(X′; [E, A] | X) = 0,\n(7)\nwhere X = S[I] and E = S[IC]. This says that if we know the value of X, then the combination of the endogenous\nstate variables and the action carries no additional information about X′, the value of the exogenous state in the next\ntime step. Note that this does not constrain the synchronic link X′ →E′.\nTo remove the synchronic link and enforce the diachronic structure, we can strengthen the conditional mutual information\nconstraint as follows\nI(X′; [E, A, E′] | X) = 0\n(8)\nThis says that given the exogenous state X, X′ carries no information about E, A, or E′.\nNote that our discovery algorithms will be evaluating these conditional mutual information constraints on the ﬁnite\nsample of tuples collected from exploring the MDP. Below, we will discuss the conditions that must be satisﬁed by the\nMDP and the exploration policy to ensure the soundness of these conditional mutual information computations.\n3.1.1\nCoupled Formulation\nWe can now formalize the exogenous space discovery problem for the full setting as follows\nI∗, ˆm∗\nexo =\narg min\nI⊆[d], ˆmexo:X7→R\nE[( ˆmexo(X) −R)2]\nsubject to I(X′; [E, A] | X) = 0, X = S[I], E = S[Ic].\n(9)\nFor the diachronic setting, we must use the stronger CMI constraint of Equation (8) instead.\n3.1.2\nHierarchical Formulation\nFormulation (9) is coupled—that is, it simultaneously involves the exo/endo state decomposition and the exogenous\nreward regression. The following gives an alternative hierarchical formulation that decouples the optimization of the\nstate representation from the reward regression\nˆm∗\nexo =\narg min\nˆmexo:X ∗7→R\nE[( ˆmexo(X) −R)2], where X∗= S[I∗] and\n(10)\nI∗= arg max\nI⊆[d]\n|I|\n(11)\nsubject to I(X′; [E, A] | X) = 0, X = S[I], E = S[Ic].\n(12)\nFormulation (10)-(12) breaks the coupled problem into two subproblems: Subproblem (11)-(12) computes the maximal\nexo/endo decomposition, while subproblem (10) computes the best exogenous reward estimator corresponding to the\n10\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\npreviously-computed exo/endo decomposition. The formulation is hierarchical, since we ﬁrst compute the exo/endo\nstate representation and subsequently ﬁt the exogenous reward function. Our ultimate goal is still to solve the coupled\nformulation; the hierarchical formulation serves as a computationally simpler proxy. How do the two formulations\nrelate? Let Icoupled be the solution to the coupled formulation and Ihier be the solution to the hierarchical formulation.\nThe containment property of Corollary 2 implies that Icoupled ⊆Ihier, since the maximal exo set of (11)-(12) contains\nthe exo set of any other exo/endo decomposition, including the endo/exo decomposition of (9). It is possible that\nIcoupled ⊂Ihier if there are exogenous variables that carry no information about the exogenous reward function Rexo.\nThe hierarchical formulation will include those variables in the reward regression, and this may increase the variance\nof the estimated ˆmexo, because the regression will need to ﬁt zero-valued weights for these irrelevant variables. The\nvariance can be addressed in the usual way by employing L1 regularization when ﬁtting ˆmexo. Hence in practice (and\nin the limit of inﬁnite data), the two formulations will produce the same solution.\n3.2\nContinuous Formulation\nIn the continuous formulation, we assume that the state space S ⊆Rd is an open subset U ⊆Rd. Each s ∈S is thus\na d-dimensional real-valued vector. The discovery problem is to ﬁnd a mapping ξ : U 7→V where V ⊆Rd is also\nan open set such that the endogenous and exogenous state spaces can be readily extracted. Without loss of generality,\nwe stipulate that the ﬁrst dexo components in ξ(S) deﬁne the exogenous state, while the remaining dend = d −dexo\ncomponents deﬁne the endogenous state. Hence, I = [dexo], so that X = ξ(S)[I] and E = ξ(S)[Ic].\nWhat conditions must the mapping ξ satisfy? It is desirable that the mapping preserve probability densities so that the\nresulting exo/endo MDP is equivalent to the original MDP. A natural choice is the class of diffeomorphisms [Tu, 2010].\nA diffeomorphism from an open subset U ⊆Rd to an open subset V ⊆Rd is deﬁned as a bijective map ξ : U 7→V so\nthat both ξ and its inverse ξ−1 are continuously differentiable. We denote the class of such diffeomorphisms from U to\nany possible open set V as Xd\nU.\nDiffeomorphisms have several important properties. From an information theoretic point of view, they do not lose\nany information about the original random variables. In particular, by using the change of variables formula, we can\nanalytically write the probability density function of the transformation ξ using the probability density of S and the\nJacobian of the transformation. Assuming the density of S is pS(s′ | s, a), the density pZ(z′ | z, a) for the transformed\nvariables Z = f(S) is given by\npZ(z′ | z, a) = pS(f −1(z′) | f −1(z), a) · | det(Jf −1(z′))|,\nwhere Jf −1(z′) is the Jacobian matrix of the inverse transformation f −1 and det(·) is the determinant. In particular,\nR\nU pS(s′ | s, a)ds′ =\nR\nV pZ(z′ | z, a)dz′ = 1. Hence, the original MDP deﬁned on S can be transformed into an\nequivalent MDP deﬁned on the transformation ξ(S). An implication of the above is that mutual information between\ntwo random vectors is preserved under diffeomorphisms [Kraskov et al., 2004]. This result can be extended to the\nconditional mutual information. As a result, independence and conditional independence are also preserved under\ndiffeomorphisms, as they correspond to the special case of zero mutual or conditional mutual information.\nWith these assumptions, the coupled formulation takes the following form\nξ∗, ˆm∗\nexo, d∗\nexo =\narg min\nξ∈Xd\nS, ˆmexo:X7→R,dexo∈{0,...,d}\nE[( ˆmexo(X) −R)2]\nsubject to I(X′; [E, A] | X) = 0,\nwhere I = [dexo], X = ξ(S)[I], and E = ξ(S)[Ic].\n(13)\nThis formulation jointly optimizes the diffeomorphic state transformation and the exogenous reward function, so that\nthe latter best ﬁts the observed reward. Notice that the optimal dexo is unknown, so the formulation for the optimal\ndecomposition must consider all possible values for dexo.\nSimilarly, we can deﬁne the hierarchical formulation\nˆm∗\nexo = arg min\nˆmexo:X ∗7→R\nE[( ˆmexo(X∗) −R)2], where X ∗= S[I∗], I∗= [d∗\nexo], and\n(14)\nξ∗, d∗\nexo =\narg max\nξ∈Xd\nS,dexo∈{0,...,d}\ndexo\n(15)\nsubject to I(X′; [E, A] | X) = 0,\n(16)\nwhere I = [dexo], X = ξ(S)[I], and E = ξ(S)[Ic].\n(17)\nEquations (15)-(17) ﬁrst compute the diffeomorphic state transformation so that the corresponding exo/endo decompo-\nsition is valid and has the highest possible number of exogenous state variables. Equation (14) subsequently estimates\nthe exogenous reward function.\n11\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nAs before, we can replace the CMI constraint in these formulations with the stronger version from Equation (8) to\nenforce the diachronic DBN structure.\nThe diffeomorphic formulation provides a general framework for continuous state variables. However, diffeomorphic\ntransformations do not usually accept a simple parameterization. They can in principle be represented by invertible\nneural networks [Ardizzone et al., 2019, Behrmann et al., 2021] or normalizing ﬂows [Rezende and Mohamed, 2015,\nKobyzev et al., 2021]. Particular care must be taken to ensure that the corresponding mappings are not just bijective\nbut also continuously differentiable with a continuously differentiable inverse. This can be very restrictive in practice.\nRecent work by Koenen et al. [2021] relaxes the deﬁnition of a diffeomorphism to allow for mappings that are bijective\nand continuously differentiable almost everywhere, with a set of critical points of Lebesgue measure 0. The new\nclass of transformations, named Lebesgue-diffeomorphisms or L-diffeomorphisms, is consistent with several types of\nnormalizing ﬂows and can thus replace the more rigid class of diffeomorphisms.\n3.3\nLinear Formulation\nWe now introduce a tractable formulation for the general class of continuous diffeomorphisms from Section 3.2.\nConcretely, we consider the general linear group GL(d, R) of invertible linear transformations in vector space Rd.\nInstead of directly deﬁning the full state transformation ξ, we start by deﬁning a linear mapping ξexo from the full state\nspace to the exogenous state space. We then deﬁne the endogenous state space as its orthogonal complement. Let ξexo\nbe speciﬁed by a matrix Wexo ∈Rd×dexo with 0 ≤dexo ≤d, where dexo is the dimension of subspace X. Given a\nstate s, its projected exogenous state is W ⊤\nexo · s. The endogenous subspace is the orthogonal complement of X of\ndimension dend = d −dexo, written E = X ⊥and deﬁned by some matrix Wend. The endogenous state e contains the\ncomponents of s in subspace E. In the linear setting, E and X are vector subspaces of vector space S, and we can write\nS = E ⊕X, with dim(S) = dim(E) + dim(X). We will use the notation\nS\nW= [E, X],\nto denote the state-space decomposition deﬁned by W = (Wexo, Wend). The matrix W is invertible with rank d, and\nhence, deﬁnes a linear diffeomorphism.\nLet P ∈P(S) denote a probability distribution over the state space. We can map this distribution into a distribution ˜P\nin the decomposed state space through the change of variables formula:\n˜P(X = x, E = e) =\n1\n| det(W)|P(S = W −1[x, e]).\n(18)\nThe marginal distributions ˜P(X) and ˜P(E) can be computed by marginalizing in the original space. For example,\n˜P(X = x) =\n1\n| det(W)|\nZ\ne\nP(S = W −1[x, e])de.\nIn the remainder of the paper, we will simply write P(X) and P(E) for the probability distributions in the transformed\nspace.\nFor computational convenience, we impose the requirement that the columns of Wexo consist of orthonormal vectors,\nso that W ⊤\nexo · Wexo = Idexo. Linear decomposition theory [Friedberg et al., 2002] then tells us that the exogenous state\ncan be represented in the original d-dimensional basis as x = ξexo(s) = Wexo · W ⊤\nexo · s, and the endogenous state\ne = ξend(s) = s −x = s −Wexo · W ⊤\nexo · s. Under this approach, ξexo(s) + ξend(s) = x + e = s. Every state s ∈S\ncan be written uniquely in this way (i.e., this is a direct sum) for a given exogenous projection matrix Wexo. Of course,\nlike Wexo, the endogenous matrix Wend can also be expressed using orthonormal components.\nBecause diffeomorphisms preserve mutual information [Kraskov et al., 2004], the conditional mutual information\nconstraint holds in the mapped state space\nI(Wexo · S′; [Wend · S, A] | Wexo · S) = 0\nif and only if it holds in the original space,\nI(X′; [E, A] | X) = 0.\nNote that it is possible for two different matrices Wexo and W ′\nexo to satisfy the conditional mutual information constraint\nif they are related by an invertible matrix U ∈Rdexo×dexo as Wexo = W ′\nexo · U (and similarly for Wend). Hence, when\nwe solve the coupled (Equation (13)) or hierarchical (Equations (14)-(17)) formulations, the exogenous subspace is only\n12\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nidentiﬁed up to multiplication by an invertible matrix U. This can make it difﬁcult to interpret the discovered subspace.\nOne suggestion is to put Mexo into block-diagonal canonical form, so that to the extent possible, the coordinates of the\nmapped space are the same as in the original space.\nTo develop the linear formulations of the discovery problem, let us consider the database D of {(si, ai, ri, s′\ni)}N\ni=1\nsample transitions collected in Algorithm 1. We start by centering si and s′\ni by subtracting off the mean of the observed\nstates. Let S ∈RN×d, A ∈RN×k, R ∈RN×1, S′ ∈RN×d be the matrices containing the observations of si, ai, ri,\nand s′\ni, respectively. These are samples from the random variables S, A, R, S′, and we will estimate the expectations\nrequired in the optimization formulations (expected squared reward regression error and CMI values) from these\nsamples. Given observation matrices S, S′, we can write the corresponding exogenous and endogenous states as\nX = S · Wexo, X′ = S′ · Wexo, E = S −S · Wexo · W ⊤\nexo, and E′ = S′ −S′ · Wexo · W ⊤\nexo.\nThe exogenous reward regression problem takes a particularly simple form if we adopt linear regression. Let ˆX =\nS · Wexo be the matrix of estimated exogenous state vectors, and let w∗\nR be the ﬁtted coefﬁcients of the linear regression.\nThis coefﬁcient vector can be computed as the solution of the usual least squares problem\nw∗\nR = arg min\nwR∈Rdexo\n∥ˆX · wR −R∥2\n2 = arg min\nwR∈Rdexo\n{( ˆX · wR −R)⊤· ( ˆX · wR −R)} = ( ˆX⊤ˆX)−1 ˆX⊤R.\n(19)\nThis gives us the optimization objective for the linear formulation. Now let us consider how to express the conditional\nmutual information constraints.\nEstimating mutual information (and conditional mutual information) has been studied extensively in machine learning.\nRecent work exploits variational bounds [Donsker and Varadhan, 1983, Nguyen et al., 2007, Nowozin et al., 2016,\nBarber and Agakov, 2003, Blei et al., 2017] to enable differentiable end-to-end estimation of mutual information with\ndeep nets [Belghazi et al., 2018, Poole et al., 2019, Alemi et al., 2018, Hjelm et al., 2019, van den Oord et al., 2018].\nDespite their promise, mutual information estimation by maximizing variational lower bounds is challenging due\nto inherent statistical limitations [McAllester and Stratos, 2020]. Alternative approaches for estimating the mutual\ninformation include k-nearest neighbors [Kraskov et al., 2004], ensemble estimation [Moon et al., 2017], jackknife\nestimation [Zeng et al., 2018], kernel density estimation [Kandasamy et al., 2015, Han et al., 2020], and Gaussian copula\nmethods [Singh and P´oczos, 2017]. All of these require substantial computation, and some of them also require delicate\nhyperparameter tuning. Extending them to estimate conditional mutual information raises additional challenges.\nWe have chosen instead to replace conditional mutual information with a quantity we call the conditional correlation\ncoefﬁcient (CCC). To motivate the CCC, assume that variables X, Y, Z are distributed according to a multivariate\nGaussian distribution. In this case, it is known [Baba et al., 2004] that X and Y are conditionally independent given Z,\nif and only if\nΣXY −ΣY ZΣ−1\nZZΣXZ = 0,\nwhere ΣAA is the covariance matrix of A and ΣAB is the cross-covariance matrix of A and B. We can normalize the\nabove expression to obtain the normalized cross-covariance matrix\nV (X, Y, Z) = Σ−1/2\nXX (ΣXY −ΣXZΣ−1\nZZΣZY )Σ−1/2\nY Y\n= 0.\n(20)\nIt is not hard to see that Equation (20) holds if and only if\ntr(V ⊤(X, Y, Z) · V (X, Y, Z)) = 0,\nwhere tr(·) is the trace function. We call the quantity tr(V ⊤(X, Y, Z) · V (X, Y, Z)) the conditional correlation\ncoefﬁcient (CCC), and we denote it by CCC(X, Y | Z).2 Because (20) involves matrix inversion, we apply Tikhonov\nregularization [Tikhonov and Arsenin, 1977] to all inverse matrices with a small positive constant λ > 0 for numerical\nstability. For instance, Σ−1/2\nXX becomes (ΣXX + λ · In)−1/2, where n is the size of random vector X.\nOf course, the equivalence of zero CCC and conditional independence does not necessarily hold outside of the\nmultivariate Gaussian distribution. A more general approach, which inspired our CCC method, would be to employ\nkernel measures of conditional independence [Fukumizu et al., 2004, 2008]. These deﬁne normalized cross-covariance\noperators on reproducing kernel Hilbert spaces (RKHS) and establish that conditional independence holds if and only\nif the Hilbert-Schmidt norm of the operator is 0. Unfortunately, mapping our data into an RKHS would make the\nexogenous space discovery problem hard to optimize in terms of the underlying projection matrix Wexo. Instead, we\ndirectly optimize over the linearly projected data. Our experiments in Section 5 demonstrate that, despite being much\nsimpler than kernel measures of conditional independence, our method can still provide useful results in MDPs with\nnonlinear dynamics.\n2In Dietterich et al. [2018], we referred to this quantity as the Partial Correlation Coefﬁcient (PCC), but this was an error. While\nthe PCC can be used to determine conditional independence for Gaussians, it is a different quantity.\n13\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nWe can now express the coupled formulation of the exogenous subspace discovery problem for the full setting as follows\nmin\n0≤dexo≤d,Wexo∈Rd×dexo,wR∈Rdexo ∥S · Wexo · wR −R∥2\n2\nsubject to W ⊤\nexoWexo = Idexo\nCCC(S′Wexo; [S −SWexoW ⊤\nexo, A] | SWexo) < ϵ.\n(21)\nNote that we have relaxed the CCC constraint slightly to allow non-zero values less than a small constant ϵ.\nTo obtain a linear coupled formulation for the diachronic case, let\nY (Wexo) = [S −SWexoW ⊤\nexo, S′ −S′WexoW ⊤\nexo, A].\nThis creates a stacked matrix corresponding to [E, E′, A]. We can then write the CCC constraint for the diachronic\ncase as\nCCC(S′Wexo; Y (Wexo) | SWexo) = 0.\nThis is the CCC equivalent of the diachronic CMI constraint of Equation (8).\nThe linear hierarchical formulation for the full case can be written as\nmin\nwR∈Rd∗exo ∥S · W ∗\nexo · wR −R∥2\n2\nwhere\nd∗\nexo, W ∗\nexo =\narg max\ndexo∈{0,...,d},Wexo∈Rd×dexo\ndexo\nsubject to W ⊤\nexoWexo = Idexo\nCCC(S′Wexo; [S −SWexoW ⊤\nexo, A] | SWexo) < ϵ.\n(22)\nAlthough we have written the outer objective in terms of linear regression, this is not essential. Once the optimal linear\nprojection matrix W ∗\nexo ∈Rd×dexo has been determined and the reward regression dataset Dexo has been constructed,\nany form of regression—including nonlinear neural network regression—can be employed.\n3.4\nConditions Establishing Sound Inference\nWhat conditions must hold so that the coupled or hierarchical optimization discovery formulations, when applied\nto the collected data D, ﬁnd valid exo/endo decompositions? In this section, we address this question for “tabular”\nMDPs—that is, MDPs with ﬁnite, discrete state and action spaces. To ﬁnd valid exo/endo decompositions, we need\nto ensure that the factorizations of Equations (2) or (3) hold in all states of the MDP. This means that our exploration\npolicy needs to visit all states, and it needs to execute all possible actions in each state to verify that the action does not\naffect (either directly or indirectly) the exogenous variables. We formalize this as follows.\nConsider an idealized version of Algorithm 1 that collects the tuple dataset D by executing a ﬁxed exploration policy\nπx for a large number of steps. We will require that πx is fully randomized, according to the following deﬁnition:\nDeﬁnition 6 (Fully Randomized Policy). An exploration policy πx is fully randomized for a tabular MDP with action\nspace A if πx assigns non-zero probability to every possible action a ∈A in every state s ∈S.\nWe will also require that the structure of the MDP is such that a fully-randomized policy will visit every state s ∈S\ninﬁnitely often.\nDeﬁnition 7 (Admissible MDP). A tabular MDP is admissible if any fully-randomized policy will visit every state in\nthe MDP inﬁnitely often.\nExamples of admissible MDPs include episodic MDPs and ergodic MDPs. An episodic MDP begins each episode in a\nﬁxed start state s0 and executes a policy until a terminal state is reached. Then it resets to the starting state. It must\nsatisfy the requirement that all policies will reach a terminal state in a ﬁnite number of steps. The simplest episodic\nMDP always terminates after a ﬁxed number of steps H, which is called the horizon time of the MDP. Note that if an\nepisodic MDP contains states that are not reachable from the start state s0 by any policy, then these must be deleted\nfrom the MDP in order to satisfy the deﬁnition of admissibility.\nAn ergodic MDP has the property that for all policies, every state is reachable from every other state in a ﬁnite number\nof steps, and the time between successive visits to any given state is aperiodic.\n14\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nTheorem 7 (Soundness of Empirical Conditional Mutual Information). Let M be an admissible MDP, and let\nD be a set of ⟨s, a, r, s′⟩tuples collected by executing fully-randomized policy πx for n steps and recording the\nstate, action, reward, and result state at each step. Let (X, E) be a proposed exo/endo decomposition of S. Let\nˆP(E, X, A, E′, X′) be the maximum-likelihood estimate of the joint distribution of the decomposed (S, A, S′) triples,\nand let ˆI(X′; E, A, E′|X) be the corresponding estimate of the conditional mutual information for the diachronic form.\nIf limn→∞ˆI(X′; E, A, E′|X) = 0 then (X, E) is a valid exo/endo decomposition of S.\nProof. From the structure of an MDP, we know that the joint distribution ˆP(E, A, E′X′) can be factored as\nˆP(E, X, A, E′X′) = ˆP(E, X) ˆP(A|E, X) ˆP(E′, X′|E, X, A).\nThe ﬁrst term is the empirical probability of visiting state S = (E, X), the second term is the empirical estimate of πx,\nand the third term is the estimated transition dynamics of the MDP. The empirical conditional mutual information can\nbe written as\nˆI(X′; E, A, E′|X) =\nX\nE,X,A,E′,X′\nˆP(E, X, A, E′, X′)\n\"\nlog\nˆP(X′, E, A, E′|X)\nˆP(X′|X) ˆP(E, A, E′|X)\n#\n.\nIf ˆI(X′; E, A, E′|X) = 0, then it is easy to show that the fraction inside the log must be equal to 1 for all\n(E, X, E′, X′, A) for which ˆP(E, X, E′, X′, A) > 0. As n gets large, this will hold for all possible state transi-\ntions. Hence,\nˆP(X′, E, A, E′|X) = ˆP(X′|X) ˆP(E, A, E′|X).\n(23)\nFrom the structure of the MDP, we know the left-hand side of (23) can be rewritten as\nˆP(X′, E, A, E′|X) = ˆP(X′, E′|E, X, A) ˆP(A|E, X) ˆP(E|X).\nBy applying the chain rule of conditional probability, we can rewrite the right-hand side of (23) as\nˆP(X′|X) ˆP(E, A, E′|X) = ˆP(X′|X) ˆP(E′|E, X, A) ˆP(A|E, X) ˆP(E|X).\nSubstituting these into (23) gives\nˆP(X′, E′|E, X, A) ˆP(A|E, X) ˆP(E|X) = ˆP(X′|X) ˆP(E′|E, X, A) ˆP(A|E, X) ˆP(E|X).\nBecause the MDP is admissible and πx is fully randomized, then for n sufﬁciently large,\nˆP(A|E, X) > 0\n∀E, X, A\nˆP(E|X) > 0\n∀E, X.\nHence, we can cancel them from both sides of the equation to obtain\nˆP(X′, E′|E, X, A) = ˆP(X′|X) ˆP(E′|E, X, A).\nAs n →∞, all estimates will converge to their true values, and we will obtain\nP(X′, E′|E, X, A) = P(X′|X)P(E′|E, X, A),\n(24)\nwhich is the factorization for the diachronic Exogenous State MDP. This proves that (X, E) is a valid exo/endo\ndecomposition of the diachronic form.\nBecause we know the exploration policy πx, we could replace ˆP(A|E, X) by πx(A|E, X) throughout this derivation.\nNote also that P(E′, X′|E, X, A) will be zero for states S′ = (E′, X′) that cannot be directly reached from (E, X) by\nany action A. This is not a problem, because the conditional mutual information formula is only concerned with the\nterms where P(E, X, A, E′, X′) > 0.\nAn analogous theorem and proof can easily be provided for Exogenous State MDPs of the full form.\nFor purposes of computing a valid exo/endo decomposition that respects the full or diachronic factorization, this theorem\nsufﬁces. But it only shows that the variables in X are statistically exogenous. We might also wish to prove that they are\ncausally exogenous according to Deﬁnition 1. To apply this deﬁnition, we need a causal graph. To infer the structure of\na causal graph from observational data, we can make the following faithfulness assumption [Spirtes et al., 2000].\nAssumption 1 (Faithfulness). The set of conditional independencies exhibited by the observed distribution P(S′|S, A)\nis exactly the set of conditional independencies entailed by the causal graph of the MDP.\n15\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nCorollary 4. If the conditions of Theorem 7 hold, the underlying structural causal model has the structure of a\nstationary MDP, and the causal model satisﬁes the faithfulness assumption, then the set X satisﬁes the causal deﬁnition\nof exogeneity in Deﬁnition 1.\nProof. From the assumptions (faithfulness, MDP structure, and Theorem 7), we know that every probabilistic de-\npendency (and independency) implied by the structure of the causal graph will be exhibited by the observed data\nD.\nConsider the diachronic setting ﬁrst. Assume that any of the links E →X, A →X or E′ →X were present in the\ncausal graph G. In that case, variables E, A, E′ are not d-separated from X′ given X. But then faithfulness would\nimply that X′ ̸⊥⊥E, A, E′ | X, which is a contradiction. Hence the causal graph has the form of Figure 5.\nFor the full setting, we can similarly show the links A →X and E →X must be missing. What about link E′ →X′?\nAssume that such a link indeed exists in G. Consider any state variable E′\ni in set E′, so that the link E′\ni →X′ is\npresent. If either link A →E′\ni or E →E′\ni is present, then there is a directed path from A (or E) to X′ through E′\ni.\nBut in that case variables A (or E) and X′ are not d-separated given X, so the faithfulness assumption implies that\nX′ ̸⊥⊥A | X (or X′ ̸⊥⊥E | X), which is a contradiction. What if there is no link from A or E to E′\ni? In that case, we\ncan immediately see that both X and E′\ni are causally exogenous. Hence the causal graph has the form of Figure 2.\nIn summary, if the conditional mutual information I(X′; E, A, E′|X) is zero, then the factorization in (24) matches the\nstructure of the causal graph G, and the set X satisﬁes the causal deﬁnition of exogeneity.\nNote that if the faithfulness assumption does not hold, then the set X may contain variables that coincidentally satisfy\nthe factorization of (24) but are not causally exogenous. However, if X is a maximal set of exogenous variables, then\nwe know that it contains all of the causally exogenous variables (and possibly others).\nIn our experiments, we initialize our chosen reinforcement learning algorithm, PPO, to implement a fully-randomized\npolicy. However, as PPO is an on-policy RL algorithm, the policy gradually departs from full randomization, and we\nlose any guarantee that all states will be visited and all actions exercised. Consequently, the resulting exogenous sets X\nmay not be valid.\n4\nAlgorithms for Decomposing an MDP into Exogenous and Endogenous Components\nThis section introduces two practical algorithms for addressing the exogenous subspace discovery problem. The ﬁrst\nalgorithm, GRDS (Global Rank Descending Scheme), is based on the linear hierarchical formulation of Equation (22).\nIt initializes dexo to d and decreases dexo one dimension at a time until it can ﬁnd a Wexo matrix whose CCC is near\nzero. We refer to it as a “global” scheme, because it must solve a series of global manifold optimization problems. The\nsecond algorithm, SRAS (Stepwise Rank-Ascending Scheme), starts with dexo := 0 and constructs the Wexo matrix by\nadding one column at a time as long as it can keep CCC near zero. SRAS only needs to solve one-dimensional manifold\noptimization problems, so it has the potential to be faster.\n4.1\nGRDS: Global Rank Descending Scheme\nAlgorithm 2 gives the pseudo-code for the global rank descending scheme, GRDS. GRDS solves the inner objective\n(Equation (22)) by iterating from dexo := d down to zero. Instead of treating the CCC < ϵ condition as a constraint,\nwe put CCC into the objective and minimize it (line 6). If the optimization ﬁnds a Wexo with CCC < ϵ, we know that\nthis gives the maximum value, d∗\nexo. Hence, we can halt and return Wexo as the solution.\nOne might hope that we could use a more efﬁcient search procedure, such as binary search, to ﬁnd d∗\nexo. Unfortunately,\nbecause not all subsets of the maximal exogenous subspace are valid decompositions (Theorem 3), it is possible for an\nexogenous subset with ˆd < d∗\nexo to violate the CCC < ϵ constraint.\nThe orthonormality constraint in the minimization (line 6) forces the weight matrix Wexo to lie on a Stiefel manifold\n[Stiefel, 1935]. Hence, line 6 seeks to minimize a function on a manifold, a problem to which we can apply familiar\ntools for Euclidean spaces such as gradient descent, steepest descent and conjugate gradient. Several optimization\nalgorithms exist for optimizing on Stiefel manifolds [Jiang and Dai, 2015, Absil et al., 2007, Edelman et al., 1999].\nManifold optimization on a Stiefel manifold has previously been considered by Bach and Jordan [2003] in the context\nof Independent Component Analysis, but in their case the linearly projected data are subsequently mapped to a\nReproducing Kernel Hilbert Space (RKHS).\n16\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nAlgorithm 2 GRDS: Global Rank-Descending Scheme\n1: Inputs: A database of transitions {(si, ai, s′\ni)}N\ni=1 provided as matrices S, A, S′\n2: Output: The exogenous state projection matrix W ∗\nexo\n3: for dexo = d down to 1 do\n4:\nSet\nY (Wexo) ←[S −SWexoW ⊤\nexo, S′ −S′WexoW ⊤\nexo, A] for the diachronic setting\n5:\nor\nY (Wexo) ←[S −SWexoW ⊤\nexo, A] for the full setting\n6:\nSolve the following optimization problem\nW ∗\nexo :=\narg min\nWexo∈Rd×dexo\nCCC(S′Wexo; Y (Wexo) | SWexo)\nsubject to W ⊤\nexoWexo = Idexo\n7:\nSet\nCCC ←CCC(S′W ∗\nexo; Y (W ∗\nexo) | SW ∗\nexo)\n8:\nif CCC < ϵ then\n9:\nreturn W ∗\nexo\n10:\nend if\n11: end for\n12: return null projection 0\n4.2\nAnalysis of the Global Rank-Descending Scheme\nIn this section, we study the properties of the global rank-descending scheme. We assume that we ﬁt the exo reward\nfunction using linear regression (19), since this simpliﬁes our analysis. Directly analyzing Algorithm 2 is hard, because\nit (i) uses the CCC objective as a proxy for conditional independence, (ii) involves estimation errors due to having a\nﬁnite number of samples, and (iii) involves approximations in the optimization (both from numerical errors and from\nthe ϵ threshold). To side-step these challenges, we consider an oracle variant of our setting, where we have access to\nthe true joint distribution P(S, A, S′) and perfect algorithms for solving all optimization problems (including the exo\nreward linear regression). Access to P(S, A, S′) is equivalent to having an inﬁnite training sample collected by visiting\nall states and executing all actions so that estimation errors vanish when computing the conditional mutual information\nand the expected value of the residual error in (19).\nUnder this oracle setting, we prove that the global rank-descending scheme returns the unique exogenous subspace of\nmaximum rank. In practice, if we have a sufﬁciently representative sample of ⟨s, a, s′, r⟩tuples and the CCC captures\nconditional independence reasonably well, we can hope that our methods will still give useful results.\nAlgorithm 3 shows the oracle version of GRDS. It is identical to GRDS except that the optimization step of minimizing\nthe CCC is replaced by the following feasibility problem:\nFind Wexo ∈Rd×dexo such that:\nW ⊤\nexoWexo = Idexo\nI(S′Wexo; [S −SWexoW ⊤\nexo, A] | SWexo) = 0.\n(25)\nTheorem 8. The Oracle-GRDS algorithm returns a matrix Wexo such that\n(a) the subspace X deﬁned by Wexo and the subspace E deﬁned as the orthogonal complement of X form a valid\nexo/endo decomposition of the full form;\n(b) the subspace X has maximal dimension over all valid exo/endo decompositions; and\n(c) the subspace X is unique and contains all other exogenous subspaces ˜\nX that could form valid exo/endo decomposi-\ntions.\nProof. To prove property (a), ﬁrst note that we can deﬁne the joint distribution P(X = WexoW ⊤\nexos, E = s −\nWexoW ⊤\nexos) = P(S = s), because each column of Wexo is a unit vector and they are orthogonal. Because Wexo is a\nfeasible solution to the manifold optimization Problem 25 and the conditional mutual information is zero, we know\nfrom Theorem 7 that P(S′|S, A) factors as P(X′|X)P(E′|X, E, A, X′). Hence, it is a valid exo/endo decomposition\naccording to Theorem 1.\n17\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nAlgorithm 3 Oracle-GRDS for the Full Setting\n1: Input: Joint distribution P(S, A, S′) corresponding a fully-randomized policy controlling an admissible exogenous\nstate MDP with maximal exogenous subspace X deﬁned by W ∗\nexo\n2: Output: Matrix ˆW ∗\nexo\n3: for dexo = d down to 1 do\n4:\nSolve the following system of equations for Wexo ∈Rd,dexo:\nW ⊤\nexoWexo = Idexo\nI(S′Wexo; [S −SWexoW ⊤\nexo, A] | SWexo) = 0.\n5:\nif the above system is feasible with solution Wexo of rank dexo then\n6:\nˆW ∗\nexo := Wexo\n7:\nreturn ˆW ∗\nexo\n8:\nend if\n9: end for\n10: return null matrix 0\nProperty (b) follows from the fact that dexo is the largest value that yields a feasible solution to Problem 25.\nTo establish property (c), we need to prove three lemmas, which are the vector space versions of Theorem 2, Corollary 1,\nand Corollary 2.\nLemma 1 (Union of Exo/Endo Decompositions). Let [X1, E1] and [X2, E2] be two full exo/endo decompositions of an\nMDP M with state space S, where X1 = {W ⊤\n1 s : s ∈S} and X2 = {W ⊤\n2 s : s ∈S} and where W ⊤\n1 W1 = Id1×d1\nand W ⊤\n2 W2 = Id2×d2, 1 ≤d1, d2 ≤d. Let X = X1 + X2 be the subspace formed by the sum of subspaces X1 and X2,\nand let E be its complement. It then holds that the state decomposition S = [E, X] with E ∈E and X ∈X is a valid\nfull exo/endo decomposition of S.\nProof. We wish to follow the same reasoning as in Theorem 2. To do this, we need to compute the linear subspaces\nequivalent to X = X1 + X2 and E = E1 ∩E2. Consider the following subspaces expressed in the original d-dimensional\ncoordinate system:\nX = X1 ∩X2 = {W2W ⊤\n2 W1W ⊤\n1 s : s ∈Rd}\n(26)\nˆ\nX1 = X1 ∩E2 = {s ∈X1 : W2W ⊤\n2 s = 0}\n(27)\nˆ\nX2 = X2 ∩E1 = {s ∈X2 : W1W ⊤\n1 s = 0}\n(28)\nX = X1 + X2 = X ⊕ˆ\nX1 ⊕ˆ\nX2\n(29)\nE = X ⊥= {s ∈Rd : W1W ⊤\n1 s = 0 ∧W2W ⊤\n2 s = 0}\n(30)\nEquation (26) deﬁnes the intersection of the two subspaces X1 and X2, because W1W ⊤\n1 is a projection matrix that\nprojects s into X1, and W2W ⊤\n2 is a projection matrix that projects W1W ⊤\n1 s into X2. In Equation (27), we deﬁne ˆ\nX1 as\nthe set of points in X1 that are not in X2, because W2W ⊤\n2 maps them to zero. Hence, they are also not in the intersection\nX. This implies that X1 = X ⊕ˆ\nX1. Note that because E2 is deﬁned as the points that W2W ⊤\n2 maps to zero, ˆ\nX1 can\nalso be expressed as X1 ∩E2. Similarly, ˆ\nX2 = X2 ∩E1, and X2 = X ⊕ˆ\nX2. We can therefore express X1 + X2 as the\ndirect sum X ⊕ˆ\nX1 ⊕ˆ\nX2 (Equation (29)). Equation (30) deﬁnes E as the set of points that are simultaneously in the\northogonal complements of both X1 and X2. Either ˆ\nX1 or ˆ\nX2 may of course be the zero vector spaces, if one exogenous\nsubspace is contained in the other.\nNow we can deﬁne random variables that permit us to apply the proof of Theorem 2. Deﬁne random variables for the\ninput spaces: X1 ∈X1, E1 ∈E1, X2 ∈X2, for the (X, E) decomposition. Similarly, deﬁne random variables for the\noutput spaces: X = (X, ˆX1, ˆX2), where X ∈X, ˆX1 ∈ˆ\nX1, and ˆX2 ∈ˆ\nX2. Let S ∈S.\nBecause (X1, E1) is a valid decomposition, there can be no edges from E1 or A to any variable in X1. Similarly,\nbecause (X2, E2) is a valid decomposition, there can be no edges from E2 or A to any variable in X2. Consequently,\nthere can be no edges from E and A to any subspace in X. This demonstrates that (X, E) is a valid full exo/endo\ndecomposition of the state space.\nLemma 2 (Unique Maximal Subspace). The maximal exo vector subspace Xmax deﬁned by Wexo,max is unique.\n18\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nAlgorithm 4 Stepwise Rank Ascending Scheme: SRAS\n1: Inputs: A database of transitions {(si, ai, ri, s′\ni)}N\ni=1\n2: Output: The exogenous state projection matrix Wexo\n3: Initialize Wexo ←[ ], Wtemp ←[ ], Cx ←[ ], k ←0\n4: repeat\n5:\nN ←orthonormal basis for the null space of Cx\n6:\nSolve the following optimization problem\nˆw :=\narg min\nw∈R(d−k)×1CCC(S′[Wtemp, N ⊤w]; A | S[Wtemp, N ⊤w])\nsubject to w⊤w = 1\n7:\nwk+1 ←N ⊤ˆw\n8:\nCx ←Cx ∪{wk+1}\n9:\nCCCsim ←CCC(S′[Wtemp, wk+1]; A | S[Wtemp, wk+1])\n10:\nif CCCsim < ϵ then\n11:\nWtemp ←Wtemp ∪{wk+1}\n12:\nE ←S −SWtempW ⊤\ntemp\n13:\nCCCfull ←CCC(S′Wtemp; [E, A] | SWtemp)\n14:\nif CCCfull < ϵ then\n15:\nWexo ←Wtemp\n16:\nend if\n17:\nend if\n18:\nk ←k + 1\n19: until k = d\n20: return Wexo\nProof. By contradiction. If there were 2 distinct maximal subspaces, then Lemma 1 would allow us to combine them to\nget an even larger exogenous vector subspace. This is a contradiction.\nLemma 3. Let Wexo,max deﬁne the maximal exogenous vector subspace Xmax, and let Wexo deﬁne any other\nexogenous vector subspace X. Then X ⊑Xmax.\nProof. By contradiction. If there were an exogenous subspace X not contained within Xmax, then by Lemma 1\nwe could combine the 2 exo/endo decompositions to get an even larger exogenous subspace. This contradicts the\nassumption that Xmax is maximal.\nThese three lemmas establish property (c) and complete the proof of Theorem 8.\nThis concludes our analysis of the Oracle-GRDS for the full setting (Algorithm 3). The analysis can trivially be\nextended to the diachronic setting.\nHow well does this analysis carry over to the non-oracle GRDS algorithm? GRDS departs from the oracle version\nin three ways. First, GRDS employs the CCC in place of conditional mutual information (CMI). This may assign\nnon-zero CCC values to Wexo matrices that actually have zero CMI. This will cause GRDS to under-estimate dexo, the\ndimensionality of the exogenous state space. Second, GRDS only requires CCC to be less than a parameter ϵ. If ϵ is\nlarge, then GRDS may stop too soon and over-estimate dexo. Hence, by introducing ϵ, GRDS is able to compensate\nsomewhat for the failures of CCC. Third, the database of transitions is not inﬁnite, so the value of CCC that GRDS\ncomputes may be too high or too low. This in turn may cause dexo to be too small or too large. In our experiments, we\nwill compare the estimated dexo to our understanding of its true value.\n4.3\nStepwise Algorithm SRAS\nThe global scheme computes the entire Wexo matrix at once. In this section, we introduce an alternative stepwise\nalgorithm, the Stepwise Rank Ascending Scheme (SRAS, see Algorithm 4), which constructs the matrix Wexo\nincrementally by solving a sequence of small manifold optimization problems.\n19\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nSRAS maintains the current partial solution Wexo, a temporary matrix Wtemp that may be extended to update Wexo, a\nset of all candidate column vectors generated so far, Cx, and an orthonormal basis N for the null space of Cx. (N is a\nmatrix with a number of columns equal to the dimension of the null space of Cx.) The set of candidate column vectors\nCx contains all of the column vectors in Wexo and possibly some additional vectors that were rejected for violating the\nfull CCC constraint, as we will discuss below.\nSuppose we have already found the ﬁrst k columns of Wexo = [w1, w2, . . . , wk]. To ensure that the new column wk+1\nis orthogonal to all k previous vectors, we restrict wk+1 to lie in the space deﬁned by N by requiring it to have the form\nwk+1 = N ⊤w. This ensures that it is orthogonal to all columns of Wexo and to any additional vectors in Cx.\nIn Line 6, we compute a new candidate vector ˆw by solving a simpliﬁed CCC minimization problem on the (d −k) × 1-\ndimensional Stiefel manifold. Recall that the full objective I(X′; [E, A] | X) = 0 seeks to enforce the conditional\nindependence X′ ⊥⊥E, A | X. This requires us to know X and E, whereas at this point in the algorithm, we only know\na portion of X, and we therefore do not know E at all. We circumvent this problem by using the simpliﬁed objective\nI(X′\nk; A | X1, . . . , Xk) (approximated via the CCC). This objective ensures that A has no effect on the exogenous\nvariables X′ in the next time step, which eliminates the edge A →X′, but it does not protect against the possibility\nthat A causes a change in some chain of endogenous variables that affect X in some subsequent time step. Hence, the\nsimpliﬁed objective is a necessary but not sufﬁcient condition for X to be a valid exogenous subspace. See Appendix B\nfor a detailed discussion of this point.\nLines 7 and 8 compute the new candidate vector wk+1 by mapping ˆw into the null space deﬁned by N and then adding\nit to Cx. In Line 9, we compute CCCsim, the value of the simpliﬁed objective (which is the same as the value that\nminimized the objective in Line 6). In Line 10, we check whether this is less than ϵ. If not, we increment k and loop\nback to Line 5 and ﬁnd another ˆw vector. But if CCCsim < ϵ, then in Lines 11-13, we compute the corresponding E\nmatrix and compute CCCfull, the CCC of the full objective. In Line 14, we check whether CCCfull < ϵ. If so, then\nwe have a valid new column to add to Wexo. If not, we increment k and loop back to Line 5.\nRecall that not all subsets of the maximal exogenous subspace are themselves valid exogenous subspaces that satisfy\nthe full CCC constraint (Theorem 3). Hence, it is important that SRAS does not terminate when adding a candidate\nvector to Wexo causes the full constraint to be violated. Note, however, that every subset of the maximal exogenous\nsubspace must satisfy the simpliﬁed objective, because otherwise, the action variable is directly affecting one of the\nexogenous state variables.\nTo allow SRAS to continue making progress when the full constraint is violated, the algorithm maintains the matrix\nWtemp. This matrix contains all of the candidates that have satisﬁed the simpliﬁed objective. If a subsequent candidate\nwk+1 allows Wtemp to satisfy the full constraint, then we set Wexo to Wtemp and continue. The algorithm terminates\nwhen k = d.\nThe primary advantage of SRAS compared to GRDS is that the CCC minimization problems have dimension (d−k)×1.\nHowever, GRDS can halt as soon as it ﬁnds a Wexo that satisﬁes the full CCC objective, whereas SRAS must solve\nall d problems. We can introduce heuristics to terminate d early. For example, we can monitor the residual variance\n∥ˆX · wR −R∥2\n2 of the reward regression. This decreases monotonically as columns are added to Wexo, and when\nthose decreases become very small, we can terminate SRAS. This can make SRAS very efﬁcient when the exogenous\nsubspace has low rank dexo relative to the rank d of the full state space. In such cases, GRDS must solve d −dexo + 1\nlarge manifold optimization problems of dimension at least d × dexo, whereas SRAS must only solve dexo problems of\ndimension (d −k) × 1.\nWhat can we say about the correctness of SRAS? First, in an oracle version of SRAS (where the MDP was admissible,\nthe data were collected using a fully-randomized policy, and CMI was computed instead of CCC), the Wexo matrix\nreturned by SRAS would deﬁne a valid exo/endo decomposition. This is because it would satisfy the full CMI constraint.\nHowever, it would not necessarily deﬁne the maximal exogenous subspace, because the w vectors found using the\nsimpliﬁed objective and stored in Wtemp might not be a subset of a satisfying Wexo matrix. Of course, because the\nactual SRAS algorithm introduces the CCC approximation and only requires the CCC to be less than ϵ, we do not have\nany guarantee that the Wexo matrix returned by SRAS deﬁnes a valid exogenous subspace. We now turn to experimental\ntests of the algorithms to see whether they produce useful results despite their several approximations.\n5\nExperimental Study\nWe conducted a series of experiments to understand the behavior of our algorithms. In addition to GRDS and SRAS,\nwe deﬁned a third algorithm, Simpliﬁed-GRDS that applies the simpliﬁed objective CCC(S′WExo; A|SWexo) in Line\n6 of Algorithm 2 but then still uses the full objective in Line 7. Like SRAS, Simpliﬁed-GRDS will always return a valid\nWexo, but it may not be maximal.\n20\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nIn this section, we present experiments to address the following research questions:\nRQ1: Do our methods speed up reinforcement learning in terms of sample complexity? In terms of total CPU time?\nRQ2: Do our methods discover the correct maximal exogenous subspaces?\nRQ3: What is the best approach to reward regression? Single linear, repeated linear, or online neural network\nregression?\nRQ4: How do the algorithms behave when the MDPs are changed to have different properties: (a) rewards are\nnonlinear, (b) transition dynamics are nonlinear, (c) action space is combinatorial, and (d) states and actions\nare discrete?\nRQ5: How do the algorithms behave when the covariance condition is violated?\nRQ6: What are the risks and beneﬁts of using the simpliﬁed objective in SRAS and Simpliﬁed-GRDS?\nRQ7: How should hyperparameters be set? How many training tuples should be collected before performing\nexogenous subspace discovery? When should reward regression begin and on what schedule?\n5.1\nExperimental Details\nWe compare ﬁve methods:\n• Baseline: Reinforcement learning applied to the full reward (sum of exogenous and endogenous components)\n• GRDS: The Global Rank Descending Scheme\n• Simpliﬁed-GRDS: GRDS using the simpliﬁed objective\n• SRAS: The Stepwise Rank Ascending Scheme\n• Endo Reward Oracle: Reinforcement learning applied with the oracle endogenous reward.\nAs the reinforcement learning algorithm, we employ the PPO implementation from stable-baselines3 [Rafﬁn et al., 2021]\nin PyTorch [Paszke et al., 2019], and we model the MDPs in the OpenAI Gym framework [Brockman et al., 2016]. We\nuse the default PPO hyperparameters in stable-baselines3, which include a clip range of 0.2, a value function coefﬁcient\nof 0.5, an entropy coefﬁcient of 0, and a generalized advantage estimation (GAE) parameter of 0.95. The policy and\nvalue networks have two hidden layers of 64 tanh units each. For PPO optimization, we employ the default Adam\noptimizer [Kingma and Ba, 2015] in PyTorch with default hyperparameters β1 = 0.9, β2 = 0.999, eps = 1 × 10−5\nand a default learning rate of lrP P O = 0.0003. The batch size for updating the policy and value networks is 64\nsamples. The discount factor in the MDPs is set to γ = 0.99. The default number of steps between each policy update\nis K = 1536. The number of steps L after which we compute the exo/endo decomposition and the total number of\ntraining steps in the experiment N vary per experiment, but their default values are L = 3000 and N = 6000. We\nsummarize all hyperparameters in Table 2.\nIn each experimental run, we maintain two instances of the Gym environment. Training is carried out in the primary\ninstance. After every policy update, we copy the policy to the second instance and evaluate the performance of the\npolicy (without learning) for 1000 steps. To reduce measurement variance, these evaluations always start with the same\nrandom seed.\nTo solve the manifold optimization problems, we apply the solvers implemented in the Pymanopt package [Townsend\net al., 2016]. We use the Steepest Descent solver with line search with the default Pymanopt hyperparameters. For the\nCCC constraint, ϵ is set to 0.05. The Tikhonov regularizer inside the CCC is set to λ = 0.01. These hyperparameters\nwere determined empirically via random search [Bergstra and Bengio, 2012].\nTo implement neural network reward regression, we use a separate neural network in sklearn [Buitinck et al., 2013] with\n2 hidden layers of 50 and 25 units, respectively, and ReLU activations. We train with Adam using the default Adam\ncoefﬁcients. The learning rate is set by default to lrregr = 0.0003 and the L2 regularization to 3 × 10−5. The batch size\nis set to 256. During Phase 1, we train the net with the L collected samples until convergence (or a maximum number\nof 125 epochs). During Phase 2, we perform online learning by updating the neural net every M = 256 training steps\nwith a single pass over the last 256 samples. Linear regression is computed using the standard least squares matrix\nsolution without regularization.\nWe chose to use the default library hyperparameters for PPO, Adam optimization, and manifold optimization to enable\na fair comparison of the different methods. Furthermore, for online reinforcement learning algorithms, it is not feasible\nto perform extensive hyperparameter searches because of the cost (and risk) of interacting in the real world. Algorithms\n21\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nUsed for\nDescription\nSymbol\nDefault Value\nFixed\nPPO\nclipping parameter\n-\n0.2\nYes\nvalue function coefﬁcient\n-\n0.5\nYes\nentropy coefﬁcient\n-\n0\nYes\nGAE parameter\n-\n0.95\nYes\nPolicy & Value Nets\nnumber of layers\n-\n2\nYes\nunits per layer\n-\n64, 64\nYes\nactivation function\n-\ntanh\nYes\nPPO Optimization\nwith Adam\nAdam learning rate\nlrP P O\n0.0003\nYes\nAdam coefﬁcients\nβ1, β2, eps\n0.9, 0.99, 1e-5\nYes\nbatch size\n-\n64\nYes\nL2 regularization\n-\n0\nYes\nReinforcement\nLearning\ndiscount factor\nγ\n0.99\nYes\npolicy update steps\nK\n1536\nYes\ntotal training steps\nN\n60000\nNo\nsteps for decomposition\nL\n3000\nNo\nsteps for exo regression\nM\n256\nYes\nevaluation steps\n-\n1000\nYes\nManifold\nOptimization\nCCC threshold\nϵ\n0.05\nYes\nTikhonov regularizer\nλ\n0.01\nYes\nExo regression Net\nnumber of layers\n-\n2\nYes\nunits per layer\n-\n50, 25\nYes\nactivation function\n-\nrelu\nYes\nExo Regression\nOptimization\nwith Adam\nAdam learning rate\nlrregr\n0.0003\nNo\nAdam coefﬁcients\nβ1, β2, eps\n0.9, 0.99, 1e-8\nYes\nbatch size\n-\n256\nYes\nL2 regularization\n-\n0.00003\nYes\nTable 2: Hyperparameters for High-D setting.\nthat only perform well after extensive hyperparameter search are not usable in practice. Hence, we wanted to minimize\nhyperparameter tuning.\nIn all our MDPs, we use the default values in Table 2 for all hyperparameters except for the number of decomposition\nsteps L and the number of training steps L. The former is the most critical hyperparameter; it is discussed in detail in\nSection 5.6. The latter is set to a number that is high enough for our methods to converge or be near the limit. Finally, in\na few settings we found it beneﬁcial to use a regression learning rate of 0.0006 instead of 0.0003 for better convergence.\nFor each setting, we report the values of the hyperparameters that are different from their default values. We run all\nexperiments on a c5.4xlarge EC2 machine on AWS3.\n5.2\nPerformance Comparison on High-Dimensional Linear Dynamical MDPs\nTo address RQ1 and RQ2, we deﬁne a set of high-dimensional MDPs with linear dynamics. Each MDP, by design, has\nm endogenous and n exogenous variables, so that et ∈Rm and xt ∈Rn. There is a single action variable at that takes\n10 discrete values (−1, −0.777, −0.555, . . . , 0, . . . , 0.555, 0.777, +1). The policy chooses one of these values at each\ntime step. The exo and endo transition functions are\nxt+1 = Mexo · xt + εexo\net+1 = Mend ·\n\u0014\net\nxt\n\u0015\n+ Ma · at + εend,\nwhere Mexo ∈Rn×n is the transition function for the exogenous MRP; Mend ∈Rm×m is the transition function for\nthe endogenous MDP involving et and xt; Ma ∈Rm is the coefﬁcient for the action at, and it is set to a vector of\nones; εexo ∈Rn is the exogenous noise, whose elements are distributed according to N(0, 0.09); and εend ∈Rm is\nthe endogenous noise, whose elements are distributed according to N(0, 0.04). The observed state vector st ∈Rm+n\nis a linear mixture of the hidden exogenous and endogenous states deﬁned as\nst = M ·\n\u0014\net\nxt\n\u0015\n,\n3https://aws.amazon.com/ec2/instance-types/c5/.\n22\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nwhere M ∈R(m+n)×(m+n). The elements in Mexo, Mend, and M are generated according to N(0, 1) and then each\nrow of each matrix is normalized to sum to 0.99 for stability4. The elements of the initial endo and exo states are\nrandomly initialized from a uniform distribution over [0, 1].\nIn our ﬁrst set of experiments, the reward at time t is\nRt = Rexo,t + Rend,t,\nwhere Rexo,t = −3 · avg(xt) is the exogenous reward and Rend,t = e−|avg(et)−1| is the endogenous reward, and avg(·)\ndenotes the average over a vector’s elements. For this class of MDPs, the optimal policy seeks to drive the average of et\nto 1.\nWe experiment in total with 6 MDPs with different choices for the numbers n and m of the exogenous and endogenous\nvariables, respectively: (i) 5-D state with m = 2 endo variables and n = 3 exo variables; (ii) 10-D state with m = 5\nand n = 5; (iii) 20-D state with m = 10 and n = 10; (iv) 30-D state with m = 15 and n = 15; (iv) 45-D state with\nm = 22 and n = 23; and (vi) 50-D state with m = 25 endo variables and n = 25 exo variables. For the 50-D setting,\nwe use N = 100000 training steps and L = 10000 decomposition steps due to its higher dimensionality. Similarly,\nwe use N = 80000 and L = 5000 for the 45-D MDP, and L = 5000 for the 30-D MDP. On the other hand, we set\nN = 50000 and L = 2000 for the 5-D MDP. For each MDP, we run 20 replications with different random seeds and\nreport the average results and standard deviations.\n5.2.1\nRL Performance\nTo address RQ1, we report the performance for each of the 6 MDPs over 20 replications in Figure 6. In all 6 MDPs, all\nof our methods far-outperform the baseline. Indeed, in the 5-D and 10-D MDPs, the baseline does not show any sign of\nlearning, and in the larger problems, the baseline’s performance has attained roughly half of the performance of our\nmethods after 65 policy updates. A simple linear extrapolation of the baseline learning curve for the 50-D problem\nsuggests that it will require 132 policy updates to attain the performance of the other methods. This is more than 3\ntimes as long as the 40 updates our methods require. Hence, in terms of sample complexity, our methods are much\nmore efﬁcient than the baseline method.\nOn these MDPs, the Simpliﬁed-GRDS and SRAS methods are able to match the performance of the Endo Reward\nOracle, which is given the correct endogenous reward from the very start. GRDS performs very well on all MDPs\nexcept for the 5-D one, where it is still able to outperform the baseline.\nRQ1 also asks whether our methods are superior in terms of CPU time. Table 3 reports the CPU time required by\nthe various methods. The Baseline and Endo Reward Oracle consume identical amounts of time, so the table only\nlists the Baseline CPU time. We observe that even the slowest of our methods (SRAS on the 50-D problem) requires\nonly 45% more time than the Baseline. However, if we again extrapolate the baseline to 132 policy updates, where its\nperformance would match our methods, that would require 2997 seconds of CPU time, which is 49% more than SRAS.\nHence, even if there is zero cost to collecting training samples in the real world, our methods are still faster.\n5.2.2\nRank of Discovered Exo Subspace\nRQ2 asks whether our methods ﬁnd the correct maximal exogenous subspaces. Our experiments revealed a surprise.\nAlthough we constructed the MDPs with the goal of creating an n-dimensional exogenous space and an m-dimensional\nendogeous space, our methods usually discover exogenous spaces with n + m −1 dimensions. Upon further analysis,\nwe realized that because the action variable is 1-dimensional, it can only affect a 1-dimensional subspace of the\nn + m-dimensional state space. Consequently, the true maximal exogenous subspace has dimension n + m −1. The\nresults in Table 3 show that the Simpliﬁed-GRDS always ﬁnds an exogenous subspace of the correct dimension. The\nexogenous space computed by SRAS is sometimes slightly smaller on the smaller MDPs, and the space computed by\nGRDS is sometimes slightly smaller on the larger MDPs. We believe the failures of SRAS are due to the approximations\nthat we discussed in Section 4.3. We suspect the failures of GRDS reﬂect failures of the manifold optimization to ﬁnd\nthe optimum in high-dimensional problems.\nThe fact that the exogenous space has dimension n + m −1 explains the relative amount of CPU time consumed by the\ndifferent algorithms. SRAS is often the slowest, because it must solve n + m optimization problems whereas GRDS\nand Simpliﬁed-GRDS must only solve two (large) manifold optimization problems before terminating.\n4Notice that all matrices Mexo, Mend, and M in our synthetic linear MDPs are stochastic. Future work could explore more\ngeneral classes of MDPs.\n23\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n5\n10\n15\n20\n25\n30\n# of Policy Updates\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 50000\nDecomposition steps = 2000\nPolicy update steps = 1536\nExo variables = 3\nEndo variables = 2\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(a) 5-D MDP (m = 2, n = 3).\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(b) 10-D MDP (m = 5, n = 5).\n0\n10\n20\n30\n40\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 10\nEndo variables = 10\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(c) 20-D MDP (m = 10, n = 10).\n0\n10\n20\n30\n40\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 5000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(d) 30-D MDP (m = 15, n = 15).\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 8000\nPolicy update steps = 1536\nExo variables = 23\nEndo variables = 22\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(e) 45-D MDP (m = 22, n = 23).\n0\n10\n20\n30\n40\n50\n60\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 100000\nDecomposition steps = 10000\nPolicy update steps = 1536\nExo variables = 25\nEndo variables = 25\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(f) 50-D MDP (m = 25, n = 25).\nFigure 6: Comparison of various methods in high-D linear MDPs.\n5.2.3\nComparison of Methods for Exo Reward Regression\nWe performed a second set of experiments to investigate how the type and conﬁguration of exo reward regression affects\nthe performance of our methods. We compare three reward regression conﬁgurations. The ﬁrst conﬁguration is Single\nLinear Regression, which ﬁts a linear model for the exo reward and performs regression only once at the end of Phase\n1 of Algorithm 1. The second conﬁguration is Repeated Linear Regression. Like Single Linear Regression, it ﬁts a\nlinear model at the end of Phase 1. In addition, it re-ﬁts the model in Phase 2 every 1,000 collected samples using all\ntransition data so far. Note that this is different from online Algorithm 1, which updates the exogenous reward function\nin Phase 2 every M observations using only the last M observations in Dexo. The goal was to understand whether\nregular regression with all observed transition data can perform better than a single linear regression at the end of Phase\n1. The third conﬁguration is Online Neural Net Regression which ﬁts a neural network to the exo reward data. At the\n24\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nTotal State\nExo State\nEndo State\nMethod\nExo Subspace\nTotal Time\nDecomposition\nVariables\nVariables\nVariables\nRank\n(secs)\nTime (secs)\n5\n3\n2\nBaseline\n-\n245.9±14.7\n-\nGRDS\n4.0±4.0\n294.9±113.0\n6.3±4.6\nSimpliﬁed-GRDS\n4.0±0.0\n297.0±115.4\n9.6±5.0\nSRAS\n3.95±0.22\n297.1±113.3\n14.0±14.0\n10\n5\n5\nBaseline\n-\n345.3±10.48\n-\nGRDS\n8.65±0.48\n413.5±148.0\n22.6±18.0\nSimpliﬁed-GRDS\n9.0±0.0\n413.7±144.4\n13.1±9.1\nSRAS\n8.75±0.54\n434.3±157.6\n34.2±46.7\n20\n10\n10\nBaseline\n-\n450.2±34.8\n-\nGRDS\n18.1±1.18\n525.3±167.9\n41.8±47.8\nSimpliﬁed-GRDS\n19.0±0.0\n513.6±141.3\n8.6±4.4\nSRAS\n18.4±0.66\n562.4±188.5\n86.0±72.3\n30\n15\n15\nBaseline\n-\n509.9±43.0\n-\nGRDS\n28.25±1.22\n597.5±185.6\n56.9±84.1\nSimpliﬁed-GRDS\n29.0±0.0\n584.3±136.9\n12.5±6.0\nSRAS\n27.9±1.58\n688.9±276.5\n177.5±234.9\n45\n23\n22\nBaseline\n-\n895.4±159.3\n-\nGRDS\n43.4±0.86\n1041.7±287.6\n84.9±154.2\nSimpliﬁed-GRDS\n44.0±0.0\n1006.3±207.8\n13.3±8.4\nSRAS\n44.0±0.0\n1323.4±716.1\n605.1±538.7\n50\n25\n25\nBaseline\n-\n1472.4±126.0\n-\nGRDS\n48.45±0.86\n1659.6±282.7\n81.7±105.8\nSimpliﬁed-GRDS\n49.0±0.0\n1634.5±239.1\n15.7±9.2\nSRAS\n49.0±0.0\n2009.2±840.3\n667.8±616.9\nTable 3: Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition\ntime for the linear MDPs.\nend of Phase 1, we perform reward regression until convergence. During Phase 2, we then perform a single epoch every\n256 steps.\nWe plot RL performance over the 20 replications in Figure 6 on two MDPs: (i) the 10-D state MDP (Figures 7a-7b)\nand (ii) the 50-D state MDP (Figures 7c-7d). We compare the three regression methods applied to GRDS and SRAS.\nThe results show that Online Neural Net Regression generally outperforms Single and Repeated Linear Regression,\neven though the exo reward function Rexo,t is a linear function of the exo state. We speculate that this is because it is\ncontinually incorporating new data, which in turn may allow PPO to make more progress. Repeated Linear Regression\nalso incorporates new data, but at a slower rate. Furthermore, when applied to SRAS, it becomes unstable and exhibits\nhigh variance. We speculate that this may be because it is only ﬁtting to the most recent 1000 data points. Future work\nmight consider training on all accumulated data and imposing strong regularization to improve stability.\nBased on the superior performance of online neural network regression, we adopt it as the default reward regression\nmethod in the remainder of our experiments.\n5.3\nExploring Modiﬁcations of the MDPs\nTo address RQ4, we now study the performance of our methods when they are applied to MDPs that depart in various\nways from the linear dynamical MDPs studied thus far:\n(a) Rewards are nonlinear functions of the state,\n(b) Transition dynamics are nonlinear,\n(c) The action space is combinatorial, and\n(d) The states and actions are discrete.\n5.3.1\nNonlinear Exogenous Reward Functions\nBecause we have adopted online neural network reward regression, we expect that our methods should be able to\nﬁt nonlinear exogenous reward functions. We consider the high-D linear setting of Section 5.2 with m = n = 15.\nWe perform exo/endo decomposition after L = 5000 steps and train for a total of N = 80000 steps. We found it\n25\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 20\nAlgorithm\nBaseline\nSingle Linear Regression\nRepeated Linear Regression\nOnline Neural Net Regression\nEndo Reward Oracle\n(a) GRDS (m = 5, n = 5).\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 20\nAlgorithm\nBaseline PPO\nSingle Linear Regression\nRepeated Linear Regression\nOnline Neural Net Regression\nEndo Reward Oracle\n(b) SRAS (m = 5, n = 5).\n0\n10\n20\n30\n40\n50\n60\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 100000\nDecomposition steps = 10000\nPolicy update steps = 1536\nExo variables = 25\nEndo variables = 25\nReplications = 20\nAlgorithm\nBaseline\nSingle Linear Regression\nRepeated Linear Regression\nOnline Neural Net Regression\nEndo Reward Oracle\n(c) GRDS (m = 25, n = 25).\n0\n10\n20\n30\n40\n50\n60\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 100000\nDecomposition steps = 10000\nPolicy update steps = 1536\nExo variables = 25\nEndo variables = 25\nReplications = 20\nAlgorithm\nBaseline PPO\nSingle Linear Regression\nRepeated Linear Regression\nOnline Neural Net Regression\nEndo Reward Oracle\n(d) SRAS (m = 25, n = 25).\nFigure 7: Impact of the type of exo reward regression on RL performance.\nbeneﬁcial to use a learning rate for the exogenous reward regression of 0.0006 instead of the default 0.0003; the higher\nlearning rate can help the exogenous reward neural net to adapt faster. We perform 20 replications with different seeds.\nFurthermore, we replace the linear exogenous reward function Rexo,t by the following four choices:\n• R1\nexo,t = clip(6 · (avg(xt) + 1\n3 · avg(x2\nt) −2\n15 · avg(x3\nt)), −5.0, 5.0), a 3rd degree polynomial.\n• R2\nexo,t = −3 · e−|avg(xt)|1.5, a function of avg(xt) with a single mode.\n• R3\nexo,t = −3 · (e−|avg(xt+1.5)|2 −e−|avg(xt−1.5)|2), a function of avg(xt) with two modes.\n• R4\nexo,t = −3 · (e−|avg(xt+1)|2 + 3\n2 · e−|avg(xt−1.5)|2 −5\n3e−|avg(xt)|2), a function of avg(xt) with three modes.\nFigures 8a-8d plot the results.\nWe generally observe that all methods match the Endo Reward Oracle’s performance and outperform the baseline by a\nlarge margin. This conﬁrms that the nonlinear reward regression is able to ﬁt these nonlinear reward functions. As we\nhave observed before, the RL performance of SRAS is a bit unstable, perhaps because it is not always able to detect the\nmaximal exogenous subspace. The Simpliﬁed-GRDS method also shows a tiny bit of instability.\nTable 4 reports the average rank of the discovered exogenous subspaces. The true exogenous space has rank 29, but the\nexogenous reward only depends on 15 of those dimensions. The Simpliﬁed-GRDS method is most consistently able to\nﬁnd the true rank, whereas SRAS and GRDS struggle to capture that last dimension.\n5.3.2\nNonlinear State Transition Dynamics\nSo far, we have considered MDPs with linear state transitions for the endogenous and exogenous states. It is a natural\nquestion whether our algorithms can handle more general MDPs. In this section, we provide experimental results on a\nmore general class of nonlinear MDPs. Even though we lack a rigorous theoretical understanding, our results hint at the\npotential of the CCC objective to discover useful exo/endo state decompositions even when the dynamics are nonlinear.\n26\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 5000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nRegression net learning rate = 0.0006\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(a) R1\nexo,t.\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 5000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nRegression net learning rate = 0.0006\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(b) R2\nexo,t.\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 5000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nRegression net learning rate = 0.0006\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(c) R3\nexo,t.\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 5000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nRegression net learning rate = 0.0006\nReplications = 20\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(d) R4\nexo,t.\nFigure 8: RL performance for MDPs with nonlinear exo reward functions.\nExo Reward\nMethod\nExo Subspace\nTotal Time\nDecomposition\nFunction\nRank\n(secs)\nTime (secs)\nR1\nexo,t\nBaseline\n-\n1520.0±153.0\n-\nGRDS\n28.05±1.02\n1874.6±357.3\n155.8±122.1\nSimpliﬁed-GRDS\n29.0±0.0\n1808.6±278.7\n40.5±11.3\nSRAS\n27.95±1.56\n1914.4±510.3\n251.8±249.5\nR2\nexo,t\nBaseline\n-\n1510.9±136.2\n-\nGRDS\n28.3±0.9\n1860.4±359.9\n150.3±155.1\nSimpliﬁed-GRDS\n29.0±0.0\n1797.8±264.4\n31.9±7.7\nSRAS\n27.95±1.56\n1925.4±516.3\n297.7±244.1\nR3\nexo,t\nBaseline\n-\n1518.1±146.4\n-\nGRDS\n28.0±1.0\n1879.6±340.0\n150.3±118.1\nSimpliﬁed-GRDS\n29.0±0.0\n1818.0±262.2\n28.6±8.5\nSRAS\n27.95±1.56\n1932.4±505.4\n272.9±265.5\nR4\nexo,t\nBaseline\n-\n1513.8±103.7\n-\nGRDS\n28.35±0.79\n1849.8±366.1\n123.2±111.5\nSimpliﬁed-GRDS\n28.35±0.79\n1812.0±268.9\n37.1±11.3\nSRAS\n29.0±0.0\n1926.7±560.5\n307.4±320.7\nTable 4: Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition\ntime for the MDPs with nonlinear exo rewards.\nIn the experiments in this section, we introduce nonlinear dynamics, but we still conﬁgure the exogenous and endogenous\nstate spaces so that they are linear projections of the full state space. We study the following three MDPs, which are\ndeﬁned according to the recipe in Section 5.2 with the following modiﬁcations:\n27\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(a) M1.\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(b) M2.\n0\n10\n20\n30\n40\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(c) M3.\n0\n10\n20\n30\n40\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 60000\nDecomposition steps = 3000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(d) M3 (ϵ = 0.1).\nFigure 9: RL performance for MDPs with nonlinear state transitions.\n• M1 is a 10-D MDP with m = n = 5 and a single action variable. The exo and endo state transitions are\nxt+1 = clip(Mexo · xt + 1\n3 · Nexo · x2\nt −2\n15 · Kexo · x3\nt, −4, 4) + εexo\net+1 = Mend ·\n\u0014\net\nxt\n\u0015\n+ Ma · at + εend,\nwhere Mexo, me, Ma and εexo, εend are deﬁned as in Section 5.2. Furthermore, the two matrices Nexo ∈Rn,n\nand Kexo ∈Rn,n are generated following the same procedure as Mexo. M1 has nonlinear exogenous\ndynamics but linear endogenous dynamics.\n• M2 is exactly the same as M1 except that the endogenous transition function now has a nonlinear dependence\non the action:\net+1 = Mend ·\n\u0014\net\nxt\n\u0015\n+ Ma · at + Na · a2\nt + εend.\nThe entries in Na ∈Rm are sampled from the uniform distribution over [0.5, 1.5).\n• M3 is a 10-D MDP with m = n = 5 and a single action variable. The exogenous and endogenous state\ntransitions functions are\nxt+1 = clip(5 · sign(xt) ·\np\n|xt| −sin(xt), −2, 2) + εexo\net+1 = Mend ·\n\u0014\net\nxt\n\u0015\n+ sin(3 · at) + εend.\nThe entries in the noise vectors εexo and εend are sampled from N(0, 0.16) and N(0, 0.09), respectively. Like\nM2, M3’s exo and endo transition functions are both nonlinear.\nFigures 9a-9c plot the RL performance over 15 replications with different seeds and Table 5 reports the ranks of the\ndiscovered exogenous subspaces. Consider ﬁrst MDPs M1 and M2. The Simpliﬁed-GRDS and SRAS algorithms\n28\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nMDP\nExo/Endo State\nMethod\nExo Subspace\nTotal Time\nDecomposition\nVariables\nRank\n(secs)\nTime (secs)\nM1\n5/5\nBaseline\n-\n665.3±39.9\n-\nGRDS\n9.0±0.0\n890.4±140.6\n55.0±34.0\nSimpliﬁed-GRDS\n9.0±0.0\n906.7±154.6\n72.2±19.8\nSRAS\n9.0±0.0\n951.2±253.2\n172.8±73.2\nM2\n5/5\nBaseline\n-\n717.0±30.3\n-\nGRDS\n9.0±0.0\n959.5±137.1\n61.9±41.2\nSimpliﬁed-GRDS\n9.0±0.0\n981.2±138.0\n85.0±18.1\nSRAS\n9.0±0.0\n1019.8±271.9\n187.4±72.6\nM3\n(ϵ = 0.05)\n5/5\nBaseline\n-\n768.2±50.1\n-\nGRDS\n7.67±1.44\n1033.0±241.2\n125.8±126.7\nSimpliﬁed-GRDS\n3.60±3.28\n988.1±160.4\n38.7±19.0\nSRAS\n3.47±3.36\n1081.3±370.7\n168.8±70.7\nM3\n(ϵ = 0.1)\n5/5\nBaseline\n-\n768.2±50.1\n-\nGRDS\n7.67±1.44\n1033.0±241.2\n125.8±126.7\nSimpliﬁed-GRDS\n9.0±0.0\n918.3±129.7\n17.7±4.6\nSRAS\n9.0±0.0\n1023.3±341.5\n230.4±69.7\nTable 5: Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition\ntime for the MDPs with nonlinear state transitions.\nperform very well. They converge quickly, and they are able to match the Endo Reward Oracle’s performance. In\nboth settings, the Baseline converges to a suboptimal value and suffers from high variance. Most shocklingly, GRDS\nperforms catastrophically and exhibits very high variance even though it discovers an exogenous subspace of the correct\nrank.\nNow consider M3 in Figure 9c. On this problem, Simpliﬁed-GRDS and SRAS perform poorly (although still better\nthan the baseline), while GRDS performs much better. However, none of the methods is able to match the Endo Reward\nOracle. Table 5 reveals that all of the methods, but particularly Simpliﬁed-GRDS and SRAS, are failing to discover\nthe correct exogenous subspace. This suggests that the CCC computation when applied to the simpliﬁed objective is\nnot ﬁnding good solutions. To evaluate this possibility, we reran the M3 experiment with a larger value of ϵ = 0.1\ninstead of its default value of 0.05. With this change, the results improve dramatically for Simpliﬁed-GRDS and SRAS,\nand they are able to match the performance of the Endo Reward Oracle. However, the performance of GRDS does\nnot improve, which suggests that it is not able to ﬁnd good solutions to the large manifold optimization problems that\nit is solving. This could be because the CCC objective is confused by the nonlinear dynamics or it could be that the\noptimization is trapped in local minima.\n5.3.3\nCombinatorial Action Spaces\nA limitation of our experiments so far has been that the action space is one-dimensional. We have seen that this implies\nthat the maximal exogenous subspace has dimension n + m −1 rather than n as originally intended. In this section, we\ndescribe experiments where we introduce higher-dimensional action spaces. For example, with a 5-dimensional action\nspace, the policy must now select one of 10 values for each of the 5 action variables. In effect, the MDP now has 105\nprimitive actions.\nTo design MDPs with high-dimensional action spaces, we modify the general linear setting of Section 5.2, so that the\naction at ∈Rl is an l-D vector, and the matrix Ma multiplying at is in Rm,l. As in the single-action setting, each action\nvariable takes 10 possible values evenly-spaced in [−1, 1]. We consider 6 MDPs with a variety of different structures,\nwhich may in principle appear in real applications:\n• 10-D MDP with m = n = 5 and l = 5. The action matrix Ma is dense, meaning that all its entries are nonzero.\nWe sample the entries in Ma from the uniform distribution over [0, 1) and subsequently normalize each row of\nMa to sum to 0.99 for stability. We apply the decomposition algorithms after L = 6000 steps and train for a\ntotal of N = 200000 steps.\n• 20-D MDP with m = n = 10 and l = 10. The action matrix Ma is dense, and generated as above. We set\nL = 10000 and N = 200000.\n• 30-D MDP with m = n = 15 and l = 8. The action matrix Ma is partial dense, meaning that only l = 8\nout of the m = 15 endogenous states are controlled, but these 8 states are controlled by all actions. Ma\nis generated as above, except that the rows corresponding to non-controlled endo variables are 0. We set\nL = 15000 and N = 200000.\n29\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nAction\nExo/Endo\nMethod\nExo Subspace\nTotal Time\nDecomposition\nVariables\nState Variables\nRank\n(secs)\nTime (secs)\n5 (dense)\n5/5\nBaseline\n-\n3576.8±293.2\n-\nGRDS\n7.47±0.50\n4835.2±960.6\n69.8±39.1\nSimpliﬁed-GRDS\n7.47±0.50\n4864.5±979.3\n148.1±70.2\nSRAS\n7.27±0.44\n4914.6±1126.0\n402.8±134.0\n10 (dense)\n10/10\nBaseline\n-\n6162.7±478.0\n-\nGRDS\n15.6±0.49\n7745.5±1299.4\n396.5±228.8\nSimpliﬁed-GRDS\n16.0±0.0\n7731.9±1292.3\n387.0±145.0\nSRAS\n16.0±0.0\n8064.7±2006.7\n1546.7±483.1\n8 (partial\ndense)\n15/15\nBaseline\n-\n8734.0±878.1\n-\nGRDS\n27.0±0.0\n10400.0±1599.6\n397.0±244.3\nSimpliﬁed-GRDS\n27.0±0.0\n10150.5±1452.2\n556.7±121.8\nSRAS\n26.93±0.25\n11448.9±4052.9\n3702.0±1730.1\n8 (partial\ndisjoint)\n15/15\nBaseline\n-\n8388.9±706.1\n-\nGRDS\n22.0±0.0\n10399.5±2200.5\n1136.4±433.9\nSimpliﬁed-GRDS\n22.0±0.0\n10362.2±1687.5\n1520.7±304.4\nSRAS\n22.0±0.0\n10389.7±1877.7\n1224.7±506.5\n10 (partial\ndisjoint\nsparse)\n15/20\nBaseline\n-\n12088.9±973.6\n-\nGRDS\n23.2±0.4\n16126.3±1096.9\n2799.9±473.5\nSimpliﬁed-GRDS\n5.47±1.89\n18627.7±3103.6\n7019.8±1427.1\nSRAS\n6.2±1.51\n13984.1±6876.4\n1976.3±1504.9\n20 (partial\ndisjoint\nsparse)\n30/40\nBaseline\n-\n35134.1±2877.3\n-\nGRDS\n50.0±0.0\n43345.1±3470.3\n10256.2±2719.9\nSimpliﬁed-GRDS\n50.0±0.0\n44187.4±3672.7\n10670.1±3015.7\nSRAS\n49.0±0.0\n44350.5±3540.4\n11691.1±2925.2\nTable 6: Mean and standard deviation of the rank of the discovered exo subspace, total execution time, and decomposition\ntime for MDPs with multiple action variables.\n• 30-D MDP with m = n = 15 and l = 8. The action matrix Ma is partial disjoint, meaning that only l = 8 out\nof the m = 15 endo states are controlled but each of these 8 states is controlled by a distinct action variable.\nWe sample the l = 8 nonzero entries of Ma from the uniform distribution over [0.5, 1.5). We set L = 15000\nand N = 200000.\n• 35-D MDP with m = 20, n = 15 and l = 10. The action matrix Ma is partial disjoint, i.e., only 10 endogenous\nstate variables are directly controlled through the 10 actions (each by a distinct action variable) whereas the\nremaining ones are controlled indirectly through the other endogenous states; furthermore, the endo transition\nmatrix Me is sparse with sparsity (fraction of nonzeros) 14.3%. Speciﬁcally, Me is generated as in Section 5.2\nexcept that only a small part of the matrix (equal to 14.3%) is initialized to nonzero values. We set L = 20000\nand N = 200000.\n• 70-D MDP with m = 40, n = 30 and l = 20. The action matrix Ma is partial disjoint,i.e., only 20 endogenous\nstates are directly controlled through the 20 actions whereas the remaining ones are controlled indirectly\nthrough the other endogenous states. The endo transition matrix Me is sparse with sparsity 14.3%. We set\nL = 35000 and N = 300000.\nFigures 10a-10f plot RL performance for these 6 MDPs. We run 15 replications with different seeds and a reward\nregression learning rate of 0.0006. A ﬁrst observation is that in all 6 settings the baseline struggles and shows very slow\nimprovement over time time (e.g., Figure 10b). Its performance appears to decay on the smallest of these MDPs (Figure\n10a). The Endo Reward Oracle performs visibly better than the Baseline and is able to attain higher rewards. However,\nit exhibits high variance and it improves very slowly (e.g., Figure 10b).\nSurprisingly, the Simpliﬁed-GRDS and SRAS methods substantially outperform the Endo Reward Oracle. It appears\nthat our methods are able to discover additional exogenous dimensions that reduce the variance of the endogenous\nreward below the level of the reward oracle. Table 6 conﬁrms that, with the exception of the 35-dimensional sparse,\npartial disjoint MDP, the algorithms are all discovering exogenous spaces of the expected size. For example, in the\nlargest MDP, which has 70 dimensions and a 20-dimensional action space, GRDS and Simpliﬁed-GRDS both discover\na 50-dimensional exogenous space. The algorithms are challenged by the fourth MDP (n=15, m = 20, l = 10). On this\nMDP, GRDS ﬁnds a 23.2-dimensional exo space on average, but Simpliﬁed-GRDS and SRAS only ﬁnd exo spaces\nwith an average dimension of 5.47 and 6.2, respectively. They also exhibit high variation in the number of discovered\ndimensions. Despite these failures, Simpliﬁed-GRDS and SRAS perform very well on all six of these MDPs. GRDS\nstruggles on the dense MDPs, but does quite well on the sparse and partial disjoint MDPs.\n30\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n20\n40\n60\n80\n100\n120\n# of Policy Updates\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 200000\nDecomposition steps = 6000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nAction variables = 5\nRegression net learning rate = 0.0006\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(a) Dense 10-D MDP.\n0\n20\n40\n60\n80\n100\n120\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 200000\nDecomposition steps = 10000\nPolicy update steps = 1536\nExo variables = 10\nEndo variables = 10\nAction variables = 10\nRegression net learning rate = 0.0006\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(b) Dense 20-D MDP.\n0\n20\n40\n60\n80\n100\n120\n# of Policy Updates\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nTrue Endo Reward\nTotal training steps = 200000\nDecomposition steps = 15000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nAction variables = 8\nRegression net learning rate = 0.0006\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(c) Partial dense 30-D MDP.\n0\n20\n40\n60\n80\n100\n120\n# of Policy Updates\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 200000\nDecomposition steps = 15000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 15\nAction variables = 8\nRegression net learning rate = 0.0006\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(d) Partial disjoint 30-D MDP.\n0\n20\n40\n60\n80\n100\n120\n# of Policy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 200000\nDecomposition steps = 20000\nPolicy update steps = 1536\nExo variables = 15\nEndo variables = 20\nAction variables = 10\nRegression net learning rate = 0.0006\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(e) Partial disjoint sparse 35-D MDP.\n0\n50\n100\n150\n200\n# of Policy Updates\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrue Endo Reward\nTotal training steps = 300000\nDecomposition steps = 35000\nPolicy update steps = 1536\nExo variables = 30\nEndo variables = 40\nAction variables = 20\nRegression net learning rate = 0.0006\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(f) Partial disjoint sparse 70-D MDP.\nFigure 10: RL performance for MDPs with multiple action variables.\n5.3.4\nDiscrete MDPs\nThe ﬁnal variation in MDP structure that we studied was to create an MDP with discrete states. Figure 11 shows a\nsimple routing problem deﬁned on a road network. There are 9 endogenous states corresponding to the nodes of the\nnetwork. This MDP is episodic; each episode starts in the starting node, v0, and ends in the terminal node v8. Each\nedge in the network has a corresponding traversal cost, and the goal of the agent is to reach the terminal node while\nminimizing the total cost. There are 4 exogenous state variables; each of them is independent of the others and evolves\nas xt+1,i = 0.9 · xt,i + εexo, where εexo is distributed according to N(0, 1) and i ∈{1, 2, 3, 4}. These exogenous state\nvariables are intended to model global phenomena such as amount of automobile trafﬁc, fog, snow, and pedestrian\ntrafﬁc. These quantities evolve independently of the navigation decisions of the agent, but they modify the cost of\ntraversing the edges.\n31\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nThe reward function is the sum of two terms:\nrt = −cost(st →st+1) −\n4\nX\ni=1\nxt,i.\nThe ﬁrst term is the endogenous reward Rend,t and the second term is the exogenous reward Rexo,t.\nThe actions at each node consist in choosing one of the outbound edges to traverse. We restrict the set of actions to\nmove only rightward (i.e., toward states with higher subscripts). For instance, there are three available actions at node\nv0 corresponding to the three outgoing edges, but only a single action at node v4. The cost of traversing an edge is\nshown by the edge weights in Figure 11. The MDP is deterministic: a given action (i.e., edge selection) at a given node\nalways results in the same transition. The observed state consists of the 1-hot encoding for the 9 endo states plus the\n4-D continuous exo state variables.\nWe apply episodic RL with PPO. Since the MDP is small, we modify some of the hyperparameters as follows. We\nset the total training steps to N = 10000 and the policy update steps to K = 128. We perform decomposition after\nL = 300 episodes instead of the default value of 3,000 steps. The PPO batch size is set to 32. Every time we update the\npolicy, we execute 300 evaluation episodes (in a separate instance of the MDP environment). For reward regression,\nwe compute a Single Linear Regression rather than Online Neural Net Regression (see Section 5.2.3). We perform 15\nreplications, each with a different random seed.\nFigure 12a plots RL performance. Note that our methods perform on par with the Endo Reward Oracle and exhibit\nvery little variance, with the exception of SRAS. On the other hand, the Baseline makes very slow progress. Table\n7 reports the rank of the discovered exogenous subspace as well as the total time and decomposition time. GRDS\nand Simpliﬁed-GRDS discover large exogenous subspaces of rank 11 and 10, respectively. The computed exogenous\nsubspace includes the 4 exogenous state variables we deﬁned as well as linear combinations of the endogenous states\nthat do not depend or have only weak dependence on the action. In contrast, SRAS discovers an exo subspace of very\nlow rank, which explains its suboptimal performance.\nThe strong performance of our methods might be due to deterministic dynamics of the problem. To test this, we created\na stochastic version of the road network MDP. The transition probabilities are speciﬁed as follows:\n• Taking action 0 / 1 / 2 at node v0 leads to nodes v1, v2, v4 with probabilities (0.5, 0.3, 0.2) /(0.3, 0.5, 0.2) /\n(0.3, 0.2, 0.5), respectively.\n• Taking action 0 / 1 at node v1 leads to nodes v4, v5 with probabilities (0.6, 0.4) / (0.5, 0.5), respectively.\n• Taking action 0 / 1 at node v2 leads to nodes v3, v4 with probabilities (0.5, 0.5) / (0.3, 0.7), respectively.\n• Taking action 0 / 1 at node v3 leads to nodes v6, v7 with probabilities (0.7, 0.3) / (0.4, 0.6), respectively.\n• Taking action 0 / 1 / 2 at node v4 leads to nodes v5, v6, v8 with probabilities (0.6, 0.2, 0.2) /(0, 1, 0) /\n(0.3, 0.2, 0.5), respectively.\n• There is only one action (action 0) available at nodes v5, v6, v7, and this leads with probability 1 to nodes\nv8, v7, v8, respectively.\n• Node v8 remains a terminal node.\nWe employ the same hyperparameters as in the deterministic setting, except that we set the total training steps\nN := 20000 and the number of decomposition episodes L := 600.\nFigure 12b plots the RL performance over 15 trials (each with a separate random seed). We observe that Simpliﬁed-\nGRDS and GRDS perform only slightly worse than the Endo Reward Oracle, while SRAS exhibits slightly lower average\nperformance and higher variance. The Baseline improves very slowly and lags far behind the other methods. Due to the\nstochastic transitions, all methods (and particularly the Baseline) exhibit larger variance than in the deterministic MDP\nin Figure 12a.\n𝑣0\n𝑣1\n𝑣2\n𝑣3\n𝑣4\n𝑣5\n𝑣6\n𝑣7\n𝑣8\n2\n5\n3\n2\n2\n3\n2\n3\n3\n8\n2\n4\n4\n4\n2\nFigure 11: Graph for discrete MDP.\n32\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n20\n40\n60\n80\n# of Policy Updates\n11.5\n11.0\n10.5\n10.0\n9.5\n9.0\nTrue Endo Reward\nTotal training steps = 10000\nDecomposition episodes = 300\nPolicy update steps = 128\nExo variables = 4\nEndo variables = 9\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(a) Deterministic MDP.\n0\n25\n50\n75\n100\n125\n150\n# of Policy Updates\n11.5\n11.4\n11.3\n11.2\n11.1\n11.0\nTrue Endo Reward\nTotal training steps = 20000\nDecomposition episodes = 600\nPolicy update steps = 128\nExo variables = 4\nEndo variables = 9\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(b) Non-deterministic MDP.\nFigure 12: RL performance for discrete MDPs.\nMDP\nExo/Endo State\nMethod\nExo Subspace\nTotal Time\nDecomposition\nVariables\nRank\n(secs)\nTime (secs)\nDeterministic\n4/9\nBaseline\n-\n225.6±16.7\n-\nGRDS\n9.47±0.50\n271.4±55.8\n68.4±26.9\nSimpliﬁed-GRDS\n11.0±0.0\n229.3±18.0\n10.2±1.9\nSRAS\n2.13±1.63\n272.9±39.2\n72.3±13.5\nNon-deterministic\n4/9\nBaseline\n-\n419.9±10.8\n-\nGRDS\n9.66±0.70\n503.6±102.2\n109.1±42.7\nSimpliﬁed-GRDS\n11.0±0.0\n446.6±21.5\n15.6±2.1\nSRAS\n2.8±1.38\n488.0±56.0\n82.7±9.7\nTable 7: Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition\ntime for the discrete MDPs.\nRegarding the rank of the computed subspace, we notice from Table 7 that the ranks of the discovered exo subspaces\nare the same as for the deterministic MDP. This demonstrates the robustness of the algorithms. Simpliﬁed-GRDS ﬁnds\nthe largest exogenous space with GRDS close behind. SRAS ﬁnds much smaller spaces, which partially explains its\nslightly poorer performance. It is important to remember that not all dimensions of the discovered exogenous subspaces\nmay be relevant to reward regression. SRAS may only be discovering 2.8 dimensions, on average, but these are enough\nto give it a huge performance advantage over the Baseline.\nThe total CPU cost and decomposition cost are higher than the deterministic setting, which reﬂects the added cost of\nrunning online reward regression for twice as many steps.\nIn this problem, the rank-descending methods worked better than SRAS, but even SRAS gave excellent results, and the\ntotal CPU time required is not signiﬁcantly higher than the Baseline.\n5.3.5\nRQ4 Summary\nThe experiments demonstrate that our methods are able to handle nonlinear rewards, combinatorial action spaces, and\ndiscrete state spaces without modiﬁcation. When we introduced nonlinear transition dynamics, however, all of our\nmethods performed poorly on at least one of the three MDPs tested. Excellent performance for Simpliﬁed-GRDS and\nSRAS was restored by increasing the value of ϵ, but this did not improve GRDS very much. This is additional evidence\nthat the high dimensional manifold optimization problems that GRDS solves are difﬁcult, particularly when the CCC is\ncomputed in a nonlinear setting.\n5.4\nAnti-correlated Exo and Endo Rewards\nRecall in Section 2.3 we showed that if the endogenous and exogenous rewards are negatively correlated, the variance\nof the endogenous reward can be less than the variance of the full reward and there is no variance reduction beneﬁt\nfrom computing the exo/endo reward decomposition. Speciﬁcally, Theorem 6 introduced the covariance condition\nthat the variance of the exogenous reward must be greater than twice the negative covariance between the exogenous\n33\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n5\n10\n15\n20\n# of Policy Updates\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nTrue Endo Reward\nTotal training steps = 30000\nDecomposition steps = 1000\nPolicy update steps = 1536\nExo variables = 1\nEndo variables = 1\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(a) 2-D MDP (n = 1).\n0\n5\n10\n15\n20\n25\n# of Policy Updates\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nTrue Endo Reward\nTotal training steps = 40000\nDecomposition steps = 2000\nPolicy update steps = 1536\nExo variables = 2\nEndo variables = 2\nReplications = 15\nAlgorithm\nBaseline\nGRDS\nSimplified-GRDS\nSRAS\nEndo Reward Oracle\n(b) 4-D MDP (n = 2).\nFigure 13: RL performance for MDPs with anti-correlated exo and endo rewards.\nand endogenous rewards. RQ5 asks how our algorithms behave when this covariance condition is violated. Does RL\nperformance suffer?\nTo study this question, we deﬁne the following family of 2n-D MDPs with a nonlinear exogenous reward\nxt+1 = 0.9 · Mexo · xt + εexo\net+1 = 0.45 · Mend · et + 0.55 · Mexo · xt + Ma · at + εend\nRend,t = e−|avg(et)−2.5|/3\nRexo,t = e−|avg(xt)+2.5|/3.\nThe endo and exo states et and xt are both n-D. The matrices Mexo ∈Rn×n and Mend ∈Rn×n follow the recipe\ngiven in Section 5.2. Ma ∈Rn is the coefﬁcient for the action at, and it is set to be a vector of ones. The elements of\nthe exogenous noise εexo ∈Rn are distributed according to N(0, 0.16), while the elements of the endogenous noise\nεend ∈Rn are distributed according to N(0, 0.04). The observed state vector st ∈R2n is a linear mixture of the\nhidden exogenous and endogenous states, as in Section 5.2.\nAn interesting property of this class of MDPs is that the product Mexo · xt appears with a relatively high coefﬁcient\nin both xt+1 and et+1. Hence, et+1 and xt+1 will exhibit positive correlation. As a result, when avg(et) (and thus\navg(xt)) is high and close to 2.5, Rend,t will be high and Rexo,t will be low. Conversely, when avg(et) (and thus\navg(xt)) is low and close to −2.5, Rend,t will be low and Rexo,t will be high. This causes the endo and exo rewards to\nbe anti-correlated to the point that the covariance condition is violated.\nSo verify this, we measured the variance Var(Rexo,t) and the covariance −2 · Cov(Rend,t, Rexo,t) for both MDPs\nunder 2 policies: a random policy and the optimal policy. For the 2-D MDP, we ﬁnd that Var(Rexo,t) = 0.026 and −2 ·\nCov(Rend,t, Rexo,t) = 0.04 under a random policy, and Var(Rexo,t) = 0.026 and −2 · Cov(Rend,t, Rexo,t) = 0.037\nunder the optimal policy. For the 4-D MDP, we ﬁnd that Var(Rexo,t) = 0.021 and −2 · Cov(Rend,t, Rexo,t) = 0.033\nunder a random policy, and Var(Rexo,t) = 0.021 and −2 · Cov(Rend,t, Rexo,t) = 0.040 under the optimal policy.\nHence, the covariance condition is violated in all cases.\nFigures 13a and 13b show the RL performance for the different methods when n = 1 and n = 2, respectively.The\nexo subspace ranks and time costs are summarized in Table 8. Since the MDPs are rather small, for the former we set\nL = 1000 for the number of steps prior to applying the state-space decomposition methods and train for N = 30000.\nFor the latter set L = 2000 and N = 40000. We perform 15 trials with different seeds.\nNotice how the Endo Reward Oracle performs worse than the Baseline; this is to be expected, because the Endo Reward\nOracle is forced to use the Endo reward, which has higher variance than the full reward. Our naive expectation was that\nour decomposition methods would exhibit the same behavior, since they seek to ﬁnd the maximal exogenous subspace.\nHowever, what we observe is that all of our methods do much better than the Endo Reward Oracle and, in the case of\nn = 1, they exceed the Baseline as well.\nThe key to understanding what is happening is to focus on the reward regression. The loss function for the reward\nregression is precisely the residual variance of the predicted reward, and the residual reward is precisely our estimate\nof the endogenous reward. Consequently, reward regression will ignore any exogenous variables that would cause an\nincrease in the variance of the endogenous reward. In the case of n = 1, the reward regression has apparently found\n34\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nsome component of the exogenous reward that is not anti-correlated with the endogenous reward and exploited it to\nreduce the variance of the endogenous reward. Consequently our methods are able to learn faster than the Baseline.\nThe larger result is that our algorithms can never be affected by anti-correlated rewards. In the worst case, the reward\nregression will ignore the exogenous variables and simply predict the mean exogenous reward. This will have no effect\non the endo-MDP or the RL performance of our methods.\nExo State\nEndo State\nMethod\nExo Subspace\nTotal Time\nDecomposition\nVariables\nVariables\nRank\n(secs)\nTime (secs)\n1\n1\nBaseline\n-\n221.9±9.7\n-\nGRDS\n1.0±0.0\n276.0±38.3\n2.2±0.7\nSimpliﬁed-GRDS\n1.0±0.0\n276.9±36.8\n2.2±0.6\nSRAS\n1.0±0.0\n276.6±39.6\n2.3±0.8\n2\n2\nBaseline\n-\n308.4±9.4\n-\nGRDS\n2.4±0.8\n402.8±67.0\n22.5±25.2\nSimpliﬁed-GRDS\n3.0±0.0\n394.3±55.5\n6.5±0.8\nSRAS\n2.26±0.93\n403.8±80.6\n24.0±23.1\nTable 8: Average and standard deviation for rank of discovered exo subspace, total execution time, and decomposition\ntime for the MDPs with anti-correlated exo and endo rewards.\n5.5\nRisks and Beneﬁts of Simpliﬁed-GRDS\nOur experimental results suggest that the Simpliﬁed-GRDS method performs very well. RQ6 asks about the risks\nof the Simpliﬁed-GRDS method. From a mathematical perspective, when we minimize I(X′; A | X) (or the CCC\napproximation), we are only eliminating edges from A to X′, which are the direct effects of A. We are not excluding any\nindirect path by which A might affect E′, and E′ might then affect X in some future time step. In the linear dynamical\nMDPs that we have studied, such indirect effects do not arise, and all effects of A on X′ are visible immediately. Of\ncourse, because Simpliﬁed-GRDS veriﬁes the full constraint I(X′; [E, A] | X) < ϵ, Simpliﬁed-GRDS is still sound,\nbut it may fail to ﬁnd the maximal exogenous subspace in complex MDPs (as we saw in some of the modiﬁed MDPs).\nThis suggests that for general MDPs, we should choose GRDS rather than Simpliﬁed-GRDS. However, our experiments\nhave also shown that there are several cases where GRDS encounters difﬁculty minimizing the CCC proxy of the\nI(X′; [E, A] | X) objective. Hence, we believe additional research is needed to improve the manifold optimization\nprocess so that GRDS can overcome these challenging cases.\n5.6\nPractical Considerations\nIn this section, we switch our attention to practical aspects of our proposed methods. Our goal is to answer RQ7, which\nconcerns how to set the hyperparameters of our method.\n5.6.1\nImpact of Hyperparameters on Baseline\nFirst, we investigate the impact of hyperparameters on the Baseline. For this purpose, we consider the High-D linear\nsetting of Section 5.2. Recall that for this setting we used the default PPO and Adam hyperparameters in stable-\nbaselines3, summarized in Table 2. We now ask whether the performance of Baseline can be improved by using\ndifferent hyperparameters. If that were the case, then a more careful hyperparameter tuning could be an alternative to\nour proposed algorithms.\nWe consider the 5-D MDP with 3 exo and 2 end variables of Figure 6a, where the Baseline ﬂuctuates between 0.4 and\n0.5 with an average of 0.45, visibly lower than all other methods. To understand whether this can be improved further,\nwe tune 3 critical hyperparameters of PPO optimization. We let the batch size take values in {16, 32, 64, 128, 256, 512}\n(Figure 14a), the learning rate take values in {5 × 10−5, 1 × 10−4, 5 × 10−4, 1 × 10−3, 5 × 10−3, 1 × 10−2} (Figure\n14b), and the number of steps per policy update K take values in {250, 500, 750, 1000, 2000, 3000} (Figure 14c). For\neach experiment, all hyperparameters except for the tuned one are set to the values in Figure 6a. For Figure 14c, we\nperform policy evaluation every 1536 steps, instead of after each policy update, to ensure that all curves have the same\nnumber of evaluation points. We use 10 independent replications with different seeds. Finally, we increase the number\nof training steps to N = 80000 to ensure that the Baseline has enough training budget to converge.\nThe results demonstrate that none of the parameter combinations can raise performance signiﬁcantly. Some values for\nthe number of policy update steps manage to slightly improve the average Baseline performance to 0.5 from 0.45, but\n35\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 3000\nPolicy update steps = 1536\nLearning rate = 0.0003\nExo variables = 3\nEndo variables = 2\nReplications = 10\nBaseline\nBatch size = 16\nBatch size = 32\nBatch size = 64\nBatch size = 128\nBatch size = 256\nBatch size = 512\n(a) Tuning batch size.\n0\n10\n20\n30\n40\n50\n# of Policy Updates\n0.35\n0.40\n0.45\n0.50\n0.55\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 3000\nPolicy update steps = 1536\nBatch size = 64\nExo variables = 3\nEndo variables = 2\nReplications = 10\nBaseline\nLearning rate = 0.00005\nLearning rate = 0.0001\nLearning rate = 0.0005\nLearning rate = 0.001\nLearning rate = 0.005\nLearning rate = 0.01\n(b) Tuning learning rate.\n0\n10\n20\n30\n40\n50\n# of Evaluations\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nTrue Endo Reward\nTotal training steps = 80000\nDecomposition steps = 3000\nLearning rate = 0.0003\nBatch size = 64\nExo variables = 3\nEndo variables = 2\nReplications = 10\nBaseline\nPolicy update steps = 250\nPolicy update steps = 500\nPolicy update steps = 750\nPolicy update steps = 1000\nPolicy update steps = 2000\nPolicy update steps = 3000\n(c) Tuning policy update steps K.\nFigure 14: RL performance for Baseline of Figure 6a under various hyperparameters.\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 10\nSimplified-GRDS\nDecomposition steps = 250\nDecomposition steps = 500\nDecomposition steps = 750\nDecomposition steps = 1000\nDecomposition steps = 1500\nDecomposition steps = 2000\nDecomposition steps = 2500\nDecomposition steps = 3000\nDecomposition steps = 4000\nDecomposition steps = 5000\nDecomposition steps = 6000\nDecomposition steps = 7000\n(a) Simpliﬁed-GRDS.\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 10\nSRAS\nDecomposition steps = 250\nDecomposition steps = 500\nDecomposition steps = 750\nDecomposition steps = 1000\nDecomposition steps = 1500\nDecomposition steps = 2000\nDecomposition steps = 2500\nDecomposition steps = 3000\nDecomposition steps = 4000\nDecomposition steps = 5000\nDecomposition steps = 6000\nDecomposition steps = 7000\n(b) SRAS.\nFigure 15: RL performance for varying values of L, the number of steps prior to applying the decomposition algorithms,\nfor the 10D setting of Figure 6b.\nthey still suffer from signiﬁcant variance. This is in sharp contrast to our algorithms and the Endo Reward Oracle in\nFigure 6a, which all exhibit much lower variance. Given that this experiment only tunes one hyperparameter at a time,\nwe cannot exclude the possibility that there are combinations of hyperparameters that can achieve a higher and more\nstable performance for the Baseline. However, its low performance under the default hyperparameters and on a range of\nreasonable values provides evidence that its poor performance mainly stems from the stochasticity of the exogenous\nrewards and not badly-chosen hyperparameters.\n36\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n2\n4\n6\n8\n10\nIndex i\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\nAngle (radians)\nTotal training steps = 60000\nPolicy update steps = 1536\nExo variables = 5\nEndo variables = 5\nReplications = 10\nAlgorithm\nSimplified-GRDS\nSRAS\nFigure 16: Angle between orthogonal complements of computed exo subspaces corresponding to Li and Li+1\ndecomposition steps, where the index i ranges from 0 to 10.\n5.6.2\nSensitivity Analysis\nRecall from Section 5.1 that the main hyperparameter we need to set for our proposed methods is the number of\nsteps L prior to applying the exogenous subspace discovery algorithms. This speciﬁes the number of ⟨s, a, r, s′⟩\ntuples that are collected for subspace discovery. In this section, we shed light on the impact of L on RL performance.\nIn this direction, we consider the 10-D setting with 5 exo and 5 endo variables of Figure 6b, where our methods\nand the Endo Reward Oracle converge to a total reward of 0.8 in N = 50000 steps. In contrast, the Baseline only\nattains a reward of around 0.3 on average. We perform sensitivity analysis by considering twelve possible values\nfor L: 250, 500, 750, 1000, 1500, 2000, 2500, 3000, 4000, 5000, 6000, 7000. We denote each value by the increasing\nsequence Li, i ∈{0, . . . , 11}, where L0 = 250 and L10 = 7000. We report the average RL performance for the\nSimpliﬁed-GRDS and SRAS methods in Figure 15(a,b), since they both perform very well. For both methods, when\nL0 = 250, performance is barely better than the Baseline. As we increase L, performance improves steadily and almost\nmatches the Endo Reward Oracle as soon as we reach 2000 decomposition steps. Increasing L beyond 2000 gives\nminor beneﬁt and delays the time at which PPO can take advantage of the improved reward function.\nThis suggests that the decomposition algorithms have converged after L = 2000. Can we verify this? The rank of the\ndiscovered exogenous space is not informative, because it is always 9-dimensional for all 12 values of L and across all\n10 replications. To get a ﬁner-grained measure, we can take advantage of the fact that the complement of the discovered\n9-dimensional exogenous space is a 1-dimensional space. This means it can be represented by a direction vector, and\nwe can compare different solutions by computing the angles between these direction vectors.\nFigure 16 depicts the angle (in radians) between the orthogonal complements of the exogenous subspaces for each\nconsecutive pair of Li and Li+1 values. If the methods had converged, these angles would be zero. They are not zero,\nwhich shows that the exogenous subspaces are continuing to change as L increases. But the angles are all very small\n(less than 0.5 degrees in the largest case), so these changes are not large, and they are converging (with few exceptions)\nmonotonically toward zero. Simpliﬁed-GRDS exhibits smooth convergence, whereas SRAS shows higher variance and\na few bumps.\nThe results suggest that manifold optimization performs very well on this problem, even with relatively small numbers\nof samples. What is then the reason for the different performance levels in Figure 15? The answer lies in the exogenous\nreward regression. Recall that after L steps, we conclude Phase 1 by computing the decomposition and then ﬁtting the\nexogenous reward neural net to the L collected observations. Even though different numbers of decomposition steps\nresult in almost identical exogenous subspaces, the subsequent exogenous reward regression can yield dramatically\ndifferent exo reward models. When the value of L is very low, we have only a limited number of samples for the exo\nreward regression, and these might not to cover the exo state subspace adequately. As a result, the learned exo reward\nmodel may overﬁt the observations and fail to generalize to other subspaces. The subsequent online neural network\nreward regression in Phase 2 only processes each new observation once, so learning the correct exo reward model can\ntake many steps, and cause PPO to learn slowly.\nTo conﬁrm the above, we perform a second experiment. Unlike previous experiments, we decouple decomposition and\nexogenous reward regression. Decomposition still takes place after Li steps. But reward regression is now performed\n37\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n0\n10\n20\n30\n40\n# of Policy Updates\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nPolicy update steps = 1536\nRegression steps = 3000\nReplications = 10\nSimplified-GRDS\nDecomposition steps = 250\nDecomposition steps = 500\nDecomposition steps = 750\nDecomposition steps = 250,\n Single Regression\nDecomposition steps = 500,\n Single Regression\nDecomposition steps = 750,\n Single Regression\nDecomposition steps = 250,\n Repeated Regression\nDecomposition steps = 500,\n Repeated Regression\nDecomposition steps = 750,\n Repeated Regression\n(a) Simpliﬁed-GRDS\n0\n10\n20\n30\n40\n# of Policy Updates\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrue Endo Reward\nTotal training steps = 60000\nPolicy update steps = 1536\nRegression steps = 3000\nReplications = 10\nSRAS\nDecomposition steps = 250\nDecomposition steps = 500\nDecomposition steps = 750\nDecomposition steps = 250,\n Single Regression\nDecomposition steps = 500,\n Single Regression\nDecomposition steps = 750,\n Single Regression\nDecomposition steps = 250,\n Repeated Regression\nDecomposition steps = 500,\n Repeated Regression\nDecomposition steps = 750,\n Repeated Regression\n(b) SRAS\nFigure 17: RL performance when computing the exo/endo decomposition at Li = 250, 500, and 750 steps. Reward\nregression starts at 3000 steps. Default: online neural network regression; Single Regression: single linear regression at\n3000 steps; Repeated Regression: linear regression every 256 steps.\nafter 3000 samples have been collected (3000 is the default value for L in the high-D experiments). After learning\nthe exogenous reward model with the 3000 samples, we proceed to Phase 2. We experiment with three options for\nexogenous reward regression in Phase 2: (i) standard online learning where we update the exogenous reward model\nevery M = 256 steps; (ii) a single linear regression after which the exogenous reward model is never updated; and (iii)\nrepeated linear regression where we ﬁt a new exogenous reward model from scratch every M = 256 steps. We study\nthe three lowest values for Li (L0 = 250, L1 = 500 and L2 = 750), as these were the values in Figure 15 with the\nworst performance.\nFigure 17 plots the RL performance averaged over 10 independent trials. We make several observations. First, increasing\nthe number of steps for learning the initial exogenous reward model from Li to 3000 improves RL performance. With\nonline learning, we match the Endo Reward Oracle’s average performance of 0.8. This conﬁrms our previous hypothesis\nthat the reason for the bad performance in Figure 15 was the poor initial exogenous reward model. With a single\nregression, RL performance improves (especially for L0 = 250 and L1 = 500), but it performs worse than online\nlearning while having greater variance. Interestingly, learning a new linear regression model every M = 256 steps\nperforms the best and slightly outperforms online neural network regression. A plausible reason for this is that online\nregression only performs a single pass over the data, so it may adapt to the changing state and reward distribution more\nslowly. On the other hand, repeated linear regression requires 10 times as much computation time as online regression\nfor the settings in Figures 17a and 17b.\n5.6.3\nPractical Guidelines For State Decomposition and Exo Reward Regression\nThis sensitivity analysis suggests the following procedure for computing state space decompositions and reward\nregressions. Start with a small number for L (e.g., 250) and compute the corresponding exo subspace after L steps.\nThen, every ∆L steps (e.g., 250), recompute the exo subspace until the discovered exo subspace stops changing.\nThis can be detected when (i) the rank of the subspace does not change, and (ii) the largest principal angle between\nconsecutive subspaces is close to 0 [Bj¨orck and Golub, 1973].\nAs soon as we have an initial exo subspace, we can ﬁt the exogeneous reward model, and each time we recompute\nthe subspace, we can re-ﬁt the model. These initial ﬁts could be performed with linear regression or neural network\nregression. Once the exogenous subspace as converged, we can switch to online neural network regression, because the\nregression inputs will have stabilized.\nApplication constraints may suggest alternative procedures. If each step executed in the MDP is very expensive, then the\ncost of the multiple decomposition and reward regression computations is easy to justify. However, if MDP transitions\nare cheap, then we need to take a different approach. If many similar MDPs will need to be optimized, we can use\nthis full procedure on a few of them to determine the value of L at which the exo space converges. We then just use\nthat value to trigger exo/endo decomposition and perform neural network reward regression starting at step L and\ncontinuing online. If there is only one MDP to be solved, L could be selected using a simulation of the MDP. In all of\nour experiments, we have followed this procedure for setting L.\n38\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nA last question concerns the value for the CCC threshold ϵ. In theory, we should use very low values to minimize the\nchance of discovering an invalid exo/endo state decomposition, but this could come at the cost of having to perform\nmore steps in GRDS and SRAS. We lack theoretical guidance for making this decision. In principle, one could start\nwith a somewhat large value for ϵ (e.g., 0.1) and perform multiple runs (in simulation or on a sample of MDPs) with\nprogressively smaller values of ϵ until either the discovered exo subspace converges or RL performance stabilizes.\n5.7\nDiscussion\nLet us review the research questions and our results.\nRQ1: Do our methods speed up reinforcement learning in terms of sample complexity? In terms of total CPU time?\nWe found that our methods yield dramatic speedups for reinforcement learning both in terms of the number of\nMDP transitions observed and in terms of clock time. The improvements are so dramatic that we were not able\nto run the Baseline method long enough for it to match the level of RL performance achieved by our methods,\nso we estimated this by linear extrapolation.\nRQ2: Do our methods discover the correct maximal exogenous subspaces? Our methods surprised us by ﬁnding\nexogenous subspaces that were larger than we had naively expected. Analysis showed that these subspaces did\nindeed have maximal dimension. Our experiments measured the correctness of the subspaces by the resulting\nRL performance. In nearly all cases, our methods matched the performance of the Endo Reward Oracle that\nwas told the correct subspace for the endogenous reward. In the 2-D covariance condition experiment, our\nmethods did better than the Endo Reward Oracle.\nRQ3: What is the best approach to reward regression? Single linear, repeated linear, or online neural network\nregression? We found that online neural network regression worked best and was more stable, even in cases\nwhere the reward function was linear.\nRQ4: How do the algorithms behave when the MDPs are changed to have different properties: (a) rewards are\nnonlinear, (b) transition dynamics are nonlinear, (c) action space is combinatorial, and (d) states and actions\nare discrete? Our methods, combined with neural network regression, handled the nonlinear rewards well.\nWhen the transition dynamics are nonlinear, our methods sometimes struggled, but we found that they worked\nwell if we increased the ϵ parameter to allow the CCC objective to be substantially nonzero. Our methods\nworked very well in combinatorial action spaces and in discrete MDPs.\nRQ5: How do the algorithms behave when the covariance condition is violated? When the covariance condition is\nviolated, our methods still give excellent results. This is because reward regression minimizes the variance\nof the endogenous reward, so it only ﬁts that aspect of the exogenous reward that can be removed without\nincreasing the variance of the endogenous reward.\nRQ6: What are the risks and beneﬁts of using the simpliﬁed objective in SRAS and Simpliﬁed-GRDS? The\nSimpliﬁed-GRDS method gave the best overall RL performance. It provides a nice balance of computational\nefﬁciency and optimization performance while remaining sound.\nRQ7: How should hyperparameters be set? How many training tuples should be collected before performing\nexogenous subspace discovery? When should reward regression begin, and on what schedule? We found that\nthe default values of the hyperparameters worked well, and we developed a tuning procedure for determining\nthe training set size K for exogenous subspace discovery. It starts with K = 250 and repeatedly adds 250\ntuples until the exo space stabilized. Small values of K worked surprisingly well.\nThe beneﬁts of our algorithms are not necessarily limited to linear MDPs. We observed strong performance with\ngeneral nonlinear exo reward functions, and often on MDPs with nonlinear exo/endo transition dynamics but where\nthe state space could be linearly decomposed into the exo and endo subspaces. This showed that our CCC objective\ncan provide good results even in these nonlinear cases. An alternative to our CCC-based linear approach would be to\nenforce the exo/endo decomposition using cross-covariance operators on a Reproducing Kernel Hilbert Space (RKHS).\nHowever, RKHS computations can be computationally challenging and are difﬁcult to scale to high-dimensional spaces\n[Fukumizu et al., 2008].\n6\nRelated Work\nIn this section, we review prior work on reinforcement learning with exogenous information that is not covered\nelsewhere in the paper. Efroni et al. [2022b] introduce the Exogenous Block Markov Decision Process (EX-BMDP)\nsetting to model environments with exogenous noise. Under the assumption that the endogenous state transition is\nnear deterministic, they propose the Path Predictive Elimination (PPE) algorithm. PPE learns a form of multi-step\n39\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\ninverse dynamics [Paster et al., 2021]. It can recover the latent endogenous model while being both sample efﬁcient\nand computationally efﬁcient. PPE runs in a reward-free setting and does not make any assumptions about the reward\nfunction. In their follow-up work, Efroni et al. [2022a] introduce the ExoMDP setting, which is a ﬁnite state-action\nvariant of the EX-BMPD where the state directly decomposes into a set of endogenous and exogenous factors, while\nthe reward only depends on the endogenous state. They propose ExoRL, an algorithm that learns a near-optimal policy\nfor the ExoMDP with sample complexity polynomial in the number of endogenous state variables and logarithmic in\nthe number of exogenous components. Because the reward does not depend on the exogenous state variables, those\nvariables are not only exogenous but irrelevant, and they can be ignored during reinforcement learning. In our work, in\ncontrast, the exogenous variables can still be important for the policy, so the RL algorithm must consider them as inputs\nwhen learning the policy.\nIn a different thread of work, Chitnis and Lozano-P´erez [2020] address the problem of learning a compact model of\nan MDP with endogenous and exogenous components for the purpose of planning. They consider only reducing the\nexogenous part of the state, given that is the part that induces the most noise, and assume that the reward function\ndecomposes into a sum over the individual effects of each exogenous state variable. They introduce an algorithm based\non the mutual information among the exogenous state variables that generates a compact representation, and they\nprovide conditions for the optimality of their method. Our assumption of additive reward decomposition is weaker than\ntheir per-state-variable assumption.\nAnother line of work related to exogenous information concerns curiosity-driven exploration by Pathak et al. [2017],\nwhere the goal is to learn a reward to enable the agent to explore its environment better in the presence of very sparse\nextrinsic rewards. It falls under the well-studied class of methods with intrinsic rewards [Bellemare et al., 2016, Haber\net al., 2018, Houthooft et al., 2016, Oh et al., 2015, Ostrovski et al., 2017, Badia et al., 2020] This work employs a\nself-supervised inverse dynamics model, which encodes the states into representations that are trained to predict the\naction. As a result, the learned representations do not include environmental features that cannot inﬂuence or are not\ninﬂuenced by the agent’s actions. Based on these representations, the agent can learn an exploration strategy that\nremoves the impact of the uncontrollable aspects of the environment. Note that the inverse dynamics objective is\ngenerally not sufﬁcient for control and does not typically come with theoretical guarantees [Rakelly et al., 2021]. Finally,\nwe mention that reinforcement learning with exogenous information has also been studied from a more empirical\nstandpoint in real problems such as learning to drive [Chen et al., 2021].\nFinally, our work shares similarities with other work on modeling the controllable aspects in the environment [e.g.,\nChoi et al., 2019, Song et al., 2020, Burda et al., 2019, Bellemare et al., 2012, Corcoll et al., 2022, Thomas et al.,\n2018]. The main difference is that in exo/endo MDPs, we capture the controllable versus uncontrollable aspects via\nthe exo/endo factorization, where endo (resp., exo) states correspond to the controllable (resp. uncontrollable) aspects.\nCombining the two families of approaches could be an avenue for future research. We also note the work by Yang et al.\n[2022], which proposes the dichotomy of control for return-conditioned supervised learning. This is accomplished\nby conditioning the policy on a latent variable representation of the future and introducing two conditional mutual\ninformation constraints that remove any information from the latent variable that has to do with randomness in the\nenvironment. However, unlike ours, their work is not concerned with exogenous state variables and decompositions.\n7\nConcluding Remarks\nIn this paper, we proposed a causal theory of exogeneity in reinforcement learning and showed that in causal models\nsatisfying the full and diachronic two-step DBN structures, the exogenous variables are causally exogenous. We intro-\nduced exogenous-state MDPs with additively decomposable rewards and proved that such MDPs can be decomposed\ninto an exogenous Markov reward process and an endogenous MDP such that any optimal policy for the endogenous\nMDP is an optimal policy for the original MDP. We studied the properties of valid exo/endo decompositions and proved\nthat there is a maximal exogenous subspace that contains all other exogenous subspaces. We also showed that not all\nsubsets of the maximal exogenous subspace deﬁne valid exo/endo decompositions.\nGiven an exogenous-state MDP, we wish to discover a useful exogenous subspace of its state space. We explored several\noptimization formulations based on conditional mutual information for discovering the exogenous state variables and\nestimating the exogenous reward. We developed two practical algorithms, GRDS and SRAS, for the case when the\nexogenous space is a linear projection of the full state space. Our algorithms use the conditional correlation coefﬁcient\n(CCC) as a measure for conditional independence and rely on solving a manifold optimization problem. Under the\nassumption that the exploration policy visits all states and tries all actions inﬁnitely often, we showed that GRDS\ndiscovers the maximal exogenous subspace. Furthermore, under the assumption of faithfulness, the exogenous subspace\nwe discover is guaranteed to contain only causally-exogenous components. We also introduced Simpliﬁed-GRDS,\nwhich employs a simpliﬁed CCC objective and then checks the solution to see if it satisﬁes the full CCC objective.\n40\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nOur experiments on a variety of high-dimensional linear dynamical MDPs demonstrated that our algorithms, particularly\nSimpliﬁed-GRDS and SRAS, can greatly accelerate reinforcement learning. Additional experiments showed that\nthese methods can also give excellent performance on MDPs with nonlinear rewards, nonlinear transition dynamics,\ncombinatorial action spaces, and discrete state spaces. The methods are robust to hyperparameter settings and do not\nrequire delicate hyperparameter tuning.\nThere are several important directions for future research. First, we lack a deep theoretical understanding of the manifold\noptimization problem corresponding to the CCC. It would be helpful to shed light on its sample complexity and on its\nloss landscape, which may contain local minima. It is also critical to elucidate the conditions under which the CCC can\ncapture conditional independence, at least for linearly decomposable MDPs. Second, we lack a rigorous theory of the\nimpact of exogenous reward regression on RL convergence. Such knowledge could help us design better exogenous\nreward regression methodologies. Third, in our work we constrain the exogenous rewards to be additive. However, it is\npossible that in real settings exogenous rewards are non-additive (e.g., multiplicative), and our decomposition theorem\nwill not hold. Investigating non-additive decompositions would be valuable. Fourth, a more precise mathematical\nanalysis of the simpliﬁed objective that we employ in the Simpliﬁed-GRDS and SRAS algorithms might help explain\ntheir strong empirical performance. Fifth, it would be nice to study nonlinear exo/endo decompositions, especially\nin the presence of nonlinear transition dynamics. Can efﬁcient, general strategies be discovered? Finally, are there\nalternatives to the assumptions (admissible MDP, fully-randomized policy, and faithfulness) that we adopted to prove\ncausal exogeneity of the subspaces discovered by GRDS?\nAcknowledgments\nWe would like to acknowledge support for this project from the National Science Foundation (NSF grant IIS-9988642),\nthe Multidisciplinary Research Program of the Department of Defense (MURI N00014-00-1-0637), and a gift to the\nOregon State University Foundation from Huawei. We thank Yuvraj Sharma for porting our implementation to Python\nand assisting with some of the experiments.\nA\nAdditional Theory on Exogenous State Variables\nDeﬁnition 1 for causal exogeneity is inherently structural: it deﬁnes causal exogeneity as a property of the structure\nof the causal graph. In this appendix, we exploit this to structurally characterize causally exogenous state variables\n(Appendix A.1) as well as the causal graphs with causally exogenous state variables (Appendix A.2).\nA.1\nStructural Characterization of Causally-Exogenous State Variables\nWe begin by providing a general structural condition in terms of the unrolled DBN of an MDP that guarantees causal\nexogeneity. To this goal, we introduce the structural notion of action-disconnected states; we subsequently show that\naction-disconnected state variables are causally exogenous.\nDeﬁnition 8 (Action-Disconnected State Variables). A state variable C is action-disconnected, if the unrolled decision\ndiagram contains no directed chain of the form At →· · · →Cτ, ∀t ∈{0, . . . , H −1}, ∀τ ∈{t + 1, . . . , H}.\nTo visualize action-disconnected state variables, consider the 3-state MDP of Figure 18a and assume a horizon H = 2.\nThe unrolled diagram for that value of H is depicted in Figure 18b. State variables S1 and S2 are action-disconnected,\nwhereas S3 is not due to the presence of the directed chains A →S′\n3 →S′′\n3 →S′′′\n3 , A′ →S′′\n3 →S′′′\n3 , and A′′′ →S′′′\n3 .\nTheorem 9. If a state variable C is action-disconnected, then C is causally exogenous.\nProof. Let G be the graph corresponding to the unrolled decision diagram of the MDP from timestep 0 to timestep H.\nDeﬁne a trail as a loop-free and undirected (i.e., all edge directions are ignored) path between two nodes. We will make\nuse of the d-separation theory with trails [see Pearl, 1988, 2009]. Furthermore, recall Rule 3 of the do-calculus for any\ncausal graph G\nP(F | do(G), do(H), J) = P(F | do(G), J), if F ⊥⊥H | G ∪J in ˜G,\nwhere ˜G is the graph obtained by ﬁrst deleting all edges pointing into G and then deleting all arrows pointing into H\nfrom nodes that are not ancestors of J. According to Deﬁnition 1, in order to show that C must be exogenous, we must\nprove that\nP(Ct+1, . . . , CH | Ct, do(At)) = P(Ct+1, . . . , CH | Ct).\nWe will prove this statement by applying Rule 3 of the do-calculus above. For this purpose, we set F ←Ct+1, . . . , CH,\nG ←∅, H ←At, and J ←Ct. Given G is the empty set and At cannot have incoming edges from any ancestor of Ct,\n41\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nthe graph ˜G will be identical to G except that the incoming edges to At are deleted. To show exogeneity, it then sufﬁces\nto show that\nCt+1, . . . , CH ⊥⊥At | Ct in ˜G.\nIndeed, consider any trail P connecting node At to node Ct+τ in ˜G. Since C is action-disconnected, we know that\nthere can be no directed chain from At to Ct+τ of the form At →· · · →Ct+τ. We argue that this then implies that\nP must necessarily contain a collider node Z of the form →Z ←. To see why, notice that the ﬁrst link in P has the\nform At →· · · . If all subsequent links in P are of the form · · · →· · · , then P would be a directed chain, which is a\ncontradiction. Hence, the edge directionality at P must change at some node, which implies there must be some collider\nnode Z in P. The following two cases are then possible:\n1. If Z is not Ct or an ancestor of Ct, then the P is d-separated and thus does not violate the causal Deﬁnition 1.\n2. Otherwise, assume that Z is either Ct or an ancestor of Ct. By the structure of the causal graph, it is not\nhard to see that in order for this assumption to hold, P must contain a link Vt ←Ut−1 from some state or\naction variable Ut−1 at time step t −1 to some state variable Vt at time step t. Consider the sub-trail up to\nnode Ut−1, which will have the form At →· · · Vt ←Ut−1. The sub-trail must contain a collider node ˜Z;\nfurthermore, ˜Z has time index t or higher, so it cannot be Ct or an ancestor of Ct. But this shows that trail P\nis also d-separated and, hence, does not violate the causal exogeneity Deﬁnition 1.\nAccording to Corollary 5, state variables S1 and S2 in the MDP of Figure 9 are causally exogenous. What about the\nreverse direction? We show that it is a natural implication of Deﬁnition 1 for causal exogeneity.\nTheorem 10. If state variable C is causally exogenous, then C must be action-disconnected in the corresponding\ncausal graph G.\nProof. Assume C is not action-disconnected. Then there must exist some directed chain At →· · · →Ct+τ connecting\nAt to some future state Ct+τ. This chain cannot go through Ct due to the structure of G. In that case, At and Ct+τ\nare not d-separated by Ct in graph ˜G, where ˜G is identical to G except that the incoming edges to At are deleted. As\na result, we cannot apply Rule 3 of the do-calculus to simplify expression P(Ctτ | do(At), Ct) to P(Ctτ | Ct). The\nproperty P(Ct+τ | do(At), Ct) = P(Ct+τ | Ct) cannot thus hold as a result of the DAG structure of G. Intuitively, the\nexistence of chain At →· · · →Ct+τ in ˜G means that there is a path through which an intervention do(At) on At can\ninﬂuence Ct+τ, which implies that state variable C cannot be causally exogenous.\nIt is not hard to see that Theorems 9 and 10 also hold, when C is a set of causally exogenous state variables. Taken\ntogether, Theorems 9 and 10 imply the next result.\nCorollary 5. The set of causally exogenous state variables is identical to the set of action-disconnected variables.\nProof. Theorem 9 establishes the forward direction, while Theorem 10 establishes the reverse direction.\nThe previous result allows us to prove two basic properties of causally exogenous state variables.\n𝐴\n𝑆2\n𝑆3\n𝑆2\n′\n𝑆3\n′\n𝑆1\n𝑆1\n′\n(a) MDP.\n𝐴\n𝑆2\n𝑆3\n𝑆2\n′\n𝑆3\n′\n𝑆1\n𝑆1\n′\n𝐴′\n𝑆2\n′′\n𝑆3\n′′\n𝑆1\n′′\n𝐴′′\n𝑆2\n′′′\n𝑆1\n′′′\n𝑆3\n′′′\n(b) Unrolling the MDP for H = 3.\nFigure 18: An MDP with 3 state variables S1, S2, S3, where S1 and S2 are exogenous.\n42\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n𝐴\n𝑋\n𝐸\n𝑋′\n𝐸′\nFigure 19: State transition diagram for an MDP with all links (black and dashed red) present. Links to the action node\nA from E and X are missing for fully randomized policies.\nTheorem 11. Consider any MDP with causally exogenous state variables.\n1. Assume that the two sets of state variables X1 = S[I1] and X2 = S[I2] are causally exogenous. Then the\nunion set X = S[I1 ∪I2] is also causally exogenous.\n2. Assume X = S[I] is a set of causally exogenous state variables. Any subset ˜X ⊆X is then also causally\nexogenous.\nProof. We trivially have from Corollary 5:\n1. Any state variable in X1 or X2 must be action-disconnected, so the union X1 ∪X2 can only contain action-\ndisconnected state variables.\n2. Any state variable in X must be action-disconnected, hence state variables in ˜X ⊆X must be action-\ndisconnected.\nFinally, we remark that deﬁning exogeneity in causal terms as in Deﬁnition 1 is the most natural choice. To see why,\nconsider the MDP in Figure 18a. For this MDP, S1 and S2 are exogenous, since their evolution does not depend\non the action. Indeed, both S1 and S2 are exogenous by Corollary 5, since it is easy to verify that they are both\naction-disconnected in the unrolled diagram. In particular, S1 will satisfy the causal deﬁnition P(S′\n1 | S1, do(A)) =\nP(S′\n1 | S1). On the other hand, it may not be true that P(S′\n1 | S1, A) equals P(S′\n1 | S1). This is because of the trail\nA ←S2 →S′\n2 →S′\n1, which could make S′\n1 conditionally dependent on A given S1 even though S′\n1 is not causally\ndependent on A.\nA.2\nCharacterization of Causally-Exogenous Causal Graphs with the Complete Exogenous Set\nTheorem 7 establishes conditions under which the discovered set X of a full or diachronic exo/endo decomposition\nis causally exogenous. In particular, under these conditions, the maximal exo set will contain causally exogenous\nstate variables only. A natural question then arises: does the maximal exo set contain all causally exogenous state\nvariables? Notice that the union property (e.g., in Corollary 1 or Lemma 1) only allows us to take the union of exo/endo\ndecompositions, not the union of arbitrary sets of exogenous state variables. So, it is not immediately obvious whether\nthe maximal exo set will contain all exo state variables.\nTo answer this question, we ﬁrst introduce the concept of the complete exogenous set, which we then use to characterize\nthe structure of all possible causal graphs with casually exogenous state variables.\nDeﬁnition 9 (Complete Exogenous Set). The complete exogenous set Xf of an MDP is deﬁned as the set of all causally\nexogenous state variables. Its complement Ef = Xc\nf is called the complete endogenous set.\nObviously, Ef can only contain state variables that are not causally exogenous. Our next result is central and\ncharacterizes all possible causal graphs with causally exogenous state variables.\nTheorem 12. Consider any stationary MDP with causally exogenous state variables. Its underlying causal graph can\nonly have two possible forms: (i) the full exo DBN of Figure 2 with X = Xf, or (ii) a special class of DAGs where all\ndirected paths in the unrolled DBN from A to state variables in Ef terminate at a timestep equal to the horizon H.\n43\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n𝐴\n𝑆3\n𝑆3\n′\n𝑆2\n𝑆2\n′\n𝑆1\n𝑆1\n′\n(a) MDP.\n𝐴\n𝑆3\n𝑆3\n′\n𝑆2\n𝑆2\n′\n𝑆1\n𝑆1\n′\n𝑆3\n′′\n𝑆2\n′′\n𝑆1\n′′\n𝐴′\n(b) Unrolling the MDP for H = 2.\nFigure 20: An edge case for Theorem 12, where the MDP does not accept any full exo/endo decomposition, even for\nthe maximal exo state space.\nProof. Consider the complete exo set Xf. By Corollary 5, we know that Xf must be action-disconnected. Furthermore,\nstate variables in Ef must not be action-disconnected, or else they would be causally exogenous and thus belong to Xf;\nthe latter is impossible since Xf by deﬁnition contains all causally exogenous state variables. As a result, for each\nstate variable Ei in Ef, there must be at least one Et,i with t ∈{1, . . . , H} so that there is a directed path from some\nAτ, τ ∈{0, t −1} to Et,i. Let’s now consider the state partition (Ef, Xf) in causal graph G. The unrestricted state\ntransition diagram will contain all links (black and dashed red) in Figure 19. In order to show that (Ef, Xf) matches\nthe causal full exo DBN of Figure 2, we must show that it must be missing all the red dashed links. We separately\nconsider the 3 links:\n1. The link A →X′ must not be present because it would trivially give a directed path from At to Xt+1, which\nviolates the exogeneity of Xj according to Corollary 5.\n2. The link E′ →X′ must also not be present. Indeed, assume there is some link E′\ni →X′\nj. By the observation\nabove, there is a directed path from some Aτ to Et,i. But then we can extend this path to get a directed path\nfrom Aτ to Xt,j using the link E′\ni →X′\nj, which violates the exogeneity of Xj according to Corollary 5.\n3. We ﬁnally discuss the link E →X′. Assume there is such a link Ei →X′\nj. Given Ei is not exogenous, there\nis a directed path from some Ak to Et,i. We can then extend that path to Xt+1,j to get a directed path from\nAk to Xt+1,j using the link Ei →X′\nj, which violates the exogeneity of Xj according to Corollary 5.\nThere is only one special edge case we need to consider for point (3). If there is a single directed path from A to Ei\nthat terminates at EH,i, then it is not possible to extend that path to XH+1,j, since we cannot go beyond time horizon\nH.\nFigure 20 depicts a graph where case (ii) of Theorem 12 applies. Consider the MDP in Figure 20a with 3 state variables\nand a horizon of H = 2. We depict the unrolling process over H in Figure 20b. Directed paths starting from action\nnodes are shown in blue color. Notice that S1 is the only action-disconnected state variable, and according to Corollary\n5, it will be the only causally exogenous state variable. Hence, the maximal exo set will simply be Xf = {S1} with\ncorresponding Ef = {S2, S3}. Notice, however, the link from S2 to S′\n1; since this link originates from Ef to X′\nf, the\ncorresponding state partition (Ef, Xf) does not abide by the full causally exogenous DBN of Figure 2.\nTheorem 12 has important implications in practice. Assume any admissible MDP with a fully randomized exploration\npolicy. Under faithfulness, any MDP with exogenous state variables will generally accept a full exo/endo (statistical)\ndecomposition S = (E, X), namely, the one where X is the full exo set Xf containing all exogenous state variables.\nHence, the full exo DBN is not just a convenient tool based on the 2-timestep dynamics, but a powerful abstraction that\ncan usually capture the maximal exo set Xm under certain conditions. However, edge cases exist according to Theorem\n12: for instance, in the MDP of Figure 20 there is no valid full exo/endo decomposition even though the maximal exo\nset is nonempty, under the standard conditions. This answers the question we asked at the beginning of Appendix A.2.\nB\nThe Simpliﬁed Objective\nOur experiments showed that the Simpliﬁed-GRDS and SRAS algorithms often give excellent performance. Both\nof these algorithms employ the simpliﬁed information theoretic objective I(X′; A | X) = 0 in place of either the\n44\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\n𝐴\n𝑆3\n𝑆2\n𝑆3\n′\n𝑆2\n′\n𝑆1\n𝑆1\n′\nFigure 21: MDP with 3 endogenous state variables S1, S2, S3.\ndiachronic objective I(X′; A, E, E′ | X) = 0 or the full objective I(X′; A, E | X) = 0. In this appendix, we analyze\nthe properties of the simpliﬁed objective.\nIt is easy to ﬁnd examples where the simpliﬁed objective fails. Consider the DBN in Figure 21 of an MDP with the\nstate variables S1, S2 and S3. The policy determining the action depends only on S2 and S3 but not on S1. All state\nvariables can be endogenous (S1 and S2 directly, and S3 indirectly through its dependence on S1). However, the MDP\nsatisﬁes the condition I(S′\n3; A | S3) = 0. Hence, we cannot use the simpliﬁed objective to safely conclude whether a\nstate variable is exogenous or not when analyzing the 2-timestep DBN.\nAs we would expect, the simpliﬁed setting does not necessarily lead to valid full or diachronic exo/endo decompositions.\nIn this direction, ﬁrst note that even when the simpliﬁed objective returns a set X of variables that are causally\nexogenous, X and its complement E = Xc may not correspond to a valid exo/endo decomposition (E, X). Consider\nfor example X = {S3} in Figure 22a, which satisﬁes I(X′; A | X) = 0 but X and the corresponding E = {S1, S2}\nare not a valid full exo/endo decomposition because S′\n3 depends on S′\n2. This is not the case when using the full\nor diachronic objectives. Furthermore, it is possible for a state variable X to be exogenous, even though it fails to\nsatisfy I(X′; A | X) = 0. Consider X = {S3} in Figure 22b. It may not satisfy I(S′\n3; A | S3) = 0 due to the trail\nA ←S2 →S′\n3 from A to S′\n3.\nDespite the fact that the simpliﬁed objective I(X′; A | X) = 0 is not sufﬁcient to identify exogenous variables,\nit can be used as a simpler proxy for the full objective I(X′; [E, A] | X) = 0. Any set of state variables X that\nsatisﬁes the full objective must necessarily satisfy the simpliﬁed objective, since the latter has fewer constraints than\nthe former. Of course, the simpliﬁed objective may return an over-estimate of the set of exogenous state variables,\npossibly contaminated with endogenous components. For this reason, it is always important to check the decomposition\n(E = Xc, X) that satisﬁes the simpliﬁed objective against the full (or diachronic) objective.\nThe shortcomings of the simpliﬁed objective result from attempting to apply it within the framework of the 2-timestep\nDBN. If we unroll the DBN and consider the H-horizon MDP, then the simpliﬁed objective corresponds to directly\nchecking that all variables in X are disconnected from A and therefore X is causally exogenous. The next theorem\nresembles Theorem 7 for full or diachronic factorizations.\n𝐴\n𝑆3\n𝑆2\n𝑆3\n′\n𝑆2\n′\n𝑆1\n𝑆1\n′\n(a) I(S′\n3; A | S3) = 0.\n𝐴\n𝑆3\n𝑆2\n𝑆3\n′\n𝑆2\n′\n𝑆1\n𝑆1\n′\n(b) I(S′\n3; A | S3) ̸= 0.\nFigure 22: State transition diagrams for two MDPs. In both MDPs, S2 and S3 are exogenous.\n45\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nTheorem 13. Assume that an H-horizon MDP is admissible, data D has been collected by executing a fully-randomized\npolicy for n steps, and the faithfulness assumption holds for the causal graph G describing the MDP. If\nlim\nn→∞\nˆI(Xτ; At | Xt) = 0, ∀t ≤H −1, t + 1 ≤τ ≤H,\n(31)\nthen X is causally exogenous.\nProof. (sketch) Equation (31) is equivalent to the statement that P(Xτ | Xt, At) = P(Xτ | Xt) for all t and τ.\nUnder the faithfulness assumption, we can infer that there is no directed path from any At to any Xτ. Hence, X is\naction-disconnected in the causal graph G, and it follows from Theorem 9 that X is causally exogenous.\nThe reverse direction is trivial. If X is causally exogenous and the faithfulness assumption holds, then the simpliﬁed\ncondition must hold between each At and every Xτ. It follows that in the unrolled MDP, the simpliﬁed objective will\ndetect exactly the exogenous variables (under the assumptions of the theorem).\nTheorem 13 has the drawback that it must consider all long-range dependencies, and this requires estimating\nP(Xt, Xτ, At) for all t ≤H −1 and all τ such that t + 1 ≤τ ≤H. This makes much less effective use of\nthe data set D. In practical applications, it might be fruitful to explore approximate variants of this theorem that\nmake use of a small number F of forward steps. In this case, we could enforce just F constraints per timestep, i.e.,\nI(Xt+k; At | Xt) = 0, ∀1 ≤k ≤F, where F is a small number.\nC\nDynamic Program for the Covariance Condition\nTheorem 14. The variance of the H-step return Bπ(s; H) can be computed as the solution to the following dynamic\nprogram:\nV π(s; 0) := 0;\nVar[Bπ(s; 0)] := 0\n(32)\nV π(s; h) := m(s, π(s)) + γEs′∼P (s′|s,π(s))[V π(s′; h −1)]\n(33)\nVar[Bπ(s; h)] := σ2(s, π(s)) −V π(s; h)2 +\n(34)\nγ2Es′∼P (s′|s,π(s))[Var[Bπ(s′; h −1)]] +\nEs′∼P (s′|s,π(s)[m(s, π(s)) + γV π(s′; h −1)]2\nEquations (33) and (34) apply for all h > 0.\nProof. Sobel (1982) analyzed the variance of inﬁnite horizon discounted MDPs with deterministic rewards. We modify\nhis proof to handle a ﬁxed horizon and stochastic rewards. We proceed by induction on h. To simplify notation, we\nomit the dependence on π.\nBase Case: H = 0. This is established by Equations (32).\nInductive Step: H = h. Write\nB(s; h) = R(s) + γB(s′; h −1),\nwhere the rhs involves the three random variables R(s), S′, and B(s′; h −1) (with samples s′ ∼S′). To obtain\nEquation (33), compute the expected value\nV (s; h) = EB(s;h)[B(s; h)] = Es′,R(s),B(s′;h−1)[R(s) + γB(s′; h −1)],\nand take each expectation in turn. To obtain the formula for the variance, write the standard formula for the variance:\nVar[B(s; h)] = EB(s;h)[B(s; h)2] −EB(s;h)[B(s; h)]2.\nSubstitute R(s) + γB(s′; h −1) in the ﬁrst term and simplify the second term to obtain\nEs′,R(s),B(s′;h−1)\n\u0002\n{R(s) + γB(s′; h −1)}2\u0003\n−V (s; h)2.\nExpand the square in the ﬁrst term:\nEs′,R(s),B(s′;h−1)[R(s)2 + 2R(s)γB(s′; h −1) + γ2B(s′; h −1)2] −V (s; h)2.\nDistribute the two innermost expectations over the sum:\nEs′[ER(s)[R(s)2] + 2m(s)γV (s′; h −1) + γ2EB(s′;h−1)[B(s′; h −1)2]] −V (s; h)2.\n46\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nApply the deﬁnition of variance in reverse to terms 1 and 3 in brackets:\nEs′[Var[R(s)] + m(s)2 + 2m(s)γV (s′; h −1) + γ2Var[B(s; h −1)] + γ2V (s′; h −1)2] −V (s; h)2\nFactor the quadratic involving terms 1, 3, and 4:\nEs′[σ2(s) + [m(s) + γV (s′; h −1)]2 + γ2Var[B(s′; h −1)]] −V (s; h)2.\nFinally, distribute the expectation with respect to s′ to obtain Equation (34).\nTheorem 15. The covariance between the exogenous H-step return Bπ\nx(x; H) and the endogenous H-step return\nBπ\ne (e, x; H) can be computed via the following dynamic program:\nCov[Bπ\nx(x; 0), Bπ\ne (e, x; 0)] := 0\n(35)\nCov[Bπ\nx(x; h), Bπ\ne (e, x; h)] :=\nγ2Ee′∼P (e′|e,x),x′∼P (x′|x)\n\b\nCov[Bπ\nx(x′; h −1), Bπ\ne (e′, x′; h −1)] +\n(36)\n[mx(x) + γVx(x′; h −1)] × [me(e, x, π(e, x)) + γV π\ne (e′, x′; h −1)]\n\t\n−V π\nx (x; h)V π\ne (e, x; h).\nEquation (36) applies for all h > 0.\nProof. By induction. The base case is established by Equation (35). For the inductive case, we begin with the formula\nfor non-centered covariance:\nCov[Bx(x; h), Be(e, x; h)] = EBx(x;h),Be(e,x;h)[Bx(x; h)Be(e, x; h)] −Vx(x; h)Ve(e, x; h).\nReplace Bx(x; h) by Rx(x) + γBx(x′; h −1) and Be(e, x; h) by Re(e, x) + γBe(e′, x′; h −1) and re-\nplace the expectations wrt Bx(x; h) and Be(e, x; h) by expectations wrt the six variables {e′\n∼E′, x′\n∼\nX′, Rx(x), Re(e, x), Bx(x′; h −1), Be(e′, x′; h −1)}. We will use the following abbreviations for these variables:\ns′ = {e′, x′}, r = {Re(x), Re(e, x)}, and B = {Bx(x′; h −1), Be(e′, x′; h −1)}.\nCov[Bx(x; h), Be(e, x; h)] = −Vx(x; h)Ve(e, x; h) +\nEs′,r,B[(Rx(x) + γBx(x′; h −1))(Re(e, x) + γBe(e′, x′; h −1))].\nWe will focus on the expectation term. Multiply out the two terms and distribute the expectations wrt r and B:\nEs′[mx(x)me(e, x) + γmx(x)Ve(e′, x′; h −1) + γme(e, x)Vx(x′; h −1) +\nγ2EB[Bx(x′; h −1)Be(e′, x′; h −1)]].\nApply the non-centered covariance formula “in reverse” to term 4.\nEs′[mx(x)me(e, x) + γmx(x)Ve(e′, x′; h −1) + γme(e, x)Vx(x′; h −1) +\nγ2Vx(x′; h −1)Ve(e′, x′; h −1) + γ2Cov[Bx(x′; h −1)Be(e′, x′; h −1)]].\nDistribute expectation with respect to s′:\nmx(x)me(e, x) + γmx(x)Es′[Ve(e′, x′; h −1)] + γme(e, x)Es′[Vx(x′; h −1)] +\nγ2Es′[Vx(x′; h −1)]Es′[Ve(e′, x′; h −1)] + γ2Cov[Bx(x′; h −1)Be(e′, x′; h −1)]]\nObtain Equation (36) by factoring the ﬁrst four terms, writing the expectations explicitly, and including the\n−Vx(x; h)Ve(e, x; h) term.\nTo gain some intuition for this theorem, examine the three terms on the right-hand side of Equation (36). The ﬁrst is the\n“recursive” covariance for h −1. The second is the one-step non-centered covariance, which is the expected value of\nthe product of the backed-up values for V π\ne and V π\nx . The third term is the product of V π\ne and V π\nx for the current state,\nwhich re-centers the covariance.\n47\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nReferences\nP.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University Press,\nPrinceton, NJ, USA, 2007.\nAlexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A. Saurous, and Kevin Murphy. Fixing a broken ELBO.\nIn Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 159–168, 2018.\nLynton Ardizzone, Jakob Kruse, Carsten Rother, and Ullrich K¨othe. Analyzing inverse problems with invertible neural\nnetworks. In International Conference on Learning Representations, 2019.\nKunihiro Baba, Ritei Shibata, and Masaaki Sibuya. Partial correlation and conditional correlation as measures of\nconditional independence. Australian & New Zealand Journal of Statistics, 46(4):657–664, 2004.\nFrancis R. Bach and Michael I. Jordan. Kernel independent component analysis. J. Mach. Learn. Res., 3:1–48, 2003.\nAdri`a Puigdom`enech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier\nTieleman, Martin Arjovsky, Alexander Pritzel, Andrew Bolt, and Charles Blundell. Never give up: Learning directed\nexploration strategies. In International Conference on Learning Representations, 2020.\nDavid Barber and Felix Agakov. The im algorithm: A variational approach to information maximization. In Proceedings\nof the 16th International Conference on Neural Information Processing Systems, page 201–208, 2003.\nJens Behrmann, Paul Vicol, Kuan-Chieh Wang, Roger Grosse, and Joern-Henrik Jacobsen. Understanding and\nmitigating exploding inverses in invertible neural networks. In Proceedings of The 24th International Conference on\nArtiﬁcial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 1792–1800.\nPMLR, 2021.\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and\nDevon Hjelm. Mutual information neural estimation. In Proceedings of the 35th International Conference on\nMachine Learning, volume 80, pages 531–540. PMLR, 2018.\nMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-\nbased exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, volume 29,\n2016.\nMarc G. Bellemare, Joel Veness, and Michael Bowling. Investigating contingency awareness using atari 2600 games.\nIn Proceedings of the Twenty-Sixth AAAI Conference on Artiﬁcial Intelligence, AAAI’12, page 864–871, 2012.\nJames Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. J. Mach. Learn. Res., 13:\n281–305, 2012.\n˚Ake Bj¨orck and Gene H. Golub. Numerical methods for computing angles between linear subspaces. Mathematics of\nComputation, 27(123):579–594, 1973.\nDavid M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians. Journal of the\nAmerican Statistical Association, 112(518):859–877, 2017.\nRobert Bray. Markov decision processes with exogenous variables. Management Science, 65(10):4598–4606, 2019.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba.\nOpenai gym, 2016.\nLars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter\nPrettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and\nGa¨el Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In ECML\nPKDD Workshop: Languages for Data Mining and Machine Learning, pages 108–122, 2013.\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. In\nInternational Conference on Learning Representations, 2019.\nDian Chen, Vladlen Koltun, and Philipp Kr¨ahenb¨uhl. Learning to drive from a world on rails. In 2021 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages 15570–15579, 2021.\nRohan Chitnis and Tom´as Lozano-P´erez. Learning compact models for planning with exogenous processes. In\nProceedings of the Conference on Robot Learning, volume 100 of Proceedings of Machine Learning Research, pages\n813–822, 2020.\nJongwook Choi, Yijie Guo, Marcin Moczulski, Junhyuk Oh, Neal Wu, Mohammad Norouzi, and Honglak Lee.\nContingency-aware exploration in reinforcement learning. In International Conference on Learning Representations,\n2019.\n48\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nOriol Corcoll, Youssef Sherif Mansour Mohamed, and Raul Vicente. Did i do that? blame as a means to identify\ncontrolled effects in reinforcement learning. Transactions on Machine Learning Research, 2022.\nThomas Dietterich, George Trimponias, and Zhitang Chen. Discovering and removing exogenous state variables and\nrewards for reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning,\nvolume 80, pages 1262–1270. PMLR, 2018.\nM. D. Donsker and S. R.S. Varadhan. Asymptotic evaluation of certain markov process expectations for large time. iv.\nCommunications on Pure and Applied Mathematics, 36(2):183–212, 1983.\nAlan Edelman, Tom´as A. Arias, and Steven T. Smith. The geometry of algorithms with orthogonality constraints. SIAM\nJ. Matrix Anal. Appl., 20(2):303–353, 1999.\nYonathan Efroni, Dylan J Foster, Dipendra Misra, Akshay Krishnamurthy, and John Langford. Sample-efﬁcient\nreinforcement learning in the presence of exogenous information. In Proceedings of Thirty Fifth Conference on\nLearning Theory, volume 178, pages 5062–5127, 2022a.\nYonathan Efroni, Dipendra Misra, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Provably ﬁltering\nexogenous distractors using multistep inverse dynamics. In International Conference on Learning Representations,\n2022b.\nRaphael Fonteneau, Susan A Murphy, Louis Wehenkel, and Damien Ernst. Batch mode reinforcement learning\nbased on the synthesis of artiﬁcial trajectories. Annals of Operations Research, 208(1):383–416, 2012. doi:\n10.1007/s10479-012-1248-5.\nStephen H. Friedberg, Arnold J. Insel, and Lawrence E. Spence. Linear algebra. Pearson Education, 4th ed. edition,\n2002.\nKenji Fukumizu, Francis R Bach, and Michael I Jordan. Dimensionality reduction for supervised learning with\nreproducing kernel hilbert spaces. Journal of Machine Learning Research, 5(Jan):73–99, 2004.\nKenji Fukumizu, Arthur Gretton, Xiaohai Sun, and Bernhard Sch¨olkopf. Kernel measures of conditional dependence.\nIn Advances in neural information processing systems, pages 489–496, 2008.\nNick Haber, Damian Mrowca, Stephanie Wang, Li F Fei-Fei, and Daniel L Yamins. Learning to play with intrinsically-\nmotivated, self-aware agents. In Advances in Neural Information Processing Systems, volume 31, 2018.\nYanjun Han, Jiantao Jiao, Tsachy Weissman, and Yihong Wu. Optimal rates of entropy estimation over Lipschitz balls.\nThe Annals of Statistics, 48(6):3228 – 3250, 2020.\nR. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, and\nYoshua Bengio.\nLearning deep representations by mutual information estimation and maximization.\nIn 7th\nInternational Conference on Learning Representations, ICLR, 2019.\nRein Houthooft, Xi Chen, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational\ninformation maximizing exploration. In Advances in Neural Information Processing Systems, volume 29, 2016.\nBo Jiang and Yu-Hong Dai. A framework of constraint preserving update schemes for optimization on stiefel manifold.\nMath. Program., 153(2):535–575, 2015.\nKirthevasan Kandasamy, Akshay Krishnamurthy, Barnab¨ys P´oczos, Larry Wasserman, and James M. Robins. Non-\nparametric von mises estimators for entropies, divergences and mutual informations. In Proceedings of the 28th\nInternational Conference on Neural Information Processing Systems - Volume 1, page 397–405, 2015.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\nIvan Kobyzev, Simon J.D. Prince, and Marcus A. Brubaker. Normalizing ﬂows: An introduction and review of current\nmethods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):3964–3979, 2021.\nNiklas Koenen, Marvin N. Wright, Peter Maass, and Jens Behrmann. Generalization of the change of variables formula\nwith applications to residual ﬂows. In ICML Workshop on Invertible Neural Networks, Normalizing Flows, and\nExplicit Likelihood Models, 2021.\nDaphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. The MIT Press,\nCambridge, MA, 2009.\nAlexander Kraskov, Harald St¨ogbauer, and Peter Grassberger. Estimating mutual information. Phys. Rev. E, 69, 2004.\nDavid McAllester and Karl Stratos. Formal limitations on the measurement of mutual information. In Proceedings\nof the Twenty Third International Conference on Artiﬁcial Intelligence and Statistics, volume 108, pages 875–884.\nPMLR, 2020.\nSean McGregor, Rachel Houtman, Claire Montgomery, Ronald Metoyer, and Thomas G. Dietterich. Factoring\nExogenous State for Model-Free Monte Carlo. arXiv 1703.09390, 2017.\n49\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nKevin R. Moon, Kumar Sricharan, and Alfred O. Hero. Ensemble estimation of mutual information. In 2017 IEEE\nInternational Symposium on Information Theory (ISIT), pages 3030–3034, 2017.\nXuanLong Nguyen, Martin J Wainwright, and Michael Jordan. Estimating divergence functionals and the likelihood\nratio by penalized convex risk minimization. In Advances in Neural Information Processing Systems, volume 20,\n2007.\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. F-gan: Training generative neural samplers using variational\ndivergence minimization. In Proceedings of the 30th International Conference on Neural Information Processing\nSystems, page 271–279, 2016.\nJunhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional video prediction\nusing deep networks in atari games. In Proceedings of the 28th International Conference on Neural Information\nProcessing Systems - Volume 2, page 2863–2871, 2015.\nGeorg Ostrovski, Marc G. Bellemare, A¨aron van den Oord, and R´emi Munos. Count-based exploration with neural\ndensity models. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, page\n2721–2730, 2017.\nKeiran Paster, Sheila A. McIlraith, and Jimmy Ba. Planning from pixels using inverse dynamics models. In International\nConference on Learning Representations, 2021.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin\nRaison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch:\nAn imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems\n32, pages 8024–8035, 2019.\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised\nprediction. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, page 2778–2787,\n2017.\nJudea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann\nPublishers Inc., San Francisco, CA, USA, 1988.\nJudea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, USA, 2nd edition, 2009.\nBen Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual\ninformation. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 5171–5180.\nPMLR, 2019.\nMartin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons,\nInc., New York, NY, USA, 1st edition, 1994.\nAntonin Rafﬁn, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-\nbaselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1–8,\n2021.\nKate Rakelly, Abhishek Gupta, Carlos Florensa, and Sergey Levine. Which mutual-information representation learning\nobjectives are sufﬁcient for control? In Advances in Neural Information Processing Systems, volume 34, pages\n26345–26357, 2021.\nDanilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In Proceedings of the 32nd\nInternational Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages\n1530–1538. PMLR, 2015.\nShashank Singh and Barnab´as P´oczos. Nonparanormal information estimation. In Proceedings of the 34th International\nConference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3210–3219,\n2017.\nMatthew J. Sobel. The Variance of Discounted Markov Decision Processes. Journal of Applied Probability, 19(4):\n394–802, 1982.\nYuhang Song, Jianyi Wang, Thomas Lukasiewicz, Zhenghua Xu, Shangtong Zhang, Andrzej Wojcicki, and Mai Xu.\nMega-reward: Achieving human-level play without extrinsic rewards. Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, 34(04):5826–5833, 2020.\nP. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press, 2nd edition, 2000.\nE. Stiefel. Richtungsfelder und fernparallelismus in n-dimensionalen mannigfaltigkeiten. Commentarii Mathematici\nHelvetici, 8(1):305–353, 1935.\n50\nReinforcement Learning with Exogenous States and Rewards\nA PREPRINT\nRichard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA,\n1st edition, 1998.\nValentin Thomas, Emmanuel Bengio, William Fedus, Jules Pondard, Philippe Beaudoin, Hugo Larochelle, Joelle\nPineau, Doina Precup, and Yoshua Bengio. Disentangling the independently controllable factors of variation by\ninteracting with the world. CoRR, abs/1802.09484, 2018.\nAndrey N. Tikhonov and Vasiliy Y. Arsenin. Solutions of ill-posed problems. V. H. Winston & Sons, Washington, D.C.:\nJohn Wiley & Sons, New York, 1977.\nJames Townsend, Niklas Koep, and Sebastian Weichwald. Pymanopt: A python toolbox for optimization on manifolds\nusing automatic differentiation. Journal of Machine Learning Research, 17(137):1–5, 2016.\nL.W. Tu. An Introduction to Manifolds. Universitext. Springer New York, 2010.\nA¨aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR,\nabs/1807.03748, 2018. URL http://arxiv.org/abs/1807.03748.\nMengjiao Yang, Dale Schuurmans, Pieter Abbeel, and Oﬁr Nachum. Dichotomy of Control: Separating What You Can\nControl from What You Cannot. arXiv, 2210.13435(v1):1–19, 2022. URL http://arxiv.org/abs/2210.13435.\nXianli Zeng, Yingcun Xia, and Howell Tong. Jackknife approach to the estimation of mutual information. Proceedings\nof the National Academy of Sciences, 115(40):9956–9961, 2018.\n51\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2023-03-22",
  "updated": "2023-03-22"
}