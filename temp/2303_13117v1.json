{
  "id": "http://arxiv.org/abs/2303.13117v1",
  "title": "RLOR: A Flexible Framework of Deep Reinforcement Learning for Operation Research",
  "authors": [
    "Ching Pui Wan",
    "Tung Li",
    "Jason Min Wang"
  ],
  "abstract": "Reinforcement learning has been applied in operation research and has shown\npromise in solving large combinatorial optimization problems. However, existing\nworks focus on developing neural network architectures for certain problems.\nThese works lack the flexibility to incorporate recent advances in\nreinforcement learning, as well as the flexibility of customizing model\narchitectures for operation research problems. In this work, we analyze the\nend-to-end autoregressive models for vehicle routing problems and show that\nthese models can benefit from the recent advances in reinforcement learning\nwith a careful re-implementation of the model architecture. In particular, we\nre-implemented the Attention Model and trained it with Proximal Policy\nOptimization (PPO) in CleanRL, showing at least 8 times speed up in training\ntime. We hereby introduce RLOR, a flexible framework for Deep Reinforcement\nLearning for Operation Research. We believe that a flexible framework is key to\ndeveloping deep reinforcement learning models for operation research problems.\nThe code of our work is publicly available at https://github.com/cpwan/RLOR.",
  "text": "RLOR: A FLEXIBLE FRAMEWORK OF DEEP REIN-\nFORCEMENT LEARNING FOR OPERATION RESEARCH\nChing Pui WAN\nLenovo Machine Intelligence Center\nHong Kong\ncpwandeep@gmail.com\nTung LI\nLenovo Machine Intelligence Center\nHong Kong\ntonyli537@gmail.com\nJason Min WANG\nLenovo Machine Intelligence Center\nHong Kong\njasonwangm@connect.ust.hk\nABSTRACT\nReinforcement learning has been applied in operation research and has shown\npromise in solving large combinatorial optimization problems. However, exist-\ning works focus on developing neural network architectures for certain problems.\nThese works lack the ﬂexibility to incorporate recent advances in reinforcement\nlearning, as well as the ﬂexibility of customizing model architectures for opera-\ntion research problems. In this work, we analyze the end-to-end autoregressive\nmodels for vehicle routing problems and show that these models can beneﬁt from\nthe recent advances in reinforcement learning with a careful re-implementation\nof the model architecture. In particular, we re-implemented the Attention Model\nand trained it with Proximal Policy Optimization in CleanRL, showing at least 8\ntimes speed up in training time. We hereby introduce RLOR, a ﬂexible frame-\nwork for Deep Reinforcement Learning for Operation Research. We believe that\na ﬂexible framework is key to developing deep reinforcement learning models\nfor operation research problems. The code of our work is publicly available at\nhttps://github.com/cpwan/RLOR.\n1\nINTRODUCTION\nPointer Network (Vinyals et al., 2015) is a milestone work of applying neural networks in combi-\nnatorial optimization problems. It enabled dynamic input size and permutation invariance of input\nin the neural networks. In other words, we can feed a set to a neural network. PN+RL (Bello et al.,\n2019) is another milestone work. It enabled training neural networks with reinforcement learning\n(RL) with REINFORCE algorithm, instead of requiring expensive ground truths from solvers for su-\npervised learning. Since then, the REINFORCE algorithm (but rarely other RL algorithms) has been\nused in subsequent works for vehicle routing problems, including Order-invariant PN+ RL (Nazari\net al., 2018), Attention Model (Kool et al., 2019), and POMO (Kwon et al., 2020). Stemming from\nthe same milestone works, DQN was preferred for graph problems, such as in S2V-DQN (Dai et al.,\n2017), ECO-DQN (Barrett et al., 2020), and GCOMB (Manchanda et al., 2020). These streams of\nwork focused on improving model architectures instead of exploring more advanced RL algorithms.\nOn the other hand, the RL community has been growing. Several RL platforms have been pro-\nposed for academic research and industrial applications, including StableBaselines3 (Rafﬁn et al.,\n2021), RLlib(Liang et al., 2017), DI-Engine(Contributors, 2021), Tianshou(Weng et al., 2022), and\nCleanRL(Huang et al., 2022b). These RL platforms have their design philosophies and different\nlevels of abstraction to accommodate numerous RL algorithms. Nevertheless, they share several\nsimilarities. In contrast to the complicated model architectures used for vehicle routing problems,\nan MLP model architecture and a 1-d vector input are assumed in most of the RL platforms. More-\nover, REINFORCE (or policy gradient), the most popular algorithm for end-to-end vehicle routing\nproblems, was not implemented in most of these RL platforms. It was claimed by their develop-\n1\narXiv:2303.13117v1  [math.OC]  23 Mar 2023\ners that the 31 years old REINFORCE algorithm (Williams, 1992) usually does not perform well\ncompared to the recent RL algorithms. The disentanglement has been demonstrated between the\nadvances in model architecture and RL algorithms.\nA natural question arises: can we leverage the advanced algorithms in the RL platforms for ve-\nhicle routing problems? In this work, we will demonstrate a Yes to this question. However, it is\na challenging task, due to two major reasons: compatibility and efﬁciency. In the RL platforms,\nthey accept agents built with an MLP model, a CNN model, or even an RNN model, but none of\nthem has considered the attention model in their design. (DI-Engine (Contributors, 2021) imple-\nments DecisionTransformer (Chen et al., 2021) but it is considered as an algorithm instead of an\nagent building block) As a result, it would require a huge effort to modify the RL platform to allow\nnested observations, dynamic input size, and management between hidden states. Eventually, the\nmodel architecture can be trained in the RL platform after ﬁxing the compatibility issue. However,\nit still suffers from the efﬁciency issue. For instance, the RL platforms have their own environment\nAPIs and there are different levels of overhead in data transformation and device communications\n(between CPU/GPU). We performed experiments on several RL platforms and ﬁnd that CleanRL\n(Huang et al., 2022b) has the lowest overhead and thus the highest efﬁciency.\nWe introduce the RLOR framework, which consists of four parts: model, algorithm, environment,\nand search. The model describes the neural network model architecture. Our default model is\ndeveloped and refactored from the Attention Model (Kool et al., 2019). The algorithm describes the\nRL algorithm. We adapted the Proximal Policy Optimization (PPO) algorithm from CleanRL to our\nproblems. The environment describes the RL environments. We followed the protocol of the OpenAI\nGym (Brockman et al., 2016) when deﬁning RL environments for the operation research problems.\nThe search deﬁnes the decoding strategies of the neural network, such as greedy or sampling.\nOur major contributions are in three-folds:\n1. As far as we know, it is the ﬁrst work to incorporate end-to-end vehicle routing model in\nmodern RL platforms\n2. It speeds up the training of Attention Model by 8 times (e.g., 25 hours →3 hours)\n3. It introduces a ﬂexible framework for developing model, algorithm, environment, and\nsearch for operation research\nIn Section 2, we will discuss the related works. In Section 3, 4,5, 6, we will discuss how we solve\nthe compatibility issues and integrate model, algorithm, environment, and search for OR models in\nRL platforms. In Section 7, we will discuss how we solve the efﬁciency issues of the OR model in\nRL platforms. In Section 8, we will discuss the results of our experiments. We will refer “model”\nas the neural network and “OR models” as neural networks applied to operation research problems.\nWe will refer “trajectory” as the sequence of actions and states until a given number of steps while\na “rollout” is a complete trajectory.\n2\nRELATED WORK\nSupervised learning methods.\nEarlier deep learning approaches to solving combinatorial opti-\nmization problems mostly featured supervised learning. Pointer Network (Vinyals et al., 2015) for-\nmulated the combinatorial optimization problem as a sequence-to-sequence problem and predicted\nthe target sequence obtained from a numerical solver. Later works (Joshi et al., 2019; Li et al., 2018)\nemployed Graph Neural Network to predict the adjacency matrix of the optimal solution and per-\nformed searches over the adjacency matrix, showing better performance. However, their supervised\nlearning natures implied expensive training set collection. The resulting model also had limited gen-\neralization power. Generalized GNN (Fu et al., 2020) and DPDP (Kool et al., 2021) tried to address\nthe generalization issues with GNN model by introducing advanced search techniques (Monte Carlo\nTree Search, dynamic programming). However, the analysis from B¨other et al. (2022) suggested\nthat the performance of the supervised learning model may (in general) be contributed by the search\nroutine instead of the model architecture. Therefore, we turn our attention to the deep RL approach.\n2\nnode's features\nglobal features\nNode 0\nNode 1\nNode n\nstatic dynamic\nEncoder\nDynamic\nembedding\nContext\nDecoder\nActor\nCritic\nAction\nValue\nBackbone\nStatic\nembedding\nFigure 1: The model architecture (Backbone, Actor, and Critic) of the refactored Attention Model.\nThe neural networks in blue represent MLPs while those in purple represent multi-head attention\nlayers.\nDeep RL methods.\nThere are two streams of approaches for applying RL in solving combinato-\nrial optimization problems: the construction method, and the improvement method. Construction\nmethods, such as PN+RL (Bello et al., 2019), Attention Model (Kool et al., 2019), POMO (Kwon\net al., 2020), constructed the solution step by step. On the other hand, the improvement methods\nsuch as Learning Improvement Heuristics (Wu et al., 2019), (Lu et al., 2020), NeuRewritter (Chen\n& Tian, 2019), DACT (Ma et al., 2021), started with a complete solution and predicted the location\nfor local rewriting to search for a better solution. The improvement methods were much slower than\nthe construction methods but could obtain a better solution. Instead of predicting the local rewriting\nwith a neural network, eMagic (Ouyang et al., 2021) performed local search (a heuristic) to ﬁnd bet-\nter rollouts during the RL training of their construction method. There were also efforts to combine\ntree search techniques from constraint programming in the work of Cappart et al. (2021). Readers\ninterested in the recent advances in neural combinatorial optimization can refer to the survey blog\nof Joshi & Anand (2022).\nRL libraries.\nRL platforms provide training pipelines for a variety of RL algorithms. Popular RL\nplatforms include StableBaselines3 (Rafﬁn et al., 2021), RLlib (Liang et al., 2017), DI-Engine (Con-\ntributors, 2021), Tianshou (Weng et al., 2022), and CleanRL (Huang et al., 2022b). In our work, we\nemploy the PPO algorithm provided by these RL platforms. Readers interested in the implemen-\ntation of PPO can refer to the blog of Huang et al. (2022a). On the other hand, we reformulated\nOR problems into RL environments so that they can communicate with RL algorithms through the\nenvironment API. OpenAI Gym (Brockman et al., 2016) provides standardized API deﬁning RL en-\nvironments and is supported in most of the RL platforms. OR-Gym (Hubbs et al., 2020) implements\nRL environments for a couple of small-scale operation research problems. Graphenv (Biagioni et al.,\n2022) implements the RL environment for travelling salesman problem (TSP) while conforming to\nRLlib’s environment API.\n3\nMODEL\nWe performed detailed code reviews for the Pointer Network (Vinyals et al., 2015) and the Attention\nModel (Kool et al., 2019). The pseudocodes for these model architectures can be found in Appendix\nB. We refactored both models and upgraded them to the latest PyTorch 1.13. In this section, we will\nfocus on the Attention Model, which we found easier to integrate into RL platforms.\nThe overview of the refactored Attention Model can be found in Figure 1. We will use the capac-\nitated vehicle routing problem (CVRP) as an example to illustrate the workﬂow of the refactored\nAttention Model. The node-wise static features (coordinates of customers and depots) and the node-\nwise dynamic features (demands of customers) are projected to the static embedding and dynamic\nembedding independently for each node. The static embeddings of different nodes are integrated in\nthe encoder. The dynamic embedding and the encoder output are concatenated and serve the key\nand value of the decoder. The global features (the current load of the vehicle) are projected to a\n3\nstate\nUpdate inernal states\naction\nencoder\ndecoder\n(a) OR model pipeline\nstate\nstep\naction\nencoder\ndecoder\nRL environment\nupdate with observations\n(b) RL platform pipeline\nFigure 2: Comparison of action sampling pipelines between OR model and RL platform. The OR\nmodel samples action from the decoder and updates the internal states directly until all steps in a\nrollout are completed. In contrast, RL platforms sample action step-by-step during the interaction\nwith the RL environment and require a complete forward pass of the neural network.\nlatent representation of the global context and serve the query of the decoder. The decoder predicts\na probability distribution of the next action (which node to visit next) along with the latent represen-\ntation of the current environment state. The original implementation of Attention Model Kool et al.\n(2019) concluded the model at the decoder. In our refactored version, we add additional modules —\nActor and Critic to better conform to the design of RL algorithms.\nBackbone. We regard the aforementioned steps from input embedding to the decoder as the back-\nbone. It is responsible for most of the computation. The backbone provides the probability distribu-\ntion for the actor model and latent representation of the environment state to the critic model.\nActor model. The actor model selects an action from the probability distribution and sends it to the\nRL environment. The RL environment then returns the next observation. The state (node’s features\nand global features) is updated accordingly and is used for the next iteration.\nCritic model. The critic model predicts the value of the current state (i.e., the average reward we\ncan claim starting from the current state). To predict the value effectively, we need to feed the critic\nmodel with sufﬁcient information about the state. In particular, we feed the critic model with the\nglimpse of the last attention layers of the decoder.\nOur reformulation of the Attention Model provides a more compatible interaction among the neural\nnetwork model, the RL algorithm, and the RL environment.\n3.1\nADAPTING OR MODELS TO RL PLATFORMS\nSince the RL platforms employ step-wise sampling, instead of rollout-wise sampling in OR models,\nwe need to modify the OR models. In Figure 2, we illustrate the difference in the pipelines. In RL\nplatforms, a forward pass of the entire neural network model is performed and only the action can\nbe used to update the next state. This is in contrast to OR models, where the internal state of the\nneural network can also affect the next prediction.\nPointer Network. The step-wise sampling of environment interaction makes it difﬁcult to imple-\nment Pointer Network in RL platforms. In Pointer Network, the hidden state of the LSTM layers in\nthe decoder is used to pass information across steps in a rollout while the output of the encoder is\nreused for each of the decoding steps. Some RL platforms (RLlib, Tianshou) support RNN models\nby passing the detached hidden state as additional information along with the observations, allowing\nthe decoder to be trained. However, if we also treat the encoder output as additional information,\nthe gradient would not be back-propagated to the encoder. Alternatively, some RL platforms (Tian-\nshou) allow passing a non-detached hidden state, but they do not guarantee that hidden states can be\nconsumed in the same iteration. As a result, the non-detached encoder output may be consumed in\nlater iterations when the model has already been updated, resulting in a runtime error in PyTorch.\nAttention Model. The same difﬁculty exists in implementing the Attention Model in the RL plat-\nforms. However, there is a workaround — with a sacriﬁce of efﬁciency. In our refactored Attention\nModel, the decoder does not depend on its previous hidden states. Instead, it can obtain all step-wise\ninformation from the state extracted from the RL environment observation. Hence, for each RL en-\nvironment step, we can perform a feed-forward for the entire refactored Attention Model as shown\nin Figure 2b. Each component of the Attention Model can be trained with this paradigm. However,\nthe same encoder output is recomputed every time. In most of the RL platforms, it is unavoidable to\nmake this trade-off in order to enable the training of the Attention Model.\n4\nOR models\nRL platforms\nAlgoritms\nREINFORCE\nA2C, PPO, DQN, ...\nUnit of sample\na full rollout {s0, π0, s1, ..., πT , sT +1}\none step st, πt, st+1\nActor predicts...\nn actions in one go\n1 action each time\nCritic predicts...\nnot used, baseline is derived from previous rollouts\nthe value of the current state\nTable 1: Comparison of the RL algorithm used in OR models and RL platforms.\n4\nALGORITHM\n4.1\nRL ALGORITHMS IN OR MODELS\nIn preceding works for vehicle routing problems, REINFORCE algorithm was used. In RL plat-\nforms, the REINFORCE algorithm is more often referred as Policy Gradient (PG). The goal of\npolicy gradient with respect to an environment instance s is to maximize the expected return\nJ(θ|s) = Eπ∼pθ(.|s)[L(π|s)]\n(1)\nwhere θ is the neural network parameter, π is the policy (in OR model context, the complete se-\nquence of actions) drawn from the actor pθ(.|s), and L(π|s) is the return of executing policy π on\nthe environment instance s.\nThe REINFORCE algorithm derived the gradient of the expected return as the following:\n∇θJ(θ|s) = Eπ∼pθ(.|s)[(L(π|s) −b(s))∇θ log(pθ(π|s))].\n(2)\nwhere b(s) is the baseline. Intuitively, the return L(π|s) is compared against the baseline b(s). The\nbetter (relatively) the return L(π|s), the higher the log-likelihood the policy log(pθ(π|s)) contributes\nto the gradient. Theoretically, a good choice of baseline b(s) reduces the variance of the gradient\nand potentially leads to faster convergence. The preceding works for vehicle routing problems de-\nsigned different baselines for the policy gradient. Interested readers may refer to Appendix A for\nthe modiﬁcations they made.\n4.2\nRL ALGORITHMS IN RL PLATFORMS\nThe formulation of policy gradient and its variants in RL platforms follows a similar derivation from\nthe REINFORCE algorithm. However, the policy π in Equation 1 is explicitly broken down into\na sequence of actions π = π0, π1, ..., πT and the expectation in Equation 2 is then with respect\nto each of these actions. The return L(π|s) is broken down to the sum of rewards r(πt|st). The\nL(π|s) −b(s) term is replaced by a step-wise estimate L(πt|st) −b(st), in which L(πt|st) is the\nsum of future rewards and b(st) can be predicted by the critic. As an illustration of the algorithmic\ndifference, we may compare Equation 2 with the gradient computed from the step-wise estimate in\nRL platforms:\n∇θJ(θ|s) = Et∼{0...T },πt∼pθ(.|st)[(L(πt|st) −b(st))∇θ log(pθ(πt|st))].\n(3)\nVariants of policy gradient use different step-wise estimate. For example, A2C uses the advantage\nA(πt|st) computed from the rewards and critic predictions; PPO further applies clipping on the\nadvantage with a regulation on the deviation of the policy. The comparison of RL algorithms in\nOR models and RL platforms is summarized in Table 1. We accommodated the pipeline of the OR\nmodel to that of the RL platforms in Section 3.1.\n5\nENVIRONMENT\nThe primary settings and constraints of the vehicle routing problem environments are referenced\nfrom Attention Model paper (Kool et al., 2019), and adapted into OpenAI Gym (Brockman et al.,\n2016). The OpenAI Gym environment interface is supported in most RL platforms. The environ-\nment for each problem needs to be carefully designed such that the necessary information would be\nincluded in the observation returned by the OpenAI Gym environment, and a wrapper is required to\ntransform the observation into the schema that the RL platforms intend to receive.\n5\n5.1\nINTERFACE\nThe following steps of the OR model involve interactions with the environment:\n• Initialize the environment for the problem instance s.\n• Update the environment with the selected action πππt.\n• Get whether the policy {πππt} has ﬁnished.\n• Context needs to get the global information, such as remaining vehicle capacity.\n• DynamicEmbedding needs to get the dynamic node features.\n• maskt needs to get the feasible actions.\nAn RL environment for vehicle routing problems should provide the following interfaces:\nclass RequiredEnv():\ndef __init__(self, s: problem_instance): ...\ndef step(self, action): ...\ndef get_global_context(self): ...\ndef get_dynamic_node_features(self): ...\ndef get_mask(self): ...\ndef all_finished(self): ...\nCompare to the OpenAI Gym environment interface:\nclass GymEnv():\ndef __init__(self, arg1, arg2, ...):\n# What action can be taken?\nself.action_space = ...\n# What information can we get from the problem?\nself.observation_space = ...\ndef step(self, action):\n...\nreturn observation, reward, done, info\ndef reset(self):\n...\nreturn observation\nTo translate the GymEnv to our RequiredEnv,\nwe need to store the results of the\nGymEnv.step(action).\nWhen the RequiredEnv.get_xxxx function is called,\nwe extract the corresponding information from the stored observation.\nWhen the\nRequiredEnv.all_finished function is called, we extract the corresponding information\nfrom the stored done.\n5.2\nADAPTATION TO RL LIBRARIES\nHere, we give an example to adapt the RL libraries interface to the Attention Model. We use\nCleanRL as an example for demonstration purpose. In CleanRL, the observations from the envi-\nronment is referred as next_obs and is passed to the model in each step of an episode. The\nnext_obs is a dict of tensors. In particular, for TSP, the next_obs has the following contents:\nnext_obs.keys()\nshape\ntype\ndescriptions\n'observations'\n[B,n,2]\nfloat\ncoordinates of nodes\n'action_mask'\n[B,n]\nfloat\n0: forbidden; 1: permitted\n'first_node_idx'\n[B]\nint\nthe index of the ﬁrst node visited\n'last_node_idx'\n[B]\nint\nthe index of the previous node visited\n'is_initial_action'\n[B]\nbool\nwhether the current step is the ﬁrst step\n6\nOn the other hand, the Attention Model for TSP requires a couple of data members and functions\nfrom a state object:\nRequired data/ functions\nshape\ntype\ndescriptions\nstate.get_mask()\n[B,1,n]\nbool\nTrue: forbidden, False: permitted\nstate.first_a\n[B,1]\nint\nthe index of the ﬁrst node visited\nstate.get_current_node()\n[B,1]\nint\nthe index of the previous node visited\nstate.is_initial_action\n[B]\nbool\nwhether the current step is the ﬁrst step\nHence, we deﬁne a wrapper to translate from the nested observations dict (next_obs : dict)\ncollected by CleanRL to the object (state : object) that our model required.\n# wrapper for the problem\nclass stateWrapper:\n\"\"\"\nFrom dict of numpy arrays\nto an object that supplies PyTorch tensors.\n\"\"\"\ndef __init__(self, next_obs, device, problem):\nself.device = device\nself.states = {k: torch.tensor(v, device=self.device)\nfor k, v in next_obs.items()}\nif problem == 'tsp':\nself.is_initial_action = \\\nself.next_obs[\"is_initial_action\"].to(torch.bool)\nself.first_a = self.next_obs[\"first_node_idx\"]\nelif problem == 'cvrp':\ninput = {'loc': self.next_obs['observations'],\n'depot': self.next_obs['depot'].squeeze(-1),\n'demand': self.next_obs['demand']}\nself.states['observations'] = input\nself.VEHICLE_CAPACITY = 0\nself.used_capacity = -self.next_obs[\"current_load\"]\ndef get_current_node(self):\nreturn self.states[\"last_node_idx\"]\ndef get_mask(self):\nreturn (1 - self.states[\"action_mask\"]).to(torch.bool)\n# Inside the Attention Model\nstate = stateWrapper(next_obs)\n6\nSEARCH\nDuring inference, we are searching for a solution with the probability distribution predicted by the\nAttention Model. In RL platforms, the action of an agent is mostly selected in two ways: greedy\n(choosing the highest probability action), or sampling (selecting an action according to the proba-\nbility distribution). In contrast, some works on OR models borrow beam search from NLP (Nazari\net al., 2018) or ﬁne-tune the model on test instance with Efﬁcient Active Search (Hottung et al.,\n2022). In the work by Cappart et al. (2021), they treated the probability distribution as a heuristic\nand performed a tree search accordingly. In our implementation, we borrow the idea of POMO\n(Kwon et al., 2020) for searching. In POMO, it performed greedy rollouts with different starting\nnodes during inference. We denote this decoding strategy as Multi-Greedy.\n7\n7\nEFFICIENT TRAINING ON RL PLATFORM\nWe implemented our refactored Attention Model in 4 major RL platforms: DI-engine, RLlib, Tian-\nshou, and CleanRL. We did not implement it on StableBaselines3 due to engineering difﬁculties.\nWe found that CleanRL has the highest efﬁciency in environment step collection and the best con-\nvergence. The results will be discussed in Section 8. We found that even the best RL platform\ncould not beat the performance of the original implementation. We performed a detailed analysis\nand developed several ways to improve the training efﬁciency of our OR model on CleanRL.\n7.1\nREDUCING COMMUNICATION OVERHEAD\nWe found that the communication overhead between GPU/ CPU devices is one of the major over-\nheads in the RL platforms. For example, we proﬁled DI-engine and found that 30% of the com-\nputation was spent on data manipulation for observations and moving the data across devices. In\nCleanRL, we implement the RL environment with Numpy and convert the observations (in NumPy\narray) to PyTorch Tensor on GPU in the state wrapper. That is, data collected from the environment\nare moved to GPU only when it is needed for the forward pass. The ad-hoc conversion of data across\ndevices reduces the communication overhead and makes CleanRL the most efﬁcient RL platform\nfor our problems.\n7.2\nCACHED ENCODER\nIn Section 3.1, we mentioned that we have to sacriﬁce the efﬁciency of the OR model to accommo-\ndate the workﬂow of the RL platforms. To retain the efﬁciency, we modify the CleanRL to allow\ncaching of the encoder output. We enable cached encoder during both value bootstrapping and\nmodel training. The encoder output is computed once and is passed to the decoder for sampling\nthe action and values until the environment is done. We enforce that the encoder output has to be\nconsumed in the same iteration. The decoder is fed with the non-detached version of the encoder\noutput. The back-propagation is performed only when the forward pass for every step is done. In\nthis way, the encoder can be updated with the gradients computed from every environment step. We\nproﬁled the implementation and found that it could save ∼80% computation during forward pass\nby caching the encoder output.\n7.3\nLARGER BATCH\nWith the PPO algorithm, the training is more stable and we can use a larger learning rate with a\nlarger batch-size. In particular, we collect 1024 · 51 steps in a batch and train our model with 8\nminibatch (minibatch-size = 6528) for the 50-nodes TSP. A larger learning rate of 10−3 can be used\nwithout weight decay. It takes 6.9 GiB GPU memory. It may scale further but we want to keep\nit compact so that it can be ﬁtted in an affordable GPU (for instance, those in the free-tier Colab\nnotebook).\n7.4\nPARALLEL DECODING\nIn Section 7.2, we share the encoder output across environment steps. In this section, we further\nspeed up computation by sharing the encoder output across different trajectories of the same RL\nenvironment instance. The dynamic nodes features and global context constitute the query in the\nattention layers of the decoder. The Attention Model only passes one query (which corresponds to\none trajectory) to the decoder per environment. We can pass multiple queries, each corresponding\nto different trajectories, to the decoder. By decoding multiple trajectories in parallel in the GPU,\nwe obtain better efﬁciency than decoding them separately. The pseudo-code of the attention layer\ncan be found in Appendix B.2. We replace the query q with a higher dimension tensor q. Note\nthat, during training, we sample the trajectories according to the probability distribution predicted\nby the decoder. This is in contrast to POMO (Kwon et al., 2020) in which different starting nodes\nare enforced.\n8\nMethod\nTSP50\nCVRP50\nLen.\nGap\nTime\nSteps\nLen.\nGap\nTime\nSteps\nOptimal\n5.69\n0.00%\n-\n-\n10.38\n0.00%\n-\n-\nKool et al. (2019) (PG)\n5.80\n1.93%\n25h\n6.5G\n10.98\n5.78%\n33h\n7.8G\nDI-Engine (PPO)\n5.90\n3.69%\n3d\n480M\n-\n-\n-\n-\nRLlib (PPO)\n6.15\n8.08%\n3d\n153M\n-\n-\n-\n-\nTianshou (PPO)\n6.19\n8.79%\n3d\n611M\n-\n-\n-\n-\nCleanRL (PPO)\n5.83\n2.46%\n3d\n1.7G\n-\n-\n-\n-\nOurs (PPO)\n5.79\n1.76%\n24h\n30G\n10.91\n5.11%\n24h\n21G\n+ Multi-Greedy\n5.71\n0.35%\n24h\n30G\n10.82\n4.23%\n24h\n21G\nTable 2: Experiment results across implementations and RL platforms. The second row refers to\nthe original implementation of the Attention Model. The middle rows refer to our implementation\non different RL platforms. The last two rows refer to our implementation with a bag of tricks. The\nRL algorithm in each implementation is indicated in the bracket. Experiments are done on 50-nodes\nTSP and 50-nodes CVRP environment.\nMethod\nTime\nEnv Steps\nPerformance\nKool et al. (2019) (PG)\n50m\n65M\n6.00\nDI-Engine (PPO)\n12h\n40M\n6.00\nRLlib (PPO)\n-\n-\n6.00\nTianshou (PPO)\n-\n-\n6.00\nCleanRL (PPO)\n3h\n70M\n6.00\nOurs (PPO)\n14m\n250M\n6.00\nTable 3: Time and environment steps required to reach 6.0 in route length (∼5% gap) in the 50-\nnodes TSP. The ﬁrst row refers to the original implementation of the Attention Model. The middle\nrows refer to our implementation on different RL platforms. The last row refers to our implemen-\ntation with a bag of tricks. The RL algorithm in each implementation is indicated in the bracket.\nRLlib and Tianshou did not attain the performance in a reasonable time.\n7.5\nENVIRONMENT VECTORIZATION\nWe implemented a bi-level vectorized environment. The ﬁrst level is the OpenAI Gym vector en-\nvironment consisting of multiple sub-environments. In each of these sub-environments, we imple-\nmented another layer of sub-environment — vectorized environment written in Numpy’s vectorized\noperations. For example, we can create a M × N bi-level vectorized environment, in which there\nare M RL environment instances and N trajectories for each of the instances. We proﬁled the im-\nplementation and found that the bi-level environment vectorization can bring a 10x speed up for\nenvironment steps collection for M = 1024, N = 50. More on the design of the bi-level environ-\nment vectorization can be found in Appendix C\n8\nEXPERIMENT RESULTS\nTraining.\nWe ﬁrst describe our experiment settings. We used the refactored Attention Model as\nour agent for all OR problems. We trained our agent with the PPO algorithm in RL platforms. The\ntraining was done with the Adam optimizer with a learning rate of η = 10−3 on a single Titan RTX\nGPU. Each epoch consists of 1024 randomly generated RL environment instances. For comparison,\nwe also trained the original implementation of the Attention Model from Kool et al. (2019). We\nfollowed its default setting and trained it for 100 epochs on the same GPU.\n9\nMethod\nTraining Time\nEnv Steps\nPerformance\nKool et al. (2019)\n25h\n6.5G\n5.80\nBaseline\n11d\n6G\n5.79\n+ Cached Encoder\n4d\n6G\n5.79\n+ Larger Batch\n2.5d\n6G\n5.79\n+ Parallel Decoding & Env Vectorization\n24h\n32G\n5.79\n+ Multi-Greedy\n3h\n5G\n5.79\nTable 4: Ablation study on different components of our framework. The ﬁrst row refers to the orig-\ninal implementation of Attention Model. The following rows refer to the designs in our framework.\nExperiments were done on the 50-nodes TSP.\nInference.\nFor each problem, the performance is measured on the 10000 test instances as being\ndone in Kool et al. (2019). The optimality gap is calculated with respect to the length of the optimal\nroute of the vehicle routing problem instance. Greedy search is the default decoding strategy for the\nAttention Model. For Multi-Greedy search, we perform n = 50 greedy searches with different start\nnodes and select the best trajectory.\n8.1\nATTENTION MODEL ON DIFFERENT RL PLATFORMS\nWe integrated our refactored Attention Model on several RL platforms and trained it with the PPO\nalgorithm provided in these RL platforms. The results are summarized in Table 2. We found that\nnone of these RL platforms were able to attain the performance of the original implementation of\nthe Attention Model. We speculate that the discrepancy between RL platforms and the original im-\nplementation originated from the efﬁciency of the model-environment interactions and data manipu-\nlations. In particular, CleanRL employed the most efﬁcient vector API (with the fastest environment\nstep collection among the RL platforms), and produced the best results among the RL platforms. We\nobserved a positive correlation between the number of environment steps and the ﬁnal performance\nacross the RL platforms. It indicated that the efﬁciency in environment step collection could be vital\nfor reaching a tighter optimality gap. Based on this assumption, we improved the training efﬁciency\non CleanRL with a bag of tricks which has been elaborated on in Section 7. With the improved\nimplementation, we were able to surpass the Attention Model (Kool et al., 2019) in both training\ntime and the optimality gap. Our approach collected 5 times more environment steps and reached\na better objective given at the same time. With Multi-Greedy search, we can obtain an even better\nperformance. We observed a similar performance boost for the CVRP50 instances.\nFrom another perspective, our approach can reach the same optimality gap with a shorter training\ntime. Table 3 reported the time required to reach 5% gap for the 50-nodes TSP. Among the others,\nour approach was the most efﬁcient and it took only 14 minutes for the training. Our approach is\nappealing in that it can train a sufﬁciently good OR model in a short training time. It can accelerate\nthe development of a new OR model. For instance, hyper-parameter tuning can be done in a shorter\ntime.\n8.2\nABLATION STUDY\nAs an ablation study, we performed experiments with CleanRL with different parts of our design.\nThe results are shown in Table 4. We can observe signiﬁcant improvements with the components.\nBy caching the encoder outputs, we were able to greatly reduce unnecessary re-computation during\ntraining. We proﬁled the implementation and found that 80% of the training time can be saved by\ncaching the encoder output. By using a larger batch with a higher learning rate, we could scale the\ntraining while ﬁtting the training inside a single GPU. By using parallel decoding and the bi-level\nenvironment vectorization, we were able to substantially increase the environment steps trained per\nsecond, boosting the collection efﬁciency of the environment observations. With these designs, our\nimplementation has already surpassed the original implementation of the Attention Model. Lastly,\nby employing Multi-Greedy search during inference, we were able to achieve the same perfor-\nmance in just 3 hours.\n10\nMethod\nTSP50\nCVRP50\nLen.\nGap\nTime\nSteps\nLen.\nGap\nTime\nSteps\nOptimal\n5.69\n0.00%\n-\n-\n10.38\n0.00%\n-\n-\nPOMO (PG)\n5.73\n0.70%\n24h\n61G\n10.84\n4.43%\n24h\n55G\n+ Multi-Greedy\n5.71\n0.32%\n24h\n61G\n10.60\n2.21%\n24h\n55G\nOurs (PPO)\n5.79\n1.76%\n24h\n30G\n10.91\n5.11%\n24h\n21G\n+ Multi-Greedy\n5.71\n0.35%\n24h\n30G\n10.82\n4.23%\n24h\n21G\nTable 5: Experiment results compared to POMO. The RL algorithm in each implementation is\nindicated in the bracket. Experiments are done on 50-nodes TSP and 50-nodes CVRP.\n8.3\nPOTENTIAL EXTENSION WITH POMO\nAs a comparison, we also trained the implementation from POMO (Kwon et al., 2020), a state-\nof-the-art construction method. We used equivalent model settings (e.g., 3 encoder layers) and\ntrained POMO for 24 hours. The result is reported in Table 5. We note that our method did not\noutperform POMO. In our design, we attempt to modularize the model, algorithm, environment, and\nsearch, in order to create a ﬂexible and comprehensive framework for developing deep RL model\nfor the operation research problem. We made a trade-off between ﬂexibility and the environment\nstep collection efﬁciency. For instance, we could implement the entire vectorized environment in\nPyTorch and obtain higher efﬁciency in environment step collection. Yet, it will incur burdens on\nﬁne-grained control over the RL environment. Nevertheless, we are still close behind the state-of-\nthe-art benchmark, with a clean one-page code deﬁning the environment, a one-page code for the\nalgorithm, and a refactored modularized neural network architecture.\nIn addition, we demonstrated that PPO is a viable algorithm for solving vehicle routing problems,\nas seen from its better performance with the Attention Model. Moving forward, our work can be\nfurther improved with more recent advances in OR models and RL algorithms. For instance, we only\nborrow the Multi-Greedy search strategies from POMO, yet its multi-start-nodes training strategy\nand inference augmentation strategy may also be helpful. We hope our work will serve as a new\navenue for future vehicle routing problem research.\n9\nCONCLUSION\nIn this paper, we presented RLOR, a ﬂexible framework of deep reinforcement learning for opera-\ntion research that leverages recent advances in reinforcement learning algorithms. RLOR provides\na comprehensive end-to-end deep learning model on a modern RL platform while improving the\ntraining efﬁciency of the original implementation of the Attention Model. We evaluated the perfor-\nmance of RLOR on the travelling salesman problem and capacitated vehicle routing problem. We\ndemonstrated the improvement of RLOR over the original implementation of the Attention Model,\nshowing the potential of the RLOR framework.\nREFERENCES\nThomas D. Barrett, William R. Clements, Jakob N. Foerster, and A. I. Lvovsky. Exploratory Com-\nbinatorial Optimization with Reinforcement Learning. Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, 34(04):3243–3250, 4 2020. ISSN 2374-3468. doi: 10.1609/AAAI.V34I04.\n5723. URL https://ojs.aaai.org/index.php/AAAI/article/view/5723.\nIrwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combina-\ntorial optimization with reinforcement learning. In 5th International Conference on Learning\nRepresentations, ICLR 2017 - Workshop Track Proceedings, 2019.\nDavid Biagioni, Charles Edison Tripp, Struan Clark, Dmitry Duplyakin, Jeffrey Law, and Peter C St\nJohn. graphenv: a Python library for reinforcement learning on graph search spaces. Journal of\nOpen Source Software, 7(77):4621, 2022.\n11\nMaximilian B¨other, Otto Kißig, Martin Taraz, Sarel Cohen, Karen Seidel, and Tobias Friedrich.\nWhat’s wrong with deep learning in tree search for combinatorial optimization. arXiv preprint\narXiv:2201.10494, 2022.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. OpenAI Gym, 2016.\nQuentin Cappart, Thierry Moisan, Louis Martin Rousseau, Isabeau Pr´emont-Schwarz, and Andre A.\nCire. Combining Reinforcement Learning and Constraint Programming for Combinatorial Op-\ntimization. AAAI 2021, 5A:3677–3687, 2021. URL https://github.com/qcappart/\nhybrid-cp-rl-solver.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision Transformer: Reinforcement Learning\nvia Sequence Modeling. Advances in Neural Information Processing Systems, 18:15084–15097,\n6 2021. ISSN 10495258. doi: 10.48550/arxiv.2106.01345. URL https://arxiv.org/\nabs/2106.01345v2.\nXinyun Chen and Yuandong Tian.\nLearning to perform local rewriting for combinatorial opti-\nmization.\nIn Advances in Neural Information Processing Systems, volume 32, 2019.\nURL\nhttps://github.com/facebookresearch/neural-rewriter.\nDI-engine\nContributors.\n{DI-engine:\nOpenDILab}\nDecision\nIntelligence\nEngine.\n\\url{https://github.com/opendilab/DI-engine}, 2021.\nHanjun Dai, Elias B Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial\noptimization algorithms over graphs. In Advances in Neural Information Processing Systems,\nvolume 2017-Decem, pp. 6349–6359, 2017.\nZhang-Hua Fu, Kai-Bin Qiu, and Hongyuan Zha. Generalize a Small Pre-trained Model to Arbi-\ntrarily Large {TSP} Instances. CoRR, abs/2012.1, 2020. URL https://arxiv.org/abs/\n2012.10658.\nAndr´e Hottung, Yeong-Dae Kwon, and Kevin Tierney. Efﬁcient Active Search for Combinatorial\nOptimization Problems. ICLR 2022, 2022. URL http://arxiv.org/abs/2106.05126.\nShengyi Huang, Rousslan Fernand Julien Dossa, Antonin Rafﬁn, Anssi Kanervisto, and\nWeixun Wang.\nThe 37 Implementation Details of Proximal Policy Optimization.\nIn ICLR\nBlog Track, 2022a.\nURL https://iclr-blog-track.github.io/2022/03/25/\nppo-implementation-details/.\nShengyi Huang, Rousslan Fernand, Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal\nMehta, and Jo˜ao G M Ara´ujo. CleanRL: High-quality Single-ﬁle Implementations of Deep Rein-\nforcement Learning Algorithms. Journal of Machine Learning Research, 23:1–18, 2022b. URL\nhttp://jmlr.org/papers/v23/21-1342.html.\nChristian D Hubbs, Hector D Perez, Owais Sarwar, Nikolaos V Sahinidis, Ignacio E Grossmann,\nand John M Wassick. OR-Gym: A Reinforcement Learning Library for Operations Research\nProblems, 2020.\nChaitanya K Joshi and Rishabh Anand. Recent Advances in Deep Learning for Routing Problems.\nIn ICLR Blog Track, 2022. URL https://iclr-blog-track.github.io/2022/03/\n25/deep-learning-for-routing-problems/.\nChaitanya K Joshi, Thomas Laurent, and Xavier Bresson. An efﬁcient graph convolutional network\ntechnique for the travelling salesman problem. arXiv preprint arXiv:1906.01227, 2019.\nWouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing problems! 7th\nInternational Conference on Learning Representations, ICLR 2019, 2019.\nWouter Kool, Herke van Hoof, Joaquim A S Gromicho, and Max Welling. Deep Policy Dynamic\nProgramming for Vehicle Routing Problems. CoRR, abs/2102.1, 2021. URL https://arxiv.\norg/abs/2102.11756.\n12\nYeong Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai Min.\nPOMO: Policy optimization with multiple optima for reinforcement learning. In Advances in\nNeural Information Processing Systems, volume 2020-Decem, 2020.\nZhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional\nnetworks and guided tree search. Advances in Neural Information Processing Systems, 2018-\nDecem(Nips):539–548, 2018. ISSN 10495258.\nEric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox, Ken Goldberg, Joseph E.\nGonzalez, Michael I. Jordan, and Ion Stoica. RLlib: Abstractions for Distributed Reinforcement\nLearning. 35th International Conference on Machine Learning, ICML 2018, 7:4768–4780, 12\n2017. doi: 10.48550/arxiv.1712.09381. URL https://arxiv.org/abs/1712.09381v4.\nHao Lu, Xingwen Zhang, and Shuang Yang. A Learning-Based Iterative Method for Solving Vehicle\nRouting Problems. In ICLR 2022, volume 3, pp. 1–15, 2020. URL https://openreview.\nnet/forum?id=BJe1334YDH.\nYining Ma, Jingwen Li, Zhiguang Cao, Wen Song, Le Zhang, Zhenghua Chen, and Jing Tang.\nLearning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer.\nNeurIPS 2021, 34:11096–11107, 12 2021.\nSahil Manchanda, Akash Mittal, Anuj Dhawan, Sourav Medya, Sayan Ranu, and Ambuj Singh.\nGCOMB: Learning budget-constrained combinatorial algorithms over billion-sized graphs. Ad-\nvances in Neural Information Processing Systems, 2020-Decem, 2020. ISSN 10495258.\nMohammadreza Nazari, Afshin Oroojlooy, Martin Tak´aˇc, and Lawrence V Snyder. Reinforcement\nlearning for solving the vehicle routing problem. In Advances in Neural Information Processing\nSystems, volume 2018-Decem, pp. 9839–9849, 2018.\nWenbin Ouyang, Yisen Wang, Paul Weng, and Shaochen Han. Generalization in Deep {RL} for\n{TSP} Problems via Equivariance and Local Search. CoRR, abs/2110.0, 2021. URL https:\n//arxiv.org/abs/2110.03595.\nAntonin Rafﬁn, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah\nDormann. Stable-Baselines3: Reliable Reinforcement Learning Implementations. Journal of\nMachine Learning Research, 22(268):1–8, 2021. URL http://jmlr.org/papers/v22/\n20-1364.html.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural\nInformation Processing Systems, volume 2015-Janua, pp. 2692–2700, 2015.\nJiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang\nSu, and Jun Zhu. Tianshou: A Highly Modularized Deep Reinforcement Learning Library. Jour-\nnal of Machine Learning Research, 23(267):1–6, 2022. URL http://jmlr.org/papers/\nv23/21-1127.html.\nRonald J. Williams.\nSimple statistical gradient-following algorithms for connectionist rein-\nforcement learning.\nMachine Learning 1992 8:3, 8(3):229–256, 5 1992.\nISSN 1573-0565.\ndoi: 10.1007/BF00992696. URL https://link.springer.com/article/10.1007/\nBF00992696.\nYaoxin Wu, Wen Song, Zhiguang Cao, Jie Zhang, and Andrew Lim.\nLearning Improvement\nHeuristics for Solving the Travelling Salesman Problem.\nCoRR, abs/1912.0, 2019.\nURL\nhttp://arxiv.org/abs/1912.05784.\n13\nA\nALGORITHMIC STRUCTURES\nIn this section, we will look at the algorithm structures of three works: PN+RL (Bello et al., 2019),\nAttention Model (Kool et al., 2019), and POMO (Kwon et al., 2020). Without speciﬁcation, the\ntrain function describes the algorithm in a single epoch.\nA.1\nBASIC ALGORITHM\n# REINFORCE Algorithm (basic)\ndef train(policy network pθ, training set S, batch size B):\nfor i in 1...B:\nsi = RandomInstance(S)\nπi = SampleRollout(si, pθ)\nb = UpdateBaseline(s, π)\n∇L =\n1\nB\nPB\ni=1(L(πi|si) −bi)∇θ log(pθ(πi|si))\nθ = GradientDescent(θ, ∇L)\nNote that, the same REINFORCE algorithm can be used to ﬁne-tune on a testing sample. It is\ndenoted as Active Search in PN+RL from Bello et al. (2019).\n# Fine tuning with Active Search\ndef finetune(policy network pθ, testing sample s, batch size B):\nfor i in 1...B:\nπi = SampleRollout(s, pθ) # roll for a single instance\nb = UpdateBaseline(s, π)\n∇L =\n1\nB\nPB\ni=1(L(πi|s) −bi)∇θ log(pθ(πi|s))\nθ = GradientDescent(θ, ∇L)\nA.2\nMODIFICATIONS\nWe describe the changes different works have made.\n# Attention model : new baseline\ndef train(policy network pθ, training set S, batch size B,\n+\nsignificance α):\nfor i in 1...B:\nsi = RandomInstance(S)\nπi = SampleRollout(si, pθ)\n-\nb = UpdateBaseline(s, π)\n+\nb = UpdateBaseline(s, π, pθBL)\n∇L =\n1\nB\nPB\ni=1(L(πi|si) −bi)∇θ log(pθ(πi|si))\nθ = GradientDescent(θ, ∇L)\n+\nif OneSidedPairedTest(pθ, pθBL) < α: # pθ is better than pθBL\n+\nθBL = θ\n# POMO : new policy sampler\ndef train(policy network pθ, training set S, batch size B,\n+\nnumber of start nodes N):\nfor i in 1...B:\nsi = RandomInstance(S)\n-\nπi = SampleRollout(si, pθ)\n14\n+\nα1\ni , ..., αN\ni\n= SelectStartNodes(si)\n+\nπ1\ni , ..., πN\ni\n= SampleRollout(si, pθ, {αi,j})\nb = UpdateBaseline(s, π)\n-\n∇L =\n1\nB\nPB\ni=1(L(πi|si) −bi)∇θ log(pθ(πi|si))\n+\n∇L =\n1\nNB\nPB\ni=1\nPN\nj=1(L(πj\ni |si) −bi)∇θ log(pθ(πj\ni |si))\nθ = GradientDescent(θ, ∇L)\nA.3\nBASELINES\nFor different works, they use different baselines.\n# PN+RL\ndef UpdateBaseline(s, π):\nfor i in 1...B:\nbi = criticNetwork(si)\nreturn b\n# Attention model\ndef UpdateBaseline(s, π, pθBL):\nfor i in 1...B:\nπBL\ni\n= GreedyRollout(si, pθBL) # pθBL is the best model so far\nbi = L(πBL\ni\n|si)\nreturn b\n# POMO\ndef UpdateBaseline(s, π):\nfor i in 1...B:\nbi =\n1\nN\nPN\nj=1(L(πj\ni |si)) # N rollouts for each πi\nreturn b\nA.4\nDECODING STRATEGIES\nIn this subsection, we denote πt to be the action chosen at step t.\n# Greedy strategy\ndef GreedyRollout(instance s, policy network pθ):\nπ = []\nwhile not Done(s, π):\nπnext = argmaxi pθ(i|s, π)\nπ.append(πnext)\nreturn π\ndef inference(instance s, policy network pθ):\nπ = GreedyRollout(s,pθ)\nreturn π\n# Sampling strategy\ndef SampleRollout(instance s, policy network pθ):\nπ = []\nwhile not Done(s, π):\nπnext = Sample(pθ(.|s, π))\nπ.append(πnext)\nreturn π\n15\ndef inference(instance s, policy network pθ, sampling size N):\nfor i in 1...N:\nπi = SampleRollout(s,pθ)\nπbext = argminπi L(πi|s)\nreturn πbest\n# Beam Search\ndef inference(instance s, policy network pθ, beam size w):\nbeam = [([],0)] # [seq, score = P\nπt log pθ(πt|s, π1:t−1)]\nfinalBeam = []\nwhile beam is not empty:\nexpansion = []\nfor (π, score) in beam:\nif Done(s,π):\nfinalBeam.append(π)\ncontinue\nfor j in pθ(.|s, π)>0: # only feasible actions\nexpansion.append((π+[j], score + log pθ(πt = j|s, π)))\nbeam = TopK(expansion)\nπbext = argminπ∈finalBeam L(π|s)\nreturn πbext\nAs mentioned in Section A.1, the model can be ﬁne-tuned during inference with Active Search.\n# Active Search\ndef finetune(policy network pθ, testing sample s, batch size B):\nfor i in 1...B:\nπi = SampleRollout(s, pθ)\nb = UpdateBaseline(s, π)\n∇L =\n1\nB\nPB\ni=1(L(πi|s) −bi)∇θ log(pθ(πi|s))\nθ = GradientDescent(θ, ∇L)\ndef inference(policy network pθ, testing sample s, batch size B,\nepochs T):\nfor t in 1...T:\nfinetune(pθ,s,B)\nπbext = best policy encounted during fine-tuning\nreturn πbext\nWe can integrate the end2end learning model with classical exact algorithms and use our policy\nnetwork output as a heuristic. This converts the end2end learning model to an exact model (we can\nrun exhaustive search under the classical exact algorithm framework).\ndef DepthFirstBranchAndBound(instance s, policy network pθ ):\nbound = ∞\nsolution = None\ndef dfs(π):\n# bound stage: prune\n# Prune if the partial solution is worse than the incumbent\nif L(π|s) >= bound:\nreturn\n# bound stage: update bound\n# Update the bound if a better solution is found\n16\nif Done(s, π) and L(π|s) < bound:\nbound = L(π|s)\nsolution = π\nreturn\n# branch stage: RL prediction as the heuristic\nvalues = argsortjpθ(πt = j|s, π)\nfor action in values:\nπnew = copy(π).append(action)\ndfs(πnew)\nπ0 = []\ndfs(π0)\nreturn solution\n17\nB\nMODELS\nIn this section, we will look at the neural network architecture of two works: Pointer Network\n(Vinyals et al., 2015), and Attention Model (Kool et al., 2019).\nB.1\nPOINTER NETWORK\nEmbedding(s)\n-> x = W0s\nEncoder(x) # 1 layer lstm\n-> o, (h, c) = LSTM(x)\n# output, (hidden state, cell state)\nAttention(q, k)\n-> (W1kj, uj = vT tanh(W1kj + W2q)) for each key kj\nAttentionclip(q, k) # clip the range of logits to [-C,C]\n-> (W1kj, Ctanh(uj) )\nDecoder(x, (h0, c0), o):\nfor t in 1...T:\n# Predict Step\nht, ct = LSTMCell(xπt−1, (ht−1, ct−1))\net, ut = Attention(ht, o)\nat = Softmax(Maskedt(ut))\nh′\nt = P\ni at,iet,i # glimpse\ne′\nt, u′\nt = Attentionclip(h′\nt, o)\npt = Softmax(Maskedt(u′\nt))\n# Decode Step\nπt = DecodingStartegy(pt)\n# Update Step\nMaskedt+1 = Maskedt.update(πt)\nreturn {log(pt)}, π\nPointerNetwork(s):\nx = Embedding(s)\no, (h, c) = Encoder(x)\n{log(pt)}, π = Decoder(x, (h−1, c−1), o)\nlog(pθ(π|s)) = P\nt log(pt,πt)\nreturn log(pθ(π|s)), π\n18\nB.2\nATTENTION MODEL\n#####################################\n###\nHelper classes for attention ###\n#####################################\nAttentionScore(q, k, mask)\n-> uj =\n( qT kj\n√dq for node j /∈mask\n−∞otherwise.\nAttentionScoreclip(q, k, mask)\n# clip the range of logits to [-C,C]\n-> uj =\n(\nCtanh( qT kj\n√dq ) for node j /∈mask\n−∞otherwise.\nMultiHeadAttention(q, k, v, mask):\naaa(j) = Softmax(AttentionScore(q(j),kkk(j), mask)) for each head j\nh(j) = P\ni aaa(j)\ni vvvi\n#reweigh value with attention weight in each head\nq′ = W O \u0002\nh(1), ..., h(J)\u0003\n# flatten to single head\nreturn q′\nMultiHeadAttentionProj(q0, h, mask): # with projection\nq,kkk,vvv = W Qq0, W Kh, W V h\nq′ = MultiHeadAttention(q, k, v, mask)\nreturn q′\n19\n#####################################\n### Main code for attention model ###\n#####################################\nEmbedding(s)\n-> x = [W0[sloc, sfea], Wdepotsdepot]\nMHALayer(xxx): # self attention, then MLP\nxxx0 = xxx + MultiHeadAttentionProj(xxx)\nxxx1 = BatchNorm(xxx0)\nxxx2 = xxx1 + MLP2 layers(xxx1)\nhhh = BatchNorm(xxx2)\nreturn hhh\nEncoder(x) # 2 MHA layers\nhhh = xxx\nfor i in n_layers:\nhhh = MHALayeri(hhh)\nreturn hhh\nContext(s, h):\n-> (hπt−1, hs) = (previous node, global state (e.g. remaining capacity)\nDecoder(x, h):\nfor t in 1...T:\n# Predict Step\n¯hhh = 1\nN\nP\ni hhhi\n# graph embedding\nhhh(c) = [¯hhh, Context(s,hhh)]\nqgl = MultiHeadAttention(W Qhhh(c), W Khhh, W V hhh, maskt)\npppt = Softmax(AttentionScoreclip(qgl, W K′hhh, maskt))\n# Decode Step\nπt = DecodingStartegy(pt)\n# Update Step\nmaskt+1 = maskt.update(πt)\nreturn {log(pt)}, π\nAttentionModel(s):\nx = Embedding(s)\nh = Encoder(x)\n{log(pt)}, π = Decoder(s, h)\nlog(pθ(π|s)) = P\nt log(pt,πt)\nreturn log(pθ(π|s)), π\nNote that there are some tricks to speed up the training and inference for the decoder, such as\nprecomputing and caching the projection of the graph embedding ¯hhh, as well as the keys and values\nW Khhh, W V hhh, W K′hhh.\nFor dynamic node features, their projections are recomputed in every decoding steps with\nDynamicEmbedding. The projections are added to the keys and values to form the new keys\nand values in the decoder.\n20\nC\nMORE ON VECTORIZED ENVIRONMENT\nA vectorized environment runs multiple environments, either sequentially/ in parallel/ in other way.\nIt receives a batch of actions, runs environment steps, and returns a batch of observations at that\nstep. The way a RL library manipulates multiple environments (including creation, step, reset) is\ndenoted as the vector API.\nAmong the RL platforms, CleanRL has the most efﬁcient vector API — the one from OpenAI Gym,\ninstead of writing its own. RLlib is also planning to depreciate its own vector API and migrate it to\nGym’s vector API as indicated in the github repo.\nThe Gym’s vector API is a gym environment. It can be treated as a wrapper to interact with multiple\nenvironments. Take gym.vector.SyncVectorEnv as an example. When it is instantiated,\nit sequentially instantiates each of the sub-environments. In each step, it sequentially feeds the\nactions to the sub-environments, receives the results from them and stores the reward, done\ndirectly to the pre-allocated numpy arrays while the observations are concatenated according\nto their type (dict, list, tuple, ﬂoat, int, ... ). To handle the concatenation/ array pre-allocation of\ndifferent types, Gym’s vector API makes heavy use of single-dispatch function instead of nested\nisinstance as in DI-engine. The single-dispatch function approach is cleaner and is easier to\nextend.\nWe compared SyncVectorEnv (run sequentially) and AsyncVectorEnv (run in parallel) on\nour routing problem environments. We found that the sequential one is faster. It may be attributed\nto the communication overhead across the pipes for the parallel settings.\nExplicitly vectorized environment.\nInstead of using Gym’s vector API, we can implement the\nvectorized environment explicitly. For example, the environments in the Attention Model paper\n(Kool et al., 2019) are written with PyTorch tensors and PyTorch’s vectorized operations. The Gym\nenvironment for a routing problem written with numpy can also be re-implemented with numpy’s\nvectorized operations. The explicit implementation of vectorized environment provides the same\ninterface as the vector API, yet allows more room of optimization in computation efﬁciency.\nC.1\nCHUNKING VECTORIZED ENVIRONMENT\nThanks to the ﬂexibility of the Gym’s vector API, it is possible to implement chunking: vectorization\nof multiple vectorized envinronment. There are a couple of advantages of chunking: efﬁciency and\nﬂexibility. The explicitly vectorized environments are faster than running individual environments\nsequentially/ in parallel with the vector API. However, it requires careful design for ﬁne-grained\ncontrol in these explicitly vectorized environments. With chunking, we do not need to dive into the\nimplementation of explicitly vectorized environments. Instead, we can apply different wrappers to\nthese vectorized environments.\nAn example usage of chunking is that, we want to create M TSP problems and explore N trajectories\nin each of the TSP problems. First, we can create a vectorized environment (denote as TrajEnvi)\nof N sub-environments. We can apply a wrapper to ensure the same nodes coordinates in these sub-\nenvironments. Then, we create a higher level of vectorized environment (denote as ProblemEnv) of\nthese M sub-environments {TrajEnvi, i = 1...M}. The bi-level vectorized environment can now\nwell describe the setting we required.\nWith Gym’s vector API, the observations from the bi-level vectorized environment has its\ndimensions extended accordingly. It provides clearer semantics for performing operations on obser-\nvations from different level of vectorized environments. For example, the static features from the\nsame TrajEnvi need only to be computed once and can be shared among different trajectories in the\nsub-environments. It is straight-forward to do so with Gym’s vector API, otherwise it would require\ncareful reshaping and indexing.\n21\n",
  "categories": [
    "math.OC",
    "cs.LG",
    "cs.NE"
  ],
  "published": "2023-03-23",
  "updated": "2023-03-23"
}