{
  "id": "http://arxiv.org/abs/2303.13489v2",
  "title": "Boosting Reinforcement Learning and Planning with Demonstrations: A Survey",
  "authors": [
    "Tongzhou Mu",
    "Hao Su"
  ],
  "abstract": "Although reinforcement learning has seen tremendous success recently, this\nkind of trial-and-error learning can be impractical or inefficient in complex\nenvironments. The use of demonstrations, on the other hand, enables agents to\nbenefit from expert knowledge rather than having to discover the best action to\ntake through exploration. In this survey, we discuss the advantages of using\ndemonstrations in sequential decision making, various ways to apply\ndemonstrations in learning-based decision making paradigms (for example,\nreinforcement learning and planning in the learned models), and how to collect\nthe demonstrations in various scenarios. Additionally, we exemplify a practical\npipeline for generating and utilizing demonstrations in the recently proposed\nManiSkill robot learning benchmark.",
  "text": "1\nBoosting Reinforcement Learning and Planning\nwith Demonstrations: A Survey\nTongzhou Mu, Hao Su\nAbstract—Although reinforcement learning has seen tremen-\ndous success recently, this kind of trial-and-error learning can\nbe impractical or inefﬁcient in complex environments. The use\nof demonstrations, on the other hand, enables agents to beneﬁt\nfrom expert knowledge rather than having to discover the best\naction to take through exploration. In this survey, we discuss\nthe advantages of using demonstrations in sequential decision\nmaking, various ways to apply demonstrations in learning-based\ndecision making paradigms (for example, reinforcement learning\nand planning in the learned models), and how to collect the\ndemonstrations in various scenarios. Additionally, we exemplify\na practical pipeline for generating and utilizing demonstrations\nin the recently proposed ManiSkill robot learning benchmark.\nIndex Terms—Reinforcement Learning, Planning, Learning\nfrom Demonstrations, Embodied AI\nI. INTRODUCTION\nR\nEINFORCEMENT learning (RL) and planning are two\npopular methods to solving sequential decision making\nproblems. Both approaches, as well as their combination, made\nremarkable progress in solving difﬁcult tasks (e.g., Go [1],\n[2], video games [3], [4], robot control [5]) when given the\nadvantages of deep learning. RL algorithms allow agents to\nlearn through trial and error, adjusting their behavior based on\nthe consequences of their actions. Planning algorithms, on the\nother hand, involve the use of a model of the environment to\nmake decisions based on the predicted outcomes of different\nactions. While traditional planning techniques demand a model\nas an input, many recent works [6]–[10] learn a model by\ninteracting with environments and then plan in the learned\nmodel, which is very reminiscent of the RL framework. As a\nresult, nowadays the distinction between RL and planning is\nhazy.\nTypically, deep RL algorithms require tremendous training\nsamples, resulting in very high sample complexity, which refers\nto the number of samples required for learning an approximately\noptimal policy. Particularly, unlike the supervised learning\nparadigm that learns from historically labeled data, typical\nRL algorithms require the interaction data by running the\nlatest policy in the environment. Once the policy updates, the\nunderlying data distribution (formally the occupancy measure\n[11]) changes, and the data has to be collected again by running\nthe policy. This also applies to the planning methods that learn\nthe model interactively. As such, RL algorithms with high\nManuscript received March DD, 2023; revised MM, DD, 2023. (Corre-\nsponding author: Tongzhou Mu.)\nTongzhou Mu and Hao Su are with the Department of Computer Science\nand Engineering, University of California, San Diego, CA, USA (e-mail:\nt3mu@eng.ucsd.edu; haosu@eng.ucsd.edu).\nsample complexity are hard to be directly applied in real-world\ntasks, where trial-and-errors can be highly costly.\nHowever, many domains, such as autonomous driving\n[12] and robotics [13] contain a wealth of pre-collected\ndemonstrations for various tasks, which can be leveraged to\nboost reinforcement learning and planning agents. One key\nbeneﬁt of using demonstrations is that it allows agents to learn\nfrom expert knowledge, rather than having to discover the\noptimal actions through exploration. This can be particularly\nuseful in complex or dynamic environments, where trial-and-\nerror learning may be impractical or infeasible. For example,\nlearning from demonstrations has been used to train agents to\nperform tasks such as robot manipulation [14], [15], where\nthe expert knowledge of a human operator can be used to\nguide the learning process. Another advantage of leveraging\ndemonstrations is that it can reduce the amount of data and\ncomputational resources required for training. Because the\nagent can learn from expert demonstrations, rather than having\nto discover the optimal actions through exploration, the amount\nof data needed to train the agent can be signiﬁcantly smaller.\nThis can make learning from demonstrations particularly\nattractive in situations where data collection is expensive or\ntime-consuming.\nThe demonstration data can be used in numerous ways. When\nused ofﬂine, one can learn the policies, skills, world models,\nrewards, representations, or even the learning algorithm itself\nfrom the demonstrations. When used online, the demonstrations\ncan serve as experience, regularization, reference, or curriculum.\nIn Sec III and IV, we will go into greater detail about these\nvarious ways to use demonstrations.\nIt is crucial to research effective methods for collecting\ndemonstrations of high quality and quantity to support those\napproaches that rely on them. Note that the demonstrations\ncould be collected either from human experts or artiﬁcial agents\n(like learned policy or planners). Therefore, in Sec V we\ndiscuss the systems that collect demonstrations with and without\nhumans, in simulated environments and in the real world.\nTo summarize, we conduct a thorough analysis of numerous\naspects of boosting reinforcement learning and planning\nthrough demonstrations. We begin by summarizing the back-\nground and preliminary knowledge about RL and planning in\nSec II. The use of demonstrations in planning and reinforcement\nlearning is then covered in Sec III and IV. Sec V describes\nseveral existing methods of collecting demonstrations. In Sec\nVI, a complete pipeline of collecting and utilizing demonstra-\ntions is demonstrated using ManiSkill [16] as an example.\nSec VI takes ManiSkill [16] as an example to show a full\npipeline of collecting and utilizing demonstrations. In Sec VII,\narXiv:2303.13489v2  [cs.LG]  27 Mar 2023\n2\nFig. 1.\nThe agent–environment interaction in a Markov decision process\nfrom [17]\nwe conclude the entire survey and discuss the future directions\nin this area.\nII. BACKGROUND & PRELIMIARY KNOWLEDGE\nIn this section, we ﬁrst deﬁne the Markov Decision Process\n(Sec II-A), which is the environment or task to be solved in\nthe sequential decision making problem. And Sec II-B, II-C,\nand II-D brieﬂy introduce three types of methods used to solve\nMDP.\nA. Markov Decision Process\nAn MDP, denoted as M := ⟨S, A, T, R, γ⟩, serves as a\nmodel for an agent’s sequential decision-making process. Here,\nS represents a ﬁnite set of states, and A is a set of actions.\nThe mapping T : S × A →Prob(S) deﬁnes a probability\ndistribution over the set of next states, given that the agent\ntakes action a in state s. Here, Prob(S) refers to the set of\nall probability distributions over S. The transition probability\nT(s′|s, a) ∈[0, 1] denotes the probability that the system\ntransitions to state s′.\nThe reward function R can be speciﬁed in various ways.\nWhen R : S →R, it gives the scalar reinforcement at state\ns. Alternatively, R : S × A →R maps a tuple (state s, action\na taken in state s) to the reward received upon performing\nthe action. Finally, R : S × A × S →R maps a triplet (state\ns, action a, resultant state s′) to the reward obtained upon\nperforming the transition.\nThe\ndiscount\nfactor\nγ\n∈\n[0, 1]\nserves\nas\nthe\nweight\nfor\npast\nrewards\naccumulated\nin\na\ntrajectory\n⟨(s0, a0), (s1, a1), . . . , (sj, aj)⟩, where sj ∈S, aj ∈A, and\nj ∈N. The trajectory represents a sequence of state-action\npairs that the agent takes during its decision-making process.\nFig 1 shows the agent–environment interaction interface in\na Markov decision process.\nB. Reinforcement Learning\nReinforcement Learning (RL) aims to determine an optimal\npolicy in an MDP M by interacting with it.\nA policy refers to a function that maps the current state to\nthe next action choice(s). It can be deterministic, π : S →A,\nor stochastic, π : S →Prob(A). For a given policy π, the\nvalue function V π : S →R provides the value of a state s,\nwhich is the long-term expected cumulative reward incurred by\nfollowing π from that state. Speciﬁcally, the value of a policy\nπ for a given start state s0 is deﬁned as follows:\nFig. 2.\nAn illustration of Model Predictive Control from [6]\nV π(s0) = Es,π(s)\n\" ∞\nX\nt=0\nγtR(st, π(st))|s0\n#\n(1)\nIn RL, the goal is to ﬁnd an optimal policy π∗that satisﬁes\nV π∗(s) = V ∗(s) = supπ V π(s) for all s ∈S. The action-\nvalue function for π, Qπ : S × A →R, maps a state-action\npair to the long-term expected cumulative reward incurred\nafter taking action a from s and following policy π thereafter.\nAdditionally, we deﬁne the optimal action-value function as\nQ∗(s, a) = supπ Qπ(s, a). Then, V ∗(s) = supa∈A Q∗(s, a).\nRL offers an online approach for solving an MDP. The input\nfor RL consists of sampled experiences in the form of (s, a, r)\nor (s, a, r, s′), which includes the reward or reinforcement due\nto the agent performing action a in state s. In the model-free\nsetting of RL, the transition function T is unknown. Both the\ntransition function and policy are estimated from the samples,\nand the goal of RL is to learn an optimal policy.\nC. Imitation Learning\nImitation learning (IL) is an alternative approach to ﬁnding\ngood policies for an MDP M. IL seeks to extract knowledge\nfrom demonstrations provided by experts in order to replicate\ntheir behaviors. It is often used in complex sequential decision-\nmaking problems where pure reinforcement learning would\nrequire a large number of samples to solve.\nThere are generally two types of imitation learning methods.\nBehavior cloning (BC) [18] is a simple yet effective method\nthat learns a policy in a supervised manner by cloning the\nexpert’s actions at each state in the demonstration dataset.\nAnother approach to imitating expert behaviors is through\ninverse reinforcement learning (IRL) [19], [20], which ﬁnds\na reward function that allows for the solution of the expert\ndemonstrations.\nD. Planning with Models\nWhen the environment model, including the transition\nfunction T and reward function R, is available to the agent,\nplanning can be used to solve the MDP M. Planning refers\nto any computational process that takes a model as input and\nproduces or improves a policy for interacting with the modeled\nenvironment. This approach is sometimes considered a type of\nmodel-based RL.\nModel predictive control (MPC) [21] is a model-based\ncontrol method that plans an optimized sequence of actions in\nthe model. Typically, at each time step, MPC obtains an optimal\naction sequence by sampling multiple sequences and applying\nthe ﬁrst action of the sequence to the environment. Formally,\n3\nat time step t, an MPC agent seeks an action sequence at:t+τ\nby optimizing:\nmax\nat:t+τ Est′+1∼p(st′+1|st′,at′)\n\"t+τ\nX\nt′=t\nr(st′, at′)\n#\n(2)\nwhere τ denotes the planning horizon. Then the agent will\nchoose the ﬁrst action at from the action sequence and apply\nit to the environment. An illustration of MPC is shown in Fig\n2.\nMonte Carlo tree search (MCTS) [22] is an extension of\nMonte Carlo sampling methods that aim to solve Eq.(2). Unlike\nthe Monte Carlo methods used in the MPC approach, MCTS\nuses a tree-search method. At each time step, MCTS incre-\nmentally extends a search tree from the current environment\nstate. Each node in the tree corresponds to a state, which\nis evaluated using some approximated value functions or the\nreturn obtained after rollouts in the model with a random\npolicy [22] or a neural network policy [1], [2], [23]. Finally,\nan action is chosen that is more likely to transition the agent\nto a state with a higher evaluated value. In MCTS, models\nare typically used to generate the search tree and evaluate the\nstate.\nIII. USING DEMONSTRATIONS OFFLINE\nMost methods that utilize demonstrations can be divided into\ntwo stages: an ofﬂine stage and an online stage, both of which\nare optional. During the ofﬂine stage, the agent cannot access\nthe environment and can only learn from the demonstrations.\nIn the online stage, the agent is allowed to interact with the\nenvironment and can learn from both the environment and the\ndemonstrations to boost the learning process.\nIn this section, we will mainly discuss how to learn from\ndemonstrations in the ofﬂine stage. There can be a subsequent\nonline stage to further improve the agent’s performance, which\nwe will discuss in Sec IV. Note that demonstrations can be\nused in both stages, but we separate them to better categorize\ndifferent strategies for utilizing demonstrations.\nWhen learning from demonstrations ofﬂine, there are various\nthings that can be learned, such as policies, skills, world models,\nrewards, representations, and more. We will discuss these topics\none by one.\nA. Learning Policy\nLearning policies directly from demonstrations is perhaps the\nmost straightforward way to leverage the knowledge contained\nin the demonstrations. The basic idea behind imitation learning\n(IL), as we introduced in Sec II-C, is to imitate the actions\nin demonstrations based on observations. IL methods can\nbe roughly divided into two types: behavior cloning (BC)\nand inverse reinforcement learning (IRL). Although behavior\ncloning may seem simple, it is widely used in practice, even\nfor challenging tasks like real-world robot manipulation [14],\n[15]. IRL is a bit more complex because it ﬁrst estimates a\nreward function from the demonstration data and then solves\na forward RL problem using this reward function. We will\ndiscuss IRL in more detail in Sec III-D and IV-C.\nOne limitation of IL is that it requires expert demonstrations,\nmeaning that the policy used to collect the demonstrations\nshould be as optimal as possible. Since the agent only imitates\nthe behavior in the demonstration, it cannot outperform the\ndemonstrator. However, in many scenarios, we may only have\naccess to sub-optimal demonstrations, such as random-play\ndemonstrations. In this case, we need a method that can\npotentially learn a better policy solely from the demonstrations,\nand this is the goal of ofﬂine reinforcement learning.\nRecently, there have been many exciting works in the area of\nofﬂine RL [24]–[28], which focus on learning a policy using\nonly the ofﬂine data without any online learning or online\nﬁne-tuning.\nIn the methods we discussed above, the policy is learned\nsolely from the demonstrations, and therefore, it is limited by\nthe performance of the policy that generated the demonstration\ndata. Even with ofﬂine RL methods, the learned policy may\nnot be very strong due to the distributional shift between\nthe demonstrated states and the policy’s own states [29].\nTherefore, a common practice is to ﬁne-tune the policies learned\nofﬂine through online learning. For example, in AlphaGo\n[1], the policy network is ﬁrst pre-trained by cloning human\ndemonstrations and then ﬁne-tuned through reinforcement\nlearning. Similarly, [30], [31] ﬁne-tunes the policy pre-trained\nby ofﬂine RL. Another approach is to leverage a pre-trained\npolicy to generate potential action candidates during online\nplanning, similar to the approach used by [32].\nB. Skill Discovery\nIn some cases, the demonstrations may not be for the exact\nsame task that we want to solve. However, those tasks may\nshare the same skill set. For instance, making coffee and\nwashing a cup both require the skill “pick and place a cup.”\nHere, we use the term “skill” to refer to some short-horizon\npolicies shared among many tasks. It can be challenging to\nlearn a monolithic RL policy for complex, long-horizon tasks\ndue to issues such as high sample complexity, complicated\nreward design, and inefﬁcient exploration. Therefore, a natural\napproach is to learn a set of skills from demonstrations and\nthen use the learned skills in downstream tasks.\nThere is a line of research [33]–[50] that predominantly\nlearns skills using a latent variable model, where the latent\nvariables partition the experience into skills, and the overall\nmodel is learned by maximizing (a lower bound on) the\nlikelihood of the experiences. This approach is structurally\nsimilar to a long line of work in hierarchical Bayesian modeling\n[51]–[54].\nAnother approach is to use a combination of the maximum\nlikelihood objective and a penalty on the description length of\nthe skills, which is proposed by LOVE [55]. This penalty\nincentivizes the skills to maximally identify and extract\ncommon structures from the experiences.\nGoal-conditioned policies can also be viewed as a type of\nskill. For instance, [56] learns a goal-conditioned policy from\nunstructured demonstrations by goal relabeling [57]. This goal-\nconditioned policy provides a good policy initialization for\nsubsequent hierarchical reinforcement ﬁne-tuning.\n4\nTo use the learned skills to boost online reinforcement\nlearning, Parrot [58] provides a concrete example. Parrot ﬁrst\nlearns a behavioral prior from a multi-task demonstration\ndataset, which is essentially an invertible mapping that maps\nnoise to useful action. When training to solve a new task,\ninstead of directly executing its actions in the original MDP,\nthe agent learns a policy that outputs z, which is taken by the\nbehavioral prior as input. Then, the output from the behavioral\nprior is executed in the environment.\nC. Learning World Model\nIn the methods we discussed earlier, the components learned\nfrom demonstrations, such as policies and skills, are usually\nﬁne-tuned online using model-free reinforcement learning\nalgorithms. However, in model-based RL, which is a crucial\ncategory of RL algorithms, the world model is an essential\ncomponent. As noted in the previous section, the world model\ncan be represented as the MDP ⟨S, A, T, R, γ⟩, where S, A,\nand γ are typically predeﬁned, and the state transition dynamics\nT and reward function R must be learned. Since the reward\nfunction can be learned in a similar manner to the transition\nfunction, we will focus on the learning of the transition function\nfor simplicity.\nWhen given a demonstration dataset, one straightforward\napproach to learning the world model is to ﬁt the one-step\ntransitions. If Tθ is deterministic, the model learning objective\ncan be the mean squared prediction error of Tθ on the next\nstate.\nmin\nθ\nE(s,a)∼D,s′∼T ∗(·|s,a)\nh\n∥s′ −Tθ(s, a)∥2\n2\ni\n.\n(3)\nHere T ∗is the real transition dynamics, and D is the\ndemonstration dataset.\nHowever, deterministic transition models fail to capture\nthe aleatoric uncertainty [6] that arises from the inherent\nstochasticities of the environment. To model this uncertainty,\na natural approach is to use a probabilistic transition model\nTθ(·|s, a) [6]. In this case, the objective can be to minimize\nthe KL divergence between the true transition distribution\nT ∗(·|s, a) and the learned distribution Tθ(·|s, a), as shown in\nEq. (4).\nmin\nθ\nE(s,a)∼D [DKL (T ∗(·|s, a), Tθ(·|s, a))] :=\nE(s,a)∼D,s′∼T ∗(·|s,a)\n\u0014\nlog\n\u0012T ∗(s′ | s, a)\nTθ (s′ | s, a)\n\u0013\u0015\n.\n(4)\nWhen learning the world model, the requirements of the\ndemonstration dataset are different from those used for learning\npolicy. A good world model should cover the most possible\nstates, so the trajectories in the demonstration dataset should\nbe as diverse as possible. Furthermore, the actions in the\ndemonstrations do not necessarily have to be optimal. In other\nwords, the demonstration dataset should provide a diverse set\nof experiences.\nAfter the world model is learned ofﬂine, it can be used to\nsolve the MDP in two different ways.\nThe ﬁrst way is to use it as a proxy of the true MDP and\nrun any RL algorithm within this learned model. If the agent\nis allowed to interact with the true MDP, the collected data\ncan be used to further improve the learned world model. This\nstrategy is adopted by several works such as [59]–[61].\nThe second way is to plan in it. For example, [7] runs MPC\nin a learned world model to perform vision-based continuous\ncontrol, and [8] uses MCTS with the learned model to play\nvarious video games.\nHowever, directly planning in the world model learned from\ndemonstrations can have arbitrarily large sub-optimality, as\ndemonstrated by [62], because the demonstration dataset may\nnot span the entire state space, and the learned world model\nmay not be globally accurate.\nOne simple solution is to ﬁne-tune the world model by\ninteracting with the real MDP, which is also a common practice\nin model-based RL. Recently, more sophisticated methods have\nbeen proposed to address this issue. For instance, MOReL [63]\nconstructs a pessimistic MDP (P-MDP) using the learned model\nand an unknown state-action pair detector, which penalizes\npolicies that venture into unknown parts of state-action space.\nAnother approach is taken by [64], which learns a world\nmodel from the demonstration dataset along with an uncertainty\nestimator u(s, a) of the world model. This uncertainty estimator\nis then used to penalize the reward function.\nD. Inferring Reward\nIn some scenarios, the reward function is not present in the\ndemonstrations, making it challenging to learn the reward as\npart of the world model. However, it is still possible to infer\nthe reward function if the trajectories have sparse labels, such\nas success and failure. Note that the methods described in this\nsection can be regarded as inverse RL methods, but they differ\nfrom the most common inverse RL methods, which typically\ninvolve interaction with the environment. Online IRL methods\nwill be discussed in Sec IV-C.\nOne approach to inferring the reward function from demon-\nstrations is to learn a goal classiﬁer from the demonstrations and\nuse it as the reward function [65]–[67]. Speciﬁcally, the goal\nobservations (usually in the form of images) can be extracted\nfrom successful trajectories, and the negative samples can be\nextracted from failed trajectories. The reward function (goal\nclassiﬁer) can then be trained with a binary cross-entropy loss\nand used in online reinforcement learning problems where the\nreward function is not presented.\nAnother approach that leverages almost all data samples is\nproposed by [68]. This method ﬁrst learns a reward function by\ncontrasting observations from the demonstrator and unlabeled\ntrajectories, annotates all data with the learned reward, and\nﬁnally trains an agent via ofﬂine reinforcement learning.\nE. Learning Representations\nRepresentation learning via supervised / self-supervised /\nunsupervised pre-training on large-scale datasets has emerged\nas a powerful paradigm in computer vision [69]–[72] and\nnatural language processing [73]–[75], where large datasets\nare available. When a large demonstration dataset is available,\nit is natural to apply representation learning to improve policy\nlearning. In this case, the demonstration dataset does not need\n5\nFig. 3.\nAn illustration of One-Shot Imitation Learning from [83]\nto be in the form of trajectories since the observation alone\ncan be sufﬁcient for representation learning.\nThe intuition behind using representation learning to improve\npolicy learning is straightforward. Standard visual policy\nlearning frameworks attempt to simultaneously learn a succinct\nbut effective representation from diverse visual data while also\nlearning to associate actions with such representations. Such\njoint learning often requires large amounts of demonstrations\nto be effective. Decoupling the two problems by ﬁrst learning a\nvisual representation encoder from ofﬂine data using standard\nsupervised and self-supervised learning methods and then\ntraining the policy based on the learned, frozen representations\ncan be an effective approach.\nEmpirically, VINN [76] demonstrates that this simple decou-\npling improves the performance of visual imitation models on\nboth ofﬂine demonstration datasets and real-robot door-opening\ntasks. Similarly, [77] shows that pre-training with unsupervised\nlearning objectives can signiﬁcantly enhance the performance\nof policy learning algorithms that otherwise yield mediocre\nperformance.\nRecently, there has been rapid progress in the development\nof algorithms for representation learning, but the importance of\ndata in representation learning for sequential decision making\ncannot be overstated. To this end, [78] has proposed collecting\nan in-domain dataset and pre-training the visual encoder on\nit. An in-domain dataset is one that is obtained from the same\ndomain as the tasks to be solved. Conversely, several works\nhave explored learning policies with visual representations pre-\ntrained on large external datasets [76], [79]–[82]. However, it\nremains unclear whether one approach is inherently superior\nto the other.\nF. Learning Trajectory Imitator\nImitation learning is a commonly used technique for solving\ndifferent tasks, but each new task requires separate training of\nthe agent. Ideally, agents should be able to perform a novel\ntask by imitating a demonstration trajectory given at test time.\nThis is the one-shot imitation learning problem proposed by\n[83]. We refer to the agent with such capability as a trajectory\nimitator, which can also be learned from demonstrations ofﬂine.\nGiven multiple demonstrations from different tasks, the\none-shot imitation learning problem can be formulated as a\nsupervised learning problem. To train the trajectory imitator,\nin each iteration a demonstration is sampled from one of the\ntraining tasks and fed to the network. Then, a state-action pair\nis sampled from a second demonstration of the same task. The\nnetwork is trained to output the corresponding action when\nTask  \no0  a0  r0  o1  a1  r1  …  oT-1  aT-1  rT-1  oT  aT  rT\no0  a0  r0  o1  a1  r1  …  oT-1  aT-1  rT-1  oT  aT  rT\no0  a0  r0  o1  a1  r1  …  oT-1  aT-1  rT-1  oT  aT  rT\nRL algorithm \nlearning histories \nCausal Transformer\nPredict actions using \nacross-episodic contexts\nTask 1  \nData Generation  \nModel Training  \nlearning progress\nFig. 4.\nAn illustration of Algorithm Distillation from [90]\nconditioned on both the ﬁrst demonstration and this state. An\nillustration of the problem setting is shown in Fig 3. This\nframework has been extended to visual imitation [84], [85],\ncross-domain imitation [86], language-conditioned imitation\n[87], and more.\nThe process of providing a demonstration trajectory at test\ntime is similar to the prompting process [88] used in large\nlanguage models. Recently, Gato [89] combined the idea of one-\nshot imitation learning with the large language model to build\na single generalist agent beyond the realm of text outputs. The\nagent works as a multi-modal, multi-task, multi-embodiment\ngeneralist policy. It can play Atari, caption images, chat, stack\nblocks with a real robot arm, and much more, deciding based\non its context whether to output text, joint torques, button\npresses, or other tokens.\nG. Learning Algorithm\nAssuming the demonstration dataset does not only contain\nthe expert demonstration trajectories, but also the entire\nlearning history of an agent, e.g., the whole replay buffer\nof an RL agent, is it possible to learn the “learning progress”\nfrom the dataset?\nAlgorithm Distillation (AD) [90] is a recent approach that\nallows us to learn reinforcement learning algorithms directly\nfrom ofﬂine datasets, including the complete learning histories\nof agents. AD involves two steps. First, we train multiple\ninstances of an RL algorithm to solve a variety of tasks and\nsave their learning histories. Then, using this dataset of learning\nhistories, we train a transformer model to predict actions given\nthe preceding learning history. Since the policy improves over\ntime, accurately predicting actions requires the transformer\nto model policy improvement. The resulting transformer can\nexplore, exploit, and maximize return when its weights are\nfrozen, allowing it to learn the reinforcement learning algorithm\nitself. Figure 4 provides a visual illustration of the entire\nalgorithm distillation pipeline.\nIV. USING DEMONSTRATIONS ONLINE\nIn addition to ofﬂine learning from demonstrations, it is also\npossible to utilize demonstration data directly during online\nlearning, without the need for a preceding ofﬂine learning\nstage. In this section, we will discuss the different methods of\nonline reinforcement learning or planning that can be enhanced\nthrough the use of demonstrations.\n6\nFig. 5.\nPOfD [100] explores in the high-reward regions (red arrows), with\nthe aid of demonstrations (the blue curve). It thus performs better than random\nexplorations (olive green dashed curves) in sparse-reward environments.\nA. Demo as Off-Policy Experience\nOne approach to incorporating demonstration data directly\ninto online learning is to integrate it with modern off-policy\nreinforcement learning methods [3], [91]–[94], which maintain\na replay buffer [3] and optimize the policy by training on the\nexperience sampled from the replay buffer. In this approach,\ndemonstration trajectories are simply added to the replay buffer,\nand off-policy reinforcement learning is run as usual. The\ndemonstration data is never overwritten in the replay buffer,\nensuring that it remains available throughout the training\nprocess. This strategy, which is simple and easy to implement,\nhas been widely adopted by methods such as [95]–[99]. By\nincluding the demonstration data in the replay buffer, the agent\nis exposed to a more diverse set of states before exploring\nthem, addressing the exploration problem, which is a central\nchallenge in reinforcement learning.\nDespite its potential advantages, the aforementioned strategy\nhas certain limitations that prevent it from effectively exploiting\nthe demonstration data. Firstly, the approach relies solely on\nexpert trajectories as learning references and may fail to fully\nutilize their effect when the number of demonstrations is limited,\nas conﬁrmed by [100]. Secondly, the demonstrated trajectories\nmust be associated with rewards for each state transition to\nbe compatible with collected data during training. However,\nthe rewards used in the demonstrations may differ from those\nused to learn the policy in the current environment [101], or\nthey may be unavailable.\nIn situations where rewards are absent from the demon-\nstrations, the SQIL algorithm [102] can still be applied. This\napproach adds the demonstrations to the replay buffer with\na constant reward of 1, while all other experiences collected\nonline are added to the buffer with a constant reward of 0. This\ncan be viewed as a regularized variant of behavior cloning that\nemploys a sparsity prior to encourage long-horizon imitation.\nB. Demo as On-Policy Regularization\nOff-policy methods have the potential to be more sample\nefﬁcient, but are often plagued with instability issues [103],\n[104]. In contrast, on-policy methods tend to be more stable and\nperform well in high-dimensional spaces [103]. Consequently,\nseveral studies have explored ways to incorporate demonstra-\ntions into on-policy reinforcement learning algorithms.\nIn POfD [100], the authors propose using expert demon-\nstrations to guide exploration in reinforcement learning by\nenforcing occupancy measure matching between the learned\npolicy and the demonstrations. This is achieved by introducing\na demonstration-guided exploration term to the vanilla RL\nobjective η(πθ):\nL (πθ) = −η (πθ) + λ1DJS (ρθ, ρE)\n(5)\nHere, DJS denotes the Jensen-Shannon divergence, while ρθ\nand ρE represent the occupancy measures of the agent policy\nand the expert policy, respectively. By adding this regularization\nterm, the agent is encouraged to explore areas demonstrated\nby the expert policy, as depicted in Figure 5. This approach is\ncompatible with a variety of policy gradient methods, such as\nTRPO [105] and PPO [106].\nDAPG [107] ﬁrst pre-trains the policy using behavior cloning\nto provide an informed initialization that can efﬁciently guide\nexploration. During the online RL ﬁne-tuning phase, DAPG\nadds an additional term to the policy gradient to make full use\nof the information in the demonstration data:\ngaug =\nX\n(s,a)∈ρπ\n∇θ ln πθ(a|s)Aπ(s, a)+\nX\n(s,a)∈ρD\n∇θ ln πθ(a|s)w(s, a)\n(6)\nHere, ρπ represents the dataset obtained by executing policy\nπ on the MDP, and w(s, a) is a weighting function. The ﬁrst\nterm is the gradient from the RL algorithm, and the second\nterm is the gradient from the behavior cloning loss. In practice,\nw(s, a) is implemented using a heuristic weighting function:\nw(s, a) = λ0λk\n1\nmax\n(s′,a′)∈ρπ Aπ (s′, a′)\n∀(s, a) ∈ρD\n(7)\nwhere λ0 and λ1 are hyperparameters, and k is the iteration\ncounter. The decay of the weighting term via λk\n1 is motivated\nby the premise that, initially, the actions suggested by the\ndemonstrations are at least as good as the actions produced\nby the policy. However, towards the end, when the policy\nis comparable in performance to the demonstrations, we do\nnot want to bias the gradient. Thus, the auxiliary objective\nasymptotically decays.\nC. Demo as Reference for Reward\nIn cases where the reward is sparse or unavailable, demon-\nstrations can be used to compute the reward. This can be\naccomplished by either directly deﬁning a reward based on the\ndemonstration or by matching the agent’s trajectory distribution\nwith the demonstration distribution. The latter is a common\nstrategy employed in inverse reinforcement learning. We discuss\nboth of these methods in the following subsections.\n1) Directly Deﬁne Reward based on Demo: If only a\nsingle (or a few) demonstration trajectories are available, a\nreward function can be manually designed to incentivize the\nagent to follow the demonstration trajectory. This is typically\naccomplished by deﬁning the reward as some kind of distance\nbetween the current state and the demonstration trajectory.\nVarious approaches can be used to specify the re-\nward/distance function. For example, in [108], a sequence\nof important checkpoints is extracted from the demonstration\ntrajectory, and the agent is rewarded if it visits the checkpoints\n7\nin a soft order. A checkpoint is considered visited when the\ndistance to it is less than a threshold, and the distance is deﬁned\nas cosine similarity in a learned latent space. Similarly, AVID\n[109] divides a demonstration trajectory into stages and trains\na classiﬁer to predict whether a state belongs to a speciﬁc\nstage. These classiﬁers can then be combined to form a reward\nfunction.\nIn [110], the potential of a state is ﬁrst deﬁned as the highest\nsimilarity with the states in the demonstration, and then the\nγ-difference of the potential function is used to construct a\npotential-based reward function [111]. In [112] and [113], the\nreward is deﬁned as the distance to the goal in the learned\nlatent space, where the goal is chosen as the ﬁnal state in the\ndemonstration trajectory. Recent works on motion imitation\n[114]–[116] carefully design the reward as a weighted distance\nto the reference state, taking joint orientations, joint velocities,\nend effectors, and centers of mass into account.\n2) Match the Distribution of Demo: In addition to deﬁning\na reward based on demonstrations, another approach involves\nallowing the agent to interact with the environment and at-\ntempting to match the distributions of agent and demonstration\ntrajectories. During this process, the distance or divergence\nbetween agent and demonstration trajectories can be converted\nto a reward and used to improve the agent policy. This approach\nforms the basis of most inverse reinforcement learning (IRL)\nmethods. The process can be summarized using the algorithm\ntemplate shown in Algorithm 1 (from [117]).\nAlgorithm 1 Template for Typical Inverse RL [117]\nInput: M\\RE= ⟨S, A, T, γ⟩, Set of trajectories demonstrating\ndesired behavior: D = {⟨(s0, a0), (s1, a1), . . . , (st, at)⟩, . . .},\nst ∈S, at ∈A, t ∈N, or expert’s policy: πE, and reward\nfunction features\nOutput: ˆRE\n1: Model the expert’s observed behavior as the solution of\nan MDP whose reward function is not known\n2: Initialize the parameterized form of the reward function\nusing any given features (linearly weighted sum of feature\nvalues, distribution over rewards, or other)\n3: Solve the MDP with the current reward function to generate\nthe learned behavior or policy\n4: Update the optimization parameters to minimize the\ndivergence between the observed behavior (or policy) and\nthe learned behavior (policy)\n5: Repeat the previous two steps till the divergence is reduced\nto the desired level.\nAs shown in Algorithm 1, IRL methods typically involve an\niterative process that alternates between reward estimation and\nRL, which can result in poor sample efﬁciency. Earlier IRL\nmethods [101], [118], [119] typically required multiple calls\nto a Markov decision process solver [120].\nRecently, adversarial imitation learning (AIL) approaches\nhave been proposed [20], [121]–[123] that interleave the\nlearning of the reward function with the learning process of\nthe agent.\nAIL methods operate similarly to generative adversarial\nnetworks (GANs) [124]. In these methods, a generator (the\npolicy) is trained to maximize the confusion of a discriminator\n(the reward) that is itself trained to differentiate between the\nagent’s state-action pairs and those of the expert. Adversarial\nIL methods essentially boil down to a distribution matching\nproblem, i.e., the problem of minimizing the distance between\nprobability distributions in a metric space. According to the\nanalysis in [123], these methods implicitly minimize an f-\ndivergence between the state-action distribution of an expert\nand that of the learning agent.\nIn addition, the POfD algorithm [100], mentioned in Section\nIV-B, can also be regarded as a variant of AIL. In practice,\nPOfD is implemented by adding a shaped reward to the\nenvironment’s original reward, and the shaped reward is also\ncomputed from a discriminator that differentiates between the\nagent’s and expert’s behaviors.\nBesides adversarial imitation learning (AIL) methods, there\nare also imitation learning-based methods that approach the\nproblem as a distribution matching problem but do not use\nadversarial approaches. PWIL [125], for example, matches the\ndemonstration distribution by minimizing the primal form of the\nWasserstein distance, also known as the earth mover’s distance.\nAn upper bound on the Wasserstein distance can be used\nto compute the reward ofﬂine, avoiding the computationally\nexpensive min-max optimization problem encountered in AIL.\nD. Demo as Curriculum of Start States\nWhen the environment is a simulator that can be fully con-\ntrolled, demonstrations can be used to construct a curriculum\nof start states. This approach is a highly efﬁcient exploration\nstrategy in RL and has been studied in several prior works.\nTypically, these works assume the ability to reset the\nsimulator to arbitrary states. Therefore, they can modify the\ndistribution of start states based on the demonstrations to\nsimplify exploration. Some methods, such as [99], [114], [126],\nuniformly sample states from the demonstrations as start states,\nwhile others generate curriculums of start states [127], [128].\nUsing a curriculum of start states is helpful because it reduces\nthe exploration difﬁculty and accelerates learning. For example,\nwhen attempting a backﬂip, starting from a standing pose\nrequires learning the early phases before progressing towards\nthe later phases. However, if start states are sampled from the\ndemonstrations, the agent encounters desirable states earlier\nand can learn faster.\nV. DEMONSTRATION COLLECTION\nIn this section, we discuss how to collect demonstrations.\nParticularly, we focus on demonstration collection for embodied\nAI (or learning-based robotics) tasks, because it involves\ndemonstrations from both artiﬁcial agents and humans, as\nwell as from both simulation environments and the real world.\nWe primarily discuss how to acquire expert (or near-expert)\ndemonstrations, as suboptimal demonstrations are relatively\neasy to obtain.\nIn demonstrations for embodied AI tasks (e.g., object\nmanipulation), the embodiment of the agent to complete the\ntask can be either robot or human, and the operator of the agent\ncan be either an autonomous system or human. We, therefore,\n8\nFig. 6.\nThe VR interface with HTC Vive and Noitom Glove in RFUniverse\n[129].\ncategorize the demonstration collection methods by these two\nperspectives.\nA. Robot Embodiment - Human Operator\n1) Simulation: In the context of Embodied AI, demonstra-\ntions are often collected from human operators via teleoperation.\nThis can be achieved using simple devices such as a mouse,\nkeyboard, or smartphone. As these devices are easily accessible,\nmany robot simulation environments provide interfaces that\nallow users to control the robot in the simulation. For example,\nrobosuite [130] provides utilities for collecting human demon-\nstrations using a keyboard or 3D mouse devices, while iGibson\n[131] is equipped with a graphical user interface that facilitates\nuser interactions with the simulation environment. RoboTurk\n[132] offers intuitive 6 degrees-of-freedom motion control,\nwhich maps smartphone movement to robot arm movement.\nThe simplicity of these devices makes it possible to leverage\ncrowdsourcing to obtain data for various robotic tasks.\nRecently, virtual reality (VR) interfaces have gained popular-\nity for collecting robot demonstrations due to their ﬂexibility.\nFor instance, the demonstrations used in DAPG [107] were\ncollected in VR. Franka Kitchen environment [56] collects\ndemonstrations using the PUPPET MuJoCo VR system [133].\niGibson 2.0 [134] includes a novel VR interface that is\ncompatible with major commercially available VR headsets\nthrough OpenVR [135]. RFUniverse [129] can connect to\ndifferent VR devices through SteamVR, including novel VR\ngloves, as illustrated in Fig. 6.\nOne of the major advantages of VR is the immersive\nexperience: humans can embody an avatar in the same scene\nand for the same task as the AI agents. This allows for a more\nrealistic interaction between humans and robots. For example,\nas shown in Fig 7, the virtual reality avatar in iGibson 2.0 is\ncomposed of a main body, two hands, and a head. The human\ncontrols the motion of the head and the two hands via the\nVR headset and hand controllers, with an optional additional\ntracker for control of the main body. Humans receive stereo\nimages as generated from the point of view of the head of the\nvirtual avatar.\n2) Real World: Furthermore, VR-based teleoperation has\nalso been used to collect demonstrations in the real world.\nThe SayCan system [14] is an example of such an approach,\nwhere 68,000 teleoperated demonstrations were performed by\n10 robots over 11 months. To collect the demonstrations, the\nFig. 7.\nThe VR interface in iGibson 2.0 [134].\noperators used VR headset controllers to track the motion\nof their hand, which was then mapped onto the robot’s end-\neffector pose. In addition, a joystick was used to move the\nrobot’s base. Human raters ﬁltered the data to exclude unsafe,\nundesirable, or infeasible episodes.\nWith sufﬁcient resources, VR-based teleoperation can be\nused to collect even larger demonstration datasets. For instance,\nthe multi-task robot transformer RT-1 [15] is trained on a large-\nscale real-world robotics dataset of 130k episodes covering\nover 700 tasks, which was collected using a ﬂeet of 13 robots\nover 17 months. The demonstration collection process involves\neach of the robots autonomously approaching its station at the\nbeginning of the episode and communicating the instruction that\nthe operator should demonstrate to the robot. Demonstrations\nare collected with direct line-of-sight between the operator\nand robot using two virtual reality remotes, with 3D position\nand rotational displacements of the remote mapped to 6D\ndisplacements of the robot tool. The joystick’s X and Y position\nis mapped to the turning angle and driving distance of the\nmobile base, respectively. An illustration of their dataset is\nshown in Fig 8.\nB. Robot Embodiment - Autonomous Operator\n1) Simulation: While collecting demonstrations by humans\nis a straightforward approach, scalability is still a potential\nissue. To address this, a number of works have explored the\nuse of autonomous agents to generate demonstrations in a more\nscalable manner.\nOne approach to generate large-scale demonstrations is by\nusing a planner. In some high-level planning tasks, such\nas Watch-And-Help [136] and ALFRED [137], symbolic\nplanners [138] can be used to generate demonstrations at a\nlow cost. Meanwhile, in low-level control tasks, such as robot\nmanipulation, motion planners [139], [140] can be utilized.\nFor example, RLbench [141] relies on an inﬁnite supply\nof generated demonstrations collected via motion planners.\nCompared to the crowd-sourcing way to collect demonstrations,\nthe planner-based system cannot be run in the real world, but\nin exchange, it receives the ability to generate a diverse range\nof tasks in a scalable way.\nIn addition to planning, learning-based methods can also\nbe used to generate demonstrations. Various different methods\n9\nFig. 8.\nRT-1’s [15] large-scale, real-world training (130k demonstrations) and evaluation (3000 trials) dataset.\nFig. 9.\nQT-opt [143] collects grasping demonstration data with autonomous\nself-supervision.\nare deployed in D4RL [142]. For Gym-MuJuCo tasks, expert\ndemonstrations are generated by RL agents. For AntMaze\ntasks, they are generated by training a goal-reaching policy and\nusing it in conjunction with a high-level waypoint generator.\nIn Adroit, a large amount of expert data can be obtained by a\nlearning-from-demonstration agent trained with a small number\nof human demonstrations.\nManiSkill [16] also uses RL agents to collect demonstrations,\nbut training a single RL agent to solve a task can be challenging\ndue to the difﬁculty of the tasks. Therefore, it trains RL agents\nin a divide-and-conquer way, which is a more scalable way to\nsolve tasks and generate demonstrations. This will be discussed\nin detail in Section VI-A.\n2) Real World: Collecting demonstration data in the real\nworld can be challenging due to the high cost of sample\ncollection and the limitations of some methods, such as motion\nplanners. To address these challenges, demonstration collection\nsystems must be designed carefully.\nOne example of such a system is QT-opt [143], which\ncollects self-supervised demonstration data for robot grasping\nin the real world. The system runs a reinforcement learning\nalgorithm directly on the robot, receiving a binary reward for\nlifting an object successfully without any other reward shaping.\nSuccess is determined using a background subtraction test after\nthe picked object is dropped. The setup for collecting data in\nQT-opt is shown in Figure 9.\nMT-opt [144] is another demonstration collection system\nthat continuously improves multi-task policies. Like QT-opt,\nit uses success detectors for self-supervised reward labeling,\nbut in MT-opt, the success detectors are trained on data from\nall tasks and continuously updated to account for distribution\nshifts caused by factors such as varying lighting conditions and\nchanging backgrounds in the real world. Additionally, MT-opt\nleverages the solutions to easier tasks to effectively bootstrap\nlearning of more complex tasks, allowing the robot to train\npolicies for harder tasks over time and collect better data for\nthose tasks.\nC. Human Embodiment - Human Operator\nWhile robot demonstrations are naturally good for learning\nrobotic tasks, there are a lot of works that try to let robots\nlearn from human videos [80], [86], [109]. Therefore, in this\nsection, we brieﬂy discuss the data collection process of some\nrepresentative datasets of human daily activities.\nEgo4D [145] is a large-scale egocentric video dataset and\nbenchmark suite that includes 3,670 hours of daily life activity\nvideos captured by 931 unique camera wearers from 74\nworldwide locations and 9 different countries. The dataset spans\nhundreds of scenarios, such as household, outdoor, workplace,\nleisure, and more. The key challenge of data collection is to\nrecruit participants to record their daily activities. To overcome\nthis challenge, the Ego4D project consists of 14 teams from\nuniversities and labs in 9 countries and 5 continents. Each team\nrecruited participants to wear a camera for 1 to 10 hours at a\ntime, and participants in 74 cities were recruited through word\nof mouth, ads, and postings on community bulletin boards.\nRoboTube [146] is a recently proposed dataset that provides\na human video dataset and its digital twins for learning various\nrobotic manipulation tasks. To collect the human video datasets,\na portable video collection system with two RealSense D435\ncameras was designed. The recording process streams two\nviewpoints: one is the ﬁrst-person perspective from the camera\nmounted on the human head, and the other is the third-person\nperspective from the camera ﬁxed on a tripod placed near the\nscene. These two streams are temporally synchronized, which\nenables the collection of high-quality video data for learning\nrobotic manipulation tasks.\nVI. CASE STUDY: THE PIPELINE OF COLLECTING AND\nUTILIZING DEMONSTRATIONS IN MANISKILL\nIn this section, we present ManiSkill [16] (as well as its\nsuccessor [147]) as an example to demonstrate the entire\npipeline of collecting and utilizing demonstrations. Henceforth,\nwe refer to this series of works as ManiSkill for the sake of\nbrevity.\n10\nFig. 10.\nThe tasks in ManiSkill2 [147].\nManiSkill is a benchmark for manipulation skills that can\nbe generalized across a range of tasks. It comprises 20\nmanipulation task families with over 2000 object models\nand more than 4 million demonstration frames. These tasks\ninclude stationary/mobile-base, single/dual-arm, and rigid/soft-\nbody manipulation, with 2D/3D-input data simulated by fully\ndynamic engines. It provides a uniﬁed interface and evaluation\nprotocol to support a wide range of algorithms such as classic\nsense-plan-act, RL, and IL. It also accommodates various visual\nobservations (point cloud, RGBD) and controllers (e.g., action\ntype and parameterization). Fig 10 displays the visualization\nof tasks in ManiSkill2.\nA. Collecting Demonstrations\nManiSkill’s approach of using different methods to generate\ndemonstrations is motivated by the fact that the various tasks in\nthe benchmark exhibit distinct characteristics, and thus, require\ntailored techniques. Speciﬁcally, task and motion planning\n(TAMP), model predictive control (MPC), and reinforcement\nlearning (RL) are employed to generate demonstrations based\non the task’s difﬁculty and requirements.\nTAMP is a suitable approach for many stationary manipula-\ntion tasks such as pick-and-place, as it does not require crafting\nrewards. However, it can face difﬁculties in dealing with\nunderactuated systems like pushing chairs or moving buckets.\nIn contrast, MPC is capable of searching for solutions to\ndifﬁcult tasks when given well-designed shaped rewards without\nthe need for training or observations. However, designing\na universal shaped reward for a variety of objects is non-\ntrivial. On the other hand, RL requires additional training and\nhyperparameter tuning but is more scalable than MPC during\ninference.\nFor most rigid-body manipulation tasks and soft-body\nmanipulation tasks, the demonstrations are generated through\nTAMP. However, for tasks involving manipulating articulated\nobjects, the demonstration collection becomes more complex.\nNot only is manipulating articulated objects more challenging,\nbut ﬁnding a universal solution for all object instances within\na single task is also not straightforward.\nFor stationary manipulation tasks, such as turning a faucet,\nsome faucet models can be solved by TAMP, while the\nremaining ones require solutions from MPC-CEM, utilizing\ndesigned dense rewards.\nIn the case of mobile manipulation tasks, such as pushing\na swivel chair, TAMP is not a viable approach, and training\na single RL agent to collect demonstrations for all objects is\ndifﬁcult. However, since training an agent to solve a speciﬁc\nobject is feasible and well-studied, the demonstrations can be\ngenerated using a divide-and-conquer approach, wherein an RL\nagent is trained for each environment to generate successful\ndemonstrations.\nB. Utilizing Demonstrations\nIn ManiSkill, various demonstration-based baselines are\nevaluated to demonstrate the effectiveness of utilizing demon-\nstrations.\nTo utilize demonstrations ofﬂine, ManiSkill benchmarks\nbehavior cloning (BC), Batch-Constrained Q-Learning (BCQ)\n[24], and Twin-Delayed DDPG [93] with Behavior Cloning\n(TD3+BC) [148]. The results show that when near-optimal\ndemonstrations are available, BC achieves the strongest perfor-\nmance.\nTo augment online reinforcement learning with demon-\nstrations, ManiSkill tests DAPG [107], which signiﬁcantly\noutperforms the PPO [106] baseline.\nIn addition, [149] investigates how to leverage large-scale\ndemonstrations in ManiSkill by combining GAIL [20] and\nGASIL [150] to utilize demonstrations in online reinforcement\nlearning. The results show that combining ofﬂine imitation\nlearning and online reinforcement learning yields much better\nobject-level generalization in manipulation tasks.\nVII. CONCLUSION & DISCUSSION\nIn conclusion, demonstrations have proven to be an effective\nway of improving the performance of agents in reinforcement\nlearning and planning tasks. They offer valuable insights into\nthe desired behavior of an agent, and can help accelerate\nthe learning process. This survey has highlighted the various\n11\nmethods and approaches for utilizing demonstrations, especially\nin the context of embodied AI.\nDespite the growing interest in demonstrations, there are still\nmany challenges to overcome. One major issue is scaling up the\ndemonstration collection process, which is currently hindered\nby limited scalability of teleoperation-based methods and\nquality control issues in learning-based autonomous pipelines.\nA promising solution could be the development of human-in-\nthe-loop autonomous systems that can improve over time.\nIn addition to the collection of demonstrations, there is also\na need to consider the types of demonstrations that are required.\nQuestions such as the necessity of near-optimal demonstrations,\nthe potential of learning from different embodiments, and the\nuse of abstract demonstrations like natural language need to\nbe addressed.\nAnother critical problem is the integration of ofﬂine and\nonline learning. While some initial attempts have been made,\nthere is still a need for solutions that can accommodate different\nforms and qualities of demonstrations in a variety of scenarios,\nparticularly in partially observed and cross-domain contexts.\nOverall, demonstrations have the potential to signiﬁcantly\nenhance the performance of reinforcement learning and plan-\nning algorithms in real-world settings. However, there is still\nmuch to be explored and discovered in this area, and numerous\nopportunities exist for further research and development.\nREFERENCES\n[1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot et al., “Mastering the game of go with deep neural networks\nand tree search,” nature, vol. 529, no. 7587, pp. 484–489, 2016.\n[2] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,\nA. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering the\ngame of go without human knowledge,” nature, vol. 550, no. 7676, pp.\n354–359, 2017.\n[3] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al.,\n“Human-level control through deep reinforcement learning,” nature, vol.\n518, no. 7540, pp. 529–533, 2015.\n[4] C. Berner, G. Brockman, B. Chan, V. Cheung, P. D˛ebiak, C. Dennison,\nD. Farhi, Q. Fischer, S. Hashme, C. Hesse et al., “Dota 2 with large\nscale deep reinforcement learning,” arXiv preprint arXiv:1912.06680,\n2019.\n[5] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun,\nand M. Hutter, “Learning agile and dynamic motor skills for legged\nrobots,” Science Robotics, vol. 4, no. 26, p. eaau5872, 2019.\n[6] K. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep reinforce-\nment learning in a handful of trials using probabilistic dynamics models,”\nAdvances in neural information processing systems, vol. 31, 2018.\n[7] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and\nJ. Davidson, “Learning latent dynamics for planning from pixels,” in\nInternational conference on machine learning.\nPMLR, 2019, pp.\n2555–2565.\n[8] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre,\nS. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel et al.,\n“Mastering atari, go, chess and shogi by planning with a learned model,”\nNature, vol. 588, no. 7839, pp. 604–609, 2020.\n[9] W. Ye, S. Liu, T. Kurutach, P. Abbeel, and Y. Gao, “Mastering atari\ngames with limited data,” Advances in Neural Information Processing\nSystems, vol. 34, pp. 25 476–25 488, 2021.\n[10] N. Hansen, X. Wang, and H. Su, “Temporal difference learning for\nmodel predictive control,” arXiv preprint arXiv:2203.04955, 2022.\n[11] U. Syed, M. Bowling, and R. E. Schapire, “Apprenticeship learning\nusing linear programming,” in Proceedings of the 25th international\nconference on Machine learning, 2008, pp. 1032–1039.\n[12] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,\nJ. Guo, Y. Zhou, Y. Chai, B. Caine et al., “Scalability in perception\nfor autonomous driving: Waymo open dataset,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, 2020,\npp. 2446–2454.\n[13] S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper,\nS. Singh, S. Levine, and C. Finn, “Robonet: Large-scale multi-robot\nlearning,” arXiv preprint arXiv:1910.11215, 2019.\n[14] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David,\nC. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog et al., “Do as i\ncan, not as i say: Grounding language in robotic affordances,” arXiv\npreprint arXiv:2204.01691, 2022.\n[15] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn,\nK. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al., “Rt-1:\nRobotics transformer for real-world control at scale,” arXiv preprint\narXiv:2212.06817, 2022.\n[16] T. Mu, Z. Ling, F. Xiang, D. C. Yang, X. Li, S. Tao, Z. Huang, Z. Jia,\nand H. Su, “Maniskill: Generalizable manipulation skill benchmark\nwith large-scale demonstrations,” in Thirty-ﬁfth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round\n2), 2021.\n[17] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[18] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural\nnetwork,” Advances in neural information processing systems, vol. 1,\n1988.\n[19] A. Y. Ng, S. J. Russell et al., “Algorithms for inverse reinforcement\nlearning.” in Icml, vol. 1, 2000, p. 2.\n[20] J. Ho and S. Ermon, “Generative adversarial imitation learning,”\nAdvances in neural information processing systems, vol. 29, 2016.\n[21] E. F. Camacho and C. B. Alba, Model predictive control.\nSpringer\nscience & business media, 2013.\n[22] G. Chaslot, S. Bakkes, I. Szita, and P. Spronck, “Monte-carlo tree search:\nA new framework for game ai,” in Proceedings of the AAAI Conference\non Artiﬁcial Intelligence and Interactive Digital Entertainment, vol. 4,\nno. 1, 2008, pp. 216–217.\n[23] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,\nM. Lanctot, L. Sifre, D. Kumaran, T. Graepel et al., “Mastering chess\nand shogi by self-play with a general reinforcement learning algorithm,”\narXiv preprint arXiv:1712.01815, 2017.\n[24] S. Fujimoto, E. Conti, M. Ghavamzadeh, and J. Pineau, “Benchmark-\ning batch deep reinforcement learning algorithms,” arXiv preprint\narXiv:1910.01708, 2019.\n[25] A. Kumar, J. Fu, M. Soh, G. Tucker, and S. Levine, “Stabilizing off-\npolicy q-learning via bootstrapping error reduction,” Advances in Neural\nInformation Processing Systems, vol. 32, 2019.\n[26] Y. Wu, G. Tucker, and O. Nachum, “Behavior regularized ofﬂine\nreinforcement learning,” arXiv preprint arXiv:1911.11361, 2019.\n[27] R. Agarwal, D. Schuurmans, and M. Norouzi, “An optimistic perspective\non ofﬂine reinforcement learning,” in International Conference on\nMachine Learning.\nPMLR, 2020, pp. 104–114.\n[28] A. Kumar, A. Zhou, G. Tucker, and S. Levine, “Conservative q-learning\nfor ofﬂine reinforcement learning,” Advances in Neural Information\nProcessing Systems, vol. 33, pp. 1179–1191, 2020.\n[29] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning\nand structured prediction to no-regret online learning,” in Proceedings\nof the fourteenth international conference on artiﬁcial intelligence and\nstatistics.\nJMLR Workshop and Conference Proceedings, 2011, pp.\n627–635.\n[30] A. Nair, A. Gupta, M. Dalal, and S. Levine, “Awac: Accelerating\nonline reinforcement learning with ofﬂine datasets,” arXiv preprint\narXiv:2006.09359, 2020.\n[31] Y. Lu, K. Hausman, Y. Chebotar, M. Yan, E. Jang, A. Herzog, T. Xiao,\nA. Irpan, M. Khansari, D. Kalashnikov et al., “Aw-opt: Learning robotic\nskills with imitation andreinforcement at scale,” in Conference on Robot\nLearning.\nPMLR, 2022, pp. 1078–1088.\n[32] Z.-H. Yin, W. Ye, Q. Chen, and Y. Gao, “Planning for sample efﬁcient\nimitation learning,” in Advances in Neural Information Processing\nSystems.\n[33] S. Niekum, S. Chitta, A. G. Barto, B. Marthi, and S. Osentoski,\n“Incremental semantically grounded learning from demonstration.” in\nRobotics: Science and Systems, vol. 9.\nBerlin, Germany, 2013, pp.\n10–15 607.\n[34] R. Fox, S. Krishnan, I. Stoica, and K. Goldberg, “Multi-level discovery\nof deep options,” arXiv preprint arXiv:1703.08294, 2017.\n12\n[35] S. Krishnan, R. Fox, I. Stoica, and K. Goldberg, “Ddco: Discovery of\ndeep continuous options for robot learning from demonstrations,” in\nConference on robot learning.\nPMLR, 2017, pp. 418–437.\n[36] A. Sharma, M. Sharma, N. Rhinehart, and K. M. Kitani, “Directed-info\ngail: Learning hierarchical policies from unsegmented demonstrations\nusing directed information,” arXiv preprint arXiv:1810.01266, 2018.\n[37] T. Kipf, Y. Li, H. Dai, V. Zambaldi, A. Sanchez-Gonzalez, E. Grefen-\nstette, P. Kohli, and P. Battaglia, “Compile: Compositional imitation\nlearning and execution,” in International Conference on Machine\nLearning.\nPMLR, 2019, pp. 3418–3428.\n[38] R. Bera, V. G. Goecks, G. M. Gremillion, J. Valasek, and N. R.\nWaytowich, “Podnet: A neural network for discovery of plannable\noptions,” arXiv preprint arXiv:1911.00171, 2019.\n[39] K. Pertsch, Y. Lee, and J. J. Lim, “Accelerating reinforcement learning\nwith learned skill priors,” arXiv preprint arXiv:2010.11944, 2020.\n[40] W. Zhou, S. Bajracharya, and D. Held, “Plas: Latent action space for\nofﬂine reinforcement learning,” arXiv preprint arXiv:2011.07213, 2020.\n[41] Z. Zhao, C. Gan, J. Wu, X. Guo, and J. B. Tenenbaum, “Augmenting\npolicy learning with routines discovered from a demonstration,” arXiv\npreprint arXiv:2012.12469, 2020.\n[42] S.-H. Lee and S.-W. Seo, “Learning compound tasks without task-\nspeciﬁc knowledge via imitation and self-supervised learning,” in\nInternational Conference on Machine Learning.\nPMLR, 2020, pp.\n5747–5756.\n[43] A. Ajay, A. Kumar, P. Agrawal, S. Levine, and O. Nachum, “Opal:\nOfﬂine primitive discovery for accelerating ofﬂine reinforcement\nlearning,” arXiv preprint arXiv:2010.13611, 2020.\n[44] T. Shankar, S. Tulsiani, L. Pinto, and A. Gupta, “Discovering\nmotor programs by recomposing demonstrations,” in International\nConference on Learning Representations, 2020. [Online]. Available:\nhttps://openreview.net/forum?id=rkgHY0NYwr\n[45] T. Shankar and A. Gupta, “Learning robot skills with temporal\nvariational inference,” in International Conference on Machine Learning.\nPMLR, 2020, pp. 8624–8633.\n[46] Y. Lu, Y. Shen, S. Zhou, A. Courville, J. B. Tenenbaum, and C. Gan,\n“Learning task decomposition with ordered memory policy network,”\narXiv preprint arXiv:2103.10972, 2021.\n[47] Y. Zhu, P. Stone, and Y. Zhu, “Bottom-up skill discovery from\nunsegmented demonstrations for long-horizon robot manipulation,”\narXiv preprint arXiv:2109.13841, 2021.\n[48] D. Rao, F. Sadeghi, L. Hasenclever, M. Wulfmeier, M. Zambelli,\nG. Vezzani, D. Tirumala, Y. Aytar, J. Merel, N. Heess et al., “Learning\ntransferable motor skills with hierarchical latent mixture policies,” arXiv\npreprint arXiv:2112.05062, 2021.\n[49] D. Tanneberg, K. Ploeger, E. Rueckert, and J. Peters, “Skid raw: Skill\ndiscovery from raw trajectories,” IEEE Robotics and Automation Letters,\nvol. 6, no. 3, pp. 4696–4703, 2021.\n[50] M. Yang, S. Levine, and O. Nachum, “Trail: Near-optimal imitation\nlearning with suboptimal data,” arXiv preprint arXiv:2110.14770, 2021.\n[51] Z. Ghahramani and G. E. Hinton, “Variational learning for switching\nstate-space models,” Neural computation, vol. 12, no. 4, pp. 831–864,\n2000.\n[52] D. M. Blei and P. J. Moreno, “Topic segmentation with an aspect\nhidden markov model,” in Proceedings of the 24th annual international\nACM SIGIR conference on Research and development in information\nretrieval, 2001, pp. 343–348.\n[53] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “A sticky\nhdp-hmm with application to speaker diarization,” The Annals of Applied\nStatistics, pp. 1020–1056, 2011.\n[54] S. Linderman, M. Johnson, A. Miller, R. Adams, D. Blei, and\nL. Paninski, “Bayesian learning and inference in recurrent switching\nlinear dynamical systems,” in Artiﬁcial Intelligence and Statistics.\nPMLR, 2017, pp. 914–922.\n[55] Y. Jiang, E. Z. Liu, B. Eysenbach, Z. Kolter, and C. Finn, “Learning\noptions via compression,” arXiv preprint arXiv:2212.04590, 2022.\n[56] A. Gupta, V. Kumar, C. Lynch, S. Levine, and K. Hausman, “Relay pol-\nicy learning: Solving long-horizon tasks via imitation and reinforcement\nlearning,” arXiv preprint arXiv:1910.11956, 2019.\n[57] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,\nB. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba, “Hindsight\nexperience replay,” Advances in neural information processing systems,\nvol. 30, 2017.\n[58] A. Singh, H. Liu, G. Zhou, A. Yu, N. Rhinehart, and S. Levine, “Parrot:\nData-driven behavioral priors for reinforcement learning,” arXiv preprint\narXiv:2011.10024, 2020.\n[59] T. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel,\n“Model-ensemble trust-region policy optimization,” arXiv preprint\narXiv:1802.10592, 2018.\n[60] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, “Dream to control: Learn-\ning behaviors by latent imagination,” arXiv preprint arXiv:1912.01603,\n2019.\n[61] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Camp-\nbell, K. Czechowski, D. Erhan, C. Finn, P. Kozakowski, S. Levine\net al., “Model-based reinforcement learning for atari,” arXiv preprint\narXiv:1903.00374, 2019.\n[62] S. Ross and J. A. Bagnell, “Agnostic system identiﬁcation for model-\nbased reinforcement learning,” arXiv preprint arXiv:1203.1007, 2012.\n[63] R. Kidambi, A. Rajeswaran, P. Netrapalli, and T. Joachims, “Morel:\nModel-based ofﬂine reinforcement learning,” Advances in neural\ninformation processing systems, vol. 33, pp. 21 810–21 823, 2020.\n[64] T. Yu, G. Thomas, L. Yu, S. Ermon, J. Y. Zou, S. Levine, C. Finn,\nand T. Ma, “Mopo: Model-based ofﬂine policy optimization,” Advances\nin Neural Information Processing Systems, vol. 33, pp. 14 129–14 142,\n2020.\n[65] A. Xie, A. Singh, S. Levine, and C. Finn, “Few-shot goal inference for\nvisuomotor learning and planning,” in Conference on Robot Learning.\nPMLR, 2018, pp. 40–52.\n[66] M. Vecerik, O. Sushkov, D. Barker, T. Rothörl, T. Hester, and J. Scholz,\n“A practical approach to insertion with variable socket position using\ndeep reinforcement learning,” in 2019 international conference on\nrobotics and automation (ICRA).\nIEEE, 2019, pp. 754–760.\n[67] A. Singh, L. Yang, K. Hartikainen, C. Finn, and S. Levine, “End-to-\nend robotic reinforcement learning without reward engineering,” arXiv\npreprint arXiv:1904.07854, 2019.\n[68] K. Zolna, A. Novikov, K. Konyushkova, C. Gulcehre, Z. Wang, Y. Aytar,\nM. Denil, N. de Freitas, and S. Reed, “Ofﬂine learning from demon-\nstrations and unlabeled experience,” arXiv preprint arXiv:2011.13885,\n2020.\n[69] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework\nfor contrastive learning of visual representations,” in International\nconference on machine learning.\nPMLR, 2020, pp. 1597–1607.\n[70] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast\nfor unsupervised visual representation learning,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, 2020,\npp. 9729–9738.\n[71] C. Doersch, A. Gupta, and A. A. Efros, “Unsupervised visual repre-\nsentation learning by context prediction,” in Proceedings of the IEEE\ninternational conference on computer vision, 2015, pp. 1422–1430.\n[72] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” arXiv preprint arXiv:1807.03748, 2018.\n[73] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[74] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models\nare few-shot learners,” Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[75] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., “Palm: Scaling\nlanguage modeling with pathways,” arXiv preprint arXiv:2204.02311,\n2022.\n[76] J. Pari, N. M. Shaﬁullah, S. P. Arunachalam, and L. Pinto, “The\nsurprising effectiveness of representation learning for visual imitation,”\narXiv preprint arXiv:2112.01511, 2021.\n[77] M. Yang and O. Nachum, “Representation matters: ofﬂine pretraining\nfor sequential decision making,” in International Conference on Machine\nLearning.\nPMLR, 2021, pp. 11 784–11 794.\n[78] A. Zhan, R. Zhao, L. Pinto, P. Abbeel, and M. Laskin, “Learning\nvisual robotic control efﬁciently with contrastive pre-training and\ndata augmentation,” in 2022 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS).\nIEEE, 2022, pp. 4040–4047.\n[79] R. Shah and V. Kumar, “Rrl: Resnet as representation for reinforcement\nlearning,” arXiv preprint arXiv:2107.03380, 2021.\n[80] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, “R3m: A\nuniversal visual representation for robot manipulation,” arXiv preprint\narXiv:2203.12601, 2022.\n[81] C. Wang, X. Luo, K. Ross, and D. Li, “Vrl3: A data-driven\nframework for visual deep reinforcement learning,” arXiv preprint\narXiv:2202.10324, 2022.\n[82] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell,\n“Real-world robot learning with masked visual pre-training,” arXiv\npreprint arXiv:2210.03109, 2022.\n13\n[83] Y. Duan, M. Andrychowicz, B. Stadie, O. Jonathan Ho, J. Schneider,\nI. Sutskever, P. Abbeel, and W. Zaremba, “One-shot imitation learning,”\nAdvances in neural information processing systems, vol. 30, 2017.\n[84] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine, “One-shot visual\nimitation learning via meta-learning,” in Conference on robot learning.\nPMLR, 2017, pp. 357–368.\n[85] D. Pathak, P. Mahmoudieh, G. Luo, P. Agrawal, D. Chen, Y. Shentu,\nE. Shelhamer, J. Malik, A. A. Efros, and T. Darrell, “Zero-shot visual\nimitation,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition workshops, 2018, pp. 2050–2053.\n[86] T. Yu, C. Finn, A. Xie, S. Dasari, T. Zhang, P. Abbeel, and S. Levine,\n“One-shot imitation from observing humans via domain-adaptive meta-\nlearning,” arXiv preprint arXiv:1802.01557, 2018.\n[87] C. Lynch and P. Sermanet, “Language conditioned imitation learning\nover unstructured data,” arXiv preprint arXiv:2005.07648, 2020.\n[88] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou,\n“Chain of thought prompting elicits reasoning in large language models,”\narXiv preprint arXiv:2201.11903, 2022.\n[89] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov,\nG. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg\net al., “A generalist agent,” arXiv preprint arXiv:2205.06175, 2022.\n[90] M. Laskin, L. Wang, J. Oh, E. Parisotto, S. Spencer, R. Steiger-\nwald, D. Strouse, S. Hansen, A. Filos, E. Brooks et al., “In-context\nreinforcement learning with algorithm distillation,” arXiv preprint\narXiv:2210.14215, 2022.\n[91] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski,\nW. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver, “Rainbow:\nCombining improvements in deep reinforcement learning,” in Thirty-\nsecond AAAI conference on artiﬁcial intelligence, 2018.\n[92] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver,\nand D. Wierstra, “Continuous control with deep reinforcement learning,”\narXiv preprint arXiv:1509.02971, 2015.\n[93] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approxi-\nmation error in actor-critic methods,” in International conference on\nmachine learning.\nPMLR, 2018, pp. 1587–1596.\n[94] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in International conference on machine learning.\nPMLR, 2018,\npp. 1861–1870.\n[95] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot,\nD. Horgan, J. Quan, A. Sendonaris, I. Osband et al., “Deep q-learning\nfrom demonstrations,” in Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, vol. 32, no. 1, 2018.\n[96] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney,\n“Recurrent experience replay in distributed reinforcement learning,” in\nInternational conference on learning representations, 2018.\n[97] T. L. Paine, C. Gulcehre, B. Shahriari, M. Denil, M. Hoffman, H. Soyer,\nR. Tanburn, S. Kapturowski, N. Rabinowitz, D. Williams et al., “Making\nefﬁcient use of demonstrations to solve hard exploration problems,”\narXiv preprint arXiv:1909.01387, 2019.\n[98] M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess,\nT. Rothörl, T. Lampe, and M. Riedmiller, “Leveraging demonstrations for\ndeep reinforcement learning on robotics problems with sparse rewards,”\narXiv preprint arXiv:1707.08817, 2017.\n[99] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel,\n“Overcoming exploration in reinforcement learning with demonstrations,”\nin 2018 IEEE international conference on robotics and automation\n(ICRA).\nIEEE, 2018, pp. 6292–6299.\n[100] B. Kang, Z. Jie, and J. Feng, “Policy optimization with demonstrations,”\nin International conference on machine learning.\nPMLR, 2018, pp.\n2469–2478.\n[101] B. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey et al., “Maximum\nentropy inverse reinforcement learning.” in Aaai, vol. 8.\nChicago, IL,\nUSA, 2008, pp. 1433–1438.\n[102] S. Reddy, A. D. Dragan, and S. Levine, “Sqil: Imitation learn-\ning via reinforcement learning with sparse rewards,” arXiv preprint\narXiv:1905.11108, 2019.\n[103] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel,\n“Benchmarking deep reinforcement learning for continuous control,”\nin International conference on machine learning.\nPMLR, 2016, pp.\n1329–1338.\n[104] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger,\n“Deep reinforcement learning that matters,” in Proceedings of the AAAI\nconference on artiﬁcial intelligence, vol. 32, no. 1, 2018.\n[105] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust\nregion policy optimization,” in International conference on machine\nlearning.\nPMLR, 2015, pp. 1889–1897.\n[106] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[107] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman,\nE. Todorov, and S. Levine, “Learning complex dexterous manipulation\nwith deep reinforcement learning and demonstrations,” arXiv preprint\narXiv:1709.10087, 2017.\n[108] Y. Aytar, T. Pfaff, D. Budden, T. Paine, Z. Wang, and N. De Freitas,\n“Playing hard exploration games by watching youtube,” Advances in\nneural information processing systems, vol. 31, 2018.\n[109] L. Smith, N. Dhawan, M. Zhang, P. Abbeel, and S. Levine, “Avid:\nLearning multi-stage tasks via pixel-level translation of human videos,”\narXiv preprint arXiv:1912.04443, 2019.\n[110] T. Brys, A. Harutyunyan, H. B. Suay, S. Chernova, M. E. Taylor, and\nA. Nowé, “Reinforcement learning from demonstration through shaping,”\nin Twenty-fourth international joint conference on artiﬁcial intelligence,\n2015.\n[111] A. Y. Ng, D. Harada, and S. Russell, “Policy invariance under reward\ntransformations: Theory and application to reward shaping,” in Icml,\nvol. 99, 1999, pp. 278–287.\n[112] Z. Wu, W. Lian, V. Unhelkar, M. Tomizuka, and S. Schaal, “Learning\ndense rewards for contact-rich manipulation tasks,” in 2021 IEEE\nInternational Conference on Robotics and Automation (ICRA).\nIEEE,\n2021, pp. 6214–6221.\n[113] S. Kumar, J. Zamora, N. Hansen, R. Jangir, and X. Wang, “Graph\ninverse reinforcement learning from diverse videos,” arXiv preprint\narXiv:2207.14299, 2022.\n[114] X. B. Peng, P. Abbeel, S. Levine, and M. Van de Panne, “Deepmimic:\nExample-guided deep reinforcement learning of physics-based character\nskills,” ACM Transactions On Graphics (TOG), vol. 37, no. 4, pp. 1–14,\n2018.\n[115] X. B. Peng, E. Coumans, T. Zhang, T.-W. Lee, J. Tan, and S. Levine,\n“Learning agile robotic locomotion skills by imitating animals,” arXiv\npreprint arXiv:2004.00784, 2020.\n[116] X. B. Peng, Y. Guo, L. Halper, S. Levine, and S. Fidler, “Ase: Large-\nscale reusable adversarial skill embeddings for physically simulated\ncharacters,” arXiv preprint arXiv:2205.01906, 2022.\n[117] S. Arora and P. Doshi, “A survey of inverse reinforcement learning:\nChallenges, methods and progress,” Artiﬁcial Intelligence, vol. 297, p.\n103500, 2021.\n[118] A. Y. Ng, S. Russell et al., “Algorithms for inverse reinforcement\nlearning.” in Icml, vol. 1, 2000, p. 2.\n[119] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse rein-\nforcement learning,” in Proceedings of the twenty-ﬁrst international\nconference on Machine learning, 2004, p. 1.\n[120] M. L. Puterman, Markov decision processes: discrete stochastic dynamic\nprogramming.\nJohn Wiley & Sons, 2014.\n[121] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson,\n“Discriminator-actor-critic: Addressing sample inefﬁciency and reward\nbias in adversarial imitation learning,” arXiv preprint arXiv:1809.02925,\n2018.\n[122] J. Fu, K. Luo, and S. Levine, “Learning robust rewards with adversarial\ninverse reinforcement learning,” arXiv preprint arXiv:1710.11248, 2017.\n[123] S. K. S. Ghasemipour, R. Zemel, and S. Gu, “A divergence minimization\nperspective on imitation learning methods,” in Conference on Robot\nLearning.\nPMLR, 2020, pp. 1259–1277.\n[124] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,”\nCommunications of the ACM, vol. 63, no. 11, pp. 139–144, 2020.\n[125] R. Dadashi, L. Hussenot, M. Geist, and O. Pietquin, “Primal wasserstein\nimitation learning,” arXiv preprint arXiv:2006.04678, 2020.\n[126] I.-A. Hosu and T. Rebedea, “Playing atari games with deep rein-\nforcement learning and human checkpoint replay,” arXiv preprint\narXiv:1607.05077, 2016.\n[127] T. Salimans and R. Chen, “Learning montezuma’s revenge from a single\ndemonstration,” arXiv preprint arXiv:1812.03381, 2018.\n[128] Y. Zhu, Z. Wang, J. Merel, A. Rusu, T. Erez, S. Cabi, S. Tunyasuvu-\nnakool, J. Kramár, R. Hadsell, N. de Freitas et al., “Reinforcement\nand imitation learning for diverse visuomotor skills,” arXiv preprint\narXiv:1802.09564, 2018.\n[129] H. Fu, W. Xu, H. Xue, H. Yang, R. Ye, Y. Huang, Z. Xue, Y. Wang, and\nC. Lu, “Rfuniverse: A physics-based action-centric interactive environ-\nment for everyday household tasks,” arXiv preprint arXiv:2202.00199,\n2022.\n[130] Y. Zhu, J. Wong, A. Mandlekar, and R. Martín-Martín, “robosuite:\nA modular simulation framework and benchmark for robot learning,”\narXiv preprint arXiv:2009.12293, 2020.\n14\n[131] B. Shen, F. Xia, C. Li, R. Martín-Martín, L. Fan, G. Wang, C. Pérez-\nD’Arpino, S. Buch, S. Srivastava, L. Tchapmi et al., “igibson 1.0: a\nsimulation environment for interactive tasks in large realistic scenes,”\nin 2021 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS).\nIEEE, 2021, pp. 7520–7527.\n[132] A. Mandlekar, Y. Zhu, A. Garg, J. Booher, M. Spero, A. Tung, J. Gao,\nJ. Emmons, A. Gupta, E. Orbay et al., “Roboturk: A crowdsourcing\nplatform for robotic skill learning through imitation,” in Conference on\nRobot Learning.\nPMLR, 2018, pp. 879–893.\n[133] V. Kumar and E. Todorov, “Mujoco haptix: A virtual reality system for\nhand manipulation,” in 2015 IEEE-RAS 15th International Conference\non Humanoid Robots (Humanoids).\nIEEE, 2015, pp. 657–663.\n[134] C. Li, F. Xia, R. Martín-Martín, M. Lingelbach, S. Srivastava, B. Shen,\nK. Vainio, C. Gokmen, G. Dharan, T. Jain et al., “igibson 2.0: Object-\ncentric simulation for robot learning of everyday household tasks,” arXiv\npreprint arXiv:2108.03272, 2021.\n[135] W. Wang, Y. Suga, H. Iwata, and S. Sugano, “Openvr: A software tool\ncontributes to research of robotics,” in 2011 IEEE/SICE International\nSymposium on System Integration (SII).\nIEEE, 2011, pp. 1043–1048.\n[136] X. Puig, T. Shu, S. Li, Z. Wang, Y.-H. Liao, J. B. Tenenbaum, S. Fidler,\nand A. Torralba, “Watch-and-help: A challenge for social perception\nand human-ai collaboration,” arXiv preprint arXiv:2010.09890, 2020.\n[137] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi,\nL. Zettlemoyer, and D. Fox, “Alfred: A benchmark for interpreting\ngrounded instructions for everyday tasks,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition,\n2020, pp. 10 740–10 749.\n[138] J. Hoffmann and B. Nebel, “The ff planning system: Fast plan generation\nthrough heuristic search,” Journal of Artiﬁcial Intelligence Research,\nvol. 14, pp. 253–302, 2001.\n[139] J. J. Kuffner and S. M. LaValle, “Rrt-connect: An efﬁcient approach to\nsingle-query path planning,” in Proceedings 2000 ICRA. Millennium\nConference. IEEE International Conference on Robotics and Automation.\nSymposia Proceedings (Cat. No. 00CH37065), vol. 2.\nIEEE, 2000, pp.\n995–1001.\n[140] R. Bohlin and L. E. Kavraki, “Path planning using lazy prm,” in\nProceedings 2000 ICRA. Millennium conference. IEEE international\nconference on robotics and automation. Symposia proceedings (Cat.\nNo. 00CH37065), vol. 1.\nIEEE, 2000, pp. 521–528.\n[141] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, “Rlbench: The\nrobot learning benchmark & learning environment,” IEEE Robotics and\nAutomation Letters, vol. 5, no. 2, pp. 3019–3026, 2020.\n[142] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine, “D4rl:\nDatasets for deep data-driven reinforcement learning,” arXiv preprint\narXiv:2004.07219, 2020.\n[143] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang,\nD. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke et al., “Scalable\ndeep reinforcement learning for vision-based robotic manipulation,” in\nConference on Robot Learning.\nPMLR, 2018, pp. 651–673.\n[144] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jon-\nschkowski, C. Finn, S. Levine, and K. Hausman, “Mt-opt: Continuous\nmulti-task robotic reinforcement learning at scale,” arXiv preprint\narXiv:2104.08212, 2021.\n[145] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar,\nJ. Hamburger, H. Jiang, M. Liu, X. Liu et al., “Ego4d: Around the world\nin 3,000 hours of egocentric video,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2022, pp.\n18 995–19 012.\n[146] haoyu Xiong, H. Fu, J. Zhang, C. Bao, Q. Zhang, Y. Huang, W. Xu,\nA. Garg, and C. Lu, “Robotube: Learning household manipulation\nfrom human videos with simulated twin environments,” in 6th\nAnnual Conference on Robot Learning, 2022. [Online]. Available:\nhttps://openreview.net/forum?id=VD0nXUG5Qk\n[147] J. Gu, F. Xiang, X. Li, Z. Ling, X. Liu, T. Mu, Y. Tang, S. Tao,\nX. Wei, Y. Yao, X. Yuan, P. Xie, Z. Huang, R. Chen, and H. Su,\n“Maniskill2: A uniﬁed benchmark for generalizable manipulation skills,”\nin International Conference on Learning Representations, 2023.\n[148] S. Fujimoto and S. S. Gu, “A minimalist approach to ofﬂine reinforce-\nment learning,” Advances in neural information processing systems,\nvol. 34, pp. 20 132–20 145, 2021.\n[149] H. Shen, W. Wan, and H. Wang, “Learning category-level generalizable\nobject manipulation policy via generative adversarial self-imitation\nlearning from demonstrations,” arXiv preprint arXiv:2203.02107, 2022.\n[150] Y. Guo, J. Oh, S. Singh, and H. Lee, “Generative adversarial self-\nimitation learning,” arXiv preprint arXiv:1812.00950, 2018.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2023-03-23",
  "updated": "2023-03-27"
}