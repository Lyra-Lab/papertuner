{
  "id": "http://arxiv.org/abs/2303.14376v1",
  "title": "ViPFormer: Efficient Vision-and-Pointcloud Transformer for Unsupervised Pointcloud Understanding",
  "authors": [
    "Hongyu Sun",
    "Yongcai Wang",
    "Xudong Cai",
    "Xuewei Bai",
    "Deying Li"
  ],
  "abstract": "Recently, a growing number of work design unsupervised paradigms for point\ncloud processing to alleviate the limitation of expensive manual annotation and\npoor transferability of supervised methods. Among them, CrossPoint follows the\ncontrastive learning framework and exploits image and point cloud data for\nunsupervised point cloud understanding. Although the promising performance is\npresented, the unbalanced architecture makes it unnecessarily complex and\ninefficient. For example, the image branch in CrossPoint is $\\sim$8.3x heavier\nthan the point cloud branch leading to higher complexity and latency. To\naddress this problem, in this paper, we propose a lightweight\nVision-and-Pointcloud Transformer (ViPFormer) to unify image and point cloud\nprocessing in a single architecture. ViPFormer learns in an unsupervised manner\nby optimizing intra-modal and cross-modal contrastive objectives. Then the\npretrained model is transferred to various downstream tasks, including 3D shape\nclassification and semantic segmentation. Experiments on different datasets\nshow ViPFormer surpasses previous state-of-the-art unsupervised methods with\nhigher accuracy, lower model complexity and runtime latency. Finally, the\neffectiveness of each component in ViPFormer is validated by extensive ablation\nstudies. The implementation of the proposed method is available at\nhttps://github.com/auniquesun/ViPFormer.",
  "text": "ViPFormer: Efﬁcient Vision-and-Pointcloud Transformer for\nUnsupervised Pointcloud Understanding\nHongyu Sun, Yongcai Wang, Xudong Cai, Xuewei Bai and Deying Li\nAbstract— Recently, a growing number of work design unsu-\npervised paradigms for point cloud processing to alleviate the\nlimitation of expensive manual annotation and poor transfer-\nability of supervised methods. Among them, CrossPoint follows\nthe contrastive learning framework and exploits image and\npoint cloud data for unsupervised point cloud understand-\ning. Although the promising performance is presented, the\nunbalanced architecture makes it unnecessarily complex and\ninefﬁcient. For example, the image branch in CrossPoint is\n∼8.3x heavier than the point cloud branch leading to higher\ncomplexity and latency. To address this problem, in this paper,\nwe propose a lightweight Vision-and-Pointcloud Transformer\n(ViPFormer) to unify image and point cloud processing in\na single architecture. ViPFormer learns in an unsupervised\nmanner by optimizing intra-modal and cross-modal contrastive\nobjectives. Then the pretrained model is transferred to various\ndownstream tasks, including 3D shape classiﬁcation and se-\nmantic segmentation. Experiments on different datasets show\nViPFormer surpasses previous state-of-the-art unsupervised\nmethods with higher accuracy, lower model complexity and\nruntime latency. Finally, the effectiveness of each component\nin ViPFormer is validated by extensive ablation studies. The\nimplementation of the proposed method is available at https:\n//github.com/auniquesun/ViPFormer.\nI. INTRODUCTION\nPoint cloud understanding is a crucial problem which has\nattracted widespread attention for its values in autonomous\ndriving, mixed reality, and robotics. There are three common\ntasks in point cloud understanding: 3D object classiﬁca-\ntion [1], [2], semantic segmentation [3], [4], [5] and object\ndetection [6], [7], [8], [9]. A large portion of previous\nmethods design different neural networks and learn from\nlarge-scale annotated data for point cloud understanding\ntasks. However, point cloud labels are rare in most scenarios\nand acquiring them is time-consuming and expensive.\nHence, in recent years, researchers have begun to shift\ntheir attention to developing unsupervised methods for point\ncloud understanding, without the need of hand-crafted an-\nnotations. Unsupervised methods are designed in different\nways, such as auto-encoders [10], mask auto-encoders [11],\n[12], reconstruction [13], occlusion completion [14], [15],\nGANs [16], [17], [18] and contrastive learning [19], [20],\n[21], [22], [23], etc.\nCurrently, a growing number of methods embrace con-\ntrastive learning because it is a simple yet effective frame-\nAll authors are with the Department of Computer Science, School of In-\nformation, Renmin University of China, Beijing 100872, China. Correspond-\ning author: Yongcai Wang. {sunhongyu, ycw, xudongcai,\nbai xuewei, deyingli}@ruc.edu.cn\nThis work was supported in part by the National Natural Science\nFoundation of China under Grants No. 61972404 and No. 12071478, and\nPublic Computing Cloud, Renmin University of China.\nFig. 1: Comparison of classiﬁcation accuracy and the number\nof parameters of different unsupervised methods on Model-\nNet40 (M) and ScanObjectNN (S), respectively.\nwork and has shown improvements in vision [24] and\nlanguage processing [25]. This framework can be easily\nextended to incorporate multi-modal data to further exploit\nricher semantics in multi-modal data to improve perfor-\nmances. Inspired by the success of vision + language [26],\n[27] and video + audio [28], [29], point cloud understanding\npowered by cross-modal data has drawn research interests.\nCrossPoint [22] takes images and point clouds as inputs\nand follows the contrastive framework for unsupervised point\ncloud processing. It utilizes ResNet50 [30] as image feature\nextractor and PointNet [31]/DGCNN [32] as point cloud\nfeature extractor. [33] uses CNN and U-Net [34] architecture\nin the image branch and PointNet++ [35] architecture in\nthe point cloud branch for contrastive learning. Although\npromising performances are obtained, the unbalanced image\nand point cloud processing architecture makes them unneces-\nsarily complex and inefﬁcient. For example, in [22], the point\ncloud branch has 3M parameters while the image branch has\n25M. The image processing branch is ∼8.3x heavier than the\npoint cloud one and consumes much more time.\nThe different and unbalanced architecture when dealing\nwith data from different modalities is often neglected in\nacademic studies but is a critical problem in practice because\nit severely hinders efﬁciency. However, it is possible to\ndesign a uniﬁed and efﬁcient architecture to process cross-\nmodal data since Transformer [36] has shown the ﬂexibility\nand superiority in vision [37], [38], [24] and language [39],\n[40], [41] modeling. And recently, Point-BERT [11] and\nPoint-MAE [42] show point clouds can be sampled to groups\nthen processed by Transformer and the performances are\npromising.\nIn this\npaper, we propose an\nefﬁcient Vision-and-\nPointcloud Transformer (ViPFormer) for unsupervised point\ncloud understanding. ViPFormer uniﬁes image and point\narXiv:2303.14376v1  [cs.CV]  25 Mar 2023\ncloud processing in a single architecture, which ensures the\ntwo branches have the same size and complexity. Then it\nfollows the contrastive learning framework to optimize image\nand point cloud feature representations. Finally, the learned\nrepresentations are transferred to target tasks like 3D point\ncloud classiﬁcation and semantic segmentation.\nViPFormer is evaluated on a wide range of point cloud\nunderstanding tasks, including synthetic and real-world 3D\nshape classiﬁcation, object part segmentation. Results show\nit not only reduces the model complexity and running latency\nbut also outperforms all existing unsupervised methods. The\nmajor contributions of this paper are summarized as follows:\n• We propose ViPFormer, handling image and point cloud\ndata in a uniﬁed architecture, simpliﬁes the model\ncomplexity, reduces running latency and boosts overall\nperformances for unsupervised point cloud understand-\ning.\n• We show that ViPFormer can be generalized better\nto different tasks by simultaneously optimizing intra-\nmodal and cross-modal contrastive objectives.\n• The proposed method is validated on various down-\nstream tasks, e.g., it achieves 90.7% classiﬁcation accu-\nracy on ScanObjectNN, leading CrossPoint by 9%, and\nsurpassing the previous best performing unsupervised\nmethod by 0.7% with ∼77% fewer parameters. Simi-\nlarly, ViPFormer reaches a 93.9% score on ModelNet40,\noutperforming the previous state-of-the-art method and\nreducing the number of parameters by 24%.\n• We conduct extensive ablation studies to clarify the\nadvantages of the architecture design, contrastive opti-\nmization objectives, and unsupervised learning strategy.\nII. RELATED WORK\nUnsupervised Point Cloud Understanding Unsupervised\nlearning becomes more and more popular since it can unleash\nthe potential of large-scale unlabeled data and save consid-\nerable costs. According to the pretext task, unsupervised\nmethods for point cloud understanding can be classiﬁed\ninto generative models and discriminative models. Generative\nmodels usually learn the latent representations of point\nclouds by predicting some parts or all of the input data. The\nassumption is that only after the model understands the point\ncloud can it predict the occluded parts or generate the entire\npoint cloud. Auto-encoders like FoldingNet [10], GANs like\nLRGM [17], URL [18] and 3D GAN [16], reconstruction\nmethods like JigSaw [13], cTree [43], all of them generate\na whole point cloud and maximize the similarity with the\ninput point cloud. Mask encoders like OcCo [14], Point-\nBERT [11], Point-MAE [12] complete the masked parts of\na point cloud to keep it same as the input. On the other\nhand, the discriminative models aim to learn discriminative\nfeatures from different object/semantic categories. Most of\nthem follow the contrastive learning framework [19], [20],\n[21], [44], [23], [22], where CrossPoint [22] is the most\nrelevant to us since it also fuses cross-modal data, images\nand point clouds, for point cloud understanding. However,\nthe unbalanced feature extractors in CrossPoint caused much\nhigher running latency and model complexity. Instead, we\npropose Vision-and-Pointcloud Transformer to unify image\nand point cloud processing in a single architecture, reduce\nlatency and boost performance.\nArchitecture for Image and Pointcloud Processing An\nimage consists of regular and dense pixel grids, while a\npoint cloud is a set of irregular, sparse and unordered points.\nThe huge differences make it difﬁcult to process images\nand point clouds in the same way. Researchers developed\ndifferent architectures for image and point cloud process-\ning. In many cases, CNNs are the ﬁrst choices of image\nprocessing and PointNet [31] and its variants [35], [45] are\ngood starts for point cloud processing. However, the situation\nhas changed since the advent of Transformer [36]. Due to\nthe notable improvements, Transformer quickly became the\nde facto standard architecture for language understanding\ntasks [39], [40], [41] then entered vision [46], [37], [38],\n[24] and 3D ﬁeld [47], [48], [49], [50], [42]. Guo et al. and\nZhao et al. proposed PCT [51] and Point Transformer [4],\nrespectively, but their architectures were different from the\nstandard Transformer [36] and can not be generalized to\nvision modality. Perceiver [52] and PerceiverIO [53] have\ntaken important steps toward general architecture for various\nmodalities (audio, image, point cloud). However, Perceiver\nand PerceiverIO learn in a supervised fashion. Differently,\nthe proposed ViPFormer uniﬁes image and point cloud\nprocessing in a single architecture and learns from large-\nscale unlabeled data.\nIII. METHODOLOGY\nIn this section, ﬁrstly, we introduce the overall architecture\nof ViPFormer. Secondly, we elaborate on its unsupervised\nlearning strategy.\nA. The Overall Architecture of ViPFormer\nAs Fig. 2 shows, ViPFormer consists of three components,\nwhich are a lightweight Input Adapter, Transformer Encoder\nand Output Adapter. In image and point cloud branches,\nmodules with the same color are identical. And the images\nand point clouds are serialized in different ways.\nImage and Point Cloud Preparing To exploit the power\nof Transformer [36] we need to convert images and point\nclouds into sequence data as Transformer requires. Inspired\nby ViT [37], we divide an image into small patches and\nthen ﬂatten them into a sequence. For example, an image\nI is of size H × W × C1 and the patch size is Q, we can\ngenerate an image patch sequence xi ∈RM×(Q2·C1), where\nM = HW/Q2.\nFor a point cloud P of size N × C2, we convert it into a\npatch sequence as follows [11], [12]. First, the farthest point\nsampling (FPS) is applied to P to get G centers. Second, for\neach center, we search its k nearest neighbors (kNN) in P\nto aggregate local geometry information, resulting in a patch\nsequence xp ∈RG×(k·C2).\nInput Adapter We design a lightweight image patch\nadapter EI and a point patch adapter EP to project the\nsequences to high dimensional feature representations. EI\nFig. 2: The overall architecture of ViPFormer.\n∈R(Q2·C1)×D is a linear layer and EP is a multi-layer\nperception (MLP). The outputs are denoted as image patch\nembeddings zi and point patch embeddings zp.\nzi = xiEI,\nzp = xpEP\n(1)\nBefore being fed into Encoder, the position information is\ninjected to zi and zp by adding the image patch position\nembeddings Epos\nI\nand point patch position embeddings Epos\nP .\nzi = zi + Epos\nI\n,\nzp = zp + Epos\nP\n(2)\nEncoder The image and point cloud branches share\nthe Encoder architecture, which ensures image and point\ncloud processing have balanced complexity and low latency.\nViPFormer Encoder consists of L stacked multi-head self-\nattention (MSA) and MLP layers. MLP has two layers with\na GELU non-linear activation. LayerNorm (LN) is applied\nbefore MSA and MLP layers, while Dropout is applied after\nthem.\nˆzl = Dropout(MSA(LN(zl−1))) + zl−1,\nl = 1...L\n(3)\nzl = Dropout(MLP(LN(ˆzl))) + ˆzl,\nl = 1...L\n(4)\nBefore proceeding, the output sequence of ViPFormer En-\ncoder needs to be converted into an object-level feature. We\nimplement it by concatenating the max and mean value of\nzL.\nr = Concat(Max(zL), Mean(zL))\n(5)\nOutput Adapter The image and point cloud branches also\nshare the Output Adapter. As suggested by SimCLR [54],\na learnable nonlinear transformation between the encoder\nand the contrastive loss can improve the quality of feature\nrepresentations. The Output Adapter is implemented by two\nconsecutive Linear layers, preceding with BatchNorm (BN)\nand ReLU.\nˆr = Linear(ReLU(BN(r)))\n(6)\no = Linear(ReLU(BN(ˆr)))\n(7)\nAt this point, the input image I and point cloud P are\ntransformed into image feature f = oI and point cloud\nfeature p = oP . We can use these features for unsupervised\ncontrastive learning.\nB. Unsupervised Contrastive Pretraining of ViPFormer\nWe conduct unsupervised pretraining for ViPFormer by\nintroducing two contrastive objectives, intra-modal contrast\nand cross-modal contrast. They are formulated as follows.\nThe Intra-Modal Contrastive (IMC) Objective injects\nViPFormer with the ability to resist data transformations and\nsmall perturbations (e.g., translation, rotation, jittering) to\nthe same objects while maximizing the distance of different\nobjects in feature space. This strategy will make the pre-\ntrained model insensitive to random noises and generalize\nbetter. Speciﬁcally, a point cloud P is transformed by two\ndata augmentations t1 and t2, resulting in P t1 and P t2.\nAfter going through ViPFormer, their feature representations\npt1=ot1\nP and pt2=ot2\nP are obtained. The IMC objective Limc\nis formulated by NT-Xent loss [54]:\nl(i,t1,t2)=−log\nexp(s(pt1\ni , pt2\ni )/τ)\nN\nP\nk=1\nk̸=i\nexp(s(pt1\ni ,pt1\nk )/τ)+\nN\nP\nk=1\nexp(s(pt1\ni ,pt2\nk )/τ)\n(8)\nLimc =\n1\n2N\nN\nX\ni=1\n(l(i, t1, t2) + l(i, t2, t1))\n(9)\nwhere N is the batch size, τ is the temperature coefﬁcient\nand s(·) represents the cosine similarity.\nThe Cross-Modal Contrastive (CMC) Objective max-\nimizes the agreement of feature representations of paired\nimages and point clouds, while minimizing that of unpaired\nones in the same feature space. ViPFormer achieves this goal\nonly when it understands the information contained in both\nmodalities. Similarly, the CMC objective Lcmc is formulated\nby NT-Xent loss [54]:\nl(i, p, f)=−log\nexp(s(pi, fi)/τ)\nN\nP\nk=1\nk̸=i\nexp(s(pi, pk)/τ)+\nN\nP\nk=1\nexp(s(pi, fk)/τ)\n(10)\nLcmc =\n1\n2N\nN\nX\ni=1\n(l(i, p, f) + l(i, f, p))\n(11)\nN, τ and s(·) have the same meaning as those in Eq. 8.\nDuring pretraining, ViPFormer combines IMC and CMC\nas the overall loss. A balancing factor α is deployed between\ntwo objectives as the cross-modal loss Lcmc is harder to\noptimize and usually several times bigger than the intra-\nmodal loss Limc.\nL = Limc + αLcmc\n(12)\nIV. EXPERIMENTS\nIn this section, ﬁrstly, we elaborate on the pretraining\nsettings of ViPFormer. Then the pretrained ViPFormer is\ntransferred to various downstream tasks to evaluate its perfor-\nmances. Thirdly, the effectiveness of different components in\nViPFormer is validated by extensive ablation studies. Finally,\nthe predictions of ViPFormer on different tasks are visualized\nfor a better understanding.\nA. Pretraining Settings\nDatasets We use the same dataset as in [22]. The point\nclouds and images come from ShapeNet [55] and DISN [56].\nThere are 43,783 point clouds and each point cloud P\ncorresponds to 24 rendered images, from which an image\nI is randomly selected to pair with P. During pretraining,\npoint cloud P contains 2048 points and the corresponding\nimage I is resized to 144×144×3. In FPS and kNN, the\npoint cloud is divided into G=128 centers and k=32 nearest\nneighbors are retrieved for each center. The image patch size\nQ is set to 12.\nArchitecture\nIn\nInput\nAdapter,\nthe\ndimension\nof\npoint/image patch embedding is projected to 384. In Encoder,\nthere are L=9 stacked MSA and MLP layers. All MSA layers\nhave 6 heads. The widening ratio of the MLP hidden layer\nis 4. In Output Adapter, the 2-layer MLP is of size {768,\n384, 384}. We justify the design choices of the architecture\nthrough controlled experiments in Section IV-C.1.\nOptimization We pretrain ViPFormer for 300 epochs,\nadopting AdamW [57] as the optimizer and CosineAnneal-\ningWarmupRestarts [58] as the learning rate scheduler. The\nrestart interval is 100 epochs and the warmup period is\nthe ﬁrst 5 epochs. The learning rate scales linearly to peak\nduring each warmup, then decays with the cosine annealing\nschedule. The initial learning rate peak is 0.001, multiplied\nby 0.6 after each interval. The balancing factor α is set to\n1, which works well. We record the best pretrained model\naccording to the zero-shot accuracy on ModelNet40 [59].\nB. Model Complexity, Latency and Performance on Down-\nstream Tasks\nIn this part, the pretrained ViPFormer is transferred to\nvarious downstream tasks to evaluate its complexity, latency\nand performance. These metrics are critical dimensions for\nassessing point cloud understanding methods. Complexity\nis reﬂected by a model’s number of parameters (#Params).\nLatency is counted by running time and performance is\nsubject to overall accuracy (OA) in the classiﬁcation task\nand mean class-wise Intersection of Union (mIoU) in the\nsegmentation task. We compare with previous state-of-the-\nart unsupervised methods.\nPoint Cloud Classiﬁcation The experiments are con-\nducted on two widely used datasets: ScanObjectNN [60]\nand ModelNet40 [59]. ScanObjectNN contains 2880 objects\nfrom 15 categories. It is challenging because objects in this\ndataset are usually cluttered with background or are partial\ndue to occlusions. ModelNet40 is a synthetic point cloud\ndataset, including 12308 objects across 40 categories. We use\nthe same settings as previous work [19], [20], [14], [22] to\nsample 1024 points to represent a 3D object. We reimplement\nprevious methods according to the released codes since they\ndo not report the #Params and latency. For latency, we\nconsider two stages (Pretrain and Finetune) and count the\nrunning time of a single epoch in each stage.\nFor the ScanObjectNN [60] dataset, all methods are ﬁne-\ntuned on it and the results are recorded in Tab. I. The best\nscore is in bold black and the second best score is marked in\nblue. ViPFormer not only outperforms previous state-of-the-\nart Point-MAE by 0.7% in classiﬁcation accuracy but also\nreduces 76.9% #Params and runs ∼2.6x faster than Point-\nMAE.\nTABLE I: Comparison of model complexity, latency and\nperformance with existing unsupervised methods on ScanOb-\njectNN.\nMethod\n#Params\nLatency\nOA\nPretrain\nFinetune\n(M)\n(s)\n(ms)\n(%)\nFoldingNet [10]\n2.0\n–\n–\n81.0\nPointContrast [19]\n37.9\n–\n–\n79.6\nDepthContrast [20]\n8.2\n–\n–\n80.4\nOcCo [14]\n3.5\n∼600.0\n16,100\n83.3\nCrossPoint [22]\n27.7\n946.0\n14,000\n81.7\nPoint-BERT [11]\n39.1\n633.5\n3,973\n87.4\nPoint-MAE [12]\n22.1\n576.0\n3,612\n90.0\nViPFormer\n5.1\n22.2\n1,015\n90.7\nThe classiﬁcation results on ModelNet40 are shown in\nTable II. The Pretrain latency is not changed because pre-\ntraining is independent of the downstream datasets, including\nScanObjectNN and ModelNet40. ModelNet40 is a larger\ndataset so ﬁnetuning on it consumes more time. ViPFormer\nachieves higher classiﬁcation accuracy with lower model\ncomplexity and runtime latency. It leads Point-MAE by 0.7%\naccuracy while reducing #Params by 24.1%.\nObject Part Segmentation We also transfer ViPFormer\nto the task of object part segmentation. The experiments\nTABLE II: Comparison of model complexity, latency and\nperformance with existing unsupervised methods on Model-\nNet40.\nMethod\n#Params\nLatency\nOA\nPretrain\nFinetune\n(M)\n(s)\n(ms)\n(%)\nFoldingNet [10]\n2.0\n–\n–\n90.6\nPointContrast [19]\n37.9\n–\n–\n90.0\nDepthContrast [20]\n8.2\n–\n–\n89.2\nOcCo [14]\n3.5\n∼600.0\n39,295\n92.5\nCrossPoint [22]\n27.7\n946.0\n35,258\n90.3\nPoint-BERT [11]\n39.1\n633.5\n10,329\n93.0\nPoint-MAE [12]\n22.1\n576.0\n9,344\n93.2\nViPFormer\n16.7\n60.9\n4,198\n93.9\nare conducted on the ShapeNetPart [61] dataset which con-\ntains 16881 point clouds and each point cloud consists of\n2048 points. Objects in ShapeNetPart are divided into 16\ncategories and 50 annotated parts. For a fair comparison,\nwe follow previous work [11] [12] to add a simple part\nsegmentation head on ViPFormer Encoder. The pretrained\nweights of ViPFormer are used to initialize the part segmen-\ntation model. In addition to the above metrics, mean class-\nwise IoU (mIoU) is added to evaluate the part segmentation\nperformance. The results are reported in Tab. III. ViPFormer\nreaches comparable OA and mIoU with best performing\nPoint-MAE while having lower model complexity.\nTABLE III: Object part segmentation on ShapeNetPart.\nMethod\n#Params\nLatency\nOA\nmIoU\n(M)\n(s)\n(%)\n(%)\nPointContrast [19]\n37.9\n–\n–\n–\nOcCo [14]\n1.5\n32.0\n93.9\n79.7\nCrossPoint [22]\n27.5\n80.0\n93.8\n84.3\nPoint-BERT [11]\n44.1\n58.8\n–\n84.1\nPoint-MAE [12]\n27.1\n46.3\n94.8\n84.7\nViPFormer\n26.8\n42.1\n94.8\n84.7\nFew-shot Object Classiﬁcation Few-shot evaluation is\nused to validate the transferability of a pretrained model\nwith limited labeled data. The conventional setting is “N-\nway, K-shot” [20], [14], [22]. Under this setting, N classes\nand K samples in a downstream task dataset are randomly\nselected for training an SVM model of the linear kernel.\nThe test score on the downstream task given by SVM\ncan reﬂect the quality of the pretrained model as the in-\nputs to the SVM model are the features extracted by the\npretrained model. Here the downstream task datasets are\nModelNet40 and ScanObjectNN, respectively. We perform\n10 runs for each “N-way, K-shot” and compute their mean\nand standard deviation. The results are shown in Table IV.\nOn ModelNet40, ViPFormer achieves comparable accuracy\nwith previous strong baselines, whereas it shows consis-\ntent improvements on ScanObjectNN. The IMC and CMC\nobjectives enable ViPFormer to understand the information\ncontained in both modalities, so it can better deal with the\nchallenging scenarios in ScanObjectNN.\nTABLE IV: Comparison of few-shot classiﬁcation accuracy\nwith existing methods on ModelNet40 and ScanObjectNN.\nMethod\n5-way\n10-way\n10-shot\n20-shot\n10-shot\n20-shot\nModelNet40\nOcCo [14]\n90.6±2.8\n92.5±1.9\n82.9±1.3\n86.5±2.2\nCrossPoint [22]\n91.0±2.9\n95.0±3.4\n82.2±6.5\n87.8±3.0\nViPFormer\n91.1±7.2\n93.4±4.5\n80.8±4.2\n87.1±5.8\nScanObjectNN\nOcCo [14]\n72.4±1.4\n77.2±1.4\n57.0±1.3\n61.6±1.2\nCrossPoint [22]\n72.5±8.3\n79.0±1.2\n59.4±4.0\n67.8±4.4\nViPFormer\n74.2±7.0\n82.2±4.9\n63.5±3.8\n70.9±3.7\nC. Ablation Studies\nAblation studies are conducted to 1) justify the architecture\nof ViPFormer, 2) demonstrate the effectiveness of IMC and\nCMC optimization objectives, and 3) analyze the advantages\nof the pretrain-ﬁnetune strategy over training from scratch.\n1) Architecture: The controlled variables of ViPFormer\narchitecture\nare\nthe\nnumber\nof\nself-attention\nlayers\n(#SA Layers), the widening ratio of the MLP hidden layer\n(MLP ratio), the number of attention heads (#Heads),\nthe sequence length (#Length) and the model dimension\n(D model). For different architectures, the accuracy of the\npretrain-ﬁnetune scheme is reported on ModelNet40 and\nScanObjectNN, respectively, shown in Tab. V. The overall\ntrend is the larger the model, the better the performance.\nWe choose the best-performing architecture to compare with\nother methods.\n2) Contrastive Optimization Objectives: The effectiveness\nof proposed IMC and CMC contrastive objectives are eval-\nuated by training ViPFormer in three modes: i) only use\nIMC, ii) only use CMC, and iii) use IMC and CMC together.\nThe experiments are conducted on different learning stages\n(Pretrain vs. Finetune) and different datasets (ModelNet40\nvs. ScanObjectNN). The results are shown in Tab. VI.\nApparently, the combination of IMC and CMC optimization\nobjectives signiﬁcantly improves the model performance for\ntarget tasks across different datasets.\n3) Learning Strategies:\nThe differences between two\nkinds of learning strategies, Train from scratch and Pretrain-\nFinetune, are also investigated. As Tab. VII shows, The\nPretrain-Finetune strategy outperforms Train from scratch\nby 1.9% and 4.1% on ModelNet40 and ScanObjectNN,\nrespectively. The results indicate the initialization provided\nby the pretrained ViPFormer really helps the model ﬁnd\nbetter directions and solutions in downstream tasks.\nD. Visualization\nObject Part Segmentation We conduct experiments on\nShapeNetPart [61] to visualize the predictions of ViPFormer\nto different object parts. This dataset has 16 object categories\nand we randomly select one object from each category.\nViPFormer predicts part labels for all points in the selected\nobject. Then different part labels are mapped to different\ncolors. As Fig. 3 presents, ViPFormer successfully handles\ndifferent objects and segments their parts in most cases.\nTABLE V: Ablation Study: Model Architecture.\n#SA Layers\n7\n7\n9\n9\n7\n7\n9\n9\n7\n7\n9\n9\n7\n7\n9\n9\nMLP ratio\n2\n2\n2\n2\n4\n4\n4\n4\n2\n2\n2\n2\n4\n4\n4\n4\n#Heads\n4\n4\n4\n4\n4\n4\n4\n4\n6\n6\n6\n6\n6\n6\n6\n6\n#Length\n96\n128\n96\n128\n96\n128\n96\n128\n96\n128\n96\n128\n96\n128\n96\n128\nD model\n256\n256\n256\n256\n256\n256\n256\n256\n384\n384\n384\n384\n384\n384\n384\n384\nAccuracy\nModelNet40\n91.5\n91.5\n91.5\n92.5\n93.2\n91.0\n92.7\n91.3\n93.0\n93.2\n92.0\n92.2\n92.2\n92.2\n93.2\n93.9\nScanObjectNN\n84.5\n83.5\n86.6\n90.7\n85.6\n85.6\n86.6\n87.6\n88.7\n84.5\n84.5\n89.7\n89.7\n88.7\n89.7\n89.7\nTABLE VI: Ablation Study: Performance comparison on\nModelNet40 (M) and ScanObjectNN (S) when using dif-\nferent contrastive objectives.\nModality Types\nPretrain\nFinetune\nOAM\nOAS\nOAM\nOAS\nIMC only\n86.4\n76.4\n91.3\n87.6\nCMC only\n87.3\n66.4\n91.3\n81.4\nIMC & CMC\n87.0\n75.7\n93.9\n89.7\nTABLE VII: Ablation Study: Comparison of the classi-\nﬁcation accuracy using Pretrain-Finetune and Train-from-\nscratch strategy. The used datasets are ModelNet40 (M) and\nScanObjectNN (S).\nLearning Strategy\nOAM\nOAS\nTrain from scratch\n92.0\n85.6\nPretrain-Finetune\n93.9\n89.7\n(a) Airplane\n(b) Chair\n(c) Earphone\n(d) Mug\n(e) Rocket\n(f) Bag\n(g) Guitar\n(h) Lamp\n(i) Knife\n(j) Pistol\n(k) Skateboard\n(l) Motorbike\n(m) Table\n(n) Cap\n(o) Laptop\n(p) Car\nFig. 3: Object part segmentation predictions of ViPFormer\nFeature Distribution The distributions of pretrained and\nﬁnetuned features are visualized by t-SNE [62], exhibited\nin Fig. 4. The experiments are conducted on ModelNet40\nand ScanObjectNN. The pretrained features roughly scatter\ninto different locations and provide good initialization for\ndownstream tasks. After ﬁnetuning on the target datasets,\nthe features are clearly separated by different clusters.\n(a) PT on MN, 40 categories\n(b) PT on SO, 15 categories\n(c) FT on MN, 40 categories\n(d) FT on SO, 15 categories\nFig. 4: t-SNE [62] Visualization of pretrained (PT) and\nﬁnetuned (FT) features on ModelNet40 (MN) and ScanOb-\njectNN (SO).\nV. CONCLUSION\nIn this paper, We propose an efﬁcient Vision-and-\nPointcloud Transformer to unify image and point cloud\nprocessing in a single architecture. ViPFormer is pretrained\nby optimizing intra-modal and cross-modal contrastive objec-\ntives. When transferred to downstream tasks and compared\nwith existing unsupervised methods, ViPFormer shows ad-\nvantages in model complexity, runtime latency and perfor-\nmances. And the contribution of each component is validated\nby extensive ablation studies. In the future, we should\npay more attention to the image branch and explore its\nperformances on downstream tasks since the current version\nfocuses on point cloud understanding.\nREFERENCES\n[1] T. Xiang, C. Zhang, Y. Song, J. Yu, and W. Cai, “Walk in the cloud:\nLearning curves for point clouds shape analysis,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV),\nOctober 2021, pp. 915–924.\n[2] X. Ma, C. Qin, H. You, H. Ran, and Y. Fu, “Rethinking network design\nand local geometry in point cloud: A simple residual MLP framework,”\nin International Conference on Learning Representations, 2022.\n[Online]. Available: https://openreview.net/forum?id=3Pbra- u76D\n[3] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, and\nA. Markham, “Randla-net: Efﬁcient semantic segmentation of large-\nscale point clouds,” Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2020.\n[4] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, “Point trans-\nformer,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), October 2021, pp. 16 259–16 268.\n[5] C. Park, Y. Jeong, M. Cho, and J. Park, “Fast point transformer,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), June 2022, pp. 16 949–16 958.\n[6] B. Yang, W. Luo, and R. Urtasun, “Pixor: Real-time 3d object\ndetection from point clouds,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June 2018.\n[7] Y. Zhou and O. Tuzel, “Voxelnet: End-to-end learning for point cloud\nbased 3d object detection,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), June 2018.\n[8] C. R. Qi, O. Litany, K. He, and L. J. Guibas, “Deep hough voting for\n3d object detection in point clouds,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), October 2019.\n[9] Y. Wang, T. Ye, L. Cao, W. Huang, F. Sun, F. He, and D. Tao,\n“Bridged transformer for vision and point cloud 3d object detection,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), June 2022, pp. 12 114–12 123.\n[10] Y. Yang, C. Feng, Y. Shen, and D. Tian, “Foldingnet: Point cloud\nauto-encoder via deep grid deformation,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), June\n2018.\n[11] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, “Point-bert:\nPre-training 3d point cloud transformers with masked point modeling,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), June 2022, pp. 19 313–19 322.\n[12] Y. Pang, W. Wang, F. E. H. Tay, W. Liu, Y. Tian, and L. Yuan, “Masked\nautoencoders for point cloud self-supervised learning,” in Computer\nVision – ECCV 2022.\nSpringer International Publishing, 2022.\n[13] J.\nSauder\nand\nB.\nSievers,\n“Self-supervised\ndeep\nlearning\non\npoint\nclouds\nby\nreconstructing\nspace,”\nin\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems,\nH.\nWallach,\nH.\nLarochelle,\nA.\nBeygelzimer,\nF.\nd'Alch´e-Buc,\nE.\nFox,\nand\nR.\nGarnett,\nEds.,\nvol.\n32.\nCurran\nAssociates,\nInc.,\n2019.\n[Online].\nAvailable:\nhttps://proceedings.neurips.cc/paper/2019/ﬁle/\n993edc98ca87f7e08494eec37fa836f7-Paper.pdf\n[14] H. Wang, Q. Liu, X. Yue, J. Lasenby, and M. J. Kusner, “Unsupervised\npoint cloud pre-training via occlusion completion,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV),\nOctober 2021, pp. 9782–9792.\n[15] X. Yu, Y. Rao, Z. Wang, Z. Liu, J. Lu, and J. Zhou, “Pointr:\nDiverse point cloud completion with geometry-aware transformers,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV), October 2021, pp. 12 498–12 507.\n[16] J.\nWu,\nC.\nZhang,\nT.\nXue,\nB.\nFreeman,\nand\nJ.\nTenenbaum,\n“Learning\na\nprobabilistic\nlatent\nspace\nof\nobject\nshapes\nvia\n3d\ngenerative-adversarial\nmodeling,”\nin\nAdvances\nin\nNeural\nInformation Processing Systems, D. Lee, M. Sugiyama, U. Luxburg,\nI. Guyon, and R. Garnett, Eds., vol. 29.\nCurran Associates, Inc.,\n2016. [Online]. Available: https://proceedings.neurips.cc/paper/2016/\nﬁle/44f683a84163b3523afe57c2e008bc8c-Paper.pdf\n[17] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas, “Learning\nrepresentations and generative models for 3D point clouds,” in\nProceedings of the 35th International Conference on Machine\nLearning, ser. Proceedings of Machine Learning Research, J. Dy and\nA. Krause, Eds., vol. 80. PMLR, 10–15 Jul 2018, pp. 40–49. [Online].\nAvailable: https://proceedings.mlr.press/v80/achlioptas18a.html\n[18] Z. Han, M. Shang, Y.-S. Liu, and M. Zwicker, “View inter-prediction\ngan: Unsupervised representation learning for 3d shapes by learning\nglobal shape memories to support local view predictions,” in\nProceedings of the Thirty-Third AAAI Conference on Artiﬁcial\nIntelligence and Thirty-First Innovative Applications of Artiﬁcial\nIntelligence Conference and Ninth AAAI Symposium on Educational\nAdvances in Artiﬁcial Intelligence, ser. AAAI’19/IAAI’19/EAAI’19.\nAAAI Press, 2019. [Online]. Available: https://doi.org/10.1609/aaai.\nv33i01.33018376\n[19] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, and O. Litany, “Pointcon-\ntrast: Unsupervised pre-training for 3d point cloud understanding,” in\nComputer Vision – ECCV 2020, A. Vedaldi, H. Bischof, T. Brox, and\nJ.-M. Frahm, Eds.\nCham: Springer International Publishing, 2020,\npp. 574–591.\n[20] Z. Zhang, R. Girdhar, A. Joulin, and I. Misra, “Self-supervised\npretraining of 3d features on any point-cloud,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV),\nOctober 2021, pp. 10 252–10 263.\n[21] J. Hou, B. Graham, M. Nießner, and S. Xie, “Exploring data-efﬁcient\n3d scene understanding with contrastive scene contexts,” in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2021, pp. 15 587–15 597.\n[22] M. Afham, I. Dissanayake, D. Dissanayake, A. Dharmasiri, K. Thi-\nlakarathna, and R. Rodrigo, “Crosspoint: Self-supervised cross-modal\ncontrastive learning for 3d point cloud understanding,” in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2022, pp. 9902–9912.\n[23] L. Tang, Y. Zhan, Z. Chen, B. Yu, and D. Tao, “Contrastive bound-\nary learning for point cloud segmentation,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2022, pp. 8489–8499.\n[24] K. He, X. Chen, S. Xie, Y. Li, P. Doll´ar, and R. Girshick, “Masked\nautoencoders are scalable vision learners,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2022, pp. 16 000–16 009.\n[25] W. Kim, B. Son, and I. Kim, “Vilt: Vision-and-language transformer\nwithout convolution or region supervision,” in Proceedings of the\n38th International Conference on Machine Learning, ser. Proceedings\nof Machine Learning Research, M. Meila and T. Zhang, Eds., vol.\n139.\nPMLR, 18–24 Jul 2021, pp. 5583–5594. [Online]. Available:\nhttp://proceedings.mlr.press/v139/kim21k.html\n[26] K. Desai and J. Johnson, “VirTex: Learning Visual Representations\nfrom Textual Annotations,” in 2021 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR).\nNashville, TN,\nUSA: IEEE, June 2021, pp. 11 157–11 168. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/9577368/\n[27] M.\nB.\nSariyildiz,\nJ.\nPerez,\nand\nD.\nLarlus,\n“Learning\nVisual\nRepresentations with Caption Annotations,” in Computer Vision –\nECCV 2020, A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm,\nEds.\nCham: Springer International Publishing, 2020, vol. 12353, pp.\n153–170, series Title: Lecture Notes in Computer Science. [Online].\nAvailable: https://link.springer.com/10.1007/978-3-030-58598-3 10\n[28] A. Owens and A. A. Efros, “Audio-Visual Scene Analysis with\nSelf-Supervised Multisensory Features,” in Computer Vision – ECCV\n2018, V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss, Eds.\nCham: Springer International Publishing, 2018, vol. 11210, pp.\n639–658, series Title: Lecture Notes in Computer Science. [Online].\nAvailable: http://link.springer.com/10.1007/978-3-030-01231-1 39\n[29] P. Morgado, N. Vasconcelos, and I. Misra, “Audio-Visual Instance\nDiscrimination with Cross-Modal Agreement,” in 2021 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR).\nNashville, TN, USA: IEEE, June 2021, pp. 12 470–12 481. [Online].\nAvailable: https://ieeexplore.ieee.org/document/9578129/\n[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), June 2016.\n[31] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning\non point sets for 3d classiﬁcation and segmentation,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), July 2017.\n[32] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M.\nSolomon, “Dynamic graph cnn for learning on point clouds,” ACM\nTransactions on Graphics (TOG), 2019.\n[33] P. Jiang and S. Saripalli, “Contrastive Learning of Features between\nImages and LiDAR,” arXiv, Tech. Rep. arXiv:2206.12071, June\n2022,\narXiv:2206.12071\n[cs]\ntype:\narticle.\n[Online].\nAvailable:\nhttp://arxiv.org/abs/2206.12071\n[34] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional\nnetworks for biomedical image segmentation,” in Medical Image Com-\nputing and Computer-Assisted Intervention – MICCAI 2015, N. Navab,\nJ. Hornegger, W. M. Wells, and A. F. Frangi, Eds.\nCham: Springer\nInternational Publishing, 2015, pp. 234–241.\n[35] C.\nR.\nQi,\nL.\nYi,\nH.\nSu,\nand\nL.\nJ.\nGuibas,\n“Pointnet++:\nDeep\nhierarchical\nfeature\nlearning\non\npoint\nsets\nin\na\nmetric\nspace,” in Advances in Neural Information Processing Systems,\nI. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Associates,\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/\n2017/ﬁle/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf\n[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you\nneed,” in Advances in Neural Information Processing Systems,\nI. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Associates,\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/\n2017/ﬁle/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[37] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” in International\nConference on Learning Representations, 2021. [Online]. Available:\nhttps://openreview.net/forum?id=YicbFdNTTy\n[38] H. Bao, L. Dong, S. Piao, and F. Wei, “BEit: BERT pre-training\nof image transformers,” in International Conference on Learning\nRepresentations, 2022. [Online]. Available: https://openreview.net/\nforum?id=p-BhZSz59o4\n[39] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training\nof\ndeep\nbidirectional\ntransformers\nfor\nlanguage\nunderstanding,” in Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational\nLinguistics:\nHuman\nLanguage\nTechnologies,\nVolume\n1\n(Long\nand\nShort\nPapers).\nMinneapolis,\nMinnesota:\nAssociation\nfor\nComputational Linguistics, June 2019, pp. 4171–4186. [Online].\nAvailable: https://aclanthology.org/N19-1423\n[40] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” 2019.\n[41] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei, “Language models are\nfew-shot learners,” in Advances in Neural Information Processing\nSystems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and\nH. Lin, Eds., vol. 33.\nCurran Associates, Inc., 2020, pp. 1877–\n1901. [Online]. Available: https://proceedings.neurips.cc/paper/2020/\nﬁle/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n[42] X. Pan, Z. Xia, S. Song, L. E. Li, and G. Huang, “3d object detection\nwith pointformer,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), June 2021, pp.\n7463–7472.\n[43] C. Sharma and M. Kaul, “Self-supervised few-shot learning on\npoint\nclouds,”\nin\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and\nH. Lin, Eds., vol. 33.\nCurran Associates, Inc., 2020, pp. 7212–\n7221. [Online]. Available: https://proceedings.neurips.cc/paper/2020/\nﬁle/50c1f44e426560f3f2cdcb3e19e39903-Paper.pdf\n[44] L. Nunes, R. Marcuzzi, X. Chen, J. Behley, and C. Stachniss,\n“SegContrast: 3D Point Cloud Feature Representation Learning\nthrough Self-supervised Segment Discrimination,” ral, vol. 7, no. 2,\npp. 2116–2123, 2022. [Online]. Available: http://www.ipb.uni-bonn.\nde/pdfs/nunes2022ral-icra.pdf\n[45] X. Ma, C. Qin, H. You, H. Ran, and Y. Fu, “Rethinking network design\nand local geometry in point cloud: A simple residual MLP framework,”\nin International Conference on Learning Representations, 2022.\n[Online]. Available: https://openreview.net/forum?id=3Pbra- u76D\n[46] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,”\nin Computer Vision – ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part I.\nBerlin,\nHeidelberg: Springer-Verlag, 2020, p. 213–229. [Online]. Available:\nhttps://doi.org/10.1007/978-3-030-58452-8 13\n[47] I. Misra, R. Girdhar, and A. Joulin, “An end-to-end transformer model\nfor 3d object detection,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), October 2021, pp. 2906–\n2917.\n[48] J. Mao, Y. Xue, M. Niu, H. Bai, J. Feng, X. Liang, H. Xu, and\nC. Xu, “Voxel transformer for 3d object detection,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV),\nOctober 2021, pp. 3164–3173.\n[49] H. Sheng, S. Cai, Y. Liu, B. Deng, J. Huang, X.-S. Hua, and M.-J.\nZhao, “Improving 3d object detection with channel-wise transformer,”\nin Proceedings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV), October 2021, pp. 2743–2752.\n[50] Z. Liu, Z. Zhang, Y. Cao, H. Hu, and X. Tong, “Group-free 3d\nobject detection via transformers,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), October 2021,\npp. 2949–2958.\n[51] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M.\nHu, “Pct: Point cloud transformer,” 2020.\n[52] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and\nJ. Carreira, “Perceiver: General perception with iterative attention,”\nin Proceedings of the 38th International Conference on Machine\nLearning, ser. Proceedings of Machine Learning Research, M. Meila\nand T. Zhang, Eds., vol. 139. PMLR, 18–24 Jul 2021, pp. 4651–4664.\n[Online]. Available: https://proceedings.mlr.press/v139/jaegle21a.html\n[53] A. Jaegle, S. Borgeaud, J.-B. Alayrac, C. Doersch, C. Ionescu,\nD. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, O. J. Henaff,\nM. Botvinick, A. Zisserman, O. Vinyals, and J. Carreira, “Perceiver\nIO:\nA\ngeneral\narchitecture\nfor\nstructured\ninputs\n&\noutputs,”\nin International Conference on Learning Representations, 2022.\n[Online]. Available: https://openreview.net/forum?id=fILj7WpI-g\n[54] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple\nframework for contrastive learning of visual representations,” in\nProceedings of the 37th International Conference on Machine\nLearning, ser. Proceedings of Machine Learning Research, H. D. III\nand A. Singh, Eds., vol. 119. PMLR, 13–18 Jul 2020, pp. 1597–1607.\n[Online]. Available: https://proceedings.mlr.press/v119/chen20j.html\n[55] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan,\nQ. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao,\nL. Yi, and F. Yu, “Shapenet: An information-rich 3d model\nrepository,” CoRR, vol. abs/1512.03012, 2015. [Online]. Available:\nhttp://arxiv.org/abs/1512.03012\n[56] Q.\nXu,\nW.\nWang,\nD.\nCeylan,\nR.\nMech,\nand\nU.\nNeumann,\n“Disn: Deep implicit surface network for high-quality single-view\n3d reconstruction,” in Advances in Neural Information Processing\nSystems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc,\nE. Fox, and R. Garnett, Eds., vol. 32.\nCurran Associates, Inc.,\n2019. [Online]. Available: https://proceedings.neurips.cc/paper/2019/\nﬁle/39059724f73a9969845dfe4146c5660e-Paper.pdf\n[57] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\nin International Conference on Learning Representations, 2019.\n[Online]. Available: https://openreview.net/forum?id=Bkg6RiCqY7\n[58] N. Katsura, “Pytorch cosineannealing with warmup restarts,” https://\ngithub.com/katsura-jp/pytorch-cosine-annealing-with-warmup, 2021.\n[59] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao,\n“3d shapenets: A deep representation for volumetric shapes,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2015.\n[60] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, and S.-K. Yeung,\n“Revisiting point cloud classiﬁcation: A new benchmark dataset\nand classiﬁcation model on real-world data,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV),\nOctober 2019.\n[61] L. Yi, V. G. Kim, D. Ceylan, I.-C. Shen, M. Yan, H. Su,\nC. Lu, Q. Huang, A. Sheffer, and L. Guibas, “A scalable active\nframework for region annotation in 3d shape collections,” ACM\nTrans. Graph., vol. 35, no. 6, nov 2016. [Online]. Available:\nhttps://doi.org/10.1145/2980179.2980238\n[62] L. van der Maaten and G. Hinton, “Visualizing data using t-sne,”\nJournal of Machine Learning Research, vol. 9, no. 86, pp. 2579–2605,\n2008. [Online]. Available: http://jmlr.org/papers/v9/vandermaaten08a.\nhtml\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2023-03-25",
  "updated": "2023-03-25"
}