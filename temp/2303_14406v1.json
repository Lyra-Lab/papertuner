{
  "id": "http://arxiv.org/abs/2303.14406v1",
  "title": "Natural Language Processing in Ethiopian Languages: Current State, Challenges, and Opportunities",
  "authors": [
    "Atnafu Lambebo Tonja",
    "Tadesse Destaw Belay",
    "Israel Abebe Azime",
    "Abinew Ali Ayele",
    "Moges Ahmed Mehamed",
    "Olga Kolesnikova",
    "Seid Muhie Yimam"
  ],
  "abstract": "This survey delves into the current state of natural language processing\n(NLP) for four Ethiopian languages: Amharic, Afaan Oromo, Tigrinya, and\nWolaytta. Through this paper, we identify key challenges and opportunities for\nNLP research in Ethiopia. Furthermore, we provide a centralized repository on\nGitHub that contains publicly available resources for various NLP tasks in\nthese languages. This repository can be updated periodically with contributions\nfrom other researchers. Our objective is to identify research gaps and\ndisseminate the information to NLP researchers interested in Ethiopian\nlanguages and encourage future research in this domain.",
  "text": "Natural Language Processing in Ethiopian Languages: Current State,\nChallenges, and Opportunities\nAtnafu Lambebo Tonja 1,∗, Tadesse Destaw Belay2,∗, Israel Abebe Azime3,∗,\nAbinew Ali Ayele4,5,∗, Moges Ahmed Mehamed6,∗, Olga Kolesnikova1, Seid Muhie Yimam5,∗,\n∗EthioNLP, 1Instituto Politécnico Nacional, Mexico, 2Wollo University, Ethiopia, 3 Saarland University, Germany,\n4Bahir Dar University, Ethiopia,5Universität Hamburg, Germany, 6Wuhan University of Technology, China.\nAbstract\nThis survey delves into the current state of\nnatural language processing (NLP) for four\nEthiopian languages: Amharic, Afaan Oromo,\nTigrinya, and Wolaytta. Through this paper,\nwe identify key challenges and opportunities\nfor NLP research in Ethiopia.\nFurthermore,\nwe provide a centralized repository on GitHub\nthat contains publicly available resources for\nvarious NLP tasks in these languages. This\nrepository can be updated periodically with\ncontributions from other researchers. Our ob-\njective is to identify research gaps and dissem-\ninate the information to NLP researchers inter-\nested in Ethiopian languages and encourage fu-\nture research in this domain.\n1\nIntroduction\nDue to the rise of its applications in many ﬁelds,\nNatural Language Processing (NLP), a sub-ﬁeld\nof Artiﬁcial Intelligence (AI), is receiving a lot\nof attention in terms of research and development\n(Kalyanathaya et al., 2019). NLP tasks such as\nMachine Translation (MT), Sentiment or Opinion\nAnalysis, Parts of Speech (POS) Tagging, Question\nClassiﬁcation (QC) and Answering (QA), Chunk-\ning, Named Entity Recognition (NER), Emotion\nDetection, and Semantic Role Labeling is currently\nhighly researched areas in different high-resource\nlanguages.\nBecause of the advancement of deep learning\nand transformer approaches, modern NLP systems\nrely largely on the availability of vast volumes of\nannotated and unannotated data to function well.\nThe majority of the languages in the world do not\nhave access to such enormous information tools,\ndespite the fact that a few high-resource languages\nhave received more attention. Ethiopia is a country\nwith more than 85 spoken languages, but only a\nfew are presented in NLP progress. Figure 1 shows\na search result for articles found in the ACL an-\nthology for high and low-resource languages. As\nFigure 1: ACL paper search results for high and low-\nresource languages.\nwe can see from Figure 1, the search result for\nlow-resource languages shows a very insigniﬁcant\nnumber of research works, while high-resource lan-\nguages like English dominate in the ACL anthology\npaper repository. This might be a reﬂection of the\nunavailability of resources in the digital world, like\nin other high-resource languages, which affected\nthe NLP progress in low-resource languages in gen-\neral and Ethiopian languages in particular.\nIn this paper, we overview research works done\nin the area of selected NLP tasks for four Ethiopian\nlanguages. We cover mainly the following 4 lan-\nguages, namely Amharic (Amh), Afaan Oromo\n(Orm), Tigrinya (Tir), and Wolaytta (Wal). We also\nreviewed works on a small set of local languages in-\ncluding Awigna (Awn) and Kistane(Gru), specially\nfor the machine translation tasks. The contributions\nof this paper are as follows: (1) Reporting the cur-\nrent state-of-the-art NLP research for Ethiopian lan-\nguages. (2) Discussing NLP progress in Ethiopian\nlanguages and the main challenges and opportu-\nnities for Ethiopian NLP research. (3) Collecting\nand presenting publicly available resources for dif-\nferent NLP tasks in Ethiopian languages in one\nGitHub repository that can be extended periodi-\ncally in collaboration with other researchers. The\ncollected publicly available datasets and models for\nEthiopian languages are in our GitHub repository1.\n1https://github.com/EthioNLP/Ethiopian-Language-\narXiv:2303.14406v1  [cs.CL]  25 Mar 2023\n2\nLanguage Details\nThis paper assesses the progress of NLP research\nfor four Ethiopian languages: Amharic, Afaan\nOromo, Tigrinya, and Wolaytta. As Ethiopia is a\nmultilingual, multicultural, and multi-ethnic coun-\ntry, those selected languages have more speakers\nand native speakers in the country. Additionally, we\nhave searched papers in the major eight Ethiopian\nlanguages and taken the top four based on fre-\nquency from the ACL anthology. This section gives\nsome descriptions of those four targeted languages.\nAmharic: is an Ethio-Semitic and Afro-Asiatic\nlanguage. It is the ofﬁcial working language of the\nFederal Democratic Republic of Ethiopia (FDRE).\nIt has about 57 million speakers, which makes it\nthe second most spoken Semitic language in the\nworld, where 32 million of them are native speakers\n(Eberhard et al., 2022). Other known names for this\nlanguage are Amarigna and Amarinya.\nAfaan Oromo: is a Cushitic language family.\nThe language name may be written in different\nalternatives: (Afan, Afaan, affan) Oromo, or simply\nOromo. There are over 50 million native Oromo\nspeakers (Eberhard et al., 2022).\nTigrinya: (alternatively: Tigregna, Tigrinya or\nTigrigna) is a Semitic language family spoken in\nthe Tigray region of Ethiopia and in Eritrea. The\nlanguage uses Ge’ez script with some additional al-\nphabets that have been created for the Tigrinya lan-\nguage and are closely related to Ge’ez and Amharic.\nThe language has around 9.9 million native speak-\ners (Eberhard et al., 2022).\nWolaytta: (alternatively: Wolayita, Wolaytegna,\nWolaytigna, Welaitta, and Welayita) is an Omotic\nlanguage family spoken in the Wolaytta zone of\nEthiopia. Both Afan Oromo and Wolaytta are writ-\nten in the Latin alphabet.\n3\nLow-resource Languages\nResearchers concerned with NLP have used data\navailability (either in the form of labeled, unla-\nbeled, or auxiliary data) and NLP tools and re-\nsources as criteria for deﬁning low-resource lan-\nguages (Ranathunga et al., 2021). According to\nthe work by Gereme et al. (2021), low-resource\nlanguages lack the tools and resources important\nfor NLP and other techno-linguistic solutions. In\naddition, low-resource languages lack new lan-\nguage technology designs. Due to all these lim-\nitations, it is very difﬁcult to develop new powerful\nSurvey\nmethods for language applications (Tonja et al.,\n2023). For resource-rich languages such as En-\nglish, Germany, French, Spanish and etc, the size\nof the dataset is not a problem because researchers\nhave created a large set of corpora and tools for\nmany NLP tasks. However, many other languages\nare deemed to be low-resource languages (Fesseha\net al., 2021a). With this intuition, Ethiopian lan-\nguages such as Amharic (Gereme et al., 2021),\nAfaan Oromo (Abate et al., 2019), Tigrinya (Os-\nman and Mikami, 2012), Wolayitta (Tonja et al.,\n2023) are \"low-resource\" languages due to lack of\ndata resources, linguistic materials, and tools. This\naffected the development of different NLP tasks\nand tools.\n4\nPossible Resource Sources and Tools\nData is one of the crucial building blocks for any\nNLP application. The availability of data is one of\nthe criteria to categorize one language as a high or\nlow-resource language (Ranathunga et al., 2021).\nAs discussed in Section 3 Ethiopian languages be-\nlong to low-resource languages due to the unavail-\nability of data. Table 1 shows some possible digital\ndata sources for different NLP tasks.\nLike data, NLP tools are also one of the building\nblocks for NLP applications, and the unavailability\nof these tools for a certain language also directly\naffects the development of NLP applications for\nthat language. Table 2 shows available NLP tools\nfor Ethiopian languages. As it can be seen from\nTables 1 and 2, there are still very few sources to\ngather digital data and tools, available for Ethiopian\nlanguages.\n5\nNLP Tasks and Their Progress\nIn this section, we discuss what work has been\ndone, what datasets of what sizes were used, what\nmethods or approaches the authors proposed, and\nthe availability of their dataset and models for\nNLP tasks and their progress in selected Ethiopian\nlanguages. We focused on Machine Translation\n(MT), Part-of-speech (POS) tagging, Named Entity\nRecognition (NER), Question Classiﬁcation (QC),\nQuestion Answering (QA), text classiﬁcation, and\ntext summarization tasks due to the large number\nof works done for the targeted low-resource lan-\nguages. The available models, the datasets for the\ntasks, and their links are found in Table 7.\nSources\nLink\nReligion books\nBible\nhttps://www.bible.com/\nQuran\nMultilingual data repositories\nOpus\nhttps://opus.nlpl.eu\nLanfrica\nhttps://lanfrica.com\nMasakhane\nhttps://github.com/masakhane-io\nHugging face\nhttps://huggingface.co/\nNews medias\nFana\nhttps://www.fanabc.com\nEBC\nhttps://www.ebc.et\nBBC\nhttps://www.bbc.com\nDW\nhttps://www.dw.com\nWalata\nhttps://waltainfo.com/\nSocial medias\nTwitter\nhttps://twitter.com/\nFacebook\nhttps://www.facebook.com/\nReddit\nhttps://www.reddit.com/\nText Corpus\nAmharic Text Corpus\nAmharic Corpus at Mendeley\nTable 1: Possible data sources\nAuthor (s)\nTool’s name\nTool’s task\nLanguage (s) support\nResource link\nYimam et al. (2021); Belay et al. (2022b)\namseg\nSegmenter, tokenizer, transliteration,\nromanization and normalization\nAmh\namseg\nGasser (2011)\nHornMorpho\nMorphological analysis\nAmh, Orm, Tig\nHornMorpho\nSeyoum et al. (2018)\nlemma\nLemmatizer\nAmh\nLemmatizer\nTable 2: Available language tools that are developed for low-resource Ethiopian languages.\n5.1\nPOS Tagging\nPOS tagging is one of the popular NLP tasks that\nrefer to categorizing words in a text (corpus) in\ncorrespondence with a particular part of speech,\ndepending on the deﬁnition of the word and its\ncontext (Pailai et al., 2013).\nTable 3 summarizes the current state of POS\ntagging research for selected Ethiopian languages.\nThe table shows the name(s) of the author(s), the\nsize of the dataset, the method used, the accuracy\nscore of the models, and the availability of datasets\nand models in public repositories.\nFor Amharic, seven studies are listed, which\nused different approaches such as Conditional Ran-\ndom Fields (CRF), Maximum Entropy (MaxEnt),\nSupport Vector Machines (SVM), CRFSuit, and\nMemory-Based Tagger (MBT). The highest accu-\nracy score was achieved using the CRFSuit ap-\nproach by Gashaw and Shashirekha (2020). For\nAfaan Oromo, two studies are listed that used the\nHidden Markov Model (HMM) and Brill’s tag-\nger. The highest accuracy score was achieved us-\ning Bill’s tagger by Ayana (2015). For Tigrinya,\ntwo studies are listed that used CRF and Long\nShort-Term Memory (LSTM). The highest accu-\nracy score was achieved by the LSTM approach in\nTesfagergish and Kapociute-Dzikiene (2020) and\nfor Wolayitta, one study is listed, that used HMM\nand achieved an accuracy score of 92.96.\nFrom Table 3, we can conclude that POS tagging\nis less researched for Ethiopian languages, the ma-\njority of the works were found for Amharic than\nfor the other languages. From the works discussed\nin Table 3 only the work by Yimam et al. (2021)\nmade their models and datasets available for public\nuse.\n5.2\nNamed Entity Recognition (NER)\nIn this section, we present works related to Named\nEntity Recognition (NER) for Ethiopian languages.\nFor Amharic, Mehamed (2019) conducted the\nNER experiment on a corpus of 10,405 tokens us-\ning the CRF classiﬁer. Alemu (2013) conducted\nthe experiments on a manually developed corpus\nof 13,538 words with the Stanford tagging scheme.\nTadele (2014) used a hybrid of machine learning\n(decision trees and support vector machines) and\nrule-based methods. The datasets for these works\nare not available. The work done by the Masakhane\nNLP group (Adelani et al., 2021) analyzed a 10\nAfrican languages dataset and conducted an exten-\nsive empirical evaluation of state-of-the-art meth-\nods across both supervised and transfer learning\nsettings, including Amharic. The data and mod-\nels are available on GitHub. Gambäck and Sik-\nLanguages\nAuthor(s)\nSize\nApproach\nScore\nDataset\nModel\nAmharic\nAdafre (2005)\n1000\nCRF\n74.00\nNo\nNo\nGambäck et al. (2009)\n210,000\nMaxEnt\n94.52\nNo\nNo\nTachbelie and Menzel (2009)\n210,000\nSVM\n85.50\nNo\nNo\nGebre (2010)\n206,929\nSVM\n90.95\nNo\nNo\nHIRPSSA and Lehal (2020)\n210,000\nCRF\n94.08\nNo\nNo\nYimam et al. (2021)\n210,000\nCRF\n92.27\nYes\nYes\nGashaw and Shashirekha (2020)\n109,676\nCRFSuit\n95.10\nNo\nNo\nTachbelie et al. (2011)\n210,000\nMBT\n93.51\nNo\nNo\nAfaan Oromo\nWegari and Meshesha (2011)\n1621\nHMM\n91.97\nNo\nNo\nAyana (2015)\n17,473\nBill’s tagger\n95.60\nNo\nNo\nTigrinya\nTedla et al. (2016)\n72,080\nCRF\n90.89\nYes\nNo\nTesfagergish and Kapociute-Dzikiene (2020)\n72,080\nLSTM\n91.00\nNo\nNo\nWolayitta\nShirko (2020)\n14,358\nHMM\n92.96\nYes\nNo\nTable 3: Summary of related works for selected Ethiopian languages in POS tag tasks, Size shows the number of\ntokens used during the experiment, Score shows the outperformed model results evaluated using accuracy score,\nDataset and Model shows the availability of dataset and models in publicly accessible repositories.\ndar (2017); Yimam et al. (2021); Sikdar and Gam-\nbäck (2018) built a deep learning-based NER sys-\ntem for Amharic using the available SAY project\nNER dataset. Jibril and Tant˘g (2022) proposed\na transformer-based NER recognizer for Amharic\nusing a new annotated 182,691 word dataset. All\navailable NER datasets for Ethiopian languages are\nshown in Table 7.\nFor Afan Oromo, the work by Legesse (2012)\nimplemented the ﬁrst NER system using a hybrid\napproach (rule-based and statistical) which con-\ntains 23k words. Abdi (2015) deals with NER\nin a hybrid (machine learning and rule-based) ap-\nproach using the data from the work of Legesse\n(2012). Abafogi (2021) adopted boosting NER\nby combinations of such approaches as, machine\nlearning, stored rules, and pattern matching using\n44k words out of which around 7.8k were named\nentities. Gardie and Solomon (2022) developed\na NER system using 12,479 data instances and\nBiLSTM, word embedding, and CNN approaches.\nHowever, none of the datasets in the above Afan\nOromo works are publicly available.\nFor Tigrinya, the research by Yohannes and Am-\nagasa (2022b) proposed a method for NER using\na pre-trained language model, TigRoBERTa. The\ndataset contains 69,309 manually annotated words.\nLater Yohannes and Amagasa (2022a) employed\nTigrinya NER with an addition of 40,627 words.\nThe only NER work attempted Wolaytta Lan-\nguage was conducted by Biruk (2021) using a ma-\nchine learning approach. Figures 2 and 3 show\nNER publication types and dataset availability for\ntargeted Ethiopian languages, respectively.\nWe\ncan summarize that NER is a little more developed\nFigure 2: NER publication types for Ethiopian lan-\nguages: the ﬁgure description is the same as in Fig-\nure 4.\nWolaytta has no published works in confer-\nences/journals.\nFigure 3: NER dataset availability per language: as it\ncan be seen relatively more NER datasets are available\nonly for the Amharic language.\nthan the POS tagging for Afan Oromo, Tigriyna,\nand Wolaytta languages. However, like POS tag-\nging, only small Amharic NER datasets shown in\nFigure 3 are available.\n5.3\nMachine Translation (MT)\nWith the increasing popularity of computational\ntasks and the Internet’s expanding reach to di-\nverse, multilingual communities, the ﬁeld of MT\nis rapidly progressing (Kenny, 2018). While im-\npressive translation results have been achieved for\nlanguage pairs with abundant resources, such as\nEnglish-Spanish, English-French, English-Russian,\nand English-Portuguese, MT systems struggle in\nenvironments with limited resources, where in-\nsufﬁcient training data for certain languages is\nthe main obstacle.\nIn this section, we discuss\nthe MT progress for Ethiopian languages in three\ncategories: (i) English Centeric- works done for\nthe above target Ethiopian languages with English\npair, (ii) Ethiopian - Ethiopian - works done for\nEthiopian language pairs without involving other\nlanguages, and (iii) Multilingual MT - works done\nfor Ethiopian languages with other languages in a\nmultilingual setting.\nTable 4 summarizes several studies on MT in\nselected Ethiopian languages, focusing on the three\ncategories. The studies vary in size of the paral-\nlel dataset, approach, score, availability of dataset,\nand model for public use. For English-centric lan-\nguage pairs, ﬁve studies used Amh-Eng language\npairs. Biadgligne and Smaïli (2021) used NMT,\nand the size of their dataset was 231,898, while\nGezmu et al. (2021) used NMT and had a dataset\nsize of 145,364. Ashengo et al. (2021) used RN-\nNMT, and their dataset size was 8,603. The study\nby Biadgligne and Smaïli (2022) used NMT, and\ntheir dataset size was the same as that of Biadg-\nligne and Smaïli (2021). Belay et al. (2022a) used\nNMT with a dataset size of 1,140,130. Finally,\nfour studies used language pairs: Orm-Eng, Tir-\nEng, and Wol-Eng. These studies applied different\napproaches such as SMT, NMT, and hybrid, with\ndataset sizes ranging from 6,400 to 336,000.\nThree studies used Amharic (Amh) and other\nlocal language pairs, with different approaches\nand parallel dataset sizes. The study by Mekon-\nnen (2019) used Amh-Awn language pairs, with\na parallel dataset size of 5,000 and an SMT ap-\nproach, while the study by Woldeyohannis and\nMeshesha (2018) worked on Amh-Tir language\npairs with a parallel dataset size of 27,000 and an\nSMT approach. Finally, the study by Ashengo et al.\n(2021) used Amh-Gur language pairs with a par-\nallel dataset size of 9,225 and an NMT approach.\nThe performance in these studies ranged from 7.73\nto 17.26 in BLEU scores. For multilingual MT, we\nfound two studies by Lakew et al. (2020) and Vegi\net al. (2022) that included Ethiopian languages with\nother African languages.\nIn Table 4, we can see some of the notable ﬁnd-\nings of the studies, for example, Solomon et al.\n(2017) achieved a high BLEU score of 47.00 with\ntheir SMT approach, although their parallel dataset\nsize was small (6,400). The study by Berihu et al.\n(2020) used a hybrid approach and achieved a high\nBLEU score of 67.57 with a parallel dataset size\nof 32,000. Kidane et al. (2021) used NMT with\na large parallel dataset size (336,000), but their\nBLEU score was relatively low (15.52). Tonja et al.\n(2021) and Tonja et al. (2023) used NMT for Wal-\nEng language pairs, with parallel dataset sizes of\n26,943, but their scores were relatively low (13.8\nand 16.1, respectively). Lastly, in multilingual MT\nstudies, the work by Lakew et al. (2020) made the\ndatasets and models available for public use. More\nanalysis of MT studies for the selected languages\nare discussed in Appendix A.\n5.4\nQuestion Answering and Classiﬁcation\nEven though question classiﬁcation (QC) and ques-\ntion answering (QA) have been largely studied for\nvarious languages, they have barely been studied\nfor Amharic, Afaan Oromo, Tigrinya, and Wolaytta.\nSome of the QC and QA work conducted for these\nlanguages are discussed below.\nFor Amharic, the work by Habtamu (2021) im-\nplemented a Convolutional Neural Network (CNN)\nbased Amharic QC model using around 8k generic\nAmharic questions from different websites and la-\nbeled into 6 classes, similar to the question classes\nproposed by Li and Roth (2006). The work done\nby Taffa and Libsie (2019) developed Amharic\nnon-factoid QA for biography, deﬁnition, and de-\nscription questions. Yimam and Libsie (2009) de-\nveloped an Amharic QA system for factoid ques-\ntions. However, the datasets of the aforementioned\nworks are still not available for further investiga-\ntion. Nega et al. (2016) presented machine learn-\ning (SVM) based Amharic QC using a total of\n180 questions collected from the Agriculture do-\nmain. Lastly, the work done by Belay et al. (2022b)\nbuilt a QC dataset from a Telegram public channel\ncalled Ask Anything Ethiopia and developed deep\nlearning-based Amharic question classiﬁers. Nega\net al. (2016) and Belay et al. (2022b) datasets are\nreleased in a GitHub repository (see Table 7).\nFor Afaan Oromo, the work by Chaltu (2016)\nproposed the Afaan Oromo list, deﬁnition, and\ndescription QA system. Daba (2021) improved\nthe result of Chaltu (2016) work for Afaan Oromo\nnon-factoid questions. AMARE (2016) conducted\nthe Tigrinya factoid QA system using 1200 ques-\nCategories\nAuthor(s)\nLang. pairs\nSize\nApproach\nScore\nDataset\nModel\nEnglish centeric\nBiadgligne and Smaïli (2021)\nAmh-Eng\n231,898\nNMT\n32.44\nNo\nNo\nGezmu et al. (2021)\nAmh-Eng\n145,364\nNMT\n32.20\nYes\nNo\nAshengo et al. (2021)\nAmh-Eng\n8,603\nRNNMT\n21.46\nNo\nNo\nBiadgligne and Smaïli (2022)\nAmh-Eng\n231,898\nNMT\n37.79\nNo\nNo\nBelay et al. (2022a)\nAmh-Eng\n1,140,130\nNMT\n37.79\nNo\nNo\nSolomon et al. (2017)\nOrm-Eng\n6,400\nSMT\n47.00\nNo\nNo\nMeshesha and Solomon (2018)\nOrm-Eng\n6,400\nSMT\n27.00\nNo\nNo\nAdugna and Eisele (2010)\nOrm-Eng\n21,085\nSMT\n17.74\nNo\nNo\nChala et al. (2021)\nOrm-Eng\n40,000\nNMT\n26.00\nNo\nNo\nGemechu and Kanagachidambaresan (2021)\nOrm-Eng\n10,000\nNMT\n41.62\nNo\nNo\nTedla and Yamamoto (2016)\nTir-Eng\n31,279\nSMT\n20.90\nNo\nNo\nTedla and Yamamoto (2017)\nTir-Eng\n31,279\nSMT\n20.00\nNo\nNo\nBerihu et al. (2020)\nTir-Eng\n32,000\nHybrid\n67.57\nNo\nNo\nAzath and Kiros (2020)\nTir-Eng\n17,338\nSMT\n23.27\nNo\nNo\nKidane et al. (2021)\nTir-Eng\n336,000\nNMT\n15.52\nYes\nNo\nTonja et al. (2021)\nWal-Eng\n26,943\nNMT\n13.80\nNo\nNo\nTonja et al. (2023)\nWal-Eng\n26,943\nNMT\n16.10\nNo\nNo\nAbate et al. (2019)\nAmh-Eng\n40,726\nSMT\n13.31\nYes\nNo\nAbate et al. (2019)\nOrm-Eng\n14,706\nSMT\n14.68\nYes\nNo\nAbate et al. (2019)\nTir-Eng\n35,378\nSMT\n17.89\nYes\nNo\nAbate et al. (2019)\nWal-Eng\n30,232\nSMT\n10.49\nYes\nNo\nLocal -Local\nMekonnen (2019)\nAmh-Awn\n5,000\nSMT\n17.26\nNo\nNo\nWoldeyohannis and Meshesha (2018)\nAmh-Tir\n27,000\nSMT\n9.11\nNo\nNo\nAshengo et al. (2021)\nAmh-Gur\n9,225\nNMT\n7.73\nNo\nNo\nMultilingual\nLakew et al. (2020)\nAmh-Eng\n373,358\nNMT\n20.86\nYes\nYes\nLakew et al. (2020)\nOrm-Eng\n14,706\nNMT\n32.24\nYes\nYes\nLakew et al. (2020)\nTir-Eng\n917,632\nNMT\n32.21\nYes\nYes\nVegi et al. (2022)\nAmh-Eng\n46,000\nNMT\n24.17\nYes\nNo\nVegi et al. (2022)\nOrm-Eng\n7,000\nNMT\n12.13\nYes\nNo\nTable 4: Summary of related works for selected Ethiopian languages in MT task, Lang. pairs is language pairs\nused for translation, Size shows the number of parallel sentences used in each paper, Score shows the outperformed\nmodel results evaluated using BLEU score, Dataset and Model shows the availability of dataset and models in\npublicly accessible repositories, respectively.\ntions. No QC or QA works have been done previ-\nously for Wolaytta language. Figures 4 and 5 show\nQC/QA publication types and dataset availability\nfor Ethiopian languages, respectively.\nFigure 4: QC/QA publication type: MSc/Ph.D. is an\nunpublished master or Ph.D. thesis uploaded in lo-\ncal universities repositories and archives.\nA Confer-\nence/Journal label is a work that is published in a con-\nference or journal.\nFrom Table 4 and 5, we can conclude that QC\nand QA are less researched for Ethiopian lan-\nguages, compared to the other NLP tasks. Most\nof the conducted works are unpublished MSc or\nFigure 5: QA dataset availability per language: as it can\nbe seen some QC datasets are available for Amharic but\nnot for the other languages.\nPh.D. theses. Relatively, Amharic has received\nmore attention for QA and QC tasks.\n5.5\nText Classiﬁcation\n5.5.1\nHate Speech\nDespite many works conducted on hate speech de-\ntection for resource-rich languages, low-resource\nlanguages such as Amharic, Afan Oromo, Tigrinya,\nand Wolaytta, are less researched. Table 5 presents\na summary of the related works in hate speech de-\ntection for selected Ethiopian languages. The table\nLanguages\nAuthor(s)\nSize\nAlgorithm\nScore\nDataset\nModel\nAmharic\nMossie and Wang (2018)\n6,120\nWord2Vec\n85.34\nNo\nNo\nMossie and Wang (2019)\n14,266\nCNN-GRU\n97.85\nNo\nNo\nAbebaw et al. (2022)\n2,000\nMC-CNN\n74.50\nYes\nNo\nBawoke (2020)\n30,000\nBILSTM\n90.00\nNo\nNo\nAyele et al. (2022)\n5,267\nRoBERTa\n50.00\nYes\nYes\nAfaan Oromo\nAbabu and Woldeyohannis (2022)\n12,812\nBiLSTM\n88.00\nNo\nNo\nDefersha and Tune (2021)\n13,600\nL-SVM\n63.00\nNo\nNo\nKanessa and Tulu (2021)\n2,780\nSVM+TF-IDF\n96.00\nNo\nNo\nTigrinya\nBahre (2022)\n7,793\nNB+TF-IDF\n79.00\nNo\nNo\nTable 5: Summary of related works for selected Ethiopian languages in hate speech tasks, Size shows the number\nof sentences used during the experiment, Score shows the outperformed model results evaluated using F1 score,\nDataset and Model shows the availability of dataset and models in publicly accessible repositories, respectively.\nincludes the name of the language, the author(s) of\nthe paper, the size of the dataset used, the algorithm\nused, the score obtained, and the availability of the\ndataset and model in publicly accessible reposito-\nries.\nFor Amharic, ﬁve studies were conducted with\ndifferent approaches. Mossie and Wang (2018)\nused Word2Vec to detect hate speech in a dataset\nof 6,120 sentences and achieved an F1 score of\n85.34. In another study, Mossie and Wang (2019)\nused CNN-GRU in a dataset of 14,266 sentences\nand achieved an F1 score of 97.85. Abebaw et al.\n(2022) used MC-CNN in a dataset of 2,000 sen-\ntences, achieving an F1 score of 74.50. Bawoke\n(2020) used BILSTM on a dataset of 30,000 sen-\ntences, achieving an F1 score of 90.00. Lastly,\nAyele et al. (2022) used RoBERTa on a dataset of\n5,267 sentences, achieving an F1 score of 50.00.\nFor Afaan Oromo, three studies were con-\nducted, and none of them made their dataset or\nmodel publicly accessible. Ababu and Woldey-\nohannis (2022) used BiLSTM on a dataset of\n12,812 sentences, achieving an F1 score of 88.00.\nDefersha and Tune (2021) used L-SVM on a\ndataset of 13,600 sentences, achieving an F1 score\nof 63.00. Kanessa and Tulu (2021) used SVM+TF-\nIDF on a dataset of 2,780 sentences, achieving an\nF1 score of 96.00. Bahre (2022) used NB+TF-IDF\non a dataset of 7,793 sentences in the Tigrinya\nlanguage and achieved an F1 score of 79.00. The\ndataset and model used in this study were not pub-\nlicly accessible. In summary, Table 5 shows that\nhate speech detection in Ethiopian languages is one\nof the topics of research interest. However, similar\nto other tasks there is still a lack of publicly acces-\nsible datasets and models, which could hinder the\ndevelopment and evaluation of future research. It\nis worth noting that only two of the nine studies\nmade their dataset and model publicly accessible.\nWe can also see from Table 5 that for the Wolayitta\nlanguage, there is no literature found for the hate\nspeech task. Additionally, the F1 scores obtained\nvary greatly among the different studies, indicat-\ning that for all tasks the results are not comparable\nsince the datasets are different.\n5.5.2\nSentiment Analysis\nTable 6 summarizes recent studies on sentiment\nanalysis tasks for selected Ethiopian languages, in-\ncluding Amharic, Afaan Oromo, and Tigrinya. The\nstudies utilize various algorithms such as Role2Vec,\nNaïve Bayes, LSTM, SVM, hybrid, and XLNet.\nFor Amharic, Yimam et al. (2020) achieved the\nhighest F1 score of 58.48% using Role2Vec with\na dataset and a model publicly available, while\nAbeje et al. (2022) achieved the highest accu-\nracy of 90.10% using LSTM. For Afaan Oromo,\nthe highest accuracy of 93.00% was achieved by\nOljira (2020) using Naïve Bayes, while Rase (2020)\nachieved 87.70% accuracy using LSTM. In con-\ntrast, Wayessa and Abas (2020) achieved 90.00%\naccuracy using SVM. For Tigrinya, Tela (2020)\nachieved an F1 score of 81.62% using XLNet with\na 4000 manually labeled dataset. For Wolaita, sim-\nilar to the hate speech task, there is no literature\nfound for the sentiment analysis task. None of the\ndatasets and models for Afaan Oromo, Tigrinya,\nand most of the works for Amharic are publicly\naccessible, hence results are not also comparable.\nThis suggests that more work needs to be done in\ncreating publicly accessible datasets and models\nfor sentiment analysis tasks in Ethiopian languages.\nIn conclusion, the studies in Table 6 indicate the\npotential for sentiment analysis in Ethiopian lan-\nguages. The results show that the models’ perfor-\nmance varies depending on the algorithm, dataset,\nLanguages\nAuthor(s)\nSize\nAlgorithm\nScore\nDataset\nModel\nAmharic\nYimam et al. (2020)\n9,400\nF-Role2Vec\n58.48\nYes\nYes\nPhilemon and Mulugeta (2014)\n600\nNaïve Bayes\n51.00\nNo\nNo\nAbeje et al. (2022)\n2,000\nLSTM\n90.10 (accuracy)\nYes\nNo\nAlemneh et al. (2020)\n30,000\nhybrid\n98.00(accuracy)\nNo\nNo\nAfaan Oromo\nOljira (2020)\n3000\nNaive Bayes\n93.00\nNo\nNo\nRase (2020)\n1,452\nLSTM\n87.70\nNo\nNo\nWayessa and Abas (2020)\n1,810\nSVM\n90.00\nNo\nNo\nYadesa et al. (2020)\n341\ndictionary + contextual valance shifter\n86.10\nNo\nNo\nTigrinya\nTela (2020)\n4,000\nXLNet\n81.62\nNo\nNo\nTable 6: Summary of related works for selected Ethiopian languages in sentiment analysis tasks, Size shows\nthe annotated dataset used during the experiment, Score shows the outperformed model results evaluated using\nF1 score, Dataset and Model shows the availability of dataset and models in publicly accessible repositories,\nrespectively.\nand model availability. Still, there is a need for fur-\nther research to create publicly accessible datasets\nand models to improve the models’ performance\nand make them available for use in different appli-\ncations.\n5.5.3\nNews Classiﬁcation and Text\nSummarization\nThe development of an Amharic news text clas-\nsiﬁcation dataset is described in a publication by\nAzime and Mohammed (2021). The dataset con-\nsists of 50,000 sentences and is classiﬁed into six\ncategories, including local news, sports, politics,\ninternational news, business, and entertainment.\nFesseha et al. (2021b) created a Tigrigna text classi-\nﬁcation dataset with manual annotation, consisting\nof 30k news sentences categorized into six classes,\nincluding sport, agriculture, politics, religion, edu-\ncation, and health. To enhance their analysis, the\nauthors investigated the use of various word em-\nbedding techniques such as CNN, bag of words,\nskip-gram, and fastText. The dataset used for these\nexperiments was made publicly available, as shown\nin Table 7. The work by Megersa (2020) utilized a\ndataset collected from the Ethiopian News Agency\nto experiment with 8 and 20 classes, but unfortu-\nnately, both the model and datasets are not publicly\navailable.\nHasan et al. (2021) created an abstractive sum-\nmarization dataset for 44 different languages using\nBBC articles collected via crawling. The resulting\ndataset comprises 5461 Amharic, 4827 Tigrinya,\nand 5,738 Afaan Oromo samples, which can poten-\ntially be employed for various Ethiopian language-\nrelated tasks. The authors ﬁne-tuned mt5 models\nusing this dataset and subsequently reported the\noutcomes. All publicly available data and code are\nlisted in Table 7 for exploration. In general, news\nclassiﬁcation and text summarization has not yet\nbeen properly researched for Ethiopian languages.\n6\nSummary of Challenges, Opportunities\nand Future Directions\nChallenges: Based on the ﬁndings of the above\nstudies, we identiﬁed the following challenges: (i)\nA scarcity of publicly available data for Ethiopian\nlanguages. As the data and resources are not mostly\npublicly available, researchers are going to \"re-\ninventing the wheel\" by trying to address the prob-\nlem. This leaves the low-resource language re-\nsearch usually ‘in limbo‘, as it is not clear if the\nproblem is addressed or not. This further makes it\ndifﬁcult to train different NLP tasks for Ethiopian\nlanguages and limits the scope of NLP applications.\nMoreover, it is very difﬁcult to reproduce results\nsince the benchmark datasets are not maintained.\n(ii) A lack of resources, tools, and infrastructure\nfor NLP research in low-resource Ethiopian lan-\nguages, can make it difﬁcult to attract funding and\ntalented researchers to work on the problem. (iii)\nFew people are interested in NLP for low-resource\nEthiopian languages. This can make it difﬁcult to\nattract resources and support for NLP research in\nthese languages.\nOpportunities:\nHere are some suggestions\nand ideas for the future that will help get more\nEthiopian languages into NLP research: (i) There\nneeds to be more work done to collect and label\ndata in Ethiopian languages. This will require col-\nlaboration between linguists, NLP experts, and na-\ntive speakers of the languages. (ii) As the results\nof the addressed NLP tasks are not comparable\nsince the datasets are different, one big issue to\naddress in the future is the release of benchmark\ndatasets on which researchers can work on improv-\ning performance and developing new approaches.\nThis will require sustained funding and collabo-\nration among researchers. (iii) The development\nAuthor(s)\nTask\nLanguage\ndataset link\nGezmu et al. (2021)\nMT\nAmh-Eng\nhttp://dx.doi.org/10.24352/ub.ovgu-2018-144\nBelay et al. (2022a)\nMT\nAmh-Eng\nhttps://github.com/atnafuatx/EthioNMT-datasets\nAbate et al. (2019)\nMT\nAmh-Eng, Orm-Eng, Tir-Eng, Wal-Eng\nhttp://github.com/AAUThematic4LT/\nLakew et al. (2020)\nMT\nAmh-Eng, Orm-Eng, Tir-Eng\nhttps://github.com/surafelml/Afro-NMT\nVegi et al. (2022)\nMT\nAmh-Eng, Orm-Eng\nhttps://github.com/pavanpankaj/Web-Crawl-African\nTedla et al. (2016)\nPOS\nTir\nhttps://eng.jnlp.org/yemane/ntigcorpus\nBelay et al. (2022b)\nQC\nAmh\nhttps://github.com/uhh-lt/amharicmodels\nNega et al. (2016)\nQC\nAmh\nhttps://github.com/seyyaw/amharicquestionanswering\nAdelani et al. (2021)\nNER\nAmh\nhttps://github.com/masakhane-io/masakhane-ner\nJibril and Tant˘g (2022)\nNER\nAmh\nhttps://github.com/Ebrahimc/\nSAY project NER dataset\nNER\nAmh\nhttps://github.com/geezorg/data\nYimam et al. (2020)\nSA\nAmh\nhttps://github.com/uhh-lt/ASAB\nAyele et al. (2022)\nhate\nAmh\nhttps://github.com/uhh-lt/amharicmodels\nMinale (2022)\nhate\nAmh (dataset only)\nhttps://data.mendeley.com/datasets/p74pfhz3yx/\nAbebaw et al. (2022)\nhate\nAmh\nhttps://zenodo.org/record/5036437\nFesseha et al. (2021b)\nnews\nTir\nhttps://github.com/canawet/\nAzime and Mohammed (2021)\nnews\nAmh\nhttps://github.com/IsraelAbebe/\nHasan et al. (2021)\ntext summ.\nAmh, Orm, Tir\nhttps://github.com/csebuetnlp/xl-sum\nTable 7: Available datasets for Ethiopian languages.\nof machine translation systems for low-resource\nEthiopian languages can help bridge the language\ngap and enable communication across different\nlanguages. (iv) Transfer learning techniques can\nbe used to leverage pre-trained models in high-\nresource languages to improve the performance of\nmodels in low-resource languages. (v) The involve-\nment of local communities and stakeholders is criti-\ncal for the success of NLP research in low-resource\nEthiopian languages. People in the community can\ngive researchers and developers important informa-\ntion about the language and culture.\nImpact of this work and future directions: The\nresults of this survey could be used to support\nfuture research initiatives in the ﬁeld of NLP in\nEthiopian Languages. Researchers can use the ﬁnd-\nings of the survey to identify areas that require fur-\nther investigation and to develop research proposals\nthat address the challenges and opportunities identi-\nﬁed in the survey. This work also helps to conduct\nmore surveys and develop a low-resource language\ndemarcation. The demarcation helps to identify\nlanguages that need more NLP research attention.\nAdding more Ethiopian languages to NLP research\nwill require researchers, linguists, and native speak-\ners of the languages to work together, hence, at\nsome point, these languages will be not considered\nlow-resource languages anymore. Moreover, we\npoint out with caution that not all the gaps and chal-\nlenging problems can be instantly and readily ﬁxed\nby researchers and research teams alone. Some of\nthese problems call for sustained community coop-\neration as well as signiﬁcant research funding from\nacademic funding organizations. The difﬁculties\nwe discussed in this paper are based on what we\nhave learned from published research work and a\nquick scan of available corpora. Further studies\nwith more comprehensive analysis, such as ques-\ntionnaires directed to resource authors and users, or\na more systematic inspection of the available data,\ncan provide a deeper understanding of the causes\nof these problems and suggest effective solutions.\n7\nConclusion\nIn this work, we investigated the most common\nNLP tasks and research works carried out in four\nEthiopian languages. We explored the main NLP\nresearch directions, progress, challenges, and op-\nportunities for Ethiopian languages.\nOur ﬁnd-\nings revealed that a signiﬁcant amount of research\nhas been centered on English or Amharic-centric\nmachine translation tasks. Despite there being a\nplethora of written languages in Ethiopia, only a\nfew of them have been explored in common re-\nsearch studies. Additionally, we observed a low\nprevalence of valuable resource publications in in-\nternational conference venues. The majority of\nworks are master’s theses. The publicly available\ndatasets, models, and tools are released in a central-\nized GitHub repository2. In the future, we plan to\nconduct a survey on more African languages and\ntry to come up with an NLP resource demarcation\nline that could help funders to prioritize research\ntopics and languages.\nReferences\nTeshome Mulugeta Ababu and Michael Melese Wold-\neyohannis. 2022. Afaan oromo hate speech detec-\ntion and classiﬁcation on social media. In Proceed-\n2https://github.com/EthioNLP/Ethiopian-Language-\nSurvey\nings of the Thirteenth Language Resources and Eval-\nuation Conference, pages 6612–6619.\nAbdo Ababor Abafogi. 2021. Boosting afaan oromo\nnamed entity recognition with multiple methods. In-\nternational Journal of Information Engineering and\nElectronic Business (IJIEEB), 13(5):51–59.\nSolomon Teferra Abate, Michael Melese, Martha Yi-\nﬁru Tachbelie, Million Meshesha, Solomon Atinafu,\nWondwossen Mulugeta, Yaregal Assabie, Hafte\nAbera, Biniyam Ephrem, Tewodros Gebreselassie,\net al. 2019. English-ethiopian languages statistical\nmachine translation.\nIn Proceedings of the 2019\nWorkshop on Widening NLP, pages 27–30.\nSani Genemo Abdi. 2015. Afaan Oromo Named Entity\nRecognition Using Hybrid Approach. Unpublished\nmaster thesis, Addis Ababa University.\nZeleke Abebaw, Andreas Rauber, and Solomon At-\nnafu. 2022. Multi-channel convolutional neural net-\nwork for hate speech detection in social media. In\nAdvances of Science and Technology: 9th EAI In-\nternational Conference, ICAST 2021, Hybrid Event,\nBahir Dar, Ethiopia, August 27–29, 2021, Proceed-\nings, Part I, pages 603–618. Springer.\nBekalu Tadele Abeje, Ayodeji Olalekan Salau, Hab-\ntamu Abate Ebabu, and Aleka Melese Ayalew. 2022.\nComparative analysis of deep learning models for as-\npect level amharic news sentiment analysis. In 2022\nInternational Conference on Decision Aid Sciences\nand Applications (DASA), pages 1628–1633. IEEE.\nSisay Fissaha Adafre. 2005.\nPart of speech tagging\nfor amharic using conditional random ﬁelds. In Pro-\nceedings of the ACL workshop on computational ap-\nproaches to semitic languages, pages 47–54.\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neu-\nbig, Daniel D’souza, Julia Kreutzer, Constantine\nLignos, Chester Palen-Michel, Happy Buzaaba,\nShruti Rijhwani, Sebastian Ruder, Stephen May-\nhew, Israel Abebe Azime, Shamsuddeen H. Muham-\nmad, Chris Chinenye Emezue, Joyce Nakatumba-\nNabende,\nPerez\nOgayo,\nAremu\nAnuoluwapo,\nCatherine Gitau, Derguene Mbaye, Jesujoba Al-\nabi, Seid Muhie Yimam, Tajuddeen Rabiu Gwad-\nabe, Ignatius Ezeani, Rubungo Andre Niyongabo,\nJonathan Mukiibi, Verrah Otiende, Iroro Orife,\nDavis David, Samba Ngom, Tosin Adewumi, Paul\nRayson, Mofetoluwa Adeyemi, Gerald Muriuki,\nEmmanuel Anebi, Chiamaka Chukwuneke, Nkiruka\nOdu,\nEric Peter Wairagala,\nSamuel Oyerinde,\nClemencia Siro, Tobius Saul Bateesa, Temilola\nOloyede, Yvonne Wambui, Victor Akinode, Deb-\norah Nabagereka,\nMaurice Katusiime,\nAyodele\nAwokoya, Mouhamadane MBOUP, Dibora Gebrey-\nohannes, Henok Tilaye, Kelechi Nwaike, Degaga\nWolde, Abdoulaye Faye, Blessing Sibanda, Ore-\nvaoghene Ahia, Bonaventure F. P. Dossou, Kelechi\nOgueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,\nAdewale Akinfaderin, Tendai Marengereke, and Sa-\nlomey Osei. 2021.\nMasakhaNER: Named Entity\nRecognition for African Languages.\nTransactions\nof the Association for Computational Linguistics,\n9:1116–1131.\nSisay Adugna and Andreas Eisele. 2010.\nEn-\nglish—oromo machine translation: An experiment\nusing a statistical approach. In Proceedings of the\nSeventh International Conference on Language Re-\nsources and Evaluation (LREC’10).\nGirma Neshir Alemneh, Andreas Rauber, and Solomon\nAtnafu. 2020.\nNegation handling for amharic\nsentiment classiﬁcation.\nIn Proceedings of the\nThe Fourth Widening Natural Language Processing\nWorkshop, pages 4–6.\nBesuﬁkad Alemu. 2013. A named entity recognition\nfor Amharic.\nUnpublished master thesis, Addis\nAbaba University.\nKIBROM HAFTU AMARE. 2016.\nTIGRIGNA\nQUESTION ANSWERING SYSTEM FOR FACTOID\nQUESTIONS.\nUnpublished master thesis, Addis\nAbaba University.\nYeabsira Asefa Ashengo, Rosa Tsegaye Aga, and\nSurafel Lemma Abebe. 2021.\nContext based ma-\nchine translation with recurrent neural network for\nenglish–amharic translation. Machine Translation,\n35(1):19–36.\nAbraham Gizaw Ayana. 2015.\nTowards improving\nbrill’s tagger lexical and transformation rule for\nafaan oromo language. Department of Geographic\nInformation Science, Hawassa Universty, Hawassa.\nAbinew Ali Ayele, Skadi Dinter, Tadesse Destaw Be-\nlay, Tesfa Tegegne Asfaw, Seid Muhie Yimam, and\nChris Biemann. 2022. The 5js in ethiopia: Amharic\nhate speech data annotation using toloka crowd-\nsourcing platform.\nIn 2022 International Confer-\nence on Information and Communication Technol-\nogy for Development for Africa (ICT4DA), pages\n114–120. IEEE.\nM Azath and Tsegay Kiros. 2020. Statistical machine\ntranslator for english to tigrigna translation. Int. J.\nSci. Technol. Res, 9(1):2095–2099.\nIsrael Abebe Azime and Nebil Mohammed. 2021. An\namharic news text classiﬁcation dataset.\nCoRR,\nabs/2103.05639.\nWeldemariam Bahre. 2022.\nHate Speech Detection\nfrom Facebook Social Media Posts and Comments\nin Tigrigna language. Ph.D. thesis, St. Mary’s Uni-\nversity.\nEmuye Bawoke. 2020. Amharic text hate speech detec-\ntion in social media using deep learning approach.\nPh.D. thesis.\nTadesse Destaw Belay, Atnafu Lambebo Tonja, Olga\nKolesnikova, Seid Muhie Yimam, Abinew Ali Ayele,\nSilesh Bogale Haile, Grigori Sidorov, and Alexan-\nder Gelbukh. 2022a. The effect of normalization for\nbi-directional amharic-english neural machine trans-\nlation. In 2022 International Conference on Infor-\nmation and Communication Technology for Devel-\nopment for Africa (ICT4DA), pages 84–89. IEEE.\nTadesse Destaw Belay, Seid Muhie Yimam, Abinew\nAyele, and Chris Biemann. 2022b. Question answer-\ning classiﬁcation for Amharic social media commu-\nnity based questions.\nIn Proceedings of the 1st\nAnnual Meeting of the ELRA/ISCA Special Inter-\nest Group on Under-Resourced Languages, pages\n137–145, Marseille, France. European Language Re-\nsources Association.\nZemicheal Berihu, Gebremariam Mesﬁn Assres, Mu-\nlugeta Atsbaha,\nand Tor-Morten Grønli. 2020.\nEnhancing bi-directional english-tigrigna machine\ntranslation using hybrid approach.\nIn Norsk IKT-\nkonferanse for forskning og utdanning, 1.\nYohanens Biadgligne and Kamel Smaïli. 2021. Paral-\nlel corpora preparation for english-amharic machine\ntranslation. In Advances in Computational Intelli-\ngence: 16th International Work-Conference on Arti-\nﬁcial Neural Networks, IWANN 2021, Virtual Event,\nJune 16–18, 2021, Proceedings, Part I 16, pages\n443–455. Springer.\nYohannes Biadgligne and Kamel Smaïli. 2022. Ofﬂine\ncorpus augmentation for english-amharic machine\ntranslation.\nIn 2022 5th International Conference\non Information and Computer Technologies (ICICT),\npages 128–135. IEEE.\nSidamo Biruk. 2021.\nNamed Entity Recognition for\nWolaytta Language Using Machine Learning Ap-\nproach. Unpublished master thesis, Adama Science\nand Technology University.\nSisay Chala, Bekele Debisa, Amante Diriba, Silas\nGetachew, Chala Getu, and Solomon Shiferaw. 2021.\nCrowdsourcing parallel corpus for english-oromo\nneural machine translation using community engage-\nment platform. arXiv preprint arXiv:2102.07539.\nFita Elanso Chaltu. 2016. Afaan Oromo List, Deﬁnition\nand Description Question Answering System. Un-\npublished master thesis, Addis Ababa University.\nEndale Daba. 2021. Improving Afaan Oromo question\nanswering system: deﬁnition, list and description\nquestion types for non-factoid questions.\nUnpub-\nlished master thesis, St. Mary’s University.\nNB Defersha and KK Tune. 2021. Detection of hate\nspeech text in afan oromo social media using ma-\nchine learning approach.\nIndian J Sci Technol,\n14(31):2567–78.\nDavid M. Eberhard, Gary F. Simons, and Charles D.\nFennig. 2022. Ethnologue: Languages of the World.\nTwenty-ﬁfth edition. Dallas, Texas:\nSIL Interna-\ntional. Url: http://www.ethnologue.com.\nAwet Fesseha, Shengwu Xiong, Eshete Derb Emiru,\nMoussa Diallo, and Abdelghani Dahou. 2021a. Text\nclassiﬁcation based on convolutional neural net-\nworks and word embedding for low-resource lan-\nguages: Tigrinya. Information, 12(2):52.\nAwet Fesseha, Shengwu Xiong, Eshete Derb Emiru,\nMoussa Diallo, and Abdelghani Dahou. 2021b. Text\nclassiﬁcation based on convolutional neural net-\nworks and word embedding for low-resource lan-\nguages: Tigrinya. Information, 12(2).\nBjörn Gambäck, Fredrik Olsson, Atelach Alemu Ar-\ngaw, and Lars Asker. 2009. Methods for amharic\npart-of-speech tagging. In First Workshop on Lan-\nguage Technologies for African Languages, March\n2009, Athens, Greece.\nBjörn Gambäck and Utpal Kumar Sikdar. 2017.\nNamed entity recognition for amharic using deep\nlearning. In 2017 IST-Africa Week Conference (IST-\nAfrica), pages 1–8. IEEE.\nB Gardie and Z Solomon. 2022. Afan-oromo named\nentity recognition using bidirectional rnn.\nIndian\nJournal of Science and Technology, 15(16):736–\n741.\nIbrahim Gashaw and H L Shashirekha. 2020. Machine\nlearning approaches for amharic parts-of-speech tag-\nging. arXiv preprint arXiv:2001.03324.\nMichael Gasser. 2011.\nHornmorpho: a system for\nmorphological processing of amharic, oromo, and\ntigrinya. In Conference on Human Language Tech-\nnology for Development, Alexandria, Egypt, pages\n94–99.\nBinyam Gebrekidan Gebre. 2010.\nPart of speech\ntagging for Amharic.\nPh.D. thesis, University of\nWolverhampton Wolverhampton.\nEbisa A Gemechu and GR Kanagachidambaresan.\n2021. Machine learning approach to english-afaan\noromo text-text translation: Using attention based\nneural machine translation.\nIn 2021 4th Interna-\ntional Conference on Computing and Communica-\ntions Technologies (ICCCT), pages 80–85. IEEE.\nFantahun Gereme, William Zhu, Tewodros Ayall, and\nDagmawi Alemu. 2021.\nCombating fake news in\n“low-resource” languages: Amharic fake news detec-\ntion accompanied by resource crafting. Information,\n12(1):20.\nAndargachew Mekonnen Gezmu, Andreas Nürnberger,\nand Tesfaye Bayu Bati. 2021. Extended parallel cor-\npus for amharic-english machine translation. arXiv\npreprint arXiv:2104.03543.\nSaron Habtamu. 2021. Amharic Question Classiﬁca-\ntion System Using Deep Learning Approach.\nUn-\npublished master thesis, Addis Ababa University.\nTahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam,\nKazi Samin, Yuan-Fang Li, Yong-Bin Kang, M. So-\nhel Rahman, and Rifat Shahriyar. 2021.\nXl-sum:\nLarge-scale multilingual abstractive summarization\nfor 44 languages. CoRR, abs/2106.13822.\nSINTAYEHU HIRPSSA and GS Lehal. 2020. Pos tag-\nging for amharic text: A machine learning approach.\nINFOCOMP: Journal of Computer Science, 19(1).\nEbrahim Chekol Jibril and A Cüneyd Tant˘g. 2022.\nAnec:\nAn amharic named entity corpus and\ntransformer based recognizer.\narXiv preprint\narXiv:2207.00785.\nKrishna Prakash Kalyanathaya, D Akila, and P Rajesh.\n2019. Advances in natural language processing–a\nsurvey of current research trends, development tools\nand industry applications.\nInternational Journal\nof Recent Technology and Engineering, 7(5C):199–\n202.\nLata Guta Kanessa and Solomon Gizaw Tulu. 2021.\nAutomatic hate and offensive speech detection\nframework from social media: the case of afaan oro-\nmoo language. In 2021 International Conference on\nInformation and Communication Technology for De-\nvelopment for Africa (ICT4DA), pages 42–47. IEEE.\nDorothy Kenny. 2018.\nMachine translation.\nIn The\nRoutledge handbook of translation and philosophy,\npages 428–445. Routledge.\nLidia Kidane, Sachin Kumar, and Yulia Tsvetkov. 2021.\nAn exploration of data augmentation techniques for\nimproving english to tigrinya translation.\narXiv\npreprint arXiv:2103.16789.\nSurafel M Lakew, Matteo Negri, and Marco Turchi.\n2020.\nLow resource neural machine translation:\nA benchmark for ﬁve african languages.\narXiv\npreprint arXiv:2003.14402.\nMandefro Legesse. 2012. Named Entity Recognition\nfor Afan Oromo. Unpublished master thesis, Addis\nAbaba University.\nXin Li and Dan Roth. 2006. Learning question clas-\nsiﬁers: the role of semantic information. Natural\nLanguage Engineering, 12(3):229–249.\nFanta Teferi Megersa. 2020.\nHierarchical afaan oro-\nmoo news text classiﬁcation.\nMoges Ahmed Mehamed. 2019. Named entity recog-\nnition for Amharic language. LAP LAMBERT Aca-\ndemic Publishing.\nHabtamu Mekonnen. 2019. Amharic-awngi machine\ntranslation:\nAn experiment using statistical ap-\nproach. International Journal of Computer Sciences\nand Engineering, 7:6–10.\nMillion\nMeshesha\nand\nYitayew\nSolomon.\n2018.\nEnglish-afaan oromo statistical machine translation.\nInternational Journal of Computational Linguistic\n(IJCL), 9(1).\nSamuel Minale. 2022. Amharic social media dataset\nfor hate speech detection and classiﬁcation in\namharic text with deep learning.\nZewdie Mossie and Jenq-Haur Wang. 2018.\nSo-\ncial network hate speech detection for amharic lan-\nguage. Computer Science & Information Technol-\nogy, page 41.\nZewdie Mossie and Jenq-Haur Wang. 2019. Vulnera-\nble community identiﬁcation using hate speech de-\ntection on social media. Information Processing &\nManagement, page 102087.\nAdane Nega, Workneh Chekol, and Alemu Kumlachew.\n2016. Question classiﬁcation in amharic question\nanswering system: Machine learning approach. In-\nternational Journal of Advanced Studies in Comput-\ners, Science and Engineering, 5(10):14–21.\nMegersa Oljira. 2020.\nSentiment analysis of afaan\noromo using machine learning approach. Interna-\ntional Journal of Research Studies in Science, Engi-\nneering and Technology, 7(9):7–15.\nOmer Osman and Yoshiki Mikami. 2012. Stemming\ntigrinya words for information retrieval.\nIn Pro-\nceedings of COLING 2012: Demonstration Papers,\npages 345–352.\nJaruwat Pailai, Rachada Kongkachandra, Thepchai\nSupnithi, and Prachya Boonkwan. 2013.\nA com-\nparative study on different techniques for thai part-\nof-speech tagging. In 2013 10th International Con-\nference on Electrical Engineering/Electronics, Com-\nputer, Telecommunications and Information Technol-\nogy, pages 1–5. IEEE.\nWondwossen Philemon and Wondwossen Mulugeta.\n2014.\nA machine learning approach to multi-\nscale sentiment analysis of amharic online posts.\nHiLCoE Journal of Computer Science and Technol-\nogy, 2(2):8.\nSurangika Ranathunga, En-Shiun Annie Lee, Mar-\njana Prifti Skenduli, Ravi Shekhar, Mehreen Alam,\nand Rishemjit Kaur. 2021. Neural machine transla-\ntion for low-resource languages: A survey.\nACM\nComputing Surveys.\nMegersa Oljira Rase. 2020.\nSentiment analysis of\nafaan oromoo facebook media using deep learning\napproach.\nNew Media and Mass Communication,\n90(2020):2224–3267.\nBinyam Ephrem Seyoum, Yusuke Miyao, and Baye Yi-\nmam Mekonnen. 2018. Universal dependencies for\namharic.\nIn Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018).\nBirhanesh Fikre Shirko. 2020. Part of speech tagging\nfor wolaita language using transformation based\nlearning (tbl) approach.\nUtpal Kumar Sikdar and Björn Gambäck. 2018.\nNamed entity recognition for amharic using stack-\nbased deep learning.\nIn Computational Linguis-\ntics and Intelligent Text Processing: 18th Interna-\ntional Conference, CICLing 2017, Budapest, Hun-\ngary, April 17–23, 2017, Revised Selected Papers,\nPart I 18, pages 276–287. Springer.\nYitayew Solomon, Million Meshesha, and Wendewe-\nsen Endale. 2017.\nOptimal alignment for bi-\ndirectional afaan oromo-english statistical machine\ntranslation. vol, 3:73–77.\nMartha Yiﬁru Tachbelie, Solomon Teferra Abate, and\nLaurent Besacier. 2011.\nPart-of-speech tagging\nfor underresourced and morphologically rich lan-\nguages—the case of amharic. HLTD (2011), pages\n50–55.\nMartha Yiﬁru Tachbelie and Wolfgang Menzel. 2009.\nAmharic part-of-speech tagger for factored language\nmodeling. In Proceedings of the International Con-\nference RANLP-2009, pages 428–433.\nMikiyas Tadele. 2014. Amharic named entity recogni-\ntion using a hybrid approach.\nTilahun Abedissa Taffa and Mulugeta Libsie. 2019.\nAmharic question answering for biography, deﬁni-\ntion, and description questions. In Proceedings of\nthe 2019 Workshop on Widening NLP, pages 110–\n113, Florence, Italy. Association for Computational\nLinguistics.\nYemane Tedla and Kazuhide Yamamoto. 2016. The ef-\nfect of shallow segmentation on english-tigrinya sta-\ntistical machine translation. In 2016 International\nConference on Asian Language Processing (IALP),\npages 79–82. IEEE.\nYemane Tedla and Kazuhide Yamamoto. 2017. Mor-\nphological segmentation for english-to-tigrinya sta-\ntistical machinetranslation. Int. J. Asian Lang. Pro-\ncess, 27(2):95–110.\nYemane Keleta Tedla, Kazuhide Yamamoto, and Ashu-\nboda Marasinghe. 2016.\nTigrinya part-of-speech\ntagging with morphological patterns and the new\nnagaoka tigrinya corpus. International Journal of\nComputer Applications, 146(14).\nAbrhalei Frezghi Tela. 2020. Sentiment analysis for\nlow-resource language: The case of tigrinya. Mas-\nter’s thesis, Itä-Suomen yliopisto.\nSenait\nGebremichael\nTesfagergish\nand\nJurgita\nKapociute-Dzikiene. 2020.\nDeep learning-based\npart-of-speech tagging of the tigrinya language.\nIn Information and Software Technologies:\n26th\nInternational Conference, ICIST 2020, Kaunas,\nLithuania, October 15–17, 2020, Proceedings 26,\npages 357–367. Springer.\nAtnafu Lambebo Tonja, Olga Kolesnikova, Alexan-\nder Gelbukh, and Grigori Sidorov. 2023.\nLow-\nresource neural machine translation improvement us-\ning source-side monolingual data. Applied Sciences,\n13(2):1201.\nAtnafu Lambebo Tonja, Michael Melese Woldeyohan-\nnis, and Mesay Gemeda Yigezu. 2021.\nA paral-\nlel corpora for bi-directional neural machine transla-\ntion for low resourced ethiopian languages. In 2021\nInternational Conference on Information and Com-\nmunication Technology for Development for Africa\n(ICT4DA), pages 71–76. IEEE.\nPavanpankaj Vegi, Sivabhavani J, Biswajit Paul, Abhi-\nnav Mishra, Prashant Banjare, Prasanna K R, and\nChitra Viswanathan. 2022. WebCrawl African : A\nmultilingual parallel corpora for African languages.\nIn Proceedings of the Seventh Conference on Ma-\nchine Translation (WMT), pages 1076–1089, Abu\nDhabi, United Arab Emirates (Hybrid). Association\nfor Computational Linguistics.\nNegessa Wayessa and Sadik Abas. 2020. Multi-class\nsentiment analysis from afaan oromo text based on\nsupervised machine learning approaches.\nInterna-\ntional Journal of Research Studies in Science, Engi-\nneering and Technology, 7(7):10–18.\nGetachew Mamo Wegari and M Meshesha. 2011. Parts\nof speech tagging for afaan oromo.\nInternational\nJournal of Advanced Computer Science and Appli-\ncations, 1(3):1–5.\nMichael Melese Woldeyohannis and Million Meshesha.\n2018. Experimenting statistical machine translation\nfor ethiopic semitic languages: The case of amharic-\ntigrigna. In Information and Communication Tech-\nnology for Development for Africa, pages 140–149,\nCham. Springer International Publishing.\nTariku Birhanu Yadesa, Syed Umar, and Tagay Takele\nFikadu. 2020. Sentiment mining model for opinion-\nated afaan oromo texts.\nSeid Muhie Yimam,\nHizkiel Mitiku Alemayehu,\nAbinew Ayele, and Chris Biemann. 2020. Exploring\namharic sentiment analysis from social media texts:\nBuilding annotation tools and classiﬁcation models.\nIn Proceedings of the 28th International Conference\non Computational Linguistics, pages 1048–1060.\nSeid Muhie Yimam, Abinew Ali Ayele, Gopalakrish-\nnan Venkatesh, Ibrahim Gashaw, and Chris Bie-\nmann. 2021.\nIntroducing various semantic mod-\nels for amharic:\nExperimentation and evaluation\nwith multiple tasks and datasets.\nFuture Internet,\n13(11):275.\nSeid Muhie Yimam and Mulugeta Libsie. 2009.\nTETEYEQ: Amharic question answering for factoid\nquestions. IE-IR-LRL, 3(4):17–25.\nHailemariam Mehari Yohannes and Toshiyuki Ama-\ngasa. 2022a. A method of named entity recognition\nfor tigrinya. ACM SIGAPP Applied Computing Re-\nview, 22(3):56–68.\nHailemariam Mehari Yohannes and Toshiyuki Ama-\ngasa. 2022b. Named-entity recognition for a low-\nresource language using pre-trained language model.\nIn Proceedings of the 37th ACM/SIGAPP Sympo-\nsium on Applied Computing, pages 837–844.\nA\nMT Summary\nFigure 6 shows the MT progress per year. As it can\nbe seen from the ﬁgure in recent years MT research\nfor Ethiopian languages getting attention.\nFigure 6: MT progress per year\nFigure 7 shows the dataset availability per publi-\ncation year. It can be noted from the table that in\nrecent works there are attempts to make datasets\navailable for Ethiopian languages but this still\nneeds more effort.\nFigure 7: (MT=>English centeric) Dataset availability\nper publication year\nFigure 8 shows the publications and methodolo-\ngies used. It can be seen from the ﬁgure that before\n2021 the dominant methodology used by different\nresearchers was SMT, but in recent years different\nresearchers have applied a neural network-based\napproach even if its performance depends on the\navailability of parallel datasets.\nFigure 8: (MT=>English centeric) Methodology per\npublication year\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-03-25",
  "updated": "2023-03-25"
}