{
  "id": "http://arxiv.org/abs/2303.14705v1",
  "title": "Control of synaptic plasticity via the fusion of reinforcement learning and unsupervised learning in neural networks",
  "authors": [
    "Mohammad Modiri"
  ],
  "abstract": "The brain can learn to execute a wide variety of tasks quickly and\nefficiently. Nevertheless, most of the mechanisms that enable us to learn are\nunclear or incredibly complicated. Recently, considerable efforts have been\nmade in neuroscience and artificial intelligence to understand and model the\nstructure and mechanisms behind the amazing learning capability of the brain.\nHowever, in the current understanding of cognitive neuroscience, it is widely\naccepted that synaptic plasticity plays an essential role in our amazing\nlearning capability. This mechanism is also known as the Credit Assignment\nProblem (CAP) and is a fundamental challenge in neuroscience and Artificial\nIntelligence (AI). The observations of neuroscientists clearly confirm the role\nof two important mechanisms including the error feedback system and\nunsupervised learning in synaptic plasticity. With this inspiration, a new\nlearning rule is proposed via the fusion of reinforcement learning (RL) and\nunsupervised learning (UL). In the proposed computational model, the nonlinear\noptimal control theory is used to resemble the error feedback loop systems and\nproject the output error to neurons membrane potential (neurons state), and an\nunsupervised learning rule based on neurons membrane potential or neurons\nactivity are utilized to simulate synaptic plasticity dynamics to ensure that\nthe output error is minimized.",
  "text": "1 \n \nControl of synaptic plasticity via the fusion of \nreinforcement learning and unsupervised learning in neural \nnetworks1 \nMohammad Modiri2  \n \n \n \nAbstract \nThe brain can learn to execute a wide variety of tasks quickly and efficiently. \nNevertheless, most of the mechanisms that enable us to learn are unclear or incredibly \ncomplicated. Recently, considerable efforts have been made in neuroscience and \nartificial intelligence to understand and model the structure and mechanisms behind \nthe amazing learning capability of the brain. However, in the current understanding of \ncognitive neuroscience, it is widely accepted that synaptic plasticity plays an essential \nrole in our amazing learning capability. This mechanism is also known as the Credit \nAssignment Problem (CAP) and is a fundamental challenge in neuroscience and \nArtificial Intelligence (AI). The observations of neuroscientists clearly confirm the \nrole of two important mechanisms including the error feedback system and \nunsupervised learning in synaptic plasticity. With this inspiration, a new learning rule \nis proposed via the fusion of reinforcement learning (RL) and unsupervised learning \n(UL). In the proposed computational model, the nonlinear optimal control theory is \nused to resemble the error feedback loop systems and project the output error to state \nof neurons, and an unsupervised learning rule based on state of neurons or activity of \nneurons are utilized to simulate synaptic plasticity dynamics to ensure that the output \nerror is minimized.  \n \nKeywords: learning rule, reinforcement learning, unsupervised learning, neural \nnetwork, nonlinear control. \n                                                           \n1 This article is part of a new learning rule that shared to get feedback from researchers. \n2  Email address: Mohammad.Modiri@Gmail.com \n2 \n \n1. Proposed framework \nIn this section, we propose a novel brain-inspired learning rule. The main idea is to \ncombine a RL-based method and unsupervised learning to control synaptic plasticity. \nFig. 1 shows the main ideas underlying this computational model that can be applied \nin different tasks such as classification, prediction, and robotics. \nIn the proposed computational model, learning law reformulated as an optimal \ncontrol problem. For this purpose, all tasks even classification assumed as a dynamic \nsystem that illustrated in Fig. 1 (C). Since the NNs can be considered a special class \nof high-dimensional nonlinear dynamic systems, where both state and time are \ncontinuous. Therefore, continuous-time formulation of the HJB equation and ADP \ncan be applied to derive the learning rule (optimal control law). The derived learning \nrule forces the proposed framework's output to mimic the reference trajectory that \nimplicitly minimized the cross-entropy. The following subsections elaborate on the \nnetwork architecture, and the learning rule in further detail. \n1.1 Network architecture and dynamics \nThe proposed computational model consists of a feedforward and feedback \npathway. The feedforward pathway can be any form of neural network, however a \nthree parts module composed of an input layer, a neural network (NN), and an output \nlayer is recommended. The feedback pathway also different types of Actor-Critic or \nCritic only can be employed, although an Actor-Critic is recommended. \nFig. 1 shows the block diagram of the proposed computational model architecture. \nThe functionality of the proposed architecture is based on the following steps: \na. \nThe encoding layer projects input patterns 𝑥(𝑡) into the NN.  \nb. In the pertaining process, a supervised learning algorithm like Gradient descent, \nRidge, etc., can be employed to adjust the decoder parameters associated with \neach training sample. Once the decoder is trained, it is considered constant \nduring the NN training process. (Optional) \nc. \nIn the NN training process. An RL-based method and unsupervised learning are \nproposed in subsection 1.2 is applied to make the NN learn temporal relations \n3 \n \nbetween the input patterns 𝑥(𝑡)  and desired output 𝑦(𝑡) by adjusting the state \n(membrane potential) of neurons and consequently parameters in the NN using a \nnovel learning rule. In the proposed error feedback mechanism, two RC-based \nactor and critic are utilized to approximate true state of neurons and NN \nparameters. \nd. Model recall on new data. \nThe feedforward pathway resembles the structure of traditional neural networks, \nnevertheless the approach of the feedback pathway particularly proposed in this study \ndiffers considerably from classical artificial intelligence methods [1].  \n \n \nFig. 1. A schematic of the proposed architecture (plastic weights in red). (A) The feedforward pathway \nconsists of an encoding layer with fix and random parameters, a NN with trainable parameter and a decoder \nlayer, which is trained in the pre-training process and stay constant during the NN training process. (B) In \nthe feedback pathway, the nonlinear optimal control is applied for estimating state (membrane potential) of \nneurons and consequently NN parameters. It consists of actor and critic neural networks. (C) The subplots \nshows the reference trajectories and learned trajectories in difference tasks. \n \nRemark 1 For the sake of brevity, time dependence is suppressed while denoting \nvariables of dynamical systems. For instance, the notations 𝑥(𝑡), 𝑦(𝑡), 𝑢(𝑡), 𝑣(𝑡) and \n𝑒(𝑡) are rewritten as 𝑥, 𝑦, 𝑢, 𝑣  and 𝑒. Moreover, all math symbols used in this study \nare listed in Table 1. \n \n4 \n \nTable 1 The math symbols used in this paper and corresponding meanings \n \nSymbol \nMeaning \n𝒏𝒓 \nNumber of neurons \n𝑵𝒓 \nNumber of connections \n𝒙 \nInput pattern \n𝒚 \nDesired output  \n𝒚̂ \nEstimated output \n𝒆 \nOutput error \n𝑾𝑬 \nEncoder connections \n𝑾𝑫 \nDecoder connections \n𝑾𝒓 \nNetwork connections \n𝒗𝒅 \nDesirable state (membrane potential) of neuron \n𝒗𝒆 \nDifference between desirable and estimated state (membrane potential) of \nneuron \n𝒒 \n𝑬 \nfiltered spike activity of input in spiking NN  \nand is equal to input in non-spiking NN \n𝒒 \n 𝒓 \nfiltered spike activity of reservoir in spiking NN and is equal to state in non-\nspiking neurons \n𝑺  \nspike train in spiking NN \n𝛙(∙) \nLeaky term \n∅(∙) \nActivation function \n𝓵(∙.∙) \nUtility function \n𝒇(∙) \nDrift dynamic function \n𝒈(∙) \nInput dynamic \n𝓗(∙.∙.∙) \nHamiltonian function \n𝒖∗ \nDesirable state (membrane potential) of neuron \n𝒖 \nAdmissible policy \n𝒖𝟐 \nEstimated state (membrane potential) of neuron (input control) \n𝑽∗ \nOptimal cost function \n𝑽 \nCost function \n𝑽𝒗\n  \nDerivative of the cost function \n𝑾\n̂𝟏 \nCritic weights \n𝑾\n̂𝟐 \nActor weights \n𝜶𝟏 \nLearning rate of the actor \n𝜶𝟐 \nLearning rate of the critic \n \n \n \nAs illustrated in Fig. 1, The core element of the proposed computational model is a \nneural network with the adjacency matrix 𝑊𝑟∈ℝ𝑛𝑟×𝑛𝑟. Here, the NN consists of 𝑛𝑟 \nneurons, for which the membrane potential dynamics are described as: \n(1) \n𝜐̇ 𝑖= ψ(𝑣𝑖) + Ι𝑖 \n \n \n5 \n \n(2) \n𝐼𝑖= 𝑊𝑖\n𝐸∅(𝑣 \n𝐸) + 𝑊𝑖\n𝑟∅(𝑣) \n \n \n,where 𝜐= [𝜐1. 𝜐2. ⋯. 𝜐𝑛𝑟] ∈ℝ𝑛𝑟 is the state (membrane potential) of neuron of \nthe NN neurons,  ∅(∙) is a nonlinear dendrite, and, ψ(𝜐): ℝ𝑛𝑟→ℝ𝑛𝑟 is a leak-term \nwhere we used −𝛼𝑙𝑣, 𝑣 \n𝐸, 𝑣 \n 𝑟 in non-spiking neurons are directly equal to input 𝑥 and \nNN state 𝜐, and in spiking neurons 𝑣𝑖\n  shows the filtered spike activity of neuron 𝑖. \n𝑆𝑖\n \n (𝑡) is the spike train of the neuron 𝑖 and modelled as a sum of Dirac delta-\nfunctions: \n(3) \n𝑞𝑖\n = (𝑆𝑖\n \n ∗𝜅)(𝑡) = ∫\n𝑆𝑖\n (𝑠)𝜅(𝑡−𝑠)𝑑𝑠\n𝑡\n−∞\n   \n \n \n(4) \n𝜅(𝑡) = exp (−𝑡/𝜏)/𝜏  \n  \n \nIn additional, a linear decoder is supposed as: \n(5) \n𝑦̂ = ℎ(𝑣) = 𝑊𝐷𝑣  \n \n \n \n \n1.2 Synaptic plasticity mechanism \nThe equations mentioned in section 1.1 shows that the NN synaptic plasticity \nmechanism can be modeled as a dynamical system. Thus, principles of optimal \ncontrol theory can be utilized to derive a learning rule (optimal control law). For this \npurpose, we reformulate the AI tasks as a control problem. Therefore, the output error \nwere considered as follows: \n(6) \n𝑒= 𝑦̂ −𝑦∈ℝ𝑐 \n \n, where 𝑦∈ℝ𝑐 denotes the desired output at time 𝑡, 𝑦̂ ∈ℝ𝑐 is the corresponding \npredicted output, and 𝑐 is a number of output variables. By derivation of Eq. (6) with \nrespect to  𝑡, the error dynamics can be written as: \n(7) \n𝑒̇ = 𝑦̂̇ −𝑦̇  ∈ℝ𝑐  \n  \n \n \n  \n \n \nThus, the error dynamics only depend on the NN neural dynamics is equal to: \n \n6 \n \n(8) \n𝑒̇ = 𝑊𝐷𝑣̇ −𝑊𝑑\n𝐷𝑣̇𝑑\n= 𝑊𝐷[ψ(𝑣(𝑡)) + 𝑊𝐸𝑣 \n𝐸(𝑡) + 𝑊𝑟(𝑡)𝑣(𝑡)]  \n−𝑊𝑑\n𝐷[ψ𝑑(𝑣(𝑡)) + 𝑊𝑑\n𝐸𝑣 \n𝐸(𝑡) + 𝑊𝑑\n𝑟(𝑡)𝑣𝑑(𝑡)] \n \nAssumption 1 Assume the following simplification conditions: \n(9) \n𝑊𝑑\n𝐸= 𝑊𝐸 \n𝑊𝑑\n𝐷= 𝑊𝐷 \n \n \nAccording to Assumption 1, the error dynamics only depend on the NN.  \n \n(10) \n𝑒̇ = 𝑊𝐷[ψ𝑒(𝑣(𝑡)) + 𝑊𝑟(𝑡)𝑣𝑓(𝑡)] = 𝑊𝐷𝑣̇𝑒 \n  \n \n \nWe assumed: \n(11) \nψ𝑒(𝑣(𝑡)) = 𝑓(𝑣(𝑡)) \n  \n𝑊𝑟(𝑡) = 𝑔(𝑣(𝑡)) \n𝑢𝑟(𝑡) = 𝑣(𝑡) −𝑣𝑑(𝑡) = 𝑣𝑓(𝑡) \n \n \nTherefore, neural space Eq. (8) can be rewritten as the following affine state space \nequation from: \n(12) \n𝑒̇ = 𝑊𝐷𝑣̇𝑒= 𝑊𝐷[𝑓(𝑣(𝑡)) + 𝑔(𝑣(𝑡))𝑢𝑟(𝑡)] \n  \n \n \nWhere the control input is considered as state of neurons changes 𝑣̇(𝑡) and 𝑤𝑟 are \nfunction of control input of the NN, and 𝑢𝑟, 𝑊𝑟 which the number are selected \naccording to the problem: \n(13) \n𝑢𝑟= [𝑢𝑓; 𝑢] ∈ℝ𝑛𝑟 \n \n \n  \n \n \n(14) \n𝑊𝑟= [𝑊𝑓; 𝑊] ∈ℝ𝑛𝑟×𝑛𝑟 \n \n \n  \n \n \n7 \n \n, in 𝑢∈ℝ𝑛𝑟−𝑛𝑓 and 𝑊∈ℝ(𝑛𝑟−𝑛𝑓)×(𝑛𝑟−𝑛𝑓) is fixed or even zero NN \nparameters, 𝑢∈ℝ𝑛 and 𝑊∈ℝ𝑛×𝑛  is the NN control input and synaptic plasticity \nrespectively and [𝑥; 𝑦] means combination of 𝑥 and 𝑦.  \nIn the artificial intelligence (AI) tasks, the goal of learning is to adapt the NN’s \nparameters 𝑊𝑟 such that the error is minimized and the NN’s parameters and control \ninput remains bounded. Thus, the following cost function was considered: \n \n(15) \nmin\n𝑢(∙)∈𝑈ℑ(𝑒(∙). 𝑢(∙)) = ∫\nℓ(𝑒(𝜏). 𝑢(𝜏))𝑑𝜏\n𝑡𝑓\n𝑡\n  \n  \n \n \n(16) \nℓ(𝑒. 𝑢) = 𝑒𝑇𝑄𝑒+ 𝑢𝑇𝑅𝑢≈ℓ(𝑣𝑒. 𝑢) = 𝑣𝑒\n𝑇𝑄𝑣𝑒+ 𝑢𝑇𝑅𝑢   \n \n \n, wherein ℓ(∙.∙) is the utility function, 𝑡𝑓 is the final time or last element of input \npatterns, 𝑄. 𝑅 symmetric positive definite matrix for ensuring that the error in cost \nfunction is sufficiently affective. Hence, the optimal value function can be written: \n \n(17) \n𝑉(𝑒) ≈𝑉(𝑣𝑒) = 𝐼𝑛𝑓\n𝑢(∙)∈𝑈\nℑ(𝑢. 𝑣𝑒)   \n \nUltimately, it is ideal to achieve an optimal input control  𝑢∗ or 𝑣𝑒\n∗ and weight \nupdate law 𝑔∗(𝑣) that stabilizes the system Eq. (10) and minimizes the cost function \nEq. (17). This kind of input control 𝑢 is called admissible control [2].  \nNow, the learning rule has been reformulated given the error dynamic in Eq. (10) \nand the cost function Eq. (17). To solve this dynamic optimization problem, the HJB \nequation is utilized, so the Hamiltonian of the cost function Eq. (17)  associated with \ncontrol input 𝑢 is defined as: \n \n(18) \nℋ(𝑣𝑒. 𝑢. 𝑉𝑒\n  ) = ℓ(𝑣𝑒. 𝑢) + 𝑉𝑒\n𝑇(𝑣𝑒̇ ) \n \n \n, where 𝑉𝑒\n = 𝜕𝑉/𝜕𝑒 is the partial derivative of the cost function, for admissible \ncontrol policy 𝜇 we have: \n \n(19) \nℋ(𝑣𝑒. 𝜇. 𝑉𝑒\n  ) = 0 \n \n8 \n \nThe present study assumed that the solution to Eq. (19) is smooth giving the \noptimal cost function: \n(20) \n𝑉∗(𝑒) ≈𝑉∗(𝑣𝑒) = min\n𝑢(∫\nℓ(𝑢(𝜏). 𝑣𝑒(𝜏))𝑑𝜏\n𝑡𝑓\n𝑡\n) \n \n \n, which satisfies the HJB equation \n \n(21) \n𝑚𝑖𝑛\n𝑢  ℋ(𝑣𝑒. 𝑢. 𝑉𝑒\n∗ ) = 0 \n \nAssuming that the minimum on the left hand side Eq. (21) exists then by applying \nstationary condition 𝜕ℋ(𝑣𝑒. 𝑢. 𝑉𝑒\n  ) 𝜕\n⁄ 𝑢= 0, the learning rule (optimal control)  can \nbe obtained as: \n(22) \n𝑢∗(𝑒) ≈𝑢∗(𝑣𝑒) = −1\n2 𝑅−1𝑔𝑇(𝑣) 𝑉𝑒\n∗ \n \nThe optimal value function can be obtained as: \n(23) \n𝑉∗(𝑣𝑒) = min\n𝑢(∫\n𝑣𝑒\n𝑇𝑄𝑣𝑒+ 𝑢∗𝑇𝑅𝑢∗𝑑𝜏\n𝑡𝑓\n𝑡\n) \n \nInserting this optimal learning rule Eq. (22) into nonlinear Lyapunov equation \nEq.(18) gives the formulation of the HJB equation Eq.  (21) in terms of 𝑉𝑒\n∗ \n \n(24) \n0 = 𝑣𝑒\n𝑇𝑄𝑣𝑒+ 𝑉𝑒\n∗𝑇(𝑣𝑒)𝑓(𝑣) −1\n4 𝑉𝑒\n∗𝑇(𝑣𝑒)𝑔(𝑣)𝑅−1𝑔𝑇(𝑣)𝑉𝑒\n∗(𝑣𝑒) \n \n \nFinding the error feedback mechanism to stabilize the NN requires solving the HJB \nequation for the value function and then substituting the solution to obtain the desired \nstate of neurons update and consequently learning rule. Although HJB gives the \nnecessary and sufficient condition for optimality of a learning rule (control law) with \nrespect to a loss function; Unfortunately, according to the nonlinear characteristics of \nthe NN, solving the HJB equation in explicit form is difficult or even impossible to \nderive for systems of interest in practice. However, a few solution such as Feynman-\nKac lemma, Al’brecht, Garrard, Viscosity, Games based, Adaptive Dynamic \nProgramming (ADP) methods have been proposed to solve HJB approximately. \nMeanwhile, ADP offers a data driven solution that is more homogeneous with \n9 \n \nproposed framework. Thus, the proposed computational focuses on the ADP method \nto approximate its solution. To address this mater, with the inspiration of the \ndopaminergic region structure and conformity with the Weierstrass high-order \napproximation theorem [3], we utilized a new actor-critic based on the seminal \nReservoir Computing (s-RC) that proposed in [4]. Briefly, the objective of tuning the \nactor weights is to minimize the approximate value, and the objective of tuning the \ncritic weights is to minimize the Bellman equation error. \nTherefore, a s-RC is used as critic to approximate the derivatives of the value \nfunction: \n(25) \n𝑉𝑒\n  (𝑒) ≈𝑉𝑣𝑒\n  (𝑣𝑒) = 𝑊𝑐\n𝐷 𝑧𝑐+ 𝜀𝑐∈ℝ𝑐×1 \n𝑧𝑐∈ℝ𝑛𝑐×1 \n𝑊𝑐\n𝐷∈ℝ 𝑐×𝑛𝑐 \n \n \nWhere 𝑊𝑐\n𝐷 is the decoder weight, and 𝑧𝑐 is a representation of input 𝑣𝑒. By \napplying the gradient descent algorithm, critic weight update rule will be: \n(26) \n𝑊̂̇\n𝑐\n𝐷= −𝛼𝑐\n𝜕𝐸𝑐\n𝜕𝑊̂𝑐𝐷= −𝛼𝑐(𝑓(𝑣) + 𝑔(𝑣)𝑢)𝑧𝑐\n𝑇 𝑒𝑐\n= −𝛼𝑐𝜎𝑐(𝜎𝑐\n𝑇𝑊̂𝑐\n𝐷+ 𝑣𝑒\n𝑇𝑄𝑣𝑒+ 𝑢𝑇𝑅𝑢) \n \nLet 𝑊̂𝑐\n𝐷 is an estimation of unknown matrix 𝑊𝑐\n𝐷, 𝛼𝑐> 0 is critic learning rate, and \n𝜎𝑐= (𝑓(𝑣) + 𝑔(𝑣)𝑢)𝑧𝑐\n𝑇. \nSimilarly, one more s-RC is utilized as the actor to approximate the state \n(membrane potential) of neurons: \n(27) \n𝑢(𝑣𝑒) = 𝑊𝑎\n𝐷 𝑧𝑎+ 𝜀𝑎∈ℝ𝑁×1 \n𝑧𝑎∈ℝ𝑛𝑎×1 \n𝑊𝑎\n𝐷∈ℝ𝑁×𝑛𝑎 \n \n \nWhere 𝑧𝑎 be the new representation of input 𝑣𝑒,   𝑊̂𝑎\n𝐷 an estimation of unknown \nmatrix 𝑊𝑎\n𝐷. By using the gradient descent algorithm, a weight update expression for \nthe actor can be written as follows: \n(28)\n \n𝑊̂̇\n𝑎\n𝐷= −𝛼𝑎\n𝜕𝐸𝑎\n𝜕𝑊̂𝑎𝐷= −𝛼𝑎(𝑊̂𝑎\n𝐷𝑧𝑎+ 1\n2 𝑅−1𝑔𝑇(𝑣)(𝑊̂𝑐\n𝐷𝑧𝑐))𝑧𝑎\n𝑇 \n \nWhere 𝛼𝑎> 0 is actor learning rate. \n10 \n \n \nConsequently, in the course of the NN learning process, the feedback pathway \nforces the output of the computational model to follow the reference trajectory, an \neffect that is widely used in control theory. After an adequate learning time, the \nfeedforward pathway in the absent of feedback pathway can fallow desirable patterns \nwith acceptable accuracy. \nThe stability and convergence of the proposed framework and error can be \ninvestigated similar to [4]. However, there is an important issue that should be noted, \nwhere unlike the [4], the 𝑔(∙) is not clearly specified. Although, varied candidate \nsolutions that can be unsupervised learning such as the bounded spike timing \ndependent plasticity (STDP) can be suggested. Nevertheless, a comprehensive \ninvestigation requires for providing an ideal solution for 𝑔(∙), which is ignored in the \ncurrent version of article. \n \n \n11 \n \nReferences \n \n[1]  M. Modiri, M. M. Homayounpour and M. M. Ebadzadeh, \n\"Reservoir \nweights \nlearning \nbased \non \nadaptive \ndynamic \nprogramming and its application in time series classification,\" \nNeural Computing & Applications, 2020.  \n[2]  M. Khalaf and F. L. Lewis, \"Nearly optimal control laws for \nnonlinear systems with saturating actuators using a neural network \nHJB approach,\" Automatica, vol. 41, no. 5, pp. 779-791, 2005.  \n[3]  A. Finlayson, The method of weighted residuals and variational \nprinciples, New York: Academic Press, 1990.  \n[4]  M. Modiri, \"Control of synaptic plasticity in neural networks,\" \narXive.org, 2023.  \n[5]  H. K. Khalil, Nonlinear Control, Edinburgh: Pearson, 2014.  \n[6]  S. Ge, C. Hang and T. Zhang, \"Adaptive neural network control of \nnonlinear systems by state and output feedback,\" IEEE Transactions \non Systems, Man, and Cybernetics, vol. 29, no. 6, pp. 818-828, \n1999.  \n \n \n \n \n",
  "categories": [
    "cs.NE",
    "cs.AI",
    "cs.LG",
    "cs.RO",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2023-03-26",
  "updated": "2023-03-26"
}