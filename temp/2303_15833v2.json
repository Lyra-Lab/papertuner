{
  "id": "http://arxiv.org/abs/2303.15833v2",
  "title": "Complementary Domain Adaptation and Generalization for Unsupervised Continual Domain Shift Learning",
  "authors": [
    "Wonguk Cho",
    "Jinha Park",
    "Taesup Kim"
  ],
  "abstract": "Continual domain shift poses a significant challenge in real-world\napplications, particularly in situations where labeled data is not available\nfor new domains. The challenge of acquiring knowledge in this problem setting\nis referred to as unsupervised continual domain shift learning. Existing\nmethods for domain adaptation and generalization have limitations in addressing\nthis issue, as they focus either on adapting to a specific domain or\ngeneralizing to unseen domains, but not both. In this paper, we propose\nComplementary Domain Adaptation and Generalization (CoDAG), a simple yet\neffective learning framework that combines domain adaptation and generalization\nin a complementary manner to achieve three major goals of unsupervised\ncontinual domain shift learning: adapting to a current domain, generalizing to\nunseen domains, and preventing forgetting of previously seen domains. Our\napproach is model-agnostic, meaning that it is compatible with any existing\ndomain adaptation and generalization algorithms. We evaluate CoDAG on several\nbenchmark datasets and demonstrate that our model outperforms state-of-the-art\nmodels in all datasets and evaluation metrics, highlighting its effectiveness\nand robustness in handling unsupervised continual domain shift learning.",
  "text": "Complementary Domain Adaptation and Generalization\nfor Unsupervised Continual Domain Shift Learning\nWonguk Cho1, Jinha Park2, and Taesup Kim1∗\n1Graduate School of Data Science, 2Department of Electrical and Computer Engineering\nSeoul National University\n{wongukcho, jhpark410, taesup.kim}@snu.ac.kr\nAbstract\nContinual domain shift poses a significant challenge in\nreal-world applications, particularly in situations where la-\nbeled data is not available for new domains. The challenge\nof acquiring knowledge in this problem setting is referred\nto as unsupervised continual domain shift learning. Exist-\ning methods for domain adaptation and generalization have\nlimitations in addressing this issue, as they focus either on\nadapting to a specific domain or generalizing to unseen do-\nmains, but not both. In this paper, we propose Comple-\nmentary Domain Adaptation and Generalization (CoDAG),\na simple yet effective learning framework that combines\ndomain adaptation and generalization in a complementary\nmanner to achieve three major goals of unsupervised con-\ntinual domain shift learning: adapting to a current do-\nmain, generalizing to unseen domains, and preventing for-\ngetting of previously seen domains. Our approach is model-\nagnostic, meaning that it is compatible with any existing do-\nmain adaptation and generalization algorithms. We evalu-\nate CoDAG on several benchmark datasets and demonstrate\nthat our model outperforms state-of-the-art models in all\ndatasets and evaluation metrics, highlighting its effective-\nness and robustness in handling unsupervised continual do-\nmain shift learning.\n1. Introduction\nMachine learning algorithms have found extensive appli-\ncations in various fields such as image recognition [19, 23],\nnatural language processing [3, 9], and autonomous driv-\ning [5, 10]. Typically, these algorithms learn from a train-\ning dataset to build a model that performs a target task on\nnew data. The assumption underlying these algorithms is\nthat the training data and the test data are identically and\nindependently distributed (IID) [18], drawn from the same\ndistribution that is characterized by an environment or do-\n*Corresponding Author\nModel Initialization\n(Improve Adaptation)\nPseudo Label Generation\n(Improve Generalization)\nDomain\nGeneralization\nDomain\nAdaptation\n…\n…\nTarget\nDomain\nTarget\nDomain\nTarget\nDomain\nContinual Domain Shift\nwith Unlabeled Data\nCoDAG\nFigure 1. Our proposed Complementary Domain Adaptation and\nGeneralization (CoDAG) framework for unsupervised continual\ndomain shift learning.\nmain. However, this IID assumption is often not valid in\nreal-world scenarios, as the environment in which the model\nis applied is more likely to change over time than remain\nfixed. This implies that the model will encounter new data\nfrom various domains over time, and its performance may\ndecline if the data is from a domain that differs significantly\nfrom the one it was trained on.\nTo address this issue, two main approaches have been\ndeveloped: domain generalization and domain adaptation.\nDomain generalization [60] is a method to enhance a\nmodel’s ability to generalize to unseen domains by training\nthe model on labeled data from one or more domains, with-\nout assuming any prior knowledge of the test environment\nin which the model will be applied. However, collecting\na large volume of labeled data from various domains for a\nparticular task can be challenging in practice. Single-source\ndomain generalization techniques [46] have been developed\nto address this issue, which rely solely on data from a single\ndomain. Although more practical, these techniques gener-\nally have lower generalization abilities compared to multi-\nsource domain generalization techniques.\narXiv:2303.15833v2  [cs.LG]  13 Oct 2023\nOn the other hand, domain adaptation [61] aims to en-\nhance model performance only on the current target domain\nand does not prioritize performance on all other domains.\nIn particular, unsupervised domain adaptation (UDA) [44]\ntechniques leverage unlabeled data to adapt the model to\na new target domain. However, domain adaptation meth-\nods fundamentally suffer from performance degradation on\na new target domain before and during the adaptation pro-\ncess due to the lack of inherent mechanism to prepare for\nunseen domains.\nIn this paper, we tackle a challenging problem that simu-\nlates real-world scenarios where models face continual do-\nmain shifts and no labeled data is available for new do-\nmains. We refer to this problem as unsupervised continual\ndomain shift learning. In this setting, the model must con-\ntinually adapt to new domains (domain adaptation), while\nmaintaining its generalization ability for upcoming and un-\nseen domains (domain generalization), in an unsupervised\nmanner.\nHowever, achieving both objectives simultane-\nously is not always feasible since they involve related but\ndistinct goals. For instance, if the current target domain is\nvastly dissimilar from any other domains, none of the opti-\nmal solutions for adapting to the current target domain with\nDA would necessarily result in optimal generalization for\nperforming well on other domains. Similarly, achieving op-\ntimal generalization for unseen domains through DG may\nnot result in the best solution for the current target domain.\nTherefore, to address unsupervised continual domain shift\nlearning, it is necessary to find a solution that resolves this\ntrade-off between domain adaptation and generalization.\nTo address the trade-off between domain adaptation and\ngeneralization, we propose Complementary Domain Adap-\ntation and Generalization (CoDAG), a learning framework\nthat combines domain adaptation and domain generaliza-\ntion in a complementary manner. As shown in Fig. 1, our\napproach involves training two separate models: one for do-\nmain adaptation and the other for domain generalization.\nWe use the domain adaptation model to adapt to the tar-\nget domain, generating more accurate and reliable pseudo-\nlabels for training the domain generalization model. In turn,\nthe domain generalization model learns more generalized\nrepresentations across multiple domains and provides the\ndomain adaptation model with initializing parameters, en-\nhancing its adaptability to a new domain. As a result, the\ndomain adaptation and generalization models complement\neach other in our framework, leading to improved perfor-\nmance for both.\nThe main contribution of our framework, CoDAG, lies\nin the complementary manner in which we leverage exist-\ning domain adaptation and domain generalization meth-\nods to address unsupervised continual domain shift learn-\ning, a unique and challenging problem that has not been\nthoroughly explored. We deliberately apply existing meth-\nods to our framework, rather than introducing new ones, to\nunderscore that the effectiveness of our framework is due\nto its complementary structure, not its individual compo-\nnents. Indeed, without requiring any models tailored for the\npresent problem, our framework proved its merit by achiev-\ning SoTA performance against all baselines, including the\none which is explicitly designed for this setting [37].\nFinally, it is important to note that that our work is one of\nthe first attempts to explore the potential synergies between\ndomain adaptation and domain generalization methods. We\nare breaking new ground by bridging the divide between\nthe disparate fields of domain adaptation and generalization,\nwhich were primarily studied independently. This paradigm\nshift represents not just a novel approach, but one with pro-\nfound practical implications.\nOur contributions can be summarized as follows:\n• We introduce a novel framework that combines do-\nmain adaptation and generalization models in a com-\nplementary manner, resulting in a synergistic process\nthat enhances overall performance.\n• Our method consistently outperforms state-of-the-art\nmodels across all datasets and metrics, demonstrating\nsuperior robustness with the lowest standard deviation\nacross different orders in almost all cases.\n• Our approach does not necessitate the use of models\ndesigned for the present problem, allowing seamless\nintegration with existing domain adaptation and gener-\nalization algorithms for broader applications.\n2. Related Work\nDomain generalization\nDomain generalization (DG) is\nthe process of training a model using labeled data from\none or multiple domains, with the objective of achieving\ngood generalization performance across unseen domains.\nExisting DG methods are based on domain-invariant learn-\ning [13, 16, 15, 29, 32, 40], meta-learning [2, 11, 26, 28],\nand data augmentation [52, 67]. To address practical sce-\nnarios, single-source DG methods [46, 50, 58, 65, 66] have\nbeen proposed, which use labeled data collected from a\nsingle domain. However, these techniques require a large\namount of labeled data and can suffer from severe catas-\ntrophic forgetting when applied to scenarios with continual\ndomain shift.\nUnsupervised domain adaptation\nUnsupervised do-\nmain adaptation (UDA) [44] aims to improve the perfor-\nmance of a target model in scenarios where there is a do-\nmain shift between the labeled source domain and the un-\nlabeled target domain. UDA methods often achieve distri-\nbution alignment through domain invariant feature transfor-\nmation [34, 39, 43] or feature space alignment [12, 17, 55].\nDG\nReplay \nBuffer\nDG\nReplay \nBuffer\nDA\ncopy & distill\npseudo\nlabel\ninit\ncopy & update\nDG\nReplay \nBuffer\nDA\ncopy & distill\npseudo\nlabel\ninit\ncopy & update\nDG\nReplay \nBuffer\nDA\ncopy & distill\npseudo\nlabel\ninit\ncopy & update\n…\nTarget Domain\nSource Domain\nTarget Domain\nTarget Domain\nFigure 2. An implementation of CoDAG for unsupervised continual domain shift learning.\nTypical domain adaptation techniques generally assume\nhaving access to the source data throughout the adaptation\nprocess, which is impractical in real-world scenarios. How-\never, source-free domain adaptation (SFDA) methods [35,\n36] adapt a model to the unlabeled target domain even when\nthe source dataset is not available during the target adap-\ntation process. Recent works on SFDA [1, 7, 31, 38, 63]\nhave arisen, which use generative models to model the\ndistribution of target data by creating pseudo-label refine-\nment [1, 7], target-style images [31, 38], or variational in-\nference for generating latent source features [63]. Never-\ntheless, these techniques are not intended to accumulate the\nknowledge acquired from continually shifting domains.\nContinual\ndomain\nadaptation\nContinual\nlearning\n(CL) [8] focuses on avoiding catastrophic forgetting\nwhen learning new tasks by using regularization-based\nmethods [53, 64] and replay-based methods [47]. Recent\nworks [4, 51, 56] have adopted the ideas from continual\nlearning to tackle the continual domain adaptation (CDA)\nproblem.\nThese methods involve distilling probability\ndistributions at multiple levels from the previous models\nto solve the catastrophic forgetting problem [51], using\nsample replay buffers along with domain adversarial\ntraining [4], and utilizing a domain-specific memory buffer\nfor each domain [56]. Despite the use of CDA methods,\nthe model’s performance on the target domain is often\npoor before and during the adaptation process, although\nit may improve after sufficient adaptation on the new\ntarget domain. This can be particularly problematic when\nthere is a significant domain shift.\nOur complementary\nframework overcomes this issue by leveraging the DG\nmodel’s generalization ability to initialize the DA model.\n3. Methodology\n3.1. Unsupervised continual domain shift learning\nWe adopt the problem setting introduced by [37] to ad-\ndress the challenge of unsupervised continual domain shift\nlearning. Specifically, our approach works with T + 1 dis-\ntinct domains Dt over t = 0, 1, . . . , T for a given target\ntask, which are sequentially encountered. Here, we tackle\nthe K-way image classification problem as the target task,\nand the image space X and the label space Y are shared\nacross all domains. For the sake of notational simplicity,\nhere we use the notation Dt to denote both the t-th domain\nand the dataset sampled from it interchangeably.\nIn this setting, the first domain D0 is regarded as the\nsource domain S that is used to initially learn the target\ntask. Different from other domains, it consists of labeled\nsamples S = D0 = {(x(i)\n0 , y(i)\n0 )}N0\ni=1, where x(i)\n0\nand y(i)\n0\ndenote input data and its corresponding label of the i-th\nsample and N0 indicates the total number of samples in the\nsource domain. After a model is initially trained with the\nsource domain, it sequentially encounters a series of tar-\nget domains T = {D1, D2, . . . , DT }. In contrast to the\nsource domain, the target domains do not provide any label\ninformation to the model, as we assume that the model en-\ncounters them after the deployment. Moreover, we consider\nmore realistic settings that samples from previously expe-\nrienced domains are not fully accessible but are partially\navailable via a replay buffer with a limited capacity. There-\nfore, the model has to properly improve its generalization\nability in an unsupervised manner by mainly using unla-\nbeled samples Dt = {x(i)\nt }Nt\ni=1 from the target domain Dt\nat each stage t.\nTo deal with this problem setting, a model f(x; θ) is\nused to map an input x ∈X to its corresponding label\ny ∈Y, where θ represents the set of model parameters to\nbe learned. The model is expected to be initially trained\nwith samples from the source domain S and then adapted to\nthe target domains T in a continual manner (i.e., from D1\nto DT ). We denote the model parameters after training on\nthe t-th domain as θ∗\nt and evaluate the corresponding model\nf(x; θ∗\nt ) with x from different domains. There exist three\nmain goals for this problem setting. (i) Firstly, we seek to\nachieve domain adaptation for the target domain Dt ∈T .\n(ii) Secondly, we aim to achieve domain generalization for\nunseen target domains {Dt′}t<t′≤T . (iii) Finally, we aim to\nprevent catastrophic forgetting of knowledge gained from\npreviously seen domains {Dt′}0≤t′<t.\nMost of the existing models for domain adaptation or\ngeneralization, which are able to handle unlabeled data, can\nbe applied for unsupervised continual domain shift learn-\ning. A naive approach is to start with the model f(x; θ∗\nt−1)\ntrained on domain Dt−1 and update it with samples from\nDt, resulting in a new model f(x; θ∗\nt ) that is either adapted\nto Dt or generalized for unseen domains Dt′ for t′ > t, de-\npending on its designed purpose. All the baselines used in\nour experiments reflect this naive approach.\nHowever, achieving both DA and DG objectives in un-\nsupervised continual domain shifts is challenging with a\nnaive single-model approach, given the potential contradic-\ntion between adapting to a target domain (DA) and learning\ndomain-invariant features (DG). For instance, when the cur-\nrent target domain is significantly different from any other\ndomains, optimal solutions for domain adaptation may not\nlead to optimal generalization for other domains. Similarly,\nusing domain generalization to attain optimal generaliza-\ntion for unseen domains does not necessarily yield the best\nsolution for the current target domain.\n3.2. CoDAG: Complementary Domain Adaptation\nand Generalization\nTo resolve this trade-off between DA and DG, we pro-\npose a novel framework called Complementary Domain\nAdaptation and Generalization (CoDAG). CoDAG is highly\neffective yet remarkably simple. Unlike other approaches\nthat require complicated architectures or learning methods,\nour CoDAG framework relies on a straightforward and intu-\nitive idea: maintaining two separate models for DA and DG.\nBy having two distinct models optimized for their respec-\ntive goals, the CoDAG framework eliminates the need for\ncomplicated techniques that attempt to balance DA and DG\nwithin a single model. Instead, our framework facilitates\na synergistic relationship between the two models, where\nthey complement each other, leading to improved perfor-\nmance for both.\nTo carry out this framework, DA and DG are conducted\nin an interleaved order with two separate models for DA\nand DG, and we make each of them to be dependent on\nthe other, as shown in Fig. 2. Specifically, we duplicate the\nmodel f to use one for DA, denoted as fDA, and the other for\nDG, denoted as fDG, with each model trained with DA and\nDG algorithms, respectively. This setting allows the models\nto effectively exchange knowledge and continually improve\nthe performance for both.\nAs domain shifts occur continuously and there is no la-\nbeled information available from target domains, nor ac-\ncess to data from previously encountered domains, we have\nimplemented a source-free unsupervised domain adapta-\ntion algorithm for our DA model. There are several op-\ntions for this, but we choose a simple pseudo-labeling [25]\nalong with regularization from SHOT [35] due to its sim-\nplicity. For the DG model, we simply apply an empirical\nrisk minimization (ERM) method along with a RandMix\naugmentation technique [37] that generates diversified data,\nwhich boost the DG model’s ability to generalize to unseen\ndomains. However, note that our proposed framework is\nmodel-agnostic, in the sense that it can be applied to any\nDA or DG algorithms that are suitable for the given prob-\nlem setting. The implementation details of auxiliary meth-\nods and algorithms employed in this paper are provided in\nthe Supplementary Material.\nSource domain training\nTo initially learn a given target\ntask, we train the DG model from scratch with the set of\nlabeled samples from the source domain S (= D0). This\nstage can be approached by a single-source domain gener-\nalization method. In that way, we simply minimize the fol-\nlowing cross-entropy loss over the source domain data with\nenhanced data augmentation,\n  \\label { eq:dg} \\m athcal {L}_{\\t ext {DG },0 }(\\mathcal {D}_0) = \\mathbb {E}_{(x,y)\\in \\mathcal {D}_0} \\left [ \\mathcal {L}^{\\text {ce}}(f_{\\text {DG}}(R(x);\\theta _{\\text {DG},0}), y)\\right ], (1)\nwhere θDG,0 is the DG model parameters, Lce is the cross-\nentropy loss for a classification setting, and R represents the\nRandMix augmentation [37].\nGeneralized initialization with DG for DA\nIn general,\nunsupervised source-free domain adaptation approaches in-\nvolve initially training a source model with the source do-\nmain data and further updating the source model with unla-\nbeled data from a new target domain. However, in our pro-\nposed framework, the DA model utilizes the parameters of\nthe previous DG model for its initialization. This allows the\nDA model to leverage the DG model’s generalization abil-\nity to learn domain-invariant features and reduce domain-\nspecific factors. As a result, we achieve efficient adaptation\nto a new target domain, even when there is a large gap be-\ntween previously experienced domains and the new target\ndomain (see Section 4.2 for experimental results).\nTo apply this approach for the current target domain Dt,\nwe first initialize the DA model fDA with the parameters\nof the DG model trained with previously experienced do-\nmains, or θ∗\nDG,t−1, treating it as a source model. Then, we\nfreeze the classifier head in fDA and only update the fea-\nture extractor part of it using information maximization and\nself-supervised pseudo-labeling with data from the current\ntarget domain Dt. Accordingly, the loss to adapt to Dt is\nwritten as,\n  \\label { eq:da\n}\n \\mathcal {L }_{\\tex\nt\n {DA},t}(\\mathcal {D}_t)= \\mathbb {E}_{x\\in \\mathcal {D}_t} \\left [ \\mathcal {L}^{\\text {shot}}(f_\\text {DA}(x;\\theta _{\\text {DA},t})) \\right ], \n(2)\nwhere θDA,t is the parameters of the DA model initialized\nwith the optimal parameters of the DG model trained on the\nprevious domain Dt−1, or θ∗\nDG,t−1, and Lshot is the cross-\nentropy loss with pseudo-labels on target predictions along\nwith regularization from SHOT [35].\nPseudo-label generation with DA for DG\nIn our pro-\nposed framework, we simply use an empirical risk mini-\nmization (ERM) method along with an enhanced data aug-\nmentation method for training the DG model. Although la-\nbeled samples are necessary for this process, none of the\ntarget domains T provide any labels. Thus, to make use of\nunlabeled samples from the current target domain Dt, we\nadopt a pseudo-label generation strategy [25] based on the\nhighest prediction confidence of the DA model adapted to\nDt. This involves applying the DA model to the unlabeled\nsamples from Dt to generate pseudo-labels, which are then\nused as training labels for the DG model on Dt.\nSpecifically, for each unlabeled sample x, we compute\nits pseudo-label ˆyt(x) as follows:\n  \\hat  {y}_{t\n}\n(x) = \\ar gm\nax _{k}\\delta _{k}(f_\\text {DA}(x;\\theta _{\\text {DA},t}^*)), \n(3)\nwhere θ∗\nDA,t is obtained by optimizing LDA,t in Eq. 2 and\nδk(·) is the k-th element of a softmax output.\nBy us-\ning the resulting pseudo-labels as training labels for the\nDG model on Dt, we construct a pseudo-labeled dataset\nˆDt = {(x(i)\nt , ˆyt(x(i)\nt ))}Nt\ni=1. We then update the DG model\nwith ERM in the same way as the source training in Eq. 1\nwith the following loss:\n  \\l\nabel {eq: p l} \\mat h\nca l {L}_{\\text { DG},t}^ {\\t ext {erm}}(\\hat {\\mathcal {D}}_t) = \\mathbb {E}_{(x,y) \\in \\hat {\\mathcal {D}}_t} \\left [\\mathcal {L}^{\\text {ce}}(f_\\text {DG}(R(x);\\theta _{\\text {DG},t}),y)\\right ], \n(4)\nwhere θDG,t is initialized with the optimal parameters of the\nDG model trained on the previous domain Dt−1, or θ∗\nDG,t−1.\nLearning from noisy labels\nAs the DA model is opti-\nmized to adapt to the current domain Dt, we assume that\nit can generate high-quality pseudo-labels. However, some\nof the labels may still contain errors due to the imperfect-\nness of the DA method. Unfortunately, the errors in pseudo-\nlabels (i.e., noisy labels [54]) can negatively affect the per-\nformance of the DG model. To alleviate this problem, we\nuse an algorithm that can properly handle noisy labels to\nprevent the performance degradation of the DG model.\nIn this paper, we adopt an algorithm called Selective\nNegative Learning and Positive Learning (SelNLPL) [22]\nto reduce the risk of overfitting to noisy labels and improve\nperformance of the DG model. We show the effectiveness\nof this approach in Sec. 4.2. Note that our approach offers\ngreater flexibility that is not restricted to SelNLPL but rather\ncan leverage any label noise-resilient methods [33, 48, 57].\nForgetting alleviation\nForgetting alleviation is crucial in\nunsupervised continual domain shift learning, where lim-\nited access to data from previous domains makes it chal-\nlenging to maintain performance on previous tasks. When\nencountering a new target domain, the performance of the\nDG model on previous domains tends to degrade, which is\ncommonly referred to as catastrophic forgetting in continual\nlearning.\nTo address this issue, we add a simple distillation loss\nterm [20] Ldistill\nDG,t to the loss of the DG model in Eq. 4 to en-\nsure that the model retains the knowledge gained from pre-\nvious domains {Dt′}0≤t′<t while learning from the current\ntarget domain Dt, given by\n  \\mathc\nal {L}_{ \\ text \n{\nDG},t}^{\\text {di\ns\ntill}}(\\mathcal {D}_t) = \\mathbb {E}_{x\\in \\mathcal {D}_t} \\left [\\mathcal {L}^{\\text {kl}}(q_{t}(x)||p_{t}(x)) \\right ], \n(5)\nwhere Lkl represents the KL divergence loss, and qt(x) =\nδ(fDG(R(x); θ∗\nDG,t−1)) and pt(x) = δ(fDG(R(x); θDG,t))\nare the predicted softmax probabilites from the previous and\ncurrent DG models, respectively.\nAnother method we employ to prevent catastrophic for-\ngetting is a replay buffer Mt of size M ≪Nt [49], which\ncontains selected samples from previously experienced do-\nmains {Dt′}0≤t′<t. We build the replay buffer based on the\niCaRL approach [37, 47]. By using a replay buffer, along\nwith the samples from Dt, additional M selected samples\nfrom D0 ∪{ ˆDt′}1≤t′<t are available for training the DG\nmodel on Dt. This allows the DG model to not only im-\nprove its generalization ability but also prevent catastrophic\nforgetting. Then, our final loss to update the DG model on\nthe current target domain Dt is given by,\n  \\mat hcal  {L}_\n{\\tex t {D G } , t}(\\tild\ne {\\m athcal {D}}_{t}) = \\mathcal {L}_{\\text {DG},t}^{\\text {erm}}(\\tilde {\\mathcal {D}}_{t}) + \\alpha \\cdot \\mathcal {L}_{\\text {DG},t}^{\\text {distill}}(\\tilde {\\mathcal {D}}_{t}), \n(6)\nwhere ˜Dt = ˆDt ∪Mt represents Nt + M samples avail-\nable from the t-th domain with a replay buffer and α is a\nbalancing hyperparameter.\nOur findings suggest that utilizing a replay buffer leads to\na significant improvement in performance, especially when\ndealing with previous domains.\nHowever, even without\na replay buffer, our experiment shows that our model re-\nmains competitive against state-of-the-art models that are\nequipped with replay buffers (see Sec. 4.2 for experimental\nresults).\n4. Experiments\nTo validate the effectiveness of our framework, we com-\npare our proposed framework, CoDAG, against state-of-the-\nart methods on three benchmark datasets: (i) PACS [27],\n(ii) Digits-five [14, 21, 24, 41], and (iii) DomainNet [45].\nFor a fair comparison, we adhere to the experimental setup\nfrom [37] in the following sections.\nDatasets\nPACS consists of 4 distinct domains with 7\nclasses, including Photo (P), Art painting (A), Cartoon\nTable 1. Comparison of the performance on the PACS, Digits-five, and DomainNet datasets for different state-of-art methods in TDA,\nTDG, FA, and All. The results are averaged over 10 different orders from each dataset. The results of the baseline models are referenced\nfrom [37]. The best results are highlighted in bold. Our method outperforms all the baselines across all datasets and evaluation metrics\ntested.\nDataset\nMetric\nComparison Baselines (w/ Replay Buffer)\nOurs\nSHOT+\nSHOT++\nTent\nAdaCon\nEATA\nL2D\nPDEN\nRaTP\nCoDAG\n[36, 37]\n[36]\n[59]\n[6]\n[42]\n[62]\n[30]\n[37]\nPACS\nTDA\n81.9\n84.4\n78.7\n79.9\n80.3\n78.8\n77.8\n84.7\n87.6\n±9.2\n±8.0\n±6.9\n±5.9\n±7.1\n±5.6\n±5.2\n±5.1\n±4.0\nTDG\n54.9\n56.0\n65.8\n65.2\n64.1\n65.8\n64.4\n70.6\n72.2\n±13.1\n±10.9\n±11.5\n±10.5\n±12.1\n±9.6\n±9.8\n±9.1\n±8.3\nFA\n74.9\n83.0\n81.0\n81.6\n82.6\n77.6\n76.3\n83.9\n88.8\n±8.1\n±4.0\n±6.2\n±5.9\n±7.0\n±4.6\n±4.0\n±4.7\n±3.0\nAll\n70.6\n74.5\n75.2\n75.6\n75.7\n74.1\n72.9\n79.7\n82.9\n±9.2\n±5.7\n±7.8\n±7.1\n±8.6\n±6.2\n±5.9\n±5.7\n±4.8\nDigits-five\nTDA\n78.6\n81.3\n68.7\n71.6\n72.0\n84.3\n82.3\n88.7\n92.7\n±13.2\n±14.0\n±11.0\n±9.2\n±9.8\n±5.4\n±5.8\n±1.8\n±1.7\nTDG\n61.0\n62.3\n64.0\n63.3\n64.0\n70.9\n69.7\n76.8\n77.4\n±14.9\n±13.8\n±13.6\n±13.1\n±12.9\n±6.8\n±7.0\n±3.9\n±4.3\nFA\n58.2\n64.5\n66.1\n72.2\n73.0\n76.5\n74.0\n85.0\n87.1\n±14.9\n±13.3\n±15.7\n±11.2\n±10.9\n±3.8\n±4.0\n±2.2\n±2.1\nAll\n65.9\n69.4\n66.2\n69.1\n69.6\n77.2\n75.3\n83.5\n85.7\n±13.5\n±12.9\n±13.3\n±11.0\n±10.9\n±4.8\n±5.1\n±2.1\n±2.2\nDomainNet\nTDA\n66.0\n66.9\n53.6\n62.2\n62.5\n56.2\n55.6\n65.4\n71.0\n±8.8\n±8.7\n±13.2\n±7.7\n±7.3\n±6.2\n±6.6\n±5.1\n±5.7\nTDG\n47.3\n48.1\n47.7\n51.3\n52.1\n50.7\n49.3\n55.2\n56.2\n±11.0\n±10.7\n±11.0\n±10.0\n±9.9\n±9.1\n±9.1\n±7.4\n±7.2\nFA\n58.5\n66.9\n56.1\n61.8\n62.8\n52.2\n50.2\n63.5\n70.9\n±8.3\n±6.0\n±14.5\n±9.0\n±8.8\n±9.4\n±9.5\n±6.6\n±6.6\nAll\n57.3\n60.6\n52.5\n58.4\n59.1\n53.0\n51.7\n61.4\n66.0\n±8.9\n±8.0\n±12.4\n±8.6\n±8.3\n±7.6\n±7.8\n±6.0\n±6.2\n(C), and Sketch (S). Digits-five contains 5 different do-\nmains with 10 classes, 0 to 9, including MNIST (MT) [24],\nMNIST-M (MM) [14], SVHN (SN) [41], SYN-D (SD) [14]\nand USPS (US) [21].\nDomainNet is the most challeng-\ning dataset, which includes Quickdraw (Qu), Clipart (Cl),\nPainting (Pa), Infograph (In), Sketch (Sk) and Real (Re).\nDomainNet has an imbalance in class distribution, where\nsome domains have limited images for certain classes. To\naddress this issue, a subset of DomainNet is used by se-\nlecting the top 10 classes with most images in the whole\ndataset.\nExperimental settings\nIn the source domain, 80% of the\ndata is randomly assigned as a training set, and the remain-\ning 20% as a testing set. In target domains, all data is used\nfor training and testing as we assume any label information\nis not available. The experiments are repeated three times\nusing different seeds (2022, 2023, 2024), and the average\nperformance is reported. For Digits-five, we use DTN [35]\nas a feature extractor, while for both PACS and DomainNet,\nwe employ ResNet-50 [19]. The SGD optimizer is used\nwith a batch size of 64 for all experiments. The size of re-\nplay buffer is set to 200 for all datasets. See the Supplemen-\ntary Material for more details on the experimental settings\nand network architectures.\nEvaluation metrics\n(i) TDA: Target domain adaptation\nfor each domain is the performance measured on the domain\nright after its training stage. The TDA of the t-th domain for\nt = 0, . . . , T is\n  \\t e xt {TD A}\n_ t = \\mathcal {A}(f(x;\\theta _{t}^*),\\mathcal {D}_t), \n(7)\nwhere A(f(x; θ∗\nt′), Dt) represents the test accuracy on the\ndomain Dt with the model f(x; θ∗\nt′) obtained after training\non the domain Dt′. (ii) TDG: Target domain generalization\nfor each domain is evaluated by the average performance on\nthe domain prior to its training stage. The TDG of the t-th\n0\n60\n120\n180\n240\nEpoch\n60\n70\n80\n90\n100\nAccuracy (%)\nSource domain \n0\n Target domain \n1\n Target domain \n2\n Target domain \n3\nFigure 3. The training curves of the DG model which depict the\nmodel’s performance on their respective domains at every training\nepoch. The domain shift occurs every 60 epochs, starting from\nthe source domain and continuing until the third target domain.\nThe results are averaged over 10 different orders from the PACS\ndataset.\ndomain for t = 1, . . . , T is given by\n  \\t e x\nt\n {T\nD\nG}_t\n = \\fr ac\n {1} {t}\\sum _{t'=0}^{t-1}\\mathcal {A}(f(x;\\theta _{t'}^*),\\mathcal {D}_t). \n(8)\n(iii) FA: Forgetting alleviation for each domain is evaluated\nby the average performance on the domain after the model\nhas been trained on subsequent domains. The FA of the t-th\ndomain for t = 0, . . . , T −1 can be written as\n  \\ t\ne\nx t  \n{\nF\nA}_t =\n \\frac  {\n1}{T -t}\\sum _{t'=t+1}^{T}\\mathcal {A}(f(x;\\theta _{t'}^*),\\mathcal {D}_t). \n(9)\nFor comparative analysis between different models, we use\nthe averaged value of each metric over all domains for\nwhich the metric is defined. Additionally, we define the\naverage of all the metrics as a composite score (All) to eval-\nuate the overall performance of the models. To properly\nevaluate our proposed framework, CoDAG, we use the DA\nmodel fDA to evaluate TDA, while the DG model fDG is\nused to evaluate TDG and FA.\n4.1. Effectiveness of the CoDAG framework\nComparing with state-of-the-art\nTo compare the perfor-\nmance of our model with state-of-the-art methods, we use\nthe experiment results of different methods reported in [37]\nas baselines. These methods include several state-of-the-\nart models from Source-Free DA (SHOT+ [36, 37] and\nSHOT++ [36]), Test-Time/Online DA (Tent [59], AdaCon\n[6], and EATA [42]), Single DG (L2D [62] and PDEN [30]),\nas well as Continual DG (RaTP [37]). To ensure consis-\nTable 2. Evaluation of two distinct initialization methods for the\nDA model with performance averaged across 10 different orders\nfrom the PACS dataset.\nMethod\nTDA\nTDG\nFA\nAll\nInitialization w/\nthe DG model\n87.6\n72.2\n88.8\n82.9\n±4.0\n±8.3\n±3.0\n±4.8\nInitialization w/\nthe DA model\n83.8\n71.7\n86.6\n80.7\n±4.0\n±8.4\n±4.0\n±4.8\nDiff.\n+3.8\n+0.5\n+2.2\n+2.2\ntency and fairness, we adopt the same backbone feature ex-\ntractor with the baseline methods. In the present experi-\nments, all baseline methods are equipped with the replay\nbuffer of size 200.\nWe present an evaluation of our CoDAG framework\nagainst the baseline models on the three datasets (PACS,\nDigits-five, and DomainNet) using the four evaluation met-\nrics (TDA, TDG, FA, and All). The results are averaged\nover 10 different orders from each dataset to ensure the ro-\nbustness of our findings. The overall results presented in Ta-\nble 1 demonstrate that our method consistently outperforms\nall the baseline models across all datasets and evaluation\nmetrics.\nCompared to RaTP [37], a continual domain generaliza-\ntion method that achieves the best performance among the\nbaselines in most cases, our method demonstrates signifi-\ncantly higher performance in TDA and FA. This highlights\nthe effectiveness of our domain adaptation stage based on\nthe generalized initialization using the DG model.\nThe\nmore accurate pseudo-labels generated by the DA model\nalso contribute to the DG model’s superior performance in\nFA.\nOn the DomainNet dataset, which is the most chal-\nlenging among our benchmark datasets, the DA-specialized\nmodel SHOT++ [36] outperforms RaTP in terms of TDA\nand FA. However, our CoDAG consistently outperforms\nany DA-specialized models in terms of TDA and FA, while\nmaintaining the improved generalization performance in\nterms of TDG.\nFurthermore, Table 1 shows that our method acheives\nthe lowest standard deviation across ten different orders\nin nearly all cases.\nNotably, even our lower bound per-\nformance (µ −σ) surpasses the upper bound performance\n(µ + σ) of other baselines in many cases. These findings\ndemonstrate the robustness of our CoDAG framework in ad-\ndressing the challenges of unsupervised continual domain\nshift learning.\nTraining curves\nIn Fig. 3, we display the training curves\nof the DG model, each of which represents the accuracy of\nthe model on its corresponding domain at different training\nTable 3. The ablation study of SelNLPL conducted for all possible pairs of two domains (D0 →D1) from the PACS dataset. TDG was\nmeasured by the average performance on two unseen domains (D2 and D3), while FA was measured by the performance on the source\ndomain (D0). Diff. denotes the result obtained by subtracting the performance without (w/o) SelNLPL from the performance with (w/)\nSelNLPL.\nMetric\nMethod\nP→A\nP→C\nP→S\nA→P\nA→C\nA→S\nC→P\nC→A\nC→S\nS→P\nS→A\nS→C\nAvg.\nTDG\nw/ SelNLPL\n58.3\n74.6\n58.4\n59.8\n85.3\n77.7\n79.8\n86.8\n78.1\n69.9\n84.3\n85.3\n74.9\nw/o SelNLPL\n57.4\n74.2\n54.3\n58.0\n84.9\n71.2\n78.6\n86.6\n74.2\n68.3\n83.4\n85.0\n73.0\nDiff.\n+0.9\n+0.4\n+4.1\n+1.8\n+0.4\n+6.5\n+1.2\n+0.2\n+3.9\n+1.6\n+0.9\n+0.3\n+1.8\nFA\nw/ SelNLPL\n98.5\n98.2\n95.7\n94.1\n93.2\n83.0\n90.4\n92.6\n85.8\n85.7\n91.2\n89.7\n91.5\nw/o SelNLPL\n98.5\n97.2\n93.0\n93.2\n92.6\n76.7\n89.4\n91.3\n84.7\n85.1\n89.3\n88.4\n89.9\nDiff.\n0.0\n+1.0\n+2.7\n+0.9\n+0.6\n+6.3\n+1.0\n+1.3\n+1.1\n+0.6\n+1.9\n+1.3\n+1.6\nstages. We observe that the model’s performance on unseen\ndomains gradually increases as training progresses, which\nillustrates the model’s continually improving generalization\nability. Moreover, Fig. 3 suggests that the model is capa-\nble of avoiding catastrophic forgetting, as its performance\non previously encountered domains remains relatively sta-\nble even after domain shifts.\n4.2. Further analysis\nIn this section, we perform additional analyses to inves-\ntigate the key components of our method using the PACS\ndataset under different experimental settings.\nEffectiveness of generalized initialization for DA\nTo\nevaluate the effectiveness of the generalized initialization\napproach for DA, we compare model performance between\ntwo different approaches for initializing θDA,t of the DA\nmodel on Dt: (1) initializing with the previous DG model\nusing θ∗\nDG,t−1 and (2) initializing with the previous DA\nmodel using θ∗\nDA,t−1. The results presented in Table 2 show\nthat initializing with the DG model significantly improves\nthe performance in TDA. This exhibits the effectiveness of\nthe generalized initialization, which enables the DA model\nto adapt more efficiently to a new domain, compared to re-\nlying on parameters specifically adapted to the previous do-\nmain. Furthermore, we observe that the improved TDA of\nthe DA model has a positive impact on the performance of\nthe DG model in FA and TDG by providing more accurate\npseudo-labels.\nAblation study of SelNLPL\nTo understand SelNLPL’s\ncontribution, we conduct experiments with and without\nSelNLPL in the training of our model for pairs of two dif-\nferent domains (D0 →D1). We then compare the resulting\nchanges in TDG and FA. To isolate the effect of SelNLPL,\nwe remove the replay buffer. The experiment results in Ta-\nble 3 demonstrate that SelNLPL can improve model perfor-\nmance in both TDG and FA, which underscores the effec-\ntiveness of noise-resilient methods to amplify the comple-\nTDA\nTDG\nFA\nAll\nMetric\n60\n65\n70\n75\n80\n85\n90\nPerformance (%)\nCoDAG (w/ Replay Buffer)\nCoDAG (w/o Replay Buffer)\nRaTP (w/ Replay Buffer)\nFigure 4. The ablation study of replay buffer with averaged perfor-\nmance across 10 different orders from the PACS dataset.\nmentary effect of the DA model on the DG model within\nour framework.\nAblation study of replay buffer\nWe perform ablation\nstudies to assess the effectiveness of utilizing a replay buffer\nin CoDAG, by comparing the performance of the model\nwith and without the buffer. The results presented in Fig. 4\nclearly display that the absence of the buffer leads to a\ndegradation in performance across all metrics, with the met-\nric for FA being the most adversely affected. This indicates\nthat the replay buffer plays a crucial role in preventing catas-\ntrophic forgetting.\nFurthermore, our findings suggest that the replay buffer\nenhances the model’s ability to generalize by continually\nexposing it to multiple domains, thereby boosting its per-\nformance in TDG. This, in turn, leads to an improvement\nin TDA as the model can adapt to a new domain with more\ngeneralized initialization.\nHowever, despite the decrease in performance after the\nremoval of the buffer, our method without replay buffer still\noutperforms all other baselines with replay buffer, further\nconfirming the effectiveness of our framework.\n5. Conclusion\nIn this paper, we propose a learning framework that\ncombines domain adaptation and generalization models in\na complementary manner, which effectively addresses the\nchallenge of unsupervised continual domain shift learn-\ning. Our method outperforms state-of-the-art performance\nacross different datasets and evaluation metrics.\nIt also\nachieves competitive results even without a replay buffer,\ndemonstrating its effectiveness and robustness in real-world\nscenarios.\nOur approach is model-agnostic, meaning it can be used\nwith any domain adaptation and generalization algorithms\nsuitable for a given problem. We envision extending our\nmethod to complex scenarios beyond a pre-defined set of\ndomains, dynamically discovering domain shifts and adapt-\ning to new domains. This can increase its applicability to a\nbroader range of scenarios, with potential contributions to\npractical applications in computer vision and other fields.\nAcknowledgement\nThis work was supported by the Hyundai Motor Chung\nMong-Koo Foundation, the New Faculty Startup Fund from\nSeoul National University, and IITP (RS-2023-00232046).\nReferences\n[1] Waqar Ahmed, Pietro Morerio, and Vittorio Murino. Adap-\ntive pseudo-label refinement by negative ensemble learn-\ning for source-free unsupervised domain adaptation. arXiv\npreprint arXiv:2103.15973, 2021.\n[2] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel-\nlappa. Metareg: Towards domain generalization using meta-\nregularization. Advances in neural information processing\nsystems, 31, 2018.\n[3] Yoshua Bengio, R´ejean Ducharme, and Pascal Vincent. A\nneural probabilistic language model. Advances in neural in-\nformation processing systems, 13, 2000.\n[4] Andreea Bobu, Eric Tzeng, Judy Hoffman, and Trevor Dar-\nrell. Adapting to continuously shifting domains. 2018.\n[5] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski,\nBernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D\nJackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al.\nEnd to end learning for self-driving cars.\narXiv preprint\narXiv:1604.07316, 2016.\n[6] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna\nEbrahimi. Contrastive test-time adaptation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 295–305, 2022.\n[7] Weijie Chen, Luojun Lin, Shicai Yang, Di Xie, Shiliang Pu,\nand Yueting Zhuang. Self-supervised noisy label learning\nfor source-free unsupervised domain adaptation.\nIn 2022\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), pages 10185–10192. IEEE, 2022.\n[8] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah\nParisot, Xu Jia, Aleˇs Leonardis, Gregory Slabaugh, and\nTinne Tuytelaars. A continual learning survey: Defying for-\ngetting in classification tasks. IEEE transactions on pattern\nanalysis and machine intelligence, 44(7):3366–3385, 2021.\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018.\n[10] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Anto-\nnio Lopez, and Vladlen Koltun. Carla: An open urban driv-\ning simulator. In Conference on robot learning, pages 1–16.\nPMLR, 2017.\n[11] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas,\nand Ben Glocker. Domain generalization via model-agnostic\nlearning of semantic features. Advances in Neural Informa-\ntion Processing Systems, 32, 2019.\n[12] Basura Fernando, Amaury Habrard, Marc Sebban, and Tinne\nTuytelaars.\nUnsupervised visual domain adaptation using\nsubspace alignment. In Proceedings of the IEEE interna-\ntional conference on computer vision, pages 2960–2967,\n2013.\n[13] Chuang Gan, Tianbao Yang, and Boqing Gong. Learning at-\ntributes equals multi-source domain generalization. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 87–97, 2016.\n[14] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain\nadaptation by backpropagation. In International conference\non machine learning, pages 1180–1189. PMLR, 2015.\n[15] Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn,\nand Mengjie Zhang.\nScatter component analysis: A uni-\nfied framework for domain adaptation and domain general-\nization. IEEE transactions on pattern analysis and machine\nintelligence, 39(7):1414–1430, 2016.\n[16] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang,\nand David Balduzzi. Domain generalization for object recog-\nnition with multi-task autoencoders. In Proceedings of the\nIEEE international conference on computer vision, pages\n2551–2559, 2015.\n[17] Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Un-\nsupervised adaptation across domain shifts by generating in-\ntermediate data representations. IEEE transactions on pat-\ntern analysis and machine intelligence, 36(11):2288–2302,\n2013.\n[18] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and\nJerome H Friedman. The elements of statistical learning:\ndata mining, inference, and prediction, volume 2. Springer,\n2009.\n[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In 2016 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 770–\n778, Los Alamitos, CA, USA, jun 2016. IEEE Computer So-\nciety.\n[20] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistill-\ning the knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2015.\n[21] Jonathan J. Hull. A database for handwritten text recogni-\ntion research. IEEE Transactions on pattern analysis and\nmachine intelligence, 16(5):550–554, 1994.\n[22] Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim.\nNlnl: Negative learning for noisy labels. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 101–110, 2019.\n[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. Communications of the ACM, 60(6):84–90, 2017.\n[24] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[25] Dong-Hyun Lee et al. Pseudo-label: The simple and effi-\ncient semi-supervised learning method for deep neural net-\nworks. In Workshop on challenges in representation learn-\ning, ICML, volume 3, page 896, 2013.\n[26] Da\nLi,\nYongxin\nYang,\nYi-Zhe\nSong,\nand\nTimothy\nHospedales. Learning to generalize: Meta-learning for do-\nmain generalization. In Proceedings of the AAAI conference\non artificial intelligence, volume 32, 2018.\n[27] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M\nHospedales. Deeper, broader and artier domain generaliza-\ntion. In Proceedings of the IEEE international conference on\ncomputer vision, pages 5542–5550, 2017.\n[28] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe\nSong, and Timothy M Hospedales. Episodic training for do-\nmain generalization. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1446–1455,\n2019.\n[29] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot.\nDomain generalization with adversarial feature learning. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5400–5409, 2018.\n[30] Lei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xi-\naoyue Mi, Zhengze Yu, Xiaoya Li, and Boyang Xia. Pro-\ngressive domain expansion network for single domain gen-\neralization. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 224–233,\n2021.\n[31] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and\nSi Wu. Model adaptation: Unsupervised domain adaptation\nwithout source data. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n9641–9650, 2020.\n[32] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang\nLiu, Kun Zhang, and Dacheng Tao.\nDeep domain gener-\nalization via conditional invariant adversarial networks. In\nProceedings of the European conference on computer vision\n(ECCV), pages 624–639, 2018.\n[33] Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao,\nJiebo Luo, and Li-Jia Li. Learning from noisy labels with\ndistillation. In Proceedings of the IEEE international con-\nference on computer vision, pages 1910–1918, 2017.\n[34] Jian Liang, Ran He, Zhenan Sun, and Tieniu Tan. Aggregat-\ning randomized clustering-promoting invariant projections\nfor domain adaptation. IEEE transactions on pattern analy-\nsis and machine intelligence, 41(5):1027–1042, 2018.\n[35] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need\nto access the source data? source hypothesis transfer for un-\nsupervised domain adaptation. In International Conference\non Machine Learning, pages 6028–6039. PMLR, 2020.\n[36] Jian Liang, Dapeng Hu, Yunbo Wang, Ran He, and Jiashi\nFeng. Source data-absent unsupervised domain adaptation\nthrough hypothesis transfer and labeling transfer.\nIEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n44(11):8602–8617, 2021.\n[37] Chenxi Liu, Lixu Wang, Lingjuan Lyu, Chen Sun, Xiao\nWang, and Qi Zhu. Deja vu: Continual model generalization\nfor unseen domains. In The Eleventh International Confer-\nence on Learning Representations, 2023.\n[38] Yuang Liu, Wei Zhang, and Jun Wang. Source-free domain\nadaptation for semantic segmentation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1215–1224, 2021.\n[39] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang\nSun, and Philip S Yu. Transfer feature learning with joint\ndistribution adaptation. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 2200–2207,\n2013.\n[40] Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gi-\nanfranco Doretto. Unified deep supervised domain adapta-\ntion and generalization. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 5715–5725,\n2017.\n[41] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-\nsacco, Bo Wu, and Andrew Y Ng. Reading digits in natural\nimages with unsupervised feature learning. 2011.\n[42] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen,\nShijian Zheng, Peilin Zhao, and Mingkui Tan.\nEfficient\ntest-time model adaptation without forgetting. In Interna-\ntional conference on machine learning, pages 16888–16905.\nPMLR, 2022.\n[43] Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang\nYang.\nDomain adaptation via transfer component analy-\nsis. IEEE transactions on neural networks, 22(2):199–210,\n2010.\n[44] Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama\nChellappa. Visual domain adaptation: A survey of recent\nadvances. IEEE signal processing magazine, 32(3):53–69,\n2015.\n[45] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\nSaenko, and Bo Wang. Moment matching for multi-source\ndomain adaptation. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 1406–1415,\n2019.\n[46] Fengchun Qiao, Long Zhao, and Xi Peng.\nLearning to\nlearn single domain generalization. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12556–12565, 2020.\n[47] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nSperl, and Christoph H Lampert. icarl: Incremental classifier\nand representation learning. In Proceedings of the IEEE con-\nference on Computer Vision and Pattern Recognition, pages\n2001–2010, 2017.\n[48] Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat,\nand Mubarak Shah.\nIn defense of pseudo-labeling: An\nuncertainty-aware pseudo-label selection framework for\nsemi-supervised learning. arXiv preprint arXiv:2101.06329,\n2021.\n[49] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lil-\nlicrap, and Gregory Wayne. Experience replay for continual\nlearning. Advances in Neural Information Processing Sys-\ntems, 32, 2019.\n[50] Eduardo Romera, Luis M Bergasa, Jose M Alvarez, and Mo-\nhan Trivedi. Train here, deploy there: Robust segmentation\nin unseen domains. In 2018 IEEE Intelligent Vehicles Sym-\nposium (IV), pages 1828–1833. IEEE, 2018.\n[51] Antoine Saporta, Arthur Douillard, Tuan-Hung Vu, Patrick\nP´erez, and Matthieu Cord. Multi-head distillation for contin-\nual unsupervised domain adaptation in semantic segmenta-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 3751–3760,\n2022.\n[52] Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Sid-\ndhartha Chaudhuri, Preethi Jyothi, and Sunita Sarawagi.\nGeneralizing across domains via cross-gradient training.\narXiv preprint arXiv:1804.10745, 2018.\n[53] Daniel L Silver and Robert E Mercer. The task rehearsal\nmethod of life-long learning:\nOvercoming impoverished\ndata. In Advances in Artificial Intelligence: 15th Confer-\nence of the Canadian Society for Computational Studies of\nIntelligence, AI 2002 Calgary, Canada, May 27–29, 2002\nProceedings 15, pages 90–101. Springer, 2002.\n[54] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin,\nand Jae-Gil Lee. Learning from noisy labels with deep neural\nnetworks: A survey. IEEE Transactions on Neural Networks\nand Learning Systems, 2022.\n[55] Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frus-\ntratingly easy domain adaptation. In Proceedings of the AAAI\nconference on artificial intelligence, volume 30, 2016.\n[56] Shixiang Tang, Peng Su, Dapeng Chen, and Wanli Ouyang.\nGradient regularized contrastive learning for continual do-\nmain adaptation. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 35, pages 2665–2673, 2021.\n[57] Brendan Van Rooyen,\nAditya Menon,\nand Robert C\nWilliamson. Learning with symmetric label noise: The im-\nportance of being unhinged. Advances in neural information\nprocessing systems, 28, 2015.\n[58] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C\nDuchi, Vittorio Murino, and Silvio Savarese. Generalizing\nto unseen domains via adversarial data augmentation. Ad-\nvances in neural information processing systems, 31, 2018.\n[59] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol-\nshausen, and Trevor Darrell. Tent: Fully test-time adaptation\nby entropy minimization. arXiv preprint arXiv:2006.10726,\n2020.\n[60] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang,\nTao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip\nYu. Generalizing to unseen domains: A survey on domain\ngeneralization. IEEE Transactions on Knowledge and Data\nEngineering, 2022.\n[61] Mei Wang and Weihong Deng. Deep visual domain adapta-\ntion: A survey. Neurocomputing, 312:135–153, 2018.\n[62] Zijian Wang, Yadan Luo, Ruihong Qiu, Zi Huang, and\nMahsa Baktashmotlagh. Learning to diversify for single do-\nmain generalization. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 834–843,\n2021.\n[63] Hao-Wei Yeh, Baoyao Yang, Pong C Yuen, and Tatsuya\nHarada.\nSofa:\nSource-data-free feature alignment for\nunsupervised domain adaptation.\nIn Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 474–483, 2021.\n[64] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-\nual learning through synaptic intelligence. In International\nconference on machine learning, pages 3987–3995. PMLR,\n2017.\n[65] Ling Zhang, Xiaosong Wang, Dong Yang, Thomas Sanford,\nStephanie Harmon, Baris Turkbey, Bradford J Wood, Holger\nRoth, Andriy Myronenko, Daguang Xu, et al. Generalizing\ndeep learning for medical image segmentation to unseen do-\nmains via deep stacked transformation. IEEE transactions\non medical imaging, 39(7):2531–2540, 2020.\n[66] Long Zhao, Ting Liu, Xi Peng, and Dimitris Metaxas.\nMaximum-entropy adversarial data augmentation for im-\nproved generalization and robustness. Advances in Neural\nInformation Processing Systems, 33:14435–14447, 2020.\n[67] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao\nXiang. Deep domain-adversarial image generation for do-\nmain generalisation. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 34, pages 13025–13032,\n2020.\nSupplementary Material:\nComplementary Domain Adaptation and Generalization\nfor Unsupervised Continual Domain Shift Learning\nWonguk Cho1, Jinha Park2, and Taesup Kim1\n1Graduate School of Data Science, Seoul National University\n2Department of Electrical and Computer Engineering, Seoul National University\n{wongukcho, jhpark410, taesup.kim}@snu.ac.kr\nThis Supplementary Material provides additional details\nof the experiments conducted using our CoDAG frame-\nwork, which are not included in the main paper due to space\nconstraints.\n1. The Details of the Experimental Settings\nWe conduct experiments on three datasets: PACS [7],\nDigits-five [2, 4, 6, 10], and DomainNet [11]. We maintain\nconsistent training steps per epoch across all domains and\ndomain orders for every dataset. We use 50 steps for PACS,\n800 steps for Digits-five, and 75 steps for DomainNet. We\nset the number of training epochs per domain to 60 for both\nPACS and DomainNet, and to 75 for Digits-five. For dis-\ntillation loss, we set the balancing parameter α to 0.2 for\nPACS and DomainNet, and 0.5 for Digits-five.\nFor optimization, we use the SGD optimizer with a\nweight decay of 0.0005 and a polynomial learning rate\nscheduler. An initial learning rate is set to 0.01 for both\nPACS and Digits-five, and 0.005 for DomainNet. The batch\nsize for mini-batch training is set to 64.\nFor training of PACS and DomainNet, we use the stan-\ndard augmentation techniques including random cropping,\nhorizontal flipping, color jittering, and grayscaling. For do-\nmain adaptation, we use the Mixup [13] method with the\nmixup hyperparameter set to 2.0.\n2. The Details of the Network Architecture\nOur model network consists of three parts: feature ex-\ntractor, intermediate module, and classifier.\nWe utilize\nResNet-50 [3] as the feature extractor for both PACS and\nDomainNet, while for Digits-five, we use DTN [8]. ResNet-\n50 is initialized with the weights pretrained with Ima-\ngeNet [1].\nThe intermediate module that connects the feature ex-\ntractor and classifier is made up of a fully connected layer, a\nBatch Normalization layer, a ReLU layer, and another fully\nconnected layer. The output dimension of the first fully con-\nnected layer is 512 for both PACS and DomainNet, and 256\nfor Digits-five. The output dimension of the other fully con-\nnected layer is 256 for both PACS and DomainNet, and 128\nfor Digits-five. The classifier consists of a single fully con-\nnected layer with weight normalization.\n3. The Details of the Auxiliary Methods\nIn this section, we explain the implementation details of\nthe auxiliary methods we employed for the experiments of\nour CoDAG framework in this paper.\nSHOT\nWe use simple self-supervised pseudo-labeling,\nalong with information maximization proposed by Liang et\nal. [8]. The balancing parameter β is set to 0.1.\nRandMix\nWe use the Randmix augmentation imple-\nmented by Liu et al. [9]. For the samples from target do-\nmains, Randmix is applied only for the ones with predic-\ntion confidence over 0.5 for PACS and 0.8 for Digits-five\nand DomainNet.\nSelNLPL\nTo train the DG model using SelNLPL [5] for\na given number of training epochs, we divide the epochs\nequally into three parts for NL, SelNL, and SelPL. For\nSelPL, γ is set to 0.5.\nReplay buffer\nWe build the replay buffer based on the\niCaRL approach [9, 12]. The prototypes are created for each\nclass in the current domain to prioritize which data to save\nand remove in the replay buffer based on their proximity to\nthe prototypes.\nTo accommodate the fixed memory size of the replay\nbuffer, we remove some of the stored samples to make room\nfor new ones, while retaining M/(K × t) samples for each\nclass in every previous domain, where M is the maximum\nTable 1. The list of different domain orders from each dataset for the main experiments, which are referenced from [9].\nOrder\nPACS\nDigits-five\nDomainNet\nOrder 1\nA→C→P→S\nSN→MT→MM→SD→US\nRe→Pa→In→Cl→Sk→Qu\nOrder 2\nA→C→S→P\nSN→SD→MT→US→MM\nCl→In→Pa→Qu→Re→Sk\nOrder 3\nA→P→C→S\nMM→US→MT→SD→SN\nCl→Re→In→Qu→Sk→Pa\nOrder 4\nC→A→S→P\nMT→MM→SN→SD→US\nIn→Qu→Cl→Pa→Re→Sk\nOrder 5\nC→S→P→A\nMT→MM→US→SN→SD\nPa→Sk→Qu→In→Re→Cl\nOrder 6\nP→A→C→S\nSD→MM→SN→MT→US\nQu→Re→Cl→Pa→In→Sk\nOrder 7\nP→S→A→C\nSD→SN→US→MM→MT\nQu→Sk→Cl→In→Pa→Re\nOrder 8\nP→S→C→A\nSD→US→MM→SN→MT\nSk→In→Pa→Cl→Re→Qu\nOrder 9\nS→C→A→P\nUS→MT→SN→MM→SD\nSk→Re→Pa→Cl→Qu→In\nOrder 10\nS→P→C→A\nUS→SD→SN→MM→MT\nSk→Re→Qu→Pa→In→Cl\nnumber of samples that can be stored in the replay buffer,\nK is the number of classes, and t represents the number of\npast domains. In our main experiments, M is set to 200 for\nall datasets.\n4. The Details of the Main Experiments\nFor the main experiments, we use the 10 different orders\nfrom each dataset, presented in Table 1, which are randomly\nselected by [9]. The first domain in a given order is used\nas a source domain, and the rest are used target domains.\nFor each order, experiments are repeated three times with\ndifferent seeds (2022, 2023, 2024).\nTable 2, 3 and 4 display the experiment results for 10\nindividual orders from each dataset, respectively. The re-\nsults of the baseline models are referenced from [9]. These\nresults show that our CoDAG outperforms all other compar-\nison baselines in most of the individual orders across differ-\nent datasets and metrics.\nNotably, in terms of the composite score metric All,\nwhich assesses the overall performance of the models by\naveraging TDA, TDG, and FA, our CoDAG outperforms\nall other baseline models in every order, without exception.\nThese results provide further confirmation of the effective-\nness and robustness of our CoDAG framework.\nReferences\n[1] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009.\n[2] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain\nadaptation by backpropagation. In International conference\non machine learning, pages 1180–1189. PMLR, 2015.\n[3] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In 2016 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 770–\n778, Los Alamitos, CA, USA, jun 2016. IEEE Computer So-\nciety.\n[4] Jonathan J. Hull. A database for handwritten text recogni-\ntion research. IEEE Transactions on pattern analysis and\nmachine intelligence, 16(5):550–554, 1994.\n[5] Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim.\nNlnl: Negative learning for noisy labels. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 101–110, 2019.\n[6] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[7] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M\nHospedales. Deeper, broader and artier domain generaliza-\ntion. In Proceedings of the IEEE international conference on\ncomputer vision, pages 5542–5550, 2017.\n[8] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need\nto access the source data? source hypothesis transfer for un-\nsupervised domain adaptation. In International Conference\non Machine Learning, pages 6028–6039. PMLR, 2020.\n[9] Chenxi Liu, Lixu Wang, Lingjuan Lyu, Chen Sun, Xiao\nWang, and Qi Zhu. Deja vu: Continual model generalization\nfor unseen domains. In The Eleventh International Confer-\nence on Learning Representations, 2023.\n[10] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-\nsacco, Bo Wu, and Andrew Y Ng. Reading digits in natural\nimages with unsupervised feature learning. 2011.\n[11] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\nSaenko, and Bo Wang. Moment matching for multi-source\ndomain adaptation. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 1406–1415,\n2019.\n[12] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nSperl, and Christoph H Lampert. icarl: Incremental classifier\nand representation learning. In Proceedings of the IEEE con-\nference on Computer Vision and Pattern Recognition, pages\n2001–2010, 2017.\n[13] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. In International Conference on Learning Representa-\ntions, 2018.\nTable 2. Comparison of the performance on the PACS dataset for different state-of-art methods in TDA, TDG, FA, and All. The results are\npresented for each domain order. The results of the baseline models are referenced from [9]. The best results are highlighted in bold.\nMetric & Orders\nSHOT\nSHOT++\nTent\nAdaCon\nEATA\nL2D\nPDEN\nRaTP\nOurs\nTDA\nOrder 1\n86.7\n89.4\n84.0\n85.8\n86.7\n84.5\n83.7\n85.5\n88.3\nOrder 2\n87.8\n89.4\n82.0\n81.6\n85.3\n83.6\n83.3\n87.5\n87.9\nOrder 3\n88.7\n88.8\n82.5\n82.8\n85.6\n82.9\n79.9\n85.6\n89.0\nOrder 4\n89.2\n91.2\n88.2\n88.7\n89.2\n84.6\n83.0\n87.6\n89.9\nOrder 5\n85.2\n85.4\n88.6\n86.4\n88.2\n80.1\n78.2\n85.6\n89.8\nOrder 6\n83.1\n85.3\n75.7\n78.6\n79.2\n75.5\n74.0\n83.2\n86.0\nOrder 7\n66.9\n69.9\n74.4\n74.0\n73.0\n71.3\n71.6\n75.9\n80.6\nOrder 8\n64.0\n68.8\n72.5\n73.9\n72.3\n68.5\n69.8\n74.9\n80.0\nOrder 9\n91.5\n92.2\n69.6\n77.8\n73.0\n83.0\n82.5\n89.6\n92.6\nOrder 10\n75.9\n83.2\n69.8\n69.7\n70.4\n74.4\n72.1\n91.3\n91.7\nAvg.\n81.9\n84.4\n78.7\n79.9\n80.3\n78.8\n77.8\n84.7\n87.6\nTDG\nOrder 1\n69.4\n70.4\n75.5\n75.2\n75.1\n74.0\n73.7\n76.8\n77.8\nOrder 2\n67.0\n68.7\n73.1\n74.6\n72.5\n76.0\n71.6\n76.7\n77.2\nOrder 3\n67.8\n63.3\n75.6\n75.9\n76.1\n72.8\n73.5\n77.7\n76.2\nOrder 4\n69.5\n66.1\n78.5\n77.1\n77.4\n78.1\n77.2\n79.5\n82.5\nOrder 5\n61.1\n62.2\n81.6\n74.6\n78.3\n74.6\n73.5\n78.5\n81.2\nOrder 6\n48.5\n50.0\n56.2\n57.2\n57.4\n56.5\n55.8\n63.4\n62.1\nOrder 7\n36.6\n43.2\n52.5\n55.4\n54.3\n54.9\n52.0\n56.1\n60.1\nOrder 8\n37.2\n39.0\n50.6\n52.1\n51.8\n52.8\n51.5\n53.8\n58.8\nOrder 9\n53.1\n52.7\n54.3\n57.3\n48.2\n62.0\n60.9\n73.3\n74.6\nOrder 10\n39.1\n44.6\n60.2\n52.3\n50.0\n56.7\n54.6\n69.7\n71.4\nAvg.\n54.9\n56.0\n65.8\n65.2\n64.1\n65.8\n64.4\n70.6\n72.2\nFA\nOrder 1\n73.0\n78.6\n89.5\n90.7\n91.4\n85.6\n85.2\n87.8\n91.5\nOrder 2\n72.4\n82.3\n79.5\n77.7\n83.7\n80.6\n77.4\n79.8\n86.8\nOrder 3\n81.8\n78.9\n88.5\n89.5\n90.5\n84.8\n78.7\n87.1\n91.7\nOrder 4\n76.9\n77.6\n83.3\n84.5\n87.7\n77.5\n77.1\n83.2\n89.4\nOrder 5\n82.9\n86.1\n89.0\n88.0\n90.8\n76.7\n76.5\n84.1\n90.2\nOrder 6\n79.6\n84.3\n81.4\n80.7\n83.5\n71.0\n70.9\n86.4\n87.7\nOrder 7\n65.3\n80.5\n78.0\n78.0\n74.4\n75.7\n75.4\n78.8\n83.9\nOrder 8\n58.3\n83.5\n73.3\n74.0\n73.3\n72.6\n72.1\n74.2\n83.5\nOrder 9\n86.5\n88.8\n74.1\n79.0\n76.6\n78.0\n78.3\n87.7\n91.1\nOrder 10\n72.0\n89.5\n73.1\n73.7\n74.3\n73.4\n71.4\n89.8\n91.8\nAvg.\n74.9\n83.0\n81.0\n81.6\n82.6\n77.6\n76.3\n83.9\n88.8\nAll\nOrder 1\n76.4\n79.5\n83.0\n83.9\n84.4\n81.4\n80.9\n83.4\n85.9\nOrder 2\n75.7\n80.1\n78.2\n78.0\n80.5\n80.1\n77.4\n81.3\n84.0\nOrder 3\n79.4\n77.0\n82.2\n82.7\n84.1\n80.2\n77.4\n83.5\n85.6\nOrder 4\n78.5\n78.3\n83.3\n83.4\n84.8\n80.1\n79.1\n83.4\n87.3\nOrder 5\n76.4\n77.9\n86.4\n83.0\n85.8\n77.1\n76.1\n82.7\n87.1\nOrder 6\n70.4\n73.2\n71.1\n72.2\n73.4\n67.7\n66.9\n77.7\n78.6\nOrder 7\n56.3\n64.5\n68.3\n69.1\n67.2\n67.3\n66.3\n70.3\n74.9\nOrder 8\n53.2\n63.8\n65.5\n66.7\n65.8\n64.6\n64.5\n67.6\n74.1\nOrder 9\n77.0\n77.9\n66.0\n71.4\n65.9\n74.3\n73.9\n83.5\n86.1\nOrder 10\n62.3\n72.4\n67.7\n65.2\n64.9\n68.2\n66.0\n83.6\n85.0\nAvg.\n70.6\n74.5\n75.2\n75.6\n75.7\n74.1\n72.9\n79.7\n82.9\nTable 3. Comparison of the performance on the Digits-five dataset for different state-of-art methods in TDA, TDG, FA, and All. The results\nare presented for each domain order. The results of the baseline models are referenced from [9]. The best results are highlighted in bold.\nMetric & Orders\nSHOT\nSHOT++\nTent\nAdaCon\nEATA\nL2D\nPDEN\nRaTP\nOurs\nTDA\nOrder 1\n84.0\n87.5\n71.5\n77.4\n76.8\n85.9\n81.9\n89.7\n95.5\nOrder 2\n91.6\n94.8\n77.5\n76.0\n76.9\n91.3\n89.5\n90.7\n95.7\nOrder 3\n81.2\n79.9\n70.7\n75.8\n76.4\n85.9\n86.2\n87.8\n91.8\nOrder 4\n73.8\n79.6\n59.9\n64.9\n65.0\n77.6\n75.3\n86.8\n90.9\nOrder 5\n79.7\n84.9\n59.5\n65.3\n65.8\n79.3\n78.3\n87.5\n91.5\nOrder 6\n87.0\n92.1\n80.2\n80.5\n81.1\n89.7\n89.0\n90.0\n93.6\nOrder 7\n89.9\n91.2\n80.9\n82.1\n83.2\n87.6\n85.2\n91.6\n92.6\nOrder 8\n89.0\n91.5\n80.5\n80.2\n82.2\n88.6\n85.9\n89.7\n92.6\nOrder 9\n48.4\n48.8\n48.7\n55.7\n55.4\n74.2\n70.9\n85.9\n91.2\nOrder 10\n61.2\n62.9\n57.3\n58.3\n57.1\n82.9\n80.3\n87.1\n91.1\nAvg.\n78.6\n81.3\n68.7\n71.6\n72.0\n84.3\n82.3\n88.7\n92.7\nTDG\nOrder 1\n66.2\n68.3\n71.1\n72.6\n71.3\n72.3\n69.4\n77.0\n79.2\nOrder 2\n78.0\n78.2\n72.9\n75.8\n71.5\n78.1\n78.4\n79.5\n81.8\nOrder 3\n68.3\n65.8\n70.7\n67.0\n69.6\n71.7\n70.5\n77.0\n77.1\nOrder 4\n49.1\n52.0\n52.2\n53.2\n53.7\n62.3\n60.4\n72.0\n71.9\nOrder 5\n54.0\n54.1\n53.1\n51.1\n53.6\n62.7\n61.4\n72.9\n72.5\nOrder 6\n72.3\n75.2\n76.9\n75.8\n77.8\n78.2\n76.8\n81.0\n82.6\nOrder 7\n74.8\n76.0\n76.9\n73.0\n76.1\n78.1\n76.8\n81.9\n81.5\nOrder 8\n73.9\n72.6\n79.3\n76.9\n77.9\n78.0\n77.3\n81.3\n82.2\nOrder 9\n35.1\n39.0\n41.3\n41.3\n44.1\n61.7\n61.7\n73.2\n73.2\nOrder 10\n38.6\n41.7\n45.9\n46.3\n44.2\n65.5\n63.8\n71.7\n72.3\nAvg.\n61.0\n62.3\n64.0\n63.3\n64.0\n70.9\n69.7\n76.8\n77.4\nFA\nOrder 1\n60.0\n67.1\n67.8\n75.2\n76.2\n75.2\n71.4\n83.8\n87.5\nOrder 2\n73.9\n75.5\n82.2\n82.7\n83.6\n81.1\n79.6\n87.4\n89.8\nOrder 3\n70.7\n71.2\n72.9\n80.4\n85.5\n85.1\n81.9\n90.1\n91.7\nOrder 4\n56.5\n65.3\n50.8\n59.0\n58.8\n72.3\n70.0\n82.3\n85.2\nOrder 5\n77.0\n79.1\n61.4\n71.7\n71.2\n74.9\n73.9\n85.2\n87.8\nOrder 6\n59.3\n67.4\n81.2\n80.4\n79.7\n76.8\n74.1\n84.9\n86.5\nOrder 7\n62.2\n71.0\n79.8\n82.1\n80.9\n77.6\n76.1\n84.7\n86.4\nOrder 8\n57.2\n66.0\n80.0\n81.9\n79.4\n75.0\n72.6\n83.3\n85.3\nOrder 9\n25.1\n30.0\n33.1\n56.8\n61.8\n72.5\n68.5\n85.1\n86.5\nOrder 10\n39.7\n52.5\n51.5\n52.0\n52.4\n74.1\n72.0\n82.8\n84.2\nAvg.\n58.2\n64.5\n66.1\n72.2\n73.0\n76.5\n74.0\n85.0\n87.1\nAll\nOrder 1\n70.1\n74.3\n70.1\n75.1\n74.8\n77.8\n74.2\n83.5\n87.4\nOrder 2\n81.2\n82.8\n77.5\n78.2\n77.3\n83.5\n82.5\n85.9\n89.1\nOrder 3\n73.4\n72.3\n71.4\n74.4\n77.2\n80.9\n79.5\n85.0\n86.9\nOrder 4\n59.8\n65.6\n54.3\n59.0\n59.2\n70.7\n68.6\n80.4\n82.7\nOrder 5\n70.2\n72.7\n58.0\n62.7\n63.5\n72.3\n71.2\n81.9\n83.9\nOrder 6\n72.9\n78.2\n79.4\n78.9\n79.5\n81.6\n80.0\n85.3\n87.6\nOrder 7\n75.6\n79.4\n79.2\n79.1\n80.1\n81.1\n79.4\n86.1\n86.8\nOrder 8\n73.4\n76.7\n79.9\n79.7\n79.8\n80.5\n78.6\n84.8\n86.7\nOrder 9\n36.2\n39.3\n41.0\n51.3\n53.8\n69.5\n67.0\n81.4\n83.6\nOrder 10\n46.5\n52.4\n51.6\n52.2\n51.2\n74.2\n72.0\n80.5\n82.5\nAvg.\n65.9\n69.4\n66.2\n69.1\n69.6\n77.2\n75.3\n83.5\n85.7\nTable 4. Comparison of the performance on the DomainNet dataset for different state-of-art methods in TDA, TDG, FA, and All. The\nresults are presented for each domain order. The results of the baseline models are referenced from [9]. The best results are highlighted in\nbold.\nMetric & Orders\nSHOT\nSHOT++\nTent\nAdaCon\nEATA\nL2D\nPDEN\nRaTP\nOurs\nTDA\nOrder 1\n68.4\n70.5\n59.0\n60.4\n60.0\n59.9\n60.8\n68.6\n70.3\nOrder 2\n69.7\n66.2\n28.9\n66.2\n65.8\n56.5\n54.2\n72.0\n74.7\nOrder 3\n72.6\n73.4\n65.6\n68.6\n69.4\n54.8\n52.7\n66.1\n74.3\nOrder 4\n51.3\n53.5\n54.6\n52.2\n52.9\n57.3\n55.2\n61.7\n67.0\nOrder 5\n68.5\n70.9\n60.3\n60.3\n58.7\n56.9\n55.7\n64.4\n74.4\nOrder 6\n63.1\n65.3\n51.8\n52.4\n55.0\n49.3\n50.2\n56.3\n63.2\nOrder 7\n47.7\n48.1\n50.0\n50.8\n51.7\n41.7\n40.4\n58.0\n59.2\nOrder 8\n72.8\n73.2\n67.6\n71.6\n70.7\n64.0\n63.4\n67.4\n76.4\nOrder 9\n74.2\n75.9\n67.6\n71.3\n69.3\n61.9\n62.2\n72.5\n76.4\nOrder 10\n71.9\n72.1\n31.0\n67.9\n71.2\n59.9\n61.0\n66.9\n74.2\nAvg.\n66.0\n66.9\n53.6\n62.2\n62.5\n56.2\n55.6\n65.4\n71.0\nTDG\nOrder 1\n46.9\n45.5\n52.1\n51.3\n51.2\n49.6\n48.0\n53.3\n54.0\nOrder 2\n52.2\n50.0\n31.0\n52.7\n55.2\n55.6\n51.2\n57.6\n58.1\nOrder 3\n53.6\n53.3\n58.4\n53.7\n58.7\n52.8\n51.1\n57.5\n60.2\nOrder 4\n40.8\n41.9\n50.6\n51.3\n51.1\n48.2\n47.0\n55.8\n57.7\nOrder 5\n48.4\n49.6\n52.8\n53.0\n52.8\n53.1\n51.0\n54.2\n56.0\nOrder 6\n34.0\n35.3\n33.1\n32.9\n33.6\n36.5\n37.2\n41.8\n43.5\nOrder 7\n23.2\n25.7\n35.4\n32.9\n34.4\n32.2\n30.2\n42.5\n42.6\nOrder 8\n59.2\n59.9\n61.0\n62.0\n62.0\n62.1\n61.4\n63.2\n62.7\nOrder 9\n58.7\n59.3\n61.3\n61.6\n63.3\n59.4\n59.9\n63.8\n63.5\nOrder 10\n56.2\n60.1\n41.2\n61.3\n59.0\n57.4\n56.4\n62.3\n63.2\nAvg.\n47.3\n48.1\n47.7\n51.3\n52.1\n50.7\n49.3\n55.2\n56.2\nFA\nOrder 1\n61.4\n66.5\n67.4\n67.0\n64.3\n63.7\n61.1\n67.5\n70.9\nOrder 2\n64.5\n68.9\n34.1\n62.6\n65.8\n48.9\n46.3\n70.4\n74.3\nOrder 3\n62.9\n67.7\n65.6\n66.3\n69.2\n45.2\n43.1\n64.7\n72.9\nOrder 4\n42.1\n65.4\n56.4\n53.3\n52.7\n41.5\n39.5\n57.1\n66.4\nOrder 5\n60.9\n68.5\n58.0\n56.6\n57.4\n51.2\n48.6\n62.0\n72.4\nOrder 6\n61.1\n66.3\n52.4\n49.4\n54.8\n48.0\n46.0\n53.8\n63.6\nOrder 7\n42.8\n51.7\n48.5\n48.5\n47.7\n37.2\n36.0\n55.0\n57.5\nOrder 8\n61.6\n67.5\n71.6\n72.8\n73.5\n58.8\n55.1\n63.1\n74.9\nOrder 9\n67.4\n77.3\n76.8\n76.1\n76.0\n66.5\n65.6\n76.3\n82.8\nOrder 10\n60.4\n69.6\n30.4\n65.5\n66.3\n61.4\n60.9\n64.6\n72.9\nAvg.\n58.5\n66.9\n56.1\n61.8\n62.8\n52.2\n50.2\n63.5\n70.9\nAll\nOrder 1\n58.9\n60.8\n59.5\n59.6\n58.5\n57.7\n56.6\n63.1\n65.1\nOrder 2\n62.1\n61.7\n31.3\n60.5\n62.3\n53.7\n50.6\n66.7\n69.0\nOrder 3\n63.0\n64.8\n63.2\n62.9\n65.8\n50.9\n49.0\n62.8\n69.1\nOrder 4\n44.7\n53.6\n53.9\n52.3\n52.2\n49.0\n47.2\n58.2\n63.7\nOrder 5\n59.3\n63.0\n57.0\n56.6\n56.3\n53.7\n51.8\n60.2\n67.6\nOrder 6\n52.7\n55.6\n45.8\n44.9\n47.8\n44.6\n44.5\n50.6\n56.8\nOrder 7\n37.9\n41.8\n44.6\n44.1\n44.6\n37.0\n35.5\n51.8\n53.1\nOrder 8\n64.5\n66.9\n66.7\n68.8\n68.7\n61.6\n60.0\n64.6\n71.3\nOrder 9\n66.8\n70.8\n68.6\n69.7\n69.5\n62.6\n62.6\n70.9\n74.2\nOrder 10\n62.8\n67.3\n34.2\n64.9\n65.5\n59.6\n59.4\n64.6\n70.1\nAvg.\n57.3\n60.6\n52.5\n58.4\n59.1\n53.0\n51.7\n61.4\n66.0\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2023-03-28",
  "updated": "2023-10-13"
}