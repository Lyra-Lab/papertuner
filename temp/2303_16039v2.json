{
  "id": "http://arxiv.org/abs/2303.16039v2",
  "title": "Exploring Natural Language Processing Methods for Interactive Behaviour Modelling",
  "authors": [
    "Guanhua Zhang",
    "Matteo Bortoletto",
    "Zhiming Hu",
    "Lei Shi",
    "Mihai Bâce",
    "Andreas Bulling"
  ],
  "abstract": "Analysing and modelling interactive behaviour is an important topic in\nhuman-computer interaction (HCI) and a key requirement for the development of\nintelligent interactive systems. Interactive behaviour has a sequential\n(actions happen one after another) and hierarchical (a sequence of actions\nforms an activity driven by interaction goals) structure, which may be similar\nto the structure of natural language. Designed based on such a structure,\nnatural language processing (NLP) methods have achieved groundbreaking success\nin various downstream tasks. However, few works linked interactive behaviour\nwith natural language. In this paper, we explore the similarity between\ninteractive behaviour and natural language by applying an NLP method, byte pair\nencoding (BPE), to encode mouse and keyboard behaviour. We then analyse the\nvocabulary, i.e., the set of action sequences, learnt by BPE, as well as use\nthe vocabulary to encode the input behaviour for interactive task recognition.\nAn existing dataset collected in constrained lab settings and our novel\nout-of-the-lab dataset were used for evaluation. Results show that this natural\nlanguage-inspired approach not only learns action sequences that reflect\nspecific interaction goals, but also achieves higher F1 scores on task\nrecognition than other methods. Our work reveals the similarity between\ninteractive behaviour and natural language, and presents the potential of\napplying the new pack of methods that leverage insights from NLP to model\ninteractive behaviour in HCI.",
  "text": "Exploring Natural Language Processing\nMethods for Interactive Behaviour Modelling\nGuanhua Zhang1, Matteo Bortoletto1, Zhiming Hu1,2⋆, Lei Shi1, Mihai Bˆace1,\nand Andreas Bulling1\n1Institute for Visualisation and Interactive Systems, 2Institute for Modelling and\nSimulation of Biomechanical Systems, University of Stuttgart, Stuttgart, Germany\n{guanhua.zhang, matteo.bortoletto, zhiming.hu, lei.shi, mihai.bace,\nandreas.bulling}@vis.uni-stuttgart.de\nAbstract. Analysing and modelling interactive behaviour is an impor-\ntant topic in human-computer interaction (HCI) and a key requirement\nfor the development of intelligent interactive systems. Interactive be-\nhaviour has a sequential (actions happen one after another) and hier-\narchical (a sequence of actions forms an activity driven by interaction\ngoals) structure, which may be similar to the structure of natural lan-\nguage. Designed based on such a structure, natural language processing\n(NLP) methods have achieved groundbreaking success in various down-\nstream tasks. However, few works linked interactive behaviour with nat-\nural language. In this paper, we explore the similarity between interac-\ntive behaviour and natural language by applying an NLP method, byte\npair encoding (BPE), to encode mouse and keyboard behaviour. We\nthen analyse the vocabulary, i.e., the set of action sequences, learnt by\nBPE, as well as use the vocabulary to encode the input behaviour for\ninteractive task recognition. An existing dataset collected in constrained\nlab settings and our novel out-of-the-lab dataset were used for evalua-\ntion. Results show that this natural language-inspired approach not only\nlearns action sequences that reﬂect speciﬁc interaction goals, but also\nachieves higher F1 scores on task recognition than other methods. Our\nwork reveals the similarity between interactive behaviour and natural\nlanguage, and presents the potential of applying the new pack of meth-\nods that leverage insights from NLP to model interactive behaviour in\nHCI.\nKeywords: Interactive Behaviour Modelling · Natural Language Pro-\ncessing · Mouse and Keyboard Input · Out-of-the-lab Dataset.\n1\nIntroduction\nComputational modelling of interactive behaviour has emerged as a key compo-\nnent of intelligent user interfaces (IUIs) in human-computer interaction (HCI)\n[66,70,3,2,14]. For example, understanding interactive behaviour helps HCI re-\nsearchers and user experience (UX) designers analyse and improve interactive\n⋆Corresponding author\narXiv:2303.16039v2  [cs.HC]  11 May 2023\n2\nG. Zhang et al.\ntion\nI\nn\nt\ne\nr\na\nc\nt\ni\no\nn\nCharacters\nSubwords\nWords\nInput \nActions\nActivities\nTasks\nNatural Language Analogy\nInteractive Behaviour\nInter\nInteraction\nac\nLanguage Encoder\nInteractive \nBehaviour Data\nLanguage Encoder\nFig. 1.\nGiven that both interactive behaviour and natural language are sequential\nand hierarchical, we explored their similarity by applying an NLP method (a language\nencoder) to model mouse and keyboard behaviour.\nsystems [51,7]. Mouse and keyboard input is particularly promising because it\nis readily available on a large number of devices and pervasively used in daily\nlife [66,59]. Interactive behaviour consists of low-level, atomic input actions that\ncannot be further decomposed [41], which may resemble characters in natural\nlanguage. Furthermore, a sequence of such actions (an activity) that can reﬂect\nhigher-level interaction goals may resemble a (sub)word that is a sequence of\ncharacters with semantic meanings. As such, interactive behaviour has both a\nsequential (actions happen one after another) and a hierarchical structure (a\nsequence of actions forms an activity driven by speciﬁc interaction goals), and\nhence may be similar to natural language (see Fig. 1). On the other hand, NLP\nmethods, leveraging the sequential and hierarchical structure of input data, have\nrecently achieved groundbreaking success in various downstream tasks like ma-\nchine translation and question-answering [45,34,32,36]. However, analysing the\npossible similarity and link between interactive behaviour and natural language\nremains under-explored in HCI. One notable exception is the work by Han et\nal. that encoded n consecutive actions (like mouse clicks) into tokens to learn\naction embeddings [21]. However, at its core, the method uses n-gram, which\nlimits the length of action sequences to a ﬁxed length n and requires a dedicated\nsearch for its optimal value. Moreover, the vocabulary size grows exponentially\nas n increases [57]. Due to such drawback, n-gram has been dropped in NLP\nin favour of more ﬂexible methods such as byte pair encoding (BPE) [48,47].\nBPE and its variants are used in a signiﬁcant number of large language models\n(LLMs) to encode text as subwords, allowing rare or unseen words to be han-\ndled without introducing new tokens every time [54,53]. Additionally, subwords\nin the vocabulary generated by BPE can have various lengths, allowing a rich\nand ﬂexible vocabulary. In this work, we explore the similarity between mouse\nand keyboard behaviour and natural language, by using BPE to learn a vocab-\nulary, i.e., a set of activities, which is further used to encode the behaviour to\nperform interactive task recognition. Knowing which task the user is conducting\nis essential for adaptive interactive systems that aim to understand interactive\nbehaviour and interaction goals [44,17,26].\nExploring NLP Methods for Interactive Behaviour Modelling\n3\nExisting mouse and keyboard datasets were typically collected in controlled\nlaboratory settings, although behaviour tends to be more natural in out-of-the-\nlab settings [40]. We evaluate the method on two datasets that cover both set-\ntings and oﬀer both modalities. For the lab setting, we chose the Buﬀalo dataset\ncollected by Sun et al. [59] as it is the largest available dataset [43]. For the out-\nof-the-lab setting, given a lack of suitable publicly available data, we collected\na novel multimodal dataset named EMAKI (Everyday Mouse And Keyboard\nInteractions)1 EMAKI was collected from 39 participants performing three inter-\nactive tasks: text entry and editing, image editing and questionnaire completion.\nThese tasks can be found in a wide range of applications and UIs, and cover\nvarying types of mouse and keyboard actions.\nOn the two datasets, vocabulary analysis shows that BPE could learn explain-\nable activities, e.g., reﬂecting graphical user interface (GUI) layouts and indicat-\ning interaction goals such as performing mouse dragging or keyboard shortcuts.\nResults from interactive task recognition show that BPE outperformed other\nmethods on both modalities and datasets. In summary, our contributions are\nthree-fold: (1) We collect EMAKI, a novel 39-participant out-of-the-lab mouse\nand keyboard dataset. (2) We explore the potential similarity between natu-\nral language and mouse and keyboard behaviour by learning meaningful activi-\nties via a commonly used NLP method, BPE. (3) We show that encoding with\nBPE also improves the performance of interactive task recognition. As such,\nour work uncovers the similarity between natural language and interactive be-\nhaviour, showing the potential for applying the new pack of methodology, i.e.,\nNLP methods, to computational interactive behaviour modelling in HCI.\n2\nRelated Work\n2.1\nModelling Interactive Behaviour in HCI\nClassical HCI approaches include descriptive models, e.g., Fitts’s Law [1], and\npredictive models, e.g., the keystroke-level model (KLM) [12]. However, they are\nlimited in strict controls and modelling simple tasks like pointing to a target\nor routine tasks that have to be speciﬁed step by step [12]. Recent research\nused 1D convolutional neural networks (CNN) [28,25], long short-term memory\n(LSTM) [26] and gated recurrent unit (GRU) [26] to encode gaze and head be-\nhaviour, based on the sequential structure, while others focused on spatial anal-\nysis and modelling [29,28]. Speciﬁcally, Xu et al. modelled mouse and keyboard\nbehaviour by accumulating cursor positions into binary attention maps [66].\nOther researchers modelled interactive behaviour from a statistical perspective.\nFor example, Borji et al. used Hidden Markov Models (HMM) to encode mo-\ntor actions including mouse clicks, mouse positions, and joystick positions in\nvideo games [8], while Sun et al. applied Gaussian mixture models (GMM) on\nkeystrokes in text editing tasks [59]. Researchers also encoded eye movements [11]\n1 The dataset and code are available here: https://git.hcics.simtech.uni-stuttgart.de/\npublic-projects/EMAKI\n4\nG. Zhang et al.\nor gestures [63,55] into strings for activity recognition. Given that interactive be-\nhaviour has a sequential and hierarchical structure, which may resemble natural\nlanguage, we explored modelling interactive behaviour from an NLP perspective.\n2.2\nEncoding Methods for Natural Language\nRecent attractive success in NLP has been largely attributed to methods that\neﬃciently encode characters [34], words [45] or sentences [50] into a vector rep-\nresentation. HCI researchers also followed this trend to model GUIs [38,61] or\nbehavioural diﬀerences over time [21]. A key requirement for such methods is to\nencode or tokenise the input to generate a usable vocabulary of concepts. Due to\nthe clear structure of natural language, NLP methods encode at the character,\nsubword or word level. One popular approach is n-gram, which uses n words\nin a sequence to determine the context where commonly n ≤5 [21,31,30,49].\nHowever, such a method is limited by the choice of n, and the exponential in-\ncrease of vocabulary size along n. More promising approaches learn a vocabulary\nof subwords, among which BPE has been widely used given that it allows rich\nand ﬂexible vocabulary and understanding rare or unseen words [62,35,47,48].\nConsequently, we employ BPE as the NLP method to create a vocabulary for\ninteractive behaviour.\n2.3\nAnalysis and Modelling of Mouse and Keyboard Behaviour\nThe mouse and keyboard are among the most widely used input modalities in\ndaily interactions with computers [66,59]. Some researchers only focused on one\nmodality, i.e., mouse or keyboard. Arapakis et al. explored diﬀerent representa-\ntions of mouse movements in web search tasks, including time series, heatmaps,\nand trajectory-based images [5], while Antal et al. employed 1D CNN to encode\nmouse actions including click and drag [3]. Dhakal et al. analysed keystroke pat-\nterns in a transcription typing task by correlation analysis [14], while Acien et\nal. employed LSTM to encode keystroke sequences in free text typing [2]. In\ncontrast, Sun et al. explored both mouse and keyboard actions in two typing\ntasks, yet the work was limited to fully controlled laboratory settings [59].\n3\nDatasets for Evaluation\nAlthough interactive behaviour, and speciﬁcally mouse and keyboard data, has\nbeen widely studied in HCI [59,66], most existing datasets have been collected in\nstrictly controlled laboratory settings. Laboratory settings have the advantages\nof control and internal validity, but their ecological validity is highly limited [4].\nOur out-of-the-lab data collection did not control where, when, how long and\nvia which laptop or desktop participants could join, allowing more natural be-\nhaviour [40,46]. In addition, most datasets only include either mouse or keyboard\ndata, while we opted for evaluations on both modalities. As such, we analysed\nmouse and keyboard behaviour from the in-the-lab Buﬀalo dataset [59] and\nExploring NLP Methods for Interactive Behaviour Modelling\n5\nEMAKI, a novel multimodal out-of-the-lab dataset that we collected speciﬁcally\nfor this purpose, given lacking suitable publicly available data. To evaluate con-\nstraints in data collection from a time perspective, task and study completion\ntimes were calculated. The former only counts the time spent on tasks, while\nthe latter refers to ﬁnishing the entire study, including pauses.\n3.1\nThe Buﬀalo Dataset\nTo the best of our knowledge, Buﬀalo [59] is the largest publicly available in-\nthe-lab dataset containing both mouse and keyboard interactions. The dataset\nwas collected with standalone keyboards over three sessions. 148 participants\nperformed two typing tasks: transcribing a pre-deﬁned text and typical oﬃce\nactivities, such as answering predeﬁned questions and sending emails. The aver-\nage number of mouse actions and keystrokes per participant exceeded 19 K and\n17 K, respectively. 75 participants completed both tasks with the same keyboard,\nwhile the remaining used three keyboards across sessions. Data from the former\n75 participants were used in this work for a more controlled condition, follow-\ning [65]. The average task completion time was 41.71 mins (SD = 6.34), while\nthe average study completion time was slightly longer, 41.81 mins (SD = 6.27),\nindicating that participants barely took breaks in this constrained setting.\n3.2\nThe EMAKI Dataset\nWe opted for an online study including three tasks: text entry and editing,\nimage editing, and questionnaire completion. These tasks can be found in a\nwide range of interactive applications and UIs, and cover varying types of mouse\nand keyboard actions [66,59]. Furthermore, the tasks are neither limited to a\nparticular real-world application [13,9] nor too controlled or artiﬁcial [14,70,71],\ndiﬀerent from the typing-focused tasks in Buﬀalo. Two short assessments were\ndesigned to analyse if participants show diﬀerent proﬁciencies in using mouse\nand keyboard.\nThe study was implemented as a web application and hosted on our university\nserver. The link to the study was sent directly to the participants. The frontend\nwas implemented in JavaScript, while the backend consisted of a Node.js server\nand an SQLite database. We recorded clicks and key presses with separate events\nfor press and release, mouse movements and their associated timestamps.\nParticipants We recruited 52 participants through university mailing lists and\nsocial networks. 12 participants who did not ﬁnish the study and one teenage\nparticipant were ﬁltered out, leading to 39 participants in the end (18 female,\n18 male and 3 “other gender”). Their ages ranged between 18 and 54 years\n(M = 25.05, SD = 6.51). Participants completed the study from 16 countries.\nOn average, they reported having used mouse and keyboard for 13.64 years\n(SD = 6.80). 15 participants used laptop touchpads, while the others used tradi-\ntional mice. 28 participants used laptop keyboards and the rest used standalone\nkeyboards.\n6\nG. Zhang et al.\na\n8,67\na\nb\nc\nHouse\nFig. 2. Screenshots of the three interactive tasks in our online study: (a) text entry\nand editing, (b) image editing, and (c) questionnaire completion.\nInteractive Tasks In task text entry and editing, participants wrote a piece of\ntext in English in a text editor2 for one trial (Fig. 2a). We did not specify the\ntopic but oﬀered suggestions, such as “summarise a movie/TV series/documentary\nthat you recently watched” or “describe your pet”. We asked participants to\nwrite ≥200 words and apply ≥15 formatting rules, e.g. change font size or align-\nment. We allowed any operation provided by the editor, such as copy-paste and\nundo. Two counters in the top left showed the number of words they already\ntyped and formatting operations they applied. These counters were initially red\nand turned green once the minimum thresholds were reached.\nIn task image editing, participants were presented with two images shown\nside-by-side in an image editor3 (Fig. 2b). The image on the left was a real\nphotograph, whereas the image on the right was a sketch. On either or both\nsides, participants performed operations provided by the editor in any order they\nwanted. Candidate operations are drawing, cropping, ﬂipping, rotating, adding\nicons and adding ﬁlters. To proceed to the next task, they had to perform at\nleast 100 editing operations. In addition, we asked them to add at least one text\nbox that contained a minimum of 10 characters. Similarly to the previous task,\ncounters showed the task progress.\nQuestionnaire completion involved participants in completing four question-\nnaires4, leading to four trials (Fig. 2c). These questionnaires served a dual pur-\npose: providing information about participants, which can serve as metadata for\nfuture work on the dataset, while at the same time allowing us to record natural-\nistic mouse and keyboard data. The ﬁrst questionnaire focused on demographics\nand included questions on gender, age, country of origin, country of residence,\nexperience in using mouse and keyboard, and whether participants had any vi-\nsual impairments. Afterwards were three widely-used personality questionnaires:\nBFI-44 (Big Five)5, BIS-11 (Barratt Impulsiveness Scale)6 and BIS-BAS (the\nBehavioural Inhibition and Approach System)7.\n2 https://github.com/tinymce/tinymce\n3 https://github.com/nhn/tui.image-editor\n4 https://github.com/surveyjs/survey-library\n5 https://www.ocf.berkeley.edu/∼johnlab/bﬁ.php\n6 http://www.impulsivity.org/measurement/bis11\n7 https://local.psy.miami.edu/people/faculty/ccarver/availbale-self-report-instruments/\nbisbas-scales/\nExploring NLP Methods for Interactive Behaviour Modelling\n7\na\nb\na\nb\nFig. 3. Two proﬁciency assessments: (a) text typing and (b) move and click.\nProcedure Before starting with the tasks, participants were asked to carefully\nread the study goals and task descriptions. They were then asked whether they\nwere using a mouse or touchpad, and a laptop or standalone keyboard. To start\nthe study, participants had to click two checkboxes to conﬁrm that (1) they had\nread and understood the goals of the study, and (2) their data may be published\nand analysed for research purposes. Afterwards, participants performed tasks in\nfullscreen. If they left the fullscreen mode during a task, the task was restarted.\nWe opted for the design to discourage participants from multitasking. To reduce\npotential eﬀects of task order, half of the initial 52 participants performed the\ntext entry and editing task ﬁrst, followed by the image editing task, while the\nother half performed in the inverse order. After data ﬁltering, 24 participants\ndid the text task and then image task, while the other 15 in the inverse order.\nWe always showed questionnaires at the end, following studies that also collected\npersonality questionnaires [24,42]. Detailed guidelines for tasks were available to\nparticipants throughout the study. Participants could contact us whenever they\nhad questions, felt uncomfortable or unsure of any task or wanted to withdraw.\nUpon completion of the study, participants were shown their results of person-\nality questionnaires as compensation. No monetary compensation was made.\nDataset Statistics The average task completion time was 37.40 mins (SD =\n13.91), in which 16.60 mins (SD = 8.51) were spent on text entry and editing,\n6.15 mins (SD = 3.60) on image editing, and 9.84 mins (SD = 4.48) on question-\nnaires. The average study completion time was signiﬁcantly longer, 55.33 mins\n(SD = 29.32). In total, we collected 1.14 M mouse actions and 205 K keyboard\nactions. 38% of mouse actions were generated from the image editing task, 43%\nfrom questionnaire completion, while only 19% came from the text entry and\nediting task. Text entry and editing contributed 92% of the keyboard actions,\nwhile only 8% were from the other two tasks (image editing: 3%, questionnaire\ncompletion: 5%).\nAssessments of Proﬁciency Before interactive tasks, our study also included\ntwo short assessments to analyse if participants who used diﬀerent types of input\ndevices showed diﬀerent proﬁciencies in using mouse and keyboard. The two\n8\nG. Zhang et al.\nBPE\nI-DT\nTransformer\nEncoder\nFC\n+\nSoftmax\nClassifier\nInteractive Task \nRecognition\nText Entry \nand Editing\nImage Editing\nQuestionnaire \nCompletion\nAnalysis\nVocabulary \nLength\nFrequent Action \nSequences\nHierarchy\nRaw Data\nSegment\nFig. 4. Overview of our pipeline of exploring modelling interactive behaviour from an\nNLP perspective.\nassessments were text typing for keyboard proﬁciency and move and click for\nmouse proﬁciency, shown in Fig. 3. Text typing involved copying a short piece\nof text (∼100 words, Fig. 3a) as quickly as possible [19]. The average duration\nof key presses and the number of keys pressed per minute were calculated as\nkeyboard metrics [19]. Move and click was inspired by a Fitts’s Law task [56],\nwhere participants clicked an orange dot that randomly appeared at a predeﬁned\nlocation as quickly as possible over multiple rounds. Once clicked, the orange dot\nturned grey and another random dot turned orange (Fig. 3b). Fitts’s law [15]\nmodels movement time as MT = a+b log2\n\u0000 2d\nw\n\u0001\n, where d is the distance between\nthe centre of the target and the starting point; w is the width of the target; a\nand b are constants that can be interpreted as the delay and the acceleration.\nBased on d, w and MT recorded in move and click, we computed a and b via\nlinear regression and used them as metrics of mouse proﬁciency.\nBased on the type of mouse (touchpad vs. traditional mouse), we split partic-\nipants into two groups and then calculated mouse metrics from data collected in\nthe mouse assessment. A Mann-Whitney U test showed that both metrics were\nsigniﬁcantly diﬀerent between the two groups. One reason is that touchpad and\ntraditional mouse lead to diﬀerent pointing speeds and accuracies [23]. Then,\nwe split participants into two groups based on using a laptop or standalone key-\nboard. No signiﬁcant diﬀerence was found in keyboard metrics calculated from\nthe keyboard assessment.\n4\nModelling Interactive Behaviour with an NLP Method\nAs Fig. 4 shows, the raw data (mouse and keyboard action sequences) are ﬁrst\nsegmented into subsequences. Core to our approach is BPE learning a vocab-\nulary of subwords, i.e. a set of meaningful mouse and keyboard activities, and\nExploring NLP Methods for Interactive Behaviour Modelling\n9\nthen encoding the behaviour based on the vocabulary. As BPE requires discrete\ninputs, mouse data are preprocessed additionally using the dispersion-threshold\nidentiﬁcation (I-DT) algorithm, that converts continuous-valued mouse coordi-\nnates into discrete tokens. The encodings generated by BPE are then evaluated\nin two ways to explore if a natural language-like structure exists in mouse and\nkeyboard behaviour that can be captured by this widely used NLP method: (1)\nanalyse the semantic meaning of the vocabulary, i.e., interaction goals underly-\ning learnt activities, and (2) as input to train a Transformer-based classiﬁer for\ntask recognition. The two evaluations are demonstrated in Section 5.\n4.1\nData Preprocessing\nDiﬀerent from natural language where words and sentences are separated by\nspaces and punctuations, modelling interactive behaviour ﬁrst requires splitting\ndata into smaller units. Thus, a sliding non-overlapping window was used to\nsegment the long raw data. On the keyboard actions, the window lengths Lwin\nwere empirically set to 10, 50, and 100. The window lengths Lwin for the mouse\nactions were set to 20, 100 and 200, as we observed on both datasets that, the\nnumber of generated mouse actions for a ﬁxed time window is roughly twice as\nmany as the keyboard actions. When using both modalities jointly, the window\nlengths were set to the mean value of those for single modalities, i.e. Lwin =\n15, 75 and 150. For keyboard actions, the action type and the key value were\nconcatenated as a token, e.g., KeyDown a (a↓) or KeyUp Shift (Shift↑). Buﬀalo\nrecorded 91 key values, while EMAKI had 137 values, yielding 182 and 274\natomic actions forming the starting vocabulary, respectively. With more types\nof keys, EMAKI can potentially reﬂect more behaviour varieties.\nParticipants completed our study on their own computers with diﬀerent\nscreen resolutions, so we ﬁrst re-scaled the mouse coordinates to [0, 1]. For con-\nsistency, we re-scaled Buﬀalo mouse data to the same range. We observed two\ncategories of mouse behaviour: pinpoint, i.e. interacting with the target UI ele-\nment in a small area, where moves are shorter, slower and more concentrated,\nresembling gaze ﬁxations; and re-direction between targets, resembling fast sac-\ncadic eye movements between ﬁxations [52]. Inspired by gaze ﬁxation detection,\nwe used I-DT [52] to preprocess mouse data (see Appendix). Then we divided\nthe screen equally into four areas (0: top-left, 1: top-right, 2: bottom-left, 3:\nbottom-right). The action type (move or click), mouse behaviour category (pin-\npoint or re-direction), and the screen area were concatenated as a token, e.g.,\nMove Redirection Area0 or Click Pinpoint Area3. When representing clicks, Buf-\nfalo only recorded a Click, while we recorded both Down (press) and Up (re-\nlease) events. Therefore, Buﬀalo has 2×2×4=16 atomic actions and EMAKI has\n3×2×4=24.\n4.2\nEncoding Mouse and Keyboard Behaviour with BPE\nWe employed BPE (see Appendix for its algorithm) to learn a vocabulary of\nsubwords, i.e., activities that consist of various numbers of consecutive actions.\n10\nG. Zhang et al.\nEMAKI\nBuffalo\n0\n100\n200\n300\n400\nActivity Length\n (a) Mouse\nEMAKI\nBuffalo\n0\n10\n20\n30\n40\n50\n (b) Keyboard\nEMAKI\nBuffalo\n0\n50\n100\n150\n200\n250  (c) Both\nBPE-300\nBPE-600\nBPE-900\nFig. 5. Violin plots for the lengths of activities learnt by BPE after 300 (in red), 600 (in\nblue) and 900 (in green) iterations, of (a) mouse, (b) keyboard and (c) both modalities\non EMAKI and Buﬀalo datasets. Each bar shows the range of activity lengths, while\nthe middle line indicates the median length. The y-axes are scaled according to the\nrange in each subplot.\nStarting from the action sequence set D, the vocabulary V is built after k itera-\ntions. In each iteration, the most frequent pair of actions or activities form a new\nactivity, which is added into V and used to update D. We consider each action\nas a character, given it is an inseparable, atomic unit. The initial vocabulary\nis composed of actions and one extra token representing the end of the action\nsequence from one task trial. Thus, the initial vocabulary sizes are |V |mouse = 17\nand |V |key = 183 in Buﬀalo, and |V |mouse = 25, |V |key = 275 in EMAKI. We set\nk to 300, 600 and 900 empirically.\n5\nEvaluations of the NLP Method\nAs mentioned at the beginning of Section 4, BPE was evaluated in two ways:\n(1) we analysed its vocabulary to examine if the way of learning semantic sub-\nwords from characters could learn meaningful activities from interactive actions;\nand (2) we tested if encoding interactive behaviour in this NLP fashion bene-\nﬁted a downstream task, interactive task recognition, using a Transformer-based\nclassiﬁer.\n5.1\nAnalysis of the Learnt Vocabulary\nWe ﬁrst examined statistics of the vocabulary including its size and activity\nlengths. Then we analysed semantic meanings of the most frequent and long\nactivities. Frequent activities are short, low-level and pervasively exist in various\nactivities, while long activities reﬂect high-level and complex goals.\nVocabulary Statistics. As Fig. 5a shows, in EMAKI the maximum length\nof mouse activities reached 243 actions (BPE-900), while the median length\nwas 16. The longest keyboard activity had 53 actions, while the median length\nwas 3 (Fig. 5b). When using both modalities jointly, the maximum activity\nlength was 239 after 900 iterations, while the median length was 4 (Fig. 5c).\nIn Buﬀalo, the lengths of activities had a maximum of 405 and a median of\n39 from mouse behaviour (Fig. 5a); a maximum of 16 and a median of 4 from\nkeyboard behaviour (Fig. 5b); and a maximum of 158 and a median of 4 from\nExploring NLP Methods for Interactive Behaviour Modelling\n11\nDataset\nEMAKI\nBuﬀalo\nMethod\nBPE-300 BPE-600 BPE-900 BPE-300 BPE-600 BPE-900\nMouse\n322\n622\n921\n310\n609\n909\nKeyboard\n513\n808\n1103\n473\n770\n1067\nBoth\n569\n864\n1163\n496\n790\n1084\nTable 1. Vocabulary sizes generated using BPE after 300, 600 and 900 iterations, on\nEMAKI and Buﬀalo datasets.\nRank\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nEMAKI ⊔↓, ⊔↑⇐↓,⇐↑e↓, e↑t↓, t↑a↓, a↑o↓, o↑⇐↓,⇐↑,⇐↓,⇐↑i↓, i↑s↓, s↑n↓, n↑\nBuﬀalo ⊔↓, ⊔↑⇐↓,⇐↑e↓, e↑t↓, t↑o↓, o↑i↓, i↑\na↓, a↑\ns↓, s↑n↓, n↑l↓, l↑\nTable 2. The ten most frequent keyboard activities found by BPE. The ⊔symbol\nrepresents Space. The ⇐symbol means Backspace. The down arrow ↓and up arrow ↑\ndenote KeyDown and KeyUp, respectively.\njoint modalities (Fig. 5c). Mouse activities were longer than keyboard activities,\nindicating that the preprocessed mouse data were more similar compared to\npreprocessed keyboard data. Comparisons between datasets show that mouse\nactivities in EMAKI were more diverse, while Buﬀalo contained more diverse\nkeyboard activities.\nTable 1 shows the vocabulary sizes generated by BPE on the two datasets.\nNote that starting from BPE-k and running the algorithm for k more iterations,\nthe vocabulary size increases by approximately k elements – showing that BPE\novercomes the issue of exponential growth of vocabulary size in n-gram.\nFrequent Activities. The three BPE iterations learnt the same top-10 fre-\nquent keyboard action sequences as shown in Table 2. Eight out of ten action\nsequences are the same on the two datasets, although they were collected from\ndiﬀerent participants in diﬀerent experimental settings, indicating that gener-\nalised patterns underlie keyboard behaviour. The interaction goal behind the\nmost frequent activity is to press the spacebar, which is in line with the obser-\nvation that spaces occur often when typing in various languages. The second\nfrequent activity reﬂects an intention of pressing Backspace which is frequently\nand widely used to correct what has been typed. Most frequent activities cor-\nrespond to character keystrokes, and reﬂect the top-7 most frequent English\nletters: “e” (12.15%),“a” (8.67%), “t” (8.60%), “i” (7.53%), “o” (7.38%), “n”\n(7.34%) and “s” (6.63%) [20]. The diﬀerence in their order may be due to that\nthe datasets are limited to speciﬁc typing scenarios and not representative of\nthe entire English language. We also noticed that the left and right arrows, for\nredirecting typing locations, were also frequent on both datasets.\nThe most frequent ten mouse action sequences learnt by BPE were also the\nsame on the two datasets. All of them are mouse moves of pinpoint, implying\nthat participants follow similar ways to interact with UI targets even in diﬀer-\nent tasks and settings. These pinpointing regions were primarily in the top-left\nand bottom-left areas, while fewer pinpoints fell on the right side. This matches\n12\nG. Zhang et al.\nthe layouts of not only general GUIs but also those used in our user study. For\nexample, menu bars and sidebars are commonly at the top and to the left of\ninteractive windows, respectively. Also, our text formatting tools were at the\ntop of the text editor. The image editing tools were in the leftmost of the im-\nage editor. Additionally, our questionnaires were left-aligned, so the choices for\nparticipants to click lay to the left.\nInteraction Goals behind Activities. We also analysed long activities to ex-\namine if BPE learnt a hierarchy, i.e., if atomic actions form meaningful activities\ndriven by complex goals. An example is “Dot↓, Dot↑, Space↓, Space↑, Shift↓,\ni↓, i↑, Shift↑, Space↓, Space↑”. The goal behind the whole sequence is to start\na sentence with the word “I”, in line with the common texting or typing sce-\nnario of writing about oneself. It consisted of the low-level goal of pressing each\naforementioned key, which was further composed of atomic actions KeyDown\nand KeyUp. BPE also learnt “Space↓, Space↑, Backspace↓, Backspace↑” from\nEMAKI, suggesting that participants typed at a faster pace than their thought\nprocess. Another example is “Ctrl↓, s↓, s↑, Ctrl↑” from Buﬀalo, representing the\nshortcut for saving ﬁles. Looking at mouse behaviour, BPE captured drag be-\nhaviour, represented as a MouseDown action followed by multiple MouseMove\nactions and ending with a MouseUp action. Another learnt long activity had 37\nactions with 35 moves and a click as pinpoint in area 0, reﬂecting the goal of\nadjusting the cursor to a target and then clicking.\n5.2\nInteractive Task Recognition\nWe also evaluated the practical eﬀectiveness of our approach on interactive task\nrecognition. Knowing which task a user is performing enables adaptive UIs to\nunderstand the interactive behaviour and goals [26,27]. We compared our ap-\nproach with two baselines: an ablated version which bypasses encoding (noted\nas NoEncoding) and replacing BPE with an autoencoder (AE). Autoencoder,\nconsisting of an encoder and a decoder, is trained in a self-supervised way to re-\nconstruct the input with the lowest error. Therefore, it needs no annotations and\nhas a high generalisability, also used on language data [37]. To control variables,\ni.e., restrict the comparison to the encoding, we set two rules: (1) to reduce the\nimpact of sophisticated designs of the encoders, use vanilla AE and BPE; (2)\nuse the same hyperparameter sets for the classiﬁer.\nWe implemented an AE that includes four components: an embedding layer of\ndimension de = 128 to handle discrete tokens; an encoder component composed\nof one to three fully connected (FC) layers with hidden dimensions (64), (64, 32)\nand (64, 32, 16); a decoder component, which is symmetric to the encoder; and a\nreconstruction component consisting of an FC layer and a softmax layer. Dropout\nwas added after FC layers to avoid overﬁtting. We denote the autoencoder that\nhas one, two, or three FC layers in the encoder and decoder components as AE-\n1, AE-2 and AE-3. Cross entropy between the reconstructed sequences and the\ninput was used as the loss function. After training, the encoder component was\nused to encode interactive behaviour.\nExploring NLP Methods for Interactive Behaviour Modelling\n13\nOur task classiﬁer is based on a Transformer [60], which is well known for its\nsuccess in NLP and capability to handle long dependencies in temporal signals.\nThe classiﬁer is composed of N = {2, 4, 6} Transformer encoder layers, then\nan FC and softmax layer. Each Transformer encoder layer had h = 4 attention\nheads, dmodel = {16, 64} expected features, dﬀ= 4dmodel dimension in feedfor-\nward layers and uses the ReLU activation function. During training, we applied\nlabel smoothing with ϵ = 0.1 [60]. We used AdamW optimizer with learning\nrate lr = {10−3, 10−4} and β = (0.9, 0.999) [10] and the cross entropy as loss\nfunction. The training was done on a Tesla V100 GPU with a batch size of 64\nand a dropout rate of 0.5. The classiﬁer was trained for 30 epochs, while the AE\nwas trained for 10 epochs because of its faster convergence. Because activities in\nthe ﬂexible vocabulary learnt by BPE have diﬀerent lengths, we padded short\nsamples and applied padding masks.\nEMAKI has three main interactive tasks, posing a three-class classiﬁcation\nproblem, while Buﬀalo has two tasks, posing a binary classiﬁcation problem. The\nevaluation follows 5-fold participant-independent cross-validation, where data\nfrom 80% of participants form the training set and the remaining participants\nform the test set. This scheme can evaluate the performance of unseen users.\nMacro F1 score [24] was chosen as evaluation metric because of the imbalanced\nclasses, e.g., most keyboard data were from the text task on EMAKI. For each\nmodel, we report the highest F1 score achieved among all the parameter sets.\nResults show that on both datasets methods using BPE encoding outperformed\nthe others (see Fig. 6 and 7).\nResults on EMAKI On mouse data, BPE-300 consistently outperformed\nother methods (Fig. 6a). A one-way ANOVA test showed that diﬀerences be-\ntween methods are signiﬁcant (p<.001): F=9.697 on Lwin=200, F=12.396 on\nLwin=100 and F=7.194 on Lwin=20. A post-hoc Tukey HSD test further con-\nﬁrmed that BPE-300 signiﬁcantly outperformed the other methods on Lwin=200,\nLwin=100 (p<.001 for AE and p<.05 for NoEncoding) and Lwin=20 (p<.01 for\nboth AE and NoEncoding). Fig. 6b shows that BPE-600 achieved the best re-\nsults for Lwin=100 and Lwin=50, whereas when Lwin=10 the best was BPE-300.\nDiﬀerences between methods are signiﬁcant (F=13.044, p<.001 for Lwin=100,\nF=4.620, p<.01 for Lwin=50 and F=4.220, p<.01 for Lwin=10). Post-hoc Tukey\nHSD tests conﬁrmed that BPE-600 signiﬁcantly outperformed NoEncoding (p<.01)\nand AE-1 (p<.001) for Lwin=100. On joint modalities, BPE-300 performed the\nbest, with the highest F1 score of 0.693 (Fig. 6c). Diﬀerences between methods\nwere again signiﬁcant with F=13.996, p<.001 on Lwin=150, F=5.678, p<.001\non Lwin=75 and F=2.665, p<.05 on Lwin=15. Tukey HSD test indicated that\nBPE-300 signiﬁcantly outperformed AE (p<.01) and NoEncoding (p<.05) on\nLwin=150 and both of them at p<.05 when Lwin=75.\nIn Section 3.2, we report that participants using touchpads and traditional\nmice show diﬀerent proﬁciencies. Therefore, we analysed if such diﬀerences af-\nfected task recognition. We separately performed 5-fold cross-validation based\non the two groups. Since 24 participants used traditional mice while only 15\nused touchpads, we randomly selected 15 traditional mouse users to reduce the\n14\nG. Zhang et al.\n20\n100\n200\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 Score\n0.487\n0.561\n0.595\n0.491\n0.513\n0.550\n0.487\n0.538\n0.547\n0.480\n0.519\n0.558\n0.539\n0.604\n0.656\n0.527\n0.584\n0.628\n0.514\n0.560\n0.597\n  (a) Mouse  \n10\n50\n100\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 Score\n0.484\n0.549\n0.571\n0.483\n0.528\n0.534\n0.444\n0.502\n0.498\n0.426\n0.491\n0.524\n0.523\n0.620\n0.692\n0.521\n0.639\n0.703\n0.488\n0.606\n0.643\n  (b) Keyboard  \n15\n75\n150\nWindow Length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 Score\n0.584\n0.606\n0.634\n0.573\n0.593\n0.572\n0.586\n0.595\n0.609\n0.578\n0.598\n0.609\n0.608\n0.662\n0.693\n0.607\n0.645\n0.690\n0.607\n0.643\n0.689\n  (c) Both  \nNoEncoding\nAE-1\nAE-2\nAE-3\nBPE-300\nBPE-600\nBPE-900\nFig. 6. F1 scores of recognising three interactive tasks on EMAKI from (a) mouse,\n(b) keyboard and (c) both modalities, segmented by diﬀerent windows. Error bars\nrepresent the standard deviation from a 5-fold cross-validation.\ninﬂuence of data amount on performance. Because BPE-300 on the longest win-\ndow achieved the best results on mouse data (Fig. 6a), we used the same setting\nand did a Mann-Whitney U test on F1 scores achieved from two groups. To\nmitigate the randomisation introduced by participant selection, we repeated the\nabove procedure ﬁve times. None of the ﬁve tests found a signiﬁcant diﬀerence\nin performance. The reason may be that our method does not explicitly encode\ntime information, thus ignoring the speed diﬀerence in moving the cursor [23].\nResults on Buﬀalo On mouse data (Fig. 7a), BPE-300 performed the best\nand got the highest F1 score of 0.547. One-way ANOVA showed that diﬀer-\nences between methods were signiﬁcant (p<.001) with F=20.345 for Lwin=200,\nF=18.609 for Lwin=100 and F=5.589 for Lwin=20). Post-hoc Tukey HSD tests\nshowed that BPE-300 signiﬁcantly outperformed NoEncoding (p<.05) and AE-1\n(p<.001) when Lwin=200. On keyboard data (Fig. 7b), BPE-900 and BPE-600\noutperformed other methods. Diﬀerences between methods are signiﬁcant with\nF=30.218 for Lwin=100, F=5.884 for Lwin=50 (both p<.001) and F=4.791,\np<.01 for Lwin=10. According to post-hoc Tukey HSD tests, BPE-900 signiﬁ-\nExploring NLP Methods for Interactive Behaviour Modelling\n15\n20\n100\n200\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 Score\n0.485\n0.519\n0.505\n0.422\n0.473\n0.503\n0.398\n0.451\n0.474\n0.416\n0.461\n0.459\n0.522\n0.545\n0.547\n0.522\n0.538\n0.543\n0.522\n0.538\n0.547\n  (a) Mouse  \n10\n50\n100\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 Score\n0.671\n0.719\n0.791\n0.653\n0.731\n0.776\n0.578\n0.658\n0.677\n0.527\n0.609\n0.659\n0.678\n0.804\n0.840\n0.679\n0.808\n0.856\n0.676\n0.805\n0.865\n  (b) Keyboard  \n15\n75\n150\nWindow Length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 Score\n0.621\n0.608\n0.611\n0.567\n0.578\n0.590\n0.575\n0.572\n0.597\n0.583\n0.569\n0.577\n0.681\n0.696\n0.699\n0.683\n0.678\n0.701\n0.683\n0.679\n0.700\n  (c) Both  \nNoEncoding\nAE-1\nAE-2\nAE-3\nBPE-300\nBPE-600\nBPE-900\nFig. 7. F1 scores of recognising two interactive tasks on Buﬀalo from (a) mouse, (b)\nkeyboard and (c) both modalities, segmented by diﬀerent windows. Error bars represent\nthe standard deviation from a 5-fold cross-validation.\ncantly outperformed AE-1 (p<.01) and NoEncoding (p<.05) when Lwin=100,\nand BPE-600 signiﬁcantly outperformed AE-1 (p<.001) when Lwin=50. On joint\nmodalities (Fig. 7c), BPE resulted in similar yet higher F1 scores than baselines.\nThe best result was achieved by BPE-600 on the longest window of 0.701. Diﬀer-\nences between methods were again signiﬁcant (p<.001): F=10.733 for Lwin=150;\nF=11.151 for Lwin=75; and F=7.397 for Lwin=15. Tukey HSD test showed that\nBPE-600 signiﬁcantly outperformed AE-2 (p<.01) and NoEncoding (p<.05) on\nLwin=150; BPE-300 outperformed AE-1 (p<.01) and NoEncoding (p<.05) on\nLwin=75; and BPE-600 outperformed AE-3 (p<.05) on Lwin=15.\nIt is noticeable that results obtained from Buﬀalo mouse data slightly ex-\nceeded the chance level and were much worse than those from keyboard data. A\npossible reason is that the mouse behaviour on the Buﬀalo dataset was similar\nacross diﬀerent tasks. To verify this, we calculated the average distances between\nmouse trajectories in diﬀerent interactive tasks, following [64]: (1) all the mouse\nactions generated in one trial by one participant were considered one trajectory,\non which 101 points were sampled uniformly; (2) the distance between two trials\nwas deﬁned as the average Euclidean distance between each pair of points on\n16\nG. Zhang et al.\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.00\n0.05\n0.10\nProbability\nBuffalo\nEMAKI\nFig. 8. Distribution of the average Euclidean distances between mouse trajectories from\ndiﬀerent interactive tasks on the two datasets. Smaller distances mean that trajectories\nfrom diﬀerent tasks are more similar.\ntwo trajectories; (3) the distance between two tasks was computed as the aver-\nage distance between each trial from task 1 and each from task 2. Fig. 8 shows\nthat the distance between tasks from Buﬀalo is smaller than EMAKI, suggest-\ning that mouse behaviour generated from the two tasks from Buﬀalo is similar,\nconsistent with the statistics of BPE vocabulary (Section 5.1). Therefore, it is\nmore diﬃcult to classify tasks based on Buﬀalo mouse data.\n6\nDiscussion\n6.1\nModelling Interactive Behaviour from a Natural Language\nPerspective\nOur work is among the ﬁrst to explore the similarity between interactive be-\nhaviour and natural language, given that both have a sequential and hierarchical\nstructure. Towards this goal, we applied BPE, which has been commonly used in\nstate-of-the-art large language models to encode mouse and keyboard behaviour.\nAt the lowest level, input actions were considered as characters since they are\natomic and inseparable. For higher levels, BPE learned “subwords” from interac-\ntive behaviour, which were interactive activities, i.e., action sequences driven by\nunderlying interaction goals. The analysis of the learnt vocabulary showed that\nfollowing the same way of learning the semantic hierarchy of language, BPE was\nable to capture meaningful activities such as mouse drags, keyboard shortcuts\nand precisely adjusting the mouse to click on a UI element (Section 5.1). Despite\nrepresenting just a ﬁrst exploration, the insights from our analysis underline the\nsimilarity between interactive behaviour and natural language, and indicate the\npossibility of applying more powerful NLP methods like BERT [32,39] to encode\ninteractive behaviour. Besides the state-of-the-art performances achieved, such\nLLMs also have noticeable advantages of generalisability and reusability. They\ncan be pretrained on one dataset and re-used to encode other datasets to solve\nvarious downstream tasks with ﬁne-tuning, which is more cost-eﬀective than\ndedicating a speciﬁc large model towards each dataset or task [58]. Future HCI\nresearch can follow such NLP methods to build reusable pretrained interactive\nbehaviour models for better generalisability and cost-eﬀectiveness.\nExploring NLP Methods for Interactive Behaviour Modelling\n17\n6.2\nNLP Encoding for Interactive Task Recognition\nInteractive task recognition is one of the key requirements of intelligent interac-\ntive systems to understand and adapt to interactive behaviour and interaction\ngoals [44,17,18,33]. On this recognition task, encoding with BPE signiﬁcantly\noutperformed baselines on both datasets, all the modalities and windows. Speciﬁ-\ncally, on our out-of-the-lab, newly collected EMAKI dataset, encoding with BPE\nobtained the highest F1 score of 0.703 recognising three tasks (Fig. 6). On aver-\nage, BPE improved the F1 score by 0.087 on keyboard data, 0.051 on mouse, and\n0.044 on the joint modalities. On the Buﬀalo dataset, BPE achieved the highest\nF1 score of 0.865 (Fig. 7) recognising two tasks. On average, BPE improved\nthe F1 score by 0.080 on joint modalities, 0.053 on keyboard data and 0.035\non mouse data. These results, from a practical perspective, further reveal the\npromising eﬀectiveness of modelling interactive behaviour as natural language.\nWe observed that methods generally achieved better results on longer win-\ndows, which may be due to that more actions may uncover richer characteristics\nof the tasks. However, increasing the window size yields fewer training samples\nand makes the recognition model wait longer for a complete window of actions\nto provide a prediction. In our experiments, windows that led to the best perfor-\nmance on mouse, keyboard and joint modalities had 200, 100 and 150 actions,\nrespectively. These values can be a reference for future mouse and keyboard\nbehaviour modelling methods.\nIn addition, on both datasets, using BPE on keyboard behaviour improved\nthe F1 score more than on mouse behaviour, indicating its better ability of han-\ndling keyboard than mouse behaviour. This ﬁnding is expected, as typing on\na keyboard is directly linked to expressing natural language. A second reason\nmight be that discretising mouse data caused a loss of information [67,68]. On\nthe joint modalities, we observed a general performance improvement from in-\ndividual modalities on EMAKI, but not on Buﬀalo. As shown in Section 5.2,\nBuﬀalo lacks the diversity in mouse behaviour and thus performance achieved\nby combining mouse and keyboard is in-between that of individual modality.\n6.3\nEMAKI Dataset\nMost publicly available mouse and keyboard datasets were collected in con-\nstrained laboratory settings, such as the Buﬀalo dataset. In contrast, our EMAKI\nis a step towards fully unconstrained settings to allow more natural interactive\nbehaviour. Our study did not control where, when, or how long participants\njoined the study. In addition, participants used their own devices, which con-\ntributes to ecological validity. Consequently, our participant pool is more diverse\ngiven that participants are from diﬀerent countries, and used diﬀerent input de-\nvices and screen resolutions. All of Buﬀalo’s participants were university students\nbetween 20-30 years old, while ours were between 18-54 and covered non-student\nparticipants. Moreover, our participants spent various time on tasks as they were\nfreer to pause and resume (as shown in Section 3.1 and 3.2). Buﬀalo primarily\nuses typing-focused tasks, while EMAKI has complementary characteristics and\n18\nG. Zhang et al.\ntasks – like image editing and questionnaires – encouraging diverse mouse be-\nhaviour, as conﬁrmed by our analysis in Section 5.1. Furthermore, higher diver-\nsity in behaviour can lead to better task recognition performance (Section 5.2).\nWe also veriﬁed that the amount of data in EMAKI is suﬃcient for training\nthe method for task recognition (see Appendix). Besides serving as a benchmark\nfor task recognition, the questionnaires included in EMAKI also encourage fu-\nture research on the interplay between multimodal behaviour and personality\ntraits [71].\n6.4\nLimitations and Future Work\nOur user study covered diverse but predeﬁned tasks and did not allow multi-\ntasking. In the future, we will move towards fully uncontrolled settings. Time\ninformation may further improve behaviour modelling [6,16] and will be explic-\nitly encoded in future work. We chose BPE over N-gram due to its ﬂexibility,\nyet for systems where activities have similar lengths, N-gram might be eﬃcient\nenough. An interesting future work is to explore the boundary of where the\nmethods lead over the other. Also, even used on both modalities jointly, BPE\nlearned activities composed of single modalities. A possible reason is that the\nbehaviour of switching between mouse and keyboard is diverse, which BPE could\nnot capture. Future work can explore the use of other NLP methods to better\nlearn the interplay between mouse and keyboard behaviour [39,48,47]. Automatic\ninterpreters can be studied to identify meaningful and interesting insights into\nbehaviour from the BPE vocabulary, instead of human interpretation. Moreover,\nwe intend to study other interactive modalities, such as screen touch and mid-air\ngesture, as well as other HCI downstream tasks like personality recognition.\n7\nConclusion\nWe explored the similarity between interactive behaviour and natural language,\ngiven that both of them have a sequential and hierarchical structure. Towards\nthe goal, we applied a widely used NLP method, BPE, to encode mouse and key-\nboard behaviour by learning its subwords, i.e., activities. Results on an existing\ncontrolled dataset and a novel out-of-the-lab dataset showed that the method\ncan capture meaningful activities. Moreover, encoding with BPE signiﬁcantly\nimproved interactive task recognition, which is commonly required in intelligent\ninteractive systems. Taken together, our exploratory work links interactive be-\nhaviour with natural language and provides a promising NLP perspective for\nmodelling interactive behaviour, which has the potential to improve the gener-\nalisability of computational interactive behaviour models (Section 6.1) and also\nperformances of interactive behaviour-based HCI tasks.\nExploring NLP Methods for Interactive Behaviour Modelling\n19\nReferences\n1. Accot, J., Zhai, S.: Beyond ﬁtts’ law: models for trajectory-based hci tasks. In:\nProceedings of the ACM SIGCHI Conference on Human factors in computing\nsystems. pp. 295–302 (1997)\n2. Acien, A., Morales, A., Vera-Rodriguez, R., Fierrez, J., Monaco, J.V.: Typenet:\nScaling up keystroke biometrics. In: Proceedings of the 2020 IEEE International\nJoint Conference on Biometrics. pp. 1–7. IEEE (2020)\n3. Antal, M., Fej´er, N., Buza, K.: Sapimouse: Mouse dynamics-based user authenti-\ncation using deep feature learning. In: Proceedings of the 2021 IEEE International\nSymposium on Applied Computational Intelligence and Informatics. pp. 61–66.\nIEEE (2021)\n4. Apaolaza, A., Harper, S., Jay, C.: Understanding users in the wild. In: Proceedings\nof the 10th international cross-disciplinary conference on web accessibility. pp. 1–4\n(2013)\n5. Arapakis, I., Leiva, L.A.: Learning Eﬃcient Representations of Mouse Movements\nto Predict User Attention, p. 1309–1318. Association for Computing Machinery,\nNew York, NY, USA (2020)\n6. Azcarraga, J.J., Iba˜nez, J.F., Lim, I.R., Lumanas Jr, N.: Use of personality proﬁle\nin predicting academic emotion based on brainwaves signals and mouse behavior.\nIn: 2011 Third International Conference on Knowledge and Systems Engineering.\npp. 239–244. IEEE (2011)\n7. Bi, X., Smith, B.A., Zhai, S.: Multilingual touchscreen keyboard design and opti-\nmization. Human–Computer Interaction 27(4), 352–382 (2012)\n8. Borji, A., Sihite, D.N., Itti, L.: Probabilistic learning of task-speciﬁc visual at-\ntention. In: Proceedings of the 2012 IEEE Conference on Computer Vision and\nPattern Recognition. pp. 470–477. IEEE (2012)\n9. Brown, E.T., Ottley, A., Zhao, H., Lin, Q., Souvenir, R., Endert, A., Chang, R.:\nFinding waldo: Learning about users from their interactions. IEEE Transactions\non visualization and computer graphics 20(12), 1663–1672 (2014)\n10. Br¨uckner, L., Leiva, L.A., Oulasvirta, A.: Learning gui completions with user-\ndeﬁned constraints. ACM Transactions on Interactive Intelligent Systems (TiiS)\n12(1), 1–40 (2022)\n11. Bulling, A., Ward, J.A., Gellersen, H., Tr¨oster, G.: Robust Recognition of Reading\nActivity in Transit Using Wearable Electrooculography. In: Proc. International\nConference on Pervasive Computing (Pervasive). pp. 19–37 (2008)\n12. Card, S.K., Moran, T.P., Newell, A.: The keystroke-level model for user perfor-\nmance time with interactive systems. Communications of the ACM 23(7), 396–410\n(1980)\n13. Chud´a, D., Kr´atky, P.: Usage of computer mouse characteristics for identiﬁcation in\nweb browsing. In: Proceedings of the 15th International Conference on Computer\nSystems and Technologies. pp. 218–225 (2014)\n14. Dhakal, V., Feit, A.M., Kristensson, P.O., Oulasvirta, A.: Observations on typing\nfrom 136 million keystrokes. In: Proceedings of the 2018 CHI Conference on Hu-\nman Factors in Computing Systems. p. 1–12. CHI ’18, Association for Computing\nMachinery, New York, NY, USA (2018). https://doi.org/10.1145/3173574.3174220\n15. Fitts, P.M.: The information capacity of the human motor system in controlling\nthe amplitude of movement. Journal of experimental psychology 47(6), 381 (1954)\n16. Freihaut, P., G¨oritz, A.S.: Does peoples’ keyboard typing reﬂect their stress level?\nan exploratory study. Zeitschrift f¨ur Psychologie 229(4), 245 (2021)\n20\nG. Zhang et al.\n17. Fu, E.Y., Kwok, T.C., Wu, E.Y., Leong, H.V., Ngai, G., Chan, S.C.: Your mouse\nreveals your next activity: towards predicting user intention from mouse interac-\ntion. In: 2017 IEEE 41st Annual Computer Software and Applications Conference\n(COMPSAC). vol. 1, pp. 869–874. IEEE (2017)\n18. Gajos, K., Weld, D.S.: Supple: automatically generating user interfaces. In: Pro-\nceedings of the 9th international conference on Intelligent user interfaces. pp. 93–\n100 (2004)\n19. Grabowski, J.: The internal structure of university student’s keyboard skills. Jour-\nnal of writing research 1(1), 27–52 (2008)\n20. Grigas, G., Juˇskeviˇcien˙e, A.: Letter frequency analysis of languages using latin\nalphabet. International Linguistics Research 1(1), p18–p18 (2018)\n21. Han, L., Checco, A., Difallah, D., Demartini, G., Sadiq, S.: Modelling user be-\nhavior dynamics with embeddings. In: Proceedings of the 29th ACM International\nConference on Information & Knowledge Management. pp. 445–454 (2020)\n22. Heinzerling, B., Strube, M.: Bpemb: Tokenization-free pre-trained subword embed-\ndings in 275 languages. arXiv preprint arXiv:1710.02187 (2017)\n23. Hertzum, M., Hornbæk, K.: The eﬀect of target precuing on pointing with mouse\nand touchpad. International Journal of Human-Computer Interaction 29(5), 338–\n350 (2013)\n24. Hoppe, S., Loetscher, T., Morey, S.A., Bulling, A.: Eye movements during everyday\nbehavior predict personality traits. Frontiers in human neuroscience p. 105 (2018)\n25. Hu, Z., Bulling, A., Li, S., Wang, G.: Fixationnet: Forecasting eye ﬁxations in task-\noriented virtual environments. IEEE Transactions on Visualization and Computer\nGraphics 27(5), 2681–2690 (2021)\n26. Hu, Z., Bulling, A., Li, S., Wang, G.: Ehtask: recognizing user tasks from eye and\nhead movements in immersive virtual reality. IEEE Transactions on Visualization\nand Computer Graphics (2022)\n27. Hu,\nZ.,\nLi,\nS.,\nGai,\nM.:\nResearch\nprogress\nof\nuser\ntask\npredic-\ntion\nand\nalgorithm\nanalysis.\nJournal\nof\nGraphics\n42(3),\n367–375\n(2021).\nhttps://doi.org/http://www.txxb.com.cn/CN/10.11996/JG.j.2095-\n302X.2021030367\n28. Hu, Z., Li, S., Zhang, C., Yi, K., Wang, G., Manocha, D.: Dgaze: Cnn-based gaze\nprediction in dynamic scenes. IEEE Transactions on Visualization and Computer\nGraphics 26(5), 1902–1911 (2020)\n29. Hu, Z., Zhang, C., Li, S., Wang, G., Manocha, D.: Sgaze: a data-driven eye-head co-\nordination model for realtime gaze prediction. IEEE Transactions on Visualization\nand Computer Graphics 25(5), 2002–2010 (2019)\n30. Inoue, H., Hirayama, T., Doman, K., Kawanishi, Y., Ide, I., Deguchi, D., Murase,\nH.: A classiﬁcation method of cooking operations based on eye movement patterns.\nIn: Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research\n& Applications. pp. 205–208 (2016)\n31. Jansen, B.J., Jung, S.G., Robillos, D.R., Salminen, J.: Next likely behavior: Pre-\ndicting individual actions from aggregate user behaviors. In: 2021 Second Inter-\nnational Conference on Intelligent Data Science Technologies and Applications\n(IDSTA). pp. 11–15. IEEE (2021)\n32. Jawahar, G., Sagot, B., Seddah, D.: What does bert learn about the structure of\nlanguage? In: ACL 2019-57th Annual Meeting of the Association for Computa-\ntional Linguistics (2019)\n33. Joachims, T.: Optimizing search engines using clickthrough data. In: Proceedings\nof the eighth ACM SIGKDD international conference on Knowledge discovery and\ndata mining. pp. 133–142 (2002)\nExploring NLP Methods for Interactive Behaviour Modelling\n21\n34. Kim, Y., Jernite, Y., Sontag, D., Rush, A.M.: Character-aware neural language\nmodels. In: Thirtieth AAAI conference on artiﬁcial intelligence (2016)\n35. Kudo, T.: Subword regularization: Improving neural network translation models\nwith multiple subword candidates. arXiv preprint arXiv:1804.10959 (2018)\n36. Kunchukuttan, A., Bhattacharyya, P.: Learning variable length units for smt be-\ntween related languages via byte pair encoding. arXiv preprint arXiv:1610.06510\n(2016)\n37. Li, J., Luong, M.T., Jurafsky, D.: A hierarchical neural autoencoder for paragraphs\nand documents. arXiv preprint arXiv:1506.01057 (2015)\n38. Li, T.J.J., Popowski, L., Mitchell, T.M., Myers, B.A.: Screen2vec: Semantic embed-\nding of gui screens and gui components. Proceedings of the 2021 CHI Conference\non Human Factors in Computing Systems (2021)\n39. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692 (2019)\n40. Mazilu, S., Blanke, U., Dorfman, M., Gazit, E., Mirelman, A., M. Hausdorﬀ, J.,\nTr¨oster, G.: A wearable assistant for gait training for parkinson’s disease with\nfreezing of gait in out-of-the-lab environments. ACM Transactions on Interactive\nIntelligent Systems (TiiS) 5(1), 1–31 (2015)\n41. Motwani, A., Jain, R., Sondhi, J.: A multimodal behavioral biometric technique\nfor user identiﬁcation using mouse and keystroke dynamics. International Journal\nof Computer Applications 111(8), 15–20 (2015)\n42. M¨uller, P., Huang, M.X., Bulling, A.: Detecting low rapport during natural interac-\ntions in small groups from non-verbal behaviour. In: 23rd International Conference\non Intelligent User Interfaces. pp. 153–164 (2018)\n43. Murphy, C., Huang, J., Hou, D., Schuckers, S.: Shared dataset on natural human-\ncomputer interaction to support continuous authentication research. In: 2017 IEEE\nInternational Joint Conference on Biometrics (IJCB). pp. 525–530. IEEE (2017)\n44. Pasqual, P.T., Wobbrock, J.O.: Mouse pointing endpoint prediction using kine-\nmatic template matching. In: Proceedings of the SIGCHI Conference on Human\nFactors in Computing Systems. pp. 743–752 (2014)\n45. Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word repre-\nsentation. In: EMNLP (2014)\n46. Petersen, G.B., Mottelson, A., Makransky, G.: Pedagogical agents in educational\nvr: An in the wild study. In: Proceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems. pp. 1–12 (2021)\n47. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language\nmodels are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)\n48. Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y.,\nLi, W., Liu, P.J., et al.: Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. J. Mach. Learn. Res. 21(140), 1–67 (2020)\n49. Reani, M., Peek, N., Jay, C.: An investigation of the eﬀects of n-gram length\nin scanpath analysis for eye-tracking research. In: Proceedings of the 2018 acm\nsymposium on eye tracking research & applications. pp. 1–8 (2018)\n50. Reimers, N., Gurevych, I.: Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. ArXiv abs/1908.10084 (2019)\n51. Salmeron-Majadas, S., Santos, O.C., Boticario, J.G.: An evaluation of mouse and\nkeyboard interaction indicators towards non-intrusive and low cost aﬀective mod-\neling in an educational context. Procedia Computer Science 35, 691–700 (2014)\n22\nG. Zhang et al.\n52. Salvucci, D.D., Goldberg, J.H.: Identifying ﬁxations and saccades in eye-tracking\nprotocols. In: Proceedings of the 2000 symposium on Eye tracking research &\napplications. pp. 71–78 (2000)\n53. Schuster, M., Nakajima, K.: Japanese and korean voice search. In: 2012 IEEE\ninternational conference on acoustics, speech and signal processing (ICASSP). pp.\n5149–5152. IEEE (2012)\n54. Sennrich, R., Haddow, B., Birch, A.: Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909 (2015)\n55. Shirahama, K., Grzegorzek, M.: On the generality of codebook approach for sensor-\nbased human activity recognition. Electronics 6(2), 44 (2017)\n56. Soukoreﬀ, R.W., MacKenzie, I.S.: Towards a standard for pointing device evalua-\ntion, perspectives on 27 years of ﬁtts’ law research in hci. International Journal of\nHuman-Computer Studies 61(6), 751–789 (2004)\n57. Subba, B., Biswas, S., Karmakar, S.: Host based intrusion detection system us-\ning frequency analysis of n-gram terms. In: TENCON 2017-2017 IEEE Region 10\nConference. pp. 2006–2011. IEEE (2017)\n58. Sun, C., Qiu, X., Xu, Y., Huang, X.: How to ﬁne-tune bert for text classiﬁcation?\nIn: China national conference on Chinese computational linguistics. pp. 194–206.\nSpringer (2019)\n59. Sun, Y., Ceker, H., Upadhyaya, S.: Shared keystroke dataset for continuous au-\nthentication. In: 2016 IEEE International Workshop on Information Forensics and\nSecurity (WIFS). pp. 1–6. IEEE (2016)\n60. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is All you Need. In: Guyon, I., Luxburg, U.V., Bengio,\nS., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in\nNeural Information Processing Systems. vol. 30. Curran Associates, Inc. (2017)\n61. Wang, B., Li, G., Zhou, X., Chen, Z., Grossman, T., Li, Y.: Screen2words: Auto-\nmatic mobile ui summarization with multimodal learning. The 34th Annual ACM\nSymposium on User Interface Software and Technology (2021)\n62. Wang, C., Cho, K., Gu, J.: Neural machine translation with byte-level subwords.\nIn: AAAI (2020)\n63. Wang, Z., Li, B.: Human activity encoding and recognition using low-level visual\nfeatures. In: Twenty-First International Joint Conference on Artiﬁcial Intelligence.\nCiteseer (2009)\n64. Wulﬀ, D.U., Haslbeck, J.M., Kieslich, P.J., Henninger, F., Schulte-Mecklenbeck,\nM.: Mouse-tracking: Detecting types in movement trajectories. In: A Handbook of\nprocess tracing methods, pp. 131–145. Routledge (2019)\n65. Xiaofeng, L., Shengfei, Z., Shengwei, Y.: Continuous authentication by free-text\nkeystroke based on cnn plus rnn. Procedia computer science 147, 314–318 (2019)\n66. Xu, P., Sugano, Y., Bulling, A.: Spatio-temporal modeling and prediction of visual\nattention in graphical user interfaces. In: Proceedings of the 2016 CHI Conference\non Human Factors in Computing Systems. pp. 3299–3310 (2016)\n67. Yue, Z., Wang, Y., Duan, J., Yang, T., Huang, C., Tong, Y., Xu, B.: Ts2vec:\nTowards universal representation of time series. In: Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence. vol. 36, pp. 8980–8987 (2022)\n68. Zerveas,\nG.,\nJayaraman,\nS.,\nPatel,\nD.,\nBhamidipaty,\nA.,\nEickhoﬀ,\nC.:\nA\ntransformer-based framework for multivariate time series representation learning.\nIn: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery\n& Data Mining. pp. 2114–2124 (2021)\nExploring NLP Methods for Interactive Behaviour Modelling\n23\nAlgorithm 1 Byte pair encoding (BPE) [69,22]\nInput: action sequence set D, the number of iterations k\nprocedure BPE(D, k)\nV ←all unique actions in D\nfor i ←1 to k do\ntL, tR ←Most frequent two consecutive units (actions or activities) in D\ntnew ←tL + tR\n▷Merge to form a new activity\nV ←V + [tnew]\nReplace each occurrence of tL, tR with tnew in D\nend for\nreturn V\nend procedure\n69. Zhan, J., Liao, X., Bao, Y., Gan, L., Tan, Z., Zhang, M., He, R., Lu, J.: An eﬀective\nfeature representation of web log data by leveraging byte pair encoding and tf-idf.\nIn: Proceedings of the ACM Turing Celebration Conference-China. pp. 1–6 (2019)\n70. Zhang, G., Hindennach, S., Leusmann, J., B¨uhler, F., Steuerlein, B., Mayer, S.,\nBˆace, M., Bulling, A.: Predicting next actions and latent intents during text for-\nmatting. In: Proceedings of the CHI Workshop Computational Approaches for\nUnderstanding, Generating, and Adapting User Interfaces. pp. 1–6 (2022)\n71. Zhao, Y., Miao, D., Cai, Z.: Reading personality preferences from motion patterns\nin computer mouse operations. IEEE Transactions on Aﬀective Computing (2020)\nA\nPreprocessing Mouse Data with I-DT\nAs written in Section 4.1, the Dispersion-threshold identiﬁcation (I-DT) algo-\nrithm [52] was used to categorise mouse behaviour to pinpointing a target (re-\nsembling gaze ﬁxations) and re-direction between targets. I-DT operates on a\nwindow of duration-threshold consecutive samples. On this window, it calculates\nthe dispersion value as Dispersion = [max(x)−min(x)]+[max(y)−min(y)]. If\nthe dispersion value exceeds the dispersion threshold, samples inside the window\nare not considered to belong to a pinpoint and the window is slid forward by one\nsample. If the value is below the threshold, the samples within the window are\nconsidered to belong to a pinpoint. The window then expands to incorporate new\nsamples until the dispersion value is above the threshold again. We empirically\nset the duration threshold to 100 ms and the dispersion threshold to 0.1.\nB\nThe Algorithm of Byte Pair Encoding\nAlgorithm 1 shows how byte pair encoding (BPE) constructs the vocabulary V ,\nas introduced in Section 4.2.\n24\nG. Zhang et al.\n1 5\n15\n25\n50\n75\n100\nPercentage of Training Data (%)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 Score\nFig. 9. F1 scores of interactive task recognition achieved by BPE, trained on diﬀerent\npercentages of the training set.\nC\nAnalysis of EMAKI Data Amount for Interactive Task\nRecognition\nAs written in Section 6.3, we evaluated if the size of EMAKI allows our data-\ndriven method to recognise interactive tasks. We used diﬀerent percentages of\nthe training set to train the method and examined their performances. According\nto Figure 6c, the best results were achieved by BPE-300 when windows have 150\nactions. Therefore, we followed the above setting. Fig. 9 shows the results of\ninteractive task recognition by training the model with 1%, 5%, 15%, 25%, 50%,\n75% of randomly selected training instances, as well as with the entire training\nset (100%). It can be seen that as the percentage increases, the F1 score ﬁrst\nincreases fast (before 25%) but then slowly (25% to 75%). The increase in F1\nscore from using 75% of training data and the entire training set was subtle\n(only 0.004). Taken together, the amount of data in our dataset is suﬃcient to\nperform interactive task recognition.\n",
  "categories": [
    "cs.HC"
  ],
  "published": "2023-03-28",
  "updated": "2023-05-11"
}