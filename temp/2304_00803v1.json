{
  "id": "http://arxiv.org/abs/2304.00803v1",
  "title": "A Tutorial Introduction to Reinforcement Learning",
  "authors": [
    "Mathukumalli Vidyasagar"
  ],
  "abstract": "In this paper, we present a brief survey of Reinforcement Learning (RL), with\nparticular emphasis on Stochastic Approximation (SA) as a unifying theme. The\nscope of the paper includes Markov Reward Processes, Markov Decision Processes,\nStochastic Approximation algorithms, and widely used algorithms such as\nTemporal Difference Learning and $Q$-learning.",
  "text": "arXiv:2304.00803v1  [cs.LG]  3 Apr 2023\nA Tutorial Introduction to Reinforcement Learning\nMathukumalli Vidyasagar ∗\nApril 4, 2023\nAbstract\nIn this paper, we present a brief survey of Reinforcement Learning (RL), with particular em-\nphasis on Stochastic Approximation (SA) as a unifying theme. The scope of the paper includes\nMarkov Reward Processes, Markov Decision Processes, Stochastic Approximation algorithms,\nand widely used algorithms such as Temporal Diﬀerence Learning and Q-learning.\n1\nIntroduction\nIn this paper, we present a brief survey of Reinforcement Learning (RL), with particular emphasis\non Stochastic Approximation (SA) as a unifying theme. The scope of the paper includes Markov\nReward Processes, Markov Decision Processes, Stochastic Approximation methods, and widely\nused algorithms such as Temporal Diﬀerence Learning and Q-learning. Reinforcement Learning is\na vast subject, and this brief survey can barely do justice to the topic. There are several excellent\ntexts on RL, such as [4, 27, 34, 33]. The dynamics of the Stochastic Approximation (SA) algorithm\nare analyzed in [25, 22, 3, 23, 2, 9, 10]. The interested reader may consult those sources for more\ninformation.\nIn this survey, we use the phrase “reinforcement learning” to refer to decision-making with un-\ncertain models, and in addition, current actions alter the future behavior of the system. Therefore,\nif the same action is taken at a future time, the consequences might not be the same. This addi-\ntional feature distinguishes RL from “mere” decision-making under uncertainty. Figure 1 rather\narbitrarily divides decision-making problems into four quadrants. Examples from each quadrant\nare now brieﬂy described.\n• Many if not most decision-making problems fall into the lower-left quadrant of “good model,\nno alteration” (meaning that the control actions do not alter the environment). An example\nis a ﬁghter aircraft which usually has an excellent model thanks to aerodynamical modelling\nand/or wind tunnel tests. In turn this permits the control system designers to formulate and\nto solve an optimal (or some other form of) control problem.\n• Controlling a chemical reactor would be an example from the lower-right quadrant. As a\ntraditional control system, it can be assumed that the environment in which the reactor\noperates does not change as a consequence of the control strategy adopted. However, due to\nthe complexity of a reactor, it is diﬃcult to obtain a very accurate model, in contrast with a\n∗SERB National Science Chair, Indian Institute of Technology Hyderabad, Kandi, Telangana 502284, India. Email:\nm.vidyasagar@iith.ac.in This research was supported by the Science and Engineering Research Board, Government\nof India.\n1\nﬁghter aircraft for example. In such a case, one can adopt one of two approaches. The ﬁrst,\nwhich is a traditional approach in control system theory, is to use a nominal model of the\nsystem and to treat the deviations from the nominal model as uncertainties in the model.\nA controller is designed based on the nominal model, and robust control theory would be\ninvoked to ensure that the controller would still perform satisfactorily (though not necessarily\noptimally) for the actual system. The second, which would move the problem from the lower\nright to the upper right quadrant, is to attempt to “learn” the unknown dynamical model by\nprobing its response to various inputs. This approach is suggested in [33, Example 3.1]. A\nsimilar statement can be made about robots, where the geometry determines the form of the\ndynamical equations describing it, but not the parameters in the equations; see for example\n[30]. In this case too, it is possible to “learn” the dynamics through experimentation. In\npractice, such an approach is far slower than the traditional control systems approach of\nusing a nominal model and designing a “robust” controller. However, “learning control” is a\npopular area in the world of machine learning. One reason is that the initial modelling error\nis too large, then robust control theory alone would not be suﬃcient to ensure the stability\nof the actual system with the designed controller. In contrast (and in principle), a “learning\ncontrol” approach can withstand larger modelling errors. The widely-used Model Predictive\nControl (MPC) paradigm can be viewed as an example of a learning-based approach.\n• A classic example of a problem belonging to the upper-left corner is a Markov Decision\nProcess (MDP), which forms the backbone of one approach to RL. In an MDP, there is a\nstate space X, and an action space U, both of which are usually assumed to be ﬁnite. In most\nMDPs, |X| ≫|U|. Board games without an element of randomness such as tic-tac-toe or\nchess would belong to the upper-left quadrant, at least in principle. Tic-tac-toe belongs here,\nbecause the rules of the game are clear, and the number of possible games is manageable. In\nprinciple, games such as chess which are “deterministic” (i.e., there is no throwing of dice as\nin Backgammon for example) would also belong here. Chess is a two-person game in which,\nfor each board position, it is possible to assign the likelihood of the three possible outcomes:\nWhite wins, Black wins, or it is a draw. However, due to the enormous number of possibilities,\nit is often not possible to determine these likelihoods precisely. It is pointed out explicitly in\n[29] that, merely because we cannot explicitly compute this likelihood function, that does not\nmean that the likelihood does not exist! However, as a practical matter, it is not a bad idea\nto treat this likelihood function as being unknown, and to infer it on the basis of experiment\n/ experience. Thus, as with chemical reactors, it is not uncommon to move chess-playing\nfrom the lower-right corner to the upper-right corner.\n• The upper-right quadrant is the focus of RL. There are many possible ways to formulate RL\nproblems, each of which leads to its own solution methodologies. A very popular approach to\nRL is to formulate it as MDPs whose dynamics are unknown. That is the approach adopted\nin this paper.\nIn an MDP, at each time t, the learner (also known as the actor or the agent) measures the\nstate Xt ∈X. Based on this measurement, the learner chooses an action Ut ∈U, and receives a\nreward R(Xt, Ut). Future rewards are discounted by a discount factor γ ∈(0, 1). The rule by which\nthe current action Ut is chosen as a function of the current state Xt is known as a policy. With\neach policy, one can associate the expected value of the total (discounted) reward over time. The\nproblem is to ﬁnd the best policy. There is a variant called POMDP (Partially Observable Markov\nDecision Process) in which the state Xt cannot be measured directly; rather, there is an output (or\n2\nModel Quality\nInteraction Between\nAction & Environment\nGood model.\nAction doesn’t alter\nEnvironment\nGood model.\nAction can alter\nEnvironment\nPoor model.\nAction doesn’t alter\nEnvironment\nPoor model.\nAction can alter\nEnvironment\nFigure 1: The four quadrants of decision-making under uncertainty\nEnvironment:Markov Process\nStates {X0, X1, · · · }\nPolicy π\nUt = π(Xt)\nReward R\nRt = R(Xt, Ut)\nXt\nUt\nXt\nUt\nFigure 2: Depiction of a Reinforcement Learning Problem\nobservation) Yt ∈Y which is a memoryless function, either deterministic or random, of Xt. These\nproblems are not studied here; it is always assumed that Xt can be measured directly. When the\nparameters of the MDP are known, there are several approaches to determining the optimal policy.\nRL is distinct from an MDP in that, in RL, the parameters of the underlying MDP are constant\nbut not known to the learner; they must be learnt on the basis of experimentation. Figure 2 depicts\nthe situation.\nThe remainder of the paper is organized as follows: In Section 2, Markov Reward Processes are\nintroduced. These are a precursor to Markov Decision Processes (MDPs), which are introduced in\nSection 3. Speciﬁcally, in Section 3.1, the relevant problems in the study of MDPs are formulated.\nIn Section 3.2, the solutions to these problems are given in terms of the Bellman value iteration,\nthe action-value function, and the F-iteration to determine the optimal action-value function. In\nSection 3.3, we study the situation where the dynamics of the MDP under study are not known\nprecisely. Instead, one has access only to a sample path {Xt} of the Markov process under study.\nFor this situation, we present two standard algorithms, known as Temporal Diﬀerence Learning,\nand Q-Learning. Starting from Section 4, the paper consists of results due to the author. In Section\n4.1, the concept of stochastic approximation (SA) is introduced, and its relevance to Reinforcement\nLearning is outlined in Section 4.2.\nIn Section 4.3, a new theorem on the global asymptotic\n3\nstability of nonlinear ODEs is stated; this theorem is of independent interest. Some theorems on\nthe convergence of the SA algorithm are presented in Sections 4.4 and 4.5. In Section 5, the results\nof Section 4 are applied to RL problems. In Section 5.1, a technical result on the sample paths of\nan irreducible Markov process is stated. Using this result, simpliﬁed conditions are given for the\nconvergence of the Temporal Diﬀerence algorithm (Section 5.2) and Q-learning (Section 5.3). A\nbrief set of concluding remarks ends the paper.\n2\nMarkov Reward Processes\nMarkov reward processes are standard (stationary) Markov processes where each state has a “re-\nward” associated with it. Markov Reward Processes are a precursor to Markov Decision Processes;\nso we review those in this section. There are several standard texts on Markov processes, one of\nwhich is [40].\nSuppose X is a ﬁnite set of cardinality n, written as {x1, . . . , xn}. If {Xt}t≥0 is a stationary\nMarkov process assuming values in X, then the corresponding state transition matrix A is deﬁned\nby\naij = Pr{Xt+1 = xj|Xt = xi}.\n(1)\nThus the i-th row of A is the conditional probability vector of Xt+1 when Xt = xi. Clearly the row\nsums of the matrix A are all equal to one. This can be expressed as A1n = 1n, where 1n denotes\nthe n-dimensional column vector whose entries all equal one. Therefore, if we deﬁne the induced\nmatrix norm ∥A∥∞→∞as\n∥A∥∞→∞:= max\nv̸=0\n∥Av∥∞\n∥v∥∞\n,\nthen ∥A∥∞→∞equals one, which also equals the spectral radius of A.\nNow suppose that there is a “reward” function R : X →R associated with each state. There\nis no consensus within the community about whether the reward corresponding to the state Xt is\npaid at time t as in [34], or time t + 1, as in [27, 33]. In this paper, it is assumed that the reward\nis paid at time t, and is denoted by Rt; the modiﬁcations required to handle the other approach\nare easy and left to the reader. The reward Rt can either be a deterministic function of Xt, or\na random function. If Rt is a deterministic function of Xt, then we have that Rt = R(Xt) where\nR is the reward function mapping X into (a ﬁnite subset of) R. Thus, whenever the trajectory\n{Xt} of the Markov process equals some state xi ∈X, the resulting reward R(Xt) will always equal\nR(xi) =: ri. Thus the reward is captured by an n-dimensional vector r, where ri = R(xi). On\nthe other hand, if Rt is a random function of Xt, then one would have to provide the probability\ndistribution of Rt given Xt. Since Xt has only n diﬀerent values, we would have to provide n\ndiﬀerent probability distributions.\nTo avoid technical diﬃculties, it is common to assume that\nR(xi) is a bounded random variable for each index i. Note that, because the set X is ﬁnite, if the\nreward function is deterministic, then we have that\nmax\nxi∈X R(xi) < ∞.\nIn case the reward function R is random, as mentioned above, it is common to assume that R(xi)\nis a bounded random variable for each index i ∈[n], where the symbol [n] equals {1, · · · , n}. With\nthis assumption, it follows that\nmax\nxi∈X E[R(xi)] < ∞.\n4\nTwo kinds of Markov reward processes are widely studied, namely: Discounted reward processes,\nand average reward processes. In this paper, we restrict attention to discounted reward processes.\nHowever, we brieﬂy introduce average reward processes. Deﬁne (if it exists)\nV (xi) := lim\nT→0\n1\nT + 1E\n\" T\nX\nt=0\nR(Xt)|X0 = xi\n#\n.\nAn excellent review of average reward processes can be found in [1].\nIn each discounted Markov Reward Process, there is a “discount factor” γ ∈(0, 1). This factor\ncaptures the extent to which future rewards are less valuable than immediate rewards. Fix an\ninitial state xi ∈X. Then the expected discounted future reward V (xi) is deﬁned as\nV (xi) := E\n\" ∞\nX\nt=0\nγtRt|X0 = xi\n#\n= E\n\" ∞\nX\nt=0\nγtR(Xt)|X0 = xi\n#\n.\n(2)\nWe often just use “discounted reward” instead of the longer phrase.\nWith these assumptions,\nbecause γ < 1, the above summation converges and is well-deﬁned. The quantity V (xi) is referred\nto as the value function associated with xi, and the vector\nv = [ V (x1)\n· · ·\nV (xn) ]⊤,\n(3)\nis referred to as the value vector. Note that, throughout this paper, we view the value as both a\nfunction V : X →R as well as a vector v ∈Rn. The relationship between the two is given by (3).\nWe shall use whichever interpretation is more convenient in a given context.\nThis raises the question as to how the value function and/or value vector is to be determined.\nDeﬁne the vector r ∈Rn via\nr := [ r1\n· · ·\nrn ]⊤,\n(4)\nwhere ri = R(xi) if R is a deterministic function, and if Rt is a random function of Xt, then\nri := E[R(xi)].\n(5)\nThe next result gives a useful characterization of the value vector.\nTheorem 1. The vector v satisﬁes the recursive relationship\nv = r + γAv,\n(6)\nor, in expanded form,\nV (xi) = ri + γ\nn\nX\nj=1\naijV (xj).\n(7)\nProof. Let xi ∈X be arbitrary. Then by deﬁnition we have\nV (xi) = E\n\" ∞\nX\nt=0\nγtRt|X0 = xi\n#\n= ri + E\n\" ∞\nX\nt=1\nγtRt|X0 = xi\n#\n.\n(8)\n5\nHowever, if X0 = xi, then X1 = xj with probability aij. Therefore we can write\nE\n\" ∞\nX\nt=1\nγtRt|X0 = xi\n#\n=\nn\nX\nj=1\naijE\n\" ∞\nX\nt=1\nγtRt|X1 = xj\n#\n=\nγ\nn\nX\nj=1\naijE\n\" ∞\nX\nt=0\nγtRt|X0 = xj\n#\n=\nγ\nn\nX\nj=1\naijV (xj).\n(9)\nIn the second step we use fact that the Markov process is stationary. Substituting from (9) into\n(8) gives the recursive relationship (7).\nExample 1. As an illustration of a Markov Reward process, we analyze a toy snakes and ladders\ngame with the transitions shown in Figure 3. Here W and L denote “win” and “lose” respectively.\nThe rules of the game are as follows:\n• Initial state is S.\n• A four-sided, fair die is thrown at each stage.\n• Player must land exactly on W to win and exactly on L to lose.\n• If implementing a move causes crossing of W and L, then the move is not implemented.\nThere are twelve possible states in all: S, 1, . . . , 9 , W, L. However, 2, 3, 9 can be omitted,\nleaving nine states, namely S, 1, 4, 5, 6, 7, 8, W, L. At each step, there are at most four possible\noutcomes. For example, from the state S, the four outcomes are 1, 7, 5, 4. From state 6, the four\noutcomes are 7, 8, 1, and W. From state 7, the four outcomes are 8, 1, W, 7. From state 8, there\nfour possible outcomes are 1, W, L and 8 with probability 1/4 each, because if the die comes up\nwith 4, then the move cannot be implemented. It is time-consuming but straight-forward to compute\nthe state transition matrix as\nS\n1\n4\n5\n6\n7\n8\nW\nL\nS\n0\n0.25\n0.25\n0.25\n0\n0.25\n0\n0\n0\n1\n0\n0\n0.25\n0.50\n0\n0.25\n0\n0\n0\n4\n0\n0\n0\n0.25\n0.25\n0.25\n0.25\n0\n0\n5\n0\n0.25\n0\n0\n0.25\n0.25\n0.25\n0\n0\n6\n0\n0.25\n0\n0\n0\n0.25\n0.25\n0.25\n0\n7\n0\n0.25\n0\n0\n0\n0\n0.25\n0.25\n0.25\n8\n0\n0.25\n0\n0\n0\n0\n0.25\n0.25\n0.25\nW\n0\n0\n0\n0\n0\n0\n0\n1\n0\nL\n0\n0\n0\n0\n0\n0\n0\n0\n1\nWe deﬁne a reward function for this problem, as follows: We set Rt = f(Xt+1), where f is\ndeﬁned as follows: f(W) = 5, f(L) = −2, f(x) = 0 for all other states. However, there is an\nexpected reward depending on the state at the next time instant. For example, if X0 = 6, then the\nexpected value of R0 is 5/4, whereas if X0 = 7 or X0 = 8, then the expected value of R0 is 3/4. ■\n6\nS\n1\n2\n3\n4\n5\n6\n7\n8\n9\nW\nL\nFigure 3: A Toy Snakes and Ladders Game\nNow let us see how the implicit equation (6) can be solved to determine the value vector v.\nSince the induced matrix norm ∥A∥∞→∞= 1 and γ < 1, it follows that the matrix I −γA is\nnonsingular.\nTherefore, for every reward function r, there is a unique v that satisﬁes (6).\nIn\nprinciple it is possible to deduce from (6) that\nv = (I −γA)−1r.\n(10)\nThe diﬃculty wth this formula however is that in most actual applications of Markov Decision\nProblems, the integer n denoting the size of the state space X is quite large. Moreover, inverting\na matrix has cubic complexity in the size of the matrix. Therefore it may not be practicable to\ninvert the matrix I −γA. So we are forced to look for alternate approaches. A feasible approach\nis provided by the Contraction Mapping Theorem.\nTheorem 2. The map y 7→Ty := r + γAy is monotone and is a contraction with respect to the\nℓ∞-norm, with contraction constant γ. Therefore, we can choose some vector y0 arbitrarily, and\nthen deﬁne\nyi+1 = r + γAyi.\nThen yi converges to the value vector v.\nProof. The ﬁrst statement is that if y1 ≤y2 componentwise (and note that the vectors y1, y2 need\nnot consist of only positive components), then Ty1 ≤Ty2. This is obvious from the fact that the\nmatrix A has only nonnegative components, so that Ay1 ≤Ay2. For the second statement, note\nthat, because the matrix A is row-stochastic, the induced matrix norm ∥A∥∞→∞is equal to one.\nTherefore\n∥Ty1 −Ty2∥∞= ∥γA(y1 −y2)∥∞≤γ∥y1 −y2∥∞.\nThis completes the proof.\nThere is however a limitation to this approach, namely, that it requires that the state transition\nmatrix A has to be known.\nIn Reinforcement Learning, this assumption is often not satisﬁed.\nInstead, one has access to a single sample path {Xt} of a Markov process over X, whose state\ntransition matrix is A. The question therefore arises: How can one compute the value vector v in\nsuch a scenario? The answer is provided by the so-called Temporal Diﬀerence algorithm, which is\ndiscussed in Section 3.3.\n7\n3\nMarkov Decision Processes\n3.1\nProblem Formulation\nIn a Markov reward process, the state Xt evolves on its own, according to a predetermined state\ntransition matrix. In contrast, in a MDP, there is also another variable called the “action” which\naﬀects the dynamics. Speciﬁcally, in addition to the state space X, there is also a ﬁnite set of\nactions U. Each action uk ∈U leads to a corresponding state transition matrix Auk = [auk\nij ]. So at\ntime t, if the state is Xt, and an action Ut ∈U is applied, then\nPr{Xt+1 = xj|Xt = xi, Ut = uk} = auk\nij .\n(11)\nObviously, for each ﬁxed uk ∈U, the corresponding state transition matrix Auk is row-stochastic.\nIn addition, there is also a “reward” function R : X ×U →R. Note that in a Markov reward process,\nthe reward depends only on the current state, whereas in a Markov decision process, the reward\ndepends on both the current state as well as the action taken. As in Markov reward processes, it is\npossible to permit R to be a random function of Xt and Ut as opposed to a deterministic function.\nMoreover, to be consistent with the earlier convention, it is assumed that the reward R(Xt, Ut) is\npaid at time t.\nThe most important aspect of an MDP is the concept of a “policy,” which is just a systematic\nway of choosing Ut given Xt. If π : X →U is any map, this would be called a deterministic policy,\nand the set of all deterministic policies is denoted by Πd. Alternatively, let S(U) denote the set\nof probability distributions on the ﬁnite set U. Then a map π : X →S(U) would be called a\nprobabilistic policy, and the set of probabilistic policies is denoted by Πp. Note that the cardinality\nof Πd equals |U|||X|, while the set Πp is uncountable.\nA vital point about MDPs is this: Whenever any policy π, whether deterministic or probabilistic,\nis implemented, the resulting process {Xt} is a Markov process with an associated state transition\nmatrix, which is denoted by Aπ. This matrix can be determined as follows: If π ∈Πd, then at time\nt, if Xt = xi, then the corresponding action Ut equals π(xi). Therefore\nPr{Xt+1 = xj|Xt = xi, π} = aπ(xi)\nij\n.\n(12)\nIf π ∈Πp and\nπ(xi) = [ φi1\n· · ·\nφim ],\n(13)\nwhere m = |U|, then\nPr{Xt+1 = xj|Xt = xi, π} =\nm\nX\nk=1\nφikauk\nij .\n(14)\nIn a similar manner, for every policy π, the reward function R : X × U →R can be converted into\na reward map Rπ : X →R, as follows: If π ∈Πd, then\nRπ(xi) = R(xi, π(xi)),\n(15)\nwhereas if π ∈Πp, then\nRπ(xi) =\nm\nX\nk=1\nφikR(xi, uk).\n(16)\nThus, given any policy π, whether deterministic or probabilistic, we can associate with it a reward\nvector rπ. To summarize, given any MDP, once a policy π is chosen, the resulting process {Xt} is\na Markov reward process with state transition matrix Aπ and reward vector rπ.\n8\nExample 2. To illustrate these ideas, suppose n = 4, m = 2, so that there four states and two\nactions. Thus there are two 4 × 4 state transition matrices A1, A2 corresponding to the two ac-\ntions. (In the interests of clarity, we write A1 and A2 instead of Au1 and Au2.) Suppose π1 is a\ndeterministic policy, represented as a n × m matrix (in this case a 4 × 2 matrix), as follows:\nM1 =\n\n\n0\n1\n0\n1\n1\n0\n0\n1\n\n.\nThis means that if Xt = x1, x2 or x4, then Ut = u2, while if Xt = x3, then Ut = u1. Let us use the\nnotation (Ak)i to denote the i-th row of the matrix Ak, where k = 1, 2 and i = 1, 2, 3, 4. Then the\nstate transition matrix Aπ1 is given by\nAπ1 =\n\n\n(A2)1\n(A2)2\n(A1)3\n(A2)4\n\n.\nThus the ﬁrst, third, and fourth rows of Aπ1 come from A2, while the second row comes from A1.\nNext, suppose π2 is a probabilistic policy, represented by the matrix\nM2 =\n\n\n0.3\n0.7\n0.2\n0.8\n0.9\n0.1\n0.4\n0.6\n\n.\nThus, if Xt = x1, then the action Ut = u1 with probability 0.3 and equals u2 with probability 0.7,\nand so on. For this policy, the resulting state transition matrix is determined as follows:\nAπ2 =\n\n\n0.3(A1)1 + 0.7(A2)1\n0.2(A1)2 + 0.8(A2)2\n0.9(A1)3 + 0.1(A2)3\n0.4(A1)4 + 0.6(A2)4\n\n.\n■\nFor a MDP, one can pose three questions:\n1. Policy evaluation: We have seen already that, given a Markov reward process, with a\nreward vector r and a discount factor γ, there corresponds a unique value vector v.\nWe\nhave also seen that, for any choice of a policy π, whether deterministic or probabilistic, there\ncorresponds a state transition matrix Aπ and a reward vector rπ. Therefore, once a policy π is\nchosen, the Markov decision process becomes a Markov reward process with state transition\nmatrix Aπ and reward vector rπ. We can deﬁne vπ(xi) to be the value vector associated with\nthis Markov reward process. The question is: How can vπ(xi) be computed?\n2. Optimal Value Determination: For each policy π, there is an associated value vector vπ.\nLet us view vπ as a map from X to R, so that Vπ(xi) is the i-th component of vπ. Now\nsuppose xi ∈X is a speciﬁed initial state, and deﬁne\nV ∗(xi) := max\nπ∈Πp Vπ(xi),\n(17)\n9\nto be the optimal value over all policies, when the MDP is started in the initial state\nX0 = xi. How can V ∗(xi) be computed? Note that in (17), the optimum is taken over all\nprobabilistic policies. However, it can be shown that the optimum is the same even if π is\nrestricted to only deterministic policies.\n3. Optimal Policy Determination: In (17) above, we associate an optimal policy with each\nstate xi. Now we can extend the idea and deﬁne the optimal policy map X →Πd via\nπ∗(xi) := arg max\nπ∈Πd\nVπ(xi).\n(18)\nHow can the optimal policy map π∗be determined? Note that it is not a priori evident that\nthere exists one policy that is optimal for all initial states. But the existence of such an\noptimal policy can be shown. Also, we can restrict to π ∈Πd in (18) because it can be shown\nthat the maximum over π ∈Πp is not any larger. In other words,\nmax\nπ∈Πd\nVπ(xi) = max\nπ∈Πp Vπ(xi).\n3.2\nMarkov Decision Processes: Solution\nIn this subsection we present answers to the three questions above.\n3.2.1\nPolicy Evaluation:\nSuppose a policy π in Πd or Πp is speciﬁed. Then the corresponding state transition matrix Aπ\nand reward vector rπ are given by (12) (or (14)) and (15) respectively. As pointed out above, once\nthe policy is chosen, the process becomes just a Markov reward process. Then it readily follows\nfrom Theorem 1 that vπ satisﬁes an equation analogous to (6), namely\nvπ = rπ + γAπvπ.\n(19)\nAs before, it is inadvisable to compute vπ via vπ = (I −γAπ)−1rπ. Instead, one should use value\niteration to solve (19). Observe that, whatever the policy π might be, the resulting state transition\nmatrix Aπ satisﬁes ∥A∥∞→∞= 1. Therefore the map y 7→rπ + γAπy is a contraction with respect\nto ∥· ∥∞, with contraction constant γ.\n3.2.2\nOptimal Value Determination:\nNow we introduce one of the key ideas in Markov Decision Processes. Deﬁne the Bellman itera-\ntion map B : Rn →Rn via\n(Bv)i := max\nuk∈U\n\nR(xi, uk) + γ\nn\nX\nj=1\nauk\nij vj\n\n.\n(20)\nTheorem 3. The map B is monotone and a contraction with respect to the ℓ∞-norm. Therefore\nthe ﬁxed point ¯v of the map B satisﬁes the relation\n(¯v)i := max\nuk∈U\n\nR(xi, uk) + γ\nn\nX\nj=1\nauk\nij (¯v)j\n\n.\n(21)\n10\nNote that (21) is known as the Bellman Optimality equation. Thus, in principle at least,\nwe can choose an arbitrary initial guess v0 ∈Rd, and repeatedly apply the Bellman iteration. The\nresulting iterations would converge to the unique ﬁxed point of the operator B, which we denote\nby ¯v.\nThe signiﬁcance of the Bellman iteration is given by the next theorem.\nTheorem 4. Deﬁne ¯v ∈Rn to be the unique ﬁxed point of B, and deﬁne v∗∈Rn to equal\n[V ∗(xi), xi ∈X], where V ∗(xi) is deﬁned in (17). Then ¯v = v∗.\nTherefore, the optimal value vector can be computed using the Bellman iteration. However,\nknowing the optimal value vector does not, by itself, give us an optimal policy.\n3.2.3\nOptimal Policy Determination\nTo solve the problem of optimal policy determination, we introduce another function Qπ : X ×U →\nR, known as the action-value function, which is deﬁned as follows:\nQπ(xi, uk) := R(xi, uk) + Eπ\n\" ∞\nX\nt=1\nγtRπ(Xt)|X0 = xi, U0 = uk\n#\n.\n(22)\nThis function was ﬁrst deﬁned in [42]. Note that Qπ is deﬁned only for deterministic policies. In\nprinciple it is possible to deﬁne it for probabilistic policies, but this is not commonly done. In\nthe above deﬁnition, the expectation Eπ is with respect to the evolution of the state Xt under the\npolicy π.\nThe way in which a MDP is set up is that at time t, the Markov process reaches a state Xt,\nbased on the previous state Xt−1 and the state transition matrix Aπ corresponding to the policy π.\nOnce Xt is known, the policy π determines the action Ut = π(Xt), and then the reward Rπ(Xt) =\nR(Xt, π(Xt)) is generated. In particular, when deﬁning the value function Vπ(xi) corresponding to\na policy π, we start oﬀthe MDP in the initial state X0 = xi, and choose the action U0 = π(xi).\nHowever, in deﬁning the action-value function Qπ, we do not feel compelled to set U0 = π(X0) =\nπ(xi), and can choose an arbitrary action uk ∈U. From t = 1 onwards however, the action Ut is\nchosen as Ut = π(Xt). This seemingly small change leads to some simpliﬁcations.\nJust as we can interpret Vπ : X →R as an n-dimensional vector, we can interpret Qπ : X ×U →\nR as an nm-dimensional vector, or as a matrix of dimension n × m. Consequently the Qπ-vector\nhas higher dimension than the value vector.\nTheorem 5. For each policy π ∈Πd, the function Qπ satisﬁes the recursive relationship\nQπ(xi, uk) = R(xi, uk) + γ\nn\nX\nj=1\nauk\nij Qπ(xj, π(xj)).\n(23)\nProof. Observe that at time t = 0, the state transition matrix is Auk. So, given that X0 = xi and\nU0 = uk, the next state X1 has the distribution\nX1 ∼[auk\nij , j = 1, · · · , n].\n11\nMoreover, U1 = π(X1) because the policy π is implemented from time t = 1 onwards. Therefore\nQπ(xi, uk)\n=\nR(xi, uk)\n+\nEπ\n\n\nn\nX\nj=1\nauk\nij\n \nγR(xj, π(xj)) +\n∞\nX\nt=2\nγtRπ(Xt)|X1 = xj, U1 = π(xj)\n!\n\n=\nR(xi, uk)\n+\nEπ\n\nγ\nn\nX\nj=1\nauk\nij\n \nR(xj, π(xj)) +\n∞\nX\nt=1\nγtRπ(Xt)|X1 = xj, U1 = π(xj)\n!\n\n=\nR(xi, uk) + γ\nn\nX\nj=1\nauk\nij Q(xj, π(xj)).\nThis is the desired conclusion.\nTheorem 6. The functions Vπ and Qπ are related via\nVπ(xi) = Qπ(xi, π(xi)).\n(24)\nProof. If we choose uk = π(xi) then (23) becomes\nQπ(xi, π(xi)) = Rπ(xi) + γ\nn\nX\nj=1\naπ(xj)\nij\nQ(xj, π(xj)).\nThis is the same as (11) written out componentwise. We know that (11) has a unique solution,\nnamely Vπ. This shows that (24) holds.\nThe import of Theorem 6 is the following: In deﬁning the function Qπ(xi, uk) for a ﬁxed policy\nπ ∈Πd, we have the freedom to choose the initial action uk as any element we wish in the action\nspace U.\nHowever, if we choose the initial action uk = π(xi) for each state xi ∈X, then the\ncorresponding action-value function Qπ(xi, uk) equals the value function Vπ(xi), for each state\nxi ∈X.\nIn view of (24), the recursive equation for Qπ can be rewritten as\nQπ(xi, uk) = R(xi, uk) + γ\nn\nX\nj=1\nauk\nij Vπ(xj).\n(25)\nThis motivates the next theorem.\nTheorem 7. Deﬁne Q∗: X × U →R by\nQ∗(xi, uk) = R(xi, uk) + γ\nn\nX\nj=1\nauk\nij V ∗(xj).\n(26)\nThen Q∗(·, ·) satisﬁes the following relationships:\nQ∗(xi, uk) = R(xi, uk) + γ\nn\nX\nj=1\nauk\nij max\nwl∈U Q∗(xj, wl).\n(27)\n12\nV ∗(xi) = max\nuk∈U Q∗(xi, uk),\n(28)\nMoreover, every policy π ∈Πd such that\nπ∗(xi) = arg max\nuk∈U\nQ∗(xi, uk)\n(29)\nis optimal.\nProof. Since Q∗(·, ·) is deﬁned by (26), it follows that\nmax\nuk∈U Q∗(xi, uk) = max\nuk∈U\n\nR(xi, uk) + γ\nn\nX\nj=1\nauk\nij V ∗(xj)\n\n= V ∗(xi),\nThis establishes (28) and (29). Substituting from (28) into (26) gives (27).\nTheorem 7 converts the problem of determining an optimal policy into one of solving the implicit\nequation (27). For this purpose, we deﬁne an iteration on action-functions that is analogous to (20)\nfor value functions. As with the value function, the action-value function can either be viewed as a\nmap Q : X ×U →R, or as a vector in Rnm, or as an n×m matrix. We use whichever interpretation\nis convenient in the given situation.\nTheorem 8. Deﬁne F : R|X|×|U| →R|X|×|U| by\n[F(Q)](xi, uk) := R(xi, uk) + γ\nn\nX\nj=1\nauk\nij max\nwl∈U Q(xj, wl).\n(30)\nThen the map F is monotone and is a contraction. Moreover, for all Q0 : X ×U →R, the sequence\nof iterations {F t(Q0)} converges to Q∗as t →∞.\nIf we were to rewrite (21) and (27) in terms of expected values, the diﬀerences between the\nQ-function and the V -function would become apparent. We can rewrite (21) as\nV ∗(Xt) = max\nUt∈U{R(Xt, Ut) + γE[V ∗(Xt+1)|Xt]},\n(31)\nand (27) as\nQ∗(Xt, Ut) = R(Xt, Ut) + γE\n\u0014\nmax\nUt+1∈U Q∗(Xt+1, Ut+1)\n\u0015\n.\n(32)\nThus in the Bellman formulation and iteration, the maximization occurs outside the expectation,\nwhereas with the Q-formulation and F-iteration, the maximization occurs inside the expectation.\n3.3\nIterative Algorithms for MDPs with Unknown Dynamics\nIn principle, Theorem 2 can be used to compute, to arbitrary precision, the value vector of a Markov\nreward process. Similarly, Theorem 8 can be use to compute, to arbitrary precision, the optimal\naction-value function of a Markov Decision Process, from which both the optimal value function\nand the optimal policy can be determined. However, both theorems depend crucially on knowing\nthe dynamics of the underlying process. For instance, if the state transition matrix A is not known,\nit would not be possible to carry out the iterations\nyi+1 = r + γAyi.\n13\nEarly researchers in Reinforcement Learning were aware of this issue, and developed several al-\ngorithms that do not require explicit knowledge of the dynamics of the underlying process. Instead,\nit is assumed that a sample path {Xt}∞\nt=0 of the Markov process, together with the associated re-\nward process, are available for use. With this information, one can think of two distinct approaches.\nFirst, one can use the sample path to estimate the state transition matrix, call it ˆA. After a suf-\nﬁciently long sample path has been observed, the contraction iteration above can be applied with\nA replaced by ˆA.\nThis would correspond to so-called “indirect adaptive control.”\nThe second\napproach would be to use the sample path right from time t = 0, and adjust only one component\nof the estimated value function at each time instant t. This would correspond to so-called “direct\nadaptive control.” Using a similar approach, it is also possible to estimate the action-value func-\ntion based on a single sample path. We describe two such algorithms, namely Temporal Diﬀerence\nlearning for estimating the value function of a Markov reward process, and Q-learning for estimat-\ning the action-value function of a Markov Decision Process. Within Temporal Diﬀerence, we make\na further distinction between estimating the full value vector, and estimating a projection of the\nvalue vector onto a lower-dimensional subspace.\n3.3.1\nTemporal Diﬀerence Learning Without Function Approximation\nIn this subsection and the next, we describe the so-called “temporal diﬀerence” family of algorithms,\nﬁrst introduced in [31]. The objective of the algorithm is to compute the value vector of a Markov\nreward process. Recall that the value vector v of a Markov reward process satisﬁes (6). In Temporal\nDiﬀerence approach, it is not assumed that the state transition matrix A is known. Rather, it is\nassumed that the learner has available a sample path {(Xt} of the Markov process under study,\ntogether with the associated reward at each time. For simplicity it is assumed that the reward\nis deterministic and not random. Thus the reward at time t is just R(Xt) and does not add any\ninformation.\nThere are two variants of the algorithm. In the ﬁrst, one constructs a sequence of approximations\nˆvt that, one hopes, would converge to the true value vector v as t →∞. In the second, which is\nused when n is very large, one chooses a “basis representation matrix” Ψ ∈Rn×d, where d ≪n.\nThen one constructs a sequence of vectors θt ∈Rd, such that the corresponding sequence of vectors\nΨθt ∈Rn forms an approximation to the value vector v. Since there is no a priori reason to believe\nthat v belongs to the range of Ψ, there is also no reason to believe that Ψθt would converge to\nv. The second approach is called Temporal Diﬀerence Learning with function approximation. The\nﬁrst is studied in this subsection, while the second is studied in the next subsection.\nIn principle, by observing the sample path for a suﬃciently long duration, it is possible to make\na reliable estimate of A. However, a key feature of the temporal diﬀerence algorithm is that it is\na “direct” method, which works directly with the sample path, without attempting to infer the\nunderlying Markov process. With the sample path {Xt} of the Markov process, one can associate\na corresponding “index process” {Nt} taking values in [n], as follows:\nNt = i if Xt = xi ∈X.\nIt is obvious that the index process has the same transition matrix A as the process {Xt}. The\nidea is to start with an initial estimate ˆv0, and update it at each time t based on the sample path\n{(Xt, R(Xt))}.\nNow we introduce the TD(λ) algorithm studied in this paper.\nThis version of the TD(λ)\nalgorithm comes from [16, Eq. (4.7)], and is as follows: Let v∗denote the unique solution of the\n14\nequation1\nv∗= r + γAv∗.\nAt time t, let ˆvt ∈Rn denote the current estimate of v∗. Thus the i-th component of ˆvt, denoted\nby ˆVt,i, is the estimate of the reward when the initial state is xi. Let {Nt} be the index process\ndeﬁned above. Deﬁne the “temporal diﬀerence”\nδt+1 := RNt + γ ˆVt,Nt+1 −ˆVt,Nt, ∀t ≥0,\n(33)\nwhere ˆVt,Nt denotes the Nt-th component of the vector ˆvt. Equivalently, if the state at time t is\nxi ∈X and the state at the next time t + 1 is xj, then\nδt+1 = Ri + γ ˆVt,j −ˆVt,i.\n(34)\nNext, choose a number λ ∈[0, 1). Deﬁne the “eligibility vector”\nzt =\nt\nX\nτ=0\n(γλ)τI{Nt−τ =Nt}eNt−τ,\n(35)\nwhere eNs is a unit vector with a 1 in location Ns and zeros elsewhere. Since the indicator function\nin the above summation picks up only those occurrences where Nt−τ = Nt, the vector zt can also\nbe expressed as\nzt = zteNt, zt =\nt\nX\nτ=0\n(γλ)τI{Nt−τ=Nt}.\n(36)\nThus the support of the vector zt consists of the singleton {Nt}. Finally, update the estimate ˆvt as\nˆvt+1 = ˆvt + δt+1αtzt,\n(37)\nwhere {αt} is a sequence of step sizes. Note that, at time t, only the Nt-th component of ˆvt is\nupdated, and the rest remain the same.\nA suﬃcient condition for the convergence of the TD(λ)-algorithm is given in [16].\nTheorem 9. The sequence {ˆvt} converges almost surely to v∗as t →∞, provided\n∞\nX\nt=0\nα2\ntI{Nt=i} < ∞, a.s., ∀i ∈[n],\n(38)\n∞\nX\nt=0\nαtI{Nt=i} = ∞, a.s., ∀i ∈[n],\n(39)\n3.3.2\nTD-Learning with Function Approximation\nIn this set-up, we again observe a time series {(Xt, R(Xt))}. The new feature is that there is a\n“basis” matrix Ψ ∈Rn×d, where d ≪n. The estimated value vector at time t is given by ˆvt = Ψθt,\nwhere θt ∈Rd is the parameter to be updated. In this representation, it is clear that, for any index\ni ∈[n], we have that\nˆVt,i = Ψiθt = ⟨(Ψi)⊤, θt⟩,\n1For clarity, we have changed the notation so that the value vector is now denoted by v∗instead of v as in (6).\n15\nwhere Ψi denotes the i-th row of the matrix Ψ.\nNow we deﬁne the learning rule for updating θt. Let {Xt} be the observed sample path. By a\nslight abuse of notation, deﬁne\nyt = [ΨXt]⊤∈Rd.\nThus, if Xt = xi, then yt = [Ψi]⊤. The eligibility vector zt ∈Rd is deﬁned via\nzt =\nt\nX\nτ=0\n(γλ)t−τyτ.\n(40)\nNote that zt satisﬁes the recursion\nzt = γλzt−1 + yt.\nHence it is not necessary to keep track of an ever-growing set of past values of yτ. In contrast to\n(35), there is no term of the type I{Nt−τ =Nt} in (40). Thus, unlike the eligibility vector deﬁned in\n(35), the current vector zt can have more than one nonzero component. Next, deﬁne the temporal\ndiﬀerence δt+1 as in (33). Note that, if Xt = xi and Xt+1 = xj, then\nδt+1 = ri + γ[Ψj]⊤θt −[Ψi]⊤θt.\nThen the updating rule is\nθt+1 = θt + αtδt+1zt,\n(41)\nwhere αt is the step size.\nThe convergence analysis of (41) is carried out in detail in [38], based on the assumption that\nthe state transition matrix A is irreducible. This is quite reasonable, as it ensures that every state\nxi occurs inﬁnitely often in any sample path, with probability one. Since that convergence analysis\ndoes not readily ﬁt into the methods studied in subsequent sections, we state the main results\nwithout proof. However, we state and prove various intermediate results, that are useful in their\nown right.\nSuppose A is row-stochastic and irreducible, and let µ denote its stationary distribution. Deﬁne\nM = Diag(µi) and deﬁne a norm ∥· ∥M on Rd by\n∥v∥M = (v⊤Mv)1/2.\nThen the corresponding distance between two vectors v1, v2 is given by\n∥v1 −v2∥M = ((v1 −v2)⊤M(v1 −v2))1/2.\nThen the following result is proved in [38].\nLemma 1. Suppose A ∈[0, 1]n×n, is row-stochastic, and irreducible.\nLet µ be the stationary\ndistribution of A. Then\n∥Av∥M ≤∥v∥M, ∀v ∈Rn.\nConsequently, the map v 7→r + γAv is a contraction with respect to ∥· ∥M.\nProof. We will show that\n∥Av∥2\nM ≤∥v∥2\nM, ∀v ∈Rn,\n16\nwhich is clearly equivalent to the ∥Av∥M ≤∥v∥M. Now\n∥Av∥2\nM =\nn\nX\ni=1\nµi(Av)2\ni =\nn\nX\ni=1\nµi\n\n\nn\nX\nj=1\nAijvj\n\n\n2\n.\nHowever, for each ﬁxed index i, the row Ai is a probability distribution, and the function f(Y ) = Y 2\nis convex. If we apply Jensen’s inequality with f(Y ) = Y 2, we see that\n\n\nn\nX\nj=1\nAijvj\n\n\n2\n≤\nn\nX\nj=1\nAijv2\nj , ∀i.\nTherefore\n∥Av∥2\nM\n≤\nn\nX\ni=1\nµi\n\n\nn\nX\nj=1\nAijv2\nj\n\n=\nn\nX\nj=1\n n\nX\ni=1\nµiAij\n!\nv2\nj\n=\nn\nX\nj=1\nµjv2\nj = ∥v∥2\nM,\nwhere in the last step we use the fact that µA = µ.\nTo analyze the behavior of the TD(λ) algorithm with function approximation, the following\nmap T λ : Rn →Rn is deﬁned in [38]:\n[T λv]i := (1 −λ)\n∞\nX\nl=0\nλlE\n\"\nl\nX\nτ=0\nγτR(Xτ+1) + γl+1VXl+1|X0 xi\n#\n.\nNote that T λv can be written explicitly as\nT λv = (1 −λ)\n∞\nX\nl=0\nλl\n\"\nl\nX\nτ=0\nγτAτr + γl+1Al+1v\n#\n.\nLemma 2. The map T λ is a contraction with respect to ∥· ∥M, with contraction constant [γ(1 −\nλ)]/(1 −γλ).\nProof. Note that the ﬁrst term on the right side of does not depend on v. Therefore\nT λ(v1 −v2) = γ(1 −λ)\n∞\nX\nl=0\n(γλ)lAl+1(v1 −v2).\nHowever, it is already known that\n∥A(v1 −v2)∥M ≤∥v1 −v2∥M.\nBy repeatedly applying the above, it follows that\n∥Al(v1 −v2)∥M ≤∥v1 −v2∥M, ∀l.\nTherefore\n∥T λ(v1 −v2)∥M ≤γ(1 −λ)\n∞\nX\nl=0\n(γλ)l∥v1 −v2∥M = γ(1 −λ)\n1 −γλ ∥v1 −v2∥M.\nThis is the desired bound.\n17\nDeﬁne a projection Π : Rn →Rn by\nΠa := Ψ(Ψ⊤MΨ)−1Ψ⊤Ma.\nThen\nΠa = arg min\nb∈Ψ(Rd)\n∥a −b∥M.\nThus Π projects the space Rn onto the image of the matrix Ψ, which is a d-dimensional subspace,\nif Ψ has full column rank. In other words, Πa is the closest point to a in the subspace Ψ(Rn).\nNext, observe that the projection Π is nonexpansive with respect to ∥· ∥M. As a result, the\ncomposite map ΠT λ is a contraction. Thus there exists a unique ¯v ∈Rd such that\nΠT λ¯v = ¯v.\nMoreover, the above equation shows that in fact ¯v belongs to the range of Ψ. Thus there exists a\nθ∗∈Rd such that ¯v = Ψθ∗, and θ∗is unique if Ψ has full column rank.\nThe limit behavior of the TD(λ) algorithm is given by the next theorem, which is a key result\nfrom [38].\nTheorem 10. Suppose that Ψ has full column rank, and that\n∞\nX\nt=0\nαt = ∞,\n∞\nX\nt=0\nα2\nt < ∞.\nThen the sequence {θt} converges almost surely to θ∗∈Rd, where θ∗is the unique solution of\nΠT λ(Ψθ∗) = Ψθ∗.\nMoreover\n∥Ψθ∗−v∗∥M ≤1 −γλ\n1 −γ ∥Πv∗−v∗∥M.\nNote that, since Ψθ ∈Π(Rd) for all θ ∈Rd, the best that one can hope for is that\n∥Ψθ∗−v∗∥M = ∥Πv∗−v∗∥M.\nThe theorem states that the above identity might not hold, and provides an upper bound for\nthe distance between the limit Ψθ∗and the true value vector v∗.\nIt is bounded by a factor\n(1 −γλ)/(1 −γ) times this minimum.\nNote that (1 −γλ)/(1 −γ) > 1. So this is the extent to which the TD(λ) iterations miss the\noptimal approximation.\n3.3.3\nQ-Learning\nThe Q-learning algorithm proposed in [42] has the characterization (27) of Q∗as its starting point.\nThe algorithm is based on the following premise: At time t, the current state Xt can be observed;\ncall it xi ∈X. Then the learner is free to choose the action Ut; call it uk ∈U. With this choice, the\nnext state Xt+1 has the probability distribution equal to the i-th row of the state transition matrix\nAuk. Suppose the observed next stat Xt+1 is xj ∈X. With these conventions, the Q-learning\nalgorithm proceeds as follows.\n18\n1. Choose an arbitrary initial guess Q0 : X × U →R and an initial state X0 ∈X.\n2. At time t, with current state Xt = xi, choose a current action Ut = uk ∈U, and let the\nMarkov process run for one time step. Observe the resulting next state Xt+1 = xj. Then\nupdate the function Qt as follows:\nQt+1(xi, uk) = Qt(xi, uk) + αt[R(xi, uk) + γVt(xj) −Qt(xi, uk)],\nQt+1(xs, wl) = Qt(xs, wl), ∀(xs, wl) ̸= (xi, uk).\n(42)\nwhere\nVt(xj) = max\nwl∈U Qt(xj, wl),\n(43)\nand {αt} is a deterministic sequence of step sizes.\n3. Repeat.\nIt is evident that in the Q-learning algorithm, at any instant of time t, only one element (namely\nQ(Xt, Ut)) gets updated. In the original paper by Watkins and Dayan [42], the convergence of the\nalgorithm used some rather ad hoc methods. Subsequently, a general class of algorithms known\nas “asynchronous stochastic approximation,” which included Q-learning as a special case, was\nintroduced in [36, 16].\nA suﬃcient condition for the convergence of the Q-learning algorithm,\nwhich was originally presented in [42], is rederived using these methods.\nTheorem 11. The Q-learning algorithm converges to the optimal action-value function Q∗provided\nthe following conditions are satisﬁed.\n∞\nX\nt=0\nαtI(Xt,Ut)=(xi,uk) = ∞, ∀(xi, uk) ∈X × U,\n(44)\n∞\nX\nt=0\nα2\nt I(Xt,Ut)=(xi,uk) < ∞, ∀(xi, uk) ∈X × U.\n(45)\nThe main shortcoming of Theorems 9 and 11 is that the suﬃcient conditions (38), (39), (44)\nand (45) are probabilistic in nature. Thus it is not clear how they are to be veriﬁed in a speciﬁc\napplication. Note that in the Q-learning algorithm, there is no guidance on how to choose the next\naction Ut. Presumably Ut is chosen so as to ensure that (44) and (45) are satisﬁed. In Section 5,\nwe show how these theorems can be proven, and also, how the troublesome probabilistic suﬃcient\nconditions can be replaced by purely algebraic conditions.\n4\nStochastic Approximation Algorithms\n4.1\nStochastic Approximation and Relevance to RL\nThe contents of the previous section make it clear that in MDP theory, a central role is played by\nthe need to solve ﬁxed-point problems. Determining the value of a Markov reward problem requires\nthe solution of (6). Determining the optimal value of an MDP requires ﬁnding the ﬁxed point of\nthe Bellman iteration. Finally, determining the optimal policy for an MDP requires ﬁnding the\nﬁxed point of the F-iteration. As pointed out in Section 3.3, when the dynamics of an MDP are\n19\ncompletely known, these ﬁxed point problems can be solved by repeatedly applying the correspond-\ning contraction mapping. However, when the dynamics of the MDP are not known, and one has\naccess only to a sample path of the MDP, a diﬀerent approach is required. In Section 3.3, we have\npresented two such methods, namely the Temporal Diﬀerence algorithm for value determination,\nand the Q-Learning algorithm for determining the optimal action-value function. Theorems 9 and\n11 respectively give suﬃcient conditions for the convergence of these algorithms. The proofs of\nthese theorems, as given in the original papers, tend to be “one-oﬀ,” that is, tailored to the speciﬁc\nalgorithm. It is now shown that a probabilistic method known as “stochastic approximation ” (SA)\ncan be used to unify these methods in a common format. Moreover, instead of the convergence\nproofs being “one-oﬀ,” the SA algorithm provides a unifying approach.\nThe applications of SA go beyond these two speciﬁc algorithms. There is another area called\n“Deep Reinforcement Learning” for problems in which the size of the state space is very large.\nRecall that the action-value function Q : X × U can either be viewed as an nm-dimensional vector,\nor an n×m matrix. In Deep RL, one determines (either exactly or approximately) the action-value\nfunction Q(xi, uk) for a small number of pairs (xi, uk) ∈X × U. Using these as a starting point,\nthe overall function Q deﬁned for all pairs (xi, uk) ∈X × U is obtained by training a deep neural\nnetwork. Training a neural network (in this or any other application) requires the minimization\nof the average mean-squared error, denoted by J(θ) where θ denotes the vector of adjustable\nparameters. In general, the function J(·) is not convex; hence one can at best aspire to ﬁnd a\nstationary point of J(·), i.e., a solution to the equation ∇J(θ) = 0. This problem is also amenable\nto the application of the SA approach.\nNow we give a brief introduction to stochastic approximation. Suppose f : Rd →Rd is some\nfunction, and d can be any integer.\nThe objective of SA is to ﬁnd a solution to the equation\nf(θ) = 0, when only noisy measurements of f(·) are available. The SA method was introduced in\n[28], where the objective was to ﬁnd a solution to a scalar equation f(θ) = 0, where f : R →R.\nThe extension to the case where d > 1 was ﬁrst proposed in [5]. The problem of ﬁnding a ﬁxed\npoint of a map g : Rd →Rd, can be formulated as the above problem with f(θ) := g(θ) −θ. If it is\ndesired to ﬁnd a stationary point of a C1 function J : Rd →R, then we simply set f(θ) = ∇J(θ).\nThus the above problem formulation is quite versatile. More details are given at the start of Section\n4.2.\nStochastic approximation is a family of iterative algorithms, in which one begins with an initial\nguess θ0, and derives the next guess θt+1 from θt. Several variants of SA are possible. In syn-\nchronous SA, every component of θt is changed to obtain θt+1. This was the original concept of\nSA. If, at any time t, only one component of θt is changed to obtain θt+1, and the others remain\nunchanged, this is known as asynchronous stochastic approximation (ASA). This phrase was\napparently ﬁrst introduced in [36], A variant of the approach in [36] is presented in [6]. Speciﬁcally,\nin [6], a distinction is introduced between using a “local clock” versus using a “global clock.” It\nis also possible to study an intermediate situation where, at each time t, some but not necessarily\nall components of θt are updated. There does not appear to be a common name for this situation.\nThe phrase Batch Asynchronous Stochastic Approximation (BASA) is introduced in [17].\nMore details about these variations are given below. There is a fourth variant, known as two\ntime-scale SA is introduced in [8]. In this set-up, one attempts to solve two coupled equations of\nthe form\nf(θ, φ) = 0, g(θ, φ) = 0,\nwhere θ ∈Rn, φ ∈Rm, and f : Rn × Rm →Rn, g : Rn × Rm →Rm.\nthe idea is that one\nof the iterations (say θt+1) is updated “more slowly” than the other (say φt+1). Due to space\nlimitations, two time-scale SA is not discussed further in this paper.\nThe interested reader is\n20\nreferred to [8, 35, 24] for the theory, and to [21, 20] for applications to a speciﬁc type of RL, known\nas Actor-Critic Algorithms.\nThe relevance of SA to RL arises from the following factors:\n• Many (though not all) algorithms used in RL can formulated as some type of SA algorithms.\n• Examples include Temporal Diﬀerence Learning, Temporal Diﬀerence Learning with function\napproximation, Q-Learning, Deep Neural Network Learning, and Actor-Critic Learning. The\nﬁrst three are discussed in detail in Section 5.\nThus: SA provides a unifying framework for several disparate-looking RL algorithms.\n4.2\nProblem Formulation\nThere are several equivalent formulations of the basic SA problem.\n1. Finding a zero of a function: Suppose f : Rd →Rd is some function. Note that f(·)\nneed not be available in closed form. The only thing needed is that, given any θ ∈Rd, an\n“oracle” returns a noise-corrupted version of f(θ). The objective is to determine a solution\nof the equation f(θ) = 0.\n2. Finding a ﬁxed point of a mapping: Suppose g : Rd →Rd. The objective is to ﬁnd a\nﬁxed point of g(·), that is, a solution to g(θ) = θ. If we deﬁne f(θ) = g(θ) −θ, this is the\nsame problem as the above. One might ask: Why not deﬁne f(θ) = θ −g(θ)? As we shall see\nbelow, the convergence of the SA algorithm (in various forms) is closely related to the global\nasymptotic stability of the ODE ˙θ = f(θ). Also, as seen in the previous section, in many\napplications, the map g(·) of which we wish to ﬁnd a ﬁxed point is a contraction. In such a\ncase, there is a unique ﬁxed point θ∗of g(·). In such a case, under relatively mild conditions\nθ∗is a globally asymptotically stable equilibrium of the ODE ˙θ = g(θ) −θ, but not if the\nsign is reversed.\n3. Finding a stationary point of a function: Suppose J : Rd →R is a C1 function. The\nobjective is to ﬁnd a stationary point of J(·), that is, a θ such that ∇J(θ) = 0. If we deﬁne\nf(θ) = −∇J(θ), then this is the same problem as above. Here again, if we wish the SA\nalgorithm to converge to a global minimum of J(·), then the minus sign is essential. On the\nother hand, if we wish the SA algorithm to converge to a global maximum of J(·), then we\nremove the minus sign.\nSuppose the problem is one of ﬁnding a zero of a given function f(·). The synchronous version\nof SA proceeds as follows: An initial guess θ0 ∈Rd is chosen (usually in a deterministic manner,\nbut it can also be randomly chosen). At time t, the available measurement is\nyt+1 = f(θt) + ξt+1,\n(46)\nwhere ξt+1 is the measurement noise. Based on this, the current guess is updated to\nθt+1 = θt + αtyt+1 = θt + αt[f(θt) + ξt+1],\n(47)\nwhere {αt} is a predeﬁned sequence of “step sizes,” with αt ∈(0, 1) for all t. If the problem is that\nof ﬁnding a ﬁxed point of g(·), the updating rule is\nθt+1 = θt + αt[g(θt) −θt + ξt+1] = (1 −αt)θt + αt[g(θt) + ξt+1].\n(48)\n21\nIf the problem is to ﬁnd a stationary point of J(·), the updating rule is\nθt+1 = θt + αtyt+1 = θt + αt[−∇J(θt) + ξt+1].\n(49)\nThese updating rules represent what might be called Synchronous SA, because at each time t,\nevery component of θt is updated. Other variants of SA are studied in subsequent sections.\n4.3\nA New Theorem for Global Asymptotic Stability\nIn this section we state a new theorem on the global asymptotic stability of nonlinear ODEs. This\ntheorem is new and is of interest aside from its applications to the convergence of SA algorithms.\nThe contents of this section and the next section are taken from [41]. To state the result (Theorem\n12 below), we introduce a few preliminary concepts from Lyapunov stability theory. The required\nbackground can be found in [39, 15, 18].\nDeﬁnition 1. A function φ : R+ →R+ is said to belong to class K, denoted by φ ∈K, if\nφ(0) = 0, and φ(·) is strictly increasing. A function φ ∈K is said to belong to class KR, denoted\nby φ ∈KR, if in addition, φ(r) →∞as r →∞. A function φ : R+ →R+ is said to belong to\nclass B, denoted by φ ∈B, if φ(0) = 0, and in addition, for all 0 < ǫ < M < ∞we have that\ninf\nǫ≤r≤M φ(r) > 0.\n(50)\nThe concepts of functions of class K and class KR are standard. The concept of a function of\nclass B is new. Note that, if φ(·) is continuous, then it belongs to Class B if and only if φ(0) = 0,\nand φ(r) > 0 for all r > 0.\nExample 3. Observe that every φ of class K also belongs to class B. However, the converse is not\ntrue. Deﬁne\nφ(r) =\n\u001a r,\nif r ∈[0, 1],\ne−(r−1),\nif r > 1.\nThen φ belongs to Class B. However, since φ(r) →0 as r →∞, φ cannot be bounded below by any\nfunction of class K.\nSuppose we wish to ﬁnd a solution of f(θ) = 0.\nThe convergence analysis of synchronous\nSA depends on the stability of an associated ODE ˙θ = f(θ). We now state a new theorem on\nglobal asymptotic stability, and then use this to establish the convergence of the synchronous SA\nalgorithm. In order to state this theorem, we ﬁrst introduce some standing assumptions on f(·).\nNote that these assumptions are standard in the literature.\n(F1) The equation f(θ) = 0 has a unique solution θ∗.\n(F2) The function f is globally Lipschitz-continuous with constant L.\n∥f(θ) −f(φ)∥2 ≤L∥θ −φ∥2, ∀θ, φ ∈Rd.\n(51)\nTheorem 12. Suppose Assumption (F1) holds, and that there exists a function V : Rd →R+ and\nfunctions η, ψ ∈KR, φ ∈B such that\nη(∥θ −θ∗∥2) ≤V (θ) ≤ψ(∥θ −θ∗∥2), ∀θ ∈Rd,\n(52)\n˙V (θ) ≤−φ(∥θ −θ∗∥2), ∀θ ∈Rd,\n(53)\nThen θ∗is a globally asymptotically stable equilibrium of the ODE ˙θ = f(θ).\n22\nThis is [41, Theorem 4], and the proof can be found therein. Well-known classical theorems for\nglobal asymptotic stability, such as those found in [15, 39, 18], require the function φ(·) to belong\nto Class K. Theorem 12 is an improvement, in that the function φ(·) is required only to belong to\nthe larger Class B.\n4.4\nA Convergence Theorem for Synchronous Stochastic Approximation\nIn this subsection we present a convergence theorem for synchronous stochastic approximation.\nTheorem 13 below is sightly more general than a corresponding result in [41]. This theorem is\nobtained by combining some results from [41] and [17]. Other convergence theorems and examples\ncan be found in [41].\nIn order to analyze the convergence of the SA algorithm, we need to make some assumptions\nabout the nature of the measurement error sequence {ξt}. These assumptions are couched in terms\nof the conditional expectation of a random variable with respect to a σ-algebra. Readers who are\nunfamiliar with the concept are referred to [13] for the relevant background.\nLet θt\n0 denote the tuple θ0, θ1, · · · , θt, and deﬁne ξt\n1 analogously; note that there is no ξ0. Let\n{Ft}t≥0 be any ﬁltration (i.e., increasing sequence of σ-algebras), such that θt\n0, ξt\n1 are measurable\nwith respect to Ft. For example, one can choose Ft to be the σ-algebra generated by the tuples\nθt\n0, ξt\n1.\n(N1) There exists a sequence {bt} of nonnegative numbers such that\n∥E(ξt+1|Ft)∥2 ≤bt a.s., ∀t ≥0.\n(54)\nThus bt provides a bound on the Euclidean norm of the conditional expectation of the mea-\nsurement error with respect to the σ-algebra Ft.\n(N2) There exists a sequence {σt} of nonnegative numbers such that\nE(∥ξt+1 −E(ξt+1|Ft)∥2\n2|Ft) ≤σ2\nt (1 + ∥θt∥2\n2), a.s. ∀t ≥0.\n(55)\nNote that the quantity on the left side of (55) is the conditional variance of ξt+1 with respect to\nthe σ-algebra Ft.\nNow we can state a theorem about the convergence of synchronous SA.\nTheorem 13. Suppose f(θ∗) = 0, and Assumptions (F1–F2) and (N1–N2) hold. Suppose in addi-\ntion that there exists a C2 Lyapunov function V : Rd →R+ that satisﬁes the following conditions:\n• There exist constants a, b > 0 such that\na∥θ −θ∗∥2\n2 ≤V (θ) ≤b∥θ −θ∗∥2\n2, ∀θ ∈Rd.\n(56)\n• There is a ﬁnite constant M such that\n∥∇2V (θ)∥S ≤2M, ∀θ ∈Rd.\n(57)\nWith these hypothesis, we can state the following conclusions:\n23\n1. If ˙V (θ) ≤0 for all θ ∈Rd, and if\n∞\nX\nt=0\nα2\nt < ∞,\n∞\nX\nt=0\nαtbt < ∞,\n∞\nX\nt=0\nα2\nt σ2\nt < ∞,\n(58)\nthen the iterations {θt} are bounded almost surely.\n2. Suppose further that there exists a function φ ∈B such that\n˙V (θ) ≤−φ(∥θ −θ∗∥2), ∀θ ∈Rd.\n(59)\nand in addition to (58), we also have\n∞\nX\nt=0\nαt = ∞,\n(60)\nThen θt →θ∗almost surely as t →∞.\nObserve the nice “division of labor” between the two conditions: Equation (58) guarantees\nthe almost sure boundedness of the iterations, while the addition of (60) leads to the almost sure\nconvergence of the iterations to the desired limit, namely the solution of f(θ) = 0. This division\nof labor is ﬁrst found in [14]. Theorem 13 is a substantial improvement on [7], which were the\npreviously best results. The interested reader is referred to [41] for further details.\nTheorem 13 is a slight generalization of [41, Theorem 5]. In that theorem, it is assumed that\nbt = 0 for all t, and that the constants σt are uniformly bounded by some constant σ. In this case\n(58) and (60) become\n∞\nX\nt=0\nα2\nt < ∞,\n∞\nX\nt=0\nαt = ∞.\n(61)\nThese two conditions are usually referred to as the Robbins-Monro conditions.\n4.5\nConvergence of Batch Asynchronous Stochastic Approximation\nEquations (47) through (49) represent what might be called Synchronous SA, because at each\ntime t, every component of θt is updated. Variants of synchronous SA include Asynchronous SA\n(ASA), where at each time t, exactly one component of θt is updated, and Batch Asynchronous\nSA (BASA), where at each time t, some but not necessarily all components of θt are updated. We\npresent the results for BASA, because ASA is a special case of BASA. Moreover, we focus on (48),\nwhere the objective is to ﬁnd a ﬁxed point of a contractive map g. The modiﬁcations required for\n(47) and (49) are straight-forward.\nThe relevant reference for these results is [17]. As a slight modiﬁcation of (46), it is assumed\nthat, at each time t + 1, there is available a noisy measurement\nyt+1 = g(θt) −θt + ξt+1.\n(62)\nWe assume that there is a given deterministic sequence of “step sizes” {βt}. In BASA, not every\ncomponent of θt is updated at time t. To determine which components are to be updated, we\ndeﬁne d diﬀerent binary “update processes” {κt,i}, i ∈[d]. No assumptions are made regarding\ntheir independence. At time t, deﬁne\nS(t) := {i ∈[d] : κt,i = 1}.\n(63)\n24\nThis means that\nθt+1,i = θt,i, ∀i ̸∈S(t).\n(64)\nIn order to deﬁne θt+1,i when i ∈S(t), we make a distinction between two diﬀerent approaches:\nglobal clocks and local clocks. If a global clock is used, then\nαt,i = βt, ∀i ∈S(t), αt,i = 0, ∀i ̸∈S(t).\n(65)\nIf a local clock is used, then we ﬁrst deﬁne the local counter\nνt,i =\nt\nX\nτ=0\nκτ,i, i ∈[d],\n(66)\nwhich is the total number of occasions when i ∈S(τ), 0 ≤τ ≤t. Equivalently, νt,i is the total\nnumber of times up to and including time t when θτ,i is updated. With this convention, we deﬁne\nαt,i = βνt,i, ∀i ∈S(t), αt,i = 0, ∀i ̸∈S(t).\n(67)\nThe distinction between global clocks and local clocks was apparently introduced in [6]. Traditional\nRL algorithms such as TD(λ) and Q-learning, discussed in detail in Section 3.3 and again in Sections\n5.2 and 5.3, use a global clock. That is not surprising because [6] came after [31] and [42]. It is\nshown in [17] that the use of local clocks actually simpliﬁes the analysis of these algorithms.\nNow we present the BASA updating rules. Let us deﬁne the “step size vector” αt ∈Rd\n+ via\n(65) or (67) as appropriate. Then the update rule is\nθt+1 = θt + αt ◦yt+1,\n(68)\nwhere yt+1 is deﬁned in (62). Here, the symbol ◦denotes the Hadamard product of two vectors of\nequal dimensions. Thus if a, b have the same dimensions, then c = a ◦b is deﬁned by ci = aibi for\nall i.\nRecall that we are given a function g : Rd →Rd, and the objective is to ﬁnd a solution to the\nﬁxed-point equation g(θ) = θ. Towards this end, we begin by stating the assumptions about the\nnoise sequence.\n(N1’) There exists a sequence of constants {bt} such that\nE(∥ξt+1∥2|Ft) ≤bt(1 + ∥θt\n0∥∞), ∀t ≥0.\n(69)\n(N2’) There exists a sequence of constants {σt} such that\nE(∥ξt+1 −E(ξt+1|Ft)∥2\n2|Ft) ≤σ2\nt (1 + ∥θt\n0∥2\n∞), ∀t ≥0.\n(70)\nComparing (54) and (55) with (69) and (70) respectively, we see that the term ∥θt∥2\n2 is replaced\nby ∥θt\n0∥∞. So the constants bt and σt can be diﬀerent in the two cases. But because the two\nformulations is quite similar, we denote the ﬁrst set of conditions as (N1) and (N2), and the second\nset of conditions as (N1’) and (N2’).\nNext we state conditions on the step size sequence, which allow us to state the theorems in a\ncompact manner. Next, we state the assumptions on the step size sequence. Note that, if a local\nclock is used, then αt,i can be random even if βt is deterministic.\n25\n(S1) The random step size sequences {αt,i} and the sequences {bt}, {σ2\nt } and satisfy\n∞\nX\nt=0\nα2\nt,i < ∞,\n∞\nX\nt=0\nσ2\nt α2\nt,i < ∞,\n∞\nX\nt=0\nbtαt,i < ∞, a.s., ∀i ∈[d].\n(71)\n(S2) The random step size sequence {αt,i} satisﬁes\n∞\nX\nt=0\nαt,i = ∞, a.s., ∀i ∈[d].\n(72)\nFinally we state an assumption about the map g.\n(G) g is a contraction with respect to the ℓ∞-norm with some contraction constant γ < 1.\nTheorem 14. Suppose that Assumptions (N1’) and (N2’) about the noise sequence, (S1) about the\nstep size sequence, and (G) about the function g hold. Then supt ∥θt∥∞< ∞almost surely.\nTheorem 15. Let θ∗denote the unique ﬁxed point of g. Suppose that Assumptions (N1’) and\n(N2’) about the noise sequence, (S1) and (S2) about the step size sequence, and (G) about the\nfunction g hold. Then θt converges almost surely to θ∗as t →∞.\nThe proofs of these theorems can be found in [17].\n5\nApplications to Reinforcement Learning\nIn this section, we apply the contents of the previous section to derive suﬃcient conditions for two\ndistinct RL algorithms, namely Temporal Diﬀerence Learning (without function approximation),\nand Q-Learning. Previously known results are stated in Section 3.3. So what is the need to re-\nanalyze those algorithms again from the standpoint of stochastic approximation? There are two\nreasons for doing so. First, the historical TD(λ) and Q-Learning algorithms are stated using a\n“global clock” as deﬁned in Section 4.5. Subsequently, the concept of a “local clock” is introduced\nin [6]. In [17], the authors build upon this distinction to achieve two objectives. First, when a\nlocal clock is used, there are fewer assumptions. Second, by proving a result on the sample paths\nof an irreducible Markov process (proved in [17]), probabilistic conditions such as (38)–(39) and\n(44)–(45) are replaced by purely algebraic conditions.\n5.1\nA Useful Theorem About Irreducible Markov Processes\nTheorem 16. Suppose {N(t)} is a Markov process on [d] with a state transition matrix A that is\nirreducible. Suppose {βt}t≥0 is a sequence of real numbers in (0, 1) such that βt+1 ≤βt for all t,\nand\n∞\nX\nt=0\nβt = ∞.\n(73)\nThen\n∞\nX\nt=0\nβtI{N(t)=i}(ω) =\n∞\nX\nt=0\nβtfi(N(t)(ω)) = ∞, ∀i ∈[d], ∀ω ∈Ω0,\n(74)\nwhere I denotes the indicator function.\n26\n5.2\nTD–Learning Without Function Approximation\nRecall the TD(λ) algorithm without function approximation, presented in Section 3.3. One observes\na time series {(Xt, R(Xt))} where {Xt} is a Markov process over X = {x1, · · · , xn} with a (possibly\nunknown) state transition matrix A, and R : X →R is a known reward function. With the sample\npath {Xt} of the Markov process, one can associate a corresponding “index process” {Nt} taking\nvalues in [n], as follows:\nNt = i if Xt = xi ∈X.\nIt is obvious that the index process has the same transition matrix A as the process {Xt}. The\nidea is to start with an initial estimate ˆv0, and update it at each time t based on the sample path\n{(Xt, Rt)}.\nNow we recall the TD(λ) algorithm without function approximation. At time t, let ˆvt ∈Rn\ndenote the current estimate of v. Let {Nt} be the index process deﬁned above. Deﬁne the “temporal\ndiﬀerence”\nδt+1 := RNt + γ ˆVt,Nt+1 −ˆVt,Nt, ∀t ≥0,\n(75)\nwhere ˆVt,Nt denotes the Nt-th component of the vector ˆvt. Equivalently, if the state at time t is\nxi ∈X and the state at the next time t + 1 is xj, then\nδt+1 = Ri + γ ˆVt,j −ˆVt,i.\n(76)\nNext, choose a number λ ∈[0, 1). Deﬁne the “eligibility vector”\nzt =\nt\nX\nτ=0\n(γλ)τI{Nt−τ =Nt}eNt−τ,\n(77)\nwhere eNs is a unit vector with a 1 in location Ns and zeros elsewhere. Finally, update the estimate\nˆvt as\nˆvt+1 = ˆvt + δt+1αtzt,\n(78)\nwhere αt is the step size chosen in accordance with either a global or a local clock. The distinction\nbetween the two is described next.\nTo complete the problem speciﬁcation, we need to specify how the step size αt is chosen in (78).\nThe two possibilities studied here are: global clocks and local clocks. If a global clock is used, then\nαt = βt, whereas if a local clock is used, then αt = βνt,i, where\nνt,i =\nt\nX\nτ=0\nI{zτ,i̸=0}.\nNote that in the traditional implementation of the TD(λ) algorithm suggested in [31, 38, 16], a\nglobal clock is used. Moreover, the algorithm is shown to converge provided\n∞\nX\nt=0\nα2\nt < ∞,\n∞\nX\nt=0\nαt = ∞, a.s.\n(79)\nAs we shall see below, the theorem statements when local clocks are used involve slightly fewer\nassumptions than when global clocks are used. Moreover, neither involves probabilistic conditions\nsuch as (38) and (39), in contrast to Theorem 9.\nNext we present two theorems regarding the convergence of the TD(0) algorithm.\nAs the\nhypotheses are slightly diﬀerent, they are presented separately. But the proofs are quite similar,\nand can be found in [17].\n27\nTheorem 17. Consider the TD(λ) algorithm using a local clock to determine the step size. Suppose\nthat the state transition matrix A is irreducible, and that the deterministic step size sequence {βt}\nsatisﬁes the Robbins-Monro conditions\n∞\nX\nt=0\nβt = ∞,\n∞\nX\nt=0\nβ2\nt < ∞.\nThen vt →v almost surely as t →∞.\nTheorem 18. Consider the TD(λ) algorithm using a global clock to determine the step size. Sup-\npose that the state transition matrix A is irreducible, and that the deterministic step size sequence\nis nonincreasing (i.e., βt+1 ≤βt for all t), and satisﬁes the Robbins-Monro conditions as described\nabove. Then vt →v almost surely as t →∞.\n5.3\nQ-Learning\nThe Q-learning algorithm proposed in [42] is now recalled for the convenience of the reader.\n1. Choose an arbitrary initial guess Q0 : X × U →R and an initial state X0 ∈X.\n2. At time t, with current state Xt = xi, choose a current action Ut = uk ∈U, and let the\nMarkov process run for one time step. Observe the resulting next state Xt+1 = xj. Then\nupdate the function Qt as follows:\nQt+1(xi, uk) = Qt(xi, uk) + βt[R(xi, uk) + γVt(xj) −Qt(xi, uk)],\nQt+1(xs, wl) = Qt(xs, wl), ∀(xs, wl) ̸= (xi, uk).\n(80)\nwhere\nVt(xj) = max\nwl∈U Qt(xj, wl),\n(81)\nand {βt} is a deterministic sequence of step sizes.\n3. Repeat.\nIn earlier work such as [36, 16], it is shown that the Q-learning algorithm converges to the optimal\naction-value function Q∗provided\n∞\nX\nt=0\nβtI(Xt,Ut)=(xi,uk) = ∞, ∀(xi, uk) ∈X × U,\n(82)\n∞\nX\nt=0\nβ2\nt I(Xt,Ut)=(xi,uk) < ∞, ∀(xi, uk) ∈X × U.\n(83)\nThese conditions are stated here as Theorem 11. Similar hypotheses are present in all existing\nresults in asynchronous SA. Note that in the Q-learning algorithm, there is no guidance on how to\nchoose the next action Ut. Presumably Ut is chosen so as to ensure that (82) and (83) are satisﬁed.\nHowever, we now demonstrate a way to avoid such conditions, by using Theorem 16. We also\nintroduce batch updating and show that it is possible to use a local clock instead of a global clock.\nThe batch Q-learning algorithm introduced here is as follows:\n28\n1. Choose an arbitrary initial guess Q0 : X × U →R, and m initial states Xk\n0 ∈X, k ∈[m], in\nsome fashion (deterministic or random). Note that the m initial states need not be distinct.\n2. At time t, for each action index k ∈[m], with current state Xk\nt = xk\ni , choose the current\naction as Ut = uk ∈U, and let the Markov process run for one time step.\nObserve the\nresulting next state Xk\nt+1 = xk\nj . Then update function Qt as follows, once for each k ∈[m]:\nQt+1(xk\ni , uk) =\n\u001a Qt(xk\ni , uk) + αt,i,k[R(xi, uk) + γVt(xk\nj ) −Qt(xk\ni , uk)],\nif xs = xk\ni ,\nQt(xk\ns, uk),\nif xk\ns ̸= xk\ni .\n(84)\nwhere\nVt(xk\nj ) = max\nwl∈U Qt(xk\nj , wl).\n(85)\nHere αt,i,k equals βt for all i, k if a global clock is used, and equals\nαt,i,k =\nt\nX\nτ=0\nI{Xk\nt =xi}\n(86)\nif a local clock is used.\n3. Repeat.\nRemark: Note that m diﬀerent simulations are being run in parallel, and that in the k-th\nsimulation, the next action Ut is always chosen as uk. Hence, at each instant of time t, exactly m\ncomponents of Q(·, ·) (viewed as an n × m matrix) are updated, namely the (Xk\nt , uk) component,\nfor each k ∈[m]. In typical MDPs, the size of the action space m is much smaller than the size of\nthe state space n. For example, in the Blackjack problem discussed in [33, Chapter 4], n ∼2100\nwhile m = 2! Therefore the proposed batch Q-learning algorithm is quite eﬃcient in practice.\nNow, by ﬁtting this algorithm into the framework of Theorem 16, we can prove the following\ngeneral result. The proof can be found in [17].\nTheorem 19. Suppose that each matrix Auk is irreducible, and that the step size sequence {βt}\nsatisﬁes the Robbins-Monro conditions (61) with αt replaced by βt. With this assumption, we have\nthe following:\n1. If a local clock is used as in (84), then Qt converges almost surely to Q∗.\n2. If a global clock is used (i.e., αt,i,k = βt for all t, i, k), and {βt} is nonincreasing, then Qt\nconverges almost surely to Q∗.\nRemark: Note that, in the statement of the theorem, it is not assumed that every policy π\nleads to an irreducible Markov process – only that every action leads to an an irreducible Markov\nprocess. In other words, the assumption is that the m diﬀerent matrices Auk, k ∈[m] correspond\nto irreducible Markov processes. This is a substantial improvement. It is shown in [37] that the\nfollowing problem is NP-hard: Given an MDP, determine whether every policy π results in a Markov\nprocess that is a unichain, that is, consists of a single set of recurrent states with the associated state\ntransition matrix being irreducible, plus possibly some transient states. Our problem is slightly\ndiﬀerent, because we don’t permit any transient states. Nevertheless, this problem is also likely to\nbe very diﬃcult. By not requiring any condition of this sort, and also by dispensing with conditions\nanalogous to (82) and (83), the above theorem statement is more useful.\n29\n6\nConclusions and Problems for Future Research\nIn this brief survey, we have attempted to sketch some of the highlights of Reinforcement Learning.\nOur viewpoint, which is quite mainstream, is to view RL as solving Markov Decision Problems\n(MDPs) when the underlying dynamics are unknown. We have used the paradigm of Stochastic\nApproximation (SA) as a unifying approach.\nWe have presented convergence theorems for the\nstandard approach, which might be thought of as “synchronous” SA, as well as variants such as\nAsynchronous SA (ASA) and Batch Asynchronous SA (BASA). Many of these results are due to\nthe author and his collaborators.\nIn this survey, due to length limitations, we have not discussed actor-critic algorithms. These\ncan be viewed as applications of the policy gradient theorem [32, 26] coupled with stochastic\napproximation applied to two-time scale (i.e., singularly perturbed) systems [8, 24]. Some other\nrelevant references are [21, 20, 19]. Also, the rapidly emerging ﬁeld of Finite-Time SA has not been\ndiscussed. FTSA can lead to estimates of the rate of convergence of various RL algorithms, whereas\nconventional SA leads to only asymptotic results. Some recent relevant papers include [11, 12].\nReferences\n[1] Aristotle Arapostathis, Vivek S. Borkar, Emmanuel Fern´andez-Gaucherand, Mrinal K. Ghosh,\nand Steven I. Marcus. Discrete-time controlled Markov processes with average cost criterion:\nA survey.\nSIAM Journal of Control and Optimization, 31(2):282–344, 1993.\n[2] M. Benaim. Dynamics of stochastic approximation algorithms. Springer Verlag, 1999.\n[3] Albert Benveniste, Michel Metivier, and Pierre Priouret. Adaptive Algorithms and Stochastic\nApproximation. Springer-Verlag, 1990.\n[4] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.\n[5] Julius R. Blum. Multivariable stochastic approximation methods. Annals of Mathematical\nStatistics, 25(4):737–744, 1954.\n[6] V. S. Borkar. Asynchronous stochastic approximations. SIAM Journal on Control and Opti-\nmization, 36(3):840–851, 1998.\n[7] V. S. Borkar and S. P. Meyn. The O.D.E. method for convergence of stochastic approximation\nand reinforcement learning. SIAM Journal on Control and Optimization, 38:447–469, 2000.\n[8] Vivek S. Borkar. Stochastic approximation in two time scales. Systems & Control Letters,\n29(5):291–294, February 1997.\n[9] Vivek S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge\nUniversity Press, 2008.\n[10] Vivek S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint (Second Edition).\nHindustan Book Agency, 2022.\n[11] Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-\nsample analysis of contractive stochastic approximation using smooth convex envelopes.\narxiv:2002.00874v4, October 2020.\n30\n[12] Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-\nsample analysis of oﬀ-policy td-learning via generalized bellman operators. arxiv:2106.12729v1,\nJune 2021.\n[13] Rick Durrett. Probability: Theory and Examples (5th Edition). Cambridge University Press,\n2019.\n[14] E. G. Gladyshev. On stochastic approximation. Theory of Probability and Its Applications,\nX(2):275–278, 1965.\n[15] Wolfgang Hahn. Stability of Motion. Springer-Verlag, 1967.\n[16] Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. Convergence of stochastic iterative\ndynamic programming algorithms. Neural Computation, 6(6):1185–1201, November 1994.\n[17] Rajeeva L. Karandikar and M. Vidyasagar. Convergence of batch asynchronous stochastic\napproximation with applications to reinforcement learning. arxiv:2109.03445v2, July 2022.\n[18] Hassan K. Khalil. Nonlinear Systems (Third Edition). Prentice Hall, 2002.\n[19] V. Konda and J. Tsitsiklis. On actor-critic algorithms. SIAM Journal on Control and Opti-\nmization, 42(4):1143–1166, 2003.\n[20] Vijay R. Konda and John N. Tsitsiklis. Actor-critic algorithms. In Neural Information Pro-\ncessing Systems (NIPS1999), pages 1008–1014, 1999.\n[21] Vijaymohan R. Konda and Vivek S. Borkar.\nActor-critic learning algorithms for Markov\ndecision processes. SIAM Journal on Control and Optimization, 38(1):94–123, 1999.\n[22] Harold J. Kushner and Dean S. Clark. Stochastic Approximation Methods for Constrained and\nUnconstrained Systems. Applied Mathematical Sciences. Springer-Verlag, 1978.\n[23] Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms\nand Applications. Springer-Verlag, 1997.\n[24] Chandrashekar Lakshminarayanan and Shalabh Bhatnagar.\nA stability criterion for two\ntimescale stochastic approximation schemes. Automatica, 79:108–114, 2017.\n[25] Lennart Ljung. Strong convergence of a stochastic approximation algorithm. Annals of Statis-\ntics, 6:680–696, 1978.\n[26] Peter Marbach and John N. Tsitsiklis. Simulation-based optimization of markov reward pro-\ncesses. IEEE Transactions on Automatic Control, 46(2):191–209, February 2001.\n[27] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.\nJohn Wiley, 2005.\n[28] Herbert Robbins and Sutton Monro. A stochastic approximation method. Annals of Mathe-\nmatical Statistics, 22(3):400–407, 1951.\n[29] Claude E. Shannon. Programming a computer for playing chess.\nPhilosophical Magazine,\nSer.7, 41(314), March 1950.\n31\n[30] Mark W. Spong, Seth R. Hutchinson, and M. Vidyasagar. Robot Modeling and Control (Second\nEdition). John Wiley, 2020.\n[31] R. S. Sutton. Learning to predict by the method of temporal diﬀerences. Machine Learning,\n3(1):9–44, 1988.\n[32] R. S. Sutton, D. McAllester, S. Singh, and Y.Mansour. Policy gradient methods for reinforce-\nment learning with function approximation. In Advances in Neural Information Processing\nSystems 12 (Proceedings of the 1999 conference), pages 1057–1063. MIT Press, 2000.\n[33] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction (Second\nEdition). MIT Press, 2018.\n[34] Csaba Szepesv´ari. Algorithms for Reinforcement Learning. Morgan and Claypool, 2010.\n[35] Vladimir B. Tadi´c. Almost sure convergence of two time-scale stochastic approximation al-\ngorithms. In Proceedings of the American Control Conference, volume 4, pages 3802–3807,\n2004.\n[36] John N. Tsitsiklis. Asynchronous stochastic approximation and q-learning. Machine Learning,\n16:185–202, 1994.\n[37] John N. Tsitsiklis. NP-Hardness of checking the unichain condition in average cost MDPs.\nOperations Research Letters, 35:319–323, 2007.\n[38] John N. Tsitsiklis and Benjamin Van Roy. An analysis of temporal-diﬀerence learning with\nfunction approximation. IEEE Transactions on Automatic Control, 42(5):674–690, May 1997.\n[39] M. Vidyasagar. Nonlinear Systems Analysis (SIAM Classics Series). Society for Industrial\nand Applied Mathematics (SIAM), 2002.\n[40] M. Vidyasagar. Hidden Markov Processes: Theory and Applications to Biology. Princeton\nUniversity Press, 2014.\n[41] M. Vidyasagar. Convergence of stochastic approximation via martingale and converse Lya-\npunov methods. arxiv:2205.01303v1, May 2022.\n[42] C. J. C. H. Watkins and P. Dayan. Q-learning. Machine Learning, 8(3-4):279–292, 1992.\n32\n",
  "categories": [
    "cs.LG",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2023-04-03",
  "updated": "2023-04-03"
}