{
  "id": "http://arxiv.org/abs/2304.02251v1",
  "title": "ERRA: An Embodied Representation and Reasoning Architecture for Long-horizon Language-conditioned Manipulation Tasks",
  "authors": [
    "Chao Zhao",
    "Shuai Yuan",
    "Chunli Jiang",
    "Junhao Cai",
    "Hongyu Yu",
    "Michael Yu Wang",
    "Qifeng Chen"
  ],
  "abstract": "This letter introduces ERRA, an embodied learning architecture that enables\nrobots to jointly obtain three fundamental capabilities (reasoning, planning,\nand interaction) for solving long-horizon language-conditioned manipulation\ntasks. ERRA is based on tightly-coupled probabilistic inferences at two\ngranularity levels. Coarse-resolution inference is formulated as sequence\ngeneration through a large language model, which infers action language from\nnatural language instruction and environment state. The robot then zooms to the\nfine-resolution inference part to perform the concrete action corresponding to\nthe action language. Fine-resolution inference is constructed as a Markov\ndecision process, which takes action language and environmental sensing as\nobservations and outputs the action. The results of action execution in\nenvironments provide feedback for subsequent coarse-resolution reasoning. Such\ncoarse-to-fine inference allows the robot to decompose and achieve long-horizon\ntasks interactively. In extensive experiments, we show that ERRA can complete\nvarious long-horizon manipulation tasks specified by abstract language\ninstructions. We also demonstrate successful generalization to the novel but\nsimilar natural language instructions.",
  "text": "IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED MARCH, 2022\n1\nERRA: An Embodied Representation and Reasoning Architecture for\nLong-horizon Language-conditioned Manipulation Tasks\nChao Zhao*, Shuai Yuan*, Chunli Jiang, Junhao Cai,\nHongyu Yu, Michael Yu Wang, and Qifeng Chen\nAbstract—This letter introduces ERRA, an embodied learning\narchitecture that enables robots to jointly obtain three funda-\nmental capabilities (reasoning, planning, and interaction) for\nsolving long-horizon language-conditioned manipulation tasks.\nERRA is based on tightly-coupled probabilistic inferences at\ntwo granularity levels. Coarse-resolution inference is formulated\nas sequence generation through a large language model, which\ninfers action language from natural language instruction and\nenvironment state. The robot then zooms to the ﬁne-resolution\ninference part to perform the concrete action corresponding to\nthe action language. Fine-resolution inference is constructed as\na Markov decision process, which takes action language and\nenvironmental sensing as observations and outputs the action.\nThe results of action execution in environments provide feedback\nfor subsequent coarse-resolution reasoning. Such coarse-to-ﬁne\ninference allows the robot to decompose and achieve long-\nhorizon tasks interactively. In extensive experiments, we show\nthat ERRA can complete various long-horizon manipulation tasks\nspeciﬁed by abstract language instructions. We also demonstrate\nsuccessful generalization to the novel but similar natural language\ninstructions.\nIndex Terms—Manipulation, Large Language Model (LLM),\nReasoning, Reinforcement Learning, Human-robot interaction\nI. INTRODUCTION\nIf robots are to be widely deployed in workplaces, hospitals,\nand our homes to assist us, they must understand our needs,\ndiscover the underlying causal relations of environments, and\ninteract with the environment appropriately. An example is\nthe case of long-horizon manipulation tasks speciﬁed by\nnatural language. For example, when humans hear a request\nsuch as “Please put the cosmetic in the drawer”, we can\nsimultaneously understand the sentence’s semantics and observe\nthe surroundings to determine whether we need to “open\nthe drawer” ﬁrst or “grasp the cosmetic.” We then observe\nthe outcomes of attempted concrete action and plan next. In\naddition, we can take corrective measures from failure cases\n(e.g., cosmetic slips from our hands). To operate in our world,\nrobots must replicate such abilities.\nThis is the motivation for the problem tackled in this paper,\nwhich is a robot that has the following abilities: (i) reason\nManuscript received: December, 5, 2022; Revised February, 28, 2023;\nAccepted March, 29, 2023.\nThis paper was recommended for publication by Editor Hong Liu upon\nevaluation of the Associate Editor and Reviewers’ comments.\n*Authors with equal contribution. C. Zhao, S. Yuan, C. Jiang, J. Cai, H. Yu,\nand Q. Chen are with The Hong Kong University of Science and Technol-\nogy, Clear Water Bay, Hong Kong {czhaobb, syuanaf, cjiangab,\njcaiaq}@connect.ust.hk and {hongyuyu, cqf}@ust.hk. J. Cai,\nH. Yu, and M. Wang are also with HKUST Shenzhen-Hong Kong Collaborative\nInnovation Research Institute, Futian, Shenzhen. M. Wang is with Monash\nUniversity michael.y.wang@monash.edu\nDigital Object Identiﬁer (DOI): see the top of this page.\nEnvironment\nState\nInstruction: “Please clean the table”\nCoarse-resolution Inference\nAction language: Grasp an object\nFine-resolution inference\nConcrete action\nGrasp an object\nPut the object into the bin\nDone\nFig. 1: ERRA Overview. The image sequence at the top shows the action\nlanguage and execution process to complete a task with the instruction “Please\nclean the table,” utilizing ERRA. Given a language instruction, the coarse-\nresolution inference produces the next step represented by action language,\naccording to the environment state. The action language and state are then\nprocessed by ﬁne-resolution inference, which outputs the concrete action to\ninteract with the environment.\nabstract nature language instructions and plan with the causal\nrelation of the environment, (ii) develop motor skills to interact\nwith environments, and complete long-horizon manipulation\ntasks, (iii) detect failures (e.g., accidentally drop an object) and\ncorrect them (e.g., grasp the object again). Endowing robots\nwith the combination of abilities (i)-(iii) is a grand challenge\nbecause the long-horizon manipulation tasks with abstract\nlanguage instructions, for example, “clean trash on the table,”\nrequires the embodied agent to have semantic knowledge and a\nreliable interpretation of the environment, to successfully plan\nand perform a long sequence of motor skills, and to know when\nto stop (i.e., no trash on the table). While conventional methods,\nsuch as symbolic programming or hierarchical reinforcement\nlearning, can plan tasks, most approaches rely on carefully\ndesigned representations and analytical transition models, which\nlimit generalization. Recently, a few studies have explored the\nuse of pre-trained large language models (LLMs) to answer\nquestions that require reasoning and planning through prompt\ndesign (i.e., hand-crafted text prompts) and utilizing such\nability for long-horizon robot manipulation [1]–[3]. However,\nan important problem with these approaches is that there is\nno guarantee of what manipulation tasks LLMs can reason\nabout and plan without trying because LLMs lack real-world\nexperience during their original training. Furthermore, small\nchanges in prompts can deteriorate the performance of LLMs,\nmaking ﬁnding appropriate prompts time-consuming.\narXiv:2304.02251v1  [cs.RO]  5 Apr 2023\n2\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED MARCH, 2022\nTo address the above problems and endow robots with\nabilities (i)-(iii), we propose the ERRA framework based\non tightly-coupled probabilistic inferences at two levels of\ngranularity, coarse and ﬁne. An overview of ERRA is shown\nin Fig. 1. The coarse-resolution inference focuses on high-level\nreasoning and planning (i.e., what to do in the next step?),\nand the ﬁne-resolution inference focuses on learning concrete\nactions (i.e., how to do it?). The results of executing concrete\nactions in the environment provide feedback for subsequent\ncoarse-resolution inferences. Such coarse-to-ﬁne inferences are\ninvoked repeatedly to decompose long-horizon manipulation\ntasks as a sequence of concrete actions. Coarse-resolution\ninference is built on a pre-trained large language model for\ngenerating the action language. Motor skills in ﬁne-resolution\ninference are learned through reinforcement learning (RL) under\nself-supervision.\nThe primary contribution of this work is to suggest a new\napproach, ERRA, that allows an embodied agent to acquire\nreasoning, planning, and interaction abilities for solving long-\nhorizon manipulation tasks speciﬁed by natural language. Exten-\nsive experiments show that ERRA is capable of understanding\nthe semantics in abstract language instructions, reasoning\nin environments with rich functional relationships between\nobjects, and providing motor skills to complete long-horizon\nmanipulation tasks. We also show that ERRA allows the robot to\nrecover from failure cases and adapt to environmental changes\nin the real world, signiﬁcantly improving the robustness of\nrobots in dynamic environments.\nII. RELATED WORK\nTask and Motion Planning. In robotics, task and motion\nplanning [4] is capable of solving long-horizon (i.e., multi-\nstep) manipulation tasks. Traditional methods rely on symbolic\nplanning [5] or optimization [6] in abstract or symbolic spaces.\nHowever, most approaches require manually deﬁned repre-\nsentation spaces and environment kinematics models, which\nare usually domain-speciﬁc and lack generalization ability.\nMore recently, LLMs have demonstrated dawning properties\non reasoning and planning under appropriate conditions (e.g.,\nlanguage prompts) [7]–[10]. Several works [3] have studied\nusing LLMs to plan robot manipulation tasks. SayCan [1] uses\nLLM to infer the entire plans of the manipulation task and\nestimate the feasibility of each step using a model of action\naffordance. However, these methods assume that the execution\nof each planned motor skill is faultless, making them not\nrobust to intermediate failures in task execution. In this aspect,\n[2] introduces additional modules to incorporate human and\nenvironmental feedback to improve the completion of tasks.\nWhile prior works have investigated how LLMs plan via prompt\ndesign, the ability of LLMs is agnostic, requiring time and\nhuman effort to design and experiment with different prompts.\nWe introduce the prompt tuning method [11], enabling the LLM\nto be a reasoner and planner without using design prompts.\nLearning Language-Conditioned Manipulation. Natural\nlanguage provides a human-interactive interface to link humans\nto robots, which is important for deploying robots in our\nlives. Many studies [12]–[16] have explored how robots\nfollow language instructions, in which robots are required\nto complete tasks speciﬁed by the language. Some studies\n[17]–[19] have learned language-conditioned behaviors through\nimitation learning. For example, [20] learns a direct mapping\nfrom images and natural language instructions to actions using\na Transformer network. [21] uses an ofﬂine robotics dataset\nwith crowdsourced natural language labels to learn a range of\nvision-based manipulation tasks. Most of these works focus\non learning short-horizon manipulation tasks such as grasping\nor in-hand manipulation. In contrast, ERRA can understand\ninstructions with abstract semantics and achieve long-horizon\ntasks by leveraging LLMs’ semantic knowledge to interpret\ninstructions and plan tasks.\nReinforcement Learning for Manipulation. Reinforce-\nment learning combined with deep learning has recently made\nextensive progress in learning skills in different domains, such\nas beyond human experts at the games of Go [22] and Atari\n[23]. In robotic manipulation, reinforcement learning offers the\nrobot a way to acquire various manipulation skills through self-\nexploration [24]–[26]. However, most studies focus on learning\nnarrow and individual tasks. Some works achieve long-horizon\ntask planning by hierarchical reinforcement learning [27], [28],\nwhich requires manual task-level design and lacks general-\nization ability. In our work, ERRA leverages reinforcement\nlearning to acquire low-level motor skills in the simulation\nand cooperates with the coarse-resolution inference module to\nperform long-horizon manipulation tasks.\nIII. METHOD\nIn this section, we describe the architecture of ERRA, as\nshown in Fig. 2. ERRA is based on two inference modules,\ncoarse and ﬁne. The coarse-resolution module infers an\naction language (e.g., grasp the apple) based on language\ninstruction, environment state, and robot proprioception. The\naction language corresponds to a motor skill that the robot needs\nto execute. Subsequently, the ﬁne-resolution inference module\ngenerates concrete actions for executing the motor skill, using\ninputs of the action language inferred by the coarse-resolution\ninference module and the visual and tactile information. By\niteratively invoking the coarse-to-ﬁne inference process, a task\ncan be decomposed into simpler concrete actions and executed.\nThe step-by-step planning and execution processes of ERRA\nenable feedback to be established and mitigate the challenges\nof reasoning and planning, resulting in effective and robust\nperformance. In the following sections, we describe the problem\nformulation of the coarse and ﬁne-resolution inferences and\nelucidate the supervised learning and reinforcement learning\napproaches we adopted to train these two inference modules\nin the simulation.\nA. Coarse-resolution Inference\nThe objective of learning the coarse-resolution inference\nis to obtain a high-level manipulation planning strategy, as\nshown in Fig. 2A. The coarse-resolution inference is formulated\nas a sequence-to-sequence text generation task, in which the\ngenerated action language guides ﬁne-resolution inference to\npredict concrete actions performed by the robot.\nZHAO et al.: ERRA\n3\nA: Learning Coarse-resolution Inference in Simulation\nState S\nInstruction: “Please put the can into drawer”\nTactile signal: 0\nDepth image\nAction language label\nΛ\nT\nI\nCl\nΛe\nTe\nIe\nConcatenate\nConcatenate\nFCs\nEmbedding\nEmbedding\nCLIP encoder\nNetwork architecture\nInput sequence\nGoogle T5\nsoft prompts\nC\n“Open the drawer”\nCross-entropy loss L\nB: Learning Fine-resolution Inference in Simulation\nState st\nAction language: “Open the drawer”\nTactile signal: 0\nDepth image\nPolicy architecture\nMLP encoder\nConv encoder\nTrain with PPO\nAction at\nReward rt\nAuxiliary reward rau\nC: Coarse-to-ﬁne Inference in Real World\nInstruction: “Please put the cosmetic into drawer”\nDepth image\nTactile signal: 0\n“Open the drawer”\nCoarse-resolution\ninference\nFine-resolution\ninference\nFig. 2: System Overview. A: We generate a set of correspondences (S, C) in the simulation to learn the coarse-resolution inference. State S includes the\ninstruction Λ, depth image I, and tactile signal T. The provided inputs are encoded as three vectors (Λe, Te, Ie), respectively. These vectors are concatenated\nand subsequently fed to Google T5. At last, the output action language C and label of action language Cl are used to compute loss; B: To learn ﬁne-resolution\ninference, we employ PPO. The RL agent takes the state st as input and predicts the action at for the robot execution at time step t. The agent then obtains\nrewards rt and auxiliary reward rau from the simulation; C: We deploy the ERRA in the real world. Given instruction, the coarse-resolution module infers the\naction language based on current observation and the tactile signal. Then the ﬁne-resolution module predicts actions with the inputs of action language and\nenvironment state.\nProblem Formulation: Formally, it is a mapping p : S →C,\nwhere S = (Λ, I, T) is the input state and C = (c1, c2, ..., cm)\nis a sequence of text that represents the action language. The\nstate S consists of three parts: a language instruction Λ =\n(λ1, λ2, ..., λn), which is a sequence of text words; a depth\nimage I taken by the camera in the environment; and a tactile\nsignal T (i.e., a binary signal indicating the presence or absence\nof objects between ﬁngers). The coarse-resolution inference is\nconstructed as a neural network that predicts each word ci ∈\nC given the state S.\nLearning Coarse-resolution Inference: To learn the coarse-\nresolution inference, we generate a synthetic dataset D =\n(d1, d2, . . . ). The dataset is collected in simulation leverag-\ning the pre-programmed environments for various language-\nconditioned manipulation tasks. Each piece of data di contains a\ncorresponding relationship: at the current state S, what needs to\ndo next (denoted as Cl = (cl1, ..., clm)). For instance, in the ﬁrst\nexample of Fig. 3(a), the data di ∈D consists of the language\ninstruction Λ = {Please put the cosmetic into the drawer},\nthe tactile signal T = 0, the depth image I, and the label of\nthe action language Cl = {Open the drawer}.\nWe choose Google T5 [29], a large-scale pre-trained language\nmodel, as the backbone of the network, which provides beneﬁts\nof better contextual understanding and generalization ability\nfor language. As shown in Fig. 2A, the model extracts the\ninformation from the language instruction, tactile signal, and\nimage. Then, the model reasons the next action language is\n“Open the drawer,” based on the available information while\nrejecting other possibilities, such as “Grasp the cosmetic.”\nOperationally, we encode the image I with an image encoder\nfrom CLIP [30] as a dense vector Ie. The binary tactile signal\nT is transformed to a random initialized dense vector Te ∈Rn,\nwith the same dimension as the embedding vectors in T5.\nSubsequently, image embedding Ie and tactile embedding Te\nare concatenated with the word embedding Λe of the given\ninstruction Λ as the input sequence (Λe, Te, Ie), as shown in\nFig. 2A. Given such input, the model is expected to generate\nthe appropriate action language C after training.\nThe network is trained with soft-prompt tuning [11]. Con-\nventional prompt tuning methods freeze all parameters of the\npre-trained language model and use a language prompt to\nprobe it to downstream tasks [11]. In soft-prompt, the prompt\nis replaced by a group of trainable dense vectors, which avoids\nmanually designing prompts and reduces the number of training\nparameters. Fig. 3(b) shows the difference between our training\nmethod and alternatives. We add relevant soft prompts before\nthe transformer layer of T5 to control the behavior of the\nLLM. Soft prompts are parameterized by using a two-layer\nfeed-forward neural network. During training, we keep the\nlanguage model parameters constant and only ﬁne-tune the\nparameters related to these soft prompts. We learn the model\nwith the language model loss:\nL = −\nm\nX\ni=1\nlog P(ci = cli|c<i, Λ, I, T),\n(1)\n4\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED MARCH, 2022\nΛ: Please put the cosmetic\ninto the drawer\nT : 0\nCl: Open the drawer\nI\nΛ: Please put the cosmetic\ninto the drawer\nT : 0\nCl: Close the drawer\nI\nΛ: Please put the cosmetic\ninto the drawer\nT : 0\nCl: Done\nI\n(a)\nPrompt tuning\n(ERRA)\nPre-trained LLM\nFrozen weights\nPrompt design\n(e.g. SayCan)\nPre-trained LLM\nFrozen weights\nFine tuning\nPre-trained LLM\nTunable weights\nTunable soft prompt\nDesigned prompt\nInstruction\nTactile\nImage\n(b)\nFig. 3: (a) Three examples of collected data, each data di contains a state\nS = (Λ, I, T), and a label of the action language Cl; (b) Difference between\nour training and others. Instead of adjusting the parameters of LLM or using\nengineered prompts, our method introduces prompt tuning, which adds a small\nset of learnable soft prompts and shares the frozen LLM across all tasks.\nwhere m is the sequence length of C, ci ∈V is the ith word\nin sequence C, cli is the ith word of sequence Cl, and V is\nthe vocabulary.\nB. Fine-resolution Inference\nThe ﬁne-resolution inference aims to link the action language\ninferred by the coarse-resolution module to the concrete action\nthat enacts it. The ﬁne resolution module outputs the action\nparameters of the gripper for the robot to execute, to realize\nthe motor skills corresponding to the action language. These\nmotor skills are limited to four degrees of freedom in order to\nsimplify collision calculations and motion planning.\nProblem Formulation: We formulate the problem of learn-\ning ﬁne-resolution inference as a Markov Decision Process\n(MDP). An MDP comprises state space S′, action space\nA, a reward function R(st, st+1), and transition probability\nP(st+1|st, at). The RL aims to discover an optimal policy π\nthat selects action at to maximize cumulative rewards.\nLearning Fine-resolution Inference: We model the policy\nas a categorical model corresponding to a discrete-domain\nstochastic policy. The policy is trained with proximal policy\noptimization (PPO). At time step t, the agent chooses an action\nat according to the probability output by the policy π(at|st),\nand receives a reward rt from the environment.\nThe state is represented by a tuple st = (I, Le, T), where I\nis the initial depth image of the environment with a resolution\nof 240×320, T is a binary signal from the tactile sensor on the\nﬁnger, and Le is the embedded vector of the inferred action\nlanguage. Le and T are concatenated as a vector gt = (Le, T).\nThe policy network architecture comprises a convolutional\n(Conv) block and a multilayer perceptron (MLP) block, as\nshown in Fig. 2B. The depth observation I and gt are embedded\ninto two latent vectors by the Conv block and MLP block,\nrespectively. The resultant vectors are then concatenated and\npassed to the fully-connected layers (FCs) to produce an output\naction.\nThe action at consists of two components, namely gripper\npose displacement and gripper closure. The gripper pose dis-\nplacement is constructed as the difference between the current\nand desired pose of the gripper. It is formed as (xt, yt, zt, αt),\nwhere (xt, yt, zt) represents the relative displacement of the\ngripper in the workspace, and αt represents the gripper’s\nrotation about its z-axis. The displacement of the z-axis is\nexecuted last by the robot, and we ﬁx the target gripper height\nduring action execution to facilitate learning. The gripper\nclosure control is represented by a one-hot vector βt is a\none-hot vector that the gripper will be closed if βt = 1. Thus,\nthe full action is deﬁned as at = (xt, yt, zt, αt, βt), and we\ndiscretize each action coordinate according to the workspace.\nDuring operation, the robot initiates its movement along the x\nand y axes before proceeding to the z-axis.\nThe reward rt is given at the end of an episode, 1 for\nsuccessfully completing the required motor skill and 0 other-\nwise. In addition, we also provide a linear auxiliary reward\nthat encourages the robot to approach the target position. The\nauxiliary reward rau varies from 0 to 1 based on the distance\nbetween the gripper and target positions (the closer, the higher).\nC. Training Details\nCoarse-resolution and ﬁne-resolution inferences are learned\nin the Pybullet simulator [31] and then transferred to the\nreal world. In the coarse-resolution inference learning stage,\nwe generate 300 examples per task in the simulation for 17\nlanguage-conditional manipulation tasks, as shown in Fig. 5.\nSpeciﬁcally, we vary a task from the following three aspects:\nobject types, object positions, and initial setups. For example,\nin the “Put something into the drawer” task, we consider\ndifferent objects, such as cosmetics or cans, and randomly\nposition them within the workspace. Additionally, we have\ntwo different initial setups where the drawer is either open or\nclosed. The generated synthetic dataset D contains over 12000\ncorresponding relationships and is used to train the coarse-\nresolution inference with cross-entropy loss. We follow the\nimplementation in [11] for the soft-prompt tuning. We use\nthe Adam optimizer [32] and a linear learning rate scheduler\nduring training. A default setting trains for ten epochs and\nuses a learning rate of 5 × 10−5. Fig. 4 shows seven scene\nsetups, and we also create similar ones in the simulation. In the\nreal world, we deploy ERRA on a UR10 arm equipped with\na parallel gripper, an Intel L515 depth camera, and a tactile\nsensor, as shown in Fig. 4(a).\nTo learn the ﬁne-resolution inference, 32 robots in simulation\nenvironments collect training episodes by obtaining the current\npolicy from the optimizer every eight epochs. In each envi-\nronment, a manipulation task speciﬁed by an action language\nis procedurally generated, which is randomly selected from\nsubsteps in 17 language-conditional manipulation tasks and\napplies the same random variations as during data collection in\nthe coarse-resolution inference learning stage. The robot in the\nsimulation environment then collects episodes, during which the\nreward is automatically determined based on whether the task\nis completed. If the robot completes the task, the environment\nwill be reset, and a new task will be generated again. At last,\nZHAO et al.: ERRA\n5\nDepth camera\nOur gripper\nTactile sensor\n(a)\n(b)\n(c)\nCut banana and clean table\nPut cosmetic into drawer\nand clean table\nClose drawer and\ngrasp knife\nCut apple\nPick all round\nobjects into box\nCut apple\nPut cosmetic into drawer\nClean table\nFig. 4: Hardware and Scene Setup. (a) Robot hardware and objects in real-world experiments; (b) Three scene setups for the hybrid tasks; (c) Four scene\nsetups for the Long-horizon tasks. Short-horizon tasks also use these scenes. An example task is shown at the bottom right in each scene setup.\nTABLE I: task family and language instruction deﬁnitions\nTask Family\nNum\nTask Explanation\nInstruction Type\nExample Instruction\nExample Task\nShort-horizon\n10\nTasks that require one reasoning step\nStraight\n“Please grasp the apple”\nRobot needs to grasp the apple\ncompleted by a single motor skill\nLong-horizon\n4\nTasks that require many reasoning steps\nAbstract\n“Please clean the table”\nRobot needs to pick up all trash\ncompleted by a range of motor skills\non the table into the bin\nHybrid\n3\nCombined long-horizon\nAbstract\n“Please put the apple into the\nRobot needs to place the apple\nand short-horizon tasks\ndrawer and clean the table”\nand clean all trash\nthe collected episodes are returned to the optimizer for learning\nthe policy. During the training, we use Adam optimizer [32]\nwith a learning rate of 10−4. We also randomize the object’s\nphysical properties during the task generation and add noise to\nthe depth observation to make the learned policy robust to the\nvarious conditions in the real world. Speciﬁcally, the object\nsize undergoes a global scaling, which entails resizing object\ndimensions within a range from 85% (min) to 115% (max) of\nits original size. Meanwhile, the spatial and visual properties are\nimpacted by adding noise to camera properties. Speciﬁcally, we\nadd noise to the camera position, camera pointing position, and\nﬁeld of view. The camera position and pointing are perturbed\nusing three-dimensional vectors, and random noise of each\ndimension is sampled from a range {−2.5 mm, 2.5 mm}.\nSimilarly, the ﬁeld of view is perturbed with a noise range\nof {−0.025◦, 0.025◦}. Supplement materials are available at:\nhttps://robotll.github.io/ERRA/\nIV. EXPERIMENTS\nWe design a set of experiments in both simulation and\nreal-world to evaluate the ERRA and other baselines in the\nlanguage-conditioned manipulation tasks. The hypotheses we\nwant t o validate are as follows:\nH1: ERRA can perform long-horizon language-conditioned\nmanipulation tasks and outperforms other baselines.\nH2: Robot proprioception is important for completing language-\nconditioned manipulation tasks.\nH3: LLMs with prompt-tuning allow ERRA to generalize to\nunseen natural language instructions.\nH4: ERRA is able to transfer to the real world.\nH5: ERRA can respond to environmental changes caused by\nhumans or its own failures.\nA. Scenes, Tasks and Evaluation Setup\nScene and task setup: Fig. 4 shows seven scene setups, and\nwe also create similar ones in the simulation. Our hardware\nsettings in the real world are also shown in Fig. 4(a). To evaluate\nERRA, we test its performance on 17 language-conditioned\nmanipulation tasks from seven scenes in both simulation\nand real-world. These tasks cover time horizons, language\ncomplexity, and variations over the robot and environment. Tab.\nI details examples for each task family, which fall into the\nfollowing:\n• Short-horizon: Short-horizon tasks are decomposed from\nlong-horizon tasks, which involve a straight language\ninstruction that needs to be achieved by a single motor\nskill. The instruction and the action language have a one-\nto-one correspondence in such tasks.\n• Long-horizon: Tasks are speciﬁed by abstract natural\nlanguage instruction and achieved by a long sequence of\nmotor skills. The correspondence between the language\ninstruction and the action language is not one-to-one and\nis affected by the environment and robot state. This tests\nthe ERRA’s ability to reason abstract instructions and to\nplan with the environment’s causal relation.\n• Hybrid: These tasks are the combination of multiple long-\nhorizon and short-horizon tasks, which have a higher\ncomplexity than others.\nBaseline comparisons: We compare with the following\napproaches:\n• Infer-all: It is similar to the architecture of SayCan [1],\nin which all action languages are inferred together, and\nthen the robot executes them one by one without feedback\nduring the entire task planning and execution process.\n6\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED MARCH, 2022\nPlace the round object\nGrasp a round object\nPlace sth into the bin\nGrasp sth on the table\nCut sth with knife\nPick the knife\nClose the drawer\nPlace sth into the drawer\nGrasp sth near the drawer\nOpen the drawer\nPick all round objects\nClean the table\nCut sth (need ﬁnd knife)\nPut sth into the drawer\nClean table and cut sth\nPlace sth and clean table\nClose drawer and grasp sth\n0%\n20%\n40%\n60%\n80%\n100%\nShort-horizon\nLong-horizon\nHybird\nPlan success rate (ERRA)\nTask success rate (ERRA)\nPlan success rate (Infer-all)\nTask success rate (Infer-all)\nFig. 5: Task performance in the simulation. From top to bottom, there are\n14 short-horizon tasks, four long-horizon tasks, and three hybrid tasks.\nTABLE II: simulation experiments\nMethod\nShort-horizon\nLong-horizon\nHybrid\nTotal\nPlan*\nTask**\nPlan\nTask\nPlan\nTask\nPlan\nTask\nInfer-all\n100%\n94%\n55%\n46%\n52%\n31%\n69%\n57%\nERRA-w/o touch\n83%\n79%\n48%\n41%\n42%\n35%\n58%\n52%\nERRA\n100%\n94%\n91%\n81%\n77%\n64%\n89%\n80%\n* Plan success rate. ∗∗Task success rate.\n• ERRA-w/o touch: An ablated version of the ERRA\nwithout the proprioceptive input (i.e., tactile information).\nBoth coarse-resolution and ﬁne-resolution modules only\nuse the camera to observe environments.\n• ERRA: We deploy the ERRA system to the robot, which\nis the full non-ablated method we propose in this article.\nMetric: We consider two evaluation metrics: plan success\nrate (successful task planning/total attempts) and task success\nrate (completed tasks/total attempts) for validating perfor-\nmance. The plan success rate is measured by whether the\nmodule of coarse-resolution inference correctly predicts all\naction languages in a language-conditioned manipulation task,\nassuming that the execution of motor skills is ﬂawless. The\ntask success rate is calculated based on whether the target\nmanipulation task is completed. It requires the coarse-resolution\nmodule to successfully plan each step and the ﬁne-resolution\nmodule to output the correct actions for the robot to complete\ncorresponding motor skills. For each task, we repeat the test\n500 times in simulation experiments and ten times in real-world\nexperiments.\nB. Simulation Results\nComparison to baselines: Tab. II shows the performance\nof ERRA on different task families in the simulation. Across\nall tasks, ERRA achieves a plan success rate of 89% and a task\nsuccess rate of 80%. In the Long-horizon and Hybrid families,\nERRA achieves 91% and 77% plan success rates, respectively.\nSame instruction: Please clean the table\nSame camera observation\nTactile: 1\nNext: Put the object into the bin\nTactile: 0\nNext: Grasp an object\nFig. 6: A case where two scenes have the same instructions and visual\nobservations but different next plans. Language and visual information are\ninsufﬁcient to plan the next step without tactile signals.\nTABLE III: generalization to unseen language instructions\nType\nShort-horizon\nLong-horizon\nHybrid\nTotal\nPlan\nTask\nPlan\nTask\nPlan\nTask\nPlan\nTask\nUnseen Verb\n99%\n94%\n77%\n68%\n51%\n42%\n76%\n68%\nUnseen Noun\n100%\n94%\n80%\n72%\n55%\n40%\n78%\n69%\nUnseen Verb + Noun\n99%\n94%\n52%\n47%\n34%\n25%\n62%\n55%\nSuch results highlight the effectiveness of coarse-resolution\ninference in enabling ERRA to reason and plan for tasks with\nlonger horizons. The plan and task success rate for each task\nis fully illustrated in Fig. 5.\nTo demonstrate the importance of incorporating robot propri-\noception, we conduct an ablation experiment by excluding the\ntactile information from inputs during ERRA training (denoted\nas ERRA-w/o touch in Tab. II). The results show that the\nplanning performance of ERRA -w/o touch is reduced by up to\n43% on the Long-horizon family and 35% on the Hybrid family.\nThis decline is attributed to the incomplete information required\nfor reasoning. In certain tasks, the relationship between state and\naction language is not one-to-one, thereby rendering reasoning\nimpossible. An example of such a scenario is presented in Fig.\n4.\nWe then investigate the effectiveness of the coarse-to-ﬁne\ninference design in the ERRA by comparing the ERRA with\nan architecture in which the planning and execution are\nindependent (denoted as infer-all in Tab. II). Our results reveal\nthat ERRA outperforms the Infer-all by over 25% on the Long-\nhorizon and Hybrid families. Infer-all’s suboptimal performance\nis due to its reliance on accurately inferring all action language\nbefore executing corresponding motor skills, which increases\nthe difﬁculty of reasoning and planning. In contrast, the ERRA\nutilizes closed-loop feedback by inferring the next step only\nafter the robot executes the previous step, leading to more\neffective and robust task performance.\nGeneralization to unseen language instructions: We study\nthe ERRA’s generalization ability to unseen natural language\ninstructions. Speciﬁcally, we test the generalization of ERRA\nat three levels of rephrased language instructions with novel\nbut similar words. First, we replace nouns in the language\ninstructions of tasks (e.g., “cut the banana” to “chop the\nbanana”), denoted as Unseen Verb in Tab. III. Second, we\nreplace verbs (e.g., “grasp the cola” to “grasp the can”), denoted\nas Unseen Noun in Tab. III. Finally, we replace both nouns\nand verbs in instructions(e.g., “close the drawer” to “shut the\ncabinet”), denoted as Unseen Verb + Noun in Tab. III.\nZHAO et al.: ERRA\n7\nInstruction: “Please put the cosmetic into the drawer”\nInstruction: “Please put the can into the drawer”\nInstruction: “Please put clean the table”\nInstruction: “Please put all round objects into the bin”\nA:\nB:\nC:\nAction language:\nOpen the drawer\nGrasp the cosmetic\nPut the cosmetic into the drawer\nClose the drawer\nDone\nAction language:\nGrasp the can\nPut the can into the drawer\nClose the drawer\nDone\nAction language:\nObject slipped\nGrasp an object\nGrasp an object\nPut the object into the bin\nDone\nAction language:\nHuman added\nGrasp a round object\nPut the round object in the bin\nGrasp a round object\nPut the round object in the bin\nDone\nFig. 7: Qualitative results of ERRA. A: Sequences of the robot successfully placing the object into the drawer at different settings (i.e., drawer is closed or\nopen); B: A sequence of the robot successfully recovering from its execution failure (i.e., object slip from gripper) and complete table cleaning; C: A sequence\nof the robot adapting to the dynamic environment. (i.e., human places another round object).\nAs shown in Tab. III, ERRA’s generalization performance\ndegrades as the complexity of the task and the number of\nunseen words in the instruction increase. Speciﬁcally, in the\nLong-horizon family, changing either the verbs or nouns leads\nto a 12% decrease in plan success rate, while changing both\nresults in a 40% decline. The decline in planning performance\nalso leads to a corresponding decrease in task success rate.\nIn contrast, ERRA remains stable in the Short-horizon family,\nowing to the lower complexity of language abstraction and\ntask planning. Notably, we observe that the performance of\nthe Hybrid family drops by up to 35% on novel instructions\ndue to the challenging language instructions and task planning\ninvolved. In conclusion, our ﬁndings suggest that the ERRA can\ngeneralize to novel language instructions, but its generalization\nperformance is affected by the complexity of the task and the\nnumber of unseen words in the instruction.\nC. Real-world Experiments\nWe also evaluate ERRA’s performance in the real world.\nERRA achieves an average task success rate of 77% across\nthree task families. The results show that the model’s reasoning,\nplanning, and interaction abilities are transferable to real-\nworld scenes for solving long-horizon language-conditioned\nmanipulation tasks. Similar to the simulation results, ERRA\nperforms best (89% success rate) on Short-horizon tasks among\nthe three task families. The performance of ERRA decreases\nas task complexity increases, achieving a success rate of 75%\non the Long-horizon family and 68% on the Hybrid family.\nThe running time for one step in the task is approximately 12\nseconds, which includes the entire cycle time from network\ninferences (less than 0.2 s) to robot execution.\nLooking back to our initial example in Sec. I, “Please put the\ncosmetic in the drawer,” we have demonstrated that ERRA is\nable to discover whether the robot needs “Open the drawer” by\nTABLE IV: real world results\nMethod\nTask Success Rate\nShort-horizon\nLong-horizon\nHybrid\nTotal\nERRA\n89%\n75%\n68%\n77%\nreasoning the causal relation of the environment (See Fig. 7A)\nand plan and execute a long sequence in the real world, which\ninclude opening the drawer, grasping the cosmetic, putting the\ncosmetic into the drawer and then closing the drawer. Note also\nthat the robot only has one arm, ERRA necessitates planning\nthe action in reasonable order (e.g., ﬁrst, open the drawer\nand then grasp the cosmetic, not the other way around). This\nrequires the ERRA to have strong abilities of long-horizon\nreasoning and understanding of semantic knowledge in the\nlanguage instruction.\nAs shown in Fig. 7, ERRA manifests robustness to dynamic\nenvironments. ERRA discovers a new round object added by the\nhuman after the last object has been put in the bin and correctly\ninfers that the next action language is “grasp a round object”\nrather than “Done” (See Fig. 7C). Such behavior is powered\nby itself, beneﬁting from the closed-loop feedback provided by\nthe coarse-to-ﬁne inference architecture. Such feedback also\nallows the robot interactively recover from failure cases. Fig.\n7B shows the ERRA response to its failure (object slip from\nhand during the task execution).\nV. CONCLUSION, LIMITATION, AND FUTURE WORK\nWe have presented a novel solution, ERRA, that utilizes\ntightly-coupled probabilistic inferences at two granularity levels,\ncoarse and ﬁne, for solving long-horizon language-conditioned\nmanipulation tasks. Through coarse-to-ﬁne inferences, complex\nmanipulation tasks can be decomposed into concrete actions\nand executed by the robot. Extensive controlled experiments\ndemonstrate the robustness and effectiveness of ERRA on\n8\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED MARCH, 2022\nmanipulation tasks with long-horizon and abstract semantics.\nOur work is not without limitations; ﬁrst, limited by hardware\ndevices, the robot‘s position is ﬁxed without the need for\nlocalization and mapping, suggesting exciting opportunities for\nextending the current work to the scene of mobile robots. Future\nresearch can also beneﬁt from the ﬂexibility and efﬁciency of a\ndual-arm system. While planning for the dual-arm system may\ninvolve evaluating a larger number of potential actions, the\nincreased ﬂexibility and redundancy provided by the additional\narm may result in more optimal ﬁnal plans, compared to\nour single-arm system. Finally, the proposed work relies on\nsimulated data to learn inference at both coarse and ﬁne\nresolutions, which is a signiﬁcant advantage that avoids a more\ntime-consuming process, such as manual labeling. However, it\nstill needs to build simulation scenes carefully. One possible\nopportunity is to develop a method that is able to learn from\nonline videos in which humans perform manipulation tasks\nwith long-term and abstract semantics.\nREFERENCES\n[1] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn,\nK. Gopalakrishnan, K. Hausman, A. Herzog, et al., “Do as i can, not\nas i say: Grounding language in robotic affordances,” in 6th Annual\nConference on Robot Learning, 2022.\n[2] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\nJ. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, T. Jackson, N. Brown,\nL. Luu, S. Levine, K. Hausman, and brian ichter, “Inner monologue:\nEmbodied reasoning through planning with language models,” in 6th\nAnnual Conference on Robot Learning, 2022.\n[3] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models\nas zero-shot planners: Extracting actionable knowledge for embodied\nagents,” arXiv preprint arXiv:2201.07207, 2022.\n[4] A. Akbari, Muhayyuddin, and J. Rosell, “Knowledge-oriented task and\nmotion planning for multiple mobile robots,” Journal of Experimental &\nTheoretical Artiﬁcial Intelligence, vol. 31, no. 1, pp. 137–162, 2019.\n[5] M. Sridharan, M. Gelfond, S. Zhang, and J. Wyatt, “Reba: A reﬁnement-\nbased architecture for knowledge representation and reasoning in robotics,”\nJournal of Artiﬁcial Intelligence Research, vol. 65, pp. 87–180, 2019.\n[6] M. A. Toussaint, K. R. Allen, K. A. Smith, and J. B. Tenenbaum,\n“Differentiable physics and stable modes for tool-use and manipulation\nplanning,” Robotics: Science and Systems Foundation, 2018.\n[7] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, Y. Chen, L. Fei-Fei,\nA. Anandkumar, Y. Zhu, and L. Fan, “Vima: General robot manipulation\nwith multimodal prompts,” arXiv preprint arXiv:2210.03094, 2022.\n[8] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud,\nD. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto,\nO. Vinyals, P. Liang, J. Dean, and W. Fedus, “Emergent abilities of large\nlanguage models,” Transactions on Machine Learning Research, 2022.\nSurvey Certiﬁcation.\n[9] D. Shah, B. Osi´nski, S. Levine, et al., “Robotic navigation with large pre-\ntrained models of language, vision, and action,” in 6th Annual Conference\non Robot Learning.\n[10] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical\ntext-conditional image generation with clip latents,” arXiv preprint\narXiv:2204.06125, 2022.\n[11] X. L. Li and P. Liang, “Preﬁx-tuning: Optimizing continuous prompts for\ngeneration,” in Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long Papers), pp. 4582–\n4597, 2021.\n[12] Y. Chen, R. Xu, Y. Lin, and P. A. Vela, “A joint network for grasp\ndetection conditioned on natural language commands,” in 2021 IEEE\nInternational Conference on Robotics and Automation (ICRA), pp. 4576–\n4582, IEEE, 2021.\n[13] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and\nH. Ben Amor, “Language-conditioned imitation learning for robot\nmanipulation tasks,” Advances in Neural Information Processing Systems,\nvol. 33, pp. 13139–13150, 2020.\n[14] C. Wang, C. Ross, Y.-L. Kuo, B. Katz, and A. Barbu, “Learning a\nnatural-language to ltl executable semantic parser for grounded robotics,”\nin Conference on Robot Learning, pp. 1706–1718, PMLR, 2021.\n[15] V. Blukis, R. Knepper, and Y. Artzi, “Few-shot object grounding and\nmapping for natural language robot instruction following,” in Conference\non Robot Learning, pp. 1829–1854, PMLR, 2021.\n[16] W. Liu, C. Paxton, T. Hermans, and D. Fox, “Structformer: Learning\nspatial structure for language-guided semantic rearrangement of novel\nobjects,” in 2022 International Conference on Robotics and Automation\n(ICRA), pp. 6322–6329, IEEE, 2022.\n[17] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine,\nand C. Finn, “Bc-z: Zero-shot task generalization with robotic imitation\nlearning,” in Conference on Robot Learning, pp. 991–1002, PMLR, 2022.\n[18] L. Shao, T. Migimatsu, Q. Zhang, K. Yang, and J. Bohg, “Concept2robot:\nLearning manipulation concepts from instructions and human demonstra-\ntions,” The International Journal of Robotics Research, vol. 40, no. 12-14,\npp. 1419–1434, 2021.\n[19] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and\nH. Ben Amor, “Language-conditioned imitation learning for robot\nmanipulation tasks,” Advances in Neural Information Processing Systems,\nvol. 33, pp. 13139–13150, 2020.\n[20] M. Shridhar, L. Manuelli, and D. Fox, “Perceiver-actor: A multi-task\ntransformer for robotic manipulation,” arXiv preprint arXiv:2209.05451,\n2022.\n[21] S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn, et al., “Learning\nlanguage-conditioned robot behavior from ofﬂine data and crowd-sourced\nannotation,” in Conference on Robot Learning, pp. 1303–1315, PMLR,\n2022.\n[22] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,\nM. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al., “A general\nreinforcement learning algorithm that masters chess, shogi, and go through\nself-play,” Science, vol. 362, no. 6419, pp. 1140–1144, 2018.\n[23] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., “Human-level control through deep reinforcement learning,” nature,\nvol. 518, no. 7540, pp. 529–533, 2015.\n[24] H.-G. Cao, W. Zeng, and I.-C. Wu, “Reinforcement learning for\npicking cluttered general objects with dense object descriptors,” in 2022\nInternational Conference on Robotics and Automation (ICRA), pp. 6358–\n6364, IEEE, 2022.\n[25] C. Zhao, Z. Tong, J. Rojas, and J. Seo, “Learning to pick by digging: Data-\ndriven dig-grasping for bin picking from clutter,” in 2022 International\nConference on Robotics and Automation (ICRA), pp. 749–754, IEEE,\n2022.\n[26] C. Zhao and J. Seo, “Learn from interaction: Learning to pick via rein-\nforcement learning in challenging clutter,” in 2022 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), Oct. 2022.\n[27] D. Hafner, K.-H. Lee, I. Fischer, and P. Abbeel, “Deep hierarchical\nplanning from pixels,” in Advances in Neural Information Processing\nSystems (A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, eds.), 2022.\n[28] F. Xia, C. Li, R. Mart´ın-Mart´ın, O. Litany, A. Toshev, and S. Savarese,\n“Relmogen: Integrating motion generation in reinforcement learning for\nmobile manipulation,” in 2021 IEEE International Conference on Robotics\nand Automation (ICRA), pp. 4583–4590, IEEE, 2021.\n[29] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,\nW. Li, P. J. Liu, et al., “Exploring the limits of transfer learning with a\nuniﬁed text-to-text transformer.,” J. Mach. Learn. Res., vol. 21, no. 140,\npp. 1–67, 2020.\n[30] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark, et al., “Learning transferable\nvisual models from natural language supervision,” in International\nConference on Machine Learning, pp. 8748–8763, PMLR, 2021.\n[31] E. Coumans and Y. Bai, “Pybullet, a python module for physics simulation\nfor games, robotics and machine learning.” http://pybullet.org, 2016–2021.\n[32] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nin 3rd International Conference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings\n(Y. Bengio and Y. LeCun, eds.), 2015.\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2023-04-05",
  "updated": "2023-04-05"
}