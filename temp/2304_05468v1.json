{
  "id": "http://arxiv.org/abs/2304.05468v1",
  "title": "A Survey of Resources and Methods for Natural Language Processing of Serbian Language",
  "authors": [
    "Ulfeta A. Marovac",
    "Aldina R. Avdić",
    "Nikola Lj. Milošević"
  ],
  "abstract": "The Serbian language is a Slavic language spoken by over 12 million speakers\nand well understood by over 15 million people. In the area of natural language\nprocessing, it can be considered a low-resourced language. Also, Serbian is\nconsidered a high-inflectional language. The combination of many word\ninflections and low availability of language resources makes natural language\nprocessing of Serbian challenging. Nevertheless, over the past three decades,\nthere have been a number of initiatives to develop resources and methods for\nnatural language processing of Serbian, ranging from developing a corpus of\nfree text from books and the internet, annotated corpora for classification and\nnamed entity recognition tasks to various methods and models performing these\ntasks. In this paper, we review the initiatives, resources, methods, and their\navailability.",
  "text": "arXiv:2304.05468v1  [cs.CL]  11 Apr 2023\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for\nNatural Language Processing of Serbian\nLanguage\nUlfeta Marovac1, Aldina Avdi´c1* and Nikola Miloˇsevi´c2,3\n1Department of Technical and Technological Sciences, State\nUniversity of Novi Pazar, Vuka Karadˇzi´ca 9, Novi Pazar, 36300,\nSerbia.\n2*Research and Development, Bayer Pharmaceuticals,\nM¨ullerstrasse 178, Berlin, 13353, Germany.\n3 The Institute for Artiﬁcial Intelligence Research and\nDevelopment of Serbia, Fruˇskogorska 1, Novi Sad, 21102, Serbia.\n*Corresponding author(s). E-mail(s): apljaskovic@np.ac.rs;\nContributing authors: umarovac@np.ac.rs;\nnikola.milosevic@bayer.com;\nAbstract\nThe Serbian language is a Slavic language spoken by over 12 million\nspeakers and well understood by over 15 million people. In the area of nat-\nural language processing, it can be considered a low-resourced language.\nAlso, Serbian is considered a high-inﬂectional language. The combination\nof many word inﬂections and low availability of language resources makes\nnatural language processing of Serbian challenging. Nevertheless, over\nthe past three decades, there have been a number of initiatives to develop\nresources and methods for natural language processing of Serbian, rang-\ning from developing a corpus of free text from books and the internet,\nannotated corpora for classiﬁcation and named entity recognition tasks\nto various methods and models performing these tasks. In this paper,\nwe review the initiatives, resources, methods, and their availability.\nKeywords: natural language processing, text mining, language resources,\nSerbian language\n1\nSpringer Nature 2023 LATEX template\n2\nA Survey of Resources and Methods for NLP of Serbian Language\n1 Introduction\nThe Serbian language is a south Slavic language currently actively spoken by\nabout 12 million people worldwide. It is one of four mutually intelligible vari-\neties of pluricentric language called Serbo-Croatian (other varieties include\nCroatian, Bosnian, and Montenegrin). Serbo-Croatian languages are morpho-\nlogically rich (Deli´c et al., 2010), containing many inﬂections of words, due\nto three genders, seven cases for nouns, and seven tenses for verbs, whose\ninﬂections are followed by other parts of speech and word types, as well as\ntwelve sound changes occurring in word inﬂections (Klajn, 2005). Serbian is\nalso the only European language that is formally digraphic and whose speakers\nare functionally digraphic, using both Cyrillic and Latin alphabets (Magner,\n2001). The majority of Serbian speakers live in Serbia (6,330,919 based on 2011\ncensus1), but a signiﬁcant number of speakers also live in Montenegro, Bosnia\nand Herzegovina, Croatia, Macedonia, Slovenia, Albania, Hungary, Austria,\nSweden, Germany, and other countries. The Serbian language is an oﬃcial lan-\nguage in Serbia, Bosnia and Herzegovina, Montenegro, while it is recognized as\na minority language in Croatia, Macedonia, Romania, Hungary, Slovakia, and\nthe Czech Republic. Variants of the Serbo-Croatian language (Serbian, Croa-\ntian, Bosnian, and Montenegrin) are spoken by about 19 million people, and\ntherefore the importance of these languages are quite signiﬁcant (Eberhard\net al., 2022).\nNatural language processing is a branch of artiﬁcial intelligence that exam-\nines methods to analyze, process and ultimately make natural languages\nunderstandable for computers (Reshamwala et al., 2013). Therefore, the ﬁeld is\naddressing many challenges related to human/natural languages. Even though\na majority of work in the ﬁeld is predominantly done on the English language,\nthere has been also work on other languages.\nHigh morphological complexity, variety of word inﬂection, and relatively\nlow amount of resources available for Serbian and Serbo-Croatian pose a\nchallenge for natural language processing and language technologies. The mor-\nphological richness of Serbo-Croatian makes it particularly interesting for\nexamining how natural language processing methods perform on languages\nwith a variety of inﬂection and how to eﬃciently handle word inﬂection in mor-\nphologically rich and low-resource languages. In sense of language technologies\nand natural language processing, Serbian cannot be viewed in isolation, as dif-\nferences between Serbian, Croatian, Bosnian, and Montenegrin are small, and\noften approaches developed for one of these variants would perform well on\nothers. Despite these challenges, there have been several initiatives, organiza-\ntions, and signiﬁcant academic work performed to address some of the speciﬁc\nchallenges in Serbian. A number of resources and corpora for syntactic anal-\nysis, classiﬁcation, and named entity recognition were developed, as well as a\nnumber of approaches for document analysis, classiﬁcation, semantic similarity,\nand even analysis of rhetorical ﬁgures such as similes.\n1https://data.stat.gov.rs/Home/Result/3102010401?languageCode=en-US\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n3\nThe development of digital lexical resources is an important and strategic\ntask for every language and should have national priority. The results of natural\nlanguage processing are dependent on the quality and volume of available\ndigital resources, as well as the availability and comprehensiveness of tools for\nprocessing digital resources (Nenadi´c, 2004). Our goal is to collect available\nresources and methods for processing textual data in the Serbian language,\ndescribe them, and identify shortcomings that can be advanced and expanded\nwith the most needed resources according to the development trends of NLP.\nTo the best of our knowledge, this is the ﬁrst review of NLP resources and\nmethods for Serbian of newer date and scope.\nA brief history of NLP resources and method\ndevelopment in Serbia\nThe development of the ﬁrst digital corpora in the Balkans started shortly after\nthe development of the ﬁrst digital corpora in the world, and it was started by\nthe psychologist Djordje Kosti´c, in 1957. with the goal of developing language\ntechnologies for speech recognition and machine translation from the Serbo-\nCroatian language. This corpus was developed until 1962, however, it was not\ndigitally processed, so the ﬁrst digital corpus was published in Zagreb in 1967.\nThis corpus contained the epic Osman by Ivan Gunduli´c prepared by ˇZeljko\nBujas. Development of corpora and corpus linguistics in the western Balkans\nin the period between 1950 and 1990 is presented by Dobri´c (2012). Language\nresources and tools that were mainly developed at the Faculty of Mathematics,\nUniversity of Belgrade, until the year 2003, have been previously reviewed\n(Vitas et al., 2003b,a). During the project called META-NET in 2012, the\nanalysis of the language resources for 23 oﬃcial languages of the European\nUnion was done, and as a part of white pages was published book \"The Serbian\nlanguage in the digital age\" (Vitas et al., 2012). The Regional Linguistic Data\nInitiative (ReLDI) project has made a signiﬁcant contribution to promoting the\nrelevance and importance of open language resources for Serbian and related\nlanguages (Samardˇzi´c et al., 2015). The open and freely available language\nresources for processing the Serbian language, developed within the ReLDI\nproject or independently built, are brieﬂy presented by Batanovi´c et al. (2020).\nIn this paper, we aim to review corpora, resources, methods, models, and\ntools that were developed over time for the Serbian language. We intentionally\nlimit this review to the Serbian language only. While we agree that some\napproaches do work as well on related languages in the Serbo-Croatian group\nof languages, there are still small diﬀerences between them, that would make\nevaluation and comparison of the resources and methods challenging.\n2 Review methodology\nTo cover all authors who deal with natural language processing in Ser-\nbian, we started with the National Repository of Dissertations in Serbia\n(https://nardus.mpn.gov.rs/). By using keywords such as natural language\nSpringer Nature 2023 LATEX template\n4\nA Survey of Resources and Methods for NLP of Serbian Language\nprocessing, NLP, text mining, text data processing, computational linguistics,\nelectronic dictionaries, corpora, sentiment analysis, emotional analysis, text\nclassiﬁcation, lexical resources, and other synonyms and related terms, we\nidentiﬁed dissertations that contain these keywords. From the most relevant\ndissertations, those that deal with natural language processing in Serbian were\nselected. Additionally, a set of dissertations were identiﬁed by searching for\nknown NLP scientists, supervisors, and groups at Serbian universities. Disser-\ntations were identiﬁed by searching for known scientists acting as a supervisor\nor a member of the thesis committee. A total of 29 dissertations in the ﬁeld of\nnatural language processing were selected. By analyzing the dissertations and\nreferences cited in them, we identiﬁed 316 papers indicating NLP for Serbian.\nFurther searches were conducted on Google Scholar for prominent authors (or\nauthor groups) and selected topics.\nWe reviewed the dissertations and papers we identiﬁed, excluding those\nthat did not pertain to natural language processing in Serbian, and classiﬁed\nthem based on their topic and date of publication. In this review, we follow\nthis classiﬁcation, with each section covering a broad area of natural language\nprocessing. The content in each section is primarily arranged in chronological\norder.\n3 Corpora\nA corpus is a set of machine-readable texts representing a sample of a language\nor text type. Corpora can be classiﬁed based on their parameters such as\nmedium, scope (size), domain, purpose, period, source, method of annotation,\nnumber of languages involved, etc. (Vitas et al., 2003b). Given that corpora\ncan include texts in one or more languages, they are divided into monolingual\nand multilingual corpora. According to this classiﬁcation, we will present the\ncorpora of the Serbian language.\n3.1\nMonolingual corpora\nThe Diachronic Corpus of the Serbo-Croatian Language (DCSCL, Table 1)\nof Professor Kosti´c digitized in 2003 contains texts from the period from the\n12th to the 20th century, divided into ﬁve-time samples. The corpus com-\nprises 11 million words that have been manually annotated with lemmas and\ninformation on various morphological categories such as gender, number, case,\nperson, tense, and more (Kosti´c, 2014). In 1981, the NLP group at the Fac-\nulty of Mathematics (NLP_MATF) initiated the development of a corpus for\nthe contemporary Serbian language. The ﬁrst version of this corpus, named\n\"The Untagged Corpus of Contemporary Serbian Language\" (UCCSL, Table\n1), was created in 2003. This corpus contains literature published during or\nafter the 20th century and lacks any annotations. Subsequently, the corpus\nwas enhanced by incorporating bibliographic information into the corpus texts,\nand this new version was called \"SrpKor2003\" (SrpKor2003, Table 1) (Krstev\nand Vitas, 2005; Utvi´c, 2014).\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n5\nMost of the monolingual corpora have morphosyntactic annotation and\nbibliographic information about the corpus texts. Morphosyntactic annotation\nis a linguistic annotation that adds tags to the token: type of speech (Part of\nSpeech Tagging), canonical form or lemma (lemmatization), and morpholog-\nical word categories. By expanding SrpKor2003, a new version of the corpus\nof contemporary Serbian \"SrpKor2013\" (SrpKor2013, Table 1) was created,\nwhich contains literary and artistic texts by Serbian writers in the 20th and\n21st centuries, as well as scientiﬁc texts, administrative texts, and general\ntexts. The Corpus of Contemporary Serbian contains bibliographic data and\nit has been automatically morphosyntactically annotated (with part-of-speech\nand lemmas). It contains more than 122 million words. Its subset \"Lematized\nCorpus of the Modern Serbian Language\" (SrpLemKor, Table 1) contains 3.7\nmillion corpus words. Both corpora are available with registration under a\nlicense (Popovi´c, 2010; Utvi´c, 2011).\nAmong the available corpora of the Serbian/Serbo-Croatian language at\nthe Faculty of Mathematics of the University of Belgrade2, there are also the\nfollowing monolingual corpora. Henning’s Corpus of Serbo-Croatian (HennC,\nTable 1) consists of approximately 700,000 words of Serbo-Croatian. The texts\nare taken from modern Yugoslav ﬁction and all Serbo-Croatian-speaking areas\nare represented (Serbia, Croatia, Montenegro, and Bosnia-Herzegovina) (Cor-\npora etc, 1992). The Untagged Corpus of Vuk’s Folk Proverbs (UnVukC, Table\n1), containing folk proverbs along with Vuk’s comments on them (Krstev,\n1997). Besides this corpus, Vuk’s collection of similes has been augmented\nby employing grammatical rules, machine learning, and manual review. As a\nresult, a corpus of contemporary similes containing 852 similes was developed\n(VukSimC, Table 1)3 (Milosevi´c and Nenadi´c, 2016, 2018). Electoral Crisis\n2000 corpus, which includes the entire webcasts of the daily newspaper \"Poli-\ntika\" from September 10th to October 20th, 2000, and the Labeled corpus of\nthe Serbian language, which consists of texts with a minimal set of structural\nlabels, lack the detailed information on size and are available on the same\nsource4.\nThere are smaller corpora that have been collected mostly for speciﬁc\ndomains (medicine, law, etc.) and particular purposes (name entities recogni-\ntion, semantic similarity, etc.). Among them are the corpora MRCOR1 and\nMRCOR2 (Table 1) consisting of medical reports reviewed from 32 medical\ncenters in Serbia. The primary data set contains 2212 medical reports with a\ndiagnosis of measles. The other dataset consists of 2000 medical reports with\nten diﬀerent types of diagnoses. Medical and non-medical terms are manually\nmarked in the medical reports. For each medical report is assigned a diagno-\nsis code (Avdi´c et al., 2020). A corpus (DMRC, Table 1) of 100 discharge lists\nand 50 reports from doctors from the Faculty of Dentistry at the University of\nBelgrade was used to evaluate the system’s eﬀectiveness in automatically ana-\nlyzing temporal expressions of medical narrative texts. Previously, the texts\n2http://www.korpus.matf.bg.ac.rs/prezentacija/korpusi.html\n3https://ezbirka.starisloveni.com\n4http://www.korpus.matf.bg.ac.rs/prezentacija/korpusi.html\nSpringer Nature 2023 LATEX template\n6\nA Survey of Resources and Methods for NLP of Serbian Language\nTable 1 Monolingual Serbian corpora\nCorpus label\nText type\nNumber of unit\nAnnotation 1\nReference\nDCSCL\ngeneral\n11 000 000 words\nS, L, M;MA\n(Kosti´c, 2014)\nUCCSL\nliterary\n22 200 000 words\nU\n(Krstev and Vitas, 2005; Utvi´c, 2014)\nSrpKor2003\nliterary\n22 200 000 words\nB; MA\n(Krstev and Vitas, 2005; Utvi´c, 2014)\nSrpKor2013\nliterary-artistic scientiﬁc\n122 000 000 words\nB,L,PoS; MA,AA\n(Popovi´c, 2010; Utvi´c, 2011)\nSrpLemKor\nliterary-artistic scientiﬁc\n3 700 000 words\nB,L,PoS; MA,AA\n(Popovi´c, 2010; Utvi´c, 2011)\nHennC\nliterary\n728 952 words\nB; MA\n(Corpora etc, 1992)\nUnVukC\nliterary\n6919 proverbs\nU\n(Krstev, 1997)\nVukSimC\nliterary\n852 similes\nU\n(Milosevi´c and Nenadi´c, 2016, 2018)\nMRCOR1\nmedical reports\n2212 reports\nMT,NMT; MA\n(Avdi´c, 2021)\nMRCOR2\nmedical reports\n2000 reports\nMT,NMT; MA\n(Avdi´c, 2021)\nDMRC\nmedical reports\n150 reports\nU\n(Ja´cimovi´c et al., 2015)\nLAWC\ntext of laws\n59167 texts\nS; MA\n(Petrovi´c and Stankovi´c, 2019)\nLTC\ntext of laws\n7981446 tokens\nS,NE; MA\n(Vasiljevi´c, 2015)\nATC\nnewspapers scientiﬁc\n200 000 words\nM; MA\n(Seˇcujski and Deli´c, 2008)\nSrpNEval\nnews\n89425 words\nNE; AA, MA\n(Krstev et al., 2012)\nNormTagNER\nsocial network\n89 425 words\nM,NE;MA, AA\n(Miliˇcevi´c and Ljubeˇsi´c, 2016)\nSETimes.SR\nnews\n86 726 tokens\nS, L, PoS, SD, NE; MA\n(Batanovi´c et al., 2018; Batanovi´c V et al., 2018)\nparaphrase.sr\nnews\n1194 pairs\nSS; MA\n(Batanovi´c et al., 2011)\nSTS.news.sr\nnews\n1192 pairs\nSS; MA\n(Batanovi´c et al., 2018)\nSrELTeC\nnovel\n5 263 071 words\nS, L, PoS, NE; AA\n(Stankovi´c et al., 2021)\nSrpELTeC-gold\nliterary\n330 119 tokens\nNE; AA,MA\n(Todorovi´c et al., 2021)\nSrpKor4Tagging\nliterary\n342 803 tokens\nPoS, L; AA\n(Stankovi´c et al., 2020)\nRudKorP\nacademic text\n2 340 000 words\nPoS, L; AA\n(Utvi´c et al., 2019)\nTorlakKor\nculture interview transcript\n498021 tokens\nMS,A,L; MA,AA\n(Vukovi´c, 2020)\nCOPA-SR\nquestion answer\n1000 premises\nP; MA\n(Ljubeˇsi´c et al., 2022)\nCorFoA\nbiographical interviews transcripts\n171552 tokens\nMS, L; AA\n(Lemmenmeier-Batini´c et al., 2021)\nMLNews\nnews\n659084 tokens\nMS, L; AA\n(Bogeti´c and Batanovi´c, 2020a)\nMLN-COM\nnews comments\n878482 tokens\nMS, L; AA\n(Bogeti´c and Batanovi´c, 2020b)\nsrWaC\nweb text\n554627647 tokens\nMS, L, SD; AA\n(Ljubeˇsi´c and Klubiˇcka, 2016; Ljubeˇsi´c and Klubiˇcka, 2016)\nCorLeg\nlaws\n5 ﬁles\nU\n(Bogdanovi´c and Toˇsi´c, 2022)\naAnnotation target: U - unannotated, MS - morphosyntactic, L - lemmatization, M - morphological categories, PoS - Part of Speech Tagging, S - structural,\nNE - named entity, UK - unknown annotation, A - accentuation, B - bibliographic, SD - syntactic dependencies, P - plausibility, MT - medical terms, NMT -\nnon-medical terms, SS - semantic similarity; Annotation type: AA - automated annotation MA - manually annotated\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n7\nhad been automatically de-identiﬁed, but the time expressions had not been\nchanged (Ja´cimovi´c et al., 2015). The LAWC (Table 1) set of data includes a\ncollection of 1120 texts of laws, segmented into a total of 59167 texts of indi-\nvidual articles of law (Petrovi´c and Stankovi´c, 2019). The corpus LTC (Table\n1) consists of legal texts in electronic form that are available on the website of\nthe National Assembly of the Republic of Serbia. The laws passed by the end\nof May 2014 contain 681 texts (Vasiljevi´c, 2015).\nAlfaNum which deals with automatic speech recognition (ASR), has built\nits resource AlfaNum Text Corpus (ATC, Table 1), characterized by morpho-\nlogical categories and accentuation and contains approximately 200,000 words\n(Seˇcujski and Deli´c, 2008). The Named Entities Evaluation Corpus for Ser-\nbian (SrpNEval, Table 1) consists of 2000 short news Serbian daily newspapers\nfrom 2005 and 2006. Both the Cyrillic and Latin oﬃcial scripts for the Serbian\nlanguage are used in the corpus (Krstev et al., 2012). ReLDI-NormTagNER-\nsr 2.1 (NormTagNER, Table 1) is a manually annotated corpus of Serbian\ntweets for evaluation of tokenization, sentence segmentation, word normaliza-\ntion, morphosyntactic tagging, lemmatization, and named entities recognition\nof non-standard Serbian language (Miliˇcevi´c and Ljubeˇsi´c, 2016). SETimes.SR\n(SETimes.SR, Table 1) is a reference training corpus of Serbian, which has been\nannotated on multiple levels. The texts in SETimes.SR were obtained from the\nmultilingual parallel corpus SETimes (SETimes, Table 2), which is a collection\nof news articles from the now-defunct Southeast European Times news portal\n(Batanovi´c et al., 2018), (Batanovi´c V et al., 2018). Sentences from online press\nsources were collected for The Serbian Corpus of Paraphrases (paraphrase.sr,\nTable 2). A binary similarity score was manually assigned to each pair of sen-\ntences, indicating whether the sentences in the pair are semantically similar\nenough to be considered close paraphrases (Batanovi´c et al., 2011). Another\ncorpus for determining semantic similarity, The Serbian Corpus of Short News\nTexts - (STS.news.sr, Table 2), consists of 1192 pairs of sentences in Serbian\nthat were collected from news sources on the internet (Batanovi´c et al., 2018).\nOld Serbian novels from the 1840s to the 1920s are collected in SrELTeC\n(SrELTeC, Table 1) and have been digitally preserved as part of the COST\naction CA16204 (Stankovi´c et al., 2021). ELTeC’s section for Serbian contains\n120 novels (Odebrecht et al., 2021). The novels have structural annotations,\nand sentence splitting, words are POS-tagged, lemmatized and seven classes\nof named entities are annotated. Some of the other resources available through\nthe ELG5 portal are SrpELTeC-gold, SrpKor4Tagging, and RudKorP (Table 1)\n. The corpus for training the recognition of named entities SrpELTeC-gold is a\nsub-corpus of the literary corpus of the Serbian language, marked with named\nentities by the SrpNER(Krstev et al., 2014) system (Todorovi´c et al., 2021).\nThe SrpKor4Tagging corpus was formed by combining literary and adminis-\ntrative texts in the Serbian language (Stankovi´c et al., 2020). The RudKorP\ncorpus contains texts in the ﬁeld of mining and processing of mineral raw\n5https://live.european-language-grid.eu/\nSpringer Nature 2023 LATEX template\n8\nA Survey of Resources and Methods for NLP of Serbian Language\nmaterials, created at the University of Belgrade, Faculty of Mining and Geol-\nogy (Utvi´c et al., 2019). There are several more corpora available through the\nClarin.si 6 platform, which are shown at the bottom of Table 1. TorlakKor is\na corpus of transcripts of interviews with the local population of Timok (an\narea in southeastern Serbia) (Vukovi´c, 2020). The COPA-SR dataset (Choice\nof Plausible Alternatives in Serbian) is a translation of the English COPA\ndataset (Ljubeˇsi´c et al., 2022). CorFoA is a corpus of Serbian forms of the\naddress containing transcripts of biographical interviews with 19 participants\n(Lemmenmeier-Batini´c et al., 2021). MLNews is a comprehensive corpus of\nnews articles that are Serbian language-related. It is complemented with a\nseparate corpus of citizens’ online comments on the news articles, available as\nMLN-COM (Bogeti´c and Batanovi´c, 2020a,b). The web corpus of the Serbian\nlanguage srWaC was built by crawling the .rs top-level domain for Serbia in\n2014 (Ljubeˇsi´c and Klubiˇcka, 2016; Ljubeˇsi´c and Klubiˇcka, 2016). CorLeg is a\ncorpus of legislation texts of the Republic of Serbia which was created using a\nlarge number of Serbian Legislation texts gathered from the oﬃcial website 7\n(Bogdanovi´c and Toˇsi´c, 2022).\n3.2 Multilingual corpora\nMultilingual corpora are a particular type of corpus that contains texts writ-\nten in multiple languages. Parallel corpora include both the original texts and\ntheir translations into one or more other languages presented in such a way\nthat their logical structure is explicitly connected at the document, chapter,\nparagraph, sentence, or word level. Table 2 shows multilingual corpora con-\ntaining original texts or translations in the Serbian language. One of the early\nattempts to develop multilingual corpora is the creation of an alignment corpus\nof Plato’s \"Republic\" containing translations into 21 languages, including Ser-\nbian. The corpus has been annotated at the sentence level and has been utilized\nfor both tool development and automated alignment (Krstev and Vitas, 2011).\nThe multilingual language resources and tools for extracting information from\nthe language corpora of CEE languages (Central and Eastern European Lan-\nguages), called MULTEXT-East8 were created as part of the project Multext.\nThe book \"1984\" is included in this parallel and sentence-aligned corpora,\nMultext-East Corpora (G. Orwell’s \"1984\")(G.O.1984, Table 2), along with\ntranslations into several other languages. Krstev and Vitas (2011) created a\ntranslation of this novel into Serbian and a morphosyntactic annotation in\nthe MULTEXT-East format, for which they had previously developed a spec-\niﬁcation for the Serbian language. The parallel corpus Verne80days (Table\n2) contains the French original and 17 translations of Jules Verne’s novel\n\"Around the World in 80 Days\". The alignment was performed on the sub-\nsentence level for each language (Vitas et al., 2008). The Serbian-French\nCorpus (SrpFranKor, Table 2), which consists of 31 subsentence-aligned texts\n6https://www.clarin.si\n7https://www.pravno-informacioni-sistem.rs/\n8http://nl.ijs.si/ME/\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n9\nthat were originally written in French and then translated into Serbian and\nvice versa, is the ﬁrst bilingual corpus in the Serbian language (Vitas and\nKrstev, 2006; Vitas et al., 2006). ParCoLab (Table 2) is a parallel online search-\nable corpus consisting of sentence-alignment texts in French, Serbian, English,\nSpanish, and Occitan. Each of these languages is at the same time a source\nlanguage and a translation language (Balvet et al., 2014).\nThe Serbian-English Corpus (SrpEngKo, Table 2) is the second bilingual\ncollection. It consists of English source texts aligned with their translations\ninto Serbian, and visa-versa, as well as several aligned English and Serbian\ntranslations of literary texts originally written in French (Krstev and Vitas,\n2011). The corpus SETimes is based on the articles posted on the news web-\nsite SETimes.com. Bulgarian, Bosnian, Greek, English, Croatian, Macedonian,\nRomanian, Albanian, and Serbian are among the ten languages in which the\nnews is available. Part of the SETimes, sub-corpus BALKANTIMES was\nused for the expansion of SrpEngKo (Batanovi´c et al., 2018). Parallel texts\nfrom the ﬁelds of law, business, education, and health care are also added to\nSrpEngKor, resulting in the creation of the sub-corpus Serbian-English Law\nFinance Education and Health (SELFEH, Table 2). Almost 150 parallel texts\nmake up SELFEH, which was utilized in term extraction and machine transla-\ntion research as well as to test various taggers for the Serbian language (Utvi´c,\n2011). Another Serbian-English corpus is srenWaC (Table 2), which consists\nof sentence-aligned parallel texts pulled from the .rs top-level domain (Ljubeˇsi´c\net al., 2016). In addition to the SrpFranKo and SrpEngKo bilingual corpora,\na similar corpus was created for the German (SrpNemKor, Table 2). It con-\ntains 48,004 translated pairs of literary texts in Serbian and German, which\nare aligned to the sentence level. Available tools for annotation of named enti-\nties in texts in both languages as well as tools for terminology extraction were\napplied to the prepared parallel corpus (Andonovski et al., 2019; Andonovski,\n2020).\nAdditionally, there are multilingual parallel corpora, some of which are dis-\nplayed at the table’s end (Table 2). OpenSubtitles is a database with about 4\nmillion sentence-level translations of movies and television shows in more than\n62 diﬀerent languages (Tiedemann, 2012). The Bosnian, Croatian, and Serbian\nWeb Corpora (BsHrSrWaC, Table 2) are top-level-domain web corpora. They\nwere used to create a method for separating similar languages that is based on\nunigram language modeling on the crawled data only (Ljubeˇsi´c and Klubiˇcka,\n2014). The Twitter user dataset (Twitter-HBS, Table 2) consists of tweets and\ntheir language tag (Bosnian, Croatian, Montenegrin, or Serbian). The main\ngoal of creating this corpus is discrimination between closely related languages\nat the level of Twitter users (Ljubeˇsi´c and Rupnik, 2022). The PE2rr cor-\npus includes source language texts from many ﬁelds, as well as automatically\nproduced translations into a number of morphologically rich languages, post-\nedited versions of those texts, and error annotations of the post-edit processes\nthat were carried out. This corpus contains texts in Spanish, German, Serbian,\nSpringer Nature 2023 LATEX template\n10\nA Survey of Resources and Methods for NLP of Serbian Language\nTable 2 Parallel Serbian corpora\nCorpus label\nText type\nNumber of unit\nAnnotation1\nLanguages\nReference\nPlato’s Republic\nphilosophical text\n21 text translation\nS, A; MA, AA\nmultilingual\n(Vitas et al., 1998)\nG.O.1984\ngeneral\n100000 words\nL, M, PoS, A;AA,\nMA\nmultilingual\n(Krstev and Vitas, 2011)\nVerne80days\nliterary\n32 aligned texts\nL,\nM,\nPoS,\nA;\nAA,MA\nmultilingual\n(Vitas et al., 2008)\nSrpFranKor\nliterary\n1738752 words\nS, A; AA,MA\nSerbian, French\n(Vitas and Krstev, 2006; Vitas et al., 2006)\nParCoLab\ngeneral\n32 000 000 words\nS, A; AA,MA\nmultilingual\n(Balvet et al., 2014)\nSrpEngKo\ngeneral\n4.420.711 words\nS,A; AA,MA\nSerbian, English\n(Krstev and Vitas, 2011)\nSETimes\nnews\n86 726 tokens\nL,M,PoS; MA\nmultilingual\n(Batanovi´c et al., 2018)\nSELFEH\nlaw, ﬁnance, education, and\nhealth\n2 000 000 words\nL,M,PoS,\nA;AA,\nMA\nSerbian, English\n(Utvi´c, 2011)\nsrenWaC 1.0\nweb text\n23139804 words\nA;AA\nSerbian, English\n(Ljubeˇsi´c et al., 2016)\nSrpNemKor\nliterary\n1 657 329 words\nS,A; AA,MA\nSerbian, German\n(Andonovski et al., 2019; Andonovski, 2020)\nOpenSubtitles\nﬁlm translations\n2 793 243 tokens\nS,A; AA,MA\nmultilingual\n(Tiedemann, 2012)\nBsHrSrWaC\nweb text\n894 000 000 tokens\nM,L,S,D; AA,MA\nmultilingual\n(Ljubeˇsi´c and Klubiˇcka, 2014)\nTwitter-HBS\nsocial network\n390268 texts\nS; AA,MA\nmultilingual\n(Ljubeˇsi´c and Rupnik, 2022)\nPE2rr\ngeneral\n43938 words\nS,ER,A; AA,MA\nmultilingual\n(Popovi´c and Arˇcan, 2016)\nBERTi´c-data\ngeneral\n8387681518 words\nS; MA\nmultilingual\n(Ljubeˇsi´c and Lauc, 2021)\nCLASSLA-Wiki\ngeneral\n486258862 tokens\nLN; AA\nmultilingual\n(Ljubeˇsi´c et al., 2021)\naAnnotation target: U - unannotated, L - lema, MS - morphosyntactic, M - morphological categories, LN - linguistic, PoS - Part of Speech Tagging, UK\n- unknown annotation, S - structural, SD - syntactic dependencies, A - aligned, ER - error; Annotation type: AA - automated annotation MA - manually\nannotated\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n11\nSlovene and English (Popovi´c and Arˇcan, 2016). The BERTi´c-data text collec-\ntion contains more than 8 billion tokens of mostly web-crawled text written in\nBosnian, Croatian, Montenegrin, or Serbian. The collection was used to train\nthe BERTi´c transformer model (Ljubeˇsi´c and Lauc, 2021). The Wikipedia\ndumps of the Bosnian, Croatian, Macedonian, Montenegrin, Serbian, Serbo-\nCroatian, and Slovenian Wikipedias were collected in the comparable corpus\nCLASSLA-Wikipedia (CLASSLA-Wiki, Table 2). The linguistic annotation\nwas performed with the classla package 9, on all levels available for a speciﬁc\nlanguage (Ljubeˇsi´c et al., 2021). Corpora for sentiment analysis are presented\nin a separate chapter.\n4 Language resources\n4.1 Dictionaries and terminologies\nThe term electronic dictionary considers the dictionary which is used for text\nprocessing. It consists of valuable information for solving problems of seg-\nmentation, morphological, and partly syntactic and semantic text processing\n(Vitas and Krstev, 2009). The automatic processing of text begins by analyz-\ning individual words, which are the base units of the analyzed text. At times,\nindividual words may not be the most appropriate base units for processing\nnatural language. Therefore, there are two types of dictionaries: mono-lexemic,\nwhich consists of single words, and polylexemic which consists of multi-word\nunits (Andonovski, 2020).\nThe international network of laboratories for computational linguistics,\nRELEX (Laporte, 2003), has created a model for building electronic morpho-\nlogical dictionaries that have been adopted by numerous organizations dealing\nwith natural language processing. The Unitex 10 system works with electronic\nmorphological dictionaries developed according to this model. These are dic-\ntionaries in DELA format (Dictionnaires Electroniques du LADL - Laboratoire\nd’Automatique Documentaire et Linguistique). In order to distinguish between\nmonolexemic and polylexemic units, this electronic dictionary is organized into\ntwo separate subsystems: a dictionary of monolexemic units (DELAS - sim-\nple forms and DELAF - inﬂected forms) and a dictionary of polylexemic units\n(DELAC - compound forms, and DELACF - compound inﬂected forms).\nBased on these models, within the Group for Language Technologies of\nthe University of Belgrade, electronic morphological dictionaries of the Ser-\nbian language in Latin and Cyrillic (SrbMD) were built (Krstev, 1997; Vitas\net al., 2003b,a; Krstev et al., 2006, 2010). According to (Mladenovi´c, 2016), the\nSrpMD system currently contains 148,000 lemmas and over 1,000 ﬁnal trans-\nducers that generate more than 5 million DELAF determinations. The tool\nLeximir (Stankovi´c et al., 2011) is used as a dictionary management system.\nIt is a multipurpose tool for supporting computational linguists in developing,\nmaintaining, and exploiting e-dictionaries.\n9https://pypi.org/project/classla/\n10https://unitexgramlab.org/language-resources\nSpringer Nature 2023 LATEX template\n12\nA Survey of Resources and Methods for NLP of Serbian Language\nThe accentuation-morphological dictionary was created at the Faculty of\nTechnical Sciences in Novi Sad and it contains over 4 million entries. It is used\nfor context analysis within text-to-speech and automatic speech recognition\nsystems for Serbian (Seˇcujski and Deli´c, 2008).\nLjubeˇsi´c et al. (2015) presented MWELex, a multilingual lexical of Croat-\nian, Slovene, and Serbian multi-word expressions (MWE) that were extracted\nfrom parsed corpora. The srMWELex lexicon v0.5 was automatically built\nduring the short-term scientiﬁc mission inside the PARSEME COST action.\nIt contains multi-word expression candidates extracted with the DepMWEx\ntool from the srWaC v1.0 web corpus. It consists of 22 290 entries and 3 273\n369 multi-word units. The freely available morphological lexicon srLex is intro-\nduced in (Ljubeˇsi´c et al., 2016). It is consisting of 105 359 lexemes and 5 327\n361 (token, lemma, MSD) triples.\nMiletic (2017) described the creation of a morphosyntactic e-dictionary for\nthe Serbian language. It is derived from the Wiktionary edition for Serbo-\nCroatian, a manually POS-tagged corpus and specialized proposition list. This\nlexicon contains 1 226 638 million wordforms for 117 445 lemmas, corre-\nsponding to a total of 3 066 214 unique triples (wordform, lemma, MSD -\nmorpho-syntactic description), and it is aimed for POS (part of speech) tagging\nand parsing tasks.\nThe DELAS-TOP and DELAS-PERS are dictionaries that respectively list\ngeographic names and Serbian personal names (Krstev et al., 2008; Pavlovi´c-\nLaˇzeti´c et al., 2004; Grass et al., 2002). The dictionary of geographic names\nDELA-TOP covers geographic concepts at the level of a high-school atlas\n(approximately 20.000 toponyms, oronyms, and hydronyms with their corre-\nsponding derivatives). The dictionary of personal names has been created from\nthe list of the names of 1.7 million inhabitants of Belgrade as established in\n1993. Based on this list, two dictionaries were constructed: DELA-FName for\nthe ﬁrst names, and DELA-LName for the last names (Vitas et al., 2003a).\nThe dictionary of librarianship and information sciences contains terminol-\nogy (Kovaˇcevi´c et al., 2004) used in the theory and practice of librarianship,\ninformation sciences, and related ﬁelds in Serbian, English, and German.\nThe online version of the dictionary currently contains: 40,000 deﬁnitions\n(approximately 14,000 in Serbian); 900 deﬁnitions or annotations of terms that\nare part of library standards; 2,300 acronyms of international and national\norganizations and institutions; 190 addresses of relevant websites11.\nThe electronic geological dictionary (GeolISSTerm) is a specially prepared\ntaxonomy of basic geological concepts and terms, and it is used for IT needs\nas an elementary resource in the formation of domains in the Geological\nInformation System of Serbia (GeolISS)(Stankovi´c et al., 2011).\n(Vujiˇci´c-Stankovi´c et al., 2014) extended the SrpMD by 636 entries of sim-\nple words and 612 entries of MWE (multi-word expressions) from the culinary\ndomain.\n11http://rbi.nb.rs/srlat/dict.html\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n13\nGrljevi´c (2016) provided several dictionaries for sentiment analysis in the\nﬁeld of education in her doctoral dissertation (sentiment words, domain-\nspeciﬁc phrases, negation keywords, and stop words that are identiﬁed from\nthe corpus). Negation signals, negative quantiﬁers, and particle intensiﬁers\nwere added to the sentiment lexicon (Ljaji´c and Marovac, 2019). Similarly,\nfor sentiment analysis, a domain-oriented stop words collection was created\n(Mladenovi´c, 2016). In a separate chapter on sentiment analysis below, senti-\nment word lexicons and other lexicons used in sentiment analysis are described\nmore.\nAvdi´c et al. (2020) created medical dictionaries for Serbian: names of\ndiagnoses (7942 entries), diagnosis code (14194 entries), Latin names of the\ndiagnosis (3794 entries), therapies (2232 drugs and 1317 ampoules, 2255 diﬀer-\nent terms), symptoms for the diagnosis of measles B05 (95 entries), specialties\n(41 entries), abbreviations from the medical domain. Non-medical dictionar-\nies created in the same research are a set of negation symbols in the medical\ndomain, places, and names.\nOstrogonac et al. (2020) created a domain vocabulary of jobs in Serbian.\nIt has two versions, one of 40 thousand, one of 80 thousand words, and 30\nthousand lemmas, and they are included in Python library nlpheart.\nThe Serbian stop word dictionary (SSW dictionary) contains 1241 diﬀer-\nent stop words for the Serbian language. It was created based on the grammar\nof the Serbian as well as by comparing with available sets of stop words for\nthe Serbian language and a set of stop words for the Croatian language. SSW\ndictionary for the Serbian language contains words in diﬀerent forms of their\nappearance. A word type label accompanies each word. The SSW dictionary\nis available as a CSV ﬁle - SSWdictionary.csv. The ﬁle contains two columns:\nword and label. The label describes the type of words: auxiliary verbs (V), pro-\nnouns (PRON), adverbs (ADV), prepositions (PREP), conjunctions (CONJ),\nexclamations (EXCL), particles (PART) and abbreviations (ABBR)(Marovac\net al., 2021).\nThe SrHurtLex (Stankovi´c et al., 2020) is a lexicon created for the detection\nof abusive words in Serbian. It is created using the lexical database Leximirka,\nthe system of Serbian morphologic dictionaries SrpMD, and The Dictionary\nof Serbian Language (DS) (Vujani´c, 2007), where the multi-word expressions\nlabeled in dictionaries as augmentative, pejorative, derogatory, vulgar, etc.\nwere collected.\n4.2 Ontologies\nThe term \"ontology\" originates from philosophy and it represents science\nabout existing concepts (types of things) and their relations (Vujiˇci´c-Stankovi´c,\n2016). In computer science, ontology is a structure that describes concepts,\ntheir relations, and existing constraints. Their purpose is the automatic sharing\nand reuse of knowledge between humans and computer, and between comput-\ners. Both parts which are included in sharing process have to have a certain\nlevel of understanding of the exchangeable information.\nSpringer Nature 2023 LATEX template\n14\nA Survey of Resources and Methods for NLP of Serbian Language\nThe hierarchy of classes is called taxonomy. Commonly, ontology describes\nterms and relationships between them for a particular domain.\nThe semantic network which describes proper names and their relations\nis developed during Prolex project (Krstev et al., 2007). It consists of 2000\nproper names, mainly names of states and their capital cities.\nThe RudOnto is a terminological resource developed at the Faculty of\nMining and Geology in Belgrade, and it is the reference resource for mining\nterminology in Serbian. It is managed by a terminological information system,\nand intended to produce the derived terminological resources in subﬁelds of\nmining engineering, such as planning and management of exploitation, mine\nsafety or mining equipment management (Stankovi´c et al., 2011).\nTomaˇsevi´c (2018) developed a mining domain ontology RuDokOnto for\nthe purpose of collecting, describing, and systematization of mining project\ndocumentation throughout the phases of the mining project’s life cycle in a\nway that links other related ontologies.\nRetFig is a linguistic domain, descriptive, formal ontology for rhetorical\nﬁgures in Serbian and describes 98 ﬁgures (Mladenovi´c and Mitrovi´c, 2013).\n4.3 Word networks\nIn traditional dictionaries, lexical concepts are alphabetically ordered and there\nis a deﬁnition for all possible meanings for each of them. In WordNet, all words\nexpressing a concept are grouped together in a set of synonyms (synset - syn-\nonymous set). Serbian WordNet (SerWN) (Krstev et al., 2004; Koeva et al.,\n2008) is the lexical-semantic net for Serbian. Its development started within\nthe project BalkanNet (Mladenovi´c et al., 2020), and when it ﬁnished in 2004,\nit had 8000 synsets. After that, the development of WordNet continued, espe-\ncially in biological, biomedical, psycho-linguistic, and gastronomical domains,\netc. Its structure is basically the same as PWN (Princeton Word Net (Miller\nand Fellbaum, 2007)), and it is organized using nodes (synsets) and relations\nbetween them. Every word in synset is represented as an array of characters\nor literal, followed by the meaning of concrete literal in concrete synset. As a\nword can have multiple meanings, it can be part of multiple synsets.\nAccording to Koeva et al. (2008), SerWN consists of 13612 synsets, 23139\nliterals, 18210 relations, 314 derived, and 83 derivatives.\nKrstev et al. (2014) developed an ontology for the culinary domain in Ser-\nbian, and the Serbian Wordnet is enhanced with the synsets from this domain.\nThis ontology is used for the determination of similarity between recipes and\nquery expansion.\nAs a lexical resource, SerWN has been applied in multi-member lexical\nunit research (Krstev et al., 2010; Mladenovi´c et al., 2014), text classiﬁca-\ntion (Pavlovic-Lazetic and Graovac, 2010), the search of multilingual digital\ndatabases (Stankovi´c et al., 2012), recognizing rhetorical ﬁgures (Mitrovi´c\net al., 2017; Mladenovi´c and Mitrovi´c, 2013; Mitrovic, 2014), analyzing feelings\nexpressed in the text (Mitrovi´c et al., 2015) and others.\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n15\nVujicic-Stankovic in created an ontology for the culinary domain, and\nexpanded SrWN by 1,404 synsets from the culinary domain so it contains a\ntotal of 1,797 such synsets (Vujiˇci´c-Stankovi´c et al., 2014; Vujiˇci´c-Stankovi´c,\n2016).\nUniversal Dependencies (UD) project12 aims to develop cross-linguistically\nconsistent treebank annotation for many languages, to provide a universal\ninventory of categories and guidelines to facilitate consistent annotation of sim-\nilar constructions across languages while allowing language-speciﬁc extensions\nwhen necessary. As a part of this project, Serbian treebank is created, based\non SETimes corpus (Samardˇzi´c et al., 2017).\n5 Lexical and syntactic analysis methods\n5.1 Transliteration and diacritic restoration\nThe tool for the automatic performing diacritic restoration of text which is\npotentially missing diacritics (e.g. transform \"kuca\" (dog) into \"ku´ca\" (house),\nif it is necessary) is described by Ljubeˇsi´c et al. (2016). The accuracy of the\ntool is 99.5% on standard and 99.2% on nonstandard language.\nTransliteration in Serbian is accommodated because each sound is a char-\nacter. Characters map almost directly from Cyrillic to Latin, with exception\nof a few letters, that map from a single Cyrillic character to two Latin char-\nacters (e.g. њ -> nj, љ -> lj, or ђ -> dj). Systems for transliteration between\nSerbian Cyrillic and Latin alphabets exist since the 1950s (Matthews, 1952;\nAurousseau, 1953; Gerych, 1965). Among newer tools for solving this prob-\nlem is the Python package nlpheart (Ostrogonac et al., 2020), which has a\npossibility of conversion between the Cyrillic and Latin alphabet.\n5.2 Tokenization and stemming\nSentence tokenization is the process of dividing the text into consisting sen-\ntences. Word tokenization’s aim is to divide sentences into simple units, tokens,\nwhich are usually words, numbers, and punctuation marks. There are a number\nof multi-language tokenizers which have the ability to tokenize Serbian texts.\nThe majority of these tools are available as Python modules, like Cutter(Gra¨en\net al., 2018), Spacy13, CLASSLA and Reldi14 tokenizers. Cutter tokenizer\nhas a variant for online tokenization15. CLASSLA tokenizer is adapted Stan-\nford NLP Python Library with improvements for speciﬁc languages - Fork of\nStanza for Processing Slovenian, Croatian, Serbian, Macedonian and Bulgar-\nian)16. Turanjin tokenizer for Serbian is available as a PHP library17. There is\nno precise information or comparison of the tokenization accuracy on Serbian\ndocuments.\n12https://universaldependencies.org/introduction.html\n13https://spacy.io/api/tokenizer\n14https://github.com/clarinsi/reldi-tokeniser\n15https://pub.cl.uzh.ch/projects/sparcling/cutter/current/\n16https://pypi.org/project/classla/\n17https://github.com/turanjanin/serbian-language-tools\nSpringer Nature 2023 LATEX template\n16\nA Survey of Resources and Methods for NLP of Serbian Language\nOstrogonac et al. (2020) present Python package nlpheart for text process-\ning of Serbian that includes transliteration, tokenization, normalization, and\nautomatic preparing for the application of machine learning models.\nStemming is a process of removing ﬁnishing letters of words, as derivation\nsuﬃxes of words. The remaining part is a reduced form of the word called a\nstem. The stem diﬀers from a dictionary form of the word (lemma). The ﬁrst\ntool for stemming (in further text, stemmer) for Serbian is described by Keˇselj\nand ˇSipka (2008), and it is rule-based (1000 rules) and its accuracy is 79%.\nBased on this stemmer, Miloˇsevi´c (2012b) created a new stemmer reducing\nthe number of rules (180 rules) with an accuracy of 90%. Another solution can\nbe found in literature, and it is created by S. Petkovi´c et al.18 and it is based\non Stemmer for Croatian (precision of 0.986 and recall of 0.961 (F1 0.973)\nfor Croatian)19. There is no information about the stemming accuracy of this\ntool. Batanovi´c et al. reimplemented the optimal and the greedy stemmers of\nKeˇselj and ˇSipka (2008), improved the greedy algorithm proposed by Miloˇsevi´c\n(2012b), and reimplemented a stemmer for Croatian by Ljubeˇsi´c & Pandˇzi´c,\nwhich is a reﬁnement of the algorithm presented by Ljubeˇsi´c et al. (2007), as\na WEKA package (Holmes et al., 1994) – SCStemmers in (Batanovi´c et al.,\n2016).\nThe stem is not a dictionary word form, it is the most common part of\nwords with the same semantic meaning. So, in some normalization methods, n-\ngram analysis is used as a stemmer alternative. This means that a word could\nbe normalized to a single sub-string of its letters whose size is n (tri-gram,\ntetra-gram etc.). The reason is that the n-gram analysis approach is language-\nindependent, which means that it doesn’t need any rules, lexicons, or corpora.\nMarovac et al. (2012) used n-gram analysis in the normalization of Serbian\ntext.\n5.3 Lemmatization and Part-of-speech tagging\nLemmatization is a process that aims to determine the base morphological form\nof the word (lemma), which corresponds to a headword in a dictionary. This\nstep in text mining is especially important for languages with rich inﬂectional\nmorphology, such as Serbian. A given word can have multiple possible lemmas,\nand it depends on the context, so some lemmatizers use information obtained\nby POS or MSD tagging to achieve better accuracy.\nThere are a number of lemmatization approaches: rule-based, simple\nstatistical-based methods, and machine learning-based methods (Akhmetov\net al., 2020).\nLemmaGen (Jurˇsic et al., 2010) is a learning algorithm for the automatic\ngeneration of lemmatization rules in the form of a reﬁned RDR (Ripple Down\nRules) tree structure. It is compared with CST (Dalianis and Jongejan, 2006)\n18Stefan\nPetkovi´c\nand\nDragan\nIvanovi´c,\nStemmer\nfor\nSerbian\nlanguage,\n2019.\nhttps://snowballstem.org/algorithms/serbian/stemmer.html (accessed Apr 26, 2022)\n19Ljubeˇsi´c,\nNikola.\nPandˇzi´c,\nIvan.\nStemmer\nfor\nCroatian,\nhttp://nlp.ﬀzg.hr/resources/tools/stemmer-for-croatian/\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n17\nand RDR (Plisson et al., 2008) lemmatization algorithms and its lemmatization\naccuracy on Serbian corpora are given in 3.\nBTagger20 (Gesmundo and Samardzic, 2012) is a bidirectional tagger-\nlemmatizer tool that implements a lemmatization-as-tagging paradigm. Mod-\nels are trained on the Serbian G.O.1984 corpus, reaching overall accuracies of\n97.72% for lemmatization and 86.65% for MSD tagging.\nAgi´c et al. (2013) tested hidden Markov model trigram taggers HunPos,\nlemmatization capable PurePos, TreeTagger, support vector machine tagger\nSVMTool, CST data-driven rule-based lemmatizer and BTagger on Serbian\ncorpora and results are given in Table 3.\nTable 3 Tools for normalization and POS tagging\nTool label\nApplication\nCorpus\nAccuracy\nKeseljStemmer (Keˇselj and ˇSipka, 2008)\nstemmer\nunknown\n79.0 %\nMilosevicStemmer (Miloˇsevi´c, 2012b)\nstemmer\nPolitika\n90.0%\nLemmaGen (Jurˇsic et al., 2010)\nlemmatizer\nMultext-East\nup to 86.1%+-0.61%\nCST (Jurˇsic et al., 2010; Dalianis and Jongejan, 2006)\nlemmatizer\nMultext-East\n64.0 %+-0.82%\nRDR (Jurˇsic et al., 2010; Plisson et al., 2008)\nlemmatizer\nMultext-East\n63.8%+-0.80%\nBTagger (Gesmundo and Samardzic, 2012)\nlemmatizer\nG.O.1984\n97.73%\nPurePOS (Agi´c et al., 2013)\nlemmatizer\nSETimes\n86.63%\nTnT tagger (Gesmundo and Samardzic, 2012)\nPOS tagger\nG.O.1984\n85.47%\nBTagger (Gesmundo and Samardzic, 2012)\nPOS tagger\nG.O.1984\n86.65%\nHunPOS (Agi´c et al., 2013)\nPOS tagger\nSETimes\n95.47%\nCRF (Ljubeˇsi´c et al., 2016)\nPOS Tagger\n500k\n97.86%\nHunPOS (Agi´c et al., 2013)\nMSD tagger\nSETimes\n87.11% (+lex 84.81%)\nPurePOS (Agi´c et al., 2013)\nMSD tagger\nSETimes\n74.4%\nSVMTool (Agi´c et al., 2013)\nMSD tagger\nSETimes\n84.99%\nCRF (Ljubeˇsi´c et al., 2016)\nMSD tagger\n500k\n92.33%\nReldi-tagger (Ljubeˇsi´c and Dobrovoljc, 2019)\nMSD tagger\nCLARIN.SI\n92.03%\nStanfordNLP (Ljubeˇsi´c and Dobrovoljc, 2019)\nMSD tagger\nCLARIN.SI\n95.23%\nPOS (part of speech) tagging is an NLP processing task where words in\nthe text are annotated with corresponding grammatical categories (parts of\nspeech: verb, noun, adjective, pronoun, etc.). POS tagging with more precise\ninformation about grammatical categories is MSD tagging (morphosyntactic\ntagging - tagging with morphosyntactic descriptions).\nFinite state automata used in the lexical and syntactic analysis, considering\nmorpho-syntactic labels were described in (Krstev, 1997).\nSeˇcujski and Kupusinac (Seˇcujski and Kupusinac) used HMM for mor-\nphosyntactic tagging on Alfanum and G.O.1984 corpora. The accuracy of\nannotation largely depends on the type of text and that some texts are more\nsuitable for automatic annotation than others. For the AlphaNum corpus, an\nerror of 18.44% was obtained, and for \"1984\" as much as 26.97%.\nPopovi´c (2008, 2010) evaluated ﬁve taggers (Tree Tagger, SVMTool, Brill\n– Rule Based Tagger, Trigrams’n’Tags and MXPOST) on three corpora\n(“Helsinˇske sveske br. 15, nacionalne manjine i pravo”, Serbian Radio diﬀu-\nsion Law and materials from UNDP workshops, G.O.1984). TnT has shown\nthe best performance, while Tree Tagger and SVMTool taggers have shown\nbetter performance in special cases.\nThe POS tagger for Serbian and Croatian based on CRF (conditional ran-\ndom ﬁelds) is described in (Ljubeˇsi´c et al., 2016). It is trained on a manually\nannotated corpus of Croatian in combination with hrLex/srLex lexicons for\n20https://github.com/agesmundo/BTagger\nSpringer Nature 2023 LATEX template\n18\nA Survey of Resources and Methods for NLP of Serbian Language\neach language. The set of morpho-syntactic labels used in the corpus is cre-\nated according to instructions of the revised MULTEXT-East V5 set of labels\nfor Croatian and Serbian. The accuracy of POS tagging for Serbian is 92.33%\nfor MSD tagging and 97.86 for POS tagging.\nThe tools for tokenization, stemming, lemmatization, and POS and MSD\ntagging and their accuracy on Serbian corpora are shown In Table 3.\n6 Classiﬁcation\nText classiﬁcation is a process of categorizing text data into predeﬁned groups\nor categories based on its content. Text classiﬁcation is often performed using\nsupervised machine learning techniques.\nGraovac (2014) proposed two methods for classifying text based on their\ncontent. The ﬁrst method is based on the representation of a document as\na proﬁle containing a ﬁxed number of n-grams of bytes that appear in the\ndocument, and a dissimilarity measure used to determine the class to which\nthe document belongs. This method is language-independent and does not\nrequire any pre-processing of the text or prior knowledge of the content of the\ntext or the language in which the text is written. The second method refers to\nthe use of the information contained in the Serbian wordnet and the Serbian\nelectronic dictionary.\nPetrovi´c proposes utilizing models and neural networks as a potential rem-\nedy to meet the demand for machine prediction of links or references within\nthe text of newly enacted laws and other regulations (Petrovi´c and Janiˇcijevi´c,\n2019; Petrovi´c, 2020). Training and validation of neural networks (RNN -\nRecurrent neural networks, CNN - convolutional neural networks, and HAN\n- hierarchical attention network model) are performed on a labeled data set,\nwhich is made by assigning to each segment of the text of the law (each article\nof the law) a corresponding label on the existence, or non-existence of a link\nor reference in that segment of the text. After that, the training procedure is\nbased on a large set of data, which includes a collection of 1120 texts of laws,\nsegmented into a total of 59 167 individual articles of law.\nFor all methods, the number of training parameters is reduced by over 99%.\n6.1 Similarity\nMarovac et al. (2013) proposed a method for similarity search of documents\nin Serbian. The searching query is represented as a word vector, as well as\ndocuments for search hing. The grouping of the documents is done using the\nk-Means clustering algorithm, and keywords are extracted using TF and IDF\nfeatures, and n-grams. The similarity values between query and documents are\ncalculated using cosine measure, Jaccard’s coeﬃcient, or Euclidian distance.\nFurlan et al. (2013) proposed a new algorithm, called LInSTSS, which, when\ndetermining the semantic similarity of two short texts, also takes into account\nthe speciﬁcs of the words these texts contain. The evaluation was carried out on\na corpus of paraphrases for the Serbian language created in the same research.\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n19\nOne solution of similarity search in e-government is described by Nikoli´c (2016)\nusing the tool “Apache Lucene\". Petrovi´c and Stankovi´c (2019) demonstrated\nhow diﬀerent preparation methods inﬂuence the calculation of text similarity.\nBatanovi´c (2020) presented the process of handling semantic tasks using\nstatistical modeling and machine learning. The STS.news.sr is a corpus of news\ncreated and used for the task of semantic similarity where the similarity of news\nis annotated by score. Implementation is given in the library STSFineGrain\n(Java), available on GitHub. For semantic similarity, the combination of word\nalignment and the average of word vectors was used. The srWaC corpus (Web\ncorpus of the Serbian language) is used for creating the word vectors. An\nevaluation of the eﬀects of 3 diﬀerent stemming techniques on text similarity\nfor Serbian has been performed. Additionally, a new technique for calculating\nsimilarity was proposed called Part-of-Speech and Term Frequency weighted\nShort-Text Semantic Similarity.\n6.2 Sentiment analysis\nSentiment analysis is the process of analyzing and deriving people’s opinions,\nthoughts, and impressions regarding various topics, products, and services\nexpressed in a part of the text. Sentiment analysis can be investigated on\nseveral levels: document level, sentence level, phrase level, and aspect level\n(Wankhade et al., 2022). For sentiment analysis, speciﬁc lexical resources\nare necessary, such as a dictionary of sentiment words, tools for processing\nnegation, stylistic ﬁgures, and so on. One of the ﬁrst tools for sentiment anal-\nysis at the sentence level for the Serbian language was given by Miloˇsevi´c\n(2012a). A binary classiﬁcation of negative and positive sentiment was per-\nformed using the Naive Bayes(NB) algorithm. A steamer (Miloˇsevi´c, 2012b)\nthat was designed for this purpose was used as part of the preprocessing.\nStop words were eliminated, and negation processing was done by preﬁxing\nthe word that follows the negation signal (words like no, none) with ’NE_’.\nThe sentiment analyzer was created as a web tool and made available to the\npublic21.\nMaximum entropy (ME), support vector machine (SVM), and NB machine\nlearning methods were used to analyze tweet sentiment (Joli´c, 2015). Proce-\ndures are oﬀered to minimize the noise in these messages to increase accuracy.\nThey achieved the best accuracy with the ME method of 80.5% using uni-\ngrams; however, when applying unigrams and bigrams, negation and phrases\nwere also considered, increasing accuracy to 82.7%.\nMladenovi´c et al. (2016) chose a hybrid approach that uses a dictionary of\nsentiments extended by morphological forms using a morphological dictionary\nSrbMD and synonyms using Serbian WordNet to reduce the disadvantages\nof using stemmers in morphologically rich languages. A sentiment dictio-\nnary was created (Mladenovi´c, 2016), containing 1053 expressions (and 10704\ninﬂectional forms) classiﬁed into 24 emotion categories, and augmented with\n21https://inspiratron.org/SerbianSentiment.php\nSpringer Nature 2023 LATEX template\n20\nA Survey of Resources and Methods for NLP of Serbian Language\nsynonyms and phrases. SentiWordNet has been integrated with Serbian Word-\nNet to provide sentiment tags to the synsets from Serbian WordNet. A total of\n4044 synsets were marked. An additional sentiment dictionary with 971 inﬂec-\ntional forms was created using these synsets. Using the TF-IDF approach, 577\n(1428 inﬂectional forms) of the most frequent words from the 122 million-word\ncorpus of the contemporary Serbian language SrpKor2013 were used to con-\nstruct the list of stop words. A domain-oriented collection of stop words with\n1372 inﬂectional forms were generated using the TF approach. The method\nwas trained on a news set (TrN, Table 4) with two topics: \"bad news\" and\n\"good news,\" which are automatically categorized and balanced by sentiment.\nTwo sets were used for testing: a set of news (TsN, Table 4) collected from a\nsource other than TrN (this set is not balanced), and a set of movie reviews\n(TsMR, Table 4) collected from a website and tagged with the sentiment, based\non the grades that were attached to them (this set is not balanced). These\nsources were used to develop the Serbian document-level sentiment analysis\nframework (SAFOS), which applies the maximum entropy approach with the\nfeatures: of unigrams, bigrams, and trigrams. They used hold-out test sets and\n10-fold cross-validation (CV) to evaluate the SAFOS system. The combina-\ntion of unigram and bigram features reduced by \"sentiment feature\" mapping\nproduced the best classiﬁcation accuracy scores for both hold-out tests (accu-\nracy 78.3% for TsMR set and 79.2% for the TsN set). Because it was trained\nand tested on data from the same domain, it performed better in a 10-fold CV\nwith a 95.6% accuracy rate.\nGrljevi´c (2016) presented a sentiment analysis of content from social net-\nworks to improve the business of higher education institutions. Sentiment\nanalysis is performed at two levels of granularity: at the document level and the\nsentence level. On the set of online reviews of professors and lectures (ORPL,\nTable 4) both a rule-based strategy (based on a vocabulary that was man-\nually built for the requirements of this domain and is available), as well as\nthe approach based on machine learning algorithms (NB, SVM, and k-Nearest\nNeighbor KNN) were applied. In sentiment classiﬁcation using machine learn-\ning algorithms, the SVM algorithm gives the best performance, with 84.94%\naccuracy at the review level, and 80.13% accuracy at the sentence level. The\nclassiﬁcation of the sentiment was done using the sentiment lexicon, by intro-\nducing separate dictionaries for 1266 positive and 1521 negative sentiment\nwords, intensiﬁers (95), neutralizers, negation (31), domain- speciﬁc phrases\n(41), stop words (179), and other words that change the sentiment of the next\nword in the sentence. The classiﬁcation accuracy at the level of reviews is\n80.71% and at the level of sentences, it is 73.70%.\nThe ﬁrst balanced and topically uniform sentiment analysis dataset in\nSerbian (SerbMR, Table 4) was generated by Batanovi´c et al. (2016) and is\navailable online in versions with two sentiment polarity classes (positive and\nnegative; 1682 documents) and three polarity classes (positive, neutral, and\nnegative; 2523 documents). The sentiment labels, in this dataset, were obtained\nautomatically by converting the numerical ratings attached to each review by\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n21\nits author. This dataset was examined to identify the best machine-learning\nfeatures and simple text-processing options for sentiment classiﬁcation. By\ncombining the obtained optimal attributes with NBSVM (combination of\npolynomial Naive Bayes classiﬁer and support vector method classiﬁer), they\nachieved an accuracy of up to 85.55% for two and up to 62.69% for three\nclasses. By comparing diﬀerent methods for morphological normalization, it\nwas concluded that the use of stemmer is better than lemmatization in the\ncase of sentiment analysis. The stemmer of Ljubeˇsi´c and Pandˇzi´c gave the best\naccuracy results on the dataset SerbMR, 86.11% for two and up to 63.02% for\nthree classes (Batanovi´c and Nikoli´c, 2017).\nAccording to the studies cited above, identifying the presence of negation is\ninsuﬃcient to ascertain sentiment. The collection of ﬁlm reviews in (Batanovi´c\net al., 2016) is subjected to the traditional method of processing negation,\nwhich involves changing the polarity of words that follow a negative signal. For\nthree classes, marking two words after the negation led to the most signiﬁcant\nimprovement in sentiment analysis accuracy (0.94%), while for two classes,\nmarking only the ﬁrst word after the negation gave the best improvement in\naccuracy (0.66%). The processing rules of semantic negation, which improved\nthe classiﬁcation of short informal texts by sentiment, are described in Ljaji´c\nand Marovac (2019). These rules were tested on a set of tweets with topic pub-\nlic personalities that were manually marked with the sentiment (TWPP, Table\n4). The machine learning method that uses additional attributes based on the\nproposed negation processing rules improves sentiment analysis accuracy on\na set of tweets for three classes by up to 1.45% and for two classes by up to\n0.82%. When this method is applied to a set of tweets containing negation,\nthe improvement in sentiment analysis accuracy increases by up to 2.65% for\nthree classes and up to 1.65% for two classes. For this study’s aims, dictionar-\nies of negation signals (25), negative quantiﬁers (56), and intensiﬁers, as well\nas a sentiment dictionary of 5632 sentiment words (reduced to the morpholog-\nical foundation of 4058 negative and 1574 positive words), were constructed.\nThe impact of various morphological normalizations on sentiment analysis was\nexamined on this set of tweets, and it was discovered that the use of stemmer\n(Miloˇsevi´c, 2012b) takes precedence over normalization using the morphologi-\ncal dictionary SrbMD (accuracy 85.27%) and that reducing words to 4-grams\nproduces good results with little resource usage (Ljaji´c et al., 2019).\nAspect-based sentiment analysis deals with the identiﬁcation of sentiments\n(negative, neutral, positive) and the determination of aspects (target senti-\nments) in a sentence. Nikoli´c et al. (2020) proposed an aspect-based sentiment\nanalysis of student opinion surveys in the Serbian language. Two sets of\ndata were used for sentiment analysis, which was done at the ﬁnest level of\ngranularity of the text - the level of the sentence segment (phrase and sentence).\nA collection of oﬃcial student surveys (OSS, Table 4) makes up the ﬁrst\ndataset, while the second dataset set of online reviews of professors and lec-\nturers (OSPL, Table 4) previously created for the paper (Grljevi´c, 2016). The\nSpringer Nature 2023 LATEX template\n22\nA Survey of Resources and Methods for NLP of Serbian Language\nOSS and OSPL corpora were automatically annotated for the sentiment (neg-\native, neutral, positive), then manually annotated for aspects (ranging from\nlower-level features, such as lectures, helpfulness, materials, and organization,\nto higher-level aspects, such as professor, course, and other). For aspect clas-\nsiﬁcation, a cascade classiﬁer (a collection of SVM binary classiﬁers trained\nto distinguish between two distinct aspects) was employed. The quality of the\naspect analysis was inﬂuenced by the corpus, as seen by the F-measures of\n0.89 for the OSS corpus and 0.78 for the OSPL corpus, respectively.\nTable 4 Corpora for sentiment analysis\nCorpus label\nText type\nNumber\nof\nitems\nAnnotation1\nReference\nTrN\nNews\n2000\nS; AA\n(Mladenovi´c et al., 2016)\nTsN\nNews\n779\nS; AA\n(Mladenovi´c et al., 2016)\nTsMR\nMovie reviews\n2237\nS; AA\n(Mladenovi´c et al., 2016)\nORPL\nEducation\nReviews\n3863\nS, A; AA + MA\n(Grljevi´c, 2016)\nOSS\nEducation\nReviews\n2472\nS,A; AA +MA\n(Nikoli´c et al., 2020)\nSerbMR\nNews\n2523\nS;AA\n(Batanovi´c et al., 2016)\nSentiComments.SR\nShort texts\n3490\nS; MA\n(Batanovi´c, 2020)\nParlaSent-BCS v1.0\nSentences\nof\nparliamentary\ndebates\n2600\nS; MA\n(Mochtak et al., 2022)\nTWPP\nTweets\n7664\nS; MA\n(Ljaji´c and Marovac, 2019)\nTWVA\nTweets\n8817\nS,R; MA\n(Ljaji´c et al., 2022)\nMRSA\nMusic Reviews\n1830\nS; AA\n(Draˇskovi´c et al., 2022)\nSMSSA\nSMS messages\n6171\nS; MA\n(ˇSandrih, 2019)\nTW15\nTweets\n1643735\nS; MA\n(Mozetiˇc et al., 2016)\n1Annotation target: S - sentiment, A - aspect, R - relevance; Annotation type: AA - automated\nannotation MA - manually annotated\nSentiment analysis includes speciﬁc subtasks such as polarity detection,\nsubjectivity detection, sarcasm detection, etc. An annotation approach with six\nsentiment labels was created to satisfy the requirements of processing particu-\nlar tasks and enabling multiple interpretations of sentiment (Batanovi´c et al.,\n2020). SentiComments.SR (Table 4), a corpus of short texts in the Serbian lan-\nguage, has been manually annotated using this multi-level annotation scheme.\nIt contains 3490 short movie comments (length up to 50 tokens) (Batanovi´c,\n2020). On this corpus, the outcomes of applying linear classiﬁers using bag-of-\nwords and/or bag-of-embedding features were evaluated under the inﬂuence\nof diﬀerent morphological normalizations and negation processing techniques.\nThe combination of bag-of-words and bag-of-embeddings attributes resulted in\nsigniﬁcant improvements in classiﬁcation for all sentiment analysis subtasks (F\n- measure: polarity 0.783, subjectivity 0.885 four-class sentiment analysis 0.655,\nsix-class sentiment analysis 0.586). Due to the insuﬃcient number of sarcastic\ntexts in the corpus, the results of sarcasm detection are not representative.\nSentiment lexicon Senti-Pol-sr (Stankovi´c et al., 2022) was created based on\nthree existing lexicons (NRC, AFFIN, and Bing) and was manually corrected.\nThe dictionary contains 6454 diﬀerent tokens. Its initial version is available.\nThe lexicon was utilized to conduct sentiment analysis on a well-balanced\ndataset extracted from SrpELTeC, which consisted of 1089 sentences that were\nmanually labeled, with each sentiment category containing 363 instances of\npositive, neutral, and negative sentiments. This approach achieved the best\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n23\naccuracy of 87.8% on SrpELTeC 2 classes and 71.9% on SrpELTeC 3 classes\nusing MNB with the Bag-of-Words approach combined with our sentiment\nlexicon features. The results of trained models using LR, NB, decision tree,\nrandom forest, SVN, and k-NN methods gave the best accuracy of 87.8%\nfor LR. It has also been shown that training on a dataset of labeled movie\nreviews (SerbMR) indicates that it cannot be successfully used for sentence\nsentiment analysis in old novels. Draˇskovi´c et al. (2022) developed a machine-\nlearning model for sentiment analysis using three diﬀerent data sets. The ﬁrst\nset (MRSA, Table 4) was created for this research by collecting music reviews\nfrom 13 portals, which made sure that the set was balanced. The second data\nset is the already mentioned set of movie reviews, while the third set is music\nalbum reviews—MARD. MARD was originally composed in English and then\ntranslated into Serbian using the Google Translate API. Standard classiﬁca-\ntion models (NB, LR, and SVM) and hybrid models (combining a linear model\nwith NB) were applied to these datasets. The hybrid model NB-LR gave aver-\nage good results (58% for three classes and 79% for two classes). It is shown\nthat a set of ﬁlm and music reviews can be used together to improve the qual-\nity model. Extending the model with reviews translated from English does\nnot improve performance, due to the diﬀerent vocabulary and review writing\nstyles, as well as the quality of the translated text. Emoticon inﬂuence, informal\nspeech, lexical, and other language features about the mood in the set of SMS\nmessages (SMSSA, Table 4) are presented by ˇSandrih (2019). They selected 621\nfeatures and divided them into three main categories: lexical (based on signs\nand words), syntactic (emoticons, abbreviations), and stylistic. Using linear\nSVM classiﬁcation, an accuracy of 92.3% was obtained. Sentence-based senti-\nment classiﬁcation as well as emotion recognition is suggested to improve the\nclassiﬁcation of SMS messages. Mozetiˇc and Grˇcar (Mozetiˇc and Grˇcar) found\nthat the quality of the classiﬁcation model depends much more on the quality\nand size of the training data than on the type of trained model by analyz-\ning 1.6 million manually tagged tweets in 15 diﬀerent European languages, of\nwhich 73,783 tweets are in Serbian (TW15, Table 4). Based on the performed\nexperiments, it was shown that there is no statistically signiﬁcant diﬀerence\nbetween the performance of the top classiﬁcation models (ﬁve of these models\nare based on SVM, and for reference, the NB classiﬁer was applied).\nTransfer learning is one of the advanced techniques in AI, which allows a\npre-trained model to transfer its knowledge to a new model. Transfer learning is\nfrequently used in sentiment analysis to classify sentiments, and it can produce\nsuccessful results, particularly in the absence of large labeled data sets.\nBatanovic presented the results of applying neural language models based\non transformer architectures to sentiment analysis subtasks of short texts from\nthe SentiComments.SR corpus (Batanovi´c et al., 2020).Three transformer-\nbased models were used: Multilingual BERT (Devlin et al., 2018), Multilingual\ndistilBERT (Sanh et al., 2019), and XLM MLM (Conneau and Lample, 2019).\nFine-tuning multilingual transformer-based models gain the same or better\nperformance than linear models for all sentiment analysis subtasks. For each\nSpringer Nature 2023 LATEX template\n24\nA Survey of Resources and Methods for NLP of Serbian Language\nsubtask, XLM MLM produced the best F-measure results: 0.793 for polarity,\n0.887 for subjectivity, 0.686 for four-class sentiment analysis, and 0.627 for\nsix-class sentiment analysis.\nBased on a sample of parliamentary discussions, Mochtak et al. (2022)\ndemonstrated that using transformer models produces outcomes that are\nnoticeably superior to those obtained using a simpler architecture. The dataset\nconsists of sentences of average length from the corpus of parliamentary pro-\nceedings in the region of the former Yugoslavia - Bosnia and Herzegovina,\nCroatia, and Serbia. A set of 2600 sentences (ParlaSent-BCS v1.0, Table 4),\nincluding 876 with only positive, 876 with only negative, and 866 without sen-\ntiment words, were chosen for the dataset using the Croatian gold standard\nsentiment lexicon (Glavaˇs et al., 2012) (translated to Serbian with a rule-based\nCroatian-Serbian translator (Klubiˇcka et al., 2016)). This dataset contains\n1059 sentences from the Serbian parliament. The dataset is manually anno-\ntated using the multiple-level annotation schema described by Batanovi´c et al.\n(2020), and it is available online. A sentiment analysis approach was applied\nat the sentence level. The results of classiﬁcation four of the transformer\nmodels were compared: FastText (Bojanowski et al., 2017) with pre-trained\nCLARIN.SI word embeddings (Ljubeˇsi´c and Erjavec, 2018), XLM-R (Con-\nneau et al., 2019), CroSloEngual BERT (Ulˇcar and Robnik-ˇSikonja, 2020),\nand BERTi´c (Ljubeˇsi´c and Lauc, 2021). BERTi´c gave the best results com-\npared to the others (model macro F1 0.7941 ± 0.0101). Compared to Bosnian\nand Croatian, the Serbian language proved to be the most diﬃcult to pre-\ndict. Using BERTi´c for sentiment analysis, Ljaji´c et al. (2022) expanded the\nannotated dataset that was used for the topic analysis of tweets containing\nnegative sentiment towards the COVID-19 vaccination. A collection of 8817\nvaccination-related tweets in the Serbian language (TWVA, Table 4) were\nmanually labeled as relevant or irrelevant regarding the COVID-19 vaccina-\ntion sentiment. Relevant tweets were manually marked with sentiment labels:\npositive, negative, or neutral. On this data set, BERTi´c correctly categorized\ntweets as relevant or irrelevant with a 94.7% accuracy rate and correctly clas-\nsiﬁed relevant tweets as negative, positive, or neutral with an 85.7% accuracy\nrate. The annotated set was expanded by this classiﬁer, and from the orig-\ninal manually annotated 1770 tweets with negative sentiment, another 1516\ntweets with negative sentiment were automatically marked, forming the data\nset used for the topic analysis. The topic analysis was carried out using the\nlatent Dirichlet allocation (LDA) and nonnegative matrix factorization (NMF)\nmethods. Topics that are potential reasons for vaccine skepticism are high-\nlighted by topic analysis: worries about adverse reactions, eﬃcacy, inadequate\ntesting, mistrust of authorities, and conspiracy theories.\n7 Named entity recognition\nNamed-entity recognition (NER) is a task that seeks to locate and classify\nnamed entities mentioned in unstructured text into categories such as personal\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n25\nnames, organizations, locations, medical codes, time expressions, quantities,\nmonetary values, percentages, etc. Named entity recognition (NER) as an\nNLP task is fairly old, gaining popularity with Message Understanding Con-\nferences in the mid-1990s (Sekine, 2004), However, NER for Serbian has not\nbeen addressed substantially until the 2010s.\nVitas and Pavlovi´c-Laˇzeti´c (2008) developed a system that uses morpho-\nlogical and lexical analysis in combination with dictionaries (Serbian and\ntranscribed English ﬁrst names, and geographical locations) for recognition of\npeople’s names and geographical entities. The system is using e-dictionaries\nand transducer-based rules or grammars for disambiguation of proper names\nand geopolitical entities (Krstev et al., 2007; Vitas and Pavlovi´c-Laˇzeti´c, 2008).\nLjubeˇsi´c et al. (2013) proposed a ﬁrst system based on machine learning\nand conditional random ﬁelds for the recognition of names, organizations, and\nlocations for Croatian and Slovene, which are closely related to Serbian. They\nhave used a set of annotated web and news corpora (SETimes, Vjesnik, and\ncorpora for both Slovene and Croatian developed as a student project (Filipi´c\net al., 2012)) to train their method. For features, they used linguistic fea-\ntures and distributional similarity features calculated from large unannotated\nmonolingual corpora. Their experiments showed that distributional features\nimprove the F1 score by 7-8 points, while morphological features can improve\nby additional 3-4 points. However, as the size of the dataset increases, the mor-\nphosyntactic and distributional features lose their importance for NER. They\nhave made resources used for building this NER system publicly available.\nAnother approach, based on the previous application of rules encoded in\ntransducers and thesauri (Krstev, 1997) was enhanced for recognition of per-\nsonal names and geopolitical names (Krstev et al., 2014). Dictionaries are used\nfor matching tokens and phrases, while recursive transition networks (gram-\nmar graphs) from Unitex (Paumier et al., 2002) are used to resolve ambiguities\n(e.g. taking into account grammatical rules such as case-number-gender agree-\nment). They reported that the system prefers precision over recall, with a\nprecision of 0.96 and a recall of 0.88.\nFor the purpose of comparing NER approaches in multi-lingual aligned\ntexts (bitexts), a system called NERosetta was developed (Krstev et al., 2013;\nKrstev C et al., 2013). To illustrate the system, 7 bitexts involving 5 languages\n(French, English, Greek, Serbian, Croatian) and 5 diﬀerent NER systems were\nused (1 for Serbian (Krstev et al., 2014), 1 for Croatian (Ljubeˇsi´c et al., 2013),\n1 for English (Stanford NER) and 2 for French). The entities that were evalu-\nated were Person, Organization, and Location, with some of the NER systems\nproviding annotations for time, date, money, percent, and others. The demo\napplication is available on the web 22.\nA dictionary approach with the addition of transducer-based grammars\n(Krstev et al., 2014) was used to create a gold standard data set based on\nnews articles annotated with personal names. This data set was then used to\ntrain machine learning-based approaches, namely Stanford NER and SpaCy\n22http://www.korpus.matf.bg.ac.rs/nerosetta/\nSpringer Nature 2023 LATEX template\n26\nA Survey of Resources and Methods for NLP of Serbian Language\n(ˇSandrih et al., 2019). Their evaluation indicated that the rule-based approach\nperformed the best (based on the F1-score), while Stanford NER had the best\nrecall.\nTanasijevi´c (2019) developed a system for labeling cultural heritage doc-\numents with metadata. In order to do this, she developed a system that\nrecognizes entities, such as years and person names, as well as topics of the\ntagged documents.\nThe transformer-based model was also introduced for several tasks in Ser-\nbian, Croatian, and Slovene, including NER (Ljubeˇsi´c and Lauc, 2021). The\nmodel was pre-trained on web-crawled texts in Serbian, Bosnian, Croatian,\nand Slovene, consisting of 8 billion tokens, and then ﬁne-tuned for NER on\nseveral openly available datasets, such as SETimes.SR (Batanovi´c et al., 2018),\ncorpora of news articles, or ReLDI-sr (Ljubeˇsi´c et al., 2017), corpora of anno-\ntated tweets. For reference, authors compared this model with CroSloEngual\nBERT (Ulˇcar and Robnik-ˇSikonja, 2020) and multilingual BERT (Devlin et al.,\n2018), where language-speciﬁc BERT-based models signiﬁcantly outperformed\nmulti-lingual BERT.\nApart from a general domain for Serbian, a decent amount of work has been\ndone for medical named entity recognition. One of the previously described\nsystems for a general domain was adapted for medical de-identiﬁcation of clin-\nical texts (Ja´cimovi´c et al., 2014, 2015). The system recognized persons, dates,\ngeographic locations, organizations, and numbers using vocabularies and trans-\nducer grammar rules. The authors reported an overall F1 score of 0.94. On\nthe other hand, (Puﬂovi´c et al., 2016) created a model based on character\nand word n-grams. The dataset they used was obtained from a neurological\nclinic and their system was designed to recognize names of diseases, names\nof medications, abbreviations, and numbers that represent dosage, dates or\ntimes, and medical treatment success. They have manually checked 100 docu-\nments, reporting accuracy ranging from 64% to 90%. A mathematical model\nfor medical term recognition was proposed by (Avdi´c et al., 2020). They have\nproposed three methods. The ﬁrst method was based on the dictionary match-\ning of terms. The second method uses a formula for labeling words contained\nin the training set, where conﬁdence is calculated by the number of instances\nin which a word is labeled with a certain label in the training set divided by\nthe total count of that word in the training set. The third method is an exten-\nsion of the second method, where several rules are added to terms with errors\nand abbreviations, so they can be tagged well. Labeled entities included medi-\ncal terms (symptoms, symptom descriptions, diagnoses, biochemical analyses,\nLatin words, anatomic names of organs, therapies, and other medical terms)\nand non-medical terms (numbers, negation symbols, and other words). The\nbest-performing model was the third one, with an F1 score of 0.937, while the\nhighest F1 score for medical terms was 0.896. The methods based on deep\nneural networks and multilingual language models were proposed by Kaplar\net al. (2022). They used a manually annotated corpora from the Clinic for\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n27\nNephrology at the University Clinical Center of Serbia (203 discharge sum-\nmaries annotated by 2 computer science Ph.D. students adapting the 2012 i2b2\ntemporal relation challenge annotation schema). They have created models\nbased on conditional random ﬁelds (CRF), multilingual transformers (BERT\nMultilingual and XLM RoBERTa), long short-term memory (LSTM) recurrent\nneural networks, and their ensembles. CRF method had hand-crafted features\nthat are commonly used in literature (word, word stem, shape of the word,\nprevious 3 words, next 3 words, etc.). For the LSTM model, the authors used\ngensim’s word2vec model before feeding the embeddings to the LSTM net-\nwork, followed by the CRF token classiﬁer. The study showed that the highest\nprecision was achieved with the CRF-based model, while the highest recall\nhad a multi-lingual transformer model. The best F1 score had an LSTM-CRF-\nbased model. The best performance was achieved by creating an ensemble of\nthe models with majority voting (F1 score of 0.892).\n8 Language models\nA language model determines word probability in a sequence. In order to\ncreate a language model, many approaches were proposed, ranging from simply\ncalculating word appearance in a larger text corpus to adding more lexical\nand syntactic features to learning word probabilities. Early language models\nwere purely statistical, while since 2014, we have seen a proliferation of neural\nlanguage models - language models based on neural networks.\nLanguage models are prerequisites for many natural language tasks.\nTherefore, many works in classiﬁcation (Graovac, 2014), sentiment analysis\n(Miloˇsevi´c, 2012a; Joli´c, 2015; Grljevi´c, 2016; Batanovi´c and Nikoli´c, 2017;\nLjaji´c and Marovac, 2019) or named entity recognition (ˇSandrih et al., 2019),\nused traditional n-gram language models, at times enriched with lexical, mor-\nphological or syntactic features. These systems were previously described in\nthis review.\nOstrogonac (2018) in his PhD thesis does a review and comparison of lan-\nguage models for Serbian up to 2018. In this work, he proposes the ﬁrst neural\nlanguage model for Serbian, based on recurrent neural networks trained on a\ncorpus of morphologically annotated text in Serbian. Also, he creates a hybrid\nmodel that uses parts-of-speech and lemmas, and matches sequences of words\nto either n-grams in corpus, or to partially lemmatized sequences. These mod-\nels are compared with more traditional n-gram models for correcting semantic\nand grammatical errors in the text. The error is detected by setting a threshold\nfor a diﬀerence in log likelihood between a language model with morphological\nfeatures and one without it. While setting thresholds may be challenging, it\nshowed the potential use cases for speciﬁcally trained neural language models\nfor Serbian.\nThere has been a signiﬁcant eﬀort done by international researchers to\ncreate multilingual neural language models. Some of these models included\nalso Serbian, such as FastText (Bojanowski et al., 2017), multilingual BERT\nSpringer Nature 2023 LATEX template\n28\nA Survey of Resources and Methods for NLP of Serbian Language\n(Devlin et al., 2018), XLM-R (Conneau et al., 2019), and XLM MLM (Con-\nneau and Lample, 2019). Batanovi´c in his Ph.D. thesis (Batanovi´c, 2020),\ncompared a number of n-gram language models for the tasks of sentiment\nanalysis and text similarity. He further compared these language models and\nmethods with ﬁne-tuned multi-lingual transformer-based models (multilingual\nBERT base (Devlin et al., 2018), DistilBERT Multilingual (Sanh et al., 2019),\nand XLM MLM (Conneau and Lample, 2019)), showing transformer models\nin all cases outperforming all n-gram based models (including ones containing\na large amount of morphological, lexical, and syntactic features).\nThe ﬁrst, and, at the time of writing of this paper, the only transformer-\nbased language model speciﬁcally trained for Serbian, Croatian, Bosnian, and\nMontenegrin is BERTi´c (Ljubeˇsi´c and Lauc, 2021). BERTi´c is trained using\nthe ELECTRA approach (Clark et al., 2020) for training transformer mod-\nels. This approach involves training a smaller generator model and the main\ndiscriminator model with the task to discriminate whether a speciﬁc word is\nan original word from the text or a word generated by the generator model.\nThe model is trained on a corpus of 8 billion tokens crawled from the web in\nSerbian, Croatian, Bosnian, and Montenegrin. While there was previously a\nBERT-based model for Croatian and Slovenian, called CroSloBERT (Ulˇcar and\nRobnik-ˇSikonja, 2020), BERTi´c outperformed it on almost all tasks (morpho-\nlogical annotation, NER, social media geolocation prediction, commonsense\ncausal reasoning task). This is mainly because of the bigger corpus, and\ncomputational eﬃciency of the ELECTRA approach that was used.\n9 Conculsion and future directions\nResearch on natural language processing for the Serbian language has a long\ntradition, going back to the second half of the 1990s. During this time,\nmany approaches for lexical, morphological, syntactic, and semantic process-\ning of text were explored. In the past decade, the number of researchers and\nresearch published on natural language processing for Serbian signiﬁcantly\nincreased. Several Universities and research institutes in Serbia established\nnatural language research groups.\nThe Serbian language is a highly inﬂected language and therefore many\nchallenges in natural language processing are speciﬁc to Serbian, such as the\nmost eﬃcient way for tokenization, handling the inﬂections in various tasks,\nhandling negations, etc. While some work has been done on these challenges,\nthey are still open research questions. Basic lexical and morphological tasks,\nsuch as transliteration, diacritic restoration, tokenization, stemming, lemma-\ntization, and part-of-speech tagging are quite well-researched, with many\napproaches presented, evaluated, and compared. Some of the classiﬁcation\ntasks, such as sentiment analysis were, as well, extensively researched. Sen-\ntiment analysis seems to gain a lot of interest after 2012. Named entity\nrecognition has also been researched for several named entities, such as proper\nand personal names, and locations. Also, few approaches have been proposed\nfor biomedical NER.\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n29\nOn the other hand, some methods and tasks were still not adequately\naddressed for Serbian. Many classiﬁcation tasks, except sentiment analysis,\nhave not been explored and language resources for them are missing. As it\nwas previously said, methods for only a basic set of named entities have been\nproposed. Domain-speciﬁc classiﬁcation and named entity recognition methods\nare still missing.\nMethods in the semantic web, ontology, and semantic networks have not\nmuch proliferated in the area of Serbian NLP, as only a few papers are touching\non this subject. Most signiﬁcant research in network space has been done in\ndeveloping Serbian WordNet, but this is a rather morphological and lexical\nnetwork, then something that can be considered a semantic network. Language\nresources for many of the tasks are still missing.\nWhile the language-speciﬁc BERT-based model has been trained, there is\nonly a single initiative to create this kind of language model. Also, resources\nsuch as sentence embedding or document embedding methods have not been\nyet developed. These methods would also contribute signiﬁcantly to the cre-\nation of methods for summarization, question answering, language-speciﬁc\nsemantic search, or machine translation.\nAt the moment, there is a proliferation of large language models, such as\nGPT-3 (Brown et al., 2020), Lambda (Thoppilan et al., 2022) or ChatGPT\n(Ouyang et al., 2022). While these models are multilingual and can generate\ntext in Serbian, there has not yet been much research on prompt engineering\nor ﬁne-tuning these language models for Serbian.\n10 Acknowledgements\nThis paper is partially supported by the Ministry of Education, Science, and\nTechnological Development of the Republic of Serbia, Projects No. III44007.\nReferences\nAgi´c, ˇZ., N. Ljubeˇsi´c, and D. Merkler 2013. Lemmatization and morphosyn-\ntactic tagging of croatian and serbian. In Proceedings of the 4th Biennial\nInternational Workshop on Balto-Slavic Natural Language Processing, pp.\n48–57.\nAkhmetov, I., A. Pak, I. Ualiyeva, and A. Gelbukh. 2020. Highly language-\nindependent word lemmatization using a machine-learning classiﬁer. Com-\nputaci´on y Sistemas 24(3): 1353–1364 .\nAndonovski, J. 2020.\nMreˇza otvorenih podataka i jeziˇcki resursi u procesu\nizgradnje srpsko-nemaˇckog literarnog korpusa. Ph. D. thesis.\nAndonovski, J., B. ˇSandrih, and O. Kitanovi´c. 2019. Bilingual lexical extrac-\ntion based on word alignment for improving corpus search. The Electronic\nLibrary .\nSpringer Nature 2023 LATEX template\n30\nA Survey of Resources and Methods for NLP of Serbian Language\nAurousseau, M. 1953. Transliteration of cyrillic script. Nature 171: 940–940 .\nAvdi´c, A. 2021. Realizacija servisa pametnog zdravstva i njihova integracija u\nkoncept pametnih gradova. Ph. D. thesis.\nAvdi´c, A., U. Marovac, and D. Jankovi´c. 2020. Automated labeling of terms\nin medical reports in serbian. Turkish Journal of Electrical Engineering and\nComputer Sciences 28(6): 3285–3303 .\nBalvet, A., D. Stosic, and A. Miletic 2014. Talc-sef a manually-revised pos-\ntagged literary corpus in serbian, english and french. In LREC 2014.\nBatanovi´c, V., M. Cvetanovi´c, and B. Nikoli´c 2018. Fine-grained semantic\ntextual similarity for serbian. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation (LREC 2018).\nBatanovi´c, V., M. Cvetanovi´c, and B. Nikoli´c. 2020. A versatile framework\nfor resource-limited sentiment articulation, annotation, and analysis of short\ntexts. PLoS One 15(11): e0242050 .\nBatanovi´c, V., B. Furlan, and B. Nikoli´c. 2011. Softverski sistem za odred-\njivanje semantiˇcke sliˇcnosti kratkih tekstova na srpskom jeziku.\nZbornik\nradova sa 19. telekomunikacionog foruma (TELFOR 2011): 1249–1252 .\nBatanovi´c, V., N. Ljubeˇsi´c, and T. Samardˇzi´c 2018. Setimes. sr–a reference\ntraining corpus of serbian. In Proceedings of the Conference on Language\nTechnologies & Digital Humanities 2018 (JT-DH 2018), pp. 11–17.\nBatanovi´c, V., N. Ljubeˇsi´c, T. Samardˇzi´c, and M.M. Petrovi´c. 2020. Otvoreni\nresursi i tehnologije za obradu srpskog jezika. Proc. of the Primena slobodnog\nsoftvera i otvorenog hardvera .\nBatanovi´c, V. and B. Nikoli´c. 2017. Sentiment classiﬁcation of documents in\nserbian: The eﬀects of morphological normalization and word embeddings.\nTelfor Journal 9(2): 104–109 .\nBatanovi´c, V., B. Nikoli´c, and M. Milosavljevi´c 2016. Reliable baselines for\nsentiment analysis in resource-limited languages: The serbian movie review\ndataset. In Proceedings of the Tenth International Conference on Language\nResources and Evaluation (LREC’16), pp. 2688–2696.\nBatanovi´c, V.V. 2020. Metodologija reˇsavanja semantiˇckih problema u obradi\nkratkih tekstova napisanih na prirodnim jezicima sa ograniˇcenim resursima.\nPh. D. thesis, Univerzitet u Beogradu-Elektrotehniˇcki fakultet.\nBatanovi´c V, V., N. Ljubeˇsi´c, T. Samardzi´c, and T. Erjavec. 2018. Training\ncorpus setimes. sr 1.0 .\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n31\nBogdanovi´c, M. and J. Toˇsi´c. 2022. Corpus of legislation texts of republic of\nserbia 1.0. Slovenian language resource repository CLARIN.SI.\nBogeti´c, K. and V. Batanovi´c. 2020a. Annotated corpus of serbian language-\nrelated news articles MetaLangNEWS-sr.\nSlovenian language resource\nrepository CLARIN.SI.\nBogeti´c, K. and V. Batanovi´c. 2020b. Annotated corpus of serbian language-\nrelated news comments MetaLangNEWS-COMMENTS-sr. Slovenian lan-\nguage resource repository CLARIN.SI.\nBojanowski, P., E. Grave, A. Joulin, and T. Mikolov. 2017. Enriching word\nvectors with subword information.\nTransactions of the association for\ncomputational linguistics 5: 135–146 .\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J.D. Kaplan, P. Dhariwal, A. Nee-\nlakantan, P. Shyam, G. Sastry, A. Askell, et al. 2020. Language models are\nfew-shot learners. Advances in neural information processing systems 33:\n1877–1901 .\nClark, K., M.T. Luong, Q.V. Le, and C.D. Manning. 2020.\nElectra: Pre-\ntraining text encoders as discriminators rather than generators.\narXiv\npreprint arXiv:2003.10555 .\nConneau, A., K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzm´an,\nE. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov. 2019.\nUnsu-\npervised cross-lingual representation learning at scale.\narXiv preprint\narXiv:1911.02116 .\nConneau, A. and G. Lample. 2019. Cross-lingual language model pretraining.\nAdvances in neural information processing systems 32 .\nCorpora etc, C. 1992. Serbo-croatian text corpus. Oxford Text Archive.\nDalianis, H. and B. Jongejan 2006.\nHand-crafted versus machine-learned\ninﬂectional rules: The euroling-siteseeker stemmer and cst’s lemmatiser. In\nProceedings of the Fifth International Conference on Language Resources\nand Evaluation (LREC’06).\nDeli´c, V., M. Seˇcujski, N. Jakovljevi´c, M. Janev, R. Obradovi´c, and D. Pekar.\n2010. Speech technologies for serbian and kindred south slavic languages.\nAdvances in Speech Recognition: 141–164 .\nDevlin, J., M.W. Chang, K. Lee, and K. Toutanova. 2018. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805 .\nSpringer Nature 2023 LATEX template\n32\nA Survey of Resources and Methods for NLP of Serbian Language\nDobri´c, N. 2012. Savremeni jeziˇcki korpusi na zapadnom balkanu–istorijat,\ntrenutno stanje i buduˇcnost. Slavistiˇcna revija 60(4): 677–692 .\nDraˇskovi´c, D., D. Zeˇcevi´c, and B. Nikoli´c. 2022.\nDevelopment of a mul-\ntilingual model for machine sentiment analysis in the serbian language.\nMathematics 10(18): 3236 .\nEberhard, D.M., F.S. Gary, and D.F. Charles. 2022. Ethnologue: Languages\nof the world.\nFilipi´c, L., T. Juri´c, and M. Stupar. 2012. Strojno prepoznavanje naziva u tek-\nstovima pisanima hrvatskim jezikom. Studentski znanstveni rad, Rektorova\nnagrada, Filozofski fakultet, Sveuˇciliˇste u Zagrebu .\nFurlan, B., V. Batanovi´c, and B. Nikoli´c. 2013. Semantic similarity of short\ntexts in languages with a deﬁcient natural language processing support.\nDecision Support Systems 55(3): 710–719 .\nGerych, I. 1965. Transliteration of Cyrillic alphabets. Ph. D. thesis, University\nof Ottawa (Canada).\nGesmundo, A. and T. Samardzic 2012.\nLemmatising serbian as category\ntagging with bidirectional sequence classiﬁcation.\nIn Proceedings of the\nEighth International Conference on Language Resources and Evaluation\n(LREC’12), pp. 2103–2106.\nGlavaˇs, G., J. ˇSnajder, and B. Dalbelo Baˇsi´c 2012. Semi-supervised acquisition\nof croatian sentiment lexicon. In International Conference on Text, Speech\nand Dialogue, pp. 166–173. Springer.\nGra¨en, J., M. Bertamini, M. Volk, M. Cieliebak, D. Tuggener, and F. Ben-\nites 2018. Cutter–a universal multilingual tokenizer. In CEUR Workshop\nProceedings, Number 2226, pp. 75–81. CEUR-WS.\nGraovac, J.B. 2014. Prilog metodama klasiﬁkacije teksta: matematiˇcki modeli\ni primene. Ph. D. thesis.\nGrass, T., D. Maurel, and O. Piton 2002.\nDescription of a multilingual\ndatabase of proper names. In Advances in Natural Language Processing:\nThird International Conference, PorTAL 2002 Faro, Portugal, June 23–26,\n2002 Proceedings, pp. 137–140. Springer.\nGrljevi´c, O. 2016. Sentiment u sadrˇzajima sa druˇstvenih mreˇza kao instrument\nunapredjenja poslovanja visokoˇskolskih institucija. Ph. D. thesis.\nHolmes, G., A. Donkin, and I.H. Witten 1994.\nWeka: A machine learning\nworkbench. In Proceedings of ANZIIS’94-Australian New Zealnd Intelligent\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n33\nInformation Systems Conference, pp. 357–361. IEEE.\nJa´cimovi´c, J., C. Krstev, and D. Jelovac 2014. Automatic de-identiﬁcation\nof protected health information. In Proceedings of the 17th International\nMulticonference INFORMATION SOCIETY–IS 2014, Language Technolo-\ngies, October 9th- 10th, 2014, Ljubljana, Slovenia, pp. 73–78. Joˇzef Stefan\nInstitute, Ljubljana, Slovenia.\nJa´cimovi´c, J., C. Krstev, and D. Jelovac. 2015.\nA rule-based system for\nautomatic de-identiﬁcation of medical narrative texts. Informatica 39(1) .\nJoli´c, N. 2015.\nKlasiﬁkacija sentimenta u twitter postovima koriˇs´cenjem\nudaljenog nadzora .\nJurˇsic, M., I. Mozetic, T. Erjavec, and N. Lavrac. 2010. Lemmagen: Multi-\nlingual lemmatisation with induced ripple-down rules. Journal of Universal\nComputer Science 16(9): 1190–1214 .\nKaplar, A.,\nM.\nStoˇsovi´c, A.\nKaplar, V.\nBrkovi´c, R.\nNaumovi´c, and\nA. Kovaˇcevi´c. 2022. Evaluation of clinical named entity recognition meth-\nods for serbian electronic health records. International Journal of Medical\nInformatics: 104805 .\nKeˇselj, V. and D. ˇSipka. 2008. A suﬃx subsumption-based approach to build-\ning stemmers and lemmatizers for highly inﬂectional languages with sparse\nresources. INFOtheca-Journal of Informatics & Librarianship 9 .\nKlajn, I. 2005. Gramatika srpskog jezika (I ed.). Belgrade: Zavod za udˇzbenike\ni nastavna sredstva.\nKlubiˇcka, F., G. Ram´ırez-S´anchez, and N. Ljubeˇsi´c 2016. Collaborative devel-\nopment of a rule-based machine translator between croatian and serbian. In\nProceedings of the 19th Annual Conference of the European Association for\nMachine Translation, pp. 361–367.\nKoeva, S., C. Krstev, and D. Vitas 2008.\nMorpho-semantic relations in\nwordnet–a case study for two slavic languages. In Global wordnet conference,\npp. 239–253. University of Szeged, Department of Informatics.\nKosti´c, A. 2014.\nElectronic corpus of serbian language from 12th to 18th\ncentury. Review of the National Center for Digitization .\nKovaˇcevi´c, L., V. Injac, and D. Begeniˇsi´c. 2004. Bibliotekarski terminoloˇski\nreˇcnik: englesko-srpski, srpsko-engleski. Narodna biblioteka Srbije.\nKrstev, C. 1997. Jedan prilaz informatiekom modeliranju teksta i algoritmi\nnjegove transformacije. Ph. D. thesis.\nSpringer Nature 2023 LATEX template\n34\nA Survey of Resources and Methods for NLP of Serbian Language\nKrstev, C., J. Ja´cimovi´c, and D. Vitas 2012. Recognition and normalization of\nsome classes of named entities in serbian. In Proceedings of the Fifth Balkan\nConference in Informatics, pp. 52–57.\nKrstev, C., D. Maurel, et al. 2007.\nA note on the semantic and mor-\nphological properties of proper names in the prolex project. Lingvisticae\nInvestigationes 30(1): 115–133 .\nKrstev, C., I. Obradovi´c, M. Utvi´c, and D. Vitas. 2014.\nA system for\nnamed entity recognition based on local grammars. Journal of Logic and\nComputation 24(2): 473–489 .\nKrstev, C., G. Pavlovi´c-Laˇzeti´c, and I. Obradovi´c. 2004. Using textual and\nlexical resources in developing serbian wordnet.\nRomanian Journal of\nInformation Science and Technology 7(1-2): 147–161 .\nKrstev, C., R. Stankovi´c, I. Obradovi´c, D. Vitas, and M. Utvi´c 2010. Auto-\nmatic construction of a morphological dictionary of multi-word units. In\nInternational Conference on Natural Language Processing, pp. 226–237.\nSpringer.\nKrstev, C., S.V. Stankovi´c, and D. Vitas 2014. Approximate measures in the\nculinary domain: Ontology and lexical resources. In Proceedings of the 9th\nLanguage Technologies Conference IS-LT, pp. 38–43.\nKrstev, C. and D. Vitas 2005. Corpus and lexicon-mutual incompletness. In\nProceedings of the Corpus Linguistics Conference, Volume 14, pp. 17.\nKrstev, C. and D. Vitas. 2011.\nAn aligned english-serbian corpus.\nELL-\nSIIR Proceedings (English Language and Literature Studies: Image, Identity,\nReality) 1: 495–508 .\nKrstev, C., D. Vitas, and G. Pavlovi´c-Laˇzeti´c 2008. Resources and methods\nin the morphosyntactic processing of serbo-croatian. In Formal Description\nof Slavic Languages: The Fifth Conference, pp. 3–17.\nKrstev, C., D. Vitas, and A. Savary 2006. Prerequisites for a comprehensive\ndictionary of serbian compounds. In International Conference on Natural\nLanguage Processing (in Finland), pp. 552–563. Springer.\nKrstev, C., A. Zeˇcevi´c, D. Vitas, and T. Kyriakopoulou 2013.\nNerosetta–\nan insight into named entity tagging.\nIn 6th Language and Technology\nConference, pp. 168–172.\nKrstev C, C., A. Zeˇcevi´c, D. Vitas, and T. Kyriacopoulou 2013. Nerosetta\nfor the named entity multi-lingual space.\nIn Language and Technology\nConference, pp. 327–340. Springer.\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n35\nLaporte, E. 2003. The relex network. Преузето са http://infolingu. univmlv.\nfr/Relex/Relex. htm .\nLemmenmeier-Batini´c, D., N. Ljubeˇsi´c, and T. Samardˇzi´c. 2021.\nCorpus\nof serbian forms of address 1.0.\nSlovenian language resource repository\nCLARIN.SI.\nLjaji´c, A. and U. Marovac. 2019. Improving sentiment analysis for twitter data\nby handling negation rules in the serbian language. Computer Science and\nInformation Systems 16(1): 289–311 .\nLjaji´c, A., U. Marovac, and M. Stankovi´c. 2019. Comparison of the inﬂuence of\ndiﬀerent normalization methods on tweet sentiment analysis in the serbian\nlanguage. Facta Universitatis, Series: Mathematics and Informatics: 683–\n696 .\nLjaji´c, A., N. Prodanovi´c, D. Medvecki, B. Baˇsaragin, J. Mitrovi´c, et al.\n2022.\nUncovering the reasons behind covid-19 vaccine hesitancy in\nserbia: Sentiment-based topic modeling.\nJournal of Medical Internet\nResearch 24(11): e42261 .\nLjubeˇsi´c, N., D. Boras, and O. Kubelka. 2007.\nRetrieving information in\ncroatian: Building a simple and eﬃcient rule-based stemmer .\nLjubeˇsi´c, N. and K. Dobrovoljc 2019.\nWhat does neural bring? analysing\nimprovements in morphosyntactic annotation and lemmatisation of slove-\nnian, croatian and serbian. In Proceedings of the 7th workshop on balto-slavic\nnatural language processing, pp. 29–34.\nLjubeˇsi´c, N., K. Dobrovoljc, and D. Fiˇser. 2015. Mwelex–mwe lexica of croat-\nian, slovene and serbian extracted from parsed corpora. Informatica 39(3)\n.\nLjubeˇsi´c, N. and T. Erjavec. 2018. Word embeddings clarin. si-embed. hr 1.0,\nslovenian language resource repository clarin. si (2018).\nLjubeˇsi´c, N., T. Erjavec, and D. Fiˇser 2016. Corpus-based diacritic restoration\nfor south slavic languages. In Proceedings of the Tenth International Con-\nference on Language Resources and Evaluation (LREC’16), pp. 3612–3616.\nLjubeˇsi´c, N., T. Erjavec, M. Miliˇcevi´c, and T. Samardˇzi´c. 2017. Serbian twitter\ntraining corpus reldi-normtagner-sr 2.0 .\nLjubeˇsi´c, N., M. Espl`a-Gomis, S. Ortiz Rojas, F. Klubiˇcka, and A. Toral. 2016.\nSerbian-english parallel corpus srenWaC 1.0. Slovenian language resource\nrepository CLARIN.SI.\nSpringer Nature 2023 LATEX template\n36\nA Survey of Resources and Methods for NLP of Serbian Language\nLjubeˇsi´c, N. and F. Klubiˇcka 2014, April.\nbs,hr,srWaC - web corpora of\nBosnian, Croatian and Serbian.\nIn Proceedings of the 9th Web as Cor-\npus Workshop (WaC-9), Gothenburg, Sweden, pp. 29–35. Association for\nComputational Linguistics.\nLjubeˇsi´c, N. and F. Klubiˇcka. 2016. The serbian web corpus srwac. Ljubljana:\nJoˇzef Stefan Institute .\nLjubeˇsi´c, N. and F. Klubiˇcka. 2016. Serbian web corpus srWaC 1.1. Slovenian\nlanguage resource repository CLARIN.SI.\nLjubeˇsi´c, N., F. Klubiˇcka, ˇZ. Agi´c, and I.P. Jazbec 2016. New inﬂectional\nlexicons and training corpora for improved morphosyntactic annotation of\ncroatian and serbian. In Proceedings of the Tenth International Conference\non Language Resources and Evaluation (LREC’16), pp. 4264–4270.\nLjubeˇsi´c,\nN.\nand\nD.\nLauc.\n2021.\nBerti´c–the\ntransformer\nlanguage\nmodel for bosnian, croatian, montenegrin and serbian.\narXiv preprint\narXiv:2104.09243 .\nLjubeˇsi´c, N., F. Markoski, E. Markoska, and T. Erjavec. 2021.\nCompara-\nble corpora of south-slavic wikipedias CLASSLA-wikipedia 1.0. Slovenian\nlanguage resource repository CLARIN.SI.\nLjubeˇsi´c, N. and P. Rupnik. 2022. The twitter user dataset for discriminat-\ning between bosnian, croatian, montenegrin and serbian twitter-HBS 1.0.\nSlovenian language resource repository CLARIN.SI.\nLjubeˇsi´c, N., M. Starovi´c, T. Kuzman, and T. Samardˇzi´c. 2022. Choice of plau-\nsible alternatives dataset in serbian COPA-SR. Slovenian language resource\nrepository CLARIN.SI.\nLjubeˇsi´c, N., M. Stupar, T. Juri´c, and ˇZ. Agi´c. 2013.\nCombining avail-\nable datasets for building named entity recognition models of croatian and\nslovene. Slovenˇscina 2(1): 35–57 .\nMagner,\nT.\n2001.\nDigraphia\nin\nthe\nterritories\nof\nthe\ncroats\nand\nserbs.\nInternational\nJournal\nof\nSociology\nand\nLanguages:\n11–26.\nhttps://doi.org/doi:10.1515/ijsl.2001.028 .\nMarovac, U., A. Avdi´c, and A. Ljaji´c. 2021. Creating a stop word dictionary in\nserbian. Scientiﬁc Publications of the State University of Novi Pazar Series\nA: Applied Mathematics, Informatics and mechanics 13(1): 17–25 .\nMarovac, U., A. Ljaji´c, E. Kajan, and A. Avdi´c. 2013. Similarity search in text\ndata for the serbian language. Proceedings of ICEST: 607–610 .\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n37\nMarovac, U., A. Pljaskovi´c, A. Crniˇsanin, and E. Kajan 2012. N-gram analysis\nof text documents in serbian language. In 2012 20th Telecommunications\nForum (TELFOR), pp. 1385–1388. IEEE.\nMatthews, W.K. 1952. The latinisation of cyrillic characters. The Slavonic\nand East European Review 30(75): 531–548 .\nMiletic, A. 2017. Building a morphosyntactic lexicon for serbian using wik-\ntionary. In 6e ´edition des Journ´ees d’´etude toulousaines: Les interfaces en\nsciences du langage, pp. 30–34.\nMiliˇcevi´c, M. and N. Ljubeˇsi´c. 2016. Tviterasi, tviteraˇsi or twitteraˇsi? pro-\nducing and analysing a normalised dataset of croatian and serbian tweets.\nSlovenˇsˇcina 2.0: empirical, applied and interdisciplinary research 4(2):\n156–188 .\nMiller, G.A. and C. Fellbaum. 2007.\nWordnet then and now.\nLanguage\nResources and Evaluation 41: 209–214 .\nMiloˇsevi´c, N. 2012a. Maˇsinska analiza sentimenta reˇcenica na srpskom jeziku.\nMaster’s Degree Thesis .\nMiloˇsevi´c, N. 2012b.\nStemmer for serbian language.\narXiv preprint\narXiv:1209.4471 .\nMilosevi´c, N. and G. Nenadi´c. 2016. As cool as a cucumber: Towards a corpus\nof contemporary similes in serbian. arXiv preprint arXiv:1605.06319 .\nMilosevi´c, N. and G. Nenadi´c. 2018.\nCreating a contemporary corpus of\nsimiles in serbian by using natural language processing.\narXiv preprint\narXiv:1811.10422 .\nMitrovic, J. 2014. Electronic tools and resources for multi-word unit detection\nand research in serbian. In The 2th General Meeting of The IC1207 COST\nAction, PARSEME, Athens, Greece, pp. 10–11.\nMitrovi´c, J., M. Mladenovi´c, and C. Krstev 2015. Adding mwes to serbian lexi-\ncal resources using crowdsourcing. In poster presented at Тhe 5th PARSEME\ngeneral meeting. Ias,i, Romania, pp. 23–24.\nMitrovi´c, J., C. O’Reilly, M. Mladenovi´c, and S. Handschuh. 2017. Ontologi-\ncal representations of rhetorical ﬁgures for argument mining. Argument &\nComputation 8(3): 267–287 .\nMladenovi´c, M. 2016. Informatiˇcki modeli u analizi ose´canja zasnovani na\njeziˇckim resursima. Ph. D. thesis.\nSpringer Nature 2023 LATEX template\n38\nA Survey of Resources and Methods for NLP of Serbian Language\nMladenovi´c, M. and J. Mitrovi´c 2013. Ontology of rhetorical ﬁgures for serbian.\nIn International Conference on Text, Speech and Dialogue, pp. 386–393.\nSpringer.\nMladenovi´c, M., J. Mitrovi´c, and C. Krstev 2014. Developing and maintain-\ning a wordnet: Procedures and tools. In Proceedings of the Seventh Global\nWordnet Conference, pp. 55–62.\nMladenovi´c, M., J. Mitrovi´c, C. Krstev, and D. Vitas. 2016. Hybrid senti-\nment analysis framework for a morphologically rich language. Journal of\nIntelligent Information Systems 46(3): 599–620 .\nMladenovi´c, M., S.V. Stankovi´c, and V. Paji´c. 2020. Two ways for the auto-\nmatic generation of application ontologies by using balkanet. International\nJournal on Semantic Web and Information Systems (IJSWIS) 16(2): 18–41\n.\nMochtak, M., P. Rupnik, and N. Ljubeˇsiˇc. 2022. The parlasent-bcs dataset\nof sentiment-annotated parliamentary debates from bosnia-herzegovina,\ncroatia, and serbia. arXiv preprint arXiv:2206.00929 .\nMozetiˇc, I. and M. Grˇcar. Smailovi c j. 2016. Multilingual Twitter sentiment\nclassiﬁcation: the role of human annotators. PLOS ONE 11(5): e0155036 .\nMozetiˇc, I., M. Grˇcar, and J. Smailovi´c. 2016.\nTwitter sentiment for 15\neuropean languages. Slovenian language resource repository CLARIN.SI.\nNenadi´c, G. 2004. Creating digital language resources. Pregled nacionalnog\ncentra za digitalizaciju (5): 191–30 .\nNikoli´c, N., O. Grljevi´c, and A. Kovaˇcevi´c. 2020.\nAspect-based sentiment\nanalysis of reviews in the domain of higher education.\nThe Electronic\nLibrary 38(1): 44–64 .\nNikoli´c, V. 2016. Modelovanje i pretraˇzivanje nad nestruktuiranim podacima i\ndokumentima u e-Upravi Republike Srbije. Ph. D. thesis.\nOdebrecht, C., L. Burnard, and C. Sch¨och. 2021. European literary text col-\nlection (eltec): April 2021 release with 14 collections of at least 50 novels.\nzenodo.\nOstrogonac, S. 2018. Modeli srpskog jezika i njihova primena u govornim i\njeziˇckim tehnologijama. Ph. D. thesis, University of Novi Sad (Serbia).\nOstrogonac, S., B. Rastovi´c, and E. Liliom. 2020. A python package for text\nprocessing for serbian–nlpheart. Scientiﬁc Technical Review 70(3): 41–45 .\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n39\nOuyang, L., J. Wu, X. Jiang, D. Almeida, C.L. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray, et al. 2022.\nTraining lan-\nguage models to follow instructions with human feedback. arXiv preprint\narXiv:2203.02155 .\nPaumier, S., F. Malchok, C. Marschner, C. Martineau, C. Mart´ınez, D. Maurel,\nS. Nagel, A. Neme, M. Petit, J. Stiehler, et al. 2002. Unitex 3.2 .\nPavlovic-Lazetic, G. and J. Graovac 2010.\nOntology-driven conceptual\ndocument classiﬁcation. In KDIR, pp. 383–386.\nPavlovi´c-Laˇzeti´c, G., D. Vitas, and C. Krstev 2004.\nTowards full lexical\nrecognition. In Text, Speech and Dialogue: 7th International Conference,\nTSD 2004, Brno, Czech Republic, September 8-11, 2004. Proceedings 7, pp.\n179–186. Springer.\nPetrovi´c, D. 2020.\nAnaliza strukture kolekcije pravnih dokumenata na\nosnovu njihove povezanosti preko odredjenih jeziˇckih izraza. Ph. D. thesis,\nУниверзитет у Нишу, Електронски факултет.\nPetrovi´c, D. and S. Janiˇcijevi´c 2019. Domain speciﬁc word embedding matrix\nfor training neural networks. In 2019 International Conference on Artiﬁcial\nIntelligence: Applications and Innovations (IC-AIAI), pp. 71–714. IEEE.\nPetrovi´c, D. and M. Stankovi´c. 2019.\nThe inﬂuence of text preprocessing\nmethods and tools on calculating text similarity. Ser. Math. Inform 34(5):\n973–994 .\nPlisson, J., N. Lavraˇc, D. Mladeni´c, and T. Erjavec. 2008. Ripple down rule\nlearning for automated word lemmatisation.\nAi Communications 21(1):\n15–26 .\nPopovi´c, M. and M. Arˇcan. 2016. Post-edited and error annotated machine\ntranslation corpus {PErr} 1.0.\nSlovenian language resource repository\nCLARIN.SI.\nPopovi´c, Z. 2008. Evaluacija programa za obeleˇzavanje (etiketiranje) teksta na\nsrpskom jeziku. Ph. D. thesis.\nPopovi´c, Z. 2010. Taggers applied on texts in serbian. INFOtheca-Journal of\nInformatics & Librarianship 11(2) .\nPuﬂovi´c, D., G. Velinov, T. Stankovi´c, D. Jankovi´c, and L. Stoimenov. 2016. A\nsupervised named entity recognition for information extraction from medical\nrecords.\nSpringer Nature 2023 LATEX template\n40\nA Survey of Resources and Methods for NLP of Serbian Language\nReshamwala, A., D. Mishra, and P. Pawar. 2013. Review on natural language\nprocessing. IRACST Engineering Science and Technology: An International\nJournal (ESTIJ) 3(1): 113–116 .\nSamardˇzi´c, T., N. Ljubeˇsi´c, and M. Miliˇcevi´c. 2015. Regional linguistic data\ninitiative (reldi) .\nSamardˇzi´c, T., M. Starovi´c, ˇZ. Agi´c, and N. Ljubeˇsi´c. 2017. Universal depen-\ndencies for serbian in comparison with croatian and other slavic languages\n.\nˇSandrih, B. 2019. Sms sentiment classiﬁcation based on lexical features, emoti-\ncons and informal abbreviations. Serdica Journal of Computing 13(1-2):\n081p–096p .\nˇSandrih, B., C. Krstev, and R. Stankovi´c 2019. Development and evaluation\nof three named entity recognition systems for serbian-the case of personal\nnames. In Proceedings of the International Conference on Recent Advances\nin Natural Language Processing (RANLP 2019), pp. 1060–1068.\nSanh, V., L. Debut, J. Chaumond, and T. Wolf. 2019.\nDistilbert, a dis-\ntilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint\narXiv:1910.01108 .\nSeˇcujski, M. and V. Deli´c. 2008. A software tool for automatic part of speech\ntagging in serbian language. Applied Linguistics 1(9): 97–103 .\nSeˇcujski, M.S. and A.D. Kupusinac. Automatska morfoloˇska anotacija tekstova\nna srpskom jeziku koriˇs´cenjem hmm .\nSekine, S. 2004. Named entity: History and future. Project notes, New York\nUniversity: 4 .\nStankovi´c, R., M. Koˇsprdi´c, M.I. Neˇsi´c, and T. Radovi´c 2022. Sentiment anal-\nysis of serbian old novels. In Proceedings of the 2nd Workshop on Sentiment\nAnalysis and Linguistic Linked Data, pp. 31–38.\nStankovi´c, R., C. Krstev, I. Obradovi´c, A. Trtovac, and M. Utvi´c 2012. A\ntool for enhanced search of multilingual digital libraries of e-journals. In\nProceedings of the 8th International Conference on Language Resources and\nEvaluation, LREC 2012, May 2012, Istanbul, Turkey, pp. 1710–1717.\nStankovi´c, R., J. Mitrovi´c, D. Joki´c, and C. Krstev 2020. Multi-word expres-\nsions for abusive speech detection in serbian. In Proceedings of the Joint\nWorkshop on Multiword Expressions and Electronic Lexicons, pp. 74–84.\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n41\nStankovi´c, R., I. Obradovi´c, C. Krstev, and D. Vitas 2011.\nProduction of\nmorphological dictionaries of multi-word units using a multipurpose tool.\nIn Proceedings of the Computational Linguistics-Applications Conference,\nOctober 2011, Jachranka, Poland.\nStankovi´c, R., B. Trivi´c, O. Kitanovi´c, B. Blagojevi´c, and V. Nikoli´c. 2011.\nThe development of the geolissterm terminological dictionary. INFOtheca-\nJournal of Informatics & Librarianship 12(1) .\nStankovi´c, R., C. Krstev, B.ˇS. Todorovi´c, and M. ˇSkoric. 2021. Annotation of\nthe serbian eltec collection. Infotheca-Journal for Digital Humanities 21(2):\n43–59 .\nStankovi´c, R., B. ˇSandrih, C. Krstev, M. Utvi´c, and M. Skoric 2020, May.\nMachine learning and deep neural network-based lemmatization and mor-\nphosyntactic tagging for Serbian. In Proceedings of the Twelfth Language\nResources and Evaluation Conference, Marseille, France, pp. 3954–3962.\nEuropean Language Resources Association.\nTanasijevi´c, I. 2019. Toward automatic tagging of cultural heritage documents.\nIPSI Transactions on Advanced Research, TAR 15(1) .\nThoppilan, R., D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.T.\nCheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. 2022. Lamda: Language\nmodels for dialog applications. arXiv preprint arXiv:2201.08239 .\nTiedemann, J. 2012.\nParallel data, tools and interfaces in opus.\nIn Lrec,\nVolume 2012, pp. 2214–2218.\nTodorovi´c, B.ˇS., R. Stankovi´c, C. Krstev, and M.I. Neˇsi´c 2021.\nSerbian\nner&beyond: The archaic and the modern intertwinned. In Deep Learning\nNatural Language Processing Methods and Applications–Proceedings of the\nInternational Conference Recent Advances in Natural Language Processing\n(RANLP 2021), pp. 1252–1260.\nTomaˇsevi´c, A.D. 2018. Razvoj modela za upravljanje rudarskom projektnom\ndokumentacijom. Ph. D. thesis.\nUlˇcar, M. and M. Robnik-ˇSikonja 2020. Finest bert and crosloengual bert:\nless is more in multilingual models. In Text, Speech, and Dialogue: 23rd\nInternational Conference, TSD 2020, Brno, Czech Republic, September 8–\n11, 2020, Proceedings 23, pp. 104–111. Springer.\nUtvi´c, M. 2011. Annotating the corpus of contemporary serbian. In Proceedings\nof the INFOtheca ‘12 Conference, pp. 36–47.\nSpringer Nature 2023 LATEX template\n42\nA Survey of Resources and Methods for NLP of Serbian Language\nUtvi´c, M. 2014. Izgradnja referentnog korpusa savremenog srpskog jezika. Ph.\nD. thesis.\nUtvi´c, M., R. Stankovi´c, A. Tomaˇsevi´c, M. ˇSkori´c, and B. Lazi´c. 2019. Pre-\ntraga korpusa zasnovana na upotrebi eksternih leksiˇckih resursa putem\nveb-servisa. Nauˇcni sastanak slavista u Vukove dane-Vol. 48/3 Srpski jezik\ni njegovi resursi .\nVasiljevi´c, N. 2015. Automatska obrada pravnih tekstova na srpskom jeziku.\nPh. D. thesis.\nVitas, D., S. Koeva, C. Krstev, and I. Obradovi´c 2008. Tour du monde through\nthe dictionaries. In Actes du 27eme Colloque International sur le Lexique et\nla Gammaire, pp. 249–256.\nVitas, D. and C. Krstev. 2006.\nLiterature and aligned texts.\nReadings in\nMultilinguality: 148–155 .\nVitas, D. and C. Krstev. 2009.\nSerbian language and sstbi.\nSNTPI ’09 -\nNauˇcno-struˇcni skup Sistem nauˇcnih, tehnoloˇskih i poslovnih informacija .\nVitas, D., C. Krstev, and ´E. Laporte. 2006. Preparation and exploitation of\nbilingual texts. Lux Coreana (1): 110–132 .\nVitas, D., C. Krstev, I. Obradovi´c, L. Popovi´c, and G. Pavlovi´c-Laˇzeti´c 2003a.\nAn overview of resources and basic tools for processing of serbian written\ntexts. In Proc. of the Workshop on Balkan Language Resources, 1st Balkan\nConference in Informatics. Citeseer.\nVitas, D., C. Krstev, I. Obradovi´c, L. Popovi´c, and G. Pavlovi´c-Laˇzeti´c 2003b.\nProcessing serbian written texts: An overview of resources and basic tools. In\nWorkshop on Balkan Language Resources and Tools, Volume 21, pp. 97–104.\nVitas, D., P. Ljubomir, K. Cvetana, O. Ivan, P.L. Gordana, and S. Mladen.\n2012. The serbian language in the digital age. META-NET White Paper\nSeries, G. Rehm, H. Uszkoreit (eds.) .\nVitas, D., G. Nenadi´c, and C. Krstev 1998.\nElectronic edition of serbian\ntranslation of plato’s republic aligned with 17 languages.\nIn East meets\nWest – A compendium of Multilingual Resources, TELRI Association e.V.,\nInstitut fur deutsche Sprache, Mannheim.\nVitas, D. and G. Pavlovi´c-Laˇzeti´c. 2008. Resources and methods for named\nentity recognition in serbian. INFOtheca-Journal of Informatics & Librari-\nanship 9 .\nVujani´c, M. ed. 2007. Reˇcnik srpskoga jezika. Matica srpska.\nSpringer Nature 2023 LATEX template\nA Survey of Resources and Methods for NLP of Serbian Language\n43\nVujiˇci´c-Stankovi´c, S.\n2016.\nEkstrakcija\ninformacija\nvodjena\nontologi-\njama:(model za srpski jezik). Ph. D. thesis.\nVujiˇci´c-Stankovi´c, S., C. Krstev, and D. Vitas 2014. Enriching serbianword-\nnet and electronic dictionaries with terms from the culinary domain.\nIn\nProceedings of the Seventh Global Wordnet Conference, pp. 127–132.\nVukovi´c, T. 2020. Spoken torlak dialect corpus 1.0 (transcription). Slovenian\nlanguage resource repository CLARIN.SI.\nWankhade, M., A.C.S. Rao, and C. Kulkarni. 2022.\nA survey on senti-\nment analysis methods, applications, and challenges. Artiﬁcial Intelligence\nReview: 1–50 .\n",
  "categories": [
    "cs.CL",
    "cs.DL",
    "cs.HC",
    "A.1"
  ],
  "published": "2023-04-11",
  "updated": "2023-04-11"
}