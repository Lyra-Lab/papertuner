{
  "id": "http://arxiv.org/abs/2304.07159v1",
  "title": "Unsupervised Learning Optical Flow in Multi-frame Dynamic Environment Using Temporal Dynamic Modeling",
  "authors": [
    "Zitang Sun",
    "Shin'ya Nishida",
    "Zhengbo Luo"
  ],
  "abstract": "For visual estimation of optical flow, a crucial function for many vision\ntasks, unsupervised learning, using the supervision of view synthesis has\nemerged as a promising alternative to supervised methods, since ground-truth\nflow is not readily available in many cases. However, unsupervised learning is\nlikely to be unstable when pixel tracking is lost due to occlusion and motion\nblur, or the pixel matching is impaired due to variation in image content and\nspatial structure over time. In natural environments, dynamic occlusion or\nobject variation is a relatively slow temporal process spanning several frames.\nWe, therefore, explore the optical flow estimation from multiple-frame\nsequences of dynamic scenes, whereas most of the existing unsupervised\napproaches are based on temporal static models. We handle the unsupervised\noptical flow estimation with a temporal dynamic model by introducing a\nspatial-temporal dual recurrent block based on the predictive coding structure,\nwhich feeds the previous high-level motion prior to the current optical flow\nestimator. Assuming temporal smoothness of optical flow, we use motion priors\nof the adjacent frames to provide more reliable supervision of the occluded\nregions. To grasp the essence of challenging scenes, we simulate various\nscenarios across long sequences, including dynamic occlusion, content\nvariation, and spatial variation, and adopt self-supervised distillation to\nmake the model understand the object's motion patterns in a prolonged dynamic\nenvironment. Experiments on KITTI 2012, KITTI 2015, Sintel Clean, and Sintel\nFinal datasets demonstrate the effectiveness of our methods on unsupervised\noptical flow estimation. The proposal achieves state-of-the-art performance\nwith advantages in memory overhead.",
  "text": "PREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n1\nUnsupervised Learning Optical Flow in Multi-frame Dynamic\nEnvironment Using Temporal Dynamic Modeling\nZitang Sun, Shin’ya Nishida, and Zhengbo Luo\nAbstract—For visual estimation of optical ﬂow, a crucial\nfunction for many vision tasks, unsupervised learning, using\nthe supervision of view synthesis, has emerged as a promising\nalternative to supervised methods, since ground-truth ﬂow is\nnot readily available in many cases. However, unsupervised\nlearning is likely to be unstable when pixel tracking is lost\ndue to occlusion and motion blur, or the pixel matching is\nimpaired due to variation in image content and spatial structure\nover time. In natural environments, dynamic occlusion or object\nvariation is a relatively slow temporal process spanning several\nframes. We, therefore, explore the optical ﬂow estimation from\nmultiple-frame sequences of dynamic scenes, whereas most of the\nexisting unsupervised approaches are based on temporal static\nmodels. We handle the unsupervised optical ﬂow estimation with\na temporal dynamic model by introducing a spatial-temporal\ndual recurrent block based on the predictive coding structure,\nwhich feeds the previous high-level motion prior to the current\noptical ﬂow estimator. Assuming temporal smoothness of optical\nﬂow, we use motion priors of the adjacent frames to provide more\nreliable supervision of the occluded regions. To grasp the essence\nof challenging scenes, we simulate various scenarios across\nlong sequences, including dynamic occlusion, content variation,\nand spatial variation, and adopt self-supervised distillation to\nmake the model understand the object’s motion patterns in a\nprolonged dynamic environment. Experiments on KITTI 2012,\nKITTI 2015, Sintel Clean, and Sintel Final datasets demonstrate\nthe effectiveness of our methods on unsupervised optical ﬂow\nestimation. The proposal reaches state-of-the-art performance\nwith advantages in memory overhead.\nIndex Terms—Optical ﬂow estimation, Unsupervised learning,\nSelf-supervised learning, Temporal dynamic model.\nI. INTRODUCTION\nI\nN computer vision, motion perception is deﬁned as optical\nﬂow estimation, a visual cue deﬁned as the projection of the\napparent motion of objects in a scene onto the image plane of a\nvision system or a visual sensor. As a fundamental vision task,\noptical ﬂow estimation plays an essential role in various high-\nlevel vision tasks, such as video understanding [1], behavior\nrecognition [2], object tracking [3], etc.\nEarly optical ﬂow estimation focused on minimizing elab-\norate energy equations [4], [5], which often suffers various\nlimitations, including slow inference speed, inability to gen-\nerate dense optical ﬂows and handle complicated situations,\netc. The performance of optical ﬂow techniques has recently\nseen dramatic improvements with the introduction of deep\nlearning. FlowNet [6] pioneered convolutional neural networks\nto address dense optical ﬂow estimation, and a variety of\napproaches based on DNN have sprung up subsequently. Cur-\nrently, dense optical ﬂow estimation is divided into two main\ncategories: supervised learning and unsupervised learning. Be-\ncause ground-truth labels for dense optical ﬂow are extremely\ndifﬁcult to obtain for real image pairs, supervised optical ﬂow\nFig. 1. Graphical summary of our work (A) and timeline of average end-\npoint error (AEPE) and F1-all in unsupervised optical ﬂow estimation\n(B). A: The proposed temporal recurrent network understands optical ﬂow\nestimation in dynamic environments. The relationships between different\ncomponents are brieﬂy described in the ﬁgure, where the Temporal Dynamic\nModel is introduced in subsection IV-A, the Occlusion-aware Temporal\nSmoothness Regularization is described in subsection IV-B, and the Dynamic\nTraining Enhancer is proposed in section V for Dynamic Occlusion (V-A),\nContent Variation and Spatial Variation (V-B), and Self-supervised Distillation\n(V-C), respectively; B: Marker size indicates network size, and oversized\nmarkers have been adjusted. Please refer to Table I and Table II for details.\ntechniques are basically trained using synthetic data. Although\nmodels trained on synthetic data often generalize well to\nnatural images, there is an inherent mismatch between two data\nsources, which those approaches may struggle to overcome [7].\nRedundant label rendering and unreal data make unsupervised\nlearning optical ﬂow estimation increasingly desirable.\nUnsupervised learning frameworks are proposed to utilize\nthe resources of unlabeled videos [8], the overall strategy\nbehind which is adopting a photometric loss that measures the\ndifference between the target image and the (inversely) warped\nsource image based on the dense ﬂow ﬁeld predicted from the\nnetwork. Yet there are many challenges with image warping-\nbased unsupervised learning, including brightness variations,\ncolor variations, and motion blur in multi-frame dynamic envi-\nronments. More importantly, since images are two-dimensional\nprojections of 3D space, the spatial occlusions induced by dif-\nferent objects in motion also cause pixel information loss over\nframes, which are unable to be recovered by image warping.\narXiv:2304.07159v1  [cs.CV]  14 Apr 2023\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n2\nAll aforementioned factors largely mislead the networks and\nthus degrade the performance.\nSeveral works have devoted to solving the above problems,\nincluding designing more robust loss functions [9], [10], gen-\nerating occlusion masks to circumvent the occluded region’s\nsupervision [11], tracking the occluded pixels in adjacent\nframes [12], and artiﬁcially adding random noise to simulate\nthe occlusion scenes [13]. Besides, there is a growing tendency\nto improve performance by integrating multiple strategies\n[14]–[16].\nWe notice that most approaches are based on a pair of\nimages to estimate optical ﬂow. Although a few works utilized\nmultiple frames [12], [15], the structure is still temporally\nstatic feedforward CNNs per se. As humans perceive motion\nand understand the world from dynamic environments in\nan unsupervised manner, here we hold the view that neural\nnetworks for motion perception should also be trained in\nextended temporal dynamic environments. Structurally, we\npartially refer to temporal predictive coding in human visual\nsystems [17], [18], where higher-order neurons send feedback\nsignals to control lower-order neurons. In continuous motion\nof objects, the loss of pixels information due to occlusion and\nblurring, which may not be a problem in supervised learning\ndue to the presence of ground truth from God’s view, is\nuntraceable and tricky in unsupervised learning. The predictive\ncoding structure based on temporal dynamics has at least the\nfollowing potential advantages for alleviating the problem.\nFirst, Based on the motion temporal smoothness, temporal\ndynamic networks can propagate the previous motion prior\nthrough hidden states to provide a more reliable reference\nfor untraceable pixels. Second, the motion estimation of the\noccluded object is practically equivalent to the prediction of\nthe location of the occluded object in the latter frame, which\nhas a strong connection with predictive coding mechanism as\none of the motivations for introducing temporal dynamics.\nAccordingly, this work constructs an CNN-RNN based\nmodel that understands optical ﬂow estimation in dynamic\nenvironments. Recent supervised learning work RAFT [19]\ndemonstrates the strength of recurrent blocks in optical ﬂow\nestimation. Instead of recurrent reﬁnement of a single frame,\nwe introduce both spatial and temporal recurrences into the\nmodel, constructing a spatial-temporal dual recurrent block\nwith predictive coding arrangement. The ﬁnal resulting net-\nwork has a small number of parameters yet stably converges\nin the sequence of arbitrary length and demonstrates higher\nperformance in ablation study compared to other models.\nWe trained the model in the temporal causal sequence.\nTo make the model correctly grasp the object occlusion,\nbrightness variation, color variation, and various content blur-\nring caused by the movement in the dynamic environment,\nthree training enhancers based on self-supervised learning are\nproposed, including Dynamic Occlusion Enhancer (DOE),\nContent Variation Enhancer (CVE), and Spatial Variation\nEnhancer (SVE). In which DOE extracts sub-object blocks\nfrom the original data distribution and simulates the random\nnatural motion of objects in multi-frame images, allowing\nthe model to understand object occlusion patterns over pro-\nlonged temporal environments. Furthermore, we propose a\nmixed supervision strategy by combining the unsupervised loss\nand self-supervised loss in the DOE, which simultaneously\nprovides reliable supervision on the occluded region and the\nocclusion per se, facilitating a better generalization of the\nnetwork to occlusion scenes. CVE simulates the variation of\nimage content, such as brightness and chromaticity of the ob-\nject across time. Also, it simulates motion blur, defocuses blur,\netc., caused by high-speed moving, boosting the robustness\nof the model in dealing with such challenging scenes. The\nﬁnal SVE simulates the spatial variation of the object that\nis often caused by camera shake and environmental vibration\nin dynamic environments, including random rotation, scaling,\ntranslation, and other series of afﬁne transformations.\nTo solve the occlusion issue, the majority of the works\nutilize occlusion masks to circumvent the supervision of the\noccluded region, which, nonetheless, results in inadequate\nsupervision of the occluded region. Based on a multi-frame\ntraining environment, we propose a novel temporal smoothness\nregularization to alleviate this problem by assuming approxi-\nmately the constant velocity of identical objects within a short\ntemporal window (due to object inertia), which provides more\nreliable motion prior to the occluded regions.\nCombined with the proposed multiple strategies, we even-\ntually achieve the state-of-the-art performance on multiple\ndatasets with a lightweight network conﬁguration, as shown\nin Fig. 1.\nOur strategy is motivated by multiple recent works, includ-\ning [14], [15], [20], [21]. The contributions in this study are\nlisted as follows:\n• We introduce temporal dynamic modeling to the un-\nsupervised learning-based optical ﬂow estimation task\nand design an efﬁcient CNN-RNN based on a predictive\ncoding structure that can process the sequence of arbitrary\nlength recursively, achieving state-of-the-art results on\nseveral leading datasets, as shown in Fig. 1.\n• We build three self-supervised training enhancers based\non multi-frame dynamic environments, which substan-\ntially improve the generalization performance in various\nchallenging environments by reducing 20% errors in the\nSintel dataset. The method is also ﬂexible and compat-\nible with other multi-frame methods to improve model\nperformance.\n• We propose two strategies for solving the occlusion\nproblem, including a mixed supervision strategy and tem-\nporal smoothing regularization, alleviating the problem of\ninadequate supervision for occluded regions.\nThe remaining part of this work is organized as follows:\nIn Section II, recent trends of unsupervised learning-based\noptical ﬂow estimation approaches are brieﬂy reviewed. The\npreliminaries of unsupervised learning based approach and our\nnotion convention are introduced in Section III. The proposed\nnetwork structure is introduced in Section IV. Section V\npresents the proposed three types of self-supervised learning-\nbased training enhancers. Experiments and ablation studies are\ngiven in Section VI. Finally, we conclude this work in Section\nVII.\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n3\nII. RELATED WORK\nSupervised methods require annotated ﬂow ground truth\nto train the network. FlowNet [6] is the ﬁrst work to propose\nlearning optical ﬂow estimation by training a fully convolu-\ntional network on the synthetic dataset FlyingChairs. Then,\nFlowNet2 [22] proposed stacking multiple networks as an\niterative improvement. SpyNet [23] built a spatial pyramid\nnetwork to estimate optical ﬂow in a coarse-to-ﬁne manner\nto cover challenging scenes with large displacements. PWC-\nNet [24], and LiteFlowNet [25] proposed to warp features and\ncalculate the cost volume of each pyramid layer by building\nefﬁcient and lightweight networks. IRR-PWC [26] proposes\nto design pyramid networks by iterative residual reﬁnement\nschemes. Recently, RAFT [19] proposed to estimate ﬂow ﬁelds\nby 4D correlation volume and recurrent network, yielding an\nexcellent performance.\nUnsupervised approaches circumvent the need for labels\nby optimizing photometric consistency with some regulariza-\ntion. Yu et al. [8] ﬁrst introduced a method for learning optical\nﬂow with brightness constancy and motion smoothness, which\nis similar to the energy minimization in conventional methods.\nFurther researches improve the accuracy through occlusion\nreasoning [10], [11], multi-frame extension [21], [27], epipolar\nconstraint [28], 3D geometrical constraints with monocular\ndepth [29], [30] and stereo depth [31], [32]. Although these\nmethods have been complicated, there is still a large gap with\nstate-of-the-art supervised methods. Based on self-supervised\nlearning, recent works improve the performance by learning\nthe ﬂow of occluded pixels in a knowledge distillation manner\n[13], [15], [33]. There is a growing tendency to improve\nperformance by integrating multiple strategies [14], [16].\nThe treatment of occlusion has been a fundamental prob-\nlem of unsupervised learning. The most common approach\nis to use an occlusion mask to ignore the occluded regions’\nphotometric loss [11], which, nevertheless, results in the\noccluded regions never getting effective supervision. In self-\nsupervised learning, some methods add random noise to the\nlatter frame as a simulation of the occlusion scene [13], [15],\n[33]. However, occlusion is a long temporal process that varies\nin the interaction among different moving objects. Sudden\nmasking by random noise of the above approaches does not\nfollow the smooth motion of objects in natural scenes that\nobey the inertia criterion. Also, images contaminated by a large\npart of random noise can be regarded as an outlier outside the\ndistribution of the original dataset, which has potential factors\nto degrade the network’s performance on the original dataset.\nIn this work, we proposed a new strategy that extracts sub-\nobjects from original data distribution and simulates a smooth\ndynamic occlusion process in self-supervised training. With a\nwell-designed mixed loss function, the full supervision of ﬂow\nboth on occluded regions and occluder per se is realized.\nTemporal dynamic models based on recurrent neural net-\nworks are not dominant approaches in optical ﬂow estimation.\nA few methods have tried to model multi-frame optical ﬂow\nestimation using RNN and LSTM [20], [34], which employ\nconventional supervised training strategies and can only be\napplied to a limited number of synthetic sequences. There are\nalso several approaches in unsupervised learning that utilize\nmulti-frame to estimate optical ﬂow [12], [13], [15], [27].\nHowever, essentially they are still temporally static networks\nand cannot handle real-time causal sequences with arbitrary\nlengths. In this work, we introduce the predictive coding struc-\nture and construct a spatial-temporal dual recurrent network\nthat unsupervisedly learns optical ﬂow in a prolonged temporal\nenvironment. Moreover, we combine existing self-supervised\nmethods and design a novel dynamic training environment\nbased on conﬁdence propagation, driving the network more\ncapable of handling complex dynamic scenarios.\nIII. PRELIMINARIES AND NOTATION\nIn this section, we ﬁrst brieﬂy introduce the general frame-\nwork of the unsupervised optical ﬂow used in our approach\nand deﬁne the basic notation used in subsequent sections. Let\ncasual RGB image sequence I = {I0, I1, ..., It−1, It}, our\ntarget is to train a model that estimate current optical ﬂow\nFt−1→t ∈RH×W ×2 from I, i.e. , Ft−1→t = f(I; Θ), where\nΘ is a set trainable parameters of the model.\nWe follow the general warping-based strategy as our basic\nunsupervised learning approach, by which the network can\nbe trained implicitly with view synthesis. Speciﬁcally, each\nimage It could be reconstruct by its next frame It+1 via\ninverse warping operation, i.e., ˆIt = It+1(p + Ft→t+1(p)),\nwhere p ∈RH×W is the spatial position across whole\nimages. Utilizing bilinear interploation, the method can be\nrealized in a differentiable way at sub-pixel level. Then one\nonly needs to deﬁne the similarity evaluation function ρ(·)\nbetween the reconstructed image ˆIt and the original imageIt,\ni.e. Lph ∼P\np ρ(ˆI(Θ), I), so as to implicitly supervise the\ngenerated optical ﬂow. The basic similarity evaluation loss\nis L1, or Charbonnier loss [35], which simply considers the\npixel-wise similarity between a pair of images. The more\nrobust loss that focuses on structural similarities, like SSIM\nloss and Census loss [10], are wildly used in the community to\novercome the variations in color and brightness across frames.\nDue to the aperture problem and the ambiguity of local\nappearance, supervision solely based on the photometric loss\ndoes not sufﬁciently constrain the problem for somewhere\ntextureless or with repetitive patterns. One of the most com-\nmon ways to reduce ambiguity is named edge-aware ﬁrst and\nsecond-order smooth regularization [36]. Given frame It−1\nwith ﬂow ﬁeld Ft−1→t, the k-order edge-aware loss Lsm(k)\ncould be deﬁned as:\nLsm(k) ∼\nX\np\n \nwx\np ·\n\f\f\f\f\n∂kFt−1→t\n∂xk\n\f\f\f\f\np\n+ wy\np\n\f\f\f\f\n∂kFt−1→t\n∂yk\n\f\f\f\f\np\n!\n,\nwx\np = exp\n \n−λ\nX\nc\n\f\f\f\f\n∂It−1\n∂x\n\f\f\f\f\np\n!\n,\nwy\np = exp\n \n−λ\nX\nc\n\f\f\f\f\n∂It−1\n∂y\n\f\f\f\f\np\n!\n.\n(1)\nWhere wx\np, wy\np is the attenuation weight determined by the\nsmoothness of the ﬁrst-order image gradient, which intuitively\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n4\nFig. 2. The inference architecture of the proposed spatial-temporal dual recurrent block. The spatial recurrence is modiﬁed from the basic ﬂow inference\npipeline of PWCNet [24], including a cascade of image warping, correlation volume, ﬂow estimator, and context network, which decodes the ﬂow ﬁeld from\nsmall to large scales. By designing a self-guided warping-based GRU (SGW-GRU) block, we introduce the temporal recurrence based on a predictive coding\nstructure that transmits feedback from deep to shallow layers. The right subﬁgure demonstrates that the network can infer temporal causal sequences with\narbitrary length. Please view subsection IV-A for details.\nforces the ﬂow ﬁeld in areas with similar pixels to be smoother.\nFollowing previous work [14], [15], we use ﬁrst-order smooth\nloss on Sintel dataset, second-order smooth loss on KITTI\ndataset.\nMotion occlusion is a challenging problem in unsupervised\nlearning, which results in pixel-wised information loss from\nIt to It−1. The most common method is to circumvent the\nloss calculation for the occluded region by inferring the binary\nocclusion mask Ot−1→t ∈{0, 1}H×W from It−1 to It, where\nOt−1→t(p) = 0 means the locations p from It−1 are occluded\nin It and vice versa. Following the previous method [14], [15],\na forward-backward check [37] is used in our basic approach\nto estimate the occlusion mask.\nWe simplify the notion in the following section. Given two\nadjacent time step {a, b | b > a}, one denotes forward ﬂow\nFa→b as Fa, and backward ﬂow Fb→a as F −1\nb\n; p ∈RH×W\nis denoted as spatial position set of a speciﬁc image; Wa(·) :\nIb 7−→ˆIa is denoted as warping operation from Ib via Fa→b;\nEp(·) represents the statistical expectations along the p.\nIV. BASIC APPROACH\nIn this section, we introduce the proposed spatial-temporal\ndual recurrent network and our basic unsupervised train-\ning approach based on temporal casual sequence. Also, a\nnovel occlusion-aware temporal smoothness regularization is\nintroduced in subsection IV-B. The effectiveness of several\nproposal would be demonstrated in ablation study part.\nA. Network structure\nWe need to construct a dynamic RNN model that can handle\narbitrary temporal series I = {I0, I1, ..., It−1, It}, which takes\nthe previous frame It−1, the current frame It, as well as the\nprevious hidden state Ht−2 as input, and outputs the current\noptical ﬂow Ft−1, and the hidden state Ht−1 served for the\nnext time step.\nIn order to introduce temporal recurrence, we design the\nconvolutional GRU block with a self-guided warping opera-\ntion, namely the SGW-GRU block. Given a pair of images, a\nﬁve-level feature map pyramid P = {I5, ..., I1} are generated\nby a concise feedforward CNN for each image respectively,\nwith size gradually reducing from H\n4 × W\n4 to\nH\n64 × W\n64 . The\nSGW-GRU block directly operates on feature pyramids from\nlow-scale to large-scale in a spatially recurrent manner. Also,\nit operates across image sequences in a recurrent temporal\nmanner. For feature pyramid at level l, the spatial recurrent\nprocess basically follows the four stages of PWCNet [24]:\nwarping →correlation volume →ﬂow estimator →context\nnetwork, as shown in Fig. 2. The main difference is that an\nextra convolutional block is plugged into the context network\nfor generating the new hidden state Ht−1. The hidden state is\ncharacterized by 32-channel feature maps at different scales,\nwhich implicitly contain higher-order motion prior and con-\ntextual information from previous moments, served as input\nfor the next time stage by feeding into the SGW-GRU block.\nDue to the object’s motion, there is constantly a difference\nin spatial location between the features of two adjacent time\nsteps. Accordingly, we design a self-guided warping (SGW)\nblock that enables the model to adjust the spatial location of\nhidden feature maps by itself. In SGW blocks, the correlation\nvolume between the previously hidden feature and the current\nfeature is ﬁrst calculated, based on which a mini ﬂow estimator\nis subsequently employed to estimate a ﬂow ﬁeld, guiding\nthe warping operation to adjust the spatial arrangement of\nHl+1\nt−2. Finally, a convolutional GRU module fuses the hidden\nstate Hl+1\nt−2 with the current features Il\nt−1 and outputs the\nfused feature, as shown in the lower-left corner of Fig. 2.\nConv-GRU cell basically follows the principle of the original\nGate Recurrent Unit [38] but replaces the operation with fully\nconvolutional approaches. Speciﬁcally, it takes previous high-\nlevel hidden state Hl+1\nt−2 and current feature It−1 as input and\ngenerates fused feature Inew served as input for ﬂow estimator,\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n5\nFig. 3.\nIllustration of Occlusion-aware Temporal Smoothness Regularization. Scenario A: pixels in the current frame Ic become invisible in the next\nframe If; Scenario B: pixels visible in the current frame Ic are invisible in the previous frame Ip. Both Scenario A and B cause errors in spatial alignment,\nwhich should be considered in temporal smoothness regularization. The right side shows the actual Sintel clean scene. We introduce a temporal smoothing\nconstraint on the current ﬂow ﬁeld Fc by tracking the motion prior from previous Fp and future frames Ff under the assumption of constant velocity within\nshort time windows. Due to the smoothness of the motion, the object’s forward and backward occlusions are usually complementary, which means that it is\npossible to track the motion prior as reference alternatively in Fp or Ff, as shown in regions marked by the red dashed box. For more details, please access\nsubsection IV-B\nthe process of which could be formalized as:\nz = σ\n\u0000Conv3×3\n\u0000ConCat\n\u0002\nHl+1\nt−2, It−1\n\u0003\u0001\u0001\nr = σ\n\u0000Conv3×3\n\u0000ConCat\n\u0002\nHl+1\nt−2, It−1\n\u0003\u0001\u0001\nq = Tanh\n\u0000Conv3×3(ConCat[r ⊙It−1, Hl+1\nt−2])\n\u0001\nInew = (1 −z) ⊙It−1 + z ⊙q.\n(2)\nWhere σ(·) denotes sigmoid function; ConCat(·) denotes\nconcatenation of feature maps; and ⊙means element-wise\nproduction. The other difference is that Conv-GRU cells do\nnot generate new hidden states, which are instead realized by\ncontext networks subsequently.\nIn the structure of temporal recurrence, the feedback signal\nfrom the end of the SGW-GRU block is transferred to the\nhead in shallow layers at the next time state to control its\ncoding behavior, so the neurons at the current time state would\npredictively consider future motion patterns, as shown in the\nright side of Fig. 2, the mechanism behind which is similar to\nthe temporal predictive coding structure in the human visual\nsystem [17], [18], where higher-order neurons send feedback\nsignals to lower-order neurons and control their behavior. This\nmechanism is considered to implicitly propel neurons to learn\nmotion perception in a dynamic environment [39].\nThe integration of the above proposals leads to a lightweight\nnetwork with only 2.50 M parameters, which can handle causal\nsequences of arbitrary length and exhibits fair effectiveness in\nablation study.\nB.\nOcclusion-aware Temporal Smoothness regularization\nAs mentioned in Section I, simply masking the occluded\nregion causes the inadequate supervision for the occluded\nregion, as where only supervised by spatial smoothing loss that\nsimply evaluates the low-level RGB similarity. We propose\na multi-frame-based temporal smoothness regularization to\nalleviate this problem as an alternative supervision method.\nFirst we deﬁne four adjacent frames {Ik−1, Ik, Ik+1, Ik+2}\nand three motion ﬂow ﬁeld {Fp, Fc, Ff} as forward optical\nﬂow of Ik−1\n→Ik, Ik\n→Ik+1 and, Ik+1\n→Ik+2,\nrespectively. Due to the limitation of physical laws, the motion\ntrajectory S(t) of an object is always a differential curve\nwith smoothness, and the video sequence can be regarded\nas a sampling of its locational state. With a sufﬁcient short\nsampling interval ∆t, one can pick a small temporal window,\nand make a Taylor ﬁrst-order approximation of the S(t), i.e.,\nS(t) = S(t0) + ∆t ·\ndS\ndt\n\f\f\nt=t0, or, S(t) = S(t0) + t · V (t0),\nwhere V could be regard as velocity vector or so-called optical\nﬂow within the deﬁned temporal window.\nAs a result, a warping operation based on the ﬂow ﬁeld of\nthe current frame Fc can be used to spatially align the ﬂow\nﬁeld of the previous frame Fp and the future frame Ff, which\ncan be represented by the following equation:\nˆF f\nc = Ff(p + Fc(p))\nˆF p\nc = Fp(p + F −1\np\n(p)).\n(3)\nWhere ˆF f\nc represents reconstructed Fc from Ff, and F −1\np\ndenotes backward ﬂow of Ik →Ik−1. Under the aforemen-\ntioned assumption of velocity constancy, the three adjacent\noptical ﬂow ﬁelds can be approximately aligned in space,\nand their derivation along time gives the basic temporal\nsmoothness loss function. However, an important issue is that\nwarping is not a reversible operation, and there is an occlusion\nproblem in either the forward or backward optical ﬂow, as\nshown on the left side of Fig. 3. Typically, there are two\nproblem scenarios, the ﬁrst in which pixels in the current frame\nbecome invisible in the next frame (scene A), and the second\nin which pixels visible in the current frame are invisible in\nthe previous frame (scene B). Both of these occlusion regions\ncause errors in spatial alignment. For simplicity, the ΩA, ΩB\nare denoted as two location sets that pixels of Fc are invisible\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n6\nin the Fp and Ff, respectively. Accordingly, we generate two\nocclusion masks OA, OB via forward-backward check that\nfollows:\nO(p) =\n(\n1,\np /∈Ω\n0\np ∈Ω\n(4)\nTemporal smoothness depends on the computation of the\ndifferential along time, which in the discrete case is equivalent\nto the computation of the ﬁrst-order difference across frames.\nHere we simplify it by directly using Charbonnier loss to\nevaluate the difference between frames, thus ensuring the\nsmoothness between Fc and the two adjacent frames. Besides,\nthe displacement size of the Fc is utilized as the decay\ncoefﬁcient for temporal smoothing, i.e., locations with more\nviolent motion have smaller smoothing constraints, and the\nﬁnal temporal smoothing regularization of current ﬂow Fc is\ndeﬁned as:\nℓtsm ∼\nP\np\n\u0014\nC\n\u0010\nS( ˆF p\nc ), Fc\n\u0011\n⊙OA\n\u001e\n|Fc|\n\u0015\nP\np OA(p)\n+\nP\np\n\u0014\nC\n\u0010\nS( ˆF f\nc ), Fc\n\u0011\n⊙OB\n\u001e\n|Fc|\n\u0015\nP\np OB(p)\n.\n(5)\nWhere S(·) means stop the gradient from computational graph,\nC(·) is Charbonnier loss. As for locations p ∈{ΩA∩ΩB}, we\ndo not add any temporal smoothness regularization since no\nreliable ﬂow priors exist in adjacent frames; As for positions\n{p|p ∈ΩA, p /∈ΩB}, we only add temporal smoothness\nfrom the previous ﬂow Fp; Similarly, temporal smoothness\nfrom the future ﬂow Ff only available for positions {p|p ∈\nΩB, p /∈ΩA}; Eventually temporal smoothness from both Fp\nand Ff would be added to positions {p|p /∈ΩA∪ΩB}, and the\noptimized result of the last situation is that the Ic approximate\nthe value of linear interpolation between Ip and If.\nOur design includes a priori that due to the temporal\nsmoothness of motion pattern, ΩA and ΩB tend to have\nonly a few intersections, i.e., objects occluded in the next\nframe are often visible in the previous frame. Therefore,\nmotion information occluded in forwarding propagation can be\nalternatively supplied via the past ﬂow. As in the lower right\nside of Fig. 3, the background area occluded by the arm in the\nforward inference is visible in the backward inference, so the\nmotion information of its previous frame can be tracked using\nthe backward ﬂow as a reference. This temporal smoothness\nconstraint provides more reliable supervision of the occluded\nregion compared to spatial smoothness.\nC.\nTraining in Temporal Sequence\nIn training, we feed N consecutive frames into the network\nsequentially per iteration and simultaneously supervise the\ngenerated N −1 optical ﬂows. The combined strategies from\nSection III are adopted to construct the loss function, i.e.,\nwarping-based image photometric similarity, spatial smoothing\nloss, and occlusion masking. It is worth noting that since\nour method is a temporal-recursive network, it can only infer\ncausal sequences and generate optical ﬂow in one direction.\nTherefore, in training, we ﬁrst forward infer the forward\noptical ﬂow for the whole sequence and then reverse the image\nsequence to infer backward optical ﬂow sequences. Finally, the\nforward-backward check is practiced to obtain the occlusion\nmask sequence {Ot}N−1\nt=1 .\nSpeciﬁcally, at the end of the sequence, we update the\nweights so as to decrease:\nL =\n1\nN −1\nN−1\nX\nt=1\nLt\n(6)\nwhere Lt is combination of a multi-scale loss for image pair\n(It, It+1) and the temporal smoothness loss, which could is\nrepresented as:\nLt =λ1ℓsm(Ft) + λ2ℓtsm(Ft−1, Ft, Ft+1)+\nℓwarp(It, It+1, Ft, Ot).\n(7)\nThe λ1 and λ2 are used to balance the loss weight as hyper-\nparameters. Please refer to Section VI-A for more detailed\nconﬁguration of loss function.\nV. DYNAMIC TRAINING ENHANCER\nAs proposed by DDFlow [40], self-supervised learning-\nbased distillation is an effective means to improve unsuper-\nvised optical ﬂow estimation, which has been adopted in a\nlarge amount of work [13]–[16]. Generally, it depends on\none teacher model ft(·) that processes the original pair of\nsamples I\n= {Ia, Ib} to generate optical ﬂow F ∗\na as a\npseudo label for training a student model fs(·). Before I is\nfed to the fs(·), a series of speciﬁc image transformations\nTI(·) : I 7−→eI can be used to increase the scene’s diversity\nas a kind of data augmentation to generate optical ﬂow Fa\nwith lower conﬁdence, which will be self-supervised by the\npseudo label from ft(·). To ensure the spatial consistency,\nthe pseudo label is required to be transformed consistently\nby TF (·) : F ∗7−→f\nF ∗. After that, the self-supervision based\non pseudo label can be realized by considering Charbonnier\nsimilarity between TF (F ∗\na ) and Fa, as:\nℓself ∼Ep\n\u0010\f\f\fS (Fa(p)) −f\nF ∗a (p)\n\f\f\f + ϵ\n\u0011q\n(8)\nIn this work, we extend this strategy to multi-frame se-\nquences and simulate multiple dynamic variation scenarios\nin nature, thus making the network understand the occlusion\nand variation in a dynamic environment. Speciﬁcally, we\ndesign three training enhancers, namely Dynamic occlusion\nenhancer (DOE), Spatial variation enhancer (SVE), and\nContent variation enhancer (CVE), which would be de-\nscribed as follows.\nA. Dynamic Occlusion Enhancer\nWe integrate the occlusion hallucination into our framework,\nnamely occlusion enhancer. Speciﬁcally, there are two steps:\n(i) Random crop. Actually, the random crop belongs to\nspatial transformation, but it efﬁciently creates new occlusion\nin the boundary region. We crop the sequence of images\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n7\nas a preprocess of occlusion transformation. (ii) Dynamic\nocclusion simulation. Given a set of frame sequences, we\ndivide the image into multiple sub-regions by using a super-\npixel segmentation algorithm [41]. Subsequently, n regions\nare randomly selected from the sub-regions as “occluders”.\nInstead of using random noise, we create natural textures for\neach occluder by extracting the texture from original sample\nbatches and adding random noise while using a large kernel\nGaussian ﬁlter to make the texture spatially correlated. Finally,\nwe simulate the occlusion by randomly placing n occluders\ninto any spatial position of the ﬁrst image.\n1) Simulation of Dynamic Motion Process : Unlike previ-\nous work, we simulate the motion of the occluders dynamically\nin multi-frame sequences. Obviously, objects’ motion always\nkeeps smoothness in natural environments, and the object’s\nmotion at the next time step depends heavily on the motion\nstate of the current moment. To better simulate random mo-\ntions while keeping the above rule, we partially follow the\nMarkov chain principle to simulate the process.\nSpeciﬁcally, we assume the moving state of an object as ran-\ndom process S(t), which can be decomposed two orthogonal\nvector S(t) = [U(t), V (t)]. For any time step t, we introduce\nMarkov properties by assuming its moving state only depends\non the motion state of the previous time step t −1, i.e.:\nPr [S(t) = st | S(t −1) = st−1, S(t −2) = st−2, . . .]\n= Pr[S(t) = st | S(t −1) = st−1]\n(9)\nThe motion states [U, V ] at time t are sampled from a two\ndimensional Gaussian distributions:\n[U(t), V (t)] ∼N(µ, Σ)\n(10)\nwhere we let\nµ =\n\u0012 µU(t)\nµV (t)\n\u0013\n,\nΣ =\n\u0012 σ2\nU\nσ2\nV\n\u0013\n,\n(11)\nby simplistically assume an independent relation between U(t)\nand V (t). σU, σV are set as a constant value that controls\nthe variation of the moving process, and the mean µU, µV\nare set as equal to the motion state at the previous moment,\ni.e., [µU(t), µV (t)]T = [U(t −1), V (t −1)]T , by which a\nrandom motion process while keeping smoothness is simulated\nfor each artiﬁcial occluder.\nThe initial moving state S(0) is also deﬁned as a random\nvariable, where moving speed |S(0)| is sampled from a 1-D\nGaussian distributions N(µ, σ), and moving angle is sampled\nfrom U(0, 2π). To ﬁt the distribution of the dataset, µ is set\nto be equal to the statistical expectations of the moving speed\nacross the whole training dataset and σ = µ\n3 .\nIn summary, given a sequence of images {It}, the process\nof dynamically simulating occlusion consists of the following\nsteps: 1). Randomly cropping the image sequence. 2). Ran-\ndomly selecting a keyframe Ik from {It}. 3). Segmenting Ik\ninto sub-regions by the super-pixel segmentation algorithm.\n4). Randomly extracting N sub-regions {Ψn}N\nn=1 and creating\nnew textures. 5). Randomly assigning {Ψn} at any location in\nthe ﬁrst frame I0 and initialing the moving state. 6). Starting\nfrom the I0, the dynamic masking process is simulated based\non Markov’s principle.\nFig. 4. The illustration of mixed Supervision strategy and dynamic occlusion\nscene. To simulate natural object motion scenes, we utilize the superpixels\nalgorithm to extract sub-objects of the image to act as artiﬁcial “occluders”\nwith natural texture, and Markov processes are subsequently used to simulate\nthe smoothness of the object’s random motion, the effect of which is shown\nin the bottom of the ﬁgure, where the occluders are highlighted by the yellow\nareas. Using the known prior of occlusion masks, we combine self-supervised\nlearning and unsupervised learning to design a mixed supervised strategy, as\nshown in the blue box above. The details are described in subsection V-A.\nIn addition to translations, we also simulate more complex\nmotions, such as twists, rotations, and a series of afﬁne trans-\nformations, which basically follow the same Markov process\nas above to maintain the smoothness and randomness of the\nmoving process. For simplicity, we denote the transformation\nof the above process as T O\nI (·) : {It} 7−→{f\nIO\nt } for image\nsequence, and T O\nF (·) : {F ∗\nt } 7−→{f\nF ∗\nt } for pseudo label,\ncorrespondingly. The visualization results of T O\nI (·) and T O\nF (·)\nare shown in the Fig. 4.\n2) Mixed Supervision Strategy : Two types of supervision\nstrategies are proposed for the occlusion enhancer, which are\nSparse Supervision and Mixed Supervision, respectively, both\nof which need to utilize pseudo label f\nF ∗\nt for self-supervision.\nAs shown in Fig. 4, at the moment t, each occluder {Ψt\nn}N\nn=1\ncould be considered as the closest object to the lens as the\nhighest level of occlusion. It is not difﬁcult to generate the\ncorresponding occlusion mask Ot that follows:\nOt(p) =\n(\n1,\np /∈ΩΨ\n0,\np ∈ΩΨ\n(12)\nwhere ΩΨ = {Ψt\n1 ∪Ψt\n2 ∪· · · ∪Ψt\nN}. Based on the occlusion\nmask, the sparse supervision avoids the loss calculation of the\noccluder itself and only the supervise the regions without the\noccluder via pseudo label:\nℓ1(t) ∼Ep\n\u0010\nC\n\u0010\nf\nF ∗\nt (p), Ft(p)\n\u0011\n⊙Ot(p)\n\u0011\n.\n(13)\nThe principle of sparse supervision here is only on re-\nconstructing the occluded region instead of the occluder per\nse. In contrast, the mixed supervision strategy combines the\nself-supervised loss ℓ1 and the unsupervised loss ℓ2, which\nconsiders all regions of the ﬂow ﬁeld simultaneously. The self-\nsupervised loss is used for the p /∈ΩΨ, while the unsupervised\nloss based on image warping is available for the regions\np ∈ΩΨ, which can be expressed by the following equation:\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n8\nFig. 5. Qualitative presentation of simulated temporal dynamic variation scenes, including content variation scenes, spatial variation scenes, and dynamic\nocclusion scenes. Across the sequence, please note the dynamic changes in color and illumination, the dynamic shifts in spatial position, and the dynamic\nmovement of occluders. Through self-supervised distillation, these dynamic scenes are well adapted by the model. See Section V for more details about\nDynamic Occlusion V-A, and Content & Spatial Variation V-B.\nℓ2(t) ∼SSIM\n\u0000Wt(It+1), It\n\u0001\n⊙(1 −Ot) + ℓsm,\n(14)\nwhere SSIM(·) only operate on regions with occluder by\nmultiplying with mask 1 −Ot. Since the artiﬁcial occluders\nhave rich texture information with time-invariant property, it\nis more suitable for using the pixel-wise similarity evaluation\nfunction, and the SSIM loss function is adopted in this case.\nHere ℓsm is similar to equation 1, but the image gradient-\nbased attenuation term is modiﬁed by replacing image It\nwith occlusion mask Ot, which will restrict the smoothing\nregularization within a sub-region of each occluder.\nFinally, the mixed supervision strategy of DOE is expressed\nas:\nℓdoe(t) = ℓ1(t) + ℓ2(t).\n(15)\nIt is worth noting that the occluder produces re-occlusion\nin the moving process, which also affects the warp-based\nunsupervised loss function. Therefore, we add a collision\ndetection mechanism to the mixed supervision, which can\navoid collisions between occluders during the random mo-\ntion process, i.e., the whole moving process should satisfy\n{Ψt\n1 ∩Ψt\n2 ∩· · · ∩Ψt\nN} = ∅.\nThe sparse loss and mixed loss strategies are separately\nevaluated in the ablation study, in which the latter shows a\nbetter performance. Please check Table VII for details.\nB. Spatial & Content Variation Enhancer\nA series of spatial transformations are implemented in the\nSVE, including random cropping of sequences, rotation, hori-\nzontal and vertical ﬂipping, and more complex transformations\nsuch as thin-plate-spline or CPAB transformations [42], which\ncan actually be regarded as spatial data augmentation. Further-\nmore, we extend the spatial transformation to continuous time\nseries and simulate various spatial variation scenarios across\ntime. In natural image sequences, there are scenes of spatial\njitter and distortion due to environmental vibrations, camera\nshake, etc. Several such challenging scenes are rendered as\nexamples in the Sintel Final dataset [43], which we generalize\nas the spatial variation. Accordingly, a series of random afﬁne\ntransformations along the sequence are used to simulate the\nvariation of the scene, including the dynamic rotation, dis-\ntortion, scaling, and translation of objects. Speciﬁcally, these\nspatial transformations (except image cropping) on an image\nare just the shift of the spatial position of pixels, which act\non the whole sequence T S\nI\n: {It} 7−→{f\nIS\nt }, also can be\nrepresented as:\n{eIS\nt (p)} =\n\b\nIt\n\u0000τθt(p)\n\u0001\t\n,\n(16)\nwhere τθt(·) is the transformation of pixel coordinates at image\nIt with transform parameters of θt. To keep the consistency\nbetween transformed scene and pseudo label F ∗\nt , the optical\nﬂow needs to undergo different transformations, as different\ntransformation parameters applied on It and It+1 will lead\nto a variation in optical ﬂow ﬁeld. In this case, one should\nﬁrst track the ﬂow variation between {τθt, τθt+1} via inverse-\nafﬁne transformation, which subsequently be superimposed by\nthe offset of original ﬂow ﬁeld F ∗\nt , and ﬁnally τθt(·) is used\nto keep spatially consistent with eIt. The whole process T S\nF :\n{F ∗\nt } 7−→{f\nF ∗\nt } could be represented as:\n(\nFnew(p) = τ −1\nθt+1 (p + F ∗\nt (p)) −τ −1\nθt (p)\nf\nF ∗\nt (p) = Fnew (τθt(p))\n(17)\nGiven pseudo label f\nF ∗\nt (p), the loss function based on self-\nsupervised learning of spatial variation enhancer can be ex-\npressed as:\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n9\nℓsve(t) ∼Ep\n\u0010\nC\n\u0010\nf\nF ∗\nt (p), F S\nt (p)\n\u0011\u0011\n(18)\nSimilar to the SVE, we introduce various transformations\nto the image content in the CVE. First, we introduce overall\ncolor jitter following the basic data augmentation principles\n[15], including random brightness, random saturation, random\nhue, gamma transformation, etc. In addition, Gaussian blur and\nnoise are randomly practiced on the whole image sequence to\nmake the scene more challenging.\nIn addition to static augmentation, there are considerable\ncontent variation scenes in natural environments, such as\nchanges in brightness and hue of objects due to the variation\nof illumination setting across time. Besides, various blurring\nphenomena that occur during motion, such as motion blur,\ndefocus blur, etc., can also yield pixel mismatch across frames,\nposing a massive challenge for unsupervised optical ﬂow\nestimation. We simulate such scenarios in sequences with\ntwo types of variation, one with monotonically increasing or\ndecreasing illumination, saturation, and hue along time, and\nthe other with random jitter across times, which propel the\nnetwork to focus on structural information of the object instead\nof the pixel differences. Similarly, we simulate a variety of\ndynamic blur scenarios, including box blur and Gaussian blur\nwith random kernel size, defocus blur with random kernel size\nand location, linear motion blur with random orientation and\nkernel size, as well as the more complicated Point Spread\nFunction (PSF) Blur [44]. Each of these blurs is assigned with\nrandom parameters in processing the sequence, accompanied\nby dynamic Gaussian noise that changes over time.\nSpeciﬁcally, the content transformation of a sequence of\nimages is denoted as T C\nI (·) : {It} 7−→{eIC\nt }. As a relatively\nsimple case, T C\nI (·) does not change the location of pixels nor\nintroduce new occlusion. Therefore, there is no extra transform\nfor the corresponding pseudo label, and the self-supervised\nloss of content variation training enhancer can be expressed\nas:\nℓcve(t) ∼Ep\n\u0010\nC\n\u0000F ∗\nt (p), F C\nt (p)\n\u0001\u0011\n(19)\nThe qualitative illustration of three transformations T O\nI (·),\nT S\nI (·), and T C\nI (·) is shown in Fig. 5.\nC. Self-supervised Distillation\nIn self-supervised learning, a teacher model is required\nto generate pseudo-labels to supervise the predictions of the\nstudent model. However, the process can be simpliﬁed to\nonline learning based on a single model that acts as both the\nteacher and the student model.\nWe adopt the strategy of ARFlow [15] by running an\nextra forward pass with each transformed scene, along with\nusing transformed pseudo label as the self-supervision signal.\nSpeciﬁcally, we ﬁrst expand the scenes using the previously\ndeﬁned transformations {T O\nI (·), T S\nI (·), T C\nI (·)} to obtain the\nthree scenes {f\nIO\nt }, {f\nIS\nt }, {f\nIC\nt }} based on the original scene\nsequence {It}. The training process will perform four times\nforward inferences, in which the ﬁrst one will generate the\noptical ﬂow pseudo-label {F ∗\nt } of the original scene, and\nthe following three produce the optical ﬂow of the dynamic\nocclusion scene, spatial variation scene, and content variation\nscene, respectively, which need to undergo corresponding ﬂow\ntransformation {T O\nF (·), T S\nF (·), T C\nF (·)} to keep the consistency\nwith the image transformation. By combining the loss func-\ntions in Eq. 7, Eq. 15, Eq. 18, and Eq. 19, the ﬁnal loss\nfunction for self-supervised distillation can be formalized as:\nLself ∼\n1\nN −1\nN−1\nX\nt=1\nLt\n|{z}\n1st Infer\n+Λ ·\n\u0002\nℓdoe(t), ℓsve(t), ℓcve(t)\n|\n{z\n}\n2nd, 3rd, 4th Infer\n\u0003\n.\n(20)\nWhere Λ = [λ3, λ4, λ5]T is the combined weight parameters\nfor self-supervised loss. By back-propagating Lself, one can\nachieve both the basic unsupervised optical ﬂow learning and\nself-supervised distillation in a single iteration.\nIn the process of self-supervision, we add a conﬁdence\npropagation mechanism with reference to DDFlow [40]. Based\non the forward-backward consistency check, we deﬁne a\nconﬁdence measure that describes the network’s conﬁdence for\nall spatial locations. For a moment t, the forward ﬂow Ft and\nbackward ﬂow F −1\nt+1 should satisfy: Ft = −F −1\nt+1(p + Ft(p)),\nor |Ft + F −1\nt+1(p + Ft(p))| = 0. This equation does not hold\nin regions where either there is an error in the prediction or\nwhere the occlusion happened, both of which are deﬁned as\nlow conﬁdence region in this case. Based on this deﬁnition,\nthe conﬁdence mask could be generated as:\nOconf = MAP\n \f\f\fFt + F −1\nt\n(p + Ft\n\u0000p)\n\u0001\f\f\f\n2\n\u0000|Ft|2 + |F −1\nt\n|2\u0001\n× δ\n!\n.\n(21)\nWhere the denominator is the weight decay based on the\noptical ﬂow displacement; δ = 0.01 is a scale factor, and\nMAP(·)is a nonlinear exponential mapping function that com-\npresses the conﬁdence level between (0, 1). Besides, we also\nassign zero conﬁdence to locations with excessive displace-\nment based on prior knowledge.\nThe overall pipeline of the proposed self-supervised distil-\nlation training process is illustrated in Fig 6.\nVI. EXPERIMENT\nWe carry out comprehensive experiments on several well-\nestablished datasets, including MPI-Sintel [43], KITTI 2012\n[45], KITTI 2015 [46], FlyingChair [6], ChairSDHom [22].\nIn the experiment, MPI-Sintel and KITTI are used to validate\nthe performance of the proposed model against a range of\nstate-of-the-art methods. The FlyingChair dataset is used for\nmodel pre-training, and the ChairSDHom is used for validating\ncross-datasets generalizability. For simplicity, our approach is\ndenoted as ULDENet (Unsupervised Learning in Dynamic\nEnvironments) in comparison.\nA. Implementation Details\nAccording to Eq. 7 and Eq. 20, we set the loss function\nweights {λ1, λ2, λ3, λ4, λ5} to {50, 0.005, 0.3, 0.3, 0.3} for\nSintel dataset and {75, 0.001, 0.2, 0.2, 0.2} for KITTI dataset,\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n10\nFig. 6. The complete training pipeline based on self-supervised learning. Three of the dynamic training enhancers expand the original sequence into content\nvariant scenes, spatial variant scenes, and dynamic occlusion scenes, respectively. Four forward and one backward inference are performed in each iteration,\nwhere the ﬁrst forward and backward inferences are used for the basic unsupervised training of the original scenes while computing the conﬁdence mask. The\nsubsequent triple forward inferences are carried out in extended scenes and back-propagated by a conﬁdence-based self-supervised loss function. The entire\nprocess is operated on long dynamic sequences, allowing the network to understand the patterns of scene variation and occlusion in a dynamic environment.\nThe execution order is marked from 1⃝to 9⃝, where 1⃝and 2⃝corresponds to subsection IV-C; 3⃝, 4⃝and 5⃝to V-A, and V-B respectively; the ﬁnal\nconﬁdence inference 7⃝and self-supervised distillation 8⃝correspond to subsection V-C.\nTABLE I\nCOMPARISON WITH PREVIOUS METHODS ON SINTEL BENCHMARK. WE USE THE\nAVERAGE EPE ERROR (THE LOWER THE BETTER) AS EVALUATION METRIC FOR THE\nSINTEL DATASETS. MISSING ENTRIES ‘-’ INDICATES THAT THE RESULT IS NOT\nREPORTED IN THE COMPARED PAPER, AND (·) INDICATES THAT THE TESTING IMAGES\nARE USED DURING UNSUPERVISED TRAINING. THE BEST UNSUPERVISED AND\nSUPERVISED RESULTS ARE BOLDED, RESPECTIVELY.\nMethod\nSintel Train\nSintel Test\nParam.\nClean\nFinal\nClean\nFinal\nsupervised\nFlowNetS [6]\n4.50\n5.45\n7.42\n8.43\n32.07M\nLiteFlowNet [47]\n(1.64)\n(2.23)\n4.86\n6.09\n5.37M\nPWCNet [24]\n(2.02)\n(2.08)\n4.39\n5.04\n8.75M\nIRR-PWC [26]\n(1.92)\n(2.51)\n3.84\n4.58\n6.36M\nSelFlow-ft [13]\n(1.68)\n(1.77)\n3.74\n4.26\n4.79M\nSTARFlow [20]\n(2.10)\n(3.49)\n2.72\n3.71\n4.77M\nRAFT [19]\n(0.77)\n(1.20)\n2.08\n3.41\n5.26M\nunsupervised\nUnFlow [10]\n-\n(7.91)\n9.38\n10.22\n116.58M\nOAFlow [11]\n(4.03)\n(5.95)\n7.95\n9.15\n5.12M\nMFOccFlow [21]\n(3.89)\n(5.52 )\n7.23\n8.81\n12.21M\nDDFlow [40]\n(2.92)\n(3.98)\n6.18\n7.40\n4.27M\nSelFlow [13]\n(2.88)\n(3.87)\n6.56\n6.57\n4.79 M\nARFlow [15]\n(2.79)\n(3.73)\n4.78\n5.89\n2.24 M\nARFlow-MV [15]\n(2.73)\n(3.69)\n4.49\n5.67\n2.37M\nSimFlow [9]\n(2.86)\n(3.57)\n5.92\n6.92\n9.70M\nUFlow [14]\n(2.50)\n(3.39)\n5.21\n6.50\n-\nUPFlow [16]\n(2.33)\n(2.67)\n5.24\n6.50\n3.49M\nOIFlow [48]\n(2.44)\n(3.35)\n4.26\n5.71\n5.26M\nOurs (ULDENet)\n(2.20)\n(2.65)\n3.50\n4.70\n2.50M\nrespectively. We implement our method using Pytorch on a\nworkstation with four paralleled RTX A6000 GPUs under\nCUDA 10.1. All models are trained by Adam optimizer [50]\nwith β1 = 0.9, β2 = 0.99. We ﬁrst pre-train the model on\nFlyingChair with batch size of 8 and a learning rate of 1e−4,\nfollowed by two stages of formal training on the Sintel or\nKITTI dataset. In the ﬁrst stage, we use the basic unsupervised\nloss function with a sequence length of 6, batch size of 4, and\nlearning rate of 1e−4 for 100 epochs of training. In the second\nstage, we perform 50 epochs of intensive training using self-\nTABLE II\nCOMPARISON WITH PREVIOUS METHODS ON KITTI BENCHMARK. WE USE THE\nAVERAGE EPE ERROR (THE LOWER THE BETTER) AS EVALUATION METRIC FOR THE\nKITTI 2012 DATASETS, F1 MEASUREMENT FOR KITTI 2015 TEST BENCHMARK.\nMISSING ENTRIES ‘-’ INDICATES THAT THE RESULT IS NOT REPORTED IN THE\nCOMPARED PAPER, AND (·) INDICATES THAT THE TESTING IMAGES ARE USED\nDURING UNSUPERVISED TRAINING. THE BEST UNSUPERVISED AND SUPERVISED\nRESULTS ARE BOLDED, RESPECTIVELY.\nMethod\nKITTI 2012\nKITTI 2015\nParam.\nTraining\nTest\nTraining\nTest\nsupervised\nFlowNet2-ft [22]\n1.28\n1.8\n2.30\n11.48%\n162.5M\nPWCNet [24]\n1.45\n1.7\n2.16\n9.60%\n8.75M\nIRR-PWC [26]\n-\n-\n1.63\n7.65%\n6.36M\nSelFlow-ft [13]\n0.76\n1.5\n1.18\n8.42%\n4.79M\nSTARFlow [20]\n-\n-\n-\n7.65%\n4.77M\nRAFT [19]\n-\n-\n0.64\n5.27%\n5.26M\nunsupervised\nUnFlow [10]\n3.29\n-\n8.10\n23.3%\n116.58M\nOAFlow [21]\n3.55\n4.2\n8.88\n31.2%\n5.12M\nDDFlow [40]\n2.35\n3.0\n5.72\n14.29%\n4.27M\nSelFlow [13]\n1.69\n2.2\n4.84\n14.19%\n4.79M\nEpiFlow [28]\n(2.51)\n3.4\n(5.55)\n16.95%\n8.75M\nNLFlow [49]\n3.02\n4.5\n6.05\n22.75%\n-\nARFlow [15]\n1.44\n1.8\n2.85\n11.8%\n2.24M\nARFlow-MV [15]\n1.26\n1.5\n3.46\n11.79%\n2.37M\nUFlow [14]\n1.68\n1.9\n2.71\n11.13%\n-\nUPFlow [16]\n1.27\n1.4\n2.45\n9.38%\n3.49M\nOIFlow [48]\n1.33\n1.6\n2.57\n9.81%\n5.26M\nOurs (ULDENet)\n(1.15)\n1.3\n(2.23)\n9.13%\n2.50M\nsupervised distillation, where the sequence length is set to 8\nper iteration, the batch size to 4, and the learning rate is set to\n5e−5 for Sintel and 8e−5 for KITTI, respectively. The gradient\nclipping technology is practiced in all training stage for faster\nconvergence.\nFor image preprocessing, the images from the Sintel dataset\nare cropped to 384 × 832 and inferred at the original size in\nthe validation; the images in the KITTI dataset are resized\nto 256 × 832 for both training and validation. We also adopt\nrandom image horizontal ﬂipping and sequence reversion for\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n11\nFig. 7. Visualization of results on Sintel and KITTI datasets. We qualitatively compare the results with ARFlow-MV [15] and UFlow [14], where the red\nboxed area marks our dominant region. Please zoom in to see the detailed information, and the quantitative comparisons are shown in Table I and Table II.\nAll results in this ﬁgure were generated by the ofﬁcial server on the test set without subjective picking.\ndataset augmentation.\nIn training, we mix Clean, Final, and Albedo scenes from\nthe Sintel training set and extracted them randomly for train-\ning. As for training on the KITTI dataset, we employ its\nmulti-view extension version for sequence training. It is worth\nto note we only touch the images of the training set in all\ntraining procedures, and the ﬂow ground-truth is only used for\nvalidation. The standard average endpoint error (EPE) and the\npercentage of erroneous pixels (F1) are used as the evaluation\nmetric of optical ﬂow. In the validation of the Sintel dataset,\nwe dynamically input all the frames of each scene into the\nmodel at once and validate the results of each frame; In the\nvalidation of the KITTI dataset, we input frames from 1th to\n11th into the model and only validate the results of frame 10th.\nB. Comparison on MPI-Sintel and KITTI Benchmarks\nWe compare the proposed method with both supervised\nand unsupervised approaches on the Sintel Clean and Final\ndataset. The test set result is ﬁrst generated and then veriﬁed by\nsubmitting it to the Sintel ofﬁcial server. In Table I, our method\nachieves the best accuracy among all compared unsupervised\nmethods on both training and test datasets with fairly low\nparameter overhead. In particular, we achieved a signiﬁcant\nlead in the Sintel test set, where the result of the Clean scene\noutperforms the previous best approach OIFlow [48] by an\n18% decrease on the EPE metric with only 48% memory\noverhead. Moreover, we improve the previous best method\nARﬂow-MV from EPE=5.67 to EPE=4.70 with a 17% error\nreduction on the Sintel Final test set.\nCompared to supervised methods, ULDENet is comparable\nto SelFlow-ft [13] on the Sintel test benchmark and to the 2020\nwork STARFlow [20] on the training set, with the advantages\nof lower parameters overhead.\nWe evaluate the proposed method both on KITTI 2012\nand KITTI 2015 benchmarks. The quantitative result could\nbe checked in Table II. Compared with other unsupervised\nlearning methods, ULDENet achieves the best results on both\nKITTI 2012 and KITTI 2015 benchmarks, demonstrating the\nproposed strategy’s efﬁciency. On the KITTI 2015 dataset,\nour method improves the previous best method UPFLow [16]\nwith EPE=2.45 to EPE=2.23, leading by 9% error reduction\nwith the advantage that the number of parameters is only 71%\nof UPFlow. Moreover, on KITTI 2015 online evaluation, our\nmethod reduces the F1-all value of 11.1% in UFlow [14] to\n9.1%, with an 18.0% error reduction. As for the KITTI 2012\nevaluation, the results of ULDENet on the training set decrease\nthe EPE=1.27 of UPFlow to EPE=1.15, with a reduction of 9\npercent.\nCompared with the supervised learning methods in the table,\nthe performance of ULDENet is between that of PWCNet [24]\nand SelFlow-ft [13], where the EPEs on both KITTI 2012 and\n2015 datasets are ahead of PWCNet, but the number of the\nparameters is only 29% of it.\nThe qualitative comparisons of the Sintel dataset and KITTI\ndataset are both visualized in Fig. 7\nC. Ablation Study\nAn extensive and rigorous ablation study is conducted to\ndemonstrate the effectiveness of each technical component.\nSintel training set was subdivided into separate training and\nvalidation sets following the setting in [15]. AEPE errors for\nall pixels (all), non-occluded pixels (NOC) and occluded pixels\n(OCC) are reported for quantitative comparison.\n1) Main Ablation: A series of components is proposed\nin this work, including the multi-frame recurrent inference\nstructure (MFI), temporal smoothness regularization (TSR),\nand dynamic training enhancer (DTE), which have been\ncomprehensively veriﬁed on Sintel dataset. As summarized\nin Table III, the ‘Base’ represents the baseline, i.e., the\noriginal lightweight PWCNet in a double-frames inference\nmanner; MF represents the model adopting a multi-frame\ntraining and inference way by adding the spatial-temporal\ndual recurrent block, which has a comprehensive improvement\nfor performance and increases the number of parameters\nby 0.4M. After introducing TSR, the model becomes more\nrobust for the occluded region. Attributing that TSR can\nprovide a more reliable reference for the occluded region\nvia the motion prior to the adjacent frames, the EPE of\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n12\nTABLE III\nMAIN ABLATION STUDY ON DIFFERENT COMBINATIONS OF THE PROPOSED\nCOMPONENTS. BASE: THE BASELINE OF ORIGINAL LIGHTWEIGHT PWCNET IN A\nDOUBLE-FRAMES INFERENCE MANNER. MF: MULTI-FRAME TRAINING AND\nINFERENCE STRUCTURE. TSR: TEMPORAL SMOOTHNESS REGULARIZATION. DTE:\nDYNAMIC TRAINING ENHANCER.\nBase MF TSR DTE\nSintel Clean\nSintel Final\nParam.\nALL NOC\nOCC\nALL NOC\nOCC\n✓\n2.23\n0.97\n19.71\n3.23\n1.80\n23.69\n2.10M\n✓\n✓\n2.17\n0.91\n19.36\n3.12\n1.74\n22.86\n2.50M\n✓\n✓\n✓\n2.10\n0.88\n18.74\n3.06\n1.70\n22.51\n2.50M\n✓\n✓\n✓\n1.75\n0.81\n15.21\n2.81\n1.63\n19.83\n2.50M\n✓\n✓\n✓\n✓\n1.67\n0.77\n14.54\n2.76\n1.61\n19.37\n2.50M\nOCC regions on the clean dataset reduces from 19.36 to\n18.74. Furthermore, DTE signiﬁcantly improves the network’s\nperformance without additional memory and computational\noverhead. Quantitatively, the network performance improves\nfrom EPE=2.17 to EPE=1.75 by DTE, with a 21% decrease\nin EPE for the OCC region. Integrating all the strategies, we\neventually improved the performance from EPE = 2.23 to EPE\n= 1.67 for the Clean dataset, and from EPE = 3.23 to EPE\n= 2.76 on the Final dataset, reducing the end-point error by\n20% on average.\n2) Network Structure: Table IV provides a concise com-\nparison of the models with different structures. To separately\ndemonstrate the efﬁciency of the proposed model structure,\nwe use the same loss strategy as ARFlow, and DTE and TSR\nare not adopted in the comparison. The only difference in\ntraining is that our model is trained and inferred in a multi-\nframe environment. The results indicate that our network is\nsubstantially ahead of PWCNet [24] in terms of performance\nand network parameters. Compared to the multi-frame ap-\nproach ARFlow-MV [15], which extracts ﬁve adjacent frames\nbefore and after the current time, we only access the frames\nas inputs before the current time, allowing the ULDENet\nstructure to ﬁt well in a real-time task. Also, our method can\nhandle sequences of arbitrary length, which can not be realized\nby most static multi-frame methods such as ARFlow-MV,\nSelFlow [13], MFOccFlow [12], etc. In terms of quantitative\nresults, we signiﬁcantly reduce the EPE of the network at the\ncost of a slight increase in memory overhead.\nIt is worth mentioning that even though our model is\ntrained in a dynamic environment with multiple frames, it\nstill demonstrates good generalization in a two-frame inference\napproach after reaching convergence. Speciﬁcally, we split\neach scene of the Sintel dataset into different frame patches\nto verify the impact of accuracy w.r.t inference frames of the\nnetwork, and the results are shown in Fig. 8.\nObviously, even if the inference is performed with only two\nframes, the EPE of our method for the Clean (EPE = 2.30)\nand Final (EPE = 2.86) dataset is still substantially lower than\nthat of the multi-frame method ARFlow-MV. Besides, the EPE\nof the model gradually decreases as the number of inference\nframes rises, where multi-frame inference brings more signif-\nicant gain on the Final dataset with more complicated scenes,\nsuggesting ULDENet is more suited for working in dynamic\nreal-time environments with complex circumstances.\n3) Temporal Smoothness Regularization: Based on the grid\nsearch method, we veriﬁed the effect of temporal smoothing\nregularization (TSR) on optical ﬂow estimation with different\nFig. 8.\nThe network EPE performance w.r.t the increasing of inference\nframes. After multi-frame dynamic training, the proposed model converges\nin sequences of arbitrary length. The sequence in the table indicate that\nthe network utilizes all frames from the scene per inference. The model\nperformance gradually improves as the inference frames rise.\nTABLE IV\nABLATION STUDY OF OUR FRAMEWORK WITH MULTIPLE MODEL\nARCHITECTURES. AEPE IN SPECIFIC REGIONS OF THE SCENE AND THE NUMBER OF\nCNN PARAMETERS ARE REPORTED.\nNet Structure\nSintel Clean\nSintel Final\nParam.\nALL\nNOC\nOCC\nALL\nNOC\nOCC\nPWCNet [24]\n2.48\n1.19\n21.71\n3.47\n1.98\n25.19\n8.75M\nPWCNet-small [24]\n2.76\n1.28\n23.92\n3.62\n2.16\n28.15\n4.05M\nARFlow [15]\n2.30\n1.08\n20.00\n3.19\n1.84\n22.77\n2.24M\nARFlow-MV [15]\n2.24\n1.04\n19.60\n3.18\n1.86\n22.36\n2.37M\nOurs-seq\n2.17\n0.91\n19.36\n3.12\n1.74\n22.83\n2.50M\nTABLE V\nABLATION STUDY ON WEIGHT OF TEMPORAL SMOOTHNESS LOSS Wtsm. AEPE\nIN SPECIFIC REGIONS OF THE SINTEL AND KITTI DATASETS ARE REPORTED.\nWtsm\nSintel Clean\nKITTI-15\nALL\nNOC\nOCC\nALL\nNOC\nOCC\n0\n2.17\n0.91\n19.36\n2.65\n1.92\n5.86\n0.01\n2.15\n0.92\n19.07\n2.52\n1.94\n5.07\n0.05\n2.10\n0.88\n18.74\n2.87\n2.07\n6.39\n0.1\n2.26\n1.04\n19.21\n2.95\n2.12\n6.60\n0.5\n3.24\n2.05\n21.86\n3.46\n2.35\n8.35\nweights, as summarized in Table V. We used the ULDENet\nstructure with the basic unsupervised loss strategy as a base-\nline, and DTE was not operated in the validation. Both KITTI\n2015 and Sintel datasets were included in the validation. From\nthe results, large-scale TSR leads to degradation of the network\nperformance, and accordingly, its weights should be restricted\nto a small range. Compared to NOC regions, the TSR brings\nmore signiﬁcant gains for estimating OCC regions, as NOC\nregions are robustly governed by the photometric loss function,\nwhile mixed supervision of temporal smoothing regularization\nand spatial smoothing regularization is instead required for\nOCC regions.\nAccording to Table V, The TSR weight is set to 0.01 for\nthe KITTI dataset and 0.05 for the Sintel dataset, respectively.\n4) Dynamic Training Enhancer: Based on self-supervised\nlearning, the dynamic training enhancer (DTE) improves the\nmodel’s generalization to the three speciﬁc scenes and thus\nenhances the model performance by a large margin. We\nseparately validate the performance of its three components:\nDynamic Occlusion Training Enhancer (DOE), Spatial Varia-\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n13\nTABLE VI\nABLATION STUDY ON DIFFERENT COMBINATIONS OF THREE TRAINING ENHANCERS. BASELINE: ULDENET STRUCTURE WITH BASIC MULTI-FRAME UNSUPERVISED\nTRAINING. DOE: DYNAMIC OCCLUSION TRAINING ENHANCER; SVE: SPATIAL VARIATION TRAINING ENHANCER; CVE: CONTENT VARIATION TRAINING ENHANCER.\nBaseline\nDOE\nSVE\nCVE\nSintel Clean\nSintel Final\nKITTI-15\nKITTI-12\nALL\nNOC\nOCC\nALL\nNOC\nOCC\nALL\nNOC\nOCC\nALL\nNOC\nOCC\n✓\n2.17\n0.91\n19.36\n3.12\n1.74\n22.86\n2.65\n1.92\n5.86\n1.42\n0.92\n4.32\n✓\n✓\n1.86\n0.85\n16.25\n2.90\n1.69\n20.42\n2.43\n1.76\n5.38\n1.28\n0.85\n3.81\n✓\n✓\n1.96\n0.75\n17.87\n2.98\n1.63\n22.17\n2.55\n1.82\n5.77\n1.35\n0.86\n4.18\n✓\n✓\n✓\n1.79\n0.78\n15.82\n2.88\n1.68\n20.22\n2.37\n1.72\n5.24\n1.24\n0.83\n3.67\n✓\n✓\n1.92\n0.79\n17.23\n2.94\n1.66\n21.36\n2.50\n1.86\n5.60\n1.31\n0.86\n3.96\n✓\n✓\n✓\n✓\n1.75\n0.81\n15.21\n2.81\n1.63\n19.83\n2.28\n1.62\n5.17\n1.19\n0.78\n3.59\ntion Training Enhancer (SVE), and Content Variation Training\nEnhancer (CVE), as well as the performance of their different\ncombinations strategies.\nTable VI summarizes the ablation studies of DTE on the\nSintel Clean, Sintel Final, KITTI 2012, and KITTI 2015\ndatasets, where the Baseline represents the ULDENet with\nthe basic unsupervised loss combination, and the temporal\nsmoothing regularization is not used here. Quantitatively, the\nintroduction of DOE alone reduces the EPE from 2.17 to\n1.86, bringing the most considerable improvement of 14.3%,\nand the following SVE and CVE obtain 10% and 11.5%\nperformance gain, respectively. Combining the three can sig-\nniﬁcantly reduce the EPE from 2.17 to 1.75, reducing the\nerror of nearly 20 percent. As the result of the dynamic and\nrealistic simulation of occlusion, the DOE is most signiﬁcant\nfor improving the prediction of the OCC region, as diminishing\nthe EPE from 19.36 to 16.25 on the Sintel Clean dataset. The\nCVE also slightly improves around the OCC region, as the\ndynamic noise and regional blurring can be considered similar\nto information loss from the occlusion. In addition, both CVE\nand SVE can reduce the prediction error of the network\nfor regular NOC regions due to the simulation of multiple\ndynamic variation scenarios and self-supervised distillation.\nThe dynamic variation transformation and static image\ntransformations are further validated in Table VII, where\nthe CT and ST are deﬁned as identically performing the\nContent & Spatial image transformations across all frame\nsequences, such as image ﬂipping, Gaussian blur, etc., which\nare similar to the data augmentation method [15]; Stc Occ\nis deﬁned as image cropping and random noise occlusion of\nimage subregions with the original unsupervised loss [13].\nAll transform hyperparameters are set to be consistent with\nthe dynamic scene. From the results, it is straightforward that\ndynamic variant scenes and dynamic occlusion have consider-\nable performance advantages over static image augmentation,\nin which the proposed dynamic occlusion simulation with a\nmixed occlusion loss strategy (Dyc Occ-mixed) holds a gain\nof nearly 10% over Stc Occ.\nD. Generalization Validation\nWe validate the cross-dataset generalization performance\nof the proposed model and compare it with both supervised\nand unsupervised models. In Table VIII, we choose Sintel\nas the training set and FlyingChairs [51] and ChairSDHom\n[22] as the validation sets. Both supervised learning methods,\nRAFT [19], PWCNet [24], and unsupervised learning method\nARFlow [15] are selected as comparisons.\nTABLE VII\nABLATION STUDY BETWEEN DYNAMIC SCENE VARIATION AND STATIC DATA\nAUGMENTATION. BASELINE: ULDENET STRUCTURE WITH BASIC MULTI-FRAME\nUNSUPERVISED TRAINING; STC OCC: STATIC RANDOM NOISE MASKING; DYC\nOCC-SPARSE / MIX: DYNAMIC OCCLUSION SIMULATION WITH SPARES / MIXED LOSS\nSUPERVISION. STC CT / ST: STATIC CONTENT TRANSFORMATION / SPATIAL\nTRANSFORMATION. DYC CV / SV: DYNAMIC CONTENT VARIATION / SPATIAL\nVARIATION.\nEhancer Strategy\nSintel Clean\nSintel Final\nALL\nNOC\nOCC\nALL\nNOC\nOCC\nBaseline\n2.17\n0.91\n19.36\n3.12\n1.74\n22.83\n+Stc Occ\n2.06\n0.90\n18.21\n3.03\n1.78\n21.15\n+Dyc Occ-sparse\n1.90\n0.89\n16.47\n2.95\n1.72\n20.74\n+Dyc Occ-mix\n1.86\n0.85\n16.25\n2.90\n1.69\n20.42\n+ Stc CT\n2.04\n0.81\n18.44\n3.03\n1.71\n22.01\n+ Dyc CV\n1.92\n0.79\n17.23\n2.94\n1.66\n21.36\n+ Stc ST\n2.07\n0.77\n19.00\n3.08\n1.73\n22.44\n+ Dyc SV\n1.96\n0.75\n17.87\n2.98\n1.63\n22.17\nTABLE VIII\nGENERALIZATION VALIDATION. ALL MODELS WERE FINE-TUNED ON THE SINTEL\nDATASET AND VALIDATED CROSS-DATASETS.\nMethod\nARFlow [15]\nPWCNet [24]\nRAFT [19]\nOurs\nSintel Clean\n(2.79)\n(1.86)\n(0.74)\n(2.20)\nSintel Final\n(3.73)\n(2.31)\n(1.21)\n(2.65)\nChairSDHom\n0.67\n2.67\n0.48\n0.47\nFlyingChair\n3.50\n3.69\n1.20\n3.33\nIt should be noted that both FlyingChair and ChairSDHom\ndatasets only allow two frames per inference, which limits\nthe capability of our model. However, the generalization per-\nformance of the ULDENet on the FlyingChair dataset is still\nhigher than that of ARFlow and PWCNet. In particular, our\ngeneralization performance on ChairSDHom is even higher\nthan that of one of the most advanced supervised learning\nmethods, RAFT.\nInterestingly, we found that most of the existing models\nare unable robustly handle the samples with non-textured\nbackgrounds contained in ChairSDHom. To better illustrate,\nwe produced similar samples of a rightward moving box\nwith uniformly distributed texture to verify the generalization\nability of the multiple models, and all models were taken from\ntheir ofﬁcial models ﬁne-tuned on the Sintel dataset, includ-\ning both unsupervised and supervised training. As shown in\nFig. 9, all compared models exhibit relatively poor results,\nwhereas our ULDENet performs the best in comparison, which\nsomehow demonstrates a behavior that is closest to the human\ninterpretation of such scenarios.\nVII. CONCLUSION\nIn this work, we explore the possibility of unsupervised\nlearning-based optical ﬂow estimation exposure to prolonged\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n14\nFig. 9. Demonstration of the generalization performance of the model in\nthe non-textured stimuli. We synthesized 100 consecutive image frames with\na constant rightward moving of 4 pixels per frame. All compared models are\nﬁne-tuned on the Sintel dataset.\nlong-temporal dynamic environments and propose multiple\nunsupervised learning strategies based on multi-frame dynamic\ntraining. The contributions of this work are manifold: ﬁrstly, by\npartially following the well-known predictive coding mecha-\nnism, we construct a lightweight model with a spatial-temporal\ndual recurrent structure, which is demonstrated effectiveness\nin optical ﬂow estimation in ablation studies; further, we\ndesign three temporal dynamic training enhancers, namely\nDOE, CVE, and SVE. The reasonable combination of the three\nimproves the performance by approximately 20% without\nincurring any computational and memory overheads. Remark-\nably, in coping with the intractable occlusion problem, we\npropose the temporal smoothing regularization that can supply\na reliable reference for the occlusion region based on the\nmotion prior from adjacent frames. In addition, we present\nan efﬁcient method based on the Markov process in DOE to\nsimulate the dynamic occlusion process, which, combined with\nthe well-designed mixed supervision strategy, reduces the EPE\nof the OCC region on the Sintel training set by 16%.\nCombining all the proposals, we reach the state-of-the-art\nresults with a fairly lightweight model for both the test set and\nthe training set of multiple standard benchmarks.\nREFERENCES\n[1] H. Jiang, D. Sun, V. Jampani, M.-H. Yang, E. Learned-Miller, and\nJ. Kautz, “Super slomo: High quality estimation of multiple intermediate\nframes for video interpolation,” in Proceedings of the IEEE conference\non computer vision and pattern recognition, 2018, pp. 9000–9008.\n[2] K. Simonyan and A. Zisserman, “Two-stream convolutional networks for\naction recognition in videos,” Advances in neural information processing\nsystems, vol. 27, 2014.\n[3] A. Behl, O. Hosseini Jafari, S. Karthik Mustikovela, H. Abu Alhaija,\nC. Rother, and A. Geiger, “Bounding boxes, segmentations and object\ncoordinates: How important is recognition for 3d scene ﬂow estimation\nin autonomous driving scenarios?” in Proceedings of the IEEE Interna-\ntional Conference on Computer Vision, 2017, pp. 2574–2583.\n[4] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert, “High accuracy\noptical ﬂow estimation based on a theory for warping,” in European\nconference on computer vision.\nSpringer, 2004, pp. 25–36.\n[5] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid, “Epicﬂow:\nEdge-preserving interpolation of correspondences for optical ﬂow,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2015, pp. 1164–1172.\n[6] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,\nP. Van Der Smagt, D. Cremers, and T. Brox, “Flownet: Learning\noptical ﬂow with convolutional networks,” in Proceedings of the IEEE\ninternational conference on computer vision, 2015, pp. 2758–2766.\n[7] N. Mayer, E. Ilg, P. Fischer, C. Hazirbas, D. Cremers, A. Dosovitskiy,\nand T. Brox, “What makes good synthetic training data for learning dis-\nparity and optical ﬂow estimation?” International Journal of Computer\nVision, vol. 126, no. 9, pp. 942–960, 2018.\n[8] J. J. Yu, A. W. Harley, and K. G. Derpanis, “Back to basics: Unsu-\npervised learning of optical ﬂow via brightness constancy and motion\nsmoothness,” in European Conference on Computer Vision.\nSpringer,\n2016, pp. 3–10.\n[9] W. Im, T.-K. Kim, and S.-E. Yoon, “Unsupervised learning of optical\nﬂow with deep feature similarity,” in European Conference on Computer\nVision.\nSpringer, 2020, pp. 172–188.\n[10] S. Meister, J. Hur, and S. Roth, “Unﬂow: Unsupervised learning of\noptical ﬂow with a bidirectional census loss,” in Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\n[11] Y. Wang, Y. Yang, Z. Yang, L. Zhao, P. Wang, and W. Xu, “Occlusion\naware unsupervised learning of optical ﬂow,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2018,\npp. 4884–4893.\n[12] J. Janai, F. Guney, A. Ranjan, M. Black, and A. Geiger, “Unsupervised\nlearning of multi-frame optical ﬂow with occlusions,” in Proceedings\nof the European Conference on Computer Vision (ECCV), 2018, pp.\n690–706.\n[13] P. Liu, M. Lyu, I. King, and J. Xu, “Selﬂow: Self-supervised learning of\noptical ﬂow,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2019, pp. 4571–4580.\n[14] R. Jonschkowski, A. Stone, J. T. Barron, A. Gordon, K. Konolige, and\nA. Angelova, “What matters in unsupervised optical ﬂow,” in European\nConference on Computer Vision.\nSpringer, 2020, pp. 557–572.\n[15] L. Liu, J. Zhang, R. He, Y. Liu, Y. Wang, Y. Tai, D. Luo, C. Wang, J. Li,\nand F. Huang, “Learning by analogy: Reliable supervision from trans-\nformations for unsupervised optical ﬂow estimation,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 6489–6498.\n[16] K. Luo, C. Wang, S. Liu, H. Fan, J. Wang, and J. Sun, “Upﬂow: Upsam-\npling pyramid for unsupervised optical ﬂow learning,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2021, pp. 1045–1054.\n[17] R. P. Rao and D. H. Ballard, “Predictive coding in the visual cortex: a\nfunctional interpretation of some extra-classical receptive-ﬁeld effects,”\nNature neuroscience, vol. 2, no. 1, pp. 79–87, 1999.\n[18] W. Lotter, G. Kreiman, and D. Cox, “Deep predictive coding net-\nworks for video prediction and unsupervised learning,” arXiv preprint\narXiv:1605.08104, 2016.\n[19] Z. Teed and J. Deng, “Raft: Recurrent all-pairs ﬁeld transforms for\noptical ﬂow,” in European conference on computer vision.\nSpringer,\n2020, pp. 402–419.\n[20] P. Godet, A. Boulch, A. Plyer, and G. Le Besnerais, “Starﬂow: A\nspatiotemporal recurrent cell for lightweight multi-frame optical ﬂow\nestimation,” in 2020 25th International Conference on Pattern Recogni-\ntion (ICPR).\nIEEE, 2021, pp. 2462–2469.\n[21] J. Janai, F. Guney, A. Ranjan, M. Black, and A. Geiger, “Unsupervised\nlearning of multi-frame optical ﬂow with occlusions,” in Proceedings\nof the European Conference on Computer Vision (ECCV), 2018, pp.\n690–706.\n[22] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox,\n“Flownet 2.0: Evolution of optical ﬂow estimation with deep networks,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2017, pp. 2462–2470.\n[23] A. Ranjan and M. J. Black, “Optical ﬂow estimation using a spatial\npyramid network,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2017, pp. 4161–4170.\n[24] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz, “Pwc-net: Cnns for optical\nﬂow using pyramid, warping, and cost volume,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition, 2018, pp.\n8934–8943.\n[25] T.-W. Hui, X. Tang, and C. C. Loy, “Liteﬂownet: A lightweight convo-\nlutional neural network for optical ﬂow estimation,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2018,\npp. 8981–8989.\n[26] J. Hur and S. Roth, “Iterative residual reﬁnement for joint optical ﬂow\nand occlusion estimation,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2019, pp. 5754–5763.\n[27] S. Guan, H. Li, and W.-S. Zheng, “Unsupervised learning for optical\nﬂow estimation using pyramid convolution lstm,” in 2019 IEEE Inter-\nPREPRINT TO ARXIV.ORG , VOL. XX, NO. XX, XXX XXXX\n15\nnational Conference on Multimedia and Expo (ICME).\nIEEE, 2019,\npp. 181–186.\n[28] Y. Zhong, P. Ji, J. Wang, Y. Dai, and H. Li, “Unsupervised deep epipolar\nﬂow for stationary or dynamic scenes,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2019, pp.\n12 095–12 104.\n[29] Y. Zou, Z. Luo, and J.-B. Huang, “Df-net: Unsupervised joint learning\nof depth and ﬂow using cross-task consistency,” in Proceedings of the\nEuropean conference on computer vision (ECCV), 2018, pp. 36–53.\n[30] Z. Yin and J. Shi, “Geonet: Unsupervised learning of dense depth,\noptical ﬂow and camera pose,” in Proceedings of the IEEE conference\non computer vision and pattern recognition, 2018, pp. 1983–1992.\n[31] Y. Wang, P. Wang, Z. Yang, C. Luo, Y. Yang, and W. Xu, “Unos:\nUniﬁed unsupervised optical-ﬂow and stereo-depth estimation by watch-\ning videos,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2019, pp. 8071–8081.\n[32] L. Liu, G. Zhai, W. Ye, and Y. Liu, “Unsupervised learning of scene\nﬂow estimation fusing with local rigidity.” in IJCAI, 2019, pp. 876–882.\n[33] P. Liu, I. King, M. R. Lyu, and J. Xu, “Ddﬂow: Learning optical ﬂow\nwith unlabeled data distillation,” in Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, vol. 33, no. 01, 2019, pp. 8770–8777.\n[34] M. Neoral, J. ˇSochman, and J. Matas, “Continual occlusion and optical\nﬂow estimation,” in Asian Conference on Computer Vision.\nSpringer,\n2018, pp. 159–174.\n[35] D. Sun, S. Roth, and M. J. Black, “Secrets of optical ﬂow estimation and\ntheir principles,” in 2010 IEEE computer society conference on computer\nvision and pattern recognition.\nIEEE, 2010, pp. 2432–2439.\n[36] C. Tomasi and R. Manduchi, “Bilateral ﬁltering for gray and color\nimages,” in Sixth international conference on computer vision (IEEE\nCat. No. 98CH36271).\nIEEE, 1998, pp. 839–846.\n[37] S. Ince and J. Konrad, “Occlusion-aware optical ﬂow estimation,” IEEE\nTransactions on Image Processing, vol. 17, no. 8, pp. 1443–1451, 2008.\n[38] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y. Bengio, “Learning phrase representations using\nrnn encoder-decoder for statistical machine translation,” arXiv preprint\narXiv:1406.1078, 2014.\n[39] K. Storrs and R. Fleming, “Learning to see material from motion by\npredicting videos,” Journal of Vision, vol. 21, no. 9, pp. 1993–1993,\n2021.\n[40] P. Liu, I. King, M. R. Lyu, and J. Xu, “Ddﬂow: Learning optical ﬂow\nwith unlabeled data distillation,” in Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, vol. 33, no. 01, 2019, pp. 8770–8777.\n[41] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. S¨usstrunk,\n“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE\ntransactions on pattern analysis and machine intelligence, vol. 34,\nno. 11, pp. 2274–2282, 2012.\n[42] O. Freifeld, S. Hauberg, K. Batmanghelich, and J. W. Fisher, “Trans-\nformations based on continuous piecewise-afﬁne velocity ﬁelds,” IEEE\ntransactions on pattern analysis and machine intelligence, vol. 39,\nno. 12, pp. 2496–2509, 2017.\n[43] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, “A naturalistic\nopen source movie for optical ﬂow evaluation,” in European Conf. on\nComputer Vision (ECCV), ser. Part IV, LNCS 7577, A. Fitzgibbon et al.\n(Eds.), Ed.\nSpringer-Verlag, Oct. 2012, pp. 611–625.\n[44] M. Hradiˇs, J. Kotera, P. Zemcık, and F. ˇSroubek, “Convolutional neural\nnetworks for direct text deblurring,” in Proceedings of BMVC, vol. 10,\nno. 2, 2015.\n[45] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous\ndriving? the kitti vision benchmark suite,” in Conference on Computer\nVision and Pattern Recognition (CVPR), 2012.\n[46] M. Menze, C. Heipke, and A. Geiger, “Joint 3d estimation of vehicles\nand scene ﬂow,” in ISPRS Workshop on Image Sequence Analysis (ISA),\n2015.\n[47] T.-W. Hui, X. Tang, and C. C. Loy, “Liteﬂownet: A lightweight convo-\nlutional neural network for optical ﬂow estimation,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2018,\npp. 8981–8989.\n[48] S. Liu, K. Luo, N. Ye, C. Wang, J. Wang, and B. Zeng, “Oiﬂow:\nocclusion-inpainting optical ﬂow estimation by unsupervised learning,”\nIEEE Transactions on Image Processing, vol. 30, pp. 6420–6433, 2021.\n[49] L. Tian, Z. Tu, D. Zhang, J. Liu, B. Li, and J. Yuan, “Unsupervised\nlearning of optical ﬂow with cnn-based non-local ﬁltering,” IEEE\nTransactions on Image Processing, vol. 29, pp. 8429–8442, 2020.\n[50] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n[51] A. Dosovitskiy, P. Fischer, E. Ilg, P. H¨ausser, C. Hazırbas¸, V. Golkov,\nP. v.d. Smagt, D. Cremers, and T. Brox, “Flownet: Learning optical\nﬂow with convolutional networks,” in IEEE International Conference\non Computer Vision (ICCV), 2015.\n",
  "categories": [
    "cs.CV",
    "68T45",
    "F.2.2; I.2.7"
  ],
  "published": "2023-04-14",
  "updated": "2023-04-14"
}