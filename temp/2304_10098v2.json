{
  "id": "http://arxiv.org/abs/2304.10098v2",
  "title": "Two-Memory Reinforcement Learning",
  "authors": [
    "Zhao Yang",
    "Thomas. M. Moerland",
    "Mike Preuss",
    "Aske Plaat"
  ],
  "abstract": "While deep reinforcement learning has shown important empirical success, it\ntends to learn relatively slow due to slow propagation of rewards information\nand slow update of parametric neural networks. Non-parametric episodic memory,\non the other hand, provides a faster learning alternative that does not require\nrepresentation learning and uses maximum episodic return as state-action values\nfor action selection. Episodic memory and reinforcement learning both have\ntheir own strengths and weaknesses. Notably, humans can leverage multiple\nmemory systems concurrently during learning and benefit from all of them. In\nthis work, we propose a method called Two-Memory reinforcement learning agent\n(2M) that combines episodic memory and reinforcement learning to distill both\nof their strengths. The 2M agent exploits the speed of the episodic memory part\nand the optimality and the generalization capacity of the reinforcement\nlearning part to complement each other. Our experiments demonstrate that the 2M\nagent is more data efficient and outperforms both pure episodic memory and pure\nreinforcement learning, as well as a state-of-the-art memory-augmented RL\nagent. Moreover, the proposed approach provides a general framework that can be\nused to combine any episodic memory agent with other off-policy reinforcement\nlearning algorithms.",
  "text": "Two-Memory Reinforcement Learning\nZhao Yang, Thomas. M. Moerland, Mike Preuss, Aske Plaat\nLeiden Institute of Advanced Computer Science\nLeiden University\nz.yang@liacs.leidenuniv.nl\nAbstract—While deep reinforcement learning has shown im-\nportant empirical success, it tends to learn relatively slow due\nto slow propagation of rewards information and slow update\nof parametric neural networks. Non-parametric episodic mem-\nory, on the other hand, provides a faster learning alternative\nthat does not require representation learning and uses maxi-\nmum episodic return as state-action values for action selection.\nEpisodic memory and reinforcement learning both have their\nown strengths and weaknesses. Notably, humans can leverage\nmultiple memory systems concurrently during learning and\nbeneﬁt from all of them. In this work, we propose a method\ncalled Two-Memory reinforcement learning agent (2M)1 that\ncombines episodic memory and reinforcement learning to distill\nboth of their strengths. The 2M agent exploits the speed of the\nepisodic memory part and the optimality and the generalization\ncapacity of the reinforcement learning part to complement\neach other. Our experiments demonstrate that the 2M agent is\nmore data efﬁcient and outperforms both pure episodic memory\nand pure reinforcement learning, as well as a state-of-the-art\nmemory-augmented RL agent. Moreover, the proposed approach\nprovides a general framework that can be used to combine\nany episodic memory agent with other off-policy reinforcement\nlearning algorithms.\nIndex Terms—episodic control, reinforcement learning, mem-\nory, Atari\nI. INTRODUCTION\nDeep reinforcement learning (DRL) achieves impressive\nresults in a wide range of domains. It reaches super-human\nperformance in games such as Atari [1], Go [2] and Gran\nTurismo [3]. Recently, it also has shown promise in scientiﬁc\napplications such as controlling nuclear plasma fusion [4] and\ndiscovering new matrix multiplication algorithms [5]. How-\never, DRL is well-known for being data inefﬁcient, since back-\npropagation of reward signals and learning updates (including\nrepresentation learning) can be slow.\nIn contrast to such parametric learning approaches, non-\nparametric episodic memory approaches maintain a memory\nbuffer to store high-rewarded trajectories for either action\nselection [6], [7] or for enhancing other reinforcement learning\nmethods [8], [9], [10]. These papers have been demonstrated\nto outperform conventional reinforcement learning methods\nin certain tasks, such as Atari [11], and Labyrinth [12]. In\nepisodic memory, reward signals are back-propagated con-\nsiderably faster than in one-step temporal difference (TD)\nlearning that is commonly used in reinforcement learning.\nIn addition, one-step TD learning is further slowed down by\nrepresentation learning and the use of function approximation.\n1Code will be available after authors notiﬁcation.\nFig. 1. The workﬂow of the 2M agent. Before the episode starts, the 2M agent\nselects one type of memory for action selection in the next episode, which\ncould either be episodic control (EC) or parametric reinforcement learning\n(RL). The data subsequently collected is used to update the EC solution\n(directly), and also enters an experience replay buffer (B) for future updating\nof the parametric RL solution.\nA potential problem of episodic memory is that fast back-\npropagation is problematic in stochastic tasks, and the lack\nof learnable feature representations can make generalization\ndifﬁcult in episodic memory. The question therefore becomes:\ncan we combine the best of both approaches in a single\nalgorithm?\nEvidence from neuroscience shows that multiple memory\nsystems are activated when humans are learning, and these\nalso interact with each other [13], [14]. Previous research [8],\n[9], [10] has shown that integrating episodic memory and\nreinforcement learning can improve overall performance. In\nthese works, episodic memory is mainly used to provide\nlearning signals for DRL methods, but they again face the\nsame challenges that are inherent to DRL. To fully capitalize\non the advantages of episodic memory and reinforcement\nlearning, we propose a novel approach called Two-Memory\nreinforcement learning (2M), in which both approaches com-\nplement eachother.\nThe workﬂow of the 2M agent is shown in Fig. 1. The\n2M agent maintains two memories, namely ‘episodic control’\n(episodic memory for control) (2M-EC) and ‘reinforcement\nlearning’ (2M-RL). In the beginning, the 2M agent decides\nwhich memory to employ for action selection in the upcoming\nepisode. Then the collected episodic data is pushed into the\nexperience replay buffer where data for training 2M-RL is\nsampled from occasionally. Meanwhile, episodic trajectories\nare used to update 2M-EC.\nThe intuition is that after 2M-EC discovers high-reward\narXiv:2304.10098v2  [cs.LG]  23 Apr 2023\ntrajectories, the 2M agent is able to retrieve these trajectories\nquickly although they might be suboptimal. However, at this\nstage, 2M-RL still has not learned anything substantial due to\nthe one-step reward backup and the slow updates required with\nfunction approximation (in DRL). Thus, the 2M agent should\nprefer to use 2M-EC initially. As the amount of data collected\nincreases and training continues, 2M-RL becomes increasingly\ngood at dealing with stochasticity and developing better feature\nrepresentations; the 2M agent should then gradually switch to\n2M-RL. This is the conceptual approach we study in this work.\nIn short, our work makes two main contributions:\n• We propose a novel framework called 2M that combines\nepisodic control and reinforcement learning methods,\nexploiting the strengths from both sides. The framework\ncan be used with any type of EC method and any type\nof off-policy RL method.\n• We conduct experiments showing that 2M outperforms\nstate-of-the-art baselines, and we also include ablation\nstudies that examine when the 2M works well, and where\nit may still be improved.\nII. BACKGROUND\nWe will ﬁrst introduce the formal problem deﬁnition and\nthe two main approaches combined in this paper: parametric\nreinforcement learning (in the form of deep Q-learning) and\nnon-parametric episodic memory.\nA. Markov Decision Process\nWe follow standard deﬁnitions [15] and deﬁne decision\nmaking problems as a Markov Decision Process (MDP) rep-\nresented by ⟨S, A, R, P, γ⟩. S denotes the state space, A\ndenotes the action space, R denotes the reward function and\nP denotes the dynamic transition. γ is the discount factor,\nwhich is usually close to 1. The agent interacts with the\nenvironment by taking action at ∈A according to some policy\nπ given the current state st ∈S at time step t, then the next\nstate st+1 ∼P(·|st, at) and reward rt = R(st, at, st+1) is\nreturned by the environment. A policy π maps a state to a\ndistribution over actions. The agent will then take the next\naction at+1 based on the new state st+1. This process repeats\nuntil the terminal time step T. The goal of the agent is to learn\na policy π that maximizes the expected cumulative reward:\nEst+1∼P (·|st,at),at∼π(·|st),rt∼R(·|st,at)[PT\nt=0 γt · rt].\nB. Deep Q-Learning\nDeﬁne the state-action value function Q(st, at) as the\nexpected cumulative reward the agent will receive by starting\nfrom state st and taking action at. Q-learning is an algorithm\nto learn the optimal Q value function, it updates the state-\naction value function by iteratively following the Bellman\nequation. The update rule of Q-learning is:\nQ(st, at) ←Q(st, at)+α(rt+γ max\na′∈A Q(st+1, a′)−Q(st, at))\nwhere α is the learning rate and γ is the discount factor. The\nsolution of Q-learning is generally stored in a table. When\nthe state space is large, storing all possible states becomes\ninfeasible. We may then use a neural network to approximate\nthe state-action value function. The update rule of deep Q\nnetwork [2] is:\ny(st) ←rt−1 + γ max\na′∈A Q(st, a′)\nminimize Est,at,rt,st+1∈D(y(st+1) −Q(st, at))2\n(1)\nwhere D is the sampled data for training. After we have the\nstate-action value function, the policy is exacted by always\ntaking the action with the highest state-action value greedily\nin every state.\nC. Episodic Control\nEpisodic control refers to a class of methods that directly\nuse non-parametric episodic memory for action selection [6],\n[7]. It is generally implemented as a table (Qec), and rows\nrepresent different actions while columns represent different\nstates. Each entry is associated with a state-action (st, at)\npair and denotes the highest encountered episodic return\n(Gt = PT\nk=t γk−trk) after taking action at in the state st. The\nupdate only occurs after one episode has terminated, which\nis similar to tabular Monte-Carlo back-up but with a more\naggressive update rule. Instead of incrementally updating state-\naction values, episodic control replaces the stored episodic\nreturn with the best value observed so far. The update rule\nof episodic control is:\nQec(st, at) ←\n(\nGt\nif (st, at) /∈Qec,\nmax{Gt, Qec(st, at)}\notherwise\n(2)\nDuring action selection, if the queried state already has Q\nvalues for all actions, we take the action that is with the\nhighest Q value. Otherwise, missed Q values are estimated\nby averaging over its K-nearest neighbors’ Q values. When\nthe memory is full, the least updated entry will be dropped\nfrom the table.\nIII. RELATED WORK\nWe will ﬁrst discuss previous work on episodic control, and\nhow it has been used as a training target for deep reinforcement\nlearning. This last approach differs from our work, where EC\nis used for action selection (see Fig. 1). Afterwards, we also\nbrieﬂy discuss related work on experience replay, since it plays\na central role in our approach as well (see Fig. 1).\nA. Episodic Control\nModel Free Episodic Control (MFEC) [6] is the ﬁrst\nepisodic control method and it is implemented as a table with\nuntrainable features. Thus, it enjoys limited generalization\nover either a randomly projected or pre-trained feature space.\nNeural Episodic Control (NEC) [7] solves this limitation by\nmaintaining a differentiable table to learn features while using\nthe same update and action selection rule (shown in Eq. 2) as\nMFEC but achieves better performance. Since Monte-Carlo\nreturns must be stored for each state-action pair, episodic\ncontrol methods can not deal with continuous action spaces\nnaturally. Continuous Episodic Control (CEC) [16] extends\nepisodic memory to select actions directly in continuous action\nspace. The 2M agent integrates MFEC as a core component,\nand partially uses it for action selection.\nB. Episodic Memory for Learning\nWhile using episodic memory for control is fast, reinforce-\nment learning methods might still be preferred in the long\nrun due to their strength. Many approaches use the returns\nstored in episodic memory to provide richer learning targets\nfor reinforcement learning methods. Episodic Memory Deep\nQ Network (EMDQN) [8] uses returns in episodic memory\nto enhance learning targets in deep Q-Learning. Episodic\nMemory Actor-Critic (EMAC) [9] and Generalizable Episodic\nMemory (GEM) [10] uses episodic returns to enhance learning\ntargets in actor-critic methods to solve tasks with continuous\naction space. Episodic Backward Update (EBU) [17] utilizes\nstructural information of states that are in the same episode\nand executes a one-step backup for each state along the\ntrajectory. The aforementioned methods take advantage from\nricher learning signals of episodic memory, but underlying\nneural network training still progresses slowly. Thereby, the\nbeneﬁt of the EC solution will not affect action selection\nquickly. In contrast, the 2M agent does use episodic memory\nfor action selection, which may give it a fast head-start. As\na second difference, in 2M the collected data is also used to\ntrain the 2M-RL agent (in contrast to previous methods).\nC. Experience Replay\nExperience replay was originally proposed to improve data\nefﬁciency and break correlations of training data for off-policy\nreinforcement learning methods. Uniform sampling is the most\nnaive and commonly used way to sample data from the\nreplay buffer for training, where transitions are sampled at the\nsame frequency as they were experienced regardless of their\nsigniﬁcance [18]. To address this limitation, prioritized Experi-\nence Replay (PER) [18] prioritizes transitions that have larger\nTD errors during the training, and samples these transitions\nmore often because larger TD errors indicate there is more\ninformation to learn. Hindsight Experience Replay (HER) [19]\nis proposed for the multi-goal/goal-conditioned RL setting; it\ntreats states that the agent actually achieves as desired goals\nand learns from failures. Since not all failures are equally\nimportant, Curriculum-guided Hindsight Experience Replay\n(CHER) [20] adaptively replays failed experiences according\nto their similarities to true goals. An event table [21] is deﬁned\nas a buffer to store transitions related to important events. The\nauthors theoretically proved that sampling more data from the\nevent table will lead to better performance. Although the 2M\nagent doesn’t employ any special sampling strategies to sample\nfrom the replay buffer, the data stored in the buffer is actually\nfrom two different sources (2M-EC and 2M-RL). We can thus\nvary sampling preference by pushing different amounts of data\nfrom different sources.\nIV. TWO-MEMORY REINFORCEMENT LEARNING (2M)\nWe will now formally introduce the 2M framework. It con-\nsists of two ‘memories’, where one represents a fast learning\nmemory (episodic control agent) and another one represents\na slow learning memory (reinforcement learning agent, in our\nwork we use 1-step (deep) Q-learning). Intuitively, we should\nprefer to use the fast (but sub-optimal) learning memory\ninitially, then gradually switch to the slow (but with better\nasymptotic performance) learning memory. In this section, we\nwill ﬁrst motivate this intuition by using a very simple example\n(Sec. IV-A). Then we explain the designs of the proposed\napproach. Since we combine two different methods and want\nto switch between them, we also need to discuss a scheduling\nmechanism to decide when to switch (section IV-B) and need\nto decide how to utilize collected data (section IV-C). The\noverall algorithm is detailed in Alg. 1.\nFig. 2. A motivating example with seven states in the state space and two\nactions in the action space. After the agent discovers all possible trajectories\n(from s1 to all possible terminal states), EC (orange) ﬁnds the sub-optimal\nsolution while one-step off-policy RL (grey) only back-propagates reward\nsignals to direct predecessor states and thus does not change the estimates at\nthe root state. However, after many iterations, off-policy RL will converge to\nthe true optimal values (preferring action left followed by action left), while\nEC will commit to a suboptimal solution (action left followed by action right).\nColour shading indicates the preferred solution path for each method after a\nsingle visit to every possible path.\nA. A Motivating Example\nWe ﬁrst consider a very simple example domain shown in\nFig. 2, circles with numbers represent different states while\nsmaller circles represent actions. There are seven states in\nthe state space: S = {s1, s2, s3, s4, s5, s6, s7}, two actions\n(left and right) in the action space: A = {a1, a2}. Most of\ndynamic transitions are deterministic, except in state s2, after\ntaking action a2, there is 50% probability the next state ends up\nwith s5 and 50% probability ends up with s6: P(s2, a2, s5) =\nP(s2, a2, s6) = 0.5. Leaf nodes (s4, s5, s6, s7) are terminal\nstates where the agent will receive a reward: R(s2, a1, s4) =\n+10, R(s2, a2, s5)\n=\n−10, R(s2, a2, s6)\n=\n+20, and\nR(s3, a2, s7) = −20. Color shading indicates the decisions\nthat different agents will make after a single visit to every\npossible trajectory (orange for episodic control and grey for\n1-step Q-learning). We can see that after the agent discovers all\npossible trajectories, the episodic control agent is already able\nto follow a sub-optimal trajectory starting from the initial state.\nHowever, the 1-step Q-learning agent only back-propagates\nthe reward signal from terminal states to their predecessors,\nFig. 3. The ﬁve MinAtar games used in the experiments (from left to right): Breakout, Space Invaders, Asterix, Seaquest, Freeway.\nAlgorithm 1 Two-Memory Reinforcement Learning\nInput: Environment env, non-parametric episodic control\nagent ec, parameterized reinforcement learning agent rl,\nTwo-memory agent 2M, replay buffer B, probability pec,\nexploration factor ϵ\nGec ←[ ]\n{initialize return list of ec}\nGrl ←[ ]\n{initialize return list of rl}\n2M-RL ←rl\n2M-EC ←ec\nwhile training budget left do\ns ←reset the env\ndone = false\nτ ←[ ]\n{initialize episodic trajectory}\nif ∆∼U(0, 1) < pec then\n2M ←2M-EC\n{2M switches to EC}\nelse\n2M ←2M-RL\n{2M switches to RL}\nend if\nG ←0\nwhile not done do\nexecute action a selected by 2M based on s with ϵ-\ngreedy exploration\nobserve reward r and next state s′ in env\nstore (s, a, r, s′) to B and τ\ns ←s′\nG ←G + r\nupdate 2M-RL using D according to (1) if needed, D ∼\nB\nend while\nupdate 2M-RL using τ according to (2)\nif 2M is 2M-RL then\nadd G to Grl\nelse\nadd G to Gec\nend if\nupdate pec\n{decay or increase or\nconsistent}\nend while\nwhich means the 1-step Q-learning agent still is not able to\nmake decisions in the initial state. The optimal policy for\nthis MDP is to take a1 in s1 and take a1 in s2 as well.\nBy deﬁnition, episodic control will converge to a sub-optimal\npolicy that always takes a2 in s2 (shown in Eq. 3). With more\nupdates, 1-step Q-learning will learn the optimal state-action\nvalue (shown in Eq. 4) for each state-action pair, which will\nresult in the optimal policy.\nQec(s2, a1) = 10, Qec(s2, a2) = 20\n(3)\nQ∗(s2, a1) = 10, Q∗(s2, a2) = 5\n(4)\nThus, we conclude that episodic control is fast but sub-\noptimal, and reinforcement learning (1-step Q-learning in our\ncase) is slow but optimal. There should exist an intersection\npoint between these two different methods where reinforce-\nment learning surpasses episodic memory. One might ask why\nwe do not compare with (full-episode) Monte-Carlo backup.\nThe 1-step back-up can utilize off-policy learning to learn from\ndata collected by other policies, which is more data efﬁcient.\nWe therefore aim to combine the fast learning of EC with the\ndata efﬁciency of 1-step off-policy learning.\nB. Switching\nThe previous example highlighted that EC may learn a\nsolution faster, while RL may eventually converge to a better\nsolution. Therefore, we ideally want a switching mechanism,\nthat transitions from EC action selection to RL action selec-\ntion. We need to decide both when and how to switch between\nthe two different ‘memories’.\nRegarding the ‘when’, we propose to decide which memory\nto use for every episode. This way, we ensure that action\nselection within the episode stays consistent. To determine\nwhich memory we will use in a particular episode, we need\nto deﬁne a probability pec that speciﬁes the probability that\nwe will use EC in the next episode. Obviously, we then select\n2M-RL with probability 1 −pec. Since we favor the use of\n2M-EC at the beginning and 2M-RL near the end, we want\nto gradually decay pec from a high value to a lower value\naccording to the equation:\npec ←pe + (ps −pe) · e−i/τ\n(5)\nwhere ps and pe are starting value and end value of pec,\ni is the number of steps the agent takes so far and τ is the\ntemperature that controls the speed of decay. Smaller τ decays\npec faster while lager τ decays pec slower. In the ablation\nexperiments, we also experiment with different scheduling\nmechanisms.\nDuring evaluation, we exploit knowledge stored in both\nmemories in a greedy manner. However, we still need to decide\nwhich memory to use during evaluation. The scores 2M-RL\nand 2M-EC obtained during training are used as metrics to\nevaluate their respective performances, and the memory with\nthe higher recent score is selected for action selection during\nevaluation. More speciﬁcally, we keep track off cumulative\nrewards of both memories during training, and the score (srl\nand sec for 2M-RL and 2M-EC, respectively) is deﬁned as\nthe average cumulative reward over the last n episodes per\nmemory method. We ﬁx n = 50 in this work. Thus, the 2M\nagent will choose among two memories using Eq. 6.\n2M ←\n(\n2M-RL\nif srl ≥sec,\n2M-EC\notherwise\n(6)\nC. Learning\nTo foster mutual improvement of the two memories, the\ncollected data is shared between 2M-EC and 2M-RL. Regard-\nless of which memory the data is collected by, it is used\nto update 2M-EC according to Eq. 2, ensuring that 2M-EC\nis always up to date. On the other hand, since we use off-\npolicy reinforcement learning methods, data collected by 2M-\nEC can also be used to update 2M-RL. This is implemented\nby maintaining a replay buffer, and all data collected during\nthe training will be pushed into it. 2M-RL is then trained\n(Eq. 1) every x timesteps (we use x = 10) by sampling\nminibatches from the buffer. It should be noted that the value\nof pec indirectly determines the amount of data in the replay\nbuffer originating from 2M-EC, and thereby the proportion of\nsuch data used for training 2M-RL.\nV. EXPERIMENTS\nWe ﬁrst present experimental results on a simple WindyGrid\nenvironment to demonstrate the efﬁciency of the proposed\n2M agent. Subsequently, we perform extensive experiments\non ﬁve MinAtar [22] games, namely Breakout, SpaceIn-\nvaders, Asterix, Seaquest, Freeway, as illustrated in Fig. 3.\nThese games present diverse challenges, such as exploration\n(Seaquest and Freeway), classical control tasks (Breakout and\nSpaceInvaders), and so on. MinAtar is a simpliﬁed version of\nAtari Learning Environment [11], which simpliﬁes represen-\ntations of games while still capturing general mechanics. This\nallows the agent to focus on behavioural challenges rather than\nrepresentation learning. Finally, an ablation study is conducted\nto investigate the crucial choices of the proposed method.\nA. Proof of Concept Under Tabular RL Setting\nThe 2M agent integrates tabular 1-step Q-learning with\nepisodic control to solve a WindyGrid task (shown in Fig. 4).\nThe WindyGrid instance we use here is 7 × 10 large, with\na stochastic wind in the 7th row and a trap where the agent\nwill get a large penalty. The agent needs to navigate from\nthe initial state (3, 0) to the terminal state (3, 7). The results\npresented in Fig. 5 demonstrate the performance of various\nFig. 4. An illustration of the WindyGrid environment. The agent (smile face)\nneeds to navigate from the starting point to the given goal (star). There is the\nstochastic wind that will (black column with up arrows) blow the agent up\nand the agent gets a penalty when it reaches the trap (red cell).\nFig. 5. Results on WindyGrid under tabular settings. Left: Evaluation returns\nof different agents, EC learns very fast but converges to local optima while\nRL learns slowly but converges to global optima. 2M learns faster (compare to\nRL) at the beginning and has better asymptotic performance (compare to EC).\nColors in the background indicate memories used during evaluation, orange\nand grey represent the use of 2M-EC and 2M-RL. Right: Learned Q-values\nsummed across the entire state-action space, 2M-RL learns Q-values faster\nthan pure RL which is trained using monotonic RL data.\nagents. The left ﬁgure shows returns the agent obtains during\nthe evaluation, while the right ﬁgure shows learned Q-values\nsummed across the whole state-action space. The orange line\nrepresents the EC agent, which achieves quick learning and\ndecent performance from the start but only has sub-optimal\nasymptotic performance. In contrast, the grey line corresponds\nto the RL agent, which initially performs poorly but gradually\nimproves and eventually converges to the optimal solution. The\nblue line corresponds to the 2M agent, which learns faster at\nthe beginning compared to RL and achieves better asymptotic\nperformance compared to EC. The background colors indicate\nthe use of different memories by the 2M agent during the\nevaluation, with the agent using 2M-EC (orange) for evaluation\nat the beginning and then switching to 2M-RL (grey) after\napproximately 25k steps.\nSince the memories 2M-RL and 2M-EC share data, and 2M-\nRL is trained on data collected by both of them, we investigate\nwhether this approach has a positive impact compared to train-\ning solely on data collected by pure RL. As Varun et al. [21]\ndemonstrated theoretically and experimentally, learning from\ndata correlated to the optimal policy will results in lower\ncomplexity (which is an intuitive result). If we assume the\nEC data is at least somewhat correlated to the optimal policy,\nwe expect that use of EC data in RL updates will lead to\nfaster learning. We experimentally test this idea in Fig. 5. The\nright panel of Fig. 5 shows that using mixed data to train\nthe RL agent (2M-RL) can indeed result in faster learning of\nstate-action values compared to using data solely collected by\nRL. This suggest EC data may actually improve RL sample\nefﬁciency.\nB. Results on MinAtar Games\nNext, we perform experiments on ﬁve MinAtar games,\nwhich also vary in the amount of stochasticity. We optimize\nhyper-parameters over the values shown in Tab. I, where the\nsettings used for the ﬁnal results are highlighted in bold.\nTABLE I\nTUNED HYPER-PARAMETERS, WHERE THE ONES WE USE FOR THE FINAL\nEXPERIMENTS ARE HIGHLIGHTED IN BOLD. ps IS THE STARTING VALUE\nFROM EQ.5\nϵ (for exploration)\n0.1, 0.9\nk (for k-NN in EC)\n1, 3, 10\nps →pe (for switching)\n0.9 →0.1, 0.1 →0.1, 0.1 →0.9\nlearning rate\n0.001, 0.0001\nIn the results presented in Fig. 6, the top rows depict the\nvarious memories the 2M agent selected during the evaluation,\nwhereas the bottom rows display the returns agents obtain.\nThe performance of EC (orange lines) and DQN (grey lines)\nis consistent with the ﬁndings observed in the toy example,\nwhere EC learns quickly at the beginning but converges\nto sub-optimal solutions while DQN learns slowly but has\nbetter asymptotic performance. 2M agents (blue lines) perform\nthe best (comparably to EC) at the beginning on all ﬁve\ngames, and in most games, they also exhibit better asymptotic\nperformance (or at least equally good performance) compared\nto other baselines (except maybe for Asterix). In Asterix, the\n2M agent underperforms EMDQN (green lines), but is still\nbetter than DQN and EC.\nDuring the evaluation, most 2M agents demonstrate a pref-\nerence for using 2M-EC initially and then switching to 2M-\nRL over time. In Freeway, the agent continually switches\nbetween 2M-EC and 2M-RL, suggesting a mutually beneﬁcial\nrelationship between these two memories. However, in Break-\nout, the agent consistently favors to use 2M-EC. This may\nbe attributed to the fact that stochastic dynamic transitions\nhave a less pronounced impact on performance in this game,\nwith the most critical time step being the one at which the\npaddle must bounce the ball. We hypothesize 2M-RL can\nhelp 2M-EC escape local optima and then 2M-EC can rapidly\nlearn improved solutions and continue to progress. To test this\nhypothesis, we need to check whether a 2M agent that does\nnot share collected data between both memories indeed 1) has\nworse performance, and 2) prefers RL in the long run, instead\nof getting to a more optimal EC solution.\nFig. 7 shows the performance of an 2M agent with data\nsharing enabled and disabled. With data sharing (2Mw/DaS),\nthe agent mostly prefers to use EC during evaluation (top-\nleft ﬁgure), as expected from Fig. 6. When we deactivate\ndata sharing (2Mw/oDaS, two memories are only trained\nusing data collected by the corresponding memory separately),\nthe 2M agent only prefers 2M-EC at the beginning and\nthen sticks to 2M-RL (bottom-left graph of the ﬁgure). The\nperformance graph on the right of the ﬁgure conﬁrms these\nresults. Without data sharing, 2M does not reach the same\nperformance (blue line stays above the orange line). The\ncircles show the performance of 2M-EC at the end of training\nfor both methods. Without data sharing, 2M-EC (the orange\ncircle in Fig. 7) converges to a sub-optimal solution. With\ndata sharing enabled, 2M-EC (the blue circle in Fig. 7) has a\nway higher performance. This observation provides evidence\nto support the aforementioned notion that 2M-RL and 2M-EC\ncomplement eachother.\nC. Ablation Study\nIn this section, we conduct two groups of ablation experi-\nments to study the design choices in our work. First, we would\nlike to investigate the impacts of data sharing. Deactivating\ndata sharing (2Mw/oDS), resulting in 2M-RL being solely\ntrained on data collected by 2M-RL and 2M-EC being solely\ntrained on data collected by 2M-EC. This transforms our pro-\nposed method becomes a ‘scheduler’ that schedules the train-\ning between two distinct models and uses the better one for\nevaluation. Second, we aim to study different ways of schedul-\ning pec. Speciﬁcally, we examine three different scheduling\napproaches: decayed scheduling (2Mw/DS), constant schedul-\ning (2Mw/CS) and increased scheduling (2Mw/IS).\nIntuitively, data sharing can be helpful since each memory\nwill get different data from another memory, hopefully, they\ncould also learn from each other. It should result in a better\nperformance compared to only training the agent on its own\ncollected data. In fact, such sharing has different impacts on\ndifferent games, shown in Fig. 8. In Asterix, data sharing\nimproves 2M agent’s performance, and harms the performance\nof the agent in Seaquest. To understand the reasons for these\nopposite impacts, we separately track the performance of 2M-\nEC and 2M-RL during the training, and ﬁnal performance\nare represented by circles and triangles in Fig. 8 and larger\nsize represents more use of 2M-EC (larger ps) during the\ntraining. In Asterix, data sharing pulls down the performance\nof 2M-RL (blue triangles are always below orange ones), and\ntraining 2M-RL on more data collected by 2M-EC leads to even\nworse performance (the large blue triangle is way below the\nlarge orange one), indicating that data collected by 2M-EC is\nactually harmful for training 2M-RL in this game. Conversely,\nin Seaquest, data sharing improves the performance of 2M-\nEC (blue circles are always above orange ones), which again\nindicates that 2M-RL can help 2M-EC escape from local\noptima. However, overusing data collected by 2M-EC to train\n2M-RL also leads to worse performance (the large blue triangle\nis way below the large orange one). All in all, 2M-RL should\nnot use too much data collected by 2M-EC. Although we\nshow that such training is helpful to learn the optimal state-\naction values when collected data is correlated to the optimal\npolicy, this assumption is not always satisﬁed. Meanwhile,\n2M-RL can help 2M-EC escape from local optima but not\nalways. We presume it helps when stochastic transitions have\nFig. 6. Results on MinAtar games: Breakout, SpaceInvaders, Asterix, Seaquest, Freeway. The top row shows the relevant memory that the 2M agent chooses for\nevaluation: orange represents 2M-EC and grey represents 2M-RL. The bottom row shows the returns obtained during training (running independent evaluation\nepsisodes). The 2M agent either outperforms or is on par with all baseline comparisons (EC, DQN and EMDQN). EC generally learns fast, but then reaches\na plateau. 2M learns equally fast initially, but then adopts the better long-term performance of RL.\nFig. 7.\nLeft: Switching schedule of 2M agent with data sharing (top) and\nwithout data sharing (bottom) during evaluation. Right: Returns two different\n2M agents are able to get in evaluation. The ﬁnal performance of 2M-EC is\nrepresented by coloured circles. We see that with data sharing, EC reaches a\nreturn of 6, while without data sharing, EC only manages to reach a return\nof 3.\nfewer negative impacts, then 2M-EC is able to catch improved\nsolutions provided by 2M-RL. For example, in Asterix, there\nare many enemies and the agent dies immediately when it\ntouches enemies, meaning the agent will more likely die if\na single wrong (sub-optimal) action is made. Therefore, once\n2M-EC discovers a sub-optimal trajectory with a very high\nreturn (luckily manage to survive for a long time, like ending\nup with s6 in the motivating example in Fig. 2) with a tiny\nprobability, it will stick to it. Then it is difﬁcult for 2M-EC\nto escape from local optima even though improved solutions\nare provided. We leave more systematic investigations on this\nphenomenon for future work.\nNext we examine how different scheduling mechanisms\naffect performance. The 2M agent with decayed scheduling\n(2Mw/DS) will initially give a higher preference to use 2M-EC\nfor data collection during the training, then gradually shifts to-\nwards 2M-RL. On the contrary, increased scheduling (2Mw/IS)\nwill start with a strong preference for using 2M-RL, then\nFig. 8. Performance of 2M agents with and without data sharing on Asterix\n(left) and Seaquest (right). Results are averaged over 2 different settings, one\nis with larger pec and one is with smaller pec. Data sharing has different\nimpacts on these two games. Circles represent the ﬁnal performance of 2M-\nEC while triangles represent the performance of 2M-RL. The larger size means\nthe larger value of pec during the training.\ngradually switch to 2M-EC. Constant scheduling maintains\na constant preference for 2M-EC and 2M-RL throughout the\ntraining process. Given that 2M agents with 2Mw/DS perform\nthe best or one of the best on all games, we only present the\nperformance on Seaquest as a representative in Fig. 9. The\nresults demonstrate that the agent with 2Mw/DS outperforms\nthe other two scheduling mechanisms explicitly.\nOur experimental results demonstrate that the 2M agent\nsurpasses its constituent components, i.e. a pure reinforce-\nment learning approach (Q-learning in tabular settings and\nDQN [23] in deep RL settings) and a pure episodic mem-\nory approach (MFEC [6] in both tabular and deep RL\nsettings). Furthermore, our proposed method exhibits better\nperformance than a state-of-the-art memory-augmented rein-\nforcement learning method EMDQN [8]. Since the proposed\nframework allows for the integration of various pure EC and\nRL methods, we only compare the performance of the 2M\nFig. 9. Performance of 2M agents with different scheduling mechanisms on\nSeaquest. The one with the decayed scheduling mechanism works the best\nwhile other two mechanisms have similar performance.\nagent with those methods that are integrated into it in this\nwork. Therefore, we do not compare our results with other\npure episodic memory and reinforcement learning approaches.\nLastly, we conduct ablation studies to investigate the impact of\ntwo essential design choices: the utilization of data sharing for\nthe mutual improvement of the memories, and the scheduling\nof pec for switching between the two memories.\nVI. CONCLUSION AND FUTURE WORK\nIn this work, we proposed a novel approach, termed Two-\nMemory (2M) reinforcement learning agent, which inte-\ngrates two distinct learning methods, namely non-parametric\nepisodic memory and (parametric) reinforcement learning.\nThis approach capitalizes on the advantages of both methods\nby leveraging the episodic memory’s rapid starting and the re-\ninforcement learning’s superior generalization and optimality\nto create an agent that achieves higher data efﬁciency than\nthe individual methods. Experimental results show that 2M\nmatches or outperforms both DQN and EMDQN on a range\nof MinAtar games. In addition, we show that these two distinct\nmemory modules may complement eachother, leading to even\nbetter ﬁnal performance.\nFor future work, it would be interesting to automatically\ndetermine when data sharing is useful, for example based\non the discrepancy between both memories. Another clear\ndirection to improve the 2M agent could be an adaptive\nscheduling mechanism to switch between 2M-EC and 2M-\nRL, instead of the hand-designed decay schedules used in\nthis work. Moreover, combining stronger episodic memory\nmethods (such as NEC) with off-policy reinforcement learning\nmethods could lead to further improvements in performance.\nIn our work, we uniformly replay data from the replay buffer\nto train the 2M-RL component, but a more sophisticated replay\nstrategy, such as prioritized experience replay (PER), may\nfurther enhance performance. Overall, our proposed approach\nprovides a general framework for combining two funda-\nmentally different approaches to sequential decision-making,\ncombining their respective strengths.\nREFERENCES\n[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\nstra, and M. Riedmiller, “Playing atari with deep reinforcement learn-\ning,” arXiv preprint arXiv:1312.5602, 2013.\n[2] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,\nA. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering\nthe game of go without human knowledge,” nature, vol. 550, no. 7676,\npp. 354–359, 2017.\n[3] P. R. Wurman, S. Barrett, K. Kawamoto, J. MacGlashan, K. Subrama-\nnian, T. J. Walsh, R. Capobianco, A. Devlic, F. Eckert, F. Fuchs et al.,\n“Outracing champion gran turismo drivers with deep reinforcement\nlearning,” Nature, vol. 602, no. 7896, pp. 223–228, 2022.\n[4] J. Degrave, F. Felici, J. Buchli, M. Neunert, B. Tracey, F. Carpanese,\nT. Ewalds, R. Hafner, A. Abdolmaleki, D. de Las Casas et al., “Magnetic\ncontrol of tokamak plasmas through deep reinforcement learning,”\nNature, vol. 602, no. 7897, pp. 414–419, 2022.\n[5] A. Fawzi, M. Balog, A. Huang, T. Hubert, B. Romera-Paredes,\nM. Barekatain, A. Novikov, F. J. R Ruiz, J. Schrittwieser, G. Swirszcz\net al., “Discovering faster matrix multiplication algorithms with rein-\nforcement learning,” Nature, vol. 610, no. 7930, pp. 47–53, 2022.\n[6] C. Blundell, B. Uria, A. Pritzel, Y. Li, A. Ruderman, J. Z. Leibo,\nJ. Rae, D. Wierstra, and D. Hassabis, “Model-free episodic control,”\narXiv preprint arXiv:1606.04460, 2016.\n[7] A. Pritzel, B. Uria, S. Srinivasan, A. P. Badia, O. Vinyals, D. Hassabis,\nD. Wierstra, and C. Blundell, “Neural episodic control,” in International\nConference on Machine Learning.\nPMLR, 2017, pp. 2827–2836.\n[8] Z. Lin, T. Zhao, G. Yang, and L. Zhang, “Episodic memory deep q-\nnetworks,” arXiv preprint arXiv:1805.07603, 2018.\n[9] I. Kuznetsov and A. Filchenkov, “Solving continuous control with\nepisodic memory,” arXiv preprint arXiv:2106.08832, 2021.\n[10] H. Hu, J. Ye, G. Zhu, Z. Ren, and C. Zhang, “Generalizable\nepisodic memory for deep reinforcement learning,” arXiv preprint\narXiv:2103.06469, 2021.\n[11] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, “The arcade\nlearning environment: An evaluation platform for general agents,” Jour-\nnal of Artiﬁcial Intelligence Research, vol. 47, pp. 253–279, 2013.\n[12] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-\nforcement learning,” in International conference on machine learning.\nPMLR, 2016, pp. 1928–1937.\n[13] R. A. Poldrack and M. G. Packard, “Competition among multiple\nmemory systems: converging evidence from animal and human brain\nstudies,” Neuropsychologia, vol. 41, no. 3, pp. 245–251, 2003.\n[14] B. H. Schott, R. N. Henson, A. Richardson-Klavehn, C. Becker,\nV. Thoma, H.-J. Heinze, and E. D¨uzel, “Redeﬁning implicit and explicit\nmemory: The functional neuroanatomy of priming, remembering, and\ncontrol of retrieval,” Proceedings of the National Academy of Sciences,\nvol. 102, no. 4, pp. 1257–1262, 2005.\n[15] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[16] Z. Yang, T. M. Moerland, M. Preuss, and A. Plaat, “Continuous episodic\ncontrol,” arXiv preprint arXiv:2211.15183, 2022.\n[17] S. Y. Lee, C. Sungik, and S.-Y. Chung, “Sample-efﬁcient deep rein-\nforcement learning via episodic backward update,” Advances in Neural\nInformation Processing Systems, vol. 32, 2019.\n[18] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience\nreplay,” arXiv preprint arXiv:1511.05952, 2015.\n[19] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,\nB. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba, “Hindsight\nexperience replay,” Advances in neural information processing systems,\nvol. 30, 2017.\n[20] M. Fang, T. Zhou, Y. Du, L. Han, and Z. Zhang, “Curriculum-guided\nhindsight experience replay,” Advances in neural information processing\nsystems, vol. 32, 2019.\n[21] V. Kompella, T. Walsh, S. Barrett, P. Wurman, and P. Stone, “Event\ntables for efﬁcient experience replay,” arXiv preprint arXiv:2211.00576,\n2022.\n[22] K. Young and T. Tian, “Minatar: An atari-inspired testbed for thorough\nand reproducible reinforcement learning experiments,” arXiv preprint\narXiv:1903.03176, 2019.\n[23] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,\nD. Wierstra, and M. Riedmiller, “Playing atari with deep reinforcement\nlearning,” 2013, cite arxiv:1312.5602Comment: NIPS Deep Learning\nWorkshop 2013. [Online]. Available: http://arxiv.org/abs/1312.5602\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-04-20",
  "updated": "2023-04-23"
}