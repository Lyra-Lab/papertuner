{
  "id": "http://arxiv.org/abs/2304.12602v2",
  "title": "Is deep learning a useful tool for the pure mathematician?",
  "authors": [
    "Geordie Williamson"
  ],
  "abstract": "A personal and informal account of what a pure mathematician might expect\nwhen using tools from deep learning in their research.",
  "text": "IS DEEP LEARNING A USEFUL TOOL FOR THE PURE\nMATHEMATICIAN?\nGEORDIE WILLIAMSON\nAbstract. A personal and informal account of what a pure mathematician\nmight expect when using tools from deep learning in their research.\n1. Introduction\nOver the last decade, deep learning has found countless applications throughout\nindustry and science. However, its impact on pure mathematics has been modest.\nThis is perhaps surprising, as some of the tasks at which deep learning excels—like\nplaying the board-game Go or finding patterns in complicated structures—appear\nto present similar difficulties to problems encountered in research mathematics. On\nthe other hand, the ability to reason—probably the single most important defining\ncharacteristic of mathematical enquiry—remains a central unsolved problem in ar-\ntificial intelligence. Thus, mathematics can be seen as an important litmus test as\nto what modern artificial intelligence can and cannot do.\nThere is great potential for interaction between mathematics and machine learn-\ning.1 However, there is also a lot of hype, and it is easy for the mathematician\nto be put off. In my experience, it remains hard to use deep learning to aid my\nmathematical research. However it is possible. One also has the sense that the\npotential, once the right tools have been uncovered, is significant.\nThis is a very informal survey of what a working mathematician might expect\nwhen using the tools of deep learning on mathematics problems. I outline some\nof the beautiful ideas behind deep learning. I also give some practical hints for\nusing these tools. I finish with some examples where deep learning has been used\nproductively in pure mathematics research. (I hope it goes without saying that the\nimpact of deep learning on applied mathematics has been enormous.)\nFinally, in my experience, the more one uses the tools of deep learning, the\nmore difficult it becomes not to ask oneself foundational questions about why they\nwork. This raises an entirely different set of questions. Although fascinating, the\nmathematical theory of deep learning is not the focus here.\nRemark 1.1. The elephant in the room in any discussion today of deep learning is\nthe recent success of ChatGPT and other large language models. The internet is\nfull of examples of ChatGPT doing both very well and very poorly on reasoning\nand mathematics problems.\nIt seems likely that large language models will be\nable to interact well with proof assistants in the near future (see e.g. [HRW`21,\nJWZ`23]). It is also likely that a greater role will be played in mathematics research\n1In 1948, Turing [Tur48, §6] identifies games, mathematics, cryptography and language transla-\ntion and acquisition as five “suitable branches of thought” in which experimentation with machine\nintelligence might be fruitful.\n1\narXiv:2304.12602v2  [math.RT]  26 May 2023\n2\nby very large models, possibly with emergent capabilities (“foundation models” in\nthe language of the excellent [BHA`21]). The impacts of such developments on\nmathematics are difficult to predict. In this article I will ignore these questions\nentirely. Thus I will restrict myself to situations in which deep learning can be used\nby mathematicians without access to these large models.\n1.1. About the author. I am a pure mathematician, working mostly in geometric represen-\ntation theory and related fields. I began an ongoing collaboration with DeepMind in 2020,\non possible interactions of machine learning and mathematics, and have been fascinated\nby the subject ever since.2\n2. What is a neural network?\nArtificial neural networks emulate the biological neural networks present in the\nbrains of humans and other animals. Typically this emulation takes place on a\ncomputer. The idea of doing so is very natural. See [MP43, Tur48] for remarkable\nearly accounts.\nA cartoon picture of a neuron imagines it as a unit with several inputs and a\nsingle output, which may then be connected to other neurons:\n‚\nNeurons “fire” by emitting electrical charge along their axon. We may encode the\ncharges arriving along each node by a real number, in which case the charge emitted\nby a neuron is given by\n‚\nx1\nx1\nx2\nx2\nx3\nx3\nx4\nx4\nx5\nx5\nz\nz “ f\n´ÿ\nxi\n¯\nwhere f is a (typically monotone increasing and non-linear) activation function.\nSoon we will assume that our activation function is fixed3, however at this level of\nprecision the reader is encouraged to imagine something like fpxq “ tanhpxq. The\nactivation function is meant to model the non-linear response curves of neurons to\n2My understanding of this landscape has benefitted enormously from discussions with Charles\nBlundell, Lars Buesing, Alex Davies, Joel Gibson, Georg Gottwald, Camilo Libedinsky, S´ebastien\nRacaniere, Carlos Simpson, Grzegorz Swirszcz, Petar Veliˇckovi´c, Adam Wagner, Th´eophane We-\nber and Greg Yang.\nWithout their help this journey would have been slower and much more\npainful. This article is based on a lecture given at the Fields Institute Symposium on the future\nof mathematical research, which was held at the instigation of Akshay Venkatesh. I would like to\nthank the organizers for organising a wonderful and thought-provoking meeting, and in particular\nMaia Fraser for very useful feedback on an earlier version of this article.\n3and equal to “ReLU”: fpxq “ maxp0, xq\n3\nstimuli. For example, some neurons may not fire until a certain charge is reached\nat their source.4\nAnother important feature of neurons is that their firing may be excitatory or\ninhibitory of downstream neurons to varying degrees. In order to account for this,\none allows modification of the input charges via weights (the wi):\n(1)\n‚\nx1\nx1\nx2\nx2\nx3\nx3\nx4\nx4\nx5\nx5\nz\nz “ f\n´ÿ\nwixi\n¯\nThus positive and negative weights correspond to excitatory and inhibitory connec-\ntions respectively.\nHaving settled on a crude mathematical model of a single neuron, we may then\nassemble them together to form a neural network:\n‚\n‚\n‚\n‚\n‚\n‚\n‚\n‚\n‚\n‚\n‚\nImplicit in this picture is the assignment of a weight to each edge. Thus our neural\nnetwork yields a function which takes real valued inputs (5 in the above picture),\nand outputs real values (2 above), via repeated application of (1) at each node.\nThis is a good picture for the layperson to have in mind. It is useful to visualize\nthe complex interconnectedness present in artificial neural networks, as well as the\nlocality of the computation taking place. However for the mathematician, one can\nexplain things a little differently. The configuration\n‚\n‚\n‚\n‚\n‚\n‚\n‚\n‚\n‚\n4In biological neural nets there is typically large variation in the responses of neurons to stimuli,\ndepending on where they are in the brain (see e.g. [HW62]). This is one of the many features of\nbiological neural nets that is usually ignored when building artificial neural networks.\n4\nis simply a complicated way of drawing a 5 ˆ 4 matrix. In other words, we can\nrewrite our neural network above economically in the form\nR5\nW1\nÝÑ R4\nf\nÝÑ R4\nW2\nÝÑ R2\nf\nÝÑ R2\nwhere the Wi are linear maps determined by matrices of weights, and f is shorthand\nfor the coordinatewise application of our activation function f.\nFor the purposes of this article, a vanilla neural network 5 is a gadget of the form\nRd1\nA1\nÝÑ Rd2\nf\nÝÑ Rd2\nA2\nÝÑ Rd3\nf\nÝÑ Rd3\nA3\nÝÑ . . .\nf\nÝÑ Rdℓ´1 Aℓ´1\nÝÑ Rdℓ\nwhere Ai are affine linear maps. We refer to Rd1, Rd2, . . . , Rdℓas the layers of the\nnetwork. In order to simplify the discussion, we always assume that our activation\nfunction f is given by ReLU (the “rectified linear unit”), that is\nf\n´ÿ\nλiei\n¯\n“\nÿ\nmaxpλi, 0qei\nwhere the ei are standard basis vectors.\nRemark 2.1. We make the following remarks:\n(1) The attentive reader might have observed a sleight of hand above, where we\nsuddenly allowed affine linear maps in our definition of a vanilla neural net.\nThis can be justified as follows: In biological neural nets both the charge\ntriggering a neuron to fire, as well as the charge emitted, varies across the\nneural network. This suggests that each activation function should have\nparameters, i.e.\nbe given by x ÞÑ fpx ` aq ` b for varying a, b P R at\neach node. Things just got a lot more complicated! Affine linear maps\ncircumvent this issue: by adding the possibility of affine linear maps one\ngets the same degree of expressivity, with a much simpler setup.\n(2) We only consider ReLU activation functions below. This one of the standard\nchoices, and provides a useful simplification. However one shouldn’t forget\nthat it is possible to vary activation functions.\n(3) We have tried to motivate the above discussion of neural networks as some\nimitation of neural activity. It is important to keep in mind that this is a\nvery loose metaphor at best. However I do find it useful in understanding\nand motivating basic concepts. For an excellent account along these lines\nby an excellent mathematician, the reader is referred to [Mum20].\n(4) The alert reader will notice that we have implicitly assumed above that our\ngraphs representing neural networks do not have any cycles or loops. This\nis again a simplification, and it is desirable in certain situations (e.g. in\nrecurrent neural networks) to allow loops.\nVanilla neural networks are often referred to as fully-connected because each\nneuron is connected to every neuron in the next layer. This is almost opposite to\nthe situation encountered in the brain, where remarkably sparse neural networks\nare found.\nThe connection pattern of neurons is referred to the architecture of\nthe neural network. As well as vanilla neural networks, important artificial neural\nnetwork architectures include convolutional neural networks, graph neural networks\nand transformers. Constraints of length prohibit us from discussing these architec-\ntures in any depth.\n5One often encounters the term “Multi Layer Perceptron (MLP)” in the literature.\n5\nRemark 2.2. More generally, nowadays the term “neural network” is often used to\nrefer to any program in which the output depends in a smooth way on the input\n(and thus the program can be updated via some form of gradient descent). We\nignore this extra generality here.\n3. Motivation for deep learning\nIn order to understand deep learning, it is useful to keep in mind the tasks\nat which it first excelled.\nOne of the most important such examples is image\nclassification. For example, we might want to classify hand-written digits:\nÞÑ 6\nÞÑ 2\nHere each digit is given as (say) a 28 ˆ 28 matrix of grayscale values between 0\nand 255. This is a task which is effortless for us, but is traditionally difficult for\ncomputers.\nWe can imagine that our brain contains a function which sees a hand-written\ndigit and produces a probability distribution on t0, 1, . . . , 9u, i.e. “what digit we\nthink it is”.6 We might attempt to imitate this function with a neural network.\nLet us consider a simpler problem in which we try to decide whether a hand-\nwritten digit is a 6 or not:\nÞÑ “yes”\nÞÑ “no”\nWe assume that we have “training data” consisting of images labelled by “6” or “not\n6”. As a first attempt we might consider a network having a single linear layer:\nR28ˆ28\nA\nÝÑ R\n1{p1`e´xq\nÝÝÝÝÝÝÝÝÑ\nR.\nHere A is affine linear, and the second function (the “logistic function”7) is a conve-\nnient way of converting an arbitrary real number into a probability. Thus, positive\nvalues of A mean that we think our image is a 6, and negative values of A mean we\nthink it is not.\nWe will be successful if we can find a hyperplane separating all vectors corre-\nsponding to 6’s (red dots) from those that do not represent 6’s (blue dots):\n‚\n‚\n‚\n‚\n‚\n‚\n‚\n‚\nvectors in\nR28ˆ28\n`\n´\nOf course, it may not be possible to find such a hyperplane.\nAlso, even if we\nfind a hyperplane separating red and blue dots, it is not clear that such a rule\nwould generalize, to correctly predict whether an unseen image (i.e. image not in\nour training data) represents a 6 or not. Remarkably, techniques of this form (for\nexample logistic regression, Support Vector Machines (SVMs), . . . ) do work in many\n6I can convince myself that my brain produces a probability distribution and not a yes/no\nanswer by recalling my efforts to decipher my grandmother’s letters when I was a child.\n7a.k.a. sigmoid in the machine learning literature\n6\nsimple learning scenarios. Given training data (e.g. a large set of vectors labelled\nwith “yes” and “no”) the optimal separating hyperplane may be found easily.8\n4. What is deep learning?\nIn many classification problems the classes are not linearly separable:\nLinear methods such as SVM can nevertheless still be used in many cases, after\napplication of a suitable feature map, namely a (non-linear) transformation whose\napplication on the data makes linear separation of classes possible:\nnon-linear\nÝÑ\nIt is on such more difficult learning tasks that deep learning can come into its own.\nThe idea is that successive layers of the neural net transform the data gradually,\neventually leading to an easier learning problem.9\nIn the standard setting of supervised learning, we assume the existence of a\nfunction\nϕ : Rn Ñ Rm\nand know a (usually large) number of its values. The task is to find a reasonable\napproximation of ϕ, given these known values. (The reader should keep in mind the\nmotivating problem of the previous section, where one wants to learn a function\nϕ : R28ˆ28 Ñ R10\ngiving the probabilities that a certain 28 ˆ 28-pixel grayscale image represents one\nof the 10 digits 0, 1, . . . , 9.)\nWe fix a network architecture, which in our simple setting of a vanilla neural\nnet, means that we fix the number of layers ℓand layer dimensions n2, . . . , nℓ´1.\nWe then build a neural net (see §2) which serves as our function approximator:\n(2)\nϕ« : Rn\nA1\nÝÑ Rn2\nf\nÝÑ Rn2\nA2\nÝÑ Rn\n3\nf\nÝÑ Rn3\nA3\nÝÑ . . .\nf\nÝÑ Rdn´1 Aℓ´1\nÝÑ Rm\n8For a striking mathematical example of support vector machines see [HK22], where SVMs are\ntrained to distinguish simple and non-simple finite groups, by inspection of their multiplication\ntable.\n9This idea seems to have been present in the machine learning literature for decades, see e.g.\n[LBBH98]. It is well explained in [GBC16, §6]. For illustrations of this as well as the connection\nto fundamental questions in topology, see the work of Olah [Ola14].\n7\nTo begin with, the affine linear maps Ai are initialised via some (usually random)\ninitialization scheme, and hence the function ϕ« output by our neural network will\nbe random and have no relation to our target function ϕ. We then measure the\ndistance between our function ϕ« and ϕ via some loss function L. (For example, L\nmight be the mean squared distance between the values of ϕ and ϕ«.10) A crucial\nassumption is that this loss function is differentiable in terms of the weights of\nour neural network. Finally, we perform gradient descent with respect to the loss\nfunction, in order to update the parameters in (2) to (hopefully) better and better\napproximate ϕ.\nIn order to get an intuitive picture of what is happening during training, let\nus assume that m “ 1 (so we are trying to learn a scalar function), and that our\nactivation functions are ReLU. Thus ϕ« is the composition of affine linear and\npiecewise linear functions, and hence is piecewise linear. As with any piecewise\nlinear function, we obtain a decomposition of Rn into polytopal regions\n(3)\nsuch that ϕ« is affine linear on each region.\nAs training progresses, the affine\nlinear functions move in a way similar to the learning of a line of best fit, but more\ncomplex since the regions we are dealing with may also move, disappear or spawn\nnew regions.\nRemark 4.1. For an excellent interactive animation of a simple neural network\nlearning a classification task, the reader is urged to experiment with the Tensor\nFlow Playground [SC]. Karpathy’s convolutional neural network demo [Kar] is also\nillustrative to play with.\nRemark 4.2. Some remarks:\n(1) Typically, one splits the known values of ϕ into two disjoint sets, consisting\nof training data and validation data. Steps of gradient descent are only\nperformed using the training data and the validation data allows us to pe-\nriodically check whether our model is also making reasonable predictions at\npoints not present in the training data (“validation error”). It is sometimes\nuseful to have an additional set of test data, completely unseen during train-\ning, which one can use to compute the performance of the trained model\n(“test error”).\n10There are many subtleties here, and a good choice of loss function is one of them.\nIn\nmy limited experience, neural networks do a lot better learning probability distributions than\ngeneral functions. When learning probability distributions, cross entropy [GBC16, §3.13] is the\nloss function of choice.\n8\n(2) In most applications of machine learning, the training data is enormous and\nfeeding it all through the neural network (2) in order to compute the loss\nfunction is unduly expensive. Thus one usually employs stochastic gradient\ndescent: at every step the gradient of the loss function is computed using\na small random subset (a “minibatch”) of the training data.\n(3) Using a model with a small number of parameters (as traditionally done in\nstatistics, and some ML methods) has advantages for interpretability and\ncomputation. It can also help avoid overfitting, where the chosen predictor\nmay fit the data set so closely it ends up fitting noise and fails to ade-\nquately capture the underlying data generating process. A simple example\nof a model with few parameters in a line of best fit. Deep learning is differ-\nent, in that often there are enough parameters to allow overfitting. What\nis surprising is that often neural nets generalize well (i.e.\ndon’t overfit)\neven though they could in principle (this is an enormous subject, see e.g.\n[BHMM19]).\n5. Simple examples from pure mathematics\nIt is important to keep in mind that the main motivating applications for deep\nlearning research are very different from those arising in pure mathematics. For\nexample, the “recognize a hand-written digit” function considered in the previous\ntwo sections is rather different to the Riemann zeta function11!\nThis means that the mathematician wanting to use machine learning should\nkeep in mind that they are using tools designed for a very different purpose. The\nhype that “neural nets can learn anything” also doesn’t help. The following rules\nof thumb are useful to keep in mind when selecting a problem for deep learning:\n(1) Noise stable. Functions involved in image and speech recognition motivated\nmuch research in machine learning. These functions typically have very\nhigh-dimensional input (e.g.\nR100ˆ100 for a square 100 ˆ 100 grayscale\nimage) and are noise stable.\nFor example, we can usually recognise an\nimage or understand speech after the introduction of a lot of noise. Neural\nnets typically do poorly on functions which are very noise-sensitive.12\n(2) High dimensional. If one thinks of a neural network as a function approx-\nimator, it is a function approximator that comes into its own on high-\ndimensional input. These are the settings in which traditional techniques\nlike Fourier series break down, due to the curse of dimensionality. Deep\nlearning should be considered when the difficulty comes from the dimen-\nsionality, rather than from the inherent complexity of the function.\n(3) Unit cube. Returning to our (unreliable) analogy with biological neural nets,\none expects all charges occurring in the brain to belong to some fixed small\ninterval. The same is true of artificial neural networks: they perform best\nwhen all real numbers encountered throughout the network from input to\noutput belong to some bounded interval. Deep learning packages are often\nwritten assuming that the inputs belong to the unit cube r0, 1sn Ă Rn.\n11One should keep in mind that neural networks are universal approximators: a large enough\nneural network can approximate any continuous function accurately [Wik]. However, in practice\nsome functions are much more easily learnt than others.\n12This point should be read with some caution. For example, evaluation of board positions in\nGo is not a particularly noise-stable problem.\n9\n(4) Details matter. Design choices like network architecture and size, initial-\nization scheme, choice of learning rate (i.e. step size of gradient descent),\nchoice of optimizer etc. matter enormously. It is also important how the\ninputs to the neural network are encoded as vectors in Rn (the representa-\ntion).13 Overcoming these difficulties is best done with a collaborator who\nhas experience in deep learning research and implementation.\nWith these rules of thumb in mind we will now discuss three examples in pure\nmathematics.\n5.1. Learning the parity bit. Consider the parity bit function\nσ : t0, 1um Ñ t0, 1u\npxiq ÞÑ\nm\nÿ\ni“1\nxi\nmod 2.\nWe might be tempted to use a neural network to try to learn a function\nσ« : Rm Ñ R\nwhich agrees with σ under the natural embedding t0, 1um Ă Rm.\nThis is a classic problem in machine learning [MP17, I § 3.1].\nIt generalizes\nthe problem of learning the XOR function (the case m “ 2), which is one of the\nsimplest problems which cannot be learned without non-linearities.\nThere exist\nelegant neural networks extending σ to the unit cube, and given a large proportion\n(e.g. 50%) of the set t0, 1um a neural network can be trained to express σ [RHW85,\npp. 14-16]. However, given only a small proportion of the values of σ (e.g. 10% for\nm “ 10) a vanilla neural network will not reliably generalize to all values of σ (for\nexperiments, see [GGW22, ‘Playing with parity’]).\nThe issue here is that σ is highly noise sensitive. (Indeed, σ is precisely the\nchecksum of signal processing!) This is an important example to keep in mind, as\nmany simple functions in pure mathematics resemble σ. For example, see [GGW22,\nWeek 2] where we attempt (without much luck!) to train a neural network to learn\nthe M¨obius function from number theory.\n5.2. Learning descent sets. Consider the symmetric group Σn consisting of all per-\nmutations of 1, 2, . . . , n. Given a permutation we can consider its left and right\ndescent sets:\nLpxq “ t1 ď i ă n | x´1piq ą x´1pi ` 1qu,\n(4)\nRpxq “ t1 ď i ă n | xpiq ą xpi ` 1qu.\n(5)\nObviously, Lpx´1q “ Rpxq and Rpx´1q “ Lpxq. The left an right descent sets are\nimportant invariants of a permutation.\nIt is interesting to see whether a neural network can be trained to learn the left\nand right descent sets. In other words, we would like to train a neural network\nϕ« : Rn Ñ Rn´1\n13It seems silly to have to write that details matter in any technical subject. However many\npeople I have spoken to are under the false impression that one model works for everything, and\nthat training happens “out of the box” and is easy. For an excellent and honest summary by an\nexpert of the difficulties encountered when training large models, see [Kar19].\n10\nwhich given the vector pxp1q, xp2q, . . . , xpnqq returns a sequence of n´1 probabilities\ngiving whether or not 1 ď i ă n belongs to the left (resp. right) descent set.\nThis example is interesting in that (5) implies that the right descent set can be\npredicted perfectly with a single linear layer. More precisely, if we consider\nγ : Rn Ñ Rn´1\npv1, . . . , vnq ÞÑ pv1 ´ v2, v2 ´ v3, . . . , vn´1 ´ vnq\nthen the ith coordinate of γ evaluated on a permutation pxp1q, . . . , xpnqq is positive\nif and only if i P Rpxq. On the other hand, it seems much harder to handcraft a\nneural network which extracts the left descent set from pxp1q, . . . , xpnqq.\nThis might lead us to guess that a neural network will have a much easier time\nlearning the right descent set than the left descent set. This turns out to be the\ncase, and the difference is dramatic: a vanilla neural network with two hidden layers\nof dimensions 500 and 100 learns to predict right descent sets for n “ 35 with high\naccuracy after a few seconds. Whereas, the same network struggles to get even a\nsingle correct answer for the left descent set, after significant training!14 It is striking\nthat using permutation matrices as inputs rather than the vectors pxp1q, . . . , xpnqq\ngives perfect symmetry in training between left and right.15 The issue here is the\nrepresentation: how the model receives its input can have a dramatic effect on\nmodel performance.\n5.3. Transformers and linear algebra. Our final example is much more sophisti-\ncated, and illustrates how important the choice of training data can be. It also\nshows how surprising the results of training large neural networks can be.\nA transformer is a neural network architecture which first emerged in machine\ntranslation [VSP`17]. We will not go into any detail about the transformer archi-\ntecture here, except to say that it is well-suited to tasks where the input and output\nare sequences of tokens (“sequence to sequence” tasks):\nx\ny\nz\ntransformer\na\nb\nc\nMore precisely, the input sequence (“xyz”) determines a probability distribution\nover all tokens. We then sample from this distribution to obtain the first token\n(“a”). Now the input and sequence sampled so far (“xyz” + “a”) provides a new\ndistribution over tokens, from which we sample our second token (“b”), etc.\nIn a recent work [Cha21] Charton trains a transformer to perform various tasks\nin linear algebra: matrix transposition, matrix addition, matrix multiplication,\ndetermination of eigenvalues, determination of eigenvectors etc. For example, the\neigenvalue task is regarded as the “translation”:\nreal 5 ˆ 5-symmetric matrix\nM “ pm11, m12, m13, . . . , m55q\ntransformer\nlist of eigenvalues\nλ1 ě λ2 ě ¨ ¨ ¨ ě λ5.\nCharton considers real symmetric matrices, all of whose entries are signed floating\npoint numbers with three significant figures and exponent lying between ´100 and\n14Decreasing n and allowing longer training suggests that the network can learn the left descent\nset, however it is much harder.\n15For a colab containing all of these experiments, see [GGW22, Classifying descent sets in Sn].\n11\n100.16 The transformer obtains impressive accuracy on most linear algebra tasks.\nWhat is remarkable is that for the transformer the entries of the matrix (e.g. 3.14,\n-27.8, 0.000132, . . . ) are simply tokens—the transformer doesn’t “know” that 3.14\nis close to 3.13, or that both are positive; it doesn’t even “know” that its tokens\nrepresent numbers!\nAnother remarkable aspect of this work concerns generalization. A model trained\non Wigner matrices (e.g. entries sampled uniformly from r´10, 10s) does not gen-\neralize well at all to matrices with positive eigenvalues. On the other hand, a model\ntrained on matrices with eigenvalues sampled from a Laplace distribution (which\nhas heavy tails) does generalize to matrices whose eigenvalues are all positive, even\nthough it has not seen a single such matrix during training! The interested reader\nis referred to Charton’s paper [Cha21] (in particular Table 12) and his lecture on\nyoutube [Cha22].\n6. Examples from research mathematics\nWe now turn to some examples where deep learning has been used in pure\nmathematics research.\n6.1. Counter-examples in combinatorics. One can dream that deep learning might\none day provide a mathematician’s “bicycle for the mind”: an easy to use and\nflexible framework for exploring possibilities and potential counter-examples.\n(I\nhave certainly lost many days trying to prove a statement that turned out to be\nfalse, with the counter-example lying just beyond my mental horizon.)\nWe are certainly not there yet, but the closest we have come to witnessing such\na framework is provided in the work of Adam Wagner [Wag21]. He focuses on con-\njectures of the form: over all combinatorial structures X, an associated numerical\nquantity Z is bounded by B. He considers situations where there is some simple\nrecipe for generating objects in X, and that the numerical quantity Z is efficiently\ncomputable.\nFor example, a conjecture in graph theory states that for any connected graph\nG on n ě 3 vertices, with largest eigenvalue λ and matching number µ we have\n(6)\nλ ` µ ´\n?\nn ´ 1 ´ 1 ě 0.\n(It is not important for this discussion to know what the matching number or largest\neigenvalue are!)\nWagner fixes an enumeration e1, e2, . . . of the edges E in a complete graph on n-\nvertices. Graphs are generated by playing a single player game: the player is offered\ne1, e2 etc. and decides at each point whether to accept or reject the edge, the goal\nbeing to minimize (6). A move in the game is given by a 01-vector indicating edges\nthat have been taken so far, together with a vector indicating which edge is under\nconsideration. For example, when n “ 4 the pair pp1, 0, 1, 1, 0, 0q, p0, 0, 0, 0, 1, 0qq\nindicates that edge number 5 is under consideration, and that edges 1, 3 and 4 have\nalready been selected, and 2 rejected. Moves are sampled according to a neural\nnetwork\n(7)\nµ : RE ‘ RE Ñ R,\n16Charton considers various encodings of these numbers via sequences of tokens of various\nlengths, see [Cha21].\n12\nFigure 1. The evolution of graphs towards Wagner’s counter-\nexample, from [Wag21].\nwhich (after application of sigmoid) gives the probability that we should take the\nedge under consideration.\nWagner then employs the cross entropy method to gradually train the neural\nnetwork. A fixed (and large) number of graphs are sampled according to the neural\nnetwork (7).\nThen a fixed percentage (say 10%) of the games resulting in the\nsmallest values of the LHS of (6) are used as training data to update the neural\nnetwork (7). (That is, we tweak the weights of the neural network to make decisions\nthat result in graphs that are as close as possible to providing a counter-example\nto (7).) We then repeat. This method eventually finds a counter-example to (6)\non 19 vertices. The evolution of graphs sampled from the neural network is shown\nin Figure 6.1—note how the neural network learns quickly that tree-like graphs do\nbest. Exactly the same method works to discover counter-examples to several other\nconjectures in combinatorics, see [Wag21].\n6.2. Conjecture generation. The combinatorial invariance conjecture is a conjec-\nture in representation theory which was proposed by Lusztig and Dyer in the early\n1980s [Bre04]. To any pair of permutations x, y P Σn in the symmetric group one\nmay associate two objects: the Bruhat graph (a directed graph); and the Kazhdan-\nLusztig polynomial (a polynomial in q), see Figure 2 for an example of both. The\nconjecture states that an isomorphism between Bruhat graphs implies equality be-\ntween Kazhdan-Lusztig polynomials. A more optimistic version of this conjecture\nasks for a recipe which computes the Kazhdan-Lusztig polynomial from the Bruhat\ngraph. One interesting aspect of this conjecture is that it is (to the best of my\nknowledge) a conjecture born of pure empiricism.\nFor the Bruhat graph, the definition is simple, but the resulting graph is com-\nplicated. On the other hand, the definition of the Kazhdan-Lusztig polynomial is\ncomplicated, however the resulting polynomial is simple. Thus, there is at least a\n13\npassing resemblance to traditional applications of machine learning, where a sim-\nple judgement (e.g. “it’s a cat”) is made from complicated input (e.g. an array of\npixels).\nIt is natural to use neural networks as a testing ground for this conjecture: if a\nneural network can easily predict the Kazhdan-Lusztig polynomial from the Bruhat\ngraph, perhaps we can too! We trained a neural network to predict Kazhdan-Lusztig\npolynomials from the Bruhat graph. We used a neural network architecture known\nas a graph neural network, and trained the neural network to predict a probability\ndistribution on the coefficients of q, q2, q3 and q4.17 The neural network was trained\non « 20 000 Bruhat graphs, and achieved very high accuracy (« 98%) after less\nthan a day’s training. This provides reasonable evidence that there is some way of\nreliably guessing the Kazhdan-Lusztig polynomial from the Bruhat graph.\nIt is notoriously difficult to go from a trained neural network to some kind of\nhuman understanding. One technique to do so is known as saliency analysis. Re-\ncall that neural networks often learn a piecewise linear function, and hence one can\ntake derivatives of the learned function to try to learn which inputs have the most\ninfluence on a given output.18 In our example, saliency analysis provided subgraphs\nof the original Bruhat graph which appeared to have remarkable “hypercube” like\nstructure (see Figure 3 and [DVB`21, Figure 5a]). After considerable work this\n17The coefficient of q0 is known to always equal 1. In our training sets no coefficients of q5 or\nhigher occur.\n18This technique is often called “vanilla gradient” in the literature. Apparently it is very brittle\nin real-world applications.\nØ 1 ` 3q ` q2\nFigure 2. Bruhat interval and Kazhdan-Lusztig polynomial for\nthe pair of permutations x “ p1, 3, 2, 5, 4, 6q and y “ p3, 4, 5, 6, 1, 2q\nin Σ6, from [BBD`22]\n.\n14\nFigure 3. Bruhat interval pre and post saliency analysis.\neventually led to a conjecture [BBD`22] which would settle the combinatorial in-\nvariance conjecture for symmetric groups if proven, and has stimulated research on\nthis problem from pure mathematicians [GW23, BG23b, BG23a, BM23].\nIn a parallel development, Davies, Juh´asz, Lackenby and Tomasev were able to\nuse saliency analysis to discover a new relationship between the signature and hy-\nperbolic invariants of knots [DJLT22]. The machine learning background of both\nworks is explained in [DVB`21]. It would be very interesting to find further exam-\nples where saliency leads to new conjectures and theorems.\n6.3. Guiding calculation. Another area where deep learning has promise to impact\nmathematics is in the guiding of calculation. In many settings a computation can\nbe done in many ways. Any choice will lead to a correct outcome, but choices\nmay drastically effect the length of the computation. It is interesting to apply deep\nlearning in these settings, as false steps (which deep learning models are bound to\nmake) effects efficiency but not accuracy.\nOver the last three years there have been several examples of such applications.\nIn [PSHL20], the authors use a machine learning algorithm to guide selection strate-\ngies in Buchberger’s algorithm, which is a central algorithm in the theory of Gr¨obner\nbases in polynomial rings. In [Sim21], Simpson uses deep neural networks to sim-\nplify proofs in the classification of nilpotent semi-groups. In [HKS22], the authors\nuse a deep neural network to predict computation times of period matrices, and\nuse it to more efficiently compute the periods of certain hypersurfaces in projective\nspace.\n6.4. Prediction. Due to limitations of space, we cannot begin to survey all the\nwork done in this infant subject. In particular, there has been much work (see\ne.g. [BHH`21, BCDL20]) training neural networks to predict difficult quantities in\nmathematics (e.g. volumes of polytopes, line bundle cohomology,. . . ).\n7. Conclusion\nThe use of deep learning in pure mathematics is in its infancy. The tools of\nmachine learning are flexible and powerful, but need expertise and experience to\nuse. One should not expect things to work “out of the box”. Deep learning has\nfound applications in several branches of pure mathematics including combinatorics,\nrepresentation theory, topology and algebraic geometry. Applications so far support\n15\nthe thesis that deep learning most usefully aids the more intuitive (“system 1”) parts\nof the mathematical process: spotting patterns, deciding where counter-examples\nmight lie, choosing which part of a calculation to do next. However, the possibilities\ndo seem endless, and only time will tell.\nReferences\n[BBD`22] C. Blundell, L. Buesing, A. Davies, P. Veliˇckovi´c, and G. Williamson. Towards com-\nbinatorial invariance for Kazhdan-Lusztig polynomials. Represent. Theory, 26:1145–\n1191, 2022. 13, 14\n[BCDL20]\nC. R. Brodie, A. Constantin, R. Deen, and A. Lukas. Machine learning line bundle\ncohomology. Fortschritte der Physik, 68(1):1900087, 2020. 14\n[BG23a]\nG. Barkley and C. Gaetz. Combinatorial invariance for elementary intervals. arXiv\npreprint, arXiv:2303.15577, 2023. 14\n[BG23b]\nG. Barkley and C. Gaetz. Combinatorial invariance for lower intervals using hypercube\ndecompositions, 2023. 14\n[BHA`21] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S.\nBernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258, 2021. 2\n[BHH`21] J. Bao, Y.-H. He, E. Hirst, J. Hofscheier, A. Kasprzyk, and S. Majumder. Polytopes\nand machine learning. arXiv preprint arXiv:2109.09602, 2021. 14\n[BHMM19] M. Belkin, D. Hsu, S. Ma, and S. Mandal. Reconciling modern machine-learning\npractice and the classical bias–variance trade-off. Proceedings of the National Academy\nof Sciences, 116(32):15849–15854, 2019. 8\n[BM23]\nF. Brenti and M. Marietti. Kazhdan–Lusztig R-polynomials, combinatorial invariance,\nand hypercube decompositions. preprint 2023, 2023. 14\n[Bre04]\nF. Brenti. Kazhdan-Lusztig polynomials: history, problems, and combinatorial invari-\nance. S´em. Lothar. Combin., 49:Art. B49b, 30, 2002/04. 12\n[Cha21]\nF. Charton. Linear algebra with transformers. CoRR, abs/2112.01898, 2021. 10, 11\n[Cha22]\nF.\nCharton.\nMath\nwith\nTransformers.\nhttps://www.youtube.com/watch?v=81o-\nUiop5CA, October 2022. Accessed on 20 March, 2023. 11\n[DJLT22]\nA. Davies, A. Juh´asz, M. Lackenby, and N. Tomasev. The signature and cusp geometry\nof hyperbolic knots. Geometry and Topology, 2022. 14\n[DVB`21] A. Davies, P. Veliˇckovi´c, L. Buesing, S. Blackwell, D. Zheng, N. Tomaˇsev, R. Tan-\nburn, P. Battaglia, C. Blundell, A. Juh´asz, M. Lackenby, G. Williamson, D. Hassabis,\nand P. Kohli. Advancing mathematics by guiding human intuition with AI. Nature,\n600(7887):70–74, 2021. 13, 14\n[GBC16]\nI. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press, 2016. 6, 7\n[GGW22]\nJ. Gibson, G. Gottwald, and G. Williamson. Machine Learning for the Working Mathe-\nmatician. https://sites.google.com/view/mlwm-seminar-2022, June 2022. Accessed on\n20 March, 2023. 9, 10\n[GW23]\nM. Gurevich and C. Wang. Parabolic recursions for Kazhdan-Lusztig polynomials and\nthe hypercube decomposition. arXiv preprint arXiv:2303.09251, 2023. 14\n[HK22]\nY.-H. He and M. Kim. Learning algebraic structures: preliminary investigations. In-\nternational Journal of Data Science in the Mathematical Sciences, pages 1–20, 2022.\n6\n[HKS22]\nK. Heal, A. Kulkarni, and E. C. Sert¨oz. Deep learning Gauss–Manin connections.\nAdvances in Applied Clifford Algebras, 32(2):24, 2022. 14\n[HRW`21] J. M. Han, J. Rute, Y. Wu, E. W. Ayers, and S. Polu. Proof artifact co-training for\ntheorem proving with language models. arXiv preprint arXiv:2102.06203, 2021. 1\n[HW62]\nD. H. Hubel and T. N. Wiesel. Receptive fields, binocular interaction and functional\narchitecture in the cat’s visual cortex. The Journal of physiology, 160(1):106, 1962. 3\n[JWZ`23] A. Q. Jiang, S. Welleck, J. P. Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and\nG. Lample. Draft, sketch, and prove: Guiding formal theorem provers with informal\nproofs, 2023. 1\n[Kar]\nA. Karpathy. Convnet Javascript Demo.\nhttps://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html.\nAccessed\non 18 March, 2023. 7\n16\n[Kar19]\nA.\nKarpathy.\nA\nRecipe\nfor\nTraining\nNeural\nNetworks.\nhttp://karpathy.github.io/2019/04/25/recipe/,\nApr\n25,\n2019.\nAccessed\non\n20\nMarch, 2023. 9\n[LBBH98]\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 6\n[MP43]\nW. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous\nactivity. The bulletin of mathematical biophysics, 5:115–133, 1943. 2\n[MP17]\nM. Minsky and S. A. Papert. Perceptrons, Reissue of the 1988 Expanded Edition with\na new foreword by L´eon Bottou: An Introduction to Computational Geometry. MIT\npress, 2017. 9\n[Mum20]\nD. Mumford. The Astonishing Convergence of AI and the Human Brain, October 1,\n2020. Accessed on 13 March, 2023. 4\n[Ola14]\nC.\nOlah.\nNeural\nNetworks,\nManifolds,\nand\nTopology.\nhttps://colah.github.io/posts/2014-03-NN-Manifolds-Topology/,\nApril\n6,\n2014.\nAccessed on 18 March, 2023. 6\n[PSHL20]\nD. Peifer, M. Stillman, and D. Halpern-Leistner. Learning selection strategies in buch-\nberger’s algorithm. In International Conference on Machine Learning, pages 7575–\n7585. PMLR, 2020. 14\n[RHW85]\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations\nby error propagation. Technical report, California Univ San Diego La Jolla Inst for\nCognitive Science, 1985. 9\n[SC]\nD.\nSmilkov\nand\nS.\nCarter.\nThe\nTensorflow\nPlayground.\nhttps://playground.tensorflow.org. Accessed on 18 March, 2023. 7\n[Sim21]\nC. Simpson. Learning proofs for the classification of nilpotent semigroups. arXiv\npreprint arXiv:2106.03015, 2021. 14\n[Tur48]\nA. M. Turing. The Essential Turing, chapter Intelligent machinery, pages 395–432.\nOxford University Press (reprinted 2004), 1948. 1, 2\n[VSP`17]\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,\nand I. Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017. 10\n[Wag21]\nA. Z. Wagner. Constructions in combinatorics via neural networks. arXiv preprint\narXiv:2104.14516, 2021. 11, 12\n[Wik]\nWikipedia. Universal approximation theorem. https://en.wikipedia.org/wiki/Universal approximation theorem.\nAccessed on 17 May, 2023. 8\nUniversity of Sydney,Australia.\nEmail address: g.williamson@sydney.edu.au\n",
  "categories": [
    "math.RT",
    "cs.LG",
    "math.AG",
    "math.CO"
  ],
  "published": "2023-04-25",
  "updated": "2023-05-26"
}