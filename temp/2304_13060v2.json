{
  "id": "http://arxiv.org/abs/2304.13060v2",
  "title": "Injecting structural hints: Using language models to study inductive biases in language learning",
  "authors": [
    "Isabel Papadimitriou",
    "Dan Jurafsky"
  ],
  "abstract": "Both humans and large language models are able to learn language without\nexplicit structural supervision. What inductive biases make this learning\npossible? We address this fundamental cognitive question by leveraging\ntransformer language models: we inject inductive bias into language models by\npretraining on formally-structured data, and then evaluate the biased learners'\nability to learn typologically-diverse natural languages. Our experimental\nsetup creates a testbed for hypotheses about inductive bias in human language\nlearning. We investigate the effect of injecting models with three types of\ninductive bias: 1) recursive, hierarchical processing, 2) crossing token-token\nrelationships that can't be modeled by context-free grammars, and 3) a Zipfian\npower-law vocabulary distribution. We show that non-context-free relationships\nform the best inductive biases. Our study leverages the capabilities of\ntransformer models to run controlled language learning experiments that are not\npossible to run on humans, and surfaces hypotheses about the structures that\nfacilitate language learning in both humans and machines.",
  "text": "Injecting structural hints:\nUsing language models to study inductive biases in language learning\nIsabel Papadimitriou and Dan Jurafsky\nComputer Science Department\nStanford University\n{isabelvp,jurafsky}@stanford.edu\nAbstract\nBoth humans and large language models are\nable to learn language without explicit struc-\ntural supervision. What inductive biases make\nthis learning possible? We address this funda-\nmental cognitive question by leveraging trans-\nformer language models: we inject inductive\nbias into language models by pretraining on\nformally-structured data, and then evaluate the\nbiased learners’ ability to learn typologically-\ndiverse natural languages. Our experimental\nsetup creates a testbed for hypotheses about in-\nductive bias in human language learning. We\ninvestigate the effect of injecting models with\nthree types of inductive bias: 1) recursive, hi-\nerarchical processing, 2) crossing token-token\nrelationships that can’t be modeled by context-\nfree grammars, and 3) a Zipfian power-law\nvocabulary distribution. We show that non-\ncontext-free relationships form the best induc-\ntive biases. Our study leverages the capabil-\nities of transformer models to run controlled\nlanguage learning experiments that are not pos-\nsible to run on humans, and surfaces hypothe-\nses about the structures that facilitate language\nlearning in both humans and machines.\n1\nIntroduction\nNatural languages are complex and structured sys-\ntems which humans learn without direct structural\nsupervision. It is a fundamental and open prob-\nlem in linguistics and cognitive science to under-\nstand the inductive biases that make such learning\npossible: what structural predispositions does a\nsuccessful language learner need to start with? In\nthis work, we shed light on this cognitive problem\nusing artificial learners. Using our method of struc-\ntural injection, we causally intervene in transformer\nlanguage models and manipulate their structural in-\nductive biases, before training on natural language.\nWe predispose transformers with three struc-\ntural biases from the cognitive literature: a bias\nfor recursive processing, a bias for keeping track\nUntrained\nGPT-2 model \nFormal\nlanguage\nStructurally-\nbiased \nlearner\nWikitext \nStructural Principle:\nRecursion, or\nContext-sensitivity, or\n...\nPretraining:\nInject structural\ninductive bias\nMetric: perplexity\nafter fine-tuning\nFigure 1: Our method: we take a GPT-2-sized model\nand pretrain it with a formal language corpus (see Fig-\nure 3 for examples). We then take these pretrained\nmodels and fine-tune them on Wikipedia data to asses\neach formal structure as an inductive bias for learning\nEnglish, Japanese, and Basque.\nof context-sensitive dependencies, and a bias for\na power-law Zipfian vocabulary distribution. We\ninject untrained transformer language models with\neach structural bias by pretraining on synthetic\nstructured data and then evaluating language mod-\neling fine-tuning on human languages (English,\nJapanese, and Basque). Our inquiry is structured\naround three experimental questions:\n• Experiment 1: How does an inductive bias\nfor recursion compare to an inductive bias\nfor context-sensitive crossing relationships?\n(Section 5)\n• Experiment 2: Is a bias for pure constituent\nrecursion better when mixed with a small\namount of tokens that break context-free con-\nstituency structure? (Section 6)\n• Experiment 3: Does a learner biased to-\nwards learning a power-law Zipfian vocab-\nulary distribution learn language more effec-\ntively? (Section 7)\narXiv:2304.13060v2  [cs.CL]  29 Oct 2023\nThe lawyer that the man that the dog bit hired was disbarred\n(a) Center embedding in English\n... mer\nd’chind\nem Hans es huus lönd hälfe aastriiche\n... we the children\nHans\nthe house\nlet\nhelp\npaint\n(b) Cross-serial dependencies in Swiss German, example from\nShieber (1985): “We let the children help Hans paint the house”\n“I voted for him even though I am negatively affected by his redistribution policies” he said\n(c) Crossing discourse anaphora links in English\nFigure 2: Examples of recursive and context-sensitive structures in natural language.\nIn Experiment 1, we disentangle the effects of re-\ncursion and context-sensitivity: both occur in lan-\nguage, but which one is more useful as a sole learn-\ning bias? Language is characterized by recursive\nstructures in context-free constituent relationships\nlike those in Figure 2a, and some linguistic theories\nposit that recursive processing is a crucial (and per-\nhaps the only) inductive bias that makes human lan-\nguage learning possible (Hauser et al., 2002; Chom-\nsky, 1995). However, it is widely hypothesized\nthat human language is mildly context sensitive:\nwhile there is recursive structure, there are also non-\ncontext-free relationships between tokens, both in\nsyntactic structure (Figure 2b) and in the structure\nof meaning and discourse relationships (Figure 2c)\n(Joshi et al., 1990; Stabler, 2010; Shieber, 1985;\nSteedman, 1990; Frank and Hunter, 2021; Joshi,\n1985). We compare the two principles of recursion\nand non-context-free relationships, and find that an\ninductive bias for non-context-free crossing depen-\ndencies is better for downstream language learning.\nHowever, both inductive biases greatly outperform\nrandom and regular language controls.\nIn Experiment 2, we combine recursion and\ncontext-sensitivity and ask: is a recursive induc-\ntive bias better with slight context-sensitivity? We\nshow that there is a significant improvement in\ndownstream language learning if we add just 1%\nof a bias for crossing dependencies, breaking the\nconstituent structure of the other 99% recursive\nbias we give the learner. Even when learners are\nmostly biased towards recursion, they learn lan-\nguage faster when their bias includes constituent-\nbreaking context-sensitive structures.\nIn Experiment 3, we test the effect of induc-\ntive biases in vocabulary distribution: does a\nbias towards a human-like Zipfian vocabulary dis-\ntribution (Zipf, 1936) help language learning? Lan-\nguage is structured not only in how tokens relate,\nbut also in the structure of the distribution that\ntokens are drawn from, a cognitive bias that is es-\npecially significant for memory-based theories of\ngrammar (Shufaniya and Arnon, 2022; Piantadosi,\n2014; Ellis and O’Donnell, 2012). We show that\na Zipfian pretraining bias makes models better at\ndownstream language learning even when there\nis no correspondence between the pretraining and\nfine-tuning vocabularies.\nWhile our experiments work with computational\nmodels, the question that we are examining is about\nhumans: what are the possible inductive biases that\nmake human language learning possible without ex-\nplicit structural supervision? Using computational\nmodels, we can investigate this question through\nempirically manipulating the inductive bias of a\nlanguage learner — a causal experimental route\nthat is not possible when working with humans.\nResults from such experiments can act as firstly as\nproofs of concept, showing how language learning\nis or isn’t possible with different inductive biases.\nSecondly, such experiments help with hypothesis\ngeneration: with structural injection, we can test\narbitrary inductive biases in a theory-independent\nway (see Baroni, 2022; Portelance, 2022; Lappin,\n2021; Wilcox et al., 2023; Kauf et al., 2023, for\nfurther discussions on the role of neural NLP mod-\nels in cognitive science). In Section 8 we discuss\nhow our method of leveraging artificial learners to\ncausally investigate inductive biases adds a new\ndirection to the rich prior literature on inductive\nbias in neural learners.\nIn summary, our findings indicate that biases for\ncomplex token-token interactions, whether these\ninvolve recursion or not, form a powerful inductive\nbias for learning natural language. Crucially, our\nresults are compatible with and can inform a wide\nvariety of cognitive architectures: models in which\nstructural inductive biases in humans arise from\nprior statistical learning (Elman, 1996), models in\nwhich they arise from other aspects of cognition\nor communication (Lieven and Tomasello, 2008;\nHahn et al., 2020; Gibson et al., 2019), and models\nin which they are presumed to be innate (Hauser\net al., 2002). 1\n2\nBackground: Structural inductive bias\nin humans\nIn order to use our experiments as a window into\nhypotheses about human inductive biases, we look\nat three families of structural bias from the linguis-\ntics and cognitive science literature.\n2.1\nRecursion\nOne very prominent hypothesis for linguistic in-\nductive bias focuses on recursion: the ability for\nhierarchically-structured constituents to contain\nother constituents of the same type, and as such\nallow for potentially infinitely deep hierarchical\nstructure.\nRecursive structures can be described in terms\nof a context-free language: a grammar with rules\nof the form A →β, where a non-terminal node A\nconsists of a string β that can contain both terminal\nand non-terminal nodes. Such a phrase-structure\ngrammar is recursive when a non-terminal can con-\ntain a string that includes another non-terminal of\nthe same type. For example, one rule describing\nthe language of well-nested parentheses is\nS →( S )\nwhere any well-formed sentence S can make an-\nother well-formed sentence S if it is inserted into\na pair of parentheses. Rules of this type allow for\ninfinite nesting.\nA canonical linguistic example of recursion is\ncenter embedding. In the center embedding exam-\nple in Figure 2a, a noun phrase (“the man that the\ndog bit”) can be used as a part of another noun-\nphrase (“the lawyer that [the man that the dog bit]\nhired”), and this can carry on recursively. Such self-\nembedding structures, which are attested in human\nlanguage, are possible with recursive grammars but\nnot in finite-state languages (Chomsky, 1959). The\nrecursion hypothesis for linguistic inductive bias\n1Code\nand\ninstructions\nfor\nrunning\nour\nex-\nperiments\nis\nat\nhttps://github.com/toizzy/\ninjecting-structural-hints.\nstates that the ability for such constituent recursion\nis a crucial linguistic inductive bias, constitutes the\nunderlying structural bias for the faculty of lan-\nguage, and that recursion is what distinguishes lan-\nguage from animal communication (Hauser et al.,\n2002; Chomsky, 1995).\n2.2\nContext-sensitivity\nAlthough context-free grammars allow recursive\nstructure, they are not complex enough to model\nmany attested linguistic effects, which require a\nnon-context-free (i.e., at least context-sensitive)\ngrammar (Joshi et al., 1990; Stabler, 2010; Shieber,\n1985; Steedman, 1990; Frank and Hunter, 2021;\nJoshi, 1985). For example, Shieber (1985) proves\nthat a grammar modeling the unbounded cross-\nserial dependency structures possible in Swiss Ger-\nman (Fig. 2b) must be non-context-free, and Steed-\nman (1990) shows that gapping effects (sentences\nlike “Harry eats beans, and Fred potatoes”) simi-\nlarly cannot be analyzed by context-free grammars.\nLooking beyond syntax, reference and discourse\nstructures (Fig. 2c) as well as meaning-based inter-\nactions do not abide by context-free restrictions.\nWhile it is broadly hypothesized that both recur-\nsion and non-context-freeness exist in natural lan-\nguage, our experiments empirically disentangle the\ntwo hypotheses. We compare two inductive biases:\nan inductive bias for recursive processing, and an\ninductive bias for non-context-free structures that\ndo not have recursion. Non-context-free structures\nallow for unbounded and crossing interactions be-\ntween tokens, while recursive grammars only allow\ntokens to influence each other in specific subtree\nrelationships. To disentangle these biases, we test\nthe effect of an inductive bias where there are very\nminimal (and non-tree-structured) restrictions on\nwhere related tokens can appear (see Section 4.2\nand Fig. 3b for details of our formalization).\n2.3\nVocabulary distribution\nWe next investigate structural biases in the lexicon:\nhow a cognitive bias towards vocabulary distribu-\ntions (i.e., which tokens are more likely to appear\nin a corpus of language) affects learning. We aim\nto answer: does a bias towards a Zipfian vocabu-\nlary distribution (Shufaniya and Arnon, 2022; Pi-\nantadosi, 2014) act as a structural bias that aids\nlanguage learning?\nA feature pervasive across human languages is\nthe unbalanced nature of vocabulary distributions.\nIn human languages, some words are very common\nS\nS\n248)\nS\n103)\nS\n123)\n123(\n103(\n248(\nS\n1)\nS\nS\n225)\n225(\nS\n54)\n54(\n1(\n(a) The recursive NEST language. Every S node represents a\nwell-formed NEST substring.\n1( 54( 225( 1) 54) 225) 248( 248) 123( 103( 123) 103)\n(b) The context-sensitive CROSS language. Edges connect\nmatching parentheses.\n499 472 300 345 272 309 17 15 329 233 9 267 122\n(c) The random language, RAND\n499 472 300 345 272 499 472 300 345 272 309 17 15\n(d) The regular REP language, with repetition length k = 5.\nThe repeated block is circled. For our experiments, we use\nk = 10.\nFigure 3: The formal languages we use to pretrain our models to give them different structural inductive biases. For\nsimplicity, we represent NEST and CROSS as 250 open tokens with 250 close tokens, while we present RAND and\nREP with 500 tokens. The vocabulary distributions are the same between all languages if we concatenate the open\nand close tokens: token 1) in the parentheses languages is equivalent to token 251 in RAND and REP.\n(like “the” or “a” in English) and most words are\nused very rarely, roughly following a power-law\ndistribution. Zipf’s law states that the rth most\ncommon word has frequency proportional to\n1\n(r + β)α\n(1)\nwith α ≈1 and β ≈2.7 providing a good em-\npirical estimate for human languages (Zipf, 1936;\nMandelbrot, 1953). The literature on explaining\nand deriving this empirical fact is rich and varied\n(see Piantadosi, 2014, for a thorough review), while\nexperimental and theoretical works examine the\nlearnability of Zipfian-distributed data (Ellis and\nO’Donnell, 2012; Lavi-Rotbain and Arnon, 2022;\nSchuler et al., 2017; Chan et al., 2022). In our\nwork, we examine the effect of a Zipfian inductive\non learning language (Experiment 3).\n3\nMethod Overview: Pretraining as\nstructural inductive bias\nWe set up experiments where we control the induc-\ntive bias of a language model learner, and examine\nhow inductive biases influence language learning.\nOur experiments consist of two steps: 1) pretrain-\ning a GPT-2-sized model on a formal language to\ninject a structural bias and 2) fine-tuning on lim-\nited natural language data (drawing on our prior\nmethods in Papadimitriou and Jurafsky (2020), see\nFigure 1 for a depiction). To instill an inductive\nbias in an artificial language learner, we pretrain\nuntrained transformer language model on synthetic\ndata which exhibits a single formal structural prin-\nciple. After sufficient pretraining on such a cor-\npus, we have a transformer model which has never\nseen human language, but has successfully learnt\nto model a type of structure. We can then train this\nmodel on natural language data, taking its pretrain-\ning to be a structural inductive bias. We fine-tune\neach model on three different languages (English,\nJapanese, and Basque). The final performance on\neach language, measured in perplexity, is an indica-\ntor of how learnable the raw language data is when\na learner starts with the specific inductive bias.\n3.1\nImplementation Details\nFor each experiment, we randomly initialize a GPT-\n2-small model with a max sequence length of 512,\nand train on one formal language.2. The specific\nformal languages we use are described in Section 4.\nWe use a batch size of 512 to train all models, and\ntrain for 5,000 steps with 1,000 steps of warmup.\nEach formal language corpus consists of 1 billion\ntokens, which comes out to 3,814 batches. There-\nfore, in training 5,000 steps the model sees each\ncorpus approximately one and a half times.\nWe fine-tune each pretrained model on three sep-\narate languages to measure final test perplexity. We\nuse the the wikitext-103 English dataset (Merity\net al., 2017, 103 million tokens), Papadimitriou and\nJurafsky (2020)’s Japanese Wikipedia (129 million\ntokens) and Basque Wikipedia (63 million tokens)\ncorpora. We fine-tune with two epochs on the En-\n2We use GPT-2-small to refer to the model config accessed\nusing Hugging Face AutoConfig with key gpt2: 12 trans-\nformer layers with 12 attention heads per layer, and a hidden\nsize of 768\nglish and Japanese datasets, and 4 epochs on the\nBasque datasets. Though the three corpora are not\ncontrolled for size and train-test difficulty, we do\nnot compare results in order to analyze differences\nbetween languages and draw conclusions from this.\nWe instead use the three languages to verify that\nany claims about language learning and inductive\nbias that we make are likely cross-linguistic and\nnot English-specific.\nOur pretraining languages all have a vocabulary\nsize of 500, which has no correspondences with the\n50,257-size vocabulary of GPT-2. To accomodate\nthis new tokenizer, we have to initialize a new word\nembedding matrix, with 50,257 rows for the model\nto learn in fine-tuning. We initialize the embedding\nmatrix by randomly sampling with replacement\nfrom the rows of the old embedding matrix with\n500 rows, following Wu et al. (2023), who show\nthat initializing an embedding matrix for transfer\nlearning by sampling from the old embedding ma-\ntrix leads to far better transfer learning than random\nreinitialization even for unrelated vocabularies (see\nalso Hewitt, 2021). To account for the effect of hav-\ning to relearn the vocabulary, we also add a control\ncase where we take the pretrained GPT-2 model\nand randomly resample the rows of the embedding\nmatrix. To re-learn the embedding matrix, we fine-\ntune on the natural language corpora. This control\nappears on the right of our results graphs.\n4\nPretraining languages\nOur experimental method rests on pretraining lan-\nguage models on well-chosen formal languages.\nHere, we describe the four families of formal lan-\nguages that we use for pretraining, and present\nexamples of each of the languages in Fig. 3.\n4.1\nRecursive and context free: the nested\nparentheses language NEST\nThe first language that we pretrain on is a language\nof matching nested parentheses, also known as the\nDyck language. The vocabulary of this language\nconsists of a set of matched open and closed tokens,\nand vocabulary size can be set to any even number\n(for our experiments, we used a total vocabulary\nsize of 500 for all pretraining language). Writing\nthe language from left to right, at each step we\nrandomly pick between two choices: either 1) open\na parenthesis (p = 0.49) or 2) close the last opened\nparenthesis (p = 0.51). Since we only ever close\nthe most recently-opened parenthesis token, there\nwill never be any crossing arcs in how we connect\nparentheses. The nested parenthesis grammar is\ncontext-free. An example of this language is shown\nin Fig. 3a.\n4.2\nNon-context-free: the crossing\nparentheses language CROSS\nThe Crossing Parentheses language CROSS is a\nparentheses language with the same vocabulary\nof open and close tokens as the NEST language, but\nwhere parentheses do not have to be well-nested,\nbut just have to be balanced: opened parentheses\nmust close. As such, pairs of parentheses can in-\nterleave and cross. Whereas the NEST language\nimposes strict limitations to how different tokens\ncan interact (a parenthesis can only close in the\nspace between its parent opening and closing, so\nthere is no ability to see past the top of the stack),\nin the CROSS a token can have its pair in any lo-\ncation. In order to control the CROSS language to\nbe like the NEST language as much as possible (ex-\ncept for the dependent variable of non-context-free\ncrossing) we add another structural constraint: the\ndistribution of distances between open and close\ntokens is matched to the empirical dependency dis-\ntance distribution of the NEST language. This way,\nany difference in the inductive bias is not due to the\nlengths of dependencies, but due to the structural\nbiases in the pretraining data. This dependency link\ndistribution is what gives the CROSS language struc-\ntural information for a language model to learn:\nfor every opened parenthesis, there are positions\nwhen it is more likely to be closing than others, and\nmodeling this language involves modeling those\nprobabilities.\nThe crossing parentheses language is not context\nfree. Though every NEST string is a legitimate (but\nexponentially unlikely) CROSS string, the CROSS\nlanguage can include other strings, like arbitrarily\nlong cross-serial dependency structures that are\nknown to be non-context-free (Shieber, 1985):\n(1 (2 (3 · · · (k )1 )2 )3 · · · )k\nIn the NEST language, arbitrarily deep nesting (and\narbitrarily long dependencies) are in the limit pos-\nsible, though in practice unlikely, and similarly in\nthe CROSS language arbitrarily long dependencies\nare possible but unlikely. This is roughly anal-\nogous to human language, where processes like\ncenter embedding and cross-serial dependencies\ncan in theory happen infinitely but in practice are\nFigure 4: Results for Experiment 1, CROSS is a better inductive bias than NEST Each model pretrained with\nthe formal language on the x-axis is evaluated on a wikipedia test set after natural language fine-tuning. Error bars\nrepresent 95% confidence intervals over 5 fine-tuning runs with different random seeds. Since we cannot directly\ncompare perplexities between different test sets, we can only compare the ranking of the test conditions. The rank\n(CROSS is better than NEST is better than REP is better than RAND) is consistent across English, Japanese, and\nBasque.\nlimited. This probabilistic weighting doesn’t af-\nfect our proof of non-context-freeness: the Shieber\n(1985) proof is independent of the probabilities of\nthese constructions.\n4.3\nBaselines: the Random language and the\nregular Repetition language\nThe Random language RAND\nFor our RAND\nbaseline, we take the vocabulary of our experimen-\ntal languages NEST and CROSS, and create a dataset\nby sampling each token randomly from the vocabu-\nlary distribution without any structural limitations.\nThe Repetition language REP\nWe also test a\nslightly stronger baseline than RAND, the repetition\nbaseline. In the REP language, tokens are placed\nrandomly, like in RAND, but every 10 random to-\nkens are then immediately repeated. Using the\nrepetition language as a baseline lets us control for\nthe effect of there being any structure that connects\ntwo tokens. This way, we can better measure the\nutility of more complex structural inductive biases.\nNote that REP is not the same as the ‘copy language’\n(the set of arbitrarily long strings followed by their\nrepetition), which is famously context-sensitive. In\nthe case of REP, we’re restricting any copied chunk\nto be exactly k in length, which makes the language\nregular, indeed subregular.\n5\nExperiment 1: Disentangling recursive\nand context-sensitive inductive biases\nFor our first experiment, we compare the fine-\ntuning perplexity of models pretrained on the NEST\nlanguage and the CROSS language to each other and\nto random and regular controls. Our fine-tuning\nresults are shown in Fig. 4. From the fine-tuning\nperplexity that we get with different structural in-\nductive biases, we can present two findings:\nFigure 5: Results for Experiment 2, even small amounts of non-context-free structures in the inductive bias\ncause significant improvement in downstream perplexity. Mixing 1% and 10% of the CROSS language in with\nthe NEST language, which breaks the recursive constituency structure of NEST , causes significant downstream\nlanguage learning improvements over plain NEST . Error bars represent 95% confidence intervals over 5 fine-tuning\nruns with different random seeds\nAny structure has a significant effect as an in-\nductive bias\nAll of the models with a structural\npretraining language fare significantly better than\nthe model with the random, unstructured pretrain-\ning language. This especially surprising in the\ncase of the REP language, which has a very simple\nstructure that is similar to RAND. However, our\nexperiments also show that the shallow structure of\nREP is worse than both of our languages with more\ncomplex structural organization. Comparing the\nperformance of the NEST and the CROSS language\nleads us to our next finding:\nContext sensitivity acts as a better inductive bias\nthan recursion\nOur experiments disentangle the\neffects of recursive structure and non-context-free\nlinking structure, and show that the non-context-\nfree CROSS language acts as a stronger inductive\nbias for language learning than the recursive NEST\nlanguage. Our current methodology does not as-\ncertain which aspects of language learning the\nCROSS language is most helpful for, which is an\nespecially fruitful question for future work since\ncontext-sensitive linking structures arise in syn-\ntax, semantics, and discourse. Understanding the\nrole of different complex structures in influencing\nlanguage learning in an in-vitro paradigm such as\nours shines light on the properties of language as\na learnable system under different cognitive struc-\ntural assumptions.\n6\nExperiment 2: Mixing context-free and\nnon-context-free structures\nIn Experiment 1, we saw that the non-context-free\ninductive bias of the CROSS language is more\nbeneficial for downstream language learning than\nthe constituent recursion of the NEST language.\nHowever, neither of these extremes encompass the\nmore widely agreed nature of linguistic syntactic\nstructure: likely a recursive grammar with mildly\ncontext-sensitive elements.\nTo examine the\neffects of non-context-free elements in otherwise\ncontext-free grammars, we create the NEST-MIX-P\nlanguages.\nThese languages follow the NEST\ngrammar, except that with probability p% an ‘open’\ntoken does not follow the NEST grammar and\ninstead follows the CROSS grammar. This means\nthat we sample a dependency distance and place a\n‘close’ token for it without taking into account the\nconstituent structure of the NEST language. The\nNEST example from Figure 3a, with one CROSS to-\nken added and shown in green, would look like this:\n1( 54( 54) 225( 225) 123( 1) 248( 103( 123) 103) 248)\nWe test two such languages: NEST-MIX-1, with\n1% CROSS tokens, and NEST-MIX-10, with 10%\nCROSS tokens. Our results are shown in Figure 5.\nAdding 1% CROSS tokens to a NEST language\ncauses it to act as a significantly better inductive\nbias, with an average improvement of 5 perplexity\npoints, while adding 10% CROSS tokens causes an\naverage downstream improvement of 9 perplexity\npoints. Our results show that even small amounts\nof context-sensitive inductive bias have a big effect\non language learning, aligning with theoretical\nlinguistics results about human language being\nmildly context sensitive.\n7\nExperiment 3: Zipfian structural bias\nHow does a bias towards a particular vocabulary\ndistribution affect a language learner? We test the\neffect of a uniform vocabulary distribution versus\na Zipfian vocabulary distribution as a pretrained\ninductive bias. As shown in Figure 6, a Zipfian pre-\ntraining distribution of random tokens predisposes\na model with a better language-learning bias.\nCrucially, there is no connection or correspon-\ndence between the pretraining and fine-tuning vo-\ncabularies: the pretraining vocabulary is made up\nof 500 parentheses tokens (whose frequency rank-\ning is randomly ordered in order to make a Zipfian\ndistribution in the Zipfian case), whereas the GPT-\n2 tokenizer’s fine-tuning vocabulary is over 50K\ntokens. No token correspondence of specific to-\nken information is transferred between pretraining\nand fine-tuning as the tokens are unrelated and we\nre-sample the rows of the embedding matrix.\nSince there is no correspondence, our vocabu-\nlary distribution ablations measure the effect of\nan abstract bias towards one type of vocabulary\ndistribution, rather than the effect of knowing the\nspecific distribution of tokens in a language. Our\nFigure 6: Results for Experiment 3, a Zipfian induc-\ntive bias improves downstream learning, even when\nthe pretraining and fine-tuning vocabularies are unre-\nlated. We pretrain models with random tokens sampled\neither from a uniform or a Zipfian distribution.\nexperiments show that pretraining on a random\nZipfian corpus biases a model for better language\nlearning than a uniform corpus, and that a struc-\ntural bias for vocabulary distribution is encoded\nbeyond the word embedding matrix and more ab-\nstractly in network parameters. Due to the lack\nof any structure beyond vocabulary distribution in\nthe random corpora, neither of the test cases lead\nto very good downstream language performance.\nResults around combining Zipfian vocabulary with\nother structural biases like those in Experiment 1\nare mixed, and we do not have clear evidence that\nZipfian vocabulary has a structural effect that can\nadd on to the effect of structure in data. We discuss\nthese results in more detail in Appendix A.\n8\nRelated Work: Inductive bias\nOur experiments use transformer models in order\nto create a controllable testbed for understanding\nhuman language learning: by manually varying\nthe inductive bias through pretraining, we can test\nthe effect of different inductive biases in making\na learner fit for language learning. Our focus is\ndifferent, though related, to the focus of much\nof the work about inductive bias in transformers.\nWhile we set the inductive bias of our learners\nthrough pretraining, the built-in structural induc-\ntive biases of untrained deep neural network archi-\ntectures are unknown and difficult to assess (Ba-\nroni, 2022; Warstadt and Bowman, 2022). A rich\nline of past experiments work towards defining\nthis inductive bias of neural models through em-\npirical methods, looking at how much transform-\ners are biased towards learning hierarchical rules\n(Kharitonov and Chaabouni, 2021; Petty and Frank,\n2021; Mulligan et al., 2021) or tree-structured pro-\ncessing (Murty et al., 2023), or biases towards dif-\nferent typological linguistic structures (White and\nCotterell, 2021; Ravfogel et al., 2019; Ravishankar\nand Nivre, 2022), and how inductive bias changes\nwith language modeling pretraining (Mueller et al.,\n2022; Warstadt et al., 2020).\nWe build on these results by working to in-\nfluence, rather than measure, the inductive bias\nof transformers. Our experimental paradigm re-\nlies on structural transfer between pretraining and\nfine-tuning data in unrelated modalities, an effect\ndemonstrated by past research. We showed in pre-\nvious work how the structures of non-linguistic\nmodalities like music and code can predispose\nlearners for language learning (Papadimitriou and\nJurafsky, 2020), work that has been reproduced\n(Chiang and Lee, 2022; Ri and Tsuruoka, 2022),\nextended to other modalities like amino acid se-\nquences (Chiang and Lee, 2020), and between lan-\nguage and multiple symbolic tasks (Lu et al., 2022).\nIn a similar line of inquiry, Krishna et al. (2021)\nshow structural transfer from the abstract task struc-\nture of summarizing random data to summarizing\nreal natural language passages. The paradigm of\nstructural transfer lets us study the fundamental\ncognitive issue of inductive bias from an exciting\nangle: through causal experiments where we con-\ntrol the inductive bias of language learners.\n9\nDiscussion\nOur experiments and methodology provide a new\nview into the biases that make language learn-\ning possible in both human and artificial learn-\ners. We use transformer language learners and\nperform causal interventions that alter their induc-\ntive learning biases before training them on natural\nlanguages. Our findings show the relative strengths\nof different structural inductive biases: simple, reg-\nular structure is far outperformed by both context-\nfree and non-context-free structural relationships,\nbut a recursive structure is not necessarily the best\nsuch complex bias.\nWe obtain our results from experimenting on\nartificial language learners, rather than working\nwith humans or analyzing human language. In-\nductive bias experiments on artificial learners let\nus easily identify possible hypotheses of structural\ninductive biases in human cognition. More impor-\ntantly, since we are working with systems we can\ninfluence (rather than analyzing properties and uni-\nversals of language) we can test hypotheses that are\nnot dependent on linguistic theory-building. We\ncan therefore explore outside the well-studied hy-\npotheses in the linguistics literature.\nOur experiments address questions about the\nlearnability of human languages under different\nstructural inductive biases which have been pro-\nposed in the linguistics and cognitive science liter-\nature as bases for language. However, experiments\nsuch as ours cannot directly prove how the human\nlanguage system is structurally biased. What such\nexperiments can achieve is to shine light on lan-\nguage as a learnable system as a whole, and pro-\nduce hypotheses about the relevant structures that\ncould be guiding human language learning. In our\ncase, we find that recursion is indeed a strong in-\nductive bias for language learning. However, our\nexperiments also serve to point out that it’s not\nnesting constituent, structurally-recursive proper-\nties that necessarily most help language learning.\nUsing the CROSS language, we show that when\nwe take away recursion from the inductive bias,\nbut keep the overall framework of paired tokens\nthat connect in non-trivial ways, we get a better\ninductive learning bias. Thus, our work serves to\nshowcase possible alternatives to the Hauser et al.\n(2002) hypothesis that recursion is a necessary bias:\nother structurally-complex ways of relating tokens\nmay be just as strong. Differences such as those be-\ntween recursion and non-recursive complex struc-\ntures are hard to disentangle theoretically. As such,\nan experimental paradigm such as ours is a helpful\nstep in widely exploring the hypothesis space in\nquestions around human language cognition.\nArtificial models of language can never defini-\ntively prove facts about human language process-\ning. The models pretrained and fine-tuned in this\npaper are products of opaque optimization pro-\ncesses, and the results that we get do not neces-\nsarily match human learning. Nevertheless, our\nwork serves to identify and examine different hy-\npotheses about human inductive learning biases.\nWe hope that such work can help enrich the hypoth-\nesis space over which theoretical argumentation, as\nwell as human subject experiments, are conducted,\nultimately leading to a richer understanding of the\nhuman language learning process.\nLimitations\nAs discussed throughout the paper, our method uses\nlanguage models to better understand language\nlearning and cognitive inductive bias in humans,\nand this comes with all of the limitations of using\ncomputational schematic models of cognition to\nunderstand the complex and intractable problem of\nhuman cognition. Though strong artificial language\nlearners give us a tool with which to run control-\nlable language learning experiments, experiments\non artificial learners do not provide any proof about\nthe actual cognitive processes that happen in hu-\nmans, and our experiments cannot be taken to pro-\nvide any such proof. Our results serve to showcase\nthe properties of language as a learnable system\nunder different inductive bias constraints, and can\ninform hypotheses and theories that are worked\nout and evaluated on human language learning and\nhuman subjects.\nA limitation of our experiments is that we only\nevaluate models on natural language perplexity.\nPerplexity is a coarse-grained measure, and to ex-\npand these epxeriments we plan to further examine\nwhich specific parts of language production dif-\nferent models are doing better in. Understanding\nwhich inductive biases lead to which acquisition\npatterns, especially with respect to the acquisition\nof semantic and syntactic patterns, would be a very\ninteresting addition to these findings.\nLastly, our experiments evaluate language learn-\ning by fine-tuning on modeling wikitext, which\nis not similar to the learning environment of hu-\nman language. Our methodological contribution\nis independent from the actual data we use, and\none way our method could be applied would be in\nmore realistic language acquisition situations, us-\ning datasets of child-directed speech for fine-tuning\n(Warstadt et al., 2023). Understanding the effects\nof inductive biases in scenarios closer to human\nlanguage acquisition would be a great next step for\nthis paradigm.\nEthics Statement\nOur experiments address a cognitive question about\nlanguage learning by running in-vitro experiments\non language models. Such methodologies do not\nprovide definitive proof about any human cognitive\nprocesses. Instead, such experiments use language\nmodels in the way that computational models of\ncognition are generally used: in order to surface\nand experiment on interesting cognitive hypotheses.\nMethodologies such as ours, especially applied to\nmore narrow definitions of learning than language\nlearning, could become harmful if they are taken\nto provide actionable proof about human learning\npatterns.\nAcknowledgements\nWe would like to thank Michael Hahn, Philip\nResnik, John Hewitt, the members of the NYU\nCAP Lab, and the EMNLP anonymous reviewers\nfor enlightening discussions and comments on this\npaper, as well as all the members of the Jurafsky lab\nfor their consistent and inspiring research support.\nThis research was funded in part by NSF award\nnumber IIS-2128145.\nReferences\nMarco Baroni. 2022. On the proper role of linguistically\noriented deep net analysis in linguistic theorising. In\nAlgebraic Structures in Natural Language, pages 1–\n16. CRC Press.\nStephanie Chan, Adam Santoro, Andrew Lampinen,\nJane Wang, Aaditya Singh, Pierre Richemond, James\nMcClelland, and Felix Hill. 2022.\nData distribu-\ntional properties drive emergent in-context learning\nin transformers. In Advances in Neural Information\nProcessing Systems, volume 35, pages 18878–18891.\nCurran Associates, Inc.\nCheng-Han Chiang and Hung-yi Lee. 2020.\nPre-\ntraining a language model without human language.\narXiv preprint arXiv:2012.11995.\nCheng-Han Chiang and Hung-yi Lee. 2022.\nOn\nthe transferability of pre-trained language mod-\nels: A study from artificial datasets. Proceedings\nof the AAAI Conference on Artificial Intelligence,\n36(10):10518–10525.\nNoam Chomsky. 1959. On certain formal properties of\ngrammars. Information and Control, 2(2):137–167.\nNoam Chomsky. 1995. The minimalist program. MIT\npress.\nNick C Ellis and Matthew Brook O’Donnell. 2012. Sta-\ntistical construction learning: Does a zipfian problem\nspace ensure robust language learning. In Rebuschat\nPatrick and Williams John N., editors, Statistical\nlearning and language acquisition, number Vol. 1\nin Studies in Second and Foreign Language Educa-\ntion. De Gruyter Mouton.\nJeffrey L Elman. 1996. Rethinking innateness: A con-\nnectionist perspective on development, volume 10.\nMIT press.\nRobert Frank and Tim Hunter. 2021. Variation in mild\ncontext-sensitivity. Evolutionary Linguistic Theory,\n3(2):181–214.\nEdward Gibson, Richard Futrell, Steven P Piantadosi,\nIsabelle Dautriche, Kyle Mahowald, Leon Bergen,\nand Roger Levy. 2019. How efficiency shapes human\nlanguage. Trends in cognitive sciences, 23(5):389–\n407.\nMichael Hahn, Dan Jurafsky, and Richard Futrell. 2020.\nUniversals of word order reflect optimization of gram-\nmars for efficient communication. Proceedings of the\nNational Academy of Sciences, 117(5):2347–2353.\nMarc D. Hauser, Noam Chomsky, and W. Tecumseh\nFitch. 2002.\nThe faculty of language: What is\nit, who has it, and how did it evolve?\nScience,\n298(5598):1569–1579.\nJohn Hewitt. 2021. Initializing new word embeddings\nfor pretrained language models.\nAravind K Joshi. 1985. Tree adjoining grammars: How\nmuch context-sensitivity is required to provide rea-\nsonable structural descriptions? Natural language\nparsing: Psychological, computational and theoreti-\ncal perspectives, pages 206–250.\nAravind K Joshi, K Vijay Shanker, and David Weir.\n1990. The convergence of mildly context-sensitive\ngrammar formalisms. Technical Reports (CIS), page\n539.\nCarina Kauf, Greta Tuckute, Roger Levy, Jacob An-\ndreas, and Evelina Fedorenko. 2023.\nLexical se-\nmantic content, not syntactic structure, is the main\ncontributor to ann-brain similarity of fMRI responses\nin the language network. bioRxiv.\nEugene Kharitonov and Rahma Chaabouni. 2021. What\nthey do when in doubt: a study of inductive biases\nin seq2seq learners. In International Conference on\nLearning Representations.\nKundan Krishna, Jeffrey Bigham, and Zachary C. Lip-\nton. 2021. Does pretraining for summarization re-\nquire knowledge transfer? In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2021,\npages 3178–3189, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nShalom Lappin. 2021. Deep Learning and Linguistic\nRepresentation. Routledge.\nOri Lavi-Rotbain and Inbal Arnon. 2022. The learnabil-\nity consequences of zipfian distributions in language.\nCognition, 223:105038.\nElena Lieven and Michael Tomasello. 2008. Children’s\nfirst language acquistion from a usage-based perspec-\ntive. In Handbook of cognitive linguistics and second\nlanguage acquisition, pages 178–206. Routledge.\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mor-\ndatch. 2022. Frozen pretrained transformers as uni-\nversal computation engines. Proceedings of the AAAI\nConference on Artificial Intelligence, 36(7):7628–\n7636.\nBenoit Mandelbrot. 1953. An informational theory of\nthe statistical structure of language. Communication\ntheory, 84:486–502.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations.\nAaron Mueller, Robert Frank, Tal Linzen, Luheng Wang,\nand Sebastian Schuster. 2022. Coloring the blank\nslate: Pre-training imparts a hierarchical inductive\nbias to sequence-to-sequence models. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 1352–1368.\nKarl Mulligan, Robert Frank, and Tal Linzen. 2021.\nStructure here, bias there: Hierarchical generaliza-\ntion by jointly learning syntactic transformations. In\nProceedings of the Society for Computation in Lin-\nguistics 2021, pages 125–135, Online. Association\nfor Computational Linguistics.\nShikhar Murty, Pratyusha Sharma, Jacob Andreas, and\nChristopher D Manning. 2023. Characterizing intrin-\nsic compositionality in transformers with tree projec-\ntions. In The Eleventh International Conference on\nLearning Representations.\nIsabel Papadimitriou and Dan Jurafsky. 2020. Learn-\ning music helps you read: Using transfer to study\nlinguistic structure in language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n6829–6839.\nJackson Petty and Robert Frank. 2021.\nTrans-\nformers generalize linearly.\narXiv preprint\narXiv:2109.12036.\nSteven T Piantadosi. 2014. Zipf’s word frequency law\nin natural language: A critical review and future di-\nrections. Psychonomic bulletin & review, 21:1112–\n1130.\nEva Portelance. 2022. Neural Network Approaches to\nthe Study of Word Learning. Stanford University.\nShauli Ravfogel, Yoav Goldberg, and Tal Linzen. 2019.\nStudying the inductive biases of RNNs with synthetic\nvariations of natural languages. In Proceedings of\nNAACL-HLT, pages 3532–3542.\nVinit Ravishankar and Joakim Nivre. 2022. The ef-\nfects of corpus choice and morphosyntax on multilin-\ngual space induction. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n4130–4139.\nRyokan Ri and Yoshimasa Tsuruoka. 2022. Pretraining\nwith artificial language: Studying transferable knowl-\nedge in language models. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7302–\n7315.\nKathryn D. Schuler, Patricia A. Reeder, Elissa L. New-\nport, and Richard N. Aslin. 2017. The effect of zip-\nfian frequency variations on category formation in\nadult artificial language learning. Language Learn-\ning and Development, 13(4):357–374.\nStuart M. Shieber. 1985. Evidence against the context-\nfreeness of natural language. In Jack Kulas, James H.\nFetzer, and Terry L. Rankin, editors, Philosophy, Lan-\nguage, and Artificial Intelligence: Resources for Pro-\ncessing Natural Language, pages 79–89. Springer\nNetherlands, Dordrecht.\nAmir Shufaniya and Inbal Arnon. 2022. A Cognitive\nBias for Zipfian Distributions? Uniform Distributions\nBecome More Skewed via Cultural Transmission.\nJournal of Language Evolution, 7(1):59–80.\nEdward P Stabler. 2010. Computational perspectives\non minimalism. In Cedric Boeckx, editor, Oxford\nHandbook of Linguistic Minimalism, pages 616–641.\nOxford University Press.\nMark J. Steedman. 1990. Gapping as constituent coor-\ndination. Linguistics and Philosophy, 13:207–263.\nA. Warstadt and Samuel R. Bowman. 2022.\nWhat\nartificial neural networks can tell us about human\nlanguage acquisition. In Shalom Lappin and Jean-\nPhilippe Bernady, editors, Algebraic Structures in\nNatural Language. CRC Press.\nAlex Warstadt, Leshem Choshen, Aaron Mueller, Ad-\nina Williams, Ethan Wilcox, and Chengxu Zhuang.\n2023. Call for papers–the babylm challenge: Sample-\nefficient pretraining on a developmentally plausible\ncorpus. arXiv preprint arXiv:2301.11796.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu,\nand Samuel R. Bowman. 2020. Learning which fea-\ntures matter: RoBERTa acquires a preference for\nlinguistic generalizations (eventually). In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n217–235, Online. Association for Computational Lin-\nguistics.\nJennifer C White and Ryan Cotterell. 2021. Examining\nthe inductive bias of neural language models with ar-\ntificial languages. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing, volume 1, pages\n454–463. Association for Computational Linguistics.\nEthan Gotlieb Wilcox, Richard Futrell, and Roger Levy.\n2023. Using Computational Models to Test Syntactic\nLearnability. Linguistic Inquiry, pages 1–44.\nZhengxuan Wu, Alex Tamkin, and Isabel Papadimitriou.\n2023. Oolong: Investigating what makes transfer\nlearning hard with controlled studies. In \"Proceed-\nings of the 2023 Conference on Empirical Methods\nin Natural Language Processing (EMNLP)\".\nGeorge Kingsley Zipf. 1936. The Psychobiology Of\nLanguage. Routledge.\nA\nCombining syntactic structure and\nvocabulary distribution\nIn Experiment 3 (Section 7), we showed that a\nZipfian vocabulary distribution provides a stronger\nstructural inductive bias than a uniform distribution.\nHere, we present results in combining a Zipfian dis-\ntribution with the grammatical structural biases in\nExperiment 1. The results are shown in Figure 7,\nand do not definitively point one way or the other\nregarding combining grammatical structure and vo-\ncabulary distribution: a Zipfian pretraining distri-\nbution creates a stronger bias in some cases and a\nweaker bias in others. More well-powered and con-\ntrolled experiments, looking at carefully-chosen\ngrammatical structure to combine with vocabulary\ndistribution, in order to understand the interactions\nbetween these two aspects of structural information.\nFigure 7: Results for downstream perplexity in all three\nlanguages when combining vocabulary distribution bi-\nases with ).\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-04-25",
  "updated": "2023-10-29"
}