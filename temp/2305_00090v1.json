{
  "id": "http://arxiv.org/abs/2305.00090v1",
  "title": "NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis",
  "authors": [
    "Mingyang Wang",
    "Heike Adel",
    "Lukas Lange",
    "Jannik Strötgen",
    "Hinrich Schütze"
  ],
  "abstract": "This paper describes our system developed for the SemEval-2023 Task 12\n\"Sentiment Analysis for Low-resource African Languages using Twitter Dataset\".\nSentiment analysis is one of the most widely studied applications in natural\nlanguage processing. However, most prior work still focuses on a small number\nof high-resource languages. Building reliable sentiment analysis systems for\nlow-resource languages remains challenging, due to the limited training data in\nthis task. In this work, we propose to leverage language-adaptive and\ntask-adaptive pretraining on African texts and study transfer learning with\nsource language selection on top of an African language-centric pretrained\nlanguage model. Our key findings are: (1) Adapting the pretrained model to the\ntarget language and task using a small yet relevant corpus improves performance\nremarkably by more than 10 F1 score points. (2) Selecting source languages with\npositive transfer gains during training can avoid harmful interference from\ndissimilar languages, leading to better results in multilingual and\ncross-lingual settings. In the shared task, our system wins 8 out of 15 tracks\nand, in particular, performs best in the multilingual evaluation.",
  "text": "NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source\nLanguage Selection for Low-Resource Multilingual Sentiment Analysis\nMingyang Wang1,2, Heike Adel1, Lukas Lange1, Jannik Strötgen1,3, Hinrich Schütze2\n1 Bosch Center for Artiﬁcial Intelligence, Renningen, Germany\n2 Center for Information and Language Processing (CIS), LMU Munich, Germany\n3 Karlsruhe University of Applied Sciences, Karlsruhe, Germany\n{Mingyang.Wang2,Heike.Adel,Lukas.Lange}@de.bosch.com\nAbstract\nThis paper describes our system developed for\nthe SemEval-2023 Task 12 “Sentiment Analy-\nsis for Low-resource African Languages using\nTwitter Dataset”. Sentiment analysis is one of\nthe most widely studied applications in natu-\nral language processing. However, most prior\nwork still focuses on a small number of high-\nresource languages.\nBuilding reliable senti-\nment analysis systems for low-resource lan-\nguages remains challenging, due to the limited\ntraining data in this task. In this work, we pro-\npose to leverage language-adaptive and task-\nadaptive pretraining on African texts and study\ntransfer learning with source language selec-\ntion on top of an African language-centric pre-\ntrained language model. Our key ﬁndings are:\n(1) Adapting the pretrained model to the target\nlanguage and task using a small yet relevant\ncorpus improves performance remarkably by\nmore than 10 F1 score points. (2) Selecting\nsource languages with positive transfer gains\nduring training can avoid harmful interference\nfrom dissimilar languages, leading to better re-\nsults in multilingual and cross-lingual settings.\nIn the shared task, our system wins 8 out of 15\ntracks and, in particular, performs best in the\nmultilingual evaluation.\n1\nIntroduction\nIn recent years, natural language processing re-\nsearch has attracted considerable interest. How-\never, most studies remain conﬁned to a small num-\nber of languages with large amounts of training\ndata available. Low-resource languages, for exam-\nple, e.g., African languages, are still underrepre-\nsented although they are spoken by over a billion\npeople. In this context, the AfriSenti shared task\nprovides a Twitter dataset for sentiment analysis\non 14 African languages, promoting the future de-\nvelopment of this ﬁeld. The shared task consists\nof three sub-tasks: monolingual (Subtask A), mul-\ntilingual (Subtask B), and zero-shot cross-lingual\nsentiment analysis (Subtask C). A detailed descrip-\ntion can be found in the shared task description\npapers (Muhammad et al., 2023a,b).\nIn this paper, we describe our submission as Nei-\nther Language Nor Domain Experts (NLNDE)1 to\nthe AfriSenti shared task. Given the key challenge\nof limited training data, we ﬁrst adopt the language-\nadaptive and task-adaptive pretraining approaches\n(Gururangan et al., 2020), i.e., LAPT and TAPT, to\nadapt a pretrained language model to the language\nand task of interest. Further pretraining the model\nwith such smaller but more task-relevant corpora\nleads to performance gains in all subtasks.\nSecond, Cross-lingual transfer has been shown\nto be an effective method for enhancing the per-\nformance of low-resource languages by leveraging\none or more similar languages as source languages\n(Lin et al., 2019; Ruder and Plank, 2017; Nasir\nand Mchechesi, 2022). However, the 14 African\nlanguages covered in this shared task (see Table 1)\ncome from different language families and, there-\nfore, hold different linguistic characteristics. As\ndissimilar languages could hurt the transfer perfor-\nmance (Lin et al., 2019; Adelani et al., 2022), it\nis important to choose promising languages as the\ntransfer source. Therefore, our system uses trans-\nfer learning with an explicit selection of source\nlanguages. We apply this approach to the multilin-\ngual and zero-shot cross-lingual sentiment analysis\ntasks (Subtask B and C) and demonstrate that it ben-\neﬁts the performance of each target language. In\naddition, we investigate different source language\nselection strategies and show their impact on the\nﬁnal transfer performance.\nOur ﬁnal submission results are the ensemble of\nthe best models with different random seeds. Our\nsystem is ranked ﬁrst in 6 out of 12 languages in\nsubtask A (monolingual), achieves the ﬁrst place\nin subtask B (multilingual), and wins for one of\n1We neither know any African languages nor have prior\nknowledge of the Twitter domain dataset.\narXiv:2305.00090v1  [cs.CL]  28 Apr 2023\ntwo languages in subtask C for the zero-shot cross-\nlingual transfer.\n2\nSystem Overview\nOur system is based on the AfroXLM-R large\nmodel (Alabi et al., 2022), which applys multi-\nlingual adaptive ﬁne-tuning on XLM-R (Conneau\net al., 2020) with a special focus on African lan-\nguages.\nIn all three subtasks, we ﬁrst apply\nlanguage- and/or task-adaptive pretraining with\nlanguage- and/or task-speciﬁc data to tailor the\nvanilla AfroXLM-R to our setting. In subtasks B\nand C, after the adaptive pretraining, we perform\nsource language selection to improve the multilin-\ngual and cross-lingual transfer performance.\n2.1\nLanguage- and Task-Adaptive\nPretraining\nMost of the current NLP research is based on large\nlanguage models that have been pretrained on mas-\nsive amounts of heterogeneous corpora. Gururan-\ngan et al. (2020) demonstrate that it is helpful to\nfurther tailor a pretrained model to the domain of\nthe target task. They show that continued pretrain-\ning with domain-speciﬁc and task-speciﬁc data con-\nsistently improves performance on domain-speciﬁc\ntasks across different domains and tasks. Specif-\nically, they introduce domain-adaptive pretrain-\ning (DAPT), i.e., the continued pretraining of the\nbase model on a large corpus of unlabeled domain-\nspeciﬁc text. Analogously, task-adaptive pretrain-\ning (TAPT) refers to adapting the pretrained model\nto the task’s unlabeled training data.\nA natural extension is the application of this\nmethod to multilingual scenarios. Considering dif-\nferent languages as different domains, language-\nadaptive pretraining can be viewed as a special\ncase of domain-adaptive pretraining. Therefore,\nwe also explore two types of continued pretraining:\nFirst, we pretrain the base model with a language-\nspeciﬁc corpus, which we will refer to as language-\nadaptive pretraining (LAPT).2 Second, we adapt\nthe language model on the unlabeled task dataset,\ni.e., perform task-adaptive pretraining (TAPT).\nFor the language-speciﬁc pretraining, we col-\nlect open-source corpora from the multi-domain\nLeipzig Corpus Collection (Goldhahn et al., 2012),\ncovering Wikipedia, Community, Web, and News\ncorpora. Note that the ﬁnal set of monolingual cor-\n2In some works (Dossou et al., 2022; Alabi et al., 2022),\nLAPT is also called language-adaptive ﬁne-tuning (LAFT).\npora depends on their availability. There is not a\nsingle corpus covering all these languages. Table\n1 provides a summary of the monolingual corpora\nwe used for our language-adaptive pretraining.\n2.2\nTransfer Learning and Source Selection\nTable 1 provides an overview of the languages\ncovered in the shared task.\nThey come from\nfour language families (Afro-Asiatic, Niger-Congo,\nEnglish-Creole and Indo-European) and, therefore,\nhave different linguistic characteristics. Even in-\nside the same language family, languages can still\nexhibit distinct linguistic features. For example, al-\nthough many languages are from the Niger-Congo\nfamily (6 out of 14 languages), afﬁxes are very\ncommon in Bantu languages (a subgroup of Niger-\nCongo), but are not typically used in non-Bantu\nsubgroups like Volta-Niger and Kwa.\nPrevious work has demonstrated that it can still\nbe beneﬁcial to leverage one or more similar lan-\nguages for cross-lingual transfer learning to the\ntarget language (Lin et al., 2019). Nevertheless,\nlanguages that are dissimilar to the target language\ncould also hinder performance (Adelani et al.,\n2022; Lange et al., 2021). Therefore, it is crucial\nto properly select source languages to improve the\ntransfer results on the target language.\nIn this work, we use transfer learning with se-\nlected sources for Subtask B and C, as they both\ninvolve transferring from multiple languages to a\ntarget language. For source language selection, we\nperform forward and backward source language se-\nlection, similar to the corresponding feature selec-\ntion approaches (Tsamardinos and Aliferis, 2003;\nBorboudakis and Tsamardinos, 2019).3 Essentially,\nfeature selection is deﬁned as the problem of se-\nlecting a minimal-size subset of features that leads\nto an optimal, multivariate predictive model for a\ntarget of interest (Tsamardinos and Aliferis, 2003).\nIn our task, we consider each candidate source lan-\nguage as a feature. For each target language, we\naim to ﬁlter out irrelevant or harmful source lan-\nguages and only keep the beneﬁcial languages as\nthe transfer source.\nForward feature selection usually starts with\nan empty set of features and adds variables to it,\nwhile backward feature selection starts with a com-\nplete set of variables and then excludes variables\nfrom it. In particular, for forward language selec-\ntion, given a target language Lt, we start with a\n3also known as variable selection.\nLanguage\nFamily\nLAPT Source\nSize (MB)\nNo. of Sent\nFwd. source\nBwd. source\nSubtask A and B\nAmharic (am)\nAfro-Asiatic / Semitic\nWikipedia\n17\n12,861\ndz, ha, kr, ma\nkr, ma\nAlgerian Arabic (dz)\nAfro-Asiatic / Semitic\nNews\n15.3\n59,998\nha, kr, ma\nkr, ts, twi, ma, am\nHausa (ha)\nAfro-Asiatic / Chadic\nWikipedia\n6.5\n59,664\nkr, twi, dz, pcm\nkr, ts, twi, dz, am\nIgbo (ig)\nNiger-Congo / Volta-Niger\nWikipedia\n2.2\n17,786\nsw, pcm, dz, yo\nkr. ts, am, ma, dz\nKinyarwanda (kr)\nNiger-Congo / Bantu\nCommunity\n7.9\n60,480\n-\nma, am, ts, dz, twi\nMoroccan Arabic (ma)\nAfro-Asiatic / Semitic\nNews\n7.4\n29,997\nyo, dz, pcm, ha\nkr, ts, am, dz, twi\nNigerian Pidgin (pcm)\nEnglish-Creole\nCommunity\n0.4\n7,126\ndz\nkr, ts, twi, dz, ma\nMozambican Portuguese (pt)\nIndo-European\nWeb\n27\n197,340\ndz, yo, ma, am\nkr, ma, am\nSwahili (sw)\nNiger-Congo / Bantu\nWikipedia\n3.2\n28,945\nyo, dz, pcm, ha\nma, ts, yo, pcm\nXitsonga (ts)\nNiger-Congo / Bantu\nWeb, Community\n3.6\n30,513\nha, ma, pcm, pt\nkr, twi, pcm, dz\nTwi (tw)\nNiger-Congo / Kwa\nWikipedia\n3.4\n2,478\nig, kr\nkr, ts, pcm, dz\nYoruba (yo)\nNiger-Congo / Volta-Niger\nWikipedia, Community\n4.5\n41,291\n-\nkr, ts, twi, dz\nSubtask C\nOromo (or)\nAfro-Asiatic / Cushitic\nkr, ha, yo, ts\nyo, pt, ts\nTigrinya(tg)\nAfro-Asiatic / Semitic\nha, kr, am, ma, pt\npt, yo, ha\nTable 1: Language information, pretraining corpora statistics and forward/backward source language selection\nresults. Source corpora for language-adaptive pretraining(LAPT) sources are collected from Leipzig Corpus Col-\nlection (Goldhahn et al., 2012). The source language selection process is described in Section 2.2. The selection\nis based on the weighted F1 as transfer score averaged over 5 random seeds.\nset Sfwd = {Lt} containing only the target lan-\nguage. We then add each of the other languages\nLsi, i = 1 . . . N −1 at a time and obtain N −1\nbilingual sets {(Lt, Lsi)}i=1...N−1, each with the\ntarget language Lt and one source language Lsi.\nN refers to the total number of given languages.\nWe experiment with each bilingual language set\nto build the training dataset for transfer learning.\nAdditionally, we run N monolingual experiments\n(one per target language) and use the monolingual\nperformance as the baseline to determine if a candi-\ndate source language leads to positive or negative\ntransfer gains. To be more speciﬁc: If a bilingual\nlanguage set (Lt, Lsi) yields a score of more than\n5% above the monolingual performance with Lt,\nwe consider Lsi as a positive source with respect\nto the target language Lt.\nFor backward selection, we start with the com-\nplete language set with all N languages. For each\ntarget language, we exclude each of the other N −1\nlanguages and get N −1 language sets, denoted\nas {(Lt, Ls1 . . . Lsi−1, Lsi+1 . . . LsN−1)}i=1...N−1.\nTo get a baseline performance for comparison, we\nrandomly select 500 samples from each language to\nbuild a small multilingual set. We choose a constant\nnumber of samples per language to avoid side ef-\nfects of the data size on the performance. Given the\nset of all languages, we remove each language at a\ntime and compare it with the baseline results from\nthe complete language set to investigate the transfer\ngain of each candidate language on the ﬁnal perfor-\nmance. If the performance from the language set\n(Lt, Ls1 . . . Lsi−1, Lsi+1 . . . LsN−1) is more than\n5% below the baseline, it shows that the absence of\nLsi has a large negative impact on performance.\nFor each of the N languages, we need to run\nN −1 experiments with the bilingual language\nset and 1 baseline experiment with the monolin-\ngual dataset. Therefore, for forward source lan-\nguage selection, we need to run N × N transfer\nexperiments and then select the source languages\nwith positive transfer gains corresponding to each\ntarget language via the performance comparison.\nSimilarly, for backward source language selection,\nN × N experiments are required for the source\nlanguage selection.\nWe apply both selection strategies for subtasks B\nand C. In subtask C, the language sets do not con-\ntain the target language Lt (as it is a zero-shot task).\nIn particular, for forward selection, this means that\nwe start with an empty set. For the same reason,\nWe run experiments with the complete datasets\nof all languages as the baseline for both forward\nand backward selection, as there is no monolingual\ndataset for the target language in subtask C. Results\nfor the source language selection of Subtask B and\nC are given in Table 1.\n3\nExperimental Setup\nWe now provide details on our preprocessing steps,\nthe language models and their training.\n3.1\nData Preprocessing\nWe preprocess the raw input tweets by removing ex-\ntra whitespaces, incorrect repeated characters and\npunctuation. Similar to Nguyen et al. (2020), we\nreplace all URLs with “HTTPURL” and username\nmentions with “USER” as they have little to no\nimpact on sentiment analysis. When analyzing the\nModel\nam\ndz\nha\nig\nkr\nma\npcm\npt\nsw\nts\ntwi\nyo\naverage\nAfroXLM-R\n31.60\n60.95\n80.78\n68.82\n70.34\n43.15\n48.27\n62.60\n58.96\n47.39\n36.59\n55.67\n62.91\nAfroXLM-R with adaptive pretraining\nLAPT\n47.26\n66.48\n79.53\n80.40\n70.15\n48.16\n67.22\n69.29\n62.66\n57.92\n56.81\n76.39\n69.56\nTAPT\n61.92\n68.28\n81.61\n81.49\n70.40\n63.58\n70.08\n71.07\n63.38\n38.96\n67.10\n78.03\n73.49\nLAPT + TAPT\n63.87\n67.63\n80.74\n81.45\n70.67\n61.7\n69.91\n70.66\n64.04\n59.49\n66.49\n78.57\n73.23\nSubmitted systems\nking001\n69.77\n73.00\n81.11\n81.39\n60.26\n57.94\n75.75\n73.53\n64.89\n68.28\n56.26\n80.16\n73.82\nPALI\n65.56\n72.62\n81.10\n81.30\n69.61\n55.92\n75.16\n73.83\n64.37\n67.58\n56.26\n80.06\n73.64\nstce\n65.56\n71.72\n80.99\n81.37\n69.61\n55.42\n75.30\n73.57\n64.37\n67.58\n56.26\n80.08\n73.56\nUM6P\n72.18\n72.02\n82.04\n81.51\n70.71\n60.15\n69.14\n67.35\n60.26\n66.98\n56.13\n76.01\n73.31\nNLNDE (ours)\n64.04\n69.98\n82.62\n82.96\n72.63\n64.82\n71.93\n72.90\n65.67\n60.70\n67.51\n79.95\n74.78\nTable 2: Performance on subtask A: Monolingual sentiment analysis. We apply LAPT, TAPT and a combination of\nboth on top of the AfroXLM-R model. All adaptive pertraining methods signiﬁcantly improve performance. TAPT\nachieves the best overall F1 score, and also the best on 7 out of 12 languages. We calculate the average F1 scores\nof each model (weighted based on the number of samples of each language). For the submission as NLNDE, we\nensemble different random seeds of all three models with adaptive pretraining and achieve even better performance.\ndata, we noticed a small portion of samples over-\nlapped in the train and dev sets for some languages.\nTherefore, to measure the actual generalizability of\nour models, we remove all the overlapping samples\nfrom the dev set. We will use dev set* to denote\nthe processed dev set in the following.\n3.2\nPretrained Language Models\nLarge multilingual pretrained language models\n(PLMs) like mBERT (Devlin et al., 2019) and\nXLM-R (Conneau et al., 2020) have shown im-\npressive capability on many languages for a variety\nof downstream NLP tasks. They are also often\nused as initialization checkpoints for adapting to\nother languages, such as AfroXLM-R, which is ini-\ntialized from XLM-R and specialized to African\nlanguages. In initial experiments, we compare the\nperformance of several multilingual PLMs, includ-\ning BERT and XLM-R which are trained on hun-\ndreds of languages, and AfroLM (Dossou et al.,\n2022), Afro-XLM-R (Alabi et al., 2022) as African\nlanguage-speciﬁc models. AfroXLM-R performs\nbest across all three subtasks. Therefore, we se-\nlect AfroXLM-R large as our base model and ap-\nply adaptive pretraining and source language se-\nlection on top of it. In addition, in subtask C, we\nexperiment with translating the tweets into English\nand apply BERTweet (Nguyen et al., 2020), a pre-\ntrained language model for English tweets.\n3.3\nTraining Details\nFor task- and language-adaptive pretraining, we\nuse the AdamW optimizer (Loshchilov and Hut-\nter, 2017) with a learning rate of 5e-5 and a batch\nsize of 8. For ﬁne-tuning, we use Adam with a\nlearning rate of 2e-5 and a batch size of 32. In\nboth phases, we use a maximum sequence length\nof 128. The training was done on Nvidia A100 and\nV100 GPUs.4 The results are evaluated using the\nweighted F1 score on the test set averaged over 5\nrandom seeds. The ﬁnal submission comes from\nthe majority vote ensemble of different random\nseeds of the best models.\n4\nResults\nIn this section, we report our results on the three\nsubtasks and discuss our ﬁndings and observed\nlimitations of the current work. Our evaluation is\nbased on the weighted F1 score on the test set av-\neraged over 5 random seeds. We use the majority\nvote method to ensemble our models from differ-\nent random seeds for submission, we provide the\nﬁnal submission results, as well as the results from\nseveral top-ranked systems in the last lines in Table\n2 ∼4. We refer to Appendix A.1 for the results on\nthe development set.\n4.1\nSubtask A: Monolingual Sentiment\nAnalysis\nIn subtask A, we mainly study the impact of adap-\ntive pretraining on monolingual sentiment analy-\nsis. We use the off-the-shelf AfroXLM-R large\nmodel as our baseline and ﬁne-tune it on the train-\ning dataset of each language, yielding one ﬁne-\ntuned model per language. Then, we apply LAPT,\nTAPT and their combination on top of AfroXLM-\nR. For combined LAPT and TAPT, we begin with\nAfroXLM-R and apply LAPT then TAPT for the\nmodel adaptation. After pretraining, we ﬁne-tune\n4All experiments ran on a carbon-neutral GPU cluster.\nthe adapted model on each monolingual dataset for\nsentiment analysis.\nAs shown in Table 2, the performance is re-\nmarkably improved with adaptive pretraining for\nmost languages, especially with task-adaptive pre-\ntraining (TAPT), which leads to a performance\ngain of 10.58 F1 score on average. LAPT also\nincreases the performance in general, but does not\ncontribute that much in comparison and even de-\ngrades the performance for the languages Hausa\n(ha) and Kinyarwanda (kr). We speculate that, on\nthe one hand, we use relatively small language-\nspeciﬁc corpora for LAPT as the covered African\nlanguages are indeed low-resource. In contrast, Gu-\nrurangan et al. (2020) used much larger adaptation\ncorpora for domain-speciﬁc pretraining (DAPT).\nOn the other hand, the mismatch of text domains\nmight be another reason: As described in Section\n2.1, we use corpora from domains, such as news\nand Wikipedia for LAPT, while the actual task\ndataset consists of multilingual tweets involving\nmany Twitter-speciﬁc factors, such as code-mixing,\nmisspellings, emojis, or hashtags.\nCombining LAPT and TAPT also shows promis-\ning results. However, as analyzed before, we hy-\npothesize that most of the beneﬁts come from the\nmore effective TAPT.\n4.2\nSubtask B: Multilingual Sentiment\nAnalysis\nIn the multilingual subtask, we categorize our ex-\nperiments into three groups: (1) multilingual train-\ning of a single model, (2) monolingual training of\nlanguage-speciﬁc models and (3) transfer learning\nwith selected sources. They differ in the composi-\ntion of the training datasets. In multilingual train-\ning, we use all training data from 12 languages. In\nmonolingual training, we use the same language-\nspeciﬁc models as in Subtask A (see Section 4.1)\nand combine the predictions in the end. In transfer\nlearning with selected sources, we perform forward\nand backward source selection as described in Sec-\ntion 2.2. With the selected source languages given\nin Table 3, we build the respective training datasets\nand ﬁne-tune the model for each language. As\na baseline, we further group languages based on\ntheir language family. This results in four groups,\nnamely Afro-Asiatic, Niger-Congo, English-Creole\nand Indo-European, details are given in Table 1.\nOur multilingual sentiment analysis results are\ngiven in Table 3. First, as in Subtask A, task-\nModel\nOverall F1\nMultilingual\n48.41\n+ TAPT\n70.65\nMonolingual\n62.91\n+ LAPT\n69.56\n+ TAPT\n73.49\n+ LAPT & TAPT\n73.23\nTransfer with selected sources\nLanguage family grouping\n61.81\nFwd source transfer\n66.73\nFwd source transfer + TAPT\n73.50\nBwd source transfer\n66.41\nBwd source transfer + TAPT\n74.08\nSubmitted systems\nking001\n74.96\nDN\n72.55\nymf924\n72.34\nmitchelldehaven\n72.33\nNLNDE (ours)\n75.06\nTable 3: Performance on subtask B: Multilingual senti-\nment analysis. We conduct (1) multilingual (2) mono-\nlingual and (3) transfer experiments. The single model\nwith TAPT and backward language selection achieves\nthe best overall results with an F1 score of 74.08. For\nthe submission as NLNDE, we ensemble different ran-\ndom seeds of models with source selection and TAPT\n(forward and backward) and get the ﬁnal results.\nadaptive pretraining notably improves classiﬁca-\ntion performance in all task settings. Combining\nLAPT and TAPT is not better than TAPT only.\nTherefore, due to time constraints, we apply LAPT\nand LAPT+TAPT only in monolingual training, but\nnot in multilingual training and the transfer learn-\ning with selected sources.\nSecond, ﬁne-tuning the model individually to\neach target language (monolingual training) out-\nperforms the joint multilingual training, in both\nthe vanilla training (62.91 vs. 48.41) and adap-\ntive pretraining (73.49 vs. 70.65) cases. Further-\nmore, selecting source languages with positive\ngains for each target language can further enhance\nperformance over monolingual training. Group-\ning languages based on their language families\n(61.81) shows better results than multilingual train-\ning (48.41), but it underperforms monolingual train-\ning (62.91) and falls behind the transfer learning\nwith selected sources (66.73 and 66.41) by around\n5%. Forward and backward source selection gives\ndifferent results, but they both contribute to the\nﬁnal results.\nFinally, another interesting ﬁnding is that, in the\npresence of TAPT, the advantage of specifying lan-\nguages as training data, i.e., in the cases of mono-\nModel\nor\ntg\nMultilingual\n40.16\n54.35\nMultilingual + TAPT\n37.68\n59.25\nTransfer with selected sources\nTOP3 source fwd\n36.81\n55.84\nTOP3 source fwd + TAPT\n42.43\n62.94\nTOP3 source bwd\n41.79\n66.77\nTOP3 source bwd + TAPT\n43.50\n67.32\nBERTweet with English translation\nBERTweet\n36.84\n38.27\nBERTweet + TAPT\n40.93\n64.09\nSubmitted systems\nmitchelldehaven\n46.23\n66.96\nUCAS\n45.82\n70.47\nymf924\n45.34\n70.39\nUM6P\n45.27\n69.53\nNLNDE (ours)\n44.97\n70.86\nTable 4: Performance on subtask C: Zero-shot cross-\nlingual sentiment analysis.\nWe experiment with (1)\nmultilingual transfer, (2) transfer with selected sources,\nand (3) BERTweet with English translations.\nThe\nmodel with TAPT and backward source language se-\nlection achieves the best overall results, which again\ndemonstrates the effectiveness of the two approaches.\nThe submission results come from the ensemble of dif-\nferent seeds of models with source selection and TAPT\n(forward and backward).\nlingual training and transfer learning with selected\nsources, becomes less pronounced. Speciﬁcally,\nwithout TAPT, multilingual training achieves an F1\nscore of 48.41, while monolingual training achieves\n62.91 and transfer learning with selected sources\nachieves 66.73 and 66.41. However, with TAPT,\nthe multilingual training shows a large improve-\nment, yielding an F1 score of 70.65. Although\nmonolingual (73.49) and transfer with selected lan-\nguages (73.50 and 74.08) still outperform the multi-\nlingual result, they become less advantageous. We\nsuppose this is because, with task-adaptive pretrain-\ning, the model already adapts to the target language\ncompared with the vanilla model pretrained on a\nlarger language set. As a result, the effect of addi-\ntionally specifying the source languages is limited.\n4.3\nSubtask C: Zero-shot Cross-Lingual\nSentiment Analysis\nThe zero-shot cross-lingual transfer task is particu-\nlarly challenging, especially when both the source\nand target languages are low-resource. In this sub-\ntask, we also employ different strategies: (1) mul-\ntilingual transfer, (2) transfer with selected source\nand (3) BERTweet with English-translated samples.\nFirst, we perform multilingual training, i.e., use\nall available training datasets from subtask A to\nﬁne-tune the AfroXLM-R model. We also perform\ntask-adaptive pretraining with unlabeled multilin-\ngual texts here, as in the previous two subtasks.\nSecond, we perform forward and backward\nsource language selection for the cross-lingual\ntransfer (as detailed in Section 2.2). Here, we use\nthe top 3 selected languages as the transfer source,\nas they show better performance than using all se-\nlected languages as the source in practice. We\napply TAPT by using the unlabelled task-speciﬁc\ndata from selected sources and the target language.\nFinally, we experiment with translating all\ntweets from the 14 languages to English using the\npygoogletranslate API.5 We investigate how the\nEnglish BERTweet model performs for sentiment\nanalysis. We also perform TAPT on the BERTweet\nmodel to adapt it to the unlabelled translated En-\nglish dataset and then ﬁne-tune the model with the\nlabelled translated English dataset.\nThe results of subtask C are given in Table 4. As\nin the previous subtasks, task-adaptive pretraining\nlargely improves performance in all settings. Sec-\nond, in 7 out of 8 cases, transfer learning with only\nselected source languages outperforms the multi-\nlingual counterparts trained on all languages. The\nmodel with a combination of TAPT and backward\nsource language selection achieves the best over-\nall results, which demonstrates the effectiveness of\nboth strategies in subtask C.\nSentiment analysis based on English translations\nshows competitive performance, but still underper-\nforms transfer learning with source selection. One\npossible reason could be that the translation qual-\nity is not good enough to accurately translate all\nrelevant words with emotional meanings.\n4.4\nDiscussion\nIn summary, our work shows that adaptive pretrain-\ning and transfer learning with source language se-\nlection are effective approaches to tackle sentiment\nanalysis in low-resource languages. Speciﬁcally,\nwe demonstrate that (1) adaptive pretraining, espe-\ncially task-adaptive pretraining, is generally effec-\ntive across different subtasks and task settings, and\n(2) transfer learning with source language selection\nleads to better results than monolingual training.\nUsing only source languages with positive transfer\ngains for training increases the available training\ndata size on the one hand, and avoids interference\n5https://github.com/Saravananslb/py-googletranslation\nfrom dissimilar languages on the other hand. No-\ntably, forward and backward source selection out-\nperform groupings based on language families in\nour multilingual experiments.\n5\nLimitation and Future Work\nOne limitation of our work is that the forward and\nbackward selection strategies require a lot of com-\nparative experiments to determine if a candidate\nlanguage has a positive or negative effect on the tar-\nget language. For N languages, we need to perform\nN ∗N transfer experiments for the comparison (as\ndescribed in Section 2.2). How to automatically\nselect source languages with little manual work is\nan interesting research question for future work.\nAdditionally, we found that forward and back-\nward source selection produce different source\nlanguage results and thus show different transfer\nscores. In our experiments, neither method com-\npletely outperformed the other. We have no conclu-\nsive answer to which method is better. Also, We\nhave not conducted an in-depth study on the rela-\ntionship between the selected sources for the target\nlanguage and their linguistic correlation. This is\nanother limitation of the current work that could be\naddressed in future research – in particular when\ninvolving language experts.\n6\nRelated Work\nAfrican language-centric PLMs.\nLarge multi-\nlingual PLMs, such as mBERT (Devlin et al., 2019)\nand XLM-R (Conneau et al., 2020) cover more\nthan 100 languages for natural language process-\ning tasks and exhibit good generalization abili-\nties over a large number of languages. However,\nmost of them include few African languages due\nto the lack of large open-source monolingual cor-\npora (Hedderich et al., 2021). Prior work devel-\noped African language-centric PLMs to address\nthis under-representation. Among them, AfriB-\nERTa (Ogueji et al., 2021) uses the RoBERTa\n(Zhuang et al., 2021) architecture and trains the\nmodel from scratch with corpora from 11 African\nlanguages. AfroLM (Dossou et al., 2022) proposes\nto use a novel self-active learning framework and\nthe model is trained from scratch on 23 African\nlanguages. Another strategy is to initialize a lan-\nguage model from an existing model and continue\nto train it with a special focus on African languages.\nAfroXLM-R (Alabi et al., 2022) performs multilin-\ngual adaptive ﬁne-tuning based on XLM-R on 17\nhighest-resourced African languages and 3 other\nhigh-resource languages spoken on the African con-\ntinent. AfroXLM-R performs well on African lan-\nguage tasks, such as named entity recognition and\nsentiment analysis (Alabi et al., 2022; Dossou et al.,\n2022). We therefore use it as the base model in this\nshared task.\nAdaptive pertaining.\nGururangan et al. (2020)\ndemonstrate that it is helpful to further tailor a pre-\ntrained model to a target domain and task. In par-\nticular, they introduce domain-adaptive pretraining,\nwhich continues the pretraining of the model on\ndomain-speciﬁc unlabeled data, and task-adaptive\npretraining, which further pretrains the model on\nthe task’s unlabeled data. Experimental results\nshow that these two strategies lead to remarkable\nperformance gains. We adopt this idea to our tasks.\nTransfer learning with source selection.\nSe-\nlecting data for transfer learning has been ex-\nplored in different prior work, i.e., Ruder and Plank\n(2017); Lin et al. (2019); Lange et al. (2022). For\nexample, Ruder and Plank (2017) learn to select\npositive sources using Bayesian optimization. Lan-\ngRank (Lin et al., 2019) considers the source lan-\nguage selection for transfer learning as a ranking\nproblem. They train a ranking model to select lan-\nguages with a positive transfer gain from a larger\nset of possible languages. In contrast, we adopt\nthe idea of forward and backward feature selection\n(Tsamardinos and Aliferis, 2003) and use a much\nsimpler approach based on transfer score compari-\nson to select source languages.\n7\nConclusion\nIn this work, we introduce our sentiment analy-\nsis system for the AfriSenti shared task, which\nis ranked ﬁrst in 8 out of 15 tracks and performs\ncompetitively on the others. It consists of language-\nadaptive and task-adaptive pretraining on top of the\nAfroXLM-R model, together with transfer learning\nwith source language selection. We demonstrate\nthat tailoring the pretrained model to the target\nlanguage and task considerably improves the per-\nformance across all task settings. Additionally,\ntransfer learning with source language selection\nfurther improves the results in the multilingual and\nzero-shot cross-lingual tasks by avoiding potential\nnegative transfer gains from dissimilar languages.\nA future research direction is to automatically se-\nlect source languages with positive transfer gains\nwithout the need of manually comparing the source-\nto-target transfer score.\nAcknowledgments\nWe thank the AfriSenti organizers for their time to\nprepare the data for a large variety of languages and\nrun the competition in a smooth way. The shared\ntask provides an excellent platform for researchers\nto collaborate and share their knowledge, and also\npromotes the future development in the ﬁeld of\nNLP for African languages.\nReferences\nDavid Adelani, Graham Neubig, Sebastian Ruder,\nShruti Rijhwani, Michael Beukman, Chester Palen-\nMichel, Constantine Lignos, Jesujoba Alabi, Sham-\nsuddeen\nMuhammad,\nPeter\nNabende,\nCheikh\nM. Bamba Dione,\nAndiswa Bukula,\nRoowei-\nther Mabuya, Bonaventure F. P. Dossou, Bless-\ning Sibanda, Happy Buzaaba, Jonathan Mukiibi,\nGodson Kalipe, Derguene Mbaye, Amelia Tay-\nlor, Fatoumata Kabore, Chris Chinenye Emezue,\nAnuoluwapo Aremu, Perez Ogayo, Catherine Gi-\ntau,\nEdwin Munkoh-Buabeng,\nVictoire Memd-\njokam Koagne, Allahsera Auguste Tapo, Tebogo\nMacucwa, Vukosi Marivate, Mboning Tchiaze Elvis,\nTajuddeen Gwadabe, Tosin Adewumi, Orevaoghene\nAhia,\nJoyce Nakatumba-Nabende,\nNeo Lerato\nMokono, Ignatius Ezeani, Chiamaka Chukwuneke,\nMofetoluwa Oluwaseun Adeyemi, Gilles Quentin\nHacheme, Idris Abdulmumin, Odunayo Ogundepo,\nOreen Yousuf, Tatiana Moteu, and Dietrich Klakow.\n2022.\nMasakhaNER 2.0: Africa-centric transfer\nlearning for named entity recognition. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 4488–4508,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius\nMosbach, and Dietrich Klakow. 2022. Adapting pre-\ntrained language models to African languages via\nmultilingual adaptive ﬁne-tuning.\nIn Proceedings\nof the 29th International Conference on Computa-\ntional Linguistics, pages 4336–4349, Gyeongju, Re-\npublic of Korea. International Committee on Com-\nputational Linguistics.\nGiorgos Borboudakis and Ioannis Tsamardinos. 2019.\nForward-backward selection with early dropping.\nThe\nJournal\nof\nMachine\nLearning\nResearch,\n20(1):276–314.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nIn\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. ArXiv, abs/1810.04805.\nBonaventure F. P. Dossou,\nAtnafu Tonja,\nOreen\nYousuf, Salomey Osei, Abigail Oppong, Iyanuoluwa\nShode, Oluwabusayo Olufunke Awoyomi, and Chris\nEmezue. 2022.\nAfroLM: A self-active learning-\nbased multilingual pretrained language model for 23\nAfrican languages.\nIn Proceedings of The Third\nWorkshop on Simple and Efﬁcient Natural Language\nProcessing (SustaiNLP), pages 52–64, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for\nComputational Linguistics.\nDirk Goldhahn, Thomas Eckart, Uwe Quasthoff, et al.\n2012.\nBuilding large monolingual dictionaries at\nthe leipzig corpora collection: From 100 to 200 lan-\nguages. In LREC, volume 29, pages 31–43.\nSuchin\nGururangan,\nAna\nMarasovi´c,\nSwabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. arXiv\npreprint arXiv:2004.10964.\nMichael A. Hedderich, Lukas Lange, Heike Adel, Jan-\nnik Strötgen, and Dietrich Klakow. 2021. A survey\non recent approaches for natural language process-\ning in low-resource scenarios. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 2545–2568, On-\nline. Association for Computational Linguistics.\nLukas Lange, Heike Adel, Jannik Strötgen, and Diet-\nrich Klakow. 2022. CLIN-X: pre-trained language\nmodels and a study on cross-task transfer for con-\ncept extraction in the clinical domain. Bioinformat-\nics, 38(12):3267–3274.\nLukas Lange, Jannik Strötgen, Heike Adel, and Diet-\nrich Klakow. 2021. To share or not to share: Pre-\ndicting sets of sources for model transfer learning.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n8744–8753, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nYu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li,\nYuyan Zhang, Mengzhou Xia, Shruti Rijhwani,\nJunxian He, Zhisong Zhang, Xuezhe Ma, Antonios\nAnastasopoulos, Patrick Littell, and Graham Neubig.\n2019. Choosing transfer languages for cross-lingual\nlearning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 3125–3135, Florence, Italy. Association\nfor Computational Linguistics.\nIlya Loshchilov and Frank Hutter. 2017.\nDecou-\npled weight decay regularization.\narXiv preprint\narXiv:1711.05101.\nShamsuddeen Hassan Muhammad, Idris Abdulmumin,\nAbinew Ali Ayele, Nedjma Ousidhoum, David Ife-\noluwa Adelani, Seid Muhie Yimam, Ibrahim Sa’id\nAhmad, Meriem Beloucif, Saif M. Mohammad, Se-\nbastian Ruder, Oumaima Hourrane, Pavel Brazdil,\nFelermino Dário Mário António Ali, Davis David,\nSalomey Osei, Bello Shehu Bello, Falalu Ibrahim,\nTajuddeen Gwadabe, Samuel Rutunda, Tadesse\nBelay, Wendimu Baye Messelle, Hailu Beshada\nBalcha, Sisay Adugna Chala, Hagos Tesfahun Ge-\nbremichael, Bernard Opoku, and Steven Arthur.\n2023a.\nAfriSenti: A Twitter Sentiment Analysis\nBenchmark for African Languages.\nShamsuddeen Hassan Muhammad, Idris Abdulmu-\nmin, Seid Muhie Yimam, David Ifeoluwa Ade-\nlani, Ibrahim Sa’id Ahmad, Nedjma Ousidhoum,\nAbinew Ali Ayele, Saif M. Mohammad, Meriem\nBeloucif, and Sebastian Ruder. 2023b.\nSemEval-\n2023 Task 12: Sentiment Analysis for African Lan-\nguages (AfriSenti-SemEval).\nIn Proceedings of\nthe 17th International Workshop on Semantic Eval-\nuation (SemEval-2023). Association for Computa-\ntional Linguistics.\nMuhammad Umair Nasir and Innocent Mchechesi.\n2022.\nGeographical distance is the new hyperpa-\nrameter: A case study of ﬁnding the optimal pre-\ntrained language for English-isiZulu machine trans-\nlation. In Proceedings of the Workshop on Multilin-\ngual Information Access (MIA), pages 1–8, Seattle,\nUSA. Association for Computational Linguistics.\nDat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.\n2020.\nBERTweet: A pre-trained language model\nfor English tweets.\nIn Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, pages 9–\n14, Online. Association for Computational Linguis-\ntics.\nKelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021.\nSmall data?\nno problem!\nexploring the viabil-\nity of pretrained multilingual language models for\nlow-resourced languages. In Proceedings of the 1st\nWorkshop on Multilingual Representation Learning,\npages 116–126, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nSebastian Ruder and Barbara Plank. 2017. Learning to\nselect data for transfer learning with Bayesian opti-\nmization. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 372–382, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nIoannis Tsamardinos and Constantin F Aliferis. 2003.\nTowards principled feature selection: Relevancy, ﬁl-\nters and wrappers.\nIn International Workshop on\nArtiﬁcial Intelligence and Statistics, pages 300–307.\nPMLR.\nLiu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. 2021.\nA robustly optimized BERT pre-training approach\nwith post-training. In Proceedings of the 20th Chi-\nnese National Conference on Computational Lin-\nguistics, pages 1218–1227, Huhhot, China. Chinese\nInformation Processing Society of China.\nModel\nam\ndz\nha\nig\nkr\nma\npcm\npt\nsw\nts\ntwi\nyo\noverall\nAfroXLM-R\n45.20\n58.63\n80.27\n80.97\n70.77\n59.43\n51.83\n47.10\n63.27\n54.40\n35.97\n77.00\n66.19\nAfroXLM-R with adaptive pretraining\nLAPT\n56.84\n66.96\n79.72\n82.06\n71.76\n74.74\n72.50\n68.24\n62.42\n64.06\n52.84\n77.34\n73.67\nTAPT\n62.06\n71.38\n80.96\n82.30\n71.32\n80.58\n74.32\n71.12\n63.44\n51.10\n63.58\n78.00\n75.01\nLAPT + TAPT\n63.42\n71.12\n80.36\n82.72\n73.20\n79.44\n74.06\n69.04\n62.14\n63.78\n63.56\n78.14\n75.13\nTable 5: Subtask A (Monolingual sentiment analysis) results on dev set*. Same as in Section 4.2, TAPT achieves\nthe best overall F1 score.\nA\nAppendix\nA.1\nResults on dev set*\nHere, we provide the experimental results of all\nthree subtasks on our processed dev set* (for de-\ntails, please refer to Section 3.1).\nModel\nOverall\nMultilingual training\nMultilingual\n51.64\n+ TAPT\n73.64\nMonolingual training\nMonolingual\n66.19\n+ LAPT\n73.67\n+ TAPT\n75.01\n+ LAPT & TAPT\n75.13\ntransfer with selected sources\nFwd source transfer\n71.39\nFwd source transfer + TAPT\n75.53\nBwd source transfer\n71.37\nBwd source transfer + TAPT\n75.49\nTable 6: Subtask B (Multilingual sentiment analysis)\nresults on dev set*. The model with TAPT and forward\nsource language selection achieves the best overall re-\nsults with an F1 score of 75.53.\nModel\nor\ntg\nMultilingual transfer\nMultilingual\n48.16\n50.94\nMultilingual + TAPT\n52.20\n57.14\nTransfer with selected sources\nTOP3 source fwd\n49.00\n56.96\nTOP3 source fwd + TAPT\n57.90\n61.83\nTOP3 source bwd\n52.42\n62.88\nTOP3 source bwd + TAPT\n57.22\n63.48\nBERTweet with English translation\nBERTweet\n50.12\n54.10\nBERTweet + TAPT\n56.47\n63.43\nTable 7: Subtask C (Zero-shot cross-lingual sentiment\nanalysis) results on dev set*. Models with TAPT and\nsource selection achieves the best overall results with\n57.90 F1 score on or and 63.48 F1 score on tg.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-04-28",
  "updated": "2023-04-28"
}