{
  "id": "http://arxiv.org/abs/2305.00273v1",
  "title": "Sparsity-Aware Optimal Transport for Unsupervised Restoration Learning",
  "authors": [
    "Fei Wen",
    "Wei Wang",
    "Wenxian Yu"
  ],
  "abstract": "Recent studies show that, without any prior model, the unsupervised\nrestoration learning problem can be optimally formulated as an optimal\ntransport (OT) problem, which has shown promising performance on denoising\ntasks to approach the performance of supervised methods. However, it still\nsignificantly lags behind state-of-the-art supervised methods on complex\nrestoration tasks such as super-resolution, deraining, and dehazing. In this\npaper, we exploit the sparsity of degradation in the OT framework to\nsignificantly boost its performance on these tasks. First, we disclose an\nobservation that the degradation in these tasks is quite sparse in the\nfrequency domain, and then propose a sparsity-aware optimal transport (SOT)\ncriterion for unsupervised restoration learning. Further, we provide an\nanalytic example to illustrate that exploiting the sparsity helps to reduce the\nambiguity in finding an inverse map for restoration. Experiments on real-world\nsuper-resolution, deraining, and dehazing demonstrate that SOT can improve the\nPSNR of OT by about 2.6 dB, 2.7 dB and 1.3 dB, respectively, while achieving\nthe best perception scores among the compared supervised and unsupervised\nmethods. Particularly, on the three tasks, SOT significantly outperforms\nexisting unsupervised methods and approaches the performance of\nstate-of-the-art supervised methods.",
  "text": "1\nSparsity-Aware Optimal Transport for\nUnsupervised Restoration Learning\nFei Wen, Wei Wang and Wenxian Yu\nAbstract—Recent studies show that, without any prior model, the unsupervised restoration learning problem can be optimally\nformulated as an optimal transport (OT) problem, which has shown promising performance on denoising tasks to approach the\nperformance of supervised methods. However, it still signiﬁcantly lags behind state-of-the-art supervised methods on complex\nrestoration tasks such as super-resolution, deraining, and dehazing. In this paper, we exploit the sparsity of degradation in the OT\nframework to signiﬁcantly boost its performance on these tasks. First, we disclose an observation that the degradation in these tasks is\nquite sparse in the frequency domain, and then propose a sparsity-aware optimal transport (SOT) criterion for unsupervised restoration\nlearning. Further, we provide an analytic example to illustrate that exploiting the sparsity helps to reduce the ambiguity in ﬁnding an\ninverse map for restoration. Experiments on real-world super-resolution, deraining, and dehazing demonstrate that SOT can improve\nthe PSNR of OT by about 2.6 dB, 2.7 dB and 1.3 dB, respectively, while achieving the best perception scores among the compared\nsupervised and unsupervised methods. Particularly, on the three tasks, SOT signiﬁcantly outperforms existing unsupervised methods\nand approaches the performance of state-of-the-art supervised methods.\nIndex Terms—Restoration, unsupervised learning, optimal transport, super-resolution, deraining, dehazing.\n!\n1\nINTRODUCTION\nI\nMAGE restoration plays a fundamental role in low-level\ncomputer vision, which has attracted much research at-\ntention in the past few decades. Traditional model based\nmethods typically utilize prior models of image and/or\ndegradation, such as sparsity, low-rankness, smoothness,\nself-similarity or other priors of natural image, and Gaus-\nsianity, spatially independent i.i.d. of noise [1]–[5]. Recently,\nbeneﬁted from the powerful deep learning techniques, the\nﬁeld of image restoration has made much progress in the\npast few years [6], [7].\nGenerally, the success of deep network based methods\nusually rely on sufﬁcient paired degraded-clean data. Nev-\nertheless, in some applications such paired data is difﬁcult\nor even impractical to collect. For such applications, as\na common practice, synthesized degraded-clean data can\nbe used to learn a restoration model. However, the gap\nbetween synthesized and real-world data limits the practical\nperformance of the learned model on real-world data, espe-\ncially when the degradation is too complex to accurately\nsimulate. In these circumstances, unsupervised methods\nwithout requiring any degraded-clean pairs are preferred.\nTo relax the requirement of paired training data, un-\nsupervised learning methods have recently been actively\nstudied for various image restoration tasks. For example,\nfor unsupervised/self-supervised denoising learning, the\nnoise-to-noise (N2N) [8], noise-to-void (N2V) [9], noise-to-\nself (N2S) [10] methods, as well as many variants have been\ndeveloped, see [11]–[15] and the references therein. While\nN2N uses noisy pairs, N2V and N2S learn the restoration\n•\nF. Wen, W. Wang and W. Yu are with School of Electronic Information\nand Electrical Engineering, Shanghai Jiao Tong University, Shanghai,\nChina, 200240. E-mail: wenfei@sjtu.edu.cn; wangwei0803@sjtu.edu.cn;\nwxyu@sjtu.edu.cn.\nmodels in a self-supervised manner. Typically, these meth-\nods rely on prior assumptions, such as the noise in noisy\npairs are independent [8], [11], [13], or the noise is spatially\nindependent and/or the noise type is a priori known [9],\n[10], [14], [15]. Such assumptions limit the performance\nwhen the noise does not well conform with the assumptions.\nIn the recent work [16], it has been shown that, in\nthe absence of any prior model of the degradation, the\nunsupervised restoration learning problem can be optimally\nformulated as an optimal transport (OT) problem. The OT\ncriterion based unsupervised learning method has achieved\npromising performance on various denoising tasks to ap-\nproach the performance of supervised methods. However,\nit still signiﬁcantly lags behind state-of-the-art supervised\nmethods on more complex restoration tasks such as super-\nresolution, deraining, and dehazing. A main reason is that,\nfor these restoration tasks, seeking an inverse of the degra-\ndation map is more ambiguous. In light of this understand-\ning, this work is motivated to exploit prior models of the\ndegradation in the OT criterion to reduce the ambiguity in\nlearning an inverse map of the degradation.\nSpeciﬁcally, in this work, we exploit the sparsity of\ndegradation in the OT framework to signiﬁcantly boost the\nperformance on these complex restoration tasks. The main\ncontributions are as follows.\nFirst, we disclose an empirical observation that, for the\nsuper-resolution, deraining, and dehazing tasks, the degra-\ndation is quite sparse in the frequency domain. Based on this\nobservation, we propose a sparsity-aware optimal transport\n(SOT) criterion for unsupervised restoration learning.\nThen, we provide an analysis to illustrate that exploiting\nthe sparsity of degradation in the OT criterion helps to\nreduce the ambiguity in ﬁnding an inverse map of the\ndegradation.\nFinally, we provide extensive experimental results on\narXiv:2305.00273v1  [cs.CV]  29 Apr 2023\n2\nboth synthetic and real-world data for the super-resolution,\nderaining, and dehazing tasks. The results demonstrate that\nexploiting the sparsity of degradation can signiﬁcantly boost\nthe performance of the OT criterion. For example, compared\nwith the vanilla OT criterion on real-world super-resolution,\nderaining and dehazing, it achieves a PSNR improvement\nof about 2.6 dB, 2.7 dB and 1.3 dB, respectively, and at\nmeantime, attains the best perception scores among the\ncompared supervised and unsupervised methods. Mean-\nwhile, in synthetic data experiments on the three tasks,\nthe PSNR improvement is about 2.5, 1.6, 3.4 dB, respec-\ntively. Noteworthily, on all the tasks, SOT considerably\noutperforms existing unsupervised methods to approach\nthe performance of state-of-the-art supervised methods.\nOur work provides a generic unsupervised (unpaired)\nlearning framework, which applies to various restoration\napplications as long as the degradation can be sparsely\nrepresented. Relaxing the requirement of collecting or syn-\nthesizing paired degraded-clean data is of practical im-\nportance, especially for applications where paired data is\ndifﬁcult to collect or accurately simulate. In such realistic\ncircumstances, the proposed method has much potential as\nit shows favorable performance on real-world data even\ncompared with state-of-the-art supervised methods.\n2\nRELATED WORK\nWe review the works related to this work, especially on\nunsupervised methods in various image restoration tasks.\nImage denoising. Traditional hand-crafted model based\ndenoising methods typically design models based on a\npriori information on the signal and noise [1], [2], [17],\n[18]. The performance of these methods is usually closely\nrelated to hyperparameter setting. Recently, deep learning\nmethods without requiring paired noisy-clean data have\nattracted active attention. The N2N method [8], as well\nas its variants [11], [19]–[21], utilize noisy pairs to train\nrestoration models under the assumption that the noise is\nzero-mean and independent homogeneous across paired\nsamples. Self-supervised methods adopt special networks\nor training methods that extract information from the noisy\nimage itself for supervised learning, e.g. adopting a blind-\nspot network to predict masked pixel by neighboring pixels\nor designing sampling methods to construct paired training\ndata from noisy images [9], [10], [12], [14], [15], [22], [23].\nCommonly, these methods rely on the assumption that the\nnoise follows independent homogeneous distribution, e.g.\nspatially independent. Generally, speciﬁc prior assumptions\nlimit realistic generalization of these methods. More re-\ncently, based on the OT theory, the work [16] constructs\nan optimal criterion for unpaired restoration learning in the\nabsence of any prior noise model. While this method has\nshown promising performance that approach supervised\nmethods on denoising tasks, it still signiﬁcantly lags behind\nsupervised methods on more complex tasks such as super-\nresolution, deraining, and dehazing.\nImage super-resolution. The research of image super-\nresolution has a long history [3], [24]–[26]. The work [27]\nproposes the ﬁrst deep network based super-resolution\nmethod, then deep network based super-resolution has at-\ntracted much attention in the past few years [28]–[33]. Since\nthe method SRGAN [29] ﬁrstly introduces GAN [34] into\nimage super-resolution to improve perceptual quality, GAN\nhas been widely used as a basic component in learning\nsuper-resolution models [30]–[33]. Such methods typically\nuse synthesized paired data for supervised learning, e.g. by\nbicubic interpolation. In order to achieve better performance\non realistic data, many unsupervised super-resolution learn-\ning methods have been developed recently. Most of these\nmethods considers an unpaired setting that only unpaired\nlow-resolution and high-resolution samples are available.\nBasically, a mainstream of these methods are designed based\non CycleGAN [35]. To compensate for the lack of generative\nconstraints in CycleGAN, many improvements have been\nmade in [36]–[42].\nImage deraining. For image deraining, the DSC method\n[4] uses dictionary learning as well as sparse coding, of\nwhich the basic idea is to use a learned dictionary with\nstrong mutual exclusion on a very high discriminative code\nthat sparsely approximates the image blocks of both layers.\nHowever, its performance degrades signiﬁcantly when the\nbackground of the input image is similar to rain drops.\nIn the past few years, a number of deep network based\nmethods have been developed and much progress has\nbeen made [43]–[47]. Considering unsupervised methods,\nwhile the conditional GAN [48] requires paired training\ndata, CycleGAN [35] does not and hence can be naturally\nemployed for unsupervised image deraining to relax the\nrequirement of paired data through a cyclic training struc-\nture. However, the generation guidance of CycleGAN is\nweak, which results in artifacts in the generated images and\naffects the restoration quality. To address this problem, De-\nCyGAN [49] designs an unsupervised attention guided rain\nstreak extractor to extract the rain streak masks with two\nconstrained cycle-consistency branches jointly. There also\nexits approaches based on CycleGAN, such as [50], [51]. The\nwork [52] introduces contrastive learning into unsupervised\nderaining. It can separate the rain layer from clean image\nwith the help of the intrinsic self-similarity property within\nsamples and the mutually exclusive property between the\ntwo layers.\nImage dehazing. Image dehazing has long been a chal-\nleging problem [53], for which the dark channel prior (DCP)\nbased method [5] is a classic model-based method. In [5], it\nis observed that in most non-sky patches of clean images,\nthere exists at least one color channel has very low intensity\nat some pixels, and in comparison, local patches of hazy im-\nages tend to have a greater brightness (less dark). Recently,\nthe work [54] only uses hazy images for model training by\nminimizing a DCP loss. In addition to the supervised meth-\nods [55]–[59], many unsupervised methods, which learn the\ndehazing map from unpaired clean and hazy images, have\nbeen developed recently. For example, the works [60], [61]\nuse GAN to extract the information of clean images from\nhazy ones. As CycleGAN is a typical unsupervised frame-\nwork, many methods improve on it with speciﬁc designs\n[62]–[65]. Moreover, the D4 method [66] explored the scat-\ntering coefﬁcients and depth information contained in hazy\nand clean images. By estimating the scene depth, it is able\nto re-render hazy images with different thicknesses, which\nfurther facilitates the training of the dehazing network.\n3\n3\nPRELIMINARIES\nThis section ﬁrst presents the problem formulation and\nthen introduces the OT based unsupervised restoration\nlearning method.\n3.1\nProblem Formulation\nConsider a degradation to restoration process as\nX −→Y\nf\n−→\nˆX,\nwhere X ∼pX is the source, Y is the degraded observation,\nˆX := f(Y ) is the restoration with f being the restoration\nmodel to be learned. Meanwhile, without loss of generality,\nwe consider a typical additive model for the degradation as\nY = X + N,\n(1)\nwhere N stands for the degradation.\nNote that among the three restoration tasks considered\nin this work, while the degradation models for the deraining\nand dehazing tasks directly conform to this additive model,\nit is not the case for the super-resolution task as it involves\nresolution reduction from X to Y . However, model (1) still\napplies when we consider the frequency domain model,\nsince resolution reduction is in fact equivalent to a loss of\nhigh-frequency components in the frequency domain. As\nwill be presented in Section 4, the proposed method uses a\nﬁdelity loss in frequency domain.\nGenerally, due to the information loss in the data process\nchain X →Y , seeking an inverse process from Y to X\nfor restoration is ambiguous and suffers from distortion.\nThe ideal goal of restoration is to seek an inverse process\nwith the lowest distortion. It is to suppress/remove/rectify\nthe degradation in the observation Y as much as possible,\nand at meantime, preserve the information of the source\nX contained in Y as much as possible. Besides, for some\nimage restoration tasks, high perception quality is another\nobjective in addition to low distortion, which reﬂects the\ndegree to which the restoration ˆX looks like a valid natural\nclean sample from human’s perception.\nAccordingly, an optimal criterion to fulﬁll the above\ngoals, e.g., degradation suppression, maximally information\npreserving of X, and high perception quality, is given by\nmax\nf\nI (f(Y ); X)\nsubject to\nd(p ˆ\nX, pX) ≤0,\n(2)\nwhere I(·; ·) is the mutual information. d(·, ·) is a divergence\nmeasures the deviation between two distributions, such as\nthe Kullback-Leibler divergence or Wasserstein distance,\nwhich satisﬁes d(p, q) ≥0 and d(p, q) = 0 ⇔p = q for any\ndistributions p and q. The perceptual quality of restoration\nis measured by the distribution divergence from natural\nsamples as d(p ˆ\nX, pX). It has been well recognized that per-\nception quality is associated with the deviation from natural\nsample statistics [67]–[70]. The constraint d(p ˆ\nX, pX) ≤0\nenforces p ˆ\nX = pX, which ensure the restoration having\nperfect perception quality. It has been recently revealed that\npursuing high perception quality would inevitably lead to\nincrease of the lowest achievable distortion [71]–[74]. To\nimplement criterion (2), degraded-clean pairs {(Y, X)} are\nrequired for supervision.\n3.2\nOT for Unsupervised Restoration Learning\nOT can be traced back to the seminal work of Monge\n[75] in 1781, with signiﬁcant advancements by Kantorovich\n[76] in 1942. It has a well-established theoretical foundation\nand provides a powerful framework for comparing proba-\nbility measures based on their underlying geometry. OT has\nrecently received increasing attention in machine learning\n[77]–[79].\nComputationally, the OT problem seeks the most ef-\nﬁcient transport map of transforming one distribution of\nmass to another with minimum cost. Speciﬁcally, let P(X)\nand P(Y ) denote two sets of probability measures on X and\nY , respectively. Meanwhile, let ν ∼P(Y ) and µ ∼P(X)\ndenote two probability measures. The OT problem seeks the\nmost efﬁcient transport plan from ν to µ that minimizes the\ntransport cost.\nDeﬁnition 1. (Transport map): Given two probability\nmeasures ν ∼P(Y ) and µ ∼P(X), f : Y →X is a\ntransport map from ν to µ if\nµ(A) = ν(f −1(A)),\nfor all µ-measurable sets A.\nDeﬁnition 2. (Monge’s optimal transport problem) [75]:\nFor a probability measure ν, let f♯ν denote the transport of ν\nby f. Let c : Y ×X →[0, +∞] be a cost function that c(y, x)\nmeasures the cost of transporting y ∈Y to x ∈X. Then,\ngiven two probability measures ν ∼P(Y ) and µ ∼P(X),\nthe OT problem is deﬁned as\ninf\nf\nR\nY c (f(y), y)dν(y)\nsubject to\nµ = f#ν,\n(3)\nover ν-measurable maps f : Y →X. A minimum to this\nproblem, e.g. denoted by f ∗, is called an OT map from ν to\nµ.\nThe OT problem seeks a transport plan to turn the mass\nof ν into µ at the minimal geometric cost measured by the\ncost function c, e.g. typically c (f(y), y) := ∥f(y) −y∥β with\nβ ≥1.\nIn the absence of any degraded-clean pairs for super-\nvised learning of the restoration model, an unsupervised\nlearning method using only unpaired noisy and clean data\nhas been proposed in [16] based on the OT formulation (3)\nas\nmin\nf\nEY ∼pY\n\u0010\n∥Y −f(Y )∥β\u0011\nsubject to\np ˆ\nX = pX.\n(4)\nFormulation (4) is an OT problem seeks a restoration\nmap f from Y to X. The constraint p ˆ\nX = pX enforces that\nthe restoration { ˆX} has the same distribution as natural\nclean images {X}. Under this constraint, the restoration\nwould have good perception quality, i.e. looks like natural\nclean images, since each ˆX lies in the manifold (set) of X\n[71]–[74]. Meanwhile, the constraint p ˆ\nX = pX imposes an\nimage prior stronger than any hand-crafted priors, such as\nsparsity, low-rankness, smoothness and dark-channel prior.\nThese priors have been widely used in traditional image\nrestoration methods. Any reasonable prior model for nat-\nural clean images would be fulﬁlled under the constraint\np ˆ\nX = pX.\n4\nThe objective in (4) imposes ﬁdelity of the restoration ˆX\nto Y , which ensures minimum distance transport and hence\nthe restoration map can maximally preserve the information\nof X contained in Y [16]. From the data process chain X →\nY →ˆX, it follows that I( ˆX; X) ≤I(Y ; X). Hence, under\nthe constraint on ˆX, maximally preserving the information\nof X contained in Y in the restoration can be fulﬁlled by\nmaximizing the mutual information I( ˆX; Y ).\nIn contrast, when without the ﬁdelity term, formulation\n(4) can be implemented by standard GAN to generate ˆX\nsatisﬁes p ˆ\nX = pX, e.g, by min\nf\nd(pX, p ˆ\nX). However, the map\nfrom Y to X is no longer an minimum distance transport\nand the restoration ˆX may be excessively far from the clean\nsource X. For instance, the generator f can disregard the\ninput Y and randomly generate samples ˆX from the distri-\nbution pX to satisfy p ˆ\nX = pX but with ˆX be independent\non X, i.e., I( ˆX; X) = 0. The conditional GAN [48] does\nnot suffer from this degeneration problem by discriminating\nbetween (Y, X) and (Y, ˆX), but requires paired degraded-\nclean data (Y, X) for supervision. RoCGAN [80] also uses\npaired degraded-clean data. Although AmbientGAN does\nnot require paired degraded-clean, it requires a pre-deﬁned\ndegradation model which can be easily sampled [81]. Sim-\nilarly, NR-GAN [82] does not suffer from such limitation,\nbut requires either known noise distribution type or noise\nsatisfying some invariant properties.\nIn implementation, the OT based formulation (4) is re-\nlaxed into an unconstrained form as\nmin\nf\nEY ∼pY\n\u0010\n∥Y −f(Y )∥β\u0011\n+ λd(pX, p ˆ\nX),\n(5)\nwhere λ > 0 is a balance parameter. Then, formulation (5)\nis implemented based on WGAN-gp [83]. Though relaxed,\nit has been shown in [16] that under certain conditions the\nunconstrained formulation (5) has the same solution as the\noriginal constrained formulation (4) in theory. However,\nin practice the balance parameter λ needs to be tuned\nto achieve satisfactory performance. More recently, an OT\nalgorithm for unpaired super-resolution has been proposed\nin [84], which is an alternative for solving (4).\n4\nINCORPORATING SPARSITY PRIOR INTO OT FOR\nUNSUPERVISED RESTORATION LEARNING\nThe degradation map X →Y in (1) is typically non-\ninjective and inevitably incurs information loss of the source\nX. Hence, the degradation map is non-invertible and seek-\ning an inverse of it is ambiguous. In this scenario, ex-\nploiting prior information of the degradation map is an\neffective way to reduce the inverse ambiguity. In this sec-\ntion, we ﬁrst show the sparsity property of the degradation\nfor three representative restoration tasks, and then exploit\nthis property in the OT framework to propose a sparsity-\naware formulation for restoration learning. Furthermore, we\nprovide an analytic example to show the effectiveness of\nexploiting sparsity in reducing the ambiguity of inverting\nthe degradation process.\n4.1\nThe Sparsity of Degradation\nIn what follows, we show that for three representative\nrestoration tasks, super-resolution, deraining, and dehazing,\nthe degradation N is very sparse in the frequency domain.\nFig. 1 presents the histogram statistic of the degradation in\nthe frequency domain for each of the three tasks. 500 real-\nworld images from the super-resolution dataset RealVSR\n[85], 400 real-world images from the deraining dataset SPA\n[86], and 200 real-world images from the dehazing dataset\nDense-haze [87] are used to compute the histogram statistic\nfor the three tasks, respectively. Given L degraded-clean\npairs {(yi, xi)}i=1,··· ,L from a dataset, the histogram in the\nfrequency domain is computed based on the absolute FFT\ncoefﬁcients {|ﬀt2(yi −xi)|}i=1,··· ,L, ﬀt2(·) is the 2D FFT.\nEach histogram is averaged over the image pair number L.\nFor super-resolution, Y is obtained from a 1/4 times low\nresolution version of X by 4 times bicubic up-sampling.\nIt can be seen from Fig. 1 that, for each of the three tasks,\nthe frequency domain representation of the degradation is\nvery sparse, which follows a hyper-Laplacian distribution.\nThis signiﬁcant sparsity feature of the degradation provides\na natural and useful prior for restoration inference. It is\nexploited in our method by promoting sparsity of the degra-\ndation in the frequency domain to achieve more accurate\nrestoration. Note that although these statistic results are\nderived based on real-world degradation data, the imple-\nmentation of our algorithm does not require an accurate\nestimation of the sparsity parameter of the degradation and,\nas will be shown in experiments, a rough selection of q of the\nℓq cost (e.g. q ∈{0.5, 1}) is enough for achieving satisfactory\nperformance.\n4.2\nProposed Method Exploiting Degradation Sparsity\nin OT\nThe optimal transport criterion (4) does not impose any\nconstraint on the degradation N. Though optimal in the\ncase without prior information of the degradation, it is\nsuboptimal when prior information of the degradation is\navailable. For example, if the distribution of N is a priori\nknown, an ideal criterion extending (4) becomes\nmin\nf\nEY ∼pY\n\u0010\n∥f(Y ) −Y ∥β\u0011\nsubject to\np ˆ\nX = pX, p ˆ\nN = pN,\n(6)\nwhere N := Y −f(Y ) = Y −ˆX. Adding the constraint\np ˆ\nN = pN can effectively leverage the prior information of\nthe degradation process (1) to ﬁnd a better transport map\nfor the inverse problem Y\n→X with lower distortion.\nWhile the distribution pX can be learned from natural clean\nimages, seeking an estimation of pN requires the collection\nof sufﬁcient degradation {ni}. In fact, if a collection of suf-\nﬁcient degradation {ni} is available, degraded-clean pairs\n{xj + ni, xj} can be directly obtained and the restoration\nmodel can be learned in a supervised manner. However,\nin practice, collecting real-world degradation {ni} would\nbe as difﬁcult as collecting real-world degraded-clean pairs\n{(yi, xi)}.\nIn this work, we relax the requirement of pN and employ\na simple generic prior for the degradation instead. It is\ninspired by the above empirical observation that for some\nrestoration tasks the degradation is very sparse in the fre-\nquency domain, as shown in Fig. 1. Speciﬁcally, we propose\n5\n(a) Super-resolution\n(b) Deraining\n(c) Dehazing\nFig. 1. Histograms of the degradation N in the frequency domain for three restoration tasks. (a) Super-resolution (500 real-world images from the\nRealVSR dataset [85]). (b) Deraining (400 real-world images from the SPA dataset [86]). (c) Dehazing (200 real-world images from the Dense-haze\ndataset [87]). The degradation in the three tasks are very sparse in the frequency domain, which follows a hyper Laplacian distribution.\na formulation to make use of the frequency domain sparsity\nof degradation as\nmin\nf\nEY ∼pY\n\u0010\n∥F(f(Y ) −Y )∥q\nq\n\u0011\nsubject to\np ˆ\nX = pX,\n(7)\nwhere F(·) stands for the discrete Fourier transform, and\n∥· ∥q\nq is the ℓq-norm with 0 ≤q ≤1. Since the FFT represen-\ntation of N, i.e. F(N), is sparse, a necessary condition for an\ninverting map f to be optimal is that it should conform to\nthis property such that F(N) = F(f(Y ) −Y ) is sparse. To\nachieve this, in formulation (7) we use the ℓq-norm loss with\n0 ≤q ≤1 to promote the sparsity. The ℓq-norm is a sparsity-\npromotion loss in data ﬁtting, which has been widely used\nin sparse recovery to obtain sparse solution [88], [89].\nFor the particular case of q = 2, formulation (7) reduces\nto the OT problem (4) with ℓ2 cost since ∥F(f(Y ) −Y )∥2\n2 =\n∥f(Y ) −Y ∥2\n2. The ℓ2 cost is optimal for degradation with\nGaussian distribution but not optimal for sparse degrada-\ntion with super-Gaussian distribution. As it has been shown\nin Fig. 1 that, the frequency domain distribution of degrada-\ntion is far from Gaussian rather being hyper-Laplace, using\nℓ2 cost can result in unsatisfactory performance far from\noptimal.\nIn implementation, an unconstrained form of (7) is used\nas\nmin\nf\nEY ∼pY\n\u0010\n∥F(f(Y ) −Y )∥q\nq\n\u0011\n+ λd(p ˆ\nX, pX),\n(8)\nwhere λ > 0 is a balance parameter. As the discrete Fourier\ntransform is complex-valued, for a vector M ∈Cm+1, the ℓq\nloss is computed as\n∥M∥q\nq =\nm\nX\ni=0\n\u0000ℜ2{M(i)} + ℑ2{M(i)}\n\u0001 q\n2 .\n(9)\n4.3\nAn Analysis on the Effectiveness of the Sparsity\nPrior\nUnder the degradation process Y = X + N, the dis-\ntribution of Y is a convolution of the distributions of X\nand N, i.e., pY = pX ⊗pN. Generally, the inverse problem\nfrom Y to X is ambiguous due to the information loss in the\ndegradation process. In the absence of any prior information\nof N, the optimal transport map f♯pY\n= pX obtained\nby (4) can be used as an ideal inverse (restoration) map\n[16]. However, when prior information of N is available,\nthe map f is no longer optimal and does not necessarily\napproach the lower bound of inverse distortion. Naturally,\nthe prior information of N can be exploited to reduce the\ninverting ambiguity to some extent and hence result in\nlower restoration distortion.\nAs our objective is to exploit the underlying sparsity of\nN to reduce the inverse ambiguity, here we use an example\nto show the effectiveness of exploiting the sparsity of the\ndegradation in helping to ﬁnd the desired correct inverse\nmap.\nExample 1 [Exploiting the sparsity of degradation helps\nto ﬁnd an inverse map with lower distortion]. Consider a\ndiscrete random source X ∈Rm+1, which follows a two-\npoint distribution with probability mass function as\npX(x) =\n\u001a p1,\nx = x1\np2,\nx = x2\n,\nwith\nx1 = [−a, b, · · · , b\n|\n{z\n}\nm\n]T ,\nx2 = [−a, −b, · · · , −b\n|\n{z\n}\nm\n]T ,\nwhere a ≫b > 0 such that a2 > mb2 and aq < mbq for\nany 0 ≤q ≤1. For example, these two conditions hold for\na = 1, b = 0.1 and 10 < m < 100. If only considering q = 0,\nthey hold for a = 1, b = 0.1 and 1 < m < 100. Furthermore,\nconsider a degradation process as (1), where N ∈Rm+1\nalso follows a two-point distribution with probability mass\nfunction given by\npN(n) =\n\u001a ˜p1,\nn = n1\n˜p2,\nn = n2\n,\nwith\nn1 = [−2a, 0, · · · , 0]T ,\nn2 = [2a, 0, · · · , 0]T .\nNote that N is sparse as only one of its elements is nonzero.\nIn this setting, the distribution of Y is given by the convo-\n6\n(a) Degradation pro-\ncess\n(b) Inverse\nvia\nOT\nwith ℓ2 cost\n(c) Inverse\nvia\nOT\nwith ℓq cost for any\n0 ≤q ≤1\nFig. 2. An illustration of the optimal transport from pY to pX with different\ncost function in Example 1. (a) The degradation process Y = X + N.\n(b) Inverse via the optimal transport f2 with ℓ2 cost. (c) Inverse via the\noptimal transport fq with ℓq cost for any 0 ≤q ≤1. In this example, the\noptimal transport map under the ℓq cost yields the correct inverse map\nwith zero distortion, while that under the ℓ2 cost does not.\nlution between pX and pN as\npY (y) =\n\n\n\n\n\n\n\np1˜p1,\ny = y1\np1˜p2,\ny = y2\np2˜p1,\ny = y3\np2˜p2,\ny = y4\n,\nwith\ny1 = [−3a, b, · · · , b]T ,\ny2 = [a, b, · · · , b]T ,\ny3 = [−a, −b, · · · , −b]T ,\ny4 = [3a, −b, · · · , −b]T .\nThen, given the distributions of X and Y , we investigate\nthe inverse process Y →X by seeking the OT from pY to\npX with different cost functions. Particularly, we evaluate\nthe ℓq cost with 0 ≤q ≤1 in comparison with the widely\nused ℓ2 cost.\nFirst, with the ℓ2 cost and under the condition a2 > mb2,\na solution f ∗\n2 : Y →X to the OT problem\nmin\nf2 EY ∼pY\n\u0010\n∥f2(Y ) −Y ∥2\n2\n\u0011\nsubject to\n(f2)♯pY = pX,\n(10)\nwould map y2 →x2 rather than y2 →x1 since ∥y2 −x2∥2\n2 =\n4mb2 < ∥y3 −x2∥2\n2 = 4a2 and y3 →x1 rather than y3 →x2\nsince ∥y3 −x1∥2\n2 = 4mb2 < ∥y3 −x2∥2\n2 = 4a2. For example,\nlet p1 = ˜p1 = 0.5 and p2 = ˜p2 = 0.5, the optimal map f ∗\n2\nis given by f ∗\n2 (y1) = x1, f ∗\n2 (y2) = x2, f ∗\n2 (y3) = x1, and\nf ∗\n2 (y4) = x2, as illustrated in Fig. 2(b). In contrast, with the\nℓq cost and under the condition aq < mbq, ∀q ∈[0, 1], it\nfollows that ∥y2 −x2∥q\nq = m(2b)q > ∥y3 −x2∥q\nq = (2a)q.\nHence, the solution f ∗\nq : Y →X to the OT problem\nmin\nfq EY ∼pY\n\u0010\n∥fq(Y ) −Y ∥q\nq\n\u0011\nsubject to\n(fq)♯pY = pX,\n(11)\nis given by f ∗\nq (y1) = x1, f ∗\nq (y2) = x1 and f ∗\nq (y3) = x2,\nf ∗\nq (y4) = x2, as illustrated in Fig. 2(c).\nIn this example, the residual N is very sparse as it has\nonly one nonzero element. This sparsity property can be\nexploited by using a sparsity-promotion data ﬁtting cost,\nsuch as the ℓq-norm with 0 ≤q ≤1 [88], [89]. Since using the\nℓq cost can well exploit the underlying sparsity prior of the\ndegradation to reduce the ambiguity in ﬁnding the inverse\nmap, in this example it yields the desired correct inverse\nmap under the OT criterion, which has a zero distortion.\nIn comparison, the ℓ2 cost yields an undesired map, which\ndoes not align with the degradation map and has a nonzero\ndistortion, as shown in Fig. 2. This example illustrates that\nexploiting the sparsity prior of the degradation (e.g., by the\nℓq cost) can effectively help to ﬁnd an inverse transport map\nwith lower restoration distortion.\n5\nEXPERIMENTAL RESULTS\nWe conduct experimental evaluation on three image\nrestoration tasks, including super-resolution, deraining, and\ndehazing. For each task, the proposed method is compared\nwith state-of-the-art supervised and unsupervised methods\non both synthetic and real-world data. Note that, since for\neach of the tasks there exists a number of supervised and\nunsupervised methods, it is difﬁcult to compare with all the\nrepresentative methods in each task. The focus here is to\ncompare with state-of-the-art supervised and unsupervised\nmethods in each task. Particularly, state-of-the-art super-\nvised methods are used as ideal baselines for comparison.\nFor our method, two variants, denoted by SOT (ℓ0.5) and\nSOT (ℓ1), are evaluated in each task, which use the ℓ0.5\ncost and ℓ1 cost, respectively. The compared methods are\nas follows.\n•\nFor the super-resolution task, the compared super-\nvised methods include RankSR [31], RCAN [32],\nESRGAN [30] and RNAN [33]. The compared un-\nsupervised methods include USIS [38], OT [16].\n•\nFor the deraining task, the compared methods in-\nclude DSC [4], RESCAN [43], MPRNet [44], SIRR\n[45], CycleGAN [35], DeCyGAN [49], OT [16], where\nDSC is a traditional method, RESCAN and MPRNet\nare supervised methods, SIRR is a semi-supervised\nmethod, while CycleGAN, DeCycleGAN and OT are\nunsupervised methods.\n•\nFor the dehazing task, the compared methods in-\nclude DCP [5], AODNet [56], Dehamer [57], GCANet\n[58], FFANet [59], D4 [66], OT [16], where DCP\nis a traditional model-based method, AODNet, De-\nhamer, GCANet and FFANet are supervised learning\nmethods, wile D4 and OT are unsupervised learning\nmethods.\nIn order to make a comprehensive evaluation, the\nrestoration quality is evaluated in terms of both distortion\nmetrics, including peak signal to noise ratio (PSNR) and\nstructural similarity (SSIM), and perceptual quality metrics,\nincluding perception index (PI) [90] and learned perceptual\nimage patch similarity (LPIPS) [91].\nWe implement the proposed formulation (8) based on\nWGAN-gp [83], with 0 ≤q ≤1 and λ being tuned for each\ntask. Note that the best selection of the value of q depends\non the statistics of the data and hence is application and\ndata dependent. In practice it is generally difﬁcult to select\nthe optimal value of q. Therefore, in the implementation we\nonly roughly test two values of q, e.g., q = 0.5 and q = 1 for\nthe ℓq cost of SOT. Experimental results show that this rough\nselection is sufﬁcient to yield satisfactory performance of\nSOT.\n7\nTABLE 1\nQuantitative comparison on 4x super-resolution of synthetic images\n(using the DIV2K dataset).\nMethod\nPSNR/SSIM\nLPIPS/PI\nTraditional\nBicubic\n26.77/0.755\n0.1674/7.07\nSupervised\nRankSR [31]\n26.56/0.734\n0.0541/3.01\nRCAN [32]\n29.33/0.828\n0.0925/5.24\nESRGAN [30]\n26.66/0.749\n0.0520/3.26\nRNAN [33]\n29.31/0.827\n0.0966/5.40\nUnsupervised\nUSIS [38]\n22.22/0.628\n0.1761/3.52\nOT [16]\n25.73/0.719\n0.0838/4.44\nSOT (ℓ0.5)\n28.25/0.801\n0.0612/3.03\nSOT (ℓ1)\n27.91/0.783\n0.0821/3.45\nFig. 3. Restoration PSNR of SOT in image super-resolution for different\nvalues of λ.\nFor a fair comparison, our method and the OT method\n[16] use the same network structure, which consists of\na generator and a discriminator. The generator uses the\nnetwork in MPRNet [44], which is one of the state-of-the-\nart models, while the discriminator is the same as that in\n[16]. The discriminator takes the generator output (restored\nimages) and clean images as input. It should be noted\nthat although clean images are used here, the proposed\nmethod is unsupervised because the noisy input of the\ngenerator (restoration model) and the clean images input\nto the discriminator are not paired.\n5.1\nSynthetic Image Super-Resolution\nFirst, we conduct super-resolution experiment on syn-\nthetic data. The used DIV2K [92] dataset contains a total of\n1000 high-quality RGB images with a resolution of about\n2K. 100 images are used for testing. Since OT requires the\ninput to have the same size as the output, we follow the pre-\nupsampling method [93] to upsample the low-resolution\nimages before feeding them into the network by bicubic.\nTABLE 2\nQuantitative comparison on real-world image super-resolution (using\nthe RealVSR dataset).\nMethod\nPSNR/SSIM\nLPIPS/PI\nTraditional\nBicubic\n23.15/0.749\n0.1347/5.94\nSupervised\nRankSR [31]\n21.05/0.607\n0.1031/2.79\nRCAN [32]\n23.39/0.772\n0.1573/5.79\nESRGAN [30]\n21.13/0.632\n0.1024/3.53\nRNAN [33]\n23.19/0.752\n0.1599/5.88\nUnsupervised\nUSIS [38]\n19.06/0.502\n0.2122/3.35\nOT [16]\n21.34/0.658\n0.1110/4.12\nSOT (ℓ0.5)\n23.89/0.782\n0.0839/3.27\nSOT (ℓ1)\n22.85/0.722\n0.0758/3.23\nTable 1 presents quantitative results of 4x super-\nresolution on the DIV2K dataset. It can be seen that the\nproposed method SOT can achieve PSNR and SSIM re-\nsults close to those of state-of-the-art supervised learning\nmethods, e.g., with about 1.06 dB difference compared with\nRCAN. Meanwhile, in terms of the perception indices, its\nperceptual quality exceeds some of the supervised methods\nsuch as RCAN. It should be noted that both RankSR and\nESRGAN take perceptual quality as the reconstruction tar-\nget and use perceptual quality metrics such as LPIPS as the\noptimization target during training. Hence, they can achieve\nbetter perceptual quality scores. However, recent studies\non the trade-off between distortion and perceptual quality\nshow that, the improvement in perceptual quality would\nnecessarily lead to increase of reconstruction distortion, e.g.\nthe deterioration of the MSE, PSNR and SSIM metrics [72],\n[73]. Accordingly, the PSNR and SSIM results of RankSR\nand ESRGAN are worse. Moreover, by exploiting the degra-\ndation sparsity, the proposed SOT signiﬁcantly outperforms\nthe vanilla OT method, e.g. an improvement of about 2.52\ndB in PSNR. Using the ℓ0.5 cost yields better results of SOT\nthan the ℓ1 cost.\nFig. 3 shows the PSNR of SOT versus the value of\nλ, which investigates the effect of the parameter λ. Fig.\n4 compares the visual quality of 4x super-resolution on\na typical sample from the DIV2K dataset. It can be seen\nthat the result of RCAN, which yields the highest PSNR,\nis closer to the ground-truth, but the reconstructed images\nappear to be blurred and the details are not clear enough.\nThe proposed SOT method with the ℓ0.5 cost yields higher\nPSNR than RankSR and ESRGAN, while having comparable\nperception quality.\n5.2\nReal-Word Image Super-Resolution\nIn\nsynthetic\nsuper-resolution\nexperiments,\nbicubic\ndown-sampling is widely used to construct paired training\ndata. However, real-word degradation can substantially de-\nviate from bicubic down-sampling. This limits the perfor-\nmance of the synthetic data learned model on real-world\n8\n(a) Test image\n(b) Ground truth\n(c) Bicubic (26.59 dB)\n(d) RankSR (26.64/0.743/0.053)\n(e) RCAN (29.35/0.829/0.093)\n(f) ESRGAN (26.66/0.751/0.052)\n(g) USIS (23.44/0.701/0.167)\n(h) OT (26.88/0.754/0.082)\n(i) SOT (ℓ0.5) (28.62/0.821/0.060)\nFig. 4. Visual comparison on 4x synthetic image super-resolution. The PSNR/SSIM/LPIPS results are provided in the brackets. The images are\nenlarged for clarity.\ndata. To further verify the performance of the proposed\nmethod on real scenes, we conduct experiment on a real-\nworld super-resolution dataset RealVSR [85]. In this dataset,\npaired data is constructed by ﬁrstly using the multi-camera\nsystem of iPhone 11 Pro Max to capture images of different\nresolutions in the same scene separately, and then adopting\npost-processing such as color correction and pixel align-\nment. The results on this dataset are shown in Table 2.\nFrom the results, the proposed method can achieve better\nPSNR, SSIM and LPIPS scores even compared with state-\nof-the-art supervised methods. Moreover, exploiting degra-\ndation sparsity can improve the performance of the OT\nmethod by a large margin. For example, SOT(ℓ0.5) achieves\na PSNR about 2.55 dB higher than OT with signiﬁcant\nbetter perception scores. SOT(ℓ0.5) has lower distortion than\nSOT(ℓ1) (e.g. about 1 dB higher in PSNR) but with worse\nperception scores. This accords well with the distortion-\nperception tradeoff theory.\nFig. 5 compares the visual quality of the methods on\na typical sample from the RealVSR dataset. It can be seen\nthat SOT achieves the best PSNR, SSIM and LPIPS scores.\nQualitatively, it can achieve high-quality detail reconstruc-\ntion while having less artifacts than the perception-oriented\nmethods.\n5.3\nSynthetic Image Deraining\nFor synthetic image deraining, we train the models on\nthe Rain1800 dataset [94] and test on the Rain100L dataset\n[95]. These two datasets respectively contain 1800 and 100\nimages of natural scenes with simulated raindrops.\nTable 3 shows the quantitative results tested on the\nRain100 dataset. Clearly, the proposed method can achieve\na PSNR close to the state-of-the-art supervised method.\nFor example, the difference in PSNR between SOT(ℓ1) and\nMPRNet is about 1.07 dB. Noteworthily, SOT(ℓ1) achieves\nthe best perception scores among all the compared unsu-\npervised and supervised methods. Again, SOT performs\nmuch better than OT, which demonstrates the effectiveness\nsparsity exploitation. For SOT, the ℓ1 cost yields better\nperformance than the ℓ0.5 cost.\n9\n(a) Test image\n(b) Ground truth\n(c) bicubic (23.01 dB)\n(d) RankSR (20.87/0.612/0.104)\n(e) RCAN (23.48/0.784/0.157)\n(f) ESRGAN (21.06/0.634/0.101)\n(g) USIS (19.57/0.527/0.211)\n(h) OT (21.65/0.674/0.111)\n(i) SOT (ℓ0.5) (24.03/0.792/0.084)\nFig. 5. Visual comparison on real-world image super-resolution. The PSNR/SSIM/LPIPS results are provided in the brackets. The images are\nenlarged for clarity.\nTABLE 3\nQuantitative comparison of the deraining methods on synthetic data\n(the Rain1800 and Rain100L datasets are used for training and testing,\nrespectively).\nmethod\nPSNR/SSIM\nLPIPS/PI\nRainy\n25.52/0.825\n0.1088/3.77\nTraditional\nDSC [4]\n25.72/0.831\n0.1116/2.79\nRESCAN [43]\n29.80/0.881\n0.0731/2.87\nSupervised\nMPRNet [44]\n36.40/0.965\n0.0167/3.21\nSemi-supervised\nSIRR [45]\n23.48/0.800\n0.0978/2.88\nCycleGAN [35]\n24.03/0.820\n0.0960/2.80\nDeCyGAN [49]\n24.89/0.821\n0.0952/3.12\nOT [16]\n33.71/0.954\n0.0158/2.76\nSOT (ℓ0.5)\n34.74/0.948\n0.0132/2.54\nUnsupervised\nSOT (ℓ1)\n35.33/0.963\n0.0108/2.51\nFig. 6 compares the visual quality of the methods. It can\nbe observed that the supervised methods, such as MPRNet,\ncan achieve excellent rain removal but with the restoration\nbeing over-smoothing. The proposed method can recon-\nstruct better texture details, while achieving effective rain\nTABLE 4\nQuantitative comparison of the deraining methods on real-world data\n(using the SPA dataset [86]).\nmethod\nPSNR/SSIM\nLPIPS/PI\nRainy\n34.30/0.923\n0.0473/8.49\nTraditional\nDSC [4]\n32.29/0.921\n0.0498/7.89\nRESCAN [43]\n38.34/0.961\n0.0250/8.03\nSupervised\nMPRNet [44]\n46.12/0.986\n0.0109/7.68\nSemi-supervised\nSIRR [45]\n22.66/0.710\n0.1323/7.87\nCycleGAN [35]\n28.79/0.923\n0.0422/7.58\nDeCyGAN [49]\n34.78/0.929\n0.0528/7.50\nOT [16]\n41.68/0.951\n0.0098/7.35\nSOT (ℓ0.5)\n42.69/0.958\n0.0096/7.15\nUnsupervised\nSOT (ℓ1)\n44.37/0.982\n0.0084/7.06\nremoval.\n5.4\nReal-world Image Deraining\nFor the real-world image derainging experiment, we\nchose the real scene dataset SPA [86] for training and testing.\nThis dataset takes images with and without rain in the\n10\n(a) Test image\n(b) Ground truth\n(c) Rainy (25.41 dB)\n(d) DSC (25.78/0.835/0.112)\n(e) RESCAN (30.21/0.892/0.072)\n(f) MPRNet (36.58/0.972/0.018)\n(g) SIRR (24.59/0.825/0.098)\n(h) DeCyGAN (25.32/0.865/0.095)\n(i) SOT (ℓ1) (36.02/0.968/0.011)\nFig. 6. Visual comparison on synthetic image deraining. The PSNR/SSIM/LPIPS results are provided in the brackets. The images are enlarged for\nclarity.\nsame scene by ﬁxing the camera position, hence the rain-\nfree images can be used as the ground-truth for supervised\nmodel training. The experimental results are shown in Table\n4.\nSimilar to the results in the synthetic deraining experi-\nment, the proposed method achieves the best performance\namong the unsupervised methods. It achieves a PSNR only\n1.75 dB lower than the state-of-the-art supervised method\nMPRNet. Particularly, SOT achieves the best perception\nscores, which even surpasses that of the supervised meth-\nods. In addition, SOT(ℓ1) attains a PSNR 2.69 dB higher than\nthat of the vanilla OT method.\nFig. 7 compares the visual quality of the deraining meth-\nods on a typical real sample. It can be seen that, SOT can\nachieve a quality on par with the state-of-the-art supervised\nmethod MPRNet to provide a visually plausible restoration,\nwhich demonstrate the effectiveness of SOT on real data. It\nshould be noted that although CycleGAN can also achieve\nexcellent rain removal, it introduces additional distortion\nsuch as color, resulting in larger distortion, e.g., with a PSNR\nmore than 10 dB lower than that of MPRNet and SOT.\n5.5\nSynthetic Image Dehazing\nHazy scenes, like rainy scenes, can signiﬁcantly affect\nthe quality of the captured images, but the difference in\ndistribution between the two is obvious. Under a hazy sky,\nthe whole image will be covered with a gray or white fog\nlayer. Therefore, a global image reconstruction is needed.\nFor the synthetic image dehazing task, we train and test the\nmodels on the OTS dataset [96]. This dataset contains a large\nnumber of images of outdoor scenes with various levels of\nsynthetic fog layers. We selected 100 images from the dataset\nas the test set.\nTable 5 shows the quantitative results of the compared\nmethods on the OTS dataset. It can be seen that the proposed\nmethod can achieve a PSNR approaching that of the state-of-\nthe-art supervised methods. For instance, the difference in\nPSNR between SOT(ℓ1) and the state-of-the-art transformer\nbased supervised method Dehamer is about 1.18 dB, while\nSOT(ℓ1) achieves the best perception scores among all the\ncompared supervised and unsupervised methods. In this\nexperiment, SOT with the ℓ1 cost attains a PSNR 3.37 dB\nhigher than the standard OT method, e.g., 32.20 dB versus\n28.83 dB. Fig. 8 compares the visual quality of the methods.\nThe restoration quality of the proposed method is even on\n11\n(a) Ground truth\n(b) Rainy (35.26 dB)\n(c) DSC (32.36/0.915/0.050)\n(d) RESCAN (38.65/0.962/0.024)\n(e) MPRNet (45.62/0.982/0.011)\n(f) SIRR (22.31/0.689/0.132)\n(g) CycleGAN (30.27/0.856/0.040)\n(h) DeCyGAN (34.58/0.918/0.053)\n(i) SOT (ℓ1) (45.72/0.979/0.008)\nFig. 7. Visual comparison on real-world image deraining. The PSNR/SSIM/LPIPS results are provided in the brackets.\nTABLE 5\nQuantitative comparison of the dehazing methods on synthetic data\n(using the OTS dataset [96]).\nMethod\nPSNR/SSIM\nLPIPS/PI\nHazy\n18.13/0.851\n0.0747/2.96\nTraditional\nDCP [5]\n16.83/0.863\n0.0670/2.27\nAODNet [56]\n18.42/0.828\n0.0703/2.69\nDehamer [57]\n33.38/0.946\n0.0168/2.35\nGCANet [58]\n19.85/0.704\n0.0689/2.39\nSupervised\nFFANet [59]\n30.80/0.935\n0.0182/2.68\nD4 [66]\n21.89/0.845\n0.0466/2.39\nOT [16]\n28.83/0.919\n0.0236/2.60\nSOT (ℓ0.5)\n31.63/0.905\n0.0165/2.14\nUnsupervised\nSOT (ℓ1)\n32.20/0.935\n0.0157/2.08\npar with Dehamer.\n5.6\nReal-World Image Dehazing\nFor the real-world image dahazing task, we chose the\nreal scene dataset Dense-haze [87] for training and testing.\nThis dataset was obtained from two sets of images in the\nsame scene with fog and under normal conditions through\nartiﬁcial smoke. The artiﬁcial smoke in the dataset is quite\ndense and hence the restoration task is extremely challeng-\ning.\nTable 6 presents the results of the compared methods\non this dataset. Similar to the results in the synthetic de-\nhazing task, SOT achieves the best performance among the\nunsupervised methods. Besides, in term of the LPIPS and\nPI scores, its perception quality even surpasses that of the\nsupervised methods, i.e. the best among all the compared\nsupervised and unsupervised methods. Compared with the\nstandard OT method, the performance of SOT still improves\nconsiderably on this challenging realistic task.\nFig. 9 compares the visual quality of the methods. Note-\nworthily, the visual quality of SOT is distinctly better com-\npared with the state-of-the-art transformer based supervised\nmethod Dehamer, e.g. the color of the palette restored by\nSOT is much closer to the real scene. This task is quite\nchallenging as the observation (hazy images) is severely\ndegraded with an average PSNR of only about 10 dB. The\n12\n(a) Test image\n(b) Ground truth\n(c) Hazy (20.72 dB)\n(d) DCP (18.23/0.851/0.067)\n(e) Dehamer (34.62/0.953/0.017)\n(f) GCANet (21.69/0.756/0.070)\n(g) FFANet (32.81/0.936/0.019)\n(h) D4 (21.98/0.864/0.047)\n(i) SOT (ℓ1) (34.57/0.914/0.016)\nFig. 8. Visual comparison on synthetic image dehazing. The PSNR/SSIM/LPIPS results are provided in the brackets. The images are enlarged for\nclarity.\nTABLE 6\nQuantitative comparison of the compared methods on real-world data\n(using hteDense-haze dataset [87]).\nMethod\nPSNR/SSIM\nLPIPS/PI\nHazy\n10.55/0.435\n0.2057/7.04\nTraditional\nDCP [5]\n11.01/0.415\n0.3441/5.91\nAODNet [56]\n10.64/0.469\n0.2453/6.11\nDehamer [57]\n16.63/0.585\n0.1523/5.65\nGCANet [58]\n12.46/0.454\n0.2524/5.04\nSupervised\nFFANet [59]\n8.77/0.452\n0.1985/6.71\nD4 [66]\n9.69/0.462\n0.2140/5.03\nOT [16]\n14.17/0.503\n0.1699/4.89\nSOT (ℓ0.5)\n15.01/0.526\n0.1523/4.56\nUnsupervised\nSOT (ℓ1)\n15.40/0.567\n0.1451/4.50\nresults demonstrate the potential of the proposed method\non realistic difﬁcult tasks to handle complex degradation.\n6\nCONCLUSIONS\nAn unsupervised restoration learning method has been\ndeveloped, which exploits the sparsity of degradation in\nthe OT criterion to reduce the ambiguity in seeking an\ninverse map for the restoration problem. It is based on an\nobservation that, the degradation for some restoration tasks\nis sparse in the frequency domain. The proposed method\nhas been extensively evaluated in comparison with existing\nsupervised and unsupervised methods on super-resolution,\nderaining, and dehazing tasks. The results demonstrate that\nit can signiﬁcantly improve the performance of the OT\ncriterion to approach the performance of state-of-the-art\nsupervised methods. Particularly, among the compared su-\npervised and unsupervised methods, the proposed method\nachieves the best PSNR, SSIM and LPIPS results in real-\nworld super-resolution, and the best perception scores\n(LPIPS and PI) in real-world deraining and dehazing. Our\nmethod is the ﬁrst generic unsupervised method that can\nachieve favorable performance in comparison with state-\n13\n(a) Test image\n(b) Ground truth\n(c) Hazy (9.89 dB)\n(d) DCP (10.35/0.407/0.342)\n(e) Dehamer (15.87/0.569/0.155)\n(f) GCANet (13.62/0.487/0.256)\n(g) FFANet (8.61/0.464/0.199)\n(h) D4 (9.77/0.471/0.217)\n(i) SOT (ℓ1) (14.78/0.536/0.146)\nFig. 9. Visual comparison on a challenging real-word image dehazing task with severe haze. The PSNR/SSIM/LPIPS results are provided in the\nbrackets. The images are enlarged for clarity.\nof-the-art supervised methods on all the super-resolution,\nderaining, and dehazing tasks.\nREFERENCES\n[1]\nA. Buades, B. Coll, and J.-M. Morel, “A non-local algorithm for\nimage denoising,” in IEEE Conference Computer Vision and Pattern\nRecognition (CVPR), vol. 2, pp. 60–65, 2005.\n[2]\nK. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, “Image denois-\ning by sparse 3-d transform-domain collaborative ﬁltering,” IEEE\nTransactions on Image Processing, vol. 16, no. 8, pp. 2080–2095, 2007.\n[3]\nM. Elad and A. Feuer, “Restoration of a single superresolution\nimage from several blurred, noisy, and undersampled measured\nimages,” IEEE Transactions on Image Processing, vol. 6, no. 12,\npp. 1646–1658, 1997.\n[4]\nY. Luo, Y. Xu, and H. Ji, “Removing rain from a single image via\ndiscriminative sparse coding,” in IEEE/CVF International Confer-\nence on Computer Vision, pp. 3397–3405, 2015.\n[5]\nK. He, J. Sun, and X. Tang, “Single image haze removal using dark\nchannel prior,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 33, no. 12, pp. 2341–2353, 2010.\n[6]\nX. Mao, C. Shen, and Y.-B. Yang, “Image restoration using very\ndeep convolutional encoder-decoder networks with symmetric\nskip connections,” in Advances in Neural Information Processing\nSystems, pp. 2802–2810, 2016.\n[7]\nY. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, “Residual\ndense network for image restoration,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 43, no. 7, pp. 2480–2495, 2020.\n[8]\nJ. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Karras,\nM. Aittala, and T. Aila, “Noise2noise: Learning image restoration\nwithout clean data,” in International Conference on Machine Learning\n(ICML), pp. 4620–4631, 2018.\n[9]\nA. Krull, T.-O. Buchholz, and F. Jug, “Noise2void-learning denois-\ning from single noisy images,” in IEEE Conference Computer Vision\nand Pattern Recognition (CVPR), pp. 2129–2137, 2019.\n[10] J. Batson and L. Royer, “Noise2self: Blind denoising by self-\nsupervision,” in International Conference on Machine Learning\n(ICML), pp. 524–533, 2019.\n[11] T. Ehret, A. Davy, J.-M. Morel, G. Facciolo, and P. Arias, “Model-\nblind video denoising via frame-to-frame training,” in IEEE Con-\nference Computer Vision and Pattern Recognition (CVPR), pp. 11369–\n11378, 2019.\n[12] Y. Quan, M. Chen, T. Pang, and H. Ji, “Self2self with dropout:\nLearning\nself-supervised\ndenoising\nfrom\nsingle\nimage,”\nin\nIEEE Conference Computer Vision and Pattern Recognition (CVPR),\npp. 1890–1898, 2020.\n[13] M. Zhussip, S. Soltanayev, and S. Y. Chun, “Extending stein’s\nunbiased risk estimator to train deep denoisers with correlated\npairs of noisy images,” in Advances in Neural Information Processing\nSystems, pp. 1465–1475, 2019.\n[14] S. Laine, T. Karras, J. Lehtinen, and T. Aila, “High-quality self-\n14\nsupervised deep image denoising,” Advances in Neural Information\nProcessing Systems, vol. 32, pp. 6970–6980, 2019.\n[15] X. Wu, M. Liu, Y. Cao, D. Ren, and W. Zuo, “Unpaired learning of\ndeep image denoising,” in European Conference on Computer Vision\n(ECCV), pp. 352–368, 2020.\n[16] W. Wang, F. Wen, Z. Yan, and P. Liu, “Optimal transport for\nunsupervised denoising learning,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 45, no. 2, pp. 2104–2118, 2023.\n[17] A. Buades, B. Coll, and J.-M. Morel, “A review of image denoising\nalgorithms, with a new one,” Multiscale Modeling & Simulation,\nvol. 4, no. 2, pp. 490–530, 2005.\n[18] M. Lebrun, “An analysis and implementation of the bm3d image\ndenoising method,” Image Processing on Line, vol. 2012, pp. 175–\n213, 2012.\n[19] N. Moran, D. Schmidt, Y. Zhong, and P. Coady, “Noisier2noise:\nLearning to denoise from unpaired noisy data,” in IEEE Conference\nComputer Vision and Pattern Recognition (CVPR), pp. 12064–12072,\n2020.\n[20] J. Xu, Y. Huang, M.-M. Cheng, L. Liu, F. Zhu, Z. Xu, and\nL. Shao, “Noisy-as-clean: learning self-supervised denoising from\ncorrupted image,” IEEE Transactions on Image Processing, vol. 29,\npp. 9316–9329, 2020.\n[21] S. Cha, T. Park, B. Kim, J. Baek, and T. Moon, “Gan2gan: Genera-\ntive noise learning for blind denoising with single noisy images,”\nin International Conference on Learning Representations, 2020.\n[22] S. Soltanayev and S. Y. Chun, “Training deep learning based\ndenoisers without ground truth data,” in Advances in Neural In-\nformation Processing Systems, pp. 3261–3271, 2018.\n[23] A. Krull, T. Viˇcar, M. Prakash, M. Lalit, and F. Jug, “Probabilistic\nnoise2void: Unsupervised content-aware denoising,” Frontiers in\nComputer Science, vol. 2, p. 5, 2020.\n[24] R. Tsai, “Multiframe image restoration and registration,” Advance\nComputer Visual and Image Processing, vol. 1, pp. 317–339, 1984.\n[25] R. R. Schultz and R. L. Stevenson, “Extraction of high-resolution\nframes from video sequences,” IEEE Transactions on Image Process-\ning, vol. 5, no. 6, pp. 996–1011, 1996.\n[26] D. Glasner, S. Bagon, and M. Irani, “Super-resolution from a\nsingle image,” in IEEE International Conference on Computer Vision,\npp. 349–356, 2009.\n[27] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-resolution\nusing deep convolutional networks,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 38, no. 2, pp. 295–307, 2015.\n[28] J. Kim, J. K. Lee, and K. M. Lee, “Accurate image super-resolution\nusing very deep convolutional networks,” in IEEE Conference on\nComputer Vision and Pattern Recognition, pp. 1646–1654, 2016.\n[29] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,\nA. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al., “Photo-\nrealistic single image super-resolution using a generative adver-\nsarial network,” in IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 4681–4690, 2017.\n[30] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, and\nC. Change Loy, “Esrgan: Enhanced super-resolution generative\nadversarial networks,” in European Conference on Computer Vision\n(ECCV) Workshops, 2018.\n[31] W. Zhang, Y. Liu, C. Dong, and Y. Qiao, “Ranksrgan: Generative\nadversarial networks with ranker for image super-resolution,” in\nIEEE/CVF International Conference on Computer Vision, pp. 3096–\n3105, 2019.\n[32] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, “Image super-\nresolution using very deep residual channel attention networks,”\nin European Conference on Computer Vision (ECCV), pp. 286–301,\n2018.\n[33] Y. Zhang, K. Li, K. Li, B. Zhong, and Y. Fu, “Residual non-\nlocal attention networks for image restoration,” in International\nConference on Learning Representations (ICLR), 2018.\n[34] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial\nnets,” in Advances in Neural Information Processing Systems, 2014.\n[35] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-\nimage translation using cycle-consistent adversarial networks,” in\nIEEE/CVF International Conference on Computer Vision, pp. 2223–\n2232, 2017.\n[36] A. Lugmayr, M. Danelljan, and R. Timofte, “Unsupervised learn-\ning for real-world super-resolution,” in IEEE/CVF International\nConference on Computer Vision Workshop (ICCVW), pp. 3408–3416,\n2019.\n[37] Y. Yuan, S. Liu, J. Zhang, Y. Zhang, C. Dong, and L. Lin, “Un-\nsupervised image super-resolution using cycle-in-cycle generative\nadversarial networks,” in IEEE Conference on Computer Vision and\nPattern Recognition Workshops, pp. 701–710, 2018.\n[38] K. Prajapati, V. Chudasama, H. Patel, K. Upla, R. Ramachan-\ndra, K. Raja, and C. Busch, “Unsupervised single image super-\nresolution network (usisresnet) for real-world data using gener-\native adversarial network,” in IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) Workshops, pp. 464–465, 2020.\n[39] A. Bulat, J. Yang, and G. Tzimiropoulos, “To learn image super-\nresolution, use a gan to learn how to do image degradation ﬁrst,”\nin European Conference on Computer Vision (ECCV), pp. 185–200,\n2018.\n[40] T. Zhao, W. Ren, C. Zhang, D. Ren, and Q. Hu, “Unsupervised\ndegradation learning for single image super-resolution,” arXiv\npreprint arXiv:1812.04240, 2018.\n[41] L. Wang, Y. Wang, X. Dong, Q. Xu, J. Yang, W. An, and\nY. Guo, “Unsupervised degradation representation learning for\nblind super-resolution,” in IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 10581–10590, 2021.\n[42] N. Ahn, J. Yoo, and K.-A. Sohn, “Simusr: A simple but strong\nbaseline for unsupervised image super-resolution,” in IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops,\npp. 474–475, 2020.\n[43] X. Li, J. Wu, Z. Lin, H. Liu, and H. Zha, “Recurrent squeeze-and-\nexcitation context aggregation net for single image deraining,” in\nEuropean Conference on Computer Vision (ECCV), pp. 254–269, 2018.\n[44] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, M.-H.\nYang, and L. Shao, “Multi-stage progressive image restoration,”\nin IEEE Conference Computer Vision and Pattern Recognition (CVPR),\npp. 14821–14831, 2021.\n[45] W. Wei, D. Meng, Q. Zhao, Z. Xu, and Y. Wu, “Semi-supervised\ntransfer learning for image rain removal,” in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 3877–3886,\n2019.\n[46] X. Fu, J. Huang, X. Ding, Y. Liao, and J. Paisley, “Clearing the skies:\nA deep network architecture for single-image rain removal,” IEEE\nTransactions on Image Processing, vol. 26, no. 6, pp. 2944–2956, 2017.\n[47] R. Li, L.-F. Cheong, and R. T. Tan, “Heavy rain image restoration:\nIntegrating physics model and conditional adversarial learning,”\nin IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 1633–1642, 2019.\n[48] M. Mirza and S. Osindero, “Conditional generative adversarial\nnets,” arXiv preprint arXiv:1411.1784, 2014.\n[49] Y. Wei, Z. Zhang, Y. Wang, M. Xu, Y. Yang, S. Yan, and M. Wang,\n“Deraincyclegan: Rain attentive cyclegan for single image derain-\ning and rainmaking,” IEEE Transactions on Image Processing, vol. 30,\npp. 4788–4801, 2021.\n[50] X. Jin, Z. Chen, J. Lin, Z. Chen, and W. Zhou, “Unsupervised\nsingle image deraining with self-supervised constraints,” in IEEE\nInternational Conference on Image Processing (ICIP), pp. 2761–2765,\n2019.\n[51] H. Zhu, X. Peng, J. T. Zhou, S. Yang, V. Chanderasekh, L. Li, and\nJ.-H. Lim, “Singe image rain removal with unpaired information:\nA differentiable programming perspective,” in Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, vol. 33, pp. 9332–9339,\n2019.\n[52] Y. Ye, C. Yu, Y. Chang, L. Zhu, X.-l. Zhao, L. Yan, and Y. Tian,\n“Unsupervised deraining: Where contrastive learning meets self-\nsimilarity,” in IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 5821–5830, 2022.\n[53] R. Fattal, “Single image dehazing,” ACM Transactions on Graphics\n(TOG), vol. 27, no. 3, pp. 1–9, 2008.\n[54] A. Golts, D. Freedman, and M. Elad, “Unsupervised single image\ndehazing using dark channel prior loss,” IEEE Transactions on\nImage Processing, vol. 29, pp. 2692–2701, 2019.\n[55] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao, “Dehazenet: An end-to-\nend system for single image haze removal,” IEEE Transactions on\nImage Processing, vol. 25, no. 11, pp. 5187–5198, 2016.\n[56] B. Li, X. Peng, Z. Wang, J. Xu, and D. Feng, “Aod-net: All-in-\none dehazing network,” in IEEE/CVF International Conference on\nComputer Vision, pp. 4770–4778, 2017.\n[57] C.-L. Guo, Q. Yan, S. Anwar, R. Cong, W. Ren, and C. Li, “Im-\nage dehazing transformer with transmission-aware 3d position\nembedding,” in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 5812–5820, 2022.\n15\n[58] D. Chen, M. He, Q. Fan, J. Liao, L. Zhang, D. Hou, L. Yuan, and\nG. Hua, “Gated context aggregation network for image dehazing\nand deraining,” in IEEE Winter Conference on Applications of Com-\nputer Vision (WACV), pp. 1375–1383, 2019.\n[59] X. Qin, Z. Wang, Y. Bai, X. Xie, and H. Jia, “Ffa-net: Feature fusion\nattention network for single image dehazing,” in AAAI Conference\non Artiﬁcial Intelligence, vol. 34, pp. 11908–11915, 2020.\n[60] X. Yang, Z. Xu, and J. Luo, “Towards perceptual image dehazing\nby physics-based disentanglement and adversarial training,” in\nAAAI Conference on Artiﬁcial Intelligence, vol. 32, 2018.\n[61] S. Zhao, L. Zhang, Y. Shen, and Y. Zhou, “Reﬁnednet: A weakly\nsupervised reﬁnement framework for single image dehazing,”\nIEEE Transactions on Image Processing, vol. 30, pp. 3391–3404, 2021.\n[62] A. Dudhane and S. Murala, “Cdnet: Single image de-hazing us-\ning unpaired adversarial training,” in IEEE Winter Conference on\nApplications of Computer Vision (WACV), pp. 1147–1155, IEEE, 2019.\n[63] D. Engin, A. Genc¸, and H. Kemal Ekenel, “Cycle-dehaze: En-\nhanced cyclegan for single image dehazing,” in IEEE Conference\non Computer Vision and Pattern Recognition Workshops, pp. 825–833,\n2018.\n[64] J. Zhao, J. Zhang, Z. Li, J.-N. Hwang, Y. Gao, Z. Fang, X. Jiang, and\nB. Huang, “Dd-cyclegan: Unpaired image dehazing via double-\ndiscriminator cycle-consistent generative adversarial network,”\nEngineering Applications of Artiﬁcial Intelligence, vol. 82, pp. 263–\n271, 2019.\n[65] W. Liu, X. Hou, J. Duan, and G. Qiu, “End-to-end single image fog\nremoval using enhanced cycle consistent adversarial networks,”\nIEEE Transactions on Image Processing, vol. 29, pp. 7819–7833, 2020.\n[66] Y. Yang, C. Wang, R. Liu, L. Zhang, X. Guo, and D. Tao, “Self-\naugmented unpaired image dehazing via density and depth de-\ncomposition,” in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 2037–2046, 2022.\n[67] Z. Wang, G. Wu, H. R. Sheikh, E. P. Simoncelli, E.-H. Yang, and\nA. C. Bovik, “Quality-aware images,” IEEE Transactions on Image\nProcessing, vol. 15, no. 6, pp. 1680–1689, 2006.\n[68] A. Mittal, A. K. Moorthy, and A. C. Bovik, “No-reference image\nquality assessment in the spatial domain,” IEEE Transactions on\nImage Processing, vol. 21, no. 12, pp. 4695–4708, 2012.\n[69] A. K. Moorthy and A. C. Bovik, “Blind image quality assessment:\nFrom natural scene statistics to perceptual quality,” IEEE Transac-\ntions on Image Processing, vol. 20, no. 12, pp. 3350–3364, 2011.\n[70] M. A. Saad, A. C. Bovik, and C. Charrier, “Blind image quality\nassessment: A natural scene statistics approach in the dct domain,”\nIEEE Transactions on Image Processing, vol. 21, no. 8, pp. 3339–3352,\n2012.\n[71] Y. Blau and T. Michaeli, “The perception-distortion tradeoff,” in\nIEEE Conference Computer Vision and Pattern Recognition (CVPR),\npp. 6228–6237, 2018.\n[72] Z. Yan, F. Wen, R. Ying, C. Ma, and P. Liu, “On perceptual lossy\ncompression: The cost of perceptual reconstruction and an optimal\ntraining framework,” in International Conference on Machine Learn-\ning (ICML), pp. 11682–11692, 2021.\n[73] Y. Blau and T. Michaeli, “Rethinking lossy compression: The\nrate-distortion-perception tradeoff,” in International Conference on\nMachine Learning (ICML), pp. 675–685, 2019.\n[74] Z. Yan, F. Wen, and P. Liu, “Optimally controllable perceptual\nlossy compression,” in International Conference on Machine Learning\n(ICML), 2022.\n[75] G. Monge, “M´emoire sur la th´eorie des d´eblais et des remblais,”\nHistoire de l’Acad´emie Royale des Sciences de Paris, 1781.\n[76] L. Kantorovich, “On translation of mass,” in Dokl. AN SSSR,\nvol. 37, p. 20, 1942.\n[77] S. Kolouri, S. R. Park, M. Thorpe, D. Slepcev, and G. K. Rohde,\n“Optimal mass transport: Signal processing and machine-learning\napplications,” IEEE Signal Processing Magazine, vol. 34, no. 4,\npp. 43–59, 2017.\n[78] C. Villani, Topics in Optimal Transportation. No. 58, 2003.\n[79] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative\nadversarial networks,” in International Conference on Machine Learn-\ning (ICML), pp. 214–223, 2017.\n[80] G. G. Chrysos, J. Kossaiﬁ, and S. Zafeiriou, “Rocgan: Robust\nconditional gan,” International Journal of Computer Vision, vol. 128,\nno. 10, pp. 2665–2683, 2020.\n[81] A. Bora, E. Price, and A. G. Dimakis, “Ambientgan: Generative\nmodels from lossy measurements,” in International Conference on\nLearning Representations, 2018.\n[82] T. Kaneko and T. Harada, “Noise robust generative adversarial\nnetworks,” in IEEE Conference Computer Vision and Pattern Recogni-\ntion (CVPR), pp. 8404–8414, 2020.\n[83] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C.\nCourville, “Improved training of wasserstein gans,” in Advances\nin Neural Information Processing Systems, pp. 5767–5777, 2017.\n[84] M. Gazdieva, L. Rout, A. Korotin, A. Filippov, and E. Burnaev,\n“Unpaired image super-resolution with optimal transport maps,”\nPreprint arXiv:2202.01116, 2022.\n[85] X. YANG, W. Xiang, H. Zeng, and L. Zhang, “Real-world video\nsuper-resolution: A benchmark dataset and a decomposition\nbased learning scheme,” IEEE/CVF International Conference on Com-\nputer Vision, 2021.\n[86] T. Wang, X. Yang, K. Xu, S. Chen, Q. Zhang, and R. W. Lau,\n“Spatial attentive single-image deraining with a high quality real\nrain dataset,” in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 12270–12279, 2019.\n[87] C. O. Ancuti, C. Ancuti, M. Sbert, and R. Timofte, “Dense-haze:\nA benchmark for image dehazing with dense-haze and haze-free\nimages,” in IEEE International Conference on Image Processing (ICIP),\npp. 1014–1018, 2019.\n[88] G. Marjanovic and V. Solo, “Lq sparsity penalized linear regression\nwith cyclic descent,” IEEE Transactions on Signal Processing, vol. 62,\nno. 6, pp. 1464–1475, 2014.\n[89] F. Wen, P. Liu, Y. Liu, R. C. Qiu, and W. Yu, “Robust sparse recov-\nery in impulsive noise via ℓp-ℓ1 optimization,” IEEE Transactions\non Signal Processing, vol. 65, no. 1, pp. 105–118, 2017.\n[90] Y. Blau, R. Mechrez, R. Timofte, T. Michaeli, and L. Zelnik-Manor,\n“The 2018 pirm challenge on perceptual image super-resolution,”\nin Proceedings of the European Conference on Computer Vision (ECCV)\nWorkshops, pp. 0–0, 2018.\n[91] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The\nunreasonable effectiveness of deep features as a perceptual met-\nric,” in IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 586–595, 2018.\n[92] E. Agustsson and R. Timofte, “Ntire 2017 challenge on single\nimage super-resolution: Dataset and study,” in IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR) workshops,\npp. 126–135, 2017.\n[93] Z. Wang, J. Chen, and S. C. Hoi, “Deep learning for image super-\nresolution: A survey,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, vol. 43, no. 10, pp. 3365–3387, 2020.\n[94] H. Zhang, V. Sindagi, and V. M. Patel, “Image de-raining using a\nconditional generative adversarial network,” IEEE Transactions on\nCircuits and Systems for Video Technology, vol. 30, no. 11, pp. 3943–\n3956, 2019.\n[95] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan, “Deep joint\nrain detection and removal from a single image,” in IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR),\npp. 1357–1366, 2017.\n[96] B. Li, W. Ren, D. Fu, D. Tao, D. Feng, W. Zeng, and Z. Wang,\n“Benchmarking single-image dehazing and beyond,” IEEE Trans-\nactions on Image Processing, vol. 28, no. 1, pp. 492–505, 2018.\n",
  "categories": [
    "cs.CV",
    "eess.IV"
  ],
  "published": "2023-04-29",
  "updated": "2023-04-29"
}