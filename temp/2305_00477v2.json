{
  "id": "http://arxiv.org/abs/2305.00477v2",
  "title": "Posterior Sampling for Deep Reinforcement Learning",
  "authors": [
    "Remo Sasso",
    "Michelangelo Conserva",
    "Paulo Rauber"
  ],
  "abstract": "Despite remarkable successes, deep reinforcement learning algorithms remain\nsample inefficient: they require an enormous amount of trial and error to find\ngood policies. Model-based algorithms promise sample efficiency by building an\nenvironment model that can be used for planning. Posterior Sampling for\nReinforcement Learning is such a model-based algorithm that has attracted\nsignificant interest due to its performance in the tabular setting. This paper\nintroduces Posterior Sampling for Deep Reinforcement Learning (PSDRL), the\nfirst truly scalable approximation of Posterior Sampling for Reinforcement\nLearning that retains its model-based essence. PSDRL combines efficient\nuncertainty quantification over latent state space models with a specially\ntailored continual planning algorithm based on value-function approximation.\nExtensive experiments on the Atari benchmark show that PSDRL significantly\noutperforms previous state-of-the-art attempts at scaling up posterior sampling\nwhile being competitive with a state-of-the-art (model-based) reinforcement\nlearning method, both in sample efficiency and computational efficiency.",
  "text": "Posterior Sampling for Deep Reinforcement Learning\nRemo Sasso 1 Michelangelo Conserva 1 Paulo Rauber 1\nAbstract\nDespite remarkable successes, deep reinforce-\nment learning algorithms remain sample inefﬁ-\ncient: they require an enormous amount of trial\nand error to ﬁnd good policies. Model-based\nalgorithms promise sample efﬁciency by build-\ning an environment model that can be used for\nplanning. Posterior Sampling for Reinforcement\nLearning is such a model-based algorithm that\nhas attracted signiﬁcant interest due to its per-\nformance in the tabular setting. This paper in-\ntroduces Posterior Sampling for Deep Reinforce-\nment Learning (PSDRL), the ﬁrst truly scalable\napproximation of Posterior Sampling for Rein-\nforcement Learning that retains its model-based\nessence. PSDRL combines efﬁcient uncertainty\nquantiﬁcation over latent state space models with\na specially tailored continual planning algorithm\nbased on value-function approximation. Exten-\nsive experiments on the Atari benchmark show\nthat PSDRL signiﬁcantly outperforms previous\nstate-of-the-art attempts at scaling up posterior\nsampling while being competitive with a state-\nof-the-art (model-based) reinforcement learning\nmethod, both in sample efﬁciency and computa-\ntional efﬁciency.\n1. Introduction\nIn a typical reinforcement learning problem, an agent inter-\nacts with an environment in a sequence of episodes by ob-\nserving states and rewards and acting according to a policy\nthat maps states to actions. A reinforcement learning algo-\nrithm seeks a policy that maximizes expected cumulative\nrewards. Because many problems in healthcare, robotics, lo-\ngistics, ﬁnance, and advertising can be naturally formulated\nas problems of maximizing a measure of success through a\nsequence of decisions informed by data, the recent successes\n1School of Electronic Engineering and Computer Science,\nQueen Mary University of London, United Kingdom. Correspon-\ndence to: Remo Sasso <r.sasso@qmul.ac.uk>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\nof reinforcement learning have attracted signiﬁcant inter-\nest. In particular, the combination of reinforcement learning\nwith artiﬁcial neural networks has led to the best computer\nagents that play games such as Chess and Go (Schrittwieser\net al., 2020), Dota 2 (Berner et al., 2019), and StarCraft II\n(Vinyals et al., 2019).\nDespite these notable successes, the corresponding rein-\nforcement learning algorithms are remarkably sample inefﬁ-\ncient: they require an enormous amount of trial-and-error\nto ﬁnd good policies. In contrast with games and simula-\ntions, real-world applications are heavily constrained by the\ncost of trial-and-error and available data. As a consequence,\nsample inefﬁciency limits the applicability of reinforcement\nlearning. This inefﬁciency is fundamentally linked with the\ntrade-off between exploring an environment in order to learn\nabout potentially better sources of reward and exploiting the\nwell-known sources of reward. In the tabular reinforcement\nlearning setting, where the number of states is ﬁnite, several\n(theoretically and often empirically) efﬁcient exploration\nmethods are well understood (Osband et al., 2013; Agrawal\n& Jia, 2017; Azar et al., 2017; Zanette & Brunskill, 2019;\nRusso, 2019; M´enard et al., 2021). However, there are no\ngenerally efﬁcient methods that cope with the non-linear\nfunction approximation required in non-tabular settings.\nModel-based reinforcement learning methods seek sample\nefﬁciency by building an environment model that enables\npredicting how actions affect the state of the environment\nand how the states of the environment relate to rewards.\nBecause such a model can be used for planning (searching\nfor a good policy without interacting with the environment),\nmodel-based methods have the potential to be substantially\nmore sample efﬁcient than model-free algorithms (Kaiser\net al., 2019; Janner et al., 2019), which attempt to ﬁnd good\npolicies without building a model. Recent work has shown\nthat learning models in latent state space can signiﬁcantly re-\nduce the computational cost of model-based reinforcement\nlearning (Ha & Schmidhuber, 2018; Hafner et al., 2019b;a;\nSchrittwieser et al., 2020), allowing its application in envi-\nronments with high-dimensional state spaces.\nPosterior Sampling for Reinforcement Learning is a model-\nbased algorithm that has attracted signiﬁcant interest due\nto its strong (theoretical and empirical) performance in the\ntabular setting (Osband et al., 2013). This algorithm repre-\n1\narXiv:2305.00477v2  [cs.LG]  17 May 2023\nPosterior Sampling for Deep Reinforcement Learning\nsents its knowledge about an environment by a distribution\nover environment models and repeats the following steps: a\nsingle model is drawn from the distribution over models; an\noptimal policy is found for this model; this policy is used\nto interact with the environment for one episode; and the re-\nsulting data are used to update the distribution over models.\nIntuitively, exploration decreases as knowledge increases.\nThis paper introduces Posterior Sampling for Deep Rein-\nforcement Learning (PSDRL), the ﬁrst truly scalable approx-\nimation of Posterior Sampling for Reinforcement Learning\nthat retains its model-based essence. PSDRL encodes a high-\ndimensional state into a lower-dimensional latent state to en-\nable predicting transitions in latent state space for any given\naction. PSDRL represents uncertainty through a Bayesian\nneural network that maintains a distribution over the pa-\nrameters of this transition model. This enables sampling a\nmodel that can be used for planning, which is accomplished\nby value function approximation with an artiﬁcial neural\nnetwork. This so-called value network retains information\nacross sampled models, which is crucial for sample efﬁ-\nciency. Completing the algorithm, the data collected by the\nagent while acting greedily with respect to the value network\nis regularly used to update the latent state representations\nand the distribution over the parameters of the model.\nExtensive experiments on the Atari benchmark (Bellemare\net al., 2013) show that PSDRL signiﬁcantly outperforms pre-\nvious state-of-the-art attempts at scaling up posterior sam-\npling (Bootstrapped DQN with randomized priors (Osband\net al., 2018) and Successor Uncertainties (Janz et al., 2019)).\nThey also show that PSDRL is competitive with a state-\nof-the-art (model-based) reinforcement learning method\n(DreamerV2 (Hafner et al., 2020)), both in sample efﬁciency\nand computational efﬁciency.\nThe remaining text is organized as follows. Section 2 relates\nour contributions to previous works. Section 3 describes PS-\nDRL in technical detail. Section 4 presents the experimental\nprotocol and its results. Finally, Section 5 summarizes our\ncontributions and suggests future work.\n2. Related works\nModel-based reinforcement learning has historically un-\nderperformed in complex environments that require non-\nlinear function approximation when compared with model-\nfree reinforcement learning. However, two model-based\nmethods have recently matched (or surpassed) the perfor-\nmance of model-free methods in the common Atari game\nplaying benchmark (Schrittwieser et al. (2020) and Hafner\net al. (2020)). This is an important development since plan-\nning has the potential to make model-based methods highly\nsample efﬁcient (Kaiser et al., 2019; Janner et al., 2019).\nExploration methods that cope with the non-linear func-\ntion approximation required in challenging environments\ncan be based on one of four foundations (Badia et al., 2020):\ndomain-speciﬁc knowledge, unsupervised policy learning,\nintrinsic motivation, or posterior sampling. Domain-speciﬁc\nknowledge methods typically combine human demonstra-\ntions, handcrafted features, and heuristics (Aytar et al., 2018;\nEcoffet et al., 2021). Despite their potential to be highly\nsample efﬁcient in their target environments, adapting these\nmethods to new environments requires signiﬁcant effort\nand expertise. Unsupervised policy learning methods en-\ncourage agents to acquire a diverse set of skills without\nreceiving reward signals (Eysenbach et al., 2018). Given\nsuch limited feedback, identifying generally useful skills for\nefﬁcient exploration through reuse and composition is a dif-\nﬁcult task. Intrinsic motivation methods aim to encourage\n(re)visiting states with bonus rewards derived from ensem-\nbles of Q-functions (Chen et al., 2017; Bai et al., 2021;\nTiapkin et al., 2022), visitation counts (Bellemare et al.,\n2016; Rashid et al., 2020), or episodic curiosity (Savinov\net al., 2018; Badia et al., 2019). Although many of these\nexploration methods are certainly promising and often ef-\nfective, our focus on posterior sampling is justiﬁed by the\nfact that Posterior Sampling for Reinforcement Learning is\nthe simplest among the potentially scalable and principled\nmethods that is capable of leveraging the strengths of both\nBayesian methods and model-based reinforcement learning.\nPosterior Sampling for Reinforcement Learning has\nbeen scaled up to non-tabular settings by Tziortziotis et al.\n(2013) and Fan & Ming (2021). Both works rely on state-\naction embeddings and a posterior distribution based on\nBayesian linear regression. Tziortziotis et al. (2013) em-\nploy a ﬁxed embedding and explore different approximate\ndynamic programming techniques (such as least-square pol-\nicy iteration), which makes their algorithm limited to very\nsimple environments. Fan & Ming (2021) learn state-action\nembeddings and use model predictive control (Camacho &\nBordons, 2007) via the cross-entropy method (Botev et al.,\n2013) for planning instead of utilizing a value function, also\nlimiting scalability relatively simple environments. The\nmain obstacle in scaling up model-based posterior sampling\nto complex environments is the extreme computational and\nmemory cost of searching for a good policy for a sampled\nmodel while retaining too little information from previous\nsearches.\nRandomized value function methods are the model-free\ncounterparts of Posterior Sampling for Reinforcement\nLearning (Osband et al., 2016a; 2018; 2019). These meth-\nods (implicitly) represent their knowledge about the optimal\nvalue function by a distribution over value functions and\nrepeat the following steps: a single value function is (im-\nplicitly) drawn from the distribution over value functions; a\ngreedy policy is derived from this value function; this policy\nis used to interact with the environment during one (or more)\n2\nPosterior Sampling for Deep Reinforcement Learning\nepisodes; and the resulting data are used to (approximately)\nupdate the (implicit) distribution over value functions. In\nthis context, Osband et al. (2016b) approximate random-\nized least-square value iteration. Engel et al. (2003; 2005)\nemploy Gaussian processes trained via temporal-difference\nlearning to induce a distribution over Q-functions. Similarly,\nAzizzadenesheli et al. (2018), O’Donoghue et al. (2018),\nand Janz et al. (2019) rely on Bayesian linear regression\non top of learned state embeddings, while Flennerhag et al.\n(2020) approximate a distribution over temporal differences.\nHowever, explicitly maintaining and accurately updating\na distribution that represents knowledge about the optimal\nvalue function is generally infeasible (Osband et al., 2019),\nand natural approximations of this approach may fail to\nrealize its potential (Janz et al., 2019). This is partially due\nto the fact that randomized value function methods are often\nbased on ﬁtted value iteration, which provides unreliable\nand noisy learning targets (Kumar et al., 2019; 2020). In\ncontrast with randomized value functions, model-based pos-\nterior sampling methods have the potential advantage of\nseparating uncertainty about the environment from uncer-\ntainty about the optimal value function (or policy).\n3. Posterior Sampling for Deep\nReinforcement Learning\nThis section introduces the PSDRL algorithm, which ap-\nproximates Posterior Sampling for Reinforcement Learning\nwhile retaining its model-based essence. Section 3.1 intro-\nduces our notation. Section 3.2 describes the latent state\nspace transition model. Section 3.3 explains how uncer-\ntainty over these models is represented. Section 3.4 details\nplanning based on sampled models. Finally, Section 3.5\njustiﬁes important implementation choices and reports po-\ntential pitfalls in scaling up model-based posterior sampling.\n3.1. Preliminaries\nWe consider a deterministic sequential decision-making\nproblem where, at a given time step t, an agent observes a\nstate st from a state space S and chooses an action at from\nan action space A using a policy π : S →A, which results\nin a next state st+1 = T(st, at) and reward rt = R(st, at).\nThe goal of the agent is to ﬁnd a policy π that maximizes\nthe return G(π) = P∞\nt=0 γtrt, where γ ∈[0, 1) is a dis-\ncount factor. The action-value Qπ(s, a) is the return from\nfollowing policy π starting from state s and action a, while\nthe value Vπ(s) is given by Vπ(s) = Qπ(s, π(s)).\n3.2. Transition model\nInspired by the recent remarkable successes of latent space\nmodels in model-based reinforcement learning (Ha &\nSchmidhuber, 2018; Schrittwieser et al., 2020; Hafner et al.,\n2019a; 2020), PSDRL employs a latent space transition\nmodel (Fig. 1) composed of the following components\n(subscripts denoting parameters): autoencoder (Eχ, Dχ);\nforward model fθ; and termination model ωη .\nThe autoencoder is convolutional (Masci et al., 2011). For\na given state st, the encoder Eχ produces a latent state\nzt = Eχ(st) that may be decoded by the decoder Dχ into a\nstate estimate ˆst = Dχ(zt). Encoding a state space into a\nlower dimensional latent state space enables more efﬁcient\nplanning. The forward model is an artiﬁcial neural network\nwith a Gated Recurrent Unit (GRU, Bahdanau et al., 2015)\nthat aggregates temporal information to deal with partial\nobservability. The forward model receives a latent state zt\nand an action at together with its hidden state ht from a\nprevious time step t and outputs a next latent state estimate\nˆzt+1, a next reward estimate ˆrt, and a next hidden state ht+1.\nThe termination model ωη receives a latent state zt and\noutputs an estimate ˆδt ∈[0, 1] of whether zt corresponds to\nan absorbing state (end of a so-called episode).\nThe parameters of these three models are iteratively updated\nin a speciﬁc order (autoencoder, forward model, then ter-\nmination model) using mini-batches stochastic gradient de-\nscent. A batch B = {{(si,t, ai,t, ri,t, si,t+1, δi,t)}L−1\nt=0 }B−1\ni=0\nis composed of B sequences of length L. Each sequence\ncorresponds to an episode chosen at random from a replay\nbuffer D with capacity C and starts at a time step chosen at\nrandom within the corresponding episode.\nFirst, the autoencoder parameters χ are updated to minimize\nthe reconstruction loss LAE given by\nLAE(χ) =\n1\nBL\nB−1\nX\ni=0\nL−1\nX\nt=0\n∥Dχ(Eχ(si,t)) −si,t∥2.\n(1)\nIf we let zi,t = Eχ(si,t), the forward model parameters θ\nare then updated to minimize the loss LF given by\nLF(θ) =\n1\nBL\nB−1\nX\ni=0\nL−1\nX\nt=0\n∥ˆzi,t+1 −zi,t+1∥2 + (ˆri,t −ri,t)2, (2)\nwhere (ˆzi,t+1, ˆri,t, hi,t+1) = fθ(zi,t, ai,t, hi,t) and hi,0 =\n0 for every i. Finally, the termination model parameters η\nare updated to minimize the loss LT given by\nLT(η) =\n1\nBL\nB−1\nX\ni=0\nL−1\nX\nt=0\n(ωη(zi,t+1) −δi,t)2,\n(3)\nwhere δi,t indicates whether si,t+1 is an absorbing state.\n3.3. Uncertainty model\nPSDRL represents its uncertainty about the forward model\nusing the neural-linear method (Snoek et al., 2015). This\napproach achieves remarkable success when combined with\n3\nPosterior Sampling for Deep Reinforcement Learning\n(a)\n(b)\nFigure 1. (a) The autoencoder learns latent state representations zt for true environment states st through reconstruction. (b) The forward\nmodel learns to predict rewards rt and next latent states zt+1 given the current latent states zt, actions at, and previous hidden states ht.\nposterior sampling in the contextual bandits setting, espe-\ncially when compared to other (often much more compu-\ntationally expensive) implementations of Bayesian neural\nnetworks (Riquelme et al., 2018).\nLinear forward model. In our context, the neural-linear\napproach models uncertainty over the parameters W of a\nforward model gW : X →Y that ideally maps each possible\nvector x = (z, a, h) to a vector y = (z′, r′), where z is a\nlatent state, a is an action, h is a hidden state, z′ is the\nresulting latent state, and r′ is the resulting reward.\nFirst, suppose there is a known feature map φ : X →Rk\nsuch that y = gW ∗(x) = W ∗φ(x) for every input x ∈X\nand some unknown matrix W ∗. In words, suppose that the\noutput of the (unknown) forward model gW ∗is the result\nof multiplying an (unknown) parameter matrix W ∗by a\n(known) feature vector φ(x) that represents the input x. Un-\nder this assumption, Bayesian linear regression (Rasmussen,\n2005) may be used to model uncertainty over each row wj\nof the parameter matrix W ∗, which is associated with pre-\ndicting the j-th element yj of the output vector y given the\ninput x. Note that yj is either an element of the next latent\nstate z′ or a reward r′.\nIn order to enable efﬁcient inference, PSDRL supposes\nthat the prior density for the vector wj is given by N(wj |\n0, σ2\nj I), where N(· | µ, Σ) denotes the multivariate normal\ndensity function with mean µ and covariance matrix Σ and\nI denotes the identity matrix. If the index j corresponds to\na latent state element, we let σ2\nj = σ2\nS for a hyperparameter\nσ2\nS. If the index j corresponds to a reward, we let σ2\nj = σ2\nR\nfor a hyperparameter σ2\nR.\nConsider a dataset {(x(i), y(i))}N\ni=1 composed of all latent\nstate transitions derived from the replay buffer D. Let Φ\ndenote a matrix whose i-th row is given by φi = φ(x(i)),\nand let tj denote the j-th vector of targets such that tj =\n(y(1)\nj , y(2)\nj , . . . , y(N)\nj\n). In order to account for potential mod-\neling errors while still enabling efﬁcient inference, PSDRL\nalso supposes that the likelihood of the parameter vector wj\nis given by N(tj | Φwj, σ2I), where the noise variance σ2\nis a hyperparameter. Under these assumptions, the posterior\ndensity for the vector wj is given by N(wj | µj, Σj), where\nΣ−1\nj\n= σ−2\nj I + σ−2ΦT Φ\nand\nµj = σ−2ΣjΦT tj. (4)\nLet ˜wj denote a parameter vector drawn from this posterior\ndistribution, and let ˜W denote a matrix whose j-th row is\ngiven by ˜wj. For a given feature map φ, the forward model\ng ˜\nW may be employed to predict the output ˆy = g ˜\nW (x) =\n˜Wφ(x) for a given input x. In other words, g ˜\nW corresponds\nto an environment model sampled from the posterior, as\nrequired by Posterior Sampling for Reinforcement Learning.\nFeature map learning. The previous paragraphs have pre-\nsupposed the existence of a known feature map φ : X →Rk\nsuch that an output y could be predicted by y = gW ∗(x) =\nW ∗φ(x) for every input x ∈X and some unknown matrix\nW ∗. In practice, the neural-linear approach derives this\nfeature map from an (iteratively trained) forward model.\nMore concretely, the architecture of an artiﬁcial neural net-\nwork fθ is chosen such that its prediction ˆy for an input x is\ngiven by ˆy = fθ(x) = Wφθ(x), where φθ is a feature map\nsubnetwork and the matrix W is contained in θ.\nIn summary, the neural-linear approach represents uncer-\ntainty solely over the parameters of the output layer of an\nartiﬁcial neural network whose output layer is a linear func-\ntion of the last hidden layer while disregarding uncertainty\nabout the parameters of earlier layers.\n3.4. Planning\nPosterior Sampling for Reinforcement Learning prescribes\nsampling a forward model f˜θ from the corresponding pos-\nterior distribution and ﬁnding and following an optimal\npolicy ˜π∗for this model until the end of an episode. Nat-\nurally, it is infeasible to ﬁnd such an optimal policy in the\nnon-tabular setting. This section describes how PSDRL efﬁ-\nciently searches for a policy for a sampled forward model.\nValue function approximation. Consider once again a\n4\nPosterior Sampling for Deep Reinforcement Learning\nbatch B composed of B sequences of length L obtained\nfrom the replay buffer D. Let f˜θ denote a forward model\nsampled from the posterior such that ˜θ is composed of a\nsampled matrix ˜W and parameters of a feature map subnet-\nwork. PSDRL attempts to approximate the value function\nV˜π∗of an optimal policy ˜π∗for the sampled forward model\nf˜θ through a continual procedure that updates the parame-\nters of an artiﬁcial neural network Vψ. The parameters ψ are\nupdated through mini-batches stochastic gradient descent to\nminimize the loss LV given by\nLV (ψ) =\n1\nBL\nB−1\nX\ni=0\nL−1\nX\nt=0\nL(i,t)\nV\n(ψ),\n(5)\nwhere\nL(i,t)\nV\n(ψ) =\n\u0010\nVψ(zi,t, hi,t) −max\na\nh\nˆr(a)\ni,t + γˆv(a)\ni,t+1\ni\u00112\n,\n(ˆz(a)\ni,t+1, ˆr(a)\ni,t , h(a)\ni,t+1) = f˜θ(zi,t, a, hi,t),\n(6)\nˆv(a)\ni,t+1 = 1[ωη(ˆz(a)\ni,t+1) < 0.5]Vψ′(ˆz(a)\ni,t+1, h(a)\ni,t+1),\n(7)\nzi,t = Eχ(si,t), hi,0 = 0, hi,t+1 = h(ai,t)\ni,t\n, and ψ′ are\nparameters stored from a previous iteration.\nThis approach is highly related to ﬁtted value iteration\n(Munos & Szepesv´ari, 2008). In summary, the predicted\nvalue of each latent state is updated to become closer to the\nmaximum (considering all actions) predicted next reward\nplus discounted value predicted for the next predicted latent\nstate (treated as a constant). The current forward model f˜θ\nis responsible for predicting next latent states and rewards.\nGreedy policy. PSDRL derives a (greedy) policy ˜π from\nthe value network Vψ such that\n˜π(zt, ht) = argmax\na\nh\nˆr(a)\nt\n+ γˆv(a)\nt+1\ni\n,\n(8)\nwhere zt is a latent state, ht is a hidden state, ˆr(a)\nt\nis the\npredicted reward after action a, and ˆv(a)\nt+1 is the predicted\nvalue for the predicted next latent state, which are obtained\nin analogy with Equations 6 and 7 (except ψ overrides ψ′).\nAlgorithm 1 summarizes PSDRL (forward models are sam-\npled every m time steps instead of episodically).\n3.5. Rationale\nThis section discusses crucial implementation choices that\nenable PSDRL to scale up to complex environments.\nThe choice of a latent state space transition model (Sec. 3.2)\nenables more efﬁcient planning in comparison with employ-\ning a (higher-dimensional) state space transition model. It\nis also justiﬁed by the recent success of latent state space\nmodels in model-based reinforcement learning . Similarly,\nthe choice of the neural-linear approach for modeling uncer-\ntainty over forward models (Sec. 3.3) enables (relatively)\nAlgorithm 1 PSDRL\n1: D ←empty (FIFO) buffer with capacity C\n2: s0 ←initial state\n3: h0 ←0\n4: for each t ∈{0, . . . , T −1} do\n5:\nif t mod m = 0 then\n6:\nχ ←update for autoencoder loss LAE (Eq. 1)\n7:\nθ ←update for forward model loss LF (Eq. 2)\n8:\nη ←update for termination model loss LT (Eq. 3)\n9:\nµj, Σj ←update posterior for each j (Eq. 4)\n10:\n˜W ←sample from posteriors\n11:\nf˜θ ←forward model derived from ˜W and feature\nmap subnetwork φθ\n12:\nψ ←update for value network loss LV (Eq. 5)\nbased on the sampled forward model f˜θ\n13:\nend if\n14:\nzt ←Eχ(st)\n15:\nat ←˜π(zt, ht) (Eq. 8)\n16:\nrt, st+1, δ ←outcome of action at\n17:\nht+1 ←corresponding output from f˜θ(zt, at, ht)\n18:\nD ←D ∪{(st, at, rt, st+1, δ)}\n19:\nif δ = 1 then\n20:\nst+1 ←initial state\n21:\nht+1 ←0\n22:\nend if\n23: end for\ninexpensive posterior sampling. Furthermore, the neural-\nlinear approach has achieved remarkable empirical success\nwith posterior sampling in the contextual bandits setting.\nHowever, the choice of planning algorithm (Sec. 3.4) is\ncomparatively much more involved since it requires care-\nful consideration of the strengths and weaknesses of the\naforementioned components. In particular, the planning\nalgorithm needs to avoid the following potential pitfalls.\nRecency bias. The posterior update step (Alg. 1, line 9)\nimplicitly requires recomputing all latent state transitions\nderived from the replay buffer D. This is a consequence of\nthe fact that the latent state representations are learned (as\nopposed to given), which precludes the typical Bayesian ap-\nproach of using a posterior as the new prior when additional\ndata is acquired. Therefore, although the forward model\nparameters θ are potentially inﬂuenced by all previous tran-\nsitions (since they reﬂect all previous updates), the posterior\nover (output layer) parameters is strictly inﬂuenced by the\ntransitions in the replay buffer. Because the replay buffer\nis necessarily limited in capacity, this introduces a bias that\nmay severely impact the efﬁciency of the algorithm.\nPrevious works that adapt Posterior Sampling for Reinforce-\nment Learning to non-tabular settings (Tziortziotis et al.,\n2013; Fan & Ming, 2021) sidestep this pitfall by employ-\n5\nPosterior Sampling for Deep Reinforcement Learning\ning ﬁxed state representations or an unlimited replay buffer,\nwhich pose signiﬁcant scalability challenges.\nPSDRL addresses this recency bias by retaining informa-\ntion obtained from previous sampled models in a value\nnetwork Vψ through continual training. More concretely,\nthe value network trained for a previous sampled model is\nused as the starting point for the current sampled model.\nTherefore, even if the entire replay buffer D is composed\nof relatively uninformative transitions (for instance, zero-\nreward transitions), the decisions of the agent are potentially\ninﬂuenced by previous planning results. This is notably\ndistinct from the planning approaches that rely entirely on a\ncurrent sampled model employed in previous works. In com-\nparison with search methods, derivative-free optimization,\nand policy gradient methods, value-function approximation\nis naturally suited to aggregate information across sampled\nmodels, which justiﬁes its choice. Importantly, this contin-\nual approach also improves planning efﬁciency when the\ncurrent sampled model is similar to the previous sampled\nmodel. However, training a value function approximator\nacross sampled models introduces another potential pitfall.\nStatus quo bias. Posterior Sampling for Reinforcement\nLearning prescribes sampling a forward model from the cor-\nresponding posterior distribution and following an optimal\npolicy for this model for at least one episode. The fact that a\nposterior sampling agent may radically change its behavior\nbased on a sampled model is crucial for efﬁcient exploration.\nTherefore, employing a value network that aggregates infor-\nmation across sampled models as a starting point to train a\nvalue network for a current model may bias the agent in a\nway that is detrimental to exploration.\nPSDRL addresses this status quo bias by choosing actions\nthat maximize the one-step return, which combines the next\nreward predicted by the sampled model with the value pre-\ndicted by the value network for the next (latent) state pre-\ndicted by the sampled model (Eq. 8). For instance, instead\nof considering the one-step return, it would be possible to\ntrain a typical action-value network Qψ and choose the ac-\ntion argmaxa Qψ(zt, ht, a) for any given latent state zt and\nhidden state zt. However, unless Qψ changes signiﬁcantly\nafter a relatively brief training procedure (Alg. 1, line 12),\nthe resulting policy (Alg. 1, line 15) could fail to incorpo-\nrate sufﬁcient feedback from the current sampled model. In\norder to further reduce the status quo bias, it would also be\npossible (but more expensive) to consider the k-step return\nof the greedy policy with respect to Vψ (or Qψ).\nIn summary, there is a natural trade-off between recency\nbias and status quo bias. Section 4.3 reports an ablation\nstudy that conclusively shows that disregarding planning\nresults from previous sampled models is not a suitable al-\nternative under realistic computational constraints, whereas\nour choices lead to efﬁcient exploration.\nTable 1. Median and mean human-normalized score (Mnih et al.,\n2015), mean record-normalized score (Toromanoff et al., 2019),\nand mean clipped record-normalized score (Hafner et al., 2020).\nMetric\nPSDRL\nDv2\nB+P\nSU\nGamer Median\n23%\n18%\n7%\n0%\nGamer Mean\n58%\n100%\n0%\n21%\nRecord Mean\n6%\n8%\n0%\n0%\nClipped Record Mean\n6%\n8%\n0%\n0%\n4. Experiments\n4.1. Experimental protocol\nEnvironments. We provide an experimental comparison\nbetween PSDRL and other algorithms on 55 Atari 2600\ngames that are commonly used in the literature (Mnih et al.,\n2015). We evaluate each algorithm on all 55 games using\nno full action space, no access to life information, no sticky\nactions, and an action repeat of four for three random seeds.\nIn order to keep to a feasible computational budget, we\nrestrict environment steps to 1M (4M frames).\nAlgorithms. Since previous model-based posterior sam-\npling algorithms do not scale to complex environments, we\nmake a comparison with two state-of-the-art randomized\nvalue function (RVF) algorithms: Bootstrapped DQN with\nrandomized priors (B+P, Osband et al., 2018) and Succes-\nsor Uncertainties (SU, Janz et al., 2019). Additionally, we\nmake a comparison with the model-based algorithm Dream-\nerV2 (Dv2, Hafner et al., 2020), which is the state-of-the-art\nsingle-GPU algorithm for the Atari benchmark.\nEvaluation metrics. Traditionally, authors report the mean\nand median human-normalized scores across all games\n(gamer mean and gamer median) (Mnih et al., 2015). A\nhuman-normalized score of zero corresponds to a randomly\nacting agent, whereas a human-normalized score of one\ncorresponds to a professional gamer. However, Toromanoff\net al. (2019) convincingly argue that the gamer mean can\nbe misleading due to outliers, whereas the gamer median\ncan be misleading if almost half of the scores are zero.\nTherefore, they propose to normalize according to the hu-\nman world records instead. Hafner et al. (2020) further\npropose the clipped record-normalized score. A record-\nnormalized score of zero corresponds to a randomly act-\ning agent, whereas a record-normalized score of one corre-\nsponds to a world record performance. Performances that\nexceed the world record receive a clipped record-normalized\nscore of one. We report both traditional human-normalized\nscores and (clipped) record-normalized scores, which are\ncomputed using the average evaluation return across 1M\nenvironment steps, measured every 10k environment steps.\nHyperparameters. As the original hyperparameters for the\n6\nPosterior Sampling for Deep Reinforcement Learning\nbaseline algorithms were tuned for 200M frames, and sticky\nactions in the case of Dv2, we have tuned each baseline\nfor the deterministic 4M frames setting to guarantee a fair\ncomparison. Hyperparameters are obtained with an exhaus-\ntive grid search on six Atari games commonly used for this\npurpose (Munos et al., 2016; Janz et al., 2019): ASTERIX,\nENDURO, FREEWAY, HERO, QBERT, and SEAQUEST.\nReproducibility. The source code for replicating all experi-\nments is available as supplementary material. A description\nof the hyperparameter search procedure and other imple-\nmentation decisions are available in Appendix D.\n4.2. Main results\nThe results for each evaluation metric are summarized in\nTable 1. Note that although SU signiﬁcantly outperforms\nB+P in terms of gamer mean, this is almost entirely due\nto its excellent performance on BERZERK, which is just\none example of how this traditional metric is ﬂawed. A\nmore informative visualization comparing PSDRL to each\nbaseline algorithm in terms of human-normalized score is\npresented in Figure 4 (Appendix C details this visualization).\nThe raw game scores and corresponding learning curves for\neach individual game can be found in Appendices A and B,\nrespectively. Remarkably, the results show that PSDRL is\ncompetitive with the state-of-the-art algorithm Dv2 in terms\nof sample efﬁciency while using a similar computational\nbudget. Additionally, PSDRL substantially outperforms the\nstate-of-the-art RVF baselines in almost all of the games.\nAppendix F shows that PSDRL is able to decode latent state\npredictions obtained from sampled forward models close\nto perfectly for many games at the end of training, even\nwhen the corresponding state images have many details.\nExceptionally, the fact that PSDRL and Dv2 receive down-\nscaled 64 × 64 images (compared to 84 × 84 for B+P) as\nstate representations makes it difﬁcult for their agents to\ndistinguish between the enemies depicted with small differ-\nences in color and shape in games like BEAMRIDER within\n1M environment steps, which explains why they are out-\nperformed by B+P in this case (see Table A). Therefore,\nthe state representation and the model capacity of an au-\ntoencoder introduce an additional trade-off between sample\nefﬁciency and computational efﬁciency.\n4.3. Ablation studies\nThis section details two ablation studies that we conducted\nto better understand the importance of posterior sampling\nand our continual value function approximation approach.\nPosterior sampling. In order to investigate the signiﬁcance\nof posterior sampling in PSDRL, we make a comparison\nwith two ϵ-greedy approaches that select exploratory ac-\ntions at random with probability ϵ. Concretely, these ap-\n0\n200\n400\nEnduro\n0\n20\nFreeway\n0\n5000\n10000\nHero\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n20\n0\n20\nPong\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0\n2500\n5000\nQbert\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0\n1000\nSeaquest\nExploitative\nExplorative\nPosterior Sampling\nFigure 2. Average evaluation episode returns comparing posterior\nsampling to exploitative and explorative ϵ-greedy approaches.\nproaches employ the same architecture and hyperparameters\nas PSDRL but do not sample forward models from the corre-\nsponding posterior. Instead, these approaches always use the\ncurrent forward model parameters θ to make predictions. In\norder to provide a fair comparison, we linearly anneal ϵ from\n1 to 0.01 in two different schemes: an exploitative annealing\nscheme across 50k environment steps, and an explorative an-\nnealing scheme across 1M environment steps. The results in\nFigure 2 clearly show that the success of PSDRL is heavily\ndependent on the natural balancing between exploration and\nexploitation provided by posterior sampling. For instance,\nin the sparse-reward environment FREEWAY, posterior sam-\npling allows the agent to learn the optimal policy rapidly,\nwhereas the explorative agent is slowed down signiﬁcantly\nand the exploitative agent settles on a sub-optimal policy.\nContinual value network. Section 3.5 provides the ra-\ntionale behind our continual approach that trains a value\nfunction approximator across sampled models instead of\nusing a newly initialized value network for each sampled\nmodel. In order to provide a fair comparison between these\ntwo approaches, we quadruple the number of iterations used\nto train newly initialized value functions at every planning\nstep (Alg 1, line 12). The results in Figure 3 show that,\n0\n200\n400\nEnduro\n0\n20\nFreeway\n0\n5000\n10000\nHero\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n20\n0\n20\nPong\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0\n2500\n5000\nQbert\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0\n1000\nSeaquest\nContinual\nReset\nFigure 3. Average evaluation episode returns comparing planning\ncontinually with planning with newly initialized value functions.\n7\nPosterior Sampling for Deep Reinforcement Learning\n               B+P\n          SU\nKrull\nRoad Runner\nBoxing\nBreakout\nTime Pilot\nPong\nSkiing\nKangaroo\nFishing Derby\nFreeway\nBank Heist\nEnduro\nPhoenix\nGopher\nKung Fu Master\nZaxxon\nAtlantis\nTutankham\nQbert\nYars Revenge\nHero\nFrostbite\nStar Gunner\nMs Pacman\nCrazy Climber\nBattle Zone\nJames bond\nSpace Invaders\nGravitar\nAlien\nRiverraid\nAsterix\nSolaris\nAmidar\nPitfall\nWizard Of Wor\nChopper Command\nSeaquest\nAsteroids\nPrivate Eye\nBowling\nMontezuma Revenge\nVenture\nBeam Rider\nName This Game\nCentipede\nVideo Pinball\nIce Hockey\nAssault\nRobotank\nDemon Attack\nUp N Down\nBerzerk\nTennis\nDouble Dunk\nDreamerV2\nDifference in human\nnormalized score versus PSDRL\nFigure 4. A comparison between PSDRL and B+P (top), SU (middle), and Dv2 (bottom) for all 55 Atari games. Blue indicates that\nPSDRL scores higher than the baseline, red indicates the opposite. Y-axis has been clipped to [−2.5, 2.5] to facilitate visualization.\neven when the total computational cost is increased by ap-\nproximately 60%, using a newly initialized value network\nat every planning step is detrimental to sample efﬁciency.\nThis clearly justiﬁes biasing planning towards the optimal\npolicy for previously sampled models.\n5. Conclusion\nWe introduced Posterior Sampling for Deep Reinforcement\nLearning (PSDRL), the ﬁrst truly scalable approximation\nof Posterior Sampling for Reinforcement Learning that re-\ntains its model-based essence. Although there are many\nother promising approaches towards efﬁcient exploration,\nPosterior Sampling for Reinforcement Learning is the sim-\nplest among the potentially scalable and principled methods\nthat is capable of leveraging the strengths of both Bayesian\nmethods and model-based reinforcement learning.\nIn addition to an efﬁcient architecture for representing uncer-\ntainty over latent state space transition models inspired by\nrecent successes in model-based reinforcement learning and\ncontextual multi-armed bandits, our technical contributions\ninclude a specially tailored continual planning algorithm\nbased on value-function approximation that addresses two\nnewly identiﬁed potential pitfalls in scaling up Posterior\nSampling for Reinforcement Learning (recency bias and\nstatus quo bias). Our extensive experiments on the Atari\nbenchmark show that PSDRL signiﬁcantly outperforms pre-\nvious state-of-the-art randomized value function approaches,\nits natural model-free counterparts, while being competitive\nwith a state-of-the-art (model-based) reinforcement learning\nmethod in both sample efﬁciency and computational efﬁ-\nciency. Our approach is further validated by ablation studies\nthat show the importance of retaining planning informa-\ntion across sampled models and the superiority of posterior\nsampling over a naive exploration method.\nSimilarly to most deep reinforcement learning algorithms,\nthe main weakness of PSDRL is the cost of successfully\ndealing with traditionally hard environments (such as those\nwith very sparse rewards), which are deﬁned by having high\nvisitation complexity or estimation complexity (Conserva &\nRauber, 2022). Such environments naturally require large re-\nplay buffers (to counteract the recency bias), lower forward\nmodel sampling frequency (to encourage deep exploration\n(Osband et al., 2019)), and longer planning times (to obtain\nbetter policies). Some of these issues may be mitigated by\nemploying smart replay buffers and strategies such as prior-\nitized experience replay (Schaul et al., 2016). Employing a\nlatent state space model architecture that allows incremental\nposterior updates is another promising approach, although\nit requires ﬁnding a careful balance between reliable uncer-\ntainty quantiﬁcation and computational efﬁciency.\nThere are numerous possibilities for future work. The most\nimportant and demanding is to investigate the many feasible\ncombinations of latent state space model architectures with\nBayesian neural networks. Our experience indicates that it is\ncrucial to tailor the corresponding planning algorithm while\nconsidering the strengths and weaknesses of the remaining\ncomponents. Extending our approach to deal with more\ngeneral non-deterministic environments is also especially\nimportant. Finally, signiﬁcant insight into our approach may\nbe gained from studying it from a theoretical perspective.\nAcknowledgements\nThis research was ﬁnancially supported by the Intelligent\nGames and Games Intelligence CDT (IGGI;EP/S022325/1)\nand utilized Queen Mary’s Apocrita HPC facility (http:\n//doi.org/10.5281/zenodo.438045).\n8\nPosterior Sampling for Deep Reinforcement Learning\nReferences\nAgrawal, S. and Jia, R. Optimistic posterior sampling for\nreinforcement learning: worst-case regret bounds. Ad-\nvances in Neural Information Processing Systems, 30,\n2017.\nAytar, Y., Pfaff, T., Budden, D., Paine, T., Wang, Z., and\nDe Freitas, N. Playing hard exploration games by watch-\ning youtube. Advances in Neural Information Processing\nSystems, 31, 2018.\nAzar, M. G., Osband, I., and Munos, R. Minimax regret\nbounds for reinforcement learning. In International Con-\nference on Machine Learning, volume 70, pp. 263–272.\nPMLR, 2017.\nAzizzadenesheli, K., Brunskill, E., and Anandkumar, A. Ef-\nﬁcient Exploration Through Bayesian Deep Q-Networks.\nIn 2018 Information Theory and Applications Workshop\n(ITA), pp. 1–9, 2018.\nBadia, A. P., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot,\nB., Kapturowski, S., Tieleman, O., Arjovsky, M., Pritzel,\nA., Bolt, A., et al. Never Give Up: Learning Directed\nExploration Strategies. In International Conference on\nLearning Representations, 2019.\nBadia, A. P., Piot, B., Kapturowski, S., Sprechmann, P.,\nVitvitskyi, A., Guo, Z. D., and Blundell, C. Agent57:\nOutperforming the atari human benchmark. In Interna-\ntional Conference on Machine Learning, pp. 507–517.\nPMLR, 2020.\nBahdanau, D., Cho, K. H., and Bengio, Y. Neural machine\ntranslation by jointly learning to align and translate. In\nInternational Conference on Learning Representations,\n2015.\nBai, C., Wang, L., Han, L., Hao, J., Garg, A., Liu, P., and\nWang, Z. Principled exploration via optimistic bootstrap-\nping and backward induction. In International Confer-\nence on Machine Learning, pp. 577–587. PMLR, 2021.\nBellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T.,\nSaxton, D., and Munos, R. Unifying count-based ex-\nploration and intrinsic motivation. Advances in Neural\nInformation Processing Systems, 29, 2016.\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.\nThe arcade learning environment: An evaluation plat-\nform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, 2013.\nBerner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P.,\nDennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,\nC., et al. Dota 2 with large scale deep reinforcement\nlearning. arXiv preprint, 2019.\nBotev, Z. I., Kroese, D. P., Rubinstein, R. Y., and L’Ecuyer, P.\nChapter 3 - The Cross-Entropy Method for Optimization.\nIn Handbook of Statistics, volume 31, pp. 35–59. Elsevier,\n2013.\nCamacho, E. F. and Bordons, C. Model Predictive control.\nAdvanced Textbooks in Control and Signal Processing.\nSpringer London, 2007.\nChen, R. Y., Sidor, S., Abbeel, P., and Schulman, J.\nUcb exploration via q-ensembles.\narXiv preprint\narXiv:1706.01502, 2017.\nConserva, M. and Rauber, P. Hardness in markov decision\nprocesses: Theory and practice. In Advances in Neural\nInformation Processing Systems, volume 35, 2022.\nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., and\nClune, J. First return, then explore. Nature, 590(7847):\n580–586, 2021.\nEngel, Y., Mannor, S., and Meir, R. Bayes meets Bellman:\nThe Gaussian process approach to temporal difference\nlearning. In International Conference on Machine Learn-\ning, pp. 154–161. PMLR, 2003.\nEngel, Y., Mannor, S., and Meir, R. Reinforcement learning\nwith Gaussian processes. In International Conference on\nMachine Learning, pp. 201–208. PMLR, 2005.\nEysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diver-\nsity is All You Need: Learning Skills without a Reward\nFunction. In International Conference on Learning Rep-\nresentations, 2018.\nFan, Y. and Ming, Y. Model-based Reinforcement Learn-\ning for Continuous Control with Posterior Sampling. In\nInternational Conference on Machine Learning, pp. 3078–\n3087. PMLR, 2021.\nFlennerhag, S., Wang, J. X., Sprechmann, P., Visin, F.,\nGalashov, A., Kapturowski, S., Borsa, D. L., Heess, N.,\nBarreto, A., and Pascanu, R. Temporal difference un-\ncertainties as a signal for exploration. arXiv preprint\narXiv:2010.02255, 2020.\nHa, D. and Schmidhuber, J. Recurrent world models facil-\nitate policy evolution. Advances in Neural Information\nProcessing Systems, 31, 2018.\nHafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to\nControl: Learning Behaviors by Latent Imagination. In\nInternational Conference on Learning Representations,\n2019a.\nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D.,\nLee, H., and Davidson, J. Learning latent dynamics for\nplanning from pixels. In International Conference on\nMachine Learning, pp. 2555–2565. PMLR, 2019b.\n9\nPosterior Sampling for Deep Reinforcement Learning\nHafner, D., Lillicrap, T. P., Norouzi, M., and Ba, J. Master-\ning Atari with Discrete World Models. In International\nConference on Learning Representations, 2020.\nJanner, M., Fu, J., Zhang, M., and Levine, S. When to trust\nyour model: Model-based policy optimization. Advances\nin Neural Information Processing Systems, 32, 2019.\nJanz, D., Hron, J., Mazur, P., Hofmann, K., Hern´andez-\nLobato, J. M., and Tschiatschek, S. Successor uncertain-\nties: exploration and uncertainty in temporal difference\nlearning. Advances in Neural Information Processing\nSystems, 32, 2019.\nKaiser, Ł., Babaeizadeh, M., Miłos, P., Osi´nski, B., Camp-\nbell, R. H., Czechowski, K., Erhan, D., Finn, C., Koza-\nkowski, P., Levine, S., et al. Model Based Reinforcement\nLearning for Atari. In International Conference on Learn-\ning Representations, 2019.\nKingma, D. P. and Ba, J. Adam: A Method for Stochastic\nOptimization. In International Conference on Learning\nRepresentations, 2015.\nKumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. Sta-\nbilizing Off-Policy Q-Learning via Bootstrapping Error\nReduction. In Advances in Neural Information Process-\ning Systems, volume 32, 2019.\nKumar, A., Gupta, A., and Levine, S. Discor: Correc-\ntive feedback in reinforcement learning via distribution\ncorrection. Advances in Neural Information Processing\nSystems, 33, 2020.\nMasci, J., Meier, U., Cires¸an, D., and Schmidhuber, J.\nStacked convolutional auto-encoders for hierarchical fea-\nture extraction. In International conference on artiﬁcial\nneural networks, pp. 52–59. Springer, 2011.\nM´enard, P., Domingues, O. D., Shang, X., and Valko, M.\nUCB Momentum Q-learning: Correcting the bias with-\nout forgetting. In International Conference on Machine\nLearning, pp. 7609–7618. PMLR, 2021.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529–533, 2015.\nMunos, R. and Szepesv´ari, C. Finite-Time Bounds for Fitted\nValue Iteration. Journal of Machine Learning Research,\n9(5):815–857, 2008.\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare,\nM. Safe and efﬁcient off-policy reinforcement learning.\nAdvances in Neural Information Processing Systems, 29,\n2016.\nOsband, I., Russo, D., and Van Roy, B. (More) efﬁcient\nreinforcement learning via posterior sampling. Advances\nin Neural Information Processing Systems, 26, 2013.\nOsband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep\nexploration via bootstrapped DQN. Advances in Neural\nInformation Processing Systems, 29, 2016a.\nOsband, I., Van Roy, B., and Wen, Z. Generalization and\nexploration via randomized value functions. In Interna-\ntional Conference on Machine Learning, pp. 2377–2386.\nPMLR, 2016b.\nOsband, I., Aslanides, J., and Cassirer, A. Randomized prior\nfunctions for deep reinforcement learning. Advances in\nNeural Information Processing Systems, 31, 2018.\nOsband, I., Van Roy, B., Russo, D. J., and Wen, Z. Deep\nExploration via Randomized Value Functions. Journal of\nMachine Learning Research, 20(124):1–62, 2019.\nO’Donoghue, B., Osband, I., Munos, R., and Mnih, V. The\nuncertainty bellman equation and exploration. In Interna-\ntional Conference on Machine Learning, pp. 3839–3848.\nPMLR, 2018.\nRashid, T., Peng, B., B¨ohmer, W., and Whiteson, S. Opti-\nmistic Exploration even with a Pessimistic Initialisation.\nIn International Conference on Learning Representations,\n2020.\nRasmussen, C. E. Gaussian processes for machine learning.\nMIT press, 2005.\nRiquelme, C., Tucker, G., and Snoek, J. Deep Bayesian Ban-\ndits Showdown: An Empirical Comparison of Bayesian\nDeep Networks for Thompson Sampling. In International\nConference on Learning Representations, 2018.\nRusso, D. Worst-case regret bounds for exploration via\nrandomized value functions. Advances in Neural Infor-\nmation Processing Systems, 32, 2019.\nSavinov, N., Raichuk, A., Vincent, D., Marinier, R., Polle-\nfeys, M., Lillicrap, T., and Gelly, S. Episodic Curiosity\nthrough Reachability. In International Conference on\nLearning Representations, 2018.\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-\ntized Experience Replay. In International Conference on\nLearning Representations, 2016.\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K.,\nSifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis,\nD., Graepel, T., et al. Mastering atari, go, chess and shogi\nby planning with a learned model. Nature, 588(7839):\n604–609, 2020.\n10\nPosterior Sampling for Deep Reinforcement Learning\nSnoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N.,\nSundaram, N., Patwary, M., Prabhat, M., and Adams,\nR. Scalable bayesian optimization using deep neural net-\nworks. In International Conference on Machine Learning,\npp. 2171–2180. PMLR, 2015.\nTiapkin, D., Belomestny, D., Moulines, E., Naumov, A.,\nSamsonov, S., Tang, Y., Valko, M., and Menard, P. From\ndirichlet to rubin: Optimistic exploration in rl without\nbonuses. In International Conference on Machine Learn-\ning. PMLR, 2022.\nToromanoff, M., Wirbel, E., and Moutarde, F. Is Deep Re-\ninforcement Learning Really Superhuman on Atari? Lev-\neling the playing ﬁeld. arXiv preprint arXiv:1908.04683,\n2019.\nTziortziotis, N., Dimitrakakis, C., and Blekas, K. Linear\nBayesian reinforcement learning. In International joint\nconference on artiﬁcial intelligence, 2013.\nVinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,\nDudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,\nT., Georgiev, P., et al. Grandmaster level in StarCraft II\nusing multi-agent reinforcement learning. Nature, 575\n(7782):350–354, 2019.\nZanette, A. and Brunskill, E. Tighter problem-dependent\nregret bounds in reinforcement learning without domain\nknowledge using value function bounds.\nIn Interna-\ntional Conference on Machine Learning, pp. 7304–7312.\nPMLR, 2019.\n11\nPosterior Sampling for Deep Reinforcement Learning\nA. Atari benchmark: average returns\nTable 2. Average return for evaluation episodes in each individual game (1M environment steps, averaged across three seeds).\nGame\nPSDRL\nB+P\nSU\nDv2\nAlien\n1199\n505\n386\n844\nAmidar\n156\n36\n19\n117\nAssault\n662\n461\n147\n1589\nAsterix\n1235\n514\n431\n1090\nAsteroids\n1141\n932\n760\n549\nAtlantis\n35273\n20578\n11174\n163070\nBank Heist\n596\n125\n18\n673\nBattle Zone\n10612\n8127\n6007\n3275\nBeam Rider\n779\n2091\n370\n587\nBerzerk\n386\n226\n33644\n339\nBowling\n15\n5\n4\n36\nBoxing\n79\n6\n-46\n73\nBreakout\n46\n8\n0\n2\nCentipede\n2594\n2446\n2888\n4067\nChopper Command\n899\n502\n589\n1289\nCrazy Climber\n30392\n17942\n2086\n65140\nDemon Attack\n410\n1225\n776\n1233\nDouble Dunk\n-16\n-6\n-1\n-4\nEnduro\n363\n0\n0\n178\nFishing Derby\n-60\n-86\n-96\n-85\nFreeway\n28\n1\n7\n28\nFrostbite\n929\n204\n6\n354\nGopher\n2683\n865\n316\n5194\nGravitar\n494\n92\n208\n222\nHero\n7965\n2202\n768\n3096\nIce Hockey\n-12\n-13\n-13\n-6\nJames bond\n231\n69\n41\n470\nKangaroo\n3180\n384\n190\n3063\nKrull\n10802\n2235\n903\n7301\nKung Fu Master\n17528\n6647\n412\n22249\nMontezuma Revenge\n0\n0\n7\n0\nMs Pacman\n1824\n812\n429\n1502\nName This Game\n4037\n3767\n2473\n6534\nPhoenix\n3876\n1756\n323\n1981\nPitfall\n-44\n-63\n-469\n-39\nPong\n11\n-20\n-21\n0\nPrivate Eye\n68\n-53\n-793\n79\nQbert\n4245\n560\n337\n2188\nRiverraid\n3858\n2488\n1235\n4174\nRoad Runner\n22272\n528\n984\n11954\nRobotank\n3\n5\n2\n10\nSeaquest\n1070\n307\n96\n1282\nSkiing\n-14980\n-28748\n-22105\n-19777\nSolaris\n1794\n396\n2083\n866\nSpace Invaders\n511\n240\n335\n403\nStar Gunner\n2542\n1349\n531\n1188\nTennis\n-22\n-17\n-13\n-10\nTime Pilot\n4156\n2996\n2390\n2767\nTutankham\n94\n60\n19\n89\nUp N Down\n8596\n1587\n794\n90684\nVenture\n0\n3\n0\n8\nVideo Pinball\n8857\n1145\n8641\n14973\nWizard Of Wor\n1459\n848\n1181\n2124\nYars Revenge\n17038\n3657\n1804\n12144\nZaxxon\n4413\n1158\n206\n2671\n12\nPosterior Sampling for Deep Reinforcement Learning\nB. Atari benchmark: learning curves\n500\n1000\n1500\nAlien\n0\n200\nAmidar\n0\n2000\nAssault\n1000\n2000\nAsterix\n500\n1000\nAsteroids\n0\n500000\nAtlantis\n0\n1000\nBank Heist\n5000\n10000\n15000\nBattle Zone\n0\n5000\nBeam Rider\n0\n500000\nBerzerk\n0\n50\nBowling\n0\n100\nBoxing\n0\n100\nBreakout\n2500\n5000\nCentipede\n1000\n2000\nChop. Comm.\n0\n100000\nCrazy Climber\n0\n2500\nDemon Attack\n20\n0 Double Dunk\n0\n500\nEnduro\n100\n50\nFishing Derby\n0\n20\nFreeway\n0\n2000\nFrostbite\n0\n10000\nGopher\n0\n500\nGravitar\n0\n10000\nHero\n15\n10\n5\nIce Hockey\n0\n500\nJames bond\n0\n10000\nKangaroo\n0\n10000\nKrull\n0\n25000\nKung Fu Mast.\n0\n20\nMontez. Rev.\n1000\n2000\nMs Pacman\n2500\n5000\n7500\nName This Game\n0\n5000\nPhoenix\n500\n0\nPitfall\n20\n0\n20\nPong\n1000\n0\n1000\nPrivate Eye\n0\n5000\nQbert\n2500\n5000\nRiverraid\n0\n20000\nRoad Runner\n0\n10\nRobotank\n0\n1000\nSeaquest\n30000\n20000\n10000\nSkiing\n0\n2500\nSolaris\n250\n500\nSpace Inv.\n1000\n2000\n3000\nStar Gunner\n20\n0\nTennis\n2000\n4000\nTime Pilot\n0\n100\nTutankham\n0\n200000\nUp N Down\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0\n50\nVenture\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0\n25000\nVideo Pinball\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0\n2500\nWizard Of Wor\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n10000\n20000\nYars Revenge\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0\n5000\nZaxxon\nSU\nPSDRL\nB+P\nDreamerV2\nFigure 5. Returns for evaluation episodes in each individual game (1M environment steps, averaged across three seeds).\n13\nPosterior Sampling for Deep Reinforcement Learning\nC. Normalized score visualizations\nFigures 6 and 7 compare PSDRL with the baselines on 55 Atari games in terms of scores normalized with respect to a\nrepresentative human performance or the human record performance, respectively. Concretely, the height of each bar\ncorresponds to the difference between the normalized score of PSDRL and the normalized score of a given baseline. The\nvertical black lines represent 95% bootstrapped conﬁdence intervals. Darker shades of blue correspond to a wider gap in\nfavor of PSDRL, darker shades of red correspond to a wider gap in favor of a given baseline. In Figure 6, the height of each\nbar has been clipped to the interval [−2.5, 2.5], since otherwise most bars would be difﬁcult to see.\n               B+P\n          SU\nKrull\nRoad Runner\nBoxing\nBreakout\nTime Pilot\nPong\nSkiing\nKangaroo\nFishing Derby\nFreeway\nBank Heist\nEnduro\nPhoenix\nGopher\nKung Fu Master\nZaxxon\nAtlantis\nTutankham\nQbert\nYars Revenge\nHero\nFrostbite\nStar Gunner\nMs Pacman\nCrazy Climber\nBattle Zone\nJames bond\nSpace Invaders\nGravitar\nAlien\nRiverraid\nAsterix\nSolaris\nAmidar\nPitfall\nWizard Of Wor\nChopper Command\nSeaquest\nAsteroids\nPrivate Eye\nBowling\nMontezuma Revenge\nVenture\nBeam Rider\nName This Game\nCentipede\nVideo Pinball\nIce Hockey\nAssault\nRobotank\nDemon Attack\nUp N Down\nBerzerk\nTennis\nDouble Dunk\nDreamerV2\nDifference in human\nnormalized score versus PSDRL\nFigure 6. A comparison between PSDRL and B+P (top), SU (middle), and Dv2 (bottom) in terms of human-normalized score. Blue\nindicates that PSDRL scores higher than the baseline, red the opposite. Y-axis has been clipped to [−2.5, 2.5] to facilitate visualization.\nThis ﬁgure is identical to Figure 4 and is presented here to facilitate comparison with Figure 7.\n        B+P\n     SU\nBoxing\nSkiing\nPong\nFreeway\nFishing Derby\nKrull\nBreakout\nZaxxon\nEnduro\nTime Pilot\nStar Gunner\nCrazy Climber\nRoad Runner\nTutankham\nKung Fu Master\nSolaris\nHero\nBattle Zone\nBank Heist\nPrivate Eye\nMs Pacman\nAlien\nGravitar\nFrostbite\nPitfall\nGopher\nKangaroo\nQbert\nBowling\nRiverraid\nAmidar\nJames bond\nYars Revenge\nPhoenix\nAsterix\nSeaquest\nSpace Invaders\nWizard Of Wor\nChopper Command\nAsteroids\nVideo Pinball\nMontezuma Revenge\nVenture\nBeam Rider\nCentipede\nDemon Attack\nAtlantis\nAssault\nName This Game\nBerzerk\nRobotank\nIce Hockey\nTennis\nUp N Down\nDouble Dunk\nDreamerV2\nDifference in human record\nnormalized score versus PSDRL\nFigure 7. A comparison between PSDRL and B+P (top), SU (middle), and Dv2 (bottom) in terms of record-normalized score. Blue\nindicates that PSDRL scores higher than the baseline, red the opposite.\n14\nPosterior Sampling for Deep Reinforcement Learning\nD. Hyperparameters and implementation details\nHyperparameters search was conducted through a grid search over a carefully selected subset of hyperparameters for each\nalgorithm. The evaluation metric employed for this search (average return during evaluation episodes) is further averaged\nover results on six Atari games that are commonly used for this purpose (Munos et al., 2016; Janz et al., 2019): ASTERIX,\nENDURO, FREEWAY, HERO, QBERT, and SEAQUEST. In order to guarantee a fair comparison, we chose hyperparameters\nthat allow each algorithm to ﬁnish training for each game in under 10 hours.\nD.1. Posterior sampling for deep reinforcement learning\nThe implementation for PSDRL can be found at https://github.com/remosasso/PSDRL. Table 3 presents\nthe hyperparameters for PSDRL, the search sets used for grid search, and the resulting values used for the experiments.\nAdditionally, note that we update the components more frequently (m = 250) during the ﬁrst 100k environment steps in\ncomparison with the remaining steps (m = 1000). See Figure 8 for a diagram that demonstrates how the components of\nPSDRL interact when interacting with the environment.\nPSDRL receives states represented as 64 × 64 grayscale images and does not require frame stacking due to employing a\nrecurrent forward model. The autoencoder model, forward model, termination model, and value network are trained with\nthe Adam optimizer (Kingma & Ba, 2015) using a learning rate of 1e-4.\nIn our implementation, a batch B containing B × L transitions is sampled and used in slightly different ways by each\ncomponent in a given training iteration. The autoencoder model performs B gradient updates with an inner batch of size L.\nThe forward model, termination model, and value network perform L/l gradient updates with a batch of size B, where l\ndenotes the horizon for truncated backpropagation through time. For the forward and termination models, l = 4. For the\nvalue network, l = 1. Therefore, according to the parameters in Table 3, the parameters of the value network are updated\nκL/l = 750 times with batches of size B = 125 during each planning iteration (Alg. 1, line 12).\nInstead of choosing actions strictly according to the greedy policy (Eq. 8), our implementation of PSDRL chooses actions\naccording to the corresponding ˆϵ-greedy policy, where ˆϵ is a very small value. This ensures that the agent does not waste all\nof its time caught in a loop between two states early in training even if the sampled model would justify this behavior.\nWe make use of an NVIDIA A100 GPU for training. PSDRL takes about 9 hours per 1 million environment steps per game.\nFigure 8. Diagram of the interactions between components in PSDRL when interacting with the environment. At timestep t, the agent\nencodes an observation st to latent state zt. Given zt and the previous hidden state ht, the forward model f˜θ with sampled parameters ˜\nW\npredicts the next latent state ˆza\nt+1 and reward ˆra\nt for every action a ∈A. For each of the predicted next latent states, the value function Vψ\npredicts a valuation ˆva\nt+1. Finally, an action is yielded by computing: at+1 = argmaxa\nh\nˆr(a)\nt\n+ γˆv(a)\nt+1\ni\n, and the corresponding hidden\nstate ha\nt+1 is carried over to the next timestep.\n15\nPosterior Sampling for Deep Reinforcement Learning\nTable 3. Hyperparameters for PSDRL.\nName\nSymbol\nSearch set\nValue used\nBayesian linear regression\nPrior variance for latent state parameters\nσ2\nS\n{1e1, 1e3, 1e5}\n1e3\nPrior variance for reward parameters\nσ2\nR\n{1e1, 1e3, 1e5}\n1e3\nNoise variance\nσ2\n—\n1\nForward model\nNumber of layers\n—\n5\nActivation function\n—\nTanh\nHidden units\n—\n2292\nLearning rate\n—\n1e-4\nTraining iterations\n—\n3\nRecurrent hidden units\n—\n756\nWindow update length\nl\n4\nTerminal model\nNumber of layers\n—\n4\nActivation function\n—\nTanh\nHidden units\n—\n1536\nLearning rate\n—\n1e-4\nTraining iterations\n—\n3\nValue network\nNumber of layers\n—\n5\nActivation function\n—\nTanh\nHidden units\n—\n2292\nLearning rate\n—\n1e-4\nTraining iterations\nκ\n—\n3\nTarget update frequency\n4\nDiscount factor\nγ\n{0.99, 0.999}\n0.99\nAutoencoder model\nNumber of encoder layers\n—\n4\nNumber of decoder layers\n—\n4\nActivation function\n—\nReLu\nEncoded dimensions\n—\n1536\nLearning rate\n—\n1e-4\nTraining iterations\n—\n3\nReplay buffer\nBatch size\nB\n{125, 250}\n125\nSequence length\nL\n—\n250\nCapacity\nC\n—\n1e5\nEnvironment interaction\nUpdate frequency\nm\n—\n1000\nPolicy noise\nˆϵ\n0.001\n16\nPosterior Sampling for Deep Reinforcement Learning\nD.2. Successor uncertainties\nVia private communication, the authors of SU suggested that three hyperparameters should be tuned to improve the sample\nefﬁciency of their algorithm. These hyperparameters are reported in Table 4.\nWe make use of the implementation published by the authors at https://github.com/DavidJanz/successor_\nuncertainties_atari.\nTable 4. Hyperparameters for SU.\nName\nSearch set\nValue used\nLikelihood prior\n{1e-1, 1e-3, 1e-5}\n1e-3\nTraining interval\n{2, 4}\n2\nBatch size\n{32, 64}\n64\nD.3. Bootstrapped DQN with randomized priors\nBecause B+P shares several components with SU, we selected a similar set of hyperparameters to tune for sample efﬁciency.\nThese hyperparameters are reported in Table 5.\nWe make use of an implementation for Atari available at https://github.com/johannah/bootstrap_dqn.\nTable 5. Hyperparameters for B+Q.\nName\nSearch set\nValue used\nLikelihood prior\n{1,3,10}\n1\nTraining interval\n{2, 4}\n2\nBatch size\n{32, 64}\n64\nD.4. DreamerV2\nThe authors of Dv2 mention that the training interval (originally 16) should be decreased (Hafner et al., 2020) to increase\nsample efﬁciency. Table 6 reports the corresponding values employed for grid search.\nWe make use of the implementation published by the authors at https://github.com/danijar/dreamerv2.\nTable 6. Hyperparameters for Dv2.\nName\nSearch set\nValue used\nTraining interval\n{4,8,12}\n8\n17\nPosterior Sampling for Deep Reinforcement Learning\nE. Runtime comparison\nTable 7 reports the amount of wall clock time required for 1M environment steps in 7 games for each of the algorithm\nimplementations used in this paper. Note that this is the computational efﬁciency on an NVIDIA A100 GPU after tuning the\nbaselines for data efﬁciency.\nTable 7. Wall clock time for 1M environment steps for each of the algorithms after tuning.\nGame\nSU\nPSDRL\nB+P\nDv2\nFreeway\n5h3m ± 20m\n8h53m ± 0m\n8hr11m ± 24m\n11h20m ± 31m\nQbert\n6h28m ± 23m\n7h39m ± 43m\n9h43m ± 9m\n10h52m ± 20m\nEnduro\n5h13m ± 25m\n9h35m ± 15m\n8h12m ± 32m\n10h58m ± 3m\nAsterix\n4h54m ± 4m\n7h44m ± 24m\n9h7m ± 14m\n10h18m ± 18m\nSeaquest\n5h30m ± 27 m\n8h31m ± 2m\n8h56m ± 15m\n10h33m ± 7m\nPong\n7h0m ± 34m\n7h58m ± 42m\n8h58m ± 6m\n11h7m ± 35m\nHero\n5h46m ± 19m\n9h26m ± 2m\n9h32m ± 28m\n12h24m ± 26m\nAverage\n5h41m ± 64m\n8h31m ± 44m\n8h56m ± 32m\n11h3m ± 37m\n18\nPosterior Sampling for Deep Reinforcement Learning\nF. Decoded sampled forward model predictions\nFigures 9 and 10 provide a comparison between a true environment state image st+1 and a decoded state image ˆst+1 =\nDχ(ˆzt+1) after training 1M environment steps, where ˆzt+1 is the latent state predicted by a sampled forward model f˜θ given\na state image st encoded as zt = Eχ(st), action at, and hidden state ht. Figures 9 and 10 also show the corresponding error\nimage ˆst+1 −st+1. As discussed in Section 4.2, BEAM RIDER state images contain important details that are missed (see\nsecond and third error images in the last row of Fig. 10).\nAdditionally, using decoded predictions of sampled forward models we can illustrate the uncertainty quantiﬁcation over\nenvironment models. The following video showcases decoded latent rollouts generated by different sampled forward\nmodels, where each rollout was initiated from the same starting state after 75k environment steps of training: https:\n//gifyu.com/image/SIMGF. The video clearly demonstrates signiﬁcant diversity in predictions made by distinct\nsampled models, resulting in disparate trajectories over time, consequently driving the exploration of the agent.\nTrue\nModel\nError\nTrue\nModel\nError\nTrue\nModel\nError\nFigure 9. Decoded sampled forward model predictions compared to the true environment state for PONG, ENDURO, and FREEWAY.\n19\nPosterior Sampling for Deep Reinforcement Learning\nTrue\nModel\nError\nTrue\nModel\nError\nTrue\nModel\nError\nTrue\nModel\nError\nFigure 10. Decoded sampled forward model predictions compared to the true environment state for HERO, QBERT, SEAQUEST and BEAM\nRIDER.\n20\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "68T07",
    "I.2.m"
  ],
  "published": "2023-04-30",
  "updated": "2023-05-17"
}