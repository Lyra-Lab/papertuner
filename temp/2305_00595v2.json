{
  "id": "http://arxiv.org/abs/2305.00595v2",
  "title": "Impact of Deep Learning Libraries on Online Adaptive Lightweight Time Series Anomaly Detection",
  "authors": [
    "Ming-Chang Lee",
    "Jia-Chun Lin"
  ],
  "abstract": "Providing online adaptive lightweight time series anomaly detection without\nhuman intervention and domain knowledge is highly valuable. Several such\nanomaly detection approaches have been introduced in the past years, but all of\nthem were only implemented in one deep learning library. With the development\nof deep learning libraries, it is unclear how different deep learning libraries\nimpact these anomaly detection approaches since there is no such evaluation\navailable. Randomly choosing a deep learning library to implement an anomaly\ndetection approach might not be able to show the true performance of the\napproach. It might also mislead users in believing one approach is better than\nanother. Therefore, in this paper, we investigate the impact of deep learning\nlibraries on online adaptive lightweight time series anomaly detection by\nimplementing two state-of-the-art anomaly detection approaches in three\nwell-known deep learning libraries and evaluating how these two approaches are\nindividually affected by the three deep learning libraries. A series of\nexperiments based on four real-world open-source time series datasets were\nconducted. The results provide a good reference to select an appropriate deep\nlearning library for online adaptive lightweight anomaly detection.",
  "text": "Impact of Deep Learning Libraries on Online \nAdaptive Lightweight Time Series Anomaly \nDetection \nMing-Chang Lee! and Jia-Chun Lin? \n1Department of Computer science, Electrical engineering and Mathematical sciences, Hogskulen pa Vestlandet (HVL), Bergen, \nNorway \n?Department of Information Security and Communication Technology, Norwegian University of Science and Technology, \nGjovik, Norway \n1 mingchang1109@gmail.com \n*jia-chun.lin@ntnu.no \n10 May 2023 \nNote: This is the preprint version of the paper accepted by the 18th International Conference on Software \nTechnologies (ICSOFT 2023).\nImpact of Deep Learning Libraries on Online Adaptive Lightweight \nTime Series Anomaly Detection \nMing-Chang Lee!®* and Jia-Chun Lin?®> \n' Department of Computer science, Electrical engineering and Mathematical sciences, Hogskulen pa Vestlandet (HVL), \nBergen, Norway \n2 Department of Information Security and Communication Technology, Norwegian University of Science and Technology \nKeywords: \nAbstract: \n(NTNU), Gjovik, Norway \nmingchang1109@ gmail.com, jia-chun.lin@ntnu.no \nTime series, univariate time series, anomaly detection, online model training, unsupervised learning, Tensor- \nFlow, Keras, PyTorch, Deeplearning4j \nProviding online adaptive lightweight time series anomaly detection without human intervention and domain \nknowledge is highly valuable. Several such anomaly detection approaches have been introduced in the past \nyears, but all of them were only implemented in one deep learning library. With the development of deep learn- \ning libraries, \nit is unclear how different deep learning libraries impact these anomaly detection approaches \nsince there is no such evaluation available. \nRandomly choosing a deep learning library to implement an \nanomaly detection approach might not be able to show the true performance of the approach. \nIt might also \nmislead users in believing one approach is better than another. Therefore, in this paper, we investigate the im- \npact of deep learning libraries on online adaptive lightweight time series anomaly detection by implementing \ntwo state-of-the-art anomaly detection approaches in three well-known deep learning libraries and evaluating \nhow these two approaches are individually affected by the three deep learning libraries. A series of experi- \nments based on four real-world open-source time series datasets were conducted. The results provide a good \nreference to select an appropriate deep learning library for online adaptive lightweight anomaly detection. \n1 \nINTRODUCTION \nA time series refers to a sequence of data points in- \ndexed in time order, and it is a collection of observa- \ntions obtained via repeated measurements over time \n(Ahmed et al., 2016). \nExamples of time series in- \nclude stock prices, retail sales, electricity consump- \ntion, temperatures, humidity, CO2, blood pressures, \nheart rates, etc. Due to the increasing prevalence of \nthe Internet of Things (IoT), more and more different \ntime series are continuously generated by diverse IoT \nsensors and devices over time. \nAnalyzing time se- \nries is valuable to businesses and organizations since \nit gives insight into what has happened and identifies \ntrends and seasonal variances to aid in the forecasting \nof future events. \nIt also enables businesses and or- \nganizations to take appropriate policies or make bet- \nter decisions (Kieu et al., 2018; Yatish and Swamy, \n2020). \nTime series anomaly detection is an analysis task \n© https://orcid.org/0000-0003-2484-4366 \n© https://orcid.org/0000-0003-3374-8536 \nfocusing on detecting anomalous or abnormal data \npoints in time series, and \nit has been widely used \nin various applications ranging from cloud systems \n(Deka et al., 2022), smart grids (Zhang et al., 2021), \nhealthcare (Pereira and Silveira, 2019) to agriculture \n(Moso et al., 2021). Many time series anomaly de- \ntection approaches have been introduced in the last \ndecade. Some were designed for univariate time se- \nries where there is only one time-dependent variable, \nand the other approaches were designed for multivari- \nate time series that consists of more than one time- \ndependent variables. \nIn this paper, we focus on the \nstudies for univariate time series. To be more specific, \nwe focus on univariate time series anomaly detection \napproaches that possess the following features: Unsu- \npervised learning, online model training, adaptability, \nand lightweight since these features decide whether \nan approach \nis practical or not. \n(Blazquez-Garcia \net al., 2021). \nUnsupervised learning refers to machine learning \nmodels that have a self-learning ability to draw in- \nference from a dataset containing a small minority\nof abnormal data without any label. \nSince most of \nreal-world time series data do not have any label, it is \ndesirable to have an unsupervised anomaly detection \napproach. Conventional machine learning models are \nusually trained with a pre-collected dataset in an of- \nfline manner. Once the models are trained, they are \nused for inference without any change. Hence, they \ncannot reflect unseen situations or adapt to changes \non time series (Eom \net \nal., 2015). \nUnlike offline \nmodel training, online model training enables a ma- \nchine learning model to be trained on the fly, imply- \ning that the model can adapt to changes in the pat- \ntern of the time series (i.e., adaptability). \nThis fea- \nture \nis getting more and more popular, and \nit has \nbeen provided by some systems or approaches such \nas (Lee et al., 2020b; Eom et al., 2015; Chi et al., \n2021). \nFinally, lightweight means that an anomaly \ndetection approach neither has \na complex network \nstructure/design nor requires excessive computation \nresources such as General-Purpose Graphics process- \ning units (GPGPUs) or high-performance computers. \nAccording to our survey, only few state-of-the-art \napproaches satisfy all the above-mentioned charac- \nteristics, such as RePAD (Lee et al., 2020b), ReRe \n(Lee et al., 2020a), SALAD (Lee et al., 2021b), and \nRePAD?2 (Lee and Lin, 2023). However, all of them \nwere only implemented in one specific deep learning \nlibrary. \nIn fact, \na number of deep learning (DL) li- \nbraries have been introduced and widely used, such \nas TensorFlow (Abadi et al., 2016), PyTorch (Paszke \net al., 2019), and Deeplearning4j (Deeplearning4j, \n2023). \nThey have a common goal to facilitate the \ncomplicated data analysis process and offer integrated \nenvironments on top of standard programming lan- \nguages (Nguyen et al., 2019). However, it is unclear \nthe impact of these DL libraries on online adaptive \nlightweight anomaly detection. \nTherefore, \nthis paper focuses \non investigating \nhow \ndifferent DL \nlibraries \naffect online adaptive \nlightweight time series anomaly detection by imple- \nmenting two state-of-the-art anomaly detection ap- \nproaches in three widely-used deep learning libraries. \nIt is worth noting that our focus is not to compare dif- \nferent time series anomaly detection approaches re- \ngarding their detection accuracy or response time. In- \nstead, we emphasize on investigating how these ap- \nproaches are individually affected by different DL li- \nbraries. \nA series of experiments based on open-source \ntime series datasets were performed. The results show \nthat DL libraries have \na great impact on not only \nanomaly detection accuracy but also response time. \nTherefore, it is important to take the selection of DL \nlibraries into consideration when one would like to \ndesign and implement an online adaptive lightweight \ntime series anomaly detection approach. \nThe rest of the paper \nis organized as follows: \nSection \n2 describes time series anomaly detection \napproaches and DL libraries. \nSection \n3 gives an \noverview of the related work. \nSection 4 introduces \nevaluation setup. \nSection 5 presents the evaluation \nresults. \nSection 6 concludes this paper and outlines \nfuture work. \n2 \nBACKGROUND \nIn this section, we introduce state-of-the-art anomaly \ndetection approaches for univariate time series and \nsome well-known DL libraries. \n2.1 \nAnomaly Detection Approaches for \nUnivariate Time Series \nExisting anomaly detection approaches for univariate \ntime series can be roughly classified into two cate- \ngories: statistical based and machine learning based. \nStatistical-based anomaly detection approaches. \nat- \ntempt to create a statistical model for normal time \nseries data and use this model to determine if a data \npoint is anomalous or not. Example approaches in- \nclude AnomalyDetectionTs and AnomalyDetection- \nVec proposed by Twitter (Twitter, 2015), and Luminol \nintroduced by LinkedIn (LinkedIn, 2018). However, \nstatistical-based approaches might not perform well if \nthe data does not follow a known distribution (Alimo- \nhammadi and Chen, 2022). \nOn the other hand, machine learning based ap- \nproaches attempt to detect anomalies without assum- \ning \na specific generative model based on the fact \nthat \nit \nis unnecessary to know the underlying pro- \ncess of the data (Braei and Wagner, 2020). \nGreen- \nhouse (Lee et al., 2018) is a time series anomaly de- \ntection algorithm based on Long Short-Term Mem- \nory (LSTM), which is a special recurrent neural net- \nwork suitable for long-term dependent tasks (Hochre- \niter and Schmidhuber, 1997). \nGreenhouse adopts a \nLook-Back and Predict-Forward strategy to learn the \ndistribution of the training data. \nFor a given time \npoint, a window of most recently observed data point \nvalues are used to predict future data point values. \nHowever, Greenhouse is not an online approach since \nits LSTM model is trained with a pre-collected train- \ning data. \nBesides, \nit requires users to determine \na \nproper detection threshold. \nRePAD (Lee et al., 2020b) is an online real-time \nlightweight unsupervised time series anomaly detec- \ntion approaches based on LSTM and the Look-Back\nand Predict-Forward strategy. RePAD utilizes a sim- \nple LSTM network (with only one hidden layer and \nten hidden units) to train a LSTM model with short- \nterm historical data points, predict each upcoming \ndata point, \nand then decide \nif each data point \nis \nanomalous based on a dynamically calculated detec- \ntion threshold. \nDifferent from Greenhouse, RePAD \ndoes not need \nto go through any offline training. \nInstead, RePAD trains \nits LSTM model on the fly. \nRePAD will keep using the same LSTM model if the \nmodel predicts well. When the prediction error of the \nmodel is higher than or equal to a dynamically calcu- \nlated detection threshold, RePAD will retrain another \nnew model with recent data points. \nReRe (Lee et al., 2020a) is an enhanced time se- \nries anomaly detection based on RePAD, and it was \ndesigned to further reduce false positive rates. ReRe \nutilizes two LSTM models to jointly detect anoma- \nlous data points. \nOne model works exactly like \nRePAD, whereas the other model works similar to \nRePAD but with a stricter detection threshold. Com- \npared with RePAD, ReRe requires more compute re- \nsources due to the use of two LSTM models. \nSALAD (Lee et al., 2021b) is another online self- \nadaptive unsupervised time series anomaly detection \napproach designed for time series with a recurrent \ndata pattern, and it is also based on RePAD. Different \nfrom RePAD, SALAD consists of two phases. \nThe \nfirst phase converts the target time series into a series \nof average absolute relative error (AARE) values on \nthe fly. The second phase predicts an AARE value for \nevery upcoming data point based on short-term his- \ntorical AARE values. If the difference between a cal- \nculated AARE value and the corresponding forecast \nAARE value is higher than a self-adaptive detection \nthreshold, the corresponding data point is considered \nanomalous. \nZiu et al. \n(Niu et al., 2020) introduced LSTM- \nbased VAE-GAN, which stands for \na Long Short- \nTerm Memory-based variational autoencoder gener- \nation adversarial networks. \nThis method consists of \none offline training stage to learn the distribution of \nnormal time series, and one anomaly detection stage \nto calculate anomaly score for each data point in the \ntarget time series. This method jointly trains the en- \ncoder, the generator, and the discriminator to take ad- \nvantage of the mapping ability of the encoder and the \ndiscriminatory ability of the discriminator. However, \nthe method requires that the training data contains no \nanomalies. Besides, the method is not an online ap- \nproach since its detection model will not be retrained \nor updated after the training stage, meaning that it is \nnot adaptive. \nIbrahim et \nal. \n(Ibrahim et al., 2022) proposed \na hybrid deep learning approach that combines one- \ndimensional convolutional neural network with bidi- \nrectional \nlong \nshort-term memory (BiLSTM) \nfor \nanomaly detection in univariate time series. However, \nthe approach requires offline training and consider- \nable training time due to parameter tuning required \nby the used hybrid approach. \n2.2 \nDeep Learning Libraries \nOver the last few years, machine learning has seen \nsignificant advances. Many different machine learn- \ning algorithms have been introduced to address dif- \nferent problems. In the meantime, many DL libraries \nhave been developed by academy, industry, and open- \nsource communities, attempting to provide a fair ab- \nstraction on the ground complex tasks with simple \nfunctions that can be used as tools for solving larger \nproblems (Ketkar and Santana, 2017). \nTensorFlow (Abadi et al., 2016) is a popular open- \nsource Python-based DL library created and main- \ntained by Google. It uses dataflow graphs to represent \nboth the computation in an algorithm and the state on \nwhich the algorithm operates. TensorFlow is designed \nfor large-scale distributed training and inference. \nIt \ncan run on a single CPU system, GPUs, mobile de- \nvices, and large-scale distributed systems. However, \nits low-level application programming interface (API) \nmakes it difficult to use (Nguyen et al., 2019). \nBe- \ncause of this, TensorFlow is usually used in combi- \nnation with Keras (Keras, 2023), which is a Python \nwrapper library providing high-level, highly modular, \nand user-friendly API. \nCNTK \n(CNTK, \n2023) \nstands \nfor \nCognitive \nToolkit, and it was introduced by Microsoft and writ- \nten in C++ programming language. \nIt supports the \nOpen Neural Network Exchange (ONNX) format, al- \nlowing easy model transformation from one DL li- \nbrary to another one. As compared with TensorFlow, \nCNTK is less popular (Nguyen et al., 2019). More- \nover, the official website of CNTK shows that CNTK \nis no longer actively developed. \nPyTorch (Paszke et al., 2019) is an open-source \nDL framework based on the Torch library. \nIt aims to \nprovide an easy to use, extend, develop, and debug \nframework. \nIt is equipped with a high-performance \nC++ runtime that developers can leverage for pro- \nduction environments while avoiding inference via \nPython (Ketkar and Santana, 2017). \nPyTorch sup- \nports tensor computation with strong GPU accelera- \ntion and allows a network to change the way it be- \nhaves with small effort using dynamic computational \ngraphs. Similar to CNTK, it also supports the ONNX \nformat.\nDeeplearning4j is an open source distributed deep \nlearning library released by a startup company called \nSkymind in 2014 (Deeplearning4j, 2023)(Wang et al., \n2019). \nDeeplearning4j \nis written for java program- \nming language and java virtual machine (JVM). It is \npowered by its own open-source numerical comput- \ning library called ND4J, and it supports both CPUs \nand GPUs. \nDeeplearning4j provides implementa- \ntions of the restricted Boltzmann machine, deep be- \nlief net, deep autoencoder, recurrent neural network, \nword2vec, doc2vec, etc. \n3 \n> RELATED WORK \nNguyen et al. \n(Nguyen et al., 2019) conducted a \nsurvey on several DL libraries. \nThey also analyzed \nstrong points and weak points for each library. How- \never, they did not conduct any experiments to com- \npare these DL libraries. \nWang et al. \n(Wang et al., \n2019) compared several DL libraries \nin terms \nof \nmodel design ability, interface property, deployment \nability, performance, framework design, and devel- \nopment prospects by using some benchmarks. \nThe \nauthors also made suggestions about how to choose \nDL frameworks in different scenarios. Nevertheless, \ntheir general evaluation and analysis are unable to \nanswer the specific question that this paper attempts \nto answer, i.e., how DL libraries affect online adap- \ntive lightweight time series anomaly detection ap- \nproaches. \nKovalev et al. (Kovalev et al., 2016) evaluated the \ntraining time, prediction time, and classification ac- \ncuracy of a fully connected neural network (FCNN) \nunder five different DL libraries: Theano with Keras, \nTorch, Caffe, Tensorflow, and Deeplearning4j. \nAp- \nparently, their results are not applicable to lightweight \nanomaly detection approaches. \nZhang et al. \n(Zhang et al., 2018) evaluated the \nperformance of several state-of-the-art DL libraries, \nincluding TensorFlow, Caffe2, MXNet, PyTorch and \nTensorFlow Lite on different kinds of hardware, in- \ncluding MacBook, FogNode, Jetson TX2, Raspberry \nPi, and Nexus 6P. The authors chose a large-scale \nconvolutional neural network (CNN) model called \nAlexNet (Krizhevsky et al., 2017) and a small-scale \nCNN model called SqueezeNet (Iandola et al., 2016), \nand evaluated how each of them performs under dif- \nferent combination of hardware and DL libraries in \nterms of latency, memory footprint, and energy con- \nsumption. According to the evaluation results, there \nis no single winner on every metric since each has \nits own metric. \nDue to the fact that two used CNN \nmodels are much complex than lightweight anomaly \ndetection approaches, their evaluation results and sug- \ngestions may not be applicable. \nZahidi et al. \n(Zahidi et al., 2021) conducted an \nanalysis to compare different Python-based and Java- \nbased DL libraries and to see how they support dif- \nferent natural language processing (NLP) tasks. Due \nto the difference between NLP tasks and time series \nanalysis, their results still cannot be applied to the \nwork of this paper. \nZhang et al. \n(Zhang et al., 2022) built a bench- \nmark that includes \nsix representative DL libraries \non mobile devices (TFLite, PyTorchMobile, \nncnn, \nMNN, Mace, and SNPE) and 15 DL models (10 of \nthem are for image classification, \n3 of them are for \nobject detection, \n1 for semantic segmentation, and \n1 \nfor text classification). The authors then performed a \nseries of experiments to evaluate the performance of \nthese DL libraries on the 15 DL models and different \nmobile devices. According to their analysis and ob- \nservation, there is no DL libraries that perform best \non all tested scenarios and that the impacts of DL \nlibraries may overwhelm DL algorithm design and \nhardware capacity. Apparently, the target of our pa- \nper is completely different from that of Zhang et al.’s \npaper. Even though their results point out some use- \nful conclusions, their results cannot help us get a clear \nanswer about how different DL libraries affect online \nadaptive lightweight anomaly detection. \n4 \nEVALUATION SETUP \nBased on the description \nin \nthe Background \nsec- \ntion, we chose RePAD and SALAD to be our target \nanomaly detection approaches because both of them \npossess all previously mentioned desirable features \n(i.e., unsupervised learning, online model training, \nadaptability, and lightweight). As for DL libraries, we \nchose TensorFlow-Keras, PyTorch, and Deeplearn- \ning4j because they are popular and widely used. Re- \ncall both TensorFlow-Keras and PyTorch are based on \nPython, it would be interesting to see how Deeplearn- \ning4j performs as compared with TensorFlow-Keras \nand PyTorch. \nHere, \nthe versions \nof TensorFlow- \nKeras, PyTorch, and Deeplearning4j are 2.9.1, 1.13.1, \nand 0.7-SNAPSHOT, respectively. \nWe implemented RePAD and SALAD in the three \nDL libraries. \nHence, there are six combinations as \nshown in Table \n1. RePAD-TFK refers to RePAD im- \nplemented in TensorFlow-Keras, SALAD-PT refers \nto SALAD implemented in PyTorch, and so on so \nforth.\nTable 1: The six combinations studied in this paper. \n  \n  \n  \n  \nRePAD \nSALAD \nTensorFlow-Keras \nRePAD-TFK \nSALAD-TFK \nPyTorch \nRePAD-PT \nSALAD-PT \nDeeplearning4j \nRePAD-DL4J \nSALAD-DL4J \n  \n4.1 \nReal-world datasets \nTo \nevaluate \nthe \nthree RePAD \ncombinations, \ntwo \nreal-world time \nseries were used. \nOne \nis called \nec2-cpu-utilization-825cc2 (CC2 for short), and the \nother is called rds-cpu-utilization-e47b3b (B3B for \nshort). \nBoth time series are provided by the Nu- \nmenta Anomaly Benchmark (NAB) (Lavin and Ah- \nmad, 2015). CC2 contains two point anomalies and \none collective anomaly, whereas B3B contains one \npoint anomaly and one collective anomaly. Note that \na point anomaly is a single data point which is identi- \nfied as anomalous with respect to the rest of the time \nseries, whereas a collective anomaly is defined as a se- \nquence of data points which together form an anoma- \nlous pattern (Schneider et al., 2021). \nSince CC2 and B3B consist of only 4032 data \npoints, they are unable to show the long-term per- \nformance of the three RePAD combinations. Hence, \nwe created two long time series called CC2-10 and \nB3B-10 by individually duplicating CC2 and B3B ten \ntimes. Table 2 lists their details. Figures 1 and 2 illus- \ntrate all data points in CC2-10 and B3B-10, respec- \ntively. Each point anomaly is marked as a red circle, \nwhereas each collective anomaly is marked as a red \ncurve line. \nTable 2: Two extended real-world time series used to eval- \nuate RePAD-TFK, RePAD-PT, and RePAD-DL4J. \n  \n  \n  \n  \nName \nNumber of data \nTime \nDuration \nNumber of anomalies \npoints \ninterval \nCC2-10 \n40,320 \n5 \n140 days \n70 Pointand 10 \ncollective anomalies \nB3B-10 \n40,320 \n5 \n140 days \n[0 pointand 10 \ncollective anomalies \n  \nCPU Utilization \n  \n762 \n  \nTimestamp \nFigure \n1: All data points on the CC2-10 time series. Each \nanomaly is marked in red. \nOn the other hand, to evaluate the three SALAD \ncombinations, we selected another two real-world re- \nCPU Utilization \n30 \n  \nTimestamp \nFigure 2: All data points on the B3B-10 time series. Each \nanomaly is marked in red. \ncurrent time series. One is Taipei Mass Rapid Transit \n(TMRT for short) (Yeh et al., 2019), and the other is \nNew York City Taxi demand (NYC for short) from \nthe Numenta Anomaly Benchmark (Lavin and Ah- \nmad, 2015). The former consists of 1260 data points, \nwhereas the latter consists of 10320 data points. Table \n3 summarizes the details of TMRT and NYC. They \ncontain only collective anomalies. \nTable \n3: \nTwo real-world time \nseries \nused \nto \nevaluate \nSALAD-TFK, SALAD-PT, and SALAD-DL4J. \n  \nNumber of \n  \n  \nName \nNumber \nof \ndata \nInterval \nDuration \n, \ni \nanomalies \npoints \nTMRT \n1,260 \nThour \n2016/02/01 00:00to —_‘| S*llective \n2016/03/31 23:00 \nanomaly \n  \n2014/07/01 00:00 to \n> “Allective \nNYC \n10,320 \n30 min \nanomalies \n2015/01/31 23:30 \n. \n  \n4.2 \nHyperparameters, parameters, and \nenvironment \nTo ensure a fair evaluation, the three RePAD combi- \nnations were configured with the same hyperparam- \neters and parameters, as listed in Table 4, following \nthe setting used by RePAD (Lee et al., 2020b). \nRe- \ncall that RePAD utilizes the Look-Back and Predict- \nForward strategy to determine data size for online \nmodel training and data size for prediction. \nIn this \npaper, we respectively set the Look-Back parameter \nand the Predict-Forward parameter to 3 and \n1 based \non the setting suggested by (Lee et al., 2021a). \nIn \nother words, the LSTM models used by RePAD-TFK, \nRePAD-PT, and RePAD-DL4J will be always trained \nwith three historical data points, and the trained mod- \nels will be used to predict the next upcoming data \npoint in the target time series. \nIn \naddition, \nRePAD-TFK, \nRePAD-PT, \nand \nRePAD-DL4J inherited the simple LSTM structure \nused by RePAD (Lee \net al., 2020b), \ni.e., only one \nhidden layer and ten hidden units. \nNote that Early \nstopping \n(EarlyStopping, \n2023) \nwas \nnot \nused \nto \nautomatically determine the number of epochs since \nthis technique is not officially supported by PyTorch.\nFor fairness, the number of epochs was set to 50 for \nthe three RePAD combinations. \nTable 4: The hyperparameter and parameter setting used by \nRePAD-TFK, RePAD-PT, and RePAD-DL4J. \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \nHyperparameters/parameters \nValue \nThe Look-Back parameter \n3 \nThe Predict-Forward parameter \n1 \nThe number of hidden layers \n1 \nThe number of hidden units \n10 \nThe number of epochs \n50 \nLearning rate \n0.005 \nActivation function \ntanh \nRandom seed \n140 \n  \nTable 5: The hyperparameter and parameter setting used by \nSALAD-TFK, SALAD-PT, and SALAD-DL4J. \n  \nHyperparameters/parameters \nThe conversion phase \nThe detection phase \n  \n  \n  \n  \n  \n  \n  \n  \n  \nThe Look-Back parameter \n288 for NYC, \n3 \n63 for TMRT \nThe Predict-Forward parameter \n1 \n1 \nThe number of hidden layers \n1 \n1 \nThe number of hidden units \n10 \n10 \nThe number of epochs \n100 \n50 \nLearning rate \n0.001 \n0.001 \nActivation function \ntanh \ntanh \nRandom seed \n140 \n140 \n  \nSimilarly, to make sure a fair evaluation, the three \nSALAD combinations were all configured with the \nsame hyperparameter and parameter setting, as listed \nin Table 5. \nHowever, the setting is slightly differ- \nent when it comes to the two used time series TMRT \nand NYC. Recall that SALAD consists of one con- \nversion phase and one detection phase. The conver- \nsion phase requires more data points for model train- \ning than the detection phase does. Hence, the Look- \nBack parameter for the conversion phase of SALAD- \nTFK, SALAD-PT, and SALAD-DL4J were all set to \n288 and 63 on NYC and TMRT, respectively. \nDue \nto the same reason, we configured 100 and 50 epochs \nfor the conversion phase and the detection phase of \nthe three SALAD combinations, respectively. On the \nother hand, the Look-Back parameter for the detec- \ntion phase of the three SALAD combinations were all \nset to 3 no matter the used time series is TMRT or \nNYC. This is because the detection phase works ex- \nactly like RePAD, and three is the recommend value \nsuggested by (Lee et al., 2021a) for the Look-Back \nparameter of RePAD. \nThe evaluations for all the six combinations were \nindividually performed on the same laptop running \nMacOS 10.15.1 with 2.6 GHz 6-Core Intel Core i7 \nand 16GB DDR4 SDRAM. Note that we did not \nchoose GPUs or high-performance computers to con- \nduct the evaluation since it is interesting to know how \nTensorFlow-Keras, PyTorch, and Deeplearning4j im- \npact RePAD and SALAD on a commodity computer. \n5 \nEVALUATION RESULTS \nIn this section, we detail the evaluation results of \nthe three RePAD combinations and the three SALAD \ncombinations. \n5.1 \nThree RePAD combinations \nTo measure the detection accuracy for each RePAD \ncombination, \nwe \nchose \nprecision, \nrecall, \nand \nF- \nscore. \nPrecision is the ratio between the true pos- \nitives \n(TP) \nand \nall \nthe \npositives, \ni.e., \nprecision= \nTP/(TP+FP) where FP represents false positive. Re- \ncall is the measure of the correctly identified anoma- \nlies \nfrom \nall \nthe \nactual \nanomalies, \ni.e., \nrecall= \nTP/(TP+FN) where FN represents false negative. \nF- \nscore \nis \na well-known composite measure to eval- \nuate \nthe accuracy \nof \na model, \nand \nit \nis \ndefined \nas 2-(precision-recall) /(precision+recall). \nA higher \nvalue of F-score indicates better detection accuracy. \nIt is worth noting that we did not utilize the tra- \nditional pointwise approach to measure precision, re- \ncall, and F-score. Instead, we refer to the evaluation \nmethod used by (Lee et al., 2020a). More specifically, \nif a point anomaly occurring at time point Z can be \ndetected within a time period ranging from time point \nZ—K to time point Z+K, this anomaly is considered \ncorrectly detected. \nOn the other hand, for any col- \nlective anomaly, if it starts at time point A and ends \nat time point B (B>A), and it can be detected within a \nperiod between A—K and B, we consider this anomaly \ncorrectly detected. In this paper, we set K to 7 follow- \ning the setting suggested by (Ren et al., 2019), i.e., K \nis 7 if the measurement interval of a time series is a \nminute, and K is 3 for a hourly time series. \nIn addition, we used three performance metrics to \nevaluate the efficiency of each RePAD combination. \nThe first one is LSTM training ratio, which is the \nratio between the number of data points that require \na new LSTM model training and the total number \nof data points in the target time series. \nA lower ra- \ntio indicates less computation resources and quicker \nresponse time because LSTM model training takes \nsome time. The second one is average detection time \nfor each data point when LSTM model training is not \nrequired (ADT-NT for short). According to the design \nof RePAD, the LSTM model will not be replaced if it \ncan accurately predict the next data point, which also \nmeans that the detection can be performed immedi- \nately without any delay. The last performance metric \nis average detection time when LSTM model training \nis required (ADT-T for short). \nWhen LSTM model \ntraining is required, the time to detect if a data point is \nanomalous consists of the time to train a new LSTM\nmodel, the time for this new model to re-predict the \nvalue of the data point, and the time to determine if the \ndata point is anomalous. Apparently, ADT-T would \nbe longer than ADT-NT due to LSTM model training. \nTables 6 to 9 show the performance of the three \nRePAD combinations on the CC2-10 time series. It is \nclear that RePAD-PT performs the best since it pro- \nvides the highest detection accuracy, the least number \nof LSTM training, and the shortest ADT-T. The re- \nsult shows that PyTorch seems to be a good choice \nfor RePAD. \nAlthough RePAD-TFK provides the second best \ndetection accuracy, its ADT-NT and ADT-T were ob- \nviously the longest. \nIt seems like TensorFlow-Keras \nis less efficient than PyTorch and Deeplearning4j. \nOn the other hand, we can see from Table 6 that \nRePAD-DL4J provides the lowest detection accuracy \ndue to the lowest recall. \nNevertheless, \nits ADT-NT \nis the shortest and its ADT-T is the second shortest \nwith the smallest standard deviation. \nIt seems that \nDeeplearning4j offers more stable execution perfor- \nmance than the other two libraries. \nTable 6: The detection accuracy of the three REPAD com- \nbinations on the CC2-10 time series. \n  \n  \n  \n  \n  \nCombination \nPrecision \nRecall \nF-score \nRePAD-TFK \n0.957 \n0.9 \n0.928 \nRePAD-PT \n0.954 \n0.934 \n0.944 \nRePAD-DL4J \n0.964 \n0.7 \n0.811 \n  \nTable 7: The LSTM training ratio of the three RePAD com- \nbinations on the CC2-10 time series. \n  \nCombination \nLSTM training ratio \n  \n  \n  \n  \nRePAD-TFK \n0.0094 (379/40320) \nRePAD-PT \n0.0089 (357/40320) \nRePAD-DL4J \n0.0131 (528/40320) \n  \nTable 8: The ADT-NT of the three RePAD combinations on \nthe CC2-10 time series. \n  \n  \n  \n  \n  \nCombination \nADT-NT (sec) \nStd. Dev. (sec) \nRePAD-TFK \n0.518 \n0.726 \nRePAD-PT \n0.069 \n0.263 \nRePAD-DL4J \n0.028 \n0.022 \n  \nTables \n10 \nto \n13 \nshow \nthe detection results \nof \nthe three RePAD combinations on another time se- \nries B3B-10. Apparently, REPAD-TFK has the high- \nest detection accuracy and the lowest LSTM train- \ning ratio. However, its ADT-NT and ADT-T are the \nlongest. This result confirms that TenserFlow-Keras \nintroduces more overhead to RePAD than the other \ntwo libraries do. \nWhen RePAD was implemented in PyTorch, it has \nthe second best detection accuracy, the second short- \nTable 9: The ADT-T of the three RePAD combinations on \nthe CC2-10 time series. \n  \n  \n  \n  \n  \nCombination \nADT-T (sec) \nStd. Dev. (sec) \nRePAD-TFK \n1.913 \n1.409 \nRePAD-PT \n0.100 \n0.318 \nRePAD-DL4J \n0.375 \n0.030 \n  \nest ADT-NT, and the shortest ADT-T. In other words, \nPyTorch provides a very good balance between detec- \ntion accuracy and response time. On the other hand, \nwhen RePAD-DL4J worked on B3B-10, \nits perfor- \nmance is similar to its performance on CC2-10 (i.e., \nthe lowest detection accuracy but satisfactory execu- \ntion performance). \nTable 10: The detection accuracy of the three RePAD com- \nbinations on B3B-10. \n  \n  \n  \n  \n  \nCombination \nPrecision \nRecall \nF-score \nRePAD-TFK \n0.892 \n1 \n0.943 \nRePAD-PT \n0.872 \n1 \n0.932 \nRePAD-DL4J \n0.828 \n1 \n0.906 \n  \nTable \n11: \nThe LSTM training ratio of the three RePAD \ncombinations on B3B-10. \n  \nCombination \nLSTM training ratio \n  \n  \n  \n  \nRePAD-TFK \n0.0026 (105/40320) \nRePAD-PT \n0.0028 (112/40320) \nRePAD-DL4J \n0.0042 (168/40320) \n  \n5.2. \nThree SALAD combinations \nTo \nevaluate \nthe \ndetection \naccuracy \nof \nthe \nthree \nSALAD combinations, we also used precision, recall, \nand F-Score. Furthermore, we measured the average \ntime for each SALAD combination to process each \ndata point in their conversion phases and detection \nphases. \nFigure 3 shows the detection results of the three \nSALAD combinations on the TMRT time series. Ap- \nparently, all of them can detect the collective anomaly \nwithout any false positive or false negative. \nHence, \nthe precision, recall, and F-score of the three combi- \nnations are all one as shown in Table 14. \nTable \n15 lists the time consumption of the three \nSALAD combinations \non TMRT. \nIt \nis \nclear \nthat \nSALAD-PT has the shortest average conversion time \nand average detection time, whereas SALAD-TFK \nhas the longest average conversion time and average \ndetection time. \nIt seems like PyTorch is also the best \nchoice for SALAD so far. \nTable \n16 lists the detection results of the three \nSALAD combinations on the NYC time series. We \ncan see that SALAD-DL4J has the best detection ac- \ncuracy. Recall that the conversion phase of SALAD\nTable 12: The ADT-NT of the three RePAD combinations \non the B3B-10 time series. \n  \n  \n  \n  \n  \nCombination \nADT-NT (sec) \nStd. Dev. (sec) \nRePAD-TFK \n0.517 \n0.724 \nRePAD-PT \n0.069 \n0.263 \nRePAD-DL4J \n0.028 \n0.015 \n  \nTable 13: The ADT-T of the three RePAD combinations on \nthe B3B-10 time series. \n  \n  \n  \n  \n  \nCombination \nADT-NT (sec) \nStd. Dev. (sec) \nRePAD-TFK \n1.989 \n1.436 \nRePAD-PT \n0.105 \n0.325 \nRePAD-DL4J \n0.388 \n0.039 \n  \n(Lee et al., 2021b) aims to convert a complex time se- \nries into a less complex AARE series by predicting \nthe value for each future data point, measuring the \ndifference between every pair of predicted and actual \ndata points, and deriving the corresponding AARE \nvalues. \nAs we can see from Figure 4 that most of \nthe data points predicted by the conversion phase of \nSALAD-DL4J matched the real data points. Conse- \nquently, as shown in Figure 5, the detection phase \nof SALAD-DL4J was able to detect all the collec- \ntive anomalies even though there are some false posi- \ntives. However, the good performance of the conver- \nsion phase of SALAD-DL4J comes at the price of a \nlong conversion time (see Table 17) due to required \nLSTM model training for many data points. \nOn \nthe \nother \nhand, \nwhen SALAD-TFK \nand \nSALAD-PT worked on NYC, they both had very \npoor detection accuracy (see Table \n16). \nSALAD- \nTFK could detect only one collective anomaly, i.e., \nthe snow storm. This is because the conversion phase \nof SALAD-TFK was unable to correctly predict data \npoints (as shown in Figure 6). This bad performance \nconsequently affected the detection phase of SALAD- \nTFK and disabled it to detect anomalies. We can see \nfrom Figure 7 that almost all AARE values are lower \nthan the detection threshold. \nIf we look at Figure 7 more closely, we can see \nthat the detection threshold was very high in the be- \nmo \n| \n‘5 1400 \n@ 1200 \n2 \n—Anomaly \nOSALAD_DL4J \n©SALAD_TFK \nASALAD_Pytorch \n| \n  \n10 \n10 \nml \nB \noe \nS \nss \n66 \nThe total nu \nROD \nés \n88 \nN S 8 \n0 \n  \n| \nFigure 3: The detection results of the three SALAD combi- \nnations on the TMRT time series. \nTable 14: The detection accuracy of the three SALAD com- \nbinations on the TMRT time series. \n  \n  \n  \n  \n  \nCombination \nPrecision \nRecall \nF-score \nSALAD-TFK \n1 \n1 \n1 \nSALAD-PT \n1 \n1 \n1 \nSALAD-DL4J \n1 \n1 \n1 \n  \nTable 15: The time consumption of the three SALAD com- \nbinations on the TMRT time series. \n  \n  \n  \n  \n  \nCombination \nAverage \nConversion \nAverage \nDetection \nTime/Std. Dev.(sec) \nTime/Std. Dev. (sec) \nSALAD-TFK \n0.949/1.017 \n0.472/0.703 \nSALAD-PT \n0.023/0.163 \n0.008/0.087 \nSALAD-DL4J \n0.162/0.399 \n0.011/0.027 \n  \nTable 16: The detection accuracy of the three SALAD com- \nbinations on the NYC time series. \n  \n  \n  \n  \n  \nCombination \nPrecision \nRecall \nF-score \nSALAD-TFK \n0.447 \n0.2857 \n0.349 \nSALAD-PT \n0.338 \n0.2857 \n0.310 \nSALAD-DL4J \n0.709 \n1 \n0.830 \n  \nTable 17: The time consumption of the three SALAD com- \nbinations on the NYC time series. \n  \n  \n  \n  \n  \nCombination \nAverage \nConversion \nAverage \nDetection \nTime/Std. Dev.(sec) \nTime/Std. Dev. (sec) \nSALAD-TFK \n0.477/0.798 \n0.488/0.721 \nSALAD-PT \n0.045/0.273 \n0.022/0.147 \nSALAD-DL4J \n2.306/4.969 \n0.018/0.042 \n  \n  \n50000 \n| —The original data points in the NYC detaset_ \n—The data points predicted by the conversion phase of SALAD_DI4I \n| \n  \nNumber of NYC taxi passengers \n   \nFigure 4: The original data points in the NYC time series \nversus the data points predicted by the conversion phase of \nSALAD-DL4J. \n0.5 \n4s \n0.4 \n0.35 \n0.3 \n0.25 \nNYC marathon \nSnow storm ——\" \nNew Year's Day \n0.2 \n0.15 \n01 \n0.05 \nThanksgiving \nChristmas \n  \n  \n  \nBREESE ESSRSSSSRRESSS \n8 \nFigure 5: \nThe AARE values generated by the detection \nphase of SALAD-DL4J versus the self-adaptive detection \nthreshold of SALAD-DL4J on the NYC time series. \nRRS \nRRR \ng\n  \n—The original data points in the NYC dataset \n—The data points predicted by the conversion phase af SALAD_TFK \nF TRAM \nAnn \nFigure 6: The original data points in the NYC time series \nversus the data points predicted by the conversion phase of \nSALAD-TFK. \n  \n  \n1.8 \n—AARE_ —Threshold \n1.6 \n1.4 \n1.2 \n1 \nSnow storm \n0.8 \n0.6 \n0.4 \n0.2 \n  \n  \n  \n0 \n  \nFigure 7: \nThe AARE values generated by the detection \nphase of SALAD-TFK versus the self-adaptive detection \nthreshold of SALAD-TFK on the NYC time series. \nginning due to the high AARE values, which makes \nSALAD felt that its current LSTM model did not need \nto be replaced. \nEven though the threshold dropped \nafterwards, it was still much higher than many subse- \nquent AARE values. This is why most of the anoma- \nlies could not be detected. \nSince SALAD-TFK re- \nquires only a few model training, its average conver- \nsion time is much shorter than that of SALAD-DL4J \n(see Table 17). \nThe same situation happened to SALAD-PT when \nit worked on the NYC series. SALAD-PT has very \npoor detection accuracy even though its average con- \nversion time and average detection time are the short- \nest. \n6 \nCONCLUSIONS AND FUTURE \nWORK \nIn this paper, we investigated how DL libraries impact \nonline adaptive lightweight time series anomaly de- \ntection by implementing two state-of-the-art anomaly \ndetection approaches (RePAD and SALAD) in three \nwell-known \nDL \nlibraries \n(TensorFlow-Keras, \nPy- \nTorch, and Deeplearning4j) and conducting a series of \nexperiments to evaluate their detection performance \nand time consumption based on four open-source time \nseries. \nThe results indicate that DL libraries have a \nsignificant impact on RePAD and SALAD in terms of \nnot only their detection accuracy but also their time \nconsumption and response time. \nAccording to the results, TensorFlow-Keras is not \nrecommended for online adaptive lightweight time se- \nries anomaly detection because it might lead to unsta- \nble detection accuracy and more time consumption. \nWhen it was used to implement RePAD, RePAD had \nsatisfactory detection accuracy. However, when it was \nused to implement SALAD, SALAD had unstable de- \ntection accuracy on one used time series. \nBesides, \nTensorFlow-Keras is less efficient than PyTorch and \nDeeplearning4j because it causes the longest response \ntime for both RePAD and SALAD. \nOn the other hand, PyTorch is the most efficient \nlibrary among the three DL libraries since it enables \nRePAD and SALAD to provide real-time processing \nand instant responses. \nIt also enables RePAD to pro- \nvide high detection accuracy. \nHowever, \nsimilar to \nTensorFlow-Keras, \nit causes unstable detection ac- \ncuracy when it was used to implement SALAD and \nworked on the NYC time series. \nDeeplearning4j \nis considered the most stable li- \nbrary among the three DL libraries because \nit not \nonly enables RePAD and SALAD to provide satisfac- \ntory detection accuracy, but also enables RePAD and \nSALAD to have reasonable time consumption and re- \nsponse time. \nWe found that \nit is very important to carefully \nchoose DL libraries for online adaptive lightweight \ntime series anomaly detection because DL libraries \nmight not show the true performance of an anomaly \ndetection approach. \nWhat makes \nit even worse \nis \nthat they might mislead developers or users in believ- \ning that one bad anomaly detection approach imple- \nmented in a good DL library is better than a good \nanomaly detection approach implemented in a bad DL \nlibrary. \nIn our future work, we would like to release all the \nsource code (i.e., RePAD and SALAD implemented \nin the three DL libraries) on a public software reposi- \ntory such as GitHub, GitLab, or Bitbucket. \nACKNOWLEDGEMENT \nThe authors want to thank the anonymous reviewers \nfor their reviews and suggestions for this paper.\nREFERENCES \nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, \nJ., Devin, M., Ghemawat, \nS., Irving, G., Isard, M., \net al. (2016). \nTensorflow: \na system for large-scale \nmachine learning. \nIn Osdi, volume 16, pages 265— \n283. Savannah, GA, USA. \nAhmed, M., Mahmood, A. N., and Hu, J. (2016). A survey \nof network anomaly detection techniques. Journal of \nNetwork and Computer Applications, 60:19-31. \nAlimohammadi, \nH. \nand Chen, \nS. \nN. \n(2022). \nPerfor- \nmance evaluation of outlier detection techniques in \nproduction \ntimeseries: \nA \nsystematic \nreview \nand \nmeta-analysis. \nExpert Systems with Applications, \n191:116371. \nBlazquez-Garcia, \nA., Conde, \nA., Mori, \nU., and Lozano, \nJ. A. (2021). \nA review on outlier/anomaly detection \nin time series data. ACM Computing Surveys (CSUR), \n54(3):1-33. \nBraei, M. and Wagner, \nS. (2020). \nAnomaly detection in \nunivariate time-series: \nA survey on the state-of-the- \nart. arXiv preprint arXiv:2004.00433. \nChi, H., Zhang, Y., Tang, T. L. E., Mirabella, L., Dalloro, \nL., Song, L., and Paulino, G. H. (2021). \nUniversal \nmachine learning for topology optimization. \nCom- \nputer Methods in Applied Mechanics and Engineer- \ning, 375:112739. \nCNTK (2023). The microsoft cognitive toolkit is a unified \ndeep learning toolkit. \nhttps://github.com/microsoft/ \nCNTK. [Online; accessed 25-February-2023]. \nDeeplearning4j (2023). Introduction to core Deeplearning4j \nconcepts. https://deeplearning4j.konduit.ai/. [Online; \naccessed 24-February-2023]. \nDeka, \nP. K., Verma, \nY., Bhutto, A. B., Elmroth, \nE., and \nBhuyan, \nM. (2022). \nSemi-supervised range-based \nanomaly detection for cloud systems. [EEE Transac- \ntions on Network and Service Management. \nEarlyStopping (2023). \nWhat \nis early stopping? \nhttps: \n//deeplearning4j.konduit.ai/. \n[Online; accessed 24- \nFebruary-2023]. \nEom, H., Figueiredo, R., Cai, H., Zhang, Y., and Huang, \nG. (2015). Malmos: Machine learning-based mobile \noffloading scheduler with online training. \nIn 20/5 \n3rd IEEE International Conference on Mobile Cloud \nComputing, Services, and Engineering, pages 51-60. \nIEEE. \nHochreiter, S. and Schmidhuber, J. (1997). Long short-term \nmemory. Neural computation, 9(8):1735—1780. \nIandola, \nF N., Han, \nS., Moskewicz, M. W., Ashraf, K., \nDally, W. \nJ., and Keutzer, K. (2016). \nSqueezenet: \nAlexnet-level \naccuracy \nwith \n50x \nfewer \nparame- \nters \nandj \n0.5 \nmb \nmodel \nsize. \narXiv preprint \narXiv: 1602.07360. \nIbrahim, M., Badran, K. M., and Hussien, A. E. (2022). \nArtificial \nintelligence-based \napproach \nfor \nunivari- \nate time-series anomaly detection using hybrid cnn- \nbilstm model. In 2022 13th International Conference \non Electrical Engineering (ICEENG), pages 129-133. \nIEEE. \nKeras (2023). \nKeras \n- \na deep learning API written \nin \npython. https://keras.io/about/. [Online; accessed 25- \nFebruary-2023]. \nKetkar, N. and Santana, \nE. (2017). \nDeep learning with \nPython, volume 1. Springer. \nKieu, T., Yang, B., and Jensen, C. S. (2018). Outlier detec- \ntion for multidimensional time series using deep neu- \nral networks. In 20/8 19th IEEE international confer- \nence on mobile data management (MDM), pages 125— \n134. IEEE. \nKovalev, V., Kalinovsky, A., and Kovalev, S. (2016). Deep \nlearning with theano, \ntorch, \ncaffe, \ntensorflow, \nand \ndeeplearning4j: Which one is the best in speed and \naccuracy? \nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2017). Im- \nagenet classification with deep convolutional neural \nnetworks. Communications of the ACM, 60(6):84—90. \nLavin, \nA. and Ahmad, \nS. (2015). \nEvaluating real-time \nanomaly detection algorithms—the numenta anomaly \nbenchmark. \nIn 20/5 IEEE 14th international confer- \nence on machine learning and applications (ICMLA), \npages 38-44. IEEE. \nLee, M.-C. and Lin, J.-C. (2023). \nRePAD2: \nReal-time, \nlightweight, and adaptive anomaly detection for open- \nended time series. \nIn Proceedings of the 8th Inter- \nnational Conference on Internet of Things, Big Data \nand Security \n- IoTBDS, pages 208-217. INSTICC, \nSciTePress. arXiv preprint arXiv:2303.00409. \nLee, M.-C., Lin, J.-C., and Gan, E. G. (2020a). \nReRe: A \nlightweight real-time ready-to-go anomaly detection \napproach for time series. \nIn 2020 IEEE 44th Annual \nComputers, \nSoftware, \nand Applications Conference \n(COMPSAC), pages 322-327. IEEE. arXiv preprint \narXiv:2004.02319. The updated version of the ReRe \nalgorithm from arXiv was used here. \nLee, M.-C., Lin, J.-C., and Gran, E. G. (2020b). \nRePAD: \nreal-time proactive anomaly detection for time series. \nIn Advanced Information Networking and Applica- \ntions: Proceedings of the 34th International Confer- \nence on Advanced Information Networking and Ap- \nplications (AINA-2020), pages 1291-1302. Springer. \narXiv preprint arXiv:2001.08922. The updated ver- \nsion of the RePAD algorithm from arXiv was used \nhere. \nLee, M.-C., Lin, J.-C., and Gran, E. G. (2021a). How far \nshould we look back to achieve effective real-time \ntime-series anomaly detection? \nIn Advanced Infor- \nmation Networking and Applications: Proceedings of \nthe 35th International Conference on Advanced In- \nformation Networking and Applications (AINA-2021), \nVolume \n1, pages 136-148. Springer. arXiv preprint \narXiv:2102.06560. \nLee, M.-C., Lin, J.-C., and Gran, E. G. (2021b). SALAD: \nSelf-adaptive lightweight anomaly detection for real- \ntime recurrent time series. \nIn 202] IEEE 45th An- \nnual Computers, Software, and Applications Confer- \nence (COMPSAC), pages 344-349. IEEE. \nLee, \nT. \nJ., \nGottschlich, \nJ., \nTatbul, \nN., Metcalf, \nE., and \nZdonik, S. (2018). Greenhouse: A zero-positive ma- \nchine learning system for time-series anomaly detec- \ntion. arXiv preprint arXiv: 1801.03168.\nLinkedIn (2018). \nlinkedin/luminol [online code reposi- \ntory]. \nhttps://github.com/linkedin/luminol. \n[Online; \naccessed 24-February-2023]. \nMoso, \nJ. \nC., \nCormier, \nS., \nde Runz, \nC., \nFouchal, \nH., \nand Wandeto, \nJ. \nM. \n(2021). \nAnomaly detection \non data streams for smart agriculture. \nAgriculture, \n11(11):1083. \nNguyen, \nG., \nDlugolinsky, \nS., \nBobdék, \nM., \nTran, \nV., \nLopez Garcia, A., Heredia, I., Malik, P., and Hluchy, \nL. (2019). Machine learning and deep learning frame- \nworks and libraries for large-scale data mining: a sur- \nvey. Artificial Intelligence Review, 52:77-124. \nNiu, Z., Yu, K., and Wu, X. (2020). \nLSTM-based VAE- \nGAN for time-series anomaly detection. \nSensors, \n20(13):3738. \nPaszke, A., Gross, \nS., Massa, \nF., Lerer, A., Bradbury, \nJ., \nChanan, \nG., \nKilleen, \nT., \nLin, \nZ., \nGimelshein, \nN., \nAntiga, L., et al. (2019). Pytorch: An imperative style, \nhigh-performance deep learning library. Advances in \nneural information processing systems, 32. \nPereira, J. and Silveira, M. (2019). \nLearning representa- \ntions from healthcare time series data for unsuper- \nvised anomaly detection. In 2019 IEEE international \nconference on big data and smart computing (Big- \nComp), pages 1-7. IEEE. \nRen, H., Xu, B., Wang, Y., Yi, C., Huang, C., Kou, X., Xing, \nT., Yang, M., Tong, J., and Zhang, Q. (2019). Time- \nseries anomaly detection service at microsoft. In Pro- \nceedings of the 25th ACM SIGKDD international con- \nference on knowledge discovery & data mining, pages \n3009-3017. \nSchneider, J., Wenig, P., and Papenbrock, T. (2021). \nDis- \ntributed detection of sequential anomalies in univari- \nate time series. The VLDB Journal, 30(4):579-602. \nTwitter \n(2015). \nAnomalyDetection \nR \npackage \n[on- \nline \ncode \nrepository]. \n— https://github.com/twitter/ \nAnomalyDetection. \n[Online; accessed 24-February- \n2023]. \nWang, Z., Liu, K., Li, J., Zhu, Y., and Zhang, Y. (2019). \nVarious frameworks and libraries of machine learning \nand deep learning: \na survey. \nArchives of computa- \ntional methods in engineering, pages 1-24. \nYatish, \nH. \nand Swamy, \nS. \n(2020). \nRecent trends \nin \ntime series forecasting—a survey. \nInternational Re- \nsearch Journal of Engineering and Technology (IR- \nJET), 7(04):5623-5628. \nYeh, \nC.-C. \nM., \nZhu, \nY., \nDau, \nH. \nA., \nDarvishzadeh, \nA., \nNoskov, \nM., \nand Keogh, \nE. \n(2019). \nOnline \namnestic dynamic time warping to allow real-time \ngolden batch monitoring. \n_https://sites.google.com/ \nview/gbatch?pli=1. \nZahidi, \nY., \nEl Younoussi, \nY., and Al-Amrani, \nY. (2021). \nA powerful comparison of deep learning frameworks \nfor arabic sentiment analysis. \nInternational Journal \nof Electrical & Computer Engineering (2088-8708), \n11(1). \nZhang, \nJ. E., Wu, D., and Boulet, B. (2021). \nTime se- \nries anomaly detection for smart grids: A survey. \nIn \n2021 IEEE Electrical Power and Energy Conference \n(EPEC), pages 125-130. IEEE. \nZhang, Q., \nLi, X., Che, X., Ma, \nX., Zhou, \nA., Xu, M., \nWang, S., Ma, Y., and Liu, X. (2022). A comprehen- \nsive benchmark of deep learning libraries on mobile \ndevices. In Proceedings of the ACM Web Conference \n2022, pages 3298-3307. \nZhang, X., Wang, Y., and Shi, W. (2018). \npcamp: Perfor- \nmance comparison of machine learning packages on \nthe edges. In HotEdge.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-04-30",
  "updated": "2023-05-10"
}