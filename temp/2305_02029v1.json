{
  "id": "http://arxiv.org/abs/2305.02029v1",
  "title": "Natural language processing on customer note data",
  "authors": [
    "Andrew Hilditch",
    "David Webb",
    "Jozef Baca",
    "Tom Armitage",
    "Matthew Shardlow",
    "Peter Appleby"
  ],
  "abstract": "Automatic analysis of customer data for businesses is an area that is of\ninterest to companies. Business to business data is studied rarely in academia\ndue to the sensitive nature of such information. Applying natural language\nprocessing can speed up the analysis of prohibitively large sets of data. This\npaper addresses this subject and applies sentiment analysis, topic modelling\nand keyword extraction to a B2B data set. We show that accurate sentiment can\nbe extracted from the notes automatically and the notes can be sorted by\nrelevance into different topics. We see that without clear separation topics\ncan lack relevance to a business context.",
  "text": "Natural language processing on customer note data\nAndrew Hilditcha, David Webbb, Jozef Baˇcab, Tom Armitageb, Matthew\nShardlowa, Peter Applebyb\naManchester Metropolitan University, Manchester, M15 6BH, UK\nbAuto Trader Group, 1 Tony Wilson Place, Manchester, M15 4FN, UK\nAbstract\nAutomatic analysis of customer data for businesses is an area that is of\ninterest to companies. Business to business data is studied rarely in academia\ndue to the sensitive nature of such information. Applying natural language\nprocessing can speed up the analysis of prohibitively large sets of data. This\npaper addresses this subject and applies sentiment analysis, topic modelling\nand keyword extraction to a B2B data set. We show that accurate sentiment\ncan be extracted from the notes automatically and the notes can be sorted by\nrelevance into diﬀerent topics. We see that without clear separation topics\ncan lack relevance to a business context.\nKeywords:\nNatural language processing, Sentiment analysis, Topic\nmodelling, Keyword extraction, Transformers\n1. Introduction\nTo foster customer loyalty and provide assistance, companies frequently\ncommunicates with and collect feedback from their customers. Feedback is\ngathered in form of notes, that call handlers make after a conversation with a\ncustomer. Multiple studies have shown the impact of eﬃciently assessing cus-\ntomer feedback and implementing change, increasing customer satisfaction\nand loyalty (Lam et al., 2013); (Azzam, 2014) ; (Mulyono and Situmorang,\n2018) as well as company revenues (Phan and Vogel, 2010). This study fo-\nEmail addresses: andrew.hilditch@mmu.ac.uk (Andrew Hilditch),\ndave.webb@autotrader.co.uk (David Webb), jozef.baca@autotrader.co.uk (Jozef\nBaˇca), tom.armitage@autotrader.co.uk (Tom Armitage), m.shardlow@mmu.ac.uk\n(Matthew Shardlow), peter.appleby@autotrader.co.uk (Peter Appleby)\nPreprint submitted to Expert Systems with Applications\nMay 4, 2023\narXiv:2305.02029v1  [cs.CL]  3 May 2023\ncusses on the customer notes collected by AutoTrader, a UK company that\nprovides an online marketplace for UK car dealers. Currently, analysis is done\nmanually on a subset of the notes with the intent to extract information that\nwould be valuable for the company.\nThese insights would be provided with Natural Language Processing\n(NLP). NLP refers to the branch of computer science concerned with giving\ncomputers the ability to understand text and spoken words in much the same\nway human beings can (IBM, 2023). The areas of interest were sentiment\nanalysis, topic modelling and keyword extraction.\n2. Related work\nThe ﬁeld of note analysis with NLP is well researched, with work being\ndone on medical notes (Sheikhalishahi et al., 2019), (Juhn and Liu, 2020) and\non data from social media networks like Twitter (Sanders et al., 2021) and\nReddit (Okon et al., 2020). Note analysis data is typically available in large\nquantities but requires pre-cleaning to remove irrelevant entries. It tends to\ninclude more errors than formal text and is typically not examined in great\ndetail by hand. Work has been done to analyse the notes of those attempting\nsuicide (Pestian et al., 2010). This work classiﬁed the notes to attempt to\npredict repeated suicide attempts.\n2.1. Industrial applications of NLP\nWork has been done to look at NLP applications in industry. (Kalyanatha\net al., 2019) looked at diﬀerent applications in areas such as ﬁnance and retail.\nMany of the current applications focus on chat bots (Khan and Rabbani,\n2021), (Sari et al., 2020). They look at banking and social media networks.\nWork has also been done to examine the use of NLP in the construction\nsector (Ding et al., 2022). This worked examined the use of NLP for risk\nmanagement and building information modelling among other uses.\n2.2. Sentiment analysis\nSentiment analysis is a subject of huge importance in data science that\nhas gained signiﬁcantly in prevalence over the last decade or so, with more\nthan 99% of papers published on the subject coming after 2004 due to the\nvast expansion of unstructured text based datasets available (Mantyla et\nal., 2018). Sentiment analysis can be performed using both supervised and\nunsupervised methodologies, we will focus on the unsupervised approach.\n2\nMany papers that study sentiment using NLP have been published. One\nis the aforementioned study looking at Twitter sentiment (Okon et al., 2020).\nThere are many other papers looking at Twitter data (Kanakaraj and Gud-\ndeti, 2015), (Hasan et al, 2019). Previous researchers in the area have used\nlexicon approaches to study sentiment for topics within a document (Na-\nsukawa and Li, 2003). The work on sentiment in this paper is in section\nthree.\n2.3. Transformers\nNLP has changed since the introduction of the Transformer architec-\nture (Vaswani et al., 2017). It led to a number of papers that utilised and\ndeveloped the architecture such as Bidirectional Encoder Representations\nfrom Transformers (BERT) (Devlin et al., 2018) and RoBERTa (Liu et al.,\n2019). Like a neural network, Transformers work by utilising a deep learn-\ning model where all output elements are connected to all input elements,\nwith the weightings between them calculated dynamically based on the con-\nnection. BERT diﬀers however in that the text is read by the algorithm\nboth forwards and backwards at the same time, allowing text to attenuate\nwith itself rather than an output layer. This feature drastically increases\nthe speed at which BERT can train and allows entire sentences to be used\nas inputs rather than tokenized data, meaning BERT is capable of contex-\ntualising words within their sentence structure. This last point has proved\nrevolutionary in the world of NLP research as prior to Transformers all works\nhad to be performed with word tokens taken in isolation, severely limiting\ntheir use to the speciﬁc data they were trained upon.\nBERT was made open source by Google in 2018 having been trained on\nthe entire content of the English language version of Wikipedia (Devlin et\nal., 2018). Since then BERT has been converted into libraries capable of a\nwide range of NLP tasks such as sentiment analysis, sentence completion and\ntext summarisation. Additionally, these libraries come with added advantage\nof being able to ﬁne tune the algorithm with relatively small datasets to\nincrease the speciﬁcity to a corpus of choice (such as the AutoTrader note\ndata), although classiﬁcation tasks such as sentiment analysis will require\nlabelled data. Utilisation of these libraries is easy to implement but can be\ncomputationally expensive, often requiring access to a GPU to run in eﬃcient\ntime frames which should also be considered before use. Many of these new\nTransformer models can now be found on the HuggingFace (Wolf et al., 2019)\nwebsite where the Transformers library is located (Brasoveanu and Andonie.,\n3\n2020). The models used in this paper were obtained from the HuggingFace\nlibrary (Hugging Face, 2023).\n2.4. Topic modelling\nTopic modelling provides an unsupervised approach to topic allocation in\ntexts, with a broad range of complexities. It will be explored in sections four\nand ﬁve of this paper Simple approaches such as Latent Semantic Indexing\n(LSI) (Hofmann, 1999) involve the vectorisation of texts within a corpus and\ngrouping together based on co-sine similarity (an eﬀective measure used to\ncompare similarity of vectors (Han et al., 2001)). Such methods are quick to\nimplement and require little resources but come with the large disadvantage\nthat the nature of the topic groupings remains unknown, making the results\nvery diﬃcult to interpret. More sophisticated methods tend to be based on\nmore complex algorithms such as neural networks, such as the work from\n(Gavval et al., 2019) which explores the use of self-organising maps (SOMs)\nto reduce dimensionality within the data and create an interpretable 2D map\nof topics. But whilst having the advantage of interpretability these methods\nare often computationally expensive and can tie up valuable resources within\nan organisation.\nLatent Dirichlet Allocation (LDA) (Blei et al., 2003) provides a middle\nground between overly simply non-interpretable and overly-complex resource\nheavy topic modelling techniques, and is one of the most commonly used\nmethods in the ﬁeld (Jelodar et al., 2019). LDA involves creating a latent\nlayer of topics within a dataset where words that are likely to be found\nnear each other within texts are grouped. Each text within a corpus is then\nevaluated for a percentage match with each of the topics in the latent layer\nto allow allocation. One of the drawbacks with LDA is that as a statistical\napproach, interpretation of the topics is still required to achieve sensible\nresults, just because words are statistically found near each other does not\nnecessarily mean they will be considered related by a human observer. This\nmeans LDA topic modelling can require extensive hyperparameter tuning\nto produce good results but having a sector expert on the texts at hand to\nperform this can add a built-in sanity check step to the evaluation. This is\none of the methods used in section four of this paper.\n2.5. Keyword extraction\nKeyword extraction (KE) has been used to improve the eﬃciency of other\nNLP methods, most notably Information Retrieval (Yang et al., 2019). These\n4\nmethods then don’t have to processes the whole text of a document, but only\ntext that carries the subject of the document. Use with other NLP appli-\ncations implies that the KE algorithm should be quick, eﬃcient, robust and\neasily applied to new domains. This led to a new method being created called\nRapid Automatic Keyword Extraction (RAKE) (Rose et al., 2010). RAKE\nis an unsupervised, language-independent document-oriented method for ex-\ntracting keywords from individual documents. RAKE’s creators observed\nthat keywords often appear as a combination of multiple words rather than a\nsingle word, and almost never contain any stop words or punctuation. RAKE\nconsiders these non-stop words as the main candidates then uses a graph-\nbased approach to capture the co-occurrences of these candidates, which are\nused to calculate a score to choose the best keywords.\nDeep learning approaches of KE are not common, as Deep neural networks\n(DNNs) require large annotated datasets. The introduction of transformers\nhas inspired scientists to propose new methods utilising the self-attention\nlayers without the need of labelled datasets. In 2019 a new approach was\nproposed, called the Self-supervised Contextual Keyword and Keyphrase Re-\ntrieval with Self-labelling (SCKKRS) (Sharma and Li., 2019). The aim of\nSCKKRS is for it to be useful for both long as well as short text inputs to\nextract keywords and keyphrases. SCKKRS uses feature vectors to obtain a\nkeyword that is most similar to the meaning of the sentence. It obtains the\nfeature vectors using BERT.\nKeyBERT is an implementation of SCKKRS approach, which combines\nthe feature vectors containing the meanings from BERT with statistical n-\ngram approach. KeyBERT works by extracting groups of words, which have\nthe highest similarity between their embeddings and the sentence embedding\n(Rao et al., 2022). This approach will be used in section ﬁve of this paper.\n2.6. Clustering\nGeneric K-Means algorithm, as described by in 2013 (Kodinariya and\nMakwana, 2013), is an unsupervised learning algorithm that does not have\nhigh computational complexity. It has been utilised in many ﬁelds of NLP.\nThe process of K-Means provides an easy solution to a classiﬁcation problem\nof classifying data into known number of clusters. The main downside of the\nK-Means algorithm is the need to know the number of clusters. There are\nseveral approaches to ﬁnd the best k value, but they don’t generally produce\nan answer without running the algorithm several times with diﬀerent values,\ncosting time and computational resources. K-Means clustering is commonly\n5\nused in the NLP problem of Topic Modelling (TM). The method ﬁrst applies\na NLP function, which captures the textual content and then uses the K-\nMeans algorithm to categorise data into clusters (Curiskis et al., 2020). In\nthe same study (Curiskis et al., 2020), a combination of Doc2Vec feature\nrepresentation and K-Means clustering gave the best performance across two\ndatasets.\n3. Sentiment analysis\n3.1. Introduction\nSentiment analysis is an important feature of the note analysis for Auto-\nTrader as it provides a measure on customer opinion towards product and\npolicy changes implemented by the company.\nCurrent sentiment analysis\nworks within AutoTrader are done manually by dedicated sector experts,\nbut this method is problematic for three reasons.\n1. The process is labour intensive and requires full reading and compre-\nhension of the notes from a sector expert, and with thousands of notes\nreceived a month processing every one is not realistic, meaning arbi-\ntrary prioritisation methods such as selection based on anecdotal evi-\ndence must be used.\n2. Individual sector experts may carry inherent bias in their assessment\nof note sentiment leading to added variation in the results.\n3. Manual sentiment analysis is very diﬃcult to classify in a granular scale\n(i.e. a scored value rather than simply positive or negative), a feature\nwhich has been highlighted of importance by the AutoTrader team in\nallowing them to identify an underlying baseline sentiment for their\ncustomer feedback.\nThese issues can all be solved by automated unsupervised sentiment analysis\ntechniques which allow for eﬃcient processing in a non-biased fashion with\nthe option for continuous scoring depending on the approach used.\nAutomated unsupervised sentiment analysis approaches do come with\nan issue for the AutoTrader note data. Unsupervised techniques will still\nbe trained on available datasets to the creator, which whilst not making\nthem speciﬁc to that dataset will provide a better accuracy for similarly\nstructured data. A literature search has found few examples of techniques\nthat are tuned on B2B customer feedback data similar to the AutoTrader\nnotes. We hypothesise that this is due to B2B customer feedback containing\n6\nsensitive data which companies would be reluctant to disclose for academic\npublishing. Most unsupervised techniques are therefore trained on publicly\navailable B2C data where customer feedback is collected from open access\npublic sources such as Amazon or Yelp. B2B data diﬀers from B2C (business\nto customer) data as it is typically collected using dyadic (person to person)\nrather than automated (person to device) techniques (Murphy and Sashi,\n2018). Dyadic conversation tends to more personable than automated, with\na level of mediation which tones downs language used (Aguilar et al., 2016).\nThis toning down of language may lead to a miscalibration of any scoring\nsystems used to gauge sentiment within a technique that this trained using\nautomated communication with more extreme language usage.\n3.2. Methodology\nFigure 1 shows the method for calculating the sentiment of the data set.\nFigure 1: Flow diagram of the sentiment analysis process.\n1. Load dataset\n(a) Dataset is loaded into the notebook as a pandas data frame\n2. Initialise sentiment algorithm\n7\n(a) Sentiment algorithm of choice library is loading into the notebook\nand initialised based on instructions from the HuggingFace direc-\ntions\n(b) Note that no hyperparameter choices are required as the Hugging-\nFace library does not support this\n3. Load data sample\n(a) Loop established to cycle through each text in the loaded dataset\nin turn through steps 4 to 6\n4. Sentence tokenize data\n(a) Individual text is tokenized at a sentence level, with a loop gener-\nated to run through each tokenized sentence in turn through step\n5\n(b) Sentences longer than 522 characters are truncated due to the\nword limit of the HuggingFace algorithm\n(c) Note that sentence tokenization was preferred as it produced bet-\nter results than analysing each text as a whole\n5. Calculate sentence sentiment\n(a) Sentence text is run through the sentiment analysis function to\ncalculate the score\n(b) HuggingFace outputs a score and a sentiment label (e.g. [‘label’:\n‘POSITIVE’, ‘score’: 0.9998], score values were extracted from\nthis output with negative labelled score multiplied by -1\n6. Data is collected and appended to the text entry data row in this initial\nloaded data frame, collected data includes\n(a) Average sentence sentiment\n(b) Negative sentiment sentence count\n(c) Max negative sentence score\n(d) Positive sentence count\n(e) Max positive sentence score\nA subset of 1000 of the notes were annotated with sentiment sorted into\nﬁve categories. These are very bad, bad, neutral, good, very good. The\nbreakdown is shown in ﬁgure 2.\n3.3. Results\nInitial analysis of the sentiment distribution contained within the Au-\ntoTrader note data showed that over 65% of the notes were classiﬁed as\n8\nFigure 2: Bar chart showing distribution of annotated sentiment.\nextremely negative (ﬁgure 3), and almost 87% of all notes showed a negative\nsentiment score. The reason for this is that the notes worked on were marked\nas feedback, which tends towards negative sentiment. Additionally research\nhas shown that sector experts tend to provide more negative feedback than\nnovices (Finkelstein and Fishbach, 2012) in order to help companies develop\ntheir product, and as AutoTrader specialises in B2B trading with sector ex-\nperts it is not unexpected that their customer feedback also reﬂects this.\nAlso noted in the data are notes with non-negative sentiment. Neutral\nsentiment notes tended to be created by feedback which was purely informa-\ntional with no information on customer mood shared, for example:\n“he had his 3 lads running around his feet and his wife was doing the big\nshop asked i call back either this afternoon or monday.”\n9\nFigure 3: Bar chart showing distribution of calculated sentiment.\nwhilst on analysis the extreme positive notes appeared to be mostly single\nsentence with a focus on a single positive topic, for example:\n“conﬁrmed the communication with him he is happy with what we have\nmentioned”\nThough not seen as statistically signiﬁcant, it is noted that longer multiple\nsentence notes produced fewer extreme scores due to the average sentiment\nbeing taken from multiple diﬀerent topic sentences.\nReviewing the change in note sentiment over time (ﬁgure 4) also revealed\nsome interesting trends in the data. We can observe spikes in the relative\nnote sentiment during April – July 2020 and in January – February 2021,\ncoinciding with the full lockdown dates for the UK. This increase in sen-\ntiment correlated with a popular policy from the company. Reﬂecting the\n10\noverall sentiment distribution, the general baseline sentiment value over time\nis negative, between -0.80 and -0.65. This overall negativity is not an issue\nhowever as the changing sentiment over time was the focus of the research.\nThe interest can be placed instead on the relative sentiment values with re-\nspect to the baseline when assessing for the outcomes of any further policy\nchanges aﬀecting customers.\nFigure 4: Chart displaying the change in sentiment over time.\nThe calculated sentiment when compared to the annotated sentiment was\nfar more negative. Using a pairwise comparison between the two the model\nachieves an accuracy score of 24% when compared against all ﬁve categories.\nWhen compared for three categories, (positive, negative and neutral) there\nis an agreement of 60%.\n3.4. Recommendations\nThere are a few suggestions that we have for anyone looking to perform\nsimilar analysis.\n1. Look to reduce granularity wherever possible to speed up analysis. The\nanalysis done here can take hours of compute time to calculate so the\nmore operations that can be performed on the entire dataset the more\n11\ntime can be saved. Be aware that this cannot be done for some opera-\ntions.\n2. Be cautious of the reserved language and reduced punctuation present\nin B2B data.\nThis could see worse performance for models trained\non B2C data when predicting sentiment.\nOur dataset is examining\nfeedback so be aware and check with sector experts as to what to expect\nfrom your dataset.\n3. Look at diﬀerent tokenisation levels such as sentences paragraphs or\nfull documents. Higher level analysis will return neutral sentiment on\na more frequent basis. When we examined sentiment at a whole note\nlevel the longer notes tended towards neutrality.\n4. Examine analogous labels to your subject area, this will better allow\nyou to compare results. So the note data here is best compared to\ncustomer review data. It would be better compared to B2B reviews\nbut these are not publicly accessible.\n3.5. Conclusion\nIn conclusion we have demonstrated a robust method for assessing the\nsentiment of the AutoTrader note library utilising a deep learning based\napproach with the HuggingFace library. Conﬁrmation from the AutoTrader\nteam that the general trends seen within the data ﬁt with expected outcomes\nof policy changes implemented by the company also acted as a sanity check\nfor the eﬃcacy with their data.\nIt is noted that a large portion of the notes were seen to be extremely\nnegative with the note set, which despite justiﬁcation of the outcome does\ncause some concern. As the focus for the analysis is for change in sentiment\nhowever this may not prove an issue as relative sentiment is considered for\ndrawing conclusions.\nThe comparison to the annotated data shows that\nthe implemented method has a tendency to misjudge the strength of the\nsentiment within a message.\nThis could be due to the AutoTrader data\ncontaining unfamiliar terms. The other reason could be that the removal\nof personal identiﬁable data could have left sentences without subjects to\nattribute sentiment to. This model could be more robustly calculated with\nthe use of more than one annotator to verify the reliability of the annotations.\nTransfer learning (Weiss and Khoshgoftaar, 2016) could be used to allow the\nmodel to understand the data better.\n12\n4. Topic modelling utilising Latent Dirichlet Allocation\n4.1. Introduction\nIf the notes could be sorted automatically into relevant categories it would\nbe easier to analyse issues in diﬀerent areas. Issues are raised anecdotally\nfrom team members collecting the notes and evidence is searched for within\nthe data using sector expert derived keywords. This approach can lead to\nissues within identifying target areas for policy improvements, in particular\n“ﬁreﬁghting” where only big issues and complaints are dealt with as they\ngrab the most attention, while smaller issues that may be more prevalent can\nremain untouched. Additionally, even issues that have been identiﬁed may\nbe missed if customer feedback cannot be found due to the use of incorrect\nkeywords during evidence searches. Approaches to tackle this issue have been\nundertaken such as adding selective hashtags to key words in customer note\ndata and initiating topic check boxes but these have so far been unsuccessful.\nTopic modelling can provide a diﬀerent approach to the problem for Au-\ntoTrader. It utilises a data driven approach where the topics of interest are\nderived from the data itself rather than expert led anecdotal evidence which\nmay contain unintended bias. Topic modelling however is not ﬂawless as\nhuman interaction is still required to make sense of the topics suggested by\nthe chosen methodology, but this may present itself as an additional oppor-\ntunity. Human interaction with the algorithm will ensure investment from\nthe interested parties and can be used to sense check the model during train-\ning rather than ﬁnding the results are faulty at the analysis stage. For this\nsection we shall demonstrate the use of LDA topic modelling with the Auto-\nTrader note data, with a view to demonstrate the types of note data that can\nbe extracted and how the data can be merged with the sentiment analysis\nresults from section three to be used for further analysis.\n4.2. Methodology\nThe data used here was the same as in section three with the additional\nsentiment analysis added to the data. Additional data preparation was per-\nformed via technical term extraction and joining into n-grams, using the\nfollowing ﬂow process outlined in ﬁgure 5, with a step-by-step breakdown\ndetailed below.\n1. Load dataset\n(a) Dataset is loaded into the notebook as a pandas data frame\n13\nFigure 5: Flow diagram of the technical term extraction process.\n2. Load data sample\n(a) Loop established to cycle through each text in the loaded dataset\nin turn through steps 3 and 4\n3. Tokenize and Part-of-Speech (POS) tag samples\n(a) Notes are tokenized at the word level\n(b) Each token is POS tagged with a semantic label using the nltk\nPOS tagging library (Bird et al, 2019)\n4. Pass sample through semantic ﬁlter to identify n-grams\n(a) Samples are passed through the technical term semantic ﬁlter de-\nﬁned by Justedon and Katz (1995)\n(b) Technical terms limited to bi-grams and tri-grams as longer terms\nare typically only associated with highly technical ﬁelds (Justedon\nand Katz, 1995)\n5. Assess collected technical terms and generate stoplist\n(a) Top 100 most frequent terms manually assessed and common ter-\nminology phrases removed\n(b) Top 100 most frequent terms reassessed after stoplist terms re-\nmoved and additional identiﬁed stop terms removed\n(c) Process repeated until top 100 phrases clear of common terminol-\nogy phrases\n(d) Process repeated until top 100 phrases clear of common terminol-\nogy phrases - c-score weightings of the technical terms calculated\nusing the method deﬁned by Frantzi et al. (2000)\n6. Aﬃx technical term words in dataset and replace\n(a) All technical term tokens within the list adjoined with an un-\nderscore to create a single token (e.g.\n“new”, “cars” becomes\n“new cars”)\n(b) Technical terms individual tokens removed from the dataset\n14\nFigure 6 shows the process by which the topic modelling is done. Again\nthe ﬂow chart is explained in more detail below.\nFigure 6: Flow diagram of the topic modelling process.\n1. Load dataset\n(a) Dataset is loaded into the notebook as a pandas data frame\n2. Tokenization, lemmatization and stopword removal\n(a) Each note in the corpus is run through in turn\n(b) Tokenization is performed at the word level using the gensim.simple preprocess\nlibrary in python (ˇReh˚uˇrek, 2022)\n(c) Lemmatization of tokens is performed using the nltk WordNetLem-\nmatizer library (NLTK Project, 2023)\n(d) Stopwords are removed from each note using the nltk library stop-\nword corpus (NLTK Project, 2023)\n3. Dictionary creation and ﬁltering\n(a) A dictionary is created from the entire pre-processed corpus using\nthe gensim.corpora dictionary library (ˇReh˚uˇrek, 2022)\n(b) Once created the dictionary is ﬁltered to remove noise from ex-\ntreme case tokens\ni. Tokens featured in 5 or less notes are removed\nii. Top 100,000 used terms are reserved\niii. Limit of the number of texts a token can feature in is also used\nin the dictionary ﬁlter which is tuned by the expert operator\nas a hyperparameter\n4. BOW corpus creation\n(a) Each note is compared against the dictionary to create a BOW\nnote using the gensim.corpora dictionary library doc2bow feature\n(ˇReh˚uˇrek, 2022) and tokens in the dictionary are retained and\ntheir frequency recorded\n5. LDA modelling and hyperparameter tuning\n15\n(a) LDA model is generated using gensim.models.ldamulticore library\n(ˇReh˚uˇrek, 2022) using the created dictionary and BOW corpus as\ninputs\n(b) Hyperparameters tuned as follows\ni. Number of passes set to ten (experimentation showed consis-\ntent convergence of the model at this number)\nii. Number of topics to be tuned by an expert user assessing the\nresultant topics for sense\niii. Minimum probability level for a note to be contained within\na topic to be tuned by an expert user assessing the resultant\ntopics for sense\n4.3. Results\nA list of the top 25 technical term n-grams identiﬁed from the AutoTrader\ncorpus is shown in Figure 7. Noted from the results is that bi-grams dominate\nthe list, justifying the decision to only look for bi and tri-grams within the\ncorpus. The only tri-gram within the list is “end-of-April”, which despite\nbeing a generic date based term holds weight as this is the typical annual date\nof a customer based policy change and is considered the end of the ﬁnancial\nyear. Also observed is that the word frequency and weight (c-score) drop oﬀ\nsigniﬁcantly after the ﬁrst 4 n-grams on the list, indicating that the set of\nnotes analysed are potentially dominated by topics revolving around those\nterms. In fact, the weighting for the third term on the list (“price-indicator”)\nappears to have been dampened as the same term is referred to in several\ndiﬀerent variations (“price-ﬂag”, “price-indicators”, “price-ﬂag”, “pi-ﬂags”),\nwhich, if combined, would signiﬁcantly increase the weighting of the term.\nThis highlights a potential process improvement for data collection where\nterms with the same meaning referred to in diﬀerent ways can be combined\nin the initial cleaning stage of note processing. If this is done before running\nthrough the sentiment and topic analysis algorithms the analysis would be\nbetter sorted by toipcs.\nAs previously described the topic modelling hyperparameter tuning was\nperformed alongside a sector expert from AutoTrader to ensure sensible re-\nsults. Table 1 shows the ﬁnal satisfactory set of identiﬁed topics from the\ncorpus. 10 topics were identiﬁed using a heavy dictionary ﬁlter with no terms\nfeatured in more than 5% of the notes included and a minimum probability\nlevel of 0.005% of being within a topic. Feedback from the AutoTrader team\nindicated that most of the topics identiﬁed showed consistent language and\n16\nFigure 7: Identiﬁed technical term n-grams identiﬁed from the Autotrader note corpus.\nWeights denote the c-score given to each of the terms.\ncould be classiﬁed as relevant topics within the business (labelled as such in\ntable 1), with only one of the topics being deemed as nonsense (labelled as\n“no recognised subject”). Also identiﬁed by the AutoTrader team were two\ntopics not expected to be within the notes, namely “live chat” and “package”,\nwhich would not have searched for using the old knowledge led approach.\nIn addition to assessing the topic modelling in isolation the results were\ncombined with the earlier sentiment analysis works to assess the variation\nin sentiment amongst the identiﬁed topics. Figure 8 shows the distributions\nof the mean average note sentiment over each of the identiﬁed topics within\nthe note corpus.\nAnalysis shows that each of the topics follows the gen-\neral trend of the overall note sentiment identiﬁed in section 3.3, with the\nnotes showing a general negative sentiment skew with a long tail and outliers\npresent for extreme positive sentiment. From the chart we can see that the\n“price indicator ﬂags” and “live chat” topics show a higher median sentiment\nvalue and wider distribution towards the positive than the other identiﬁed\ntopic. These signiﬁcant diﬀerences would indicate that these topics warrant\nfurther investigation to assess why their average sentiment diﬀers from the\nunderlying population. Their higher average sentiments are due to the same\npositive sentiment spike as seen in section three and correlate with the ﬁrst\n17\nHigh Probability Words\nSuggested\ntopic\n0.018*”quote” + 0.013*”ebay” + 0.010*”ﬁnance” + 0.009*”pre-\nmium” + 0.009*”level” + 0.008*”performance” + 0.007*”new car”\n+ 0.007*”expensive” + 0.007*”struggle” + 0.006*”advance”\npackage\n0.019*”data”\n+\n0.015*”ﬂag”\n+\n0.013*”meet”\n+\n0.011*”re-\ntail check” + 0.010*”price indicator” + 0.010*”spec” + 0.008*”sit”\n+ 0.008*”valuations” + 0.007*”group” + 0.007*”price ﬂags”\nprice\nindi-\ncator ﬂags\n0.012*”request”\n+\n0.012*”admin fees”\n+\n0.011*”video”\n+\n0.007*”ﬁnd” + 0.007*”image” + 0.006*”actually” + 0.006*”query”\n+ 0.006*”frustrate” + 0.005*”spec” + 0.005*”unhappy”\nunhappy\n0.017*”image” + 0.013*”rat” + 0.012*”new car” + 0.010*”highly”\n+ 0.009*”upload” + 0.008*”reply” + 0.008*”award” + 0.007*”con-\nsumers” + 0.006*”info” + 0.006*”message”\nlive chat\n0.012*”text”\n+\n0.011*”valuations”\n+\n0.011*”product”\n+\n0.010*”chat”\n+\n0.009*”lose”\n+\n0.007*”tech”\n+\n0.007*”mar-\ngin” + 0.006*”platform” + 0.006*”retail” + 0.006*”higher”\nvaluations\n0.010*”retract” + 0.008*”close” + 0.008*”open” + 0.007*”watch”\n+\n0.007*”webinar”\n+\n0.006*”book”\n+\n0.006*”phone”\n+\n0.006*”process” + 0.006*”answer” + 0.006*”charge”\nprocess re-\nlated\n0.011*”staﬀ”\n+\n0.010*”coronavirus”\n+\n0.010*”reduce”\n+\n0.010*”lockdown” + 0.009*”canx” + 0.009*”plan” + 0.008*”strug-\ngle” + 0.008*”online” + 0.008*”june” + 0.008*”continue”\ncoronavirus\n0.016*”lockdown”\n+\n0.010*”june”\n+\n0.010*”collect”\n+\n0.008*”open” + 0.008*”retract” + 0.007*”confuse” + 0.007*”fol-\nlow” + 0.007*”aware” + 0.007*”extend” + 0.007*”appreciate”\nlockdown\nextensions\n0.061*”xxxemailxxx”\n+\n0.023*”subject”\n+\n0.015*”group”\n+\n0.013*”kind”\n+\n0.009*”xxxtelephonexxx”\n+\n0.008*”sit”\n+\n0.008*”retail” + 0.007*”lead” + 0.007*”manheim” + 0.006*”op-\ntion”\nno\nrecog-\nnised\nsubject\n0.027*”year”\n+\n0.016*”experian”\n+\n0.008*”car gurus”\n+\n0.007*”ebay” + 0.006*”meet” + 0.006*”zuto” + 0.006*”july”\n+ 0.005*”achieve” + 0.005*”award” + 0.005*”normal”\nrival valua-\ntion\nprod-\nucts\nTable 1: Topic modelling results from the AutoTrader note corpus, with sector expert led\ntopic naming suggestions.\n18\ncoronavirus lockdown.\nFigure 8: Mean average note sentiment distribution for the unidentiﬁed topics within the\nAutoTrader dataset.\n4.4. Recommendations\nThere are also suggestions that we would have when examining topic\nmodelling for B2B data.\n1. The ﬁrst is that where possible use the bespoke knowledge of sector\nexperts. They will likely have a great understanding of acronyms and\nkeywords. They will also understand the context behind topics and\nhow relevant they are to the area of study.\n2. Beware possible bias from experts on certain topics, let the data guide\nthe overall analysis. The experts may already have decided their view\non a topic or have a vested interest in the sentiment analysis of a topic.\nThe underlying data will inform as to whether that view is justiﬁable\nor not.\n3. Use harsh dictionary ﬁlters to remove terms that are too frequent.\nThese terms can distort the desired keyword output. This also removes\na greater amount of stop words.\n19\n4. Apply advanced lemmatisation for the relevant area as well as technical\nN-gram creation which will be required to examine speciﬁc sector terms.\nThis will allow for the adaptation to the terminology and technical\nlanguage found within the dataset that you are working with.\n4.5. Conclusion\nIn conclusion we have demonstrated a robust repeatable method for topic\nmodelling using the AutoTrader note data. The sector specialist input during\nthe method hyperparameter tuning allows for an expert knowledge sense\ncheck of the notes before further analysis and dissemination to a wider team.\nFeedback from the AutoTrader team noted that the method revealed several\ntechnical terms that were previously missed in the topics using their previous\nexpert led technical term search. Additionally, the topic modelling was noted\nto produce two additional topics that were relevant but not expected to be\nseen within the content of the notes provided.\n5. Topic modelling utilising Keyword Extraction\n5.1. Introduction\nThe last section looked at topic modelling using LDA. The problem with\nLDA was that it provided little information as to why the topic was chosen or\nthe subject of the topic. The lack of explanation for the topics is detrimental\nto understanding them. The ﬁrst steps of this work are dedicated to ﬁnd\na solution to this problem and apply NLP techniques to help describe the\ncontext of identiﬁed topics by LDA. The data was cleaned in the same way\nas the last section but now we had access to live note data which required\nsome more cleaning. For this reason, additional pre-processing steps were\nadded based on a heuristic knowledge obtained from an exploration of the\ndata. These steps were:\n1. Remove new set of tokens that was identiﬁed by data exploration, e.g.\n‘—original message—’\n2. Remove notes shorter than 20 characters assumed to be accidental or\nnot intended to be a note, e.g. ‘insert text’, ‘test’, ‘hjbyhkjbkh’\n3. Remove set of special characters resulting from wrong parsing, e.g.\nsingle characters appearing as ‘ˆaCTM’\n20\nThe ﬁrst step taken to provide more information about the notes, was to\nwork on the topics provided with the use of the LDA analysis. The LDA\nidentiﬁed topics within notes and grouped the notes into these topics, but\nassigned them no description or name. For a user, this means that they are\npresented with a topic, some group of notes, and they are left to ﬁnd the\nmeaning of it themselves. This could be enhanced by using the approach of\nKE on the topics, as keywords should best describe the subject of the text in\nthese topics. Extracting the most important words within the topic should\ngive information about the topic as a whole. Main challenges rising from the\nAutoTrader dataset for KE are considered to be :\n1. Dataset obtains large volume of notes, requiring an eﬃcient algorithm\nthat would practically run quickly\n2. Language and jargon of the dataset is unique to the automotive in-\ndustry, making it even more necessary for the algorithm to be able to\nencapsulate the subject and meaning notes have\n3. Notes vary in text range, with some of them containing only the im-\nportant information from a conversation, approach based purely on\nstatistical appearance may not be enough to cope with the lack of text.\nWe compare RAKE and KeyBERT. The chosen way of evaluating the\nKE is with feedback from sector experts.\n5.2. Results\n5.2.1. RAKE and KeyBERT applied to LDA topics\nThe ﬁrst method applied in the project was RAKE, with its potential to\nquickly and eﬃciently deliver keywords. LDA was executed again in the same\nway as the last section, to generate topics. To prepare pre-processed data for\nRAKE, lemmatised words of each note were joined into single strings. RAKE\nwas run on these strings for each topic to generate keywords for them. To\nevaluate the keywords, the following action was taken:\n1. General sanity check of keywords by answering questions:\n(a) Are keywords short and brief?\n(b) Are keywords readable and understandable?\n2. Generate word cloud graphs for the topics\n3. Compare the words of word clouds with their relative keywords\n21\n4. Together with members of the AutoTrader team, compare word cloud\ngraphs to the extracted keywords and consider their meanings in re-\ngards to the automotive industry\nFor the ﬁrst point, RAKE extracted extensively long keyphrases, which\nwere too long to be easily readable. For further evaluation to make sense, the\nlength of keywords was limited to maximum of ten words. With now shorter\nkeywords, they were made up of lemmatised words from the pre-processing,\nforming a set of ten words that together do not create a grammatically con-\nsistent sentence. However, some understanding can be achieved by ﬁnding\ncorrelations between the words of the keyphrases. As an example, in Figure\n9, RAKE identiﬁed the most important keyphrase containing words ‘issue\nupload video go online’, which could translate to the customers having issues\nwith uploading videos online on AutoTrader’s web page. When the word\nclouds were generated, the graph’s and keyphrases’ words were compared.\nWhen comparing each word cloud with its related keyphrases of a topic,\nit seemed that the keyphrases only sometimes included words contained in\nword clouds and when they did, normally a single keyphrase included had\nonly one word from the word cloud. When comparing the word cloud to the\nkeyphrase pairs among each other for diﬀerent topics, they tended to be sim-\nilar in information, suggesting that topics, and therefore their keyphrases as\nwell, were broad and inconsistent with each other within topic, not bringing\nany new speciﬁc information to the topics.\nAfterwards, the group of sector experts were presented with a set of word\ncloud and keyword pairs. An example of this is shown in Figure 9 for topic\nnumber 2. A set of words may be useful to get a sense of what the subject is\nabout, but it was not practical enough to easily bring insight into the data,\nleaving major portion of the work to be still done by hand. The foremost\nissue needed tackling was agreed to be the need of the keywords to be more\ncomprehensive, intelligible and a subject of a topic should be more easily\nidentiﬁed from them.\nTo provide more comprehensible keywords, the next approach was to\napply KeyBERT. The idea is that embeddings from BERT should provide\ndeeper understanding of notes, although requiring more computational re-\nsources. The implementation of KeyBERT takes as an input one whole text,\nwhich meant the preprocessed data had to be formed into a single text.\nFirstly, sentences were formed by joining words into sentences, separating\nwords with blank spaces. Secondly, these sentences were separated by dots\n22\nFigure 9: Word cloud - RAKE keyphrases on topic 2.\nand joined into a single text. All notes that were chosen to be together in\na single text, belonged under the same topic group identiﬁed by the LDA.\nHence, KeyBERT was run on text, that consisted of lemmatised words from\nnotes, separating notes by dots, as if they were sentences, for each of the\ntopics. The parameters of KeyBERT were selected to tackle the issues which\narose from RAKE. To have keyphrases contain a lower number of words, the\nn-gram range was set to be from 2 to 3, expecting keyphrases to have either\n2 or 3 words. The ‘top n’ number of keyphrases was left with the default\noption on top 5. The keyphrases evaluation was similar to the one of RAKE,\nhaving RAKE’s results as a base line:\n1. General sanity check of keywords by answering questions:\n(a) Are keywords short and brief?\n(b) Are keywords readable and understandable?\n(c) How do they compare to RAKE’s results?\n2. Generate word cloud graphs for the topics\n3. Set the words contained in word clouds side by side with their relative\nkeywords and compare results to RAKE\n4. Repeat the previous step with members of the AutoTrader team, weight\nif the issues were tackled and consider future options\nKeyBERT extracted mostly 3 word n-grams with 2 word n-grams only\noccurring infrequently. Keyphrases consisted of combinations of words, that\ntogether seemed to have a grammatical structure as well as some meaning,\ne.g. ‘concerns around priceindicator’. Compared to RAKE, they are more\neasily interpretable and the information is quickly readable from them. After\n23\nFigure 10: Word cloud - KeyBERT keyphrases on topic 2.\nword clouds were created, there were instances when it was clear that the\nsubject was captured in the keyphrases.\nOn the other hand, keyphrases appeared to start with the same words\nwithin a topic.\nFurthermore, they seemed to be similar in word choice,\nsometimes the diﬀerence only being the order of the words. This similarity\nin keywords does not bring any new information about the context of the\ntopic. Compared to RAKE, however, it gives more interpretable and more\nuseful insight into the subject a topic could be about. Following that, the\nteam of experts was presented with the word cloud and keyphrase pairs, in\nthe same way as with RAKE. The expert feedback was that this approach\nwas an improvement on the the RAKE method but the pairs were still too\ndissimilar.\nIssues were noted in both variations of KE algorithms. With the lack of\nmeasurements of accuracy for KE algorithms on an unlabelled dataset, word\nclouds with clear meanings were needed to assess the generated keywords.\nHowever, this was not delivered by the LDA algorithm, as words in the word\nclouds did not always appeared to be related. A new suggestion was made to\ntry to ﬁnd a new way to categorise notes into topics. From experiments con-\nducted until that point, statistical approaches were seen as insuﬃcient when\ndealing with given dataset. Inspired by the better performance of BERT, the\nnew approach should capitalise on the semantic approach of BERT-based al-\ngorithms to identify topics.\n24\n5.2.2. Rake and KeyBERT applied to topics generated by K-means clustering\nThe topics provided by LDA were considered as the issue preventing\ncoherent keyword detection, since KE algorithms did not manage to return\nsane keywords consistently or keywords clearly speciﬁc to a topic. This could\nmean that topics identiﬁed by LDA were overlapping or it was not successful\nat identifying topics. AutoTrader note data is too varied and could be too\ndiﬃcult for the LDA to pick up on the topics and identify them correctly. The\nsolution decided on was to try more modern approaches to Topic Modelling\n(TM) with methods that can capture the meanings of notes and generate\ntrends based on them. As was in the case of KE, a BERT-based approach\nwas seen to be more successful than statistical methods.\nBased on that,\nthe next step was to try to get topics using BERT-based embeddings in\ncombination with K-Means clustering. A BERT-based sentence-transformer\nfrom HuggingFace library would be ﬁrst used to encode embeddings for the\ndata. The all-MiniLM-L6-v2 (Hugging Face, 2023) transformer model was\nchosen, because it was designed to map sentences to a 384 dimensional dense\nvector space and was meant for the application of clustering and semantic\nsearch. The embeddings would be then used to cluster them into topics by K-\nMeans clustering method. K-Means is frequently used in NLP clustering and\nprovides easily computable solution to clustering problems. The main issue\nwas the choice of K. To ﬁnd optimal way to determine K a rigorous research\nwas conducted. Keeping in mind the interactive goal of the dashboard, the\naim was to ﬁnd the least complicated solution. The most appropriate solution\nwas the ’rule of thumb’, as mentioned in Naeem and Wumaier (2018). It is\na heuristic approach that does not have a mathematical proof, but is still\npreferred by researchers. The ’rule of thumb’ formula for K is:\nK =\nrn\n2\n(1)\nwhere n is the number of data points. In our case, n is the size of the\nentire dataset. The aim of this approach was to get results that make more\nsense than when using LDA method. To compare it, same comparisons with\nword clouds would be made as when LDA was evaluated. This way a sanity\ncheck could be performed to evaluate the success of this approach.\nThis\nnew approach was run on a dataset used by the previous project, containing\n10544 notes. The number of topics was computed by the ’rule of thumb’ to\nbe 22. An example of a word cloud from identiﬁed generated topic is shown\n25\nin Figure 11, Both RAKE and KeyBERT were subsequently applied on the\ngenerated topics.\nFigure 11: Word cloud using K-Means of cluster 1.\nIn the case of RAKE, keyphrases again contained an abundance of words\nand the result had to be trimmed down to 10 words per keyphrase to allow us\nto make judgement. As for the evaluation of this approach, RAKE keyphrases\nobtained words that were judged to have more meaning by the expert panel\nas compared to the LDA approach. BERT-based clustering in combination\nwith RAKE seemed to produce more comprehensive keywords. Keyphrases\nstill included a lot of words, resembling the word clouds they were supposed\nto replace.\n5.3. Recommendations\n1. TM algorithms recognise a given number of topics. Although there\nexist methods to optimise the number of clusters, they are costly in\ncomputational resource. This optimisation would have to be run every\ntime new notes are added to the dataset. That happens on a daily\nbasis. So this method does not work well with live data. When TM\nis run twice on the same data, it generates topics that, although are\nsimilar, diﬀer. If TM was run on future datasets, it would create a new\n26\nset of topics that may not align with previously created topics. This\nwould make topic analysis over a long period of time impossible.\n2. Decide on an agreed objective or milestone before beginning TM anal-\nysis. Without a clear end goal the process of selecting new methods\nand reﬁning existing ones can continue indeﬁnitely. An appropriate\nobjective could be identifying the most prominent keywords associated\nwith the most important topics. The number of these should be agreed\nat the beginning of the analysis.\n5.4. Conclusion\nKeyBERT again proved to provide concise keyphrases. In contrast to\nRAKE, KeyBERT’s keyphrases were shorter and neatly formed to contain\nsome meaning. KeyBERT coupled with BERT-based clustering produced\nthe most sane keywords for the topics. It was hard to see, however, if the\nmain goal was achieved. The keywords merely seemed to contain speciﬁc\nwords from the word clouds, no new information was obtained.\nAfter the AutoTrader team was shown the word cloud - keyphrases pairs\nobtained by using sentence transformer with K-Means, it performed a sanity\ncheck same as was done previously. The team concluded that although there\nis stronger correlation between the word cloud with its keyphrases, there is\nno guarantee that they are actually correlated. Moreover, team stressed the\nlack of new insights into the data even when more modern approaches were\nused.\n6. Information retrieval\n6.1. Introduction\nWe then decided on a new approach and the next objective was to change\nthe goal itself. Instead of trying to generate topics, information in the data\ncould be retrieved. Information retrieval (IR) would allow users to search\nnotes and have data visualisation done. Semantic search is a method that\nprovides a NLP solution to the IR problem. It takes a query from the user\nand enables them to search for notes relating to the given query.\n6.2. Methodology\nDuring this project, BERT-based approaches had achieved the best re-\nsults and so it was again chosen to assist with IR. Semantic search could be\nexecuted by the use of embeddings from a BERT-based model. Embeddings\n27\nare high dimensional vectors that should capture the sentimental meaning of\na text. The more similar two vectors are, the more similar meanings of two\ntexts are. Vector similarity is attained with the use of cosine similarity func-\ntion. The main idea of this approach is to accomplish this task by executing\nthese steps:\n1. Use BERT-based model to encode embeddings for all notes\n2. Use the same BERT-based model to create an embedding for a given\nquery\n3. Compute cosine similarity between the query embedding and all note\nembeddings, this is named a similarity score, and assign embeddings\nto their respective notes\n4. Compile a list of notes based on ordering notes by their similarity from\nhighest to lowest.\nIn the previous approaches in this project, only qualitative analysis could\nbe performed without manually labelling thousands of notes. So to obtain\na quantitative analysis the labelling was required. Manually labelling notes\ncan take time and the input of sector experts is of great assistance in this\ntask. For the labelling the following approach was designed:\n1. With the help of the AutoTrader team, agree on 7 topics that could be\nqueried\n2. Create a labelling dataset by randomly selecting 1000 notes\n3. Manually label the dataset by noting 1/0 as to whether the note belongs\nto the topic, consulting the team to try to minimalise bias\n4. Use a ranking score to assess the ranking accuracy of this approach\nFor this task, the labelling dataset was constructed by randomly choosing\n1000 notes from the live dataset from year 2021. Admittedly, it was not a\nlarge dataset, but it would still help to create a certain sense of how this\napproach is behaving. Two annotators worked to label the dataset and one\nannotated 169 notes and the other annotated all 1000. Agreement between\nthe annotators was calculated using Cohen’s kappa (Cohen, 1960). The result\nwas 0.64 indicating substantial agreement. Topics agreed upon were:\n1. Valuation\n(a) Note contains feedback or valuation on AutoTrader’s services\n28\n(b) Considering that this AutoTrader dataset should consist mainly\nof feedback, it is used to distinguish notes that do not contain\nfeedback\n(c) Example text: feels like service is X years out of date\n2. Price\n(a) Note discusses a price of services or products, when in context of\nthe AutoTrader’s prices or the dealer’s prices\n(b) Example text: advanced to help with this by increasing his prices,\nPRODUCTNAME he loves but didn’t realise it was MONEY per\nmonth\n3. Package\n(a) Note discussing AutoTrader product package\n(b) Example text: PRODUCT-NAME not performing as well it he\nhoped to, PRODUCT-NAME he loves but didn’t realise it was\nMONEY per month\n4. Cancellation\n(a) Note considers cancelling a service or informs about cancelling it\n(b) Example text: process the cancellation to downgrade\n5. Stock\n(a) Stock in AutoTrader’s journal refers to a vehicle being sold on\ntheir website\n(b) Note talks about some stock\n(c) Example text: uses all auction sites to sell it as well\n6. Tech\n(a) Note mentions any of AutoTrader’s online services\n(b) Example text: gets an error when you click on ’see’, allows the\nupload for images only\n7. Billing\n(a) Note contains information about money transport or concerns\nabout it\n(b) Example text: at the moment not selling well but has to pay X\noutstanding money\nAs for the ranking score, a Normalised Discounted Cumulative Gain\n(NDCG) was chosen. NDCG provides a relatively easy to compute rank-\ning score. It is deemed to be a ”classic” information retrieval (IR) metric.\nThe goal of NDCG is to rank results from IR by comparing them to an ideal\n29\nrank ordering.\nNDCG is computed from the discounted cumulative gain\nDCG as deﬁned in Agrawal et al. (2009).The process of evaluation was as\nfollows:\n1. Let us have a query Q belonging to one of the topics\n2. Query the note corpus with Q\n3. Provide NDCG scores for all topics\nThe idea behind this process is that if the query Q belongs to a topic,\nNDCG score for this topic should be higher. If the query Q does not belong\nto a topic, it should drop. As the querying process returns an ordered list\nof notes with similarity scores, similarity scores can be compared to 1s and\n0s from the labelling dataset to achieve NDCG score. This way, the NDCG\nscore compares the created similarity scores to an ideal ranking. The result\nof this are NDCG scores, which can be compared to a baseline. This baseline\nis that scores for all notes are 0.5. Another analysis could be performed to\nsee the impact of pre-processing on the NDCG scores. To see the impact of\nthis, the same NDCG analysis is performed on pre-processed notes, as well\nas original notes. Computed NDCG scores for them are shown in table 2 and\ntable 3.\nClean\nBaseline\nValuation\n0.97\nPrice\n0.76\nPackage\n0.78\nCancellation\n0.63\nStock\n0.68\nTech\n0.86\nBilling\n0.56\nTable 2:\nBaseline NDCG evaluations for the clean data.\nFrom observing the Baseline it is clear, that topics are not equally repre-\nsented in the labelled dataset. Because all values are 0.5, higher NDCG for\nvaluation means that there is higher volume of 0.5 data points. There is not\ndiﬀerence between clean and pre-processed data.\nHuggingFace provides many models for the task of semantic search. A\nset of them was picked to be analysed. The 768 dimensional vector models\n30\nPre-processed\nBaseline\nValuation\n0.97\nPrice\n0.76\nPackage\n0.78\nCancellation\n0.63\nStock\n0.68\nTech\n0.86\nBilling\n0.57\nTable 3:\nBaseline NDCG evaluations for the pre-processed data.\non which an analysis was performed were compared and the multi-qa-mpnet-\nbase-dotv1 (Hugging Face, 2023) model was chosen to be implemented. Only\nthe pre-processed notes are examined moving forward.\n6.3. Results\nAcross all models the valuation scored highly, similar to the baseline\nmodels this is due to the notes all containing the feedback ﬂag from the\nAutoTrader team. The model was further tested with the use of queries.\nThese queries were designed to test the model and provided a use case for\nhow the queries could be used in the future. The ﬁrst query was ‘tech issue’.\nThis was expected to result in a higher than baseline score for the tech\ncategory. Table 4 shows the scores for each topic given this query:\nTopic\nScore\nDiﬀerence from baseline\nValuation\n0.96\n-0.01\nPrice\n0.72\n-0.04\nPackage\n0.74\n-0.04\nCancellation\n0.60\n-0.03\nStock\n0.67\n-0.01\nTech\n0.92\n+0.06\nBilling\n0.55\n-0.02\nTable 4: NDCG evaluations for the query “tech issue” on the pre-processed data.\nAs expected the query resulted in a higher score for tech. All other scores\ndecreased showing that the query was less relevant to those topics.\nThe\nnext query given was “too expensive”. This was anticipated to increase the\nrelevance of the price topic. The result is given below in table 5:\n31\nTopic\nScore\nDiﬀerence from baseline\nValuation\n0.97\n0.00\nPrice\n0.86\n+0.10\nPackage\n0.79\n+0.01\nCancellation\n0.66\n+0.03\nStock\n0.68\n0.00\nTech\n0.82\n-0.04\nBilling\n0.54\n-0.03\nTable 5: NDCG evaluations for the query “too expensive” on the pre-processed data.\nThese values show an increased score for price as expected but also have\nhigher then baseline values for package and cancellation. This could be inter-\npreted that packages that are too expensive can lead to cancellations. This\nis a logic assessment of how AutoTrader’s customers show their feedback to\nhigh prices. This method of inputting queries can demonstrate the relevant\ntopics to the inquirer. The last query tested was ”send money”, this query\nshould highlight the billing topic and also may highlight the price topic.\nTopic\nScore\nDiﬀerence from baseline\nValuation\n0.96\n-0.01\nPrice\n0.75\n-0.01\nPackage\n0.74\n-0.04\nCancellation\n0.65\n-0.02\nStock\n0.64\n-0.04\nTech\n0.84\n-0.02\nBilling\n0.68\n+0.11\nTable 6: NDCG evaluations for the query “send money” on the pre-processed data.\nThe result here shows that the billing topic does increase in relevance\nsigniﬁcantly compared to the previous queries and the baseline. The other\ntopics are at values decrease in relevance compared to the baseline showing\nthat the billing topic may have a distinct set of notes that are only pertinent\nto the paying of bills.\nThe current approach enables the user to order the notes based on the\nsimilarity to a given query. This list includes all notes. Should the user\nbe presented with analysis based on given query, a subset of notes has to be\n32\ncreated from this list. The subset would include notes that are more similar to\nthe query than notes that are not included in the subset. Graphical analysis\nwould then be done on notes from the subset.\n6.4. Recommendations\nWe also have recommendations for data retrieval.\n1. Get a number of sector experts to annotate if possible and compare\nthe annotations, then reject annotators if they vary too much from\nthe standard. This method increases the reliability of annotators and\nremoves the possibility of one annotator from being indistinguishable\nfrom the ground truth. If they are sector experts then they will better\nunderstand the context behind the conversations.\n2. Use a variety of models and then select the best performer. For this\npaper we examined ﬁve diﬀerent models before selecting the best per-\nformer. If only one method is used then the performance of the analysis\ncould be worse oﬀthan if more were ﬁrst evaluated.\n3. Try to avoid having a large overlap between evaluation topics. This\ncan lead to situations where topics such as billing are almost a subset\nof the price topic. Splitting the set of notes into separate categories is\neasier with diverse topics. Topics should also be limited in scope and\nif too many notes are within one topic then the topic deﬁnition needs\nto be more narrowly deﬁned.\n6.5. Conclusion\nInformation retrieval using topics decided by experts has provided better\nresults than with topics derived from topic modelling. The manually labelled\ndata allowed for a veriﬁcation of the similarity metric. The queries allow\nthe investigator to ﬁnd notes similar to the areas of investigation. Unlike\nthe topic modelling work the results returned here could be replicated. The\nanalysis also is fast enough to be updated each day for the relevant data. The\nsimilarity threshold allows for a relevant number of notes to be analysed.\n7. Conclusion and future work\nThis paper has looked at the analysis of business to business data with\nnatural language processing. Sentiment analysis, topic modelling and infor-\nmation retrieval are applied to the data. The sentiment analysis work allowed\n33\nthe data to be analysed automatically and can help AutoTrader by reduc-\ning the time spent on this task. The approach is transferable and could be\nused to automatically analyse other note data in diﬀerent areas. The topic\nmodelling did not manage to achieve the results desired and lacked clarity.\nInformational retrieval worked more eﬀectively.\nThe work struggled with the unlabelled nature of the notes and the mask-\ning of words in the sentences further removed information that would have\nbeen helpful for analysis. The annotated data could be expanded upon with\nmore annotators and a larger number of notes studied. Recently, Large Lan-\nguage Models (LLMs) have an increased proﬁle in the area of NLP (Qin et\nal., 2023). Further work could be done to train a LLM on the notes studied\nhere. This model could then be prompted to answer questions about topics\nwithin the notes.\nReferences\nAgrawal, R., Gollapudi, S., Halverson, A., & Ieong, S. (2009, February). Di-\nversifying search results. In Proceedings of the second ACM international\nconference on web search and data mining (pp. 5-14).\nAutoTrader.\n(2023).\nAbout\nus.\nRetrieved\nfrom\nhttps://plc.autotrader.co.uk/who-we-are/about-us/.\nAccessed\nFebru-\nary 16, 2023\nAutoTrader.\n(2023).\nAbout\nAutoTrader.\nRetrieved\nfrom\nhttps://plc.autotrader.co.uk/. Accessed February 16, 2023\nCuriskis, S. A., Drake, B., Osborn, T. R., & Kennedy, P. J. (2020). An\nevaluation of document clustering and topic modelling in two online social\nnetworks: Twitter and Reddit. Information Processing & Management,\n57(2), 102034.\nFrantzi, K., Ananiadou, S., & Mima, H. (2000). Automatic recognition of\nmulti-word terms:. the c-value/nc-value method. International journal on\ndigital libraries, 3, 115-130.\nHugging\nFace.\n(2023).\nsentence-transformers/all-MiniLM-L6-v2\n..\nRe-\ntrieved from https://huggingface.co/sentence-transformers/all-MiniLM-\nL6-v2. Accessed February 28, 2023\n34\nHugging Face. (2023). sentence-transformers/multi-qa-mpnet-base-dot-v1.\nRetrieved\nfrom\nhttps://huggingface.co/sentence-transformers/multi-qa-\nmpnet-base-dot-v1\nJuhn, Y., & Liu, H. (2020). Artiﬁcial intelligence approaches using natural\nlanguage processing to advance EHR-based clinical research. Journal of\nAllergy and Clinical Immunology, 145(2), 463-469.\nJusteson, J. S., & Katz, S. M. (1995). Technical terminology: some linguistic\nproperties and an algorithm for identiﬁcation in text. Natural language\nengineering, 1(1), 9-27.\nCohen, J. (1960). A coeﬃcient of agreement for nominal scales. Educational\nand psychological measurement, 20(1), 37-46.\nNaeem, S., & Wumaier, A. (2018). Study and implementing K-mean cluster-\ning algorithm on English text and techniques to ﬁnd the optimal value of\nK. International Journal of Computer Applications, 182(31), 7-14.\nNLTK\nProject.\n(2023).\nnltk.stem\npackage\nRetrieved\nfrom\nhttps://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer\nNLTK\nProject.\n(2023).\nNLTK\nCorpora\nRetrieved\nfrom\nhttps://www.nltk.org/nltk data/\nQin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., & Yang, D. (2023). Is\nChatGPT a General-Purpose Natural Language Processing Task Solver?.\narXiv preprint arXiv:2302.06476.\nRadim ˇReh˚uˇrek. (2022). utils – Various utility functions Retrieved from\nhttps://radimrehurek.com/gensim/utils.html. Accessed February 27, 2023\nRadim\nˇReh˚uˇrek.\n(2022).\ncorpora.dictionary\n–\nCon-\nstruct\nword\nto\nid\nmappings\nRetrieved\nfrom\nhttps://radimrehurek.com/gensim/corpora/dictionary.html.\nAccessed\nFebruary 27, 2023\nRadim\nˇReh˚uˇrek.\n(2022).\nmodels.ldamulticore\n–\npar-\nallelized\nLatent\nDirichlet\nAllocation\nRetrieved\nfrom\nhttps://radimrehurek.com/gensim/models/ldamulticore.html.\nAccessed\nFebruary 27, 2023\n35\nSanders, A. C., White, R. C., Severson, L. S., Ma, R., McQueen, R., Paulo, H.\nC. A., ... & Bennett, K. P. (2021). Unmasking the conversation on masks:\nNatural language processing for topical sentiment analysis of COVID-19\nTwitter discourse. AMIA Summits on Translational Science Proceedings,\n2021, 555.\nSheikhalishahi, S., Miotto, R., Dudley, J. T., Lavelli, A., Rinaldi, F., &\nOsmani, V. (2019). Natural language processing of clinical notes on chronic\ndiseases: systematic review. JMIR medical informatics, 7(2), e12239.\nOkon, E., Rachakonda, V., Hong, H. J., Callison-Burch, C., & Lipoﬀ, J.\nB. (2020). Natural language processing of Reddit data to evaluate der-\nmatology patient experiences and therapeutics. Journal of the American\nAcademy of Dermatology, 83(3), 803-808.\nLam, A. Y., Cheung, R., & Lau, M. M. (2013). The inﬂuence of internet-\nbased customer relationship management on customer loyalty. Contempo-\nrary management research, 9(4).\nAzzam, Z. A. M. (2014). The impact of customer relationship management on\ncustomer satisfaction in the banking industry–a case of Jordan. European\nJournal of Business and Management, 6(32), 99-112.\nMulyono, H., & Situmorang, S. H. (2018). E-CRM and loyalty: A mediation\nEﬀect of Customer Experience and satisfaction in online transportation of\nIndonesia. Academic journal of Economic studies, 4(3), 96-105.\nPhan, D. D., & Vogel, D. R. (2010). A model of customer relationship man-\nagement and business intelligence systems for catalogue and online retail-\ners. Information & management, 47(2), 69-77.\nIBM. (2023). What is Natural Language Processing?. Retrieved from\nhttps://www.ibm.com/topics/natural-language-processing.\nAccessed\nFebruary 20, 2023\nPestian, J., Nasrallah, H., Matykiewicz, P., Bennett, A., & Leenaars, A.\n(2010). Suicide note classiﬁcation using natural language processing: A\ncontent analysis. Biomedical informatics insights, 3, BII-S4706.\n36\nBra¸soveanu, A. M., & Andonie, R. (2020, September). Visualizing trans-\nformers for nlp: a brief survey. In 2020 24th International Conference\nInformation Visualisation (IV) (pp. 270-279). IEEE.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,\n... & Polosukhin, I. (2017). Attention is all you need. Advances in neural\ninformation processing systems, 30.\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V.\n(2019). Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... &\nRush, A. M. (2019). Huggingface’s transformers: State-of-the-art natural\nlanguage processing. arXiv preprint arXiv:1910.03771.\nHugging Face. (2023). The AI community building the future.. Retrieved from\nhttps://huggingface.co/. Accessed February 20, 2023\nKalyanathaya, K. P., Akila, D., & Rajesh, P. (2019). Advances in natural\nlanguage processing–a survey of current research trends, development tools\nand industry applications. International Journal of Recent Technology and\nEngineering, 7(5C), 199-202.\nKhan, S., & Rabbani, M. R. (2021). Artiﬁcial intelligence and NLP-based\nchatbot for islamic banking and ﬁnance. International Journal of Informa-\ntion Retrieval Research (IJIRR), 11(3), 65-77.\nSari, A. C., Virnilia, N., Susanto, J. T., Phiedono, K. A., & Hartono, T. K.\n(2020). Chatbot developments in the business world. Advances in Science,\nTechnology and Engineering Systems Journal, 5(6), 627-635.\nDing, Y., Ma, J., & Luo, X. (2022). Applications of natural language pro-\ncessing in construction. Automation in Construction, 136, 104169.\nM¨antyl¨a, M. V., Graziotin, D., & Kuutila, M. (2018). The evolution of senti-\nment analysis—A review of research topics, venues, and top cited papers.\nComputer Science Review, 27, 16-32.\n37\nKanakaraj, M., & Guddeti, R. M. R. (2015, February). Performance analysis\nof Ensemble methods on Twitter sentiment analysis using NLP techniques.\nIn Proceedings of the 2015 IEEE 9th international conference on semantic\ncomputing (IEEE ICSC 2015) (pp. 169-170). IEEE.\nHasan, M. R., Maliha, M., & Arifuzzaman, M. (2019, July). Sentiment\nanalysis with NLP on Twitter data. In 2019 international conference on\ncomputer, communication, chemical, materials and electronic engineering\n(IC4ME2) (pp. 1-4). IEEE.\nNasukawa, T., & Yi, J. (2003, October). Sentiment analysis: Capturing fa-\nvorability using natural language processing. In Proceedings of the 2nd\ninternational conference on Knowledge capture (pp. 70-77).\nMurphy, M., & Sashi, C. M. (2018). Communication, interactivity, and satis-\nfaction in B2B relationships. Industrial Marketing Management, 68, 1-12.\nAguilar, L., Downey, G., Krauss, R., Pardo, J., Lane, S., & Bolger, N. (2016).\nA dyadic perspective on speech accommodation and social connection:\nBoth partners’ rejection sensitivity matters. Journal of Personality, 84(2),\n165-177.\nFinkelstein, S. R., & Fishbach, A. (2012). Tell me what I did wrong: Experts\nseek and respond to negative feedback. Journal of Consumer Research,\n39(1), 22-38.\nWeiss, K., Khoshgoftaar, T. M., & Wang, D. (2016). A survey of transfer\nlearning. Journal of Big data, 3(1), 1-40.\nHofmann, T. (1999, August). Probabilistic latent semantic indexing. In Pro-\nceedings of the 22nd annual international ACM SIGIR conference on Re-\nsearch and development in information retrieval (pp. 50-57).\nHan, J., Kamber, M., & Pei, J. (2001). Data Mining: Concepts and. Tech-\nnology,, Mechanism Industrial Publishing, Company.\nGavval,\nR.,\nRavi,\nV.,\nHarshal,\nK. R.,\nGangwar,\nA.,\n& Ravi,\nK.\n(2019). CUDA-Self-Organizing feature map based visual sentiment anal-\nysis of bank customer complaints for Analytical CRM. arXiv preprint\narXiv:1905.09598.\n38\nBlei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation.\nJournal of machine Learning research, 3(Jan), 993-1022.\nJelodar, H., Wang, Y., Yuan, C., Feng, X., Jiang, X., Li, Y., & Zhao, L.\n(2019). Latent Dirichlet allocation (LDA) and topic modeling: models, ap-\nplications, a survey. Multimedia Tools and Applications, 78, 15169-15211.\nYang, Z., Yu, H., Tang, J., & Liu, H. (2019). Toward keyword extraction in\nconstrained information retrieval in vehicle social network. IEEE Transac-\ntions on Vehicular Technology, 68(5), 4285-4294.\nRose, S., Engel, D., Cramer, N., & Cowley, W. (2010). Automatic keyword\nextraction from individual documents. Text mining: applications and the-\nory, 1-20.\nSharma, P., & Li, Y. (2019). Self-supervised contextual keyword and\nkeyphrase retrieval with self-labelling.\nKodinariya, T. M., & Makwana, P. R. (2013). Review on determining number\nof Cluster in K-Means Clustering. International Journal, 1(6), 90-95.\nRao, S. X., Piriyatamwong, P., Ghoshal, P., Nasirian, S., de Salis, E.,\nMitrovi´c, S., ... & Zhang, C. (2022). Keyword Extraction in Scientiﬁc Doc-\numents. arXiv preprint arXiv:2207.01888.\nSteven Bird, Ewan Klein and Edward Loper 5. Categorizing and Tagging\nWords. Retrieved from https://www.nltk.org/book/ch05.html. Accessed\nFebruary 27,2023\n39\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-05-03",
  "updated": "2023-05-03"
}