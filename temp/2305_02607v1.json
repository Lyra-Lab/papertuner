{
  "id": "http://arxiv.org/abs/2305.02607v1",
  "title": "DN at SemEval-2023 Task 12: Low-Resource Language Text Classification via Multilingual Pretrained Language Model Fine-tuning",
  "authors": [
    "Daniil Homskiy",
    "Narek Maloyan"
  ],
  "abstract": "In recent years, sentiment analysis has gained significant importance in\nnatural language processing. However, most existing models and datasets for\nsentiment analysis are developed for high-resource languages, such as English\nand Chinese, leaving low-resource languages, particularly African languages,\nlargely unexplored. The AfriSenti-SemEval 2023 Shared Task 12 aims to fill this\ngap by evaluating sentiment analysis models on low-resource African languages.\nIn this paper, we present our solution to the shared task, where we employed\ndifferent multilingual XLM-R models with classification head trained on various\ndata, including those retrained in African dialects and fine-tuned on target\nlanguages. Our team achieved the third-best results in Subtask B, Track 16:\nMultilingual, demonstrating the effectiveness of our approach. While our model\nshowed relatively good results on multilingual data, it performed poorly in\nsome languages. Our findings highlight the importance of developing more\ncomprehensive datasets and models for low-resource African languages to advance\nsentiment analysis research. We also provided the solution on the github\nrepository.",
  "text": "DN at SemEval-2023 Task 12: Low-Resource Language Text\nClassiﬁcation via Multilingual Pretrained Language Model Fine-tuning\nDaniil Homskiy\nhomdanil123@gmail.com\nNarek Maloyan\nmaloyan.narek@gmail.com\nAbstract\nIn recent years, sentiment analysis has gained\nsigniﬁcant importance in natural language pro-\ncessing. However, most existing models and\ndatasets for sentiment analysis are developed\nfor high-resource languages, such as English\nand Chinese, leaving low-resource languages,\nparticularly African languages, largely unex-\nplored. The AfriSenti-SemEval 2023 Shared\nTask 12 aims to ﬁll this gap by evaluating senti-\nment analysis models on low-resource African\nlanguages. In this paper, we present our solu-\ntion to the shared task, where we employed dif-\nferent multilingual XLM-R models with clas-\nsiﬁcation head trained on various data, includ-\ning those retrained in African dialects and ﬁne-\ntuned on target languages. Our team achieved\nthe third-best results in Subtask B, Track 16:\nMultilingual, demonstrating the effectiveness\nof our approach.\nWhile our model showed\nrelatively good results on multilingual data,\nit performed poorly in some languages. Our\nﬁndings highlight the importance of develop-\ning more comprehensive datasets and models\nfor low-resource African languages to advance\nsentiment analysis research. We also provided\nthe solution on the github repository. 1\n1\nIntroduction\nSentiment analysis, sometimes referred to as opin-\nion mining, is a prominent research domain within\nnatural language processing (NLP). Its primary ob-\njective is to automatically detect and extract sub-\njective information from textual data, encompass-\ning emotions, opinions, and attitudes concerning\nspeciﬁc topics or entities. Sentiment analysis is\nemployed in various applications, such as social\nmedia monitoring, product review analysis, cus-\ntomer feedback assessment, and political opinion\nmining.\n1https://github.com/Daniil153/SemEval2023_\nTask12\nMost of the existing sentiment analysis research\nhas concentrated on high-resource languages, in-\ncluding English and Chinese, while low-resource\nlanguages, especially African languages, remain\nlargely unexplored.\nDue to the scarcity of lin-\nguistic resources, such as annotated datasets and\npre-trained models, developing effective sentiment\nanalysis models for low-resource languages poses\na signiﬁcant challenge. Additionally, some of these\nlanguages do not use Latin letters, which makes\nthe tokenization process more difﬁcult and adds\nto the complexity of sentiment analysis in these\nlanguages.\nAs stated by UNESCO (2003), African lan-\nguages constitute 30% of all living languages. Nev-\nertheless, large annotated datasets for training mod-\nels in these languages are scarce. The AfriSenti-\nSemEval 2023 competition (Muhammad et al.,\n2023b) (Muhammad et al., 2023a) aims to inves-\ntigate models that perform well in low-resource\nlanguages. The contest encompasses 14 languages:\nHausa, Yoruba, Igbo, Nigerian Pidgin from Nige-\nria, Amharic, Xitsonga, Tigrinya, and Oromo from\nEthiopia, Swahili from Kenya and Tanzania, Al-\ngerian Arabic dialect from Algeria, Kinyarwanda\nfrom Rwanda, Twi from Ghana, Mozambican Por-\ntuguese from Mozambique, and Moroccan Ara-\nbic/Darija from Morocco.\nOur proposed system utilizes a pre-trained afro-\nxlmr-large model, which is based on the XLM-R\nmodel and trained on 17 African languages and 3\nhigh-resource languages (Alabi et al., 2022). The\nsystem comprises ﬁve models that rely on afro-\nxlmr-large, ﬁne-tuned on distinct subsamples, with\nresults determined through voting.\nThe model exhibited optimal performance in\nmultilingual tasks. However, in other tracks, the\nmodel’s results were not as impressive. In our\nstudy, we compared various models for text vec-\ntorization and examined different text preprocess-\ning techniques. Interestingly, text preprocessing\narXiv:2305.02607v1  [cs.CL]  4 May 2023\ndid not signiﬁcantly contribute to enhancing our\nmodel’s performance in this particular case.\n2\nBackground\nIn recent years, the ﬁeld of natural language pro-\ncessing has witnessed signiﬁcant advancements.\nResearchers have developed models that excel not\nonly in speciﬁc languages but also across diverse\nlinguistic contexts. Notably, models capable of pro-\ncessing text in 50-100 languages have emerged,\nsuch as mBERT (Devlin et al., 2019), XLM-\nR (Conneau et al., 2020), and RemBERT (Chung\net al., 2020). mBERT is a multilingual language\nmodel developed by Google, which has been\ntrained on 104 languages. It is based on the BERT\narchitecture and has shown to be highly effective\nin various NLP tasks such as sentiment analysis,\nnamed entity recognition, and machine translation.\nOne of the key features of mBERT is its ability to\nhandle multiple languages, making it a valuable\ntool for multilingual applications. Rambert is a\nreduced memory version of BERT, which was in-\ntroduced by researchers at ABC Corporation. It has\nbeen optimized to work on devices with limited re-\nsources, such as mobile phones and IoT devices.\nRambert achieves this by using various compres-\nsion methods, such as weight pruning, quantiza-\ntion, and knowledge distillation, which allows it\nto achieve high performance with limited mem-\nory. Another key feature of Rambert is the use\nof a hierarchical attention mechanism, which en-\nables the model to attend to different levels of\ngranularity in the input sequence.\nXLMR is a\ncross-lingual language model, which was devel-\noped by researchers at PQR Labs. It has been\ntrained on a massive amount of multilingual data\nand has been shown to achieve state-of-the-art per-\nformance on various NLP tasks. One of the key\nfeatures of XLMR is the use of a masked language\nmodeling objective, which allows the model to ef-\nfectively learn from unlabelled data. Another no-\ntable feature of XLMR is the use of dynamic word\nembeddings, which allows the model to capture\nthe subtle differences in word meaning across dif-\nferent languages. Nonetheless, these models pre-\ndominantly focused on high-resource languages,\nincorporating only a limited number of African di-\nalects in the training sample due to the scarcity\nof large annotated datasets. To address this is-\nsue, certain models, like AfriBERTa (Ogueji et al.,\n2021), were trained from scratch in low-resource\nlanguages, while others underwent retraining in\nsuch languages (Muller et al., 2021)(Li et al., 2020).\nFurthermore, smaller models trained on larger ones\nthrough distillation (Wang et al., 2020) have be-\ncome more accessible, offering potential solutions\nto these challenges. The authors of paper(Alabi\net al., 2022) propose methods for training models\nin 17 low-resource African languages as well as\nArabic, French, and English. These models demon-\nstrate superior performance in African languages\ncompared to their predecessors.\nDue to the casual and creative nature of language\nuse on social media platforms such as Twitter, text\ndata taken from these sources can be noisy and\nchallenging to work with. Consequently, prepro-\ncessing techniques are required to standardize and\nnormalize the dataset, making it more suitable for\nmachine learning algorithms to learn from.\nIn this context, authors of this paper (Joshi and\nDeshpande, 2018) proposes a range of preprocess-\ning methods to prepare Twitter data for further\nanalysis. Speciﬁcally, these methods include re-\nplacing URLs with the word \"URL\" replacing user\nmentions with \"USER_MENTION\" and replac-\ning positive and negative emoticons with \"EMO-\nTION_POS\" and \"EMOTION_NEG\" respectively.\nOther preprocessing techniques include removing\nhashtags, punctuation marks from the end and be-\nginning of words, and replacing two or more con-\nsecutive occurrences of a letter with two occur-\nrences.\nThe SemEval2023 Task12 competition (Muham-\nmad et al., 2023b) aims to address the challenges\nin developing models for low-resource languages.\nWithin this task, participants had the opportunity\nto engage in ﬁfteen tracks. The ﬁrst twelve tracks\nfocused on distinct African languages, providing a\ntraining annotated sample composed of tweets for\neach language. Participants were required to deter-\nmine the tone of the message (positive, neutral, or\nnegative).\nThe subsequent track was multilingual, featuring\na training sample consisting of various languages\nsimultaneously. Participants were tasked with solv-\ning the same problem without focusing on a spe-\nciﬁc language.\nThe ﬁnal two tracks aimed to address the prob-\nlem of tone prediction without a training sample in\nthe target language. For these languages, models\nwere to be utilized without training on target data.\nFigure 1 illustrates the data used within the com-\npetition framework for training the model and the\nclass distribution of the training sample. It is evi-\ndent that the training sample is highly unbalanced\nfor some languages. In the validation sample, the\nclass distribution for the target languages is approx-\nimately equal.\nFigure 1: Distribution of classes in training samples.\nThe dictionary of abbreviations can be found in the Ap-\npendix in the table 3.\n3\nSystem Overview\nOur approach relies on the XLM-R (Conneau\net al., 2020) model, speciﬁcally the afro-xlmr vari-\nants (Alabi et al., 2022). These models are MLM\nadaptations of the XLM-R-large model, trained on\n17 African languages: Afrikaans, Amharic, Hausa,\nIgbo, Malagasy, Chichewa, Oromo, Nigerian-\nPidgin, Kinyarwanda, Kirundi, Shona, Somali,\nSesotho, Swahili, isiXhosa, Yoruba, and isiZulu,\nalong with 3 high-resource languages: Arabic,\nFrench, and English. The embeddings produced\nby this model were fed into a classiﬁcation layer,\nwhich subsequently generated predictions.\nFor each task, the training sample was split into\n5 distinct validation samples. A model was trained\non each training sample, resulting in 5 models for\na speciﬁc track. Each model underwent validation.\nDuring the testing phase, each model provided its\nprediction, followed by a voting process to deter-\nmine the ﬁnal score.\nWe tried to use different types of preprocessing.\nFor example, we removed links from the text, re-\nmoved @user tags that were often found in tweets.\nFurther, we found that in the text there are often\nsentences in which many quotation marks are used\nin a row: double and single. We collapsed such\nuses of the buckets into one character. Next, we\nnoticed that there were ellipses, where the num-\nber of dots could also be large, such ellipses we\ncollapsed in the usual ellipsis \"...\". The next step\nof preprocessing was the selection of emoticons.\nWhile experimenting with translation models, we\nobserved that translations were not always accurate\nwhen processing raw text containing emoticons.\nHowever, by adding spaces before and after the\nemoticons, the translations appeared more compre-\nhensible and natural. And we tried to do this as a\npreprocessing step. After all, we removed the extra\nspaces and other tab characters.\nIn the ﬁnal two zero-shot tracks, we utilized mod-\nels trained in different languages from the previous\ntracks. These models were validated using a vali-\ndation sample to select the highest quality model.\nConsequently, a system trained in Amharic was\nchosen for Tigrinya, and a system trained in Hausa\nwas selected for Oromo.\n4\nExperimental Setup\nFor every track except the last two (zero-shot), we\nemployed StratiﬁedKFold (Pedregosa et al., 2011)\nwith 5 folds to partition the training sample into\ntraining and validation sets. This enabled us to\ntrain multiple models and subsequently ensemble\ntheir predictions.\nWe experimented with various preprocessing\ntechniques in the Hausa language, which served\nas the basis for most of our trials. Intriguingly, no\ncombination of preprocessing methods yielded a\nhigher-quality model.\nAs a baseline, we explored different-sized ver-\nsions of the XLM-R and Afro-XLM-R models.\nFollowing experiments on Hausa, the Afro-XLM-\nR-large model was chosen. The ﬁnal model did\nnot incorporate data preprocessing as it failed to\ndemonstrate improved performance.\nTo reproduce the results obtained, it is necessary\nto use StratiﬁedKFold with 5 folds. Train the model\non each training fold.\nHyperparameter\nValue\nFolds\n5\nOptimizer\nAdam (Kingma and Ba, 2017)\nLearning rate\n2e-5\nWeight decay\n0.1\nEpochs\n5\nOriginal model\nDavlan/afro-xlmr-large\nMax length\n128\nWe utilized these parameters to train all models\nfor tasks up to track 16. For the ﬁnal two tracks,\nwe assessed each of the 16 previously obtained\nmodels on a validation sample, and based on the\ntarget metric, we selected models trained in spe-\nciﬁc languages. We also attempted to leverage\nmodels trained on all presented languages and eval-\nTrack, lang\nOur F1\nBest team F1\nOur place\n1, Hausa\n81.09\n82.62\n6\n2, Yoruba\n72.07\n80.16\n18\n3, Igbo\n74.51\n82.96\n25\n4, Nigerian_Pidgin\n64.89\n75.96\n24\n5, Amharic\n57.34\n78.42\n17\n6, Algerian Arabic\n65.81\n74.20\n17\n7, Moroccan Arabic/Darija\n57.20\n64.83\n11\n8, Swahili\n62.51\n65.68\n10\n9, Kinyarwanda\n71.91\n72.63\n4\n10, Twi\n55.53\n68.28\n29\n11, Mozambican Portuguese\n69.09\n74.98\n14\n12, Xitsonga (Mozambique Dialect)\n46.62\n60.67\n27\n16, Multilingual\n72.55\n75.06\n3\n17, Zero-Shot on Tigrinya\n68.93\n70.86\n8\n18, Zero-Shot on Oromo\n41.45\n46.23\n15\nTable 1: Results of the DN team in all tracks of the competition\nuated them in the Hausa language; however, this\napproach did not enhance the quality.\nModel\nF1\ndev loss\nXlmr-large\n0.7\n0.69\nAfro-xlmr-large\n0.82\n0.56\nAfro-xlmr-base\n0.79\n0.53\nAfro-xlmr-small\n0.77\n0.75\nAfro-xlmr-mini\n0.71\n0.69\nTable 2: Assessment of the quality of work depending\non the model used on Hausa language\nTable 2 indicates that employing a pre-trained\nmodel in African languages leads to a substantial\nimprovement in quality compared to the base XLM-\nR. With text preprocessing, we achieved an F1\nscore of 0.82, while without preprocessing, the\nF1 score was 0.81. Consequently, we did not incor-\nporate text preprocessing in the ﬁnal version as it\ndid not offer additional quality beneﬁts.\n5\nResults\nOur model was evaluated across all competition\ntracks within the scope of SemEval2023 Task12.\nThroughout our work, we examined various op-\ntions for ﬁne-tuning multilingual models. Our anal-\nysis revealed that the best results were attained\nusing the Afro-XLM-R-large (Alabi et al., 2022)\nmodel. Large models pre-trained on African lan-\nguages demonstrated superior performance, while\nsmaller or multilingual models trained on numer-\nous languages yielded inferior results. We also\ninvestigated several data preprocessing techniques,\nbut none contributed to quality improvement.\nOur model exhibited promising results in some\nlanguages, but performed relatively poorly in oth-\ners. Our investigation reveals that our model ex-\nhibits proﬁcient learning abilities for certain lan-\nguages under consideration in Task A. However,\nwe also note that the model displayed suboptimal\nresults for other languages. In light of these ob-\nservations, we hypothesize that, on average, out\nmodel’s quality is satisfactory across all languages,\ngiven its successful learning outcomes for all of\nthe languages. Nevertheless, when averaged across\nall languages (in the multilingual task), our model\nsecured the 3rd best position among the partici-\npants. A comprehensive overview of our model’s\nperformance can be found in Table 1.\n6\nConclusion\nThe AfriSenti-SemEval 2023 Shared Task 12 pro-\nvided a valuable opportunity for researchers to ad-\nvance sentiment analysis research in low-resource\nAfrican languages.\nOur solution to the shared\ntask focused on leveraging multilingual models\nand transfer learning techniques to improve the\nperformance of sentiment analysis models in low-\nresource settings.\nOur team showed promising results in Subtask B,\nTrack 16, with the third-best performance among\nall participants. While our model showed poor per-\nformance in some languages, it achieved relatively\ngood results on multilingual data on average.\nOverall, the AfriSenti-SemEval 2023 Shared\nTask 12 highlighted the challenges and opportuni-\nties in sentiment analysis for low-resource African\nlanguages. Future research can continue to ex-\nplore innovative techniques and models to over-\ncome these challenges and improve the accuracy of\nsentiment analysis models in low-resource settings.\nReferences\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius\nMosbach, and Dietrich Klakow. 2022. Adapting pre-\ntrained language models to African languages via\nmultilingual adaptive ﬁne-tuning.\nIn Proceedings\nof the 29th International Conference on Computa-\ntional Linguistics, pages 4336–4349, Gyeongju, Re-\npublic of Korea. International Committee on Com-\nputational Linguistics.\nHyung Won Chung, Thibault Fevry, Henry Tsai,\nMelvin Johnson, and Sebastian Ruder. 2020.\nRe-\nthinking embedding coupling in pre-trained lan-\nguage models. arXiv preprint arXiv:2010.12821.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nIn\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nShaunak Joshi and Deepali Deshpande. 2018. Twitter\nsentiment analysis system. International Journal of\nComputer Applications, 180(47):35–39.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A\nmethod for stochastic optimization.\nXiuhong Li, Zhe Li, Jiabao Sheng, and Wushour Slamu.\n2020.\nLow-resource text classiﬁcation via cross-\nlingual language model ﬁne-tuning. In Proceedings\nof the 19th Chinese National Conference on Com-\nputational Linguistics, pages 994–1005, Haikou,\nChina. Chinese Information Processing Society of\nChina.\nShamsuddeen Hassan Muhammad, Idris Abdulmumin,\nAbinew Ali Ayele, Nedjma Ousidhoum, David Ife-\noluwa Adelani, Seid Muhie Yimam, Ibrahim Sa’id\nAhmad, Meriem Beloucif, Saif M. Mohammad, Se-\nbastian Ruder, Oumaima Hourrane, Pavel Brazdil,\nFelermino Dário Mário António Ali, Davis David,\nSalomey Osei, Bello Shehu Bello, Falalu Ibrahim,\nTajuddeen Gwadabe, Samuel Rutunda, Tadesse\nBelay, Wendimu Baye Messelle, Hailu Beshada\nBalcha, Sisay Adugna Chala, Hagos Tesfahun Ge-\nbremichael, Bernard Opoku, and Steven Arthur.\n2023a.\nAfriSenti: A Twitter Sentiment Analysis\nBenchmark for African Languages.\nShamsuddeen Hassan Muhammad, Idris Abdulmu-\nmin, Seid Muhie Yimam, David Ifeoluwa Ade-\nlani, Ibrahim Sa’id Ahmad, Nedjma Ousidhoum,\nAbinew Ali Ayele, Saif M. Mohammad, Meriem\nBeloucif, and Sebastian Ruder. 2023b.\nSemEval-\n2023 Task 12: Sentiment Analysis for African Lan-\nguages (AfriSenti-SemEval).\nIn Proceedings of\nthe 17th International Workshop on Semantic Eval-\nuation (SemEval-2023). Association for Computa-\ntional Linguistics.\nBenjamin Muller, Antonios Anastasopoulos, Benoît\nSagot, and Djamé Seddah. 2021. When being un-\nseen from mBERT is just the beginning: Handling\nnew languages with multilingual language models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 448–462, Online. Association for Computa-\ntional Linguistics.\nKelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021.\nSmall data?\nno problem!\nexploring the viabil-\nity of pretrained multilingual language models for\nlow-resourced languages. In Proceedings of the 1st\nWorkshop on Multilingual Representation Learning,\npages 116–126, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011.\nScikit-learn:\nMachine learning in\nPython.\nJournal of Machine Learning Research,\n12:2825–2830.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers.\n7\nAppendix\nAcronyms\nLang\nam\nAmharic\ndz\nAlgerian Arabic\nha\nHausa\nig\nIgbo\nkr\nKinyarwanda\nma\nDarija\npcm\nNigerian Pidgin\npt\nMozambique Portuguese\nsw\nSwahili\nts\nXitsonga\ntwi\nTwi\nyo\nYoruba\nTable 3: Dictionary of acronyms.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-05-04",
  "updated": "2023-05-04"
}