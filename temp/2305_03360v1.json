{
  "id": "http://arxiv.org/abs/2305.03360v1",
  "title": "A Survey on Offline Model-Based Reinforcement Learning",
  "authors": [
    "Haoyang He"
  ],
  "abstract": "Model-based approaches are becoming increasingly popular in the field of\noffline reinforcement learning, with high potential in real-world applications\ndue to the model's capability of thoroughly utilizing the large historical\ndatasets available with supervised learning techniques. This paper presents a\nliterature review of recent work in offline model-based reinforcement learning,\na field that utilizes model-based approaches in offline reinforcement learning.\nThe survey provides a brief overview of the concepts and recent developments in\nboth offline reinforcement learning and model-based reinforcement learning, and\ndiscuss the intersection of the two fields. We then presents key relevant\npapers in the field of offline model-based reinforcement learning and discuss\ntheir methods, particularly their approaches in solving the issue of\ndistributional shift, the main problem faced by all current offline model-based\nreinforcement learning methods. We further discuss key challenges faced by the\nfield, and suggest possible directions for future work.",
  "text": "arXiv:2305.03360v1  [cs.LG]  5 May 2023\nA Survey on Ofﬂine Model-Based Reinforcement\nLearning\nHaoyang He\nElectrical and Computer Engineering\nCarnegie Mellon University\nPittsburgh, PA 15213\nhhe2@andrew.cmu.edu\nAbstract\nModel-based approaches are becoming increasingly popular in the ﬁeld of ofﬂine\nreinforcement learning, with high potential in real-world applications due to the\nmodel’s capability of thoroughly utilizing the large historical datasets available\nwith supervised learning techniques. This paper presents a literature review of\nrecent work in ofﬂine model-based reinforcement learning, a ﬁeld that utilizes\nmodel-based approaches in ofﬂine reinforcement learning. The survey provides a\nbrief overview of the concepts and recent developments in both ofﬂine reinforce-\nment learning and model-based reinforcement learning, and discuss the intersec-\ntion of the two ﬁelds. We then presents key relevant papers in the ﬁeld of ofﬂine\nmodel-based reinforcement learning and discuss their methods, particularly their\napproaches in solving the issue of distributional shift, the main problem faced by\nall current ofﬂine model-based reinforcement learning methods. We further dis-\ncuss key challenges faced by the ﬁeld, and suggest possible directions for future\nwork.\n1\nIntroduction\nReinforcement learning (RL) constitutes a subﬁeld of machine learning, concentrating on the de-\nvelopment of agents capable of making optimal decisions within uncertain environments. In an RL\nsetting, agents engage with their surroundings by performing actions and receiving feedback in the\nform of rewards or penalties. The primary objective is for the agent to ascertain a policy, a mapping\nfrom states to actions, which maximizes the accumulated reward over a speciﬁed time period [14].\n1.1\nOfﬂine Reinforcement Learning\nOfﬂine reinforcement learning, also referred to as batch reinforcement learning, entails a form of re-\ninforcement learning wherein the policy is exclusively learned based on pre-existing historical data.\nIn practical applications, the available historical dataset is typically extensive, and ofﬂine reinforce-\nment learning seeks to maximize the utilization of this data, learning the policy as a decision-making\nengine, contrasting with online reinforcement learning’s emphasis on exploration [11]. In practice,\nthe preceding data collection policy, responsible for generating the historical data, is customarily\nthe current method or model in deployment. Recent research endeavors often concentrate on op-\ntimizing the algorithm’s outcome in relation to the data collection policy, with the aim of devising\nnew algorithms possessing practical deployment value [2, 10]. The algorithm grapples with the chal-\nlenge of distributional shift, signifying that the deployment scenario exhibits a distinct distribution\nin comparison to the historical dataset, with respect to state and action space [7]. The presence of\ndistributional shift constitutes a key obstacle in ofﬂine reinforcement learning, warranting further\nin-depth examination.\nPreprint. Under review.\n1.2\nModel-Based Reinforcement Learning\nModel-based reinforcement learning encompasses a category of reinforcement learning methodolo-\ngies that focus on constructing a model to comprehend the environment by approximating state\ntransition dynamics and reward functions. By leveraging the estimated model, these algorithms typ-\nically generate simulated data to facilitate the learning of an optimal policy, in conjunction with the\nactual data, also known as experienced data. Enhancing the performance of the learned policy by\ncapitalizing on the model necessitates acquiring a model that more precisely characterizes the actual\nenvironment [9]. Numerous techniques for augmenting model learning have been investigated in\nthe literature over time, including the use of approximation functions trained with prediction loss,\nwhich is susceptible to horizontal compounding error during updates. In order to mitigate this er-\nror, research has proposed implementing constraints on the model, such as the Lipschitz Continuity\nConstraint [1]. An alternative approach to diminishing this error involves distribution matching via\nadversarial learning against a discriminator [17].\n2\nOfﬂine Model-Based Reinforcement Learning\nThe intersection of the two previously discussed concepts deﬁnes ofﬂine model-based reinforce-\nment learning. Analogous to online model-based RL methods, ofﬂine model-based RL approaches\ninitially estimate state transition dynamics and reward functions, subsequently employing them for\nenvironment simulation and planning based on both simulated data and experienced data. Owing\nto the characteristics of model-based learning, ofﬂine model-based learning may particularly ben-\neﬁt from the robust model established by supervised learning, which takes full advantage of the\nextensive historical dataset when compared to ofﬂine model-free methods [11]. Nevertheless, due to\nthe intrinsic properties of ofﬂine learning, ofﬂine model-based RL also grapples with distributional\nshifts in both state and action distributions. Unlike online methods, no exploration is available for\nthe model to rectify itself. This challenge is exacerbated by model exploitation since the model is\nderived from data collected by another policy with a distributional shift relative to the deployment\nenvironment. Consequently, simple policy optimization over the model may result in an optimized\noutcome in the historical distribution, which is out of distribution for the deployment environment,\npotentially leading to inferior results. Recent advancements in ofﬂine model-based reinforcement\nlearning have concentrated on addressing the issue of distributional shift, typically by imposing cer-\ntain constraints on the model through modiﬁcations in state transition dynamics, reward functions,\nor value functions [8, 6, 19, 10, 18, 13, 2].\n2.1\nMOReL\nKidambi et al. proposed MOReL [6], a framework for model-based ofﬂine reinforcement learning\nthat penalizes the reward for uncertainty, allowing the use of classical planning methods with altered\nreward function and state transition dynamics to address the issue of uncertainty and distributional\nshift [7]. They constructed an Unknown State Action Detector based on a threshold over the total\nvariational divergence between the learned model and the behavior model, which is then used to\ndetermine if a certain policy goes into the absorbing state HALT to act as a control for the policy to\nenter the above-threshold uncertain state, whose reward is set to a certain negative value, addressing\nthe model exploitation problem. In their practical implementation, they constructs an ensemble of\nstate transition dynamics models for uncertainty estimation, using a measurement of disagreement\nbetween the models given by eq.1,\ndisti,j(st, at) = Esi\nt+1∼Tψi(·|st,at),si\nt+1∼Tψi(·|st,at)[||si\nt+1 −sj\nt+1||].\n(1)\nin order to deﬁne the uncertainty of the state action pair as eq.2.\nUr(s, a) =\n\n\n\nrmax, max\ni,j disti,j(s, a) > threshold,\n0, max\ni,j disti,j(s, a) ≤threshold.\n(2)\n2.2\nMOPO\nYu et al. proposed MOPO [19], model-based ofﬂine policy optimization, a similar attempt to ad-\ndress the problem of distributional shift by penalizing the reward. In the practical implementation\n2\nof MOPO, the ensemble of models are estimated using the multivariate Gaussian distributions mod-\neling the ith transition dynamics of the model ensembles, denotes in eq.3,\nTψi(s′|s, a) = N(µi(s, a), Σi(s, a)).\n(3)\nwhile the uncertainty is estimated by the Frobenius norm of the maximum distributional variance in\namong the ensemble of models, as shown in eq.4.\nUr(s, a) = max\ni\n||Σi(s, a)||F .\n(4)\nBoth methods are designed to build upon previously existing online model-based RL methods, pro-\nviding a penalized reward function, which can in turn be directly used instead of the original reward\nfunction in classical planning methods [11] like linear-quadratic regulator [15] or Monte Carlo tree\nsearch [3].\n2.3\nBREMEN\nAn alternative approach to the distributional shift problem proposed by Matsushima et al. is BRE-\nMEN [10], behavior-regularized model ensemble. Instead of constructing a pessimistic or reward-\npenalized MDP model, BREMEN learns an ensemble of dynamic models in conjunction with a\npolicy using imaginary rollouts from the ensemble and behavior regularization via conservative\ntrust-region updates. Each model of the ensemble is trained to optimize the objective of eq.5, the\nmean squared error between the prediction of the next state and the true next state, and imaginary\nmodel rollouts are generated sequentially.\nmin\nφi\n1\n|D|\nX\n(st,at,st+1)∈D\n1\n2||st+1 −ˆfφi(st, at)||2\n2\n(5)\nTo address the distributional shift problem, BREMEN uses an iterative policy update via a trust-\nregion constraint, re-initialized with the behavior cloned policy after every deployment, with the\nbehavior cloning objective function of eq.6.\nmin\nβ\n1\n|D|\nX\n(st,at)∈D\n1\n2||at −ˆπβ(st)||2\n2\n(6)\nAfter obtaining the estimated behavior policy, the target policy πθ is initialized as a Gaussian policy\nwith mean from the behavior policy and standard deviation of 1. This behavior cloning initialization\nmay be seen as implicitly biasing the optimized target policy to be close to the data-collection policy,\nand thus reduces the distributional shift. To further bias the learned policy to be close to the data-\ncollection policy, KL-divergence trust region under a certain threshold is added as an additional\nconstraint, yielding the optimization problem of eq.7.\nθk+1 = arg max\nθ\nE\ns,a∼πθk , ˆfφi\n\u0014 πθ(a | s)\nπθk(a | s)Aπθk (s, a)\n\u0015\n,\ns.t.\nE\ns,a∼πθk, ˆ\nfφi\n[DKL (πθ(· | s)∥πθk(· | s))] ≤δ,\nπθ0 = Normal (ˆπβ, 1) .\n(7)\nThe authors claim an empirical improvement over direct KL-divergence penalties such as previous\nmethods, and showed that KL-divergence is controlled implicitly in the optimization. The author\nalso highlighted that BREMEN may outperform similar methods in ofﬂine settings with limited\ndata, attributing to its sequential imaginary behavioral model rollouts, hence having higher sample\nefﬁciency.\n2.4\nVI-LCB\nVI-LCB [12], ofﬂine value iteration with lower conﬁdence bound, is an alternative approach to\naddressing the uncertainty created by distributional shift proposed by Rashidinejad et al. With a\nsimilar idea of penalizing state-action pairs under poor or partial coverage, VI-LCB builds upon\nthe classic value iteration algorithm, and instead of iterating upon the original values given by the\nlearned model, VI-LCB employs the concept of pessimism [12] and subtracts a penalty function to\nthe value function. The value function for value iteration in VI-LCB is deﬁned by eq.8, 9.\nQ(s, a) ←[ r(s, a) −b(s, a) + γ\nX\ns′\nP(s′|s, a)V (s′), ∀(s, a),\n(8)\n3\nV (s) ←[ max\na\nQ(s, a), ∀s.\n(9)\nVI-LCB is developed shortly after PEVI [5], pessimistic value iteration proposed by Jin et al. It is a\nsimple framework to utilize pessimism in ofﬂine reinforcement learning, which simply changes the\naddition to subtraction for the term encouraging exploration in online RL algorithms to use the term\nto penalize less-explored state-action pairs.\nLi et al. proposed a version of VI-LCB with Bernstein-style penalties, denoted by eq.10.\nb(s, a; V ) := min{max{\ns\ncb log\nN\n(1−γ)δ\nN(s, a)\nV ar ˆ\nPs,a(V ),\n2cb log\nN\n(1−γ)δ\n(1 −γ)N(s, a)},\n1\n1 −γ } + 5\nN .\n(10)\nBased on this version of VI-LCB, Li et al. further proved that model-based approaches in ofﬂine\nreinforcement learning achieve minimax-optimal sample complexity without burn-in cost for tabular\nMarkov Decision Process (MDP) [8]. Speciﬁcally, given a single-policy clipped concentrability\ncoefﬁcient C∗\nclipped, model-based ofﬂine RL yields ǫ-accuracy with sample complexity given by\n\n\n\n\n\n\n\nSC∗\nclipped\n(1 −γ)3ǫ2 , inﬁnite-horizon MDPs,\nH4SC∗\nclipped\nǫ2\n, ﬁnite-horizon MDPs.\n(11)\nfor γ-discounted inﬁnite-horizon MDPs with effective horizon\n1\n1−γ or ﬁnite-horizon MDPs with\nhorizon H.\n2.5\nCOMBO\nCOMBO [18], conservative ofﬂine model-based policy optimization, is an approach proposed by\nYu et al. to prevent distributional shift by directly optimizing the model learning without explicitly\nproviding an uncertainty estimation in the planning process. In the construction of the model rollouts,\neach policy is repeatedly evaluated on the conservative policy estimation function 12,\nˆQk+1 ←arg min\nQ β\n\u0000Es,a∼ρ(s,a)[Q(s, a)] −Es,a∼D[Q(s, a)]\n\u0001\n+1\n2Es,a,s′∼df\n\u0014\u0010\nQ(s, a) −bBπ ˆQk(s, a)\n\u00112\u0015\n.\n(12)\nwhich penalizes rewards that are more likely to be out of distribution, based on the sampling dis-\ntribution, a hyperparameter of choice to determine the desired conservativeness. The policy is then\nimproved using the conservative critic Q obtained from previous conservative estimation 13.\nˆπ ←arg max\nπ\nEs∼ρ,a∼π(·|s)\nh\nˆQπ(s, a)\ni\n.\n(13)\nThe authors also showed that the obtained policy has a safe improvement guarantee over the behavior\npolicy. A key improvement of COMBO over similar previous methods like MOReL and MOPO is\nthat it no longer requires an uncertainty estimation in order to perform optimization, which can in\nitself be inaccurate and hard to obtain [11], but instead utilizes an adversarial training process to\noptimize itself.\n2.6\nRAMBO\nRigter et al. proposed RAMBO-RL [13], Robust Adversarial Model-Based Ofﬂine Reinforcement\nLearning, a self-adversarial model. Instead of previous adversarial RL models that train an adversary,\nRAMBO directly trains the model adversarially, while enforcing the conservatism in the training of\nthe adversarial dynamics model. The algorithm is basically an iterative update of the optimization\nof the adversarial model updating the state transition dynamics, where the optimization objective of\nthe adversarial model is based on a formulation of optimization objective to reduce distributional\nshift of a previous work Uehara et al. [16], modiﬁed to eq.14,\nmin\nb\nTφ\nV π\nφ ,\ns.t. ED\n\u0014\nTV\n\u0010\nbTMLE(· | s, a), bTφ(· | s, a)\n\u00112\u0015\n≤ξ,\n(14)\n4\nwhich can be rewritten with Lagrangian relaxation to eq.15.\nmax\nλ≥0 min\nb\nTφ\n\u0012\nL( ˆT, λ) := V π\nφ + λ\n\u0012\nED\n\u0014\nTV\n\u0010\nbTMLE(· | s, a), bTφ(· | s, a)\n\u00112\u0015\n−ξ\n\u0013\u0013\n.\n(15)\nRather than optimizing the Lagrangian multiplier, the authors claimed it was easier to ﬁx the mul-\ntiplier as a hyperparameter for tuning, reducing it as a scaling factor, which can then be reduced to\neq.16.\nmin\nb\nTφ\n\u0012\nλV π\nφ + ED\n\u0014\nTV\n\u0010\nbTMLE(· | s, a), bTφ(· | s, a)\n\u00112\u0015\u0013\n.\n(16)\nTo make the optimization more simple, the authors changed from optimizing the total variational\ndivergence to simple MLE loss, which leads to the ﬁnal optimization objective eq.17.\nLφ = λV π\nφ −E(s,a,r,s′)∼D\nh\nlog bTφ (s′, r | s, a)\ni\n.\n(17)\nThe authors explicitly compared RAMBO to COMBO, and claimed that the pessimistic value func-\ntion updates used by COMBO create local maxima in the Q-function which are present throughout\ntraining, making COMBO likely to stuck in a local maxima, whereas RAMBO’s adversarial modiﬁ-\ncations resulted in a less likelihood.\n2.7\nARMOR\nBhardwaj et al. proposed ARMOR [2], adversarial model for ofﬂine reinforcement learning, a novel\nframework capable of improving upon an arbitrary reference policy regardless of data quality. The\ntwo-player game consists of a learner policy and an adversary MDP model, deﬁned as eq.18.\nˆπ = arg max\nπ∈Π\nmin\nM∈Mα JM(π) −JM(πref).\n(18)\nIn the algorithm, the adversary MDP model is updated in the process 19 for i = 1, 2, where DM is\nthe transition tuples using model predictions based on minibatches of dataset samples from real and\nmodel data, f1, f2 are critic networks used by the double Q residual algorithm loss from [4].\nladversary (f, M) := LDM (f, π, πref ) + β\n\u0010\nEw\nDM (f, M, π) + λEDmin\nreal (M)\n\u0011\n,\nM ←M −ηfast\n\u0000∇Mladversary (f1, M) + ∇Mladversary (f2, M)\n\u0001\n,\nfi ←ProjF\n\u0000fi −ηfast ∇filadversary (fi, M)\n\u0001\n,\n¯fi ←(1 −τ) ¯fi + τfi.\n(19)\nThen the actor network is updated with respect to the ﬁrst critic f1 and the reference policy in the\nprocess 20.\nlactor (π) := −LDM (f1, π, πref ) ,\nπ ←ProjΠ\n\u0000π −ηslow ∇πlactor (π)\n\u0001\n.\n(20)\nThe model state is updated by query of the MDP model after the updates to the two players. Inspired\nby the idea of relative pessimism proposed by Cheng et al. [4], ARMOR optimizes for the worst-case\nrelative performance over uncertainty, instead of a predetermined penalty term based on the state-\naction itself. The authors showed that with relative pessimism, ARMOR policy does not degrades\nthe performance of a reference policy when given a ﬁxed set of hyperparameters, and is capable of\ncompeting against any policy covered by the data when the right hyperparameter is chosen.\n3\nChallenges and Future Work\nDistributional shift remains the most inﬂuential problem in ofﬂine model-based reinforcement learn-\ning, and warrants further research. Even though the discussed relevant work have proposed various\nmethods to mitigate this problem, none of the approaches have succeeded in providing a theoretically\nprovable data non-reliant absolute performance guarantee due to this problem.\nAnother promising direction is the absolute relative performance of ofﬂine model-based reinforce-\nment learning. As highlighted by [2], many theoretically proven methods are not deployed in real-\nworld scenarios because they failed to achieve absolute more optimal performance over the current\npolicy, making policy replacement infeasible. Future work on the absolute relative performance of\nlearned policies can greatly improve the deployment of novel ofﬂine model-based reinforcement\nlearning algorithms.\n5\nReferences\n[1] Kavosh Asadi, Dipendra Misra, and Michael Littman. Lipschitz continuity in model-based\nreinforcement learning. In International Conference on Machine Learning, pages 264–273.\nPMLR, 2018.\n[2] Mohak Bhardwaj, Tengyang Xie, Byron Boots, Nan Jiang, and Ching-An Cheng. Adversarial\nmodel for ofﬂine reinforcement learning, 2023.\n[3] Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling,\nPhilipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton.\nA survey of monte carlo tree search methods. IEEE Transactions on Computational Intelli-\ngence and AI in games, 4(1):1–43, 2012.\n[4] Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor\ncritic for ofﬂine reinforcement learning. In International Conference on Machine Learning,\npages 3852–3878. PMLR, 2022.\n[5] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efﬁcient for ofﬂine rl? In\nMarina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on\nMachine Learning, volume 139 of Proceedings of Machine Learning Research, pages 5084–\n5096. PMLR, 18–24 Jul 2021.\n[6] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel:\nModel-based ofﬂine reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F.\nBalcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,\npages 21810–21823. Curran Associates, Inc., 2020.\n[7] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning:\nTutorial, review, and perspectives on open problems, 2020.\n[8] Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei. Settling the sample complexity of\nmodel-based ofﬂine reinforcement learning. arXiv preprint arXiv:2204.05275, 2022.\n[9] Fan-Ming Luo, Tian Xu, Hang Lai, Xiong-Hui Chen, Weinan Zhang, and Yang Yu. A survey\non model-based reinforcement learning, 2022.\n[10] Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Oﬁr Nachum, and Shixiang Gu.\nDeployment-efﬁcient reinforcement learning via model-based ofﬂine optimization, 2020.\n[11] Rafael Figueiredo Prudencio, Marcos R. O. A. Maximo, and Esther Luna Colombini. A survey\non ofﬂine reinforcement learning: Taxonomy, review, and open problems. IEEE Transactions\non Neural Networks and Learning Systems, pages 1–0, 2023.\n[12] Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging ofﬂine\nreinforcement learning and imitation learning: A tale of pessimism. In M. Ranzato, A. Beygelz-\nimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Informa-\ntion Processing Systems, volume 34, pages 11702–11716. Curran Associates, Inc., 2021.\n[13] Marc Rigter, Bruno Lacerda, and Nick Hawes. Rambo-rl: Robust adversarial model-based\nofﬂine reinforcement learning, 2022.\n[14] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n[15] Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex behav-\niors through online trajectory optimization. In 2012 IEEE/RSJ International Conference on\nIntelligent Robots and Systems, pages 4906–4913. IEEE, 2012.\n[16] Masatoshi Uehara and Wen Sun. Pessimistic model-based ofﬂine reinforcement learning under\npartial coverage. arXiv preprint arXiv:2107.06226, 2021.\n[17] Tian Xu, Ziniu Li, and Yang Yu. Error bounds of imitating policies and environments for\nreinforcement learning.\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\n44(10):6968–6980, 2021.\n6\n[18] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea\nFinn.\nCombo:\nConservative ofﬂine model-based policy optimization.\nIn M. Ranzato,\nA. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neu-\nral Information Processing Systems, volume 34, pages 28954–28967. Curran Associates, Inc.,\n2021.\n[19] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea\nFinn, and Tengyu Ma. Mopo: Model-based ofﬂine policy optimization. In H. Larochelle,\nM. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information\nProcessing Systems, volume 33, pages 14129–14142. Curran Associates, Inc., 2020.\n7\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.SY",
    "eess.SY",
    "I.2.6; I.2.8"
  ],
  "published": "2023-05-05",
  "updated": "2023-05-05"
}