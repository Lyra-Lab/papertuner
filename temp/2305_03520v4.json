{
  "id": "http://arxiv.org/abs/2305.03520v4",
  "title": "Context-Aware Semantic Similarity Measurement for Unsupervised Word Sense Disambiguation",
  "authors": [
    "Jorge Martinez-Gil"
  ],
  "abstract": "The issue of word sense ambiguity poses a significant challenge in natural\nlanguage processing due to the scarcity of annotated data to feed machine\nlearning models to face the challenge. Therefore, unsupervised word sense\ndisambiguation methods have been developed to overcome that challenge without\nrelying on annotated data. This research proposes a new context-aware approach\nto unsupervised word sense disambiguation, which provides a flexible mechanism\nfor incorporating contextual information into the similarity measurement\nprocess. We experiment with a popular benchmark dataset to evaluate the\nproposed strategy and compare its performance with state-of-the-art\nunsupervised word sense disambiguation techniques. The experimental results\nindicate that our approach substantially enhances disambiguation accuracy and\nsurpasses the performance of several existing techniques. Our findings\nunderscore the significance of integrating contextual information in semantic\nsimilarity measurements to manage word sense ambiguity in unsupervised\nscenarios effectively.",
  "text": "arXiv:2305.03520v4  [cs.CL]  13 Dec 2023\nContext-Aware Semantic Similarity Measurement\nfor Unsupervised Word Sense Disambiguation\nJorge Martinez-Gil\nSoftware Competence Center Hagenberg, Softwarepark 32a, Hagenberg,\n4232, Austria.\nContributing authors: jorge.martinez-gil@scch.at;\nAbstract\nThe issue of word sense ambiguity poses a signiﬁcant challenge in natural lan-\nguage processing due to the scarcity of annotated data to feed machine learning\nmodels to face the challenge. Therefore, unsupervised word sense disambigua-\ntion methods have been developed to overcome that challenge without relying on\nannotated data. This research proposes a new context-aware approach to unsu-\npervised word sense disambiguation, which provides a ﬂexible mechanism for\nincorporating contextual information into the similarity measurement process.\nWe experiment with a popular benchmark dataset to evaluate the proposed strat-\negy and compare its performance with state-of-the-art unsupervised word sense\ndisambiguation techniques. The experimental results indicate that our approach\nsubstantially enhances disambiguation accuracy and surpasses the performance\nof several existing techniques. Our ﬁndings underscore the signiﬁcance of inte-\ngrating contextual information in semantic similarity measurements to eﬀectively\nmanage word sense ambiguity in unsupervised scenarios. The source code of this\napproach is available at: https://github.com/jorge-martinez-gil/uwsd.\nKeywords: Natural Language Processing, Knowledge Engineering, Semantic\nSimilarity Measurement\n1 Introduction\nSemantic similarity refers to the extent to which two text pieces convey the same\nmeaning [1]. Traditional semantic similarity measurement strategies rely on various\napproaches considering the overlap of features between the two texts to be compared\n[2]. However, these approaches suﬀer from several limitations, as they fail to consider\n1\nthe context in which the words and sentences are used. In other words, in conventional\nsemantic similarity measures, the resemblance between two entities is based on their\ndeﬁnitions, relationships, and other linguistic or extrinsic features [3]. However, in\nreal-world applications, the context in which entities are being compared can aﬀect\ntheir resemblance.\nFurthermore, word sense ambiguity is typical in natural language processing (NLP)\nbecause words often have numerous meanings depending on their context. Word sense\ndisambiguation (WSD) aims to identify the correct meaning of a word in a given\ncontext [4]. While supervised WSD approaches have achieved high accuracy, they are\nlimited by the availability of annotated data. In contrast, unsupervised approaches\nrely on something other than annotated data but often suﬀer from lower accuracy due\nto that lack of supervision.\nThis research proposes a Context-Aware Semantic Similarity (CASS) measurement\napproach for unsupervised WSD to overcome the accuracy of the results traditionally\nachieved through unsupervised strategies. Our strategy incorporates contextual infor-\nmation into the similarity measurement process to reduce language ambiguity. This\napproach allows us to automatically identify the most likely sense of an ambiguous\nword based on its context without relying on annotated data. In this way, CASS can\nimprove the results in many domains where relevance may vary based on the context.\nOur strategy is not the ﬁrst in that direction since several unsupervised WSD\ntechniques have been proposed [5–8]. However, these methods use diﬀerent strategies\nto consider the speciﬁc context in which the words appear. Our proposed strategy\naddresses current limitations using a novel CASS that adequately incorporates contex-\ntual information into the disambiguation process. Therefore, the primary contributions\nof this research can be summarized as follows:\n• A novel disambiguation strategy that considers the context in which words are used\nto improve the accuracy and relevance of language models beyond the traditional\nmethods to identify synonyms. The proposed approach for unsupervised WSD can\nbeneﬁt languages with limited annotated data, whereas supervised approaches may\nnot be as eﬀective.\n• Evaluation of the proposed method on a complete benchmark dataset and compar-\nison of its performance with several state-of-the-art unsupervised WSD techniques.\nThe experimental results show that the method improves disambiguation accu-\nracy and outperforms several existing techniques, especially when annotated data\nis limited or unavailable.\nThe remainder of this paper is organized as follows. Section 2 provides an overview\nof related work in unsupervised WSD. Section 3 introduces the problem statement\nof this research. Section 4 presents the details of the proposed CASS measurement\napproach. In Section 5, the experimental setup is described, and the evaluation results\nare presented. Section 6 discusses the results obtained and directions for future work.\nFinally, the paper concludes in Section 7.\n2\n2 State-of-the-art\nThis section presents an overview of CASS measurement, discussing its objectives and\nmethods. We examine the state-of-the-art strategies used for this task and discuss\ntheir challenges and limitations, such as the diﬃculty of capturing context-dependent\nnuances and the lack of a universally accepted evaluation methodology. Finally, we\nhighlight some of the potential applications.\n2.1 Semantic Similarity\nSemantic similarity is an essential concept in NLP that has been extensively studied\nin the literature [9–14]. Traditional approaches to measuring semantic similarity are\ntypically based on the study of intrinsic characteristics of the words (lexical methods)\nor their distribution in suﬃciently meaningful text corpora (distributional semantics)\n[15]. Lexical methods rely on the meaning of individual words and their relationships\nto each other [16]. In contrast, distributional semantics techniques aim to capture the\nmeaning of words based on their co-occurrence patterns in large corpora [17].\nCASS is a family of NLP techniques that measures the semantic similarity between\ntwo words or phrases in a given context. This family has gained increasing attention in\nrecent years due to its ability to capture not just the meaning of words but additional\nnuances by considering the context in which they are used [18]. It represents an exten-\nsion of traditional semantic similarity measurement since the latter does not consider\nthe text’s context. Recent advances have seen the development of CASS methods that\nconsider the context in which words are used.\nIn addition, a universally accepted evaluation methodology for assessing the perfor-\nmance of CASS measures is needed. This lack of consensus makes it diﬃcult to compare\nthe best approaches for a variety of applications, which slows down overall progress\nin the ﬁeld. In the research that has been done, numerous evaluation measures have\nbeen suggested. However, they frequently have drawbacks, such as favoring particular\ndata. Therefore, additional research is required in order to develop a methodology for\nevaluation that is both comprehensive and objective.\n2.2 Applications\nCASS measures are intended to increase text understanding capabilities. This can\nbe especially helpful in various applications, including web search, document classi-\nﬁcation, question-answering, and text summarization. CASS measures the semantic\nsimilarity between two words, and context plays a vital role in determining this sim-\nilarity. Search engines use semantic similarity to retrieve documents that match the\nmeaning of the user’s query. In document classiﬁcation, understanding the document’s\nmeaning is essential, while understanding the question’s meaning is crucial in question-\nanswering systems. Text summarization involves condensing text into a shorter version\nand retaining essential information. CASS can improve accuracy and relevance in these\napplications by capturing language nuances and context.\n3\n2.3 Word Sense Disambiguation\nCASS measurement and WSD are related concepts in the ﬁeld of NLP, but they\ndiﬀer signiﬁcantly. Semantic similarity measurement involves determining how similar\ntwo words or phrases are in terms of meaning [19]. CASS measurement considers the\ncontext in which the words or phrases appear in a sentence or document and their\ninherent semantic properties. This approach can help capture nuances and subtleties\nin meaning that might be missed by other methods that rely solely on the intrinsic\nproperties of the words or phrases.\nWSD, conversely, is determining which word’s meaning is intended in a particular\ncontext [20]. This is particularly important for words with multiple meanings [21].\nWSD can be a challenging problem, especially when the context is ambiguous or there\nare few clues to help distinguish among the possible senses [22]. The challenge of\nunsupervised WSD is also essential, as evidenced by several recent papers providing\nideas for meeting the challenge when adequate training datasets are unavailable [5–8].\nTherefore, CASS and WSD are essential tools in NLP, but they have diﬀerent goals\nand use cases.\n2.4 Contribution over the state-of-the-art\nThe training-test gap for models based on unsupervised language modeling makes it\ndiﬃcult for these methods to compute semantic similarity and perform word sense\ndisambiguation correctly. Existing annotated datasets are typically small, making it\nchallenging to train supervised neural models. Our proposed strategy, which incor-\nporates CASS, is the foundation of our contribution to the state-of-the-art since it\nalleviates the problem in scenarios where appropriate training datasets are unavailable.\nWe have tested our strategy against the most recent WSD benchmark dataset and\ndiscovered that it performs better in accuracy than other methods. Furthermore, our\napproach is appropriate for large-scale applications and information retrieval because\nit is computationally eﬀective and scalable. Our work thus advances the development\nof CASS methodologies and shows how context integration can increase the accuracy\nand robustness of WSD techniques.\n3 Problem Statement\nThere are several strategies for calculating CASS. Each CASS strategy has particular\nstrengths and limitations, and the choice depends on the speciﬁc scenario to be faced.\nHowever, it is generally possible to partition and address problem in several steps as\nwe will see below.\n3.1 Context-Aware Semantic Similarity Measurement\nThe problem that we address here can be formulated as follows: Let C be the set of\ncontexts, W be the set of words, and S be the set of semantic similarity scores between\nword pairs.\nGiven a context c ∈C and two words w1, w2 ∈W, the task is to compute a CASS\nscore S(c, w1, w2) ∈S between the word pair w1, w2 in the context c.\n4\nThis can be formally developed as a mathematical expression as in Eq. 1:\nS(c, w1, w2) = f(c, w1, w2)\n(1)\nwhere f is the function that maps a context c and two words w1 and w2 to a\nsemantic similarity score.\nTherefore, the goal is to ﬁnd a function f that considers the context c for an\naccurate calculation of the semantic similarity score S(c, w1, w2).\n3.2 Word Sense Disambiguation\nLet W be a set of words with more than one sense, and let S be a set of senses\nassociated with each word in W. Let C be a text corpus consisting of a set of documents\nD = {d1, d2, ..., dn}. For each word w in W, let T (w) be the set of occurrences of w\nin C, and let S(w) be the set of senses associated with w.\nThe goal of WSD is to assign a sense s in S(w) to each occurrence t in T (w), such\nthat the assigned sense is the most appropriate for the context in which t appears.\nFormally, let S′(w) = {s1, s2, ..., sm} be a set of candidate senses for w. For\neach occurrence t in T (w), we seek to ﬁnd the sense s in S′(w) that maximizes the\nprobability P(s|t, C), where C is the context in which t appears as expressed in Eq. 2.\ns∗= argmaxsP(s|t, C)\n(2)\nwhere s∗is the assigned sense for t, and argmaxs denotes the sense that maximizes\nthe probability.\nSeveral approaches to estimating the probability P(s|t, C) include supervised learn-\ning, unsupervised learning, and knowledge-based methods. In supervised learning, an\nannotated dataset of word occurrences with their corresponding senses is used to train\na classiﬁer that predicts the sense for new occurrences. In unsupervised learning, clus-\ntering or probabilistic models usually group similar word occurrences into clusters,\neach representing a sense. In knowledge-based methods, external knowledge sources,\nsuch as dictionaries or semantic networks, infer the most appropriate sense for a given\nword occurrence.\n4 Methods\nIn our research, we aim to tackle the challenge of word ambiguity, which refers to the\nproblem of words having multiple meanings based on the context in which they are\nused. We propose adapting existing methods incorporating contextual information to\naddress this issue.\nAmong the current state-of-the-art methods for contextual language processing,\nwe identiﬁed four stand-out approaches: BERT [23], ELMo [24], USE [25], and WMD\n[26]. Each method employs a unique technique for capturing contextual information,\nmaking them suitable for diﬀerent use cases [27].\nTo adapt these methods for addressing the issue of word ambiguity, we propose\nusing them to create contextualized embeddings. This involves representing each text\nunit as a vector that considers the context in which it appears. For instance, BERT\n5\n(Bidirectional Encoder Representations from Transformers) [23] is a deep neural net-\nwork that uses a transformer architecture to generate contextualized word embeddings.\nSimilarly, ELMo (Embeddings from Language Models) [24] creates embeddings by\ntraining bidirectional models on large text corpora.\nOn the other hand, USE (Universal Sentence Encoder) [25] is a pre-trained encoder\nthat can be used to generate sentence embeddings that capture the contextual meaning\nof a sentence. Lastly, WMD (Word Mover’s Distance) [26] is a distance-based metric\nthat calculates the similarity between two documents based on the distance between\ntheir constituent words.\nAdapting these strategies to capture contextual details might improve the perfor-\nmance of tasks that involve disambiguating words. Our contribution is to show how\nexisting strategies can be adapted for dealing with the problem of word ambiguity,\nwhich has important implications for a wide range of applications as we have already\nseen.\n4.1 Deﬁnition of our method for Context-Aware Semantic\nSimilarity\nGiven a word w, a context C, and an exclusion list E, the function CASS(w, C, E)\nshould ﬁnd a synonym s∗of w that, when substituted in C, results in the slightest\nalteration of the meaning of C. The process is deﬁned as follows:\nLet W = {s1, s2, . . . , sn} be the set of synonyms of w, excluding any synonyms that\nare contained in E. The function transforms C and Csi, where Csi is the context C\nwith w replaced by si, into embeddings using a pre-trained transformer model. These\nembeddings are multi-dimensional vectors, E(C) ∈Rd and E(Csi) ∈Rd, where d is\nthe dimension.\nThe objective is to ﬁnd s∗∈W that minimizes the semantic change or distance\nfrom C to Csi, which is inverse to the cosine similarity between their embeddings:\ns∗= arg min\nsi∈W\n\u0012\n1 −\nE(C) · E(Csi)\n∥E(C)∥∥E(Csi)∥\n\u0013\n(3)\nwhere · denotes the dot product and ∥· ∥denotes the Euclidean norm.\nNext, we will see the alternatives to build the embeddings using existing pre-trained\ntransformer models.\n4.2 BERT embeddings\nOne prevalent method that could be used for CASS is based on BERT embeddings\n[23]. BERT embeddings are vector representations of words or sentences in a high-\ndimensional space learned from large text corpora. BERT embeddings are context-\naware since they capture the meaning of text based on their surrounding context.\nLet x1, x2, ..., xn be a succession of input tokens deﬁning a sentence, and let hi be\nthe contextualized model for the i-th token obtained using the BERT model.\nWe can obtain the sentence-level embedding S by taking a weighted average of the\ntoken embeddings as in Eq. 4:\n6\nS = 1\nn\nn\nX\ni=1\nαihi\n(4)\nwhere αi is the weight assigned to the i-th token, and is given by Eq. 5:\nαi =\nexp(wT hi)\nPn\nj=1 exp(wT hj)\n(5)\nHere, w is a parameter vector that deﬁnes the importance of each token in the\nsentence. Note that the weights αi are learned during training and are used to give\nhigher importance to the most relevant tokens.\n4.3 ELMo\nELMo is a deep contextualized word representation model using a bi-directional lan-\nguage (biLM) to generate word embeddings [24]. The biLM is trained on a large corpus\nof text data to predict the next word in a sequence of words given the previous words\nin both forward and backward directions.\nCombining the hidden states of the biLM at each layer allows the production of\nthe ELMo representation of a word. Let us denote the biLM as a function fbiLM(x)\nthat takes a sequence of words x as input and produces a set of hidden states H =\nh1, h2, ..., hL at each layer l.\nThe ELMo representation of a sentence si is then computed as a weighted sum of\nthe hidden states at each layer L as in Eq. 6:\nELMos = γs hX\nj = 0L−1sj · wj\ni\n+ γsx\n\n\nL−1\nX\nj=0\nTj\nX\nk=1\nsj,k · wj,k\n\n\n(6)\nwhere ELMo represents the embedding for a given sentence s, L is the number of\nlayers in the ELMo model, T j is the number of tokens in the j-th layer, sj and sj,k\nare the activations of the j-th layer for the sentence and the k-th token in the j-th\nlayer, respectively, wj and wj, k are the weights for the j-th layer and the k-th token\nin the j-th layer, and γs and γs\nx are scalar weights obtained during training.\nThe weights capture the importance of each layer for the speciﬁc task and allow\nELMo to generate context-dependent embeddings that are useful for our purposes.\n4.4 Universal Sentence Enconder\nLet us say we have two sentences X and Y, and we want to calculate their CASS using\nthe USE embeddings [25]. We ﬁrst obtain the USE embeddings of X and Y, denoted\nby eX and eY, respectively.\nThe semantic similarity ss between eX and eY is then calculated as in Eq. 7:\nss =\neX · eY\n∥eX∥· ∥eY∥\n(7)\nWhere:\n7\n· denotes the dot product between the embeddings and ∥·∥denotes the L2 norm\nof the embeddings, i.e., the length of the embedding vectors\nIf necessary, Eq. 8 can also work with items from the sentences.\nss =\nP\ni(eX i · eY⟩)\npP\ni(e2\nX i) ·\nqP\ni(e2\nYi)\n(8)\nWhere:\neXi and eY i are the ith elements of the embeddings eX and eY, respectively.\n4.5 Word Mover’s Distance\nThe Word Mover’s Distance (WMD) measures the semantic similarity between two\ntexts, which considers the distances between the individual words in the texts [26].\nThe mathematical formulation of the WMD can be described as follows:\nLet D be a metric space of word embeddings, and let be X and Y two sentences\nof n and m words, respectively. We also have a matrix T , which tells us how much of\na word in X moves to a word in Y, and this is represented by a non-negative number\nin Tij. The cost of moving from one word to another is represented by c(i, j), which is\nthe distance between the word i and word j. We need to make sure that the total ﬂow\nfrom each word in X is equivalent to the value of Xi, which can be achieved by setting\nP\nj Tij = Xi. With these constraints in mind, we can use Eq. 9 to ﬁnd the minimum\ncumulative cost of transforming X into Y.\narg min\nn\nX\ni,j=1\nTijc(i, j)\nsubject to\nn\nX\nj=1\nTij = Xi ∀i ∈{1, 2, 3 · · ·n} ∧\nn\nX\ni=1\nTij = Yj ∀j ∈{1, 2, 3 · · ·n}\n(9)\nA wide range of word embeddings can be used here, e.g., word2vec [28]. Further-\nmore, the optimization problem can be solved using linear programming techniques.\nThe resulting WMD measures the semantic similarity between X and Y, consider-\ning the distances between the individual words in the documents. The WMD has\noutperformed traditional bag-of-words and vector space models in the past [29].\n5 Results\nHere, we showcase the results of our WSD experiments. Through a detailed analysis of\nvarious embedding models, we have compared the outcomes of our proposed strategy\nwith commonly employed strategies to measure their impact.\n5.1 Empirical Setup and Baseline Selection\nOur research proposes a CASS measurement method for unsupervised WSD. The\nproposed method measures the semantic similarity between a target word and its\n8\ncandidate senses based on the context in which the target word appears. We compare\nour approach with several unsupervised techniques for each use case in the dataset.\nThe experiments are tested on a computer with 32 GB of RAM and an i7-8700 CPU\nrunning at 3.20 GHz on Windows 10.\nWe will use two baselines here, one weak and one strong. The weak baseline (Ran-\ndom Option, RO) calculates the probability of giving a correct answer randomly. So,\nin cases where two possible alternatives are considered, there would be a probability of\n50%, in case of three, 33.33%, and so on. The strong baseline is one of the most com-\nmonly used baselines for WSD; the Most Frequent Sense (MFS) method. The MFS\nbaseline assigns the most frequent sense of a word in a given dataset to all instances\nof that word. It is a method that is diﬃcult to replicate in the real world by a com-\nputer because it requires external knowledge (i.e., the most frequent sense for a given\nword). However, it is a natural solution for people.\nWe must compute the most frequent sense of each target word in the training\ndata to implement this strong baseline. Then, we assign the most frequent sense as\nthe predicted sense for each instance of the target word in the test data. While this\nstrong baseline is very simple, the literature shows that it can be surprisingly eﬀective,\nespecially for words with a highly dominant sense.\n5.2 Dataset\nIn this work, we are working with the CoarseWSD-20 dataset [30], which is a dataset\nfor ﬁguring out the actual meaning of words that, in practice, can have diﬀerent\nmeanings. The dataset is made from Wikipedia and only includes nouns. It focuses on\n20 words that can have 2 to 5 diﬀerent meanings. The dataset contains 10,196 cases,\nwhich helps test WSD models, as it has all the senses in the test sets. This makes it\nparticularly suitable for evaluating WSD models.\nAs our method is fully unsupervised, we do not need to use the training instances\noﬀered. At the same time, if we were to compete with solutions that use such training\nsamples, the comparison would be unfair. So, we will limit ourselves to the compar-\nison with other unsupervised techniques, particularly the weak (RO) and the strong\n(MFS) baselines. In addition, we need to adapt some mapping classes to facilitate the\ndisambiguation slightly.\n5.3 Evaluation Criteria\nWe use a standard evaluation metric called accuracy to evaluate the performance\nof diﬀerent WSD models on the CoarseWSD-20 dataset. Accuracy is the proportion\nof correctly identiﬁed senses from the total number of instances in the test set. In\naddition, we will break down the results by use case and globally. Both for our approach\nand for the baselines we compare with.\n5.4 Empirical Evaluation\nWe aim to evaluate the proposed CASS measurement method for unsupervised WSD\nand compare it with several unsupervised WSD methods. The solid blue color repre-\nsents the results obtained through our strategy. The black color represents the weak\n9\nStrategy\nHits\nAccuracy\nUWSD+BERT\n7,927\n77.74%\nMFS-Baseline\n7,487\n73.43%\nUWSD+USE\n7,335\n71.94%\nUWSD+ELMo\n7,010\n68.75%\nUWSD+WMD\n6,123\n60.00%\nRO-Baseline\n4,459\n43.73%\nTable 1 Summary of the best results obtained for each of the diﬀerent embedding approaches\nbaseline (the results could be replicated by selecting a random option). In contrast,\nthe red represents the strong baseline (the results could be replicated with external\nknowledge about the most frequently used sense).\nFigure 1 shows the initial results obtained with the solution implemented by BERT.\nAs can be seen, the results are pretty good since the weak baseline is consistently out-\nperformed, and the strong baseline is almost always outperformed. In addition, there\nare many use cases where a wide margin beats the strong baseline. Considering that\nthis strategy does not use any external resources or training, this can be considered a\ngood performance.\nFigure 2 shows the results obtained with the solution implemented by ELMo. As\ncan be seen, this approach is better than the weak baseline but often fails to outperform\nthe strong baseline. Therefore, the results are not optimal.\nFigure 3 shows the results obtained with the solution implemented by USE embed-\ndings. As can be seen, several results are even lower than those from the weak baseline,\nand the strong baseline is only surpassed in limited cases. In general, when compared\nto the other approaches studied, UWSD-USE is not among the best.\nFigure 4 shows the results obtained with the solution implemented by WMD. As\ncan be seen, the results are far from optimal. There are several occasions in which\nthey are even below the weak baseline, being very rare in the cases in which they\nmanage to overcome the strong baseline. Generally speaking, this approach is the one\nthat yields the worst results among those studied.\n5.5 Comparison with existing techniques\nTable 1 summarizes all our global results. The CoarseWSD-20 dataset is still relatively\nyoung and specially designed to perform machine learning, so we are unaware of any\nother published work on the unsupervised WSD task. However, our experimentation\nhas many alternatives that have been tested. As can be seen, all the proposed strategies\ncan outperform the RO baseline (weak baseline). However, only the strategy that uses\nBERT embeddings can outperform the MFS baseline (strong baseline) as well. The\nstrategy followed with implementing BERT embeddings is an excellent result since it\ncan outperform a method that uses external knowledge without any extra knowledge\nor training phase.\n5.6 Detailed Results\nIn the following, we show the results of all the experiments carried out, i.e., taking\ninto account all the language models that have been analyzed and the results they\nhave led to\n10\napple\narm\nbank\nbass\nbow\nchair\nclub\ncrane\ndeck\ndigit\n0\n20\n40\n60\n80\n100\nCategory\nAccuracy\nhood\njava\nmole\npitcher\npound\nseal\nspring\nsquare\ntrunk\nyard\n0\n20\n40\n60\n80\n100\nCategory\nAccuracy\nFig. 1 Results obtained for the CoarseWSD-20 dataset using UWSD+BERT. The solid blue bar\nrepresents the results obtained. While the black and red colors represent the weak and strong baselines,\nrespectively\n11\napple\narm\nbank\nbass\nbow\nchair\nclub\ncrane\ndeck\ndigit\n0\n20\n40\n60\n80\n100\nCategory\nAccuracy\nhood\njava\nmole\npitcher\npound\nseal\nspring\nsquare\ntrunk\nyard\n0\n20\n40\n60\n80\n100\nCategory\nAccuracy\nFig. 2 Results obtained for the CoarseWSD-20 dataset using UWSD+ELMo. The solid blue bar\nrepresents the results obtained. While the black and red colors represent the weak and strong baselines,\nrespectively\n12\napple\narm\nbank\nbass\nbow\nchair\nclub\ncrane\ndeck\ndigit\n0\n20\n40\n60\n80\n100\nCategory\nAccuracy\nhood\njava\nmole\npitcher\npound\nseal\nspring\nsquare\ntrunk\nyard\n0\n20\n40\n60\n80\n100\nCategory\nAccuracy\nFig. 3 Results obtained for the CoarseWSD-20 dataset using UWSD+USE. The solid blue bar\nrepresents the results obtained. While the black and red colors represent the weak and strong baselines,\nrespectively\n13\napple\narm\nbank\nbass\nbow\nchair\nclub\ncrane\ndeck\ndigit\n0\n20\n40\n60\n80\n100\nCategory\nAccuracy\nhood\njava\nmole\npitcher\npound\nseal\nspring\nsquare\ntrunk\nyard\n0\n20\n40\n60\n80\n100\nCategory\nAccuracy\nFig. 4 Results obtained for the CoarseWSD-20 dataset using UWSD+WMD. The solid blue bar\nrepresents the results obtained. While the black and red colors represent the weak and strong baselines,\nrespectively\n14\nTable 2 summarizes all the results obtained with diﬀerent BERT models. All these\nmodels have been extensively evaluated for their quality of embedded sentences. As\ncan be seen, not all models lead to results superior to the baseline. A more in-depth\nanalysis of why some models considered can rank better than others remains a future\nwork in progress.\nStrategy\nHits\nAccuracy\nUWSD+BERT+all-mpnet-base-v2\n7,927\n77.74%\nUWSD+BERT+all-MiniLM-L12-v2\n7,652\n75.05%\nUWSD+BERT+all-MiniLM-L6-v2\n7,609\n74.63%\nMFS-Baseline\n7,487\n73.43%\nUWSD+BERT+paraphrase-albert-small-v2\n7,104\n69.67%\nUWSD+BERT+paraphrase-MiniLM-L3-v2\n7,098\n69.62%\nUWSD+BERT+all-distilroberta-v1\n5,547\n54.40%\nRO-Baseline\n4,459\n43.73%\nTable 2 Summary of the results results obtained using diﬀerent language models based on BERT\nTable 3 summarizes all the results obtained with diﬀerent ELMo models. These\nmodels have undergone, once again, thorough assessments to determine the quality of\nthe ELMo embeddings. No model surpasses the baseline in terms of results.\nStrategy\nHits\nAccuracy\nMFS-Baseline\n7,487\n73.43%\nUWSD+ELMo+Corpus of Historical American English\n7,010\n68.75%\nUWSD+ELMo+English Wikipedia February 2017\n5,593\n54.85%\nUWSD+ELMo+English Wikipedia October 2019\n4,786\n46.94%\nRO-Baseline\n4,459\n43.73%\nTable 3 Summary of the results obtained using diﬀerent language models based on ELMo\nTable 4 summarizes all the results of diﬀerent USE models. These models have been\nevaluated once more to assess the quality of the USE embeddings. The evaluations\nindicate that none of the models exceed the baseline in terms of performance, although\nthe Large model achieves results very close to the baseline.\nStrategy\nHits\nAccuracy\nMFS-Baseline\n7,487\n73.43%\nUWSD+USE+Large\n7,335\n71.94%\nUWSD+USE+Classic\n6,396\n62.73%\nRO-Baseline\n4,459\n43.73%\nTable 4 Summary of the results results obtained using diﬀerent language models based on USE\nLastly, Table 5 summarizes all the results obtained with diﬀerent WSD models.\nThe models were again evaluated to gauge the embedding’s quality. The ﬁndings show\nthat none of the models outperform the established baseline regarding outcomes.\n15\nStrategy\nHits\nAccuracy\nMFS-Baseline\n7,487\n73.43%\nUWSD+WMD+glove-twitter-200\n6,123\n60.00%\nUWSD+WMD+word2vec-google-news-300\n5,868\n57.55%\nUWSD+WMD+glove-wiki-gigaword-300\n5,858\n57.45%\nUWSD+WMD+fasttext-wiki-news-subwords-300\n5,847\n57.34%\nRO-Baseline\n4,459\n43.73%\nTable 5 Summary of the results results obtained using diﬀerent language models based on WMD\n6 Discussion\nOur strategy has exhibited promising results in improving the accuracy of WSD since\nit can eﬀectively capture the subtle distinctions in word senses, leading to more precise\ndisambiguation. One advantage of the proposed strategy is its ability to function\nwithout annotated data, making it more widely applicable to diverse domains. This is\nparticularly crucial for low-resource languages where annotated data is scarce.\nAnother advantage of our method is its capability to capture the complex nuances\nof meaning that traditional semantic models might overlook. The rationale behind\nincorporating contextual information is to diﬀerentiate between polysemous words\nwith various meanings in diﬀerent contexts.\nThe experimental results demonstrate that the proposed method outperforms sev-\neral unsupervised WSD techniques on the CoarseWSD-20 benchmark dataset. The\nperformance improvement is particularly remarkable for words with high levels of\nambiguity, where conventional methods often struggle to disambiguate accurately.\nWhile the proposed approach displays promise, some limitations still require\naddressing. One limitation is that the approach heavily relies on the quality of avail-\nable contextual information. Disambiguation accuracy may be compromised when the\ncontext is noisy or ambiguous. Additionally, the proposed approach may perform inad-\nequately in cases where the context is too sparse, resulting in inadequate information\nfor precise disambiguation. Future research directions could explore further improve-\nments to the approach, such as integrating additional sources of contextual information\nor combining it with other WSD techniques.\n7 Conclusion\nThis work shows how CASS can comprehend human language eﬀectively since it can\nrecognize that the meaning of a word can diﬀer based on the speciﬁc context in which\nit appears. CASS techniques aim to capture this variability, calculate more accurate\nsimilarity scores, and improve the performance of unsupervised WSD strategies.\nWe have seen that including contextual information is a proven way to improve\naccuracy when facing unsupervised WSD tasks. Our research indicates that using a\nstrategy of this kind can address the challenge of interpreting the meaning of language\nas used in particular settings. We have achieved signiﬁcant improvements in disam-\nbiguation accuracy compared to conventional methods that do not consider contextual\ninformation. The fact that our strategy performs better than numerous other unsu-\npervised strategies is further evidence of its usefulness in dealing with the ambiguity\nof word senses.\n16\nIn conclusion, CASS and WSD have a great deal of untapped potential that may\nimprove the accuracy of text understanding and inspire the development of novel\nNLP applications that use disambiguation technology. Novel strategies in this direc-\ntion could lead to developing more eﬃcient solutions and better comprehending the\ncomplexities and nuances of human language.\nAcknowledgments\nThis research has been funded by the Federal Ministry for Climate Action, Environ-\nment, Energy, Mobility, Innovation, and Technology (BMK), the Federal Ministry for\nDigital and Economic Aﬀairs (BMDW), and the State of Upper Austria in the frame\nof SCCH, a center in the COMET - Competence Centers for Excellent Technologies\nProgramme managed by Austrian Research Promotion Agency FFG.\nReferences\n[1] Navigli, R., Martelli, F.: An overview of word and sense similarity. Nat. Lang.\nEng. 25(6), 693–714 (2019) https://doi.org/10.1017/S1351324919000305\n[2] Lastra-D´ıaz, J.J., Garc´ıa-Serrano, A., Batet, M., Fern´andez, M., Chirigati, F.:\nHESML: A scalable ontology-based semantic similarity measures library with a\nset of reproducible experiments and a replication dataset. Inf. Syst. 66, 97–118\n(2017) https://doi.org/10.1016/j.is.2017.02.002\n[3] Martinez-Gil, J.: A comprehensive review of stacking methods for semantic sim-\nilarity measurement. Machine Learning with Applications 10, 100423 (2022)\nhttps://doi.org/10.1016/j.mlwa.2022.100423\n[4] Navigli, R.: Word sense disambiguation: A survey. ACM computing surveys\n(CSUR) 41(2), 1–69 (2009) https://doi.org/10.1145/1459352.1459355\n[5] Han, S., Shirai, K.: Unsupervised word sense disambiguation based on word\nembedding and collocation. In: Rocha, A.P., Steels, L., Herik, H.J. (eds.)\nProceedings of the 13th International Conference on Agents and Artiﬁcial Intel-\nligence, ICAART 2021, Volume 2, Online Streaming, February 4-6, 2021, pp.\n1218–1225. SCITEPRESS, Online Streaming (2021). https://doi.org/10.5220/\n0010380112181225\n[6] Moradi, B., Ansari, E., Zabokrtsk´y, Z.: Unsupervised word sense disambiguation\nusing word embeddings. In: 25th Conference of Open Innovations Association,\nFRUCT 2019, November 5-8, 2019, pp. 228–233. IEEE, Helsinki, Finland (2019).\nhttps://doi.org/10.23919/FRUCT48121.2019.8981526\n[7] Rahman, N., Borah, B.: An unsupervised method for word sense disambiguation.\nJ. King Saud Univ. Comput. Inf. Sci. 34(9), 6643–6651 (2022) https://doi.org/\n10.1016/j.jksuci.2021.07.022\n17\n[8] Ustalov, D., Teslenko, D., Panchenko, A., Chernoskutov, M., Biemann, C.,\nPonzetto, S.P.: An unsupervised word sense disambiguation system for under-\nresourced languages. In: Proceedings of the Eleventh International Conference\non Language Resources and Evaluation, LREC 2018, May 7-12, 2018. European\nLanguage Resources Association (ELRA), Miyazaki, Japan (2018)\n[9] Han, L., Kashyap, A.L., Finin, T., Mayﬁeld, J., Weese, J.: Umbc ebiquity-core:\nSemantic textual similarity systems. In: Diab, M.T., Baldwin, T., Baroni, M.\n(eds.) Proceedings of the Second Joint Conference on Lexical and Computa-\ntional Semantics, *SEM 2013, June 13-14, 2013, pp. 44–52. Association for\nComputational Linguistics, Atlanta, Georgia (USA) (2013)\n[10] Harispe, S., Ranwez, S., Janaqi, S., Montmain, J.: Semantic Similarity from Nat-\nural Language and Ontology Analysis. Synthesis Lectures on Human Language\nTechnologies. Morgan & Claypool Publishers, - (2015). https://doi.org/10.2200/\nS00639ED1V01Y201504HLT027\n[11] Lastra-D´ıaz, J.J., Garc´ıa-Serrano, A.: A new family of information content models\nwith an experimental survey on wordnet. Knowl.-Based Syst. 89, 509–526 (2015)\nhttps://doi.org/10.1016/j.knosys.2015.08.019\n[12] Lastra-D´ıaz, J.J., Goikoetxea, J., Taieb, M.A.H., Garc´ıa-Serrano, A., Aouicha,\nM.B., Agirre, E.: A reproducible survey on word embeddings and ontology-based\nmethods for word similarity: Linear combinations outperform the state of the art.\nEng. Appl. Artif. Intell. 85, 645–665 (2019) https://doi.org/10.1016/j.engappai.\n2019.07.010\n[13] Martinez-Gil, J., Chaves-Gonzalez, J.M.: Semantic similarity controllers: On the\ntrade-oﬀbetween accuracy and interpretability. Knowl. Based Syst. 234, 107609\n(2021) https://doi.org/10.1016/j.knosys.2021.107609\n[14] Zhu, G., Iglesias, C.A.: Computing semantic similarity of concepts in knowledge\ngraphs. IEEE Trans. Knowl. Data Eng. 29(1), 72–85 (2017) https://doi.org/10.\n1109/TKDE.2016.2610428\n[15] Martinez-Gil, J., Chaves-Gonzalez, J.M.: Sustainable semantic similarity assess-\nment. Journal of Intelligent & Fuzzy Systems 43(5), 6163–6174 (2022) https://\ndoi.org/10.3233/JIFS-220137\n[16] Martinez-Gil, J., Chaves-Gonzalez, J.M.: A novel method based on symbolic\nregression for interpretable semantic similarity measurement. Expert Syst. Appl.\n160, 113663 (2020) https://doi.org/10.1016/j.eswa.2020.113663\n[17] Bollegala, D., Matsuo, Y., Ishizuka, M.: A web search engine-based approach\nto measure semantic similarity between words. IEEE Trans. Knowl. Data Eng.\n23(7), 977–990 (2011) https://doi.org/10.1109/TKDE.2010.172\n18\n[18] Pilehvar, M.T., Navigli, R.: From senses to texts: An all-in-one graph-based\napproach for measuring semantic similarity. Artif. Intell. 228, 95–128 (2015)\nhttps://doi.org/10.1016/j.artint.2015.07.005\n[19] Chandrasekaran, D., Mago, V.: Evolution of semantic similarity - A survey. ACM\nComput. Surv. 54(2), 41–14137 (2021) https://doi.org/10.1145/3440755\n[20] Apidianaki, M.: From word types to tokens and back: A survey of approaches\nto word meaning representation and interpretation. Computational Linguistics,\n1–60 (2022) https://doi.org/10.1162/coli a 00474\n[21] Loureiro, D., Jorge, A.M., Camacho-Collados, J.: Lmms reloaded: Transformer-\nbased sense embeddings for disambiguation and beyond. Artiﬁcial Intelligence\n305, 103661 (2022)\n[22] Eyal, M., Sadde, S., Taub-Tabib, H., Goldberg, Y.: Large scale substitution-based\nword sense induction, 4738–4752 (2022) https://doi.org/10.18653/V1/2022.ACL-\nLONG.325\n[23] Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidi-\nrectional transformers for language understanding. In: Burstein, J., Doran, C.,\nSolorio, T. (eds.) Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, June 2-7, 2019, Volume 1 (Long and Short\nPapers), pp. 4171–4186. Association for Computational Linguistics, Minneapolis,\nMN, USA (2019). https://doi.org/10.18653/v1/n19-1423\n[24] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettle-\nmoyer, L.: Deep contextualized word representations. In: Walker, M.A., Ji, H.,\nStent, A. (eds.) Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2018, June 1-6, 2018, Volume 1 (Long Papers), pp.\n2227–2237. Association for Computational Linguistics, New Orleans, Louisiana,\nUSA (2018). https://doi.org/10.18653/v1/n18-1202\n[25] Cer, D., Yang, Y., Kong, S., Hua, N., Limtiaco, N., John, R.S., Constant, N.,\nGuajardo-Cespedes, M., Yuan, S., Tar, C., Strope, B., Kurzweil, R.: Universal\nsentence encoder for english. In: Blanco, E., Lu, W. (eds.) Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, EMNLP\n2018: System Demonstrations, October 31 - November 4, 2018, pp. 169–174. Asso-\nciation for Computational Linguistics, Brussels, Belgium (2018). https://doi.org/\n10.18653/v1/d18-2029\n[26] Kusner, M., Sun, Y., Kolkin, N., Weinberger, K.: From word embeddings to doc-\nument distances. In: International Conference on Machine Learning, pp. 957–966\n(2015). PMLR\n19\n[27] Martinez-Gil, J., Mokadem, R., K¨ung, J., Hameurlain, A.: A novel neurofuzzy\napproach for semantic similarity measurement. In: Golfarelli, M., Wrembel, R.,\nKotsis, G., Tjoa, A.M., Khalil, I. (eds.) Big Data Analytics and Knowledge Dis-\ncovery - 23rd International Conference, DaWaK 2021, Virtual Event, September\n27-30, 2021, Proceedings. Lecture Notes in Computer Science, vol. 12925, pp. 192–\n203. Springer, Virtual Event (2021). https://doi.org/10.1007/978-3-030-86534-4\n18\n[28] Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed rep-\nresentations of words and phrases and their compositionality. In: Advances in\nNeural Information Processing Systems 26: 27th Annual Conference on Neural\nInformation Processing Systems 2013. Proceedings of a Meeting Held December\n5-8, 2013, Lake Tahoe, Nevada, United States., pp. 3111–3119 (2013)\n[29] Skianis, K., Malliaros, F.D., Tziortziotis, N., Vazirgiannis, M.: Boosting tricks\nfor word mover’s distance. In: International Conference on Artiﬁcial Neural\nNetworks, pp. 761–772 (2020). https://doi.org/10.1007/978-3-030-61616-8 61 .\nSpringer\n[30] Loureiro, D., Rezaee, K., Pilehvar, M.T., Camacho-Collados, J.: Analysis and\nevaluation of language models for word sense disambiguation. Computational\nLinguistics 47(2), 387–443 (2021) https://doi.org/10.1162/coli a 00405\n20\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2023-05-05",
  "updated": "2023-12-13"
}