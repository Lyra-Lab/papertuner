{
  "id": "http://arxiv.org/abs/2305.04572v2",
  "title": "Putting Natural in Natural Language Processing",
  "authors": [
    "Grzegorz Chrupała"
  ],
  "abstract": "Human language is firstly spoken and only secondarily written. Text, however,\nis a very convenient and efficient representation of language, and modern\ncivilization has made it ubiquitous. Thus the field of NLP has overwhelmingly\nfocused on processing written rather than spoken language. Work on spoken\nlanguage, on the other hand, has been siloed off within the largely separate\nspeech processing community which has been inordinately preoccupied with\ntranscribing speech into text. Recent advances in deep learning have led to a\nfortuitous convergence in methods between speech processing and mainstream NLP.\nArguably, the time is ripe for a unification of these two fields, and for\nstarting to take spoken language seriously as the primary mode of human\ncommunication. Truly natural language processing could lead to better\nintegration with the rest of language science and could lead to systems which\nare more data-efficient and more human-like, and which can communicate beyond\nthe textual modality.",
  "text": "Putting Natural in Natural Language Processing\nGrzegorz Chrupała\nDepartment of Cognitive Science and Artificial Intelligence\nTilburg University\ngrzegorz@chrupala.me\nAbstract\nHuman language is firstly spoken and only sec-\nondarily written. Text, however, is a very con-\nvenient and efficient representation of language,\nand modern civilization has made it ubiquitous.\nThus the field of NLP has overwhelmingly fo-\ncused on processing written rather than spo-\nken language. Work on spoken language, on\nthe other hand, has been siloed off within the\nlargely separate speech processing community\nwhich has been inordinately preoccupied with\ntranscribing speech into text. Recent advances\nin deep learning have led to a fortuitous conver-\ngence in methods between speech processing\nand mainstream NLP. Arguably, the time is\nripe for a unification of these two fields, and\nfor starting to take spoken language seriously\nas the primary mode of human communication.\nTruly natural language processing could lead to\nbetter integration with the rest of language sci-\nence and could lead to systems which are more\ndata-efficient and more human-like, and which\ncan communicate beyond the textual modality.\n1\nIntroduction\nThe ACL 2023 theme track urges the community\nto check the reality of the progress in NLP. This\nposition paper adopts an expansive interpretation\nof this question. It is definitely worth inquiring into\nthe apparent advances of current NLP in their own\nterms. Here, however, I question these terms and\nargue that our field has focused on only a limited\nsubset of human language which happens to be\nconvenient to work with, and thus misses major\naspects of human communication.\n1.1\nHuman Language is Primarily Spoken\nHumans are an exceptional species in many ways,\nand out of these, human language is one of the\nmost salient. Unlike communication systems used\nby other organisms, human language is open-ended,\ncapable of expressing abstract concepts, and of ref-\nerence to events displaced in time and space. While\nthe capacity to acquire language is universal and\nlargely innate (Darwin, 1874; Pinker and Bloom,\n1990) it also is culturally mediated and likely arose\nvia gene-culture co-evolution (Deacon, 1998; Rich-\nerson and Boyd, 2010).\nOne revolutionary technology which turbo-\ncharged human language was writing, which was\ninvented a handful of times in the most recent few\nthousand years of the human story (Fischer, 2003).\nWriting, followed by the printing press, followed\nby the Internet, have made written text ubiquitous\nto the extent that it is easy to forget that the primary\nand universal modality for most human communi-\ncation throughout history has been spoken.1\nEven today many of the world’s languages do not\nhave a standardized written form. For those that do,\nthe written modality originated as a compressed,\nsymbolic representation of the spoken form.\nChildren acquire a spoken language (and not in-\nfrequently two or more) within the first few years\nof their life with no or little explicit instruction,\nlargely relying on weak, noisy supervision via so-\ncial interaction and perceptual grounding. In con-\ntrast, they require hundreds of hours of explicit\ninstruction and arduous conscious practice to learn\nto read and write, and most are only able to learn\nthe written modality a couple of years at best after\nbecoming fluent communicators in one or more\nspoken languages.\n1.2\nReality check\nThus, arguably, the natural language for which we\nare biologically equipped is spoken. Written lan-\nguage is a secondary development, which happens\nto be very useful and widespread, but is neverthe-\nless derivative of speech. This appears to be the\nconsensus view in linguistics going back at least a\n1I am using spoken language in the broad sense here, in-\ncluding both the oral and gestural (signed) modes of expres-\nsion, and opposing these to the written modality.\narXiv:2305.04572v2  [cs.CL]  23 May 2023\ncentury (de Saussure, 1916; Bloomfield, 1933).2\nGiven these facts, is then the field of Natural\nLanguage Processing (NLP) a misnomer? Are\nwe making less progress with getting machines\nto communicate via human language than current\nadvances with processing written text would have\nus believe?\n2\nNLP is Written Language Processing\nTo anyone with experience reading, reviewing and\npublishing papers in NLP conferences and jour-\nnals (such the ACL conferences and TACL) it is\nevident that the field is very strongly focused on\nprocessing written language. While this is evident\nto practitioners, it is also largely tacit and implicit.\n2.1\nUnstated assumptions\nThe fact that a paper is concerned with written as\nopposed to spoken oral or sign language is almost\ninvariably assumed to be the default and not ex-\nplicitly stated. Furthermore, even if there is some\ninterest in tackling a dataset of originally spoken\nlanguage (for example in much work on dialog and\nchild language acquisition), the usual approach is\nto use a written transcription of this data rather than\nthe actual audio. This is partly a matter of conve-\nnience, but partly due to the assumption that the\nwritten form of language is the canonical one while\nthe audio modality is just a weird, cumbersome\nencoding of it.\nTo some extent such an implicit belief also lurks\nin much work within the speech community: the\nmain thrust of speech research has always been on\nso called Automatic Speech Recognition (ASR),\nby which is meant automatically transcribing spo-\nken language into a written form. Written text\nis treated as an interface and an abstraction bar-\nrier between the field of speech processing and\nNLP. In Sections 3 and 4 I address problems aris-\ning from the above assumptions, as well as the\nchallenges and opportunities we have once we dis-\ncard them. Firstly, however, it will be instructive\nto briefly quantify the assertion that NLP is Writ-\nten Language Processing. by looking at historical\npublication patterns.\n2.2\nPublication patterns\nFigure 1 shows the proportion of NLP papers ex-\nplicitly mentioning speech-related terms in their\n2However see Aaron and Joshi (2006) for a dissenting\nview.\n1950\n1970\n1990\n2010\nYear\n0\n0.05\n0.10\n0.15\n0.20\nFraction of speech papers\nFigure 1: The proportion of papers in the ACL anthol-\nogy up to year 2022 which mention the words speech,\nspoken or audio in the title, excluding those with part(s)-\nof-speech or speech act(s).\ntitle over the years covered by the ACL anthology\n(1950 through 2022), which is a comprehensive\ndatabase of NLP papers from a wide variety of rel-\nevant conferences, workshops and journals.3 The\nfraction of speech-focused NLP papers varies quite\na bit over the years, but mostly stays below 10%.\nThere is a large peak going to 20% in 1989, fol-\nlowed by three years with around 10% of speech\npapers. A look at the underlying data reveals that\nthe 1989 peak is associated with the inclusion in\nthe anthology of the proceedings of the Speech and\nNatural Language Workshop (Hirshman, 1989) or-\nganized by the US Defense Advanced Research\nProjects Agency (DARPA), and featuring 79 pa-\npers. This workshop ran until 1992 and is thus\nlargely responsible for the four-year run of sizable\nrepresentation of spoken language research in the\nACL anthology.\nThe overview of the last edition of this event\nnotes the then ongoing “paradigm shift in natu-\nral language processing towards empirical, corpus\nbased methods” (Marcus, 1992). It is likely that\nthis shift in NLP methodology was at least partly\ndriven by this workshop, the associated DARPA\nprogram, and the resulting increased interaction be-\ntween researchers working on spoken and written\nlanguage.\nIn recent years (since 2010) the proportion of\nNLP papers explicitly mentioning spoken language\nhas resolutely stayed below 6%. While the major\nACL events typically include speech processing\nas a topic in their calls for papers, as well as a\ntrack including the term speech in its name, such\nas Speech and Multimodality, processing of spoken\nlanguage it clearly a rather minor concern of these\nconferences. Instead, speech work is published in\ndifferent venues organized by a separate speech\n3https://aclanthology.org/\nprocessing community.\n3\nSpoken Language is Richer\nWhile the primacy of the spoken modality as means\nof communication is the consensus view in linguis-\ntics, Section 2.1 identifies unstated assumptions\namong NLP practitioners which amount to the op-\nposite view. Here I outline why these assumptions\ncontradicting the scientific view are not only incor-\nrect but also detrimental to progress on understand-\ning and processing real human language.\n3.1\nKey features of spoken language\nSpeech and writing are two different modalities\nwith different affordances, and there is no straight-\nforward mapping between them. Some writing\nsystems such as those used for English, Arabic or\nChinese do not even represent the phonology of\nthe spoken language in a direct way. More cru-\ncially, writing only captures a small proportion\nof the information carried in the equivalent audio\nsignal. Writing discards most of the information\nfalling within the general category of paralinguistic\nphenomena, such as that related to speaker iden-\ntity, speaker emotional state and attitude; likewise,\ninformation conveyed by speech tempo and am-\nplitude, including most of suprasegmental phonol-\nogy such as intonation and rhythm is typically not\npresent in writing. In addition to the auditory sig-\nnal, oral spoken language can also feature visual\nclues in the form of accompanying gestures, fa-\ncial expressions and body posture. Sign languages\nrely on the visual channel exclusively, and in fact\nthere are no widely used writing systems for any of\nthem (Grushkin, 2017). Unlike most text, speech\nalso typically contains a variable amount of chan-\nnel noise (Shannon, 1948) such as environmental\nsounds.\nNatural spontaneous speech contains fillers, hes-\nitations, false starts, repairs and other disfluencies\n(Dinkar et al., 2023) which are usually edited out in\nthe written form of language. Even more critically,\nspontaneous speech typically takes the form of a\ndialog between two or more participants. Dialog is\nunlike common written genres: crucially it features\nturn-taking behavior which is governed by com-\nplex and incompletely understood rules (Skantze,\n2021). These features of natural dialog also mean\nthat the traditional cascaded approach of ASR fol-\nlowed by NLP faces serious limitations, not least\ndue to low ASR performance in this regime (Szy-\nma´nski et al., 2020), but also due to its inherently\ninteractive nature.\nFor all these reasons, spoken language is more in-\nformationally rich than written language;4 the same\nfactors also make it more variable, complex and\nnoisy, and consequently more challenging for auto-\nmated processing (Shriberg, 2005). Thus any un-\nderstanding of language as a human faculty gained\nvia the written modality does not necessarily gen-\neralize to the spoken modality. The same is also\nthe case about language applications: for example\nthe successes and shortcomings of state-of-the-art\ntext chatbot systems (e.g. Stiennon et al., 2020) are\nlikely to be substantially different from those of\nspoken dialog systems.\n3.2\nChallenges of speech\nAs an illustrative example, let us consider the ef-\nfectiveness of self-supervision: inducing represen-\ntations of words and phrases from just listening to\nspeech or reading text. For text, this general family\nof methods has been successful since around the\ntime of Latent Semantic Analysis (Dumais, 2004),\nand currently large written language models ex-\nhibit a constantly expanding range of abilities (Wei\net al.). In contrast, self-supervision with spoken\nlanguage has met with a limited amount of success\nonly in the last few years (e.g. Baevski et al., 2020;\nHsu et al., 2021), and these models as of now are\nusually only fine-tuned on the task of ASR. One\nobvious difference is that items such as words and\nmorphemes are either explicitly delimited or easily\ndiscovered in text, but finding them is an unsolved\nresearch problem in speech, due to the inherent\nvariability of this modality.\nOn the other hand, learning spoken language be-\ncomes much more tractable when self-supervision\nis augmented with grounding in perception. The\ncross-modal correlations, though unreliable and\nnoisy, are often sufficient to substantially facilitate\nthe discovery and representation of words (Peng\nand Harwath, 2022; Nikolaus et al., 2022) and syl-\nlables (Peng et al., 2023) in spoken language. For\nwritten language, grounding in the visual modality\nhas also been found to help in some cases (e.g. Tan\nand Bansal, 2020) but it does not appear crucial,\nas the dominance of text-only language models\ndemonstrates.\nSince spoken language is richer in information\n4One exception to this general pattern is the presence of\ntwo spatial dimensions in written language, and the role of 2D\nlayout in textual publications.\ncontent, it should in principle be possible to ex-\nploit this extra signal for improving performance.\nOne obstacle to such developments is the increased\nvariability and channel noise. Perhaps less obvi-\nously, a second obstacle is that widely used bench-\nmarks are often designed in a way which obstructs\nobtaining such gains. For example the 2021 Ze-\nrospeech challenge (Dunbar et al., 2021) which\naimed to benchmark spoken language modeling,\nevaluates systems according to the following cri-\nteria: phoneme discrimination, word recognition,\nsyntactic acceptability and correlation to human\njudgments of word similarities. None of these met-\nrics would benefit much from modeling speaker\ncharacteristics, speech tempo, pitch, loudness or\neven suprasegmental phonology. Except for the\nfirst one, these metrics would be very well suited\nfor models trained exclusively on written language.\nThe combined effect of these two obstacles was\nevident in the results of Zerospeech 2021 where\nwritten-language toplines, such as RoBERTa (Liu\net al., 2019), outperformed spoken language mod-\nels on the latter three metrics, often by large mar-\ngins.\n4\nUnifying Speech Processing and NLP\nAs evident from the examples highlighted above,\nspoken language is in some ways quite different\nfrom written language and presents a distinct set of\nchallenges and potentials. In order to understand\nhow much progress the fields of speech and NLP\nare making in understanding and implementing\nhuman language, we need to take speech seriously\nqua language, not just a cumbersome modality, and\nmeasure our progress accordingly.\n4.1\nConverging methodology\nThe time is ripe for a closer integration of the\nspeech and NLP communities and for a unified\ncomputational science of language. The set of\nmethodologies used in speech and text processing\nused to be quite distinct in the past. Since the adop-\ntion of deep learning both fields have converged to\na large extent: currently the state-of-the-art mod-\nels for both spoken and written language rely on\ntransformer architectures (Vaswani et al., 2017)\nself-trained on large amounts of minimally prepro-\ncessed data, with optional fine-tuning. The tech-\nnical communication barriers across disciplinary\nboundaries are thus much lower. The recent emer-\ngence of the concept of textless NLP (Lakhotia\net al., 2021) exemplifies the potential of unifying\nthese two fields.\n4.2\nOpportunities\nThe following paragraphs outline the most impor-\ntant benefits of making NLP more natural, ranging\nfrom basic science to practical applications.\nModeling language acquisition.\nAn increased\nattention to spoken language within NLP has the\npotential to lead to a more realistic understanding\nof how well our current methods can replicate key\nhuman language abilities. Acquiring language un-\nder constraints that human babies face is the big\none. There is a large amount of work on modeling\nhuman language acquisition which uses exclusively\nwritten data (at best transcribed from the original\naudio). Hopefully by this point the reader will be\nconvinced that the relevance of this work to the\nactual issue under consideration is highly question-\nable. We stand a much better chance of figuring out\nhuman language acquisition if we refocus attention\non spoken language.\nData efficiency.\nLinzen (2020) argues convinc-\ningly for language models which are human-like in\ntheir data-efficiency and generalization capabilities.\nIt is, however, unclear whether these properties\ncan even be properly evaluated via the medium of\nwritten language. Since the informational density\nand the signal-to-noise ratio in written vs spoken\nlanguage are so very different, it makes little sense\nto compare human children with language models\ntrained on text. Furthermore, the challenges of pure\nself-supervision may motivate us to take seriously\nthe impact of grounding in perception and interac-\ntion, which humans use universally as a learning\nsignal.\nUnwritten languages.\nMany modes of human\ncommunication lack standard written representa-\ntion. These range from major languages spoken by\nmillions of people such as Hokkien (Mair, 2003),\nto small or non-standard language varieties, to sign\nlanguages. Shifting the emphasis of NLP research\nfrom text to the primary, natural oral and gestural\nmodalities will benefit the communities using these\nvarieties.\nSpoken dialog systems.\nDingemanse and Liesen-\nfeld (2022) argue that language technology needs\nto transition from the text to talk, and provide a\nroadmap of how to harness conversational corpora\nin diverse languages to effect such a transition. In-\ndeed, one of the most obvious benefits of spoken\nlanguage NLP would be dialog systems that do not\nneed to rely on ASR and are able to exploit the\nextra information lost when transcribing speech,\nenabling them to understand humans better and\ninteract with them in a more natural way.\nNon-textual language data.\nFinally, there is a\nlarge and increasing stream of non-textual language\ndata such as podcasts, audio chat channels and\nvideo clips. Processing such content could also\nbenefit from an end-to-end holistic treatment with-\nout the need of going through the lossy conversion\nto text.\n4.3\nRecommendations\nIf you are an NLP practitioner and view spoken\nlanguage as outside the scope of your field, recon-\nsider. Getting into speech processing does require\nunderstanding its specifics, but it is not as techni-\ncally daunting as it used to. Conversely, if you are\na speech researcher, consider that ASR and text-to-\nspeech is not all there is: we can get from sound to\nmeaning and back without going through the writ-\nten word. Both fields would do well to consider\nthe whole of human language as their purview. In-\ncreased collaboration would benefit both communi-\nties, and more importantly, would give us a chance\nof making real progress towards understanding and\nsimulating natural language.\n5\nLimitations\nThe main limitation of this paper is the one ap-\nplying to any opinion piece: it is subjective and\npersonal, as the views of the authors are inherently\nlimited by their expertise and experience. More\nspecifically, this paper argues for an increased inter-\naction between the speech and NLP communities,\nbut the author is more strongly embedded in the\nlatter, and thus addresses this audience primarily.\nAdditionally, the short paper format imposes sig-\nnificant constraints on the amount of nuance, detail\nand discussion of relevant literature, and thus read-\ners may find some of the claims to be less strongly\nsupported and less hedged than would be ideal, or\nproper in a longer treatment of this topic.\nAcknowledgements\nI would like to thank Hosein Mohebbi, Afra Al-\nishahi, Mark Dingemanse, Tanvi Dinkar, Piotr Szy-\nma´nski and three anonymous reviewers for their\nvaluable feedback on this paper.\nReferences\nP. G. Aaron and R. Malatesha Joshi. 2006. Written lan-\nguage is as natural as spoken language: A biolinguis-\ntic perspective. Reading Psychology, 27(4):263–311.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations.\nAdvances in Neural Information Processing Systems,\n33:12449–12460.\nLeonard Bloomfield. 1933. Language. Henry Holt,\nNew York.\nCharles Darwin. 1874. The descent of man, and selec-\ntion in relation to sex. D. Appleton and Company,\nNew York.\nFerdinand de Saussure. 1916. Cours de linguistique\ngénérale. Payot, Paris.\nTerrence W Deacon. 1998. The Symbolic Species: The\nCo-evolution of Language and the Brain. WW Nor-\nton & Company.\nMark Dingemanse and Andreas Liesenfeld. 2022. From\ntext to talk: Harnessing conversational corpora for\nhumane and diversity-aware language technology.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 5614–5633, Dublin, Ireland.\nAssociation for Computational Linguistics.\nTanvi Dinkar, Chloé Clavel, and Ioana Vasilescu. 2023.\nFillers in spoken language understanding: Computa-\ntional and psycholinguistic perspectives. Traitement\nAutomatique des Langues, 63(3).\nSusan T Dumais. 2004. Latent semantic analysis. An-\nnual Review of Information Science and Technology\n(ARIST), 38:189–230.\nEwan Dunbar, Mathieu Bernard, Nicolas Hamilakis,\nTu Anh Nguyen, Maureen de Seyssel, Patricia\nRozé, Morgane Rivière, Eugene Kharitonov, and Em-\nmanuel Dupoux. 2021. The Zero Resource Speech\nChallenge 2021: Spoken language modelling.\nSteven Roger Fischer. 2003. History of writing. Reak-\ntion books.\nDonald A Grushkin. 2017. Writing signed languages:\nWhat for? What form? American annals of the deaf,\n161(5):509–527.\nLynette Hirshman. 1989.\nOverview of the DARPA\nspeech and natural language workshop. In Proceed-\nings of the workshop on Speech and Natural Lan-\nguage, pages 1–2.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,\nKushal Lakhotia, Ruslan Salakhutdinov, and Abdel-\nrahman Mohamed. 2021. Hubert: Self-supervised\nspeech representation learning by masked prediction\nof hidden units. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 29:3451–3460.\nKushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu,\nYossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh\nNguyen, Jade Copet, Alexei Baevski, Abdelrahman\nMohamed, and Emmanuel Dupoux. 2021. On gen-\nerative spoken language modeling from raw audio.\nTransactions of the Association for Computational\nLinguistics, 9:1336–1354.\nTal Linzen. 2020.\nHow can we accelerate progress\ntowards human-like linguistic generalization?\nIn\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5210–\n5217, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach.\nVictor H Mair. 2003. How to forget your mother tongue\nand remember your national language.\nMitchell P Marcus. 1992. Overview of the fifth DARPA\nspeech and natural language workshop. In Proceed-\nings of the workshop on Speech and Natural Lan-\nguage, pages 3–4.\nMitja Nikolaus, Afra Alishahi, and Grzegorz Chrupała.\n2022. Learning English with Peppa Pig. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:922–936.\nPuyuan Peng and David Harwath. 2022. Word Discov-\nery in Visually Grounded, Self-Supervised Speech\nModels. In Proc. Interspeech 2022, pages 2823–\n2827.\nPuyuan Peng, Shang-Wen Li, Okko Räsänen, Abdelrah-\nman Mohamed, and David Harwath. 2023. Syllable\ndiscovery and cross-lingual generalization in a visu-\nally grounded, self-supervised speech mode. In Proc.\nInterspeech 2023.\nSteven Pinker and Paul Bloom. 1990. Natural language\nand natural selection. Behavioral and Brain Sciences,\n13(4):707–727.\nPeter J Richerson and Robert Boyd. 2010. Why possibly\nlanguage evolved. Biolinguistics, 4(2-3):289–306.\nClaude Elwood Shannon. 1948. A mathematical theory\nof communication. The Bell system technical journal,\n27(3):379–423.\nElizabeth Shriberg. 2005. Spontaneous speech: how\npeople really talk and why engineers should care. In\nProc. Interspeech 2005, pages 1781–1784.\nGabriel Skantze. 2021. Turn-taking in conversational\nsystems and human-robot interaction: A review.\nComputer Speech & Language, 67:101178.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 3008–3021. Curran Associates,\nInc.\nPiotr Szyma´nski,\nPiotr\n˙Zelasko,\nMikolaj Morzy,\nAdrian Szymczak, Marzena ˙Zyła-Hoppe, Joanna Ba-\nnaszczak, Lukasz Augustyniak, Jan Mizgajski, and\nYishay Carmiel. 2020. WER we are and WER we\nthink we are. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 3290–\n3295, Online. Association for Computational Lin-\nguistics.\nHao Tan and Mohit Bansal. 2020. Vokenization: Im-\nproving language understanding with contextualized,\nvisual-grounded supervision. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2066–2080,\nOnline. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\nEmergent abilities of large language models. Trans-\nactions on Machine Learning Research.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "eess.AS"
  ],
  "published": "2023-05-08",
  "updated": "2023-05-23"
}