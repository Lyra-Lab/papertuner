{
  "id": "http://arxiv.org/abs/2305.04843v1",
  "title": "Reinforcement Learning for Topic Models",
  "authors": [
    "Jeremy Costello",
    "Marek Z. Reformat"
  ],
  "abstract": "We apply reinforcement learning techniques to topic modeling by replacing the\nvariational autoencoder in ProdLDA with a continuous action space reinforcement\nlearning policy. We train the system with a policy gradient algorithm\nREINFORCE. Additionally, we introduced several modifications: modernize the\nneural network architecture, weight the ELBO loss, use contextual embeddings,\nand monitor the learning process via computing topic diversity and coherence\nfor each training step. Experiments are performed on 11 data sets. Our\nunsupervised model outperforms all other unsupervised models and performs on\npar with or better than most models using supervised labeling. Our model is\noutperformed on certain data sets by a model using supervised labeling and\ncontrastive learning. We have also conducted an ablation study to provide\nempirical evidence of performance improvements from changes we made to ProdLDA\nand found that the reinforcement learning formulation boosts performance.",
  "text": "Reinforcement Learning for Topic Models\nJeremy Costello and Marek Z. Reformat\nDepartment of Electrical and Computer Engineering\nUniversity of Alberta\n{jeremy1, reformat}@ualberta.ca\nAbstract\nWe apply reinforcement learning techniques to\ntopic modeling by replacing the variational au-\ntoencoder in ProdLDA with a continuous ac-\ntion space reinforcement learning policy. We\ntrain the system with a policy gradient algo-\nrithm REINFORCE. Additionally, we intro-\nduced several modiﬁcations: modernize the\nneural network architecture, weight the ELBO\nloss, use contextual embeddings, and monitor\nthe learning process via computing topic diver-\nsity and coherence for each training step. Ex-\nperiments are performed on 11 data sets. Our\nunsupervised model outperforms all other un-\nsupervised models and performs on par with\nor better than most models using supervised la-\nbeling. Our model is outperformed on certain\ndata sets by a model using supervised labeling\nand contrastive learning. We have also con-\nducted an ablation study to provide empirical\nevidence of performance improvements from\nchanges we made to ProdLDA and found that\nthe reinforcement learning formulation boosts\nperformance.\n1\nIntroduction\nThe internet contains large collections of unlabeled\ntextual data. Topic modeling is a method to extract\ninformation from this text by grouping documents\ninto topics and linking these topics with words\ndescribing them. Classical techniques for topic\nmodeling, the most popular being Latent Dirich-\nlet Approximation (LDA) (Blei et al., 2003), have\nrecently begun to be overtaken by Neural Topic\nModels (NTM) (Zhao et al., 2021).\nProdLDA (Srivastava and Sutton, 2017) is a\nNTM using a product of experts in place of the\nmixture model used in classical LDA. ProdLDA\nuses a variational autoencoder (VAE) (Kingma and\nWelling, 2013) to learn distributions over topics\nand words. ProdLDA improved on NVDM (Miao\net al., 2016) by explicitly approximating the Dirich-\nlet prior from LDA with a Gaussian distribution\nand using the Adam optimizer (Kingma and Ba,\n2014) with a higher momentum and learning rate.\nPerceiving Reinforcement Learning (RL) as\nprobabilistic inference has brought practices of\nsuch an inference into the RL ﬁeld (Dayan and Hin-\nton, 1997) (Levine, 2018). New algorithms using\nthese techniques include MPO (Abdolmaleki et al.,\n2018) and VIREL (Fellows et al., 2019). MPO op-\ntimizes the evidence lower bound (ELBO), which\nis the same optimization objective used in VAEs.\nInspired by the adoption of probabilistic infer-\nence techniques in RL, we look to apply RL tech-\nniques to probabilistic inference in the realm of\ntopic models. We use REINFORCE, the simplest\npolicy gradient (PG) algorithm, to train a model\nwhich parameterizes a continuous action space, cor-\nresponding to the distribution of topics for each\ndocument in the topic model. We keep the product\nof experts from ProdLDA to compute the distri-\nbution of words for each document in the topic\nmodel.\nWe additionally improve our topic model\nby using Sentence-BERT (SBERT) embeddings\n(Reimers and Gurevych, 2019) rather than bag-of-\nword (BoW) embeddings, modernizing the neu-\nral network (NN) architecture, adding a weighting\nterm to the ELBO, and tracking topic diversity and\ncoherence metrics throughout training. The model\narchitecture is shown in Figure 1. Our method\noutperforms most other topic models. It is beaten\nonly on some data sets by advanced methods using\ndocument labels for supervised learning, while our\nprocedure is fully unsupervised.\n2\nRelated Work\nZhao et al. (2021) provide a survey of NTMs. Vari-\nations of VAEs are presented which use different\ndistributions, correlated and structured topics, pre-\ntrained language models, incorporate meta-data, or\nmodel on short texts rather than documents. Meth-\nods other than VAEs are also used for NTMing,\narXiv:2305.04843v1  [cs.CL]  8 May 2023\nSBERT\nembedding\nMean Network\n(with GELU)\nNetwork\n(with Layer \nNormalization)\nembeddings\nvariance\n(vector)\nKL\ncalculations\nLaplace approximation \nto the Dirichlet \nprior distribution\nmean\n(vector)\nsampling process\nPOLICY\nwords over documents\nwords over topic\nlog\nlikelihood\ntopics over document\nword \ndistribution\ndocument: \nbag of words\nReward\nAction\nposterior \ndistribution\n(theta)\nbeta \ndistribution\nVariance Network \n(with GELU and Layer \nNormalization)\ndocument\ndocument\ndocument\ndocument\ndocument\ndocument\nbatches\nbatches\ntopics over document\nFigure 1: Architecture Diagram: gray boxes - processing; white boxes - models/data/information; arrows across\nboxes - tune-ability\nincluding autoregressive models, generative adver-\nsarial networks, and graph NNs.\nDoan and Hoang (2021) compare ProdLDA and\nNVDM, along with six other NTMs and three clas-\nsical topic models, in terms of held-out document\nand word perplexity, downstream classiﬁcation,\nand coherence. Scholar (Card et al., 2017), an ex-\ntension of ProdLDA taking document metadata and\nlabels into account where possible, performed best\nin terms of coherence. NVDM and NVCTM (Liu\net al., 2019), an extension of NVDM which addi-\ntionally models the correlation between documents,\nperformed best in terms of perplexity and down-\nstream classiﬁcation. The other NTMs were GSM\n(Miao et al., 2017), NVLDA (Srivastava and Sutton,\n2017), NSMDM (Lin et al., 2019), and NSMTM\n(Lin et al., 2019). The classical topic models were\nnon-negative matrix factorization (NMF) (Zhao\net al., 2017), online LDA (Hoffman et al., 2010),\nand Gibbs sampling LDA (Grifﬁths and Steyvers,\n2004).\nBERTopic (Grootendorst, 2022) and Top2Vec\n(Angelov, 2020) use dimensionality reduction and\nclustering to group document embeddings from\npre-trained language models into meaningful clus-\nters. Contextualized Topic Models (CTM) (Bianchi\net al., 2020a) augments the BoW embeddings used\nin ProdLDA with SBERT (Reimers and Gurevych,\n2019) embeddings, resulting in an improved topic\nmodel.\nDieng et al. (2020) develop the embedded topic\nmodel (ETM) by using word embeddings to aug-\nment a variational inference algorithm for topic\nmodeling. Their method outperforms other topic\nmodels, especially on corpora with large vocab-\nularies containing common and very rare words.\nNguyen and Luu (2021) augment Scholar (Card\net al., 2017) with contrastive learning (Hadsell\net al., 2006) and outperform all topic models com-\npared against.\nGui et al. (2019) use RL to ﬁlter words from\ndocuments, with reward as a combination of the\nresulting topic model’s coherence and diversity, or\nhow few words overlap between topics. Kumar\net al. (2021) use REINFORCE (Williams, 1992),\na PG RL algorithm, to augment ProdLDA. Their\nmodel slightly outperforms ProdLDA in terms of\ntopic coherence.\n3\nBackground\nWe brieﬂy outline topic models, RL process, KL\ndivergence, and contextual embeddings.\n3.1\nTopic Models – Approaches\nLatent Dirichlet Allocation (LDA)\n(Blei et al.,\n2003) is a three-level hierarchical Bayesian model:\ndocuments →topics →words. Each document is\na mixture over latent topics, where the topic dis-\ntribution θ is randomly sampled from a Dirichlet\ndistribution. Each topic is a multinomial distribu-\ntion over vocabulary words.\nAutoencoding Variational Inference for Topic\nModels (AVITM)\n(Srivastava and Sutton, 2017)\nis a neural topic model using a VAE to learn a\nGaussian distribution over topics. VAEs use a repa-\nrameterization trick (RT) to randomly sample from\nthe posterior distribution to remain fully differen-\ntiable. At the time, there was no known RT for\nDirichlet distributions, so AVITM used a Gaussian\ndistribution and a Laplace approximation of the\nDirichlet prior.\nAVITM contains two models: NVLDA and\nProdLDA. NVLDA uses the mixture model from\nLDA to infer a distribution over vocabulary words,\nwhile ProdLDA uses a product of experts.\nEvidence Lower Bound (ELBO)\nis the opti-\nmization objective for AVITM. ELBO optimiza-\ntion (Jordan et al., 1999) simultaneously tries to\nmaximize the log-likelihood of the topic model and\nminimize the forward Kullback–Leibler (KL) di-\nvergence (Kullback and Leibler, 1951) between the\nposterior P and prior Q topic distributions.\nELBO = DKL(P||Q) −log-likelihood\n(1)\n3.2\nTopic Models – Evaluation\nTopic Coherence\nis a metric for evaluating topic\nmodels. It uses co-occurence in a reference corpus\nto measure semantic similarity between the top-K\nwords in a topic. Topic model coherence is the\naverage of each topic’s coherence.\nNormalized\npointwise\nmutual\ninformation\n(NPMI) (Aletras and Stevenson, 2013) was the\ncoherence measure found to correlate best with\nhuman judgment (Lau et al., 2014). When comput-\ning NPMI, a window size of 20 for co-occurrence\ncounts is used in Srivastava and Sutton (2017),\nwhile Dieng et al. (2020) uses full document co-\noccurrence.\nNPMI coherence is calculated for each of the top-\nK words in a topic and averaged to obtain the co-\nherence for that topic. The overall topic-coherence\nis the average of the coherence for each topic. For\na word i, the NPMI coherence is calculated accord-\ning to Equation 2.\nNPMI(wi) =\nK−1\nX\nj\nlog\nP(wi,wj)\nP(wi)P(wj)\n−log P(wi, wj)\n(2)\nwhere P(wi) is the probability of word i occur-\nring in a document in the corpus, and P(wi, wj) is\nthe probability of words i and j co-occurring in a\ndocument in the corpus.\nTopic Diversity\nis another metric for evaluating\ntopic models. It measures the uniqueness of the\ntop-K words across all topics. Dieng et al. (2020)\nuse K = 25 for reporting topic diversity.\ntopic-diversity = number-of-unique-words\nK ∗number-of-topics\n(3)\nTopic Quality\nis a topic modeling metric intro-\nduced by Dieng et al. (2020).\n(4)\ntopic-quality = topic-coherence\n∗topic-diversity\n3.3\nReinforcement Learning\nRL is a sequential decision-making framework fo-\ncused on ﬁnding the best sequence of actions exe-\ncuted by an agent. (Sutton and Barto, 2018). An\nagent takes actions a ∈A to traverse between\nstates s ∈S in an environment, receiving a reward\nr on each transition. The goal of an RL task is\nto ﬁnd the best set of actions —referred to as the\npolicy —which maximizes the reward. RL prob-\nlems can be episodic, where the agent completes\nthe environment and is reset, or continuing, where\nthe agent continuously traverses the environment\nwithout reset. Through traversing the environment,\nthe agent learns a policy π of which actions in each\nstate will maximize return. Return is the cumula-\ntive reward received by the agent in an episode or\nits lifetime. It is usually discounted by a factor γ\nto favor near-term reward over long-term reward.\nAn alternative to discounting is the average reward\nformulation.\nPolicy Gradient (PG) Algorithms\nMany RL al-\ngorithms learn a value function – representing val-\nues associated with selecting speciﬁc actions – and\na corresponding policy that chooses the action or\nsubsequent state with maximum value. PG algo-\nrithms (Sutton et al., 1999) provide an alternative\napproach directly learning a parameterized policy.\nThe parameters of the policy function are optimized\nthrough stochastic gradient ascent.\nREINFORCE\nis a Monte Carlo PG algorithm\nfor episodic problems (Williams, 1992). See algo-\nrithm 1, where ρ is a vector of optimized parame-\nters.\nAlgorithm 1: REINFORCE\nInput: A differentiable parameterized\npolicy function π(a|s, ρ)\nAlgorithm Parameters:\nstep size α > 0,\ndiscount factor γ < 1\n1 Initialize ρ (e.g. ρ ∼N(0, 0.02))\n2 for each episode do\n3\nGenerate an episode\n4\ns0, a0, r1, . . . , sT−1, aT−1, rT\n5\nfollowing policy π\n6\nfor each step in the episode (t from 0 to\nT −1) do\n7\nG ←PT\nk=t+1 γk−t−1rk\n8\nρ ←ρ + αγtG∇ln π(at|st, ρ)\n9\nend\n10 end\nContinuous Action Spaces\nare one advantage\nof PG algorithms (Sutton and Barto, 2018). Pa-\nrameterized policies allow action spaces that are\nparameterized by a probability distribution, such as\na Gaussian. For Gaussian action spaces, the mean\nµ and standard deviation σ are given by function\napproximators parameterized by ρ. For a state s,\nan action a is sampled from the distribution and the\npolicy is updated according to Equation 5.\nπ(a|s, ρ) .=\n1\nσ(s, ρ)\n√\n2π exp\n\u0012\n−(a −µ(s, ρ))2\n2σ(s, ρ2)\n\u0013\n(5)\nKullback-Leibler (KL) Divergence\n(Kullback\nand Leibler, 1951) measures the similarity between\ntwo probability distributions P and Q. It is used in\nAVITM (Srivastava and Sutton, 2017) to force the\nposterior distribution parameterized by the VAE\nto be the Laplace approximation of the Dirichlet\nprior. The KL divergence calculation for N topics\nis shown in Equation 6.\n(6)\nDKL(P||Q) = 1\n2\nN\nX\n1\n \n(µP −µQ)2\nσ2\nQ\n+\nσ2\nP\nσ2\nQ\n−log σ2\nP\nσ2\nQ\n−1\n!\n3.4\nContextual Embeddings\nContextual embeddings dominate NLP tasks, re-\nplacing earlier methods, including Word2Vec\n(Mikolov et al., 2013), GloVe (Pennington et al.,\n2014), and BoW. Words and sequences of words\nare encoded into vector embeddings by large Trans-\nformer models (Vaswani et al., 2017).\nThe BoW document representation used in\nProdLDA is augmented with contextual embed-\ndings from SBERT Bianchi et al. (2020a). They\ntest three models: one with BoW, one with con-\ntextual embeddings, and one with both. They ﬁnd\nthat using both embeddings produces the best re-\nsults, and the other two methods perform almost\nas well. One advantage of using solely contextual\nembeddings is that multilingual language models\ncan encode documents from different languages\ninto the same embedding space, enabling easy cre-\nation of multilingual topic models (Bianchi et al.,\n2020b).\nSentence-BERT\nis an extension of BERT using\na Siamese network to extract semantically meaning-\nful sentence embeddings (Reimers and Gurevych,\n2019). In contrast to BERT, this allows SBERT\nembeddings to be compared using dot product or\ncosine similarity, making SBERT more suitable\nfor tasks such as semantic similarity search and\nclustering.\n4\nMethodology\n4.1\nModernizing ProdLDA\nFollowing Liu et al. (2022), we contemporize\nthe architecture of the inference network within\nProdLDA. We replace the SoftPlus activation func-\ntion (Glorot et al., 2011) with a GELU activation\nfunction (Hendrycks and Gimpel, 2016), replace\nbatch normalization (Ioffe and Szegedy, 2015) with\nlayer normalization (Ba et al., 2016), and replace\nall Xavier initialization (Glorot and Bengio, 2010)\nwith ρ ∼N(0, 0.02).\nFor the inference network, we increase the num-\nber of units in each layer from 100 to 128, add\nweight decay of 0.01 to each layer, and place\ndropout layers (Srivastava et al., 2014) after each\nfully connected layer.\nWe replace the softmax activation after the topic\ndistribution with an RL policy formulation (Equa-\ntion 5). We use a training batch size of 1024. We\nclip all gradients to a maximum norm of 1.0 to\nprevent gradient explosion (Pascanu et al., 2013).\nFollowing Bianchi et al. (2020a), we set both distri-\nbutional priors as trainable parameters. We lower\noptimizer learning rate to 3e-4 and momentum to\n0.9.\n4.2\nDocument Embeddings\nFollowing Bianchi et al. (2020a), we replace the\nBoW used by ProdLDA with contextualized em-\nbeddings from SBERT. We use the \"all-MiniLM-\nL6-v2\" model for encoding unpreprocessed docu-\nments as embedding vectors. BoW embeddings,\nused to calculate the log-likelihood of the topic\nmodel, are created using preprocessed documents.\n4.3\nSingle-step REINFORCE with a\nContinuous Action Space\nWe adopt the view of RL as a statistical inference\nmethod (Levine, 2018). The modernized inference\nnetwork from ProdLDA is used to parameterize\na continuous action space from which an action\nis sampled, and the policy is computed according\nto Equation 5. The topic model distribution over\nvocabulary words uses the product of experts from\nProdLDA. We use REINFORCE to train the net-\nwork, with a weighted version of ELBO as the\nreward. Each document embedding is a state in the\nenvironment, and each episode terminates after a\nsingle step (i.e., action). Each action is a sample\nfrom the topic distribution.\n4.4\nWeighted Evidence Lower Bound\nFollowing Higgins et al. (2016), we allow modiﬁ-\nable relative entropy between the prior and poste-\nrior by weighting the KL divergence term in the\nELBO. We deﬁne a hyperparameter λ as a multi-\nplier on the KL divergence term.\nELBOweighted = λDKL(P||Q) −log-likelihood\n(7)\n5\nResults\n5.1\nInitial Experiments\nWe initially evaluate our topic model on the 20\nNewsgroups data set with 20 topics. Results av-\neraged over 30 random seeds are shown: loss in\nFigure 2, topic coherence in Figure 3, and topic\ndiversity in Figure 4. Mean and 90% conﬁdence\nintervals are plotted. Topic diversity and coherence\nare calculated with K = 10. Documents are pre-\nprocessed following Bianchi et al. (2020a) with the\nadditional step of removing all words with less than\nthree letters. Models are trained for 1000 epochs\nwith the AdamW optimizer (α = 3e −4, β1 = 0.9,\nβ2 = 0.999). We use λ = 5, inference network\ndropout of 0.2, and no dropout after the RL policy\n(policy dropout). All other experiments use these\nsame settings unless otherwise noted.\nFigure 2: Loss (30 seeds): 20 Newsgroups\nFigure 3: Topic Coherence (30 seeds): 20 Newsgroups\nFigure 4: Topic Diversity (30 seeds): 20 Newsgroups\n5.2\nComparison to Other Topic Models\nWe compare our method to recent topic models\nfound in the literature.\n5.2.1\nBenchmarking Neural Topic Models\n(BNTM)\nIn the beginning, our approach is compared with\nall models evaluated by Doan and Hoang (2021).\nWe use their preprocessed documents and replicate\ntheir results using K = 10 to calculate topic coher-\nence. Following the authors, we sweep from 0.5*N\ntopics to 3*N topics in intervals of 0.5*N (N being\nthe \"correct\" number of topics for each data set).\nNext, we do a hyperparameter sweep over λ of 1,\nFigure 5: Comparison of RL model (ours) to BNTM models\n3, 5, and 10. Results are averaged over ten random\nseeds and shown in Figure 5.\n5.2.2\nTopic Modeling in Embedding Spaces\nNext, the comparison is done with Dieng et al.\n(2020) on the New York Times data set with 300\ntopics and without using stop words. Results are\nshown in Table 1. We increase batch size to 32768\nand only train for 20 epochs on one random seed.\nAdditionally, we increase the number of units in\neach layer of the inference network to 512, increase\ndropout in the inference network to 0.5, and de-\ncrease λ to 1. Topic diversity is calculated using\nK = 25.\n5.2.3\nPre-training is a Hot Topic (PTHT)\nWe also compare our model, using all metrics,\nwith the best model as evaluated by Bianchi et al.\n(2020a). Results are shown in Table 2. Metrics are\naveraged over 25, 50, 75, 100, and 150 topics: 30\nModel\nCoherence\nDiversity\nQuality\nETM\n0.18\n0.22\n0.0405\nRL model\n(ours)\n0.24\n0.32\n0.0778\nTable 1: Comparison on no stop words data\nseeds for each number of topics. We use the same\npreprocessing as the authors. We use λ = 1.\n5.2.4\nContrastive Learning for NTM\n(CLNTM)\nWe compare results with the contrastive Scholar\nmodel from Nguyen and Luu (2021). For each\ndata set we perform a hyperparameter search with\n50 topics. Search ranges and best results for each\ndata set are shown in Table 3. We use the best\nhyperparameters from this search for ﬁnal training\nruns with 50 and 200 topics. We train for 2000\nData Set\nPaper\nNPMI\nWord2Vec\nInverse RBO\nWiki20K\nPTHT best\n0.1823\n0.2110\n0.9950\nRL model (ours)\n0.2509\n0.2368\n0.9799\nStackOverﬂow\nPTHT best\n0.0280\n0.1598\n0.9914\nRL model (ours)\n0.1249\n0.1617\n0.9860\nGoogle News\nPTHT best\n0.1207\n0.1325\n0.9965\nRL model (ours)\n0.3563\n0.1485\n0.9934\nTweets2011\nPTHT best\n0.1008\n0.1493\n0.9956\nRL model (ours)\n0.3559\n0.1417\n0.9962\n20 Newsgroups\nPTHT best\n0.1300\n0.2539\n0.9931\nRL model (ours)\n0.2696\n0.1798\n0.9932\nTable 2: Average metrics from best PTHT model (per metric) and our RL model\nExperiment\nLayer Size\nInference Dropout\nPolicy Dropout\nλ\nHyperparameter Search\n{128, 512}\n{0.2, 0.5}\n{0.0, 0.25, 0.5}\n{1, 5}\n20 Newsgroups\n128\n0.5\n0.5\n1\nIMDb Movie Reviews\n512\n0.5\n0.25\n1\nWikitext-103\n512\n0.5\n0.25\n5\nTable 3: Hyperparameter search and best results per data set for RL model\nepochs. Results are averaged over 30 random seeds\nand shown in Table 4.\nTo show the tradeoff between topic diversity and\ncoherence, we perform a sweep over policy dropout\nfrom 0 to 0.9 at intervals of 0.1 using the 20 News-\ngroups data set with 50 topics. Other hyperparam-\neters are kept the same. We train for 2000 epochs.\nResults are averaged over 30 random seeds and\nshown in Figure 6.\nFigure 6: Dropout sweep for 20 Newsgroups\n5.3\nAblation Study\nTo provide empirical evidence that performance\nimprovements come from the RL policy formula-\ntion, we do a study ablating relevant changes from\nthe ﬁnal RL model down to the original ProdLDA\nmodel. All comparisons are performed on the 20\nNewsgroups data set with 20 topics and use the\nsame settings as subsection 5.1. Results are aver-\naged over 30 random seeds and shown in Table 5.\n6\nDiscussion\nFor the initial experiments on the 20 Newsgroups\ndata set, the average loss (Figure 2) reaches a near\nplateau around the 200th epoch. Past this epoch,\ncoherence (Figure 3) continues to increase slowly,\nand topic diversity (Figure 4) increases substan-\ntially until around the 400th epoch, past which it\nalso continues to increase slowly. It shows that\ntraining beyond a plateau in loss can still improve\nNTM performance.\nCompared to Doan and Hoang (2021), the RL\nmodel performs on par with or better than other\nmodels across all four data sets, while the perfor-\nmance of other models varies greatly between data\nsets. On the Snippets, 20 Newsgroups, and W2E-\ncontent data sets, the RL model with lower values\nof λ usually performs better as the number of topics\nincreases. However, it reverses on the W2E-title\ndata set where λ = 10 outperforms λ = 1 on the\ntwo highest number of topics.\nThe RL model outperforms the Labeled ETM\nmodel from Dieng et al. (2020) in topic diversity,\ncoherence, and quality. Furthermore, this compari-\nModel\n20 Newsgroups\nIMDb Movie Reviews\nWikitext-103\n50 Topics\n200 Topics\n50 Topics\n200 Topics\n50 Topics\n200 Topics\nContrastive Scholar\n0.334\n0.280\n0.197\n0.188\n0.497\n0.478\nRL model (ours)\n0.449\n0.308\n0.199\n0.139\n0.432\n0.268\nTable 4: Comparison to CLNTM\nRL Policy\nEmbedding\nλ\nθ Softmax\nθ / Policy Dropout\nCoherence\nDiversity\n✓\nSBERT\n5\n×\n0.0\n0.3848\n0.9530\n×\nSBERT\n5\n×\n0.0\n0.2795\n0.453\n✓\nBoW\n5\n×\n0.0\n0.3379\n0.9403\n✓\nSBERT\n1\n×\n0.0\n0.3414\n0.9070\n✓\nSBERT\n5\n✓\n0.0\n0.1932\n0.6927\n✓\nSBERT\n5\n×\n0.2\n0.3769\n0.7315\n×\nBoW\n1\n✓\n0.2\n0.2650\n0.7390\nTable 5: Highlighted results from ablation study\nson had no pruning of stop words, showing the RL\nmodel can deal with vocabularies containing many\ncommon words.\nCompared to Bianchi et al. (2020a), the RL\nmodel signiﬁcantly outperforms all other models\non all data sets evaluated in terms of NPMI coher-\nence. Furthermore, the RL model performs sim-\nilarly to the best of the other models in terms of\ninverse RBO. We state the topic diversity used by\nDieng et al. (2020) is a more useful metric than\ninverse RBO, as it usually has a higher variance\nin values and is more intuitive to understand. For\nWord2Vec coherence, the RL model performs on\npar with the best of the other models, except when\ncompared to ETM (Dieng et al., 2020) on the 20\nNewsgroups data set.\nIf we consider models from Nguyen and Luu\n(2021), our RL model performs similarly on 50\ntopics but worse on 200 topics. The RL model’s\nperformance on larger topic sizes and vocabularies\ncould be improved by adding supervised labels,\napplying contrastive learning, scaling up inference\nlayer sizes, or performing a hyperparameter sweep\nwith 200 topics.\nTopic diversity and coherence values should be\nprovided when reporting topic model performance.\nIn Figure 6, the highest topic quality is achieved\nwhen there is no policy dropout. Topic diversity\ncan be sacriﬁced for some gain in coherence. Ap-\nplications of topic models may want to maximize\ntopic diversity, coherence, or both. The description\nof topic model performance should reﬂect this.\nIn the ablation study, removing the RL policy for-\nmulation causes the model to perform worse than\nthe original one. It conﬁrms RL policy augments\nthe improvements from other changes to the model.\nPerformance suffers the most when the softmax\ndistribution is re-added to the topic distribution dur-\ning training. To recapture the softmax distribution\nof topics, it can be applied to the topic distribution\nduring inference. Adding policy dropout signiﬁ-\ncantly reduces topic diversity and leads to a slight\ncoherence reduction. Performance improves with\nSBERT embeddings, and the model can still recon-\nstruct the BoW within the ELBO without direct\naccess. Increasing λ to 5 improves performance,\nbut as seen from other experiments, this is only\nsometimes the case.\n7\nConclusion\nInspired by the introduction of probabilistic infer-\nence techniques to RL, we take the approach to\ndevelop a NTM augmented with RL. Our model\nbuilds on the ProdLDA model, which uses a prod-\nuct of experts instead of the mixture model used in\nclassical LDA. We improve ProdLDA by adding\nSBERT embeddings, an RL policy formulation, a\nweighted ELBO loss, and the improved NN archi-\ntecture. In addition, we track topic diversity and\ncoherence during a training process rather than only\nevaluating these metrics for the ﬁnal model. Our\nfully unsupervised RL model outperforms most\nother topic models. It is only topped by contrastive\nScholar —a method using supervised labels during\ntraining —in a few select cases.\n8\nLimitations\nThe main limitation identiﬁed for our RL model\nis decreased performance as the vocabulary size\nincreases. Our RL model also has a higher vari-\nance than some other topic models to which we\ncompared. While our RL model performed well on\nall the data sets tested, this performance may not\ngeneralize to different data sets. The insights from\nthe policy dropout sweep conducted may not apply\nto other topic models. The performance difference\nfor NPMI coherence compared with Bianchi et al.\n(2020a) may be overstated since the model in that\npaper used a deprecated SBERT model that pro-\nduces sentence embeddings of low quality1. For the\ncomparison to Nguyen and Luu (2021), we used\nslightly different preprocessing than the authors.\nWhile the model can work on any languages with\nassociated embedding models, all data sets used\nin this paper were in English. Our model has ad-\nditional hyperparameters compared to some other\nmodels. So, it may require more tuning and, there-\nfore, more GPU computing. The initial model was\ndeveloped on a system with 8GB of RAM and a\nNvidia GTX 1060 with 3GB of VRAM for a total\nof approximately 100 GPU hours. A single run of\nthe model for 1000 epochs on this GPU requires\nless than an hour. Experiments using the New York\nTimes data set were run on a system with 256GB\nof RAM and a Nvidia RTX 3090 for approximately\n100 GPU hours. All other experiments were run\non a system with 128GB of RAM and a Nvidia\nTITAN RTX for approximately 600 GPU hours.\n9\nEthics Statement\nAll data sets used in this paper are cited. The New\nYork Times data set2 is licensed under \"The New\nYork Times Annotated Corpus Agreement\"3. The\nTweets2011 corpus4 is available under the \"TREC\n2011 Microblog Dataset Usage Agreement\"5 which\nadditionally requires following the \"Twitter terms\nof service\"6. All other data sets are obtained from\nthe recent literature. No sensitive information is\nused or inferred in this paper. The risk of harm\n1https://huggingface.co/sentence-transformers/stsb-\nroberta-large\n2https://catalog.ldc.upenn.edu/LDC2008T19\n3https://catalog.ldc.upenn.edu/license/the-new-york-\ntimes-annotated-corpus-ldc2008t19.pdf\n4https://trec.nist.gov/data/tweets/\n5https://trec.nist.gov/data/tweets/tweets2011-\nagreement.pdf\n6https://twitter.com/en/tos\nfrom our model is low. Any artifacts in this paper\nare used following their intended use cases.\nAcknolwedgements\nWe would like to thank Federico Bianchi for as-\nsistance in ﬁnding data sets. We would like to\nthank the creators and maintainers of Python and\nthe following Python packages: lda, torch, numpy,\nbiterm, scipy, gensim, tqdm, transformers, nltk,\nsentence_transformers, sklearn, and pandas. We\nwould like to thank the following GitHub users\nfor code inspiration: maifeng, smutahoang, dice-\ngroup, shion-h, karpathy, estebandito22, akashgit,\nand MilaNLProc.\nReferences\nAbbas Abdolmaleki, Jost Tobias Springenberg, Yuval\nTassa, Remi Munos, Nicolas Heess, and Martin\nRiedmiller. 2018. Maximum a posteriori policy op-\ntimisation. arXiv preprint arXiv:1806.06920.\nNikolaos Aletras and Mark Stevenson. 2013.\nEvalu-\nating topic coherence using distributional semantics.\nIn Proceedings of the 10th International Conference\non Computational Semantics (IWCS 2013)–Long Pa-\npers, pages 13–22.\nDimo Angelov. 2020. Top2vec: Distributed representa-\ntions of topics. arXiv preprint arXiv:2008.09470.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016.\nLayer normalization.\narXiv preprint\narXiv:1607.06450.\nFederico Bianchi, Silvia Terragni, and Dirk Hovy.\n2020a.\nPre-training is a hot topic:\nContextual-\nized document embeddings improve topic coher-\nence. arXiv preprint arXiv:2004.03974.\nFederico Bianchi, Silvia Terragni, Dirk Hovy, Debora\nNozza, and Elisabetta Fersini. 2020b. Cross-lingual\ncontextualized topic models with zero-shot learning.\narXiv preprint arXiv:2004.07737.\nDavid M Blei and John D Lafferty. 2006.\nDynamic\ntopic models. In Proceedings of the 23rd interna-\ntional conference on Machine learning, pages 113–\n120.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003.\nLatent dirichlet allocation.\nJournal of ma-\nchine Learning research, 3(Jan):993–1022.\nDallas Card, Chenhao Tan, and Noah A Smith. 2017.\nNeural models for documents with metadata. arXiv\npreprint arXiv:1705.09296.\nPeter Dayan and Geoffrey E Hinton. 1997.\nUsing\nexpectation-maximization for reinforcement learn-\ning. Neural Computation, 9(2):271–278.\nAdji B Dieng, Francisco JR Ruiz, and David M Blei.\n2020. Topic modeling in embedding spaces. Trans-\nactions of the Association for Computational Lin-\nguistics, 8:439–453.\nThanh-Nam Doan and Tuan-Anh Hoang. 2021. Bench-\nmarking neural topic models: An empirical study. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 4363–4368.\nMatthew Fellows, Anuj Mahajan, Tim GJ Rudner, and\nShimon Whiteson. 2019.\nVirel: A variational in-\nference framework for reinforcement learning. Ad-\nvances in neural information processing systems, 32.\nMikhail Figurnov, Shakir Mohamed, and Andriy Mnih.\n2018.\nImplicit reparameterization gradients.\nAd-\nvances in neural information processing systems, 31.\nSarah Filippi, Olivier Cappé, and Aurélien Garivier.\n2010.\nOptimism in reinforcement learning and\nkullback-leibler divergence.\nIn 2010 48th Annual\nAllerton Conference on Communication, Control,\nand Computing (Allerton), pages 115–122. IEEE.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth interna-\ntional conference on artiﬁcial intelligence and statis-\ntics, pages 249–256. JMLR Workshop and Confer-\nence Proceedings.\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n2011. Deep sparse rectiﬁer neural networks. In Pro-\nceedings of the fourteenth international conference\non artiﬁcial intelligence and statistics, pages 315–\n323. JMLR Workshop and Conference Proceedings.\nThomas L Grifﬁths and Mark Steyvers. 2004.\nFind-\ning scientiﬁc topics.\nProceedings of the National\nacademy of Sciences, 101(suppl_1):5228–5235.\nMaarten Grootendorst. 2022. Bertopic: Neural topic\nmodeling with a class-based tf-idf procedure. arXiv\npreprint arXiv:2203.05794.\nLin Gui, Jia Leng, Gabriele Pergola, Yu Zhou, Ruifeng\nXu, and Yulan He. 2019.\nNeural topic model\nwith reinforcement learning. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3478–3483.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. 2006.\nDimensionality reduction by learning an invariant\nmapping. In 2006 IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR’06), volume 2, pages 1735–1742. IEEE.\nDan Hendrycks and Kevin Gimpel. 2016.\nGaus-\nsian error linear units (gelus).\narXiv preprint\narXiv:1606.08415.\nIrina Higgins, Loic Matthey, Arka Pal, Christopher\nBurgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, and Alexander Lerchner. 2016. beta-vae:\nLearning basic visual concepts with a constrained\nvariational framework.\nTuan-Anh Hoang, Khoi Duy Vo, and Wolfgang Nejdl.\n2018. W2e: a worldwide-event benchmark dataset\nfor topic detection and tracking.\nIn Proceedings\nof the 27th ACM International Conference on Infor-\nmation and Knowledge Management, pages 1847–\n1850.\nMatthew Hoffman, Francis Bach, and David Blei. 2010.\nOnline learning for latent dirichlet allocation. ad-\nvances in neural information processing systems, 23.\nSergey Ioffe and Christian Szegedy. 2015. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift.\nIn International\nconference on machine learning, pages 448–456.\nPMLR.\nMichael I Jordan, Zoubin Ghahramani, Tommi S\nJaakkola, and Lawrence K Saul. 1999.\nAn intro-\nduction to variational methods for graphical models.\nMachine learning, 37(2):183–233.\nHilbert J Kappen, Vicenç Gómez, and Manfred Opper.\n2012. Optimal control as a graphical model infer-\nence problem. Machine learning, 87(2):159–182.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nDiederik P Kingma and Max Welling. 2013.\nAuto-\nencoding\nvariational\nbayes.\narXiv\npreprint\narXiv:1312.6114.\nTaisuke Kobayashi. 2022.\nOptimistic reinforcement\nlearning by forward kullback–leibler divergence op-\ntimization. Neural Networks, 152:169–180.\nSolomon Kullback and Richard A Leibler. 1951. On\ninformation and sufﬁciency. The annals of mathe-\nmatical statistics, 22(1):79–86.\nAmit Kumar, Nazanin Esmaili, and Massimo Piccardi.\n2021.\nA reinforced variational autoencoder topic\nmodel. In International Conference on Neural In-\nformation Processing, pages 360–369. Springer.\nKen Lang. 1995.\nNewsweeder:\nLearning to ﬁlter\nnetnews. In Machine Learning Proceedings 1995,\npages 331–339. Elsevier.\nJey Han Lau, David Newman, and Timothy Baldwin.\n2014. Machine reading tea leaves: Automatically\nevaluating topic coherence and topic model quality.\nIn Proceedings of the 14th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, pages 530–539.\nSergey Levine. 2018. Reinforcement learning and con-\ntrol as probabilistic inference: Tutorial and review.\narXiv preprint arXiv:1805.00909.\nTianyi Lin, Zhiyue Hu, and Xin Guo. 2019. Sparse-\nmax and relaxed wasserstein for topic sparsity. In\nProceedings of the twelfth ACM international con-\nference on web search and data mining, pages 141–\n149.\nLuyang Liu, Heyan Huang, Yang Gao, Yongfeng\nZhang, and Xiaochi Wei. 2019. Neural variational\ncorrelated topic modeling. In The World Wide Web\nConference, pages 1142–1152.\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Fe-\nichtenhofer, Trevor Darrell, and Saining Xie. 2022.\nA convnet for the 2020s.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 11976–11986.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis.\nIn\nProceedings of the 49th annual meeting of the as-\nsociation for computational linguistics: Human lan-\nguage technologies, pages 142–150.\nRichard McCreadie, Ian Soboroff, Jimmy Lin, Craig\nMacdonald, Iadh Ounis, and Dean McCullough.\n2012. On building a reusable twitter corpus. In Pro-\nceedings of the 35th international ACM SIGIR con-\nference on Research and development in information\nretrieval, pages 1113–1114.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv preprint arXiv:1609.07843.\nYishu Miao, Edward Grefenstette, and Phil Blunsom.\n2017. Discovering discrete latent topics with neural\nvariational inference. In International Conference\non Machine Learning, pages 2410–2419. PMLR.\nYishu Miao, Lei Yu, and Phil Blunsom. 2016. Neural\nvariational inference for text processing. In Interna-\ntional conference on machine learning, pages 1727–\n1736. PMLR.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013.\nEfﬁcient estimation of word\nrepresentations in vector space.\narXiv preprint\narXiv:1301.3781.\nThong Nguyen and Anh Tuan Luu. 2021. Contrastive\nlearning for neural topic model. Advances in Neural\nInformation Processing Systems, 34:11974–11986.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\n2013. On the difﬁculty of training recurrent neural\nnetworks. In International conference on machine\nlearning, pages 1310–1318. PMLR.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nJipeng Qiang, Zhenyu Qian, Yun Li, Yunhao Yuan,\nand Xindong Wu. 2020. Short text topic modeling\ntechniques, applications, and performance: a survey.\nIEEE Transactions on Knowledge and Data Engi-\nneering, 34(3):1427–1445.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nbert:\nSentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics.\nEvan Sandhaus. 2008. The new york times annotated\ncorpus. Linguistic Data Consortium, Philadelphia,\n6(12):e26752.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael\nJordan, and Philipp Moritz. 2015. Trust region pol-\nicy optimization. In International conference on ma-\nchine learning, pages 1889–1897. PMLR.\nAkash Srivastava and Charles Sutton. 2017. Autoen-\ncoding variational inference for topic models. arXiv\npreprint arXiv:1703.01488.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nRichard S Sutton and Andrew G Barto. 2018. Rein-\nforcement learning: An introduction. MIT press.\nRichard S Sutton, David McAllester, Satinder Singh,\nand Yishay Mansour. 1999. Policy gradient meth-\nods for reinforcement learning with function approx-\nimation. Advances in neural information processing\nsystems, 12.\nYuan Tian, Minghao Han, Chetan Kulkarni, and Olga\nFink. 2022. A prescriptive dirichlet power allocation\npolicy with deep reinforcement learning. Reliability\nEngineering & System Safety, 224:108529.\nNaonori Ueda and Kazumi Saito. 2002.\nParametric\nmixture models for multi-labeled text. Advances in\nneural information processing systems, 15.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30.\nNino Vieillard,\nTadashi Kozuno,\nBruno Scherrer,\nOlivier Pietquin, Rémi Munos, and Matthieu Geist.\n2020. Leverage the average: an analysis of kl reg-\nularization in reinforcement learning. Advances in\nNeural Information Processing Systems, 33:12163–\n12174.\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. Machine learning, 8(3):229–256.\nHe Zhao, Dinh Phung, Viet Huynh, Yuan Jin, Lan Du,\nand Wray Buntine. 2021.\nTopic modelling meets\ndeep neural networks: A survey.\narXiv preprint\narXiv:2103.00498.\nRenbo Zhao, Vincent Tan, and Huan Xu. 2017. On-\nline nonnegative matrix factorization with general\ndivergences. In Artiﬁcial Intelligence and Statistics,\npages 37–45. PMLR.\nA\nKL Divergence in RL\nKL divergence has recently become popular in con-\ntinuous action space RL algorithms. One applica-\ntion is to prevent policy updates from making large\nchanges to the policy that could result in poorer per-\nformance. Two algorithms using KL divergence for\nthis are TRPO (Schulman et al., 2015) and MPO\n(Abdolmaleki et al., 2018). Another application is\nfor optimistic RL (Filippi et al., 2010) (Kobayashi,\n2022). Vieillard et al. (2020) investigate the us-\nage of KL divergence as regularization in RL. KL\ndivergence has also been used in optimal control\n(Kappen et al., 2012), which is closely related to\nRL.\nB\nData Sets\nWe evaluate models on the test set where available,\nand on the training set if there is no test set. Coher-\nence and diversity for the training and test set are\nthe same, as they are evaluated on the word distri-\nbution over topics which doesn’t change per docu-\nment. In the code, training coherence and diversity\nare computed after each batch, while test coher-\nence and diversity are computed after each epoch.\nNumber of training/test documents and vocabulary\nsizes are shown in Table 6. Average original and\npreprocessed training document lengths are shown\nin Table 7.\nB.1\n20 Newsgroups\nThe 20 Newsgroups data set (Lang, 1995) consists\nof around 19,000 newsgroup posts from 20 top-\nics. We perform experiments on this data set with\nthree different preprocessing methods. For our ini-\ntial experiments, we follow the preprocessing in\nBianchi et al. (2020a) and additionally remove all\nwords with less than 3 letters. For the comparisons\nwith Bianchi et al. (2020a) and Nguyen and Luu\n(2021), we follow the preprocessing in Bianchi\net al. (2020a). For the comparison with Doan and\nHoang (2021), we use their already preprocessed\ndata set.\nB.2\nNew York Times\nThe New York Times data set (Sandhaus, 2008)\nconsists of over 1.8 million articles written by the\nNew York Times between 1987 and 2007. We fol-\nlow the preprocessing from Bianchi et al. (2020a),\nbut do not remove stopwords.\nB.3\nSnippets\nThe Web Snippets data set (Ueda and Saito, 2002)\nconsists of around 12,000 snippets of text from\nwebsites linked on \"yahoo.com\". The snippets are\ngrouped into 8 domains. We use the already pre-\nprocessed data set from Doan and Hoang (2021).\nB.4\nW2E\nThe W2E data set (Hoang et al., 2018) consists\nof news articles from media channels around the\nworld. The W2E-title subset is the titles from the\nnews articles, while the W2E-content subset is the\ntext content of the articles. The articles are grouped\ninto 30 topics. We use the already preprocessed\ndata set from Doan and Hoang (2021).\nB.5\nWiki20K\nThe Wiki20K data set (Bianchi et al., 2020b) con-\nsists of 20,000 English Wikipedia abstracts ran-\ndomly sampled from DBpedia. We follow the pre-\nprocessing from Bianchi et al. (2020a).\nB.6\nStackOverﬂow\nThe StackOverﬂow data set (Qiang et al., 2020)\nconsists of around 16,000 question titles randomly\nsampled from 20 different tags in a larger data\nset crawled from the website \"stackoverﬂow.com\"\nbetween July and August 2012. We use the already\npreprocessed data set from Qiang et al. (2020).\nB.7\nGoogle News\nThe Google News data set (Qiang et al., 2020) con-\nsists of around 11,000 titles and short samples from\nGoogle News articles clustered into 152 groups.\nWe use the already preprocessed data set from\nQiang et al. (2020).\nB.8\nTweets2011\nThe Tweets2011 data set (Qiang et al., 2020) con-\nsists of around 2,500 tweets in 89 clusters sampled\nfrom the larger Tweets2011 corpus (McCreadie\net al., 2012) crawled from Twitter between January\nand February 2011. We use the already prepro-\ncessed data set from Qiang et al. (2020).\nData Set\nComparison Paper\nTraining Docs\nTest Docs\nVocab Size\n20 Newsgroups\nThis one\n11,314\n7,532\n2,000\n(Bianchi et al., 2020a)\n(Nguyen and Luu, 2021)\n(Doan and Hoang, 2021)\n15,465\nN/A\n4,134\nNew York Times\n(Dieng et al., 2020)\n1,864,470\nN/A\n10,283\nSnippets\n(Doan and Hoang, 2021)\n12,295\nN/A\n4,666\nW2E-title\n(Doan and Hoang, 2021)\n105,457\nN/A\n3,703\nW2E-content\n(Doan and Hoang, 2021)\n83,548\nN/A\n10,508\nWiki20K\n(Bianchi et al., 2020a)\n20,000\nN/A\n2,000\nStackOverﬂow\n(Bianchi et al., 2020a)\n16,407\nN/A\n2,236\nGoogle News\n(Bianchi et al., 2020a)\n11,108\nN/A\n8,099\nTweets2011\n(Bianchi et al., 2020a)\n2,472\nN/A\n5,097\nIMDb Movie Reviews\n(Nguyen and Luu, 2021)\n25,000\n25,000\n5,000\nWikitext-103\n(Nguyen and Luu, 2021)\n28,472\n60\n20,000\nTable 6: Data Sets - Documents and Vocabularies\nB.9\nIMDb Movie Reviews\nThe IMDb Movie Reviews data set (Maas et al.,\n2011) consists of 50,000 movie reviews, each with\nan associated sentiment label, from the website\n\"imdb.com\". We follow the preprocessing from\nBianchi et al. (2020a).\nB.10\nWikitext-103\nThe Wikitext-103 data set (Merity et al., 2016) con-\nsists of around 28,500 Wikipedia articles classi-\nﬁed as either Featured articles or Good articles by\nWikipedia editors. We follow the preprocessing\nfrom Bianchi et al. (2020a).\nC\nEvaluation Metrics\nWe track topic diversity, coherence, perplexity, and\nloss for the training and test sets if applicable.\nTopic diversity and coherence are calculated based\non the top-K words in each topic, with K noted\nfor each experiment. We use NPMI coherence with\nco-occurence based on full document windows.\nMost previous NTMs have only reported the co-\nherence of the ﬁnal model, presumably because\ncoherence is not tracked during training for compu-\ntational reasons. To enable tracking of coherence\nduring training, we modify a vectorized implemen-\ntation of UMass coherence7 to calculate NPMI co-\nherence and add caching for further speed-up. We\nalso implement a GPU-optimized algorithm to cal-\nculate topic diversity during training.\n7https://github.com/maifeng/Examples_UMass-\nCoherence\nTracking these metrics during training provides\ntwo main beneﬁts. The ﬁrst beneﬁt is that if train-\ning is going poorly, it can be terminated. Poor\ntraining could be caused by component collapse\n(low topic diversity), or if the model is unable to\nﬁt to coherent topics (low coherence). The second\nbeneﬁt is enabling deeper performance compar-\nisons between models and between training runs\nfor a single model. Most existing NTMs only track\nloss and perplexity during training, so additionally\ntracking topic diversity and coherence could pro-\nvide additional insights on model performance.\nD\nExpanded Results\nD.1\nTopic Words from Initial Experiments\nWe choose one example of the top 10 words for\nall 20 topics from the initial experiments on the\n20 Newsgroups data set. We choose the seed with\nthe 15th highest coherence (out of 30 seeds). Topic\nwords are shown in Table 8. Each document in the\nTwenty Newsgroups data set is labeled as belong-\ning to one of 20 categories. These 20 categories\nare shown in Table 9.\nD.2\nPre-training is a Hot Topic\nWe show a further comparison between the contex-\ntual embedding model from Bianchi et al. (2020a)\nand our RL model in Table 10. Average NPMI co-\nherence over 30 seeds is compared for each number\nof topics: 25, 50, 75, 100, and 150.\nData Set\nComparison Paper\nAverage Training Document Length\nOriginal\nPreprocessed\n20 Newsgroups\nThis one\n287.5\n95.9\n(Bianchi et al., 2020a)\n287.5\n107.6\n(Nguyen and Luu, 2021)\n(Doan and Hoang, 2021)\nN/A\n73.5\nNew York Times\n(Dieng et al., 2020)\n558.1\n484.5\nSnippets\n(Doan and Hoang, 2021)\nN/A\n14.4\nW2E-title\n(Doan and Hoang, 2021)\nN/A\n6.8\nW2E-content\n(Doan and Hoang, 2021)\nN/A\n209.1\nWiki20K\n(Bianchi et al., 2020a)\n49.8\n17.5\nStackOverﬂow\n(Bianchi et al., 2020a)\nN/A\n4.9\nGoogle News\n(Bianchi et al., 2020a)\nN/A\n6.2\nTweets2011\n(Bianchi et al., 2020a)\nN/A\n8.6\nIMDb Movie Reviews\n(Nguyen and Luu, 2021)\n233.8\n101.7\nWikitext-103\n(Nguyen and Luu, 2021)\n295.8\n133.2\nTable 7: Data Sets - Training Document Lengths\nD.3\nHyperparameters\nWe show the hyperparameters for each experiment\nwe performed. Experiment seeds are generated\nwith a meta-seed for reproducibility. The meta-\nseed is randomly chosen from integers between\n0 and 232. Values in {curly brackets} indicate a\nsearch over multiple parameters. Values in [square\nbrackets] indicate NN layer sizes (e.g. [128, 128]\nrepresents two layers of size 128).\nD.3.1\nInitial Experiments and Ablation\nStudy\nWe use the same meta-seed for the ablation study\nas we did for the initial experiments. Hyperparam-\neters for the initial experiments can be found in\nTable 11. Further tables for all experiments will\nonly show hyperparameters that differ from this\ntable. Hyperparameters for the ablation study can\nbe found in Table 12.\nD.3.2\nBenchmarking Neural Topic Models\nWe show hyperparameters for the comparison with\nDoan and Hoang (2021). Hyperparameters for\nSnippets can be found in Table 15. 20 Newsgroups\nin Table 16. W2E-title in Table 17. W2E-content\nin Table 19.\nD.3.3\nTopic Modeling in Embedding Spaces\nHyperparameters for the comparison with Dieng\net al. (2020) can be found in Table 20.\nD.3.4\nPre-training is a Hot Topic\nWe show hyperparameters for the comparison with\nBianchi et al. (2020a). Data set and seed informa-\ntion can be found in Table 13. All other hyperpa-\nrameters are the same for each data set; these can\nbe found in Table 18.\nD.3.5\nContrastive Learning for NTM\nWe show hyperparameters for the comparison with\nNguyen and Luu (2021). Some hyperparameters\nare already shown in Table 3 and won’t be shown\nagain here. Data set and seed information can be\nfound in Table 14. Other hyperparameters are the\nsame for each data set; these can be found in Ta-\nble 21. Hyperparameters for the policy dropout\nsweep can be found in Table 22.\nD.4\nAblation Study\nWe show full results from the ablation study in\nTable 23.\nE\nModel Parameter Count\nThe number of parameters (P) in the model differs\nbased on the total number of parameters across all\ninference layers (L), the number of topics (N), and\nthe vocabulary size (V). Trainable parameters are\nthe inference layers, the prior distribution of topics\n(N x 1), and the distribution of words over topics\n(V * N). Total parameters can be calculated with\nEquation 8.\nP = L + N + V ∗N\n(8)\nTopic Words\nmax giz bhj chz pts buf air det pit bos\nmorality objective cramer moral livesey optilink keith homosexual clayton gay\nwindow xterm widget lib windows font usr mouse motif application\ngun guns militia ﬁrearms weapons cops weapon amendment semi arms\nteam players hockey game season nhl games play teams leafs\nmax giz bhj sale chz shipping offer monitor copies condition\njesus god bible christ christians faith church christian heaven lord\ngeb banks msg patients gordon pitt disease pain doctor medical\nfbi batf koresh compound atf waco sandvik udel ﬁre kent\ncar insurance cars dealer oil saturn honda engine bmw miles\njpeg image bits display gif ﬁle program ﬁles format color\nclipper encryption key chip escrow keys privacy crypto secure nsa\nwire ground circuit connected cable atheism electrical universe keyboard output\nisrael israeli arab jews arabs peace palestinian attacks bony villages\nturkish armenian armenians armenia turks serdar argic turkey genocide soviet\npub ftp anonymous tar graphics privacy mailing archive motif faq\nmoon space lunar orbit nasa spacecraft henry launch shuttle solar\ndog bike dod riding ride motorcycle rider bmw went cops\nscsi ide drive controller drives bus disk ﬂoppy bios isa\nstephanopoulos president jobs myers russia russian administration package launch clinton\nTable 8: Initial Experiment Topic Words\nCategory\nalt.atheism\ncomp.graphics\ncomp.os.ms-windows.misc\ncomp.sys.ibm.pc.hardware\ncomp.sys.mac.hardware\ncomp.windows.x\nmisc.forsale\nrec.autos\nrec.motorcycles\nrec.sport.baseball\nrec.sport.hockey\nsci.crypt\nsci.electronics\nsci.med\nsci.space\nsoc.religion.christian\ntalk.politics.guns\ntalk.politics.mideast\ntalk.politics.misc\ntalk.religion.misc\nTable 9: 20 Newsgroups Categories\nThe largest model we use is for the Wikitext-103\ndata set with 200 topics. This model has 4,001,224\nparameters.\nF\nFuture Work\nWe have identiﬁed some possible paths for future\nwork. The SBERT embeddings could be ﬁne-tuned\nduring training rather than calculating them dur-\ning pre-processing and freezing them during train-\ning. The RL formulation of our model could be\nextended to dynamic topic models (Blei and Laf-\nferty, 2006). More complex PG RL algorithms\ncould be used rather than REINFORCE, or a base-\nline could be added to REINFORCE. Exploration\ntechniques from RL could be applied. The inﬂu-\nence of hyperparameters (e.g. inference network\nlayer sizes) on varied corpora (e.g. those with large\nvocabularies) could be explored. The Laplace ap-\nproximation of the Dirichlet prior could be replaced\nby a true Dirichlet prior, making use of the Dirich-\nlet RT (Figurnov et al., 2018) and a Dirichlet RL\npolicy (Tian et al., 2022). Finally, λ and the policy\ndropout could be scheduled during training to pro-\nvide an automated tradeoff between topic diversity\nand coherence.\nData Set\nPaper\nNPMI Coherence\n25 Topics\n50 Topics\n75 Topics\n100 Topics\n150 Topics\nWiki20K\nPTHT\n0.17\n0.19\n0.18\n0.19\n0.17\nRL model\n(ours)\n0.33\n0.30\n0.25\n0.22\n0.19\nStackOverﬂow\nPTHT\n0.05\n0.03\n0.02\n0.02\n0.02\nRL model\n(ours)\n0.17\n0.14\n0.12\n0.11\n0.10\nGoogle News\nPTHT\n0.03\n0.10\n0.15\n0.18\n0.19\nRL model\n(ours)\n0.38\n0.41\n0.38\n0.34\n0.30\nTweets2011\nPTHT\n0.05\n0.10\n0.11\n0.12\n0.12\nRL model\n(ours)\n0.36\n0.39\n0.38\n0.35\n0.31\n20 Newsgroups\nPTHT\n0.13\n0.13\n0.13\n0.13\n0.12\nRL model\n(ours)\n0.35\n0.30\n0.27\n0.25\n0.22\nTable 10: NPMI coherence comparison between PTHT model and RL model for each number of topics\nHyperparameter\nValue(s)\nMeta-seed\n4174224060\nNum. Seeds\n30\nNum. Epochs\n1000\nData Set\n20 Newsgroups\nVocab Size\n2000\nEmbedding\nSBERT\nNum. Topics (N)\n20\nInference Dropout\n0.2\nPolicy Dropout\n0.0\nInference Layers\n[128, 128]\nActivation\nGELU\nInitialization\nρ ∼N(0, 0.02)\nNormalization\nLayer\nλ\n5\nTopic Words (K)\n10\nRL policy\n✓\nθ Softmax\n×\nLearning Rate (α)\n3e-4\nAdam β1, β2\n0.9, 0.999\nWeight Decay\n0.01\nBatch Size\n1024\nGradient Clipping\n1.0\nTable 11: Initial Experiments\nHyperparameter\nValue(s)\nMeta-seed\n4174224060\nNum. Seeds\n30\nData Set\n20 Newsgroups\nEmbedding\n{BoW, SBERT}\nθ / Policy Dropout\n{0.0, 0.2}\nλ\n{1, 5}\nRL policy\n{✓, ×}\nθ Softmax\n{✓, ×}\nTable 12: Ablation Study\nData Set\nVocab Size\nMeta-seed\nNum. Seeds\nWiki20K\n2000\n359491602\n30\nStackOverﬂow\n2236\n1459046441\n30\nGoogle News\n8099\n925040003\n30\nTweets2011\n5097\n1321150024\n30\n20 Newsgroups\n2000\n3277797161\n30\nTable 13: PTHT Data Set Seeds\nData Set\nVocab Size\nMeta-seed\nNum. Seeds\n20 Newsgroups\n2000\n1553571489\n30\nIMDb Movie Reviews\n5000\n3747305026\n30\nWikitext-103\n20000\n2672751736\n30\nTable 14: CLNTM Data Set Seeds\nHyperparameter\nValue(s)\nMeta-seed\n193270011\nNum. Seeds\n10\nData Set\nSnippets\nVocab Size\n4666\nNum. Topics (N)\n{4, 8, 12, 16, 20, 24}\nλ\n{1, 3, 5, 10}\nTable 15: BNTM Snippets\nHyperparameter\nValue(s)\nMeta-seed\n1216545997\nNum. Seeds\n10\nData Set\n20 Newsgroups\nVocab Size\n4157\nNum. Topics (N)\n{10, 20, 30, 40, 50, 60}\nλ\n{1, 3, 5, 10}\nTable 16: BNTM 20 Newsgroups\nHyperparameter\nValue(s)\nMeta-seed\n4014169843\nNum. Seeds\n10\nData Set\nW2E-title\nVocab Size\n3703\nNum. Topics (N)\n{15, 30, 45, 60, 75, 90}\nλ\n{1, 3, 5, 10}\nTable 17: BNTM W2E-title\nHyperparameter\nValue(s)\nNum. Topics (N)\n{25, 50, 75, 100, 150}\nλ\n1\nTable 18: Pre-training is a Hot Topic\nHyperparameter\nValue(s)\nMeta-seed\n1359128464\nNum. Seeds\n10\nData Set\nW2E-content\nVocab Size\n10508\nNum. Topics (N)\n{15, 30, 45, 60, 75, 90}\nλ\n{1, 3, 5, 10}\nTable 19: BNTM W2E-content\nHyperparameter\nValue(s)\nMeta-seed\n2337766308\nNum. Seeds\n1\nNum. Epochs\n20\nData Set\nNew York Times\nVocab Size\n10283\nNum. Topics (N)\n300\nInference Dropout\n0.5\nInference Layers\n[512, 512]\nλ\n1\nTopic Words (K)\n10*\nBatch Size\n32768\nTable 20: Topic Modeling in Embedding Spaces (*We\nuse K = 25 to calculate topic diversity for the ﬁnal\nmodel.)\nHyperparameter\nValue(s)\nNum. Epochs\n2000\nNum. Topics (N)\n{50, 200}\nTable 21: Contrastive Learning for NTM\nHyperparameter\nValue(s)\nMeta-seed\n3432645033\nNum. Seeds\n30\nData Set\n20 Newsgroups\nNum. Epochs\n2000\nNum. Topics (N)\n50\nInference Dropout\n0.5\nPolicy Dropout\n{0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}\nInference Layers\n[128, 128]\nλ\n1\nTable 22: CLNTM Dropout Sweep\nRL Policy\nEmbedding\nλ\nθ Softmax\nθ / Policy Dropout\nCoherence\nDiversity\n×\nBoW\n1\n✓\n0.0\n0.2906\n0.8457\n×\nBoW\n1\n×\n0.0\n0.2373\n0.6943\n✓\nBoW\n1\n✓\n0.0\n0.2748\n0.8905\n✓\nBoW\n1\n×\n0.0\n0.2738\n0.8707\n×\nBoW\n5\n✓\n0.0\n0.2526\n0.6598\n×\nBoW\n5\n×\n0.0\n0.2619\n0.6928\n✓\nBoW\n5\n✓\n0.0\n0.2032\n0.5965\n✓\nBoW\n5\n×\n0.0\n0.3379\n0.9403\n×\nBoW\n1\n✓\n0.2\n0.2650\n0.7390\n×\nBoW\n1\n×\n0.2\n0.2193\n0.5195\n✓\nBoW\n1\n✓\n0.2\n0.2082\n0.5692\n✓\nBoW\n1\n×\n0.2\n0.2798\n0.7740\n×\nBoW\n5\n✓\n0.2\n0.2526\n0.6222\n×\nBoW\n5\n×\n0.2\n0.2257\n0.5768\n✓\nBoW\n5\n✓\n0.2\n0.1222\n0.314\n✓\nBoW\n5\n×\n0.2\n0.3284\n0.8092\n×\nSBERT\n1\n✓\n0.0\n0.2845\n0.6207\n×\nSBERT\n1\n×\n0.0\n0.2948\n0.5995\n✓\nSBERT\n1\n✓\n0.0\n0.2158\n0.8080\n✓\nSBERT\n1\n×\n0.0\n0.3414\n0.9070\n×\nSBERT\n5\n✓\n0.0\n0.2726\n0.4458\n×\nSBERT\n5\n×\n0.0\n0.2795\n0.4530\n✓\nSBERT\n5\n✓\n0.0\n0.1932\n0.6927\n✓\nSBERT\n5\n×\n0.0\n0.3848\n0.9530\n×\nSBERT\n1\n✓\n0.2\n0.2532\n0.6063\n×\nSBERT\n1\n×\n0.2\n0.2554\n0.5430\n✓\nSBERT\n1\n✓\n0.2\n0.1133\n0.5520\n✓\nSBERT\n1\n×\n0.2\n0.3649\n0.7663\n×\nSBERT\n5\n✓\n0.2\n0.2435\n0.4478\n×\nSBERT\n5\n×\n0.2\n0.2080\n0.3698\n✓\nSBERT\n5\n✓\n0.2\n0.0967\n0.9227\n✓\nSBERT\n5\n×\n0.2\n0.3769\n0.7315\nTable 23: Full Results from Ablation Study\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2023-05-08",
  "updated": "2023-05-08"
}