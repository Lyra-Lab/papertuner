{
  "id": "http://arxiv.org/abs/2305.05601v1",
  "title": "Deep Learning and Geometric Deep Learning: an introduction for mathematicians and physicists",
  "authors": [
    "R. Fioresi",
    "F. Zanchetta"
  ],
  "abstract": "In this expository paper we want to give a brief introduction, with few key\nreferences for further reading, to the inner functioning of the new and\nsuccessfull algorithms of Deep Learning and Geometric Deep Learning with a\nfocus on Graph Neural Networks. We go over the key ingredients for these\nalgorithms: the score and loss function and we explain the main steps for the\ntraining of a model. We do not aim to give a complete and exhaustive treatment,\nbut we isolate few concepts to give a fast introduction to the subject. We\nprovide some appendices to complement our treatment discussing Kullback-Leibler\ndivergence, regression, Multi-layer Perceptrons and the Universal Approximation\nTheorem.",
  "text": "Deep Learning and Geometric Deep Learning:\nan introduction for mathematicians and physicists\nR. Fioresi, F. Zanchetta1\nFaBiT, via San Donato 15, 41127 Bologna, Italy\nrita.ﬁoresi@unibo.it, ferdinando.zanchett2@unibo.it\nAbstract\nIn this expository paper we want to give a brief introduction, with\nfew key references for further reading, to the inner functioning of the\nnew and successfull algorithms of Deep Learning and Geometric Deep\nLearning with a focus on Graph Neural Networks. We go over the key\ningredients for these algorithms: the score and loss function and we\nexplain the main steps for the training of a model. We do not aim to\ngive a complete and exhaustive treatment, but we isolate few concepts\nto give a fast introduction to the subject. We provide some appendices\nto complement our treatment discussing Kullback-Leibler divergence,\nregression, Multi-layer Perceptrons and the Universal Approximation\nTheorem.\nContents\n1\nIntroduction\n2\n2\nSupervised Classiﬁcation\n4\n2.1\nSupervised image classiﬁcation datasets . . . . . . . . . . . . .\n4\n2.2\nTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3\nDeep Learning\n12\n3.1\nScore Function\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.2\nLoss Function . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n4\nGeometric Deep Learning: Graph Neural Networks\n17\n4.1\nGraphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4.2\nLaplacian on graphs\n. . . . . . . . . . . . . . . . . . . . . . .\n21\n4.3\nHeat equation . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n1This research was supported by Gnsaga-Indam, by COST Action CaLISTA CA21109\nand by HORIZON-MSCA-2022-SE-01-01 CaLIGOLA.\n1\narXiv:2305.05601v1  [cs.LG]  9 May 2023\n4.4\nSupervised Classiﬁcation on Graphs . . . . . . . . . . . . . . .\n26\n4.5\nThe Score function in Graph Neural Networks . . . . . . . . .\n27\n4.6\nThe Zachary Karate Club\n. . . . . . . . . . . . . . . . . . . .\n29\n4.7\nGraph Attention Networks . . . . . . . . . . . . . . . . . . . .\n32\nA Fisher matrix and Information Geometry\n34\nB Regression tasks\n38\nC Multi-layer perceptrons and Convolutional Neural Networks 40\nD Universal Approximation Theorem\n45\n1\nIntroduction\nThe recent revolution in machine learning, including the spectacular success\nof the Deep Learning (DL) algorithms [8, 7, 29], challenges mathematicians\nand physicists to provide models that explain the elusive mechanisms inside\nthem. Indeed, the popularity of Deep Learning, a class of machine learning\nalgorithms that aims to solve problems by extracting high-level features from\nsome raw input, is rising as it is being employed successfully to solve diﬃcult\npractical problems in many diﬀerent ﬁelds such as speech recognition [18],\nnatural language processing [41, 21], image recognition [11], drug discovery\n[40], bioinformatics [2] and medical image analysis [33] just to cite a few, but\nthe list is much longer. Recently at Cern the power of deep learning was\nemployed for the analysis of LHC (Large Hadron Collider) data [16]; parti-\ncle physics is more and more being investigated by these new and powerful\nmethods (see the review [12] and refs. therein). To give a concrete exam-\nples of more practical applications, Convolutional Neural Networks (CNNs,\n[30], [29]), particular DL algorithms, were employed in the ImageNet chal-\nlenge [28, 11], a supervised classiﬁcation task challenge, where participants\nproposed their algorithms to classify a vast dataset of images belonging to\n1000 categories. The CNNs algorithms proposed by several researchers in-\ncluding LeCun, Hinton et al. [29, 28] surpassed the human performance and\ncontributed to establish a new paradigm in machine learning.\nThe purpose of this paper is to elucidate some of the main mechanisms\nof the algorithms of Deep Learning and of Geometric Deep Learning [8, 7]\n2\n(GDL), an adaptation of Deep Learning for data organized in a graph struc-\nture [25]. To this end, we shall focus on two very important types of al-\ngorithms: CNNs and Graph Neural Networks (GNNs, [17]) for supervised\nclassiﬁcation tasks. Convolutional neural networks, DL algorithms already\npart of the family of GDL algorithms, are extremely successful for problems\ninvolving data with a grid structure, in other words a regularly organized\nstructure, where the mathematical operation of convolution, in its discrete\nversion, makes sense. On the other hand, when we deal with datasets having\nan underlying geometric graph structures, it is necessary to adapt the notion\nof convolution, since every node has, in principle, a diﬀerent neighbourhood\nand local topology. This motivates the introduction of Graph Neural Net-\nworks.\nWe do not plan to give a complete treatment of all the types of CNNs and\nGNNs, but we wish to give a rigorous introduction for the mathematicians\nand physicists that wish to know more about both the implementation and\nthe theory behind these algorithms.\nThe organization of our paper is as follows.\nIn Sec.\n2, we describe CNNs for supervised classiﬁcation, with focus\non the image classiﬁcation task, since historically this is one of the main\napplications that helped to establish the Deep Learning popularity among\nthe classiﬁcation algorithms. In this section we describe the various steps\nin the creation of a model, that is the process, called training in which we\ndetermine the parameters of a network, which perform best for the given\nclassiﬁcation task. This part is common to many algorithms, besides Deep\nLearning ones.\nIn Sec. 3, we focus on the score and the loss functions, describing some\nexamples of such functions, peculiar to Deep Learning, that are used rou-\ntinely. Given a datum, for example an image, the score function assigns to\nit, the score corresponding to each class. Hence it will allow us to classify\nthe datum, by assigning to it the class with the highest score. The loss of a\ndatum measures the error committed during the classiﬁcation: the higher the\nloss, the further we are from the correct class of the datum. Some authors\nsay that the purpose of the training is to “minimize” the loss, and we shall\ngive a precise meaning to this intuitive statement.\nIn Sec. 4, we introduce some graph neural networks for supervised node\nclassiﬁcation tasks. After a brief introduction to graphs and their laplacians,\nwe start our description of the score function in this context.\nIn fact, it\n3\nis necessary to modify the “convolutional layers”, that is the convolutional\nfunctions appearing in the expression of the score function, to adapt them\nto the data in graph form, which, in general, is not grid-like. This leads to\nthe so called “message passing” mechanism, the heart of the graph neural\nnetworks algorithms and explained in Subsec. 4.3 describing the convolution\non graphs and its intriguing relation with the heat equation.\nIn the end we provide few appendices to deepen the mathematical treat-\nment that we have not included in the text not to disrupt the reading.\n2\nSupervised Classiﬁcation\nDeep Learning is a very successful family of machine learning algorithms.\nWe can use it to solve both supervised and unsupervised problems for both\nregression or classiﬁcation tasks. In this paper, we shall focus on supervised\nclassiﬁcation tasks: as supervised regression tasks are handled similarly, we\nwill mention them brieﬂy in Appendix B. To solve a classiﬁcation problem\nin the supervised learning setting, we have data points belonging to the\neuclidean space Rd and each point has a label, tipically an integer number\nbetween 0 and C −1, where C is the number of classes.\nThe goal is to\n“learn”, that is to determine, a function that associates data points with the\ncorrect label F : Rd −→{0, . . . , C −1}, through the process of training that\nwe discuss below.\nWe shall focus our attention on a classiﬁcation task the algorithm of Deep\nLearning is particularly eﬀective with: supervised image classiﬁcation.\n2.1\nSupervised image classiﬁcation datasets\nIn this supervised task, we have a database of images, each coming with a\nground truth label, that is a label representing the class of the given image.\nHence, for supervised classiﬁcation, it is necessary to have a labelled dataset;\nproducing such databases is time consuming in concrete applications, where\nlabelling may take a lot of human eﬀort. However, there are two very impor-\ntant examples of such datasets representing a key reference for all researchers\nin this ﬁeld: the MNIST and CIFAR datasets [27, 32]. Datasets of this kind\nare called “benchmark” datasets, they are free and publicly available and\nthey can be easily downloaded from several websites, including the colab\n4\nplatform2. for example. The images in these datasets are already labelled,\nhence they are ready for supervised image classiﬁcation. They are perfect to\nstart “hands on” experiments3.\nLet us brieﬂy describe these datasets, since they represent the ﬁrst and\nmost accessible way to learn how to construct and run successfully a deep\nlearning algorithm.\nThe MNIST dataset contains 70000 black and white images of handwrit-\nten digits from 0 to 9 (see Fig. 3).\nFigure 1: Examples from MNIST dataset\nEach image consists of 28×28 pixels, hence it is represented by a matrix of\nsize 28×28, with integral entries between 0 and 255 representing the grayscale\nof the pixel. Eﬀectively, an image is a point in Rd, for d = 28 × 28 = 784.\nThe MNIST dataset comes already divided into a training set of 60000\nexamples and a test set of 10000 examples. We shall see the meaning of such\npartition in the sequel.\nAnother important benchmark dataset is the CIFAR10 database, contain-\ning 60000 natural color images divided into 10 classes, ranging from airplanes,\ncars etc. to cat, dogs, horses etc. Each image consists of 32 × 32 pixels; each\npixel has three numbers associated with it, between 0 and 255 corresponding\nto one of the RGB channels4. So, practically, every image is an element in\n2colab.research.google.com\n3See the tutorials publicly available on the colab platform.\n4RGB means Red, Green and Blue the combination of shades of these three colors gives\na color image.\n5\nFigure 2: Matrix values for a sample of number 3 in MNIST, [6]\nRd, with d = 32 × 32 × 3 = 3072.\nIn this dataset each datum has a larger dimension than in MNIST, where\nd = 28×28. This larger dimension, coupled with a greater variability among\nsamples, makes the training longer and more expensive; we shall see more on\nthis later on.\nIn supervised image classiﬁcation tasks, the dataset is typically divided\ninto three disjoint subsets5: training, validation and test sets.\n1. Training set: it is used to determine the function that associates a\nlabel to a given image. Such function depends on parameters w ∈Rp, called\nweights. The process during which we determine the weights, by successive\niterations, is called training; the weights are initialized randomly (for some\nstandard random initialization algorithms, see [13]) at the beginning of the\ntraining. We refer to a set of weights, that we have determined through a\ntraining, as a model. Notice that, since the weights are initialized randomly,\nand because of the nature of the training process we will describe later, we\nget two diﬀerent models even if we perform two trainings in the very same\nway.\n2. Validation set: it is used to evaluate the performance of a model, that\nwe have determined through a training. If the performance is not satisfactory,\n5The “cross validation” technique uses a part of the training set as validation set, but,\nfor the sake of clarity, we shall not describe this common practice here.\n6\nFigure 3: Examples from CIFAR10 dataset (www.cs.toronto.edu)\nwe change the form of the function that we have used for the training and we\nrepeat the training thus determining a new model. The shape of the function\ndepends on parameters, which are called hyperparameters to distinguish them\nfrom the weights, which are determined during the training. The iterative\nprocess in which we determine the hyperparameters is called validation. After\nevery iteration we need to go back and obtain a new model via training.\n3. Test sets: it is used once at the end of both training and validation\nto determine the accuracy of the algorithm, measured in percentage of accu-\nrate predictions on total predictions regarding the labels of the test images.\nIt is extremely important to keep this dataset disjoint from training and\nvalidation; even more important, to use it only once.\nA typical partition of a labelled dataset is the following: 80% training,\n10% validation, 10% test.\nWe now describe more in detail the key process in any supervised classi-\nﬁcation task: the training.\n7\n2.2\nTraining\nThe expression training refers to the process of optimization of the weights w\nto produce a model (see previous section). In order to do this, we need three\nkey ingredients: the score function, the loss function and the optimizer.\n1. The score function s is assigning to each datum x ∈Rd, for example\nan image, and to a given set of weights in Rp a score for each class:\ns : Rd × Rp\n−→\nRC,\n(x, w)\n7→\ns(x, w) = (s0(x, w), . . . , sC−1(x, w))\nwhere C is the number of classes in our classiﬁcation task 6 . We can interpret\nthe function F : Rd −→{0, . . . , C −1}, mentioned earlier, associating to an\nimage its class, in terms of the score function. For a ﬁxed model w, F is\nthe function assigning, to an image x ∈Rd, the class corresponding to its\nhighest score, i.e.\nF(x) = i,\nsi(x, w) = max{sj(x, w), j = 0, . . . , C −1}\nThe purpose of the training and validation is to obtain the best F to perform\nthe classiﬁcation task, which is, by no means, unique.\nTo get an idea on the values of p, d, C we look at the example of the\nCIFAR10 database, where C = 10.\nThen if x is an image of CIFAR10,\nx ∈Rd, for d = 32 × 32 × 3 = 3072, while p, the dimension of the weight\nspace, is typically in the order of 106 and depends on the choice of the\nscore function. We report in Table 1 the dimension of the weight space p\nfor diﬀerent architectures, i.e. choices of the score function, on CIFAR10\n(M= 106).\nWe are going to see in our next section, focused on Deep Learning, a\ntypical form of a score function in Deep Learning.\n2. The loss function measures how accurate is the prediction that we\nobtain via the score function. In fact, given a score function, we can imme-\ndiately transform it into a probability distribution, by assigning to an image\nx the probability that x belongs to one of the classes:\npi(x, w) :=\nesi(x,w)\nP\nj esj(x,w)\n(1)\n6Notice: we write C −1, since in python all arrays start from the zero index, so we\nadhere to the convention we see in the actual programming code.\n8\nTable 1: Values for p for various architectures on CIFAR10.\nArchitecture\np = |Weights|\nResNet\n1.7M\nWide ResNet\n11M\nDenseNet (k=12)\n1M\nDenseNet (k=24)\n27.2M\nIf the probability in (1) is a mass probability distribution concentrated in\nthe class corresponding to the correct label of x, we want the loss function\nL(x, w) for x to be zero. We are going to see in our next section a concrete\nexample of such function. Since the loss function is computed via the score,\nit depends on both the weights w and the data x. The total loss function (or\nloss function for short) is the sum of all the loss functions for each datum,\naveraged on the number of data N:\nL : Rp −→R,\nL(w) = 1\nN\nX\nx\nL(x, w)\nFor example in the MNIST dataset N = 60000, hence the loss function is\nthe sum of 60000 terms, one for each image in the training set.\n3. The optimizer is the key to determine the set of weights w, which\nperform best on the training set. As we shall presently see, at each step\nthe weights will be updated according to the optimizer prescription aimed to\nminimize (only locally though) the loss function, that is the error committed\nin the prediction of the labels on the samples in the training set. In a problem\nof optimization, a standard technique to minimize the loss function L is the\ngradient descent (GD) with respect to the weights w. In GD the update step\nin the space of parameters at time7 t has the form:\nwt+1 = wt −η∇L(wt)\n(2)\nwhere η is the learning rate and it is an hyperparameter of the model, whose\noptimal value is chosen during validation. The gradient is usually computed\nwith an highly eﬃcient algorithm called backpropagation (see [29]), where\nthe chain rule, usually taught in calculus classes, plays a key role. We are\n7The time is a discrete variable in machine learning, we increase it by one at each\niteration.\n9\nunable to describe it here, but for our synthetic exposition of key concepts,\nit is not essential to understand the functioning of the algorithm.\nThe gradient descent minimization technique is guaranteed to reach a\nsuitable minimum of the loss function only if such function is convex. Since\nthe typical loss functions used in Deep Learning are non-convex, it is more\nfruitful to use a variation of GD, namely stochastic gradient descent (SGD).\nThis technique will add “noise” to the training dynamics, this fact is crucial\nfor the correct performance, and can eﬀectively thermodynamically modeled\nsee [9], for more details, we will not go into such description. SGD is one of\nthe most popular optimizers, together with variations on the same theme (like\nAdam, mixing SGD with Nesterov momentum, see [24]). In SGD, we do not\ncompute the gradient of the loss function summing over all training data, but\nonly on a randomly chosen subset of training samples, called minibatch, B.\nThe minibatch changes at each step, hence providing iteration after iteration\na statistically signiﬁcant sample of all the training dataset. The update of\nthe weights is obtained as in (2), where we replace the gradient of the loss\nfunction\n∇L(w) = 1\nN\nX\nx\n∇L(x, w)\nwith\n∇BL(w) := 1\n|B|\nX\nx∈B\n∇L(x, w)\nwhere N is the size of the dataset (i.e. the number of samples in the dataset),\nwhile |B| is the size of the minibatch, another hyperparameter of the model.\nThe SGD update step is then:\nwt+1 = wt −η∇BL(wt)\n(3)\nIn SGD the samples are extracted randomly and the same sample can be\nextracted more than once in subsequent steps for diﬀerent minibatches.This\ntechnique allows to explore the landscape of the loss function with a noisy\nmovement around the direction of steepest descent and also prevents the\ntraining dynamics from being trapped in an inconvenient local minimum of\nthe loss function.\nWe deﬁne as epoch the number of training steps necessary to cover a\nnumber of samples equal to the size of the entire training set:\nepoch = N\n|B|\nFor example in CIFAR10, where N = 60000, if we choose a minibatch\nsize of 32, we have that an epoch is equal to 60000/32 = 1875 iterations\n10\ni.e. updates of the weight as in (3). A typical training can last from 200-\n300 epochs for a simple database like MNIST, to few thousands for a more\ncomplicated one. Often the learning rate is modiﬁed during the iterations,\ndecreasing it as the training (also called learning process) goes.\nUsually,\nvalues of the loss and accuracies are printed out every 10-100 epochs to\ncheck the loss values are actually decreasing and the accuracy is improving.\nAs usual, by accuracy we mean the number of correct predictions with respect\nto the total number of data in training set. Since in our examples, MNIST\nand CIFAR10 the labels are evenly distributed among the classes, this notion\nof accuracy is meaningful.\nFor a supervised classiﬁcation task, let us summarize the training process.\nWe have the following steps:\n• Step 0: Initialize randomly the weights.\n• Step 1: Assign a score to all data.\n• Step 2: Compute the total loss based on score at Step 1.\n• Step 3: Update the weights according to the chosen optimizer (e.g.\nSGD) for a random minibatch.\n• Step 4: Repeat Steps 1, 2, 3 until the loss reaches a plateau and is not\ndecreasing anymore (usually few hundreds epochs).\nAt the end of the training, validation is necessary to understand whether\na change in our choices, like the score function, the size of the minibatch and\nthe learning rate, can produce a better prediction, by computing the accuracy\non the validation set, consisting of images not belonging to the training set,\nthat we have used to compute the optimal weight. In practice this means\nchanging the hyperparameters of the model, repeating the training on the\ntraining set and then evaluating the trained model on the validation set to\nsee if we get a better performance.\nThis process is then repeated many\ntimes trying multiple hyperparameters combinations. There are techniques\nto conduct this hyperparameter optimization in an eﬃcient way, for example,\nthe interested reader might read about one of these techniques here [1].\n11\n3\nDeep Learning\nIn this section we shall focus on score and loss functions widely used when\nsolving supervised image classiﬁcation tasks with Deep Learning algorithms.\nWe will then focus on this speciﬁc task.\nAgain, this is not meant to be\nan exhaustive treatment, but just an overview to help mathematicians and\nphysicists to understand the mathematical and geometric concepts underly-\ning these algorithms.\n3.1\nScore Function\nThe purpose of the score function is to associate to every image x in our\ndataset, for a given set of weights w ∈Rp, a score for each class:\ns : Rd × Rp −→RC\nwhere d = b × c, if the image is black and white, b and c taking into account\nthe dimensions of the image pixelwise. If we have a color image, d = 3×b×c\nbecause of the three RBG channels (see also Sec. 2.1). For clarity, we shall\nassume to have d = b×c, the general case being a small modiﬁcation of this.\nThe score function in supervised learning characterizes the algorithm and\nfor Deep Learning it consists in the composition of several functions, each\nwith a geometrical meaning, in terms of images and perception. We discuss\nsome of the most common functions appearing in the score function; this list\nis by no means exhaustive, but gives an overview on how this key function\nis obtained. We also notice that there is no predeﬁned score function: for\neach dataset some score functions perform better than others. However very\ndiﬀerent score functions, still within the scope of the Deep Learning, may\ngive comparable results in terms of accuracy.\nCommon functions appearing in the expression of the score function for\nDeep Learning algorithms are: linear and aﬃne functions, the RELU func-\ntion, convolutional functions. We shall examine each of them in some detail.\nAﬃne functions.\nSince any image is a matrix of integers, we can trans-\nform such matrix x ∈Rb × Rc into a row vector in Rd, d = b × c by\nconcatenating each row/column. Then, we deﬁne:\nℓw,b : Rd −→Rn,\nℓw,b(x) = xw + b\n12\nwhere w = (wij) is a d × n matrix of weights and b ∈Rd a vector (also of\nweights) called bias. For simplicity, from now on we will work with linear\nfunctions only ℓw, with associated matrix w, that is we take b = 0, see App.\nC for more details on the general case.\nIf we deﬁne the score as:\ns : Rd × Rp −→RC,\ns(x, w) = ℓw(x),\np = d × C\nthen, we say we have a linear classiﬁer or equivalently we perform linear\nclassiﬁcation. Linear classiﬁers do not belong to the family of Deep Learning\nalgorithms, however they always appear in the set of functions whose compo-\nsition gives the score in Deep Learning. In this case, the number of weights\nis p = d × C. For the example of MNIST and linear classiﬁcation, W is a\nmatrix of size p = 784 × 10, since an image of MNIST is a black and white\n28×28 pixel image and belongs to one of 10 classes each representing a digit.\nLinear classiﬁcation on simple datasets as MNIST yields already signiﬁcant\nresults (i.e. accuracies above 85%). It is important, however, to realize that\nlinear classiﬁcation is eﬀective only if the dataset is linearly separable, that\nis, we can partition the data space Rd using aﬃne linear hyperplanes and all\nthe samples of the same class lay into one component of the partition.\nFigure 4: Linearly separable and linearly non-separable sets (github.org)\n13\nRectiﬁed Linear Unit (RELU) function.\nLet the notation be as above.\nWe deﬁne, in a generic euclidean space Rm, the RELU function as:\nRELU : Rm −→Rm,\nRELU(z) = (max(0, z1), . . . , max(0, zm))\nwhere z = (z1, . . . , zm) ∈Rm.\nNotice that if the RELU appears in the expression of the score function,\nit brings non linearity. If we realize the score function by composing linear\nand RELU functions, we can tackle more eﬀectively the classiﬁcation task\nof a non linearly separable dataset (see Appendix D). So we can deﬁne, for\nexample, the non linear score function:\ns : Rd × Rp −→RC,\ns(x, w) = (ℓw2 ◦RELU ◦ℓw1)(x)\n(4)\nThis very simple score function already achieves a remarkable performance of\n98% on the MNIST dataset, once the training and validation are performed\nsuitably (see Sec. 2.2). Notice that we have:\nℓw1 : Rd −→Rh,\nℓw2 : Rh −→RC\n(5)\nWe know d the data size (for example for MNIST d = 784) and C the number\nof classes (for example for MNIST C = 10)), but we do not know h, which\nis an hyperparameter to be determined during validation (for example to\nachieve a 98% accuracy on MNIST, h = 500).\nIn the terminology of neural networks, that we shall not fully discuss here,\nwe say that h is the dimension of the hidden layer. So, we have deﬁned in\n(4) a neural network consisting of the following layers:\n• the input layer, deﬁned by the linear function ℓw1 and taking as input\nthe image x;\n• the hidden layer deﬁned by ℓw2 taking as input the vector RELU(ℓw1(x));\n• the output layer providing a score for each class.\nWe say that the score in (4) deﬁnes a two layer network, since we do not\ncount (by convention) the input layer8. In Fig. 5 we give schematically the\nlayers by nodes, each node corresponding to the coordinate of the vector the\nlayer takes as input; in the picture below, we have d = 3, h = 4, C = 2.\n8Some authors have RELU count as a layer, we do not adhere here to such convention.\n14\nFigure 5: Representation of a two layer network\nNotice that such score function is evidently not diﬀerentiable, while we\nwould need it to be, in order to apply the stochastic gradient descent algo-\nrithm. Such problem is solved with some numerical tricks, we do not detail\nhere.\nThe supervised classiﬁcation Deep Learning algorithm, where the score\nfunction is expressed as the composition of several aﬃne layers with RELU\nfunctions between them as in (4), takes the name of multilayer perceptron\n(see also Appendix C for a variation with convolution functions). In Ap-\npendix C we discuss the mathematical signiﬁcance of such score function in\nclassiﬁcation and regression tasks.\nConvolution functions.\nThese functions characterize a particular type\nof Deep Learning networks, called Convolutional Neural Networks (CNNs),\nthough some authors identify Deep Learning with CNNs.We shall describe\nbrieﬂy one dimensional convolutions, though image classiﬁcation requires two\ndimensional ones, see the Appendix C for more details.\nOne dimensional convolutions arise as discrete analogues of convolutions\nin mathematical analysis, more speciﬁcally with an integral kernel (integral\ntransform). Let us consider a vector x ∈Rd. We can deﬁne a simple one\ndimensional convolution as follows:\nconv1d : Rd −→Rd−r,\n(conv1d(x))i =\nr\nX\nj=1\nKjxi+j\n15\nwhere K is called a ﬁlter and is a vector of weights in Rr, r < d. We shall\ndescribe in detail and in a more general way two (and higher) dimensional\nconvolutions in the Appendix C. CNNs networks typically consist of several\nlayers of convolutions, followed by few linear layers, separated by RELU\nfunctions.\n3.2\nLoss Function\nThe loss function for a given image x measures how accurately the algorithm\nis performing its prediction, once a score s(x, w) is assigned to x, for given\nset of weights w. A popular choice for the loss function is the cross entropy\nloss, deﬁned as follows:\nL(x, w) = −log\nesyx(x,w)\nPC\nj=1 esj(x,w)\nwhere C is the number of classes in our classiﬁcation task and yx is the label\nassigned to the datum x, that is a number between 0 and C−1, corresponding\nto the class of x (in our main examples, x is an image).\nIn mathematics the cross-entropy of the probability distribution p relative\nto the probability distribution q is deﬁned as:\nH(q, p) = −Eq[log p] = −\nC\nX\ni=1\nqi log pi\nwhere Eq[log(p)] denotes the expected value of the logarithm of p according\nto the distribution q and the last equality is the very deﬁnition of it in the\ncase of discrete and ﬁnite probability distributions.\nIn our setting, q represents the ground truth distribution and it is a\nprobability mass distribution:\nqi(x) =\n\u001a 1\nif x has label i\n0\notherwise\n(6)\nand depends on x only. The distribution p is obtained from the score function\ns via the softmax function S, that is:\npi(x, w) = −log\nesyx(x,w)\nPC\nj=1 esj(x,w)\n16\nwhere\nS : RC −→RC,\n(S(z1 . . . zC))i =\nezi\nPC\nj=1 ezj\nNotice that p depends on both the image x and the weights w. With such q\nand p we immediately see that:\nH(q(x), p(x, w))\n= −Eq[log p] = −PC\ni=1 qi(x) log pi(x, w) =\n= −PC\ni=1 qi(x) log\nesi(x,w)\nPC\nj=1 esj(x,w) = L(x, w)\nsince qi(x) = 1 for i = yx and qi(x) = 0 otherwise.\nIn our setting the cross entropy of q and p coincides with the Kullback-\nLeibler divergence. In fact, in general if q and p are two (discrete) probability\ndistributions, we deﬁne their Kullback-Leibler divergence as:\nKL(q||p) =\nX\ni\nqi log qi\npi\nNotice that KL(q||p) gives a measure on how much the probability distribu-\ntions q and p diﬀer from each other. We can write:\nKL(q||p) =\nX\ni\nqi log qi\npi\n=\nX\ni\nqi log(qi) −\nX\ni\nqi log(pi) = H(q) + H(q, p)\nwhere H(q) = P\ni qi log(qi) is the Shannon entropy of the distribution q.\nH(q) measures how much the distribution q is spread; it is zero for a mass\nprobability distribution. Hence in our setting, where q(x) is as in (6) we have\nKL(q(x)||p(x, w)) = H(q(x), p(x, w)) = L(x, w)\nFor a link to the important ﬁeld of information geometry see our Appendix\nA.\n4\nGeometric Deep Learning: Graph Neural\nNetworks\nGeometric Deep Learning (GDL), in its broader sense, is a novel and emerg-\ning ﬁeld in machine learning ([8, 7] and refs therein) employing deep learning\n17\ntechniques on datasets that are organized according to some underlying ge-\nometrical structure, for example the one of a graph. Convolutional Neural\nNetworks are already part of GDL, as they are applied to some data that is\norganized in grids, as it happens for images, represented by matrices, where\nevery entry is a pixel, hence in a grid-like form. When the data is orga-\nnized on a graph, we enter the ﬁeld of the so called Graph Neural Networks\n(GNNs), deep learning algorithms that are built to leverage the geometric\ninformation underlying this type of data. In this paper we shall focus on\nGNNs for supervised node classiﬁcation only, as we shall make precise later.\nData organized on graphs are commonly referred to as non-euclidean data9.\nData coming with a graph structure are ubiquitous, from graph meshes in\n3D imaging, to biological and chemical datasets: these data have usually\nthe form of one or more graphs having a vector of features for each vertex,\nor node. Because of the abundance of datasets of this form, as the graph\nstructure usually carries important information usually ignored by standard\nDeep Learning algorithms (i.e. algorithms that would consider the node fea-\ntures only to solve the tasks at hand), Graph Neural Networks carry a great\npotential for more applications, than just Deep Learning.\nWe start with a very brief introduction on graph theory, that may be\nvery well skipped by mathematicians familiar with it, then we will concen-\ntrate on some score functions for Graph Neural Networks, discussing concrete\nexamples.\n4.1\nGraphs\nWe give here a very brief introduction to graphs and their main properties.\nFor more details see [14].\nDeﬁnition 4.1. A directed graph is a pair G = (V, E), where V is a ﬁnite\nset V = v1, . . . , vn and E ⊂V × V 10. The elements of V are called nodes or\nvertices, while the elements of E are called edges. If (vi, vj) is an edge, we\ncall vi the tail and vj the head of the edge. For each edge (vi, vj) we assume\ni ̸= j, that is there are no self loops.\n9Notice that since all graphs are embeddable into euclidean space (see [14]), this ter-\nminology is in slight conﬂict with the mathematical one.\n10We shall content ourselves to consider graphs having at most one vertex in each\ndirection between two nodes.\n18\nWe represent a graph in the plane by associating points to vertices and\narrows to edges, the head and tail of an edge corresponding to the head and\ntail of the arrow and drawn accordingly as in Fig 6. We also denote the edge\n(vi, vj) with eij. A graph is undirected if, whenever eij ∈E, we have that\nalso eji ∈E.\nWhen the graph is undirected, we do not draw arrows, just links corre-\nsponding to vertices as in Fig. 6.\nFigure 6: Directed and unidirected graph examples\nDeﬁnition 4.2. The adjacency matrix A of a graph G is a |V | × |V | matrix\ndeﬁned as\nAij =\n(\n1\neij ∈E\n0\notherwise\nNotice that in an undirected graph aij = 1 if and only if aji = 1, hence\nA is symmetric. Let us see the adjacency matrices for the graphs shown in\nFig. 6.\nA =\n\n\n\n\n0\n1\n1\n0\n1\n0\n1\n0\n1\n1\n0\n1\n0\n0\n1\n0\n\n\n\n\nFigure 7: Adjacency matrix for the\nundirected graph in Fig. 6.\nA =\n\n\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n\n\nFigure 8: Adjacency matrix for the\ndirected graph in Fig. 6.\n19\nIn machine learning is also important to consider the case of a weighted\nadjacency matrix W = (wij) associated to a graph: it is a |V | × |V | matrix\nwith real entries, with wij ̸= 0 if and only if eij is an edge. So, eﬀectively,\nit is a way to associate to an edge a weight. Similarly, we can also deﬁne\nthe node weight matrix, it is a |V | × |V | diagonal matrix and allows us to\nassociate a weight to every node of a graph.\nDeﬁnition 4.3. Given an undirected graph G = (V, E), we deﬁne the node\ndegree of the node v as the number of edges connected with v. The degree\nmatrix D is a |V | × |V | diagonal matrix with the degree of each node on its\ndiagonal, namely Dii = deg(vi) := P\nj Aij. We refer to the set of vertices\nconnected with a node vi as the neighbourhood N (vi) of the vertex vi.\nThe concept of node degree and node neighbourhood will turn out to be\nvery important in Graph Neural Networks.\nWe now introduce the incidence matrix.\nLet G = (V, E) be a graph. Let C(V ) denote the vector space of real\nvalued functions on the set of vertices V and C(E) the vector space of real\nvalued functions on edges:\nC(V ) = {f : V −→R},\nC(E) = {g : E −→R}\nWe immediately have the following identiﬁcations:\nC(V ) = R|V |,\nC(E) = R|E|\n(7)\nobtained, for C(V ), by associating to a function a vector whose ith coordinate\nis the value of f on the ith vertex; the case of C(E) being the same.\nDeﬁnition 4.4. Let G = (V, E) be a directed graph and f ∈C(V ). We\ndeﬁne the diﬀerential df of f as the function:\ndf : E −→R\neij 7−→f(vj) −f(vi).\nWe deﬁne the diﬀerential as\nd : C(V ) −→C(E),\nf 7→df\n20\nWe can think of d as a boundary operator in cohomology, viewing the\ngraph as a simplicial complex. We will not need this interpretation in the\nsequel.\nIt is not diﬃcult to verify that in the natural identiﬁcation (7) the diﬀer-\nential is represented by the matrix:\nXij =\n\n\n\n\n\n−1\nif the edge vj is the tail of the edge ei\n1\nif the edge vj is the head of the edge ei\n0\notherwise\n.\nNotice that X row indeces correspond to edges, while the column ones to\nvertices (we number edges here regardless of vertices).\nIn other words if f = (fi) is a function on V , identiﬁed with a vector in\nR|V |, we have:\n(df)k =\n|V |\nX\ni=1\nXkifi,\ndf = ((df)k) ∈R|E|\nThe matrix X is called the incidence matrix of the directed graph G =\n(V, E). Let us see an example to clarify the above statement.\nExample 4.5. For the graph in Fig. 7, we have\ndf =\n\n\n\n\n−1\n1\n0\n0\n−1\n0\n1\n0\n0\n1\n−1\n0\n0\n0\n1\n−1\n\n\n\n\n\n\n\n\nf(v1)\nf(v2)\nf(v3)\nf(v4)\n\n\n\n=\n\n\n\n\n−f(v1) + f(v2)\n−f(v1) + f(v3)\nf(v2) −f(v3)\nf(v3) −f(v4)\n\n\n\n=\n\n\n\n\ndf(e1)\ndf(e2)\ndf(e3)\ndf(e4)\n\n\n\n\nNotice that, suggestively, many authors, including [8] write the symbol\n∇for X, because this is eﬀectively the discrete analogue of the gradient in\ndiﬀerential geometry.\n4.2\nLaplacian on graphs\nIn this section we deﬁne the laplacian on undirected graphs according to\nthe reference [8]. Notice that there are other deﬁnitions, see [4, 14], but we\nadhere to [8], because of its Geometric Deep Learning signiﬁcance.\nWe start with the deﬁnition of laplacian matrix, where, for the moment,\nwe limit ourselves to the case in which both edges and nodes have weight 1\n21\n(see Sec. 4.1). We shall then relate the laplacian matrix with the incidence\nmatrix and arrive to a more conceptual deﬁnition in terms of the diﬀerential.\nDeﬁnition 4.6. The laplacian matrix L of an undirected graph is a |V |×|V |\nmatrix, deﬁned as\nL := D −A,\nwhere A and D are the adjacency matrix and degree matrix, respectively.\nWe now introduce the concept of orientation.\nDeﬁnition 4.7. An orientation of an undirected graph is an assignment of\na direction to each edge, turning the initial graph into a directed graph.\nFigure 9: An orientation on a graph\nThe following result links the laplacian on graph with the incidence ma-\ntrix.\nProposition 4.8. Let G = (V, E) be an undirected graph and let us ﬁx an\norientation. Then:\nL = XtX\nwhere X is the incidence matrix associated with the given orientation.\nNotice that this result is independent from the chosen orientation. The\nproof is not diﬃcult and we leave it to reader; the next example will clearly\nshow the argument.\nExample 4.9. Let us consider the graph of Fig.\n9.\nWe can write the\nLaplacian according to the deﬁnition as\nL = D −A =\n\n\n1\n−1\n0\n−1\n2\n−1\n0\n−1\n1\n\n.\n22\nThis is the same as writing L as\nL = XtX =\n\n\n−1\n0\n1\n1\n0\n−1\n\n\n\u0012−1\n1\n0\n0\n1\n−1\n\u0013\n=\n\n\n1\n−1\n0\n−1\n2\n−1\n0\n−1\n1\n\n.\nRemark 4.10. We warn the reader about possible confusion in the notation\npresent in the literature.\n1. The incidence matrix of an oriented undirected graph is not the same as\nthe incidence matrix of an undirected graph (that we have not deﬁned\nin here) as one can ﬁnd it for example in [14].\n2. The laplacian of a directed graph, as deﬁned for example in [4], is not\nthe laplacian of the undirected graph with an orientation and it is not\nexpressible, at least directly, in terms of its incidence matrix.\nOriginally in the pioneering works [8] and [25] the authors discuss undi-\nrected graphs only and this is the reason why we have limited our discussion\nto those, though in practical applications directed graphs may turn to be\nquite useful.\nWe end this section with a comment regarding a more intrinsec deﬁnition\nof laplacian.\nDeﬁnition 4.11. Let G = (V, E) be an undirected graph with an orientation\nand let C(V ), C(E) the functions on V and E as above.\nWe deﬁne the\nlaplacian operator as:\nL : C(V ) −→C(V ),\nL = d∗d\nwhere d∗: C(E)∗−→C(V )∗is the dual morphism and we identify C(E) and\nC(V ) with their duals with respect to their canonical bases.\nWe leave to the reader the easy check that, once we use the matrix ex-\npression for d and d∗, that is X and Xt, we obtain the same expression for\nthe laplacian as in Prop. 4.8.\nRemark 4.12. Let G = (V, E) be an undirected graph with an orientation.\nIn the suggestive interpretation of the incidence matrix as the discrete version\n23\nof the gradient of a function f on vertices, we have the correspondence:\nF : Rn −→R,\n∇(F) =\n\n\n\n∂x1F\n...\n∂xnF\n\n\n\n⇐⇒\nf : V −→R\nX(f) =\n\n\n\nf(vj1) −f(vi1)\n...\nf(vjn) −f(vin)\n\n\n\nfor edges (i1, j1), . . . , (in, jn) and where F denotes a smooth function on Rn.\nIf we furtherly extend this correspondence to the laplacian operator:\n∆(F) = (∇t∇)(F) = ∂2\nx1F + · · · + ∂2\nxnF\n⇐⇒\nL(f) = (XtX)(f)\nthus justifying the terminology “laplacian” for the operator L we have deﬁned\non C(V ).\nIn [37] this correspondence is furtherly exploited to perform spectral the-\nory on graphs, introducing the discrete analogues of divergence, Dirichlet\nenergy and Fourier transform. We do not pursue further this topic here,\nsending the interested reader to [8] for further details.\nWe also mention that a more general expression of L is obtained by writ-\ning:\nLg = Wv(D −Wa)\nwhere Wv is a diagonal matrix attributing a weight to each vertex and Wa is\nthe weighted adjacency matrix. So Lg = L if we take Wv = I and Wa = A.\nA remarkable case occurs for Wv = D−1 and Wa = A:\nLd := D−1(D −A) = I −D−1A\nin this case we speak of diﬀusive (or normalized) laplacian, since multiplying\nL by the inverse of the node degree matrix amounts in some sense to take\nthe average of each coordinate, associated to a node, by the number of links\nof that node. We will come back to Ld in our next section.\n4.3\nHeat equation\nDeep Learning success in supervised classiﬁcation tasks is due, among other\nfactors, to the convolutional layers, whose ﬁlters (i.e.\nkernels) enable an\neﬀective pattern recognition, for example in image classiﬁcation tasks.\n24\nIn translating and adapting the algorithms of Deep Learning to graph\ndatasets, it is clear that convolutions and ﬁlters cannot be deﬁned in the\nsame way, due to the topological diﬀerence of node neighbours: diﬀerent\nnodes may have very diﬀerent neighbourhoods, thus making the deﬁnition of\nconvolution diﬃcult.\nIn Graph Neural Networks, the convolutions are replaced by the mes-\nsage passing mechanism in the encoder that we shall discuss in the sequel.\nThrough this mechanism the information from one node is diﬀused in a way\nthat resembles heat diﬀusion in the Fourier heat equation, as noticed in [8].\nWe recall that the classical heat equation, with no initial condition, states\nthe following:\n∂th(x, t) = c∆h(x, t)\n(8)\nwhere h(x, t) is the temperature at point x at time t and ∆is the laplacian\noperator, while c is the thermal diﬀusivity constant, that we set equal to -1.\nWe now write the analogue of such equation in the graph context, follow-\ning [8], where now x is replaced by a node in an undirected, but oriented,\ngraph and h is a function on the vertices of the graph, depending also on the\ntime t, which is discrete. We then write:\nht+1(v) −ht(v) = −Ld(ht(v))\nwhere Ld is the diﬀusive laplacian. We now substitute the expression for the\nlaplacian Ld obtaining:\nht+1(v) = ht(v) −[(I −D−1A)ht](v) = (D−1Aht)(v) =\nX\nu∈N(v)\nht(u)\ndeg(v)\n(9)\nwhere the last equality is a simple exercise. Hence:\nht+1(v) =\nX\nu∈N(v)\nht(u)\ndeg(v)\n(10)\nwhere N(v) denotes the neighbourhood of the vertex v.\nWe are going to exploit fully this analogy in the description of graph\nconvolutional networks in the next section.\n25\n4.4\nSupervised Classiﬁcation on Graphs\nIn many applications, data is coming naturally equipped with a graph struc-\nture. For example, we can view a social network as a graph, where each node\nis a user and edges are representing, for example, the friendship between two\nusers. Also, in chemistry applications, we may have a graph associated to a\nchemical compound and a label provided for each graph classifying the com-\npound. For example, we can have a database consisting of protein molecules,\ngiven the label zero or one depending on whether or not the protein is an\nenzyme.\nSupervised classiﬁcation tasks on graphs can be roughly divided into the\nfollowing categories:\n• node classiﬁcation: given a graph with a set of features for each node\nand a labelling on the nodes, ﬁnd a function that given the graph\nstructure and the node features predicts the labels on the nodes;\n• edge classiﬁcation: given a graph with a set of features for each node\nand a labelling on the edges (like the edge exists or not), ﬁnd a function\nthat given the graph structure and the node features predicts the labels\non the edges11;\n• graph classiﬁcation: given a number of graphs with features on the\nnodes, possibly external global features and a label for each one, ﬁnd\na function to predict the label of these graphs (and similar ones).\nFor the node classiﬁcation tasks, the split train/valid/test is performed on the\nnodes, i.e. only a small subset of the nodes is used to train the score function\nand only their labels intervene to compute the loss, as we will see.\nThe\nentire graph structure and data is however available during training as GNNs\ntypically leverage the network topological structure to extract information.\nTherefore not only the information of the nodes available for training is used,\nin the sense that also the features, but not the labels, of the nodes not used in\nthe training are a priori available during the training process. For this reason,\nin this context many authors speak of semi-supervised learning. There are\nalso other classiﬁcation tasks, that we do not mention here, see [8] for more\ndetails. For clarity’s sake we focus on the node classiﬁcation task and the\n11This problem requires care, as for the training we may need to modify the graph\nstructure.\n26\nGeometric Deep Learning algorithms we will describe will fall in the category\nof Graph Neural Networks (GNNs). We shall also examine a key example,\nthe Zachary Karate Club ([25]) dataset, to elucidate the theory.\nThe ingredients for node classiﬁcation with Graph Neural Networks are\nthe same as for image classiﬁcation in Deep Learning (see Sec. 2.2), namely:\n• the score function assigning to each node with given features and to a\ngiven set of weights in Rp a score for each class;\n• the loss function measuring how accurate is the prediction obtained via\nthe score function;\n• the optimizer necessary to determine the set of weights, which perform\nbest on the training set.\nFor Graph Neural Networks loss function and optimizer are chosen in\na very similar way as in Deep Learning and the optimization procedure is\nsimilar. The most important diﬀerence occurs in the score function, hence we\nfocus on its description. In Deep Learning for a set of weights w, the score\nfunction s(x, w) assigns to the datum x ∈Rd (e.g. image), the vector of\nscores. On the other hand, in GNNs, the topological information regarding\nthe graph plays a key role.\nThe vector of features x ∈Rd of a node is\nconnected with the others according to a graph structure and therefore the\ninformation of the graph G we are considering, represented by its adjacency\nmatrix AG, needs to be given as input as well. This dependence is often\nmade explicit by denoting the resulting score functions as s(x, w, AG). Let\nus see a concrete example in more detail in the next section.\n4.5\nThe Score function in Graph Neural Networks\nIn the supervised node classiﬁcation problem, we assume to have an undi-\nrected graph G = (V, E) and for each node of the graph we have the following\ntwo key information: the features associated to the node and the label of the\nnode, which is, as in ordinary Deep Learning, a natural number between 0\nand C −1, C being the number of classes.\nWe can view the features of the nodes as a vector valued function h on\nthe nodes, hence h : V −→Rn if we have n features. Hence h ∈C(V )n =\n(R|V |)n ∼= R|V |×n and we denote h(v) or hv ∈Rn, the value of the feature h\nat the node v. When n = 1 this is equivalent to the vth coordinate of R|V |.\n27\nThe n dimensions should be thought as n feature channels according to the\nphilosophy introduced in [5] or [19].\nThe score function in the GNNs we shall consider consists of two distinct\nparts:\n1. the encoder, which produces an embedding of the graph features in a\nlatent space:\nE : R|V |×n −→R|V |×c,\n2. the decoder, which predicts the score of each node based on the embed-\nding obtained via the encoder. This is usually a multilayer perceptron\n(see Sec. 3.1) or even just a linear classiﬁer.\nWe therefore focus on the description of the encoder only.\nThe encoder implements what is called the message passing mechanism.\nThere are several ways to deﬁne it, we give one simple implementation con-\nsisting in a sequence of graph convolutions described by equation (11) below,\nthe others being a variation on this theme. We start with the initial feature\nh0\nv at each node v and we deﬁne the next feature recursively as follows:\nhk\nv = σ\n\nWk\nX\nu∈N(v)\nhk−1\nu\ndeg(v) + Bk · hk−1\nv\n\n\nk = 1, . . . , ℓ\n(11)\nwhere ℓis the number of layers in the encoder, σ is a generic non linearity,\nfor example the RELU and Wk, Bk are matrices of weights of the appropriate\nsize, which are determined during the training. We say that equation (11)\ndeﬁnes a message passing mechanism. Training typically takes place with a\nstochastic gradient descent, looking for a local minimum of the loss function.\nNotice that the equation above reminds us of the heat equation (9) we in-\ntroduced in the context of graphs having scalars as node features. Indeed, in\nthe case all Bk = 0 for all k, if we denote as Hk ∈R|V |×p the matrix having\nas rows all node features after having applied the ﬁrst k layers, the k-step of\nthe encoder in (11) above, can be written concisely, for all the node features\ntogether, as\nHk = σ(D−1AGHk−1W t\nk)\nwhere D is the diagonal matrix of degrees and AG is the adjacency matrix of\nG. As a consequence, the encoder can be thought, modulo non linearities, as a\n28\n”1-step Euler discretization” of a PDE of the form ˙H = −∆H(t), H(0) = H0\ni.e. as following the discrete equation\nH(t + 1) −H(t) = −LdH(t)\nwhere we have denoted as Ld the diﬀusive laplacian of Section 4.3 and\nH0 is the starting datum of node features. By modifying equation (11) we\nobtain some notable graph convolutions:\n• If Bk = Wk we obtain the inﬂuential Kipf and Welling graph convolu-\ntion [25] that can be written more concisely as\nf(Hk, AG) = σ(D−1 c\nAGHk−1W t\nk)\n(12)\nwhere c\nAG = AG + Id. This convolution can be seen to arise as a 1-step\nEuler discretization of an heat equation on a graph where one self loop\nis added to each vertex.\n• If we do not normalize by the degress, equation 11 gives us an instance\nof the inﬂuential GraphSAGE operator from [17].\nIn the next section we give a concrete example of the message passing\nmechanism and how the encoder and decoder work.\n4.6\nThe Zachary Karate Club\nThe Zachary Karate club is a social network deriving from a real life karate\nclub studied by W. Zachary in the article [25] and can be currently regarded\nas a toy benchmark dataset for Geometric Deep Learning. The dataset con-\nsists in 34 members of the karate club, represented by the nodes of a graph,\nwith 154 links between nodes, corresponding to pairs of members, who inter-\nacted also outside the club. Between 1970 and 1972, the club became divided\ninto four smaller groups of people, over an internal issue regarding the price\nof karate lessons. Every node is thus labeled by an integer between 0 and\n3, corresponding to the opinion of each member of the club, with respect to\nthe karate lesson cost issue (see Fig. 10). Since the dataset comes with no\nfeatures (i.e. is feature-less) associated to nodes, but only labels, we assign\n(arbitrarily) the initial matrix of features to be the identity i.e. H0 = Id34.\nWe represent schematically below a 3 layers GNN, used in [25] on this\ndataset the encoder-decoder framework, reproducing at each step the equa-\ntion (12)12.\nIn other words we have functions (conv1), (conv2), (conv3),\n12A particular case of (11) with l = 3\n29\nFigure 10: Representation of the Zachary Karate Club dataset (see [25]).\nDiﬀerent colors correspond to diﬀerent labels.\ngiven by the formula (12) for weight matrices Wk of the appropriate size,\nwith σ = tanh\n(conv1): R34 −→R4, h1\nv 7→h2\nv\n(conv2): R4 −→R4, h2\nv 7→h3\nv\n(conv3): R4 −→R2, h3\nv 7→h4\nv\n\n\nENCODER\n(classiﬁer): R2 −→R4, h4\nv 7→ℓW(h4\nv) DECODER\nThe encoding space has dimension 2 and the encoding function E ob-\ntained is:\nE : R34 −→R2,\nE = conv3 ◦conv2 ◦conv1\nThe decoder consists in a simple linear classiﬁer ℓW : R2 −→R4, where W\na 4 × 2 matrix of weights, the image being in R4, since we have 4 labels.\nIn the training, according to the method elucidated in Sec. 2.2, we choose\nrandomly 4 nodes, one for each class, with their labels and we hide the\nlabels of the remaining 30 nodes, that we use as validation. We have no\ntest dataset. In Fig. 11, we plot the node embeddings in R2, that is the\nresult of the 34 node coordinates after the encoding. As one can readily\nsee from Fig 11, our dataset, i.e. the nodes, appears more and more linearly\nseparable, as we proceed in the training. The ﬁnal accuracy is exceeding 70%\n30\nFigure 11: Representation of node embeddings at diﬀerent epochs ([25]).\nDiﬀerent colors correspond to diﬀerent labels.\non the 4 classes, a remarkable result considering that our training dataset\nconsists of 4 nodes with their labels. This is actually a feature of Graph\nNeural Networks algorithms: typically we need far less labelled samples in\nthe training datasets, in comparison with the Deep Learning algorithms. This\nis due to the fact that the topology of the dataset already encodes the key\ninformation for an eﬀective training and that the features of the validation\nset are a priori visible to the training nodes. This important characteristic\nmakes this algorithm particularly suitable for biological datasets, though it\nleaves to the researcher the diﬃcult task to translate the information coming\n31\nfrom the dataset into a meaningful graph.\n4.7\nGraph Attention Networks\nWe conclude our short treatment with the Graph Attention Networks (GATs)\n[39].\nThe diﬀusivity nature of the message passing mechanism makes all\nedges connected to the same node equally important, while it is clear that in\nsome concrete problems, some links may have more importance than others\nand we might have links that exists even if the vertices have very diﬀerent\nlabels (heterophily). The convolution described in the previous section may\nnot perform well in this setting. A ﬁrst ﬁx to this problem is to consider\nweighted adjacency matrices, whose weights can be learnt together with the\nother weights of the network during training.\nAnother approach is given by the graph attentional layers, that we are\ngoing to describe, that have the purpose to assign a weight to the edges\nconnected to a given node with a particular ”attention mechanism” to high-\nlight (or suppress) their importance in diﬀusing the features from the node.\nThere are also more advanced algorithms to handle hetherophilic datasets13,\nlike Sheaf Neural Networks (see [5]), but we shall not discuss them here.\nWe describe in detail a single attention convolution layer, introduced in the\nseminal [39]. Let W be a weight matrix and hv, hu ∈Rd the (vector valued)\nfeatures of nodes v and u. To start with, we deﬁne an attention mechanism\nas follows:\na : Rd′ × Rd′\n−→\nR\nWhv, Whu\n−→\nevu\nThe coeﬃcients evu are then normalized using the softmax function:\nαvu =\nexp(evu)\nP\nw∈N(v) exp(evw).\nobtaining the attention coeﬃcients αvu. The message passing mechanism is\nthen suitably modiﬁed to become:\nh′\nv = σ\n\nX\nu∈N(v)\nαvuWhu\n\n.\n13Hetherophilic dataset are those whose linked nodes do not share similar features.\n32\nIn the paper [39], Velickovic et al. choose the following attention mech-\nanism.\nFirst, we concatenate the vectors Whv and Whu of dimension d′\nand then we perform a scalar multiplication by a weight vector a ∈R2d′.\nThen, as commonly happens, we compose with a LeakyRELU non-linearity\n(a modiﬁed version of the RELU)14. The resulting attention coeﬃcients are\nthen\nαvu =\nexp\n\u0000σ\n\u0000a⊤[Whv||Whu]\n\u0001\u0001\nP\nw∈N(v) exp (σ (a⊤[Whv||W w]))\nwhere || is the concatenation operation and σ is the LeakyRELU activation\nfunction. After having obtained the node embeddings h′\nv, Velickovic et al.\nintroduce an optional multi-head attention that consists into concatenating K\nidentical copies of the embeddings h′\nv that will become the node embeddings\npassed to the following layer. The ﬁnal message passing becomes then\nh′\nv =∥K\ni=0 σ\n\nX\nu∈N(v)\nαvuWhu\n\n.\nK is also called the number of heads. A layer of this type in the encoder of\na GNN is called Graph Attention Layer and a GNN whose encoder consists\nof a sequence of such layers is called Graph Attention Network (GAT).\nTo see a concrete example of a classiﬁcation problem successfully solved by\nGAT, we consider the example of the Cora dataset, which is studied also by\nVelickovic et al. in [39]. The Cora dataset is an undirected graph consisting\nof\n• 2708 nodes, each representing a computer science paper,\n• 5209 edges, representing paper citations.\nEach node v is given a feature hv consisting of a 1433-dimensional vector\ncorresponding to a bag-of-words representation of the title of the document\nand a label assigning each node to one of 7 distinguished classes (Neural\nNetworks, Case Based, Reinforcement Learning, Probabilistic Methods, Ge-\nnetic Algorithms, Rule Learning, and Theory). Velickovic et al. build the\nfollowing architecture:\n(conv1) : ELU(GATConv(1433, 8))\nheads = 8\n(conv2) : σ(GATConv(64, 7))\nheads = 1\n14The choice of the activation function here is not fundamental, we mention the Leaky\nRelu only to stick to Velickovic et al. treatment.\n33\nwhere we have denoted as ELU the Exponential Linear Unit activation func-\ntion (a slight modiﬁcation of the RELU, see [10]) and as σ the softmax\nactivation used for the ﬁnal classiﬁcation. Using a training set consisting of\n20 nodes per class, a validation set of 500 nodes (to tune the hyperparame-\nters) and a test set of 1000 nodes, the above network achieves an accuracy\nof the 83%.\nAcknowledgements. R.F. wished to thank Prof. S. Soatto, Dr. A.\nAchille and Prof. P. Chaudhari for the patient explanations of the functioning\nof Deep Learning. R.F. also wishes to thank Prof. M. Bronstein, Dr. C.\nBodnar for helpful discussions on the geometric deep learning on graphs.\nF.Z. thanks Dr. A. Simonetti for helpful discussion on both the theory and\nthe practice of Geometric Deep Learning Algorithms.\nA\nFisher matrix and Information Geometry\nIn this appendix we collect some known facts about the Fisher information\nmatrix and its importance in Deep Learning. Our purpose is to establish a\ndictionary between Information Geometry and (Geometric) Deep Learning\nquestions, to help with the research in both. No prior knowledge beyond\nelementary probability theory is required for the appendix. We shall focus\non discrete probability distributions only, since they are the ones interesting\nfor our machine learning applications. For more details we send the interested\nreader to [3], [35] and refs. therein.\nLet p(x, w) = (p0(x, w), . . . , pC−1(x, w)) be a discrete probability distri-\nbution, representing the probability that a datum x is assigned a class among\n0, . . . , C −1. In machine learning p(x, w) is typically an empirical probabil-\nity and depends on parameters w ∈Rp. As we described previously, during\ntraining p(x, w) changes and the very purpose of the training is to obtain a\np(x, w) that gives a good approximation of the true probability distributions\nq(x) on the training set, and of course, also performs well on the validation\nset.\nDeﬁnition A.1. Let p(x, w) be a discrete probability distribution, we deﬁne\ninformation loss of p(x, w) as\nI(x, w) = −log(p(x, w))\n(13)\n34\nIn [3], Amari refers to I as the loss function, however, given the im-\nportance of the loss function in Deep Learning, we prefer to call I(x, w)\ninformation loss. Notice that I(x, w) is a probability distribution.\nThe information loss is very important in Deep Learning: its expected\nvalue with respect to the true probability distribution q(x) as deﬁned in (6)\nis the cross entropy loss, one the most used loss functions in Deep Learning.\nDeﬁnition A.2. Let p(x, w) be a discrete empirical probability distribu-\ntion, q(x) the corresponding true distribution. We deﬁne loss function the\nexpected value of the information loss with respect to q(x):\nL(x, w) = −Eq[log p(x, w)] = −\nC\nX\ni=1\nqi(x) log pi(x, w)\nGiven two probability distributions p and q, the Kullback Leibler diver-\ngence intuitively measures how much they diﬀer and it is deﬁned as:\nKL(q||p) :=\nX\ni\nqi log qi\npi\nAs we have seen in Sec. 3.2 we have that:\nKL(q(x)||p(x, w)) = L(x, w) + H(q(x)) = L(x, w)\nsince H(q(x)) = 0 for a mass probability density.\nNow we turn to the most important deﬁnition in Information Geometry:\nthe Fisher information matrix.\nDeﬁnition A.3. Let p(x, w) be a discrete empirical probability distribution,\nq(x) the true distribution. We deﬁne F the Fisher information matrix or\nFisher matrix for short, as\nFij(x, w) = Ep[∂wi log(p(x, w)∂wj log(p(x, w)]\nOne way to express coincisely the Fisher matrix is the following:\nF(x, w) = Ep[∇log(p(x, w)(∇log(p(x, w))t]\nwhere we think ∇log(p(x, w) as a column vector and T denotes the transpose.\nNotice that F(x, w) does not contain any information regarding the true\ndistribution q(x).\n35\nRemark A.4. Some authors prefer the Fisher matrix to depend on the\nparameters only, hence they take a sum over the data:\nF(w) =\nX\nx\nF(x, w)\nWe shall not take this point of view here, adhering to the more standard\ntreatment as in [3].\nObservation A.5. Notice that F(x, w) is symmetric, since it is a ﬁnite sum\nof symmetric matrices. Notice also that it is positive semideﬁnite. In fact\nutF(x, w)u = Ep\n\u0002\nut∇w log p(x, w)(∇w log p(x, w))tu\n\u0003\n=\n= Ep\n\u0002\n⟨∇w log p(x, w), u⟩2\u0003\n≥0.\n(14)\nwhere ⟨, ⟩denotes the scalar product in Rp.\nThe next proposition shows that the rank of the Fisher matrix is bound\nby the number of classes C. This has a great impact on the Deep Learning\napplications: in fact, while the Fisher matrix is quite a large matrix, p × p,\nwhere p is typically in the order of millions, the number of classes is usually\nvery small. Hence the Fisher matrix has a very low rank compared with its\ndimension; e.g. in MNIST, the rank of the Fisher is no larger than 9, while\nits dimension is of the order typically above 104.\nProposition A.6. Let the notation be as above. Then:\nrkF(x, w) < C\nProof. We ﬁrst show that ker F(x, w) ⊆(spani=1,...,C{∇w log pi(x, w)})⊥:\nu ∈ker F(x, w) ⇒uTF(x, w)u = 0 ⇒Ep\n\u0002\n⟨∇w log p(x, w), u⟩2\u0003\n= 0\n⇒⟨∇w log pi(x, w), u⟩= 0\n∀i = 1, . . . , C.\n(15)\nOn the other hand, if u ∈(spani=1,...,C{∇w log pi(x, w)})⊥then u ∈ker F(x, w):\nF(x, w)u = Ep [∇w log pi(x, w)⟨∇w log pi(x, w), u⟩] = 0.\n(16)\nThis shows that rank F(x, w) ≤C.\n36\nThe vectors ∇w log pi(x, w) are linearly dependent since\nEp[∇wI(x, w)] =\nC\nX\ni=1\npi(x, w)∇w log pi(x, w) =\n=\nC\nX\ni=1\n∇wpi(x, w) = ∇w\n C\nX\ni=1\npi(x, w)\n!\n=∇w1 = 0.\n(17)\nTherefore, we deduce rank F(x, w) < C.\nWe now relate the Fisher matrix to the information loss.\nProposition A.7. The Fisher matrix is the covariance matrix of the gradient\nof the information loss.\nProof. The gradient of the information loss is\n∇wI(x, w) = −∇wp(x, w)\np(x, w)\nNotice:\nEp(∇wI) = P pi\n∇wpi\npi\n= P\ni ∇wpi = ∇w(P\ni pi) = 0\nThe covariance matrix of ∇wI(x, w) is (by deﬁnition):\nCov(I) = Ep[(∇wI −Ep(∇wI))t(∇wI −Ep(∇wI))] =\n= Ep[(∇wI)t(∇wI)] = F(x, w)\nWe conclude our brief treatment of Information Geometry by some ob-\nservations regarding the metric on the parameter space.\nObservation A.8. We ﬁrst observe that:\nKL(p(x, w + δw)||p(x, w))\n∼= 1\n2(δw)tF(x, w)(δw) + O(||δw||3)\nThis is a simple exercise based on Taylor expansion of the log function.\nLet us interpret this result in the light of Deep Learning, more speciﬁcally,\nduring the dynamics of stochastic gradient descent. The Kullback-Leibler\ndivergence KL(p(x, w + δw)||p(x, w)) measures how p(x, w + δw) and p(x, w)\n37\ndiﬀer, for a small variation of the parameters w, for example for a step\nin stochastic gradient descent. If F is interpreted as a metric as in [3] 15,\nthen (δw)tF(x, w)(δw) expresses the size of the step δw. We notice however\nthat, because of Prop. A.6, this interpretation is not satisfactory. Even if\nwe restrict ourselves to the subspace of Rp where the Fisher matrix is non\ndegenerate, we cannot construct a submanifold, due to the non integrability\nof the distribution of the gradients, that we observe experimentally (see [15]).\nMoreover the dynamics does not even follow a sub-riemannian constraint\neither, due to the non constant rank of the Fisher. This very challenging\nbehaviour will be the object of investigation of a forthcoming paper.\nIn the next proposition, we establish a connection between Information\nGeometry and Hessian Geometry, since the metric, given by F, can be viewed\nin terms of the Hessian of a potential function.\nProposition A.9. Let the notation be as above. Then\nF(x, w) = Ep[H(I(x, w))]\nwhere\nI(x, w) = −log p(x, w)\nProof. In fact (write p = p(x, w)):\nH[I] = −Jac\n\u0014∇wp\np\n\u0015\n= −[H(p) · p + ∇wp · ∇wp] 1\np2\nTake the expected value:\nEp[H[I]] = −\nX\ni\npi\nH(pi)\npi\n+ Ep\n\u0014∇wp\np\n· ∇wp\np\n\u0015\n= F\nwhere P\ni H(pi) = H(P\ni pi) = 0.\nB\nRegression tasks\nIn this appendix we provide some explanations and references on another\nvery important learning task that can be handled using Deep Learning: re-\ngression. In the main text we focused on classiﬁcation tasks, such as image\nrecognition. We may be interested in other practical problems as well, for\nexample we may want to predict the price of an house or of a car given some\n15The low rank of F in Deep Learning, makes this interpretation problematic.\n38\nof its features such as dimension, mileage, year of construction/production,\netc. In general, given some numerical data we may want to predict a num-\nber, or a vector, rather than a (probability of belonging to) a class. These\ntasks are called regression tasks to distinguish them from classiﬁcation tasks.\nFrom an algorithmic point of view, regression tasks are handled and trained\nas classiﬁcation tasks, the most important diﬀerence being that diﬀerent loss\nfunctions in step 2 of Section 2.1 are usually employed. In particular, steps 0,\n1, 3, 4, 5 described in Section 2.1 can be repeated almost verbatim with the\nonly conceptual diﬀerence that we are not trying to predict a class but rather\nto infer a vector: therefore it is more appropriate to speak of a regression or\nprediction function instead of a score function, etc.\nFor example, for step 1 if our regression task consists in predicting p dimen-\nsional vectors from d dimensional features, we choose a suitable regression\nor prediction function P(x, w) : Rd × Rp →Rc that we can think of a score\nfunction where the codomain has not to be thought as a ”vector of probabil-\nities” but rather as the actual prediction or regression of our algorithm. For\nstep 2, suppose that we have a regression function P(x, w) : Rd × Rp →Rc\nand that we denote, for any sample xi ∈Rd, i = 1, ..., N, with yi ∈Rc the\nvector we would like to predict. The following are then some common loss\nfunctions we can use for training:\n• Mean Squared Error (MSE) or L2-norm:\nL(w) := 1\nN\nN\nX\ni=1\n||P(xi, w) −yi||2\n2\n• Root Mean Squared Error (RMSE):\nL(w) :=\nv\nu\nu\nt 1\nN\nN\nX\ni=1\n||P(xi, w) −yi||2\n2\n• Mean Absolute Error (MAE) or L1-norm:\nL(w) := 1\nN\nN\nX\ni=1\n||P(xi, w) −yi||1\nThe MSE is one of the most important loss functions when it comes to re-\ngression tasks and in the past also classiﬁcation tasks were sometimes treated\nas regression tasks and the MSE loss was used in these cases as well.\n39\nRemark B.1. Performing a regression task using the MSE loss function\nto train a single layer MLP model is equivalent to solve an ordinary linear\nregression (see [20]) problem using a gradient descent algorithm, as a single\nlayer MLP is simply an aﬃne map.\nA priori, loss functions whose ﬁrst-order derivatives do not exist might\nbe problematic for stochastic descent algorithm (that is why the MSE is\nsometimes preferred over the MAE, for example). To solve this issue in some\npractical cases when a non diﬀerentiable loss function like the MAE would\nbe appropriate for the regression problem at hand but problematic from the\ntraining viewpoint, the solution is often to use more regular functions that\nare similar to the desired non-diﬀerentiable loss function whose behaviour is\ndesired. For example the ”Huber” or the ”log(cosh)” loss functions could\nused in place of the MAE, see [36].\nC\nMulti-layer perceptrons and Convolutional\nNeural Networks\nIn this appendix we give a self-contained explaination of multi-layer percep-\ntrons and convolutional neural networks, complementing the one we give\nin the main body of the paper.\nGiven an aﬃne map F : Rn →Rm,\nF(x) = Ax + b,\nA ∈Mm,n(R), b ∈Rm, we will call b the bias of F. By\nviewing a matrix A ∈Mm,n(R) as a vector of Rmn we can deﬁne an aﬃne\n(or, abusing notation linear) layer as a couple (F, w) where F is an aﬃne\nmap and w is the vector of weights obtained concatenating A and the bias\nvector. One dimensional convolutions are the discrete analogues of convolu-\ntions in mathematical analysis. Let us consider a vector x ∈Rd. As already\ndid in the main text, we can deﬁne a simple one dimensional convolution as\nfollows:\nconv1d : Rd −→Rd−r,\n(conv1d(x))i =\nr\nX\nj=1\nKjxi+j\nwhere K is called a ﬁlter and is a vector of weights in Rr, r < d. This should\nserve as the basic example to keep in mind and the analogy with continuous\nconvolutions is clear in this case.\nTo generalize to the case of 2-dimensional convolutions and to obtain more\ngeneral 1-dimensional convolutions, it is convenient to deﬁne one dimensional\nconvolutions to be the following slightly more general functions:\n40\nDeﬁnition C.1. Let be K ∈Rr, and consider index functions a(i, j), α(i, j) :\n[l] × [d] →N, where we have denoted as [n] the subset {1, · · · n} ⊂N and\na(i, −) : [d] →N is assumed to be injective for all i ∈[l]. We deﬁne one\ndimensional convolution operators as functions of the following form\nconv1d : Rd −→Rl,\nconv1d(x)i =\nd\nX\nj=1\nKα(i,j)xa(i,j) + bi\nwhere Rd ∋x = (xi)d\ni=1, we set xk = 0, Kh = 0 if k /∈[d], h /∈[r] and\nbi ∈R is a bias term.\nWe will call the vector w ∈Rr+l resulting from\nthe concatenation of K and all the biases the vector of weights associated\nto conv1d. A one dimensional convolutional layer is a couple (conv1d, w)\nwhere conv1d is a one dimensional convolution and w is the vector of weights\nassociated to it.\nRemark C.2. Under the assumptions and the notations of the previous\ndeﬁnition if we set a(i, j) := i + j, α(i, j) = j, l = d −r and bi = 0 for all\ni = 1, ..., l we get the simple convolution we deﬁned before. In addition, the\nindex functions a(i, −) and α(i, j) are usually chosen to be non-decreasing,\nthus resembling the discrete analogue of the convolution usually employed in\nanalysis.\nBesides the simple convolution we deﬁned at the beginning of this para-\ngraph, there are some standard choices of the functions a(i, j), α(i, j) that\nare controlled by parameters known as stride, padding, etc. We will not in-\ntroduce them here as they become useful only when constructing particular\nmodels: we refer to [29] for a discussion.\nMany convolutions used in practice, in addition to having increasing index\nfunctions, have l, r ≤d and are said to have one output channel, or are of\nthe form conv1d×e : Rd →Rl×e where conv1d is a one output channel, one\ndimensional convolution. The number r in these cases is called the kernel\nsize. We can deﬁne also the so called ”pooling functions”.\nDeﬁnition C.3. Consider an index function a(i, j) : [l] × [d] →N, as in\nthe previous deﬁnition. For any i ≤l and any vector x ∈Rd, we denote as\nxa(i,−) ∈RR the vector (xa(i,j))d\nj=1. We say that a function P : Rd →Rl is a\npooling layer or operator if P(x)i = ϕ(xa(i,−)) for a ﬁxed pooling function ϕ.\nIf ϕ is the arithmetic mean or the maximum function we will call the layer\nP mean pooling or max pooling layer respectively. A pooling layer does not\nhave an associated set of weights.\n41\nRemark C.4. Pooling layers are not meant to be trained during the training\nof a model, therefore they do not have an associated vector of weights.\nTwo dimensional convolutions are conceptually the analogue of one di-\nmensional convolutions in the context of matrices. Indeed, some data like\nimages can be more naturally thought as grids (matrices) of numbers rather\nthan vectors. Even if two dimensional convolutions are a particular case of\none dimensional convolutions, because of their importance and their widespread\nuse it is important to discuss them on their own.\nDeﬁnition C.5. Denote as Md,q(R) the space of d × q real valued matri-\nces.\nLet be Khk ∈Md,q(R), and let be a(i, j), α(i, j) : [r] × [n] →N,\nb(i, j), β(i, j) : [s] × [m] →N index functions where a(i, −), b(j, −) are as-\nsumed to be injective for all i, j. We deﬁne two dimensional convolution\noperators as functions of the following form\nconv2d : Mn,m(R) −→Mr,s(R),\n(conv2d(A))ij =\nn\nX\nh=1\nm\nX\nk=1\nKα(i,h)β(j,k)Aa(i,h)b(j,k)+bij\nwhere Aij ∈Mn,m(R), as in the case of one dimensional convolutions we set\nAij = 0, Khk = 0 if either i > n, j > m, h > d or k > q and bij ∈R is a bias\nterm.\nRemark C.6. Using the canonical isomorphism Mn,m(R) ∼= Rn×m (notice\nthat it is Z-linear) we can see that there is a bijection between the set of\ntwo dimensional convolutions and the one of one dimensional convolutions.\nAs a consequence, we deﬁne a 2-dimensional convolutional layer as a couple\n(C, w) where C is a 2-dimensional convolution and w is its associated vector of\nweights, obtained as the vector of weights of the one dimensional convolution\nassociated to C. Moreover, as in the case of one dimensional convolutions,\nthe index functions a(i, −), b(j, −), α(i, −), β(j, −) are usually assumed to be\nnon-decreasing.\nAs in the case of one dimensional convolutions, the form of the index\nfunctions are usually standard and controlled by parameters widely used by\nthe practitioners such as stride, padding, etc., and the numbers d, q are called\nthe kernel sizes. In addition, in most cases p = q (i.e. the ﬁlter or kernel\nmatrix is a square matrix).\n42\nFigure 12: A 2d convolution as depicted in [38]\nAs we mentioned at the beginning of Section 3.1 we are not only interested\non convolutions acting on Mn,m(R) ∼= Rn × Rm but also on convolutions\nacting on Rp × Rn × Rm representing an image having p channels or a\np × n × m voxel. We can then deﬁne 3-dimensional convolutions along the\nlines of what we did for 2-dimensional ones and, more generally we can\ndeﬁne n-dimensional convolutions and convolutional layers.\nAs these are\nstraightforward generalizations and as all these cases reduce to the case of\none dimensional convolutions, we will not spell out the deﬁnitions. We deﬁne\nn-dimensional pooling layers analogously.\nWe shall now deﬁne two very important types of neural networks.\nDeﬁnition C.7. We say that F ((F, w)) is a convolution (convolution layer)\nif it is a convolution operator (layer). We say that σ : R →R is an activation\nfunction if it is either a bounded, non-constant continuous function or if it\nis the RELU function. Given an activation function σ, for all n ≥1 we can\nview it as a map Rn →Rn acting componentwise (and in this case we denote\nσ×n as σ abusing terminology).\nDeﬁnition C.8. Consider a ﬁnite sequence of positive natural numbers\nN0, ..., NL16, L ∈N>0 and ﬁx an activation function σ.\nWe say that a\nfunction F : RN0 →RNL is called\n• A σ-multilayer perceptron (MLP) if it is of the form\nF = G ◦HL−1 ◦· · · ◦H1\nwhere G : RNL−1 →RNL is an aﬃne map and for all i = 1, ..., L −1,\nHi = σ ◦Ai where Ai : RNi−1 →RNi is an aﬃne map.\n16That, under the terminology of Section 2.1, are among the hyperparameters\n43\n• A σ-convolutional neural network (CNN) if it is of the form\nF = G ◦HL−1 ◦· · · ◦H1\nwhere G : RNL−1 →RNL is an aﬃne map and, for all i = 1, ..., L −1,\nHi is either a pooling function or it is of the form Hi = σ ◦Ai where\nAi : RNi−1 →RNi is aﬃne or a convolution.\nThe number L is usually called the number of layers and σ is assumed to be\napplied componentwise. The convolutions and the aﬃne maps appearing in\na MLP or in a CNN are considered as layers. We denote as N σ(Rn, Rm) the\nset of σ-MLPs having domain Rn and codomain Rm. Finally, given a MLP\nor a CNN F, we deﬁne the vector of its weights, wF, to be the concatenation\nof all the vectors of weights of its layers. When we see a MLP or a CNN\nF as a network, we usually think of it as a couple (F(w), w), and in the\nterminology of Section 3, somewhat abusing the terminology, that w is the\nmodel.\nThere exists many more types of CNNs, therefore our deﬁnition here is\nnot meant to be the most general possible. However, all of them use at least\none convolution operator as the ones the we have deﬁned before. MLPs are\nusually referred as ”fully-connected” feedforward neural networks. Indeed,\nboth MLPs and CNNs fall under the broader category of feeedforward neu-\nral networks which are, roughly speaking, networks that have an underlying\nstructure of directed graph. As there is not a general straightforward and\nconcise deﬁnition of these networks, in this work we will only consider MLPs\nand CNNs having the structure above and refer the reader to the literature,\nsee [29] for more general deﬁnitions or Section2 in [Zhu22] for a recent and\nvery clean deﬁnition from a graph theoretical viewpoint.\nSuppose we are given a network (F(w), w) which can be either a MLP or\na CNN. If w ∈Rp and F(w) : Rn →RC we can deﬁne the score function\nassociated to (F(w), w) as\ns : Rd × Rp →RC,\ns(x, w) := F(w)(x)\nThis is the score we use for training as explained in the main text.\n44\nFigure 13: A very inﬂuential Convolutional Neural Network: the famous\nLeNet introduced in [31] (the image depicts LeNet-5 as in [34]).\nD\nUniversal Approximation Theorem\nWe conclude this appendix mentioning a so called universal approximation\ntheorem for MLPs, an important theoretical result showing that the archi-\ntectures we introduced have a great explanatory power. For more details,\nsee [22] Theorems 1 and 2.\nTheorem D.1. For any given activation function σ, the set of σ-MLPs\nN σ(Rn, R) is dense in the set of continuous real valued functions C(Rn) for\nthe topology of uniform convergence on compact sets.\nProof. This is proved as Theorem 2 in [22]. We only remark that the theorem\nholds in the case σ is the RELU, see the remark at page 253 in [22], as for a\ngiven R ∋a ̸= 0 the ”spike function”\nr(x) = RELU(x −a) −2RELU(x) + RELU(x + a)\nis bounded, continuous and non-constant\nThis theorem is conceptually very important as it states that an arbitrary\ncontinuous function can be in principle approximated by a suitable MLP. As\na consequence, trying to approximate unknown functions with Neural Net-\nworks may seem less hopeless than what one might think at a ﬁrst glance.\nIn real world applications, though it is somewhat unpractical to solve any\ntask with an MLP (for many reasons such as lack of data), and for this rea-\nson other architectures such as CNNs, GNNs etc. have been designed and\ndeployed.\nThere exist more general approximation theorems concerning more general\nMLPs (e.g. for the class N σ(Rn, Rm)), or for other neural networks archi-\ntectures such as CNNs and GNNs. For these results and for a more compre-\nhensive discussion of the topic the reader is referred to [23], [29] and [26].\n45\nReferences\n[1] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and\nMasanori Koyama. Optuna: A next-generation hyperparameter opti-\nmization framework. Proceedings of the 25th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data Mining, 2019.\n[2] Babak Alipanahi, Andrew Delong, Matthew T. Weirauch, and Bren-\ndan J. Frey. Predicting the sequence speciﬁcities of dna- and rna-binding\nproteins by deep learning. Nature Biotechnology, 33:831–838, 2015.\n[3] Shun-Ichi Amari. Natural gradient works eﬃciently in learning. Neural\ncomputation, 10(2):251–276, 1998.\n[4] Frank Bauer. Normalized graph laplacians for directed graphs. arXiv:\nCombinatorics, 2011.\n[5] Cristian Bodnar, Francesco Di Giovanni, Benjamin Paul Chamber-\nlain, and Michael M. Bronstein Pietro Li`o.\nNeural sheaf diﬀusion:\nA topological perspective on heterophily and oversmoothing in gnns.\narXiv:2202.04579 [cs.LG], 2022.\n[6] Soha Boroojerdi and George Rudolph. Handwritten multi-digit recog-\nnition with machine learning. pages 1–6, 05 2022.\n[7] Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velivckovi’c.\nGeometric deep learning: Grids, groups, graphs, geodesics, and gauges.\nArXiv, abs/2104.13478, 2021.\n[8] Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur D. Szlam,\nand Pierre Vandergheynst.\nGeometric deep learning: Going beyond\neuclidean data. IEEE Signal Processing Magazine, 34:18–42, 2016.\n[9] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent per-\nforms variational inference, converges to limit cycles for deep networks.\n2018 Information Theory and Applications Workshop (ITA), pages 1–\n10, 2017.\n[10] Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast\nand accurate deep network learning by exponential linear units (elus).\narXiv: Learning, 2015.\n46\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.\nImagenet: A large-scale hierarchical image database.\nIn 2009 IEEE\nconference on computer vision and pattern recognition, pages 248–255.\nIeee, 2009.\n[12] Matthew Feickert and Benjamin Philip Nachman. A living review of\nmachine learning for particle physics. ArXiv, abs/2102.02770, 2021.\n[13] Xavier Glorot and Yoshua Bengio. Understanding the diﬃculty of train-\ning deep feedforward neural networks. In International Conference on\nArtiﬁcial Intelligence and Statistics, 2010.\n[14] Chris D. Godsil and Gordon F. Royle.\nAlgebraic graph theory.\nIn\nGraduate texts in mathematics, 2001.\n[15] Luca Grementieri and Rita Fioresi. Model-centric data manifold: the\ndata through the eyes of the model. ArXiv, abs/2104.13289, 2021.\n[16] Daniel Guest, Kyle Cranmer, and Daniel Whiteson. Deep learning and\nits application to lhc physics. Annual Review of Nuclear and Particle\nScience, 2018.\n[17] William L. Hamilton. Graph representation learning. Synthesis Lectures\non Artiﬁcial Intelligence and Machine Learning, 2020.\n[18] Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Gre-\ngory Frederick Diamos, Erich Elsen, Ryan J. Prenger, Sanjeev Satheesh,\nShubho Sengupta, Adam Coates, and A. Ng. Deep speech: Scaling up\nend-to-end speech recognition. ArXiv, abs/1412.5567, 2014.\n[19] Jakob Hansen and Thomas Gebhart. Sheaf neural networks. ArXiv,\nabs/2012.06333, 2020.\n[20] Trevor J. Hastie, Robert Tibshirani, and Jerome H. Friedman.\nThe\nelements of statistical learning: Data mining, inference, and prediction,\n2nd edition. In Springer Series in Statistics, 2005.\n[21] Geoﬀrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman\nMohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick\nNguyen, Tara N Sainath, et al. Deep neural networks for acoustic mod-\neling in speech recognition: The shared views of four research groups.\nIEEE Signal processing magazine, 29(6):82–97, 2012.\n47\n[22] Kurt Hornik. Approximation capabilities of multilayer feedforward net-\nworks. Neural Networks, 4:251–257, 1991.\n[23] Kurt Hornik, Maxwell B. Stinchcombe, and Halbert L. White. Mul-\ntilayer feedforward networks are universal approximators. Neural Net-\nworks, 2:359–366, 1989.\n[24] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic\noptimization. CoRR, abs/1412.6980, 2014.\n[25] Thomas Kipf and Max Welling.\nSemi-supervised classiﬁcation with\ngraph convolutional networks. ArXiv, abs/1609.02907, 2016.\n[26] Anastasis Kratsios and Ievgen Bilokopytov.\nNon-euclidean universal\napproximation. ArXiv, abs/2006.02341, 2020.\n[27] Alex\nKrizhevsky,\nVinod\nNair,\nand\nGeoﬀrey\nHinton.\nCifar-\n10\n(canadian\ninstitute\nfor\nadvanced\nresearch).\nURL\nhttp://www.cs.toronto.edu/kriz/cifar.html, 5, 2010.\n[28] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet clas-\nsiﬁcation with deep convolutional neural networks. In Advances in neural\ninformation processing systems, pages 1097–1105, 2012.\n[29] Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Na-\nture, 521:436–444, 2015.\n[30] Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson,\nRichard E. Howard, Wayne E. Hubbard, and Lawrence D. Jackel. Back-\npropagation applied to handwritten zip code recognition. Neural Com-\nputation, 1:541–551, 1989.\n[31] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner.\nGradient-based learning applied to document recognition. Proc. IEEE,\n86:2278–2324, 1998.\n[32] Yann\nLeCun,\nCorinna\nCortes,\nand\nCJ\nBurges.\nMnist\nhand-\nwritten\ndigit\ndatabase.\nATT\nLabs\n[Online].\nAvailable:\nhttp://yann.lecun.com/exdb/mnist, 2, 2010.\n48\n[33] Geert J. S. Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud\nArindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen\nvan der Laak, Bram van Ginneken, and Clara I. S´anchez.\nA survey\non deep learning in medical image analysis.\nMedical image analysis,\n42:60–88, 2017.\n[34] Jose Marques, Gabriel Falc˜ao Paiva Fernandes, and Lu´ıs A. Alexandre.\nDistributed learning of cnns on heterogeneous cpu/gpu architectures.\nApplied Artiﬁcial Intelligence, 32:822 – 844, 2017.\n[35] James Martens. New insights and perspectives on the natural gradient\nmethod. Journal of Machine Learning Research, 21(146):1–76, 2020.\n[36] Resve A. Saleh and A. K. Md. Ehsanes Saleh. Statistical properties of the\nlog-cosh loss function used in machine learning. ArXiv, abs/2208.04564,\n2022.\n[37] Stefan Sommer and Alex M Bronstein. Horizontal ﬂows and manifold\nstochastics in geometric deep learning. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2020.\n[38] Xing Ping Sun, Jiayuan Peng, Yong Shen, and Hongwei Kang. Tobacco\nplant detection in rgb aerial images. Agriculture, 2020.\n[39] Petar Velickovic,\nGuillem Cucurull,\nArantxa Casanova,\nAdriana\nRomero, Pietro Lio’, and Yoshua Bengio. Graph attention networks.\nArXiv, abs/1710.10903, 2017.\n[40] Izhar Wallach, Michael Dzamba, and Abraham Heifets. Atomnet: A\ndeep convolutional neural network for bioactivity prediction in structure-\nbased drug discovery. ArXiv, abs/1510.02855, 2015.\n[41] Yonghui Wu, Mike Schuster, Z. Chen, Quoc V. Le, Mohammad\nNorouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao,\nKlaus Macherey, JeﬀKlingner, Apurva Shah, Melvin Johnson, Xiaobing\nLiu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto\nKazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliﬀ\nYoung, Jason R. Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Gre-\ngory S. Corrado, MacduﬀHughes, and Jeﬀrey Dean. Google’s neural\nmachine translation system: Bridging the gap between human and ma-\nchine translation. ArXiv, abs/1609.08144, 2016.\n49\n",
  "categories": [
    "cs.LG",
    "math-ph",
    "math.MP"
  ],
  "published": "2023-05-09",
  "updated": "2023-05-09"
}