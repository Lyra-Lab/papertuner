{
  "id": "http://arxiv.org/abs/2305.05959v2",
  "title": "Survey of Code Search Based on Deep Learning",
  "authors": [
    "Yutao Xie",
    "Jiayi Lin",
    "Hande Dong",
    "Lei Zhang",
    "Zhonghai Wu"
  ],
  "abstract": "Code writing is repetitive and predictable, inspiring us to develop various\ncode intelligence techniques. This survey focuses on code search, that is, to\nretrieve code that matches a given query by effectively capturing the semantic\nsimilarity between the query and code. Deep learning, being able to extract\ncomplex semantics information, has achieved great success in this field.\nRecently, various deep learning methods, such as graph neural networks and\npretraining models, have been applied to code search with significant progress.\nDeep learning is now the leading paradigm for code search. In this survey, we\nprovide a comprehensive overview of deep learning-based code search. We review\nthe existing deep learning-based code search framework which maps query/code to\nvectors and measures their similarity. Furthermore, we propose a new taxonomy\nto illustrate the state-of-the-art deep learning-based code search in a\nthree-steps process: query semantics modeling, code semantics modeling, and\nmatching modeling which involves the deep learning model training. Finally, we\nsuggest potential avenues for future research in this promising field.",
  "text": "1\nSurvey of Code Search Based on Deep Learning\nYUTAO XIE, Peking University & International Digital Economy Academy, China\nJIAYI LIN∗, International Digital Economy Academy, China\nHANDE DONG, International Digital Economy Academy, China\nLEI ZHANG, International Digital Economy Academy, China\nZHONGHAI WU, Key Lab of High Confidence Software Technologies (MOE), Peking University, China\nCode writing is repetitive and predictable, inspiring us to develop various code intelligence techniques. This\nsurvey focuses on code search, that is, to retrieve code that matches a given natural language query by\neffectively capturing the semantic similarity between the query and code. Deep learning, being able to extract\ncomplex semantics information, has achieved great success in this field. Recently, various deep learning\nmethods, such as graph neural networks and pretraining models, have been applied to code search with\nsignificant progress. Deep learning is now the leading paradigm for code search. In this survey, we provide a\ncomprehensive overview of deep learning-based code search. We review the existing deep learning-based\ncode search framework which maps query/code to vectors and measures their similarity. Furthermore, we\npropose a new taxonomy to illustrate the state-of-the-art deep learning-based code search in a three-steps\nprocess: query semantics modeling, code semantics modeling, and matching modeling which involves the\ndeep learning model training. Finally, we suggest potential avenues for future research in this promising field.\nCCS Concepts: • General and reference →Surveys and overviews; • Software and its engineering →\nSoftware notations and tools; Software creation and management;\nAdditional Key Words and Phrases: code search, code understanding, natural language processing, deep\nlearning, pre-training\nACM Reference Format:\nYutao Xie, Jiayi Lin, Hande Dong, Lei Zhang, and Zhonghai Wu. 2023. Survey of Code Search Based on Deep\nLearning. ACM Trans. Softw. Eng. Methodol. 1, 1, Article 1 (January 2023), 44 pages. https://doi.org/10.1145/\n3628161\n1\nINTRODUCTION\nProgramming is the process of using a programming language to write code that performs a specific\nfunction. Despite being a creative endeavor, programming also exhibits repetitive and predictable\nattributes [30], with many codes already implemented to some extent. By analyzing historical\ncodes, it is possible to anticipate the content of the code to be written. Building on this insight, code\nintelligence techniques such as code search, code completion, and code generation can be employed\nto enhance the productivity of software developers. In recent years, the application of deep learning\nAuthors’ addresses: Yutao Xie, Peking University & International Digital Economy Academy, No. 5 Shihua Road, Futian\nDistrict, Shenzhen, China, 518017, yutaoxie@idea.edu.cn; Jiayi Lin, International Digital Economy Academy, No. 5 Shihua\nRoad, Futian District, Shenzhen, China, 518017, jiayilin1024@gmail.com; Hande Dong, International Digital Economy\nAcademy, No. 5 Shihua Road, Futian District, Shenzhen, China, 518017, donghd66@gmail.com; Lei Zhang, International\nDigital Economy Academy, No. 5 Shihua Road, Futian District, Shenzhen, China, 518017, leizhang@idea.edu.cn; Zhonghai\nWu, Key Lab of High Confidence Software Technologies (MOE), Peking University, No. 5 Yiheyuan Road, Haidian District,\nBeijing, China, 100871, wuzh@pku.edu.cn.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n1049-331X/2023/1-ART1 $15.00\nhttps://doi.org/10.1145/3628161\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\narXiv:2305.05959v2  [cs.SE]  13 Dec 2023\n1:2\nXie et al.\nin the field of code intelligence has led to remarkable achievements [10, 24, 47, 84, 88, 102], thanks\nto its powerful representation capability and capacity to uncover hidden patterns. At present, a\nnumber of code intelligence tools leveraging deep learning, including Copilot and ChatGPT, have\nbeen developed and commercialized. These advanced tools enable software developers to write\nefficient and accurate code with greater ease and speed.\nThis survey focuses on the nl-to-code search task. Code search is one of the three main software\nengineering tasks that has remained active over the years [89]. While code-to-code search falls\nwithin the research domain of code search, its role bears a closer affinity to code clone detection,\nthus rendering it outside the scope of this survey. The goal of nl-to-code search (henceforth referred\nto as code search) is to retrieve code fragments that match developers’ natural language queries from\na large code corpus [74]. Software development is a creative work that requires software developers\nto write code that meets product requirements. During software development, developers often\nuse search engines to search for code-related information [71], such as reusable code snippets, API\nunderstanding, and examples of specific functionality. They seek high-quality open source code\nfor reference or reuse, which enhances the productivity of development [18]. Developers typically\nuse general search engines such as Google, Bing, and Baidu to search for target codes. However,\nthese general search engines usually search in the full database and are not specifically designed\nfor code-related information. Consequently, they are inefficient on the code search task and the\nsearch results are not satisfactory.\nThe transformative impact of deep learning on code search. Early code search engines\nprimarily utilized Information Retrieval (IR) technology. They treated source code as text, and\nsearched for the target code by matching keywords in query and code [9, 29, 58, 60]. These methods\nmainly rely on the textual similarity between code and query. However, this approach has drawbacks\nsince programming languages and natural languages differ greatly from each other, making it\ndifficult for IR-based methods to comprehend the semantics. Fortunately, deep learning boasts an\nexceptional capability for extracting high-level semantic representations. As a result, compared to\nconventional IR-based approaches, code search has been improved significantly since deep learning\nincorporation [23]. Deep learning has received great attention in recent years in code search research.\nCode search is one of the first software engineering tasks to use deep learning techniques [89].\nHence, this paper primarily investigates code search methods based on deep learning. We provide\na taxonomy which groups the current series of work into three categories: various techniques are\nemployed to enhance the semantics of the query text for a more precise search intent [7, 16, 34, 51, 65–\n67, 72, 73, 82, 90, 99]; sequence and graph-based deep learning technologies are introduced to\nmodel code representation and enhance code comprehension [2, 5, 6, 8, 12, 15, 17, 20, 21, 23–\n27, 33, 41, 43–45, 48, 49, 52, 56, 61, 68, 69, 75, 78, 83, 85–87, 91, 94, 96, 98, 101]; more efficient\ntraining techniques are introduced or proposed, making the training of large models a great\nsuccess [2, 4, 5, 8, 11, 12, 17, 21, 23–27, 31, 33, 41, 43–45, 48, 49, 52, 56, 61, 63, 75, 78, 83, 85–\n87, 91, 94, 96, 98, 101].\nSignificance of this review. With numerous deep learning-based code search works published,\nespecially the advent of pre-training technology in recent years, the field of code search has entered\na new era. Despite the existence of several studies in the field of deep learning and code search,\nthere has not been a thorough and organized examination of the correlation between them. As\na result, we undertake a study of recent deep learning research in code search, develop a new\ntaxonomy and provide insights into the future research opportunities from our discoveries. This\nreview serves to fill this gap. We believe that this review will be useful for both relevant academic\nresearchers and industrial practitioners as follows:\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:3\n(1) Individuals who are new to the field of code search and wish to gain a foundational understanding\nthrough a review.\n(2) Individuals who already have some understanding of the field of code search, but lack an orga-\nnized summary and classification, and are unable to establish a coherent knowledge structure\nfor the field.\n(3) Individuals who are interested in learning about the latest advancements and state-of-the-art\nmethods in the field of deep code search.\n(4) Individuals who aim to develop code search tools in the industry.\n(5) Individuals who are currently seeking a research direction in the field of deep code search.\nDifferences from prior reviews. At present, there have been several surveys on code search,\nbut their focus is different from ours. Khalifa [40] discusses existing code search techniques, focusing\non IR-based and deep-learning-based approaches, but only covers 5 relevant papers. Liu et al. [50]\nfocus on publication trends, application scenarios, and evaluation metrics of code search. Grazia\nand Pradel [19] provide a review of the historical development of code search and covers the various\nstages of the code search process, including query formulation, query processing, indexing, and\nranking. However, the review lacks theoretical analysis and in-depth summaries, especially in\nterms of the relationship between query and code semantic matching models. Different from the\nprevious reviews, we concentrate on code search technology based on deep learning. We collect\nand organize high-quality conference/journal papers published in the past 5 years and propose a\nnew taxonomy. In this survey, we have discussed the following research questions (RQs):\n• RQ1. How to accurately capture the user’s query intent?\n• RQ2. How to enhance the semantic understanding of code?\n• RQ3. How to train a deep code search model?\n• RQ4. How to evaluate a code search model?\n• RQ5. What are some potential avenues for future research in this promising field?\nLiteratures collection process. Our aim is to equip novice researchers and non-experts with a\nquick and systematic grasp of the latest code search techniques and motivate their future research.\nConsidering that Gu et al. [23] were the pioneers in applying deep neural networks to code search\ntasks in 2018, we establish 2018 as our reference point and adhere to the guidelines proposed\nby Kitchenham and Charters [39] as well as Petersen et al [64]. This allows us to perform a\ncomprehensive examination of the relevant literature spanning the last six years (from 2018 to the\npresent). Specifically, we identified a set of search strings, namely “code search” and “code retrieval”,\nfrom the existing literature on code search that we are already known. Subsequently, capitalizing\non our emphasis on the precise technological domain of deep learning, we broadened our search\nstrings to encompass the advanced concepts of “deep code search” and “deep code retrieval”. Based\non the aforementioned four search strings, we conducted initial manual searches on Google Scholar,\nDBLP, ACM Digital Library, and Papers With Code, resulting in 693 relevant papers. After removing\nduplicate papers, we identified a total of 431 candidate papers. Our comprehensive search was\nfinalized on June 12, 2023, encompassing research published prior to this date. Subsequently, we\nmeticulously applied a set of well-defined inclusion and exclusion criteria to identify the most\nrelevant literature that aligns with our research topic:\n! Complete research papers published in top-ranked international academic conferences or\njournals. Specifically, venues ranked A* or A in the CORE ranking: http://portal.core.edu.au.\n! Significantly impactful papers from workshops, arXiv, and lower-ranked venues. Specifically,\nthese papers have received no less than 15 citations and are highly relevant to the subject of\nour investigation.\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:4\nXie et al.\n! The paper must include a discussion or evaluation of the model on the task of natural language\ncode search.\n% The preprints of accepted papers are excluded.\n% Journal papers that are extensions of conference proceedings are discarded.\n% Non-deep learning-based code search papers are ruled out.\n% Papers focusing on code-to-code search are excluded.\n% Papers that focus on utilizing code search models to assist other code intelligence tasks, such\nas code generation and code repair, are discarded.\nTable 1. Selection of relevant papers.\nProcess\nNumber of papers\nTotal papers retrieved\n693\nAfter removing duplicate papers\n431\nAfter excluding based on title, abstract, and keywords\n97\nAfter excluding based on the full text\n64\nTable 2. Top publication venues with at least two deep code search papers.\nPublication venue types\nShort Name\nFull Name\nNumber of papers\nConference\nICSE\nInternational Conference on Software Engineering\n9\nEMNLP\nConference on Empirical Methods in Natural Language Processing\n6\nSANER\nIEEE International Conference on Software Analysis, Evolution, and Reengineering\n5\nACL\nAnnual Meeting of the Association for Computational Linguistics\n4\nFSE/ESEC\nACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering\n3\nASE\nInternational Conference on Automated Software Engineering\n3\nTSE\nIEEE Transactions on Software Engineering\n3\nNeurIPS\nConference on Neural Information Processing Systems\n3\nICPC\nIEEE International Conference on Program Comprehension\n3\nWWW\nWorld Wide Web\n3\nPLDI\nACM SIGPLAN Conference on Programming Language Design and Implementation\n2\nICSME\nInternational Conference on Software Maintenance and Evolution\n2\nJournal\nTOSEM\nACM Transactions on Software Engineering and Methodology\n2\n-\nNeural Networks\n2\nTotal\n-\n-\n50\nAs presented in Table 1, through a thorough analysis of the titles, abstracts, and keywords\nof the papers, we successfully identified 97 papers. Subsequently, after carefully cross-checking\nthe full texts of the remaining papers, we ultimately obtained a collection of 64 papers on code\nsearch leveraging deep learning techniques. These papers primarily encompass international top\nconferences and journals in the domains of Software Engineering, Content Retrieval, and Artificial\nIntelligence. Table 2 presents the esteemed publication venues have published at least two deep code\nsearch papers. These venues collectively feature 50 papers, constituting 78.1% of the entire obtained\npapers. Notably, conference proceedings accounted for 92% of the published works. Among these\n14 venues, we observed that the top five most popular conferences are ICSE, EMNLP, SANER, ACL,\nand FSE/ESEC, while the top two preferred journals are TOSEM and Neural Networks. Figure 1\nshowcases a compelling upward trajectory in the annual publication quantity of relevant literature\nfollowing the integration of deep learning techniques into the field of code search. This notable\ntrend serves as a testament to the escalating attention and burgeoning interest surrounding deep\ncode search.\nOur contributions. The main contributions of this paper are as follows:\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:5\nFig. 1. Number of publications per year.\n(1) New taxonomy. We review 64 related works on deep code search published until June 12, 2023,\nand propose a new taxonomy which groups code search based on deep learning into three\ncategories: query semantics modeling, code semantics modeling, and matching modeling.\n(2) Comprehensive review. We present a comprehensive overview of deep learning techniques in\ncode search, offering in-depth descriptions of representative models, thorough comparisons,\nand summarized algorithms.\n(3) Abundant resources. We gather a vast array of code search resources, including large-scale\ntraining corpus, benchmark datasets, and evaluation metrics suitable for various scenarios, to\naid in comparing deep code search models.\n(4) Future directions. We highlight the limitations of current deep code search methods and suggest\nvarious valuable research directions, aiming to spur further exploration in this area.\nThe rest of this paper is organized as follows. Section 2 introduces the deep learning-based\ncode search framework and defines the core problem of code search. Section 3 covers methods\nfor enriching the semantics of natural language text queries. Section 4 categorizes various code\nrepresentation and vectorization techniques. Section 5 outlines the training methods and objectives\nof models. Section 6 summarizes commonly used datasets and evaluation metrics in this field.\nSection 7 highlights potential future research directions in this promising field. Section 8 provides\na summary of this survey.\n2\nDEEP LEARNING-BASED CODE SEARCH FRAMEWORK\nDeep learning has seen rapid development in the field of software engineering in recent years, with\ncode search being one of the most successful applications. Compared with traditional methods, deep\ncode search leverages the powerful ability of deep neural network to extract hidden features from\ndata, generating semantic representation of natural language and code for improved performance.\nFigure 2 illustrates the overall framework of deep learning-based code search. The framework\nconsists primarily of three components: encoding the query, encoding the code, and measuring\ntheir similarity. During training, a bimodal deep neural network is trained on a large parallel\ncorpus of code and natural language to learn how to encode query and code into high-dimensional\nvectors, q and c. Subsequently, the similarity between the query vector q and the code vector c\nin a high-dimensional vector space is calculated, such as Cosine similarity 𝑠(q, c) =\nq𝑇·c\n∥q∥·∥c∥or\nEuclidean distance 𝑠(𝒒, 𝒄) =\n√︃Í𝑛\n𝑖=1 (𝑞𝑖−𝑐𝑖)2. Among these options, Cosine similarity stands out\nas the widely preferred calculation method [23, 31, 44, 48, 49, 78, 91, 98].\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:6\nXie et al.\nFig. 2. Code search framework based on deep learning.\nTo improve code search accuracy, the model should make query representation similar to correct\ncode representation while distinct from incorrect code representation. Hence, training instances are\ntypically structured as triplets ⟨𝑞,𝑐+,𝑐−⟩, and the model is trained to minimize the triplet sorting\nloss L (𝑞,𝑐+,𝑐−) = max (0,𝛿−𝑠(𝑞,𝑐+) + 𝑠(𝑞,𝑐−)), where 𝑞denotes the natural language query\ntext, 𝑐+ denotes correct code fragment, 𝑐−is a randomly selected negative sample code fragment\nfrom the rest of the code corpus, and 𝛿denotes the margin that ensures 𝑐+ is closer to 𝑞than 𝑐−in\nthe vector space.\nTo enhance the response efficiency of the online code search service, the trained model typically\npre-calculates the vector representation of the candidate code fragments in the codebase. Upon\nreceiving a query, the code search engine computes the vector representation of the query, traverses\nthe codebase to compute the similarity between the query and each code fragment in the semantic\nspace, and finally returns the top 𝑘code fragments based on the relevance scores.\nFig. 3. Overview of techniques for deep code search.\nAs shown in Figure 3, deep learning-based code search engines mainly deal with two types of\ninput: natural language query and source code. Deep learning techniques are widely employed to\nlearn the semantic encoding of both inputs, especially the deep semantic information of source code.\nFollowing this overview, we want to categorize, analyze, and summarize 64 papers that are highly\nrelevant to date and deep code search. To accomplish this objective, we undertook an exploration\nof five research questions (RQs):\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:7\n• RQ1. How to accurately capture the user’s query intent? To achieve the desired level of search\nresult quality, it is crucial for the model to possess a good understanding of the user’s query\nintent. The goal of this RQ is to investigate how to model the representation of queries and\nenhance the semantic representation of query, thereby assisting the model in accurately\ncapturing the user’s query intent.\n• RQ2. How to enhance the semantic understanding of code? The syntax of programming lan-\nguages differs from natural language, giving rise to a semantic gap between them. This RQ\ninvestigates through multiple perspectives, encompassing the representation and vectoriza-\ntion of code, as well as diminishing the semantic gap between code and natural language\nthrough interaction.\n• RQ3. How to train a deep code search model? This RQ aims to investigate how to pretrain\ncode intelligence large models and how to fine-tune code search models.\n• RQ4. How to evaluate a code search model? The goal of this RQ is to analyze code search\ndatasets, evaluation metrics, and the selection of appropriate code search methods.\n• RQ5. What are some potential avenues for future research in this promising field? Through\nanalysis of the aforementioned RQs, we have identified several limitations and challenges.\nBuilding upon this foundation, this RQ discusses twelve valuable future research directions\nin the field of deep code search.\nIn the following sections, we will delve into a comprehensive and detailed discussion of each RQ.\n3\nQUERY REPRESENTATION AND ENHANCEMENT (RQ1)\nProcessing a user’s input query is the initial step in conducting a code search task. Accurately\ncomprehending the search intention within the query text is crucial in providing satisfying results\nto users. This section covers the query processing procedure from two perspectives: modeling the\nquery representation and enhancing the semantic representation of the query.\n3.1\nQuery Representation\nThe deep learning-based code search represents queries and code fragments as vectors in high-\ndimensional space by encoding them. The query encoder and the code encoder can use the same or\ndifferent network architectures. To derive the vector representation of the query, tokenization of the\ninput query text is required. Tokenization involves reassembling continuous word sequences into\ntoken sequences based on specific criteria. As shown in Figure 3, the developer inputs a query “get\nthe maximum value”, and the tokenizer processes it into a token sequence [“get”, “the”, “maximum”,\n“value”], which is then looked up in the dictionary to get the id list [459, 448, 6098, 767] as the\ninput for the query encoder. The query encoder 𝐸𝑞is an embedding network that converts the\ndeveloper-given query text into a 𝑑-dimensional vector representation. To train the query encoder,\nexisting deep learning-based code search methods have tried various neural network architectures,\nsuch as RNN [23], LSTM [98], GNN [49], and transformer [17]. As collecting and annotating real\n(query, code) pairs is costly, comments are often treated as queries during training.\n3.2\nMethods For Enhancing query Representation\nDevelopers often struggle to clearly articulate their target requirements when searching for code\nand prefer to use short initial queries with broad meanings, such as keywords splicing. In this case,\nthe code search engine is likely to return many irrelevant code fragments. As a result, Developers\nhave to waste time reviewing non-relevant results and adjusting queries. To address this, several\nstudies have focused on enhancing query semantics to improve code search efficiency by enabling\nthe code search engine to expand or reconstruct the query automatically. Table 3 summarizes the\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:8\nXie et al.\ndifferent approaches along based on API or class name, based on co-occurring terms, based on\ncommit versions, based on query generation, based on search logs, based on user feedback, and\nbased on multiple perspectives. We will discuss them in detail below.\nTable 3. Overview of approaches for enhancing query representation.\nYear\nWork\nBased on\nAPI or class name\nco-occurring terms\ncommit versions\nquery generation\nOthers\n2018\nNLP2API [66]\n!\n2018\nCOCABU [72]\n!\n2018\nZhang et al. [99]\n!\n2019\nRACK [67]\n!\n2019\nRahman [65]\n!\n!\n2019\nALICE [73]\n!\n2019\nNQE [51]\n!\n2019\nQESC [34]\n!\n2019\nWu and Yang [90]\n!\n2021\nSEQUER [7]\n!\n2022\nQueCos [82]\n!\n2022\nZaCQ [16]\n!\nBased on API or class name. Enhancing natural language queries with semantically related\nidentifiers has significant potential. Researchers have leveraged source code knowledge from plat-\nforms such as Stack Overflow and Github to reformulate natural language queries. The commonality\namong query reformulation methods is using similarity measures to compare the words or identi-\nfiers in the query to words or identifiers such as API and class names in source code [65–67, 72, 99].\nFor instance, based on posts on Stack Overflow, Sirres et al. [72] extend users’ free-form queries\nwith API methods or class names in code snippets marked as acceptable. Rahman and Roy [66]\npropose automatically identifying relevant or specific API classes from Stack Overflow to refor-\nmulate queries. They first use pseudo-relevance feedback and term weighting algorithm to gather\ncandidate API classes from Stack Overflow, and then apply the maximum likelihood estimation\nbetween the keywords in the query and API classes to rank them. Finally, the top-𝑘correlated\nclasses are used to reformulate the query. Zhang et al. [99] apply a continuous bag-of-words model\nto learn the vector representation of API class names in the code corpus. These vectors are then\nutilized to calculate the semantic distance between the initial query and API class names. The most\nsemantically relevant API class names are selected for query expansion.\nBased on co-occurring terms. A few approaches explore term co-occurrence relationships\nwhen reconstructing queries. These methods identify query keywords within structured entities\nlike method and field signatures and then propose their co-occurring terms as candidates for query\nreformulation. For example, Liu et al. [51] present a model called NQE that uses an encoder-decoder\narchitecture to predict co-occurring keywords with query keywords in codebase. NQE takes an\ninitial query as input to obtain the corresponding encoding, and then inputs this encoding into a\nRecurrent Neural Network (RNN) decoder as the initial state. By collecting the output at each time\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:9\nstep, NQE generates a sequence of method names. Finally, an output set of expanded keywords is\nobtained by splitting each method name in the sequence.\nBased on commit versions. The code retrieved via code search engines may require modifica-\ntion prior to use, such as for compatibility with different API versions. A few approaches take this\npotential code change into account when expanding queries, freeing developers from manually\nmodifying codes. Huang et al. [34] analyze the changes made to codes through extracted sequences\nfrom GitHub commits to determine what modifications were made and the reasons. This aids in\ninferring fine-grained changes to queries. They expand the query with relevant terms and eliminate\nirrelevant terms to minimize the negative effects of excessive query expansion. Wu and Yang [90]\nanalyze recurrent code changes within the versions history of open source projects. For example,\nif token “𝑎” in the code snippet is frequently modified to token “𝑏”, then when the target code\ncontains token “𝑎”, the token “𝑏” can be used to expand the initial query. By expanding the query in\nthis way, the search engine can retrieve the updated code snippets, avoiding developers manually\nupdating query.\nBased on query generation. Besides extracting relevant identifiers or keywords from codebases\nto expand queries, a few approaches aim to generate queries with richer semantics. Wang et al. [82]\nargue that many existing deep learning methods in the field overlook the knowledge gap between\nquery and code description. They note that queries are typically shorter than the corresponding\ncode descriptions. Moreover, using (code, description) pairs as training data may not generalize\nwell to real-world user queries. To address these issues, they propose the model QueCos. QueCos\nemploys LSTM with an attention mechanism as its architecture and uses reinforcement learning to\ncapture the key semantics of a given query. It then generates a semantically enhanced query, and\nblends the results of the initial query and the semantically enhanced query using a mix-ranking\ntechnique to prioritize relevant code snippets. To solve the problem that the ground-truth answer\ndoes not have a high rank, Eberhart and McMillan [16] propose the model ZaCQ to generate\nquestions that clarify the developers’ search intent. When given a code search query and the top-𝑘\nmost relevant results, ZaCQ identifies ambiguous aspects in the query, extracts task information\nfrom attributes such as function names and comments, and generates a targeted clarifying question\nfor unclear requirements. Eventually, it employs the feedback association algorithm to boost the\nranking of relevant results.\nOthers. Rahman [65] reformulates the query from multiple perspectives to further enrich its\nsemantics. He gathers search keywords from texts and source codes, structured entities from bug\nreports and documents, and relevant API classes from Stack Overflow Q&A posts. After that, he\nuses this information to reformulate queries and improve code search performance with appropriate\nterm weighting and context awareness. To explore the search logs from Stack Overflow, Cao et al.\n[7] provide a unique insight into query reformulation patterns based on large-scale real search logs\nfrom Stack Overflow. They build a large-scale query reconstruction corpus, incorporating both\noriginal queries and their corresponding reconstructed queries, to train the model. When given\na user query, the trained model automatically generates a list of candidate reconstructed queries\nfor selection. User feedback can also be actively included to help users refine query. Sivaraman et\nal. [73] leverage user feedback to label whether the returned samples are desired or not desired,\nand then extract the logical information of the code from these positive and negative samples to\nreconstruct the query.\n3.3\nSummary\nCurrent code search engines struggle with understanding natural language queries and only return\nrelevant code fragments if the query includes specific identifiers like class and function names.\nHowever, developers may not know the relevant identifiers. To overcome this, data from sources\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:10\nXie et al.\nsuch as Stack Overflow, GitHub, and codebases are fully mined to extract valuable information,\nsuch as code elements and term tokens, based on API, co-occurrence relations, search logs, and\ncommit information. This information is then added to natural language queries with appropriate\nterm weighting, significantly improving the representation of the query. Moreover, techniques exist\nto produce a query with a more lucid search intent by leveraging the extracted information, thereby\nenhancing search efficiency and performance. However, semantically related terms may not always\nco-occur, and simple co-occurrence frequencies may not be sufficient for selecting appropriate\nsearch keywords for query reformulation. More importantly, the success of the extended query\nmethod also relies on the quality of the extracted words. The crowdsourced knowledge from Stack\nOverflow may contain noise and if the words are not extracted properly, it can lead to excessive\nquery expansion and negatively impact search accuracy.\nSummary of answers to RQ1:\n• Information such as API or class name, co-occurring terms, commit versions, search logs\nand user feedback are often used to reformulate the query.\n• Reformulating query based on API or class name is the most popular method in the past\n6 years.\n• When reconstructing the query, it is important to avoid introducing noisy words that\ncould result in excessive query expansion.\n4\nCODE REPRESENTATION AND VECTORIZATION (RQ2)\nThe goal of a code search engine is to retrieve code fragments that match the query semantics from\nthe codebase. To close the semantic gap between programming language and natural language,\ndeepening the understanding of code semantics and fostering robust interaction between code\nand query are essential in addition to improving the accuracy of query intent. In this section, we\ndiscuss the code encoding model, examining it through the lenses of code representation and code\nvectorization. Additionally, we discuss the crucial aspect of the interaction between query and code,\nshedding light on its significance for the comprehensive semantic understanding of both elements.\n4.1\nCode Representation\n4.1.1\nPreliminaries. Source code is the original program text written by developers in a program-\nming language, consisting of instructions and statements. It is typically compiled into machine\nlanguage that a computer can understand and execute. During the compilation process from source\ncode to machine code, various intermediate representation forms are generated, such as Abstract\nSyntax Tree (AST), Data Flow Graph (DFG), Control Flow Graph (CFG), and LLVM Intermediate\nRepresentation (IR). In this process, the compiler automatically performs program analysis tech-\nniques, including lexical, syntactic, and semantic analysis, to verify the correctness of the source\ncode.\nAbstract Syntax Tree (AST) is a tree-like representation of the source code’s syntax. It consists\nof leaf nodes, non-leaf nodes and the edges between them, reflecting the source code’s rich syntax\nstructure. For example, in Figure 4(e), the assignment statement “𝑥= 0” is represented by a non-leaf\nnode “𝑎𝑠𝑠𝑖𝑔𝑛𝑚𝑒𝑛𝑡” and three leaf nodes “𝑥”, “=”, and “0” connected by directed edges. A conditional\njump statement like𝑖𝑓−𝑐𝑜𝑛𝑑𝑖𝑡𝑖𝑜𝑛−𝑒𝑙𝑠𝑒can be represented by a node with three branches. Typically,\nthe standard compiler tool 𝑡𝑟𝑒𝑒−𝑠𝑖𝑡𝑡𝑒𝑟1 is commonly used to parse source code into AST.\n1https://github.com/tree-sitter/tree-sitter\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:11\nFig. 4. Multiple Modalities of Source Code.\nData Flow Graph (DFG) represents the logical relationships of the code, the direction of data\ntransmission and the process of logical transformation. DFG abstracts the dependencies between\nvariables in the source code, where nodes represent variables and edges represent the source of each\nvariable’s value [25]. As shown in Figure 4(b), the variable 𝑥11 returned by the function has two\nsources which can either come from 𝑎10 or 𝑏8 based on the outcome of the 𝑖𝑓condition. Evidently,\nadhering to naming conventions is not universal among software developers, and this can pose\na challenge in comprehending variable semantics at times. Fortunately, DFG helps overcome the\nsemantic deviation caused by inconsistent naming.\nControl Flow Graph (CFG) is an abstract representation of a program’s structure, composed\nof basic blocks and directed edges between them. Each directed edge reflects the execution order of\ntwo basic blocks. For example, as shown in Figure 4(c), the outcome of an 𝑖𝑓statement results in\ntwo different execution paths, namely “𝑥= 𝑎” and “𝑥= 𝑏”. CFG represents the running sequence\nand logical relationship of the code, effectively reflecting the execution semantics of the program.\nIntermediate Representation (IR) is a structured and language-agnostic representation of the\nsource code that captures the essential semantics and structure of the code, while abstracting away\nlanguage-specific details. It serves as an intermediate step during compilation or interpretation\nand enables various optimizations and transformations to be performed on the code. For example,\nas shown in Figure 4(d), the assignment statement “𝑥= 0” is represented by the IR instruction\n%x = alloca i32; store i32 0, i32 %x; , that is, first allocate storage space for the temporary\nvariable “𝑥”, and then store the constant “0” in it. IR serves as the foundation for building CFG and\nDFG. The common IR is shared by multiple objects and can realize the unified representation of\ndifferent programming languages.\nProgram Transformation (PT) denotes a code snippet that performs the same task as a\ngiven program. It allows for the creation of numerous versions that fulfill identical functional\nrequirements. For example, Figure 4(f) is another representation of the functionality expressed in\nFigure 4(a).\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:12\nXie et al.\n4.1.2\nSequence. The triumph of embedding technology in NLP has spurred researchers to investi-\ngate its potential use in code analysis. Presently, several studies treat source code as a straightforward\ntext sequence and employ NLP techniques directly on code snippets.\nSource code Token Sequence (STS). Some approaches consider the tokenized code token\nsequence itself as the code representation and feed it into the model training process to learn\nthe semantics of the code [2, 5, 6, 8, 15, 17, 33, 41, 43–45, 48, 69, 83, 94, 96, 101]. For instance,\nbased on the powerful learning ability of Transformer, Feng et al. [17] build CodeBERT, the first\nlarge-scale program language pre-training model. Inspired by BERT-based NLP, they directly feed\nthe token sequence of a code fragment into a multi-layer Transformer. This allows the model to\nincorporate context information and learn to identify tokens that are critical to the code’s semantics.\nNevertheless, obtaining all the tokens of the source code directly through tokenization ignores\nsyntactic details, making it difficult to discern whether a term originates from a variable name or a\nmethod call. To this end, Du et al. [15] perform data augmentation on code fragments from three\nperspectives: code structure, variable names, and APIs. They construct structure-centric datasets,\nvariable-centric datasets, and API-centric datasets, which are respectively fed into the model\nfor training. To assist the model in learning code features that are invariant across semantically\nequivalent programs, Wang et al. [83] develop a semantic-preserving transformation for code\nsnippets. The transformation preserves the code’s semantics but changes its lexical appearance and\nsyntactic structure. Specifically, they modify the control structure, APIs, and declarations of the\ncode to enable the model to extract and learn relevant features, while preserving code semantics.\nTransformation-based approaches (such as generating semantically equivalent code fragments\nthrough variable renaming) often produce code with highly similar superficial forms, causing the\nmodel to focus on surface-level code structure rather than its semantic information. To mitigate\nthis, Li et al. [45] construct positive samples using code comments and abstract syntax tree subtrees,\nencouraging the model to capture semantic information.\nMultimodal Token Sequence (MTS). Code snippets have different information dimensions. A\ncode token sequence only captures shallow features of source code, such as method names and\ncode tokens, but ignores structural features like AST and CFG, which hold rich and well-defined\nsource code semantics.\nMultimodal learning aims to build models that can process and aggregate information from\nmultiple modalities. Recently, many studies have attempted to utilize program analysis techniques\nto capture the structural and syntactic representation of programs. These works capture multiple\ncode modalities to form complementary code representations [20, 21, 24–27, 61, 86, 87, 91]. For\ninstance, Xu et al. [91] extract AST from the method body as a structural feature and parse a\ncode fragment into a syntax tree. The nodes in the tree represent various code types, such as loop\nstructures, conditional judgment structures, method calls, and variable declarations. They traverse\nthe AST through a breadth-first traversal strategy to obtain the AST node sequence, which they\nuse for model training along with method names, API sequences, and code tokens.\nTo enhance the semantic suitability of the AST tree structure for code search, Gu et al. [20]\ntransform AST into Simplified Semantic Tree (SST). In contrast to AST, SST eliminates redundant\nnodes, enhances node labels, accentuates the semantic information of code snippets, and has\nbroader applicability across diverse programming languages. Then they obtain tree-serialized\nrepresentations from SST by sampling tree paths or traversing tree structures, which are used\nin multimodal learning to complement traditional source code token sequence representations.\nSimilarly, Niu et al. [61] enrich the input representation of source code pre-trained models with\nsimplified and linearized AST versions. To facilitate the transformer in encoding AST, Guo et al. [24]\npropose a method for lossless serialization of AST. They transform AST into a sequential structure\nthat maintains all tree information and use it as input to enhance the code representation. To\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:13\nprovide complementary information for program semantic understanding, Guo et al. [25] construct\ndata flow graphs based on AST. Given a code fragment, they use a standard compiler to generate a\nAST, identify the variable sequence through the leaf nodes of the AST, and retain the source of\neach variable value to obtain the code’s data flow information. Compared to AST, the data flow\ninformation is lighter and does not bring redundant structures, making the model more efficient.\nWang et al. [87] recognize that different views of code offer complementary semantics. They\nutilize the compiler to convert the program into multiple views, such as AST, CFG, and equivalent\nprograms. AST provides the grammatical information of the code, CFG reveals the execution\ninformation of the code, and different variants of the same program offer the functional information\nof the code. They use a depth-first traversal to convert a AST into a sequence of AST tokens, and\ntraverse a CFG along directed edges to parse it into a sequence of tokens. Finally, the model learns\ncomplementary information among multiple views under the framework of contrastive learning.\nFig. 5. Extract feature tokens from code snippet.\nFeature Token Sequence (FTS). Gu et al. [23] propose CODEnn, the first deep learning-based\nsupervised model for code search tasks. Considering that the source code is not plain text, they\nrepresent it from three aspects: method name, API call sequence, and code tokens, as shown\nin Figure 5. They encode each aspect separately and then combine them into a single vector to\nrepresent the entire code fragment. Sachdev et al. [68] assume that source code tokens contain\nenough natural language information. To represent code information at the method level, they\nconstruct a natural language document by extracting method names, method calls, enumerations,\nstring constants, and comments. These methods generally follow the convention of not extracting\nvariable names, which can vary from one software developer to another. Cheng and Kuang [12]\nemploy a comprehensive approach to represent code fragments by utilizing method name, API\nsequence, and code tokens. They effectively convert each type of feature into its respective n-gram\nvector using the embedding layer. Eventually, they intelligently concatenate the three distinct\nfeature vectors, resulting in the acquisition of the final code fragment feature matrix.\n4.1.3\nTree/Graph. Converting natural graph structures such as AST or CFG into sequences can\nresult in loss of key structural information. To address this, some approaches try to directly use\nthe graph structure as the code representation. Wan et al. [78] regard CFG as a directed graph and\nuse Gated Graph Neural Network (GGNN) to obtain the vector representation of CFG. They define\na graph as G = {V, E}, where V is a set of vertices (𝑣, ℓ𝑣), E is a set of edges \u0000𝑣𝑖, 𝑣𝑗, ℓ𝑒\n\u0001, and ℓ𝑣\nand ℓ𝑒are the labels of vertices and edges, respectively. In the code retrieval scenario, each vertex\nrepresents a node in the CFG, and each edge signifies the control flow of the code.\nThe code snippets with the same functionality may have different implementations, while the\ncode snippets with different code semantics may have similar structural features. To achieve\nsatisfactory results in code search, Zeng et al. [98] aim to find a more precise representation\nfor the source code. They address the limitations of structural feature-based code representation\nmethods by incorporating data and control dependencies. This allows semantically similar codes to\nhave similar representations. To achieve this, they analyze various types of LLVM IR instructions,\nintegrate data dependencies and control dependencies into a graph, and build a Variable-based Flow\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:14\nXie et al.\nGraph (VFG). The nodes in this graph can be variables, opcode, or tag identifier. Considering that\nexcessive information can obstruct the model from learning the fine-grained relationship between\nsource code and query, they optimize VFG to minimize noise during training. Finally, they feed the\noptimized graph into a GGNN with an attention mechanism to learn the vector representation of\nthe code.\nWith the aim of aligning the semantics of query and code, Ling et al. [49] represent query text and\nsource code as a unified graph structure. The program graph is generated from the AST of a code\nfragment. The AST is represented as a tuple ⟨N, T, X, Δ,𝜙,𝑠⟩, where N denotes a set of non-terminal\nnodes, T denotes a set of terminal nodes, X represents a set of values, Δ : N →(N ∪T)∗is a\nfunction that maps a non-terminal node to its child nodes, 𝜙: T →X is a function that maps a\nterminal node to an associated value, and 𝑠represents the root node. Specifically, the program\ngraph is composed of syntactic nodes (terminal and non-terminal nodes in AST) and grammatical\ntokens (the corresponding values of terminal nodes in the source code). This graph uses various\ntypes of edges to model syntactic and semantic relationships between nodes and tokens, including\nChild edges to link syntactic nodes in the AST, NextToken edges to connect each syntactic token\nto its subsequent in the original code, and LastLexicalUse edges to associate identifiers with their\nnearest lexical usage in the code.\n4.2\nCode Vectorization\nOnce the code representation is obtained, we can embed it in a vector space. The choice of network\narchitecture for encoding depends on the type of code representation (such as source code tokens\nor AST) and data structure (such as AST sequences or graphs). The deep neural network exhibits its\nexcellent performance in this process. In this section, the code embedding network based on deep\nlearning is divided into five categories: Word Embedding, Recurrent Neural Network (RNN), Graph\nNeural Network (GNN), Transformer, and mixed mode. These will be discussed in details below.\n4.2.1\nBased on Word Embedding. code2vec [1] is a seminal work in the field of code understanding,\nwhich realizes the representation of a code snippet as a single fixed-length code vector. These\nvectors serve as powerful tools to predict the semantic properties of the code snippet. Prior to this,\nthe traditional word embedding algorithms include Word2Vec and GloVe. Word2Vec employs a\ntwo-layer dense neural network to calculate the vector representation of words. It can be trained\nunsupervisedly on large corpora, ensuring that words with common context are closely located in\nthe vector space. FastText, a variant of Word2Vec, enhances word vectors with subword information\nand was once popular among researchers for code vectorization [6, 48, 68]. Saksham et al. [68]\nextract five types of information from each method: namely method name, method call, enumeration,\nstring constant, and comment, to obtain the token sequence 𝑐= {𝑐1, . . . ,𝑐𝑛} in the source code.\nThen they utilize FastText to calculate the vector representation of the extracted tokens sequence\nand obtain the embedding matrix𝑇∈R|𝑉𝑐|×𝑑, where |𝑉𝑐| is the size of the extracted code vocabulary,\n𝑑is the dimension of selected token embedding, and the 𝑘-th row in 𝑇is the vector representation\nof the 𝑘-th extracted word in 𝑉𝑐, corresponding to the set of embedding vectors {𝑇[𝑐1] , . . . ,𝑇[𝑐𝑛]}\nof the code. To combine a set of embedding vectors of code tokens into a single vector, they apply a\nweighted sum using TF-IDF weights. TF-IDF weights are designed to enhance the importance of\ntokens that frequently appear in code snippets, and reduce the weight of tokens that are commonly\nused across all codebases. When searching for code, they use 𝑇to calculate the embedding vector\n{𝑇[𝑞1] , . . . ,𝑇[𝑞𝑚]} of a given query 𝑞= {𝑞1, . . . ,𝑞𝑚}, take its average value to obtain the final\nquery vector 𝑒𝑞, and then use FAISS to calculate the distance between 𝑒𝑐and 𝑒𝑞, representing the\nrelevance of the code fragment to the given query. In the second year, they further optimize the\ncode vectorization process by using supervised learning to update the token matrix𝑇obtained from\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:15\nFastText and replacing the TF-IDF weight of the merged token embedding with an attention-based\nscheme [6]. The attention weight 𝑎𝑐∈R𝑑is a 𝑑-dimensional vector learned during training. For the\ncode token embedding vector {𝑇[𝑐1] , . . . ,𝑇[𝑐𝑛]}, the attention weight of each token is calculated\nas follows:\n𝛼𝑖=\nexp \u0000𝑎𝑐· 𝑇[𝑐𝑖]⊤\u0001\nÍ𝑛\n𝑖=1 exp \u0000𝑎𝑐· 𝑇[𝑐𝑖]⊤\u0001 .\n(1)\nFinally, the code’s embedding vector is obtained by weighted summation of the attention weight 𝛼𝑖:\n𝑒𝑐=\n𝑛\n∑︁\n𝑖=1\n𝛼𝑖𝑇[𝑐𝑖] .\n(2)\n4.2.2\nBased on Recurrent Neural Networks. Recurrent Neural Networks (RNNs) are widely used\nfor embedding sequences in natural language processing. Inspired by natural language processing\ntechniques, some researchers have attempted to use RNN to encode code token sequences. For\ninstance, Gu et al. [23] extract method names, API call sequences, and tokens from code snippets,\nrepresented as 𝐶= [𝑀,𝐴, Γ], where 𝑀= 𝑤1, . . . ,𝑤𝑁𝑀is a sequence of 𝑁𝑀tokens obtained by\ndividing the method name with camel case, 𝐴= 𝑎1, . . . ,𝑎𝑁𝐴is a sequence of 𝑁𝐴consecutive API\ncalls, and Γ =\n\b\n𝜏1, . . . ,𝜏𝑁Γ\n\t is a collection of tokens in code fragments. They then encode the\nmethod name sequence and the API sequence separately using an RNN with a max-pooling layer.\nSince the tokens do not have a strict order in the source code, they are encoded using a multi-layer\nperceptron. Finally, the three vectors are combined into a single vector 𝑐, representing the entire\ncode snippet:\n𝑐= 𝐿𝑆𝑇𝑀1(𝑀) + 𝐿𝑆𝑇𝑀2(𝐴) + 𝑀𝐿𝑃(Γ).\n(3)\nSun et al. [75] translate the code into a natural language description to obtain a translation, and\napply the LSTM architecture to build a translation encoder. They use word embedding to map\nwords in the translation sequence, 𝑠= 𝑤1, · · · ,𝑤𝑁𝑠, to vector representations 𝒘𝑖= 𝜓(𝑤𝑖). These\nvectors are then arranged in an embedding matrix 𝐸∈R𝑛×𝑚, where 𝑛is the size of the vocabulary,\n𝑚is the dimension of the embedding vector. The embedding matrix 𝐸= (𝜓(𝑤1) , . . . ,𝜓(𝑤𝑖))𝑇is\nrandomly initialized and learned together with the encoder during training. The translation’s vector\nrepresentation 𝑣𝑠is obtained from the embedding matrix and inputted into the translation encoder\nto obtain the final embedding vector 𝑒𝑠. The hidden state ℎ𝑠\n𝑖for the 𝑖-th word in 𝑠is calculated as\nfollows:\nℎ𝑠\n𝑖= 𝐿𝑆𝑇𝑀\u0000ℎ𝑠\n𝑖−1,𝒘𝑖\n\u0001 .\n(4)\nIn addition, they also employ an attention mechanism to alleviate the long dependency problem in\nlong text sequences. The attention weight for each word is calculated as follows:\n𝛼𝑠\n𝑖=\nexp \u0000𝑓\u0000ℎ𝑠\n𝑖\n\u0001 · 𝑢𝑠\u0001\nÍ𝑁\n𝑗=1 exp\n\u0010\n𝑓\n\u0010\nℎ𝑠\n𝑗\n\u0011\n· 𝑢𝑠\n\u0011 ,\n(5)\nwhere 𝑓(·) represents the linear layer; 𝑢𝑠represents the context vector, which is the high-level\nrepresentation of all words in 𝑠. · represents the inner product of ℎ𝑠\n𝑖and𝑢𝑠.𝑢𝑠is randomly initialized\nand jointly learned during training. The final embedding representation 𝑒𝑠of 𝑠is computed as\nfollows:\n𝑒𝑠=\n𝑁𝑠\n∑︁\n𝑗=1\n𝛼𝑠\n𝑖· ℎ𝑠\n𝑖.\n(6)\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:16\nXie et al.\n4.2.3\nBased on Graph Neural Networks. As mentioned earlier, the code can be represented as a\ngraph structure. This representation is superior to a sequence representation as it retains more\ninformation about the code. Some approaches use Graph Neural Networks (GNNs) to encode the\ngraph structure, which is constructed based on structures such as AST and CFG [49, 52, 98].\nZeng et al. [98] construct a variable-based flow graph (VFG) based on the data dependencies\nand control dependencies of the source code. VFG is a directed graph with multiple types of\nedges, represented as 𝐺= (𝑉, 𝐸). They then use a GGNN model, which is well suited for handling\ngraph-structured data, to learn the vector representation of code. Here, 𝑉represents a set of nodes\n(𝑣,𝑙𝑣), 𝐸represents a set of edges\n\u0010\n𝑣𝑖, 𝑣𝑗,𝑙(𝑣𝑖,𝑣𝑗)\n\u0011\n, and 𝑙𝑣represents the label of node 𝑣, which\nconsists of variables in the IR instruction. 𝑙(𝑣𝑖,𝑣𝑗) represents the label of the edge from 𝑣𝑖to 𝑣𝑗,\nincluding data dependency and control dependency. GGNN learns the vector representation of\n𝐺through the message passing mechanism. In each iteration, each node 𝑣𝑖receives a message\n𝑚𝑣𝑗↦→𝑣𝑗\n𝑡\n= 𝑊𝑙(𝑣𝑖,𝑣𝑗)ℎ𝑡−1\n𝑣𝑗\nfrom its neighbor 𝑣𝑗, which is determined by the type of edge between\nthem. GGNN then aggregates all messages 𝑚𝑖\n𝑡= Í\n𝑣𝑗∈𝑁𝑒𝑖𝑏𝑜𝑢𝑟(𝑣𝑖)\n\u0010\n𝑚𝑣𝑗↦→𝑣𝑖\n𝑡\n\u0011\nfrom neighbors of 𝑣𝑖, and\nupdates the embedding ℎ𝑡\n𝑣𝑖= 𝐺𝑅𝑈\u0000𝑚𝑖\n𝑡,ℎ𝑡−1\n𝑣𝑖\n\u0001 of each node 𝑣𝑖using GRU. Considering that different\nnodes contribute differently to code semantics, they use an attention mechanism to calculate the\nimportance of different nodes:\n𝛼𝑖= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑\u0000𝑓\u0000ℎ𝑣𝑖\n\u0001 · 𝑢𝑣𝑓𝑔\n\u0001 ,\n(7)\nwhere 𝑓(·) is a linear layer and 𝑢𝑣𝑓𝑔represents the context vector, which is a high-level representa-\ntion of the entire nodes in the graph, learned together during training. The final embedding of the\nentire graph is expressed as:\nℎ𝑣𝑓𝑔=\n∑︁\n𝑣𝑖∈𝑉\n\u0000𝛼𝑖ℎ𝑣𝑖\n\u0001 .\n(8)\nGiven that query text has sequence characteristics as a natural language, Zeng et al. [98] use\nLSTM to encode a query into a vector space and calculate its similarity with the code’s semantic\nvector. Different from this, Ling et al. [49] design a unified graph structure for both query and code.\nThey then use RGCN to encode text graph and code graph, which is a variant of GNN. Given a\ncode graph 𝐺𝑒= (V𝑒, E𝑒, R𝑒), to calculate the updated embedding vector e𝑖of each node 𝑒𝑖in the\ncode graph 𝐺𝑒, RGCN defines the propagation model as follows:\ne(𝑙+1)\n𝑖\n= 𝑅𝑒𝐿𝑈©­\n«\n𝑊(𝑙)\nΘ e(𝑙)\n𝑖\n+\n∑︁\n𝑟∈R𝑒\n∑︁\n𝑗∈N𝑟\n𝑖\n1\n\f\fN𝑟\n𝑖\n\f\f𝑊(𝑙)\n𝑟\ne(𝑙)\n𝑗\nª®\n¬\n,\n(9)\nwhere e(𝑙+1)\n𝑖\nrepresents the updated embedding vector of node e𝑖in the (𝑙+ 1)th layer of RGCN, R𝑒\nrepresents a set of relations (that is, the types of edge), N𝑟\n𝑖is a set of neighbors of node 𝑒𝑖under the\nedge type 𝑟∈R𝑞, and 𝑊(𝑙)\nΘ\nand 𝑊(𝑙)\n𝑟\nare the parameters that the RGCN model needs to learn. By\nencoding the graph structure of the code with the RGCN model, they obtain the node embedding\nX𝑒=\n\b\ne𝑗\n\t𝑁\n𝑗=1 ∈R(𝑁,𝑑) of the code graph. The embedding X𝑞= {q𝑖}𝑀\n𝑖=1 ∈R(𝑀,𝑑) of the query text\ngraph can be obtained similarly.\n4.2.4\nBased on Transformers. In recent years, large pre-trained models have brought significant\nimprovements to many NLP tasks. Many approaches train deep learning models on massive plain\ntext data using self-supervised objectives, with the Transformer neural network architecture being\nthe most prominent. Transformer contains multiple self-attention layers and can continuously learn\nin an end-to-end manner through gradient descent, because each of its components is differentiable.\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:17\nThe success of pre-training models in NLP has inspired researchers to create code understanding\npre-training models based on transformers, driving the growth of code intelligence.\nFeng et al. [17] propose CodeBERT, the first large-scale natural language-programming language\npre-training model for multiple programming languages. During pre-training, they use special\ntokens to splice natural language text sequences and code token sequences, which are fed into a\nmulti-layer Transformer-based CodeBERT. The model learns the semantic relationship between\nnatural language and programming language through Masked Language Modeling (MLM) and\nReplaced Token Detection (RTD) tasks, ultimately yielding a general vector for code understanding\ntasks.\nSome methods aim to provide a more comprehensive understanding of code by feeding multiple\nmodal representations of the source code into the Transformer. For instance, Guo et al. [25] combine\nthe sequence of variables extracted from the data flow graph with the sequence of code tokens, and\nfeed both into the multi-layer transformer-based GraphCodeBERT. They then use Masked Language\nModeling (MLM), Edge Prediction (EP) and Node Alignment (NA) tasks to guide the model to learn\nthe code structure and data dependencies. Additionally, Guo et al. [24] merge the serialized AST\nwith the comment text sequence and feed both into the multi-layer Transformer-based UnixCoder.\nThey then use Masked Language Modeling (MLM), Unidirectional Language Modeling (ULM),\nDeNoiSing (DNS), Multi-modal Contrastive Learning (MCL), and Cross-Modal Generation (CMG)\nto learn the syntactic information of the code and enhance the understanding of the code semantics.\nWang et al. [87] add not only the serialized AST, which reflects the grammatical information of the\ncode, but also the CFG sequence, which reflects the logical information, and the token sequence\nof code fragments that have different implementation but the same semantics. All these are fed\ninto the CODE-MVP. They then use Multi-View Contrastive Learning (MVCL), Fine-Grained Type\nInference (FGTI), and Multi-View Masked Language Modeling (MMLM) tasks to help the model\nlearn the structural information of the code.\nTo ensure accurate code search results for each query, it’s essential for the model to make the\nquery and correct code vectors as close as possible in the shared vector space, and as far away as\npossible from incorrect code vectors. To achieve this, some researchers integrate contrastive learning\ninto the Transformer network architecture and enhance the performance of code search engines by\nconstructing positive and negative samples. For instance, Huang et al. [33] form negative sample\npairs by randomly selecting the query and code within the same batch. Meanwhile, they generate\npositive sample pairs by duplicating the query, meaning they rephrase the query without altering\nits semantics. Developers often split a complete comment over several lines to improve readability.\nInspired by this, Li et al. [44] combine consecutive comment lines into a single line to make the\nmost of code snippets without comments to form positive sample pairs for contrastive learning.\nWhen multiple modalities of code are available, Wang et al. [86] combine different modalities to\ncreate positive sample pairs and use both in-batch and cross-batch sampling methods to generate\nnegative sample pairs.\nTransformer is favored by researchers for its general and powerful modeling capabilities. To\nimprove code semantic understanding, researchers have explored various pre-training tasks such\nas MLM, RTD, EP, and FGTI, etc. to guide model learning and enable the model to learn the\ngrammatical and structural information of the code.\n4.2.5\nMixed Mode. The structures used to represent different modes of the code may vary, and\nsome methods choose the proper model to deal with each mode accordingly. For instance, Wan et\nal. [78] extract AST and CFG as code representations, and propose Tree-LSTM for the tree structure\nof AST. Compared with the traditional LTSM unit, the Tree-LSTM unit contains multiple forgetting\ngates. For the directed graph structure of the CFG, they employ the Gated Graph Neural Network\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:18\nXie et al.\n(GGNN) for encoding, and the Gated Recurrent Unit (GRU) for updating the hidden state of each\nvertex. Finally, they obtain the overall embedding vector of CFG by aggregating the embeddings of\nall vertices.\n4.3\nInteraction Between Code and Query\nThe existence of a semantic gap between natural language queries and code snippets has posed a\nsignificant challenge. However, several methods have emerged to bridge this gap by effectively\nmodeling the interaction between the two, thus enhancing the understanding of their respective\nsemantics [2, 5, 12, 26, 91]. For instance, leveraging the widely adopted approach of calculating the\noverall similarity between queries and code, Haldar et al. [26] additionally introduced the concept\nof evaluating local similarity between the two components. This perspective provides valuable\ninsights into the finer-grained aspects of their correlation. Xu et al. [91] proposed an innovative\ntwo-stage attention network architecture. In the initial stage, the semantics of both the query and\nthe code are extracted. Subsequently, in the second stage, a joint attention mechanism is employed\nto facilitate the interaction between the two, enabling the capture of their semantic relevance. This\napproach presents a significant advancement in bridging the gap between natural language queries\nand code snippets. Similarly, Cheng and Kuang [12] combined neural information retrieval (IR) and\nsemantic matching to enhance the interaction between queries and code. Their approach involved\ncapturing two matching signals simultaneously. Firstly, neural IR captured keyword matching\nsignals, encompassing words, terms, and phrases, within query-code pairs. Secondly, semantic\nmatching employed a joint attention mechanism to simultaneously focus on description attention\nand code attention, thereby acquiring comprehensive semantic vector representations for both\ncomponents. Particularly, Arakelyan et al. [2] proposed a meticulous approach wherein they parse\nthe query by leveraging distinct part-of-speech roles to decompose it into concise semantic phrases.\nSpecifically, nouns and noun phrases are associated with data entities within the code, while verbs\ndepict operations or transformations performed on those entities. The interaction between the\nquery and code is facilitated through an entity discovery module and an action module. The entity\ndiscovery module receives a string referencing a data entity and aims to identify code tokens\nthat exhibit strong correlation with the given string. The output of this module is then employed\nas input for the action module, enabling prediction of the target entity object for the intended\noperational behavior. This comprehensive methodology offers valuable insights into enhancing the\nunderstanding and alignment between natural language queries and code snippets.\n4.4\nSummary\nSource code can be represented in various modes, including code token sequences, Abstract Syntax\nTrees (AST), Control Flow Graphs (CFG), Data Flow Graphs (DFG), and Program Transformation\n(PT). Code token sequences are commonly utilized for extracting textual features of the code, while\nAST and CFG are frequently employed for extracting the structural features of the code. As Table 4\ndemonstrates, the multiple modes of code are mainly fed into the network in two forms of data\nstructures: sequence and graph. The information from these different modes can complement each\nother, enabling the model to fully grasp the semantic information of the code. Based on the type\nof input data structure, an appropriate sequence embedding model or graph embedding model\nis selected to obtain the code vector representation. As depicted in Figure 5, the predominant\nchoice for code representation is source code token sequence (STS), while Transformer architecture\nhas emerged as the widely adopted code encoder of choice. Not all modalities will have equal\nimpact on the final representation of the code. By aggregating embedding vectors from different\nmodes of the source code, assigning attention weights, and taking the weighted sum, a more\nsemantically rich code vector representation can be obtained. Furthermore, it is crucial to foster a\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:19\nfine-grained interaction between the query and code, facilitating a more robust learning of their\nintricate semantics.\nTable 4. Overview of approaches for code representation and code vectorization.\nYear\nWork\nCode Representation\nCode Vectorization\nInteraction\nSTS\nMTS\nFTS\nTree/Graph\nWord Embedding\nRNN\nGNN\nTransformer\n2018\nCODEnn [23]\n!\n!\n2018\nNCS [68]\n!\n!\n2019\nUNIF [6]\n!\n!\n2019\nMMAN [78]\n!\n!\n!\n!\n2019\nCoaCor [94]\n!\n!\n2020\nCodeBERT [17]\n!\n!\n2020\nAdaCS [48]\n!\n!\n2020\nMP-CAT [26]\n!\n!\n!\n2020\n𝑇𝑟𝑎𝑛𝑆3 [85]\n!\n!\n2020\nZhao and Sun [101]\n!\n!\n2020\nCO3 [96]\n!\n!\n2021\nCRaDLe [21]\n!\n!\n2021\nGraphCodeBERT [25]\n!\n!\n!\n2021\nDGMS [49]\n!\n!\n2021\nCoCLR [33]\n!\n!\n2021\nDOBF [41]\n!\n!\n2021\nGu et al. [20]\n!\n!\n2021\nTabCS [91]\n!\n!\n!\n2021\nMuCoS [15]\n!\n!\n2021\nSynCoBERT [86]\n!\n!\n2022\nSalza et al. [69]\n!\n!\n2022\nG2SC [70]\n!\n!\n!\n2022\nCSRS [12]\n!\n!\n!\n2022\nTranCS [75]\n!\n!\n2022\nWang et al. [83]\n!\n!\n2022\n𝑁𝑆3 [2]\n!\n!\n!\n2022\nCodeRetriever [44]\n!\n!\n2022\nCDCS [8]\n!\n!\n2022\nCSSAM [5]\n!\n!\n!\n!\n!\n2022\nSCodeR [45]\n!\n!\n2022\nSPT-Code [61]\n!\n!\n2022\nLi et al. [43]\n!\n!\n2022\nCTBERT [27]\n!\n!\n2022\nCODE-MVP [87]\n!\n!\n2022\nUniXcoder [24]\n!\n!\n2023\ndeGraphCS [98]\n!\n!\n2023\nMulCS [56]\n!\n!\n2023\nGraphSearchNet [52]\n!\n!\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:20\nXie et al.\n(a) Code representation\n(b) Code vectorization\nFig. 6. Code representation and code vectorization.\nSummary of answers to RQ2:\n• An array of diverse modalities, including Abstract Syntax Trees (AST), Data Flow Graphs\n(DFG), Control Flow Graphs (CFG), Program Trees (PT), and other code representations,\ncan be leveraged to effectively augment the model’s ability to learn the intricate semantics\nof the code.\n• When feeding the code into the encoder, it is common practice to represent it as a sequence\nor a graph. Among these forms, the source code token sequence (STS) is the prevailing\nchoice in most scenarios.\n• Transformer is the most popular code encoder in the past 6 years.\n• Facilitating a fine-grained interaction between the query and code can enhance the\nmodel’s ability to grasp their semantics.\n5\nTRAINING METHOD (RQ3)\nAs previously stated, the core problem of code search based on deep learning is to employ a model\n𝑓𝜃(𝑞,𝑐) = 𝐸𝜃1(𝑞) ·𝐸𝜃2(𝑐) to estimate the relevance scores of query and code, and then sort according\nto the scores to obtain the code that matches the query. Sections 3 and 4 elaborate on the utilization\nof deep learning models to obtain representations for both the query and code. This section provides\nan overview of the existing techniques for training the model parameter 𝜃. The section is organized\nas follows: first, the pre-training technology of the Transformer-based large model relevant to code\nintelligence is introduced, encompassing three categories: sequence tasks, code structure tasks, and\nmultimodal tasks. Second, the training of the code search model is described. Finally, alternative\ntraining methods for other scenarios, such as meta-learning and data enhancement, are introduced.\n5.1\nPre-training of Code Intelligence Large Models\nEncoding query and code using Transformer encoder is an important encoding strategy in code\nsearch tasks. Pre-trained Transformers have proven their effectiveness in many fields [13, 53].\nLarge-scale pre-training enables the model to start from a better initial state, thereby facilitating\nefficient training and fine-tuning for downstream tasks. At present, there have been many works\nto introduce pre-training technology into the field of code intelligence, resulting in the design of\nvarious pre-training tasks. These tasks can be classified into three categories: sequence tasks, code\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:21\nstructure tasks, and multi-modal matching tasks. Table 5 summarizes the approaches along these\nthree dimensions. Sequence tasks treat the input as a sequence and train by using popular NLP\ntasks. Code structure tasks build upon sequences and utilize the relationships between tokens in a\ncode structure graph for training. Multi-modal matching tasks try to introduce contrastive learning\ninto pre-training, and use different modalities to construct matching positive samples.\nTable 5. Overview of pre-training tasks.\nYear\nWork\nSequence Tasks\nCode Structure Tasks\nMultimodal Matching Tasks\nMLM\nRTD\nULM\nDNS\nEP\nIP\nICP\nMMP\nMCCL\nCMG\n2020\nCodeBERT [17]\n!\n!\n2021\nGraphCodeBERT [25]\n!\n!\n2021\nSyncobert [86]\n!\n!\n!\n2022\nCODE-MVP [87]\n!\n!\n2022\nCTBERT [27]\n!\n2022\nSPT-Code [61]\n!\n!\n!\n2022\nDOBF [41]\n!\n2022\nSCodeR [45]\n!\n2022\nUniXcoder [24]\n!\n!\n!\n!\n5.1.1\nSequence Tasks. Sequences are the most straightforward and efficient way of representing\ncode and text. Data structures like AST and CFG can be transformed into sequences through\na serialization algorithm. There are many established pre-training tasks in NLP for encoding\nsequences in Transformer models, which can be directly introduced into code intelligence.\nMasked Language Model (MLM) [17, 24, 25, 86, 87]. Given a token sequence𝑋= {[𝑐𝑙𝑠],𝑥1,𝑥2, · · ·\n,𝑥𝑁, [𝑒𝑛𝑑]}, some tokens are randomly masked as [𝑚𝑎𝑠𝑘] and a new token sequence is denoted\nas 𝑋𝑚𝑎𝑠𝑘= {𝑥1,𝑥2, · · · , [𝑚𝑎𝑠𝑘], · · · ,𝑥𝑁}. 𝑋𝑚𝑎𝑠𝑘is input ted into the model, and the objective-task\ncan restore the original tokens through the representation at the output layer. The loss function\ncan be expressed as follows:\n𝐿𝑜𝑠𝑠𝑀𝐿𝑀= −\n∑︁\n𝑖\nlog𝑝\n\u0010\n𝑥𝑖| 𝑋𝑚𝑎𝑠𝑘\u0011\n.\n(10)\nMLM can be extended to the input of multiple modal splicing, that is, splicing query, code,\nand other modalities of code (such as flattened AST and CFG sequences). This results in a new\nsequence 𝑋=\nn\n[𝑐𝑙𝑠],𝑥(1)\n1 ,𝑥(1)\n2 , · · · ,𝑥(1)\n𝑁, [𝑠𝑒𝑝],𝑥(2)\n1 ,𝑥(2)\n2 , · · · ,𝑥(2)\n𝑀, [𝑒𝑛𝑑]\no\n, on which the MLM task\nis performed. In some literatures, this type of Masked Language Model that combines multiple\nmodalities is referred to as Multi-Modal Masked Language Modeling (MMLM) [86].\nReplaced Token Detection (RTD) [17]. A token sequence is given, and some of its to-\nkens are randomly replaced with others, creating a new token sequence, denoted as 𝑋𝑐𝑜𝑟𝑟𝑢𝑝𝑡=\n\b\n[𝑐𝑙𝑠],𝑥′\n1,𝑥′\n2, · · · , 𝑥′\n𝑁, [𝑒𝑛𝑑]\n\t\n. The task makes predictions at the output layer regarding whether a\ntoken in 𝑋𝑐𝑜𝑟𝑟𝑢𝑝𝑡is a replacement token. The loss function can be expressed as follows:\n𝐿𝑜𝑠𝑠𝑅𝑇𝐷= −\n∑︁\n𝑖\n\u0000𝛿(𝑖) log \u0000𝑖| 𝑋𝑐𝑜𝑟𝑟𝑢𝑝𝑡\u0001 + (1 −𝛿(𝑖))(1 −log𝑝\u0000𝑖| 𝑋𝑐𝑜𝑟𝑟𝑢𝑝𝑡\u0001\u0001\u0001 .\n(11)\nWhen the token is not replaced (that is, 𝑥′\n𝑖= 𝑥𝑖), 𝛿(𝑖) = 1, otherwise 𝛿(𝑖) = 0.\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:22\nXie et al.\nUnidirectional Language Modeling (ULM) [24]. ULM predicts the tokens in the sequence\nfrom start to end and is a widely used auxiliary task for training the Transformer decoder. In some\nmodels that train both the encoder and decoder [24], ULM is selected to train the decoder. The loss\nfunction can be expressed as follows:\n𝐿𝑜𝑠𝑠𝑈𝐿𝑀= −\n∑︁\n𝑖\nlog𝑝(𝑥𝑖| 𝑋[< 𝑖]) ,\n(12)\nwhere 𝑋[< 𝑖] represents the tokens before the 𝑖-th token.\nDeNoiSing (DNS) [24, 61]. The DeNoiSing pre-training objective uses [𝑚𝑎𝑠𝑘𝑖] to mask multiple\nspans in the input tokens sequence to obtain 𝑋𝑚𝑎𝑠𝑘, where 𝑖represents the 𝑖-th masked span. All\n𝑋𝑚𝑎𝑠𝑘are concatenated and represented as 𝑌= {𝑦1,𝑦2, · · · }. The corresponding output can be\nreconstructed through the encoder-decoder framework, and the objective function can be expressed\nas follows:\n𝐿𝑜𝑠𝑠𝐷𝑁𝑆= −\n∑︁\n𝑖\nlog\n\u0010\n𝑦𝑖| 𝑋𝑚𝑎𝑠𝑘,𝑌[< 𝑖]\n\u0011\n.\n(13)\n5.1.2\nCode Structure Tasks. The structured representation of code, such as AST and CFG, often\nholds a lot of information, such as the variable jump relationships, code execution order, etc. These\ninformation can be mined to design auxiliary tasks that aid in pre-training code intelligence models.\nEdge Prediction (EP) [25, 27, 86]. When encoding structured code representations using a\nTransformer, it’s common to flatten it into a sequence 𝑋= {[𝑐𝑙𝑠],𝑥1,𝑥2, · · · ,𝑥𝑁, [𝑒𝑛𝑑]}. After that,\nthe connection relationship of the nodes in the original graph/tree can be inherited into the tokens\nsequence. For example, if node 𝑖points to node 𝑗in the original graph, then token 𝑥𝑖and token 𝑥𝑗\nwill inherit this connection, allowing it to guide the model’s pre-training. The probability 𝑝\u0000𝑒𝑖𝑗| 𝑋\u0001\nof whether there is an edge between two tokens is measured by the similarity of representations ®𝑒𝑖\nand ®𝑒𝑗of 𝑥𝑖and 𝑥𝑗, namely < ®𝑒𝑖, ®𝑒𝑗>. The objective function of EP can be expressed as follows:\n𝐿𝑜𝑠𝑠𝐸𝑃= −\n∑︁\n𝑖𝑗\n\u0000𝛿\u0000𝑒𝑖𝑗\n\u0001 log \u0000𝑒𝑖𝑗| 𝑋\u0001 + \u00001 −𝛿\u0000𝑒𝑖𝑗\n\u0001\u0001 (1 −𝑙𝑜𝑔𝑝\u0000𝑒𝑖𝑗| 𝑋\u0001\u0001\u0001 .\n(14)\nWhen there is an edge between token 𝑥𝑖and token 𝑥𝑗, 𝛿\u0000𝑒𝑖𝑗\n\u0001 = 1, otherwise 𝛿\u0000𝑒𝑖𝑗\n\u0001 = 0. Besides\nsingle-modal structures, cross-modal structures can also be used to construct edges between tokens,\nfor example, by combining code sequence tokens with data flow tokens. There is a correspondence\nbetween them, and edges can be constructed according to the corresponding relationship.\nIdentifier Prediction (IP) [86, 87]. The tokens in the code snippet can be classified into two\ntypes: identifiers (such as variable and function names, etc.) and non-identifiers (keywords that\nindicate grammatical operations). This classification can be performed through AST-based code\nanalysis. The information helps construct auxiliary tasks to determine whether a token is an\nidentifier. The probability 𝑝(𝑖| 𝑋) that a token is an identifier can be predicted based on its\nrepresentation through a simple transformation, namely 𝜎\u0000 ®𝑤𝑇· ®𝑒𝑖\n\u0001, where 𝜎is a sigmoid function\nthat maps the score to 0-1. The objective function of IP can be expressed as follows:\n𝐿𝑜𝑠𝑠𝐼𝑃= −\n∑︁\n𝑖\n(𝛿(𝑖) log𝑝(𝑖| 𝑋) + (1 −𝛿(𝑖))(1 −log𝑝(𝑖| 𝑋))).\n(15)\nIf the node is an identifier, 𝛿(𝑖) = 1; otherwise 𝛿(𝑖) = 0.\nIdentifier Content Prediction (ICP) [41, 61]. Identifiers such as function names and variable\nnames carry significant information that can be considered as a general expression of functions.\nTo allow the model to learn this high-level semantic information, the following proxy tasks can\nbe designed: masking the identifier as [𝑚𝑎𝑠𝑘], and predicting the masked content. Compared to\nrandomly masking tokens, accurately masking identifiers by analyzing the code structure increases\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:23\nthe difficulty of the task and makes the model learn higher-level information. When masking\nidentifiers, to avoid the model dependence on the same variable name appearing in the context for\ninference, all the same tokens can be replaced by [𝑚𝑎𝑠𝑘𝑖] at the same time, where 𝑖represents the\n𝑖-th replaced identifier. When actually designing the training task, [𝑚𝑎𝑠𝑘𝑖] can be connected to the\nmasked token. All masked information is concatenated as output and then the model is trained\nusing the encoder-decoder framework. The corresponding objective function can be expressed as\nfollows:\n𝐿𝑜𝑠𝑠𝐼𝐶𝑃= −\n∑︁\n𝑖\nlog𝑝\n\u0010\n𝑦𝑖| 𝑋𝑚𝑎𝑠𝑘,𝑦[< 𝑖]\n\u0011\n.\n(16)\n5.1.3\nMultimodal Matching Tasks. From a multi-modal viewpoint, code intelligence is a multi-\nmodal task involving natural language, code sequence representation, and code structure represen-\ntation. At present, many studies are also examining this problem from a multi-modal perspective\nand designing pre-training tasks for multi-modal matching.\nMulti-Modal Matching Prediction (MMP) [61]. The multiple modalities of code can be seen\nas different ways of describing information with the same semantics. MMP splices the descriptions\nof these different modalities to obtain 𝑀(1)//𝑀(2)//· · · //𝑀(𝑁), where 𝑀(𝑖) represents the token\nsequence composed of modality 𝑖. The number of modalities can be greater than or equal to 2, but\nattention should be paid to controlling the total amount, as sequences that are too long can result in\na high computational complexity for the Transformer. Usually, 𝑁is set to 2 or 3. Assuming 𝑁= 2,\nthe spliced sequence is denoted as 𝑀(1)\n𝑖\n//𝑀(2)\n𝑗\n, where 𝑀(𝑗)\n𝑖\nrepresents the 𝑗-th modal sequence\nrepresentation of code 𝑖. The training objective is to determine if the two modalities in the spliced\nsequence describe the same piece of code information. If they do, the label is 𝛿(𝑖𝑗) = 1, otherwise it\nis 𝛿(𝑖𝑗) = 0. The spliced sequence can be input ted to the model, and the probability of 𝑖= 𝑗can be\nestimated by the representation of [𝑐𝑙𝑠]. The loss function can be expressed as follows:\n𝐿𝑜𝑠𝑠𝑀𝑀𝑃= −\n∑︁\n𝑖𝑗\n\u0010\n𝛿(𝑖𝑗) log𝑝\n\u0010\n𝑐𝑙𝑠| 𝑀(1)\n𝑖\n//𝑀(2)\n𝑗\n\u0011\n+ (1 −𝛿(𝑖𝑗))\n\u0010\n1 −log𝑝\n\u0010\n𝑐𝑙𝑠| 𝑀(1)\n𝑖\n//𝑀(2)\n𝑗\n\u0011\u0011\u0011\n.\n(17)\nMulti-modal Concatenation Contrastive Learning (MCCL) [45]. In multi-modal contrastive\nlearning, the model encodes information from two modalities and calculates similarity in the\nrepresentation space, namely 𝑠𝑖𝑗=< 𝐸\n\u0010\n𝑀(1)\n𝑖\n\u0011\n, 𝐸\n\u0010\n𝑀(2)\n𝑗\n\u0011\n>. However, this approach doesn’t model\nthe fine-grained interaction of 𝑀(𝑖)\n𝑖\nand 𝑀(𝑗)\n𝑖\nat the token level. To address this limitation, the\ntoken sequences of the two modalities can be spliced and encoded together with a single encoder,\nnamely 𝑠𝑖𝑗= 𝐸\n\u0010\n𝑀(1)\n𝑖\n//𝑀(2)\n𝑗\n\u0011\n. Based on the similarity, the loss function is constructed as follows:\n𝐿𝑜𝑠𝑠𝑀𝐶𝐶𝐿= −\n∑︁\n𝑖\nlog\nexp (𝑠𝑖𝑖/𝜏)\nÍ\n𝑗exp \u0000𝑠𝑖𝑗/𝜏\u0001 .\n(18)\nAfter training with this objective, researchers can continue to use 𝑠𝑖𝑗to weight the samples effec-\ntively in multi-modal contrastive learning, thereby enhancing its performance.\nCross Modal Generation (CMG) [24]. CMG aims to predict the information of another modal\nsequence through the information of one modal sequence. For example, the comment of the code\ncan be predicted based on the function body sequence. If the source modality is denoted as 𝑋and\nthe target modality is denoted as 𝑌, the objective function can be expressed as follows:\n𝐿𝑜𝑠𝑠𝐶𝑀𝐺= −\n∑︁\n𝑖\nlog𝑝(𝑦𝑖| 𝑋,𝑌[< 𝑖]) .\n(19)\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:24\nXie et al.\nCMG can be regarded as a generation task that utilizes multi-modal matching information, similar\nto an implicit matching task.\n5.1.4\nSummary. This section introduces ten pre-training tasks for code intelligence, covering code\nsequence, code structure, and code multimodality. These tasks allow for training large models\nusing extensive amounts of unlabeled data, leading to pre-trained models that can excel in various\ndownstream tasks, such as code search, with minimal fine-tuning efforts. In practical applications,\nsimple training tasks like MLM are often highly effective and play a fundamental role. Their\nsimplicity and effectiveness make them a staple in most pre-training models, such as CodeBERT\n[17] and GraphCodeBERT [25]. Complex tasks show limited improvement in the presence of simple\ntraining tasks as seen in some ablation experiments [24], and they do not generalize well to all data\nand tasks [86]. Despite this, the variety of tasks also expands the possibilities for pre-training code\nintelligence, increasing its potential. Furthermore, for code search, pre-training with multimodal\ncontrastive learning has produced better results due to its greater alignment with the downstream\ntask [44]. This confirms that pre-training tasks perform better when they are consistent with the\ndownstream tasks [100].\n5.2\nCode Search Model Training/Fine-tuning\nThe pre-training tasks for training large models for code understanding are described above. Pre-\ntraining helps the parameters of the model reach a better state, which benefits the training of\ndownstream tasks. This process is commonly referred to as fine-tuning. It is important to note that\npre-training goals may not align with the target task, but training/fine-tuning should be focused on\nthe target task and evaluated using the target task’s evaluation method. Models that do not require\npre-training, like Graph Neural Networks, also require design goals for optimization. For simplicity\nand consistency with models without pre-training, we refer to fine-tuning as training below.\nDuring training, the model is guided by task-specific labels, known as supervision information.\nFor code search, supervision information consists of paired (query, code) samples. Based on different\ntraining scenarios, we categorize existing code search training into discriminative model training\nand generative model training. Table 6 summarizes the approaches along these two dimensions.\nThe discriminative model is the most commonly used in code search and models the similarity\n𝑠𝑞𝑐= 𝑓𝜃(𝑞,𝑐). The generative model in code search mainly involves a Variational Auto-Encoder,\nencoding the input query/code into a distribution, and then decoding the distribution back to the\noriginal query/code. By designing a suitable training strategy for the encoder-decoder framework,\nthe trained Variational Auto-Encoder model has a good generalization ability for its distribution in\nthe latent space, and similarity can be calculated based on the matching of the mean vector in the\nlatent space distribution.\n5.2.1\nDiscriminative Model Training. There are three types: point-level, pair-level, and sequence-\nlevel, which will be introduced separately.\nPoint-wise [2, 33]. In this task, paired (query, code) sample pairs are labeled as 1, while unpaired\n(query, code) sample pairs are labeled as 0. Therefore, the training data can be viewed as a set\nof (query, code, label) triplets, with labels being either 1 or 0. This task is a binary classification\nproblem, and the model can be optimized using a binary classification objective such as Mean\nSquared Error loss:\n𝐿𝑜𝑠𝑠𝑀𝑆𝐸=\n1\n|𝐷|\n∑︁\n(𝑞,𝑐)∈𝐷\n\f\fˆ𝑦𝑞𝑐−𝑦𝑞𝑐\n\f\f2 ,\n(20)\nwhere 𝑦𝑞𝑐represents the true label of the sample pair (𝑞,𝑐), ˆ𝑦𝑞𝑐= 𝜎(𝑓𝜃(𝑞,𝑐)) represents the\nsimilarity score output by the model, 𝜎is the sigmoid function to ensure that the value of ˆ𝑦𝑞𝑐is\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:25\nbetween 0 and 1, and 𝐷represents the sample set. During optimization, to maintain a balance\nbetween positive and negative samples, it is necessary to sample an equal number of negative\nsamples.\nTable 6. Overview of code search model training/fine-tuning.\nYear\nWork\nDiscriminative Model Training\nGenerative Model Training\nOther\nPoint-wise\nPair-wise\nList-wise\n2018\nCODEnn [23]\n!\n2018\nChen and Zhou [11]\n!\n2019\nMMAN [78]\n!\n2019\nCoaCor [94]\n!\n2020\nAdaCS [48]\n!\n2020\nMP-CAT [26]\n!\n2020\n𝑇𝑟𝑎𝑛𝑆3 [85]\n!\n2020\nZhao and Sun [101]\n!\n2020\nCO3 [96]\n!\n2021\nDGMS [49]\n!\n2021\nCoCLR [33]\n!\n2021\nTabCS [91]\n!\n2021\nCRaDLe [21]\n!\n2021\nCorder [4]\n!\n2022\nTranCS [75]\n!\n2022\nCSRS [12]\n!\n2022\nG2SC [70]\n!\n2022\nLi et al. [43]\n!\n2022\nCodeRetriever [44]\n!\n2022\nCDCS [8]\n!\n2022\n𝑁𝑆3 [2]\n!\n2022\nWang et al. [83]\n!\n2023\ndeGraphCS [98]\n!\n2023\nKeyDAC [63]\n!\n2023\nCSSAM [5]\n!\n2023\nMulCS [56]\n!\n2023\nGraphSearchNet [52]\n!\n2023\nTOSS [31]\n!\nPair-wise [4, 5, 21, 23, 26, 48, 49, 56, 75, 78, 91, 94, 98, 101]. For the paired (query, code) positive\nsample pair, the negative sample code can be randomly selected to construct a triplet (𝑞,𝑐+,𝑐−)\ncalled (query, positive sample code, negative sample code). The objective of training is to maximize\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:26\nXie et al.\nthe gap between positive sample (𝑞,𝑐+) and negative sample (𝑞,𝑐−). The loss function commonly\nused for this purpose is the Hinge loss, which can be represented as follows:\n𝐿𝑜𝑠𝑠ℎ𝑖𝑛𝑔𝑒=\n∑︁\n(𝑞,𝑐+,𝑐−)∈𝐷\nmax \u00000,𝜖−𝑠𝑞𝑐+ + 𝑠𝑞𝑐−\u0001 ,\n(21)\nwhere 𝑠𝑞𝑐= 𝑓𝜃(𝑞,𝑐) represents the similarity between query 𝑞and code 𝑐, 𝜖is a hyperparameter,\nand 𝐷represents the sample set. For paired (𝑞,𝑐) samples, negative samples 𝑐−are randomly\nselected to construct triplet sample (𝑞,𝑐+,𝑐−).\nList-wise [12, 31, 43, 44, 52, 85, 96]. For paired (query, code) positive sample pairs, several\nnegative sample codes can be randomly selected to construct the sequence \u0000𝑞,𝑐+,𝑐−\n1 , · · · ,𝑐−\n𝑛−1\n\u0001.\nThe training goal is to optimize the similarity ranking of positive sample 𝑐+ in the entire sequence\n\u0000𝑐+,𝑐−\n1 , · · · ,𝑐−\n𝑛−1\n\u0001 for the query 𝑞. The InfoNCE loss function can be utilized as the related loss\nfunction, viewing the problem as an 𝑛classification task with the number of categories equal\nto the number of positive sample categories. After passing the similarity through softmax, the\ncross-entropy is used to construct the loss. The loss function can be expressed as:\n𝐿𝑜𝑠𝑠𝐼𝑛𝑓𝑜𝑁𝐶𝐸= −\n∑︁\n(𝑞,𝑐+,𝑐−\n1 ,··· ,𝑐−\n𝑛−1)∈𝐷\nlog\n©­­\n«\nexp \u0000𝑠𝑞𝑐+/𝜏\u0001\nexp \u0000𝑠𝑞𝑐+/𝜏\u0001 + Í\n𝑗exp\n\u0010\n𝑠𝑞𝑐−\n𝑗/𝜏\n\u0011\nª®®\n¬\n,\n(22)\nwhere 𝜏is the temperature hyperparameter. In practice, the batch negative sampling strategy is\ncommonly used to generate negative samples, meaning that for a batch of positive sample pairs,\nother codes within the batch are considered as negative samples. For instance, [44] employs various\nstrategies to sample negative samples to improve model performance, while [43] improves the\nrepresentation space data and generates more positive and negative samples.\nThe above three optimization objectives are all designed for discriminative models in supervised\nlearning. Among them, the sequence-level optimization objective is currently the most popular due\nto its effectiveness.\n5.2.2\nGenerative Model Training. The Variational Auto-Encoder (VAE) map the input ®𝑥to a distri-\nbution in the hidden space through the function 𝑓𝜃(·) (assumed to be a Gaussian distribution, which\ncan be represented by a mean and variance vector), and then the vector obtained from sampling\nthe distribution is then mapped to the input space by the function 𝑔𝜙(·), hoping to reconstruct ®𝑥. A\nregularization term is added to make the latent space distribution closer to a standard Gaussian\ndistribution. The loss function of VAE is:\n𝐿𝑜𝑠𝑠𝑉𝐴𝐸= R(®𝑥;𝜃,𝜙) + 𝐾𝐿(𝑞𝜃(®𝑧| ®𝑥), N (0, 1)) ,\n(23)\nwhere R(®𝑥;𝜃,𝜙) denotes the error loss for reconstructing ®𝑥through the encoder-decoder framework,\nand 𝐾𝐿(𝑞𝜃(®𝑧| ®𝑥), N (0, 1)) denotes the KL divergence of 𝑞𝜃(®𝑧| ®𝑥) over N (0, 1).\nThe bimodal VAE, unlike general VAE, addresses the two modalities in code search and focuses\non learning the matching relationships. The bimodal VAE has two improvements over the general\nVAE to make it more suitable for code search tasks [11], as shown in Figure 7. (a) Bimodal encoder-\ndecoder framework: this framework inputs both the query and the code into the respective encoder-\ndecoder models (the parameters of each model can be the same), and they operate in parallel.\n(b) Cross-modal regularization term: the regularization involves minimizing the difference in\ndistribution between two modes of a sample pair instead of using the standard normal distribution\nregularization term. This unifies the query and code representations in the latent space, making the\nsubsequent similarity matching more rational. The regularization term can be the KL divergence\nbetween the two modes or the KL divergence between one mode and the mean of both modes.\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:27\nFig. 7. Bimodal Variational Auto-Encoder.\nHere, we use the KL divergence from query to code, expressed as 𝐾𝐿\n\u0010\n𝑞(1)\n𝜃\n\u0000®𝑧𝑞| ®𝑥𝑞\n\u0001 ,𝑞(2)\n𝜃\n(®𝑧𝒄| ®𝑥𝒄)\n\u0011\n,\nwhere 𝑞(1)\n𝜃\n\u0000®𝑧𝑞| ®𝑥𝑞\n\u0001 and 𝑞(2)\n𝜃\n(®𝑧𝒄| ®𝑥𝒄) represent the normal distribution consisting of the mean and\nvariance of the query and code. The KL divergence from code to query can be obtained by reversing\nthe above formula. The average distribution of queries and codes can be obtained by averaging their\nmean and variance, and the mean can also be used as a regularization term in the KL divergence.\n5.2.3\nOther Training Methods. Besides the pre-training and training techniques introduced above,\nthere are also works exploring model training techniques from other perspectives. For example,\nmeta-learning in [8] uses a small verification set to improve model initialization for better per-\nformance with small data. [83] proposes data enhancement that preserves semantics and uses\ncurriculum learning to control sample weight for better model convergence. In zero-shot settings,\nwith no label training process, pre-training is crucial for developing a representation ability. Hence,\nthe input is transformed into the representation space, where the resulting vectors carry semantic\ninformation. Decisions are made based on the similarity evaluation within this space [24].\nSummary of answers to RQ3:\n• Over the past six years, the Masked Language Model (MLM) has emerged as the prevailing\nchoice for pre-training tasks in code understanding. Despite its apparent simplicity, this\ntask has consistently demonstrated remarkable effectiveness and serves as a fundamental\ncornerstone in achieving comprehensive code comprehension.\n• The training/fine-tuning of code search models predominantly adopt a discriminative\nmodel training approach, with pair-wise training being the most prevalent form.\n6\nDATASETS AND EVALUATION (RQ4)\nDatasets have had a significant impact on the evolution of code search technology. They not\nonly serve as a mean to assess the performance of various models, but also play a crucial role in\naddressing real-world challenges faced in code search, thus driving the growth of this field. In this\nsection, we present an overview of 12 code search datasets that have been proposed since 2018\nand 8 commonly used metrics for evaluating code search models. To provide some guidelines for\nindustrial practitioners, we further discuss how to choose proper code search approaches to fit\ntheir needs.\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:28\nXie et al.\n6.1\nCode Search Datasets\nTo drive the advancement of code search, various datasets comprising of (text, code) pairs have been\nintroduced. This section summarizes the existing datasets on natural language code search research,\nincluding release year, data sources, programming languages, data statistics, dataset division, data\nannotation, and acquisition methods, as displayed in Table 7. In the subsequent section, we give\na concise overview of some of the classic corpora outlined in Table 7. Furthermore, We conduct\nmeticulous statistical analysis on data sources and programming languages. As depicted in Figure\n8, GitHub emerges as the primary source of code search task datasets, with Python and Java being\nthe predominant languages of interest for this task.\nTable 7. Overview of existing datasets on code search.\nRelease\nyear\nCorpus\nData\nsources\nProgramming\nlanguages\nData\nstatistics\nData\nsplits\nData\nannotation\nAcquisition\nmethods\n2018\nStaQC [95]\nSO\nPython,\nSQL\n147,546 Python (question,code) pairs;\n119,519 SQL (question,code) pairs\ntraining set,\ndev set,\ntest set\nmanual,\nautomatic\nhttps://github.com/\nLittleYUYU/StackOverflow-\nQuestion-Code-Dataset\n2018\nCoNaLa [97]\nSO\nPython,\nJava\n42 Python questions, 736 code blocks;\n100 Java questions, 434 code blocks\ntraining set,\ndev set,\ntest set\nmanual,\nautomatic\nhttps://conala-corpus.github.io/\n2018\nGu et al. [23]\nGitHub\nJava\n18,233,872 Java methods with docstring\ntraining set\nautomatic\nhttps://github.com/\nguxd/deep-code-search\n2018\nJava-50 [23]\nSO,\nGitHub\nJava\n9,950 Java projects;\n16,262,602 Java methods;\n50 queries\ncodebase,\nevaluation set\nmanual\nhttps://github.com/\nguxd/deep-code-search\n2019\nFB-Java [42]\nSO,\nGitHub\nJava\n24,549 repositories;\n4,716,814 methods;\n287 (question,answer) pairs\ncodebase,\nevaluation set\nmanual\nhttps://github.com/\nfacebookresearch/Neural-Code\n-Search-Evaluation-Dataset\n2019\nCSN [35]\nGitHub\nPython,\nJava, Ruby,\nGo, PHP,\nJavaScript\n2,326,976 (documentation,function) pairs;\n4,125,470 functions without documentation\ntraining set,\ndev set,\ntest set,\ncodebase\nautomatic\nhttps://github.com\n/github/CodeSearchNet\n2019\nCSN-99 [35]\nBing,\nGitHub\nPython,\nJava, Ruby,\nGo, PHP,\nJavaScript\n99 queries;\n4,026 (query,code) pairs\nevaluation set\nmanual\nhttps://github.com\n/github/CodeSearchNet\n2020\nSO-DS [28]\nSO,\nGitHub\nPython\n1,113 queries;\n12,137 code snippets\ntraining set,\ndev set,\ntest set\nautomatic\nhttps://github.com/\nnokia/codesearch\n2020\nCosBench [92]\nSO,\nGitHub\nJava\n1,000 Java projects;\n475,783 Java files;\n4,199,769 code snippets;\n52 queries\ncodebase,\nevaluation set\nmanual\nhttps://github.com/\nBASE-LAB-SJTU/CosBench\n2020\nWebQueryTest [54]\nBing,\nGitHub\nPython\n1,046 (web query, code) pairs\ntest set\nmanual\nhttps://github.com/microsoft/\nCodeXGLUE/tree/main/Text-Code/\nNL-code-search-WebQuery\n2020\nAdvTest [54]\nGitHub\nPython\n280,634 (documentation, function) pairs\ntraining set,\ndev set,\ntest set\nautomatic\nhttps://github.com/microsoft/\nCodeXGLUE/tree/main/Text-Code/\nNL-code-search-Adv\n2021\nCoSQA [33]\nBing,\nGitHub\nPython\n20,604 (web query, code) pairs\ntraining set,\ndev set,\ntest set\nmanual\nhttps://github.com/\nJun-jie-Huang/CoCLR\nStaQC (Stack Overflow Question-Code pairs) [95] is a dataset of (question,code) pairs that\nhas been automatically extracted from Stack Overflow (SO), which makes it ideal for predicting\nwhether a code snippet can answer a particular question. Stack Overflow is a well-known website\nwhere developers can ask programming-related questions, such as “how to read a file in Python”.\nThere are various user-generated solutions available on the site, and the “accepted” label is used\nto indicate the quality of these solutions. To construct the StaQC dataset, Yao et al. [95] filtered\nposts in the Python and SQL domains on Stack Overflow using tags, and used a binary classifier to\nselect posts that had \"how-to\" type questions. They then combined manual labeling and automatic\nextraction methods to select questions and independent, “accepted” solutions from these posts to\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:29\ncreate (question, code) pairs. As a result, they obtained 147,546 Python (question, code) pairs and\n119,519 SQL (question, code) pairs.\nHowever, collecting questions from Stack Overflow can be a tedious and time-consuming process,\nwhich limits the quality and quantity of the collected corpus and may pose challenges for systematic\ncomparison of various models. With the exponential growth of open-source software projects on\nGitHub, it has become the main source for obtaining code corpus.\nCSN (CodeSearchNet) [35] is a code search dataset that has been constructed using the open-\nsource projects on GitHub. This corpus encompasses 6 different programming languages, including\nPython, Java, Go, PHP, JavaScript, and Ruby. In order to minimize the manual labeling effort,\nHusain et al. [35] replaced natural language queries with documentations, forming pairs of (docu-\nmentation, function) along with code snippets. To ensure the corpus’s quality, multiple filtering\nrules were established. This involved eliminating (documentation, function) pairs with fewer than\n3 documentation tokens, functions containing less than 3 lines of code, functions with “test” in\ntheir names, and duplicated functions. As a result, the CSN corpus comprises a total of 6 million\nsamples, including 2 million (documentation, function) pairs and 4 million functions without paired\ndocuments. The arrival of the CSN corpus presents exciting opportunities for training large models\nof code intelligence.\nAdvTest [54] is designed for evaluating the generalization capability of code search models. It is\nconstructed using the CodeSearchNet’s Python corpus. To guard against over-fitting, Lu et al. [54]\nreplaced the function and variable names in the test set with special tokens (e.g., “Func” in place of\na function name). This helps prevent the model from over-relying on keyword matching during\ntraining and decrease the rate of keyword coincidence between query and code at the token level.\nCoSQA [33] is a unique code search dataset that is more representative of real-world code search\nscenarios compared to other datasets. Unlike other datasets that utilize documents, docstrings, or\ncomments as queries, CoSQA is based on real user queries collected from Microsoft’s Bing search\nengine logs. To construct the CoSQA dataset, Huang et al. [33] filtered queries that did not contain\nthe keyword “Python” or showed no code search intent. To build (query, code) pairs, Huang et al.\n[33] utilized CodeBERT to pre-select high-confidence functions from the CodeSearchNet Python\ncorpus for each query. Subsequently, the annotators were tasked with determining whether the\nquery was a match for the selected function. Finally, they obtained 20,604 (web query, code) pairs\nthat could assist the model in learning the semantic relationship between queries and codes in\nreal-world scenarios.\nTable 8. Manually labeled datasets.\nRelease\nyear\nCorpus\nnumber of languages\nnumber of participants\ntype of participant\ntype of query\ntype of codebase\n2018\nStaQC [95]\n2\n4\nundergraduate student\nquestion in SO post\nanswer in SO post\n2018\nCoNaLa [97]\n2\n5\nresearcher and programmer\nquestion in SO post\nanswer in SO post\n2018\nJava-50 [23]\n1\n2\nprogrammer\nquestion in SO post\ncode in GitHub\n2019\nFB-Java [42]\n1\nunknown\nunknown\nquestion in SO post\nanswer in SO post\n2019\nCSN-99 [35]\n6\nunknown\nprogrammer\nquery from Bing\ncode in GitHub\n2020\nCosBench [92]\n1\nunknown\nunknown\nquestion in SO post\nanswer in SO post and code in GitHub\n2020\nWebQueryTest [54]\n1\n13\nprogrammer\nquery from Bing\ncode in GitHub\n2021\nCoSQA [33]\n1\nmore than 100\nprogrammer\nquery from Bing\ncode in GitHub\nWe present a detailed analysis of the manually annotated datasets in Table 8. Our findings\nreveal that within these datasets, the queries predominantly originate from questions in Stack\nOverflow (SO) posts, closely followed by real queries entered by users in the Bing search engine. In\ncomparison to source code comments, both of these queries bear a closer resemblance to queries\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:30\nXie et al.\nencountered in real search scenarios. Notably, the primary source of code in the codebase is the\nopen-source code repository, Github. Moreover, code embodies knowledge within the professional\nrealm, thereby necessitating the involvement of programmers in the annotation process. This factor\nsignificantly escalates the costs associated with code annotation.\n(a) Data source\n(b) Programming language\nFig. 8. Data source and programming language of the datasets.\nBased on the analysis above, there are three main challenges in the current code search datasets:\n(1) Inconsistency between training data and real user queries. The existing datasets primarily\nconsist of (question, code) pairs, (document, code) pairs, and (comment, code) pairs. However,\nthere is a discrepancy between the query text input by a user in the search engine and the text\nfound in a question on Stack Overflow or a document/comment in the source code, leading to a\npoor performance of the trained model in real-world scenarios. (2) Scarcity of high-quality labeled\ndata. Due to the high cost of code labeling, the existing datasets are mainly manually labeled in\nthe evaluation set, and there is a shortage of a large number of manually labeled training sets,\nrestricting the training of supervised learning models. (3) Limited data volume. The number of\ntraining data in existing datasets is limited and currently only reaches a few million, which is\ninsufficient for training large-scale code understanding pre-training models. Although Markovtsev\nand Long [57] selected 182,014 repositories from Github to create the first large-scale public code\ndataset for large-scale programming analysis, it is currently not accessible. In the future, obtaining\na large-scale code corpus from Google BigQuery, which collects active data on GitHub including\ncomplete snapshots of over one million open source repositories and hundreds of millions of code\nsubmissions, may be explored.\n6.2\nEvaluation Metrics\nCode search evaluation datasets typically include carefully selected queries and annotated functions.\nThe purpose of evaluating a code search model is to assess its ability to accurately retrieve relevant\ncode snippets from the codebase in response to a given natural language query. The evaluation of\ncode search models is largely conducted through automatic methods. Common metrics used in\nrecent studies for evaluating code search models include Precision, Recall, F1-score, MAP, MRR,\nFrank, and SuccessRate.\nAssume that a given set of queries to be executed, denoted as 𝑄= [𝑞1,𝑞2, . . . ,𝑞𝑛], and each query\nis marked with its corresponding set of ground-truth answers 𝐺. As depicted in Figure 9, 𝑡𝑜𝑝@𝑘\nrepresents the top-𝑘result sets returned for a particular query, and ℎ𝑖𝑡@𝑘refers to the answer set\namong the top-𝑘results that correctly belongs to the ground-truth answer.\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:31\nFig. 9. Schematic diagram of 𝑡𝑜𝑝@𝑘and ℎ𝑖𝑡@𝑘.\nPrecision@k is a metric that shows the relationship between the results of a query and the\nground-truth set. It measures, on average, how many of the top 𝑘results returned by a query belong\nto the ground-truth set for a query set 𝑄. The higher the value of 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛@𝑘, the stronger the\ncorrelation between the returned results and the query. It is calculated as follows:\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛@𝑘= 1\n|𝑄|\n|𝑄|\n∑︁\n𝑖=1\n| 𝐻𝑖𝑡@𝑘(𝑞𝑖) |\n𝑘\n.\n(24)\nRecall@k indicates the average percentage of the ground-truth answer set for each query that\nis hit. The higher the 𝑅𝑒𝑐𝑎𝑙𝑙@𝑘value, the more ground-truth answers are retrieved. It is calculated\nas follows:\n𝑅𝑒𝑐𝑎𝑙𝑙@𝑘= 1\n|𝑄|\n|𝑄|\n∑︁\n𝑖=1\n| 𝐻𝑖𝑡@𝑘(𝑞𝑖) |\n|𝐺𝑖|\n.\n(25)\nF1-Score is employed to evaluate the performance of a model when both precision and recall\ncarry equal weight. It is calculated as follows:\n𝐹1 −𝑆𝑐𝑜𝑟𝑒= 2 · 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛· 𝑅𝑒𝑐𝑎𝑙𝑙\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+ 𝑅𝑒𝑐𝑎𝑙𝑙.\n(26)\nMAP@k, which stands for Mean Average Precision, reflects the average precision of the rankings\nproduced by all queries. It is calculated as follows:\n𝑀𝐴𝑃@𝑘= 1\n|𝑄|\n|𝑄|\n∑︁\n𝑖=1\n1\n𝑚\n𝑚\n∑︁\n𝑗=1\n𝑗\n𝑟𝑎𝑛𝑘\u0000ℎ𝑖𝑡𝑗,𝑇𝑜𝑝@𝑘(𝑞𝑖)\u0001 ,\n(27)\nwhere 𝑚is |𝐻𝑖𝑡@𝑘|, ℎ𝑖𝑡𝑗is an element in 𝐻𝑖𝑡@𝑘, and 𝑟𝑎𝑛𝑘(𝑒,𝑙) is the rank (i.e., index) of element\n𝑒in list 𝑙. When |𝐻𝑖𝑡@𝑘(𝑞𝑖)| = 0, ℎ𝑖𝑡𝑗does not exist, and the average precision of query 𝑞is 0.\nIt is evident that a larger 𝑀𝐴𝑃@𝑘value signifies that a greater number of answers that hit the\nground-truth are present in the top-𝑘results returned.\nMRR@k, which stands for Mean Reciprocal Rank, indicates the average of the reciprocals of\nthe rankings of all search results. It is calculated as follows:\n𝑀𝑅𝑅@𝑘= 1\n|𝑄|\n|𝑄|\n∑︁\n𝑖=1\n1\n𝑟𝑎𝑛𝑘(ℎ𝑖𝑡1,𝑇𝑜𝑝@𝑘(𝑞𝑖)) .\n(28)\nWhen |𝐻𝑖𝑡@𝑘(𝑞𝑖)| = 0, the reciprocal of the ranking is 0. Typically, only the first answer that hits\nthe ground-truth is taken into account when calculating 𝑀𝑅𝑅@𝑘. The greater the 𝑀𝑅𝑅@𝑘value,\nthe higher the ranking of the answer that hits the ground-truth in 𝑇𝑜𝑝@𝑘.\nFrank@k represents the average rank of the first hit answer across all queries. It is calculated\nas follows:\n𝐹𝑟𝑎𝑛𝑘@𝑘= 1\n|𝑄|\n|𝑄|\n∑︁\n𝑖=1\n𝑟𝑎𝑛𝑘(ℎ𝑖𝑡1 (𝑞𝑖) ,𝑇𝑜𝑝@𝑘(𝑞𝑖)) .\n(29)\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:32\nXie et al.\nIt is evident that a smaller 𝐹𝑟𝑎𝑛𝑘@𝑘value corresponds to a higher ranking of the ground-truth\nanswer in the search results.\nSuccessRate@k indicates the proportion of queries for which there are more than one ground-\ntruth answers among the top-𝑘results returned. It is calculated as follows:\n𝑆𝑢𝑐𝑐𝑒𝑠𝑠𝑅𝑎𝑡𝑒@𝑘= 1\n|𝑄|\n|𝑄|\n∑︁\n𝑖=1\n𝜏\u0000𝐹𝑟𝑎𝑛𝑘𝑞𝑖≤𝑘\u0001 ,\n(30)\nwhere 𝜏is an indicator function that returns 1 when the ranking of the hit answer for query 𝑞𝑖is\nless than 𝑘and 0 otherwise. 𝑆𝑢𝑐𝑐𝑒𝑠𝑠𝑅𝑎𝑡𝑒@𝑘is a crucial evaluation metric because an effective code\nsearch engine should enable software developers to find the desired code snippets by examining\nfewer results.\nNDCG, which stands for Normalized Discounted Cumulative Gain, serves as a crucial metric to\nquantify the similarity between the ranking of candidate code fragments and the ideal ranking,\nplacing significant emphasis on the overall ranking order among all candidate results. It is calculated\nas follows:\n𝑁𝐷𝐶𝐺= 1\n|𝑄|\n𝑘\n∑︁\n𝑖=1\n2𝑟𝑖−1\nlog2(𝑖+ 1),\n(31)\nwhere 𝑟𝑖is the relevance score of the top-𝑘results at position 𝑖.\nTable 9 provides an in-depth analysis of 53 deep learning-based code search models, encompassing\nthe evaluated datasets, the baseline model utilized for comparison, and the selected evaluation\nmetrics. Figure 10 highlights that the CSN dataset holds prominence in the code search task, while\nthe most widely adopted evaluation metric is MRR@k, with a utilization rate of 90.6%.\n(a) Datasets adopted by at least two papers.\n(b) Metrics adopted by at least two papers.\nFig. 10. Datasets and metrics adopted by at least two papers.\n6.3\nUsage Scenarios\nThe rapid evolution of code search technology has presented the industry with a multitude of fresh\nopportunities. An interesting and important question for industrial practitioners is how to choose\nproper code search approaches to fit their needs. This section discusses this question from three\naspects: effectiveness, efficiency, and multi-language.\nEffectiveness. By placing a heightened emphasis on the accuracy of code search, industrial\npractitioners can leverage a suite of powerful techniques. Methods such as RACK [67], NQE [51],\nSEQUER [7], and ZaCQ [16] offer effective means to reconstruct queries, aiding in the clarification\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:33\nTable 9. Deep code search models and their performance metrics.\nYear\nModels\nDataset\nBaselines\nMetric\n2018\nCODEnn [23]\nGu et al. [23]\nCodeHow [55]\nFrank@k, Precision@k, MRR@k, SuccessRate@k\n2018\nNCS [68]\nSachdev et al. [68]\nTF-IDF, BM25\nPrecision@k\n2018\nNLP2API [66]\nRahman and Roy [66]\nRahman and Roy[66]\nPrecision@k, MRR@K,\nMAP@K, Recall@K, Frank@k\n2018\nCOCABU [72]\nSirres et al. [72]\nCodota, OpenHub\nPrecision@k, MRR@k\n2018\nZhang et al. [99]\nZhang et al. [99]\nDice, Rocchio, RSV\nMRR@k, SuccessRate@k\n2019\nUNIF [6]\nGu et al. [23]\nCODEnn [23], NCS [68]\nSuccessRate@k\n2019\nMMAN [78]\nWan et al. [78]\nCodeHow [55], CODEnn [23]\nMRR@k, SuccessRate@k\n2019\nRACK [67]\nRahman [67]\nNL Keywords [67]\nPrecision@k, MRR@K, MAP@K,\nRecall@K, NDCG, Frank@k\n2019\nRahman [65]\nRahman [65]\n-\nHit@K, MAP@k, MRR@k, Recall@k, Frank@k\n2019\nNQE [51]\nLiu et al. [51]\nBM25, NCS [68]\nPrecision@k, MRR@k\n2019\nQESC [34]\nHuang et al. [34]\nCodeHow [55], QECK [60]\nPrecision@k, NDCG\n2019\nCoaCor [94]\nStaQC [95], DEV and EVAL [36]\nCODEnn [23], CODE-NN [36]\nMRR@k\n2019\nWu and Yang [90]\nWu and Yang [90]\nCodeHow [55]\nPrecision@k, NDCG\n2020\nOCoR [103]\nStaQC [95], DEV and EVAL [36]\nCODEnn [23], CODE-NN [36], CoaCor [94]\nMRR@k\n2020\nCodeBERT [17]\nCSN [35]\nNBoW, 1D-CNN, biRNN, SelfAtt, RoBERTa(code)\nMRR@k\n2020\nAdaCS [48]\nLing et al. [48]\nCodeHow [55], CODEnn [23], BVAE [11]\nHit@K, MRR@k\n2020\nMP-CAT [26]\nCoNaLa [97]\nCT\nRecall@k, MRR@k\n2020\n𝑇𝑟𝑎𝑛𝑆3 [85]\nBarone and Sennrich [3]\nCODEnn [23], CoaCor [94], Hybrid-DeepCom [32], AutoSum [81]\nMRR@k, NDCG, SuccessRate@k\n2020\nZhao and Sun [101]\nStaQC [95]\nCODEnn [23], CoaCor [94]\nMAP@k, NDCG\n2020\nCO3 [96]\nStaQC [95]\nCODEnn [23], CoaCor [94]\nMRR@k, NDCG\n2021\nGraphCodeBERT [25]\nCSN [35]\nNBoW, 1D-CNN, biRNN, SelfAtt, RoBERTa(code), CodeBERT [17]\nMRR@k\n2021\nDGMS [49]\nFB-Java [42], CSN-Python [35]\nNBoW, 1D-CNN, biRNN, SelfAtt, CODEnn [23], UNIF [6], CAT [26]\nMRR@k, SuccessRate@k\n2021\nCoCLR [33]\nCoSQA [33], WebQueryTest [54]\nRoBERTa [53], CodeBERT [17]\nMRR@k\n2021\nSEQUER [7]\nCao et al. [7]\nseq2seq [77]\nMRR@k\n2021\nDOBF [41]\nCSN-Python [35]\nCodeBERT [17], GraphCodeBERT [25]\nMRR@k\n2021\nCRaDLe [21]\nCSN [35]\nCODEnn [23], UNIF [6],\nNBoW, 1D-CNN, biRNN, SelfAtt, ConvSelfAtt\nMRR@k, SuccessRate@k\n2021\nCorder [4]\nGu et al. [23]\nNBoW, biRNN, SelfAtt, TBCNN [59], Code2vec [1]\nPrecision@k, MRR@k\n2021\nGu et al. [20]\nCSN [35]\nNBoW, 1D-CNN, biRNN, SelfAtt\nMRR@k, NDCG\n2021\nTabCS [91]\nHu et al. [32], CSN [35]\nCODEnn [23], CARLCS-CNN [71], CARLCS-TS [71], UNIF [6]\nMRR@k, SuccessRate@k\n2021\nMuCoS [15]\nCSN [35]\nNBoW, 1D-CNN, biRNN, SelfAtt, ConvSelfAtt,\nCODEnn [23], CodeBERT [17]\nMRR@k, SuccessRate@k\n2021\nSynCoBERT [86]\nCSN [35], AdvTest [54]\nNBow, CNN, BiRNN, SelfAttn, RoBERTa [53],\nRoBERTa(code), CodeBERT [17], GraphCodeBERT [25]\nMRR@k\n2022\nTranCS [75]\nCSN-Java [35]\nCODEnn [23], MMAN [78]\nMRR@k, SuccessRate@k\n2022\nWang et al. [83]\nCSN [35]\nNBoW, 1D-CNN, biRNN, SelfAtt, RoBERTa [53],\nRoBERTa(code), CodeBERT [17], GraphCodeBERT [25]\nMRR@k\n2022\nCodeRetriever [44]\nCSN [35], AdvTest [54], CoSQA [33],\nCoNaLa [97], SO-DS [28], StaQC [95]\nCodeBERT [17], GraphCodeBERT [25], SynCoBERT [86]\nMRR@k\n2022\nCDCS [8]\nCSN-Python [35], CSN-Java [35],\nSolidity and SQL [93]\nRoberta [53], CodeBERT [17]\nMRR@k, SuccessRate@k\n2022\nCSRS [12]\nGu et al. [23]\nCODEnn [23], CARLCS-CNN [71]\nRecall@k, MRR@k, NDCG\n2022\n𝑁𝑆3 [2]\nCSN [35], CoSQA [33]\nBM25, RoBERTa(code), CuBERT [37],\nCodeBERT [17], GraphCodeBERT [25]\nMRR@k, Precision@k\n2022\nCSSAM [5]\nHu et al. [32], CSN [35]\nCodeHow [55], CODEnn [23], MP-CAT [26], TabCS [91]\nMRR@k, SuccessRate@k, NDCG\n2022\nCTBERT [27]\nCSN [35], AdvTest [54]\nCodeBERT [17], GraphCodeBERT [25], SynCoBERT [86]\nMRR@k\n2022\nQueCos [82]\nCSN-Python [35], CSN-Java [35],\nWang et al. [82]\nCODEnn [23], UNIF [6], OCoR [103]\nMRR@k, SuccessRate@k\n2022\nZaCQ [16]\nCSN [35]\nV-DO, KW\nMRR@k, MAP@k, NDCG\n2022\nG2SC [70]\nCSN [35]\nCODEnn [23], MMAN [78], CodeBERT [17], GraphCodeBERT [25]\nMRR@k\n2022\nSPT-Code [61]\nCSN [35]\nCNN, Bi-GRU, SelfAtt, CodeBERT [17], GraphCodeBERT [25]\nMRR@k\n2022\nCODE-MVP [87]\nAdvTest [54], CoNaLa [97], CoSQA [33]\nRoBERTa [53], CodeBERT [17],\nGraphCodeBERT [25], SynCoBERT [86]\nMRR@k\n2022\nUniXcoder [24]\nCSN [35], AdvTest [54], CoSQA [33]\nRoBERTa [53], CodeBERT [17],\nGraphCodeBERT [25], SynCoBERT [86]\nMRR@k\n2022\nLi et al. [43]\nCSN [35]\nRoBERTa(code), CodeBERT [17],\nGraphCodeBERT [25], UniXCoder [24]\nMRR@k\n2022\nSCodeR [45]\nCSN [35], AdvTest [54], CoSQA [33]\nCodeBERT [17], GraphCodeBERT [25], SyncoBERT [86],\nCodeRetriever [44], Code-MVP [87], UniXcoder [24]\nMRR@k\n2023\nSalza et al. [69]\nSalza et al. [69]\nLUCENE, CODEnn [23]\nMRR@k, SuccessRate@k\n2023\ndeGraphCS [98]\nZeng et al. [98]\nCODEnn [23], UNIF [6], MMAN [78]\nMRR@k, SuccessRate@k\n2023\nGraphSearchNet [52]\nCSN-Python [35], CSN-Java [35]\nNBoW, 1D-CNN, biRNN, SelfAtt, UNIF [6],\nCODEnn [23], CARLCS-CNN [71], TabCS [91], Coacor [94]\nMRR@k, NDCG, SuccessRate@k\n2023\nMulCS [56]\nCSN [35], C dataset [56]\nCODEnn [23], TabCS [91], NBoW, 1D-CNN, biRNN, SelfAtt\nMRR@k, SuccessRate@k\n2023\nKeyDAC [63]\nCoSQA [33], WebQueryTest [54]\nCoCLR [33]\nMRR@k\n2023\nTOSS [31]\nCSN [35]\nJaccard, BOW, TFIDF, BM25, CODEnn [23],\nCodeBERT [17], GraphCodeBERT [25], CoCLR [33]\nMRR@k, Precision@k\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:34\nXie et al.\nof user search intent. To deepen the understanding of code semantics, practitioners can employ\nmethods such as MP-CAT [26], CRaDLe [21], GraphCodeBERT [25], TabCS [91], SynCoBERT\n[86], G2SC [70], SPT-Code [61], and UniXcoder [24]. Furthermore, methods such as MP-CAT\n[26], TabCS [91], CSRS [12], 𝑁𝑆3 [2], and CSSAM [5] enable fine-grained interaction between\nquery and code, effectively bridging the semantic gap that exists between them. Considering the\npivotal role of the training corpus in determining the performance of neural code search [76], it\nis imperative for practitioners to prioritize the quality of their data. By meticulously cleaning the\ntraining corpus, practitioners can attain a high-quality dataset that fosters the establishment of\na precise mapping from natural language to programming language. Furthermore, practitioners\ncan leverage CodeRetriever [44] to specifically concentrate on examining the semantic distinctions\nbetween query and code via Contrastive Learning. This approach facilitates a detailed analysis of\nthe nuanced differences, thereby enhancing the overall search accuracy. Additionally, the recall and\nrerank framework offered by TOSS [31] presents an invaluable opportunity to augment the accuracy\nof code search. By integrating this framework, practitioners can achieve further improvements in\nthe accuracy of their products.\nEfficiency. By prioritizing search efficiency, industrial practitioners can leverage the offline\ncalculation of code fragment embeddings within their codebases. This approach offers a time-saving\nalternative to the online calculation of vector similarity. Moreover, the utilization of CoSHC [22], a\ncode search acceleration method based on deep hashing and code classification, enables efficient\ncode search while accepting a minor trade-off in precision.\nMulti-language. By emphasizing the generalization of code search tools across multiple pro-\ngramming languages, industrial practitioners can use MulCS [56] to break down the semantic\nbarriers between different programming languages, which uses a unified data structure to represent\nmultiple programming languages.\nSummary of answers to RQ4:\n• The primary source of the code in the code search datasets originates from the open-\nsource code repository, Github. On the other hand, Stack Overflow (SO) serves as the\nmain source of queries these the datasets.\n• Python and Java are the two most concerned programming languages for code search\ntasks.\n• For nearly 6 years, the CSN dataset has dominated the evaluation of code search tasks.\n• The prevalent evaluation metrics employed in code search encompass Precision@k,\nRecall@k, F1-score, MAP@k, MRR@k, Frank@k, and SuccessRate@k, with MRR@k\nwidely utilized indicator among them.\n7\nOPEN CHALLENGES AND FUTURE DIRECTIONS (RQ5)\nThis section addresses the outstanding challenges in the field of code search and identifies potential\navenues for future advancements.\n(1) Understanding code structure. The information contained in code can be divided into two\nparts: structured information determined by keywords that define the code’s function, and aux-\niliary information reflected by identifiers such as function and variable names, which indicate\na software developer’s generalization of code functions. However, studies have shown that\nanonymizing function and variable names can lead to a significant drop in model performance\n[24], indicating that current methods rely too heavily on identifier information for inference.\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:35\nIdentifier content serves as a convenient shortcut for code intelligence tasks, but an overre-\nliance on it may hinder the model’s ability to learn the code functions defined by structured\ninformation, thereby hindering its overall understanding of code structure. The challenge is to\ndesign a strategy that allows the model to better focus on and understand the code’s structural\ninformation, and to strike a balance between the structural information defined by keywords\nand the auxiliary information provided by identifiers during both training and inference. This\nis a direction worthy of further research. First steps toward that goal have been taken, e.g., by\nLi et al. [45] and Wang et al. [87].\n(2) Graph pre-training technology. Code can be analyzed and represented as a graph data\nstructure (e.g., Data Flow Graph [25] and Variable-based Flow Graph [98]). Compared to\nsequential structures, using a graph structure to represent code provides a larger amount of\ninformation and higher accuracy. We believe that this graph structure encompasses valuable\ninsights that can lead to breakthroughs in understanding the meaning of code. However,\nbased on the results of existing research, methods utilizing Transformer-encoded sequences\n[24, 44, 86, 87] continue to maintain their lead over those based on graph neural network\napproaches for graph encoding [49, 52, 56, 98]. The remarkable success of the Transformer-\nbased methods can be largely attributed to the substantial improvement in model capacity\nachieved through pre-training techniques. We believe that the potential of graph-based methods\nin code intelligence has not been fully explored yet. A valuable research direction lies in\nexploring how to incorporate graph pre-training into code intelligence, aiming to fully unleash\nthe performance of graph-based models.\n(3) Robust optimization. Research has indicated that current code intelligence methods are\nsusceptible to adversarial attacks [46]. Perturbing identifiers significantly decreases the model’s\nperformance, highlighting its lack of robustness. Efforts have been made to enhance robustness\nthrough adversarial training [79], but further studies are required. At the same time, it is crucial\nto establish a standard evaluation method or dataset to evaluate the robustness of these models.\n(4) Interpretability research. Like other deep learning models, code search models are trained\nand operate in a relatively black-box manner. Interpretability research aims to understand the\nreasons behind a model’s decisions, improve our understanding of the model’s inference process,\nand identify the information that the model considers important. This research is crucial as it\ncan boost user confidence in the model and, in case of errors, enable the identification of the\ncause and development of a solution in a timely manner. Karmakar and Robbes [38] and Wan\net al. [80] propose promising first steps into this direction.\n(5) Comprehensive evaluation. Presently, the most efficient approach in the field of code search\nis the pre-training & fine-tuning paradigm (e.g., CodeBERT [17], GraphCodeBERT [25], Syn-\ncobert [86], and UniXcoder [24]). This paradigm encompasses several crucial design elements,\nincluding data preprocessing strategy, tokenizer, pre-training task, pre-training model & train-\ning parameters, fine-tuning model & training parameters, and negative sample construction\nstrategy. However, there is a shortage of analytical experiments on these key elements that\ncan guide practitioners in choosing the most effective training strategies for code intelligence.\nFor example, Guo et al. [25] adopt the parameters of CodeBERT [17] to initialize the Graph-\nCodeBERT model and proceed with continue pre-training on the CSN dataset using tasks like\nMLM. However, the performance improvement has not been validated to determine whether\nit is a result of additional iterations on the MLM task. Investigating the significance of these\ndesign elements will offer more direction to practitioners and also provide new insights for\nfuture research by uncovering the unique properties of code intelligence.\n(6) High-quality datasets. At present, the most widely used datasets in code search come from\nthree sources: crawling from Github (e.g., CSN [35]), which consists of code comments as\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:36\nXie et al.\nqueries and the rest as code; crawling from Stack Overflow (e.g., StaQC [95] and CoNaLa [97]),\nwhich consists of questions and code answers in posts; and actual user queries collected from\nsearch engines (e.g., CoSQA [33]), with codes annotated by annotators . Each of these data\nsources has its own limitations: in the Github data, code comments are significantly different\nfrom actual queries; in the Stack Overflow data, the answered code is often not a complete\nfunction; and in the annotated data, a vast amount of background knowledge is required to\nunderstand the code, making it difficult to guarantee the scale and quality of the data. Therefore,\nwe believe it is essential to create a more practical dataset for model training and evaluation.\nOne potential solution is to collect online user behavior records, such as clicks and stays, which\nwould require a highly performing code search engine with a large user base. Hence, there is a\npotential to use the existing model to build a preliminary code search engine, attract a group of\nusers, and collect user data to create a new dataset.\n(7) More fine-grained interaction. Current methods of code search have limitations in their\nability to model the interaction between query and code. These limitations stem from the fact\nthat existing methods only model the interaction at the representation level [12, 91] and fail to\nconsider cross-modal interaction at the token level. Additionally, these methods use a single\nlevel for discrimination, which limits the ability to capture hierarchical information. Hence,\nthe model architecture used for code search still holds potential for improvement. First steps\ntoward that goal have been taken, e.g., by Dong et al. [14].\n(8) Improving code search efficiency. Deep learning-based code search methods have demon-\nstrated promising results. However, previous approaches have predominantly focused on re-\ntrieval accuracy while neglecting the consideration of retrieval efficiency. If the code search\nmodel is intended for real-world online scenarios, enhancing retrieval efficiency becomes a\nchallenge that demands immediate attention. Addressing this crucial concern is vital to success-\nfully deploy and utilize the model in practical applications. Gu et al. [22] propose promising\nfirst steps into this direction.\n(9) Context-related code search. The current method assumes that functions in a project are\nindependent [23, 33, 35], disregarding their relationships within the project. However, functions\nin a project are actually interdependent, with function calls and shared variables. To accurately\nmodel a function’s role, its context must be considered. Designing a method to model contextual\ninformation and efficiently search for functions with context information online is a valuable\nresearch direction.\n(10) Personalized code search. Software developers possess unique programming habits, leading\nto varying speeds of understanding and preferences for different forms of code for the same\nfunction. Consequently, code search results should be tailored to individual users based on their\nprogramming preferences. A potential area for research is the implementation of a personalized\ncode search service.\n(11) Better code analysis tools. Different data structures like Abstract Syntax Trees, Control Flow\nGraphs, Program Dependency Graphs, and Data Flow Graphs can assist in comprehending the\nsemantics of code. However, the absence of useful open source tools to extract these structures\nimpedes the progress of code intelligence research. For instance, the extraction of program\ndependency graphs is currently limited to tools designed specifically for the Java programming\nlanguage. This poses a hindrance to research in the field of code intelligence. Thus, developing\nbetter open-source code analysis tools for multiple programming languages would be a beneficial\ndirection to further advance related research.\n(12) Exploring new search paradigms. Recently, large-scale language models used for code\ngeneration have provided functionality akin to code search. For instance, GitHub’s Copilot tool\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:37\n2, backed by Codex [10] at its core, maps natural language descriptions of desired functionalities\nto corresponding code snippets that provide those functionalities. Similarly, OpenAI’s ChatGPT\ntool 3, powered by GPT-4 [62], maps natural language descriptions of desired functionalities\nto code snippets that offer the desired functionalities, accompanied by explanatory natural\nlanguage text to enhance developers’ understanding and usability. In light of the impact of\nthese tools, it is imperative that we reconsider the significance and form of code search tasks.\nExploring new paradigms may usher in a new era of transformation for code search. One\npromising direction worth exploring is cross-fertilization with other code intelligence tasks,\nsuch as leveraging code search results to assist code completion or code generation.\n8\nCONCLUSION\nIn this survey, we have presented a comprehensive overview of deep learning-based methods for\nthe code search task. We start by introducing the code search task and outlining the framework of\ndeep learning-based code search method. Next, we detail the methods for extracting representations\nof query and code, respectively. Furthermore, we categorize many loss functions about the model\ntraining. Finally, we identify several open challenges and promising research directions in this area.\nWe hope this survey can help both academic researchers and industry practitioners, and inspire\nmore meaningful work in this field.\nREFERENCES\n[1] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: learning distributed representations of\ncode. Proc. ACM Program. Lang. 3, POPL (2019), 40:1–40:29. https://doi.org/10.1145/3290353\n[2] Shushan Arakelyan, Anna Hakhverdyan, Miltiadis Allamanis, Luis Garcia, Christophe Hauser, and Xiang Ren.\n2022. NS3: Neuro-symbolic Semantic Code Search. In NeurIPS. http://papers.nips.cc/paper_files/paper/2022/hash/\n43f5f6c5cb333115914c8448b8506411-Abstract-Conference.html\n[3] Antonio Valerio Miceli Barone and Rico Sennrich. 2017. A Parallel Corpus of Python Functions and Documentation\nStrings for Automated Code Documentation and Code Generation. In Proceedings of the Eighth International Joint\nConference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 - December 1, 2017, Volume 2:\nShort Papers, Greg Kondrak and Taro Watanabe (Eds.). Asian Federation of Natural Language Processing, 314–319.\nhttps://aclanthology.org/I17-2053/\n[4] Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021. Self-Supervised Contrastive Learning for Code Retrieval and\nSummarization via Semantic-Preserving Transformations. In SIGIR ’21: The 44th International ACM SIGIR Conference\non Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021, Fernando Diaz, Chirag\nShah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai (Eds.). ACM, 511–521. https://doi.org/10.1145/\n3404835.3462840\n[5] Bo Cai, Yaoxiang Yu, and Yi Hu. 2023. CSSAM: Code Search via Attention Matching of Code Semantics and Structures.\nIn IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2023, Taipa, Macao, March\n21-24, 2023, Tao Zhang, Xin Xia, and Nicole Novielli (Eds.). IEEE, 402–413. https://doi.org/10.1109/SANER56733.2023.\n00045\n[6] José Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra. 2019. When deep learning met code\nsearch. In Proceedings of the ACM Joint Meeting on European Software Engineering Conference and Symposium on the\nFoundations of Software Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia, August 26-30, 2019, Marlon Dumas,\nDietmar Pfahl, Sven Apel, and Alessandra Russo (Eds.). ACM, 964–974. https://doi.org/10.1145/3338906.3340458\n[7] Kaibo Cao, Chunyang Chen, Sebastian Baltes, Christoph Treude, and Xiang Chen. 2021. Automated Query Reformu-\nlation for Efficient Search based on Query Logs From Stack Overflow. In 43rd IEEE/ACM International Conference on\nSoftware Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021. IEEE, 1273–1285. https://doi.org/10.1109/ICSE43902.\n2021.00116\n[8] Yitian Chai, Hongyu Zhang, Beijun Shen, and Xiaodong Gu. 2022. Cross-Domain Deep Code Search with Meta\nLearning. In 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA,\nMay 25-27, 2022. ACM, 487–498. https://doi.org/10.1145/3510003.3510125\n2https://github.com/features/copilot\n3https://openai.com/blog/chatgpt\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:38\nXie et al.\n[9] Shaunak Chatterjee, Sudeep Juvekar, and Koushik Sen. 2009. SNIFF: A Search Engine for Java Using Free-Form\nQueries. In Fundamental Approaches to Software Engineering, 12th International Conference, FASE 2009, Held as Part\nof the Joint European Conferences on Theory and Practice of Software, ETAPS 2009, York, UK, March 22-29, 2009.\nProceedings (Lecture Notes in Computer Science, Vol. 5503), Marsha Chechik and Martin Wirsing (Eds.). Springer,\n385–400. https://doi.org/10.1007/978-3-642-00593-0_26\n[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,\nHeidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,\nLukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas\nTezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira\nMurati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\nZaremba. 2021. Evaluating Large Language Models Trained on Code. CoRR abs/2107.03374 (2021). arXiv:2107.03374\nhttps://arxiv.org/abs/2107.03374\n[11] Qingying Chen and Minghui Zhou. 2018. A neural framework for retrieval and summarization of source code. In\nProceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, ASE 2018, Montpellier,\nFrance, September 3-7, 2018, Marianne Huchard, Christian Kästner, and Gordon Fraser (Eds.). ACM, 826–831. https:\n//doi.org/10.1145/3238147.3240471\n[12] Yi Cheng and Li Kuang. 2022. CSRS: code search with relevance matching and semantic matching. In Proceedings\nof the 30th IEEE/ACM International Conference on Program Comprehension, ICPC 2022, Virtual Event, May 16-17,\n2022, Ayushi Rastogi, Rosalia Tufano, Gabriele Bavota, Venera Arnaoudova, and Sonia Haiduc (Eds.). ACM, 533–542.\nhttps://doi.org/10.1145/3524610.3527889\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association\nfor Computational Linguistics, 4171–4186. https://doi.org/10.18653/v1/n19-1423\n[14] Hande Dong, Jiayi Lin, Yichong Leng, Jiawei Chen, and Yutao Xie. 2023. Retriever and Ranker Framework with\nProbabilistic Hard Negative Sampling for Code Search. CoRR abs/2305.04508 (2023). https://doi.org/10.48550/arXiv.\n2305.04508 arXiv:2305.04508\n[15] Lun Du, Xiaozhou Shi, Yanlin Wang, Ensheng Shi, Shi Han, and Dongmei Zhang. 2021. Is a Single Model Enough?\nMuCoS: A Multi-Model Ensemble Learning Approach for Semantic Code Search. In CIKM ’21: The 30th ACM Interna-\ntional Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5,\n2021, Gianluca Demartini, Guido Zuccon, J. Shane Culpepper, Zi Huang, and Hanghang Tong (Eds.). ACM, 2994–2998.\nhttps://doi.org/10.1145/3459637.3482127\n[16] Zachary Eberhart and Collin McMillan. 2022. Generating Clarifying Questions for Query Refinement in Source Code\nSearch. In IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2022, Honolulu, HI,\nUSA, March 15-18, 2022. IEEE, 140–151. https://doi.org/10.1109/SANER53432.2022.00028\n[17] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu,\nDaxin Jiang, and Ming Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In\nFindings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020 (Findings\nof ACL, Vol. EMNLP 2020), Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics,\n1536–1547. https://doi.org/10.18653/v1/2020.findings-emnlp.139\n[18] Mohammad Gharehyazie, Baishakhi Ray, and Vladimir Filkov. 2017. Some from here, some from there: cross-project\ncode reuse in GitHub. In Proceedings of the 14th International Conference on Mining Software Repositories, MSR 2017,\nBuenos Aires, Argentina, May 20-28, 2017, Jesús M. González-Barahona, Abram Hindle, and Lin Tan (Eds.). IEEE\nComputer Society, 291–301. https://doi.org/10.1109/MSR.2017.15\n[19] Luca Di Grazia and Michael Pradel. 2023. Code Search: A Survey of Techniques for Finding Code. ACM Comput. Surv.\n55, 11 (2023), 220:1–220:31. https://doi.org/10.1145/3565971\n[20] Jian Gu, Zimin Chen, and Martin Monperrus. 2021. Multimodal Representation for Neural Code Search. In IEEE\nInternational Conference on Software Maintenance and Evolution, ICSME 2021, Luxembourg, September 27 - October 1,\n2021. IEEE, 483–494. https://doi.org/10.1109/ICSME52107.2021.00049\n[21] Wenchao Gu, Zongjie Li, Cuiyun Gao, Chaozheng Wang, Hongyu Zhang, Zenglin Xu, and Michael R. Lyu. 2021.\nCRaDLe: Deep code retrieval based on semantic Dependency Learning. Neural Networks 141 (2021), 385–394.\nhttps://doi.org/10.1016/j.neunet.2021.04.019\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:39\n[22] Wenchao Gu, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Michael R. Lyu. 2022. Accelerating\nCode Search with Deep Hashing and Code Classification. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan,\nPreslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, 2534–2544. https://doi.org/\n10.18653/v1/2022.acl-long.181\n[23] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In Proceedings of the 40th International\nConference on Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018, Michel Chaudron, Ivica\nCrnkovic, Marsha Chechik, and Mark Harman (Eds.). ACM, 933–944. https://doi.org/10.1145/3180155.3180167\n[24] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022. UniXcoder: Unified Cross-Modal\nPre-training for Code Representation. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, Preslav Nakov, and\nAline Villavicencio (Eds.). Association for Computational Linguistics, 7212–7225. https://doi.org/10.18653/v1/2022.acl-\nlong.499\n[25] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy,\nShengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,\nand Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with Data Flow. In 9th International\nConference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https:\n//openreview.net/forum?id=jLoC4ez43PZ\n[26] Rajarshi Haldar, Lingfei Wu, Jinjun Xiong, and Julia Hockenmaier. 2020. A Multi-Perspective Architecture for\nSemantic Code Search. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for\nComputational Linguistics, 8563–8568. https://doi.org/10.18653/v1/2020.acl-main.758\n[27] Hojae Han, Seung-won Hwang, Shuai Lu, Nan Duan, and Seungtaek Choi. 2022. Towards Compositional Generalization\nin Code Search. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP\n2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.).\nAssociation for Computational Linguistics, 10743–10750. https://aclanthology.org/2022.emnlp-main.737\n[28] Geert Heyman and Tom Van Cutsem. 2020. Neural Code Search Revisited: Enhancing Code Snippet Retrieval through\nNatural Language Intent. CoRR abs/2008.12193 (2020). arXiv:2008.12193 https://arxiv.org/abs/2008.12193\n[29] Emily Hill, Manuel Roldan-Vega, Jerry Alan Fails, and Greg Mallet. 2014. NL-based query refinement and contex-\ntualized code search results: A user study. In 2014 Software Evolution Week - IEEE Conference on Software Main-\ntenance, Reengineering, and Reverse Engineering, CSMR-WCRE 2014, Antwerp, Belgium, February 3-6, 2014, Serge\nDemeyer, Dave W. Binkley, and Filippo Ricca (Eds.). IEEE Computer Society, 34–43. https://doi.org/10.1109/CSMR-\nWCRE.2014.6747190\n[30] Abram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su, and Premkumar T. Devanbu. 2016. On the naturalness of\nsoftware. Commun. ACM 59, 5 (2016), 122–131. https://doi.org/10.1145/2902362\n[31] Fan Hu, Yanlin Wang, Lun Du, Xirong Li, Hongyu Zhang, Shi Han, and Dongmei Zhang. 2023. Revisiting Code\nSearch in a Two-Stage Paradigm. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data\nMining, WSDM 2023, Singapore, 27 February 2023 - 3 March 2023, Tat-Seng Chua, Hady W. Lauw, Luo Si, Evimaria\nTerzi, and Panayiotis Tsaparas (Eds.). ACM, 994–1002. https://doi.org/10.1145/3539597.3570383\n[32] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment generation with hybrid lexical and\nsyntactical information. Empir. Softw. Eng. 25, 3 (2020), 2179–2217. https://doi.org/10.1007/s10664-019-09730-9\n[33] Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou, and Nan Duan. 2021. CoSQA: 20,\n000+ Web Queries for Code Search and Question Answering. In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP\n2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli\n(Eds.). Association for Computational Linguistics, 5690–5700. https://doi.org/10.18653/v1/2021.acl-long.442\n[34] Qing Huang, Yang Yang, and Ming Cheng. 2019. Deep learning the semantics of change sequences for query expansion.\nSoftw. Pract. Exp. 49, 11 (2019), 1600–1617. https://doi.org/10.1002/spe.2736\n[35] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. CodeSearchNet\nChallenge: Evaluating the State of Semantic Code Search. CoRR abs/1909.09436 (2019). arXiv:1909.09436 http:\n//arxiv.org/abs/1909.09436\n[36] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing Source Code using a\nNeural Attention Model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,\nACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics.\nhttps://doi.org/10.18653/v1/p16-1195\n[37] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020. Learning and Evaluating Contextual\nEmbedding of Source Code. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:40\nXie et al.\nJuly 2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119). PMLR, 5110–5121. http://proceedings.\nmlr.press/v119/kanade20a.html\n[38] Anjan Karmakar and Romain Robbes. 2021. What do pre-trained code models know about code?. In 36th IEEE/ACM\nInternational Conference on Automated Software Engineering, ASE 2021, Melbourne, Australia, November 15-19, 2021.\nIEEE, 1332–1336. https://doi.org/10.1109/ASE51524.2021.9678927\n[39] Staffs Keele. 2007. Guidelines for performing systematic literature reviews in software engineering. Technical Report.\nVer. 2.3 EBSE Technical Report.\n[40] Muhammad Khalifa. 2019. Semantic Source Code Search: A Study of the Past and a Glimpse at the Future. CoRR\nabs/1908.06738 (2019). arXiv:1908.06738 http://arxiv.org/abs/1908.06738 Withdrawn..\n[41] Marie-Anne Lachaux, Baptiste Rozière, Marc Szafraniec, and Guillaume Lample. 2021. DOBF: A Deobfuscation\nPre-Training Objective for Programming Languages. In Advances in Neural Information Processing Systems 34: Annual\nConference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, Marc’Aurelio\nRanzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (Eds.). 14967–14979.\nhttps://proceedings.neurips.cc/paper/2021/hash/7d6548bdc0082aacc950ed35e91fcccb-Abstract.html\n[42] Hongyu Li, Seohyun Kim, and Satish Chandra. 2019. Neural Code Search Evaluation Dataset. CoRR abs/1908.09804\n(2019). arXiv:1908.09804 http://arxiv.org/abs/1908.09804\n[43] Haochen Li, Chunyan Miao, Cyril Leung, Yanxian Huang, Yuan Huang, Hongyu Zhang, and Yanlin Wang. 2022.\nExploring Representation-level Augmentation for Code Search. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav\nGoldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 4924–4936. https:\n//aclanthology.org/2022.emnlp-main.327\n[44] Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang, Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu\nChen, and Nan Duan. 2022. CodeRetriever: Unimodal and Bimodal Contrastive Learning. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates,\nDecember 7-11, 2022. Association for Computational Linguistics.\n[45] Xiaonan Li, Daya Guo, Yeyun Gong, Yun Lin, Yelong Shen, Xipeng Qiu, Daxin Jiang, Weizhu Chen, and Nan\nDuan. 2022. Soft-Labeled Contrastive Pre-Training for Function-Level Code Representation. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022,\nYoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 118–129.\nhttps://aclanthology.org/2022.findings-emnlp.9\n[46] Yiyang Li, Hongqiu Wu, and Hai Zhao. 2022. Semantic-Preserving Adversarial Code Comprehension. In Proceedings\nof the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October\n12-17, 2022, Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi,\nPum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan\nKim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na (Eds.).\nInternational Committee on Computational Linguistics, 3017–3028. https://aclanthology.org/2022.coling-1.267\n[47] Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green, Alexey\nSvyatkovskiy, Shengyu Fu, and Neel Sundaresan. 2022. Automating code review activities by large-scale pre-training.\nIn Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations\nof Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022, Abhik Roychoudhury, Cristian\nCadar, and Miryung Kim (Eds.). ACM, 1035–1047. https://doi.org/10.1145/3540250.3549081\n[48] Chunyang Ling, Zeqi Lin, Yanzhen Zou, and Bing Xie. 2020. Adaptive Deep Code Search. In ICPC ’20: 28th International\nConference on Program Comprehension, Seoul, Republic of Korea, July 13-15, 2020. ACM, 48–59. https://doi.org/10.\n1145/3387904.3389278\n[49] Xiang Ling, Lingfei Wu, Saizhuo Wang, Gaoning Pan, Tengfei Ma, Fangli Xu, Alex X. Liu, Chunming Wu, and Shouling\nJi. 2021. Deep Graph Matching and Searching for Semantic Code Retrieval. ACM Trans. Knowl. Discov. Data 15, 5\n(2021), 88:1–88:21. https://doi.org/10.1145/3447571\n[50] Chao Liu, Xin Xia, David Lo, Cuiyun Gao, Xiaohu Yang, and John C. Grundy. 2022. Opportunities and Challenges in\nCode Search Tools. ACM Comput. Surv. 54, 9 (2022), 196:1–196:40. https://doi.org/10.1145/3480027\n[51] Jason Liu, Seohyun Kim, Vijayaraghavan Murali, Swarat Chaudhuri, and Satish Chandra. 2019. Neural query expansion\nfor code search. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming\nLanguages, MAPL@PLDI 2019, Phoenix, AZ, USA, June 22, 2019, Tim Mattson, Abdullah Muzahid, and Armando\nSolar-Lezama (Eds.). ACM, 29–37. https://doi.org/10.1145/3315508.3329975\n[52] Shangqing Liu, Xiaofei Xie, Jing Kai Siow, Lei Ma, Guozhu Meng, and Yang Liu. 2023. GraphSearchNet: Enhancing\nGNNs via Capturing Global Dependencies for Semantic Code Search. IEEE Trans. Software Eng. 49, 4 (2023), 2839–2855.\nhttps://doi.org/10.1109/TSE.2022.3233901\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:41\n[53] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\nand Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692\n(2019). arXiv:1907.11692 http://arxiv.org/abs/1907.11692\n[54] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn\nDrain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming\nZhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021. CodeXGLUE: A Machine\nLearning Benchmark Dataset for Code Understanding and Generation. In Proceedings of the Neural Information\nProcessing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual,\nJoaquin Vanschoren and Sai-Kit Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\nc16a5320fa475530d9583c34fd356ef5-Abstract-round1.html\n[55] Fei Lv, Hongyu Zhang, Jian-Guang Lou, Shaowei Wang, Dongmei Zhang, and Jianjun Zhao. 2015. CodeHow: Effective\nCode Search Based on API Understanding and Extended Boolean Model (E). In 30th IEEE/ACM International Conference\non Automated Software Engineering, ASE 2015, Lincoln, NE, USA, November 9-13, 2015, Myra B. Cohen, Lars Grunske,\nand Michael Whalen (Eds.). IEEE Computer Society, 260–270. https://doi.org/10.1109/ASE.2015.42\n[56] Yingwei Ma, Yue Yu, Shanshan Li, Zhouyang Jia, Jun Ma, Rulin Xu, Wei Dong, and Xiangke Liao. 2023. MulCS:\nTowards a Unified Deep Representation for Multilingual Code Search. In IEEE International Conference on Software\nAnalysis, Evolution and Reengineering, SANER 2023, Taipa, Macao, March 21-24, 2023, Tao Zhang, Xin Xia, and Nicole\nNovielli (Eds.). IEEE, 120–131. https://doi.org/10.1109/SANER56733.2023.00021\n[57] Vadim Markovtsev and Waren Long. 2018. Public git archive: a big code dataset for all. In Proceedings of the 15th\nInternational Conference on Mining Software Repositories, MSR 2018, Gothenburg, Sweden, May 28-29, 2018, Andy\nZaidman, Yasutaka Kamei, and Emily Hill (Eds.). ACM, 34–37. https://doi.org/10.1145/3196398.3196464\n[58] Collin McMillan, Mark Grechanik, Denys Poshyvanyk, Qing Xie, and Chen Fu. 2011. Portfolio: finding relevant\nfunctions and their usage. In Proceedings of the 33rd International Conference on Software Engineering, ICSE 2011,\nWaikiki, Honolulu , HI, USA, May 21-28, 2011, Richard N. Taylor, Harald C. Gall, and Nenad Medvidovic (Eds.). ACM,\n111–120. https://doi.org/10.1145/1985793.1985809\n[59] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional Neural Networks over Tree Structures\nfor Programming Language Processing. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,\nFebruary 12-17, 2016, Phoenix, Arizona, USA, Dale Schuurmans and Michael P. Wellman (Eds.). AAAI Press, 1287–1293.\nhttp://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11775\n[60] Liming Nie, He Jiang, Zhilei Ren, Zeyi Sun, and Xiaochen Li. 2016. Query Expansion Based on Crowd Knowledge for\nCode Search. IEEE Trans. Serv. Comput. 9, 5 (2016), 771–783. https://doi.org/10.1109/TSC.2016.2560165\n[61] Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, and Bin Luo. 2022. SPT-Code: Sequence-to-Sequence\nPre-Training for Learning Source Code Representations. In 44th IEEE/ACM 44th International Conference on Software\nEngineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 1–13. https://doi.org/10.1145/3510003.3510096\n[62] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023).\nhttps://doi.org/10.48550/arXiv.2303.08774\narXiv:2303.08774\n[63] Shinwoo Park, Youngwook Kim, and Yo-Sub Han. 2023. Contrastive Learning with Keyword-based Data Augmentation\nfor Code Search and Code Question Answering. In Proceedings of the 17th Conference of the European Chapter of the\nAssociation for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, Andreas Vlachos and Isabelle\nAugenstein (Eds.). Association for Computational Linguistics, 3591–3601. https://aclanthology.org/2023.eacl-main.262\n[64] Kai Petersen, Sairam Vakkalanka, and Ludwik Kuzniarz. 2015. Guidelines for conducting systematic mapping studies\nin software engineering: An update. Inf. Softw. Technol. 64 (2015), 1–18. https://doi.org/10.1016/j.infsof.2015.03.007\n[65] Mohammad Masudur Rahman. 2019. Supporting code search with context-aware, analytics-driven, effective query\nreformulation. In Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings,\nICSE 2019, Montreal, QC, Canada, May 25-31, 2019, Joanne M. Atlee, Tevfik Bultan, and Jon Whittle (Eds.). IEEE / ACM,\n226–229. https://doi.org/10.1109/ICSE-Companion.2019.00088\n[66] Mohammad Masudur Rahman and Chanchal K. Roy. 2018. Effective Reformulation of Query for Code Search\nUsing Crowdsourced Knowledge and Extra-Large Data Analytics. In 2018 IEEE International Conference on Software\nMaintenance and Evolution, ICSME 2018, Madrid, Spain, September 23-29, 2018. IEEE Computer Society, 473–484.\nhttps://doi.org/10.1109/ICSME.2018.00057\n[67] Mohammad Masudur Rahman, Chanchal K. Roy, and David Lo. 2019. Automatic query reformulation for code search\nusing crowdsourced knowledge. Empir. Softw. Eng. 24, 4 (2019), 1869–1924. https://doi.org/10.1007/s10664-018-9671-0\n[68] Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish Chandra. 2018. Retrieval on source\ncode: a neural code search. In Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and\nProgramming Languages, MAPL@PLDI 2018, Philadelphia, PA, USA, June 18-22, 2018, Justin Gottschlich and Alvin\nCheung (Eds.). ACM, 31–41. https://doi.org/10.1145/3211346.3211353\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:42\nXie et al.\n[69] Pasquale Salza, Christoph Schwizer, Jian Gu, and Harald C. Gall. 2023. On the Effectiveness of Transfer Learning for\nCode Search. IEEE Trans. Software Eng. 49, 4 (2023), 1804–1822. https://doi.org/10.1109/TSE.2022.3192755\n[70] Yucen Shi, Ying Yin, Zhengkui Wang, David Lo, Tao Zhang, Xin Xia, Yuhai Zhao, and Bowen Xu. 2022. How to better\nutilize code graphs in semantic code search?. In Proceedings of the 30th ACM Joint European Software Engineering\nConference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November\n14-18, 2022, Abhik Roychoudhury, Cristian Cadar, and Miryung Kim (Eds.). ACM, 722–733. https://doi.org/10.1145/\n3540250.3549087\n[71] Jianhang Shuai, Ling Xu, Chao Liu, Meng Yan, Xin Xia, and Yan Lei. 2020. Improving Code Search with Co-Attentive\nRepresentation Learning. In ICPC ’20: 28th International Conference on Program Comprehension, Seoul, Republic of\nKorea, July 13-15, 2020. ACM, 196–207. https://doi.org/10.1145/3387904.3389269\n[72] Raphael Sirres, Tegawendé F. Bissyandé, Dongsun Kim, David Lo, Jacques Klein, Kisub Kim, and Yves Le Traon.\n2018. Augmenting and structuring user queries to support efficient free-form code search. In Proceedings of the 40th\nInternational Conference on Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018, Michel\nChaudron, Ivica Crnkovic, Marsha Chechik, and Mark Harman (Eds.). ACM, 945. https://doi.org/10.1145/3180155.\n3182513\n[73] Aishwarya Sivaraman, Tianyi Zhang, Guy Van den Broeck, and Miryung Kim. 2019. Active inductive logic pro-\ngramming for code search. In Proceedings of the 41st International Conference on Software Engineering, ICSE 2019,\nMontreal, QC, Canada, May 25-31, 2019, Joanne M. Atlee, Tevfik Bultan, and Jon Whittle (Eds.). IEEE / ACM, 292–303.\nhttps://doi.org/10.1109/ICSE.2019.00044\n[74] Kathryn T. Stolee, Sebastian G. Elbaum, and Daniel Dobos. 2014. Solving the Search for Source Code. ACM Trans.\nSoftw. Eng. Methodol. 23, 3 (2014), 26:1–26:45. https://doi.org/10.1145/2581377\n[75] Weisong Sun, Chunrong Fang, Yuchen Chen, Guanhong Tao, Tingxu Han, and Quanjun Zhang. 2022. Code Search\nbased on Context-aware Code Translation. In 44th IEEE/ACM 44th International Conference on Software Engineering,\nICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 388–400. https://doi.org/10.1145/3510003.3510140\n[76] Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li. 2022. On the Importance of Building High-quality Training\nDatasets for Neural Code Search. In 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022,\nPittsburgh, PA, USA, May 25-27, 2022. ACM, 1609–1620. https://doi.org/10.1145/3510003.3510160\n[77] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In Advances\nin Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December\n8-13 2014, Montreal, Quebec, Canada, Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q.\nWeinberger (Eds.). 3104–3112. https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-\nAbstract.html\n[78] Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and Philip S. Yu. 2019. Multi-modal Attention\nNetwork Learning for Semantic Source Code Retrieval. In 34th IEEE/ACM International Conference on Automated\nSoftware Engineering, ASE 2019, San Diego, CA, USA, November 11-15, 2019. IEEE, 13–25. https://doi.org/10.1109/ASE.\n2019.00012\n[79] Yao Wan, Shijie Zhang, Hongyu Zhang, Yulei Sui, Guandong Xu, Dezhong Yao, Hai Jin, and Lichao Sun. 2022. You see\nwhat I want you to see: poisoning vulnerabilities in neural code search. In Proceedings of the 30th ACM Joint European\nSoftware Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, Singapore,\nSingapore, November 14-18, 2022, Abhik Roychoudhury, Cristian Cadar, and Miryung Kim (Eds.). ACM, 1233–1245.\nhttps://doi.org/10.1145/3540250.3549153\n[80] Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jin. 2022. What Do They Capture? - A Structural\nAnalysis of Pre-Trained Language Models for Source Code. In 44th IEEE/ACM 44th International Conference on Software\nEngineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 2377–2388. https://doi.org/10.1145/3510003.3510050\n[81] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S. Yu. 2018. Improving automatic\nsource code summarization via deep reinforcement learning. In Proceedings of the 33rd ACM/IEEE International\nConference on Automated Software Engineering, ASE 2018, Montpellier, France, September 3-7, 2018, Marianne Huchard,\nChristian Kästner, and Gordon Fraser (Eds.). ACM, 397–407. https://doi.org/10.1145/3238147.3238206\n[82] Chaozheng Wang, Zhenhao Nong, Cuiyun Gao, Zongjie Li, Jichuan Zeng, Zhenchang Xing, and Yang Liu. 2022.\nEnriching query semantics for code search with reinforcement learning. Neural Networks 145 (2022), 22–32. https:\n//doi.org/10.1016/j.neunet.2021.09.025\n[83] Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong, and Xiangke Liao. 2022. Bridging Pre-trained\nModels and Downstream Tasks for Source Code Understanding. In 44th IEEE/ACM 44th International Conference\non Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 287–298. https://doi.org/10.1145/\n3510003.3510062\n[84] Wenhua Wang, Yuqun Zhang, Yulei Sui, Yao Wan, Zhou Zhao, Jian Wu, Philip S. Yu, and Guandong Xu. 2022.\nReinforcement-Learning-Guided Source Code Summarization Using Hierarchical Attention. IEEE Trans. Software Eng.\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\nSurvey of Code Search Based on Deep Learning\n1:43\n48, 2 (2022), 102–119. https://doi.org/10.1109/TSE.2020.2979701\n[85] Wenhua Wang, Yuqun Zhang, Zhengran Zeng, and Guandong Xu. 2020. TranSˆ3: A Transformer-based Framework\nfor Unifying Code Summarization and Code Search. CoRR abs/2003.03238 (2020). arXiv:2003.03238 https://arxiv.org/\nabs/2003.03238\n[86] Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, and Xin Jiang. 2021.\nSyncobert: Syntax-guided multi-modal contrastive pre-training for code representation. arXiv preprint arXiv:2108.04556\n(2021).\n[87] Xin Wang, Yasheng Wang, Yao Wan, Jiawei Wang, Pingyi Zhou, Li Li, Hao Wu, and Jin Liu. 2022. CODE-MVP:\nLearning to Represent Source Code from Multiple Views with Contrastive Pre-Training. In Findings of the Association\nfor Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022, Marine Carpuat, Marie-\nCatherine de Marneffe, and Iván Vladimir Meza Ruíz (Eds.). Association for Computational Linguistics, 1066–1077.\nhttps://doi.org/10.18653/v1/2022.findings-naacl.80\n[88] Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained\nEncoder-Decoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for\nComputational Linguistics, 8696–8708. https://doi.org/10.18653/v1/2021.emnlp-main.685\n[89] Cody Watson, Nathan Cooper, David Nader-Palacio, Kevin Moran, and Denys Poshyvanyk. 2022. A Systematic\nLiterature Review on the Use of Deep Learning in Software Engineering Research. ACM Trans. Softw. Eng. Methodol.\n31, 2 (2022), 32:1–32:58. https://doi.org/10.1145/3485275\n[90] Huaiguang Wu and Yang Yang. 2019. Code Search Based on Alteration Intent. IEEE Access 7 (2019), 56796–56802.\nhttps://doi.org/10.1109/ACCESS.2019.2913560\n[91] Ling Xu, Huanhuan Yang, Chao Liu, Jianhang Shuai, Meng Yan, Yan Lei, and Zhou Xu. 2021. Two-Stage Attention-\nBased Model for Code Search with Textual and Structural Features. In 28th IEEE International Conference on Software\nAnalysis, Evolution and Reengineering, SANER 2021, Honolulu, HI, USA, March 9-12, 2021. IEEE, 342–353.\nhttps:\n//doi.org/10.1109/SANER50967.2021.00039\n[92] Shuhan Yan, Hang Yu, Yuting Chen, Beijun Shen, and Lingxiao Jiang. 2020. Are the Code Snippets What We Are\nSearching for? A Benchmark and an Empirical Study on Code Search with Natural-Language Queries. In 27th IEEE\nInternational Conference on Software Analysis, Evolution and Reengineering, SANER 2020, London, ON, Canada, February\n18-21, 2020, Kostas Kontogiannis, Foutse Khomh, Alexander Chatzigeorgiou, Marios-Eleftherios Fokaefs, and Minghui\nZhou (Eds.). IEEE, 344–354. https://doi.org/10.1109/SANER48275.2020.9054840\n[93] Zhen Yang, Jacky Keung, Xiao Yu, Xiaodong Gu, Zhengyuan Wei, Xiaoxue Ma, and Miao Zhang. 2021. A Multi-Modal\nTransformer-based Code Summarization Approach for Smart Contracts. In 29th IEEE/ACM International Conference\non Program Comprehension, ICPC 2021, Madrid, Spain, May 20-21, 2021. IEEE, 1–12. https://doi.org/10.1109/ICPC52881.\n2021.00010\n[94] Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. 2019. CoaCor: Code Annotation for Code Retrieval with\nReinforcement Learning. In The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019,\nLing Liu, Ryen W. White, Amin Mantrach, Fabrizio Silvestri, Julian J. McAuley, Ricardo Baeza-Yates, and Leila Zia\n(Eds.). ACM, 2203–2214. https://doi.org/10.1145/3308558.3313632\n[95] Ziyu Yao, Daniel S. Weld, Wei-Peng Chen, and Huan Sun. 2018. StaQC: A Systematically Mined Question-Code\nDataset from Stack Overflow. In Proceedings of the 2018 World Wide Web Conference on World Wide Web, WWW 2018,\nLyon, France, April 23-27, 2018, Pierre-Antoine Champin, Fabien Gandon, Mounia Lalmas, and Panagiotis G. Ipeirotis\n(Eds.). ACM, 1693–1703. https://doi.org/10.1145/3178876.3186081\n[96] Wei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xiaoyin Wang, and Shikun Zhang. 2020. Leveraging Code Generation\nto Improve Code Retrieval and Summarization via Dual Learning. In WWW ’20: The Web Conference 2020, Taipei,\nTaiwan, April 20-24, 2020, Yennun Huang, Irwin King, Tie-Yan Liu, and Maarten van Steen (Eds.). ACM / IW3C2,\n2309–2319. https://doi.org/10.1145/3366423.3380295\n[97] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned\ncode and natural language pairs from stack overflow. In Proceedings of the 15th International Conference on Mining\nSoftware Repositories, MSR 2018, Gothenburg, Sweden, May 28-29, 2018, Andy Zaidman, Yasutaka Kamei, and Emily\nHill (Eds.). ACM, 476–486. https://doi.org/10.1145/3196398.3196408\n[98] Chen Zeng, Yue Yu, Shanshan Li, Xin Xia, Zhiming Wang, Mingyang Geng, Linxiao Bai, Wei Dong, and Xiangke Liao.\n2023. deGraphCS: Embedding Variable-based Flow Graph for Neural Code Search. ACM Trans. Softw. Eng. Methodol.\n32, 2 (2023), 34:1–34:27. https://doi.org/10.1145/3546066\n[99] Feng Zhang, Haoran Niu, Iman Keivanloo, and Ying Zou. 2018. Expanding Queries for Code Search Using Semantically\nRelated API Class-names. IEEE Transactions on Software Engineering 44, 11 (2018), 1070–1082. https://doi.org/10.\n1109/TSE.2017.2750682\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n1:44\nXie et al.\n[100] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2020. PEGASUS: Pre-training with Extracted Gap-\nsentences for Abstractive Summarization. In Proceedings of the 37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119). PMLR, 11328–11339.\nhttp://proceedings.mlr.press/v119/zhang20ae.html\n[101] Jie Zhao and Huan Sun. 2020.\nAdversarial Training for Code Retrieval with Question-Description Relevance\nRegularization. In Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November\n2020 (Findings of ACL, Vol. EMNLP 2020), Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational\nLinguistics, 4049–4059. https://doi.org/10.18653/v1/2020.findings-emnlp.361\n[102] Ming Zhu, Karthik Suresh, and Chandan K. Reddy. 2022.\nMultilingual Code Snippets Training for Program\nTranslation. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on\nInnovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022. AAAI Press, 11783–11790.\nhttps:\n//ojs.aaai.org/index.php/AAAI/article/view/21434\n[103] Qihao Zhu, Zeyu Sun, Xiran Liang, Yingfei Xiong, and Lu Zhang. 2020. OCoR: An Overlapping-Aware Code Retriever.\nIn 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020, Melbourne, Australia,\nSeptember 21-25, 2020. IEEE, 883–894. https://doi.org/10.1145/3324884.3416530\nACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: January 2023.\n",
  "categories": [
    "cs.SE",
    "cs.PL"
  ],
  "published": "2023-05-10",
  "updated": "2023-12-13"
}