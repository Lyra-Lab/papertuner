{
  "id": "http://arxiv.org/abs/2305.06137v3",
  "title": "A proof of convergence of inverse reinforcement learning for multi-objective optimization",
  "authors": [
    "Akira Kitaoka",
    "Riki Eto"
  ],
  "abstract": "We show the convergence of Wasserstein inverse reinforcement learning for\nmulti-objective optimizations with the projective subgradient method by\nformulating an inverse problem of the multi-objective optimization problem. In\naddition, we prove convergence of inverse reinforcement learning (maximum\nentropy inverse reinforcement learning, guided cost learning) with gradient\ndescent and the projective subgradient method.",
  "text": "arXiv:2305.06137v3  [cs.LG]  18 May 2023\nA proof of convergence\nof inverse reinforcement learning\nfor multi-objective optimization\nAkira Kitaoka\nNEC Corporation\nakira-kitaoka@nec.com\nRiki Eto\nNEC Corporation\nriki.eto@nec.com\nAbstract\nWe show the convergence of Wasserstein inverse reinforcement learning for multi-\nobjective optimizations with the projective subgradient method by formulating an\ninverse problem of the multi-objective optimization problem.\nIn addition, we prove convergence of inverse reinforcement learning (maximum\nentropy inverse reinforcement learning, guided cost learning) with gradient de-\nscent and the projective subgradient method.\n1\nIntroduction\nArtiﬁcial intelligence (AI) has been used to automate various tasks recently. Generally, automa-\ntion by AI is achieved by setting an index of goodness or badness (reward function) of a target\ntask and having AI automatically search for a decision, that is, an optimal solution in mathematical\noptimization that maximizes or minimizes the index. For example, in work shift scheduling (e.g.\n[CLLR03, GLLK79]), which is a type of combinatorial optimization or multi-objective optimiza-\ntion, we can create shifts that reﬂect our viewpoints by calculating the optimal solution of a reward\nfunction that reﬂects our intentions for several viewpoints, such as “degree of reﬂection of vacation\nrequests,” “leveling of workload,” and “personnel training,” and so on while preserving the required\nnumber of workers, required skills, labor rules. However, setting the reward function, i.e., \"what\nis optimal?\", manually requires a lot of trial-and-error, which is a challenge for the actual applica-\ntion of mathematical optimization. Creating a system that can solve this problem automatically is\nessential in freeing the user from manually designing the reward function.\nInverse reinforcement learning (IRL) [Rus98,NR00] is generally known as facilitating the setting of\nthe reward function. In IRL, a reward function that reﬂects expert’s intention is generated by learning\nexpert’s trajectories, iterating optimization using the reward function, and updating the parameters\nof the reward function. In IRLs which is fomulated by Ng and Russell [NR00], and Abbeel and Ng\n[AN04], in multi-objective optimization, the space of actions, i.e., the space of optimization results,\nis enormous. In other words, it is necessary to set the reward function for the space of actions and\nstates, which is computationally expensive.\nMaximum entropy IRL (MEIRL) [ZMBD08] and guided cost learning (GCL) [FLA16] are methods\nto adapt IRL to multi-objective optimization problems. However, these methods have their issues.\nFor example, MEIRL requires the sum of the reward functions for all trajectories to be computed.\nThis makes maximum entropy IRL computationally expensive. On the other hand, GCL approxi-\nmates the sum of the reward functions for all trajectories by importance sampling. However, since\nmulti-objective optimization problems take discrete values, it is difﬁcult to ﬁnd the probability dis-\ntribution corresponding to a given value when a speciﬁc value is input. One reason for this difﬁculty\nis that in multi-objective optimization problems, even a small change in the value of the reward\nfunction may result in a large change in the result.\nPreprint.\nEto proposed IRL for multi-objective optimization, Wasserstein inverse reinforcement learning\n(WIRL) [Eto22], inspired by Wasserstein generative adversarial networks (WGAN) [ACB17]. Ex-\nperiments have conﬁrmed that WIRL can stably perform inverse reinforcement learning in multi-\nobjective optimization problems.\nHowever, there was no theoretical way to guarantee that WIRL would converge. Eto proposed\nthat we do inverse reinforcement learning for multi-objective optimizations with WIRL [Eto22],\nalthough there was no way to conﬁrm this theoretically. Also, there exists an example in which\nWGAN does not converge [Mes18]. Therefore, WIRL is not expected to converge in general.\nThe inverse reinforcement learning algorithm formulated by Ng and Russell [NR00] is guaranteed\nto converge to the optimal solution because of the linear programming. The inverse reinforcement\nlearning algorithm formulated by Abeell and Ng [AN04] is guaranteed to converge the learner’s\nvalue function to the expert’s value function. However, the convergence of MEIRL and GCL was\nnot proven.\nIn this paper, we show the convergence of various inverse reinforcement learning for multi-objective\noptimizations with the projective subgradient method. In particular, We show the convergence of\nWasserstein inverse reinforcement learning (WIRL) for multi-objective optimizations with the pro-\njective subgradient method by formulating an inverse problem of the optimization problem that is\nequivalent to WIRL for multi-objective optimizations. In §2, we recall the deﬁnition of WIRL. In\n§3, we adapt the deﬁnition of WIRL to multi-objective optimizations. In §4, we formulate an inverse\nproblem of an multi-objective optimization problem that is equivalent to the intention learning and\nshow the convergence of WIRL to adapt the inverse problem to the projected subgradient method.\nIn §5, we see examples of WIRL applied to linear and quadratic programming. In §6, with the\nprojected subgradient method, we show the convergence of MEIRL and GCL.\n2\nWasserstein inverse reinforcement learning\nLet H, HS be inner product spaces, S ⊂HS be a space of states A ⊂H be a space of actions,\nT := Q\nk (S × A) be a space of trajectories. Let Θ ⊂H, and we call Θ a space of feature vectors.\nLet Φ ⊂H, and we call Φ a space of parameters of learner’s trajectories. Let f• : T →Θ be\n1-Lipschitz, and we call f• the feature map. For any Lipschitz function rθ : T →R, the norm of\nLipschitz ∥rθ∥L is deﬁned by\n∥rθ∥L := sup\nτ1̸=τ2\n|rθ(τ1) −rθ(τ2)|\n∥τ1 −τ2∥\n.\nLet δx be the Delta function at x. Let {τ(n)\nE }\nN\nn=1 be the data of expert’s trajectories, and we deﬁne\nthe distribution of expert’s trajectories by\nPE := 1\nN\nN\nX\nn=1\nδτ (n)\nE .\nWith the initial state s(n)\nini of expert’s trajectories τ (n)\nE , and the generator g•(•): Φ × S →T , we\ndeﬁne the distribution of learner’s trajectories by\nPφ := 1\nN\nN\nX\nn=1\nδgφ(s(n)\nini ).\nThe Wasserstein distance between the distribution PE of expert’s trajectories and that Pφ of learner’s\ntrajectories is, with the Kantrovich-Rubinstein duality (c.f. [Vil09]),\nW(PE, Pφ) =\nsup\n∥rθ∥L≤1\n(\n1\nN\nN\nX\nn=1\nrθ(τ (n)\nE ) −1\nN\nN\nX\nn=1\nrθ(gφ(s(n)\nini ))\n)\n,\nwhere rθ is 1-Lipschitz function.\nWe are interested in ﬁnding φ ∈Φ satisfying the following problem:\narg min\nφ∈Φ\nW(PE, Pφ).\n(2.1)\n2\nWith\n{rθ(τ) := θ⊺fτ | θ ∈Θ} insted of {∥rθ∥L ≤1},\n(2.2)\nto ﬁnd φ ∈Φ satisfying equation (2.1) can be roughly replaced by ﬁnding\narg min\nφ∈Φ\nsup\nθ∈Θ\n(\n1\nN\nN\nX\nn=1\nθ⊺fτ (n)\nE\n−1\nN\nN\nX\nn=1\nθ⊺fgφ(s(n)\nini )\n)\n.\n(2.3)\nBy changing the sign, we may consider solving\narg max\nφ∈Φ\ninf\nθ∈Θ\n(\n1\nN\nN\nX\nn=1\nθ⊺fgφ(s(n)\nini ) −1\nN\nN\nX\nn=1\nθ⊺fτ (n)\nE\n)\n.\n(2.4)\nThe IRL that solves equation (2.3) or equation (2.4), is called Wasserstein inverse reinforcement\nlearning (WIRL) [Eto22].\nRemark 2.1. In this paper, learning to maximize the reward function of a history-dependent policy\nis called reinforcement learning. Learning that minimizes the score between the reward function\ncalculated from the expert’s trajectory and the reward function learned by reinforcement learning is\ncalled inverse reinforcement learning.\n3\nWIRL for multi-objective optimization\nWe adapt WIRL to multi-objective optimization (e.g. linear programming, quadratic programming).\nLet φ ∈Φ, H′ be an inner product space, A′ be a set such that A′ ⊂H′, h: A′ →H be a continuous\nfunction. Let X(s) be a compact set1 in A′ for s ∈S. We set the space of trajectories T = S × A.\nThen, multi-objective optimization (e.g. [MIT96,Gun18]) is to solve for the following optimization:\na(φ, s) ∈\narg max\nh(x)∈h(X(s))\nφ⊺h(x).\n(3.1)\nWe call the solution or the learner’s action a(φ, s) ∈A the solver.\nWe set the feature map f = ProjA, where ProjA : T →A is the projection from T to A. We deﬁne\nthe generator gφ(s) by the following optimization problem:\ngφ(s) := (s, a(φ, s)).\nWe say that intention learning of WIRL is WIRL in the above setting.\nThe expert’s action a(n) is assumed to follow an optimal solution. Namely, we often run WIRL\nintention learning by assuming that there exists some φ0 ∈Φ and that for any n we can write\na(n) = a(φ0, s(n)).\n4\nA proof of convergence theorem of intention learning with the projected\nsubgradient method\nWe explain the projected subgradient method according to [BXM03].\nDeﬁnition 4.1. Let H′ be an inner product space, ℓ: H′ →R be a convex funciton, and C ⊂H′ be\na closed convex set.\nThen, we say projected subgradient method to minimize ℓon C is the method to calculate a\nsequence {φbest\nk\n}k ⊂H′ such that for any positive integer K,\nφbest\nK\n∈\narg min\nφk∈{φk}K\nk=1\nℓ(φk),\nφk+1 = ProjC(φk −αkgk),\nwhere gk is the subgradient2 of ℓat φk , the sequence {αk} ⊂R>0 be a learning rate and ProjC it\nthe projection onto C.\n1If A′ is in the Euclid space, compact sets are bounded closed sets.\n2An element gk ∈H′ is a subgradient of ℓat φk if and only if for arbitrary φ ∈H′,\nℓ(φ) ≥ℓ(φk) + (gk, φ −φk).\n3\nUnder the appropriate conditions, the projected subgradient method falls within an error ε > 0 of\nthe minimum of the function ℓin a ﬁnite number of iterations.\nProposition 4.2. (c.f. [BXM03, §3])\n(1) Let H′ be an inner product space, ℓ: H′ →R be the convex function which satisﬁes the Lips-\nchitz condition, which means there exists G > 0 such that for φ, φ′ ∈H′\n|ℓ(φ) −ℓ(φ′)| ≤G∥φ −φ′∥\nand C ⊂H′ be the closed convex set. Let {αk}k ⊂R>0 be a learning rate. We assume\nthat there exists φ∗∈arg minφ∈C ℓ(φ).\nThen, the sequence {φbest\nk\n}k which is calculated by the projected subgradient method to\nminimize ℓon C satisﬁes\nℓ(φbest\nk\n) −ℓ(φ∗) ≤d\n\u0000φ1, arg minφ∈C ℓ(φ)\n\u00012 + G2 Pk\ni=1 α2\ni\n2 Pk\ni=1 αi\n,\nwhere d is the metric induced by the inner product of H′.\n(2) We use the same notation H′, ℓ, C as (1). Let {αk}k be a nonsummable diminishing learning\nrate, that is,\nlim\nk→∞αk = 0,\n∞\nX\nk=1\nαk = ∞.\nLet ε > 0. Let K1 be the integer such that for all k > K1, αk ≤ε/G2, K2 be the integer\nsuch that for all k > K2,\nk\nX\ni=1\nαi > 1\nε\n\nd\n \nφ1, arg min\nφ∈C\nℓ(φ)\n!2\n+ G2\nK1\nX\ni=1\nα2\ni\n\n\nand K = max{K1, K2}. Then, for k > K, we have\nℓ(φbest\nk\n) −ℓ(φ∗) < ε.\nProposition 4.2 (2) means that for any error ε > 0, the sequence {φbest\nk\n} which is calculated by the\nprojected subgradient method to minimize ℓon C falls within the error ε > 0 of the minimum of ℓ\nin a ﬁnite number of iterations.\nRemark 4.3. In [BXM03, §3], they showed Proposition 4.2 on Rd. Generally, by replacing the\nargument in [BXM03, §3] with an inner product space H instead of Rd, we can show Proposition 4.2.\nWe give the following inverse problem of optimization problem that is equivalent to the problem\nhandled by intention learning of WIRL.\nDeﬁnition 4.4. Let H, HS, H′ be inner product spaces, S ⊂HS be a space of state, A′ ⊂H′,\nΦ ⊂H be a closed convex set, h: A′ →H be the continuous function, X(s) ⊂A′ be a compact\nnon-empty set for s ∈S.\nThen, the inverse problem of multi-objective optimization problem (IMOOP) for the solver a(φ, s)\nand trajectories of an expert {τ (n)\nE\n= (s(n), a(n))}n ⊂HS × H is the problem to ﬁnd φ ∈Φ\nsatisfying\nminimize 1\nN\nN\nX\nn=1\nφ⊺a(φ, s(n)) −1\nN\nN\nX\nn=1\nφ⊺a(n),\nsubject to φ ∈Φ.\n(4.1)\nRemark 4.5. We get the idea of the formulation Deﬁnition 4.4 from the formulation of maximal\nentropy IRL [HE16]. In other words,\n1\nN\nPN\nn=1 φ⊺a(φ, s(n)) is a reward funciton in reinforcement\nlearning.\n4\nProposition 4.6. In the setting of Θ = Φ, equation (4.1) is the replacement of maxφ∈Φ and infθ∈Θ\nin equation (2.4), that is,\nmin\nφ∈Φ\n(\n1\nN\nN\nX\nn=1\nφ⊺a(φ, s(n)) −1\nN\nN\nX\nn=1\nφ⊺a(n)\n)\n= min\nθ∈Φ max\nφ∈Φ\n(\n1\nN\nN\nX\nn=1\nθ⊺a(φ, s(n)) −1\nN\nN\nX\nn=1\nθ⊺a(n)\n)\n.\n(4.2)\nproof. By the deﬁnition of the solver a(φ, s(n)), equation (3.1), for any θ ∈Φ\nφ⊺a(φ, s(n)) ≥φ⊺a(θ, s(n)).\nSince\nφ⊺a(φ, s(n)) ≤max\nθ∈Φ φ⊺a(θ, s(n)),\nwe see\nφ⊺a(φ, s(n)) = max\nθ∈Φ φ⊺a(θ, s(n)).\nTherefore, we obtain\nmin\nθ∈Φ max\nφ∈Φ\n(\n1\nN\nN\nX\nn=1\nθ⊺a(φ, s(n)) −1\nN\nN\nX\nn=1\nθ⊺a(n)\n)\n= min\nθ∈Φ\n(\nmax\nφ∈Φ\n1\nN\nN\nX\nn=1\nθ⊺a(φ, s(n)) −1\nN\nN\nX\nn=1\nθ⊺a(n)\n)\n= min\nφ∈Φ\n(\n1\nN\nN\nX\nn=1\nφ⊺a(φ, s(n)) −1\nN\nN\nX\nn=1\nφ⊺a(n)\n)\n.\nIt follows equation (4.2).\nRemark 4.7. It is not guaranteed that the replacement of maxφ∈Φ and infθ∈Θ in equation (2.4)\ncoincides with equation (2.4).\nBy Proposition 4.6, we can interpret Intention learning (of WIRL) as solving the IMOOP. We can\nalso show that intention learning converges with Proposition 4.2. We explain these.\nWe set\nF(φ) := 1\nN\nN\nX\nn=1\nφ⊺a(φ, s(n)) −1\nN\nN\nX\nn=1\nφ⊺a(n).\nTo adapt Proposition 4.2 to Φ and F, we show the following lemma:\nLemma 4.8. In the setting of Deﬁnition 4.4,\n(1) the function F is convex,\n(2) the fuction F is Lipschitz,\n(3) one of the subgradient of F at φ ∈Φ is 1\nN\nPN\nn=1 a(φ, s(n)) −1\nN\nPN\nn=1 a(n).\nproof. We note that by equation (3.1), for n,\nφ⊺a(φ, s(n)) =\nmax\nx∈X(s(n)) φ⊺h(x).\n(1) Since X(s(n)) is compact and the map h: A′ →H and for φ ∈Φ the map φ⊺: H →R; H ∋\ny 7→φ⊺y are continuous, we note that there exists the maximal of φ⊺h(•) on X(s(n)). Let φ1, φ2 ∈\nΦ. For any x ∈X(s(n)), we see\ntφ⊺\n1h(x) + (1 −t)φ⊺\n2h(x) ≥(tφ1 + (1 −t)φ2)⊺h(x).\n5\nBy applying maxx∈X(s(n)) to the ﬁrst and second terms on the left-hand side, for any x ∈X(s(n)),\nwe obtain\nt\nmax\nx∈X(s(n)) φ⊺\n1h(x) + (1 −t)\nmax\nx∈X(s(n)) φ⊺\n2h(x) ≥(tφ1 + (1 −t)φ2)⊺h(x).\nBy applying maxx∈X(s(n)) to the right-hand side,\nt\nmax\nx∈X(s(n)) φ⊺\n1h(x) + (1 −t)\nmax\nx∈X(s(n)) φ⊺\n2h(x) ≥\nmax\nx∈X(s(n))(tφ1 + (1 −t)φ2)⊺h(x).\nTherefore, Φ ∋φ 7→maxx∈X(s(n)) φ⊺h(x) = φ⊺a(φ, s(n)) is convex function. Since the sum of\nconvex functions is convex, F(φ) is convex.\n(2) For any x ∈X(s(n)), by Cauchy-Schwarz’ inequality,\nφ⊺\n1h(x) −φ⊺\n2h(x) ≤∥φ1 −φ2∥∥h(x)∥.\n(4.3)\nSince X(s(n)) is compact and the map h: A′ →H and the map ∥• ∥: H →R; H ∋y 7→∥y∥are\ncontinuous, we note that there exists the maximal of ∥h(•)∥on X(s(n)). To adapt maxx∈X(s(n)) the\nright-hand side of equation (4.3), for x ∈X(s(n)),\nφ⊺\n1h(x) −φ⊺\n2h(x) ≤∥φ1 −φ2∥\nmax\nx′∈X(s(n)) ∥h(x′)∥.\nTo adapt maxx∈X(s(n)) to the second term of the left-hand side, for x ∈X(s(n)),\nφ⊺\n1h(x) −\nmax\nx′∈X(s(n)) φ⊺\n2h(x′) ≤∥φ1 −φ2∥\nmax\nx′∈X(s(n)) ∥h(x′)∥.\nTo adapt maxx∈X(s(n)) to the ﬁrst term of the left-hand side,\nmax\nx∈X(s(n)) φ⊺\n1h(x) −\nmax\nx∈X(s(n)) φ⊺\n2h(x) ≤∥φ1 −φ2∥\nmax\nx∈X(s(n)) ∥h(x)∥.\nSince the above inequation also holds if we swap φ1, φ2,\n\f\f\f\f\nmax\nx∈X(s(n)) φ⊺\n1h(x) −\nmax\nx∈X(s(n)) φ⊺\n2h(x)\n\f\f\f\f ≤∥φ1 −φ2∥\nmax\nx∈X(s(n)) ∥h(x)∥.\nIt meas that φ 7→maxx∈X(s(n)) φ⊺h(x) = φ⊺a(φ, s(n)) is Lipschitz continuous. Since the sum of\nLipschitz functions is Lipschitz, F(φ) is Lipschitz.\n(3) For any φ1, φ2 ∈Φ, we have\nφ⊺\n2\n\u0010\na(φ1, s(n)) −a(n)\u0011\n= φ⊺\n1\n\u0010\na(φ1, s(n)) −a(n)\u0011\n+ (φ2 −φ1)⊺\u0010\na(φ1, s(n)) −a(n)\u0011\n.\nBy the deﬁnition of the solver a(φ1, s(n)),\nφ⊺\n2\n\u0010\na(φ2, s(n)) −a(n)\u0011\n≥φ⊺\n2\n\u0010\na(φ1, s(n)) −a(n)\u0011\n.\nTherefore, we see\nφ⊺\n2\n\u0010\na(φ2, s(n)) −a(n)\u0011\n≥φ⊺\n1\n\u0010\na(φ1, s(n)) −a(n)\u0011\n+ (φ2 −φ1)⊺\u0010\na(φ1, s(n)) −a(n)\u0011\n.\nTaking the average of both sides for n,\nF(φ2) ≥F(φ1) + (φ2 −φ1)⊺\n \n1\nN\nN\nX\nn=1\na(φ1, s(n)) −1\nN\nN\nX\nn=1\na(n)\n!\n.\nRemark 4.9. Barmann et al. showed Lemma 4.8 (1) and (3) for linear programming [BMPS18,\nProposition 3.1]\nThe algorithm of WIRL for multi-objective optimization is given by Algorithm 1.\n6\nAlgorithm 1 Intention learning (of WIRL)\n1: initialize φ1 ∈Φ\n2: for k = 1, . . . , K −1 do\n3:\nφk+1 ←φk −αk\nN\nPN\nn=1\n\u0000a(φk, s(n)) −a(n)\u0001\n4:\nprojection onto Φ for φk+1\n5: end for\n6: return φbest\nK\n∈arg minφk∈{φk}K\nk=1 F(φk)\nRemark 4.10. In [Eto22], no operation “projection onto Φ for φk” is performed on Algorithm 1.\nWe add this operation on Algorithm 1 to discuss the intention learning with the projection onto Φ in\n§4.\nLemma 4.11. In the setting of Deﬁnition 4.4, the algorithm which solves IMOOP for the solver\na(φ, s) coninsides with Algorithm 1.\nproof. By Deﬁnition 4.1 and Lemma 4.8 (3), the projected subgradient method to minimize F on Φ\ncoincides with Algorithm 1.\nSince Proposition 4.2, Lemmas 4.8 and 4.11, intention learning of WIRL, Algorithm 1 is convergent.\nTheorem 4.12. Let H, HS, H′ be inner product spaces, S ⊂HS, A′ ⊂H′, Φ ⊂H be a closed\nconvex set, h: A′ →H be a continuous function, X(s) ⊂A′ be the compact non-empty set for\ns ∈S. Let {αk}k ⊂R>0 be a nonsummable diminishing learning rate, that is,\nlim\nk→∞αk = 0,\n∞\nX\nk=1\nαk = ∞.\nAssume that there exists the minimum of F on Φ.\nThen, for any ε > 0, the sequence {φbest\nk\n}k which is calculated by intention learning of WIRL for\nany error ε > 0, there exists a positive integer K such that for all k > K,\nF(φbest\nk\n) −min\nφ∈Φ F(φ) < ε.\nIt means that for any error ε > 0, intention learning of WIRL falls within the error ε of the minimum\nof F in a ﬁnite number of iterations.\nRemark 4.13. By Proposition 4.2 (1) if the learning rate {αk}k ⊂R>0 is constant α, Algorithm 1\nfalls within an error\nα\n2N\nN\nX\nn=1\nmax\nx′∈X(s(n)) ∥h(x′) −a(n)∥\nof the minimum of F on Φ in a ﬁnite number of iterations. However, it is not guaranteed that for any\nε > 0 Algorithm 1 falls within the error ε of the minimum of F on Φ in a ﬁnite number of iterations.\nRemark 4.14. [SWN19, Algorithm 2] coincides with Algorithm 1 for 0-1 planning problem, Φ =\nRn and a constant learning rate. However, by Remark 4.13, it is not guaranteed that for any ε > 0\n[SWN19, Algorithm 2] falls within the error ε of the minimum of F in a ﬁnite number of iterations.\nIf we take a learning rate satisfying nonsummable diminishing, it is guaranteed that for any ε > 0\n[SWN19, Algorithm 2] falls within the error ε of the minimum.\n5\nExamples\nExample 5.1 (mixed integer linear programming). We set A = Rd,\nΦ = ∆d,\n∆d :=\n(\nφ = (φ1, . . . , φd) ∈Rd\n\f\f\f\f\f ∀i, φi ≥0,\nd\nX\ni=1\nφi = 1\n)\n,\n7\nand the vector valued function h = Id: Rd →Rd. We assume that X(s) is the ﬁnite direct sum of\nbounded convex polyhedrons for any s ∈S. In the above setting, we can do intention learning of\nWIRL.\nThe action, i.e. solver is given by\na(φ, s) ∈arg max\nx∈X(s)\nφ⊺x.\nIt means that a(φ, s) is a solver of a mixed integer linear programming.\nWhen we do Algorithm 1, we use [WCP13] as the algorithm to implement Proj∆d. Since Φ is\ncompact, there exists the minimum of F on Φ, and we can adapt Theorem 4.12 to this case.\nRemark 5.2. In Example 5.1, the space on we research is\n˜Φ =\n\b\nφ = (φ1, . . . , φd) ∈Rd \f\f ∀i, φi ≥0\n\t\n\\ {(0, . . . , 0)}.\nHowever, it is more suitable to research Φ = ∆d than ˜Φ. There are two reason.\nFirst, there are cases where there is no minimum on ˜Φ. Since for arbitrary γ > 0, φ ∈˜Φ,\nF(γφ) = γF(φ)\n(5.1)\nand\nF(0, . . . , 0) = 0,\nif F(φ) > 0 for any φ ∈˜Φ, there is no minimum on ˜Φ. In addition, If there exists φ ∈˜Φ such that\nF(φ) < 0, by equation (5.1) to take a large enough γ, we can move F(γφ) close to −∞. It means\nthere is no minimum on ˜Φ if there exists φ ∈˜Φ such that F(φ) < 0.\nSecond, it is enough to research Φ. The solver a(φ, s(n)) is invariant under scalar multiplication and\n˜Φ = {cφ | c ∈R>0, φ ∈∆d}. Therefore it is enough to research ∆d, the set of equivalence classes.\nExample 5.3 (mixed integer quadratic programming). We set the space A = Sd × Rd, and Φ =\nΓd × □d(b0), where Sd is the space of symmetric matrixes of order d, Sd\n+ is the space of positive\nsemideﬁnite matrixes of order d, and\nΓd :=\n\b\nA ∈Sd\n+\n\f\f Tr(A) = 1\n\t\n,\n□d(b0) :=\nd\nY\ni=1\n{bi ∈R | −b0 ≤bi ≤b0} for b0 > 0.\nFor any square matrix A of order d, b ∈Rd we set φ = (A, b), and set the vector-valued function\nh: Rd →Sd × Rd by for x ∈Rd,\nφ⊺h(x) := −x⊺Ax −b⊺x.\nWe assume that X(s) is the ﬁnite direct sum of bounded convex polyhedrons for any s ∈S. In the\nabove setting, since Φ is convex, we can do intention learning of WIRL.\nThe action, i.e. solver is given by\na(φ, s) ∈\narg max\nh(x)∈h(X(s))\n(−x⊺Ax −b⊺x).\nIt means a(φ, s) is a solver of concave mixed integer quadratic programming.\nWhen we do Algorithm 1, we use\nProjΦ(A, b) =\n\u0010\nProjΓd(A), Proj□d(b0)(b)\n\u0011\nas the algorithm to implement ProjΦ. Here ProjΓd(A) is given by ProjΓd(A) = Pd\ni=1 µiviv⊺\ni\nwhere A = Pd\ni=1 λiviv⊺\ni is the eigenvalue decomposition of A, λ = (λ1, . . . , λd), and µ =\n(µ1, . . . , µd) = Proj∆d(λ) [Bec17, Example 7.20], and Proj□d(b0)(b) is given by\nProj□d(b0)(b) = (Proj□1(b0)(b1), . . . , Proj□1(b0)(bd)),\nProj□1(b0)(bi) = max{min{bi, b0}, −b0}, i = 1, . . . , d.\nIn the same way as Remark 5.2, Φ does not include the pair of the zero matrix and the zero vector. It\nmeans that Algorithm 1 is not convergenceat this pair. Since Φ is compact, there exists the minimum\nof F on Φ, and we can adapt Theorem 4.12 to this case.\n8\n6\nRelated work\nConvergence of Inverse reinforcement learning\nWe believe that various inverse reinforcement learning convergence properties can be guaranteed\nwith the gradient descent (e.g. [GG23, Theorem 3.4]) for L-smooth fucntion including smooth\nfuncitons and projected subgradient method Proposition 4.2 for Lipschitz functions. Speciﬁcally,\nwe give the following examples.\nIn MEIRL [ZMBD08], the objective function L is convex. In addition, if trajectories is ﬁnite, L is\nsmoooth. We assume that there exists the maximum of L on the space of weight of reward (here\ndenote Θ). For example, if Θ is compact, L has mamimum on Θ. By gradient descent, that is,\n[GG23, Theorem 3.4], we can approximate the maximum reward weight θ in L. This means that the\nconvergence of MEIRL is guaranteed.\nIn relative entropy inverse reinforcement learning (REIRL), the dual objective fucntion g is concave\n[BKP11]. In addition, if trajectories is ﬁnite, g is Lipschitz. We assume that there exists the maxi-\nmum of g on the space of weight of reward (here denote Θ). By projected subgradient method, that\nis, Proposition 4.2, we can approximate the maximum reward weight θ in g. However, since we are\napproximating the gradient of g with respect to θ using weighted sampling, there is room to consider\nwhether convergence of REIRL will actually occur.\nIn GCL [FLA16], the importance sampling of the objective fucntion LIOC (here denote L) is convex\nfor θ. In addition, if trajectories is ﬁnite, L is Lipschitz. We assume that there exists the maximum of\nL on the space of weight of reward (here denote Θ). By gradient descent, that is, [GG23, Theorem\n3.4], we can approximate the maximum reward weight θ in L. This means that the convergence of\nGCL is guaranteed.\nWGAN and WIRL\nWe explain why WGAN does not converge, while WIRL does. In WGAN, in general, parameteriz-\ning the class of 1-Lipschitz functions does not necessarily make the Critic convex. In WIRL, on the\nother hand, parameterizing the class of 1-Lipschitz functions with equation (2.2) and Proposition 4.6\nmake the Critic, i.e. the contents of sup in equation (2.3) convex. In other words, Lemma 4.8 (2)\nfollows.\n7\nConclusion\nWe proved the convergence of WIRL for multi-objective optimizations with the projective subgradi-\nent method by formulating an IMOOP that is equivalent to WIRL for multi-objective optimizations.\nIn other words, we showed WIRL is convergent at the minimum of F on Φ.\nWe raise some future works. We have proved that WIRL converges, whereas it has not been proven\nwhether learner’s trajectories mimic expert’s trajectories when WIRL is convergent. If we show\nthis, we can say that WIRL theoretically converges in the direction that learner’s trajectories imitate\nexpert’s trajectories. This means that WIRL is theoretically guaranteed to have a mechanism that\nfrees users from manually designing the reward function.\nAcknowledgments and Disclosure of Funding\nThe authors would like to thank Kei Takemura for his valuable comments. They would also like to\nthank Shinji Ito for carefully reading this paper.\nReferences\n[ACB17] M. Arjovsky, S. Chintala, and L. Bottou, Wasserstein generative adversarial networks,\n2017, pp. 214–223.\n[AN04] P. Abbeel and A. Y Ng, Apprenticeship learning via inverse reinforcement learning,\nThe 21th International Conference on Machine Learning, 2004, pp. 1.\n9\n[Bec17] A. Beck, First-order methods in optimization, SIAM, 2017.\n[BKP11] A. Boularias, J. Kober, and J. Peters, Relative entropy inverse reinforcement learn-\ning, The 14th International Conference on Artiﬁcial Intelligence and Statistics, 2011,\npp. 182–189.\n[BMPS18] A. Bärmann, A. Martin, S. Pokutta, and O. Schneider, An online-learning approach to\ninverse optimization, 2018. arXiv preprint arXiv:1810.12997.\n[BXM03] S.\nBoyd,\nL.\nXiao,\nand\nA.\nMutapcic,\nSubgradient\nmethods,\n2003.\nhttps://web.stanford.edu/class/ee392o/subgrad_method.pdf.\n[CLLR03] B. Cheang, H. Li, A. Lim, and B. Rodrigues, Nurse rostering problems–a bibliographic\nsurvey, European Journal of Operational Research 151 (2003), no. 3, 447–460.\n[Eto22] R. Eto, Learning device, learning method, and learning program, 2022. Publication\nNumber WO2022/137520, International Application No. PCT/JP2020/048791.\n[FLA16] C. Finn, S. Levine, and P. Abbeel, Guided cost learning: Deep inverse optimal control\nvia policy optimization, The 33rd International Conference on Machine Learning, 2016,\npp. 49–58.\n[GG23] G. Garrigos and R. M Gower, Handbook of convergence theorems for (stochastic) gra-\ndient methods, 2023. Available at https://arxiv.org/abs/2301.11235.\n[GLLK79] R. L. Graham, E. L. Lawler, J. K. Lenstra, and A. R. Kan, Optimization and approxi-\nmation in deterministic sequencing and scheduling: a survey, Annals of Discrete Math-\nematics, 1979, pp. 287–326.\n[Gun18] N. Gunantara, A review of multi-objective optimization: Methods and its applications,\nCogent Engineering 5 (2018), no. 1, 1502242.\n[HE16] J. Ho and S. Ermon, Generative adversarial imitation learning, The 30th Conference\non Neural Information Processing Systems, 2016, pp. 4565–4573.\n[Mes18] L. Mescheder, Which training methods for GANs do actually converge?, 2018,\npp. 3481–3490.\n[MIT96] T. Murata, H. Ishibuchi, and H. Tanaka, Multi-objective genetic algorithm and its appli-\ncations to ﬂowshop scheduling, Computers & Industrial Engineering 30 (1996), no. 4,\n957–968.\n[NR00] A. Y Ng and S. Russell, Algorithms for inverse reinforcement learning, The 17th Inter-\nnational Conference on Machine Learning, 2000, pp. 663–670.\n[Rus98] S. Russell, Learning agents for uncertain environments, The 11th annual conference on\nComputational Learning Theory, 1998, pp. 101–103.\n[SWN19] Y. Suzuki, W. M Wee, and I. Nishioka, TV advertisement scheduling by learning expert\nintentions, The 25th ACM SIGKDD International Conference on Knowledge Discovery\n& Data Mining, 2019, pp. 3071–3081.\n[Vil09] C. Villani, Optimal transport: old and new, Vol. 338, Springer, 2009.\n[WCP13] W. Wang and M. A Carreira-Perpinán, Projection onto the probability simplex: An efﬁ-\ncient algorithm with a simple proof, and an application, 2013.\n[ZMBD08] B. D Ziebart, A. L Maas, J A. Bagnell, and A. K Dey, Maximum entropy inverse\nreinforcement learning, The 23rd AAAI Conference on Artiﬁcial Intelligence, 2008,\npp. 1433–1438.\n10\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2023-05-10",
  "updated": "2023-05-18"
}