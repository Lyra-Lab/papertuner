{
  "id": "http://arxiv.org/abs/2305.06530v1",
  "title": "How Good are Commercial Large Language Models on African Languages?",
  "authors": [
    "Jessica Ojo",
    "Kelechi Ogueji"
  ],
  "abstract": "Recent advancements in Natural Language Processing (NLP) has led to the\nproliferation of large pretrained language models. These models have been shown\nto yield good performance, using in-context learning, even on unseen tasks and\nlanguages. They have also been exposed as commercial APIs as a form of\nlanguage-model-as-a-service, with great adoption. However, their performance on\nAfrican languages is largely unknown. We present a preliminary analysis of\ncommercial large language models on two tasks (machine translation and text\nclassification) across eight African languages, spanning different language\nfamilies and geographical areas. Our results suggest that commercial language\nmodels produce below-par performance on African languages. We also find that\nthey perform better on text classification than machine translation. In\ngeneral, our findings present a call-to-action to ensure African languages are\nwell represented in commercial large language models, given their growing\npopularity.",
  "text": "arXiv:2305.06530v1  [cs.CL]  11 May 2023\nAfricaNLP workshop at ICLR2022\nHOW GOOD ARE COMMERCIAL LARGE LANGUAGE\nMODELS ON AFRICAN LANGUAGES?\nJessica Ojo\nMasakhane\njessicaojo19@gmail.com\nKelechi Ogueji\nMasakhane\nkelechi.ogueji@uwaterloo.ca\nABSTRACT\nRecent advancements in Natural Language Processing (NLP) has led to the pro-\nliferation of large pretrained language models. These models have been shown to\nyield good performance, using in-context learning, even on unseen tasks and lan-\nguages. They have also been exposed as commercial APIs as a form of language-\nmodel-as-a-service, with great adoption. However, their performance on African\nlanguages is largely unknown. We present a preliminary analysis of commercial\nlarge language models on two tasks (machine translation and text classiﬁcation)\nacross eight African languages, spanning different language families and geo-\ngraphical areas. Our results suggest that commercial language models produce\nbelow-par performance on African languages. We also ﬁnd that they perform bet-\nter on text classiﬁcation than machine translation. In general, our ﬁndings present\na call-to-action to ensure African languages are well represented in commercial\nlarge language models, given their growing popularity.\n1\nINTRODUCTION\nLarge language models have risen to the fore of Natural Language Processing (NLP). These models\nhave been shown to achieve state-of-the-art performances on several tasks. More recently, focus has\nshifted from the pretrain-ﬁnetune paradigm (Howard & Ruder, 2018; Devlin et al., 2019; Liu et al.,\n2019; Raffel et al., 2020) to in-context learning (Brown et al., 2020; Lin et al., 2021; Wei et al.,\n2022a; Chowdhery et al., 2022; Chung et al., 2022; Sanh et al., 2022; Dong et al., 2023). In-context\nlearning proves that prompting large language models with some task-speciﬁc examples allows them\nperform well on test examples of that task, all without updating the model’s parameters. This has\nled to reduced computation costs and has made it possible to create language-models-as-a-service\n(Sun et al., 2022), in the form of commercial Application Programming Interfaces (APIs). Com-\nmercial language models have become very prevalent. For context, the recently released ChatGPT1\namassed 100 million users2 in two months, making it the fastest growing consumer app in recent\nhistory. Given their dominance and inevitable continual rise, it is important to understand how these\nmodels perform on African languages. Hence, we present a preliminary effort to close this gap by\nevaluating two commercial large language models using in-context learning on African languages.\nEvaluation is performed on two tasks - text classiﬁcation and machine translation. Our experiments,\nspanning 8 African languages from different language families and geographical locations, suggests\nthat commercial language models do not perform well on African languages. In particular, we note\na large disparity in performance, depending on the evaluation task - models perform better on text\nclassiﬁcation than machine translation. Our work sheds light on the need to ensure the inclusion\nof African languages in the development of commercial language models, given their inevitable\nadoption in our daily lives.\n1https://chat.openai.com/\n2https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-faste\n1\nAfricaNLP workshop at ICLR2022\n2\nRELATED WORK\n2.1\nIN-CONTEXT LEARNING\nThe use of pretrained language models has become the de-facto approach to solving natural language\nprocessing (NLP) tasks. Previous models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2019) and T5 (Raffel et al., 2020) largely follow a pretrain-ﬁnetune setting (Howard & Ruder,\n2018). In this method, the pretrained model is ﬁnetuned on a downstream task, such as text classi-\nﬁcation, and then used for that task. While this works very well, it has several downsides. For one,\nﬁnetuned models are usually task-speciﬁc and this means one has to maintain separate models for\nseparate tasks. Furthermore, the growing size of pretrained language models (Kaplan et al., 2020)\nmeans that it is becoming increasingly expensive to ﬁnetune such gigantic models. One solution\nthat has proven popular in recent times is in-context learning (Brown et al., 2020; Schick & Sch¨utze,\n2021; Wei et al., 2022a; Chowdhery et al., 2022; Chung et al., 2022; Sanh et al., 2022; Dong et al.,\n2023). The core idea behind this method is to enable pretrained language models learn from ex-\namples within the context. In this setting, a user prompts a pretrained language model with a few\nlabelled examples of a task following a speciﬁc pattern, and unlabelled examples that need to be pre-\ndicted on (Wei et al., 2022c; Liu et al., 2022; Wei et al., 2022b). In-context learning can also work\nin a zero-shot setting where no labelled examples are included in the prompt. In-context learning\nworks surprisingly well and is very efﬁcient since there is no update to the model’s parameters. As\na result, computation costs are signiﬁcantly reduced and it becomes possible to expose language\nmodels as a service (Sun et al., 2022). Commercial APIs are heavily reliant on in-context learning\nas this is the primary method through which users interact 3 with the models4.\n2.2\nMULTILINGUAL IN-CONTEXT LEARNING\nLarge language models have proven successful in multilingual settings.\nLin et al. (2021) train\nseveral multilingual models, of which the largest one (7.5B parameters) sets a state-of-the-art in\nfew-shot learning on more than 20 languages. Their model outperforms GPT3 on several mul-\ntilingual tasks. Muennighoff et al. (2022) perform multitask prompted ﬁnetuning on multilingual\npretrained language models and observe impressive zero-shot generalization to tasks in unseen lan-\nguages. Following ﬁndings from Blevins & Zettlemoyer (2022) that non-English dataset present in\nthe pretraining corpora of English language models explains their surprising cross-lingual ability,\nChowdhery et al. (2022) deliberately introduce non-English corpora (≈22%) into the pretraining\ncorpora of their PaLM model and achieve impressive few-shot multilingual performance. Shi et al.\n(2022) evaluate GPT3 and PaLM on a newly introduced grade school mathematics multilingual\nbenchmark. They ﬁnd that using prompts with intermediate reasoning steps in English consistently\nled to competitive or better results than those written in the native language of the question. They\nalso set a new state-of-the-art on a common-sense reasononing multilingual benchmark, XCOPA\n(Ponti et al., 2020), using few-shot examples. Zhao & Sch¨utze (2021) show that prompting yields\nbetter cross-lingual transfer in few-shot settings than ﬁnetuning and in-language training of multilin-\ngual natural language inference. Furthermore, Winata et al. (2021) evaluate the multilingual ability\nof GPT (Radford et al., 2019) and T5 (Raffel et al., 2020) models on multi-class text classiﬁcation,\nand ﬁnd that they work well on non-English languages given a few English examples. Concurrent\nwork (Jiao et al., 2023) evaluate ChatGPT on machine translation and ﬁnd that, while it is compet-\nitive with other commercial translation APIs such as Google translate5, it is less robust on other\ndomains such as biomedical. Another concurrent work (Zhang et al., 2023) conducts a study on\nthe performance of GLM (Zeng et al., 2022) on machine translation. They note several interesting\nﬁndings on the effect of prompt template, examples and language. Despite the plethora of works\non multilingual prompting, little to no African languages are usually contained in the evaluation\nsets of nearly all of these works. When present, they are often obtained by translating the existing\ndatasets of other languages (Yu et al., 2022) This method has been shown to contain artifacts that\ncan inﬂate the performance of models evaluated on such datasets (Artetxe et al., 2020). Our work is\northogonal to all of this works because we focus solely on commercial language model APIs, given\ntheir prevalence. The closest to our work is concurrent by Abott et al. (2023), who evaluate GPT\n3https://platform.openai.com/docs/guides/completion/prompt-design\n4https://docs.cohere.ai/docs/prompt-engineering\n5https://translate.google.com/\n2\nAfricaNLP workshop at ICLR2022\n3.5 on Named Entity Recognition and Machine Translation on only isiZulu. However, our work is\ndifferent from this as we compare two commercial APIs in the evaluation of text classiﬁcation and\nMachine Translation across 8 African language.\n3\nMETHODOLOGY\n3.1\nDATASETS\nEvaluation is done on two tasks - text classiﬁcation and machine translation.\n3.1.1\nTEXT CLASSIFICATION\nWe use the news topic classiﬁcation datasets from Hedderich et al. (2020) and Alabi et al. (2022).\nWe select the Hausa (hau) language from Hedderich et al. (2020) which has 5 categories. Pretrained\nlanguage models have been shown to work very well on this dataset in both few and zero-shot\nsettings. The dataset from Alabi et al. (2022) covers ﬁve languages, out of which we select four\n- Nigerian Pidgin (pcm), Malagasay (mlg), and Somali (som), isiZulu (zul). Each language has 5\ncategories, except Somali which has 6. For both datasets, we use the train, validation and test splits\nas released by the authors. We select these languages because they cover different language families\nand geographical areas.\n3.1.2\nMACHINE TRANSLATION\nWe use the MAFAND-MT machine translation dataset from Adelani et al. (2022) which covers 16\nAfrican languages. Running translation on commercial APIs is cumbersome and expensive, hence\nwe select 5 languages from the 16. The ﬁve languages are isiZulu (zul), Yoruba (yor), Nigerian\nPidgin (pcm), Swahili (Swa) and Lugala (lug). We use the splits as released by the authors.\n3.2\nMODELS\nTwo commercial APIs6 are considered: ChatGPT7 and Cohere8. We consider both these APIs be-\ncause they are arguably the most popular ones9. ChatGPT is based on the Instruct-GPT models\n(Ouyang et al., 2022). It is optimized for conversations and has been shown to be capable of sev-\neral NLP tasks including text classiﬁcation, machine translation, question answering, and so on.\nWe use Cohere’s multilingual model10 which is based on their multilingual embedding model11.\nThe embedding model supports 100 languages, including 15 African languages. All the languages\nwe consider, except Nigerian Pidgin, are supported by the model. However, given the linguistic\nproximity of Nigerian Pidgin to English (Faraclas, 2008; Ogueji & Ahia, 2019; Chang et al., 2020;\nAhia & Ogueji, 2020a; Lent et al., 2021; 2022), the model should be able to perform well on the\ndataset.\n3.3\nPROMPTING AND EVALUATION\nWe describe our prompting and evaluation approaches for text classiﬁcation and machine translation.\n3.3.1\nTEXT CLASSIFICATION\nFor Cohere, we use the Classify12 endpoint and follow the format speciﬁed in the API documenta-\ntion13. When using ChatGPT, we design several prompts ourselves and we also ask ChatGPT for\n6Experiments were run between January 22, 2023 and February 5, 2023.\n7https://chat.openai.com/\n8https://www.cohere.ai\n9https://venturebeat.com/uncategorized/openai-rival-cohere-launches-language-model-api/\n10https://docs.cohere.ai/changelog/multilingual-support-for-coclassify\n11https://docs.cohere.ai/docs/multilingual-language-models\n12https://api.cohere.ai/classify\n13https://docs.cohere.ai/reference/classify\n3\nAfricaNLP workshop at ICLR2022\nthe best prompt for classiﬁcation, following concurrent work (Jiao et al., 2023). We perform some\ninitial evaluation of the prompts and select the best one.\nOur best prompt is shown below:\nGiven the following news headlines and their categories:\nText:\n{Sentence}\nCategory:\n{Label}\nPlease classify the following news headlines into one of:\n{Label List}.\nText:\n{Sentence}\nCategory:\nWhere Sentence is the news headline to be classiﬁed, Category is the news topic, and LabelList\nis a comma separated list of all unique labels for that language.\nFor both models, we supply two example demonstrations per category from the training set. We\nrandomly sample 100 samples from the test set for each language and evaluate on this. Both demon-\nstrations and evaluation are done across two random seeds, such that we sample distinct demonstra-\ntions and test samples for each language with each random seed. We report the average F1 score for\neach language across both seeds. It should be noted that we decide to evaluate on a subset of the test\nset because of the tedious nature of obtaining results ChatGPT.\n3.3.2\nMACHINE TRANSLATION\nWe do not use Cohere for machine translation because its generation API currently supports only\nEnglish14. ChatGPT is used for all our machine translation evaluations. Preliminary results from\ncomparing few-shot to zero-shot translations on Nigerian Pidgin suggested no noticeable difference.\nHence, we perform all translations in a zero-shot manner because of the tedious nature and low-\nthroughput of obtaining results from ChatGPT.\nWe use the prompt used in concurrent work (Jiao et al., 2023) which is shown below:\nPlease provide the [TGT] translation for these sentences:\n{Sentence}\n{Sentence}\nWhere T GT is the target language to be translated into, and Sentence is a sentence to be translated.\nWe sample 100 sentences from the test set of each language and evaluate translating this to and from\nEnglish. We report the BLEU score (Papineni et al., 2002) which is calculated using SacreBLEU\n(Post, 2018).\nIt has been shown that English prompts perform better, on average, than in-language prompts\n(Lin et al., 2021; Shi et al., 2022), so we do not explore prompting in the target language for both\ntasks.\n4\nRESULTS\n4.1\nTEXT CLASSIFICATION\nResults are reported in table 1. As we can see, both commercial models fall well below the current\nstate of the art. Surprisingly, Cohere’s multilingual embedding model is the worst performer, despite\nsupporting almost all the languages evaluated on. Nigerian Pidgin has the highest score in the Cohere\nresults. This is most likely as a result of its close linguistic relationship with English language, which\nusually makes up a signiﬁcant portion of the pretraining corpora of pretrained language models\n(Wenzek et al., 2020; Gao et al., 2020; Laurenc¸on et al., 2022). ChatGPT is the best performing\ncommercial model, and it gets above average F1 scores on all languages. Similar to Cohere, Hausa\n14https://docs.cohere.ai/docs/generation-card#technical-notes\n4\nAfricaNLP workshop at ICLR2022\nTable 1: Text Classiﬁcation Results: We report the F1 scores for the commercial models. We also\nreport the current state of the art result obtained from Alabi et al. (2022). Best results per language\nare in bold.\nLanguage\nCohere\nChatGPT\nCurrent SOTA\nHausa (hau)\n43.2\n77.9\n91.2\nMalagasay (mlg)\n35.0\n51.1\n67.3\nNigerian Pidgin (pcm)\n48.8\n73.4\n82.2\nSomali (som)\n28.4\n51.3\n79.9\nisiZulu (zul)\n24.8\n54.8\n79.6\nand Nigerian Pidgin possess the highest F1 scores. The details of ChatGPT’s pretraining corpora\nand exact training methods are unknown, so it is hard to hypothesize a reason for its relatively\ngood performance. However, it is very likely that its pretraining corpora contains non-English text.\nFurthermore, multilinguality has been shown to be a part of possible emergent abilities of large\nlanguage models (Wei et al., 2022b), so the performance is not entirely surprising. Overall, both\ncommercial models fall signiﬁcantly short of the current state of the art. While ChatGPT is the\nbetter performer, Cohere’s performance is especially surprising since it has been trained on almost\nall of the evaluated languages15.\n4.2\nMACHINE TRANSLATION\nResults are reported in table 2. ChatGPT has very poor performance on machine translation, ob-\ntaining BLEU scores of less than 1.0 on all languages. This is very surprising given its good per-\nformance on text classiﬁcation. Our results agree with concurrent work (Abott et al., 2023) which\nﬁnds that GPT 3.5 obtains a BLEU score of 0 on Zulu to English translation. Our ﬁndings are\nalso somewhat similar to (Jiao et al., 2023), which reports signiﬁcantly worse performance on Ro-\nmanian, a relatively low-resource language, than on higher-resource languages like English and\nGerman. While the BLEU scores are too low to draw conclusions from, ChatGPT seems to perform\nbetter when translating into English than from it. This agrees with previous works (Belinkov et al.,\n2017; Bugliarello et al., 2020) which show that it is harder to translate into morphologically rich\nlanguages, like African ones, than morphologically poor ones like English. In general, our results\nsuggest that ChatGPT is not good enough for translation involving African languages. It also sug-\ngests that ChatGPT performs better on sequence classiﬁcation tasks than it does on text generation\ntasks for African languages.\n5\nERROR ANALYSIS\nWe take a closer look at some errors made by the model on machine translation. Speciﬁcally, we\nfocus on two languages - Yoruba and Nigerian Pidgin - because they are understood by the authors.\nFor each language, we randomly select 3 samples and discuss their predictions.\n5.1\nYORUBA TRANSLATIONS\nSamples are shown in table 4. Looking at sample 1, ChatGPT mistranslates “B´ı omi b´a gb´on´a\nju b´ı ´o s.e ye. lo.” which means “When water becomes too hot” to “Water is poured into the con-\ntainer”. Furthermore, the English to Yoruba translation is completely wrong and riddled with a lot\nof misspellings and grammatical errors. In sample 3, ChatGPT gets the translations wrong and also\ntransposes the words “ob`ınrin” (woman) and “ok`unrin” (man) in the translations. One notable ob-\nservation across English to Yoruba translations is that ChatGPT does not always include diacritics\nin its Yoruba predictions. Overall, ChatGPT does a really poor job in translating in either direc-\ntion. The hallucinatory nature of the model predictions is evident, as all translations barely have any\ncorrelation with the original sentences.\n15https://txt.cohere.ai/multilingual/\n5\nAfricaNLP workshop at ICLR2022\nTable 2: Machine Translation Results: We report the BLEU scores of the translations from ChatGPT.\nWe also report the current state of the art result obtained from Adelani et al. (2022)\nand NLLB Team et al. (2022). Best results per language are in bold.\nTranslation Direction\nChatGPT\nCurrent SOTA\nLug→Eng\n0.16\n30.9\nEng→Lug\n0.13\n25.8\nPcm→Eng\n0.22\n45.2\nEng→Pcm\n0.20\n35.0\nSwa→Eng\n0.18\n39.3\nEng→Swa\n0.15\n30.7\nYor→Eng\n0.10\n24.4\nEng→Yor\n0.12\n14.4\nZul→Eng\n0.31\n40.3\nEng→Zul\n0.26\n22.9\n5.2\nNIGERIAN PIDGIN TRANSLATIONS\nSamples are shown in table 3. Looking at the Nigerian Pidgin sentences, we can see the language’s\nlinguistic similarity with English. Interestingly, while the ChatGPT predictions yield low BLEU\nscores, they are somewhat semantically similar to the ground truth. However, there notable errors\nmade across board. For example, focusing on the Nigerian Pidgin to English predictions in sample\n2, there are tense errors. Also, the model seems to misunderstand what “numbers” refers to in the\ninput text, as its prediction indicates it confuses it for the number of goals. Furthermore, across\nall samples, the model seems to be poor at translating certain English words to Nigerian Pidgin\nwords, such as “The” to “Di”, so it always retains the original English word. In general, while the\npredictions in both directions for all samples have notable issues, they are more semantically similar\nto the ground truth than the BLEU scores suggests. This highlights the drawbacks of automatic\nmetrics based on N-gram overlap.\n6\nCONCLUSION\nWe have presented a preliminary analysis of commercial language models on African languages.\nJoshi et al. (2020) note that over 90% of the world’s 7000+ languages are under-studied by the NLP\ncommunity. Despite the 2000+ spoken languages and over 1 billion people in Africa16, its languages\nmake up a signiﬁcant portion of the under-studied languages (Blasi et al., 2022). While there have\nbeen several efforts (∀et al., 2020; Ahia & Ogueji, 2020b; Adelani et al., 2021; Ogueji et al., 2021;\nNLLB Team et al., 2022; Alabi et al., 2022; Dossou et al., 2022; Adebara et al., 2022) to close this\ngap, there is still a lot of work to be done. This is even more pertinent given the rapid commercial\nadoption of large scale language models. Our ﬁndings suggest that these models do not perform\nwell on African languages. In particular, there seems to be performance disparity, depending on the\ntask evaluated. Although our work reports what is, to the best of our knowledge, the ﬁrst evaluation\nof commercial language models on African languages, we note that this only a preliminary study\nthat needs to be further advanced. Future works could focus on more advanced prompting methods\nsuch as chain-of-thought (Wei et al., 2022c) and pivot prompting (Jiao et al., 2023), evaluation of\nmore test samples and a wider variety of tasks. While our ﬁnding may be impacted by the sampled\n16https://en.wikipedia.org/wiki/Demographics_of_Africa\n6\nAfricaNLP workshop at ICLR2022\nTable 3: Examples of Nigerian Pidgin translation using ChatGPT\nSample 1\nSample 2\nSample 3\nNigerian Pid-\ngin Sentence\nCommission head, Simon\nByabakama,\nassure\ndi\nnation on top live TV\nfor\nThursday\nevening\nafta polls don close say\nresult don dey enter for\ndi national tally centre\ndespite\ndi\nnationwide\ninternet blackout\nDi goal be give Eyimba\nsome hope as dem kontinu\nto press for plunty nomba\nto secure a point at least\nNa im inauguartion be\ndi ﬁrst time wey elected\nleaders dey handova give\neach oda for Niger history\nfor over 60 years wey dem\ndon get independence, but\ndi attack from jihadist and\nsome kind plan to do coup\ndon spoil di celebrate.\nEnglish Sen-\ntence\nCommission head, Simon\nByabakama, assured the\nnation on live TV on\nThursday\nevening\nafter\npolls closed that results\nwere arriving at the na-\ntional tally centre despite\nthe\nnationwide\ninternet\nblackout.\nThe goal gave Enyimba\nsome hope as they contin-\nued to press in numbers to\nat least secure a point\nHis inauguration marks\nthe\nﬁrst\nhandover\nbe-\ntween\nelected\nleaders\nin Niger’s 60-year post-\nindependence\nhistory,\nbut jihadist attacks and\nan\nalleged\nattempted\ncoup have overshadowed\nceremonies.\nChatGPT\nTranslation\nPcm→Eng\nCommission head, Simon\nByabakama, assured the\nnation on live TV on\nThursday\nevening\nafter\nthe polls had closed that\nthe results were entering\nthe national tally center\ndespite the nationwide in-\nternet blackout.\nThe goal was to give Ey-\nimba some hope as they\ncontinue to press for a suf-\nﬁcient number of goals to\nsecure at least one point.\nThe inauguration marked\nthe\nﬁrst\ntime\nin\nover\n60 years of independence\nthat elected leaders have\nhanded\npower\nto\neach\nother in Niger’s history,\nbut attacks by jihadists\nand plans for a coup have\ndisrupted the celebrations\nChatGPT\nTranslation\nEng→Pcm\nSimon Byabakama wey\ndey head the Commis-\nsion, e talk for TV on\nThursday night after elec-\ntion close say the result\ndon dey reach the centre\nwhere dem dey count the\nresult even though inter-\nnet all over the place dey\ncut.\nThe goal give Enyimba\nsome hope as dem still\ndey press wey full force to\nﬁt get at least one point.\nThe day wey dem swear\nam\nfor\nofﬁce,\nna\nthe\nﬁrst time wey leader wey\nget vote go hand over to\nanother leader since 60\nyears after independence\nfor Niger but the attacks\nwey the bad people wey\ndey carry gun dey do, and\nthe way wey dem try put\neye for the power no gree\nam to happen make e no\ndey sweet for the day.\ntest data, the use of the BLEU automatic metric (Callison-Burch et al., 2006; Mathur et al., 2020;\nFreitag et al., 2020), prompting template and prompting examples, it nonetheless presents a call-to-\naction to ensure African languages are well represented in the age of commercial large language\nmodels.\n7\nAfricaNLP workshop at ICLR2022\nTable 4: Examples of Yoruba translation using ChatGPT\nSample 1\nSample 2\nSample 3\nYoruba Sen-\ntence\nB´ı omi b´a gb´on´a ju b´ı ´o s.e\nye. lo. (t`ab´ı b´ı ´o b´a tut`u j`u)\n`awo.n iy`un y´o`o l`e ew`e omi\n— t´ı y´o`o s´ı p`ad´an`u o.wo. t`o\n´o ´n ﬁo`unje. n`u ´un.\nK´ı ´o t´o de `e´er´oo `ago. n´a`a,\nerin os. `u me.e.f`a n´a`a ti lu\nok`un.\n`Ike.l`u t´ı ´o ko.\nlu `awo.n\nob`ınrin wo.ny´ı (`ati `awo.n\no.k`unrin\nkan)\nror`o\nb´ı\ne.ranko e.h`ann`a.\nEnglish Sen-\ntence\nWhen water becomes too\nhot (or too cold) the corals\nexpel the algae — and\nwith it, its main source of\nfood.\nBefore his arrival at the\ncamp, the six-month-old\nwas snared in a wire trap.\nThe\nattacks\non\nthese\nwomen (and some men)\nare vicious and inhuman.\nChatGPT\nTranslation\nYor→Eng\nWater is poured into the\ncontainer (or it is ﬁlled)\nuntil the container is full -\nwhen it overﬂows, the ex-\ncess water is poured out.\nIf you do not pay attention\nto your work, the work of\nothers will be affected.\nThe\ntradition\nrequires\nmen to cover their heads\n(and sometimes women\ntoo) in public places.\nChatGPT\nTranslation\nEng→Yor\nTi omi ba n mu ki i fe.e.\n(tabi ki i fe.e.) awo.n corals\nni wo.n ﬁomi omi po. - ati\nni ito.ju re., iru ibe.re. ti a n\npa.\nBayi o ti gba lo. si ibeere,\no.mo. to.lo.ti si igba ni a ti\ndara ju o.nu.\n`Id´am`o\nge.ge.\nb´ı\ni\n`awo.\nn\no.mo.-`om`o-w´e\n(`ati\nn´ı\nk`ok`or`o)\nn`ıy´ın\nni\n`aj`ın`a\n`ıw´ej`u `ıto.lo.mo. w´aj`u.\nREFERENCES\nJade\nAbott,\nBonaventure\nDossou,\nand\nRooweither\nMbuya.\nCom-\nparing\nafrica-centric\nmodels\nto\nopenai’s\ngpt3.5,\n2023.\nURL\nhttps://lelapa.ai/comparing-africa-centric-models-to-openais-gpt3-5-2/.\nIfe Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed, and Alcides Alcoba In-\nciarte.\nSerengeti:\nMassively multilingual language models for africa, 2022.\nURL\nhttps://arxiv.org/abs/2212.10785.\nDavid Adelani, Jesujoba Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter,\nDietrich Klakow, Peter Nabende, Ernie Chang, Tajuddeen Gwadabe, Freshia Sackey, Bonaven-\nture F. P. Dossou, Chris Emezue, Colin Leong, Michael Beukman, Shamsuddeen Muhammad,\nGuyo Jarso, Oreen Yousuf, Andre Niyongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala,\nMuhammad Umair Nasir, Benjamin Ajibade, Tunde Ajayi, Yvonne Gitau, Jade Abbott, Mo-\nhamed Ahmed, Millicent Ochieng, Anuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi, Fa-\ntoumata Ouoba Kabore, Godson Kalipe, Derguene Mbaye, Allahsera Auguste Tapo, Victoire\nMemdjokam Koagne, Edwin Munkoh-Buabeng, Valencia Wagner, Idris Abdulmumin, Ayodele\nAwokoya, Happy Buzaaba, Blessing Sibanda, Andiswa Bukula, and Sam Manthalu. A few thou-\nsand translations go a long way! leveraging pre-trained models for African news translation. In\nProceedings of the 2022 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, pp. 3053–3070, Seattle, United States,\nJuly 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.223.\nURL https://aclanthology.org/2022.naacl-main.223.\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D’souza, Julia Kreutzer, Constan-\ntine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian Ruder, Stephen\nMayhew, Israel Abebe Azime, Shamsuddeen H. Muhammad, Chris Chinenye Emezue, Joyce\n8\nAfricaNLP workshop at ICLR2022\nNakatumba-Nabende, Perez Ogayo, Aremu Anuoluwapo, Catherine Gitau, Derguene Mbaye, Je-\nsujoba Alabi, Seid Muhie Yimam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani, Rubungo An-\ndre Niyongabo, Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba Ngom,\nTosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki, Emmanuel Anebi, Chia-\nmaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel Oyerinde, Clemencia Siro, To-\nbius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor Akinode, Deborah Nabagereka,\nMaurice Katusiime, Ayodele Awokoya, Mouhamadane MBOUP, Dibora Gebreyohannes, Henok\nTilaye, Kelechi Nwaike, Degaga Wolde, Abdoulaye Faye, Blessing Sibanda, Orevaoghene\nAhia, Bonaventure F. P. Dossou, Kelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,\nAdewale Akinfaderin, Tendai Marengereke, and Salomey Osei.\nMasakhaNER: Named En-\ntity Recognition for African Languages.\nTransactions of the Association for Computational\nLinguistics, 9:1116–1131, 10 2021.\nISSN 2307-387X.\ndoi: 10.1162/tacl a 00416.\nURL\nhttps://doi.org/10.1162/tacl_a_00416.\nOrevaoghene Ahia and Kelechi Ogueji. Towards supervised and unsupervised neural machine trans-\nlation baselines for nigerian pidgin. ArXiv, abs/2003.12660, 2020a.\nOrevaoghene Ahia and Kelechi Ogueji.\nTowards supervised and unsupervised neural ma-\nchine translation baselines for nigerian pidgin.\nCoRR, abs/2003.12660, 2020b.\nURL\nhttps://arxiv.org/abs/2003.12660.\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, and Dietrich Klakow.\nAdapting\npre-trained language models to African languages via multilingual adaptive ﬁne-tuning. In Pro-\nceedings of the 29th International Conference on Computational Linguistics, pp. 4336–4349,\nGyeongju, Republic of Korea, October 2022. International Committee on Computational Lin-\nguistics. URL https://aclanthology.org/2022.coling-1.382.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\nTranslation artifacts in cross-lingual\ntransfer learning.\nIn Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pp. 7674–7684, Online, November 2020. Asso-\nciation for Computational Linguistics.\ndoi:\n10.18653/v1/2020.emnlp-main.618.\nURL\nhttps://aclanthology.org/2020.emnlp-main.618.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. What do neural\nmachine translation models learn about morphology? In Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 861–872, Vancou-\nver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1080.\nURL https://aclanthology.org/P17-1080.\nDamian Blasi, Antonios Anastasopoulos, and Graham Neubig. Systematic inequalities in language\ntechnology performance across the world’s languages. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5486–5505,\nDublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.\nacl-long.376. URL https://aclanthology.org/2022.acl-long.376.\nTerra Blevins and Luke Zettlemoyer. Language contamination helps explain the cross-lingual capa-\nbilities of english pretrained models, 2022. URL https://arxiv.org/abs/2204.08110.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot learners.\nIn\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural\nInformation Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL\nhttps://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Pap\nEmanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan Cotterell, and Naoaki\nOkazaki. It’s easier to translate out of English than into it: Measuring neural translation difﬁculty\nby cross-mutual information. In Proceedings of the 58th Annual Meeting of the Association for\n9\nAfricaNLP workshop at ICLR2022\nComputational Linguistics, pp. 1640–1649, Online, July 2020. Association for Computational\nLinguistics. URL https://www.aclweb.org/anthology/2020.acl-main.149.\nChris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in ma-\nchine translation research. In 11th Conference of the European Chapter of the Association for\nComputational Linguistics, pp. 249–256, Trento, Italy, April 2006. Association for Computa-\ntional Linguistics. URL https://aclanthology.org/E06-1032.\nErnie Chang,\nDavid Ifeoluwa Adelani,\nXiaoyu Shen,\nand Vera Demberg.\nUnsuper-\nvised pidgin text generation by pivoting english data and self-training, 2020.\nURL\nhttps://arxiv.org/abs/2003.08272.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-\nskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica\nMoreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways,\n2022. URL https://arxiv.org/abs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pel-\nlat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-ﬁnetuned language\nmodels, 2022. URL https://arxiv.org/abs/2210.11416.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-training of\ndeep bidirectional transformers for language understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Min-\nnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL\nhttps://www.aclweb.org/anthology/N19-1423.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun,\nJingjing Xu, Lei Li, and Zhifang Sui.\nA survey for in-context learning, 2023.\nURL\nhttps://arxiv.org/abs/2301.00234.\nBonaventure F. P. Dossou, Atnafu Lambebo Tonja, Oreen Yousuf, Salomey Osei, Abigail Oppong,\nIyanuoluwa Shode, Oluwabusayo Olufunke Awoyomi, and Chris Chinenye Emezue. Afrolm: A\nself-active learning-based multilingual pretrained language model for 23 african languages, 2022.\nNicholas Faraclas. Nigerian pidgin english: morphology and syntax. Varieties of English: Africa,\nSouth and Southeast Asia, 4:340–367, 2008.\n∀, Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, Timi Fasubaa, Tajudeen Kolawole,\nTaiwo Fagbohungbe, Solomon Oluwole Akinola, Shamsuddee Hassan Muhammad, Salomon\nKabongo, Salomey Osei, et al. Participatory research for low-resourced machine translation:\nA case study in african languages. Findings of EMNLP, 2020.\nMarkus Freitag, David Grangier, and Isaac Caswell.\nBLEU might be guilty but refer-\nences are not innocent.\nIn Proceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP), pp. 61–71, Online, November 2020. As-\nsociation for Computational Linguistics.\ndoi:\n10.18653/v1/2020.emnlp-main.5.\nURL\nhttps://aclanthology.org/2020.emnlp-main.5.\n10\nAfricaNLP workshop at ICLR2022\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile:\nAn 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\nMichael A. Hedderich, David Adelani, Dawei Zhu, Jesujoba Alabi, Udia Markus, and Diet-\nrich Klakow.\nTransfer learning and distant supervision for multilingual transformer mod-\nels: A study on African languages.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pp. 2580–2591, Online, November 2020.\nAssociation for Computational Linguistics.\ndoi: 10.18653/v1/2020.emnlp-main.204.\nURL\nhttps://www.aclweb.org/anthology/2020.emnlp-main.204.\nJeremy Howard and Sebastian Ruder.\nUniversal language model ﬁne-tuning for text clas-\nsiﬁcation.\nIn Proceedings of the 56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1:\nLong Papers), pp. 328–339, Melbourne, Australia, July\n2018. Association for Computational Linguistics.\ndoi:\n10.18653/v1/P18-1031.\nURL\nhttps://aclanthology.org/P18-1031.\nWenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. Is chatgpt a good\ntranslator? a preliminary study, 2023. URL https://arxiv.org/abs/2301.08745.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and\nfate of linguistic diversity and inclusion in the NLP world.\nIn Proceedings of the 58th An-\nnual Meeting of the Association for Computational Linguistics, pp. 6282–6293, Online, July\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.560. URL\nhttps://aclanthology.org/2020.acl-main.560.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.\nHugo Laurenc¸on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz´alez Ponferrada, Huu\nNguyen, J¨org Frohberg, Mario ˇSaˇsko, Quentin Lhoest, Angelina McMillan-Major, G´erard\nDupont, Stella Biderman, Anna Rogers, Loubna Ben allal, Francesco De Toni, Giada Pis-\ntilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la Rosa,\nPaulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Romero\nMu˜noz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid Almubarak, Vu Minh Chien, Itziar\nGonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik\nBose, David Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Vio-\nlette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, and Yacine Jernite.\nThe big-\nscience ROOTS corpus: A 1.6TB composite multilingual dataset.\nIn Thirty-sixth Confer-\nence on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL\nhttps://openreview.net/forum?id=UoEw6KigkUn.\nHeather\nLent,\nEmanuele\nBugliarello,\nMiryam\nde\nLhoneux,\nChen\nQiu,\nand\nAnders\nSøgaard.\nOn language models for creoles.\nIn Proceedings of the 25th Confer-\nence on Computational Natural Language Learning, pp. 58–71, Online, November 2021.\nAssociation for Computational Linguistics.\ndoi:\n10.18653/v1/2021.conll-1.5.\nURL\nhttps://aclanthology.org/2021.conll-1.5.\nHeather Lent, Kelechi Ogueji, Miryam de Lhoneux, Orevaoghene Ahia, and Anders Søgaard. What\na creole wants, what a creole needs. In Proceedings of the Thirteenth Language Resources and\nEvaluation Conference, pp. 6439–6449, Marseille, France, June 2022. European Language Re-\nsources Association. URL https://aclanthology.org/2022.lrec-1.691.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle\nOtt, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh\nKoura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva,\nMona T. Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual language\nmodels. CoRR, abs/2112.10668, 2021. URL https://arxiv.org/abs/2112.10668.\n11\nAfricaNLP workshop at ICLR2022\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.\nWhat makes good in-context examples for GPT-3?\nIn Proceedings of Deep Learn-\ning Inside Out (DeeLIO 2022):\nThe 3rd Workshop on Knowledge Extraction and Inte-\ngration for Deep Learning Architectures, pp. 100–114, Dublin, Ireland and Online, May\n2022. Association for Computational Linguistics.\ndoi: 10.18653/v1/2022.deelio-1.10.\nURL\nhttps://aclanthology.org/2022.deelio-1.10.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre-\ntraining approach. arXiv preprint, abs/1907.11692, 2019.\nNitika Mathur, Timothy Baldwin, and Trevor Cohn. Tangled up in BLEU: Reevaluating the eval-\nuation of automatic machine translation evaluation metrics.\nIn Proceedings of the 58th An-\nnual Meeting of the Association for Computational Linguistics, pp. 4984–4997, Online, July\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.448. URL\nhttps://aclanthology.org/2020.acl-main.448.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le\nScao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir\nRadev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson,\nEdward Raff, and Colin Raffel. Crosslingual generalization through multitask ﬁnetuning, 2022.\nURL https://arxiv.org/abs/2211.01786.\nNLLB Team, Marta R. Costa-juss`a, James Cross, Onur C¸ elebi, Maha Elbayad, Kenneth Heaﬁeld,\nKevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler\nWang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia-Gonzalez,\nPrangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shan-\nnon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela\nFan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm´an, Philipp Koehn, Alexandre Mourachko,\nChristophe Ropers, Saﬁyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind:\nScaling human-centered machine translation, 2022.\nKelechi Ogueji and Orevaoghene Ahia. PidginUNMT: Unsupervised Neural Machine Translation\nfrom West African Pidgin to English. ArXiv, abs/1912.03444, 2019.\nKelechi Ogueji, Yuxin Zhu, and Jimmy Lin.\nSmall data?\nNo Problem!\nexploring the vi-\nability of pretrained multilingual language models for low-resourced languages.\nIn Pro-\nceedings of the 1st Workshop on Multilingual Representation Learning, pp. 116–126, Punta\nCana, Dominican Republic, November 2021. Association for Computational Linguistics. URL\nhttps://aclanthology.org/2021.mrl-1.11.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul\nChristiano, Jan Leike, and Ryan Lowe.\nTraining language models to follow instruc-\ntions with human feedback.\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.\nURL\nhttps://openreview.net/forum?id=TG8KACxEON.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.\nBleu: a method for auto-\nmatic evaluation of machine translation.\nIn Proceedings of the 40th Annual Meeting of the\nAssociation for Computational Linguistics, pp. 311–318, Philadelphia, Pennsylvania, USA,\nJuly 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL\nhttps://aclanthology.org/P02-1040.\nEdoardo Maria Ponti, Goran Glavaˇs, Olga Majewska, Qianchu Liu, Ivan Vuli´c, and Anna Korhonen.\nXCOPA: A multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2362–2376,\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nemnlp-main.185. URL https://aclanthology.org/2020.emnlp-main.185.\n12\nAfricaNLP workshop at ICLR2022\nMatt\nPost.\nA\ncall\nfor\nclarity\nin\nreporting\nBLEU\nscores.\nIn\nProceedings\nof\nthe Third Conference on Machine Translation:\nResearch Papers,\npp. 186–191, Bel-\ngium,\nBrussels,\nOctober\n2018.\nAssociation\nfor\nComputational\nLinguistics.\nURL\nhttps://www.aclweb.org/anthology/W18-6319.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer.\nJournal of Machine Learning Research, 21(140):1–67, 2020.\nURL\nhttp://jmlr.org/papers/v21/20-074.html.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,\nAbheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le\nScao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted\ntraining enables zero-shot task generalization. In International Conference on Learning Repre-\nsentations, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.\nTimo Schick and Hinrich Sch¨utze. It’s not just size that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language Technologies, pp. 2339–2352, Online,\nJune 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.185.\nURL https://aclanthology.org/2021.naacl-main.185.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi,\nHyung Won Chung,\nYi Tay,\nSebastian Ruder, Denny Zhou, Dipanjan Das,\nand Ja-\nson Wei.\nLanguage models are multilingual chain-of-thought reasoners, 2022.\nURL\nhttps://arxiv.org/abs/2210.03057.\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning for\nlanguage-model-as-a-service. In Proceedings of ICML, 2022.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester,\nNan Du, Andrew M. Dai, and Quoc V Le.\nFinetuned language models are zero-\nshot learners.\nIn International Conference on Learning Representations, 2022a.\nURL\nhttps://openreview.net/forum?id=gEZrGCozdqR.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,\nOriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.\nEmergent abilities of large lan-\nguage models. Transactions on Machine Learning Research, 2022b. ISSN 2835-8856. URL\nhttps://openreview.net/forum?id=yzkSU5zdwD. Survey Certiﬁcation.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H.\nChi,\nQuoc V Le,\nand Denny Zhou.\nChain of thought prompting elicits reasoning\nin large language models.\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022c.\nURL\nhttps://openreview.net/forum?id=_VjQlMeSB_J.\nGuillaume\nWenzek,\nMarie-Anne\nLachaux,\nAlexis\nConneau,\nVishrav\nChaudhary,\nFran-\ncisco Guzm´an, Armand Joulin, and Edouard Grave.\nCCNet:\nExtracting high qual-\nity monolingual datasets from web crawl data.\nIn Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pp. 4003–4012, Marseille, France, May\n2020. European Language Resources Association.\nISBN 979-10-95546-34-4.\nURL\nhttps://www.aclweb.org/anthology/2020.lrec-1.494.\n13\nAfricaNLP workshop at ICLR2022\nGenta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosinski, and Pascale\nFung. Language models are few-shot multilingual learners. In Proceedings of the 1st Workshop\non Multilingual Representation Learning, pp. 1–15, Punta Cana, Dominican Republic, Novem-\nber 2021. Association for Computational Linguistics.\ndoi: 10.18653/v1/2021.mrl-1.1. URL\nhttps://aclanthology.org/2021.mrl-1.1.\nXinyan Velocity Yu, Akari Asai, Trina Chatterjee, Junjie Hu, and Eunsol Choi. Beyond counting\ndatasets: A survey of multilingual dataset construction and necessary resources. In Findings of\nEMNLP, 2022.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan\nXu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang\nChen, Peng Zhang, Yuxiao Dong, and Jie Tang. Glm-130b: An open bilingual pre-trained model.\narXiv preprint arXiv:2210.02414, 2022.\nBiao Zhang, Barry Haddow, and Alexandra Birch. Prompting large language model for machine\ntranslation: A case study, 2023. URL https://arxiv.org/abs/2301.07069.\nMengjie Zhao and Hinrich Sch¨utze.\nDiscrete and soft prompting for multilingual models.\nIn\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Process-\ning, pp. 8547–8555, Online and Punta Cana, Dominican Republic, November 2021. As-\nsociation for Computational Linguistics.\ndoi:\n10.18653/v1/2021.emnlp-main.672.\nURL\nhttps://aclanthology.org/2021.emnlp-main.672.\n14\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2023-05-11",
  "updated": "2023-05-11"
}