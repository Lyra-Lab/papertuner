{
  "id": "http://arxiv.org/abs/2305.07466v1",
  "title": "Systematic Review on Reinforcement Learning in the Field of Fintech",
  "authors": [
    "Nadeem Malibari",
    "Iyad Katib",
    "Rashid Mehmood"
  ],
  "abstract": "Applications of Reinforcement Learning in the Finance Technology (Fintech)\nhave acquired a lot of admiration lately. Undoubtedly Reinforcement Learning,\nthrough its vast competence and proficiency, has aided remarkable results in\nthe field of Fintech. The objective of this systematic survey is to perform an\nexploratory study on a correlation between reinforcement learning and Fintech\nto highlight the prediction accuracy, complexity, scalability, risks,\nprofitability and performance. Major uses of reinforcement learning in finance\nor Fintech include portfolio optimization, credit risk reduction, investment\ncapital management, profit maximization, effective recommendation systems, and\nbetter price setting strategies. Several studies have addressed the actual\ncontribution of reinforcement learning to the performance of financial\ninstitutions. The latest studies included in this survey are publications from\n2018 onward. The survey is conducted using PRISMA technique which focuses on\nthe reporting of reviews and is based on a checklist and four-phase flow\ndiagram. The conducted survey indicates that the performance of RL-based\nstrategies in Fintech fields proves to perform considerably better than other\nstate-of-the-art algorithms. The present work discusses the use of\nreinforcement learning algorithms in diverse decision-making challenges in\nFintech and concludes that the organizations dealing with finance can benefit\ngreatly from Robo-advising, smart order channelling, market making, hedging and\noptions pricing, portfolio optimization, and optimal execution.",
  "text": "Article\nSystematic Review on Reinforcement Learning in the field of Fintech\nNadeem Malibari 1,†,‡\n0000-0003-3702-3743 , Iyad Katib 1,‡ and Rashid Mehmood 2,*\nCitation: Malibari, N.; Katib, I.;\nMehmood, R. . Journal Not Specified , 1,\n0. https://doi.org/\nReceived:\nAccepted:\nPublished:\nPublisher’s Note: MDPI stays neutral\nwith regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.\nCopyright: © 2023 by the author. Sub-\nmitted to Journal Not Specified for pos-\nsible open access publication under the\nterms and conditions of the Creative Com-\nmons Attribution (CC BY) license (https://\ncreativecommons.org/licenses/by/ 4.0/).\n1\nDepartment of Computer Science, Faculty of Computing and Information Technology;King Abdulaziz University,\nJeddah 21589, Saudi Arabia; nmahimalibari@stu.kau.edu.sa;IAKatib@kau.edu.sa\n2\nHigh Performance Computing Center, King Abdulaziz University, Jeddah 21589, Saudi\nArabia;RMehmood@kau.edu.sa\n‡\nThese authors contributed equally to this work.\nAbstract: Applications of Reinforcement Learning in the Finance Technology (Fintech) have acquired a lot\nof admiration lately. Undoubtedly Reinforcement Learning, through its vast competence and proficiency, has\naided remarkable results in the field of Fintech. The objective of this systematic survey is to perform an\nexploratory study on a correlation between reinforcement learning and Fintech to highlight the prediction\naccuracy, complexity, scalability, risks, profitability and performance. Major uses of reinforcement learning\nin finance or Fintech include portfolio optimization, credit risk reduction, investment capital management,\nprofit maximization, effective recommendation systems, and better price setting strategies. Several studies\nhave addressed the actual contribution of reinforcement learning to the performance of financial institutions.\nThe latest studies included in this survey are publications from 2018 onward. The survey is conducted using\nPRISMA technique which focuses on the reporting of reviews and is based on a checklist and four-phase\nflow diagram. The conducted survey indicates that the performance of RL-based strategies in Fintech fields\nproves to perform considerably better than other state-of-the-art algorithms. The present work discusses the\nuse of reinforcement learning algorithms in diverse decision-making challenges in Fintech and concludes\nthat the organizations dealing with finance can benefit greatly from Robo-advising, smart order channelling,\nmarket making, hedging and options pricing, portfolio optimization, and optimal execution.\nKeywords: Reinforcement learning, RL, Q-learning, recurrent unit, recurrent Q network, artificial\nintelligence, AI, machine learning, Fintech.\n1. Introduction\nThe contemporary Financial technology (Fintech) theory has fast acquired popularity both\nas an academic research framework and as a practical application in the financial services sector.\nThe volume of data has grown significantly due to presence of a range of variables, such as\ntransactions, quotations, trading, and order flow, thereby posing new computational and theoretical\nissues. Fintech, being a blend of the two concepts (finance and technology), is a rising academic\nand professional area that tries to use automation and other technologies to simplify and improve\nthe provision of financial services. The portmanteau term \"Fintech\" refers to the use of high-tech\ncomputer software and algorithmic procedures to enhance the financial management capabilities\nof professionals and businesses. The conventional use of desktop computers is increasingly being\naugmented with mobile devices, including tablets and cellphones. The objective of any investment\nin the financial market is to maximize profits while minimizing risks. Artificial intelligence\n(AI) is largely utilized to address challenges that seem to demand high levels of intellect and are\ndifficult for people to handle. Expert systems may be used to assist people in making choices, as\nrule-based systems are often utilized in this context. Manual modeling of knowledge bases of\nsuch systems involves a variety of obstacles that requires a large amount of work, and is prone to\nerrors. Some people refer to this problem as the \"knowledge acquisition bottleneck\" [1].\nThe bottleneck in the knowledge acquisition process can be handled by those systems which\nare capable of giving more accurate and quick responses while being error-free to handle the\nchallenges of large data sets. Machine learning-based knowledge has been used in a broad variety\nof domains, including health, finance, behavior analysis, natural language processing (NLP), and\nother areas [2–4]. In the financial market, decisions on purchase and sell trading may be made\neither by humans or by computer intelligence. Over the course of the last several decades, the\nVersion April 29, 2023\n2 of 31\nuse of computers to make trading decisions while participating in the foreign exchange and stock\nmarkets, has progressively risen. In addition, the fields that are directly associated with AI and\nmachine learning, are used in a wide variety of other domains as well.\nThe objective of Reinforcement Learning (RL), a subfield of computer science which falls\nunder the umbrella of AI, is to teach computers to solve problems on their own without being given\nspecific instructions to do so [1]. Instead of relying on more typical stochastic control systems,\nit has been chosen to build new ideas that are based on RL so that all of this information may\nbe put into use. RL is a trial-and-error approach to train the computers to produce autonomous\nagents with optimal interactions with their environments. The theory of RL describes how agents\nmay learn to take optimum and more accurate behaviors via iterative trial and error. This may\nbe expressed more formally as the goal of all agents (whether they are humans, animals, or\ntechnology) to maximize the discounted value of future rewards over time [5].\nThe notion of RL has not only been embraced by the Fintech industry, but it has also been\nemployed by a wide variety of other businesses. RL is a kind of machine learning and training\nbased on the principle of rewarding favorable behaviors and discouraging the bad ones, as defined\nby [6][7]. For the most part, a RL agent can \"scan\" its surroundings, \"think\" about what to do\nnext, and \"learn\" from its mistakes. The RL approach, thus, provides high rewards for agents who\ndemonstrate excellent behavior as well as punishments for those who show poor behavior. As\na result of the incentive structure, the agent is motivated to find a solution that is beneficial to\nboth the current and future parties in order to reach the best possible outcome [8]. Long-term\nobjectives serve as a shield against the agent’s tendency to focus on more immediate needs. As\na result, the agent eventually learns how to avoid potentially harmful situations by seeking out\nindividuals who have a good attitude and can produce compatible offspring. AI is quickly gaining\nground in the area of RL with punishments and rewards, due to its effectiveness as a method of\ncontrolling unsupervised learning.\nRL encompasses several algorithms, instead of referring to a specific algorithm, each of\nwhich takes a unique approach to the problem being addressed. The primary reason for the\ndisparities between the algorithms is the approach that they use while attempting to investigate\nand exploit their surroundings. State-action-reward-state-action (SARSA) algorithm resembles to\nQ-learning and Deep Q-Networks (DQN); the three are the most frequently used algorithms in\ndeep reinforcement learning (DRL). The first step in SARSA’s functioning is to appoint a policy\nto the agent. The agent is given the opportunity to learn about the chance that certain behaviors,\nin addition to receiving rewards, may yield beneficial states via the use of the policy. Q-learning\nadopts the complete opposite approach as compared to SARSA. To be more specific, the agents\nare not provided with any kind of policy, which emphasizes the fact that their investigation of the\nsurrounding world is self-directed. On the other hand, DQN in RL (Figure 1) make use of neural\nnetworks in conjunction with other RL methods. The RL strategy of self-directed ecosystem\nexploration is often used by these algorithms. Therefore, future actions are based on a sample\npopulation of past behaviors that were advantageous and were acquired by the neural network.\nFigure 1. Reinforcement learning architecture\nFigure 1 illustrates the actual mechanism in which the RL approach functions; clarifying\nall the agents, elements and concepts entailed in the RL design. Compared to supervised and\nVersion April 29, 2023\n3 of 31\nunsupervised learning environments, RL is quite different regarding the overall purpose [9].\nPrecisely, while the main target of unsupervised learning is to determine the differences and\nsimilarities between data points, the overall goal of RL is to identify an appropriate action design\nthat can optimize the aggregate agent reward [7].\nRL is widely used in the Fintech sector for a variety of purposes. One of the most widespread\nimplementations of RL in the financial sector is the use of trading bots, which are programmed to\nlearn from the stock and trading market environment via interactions with the actual market [10].\nThese RL bots use trial and error to fine-tune their learning processes according to the unique\ncharacteristics of each commodity traded on the stock market. These methods are based on the\nspecifics of each stock. In order to foster the application of DRL in Fintech and understand the\ndynamics of the financial business, systematic literature surveys on this topic are necessary. This\nsurvey reassures that the introduction of RL tools has a profound impact on the global financial\nsector.\nAnother new field of research is the use of RL to anticipate stock prices, alongside trading\nbots. RL for stock price prediction has seen far less attention than the supervised, and at\ntimes, unsupervised machine learning algorithms, despite the fact that it has the potential to\nprovide surprisingly accurate outcomes. Some problems have been discovered with supervised\nlearning-based stock price forecasts; for example, due to its chaotic and unpredictable nature,\nstock trading is often considered one of the most challenging applications. By including a market\nvolatility component into reward functions, we may be able to increase the size of the trade shares\nthat have low volatility, and vice versa [11]. Issues concerning the supervised learning-based\nstock price predictions have been identified; one of those issues is that they are not capable of\ndealing with time-delayed rewards adequately [12]. This can be described alternatively that the\ndelayed penalty or reward is not the foremost concern of supervised learning-based algorithms;\nrather they only focus on the accuracy of the price prediction at that moment. In addition to that,\nwhereas most supervised machine learning algorithms can only prescribe actions for specific\nstocks, RL can get us to the decision-making stage right away, allowing us to choose whether to\npurchase, hold, or sell any stock.\nFurther applications of RL may be found in other area of Fintech, especially portfolio\nmanagement and optimization operations. The initial stages in the process of managing a\ncustomer’s portfolio are to produce an investment policy statement and to find out the criteria\nspecified by the client. Following these phases, the succeeding ones consist of developing a\nportfolio, monitoring and re-balancing that portfolio, doing security research, allocating assets,\nevaluating performance, and lastly reporting on the process. The correlation between automated\ntrading and excessive volatility has been highlighted by researchers. When the price of a stock falls\nbelow a certain level, automated trading systems have the potential to carry out the pre-programmed\nrules included inside their trading algorithms. As these algorithms make their decisions based on\nhistorical data, they may be more sensitive to changes in price. As a result, a particular strategy\nmay enable investors to quickly optimize and re-balance their portfolios by entering and exiting\nthe market in response to sudden fluctuations in price [8].\nFurthermore, RL algorithms have shown good performance, despite the fact that under\nspecific situations they might be vulnerable to certain types of attacks. RL has a small number\nof applications and may be difficult to put into practice, although it has a significant amount of\nunrealized potential. The fact that this kind of machine learning is dependent on environment\nexploration is one of the obstacles that must be addressed before it can be used [13]. The situation\nis analogous to the case when a robot depended on RL is employed to navigate around a difficult\nphysical environment, for example, the robot would look for new states and pick up new behaviors.\nNevertheless, because of the speed and the volatility with which the financial environment changes\nin the real world, it might be difficult to make the proper judgments on a constant basis [6]. Due\nto the amount of time required to guarantee that the learning is carried out in an effective manner,\nthe method’s demand on computational resources may be substantial, even it its trivial nature may\nplace certain constraints on its application. The complexity of the training environment results\nin increased amount of time and computational resources required. When there is an adequate\nquantity of data available, supervised learning has the potential to provide better outcomes more\nVersion April 29, 2023\n4 of 31\nquickly and more effectively than what RL does. This is because it requires less resources than\nRL to put into practice.\nA broad application of our research is demonstrated by its academic significance as well as\nits non-technical understanding in the context of RL theories for Fintech. The main objectives of\nthis study are outlined as follows:\n•\nProviding in-depth insights on the framework of several RL ideas in the Fintech industry.\n•\nPresenting a non-technical understanding of how RL might improve the process of evaluating\nfinancial risk, as well as highlighting the primary benefits and disadvantages of RL-based\ntrading and optimizations.\n•\nProposing a taxonomy and a generic architecture for the use of RL in financial technology.\n•\nOutlining the most significant drawbacks and difficulties associated with RL in the Fintech\nindustry.\nThe role of AI and RL in the Fintech sector is discussed in section 2. Methodology to\nconduct the current survey based on PRISMA technique is presented in Section 3. Section 4\npresents taxonomy, as a classification scheme of this study. The Section 5 covers a review of the\nRL applications in different sectors as highlighted by the studies included in this survey. In the\nsection 6, our in-depth findings are explored and speculated on future directions of our work.\nThe conclusion is drawn in Section 7.\n2. Role of Reinforcement Learning in the FinTech Sector\nIn the recent years, significant contribution has been made in the field of machine learning to\nachieve a certain level of accuracy within limited time constraints. The research work presented\nby Serrano [14] showed a unique learning algorithm with the Random Neural Network based on\nthe genome model. The work replaced and modified the genetics algorithm where the neuron\nweights, instead of the neurons themselves, were passed on to the future generations. The idea\nwas to imitate the human brain using a complex deep learning structure. The current decisions\nwere made locally in a fast manner with the help of RL. The identity and memory are provided by\ndeep learning clusters. The ultimate strategic decisions are made by deep learning management\nclusters, and knowledge is passed down to the future generations via genetic learning. The\nsuggested algorithm and Deep Learning framework were tested in a Fintech Model, which was a\nSmart Investment application. The Intelligent Bankers in this model are skilled at determining\nwhich assets, markets, and risks are advantageous to acquire and sell. The obtained results were\npromising. The proposed model, which was based on the random neural network model and\ngenetics algorithm along with deep learning, showed that AI is similar to biological learning in\nthe sense that both learn gradually and continuously while adjusting to its environment.\nSimilarly, the findings of the study performed by Mousavi et al. [15] outlined the complete\noverview of DRL regarding its applications in Fintech. The work provided one of the best\nbackground information on RL as the authors clarified the core components of the technology,\nincluding exploration, planning, model, reward, policy, and DQNN (Deep Q Neural Network).\nThe comprehensive overview further discusses critical mechanisms of RL, including hierarchical\nRL, multi-agent RL, transfer learning, unsupervised learning, memory and attention [16].\nThe work of Bazarbash [17] also addresses the inclusion of Fintech in finance, with machine\nlearning being applied in appraising credit risk. Their work incorporated potential strengths\nand fragility in the credit assessment using machine learning-based techniques and strategies.\nAccording to the author(s), recent developments in digital technology, as well as big data, have\nenabled Fintech lending to serve as a potentially feasible alternative aimed at lessening credit costs\nfor financial trading. However, relevance of data needs to be ensured due to its vital and central\nrole in machine learning-based analysis. On the other hand, the variables that are responsible for\nactivating and triggering prejudice should not be used for the sole purpose of avoiding digital\nfinancial debarring and exclusion. They concluded with the fact that for the accessibility of\nFintech credit, there should be a development of technological infrastructure for big data-based\ndecision making. In addition, it was concluded that there should be constant reviewing of machine\nlearning models by analysts for avoiding potential weaknesses in credit rating. Finally, speaking\nVersion April 29, 2023\n5 of 31\nof machine learning-based techniques in the finance sector have proven their worth and they are\nready to be deployed.\nHu and Lin [18] examined DRL (Deep RL) to maximize the performance of the financial\nsector, as it has performed good in several other computational areas. They examined how DRL\ncan be adopted in maximizing portfolio management in the financial sector. The scholars contend\nthat DRL is one of the emerging AI research topics that integrate deep learning for RL and policy\noptimization for goal-adjusted self-learning without involving any form of human intervention.\nThe idea of their project revolved around the exploration of deep recurrent neural network models\nfor the decision-making of measures on policy optimization in non-Markov decision processes.\nThey evaluated total rewards for policy by crafting a feasible and practicable risk-adjusted reward\nfunction. Additionally, the study continued with the entitlement and authorization of combining\nRL and deep learning with the goal of boosting their already existing capabilities and capacities\nto discover a paradigmatic and standard policy. Lastly, RL approaches were inquired about and\nscrutinized to be integrated with deep learning approaches with the impetus of puzzling out the\npolicy optimization query. Despite the deep learning-based approaches, a fair amount of work has\nbeen done using RL-based techniques for portfolio optimization.\nIn regards to the prime task of Portfolio optimization, a study showed a framework of\na RL algorithm for order execution and portfolio management. Wang et al. [19] addressed\nthe shortcomings of the existing impracticalities and introduced the hierarchical system that\nis reinforced in stock trading for portfolio management and optimization. These studies were\nimplemented to be viewed for results in the markets of US (United States) and China. The\nsubstantial experimental results proved that Hierarchical Reinforced Trading System for Portfolio\nManagement (HRPM) was a notable improvement with regards to other approaches and techniques.\nHRPM supports and simplifies decision-making tasks which is the prime need for a successful\njourney in finance fields.\nFintech agents and some of the established financial sector firms have gotten stronger over\nthe pandemic [6]. Although many financial institutions have been affected; even still, there are\nmany more that are fast changing to provide financial services that are tailored for the modern\nenvironment [11]. Well before the the onset of Covid-19, certain financial services organizations\nwere already bolstering business models presented by them with cutting-edge and creative Hi-Tech\nsolutions. Today, this procedure has been accelerated even more. The finance sector, in particular,\nis changing several processes through the use of AI and machine learning.\nHambly et al. [20] aimed at surveying the latest growth and expansion in the use of RL\napproaches and techniques in finance. They begin with providing a detailed introduction to the\nMarkov decision processes followed by various other algorithms. Among these algorithms, their\nmain focus was value and policy-based techniques as these techniques proved to be assumption-free.\nThey concluded with an in-depth discussion on the future technology being supervised by RL.\nThey found several research studies in literature that have addressed (and continue to address) the\nplace of RL in Fintech. The rapid shifts in the finance sector because of the ever-rising volume of\ndata have completely transformed the methods utilized in data analysis and processing. The shifts\nhave also introduced new computational and theoretical challenges. Joseph M. Carew [7] also\nnoted that compared to conventional stochastic control theory as well as numerous other analytical\nmethods for solving fiscal decision-making issues that heavily depend on model assumptions,\ndevelopments in RL can entirely rely on the large volumes of financial data. Accordingly, these\ndevelopments can enhance decision support systems in complex financial ecosystems.\nRemarkable work in the use of RL as a base to propose high-efficiency models has been\nreported in literature. RL being the base is paving the way for ground-breaking discoveries in\nthe field of Fintech which is highly appreciable. Kuo et al. [8] formed the basis of improvement\nin trading via RL by proposing a general adversarial market model. He studied that portfolio\nmanagement was being handled extremely extraordinarily by RL yet some limitations were there\nthat needed to be addressed and resolved. The experimentation included the formation of a virtual\nmarket via a market behaviour simulator and a pragmatic security matching system. This virtual\nmarket was being used as a front face for the interactive training environment in RL-based portfolio\nagents. The results showed a 4% improvement in portfolio performances over the state-of-the-art.\nVersion April 29, 2023\n6 of 31\nAs transformation processes in the digital world have changed globally, innumerable financial\ncyber crimes have also sprouted. The actual silver lining to this is the fact that machine learning\nand AI have allowed users and companies to safeguard their accounts from fraud and any other\nforms of malicious usage [21]. Technologies such as block-chain and crypto-currencies have often\nbeen closely linked with financial cyber-security. Hendershott et al. [21] presented an overview\nof various issues in Fintech research with an anticipation that their work could stimulate future\nresearch in the field of Fintech bu utilizing AI and block-chain correlation. In future, however,\ndigital security will be associated more with AI and machine learning to prevent the existing\nforms of money laundering [14]. This is because algorithms can adequately and effectively detect\nas well as notify users of malicious activities. RL as technology will continue to monitor unusual\nactivities or trends; thus, limiting the need for physical vigilance.\nSimilarly, Cao et al. [22] reviewed the research in AI and Fintech over time. Their findings\ninvolve many latest updates and upgrades made for banking, trading, training, block-chain\nand crypto-currencies. Additionally, it makes use of data science and AI methods including\ndata analytics, deep learning, federated learning, privacy-preserving processing, augmentation,\noptimization, and system intelligence for improvements. Some other strategies include quantitative\napproaches, complex system methods, intelligent interactions, recognition and responses, and\nintelligent interactions. The authors provide a very thorough research summary of each of these\nstrategies that make it possible for new disruptive technologies to open up a wide range of\nopportunities for both users and businesses. In practice people believe that AI and machine\nlearning are just appropriate for huge businesses with large pools of finance and tech professionals.\nHowever, the truth can never be further from reality [6] because these technologies, along with\npotent apps, are being used by Fintech organizations of all sizes for a variety of objectives.\nA study found by Tian et al. [23] discussed and reviewed in detail the data-driven approaches\nin Fintech. The research work attempted on providing an extensive comparison inclusive of the\nadvantages and disadvantages of various data-driven algorithms in applications related to finance.\nAlso this work aimed at forming a firm foundation for future discoveries of data-driven approaches\nin the Fintech domain. The authors present an exploration of data-driven approaches and machine\nlearning algorithms in the fields of portfolio and risk management, sentiment analysis and data\nprivacy protection etc. The existing Finech projects are compared using both conventional\ndata analytics methods and cutting-edge innovations. The framework for the analytical process\nis created, and in this area, insights are given on implementation, regulation, and workforce\ndevelopment.\nOver time, several inflexible and predictive approaches have been suggested to indicate stock\nprice movements, especially the shifts that have failed to obtain satisfactory outcomes [21]. This\nis specific to when there are issues or crashes in the stock market. Yang et al. [24] outlined that to\ncope with the emerging issues, prediction frameworks have been proposed using deep learning,\nRL, and adversarial training approaches. RL and field experiments were aggregated. They viewed\nthat by automating two jobs, such as identifying market conditions and executing trading strategies,\nAI approaches can assist quantitative trading. The representation of high-frequency financial\ndata and striking a balance between the exploitation and exploration of the trading agent with\nAI approaches are two hurdles that the current methods in quantitative trading must overcome.\nThe main focus of the study was loan debt collection with regards to following a strict sequential\ncollection strategy rather than following private information-based actions. The results suggested\nthe use of fewer collection actions with more vigilance by the loan tenets. Further, the significance\nof personalization in debt collections was revealed by the results of borrower profiling analysis.\nThis study adds to the body of literature about AI in Fintech by offering tangible, workable and\nthrifty policy implications.\nKhuwaja et al. [25] examined adversarial and deep learning networks for applications of\nFintech by utilizing heterogeneous information origins. The study argues that the dynamic aspect\nand increasing sophistication are the principal provocations for Fintech modelling applications\nlike the stock market. The proposal begins with modifications in the newton-divided difference\npolynomial (NDDP) for the attribution of data that is missing. Long and short term memory\nnetwork (LSTM) was used for extracting the native and innate properties of the financial market.\nVersion April 29, 2023\n7 of 31\nThe two adversarial networks studied by Khuwaja et al. [25] were the confrontational Q learning\nand HDRM Q learning, with HDFM being heterogeneous data fusion representing market cast.\nThe training of the networks was started and the sole purpose was to improve the prediction level\nconsidering the times when the financial market will be evaporative and volatile. The result shown\nthat the performance of prediction was way more improved and upgraded by using the global\nindicators and proposed adversarial network in comparison to the already existing works and\nnetworks.\nA detailed survey performed by Cai et al. [13] on DRL for data analytics paved way for\nmany other pieces of research and progresses. The survey begins with the vast and foremost\nintroduction to DRL followed by theories and key concepts of DRL. It continues with the\ndiscussion of the deployment of DRL on database systems which is the subject of the following\nsection. This facilitates data processing and analytics in a variety of ways, inclusive of data\norganizing, scheduling, tuning, and indexing. The use of DRL in the analytics and processing of\nthe data is then surveyed, covering everything from data preparation and NLP to healthcare and\nFintech. Finally, they went over several significant unresolved issues and potential future research\nareas for DRL in the analytics and processing of the data.\nAlthough Fintech can never fully replace human intelligence, it can undoubtedly strengthen it.\nFinancial companies can leverage the power of Artificial Neural Network or other disruptive tools\nto construct potent goods and decision-making systems to advance in the area of financial service\nvia the use of computer-based technologies that focus on Big Data analytics [11]. This is leading\nto significant changes either on an organizational or on individual level. Fintech companies may\nprofit from AI by fulfilling their growth objectives, obtaining a competitive edge, and becoming\nmore responsive to their consumers [22]. Additionally, it can help them save operating costs\nand streamline internal processes. This will improve users’ financial behaviour, which will\nbe advantageous [6]. Fintech applications are creating innovative and engaging methods for\nconsumers to process information [14]. Visualization and data science tools’ strength makes it\nsimple to analyze data using apps and turn it into easily understandable insights. Users can make\nuse of complex data to enhance financial decision-making.\nRemarkable work by Lagna and Ravishankar [26] aimed at making the world a better place\nusing financial technologies. They concluded with a discussion on how their research will lead\nto a financially inclusive society. Other work by [11] put forward an adaptive trading model\nfor developing quantitative trading strategies by an intelligent trading system. Their model\nthen intensified to imitation and DRL techniques. The training of trading agents in the real\nfinancial market was performed for better counterfeiting. More specifically Le et al. [10] wrote\na journal discussing the actual situation of the Vietnam Fintech Market with the applications\nof Machine learning. They made predictions that determine the value for money (VFM) to use\nMachine learning techniques in their financial institutions in the future. They concluded with the\nconsideration that using machine learning in their financial matters is purely essential because it\nrelates to their existence of them in near future.\nIn addition to the above work done in the financial markets, other works in stock predictions\nfor finance technology were also performed by researchers. The prediction model is driven by\nRL in Fintech, according to Shi et al. [16], and relies on the heterogeneous information base,\nincluding global indicators, tweets, and stock prices. In the RL model, proposals have included\nthe redesigned NDDP for any imputation of data that is missing. Lagna and Ravishankar [26]\nnote that the information trends representing the inherent aspects of monetary markets can be\nretrieved via LSTM. In the 2 adversarial network models, a heterogeneous fusion of data should\ndepict the market crash or HDFM, thus leaving the world with wonders in the field of finance\ntechnology aiming at predictions, management, advising and so on [27].\nSome of the literature review discussed above is summarized as shown in the table 1 below:\nVersion April 29, 2023\n8 of 31\nReference\nFintech Area\nMethodology\nExperimentation\n[8]\nTrading\nAdversarial\nmarket\nmodel\nvirtual market setup is showed 4%\nimprovement in portfolio perfor-\nmances\n[11]\nTrading\nIntelligent trading sys-\ntem based on an imita-\ntive DRL\nTrading agent is trained in real fi-\nnancial market and results showed\nthat the model was able to extract\nrobust market features and adaptive\nin different markets\n[14]\nSmart investment\nmodel\nRNN based on the\ngenome model\nPromising results were shown as As-\nset Banker RL algorithm took the\nright investment decisions and made\nmaximum profit\n[16]\nTrading\nCNN with Recurrent\nRL framework for port-\nfolio mgt.\nBetter results over existing solutions\nwere shown on the cryptocurrency\ndatasets by obtaining higher profits\n[18]\nFinance Portfolio\nMgt.\nDeep RNNs (GRUs)\nand DRL for optimiz-\ning finance\nAuthors aimed to present empirical\nresults of their study in the near fu-\nture\n[24]\nPersonalizing\ndebt collections\nAggregated Deep RL\nand field experiments\nResults suggest the use of fewer col-\nlection actions by the loan tenets.\nAlso, the significance of personaliza-\ntion in debt collection was evident\n[25]\nFor prediction in\nvolatile Stock mar-\nket\nNDDP, LSTM, and ad-\nversial learning net-\nworks\nPerformance of prediction was re-\nported better than existing works\n[27]\nOptimizing Trad-\ning mgt. system\nQ-Learning Algo. and\nthe\nSARSA\nAlgo.\nbased on RL method\nTested stock prices time series with\n500 simulations; both algorithms\ngenerally performed well while QLa\nperformed better in individual per-\nformances\nTable 1: Summary of some reviewed studies\n3. Adopted Methodology to Conduct the Study: Using PRISMA guideline\nMethodology is defined as a system of methods used in a particular area of study or activity\nwhile research methodology, presented in the research papers, describes the procedures and tools\nthat can be used to conduct a comprehensive research. As Jain [28] points out, the success of a\nspecific research is determined by the tools and techniques used by the researchers to facilitate\ncompletion of a study. Accordingly, the methodology is crucial to inform the intended audience\nabout how the study objectives, research questions, and aims have been investigated throughout\nthe work. Consequently, scholars are inspired to define different approaches, research context,\ndata source, research area, methodology, and strategies to present the whole research methodology\ncomprehensively. Nevertheless, the methodology merely serves the purpose of presenting both\nexperimental as well as computational models that are unique. However, the method being\ndescribed can be either unique and new, or it can be a bid to offer an improved version of the\npreviously described ones. In this study, we intend to establish the foundation for new research\nby combining previous groundbreaking and significant researches that have been conducted in\nthe past with this new research. There should be extensive testing performed on the approaches,\nand ideally, although not always, they should have been applied to demonstrate their value. For\nreaders to use the findings and recommendations of a systematic review, all datasets must be\navailable and understandable to them.\nWith the aim of exploring developments of RL and its role in FinTech, this study emphasizes on\nthe existing technologies and research on the topic with an explanatory approach. Specifically, the\nstudy is secondary research; thus, it is particularly based on desktop research. This means that the\nVersion April 29, 2023\n9 of 31\nstudy has relied on current and previous studies performed by numerous other scholars in the field\nof RL and its correlation with FinTech. Given below is the track followed throughout the review:\n•\nFiguring out and defining the aim, design as well as setting of the research study.\n•\nThrough previous research, an in-depth description of the main focus of the review which is\nRL and fintech seperately as well as together.\n•\nA comprehensible and understandable illustration of all the interventions and comparisons\nbetween efficiency, accuracy and effectiveness.\n•\nThoroughly examining the relevance between the previous studies, their successes as well as\nshortcomings.\nAs aforementioned, the secondary nature of this study means that the explanatory research method\nis the most applicable. Generally, exploratory studies denote a research technique exploring\nthe reasons why something happens, especially when limited information is accessible [28].\nExploratory studies can significantly assist to increase understanding of a specific subject,\ndetermining why and how a specific phenomenon is taking place, and predicting the future. Such\nstudies are generally concerned with the creation of a knowledge base founded on an intensive\nsearch of different information and data [29]. Due to its extensive nature and need to find facts,\nexploratory studies tend to reveal different outcomes or results that a scholar did not expect.\nGenerally, the findings can contradict the research hypothesis.\nThe explanatory research method can be viewed as a research that has the starting point with\na general idea and uses this research as a mode to spot issues and problems which can serve\nas a basis for future studies. The main focus here is that the researcher must be open to new\ncontradicting facts and should be capable of moving them accordingly. This is one of the utmost\nchallenges that are a part of this research. With each passing moment, there is a new addition to\nthe previously studied and available literature. Conclusive remarks need to be made accordingly\nwith the literature being revealed. This research was limited due to the limited work available out\nthere, however this can then be served as a grounded theory approach or interpretive research.\nThe principal purpose of using the exploratory research method in the current survey is the\nlimited resources or studies focused on the actual short- and long-term implications of using\nRL by financial institutions of FinTech in sustaining and promoting performance. Through the\nexploratory nature of the study, researchers can get a general concept and apply research as a\nmodel of a quick guide to the challenges or concerns that can be discussed in future. Regardless,\nthe foremost task of using the exploratory approach is recognition of reasons for RL being one of\nthe best and most appropriate ways of fostering the performance of financial institutions.\nIn this line of work to get valuable insights from the studied literature, adopted methodology\nto conduct the current study incorporates PRISMA technique. The PRISMA technique is an\nabbreviation for Preferred Reporting Items for Systematic Reviews and Meta-analysis [30]. It\nensures improved reporting quality of a systematic review by providing significant clarity in\nfiltering out process of papers. Figure 2 depicts the flow diagram of the current study, carried out\nusing PRISMA.\nVersion April 29, 2023\n10 of 31\nFigure 2. PRISMA Flow diagram of our systematic literature review\nThe methodology of this exploratory systematic research is summarized in the following\nsections:\n3.1. Data Collection and Selection\nLiterature study and data collection was the basic and principal step concerning the topic\nunder review. This step was the most crucial as it served as the base for the whole research and\ncomparisons were made according to the studied literature.\nThe search procedure was extremely exclusive with the topic being reviewed. The main difficulty\nwas finding true and accurate relevance between our research topic and the available literature for\nstudy. We had taken start of our work with the searching process using our research keywords;\ni.e., RL, Deep Q learning, recurrent unit, recurrent Q network, AI, machine learning and Fintech\netc. This way of database search exposed a lot of material for our study. Online research, along\nwith manually available pieces of studies, produced 65 relevant studies. Although more than\n200 current studies from 2018 address RL in some way, the rationale for collecting proper and\nsufficient information on the topic relied upon search terms such as AI and finance, machine\nlearning and finance, RL and finance, and neural networks in finance, among other important key\nterms. The study focused more on works published in journals such as IEEE, Springer, Procedia\nComputer Science, Research gate, International Journal of Data Science, Information Systems\nResearch, Journal of Social Commerce and arXiv preprint, among others. After gaining exposure\nto a lot of related studies, the next task was to screen the work out for duplication. Initially almost\n200 research publications were retrieved but after removing duplication, around 100 publications\nwere left. Afterwards, we were engrossed in finding the material exclusive to our research in any\nway possible i.e., it could be a misleading topic or abstract that made us add it to our work in the\nfirst place. Through this filtering, 65 full records were remained to be read fully and understood\nfor being added to our work. After that step, it was found that there were few articles out of those\n65 that were not exactly related in a way to be used as a fine material for our survey. Only 19\narticles were found relatively closer to our research study.\n3.2. Data Evaluation\nAfter collecting all the previously available relevant data on the topic being reviewed, the\nnext step was to evaluate the collected articles, papers and research. This step is accessing study\ninclusion/exclusion details and collecting from study information coding. The 65 publications\nthat showed relevance for the full-text study were deeply studied and evaluated. Focusing recent\nVersion April 29, 2023\n11 of 31\nstudies, published works in 2018 and so were considered.We were more focused to work done in\nthe near past so that absolute conclusions and results could be extracted. The studies published\nin 2018 were 21, 17 were from 2019, 8 from 2020, 13 from 2021, and 6 were considered from\n2022. Table 2 shows these figures in a more concise way while Figure 3 presents publications\npercentages with years.\nYear\nNo. of Papers\nPercentage\n2018\n21\n32.3%\n2019\n17\n26.2%\n2020\n8\n12.3%\n2021\n13\n20%\n2022\n6\n9.2%\nTable 2: Studies by publication year\nFigure 3. Publications percentages with years\nComparing these evaluations, most of the work was done in 2018 and there were fewer\nworks in recent years yet those were powerful to be presented as the core of our research.\n3.3. Data analysis and interpretation\nThe most important step was the analysis and interpretation of all the data and gathered\nstudies. This step also included combining the effective results as well as interpreting the analysis\nresults. This collection of results from various sources and research was the way to reach to a\nconcrete conclusion.\n3.4. Results comparison\nThrough tables, pie charts, comparison graphs and schematic representations, the results\ngathered from various studies were combined and compared for a better understanding. These\ncontribute not only to easy access to all the results of different approaches but also made research\ncomprehensible and understandable. The basic idea of review work is mainly to merge various\npreviously adopted approaches and to show them in one representable form, thus this step was\nperformed.\nVersion April 29, 2023\n12 of 31\n3.5. Final write-ups and discussion\nBased on result comparisons, further discussions were made on what was retrieved from all\nof the research work. Exploration of bias and recommendations were made followed by some\nconclusive remarks.\n4. Taxonomy\nThe foremost task of presenting taxonomy of the conducted survey for RL algorithms is the\ncategorization of all RL methods and easy access to the appropriate method for future research.\nThe basic representation of taxonomies is ”is-a relationship” whereas for mereologies, it is ”has-a\nrelationship” [31]. For this reason, the presented taxonomy is high-level to pave an easy way to\nreach all the RL methods.\nFirstly, more generalized discussion starting from machine learning and its types is made in this\nsection. Afterwards, The study captures notable RL algorithms in Fintech all through.\nFollowing Figure 4 shows an illustration of the types of machine learning.\nFigure 4. Types of Machine Learning\n4.1. Supervised Learning\nIt is a type of machine learning with the main objective to train the model on a particular\nspecified set of inputs which results in the establishment of a relationship between the input feature\nand the prediction outputs [32]. Some of the frequently and customarily used supervised learning\nalgorithms are Decision trees, Naive Bayes, Linear Regression Neural Networks and SVM etc.\n4.2. Unsupervised learning\nIn unsupervised learning, the training dataset contains untagged input points data. For\nobtaining useful and meaningful inferences the algorithm detects patterns [33]. Some widely used\nalgorithms for unsupervised learning can be listed as neural networks for association problems\nand K-means etc. Dixon et al. [34] described that UL is used for drawing inferences from the\navailable dataset. The main goal of UL is to understand and absorb the structure of data rather\nthan predict certain values. UL methods can be categorized as either clustering or factor analyses.\n4.3. Semi-supervised learning\nThis type of machine learning, known as semi-supervised learning, lies in the middle of\nsupervised learning and unsupervised learning. Generally in supervised learning, some input\npoints of the data sets are presented as well as the output points corresponding to the given input\npoints. Whereas in unsupervised learning, no specific output value is provided; the underlying\nstructure is tried to infer using the input points. Typically, semi-supervised learning is known for\nVersion April 29, 2023\n13 of 31\nattempting to ameliorate the outcome and performance of these tasks by consuming statistics that\nare linked to others.\n4.4. Deep Learning\nIn deep learning, the main task of an algorithm is to correctly copy the function of the human\nbrain and use it in scientific computing. The most widely used deep learning algorithms are CNNs\n(Convolution neural networks) as shown in figure 5, recurrent networks and LSTM etc. [35].\nFigure 5. Structure of CNNs\n4.5. Reinforcement Learning\nReinforcement Learning (RL) being a type of machine learning is a learning paradigm that\nhas major concerns with controlling a system so that it can escalate and maximize the performance\nmeasures that are used to express a long-term objective. The main discern between supervised\nand RL is that in RL the feedback given about the learner’s prediction to the learner is partial.\nThese predictions are found to have persistent effects which influence the controlled system’s\nfuture condition. RL is of substantial and great interest due to its practical applications in several\ndomains inclusive of Fintech [36]. General taxonomy of RL algorithms can be visualized in\nfigure 6.\nFigure 6. General Taxonomy of RL Algorithms\nAn agent, a policy(𝜋), a reward signal, and a value function are the four primary components\nthat make up a RL system, as shown in Figure 1. In accordance with the provisions of a policy, an\nagent’s behavior at each given instant may be specified. A policy may be thought of as a blueprint\nthat outlines the connections between behavior and perception in a specified environment.\nRL is different from supervised learning (SL) as in RL the agent only learns by interacting with\nVersion April 29, 2023\n14 of 31\nthe environment repeatedly whereas in SL it can learn in one scan of the complete dataset. The\ngeneral method is that at the time t an agent performs an action 𝐴𝑡and a reward 𝑅(𝑡+1) = 𝑅(𝑆𝑡, 𝐴𝑡)\nis given to it, after that, the environment is moved on to the next state. The basic and major task for\nthe agent is to learn a way to react to the environment to maximize the total reward as formulated\nin equation 1, given below;\n𝑉𝜋(𝑆𝑡) =\n∞\n∑︁\n𝑘=0\n𝛾𝑘𝑅𝑡+𝑘+1\n(1)\nHere, the coefficient 𝛾represents the decay factor which usually considers the interest rate in\nfinance [37].\nSystems using RL have excelled in a variety of fields including self-driving automobiles,\nAtari video games, banking, and healthcare [38]. The detailed taxonomy of the RL algorithms\nnoticeably being used in Fintech is given in figure 7.\nFigure 7. Detailed Taxonomy of Model Free RL algorithms notable in Fintech\nIn figure 6, it can be seen that the RL algorithms are divided into Markov Decision Principle\nand Bandits. In the MDP, we have further classification into the model-free RL and model-based\nRL. By calculating the reward and transitional probability function in the MDP context, model-\nbased algorithms sustain an approximation of the MDP model, from which they construct the value\nfunction. The value function is then used to derive the policy. Model-free algorithms work by\ndirect learning without inferring the model, a value function or the quintessential policy whereas\nmodel-based algorithms do not work that way. They are classified into given the model and learn\nthe model. Model-free algorithms are classified into two main categories as shown in figure 7;\nthese are value-based methods and policy-based methods. Model-free RL in Fintech was seen to\nbe practised in a lot of work, such as Sato [39] used it for portfolio optimization. Model-free RL\nis further classified into Value-based methods and Policy-based methods, as discussed below:\n4.5.1. Value-Based Methods\nHere, the method of the traditional value-based algorithm is presented in an infinite time\nhorizon manner with discounting action spaces, finite state and stationary policies. Value-based\nmethods are further divided into on-policy and off-policy methods.\nVersion April 29, 2023\n15 of 31\nOn-policy methods: These approaches focus on studying and improving the policies that\nunderlie decision-making, thus form the foundation of decision-making. These techniques seek\nand make an effort to first provide an estimation, and then to alter the policy that is currently\nbeing used for decision-making.\nHere in On-policy methods, the policy that is being worked on is usually soft by soft it\nmeans;\n𝜋(𝑎|𝑠) > 0 𝑓𝑜𝑟𝑎𝑙𝑙𝑠∈𝑆, 𝑎∈𝐴\n(2)\nOne of the examples of these policies can be seen as 𝜀-greedy policies by this we mean that in\nmost cases they choose those actions which have a maximal estimated action value but when they\nhave a probability 𝜀a random action is selected instead of those actions which have a maximal\nestimated action value [40].\nOne of the most common approaches of On-policy RL, known as SARSA (which stands for\nstate-action-reward-state-action), evaluates the value of the policy that is now being followed.\nDuring this phase of the algorithm, the agent analyzes the situation and determines the best course\nof action to take. There is no difference between the policy that is used for acting and the policy\nthat is used for updating.\nUpdate rule is used to define a state action and reward state action. The update rule can be seen as;\n𝑄(𝑆𝑡, 𝐴𝑡) ←𝑄(𝑆𝑡, 𝐴𝑡) + 𝛼[𝑅𝑡+1 + 𝛾𝑄(𝑆𝑡+1, 𝐴𝑡+1) −𝑄(𝑆𝑡, 𝐴𝑡)]\n(3)\nThis is done from a 𝑆𝑡, nonterminal state, after every transition. 𝑄(𝑆𝑡+1, 𝐴𝑡+1) is set to 0 if is the\nterminal point. Every element of the quintuple i.e, 𝑆𝑡, 𝐴𝑡, 𝑅𝑡+1, 𝑆𝑡+1, 𝐴𝑡+1 which is utilized in the\nmaking of the switching from one to the other state action pair used by the rule stated above. This\ngave rise to the algorithm named SARSA. The factor 𝑞𝜋is being continually estimated for the\npolicy 𝜋and 𝜋is being changed at the same time towards the greediness concerning 𝑞𝜋.When\nyou want to maximize the benefits of an agent that is exploring its environment, using on-policy\nRL may be quite helpful.\nOff-policy methods: In contrast, an off-policy is not affected by the actions of the agent in any\nway. It analyzes the situation and determines the best course of action, irrespective of the agent’s\ngoals. The evaluation of the rewards based on these approaches does not take into account the\nactions that are now being taken. They take into consideration all of the alternative actions that\nmay be taken at the moment in order to maximize the benefit that can be achieved from the next\nstep[41]. The main off-policy method includes Q-learning and Double Q-learning algorithms.\nQ-learning: This algorithm memorizes the value of an action in a particular state. It is\nmodel-free; hence, it doesnâĂŹt need an environment model. Inclusively, it can take care of\nstochastic transition issues and rewards despite requiring adaptations.\nThe Q-learning algorithm is defined by the update rule given below:\n𝑄𝑛𝑒𝑤(𝑆𝑡, 𝐴𝑡) ←𝑄(𝑆𝑡, 𝐴𝑡)𝛼[𝑅𝑡+1 + 𝛾max\n𝑎\n𝑄(𝑆𝑡+1, 𝑎) −𝑄(𝑆𝑡, 𝐴𝑡)]\n(4)\nHere, 𝛼is the learning rate that has values 0 < 𝛼≤1 and 𝑅(𝑡+1) is the reward being gained here\nwhen moving to the state 𝑆(𝑡+1) from the state 𝑆𝑡and 𝛾is the discount factor. On the other hand,\na point that can be seen here is that the new state 𝑄𝑛𝑒𝑤(𝑆𝑡, 𝐴𝑡) is explained below;\n1.\nThe newest value being weighed by the learning rate (1 −𝛼)𝑄(𝑆𝑡, 𝐴𝑡). A fact is that the\nlearning rate values closer to 1 make rapid and fast changes in the Q function.\n2.\nThe second factor is 𝛼𝑅𝑡+1\n3.\nThe third factor that comprises the new state is 𝛾max𝑎𝑄(𝑆𝑡+1, 𝑎): this is the maximum\nreward, weighted by the discount factor and the learning rate that can be achieved from the\nstate 𝑆(𝑡+1).\nHere, the optimal function values are directly approximated by the function Q liberated of the\npolicy that is stuck upon for this reason it constitutes an off-policy algorithm [42]. Since, the\npolicy chooses which state-action pairs are reached and changed, it still has an impact. However\nfor proper convergence, it is necessary that all pairings continue updating. Obtaining the optimal\nVersion April 29, 2023\n16 of 31\nstate value function (its convergence in the deterministic and stochastic settings is ensured by\nQ-Learning) determines the action of the agent rather than finding the optimal policy.\nWith transaction costs taken into account during each re-balancing period, [43] explained that\nby using Q-Learning (without a neural network) in discrete-time market conditions; one may\nmaximize a portfolio that comprises a hazard-free asset (cash) and a risky asset (stock market\nportfolio).\nThe adaptive market hypothesis’s prediction that the market would not be able to accept new\ndetails as rapidly as it arrived led to the development of the Q-learning and SARSA methodologies.\nFor financial trading, [27] also employed Q learning and SARSA and also compared both.\nDouble Q-learning: The fact that the identical Q function, as in the current action selection\npolicy, is utilized for the calculation of future maximum approximated action in Q-learning, it can\noften hyperbolize the action values in noisy environments which results in slowing the learning\n[44]. To solve this problem, another off-policy technique was introduced called Double\nQ-learning. In this technique, a different policy is used for the evaluation of the next value than\nthat which is used for the evaluation of the next action.\nHere in this technique, two separate value functions are trained using separate experiences\nas shown below in a mutually symmetric pattern through equation 5 and equation 6:\n𝑄𝐴\n𝑡+1(𝑆𝑡, 𝐴𝑡) = 𝑄𝐴\n𝑡(𝑆𝑡, 𝐴𝑡) +𝛼𝑡(𝑆𝑡, 𝐴𝑡)(𝑟𝑡+ 𝛾𝑄𝐵\n𝑡(𝑆𝑡+1), 𝑎𝑟𝑔max\n𝑎\n𝑄𝐴\n𝑡(𝑆𝑡+1, 𝑎) −𝑄𝐴\n𝑡(𝑆𝑡, 𝑎𝑡) (5)\nand\n𝑄𝐵\n𝑡+1(𝑆𝑡, 𝐴𝑡) = 𝑄𝐵\n𝑡(𝑆𝑡, 𝐴𝑡) + 𝛼𝑡(𝑆𝑡, 𝐴𝑡)(𝑟𝑡+ 𝛾𝑄𝐴\n𝑡(𝑆𝑡+1), 𝑎𝑟𝑔max\n𝑎\n𝑄𝐵\n𝑡(𝑆𝑡+1, 𝑎) −𝑄𝐵\n𝑡(𝑆𝑡, 𝑎𝑡)\n(6)\nDeep Q Neural Network (DQN)\nThe DQN method builds a matrix to help the agent that is working to choose the precise course of\naction that would maximize its reward in the future. Characterizing a Q-table, however, becomes\nincredibly difficult and time-taking as the number of environmental states and activities rises. The\nstate is provided as the input for deep Q-learning, and the Q-value for each action is created as the\noutput. The next course of action is determined by the Q-maximum network’s output when the\nEpsilon-Greedy Exploration method is used. The Bellman equation is used to update the network\nweights since the goal value is uncertain.\nThese value-based methods were frequently used in finance technology whether be stock trading,\nportfolio management or cryptocurrencies etc.\n4.5.2. Policy-Based Methods\nHere, our target is model-free policy-based methods where a parametrized policy is trained\nwithout inferring the value function. Policy-based methods are further divided into Gradient-free\nand Gradient-Based methods. The methods that were seen to be used in finance technology were\nmainly gradient-based. Actor-Critical Approaches, Trust Region Policy Optimization (TRPO),\nand Proximal Policy Optimization (PPO), Soft Actor-Critic (SAC), among others, are examples of\npolicy-based methods.\nProximal Policy Optimization (PPO): The PPO algorithm uses a policy gradient approach for\nRL. The goal was to create an algorithm that, although utilizing solely first-order optimization,\nhad the data efficiency and dependable performance of TRPO [45]. This algorithm updates\npolicies using the below equation:\n𝜃𝑘+1 = 𝑎𝑟𝑔max\n𝜃\n𝐸\n𝑠,𝑎∼𝜋𝜃𝑘[𝐿(𝑠, 𝑎, 𝜃𝑘, 𝜃)]\n(7)\nHere, 𝐿can be presented as;\n𝐿(𝑠, 𝑎, 𝜃𝑘, 𝜃) = 𝑚𝑖𝑛\n\u0012 𝜋𝜃(𝑎|𝑠)\n𝜋𝜃𝑘(𝑎|𝑠)\n𝐴𝜋𝜃𝑘(𝑠, 𝑎), 𝑐𝑙𝑖𝑝\n\u0012 𝜋𝜃(𝑎|𝑠)\n𝜋𝜃𝑘(𝑎|𝑠)\n, 1 −𝜖, 1 + 𝜖\n\u0013\n𝐴𝜋𝜃𝑘(𝑠, 𝑎)\n\u0013\n(8)\nVersion April 29, 2023\n17 of 31\nWhere, 𝜖is a (small) hyperparameter that generally indicates how far the new policy can deviate\nfrom the previous one.\nSoft Actor-Critic: The Soft Actor-Critic (SAC) technique tides over the gap between stochastic\npolicy optimization and DDPG-style methods by optimizing a stochastic policy in an off-policy\nmanner. Although, it was released roughly concurrently with TD3; it is not a straight replacement\nfor TD3. Nevertheless, it features the clipped double-Q technique and gains from target policy\nsmoothing because of the policy’s intrinsic stochasticity.\nA measure of the randomness of a random variable is called entropy. Let 𝑃be the probability\nmass of the random variable 𝑥:\n𝐻(𝑃) = E\n𝑥∼𝑃[−𝑙𝑜𝑔𝑃(𝑥)]\n(9)\nEach time step in entropy-regularized RL includes an additional reward for the agent. This\ntransforms the RL issue into:\n𝜋∗= 𝑎𝑟𝑔max\n𝜋\nE\n𝜏∼𝜋\n∞\n∑︁\n𝑡=0\n𝛾𝑡(𝑅(𝑠𝑡, 𝑎𝑡, 𝑠𝑡+1) + 𝛼𝐻(𝜋(.|𝑠𝑡)))\n(10)\nThe trade-off coefficient is 𝛼> 0.\n4.6. Deep Reinforcement learning\nDeep Reinforcement learning (DRL) is an amalgamation of the deep learning method and\nRL method. DRL has proved to resolve many complex decision-making and prediction issues\nthat were unsolvable before [46]. It has made its mark in the fields of finance technology to a\ngreat extent. Many works are available in the context of DRL and finance technology. The figure\n8 below shows the basic agent environment interaction in DRL.\nFigure 8. Interaction between Agent and environment in DRL\n5. Review\nReinforcement learning has contributed to optimal execution, Robo-advising (Wealth\nManagement), portfolio management, cryptocurrencies and trading etc.\nThrough the lens of supervised and unsupervised algorithms, RL has participated to the rise\nof trading bots in the financial or stock market. Bots characterized by RL architectures can\nsufficiently learn from the stock and trading market environments while also interacting effectively\nwith them [11]. Specifically, the bots in the trading arena rely on a trial and error approach to\noptimize their learning strategy.\nThe second use case of RL in Fintech is its use in the development of chatbots [25]. Analysis\nindicates that the application of automated chats in the field of finance has produced innumerable\nbenefits to the trade. Specifically, chatbots can effectively serve as brokers while also providing\nreal-time quotes to the user operators [16]. Secondly, conversational user-interface-based chatbots\nhave adequately assisted customers to resolve complex and demanding issues rather than relying\non company staff or backend support. RL in conversational user interfaces has greatly helped\nmultiple firms to save time and relieve the support team from endless and repeatable tasks.\nIn Fintech, RL has also contributed greatly to the optimization of risks Wang et al. [47], particularly\nin peer-to-peer (P2P) lending; P2P lending denotes a means of offering businesses and individuals\nloans via online channels [25]. The online services perform the task of matching lenders with\ninvestors. In the existing marketplaces, RL is important because it can effectively appraise the\nscores of borrowers credit to limit lending risks [8]. Additionally, RL in risk optimization can\nsufficiently predict yearly returns. The rationale for this is that because online commerce is linked\nVersion April 29, 2023\n18 of 31\nwith low overheads, lenders can generally expect better returns than investment products and\nsavings given by financial banks.\nReward functions were used in the field of RL for fintech to improve the predictability level. As\nthe agent’s behaviour is controlled by the reward signal, it is an integral part of the RL paradigm.\nThe \"ought\" behaviour of the agent is described by reward functions. To put it differently, they\ncontain ”normative” material that specifies what you want the agent to do. Therefore, you must\nmake it up to the degree that the reward function establishes the agent’s motives. There are no\nformal restrictions, however, the agent will learn more efficiently if your reward function is \"better\nbehaved.\"\n5.1. RL in Goal-based Wealth Management\nPractices have been made on the portfolio optimization problems using the framework of\nGoal-based wealth management. In contrast to the standard deviation of the portfolios, the risk is\ndefined in this formulation as the likelihood that investors will not achieve their objectives after\na while. However, as demonstrated, there is a mathematical relationship between the original\nmean-variance problem and the GBWM problem. While this problem is static, [48] employed DP\nto solve the long-horizon portfolio problem to solve the dynamic version of the GBWM problem.\nSimilarly, [49] employed the Q learning algorithm and presented some experimental results. There\nwere some inputs to the problem the covariance matrix of returns which were later used for the\ncomputations of 15 portfolios. The outputs were the algorithm used, training epochs and the final\nvalue of the functional outcome.\nThe process encompasses Q learning which was chosen based on the fact that it was using the\nepsilon greedy algorithm for the choosing action. After that Dasa and Varmaa [49] verified the\nresults of the Q learning algorithm by comparing and contrasting the value function given by\nV(W(0),0) at t=0. Furthermore, the 𝛾parameter was used as a discount factor in the Q-learning\nalgorithm with the aim of future rewards. The fact that no discounting is needed for the rewards\nused in the GBWM problem forming 𝛾was set to 1. The window of Q values that were averaged\ncollectively was managed by the parameter. Experimentally, they found that the value of = 0.1,\nwhich corresponds to a moving average over the most recent 10 Q values, performed quite well.\nThis value was shown by plotting the moving average squared difference between the Q-tensors\nfrom different epochs. The algorithm stabilized after 20,000 epochs. There was a final expected\nreward function defined as:\n[W(T-1),T-1]\nThis is known as the value function which is used for each next node and it gave optimal action\nand expected terminal reward.\n5.2. RL in Retirement plan optimization\nDixon and Halperin [50] discussed different RL algorithms for wealth management and\nRobo-advising such as G learner and GIRL algorithms. G learning is a RL method with a\nstochastic policy; it can be seen as an entropy regularized Q learning that is more fitting while\nworking with turbulent data. G learners amount to a generative RL model. In this study, they\nobserved the reward functions of various techniques and reduced them to one non-linear equation.\nThe final obtained equation was;\n𝐺𝜋\n𝑡(𝑥, 𝑎) = ˆ𝑅(𝑥𝑡, 𝑎𝑡) + E𝑡,𝑎\n\u0014 𝛾\n𝛽𝑙𝑜𝑔\n∑︁\n𝑎𝑡+1\n𝜋0(𝑎𝑡+1|𝑥𝑡+1)𝑒𝛽𝐺𝜋\n𝑡+1(𝑥𝑡+1,𝑎𝑡+1)\n\u0015\n(11)\nThe term \"G-learning\" refers to an off-policy time-difference (TD) technique that generalizes\nQ-learning to noisy settings where an entropy-based regularization is acceptable. It may be used\nto express G-learning for finite values in a RL scenario with observed rewards.\nThe experimentation of solving the optimization problem was made by the researcher. They\nended with the results and findings that both the discussed algorithm can either be used separately\nor collectively. In particular, by modelling the real human agents as G-learners and then using\nGIRL to deduce the latent aims of these G-leaners, their amalgamation might be employed in\nVersion April 29, 2023\n19 of 31\nRobo-advising. After successfully simulating the top human investors, GIRL will be able to\nprovide clients with a Robo-advising service that will allow them to outperform the best investors\noverall.\n5.3. RL and cryptocurrency price prediction\nCryptocurrencies have earned the title of the famous key factor in the business as well as\nfinancial opportunities during recent advancements. The major activity here is that with regards\nto the volatility of high prices and inconsistency in the market the cryptocurrency investment is\nnot visible.\nBased on problems with the vivid previous approaches to price prediction, [51] presented a\nmachine-learning (RL) technique for price prediction for some financial institutions. They\npresented an outline of the price prediction process which can be seen in Figure ??.\nThe system proposed by [51] constitutes a RL algorithm for the prediction and analysis of the price\nas well as their work also ensures a secure transaction environment via a blockchain framework.\nThe result of the work showed that the accuracy gained through the RL algorithm in the price\nprediction was even better than other state of art algorithms.\nTrading cryptocurrencies has recently grown in importance and popularity [52]. Research revealed\nthat there are three important data sources contained in the cryptocurrency price prediction. A\nmarket statistic makes up the first one. The second is blockchain network information, which\nincludes hash rate, transaction count, and fee information. Google tweet and trend package are\nthe final two [53]. It is to be observed that whatever actions we take whether good or bad if the\nreward function gains high rewards then that action was good.\nFigure 9. Price prediction process outline\nThe work done contributed to the following;\n1.\nApplication of RL for the prognosis of the prices of Litecoin and Monero coins.\n2.\nEvaluation of the performance of the system using the matrices e.g. Mean Absolute error\n(MAE), Mean absolute percentage error (MAPE), Mean squared error (MSE) and Root\nmean squared error (RMSE).\n3.\nUtilization of framework of blockchain for the secure and certain environment for predicting\nprices.\nThe training and testing dataset for Litecoin and Monero was divided 80% and 20% respectively.\nResults were evaluated for the normal and their proposed model. The results were captured with\ntime frames of 3, 7 and 30 days and results were gained for the values of the coins for MAE,\nMAPE, MSE and RMSE. Tables 3, 4, 5 and 6 show the results presented by the work of [51].\nVersion April 29, 2023\n20 of 31\nModel\nCoin Type\nMean Squared Error(MSE)\nNormal\nLitecoin\n3 days\n7 days\n1 month\n196.5063\n28.2988\n287.2785\nMonero\n232.0476\n31.9216\n524.8219\nProposed\nLitecoin\n6.3949\n5.2429\n21.8329\nMonero\n11.8142\n31.3669\n410.9197\nTable 3: MSE values for 3, 7 and 30 days prediction\nModel\nCoin Type\nRoot Mean Squared Error(RMSE)\nNormal\nLitecoin\n3 days\n7 days\n1 month\n14.0572\n6.3255\n17.9273\nMonero\n16.1076\n6.6618\n23.9958\nProposed\nLitecoin\n3.3097\n3.1438\n5.6632\nMonero\n4.3826\n6.6116\n21.3548\nTable 4: RMSE values for 3, 7 and 30 days prediction\nVersion April 29, 2023\n21 of 31\nModel\nCoin Type\nRoot Mean Squared Error(RMSE)\nNormal\nLitecoin\n3 days\n7 days\n1 month\n14.0572\n5.3749\n15.8082\nMonero\n16.1076\n4.9481\n20.7854\nProposed\nLitecoin\n3.3097\n2.6536\n4.9246\nMonero\n4.3826\n5.8589\n20.6614\nTable 5: MAE values for 3, 7 and 30 days prediction\nModel\nCoin Type\nRoot Mean Squared Error(RMSE)\nNormal\nLitecoin\n3 days\n7 days\n1 month\n22.5067\n7.4219\n16.9552\nMonero\n23.3272\n6.5516\n20.2365\nProposed\nLitecoin\n4.0048\n3.1692\n5.9518\nMonero\n5.1838\n7.3865\n20.4594\nTable 6: MAPE values for 3, 7 and 30 days prediction\n5.4. Contributions of RL to trading in the stock market\nThe analyzed studies have also significantly contributed to the effectiveness of the overall\nstock-trading environment in the financial markets. [16] found that RL has greatly contributed\nto setting effective price strategies.\nDynamic and complex changes in stock price emerge\nas some of the major challenges in aligning and understanding stock prices.\nAccordingly,\nunderstanding the underlying concepts requires the use of RL models or algorithms such as\nthe gates Recurrent Unit (GRU) networks. The algorithm or network can perform well based on RL.\nThe survey findings have also highlighted the powers of RL in the field of finance by\ndescribing its actual correlation with the maximization of profits using minimal investments in the\ncapital [8]. Combining all the principles of RL, portfolio management, recommendation systems,\nstrategic setting of prices, and risk optimization means that RL has greatly contributed to the\nfield of finance in diverse ways [11]. In capital management, for instance, strategic planning and\nautomation are the ways that the use of RL technology has fostered the welfare of trading and the\nstock market not only in the United States but also globally.\nA special data augmentation technique is introduced by [54] through which the data set would be\nincreased by an adequate amount and more precise and correct training would be performed which\nwould contribute to satisfactory results. They further chose the method based on skewness and\nkurtosis to choose stocks that were to be traded with the usage of the presented algorithm. The\nexperimentation showed that the model proposed by them gave decent returns as results compare\nto the buy and hold strategy. Figure 10 presents the agent environment interaction.\nVersion April 29, 2023\n22 of 31\nFigure 10. Interaction between environment and agent\nRewards: The so-called returns, rather than the actual prices, are typically examined to\nunderstand the evolution of stock market prices and create lucrative portfolios. Returns, which\nrepresent the relative change in price over each time step, are a more reliable method than utilizing\nactual prices since they have undergone normalization, which improves generalization across\nvarious stocks.\nThe experimentation continued in a way that they selected 3 stocks from the 11 stocks. These\nthree stocks were YZGF (603886. SH), KLY (002821. SZ), and NDSD (300750. SZ) from\nthe Tushar database. Annualized return (AR) and Sharp ratio (SR) were used by them as the\ngauging metrics and the results were also obtained accordingly. The final portfolio value indicates\nthe final trading stage portfolio value. The direct return of the portfolio every year is shown by\nthe annualized return. The annualized standard error demonstrates our model’s stability. The\nevaluation is made using the Sharpe ratio, which combines return and risk [55].\nThe benefit of Annualized return usage was its capability to put various periods into the same\nscale which was useful in comparing different stock returns using different agents. On the other\nhand, the Sharp ratio can be referred to as a risk-adjusted return which uses the return 𝑅𝑝, the\nstandard deviation and risk-free rate 𝑅𝑓of excess return 𝜎𝑝. The formula is:\n𝑆𝑅= 𝑅𝑃−𝑅𝑓𝜎𝑝\nThe RL algorithms that were used were the DQN, Proximal Policy Optimization and Soft\nActor-Critic.\nBuy and hold was the baseline set for the trading strategy. It is used for the securities that are\nbeing held for quite a long period. If you purchase and hold, it can be because you think the\nshort-term volatility that comes with the stock investment will be worth it for the long-term profits\n[56]. It is a passive investing style which can be simply summarized in a way that B&H strategy\nusers believe in the importance of time in the market than timing the market. That being said\nholding on to stock is easy than timing the market perfectly.\nFor comparing with B&H they set 0 as the risk-free rate. The sharp ratio is a relative metric which\nwas implicated in comparing the risk-adjusted returns of various trading plans.\nVersion April 29, 2023\n23 of 31\nFigure 11. Comparison results of YZGF test set\nFigure 12. Comparison results of KLY test set\nVersion April 29, 2023\n24 of 31\nFigure 13. Comparison results of NDSD test set\nResults of (Yuan, Wen and Yang, 2020) can be seen in figures 11, 12 and 13 which showed\nthat Proximal policy optimization was more stable and accurate than the other methods. The main\nreason for this conclusion was that it’s SR and AR were outperforming the buy and hold strategy\ncontinuously. In contrast to training results, DQN and SAC consistently outperform the B&H\nmethod in SR and frequently outperform it in AR.\nAdditionally, DQN or SAC occasionally achieve the highest SR of all strategies or the highest\nrisk-adjusted return. It is important to note that their training dataset’s stock and period are\ncompletely different from those of the testing dataset. It can therefore be concluded that the\nagents’ excess return can be attributable to the market law they discovered on their own where\nneither time nor stock affects the law. In experimentation, PPO is a steady algorithm; nonetheless,\nDQN and SAC occasionally get greater AR and SR.\nA study performed by [57] spread light on the topic of RL in stock trading. Their work studied\nnot only studied the usage of RL algorithms in stock trading but also evaluated the approach to\nreal-world stock data. They compared their work algorithm (RL techniques) with other techniques\nutilized in the world of stock prediction. They believed that in a market where each next step is\nunpredicted, their work based on RL will make nearly accurate decisions for stock trading.\nExperimentation began with them evaluating three variants of Deep Q-learning which were\nvanilla Q-learning [58], Double DQN [44,59] and Dueling Double DQN [60]. Their data set\nincluded daily stock prices for more the seven thousand US-based stocks that were collected up\nto 10 November 2017. The testing period for each stock was set from 1st January 2017 to 10th\nNovember 2017 whereas the training time was from 1st January 2015 to 31st December 2016.\nThis gave them a total of 504 training and 218 testing samples.\nResults of the work revealed that DQN compared to Dueling DQN and Double DQN yielded\nthe highest average profit. The results obtained were consistent with the other studies too [61].\nDQN produced larger volatility than the other two approaches, as expected. They showed the\nbreakdown of profits produced by each model. It was obvious that Double Q-Network was a\nbusiness, hence occasionally it made a loss.\nIn the paper [57], they examined how DQN was used in stock trading. They tested Deep\nQ-effectiveness Network’s using sizable real-world datasets. With DQN, they can trade stocks\nwithout extra optimization steps like with other supervised learning approaches. RL algorithm\nvariations based on Q-learning can produce strategies that, on average, make a profit using only a\nsmall number of samples.\nVersion April 29, 2023\n25 of 31\n5.5. Effectiveness of RL in portfolio management\nPortfolio management is also another major theme emerging in the connection between\nRL and Fintech. From the analyzed literature, [11] offer the best argument about the actual\nimplication of DRL and portfolio management. The findings indicate that with the assistance of\nDeep Policy Network RL, possibilities of asset optimization allocation can be achieved over time.\nRL, in this regard, can significantly assist in achieving three major goals. The first goal is that RL\ncan promote the success and efficiency levels of human managers. The second goal is that RL can\nlessen organizational vulnerabilities. The third objective is that RL can foster the levels of return\non investments (ROIs) regarding organizational profits.\nAn asset allocation method termed meta policy was introduced by Jangmin O et al. in 2006 to\ndynamically adjust the asset allocation in a portfolio to optimise returns [62]. A general illustration\nof a portfolio management system can be seen in figure 14.\nFigure 14. A general illustration of the Portfolio Management System\nThe financial world has recently noticed a rise in the popularity of RL models [35].\nThe cardinality constraint, floor and ceiling constraints, round-lot restriction, pre-assignment\nconstraint, and class constraint are a few of the real-world limitations that practitioners of portfolio\noptimization take into account. [63].\nIn their study from 2020, Wang et al made the first effort to leverage an algorithmic trading-related\nconcept. His development of an HRPM is a specific example. Two decision processes were\narranged in a hierarchy in their system. While the low-level policy chooses at what price and\nwhat quantity to place the bid or ask orders within a condensed time window to accomplish\nthe high-level policy’s objectives, the high-level policy modifies portfolio weights at a lower\nfrequency.\nTo address portfolio optimization issues, both value-based techniques (Q-learning, SARSA, and\nDQN) and policy-based algorithms (DPG and DDPG ) have been used. The state variables are\nrepeatedly made up of duration, property prices, property former returns, current asset holdings,\nand the balance that is currently available.\nFor value-based algorithms, [43] considered the portfolio optimization problems of a risky asset\nand a risk-free asset. The Sharpe ratio, differential Sharpe ratio, and profit were three different\nvalue functions that were used to assess the performance of the Q-learning Recurrent RL (RRL)\nalgorithm.\nThe reward 𝑟(𝑠, 𝑎, 𝑠\n′) was calculated that is the change in the portfolio value when at a state 𝑠and\naction 𝑎is taken that makes it arrive at a new state 𝑠\n′. Figure 15 shows recurrent RL in action.\nVersion April 29, 2023\n26 of 31\nFigure 15. Illustration of Recurrent reinforcement learning in portfolio management\nThe last action is used as an input by the policy-based RRL algorithm. They concluded that\nthe Q-learning algorithm performs less consistently than the RRL method and is more susceptible\nto the selection of the value function. Furthermore, it is concluded that the profit should not be\nthe preferred reward function, but rather the (differential) Sharpe ratio.\n6. Results And Discussion\nThis survey discusses and summarizes different aspects and facets on the role of RL in\nFintech including complexity, scalability, accuracy, profitability, risk management and speed etc.\nPRISMA guideline for reporting systematic reviews is utilized to conduct this study which help to\npresent more complete reporting of systematic reviews. Among the 65 studies selected for the\ncurrent research survey, only 19 studies accurately focused on the interconnection between RL\nand FinTech. Using PRISMA approach, 19 shreds of literature are chosen for comprehensive\nanalysis, addressing the issues, merits, demerits, and future of RL in the finance or monetary\nmarket. The results revealed that the study on the challenges and potentials of RL in Fintech has\nnot been fully explored and researched comprehensively.\nTo be focused on the research purpose, along with the most appropriate studies associated with the\nstudy topic, 13 specific studies contributed significantly in understanding the connection between\nRL and the Fintech sector. The rationale for considering these thirteen sources as appropriate to\nthe current study objectives is that they satisfy all the keyword and scholarly study factors required\nto any research survey. Table 7 shows the details of these studies, meeting the requirements of the\nsurvey paper.\nOur work indicated that RL in the fields of Fintech was useful compared to other classical\nand traditional methods. RL stock trading techniques were mostly Model-Free, specifically Value-\nand Policy-based. The results achieved more accuracy than those obtained using conventional\nfintech strategies such as Buy and Hold. A little work was done concerning RL-based techniques\nin cryptocurrency predictions and goal-based wealth management systems. Most of the work\nwas observed in Portfolio optimization using RL and gave significant results in contrast to other\nalgorithms.\nIn addition, reinforcement learning was able to simulate more efficient models with more realistic\nmarket constraints, yet there were limitations in terms of scalability of the model. DRL further\naddresses the RL algorithms’ scalability issue, which is essential for rapid market and user\ndevelopment and logically works with greatly wanted high-dimensional settings in the financial\nmarket.\nReinforcement learning has much potential, but it may be challenging to apply and only has a few\nuses. Deployment issues arise from this type of machine learning’s dependence on environment\ninvestigation. For instance, a robot that uses reinforcement learning to navigate a complex physical\nenvironment would look for new states and develop new behaviors. However, it might not be easy\nto continuously make the proper choices considering how quickly the atmosphere shifts in the real\nworld. Due to the time needed to guarantee that the learning is carried out correctly, the method’s\nVersion April 29, 2023\n27 of 31\nTable 7: Studies meeting the survey paper requirements\nAuthor(s)\nJournal Title\nPublication\nKuo et al. [8]\nImproving generalization in reinforcement\nlearningâĂŞbased trading by using a gener-\native adversarial market model\nIEEE\nLiu et al. [11]\nAdaptive quantitative trading: An imitative\ndeep reinforcement learning approach\nProceedings of the AAAI conference on\nartificial intelligence\nShi et al. [16]\nGPM: A graph convolutional network based\nreinforcement learning framework for port-\nfolio management\nElsevier\nHu and Lin [18]\nDeep Reinforcement Learning for Optimiz-\ning Finance Portfolio Management\nIEEE\nYang et al. [24]\nPersonalizing debt collections: Combin-\ning reinforcement learning and field experi-\nment.\n1st International Conference on Information\nSystems (ICIS 2020)\nKhuwaja et al. [25]\nAdversarial Learning Networks for Fin-\nTech applications using Heterogeneous Data\nSources\nIEEE\nDixon and Halperin [50]\nG-learner and girl: Goal based wealth man-\nagement with reinforcement learning.\nArXiv Preprint ArXiv:2002.10990.\nShahbazi and Byun [51]\nImproving the cryptocurrency price predic-\ntion performance based on reinforcement\nlearning.\nIEEE\nYuan et al. [54]\nPersonalizing debt collections: Combining\nreinforcement learning and field experiment\n41st International Conference on Informa-\ntion Systems (ICIS 2020): Making Digi-\ntal Inclusive: Blending the Local and the\nGlobal\nDang [57]\nReinforcement Learning in Stock Trading\nSpringer\nLe et al. [64]\nApplications of Machine Learning (ML)-\nThe actual situation of the Vietnam Fintech\nMarket\nJournal of Social Commerce\nWang et al. [65]\nDeep Stock Trading: A Hierarchical Rein-\nforcement Learning Framework for Portfo-\nlio Optimization and Order Execution.\narXiv preprint arXiv:2012.12620.\nDas and Varma [66]\nDynamic Goals-Based Wealth Management\nUsing Reinforcement Learning\nJournal Of Investment Management (JOIM)\nVersion April 29, 2023\n28 of 31\ninsignificance and demand on computer resources may be constrained.\nAnother challenge reinforcement learning faces in fostering financial decision-making and\noperations is the limited samples of data available to make appropriate policies, strategies, and\ndecisions. Kuo et al. [8] refer to sample efficiency as an algorithm retrieving the most resources\nfrom a specific sample. Furthermore, sample efficiency denotes the experience level that an\nalgorithm should produce in a training session to obtain efficient performance. The major challenge\nor issue is that reinforcement learning is assumed to take considerable time to reach efficiency.\nLiu et al. [11] contends that because the action space and state space could be unprecedentedly\nhuge, it is not feasible to ask for a sample size that exceeds the fundamental thresholds established\nby the ambient tabular settings specific to a dimension.\n7. Conclusion\nAlthough reinforcement learning has captivated a lot of consciousness in the field of AI, there\nare still many limitations to its adoption and use in the actual world. Despite this, several research\narticles on its theoretical applications present some practical use cases. As long as a distinct\nreward is available, reinforcement learning can be applied to a circumstance. Reinforcement\nlearning algorithms in FinTech can distribute scarce resources across various tasks as long as a\nbroad objective is pursued. Saving time or preserving resources would be an objective in this\nsituation. Reinforcement learning has been used in a few restricted experiments in robotics.\nRobots using this type of machine learning may be able to learn skills that a human supervisor\ncannot demonstrate, apply previously taught skills to new tasks, or optimize even without an\nanalytical formulation. The financial sector aims to continue getting benefits significantly from the\nreinforcement learning technology. The findings of the conducted study, based on the PRISMA\ntechnique which aimed to provide complete reporting of systematic reviews, indicate that the\nreinforcement learning in Fintech has allowed financial companies to optimize their portfolio,\nreduce credit risk, effectively manage a portfolio, set effective price strategies, and reduce capital.\nDespite its strengths, reinforcement learning has always been overlooked. However, the current\nstudy shows obvious merits of reinforcement learning by highlights some of its best use cases.\nReferences\n1.\nJameel, F.; Sharma, N.; Khan, M.A.; Khan, I.; Alam, M.M.; Mastorakis, G.; Mavromoustakis, C.X.\nMachine Learning Techniques for Wireless-Powered Ambient Backscatter Communications: Enabling\nIntelligent IoT Networks in 6G Era. In Internet of Things; Springer, 2020; pp. 187–211.\ndoi:\n10.1007/978-3-030-44907-0{\\_}8.\n2.\nKhushi, M.; Dean, I.M.; Teber, E.T.; Chircop, M.; Arthur, J.W.; Flores-Rodriguez, N. Automated\nclassification and characterization of the mitotic spindle following knockdown of a mitosis-related\nprotein. BMC Bioinformatics 2017, 18. doi:10.1186/s12859-017-1966-4.\n3.\nKhushi, M.; Choudhury, N.; Arthur, J.W.; Clarke, C.L.; Graham, J.D. Predicting Functional Interactions\nAmong DNA-Binding Proteins. In Lecture Notes in Computer Science (including subseries Lecture\nNotes in Artificial Intelligence and Lecture Notes in Bioinformatics); Springer International Publishing,\n2018; Vol. 11305 LNCS, pp. 70–80. doi:10.1007/978-3-030-04221-9{\\_}7.\n4.\nMalibari, N.; Katib, I.; Mehmood, R. Predicting Stock Closing Prices in Emerging Markets with\nTransformer Neural Networks: The Saudi Stock Exchange Case. International Journal of Advanced\nComputer Science and Applications 2021, 12, 876–886. doi:10.14569/ĲACSA.2021.01212106.\n5.\nCharpentier, A.; Élie, R.; Remlinger, C.\nReinforcement Learning in Economics and Finance.\nComputational Economics 2021. doi:10.1007/s10614-021-10119-4.\n6.\nHuang, X.; Li, D. A Two-level Reinforcement Learning Algorithm for Ambiguous Mean-variance\nPortfolio Selection Problem.\nProceedings of the Twenty-Ninth International Joint Conference\non Artificial Intelligence; International Joint Conferences on Artificial Intelligence Organization:\nCalifornia, 2020; Vol. 2021-Janua, pp. 4527–4533. doi:10.24963/ĳcai.2020/624.\n7.\nJoseph M. Carew. What is Reinforcement Learning? A Comprehensive Overview, 2022.\n8.\nKuo, C.H.; Chen, C.T.; Lin, S.J.; Huang, S.H. Improving generalization in reinforcement learning-\nbased trading by using a generative adversarial market model. IEEE Access 2021, 9, 50738–50754.\ndoi:10.1109/ACCESS.2021.3068269.\n9.\nBhatt, S. Things you need to know about Reinforcement Learning. Retrieved July 5, 7, 2019.\nVersion April 29, 2023\n29 of 31\n10.\nLe, H.D.; Nguyen, G.H.; Bui, A.T.; Le, T.Q.; Nguyen, T.H. Applications of Machine Learning (ML) -\nThe real situation of the Vietnam Fintech Market. Journal of Social Commerce 2022, 2, 60–70. doi:\n10.56209/jommerce.v2i2.28.\n11.\nLiu, Y.; Liu, Q.; Zhao, H.; Pan, Z.; Liu, C. Adaptive Quantitative Trading: An Imitative Deep\nReinforcement Learning Approach. Proceedings of the AAAI Conference on Artificial Intelligence\n2020, 34, 2128–2135. doi:10.1609/aaai.v34i02.5587.\n12.\nJiang, X.; Pan, S.; Jiang, J.; Long, G. Cross-Domain Deep Learning Approach for Multiple Financial\nMarket Prediction. Proceedings of the International Joint Conference on Neural Networks 2018,\n2018-July, 1–8. doi:10.1109/ĲCNN.2018.8489360.\n13.\nCai, Q.; Cui, C.; Xiong, Y.; Wang, W.; Xie, Z.; Zhang, M. A Survey on Deep Reinforcement Learning\nfor Data Processing and Analytics. IEEE Transactions on Knowledge and Data Engineering 2022, pp.\n1–1. doi:10.1109/TKDE.2022.3155196.\n14.\nSerrano, W. Fintech Model: The Random Neural Network with Genetic Algorithm.\nProcedia\nComputer Science 2018, 126, 537–546. doi:10.1016/j.procS.2018.07.288.\n15.\nMousavi, S.S.; Schukat, M.; Howley, E. Deep Reinforcement Learning: An Overview. In Lecture\nNotes in Networks and Systems; 2018; Vol. 16, pp. 426–440. doi:10.1007/978-3-319-56991-8{\\_}32.\n16.\nShi, S.; Li, J.; Li, G.; Pan, P. A multi-scale temporal feature aggregation convolutional neural network\nfor portfolio management. International Conference on Information and Knowledge Management,\nProceedings 2019, pp. 1613–1622. doi:10.1145/3357384.3357961.\n17.\nBazarbash, M. FinTech in Financial Inclusion: Machine Learning Applications in Assessing Credit\nRisk. IMF Working Papers 2019, 19, 1. doi:10.5089/9781498314428.001.\n18.\nHu, Y.J.; Lin, S.J. Deep Reinforcement Learning for Optimizing Finance Portfolio Management. 2019\nAmity International Conference on Artificial Intelligence (AICAI). IEEE, 2019, number Dl, pp. 14–20.\ndoi:10.1109/AICAI.2019.8701368.\n19.\nWang, Y.; Yao, Q.; Kwok, J.; Ni, L.M. Generalizing from a Few Examples: A Survey on Few-Shot\nLearning. ACM Computing Surveys 2019, 53, 1–34. doi:10.1145/3386252.\n20.\nHambly, B.M.; Xu, R.; Yang, H. Recent Advances in Reinforcement Learning in Finance. SSRN\nElectronic Journal 2021, pp. 1–60. doi:10.2139/ssrn.3971071.\n21.\nHendershott, T.; Zhang, X.; Leon Zhao, J.; Zheng, Z. Fintech as a game changer: Overview of research\nfrontiers. Information Systems Research 2021, 32, 1–17. doi:10.1287/isre.2021.0997.\n22.\nCao, L.; Yang, Q.; Yu, P.S. Data science and AI in FinTech: an overview. International Journal of\nData Science and Analytics 2021, 12, 81–99. doi:10.1007/s41060-021-00278-w.\n23.\nTian, X.; He, J.S.; Han, M. Data-driven approaches in FinTech: a survey, 2021. doi:10.1108/IDD-06-\n2020-0062.\n24.\nYang, H.; Liu, X.Y.; Zhong, S.; Walid, A. Deep reinforcement learning for automated stock trading.\nProceedings of the First ACM International Conference on AI in Finance; ACM: New York, NY, USA,\n2020; pp. 1–8. doi:10.1145/3383455.3422540.\n25.\nKhuwaja, P.; Khowaja, S.A.; Dev, K. Adversarial Learning Networks for FinTech applications\nusing Heterogeneous Data Sources.\nIEEE Internet of Things Journal 2021, 2327, 1–8.\ndoi:\n10.1109/JIOT.2021.3100742.\n26.\nLagna, A.; Ravishankar, M.N. Making the world a better place with fintech research. Information\nSystems Journal 2022, 32, 61–102. doi:10.1111/isj.12333.\n27.\nCorazza, M.; Sangalli, A. Q-Learning and SARSA: A Comparison between Two Intelligent Stochastic\nControl Approaches for Financial Trading. SSRN Electronic Journal 2015. doi:10.2139/ssrn.2617630.\n28.\nJain, N. Survey versus interviews: Comparing data collection tools for exploratory research. Qualitative\nReport 2021, 26, 541–554. doi:10.46743/2160-3715/2021.4492.\n29.\nMohammad, S.M.; Jetty, D.V. Exploratory Research on Simulation Hypothesis and the research\nmethod analysis 2021. 8, 180–184.\n30.\nPage, M.J.; McKenzie, J.E.; Bossuyt, P.M.; Boutron, I.; Hoffmann, T.C.; Mulrow, C.D.; Shamseer,\nL.; Tetzlaff, J.M.; Akl, E.A.; Brennan, S.E.; Chou, R.; Glanville, J.; Grimshaw, J.M.; Hróbjartsson,\nA.; Lalu, M.M.; Li, T.; Loder, E.W.; Mayo-Wilson, E.; McDonald, S.; McGuinness, L.A.; Stewart,\nL.A.; Thomas, J.; Tricco, A.C.; Welch, V.A.; Whiting, P.; Moher, D. The PRISMA 2020 statement:\nan updated guideline for reporting systematic reviews. Systematic Reviews 2021, 10, 1–11. doi:\n10.1186/s13643-021-01626-4.\n31.\nPrudencio, R.F.; Maximo, M.R.O.A.; Colombini, E.L. A Survey on Offline Reinforcement Learning:\nTaxonomy, Review, and Open Problems 2022. pp. 1–21.\n32.\nRay, S. A Quick Review of Machine Learning Algorithms.\nProceedings of the International\nConference on Machine Learning, Big Data, Cloud and Parallel Computing: Trends, Prespectives and\nProspects, COMITCon 2019 2019, pp. 35–39. doi:10.1109/COMITCon.2019.8862451.\nVersion April 29, 2023\n30 of 31\n33.\nSimeone, O. A Very Brief Introduction to Machine Learning with Applications to Communication\nSystems. IEEE Transactions on Cognitive Communications and Networking 2018, 4, 648–664. doi:\n10.1109/TCCN.2018.2881442.\n34.\nDixon.; Al Mazrouei, F.; Nobanee, H. Machine Learning in Finance: A Mini-Review. SSRN Electronic\nJournal 2020, 1406. doi:10.2139/ssrn.3539038.\n35.\nMosavi, A.; Ghamisi, P.; Faghan, Y.; Duan, P.; faizollahzadeh ardabili, s.; Salwana, E.; Band, S.\nComprehensive Review of Deep Reinforcement Learning Methods and Applications in Economics.\nSSRN Electronic Journal 2020. doi:10.2139/ssrn.3711731.\n36.\nDayan, P.; Niv, Y. Reinforcement learning: The Good, The Bad and The Ugly. Current Opinion in\nNeurobiology 2008, 18, 185–196. doi:10.1016/j.conb.2008.08.003.\n37.\nSutton, R.S.; Barto, A.G. Reinforcement learning: An Introduction Second edition. Learning 2012,\n3, 322.\n38.\nAlharin, A.; Doan, T.N.; Sartipi, M. Reinforcement learning interpretation methods: A survey. IEEE\nAccess 2020, 8, 171058–171077. doi:10.1109/ACCESS.2020.3023394.\n39.\nSato, Y. Model-Free Reinforcement Learning for Financial Portfolios: A Brief Survey 2019. pp. 1–20.\n40.\nMeng, T.L.; Khushi, M. Reinforcement Learning in Financial Markets. Data 2019, 4, 110. doi:\n10.3390/data4030110.\n41.\nPendharkar, P.C.; Cusatis, P. Trading financial indices with reinforcement learning agents. Expert\nSystems with Applications 2018, 103, 1–13. doi:10.1016/j.eswa.2018.02.032.\n42.\nWatkins, C.J.C.H.; Dayan, P. $\\$cal Q-learning. Machine Learning 1992, 8, 279–292.\n43.\nDu, X.; Zhai, J.; Lv, K. Algorithm trading using q-learning and recurrent reinforcement learning.\npositions 2016, 1.\n44.\nVan Hasselt, H. Double Q-learning. Advances in Neural Information Processing Systems 23: 24th\nAnnual Conference on Neural Information Processing Systems 2010, NIPS 2010 2010, pp. 1–9.\n45.\nSchulman, J.; Levine, S.; Moritz, P.; Jordan, M.; Abbeel, P. Trust region policy optimization. 32nd\nInternational Conference on Machine Learning, ICML 2015. PMLR, 2015, Vol. 3, pp. 1889–1897.\n46.\nFrançois-Lavet, V.; Henderson, P.; Islam, R.; Bellemare, M.G.; Pineau, J. An introduction to deep\nreinforcement learning; Vol. 11, 2018; pp. 219–354. doi:10.1561/2200000071.\n47.\nWang, R.; Wei, H.; An, B.; Feng, Z.; Yao, J. Deep Stock Trading: A Hierarchical Reinforcement\nLearning Framework for Portfolio Optimization and Order Execution 2020.\n48.\nDas, S.R.; Ostrov, D. A NEW APPROACH TO GOALS-BASED 2018. 16, 4–30.\n49.\nDasa, S.R.; Varmaa, S. Dynamic Goals-Based Wealth Management using Reinforcement Learning.\nJournal Of Investment Management 2020, 18, 1–20.\n50.\nDixon, M.F.; Halperin, I. G-Learner and GIRL: Goal Based Wealth Management with Reinforcement\nLearning. SSRN Electronic Journal 2020, pp. 1–20. doi:10.2139/ssrn.3543852.\n51.\nShahbazi, Z.; Byun, Y.C. Improving the cryptocurrency price prediction performance based on\nreinforcement learning. IEEE Access 2021, 9, 162651–162659. doi:10.1109/ACCESS.2021.3133937.\n52.\nLiew, J.; Li, R.; Budavári, T.; Sharma, A. Cryptocurrency Investing Examined. The Journal of the\nBritish Blockchain Association 2019, 2, 1–12. doi:10.31585/jbba-2-2-(2)2019.\n53.\nGupta, R.; Tanwar, S.; Al-Turjman, F.; Italiya, P.; Nauman, A.; Kim, S.W. Smart Contract Privacy\nProtection Using AI in Cyber-Physical Systems: Tools, Techniques and Challenges. IEEE Access\n2020, 8, 24746–24772. doi:10.1109/ACCESS.2020.2970576.\n54.\nYuan, Y.; Wen, W.; Yang, J. Using data augmentation based reinforcement learning for daily stock\ntrading. Electronics (Switzerland) 2020, 9, 1–13. doi:10.3390/electronics9091384.\n55.\nXiong, Z.; Liu, X.Y.; Zhong, S.; Yang, H.; Walid, A. Practical Deep Reinforcement Learning Approach\nfor Stock Trading 2018.\n56.\nHui, E.C.M.; Chan, K.K.K. Alternative trading strategies to beat âĂĲbuy-and-holdâĂİ. Physica A:\nStatistical Mechanics and its Applications 2019, 534, 120800.\n57.\nDang, Q.v. Reinforcement Learning in Stock Trading To cite this version : HAL Id : hal-02306522\nReinforcement Learning in Stock Trading 2019.\n58.\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.; Wierstra, D.; Riedmiller, M.\nPlaying Atari with Deep Reinforcement Learning 2013. pp. 1–9.\n59.\nVan Hasselt, H.; Guez, A.; Silver, D. Deep reinforcement learning with double q-learning. Proceedings\nof the AAAI conference on artificial intelligence, 2016, Vol. 30.\n60.\nWang, Z.; Schaul, T.; Hessel, M.; Van Hasselt, H.; Lanctot, M.; De Frcitas, N. Dueling Network\nArchitectures for Deep Reinforcement Learning. 33rd International Conference on Machine Learning,\nICML 2016 2016, 4, 2939–2947.\n61.\nHester, T.; Deepmind, G.; Pietquin, O.; Lanctot, M.; Schaul, T.; Horgan, D.; Quan, J.; Sendonaris, A.;\nDulac-Arnold, G.; Agapiou, J.; Leibo, J.Z. Deep Q-Learning from Demonstrations Matej Vecerik Bilal\nVersion April 29, 2023\n31 of 31\nPiot Ian Osband Audrunas Gruslys. Proceedings of the AAAI Conference on Artificial Intelligence\n2018, pp. 3223–3230.\n62.\nJangmin, O.; Lee, J.; Lee, J.W.; Zhang, B.T. Adaptive stock trading with dynamic asset allocation\nusing reinforcement learning. Information Sciences 2006, 176, 2121–2147.\n63.\nAlmahdi, S.; Yang, S.Y. An adaptive portfolio trading system: A risk-return portfolio optimization\nusing recurrent reinforcement learning with expected maximum drawdown. Expert Systems with\nApplications 2017, 87, 267–279. doi:10.1016/j.eswa.2017.06.023.\n64.\nLe, H.; Vial, L.; Frej, J.; Segonne, V.; Coavoux, M.; Lecouteux, B.; Allauzen, A.; Crabbé, B.; Besacier,\nL.; Schwab, D. FlauBERT: Unsupervised language model pre-training for French. LREC 2020 - 12th\nInternational Conference on Language Resources and Evaluation, Conference Proceedings 2020, pp.\n2479–2490.\n65.\nWang, H.; Can, D.; Kazemzadeh, A.; Bar, F.; Narayanan, S. A System for Real-time Twitter Sentiment\nAnalysis of 2012 U.S. Presidential Election Cycle 2012. pp. 8–14.\n66.\nDas, S.R.; Varma, S. DYNAMIC GOALS-BASED WEALTH MANAGEMENT 2020. 18, 1–20.\n",
  "categories": [
    "q-fin.CP",
    "cs.AI",
    "cs.LG",
    "q-fin.GN"
  ],
  "published": "2023-04-29",
  "updated": "2023-04-29"
}