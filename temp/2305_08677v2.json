{
  "id": "http://arxiv.org/abs/2305.08677v2",
  "title": "Natural Language Decomposition and Interpretation of Complex Utterances",
  "authors": [
    "Harsh Jhamtani",
    "Hao Fang",
    "Patrick Xia",
    "Eran Levy",
    "Jacob Andreas",
    "Ben Van Durme"
  ],
  "abstract": "Designing natural language interfaces has historically required collecting\nsupervised data to translate user requests into carefully designed intent\nrepresentations. This requires enumerating and labeling a long tail of user\nrequests, which is challenging. At the same time, large language models (LLMs)\nencode knowledge about goals and plans that can help conversational assistants\ninterpret user requests requiring numerous steps to complete. We introduce an\napproach to handle complex-intent-bearing utterances from a user via a process\nof hierarchical natural language decomposition and interpretation. Our approach\nuses a pre-trained language model to decompose a complex utterance into a\nsequence of simpler natural language steps and interprets each step using the\nlanguage-to-program model designed for the interface. To test our approach, we\ncollect and release DeCU -- a new NL-to-program benchmark to evaluate\nDecomposition of Complex Utterances. Experiments show that the proposed\napproach enables the interpretation of complex utterances with almost no\ncomplex training data, while outperforming standard few-shot prompting\napproaches.",
  "text": "Natural Language Decomposition and Interpretation\nof Complex Utterances\nHarsh Jhamtani\nHao Fang\nPatrick Xia\nEran Levy\nJacob Andreas\nBen Van Durme\nMicrosoft Semantic Machines <sminfo@microsoft.com>\nAbstract\nDesigning natural language interfaces has his-\ntorically required collecting supervised data to\ntranslate user requests into carefully designed\nintent representations. This requires enumer-\nating and labeling a long tail of user requests,\nwhich is challenging. At the same time, large\nlanguage models (LLMs) encode knowledge\nabout goals and plans that can help conversa-\ntional assistants interpret user requests requir-\ning numerous steps to complete. We introduce\nan approach to handle complex-intent-bearing\nutterances from a user via a process of hier-\narchical natural language decomposition and\ninterpretation. Our approach uses a pre-trained\nlanguage model to decompose a complex ut-\nterance into a sequence of simpler natural lan-\nguage steps and interprets each step using the\nlanguage-to-program model designed for the\ninterface. To test our approach, we collect and\nrelease DeCU—a new NL-to-program bench-\nmark to evaluate Decomposition of Complex\nUtterances.1 Experiments show that the pro-\nposed approach enables the interpretation of\ncomplex utterances with almost no complex\ntraining data, while outperforming standard\nfew-shot prompting approaches.\n1\nIntroduction\nNeural sequence models, pre-trained on large\ndatasets of language and code, are extremely ef-\nfective at parsing natural commands into programs,\ndatabase queries, and other structured representa-\ntions of user intent (Chen et al., 2021; Li et al.,\n2021; Shin et al., 2021; Roy et al., 2022). How-\never, developing an interface that enables a user\nto interact with a new API or software system still\nrequires substantial system-specific data collection.\nUsers, meanwhile, may not be aware of the scope\nof this data collection, and pursue an open-ended\nset of goals – including goals more complicated\nthan those anticipated by system designers.\n1Our code and DeCU dataset will be released.\nIn this paper, we present DECINT, an approach\nto decompose complex utterances into a sequence\nof simpler NL steps, each resembling a simpler\nelementary utterance that an existing language-to-\nprogram interpreter for the NL interface can parse\nto a sub-program. Consider the utterance “Ex-\nchange the timing of my meetings with Jane and\nSmith” (Figure 1). DECINT breaks the utterance\ndown into four NL steps, using a pre-trained LLM\nand just a few annotated decompositions. The gen-\nerated NL steps are parsed into programs, relying\nprimarily on a relevant (to the step being parsed)\nsubset of a larger set of existing elementary ut-\nterances associated with simpler programs in the\ntarget representation. DECINT thus enables an NL\ninterface system to handle user requests represent-\ning complex goals (never seen by a semantic parser)\nby breaking them into a series of NL steps that are\ninterpreted into APIs (never seen by an LLM). Our\nwork is related to recent work which demonstrates\nthat large language models (LLMs) encode knowl-\nedge that can be used to interpret complex user\ngoals requiring numerous steps to complete, in se-\ntups such as question answering (Wolfson et al.,\n2020; Khot et al., 2022) and embodied agents (Ahn\net al., 2022; Huang et al., 2022). Compared to\nsuch past work, we are concerned with generating\nprograms in a carefully designed intent representa-\ntion. Starting with labeled elementary utterances,\nwe wish to be able to parse complex utterances\nthat are broader in scope compared to the abundant\nelementary utterances.\nTo study utterance decomposition in NL-to-\nprogram space, we collect and release DeCU—\na new benchmark dataset to evaluate models for\nDecomposition of Complex Utterance. DeCU con-\nsists of (1) a set of elementary utterances and corre-\nsponding programs for managing calendar events\nand emails and (2) a diverse set of complex user\nutterances annotated with decompositions into se-\nquences of elementary utterances and their cor-\narXiv:2305.08677v2  [cs.CL]  8 Jan 2024\nChange the end times for all my meetings in \nthis week to end earlier by 5 minutes\nRename all meetings that I have with a PM  \nthis month to be called project sync.\nI need to swap the calls that are on Monday \nand Tuesday.\nIf I don’t have an email about ship room, then \nset up a 1:1 with Smith for this.\nComplex Utterance: Exchange the timing of my meetings with  \n                                     Jane and Smith\nProgram (with step-by-step decomposition): \nStep 1: Find the meeting with Jane \nval s1 = theEvent(with_(“Jane”)) \n \nStep 2: Find the meeting with Smith \nval s2 = theEvent(with_(“Smith”)) \n \nStep 3: Update the event s1 to use start and end time of event s2 \nval s3 = modifyEvent(s1, startsAt(s2.start) and endsAt(s2.end)) \n \nStep 4: Update the event s2 to use start and end time of event s1 \nval s4 = modifyEvent(s2, startsAt(s1.start) and endsAt(s1.end))\nElementary Utterances and Programs\nUtterance:  Find my event with Jesse and Kelly? \nProgram:    val s1 = theEvent(with_(“Jesse”) and with_(“Kelly”))\nUtterance:  Rename the title of this morning's meeting to “Q&A” \nProgram:    val s1 = modifyEvent(theEvent(queryAt(morning on `this`[Date])), called(“Q&A”)) \nUtterance:  Schedule a meeting that ends at 3pm tomorrow \nProgram:    val s1 = createEvent(endsAt(3.pm) on tomorrow) \nUtterance:  Find my shiproom emails \nProgram:    val s1 = findEmails(messageTitleIs(“shiproom”))\n…\nComplex Utterances\n…\nFigure 1: Parsing NL user utterances into programs. We study a scenario in which a large number of elementary\nutterances have been annotated with programs (top block), and we wish to build a model that can generalize to\ncomplex utterances (bottom blocks) requiring more elaborate programs. We introduce a method called DECINT\nthat uses an LLM to decompose a complex utterance by predicting simpler NL steps, each of which is parsed to a\nprogram according to the annotated elementary utterances.\nresponding program fragments. Experiments on\nDeCU show that DECINT outperforms direct few-\nshot prompting approaches, making it possible to\nbuild NL interfaces that accomplish complex goals\nwithout large amounts of complex labeled data.\n2\nTask Overview\nWe study the problem of parsing an NL user ut-\nterance x into a program y that correctly reflects\nuser intent (Figure 1). We focus on a version of the\nproblem with the following characteristics:\n• A domain developer has already collected a\ndataset of elementary utterances annotated\nwith corresponding programs. These utter-\nances represent narrow user goals associated\nwith simple and short programs.\n• At test time, the system must interpret com-\nplex utterances.\nSuch utterances require\nlonger programs representing much broader\nuser goals.\n• For a small number of complex utterances, we\nhave access to annotations consisting of both\nnatural language decompositions into elemen-\ntary utterances, and program annotations for\nelementary utterances.\nAnnotated complex utterances will in general\ncover only a small part of the space of possible\nuser requests, and our goal is to build a language-\nto-program model that can generalize to requests\nof very different kinds (Figure 1).\n3\nData\nMany existing relevant decomposition datasets fo-\ncus on open-ended QA (Wolfson et al., 2020; Khot\net al., 2021, 2022) or robotics domains with a rela-\ntively small number of fixed allowed actions (Puig\net al., 2018; Shridhar et al., 2020). By contrast, we\nare interested in the task of parsing a user utter-\nance to a program that represents the actions to be\ntaken by the interface, grounded on a large number\nof fixed APIs. Moreover, we want to study how\ncomplex user utterances can be supported by the\nNL interface, without collecting a large amount of\nadditional labeled data, by using decomposition in\nNL space. To study such multi-step complex intent\ndecomposition, we introduce a new dataset we call\nDeCU (Decomposition of Complex Utterances).\nThe utterances in DeCU focus on calendar\nevents and emails. The dataset contains both el-\nementary utterances and complex utterances. El-\nementary utterances (§3.2) are paired with declar-\nUtterance 1:\nChange my meetings with Abby and those with Dan this week to start 5 minutes later.\nDecomposition:\nStep\n1:\nFind\ne v e n t s\nwith\nAbby\nt h i s\nweek\nval s1 = findEvents(with_(\"Abby\") and queryAt(`this `[Interval[Date]] and isWeek))\nStep\n2:\nFind\ne v e n t s\nwith Dan and\nw it h o u t\nAbby\nt h i s\nweek\nval s2 = findEvents(with_(\"Dan\") and not(with_(\"Abby\")) and\nqueryAt(`this `[Interval[Date]] and isWeek))\nStep\n3:\nSet\na l l\nmeetings\nfrom\nthe\nl i s t\nof\ne v e n t s\ns1\nto\ns t a r t\n5 minutes\nl a t e r\nval s3 = s1.map((x: Event) => modifyEvent(x, startsAt(x.start.local.time +\n5. minutes)))\nStep\n4:\nSet\na l l\nmeetings\nfrom\nthe\nl i s t\nof\ne v e n t s\ns2\nto\ns t a r t\n5 minutes\nl a t e r\nval s4 = s2.map((x: Event) => modifyEvent(x, startsAt(x.start.local.time +\n5. minutes)))\nUtterance 2:\nDecline any meeting invitations that are scheduled during my weekly team meeting.\nDecomposition:\nStep\n1:\nFind\nthe\nevent\nc a l l e d\n\" team\nmeeting \"\nt h a t\nr ecur s\nweekly .\nval s1 = theEvent(called(\"team meeting\") and recurringWeekly)\nStep\n2:\nFind\na l l\ne v e n t s .\nval s2 = findEvents0\nStep\n3:\nF i l t e r\ne v e n t s\nfrom\nl i s t\ns2\nto\nonly\ni n c l u d e\nones\nt h a t\ni n t e r s e c t\nwith\nevent\ns1\nt h a t\nare\nnot\ns1 .\nval s3 = s2.filter ((x: Event) => x.interval.intersects(s1.interval) && x.id != s1.id)\nStep\n4:\nDecline\ne v e n t s\nin\nthe\nl i s t\ns3 .\nval s4 = s3.map((x: Event) => respond(x, ResponseStatusType.declined))\nFigure 2: Examples of complex utterances in DeCU. Each utterance is accompanied by decompositions consisting\nof a sequence of NL steps and associated program fragments, annotated by domain experts.\native Scala3 programs based on a domain library\n(§3.1) that admits a fixed set of APIs and specified\ntypes. Complex utterances (§3.3) are annotated\nwith a corresponding sequence of elementary ut-\nterances, each paired with a program. Only a few\nof these complex utterances are included in the\ntraining set; they are mainly used to form a test set.\nFigure 1 illustrates an example, “Exchange the\ntiming of my meetings with Jane and Smith”. How\nsuch an utterance should be decomposed is domain-\ndependent: here, the calendar API does not provide\na single endpoint that can swap pairs of meetings;\ninstead, the system must search for the two meet-\nings individually, then update each of their times.\nFigure 1 shows a possible decomposition into four\nsteps. The first generated NL step, “Find the meet-\ning with Jane”, is translated to a program fragment:\nval s1 = theEvent(with_(“Jane”)). Individ-\nual steps typically represent easier-to-solve inputs\nfor the NL-to-program parser that primarily relies\non the annotated elementary utterances.\nIn addition to domain-specific knowledge of\nAPIs, decomposition of complex utterances often\nrelies on domain-general reasoning and common\nsense knowledge – for example, to avoid double-\ncounting meetings that match two search results\n(Figure 2, utterance 1), or to recognize that meet-\nings cannot conflict with themselves (utterance 2).\n3.1\nDomain Library\nThe domain library defines the set of types and\nfunctions available for program annotations. Types\nmodel objects such as Person and Event, whereas\nfunctions represent actions that can be taken\nby the agent, including high-level APIs (e.g.,\ncreateEvent, findEmails), low-level operations\n(e.g., min, +), predicate constructors (e.g., called,\nstartsAt), etc. The domain library for DeCU is\npackaged as standard Scala source code, consisting\nof 33 types and over 200 functions.2\n3.2\nElementary Utterances\nDeCU contains 841 elementary utterances paired\nwith programs. A few examples are shown in the\ntop box in Figure 1. These utterances are elemen-\ntary in that they represent narrow user goals such\nas creating or deleting a single meeting, which can\ntypically be achieved using a single API. As such,\nthey have relatively short programs, generally less\nthan 5 tokens.3 Examples are written and reviewed\nby domain experts who are familiar with the do-\nmain library (on account of their experience from\nworking with a deployed system leveraging such a\nlibrary) and annotation guidelines.\n3.3\nComplex Utterances\nTo study how complex utterances can be supported\nby an NL interface, we collect a diverse set of more\ninvolved user requests, and annotate these with\n2Some built-in types (e.g., String, Boolean), functions\n(e.g., map), and control flow statements (e.g., if) are not explic-\nitly defined and counted. Appendix B provides more details.\n3To compute this statistic, programs are split into tokens\nbased on heuristics, treating API names, argument names, and\nvalues as individual tokens.\n…\nStep by Step Decomposition\nComplex Utterance: Exchange the timing of my meetings with Jane and Smith \nStep 4: Update event s2 to start and end time of \nthe event s1\nval s4 = modifyEvent(s2,  \n  startsAt(s1.start) and endsAt(s1.end))\nNext NL Step \nGeneration\nParsing the Last \nGenerated Step\nStep 1: Find the meeting with Jane \nval s1 = theEvent(with_(“Jane”))  \n \nStep 2: Find the meeting with Smith \nval s2 = theEvent(with_(“Smith”))  \n \nStep 3: Update event s1 to start and end time of \nevent s2 \nval s3 = modifyEvent(s1,  \n  startsAt(s2.start) and endsAt(s2.end))\nA. K (=10) number of Complex Utterance Decomposition examples.\nB. M (<=25) number of Elementary Utterances similar to “Change event s2 to start \nand end time of event s1”, chosen from a larger set.\nComplex Utterance: Check if John has accepted our meeting tomorrow and if not \nthen add John's manager to the call \n \nStep 1: Find my meeting with John tomorrow \nval s1 = theEvent(with_(“John”) and queryAt(tomorrow)) \n \nStep 2: If John has not accepted the event s1 then update the event s1 to add his \nmanager \nval s2 = Option.when(!s1.attendees.isAttending(thePerson(“John”))) { \n  modifyEvent(s1, with_(thePerson(“John”).manager)) }\nUtterance:  Change the title of this morning's meeting to \"Q&A\"  \nProgram:    val s1 = modifyEvent(theEvent( \n             queryAt(morning on `this`[Date])), called(“Q&A”)) ​\nUtterance:  If list of events s2 is empty then update event s1 to end at 2:30 pm. \nProgram:    val s3 = Option.when(s2.isEmpty) { \n             modifyEvent(s1, endsAt((2 :: 30).pm)) }         \nFigure 3: DECINT maps complex utterances into elementary steps, each of which is parsed in sequence to arrive at a\nfinal program. NL decomposition and program generation steps are interleaved. While parsing a step, up to M\nsimilar examples of elementary utterances are retrieved.\ndecompositions into elementary steps, along with\nprograms for each step. As the name suggests, com-\npared to elementary utterances, these utterances\nrepresent more complex and broader user goals,\nwith the corresponding programs typically being\nmuch longer (an average of 14.5 tokens per pro-\ngram). To collect complex utterances, we employ a\nmix of manual authoring and automated utterance\ngeneration. Manual authoring is performed by do-\nmain experts with a focus on diversity and goals\nthat require the composition of multiple calls to the\ndomain APIs. For automated collection techniques,\nwe generate utterances using GPT-3 (Brown et al.,\n2020) prompted with a few random examples of\nmanually-authored utterances. About 60% of all\nthe collected utterances were generated automati-\ncally. Appendix A provides more details on utter-\nance collection. Examples are shown in Figure 1.\nDecomposition Annotations:\nSix annotators fa-\nmiliar with the domain (annotators had past experi-\nence working with the domain library) decompose\ncomplex utterances into elementary ones. When\nresults from earlier steps must be reused, these\nNL decompositions may include explicit reference\nto earlier step outputs (Figure 2). More informa-\ntion about annotator instruction is provided in Ap-\npendix A. Each annotation was additionally re-\nviewed by two additional domain experts, separate\nfrom the set of 6 annotators.\nData Statistics:\nWe collected a total of 210\nunique complex utterances. The dataset is a mix of\n126 utterances paired with annotated programs and\n84 that are unannotated. As discussed later, in ad-\ndition to reference-based metrics, we also provide\nvarious reference-less metrics that do not require\nannotations. While it is a relatively small count,\nnote that most of the data (200 out of 210) is used to\nconstruct an evaluation set, as we are interested in\nlearning to generalize from very small numbers of\ntraining examples. Annotated complex utterances\nin our full dataset exhibit a diverse range of prop-\nerties (an utterance can have multiple): 55% use\na map operation (for-loop), 36% contain actions\nbased on a condition, 31% use a filter operation,\n24% query about calendar/email, 37% contain a\ncreate meeting action, 9% contain a delete meeting\naction, and 31% contain a modify meeting action.\nThe average number of decomposition steps in our\ndata is 3, with a maximum of 7 steps. The aver-\nage number of tokens in each program is 14.54,\nwhile the average number of tokens in the program\nfragment corresponding to a single step is 4.8. For\ncomparison, the average number of tokens in the\nprograms for elementary utterances is 4.5.\n4\nApproach\nThe DECINT approach, illustrated in Figure 3,\nmaps a complex utterance x to a sequence of inter-\npretable lower-level NL steps (z1, z2, ..) that resem-\nble elementary utterances. Each step or low-level\nutterance zj is parsed into a program fragment yj.\nIn particular, DECINT maps from commands to\nprograms according to the following iterative gen-\nerative process:\n1. Natural Language Decomposition:\nzj ∼p(· | x, z<j, y<j).\n2. Program Generation:\nyj ∼p(· | x, z≤j, y<j).\nNL Decomposition (§4.1) and program generation\n(§4.2) steps are interleaved, with later portions of\nthe language decomposition conditioned on earlier\nprogram fragments. In principle, one could also\ncondition on the return values of the earlier pro-\ngram fragments (see Limitations section). We do\nnot do so in this paper, as running the programs\nwould require API implementations and input data.\n4.1\nNatural Language Decomposition\nThe NL decomposition stage generates the next\nNL step zj conditioned on the user utterance x and\nany previously generated steps and program frag-\nments. We obtain zj by greedy decoding from a\npre-trained LLM in a few-shot in-context learning\nsetup (Brown et al., 2020). The model is prompted\nwith K = 10 example decompositions, each of\nwhich consists of an utterance x followed by any\nprevious steps and their program fragments, all con-\ncatenated together (x, z1, y1, z2, , ..., zN, yN). We\nadditionally found it useful to include a list of up\nto M elementary NL utterances at the start of the\nprompt (before the K decomposition examples),\nselecting the ones with highest BM25 similarity\nto the input utterance. This is intended to inform\nthe model about the kind of elementary steps the\nNL-to-program parser can handle. (An example\nconstructed prompt is shown in Appendix C.1.) Ex-\nample decompositions are taken from the set of 10\ncomplex utterances in the training split of DeCU.\nDECINT’s ability to perform NL decomposi-\ntion thus results from a combination of parametric\nknowledge about the structure of programs in gen-\neral (the result of pretraining) and non-parametric\nknowledge about the domain of interest (obtained\nvia in-context learning). Together, these enable\ngeneralization to structurally novel user requests.\nFor example, there are no training examples that\ninvolve exchanging the timing of two meetings (the\ntest example in Figure 3), but DECINT nonetheless\nsynthesizes a correct program.\n4.2\nProgram Generation\nThe program generation step synthesizes a program\nfragment yj for a given NL step zj, conditioned on\nany preceding steps and incomplete program. This\nis a well-studied semantic parsing problem, and we\ndesign the NL-to-program parser largely follow-\ning past work that applies pre-trained LLMs. We\nuse in-context learning with dynamically selected\nprompt examples from the set of elementary exam-\nples data (Brown et al., 2020). As before, we use\ngreedy decoding. Generated program fragments\nmay refer to previously generated fragments using\nnamed step variables. For a given NL utterance or\nstep, we identify up to M examples from the set\nof elementary utterances, where each example is\nan (utterance, program) pair (as shown in box B in\nFigure 3). The selection of the examples is based\non the similarity of the utterance to the intermedi-\nate NL step being parsed. To compute similarity,\nwe again use BM25, as in past work (Rubin et al.,\n2022; Roy et al., 2022). In pilot experiments on\ntraining data, we discovered it was useful to also in-\nclude the K decomposition examples at the bottom\nof the prompt (detailed prompt example shown in\nAppendix C.1). This may be because the decompo-\nsition examples provide a demonstration of how to\ngenerate program fragments for a step conditioned\non previous steps and help bridge any possible do-\nmain shift from elementary to complex utterances.\n4.3\nBaselines\nThe DECINT method decomposes a complex utter-\nance into NL steps, separately parsing each step,\nand using internal variable references to assemble\na larger program. The standard few-shot prompting\napproach for tasks like this one (e.g., Roy et al.,\n2022) instead directly predicts the parse without\ngenerating the intermediate NL steps. We compare\nto this approach, which we denote DIRECT-PRED,\nin our experiments. There are a few key differ-\nences compared to the DECINT method. Complex\nutterance examples are presented without the in-\ntermediate NL steps (i.e., each utterance is paired\nwith a multi-line program). The output generation\nis a single-step process since there are no interme-\ndiate NL steps that need to be generated. As with\nDECINT, examples of elementary utterances are\nalso included in the prompt. We also consider a\nCOT (Wei et al., 2022) baseline, wherein the model\nfirst predicts all intermediate NL steps and then\npredicts the program. Accordingly, the complex\nutterance examples in the prompt are annotated\nwith intermediate steps. This baseline resembles\nthe method proposed in Jiang et al. (2023). Note\nthat compared to COT, DECINT interleaves step\ngeneration and parsing, and dynamically updates\nthe subset of exemplars from elementary utterances\nto be relevant to the step being parsed.\nWe also report results using a variant of DECINT\nthat relies only on K decomposition exemplars\nbut without access to elementary utterances (M=0\ninstead of 25). We refer to such a baseline as FEW-\nSHOT. Finally, we also report results for a variant\nof DECINT that uses only a single decomposition\nexemplar (K=1 instead of 10), and thus relies al-\nmost entirely on the elementary utterances from\nthe underlying domain. We refer to the variant as\nELEMENTARY-ONLY.\n5\nExperiments\n5.1\nEvaluation\nOverlap with Reference Programs:\nWe report\nExact Match (EM) and character-based edit dis-\ntance (CER) metrics5 against the gold program.\nBefore computing these metrics, we normalize the\nprograms by lowercasing the entire program and\nremoving extra spaces. Since there can be multi-\nple possible ways to express the target multi-line\nprogram, Exact Match can only be viewed as a\nlower-bound metric for this task. These metrics are\nreported only for the subset of the data that consists\nof annotated reference programs.\nWell-formed Evaluation:\nAdditionally, we re-\nport the fraction of predictions that are valid\n(WellForm) under the domain library, i.e., the full\nprogram follows correct syntax and only uses func-\ntions available in the library. Note that WellForm\ndoes not necessarily represent correctness with re-\nspect to the user goal. We report the metric for the\nentire test set.\nProgram Correctness:\nFinally, we report the\noverall correctness of the generated programs. We\ndefine a program to be correct overall if: it is\nwell-formed, and correctly represents the user re-\nquest.\nWe use GPT-4 (gpt-4-32k) (OpenAI, 2023)\nto rate the correctness of the generated programs\n(Correct). The prompt consists of an instruction\nand four manually labeled exemplars (two “correct”\n5https://huggingface.co/spaces/evaluate-metric/cer\nSystem\nCorrect↑WellForm↑EM↑CER↓\nDIRECT-PRED\n0.34\n0.36\n0.04\n0.44\nCOT\n0.25\n0.29\n0.05\n0.46\nFEW-SHOT\n0.13\n0.19\n0.00\n0.50\nELEMENTARY-ONLY\n0.23\n0.31\n0.04\n0.54\nDECINT\n0.41\n0.46\n0.05 0.40\nTable 1: Quality of the generated program for complex\nutterances under various automated metrics.\nand two “incorrect”) followed by the test exam-\nple. Each example is a user utterance followed\nby the associated program. The label is a natu-\nral language caption/explanation of the generated\nprogram, followed by a final verdict on whether\neither the generated program is “correct” or “in-\ncorrect” for the given user utterance – following a\nchain-of-thought style prediction6. Since we have\nan automatic static analysis to infer exactly which\nprograms are well-formed (WellForm), outputs that\nare not well-formed are automatically considered\nto be incorrect as per the definition above (but are\nincluded in the denominator for all evaluations).\nNote that the Correct metric is reference-less, is\neasier to scale than human evaluations, and corre-\nlates well with human ratings (Section 5.3).\n5.2\nSetup\nWe consider the task of parsing complex utterances\nin DeCU given only ten complex utterances (anno-\ntated with decompositions) to be used as training\ndata (exemplars for in-context learning). We report\nresults on the test set consisting of the remaining\n200 complex utterances. We use a maximum of\nM ≤25 additional elementary utterances (as many\nas permitted by the LM’s context window) selected\naccording to BM25 similarity with the step being\nparsed. We use OpenAI’s text-davinci-003 model\nas the LLM for generating each NL step as well as\nfor parsing it into a program.\n5.3\nEvaluation of Generated Program\nTable 1 reports various automated metrics. DECINT\noutperforms all the baselines, sometimes by a wide\nmargin. As can be seen in the table, DECINT out-\nputs receive an overall correctness score (Correct)\nof 41% for complex utterances compared to 34%\nand 25% for the baselines DIRECT-PRED and\nCOT respectively.7\nWe posit that DECINT is\n6The exact prompts used in Correct are presented in Ap-\npendix D\n7The differences are significant (p < 0.05) using bootstrap\nresampling.\nUtterance 1:\nMake me a meeting at 3 PM mountain time and refuse any meetings that happen at the same\ntime.\nDECINT\nStep\n1:\nFind\ne v e n t s\nat 3 pm mountain\ntime\nval s1 = findEvents(queryAt (3.pm inZone TimeZone(\"MST\")))\nStep\n2:\nRefuse\nany\ne v e n t s\nin\nthe\nl i s t\ns1\nval s2 = s1.map(x => respond(x, withResponse(ResponseStatusType.declined)))\nStep\n3:\nCreate a meeting\nat 3 pm mountain\ntime\nval s3 = createEvent(createAt (3.pm inZone TimeZone(\"MST\")))\nDIRECT-\nPRED\nval s1 = createEvent(createAt (3.pm inZone TimeZone(\"MST\")))\nval s2 = findEvents(queryAt (3.pm inZone TimeZone(\"MST\")))\nval s3 = s2.map(x => refuseEvent(x))\nCOT\nStep\n1:\nCreate a meeting\nat 3 pm mountain\ntime\nStep\n2:\nRefuse\nany\nmeetings\nt h a t\nhappen\nat\nthe\nsame\ntime \"}\nval s1 = createEvent(createAt (3.pm inZone TimeZone (\\\"MST \\\")))\nval s2 = findEvents(queryAt (3.pm inZone TimeZone (\\\" MST \\\"))).map(x => refuseEvent(x))\nComment\nWe observe that hallucination of functions is much more prominent in the baselines DIRECT-\nPRED and COT, as can be seen in the use of the non-existing function refuseEvent\nUtterance 2:\nI’d like to divide the meeting at noon in two, so split off the last fifteen minutes as a separate\nevent.\nDECINT\nStep\n1:\nFind\nthe\nevent\nat\nnoon\nval s1 = theEvent(createAt(noon))\nStep\n2:\nS u b t r a c t\n15\nminutes\nfrom\nthe\nend\ntime\nof\nthe\nevent\ns1\nval s2 = s1.end - 15. minutes\nStep\n3:\nUpdate\nthe\nevent\ns1\nto\nend\nat\nthe\ntime\ni n s t a n t\ns2\nval s3 = modifyEvent(s1, endsAt(s2))\nStep\n4:\nCreate a new event\ns t a r t i n g\nat\nthe\ntime\ni n s t a n t\ns2\nval s4 = createEvent(startsAt(s2))\nComment\nOutput program from DECINT was judged as incorrect since it doesn’t specify the duration of\nthe second event.\nFigure 4: Outputs from DECINT and baselines on sample complex utterances in DeCU.\nable to make more effective use of pretraining\nby breaking down a complex command into NL\nsteps and retrieving relevant exemplars for each\nstep. Further, FEW-SHOT, that is equivalent to\nDECINT with M=0, fares badly, suggesting that\nDECINT relies on information from elementary ut-\nterances in addition to supervised decompositions.\nFinally, ELEMENTARY-ONLY, which is equiva-\nlent to DECINT with K=1, also does worse than\nDECINT, suggesting the usefulness of a handful of\nsupervised decompositions. Note, however, that a\n54% of the predictions from DECINT are not well-\nformed, indicating that even structural generaliza-\ntion in DeCU remains a major challenge. Nonethe-\nless, DECINT fares better compared to other meth-\nods on WellForm metric.\nHuman Evaluation for Program Correctness:\nWe also obtained the overall program correctness\nrating (“correct\" vs “incorrect\" for a user utterance)\nfrom human evaluators familiar with the domain\nlibrary. Just as was the case with Correct met-\nric, outputs that are not well-formed are automat-\nically considered incorrect. The aggregate scores\nfor DECINT, DIRECT-PRED and COT (our method\nand the two top performing baselines as per auto-\nmated Correct metric) under human evaluation are\n41%, 33% and 26% respectively, which are very\nclose to the scores for these methods under the\nautomated Correct metric. Additionally, we ob-\nserve a high correlation between human annotator-\nprovided judgment and Correct judgments (a more\ndetailed correlation analysis is provided in the Ap-\npendix D).\nResults with other LLMs: We also report results\nusing GPT-4 (gpt-4-32k) and LLAMA-2-70B (Tou-\nvron et al., 2023) as the underlying LLM. Due to\ncost considerations, we report results only for the\ntop three methods from Table 1. We observe that\nDECINT outperforms the baselines, demonstrating\nthat the proposed approach is effective across un-\nderlying LLMs (Table 2).\nSystem\nCorrect↑/ WellForm↑\nGPT-4\nLLAMA2-70B\nDIRECT-PRED\n0.35 / 0.39\n0.25 / 0.41\nCOT\n0.37 / 0.40\n0.23 / 0.32\nDECINT\n0.49 / 0.56\n0.35 / 0.50\nTable 2: Results using GPT-4/LLAMA-2-70B as the\nunderlying LLM.\n5.4\nEvaluation of NL Decomposition\nWe measure whether the NL decomposition steps\naltogether are sufficient and correct to complete\nthe user request.8 For example, the output from\nDECINT for the second utterance in Figure 4 is not\nsufficient and correct because the fourth step fails\nto specify the duration of the meeting, which is\nsupposed to be 15 minutes as per user request. A\nrandom subset of 40 of DECINT NL predictions\nand corresponding expert annotations were man-\nually labeled by one of the authors as correct or\nincorrect. The expert annotations and DECINT\npredictions were rated as 98% and 85% correct, re-\nspectively. Future work can explore ways to further\nimprove the accuracy of the predicted NL steps.\nWe also conducted a step-level evaluation, which\nwe discuss in Appendix D.\n5.5\nQualitative Analysis\nWe provide example predictions in Figure 4, with\nadditional examples provided in Figure A4 and\nFigure A6 in the Appendix. Additionally, we per-\nform an error analysis of the NL-to-program step\nof DECINT. We restrict the study to the predic-\ntions that were labeled as incorrect in Table 1. The\nmost common issues are those that make the pro-\ngram not well-formed, as summarized in Table 1.\nMany errors are due to nonexistent APIs / API ar-\nguments (21% of the incorrect programs have at\nleast this problem) and nonexistent type attribute\n(43%). A smaller number result from even more\nbasic syntax errors and type mismatches (17%). Fu-\nture work could constrain the outputs of the parser\n(Shin et al., 2021) to only use allowed functions\nand follow correct syntax, though such approaches\ncan substantially increase the cost of decoding.\nA few errors result from predictions that capture\nonly partial user intent (6%). For example, for\nutterance 2 in Figure 4, the prediction does not cap-\nture the user intent of creating the second event for\n15 minutes. Many of the remaining errors involve\nmore fundamental semantic mismatches between\nuser intents and model outputs. For example, for\n“Loop around all my 1/1 meetings this week so that\nthey also happen next week”, the prediction up-\ndates the meetings this week instead of creating\nanother set of meetings next week.\n8Unless stated otherwise, all analysis uses outputs with\ntext-davinci-003 as the underlying LLM\n6\nRelated Work\nPast work has explored using command decompo-\nsition to break down complex tasks or requests into\nsmaller subtasks that are easier to manage. The\nLaMDA model (Thoppilan et al., 2022), for exam-\nple, is capable of breaking down “How to” type\nqueries into steps. However, generated steps are\nnot tied to any actions or APIs, and are more in the\nform of a narrative rather than executable steps.\nKhot et al. (2021) decompose a question into sub-\nquestions that can be answered by a neural factoid\nsingle-span QA model and a symbolic calculator.\nDrozdov et al. (2022) decompose an utterance us-\ning a syntactic parse. However, not all utterances\nin our dataset would lend to such a style of decom-\nposition, since all required actions might not align\nto a part of the parse. Recent work (Jiang et al.,\n2023) has also explored first generating an entire\nplan in NL and then generating a program. Paran-\njape et al. (2023) focus on using tools and python\nscripts to complete a given task such ‘Translate\ninto Pig Latin’. Compared to such past work, the\ncomplex utterances in our case are decomposed\ninto intermediate steps that are parsed into a sub-\nprogram in the target representation as opposed to\ngenerating Python programs. Additionally, these\nsub-programs are a part of the final program output\nand thus we care about the accuracy of intermediate\nsteps as well.\nA related area of research involves grounding\nhigh-level tasks, expressed in natural language, to\na chosen set of actionable steps that a robot could\ntake (Sharma et al., 2022; Singh et al., 2022; Ahn\net al., 2022; Huang et al., 2022). Huang et al. (2022)\npropose a method to ground high-level tasks such\nas ‘make breakfast’ to a set of actionable steps such\nas ‘open fridge’. Such work typically assumes a\nfixed inventory of low-level actions, and may not\ndirectly apply to setups like ours that additionally\nconcerns with the interpretation of the steps into a\nrich target domain representation.\n7\nConclusion\nWe have presented DECINT, an approach for inter-\npreting complex user utterances by decomposing\nthem into elementary natural language steps. To\nevaluate methods for generating programs from\nnatural language requests, we have introduced the\nDeCU dataset, featuring a diverse set of utterances\nrequiring substantial generalization from a small\ntraining set.\nExperiments on DeCU show that\nDECINT outperforms a standard few-shot prompt-\ning approach to program generation, with addi-\ntional analysis revealing opportunities for improve-\nment in both natural language decomposition and\nprogram generation phases.\nLimitations\nThe approach described in this paper does not con-\ndition on execution results from intermediate steps,\nonly generated programs themselves. Incorporat-\ning execution would improve the potential expres-\nsiveness of the model (e.g., by allowing it to im-\nplement control flow operations conditioned on\nprogram results or exceptions). Program results\nmight themselves be natural language strings (e.g.,\nreminders or search results), enabling future exten-\nsions of DECINT to support an even richer space of\nrequests. We used pre-trained large-language mod-\nels from OpenAI, through paid API access that may\nnot be available for everyone or in the future. How-\never, our experiments using LLAMA-2-70B model\n(Touvron et al., 2023) should be easily reproducible.\nWe report and discuss several evaluation measures\nto check the quality of the predictions. One could\nalso examine the outcome and side effects from ex-\necuting the programs. However, lots of the queries\nrequire setting up a populated database and the out-\ncome would vary as we execute the programs in\ndifferent sandbox environments. Developing an\nevaluation setup with sandbox executions is chal-\nlenging and remains an open research question.\nEthics Statement:\nWe leverage pre-trained neu-\nral language models such as GPT-3, and systems\nbuilt using our approach might inherit some biases\npresent in these pre-trained models. We build a\nsystem for NL-to-program, that users can leverage\nto command various NL interfaces. Such systems\nare not perfectly accurate and should be carefully\ndeployed since they may lead to unintended side\neffects.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alexan-\nder Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz,\nBrian Ichter, Alex Irpan, Eric Jang, Rosario Jau-\nregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J.\nJoshi, Ryan Julian, Dmitry Kalashnikov, Yuheng\nKuang, Kuang-Huei Lee, Sergey Levine, Yao Lu,\nLinda Luu, Carolina Parada, Peter Pastor, Jornell\nQuiambao, Kanishka Rao, Jarek Rettinghouse, Diego\nReyes, Pierre Sermanet, Nicolas Sievers, Clayton\nTan, Alexander Toshev, Vincent Vanhoucke, Fei Xia,\nTed Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan.\n2022. Do as I can, not as I say: Grounding language\nin robotic affordances. arXiv: 2204.01691.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. arXiv:\n2107.03374.\nAndrew Drozdov, Nathanael Schärli, Ekin Akyürek,\nNathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. 2022. Compositional\nsemantic parsing with large language models. arXiv:\n2209.15003.\nOlga Golovneva, Moya Peng Chen, Spencer Poff, Mar-\ntin Corredor, Luke Zettlemoyer, Maryam Fazel-\nZarandi, and Asli Celikyilmaz. 2023. ROSCOE: A\nsuite of metrics for scoring step-by-step reasoning.\nIn Proceedings of the Eleventh International Confer-\nence on Learning Representations.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-shot\nplanners: Extracting actionable knowledge for em-\nbodied agents. In Proceedings of the 39th Interna-\ntional Conference on Machine Learning, volume 162\nof Proceedings of Machine Learning Research, pages\n9118–9147. PMLR.\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,\nand Ge Li. 2023. Self-planning code generation with\nlarge language model. arXiv: 2303.06689.\nTushar Khot, Daniel Khashabi, Kyle Richardson, Peter\nClark, and Ashish Sabharwal. 2021. Text modular\nnetworks: Learning to decompose tasks in the lan-\nguage of existing models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2021, Online,\nJune 6-11, 2021, pages 1264–1279. Association for\nComputational Linguistics.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,\nKyle Richardson, Peter Clark, and Ashish Sabharwal.\n2022. Decomposed prompting: A modular approach\nfor solving complex tasks. arXiv: 2210.02406.\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit\nGupta, Sonal Gupta, and Yashar Mehdad. 2021.\nMTOP: A comprehensive multilingual task-oriented\nsemantic parsing benchmark. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\nEACL 2021, Online, April 19 - 23, 2021, pages 2950–\n2962. Association for Computational Linguistics.\nJekaterina Novikova, Ondrej Dusek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue,\nSaarbrücken, Germany, August 15-17, 2017, pages\n201–206. Association for Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nBhargavi Paranjape, Scott Lundberg, Sameer Singh,\nHannaneh\nHajishirzi,\nLuke\nZettlemoyer,\nand\nMarco Tulio Ribeiro. 2023. Art: Automatic multi-\nstep reasoning and tool-use for large language mod-\nels. arXiv preprint arXiv:2303.09014.\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li,\nTingwu Wang, Sanja Fidler, and Antonio Torralba.\n2018. Virtualhome: Simulating household activities\nvia programs. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition,\npages 8494–8502.\nSubhro Roy, Sam Thomson, Tongfei Chen, Richard\nShin, Adam Pauls, Jason Eisner, and Benjamin Van\nDurme. 2022. BenchCLAMP: A benchmark for eval-\nuating language models on semantic parsing. arXiv:\n2206.10668.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL 2022, Seattle, WA, United States,\nJuly 10-15, 2022, pages 2655–2671. Association for\nComputational Linguistics.\nPratyusha Sharma, Antonio Torralba, and Jacob An-\ndreas. 2022. Skill induction and planning with latent\nlanguage. In Proceedings of the Annual Association\nfor Computational Linguistics.\nRichard Shin, Christopher H. Lin, Sam Thomson,\nCharles Chen, Subhro Roy, Emmanouil Antonios\nPlatanios, Adam Pauls, Dan Klein, Jason Eisner, and\nBenjamin Van Durme. 2021. Constrained language\nmodels yield few-shot semantic parsers. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 7699–7715. Association for\nComputational Linguistics.\nMohit Shridhar, Jesse Thomason, Daniel Gordon,\nYonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke\nZettlemoyer, and Dieter Fox. 2020. Alfred: A bench-\nmark for interpreting grounded instructions for ev-\neryday tasks. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition,\npages 10740–10749.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox,\nJesse Thomason, and Animesh Garg. 2022. Prog-\nprompt: Generating situated robot task plans using\nlarge language models. arXiv: 2209.11302.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nIgor Krivokon, Will Rusch, Marc Pickett, Kathleen S.\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Agüera\ny Arcas, Claire Cui, Marian Croak, Ed H. Chi, and\nQuoc Le. 2022. Lamda: Language models for dialog\napplications. CoRR, abs/2201.08239.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nWeiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl,\nand Hermann Ney. 2016. CharacTer: Translation\nedit rate on character level. In Proceedings of the\nFirst Conference on Machine Translation: Volume\n2, Shared Task Papers, pages 505–510, Berlin, Ger-\nmany. Association for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nTomer Wolfson, Mor Geva, Ankit Gupta, Yoav Gold-\nberg, Matt Gardner, Daniel Deutch, and Jonathan\nBerant. 2020. Break it down: A question understand-\ning benchmark. Trans. Assoc. Comput. Linguistics,\n8:183–198.\nA\nData Collection\nA.1\nComplex Utterance Collection\nThe complex utterances in our data are collected by\na mix of manual authoring and automated means\ndescribed below:\nUtterances Authored by Expert Annotators: A\nset of domain experts familiar with the elementary\nutterances are requested to author new complex\nutterances. They are informed of the following\ndesiderata:\n1) utterances should represent a\nmore complex and broader intent compared to\nelementary utterances; 2) the set of utterances\nshould be diverse. To encourage creativity and\ndiversity, the annotators were prompted with a\nset of keywords, and they are asked to author an\nutterance that spans at least some of the provided\nkeywords. Similar approaches have been found\nuseful in past work (Novikova et al., 2017). The\nkeywords are randomly sampled from a list of\ncurated keywords, relevant to the calendaring\nand email domain. For each instance, we draw 5\nkeywords randomly from a much longer list of\nkeywords constructed by the authors. Some of the\nkeywords in our list are as follows: decline, pen,\nvacation, plan-my-day, project-sync, timezone,\ncount-of-meetings, calendar-update, etc.\nAutomatically Generated Utterances : To scale\nthe process of utterance collection and gather even\nmore diverse utterances, we additionally generate\ncomplex utterances using GPT-3 (Brown et al.,\n2020), a pre-trained large language model. A few\nrandom examples of human-authored utterances\nare provided as in-context examples in the prompt,\nand new utterances are sampled. Specifically, we\nrepeatedly sampled 10 utterances from the set\nof manually authored utterances to be used as\nprompt examples. We had additionally included\nan instruction ‘Now generate more utterances that\nare different from the above ones’. We sampled a\nnew utterance with a temperature of 0.8. About\n60% of all the collected utterances were generated\nautomatically via the described process.\nAdditional Information: Note that the utterances\nare limited to the English language, and expanding\nto other languages is a potential future extension.\nAdditionally, note that the expert annotators were\nprovided instructions that no personally identifiable\ninformation or offensive content should be present\nin the utterances. One of the authors also did a\nmanual check of the collected data to ensure that\nthe instructions were followed. All annotators were\nresident domain experts and were paid above the\nprevailing minimum page. The data annotators\nwere provided with relevant information about the\ntask and how the data would be used. Furthermore,\nthe authors held an interactive session with the data\nannotators to give a brief overview and answer any\nquestions.\nA.2\nGuidelines for Decomposition\nAnnotations\nExpert annotators familiar with the domain are re-\nquested to annotate the decompositions of the utter-\nances. Figure A1 shows the interface used by the\nannotators. The interface automatically checks if\nthe annotation is well-formed or not. Annotators\nare asked to skip any utterances that cannot be an-\nnotated as per the domain library. We had a total\nof 6 domain experts who annotated the data. To\nensure high quality, each annotation was addition-\nally reviewed by at least 2 domain experts, separate\nfrom the set of 6 annotators.\nAnnotators are given guidance that to the extent\npossible, each step in decomposition is supposed to\nresemble an elementary utterance. Additionally, to\nrefer to results from earlier steps, the results from\nith can be referred to using variable si. An example\ncan be seen in Figure A1. We additionally provide\nguidance that NL steps should be grammatically\ncorrect full sentences. Moreover, annotators are\nadvised to write the imperative commands using\nthe present tense verb (e.g., prefer “Filter reports\n...” over “Filtering reports ...”).\nA.3\nDecomposition Examples\nFigure A2 shows some example decompositions\nof complex utterances in DeCU.\nB\nDomain Library\nThe domain library defines the set of types and\nfunctions available in the domain. Types model the\ndomain objects such as Person, Event, whereas\nfunctions represent actions that can be taken\nby the agent, including high-level APIs (e.g.,\ncreateEvent, findEmails), low-level operations\n(e.g., min, +), predicate constructors (e.g., called,\nstartsAt), etc. The domain library is provided\nas Scala source code, so that dataset users can\nstatically validate generated code by compiling\nFigure A1: The interface used by domain experts to annotate the decompositions. The interface points out the\nsyntax, type, and missing function errors, enabling the experts to author well-formed annotations. These “express”\nannotations are subsequently normalized by stripping comments, removing type annotations, code formatting, and\nselecting canonical function names when there is ambiguity (concat vs. +). The resulting programs conform to\nScala syntax.\nit with the domain library code.\nSome builtin\ntypes (e.g., String, Boolean), functions (e.g., map,\nOption.when), control flow statements (e.g., if)\nare not explicitly defined, but they can be used in\nthe domain.\nNatural language descriptions of entities or ac-\ntions often omit some of their fields and focus on\na subset of criteria that distinguish them from oth-\ners. In DeCU, we represent these criteria as pred-\nicates, which are lambda functions that take one\nor more arguments and return a Boolean value.\nFor example, a predicate that checks whether an\nevent has the subject “planning” can be rewrit-\nten as called(“planning”), where called is a\npredicate constructor defined in the domain library.\nThese predicate constructors simplify the annota-\ntions, avoiding spelling out the details of the field\ncomparisons. It also makes the program closer to\nnatural language descriptions and potentially easier\nfor LLMs to predict. To conjoin two predicates, the\nfunction and can be used.\nFurther, the domain library provides a collec-\ntion of extension methods and implicit conversions\nwhich significantly simplify annotations for tem-\nporal expressions. For example, the function on\nbelow can be used to combine a time expression\nand a date expression of different types.\nextension [T](time: T)(using Conversion[T,\nLocalDateTime => Boolean ]) {\ndef on[U](date: U)(using Conversion[U,\nLocalDateTime => Boolean ]): LocalDateTime =>\nBoolean = ???\n}\nWith this function and corresponding implicit\nconversions, “3pm on Monday” and “morning\non May 15” can be consistently annotated as\n3.pm\non\nMonday and morning\non\n(May\n/&\n15), respectively, where 3.pm returns Time =>\nBoolean, Monday returns DayOfWeek, morning re-\nturns Interval[Time], and May /& 15 returns\nDate => Boolean.\nC\nAdditional Method Details\nC.1\nPrompt Example\nFigure A3 shows a sample constructed prompt to\ngenerate a program fragment corresponding to the\nlast generated step.\nC.2\nLLM APIs\nWe use OPEN-AI’s APIs, as per their terms\nof\nuse\nhttps://openai.com/policies/\nterms-of-use\nD\nAdditional Details on Experiments\nExperiments with Elementary Utterances\nTo contextualize model performance on complex\nutterances, we conclude by analyzing how the NL-\nto-program semantic parser fares on elementary\nutterances in the DeCU dataset. We split the el-\nementary utterances data, consisting of 841 utter-\nances, into train, dev and test splits in the ratio\n70:15:15. We manually tried a few tweaks (about\n5 variations were tried) to the prompt structure and\nvaried parameters such as the number of exemplars\nin the prompt, and picked the setup that resulted in\nUtterance 1:\nChange my meetings with Abby and those with Dan this week to start 5 minutes later.\nDecomposition:\nStep\n1:\nFind\ne v e n t s\nwith\nAbby\nt h i s\nweek\nval s1 = findEvents(with_(\"Abby\") and queryAt(`this `[Interval[Date]] and isWeek))\nStep\n2:\nFind\ne v e n t s\nwith Dan and\nw it h o u t\nAbby\nt h i s\nweek\nval s2 = findEvents(with_(\"Dan\") and not(with_(\"Abby\")) and\nqueryAt(`this `[Interval[Date]] and isWeek))\nStep\n3:\nSet\na l l\nmeetings\nfrom\nthe\nl i s t\nof\ne v e n t s\ns1\nto\ns t a r t\n5 minutes\nl a t e r\nval s3 = s1.map((x: Event) => modifyEvent(x, startsAt(x.start.local.time +\n5. minutes)))\nStep\n4:\nSet\na l l\nmeetings\nfrom\nthe\nl i s t\nof\ne v e n t s\ns2\nto\ns t a r t\n5 minutes\nl a t e r\nval s4 = s2.map((x: Event) => modifyEvent(x, startsAt(x.start.local.time +\n5. minutes)))\nUtterance 2:\nFor my reports who don’t already have a 1/1 call with me this week, schedule a 1/1 with each\none of them.\nDecomposition:\nStep\n1:\nR e t r i e v e my d i r e c t\nr e p o r t s\nval s1 = me.directReports\nStep\n2:\nF i l t e r\nout\nthe\nr e p o r t s\nt h a t\ndo not\nhave a one−on−one\nmeeting\nwith me\nt h i s\nweek\nval s2 = s1.filter ((x: Person) => findEvents(with_(x) and isOneOnOne and\nqueryAt(`this `[Interval[Date]] and isWeek)).results.isEmpty)\nStep\n3:\nCreate a one−on−one\nmeeting\nwith\nthose\nf i l t e r e d\nr e p o r t s\nin\nl i s t\ns2\nval s3 = s2.map((x: Person) => createEvent(with_(x) and isOneOnOne))\nUtterance 3:\nDecline any meeting invitations that are scheduled during my weekly team meeting.\nDecomposition:\nStep\n1:\nFind\nthe\nevent\nc a l l e d\n\" team\nmeeting \"\nt h a t\nr ecur s\nweekly .\nval s1 = theEvent(called(\"team meeting\") and recurringWeekly)\nStep\n2:\nFind\na l l\ne v e n t s .\nval s2 = findEvents0\nStep\n3:\nF i l t e r\ne v e n t s\nfrom\nl i s t\ns2\nto\nonly\ni n c l u d e\nones\nt h a t\ni n t e r s e c t\nwith\nevent\ns1\nt h a t\nare\nnot\ns1 .\nval s3 = s2.filter ((x: Event) => x.interval.intersects(s1.interval) && x.id != s1.id)\nStep\n4:\nDecline\ne v e n t s\nin\ns3 .\nval s4 = s3.map((x: Event) => respond(x, ResponseStatusType.declined))\nUtterance 4:\nFind all the meetings this week where the duration is more than 2 hours and reschedule them to\nnext week..\nDecomposition:\nStep\n1:\nFind\na l l\nmeetings\nhappening\nt h i s\nweek\nval s1 = findEvents(queryAt(`this `[Interval[Date]] and isWeek))\nStep\n2:\nF i l t e r\nthe\nl i s t\nof\ne v e n t s\ns1\nto\nkeep\ne v e n t s\nwith a\nd u r a t i o n\ng r e a t e r\nthan 2\nhours\nval s2 = s1.filter(x => x.duration > 2. hours)\nStep\n3:\nUpdate each\nevent\nin\nthe\nl i s t\ns2\nto\nhappen\nnext\nweek\nval s3 = s2.map(x => modifyEvent(x, startsAt ((next[Interval[Date]] and isWeek))))\nFigure A2: Examples of complex utterances in DeCU. Each utterance is accompanied by decompositions consisting\nof a sequence of NL steps and associated program fragments, annotated by domain experts.\nthe highest exact match accuracy on the dev split.\nWe report an exact match against the gold parse as\nhas been used in the past work as well (Roy et al.,\n2022). Additionally, we note that there might be\ncertain small deviations such as extra surrounding\nbraces that do not invalidate the generated program,\nand exact match as a binary metric would penal-\nize such deviations. So we additionally report a\ncharacter-based edit distance measure (Wang et al.,\n2016) that might provide more fine-grained insights\ncompared to binary exact match.\nResults: Results on the parsing task for the test\nsplit are shown in Table A1. The parser, referred to\nas LLM Parser in the Table, gets 0.60 exact match\nand 0.10 character edit distance. We also consider\na few variations of the parser:\n(1) Using text-davinci-001 instead of text-davinci-\n003: Leads to a large reduction in the automated\nMethod\nExact Match↑\nChar Edit↓\nLLM parser\n0.60\n0.10\nAblations:\nUsing text-davinci-001\n0.42\n0.23\nRandom exemplars\n0.21\n0.41\nMax 3 examples\n0.60\n0.13\nLimit to 20% train data\n0.33\n0.26\nTable A1: Evaluation results on parsing elementary\nutterances in the test set.\nmetrics.\n(2) Using random M examples in the prompt in-\nstead of dynamically selected ones based on simi-\nlarity to test command: Performs the worst among\nall the variations, demonstrating that relevant ex-\nample selection is very important.\n(3) Reducing the number of in-context examples\nfrom M = 20 to M = 3: The metrics show little to\nStep :\nF i l t e r\nthe\nl i s t\nof\ne v e n t s\ns2\nto\ni n c l u d e\nonly\nthe\ne v e n t s\nt h a t\nare\nnot\nthe\nevent\ns1\nProgram: val s3 = s2.filter(x => x.id != s1.id)\n<EOS >\nStep :\nF i l t e r\nthe\nl i s t\nof\ne v e n t s\ns3\nto\ni n c l u d e\nonly\nthe\ne v e n t s\norganized\nby me and which\ni n t e r s e c t\nwith\nthe\nevent\ns1\nProgram: val s4 = s3.filter(x => x.organizerIs(me) && x.interval.intersects(s1.interval))\n<EOS >\nStep :\nF i l t e r\nthe\nl i s t\nof\npersons\ns2\nto\ni n c l u d e\nonly\npeople\na t t e n d i n g\nthe\nevent\ns1\nProgram: val s4 = s2.filter(x => s1.attendees.isAttending(x))\n<EOS >\nStep :\nF i l t e r\nthe\nl i s t\nof\npersons\ns3\nto\ni n c l u d e\nonly\nthe\npeople\na t t e n d i n g\nthe\nevent\ns1\nProgram: val s5 = s3.filter(x => s1.attendees.isAttending(x))\n<EOS >\nStep :\nF i l t e r\nthe\nl i s t\nof\ne v e n t s\ns1\nto\nonly\nc on ta i n\ne v e n t s\nwith 1 other\na t t e n d e e\nProgram: val s = s1.filter(x => x.attendees.all.size == 2)\n<EOS >\nStep :\nF i l t e r\nthe\nl i s t\nof\ndays\ns1\nto\ni n c l u d e\nonly\nthose\ndays\nwhere\nt h e r e\nare\nat\nl e a s t\n5\ne v e n t s\nProgram: val s = s1.filter(x => findEvents(queryAt(x /& (next[Interval[Date]] and isWeek))).size >= 5)\n<EOS >\nStep :\nF i l t e r\nthe\nl i s t\nof\ne v e n t s\ns5\nto\nkeep\nonly\nthose\ne v e n t s\nt h a t\ni n t e r s e c t\nwith\nthe\ni n t e r v a l\ns4\nProgram: val s6 = s5.filter(x => x.interval.intersects(s4))\n<EOS >\nStep :\nF i l t e r\nthe\ne v e n t s\nin\nthe\nl i s t\ns1\nto\ni n c l u d e\nonly\nthose\nwhere\nthe\nother\na t t e n d e e\nhas a job\nt i t l e\nof\n\"PM\"\nProgram: val s = s1.filter( x => getPersonFromAttendee(x.attendees.otherPeople.head).jobTitle == Some(\"PM\"))\n<EOS >\nStep :\nDescribe\nthe\nnumber\nof\ne v e n t s\nin\nthe\nl i s t\ns1\nProgram: val s2 = s1.size\n<EOS >\nStep :\nUpdate each\nevent\nin\nthe\nl i s t\ns1\nto\nl a s t\nonly\nf o r\n30\nminutes\nProgram: val s = s1.map(x => modifyEvent(x, lastsFor (30. minutes)))\n<EOS >\nStep :\nI f\nthe\nl i s t\ns2\ni s\nempty\nthen\nupdate\nthe\nevent\ns1\nto\nend\nat\n2:30 pm .\nProgram: val s3 = Option.when(s2.isEmpty){modifyEvent(s1, endsAt ((2 :: 30).pm))}\n<EOS >\nStep :\nF i l t e r\nthe\nl i s t\nof\npersons\ns1\nto\nkeep\nonly\nthose\nt h a t\ndon ' t\nhave a one−on−one\nmeeting\nwith\nthe\nuser\nt h i s\nweek\nProgram: val s = s1.filter(x => findEvents(with_(x) and isOneOnOne and queryAt(`this `[Interval[Date]] and isWeek)).isEmpty)\n<EOS >\nStep :\nI f\nt h e r e\nare more than 5\ne v e n t s\nin\nthe\nl i s t\ns1 ,\nd e c l i n e\nthe\nl o n g e s t\nscheduled\nevent\nProgram: val s = Option.when(s1.size > 5){respond(max(s1 , (x => x.duration)), withResponse(ResponseStatusType.declined))}\n<EOS >\nStep :\nChange\nthe\nevent\ns1\nto\ni n c l u d e\nBen and remove Hao\nProgram: val s = modifyEvent(s1 , with_(\"Ben\") and not(with_(\"Hao\")))\n<EOS >\nStep :\nUpdate\nthe\ns u b j e c t\nof\neach\nevent\nin\n( l i s t\ns1 ) [ the\nl i s t ]\nto\n\"1/1\nwith\n( corresponding\nperson ) \"\nProgram: val s = s1.map(x => modifyEvent(x, called(\"1:1 with \" + x.attendees.otherPeople.head.nameAndEmail.name.get)))\n<EOS >\nStep :\nUpdate each\nevent\nin\nthe\nl i s t\ns2\nto\nbe\nc a l l e d\n\" p r o j e c t\nsync \"\nProgram: val s = s2.map(x => modifyEvent(x, called(\"project sync\")))\n<EOS >\nStep :\nForward\nt h i s\nemail\nto\nthe\nl i s t\nof\npersons\ns2\nProgram: val s3 = forwardEmail(messageIdIs(theEmail(`this `[Email]).id) and messageWithTo(s2))\n<EOS >\nStep :\nUpdate each\nevent\nin\nthe\nl i s t\ns3\nto\nhappen\na f t e r\nthe\nevent\ns1\nProgram: val s5 = s4.map(x => modifyEvent(x, startsAt(after(s1.interval))))\n<EOS >\nStep :\nFor each day\nin\nthe\nl i s t\ns2\nc r e a t e\nan\nevent\nwith\nthe\nc a l l e d\n\" v a ca t io n\ntime \"\nProgram: val s = s2.map(x => createEvent(called(\"vacation time\") and createAt(x /& (next[Interval[Date]] and isWeek))))\n<EOS >\nStep :\nDecline\neach\nmeeting\nin\nthe\nl i s t\ns1\nProgram: val s = s1.map(x => respond(x, withResponse(ResponseStatusType.declined)))\n<EOS >\nStep :\nCheck\ni f\nthe\ns i z e\nof\nthe\nl i s t\ns1\ni s\n5 or\nl e s s\nProgram: val s = s1.size <= 5\n<EOS >\nStep :\nUpdate\nthe\nevent\ns1\nto\nhappen\na f t e r\n8am and\nbe fore\n5pm in\nthe\ntime\nzone\ns2 .\nProgram: val s3 = modifyEvent(s1 , startsAt(after (8.am) inZone s2) and endsAt(before (5.pm) inZone s2))\n<EOS >\nStep :\nGet\nthe\na t t e n d e e s\nof\nthe\nevent\ns1\nProgram: val s2 = s1.attendees.all\n<EOS >\nStep :\nGet\nthe\ns t a r t\ntime\nfrom\nevent\ns1\nProgram: val s = s1.start.local.time\n<EOS >\nStep :\nGet\nthe\nend\ntime\nfrom\nevent\ns1\nProgram: val s = s1.end.local.time\n<EOS >\nUtterance: Forward this email to all the attendees in the Standup event\nStep\n1:\nFind\nthe\nevent\nc a l l e d\nStandup\nProgram 1: val s1 = theEvent(called(\"Standup\"))\nStep\n2:\nGet\nthe\na t t e n d e e s\nof\nthe\nevent\ns1\nProgram 2: val s2 = s1.attendees.all\nStep\n3:\nForward\nt h i s\nemail\nto\nthe\nl i s t\nof\npersons\ns2\nProgram 3: val s3 = forwardEmail(messageIdIs(theEmail(`this `[Email ]).id) and messageWithTo(s2))\n<EOS >\nUtterance: If there is an \"emergency review\" meeting this week , then reschedule any events that happen 30 minutes before or after the meeting to next Friday.\nStep\n1:\nFind\nthe\nevent\nc a l l e d\n\" emergency\nreview \"\nProgram 1: val s1 = theEvent(called(\"emergency review\"))\nStep\n2:\nS u b t r a c t\n30\nminutes\nfrom\nthe\ns t a r t\ntime\nof\nthe\nevent\ns1\nProgram 2: val s2 = s1.start - 30. minutes\nStep\n3: Add 30\nminutes\nto\nthe\nend\ntime\nof\nthe\nevent\ns1\nProgram 3: val s3 = s1.end + 30. minutes\nStep\n4:\nE s t a b l i s h\nthe\ntime\ni n t e r v a l\nbetween\nthe\ntime\ni n s t a n t\ns2 and\nthe\ntime\ni n s t a n t\ns3\nProgram 4: val s4 = Interval[Instant ](s2 , s3)\nStep\n5:\nFind\ne v e n t s\nt h a t\nare\nscheduled\nf o r\nt h i s\nweek\nProgram 5: val s5 = findEvents(createAt(`this `[Interval[Date]] and isWeek))\nStep\n6:\nF i l t e r\nthe\nl i s t\nof\ne v e n t s\ns5\nto\nkeep\nonly\nthose\ne v e n t s\nt h a t\ni n t e r s e c t\nwith\nthe\ni n t e r v a l\ns4\nProgram 6: val s6 = s5.filter(x => x.interval.intersects(s4))\nStep\n7:\nUpdate each\nevent\nin\nthe\nl i s t\ns6\nto\nhappen on\nthe\nnext\nFriday\nProgram 7: val s7 = s6.map(x => modifyEvent(x, createAt(next[Date] /& Friday)))\n<EOS >\nUtterance: Extend pizza party at 1 PM to end at 2.30 pm if extending it doesn't overlap with the next event.\nStep 1: Find event called \"pizza party\"\nProgram 1: val s1 = theEvent(called (\" pizza party \") and queryAt ((1).pm))\nStep 2: Find events starting between 1 pm and 2:30 pm that are not titled \"pizza party\"\nProgram 2: val s2 = findEvents(not(called (\"pizza party\")) and queryAt(timeInterval (1.pm , (2::30).pm)))\nStep 3: If the list s2 is empty then update the event s1 to end at 2:30 pm.\nProgram 3: val s3 = Option.when(s2.isEmpty){modifyEvent(s1 , endsAt ((2 :: 30).pm))}\n<EOS >\nUtterance: Create a preparation meeting this week with the attendees of the project sync who report to me or my manager.\nStep 1: Find the event called \"project sync\"\nProgram 1: val s1 = theEvent(called (\" project sync\"))\nStep 2: Find my reports\nProgram 2: val s2 = thePerson(me).directReports\nStep 3: Find my manager 's reports\nProgram 3: val s3 = thePerson(me).manager.directReports\nStep\n4:\nF i l t e r\nthe\nl i s t\nof\npersons\ns2\nto\ni n c l u d e\nonly\npeople\na t t e n d i n g\nthe\nevent\ns1\nProgram 4: val s4 = s2.filter(x => s1.attendees.isAttending(x))\nStep\n5:\nF i l t e r\nthe\nl i s t\nof\npersons\ns3\nto\ni n c l u d e\nonly\nthe\npeople\na t t e n d i n g\nthe\nevent\ns1\nProgram 5: val s5 = s3.filter(x => s1.attendees.isAttending(x))\nStep\n6:\nCreate an\nevent\nc a l l e d\np r e p a r a t i o n\nmeeting\nt h i s\nweek\nwith\nthe\nl i s t\nof\npersons\ns4 and\nthe\nl i s t\nof\npersons\ns5\nProgram 6: val s6 = createEvent(with_(s4) and with_(s5) and createAt(thisWeek) and called(\"preparation meeting\"))\n<EOS >\nUtterance: Other than my manager , how many people are attending the project sync meeting tomorrow?\nStep\n1:\nFind\nthe\nevent\nc a l l e d\np r o j e c t\nsync\ntomorrow\nProgram 1: val s1 = theEvent(called(\"project sync\") and queryAt(tomorrow))\nStep\n2:\nDescribe\nthe\nnumber\nof\na t t e n d e e s\nof\nthe\nevent\ns1\ne x c l u d i n g\nthe\nmanager\nProgram 2: val s2 = s1.attendees.all.size - 1\n<EOS >\nUtterance: Calculate how many meetings last week I had during my lunch hours of 12 noon to 1 PM\nStep\n1:\nFind\ne v e n t s\nfrom\nl a s t\nweek between 12 PM and 1 PM\nProgram 1: val s1 = findEvents(queryAt(timeInterval (12.pm , 1.pm) on (last[Interval[Date]] and isWeek)))\nStep\n2:\nDescribe\nthe\nnumber\nof\ne v e n t s\nin\nthe\nl i s t\ns1\nProgram 2: val s2 = s1.size\n<EOS >\nUtterance: Find all the meetings scheduled for next week that I created but that conflict with my doctor 's appointment. Reschedule them to after the doctor 's appointment.\nStep\n1:\nFind\nthe\nevent\nc a l l e d\n\" doctor ' s\nappointment \"\nProgram 1: val s1 = theEvent(called(\"doctor 's appointment\"))\nStep\n2:\nFind\na l l\ne v e n t s\nhappening\nnext\nweek\nProgram 2: val s2 = findEvents(queryAt(next[Interval[Date]] and isWeek))\nStep\n3:\nF i l t e r\nthe\nl i s t\nof\ne v e n t s\ns2\nto\ni n c l u d e\nonly\nthe\ne v e n t s\nt h a t\nare\nnot\nthe\nevent\ns1\nProgram 3: val s3 = s2.filter(x => x.id != s1.id)\nStep\n4:\nF i l t e r\nthe\nl i s t\nof\ne v e n t s\ns3\nto\ni n c l u d e\nonly\nthe\ne v e n t s\norganized\nby me and which\ni n t e r s e c t\nwith\nthe\nevent\ns1\nProgram 4: val s4 = s3.filter(x => x.organizerIs(me) && x.interval.intersects(s1.interval))\nStep\n5:\nUpdate each\nevent\nin\nthe\nl i s t\ns3\nto\nhappen\na f t e r\nthe\nevent\ns1\nProgram 5: val s5 = s4.map(x => modifyEvent(x, startsAt(after(s1.interval))))\n<EOS >\nUtterance: Update the team meeting on Wednesday , so that its after 8 AM and before 5 PM for Jack\nStep\n1:\nFind\nthe\nevent\nc a l l e d\n\" team\nmeeting \" on Wednesday .\nProgram 1: val s1 = theEvent(called(\"team meeting\") and queryAt(Wednesday))\nStep\n2:\nFind\nout\nwhat\ntime\nzone\nJack\ni s\nin .\nProgram 2: val s2 = thePerson(named(\"Jack\")).timeZone\nStep\n3:\nUpdate\nthe\nevent\ns1\nto\nhappen\na f t e r\n8am and\nbefore 5pm in\nthe\ntime\nzone\ns2 .\nProgram 3: val s3 = modifyEvent(s1, startsAt(after (8.am) inZone s2) and endsAt(before (5.pm) inZone s2))\n<EOS >\nUtterance: How many 1/1 meetings in total I had in the last week?\nStep\n1:\nFind\na l l\none on one\ne v e n t s\nfrom\nl a s t\nweek and\nr e t u r n\nthe\ns i z e\nof\nt h a t\nl i s t\nProgram 1: val s1 = findEvents(queryAt(last[Interval[Date]] and isWeek) and isOneOnOne).size\n<EOS >\nUtterance: If I don't have an email about shiproom , then set up a 1:1 with Smith titled Discussion about Shiproom\nStep 1: Check if there are no emails about shiproom\nProgram 1: val s1 = findEmails(messageTitleIs (\" shiproom \")).isEmpty\nStep 2: If s1 is true , set up a 1:1 with Smith titled \"Discussion about Shiproom\"\nProgram 2: val s2 = Option.when(s1){createEvent(isOneOnOne and with_ (\"Smith\") and called (\" Discussion about Shiproom \"))}\n<EOS >\nUtterance: Adjust my schedule making sure there are no conflicts with the happy hour event today\nStep 1: Find the event called \"happy hour\" today\nProgram 1: val s1 = theEvent(called (\" happy hour\") and queryAt(today))\nStep 2: Find all events happening today\nProgram 2: val s2 = findEvents(queryAt(today))\nStep 3: Filter the list of events s2 to include only the events that intersect with the event s1\nFigure A3: Example Prompt to generate program fragment for a generated step. The initial part of the prompt\ncomprises of up to M ≤25 examples similar to “Filter the list of events s2 to include only the events that intersect\nwith the event s1”. It is followed by K = 10 decompositions of complex utterances. The generated output for the\nabove prompt was val s3 = s2.filter(x => x.interval.intersects(s1.interval))\nEXPERTS\nDECINT\nGrammar\n98.3\n98.6\nFactuality\n99.4\n99.4\nHallucination\n99.4\n99.4\nRedundancy\n99.7\n99.7\nRepetition\n100\n100\nMissing Information\n99.7\n97.8\nCoherency\n98.8\n99.7\nCommonsense\n100\n100\nArithmetic\n100\n100\nTable A2: Analysis of the NL steps written by domain\nexperts and those predicted by DECINT. For the over-\nall evaluation, the score is the percentage of examples\njudged as sufficient and correct to complete the user\nrequest. For the step-by-step evaluation done by crowd\nworkers, the score of a dimension is the percentage of\nsteps judged as not containing related issues.\nno change, suggesting that just a couple of relevant\nexamples are usually enough for the parser.\n(4) Reduce train data size to around 20% of the\noriginal size: This leads to a reduction in accu-\nracy values, as in-context learning relies on having\nsimilar relevant examples in the prompt.\nAs mentioned earlier, a limitation of the exact\nmatch metric is that it is not perfect, since it fails\nto capture extra redundant braces, or small varia-\ntions in string argument values, such as the meeting\nname being “coffee meeting” instead of “coffee”.\nWe observe that some of the incorrect predictions\nare due to a few functions in gold program not\nbeing present in the training data (our splits were\nrandom). Using API documentation and contain-\ning the outputs of the parser (Shin et al., 2021; Roy\net al., 2022) to only use allowed functions could\nbe leveraged to fix such errors, though we leave\nit for future work, as our primary goal is to study\ncomplex utterance parsing in light of training data\nfor simpler elementary utterances.\nAdditional Example Outputs\nFigure A4 shows predictions from DECINT and\nDIRECT-PRED on a few sample inputs. Figure A5\nshows predictions from DECINT and the two main\nbaselines DIRECT-PRED and COT on a sample\ninput. Figure A6 shows a few cases where outputs\nfrom DECINT were incorrect.\nStep-by-Step Evaluation of NL Decomposition:\nFollowing ROSCOE (Golovneva et al., 2023),9 we\n9We adopted ROSCOE as much as possible, though we\nreceived some feedback from crowdworkers that some dimen-\nsions (such as factuality, hallucination, and coherency) were\nSystem\nCorrect↑\nCER↓\nDIRECT-PRED\n0.31\n0.46\nDECINT\n0.41\n0.41\nTable A3: Automated Metrics averaged over 3 runs.\nevaluate the quality of individual steps on 9 dimen-\nsions: grammar, factuality, hallucination, redun-\ndancy, repetition, missing information, coherency,\ncommonsense, and arithmetic. We recruited anno-\ntators from Amazon Mechanical Turk to provide\nbinary classification ratings (yes or no) for each\nstep in the decomposition on all 9 dimensions. For\neach question, 3 judgments are collected and the\nmajority-voted answer is used as the final judg-\nment. For quality control, we restrict to annotators\nlocated in the United States or Canada who have\nan approval rate higher than 85% and have success-\nfully solved a qualification task where we match\ntheir answers on the same set of questions against\nanswers manually annotated by one of the authors.\nWe pay 0.25 USD per example per question. The\nstep-by-step evaluation results shown in the bot-\ntom block of Table A2 suggest that the quality of\nindividual steps is very high. DECINT-predicted\nsteps are rated similar to expert steps on almost all\ndimensions, except on the “missing information”\ndimension, where the gap is noticeable. Note that\nit is possible for all steps in a program to be judged\nindividually correct, but fail to complete the user\nrequest.\nTrain and Test Split of Complex Utterances\nAs noted previously, we use K = 10 complex ut-\nterance examples from DeCU as train split. The\nexamples were chosen randomly. We investigate\nthe impact of random seed, by repeating the experi-\nments for 3 random seeds (i.e. each seed leads to a\ndifferent set of 10 examples in the train split while\nthe remaining are in the test split), and report the\naverage automated scores in Table A3.\nAdditional Details on Correct Reference-less\nMetric using GPT4\nCorrelation of model-based metrics with human\nlabels: On a per-example basis, we observe that\nautomatic labels and human evaluation labels agree\non correct/incorrect annotation 96.5% of the time\nwhen compared on the subset of well-formed out-\nputs from the above-mentioned methods. More-\ndifficult to judge for our evaluation task.\nover, considering human-provided labels as ground\ntruth and Correct labels as predictions in a binary\nclassification setup, we observe F1 scores of 98%\nand 84% for “correct\" and “incorrect\" classes re-\nspectively. Thus, Correct seems to be a good proxy\nfor a human label to judge the correctness of a gen-\nerated program.\nPrompt: Prompt used in the reference-less eval-\nuation is shown in Figure A7.\nUtterance 1:\nMake me a meeting at 3 pm mountain time and refuse any meetings that happen at the same time.\nDECINT\nStep\n1:\nFind\ne v e n t s\nat 3 pm mountain\ntime\nval s1 = findEvents(queryAt (3.pm inZone TimeZone(\"MST\")))\nStep\n2:\nRefuse\nany\ne v e n t s\nin\nthe\nl i s t\ns1\nval s2 = s1.map(x => respond(x, withResponse(ResponseStatusType.declined)))\nStep\n3:\nCreate a meeting\nat 3 pm mountain\ntime\nval s3 = createEvent(createAt (3.pm inZone TimeZone(\"MST\")))\nDIRECT-\nPRED\nval s1 = createEvent(createAt (3.pm inZone TimeZone(\"MST\")))\nval s2 = findEvents(queryAt (3.pm inZone TimeZone(\"MST\")))\nval s3 = s2.map(x => refuseEvent(x))\nComment\nWe observe that hallucination of functions is much more prominent in the baseline DIRECT-\nPRED, as can be seen in the use of the non-existing function refuseEvent\nUtterance 2:\nCross all my vacation events off my calendar, I need to juggle those around.\nDECINT\nStep\n1:\nFind\na l l\ne v e n t s\nc a l l e d\n\" v a c a t i o n \"\nval s1 = findEvents(called(\"vacation\"))\nStep\n2:\nDelete\na l l\ne v e n t s\nin\nthe\nl i s t\ns1\nval s2 = s1.map(x => deleteEvent(x))\nDIRECT-\nPRED\nval s = theEvent(called(\"vacation\")).map(x => deleteEvent(x))\nComment\nPrediction from DIRECT-PRED incorrectly applies a map operation to a single event. We note\nthat predictions from DECINT show fewer syntax errors compared to DIRECT-PRED.\nUtterance 3:\nIf Issac cannot make it to the LT-meeting, schedule some 1:1 time with him tonight.\nDECINT\nStep\n1:\nFind\nthe\nevent\nc a l l e d\n\"LT−meeting \"\nval s1 = theEvent(called(\"LT -meeting\"))\nStep\n2:\nCheck\ni f\nI s s a c\ni s\na t t e n d i n g\nthe\nevent\ns1\nval s2 = s1.attendees.isAttending(thePerson(\"Issac\"))\nStep\n3:\nI f\ns2\ni s\nf a l s e ,\nc r e a t e\na 1:1\nevent\nwith\nI s s a c\nt o n i g h t\nval s3 = Option.when(!s2){createEvent(isOneOnOne and with_(\"Isaac\") and\ncreateAt(tonight))}\nDIRECT-\nPRED\nval s1 = theEvent(called(\"LT -meeting\"))\nval s2 = thePerson(named(\"Issac\"))\nval s3 = s1.attendees.isAttending(s2)\nval s4 = if (s3) {\ncreateEvent(with_(s2) and createAt(tonight) and called(\"1:1 time\"))\n} else {\nnull\n}\nComment\nDIRECT-PRED output has a logical flaw — it creates a meeting when Issac is attending the\n‘LT-meeting’ – and was judged as incorrect. We observe that outputs from DECINT are less likely\nto demonstrate such logical flaws.\nUtterance 4:\nI need you to delete the meeting I have at 3 pm and replace it with \"Core\" with Jimmy.\nDECINT\nStep\n1:\nFind\nthe\nevent\nat 3 pm\nval s1 = theEvent(queryAt (3.pm))\nStep\n2:\nDelete\nthe\nevent\ns1\nval s2 = deleteEvent(s1)\nStep\n3:\nCreate a new event\nc a l l e d\n\" Core\nwith\nJimmy \"\nat 3 pm\nval s3 = createEvent(called(\"Core with Jimmy\") and createAt (3.pm))\nDIRECT-\nPRED\nval s1 = deleteEvent(theEvent(queryAt (3.pm)))\nval s2 = createEvent(called(\"Core\") and with_(\"Jimmy\"))\nComment\nPrediction from DIRECT-PRED fails to create the meeting at the correct time, and was judged as\nincorrect in human evaluations.\nUtterance 5:\nRename all the 1/1 meetings I have with a PM this month to be called project sync.\nDECINT\nStep\n1:\nFind\na l l\n1/1\nmeetings\nt h i s\nmonth\nval s1 = findEvents(queryAt(`this `[Interval[Date]] and isMonth) and isOneOnOne)\nStep\n2:\nF i l t e r\nthe\nl i s t\nof\ne v e n t s\ns1\nto\ni n c l u d e\nonly\nthe\ne v e n t s\nwith a PM\nval s2 = s1.filter( x => getPersonFromAttendee(x.attendees.otherPeople.head).jobTitle\n== Some(\"PM\"))\nStep\n3:\nUpdate each\nevent\nin\nthe\nl i s t\ns2\nto\nbe\nc a l l e d\n\" p r o j e c t\nsync \"\nval s3 = s2.map(x => modifyEvent(x, called(\"project sync\")))\nDIRECT-\nPRED\nval s1 = findEvents(queryAt(`this `[Interval[Date]] and isMonth) and isOneOnOne)\nval s2 = s1.filter(x => x.attendees.isAttending(thePerson(PM)))\nval s3 = s2.map(x => modifyEvent(x, called(\"project sync\")))\nComment\nDIRECT-PRED output was not valid as per library since ‘thePerson(PM)‘ is incorrect usage while\nDECINT output was judged as correct.\nFigure A4: Outputs from DECINT and DIRECT-PRED on a few complex utterances in DeCU.\nUtterance:\nMake me a meeting at 3 pm mountain time and refuse any meetings that happen at the same time.\nDECINT\nStep\n1:\nFind\ne v e n t s\nat 3 pm mountain\ntime\nval s1 = findEvents(queryAt (3.pm inZone TimeZone(\"MST\")))\nStep\n2:\nRefuse\nany\ne v e n t s\nin\nthe\nl i s t\ns1\nval s2 = s1.map(x => respond(x, withResponse(ResponseStatusType.declined)))\nStep\n3:\nCreate a meeting\nat 3 pm mountain\ntime\nval s3 = createEvent(createAt (3.pm inZone TimeZone(\"MST\")))\nDIRECT-\nPRED\nval s1 = createEvent(createAt (3.pm inZone TimeZone(\"MST\")))\nval s2 = findEvents(queryAt (3.pm inZone TimeZone(\"MST\")))\nval s3 = s2.map(x => refuseEvent(x))\nCOT\nStep\n1:\nCreate a meeting\nat 3 pm mountain\ntime\nStep\n2:\nRefuse\nany\nmeetings\nt h a t\nhappen\nat\nthe\nsame\ntime \"}\nval s1 = createEvent(createAt (3.pm inZone TimeZone (\\\"MST \\\")))\nval s2 = findEvents(queryAt (3.pm inZone TimeZone (\\\" MST \\\"))).map(x => refuseEvent(x))\nFigure A5: Outputs from DECINT, DIRECT-PRED and COT on a sample complex utterance in DeCU.\nUtterance 1:\nSplit my project sync call next week into two events with the same attendees, but in the second\none, invite David Smith also.\nDECINT\nStep\n1:\nFind\nthe\nevent\nc a l l e d\n\" p r o j e c t\nsync \"\nnext\nweek\nPlan 1: val s1 = theEvent(called(\"project sync\") and queryAt(next[Interval[Date]] and\nisWeek))\nStep\n2:\nGet\nthe\na t t e n d e e s\nof\nthe\nevent\ns1\nPlan 2: val s2 = s1.attendees.all\nStep\n3:\nCreate a new event\nwith\nthe\nsame\na t t e n d e e s\nas\nthe\nevent\ns1\nPlan 3: val s3 = createEvent(with_(s2) and createAt(s1.interval))\nStep\n4: Add David\nSmith\nto\nthe\nl i s t\nof\na t t e n d e e s\ns2\nPlan 4: val s4 = s2 :+ thePerson(named(\"David Smith\"))\nStep\n5:\nCreate a new event\nwith\nthe\nl i s t\nof\na t t e n d e e s\ns4\nPlan 5: val s5 = createEvent(with_(s4) and createAt(s1.interval))\nDIRECT-\nPRED\nval s1 = theEvent(called(\"project sync\") and queryAt(next[Interval[Date]] and isWeek))\nval s2 = createEvent(with_(s1.attendees.all) and createAt(s1.interval.start) and\ncalled(\"project sync 1\"))\nval s3 = createEvent(with_(s1.attendees.all) and with_(\"David Smith\") and\ncreateAt(s1.interval.end) and called(\"project sync 2\"))\nExperts\nStep\n1:\nFind\nthe\n\" p r o j e c t\nsync \"\nevent\nnext\nweek\nval s1 = theEvent(called(\"project sync\") and queryAt(next[Interval[Date]] and isWeek))\nStep\n2:\nCreate an\nevent\nwith\nthe\nsame\na t t e n d e e s\nfrom\nevent\ns1 and\ni n c l u d e\nDavid\nSmith\nval s2 = createEvent(with_(s1.attendees) and with_(\"David Smith\"))\nComment\nOutput program from DECINT as well as the output from DIRECT-PRED doesn’t delete the\noriginal meeting and creates two new meetings, which does not reflect the user intention.\nUtterance 2:\nI’d like to divide the meeting at noon in two, so split off the last fifteen minutes as a separate\nevent.\nDECINT\nStep\n1:\nFind\nthe\nevent\nat\nnoon\nval s1 = theEvent(createAt(noon))\nStep\n2:\nS u b t r a c t\n15\nminutes\nfrom\nthe\nend\ntime\nof\nthe\nevent\ns1\nval s2 = s1.end - 15. minutes\nStep\n3:\nUpdate\nthe\nevent\ns1\nto\nend\nat\nthe\ntime\ni n s t a n t\ns2\nval s3 = modifyEvent(s1, endsAt(s2))\nStep\n4:\nCreate a new event\ns t a r t i n g\nat\nthe\ntime\ni n s t a n t\ns2\nval s4 = createEvent(startsAt(s2))\nDIRECT-\nPRED\nval s1 = theEvent(createAt(noon))\nval s2 = modifyEvent(s1, endsAt(s1.start - 15. minutes))\nval s3 = createEvent(startsAt(s1.start - 15. minutes) and lastsFor (15. minutes))\nExperts\nStep\n1:\nFind my event\nat\nnoon\nval s1 = theEvent(queryAt(noon))\nStep\n2:\nReduce\nthe\nd ur at i o n\nof\nevent\ns1 by 15\nminutes\nval s2 = modifyEvent(s1, lastsFor(s1.duration - 15. minutes))\nStep\n3:\nGet\nthe\nend\ntime\nof\nevent\ns2\nval s3 = s2.end.local.time\nStep\n4:\nCreate an\nevent\nt h a t\ns t a r t s\nat\nthe\nend\nof\nevent\ns3\nt h a t\nl a s t s\nf o r\n15\nminutes\nval s4 = createEvent(createAt(s3) and lastsFor (15. minutes))\nComment\nOutput program from DECINT was judged as incorrect since it doesn’t specify the duration of\nthe second event.\nFigure A6: Sample predictions from DECINT that were judged as incorrect in human evaluations.\nTask is to identify whether a program , written in a Scala -like representation , is sufficient and correct to complete the user request.\nThe program can be considered to be an incorrect solution for various reasons such as if it addresses only a part of user request , or does more than what\nuser asked for , or uses incorrect values or variable names , etc.\nThe program can be considered to be correct if it reflects the user intent , and nothing more , nothing less.\n# Utterance: Find all the meetings scheduled for next week that I created but that conflict with my doctor 's appointment. Reschedule them to after the\ndoctor 's appointment.\n# Predicted program\nval s1 = findEvents(called(\"doctor\")).get(0)\nval s2 = findEvents(queryAt(next[Interval[Date]] and isWeek))\nval s3 = s2.filter(x => x.organizerIs(me) && x.interval.intersects(s1.interval) && x.id != s1.id)\nval s4 = s3.map(x => modifyEvent(x, startsAt(after(s1.end))))\n# Judge whether predicted program is correct or not\n## Description of predicted program:\ns1: Find events about \"doctor\"\ns2: Find events this week\ns3: Filter the events in s2 to keep only those that are organized by me and intersect with s1 and are not s1\ns4: Modify the events in list s3 to happen after event s1\n## Utterance: Find all the meetings scheduled for next week that I created but that conflict with my doctor 's appointment. Reschedule them to after the\ndoctor 's appointment.\n## Reflection on predicted program:\nIn this example , the predicted program correctly reflects the user intent in the utterance.\n## Predicted program is: Correct\n<EOS >\n# Utterance: If I don't have an email about shiproom , then set up a 1:1 with Smith titled Discussion about Shiproom\n# Predicted program\nval s1 = findEmails(messageTitleIs (\" shiproom \"))\nval s2 = e.isEmpty\n# Judge whether predicted program is correct or not\n## Description of predicted program:\ns1: Find the emails about \"shiproom\"\ns2: Check is the list s1 is empty or not\n## Utterance: If I don't have an email about shiproom , then set up a 1:1 with Smith titled Discussion about Shiproom\n## Reflection on predicted program:\nIn this example , the predicted program does not create 1:1 meeting as per the user intent from the utterance. Thus , it is not sufficient.\n## Predicted program is: Incorrect\n<EOS >\n# Utterance: Update the team meeting on Wednesday , so that its after 8 AM and before 5 PM for Jack\n# Predicted program\nval s1 = theEvent(called (\\\"team meeting \\\") and queryAt(Wednesday))\nval s2 = thePerson(named (\\\" Jack \\\")).timeZone\nval s3 = modifyEvent(s1, startsAt(after (8.am) inZone s2) and endsAt(before (5.pm) inZone s2))\n# Judge whether predicted program is correct or not\n## Description of predicted program:\ns1: Find the event called \"team meeting\" on Wednesday.\ns2: Find out what time zone Jack is in\ns3: Update the event s1 to happen after 8am and before 5pm in the time zone s2.\n## Utterance: Update the team meeting on Wednesday , so that its after 8 AM and before 5 PM for Jack\n## Reflection on predicted program:\nIn this example , the predicted program correctly reflects the user intent in the utterance.\n## Predicted program is: Correct\n<EOS >\n# Utterance: Create a preparation meeting this week with the attendees of the project sync who report to me or my manager.\n# Predicted program\nval s1 = theEvent(called(\"project sync\"))\nval s2 = thePerson(me).directReports\nval s4 = s2.filter(x => s1.attendees.isAttending(x))\nval s6 = createEvent(with_(s4) and createAt(thisWeek) and called(\"preparation meeting\"))\n# Judge whether predicted program is correct or not\n## Description of predicted program:\ns1: Find the event called \"project sync\"\ns2: Find my reports\ns4: Filter the list of persons s2 to include only people attending the event s1\ns6: Create an event called preparation meeting this week with the list of persons s4\n## Utterance: Create a preparation meeting this week with the attendees of the project sync who report to me or my manager.\n## Reflection on predicted program:\nIn this example , the predicted program does not consider the people who report to user's manager.\nThus , it is not sufficient.\n## Predicted program is: Incorrect\n<EOS >\n# Utterance: <test -utterance >\n# Predicted Program: <predicted program >\n# Judge whether predicted program is correct or not\nFigure A7: Prompt used to compute Correct metric using GPT4 .\nFigure A8: The annotation interface for the step-by-step evaluation on the NL steps.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-05-15",
  "updated": "2024-01-08"
}