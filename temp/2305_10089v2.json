{
  "id": "http://arxiv.org/abs/2305.10089v2",
  "title": "A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization",
  "authors": [
    "Akira Kitaoka",
    "Riki Eto"
  ],
  "abstract": "We prove Wasserstein inverse reinforcement learning enables the learner's\nreward values to imitate the expert's reward values in a finite iteration for\nmulti-objective optimizations. Moreover, we prove Wasserstein inverse\nreinforcement learning enables the learner's optimal solutions to imitate the\nexpert's optimal solutions for multi-objective optimizations with lexicographic\norder.",
  "text": "arXiv:2305.10089v2  [cs.LG]  18 May 2023\nA proof of imitation\nof Wasserstein inverse reinforcement learning\nfor multi-objective optimization\nAkira Kitaoka\nNEC Corporation\nakira-kitaoka@nec.com\nRiki Eto\nNEC Corporation\nriki.eto@nec.com\nAbstract\nWe prove Wasserstein inverse reinforcement learning enables the learner’s reward\nvalues to imitate the expert’s reward values in a ﬁnite iteration for multi-objective\noptimizations.\nMoreover, we prove Wasserstein inverse reinforcement learning enables the\nlearner’s optimal solutions to imitate the expert’s optimal solutions for multi-\nobjective optimizations with lexicographic order.\n1\nIntroduction\nArtiﬁcial intelligence (AI) has been used to automate various tasks recently. Generally, automa-\ntion by AI is achieved by setting an index of goodness or badness (reward function) of a target\ntask and having AI automatically search for a decision, that is, an optimal solution in mathematical\noptimization that maximizes or minimizes the index. For example, in work shift scheduling (e.g.\n[CLLR03, GLLK79]), which is a type of combinatorial optimization or multi-objective optimiza-\ntion, we can create shifts that reﬂect our viewpoints by calculating the optimal solution of a reward\nfunction that reﬂects our intentions for several viewpoints, such as “degree of reﬂection of vacation\nrequests,” “leveling of workload,” and “personnel training,” and so on while preserving the required\nnumber of workers, required skills, labor rules. However, setting the reward function, i.e., \"what\nis optimal?\", manually requires a lot of trial-and-error, which is a challenge for the actual applica-\ntion of mathematical optimization. Creating a system that can solve this problem automatically is\nessential in freeing the user from manually designing the reward function.\nInverse reinforcement learning (IRL) [Rus98,NR00] is generally known as facilitating the setting of\nthe reward function. In IRL, a reward function that reﬂects expert’s intention is generated by learning\nexpert’s trajectories, iterating optimization using the reward function, and updating the parameters\nof the reward function. In IRLs which is fomulated by Ng and Russell [NR00], and Abbeel and Ng\n[AN04], in multi-objective optimization, the space of actions, i.e., the space of optimization results,\nis enormous. In other words, it is necessary to set the reward function for the space of actions and\nstates, which is computationally expensive.\nMaximum entropy IRL (MEIRL) [ZMBD08] and guided cost learning (GCL) [FLA16] are methods\nto adapt IRL to multi-objective optimization problems. However, these methods have their issues.\nFor example, MEIRL requires the sum of the reward functions for all trajectories to be computed.\nThis makes maximum entropy IRL computationally expensive. On the other hand, GCL approxi-\nmates the sum of the reward functions for all trajectories by importance sampling. However, since\nmulti-objective optimization problems take discrete values, it is difﬁcult to ﬁnd the probability dis-\ntribution corresponding to a given value when a speciﬁc value is input. One reason for this difﬁculty\nis that in multi-objective optimization problems, even a small change in the value of the reward\nfunction may result in a large change in the result.\nPreprint.\nEto proposed IRL for multi-objective optimization including combinatorial optimization, Wasser-\nstein inverse reinforcement learning (WIRL) [Eto22], inspired by Wasserstein generative adversarial\nnetworks [ACB17]. In multi-objective optimization problems, WIRL makes it possible to learn a\nreward function that reﬂects the expert’s decision-making data, i.e., the expert’s intentions.\nFor multi-objective optimization, Kitaoka and Eto showed WIRL is convergent [KE23]. However,\nwhen WIRL is convergent, there is no known proof that the learner’s reward functions and actions\nimitate the expert’s reward functions and actions. Eto proposed that we do inverse reinforcement\nlearning for multi-objective optimizations with WIRL [Eto22], although there was no theoretical\nexplanation for this phenomenon.\nIn this paper, we show that if WIRL for multi-objective optimization is convergent, then the learner’s\nreward values converges to the expert’s reward values. Moverover, we prove that when WIRL is\nconvergent for multi-objective optimization, the learner’s actions coincide with the expert’s actions.\nIn §2, we recall the deﬁnition of WIRL. In §3, we recall the deﬁnition and propositions of WIRL\nto multi-objective optimizations. In §4, we show that if WIRL for multi-objective optimization is\nconvergent, then the learner’s reward values converge to the expert’s reward values. In §5, we show\nwhen WIRL is convergent for multi-objective optimization, the learner’s actions coincide with the\nexpert’s actions.\n2\nWasserstein inverse reinforcement learning\nLet H, HS be inner product spaces, S ⊂HS be a space of states A ⊂H be a space of actions,\nT := Q\nk (S × A) be a space of trajectories. Let Θ ⊂H, and we call Θ a space of feature maps.\nLet Φ ⊂H, and we call Φ a space of parameters of learner’s trajectories. Let f• : T →Θ be\n1-Lipschitz, and we call f• the feature map. For any Lipschitz function rθ : T →R, the norm of\nLipschitz ∥rθ∥L is deﬁned by\n∥rθ∥L := sup\nτ1̸=τ2\n|rθ(τ1) −rθ(τ2)|\n∥τ1 −τ2∥\n.\nLet δx be the Delta function at x. Let {τ(n)\nE }\nN\nn=1 be the data of expert’s trajectories, and we deﬁne\nthe distribution of expert’s trajectories by\nPE := 1\nN\nN\nX\nn=1\nδτ (n)\nE .\nWith the initial state s(n)\nini of expert’s trajectory τ (n)\nE , and the generator g•(•): Φ×S →T of learner’s\ntrajectory, we deﬁne the distribution of learner’s trajectories by\nPφ := 1\nN\nN\nX\nn=1\nδgφ(s(n)\nini ).\nThe Wasserstein distance between the distribution PE of expert’s trajectories and that Pφ of learner’s\ntrajectories is, with the Kantrovich-Rubinstein duality (c.f. [Vil09]),\nW(PE, Pφ) =\nsup\n∥rθ∥L≤1\n(\n1\nN\nN\nX\nn=1\nrθ(τ (n)\nE ) −1\nN\nN\nX\nn=1\nrθ(gφ(s(n)\nini ))\n)\n,\nwhere rθ is 1-Lipschitz function.\nWe are interested in ﬁnding φ ∈Φ satisfying the following problem:\narg min\nφ∈Φ\nW(PE, Pφ).\n(2.1)\nWith\n{rθ(τ) := θ⊺fτ | θ ∈Θ} insted of {∥rθ∥L ≤1},\n(2.2)\nto ﬁnd φ ∈Φ satisfying equation (2.1) can be roughly replaced by ﬁnding\narg min\nφ∈Φ\nsup\nθ∈Θ\n(\n1\nN\nN\nX\nn=1\nθ⊺fτ (n)\nE\n−1\nN\nN\nX\nn=1\nθ⊺fgφ(s(n)\nini )\n)\n.\n(2.3)\n2\nBy changing the sign, we may consider solving\narg max\nφ∈Φ\ninf\nθ∈Θ\n(\n1\nN\nN\nX\nn=1\nθ⊺fgφ(s(n)\nini ) −1\nN\nN\nX\nn=1\nθ⊺fτ (n)\nE\n)\n.\n(2.4)\nThe IRL that solves equation (2.3) or equation (2.4), is called Wasserstein inverse reinforcement\nlearning (WIRL) [Eto22].\nRemark 2.1. In this paper, learning to maximize the reward function of a history-dependent policy\nis called reinforcement learning. Learning that minimizes the score between the reward function\ncalculated from the expert’s trajectory and the reward function learned by reinforcement learning is\ncalled inverse reinforcement learning.\n3\nWIRL for multi-objective optimization\nWe adapt WIRL to multi-objective optimization. Let H′ be an inner product space, A′ be a set such\nthat A′ ⊂H′, h: A′ →H be a continuous function. Let X(s) be a compact set1 in A′ for s ∈S. We\nset the space of trajectories T = S × A. Then, multi-objective optimization (e.g. [MIT96,Gun18])\nis to solve for the following optimization:\na(φ, s) ∈\narg max\nh(x)∈h(X(s))\nφ⊺h(x).\n(3.1)\nWe call the solution or the learner’s action a(φ, s) the solver. For φ ∈Φ and an action a ∈A, we\ncall φ⊺a the reward value.\nWe set the feature map f = ProjA, where ProjA : T →A is the projection from T to A. We deﬁne\nthe generator gφ(s) by\ngφ(s) := (s, a(φ, s)).\nWe say that intention learning with WIRL is the result of applying WIRL to the above setup.\nThe expert’s action a(n) is assumed to follow an optimal solution. Namely, we often run WIRL inten-\ntion learning by assuming that there exists some φ0 ∈Φ and that we can write a(n) = a(φ0, s(n)).\nRemark 3.1. Examples of adapting intention learning to linear and quadratic programming are\ndescribed in [KE23, §5].\nWe give the inverse propblem of the multi-objective optimization problem that is equivalent to the\nproblem handled by intention learning with WIRL.\nDeﬁnition 3.2. ([KE23, Deﬁnition 4.4]) Let H, HS, H′ be inner product spaces, S ⊂HS, A′ ⊂H′,\nΦ ⊂H be a closed convex set h: A′ →H be the continuous function, X(s) ⊂A′ be a compact\nnon-empty set for s ∈S.\nThen, the inverse problem of multi-objective optimization problem (IMOOP) for the solver a(φ, s)\nand trajectories of an expert {τ (n)\nE\n= (s(n), a(n))}n ⊂HS × H is to ﬁnd φ ∈Φ satisfying\nminimize F(φ) := 1\nN\nN\nX\nn=1\nφ⊺a(φ, s(n)) −1\nN\nN\nX\nn=1\nφ⊺a(n),\nsubject to φ ∈Φ.\n(3.2)\nProposition 3.3. ([KE23, Lemma 4.6]) In the setting of Θ = Φ, equation (3.2) is the replacement\nof maxφ∈Φ and infθ∈Θ in equation (2.4), that is,\nmin\nφ∈Φ\n(\n1\nN\nN\nX\nn=1\nφ⊺a(φ, s(n)) −1\nN\nN\nX\nn=1\nφ⊺a(n)\n)\n= min\nθ∈Φ max\nφ∈Φ\n(\n1\nN\nN\nX\nn=1\nθ⊺a(φ, s(n)) −1\nN\nN\nX\nn=1\nθ⊺a(n)\n)\n.\nThe subgradient of F is given by the following proposition:\n1If A′ is in the Euclid space, compact sets are bounded closed sets.\n3\nProposition 3.4. ([KE23, Lemma 4.8]) In the setting of Deﬁnition 3.2, one of the subgradient of F\nat φ ∈Φ is\n1\nN\nN\nX\nn=1\na(φ, s(n)) −1\nN\nN\nX\nn=1\na(n).\nThe algorithm of WIRL for multi-objective optimization is given by Algorithm 1.\nAlgorithm 1 Intention learning (with WIRL) [KE23, Algorithm 1]\n1: initialize φ1 ∈Φ\n2: for k = 1, . . . , K −1 do\n3:\nφk+1 ←φk −αk\nN\nPN\nn=1\n\u0000a(φk, s(n)) −a(n)\u0001\n4:\nprojection onto Φ for φk+1\n5: end for\n6: return φbest\nK\n∈arg minφk∈{φk}K\nk=1 F(φk)\nProposition 3.5. ([KE23, Lemma 4.11]) In the setting of Deﬁnition 3.2, the algorithm which solves\nIMOOP for the solver a(φ, s) coninsides with Algorithm 1. Here, {αk}k is a nonsummable dimin-\nishing learning rate, that is,\nlim\nk→∞αk = 0,\n∞\nX\nk=1\nαk = ∞.\nAs a natural question from Propositions 3.4 and 3.5, when the WIRL is close to completion or\na subgradient\n1\nN\nPN\nn=1 a(φ, s(n)) −\n1\nN\nPN\nn=1 a(n) is 0, whether the learner’s reward values and\nactions imitate the expert’s.\n4\nImitation of intention learning concerning reward value\nIn this section, we show that intention learning enables the learner to imitate reward values that\nreﬂects the expert’s intentions.\nTheorem 4.1. Let HS, H′ be inner product spaces, S ⊂HS, A′ ⊂H′, Φ ⊂Rd be a closed convex\nset, h: A′ →Rd be the continuous function, X(s) ⊂A′ be a compact non-empty set for s ∈S. We\nassume that there exists φ0 ∈Φ such that a(n) = a(φ0, s(n)) for any n. Let ε > 0.\nThen, if\nF(φ) < ε,\nthen for any n, we have\n0 ≤φ⊺a(φ, s(n)) −φ⊺a(φ0, s(n)) < εN.\nproof. By the deﬁnition of the solver equation (3.1), we note that a(φ, s(n)) ∈h(X(s(n))). By the\ndeﬁnition of the solver equation (3.1), we obtain\nφ⊺a(φ, s(n)) ≥φ⊺a(φ0, s(n)).\nWith the above inequality, we see\nF(φ) = 1\nN\nN\nX\nn=1\nφ⊺a(φ, s(n)) −1\nN\nN\nX\nn=1\nφ⊺a(n)\n= 1\nN\n\u0010\nφ⊺a(φ, s(n)) −φ⊺a(φ0, s(n))\n\u0011\n.\nTherefore if F(φ) < ε, then\nφ⊺a(φ, s(n)) −φ⊺a(φ0, s(n)) < εN.\n4\nIf there exists φ0 ∈Φ so that a(n) = a(φ0, s(n)) for any n, then\nmin\nφ∈∆F(φ) = 0\nKitaoka and Eto showed that the intention learning with WIRL is covnergence [KE23].\nProposition 4.2. ([KE23, Theorem 4.12]) In the setting of Theorem 4.1, we assume that F has the\nminimum on Φ.\nThen, a sequence {φbest\nk\n}k calculated by the intention learning with WIRL has the following prop-\nerty: for any ε > 0 there exists an natural number K so that for any integer k > K,\nF(φbest\nk\n) < ε.\nTo combine Theorem 4.1 and Proposition 4.2, we obtain the following corollary:\nCorollary 4.3. In the setting of Proposition 4.2, a sequence {φbest\nk\n}k calculated by the intention\nlearning with WIRL has the following property: for any ε > 0 there exists an natural number K so\nthat for any integer k > K,\n0 ≤φbest\nk\n⊺a(φbest\nk\n, s(n)) −φbest\nk\n⊺a(φ0, s(n)) < εN.\nCorollary 4.3 means that intention learning enables the learner’s reward values to imitate the expert’s\nreward values in linear and quadratic programming problems, integer programming problems, mixed\ninteger programming problems, and so on.\n5\nImitation of intention learning concerning action\nIn this section, we set H = Rd, the d-dimensional Euclid space. Before showing the imitation of\nintention learning concerning the action, we change the deﬁnition of the solver a(φ, s):\na(φ, s) := min\ndic\narg max\nh(x)∈h(X(s))\nφ⊺h(x),\n(5.1)\nwhere mindic returns to the minimal of the lexicographical order ≤dic.23\nRemark 5.1. In [Eto22,KE23] and §4 we deﬁne the learner’s action, the solver by\na(φ, s) ∈\narg max\nh(x)∈h(X(s))\nφ⊺h(x).\nTo prove Theorem 5.2, which we discuss later, we use lexicographic order in equation (5.1) to deﬁne\nthe learner’s actions.\nFor practical purposes, it is also conceivable to output only one solution when running multi-\nobjective optimization. As one of the solutions, it is natural to choose the smallest one in the sense\nof lexicographic order.\nWe show that intention learning enables the learner to imitate an action that reﬂects the expert’s\nintentions:\nTheorem 5.2. Let HS, H′ be inner product spaces, S ⊂HS, A′ ⊂H′, Φ ⊂Rd be a closed convex\nset, h: A′ →Rd be the continuous function, X(s) ⊂A′ be a compact non-empty set for s ∈S. We\nassume that there exists φ0 ∈Φ such that a(n) = a(φ0, s(n)) for any n.\nThen, for φ ∈Φ, the following are equivalent:\n(1) The subgradient of F(φ) at φ ∈Φ is\nN\nX\nn=1\n\u0010\na(φ, s(n)) −a(φ0, s(n))\n\u0011\n= 0.\n2For x, y ∈Rd, we deﬁne x ≤dic y if and only if there exists 1 ≤k ≤d such that for any 1 ≤i ≤k −1,\nxi = yi and xk ≤yk. We call the order ≤dic the lexicographical order.\n3Let B ⊂Rd. The element b ∈Rd is the minimum of B of the lexicographical order (Rd, ≤dic), if and\nonly if for any x ∈B, b ≤dic x. We set mindic B := b.\nFor example, we set B = {(0, 0), (1, −1), (−1, 1)}. To compare the ﬁrst component, we obtain mindic B =\n(−1, 1).\n5\n(2) For any n, gφ(s(n)) = gφ0(s(n)), that is, a(φ, s(n)) = a(φ0, s(n)).\n(3) W(Pφ, Pφ0) = 0.\nFrom the equivalence of (1) and (2) in Theorem 5.2, the completion of intention learning implies that\nthe learner’s actions perfectly imitate the expert’s in linear programming, quadratic programming,\netc.\nLemma 5.3. A sufﬁcient condition for a function rθ(τ) := θ⊺fτ to be 1-Lipschitz for τ is\n∥θ∥≤1/∥f∥L.\nproof. A sufﬁcient condition for the function rθ(τ) to be 1-Lipschitz for τ is\n|θ⊺fτ1 −θ⊺fτ2|\n∥τ1 −τ2∥\n≤1.\nBy Cauchy-Schwarz’s inequality, we have\n|θ⊺fτ1 −θ⊺fτ2| ≤∥θ∥∥fτ1 −fτ2∥.\nIf\n∥θ∥∥fτ1 −fτ2∥\n∥τ1 −τ2∥\n≤1,\nthen rθ(τ) is 1-Lipschitz for τ. Therefore, to apply supτ1̸=τ2 to both side, we obtain the sufﬁcient\ncondition for the function rθ(τ) to be 1-Lipschitz for τ,\n∥θ∥∥f∥L ≤1.\nproof of Theorem 5.2. (2) ⇒(3) We assume that gφ(s(n)) = gφ0(s(n)) for n. Then,\nW(Pφ, Pφ0) =\nsup\n∥rθ∥L≤1\n(\n1\nN\nN\nX\nn=1\nrθ(gφ(s(n)\nini )) −1\nN\nN\nX\nn=1\nrθ(gφ0(s(n)\nini ))\n)\n=\nsup\n∥rθ∥L≤1\n{0} = 0.\n(3) ⇒(1) For the feature map f, we assume thet\nN\nX\nn=1\n\u0010\nfgφ(s(n)\nini ) −fτ (n)\nE\n\u0011\n̸= 0\n(5.2)\nSince there exists n such that\nfgφ(s(n)\nini ) −fτ (n)\nE\n̸= 0,\nwe see\n0 <\n\r\r\rfgφ(s(n)\nini ) −fτ (n)\nE\n\r\r\r\n\r\r\rgφ(s(n)\nini ) −τ (n)\nE\n\r\r\r\n≤∥f∥L,\ni.e., ∥f∥L ̸= 0. We take\nθ∗:=\narg max\n∥θ∥≤1/∥f∥L\nθ⊺1\nN\nN\nX\nn=1\n\u0010\nfgφ(s(n)\nini ) −fτ (n)\nE\n\u0011\n=\n1\n∥f∥L\n1\nN\nPN\nn=1\n\u0010\nfgφ(s(n)\nini ) −fτ (n)\nE\n\u0011\n\r\r\r 1\nN\nPN\nn=1\n\u0010\nfgφ(s(n)\nini ) −fτ (n)\nE\n\u0011\r\r\r\n.\nFrom equation (2.2) and Lemma 5.3,\nW(Pφ0, Pφ) ≥\nsup\n∥θ∥≤1/∥f∥L\n(\nθ⊺\n \n1\nN\nN\nX\nn=1\n\u0010\nfgφ(s(n)) −fgφ0(s(n))\n\u0011!)\n= θ∗⊺1\nN\nN\nX\nn=1\n\u0010\nfgφ(s(n)\nini ) −fτ (n)\nE\n\u0011\n=\n1\n∥f∥L\n\r\r\r\r\r\n1\nN\nN\nX\nn=1\n\u0010\nfgφ(s(n)\nini ) −fτ (n)\nE\n\u0011\r\r\r\r\r .\n6\nFrom the assumption (3), we have\n0 ≤\n1\n∥f∥L\n\r\r\r\r\r\n1\nN\nN\nX\nn=1\n\u0010\nfgφ(s(n)\nini ) −fτ (n)\nE\n\u0011\r\r\r\r\r ≤W(Pφ0, Pφ) = 0.\nTherefore,\nN\nX\nn=1\n\u0010\nfgφ(s(n)\nini ) −fτ (n)\nE\n\u0011\n= 0.\n(5.3)\nIt contradicts equation (5.2).\nSubstituting f = ProjA for equation (5.3), we get\nN\nX\nn=1\n\u0010\na(φ, s(n)) −a(φ0, s(n))\n\u0011\n= 0.\n(2) ⇒(3) We assume that the subgradient of F is given by\nN\nX\nn=1\n\u0010\na(φ, s(n)) −a(φ0, s(n))\n\u0011\n= 0\nTo act φ⊺\n0 on the both side, we have\nN\nX\nn=1\n\u0010\nφ⊺\n0a(φ, s(n)) −φ⊺\n0a(φ0, s(n))\n\u0011\n= 0.\nSince by the deﬁnition of the solver a(φ, s),\nφ⊺\n0a(φ0, s(n)) ≥φ⊺\n0a(φ, s(n)),\nfor all n, we have\nφ⊺\n0a(φ0, s(n)) = φ⊺\n0a(φ, s(n)).\nBy the deﬁnition of the solver a(φ, s), we see\na(φ, s(n)) ∈\narg max\nh(x)∈h(X(s(n)))\nφ⊺\n0h(x)\nTherefore, we obtain\na(φ0, s(n)) ≤dic a(φ, s(n)).\nAs the same way, to replace to φ0 and φ, we obtain\na(φ, s(n)) ≤dic a(φ0, s(n)).\nSumming up, we have\na(φ, s(n)) = a(φ0, s(n)).\n6\nRelated work\nMaximal entropy inverse reinforcement learning\nHo and Ermon showed that MEIRL is the inverse problem of maximum entropy reinforcement\nlearning [HE16, Corollary 3.2.1] . Signiﬁcant differences exist between the MEIRL setup used by\nGAIL and the WIRL setup. First, they differ in the design of the reward function: MEIRL uses\nan entropy-regularized value function as the reward function for maximum entropy reinforcement\nlearning, whereas WIRL uses a multi-objective optimization objective function as the reward func-\ntion. Second, the settings of state space and action space are different. [HE16] assumes that the state\nspace and action space are ﬁnite sets. In WIRL, on the other hand, the state space and action space\nare allowed to be both ﬁnite and inﬁnite sets. Therefore, the argument in [HE16] that measures are\nreplaced by occupancy measures and attributed to Lagrange’s undetermined multiplier method for\noccupancy measures and cost functions cannot be applied to multi-objective optimization.\n7\n7\nConclusion\nIntention learning concerning reward\nIf the generator gφ represents the expert’s action, then when WIRL converges for multi-objective\noptimization, we show Theorem 4.1, which claims the learner’s reward values are convergent to\nthe expert’s. On the other hand, Kitaoka and Eto showed WIRL converges for multi-objective opti-\nmization [KE23, Theorem 4.12]. To combine these theorem, we get Corollary 4.3, that is, intention\nlearning with WIRL enables the learner’s reward values to imitate the expert’s reward values in a\nﬁnite number of iterations. It means intention learning that WIRL is theoretically guaranteed to have\na mechanism that frees users from manually designing the reward values.\nIntention learning concerning action\nIf the generator gφ represents the expert’s action, then when WIRL converges for multi-objective\noptimization, the learner’s optimization actions coincide with the expert’s actions Theorem 5.2. On\nthe other hand, Kitaoka and Eto showed WIRL converges for multi-objective optimization [KE23,\nTheorem 4.12]. To combine these theorems, intentional learning with WIRL can theoretically be\nsaid to converge in the direction that the learner’s actions imitate the expert’s actions.\nAs a feature work, one question is whether intention learning with WIRL converges in a ﬁnite\nnumber of iterations. Kitaoka and Eto showed WIRL converges for multi-objective optimization\n[KE23, Theorem 4.12]. However, since it is not possible to actually try inﬁnite iterations, it is neces-\nsary to guarantee that intention learning with WIRL converges in a ﬁnite number of iterations. If we\ncan show this, then intention learning with WIRL is theoretically guaranteed to have a mechanism\nthat frees users from manually designing the action or solver.\nCases where expert actions are not represented by generators\nWe raise some future works. Suppose the expert’s actions are not represented by the generator gφ.\nIn that case, it is interesting whether the learner’s actions mimic the expert’s actions when WIRL\nfor multi-objective optimization converges. Ideally, the expert’s actions would be represented by the\ngenerator gφ. In reality, however, writing down the expert’s actions in a mathematical model is not\nalways possible.\nReferences\n[ACB17] M. Arjovsky, S. Chintala, and L. Bottou, Wasserstein generative adversarial networks,\n2017, pp. 214–223.\n[AN04] P. Abbeel and A. Y Ng, Apprenticeship learning via inverse reinforcement learning,\nThe 21th International Conference on Machine Learning, 2004, pp. 1.\n[CLLR03] B. Cheang, H. Li, A. Lim, and B. Rodrigues, Nurse rostering problems–a bibliographic\nsurvey, European Journal of Operational Research 151 (2003), no. 3, 447–460.\n[Eto22] R. Eto, Learning device, learning method, and learning program, 2022. Publication\nNumber WO2022/137520, International Application No. PCT/JP2020/048791.\n[FLA16] C. Finn, S. Levine, and P. Abbeel, Guided cost learning: Deep inverse optimal control\nvia policy optimization, The 33rd International Conference on Machine Learning, 2016,\npp. 49–58.\n[GLLK79] R. L. Graham, E. L. Lawler, J. K. Lenstra, and A. R. Kan, Optimization and approxi-\nmation in deterministic sequencing and scheduling: a survey, Annals of Discrete Math-\nematics, 1979, pp. 287–326.\n[Gun18] N. Gunantara, A review of multi-objective optimization: Methods and its applications,\nCogent Engineering 5 (2018), no. 1, 1502242.\n[HE16] J. Ho and S. Ermon, Generative adversarial imitation learning, The 30th Conference\non Neural Information Processing Systems, 2016, pp. 4565–4573.\n8\n[KE23] A.\nKitaoka\nand\nR.\nEto,\nA\nproof\nof\ncongergence\nof\ninverse\nreinforce-\nment\nlearning\nfor\nmulti-objective\noptimization,\n2023.\nAvailable\nat\nhttps://arxiv.org/abs/2305.06137.\n[MIT96] T. Murata, H. Ishibuchi, and H. Tanaka, Multi-objective genetic algorithm and its appli-\ncations to ﬂowshop scheduling, Computers & Industrial Engineering 30 (1996), no. 4,\n957–968.\n[NR00] A. Y Ng and S. Russell, Algorithms for inverse reinforcement learning, The 17th Inter-\nnational Conference on Machine Learning, 2000, pp. 663–670.\n[Rus98] S. Russell, Learning agents for uncertain environments, The 11th annual conference on\nComputational Learning Theory, 1998, pp. 101–103.\n[Vil09] C. Villani, Optimal transport: old and new, Vol. 338, Springer, 2009.\n[ZMBD08] B. D Ziebart, A. L Maas, J A. Bagnell, and A. K Dey, Maximum entropy inverse\nreinforcement learning, The 23rd AAAI Conference on Artiﬁcial Intelligence, 2008,\npp. 1433–1438.\n9\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2023-05-17",
  "updated": "2023-05-18"
}