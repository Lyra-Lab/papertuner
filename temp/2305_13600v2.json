{
  "id": "http://arxiv.org/abs/2305.13600v2",
  "title": "SiCL: Silhouette-Driven Contrastive Learning for Unsupervised Person Re-Identification with Clothes Change",
  "authors": [
    "Mingkun Li",
    "Peng Xu",
    "Chun-Guang Li",
    "Jun Guo"
  ],
  "abstract": "In this paper, we address a highly challenging yet critical task:\nunsupervised long-term person re-identification with clothes change. Existing\nunsupervised person re-id methods are mainly designed for short-term scenarios\nand usually rely on RGB cues so that fail to perceive feature patterns that are\nindependent of the clothes. To crack this bottleneck, we propose a\nsilhouette-driven contrastive learning (SiCL) method, which is designed to\nlearn cross-clothes invariance by integrating both the RGB cues and the\nsilhouette information within a contrastive learning framework. To our\nknowledge, this is the first tailor-made framework for unsupervised long-term\nclothes change \\reid{}, with superior performance on six benchmark datasets. We\nconduct extensive experiments to evaluate our proposed SiCL compared to the\nstate-of-the-art unsupervised person reid methods across all the representative\ndatasets. Experimental results demonstrate that our proposed SiCL significantly\noutperforms other unsupervised re-id methods.",
  "text": "SiCL: Silhouette-Driven Contrastive Learning for Unsupervised Person\nRe-Identification with Clothes Change\nMingkun Li†,1, Peng Xu∗,2, Chun-Guang Li†,1, Jun Guo †,1\n1Beijing University of Posts and Telecommunications, 2Tsinghua University\n†{mingkun.li,lichunguang, guojun}@bupt.edu.cn,∗peng xu@tsinghua.edu.cn\nAbstract\nIn this paper, we address a highly challenging yet critical\ntask: unsupervised long-term person re-identification with\nclothes change. Existing unsupervised person re-id methods\nare mainly designed for short-term scenarios and usually\nrely on RGB cues so that fail to perceive feature patterns\nthat are independent of the clothes. To crack this bottle-\nneck, we propose a silhouette-driven contrastive learning\n(SiCL) method, which is designed to learn cross-clothes in-\nvariance by integrating both the RGB cues and the silhou-\nette information within a contrastive learning framework.\nTo our knowledge, this is the first tailor-made framework\nfor unsupervised long-term clothes change re-id, with su-\nperior performance on six benchmark datasets. We con-\nduct extensive experiments to evaluate our proposed SiCL\ncompared to the state-of-the-art unsupervised person re-id\nmethods across all the representative datasets. Experimen-\ntal results demonstrate that our proposed SiCL significantly\noutperforms other unsupervised re-id methods.\n1. Introduction\nPerson re-identification (re-id) aims to match person iden-\ntities of bounding box images that are captured from dis-\ntributed camera views [35]. Most conventional studies for\nunsupervised person re-id have only focused on the sce-\nnarios without clothes change [7, 37]. However, re-id in\nsuch scenarios is unrealistic since people may often change\ntheir clothing. Thus, these studies for the scenarios with-\nout clothes change may only be useful in the short-term re-\nid settings but fail to handle the long-term person re-id for\nwhich we have to face the scenarios with clothes changes.\nRecently, there have been some attempts to address the\nlong-term re-id task [11, 33].\nHowever, all of these at-\ntempts employ supervised learning methods which heav-\nily rely on large amounts of labelled training data.\nUn-\nfortunately, collecting and annotating person identity labels\nunder the scenario of unconstrained clothes change is ex-\nInstance-level Neighbor Structure\nCluster-Level Neighbor Structure\nRGB Image & Silhouette Mask \nRGB Image Clustering & Result Sharing\nFeature  Fusion & Neighbor Searching\nFigure 1. Illustration of hierarchical fusion clustering: At a lower\nlevel, clustering is used to reveal the neighbor structure of in-\nstances. At a higher level, silhouette features and RGB features\nare integrated to exploit the neighbor structure of the clusters.\ntremely difficult. Preparing labelled re-id training data, e.g.,\nDeepChange [32], in a realistic scenario is quite expensive\nand exhausting.\nDue to the significance of long-term person re-id, it is\nappealing to develop unsupervised method to address long\nterm person re-id problem without the tedious requirement\nof person identity labeling. This is a more complex but\nmore realistic extension of the previous unsupervised short-\nterm person re-id [8, 21, 22] that different people may have\nsimilar clothes whilst the same person might wear variant\nclothes with very distinct appearances, as shown in Fig. 2.\nUnfortunately, prior investigations on unsupervised person\nre-identification have neglected situations involving cloth-\ning changes.\nConventional approaches are incapable of\ncapturing clothing-independent patterns since they solely\nrely on RGB features as their driving force [20]. Specifi-\ncally, the majority of existing unsupervised methods [5] are\narXiv:2305.13600v2  [cs.CV]  7 Apr 2024\ncluster-oriented and tend to yield feature extractions primar-\nily dominated by color [20].\nAs a consequence, the clustering algorithm will blindly\nassign every training sample with a color-based pseudo la-\nbel, which is error-prone with a large cumulative propa-\ngation risk and ultimately leads to sub-optimal solutions,\nsuch as simply grouping the individuals who wear the same\nclothing.\nIn this paper, we propose a novel silhouette-driven con-\ntrastive learning framework, termed SiCL, for attacking the\nchallenging task of unsupervised person re-id in long-term\nsetting. In SiCL, we incorporate the person silhouette and\nRGB images into a contrastive learning framework to learn\ncross-clothes invariance features. Specifically, SiCL em-\nploys a dual-branch network to perceive both silhouette and\nRGB image features and incorporates them to construct a\nhierarchical neighbor structure, as demonstrated in Fig. 1.\nThe RGB feature is used to construct the low-level instance\nneighbor structure, and the fused features are used to con-\nstruct the high-level cluster neighbor structure. Different\nfrom other methods that are only relying on RGB patterns\nto construct the single-level neighbor structure, in SiCL,\nwe incorporates clothing-independent features hidden in the\nsilhouette. Such an additional feature can provide further\nguidance to modeling the invariant features across different\nclothes. Moreover, we introduce a contrast-learning module\nto learn invariant features between the silhouette and RGB\nimages at various neighbor structure levels.\nTo validate the effectiveness of our SiCL approach, we\nconduct comprehensive evaluation on six long-term person\nre-id datasets comparing to the state-of-the-art unsupervised\nperson re-identification (re-id) methods. Experimental re-\nsults demonstrate the superior performance of our approach.\nNotably, our SiCL can not only outperforms all short-term\nmethods significantly, but also achieves comparable perfor-\nmance to the the state-of-the-art fully supervised methods.\nThe main contribution of the paper can be summarized\nas follows.\n1. We propose a silhouette-driven contrastive learning\nframework for unsupervised long-term person re-id with\nclothes change. To the best of our knowledge, this is the\nfirst work to investigate unsupervised long-term person\nre-id with clothes change.\n2. We propose to incorporates both person silhouette infor-\nmation and hierarchical neighbor structure into a con-\ntrastive learning framework to guide the model for learn-\ning cross-clothes invariance feature.\n3. We conduct extensive experiments on six representative\ndatasets to evaluate the performance of the proposed\nSiCL and the state-of-the-art unsupervised re-id meth-\nods.\nFigure 2. Visualizing the intrinsic challenges in long-term per-\nson re-id with clothes change.\nWe randomly selected 28 im-\nages of a single individual from the DeepChange [32] dataset.\nEvidently, the variances in visual characteristics between differ-\nent outfits (rows) are considerably more pronounced compared to\nthose within the same outfit (each column).\n2. Related Work\n2.1. Long-Term Person Re-Identification\nA few recent person re-id studies attempt to tackle [31, 33]\nthe long-term clothes-changing situations via supervised\ntraining, and emphasize the use of additional supervision\nfor the general appearance features (e.g., clothes, color),\nto enable the model to learn the cross-clothes features.\nFor example, [33] generates contour sketch images from\nRGB images and highlights the invariance between the\nsketch and RGB. [11] explores fine-grained body shape fea-\ntures by estimating masks with discriminative shape details\nand extracting pose-specific features.\nWhile these semi-\nnal works provide inspiring attempts for re-id with clothes\nchange, there are still some limitations: a) Generating con-\ntour sketches or other side-information requires additional\nmodel components and will cause extra computational cost;\nb) To the best of our knowledge, all models designed for\nre-id clothes change are supervised learning methods, and\nthus lack of generalization ability to the open-world dataset\nsettings. In this paper, we attempt to tackle the challenging\nre-id with clothes change under unsupervised setting.\n2.2. Unsupervised Person Re-Identification\nTo avoid the high consumption in data labeling, a large and\ngrowing body of literature has investigated unsupervised\nperson re-id [30, 38]. The existing unsupervised person Re-\nID methods can be divided into two categories: a) unsuper-\nvised domain adaptation methods, which require a labeled\nsource dataset and an unlabelled target dataset [25, 30]; and\nb) purely unsupervised methods that work with only an un-\nlabelled dataset [3, 5]. However, up to date, the unsuper-\nvised person re-id methods are focusing on short-term sce-\nnarios, none of them taking into account the long-term re-\nid scenario. To the best of our knowledge, this is the first\nattempt to address the long-term person re-id in unsuper-\nvised setting. Thus, as a byproduct, we systematically and\ncomprehensively evaluate the performance of the existing\nthe state-of-the-art unsupervised methods [3, 8] on six long-\nterm person re-id datasets and set up a preliminary bench-\nmarking ecosystem for the long-term person re-id commu-\nnity.\n3. Our Methodology: Silhouette-Driven Con-\ntrastive Learning (SiCL)\nOur SiCL approach integrates silhouette and RGB images\ninto a contrastive learning framework assisted with the\nguidance from hierarchical clustering structure. For clarity,\nwe provide a flowchart of our SiCL in Fig. 3. Our SiCL con-\nsists of two network branches: F(·|Θ) and F ′(·|Θ′), which\nperceive RGB and silhouette patterns, respectively. We use\nΘ and Θ′ to denote the corresponding network parameters.\nIn our SiCL, the parameters Θ and Θ′ are learned separately\n(without parameters sharing). We design a predictor layer\nG(·|Ψ) which follows the RGB branch, where Ψ denotes\nthe parameters in the predictor layer. In addition, we also\ndesign a feature fusion layer R(·|Ω), where Ωdenotes the\nparameters.\nGiven an unlabeled pedestrian image dataset I\n=\n{Ii}N\ni=1 consisting of N samples, we generate the corre-\nsponding pedestrian silhouette mask dataset S = {Si}N\ni=1\nthrough human parsing network such as SCHP [23]. For an\ninput image Ii ∈I, we use the Ii as the input of F(·|Θ)\nand Si as the inputs of F ′(·|Θ′). For simplicity, we denote\nthe output features of the F(·|Θ) and F ′(·|Θ′) as xi and ˜xi,\ndenote the output of the predictor layer in the G(·|Ψ) as zi,\nand denote the output of fusion layer R(·|Ω) as fi, where\nxi, ˜xi, zi, fi ∈RD.\nAs illustrated in Fig. 3, our SiCL compose of a con-\ntrastive learning module and a hierarchical fusion cluster-\ning module. In hierarchical fusion clustering module, the\nhierarchical neighbor structure is generated by leveraging\nRGB and silhouette features at both the instance-level and\ncluster-level. In contrastive learning module, we introduce\nthe person’s silhouette and use the hierarchical neighbor\nstructure as supervision information to train model, and\ninvestigate cross-clothes features via contrastive learning.\nFurthermore, we build an adaptive learning strategy to auto-\nmatically modify the hierarchical neighbor selection, which\ncan flexibly select neighbor clusters according to dynamic\ncriteria.\nTo store the outputs of the two branches and the fu-\nsion layer, in SiCL, we maintain the three instance-memory\nbanks M = {vi}N\ni=1,\n˜\nM = {˜vi}N\ni=1, and ˆ\nM = {ˆvi}N\ni=1,\nrespectively, where vi, ˜vi, ˆvi ∈RD. Memory banks M and\nˆ\nM are initialized with X := {x1, · · · , xN} and ˜\nM is ini-\ntialized with ˜\nX := {˜x1, · · · , ˜xN}, where X and ˜\nX are the\noutputs of F(·|Θ) and F ′(·|Θ′).\n3.1. Hierarchical Fusion Clustering\nTo construct a hierarchical neighbor structure, we sort the\nhierarchical neighbor structure into two levels: a) low-level\ninstance neighbors, and b) high-level cluster neighbors.\nLow-level Instance Neighbors. We use F(·|Θ) outputs\nto construct m clusters C := {C(1), C(2), · · · , C(m)}. The\nclustering result C indicates the connection between neigh-\nbors at the instance level since that if samples are clustered\ntogether, it also indicates that their RGB features are simi-\nlar. In the subsequent training process, this clustering result\nC will also be used to guide the training of branch F ′(·|Θ′).\nHigh-level Cluster Neighbors. Since that the silhouette\nmasks contain richer clothing invariance features, we use\nthem to find those pedestrian samples that are similar at the\ncluster level, e.g., the same person wearing different cloth-\ning. To be specific, we fuse the RGB feature and the sil-\nhouette feature to renew the representation of instance, and\nsearch the cluster neighbor based on fusion features. For an\nimage Ii, the fusion feature fi is defined as:\nfi = F(concate(xi|˜xi)),\n(1)\nwhere concate(·) denotes the channel-wise concatenating\nxi and ˜xi. Based on the fused features, we define the cluster\ncenter uω(Ii) as\nuω(Ii) =\n1\n|C(ω(Ii))|\nX\nIj∈C(ω(Ii))\nfj,\n(2)\nwhere ω(Ii) is the cluster index of image Ii.\nOnce constructing the cluster centre U = {ui}m\ni=1, we\nwill find the cluster-level nearest neighbours based on the\nsimilarity of the cluster centres Once contrasting the cluster-\ncenter , we find cluster-level neighbors and construct the\nneighbor set N. Specifically, for cluster C(ℓ), we define the\nsimilarity between clusters C(ℓ) and C(i) as\nD(C(ℓ), C(i)) =\nu⊤\nℓ\n∥uℓ∥2\nui\n∥ui∥2\n.\n(3)\nWe denote the cluster neighbor set of cluster C(ℓ) as N (ℓ),\nwhich includes the top-k similar clusters C(i) sorted by\nD(C(ℓ), C(i)). Then, we form the total cluster-level neigh-\nbor set A := {N (1), · · · , N (m)}. This process is known as\nhierarchical fusion clustering due to the use of a fused fea-\nture operation and the construction of a multi-level neighbor\nstructure.\nIn the hierarchical fusion clustering stage, we con-\nstruct neighbor structures based on some specific cluster-\ning algorithm and cluster-level neighbor searching, respec-\ntively. The clustering result of the output features X :=\n{x1, · · · , xN} from F(·|Θ) is used to generate the pseudo\nlabels Y := {y1, · · · , yN}, and cluster-level neighbor set\nA contains the neighbor index for each cluster. In the con-\ntrastive learning stage, the hierarchical neighbor structure\nis used as supervision information to guide the model learn\ncross-clothes features.\nContrastive Learning\nModule\nContrastive Learning module\nSilhouette Branch\nHierarchical \nFusion \nClustering\nFeature\nFusion\nModule\nHierarchical Neighbor Structure\nRGB Branch\nLow-level Instance-level Neighbor\nHigh-level Cluster-level Neighbor\nInstance \nClustering\nCluster \nSimilarity\nCluster-level \nNeighbor Contrastive\nLearning Module\nCross-View \nContrastive\nLearning Module\nPrototypical \nContrastive \nLearning Module\nBackbone\nBackbone\nFigure 3. Our proposed Silhouette-Driven Contrastive Learning (SiCL) framework.\n3.2. Contrastive Learning Module\nTo effectively explore the invariance features between RGB\nimages and silhouette masks, we construct three contrastive\nlearning modules to train SiCL assisted with the self-\nsupervision information provided by the hierarchical fusion\nclustering as follows: a) Prototypical contrastive learning\nmodule, which is used for contrast training between posi-\ntive samples and negative pairs; b) Cross-view contrastive\nlearning module, which is used for contrast training be-\ntween RGB images and silhouette masks; and c) Cluster-\nlevel neighbor contrastive learning module, which is used\nfor contrast training between cluster-level neighbor clusters\nand negative pairs.\nPrototypical Contrastive Learning Module.\nWe apply\nprototypical contrastive learning to discover the hidden in-\nformation inside the cluster structure. For the i-th instance,\nwe denote its cluster index as ω(Ii), the center of Cω(Ii) as\nthe positive prototype, and all other cluster centers as the\nnegative prototypes. We define the prototypical contrastive\nlearning loss as follows:\nLP = −\nX\nq∗∈{q,˜q,ˆq}\n(1 −q∗\ni )2 ln(q∗\ni ),\n(4)\nwhere qi, ˜qi and ˆqi measure the consistency between the\noutputs of F(·|Θ), F ′(·|Θ′) and R(·|Ω) and the related pro-\ntotype computed with the memory bank and are defined as\nqi =\nexp(p⊤\nω(Ii)xi/τ)\nPm\nℓ=1 exp(p⊤\nℓxi/τ),\n(5)\nwhere pω(Ii) as the RGB prototype vector of the cluster\nC(ω(Ii)) is defined by\npω(Ii) =\n1\n|C(ω(Ii))|\nX\nIj∈C(ω(Ii))\nvj,\n(6)\nhere vj is the instance feature of image Ij in M, ˜pi and ˆpi\nare calculated in the same way with corresponding instance\nmemory bank ˜\nM and ˆ\nM, respectively.\nThe prototypical contrastive learning module performs\ncontrastive learning between positive and negative proto-\ntypes to improve the discriminant ability for the networks\nF(·|Θ) and F ′(·|Θ′) and the feature fusion layer R(·|Ω).\nCross-view Contrastive Learning Module. To effectively\ntrain the contrastive learning framework across the two\nviews, we design a cross-view contrastive module to mine\nthe invariance between RGB images and silhouette masks.\nTo match the feature outputs of two network branches at\nboth the instance level and cluster level, specifically, we\nintroduce the negative cosine similarity of the outputs of\nG(·|Ψ) and F ′(·|Θ′) to define the two-level contrastive\nlosses as follows:\nLC := −z⊤\ni\n∥zi∥2\n˜xi\n∥˜xi∥2\n−\nz⊤\ni\n∥zi∥2\n˜pω(Ii)\n∥˜pω(Ii)∥2\n,\n(7)\nwhere ∥· ∥2 is the ℓ2-norm.\nThe cross-view contrastive learning module explores the\ninvariance between RGB images and silhouette masks and\nthus assist the network to mine the invariance information\nprovided by the RGB image and the silhouette mask, as\nwell as imposing such a self-supervision information on the\nmodule for learning clothing-unrelated features.\nCluster-level Neighbor Contrastive Learning Module.\nTo avoid training the model into degeneration that only\npush samples with similar appearance together, we design a\ncluster-level neighbor contrastive learning module. Partic-\nularly, we propose a weighted cluster-level neighbor con-\ntrastive loss as follows:\nLN = −\nX\nj∈N (ω(Ii))\nwij( z⊤\ni\n∥zi∥2\n˜pj\n∥˜pj∥2)),\n(8)\nwhere N (ω(Ii)) is the set of neighbors of cluster ω(Ii), and\nwij is the weight, which is defined as\nwij = D(C(i), C(j)),\n(9)\nin which D(C(i), C(j)) is defined in Eq. 3. Owing to the\nCluster-level neighbor contrastive learning module trained\nvia loss in Eq. 8, the cluster-level neighbors will be pushed\ncloser in the feature space. This will help the model to in-\nvestigate consistency across different neighbor clusters.\n3.3. Curriculum Nerighour Selecting\nSince in the early training stages, the model has a weaker\nability to distinguish samples, we hope the ability improves\nduring training. To this end, we provide a curriculum strat-\negy for searching neighbors which sets the search range ac-\ncording to the training progress. Specifically, we set the\ncluster-level neighbor searching range k as\nk := t⌊K/T⌋,\n(10)\nwhere T is the total number of training epochs and t is the\ncurrent step, K is a hyper-parameter.\n3.4. Training and Inference Procedure for SiCL\nTraining Procudure. In SiCL, the two branches are im-\nplemented with ResNet-50 [10] and do not share the pa-\nrameters. We first pre-train the two network branches on\nImageNet and use the learned features to initialize the three\nmemory banks M, ˜\nM, and ˆ\nM, respectively. In the training\nphase, we train both network branches and the fusion layer\nwith loss:\nL := LP + LC + LN.\n(11)\nWe update the three instance memory banks M, ˜\nM and ˆ\nM,\nrespectively, as follows:\nv(t)\ni\n←αv(t−1)\ni\n+ (1 −α)xi,\n(12)\nwhere ˜v(t)\ni\nand ˆv(t)\ni\nunpdate in the same way, and α is set\nas 0.2 by default.\nIn implementation, we use DBSCAN algorithm [6] to\ngenerate the raw clusters. DBSCAN is a density-based clus-\ntering algorithm. It regards a data point as density reachable\nif the data point lies within a small distance threshold d to\nother samples, where the parameter d is the distance thresh-\nold to find neighboring point.\nInference Procedure.\nAfter training, we keep only the\nResNet F(·|Θ) in for inference. We compute the distances\nbetween each image in the query and each image in the\ngallery using the feature obtained from the output of the\nfirst branch F(·|Θ). We then sort the distances in ascending\norder to discover the matched results.\n4. Experiments\n4.1. Experimental Setting\nDatasets.\nWe evaluate SiCL on six clothes change re-id\ndatasets: LTCC [33], PRCC [34], VC-Clothes [29], Celeb-\nReID [15], Celeb-ReID-Light [14] and DeepChange [32].\nTable 1 shows an overview of dataset in details.\nAlgorithm 1 SiCL algorithm\nInput: Image I and Silhouette S;\nParameter: Cluster distance d, Neighbor Searching Range\nK;\nOutput: Pbest\n1: Pre-train the two network branches on ImageNet;\n2: Initialize the instance memory banks M, ˜\nM, ˆ\nM and set\nP = Pbest = 0;\n3: while epoch ≤total epoch do\n4:\nPerform feature extraction to get X, ˜\nX and ˆ\nX;\n5:\nPerform clustering on X to yield m clusters C :=\n{C(1), C(2), · · · , C(m)};\n6:\nGenerate cluster-level neighbor set A through Eq 3;\n7:\nTrain SiCL with supervised infromation provided by\nA and C, i.e., updating Θ, Ψ, Ωand Θ′ via the total\nloss in Eq. 11;\n8:\nUpdate instance memory bank M,\n˜\nM and ˆ\nM via\nEq. 12;\n9:\nEvaluate the model performance P with F(·|Θ);\n10:\nif P > Pbest then\n11:\nOutput the best model F(·|Θ) and set Pbest ←P;\n12:\nend if\n13: end while\n14: return Pbest.\nProtocols and Metrics.\nDifferent from the traditional\nshort-term person re-id setting, there are two evaluation pro-\ntocols for long-term person re-id: a) general setting and\nb) clothes-change setting. Specifically, for a query image,\nthe general setting is looking for cross-camera matching\nsamples of the same identity, while the clothes-change set-\nting additionally demands the same identity with inconsis-\ntent clothing. For LTCC [33] and PRCC [34], we report\nperformance with both clothes-change setting and general\nsetting. As for Celeb-ReID [15], Celeb-ReID-Light [14]\nDeepChange [32] and VC-Clothes [29], we report the gen-\neral setting performance. We use both Cumulated Matching\nCharacteristics (CMC) and mean average precision (mAP)\nas retrieval accuracy metrics.\nImplementation Details. In SiCL, we use ResNet-50 [10]\npre-trained on ImageNet [19] for both network branches.\nThe features dimension D = 2048.\nWe use the output\nxi of the first branch F(·|Θ) to perform clustering, where\nxi ∈RD. The prediction layer G(·|Ψ) is a D × D full\nconnection layer, the fusion layer R(·|Ω) is a 2D × D\nfull connection layer.\nThe cluster distance d = 0.6 for\nPRCC, LTCC, Celeb-ReID and Celeb-ReID-Light datasets,\nand d = 0.7 for VC-Clothes. We optimize the network\nthrough Adam optimizer [18] with a weight decay of 0.0005\nand train the network with 60 epochs in total. The learning\nrate is initially set as 0.00035 and decreased to one-tenth\nper 20 epochs. The batch size is set to 64. The temperature\nDataset\nSize\nSubset\nIdentity\nCamera\nClothes\nTrain\nQuery\nGallery\nDeepChange\n178, 407\n75, 083\n17, 527\n62, 956\n1, 121\n17\n-\nLTCC\n17, 119\n9, 576\n493\n7, 050\n152\n12\n14\nPRCC\n33, 698\n17, 896\n3, 543\n12, 259\n221\n3\n-\nCeleb-ReID\n34, 186\n20, 208\n2, 972\n11, 006\n1, 052\n-\n-\nCeleb-ReID-Light\n10, 842\n9, 021\n887\n934\n590\n-\n-\nVC-Clothes\n19, 060\n9, 449\n1, 020\n8, 591\n256\n4\n3\nTable 1. Long-term Person Re-ID Datasets details.\nMethod\nReference\nLTCC\nPRCC\nC-C\nGeneral\nC-C\nGeneral\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nR-10\nmAP\nR-1\nR-10\nSupervised Method(ST-ReID)\nHACNN\nCVPR’18\n9.30\n21.6\n26.7\n60.2\n-\n21.8\n59.4\n-\n82.5\n98.1\nPCB\nECCV’18\n10.0\n23.5\n30.6\n38.7\n38.7\n22.8\n61.4\n97.0\n86.8\n99.8\nSupervised Method(LT-ReID)\nCESD\nACCV’20\n12.4\n26.2\n34.3\n71.4\n-\n-\n-\n-\n-\n-\nRGA-SC\nCVPR’20\n14.0\n31.4\n27.5\n65.0\n-\n42.3\n79.4\n-\n98.4\n100\nIANet\nCVPR’19\n12.6\n25.0\n31.0\n63.7\n45.9\n46.3\n-\n98.3\n99.4\n-\nGI-ReID\nCVPR’22\n10.4\n23.7\n29.4\n63.2\n-\n-\n-\n-\n-\n-\nRCSANet\nCVPR’21\n-\n-\n-\n-\n48.6\n50.2\n-\n97.2\n100\n-\n3DSL\nCVPR’21\n14.8\n31.2\n-\n-\n-\n51.3\n-\n-\n-\n-\nFSAM\nCVPR’21\n16.2\n28.5\n25.4\n73.2\n-\n54.5\n86.4\n-\n98.8\n100\nCAL\nCVPR’22\n18.0\n40.1\n40.8\n74.2\n55.8\n55.2\n-\n99.8\n100\n-\nCCAT\nIJCNN’22\n19.5\n29.1\n50.2\n87.2\n-\n69.7\n89.0\n-\n96.2\n100\nUnsupervised Method\nSpCL\nNeurIPS’20\n7.60\n15.3\n21.2\n47.3\n45.2\n33.2\n71.3\n90.6\n86.4\n98.3\nC3AB\nPR’22\n8.30\n15.2\n20.7\n46.7\n48.6\n36.7\n74.0\n90.2\n88.3\n98.1\nCACL\nTIP’22\n6.20\n9.80\n22.3\n45.6\n52.1\n41.7\n79.8\n94.7\n90.9\n99.9\nCC\nACCV’22\n6.00\n7.40\n11.0\n17.0\n46.3\n34.4\n74.4\n94.4\n90.2\n99.9\nICE\nICCV’21\n7.10\n14.5\n28.4\n61.1\n48.0\n34.8\n74.2\n95.9\n93.6\n99.9\nICE*\nICCV’21\n10.1\n16.3\n22.6\n44.0\n45.5\n32.6\n72.3\n95.7\n93.3\n99.8\nPPLR\nCVPR’22\n4.40\n4.80\n6.00\n11.2\n51.4\n40.0\n75.2\n91.7\n87.4\n99.8\nSiCL\nOurs\n10.1\n20.7\n27.6\n57.6\n55.4\n43.2\n80.2\n96.2\n93.8\n99.4\nTable 2. Comparison with the state-of-the-art methods on LTCC and PRCC, ‘C-C’ means clothes change setting, ‘General’ means general\nsetting. ‘*’ means using the camera label as side-information.\ncoefficient τ in Eq. 5 is set to 0.05 and the update factor α in\nEq. 12 is set to 0.2. The K in Eq. 10 is set as 10 on LTCC,\nCeleb-ReID and Celeb-ReID-Light, 3 on VC-Clothes and 5\non PRCC, the effect of using a different value of K will be\ntest later.1\n4.2. Comparison to State-of-the-art Methods\nCompetitors.\nTo construct a preliminary benchmark\nand conduct a thorough comparison, we evaluate some\nthe state-of-the-art of short-term unsupervised re-id mod-\n1The code for the work will be released upon the acceptance of the\npaper.\nels, which are able to achieve competitive performance un-\nder an unsupervised short-term setting, including SpCL [8],\nCC [4], CACL [20], C3AB [22], ICE [1], PPLR [3].\nWe retrain and evaluate these unsupervised methods on\nlong-term datasets, including LTCC, PRCC, Celeb-ReID,\nCeleb-ReID-Light and VC-Clothes. At the same time, we\nalso compare with other supervised long-term person re-\nid methods such as HACNN [24], PCB [28], CESD [26],\nRGA-SC [36], IANet [13], GI-ReID [17], RCSANet [16],\n3DSL [2], FSAM [12], CAL [9], CCAT [27]. The compar-\nison results of the state-of-the-art unsupervised short-term\nperson re-id methods and supervised methods are shown in\nMethod\nReference\nVC-Clothes\nC-C\nGeneral\nmAP\nR-1\nmAP\nR-1\nSupervised Method(ST-ReID)\nPCB\nECCV’18\n30.9\n34.5\n83.3\n86.2\nSupervised Method(LT-ReID)\nHACNN\nCVPR’18\n62.2\n62.0\n94.3\n94.7\nRGA-SC\nCVPR’20\n67.4\n71.1\n94.8\n95.4\nFSAM\nCVPR’21\n78.9\n78.6\n94.8\n94.7\nCCAT\nIJCNN’22\n76.8\n83.5\n85.5\n92.7\nUnsupervised Method\nSpCL\nNeurIPS’20\n38.2\n46.2\n61.0\n77.8\nC3AB\nPR’22\n44.1\n52.0\n65.0\n81.0\nCACl\nTIP’22\n49.7\n58.9\n68.0\n82.4\nCC\nACCV’22\n25.7\n31.0\n45.1\n62.4\nICE\nICCV’21\n28.7\n34.5\n51.2\n69.3\nICE*\nICCV’21\n28.5\n31.4\n51.8\n70.1\nPPLR\nCVPR’22\n23.1\n32.5\n47.7\n68.1\nSiCL\nOurs\n63.9\n71.7\n77.2\n87.9\nTable 3. Comparison on VC-Clothes.\nMethod\nReference\nCeleb-ReID\nCeleb-Light\nmAP\nR-1\nmAP\nR-1\nSupervised Method(ST-ReID)\nTS\nTOMM’17\n7.80\n36.3\n-\n-\nSupervised Method(LT-ReID)\nMLFN\nCVPR’18\n6.00\n41.4\n6.30\n10.6\nHACNN\nCVPR’18\n9.50\n47.6\n11.5\n16.2\nPA\nECCV’18\n6.40\n19.4\n-\n-\nPCB\nECCV’18\n8.20\n37.1\n-\n-\nMGN\nMM’18\n10.8\n49.0\n13.9\n21.5\nDG-Net\nCVPR’19\n10.6\n50.1\n12.6\n23.5\nCeleb\nIJCNN’19\n-\n-\n14.0\n26.8\nRe-IDCaps\nTCSVT’19\n9.80\n51.2\n11.2\n20.3\nRCSANet\nCVPR’21\n11.9\n55.6\n16.7\n29.5\nUnsupervised Method\nSpCL\nNeurIPS’20\n4.60\n39.6\n3.60\n5.30\nC3AB\nPR’22\n4.80\n41.0\n3.70\n5.10\nCACL\nTIP’22\n5.10\n42.3\n3.60\n4.30\nCC\nACCV’22\n3.40\n32.8\n3.20\n4.40\nICE\nICCV’21\n4.90\n40.7\n5.00\n7.10\nPPLR\nCVPR’22\n4.80\n41.3\n4.30\n6.20\nSiCL\nOurs\n5.60\n45.4\n5.60\n10.5\nTable 4. Comparison with the state-of-the-art methods on Celeb-\nReID and Celeb-ReID-Light.\nTables 2, 4 and 3. From the results in these tables we can\nread that our SiCL outperforms much better than all unsu-\npervised short-term methods and even comparable to some\nsupervised long-term methods.\nMethod\nVC\nPRCC\nLTCC\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nC-C\nSingle*\n44.4\n54.0\n48.4\n37.7\n9.20\n17.1\nBaseline\n62.8\n71.4\n54.2\n42.5\n9.80\n18.6\nSiCL\n63.9\n71.7\n55.4\n43.2\n10.1\n20.7\nTable 5. Ablation Study on LTCC, PRCC and VC-Clothes, Sin-\ngle* means only use RGB images in training without contrastive\nlearning.\nParticularly, we can observe that SiCL yields much\nhigher performance than the short-term unsupervised per-\nson re-id methods in clothes-change setting. Yet, the dif-\nferences are reduced and even vanished in general settings\n(e.g., ICE results on LTCC). This further demonstrates the\ndependency of short-term re-id methods on clothes features\nfor person matching. In addition, we discovered that SiCL\nperforms poorly on certain datasets, such as Celeb-ReID-\nLight; we will investigate the underlying causes in Supple-\nmentary Material.\n4.3. Ablation Study\nIn this section, we conduct a series of ablation experiments\nto evaluating each component in SiCL architecture, i.e.,\ncontrastive learning framework, the neighbor contrastive\nlearning module LN, separately. In addition, we substitute\nthe feature fusion operation with different branch feature.\nBaseline Setting. In the baseline network, we employ the\nprototypical contrastive learning module and the cross-view\ncontrastive learning module and train the model with the\ncorresponding loss functions. The training procedure and\nmemory updating strategy are kept the same as SiCL.\nThe ablation study results are presented Table 5. We can\nread that when each component is added separately, the per-\nformance increases. This verifies that each component con-\ntributes to improved performance.\nOther Experiments. We have provided more comprehen-\nsive and complex results of the ablation experiments in the\nSupplementary Material, including the range of neighbour\nsearch K, the clustering distance parameter d, and others.\n4.4. More Evaluations and Analysis\nDifferent Operations for Cluster-level Neighbor Search-\ning. To determine the requirement of feature fusion, we pre-\npare a set of tests employing different operations, i.e.using\nx and ˜x, to find the cluster neighbors.\nThe experimen-\ntal results are listed in Table 10. We can read that only\nusing ˜x in the cluster neighbor searching stage will also\nyield acceptable performance in finding cluster neighbors.\nThis observation confirms that rich information on the high-\nlevel neighbor structure may be gleaned using just a silhou-\nette source. However, the results obtained from the LTCC\nOperation\nVC\nPRCC\nLTCC\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nC-C\nx\n61.8\n69.4\n55.1\n42.9\n10.4\n21.4\n˜x\n63.6\n71.4\n53.8\n42.4\n10.9\n21.9\nSiCL\n63.9\n71.7\n55.4\n43.2\n10.1\n20.7\nTable 6. Experiments Results of Various Cluster Neighbor Search-\ning Operations on LTCC, PRCC, and VC-Clothes.\nMethod\nBackbone\nDeepChange\nmAP\nR-1\nR-10\nSupervised Method\nDeepChange\nResNet-50\n9.62\n36.6\n55.5\nDeepChange\nReIDCaps\n13.2\n44.2\n62.0\nDeepChange\nViT B16\n14.9\n47.9\n67.3\nUnsupervised Method\nSpCL\nResNet-50\n8.90\n32.9\n47.2\nSiCL\nResNet-50\n12.4\n41.0\n56.0\nTable 7. Experimental Results on DeepChange.\ndataset indicate that the feature fusion operations did not\nyield optimal performance. The rationales behind this ob-\nservation will be elaborated upon in the supplementary ma-\nterials.\nPerformance Evaluation on DeepChange. DeepChange\nis the latest, largest, and realistic person re-id benchmark\nwith clothes change. We conduct experiments to evaluate\nthe performance of SiCL and SpCL on this dataset, and re-\nport the results in Table 7. Moreover, we also listed the\nresults of supervised training methods using the different\nbackbones provided by DeepChange [32].\n5. Conclusion\nWe have addressed a challenging task: unsupervised long-\nterm person re-id with clothes change.\nSpecifically, we\nproposed a silhouette-driven contrastive learning approach,\ntermed SiCL, which takes the silhouette masks into con-\ntrastive learning and constructs a hierarchically neighbor-\nhood structure to facilitate the training. By leveraging the\nsilhouette feature and the hierarchical neighborhood rela-\ntionship, SiCL is able to effectively exploit the invariance\nwithin and between RGB images and silhouette masks to\nlearn more effective cross-clothes features. We conducted\nextensive experiments on six widely-used re-id datasets\nwith clothes change.\nExperimental results demonstrated\nthe superiority of our proposed approach. To the best of\nour knowledge, this is the first time that the unsupervised\nlong-term person re-id problem has been tackled. In ad-\ndition, our systematic evaluations can serve as a bench-\nmark for the unsupervised long-term person re-id commu-\nnity.\nReferences\n[1] Hao Chen, Benoit Lagadec, and Francois Bremond.\nIce:\nInter-instance contrastive encoding for unsupervised person\nre-identification. In ICCV, pages 14960–14969, 2021. 6\n[2] Jiaxing Chen, Xinyang Jiang, Fudong Wang, Jun Zhang,\nFeng Zheng, Xing Sun, and Wei-Shi Zheng. Learning 3d\nshape feature for texture-insensitive person re-identification.\nIn CVPR, pages 8146–8155, 2021. 6\n[3] Yoonki Cho, Woo Jae Kim, Seunghoon Hong, and Sung-Eui\nYoon. Part-based pseudo label refinement for unsupervised\nperson re-identification. In CVPR, pages 7308–7318, 2022.\n2, 6\n[4] Zuozhuo Dai, Guangyuan Wang, Siyu Zhu, Weihao Yuan,\nand Ping Tan. Cluster contrast for unsupervised person re-\nidentification. arXiv preprint arXiv:2103.11568, 2021. 6\n[5] Zuozhuo Dai, Guangyuan Wang, Weihao Yuan, Siyu Zhu,\nand Ping Tan. Cluster contrast for unsupervised person re-\nidentification. In ACCV, pages 1142–1160, 2022. 1, 2\n[6] Martin Ester, Hans-Peter Kriegel, J¨org Sander, and Xiaowei\nXu. A density-based algorithm for discovering clusters in\nlarge spatial databases with noise. In SIGKDD, 1996. 5\n[7] Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Shuai Yi,\nXiaogang Wang, and Hongsheng Li. Fd-gan: Pose-guided\nfeature distilling gan for robust person re-identification. In\nNeurIPS, 2018. 1\n[8] Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, and Hong-\nsheng Li. Self-paced contrastive learning with hybrid mem-\nory for domain adaptive object re-id. In NeurIPS, 2020. 1,\n2, 6\n[9] Xinqian Gu, Hong Chang, Bingpeng Ma, Shutao Bai,\nShiguang Shan, and Xilin Chen. Clothes-changing person\nre-identification with rgb modality only.\nIn CVPR, pages\n1060–1069, 2022. 6\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\n2016. 5\n[11] Peixian Hong, Tao Wu, Ancong Wu, Xintong Han, and Wei-\nShi Zheng. Fine-grained shape-appearance mutual learning\nfor cloth-changing person re-identification. In CVPR, 2021.\n1, 2\n[12] Peixian Hong, Tao Wu, Ancong Wu, Xintong Han, and Wei-\nShi Zheng. Fine-grained shape-appearance mutual learning\nfor cloth-changing person re-identification. In CVPR, pages\n10513–10522, 2021. 6\n[13] Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu,\nShiguang Shan, and Xilin Chen. Interaction-and-aggregation\nnetwork for person re-identification. In CVPR, pages 9317–\n9326, 2019. 6\n[14] Yan Huang, Qiang Wu, Jingsong Xu, and Yi Zhong.\nCelebrities-reid: A benchmark for clothes variation in long-\nterm person re-identification. In IJCNN, pages 1–8. IEEE,\n2019. 5\n[15] Yan Huang, Jingsong Xu, Qiang Wu, Yi Zhong, Peng\nZhang, and Zhaoxiang Zhang.\nBeyond scalar neuron:\nAdopting vector-neuron capsules for long-term person re-\nidentification. TCSVT, 30(10):3459–3471, 2019. 5\n[16] Yan Huang, Qiang Wu, JingSong Xu, Yi Zhong, and ZhaoX-\niang Zhang. Clothing status awareness for long-term person\nre-identification. In ICCV, pages 11895–11904, 2021. 6\n[17] Xin Jin, Tianyu He, Kecheng Zheng, Zhiheng Yin, Xu\nShen, Zhen Huang, Ruoyu Feng, Jianqiang Huang, Zhibo\nChen, and Xian-Sheng Hua.\nCloth-changing person re-\nidentification from a single image with gait prediction and\nregularization. In CVPR, pages 14278–14287, 2022. 6\n[18] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In 3rd International Conference on\nLearning Representations, 2015. 5\n[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In NeurIPS, 2012. 5\n[20] Mingkun L, Li Chun-Guang, and Jun Guo. Cluster-guided\nasymmetric contrastive learning for unsupervised person re-\nidentification. TIP, 2022. 1, 2, 6\n[21] Minxian Li, Xiatian Zhu, and Shaogang Gong. Unsupervised\nperson re-identification by deep learning tracklet association.\nIn ECCV, 2018. 1\n[22] Mingkun Li, He Sun, Chaoqun Lin, Chun-Guang Li, and\nJun Guo. The devil in the tail: Cluster consolidation plus\ncluster adaptive balancing loss for unsupervised person re-\nidentification. PR, 129:108763, 2022. 1, 6\n[23] Peike Li, Yunqiu Xu, Yunchao Wei, and Yi Yang.\nSelf-\ncorrection for human parsing. TPAMI, 2020. 3\n[24] Wei Li, Xiatian Zhu, and Shaogang Gong. Harmonious at-\ntention network for person re-identification. In CVPR, pages\n2285–2294, 2018. 6\n[25] Peixi Peng, Tao Xiang, Yaowei Wang, Massimiliano Pon-\ntil, Shaogang Gong, Tiejun Huang, and Yonghong Tian.\nUnsupervised cross-dataset transfer learning for person re-\nidentification. In CVPR, 2016. 2\n[26] Xuelin Qian, Wenxuan Wang, Li Zhang, Fangrui Zhu,\nYanwei Fu, Tao Xiang, Yu-Gang Jiang, and Xiangyang\nXue. Long-term cloth-changing person re-identification. In\nACCV, 2020. 6\n[27] Xuena Ren, Dongming Zhang, and Xiuguo Bao. Person re-\nidentification with a cloth-changing aware transformer. In\n2022 International Joint Conference on Neural Networks\n(IJCNN), pages 1–8. IEEE, 2022. 6\n[28] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\nWang. Beyond part models: Person retrieval with refined\npart pooling (and a strong convolutional baseline). In ECCV,\npages 480–496, 2018. 6\n[29] Fangbin Wan, Yang Wu, Xuelin Qian, Yixiong Chen, and\nYanwei Fu. When person re-identification meets changing\nclothes. In CVPRW, 2020. 5\n[30] Jingya Wang, Xiatian Zhu, Shaogang Gong, and Wei Li.\nTransferable joint attribute-identity deep learning for unsu-\npervised person re-identification. In CVPR, 2018. 2\n[31] Kai Wang, Zhi Ma, Shiyan Chen, Jinni Yang, Keke Zhou,\nand Tao Li. A benchmark for clothes variation in person re-\nidentification. IJIS, 2020. 2\n[32] Peng Xu and Xiatian Zhu. Deepchange: A long-term per-\nson re-identification benchmark with clothes change. In Pro-\nceedings of the IEEE international conference on computer\nvision (ICCV), 2023. 1, 2, 5, 8\n[33] Qize Yang, Ancong Wu, and Wei-Shi Zheng.\nPerson re-\nidentification by contour sketch under moderate clothing\nchange. TPAMI, 2019. 1, 2, 5\n[34] Qize Yang, Ancong Wu, and Wei-Shi Zheng.\nPerson re-\nidentification by contour sketch under moderate clothing\nchange. TPAMI, 2019. 5\n[35] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling\nShao, and Steven CH Hoi.\nDeep learning for person\nre-identification: A survey and outlook.\narXiv preprint\narXiv:2001.04193, 2020. 1\n[36] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Xin Jin, and\nZhibo Chen. Relation-aware global attention for person re-\nidentification. In CVPR, pages 3186–3195, 2020. 6\n[37] Rui Zhao, Wanli Oyang, and Xiaogang Wang. Person re-\nidentification by saliency learning. TPAMI, 2016. 1\n[38] Liang Zheng, Yi Yang, and Alexander G Hauptmann. Per-\nson re-identification: Past, present and future. arXiv preprint\narXiv:1610.02984, 2016. 2\nSiCL: Silhouette-Driven Contrastive Learning for Unsupervised Person\nRe-Identification with Clothes Change\nSupplementary Material\nIn the supporting materials, we provide the implementa-\ntion details of the experiments, such as evaluations on using\ndifferent K, d, and a set of visualization results. Besides,\nwe also discuss the limitation and the future work.\n6. Implementation Details and Supplementary\nExperiments\n6.1. Implementation Details\nOur experiments are implemented with PyTorch and run on\na server equipped with an Intel(R) Xeon(R) CPU E5-2630-\nv4 and four Nvidia 3090 GPUs of memory 24 GB. The con-\ntrastive learning framework is based on CACL.\n6.2. More Evaluation and Analysis\nAblation Study on Celeb-ReID and Celeb-ReID-Light .\nThe ablation study results are presented in Table 9. We can\nread that when each component is added separately, the per-\nformance increases. This verifies that each component con-\ntributes to improved performance.\nPerformance on Celeb-ReID-Light.\nFor the LTCC,\nPRCC, and VC-Clothes datasets, SiCL does not vary sub-\nstantially from the supervised methods; however, the differ-\nence is apparent on the Celeb-ReID-Light dataset. Due to\nthe small size of the Celeb-ReID-Light dataset, the look of\nthe same person apparel differs substantially while includ-\ning just one picture per clothing type. This is a significant\ndifficulty for the clustering-based methods, as it is difficult\nfor the model to accurately recognize the same pedestrians\nwearing various garments during training.\nEvaluation on Parameters in DBSCAN. We conduct ex-\nperiments on LTCC and VC-Clothes to evaluate the effects\nof changing the parameter d. Experiments are recorded in\nTable 8. we can find that while d = 0.6, SiCL demon-\nstrates the highest performance on LTCC. We can observe\nthat SiCL is comparatively sensitive to d. This suggests that\nSiCL is somewhat reliant on the quality of the clustering.\nThis is because if the clustering quality is low, the instance\nneighbor information will also be in error. Such errors can\ncumulatively at the clustering level, which negatively im-\npacts the model training. Therefore, one of our forthcom-\ning research directions is to make SiCL less sensitive to the\nclustering parameter in the future.\nDifferent operations for cluster-level neighbor search-\ning. To determine the requirement of feature fusion, we\nprepare a set of tests employing different operations, such\nas using x, ˜x, to find the cluster-level neighbors. The exper-\nTable 8. Performance Comparison of different cluster parameter d\n(the maximum distance between neighbor points) on SiCL.\nEps\nVC-Clothes\nLTCC\nmAP\nR-1\nmAP\nR-1\nGeneral\n0.8\n8.70\n15.7\n4.30\n9.90\n0.7\n77.2\n87.9\n11.8\n25.2\n0.6\n76.5\n87.4\n27.6\n57.6\n0.5\n70.4\n82.8\n22.9\n46.5\n0.4\n64.3\n78.0\n18.7\n43.6\nMethod\nCeleb-ReID\nCeleb-ReID-Light\nmAP\nR-1\nmAP\nR-1\nC-C\nSingle*\n4.80\n40.5\n5.00\n8.50\nBaseline\n5.60\n44.2\n5.20\n10.0\nSiCL\n6.00\n45.0\n5.40\n9.50\nTable 9. Ablation Study on Celeb-ReID and Celeb-ReID-Light,\nSingle* means only use RGB images in training without con-\ntrastive learning.\nOperation\nVC\nPRCC\nLTCC\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\nC-C\nBaseline\n62.8\n71.4\n54.2\n42.5\n9.80\n18.6\nx\n61.8\n69.4\n54.1\n42.4\n10.4\n20.4\n˜x\n63.6\n71.4\n54.6\n42.6\n10.5\n20.9\nSiCL\n63.9\n71.7\n55.4\n43.2\n10.1\n20.7\nTable 10. Experiments Results of Various Cluster-Level Neighbor\nSearching Operations on LTCC, PRCC, and VC-Clothes.\nimental results are listed in Table 10. We can find that using\nonly ˜x during the cluster-level neighbour searching stage\nalso yield satisfactory results. Conversely, using only x to\nsearch the cluster-level neighbor would have a detrimen-\ntal effect on the model performance on both VC-Clothes\nand PRCC. This observation confirms that rich clothes-\nindependent information can be gleaned by just using the\nsilhouette feature.\nPerformance with different K. Table 11 illustrates the\neffect of varying cluster-level neighbor search parameters\nK across various datasets.\nWe can find that our SiCL\nis not sensitive to the change of the cluster-level neigh-\nbor search parameter K on PRCC. However, for the VC-\nK\nVC-Clothes\nPRCC\nC-C\nGeneral\nC-C\nmAP\nR-1\nmAP\nR-1\nmAP\nR-1\n10\n63.9\n71.7\n77.2\n87.9\n54.4\n42.6\n8\n63.4\n71.2\n76.8\n87.7\n55.2\n43.8\n6\n62.5\n70.0\n75.5\n86.9\n54.5\n42.5\n4\n63.0\n70.5\n77.2\n87.1\n54.1\n43.1\n2\n63.1\n69.8\n74.5\n86.1\n54.6\n42.8\nTable 11. Illustration for the model performance with different K\non PRCC and VC-Clothes.\nClothes dataset, when the neighbour search range is less\nthan 6, the cluster-level neighbour information seems to\nplay a negative role and does not bring any performance\nimprovement to the model. We believe that this problem\nmight be caused due to low accuracy of cluster-level neigh-\nbor selection when the value of K is small. Hence, our\nfuture work will investigate how to enhance the quality of\ncluster-level neighbor selection.\nVisualization. To gain some intuitive understanding of the\nperformance of our proposed SiCL, we conduct a set of data\nvisualization experiments on LTCC and VC-Clothes to vi-\nsualize selected query samples with the top-10 best match-\ning images in the gallery set, and display the visualization\nresults in Figure 4 and Figure 5. Compared to CACL and\nSpCL, SiCL yields more precise matching results. The ma-\njority of the incorrect samples matched by other methods\nare in the same color as the query sample. These findings\nimply that SiCL can successfully learn more clothing in-\nvariance features and hence identify more precise matches.\n7. Limitation and Future Work\nIn experiments, we construct a hierarchical neighbor struc-\nture using person silhouette masks and person images to\ndrive the model learning in cross-clothes invariance. Ex-\nperiments have shown that clothe-independent features can\nprovide the correct guidance for the model. Therefore, it\nis promising to research generating higher quality person\nsilhouette masks without supervision to further enrich the\nrepresentation capacity, improve the stability and enhance\nthe overall performance of the proposed framework.\nBesides,\npedestrian\nsilhouette,\nperson\nclothes-\nindependent semantic sources comprise numerous cat-\negories, such as gait, skeleton. As the future work, it is\ninteresting to investigate combine multiple types of sources\nto enhance the model performance.\nSiCL\nCACL\n检索\n图像\n1st\n10th\n1st\n10th\nSiCL\nCACL\n检索\n图像\n1st\n10th\n1st\n10th\nFigure 4. Visualization of the top-10 best matched images on LTCC and VC-Clothes. We show the top-10 best matching samples in the\ngallery set for the query sample with CACL and our proposed SiCL. The images with frames in green and in red are the correctly matched\nimages and mismatched images, respectively.\nSiCL\nSpCL\nQuery\n1st\n10th\n1st\n10th\nSiCL\nSpCL\nQuery\n1st\n10th\n1st\n10th\nFigure 5. Visualization of the top-10 best matched images on LTCC and VC-Clothes. We show the top-10 best matching samples in the\ngallery set for the query sample with SpCL and our proposed SiCL. The images with frames in green and in red are the correctly matched\nimages and mismatched images, respectively.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2023-05-23",
  "updated": "2024-04-07"
}