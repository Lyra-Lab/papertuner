{
  "id": "http://arxiv.org/abs/2305.14754v1",
  "title": "SUVR: A Search-based Approach to Unsupervised Visual Representation Learning",
  "authors": [
    "Yi-Zhan Xu",
    "Chih-Yao Chen",
    "Cheng-Te Li"
  ],
  "abstract": "Unsupervised learning has grown in popularity because of the difficulty of\ncollecting annotated data and the development of modern frameworks that allow\nus to learn from unlabeled data. Existing studies, however, either disregard\nvariations at different levels of similarity or only consider negative samples\nfrom one batch. We argue that image pairs should have varying degrees of\nsimilarity, and the negative samples should be allowed to be drawn from the\nentire dataset. In this work, we propose Search-based Unsupervised Visual\nRepresentation Learning (SUVR) to learn better image representations in an\nunsupervised manner. We first construct a graph from the image dataset by the\nsimilarity between images, and adopt the concept of graph traversal to explore\npositive samples. In the meantime, we make sure that negative samples can be\ndrawn from the full dataset. Quantitative experiments on five benchmark image\nclassification datasets demonstrate that SUVR can significantly outperform\nstrong competing methods on unsupervised embedding learning. Qualitative\nexperiments also show that SUVR can produce better representations in which\nsimilar images are clustered closer together than unrelated images in the\nlatent space.",
  "text": "SUVR: A SEARCH-BASED APPROACH TO UNSUPERVISED\nVISUAL REPRESENTATION LEARNING\n␆Yi-Zhan Xu, † Chih-Yao Chen, ␅Cheng-Te Li\n␆␅Institute of Data Science, National Cheng Kung University\n† Institute of Information Science, Academia Sinica\nABSTRACT\nUnsupervised learning has grown in popularity because of the\ndifficulty of collecting annotated data and the development\nof modern frameworks that allow us to learn from unlabeled\ndata. Existing studies, however, either disregard variations at\ndifferent levels of similarity or only consider negative sam-\nples from one batch. We argue that image pairs should have\nvarying degrees of similarity, and the negative samples should\nbe allowed to be drawn from the entire dataset. In this work,\nwe propose Search-based Unsupervised Visual Representa-\ntion Learning (SUVR) to learn better image representations in\nan unsupervised manner. We first construct a graph from the\nimage dataset by the similarity between images, and adopt the\nconcept of graph traversal to explore positive samples. In the\nmeantime, we make sure that negative samples can be drawn\nfrom the full dataset. Quantitative experiments on five bench-\nmark image classification datasets demonstrate that SUVR\ncan significantly outperform strong competing methods on\nunsupervised embedding learning. Qualitative experiments\nalso show that SUVR can produce better representations in\nwhich similar images are clustered closer together than unre-\nlated images in the latent space.\nIndex Terms— Unsupervised learning, visual represen-\ntation learning, self-supervised learning, graph search, unsu-\npervised representation learning, graph traversal\n1\nINTRODUCTION\nDeep learning has achieved astonishing performance due to\nthe progress of computational power as well as the amount of\ndata. Although we could obtain satisfactory results with suf-\nficient labeled data—from a few thousand to a few hundred\nthousand in general—human-annotated data is by no means\neconomic. Therefore, the idea of effectively leveraging unla-\nbeled data, often refering to unsupervised or contrastive learn-\ning, has gained popularity recently [1, 2, 3, 4, 5]. Learn-\ning from data itself without the supervision of labels, how-\never, remains a challenging task. Existing work for unsu-\npervised contrastive learning on visual data takes the given\nimage and its slightly varied version as positive pairs, while\nviewing all the other images as negatives [1]. This assumes\nthat all images other than the given image are equally neg-\native. However, we argue that the distinct relations between\nimages should also be considered, since the degree of similar-\nities could vary a lot for different image pairs, e.g., tiger v.s.\nleopard apparently has a higher similarity than tiger v.s. bird.\nIn this work, we propose Search-based Unsupervised\nVisual Representation Learning (SUVR), a novel frame-\nwork that learns visual representations in an unsupervised\nmanner. First, we present the relations between images as a\ngraph, and find each image’s neighbors to learn better rep-\nresentations.\nConcretely, we utilize the concept of graph\ntraversal to find positive samples–images that are similar\nin different aspects to a given image, as well as negative\nsamples–images that are similar but should be separated. We\nthen minimize the distances between positive pairs, and max-\nimize the distances between negative pairs. Moreover, we\npropose using three strategies to discover neighbors for each\nimage, including (1) Breadth-First Search (BFS), (2) Depth-\nFirst Search (DFS) and (3) Greedy Search, which reflect\ndifferent aspects of information carried in the neighborhood.\nThat is to say, BFS considers 1-hop information by finding\nthe top-k similar neighbors, while DFS considers up to k-hop\ninformation sequentially. Greedy search strikes the balance\nbetween BFS and DFS to incorporate the advantages of both.\nDuring training, we adopt the memory mechanism [6] to\nalleviate the over-smoothing problem [7], and it also allows\nus to draw negative samples from the entire dataset.\nWe\nconduct experiments on five benchmark datasets for image\nclassification task to evaluate the proposed SUVR. Experi-\nmental results show that SUVR consistently outperforms all\nthe baselines by a large margin on every dataset. We also\nconduct qualitative studies to attest the quality of the learned\nrepresentations. Results indicate that SUVR produces more\nvisually meaningful embeddings compared with the most\ncompetitive baseline. In short, our contribution is three-fold,\nas summarized below.\n• We propose a novel neighbor-discovering method\nto enhance unsupervised visual embedding learning,\nwhich effectively exploits graph traversal in an image\ngraph.\narXiv:2305.14754v1  [cs.CV]  24 May 2023\n• Three neighbor-discovering strategies, including BFS,\nDFS, and Greedy Search, are proposed to provide use-\nful information that improves the robustness of image\nrepresentations.\n• Experiments conducted on five benchmark datasets\nshow that SUVR gains substantial improvements, con-\nsistently surpassing the typical competing methods of\nunsupervised visual embedding learning.\n2\nMETHODOLOGY\n2.1\nNeighbor Discovering\nGiven an unlabeled image set D, we follow [3] to obtain the\ninitial embedding for each instance. Next, a graph G = (V, E)\nis constructed from D, where each node V represents an im-\nage, and the edges E are the similarity between images. After\nthat, we propose three strategy for neighbor discovering.\nBreadth-First Search (BFS) is used to find top-k neighbors.\nSpecifically, we find k most similar images for a given in-\nstance x: N B\npos(x) = {x}∪{xi | xi ̸= x, s(xi, x) is top-k in G},\nwhere s is a function that calculates similarity between im-\nages following [3]. In particular, s measures the dot product\nsimilarity between the embedding of instance x and the mem-\nory bank M of the whole image set D. We select the top-k\nimages sorted by their similarities as the neighbors for x.\nBFS could be helpful because a variety of similar images are\ncollected. For example, given an image of a tiger, we may\nfind lynx, cheetah and puma as its neighbors. Here we denote\nneighbors found by BFS as N B\npos.\nDepth-First Search (DFS) is used to find k-hop neighbors.\nFor each iteration, we find the most similar image j for x, and\nthen we find the most similar image for j. This process is\nrepeated for k iteration. DFS aims to utilize information that\ncould reach up to k-hop neighbors, exploring the image set\nin a depth-first manner. DFS could be useful because we can\nfind different images that are somewhat similar. For example,\ngiven an image of warbler, we may sequentially find sparrow,\npigeon and peacock. Here, peacock is found because it is the\nmost similar image for pigeon. However, if we use BFS, we\nmay not find peacock because it is less similar to warbler. The\nneighbor set found by DFS is denoted as N D\npos(x).\nGreedy Search.\nSince BFS and DFS take distinct infor-\nmational aspects into account, we provide a greedy strat-\negy to merge them.\nThe similarity gain specifically de-\ntermines which strategy should be used in each iteration.\nWe compare which strategy found the image with the high-\nest degree of similarity for each iteration, and the strategy\nthat produced the higher similar neighbor is chosen.\nx:\nN G\npos(x) = {x} ∪{xi | xi ̸= x, max(BFS(G), DFS(G)))\nNegative Sampling. Different from [1, 4] that consider neg-\native samples only within a batch. We also aim to avoid the\nfalse separation problem [8]. Thus, we adaptively choose the\nnegatives from positive neighbors. Specifically, at each itera-\ntion, we select the least similar images from the positive set\nto form the negative samples Nneg. In this way, the nagatives\nNneg are expected to be more similar to the given image as\nthe hard negatives [9, 10, 11], and thereby help to learn more\nrobust representations.\n2.2\nTraining Objective\nThe objective function for SUVR is formulated as\nL = −\nX\ni\nlog P(i|v)\n−\nX\ni\nX\nj∈Npos(x)\nlog P(j|v)\n−\nX\ni\nX\nk∈Nneg(x)\nlog (1 −P(k|v))\n(1)\nThe first term is an instance-level loss that an image represen-\ntation v should be assigned to the i-th instance, instead of the\ni-th class, calculated by\nP(i|v) =\nexp(M⊤\ni v/τ)\nPt\nj=1 exp(M⊤\nj v/τ)\n(2)\nWhere M is the memory bank saving the updated embed-\ndings, and τ is the temperature. The second term maximizes\nthe likelihood between similar instances, bringing similar in-\nstances closer together.\nIn the meantime, we separate the\nrepresentations of unrelated instances in the final term to re-\nduce the likelihood between negative pairs. Once we discover\na new neighbor through the neighbor-discovery process, we\ncalculate the loss and update the image representation.\n3\nEXPERIMENTS\n3.1\nEvaluation Settings\nDatasets. We use five benchmark datasets with image classi-\nfication task to evaluate the proposed SUVR for unsupervised\nembedding learning. The datasets are categorized as follows:\n(1) Coarse-grained: CIFAR-10 [12] consists of 10 classes.\n50,000 images are available for training, and 10,000 photos\nare available for testing. SVHN [13] consists of 10 classes of\ndigital house numbers. There are 73,257 samples for training,\nand 26,032 samples for testing. (2) Fine-grained: CIFAR-\n100 is a finer version of CIFAR-10 that sub-classes are also\navailable. The sample size is the same as CIFAR-10 and there\nare 100 classes. Stanford-Dog [14] contains 120 dog breeds,\nand the training size is 12,000 whereas the testing size is\n8,580. Last, CUB200 [15] has 200 bird species. There are\n5,994 samples for training and 5,794 samples for testing.\nSetup. We employ two backbone models, AlexNet [16] and\nResNet18 [17], to demonstrate the effectiveness of SUVR.\nThe architectures and hyperparameters for the backbone mod-\nels are identical to previous research [3] for a fair comparison.\nFor hyperparameters in SUVR, we set the number of neigh-\nbors being explored (k in Section 2) to 4, then re-select the\nsame neighbor size for each iteration. We use SGD with Nes-\nterov [18] for the optimizer. The learning rate is set to 0.03\ninitially, and then is reduced 10% for every 40 epochs. For\nthe memory bank, the momentum of the exponential moving\naverage was set to 0.5.\nEvaluation Plan. The class labels are only used to evaluate a\nmodel and were left out for training. We follow the evaluation\nprotocol that utilizes k-nearest neighbors [19, 3] to vote for\nthe final prediction, which is commonly used in unsupervised\nembedding learning task. The predicted labels are derived\nfrom a voting result: querying a test image to obtain k nearest\nneighbors in the training set, and the prediction is the majority\nof its k nearest neighbors’ labels.\nBaselines. We compare our model with several strong base-\nlines. DeepCluster [2] is a clustering-based approach that de-\nrives pseudo-labels by the clustering [20] results, then utilizes\nthe pseudo-labels to train the model in a supervised-learning\nmanner. Both ISIF [4] and SimCLR [1] use image augmenta-\ntion to form the positive pairs, and train their models in a self-\nsupervised manner. We view AND [5] as the main competi-\ntor because they also select negative samples from the whole\nimage set, but the way we select negatives is different: our\nnegative samples are selected from the positive neighbors we\nexplored via graph traversal. All baselines’ hyperparameters\nare set according to the official code implementation.\n3.2\nExperimental Results\nWe compare the performance in terms of top-1 accuracy on\nthe five benchmark datasets.\nSUVR with different search\nstrategies are included for comparison, and the results are\nlisted on Table 1. It is also noticeable that compared with\nthe coarse-grained dataset, performance on the finer-grained\ndataset are remarkably lower for all baselines. This shows that\nas the similarities between classes increase, it is more chal-\nlenging to have correct predictions. Here we only present the\nperformance using ResNet18 as the backbone while AlexNet\nexhibits similar results.\nWe find that SUVR consistently outperforms all base-\nlines by a large margin. Additionally, BFS and DFS perform\nbetter on datasets with a few classes and multiple classes,\nrespectively.\nIn particular, CIFAR-100, Stanford-Dog and\nCUB-200 get better results on BFS, while CIFAR-10 and\nSVHN perform better by DFS. The reason that BFS outper-\nforms DFS on multi-class datasets is that when the number of\nclasses increases, and the granularity of labels becomes finer,\nit is more difficult for DFS to find the correct most similar\nimages. In other words, the similarity between classes will\nincrease as the number of classes increases, and this makes\nfinding the right most similar images more difficult. On the\nother hand, when the dataset contains fewer classes, DFS\nshows promising results that outperforms BFS, since it is\neasier to find the right most similar images as the classes are\ndistinguishable. Moreover, the greedy search allows SUVR\nto achieve comparatively second-order performance among\nBFS and DFS. Specifically, the performance of greedy is\nworse than DFS but better than BFS on datasets with fewer\nclasses, such as CIFAR-10 and SVHN, and worse than BFS\nbut better than DFS on datasets with more classes, such as\nCIFAR-100, Standford-Dog and CUB-200. This is as a result\nof the search strategy that chooses between BFS and DFS\nbased on the similarity gain. Such results also allow us to\nbe free from pre-defining the search strategy but still obtain\ncomparable performance to the greedy approach.\n3.3\nAnalysis of Neighbor-discovering Strategy\nWe aim to investigate the effectiveness of each neighbor-\ndiscovering strategy. First, we analyze the effect of neighbor\nsize k. In the left part of Figure 1, we find that the accuracy\nincreases as the neighbor size increases, especially for BFS\nthat it can discover broader images. The accuracy of DFS\ndrops after the neighbor size reaches 4, which might result\nfrom that DFS sequentially explores k-hop neighbors, and the\nsimilarity becomes lower after a certain hop number of ex-\nplorations. Among BFS and DFS, the greedy search steadily\nremains in a second-order, showing its the robustness with\na modest sacrifice on performance. We further visualize the\nsearching process for each strategy to discover neighbors,\nwhich is depicted in Figure 2. Specifically, we random query\nan image, then plot their neighbors by searching order. The\nsymbol above each image is its parent image that represents\nthe previous searching result. In particular, all searching re-\nsults of BFS are based on the query image x, because BFS\nsearches for top-k images at one time, and the parent of DFS\nis the node of the previous step. The result of greedy search\ntends to be a tree-like structure spanning for BFS and mining\nfor DFS. We observe that BFS finds more similar images than\nDFS, as DFS finds unrelated images when the neighbor size\nis too large (after the fourth neighbor). Such findings match\nwith the observation found in Figure 1 that the performance\nusing DFS drops after the neighbor size surpasses 4. The\ngreedy search still exhibits a steady balance here, which can\nfind more related images than DFS.\n3.4\nCase Study\nWe conduct a case study to demonstrate the effectiveness of\nnegative sampling. In Figure 3, we random sample an image,\nand then collect positive and negative neighbors of the image\nbased on the greedy search. Comparing to the competitive\nmethod AND, the embedding of using the proposed SUVR\nis more visually meaningful. Concretely, the positive neigh-\nbors are more close to the query image compared with AND,\nTable 1: Top-1 accuracy for image classification task on five benchmark datasets.\nDataset\nCIFAR-10\nCIFAR-100\nSVHN\nStanford-Dog\nCUB-200\nBackbone\nAlexNet\nResNet18\nAlexNet\nResNet18\nAlexNet\nResNet18\nResNet18\nResNet18\nDeepCluster\n60.3\n80.8\n32.7\n50.7\n79.8\n93.6\n27.0\n11.6\nISIF\n74.4\n83.6\n44.1\n54.4\n89.8\n91.3\n31.4\n13.2\nSimCLR\n73.0\n82.3\n43.2\n55.8\n88.6\n90.8\n33.7\n17.5\nAND\n74.8\n84.2\n41.5\n56.1\n90.9\n94.5\n32.3\n14.4\nSUVR (BFS)\n75.6\n85.2\n45.5\n57.5\n92.4\n95.3\n35.4\n18.3\nSUVR (DFS)\n76.5\n85.4\n44.3\n57.3\n92.9\n95.8\n34.2\n17.8\nSUVR (Greedy)\n76.4\n85.3\n45.4\n57.4\n92.8\n95.7\n34.3\n18.2\nFig. 1: Left: results for varying neighbor size k. Right: effec-\ntiveness of resetting neighbors.\nFig. 2: Illustration of neighbor discovering process.\nFig. 3: t-SNE visualization of SUVR (right) and AND (left).\nas we can see that when querying a dog image, other images\nof dog also appear in close positions at the top-right corner,\nand the negative neighbors are also separated from the query\nimage. We relate this to the method we use to choose the neg-\natives, which we select them from the positive neighbors. As\na result, the negatives are somewhat similar to the image and\nmay serve as hard negatives, which teaches SUVR how to dis-\ntinguish between positive and negative samples that are very\nsimilar to the given image. To conclude, as negative sampling\nis shown critical to unsupervised representation learning [1],\nour way to select negatives has shown more effective.\n3.5\nNeighbor Resetting\nSince the representation is updated every iteration, we sus-\npect that re-selecting neighbors could consider more accurate\ninformation. On the right side of Fig 1, we can see that re-\nsetting neighbors is critical, as it consistently boosts the per-\nformance regardless of the search strategy. DFS drops the\nmost without resetting, perhaps because DFS always searches\nfor the most similar instances, and keeping updating neigh-\nbors is especially helpful. In contrast, BFS drops the least,\nwhich could result from that BFS always searches for top-k\ninstances, lowering the impact of resetting neighbors.\n4\nConclusions\nWe propose SUVR, a novel framework for unsupervised\nvisual representation learning.\nWe use graph traversal to\nexplore positive neighbors and subsequently select negative\nsamples from the found neighbors.\nWe further present an\nobjective function where the likelihood of positive image\npairs is maximized while the likelihood of negative image\npairs is minimized. Experiments conducted on five bench-\nmark datasets demonstrate that the representations generated\nby SUVR are able to not only produce outstanding perfor-\nmance on the image classification task but also generate better\nrepresentations.\n5\nAcknowledgement\nThis work is supported by the National Science and Tech-\nnology Council (NSTC) of Taiwan under grants 110-2221-E-\n006-136-MY3, 111-2221-E-006-001, 111-2634-F-002-022.\n6\nReferences\n[1] Ting Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey Hinton, “A simple framework for contrastive\nlearning of visual representations,” in Proceedings of\nthe 37th International Conference on Machine Learn-\ning, 2020, ICML’20.\n[2] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze,\n“Deep clustering for unsupervised\nlearning of visual features,”\nin European Conference\non Computer Vision, 2018.\n[3] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua\nLin, “Unsupervised feature learning via non-parametric\ninstance discrimination,”\nin Proc. of the IEEE Con-\nference on Computer Vision and Pattern Recognition,\n2018, pp. 3733–3742.\n[4] Mang Ye, Xu Zhang, Pong C. Yuen, and Shih-Fu Chang,\n“Unsupervised embedding learning via invariant and\nspreading instance feature,”\n2019 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), Jun 2019.\n[5] Jiabo Huang, Qi Dong, Shaogang Gong, and Xiatian\nZhu, “Unsupervised deep learning by neighbourhood\ndiscovery,” in Proc. of the International Conference on\nMachine Learning, 2019, pp. 2849–2858.\n[6] Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and\nXiaogang Wang, “Joint detection and identification fea-\nture learning for person search,” 2017 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR),\nJul 2017.\n[7] Qimai Li, Zhichao Han, and Xiao-Ming Wu, “Deeper\ninsights into graph convolutional networks for semi-\nsupervised learning,”\nin Thirty-Second AAAI Confer-\nence on Artificial Intelligence, 2018.\n[8] Shaosheng Cao, Wei Lu, and Qiongkai Xu,\n“Deep\nneural networks for learning graph representations,”\nin Thirtieth AAAI conference on artificial intelligence,\n2016.\n[9] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jiten-\ndra Malik, “Rich feature hierarchies for accurate object\ndetection and semantic segmentation,”\nin 2014 IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, 2014, pp. 580–587.\n[10] Kah Sung and Tomaso Poggio, “Example based learn-\ning for view-based human face detection,” Pattern Anal-\nysis and Machine Intelligence, IEEE Transactions on,\nvol. 20, pp. 39 – 51, 02 1998.\n[11] Pedro F Felzenszwalb,\nRoss B Girshick,\nDavid\nMcAllester, and Deva Ramanan,\n“Object detection\nwith discriminatively trained part-based models,” IEEE\ntransactions on pattern analysis and machine intelli-\ngence, vol. 32, no. 9, pp. 1627–1645, 2010.\n[12] Alex Krizhevsky, “Learning multiple layers of features\nfrom tiny images,” University of Toronto, 05 2012.\n[13] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-\nsacco, Bo Wu, and Andrew Y. Ng,\n“Reading digits\nin natural images with unsupervised feature learning,”\nin NIPS Workshop on Deep Learning and Unsupervised\nFeature Learning 2011, 2011.\n[14] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng\nYao, and Li Fei-Fei, “Novel dataset for fine-grained im-\nage categorization,” in Proc. of the First Workshop on\nFine-Grained Visual Categorization, IEEE Conference\non Computer Vision and Pattern Recognition, Colorado\nSprings, CO, June 2011.\n[15] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Be-\nlongie, “The Caltech-UCSD Birds-200-2011 Dataset,”\nTech. Rep. CNS-TR-2011-001, California Institute of\nTechnology, 2011.\n[16] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton,\n“Imagenet classification with deep convolutional neural\nnetworks,” Neural Information Processing Systems, vol.\n25, 01 2012.\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun,\n“Deep residual learning for image recognition,”\n2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), Jun 2016.\n[18] Timothy Dozat,\n“Incorporating nesterov momentum\ninto adam,” International Conference on Learning Rep-\nresentations (ICLR) workshop, 2016.\n[19] Gongde Guo, Hui Wang, David Bell, and Yaxin Bi,\n“Knn model-based approach in classification,”\nOTM\nConfederated International Conferences On the Move\nto Meaningful Internet Systems, 08 2004.\n[20] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Pi-\natko, R. Silverman, and A. Y. Wu,\n“An efficient k-\nmeans clustering algorithm: analysis and implementa-\ntion,” IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, vol. 24, no. 7, pp. 881–892, 2002.\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2023-05-24",
  "updated": "2023-05-24"
}