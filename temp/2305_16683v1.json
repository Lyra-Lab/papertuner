{
  "id": "http://arxiv.org/abs/2305.16683v1",
  "title": "Future-conditioned Unsupervised Pretraining for Decision Transformer",
  "authors": [
    "Zhihui Xie",
    "Zichuan Lin",
    "Deheng Ye",
    "Qiang Fu",
    "Wei Yang",
    "Shuai Li"
  ],
  "abstract": "Recent research in offline reinforcement learning (RL) has demonstrated that\nreturn-conditioned supervised learning is a powerful paradigm for\ndecision-making problems. While promising, return conditioning is limited to\ntraining data labeled with rewards and therefore faces challenges in learning\nfrom unsupervised data. In this work, we aim to utilize generalized future\nconditioning to enable efficient unsupervised pretraining from reward-free and\nsub-optimal offline data. We propose Pretrained Decision Transformer (PDT), a\nconceptually simple approach for unsupervised RL pretraining. PDT leverages\nfuture trajectory information as a privileged context to predict actions during\ntraining. The ability to make decisions based on both present and future\nfactors enhances PDT's capability for generalization. Besides, this feature can\nbe easily incorporated into a return-conditioned framework for online\nfinetuning, by assigning return values to possible futures and sampling future\nembeddings based on their respective values. Empirically, PDT outperforms or\nperforms on par with its supervised pretraining counterpart, especially when\ndealing with sub-optimal data. Further analysis reveals that PDT can extract\ndiverse behaviors from offline data and controllably sample high-return\nbehaviors by online finetuning. Code is available at here.",
  "text": "Future-conditioned Unsupervised Pretraining for Decision Transformer\nZhihui Xie 1 Zichuan Lin 2 Deheng Ye 2 Qiang Fu 2 Wei Yang 2 Shuai Li 1\nAbstract\nRecent research in offline reinforcement learning\n(RL) has demonstrated that return-conditioned\nsupervised learning is a powerful paradigm for\ndecision-making problems. While promising, re-\nturn conditioning is limited to training data la-\nbeled with rewards and therefore faces challenges\nin learning from unsupervised data. In this work,\nwe aim to utilize generalized future conditioning\nto enable efficient unsupervised pretraining from\nreward-free and sub-optimal offline data. We pro-\npose Pretrained Decision Transformer (PDT), a\nconceptually simple approach for unsupervised\nRL pretraining. PDT leverages future trajectory\ninformation as a privileged context to predict ac-\ntions during training. The ability to make deci-\nsions based on both present and future factors\nenhances PDT’s capability for generalization. Be-\nsides, this feature can be easily incorporated into a\nreturn-conditioned framework for online finetun-\ning, by assigning return values to possible futures\nand sampling future embeddings based on their\nrespective values. Empirically, PDT outperforms\nor performs on par with its supervised pretraining\ncounterpart, especially when dealing with sub-\noptimal data. Further analysis reveals that PDT\ncan extract diverse behaviors from offline data\nand controllably sample high-return behaviors by\nonline finetuning. Code is available at here.\n1. Introduction\nLarge-scale pretraining has achieved phenomenal success\nin the fields of computer vision (Chen et al., 2020; He et al.,\n2022) and natural language processing (Devlin et al., 2019;\nRadford et al., 2018), where fast adaptation to various down-\nWork done during the first author’s internship at Tencent AI Lab.\n1John Hopcroft Center for Computer Science, Shanghai Jiao Tong\nUniversity, Shanghai, China 2Tencent AI Lab, Shenzhen, China.\nCorrespondence to: Deheng Ye <dericye@tencent.com>, Shuai\nLi <shuaili8@sjtu.edu.cn>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\nstream tasks can be achieved with a unified model trained\non a diverse corpus of data. This trend has spurred inter-\nest in applying similar paradigms to reinforcement learning\n(RL), resulting in the prevalence of offline RL (Levine et al.,\n2020; Kumar et al., 2020; Agarwal et al., 2020). Offline\nRL aims to learn a task-solving policy exclusively from\na static dataset of reward-labeled trajectories. Given the\nresemblance between offline RL and supervised learning,\nrecent research has further explored the direction of con-\nverting offline RL (Chen et al., 2021; Janner et al., 2021)\nor offline-to-online RL (Zheng et al., 2022) to a sequence\nmodeling problem and using the expressive Transformer\narchitecture (Vaswani et al., 2017) for decision making.\nAt the heart of these Transformer-based approaches is the\nidea of conditioning policies on a desired outcome. For ex-\nample, Decision Transformer (DT, Chen et al. 2020) learns\na model to predict actions based on historical context and\na target future return. By associating decisions with fu-\nture returns, DT can perform credit assignment across long\ntime spans, showing strong performance on various offline\ntasks. While promising, DT presents an incomplete pic-\nture as reward-labeled datasets are required to train return-\nconditioned policies. In practice, task rewards are usually\nhard to access and poorly scalable to large-scale pretraining.\nBesides, eschewing rewards during pretraining also allows\nthe model to acquire generic behaviors that can be easily\nadapted for use in different downstream tasks.\nIn this work, we aim to equip DT with the ability to learn\nfrom reward-free and sub-optimal data. Specifically, we\nconsider the pretrain-then-finetune scenario, in which the\nmodel is first trained on offline reward-free trajectories and\nthen finetuned on the target task via online interactions.\nTo effectively pretrain a model, it must be able to extract\nreusable and versatile learning signals in the absence of\nrewards. During finetuning, the model is required to quickly\nadapt to task rewards, which presents another challenge as\nto what learning signals can be aligned with rewards.\nTo address the above challenges, we propose an unsuper-\nvised RL pretraining method called Pretrained Decision\nTransformer (PDT). Inspired by recent study of future-\nconditioned supervised learning (Furuta et al., 2022; Venuto\net al., 2022; Yang et al., 2023), we enable PDT to condition\non the more generalized future trajectory information for\n1\narXiv:2305.16683v1  [cs.LG]  26 May 2023\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\naction prediction, which allows the model to learn from\nunsupervised offline data. PDT jointly learns an embed-\nding space of future trajectory as well as a future prior\nconditioned only on past information. By conditioning\naction prediction on the target future embedding, PDT is\nendowed with the ability to reason over the future. This\nability is naturally task-agnostic and can be generalized to\ndifferent task specifications. To achieve efficient online\nfinetuning in downstream tasks, one can easily retrofit the\nfuture-conditioned framework into return-conditioned su-\npervised learning by associating each future embedding to\nits return. This is realized by training a return prediction\nnetwork to predict the expected return value for each future\nembedding, which can be justified from the views of con-\ntrollable generation (Dathathri et al., 2020) and successor\nfeatures (Barreto et al., 2017). At evaluation, PDT utilizes\nthe learned future prior together with the return prediction\nnetwork to controllably sample high-return futures.\nWe evaluate PDT on a set of Gym MuJoCo tasks from the\nD4RL benchmark (Fu et al., 2020). Compared with its su-\npervised counterpart (Zheng et al., 2022), PDT exhibits very\ncompetitive performance, especially when the offline data is\nfar from expert behaviors. Our analysis further verifies that\nPDT can: 1) make different decisions when conditioned on\nvarious target futures, 2) controllably sample futures accord-\ning to their predicted returns, and 3) efficiently generalize\nto out-of-distribution tasks.\n2. Related Work\nOur work considers future conditioning as a powerful ap-\nproach for unsupervised RL pretraining. In this section,\nwe review relevant works in these two research directions.\n2.1. Future-conditioned Supervised Learning\nFuture-conditioned supervised learning has been gaining\npopularity recently in the field of RL due to its simplicity\nand competitiveness. The idea is to predict actions con-\nditioned on desired future outcomes, seeking to learn a\npolicy with a future-conditioned supervised loss. Among\nall kinds of outcomes, future rewards or returns (Schmid-\nhuber, 2019; Kumar et al., 2019) are most commonly used.\nChen et al. (2021) propose Decision Transformer (DT), a\nTransformer-based (Vaswani et al., 2017) approach to learn a\nreturn-conditioned policy for offline RL. Zheng et al. (2022)\nfurther extend DT to the offline-to-online RL setting by\nequipping DT with stochastic policies for online exploration.\nEmmons et al. (2022) show that simple feed-forward MLPs\nare also capable to learn powerful future-conditioned poli-\ncies. However, Transformer-based approaches usually show\ngood scaling properties (Lee et al., 2022), in accord with\nresults in the language domain (Kaplan et al., 2020).\nOther than target returns, future information such as\ngoals (Andrychowicz et al., 2017), trajectory statistics (Fu-\nruta et al., 2022), or learned trajectory embeddings (Yang\net al., 2023; Furuta et al., 2022) can also be used to condition\nthe policy. Leveraging future information has been explored\nas means to combat environment stochasticity (Villaflor\net al., 2022; Yang et al., 2023) or to improve value estima-\ntions for model-free RL (Venuto et al., 2022). Although our\napproach potentially enjoys these advantages, in this work\nwe are motivated in a different way and focus on how to\ntame future conditioning for unsupervised pretraining.\n2.2. Unsupervised Pretraining in RL\nOur work falls into the category of unsupervised pretraining\nin RL. A number of works have sought to improve sample\nefficiency of RL by pretraining the agent with online interac-\ntions (Eysenbach et al., 2019; Laskin et al., 2021) or offline\ntrajectories (Ajay et al., 2021; Schwarzer et al., 2021; Lin\net al., 2022) prior to finetuning on the target task.\nOnline unsupervised pretraining aims at learning generic\nskills by interacting with the environment. During pretrain-\ning, the agent is allowed to collect large-scale data from the\nenvironment without access to extrinsic rewards. To learn\ngeneric skills, existing methods use intrinsic rewards as a\nprincipled mechanism to encourage the agent to build its\nown knowledge. Based on how the intrinsic rewards are\ndesigned (Xie et al., 2022), prior works can be categorized\ninto three classes: curiosity-driven exploration (Burda et al.,\n2019), skill discovery (Eysenbach et al., 2019) and data\ncoverage maximization (Lee et al., 2019).\nAlthough online unsupervised pretraining provides an effec-\ntive framework to learn prior skills for downstream tasks,\nit requires abundant online samples which makes it sample\ninefficient. To address this issue, offline unsupervised pre-\ntraining has attracted attention. Existing works on offline\nunsupervised pretraining mainly fall into two classes. The\nfirst class is pretraining representation, aiming to learn good\nrepresentation from large-scale offline data. To name a few,\nYang & Nachum (2021) propose attentive contrastive learn-\ning (ACL) which borrows the idea of BERT to pretraining\nrepresentations that can be used to accelerate the behavior\nlearning on downstream tasks. SGI (Schwarzer et al., 2021)\ncombines self-predictive representation, inverse dynamics\nprediction and goal-conditioned RL to learn powerful rep-\nresentations for visual observations. The second class is\npretraining behavior. Baker et al. (2022) propose video pre-\ntraining (VPT), a semi-supervised scheme to utilize large-\nscale offline data without action information. Specifically,\nVPT learns an inverse dynamic model on a small-scale su-\npervised data with action information and use the model\nto provide action for effective behavior cloning and fine-\ntuning. Pertsch et al. (2020) propose a deep latent variable\n2\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\nCausal Transformer\ns1\nz1\na1\ns2\nz2\na2\nsK\nzK\naK\n…\na1\na2\naK\nFuture-conditioned Sequence Modeling\n∼𝒈𝜽(⋅∣\n)\nsK+1\naK+1\nsK+t\naK+t\n…\nzt\n∼𝒑𝜽(⋅∣\n)\nzt\nst\n∼𝑷𝜽(⋅∣\n)\nzt\nst\nRt෡\nTraining: Future Trajectory Encoding\nInference: Future Generation\nInference: Controllable Future Generation\nRt෡\n∼𝒇𝜽(⋅∣\n)\nzt\nst\n∼𝒑𝜽(⋅∣\n)\nzt\nst\nFigure 1. Overview of the proposed PDT model. PDT learns to make decisions based on future trajectory information. During training,\nfuture embeddings are extracted from the succeeding transitions by future encoder gθ. At evaluation, PDT samples future embeddings\nfrom future prior pθ conditioned on the current state. When rewards are given in the finetuning phase, PDT learns a return predictor fθ to\nsteer the sampling procedure towards high-return future embeddings by Bayes’ rule Pθ(zt | ˆRt, st) ∝pθ(zt | st)fθ( ˆRt | zt, st).\nmodel that jointly learns an embedding space of skills and\nthe skill prior from offline agent experience. Different from\nthese works, we seek to address the issue of reward-centric\noptimization in existing approaches and enable efficient\npretraining on large-scale unsupervised datasets.\n3. Preliminaries\n3.1. Markov Decision Process\nReinforcement learning (RL) typically models the environ-\nment as a Markov decision process (MDP), which can be\ndescribed as (S, A, P, r, ρ, γ), where S is the state space,\nA is the action space, P (st+1 | st, at) is the probability\ndistribution over transitions, r is the reward function, ρ is\nthe initial state distribution, and γ ∈(0, 1) is the discount\nfactor. The objective is to learn a policy that maximizes the\ncumulative return E\nhPT\nt=1 rt\ni\n.\nLet τ = (st, at)T\nt=1 = (s1, a1, s2, a2, · · · , sT , aT ) denote a\ntrajectory composed of a sequence of states and actions, and\nτi:j = (st, at)j\nt=i denote a sub-trajectory of τ 1. Separating\nrewards from state-action dynamics is deliberate, as rewards\nare task-specific and usually human-provided.\n3.2. RL via Supervised Sequence Modeling\nPrevious work (Chen et al., 2021; Janner et al., 2021; Lee\net al., 2022) has investigated casting offline RL as a sequen-\ntial modeling task. Specifically, Decision Transformer (DT,\nChen et al. 2021) takes the following trajectory representa-\n1τt:t−1 represents an empty sub-trajectory.\ntion as input, concatenating τ with rewards:\nˆτ = ( ˆR1, s1, a1, ˆR2, s2, a2, . . . , ˆRT , sT , aT ),\nwhere ˆRt = PT\nt′=t rt′, usually termed return-to-go or sim-\nply return, is the sum of future rewards from timestep\nt. After tokenization, ˆτ is fed into a GPT-based Trans-\nformer (Radford et al., 2018) which plays the role of an\nexpressive policy function approximator πθ to predict the\nnext action2. For offline RL, the policy is trained to maxi-\nmize the likelihood of actions in the reward-labeled offline\ndataset ˆD =\n\b\nˆτ (m)\tM\nm=1:\nLDT = Eˆτ∼ˆ\nD\n\" T\nX\nt=1\n−log πθ(at | ˆτ1:t−1, st, ˆRt)\n#\n.\nTo enable online finetuning, Zheng et al. (2022) further\npropose Online Decision Transformer (ODT), equipping\nDT with a stochastic policy and training the model via an\nadditional max-entropy objective:\nLODT = LDT −αEˆτ∼ˆ\nD\n\" T\nX\nt=1\nH(πθ(· | ˆτ1:t−1, st, ˆRt))\n#\n,\nwhere α is the temperature parameter (Haarnoja et al., 2018)\nto control its stochasticity.\nWhile return-conditioned approaches like DT and ODT have\ngained great attention due to their simplicity and effective-\nness when applying to offline RL problems, it requires re-\nward signals to train, which is naturally hard to scale to\n2For simplicity, we use θ to represent all the learned parameters.\n3\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\nlarge-scale pretraining. Besides, using a single scalar value\n(i.e., the target return) as input could fail to capture sufficient\nfuture information. These deficiencies motivate us to design\nnew pretraining algorithms.\n3.3. Unsupervised Pretraining\nWe consider the unsupervised pretraining regime where the\ngoal is to leverage easy-to-collect offline data free of rewards\nfor more data-efficient reinforcement learning.\nIn the pretraining phase, we assume reward-free offline\ndataset D =\n\b\nτ (m)\tM\nm=1is available. Similar to what we\nhave witnessed in the fields of computer vision (Chen et al.,\n2020; He et al., 2022) and natural language processing (De-\nvlin et al., 2019; Radford et al., 2018), it is a promising\ndirection to leverage highly sub-optimal and unlabeled train-\ning data to conduct RL pretraining.\nIn the downstream phase, rewards associated with a task\nare revealed to the agent. We expect the agent to quickly\nadapt to the task by reusing its prior knowledge learned\nin the pretraining phase. While offline finetuning is also\nreasonable, in this work we focus on online finetuning as it\nrequires agents to trade off exploration and exploitation and\nis usually considered harder (Zheng et al., 2022).\n4. Methodology\nIn this section, we first describe the future conditioning\nframework that scaffolds PDT. We then present how to pre-\ntrain PDT in an unsupervised manner and use the pretrained\nmodel for efficient task adaptation. Finally, we show the\nconnection between the ideas of PDT and successor fea-\ntures (Barreto et al., 2017). Figure 1 gives an overview of\nthe proposed PDT.\n4.1. Learning to Act by Incorporating the Future\nAs discussed earlier in Section 3.2, return-conditioned ap-\nproaches are greatly restricted by rewards. To eliminate\nthe need for reward information, we draw inspiration from\nfuture-conditioned RL (Furuta et al., 2022; Emmons et al.,\n2022) and use it to ground unsupervised pretraining.\nSpecifically, we condition the policy on future latent vari-\nables instead of target returns. Let gθ be a trajectory-level\nfuture encoder and πθ be a policy network. We condition\nπθ on future latent variables z ∼gθ(· | τ), where gθ(· | τ)\noutputs a multivariate Gaussian distribution. We expect the\nmodel to capture different behaviors seen in the offline data\nduring unsupervised pretraining. These task-agnostic be-\nhaviors can be combined to form different policies based on\ntask-specific information (i.e., rewards). Therefore, when\nthe pretrained model is applied to downstream tasks, the\nagent is able to control which behaviors to sample based\non the provided reward information. At test time, the algo-\nrithm takes the learned policy πθ along with a learned prior\npθ (z | st) to take actions:\nat ∼πθ (· | τ1:t−1, st, z) , z ∼pθ (· | st) .\nThe framework of future conditioning brings several bene-\nfits. Firstly, it allows us to disentangle rewards from target\noutcomes, opening up opportunities for large-scale unsu-\npervised pretraining. Secondly, it alleviates the issue of\ninconsistent behaviors induced by return-conditioned super-\nvised learning, where behaviors significantly deviate from\nthe intended targets (Yang et al., 2023).\n4.2. Future-conditioned Pretraining\nIn this work, similar to that used in DT (Chen et al., 2021;\nZheng et al., 2022), we use a GPT-based Transformer archi-\ntecture (Radford et al., 2018) to parameterize the policy net-\nwork. Given an input sequence (s1, a1, s2, a2, . . . , sT , aT ),\nthe behavior cloning objective resembles that of ODT:\nLBC = E\nτ∼D\nz∼gθ(·|τ)\n\" T\nX\nt=1\n−log πθ(at | τ1:t−1, st, z)\n#\n−αE\nτ∼D\nz∼gθ(·|τ)\n\" T\nX\nt=1\nH(πθ(· | τ1:t−1, st, z)\n#\n,\n(1)\nwhere H(πθ) denotes the entropy of action distribution, and\nα represents a hyperparameter that trades off the contribu-\ntion of entropy maximization. Note that here we apply the\nreparameterization trick to allow gradients to backpropagate\nthrough the future encoder.\nTraining the future encoder with the above objective allows\nus to efficiently leverage future trajectory information to\npredict actions. However, without explicit regularization,\nthe future encoder can collapse and fail to capture the full\ndistribution of future information. Besides, during execu-\ntion, we need a prior to guide the agent to sample from the\nfuture embedding space. Therefore, inspired by previous\nwork (Ajay et al., 2021; Pertsch et al., 2020), we use the\nfollowing objective to train the future encoder and a prior:\nLfuture = βEτ∼D [DKL (gθ(z | τ)∥N(0, I))]\n+ Eτ∼D [DKL (⌊gθ(z | τ)⌋∥pθ(z | st))] ,\n(2)\nwhere DKL denotes the Kullback–Leibler (KL) divergence,\n⌊·⌋denotes the stop-gradient operator and β is a hyperpa-\nrameter. The former serves as a regularization term on the\n4\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\nAlgorithm 1 Future-conditioned Pretraining\ninput policy πθ, future prior pθ, future encoder gθ, reward-\nfree offline dataset D, context length K, batch size B,\ntraining iteration I\n1: for i = 1, . . . , I do\n2:\nSample B trajectories from D according to p(τ) =\n|τ|/ P\nτ ′∈D |τ ′|\n3:\nfor each sampled trajectory τ do\n4:\nτt:t+K−1 ←a length-K sub-trajectory uniformly\nsampled from τ\n5:\nτt+K:t+2K−1 ←a length-K future sub-trajectory\n6:\nSample z ∼gθ(·|τt+K:t+2K−1)\n7:\nPredict actions at′ ∼πθ(· | τt:t′−1, st′, z), ∀t′ =\nt, . . . , t + K −1\n8:\nCalculate L = LBC + Lfuture (Equation 1,2)\n9:\nend for\n10:\nUpdate θ by gradient descent\n11: end for\nfuture encoder to constrain the capacity of the latent z (Hig-\ngins et al., 2017). The second term is applied to learn a prior\nmodel pθ(z|st) which helps sample behaviors based on the\ndistribution of offline data. In Appendix A, we discuss the\ndifferences between PDT and OPAL (Ajay et al., 2021) in\nterms of their learning objectives.\nAlgorithm 1 summarizes the future-conditioned pretraining\nprocedure for PDT. In practice, PDT takes a sub-trajectory\nas the history context and encodes the succeeding sub-\ntrajectory to obtain the future embedding. Intuitively, we\nempower the model to take actions based on future trajectory\ninformation. This allows PDT to learn robust and general\nbehaviors in an unsupervised manner.\n4.3. Return-conditioned Finetuning\nAlbeit useful to sample future latent variables and generate\nbehaviors imitating the distribution of offline data, pθ(z | st)\nfails to encode any task-specific information. Therefore, it\nis required to steer pθ(z | st) to sample futures embeddings\nthat lead to high future return during finetuning.\nThis leads to controllable generation, an active area of re-\nsearch in computer vision (Nie et al., 2021; Dhariwal &\nNichol, 2021) and natural language processing (Dathathri\net al., 2020).\nLee et al. (2022) consider applying\ncontrollable generation to generate expert behaviors for\nreturn-conditioned DT. In contrast to controlling a return-\nconditioned policy by assigning a scalar target return, we\nneed to address a more challenging problem of assigning\ncredits to the future. By Bayes’ rule, we have:\np(z | ˆRt, st) ∝p(z | st)p( ˆRt | z, st),\nwhich suggests that we can sample the desired high-return\nfuture embedding by steering the future prior with p( ˆRt |\nz, st). Since this distribution is unknown, we use a return\nprediction network fθ(· | z, s) to predict p( ˆRt | z, st). We\nparameterize p( ˆRt | z, st) as a Gaussian distribution with\nlearned mean and variance. The return prediction network is\ntrained along with all the other objectives during finetuning:\nLreturn = E\nˆτ∼ˆ\nD\nz∼gθ(·|ˆτ)\nh\n−log fθ\n\u0010\nˆRt | z, st\n\u0011i\n.\n(3)\nSimilar to Equation 1, we apply the reparameterization trick\nfor the future encoder gθ in Equation 3. This allows gradi-\nents to backpropagate to the future encoder, regularizing it to\nencode task-specific reward information during finetuning.\nIn practice, we warm-up the return predictor with online\nexploration transitions at the beginning of the finetuning\nphase. We consider directly sampling high-return futures,\nrather than conditioning on a high target return. Specifically,\na batch of future embeddings is randomly sampled from\nthe future prior model pθ and the one with the highest pre-\ndicted return is selected to condition πθ during inference,\nwhich eliminates the need for a pre-determined target return.\nAlgorithm 2 summaries the finetuning procedure for PDT.\n4.4. Connection to Successor Features\nAs discussed in the previous section, our approach assigns\nreturn values to future embeddings for fast task adaptation.\nThis bears resemblance to successor features (SFs, Barreto\net al. 2017), a framework for transfer learning in RL. SFs\nassume that the one-step rewards can be written as:\nr (s, a) = ϕ (s, a)⊤w,\n(4)\nwhere ϕ (s, a) represents the task-agnostic dynamics of\nthe environment whereas w specifies the task prefer-\nence. Based on this assumption, the action-value func-\ntion widely applied in model-free RL algorithms is\ngiven by Qπ(s, a) = ψπ(s, a)⊤w, where ψπ(s, a) =\nEπ [P∞\nt=0 γtϕ (st+1, at+1) | st = s, at = a] is the SF sum-\nmarizing the dynamics induced by π in the future.\nWe establish the connection between PDT and SFs by ap-\nplying the assumption in Equation 4 to returns:\nˆRt =\nT\nX\nt′=t\nrt′ =\n\" T\nX\nt′=t\nϕ(st′+1, at′+1)\n#⊤\nw.\nHere, feature encoder ϕ needs to be determined, and our\nfuture encoder gθ can be a good candidate to directly en-\ncode the summation. Since gθ is pretrained on reward-free\noffline trajectories, it naturally encodes information about\nthe environment dynamics. This allows fast task adaptation\n5\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\ndataset\nMean\nMin\nMax\nSAC\nACL\nPDT-0\nPDT\nδPDT\nODT-0\nODT\nδODT\nhopper-m\n44.32\n10.33\n99.63\n24.22 ± 10.55\n57.66 ± 6.23\n53.74\n95.26 ± 1.77\n41.52\n66.01\n87.22 ± 6.85\n21.22\nhopper-m-r\n14.98\n0.58\n98.73\n51.68 ± 48.74\n28.56\n84.96 ± 5.49\n56.40\n74.36\n75.31 ± 6.22\n0.95\nwalker2d-m\n62.09\n-0.18\n92.04\n35.26 ± 23.51\n60.21 ± 27.08\n73.70\n75.24 ± 4.60\n1.53\n72.80\n72.62 ± 5.51\n-0.18\nwalker2d-m-r\n14.84\n-1.13\n89.97\n87.54 ± 7.31\n15.64\n58.58 ± 14.78\n42.94\n73.27\n70.54 ± 2.89\n-2.73\nhalfcheetah-m\n40.68\n-0.24\n45.02\n57.05 ± 3.89\n46.59 ± 2.71\n42.86\n37.93 ± 1.82\n-4.93\n42.69\n35.07 ± 10.40\n-7.62\nhalfcheetah-m-r\n27.17\n-2.89\n42.41\n50.56 ± 3.74\n24.83\n29.70 ± 4.97\n4.88\n40.95\n35.60 ± 1.68\n-5.35\nant-m\n80.30\n-4.85\n107.31\n33.30 ± 12.10\n28.44 ± 10.78\n93.86\n89.10 ± 6.49\n-4.77\n93.08\n73.80 ± 16.77\n-19.28\nant-m-r\n30.95\n-8.87\n96.56\n9.53 ± 1.80\n53.78\n48.18 ± 9.59\n-5.60\n90.37\n60.48 ± 6.23\n-29.89\nsum\n315.33\n-7.25\n671.67\n392.22\n386.96\n518.93\n123.94\n553.53\n510.65\n-42.87\nTable 1. Performance on Gym MuJoCo tasks. We run each instance for 200k online transitions, and measure the finetuning performance\nby the averaged normalized return over 3 random seeds. We also report the zero-shot performance of the pretrained model with suffix\n“-0”. δx shows the performance gain during online finetuning. The best performance for each task is highlighted in bold.\nin the downstream finetuning phase, which is achieved by\nlearning task-specific w with the return prediction network.\n5. Experiments\nWe conduct several experiments to ascertain the effective-\nness of PDT, with the aim to gain insights into the following:\n• Can PDT extract rich prior knowledge from reward-\nfree offline data to facilitate downstream learning?\n• Is unsupervised pretraining comparable with super-\nvised pretraining? Does PDT achieve better general-\nization than its supervised counterpart?\n• How do future conditioning and controllable sampling\nrespectively contribute to PDT’s performance?\n5.1. Baselines\nWe compare the performance of PDT to several competi-\ntive baselines, including both unsupervised and supervised\npretraining methods.\n• Soft Actor-Critic (SAC, Haarnoja et al. 2018) trains a\noff-policy agent from scratch. We include this baseline\nto test the benefit of leveraging prior experience.\n• Attentive Contrastive Learning (ACL, Yang &\nNachum 2021) considers various representation learn-\ning objectives to pretrain state representations on of-\nfline data. The pretrained representations are then com-\nbined with SAC to solve downstream tasks.\n• Online Decision Transformer (ODT, Zheng et al.\n2022) pretrains a stochastic return-conditioned policy\nusing reward-labeled offline data. This baseline serves\nas a supervised counterpart of PDT, and we include it\nto test whether supervised pretraining is superior to the\nproposed unsuperivsed method.\nFor SAC, we use the open-source codebase3 and the hy-\nperparameters in Haarnoja et al. (2018). For ACL, we use\nthe official implementation4, and choose the most compet-\nitive reward-free variant and the hyperparameters in Yang\n& Nachum (2021). We use the official implementation5\nfor ODT. Our PDT implementation is based on the ODT\ncodebase. Please see Appendix C.1 for more details.\n5.2. Benchmark Datasets\nWe evaluate our method on the Gym MuJoCo datasets from\nD4RL (Fu et al., 2020). These datasets consiste of offline\ntrajectories collected by partially trained policies in four sim-\nulated locomotion domains: halfcheetah, hopper, walker2d,\nand ant. Since we care most about how well PDT learns\nfrom sub-optimal data, we choose the medium and medium-\nreplay datasets whose trajectories are far from task-solving.\nDifferent from offline RL, unsupervised pretraining assumes\nthat rewards are not available to agents when pretraining\non the offline data. After the pretraining phase, agents\nare allowed to interact with the environment, finetuning\ntheir policy from its own behaviors and the corresponding\nrewards. Following Zheng et al. (2022), we consider a\nrelatively small budget of 200k online interactions. This\nrequires the agent to learn from unlabeled data effectively\nand adapt to downstream tasks quickly. While Zheng et al.\n(2022) use offline pretraining data to initialize the replay\nbuffer for finetuning, it is infeasible for the considered base-\nlines to utilize reward-free data. Therefore, we adopt the\nstandard protocol where the agent uses its own rollouts to\ninitialize the replay buffer for finetuning.\n5.3. Gym MuJoCo\nWe train all instances with 3 random seeds. At evaluation,\nwe run the policy for 10 episodes and compute the average\n3https://github.com/denisyarats/pytorch_\nsac\n4https://github.com/google-research/\ngoogle-research/tree/master/rl_repr\n5https://github.com/facebookresearch/\nonline-dt\n6\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\n0\n50k\n100k\n150k\n200k\nonline samples\n0\n20\n40\n60\n80\n100\nnormalized score\nSAC\nACL\nODT\nPDT\n(a) hopper-medium\n0\n50k\n100k\n150k\n200k\nonline samples\n0\n20\n40\n60\n80\n100\nnormalized score\n(b) hopper-medium-replay\n0\n50k\n100k\n150k\n200k\nonline samples\n0\n20\n40\n60\n80\nnormalized score\n(c) walker2d-medium\n0\n50k\n100k\n150k\n200k\nonline samples\n0\n20\n40\n60\n80\nnormalized score\n(d) walker2d-medium-replay\nFigure 2. Downstream performance on Gym MuJoCO tasks. Each instance is evaluated over 10 episodes every 10k transitions. Results\nare averaged over 3 random seeds. The shaded area shows the 95% confidence interval. The purple dashed line represents the averaged\nepisodic return of offline trajectories, whereas the dashed lines in gray represent the min and max values.\nreturn. The reported results are normalized according to\nFu et al. (2020). Table 1 summaries the main results and\nFigure 2 shows the evaluation results during finetuning.\nIn comparison to learning from scratch using SAC, all the\npretraining approaches improve sample efficiency substan-\ntially. A closer look at the finetuning phase suggests that\nPDT not only shows good initial performance by learning\nto act based on the future, but also quickly adapts to the\ntarget task under the supervision of task rewards. Due to the\nlack of prior knowledge, SAC starts with poor initial per-\nformance and heavily relies on online exploration to collect\ntransitions, which results in relatively low sample efficiency.\nThe results of ACL suggest that pretrained representations\ncan provide improvements in terms of sample efficiency in\nmost cases, but we also observe the phenomenon of nega-\ntive transfer on ant-medium-replay where ACL performs\nfar behind SAC. Besides, since ACL only pretrains state\nrepresentations, it lacks the ability to reuse behaviors that\ncan potentially benefit the target task.\nPerhaps a more interesting observation is that PDT per-\nforms on par with its supervised counterpart ODT. If we\nonly consider offline pretraining, ODT outperforms PDT\nsubstantially with the help of supervision. But when fine-\ntuning, PDT exhibits better sample efficiency, especially\nwhen the model is pretrained on the replay datasets (e.g.,\nhopper-medium-replay) composed of highly sub-optimal\ntrajectories. This suggests that PDT can quickly associate\nfutures with return values and latch on high-return behaviors\nfor data-efficient finetuning.\nTo support that PDT achieves non-trivial performance, we\nalso compare PDT with Rewardless-DT, a simple base-\nline that builds on ODT but masks out return embeddings\nduring pretraining. Figure 7 shows the finetuning perfor-\nmances of the sequential modeling methods. We observe\nthat Rewardless-DT exhibits relatively good performance on\nthe medium datasets. However, it struggles on the medium-\nreplay datasets. This demonstrates that PDT can extract\nreusable learning signals from unsupervised pretraining,\nparticularly in cases where the offline data is sub-optimal.\n5.4. Analysis\nIn this section, we seek to gain insight into the key compo-\nnents of PDT, including future conditioning and controllable\nsampling. Out investigation focuses on how the choice of\nfuture embeddings impacts generation, as well as the effect\nof regularization on future embeddings. We also conduct an\nevaluation to test whether PDT exhibits better generalization\ncapabilities as compared to its supervised counterpart.\nFuture conditioning.\nIdeally, future conditioning en-\nables PDT to behave differently based on different target\nfutures. The ability to reason about the future can be very\nbeneficial, especially at the early stage of an episode when\nthe agent has few history transitions to ground decisions.\nFigure 3 shows the action distributions given by a pretrained\nPDT agent at the initial state of an episode, conditioned on\nthree different future embeddings sampled from the future\nprior. We can observe that PDT makes clearly different\ndecisions in response to the choice of target future. Fig-\nure 8 further demonstrates that, as the episode continues,\nPDT relies more on its history transitions to make decisions\nwhereas future information plays a less important role.\nPerformance vs. regularization.\nβ, which controls the\nregularization strength over future embeddings, is an impor-\ntant hyperparameter for PDT. Intuitively, a large β value\ndiscourages the policy to learn diverse behaviors, as mini-\nmizing the KL divergence limits how much future informa-\ntion is contained in the latent vectors. In contrast, if β is\ntoo small, PDT tends to overly rely on the privileged future\ninformation, in the sense that the latent vectors will contain\nsufficient information so that PDT ignores its own past. Fig-\nure 4 shows that, when pretrained on the medium datasets,\nPDT usually achieves better finetuning performance with\nlarger β. When pretrained on the medium-replay datasets,\nPDT favors a smaller β value for finetuning.\n7\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\n1.0\n0.5\n0.0\n0.5\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ndim 0\nfuture 0\nfuture 1\nfuture 2\n1.0\n0.5\n0.0\n0.5\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\ndim 1\n1.0\n0.5\n0.0\n0.5\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\ndim 2\n1.0\n0.5\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\ndim 3\n1.0\n0.5\n0.0\n0.5\n1.0\n0\n1\n2\n3\n4\n5\n6\ndim 4\n1.0\n0.5\n0.0\n0.5\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ndim 5\nFigure 3. Future conditioning enables diverse behavior genera-\ntion. We plot the histogram of action distribution produced by\nPDT for each dimension of the action space. The result is taken\nfrom the initial state of an episode when running PDT pretrained\non the walker2d-medium dataset. See Appendix C.3 for details.\nBehavior diversity vs. regularization.\nTo further investi-\ngate the connection between the strength of regularization\nand the diversity of generated behaviors, we conduct an-\nother set of experiments. For each PDT policy, we evaluate\nits behavior diversity by how action distributions vary with\ndifferent future embeddings to condition the policy. The\nmore dissimilar the action distributions are, the more diverse\nbehaviors PDT can generate by sampling different future\nembeddings.\nAs shown in Figure 9, β = 1 consistently leads to less diverse\nbehaviors. Besides, we observe that phenomenon becomes\nmore pronounced on x-medium-replay datasets, indicating\nthat we can regulate the behavior diversity exhibited by PDT\nwhen dealing with diverse data.\nControllable sampling.\nTo steer the pretrained model to\nperform certain behaviors as specified by the downstream\ntask, PDT uses a learned prior of future together with a re-\nturn prediction network to sample high-return target futures\nhopper-medium\nhopper-medium-replay\nwalker2d-medium walker2d-medium-replay\n0\n20\n40\n60\n80\n100\nnormalized score\nbeta\n1\n0.1\n0.0001\n0\nFigure 4. Ablation results of future regularization for down-\nstream performance. We run each instance for 200k online tran-\nsitions and report the averaged normalized return over 3 random\nseeds. The error bar shows the 95% confidence interval.\nhopper-medium\nhopper-medium-replay\nwalker2d-medium walker2d-medium-replay\n0\n20\n40\n60\n80\n100\nnormalized score\npercentile\n100%\n75%\n50%\n25%\n0%\nFigure 5. Ablation of controllable sampling for downstream per-\nformance. We condition PDT on future embeddings with different\npercentiles of predicted return. We run each instance for 250 online\nexploration episodes and report the averaged normalized return\nover 3 random seeds. The error bar shows the 95% confidence\ninterval. See Appendix C.4 for more details.\nas contexts to take actions. Hence, it is crucial for the return\nprediction network to faithfully associate futures with their\nground-truth returns. To evaluate this, we condition PDT\non the x-percentile of the predicted returns during online\nfinetuning (e.g., x = 100% recovers the original strategy),\nand varies the choice of x.\nFigure 5 shows the performance of PDT after 250 online\nexploration episodes. We can observe that PDT achieves\nvaried outcomes when conditioned on different target fu-\ntures. This again verifies our findings in our former anal-\nysis that PDT relies heavily on target future information\nto make decisions. Besides, the results demonstrate that\nthe return prediction network is effective in filtering high-\nreturn futures out of all the possibilities. This property is\npivotal, as those reusable target futures can be retrieved\nin the finetuning phase for fast adaptation. We also find\nthat the performance differences are more prominent on the\nmedium-replay datasets. This could be explained by the\nfact that medium-replay datasets consist of more diverse\nbehaviors. In such cases, controllable sampling plays an\nimportant role to distinguish the desired ones.\nMore ablation studies.\nWe also compare PDT with its\nvariants to examine how the inclusion of future latent vari-\nables affects performance.\nTo investigate how future conditioning contributes to pre-\ntraining performance, we compare PDT with a variant that\nmasks out future embedding input for action prediction dur-\ning pretraining. Masking out future embeddings disables\nPDT to take actions based on the future and hence is helpful\nin ascertaining whether PDT benefits from future informa-\ntion or just memorizes offline behaviors. We observe that\npretraining with future embeddings masked leads to signifi-\ncantly degraded performance, as shown in Figure 6.\nNext, we compare PDT with a variant that freezes all the pa-\nrameters except for those of the return predictor during fine-\ntuning. Figure 6 shows that, by mining reward-maximizing\n8\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\n0\n50k\n100k\n150k\n200k\nonline samples\n20\n40\n60\n80\n100\nnormalized score\nPDT\nPDT w/ masked future\nPDT w/ frozen backbone\n(a) hopper-medium\n0\n50k\n100k\n150k\n200k\nonline samples\n0\n20\n40\n60\n80\n100\nnormalized score\n(b) hopper-medium-replay\nFigure 6. Ablation results of PDT training. We report finetuning\nresults averaged over 3 random seeds. The shaded area shows the\n95% confidence interval. The purple dashed line represents the\naveraged episodic return of offline trajectories, whereas the dashed\nlines in gray represent the min and max values.\nactions out of diverse behaviors, the return prediction net-\nwork plays an essential role so that PDT can continually\nrefine its actions in the finetuning phase. On the flip side, the\nresults also demonstrate that solely relying on controllable\nsampling is insufficient for online finetuning.\nGeneralization.\nIn the previous experiments we describe\nthe performance of different pretraining algorithms on the\nstandard D4RL datasets. However, given that the offline\ntrajectories used for pretraining are collected from agents\nsolving the same tasks, it raises a question on how well\ndo the pretrained models generalize to a variety of down-\nstream tasks. To examine this ability of PDT, we modify\nthe reward functions (i.e., task specifications) of halfcheetah\nand walker2d in the finetuning phase and test whether the\npretrained models can quickly adapt to new tasks.\nResults are presented in Table 2. After 200k online tran-\nsitions of downstream task, PDT outperforms ODT by a\nlarge margin. This reveals the advantages of unsupervised\npretraining over supervised pretraining. Since ODT learns\nfrom reward-labeled data, it tends to rely on target returns\nfor decision making and hence struggles to improve when\na new task is specified. In contrast, PDT associates deci-\nsions to the task-agnostic future information, which can\ngeneralize well across different tasks.\n6. Conclusion\nIn this paper, we present Pretrained Decision Transformer\n(PDT), an unsupervised pretraining algorithm for reinforce-\nment learning (RL). By learning to act based on the future,\nPDT is able to extract rich prior knowledge from offline data.\nThis ability to leverage future information can be further\nexploited in the finetuning phase, as PDT associates each\nfuture possibility to its corresponding return and samples\nthe one with the highest predicted return to make better de-\ntask\nODT\nPDT\nhalfcheetah-forward-jump\n87.27 ± 14.41\n83.80 ± 2.28\nhalfcheetah-jump\n-31.00 ± 49.08\n70.39 ± 16.56\nwalker2d-forward-jump\n29.36 ± 4.55\n45.31 ± 36.81\nwalker2d-jump\n15.81 ± 14.75\n68.70 ± 2.90\nsum\n101.45\n268.21\nTable 2. Generalization performance. We run each instance for\n200k online transitions and report the averaged normalized return\nover 3 random seeds. See Appendix C.6 for more details.\ncisions. Experimental results demonstrate the effectiveness\nof PDT in comparison to a variety of competitive baselines.\nOne limitation of PDT is that it requires more training time\nand computational resources compared to DT and ODT.\nThis could result in practical challenges when the available\nresources are limited. Besides, the objective in Equation 2\ncreates a trade-off between diversity of the learned behav-\niors and behavior consistency. Empirically we find that the\noptimal value is dataset-specific. To improve future encod-\ning, more advanced techniques such as VQ-VAE (van den\nOord et al., 2017) could be applied. For future work, we\nare interested in exploring how more expressive generative\nmethods (e.g., diffusion models as policies (Janner et al.,\n2022; Wang et al., 2023)) can benefit PDT.\nAcknowledgements\nThe corresponding author Shuai Li is supported by Na-\ntional Key Research and Development Program of China\n(2022ZD0114804) and National Natural Science Founda-\ntion of China (92270201).\nReferences\nAgarwal, R., Schuurmans, D., and Norouzi, M.\nAn\noptimistic perspective on offline reinforcement learn-\ning. In Proceedings of the 37th International Confer-\nence on Machine Learning, ICML 2020, 13-18 July\n2020, Virtual Event, volume 119 of Proceedings of Ma-\nchine Learning Research, pp. 104–114. PMLR, 2020.\nURL http://proceedings.mlr.press/v119/\nagarwal20c.html.\nAjay, A., Kumar, A., Agrawal, P., Levine, S., and Nachum,\nO. OPAL: offline primitive discovery for accelerating\noffline reinforcement learning. In 9th International Con-\nference on Learning Representations, ICLR 2021, Vir-\ntual Event, Austria, May 3-7, 2021. OpenReview.net,\n2021. URL https://openreview.net/forum?\nid=V69LGwJ0lIN.\nAndrychowicz, M., Crow, D., Ray, A., Schneider, J., Fong,\nR., Welinder, P., McGrew, B., Tobin, J., Abbeel, P., and\n9\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\nZaremba, W. Hindsight experience replay. In Guyon, I.,\nvon Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R.,\nVishwanathan, S. V. N., and Garnett, R. (eds.), Advances\nin Neural Information Processing Systems 30: Annual\nConference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, pp.\n5048–5058, 2017.\nURL https://proceedings.\nneurips.cc/paper/2017/hash/\n453fadbd8a1a3af50a9df4df899537b5-Abstract.\nhtml.\nBaker,\nB.,\nAkkaya,\nI.,\nZhokov,\nP.,\nHuizinga,\nJ.,\nTang,\nJ.,\nEcoffet,\nA.,\nHoughton,\nB.,\nSampedro,\nR., and Clune, J.\nVideo pretraining (VPT): learn-\ning to act by watching unlabeled online videos.\nIn\nNeurIPS,\n2022.\nURL\nhttp://papers.\nnips.cc/paper_files/paper/2022/hash/\n9c7008aff45b5d8f0973b23e1a22ada0-Abstract-Conference.\nhtml.\nBarreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul,\nT., Silver, D., and van Hasselt, H. Successor features\nfor transfer in reinforcement learning. In Guyon, I., von\nLuxburg, U., Bengio, S., Wallach, H. M., Fergus, R.,\nVishwanathan, S. V. N., and Garnett, R. (eds.), Advances\nin Neural Information Processing Systems 30: Annual\nConference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, pp.\n4055–4065, 2017.\nURL https://proceedings.\nneurips.cc/paper/2017/hash/\n350db081a661525235354dd3e19b8c05-Abstract.\nhtml.\nBurda, Y., Edwards, H., Storkey, A. J., and Klimov, O.\nExploration by random network distillation. In 7th Inter-\nnational Conference on Learning Representations, ICLR\n2019, New Orleans, LA, USA, May 6-9, 2019. OpenRe-\nview.net, 2019. URL https://openreview.net/\nforum?id=H1lJJnR5Ym.\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A.,\nLaskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. De-\ncision transformer: Reinforcement learning via sequence\nmodeling. In Ranzato, M., Beygelzimer, A., Dauphin,\nY. N., Liang, P., and Vaughan, J. W. (eds.), Advances in\nNeural Information Processing Systems 34: Annual Con-\nference on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pp. 15084–\n15097,\n2021.\nURL https://proceedings.\nneurips.cc/paper/2021/hash/\n7f489f642a0ddb10272b5c31057f0663-Abstract.\nhtml.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. E.\nA simple framework for contrastive learning of visual\nrepresentations. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 of Proceedings\nof Machine Learning Research, pp. 1597–1607. PMLR,\n2020. URL http://proceedings.mlr.press/\nv119/chen20j.html.\nDathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E.,\nMolino, P., Yosinski, J., and Liu, R.\nPlug and play\nlanguage models: A simple approach to controlled text\ngeneration. In 8th International Conference on Learn-\ning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net, 2020. URL https:\n//openreview.net/forum?id=H1edEyBKDS.\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:\npre-training of deep bidirectional transformers for lan-\nguage understanding. In Burstein, J., Doran, C., and\nSolorio, T. (eds.), Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,\n2019, Volume 1 (Long and Short Papers), pp. 4171–4186.\nAssociation for Computational Linguistics, 2019. doi:\n10.18653/v1/n19-1423.\nURL https://doi.org/\n10.18653/v1/n19-1423.\nDhariwal, P. and Nichol, A. Q.\nDiffusion models\nbeat gans on image synthesis.\nIn Ranzato, M.,\nBeygelzimer, A., Dauphin, Y. N., Liang, P., and\nVaughan, J. W. (eds.), Advances in Neural Infor-\nmation Processing Systems 34:\nAnnual Conference\non Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pp. 8780–\n8794,\n2021.\nURL\nhttps://proceedings.\nneurips.cc/paper/2021/hash/\n49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.\nhtml.\nEmmons, S., Eysenbach, B., Kostrikov, I., and Levine, S.\nRvs: What is essential for offline RL via supervised\nlearning?\nIn The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net, 2022. URL https:\n//openreview.net/forum?id=S874XAIpkR-.\nEysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diver-\nsity is all you need: Learning skills without a reward\nfunction. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019. OpenReview.net, 2019. URL https:\n//openreview.net/forum?id=SJx63jRqFm.\nFu, J., Kumar, A., Nachum, O., Tucker, G., and Levine,\nS. D4RL: datasets for deep data-driven reinforcement\nlearning. CoRR, abs/2004.07219, 2020. URL https:\n//arxiv.org/abs/2004.07219.\n10\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\nFuruta, H., Matsuo, Y., and Gu, S. S. Generalized deci-\nsion transformer for offline hindsight information match-\ning. In The Tenth International Conference on Learn-\ning Representations, ICLR 2022, Virtual Event, April\n25-29, 2022. OpenReview.net, 2022.\nURL https:\n//openreview.net/forum?id=CAjxVodl_v.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S.\nSoft actor-critic: Off-policy maximum entropy deep\nreinforcement learning with a stochastic actor.\nIn\nDy, J. G. and Krause, A. (eds.), Proceedings of\nthe 35th International Conference on Machine Learn-\ning, ICML 2018, Stockholmsm¨assan, Stockholm, Swe-\nden, July 10-15, 2018, volume 80 of Proceedings of\nMachine Learning Research, pp. 1856–1865. PMLR,\n2018. URL http://proceedings.mlr.press/\nv80/haarnoja18b.html.\nHe, K., Chen, X., Xie, S., Li, Y., Doll´ar, P., and Gir-\nshick, R. B. Masked autoencoders are scalable vision\nlearners. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, CVPR 2022, New Orleans, LA,\nUSA, June 18-24, 2022, pp. 15979–15988. IEEE, 2022.\ndoi: 10.1109/CVPR52688.2022.01553. URL https://\ndoi.org/10.1109/CVPR52688.2022.01553.\nHiggins, I., Matthey, L., Pal, A., Burgess, C. P., Glo-\nrot, X., Botvinick, M. M., Mohamed, S., and Lerch-\nner, A. beta-vae: Learning basic visual concepts with\na constrained variational framework.\nIn 5th Interna-\ntional Conference on Learning Representations, ICLR\n2017, Toulon, France, April 24-26, 2017, Conference\nTrack Proceedings. OpenReview.net, 2017. URL https:\n//openreview.net/forum?id=Sy2fzU9gl.\nJanner, M., Li, Q., and Levine, S. Offline reinforcement\nlearning as one big sequence modeling problem.\nIn\nRanzato, M., Beygelzimer, A., Dauphin, Y. N., Liang,\nP., and Vaughan, J. W. (eds.), Advances in Neural\nInformation Processing Systems 34: Annual Conference\non Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pp. 1273–\n1286,\n2021.\nURL\nhttps://proceedings.\nneurips.cc/paper/2021/hash/\n099fe6b0b444c23836c4a5d07346082b-Abstract.\nhtml.\nJanner, M., Du, Y., Tenenbaum, J. B., and Levine, S. Plan-\nning with diffusion for flexible behavior synthesis. In\nChaudhuri, K., Jegelka, S., Song, L., Szepesv´ari, C., Niu,\nG., and Sabato, S. (eds.), International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Balti-\nmore, Maryland, USA, volume 162 of Proceedings of\nMachine Learning Research, pp. 9902–9915. PMLR,\n2022. URL https://proceedings.mlr.press/\nv162/janner22a.html.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\nCoRR, abs/2001.08361, 2020. URL https://arxiv.\norg/abs/2001.08361.\nKingma, D. P. and Ba, J. Adam: A method for stochas-\ntic optimization. In Bengio, Y. and LeCun, Y. (eds.),\n3rd International Conference on Learning Represen-\ntations, ICLR 2015, San Diego, CA, USA, May 7-9,\n2015, Conference Track Proceedings, 2015. URL http:\n//arxiv.org/abs/1412.6980.\nKumar, A., Peng, X. B., and Levine, S. Reward-conditioned\npolicies. CoRR, abs/1912.13465, 2019. URL http:\n//arxiv.org/abs/1912.13465.\nKumar, A., Zhou, A., Tucker, G., and Levine, S.\nConservative\nq-learning\nfor\noffline\nreinforcement\nlearning.\nIn Larochelle, H., Ranzato, M., Hadsell,\nR., Balcan, M., and Lin, H. (eds.), Advances in\nNeural Information Processing Systems 33: Annual\nConference on Neural Information Processing Sys-\ntems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual,\n2020.\nURL https://proceedings.\nneurips.cc/paper/2020/hash/\n0d2b2061826a5df3221116a5085a6052-Abstract.\nhtml.\nLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K.,\nCang, C., Pinto, L., and Abbeel, P. URLB: unsupervised\nreinforcement learning benchmark.\nIn Vanschoren,\nJ. and Yeung, S. (eds.), Proceedings of the Neural\nInformation Processing Systems Track on Datasets and\nBenchmarks 1, NeurIPS Datasets and Benchmarks\n2021, December 2021, virtual, 2021.\nURL https:\n//datasets-benchmarks-proceedings.\nneurips.cc/paper/2021/hash/\n091d584fced301b442654dd8c23b3fc9-Abstract-round2.\nhtml.\nLee,\nK.,\nNachum,\nO.,\nYang,\nM.,\nLee,\nL.,\nFree-\nman,\nD.,\nGuadarrama,\nS.,\nFischer,\nI.,\nXu,\nW.,\nJang,\nE.,\nMichalewski,\nH.,\nand\nMordatch,\nI.\nMulti-game\ndecision\ntransformers.\nIn\nNeurIPS, 2022.\nURL http://papers.nips.\ncc/paper_files/paper/2022/hash/\nb2cac94f82928a85055987d9fd44753f-Abstract-Conferen\nhtml.\nLee, L., Eysenbach, B., Parisotto, E., Xing, E. P., Levine,\nS., and Salakhutdinov, R. Efficient exploration via state\nmarginal matching. CoRR, abs/1906.05274, 2019. URL\nhttp://arxiv.org/abs/1906.05274.\nLevine, S., Kumar, A., Tucker, G., and Fu, J. Offline re-\ninforcement learning: Tutorial, review, and perspectives\n11\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\non open problems. CoRR, abs/2005.01643, 2020. URL\nhttps://arxiv.org/abs/2005.01643.\nLin, Z., Li, J., Shi, J., Ye, D., Fu, Q., and Yang, W. Juewu-\nmc: Playing minecraft with sample-efficient hierarchical\nreinforcement learning. In Raedt, L. D. (ed.), Proceed-\nings of the Thirty-First International Joint Conference\non Artificial Intelligence, IJCAI 2022, Vienna, Austria,\n23-29 July 2022, pp. 3257–3263. ijcai.org, 2022. doi:\n10.24963/ijcai.2022/452. URL https://doi.org/\n10.24963/ijcai.2022/452.\nNie, W., Vahdat, A., and Anandkumar, A. Controllable and\ncompositional generation with latent-space energy-based\nmodels. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N.,\nLiang, P., and Vaughan, J. W. (eds.), Advances in Neural\nInformation Processing Systems 34: Annual Conference\non Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pp. 13497–\n13510,\n2021.\nURL https://proceedings.\nneurips.cc/paper/2021/hash/\n701d804549a4a23d3cae801dac6c2c75-Abstract.\nhtml.\nPertsch, K., Lee, Y., and Lim, J. J. Accelerating reinforce-\nment learning with learned skill priors. In Kober, J.,\nRamos, F., and Tomlin, C. J. (eds.), 4th Conference on\nRobot Learning, CoRL 2020, 16-18 November 2020, Vir-\ntual Event / Cambridge, MA, USA, volume 155 of Pro-\nceedings of Machine Learning Research, pp. 188–204.\nPMLR, 2020. URL https://proceedings.mlr.\npress/v155/pertsch21a.html.\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\net al. Improving language understanding by generative\npre-training. 2018. URL http://openai-assets.\ns3.amazonaws.com/research-covers/\nlanguage-unsupervised/language_\nunderstanding_paper.pdf.\nSchmidhuber, J.\nReinforcement learning upside down:\nDon’t predict rewards - just map them to actions. CoRR,\nabs/1912.02875, 2019. URL http://arxiv.org/\nabs/1912.02875.\nSchwarzer, M., Rajkumar, N., Noukhovitch, M., Anand,\nA., Charlin, L., Hjelm, R. D., Bachman, P., and\nCourville, A. C.\nPretraining representations for\ndata-efficient reinforcement learning.\nIn Ranzato,\nM., Beygelzimer, A., Dauphin, Y. N., Liang, P., and\nVaughan, J. W. (eds.), Advances in Neural Informa-\ntion Processing Systems 34: Annual Conference on\nNeural Information Processing Systems 2021, NeurIPS\n2021, December 6-14, 2021, virtual, pp. 12686–\n12699,\n2021.\nURL https://proceedings.\nneurips.cc/paper/2021/hash/\n69eba34671b3ef1ef38ee85caae6b2a1-Abstract.\nhtml.\nTomczak, J. M. and Welling, M. VAE with a vampprior.\nIn Storkey, A. J. and P´erez-Cruz, F. (eds.), International\nConference on Artificial Intelligence and Statistics, AIS-\nTATS 2018, 9-11 April 2018, Playa Blanca, Lanzarote,\nCanary Islands, Spain, volume 84 of Proceedings of\nMachine Learning Research, pp. 1214–1223. PMLR,\n2018. URL http://proceedings.mlr.press/\nv84/tomczak18a.html.\nvan den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural\ndiscrete representation learning.\nIn Guyon, I., von\nLuxburg, U., Bengio, S., Wallach, H. M., Fergus, R.,\nVishwanathan, S. V. N., and Garnett, R. (eds.), Advances\nin Neural Information Processing Systems 30: Annual\nConference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, pp.\n6306–6315, 2017.\nURL https://proceedings.\nneurips.cc/paper/2017/hash/\n7a98af17e63a0ac09ce2e96d03992fbc-Abstract.\nhtml.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\nJones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I.\nAttention is all you need. In Guyon, I., von Luxburg, U.,\nBengio, S., Wallach, H. M., Fergus, R., Vishwanathan,\nS. V. N., and Garnett, R. (eds.), Advances in Neural\nInformation Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017,\nDecember 4-9, 2017, Long Beach, CA, USA, pp. 5998–\n6008,\n2017.\nURL\nhttps://proceedings.\nneurips.cc/paper/2017/hash/\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.\nhtml.\nVenuto, D., Lau, E., Precup, D., and Nachum, O. Policy\ngradients incorporating the future. In The Tenth Inter-\nnational Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022. OpenReview.net,\n2022. URL https://openreview.net/forum?\nid=EHaUTlm2eHg.\nVillaflor, A. R., Huang, Z., Pande, S., Dolan, J. M., and\nSchneider, J.\nAddressing optimism bias in sequence\nmodeling for reinforcement learning.\nIn Chaudhuri,\nK., Jegelka, S., Song, L., Szepesv´ari, C., Niu, G.,\nand Sabato, S. (eds.), International Conference on Ma-\nchine Learning, ICML 2022, 17-23 July 2022, Balti-\nmore, Maryland, USA, volume 162 of Proceedings of\nMachine Learning Research, pp. 22270–22283. PMLR,\n2022. URL https://proceedings.mlr.press/\nv162/villaflor22a.html.\nWang, Z., Hunt, J. J., and Zhou, M. Diffusion policies as an\nexpressive policy class for offline reinforcement learning.\n12\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\nIn The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.\nnet/forum?id=AHvFDPi-FA.\nXie, Z., Lin, Z., Li, J., Li, S., and Ye, D. Pretraining in deep\nreinforcement learning: A survey. CoRR, abs/2211.03959,\n2022. doi: 10.48550/arXiv.2211.03959. URL https:\n//doi.org/10.48550/arXiv.2211.03959.\nYang, M. and Nachum, O. Representation matters: Offline\npretraining for sequential decision making. In Meila, M.\nand Zhang, T. (eds.), Proceedings of the 38th Interna-\ntional Conference on Machine Learning, ICML 2021, 18-\n24 July 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research, pp. 11784–11794. PMLR,\n2021. URL http://proceedings.mlr.press/\nv139/yang21h.html.\nYang, S., Schuurmans, D., Abbeel, P., and Nachum,\nO.\nDichotomy of control: Separating what you can\ncontrol from what you cannot.\nIn The Eleventh In-\nternational Conference on Learning Representations,\n2023. URL https://openreview.net/forum?\nid=DEGjDDV22pI.\nYou, Y., Li, J., Reddi, S. J., Hseu, J., Kumar, S., Bho-\njanapalli, S., Song, X., Demmel, J., Keutzer, K., and\nHsieh, C. Large batch optimization for deep learning:\nTraining BERT in 76 minutes. In 8th International Con-\nference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net,\n2020. URL https://openreview.net/forum?\nid=Syx4wnEtvH.\nYu, T., Kumar, A., Chebotar, Y., Hausman, K., Levine,\nS., and Finn, C. Conservative data sharing for multi-\ntask offline reinforcement learning.\nIn Ranzato, M.,\nBeygelzimer, A., Dauphin, Y. N., Liang, P., and\nVaughan, J. W. (eds.), Advances in Neural Informa-\ntion Processing Systems 34: Annual Conference on\nNeural Information Processing Systems 2021, NeurIPS\n2021, December 6-14, 2021, virtual, pp. 11501–\n11516,\n2021.\nURL https://proceedings.\nneurips.cc/paper/2021/hash/\n5fd2c06f558321eff612bbbe455f6fbd-Abstract.\nhtml.\nZheng, Q., Zhang, A., and Grover, A. Online decision\ntransformer. In Chaudhuri, K., Jegelka, S., Song, L.,\nSzepesv´ari, C., Niu, G., and Sabato, S. (eds.), Interna-\ntional Conference on Machine Learning, ICML 2022, 17-\n23 July 2022, Baltimore, Maryland, USA, volume 162 of\nProceedings of Machine Learning Research, pp. 27042–\n27059. PMLR, 2022. URL https://proceedings.\nmlr.press/v162/zheng22c.html.\n13\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\nA. Comparison to OPAL\nThe learning objective of future embeddings in Equation 2 bears resemblance to that of OPAL (Ajay et al., 2021). Both PDT\nand OPAL encode state-action pairs into a latent space and use a latent-conditioned policy as the action decoder. However,\nwe want to clarify their differences as follows:\n• Primitive behaviors vs future information. Essentially, OPAL does not leverage future information for learning\ncontrol. For a given trajectory τt:t+K−1, it extracts a latent vector and maximizes the conditional log-likelihood of\nactions in τt:t+K−1 given the state and the latent vector. Since the latent vector already contains information about\nthe actions in τt:t+K−1, OPAL aims to distill behaviors into temporally extended primitives, rather than to learn a\nfuture-conditioned policy. In contrast, PDT learns a future encoder that embeds future trajectory τt+K:t+2K−1 into\nlatent space so that the latent-conditioned policy can leverage this privileged context to predict actions in τt:t+K−1.\n• Single latent vs latent sequence. OPAL encodes the whole trajectory of state-action pairs into a single latent vector,\nwhereas PDT encodes the future trajectory into a sequence of latent vectors with causal masking. Similarly to the\nreturn-to-go sequence in DT (Chen et al., 2021; Zheng et al., 2022), latent sequences allow PDT to perceive more\nfuture information along the trajectory.\n• Regularization. OPAL does not regularize the latent distribution, whereas PDT regularizes by minimizing the KL\ndivergence between the latent distribution and the standard Gaussian distribution. There is no rule of thumb as to\nwhich is better. Empirically, we found that applying regularization is stable, probably because it can prevent PDT from\nover-reliance on the privileged future information (see Figure 4). However, we acknowledge that this design could be\nfurther improved, in directions like mixture of Gaussians regularization (Tomczak & Welling, 2018).\nB. Pseudocode for PDT Finetuning\nAlgorithm 2 Online Finetuning\ninput policy πθ, future prior pθ, future encoder gθ, return prediction network fθ, replay buffer D, context length K, batch\nsize B, number of online rollouts iteration N, training iteration I\n1: Initialize replay buffer ˆD with online rollouts\n2: Warmup return predictor fθ with ˆD\n3: for n = 1, . . . , N do\n4:\nˆτ ←rollout trajectory with πθ, pθ, fθ using the controllable sampling mechanism in Section 4.3\n5:\nˆD ←{ ˆD} ∪{ˆτ}\n6:\nfor i = 1, . . . , I do\n7:\nSample B trajectories from ˆD according to p(ˆτ) = ˆR(ˆτ)/ P\nˆτ ′∈ˆ\nD ˆR(ˆτ ′)\n8:\nfor each sampled trajectory ˆτ do\n9:\nˆτt:t+K−1 ←a length-K sub-trajectory uniformly sampled from ˆτ\n10:\nˆτt+K:t+2K−1 ←a length-K future sub-trajectory\n11:\nSample z ∼gθ(·|τt+K:t+2K−1)\n12:\nPredict actions at′ ∼πθ(· | τt:t′−1, st′, z), ∀t′ = t, . . . , t + K −1\n13:\nPredict returns ˆRt′ ∼fθ(· | st′, z), ∀t′ = t, . . . , t + K −1\n14:\nCalculate L = LBC + Lfuture + Lreturn (Equation 1,2,3)\n15:\nend for\n16:\nUpdate θ by gradient descent\n17:\nend for\n18: end for\nC. Experiment Details\nWe conduct our experiments on a GPU cluster with 8 Nvidia 3090 graphic cards. Pretraining PDT for 50k gradient steps on\na single GPU typically takes 2-3 hours, whereas finetuning for 200k environment steps takes 6-8 hours.\n14\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\nC.1. Hyperparameters\nOur PDT implementation is based on the publicly available ODT codebase6. We parameterize the future encoder gθ using\nthe same Transformer network as the policy. The future prior network pθ uses a pair of fully connected networks with 2\nhidden layers and ReLU activation to obtain means and variances of future embedding. The return predictor fθ also uses a\npair of fully connected networks with 2 hidden layers and ReLU activation to obtain a 1-dimensional Gaussian variable.\nWe use the LAMB optimizer (You et al., 2020) to jointly optimize the policy πθ, the future encoder gθ, the future prior pθ,\nand the return prediction network fθ. The temperature parameter α is optimized by the Adam optimizer (Kingma & Ba,\n2015). Table 3 summaries the common hyperparameters for PDT. For coefficient β, we search for each task in the range of\n{1, 1e-2, 1e-3, 1e-4} on the main experiments, and fix the choice for other experiments. Table 4 lists the dataset-specific\nhyperparameters.\nFor ACL, we choose the most competitive unsupervised variant (i.e., bidir (T) + inputA/R (F) + finetune (T)) as reported\nin Yang & Nachum (2021). We also take into account the recommended embedding dimension (512) and the pretraining\nwindow size (4) that are reported to work best for online RL finetuning. ACL is pretrained for 200k gradient steps following\nthe original paper. For ODT, we use the reported hyperparameters. As suggested in Zheng et al. (2022), a long period\nof pretraining might hurt the exploration performance. Therefore, we use the model checkpoint for each task once the\npretrained model reaches the reported performance of offline pretraining. For finetuning, all ACL, ODT, and PDT collect\n10k online transitions to initialize the replay buffer at the beginning.\ndescription\nvalue\nnumber of layers\n4\nnumber of attention heads\n4\nembedding dimension\n512\nfuture latent dimension\n16\ntraining context length\n20\nevaluation context length\n5\npositional embedding\nno\nfuture sampling batch size\n256\nreturn prediction warmup steps\n1500\ndropout\n0.1\nnonlinearity function\nReLU\nbatch size\n256\nlearning rate\n0.0001\nweight decay\n0.001\ngradient norm clip\n0.25\nlearning rate warmup steps\n104\ntarget entropy\n−dim(A)\nTable 3. Common hyperparameters that are used to train PDT in all the experiments.\nC.2. Details about Rewardless-DT Baseline\nRewardless-DT has the same network architecture as ODT. The only difference is that, during pretraining, the return\nembeddings are masked to enable unsupervised learning. We use the same training protocol and hyperparameters as those of\nODT to train Rewardless-DT. Figure 7 compares the performance of Rewardless-DT, ODT, and PDT on four Gym MuJoCo\ntasks.\nC.3. Details about Future Conditioning Evaluation\n6https://github.com/facebookresearch/online-dt\n15\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\ndataset\npretraining updates\nβpretrain\nβfinetune\nhopper-medium\n20000\n1e-3\n1e-4\nhopper-medium-replay\n50000\n1\n1e-4\nwalker2d-medium\n20000\n1e-3\n1\nwalker2d-medium-replay\n50000\n1\n1e-4\nhalfcheetah-medium\n50000\n1\n1\nhalfcheetah-medium-replay\n50000\n1\n1\nant-medium\n20000\n1e-3\n1\nant-medium-replay\n20000\n1e-2\n1e-2\nTable 4. Hyperparameters we use to train PDT for each dataset.\n0\n50k\n100k\n150k\n200k\nonline samples\n20\n40\n60\n80\n100\nnormalized score\nODT\nRewardless-DT\nPDT\n(a) hopper-medium\n0\n50k\n100k\n150k\n200k\nonline samples\n0\n20\n40\n60\n80\n100\nnormalized score\n(b) hopper-medium-replay\n0\n50k\n100k\n150k\n200k\nonline samples\n0\n20\n40\n60\n80\nnormalized score\n(c) walker2d-medium\n0\n50k\n100k\n150k\n200k\nonline samples\n0\n20\n40\n60\n80\nnormalized score\n(d) walker2d-medium-replay\nFigure 7. Comparison between PDT and Transformer-based baselines on downstream finetuning performance. Each instance is\nevaluated over 10 episodes every 10k transitions. Results are averaged over 3 random seeds. The shaded area shows the 95% confidence\ninterval. The purple dashed line represents the averaged episodic return of offline trajectories, whereas the dashed lines in gray represent\nthe min and max values. See Appendix C.2 for more details about Rewardless-DT.\nThe analysis is done with an PDT agent pretrained on the walker2d-medium dataset, using the same hyperparameters\nreported in Appendix C.1. The pretrained PDT is evaluated on the walker2d task and conditioned on different future\nembeddings sampled from the pretrained future prior. Each subfigure in Figure 3 shows the distributions of action on one\ndimension of the action space. The description for each dimension can be found in the Gym documentation7.\nFigure 8 shows the action distributions of the same PDT model, at time step 500 and 1000 of the episode, respectively.\nC.4. Details about Controllable Sampling Evaluation\nThe analysis is done with PDT agents pretrained on four D4RL datasets, using the same hyperparameters reported in\nAppendix C.1. But for finetuning, we modify the sampling procedure for online exploration and evaluation. Instead of\nchoosing the future embedding with the highest predicted return (100%-percentile), we choose other percentiles.\nC.5. Details about Behavior Diversity Evaluation\nFor each PDT policy, we evaluate its behavior diversity by how action distributions vary with different future embeddings\nto condition the policy. The more dissimilar the action distributions are, the more diverse behaviors PDT can generate by\nsampling different future embeddings. Specifically, for each timestep, we sample 10 different future latent sequences from\nthe future prior and obtain the corresponding action distributions P1, . . . P10. We measure their dissimilarity by average KL\ndivergence: DKL (P1, . . . Pk) =\n1\nk(k−1)\nPk\ni,j=1 DKL (Pi∥Pj). The dissimilarity is then averaged over all the timesteps of\n10 episodes, resulting in a scalar value for each policy. We use the distributions before applying the squashing function to\ncalculate KL divergence. Results are reported in Figure 9.\n7https://www.gymlibrary.dev/environments/mujoco/walker2d/\n16\nFuture-conditioned Unsupervised Pretraining for Decision Transformer\n0.0\n0.5\n1.0\n0\n1\n2\n3\ndim 0\nfuture 0\nfuture 1\nfuture 2\n0.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\ndim 1\n0.0\n0.5\n1.0\n0\n1\n2\n3\n4\n5\n6\ndim 2\n0.4\n0.6\n0.8\n1.0\n0\n10\n20\n30\n40\n50\n60\ndim 3\n0.5\n0.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\ndim 4\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n2\n4\n6\n8\ndim 5\n(a) At timestep 500\n0.5\n0.0\n0.5\n0.0\n0.5\n1.0\n1.5\ndim 0\nfuture 0\nfuture 1\nfuture 2\n0.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\ndim 1\n0.00\n0.25\n0.50\n0.75\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\ndim 2\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n1\n2\n3\n4\ndim 3\n0.5\n0.0\n0.5\n0.0\n0.5\n1.0\n1.5\ndim 4\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n1\n2\n3\ndim 5\n(b) At timestep 1000\nFigure 8. Comparison of the action distributions produced by PDT when conditioned on different future embeddings. We plot the\nhistogram for each dimension of the action space. The result is taken from the state at timestep 500 (left) and timestep 1000 (right) of an\nepisode when running PDT pretrained on the walker2d-medium dataset. See Appendix C.3.\nhopper-medium\nhopper-medium-replay walker2d-mediumwalker2d-medium-replay\n0.000\n0.005\n0.010\n0.015\n0.020\nbehavior diversity\nbeta\n1\n0.1\n0.0001\n0\nFigure 9. Future regularization affects behavior diversity. We compare the behavior diversity of PDT finetuned with different\nhyperparameter β. See Section C.5 for details.\nC.6. Details about Generalization Tasks\nWe follow Yu et al. (2021) to construct four downstream tasks by changing the reward functions8. Specifically, we set the\nreward functions of task halfcheetah-forward-jump and halfcheetah-jump as r(s, a) = −0.1 ∗∥a∥2\n2 + velx + 15 ∗posz\nand r(s, a) = −0.1 ∗∥a∥2\n2 + 15 ∗posz. For walker2d, we set the reward functions of task walker2d-forward-jump and\nwalker2d-jump as r(s, a) = −0.001 ∗∥a∥2\n2 + velx + 10 ∗posz and r(s, a) = −0.001 ∗∥a∥2\n2 + 10 ∗posz. We use\nthe same hyperparameters as in Appendix C.1 for downstream online RL. We report scores normalized by computing\n100 ×\nscore-random score\nbest score-random score.\n8The original reward functions can be found in gymlibrary.dev.\n17\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-05-26",
  "updated": "2023-05-26"
}