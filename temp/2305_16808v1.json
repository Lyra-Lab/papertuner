{
  "id": "http://arxiv.org/abs/2305.16808v1",
  "title": "Geometric deep learning approach to knot theory",
  "authors": [
    "Lennart Jaretzki"
  ],
  "abstract": "In this paper, we introduce a novel way to use geometric deep learning for\nknot data by constructing a functor that takes knots to graphs and using graph\nneural networks. We will attempt to predict several knot invariants with this\napproach. This approach demonstrates high generalization capabilities.",
  "text": "Geometric deep learning approach to knot theory\nLennart Jaretzki\nlennart.jaretzki@gmail.com\nLeipzig Germany\nMay 29, 2023\nAbstract\nIn this paper, we introduce a novel way to use geometric deep learning for\nknot data by constructing a functor that takes knots to graphs and using graph\nneural networks. We will attempt to predict several knot invariants with this\napproach. This approach demonstrates high generalization capabilities.\n1\nIntroduction\nRecent papers have researched the application of machine learning and neural net-\nworks to knot theory. One common approach to learning knots is to use their braid\nrepresentation as input instead. This has the downside that extra crossings have to\nbe added. Also, it makes the data less dense, and the neural network has to abstract\nthe underlying knot. The neural networks used are often dense models that operate\non a fixed-sized input, sequence-to-sequence models, transformers, and other natural\nlanguage processing approaches [1, 9, 10]. There was some research conducted on using\na more geometric approach by using rectangular diagrams, but they used LSTMs in-\nstead of a geometric deep learning approach, and in the data representation, arbitrary\nchoices had to be made [3]. It seems much more intuitive to use methods from geo-\nmetric deep learning to learn knot data [13]. We address these issues by proposing a\nmethod to use graph neural networks to learn knot data. For this purpose, we construct\na functor that takes knots to graphs, which are learned by graph neural networks. We\nwill attempt to predict both geometric and combinatorial invariants. We will conduct\nexperiments using the graph transformer operator. The code for this project can be\nfound on github.\n2\nGraph construction\nUsing graph neural networks for knot learning requires a way to represent a given knot\nas a graph. One method to do this would be by using black graphs, but they suffer\n1\narXiv:2305.16808v1  [math.GT]  26 May 2023\nfrom the problem that the graph is often a self-connected graph, i.e. it contains loops.\nAnother problem is that a large number of black graphs are only pseudographs. This\nmakes it hard to apply graph neural networks to them. Our proposed functor is similar\nand takes knots to graphs by taking the faces of the graph to the vertices of the graph\nand taking edges between crossings to perpendicular graph edges. This induces taking\nthe crossings of the knot to the faces of the resulting graph. By definition, the resulting\ngraph is planar. This brings up the question if there is a unique way to reconstruct the\nknot from a given knot. Such that a unique reconstruction is possible, the edges of the\nresulting graph need to have two edge attributes. The first attribute is a measure of\ndistance that is necessary to preserve the general structure of the knot, and the second\nedge attribute is a boolean that indicates if a given edge between crossings is altering\nbetween ”over” and ”under”. Since only the alteration is given, it is not possible to\ndifferentiate chirality and handedness. Note that this procedure generalizes to links.\nIt is probably also possible to generalize the procedure to virtual knots.\nFigure 1: The described Functor at the example of the 6 3 with the crossing in the\nmiddle flipped.\n2.1\nRigorous definition\nGiven a knot, first all edges of the knot diagram need to be enumerated, i. e. labeled\nwith increasing numbers (increasing while we go along the paths that connect two\ncrossings). Next, the alteration of each edge needs to be determined. Now that we\nhave gathered this information, we can transform our knot into a graph.\nV := {F|∂F =\n[\ni∈A\nei with ∀i ∈A : ei is edge in the knot diagram, F is connected region} .\n(1)\n2\nE := {(F0, F1)|F0, F1 ∈V, F0 ∩F1 ̸= ∅} .\n(2)\nNote that this has a similar problem to black graphs because two faces may have a\ncommon edge in two or more distinct positions. This is relatively rare and did not\nhappen once in the dataset. It does occur when the knot gets twisted or two knots get\nadded.\nG = (V, E).\n(3)\nLet e ∈E:\nl(e) := label of the edge\n(4)\nNote that by definition, if and only if a, b ∈E, and a and b are on the opposite side of\nthe same crossing, then:\nl(a) ≡l(b) ± 1\nmod 2 · #Crossings(K)\n(5)\n2.2\nRestore knot\nTo restore the knot, it is necessary to find a planar embedding of the knot graph.\np : V →R2\n(6)\nSuch that p is planar, i. e. there are no two graph edges that cross each other. To\nbe able to reconstruct K uniquely, we need the additional condition that, given an\narbitrary graph face, this face is composed of exactly four edges. It is also necessary\nthat opposing edges fulfill\n“Eq. (5)”. To reconstruct the knot diagram, every face\ngets mapped to a crossing. The outgoing edges of the constructed knot crossing get\nconnected through the graph edges they are affiliated with, such that they connect\nwith the crossings created by the adjoint graph faces. The last step is to assign an\narbitrary crossing where the in-going path goes ”under” After that, you can derive the\nstate of the other crossings via the second edge attribute.\nTheorem 2.1. Two embeddings p0, p1 of G restore ambient isotopic knots.\nProof. Let v0 ∈V be an arbitrary vertex. Next, p0(G) and p1(G) need to be embedded\nin R3. Define wp(v) := (p(v), d(v0, v)) where d is a distance measure (the number of\nedges of the shortest path connecting v0 and v). We can now transform wp0(G) to\nwp1(G) by shifting the vertecies. Define ut(wp(g)) := (1 −t) · wp0(g) + t · wp1(g) with\n0 ≤t ≤1. We can find a corresponding knot embedded in R3 for each ut by embedding\nthe edges of G as straight lines and then embedding the crossings of the knot in the\ncenter of the faces that fulfill\n“Eq. (5)” while letting the knot edges intersect with\n3\nthe midpoint of the embedded graph edges. This procedure is continuous along t 1.\nSince we can also lift the two graph embeddings continuously. Applying w−1\np1 ◦u1 ◦wp0\ninduces ambient isotopy 2.\n3\nDataset\nThe foundation of the dataset consists of 2977 knots from the Knotinfo Database [2].\nThese consist of all knots up to 12 crossings.\n3.1\nData augmentation\nSince the purpose is to predict invariance, we can easily extend the dataset by applying\nrandom Reidemeister moves to the given knots. The exact algorithm used for shuffling\nthe knots is described in A. This returns a new randomly shuffled knot, which is\nambient isotopic to the original knot and is non-simplifiable, i. e. can’t be solved by\njust undoing the first and second reidemeister moves. For every knot, there are 89\nshuffled versions of the knot, such that the entire dataset of shuffled knots consists of\n264,953 knots. The mean number of crossings of these knots is 34.56, with a standard\nderivation of 11.38. Also, a dataset with 2977 knots that have at least 70 crossings was\ngenerated. Later, this dataset will be used to test how well the neural networks can\ngeneralize to bigger knots. The mean number of knot crossings in this dataset is 91.3,\nwith a standard deviation of 14.91.\n3.2\nValidation datasets\nOne validation dataset is generated by a test train split, with 20% of the data being\nused for validation. We will also attempt to measure the generalization capabilities\nby training only on knots that have 11 or fewer crossings when solved and testing on\nknots that have 12 crossings when solved. As well as testing on the knots that have at\nleast 70 crossings.\n3.3\nData preperation\nWe need to address two issues before we can use our data as input for neural networks.\nFirst of all, in the distance measure, there was an arbitrary edge chosen to enumerate\nthe edges. To work around the problem, we instead calculate the distance between the\nadjacent knot edges. Because neural networks work better with smooth values that lie\nin a predefined interval, we also apply an activation function to the distances.\n1You may have to find a homeomorphism of the embedding such that injectivity is preserved\nthroughout the entire process.\n2Note that the face may be flipped when applying u but since we only have information about the\ncrossings in relation to each other, this makes no difference.\n4\nFigure 2: Two crossings connected by one edge. The labels of the enumerated edges\nare a, b, c, d. T.\nThe edge attribute that can be used by the neural network is given by\n1 −2−d\nds .\n(7)\nwith d ≡\n|(a+b−(c+d))|\n2\nmod #Crossings(K) being the distance, i.\ne.\nthe minimal\nnumber of edges you need to walk to get from one to the other. If d is higher than ds\nthe edge attribute will be higher than 1\n2 and gets squished, while values lower than are\nmore distinguishable and can therefore be processed easier. This may be advantageous\nbecause edges that are closer together are more intertwined and are easier to process.\nds = 15 was arbitrarily chosen. Also, to make the edge attribute altering usable for\nthe network, altering and notaltering get mapped to 1 and −1 respectively.\n4\nExperiments\n4.1\nModels\nSince the information about the knot is given by the edge attributes, it is necessary to\nchoose a graph neural network architecture that can process edge attributes. One pop-\nular approach is graph convolution networks, which work by aggregating information\nfrom neighboring nodes and edges in a graph using a learned convolution operation\nto produce a new representation for each node in the graph [8]. The transformer is a\npowerful architecture that utilizes attention mechanisms and has been adeptly used in\nthe graph transformer [4, 7, 15]. We observed that the results were best when using\nthe graph transformer. We apply the layer, which we batch normalize, and apply the\ntanh activation function on this operation, which is then stacked four times. Since we\nwant to make graph-level predictions, we then use global pooling, which means that\nwe calculate the mean, max, and min of all node features of the graph. These are\nthen concatenated with 10 additional features of the given knot 3. They are then again\nnormalized, which increases results when generalizing to larger knots. These are the\nsame as in the given tensor, which is then the input for a 4-layer multi-layer perceptron\nwith 25 hidden neurons per layer. And one neuron that is predicting the feature. Then\n3Similar to [1] we use the following features: alternating, fibered, positive braid closure, small or\nlarge, crossing number, signature, arc index, determinant, and Rasmussen invariant\n5\nthe mean absolute error will be calculated, and the model parameters will be updated\nusing backpropergation and the Adam optimizer.\n4.2\nTraining\nThe model will be fitted to the data over multiple epochs with a learning rate of 0.001.\nComparison of different graph neural networks fitting different knot invariants. The\nmodels were trained, and implemented using pytorch, and pytorch geometric [5].\n4.3\nResults\nThe models have been trained over 200 epoches, and the one with the highest accuracy\nhas been picked. In the case of volume, the mean distance to the correct result is\ndisplayed.\nFeature\nused Additional\nValidation Dataset\nLarge Knot Dataset\n≤11 Results 4\nQ-Positivity\nYes\n92.38%\n91.57%\n91.13%\nRasmussen-S\nYes\n95.63\n95.36%\n-\nVolume\nYes\n0.5498\n0.7905\n0.8485\nVolume\nNo\n2.0442\n2.6864\n-\nOzsvath-Szabo τ\nYes\n99.78%\n98.79%\n99.92%\nWe can observe that even though the accuracy is not that high, the networks\ndemonstrate high generalization both to larger knots and to knots that have more\ncrossings when simplified.\nAlso note that it works better on geometric/hyperbolic\nthan on numerical/combinatorical [11, 14].\n5\nSimilar work\nWhile conducting this research, a similar approach has been published that utilizes\ngeometric deep learning to predict if two given 3-manifolds are homeomorphic [16]. This\napproach uses plumbing graphs as input. They also use techniques from reinforcement\nlearning and graph attention networks.\nA\nShuffle algorithm\nGiven a knot and a complexity c the here used shuffle algorithm starts by applying\n2 · c times random type 1 and type 2 reidemeister moves with probabilities of 20% and\n80% respectively. After this, it applies 5 random type 1 and 2 reidemeister moves with\n4When the model is trained only on knots that have crossings that are less than or equal to 11\nwhen simplified and then tested on knots that have 12 crossings when simplified.\n6\nthe same probabilities, after which ⌊c\n20⌋random type 3 reidemeister moves are applied.\nThis process is repeated c times. After this, all excess type 1 and 2 reidemeister moves\nare reversed. Increasing c increases the probability of getting bigger and more complex\nknots. This algorithm was implemented using [6].\nReferences\n[1] M. C. Hughes, A Neural Network Approach to Predicting and Computing Knot\nInvariants, J. Knot Theory Ramifications 29 (2020) 2050005.\n[2] C. Livingston and A. H. Moore, KnotInfo:\nTable of Knot Invariants, knot-\ninfo.math.indiana.edu (April 24, 2023).\n[3] Kauffman, L. & Russkikh, N. & Taimanov, I.. (2022). Rectangular knot diagrams\nclassification with deep learning. Journal of Knot Theory and Its Ramifications. 31.\n10.1142/S0218216522500675.\n[4] Shi, Yunsheng & Zhengjie, Huang & Feng, Shikun & Zhong, Hui & Wang, Wenjing\n& Sun, Yu. (2021). Masked Label Prediction: Unified Message Passing Model for\nSemi-Supervised Classification. 1548-1554. 10.24963/ijcai.2021/214.\n[5] Fey, Matthias & Lenssen, Jan. (2019). Fast Graph Representation Learning with\nPyTorch Geometric.\n[6] M. Culler,\nN. M. Dunfield,\nM. Goerner,\nand J. R. Weeks,\nSnapPy,\na\ncomputer program for studying the geometry and topology of 3-manifolds,\nhttp://snappy.computop.org\n[7] Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., & Bengio, Y. (2018).\nGraph Attention Networks. International Conference on Learning Representations.\nRetrieved from https://openreview.net/forum?id=rJXMpikCZ\n[8] Corso, Gabriele & Cavalleri, Luca & Beaini, Dominique & Lio, Pietro & Veliˇckovi´c,\nPetar. (2020). Principal Neighbourhood Aggregation for Graph Nets.\n[9] Lisitsa, Alexei & Salles, Mateo & Vernitski, Alexei. (2022). An application of neu-\nral networks to a problem in knot theory and group theory (untangling braids).\n10.48550/arXiv.2206.05373.\n[10] Halverson, James & Gukov, Sergei & Sulkowski, Piotr & Ruehle, Fabian. (2021).\nLearning to Unknot. Machine Learning: Science and Technology. 2. 10.1088/2632-\n2153/abe91f.\n[11] Jejjala,\nVishnu\n&\nKar,\nArjun\n&\nParrikar,\nOnkar.\n(2019).\nDeep\nlearn-\ning\nthe\nhyperbolic\nvolume\nof\na\nknot.\nPhysics\nLetters\nB.\n799.\n135033.\n10.1016/j.physletb.2019.135033.\n7\n[12] Craven, Jessica & Hughes, Mark & Jejjala, Vishnu & Kar, Arjun. (2023).\nLearning knot invariants across dimensions. SciPost Physics. 14. 10.21468/SciPost-\nPhys.14.2.021.\n[13] Bronstein, Michael & Bruna, Joan & Cohen, Taco & Veliˇckovi´c, Petar. (2021).\nGeometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.\n[14] Gr¨unbaum, Daniel. (2022). Narrowing the Gap between Combinatorial and Hy-\nperbolic Knot Invariants via Deep Learning.\n[15] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser,\nL. & Polosukhin, I. Attention Is All You Need. (2017)\n[16] Putrov, Pavel & Ri, Song. (2023). Graph Neural Networks and 3-Dimensional\nTopology.\n8\n",
  "categories": [
    "math.GT",
    "cs.LG",
    "57K10"
  ],
  "published": "2023-05-26",
  "updated": "2023-05-26"
}